
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/14/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.LG_2023_07_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/20/cs.LG_2023_07_20/" class="article-date">
  <time datetime="2023-07-19T16:00:00.000Z" itemprop="datePublished">2023-07-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/20/cs.LG_2023_07_20/">cs.LG - 2023-07-20 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Synthetic-Control-Methods-by-Density-Matching-under-Implicit-Endogeneity"><a href="#Synthetic-Control-Methods-by-Density-Matching-under-Implicit-Endogeneity" class="headerlink" title="Synthetic Control Methods by Density Matching under Implicit Endogeneity"></a>Synthetic Control Methods by Density Matching under Implicit Endogeneity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11127">http://arxiv.org/abs/2307.11127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masahiro Kato, Akari Ohda, Masaaki Imaizumi, Kenichiro McAlinn</li>
<li>for: 这个论文的目的是为了提出一种基于混合模型的新的同比控制方法，以估计对受试者的影响。</li>
<li>methods: 该方法使用了拟合 densities 的思想，即受试者的结果拟合到未处理单位的结果上，从而估计对受试者的影响。</li>
<li>results: 该方法可以提供准确的对受试者的影响估计，并且可以生成对受试者的影响的全 densities，不仅是预期值。<details>
<summary>Abstract</summary>
Synthetic control methods (SCMs) have become a crucial tool for causal inference in comparative case studies. The fundamental idea of SCMs is to estimate counterfactual outcomes for a treated unit by using a weighted sum of observed outcomes from untreated units. The accuracy of the synthetic control (SC) is critical for estimating the causal effect, and hence, the estimation of SC weights has been the focus of much research. In this paper, we first point out that existing SCMs suffer from an implicit endogeneity problem, which is the correlation between the outcomes of untreated units and the error term in the model of a counterfactual outcome. We show that this problem yields a bias in the causal effect estimator. We then propose a novel SCM based on density matching, assuming that the density of outcomes of the treated unit can be approximated by a weighted average of the densities of untreated units (i.e., a mixture model). Based on this assumption, we estimate SC weights by matching moments of treated outcomes and the weighted sum of moments of untreated outcomes. Our proposed method has three advantages over existing methods. First, our estimator is asymptotically unbiased under the assumption of the mixture model. Second, due to the asymptotic unbiasedness, we can reduce the mean squared error for counterfactual prediction. Third, our method generates full densities of the treatment effect, not only expected values, which broadens the applicability of SCMs. We provide experimental results to demonstrate the effectiveness of our proposed method.
</details>
<details>
<summary>摘要</summary>
synthetic control methods (SCMs) 已成为比较研究中的重要工具，用于估计 causal inference。 SCMs 的基本想法是通过使用一个权重的和 observed outcomes from untreated units 来估计 treated unit 的 counterfactual outcomes。 counterfactual outcome 的准确性是估计 causal effect 的关键，因此 SC weights 的 estimation 已经引起了很多研究的关注。在本文中，我们首先指出了现有 SCMs 存在一种隐藏的内生性问题，即 untreated units 的 outcome 和 counterfactual outcome 的模型中的偏差的相关性。我们证明这个问题会导致 causal effect 估计器偏离。然后，我们提出了一种基于density matching的新的 SCM，假设 treated unit 的 outcome 的概率密度可以被approxicate 为一个 weighted average of untreated units 的概率密度（即 mixture model）。基于这个假设，我们可以通过匹配 treated outcomes 和 weighted sum of untreated outcomes 的 moments 来估计 SC weights。我们的提出方法有以下三个优点：1. 我们的估计器是 asymptotically unbiased 的，即在 mixture model 的假设下。2. 由于 asymptotic unbiasedness，我们可以减少 counterfactual prediction 的 mean squared error。3. 我们的方法可以生成 treated effect 的全部概率密度，不仅是预期值，这扩展了 SCMs 的应用范围。我们提供了实验结果，以证明我们的提出方法的效果。
</details></li>
</ul>
<hr>
<h2 id="A-Markov-Chain-Model-for-Identifying-Changes-in-Daily-Activity-Patterns-of-People-Living-with-Dementia"><a href="#A-Markov-Chain-Model-for-Identifying-Changes-in-Daily-Activity-Patterns-of-People-Living-with-Dementia" class="headerlink" title="A Markov Chain Model for Identifying Changes in Daily Activity Patterns of People Living with Dementia"></a>A Markov Chain Model for Identifying Changes in Daily Activity Patterns of People Living with Dementia</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11126">http://arxiv.org/abs/2307.11126</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nvfl/markov-chain-model">https://github.com/nvfl/markov-chain-model</a></li>
<li>paper_authors: Nan Fletcher-Lloyd, Alina-Irina Serban, Magdalena Kolanko, David Wingfield, Danielle Wilson, Ramin Nilforooshan, Payam Barnaghi, Eyal Soreq</li>
<li>for: 这个研究是为了检测老年人忘形症患者的饮食和饮水状况，以及通过互联网物联网技术进行 дома中监测，以提高忘形症患者的生活质量。</li>
<li>methods: 这个研究使用了互联网物联网技术进行 дома中监测，并使用了线性混合效应分析和马尔可夫模型来检测老年人忘形症患者的饮食和饮水状况的变化。</li>
<li>results: 研究发现，在21户PLWD中，日间厨房活动增加，夜间厨房活动减少（t(147) &#x3D; -2.90, p &lt; 0.001）。此外，提出了一种基于远程监测数据的新的分析方法，可以用于检测PLWD的行为变化。<details>
<summary>Abstract</summary>
Malnutrition and dehydration are strongly associated with increased cognitive and functional decline in people living with dementia (PLWD), as well as an increased rate of hospitalisations in comparison to their healthy counterparts. Extreme changes in eating and drinking behaviours can often lead to malnutrition and dehydration, accelerating the progression of cognitive and functional decline and resulting in a marked reduction in quality of life. Unfortunately, there are currently no established methods by which to objectively detect such changes. Here, we present the findings of an extensive quantitative analysis conducted on in-home monitoring data collected from 73 households of PLWD using Internet of Things technologies. The Coronavirus 2019 (COVID-19) pandemic has previously been shown to have dramatically altered the behavioural habits, particularly the eating and drinking habits, of PLWD. Using the COVID-19 pandemic as a natural experiment, we conducted linear mixed-effects modelling to examine changes in mean kitchen activity within a subset of 21 households of PLWD that were continuously monitored for 499 days. We report an observable increase in day-time kitchen activity and a significant decrease in night-time kitchen activity (t(147) = -2.90, p < 0.001). We further propose a novel analytical approach to detecting changes in behaviours of PLWD using Markov modelling applied to remote monitoring data as a proxy for behaviours that cannot be directly measured. Together, these results pave the way to introduce improvements into the monitoring of PLWD in naturalistic settings and for shifting from reactive to proactive care.
</details>
<details>
<summary>摘要</summary>
营养不良和脱水是老年人失智症（PLWD）的常见问题，可能导致认知和功能退化加速，并增加入院病人数。不正常的食品和饮料消耗习惯可能导致营养不良和脱水，从而恶化认知和功能退化，并导致生活质量下降。然而，目前并无可靠的方法来评估这些变化。在这种情况下，我们提出了一种基于互联网物联网技术的评估方法，并对73户老年人失智症的家庭进行了广泛的量化分析。 COVID-19大流行在老年人失智症行为习惯中产生了深远的影响，特别是食品和饮料消耗习惯。使用COVID-19大流行作为自然实验，我们使用线性混合模型对21户老年人失智症的24小时内的厨房活动进行分析。我们发现了日间厨房活动的增加（t(147) = -2.90，p < 0.001），并且夜间厨房活动的减少。此外，我们还提出了一种基于远程监测数据的markov预测方法，用于检测老年人失智症行为变化。总之，这些结果可以帮助改进老年人失智症的监测和护理，从而转移到主动的护理。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Models-for-Probabilistic-Deconvolution-of-Galaxy-Images"><a href="#Diffusion-Models-for-Probabilistic-Deconvolution-of-Galaxy-Images" class="headerlink" title="Diffusion Models for Probabilistic Deconvolution of Galaxy Images"></a>Diffusion Models for Probabilistic Deconvolution of Galaxy Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11122">http://arxiv.org/abs/2307.11122</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yashpatel5400/galgen">https://github.com/yashpatel5400/galgen</a></li>
<li>paper_authors: Zhiwei Xue, Yuhang Li, Yash Patel, Jeffrey Regier</li>
<li>for: 这个论文是为了解决望远镜捕捉图像时的特点点雨函数（PSF）问题，即使用深度生成模型进行PSF恢复。</li>
<li>methods: 该论文使用了一种没有分类器的决策扩散模型来实现PSF恢复。</li>
<li>results: 论文表明，该扩散模型可以提供更多的可能的恢复结果，比起基于VAE的条件扩散模型。<details>
<summary>Abstract</summary>
Telescopes capture images with a particular point spread function (PSF). Inferring what an image would have looked like with a much sharper PSF, a problem known as PSF deconvolution, is ill-posed because PSF convolution is not an invertible transformation. Deep generative models are appealing for PSF deconvolution because they can infer a posterior distribution over candidate images that, if convolved with the PSF, could have generated the observation. However, classical deep generative models such as VAEs and GANs often provide inadequate sample diversity. As an alternative, we propose a classifier-free conditional diffusion model for PSF deconvolution of galaxy images. We demonstrate that this diffusion model captures a greater diversity of possible deconvolutions compared to a conditional VAE.
</details>
<details>
<summary>摘要</summary>
望远镜捕捉图像具有特定点扩函数（PSF）。根据这个函数，推断图像如果有更高的解构度，这个问题称为PSF恢复是不充分定义的，因为PSF混合不是可逆变换。深度生成模型对PSF恢复是有吸引力的，因为它们可以对候选图像进行 posterior 分布逻辑，这些图像如果与PSF混合，就可能生成观察结果。然而，经典的深度生成模型如VAEs和GANs通常提供不够多样性的样本。作为一种替代方案，我们提议一种无类别的条件扩散模型用于PSF恢复星系图像。我们表明，这种扩散模型可以捕捉更多的可能的恢复结果，比Conditional VAE更佳。
</details></li>
</ul>
<hr>
<h2 id="PASTA-Pretrained-Action-State-Transformer-Agents"><a href="#PASTA-Pretrained-Action-State-Transformer-Agents" class="headerlink" title="PASTA: Pretrained Action-State Transformer Agents"></a>PASTA: Pretrained Action-State Transformer Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10936">http://arxiv.org/abs/2307.10936</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raphael Boige, Yannis Flet-Berliac, Arthur Flajolet, Guillaume Richard, Thomas Pierrot</li>
<li>for: 这个论文的目的是研究基于自适应学习的强化学习模型，以及这些模型在多种下游任务上的性能。</li>
<li>methods: 该论文使用了一种叫做 PASTA 的模型，该模型在不同的领域中同时进行了多种基本的预训练任务，并使用了基本的预训练目标如下一个token预测。</li>
<li>results: 该论文的研究发现，使用 PASTA 模型可以在多种下游任务上实现更好的性能，包括行为假象追踪、离线强化学习、感知器故障Robustness和动力学变化适应。此外，该论文还发现，使用 parameter efficient fine-tuning (PEFT) 可以在下游适应中使用 fewer than 10,000 参数进行精细调整，这使得更多的社区可以使用这些模型并重现我们的实验。<details>
<summary>Abstract</summary>
Self-supervised learning has brought about a revolutionary paradigm shift in various computing domains, including NLP, vision, and biology. Recent approaches involve pre-training transformer models on vast amounts of unlabeled data, serving as a starting point for efficiently solving downstream tasks. In the realm of reinforcement learning, researchers have recently adapted these approaches by developing models pre-trained on expert trajectories, enabling them to address a wide range of tasks, from robotics to recommendation systems. However, existing methods mostly rely on intricate pre-training objectives tailored to specific downstream applications. This paper presents a comprehensive investigation of models we refer to as Pretrained Action-State Transformer Agents (PASTA). Our study uses a unified methodology and covers an extensive set of general downstream tasks including behavioral cloning, offline RL, sensor failure robustness, and dynamics change adaptation. Our goal is to systematically compare various design choices and provide valuable insights to practitioners for building robust models. Key highlights of our study include tokenization at the action and state component level, using fundamental pre-training objectives like next token prediction, training models across diverse domains simultaneously, and using parameter efficient fine-tuning (PEFT). The developed models in our study contain fewer than 10 million parameters and the application of PEFT enables fine-tuning of fewer than 10,000 parameters during downstream adaptation, allowing a broad community to use these models and reproduce our experiments. We hope that this study will encourage further research into the use of transformers with first-principles design choices to represent RL trajectories and contribute to robust policy learning.
</details>
<details>
<summary>摘要</summary>
自顾学学习已经在不同的计算领域引起了革命性的变革，包括自然语言处理、视觉和生物。现有的方法通常是在大量无标签数据上预训练变换器模型，作为下游任务的开始点，以提高效率。在返回学习领域，研究人员已经采用了这些方法，并开发了基于专家轨迹的模型，以解决广泛的任务，从 робо扮到推荐系统。然而，现有的方法大多依赖于特定下游应用程序的复杂预训练目标。本文发表了一项总体的调查，涵盖了我们称为预训练动作-状态变换器代理（PASTA）的模型。我们的研究使用了一种统一的方法ología，对广泛的下游任务进行了总体的调查，包括行为假象念、离线RL、感知失效鲁棒性和动力学变化适应。我们的目标是系统地比较不同的设计选择，提供价值的情况，以便实践者在建立Robust模型。关键特点包括动作和状态组件层次启用，使用基本的预训练目标如下一个元素预测，在多个领域同时训练模型，以及使用精简的 fine-tuning（PEFT）。我们的模型含 fewer than 10 million 参数，并在下游适应中使用 PEFT 进行精简，使得广泛的社区可以使用这些模型并复制我们的实验。我们希望这项研究能够鼓励更多的研究人员通过使用 transformer 模型的首要设计选择来表示RL轨迹，并贡献到Robust策略学习。
</details></li>
</ul>
<hr>
<h2 id="Inorganic-synthesis-structure-maps-in-zeolites-with-machine-learning-and-crystallographic-distances"><a href="#Inorganic-synthesis-structure-maps-in-zeolites-with-machine-learning-and-crystallographic-distances" class="headerlink" title="Inorganic synthesis-structure maps in zeolites with machine learning and crystallographic distances"></a>Inorganic synthesis-structure maps in zeolites with machine learning and crystallographic distances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10935">http://arxiv.org/abs/2307.10935</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Schwalbe-Koda, Daniel E. Widdowson, Tuan Anh Pham, Vitaliy A. Kurlin<br>for: 这篇论文旨在用计算机模拟方法研究碎 Zeolites 的合成条件和结构。methods: 这篇论文使用了一种强度距离度量和机器学习技术来创建 Zeolites 的无机合成地图。results: 研究发现，使用这种距离度量可以从已知 Zeolites 的框架结构中推断无机合成条件，并且可以用机器学习分类器来找到 Zeolites 的合成结构关系。<details>
<summary>Abstract</summary>
Zeolites are inorganic materials known for their diversity of applications, synthesis conditions, and resulting polymorphs. Although their synthesis is controlled both by inorganic and organic synthesis conditions, computational studies of zeolite synthesis have focused mostly on organic template design. In this work, we use a strong distance metric between crystal structures and machine learning (ML) to create inorganic synthesis maps in zeolites. Starting with 253 known zeolites, we show how the continuous distances between frameworks reproduce inorganic synthesis conditions from the literature without using labels such as building units. An unsupervised learning analysis shows that neighboring zeolites according to our metric often share similar inorganic synthesis conditions, even in template-based routes. In combination with ML classifiers, we find synthesis-structure relationships for 14 common inorganic conditions in zeolites, namely Al, B, Be, Ca, Co, F, Ga, Ge, K, Mg, Na, P, Si, and Zn. By explaining the model predictions, we demonstrate how (dis)similarities towards known structures can be used as features for the synthesis space. Finally, we show how these methods can be used to predict inorganic synthesis conditions for unrealized frameworks in hypothetical databases and interpret the outcomes by extracting local structural patterns from zeolites. In combination with template design, this work can accelerate the exploration of the space of synthesis conditions for zeolites.
</details>
<details>
<summary>摘要</summary>
Zeolites 是一类无机材料，known for their 多样性 of applications, synthesis conditions, and resulting polymorphs. Although their synthesis is controlled by both inorganic and organic synthesis conditions, computational studies of zeolite synthesis have focused mostly on organic template design. In this work, we use a strong distance metric between crystal structures and machine learning (ML) to create inorganic synthesis maps in zeolites. Starting with 253 known zeolites, we show how the continuous distances between frameworks reproduce inorganic synthesis conditions from the literature without using labels such as building units. An unsupervised learning analysis shows that neighboring zeolites according to our metric often share similar inorganic synthesis conditions, even in template-based routes. In combination with ML classifiers, we find synthesis-structure relationships for 14 common inorganic conditions in zeolites, namely Al, B, Be, Ca, Co, F, Ga, Ge, K, Mg, Na, P, Si, and Zn. By explaining the model predictions, we demonstrate how (dis)similarities towards known structures can be used as features for the synthesis space. Finally, we show how these methods can be used to predict inorganic synthesis conditions for unrealized frameworks in hypothetical databases and interpret the outcomes by extracting local structural patterns from zeolites. In combination with template design, this work can accelerate the exploration of the space of synthesis conditions for zeolites.
</details></li>
</ul>
<hr>
<h2 id="Modeling-3D-cardiac-contraction-and-relaxation-with-point-cloud-deformation-networks"><a href="#Modeling-3D-cardiac-contraction-and-relaxation-with-point-cloud-deformation-networks" class="headerlink" title="Modeling 3D cardiac contraction and relaxation with point cloud deformation networks"></a>Modeling 3D cardiac contraction and relaxation with point cloud deformation networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10927">http://arxiv.org/abs/2307.10927</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcel Beetz, Abhirup Banerjee, Vicente Grau</li>
<li>for: 这项研究旨在开发一种基于点云深度学习的新方法，以模拟心脏的3D弹性变形过程。</li>
<li>methods: 该方法使用点云深度学习的最新进展，建立了一个encoder-decoder结构，以便在多级别的特征学习中，能够直接使用多类3D点云表示心脏 анатомии。</li>
<li>results: 研究人员在UK Biobank数据集上进行了大规模的测试，并发现了在 Chamfer 距离下面的平均距离，并且与实际数据集中的临床指标相似。此外，该方法还能够成功地捕捉不同种群之间的差异，并在预后报告和心脏病发生预测等任务中，以13%和7%的优势超越多个临床标准。<details>
<summary>Abstract</summary>
Global single-valued biomarkers of cardiac function typically used in clinical practice, such as ejection fraction, provide limited insight on the true 3D cardiac deformation process and hence, limit the understanding of both healthy and pathological cardiac mechanics. In this work, we propose the Point Cloud Deformation Network (PCD-Net) as a novel geometric deep learning approach to model 3D cardiac contraction and relaxation between the extreme ends of the cardiac cycle. It employs the recent advances in point cloud-based deep learning into an encoder-decoder structure, in order to enable efficient multi-scale feature learning directly on multi-class 3D point cloud representations of the cardiac anatomy. We evaluate our approach on a large dataset of over 10,000 cases from the UK Biobank study and find average Chamfer distances between the predicted and ground truth anatomies below the pixel resolution of the underlying image acquisition. Furthermore, we observe similar clinical metrics between predicted and ground truth populations and show that the PCD-Net can successfully capture subpopulation-specific differences between normal subjects and myocardial infarction (MI) patients. We then demonstrate that the learned 3D deformation patterns outperform multiple clinical benchmarks by 13% and 7% in terms of area under the receiver operating characteristic curve for the tasks of prevalent MI detection and incident MI prediction and by 7% in terms of Harrell's concordance index for MI survival analysis.
</details>
<details>
<summary>摘要</summary>
全球唯一价值生物标志通常用于临床实践中，如血液泵动率（ejection fraction），只提供有限的真实3D冠状膜弹性过程的信息，因此限制了正常和疾病冠状膜机械学的理解。在这项工作中，我们提出了点云弹性网络（PCD-Net）作为一种新的几何深度学术方法，用于模型3D冠状膜收缩和放松阶段的几何变化。它利用了最新的点云基于深度学的进步，将点云表示的冠状膜解剖结构转化为多级几何特征学习的encoder-decoder结构。我们对大量的UK Biobank数据集（超过10,000个案例）进行评估，并发现在Chamfer距离下限以像素分辨率的图像获取的下方。此外，我们发现了预测和真实人口的临床指标相似，并证明PCD-Net可以成功捕捉不同种类的冠状膜疾病的差异。最后，我们发现PCD-Net已经在MI检测和预测任务中提高了13%和7%，并在MI生存分析中提高了7%。
</details></li>
</ul>
<hr>
<h2 id="Confidence-intervals-for-performance-estimates-in-3D-medical-image-segmentation"><a href="#Confidence-intervals-for-performance-estimates-in-3D-medical-image-segmentation" class="headerlink" title="Confidence intervals for performance estimates in 3D medical image segmentation"></a>Confidence intervals for performance estimates in 3D medical image segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10926">http://arxiv.org/abs/2307.10926</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rosanajurdi/SegVal_TMI">https://github.com/rosanajurdi/SegVal_TMI</a></li>
<li>paper_authors: R. El Jurdi, G. Varoquaux, O. Colliot</li>
<li>for: 这个论文主要是用来研究医学图像分割模型的评估方法。</li>
<li>methods: 这个论文使用了nnU-net框架和医学图像分割挑战赛中的两个数据集，以及两种性能指标： dice准确率和 Hausdorff 距离。</li>
<li>results: 这个论文发现，在不同的测试集大小和性能指标的散度下，参数型信息Interval是正确的近似值，并且显示了测试样本的数量需要更低的水平来达到给定的精度水平。通常需要100-200个测试样本，而更Difficult的分割任务可能需要更多的样本。<details>
<summary>Abstract</summary>
Medical segmentation models are evaluated empirically. As such an evaluation is based on a limited set of example images, it is unavoidably noisy. Beyond a mean performance measure, reporting confidence intervals is thus crucial. However, this is rarely done in medical image segmentation. The width of the confidence interval depends on the test set size and on the spread of the performance measure (its standard-deviation across of the test set). For classification, many test images are needed to avoid wide confidence intervals. Segmentation, however, has not been studied, and it differs by the amount of information brought by a given test image. In this paper, we study the typical confidence intervals in medical image segmentation. We carry experiments on 3D image segmentation using the standard nnU-net framework, two datasets from the Medical Decathlon challenge and two performance measures: the Dice accuracy and the Hausdorff distance. We show that the parametric confidence intervals are reasonable approximations of the bootstrap estimates for varying test set sizes and spread of the performance metric. Importantly, we show that the test size needed to achieve a given precision is often much lower than for classification tasks. Typically, a 1% wide confidence interval requires about 100-200 test samples when the spread is low (standard-deviation around 3%). More difficult segmentation tasks may lead to higher spreads and require over 1000 samples.
</details>
<details>
<summary>摘要</summary>
医学分割模型通常由实际评估，这种评估基于有限数量的示例图像，因此不可避免噪音。在医学图像分割中，报告信息Interval是非常重要，但是这并不常done。 confidence interval的宽度取决于测试集大小和性能指标（其标准差在测试集中）。为分类问题，需要许多测试图像来避免宽度的信息Interval。然而，在医学图像分割中，每个测试图像都带来了不同的信息量。在这篇文章中，我们研究了医学图像分割中的常见信息Interval。我们在使用标准nnU-net框架、医学十大挑战赛中的两个数据集和两个性能指标（ dice准确率和 Hausdorff 距离）进行实验。我们发现，参数信息Interval是优等的近似值，并且与Bootstrap估计相关。进一步地，我们发现，为达到给定的精度，测试样本的数量通常比分类任务要少得多。通常需要100-200个测试样本，当标准差低（约3%）时。在更Difficult分割任务时，可能会出现更高的标准差，需要更多的测试样本（超过1000个）。
</details></li>
</ul>
<hr>
<h2 id="Sequential-Multi-Dimensional-Self-Supervised-Learning-for-Clinical-Time-Series"><a href="#Sequential-Multi-Dimensional-Self-Supervised-Learning-for-Clinical-Time-Series" class="headerlink" title="Sequential Multi-Dimensional Self-Supervised Learning for Clinical Time Series"></a>Sequential Multi-Dimensional Self-Supervised Learning for Clinical Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10923">http://arxiv.org/abs/2307.10923</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/exploita123/charmedforfree">https://github.com/exploita123/charmedforfree</a></li>
<li>paper_authors: Aniruddh Raghu, Payal Chandak, Ridwan Alam, John Guttag, Collin M. Stultz</li>
<li>For: This paper is focused on addressing the limitations of existing self-supervised learning (SSL) methods for clinical time series data, which are typically designed for unimodal time series. The authors propose a new SSL method called Sequential Multi-Dimensional SSL that can capture information at both the sequence level and the individual data point level.* Methods: The proposed method uses a SSL loss function that is applied at both the sequence level and the individual data point level. The specific form of the loss function can be either contrastive or non-contrastive. The method is agnostic to the choice of loss function.* Results: The authors evaluate their method on two real-world clinical datasets, one containing high-frequency electrocardiograms and the other containing structured data from lab values and vitals signs. The results show that pre-training with their method and then fine-tuning on downstream tasks improves performance over baselines on both datasets, and can lead to improvements across different self-supervised loss functions.Here is the information in Simplified Chinese text:* For: 本研究针对现有的医学时序数据自动学习（SSL）方法存在的限制，这些方法通常是针对单modal时序序的设计。作者提出了一种新的SSL方法，即Sequential Multi-Dimensional SSL，可以更好地捕捉时序序的信息。* Methods: 提议的方法使用SSL损失函数，其应用于时序序的两级划分。特定的损失函数可以是对比或非对比的。方法是对损失函数的不同选择无关的。* Results: 作者在两个实际的医学时序数据集上评估了他们的方法。结果显示，预训练后的方法在两个数据集上的表现都超过基线，并在不同的自我超vised损失函数下进行细化后的表现也有改善。<details>
<summary>Abstract</summary>
Self-supervised learning (SSL) for clinical time series data has received significant attention in recent literature, since these data are highly rich and provide important information about a patient's physiological state. However, most existing SSL methods for clinical time series are limited in that they are designed for unimodal time series, such as a sequence of structured features (e.g., lab values and vitals signs) or an individual high-dimensional physiological signal (e.g., an electrocardiogram). These existing methods cannot be readily extended to model time series that exhibit multimodality, with structured features and high-dimensional data being recorded at each timestep in the sequence. In this work, we address this gap and propose a new SSL method -- Sequential Multi-Dimensional SSL -- where a SSL loss is applied both at the level of the entire sequence and at the level of the individual high-dimensional data points in the sequence in order to better capture information at both scales. Our strategy is agnostic to the specific form of loss function used at each level -- it can be contrastive, as in SimCLR, or non-contrastive, as in VICReg. We evaluate our method on two real-world clinical datasets, where the time series contains sequences of (1) high-frequency electrocardiograms and (2) structured data from lab values and vitals signs. Our experimental results indicate that pre-training with our method and then fine-tuning on downstream tasks improves performance over baselines on both datasets, and in several settings, can lead to improvements across different self-supervised loss functions.
</details>
<details>
<summary>摘要</summary>
医学时间序列数据自主学习（SSL）在最近的文献中受到了广泛的关注，因为这些数据具有高度的资料价值，可以提供病人生理状态的重要信息。然而，现有的大多数SSL方法仅适用于单模时间序列数据，例如序列中的结构化特征（例如医学实验室值和生物指标）或一个高维度的生理信号（例如电cardiogram）。这些现有的方法难以扩展到模式多样性时间序列数据，其中每个时间步骤都有结构化特征和高维度数据被记录。在这项工作中，我们Addressing this gap, we propose a new SSL method called Sequential Multi-Dimensional SSL, which applies a SSL loss both at the level of the entire sequence and at the level of the individual high-dimensional data points in the sequence to better capture information at both scales. Our strategy is agnostic to the specific form of loss function used at each level - it can be contrastive, as in SimCLR, or non-contrastive, as in VICReg. We evaluate our method on two real-world clinical datasets, where the time series contains sequences of (1) high-frequency electrocardiograms and (2) structured data from lab values and vitals signs. Our experimental results indicate that pre-training with our method and then fine-tuning on downstream tasks improves performance over baselines on both datasets, and in several settings, can lead to improvements across different self-supervised loss functions.
</details></li>
</ul>
<hr>
<h2 id="Language-based-Action-Concept-Spaces-Improve-Video-Self-Supervised-Learning"><a href="#Language-based-Action-Concept-Spaces-Improve-Video-Self-Supervised-Learning" class="headerlink" title="Language-based Action Concept Spaces Improve Video Self-Supervised Learning"></a>Language-based Action Concept Spaces Improve Video Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10922">http://arxiv.org/abs/2307.10922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kanchana Ranasinghe, Michael Ryoo</li>
<li>for: 学习高效转移和Robust图像表示</li>
<li>methods: 使用语言拼接自我超vised学习对图像CLIP模型进行适应</li>
<li>results: 提高了零shot和线性探测性能在三个动作识别benchmark上<details>
<summary>Abstract</summary>
Recent contrastive language image pre-training has led to learning highly transferable and robust image representations. However, adapting these models to video domains with minimal supervision remains an open problem. We explore a simple step in that direction, using language tied self-supervised learning to adapt an image CLIP model to the video domain. A backbone modified for temporal modeling is trained under self-distillation settings with train objectives operating in an action concept space. Feature vectors of various action concepts extracted from a language encoder using relevant textual prompts construct this space. We introduce two train objectives, concept distillation and concept alignment, that retain generality of original representations while enforcing relations between actions and their attributes. Our approach improves zero-shot and linear probing performance on three action recognition benchmarks.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:最近的语言图像预训练技术已经导致了图像表示的高度传输和稳定性。然而，将这些模型应用到视频领域仍然是一个未解决的问题。我们explore一个简单的方法，使用语言绑定的自我超视觉学习来适应图像CLIP模型到视频领域。一个修改了时间模型的背部是在自我热钻设置下训练，使用train目标在动作概念空间中运行。我们引入了两个train目标，概念热钻和概念对齐，以保持原始表示的通用性，同时强制行为和其属性之间的关系。我们的方法提高了零shot和线性探测性能在三个动作识别基准上。
</details></li>
</ul>
<hr>
<h2 id="The-Role-of-Entropy-and-Reconstruction-in-Multi-View-Self-Supervised-Learning"><a href="#The-Role-of-Entropy-and-Reconstruction-in-Multi-View-Self-Supervised-Learning" class="headerlink" title="The Role of Entropy and Reconstruction in Multi-View Self-Supervised Learning"></a>The Role of Entropy and Reconstruction in Multi-View Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10907">http://arxiv.org/abs/2307.10907</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/apple/ml-entropy-reconstruction">https://github.com/apple/ml-entropy-reconstruction</a></li>
<li>paper_authors: Borja Rodríguez-Gálvez, Arno Blaas, Pau Rodríguez, Adam Goliński, Xavier Suau, Jason Ramapuram, Dan Busbridge, Luca Zappella</li>
<li>for: This paper aims to understand the mechanisms behind the success of multi-view self-supervised learning (MVSSL) methods, specifically contrastive MVSSL methods, through the lens of a new lower bound on the Mutual Information (MI) called the Entropy-Reconstruction (ER) bound.</li>
<li>methods: The paper analyzes the main MVSSL families through the ER bound, showing that clustering-based methods such as DeepCluster and SwAV maximize the MI, while distillation-based approaches such as BYOL and DINO explicitly maximize the reconstruction term and implicitly encourage a stable entropy.</li>
<li>results: The paper shows that replacing the objectives of common MVSSL methods with the ER bound achieves competitive performance while making them stable when training with smaller batch sizes or smaller exponential moving average (EMA) coefficients.Here is the same information in Simplified Chinese:</li>
<li>for: 这篇论文目标是理解多视图自然学习（MVSSL）方法的成功机制，具体是对比自然学习（MVSSL）方法的InfoNCE下界的研究。</li>
<li>methods: 论文通过Entropy-Reconstruction（ER）下界分析主要MVSSL家族，发现嵌入式 clustering 方法如 DeepCluster 和 SwAV 最大化 Mutual Information（MI），而搅拌方法如 BYOL 和 DINO 则直接最大化重建项并隐式促进稳定 entropy。</li>
<li>results: 论文表明，将常见 MVSSL 方法的目标替换为 ER 下界可以实现竞争性性能，同时在训练时使用更小的批处理大小或更小的指数移动平均值（EMA）値可以使模型更稳定。<details>
<summary>Abstract</summary>
The mechanisms behind the success of multi-view self-supervised learning (MVSSL) are not yet fully understood. Contrastive MVSSL methods have been studied through the lens of InfoNCE, a lower bound of the Mutual Information (MI). However, the relation between other MVSSL methods and MI remains unclear. We consider a different lower bound on the MI consisting of an entropy and a reconstruction term (ER), and analyze the main MVSSL families through its lens. Through this ER bound, we show that clustering-based methods such as DeepCluster and SwAV maximize the MI. We also re-interpret the mechanisms of distillation-based approaches such as BYOL and DINO, showing that they explicitly maximize the reconstruction term and implicitly encourage a stable entropy, and we confirm this empirically. We show that replacing the objectives of common MVSSL methods with this ER bound achieves competitive performance, while making them stable when training with smaller batch sizes or smaller exponential moving average (EMA) coefficients.   Github repo: https://github.com/apple/ml-entropy-reconstruction.
</details>
<details>
<summary>摘要</summary>
文中提出的多视图自助学习（MVSSL）的机制尚未完全理解。对于对照式MVSSL方法，我们通过信息准确（InfoNCE）来研究其关系。然而，其他MVSSL方法和MI之间的关系仍然不清楚。我们考虑了一种基于信息 entropy和重建（ER）的下界，并通过这个下界来分析主要的MVSSL家族。我们显示了使用 DeepCluster 和 SwAV 方法时，都会最大化MI。我们还重新解释了 BYOL 和 DINO 方法的机制，并证明它们直接最大化重建项，并间接鼓励稳定的 entropy，这一点我们在实验中证实了。我们还显示了将常见MVSSL方法的目标替换为 ER 下界可以实现竞争性表现，同时使其在训练时使用 smaller batch size 或 smaller EMA 系数时变得稳定。Github 仓库：https://github.com/apple/ml-entropy-reconstruction.
</details></li>
</ul>
<hr>
<h2 id="Variational-Point-Encoding-Deformation-for-Dental-Modeling"><a href="#Variational-Point-Encoding-Deformation-for-Dental-Modeling" class="headerlink" title="Variational Point Encoding Deformation for Dental Modeling"></a>Variational Point Encoding Deformation for Dental Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10895">http://arxiv.org/abs/2307.10895</a></li>
<li>repo_url: None</li>
<li>paper_authors: Johan Ziruo Ye, Thomas Ørkild, Peter Lempel Søndergaard, Søren Hauberg</li>
<li>for:  This paper aims to address the challenges in digital dentistry by releasing a new dataset of tooth meshes and proposing a new method called Variational FoldingNet (VF-Net) for point cloud representation.</li>
<li>methods:  VF-Net extends FoldingNet to enable probabilistic learning of point cloud representations, addressing the challenge of 1-to-1 mapping between input and output points.</li>
<li>results:  The paper demonstrates the superior performance of VF-Net over existing models in terms of dental scan reconstruction and extrapolation, and highlights the robustness of VF-Net’s latent representations.<details>
<summary>Abstract</summary>
Digital dentistry has made significant advancements in recent years, yet numerous challenges remain to be addressed. In this study, we release a new extensive dataset of tooth meshes to encourage further research. Additionally, we propose Variational FoldingNet (VF-Net), which extends FoldingNet to enable probabilistic learning of point cloud representations. A key challenge in existing latent variable models for point clouds is the lack of a 1-to-1 mapping between input points and output points. Instead, they must rely on optimizing Chamfer distances, a metric that does not have a normalized distributional counterpart, preventing its usage in probabilistic models. We demonstrate that explicit minimization of Chamfer distances can be replaced by a suitable encoder, which allows us to increase computational efficiency while simplifying the probabilistic extension. Our experimental findings present empirical evidence demonstrating the superior performance of VF-Net over existing models in terms of dental scan reconstruction and extrapolation. Additionally, our investigation highlights the robustness of VF-Net's latent representations. These results underscore the promising prospects of VF-Net as an effective and reliable method for point cloud reconstruction and analysis.
</details>
<details>
<summary>摘要</summary>
“数字牙科”在最近几年内取得了重要进步，但还有许多挑战需要解决。在这项研究中，我们发布了一个新的广泛的牙齿磁共振数据集，以便进一步推动研究。此外，我们提出了变分卷积网络（VF-Net），它将卷积网络扩展到可以进行概率学学习磁点云表示。现有的磁点云latent variable模型的一个主要挑战是输入点云和输出点云之间没有1-to-1的映射，它们必须通过优化Chamfer距离来进行优化。我们示出了可以直接使用适当的编码器来取代Explicit Chamfer距离的优化，从而提高计算效率并简化概率扩展。我们的实验结果表明VF-Net在牙齿扫描重建和推导方面的性能明显高于现有模型。此外，我们的调查也表明VF-Net的秘密表示的稳定性。这些结果证明了VF-Net作为牙齿扫描和分析的有效和可靠方法。
</details></li>
</ul>
<hr>
<h2 id="Learning-and-Generalizing-Polynomials-in-Simulation-Metamodeling"><a href="#Learning-and-Generalizing-Polynomials-in-Simulation-Metamodeling" class="headerlink" title="Learning and Generalizing Polynomials in Simulation Metamodeling"></a>Learning and Generalizing Polynomials in Simulation Metamodeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10892">http://arxiv.org/abs/2307.10892</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jesperhauch/polynomial_deep_learning">https://github.com/jesperhauch/polynomial_deep_learning</a></li>
<li>paper_authors: Jesper Hauch, Christoffer Riis, Francisco C. Pereira</li>
<li>for: 本文是为了提出 multiplicative neural network（MNN）建模方法，用于模拟高阶多项式函数。</li>
<li>methods: 本文提出了一种基于 MNN 的模拟元模型方法，该方法可以在模拟时间步更新中使用 MNN 来近似高阶多项式函数。</li>
<li>results: 实验表明，相比基线模型，MNN 在适应和扩展性方面表现更好，其在验证集中的性能与对于数据集外的测试集中的性能匹配。此外，本文还提出了一种模拟时间步更新的方法，并在一个流行病模型中进行了示例。<details>
<summary>Abstract</summary>
The ability to learn polynomials and generalize out-of-distribution is essential for simulation metamodels in many disciplines of engineering, where the time step updates are described by polynomials. While feed forward neural networks can fit any function, they cannot generalize out-of-distribution for higher-order polynomials. Therefore, this paper collects and proposes multiplicative neural network (MNN) architectures that are used as recursive building blocks for approximating higher-order polynomials. Our experiments show that MNNs are better than baseline models at generalizing, and their performance in validation is true to their performance in out-of-distribution tests. In addition to MNN architectures, a simulation metamodeling approach is proposed for simulations with polynomial time step updates. For these simulations, simulating a time interval can be performed in fewer steps by increasing the step size, which entails approximating higher-order polynomials. While our approach is compatible with any simulation with polynomial time step updates, a demonstration is shown for an epidemiology simulation model, which also shows the inductive bias in MNNs for learning and generalizing higher-order polynomials.
</details>
<details>
<summary>摘要</summary>
“ polynomial 和泛化 OUT-OF- Distribution 的能力是 simulation 模型中许多领域的工程学中的关键，其时间步骤更新由 polynomials 描述。虽然前向神经网络可以适应任何函数，但它们无法泛化 OUT-OF- Distribution  для高阶 polynomials。因此，这篇文章收集了和提议了乘法神经网络（MNN）架构，用于 Recursive 构建准确高阶 polynomials 的表示。我们的实验表明，MNNs 比基eline 模型更好地泛化，并且其在验证中的性能与 OUT-OF- Distribution 测试中的性能相符。此外，我们还提出了一种 simulation 模型的 meta-modeling 方法，用于 simulations 中的 polynomial 时间步骤更新。这些 simulations 中可以通过增大步长来缩短时间间隔，这意味着需要拟合高阶 polynomials。而我们的方法与任何 polynomial 时间步骤更新的 simulation 相容，我们在 epidemiology 模型中进行了示例，并表明了 MNNs 对于学习和泛化高阶 polynomials 的适应性。”Note: Simplified Chinese is used here, as it is more widely used in mainland China and is the standard for most online content. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Syntactic-vs-Semantic-Linear-Abstraction-and-Refinement-of-Neural-Networks"><a href="#Syntactic-vs-Semantic-Linear-Abstraction-and-Refinement-of-Neural-Networks" class="headerlink" title="Syntactic vs Semantic Linear Abstraction and Refinement of Neural Networks"></a>Syntactic vs Semantic Linear Abstraction and Refinement of Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10891">http://arxiv.org/abs/2307.10891</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cxlvinchau/linna">https://github.com/cxlvinchau/linna</a></li>
<li>paper_authors: Calvin Chau, Jan Křetínský, Stefanie Mohr</li>
<li>for: 提高神经网络的可扩展性</li>
<li>methods: 使用可变的线性组合来替换神经元，以实现更高的减少率</li>
<li>results: 实验表明，这种方法可以实现更好的减少率，并且可以在 sintactic 和 semantic 抽象中实现Here’s a breakdown of each point:</li>
<li>for: The paper aims to improve the scalability of neural networks by using abstraction techniques.</li>
<li>methods: The authors propose a more flexible framework that allows a neuron to be replaced with a linear combination of other neurons, leading to better reduction. They apply this approach both on syntactic and semantic abstractions and evaluate them experimentally.</li>
<li>results: The experimental results show that the proposed method can achieve better reduction compared to previous approaches, and can find a better balance between reduction and precision through a refinement method.<details>
<summary>Abstract</summary>
Abstraction is a key verification technique to improve scalability. However, its use for neural networks is so far extremely limited. Previous approaches for abstracting classification networks replace several neurons with one of them that is similar enough. We can classify the similarity as defined either syntactically (using quantities on the connections between neurons) or semantically (on the activation values of neurons for various inputs). Unfortunately, the previous approaches only achieve moderate reductions, when implemented at all. In this work, we provide a more flexible framework where a neuron can be replaced with a linear combination of other neurons, improving the reduction. We apply this approach both on syntactic and semantic abstractions, and implement and evaluate them experimentally. Further, we introduce a refinement method for our abstractions, allowing for finding a better balance between reduction and precision.
</details>
<details>
<summary>摘要</summary>
abstraction 是一种关键的验证技术，可以提高神经网络的扩展性。然而，它在神经网络上的使用尚未得到广泛应用。前一种方法是将多个神经元换为一个相似的神经元。我们可以分类定义相似性，使用语法（基于连接 между神经元的量）或 semantics（基于神经元对各个输入的活动值）。不幸的是，这些前一种方法只能实现moderate的减少，甚至无法实现。在这项工作中，我们提供了更flexible的框架，允许一个神经元被替换为一个线性组合的其他神经元，从而提高减少。我们在语法和semantics abstractions上应用这种方法，并进行了实验验证。此外，我们还介绍了一种精化方法，可以为我们的抽象提供更好的平衡 между减少和精度。
</details></li>
</ul>
<hr>
<h2 id="Player-optimal-Stable-Regret-for-Bandit-Learning-in-Matching-Markets"><a href="#Player-optimal-Stable-Regret-for-Bandit-Learning-in-Matching-Markets" class="headerlink" title="Player-optimal Stable Regret for Bandit Learning in Matching Markets"></a>Player-optimal Stable Regret for Bandit Learning in Matching Markets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10890">http://arxiv.org/abs/2307.10890</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fang Kong, Shuai Li</li>
<li>for: 这种问题的文章主要目标是解决匹配市场中稳定匹配的问题，以实现最大化参与者利益。</li>
<li>methods: 这篇文章提出了一种新的算法名为 explore-then-Gale-Shapley（ETGS），并证明了该算法可以实现参与者最优稳定 regret。</li>
<li>results: 文章显示了该算法可以实现参与者最优稳定 regret的Upper bound为O($K\log T&#x2F;\Delta^2$)，其中$K$是武器数量，$T$是时间戳，$\Delta$是参与者最小偏好差值。这个结果超越了之前的工作，其中 either有软化参与者稳定匹配目标或者只适用于特殊情况下的市场。当参与者偏好满足某些特殊条件时，我们的 regret upper bound也与之前 derive的下界匹配。<details>
<summary>Abstract</summary>
The problem of matching markets has been studied for a long time in the literature due to its wide range of applications. Finding a stable matching is a common equilibrium objective in this problem. Since market participants are usually uncertain of their preferences, a rich line of recent works study the online setting where one-side participants (players) learn their unknown preferences from iterative interactions with the other side (arms). Most previous works in this line are only able to derive theoretical guarantees for player-pessimal stable regret, which is defined compared with the players' least-preferred stable matching. However, under the pessimal stable matching, players only obtain the least reward among all stable matchings. To maximize players' profits, player-optimal stable matching would be the most desirable. Though \citet{basu21beyond} successfully bring an upper bound for player-optimal stable regret, their result can be exponentially large if players' preference gap is small. Whether a polynomial guarantee for this regret exists is a significant but still open problem. In this work, we provide a new algorithm named explore-then-Gale-Shapley (ETGS) and show that the optimal stable regret of each player can be upper bounded by $O(K\log T/\Delta^2)$ where $K$ is the number of arms, $T$ is the horizon and $\Delta$ is the players' minimum preference gap among the first $N+1$-ranked arms. This result significantly improves previous works which either have a weaker player-pessimal stable matching objective or apply only to markets with special assumptions. When the preferences of participants satisfy some special conditions, our regret upper bound also matches the previously derived lower bound.
</details>
<details>
<summary>摘要</summary>
problém matching markets 已经在文献中研究了很长时间，因为它在各种应用场景中具有广泛的应用前景。找到稳定匹配是这个问题的通用平衡目标。由于市场参与者通常不确定他们的偏好，一条丰富的最近的工作研究在线设置，在其中一侧参与者（玩家）通过多次交互来学习他们未知的偏好。大多数前一代工作只能 deriv theoretical guarantees for player-pessimal stable regret，这是defined compared with the players' least-preferred stable matching。然而，在稳定匹配中，玩家只能获得最低奖励中的一个稳定匹配。为了提高玩家的收益，player-optimal stable matching 是最愿望的。虽然 \citet{basu21beyond} 成功地提出了 player-optimal stable regret 的Upper bound，但其结果可能是 exponentially large 的。whether a polynomial guarantee for this regret exists 是一个重要但仍然打开的问题。在这个工作中，我们提出了一种新的算法 named explore-then-Gale-Shapley (ETGS) ，并证明了每个玩家的最佳稳定 regret 可以 upper bounded by $O(K\log T/\Delta^2)$，where $K$ 是枪手数， $T$ 是时间框架， $\Delta$ 是玩家最小的偏好差值中的第 $N+1$-名枪手。这个结果与前一代工作不同，它们的目标是 player-pessimal stable matching 或者只适用于特殊的市场假设。当偏好满足特定的条件时，我们的 regret upper bound 也与之前 derive的 lower bound 匹配。
</details></li>
</ul>
<hr>
<h2 id="What-Twitter-Data-Tell-Us-about-the-Future"><a href="#What-Twitter-Data-Tell-Us-about-the-Future" class="headerlink" title="What Twitter Data Tell Us about the Future?"></a>What Twitter Data Tell Us about the Future?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02035">http://arxiv.org/abs/2308.02035</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alina Landowska, Marek Robak, Maciej Skorski</li>
<li>for: 这个研究旨在调查Twitter上的未来推测，并研究语言指示器对 anticipatory thinking 的影响。</li>
<li>methods: 该研究使用 SOTA 模型建立可扩展的 NLP 管道，并通过 LDA 和 BERTopic 方法从未来推测者的微博中提取主题。</li>
<li>results: 研究发现未来推测者的微博中存在 15 个主题，并且使用 BERTopic 方法可以分化出 100 个不同的主题。这些发现对话题模型研究做出贡献，同时也提供了关于未来推测的社交媒体用户的反应和响应的新视角。<details>
<summary>Abstract</summary>
Anticipation is a fundamental human cognitive ability that involves thinking about and living towards the future. While language markers reflect anticipatory thinking, research on anticipation from the perspective of natural language processing is limited. This study aims to investigate the futures projected by futurists on Twitter and explore the impact of language cues on anticipatory thinking among social media users. We address the research questions of what futures Twitter's futurists anticipate and share, and how these anticipated futures can be modeled from social data. To investigate this, we review related works on anticipation, discuss the influence of language markers and prestigious individuals on anticipatory thinking, and present a taxonomy system categorizing futures into "present futures" and "future present". This research presents a compiled dataset of over 1 million publicly shared tweets by future influencers and develops a scalable NLP pipeline using SOTA models. The study identifies 15 topics from the LDA approach and 100 distinct topics from the BERTopic approach within the futurists' tweets. These findings contribute to the research on topic modelling and provide insights into the futures anticipated by Twitter's futurists. The research demonstrates the futurists' language cues signals futures-in-the-making that enhance social media users to anticipate their own scenarios and respond to them in present. The fully open-sourced dataset, interactive analysis, and reproducible source code are available for further exploration.
</details>
<details>
<summary>摘要</summary>
人类的预期是一种基本的认知能力，涉及到思考和生活未来。 although research on anticipation from the perspective of natural language processing is limited, this study aims to investigate the futures projected by futurists on Twitter and explore the impact of language cues on anticipatory thinking among social media users. We address the research questions of what futures Twitter's futurists anticipate and share, and how these anticipated futures can be modeled from social data. To investigate this, we review related works on anticipation, discuss the influence of language markers and prestigious individuals on anticipatory thinking, and present a taxonomy system categorizing futures into "present futures" and "future present". This research presents a compiled dataset of over 1 million publicly shared tweets by future influencers and develops a scalable NLP pipeline using state-of-the-art models. The study identifies 15 topics from the LDA approach and 100 distinct topics from the BERTopic approach within the futurists' tweets. These findings contribute to the research on topic modeling and provide insights into the futures anticipated by Twitter's futurists. The research demonstrates the futurists' language cues signal futures-in-the-making that enhance social media users to anticipate their own scenarios and respond to them in the present. The fully open-sourced dataset, interactive analysis, and reproducible source code are available for further exploration.
</details></li>
</ul>
<hr>
<h2 id="Risk-optimized-Outlier-Removal-for-Robust-Point-Cloud-Classification"><a href="#Risk-optimized-Outlier-Removal-for-Robust-Point-Cloud-Classification" class="headerlink" title="Risk-optimized Outlier Removal for Robust Point Cloud Classification"></a>Risk-optimized Outlier Removal for Robust Point Cloud Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10875">http://arxiv.org/abs/2307.10875</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinke Li, Junchi Lu</li>
<li>for: 这篇论文旨在提高安全敏感频谱深度模型的可靠性和安全性，应对意外或自然出现的点云变化。</li>
<li>methods: 方法是提出一种点云外liers removing方法，named PointCVaR，可以让标准训练的模型删除额外的外liers并恢复数据。</li>
<li>results: 实验结果显示，PointCVaR可以优化点云标签的精度，并在不同的点云变化中实现了优秀的移除和标签结果。特别是，它可以对抗后门攻击，删除触发器87%的精度。总的来说，PointCVaR是一个可靠的插件模组，可以帮助不同的模型在不同的情况下提高点云标签的精度。<details>
<summary>Abstract</summary>
The popularity of point cloud deep models for safety-critical purposes has increased, but the reliability and security of these models can be compromised by intentional or naturally occurring point cloud noise. To combat this issue, we present a novel point cloud outlier removal method called PointCVaR, which empowers standard-trained models to eliminate additional outliers and restore the data. Our approach begins by conducting attribution analysis to determine the influence of each point on the model output, which we refer to as point risk. We then optimize the process of filtering high-risk points using Conditional Value at Risk (CVaR) as the objective. The rationale for this approach is based on the observation that noise points in point clouds tend to cluster in the tail of the risk distribution, with a low frequency but a high level of risk, resulting in significant interference with classification results. Despite requiring no additional training effort, our method produces exceptional results in various removal-and-classification experiments for noisy point clouds, which are corrupted by random noise, adversarial noise, and backdoor trigger noise. Impressively, it achieves 87% accuracy in defense against the backdoor attack by removing triggers. Overall, the proposed PointCVaR effectively eliminates noise points and enhances point cloud classification, making it a promising plug-in module for various models in different scenarios.
</details>
<details>
<summary>摘要</summary>
popularity of point cloud deep models for safety-critical purposes has increased, but the reliability and security of these models can be compromised by intentional or naturally occurring point cloud noise. To combat this issue, we present a novel point cloud outlier removal method called PointCVaR, which empowers standard-trained models to eliminate additional outliers and restore the data. Our approach begins by conducting attribution analysis to determine the influence of each point on the model output, which we refer to as point risk. We then optimize the process of filtering high-risk points using Conditional Value at Risk (CVaR) as the objective. The rationale for this approach is based on the observation that noise points in point clouds tend to cluster in the tail of the risk distribution, with a low frequency but a high level of risk, resulting in significant interference with classification results. Despite requiring no additional training effort, our method produces exceptional results in various removal-and-classification experiments for noisy point clouds, which are corrupted by random noise, adversarial noise, and backdoor trigger noise. Impressively, it achieves 87% accuracy in defense against the backdoor attack by removing triggers. Overall, the proposed PointCVaR effectively eliminates noise points and enhances point cloud classification, making it a promising plug-in module for various models in different scenarios.
</details></li>
</ul>
<hr>
<h2 id="Nonlinear-Meta-Learning-Can-Guarantee-Faster-Rates"><a href="#Nonlinear-Meta-Learning-Can-Guarantee-Faster-Rates" class="headerlink" title="Nonlinear Meta-Learning Can Guarantee Faster Rates"></a>Nonlinear Meta-Learning Can Guarantee Faster Rates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10870">http://arxiv.org/abs/2307.10870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitri Meunier, Zhu Li, Arthur Gretton, Samory Kpotufe</li>
<li>for: 本文目标是解决meta-学习中关于如何利用相关任务的表示结构，以提高目标任务的学习效率和精度。</li>
<li>methods: 本文使用了非线性表示，并通过精心设计的正则项来减少任务特定的偏见。</li>
<li>results: 本文提供了关于meta-学习中非线性表示下的理论保证，并证明在适当的正则项支持下，可以减少任务特定的偏见，提高目标任务的学习效率和精度。<details>
<summary>Abstract</summary>
Many recent theoretical works on \emph{meta-learning} aim to achieve guarantees in leveraging similar representational structures from related tasks towards simplifying a target task. Importantly, the main aim in theory works on the subject is to understand the extent to which convergence rates -- in learning a common representation -- \emph{may scale with the number $N$ of tasks} (as well as the number of samples per task). First steps in this setting demonstrate this property when both the shared representation amongst tasks, and task-specific regression functions, are linear. This linear setting readily reveals the benefits of aggregating tasks, e.g., via averaging arguments. In practice, however, the representation is often highly nonlinear, introducing nontrivial biases in each task that cannot easily be averaged out as in the linear case. In the present work, we derive theoretical guarantees for meta-learning with nonlinear representations. In particular, assuming the shared nonlinearity maps to an infinite-dimensional RKHS, we show that additional biases can be mitigated with careful regularization that leverages the smoothness of task-specific regression functions,
</details>
<details>
<summary>摘要</summary>
很多最近的理论研究中的meta-学攻击目标是利用相关任务之间的相似表示结构来简化目标任务。重要的是，理论工作的主要目标是理解学习通用表示的速率如何随着任务数量 $N$ 和每个任务的样本数量增加。在线性设定中，首先的步骤表明了这个特性，当共享表示 amongst tasks 和任务特有的回归函数都是线性时，可以通过均值算法来汇集任务。然而，在实践中，表示通常是非线性的，引入了每个任务中的偏置，无法如线性情况那样轻松均值。在本工作中，我们提供了meta-学中非线性表示的理论保证。具体来说，假设共享非线性映射到无穷维度的RKHS中，我们表明了适当的REGULATION可以减轻每个任务中的偏置，同时利用任务特有的回归函数的平滑性。
</details></li>
</ul>
<hr>
<h2 id="Performance-Issue-Identification-in-Cloud-Systems-with-Relational-Temporal-Anomaly-Detection"><a href="#Performance-Issue-Identification-in-Cloud-Systems-with-Relational-Temporal-Anomaly-Detection" class="headerlink" title="Performance Issue Identification in Cloud Systems with Relational-Temporal Anomaly Detection"></a>Performance Issue Identification in Cloud Systems with Relational-Temporal Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10869">http://arxiv.org/abs/2307.10869</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ase-submission/rtanomaly">https://github.com/ase-submission/rtanomaly</a></li>
<li>paper_authors: Wenwei Gu, Jinyang Liu, Zhuangbin Chen, Jianping Zhang, Yuxin Su, Jiazhen Gu, Cong Feng, Zengyin Yang, Michael Lyu</li>
<li>for: 本研究旨在提高大规模云服务系统的可靠性，通过精准地识别和定位性能问题。</li>
<li>methods: 本研究提出了一种基于相关和时间特征的离异检测模型（RTAnomaly），利用图注意层学习度量变量之间的相关性，并采用正例学习方法来Address the issue of potential anomalies in the training data。</li>
<li>results: 对于公共数据集和两个工业数据集，RTAnomaly比基eline模型提高了0.929的F1分数和0.920的 Hit@3 分数，表明其在离异检测中的优越性。<details>
<summary>Abstract</summary>
Performance issues permeate large-scale cloud service systems, which can lead to huge revenue losses. To ensure reliable performance, it's essential to accurately identify and localize these issues using service monitoring metrics. Given the complexity and scale of modern cloud systems, this task can be challenging and may require extensive expertise and resources beyond the capacity of individual humans. Some existing methods tackle this problem by analyzing each metric independently to detect anomalies. However, this could incur overwhelming alert storms that are difficult for engineers to diagnose manually. To pursue better performance, not only the temporal patterns of metrics but also the correlation between metrics (i.e., relational patterns) should be considered, which can be formulated as a multivariate metrics anomaly detection problem. However, most of the studies fall short of extracting these two types of features explicitly. Moreover, there exist some unlabeled anomalies mixed in the training data, which may hinder the detection performance. To address these limitations, we propose the Relational- Temporal Anomaly Detection Model (RTAnomaly) that combines the relational and temporal information of metrics. RTAnomaly employs a graph attention layer to learn the dependencies among metrics, which will further help pinpoint the anomalous metrics that may cause the anomaly effectively. In addition, we exploit the concept of positive unlabeled learning to address the issue of potential anomalies in the training data. To evaluate our method, we conduct experiments on a public dataset and two industrial datasets. RTAnomaly outperforms all the baseline models by achieving an average F1 score of 0.929 and Hit@3 of 0.920, demonstrating its superiority.
</details>
<details>
<summary>摘要</summary>
大规模云服务系统中的性能问题会导致巨大的收益损失。为保证可靠性，需要准确地识别和定位这些问题，使用云服务监控指标。由于现代云系统的复杂性和规模，这个任务可能具有挑战性，需要大量的专业知识和资源。现有的方法通常是分别分析每个指标，检测异常。然而，这可能会导致惊人的警示暴露，很难 для工程师手动诊断。为了提高性能，不仅需要考虑时间序列中的指标异常，还需要考虑指标之间的相互关系（即关系异常），可以形式化为多变量指标异常检测问题。然而，大多数研究没有明确提取这两种特征。此外，许多无标签异常可能存在在训练数据中，这可能会降低检测性能。为解决这些限制，我们提出了关系时间异常检测模型（RTAnomaly）。RTAnomaly将关系和时间信息结合，以便更好地检测异常指标，并且通过图注意层学习指标之间的依赖关系，可以更加准确地定位异常指标。此外，我们利用无标签学习来解决训练数据中的潜在异常问题。我们在公共数据集和两个工业数据集上进行了实验，RTAnomaly比基线模型表现出色，取得了0.929的平均F1分数和0.920的 Hit@3 分数，这说明了它的优势。
</details></li>
</ul>
<hr>
<h2 id="FigCaps-HF-A-Figure-to-Caption-Generative-Framework-and-Benchmark-with-Human-Feedback"><a href="#FigCaps-HF-A-Figure-to-Caption-Generative-Framework-and-Benchmark-with-Human-Feedback" class="headerlink" title="FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback"></a>FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10867">http://arxiv.org/abs/2307.10867</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/figcapshf/figcapshf">https://github.com/figcapshf/figcapshf</a></li>
<li>paper_authors: Ashish Singh, Prateek Agarwal, Zixuan Huang, Arpita Singh, Tong Yu, Sungchul Kim, Victor Bursztyn, Nikos Vlassis, Ryan A. Rossi</li>
<li>for: 这篇论文的目的是提出一种新的插图标题生成框架，以便根据读者首选项生成高质量的插图标题。</li>
<li>methods: 该论文使用了一种自动评估插图标题对的方法，以及一种基于人工反馈学习（RLHF）的生成模型优化方法，以便根据读者首选项生成插图标题。</li>
<li>results: 该论文的实验结果表明，使用RLHF方法可以提高插图标题生成的性能， Specifically, when using BLIP as the base model, the RLHF framework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and Meteor, respectively.<details>
<summary>Abstract</summary>
Captions are crucial for understanding scientific visualizations and documents. Existing captioning methods for scientific figures rely on figure-caption pairs extracted from documents for training, many of which fall short with respect to metrics like helpfulness, explainability, and visual-descriptiveness [15] leading to generated captions being misaligned with reader preferences. To enable the generation of high-quality figure captions, we introduce FigCaps-HF a new framework for figure-caption generation that can incorporate domain expert feedback in generating captions optimized for reader preferences. Our framework comprises of 1) an automatic method for evaluating quality of figure-caption pairs, 2) a novel reinforcement learning with human feedback (RLHF) method to optimize a generative figure-to-caption model for reader preferences. We demonstrate the effectiveness of our simple learning framework by improving performance over standard fine-tuning across different types of models. In particular, when using BLIP as the base model, our RLHF framework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and Meteor, respectively. Finally, we release a large-scale benchmark dataset with human feedback on figure-caption pairs to enable further evaluation and development of RLHF techniques for this problem.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>科学视觉和文档中的标题是非常重要的，现有的科学标题生成方法依靠从文档中提取的标题-图像对进行训练，但是这些方法存在帮助度、可解释性和视觉描述性等缺点，导致生成的标题与读者需求不匹配 [15]。为了生成高质量的标题，我们介绍了 FigCaps-HF 一个新的标题生成框架，可以包含领域专家反馈来生成符合读者需求的标题。我们的框架包括以下两个部分：1. 自动评估标题-图像对的质量2. 一种基于人工反馈的强化学习法（RLHF）来优化一个可生成标题-图像模型，以满足读者需求。我们的简单学习框架可以在不同的模型上提高性能，特别是在使用 BLIP 作为基础模型时，我们的 RLHF 框架可以提高 ROUGE、BLEU 和 Meteor 的表现，具体的提高量分别为 35.7%、16.9% 和 9%。最后，我们发布了一个大规模的人类反馈标题-图像对数据集，以便进一步评估和开发RLHF技术。
</details></li>
</ul>
<hr>
<h2 id="Addressing-caveats-of-neural-persistence-with-deep-graph-persistence"><a href="#Addressing-caveats-of-neural-persistence-with-deep-graph-persistence" class="headerlink" title="Addressing caveats of neural persistence with deep graph persistence"></a>Addressing caveats of neural persistence with deep graph persistence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10865">http://arxiv.org/abs/2307.10865</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ExplainableML/Deep-Graph-Persistence">https://github.com/ExplainableML/Deep-Graph-Persistence</a></li>
<li>paper_authors: Leander Girrbach, Anders Christensen, Ole Winther, Zeynep Akata, A. Sophia Koepke</li>
<li>for: This paper aims to improve the measurement of neural network complexity using a new approach called deep graph persistence.</li>
<li>methods: The paper uses a combination of theoretical analysis and empirical experiments to study the relationship between neural network complexity and the variance of network weights, as well as the spatial concentration of large weights.</li>
<li>results: The paper finds that the proposed deep graph persistence measure provides a more accurate and robust way of measuring neural network complexity, compared to traditional measures such as neural persistence. Additionally, the paper shows that the deep graph persistence measure can capture persistent patterns in the network that are not present in individual layers.<details>
<summary>Abstract</summary>
Neural Persistence is a prominent measure for quantifying neural network complexity, proposed in the emerging field of topological data analysis in deep learning. In this work, however, we find both theoretically and empirically that the variance of network weights and spatial concentration of large weights are the main factors that impact neural persistence. Whilst this captures useful information for linear classifiers, we find that no relevant spatial structure is present in later layers of deep neural networks, making neural persistence roughly equivalent to the variance of weights. Additionally, the proposed averaging procedure across layers for deep neural networks does not consider interaction between layers. Based on our analysis, we propose an extension of the filtration underlying neural persistence to the whole neural network instead of single layers, which is equivalent to calculating neural persistence on one particular matrix. This yields our deep graph persistence measure, which implicitly incorporates persistent paths through the network and alleviates variance-related issues through standardisation. Code is available at https://github.com/ExplainableML/Deep-Graph-Persistence .
</details>
<details>
<summary>摘要</summary>
neural 稳定性是深度学习中的一个重要度量，在 topological data analysis 领域提出。在这项工作中，我们发现了理论和实验上，网络权重的方差和大权重的空间归一化是影响 neural 稳定性的主要因素。尽管这些信息对于线性分类器有用，但我们发现在深度神经网络中的后几层中没有任何相关的空间结构，因此 neural 稳定性大致等于权重的方差。此外，我们提出了层之间交互的权重平均方法，这并不考虑层之间的交互。基于我们的分析，我们提出了一种扩展层次滤波的方法，该方法等同于在一个特定矩阵上计算 neural 稳定性。这种方法会提取网络中 persist 的路径，并通过标准化来降低方差相关的问题。代码可以在 https://github.com/ExplainableML/Deep-Graph-Persistence 上获取。
</details></li>
</ul>
<hr>
<h2 id="Divide-Bind-Your-Attention-for-Improved-Generative-Semantic-Nursing"><a href="#Divide-Bind-Your-Attention-for-Improved-Generative-Semantic-Nursing" class="headerlink" title="Divide &amp; Bind Your Attention for Improved Generative Semantic Nursing"></a>Divide &amp; Bind Your Attention for Improved Generative Semantic Nursing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10864">http://arxiv.org/abs/2307.10864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yumeng Li, Margret Keuper, Dan Zhang, Anna Khoreva<br>for:* 这个研究旨在提高文本至图生成模型的表现，特别是在处理复杂的提示时。methods:* 本研究引入两个新的损失函数：一个新的注意力损失函数和一个绑定损失函数，以提高GSN的表现。results:* 这个方法可以将需要的物品具体地生成出来，并且具有改善的属性调和。在多个评估标准上表现出色。更多影片和更新可以在项目页面上找到：<a target="_blank" rel="noopener" href="https://sites.google.com/view/divide-and-bind">https://sites.google.com/view/divide-and-bind</a>。<details>
<summary>Abstract</summary>
Emerging large-scale text-to-image generative models, e.g., Stable Diffusion (SD), have exhibited overwhelming results with high fidelity. Despite the magnificent progress, current state-of-the-art models still struggle to generate images fully adhering to the input prompt. Prior work, Attend & Excite, has introduced the concept of Generative Semantic Nursing (GSN), aiming to optimize cross-attention during inference time to better incorporate the semantics. It demonstrates promising results in generating simple prompts, e.g., ``a cat and a dog''. However, its efficacy declines when dealing with more complex prompts, and it does not explicitly address the problem of improper attribute binding. To address the challenges posed by complex prompts or scenarios involving multiple entities and to achieve improved attribute binding, we propose Divide & Bind. We introduce two novel loss objectives for GSN: a novel attendance loss and a binding loss. Our approach stands out in its ability to faithfully synthesize desired objects with improved attribute alignment from complex prompts and exhibits superior performance across multiple evaluation benchmarks. More videos and updates can be found on the project page \url{https://sites.google.com/view/divide-and-bind}.
</details>
<details>
<summary>摘要</summary>
新型大规模文本至图生成模型，如稳定扩散（SD），已经显示出惊人的成绩，具有高准确性。 despite the outstanding progress, current state-of-the-art models still struggle to generate images that fully conform to the input prompt. Previous work, Attend & Excite, introduced the concept of Generative Semantic Nursing (GSN), which aims to optimize cross-attention during inference time to better incorporate semantics. It has shown promising results in generating simple prompts, such as "a cat and a dog". However, its effectiveness decreases when dealing with more complex prompts, and it does not explicitly address the problem of improper attribute binding.To address the challenges posed by complex prompts or scenarios involving multiple entities and to achieve improved attribute binding, we propose Divide & Bind. We introduce two novel loss objectives for GSN: a novel attendance loss and a binding loss. Our approach stands out in its ability to faithfully synthesize desired objects with improved attribute alignment from complex prompts and exhibits superior performance across multiple evaluation benchmarks. More videos and updates can be found on the project page [https://sites.google.com/view/divide-and-bind].
</details></li>
</ul>
<hr>
<h2 id="Self-paced-Weight-Consolidation-for-Continual-Learning"><a href="#Self-paced-Weight-Consolidation-for-Continual-Learning" class="headerlink" title="Self-paced Weight Consolidation for Continual Learning"></a>Self-paced Weight Consolidation for Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10845">http://arxiv.org/abs/2307.10845</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/congwei45/spWC">https://github.com/congwei45/spWC</a></li>
<li>paper_authors: Wei Cong, Yang Cong, Gan Sun, Yuyang Liu, Jiahua Dong<br>for:这个论文的目的是提出一个自适应的重量结构架构，以便在继续学习设定下避免悖论损坏。methods:这个架构使用了一个自适应的调节过程，以评估过去任务的推奨贡献。在接受新任务时，所有的过去任务会被排序为“困难”到“容易”的顺序，根据过去任务的优先级。然后，新的持续学习模型将通过选择性地维持过去任务中更困难的知识，以免于悖论损坏。results:实验结果显示，提出的自适应重量结构架构可以有效地提高表现，与其他具有流行的持续学习算法相比。<details>
<summary>Abstract</summary>
Continual learning algorithms which keep the parameters of new tasks close to that of previous tasks, are popular in preventing catastrophic forgetting in sequential task learning settings. However, 1) the performance for the new continual learner will be degraded without distinguishing the contributions of previously learned tasks; 2) the computational cost will be greatly increased with the number of tasks, since most existing algorithms need to regularize all previous tasks when learning new tasks. To address the above challenges, we propose a self-paced Weight Consolidation (spWC) framework to attain robust continual learning via evaluating the discriminative contributions of previous tasks. To be specific, we develop a self-paced regularization to reflect the priorities of past tasks via measuring difficulty based on key performance indicator (i.e., accuracy). When encountering a new task, all previous tasks are sorted from "difficult" to "easy" based on the priorities. Then the parameters of the new continual learner will be learned via selectively maintaining the knowledge amongst more difficult past tasks, which could well overcome catastrophic forgetting with less computational cost. We adopt an alternative convex search to iteratively update the model parameters and priority weights in the bi-convex formulation. The proposed spWC framework is plug-and-play, which is applicable to most continual learning algorithms (e.g., EWC, MAS and RCIL) in different directions (e.g., classification and segmentation). Experimental results on several public benchmark datasets demonstrate that our proposed framework can effectively improve performance when compared with other popular continual learning algorithms.
</details>
<details>
<summary>摘要</summary>
CONTINUAL LEARNING算法，它们可以保持新任务的参数与之前任务的参数很近，是sequential task learning setting中具有广泛应用的方法。但是，1）新的CONTINUAL LEARNING模型会受到之前任务的影响，而无法分别评估这些任务的贡献; 2）随着任务的增加，大多数现有算法的计算成本将会增加，因为它们需要在学习新任务时对所有之前任务进行规regularization。为了解决这些挑战，我们提出了一个自适应Weight Consolidation（spWC）框架，以实现robust continual learning。具体来说，我们开发了一种自适应规则，通过测量难度来衡量过去任务的优先级。当面临新任务时，我们将所有之前任务排序为“difficult”到“easy”，根据难度来决定哪些任务需要更多的学习。然后，我们将新的CONTINUAL LEARNING模型通过选择保持更难的过去任务的知识来学习，以避免catastrophic forgetting，同时减少计算成本。我们采用了一种alternative convex search来逐渐更新模型参数和优先级权重。我们的spWC框架可以与多种常见的CONTINUAL LEARNING算法（例如EWC、MAS和RCIL）相结合，并可以在不同的方向（例如分类和分割）中应用。我们在一些公共benchmark数据集上进行了实验，结果表明，我们的提出的框架可以效果地提高性能，与其他流行的CONTINUAL LEARNING算法相比。
</details></li>
</ul>
<hr>
<h2 id="Global-Precipitation-Nowcasting-of-Integrated-Multi-satellitE-Retrievals-for-GPM-A-U-Net-Convolutional-LSTM-Architecture"><a href="#Global-Precipitation-Nowcasting-of-Integrated-Multi-satellitE-Retrievals-for-GPM-A-U-Net-Convolutional-LSTM-Architecture" class="headerlink" title="Global Precipitation Nowcasting of Integrated Multi-satellitE Retrievals for GPM: A U-Net Convolutional LSTM Architecture"></a>Global Precipitation Nowcasting of Integrated Multi-satellitE Retrievals for GPM: A U-Net Convolutional LSTM Architecture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10843">http://arxiv.org/abs/2307.10843</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/reyhaneh-92/genesis_nowcast">https://github.com/reyhaneh-92/genesis_nowcast</a></li>
<li>paper_authors: Reyhaneh Rahimi, Ardeshir Ebtehaj, Ali Behrangi, Jackson Tan</li>
<li>for: 这个论文提出了一种深度学习架构，用于在全球范围内预测降水，每隔30分钟预测4个小时前的降水情况。</li>
<li>methods: 这个架构组合了U-Net和卷积Long Short-Term Memory（LSTM）神经网络，并使用IMERG和一些关键的预测变量从全球预报系统（GFS）进行训练。</li>
<li>results: 研究发现，使用不同的训练损失函数（如mean-squared error和focal-loss）对降水预报质量有着不同的影响，并且发现在降水极端情况（&gt;8mm&#x2F;hr）下，分类网络可以在降水预报中表现更好，特别是在较长的预测时间内。<details>
<summary>Abstract</summary>
This paper presents a deep learning architecture for nowcasting of precipitation almost globally every 30 min with a 4-hour lead time. The architecture fuses a U-Net and a convolutional long short-term memory (LSTM) neural network and is trained using data from the Integrated MultisatellitE Retrievals for GPM (IMERG) and a few key precipitation drivers from the Global Forecast System (GFS). The impacts of different training loss functions, including the mean-squared error (regression) and the focal-loss (classification), on the quality of precipitation nowcasts are studied. The results indicate that the regression network performs well in capturing light precipitation (below 1.6 mm/hr), but the classification network can outperform the regression network for nowcasting of precipitation extremes (>8 mm/hr), in terms of the critical success index (CSI).. Using the Wasserstein distance, it is shown that the predicted precipitation by the classification network has a closer class probability distribution to the IMERG than the regression network. It is uncovered that the inclusion of the physical variables can improve precipitation nowcasting, especially at longer lead times in both networks. Taking IMERG as a relative reference, a multi-scale analysis in terms of fractions skill score (FSS), shows that the nowcasting machine remains skillful (FSS > 0.5) at the resolution of 10 km compared to 50 km for GFS. For precipitation rates greater than 4~mm/hr, only the classification network remains FSS-skillful on scales greater than 50 km within a 2-hour lead time.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Label-Calibration-for-Semantic-Segmentation-Under-Domain-Shift"><a href="#Label-Calibration-for-Semantic-Segmentation-Under-Domain-Shift" class="headerlink" title="Label Calibration for Semantic Segmentation Under Domain Shift"></a>Label Calibration for Semantic Segmentation Under Domain Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10842">http://arxiv.org/abs/2307.10842</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ondrej Bohdal, Da Li, Timothy Hospedales</li>
<li>for: 这篇论文是用于解决 semantic segmentation 模型在新Domain的性能下降问题。</li>
<li>methods: 论文使用了将偏移量计算为软分类标准的方法，并根据这些标准来做预测。这个适应程序快速、几乎是 Computational resources 的价格，并带来了许多性能改善。</li>
<li>results: 论文显示了这个适应程序在实际上 Synthetic-to-real semantic segmentation 问题上的好处。<details>
<summary>Abstract</summary>
Performance of a pre-trained semantic segmentation model is likely to substantially decrease on data from a new domain. We show a pre-trained model can be adapted to unlabelled target domain data by calculating soft-label prototypes under the domain shift and making predictions according to the prototype closest to the vector with predicted class probabilities. The proposed adaptation procedure is fast, comes almost for free in terms of computational resources and leads to considerable performance improvements. We demonstrate the benefits of such label calibration on the highly-practical synthetic-to-real semantic segmentation problem.
</details>
<details>
<summary>摘要</summary>
“一个先行训练的 semantic segmentation 模型在新领域数据上的性能可能会显著下降。我们显示了一个先行模型可以通过对目标领域数据中的域shiftCalculate soft-label prototype并根据最近的 вектор预测类 probabilities进行预测。我们提出的适应过程快速、 Computational resources几乎不需要Extra cost，并导致显著性能提升。我们在实际上非常重要的 synthetic-to-real semantic segmentation 问题中证明了这种标签准化的好处。”Note: Simplified Chinese is used here, as it is the most widely used variety of Chinese in mainland China and Singapore. Traditional Chinese is used in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Conversational-Shaping-for-Intelligent-Agents"><a href="#Adversarial-Conversational-Shaping-for-Intelligent-Agents" class="headerlink" title="Adversarial Conversational Shaping for Intelligent Agents"></a>Adversarial Conversational Shaping for Intelligent Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11785">http://arxiv.org/abs/2307.11785</a></li>
<li>repo_url: None</li>
<li>paper_authors: Piotr Tarasiewicz, Sultan Kenjeyev, Ilana Sebag, Shehab Alshehabi</li>
<li>for: 这项研究旨在提高智能对话代理人的性能。</li>
<li>methods: 这两个模型都使用了对抗对话shape的方法，分别是基于REGS模型（Li et al., 18）的奖励授予模型和基于seq2seq和 transformers 的 reinforcement learning框架。</li>
<li>results: 研究表明，这两个模型在不同的训练细节下表现出色，能够提高智能对话代理人的性能。<details>
<summary>Abstract</summary>
The recent emergence of deep learning methods has enabled the research community to achieve state-of-the art results in several domains including natural language processing. However, the current robocall system remains unstable and inaccurate: text generator and chat-bots can be tedious and misunderstand human-like dialogue. In this work, we study the performance of two models able to enhance an intelligent conversational agent through adversarial conversational shaping: a generative adversarial network with policy gradient (GANPG) and a generative adversarial network with reward for every generation step (REGS) based on the REGS model presented in Li et al. [18] . This model is able to assign rewards to both partially and fully generated text sequences. We discuss performance with different training details : seq2seq [ 36] and transformers [37 ] in a reinforcement learning framework.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="What-Indeed-is-an-Achievable-Provable-Guarantee-for-Learning-Enabled-Safety-Critical-Systems"><a href="#What-Indeed-is-an-Achievable-Provable-Guarantee-for-Learning-Enabled-Safety-Critical-Systems" class="headerlink" title="What, Indeed, is an Achievable Provable Guarantee for Learning-Enabled Safety Critical Systems"></a>What, Indeed, is an Achievable Provable Guarantee for Learning-Enabled Safety Critical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11784">http://arxiv.org/abs/2307.11784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saddek Bensalem, Chih-Hong Cheng, Wei Huang, Xiaowei Huang, Changshun Wu, Xingyu Zhao</li>
<li>for: 本研究旨在解决机器学习技术在安全关键领域中可靠使用的挑战。</li>
<li>methods: 本文提出了设计和验证这类系统的工程和研究挑战，并基于现有工作无法实现可证明保证的观察，提出了两步验证方法以实现可证明统计保证。</li>
<li>results: 本文提出的两步验证方法可以实现可证明统计保证，并且可以应用于各种安全关键领域。<details>
<summary>Abstract</summary>
Machine learning has made remarkable advancements, but confidently utilising learning-enabled components in safety-critical domains still poses challenges. Among the challenges, it is known that a rigorous, yet practical, way of achieving safety guarantees is one of the most prominent. In this paper, we first discuss the engineering and research challenges associated with the design and verification of such systems. Then, based on the observation that existing works cannot actually achieve provable guarantees, we promote a two-step verification method for the ultimate achievement of provable statistical guarantees.
</details>
<details>
<summary>摘要</summary>
机器学习技术已经取得了很大的进步，但在安全关键领域中使用学习能力强 componenets仍存在挑战。其中一个最大的挑战是实现可证明的安全保证。在这篇论文中，我们首先讨论了设计和验证这些系统的工程和研究挑战。然后，根据现有的工作无法实际实现可证明的保证，我们提出了一种两步验证方法以实现可证明的统计保证。
</details></li>
</ul>
<hr>
<h2 id="On-Combining-Expert-Demonstrations-in-Imitation-Learning-via-Optimal-Transport"><a href="#On-Combining-Expert-Demonstrations-in-Imitation-Learning-via-Optimal-Transport" class="headerlink" title="On Combining Expert Demonstrations in Imitation Learning via Optimal Transport"></a>On Combining Expert Demonstrations in Imitation Learning via Optimal Transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10810">http://arxiv.org/abs/2307.10810</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ilanasebag/Sliced-MMOT-Imitation-Learning">https://github.com/ilanasebag/Sliced-MMOT-Imitation-Learning</a></li>
<li>paper_authors: Ilana Sebag, Samuel Cohen, Marc Peter Deisenroth</li>
<li>for: 教学机器人特定任务 через专家示范</li>
<li>methods: 使用优化运输方法度量机器人与专家轨迹之间的距离，并将多个专家示范combined in the OT sense</li>
<li>results: 在OpenAI Gym控制环境中，提供了一种可以学习多个专家示范的方法，并且分析了其效率，发现标准方法并不总是优化的。<details>
<summary>Abstract</summary>
Imitation learning (IL) seeks to teach agents specific tasks through expert demonstrations. One of the key approaches to IL is to define a distance between agent and expert and to find an agent policy that minimizes that distance. Optimal transport methods have been widely used in imitation learning as they provide ways to measure meaningful distances between agent and expert trajectories. However, the problem of how to optimally combine multiple expert demonstrations has not been widely studied. The standard method is to simply concatenate state (-action) trajectories, which is problematic when trajectories are multi-modal. We propose an alternative method that uses a multi-marginal optimal transport distance and enables the combination of multiple and diverse state-trajectories in the OT sense, providing a more sensible geometric average of the demonstrations. Our approach enables an agent to learn from several experts, and its efficiency is analyzed on OpenAI Gym control environments and demonstrates that the standard method is not always optimal.
</details>
<details>
<summary>摘要</summary>
模仿学习（IL） aimed to teach agents specific tasks through expert demonstrations. One of the key approaches to IL is to define a distance between agent and expert and to find an agent policy that minimizes that distance. Optimal transport methods have been widely used in imitation learning as they provide ways to measure meaningful distances between agent and expert trajectories. However, the problem of how to optimally combine multiple expert demonstrations has not been widely studied. The standard method is to simply concatenate state (-action) trajectories, which is problematic when trajectories are multi-modal. We propose an alternative method that uses a multi-marginal optimal transport distance and enables the combination of multiple and diverse state-trajectories in the OT sense, providing a more sensible geometric average of the demonstrations. Our approach enables an agent to learn from several experts, and its efficiency is analyzed on OpenAI Gym control environments and demonstrates that the standard method is not always optimal.Here's the translation in Traditional Chinese as well:模仿学习（IL）目的是教导代理人特定任务通过专家示范。一个关键的IL方法是定义代理人和专家之间的距离，并寻找一个代理人策略可以最小化这个距离。优化交通方法在模仿学习中被广泛使用，因为它们提供了在代理人和专家路径之间量化意义的距离。然而，多个专家示范之间的并合尚未受到广泛研究。标准方法是将state (-action) trajectories concatenated，却当路径为多模的时候，这个方法有问题。我们提出了一个替代方法，使用多个中心优化交通距离，允许多元和多样的state-trajectories在OT意义下并合，提供一个更有意义的几何平均。我们的方法允许代理人从多个专家学习，并在OpenAI Gym控制环境中分析了其效率，证明标准方法不一定是最佳。
</details></li>
</ul>
<hr>
<h2 id="Communication-Efficient-Split-Learning-via-Adaptive-Feature-Wise-Compression"><a href="#Communication-Efficient-Split-Learning-via-Adaptive-Feature-Wise-Compression" class="headerlink" title="Communication-Efficient Split Learning via Adaptive Feature-Wise Compression"></a>Communication-Efficient Split Learning via Adaptive Feature-Wise Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10805">http://arxiv.org/abs/2307.10805</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongjeong Oh, Jaeho Lee, Christopher G. Brinton, Yo-Seb Jeon</li>
<li>for: 提高SL训练过程中通信开销的效率</li>
<li>methods: 利用矩阵列中具有不同散度的特征，并采用适应性的特征排除和特征量化两种压缩策略</li>
<li>results: 在MNIST、CIFAR-10和CelebA datasets上，对比当前SL框架，提高了5.6%以上的分类精度，同时压缩开销比vanilla SL框架减少了320倍。<details>
<summary>Abstract</summary>
This paper proposes a novel communication-efficient split learning (SL) framework, named SplitFC, which reduces the communication overhead required for transmitting intermediate feature and gradient vectors during the SL training process. The key idea of SplitFC is to leverage different dispersion degrees exhibited in the columns of the matrices. SplitFC incorporates two compression strategies: (i) adaptive feature-wise dropout and (ii) adaptive feature-wise quantization. In the first strategy, the intermediate feature vectors are dropped with adaptive dropout probabilities determined based on the standard deviation of these vectors. Then, by the chain rule, the intermediate gradient vectors associated with the dropped feature vectors are also dropped. In the second strategy, the non-dropped intermediate feature and gradient vectors are quantized using adaptive quantization levels determined based on the ranges of the vectors. To minimize the quantization error, the optimal quantization levels of this strategy are derived in a closed-form expression. Simulation results on the MNIST, CIFAR-10, and CelebA datasets demonstrate that SplitFC provides more than a 5.6% increase in classification accuracy compared to state-of-the-art SL frameworks, while they require 320 times less communication overhead compared to the vanilla SL framework without compression.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Adaptive feature-wise dropout: Intermediate feature vectors are dropped with adaptive dropout probabilities determined based on the standard deviation of these vectors. As a result, the intermediate gradient vectors associated with the dropped feature vectors are also dropped.2. Adaptive feature-wise quantization: Non-dropped intermediate feature and gradient vectors are quantized using adaptive quantization levels determined based on the ranges of the vectors. To minimize quantization error, the optimal quantization levels are derived in a closed-form expression.Compared to state-of-the-art SL frameworks, SplitFC provides more than a 5.6% increase in classification accuracy while requiring 320 times less communication overhead. This is demonstrated through simulation results on the MNIST, CIFAR-10, and CelebA datasets.</details></li>
</ol>
<hr>
<h2 id="Spatial-Temporal-Data-Mining-for-Ocean-Science-Data-Methodologies-and-Opportunities"><a href="#Spatial-Temporal-Data-Mining-for-Ocean-Science-Data-Methodologies-and-Opportunities" class="headerlink" title="Spatial-Temporal Data Mining for Ocean Science: Data, Methodologies, and Opportunities"></a>Spatial-Temporal Data Mining for Ocean Science: Data, Methodologies, and Opportunities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10803">http://arxiv.org/abs/2307.10803</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanchen Yang, Wengen Li, Shuyu Wang, Hui Li, Jihong Guan, Shuigeng Zhou, Jiannong Cao</li>
<li>for: 本研究提供了 ocean 科学领域中存在数据挖掘的彻底评估，帮助计算机科学家和海洋科学家更好地理解 ocean 数据挖掘的基本概念、关键技术和开放问题。</li>
<li>methods: 本研究涵盖了 ocean 数据质量提升技术、存储和检索技术、数据挖掘技术等多个方面。</li>
<li>results: 本研究对 ocean 数据挖掘的各种应用进行了分类和详细介绍，包括预测、事件检测、模式挖掘和异常检测等多种任务。同时，本研究还提出了一些可能的研究机遇。<details>
<summary>Abstract</summary>
With the rapid amassing of spatial-temporal (ST) ocean data, many spatial-temporal data mining (STDM) studies have been conducted to address various oceanic issues, including climate forecasting and disaster warning. Compared with typical ST data (e.g., traffic data), ST ocean data is more complicated but with unique characteristics, e.g., diverse regionality and high sparsity. These characteristics make it difficult to design and train STDM models on ST ocean data. To the best of our knowledge, a comprehensive survey of existing studies remains missing in the literature, which hinders not only computer scientists from identifying the research issues in ocean data mining but also ocean scientists to apply advanced STDM techniques. In this paper, we provide a comprehensive survey of existing STDM studies for ocean science. Concretely, we first review the widely-used ST ocean datasets and highlight their unique characteristics. Then, typical ST ocean data quality enhancement techniques are explored. Next, we classify existing STDM studies in ocean science into four types of tasks, i.e., prediction, event detection, pattern mining, and anomaly detection, and elaborate on the techniques for these tasks. Finally, promising research opportunities are discussed. This survey can help scientists from both computer science and ocean science better understand the fundamental concepts, key techniques, and open challenges of STDM for ocean science.
</details>
<details>
<summary>摘要</summary>
随着空间时间（ST）海洋数据的快速积累，许多空间时间数据挖掘（STDM）研究已经进行以解决海洋问题，如气候预测和自然灾害警报。与常见的ST数据（例如交通数据）相比，ST海洋数据更加复杂，但具有独特特征，如多样性和高稀缺。这些特征使得设计和训练STDM模型在ST海洋数据上变得更加困难。据我们所知，现有的相关研究的总体评估在文献中缺失，这不仅阻碍了计算机科学家从海洋数据挖掘中了解研究问题，也阻碍了海洋科学家应用高级STDM技术。在这篇论文中，我们提供了海洋科学领域的STDM研究的全面评估。具体来说，我们首先介绍了广泛使用的ST海洋数据集和其独特特征。然后，我们探讨了常见的ST海洋数据质量提升技术。接着，我们将现有的STDM研究在海洋科学领域分为四类任务，即预测、事件检测、模式挖掘和异常检测，并详细介绍这些任务的技术。最后，我们提出了有前途的研究机遇。这种调查可以帮助计算机科学家和海洋科学家更好地理解STDM的基本概念、关键技术和开放的挑战，并且推动海洋科学领域的发展。
</details></li>
</ul>
<hr>
<h2 id="Meta-Transformer-A-Unified-Framework-for-Multimodal-Learning"><a href="#Meta-Transformer-A-Unified-Framework-for-Multimodal-Learning" class="headerlink" title="Meta-Transformer: A Unified Framework for Multimodal Learning"></a>Meta-Transformer: A Unified Framework for Multimodal Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10802">http://arxiv.org/abs/2307.10802</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/invictus717/MetaTransformer">https://github.com/invictus717/MetaTransformer</a></li>
<li>paper_authors: Yiyuan Zhang, Kaixiong Gong, Kaipeng Zhang, Hongsheng Li, Yu Qiao, Wanli Ouyang, Xiangyu Yue</li>
<li>For: The paper aims to build a unified multimodal intelligence framework using transformers, which can handle various modalities such as natural language, images, point clouds, audio, video, and more, without paired multimodal training data.* Methods: The proposed framework, called Meta-Transformer, uses a frozen encoder to extract high-level semantic features from raw input data from different modalities, and then applies task-specific heads for downstream tasks.* Results: The paper demonstrates the effectiveness of Meta-Transformer on a wide range of tasks, including fundamental perception, practical applications, and data mining, across 12 modalities, and shows that it outperforms existing multimodal learning methods.<details>
<summary>Abstract</summary>
Multimodal learning aims to build models that can process and relate information from multiple modalities. Despite years of development in this field, it still remains challenging to design a unified network for processing various modalities ($\textit{e.g.}$ natural language, 2D images, 3D point clouds, audio, video, time series, tabular data) due to the inherent gaps among them. In this work, we propose a framework, named Meta-Transformer, that leverages a $\textbf{frozen}$ encoder to perform multimodal perception without any paired multimodal training data. In Meta-Transformer, the raw input data from various modalities are mapped into a shared token space, allowing a subsequent encoder with frozen parameters to extract high-level semantic features of the input data. Composed of three main components: a unified data tokenizer, a modality-shared encoder, and task-specific heads for downstream tasks, Meta-Transformer is the first framework to perform unified learning across 12 modalities with unpaired data. Experiments on different benchmarks reveal that Meta-Transformer can handle a wide range of tasks including fundamental perception (text, image, point cloud, audio, video), practical application (X-Ray, infrared, hyperspectral, and IMU), and data mining (graph, tabular, and time-series). Meta-Transformer indicates a promising future for developing unified multimodal intelligence with transformers. Code will be available at https://github.com/invictus717/MetaTransformer
</details>
<details>
<summary>摘要</summary>
多模态学习旨在建立能处理和关联多种模式的模型。尽管这一领域已有多年发展，仍然困难设计能处理多种模式的统一网络（例如自然语言、2D图像、3D点云、音频、视频、时间序列、表格数据）的网络，因为这些模式之间存在基本的差异。在这种情况下，我们提出了一种名为Meta-Transformer的框架，它利用一个冻结的encoder来实现多模式感知，不需要任何协调的多模式培训数据。在Meta-Transformer框架中，各种模式的原始输入数据都会被映射到一个共享的token空间中，以便后续的encoder WITH冻结参数可以从输入数据中提取高级别的 semantic feature。Meta-Transformer框架包括三个主要组件：一个统一的数据tokenizer、一个共享的encoder和下游任务特定的头。Meta-Transformer是首个可以通过不协调的数据进行多模式学习的框架，实验表明，它可以处理各种任务，包括基本的感知（文本、图像、点云、音频、视频）、实用应用（X射、抗红外、多spectral、IMU）和数据挖掘（图表、表格、时间序列）。Meta-Transformer表明了未来多模式智能的发展将受到transformer的推动。代码将在https://github.com/invictus717/MetaTransformer上提供。
</details></li>
</ul>
<hr>
<h2 id="Convergence-of-Adam-for-Non-convex-Objectives-Relaxed-Hyperparameters-and-Non-ergodic-Case"><a href="#Convergence-of-Adam-for-Non-convex-Objectives-Relaxed-Hyperparameters-and-Non-ergodic-Case" class="headerlink" title="Convergence of Adam for Non-convex Objectives: Relaxed Hyperparameters and Non-ergodic Case"></a>Convergence of Adam for Non-convex Objectives: Relaxed Hyperparameters and Non-ergodic Case</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11782">http://arxiv.org/abs/2307.11782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meixuan He, Yuqing Liang, Jinlan Liu, Dongpo Xu</li>
<li>for: 本文探讨了 Adam 优化算法在非对称 Setting 中的收敛性，特别是在实际应用中遇到的非平衡收敛问题。</li>
<li>methods: 本文引入了精确的ergodic和非ergodic收敛定义，并论证了这些定义对 Stochastic optimization algorithm 的收敛性的重要性。同时，本文提出了一种更lax的 suficient condition  для Adam 的ergodic收敛保证，并实现了对 K 的 almost sure ergodic 收敛率，其准确性可以到 $o(1&#x2F;\sqrt{K})$。</li>
<li>results: 本文证明了 Adam 的最后一个迭代 converges to a stationary point  для非对称目标函数，这是首次证明。此外，本文还证明了在 PL condition 下，Adam 的非ergodic收敛率为 O(1&#x2F;K)。这些发现为 Adam 在解决非对称 Stochastic optimization problems 提供了坚实的理论基础。<details>
<summary>Abstract</summary>
Adam is a commonly used stochastic optimization algorithm in machine learning. However, its convergence is still not fully understood, especially in the non-convex setting. This paper focuses on exploring hyperparameter settings for the convergence of vanilla Adam and tackling the challenges of non-ergodic convergence related to practical application. The primary contributions are summarized as follows: firstly, we introduce precise definitions of ergodic and non-ergodic convergence, which cover nearly all forms of convergence for stochastic optimization algorithms. Meanwhile, we emphasize the superiority of non-ergodic convergence over ergodic convergence. Secondly, we establish a weaker sufficient condition for the ergodic convergence guarantee of Adam, allowing a more relaxed choice of hyperparameters. On this basis, we achieve the almost sure ergodic convergence rate of Adam, which is arbitrarily close to $o(1/\sqrt{K})$. More importantly, we prove, for the first time, that the last iterate of Adam converges to a stationary point for non-convex objectives. Finally, we obtain the non-ergodic convergence rate of $O(1/K)$ for function values under the Polyak-Lojasiewicz (PL) condition. These findings build a solid theoretical foundation for Adam to solve non-convex stochastic optimization problems.
</details>
<details>
<summary>摘要</summary>
亚当是机器学习中常用的数学估计算法，但其对于非凸设计仍然未完全理解。本文专注于探索亚当的参数设定，以提高实际应用中的非ergodic convergence。主要贡献如下：首先，我们提出了精确的ergodic和non-ergodic convergence定义，包括大部分数学估计算法的各种对应。同时，我们强调了非ergodic convergence的超越性。第二，我们提出了一个较弱的充分condition для亚当的ergodic convergence保证，允许更宽松的参数选择。基于这个基础，我们获得了逐渐趋向于$o(1/\sqrt{K})$的almost sure ergodic convergence率，这是实际上的阶段趋向。更重要的是，我们证明了亚当的最后迭代确实会 converges to a stationary point for non-convex objectives，这是首次证明。最后，我们获得了function values的non-ergodic convergence率为$O(1/K)$，这是PL condition下的。这些成果创造了机器学习中亚当解决非凸数学估计问题的坚实理论基础。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-PatchCore-for-Few-many-shot-Anomaly-Detection"><a href="#Optimizing-PatchCore-for-Few-many-shot-Anomaly-Detection" class="headerlink" title="Optimizing PatchCore for Few&#x2F;many-shot Anomaly Detection"></a>Optimizing PatchCore for Few&#x2F;many-shot Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10792">http://arxiv.org/abs/2307.10792</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/scortexio/patchcore-few-shot">https://github.com/scortexio/patchcore-few-shot</a></li>
<li>paper_authors: João Santos, Triet Tran, Oliver Rippel</li>
<li>for: 本研究探讨了使用only few selected samples进行异常检测（AD）的新兴领域，并评估了现有的普shot AD算法在这种设定下的性能。</li>
<li>methods: 本研究使用了PatchCore算法，current state-of-the-art full-shot AD&#x2F;AS algorithm，并对其在few-shot和many-shot设定下进行了性能研究。</li>
<li>results: 研究发现，可以通过优化算法的多种 гиперparameters来实现显著性能提升，并且可以通过将supervised learning中的技术转移到AD领域来进一步提高性能。基于这些发现，我们在VisA dataset上达到了新的state of the art in few-shot AD。<details>
<summary>Abstract</summary>
Few-shot anomaly detection (AD) is an emerging sub-field of general AD, and tries to distinguish between normal and anomalous data using only few selected samples. While newly proposed few-shot AD methods do compare against pre-existing algorithms developed for the full-shot domain as baselines, they do not dedicatedly optimize them for the few-shot setting. It thus remains unclear if the performance of such pre-existing algorithms can be further improved. We address said question in this work. Specifically, we present a study on the AD/anomaly segmentation (AS) performance of PatchCore, the current state-of-the-art full-shot AD/AS algorithm, in both the few-shot and the many-shot settings. We hypothesize that further performance improvements can be realized by (I) optimizing its various hyperparameters, and by (II) transferring techniques known to improve few-shot supervised learning to the AD domain. Exhaustive experiments on the public VisA and MVTec AD datasets reveal that (I) significant performance improvements can be realized by optimizing hyperparameters such as the underlying feature extractor, and that (II) image-level augmentations can, but are not guaranteed, to improve performance. Based on these findings, we achieve a new state of the art in few-shot AD on VisA, further demonstrating the merit of adapting pre-existing AD/AS methods to the few-shot setting. Last, we identify the investigation of feature extractors with a strong inductive bias as a potential future research direction for (few-shot) AD/AS.
</details>
<details>
<summary>摘要</summary>
几个shot异常检测（AD）是一个出现的子领域，它目标是通过选择一些示例来分辨正常数据和异常数据。新提出的几个shot AD方法与全shot领域的已有算法进行比较，但它们并未专门优化这些算法 для几个shot设置。因此，它们的性能是否可以进一步改进是 unclear。我们在这里回答这个问题。我们对PatchCore，当前的全shot AD/AS算法，在几个shot和多个shot的设置下进行了研究。我们认为可以通过（I）优化其多种超参数，以及（II）将几个shot超参数优化技术转移到AD领域来实现性能改进。我们对公共的VisA和MVTec AD数据集进行了详细的实验，发现（I）可以通过优化特征提取器来实现显著性能提升，并且（II）图像级别的扩展可以，但并不一定，提高性能。根据这些发现，我们在VisA上达到了新的状态机器，进一步证明了适应已有AD/AS方法到几个shot设置的价值。最后，我们认为在（几个shot）AD/AS领域中研究具有强 inductive bias 的特征提取器是一个未来研究方向。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-attacks-for-mixtures-of-classifiers"><a href="#Adversarial-attacks-for-mixtures-of-classifiers" class="headerlink" title="Adversarial attacks for mixtures of classifiers"></a>Adversarial attacks for mixtures of classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10788">http://arxiv.org/abs/2307.10788</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas Gnecco Heredia, Benjamin Negrevergne, Yann Chevaleyre</li>
<li>for: 提高鲁棒性 against adversarial attacks</li>
<li>methods: 使用 mixtures of classifiers (a.k.a. randomized ensembles)</li>
<li>results: 引入了两个可能性的攻击方法，并提供了对binary linear setting的理论保证，并在 sintethic 和实际数据集上进行了实验。<details>
<summary>Abstract</summary>
Mixtures of classifiers (a.k.a. randomized ensembles) have been proposed as a way to improve robustness against adversarial attacks. However, it has been shown that existing attacks are not well suited for this kind of classifiers. In this paper, we discuss the problem of attacking a mixture in a principled way and introduce two desirable properties of attacks based on a geometrical analysis of the problem (effectiveness and maximality). We then show that existing attacks do not meet both of these properties. Finally, we introduce a new attack called lattice climber attack with theoretical guarantees on the binary linear setting, and we demonstrate its performance by conducting experiments on synthetic and real datasets.
</details>
<details>
<summary>摘要</summary>
Mixtures of classifiers（也称为随机集合）已经被提议用于提高对抗敌意攻击的鲁棒性。然而，已经证明现有的攻击方法不适合这种类型的分类器。在这篇论文中，我们讨论了攻击混合的问题，并引入了两种攻击的愿景性质（有效性和最大化）。然后，我们证明现有的攻击方法不符合这两种质量。最后，我们介绍了一种新的攻击方法called lattice climber attack，并提供了在二元线性设定下的理论保证。我们通过synthetic和实际数据进行了实验，以证明其性能。
</details></li>
</ul>
<hr>
<h2 id="Feed-Forward-Source-Free-Domain-Adaptation-via-Class-Prototypes"><a href="#Feed-Forward-Source-Free-Domain-Adaptation-via-Class-Prototypes" class="headerlink" title="Feed-Forward Source-Free Domain Adaptation via Class Prototypes"></a>Feed-Forward Source-Free Domain Adaptation via Class Prototypes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10787">http://arxiv.org/abs/2307.10787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ondrej Bohdal, Da Li, Timothy Hospedales</li>
<li>for: 这篇论文是为了探讨源自由适应领域的问题，即无需访问源数据就可以进行适应。</li>
<li>methods: 该论文提出了一种简单的前向方法，挑战了基于反传播的适应方法的需要。该方法基于使用预训练模型计算类别的原型，并实现了与预训练模型相比的强大改进。</li>
<li>results: 该论文的实验结果表明，该方法可以在短时间内达到高度的准确率，仅需要小于源适应方法的时间。<details>
<summary>Abstract</summary>
Source-free domain adaptation has become popular because of its practical usefulness and no need to access source data. However, the adaptation process still takes a considerable amount of time and is predominantly based on optimization that relies on back-propagation. In this work we present a simple feed-forward approach that challenges the need for back-propagation based adaptation. Our approach is based on computing prototypes of classes under the domain shift using a pre-trained model. It achieves strong improvements in accuracy compared to the pre-trained model and requires only a small fraction of time of existing domain adaptation methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Efficient-Beam-Tree-Recursion"><a href="#Efficient-Beam-Tree-Recursion" class="headerlink" title="Efficient Beam Tree Recursion"></a>Efficient Beam Tree Recursion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10779">http://arxiv.org/abs/2307.10779</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jishnu Ray Chowdhury, Cornelia Caragea</li>
<li>for: 提高 BT-RvNN 的可扩展性和可读性，以及将其作为深度学习工具箱中的另一个构建件。</li>
<li>methods: 识别 BT-RvNN 中最主要的瓶颈为探索函数和回归细胞函数的杂mix问题，并提出了解决方案，以减少 BT-RvNN 的内存使用情况。</li>
<li>results: 通过提出的策略，可以将 BT-RvNN 的内存使用情况减少 $10$-$16$ 倍，同时在 ListOps 中创造出新的状态ulse术，并在其他任务上保持相似的性能。<details>
<summary>Abstract</summary>
Beam Tree Recursive Neural Network (BT-RvNN) was recently proposed as a simple extension of Gumbel Tree RvNN and it was shown to achieve state-of-the-art length generalization performance in ListOps while maintaining comparable performance on other tasks. However, although not the worst in its kind, BT-RvNN can be still exorbitantly expensive in memory usage. In this paper, we identify the main bottleneck in BT-RvNN's memory usage to be the entanglement of the scorer function and the recursive cell function. We propose strategies to remove this bottleneck and further simplify its memory usage. Overall, our strategies not only reduce the memory usage of BT-RvNN by $10$-$16$ times but also create a new state-of-the-art in ListOps while maintaining similar performance in other tasks. In addition, we also propose a strategy to utilize the induced latent-tree node representations produced by BT-RvNN to turn BT-RvNN from a sentence encoder of the form $f:\mathbb{R}^{n \times d} \rightarrow \mathbb{R}^{d}$ into a sequence contextualizer of the form $f:\mathbb{R}^{n \times d} \rightarrow \mathbb{R}^{n \times d}$. Thus, our proposals not only open up a path for further scalability of RvNNs but also standardize a way to use BT-RvNNs as another building block in the deep learning toolkit that can be easily stacked or interfaced with other popular models such as Transformers and Structured State Space models.
</details>
<details>
<summary>摘要</summary>
“Recently proposed Beam Tree Recursive Neural Network (BT-RvNN) 可以实现列表作业中的最佳长度决定性性能，但是它的内存使用量仍然很高。在本文中，我们识别了BT-RvNN的主要瓶颈是探索函数和遗传函数的混合。我们提出了一些策略来解决这个瓶颈，包括将探索函数和遗传函数分开，以及将BT-RvNN的构成方式改进。我们的策略可以将BT-RvNN的内存使用量降低至10-16倍，并且创建了新的最佳性能。此外，我们还提出了一个使用BT-RvNN生成的隐藏树内节表示来转换BT-RvNN从句子编码器的形式($f:\mathbb{R}^{n \times d} \rightarrow \mathbb{R}^{d}$)到序列内容调整器的形式($f:\mathbb{R}^{n \times d} \rightarrow \mathbb{R}^{n \times d}$)。因此，我们的提议不仅开启了RvNN的扩展之路，还标准化了使用BT-RvNN作为深度学习工具箱中的一个建立物件，可以轻松地堆叠或与其他受欢迎的模型相互作用，如Transformers和结构状态空间模型。”
</details></li>
</ul>
<hr>
<h2 id="Assessing-the-Use-of-AutoML-for-Data-Driven-Software-Engineering"><a href="#Assessing-the-Use-of-AutoML-for-Data-Driven-Software-Engineering" class="headerlink" title="Assessing the Use of AutoML for Data-Driven Software Engineering"></a>Assessing the Use of AutoML for Data-Driven Software Engineering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10774">http://arxiv.org/abs/2307.10774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabio Calefato, Luigi Quaranta, Filippo Lanubile, Marcos Kalinowski</li>
<li>for:  This paper aims to investigate the current state of adoption and perception of AutoML (Automated Machine Learning) in the software engineering (SE) community, and to provide insights for researchers and tool builders.</li>
<li>methods:  The study uses a mixed-method approach, including a benchmark of 12 end-to-end AutoML tools on two SE datasets and a user survey with follow-up interviews.</li>
<li>results:  The study found that AutoML solutions can generate models that outperform those trained and optimized by researchers for classification tasks in the SE domain, but the currently available solutions do not fully support automation across all stages of the ML development workflow and for all team members.<details>
<summary>Abstract</summary>
Background. Due to the widespread adoption of Artificial Intelligence (AI) and Machine Learning (ML) for building software applications, companies are struggling to recruit employees with a deep understanding of such technologies. In this scenario, AutoML is soaring as a promising solution to fill the AI/ML skills gap since it promises to automate the building of end-to-end AI/ML pipelines that would normally be engineered by specialized team members. Aims. Despite the growing interest and high expectations, there is a dearth of information about the extent to which AutoML is currently adopted by teams developing AI/ML-enabled systems and how it is perceived by practitioners and researchers. Method. To fill these gaps, in this paper, we present a mixed-method study comprising a benchmark of 12 end-to-end AutoML tools on two SE datasets and a user survey with follow-up interviews to further our understanding of AutoML adoption and perception. Results. We found that AutoML solutions can generate models that outperform those trained and optimized by researchers to perform classification tasks in the SE domain. Also, our findings show that the currently available AutoML solutions do not live up to their names as they do not equally support automation across the stages of the ML development workflow and for all the team members. Conclusions. We derive insights to inform the SE research community on how AutoML can facilitate their activities and tool builders on how to design the next generation of AutoML technologies.
</details>
<details>
<summary>摘要</summary>
背景：由于人工智能（AI）和机器学习（ML）在软件应用开发中的广泛应用，公司具有深入理解这些技术的员工困难招聘。在这种情况下，AutoML 出现为填充 AI/ML 技能差距的有望解决方案，因为它承诺自动化建立结束到结束的 AI/ML 管道，该管道通常由专门的团队成员工程。目标：尽管有增加的兴趣和高期望，但有关 AutoML 现在由团队开发 AI/ML 启用系统中的采用和评估的信息缺乏。方法：为了填补这些空白，本文发表了一项混合方法研究，包括12种端到端 AutoML 工具在两个 SE 数据集上的benchmark，以及与研究人员和实践者进行后续采访，以深入了解 AutoML 的采用和评估。结果：我们发现 AutoML 解决方案可以在 SE 领域中分类任务中出perform researchers所训练和优化的模型。此外，我们的发现表明目前可用的 AutoML 解决方案并不完全支持自动化 ML 开发工作流程中的所有阶段和所有团队成员。结论：我们得出的结论可以指导 SE 研究社区如何使用 AutoML，以及工具制造者如何设计下一代 AutoML 技术。
</details></li>
</ul>
<hr>
<h2 id="Music-Genre-Classification-with-ResNet-and-Bi-GRU-Using-Visual-Spectrograms"><a href="#Music-Genre-Classification-with-ResNet-and-Bi-GRU-Using-Visual-Spectrograms" class="headerlink" title="Music Genre Classification with ResNet and Bi-GRU Using Visual Spectrograms"></a>Music Genre Classification with ResNet and Bi-GRU Using Visual Spectrograms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10773">http://arxiv.org/abs/2307.10773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junfei Zhang</li>
<li>for: 提高音乐推荐系统的精度和用户体验，增强音乐流媒体服务的用户满意度。</li>
<li>methods: 提出了一种新的方法，使用视觉spectrogram作为输入，并结合了异常神经网络（ResNet）和门控回归单元（GRU）的hybrid模型，以更好地捕捉音乐数据的复杂性。</li>
<li>results: 通过对数据进行实验，研究人员发现该模型可以更好地捕捉音乐数据的特征，并且可以提高音乐推荐系统的精度和用户体验。<details>
<summary>Abstract</summary>
Music recommendation systems have emerged as a vital component to enhance user experience and satisfaction for the music streaming services, which dominates music consumption. The key challenge in improving these recommender systems lies in comprehending the complexity of music data, specifically for the underpinning music genre classification. The limitations of manual genre classification have highlighted the need for a more advanced system, namely the Automatic Music Genre Classification (AMGC) system. While traditional machine learning techniques have shown potential in genre classification, they heavily rely on manually engineered features and feature selection, failing to capture the full complexity of music data. On the other hand, deep learning classification architectures like the traditional Convolutional Neural Networks (CNN) are effective in capturing the spatial hierarchies but struggle to capture the temporal dynamics inherent in music data. To address these challenges, this study proposes a novel approach using visual spectrograms as input, and propose a hybrid model that combines the strength of the Residual neural Network (ResNet) and the Gated Recurrent Unit (GRU). This model is designed to provide a more comprehensive analysis of music data, offering the potential to improve the music recommender systems through achieving a more comprehensive analysis of music data and hence potentially more accurate genre classification.
</details>
<details>
<summary>摘要</summary>
音乐推荐系统已经成为现代音乐流媒体服务的关键组件，提高用户体验和满意度。然而，现有的音乐推荐系统受到了音乐数据的复杂性的限制，尤其是音乐类型分类的问题。传统的机器学习技术在音乐分类方面表现了潜力，但是它们需要手动设计特征和特征选择，无法捕捉音乐数据的全面性。相反，深度学习分类架构如传统的卷积神经网络（CNN）可以 capture音乐数据中的空间层次结构，但是它们在音乐数据中的时间动态特征上表现不佳。为了解决这些挑战，本研究提出了一种新的方法，使用视觉spectrogram作为输入，并提出了一种hybrid模型，结合了Residual神经网络（ResNet）和Gated Recurrent Unit（GRU）。这种模型可以为音乐数据提供更全面的分析，并且有 potential to improve音乐推荐系统的精度。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Emotions-from-EEG-A-GRU-Based-Approach"><a href="#Unveiling-Emotions-from-EEG-A-GRU-Based-Approach" class="headerlink" title="Unveiling Emotions from EEG: A GRU-Based Approach"></a>Unveiling Emotions from EEG: A GRU-Based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02778">http://arxiv.org/abs/2308.02778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarthak Johari, Gowri Namratha Meedinti, Radhakrishnan Delhibabu, Deepak Joshi</li>
<li>for: 这个研究旨在使用 EEG 数据进行情感识别，以测量人们在不同情感情况下的脑波活动。</li>
<li>methods: 我们使用 Gated Recurrent Unit (GRU) 算法，它是一种 Recurrent Neural Networks (RNNs)，将 EEG 数据进行预测。我们使用 artifact removal、bandpass filters 和 normalization 方法进行数据预处理。</li>
<li>results: 我们的模型在验证集上达到了 100% 的准确率，并且使用 GRU 的时间相依性能力实现了出色的结果。相比其他机器学习技术，我们的 GRU 模型的 Extreme Gradient Boosting Classifier 有最高的准确率。我们的调查显示了模型的混淆矩阵中的有用信息，帮助精确地分类情感。这个研究显示了深度学习模型如 GRU 在情感识别中的潜力，并且开启了新的可能性 для与电脑互动，以及理解脑波活动中的情感表达。<details>
<summary>Abstract</summary>
One of the most important study areas in affective computing is emotion identification using EEG data. In this study, the Gated Recurrent Unit (GRU) algorithm, which is a type of Recurrent Neural Networks (RNNs), is tested to see if it can use EEG signals to predict emotional states. Our publicly accessible dataset consists of resting neutral data as well as EEG recordings from people who were exposed to stimuli evoking happy, neutral, and negative emotions. For the best feature extraction, we pre-process the EEG data using artifact removal, bandpass filters, and normalization methods. With 100% accuracy on the validation set, our model produced outstanding results by utilizing the GRU's capacity to capture temporal dependencies. When compared to other machine learning techniques, our GRU model's Extreme Gradient Boosting Classifier had the highest accuracy. Our investigation of the confusion matrix revealed insightful information about the performance of the model, enabling precise emotion classification. This study emphasizes the potential of deep learning models like GRUs for emotion recognition and advances in affective computing. Our findings open up new possibilities for interacting with computers and comprehending how emotions are expressed through brainwave activity.
</details>
<details>
<summary>摘要</summary>
一个非常重要的研究领域之一是使用EEG数据进行情感识别，这个研究使用了Gated Recurrent Unit（GRU）算法，这是一种类型的逻辑神经网络（RNN）。我们的公共可访问数据集包括休息中中性数据以及由刺激诱发的幸福、中性和负面情感的EEG记录。为了取得最佳的特征提取，我们对EEG数据进行了遗产物除掉、频率筛选和normal化处理。我们的模型在验证集上达到100%的准确率，并且使用GRU捕捉时间相互关系的能力提供了杰出的结果。与其他机器学习技术相比，我们的GRU模型的Extreme Gradient Boosting Classifier（EGBC）准确率最高。我们对模型的冲击矩阵进行了调查，并发现了模型的性能信息，帮助精确地分类情感。这个研究强调了深度学习模型如GRU的情感识别潜力，并且对情绪计算的进步产生了新的可能性。我们的发现开发了与计算机交互和理解脑波活动表达的情感的新途径。
</details></li>
</ul>
<hr>
<h2 id="Decoding-the-Enigma-Benchmarking-Humans-and-AIs-on-the-Many-Facets-of-Working-Memory"><a href="#Decoding-the-Enigma-Benchmarking-Humans-and-AIs-on-the-Many-Facets-of-Working-Memory" class="headerlink" title="Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory"></a>Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10768">http://arxiv.org/abs/2307.10768</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhanglab-deepneurocoglab/worm">https://github.com/zhanglab-deepneurocoglab/worm</a></li>
<li>paper_authors: Ankur Sikarwar, Mengmi Zhang</li>
<li>for: 这个论文的目的是开发一个robust的工作记忆（WM）benchmark数据集，以便为AI WM模型的开发和评估提供一个标准化的框架。</li>
<li>methods: 这个研究使用了10个任务和100万个试验，评估了4种功能、3种领域和11种行为和神经特征的工作记忆。研究还使用了现有的循环神经网络和变换器来jointly训练和测试这些任务。</li>
<li>results: 研究发现AI模型可以模拟人类工作记忆中的一些特征，如 primacy 和 recency 效应，以及各个领域和功能的神经团和相关特征。然而，研究还发现现有的模型存在一些限制，无法完全 aproximate 人类行为。这个数据集可以作为跨学科的资源，用于比较和改进WM模型，调查WM的神经基础，以及开发人类样式的WM模型。<details>
<summary>Abstract</summary>
Working memory (WM), a fundamental cognitive process facilitating the temporary storage, integration, manipulation, and retrieval of information, plays a vital role in reasoning and decision-making tasks. Robust benchmark datasets that capture the multifaceted nature of WM are crucial for the effective development and evaluation of AI WM models. Here, we introduce a comprehensive Working Memory (WorM) benchmark dataset for this purpose. WorM comprises 10 tasks and a total of 1 million trials, assessing 4 functionalities, 3 domains, and 11 behavioral and neural characteristics of WM. We jointly trained and tested state-of-the-art recurrent neural networks and transformers on all these tasks. We also include human behavioral benchmarks as an upper bound for comparison. Our results suggest that AI models replicate some characteristics of WM in the brain, most notably primacy and recency effects, and neural clusters and correlates specialized for different domains and functionalities of WM. In the experiments, we also reveal some limitations in existing models to approximate human behavior. This dataset serves as a valuable resource for communities in cognitive psychology, neuroscience, and AI, offering a standardized framework to compare and enhance WM models, investigate WM's neural underpinnings, and develop WM models with human-like capabilities. Our source code and data are available at https://github.com/ZhangLab-DeepNeuroCogLab/WorM.
</details>
<details>
<summary>摘要</summary>
工作记忆（WM），一种基本的认知过程，为临时存储、集成、操作和检索信息提供了重要的支持。在理解和决策任务中，WM扮演着关键的角色。为了开发和评估人工智能WM模型，我们需要一些稳定的 Referenzdatenbank。这里，我们介绍了一个完整的Working Memory（WorM） Referenzdatenbank。WorM包含10个任务和总共100万个评估，评估了WM的4种功能、3种领域和11种行为和神经特征。我们将现有的回归神经网络和转换器在所有这些任务上进行了同时训练和测试。我们还包括了人类行为标准为参照。我们的结果表明，人工智能模型在脑中的WM特征上呈现了一些相似性，特别是首先和末则效果，以及各个领域和功能特征的神经团和相关特征。在实验中，我们还发现了现有模型的一些限制，无法完全模拟人类行为。这个数据集serve as a valuable resource for cognitive psychology, neuroscience and AI communities，提供了一个标准化的框架，用于比较和提高WM模型，调查WM的神经基础，并开发人类样的WM模型。我们的源代码和数据可以在https://github.com/ZhangLab-DeepNeuroCogLab/WorM中获取。
</details></li>
</ul>
<hr>
<h2 id="Actor-agnostic-Multi-label-Action-Recognition-with-Multi-modal-Query"><a href="#Actor-agnostic-Multi-label-Action-Recognition-with-Multi-modal-Query" class="headerlink" title="Actor-agnostic Multi-label Action Recognition with Multi-modal Query"></a>Actor-agnostic Multi-label Action Recognition with Multi-modal Query</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10763">http://arxiv.org/abs/2307.10763</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mondalanindya/msqnet">https://github.com/mondalanindya/msqnet</a></li>
<li>paper_authors: Anindya Mondal, Sauradip Nag, Joaquin M Prada, Xiatian Zhu, Anjan Dutta</li>
<li>for: 提高多种演员（包括人类和动物）动作识别精度和 universality。</li>
<li>methods: 提出了一种新的多模态多标签动作识别方法，利用视觉和文本模式来更好地表示动作类别，并消除了actor-specific模型设计，从而解决了actor pose estimation问题。</li>
<li>results: 在五个公开的 benchmark 上进行了广泛的实验，结果表明，我们的 MSQNet 在人类和动物单标和多标动作识别任务上比PRIOR ARTS actor-specific alternative 高出50%。<details>
<summary>Abstract</summary>
Existing action recognition methods are typically actor-specific due to the intrinsic topological and apparent differences among the actors. This requires actor-specific pose estimation (e.g., humans vs. animals), leading to cumbersome model design complexity and high maintenance costs. Moreover, they often focus on learning the visual modality alone and single-label classification whilst neglecting other available information sources (e.g., class name text) and the concurrent occurrence of multiple actions. To overcome these limitations, we propose a new approach called 'actor-agnostic multi-modal multi-label action recognition,' which offers a unified solution for various types of actors, including humans and animals. We further formulate a novel Multi-modal Semantic Query Network (MSQNet) model in a transformer-based object detection framework (e.g., DETR), characterized by leveraging visual and textual modalities to represent the action classes better. The elimination of actor-specific model designs is a key advantage, as it removes the need for actor pose estimation altogether. Extensive experiments on five publicly available benchmarks show that our MSQNet consistently outperforms the prior arts of actor-specific alternatives on human and animal single- and multi-label action recognition tasks by up to 50%. Code will be released at https://github.com/mondalanindya/MSQNet.
</details>
<details>
<summary>摘要</summary>
现有的动作认识方法通常是actor-specific的，这是因为actor之间存在内在的拓扑和外在的差异。这会导致actor-specific的姿势估计（例如人VS动物），从而增加模型设计复杂度和维护成本。另外，它们通常会专注于学习视觉模式alone和单个标签分类，而忽略其他可用的信息源（例如类名文本）以及同时发生的多个动作。为了解决这些限制，我们提出了一种新的方法called 'actor-agnostic multi-modal multi-label action recognition，' which offers a unified solution for various types of actors, including humans and animals.我们进一步提出了一种novel Multi-modal Semantic Query Network (MSQNet)模型，这个模型在基于 transformer 的对象检测框架（例如 DETR）中，利用视觉和文本模式来更好地表示动作类。消除actor-specific模型设计是MSQNet的关键优势，这将取消actor pose estimation的需求。我们的实验结果表明，MSQNet在五个公共的 benchmark 上 consistently outperformsactor-specific alternatives on human and animal single- and multi-label action recognition tasks by up to 50%. Code will be released at https://github.com/mondalanindya/MSQNet.
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Voter-Attribute-Bias-for-Fair-Opinion-Aggregation"><a href="#Mitigating-Voter-Attribute-Bias-for-Fair-Opinion-Aggregation" class="headerlink" title="Mitigating Voter Attribute Bias for Fair Opinion Aggregation"></a>Mitigating Voter Attribute Bias for Fair Opinion Aggregation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10749">http://arxiv.org/abs/2307.10749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryosuke Ueda, Koh Takeuchi, Hisashi Kashima<br>for:This paper focuses on achieving fair opinion aggregation in decision-making tasks, such as hiring and loan review, where disagreements may occur and voter attributes like gender or race may introduce bias.methods:The paper proposes a combination of opinion aggregation models like majority voting and the Dawid and Skene model (D&amp;S model) with fairness options like sample weighting to achieve fair aggregation results. The authors also use probabilistic soft labels to evaluate the fairness of opinion aggregation.results:The experimental results show that the combination of Soft D&amp;S and data splitting as a fairness option is effective for dense data, while weighted majority voting is effective for sparse data. These findings can support decision-making by human and machine-learning models with balanced opinion aggregation.<details>
<summary>Abstract</summary>
The aggregation of multiple opinions plays a crucial role in decision-making, such as in hiring and loan review, and in labeling data for supervised learning. Although majority voting and existing opinion aggregation models are effective for simple tasks, they are inappropriate for tasks without objectively true labels in which disagreements may occur. In particular, when voter attributes such as gender or race introduce bias into opinions, the aggregation results may vary depending on the composition of voter attributes. A balanced group of voters is desirable for fair aggregation results but may be difficult to prepare. In this study, we consider methods to achieve fair opinion aggregation based on voter attributes and evaluate the fairness of the aggregated results. To this end, we consider an approach that combines opinion aggregation models such as majority voting and the Dawid and Skene model (D&S model) with fairness options such as sample weighting. To evaluate the fairness of opinion aggregation, probabilistic soft labels are preferred over discrete class labels. First, we address the problem of soft label estimation without considering voter attributes and identify some issues with the D&S model. To address these limitations, we propose a new Soft D&S model with improved accuracy in estimating soft labels. Moreover, we evaluated the fairness of an opinion aggregation model, including Soft D&S, in combination with different fairness options using synthetic and semi-synthetic data. The experimental results suggest that the combination of Soft D&S and data splitting as a fairness option is effective for dense data, whereas weighted majority voting is effective for sparse data. These findings should prove particularly valuable in supporting decision-making by human and machine-learning models with balanced opinion aggregation.
</details>
<details>
<summary>摘要</summary>
多种意见的总和在决策中发挥关键作用，如在招聘和贷款审核中，以及在supervised learning中标注数据。虽然多数投票和现有的意见总和模型对简单任务有效，但对无 objetively true标签的任务而言，它们不适用。特别是当选民特征如性别或种族引入偏见到意见中，总和结果可能因选民特征组合而异常。一个均衡的选民组合是 désirable для公平的总和结果，但可能困难实现。在这种研究中，我们考虑了基于选民特征的公平意见总和方法，并评估了总和结果的公平性。为此，我们考虑了意见总和模型如多数投票和达韦德和斯金（D&S）模型，并与公平选项如样本权重相结合。为评估意见总和的公平性，我们偏好使用概率软标签而不是精确的分类标签。首先，我们解决了不考虑选民特征时的软标签估计问题，并指出了D&S模型的一些限制。为解决这些限制，我们提出了一个新的软D&S模型，具有提高软标签估计精度的优点。此外，我们通过使用 sintética y semi-sintética数据进行了对意见总和模型、包括软D&S模型，以及不同公平选项的评估。实验结果表明，将软D&S模型与数据分割作为公平选项相结合，对于稠密数据是有效的，而Weighted Majority Voting对于稀疏数据是有效的。这些发现应该对人类和机器学习模型的均衡意见总和做出重要贡献。
</details></li>
</ul>
<hr>
<h2 id="Fairness-Aware-Client-Selection-for-Federated-Learning"><a href="#Fairness-Aware-Client-Selection-for-Federated-Learning" class="headerlink" title="Fairness-Aware Client Selection for Federated Learning"></a>Fairness-Aware Client Selection for Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10738">http://arxiv.org/abs/2307.10738</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxin Shi, Zelei Liu, Zhuan Shi, Han Yu</li>
<li>for: 这篇论文的目的是提出了一个名为“ Fairness-aware Federated Client Selection”的方法，用于在多个数据所有者（FL客户）合作训练机器学习模型时，对 FL 客户进行选择，以确保客户的公平对待。</li>
<li>methods: 这篇论文使用了 Lyapunov 优化方法，将 FL 客户的选择概率静态地调整，基于客户的声誉、参与 FL 任务的次数和对模型性能的贡献。这个方法不使用阈值基于声誉的筛选，因此为 FL 客户提供了重新证明自己的机会，进一步增强客户的公平对待。</li>
<li>results: 实验结果显示，这篇论文的 FairFedCS 方法在真实世界的多媒体数据集上，实现了19.6%的公平性和0.73%的测试准确率的提升，比最佳现有方法的平均提升率高出19.6%和0.73%。<details>
<summary>Abstract</summary>
Federated learning (FL) has enabled multiple data owners (a.k.a. FL clients) to train machine learning models collaboratively without revealing private data. Since the FL server can only engage a limited number of clients in each training round, FL client selection has become an important research problem. Existing approaches generally focus on either enhancing FL model performance or enhancing the fair treatment of FL clients. The problem of balancing performance and fairness considerations when selecting FL clients remains open. To address this problem, we propose the Fairness-aware Federated Client Selection (FairFedCS) approach. Based on Lyapunov optimization, it dynamically adjusts FL clients' selection probabilities by jointly considering their reputations, times of participation in FL tasks and contributions to the resulting model performance. By not using threshold-based reputation filtering, it provides FL clients with opportunities to redeem their reputations after a perceived poor performance, thereby further enhancing fair client treatment. Extensive experiments based on real-world multimedia datasets show that FairFedCS achieves 19.6% higher fairness and 0.73% higher test accuracy on average than the best-performing state-of-the-art approach.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Long-Tail-Theory-under-Gaussian-Mixtures"><a href="#Long-Tail-Theory-under-Gaussian-Mixtures" class="headerlink" title="Long-Tail Theory under Gaussian Mixtures"></a>Long-Tail Theory under Gaussian Mixtures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10736">http://arxiv.org/abs/2307.10736</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/armanbolatov/long_tail">https://github.com/armanbolatov/long_tail</a></li>
<li>paper_authors: Arman Bolatov, Maxat Tezekbayev, Igor Melnykov, Artur Pak, Vassilina Nikoulina, Zhenisbek Assylbekov</li>
<li>for:  validate Feldman’s long tail theory and explore the effect of nonlinear models on generalization error in long-tailed distributions</li>
<li>methods: propose a simple Gaussian mixture model and compare the performance of linear and nonlinear classifiers</li>
<li>results: demonstrate that nonlinear classifiers with memorization capacity can better handle long-tailed distributions, and the performance gap between linear and nonlinear models decreases as the tail becomes shorter.<details>
<summary>Abstract</summary>
We suggest a simple Gaussian mixture model for data generation that complies with Feldman's long tail theory (2020). We demonstrate that a linear classifier cannot decrease the generalization error below a certain level in the proposed model, whereas a nonlinear classifier with a memorization capacity can. This confirms that for long-tailed distributions, rare training examples must be considered for optimal generalization to new data. Finally, we show that the performance gap between linear and nonlinear models can be lessened as the tail becomes shorter in the subpopulation frequency distribution, as confirmed by experiments on synthetic and real data.
</details>
<details>
<summary>摘要</summary>
我们建议一个简单的 Gaussian 混合模型来生成数据，遵循 Feldman 的长尾理论（2020）。我们示出了一个线性分类器无法在我们提案的模型中降低泛化错误下限，而一个具有记忆容量的非线性分类器则可以。这证明了在长尾分布中，罕见的训练数据必须被考虑，以获得新数据的最佳泛化。最后，我们显示了在尾部变短的情况下，线性和非线性模型之间的性能差距可以降低，经过实验证明。Note: "简单" (jiǎn simple) in Simplified Chinese means "simple" in English.
</details></li>
</ul>
<hr>
<h2 id="Comparison-between-transformers-and-convolutional-models-for-fine-grained-classification-of-insects"><a href="#Comparison-between-transformers-and-convolutional-models-for-fine-grained-classification-of-insects" class="headerlink" title="Comparison between transformers and convolutional models for fine-grained classification of insects"></a>Comparison between transformers and convolutional models for fine-grained classification of insects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11112">http://arxiv.org/abs/2307.11112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rita Pucci, Vincent J. Kalkman, Dan Stowell</li>
<li>for: 本研究的目的是提高昆虫种的自动分类精度，以便更好地监测生态系统中的昆虫种类。</li>
<li>methods: 本研究使用了深度学习算法，主要是transformer和卷积层结构，对 Odonata 和 Coleoptera 两个目录进行比较研究，以确定最佳的算法。</li>
<li>results: 研究发现，混合模型在精度和执行速度两个方面都表现出色，而transformer层结构在缺乏样本时表现更加稳定，具有更快的执行速度。<details>
<summary>Abstract</summary>
Fine-grained classification is challenging due to the difficulty of finding discriminatory features. This problem is exacerbated when applied to identifying species within the same taxonomical class. This is because species are often sharing morphological characteristics that make them difficult to differentiate. We consider the taxonomical class of Insecta. The identification of insects is essential in biodiversity monitoring as they are one of the inhabitants at the base of many ecosystems. Citizen science is doing brilliant work of collecting images of insects in the wild giving the possibility to experts to create improved distribution maps in all countries. We have billions of images that need to be automatically classified and deep neural network algorithms are one of the main techniques explored for fine-grained tasks. At the SOTA, the field of deep learning algorithms is extremely fruitful, so how to identify the algorithm to use? We focus on Odonata and Coleoptera orders, and we propose an initial comparative study to analyse the two best-known layer structures for computer vision: transformer and convolutional layers. We compare the performance of T2TViT, a fully transformer-base, EfficientNet, a fully convolutional-base, and ViTAE, a hybrid. We analyse the performance of the three models in identical conditions evaluating the performance per species, per morph together with sex, the inference time, and the overall performance with unbalanced datasets of images from smartphones. Although we observe high performances with all three families of models, our analysis shows that the hybrid model outperforms the fully convolutional-base and fully transformer-base models on accuracy performance and the fully transformer-base model outperforms the others on inference speed and, these prove the transformer to be robust to the shortage of samples and to be faster at inference time.
</details>
<details>
<summary>摘要</summary>
细见分类是因为找到特征分化的困难，这个问题在种类同一类型时更加加剧。这是因为种类通常在同一类型中共享形态特征，使其很难分辨。我们考虑了昆虫纲（Insecta）的分类。 insects 是生态系统的基础居民，其分类是生态监测中非常重要的。公民科学在野外采集到了大量昆虫图像，这给专家提供了创建改进的分布图的机会。我们有 billions 的图像需要自动分类，深度学习算法是细见任务中的主要技术之一。在 SOTA 中，深度学习领域非常肥沃，因此如何选择算法？我们关注 Odonata 和 Coleoptera 两个类别，并提出了一项初步比较研究，以分析两种最为知名的计算机视觉层结构：变换层和卷积层。我们比较了 T2TViT、EfficientNet 和 ViTAE 三种模型的性能，并分析了每种模型在同一种条件下的性能，包括每种物种、每个形态、性别、推理时间和总性能。虽然我们所观察到的性能都很高，但我们的分析表明，混合模型在精度性能和推理速度方面都高于完全卷积基础和完全变换基础模型，而变换模型在精度性能方面表现出色。
</details></li>
</ul>
<hr>
<h2 id="LLM-Censorship-A-Machine-Learning-Challenge-or-a-Computer-Security-Problem"><a href="#LLM-Censorship-A-Machine-Learning-Challenge-or-a-Computer-Security-Problem" class="headerlink" title="LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?"></a>LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10719">http://arxiv.org/abs/2307.10719</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Glukhov, Ilia Shumailov, Yarin Gal, Nicolas Papernot, Vardan Papyan</li>
<li>for: 防止大语言模型（LLM）的黑客用途</li>
<li>methods: 检测LLM输出中不良内容的方法</li>
<li>results: 检测方法存在限制和攻击者可以重构禁止的输出<details>
<summary>Abstract</summary>
Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, as LLMs can still generate problematic responses. Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs. In this paper, we present the theoretical limitations of such semantic censorship approaches. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities. Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones. As a result, we propose that the problem of censorship needs to be reevaluated; it should be treated as a security problem which warrants the adaptation of security-based approaches to mitigate potential risks.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经展现出印象深刻的能力，但它们的盲目遵循提供的指令却引起了关于恶意使用的风险。现有的防御机制，如模型精致化或使用 LLM 进行出力审查，已经证明是不可靠的，因为 LLM 仍然可以产生问题的回应。常用的审查方法将这问题视为机器学习的问题，并且靠另一个 LM 检测恶意内容在 LLM 的出力中。在这篇论文中，我们展示了对于这种Semantic审查的理论限制。具体来说，我们显示了Semantic审查是一个不可解决的问题，highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities。此外，我们认为这些挑战不仅限于Semantic审查，因为知识分子可以从 permissible 的出力中重建不被允许的出力。因此，我们建议将问题重新评估为安全问题，并且适用安全性基础的方法来减轻潜在的风险。
</details></li>
</ul>
<hr>
<h2 id="Differences-Between-Hard-and-Noisy-labeled-Samples-An-Empirical-Study"><a href="#Differences-Between-Hard-and-Noisy-labeled-Samples-An-Empirical-Study" class="headerlink" title="Differences Between Hard and Noisy-labeled Samples: An Empirical Study"></a>Differences Between Hard and Noisy-labeled Samples: An Empirical Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10718">http://arxiv.org/abs/2307.10718</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mahf93/hard-vs-noisy">https://github.com/mahf93/hard-vs-noisy</a></li>
<li>paper_authors: Mahsa Forouzesh, Patrick Thiran</li>
<li>for: 本研究旨在解决难以学习的样本和涂改标签的问题，这两者在现有的方法中通常是独立地处理的，但是当两者同时存在时，大多数现有方法会导致模型的总性能下降。</li>
<li>methods: 我们首先设计了多种Synthetic Dataset，以控制样本的困难和涂改标签水平。我们的系统性的实验研究帮助我们更好地理解难以学习样本和涂改标签之间的相似性和差异。这些控制的实验为开发能够分辨难以学习和涂改标签的方法提供了基础。</li>
<li>results: 我们提出了一个简单 yet effective的度量，可以过滤掉涂改标签的样本，保留难以学习的样本。我们研究了多种数据分割方法在实际涂改标签的情况下的性能，并发现我们的提议的数据分割方法在 semi-supervised learning 框架中表现出色。<details>
<summary>Abstract</summary>
Extracting noisy or incorrectly labeled samples from a labeled dataset with hard/difficult samples is an important yet under-explored topic. Two general and often independent lines of work exist, one focuses on addressing noisy labels, and another deals with hard samples. However, when both types of data are present, most existing methods treat them equally, which results in a decline in the overall performance of the model. In this paper, we first design various synthetic datasets with custom hardness and noisiness levels for different samples. Our proposed systematic empirical study enables us to better understand the similarities and more importantly the differences between hard-to-learn samples and incorrectly-labeled samples. These controlled experiments pave the way for the development of methods that distinguish between hard and noisy samples. Through our study, we introduce a simple yet effective metric that filters out noisy-labeled samples while keeping the hard samples. We study various data partitioning methods in the presence of label noise and observe that filtering out noisy samples from hard samples with this proposed metric results in the best datasets as evidenced by the high test accuracy achieved after models are trained on the filtered datasets. We demonstrate this for both our created synthetic datasets and for datasets with real-world label noise. Furthermore, our proposed data partitioning method significantly outperforms other methods when employed within a semi-supervised learning framework.
</details>
<details>
<summary>摘要</summary>
<<SYS>> transtable id="simplified-chinese" /</SYS>>原文：抽取含有噪音或错误标签的样本从标注数据集中是一个重要又受到忽视的话题。现有两种常见且独立的方法来处理这些样本，一种是处理噪音标签，另一种是处理困难样本。然而，当这两种样本同时存在时，大多数现有方法对它们进行平等待遇，这会导致模型的总性能下降。在这篇论文中，我们首先设计了不同困难和噪音水平的synthetic数据集。我们的提议的系统性的实验研究可以更好地了解困难样本和错误标签样本之间的相似性和区别。这些控制的实验为开发可以分辨困难和噪音样本的方法提供了基础。我们介绍了一个简单 yet effective的度量，可以从噪音标签样本中筛选出噪音样本，保留困难样本。我们研究了在噪音标签存在的情况下不同的数据分割方法，并发现使用我们提议的度量筛选困难样本后的 filtered datasets 可以获得最高的测试精度。我们在 synthetic 数据集和实际世界中的噪音标签数据集上进行了实验，并证明了我们的方法的优势。此外，我们的提议的数据分割方法在 semi-supervised 学习框架中表现了出色。
</details></li>
</ul>
<hr>
<h2 id="AdjointDPM-Adjoint-Sensitivity-Method-for-Gradient-Backpropagation-of-Diffusion-Probabilistic-Models"><a href="#AdjointDPM-Adjoint-Sensitivity-Method-for-Gradient-Backpropagation-of-Diffusion-Probabilistic-Models" class="headerlink" title="AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models"></a>AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10711">http://arxiv.org/abs/2307.10711</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiachun Pan, Jun Hao Liew, Vincent Y. F. Tan, Jiashi Feng, Hanshu Yan</li>
<li>For: This paper aims to address the challenge of customizing diffusion probabilistic models (DPMs) when the only available supervision is a differentiable metric defined on the generated contents.* Methods: The proposed method, AdjointDPM, first generates new samples from diffusion models by solving the corresponding probability-flow ODEs. It then uses the adjoint sensitivity method to backpropagate the gradients of the loss to the models’ parameters (including conditioning signals, network weights, and initial noises) by solving another augmented ODE.* Results: The proposed method is demonstrated to be effective on three interesting tasks: converting visual effects into identification text embeddings, finetuning DPMs for specific types of stylization, and optimizing initial noise to generate adversarial samples for security auditing.<details>
<summary>Abstract</summary>
Existing customization methods require access to multiple reference examples to align pre-trained diffusion probabilistic models (DPMs) with user-provided concepts. This paper aims to address the challenge of DPM customization when the only available supervision is a differentiable metric defined on the generated contents. Since the sampling procedure of DPMs involves recursive calls to the denoising UNet, na\"ive gradient backpropagation requires storing the intermediate states of all iterations, resulting in extremely high memory consumption. To overcome this issue, we propose a novel method AdjointDPM, which first generates new samples from diffusion models by solving the corresponding probability-flow ODEs. It then uses the adjoint sensitivity method to backpropagate the gradients of the loss to the models' parameters (including conditioning signals, network weights, and initial noises) by solving another augmented ODE. To reduce numerical errors in both the forward generation and gradient backpropagation processes, we further reparameterize the probability-flow ODE and augmented ODE as simple non-stiff ODEs using exponential integration. Finally, we demonstrate the effectiveness of AdjointDPM on three interesting tasks: converting visual effects into identification text embeddings, finetuning DPMs for specific types of stylization, and optimizing initial noise to generate adversarial samples for security auditing.
</details>
<details>
<summary>摘要</summary>
现有的自定义方法需要访问多个参考示例，以对用户提供的概念与预训练的扩散概率模型（DPM）进行对接。本文目的是解决DPM自定义时，只有用户提供的可微分度量的概念来支持的挑战。由于扩散模型的采样过程包括 recursive calls to denoising UNet，直观梯度反推需要存储所有迭代的状态，从而导致极高的内存消耗。为解决这个问题，我们提出了一种新的方法 called AdjointDPM。 AdjointDPM首先通过解决相应的概率流ODE来生成新的样本。然后，它使用梯度敏感法来反推模型参数（包括条件信号、网络参数和初始噪声）的梯度。为了降低数值计算中的numerical errors，我们进一步折衔概率流ODE和扩展ODE为简单的非稍难ODE使用快速积分。最后，我们证明了AdjointDPM在三个有趣的任务中的效果：将视觉特效转换为标识符文本嵌入，Finetune DPMs для特定类型的风格化，以及优化初始噪声来生成安全审核中的对抗样本。
</details></li>
</ul>
<hr>
<h2 id="Reparameterized-Policy-Learning-for-Multimodal-Trajectory-Optimization"><a href="#Reparameterized-Policy-Learning-for-Multimodal-Trajectory-Optimization" class="headerlink" title="Reparameterized Policy Learning for Multimodal Trajectory Optimization"></a>Reparameterized Policy Learning for Multimodal Trajectory Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10710">http://arxiv.org/abs/2307.10710</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haosulab/RPG">https://github.com/haosulab/RPG</a></li>
<li>paper_authors: Zhiao Huang, Litian Liang, Zhan Ling, Xuanlin Li, Chuang Gan, Hao Su</li>
<li>for: 本文目的是解决RL中高维动作空间中参数策略的挑战。</li>
<li>methods: 本文提出了一种基于生成模型的策略参数化方法，通过conditioningPolicy来Derive一种新的可变 bounds，以便优化搜索环境。</li>
<li>results: 实验结果表明，我们的方法可以帮助机器人避免环境中的局部优化，并在 tasks with dense rewards 和 sparse-reward environments 中解决挑战。我们的方法比前一些方法表现更好，可以减少数据的使用。代码和补充材料可以在项目页面<a target="_blank" rel="noopener" href="https://haosulab.github.io/RPG/">https://haosulab.github.io/RPG/</a> 中找到。<details>
<summary>Abstract</summary>
We investigate the challenge of parametrizing policies for reinforcement learning (RL) in high-dimensional continuous action spaces. Our objective is to develop a multimodal policy that overcomes limitations inherent in the commonly-used Gaussian parameterization. To achieve this, we propose a principled framework that models the continuous RL policy as a generative model of optimal trajectories. By conditioning the policy on a latent variable, we derive a novel variational bound as the optimization objective, which promotes exploration of the environment. We then present a practical model-based RL method, called Reparameterized Policy Gradient (RPG), which leverages the multimodal policy parameterization and learned world model to achieve strong exploration capabilities and high data efficiency. Empirical results demonstrate that our method can help agents evade local optima in tasks with dense rewards and solve challenging sparse-reward environments by incorporating an object-centric intrinsic reward. Our method consistently outperforms previous approaches across a range of tasks. Code and supplementary materials are available on the project page https://haosulab.github.io/RPG/
</details>
<details>
<summary>摘要</summary>
我团队正在研究重点参数化策略的挑战，以便在高维连续动作空间中进行学习反馈。我们的目标是开发一种多模态策略，以超越通常使用的 Gaussian 参数化的限制。为此，我们提议一种原理性的框架，即将连续RL策略视为优质轨迹的生成模型。通过conditioning策略于隐藏变量，我们得到了一种新的可变约束，该约束促进环境的探索。然后，我们提出了一种实用的基于模型的RL方法，即Reparameterized Policy Gradient（RPG），该方法利用多模态策略参数化和学习到的世界模型，以实现强大的探索能力和高数据效率。我们的方法在多种任务中均能够超越先前的方法，并且在 dense 奖励和稀缺奖励环境中解决了本地极点问题。我们的方法可以在多种任务中均能够具有高效率和优秀的探索能力。详细的代码和补充材料可以在项目页面（https://haosulab.github.io/RPG/）上找到。
</details></li>
</ul>
<hr>
<h2 id="TwinLiteNet-An-Efficient-and-Lightweight-Model-for-Driveable-Area-and-Lane-Segmentation-in-Self-Driving-Cars"><a href="#TwinLiteNet-An-Efficient-and-Lightweight-Model-for-Driveable-Area-and-Lane-Segmentation-in-Self-Driving-Cars" class="headerlink" title="TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and Lane Segmentation in Self-Driving Cars"></a>TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and Lane Segmentation in Self-Driving Cars</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10705">http://arxiv.org/abs/2307.10705</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chequanghuy/TwinLiteNet">https://github.com/chequanghuy/TwinLiteNet</a></li>
<li>paper_authors: Quang Huy Che, Dinh Phuc Nguyen, Minh Quan Pham, Duc Khai Lam</li>
<li>for: 这篇论文主要关注于自动驾驶车辆中的内部环境理解，特别是driveable area和lane line分类。</li>
<li>methods: 这篇论文提出了一个轻量级的模型，用于实时验证driveable area和lane line。模型称为TwinLiteNet，它仅需0.4亿个参数，并可以在GPU RTX A5000上实现415 FPS的运算速度。</li>
<li>results: 根据论文的实验结果，TwinLiteNet在BDD100K数据集上实现了91.3%的mIoU分类率 для driveable area任务，并且在Lane Detection任务上实现了31.08%的IoU分类率。此外，TwinLiteNet可以在内置式计算机上实现60 FPS的实时运算速度，特别是在Jetson Xavier NX上实现415 FPS，这使得它成为了自动驾驶车辆中的理想解决方案。<details>
<summary>Abstract</summary>
Semantic segmentation is a common task in autonomous driving to understand the surrounding environment. Driveable Area Segmentation and Lane Detection are particularly important for safe and efficient navigation on the road. However, original semantic segmentation models are computationally expensive and require high-end hardware, which is not feasible for embedded systems in autonomous vehicles. This paper proposes a lightweight model for the driveable area and lane line segmentation. TwinLiteNet is designed cheaply but achieves accurate and efficient segmentation results. We evaluate TwinLiteNet on the BDD100K dataset and compare it with modern models. Experimental results show that our TwinLiteNet performs similarly to existing approaches, requiring significantly fewer computational resources. Specifically, TwinLiteNet achieves a mIoU score of 91.3% for the Drivable Area task and 31.08% IoU for the Lane Detection task with only 0.4 million parameters and achieves 415 FPS on GPU RTX A5000. Furthermore, TwinLiteNet can run in real-time on embedded devices with limited computing power, especially since it achieves 60FPS on Jetson Xavier NX, making it an ideal solution for self-driving vehicles. Code is available: url{https://github.com/chequanghuy/TwinLiteNet}.
</details>
<details>
<summary>摘要</summary>
Semantic segmentation 是自动驾驶中常见的任务，用于理解周围环境。驱动区域分割和车道检测是安全和效率导航的关键，但原始 semantic segmentation 模型 computationally 昂贵，需要高端硬件，不太适合自动驾驶车辆中的嵌入式系统。这篇论文提出了一种轻量级模型，用于驱动区域和车道分割。TwinLiteNet 是一种低成本的设计，却可以实现高精度和高效的分割结果。我们在 BDD100K 数据集上评估 TwinLiteNet，并与现代模型进行比较。实验结果表明，我们的 TwinLiteNet 与现有方法相似，却需要 significatively  fewer 计算资源。具体来说，TwinLiteNet 在 Drivable Area 任务上取得了 91.3% 的 mIoU 分数，在 Lane Detection 任务上取得了 31.08% 的 IoU 分数，只需 0.4 万个参数，并在 GPU RTX A5000 上实现了 415 FPS。此外，TwinLiteNet 可以在具有有限计算能力的嵌入式设备上运行，尤其是在 Jetson Xavier NX 上实现了 60 FPS，使其成为自动驾驶车辆的理想解决方案。代码可以在以下链接中找到：<https://github.com/chequanghuy/TwinLiteNet>。
</details></li>
</ul>
<hr>
<h2 id="Decentralized-Smart-Charging-of-Large-Scale-EVs-using-Adaptive-Multi-Agent-Multi-Armed-Bandits"><a href="#Decentralized-Smart-Charging-of-Large-Scale-EVs-using-Adaptive-Multi-Agent-Multi-Armed-Bandits" class="headerlink" title="Decentralized Smart Charging of Large-Scale EVs using Adaptive Multi-Agent Multi-Armed Bandits"></a>Decentralized Smart Charging of Large-Scale EVs using Adaptive Multi-Agent Multi-Armed Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10704">http://arxiv.org/abs/2307.10704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sharyal Zafar, Raphaël Feraud, Anne Blavette, Guy Camilleri, Hamid Ben</li>
<li>for: 提高电动车和太阳能系统的灵活性和可扩展性，并处理峰值负荷和电压限制问题</li>
<li>methods: 使用自适应多代理系统理论，利用多臂炮学习处理系统不确定性</li>
<li>results: 提出了一种基于分布式智能充电系统，具有扩展性、实时性、无模型、均衡多个参与者的特点，并进行了详细的案例研究以评估性能<details>
<summary>Abstract</summary>
The drastic growth of electric vehicles and photovoltaics can introduce new challenges, such as electrical current congestion and voltage limit violations due to peak load demands. These issues can be mitigated by controlling the operation of electric vehicles i.e., smart charging. Centralized smart charging solutions have already been proposed in the literature. But such solutions may lack scalability and suffer from inherent drawbacks of centralization, such as a single point of failure, and data privacy concerns. Decentralization can help tackle these challenges. In this paper, a fully decentralized smart charging system is proposed using the philosophy of adaptive multi-agent systems. The proposed system utilizes multi-armed bandit learning to handle uncertainties in the system. The presented system is decentralized, scalable, real-time, model-free, and takes fairness among different players into account. A detailed case study is also presented for performance evaluation.
</details>
<details>
<summary>摘要</summary>
“电动车和太阳能系统的急速增长可能会带来新的挑战，例如电力流量拥堵和电压限制违规因为峰值负载。这些问题可以通过控制电动车的运作来缓解，例如聪明充电。中央化聪明充电解决方案已经在文献中提出，但这些解决方案可能缺乏扩展性和中央化的缺点，例如单一点故障和数据隐私问题。分散化可以帮助解决这些挑战。本文提出了一个完全分散式聪明充电系统，使用了适应多智能体系统的哲学。提案的系统利用多臂帮手学来处理系统中的不确定性。提出的系统是分散式、扩展性、实时、无模型、以及对不同玩家的公平性into account。另外，一个详细的实际案例也是提供了性能评估。”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Graphs-in-State-Space-Models-for-Granger-Causality-in-Climate-Science"><a href="#Graphs-in-State-Space-Models-for-Granger-Causality-in-Climate-Science" class="headerlink" title="Graphs in State-Space Models for Granger Causality in Climate Science"></a>Graphs in State-Space Models for Granger Causality in Climate Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10703">http://arxiv.org/abs/2307.10703</a></li>
<li>repo_url: None</li>
<li>paper_authors: Víctor Elvira, Émilie Chouzenoux, Jordi Cerdà, Gustau Camps-Valls</li>
<li>for: 本研究探讨了Granger causality（GC）是否真实表示 causality，同时提出了一种基于图模型的GC测试方法。</li>
<li>methods: 本研究使用了GraphEM算法，一种基于期望最大化的方法，来估计线性 Gaussian 状态方程中的线性矩阵运算员。具有lasso regularization的M-step使用了一种 proximal splitting Douglas-Rachford 算法来解决。</li>
<li>results: 对于具体的示例和挑战性气候问题，提出的模型和推断方法在标准GC方法上表现出了优势。<details>
<summary>Abstract</summary>
Granger causality (GC) is often considered not an actual form of causality. Still, it is arguably the most widely used method to assess the predictability of a time series from another one. Granger causality has been widely used in many applied disciplines, from neuroscience and econometrics to Earth sciences. We revisit GC under a graphical perspective of state-space models. For that, we use GraphEM, a recently presented expectation-maximisation algorithm for estimating the linear matrix operator in the state equation of a linear-Gaussian state-space model. Lasso regularisation is included in the M-step, which is solved using a proximal splitting Douglas-Rachford algorithm. Experiments in toy examples and challenging climate problems illustrate the benefits of the proposed model and inference technique over standard Granger causality methods.
</details>
<details>
<summary>摘要</summary>
格兰治 causality（GC）经常被视为不是真正的 causality，但它仍然是最广泛使用的方法来评估时间序列之间的预测性。格兰治 causality 在许多应用领域中广泛使用，从神经科学和 econometrics 到地球科学。我们在图表视图下重新审视 GC。为此，我们使用 GraphEM，一种最近提出的预期-最大化算法来估计线性矩阵运算符在状态方程中的线性-加比分型模型中的状态方程。lasso 规范化包含在 M-步中，通过 proximal splitting Douglas-Rachford 算法解决。在做品例和挑战性气候问题中进行实验，我们发现提议的模型和推理技术在标准 Granger causality 方法的基础上具有优势。
</details></li>
</ul>
<hr>
<h2 id="Self2Self-Single-Image-Denoising-with-Self-Supervised-Learning-and-Image-Quality-Assessment-Loss"><a href="#Self2Self-Single-Image-Denoising-with-Self-Supervised-Learning-and-Image-Quality-Assessment-Loss" class="headerlink" title="Self2Self+: Single-Image Denoising with Self-Supervised Learning and Image Quality Assessment Loss"></a>Self2Self+: Single-Image Denoising with Self-Supervised Learning and Image Quality Assessment Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10695">http://arxiv.org/abs/2307.10695</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JK-the-Ko/Self2SelfPlus">https://github.com/JK-the-Ko/Self2SelfPlus</a></li>
<li>paper_authors: Jaekyun Ko, Sanghwan Lee</li>
<li>for: 用于降噪处理</li>
<li>methods: 使用单图自我监督学习方法，即使用含有噪声的输入图像进行网络训练，并使用阻塞卷积和无参照质量评估来导航训练过程。</li>
<li>results: 实验结果表明，提议的方法可以在synthetic和实际 dataset上达到降噪性能的国际先进水平，这说明了该方法的实用性和可行性。<details>
<summary>Abstract</summary>
Recently, denoising methods based on supervised learning have exhibited promising performance. However, their reliance on external datasets containing noisy-clean image pairs restricts their applicability. To address this limitation, researchers have focused on training denoising networks using solely a set of noisy inputs. To improve the feasibility of denoising procedures, in this study, we proposed a single-image self-supervised learning method in which only the noisy input image is used for network training. Gated convolution was used for feature extraction and no-reference image quality assessment was used for guiding the training process. Moreover, the proposed method sampled instances from the input image dataset using Bernoulli sampling with a certain dropout rate for training. The corresponding result was produced by averaging the generated predictions from various instances of the trained network with dropouts. The experimental results indicated that the proposed method achieved state-of-the-art denoising performance on both synthetic and real-world datasets. This highlights the effectiveness and practicality of our method as a potential solution for various noise removal tasks.
</details>
<details>
<summary>摘要</summary>
在本研究中，我们提出了基于单个噪声输入图像的自动学习方法。我们使用了阀门卷积来提取特征，并使用无参图像质量评估来引导训练过程。此外，我们使用ベルヌ利律采样将输入图像集采样到各个实例中，并在训练过程中使用某种抽样率。生成的结果是通过不同实例的训练网络生成的多个预测结果的平均值。实验结果表明，我们的方法在Synthetic和实际世界 dataset上达到了当前最佳的干扰除性能。这说明了我们的方法的有效性和实用性，可以用于解决各种噪声除掉任务。
</details></li>
</ul>
<hr>
<h2 id="Fractional-Denoising-for-3D-Molecular-Pre-training"><a href="#Fractional-Denoising-for-3D-Molecular-Pre-training" class="headerlink" title="Fractional Denoising for 3D Molecular Pre-training"></a>Fractional Denoising for 3D Molecular Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10683">http://arxiv.org/abs/2307.10683</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fengshikun/frad">https://github.com/fengshikun/frad</a></li>
<li>paper_authors: Shikun Feng, Yuyan Ni, Yanyan Lan, Zhi-Ming Ma, Wei-Ying Ma</li>
<li>for: 提高3D分子预训练方法的性能，尤其是在下游药物发现任务中。</li>
<li>methods: 提出了一种新的混合噪声策略，包括了两种类型的噪声：弧度噪声和坐标噪声。</li>
<li>results: 经过广泛的实验 validate，新提出的分子表示方法（Frad）在分子表示方面取得了新的状态流行，在QM9和MD17中的12个任务中取得了9个任务的最佳性能，在7个任务中取得了8个任务的最佳性能。<details>
<summary>Abstract</summary>
Coordinate denoising is a promising 3D molecular pre-training method, which has achieved remarkable performance in various downstream drug discovery tasks. Theoretically, the objective is equivalent to learning the force field, which is revealed helpful for downstream tasks. Nevertheless, there are two challenges for coordinate denoising to learn an effective force field, i.e. low coverage samples and isotropic force field. The underlying reason is that molecular distributions assumed by existing denoising methods fail to capture the anisotropic characteristic of molecules. To tackle these challenges, we propose a novel hybrid noise strategy, including noises on both dihedral angel and coordinate. However, denoising such hybrid noise in a traditional way is no more equivalent to learning the force field. Through theoretical deductions, we find that the problem is caused by the dependency of the input conformation for covariance. To this end, we propose to decouple the two types of noise and design a novel fractional denoising method (Frad), which only denoises the latter coordinate part. In this way, Frad enjoys both the merits of sampling more low-energy structures and the force field equivalence. Extensive experiments show the effectiveness of Frad in molecular representation, with a new state-of-the-art on 9 out of 12 tasks of QM9 and on 7 out of 8 targets of MD17.
</details>
<details>
<summary>摘要</summary>
“坐标降噪是一种有前途的3D分子预训方法，其在多种下游药物探索任务中实现了杰出的表现。理论上，这个目标等价于学习力场，这是有帮助的下游任务。然而，坐标降噪面临两个挑战，即低覆盖样本和iso tropic force field。这些挑战的根本原因是现有的降噪方法假设的分子分布不能捕捉分子的扩散特性。为了解决这些挑战，我们提出了一种新的混合噪音策略，包括了坐标和 dip hedral 的噪音。然而，对这种混合噪音的传统方式降噪不等于学习力场。经过理论推导，我们发现这问题是因为输入配置的弹性所致。为了解决这问题，我们提出了一种新的分解方法（Frad），它只降噪coordinate部分。这样，Frad能够同时享有更多低能构造的样本和力场等价。实验结果显示Frad在分子表现方面具有新的州际之优，在QM9上9个任务和MD17上7个任务中均成为新的州际之优。”
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-for-classification-of-noisy-QR-codes"><a href="#Deep-learning-for-classification-of-noisy-QR-codes" class="headerlink" title="Deep learning for classification of noisy QR codes"></a>Deep learning for classification of noisy QR codes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10677">http://arxiv.org/abs/2307.10677</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rebecca Leygonie, Sylvain Lobry, ), Laurent Wendling (LIPADE)</li>
<li>for: 本研究旨在定义基于深度学习的古典分类模型在抽象图像上的局限性，当应用于不可见视觉对象的图像。</li>
<li>methods: 我们使用了基于QR码的健康证件读取信息生成的图像进行训练。我们比较了基于深度学习的分类模型和传统（束缚）解码方法在噪声存在时的性能。</li>
<li>results: 我们发现基于深度学习的模型可以对抽象图像进行有效的理解。<details>
<summary>Abstract</summary>
We wish to define the limits of a classical classification model based on deep learning when applied to abstract images, which do not represent visually identifiable objects.QR codes (Quick Response codes) fall into this category of abstract images: one bit corresponding to one encoded character, QR codes were not designed to be decoded manually. To understand the limitations of a deep learning-based model for abstract image classification, we train an image classification model on QR codes generated from information obtained when reading a health pass. We compare a classification model with a classical (deterministic) decoding method in the presence of noise. This study allows us to conclude that a model based on deep learning can be relevant for the understanding of abstract images.
</details>
<details>
<summary>摘要</summary>
我团队想要定义深度学习模型在抽象图像分类中的限制，当应用于不可读取视觉标识 объек 的图像。二维码（Quick Response codes）是这类抽象图像的例子：每一比特对应一个编码字符，二维码并不是为人工手动解码设计的。通过使用一个基于深度学习的图像分类模型，我们在读取健康证的信息生成的二维码上进行训练。我们将这种模型与传统的束定解码方法进行比较，以确定在噪声存在时，深度学习模型对于抽象图像的理解是否有所有限。这项研究允许我们结论：基于深度学习的模型可以对抽象图像进行有效的理解。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-What-to-Share-in-Federated-Learning-Perspectives-on-Model-Utility-Privacy-Leakage-and-Communication-Efficiency"><a href="#A-Survey-of-What-to-Share-in-Federated-Learning-Perspectives-on-Model-Utility-Privacy-Leakage-and-Communication-Efficiency" class="headerlink" title="A Survey of What to Share in Federated Learning: Perspectives on Model Utility, Privacy Leakage, and Communication Efficiency"></a>A Survey of What to Share in Federated Learning: Perspectives on Model Utility, Privacy Leakage, and Communication Efficiency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10655">http://arxiv.org/abs/2307.10655</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiawei Shao, Zijian Li, Wenqiang Sun, Tailin Zhou, Yuchang Sun, Lumin Liu, Zehong Lin, Jun Zhang</li>
<li>for: 本研究旨在系统地检采 Federated Learning (FL) 中可以共享的信息，包括模型参数、静态数据和知识。</li>
<li>methods: 本文提出了一种新的分类方法，将 FL 方法分为三类：模型分享、静态数据分享和知识分享。此外，本文还对不同的分享方法的隐私攻击性进行分析，并评估了不同防御策略的效果。</li>
<li>results: 本文通过实验比较不同分享方法的性能和通信开销，并评估了不同防御策略的效果。 results 表明，模型分享和知识分享可以提高 FL 的性能，但是可能导致隐私泄露。静态数据分享可以减少通信开销，但是可能导致模型质量下降。<details>
<summary>Abstract</summary>
Federated learning (FL) has emerged as a highly effective paradigm for privacy-preserving collaborative training among different parties. Unlike traditional centralized learning, which requires collecting data from each party, FL allows clients to share privacy-preserving information without exposing private datasets. This approach not only guarantees enhanced privacy protection but also facilitates more efficient and secure collaboration among multiple participants. Therefore, FL has gained considerable attention from researchers, promoting numerous surveys to summarize the related works. However, the majority of these surveys concentrate on methods sharing model parameters during the training process, while overlooking the potential of sharing other forms of local information. In this paper, we present a systematic survey from a new perspective, i.e., what to share in FL, with an emphasis on the model utility, privacy leakage, and communication efficiency. This survey differs from previous ones due to four distinct contributions. First, we present a new taxonomy of FL methods in terms of the sharing methods, which includes three categories of shared information: model sharing, synthetic data sharing, and knowledge sharing. Second, we analyze the vulnerability of different sharing methods to privacy attacks and review the defense mechanisms that provide certain privacy guarantees. Third, we conduct extensive experiments to compare the performance and communication overhead of various sharing methods in FL. Besides, we assess the potential privacy leakage through model inversion and membership inference attacks, while comparing the effectiveness of various defense approaches. Finally, we discuss potential deficiencies in current methods and outline future directions for improvement.
</details>
<details>
<summary>摘要</summary>
Federated learning (FL) 已经 emerged as a highly effective paradigm for privacy-preserving collaborative training among different parties. Unlike traditional centralized learning, which requires collecting data from each party, FL allows clients to share privacy-preserving information without exposing private datasets. This approach not only guarantees enhanced privacy protection but also facilitates more efficient and secure collaboration among multiple participants. Therefore, FL has gained considerable attention from researchers, promoting numerous surveys to summarize the related works. However, the majority of these surveys concentrate on methods sharing model parameters during the training process, while overlooking the potential of sharing other forms of local information. In this paper, we present a systematic survey from a new perspective, i.e., what to share in FL, with an emphasis on the model utility, privacy leakage, and communication efficiency. This survey differs from previous ones due to four distinct contributions. First, we present a new taxonomy of FL methods in terms of the sharing methods, which includes three categories of shared information: model sharing, synthetic data sharing, and knowledge sharing. Second, we analyze the vulnerability of different sharing methods to privacy attacks and review the defense mechanisms that provide certain privacy guarantees. Third, we conduct extensive experiments to compare the performance and communication overhead of various sharing methods in FL. Besides, we assess the potential privacy leakage through model inversion and membership inference attacks, while comparing the effectiveness of various defense approaches. Finally, we discuss potential deficiencies in current methods and outline future directions for improvement.
</details></li>
</ul>
<hr>
<h2 id="Conditional-expectation-network-for-SHAP"><a href="#Conditional-expectation-network-for-SHAP" class="headerlink" title="Conditional expectation network for SHAP"></a>Conditional expectation network for SHAP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10654">http://arxiv.org/abs/2307.10654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ronald Richman, Mario V. Wüthrich</li>
<li>for: 这个论文旨在介绍一种能够explaining predictive models的技术，即SHapley Additive exPlanation（SHAP）。</li>
<li>methods: 这种技术使用了一种名为conditional expectation version和unconditional expectation version（后者也被称为interventional SHAP）的两种版本。通常情况下，用到了后者，因为它更加 computationally efficient。</li>
<li>results: 这种技术可以有效地计算出conditional SHAP，并且能够正确地考虑特征组件之间的依赖关系。此外，这种技术还可以提供drop1和anova分析，这与其普通的 GLM 对应项类似。此外，这种技术还可以提供一种名为partial dependence plot（PDP）的 counterpart，它考虑了特征组件之间的正确依赖关系。<details>
<summary>Abstract</summary>
A very popular model-agnostic technique for explaining predictive models is the SHapley Additive exPlanation (SHAP). The two most popular versions of SHAP are a conditional expectation version and an unconditional expectation version (the latter is also known as interventional SHAP). Except for tree-based methods, usually the unconditional version is used (for computational reasons). We provide a (surrogate) neural network approach which allows us to efficiently calculate the conditional version for both neural networks and other regression models, and which properly considers the dependence structure in the feature components. This proposal is also useful to provide drop1 and anova analyses in complex regression models which are similar to their generalized linear model (GLM) counterparts, and we provide a partial dependence plot (PDP) counterpart that considers the right dependence structure in the feature components.
</details>
<details>
<summary>摘要</summary>
非常受欢迎的模型无关技术之一是 SHapley Additive exPlanation（SHAP）。这两个最受欢迎的版本是 conditional expectation version 和 interventional SHAP（后者也被称为 conditional SHAP）。除了树状方法外，通常使用 unconditional version（由 computational reasons）。我们提供一种（surrogate）神经网络方法，可以有效计算 conditional version，并且正确地考虑特征组件之间的依赖关系。这种提议还有用于提供 drop1 和 anova 分析在复杂回归模型中，与其普通线性模型（GLM） counterpart 类似，并提供了一种考虑特征组件之间正确依赖关系的 PDP 对应。
</details></li>
</ul>
<hr>
<h2 id="Refining-the-Optimization-Target-for-Automatic-Univariate-Time-Series-Anomaly-Detection-in-Monitoring-Services"><a href="#Refining-the-Optimization-Target-for-Automatic-Univariate-Time-Series-Anomaly-Detection-in-Monitoring-Services" class="headerlink" title="Refining the Optimization Target for Automatic Univariate Time Series Anomaly Detection in Monitoring Services"></a>Refining the Optimization Target for Automatic Univariate Time Series Anomaly Detection in Monitoring Services</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10653">http://arxiv.org/abs/2307.10653</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manqing Dong, Zhanxiang Zhao, Yitong Geng, Wentao Li, Wei Wang, Huai Jiang</li>
<li>for: 本研究旨在提供一个自动化参数优化框架，以便在时间序列异常检测中实现更高的可靠性和系统性能。</li>
<li>methods: 该框架基于三个优化目标：预测分数、形态分数和敏感度分数，可以轻松地适应不同的模型背景而无需先知或手动标注efforts。</li>
<li>results: 该框架在在线应用了超过六个月，处理了每分钟50,000个时间序列，并提供了一个易用的用户界面，以及达到了检测结果的期望。  Comparative evaluations on public datasets further confirm the effectiveness of the proposed framework.<details>
<summary>Abstract</summary>
Time series anomaly detection is crucial for industrial monitoring services that handle a large volume of data, aiming to ensure reliability and optimize system performance. Existing methods often require extensive labeled resources and manual parameter selection, highlighting the need for automation. This paper proposes a comprehensive framework for automatic parameter optimization in time series anomaly detection models. The framework introduces three optimization targets: prediction score, shape score, and sensitivity score, which can be easily adapted to different model backbones without prior knowledge or manual labeling efforts. The proposed framework has been successfully applied online for over six months, serving more than 50,000 time series every minute. It simplifies the user's experience by requiring only an expected sensitive value, offering a user-friendly interface, and achieving desired detection results. Extensive evaluations conducted on public datasets and comparison with other methods further confirm the effectiveness of the proposed framework.
</details>
<details>
<summary>摘要</summary>
时序系列异常检测是对industrial monitoring服务的重要任务，旨在确保可靠性并优化系统性能。现有方法frequently require extensive labeled resources and manual parameter selection, highlighting the need for automation。本文提出了一个完整的自动参数优化框架 для时序系列异常检测模型。该框架引入了三个优化目标：预测得分、形态得分和敏感度得分，可以方便地适应不同的模型脊梁而无需先知或手动标注efforts。提议的框架已经在线上运行了超过六个月，处理了每分钟50,000个时序数据。它简化了用户的经验，只需要提供预期的敏感值，提供了易用的界面，并实现了所求的检测结果。对公共数据集和其他方法进行了广泛的评估，证明了提议的框架的有效性。
</details></li>
</ul>
<hr>
<h2 id="Data-Driven-Latency-Probability-Prediction-for-Wireless-Networks-Focusing-on-Tail-Probabilities"><a href="#Data-Driven-Latency-Probability-Prediction-for-Wireless-Networks-Focusing-on-Tail-Probabilities" class="headerlink" title="Data-Driven Latency Probability Prediction for Wireless Networks: Focusing on Tail Probabilities"></a>Data-Driven Latency Probability Prediction for Wireless Networks: Focusing on Tail Probabilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10648">http://arxiv.org/abs/2307.10648</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/samiemostafavi/wireless-pr3d">https://github.com/samiemostafavi/wireless-pr3d</a></li>
<li>paper_authors: Samie Mostafavi, Gourav Prateek Sharma, James Gross</li>
<li>for:  guarantees end-to-end network latency with extremely high reliability (99.999%) for cyber-physical systems and human-in-the-loop applications.</li>
<li>methods:  uses state-of-the-art data-driven approaches, such as mixture density networks (MDN) and extreme value mixture models, to predict the tail of the latency distribution and estimate the likelihood of rare latencies conditioned on network parameters.</li>
<li>results:  benchmarks the proposed approaches using actual latency measurements of IEEE 802.11g (WiFi), commercial private, and a software-defined 5G network, and evaluates their sensitivities concerning the tail probabilities.Here’s the full text in Simplified Chinese:</li>
<li>for: 这项研究旨在为新兴应用领域，如Cyber-Physical Systems和人在循环应用，提供绝对保证99.999%的终端到终端网络延迟，以确保高可靠性。</li>
<li>methods: 这项研究使用现代数据驱动方法，如混合密度网络（MDN）和极值混合模型，来预测延迟分布的尾部，并估计基于网络参数的罕见延迟的可能性。</li>
<li>results: 研究使用IEEE 802.11g（WiFi）、商业私人网络和软件定义5G网络的具体延迟测量来评估提议方法的可靠性和敏感性，并评估尾部概率的变化。<details>
<summary>Abstract</summary>
With the emergence of new application areas, such as cyber-physical systems and human-in-the-loop applications, there is a need to guarantee a certain level of end-to-end network latency with extremely high reliability, e.g., 99.999%. While mechanisms specified under IEEE 802.1as time-sensitive networking (TSN) can be used to achieve these requirements for switched Ethernet networks, implementing TSN mechanisms in wireless networks is challenging due to their stochastic nature. To conform the wireless link to a reliability level of 99.999%, the behavior of extremely rare outliers in the latency probability distribution, or the tail of the distribution, must be analyzed and controlled. This work proposes predicting the tail of the latency distribution using state-of-the-art data-driven approaches, such as mixture density networks (MDN) and extreme value mixture models, to estimate the likelihood of rare latencies conditioned on the network parameters, which can be used to make more informed decisions in wireless transmission. Actual latency measurements of IEEE 802.11g (WiFi), commercial private and a software-defined 5G network are used to benchmark the proposed approaches and evaluate their sensitivities concerning the tail probabilities.
</details>
<details>
<summary>摘要</summary>
随着新的应用领域的出现，如Cyber-physical systems和human-in-the-loop应用，需要保证一定的端到端网络延迟水平，例如99.999%的可靠性。而IEEE 802.1as时间敏感网络（TSN）的机制可以用来实现这些需求 для交换网络，但是在无线网络中实现TSN机制具有挑战性，因为无线链路具有随机性。为了将无线链路修正到99.999%的可靠性水平，需要分析和控制无线链路的偶极异值（tail）的延迟分布。本工作提议使用当今最佳的数据驱动方法，如混合概率网络（MDN）和极值混合模型，来预测延迟分布的尾部，以估计基于网络参数的罕见延迟的可能性。这些预测结果可以用于更 Informed decisions in wireless transmission。实际测量IEEE 802.11g（WiFi）、商业私人网络和软件定义5G网络的延迟测量被用来评估提议的方法和其对尾概率的敏感性。
</details></li>
</ul>
<hr>
<h2 id="Fisher-Rao-distance-and-pullback-SPD-cone-distances-between-multivariate-normal-distributions"><a href="#Fisher-Rao-distance-and-pullback-SPD-cone-distances-between-multivariate-normal-distributions" class="headerlink" title="Fisher-Rao distance and pullback SPD cone distances between multivariate normal distributions"></a>Fisher-Rao distance and pullback SPD cone distances between multivariate normal distributions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10644">http://arxiv.org/abs/2307.10644</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frank Nielsen</li>
<li>for: 本文主要针对多变量正态分布的处理，具体来说是定义正态分布之间的距离和路径。</li>
<li>methods: 本文提出了一种快速和稳定的方法来 aproximate Fisher-Rao 距离，以及基于 diffeomorphic embedding的一种新的距离计算方法。</li>
<li>results: 本文显示了 pullback Hilbert cone 距离的计算只需要计算extreme minimal和 maximal eigenvalues of matrices，而不是计算整个 Fisher information metric。此外，本文还应用了这些距离在 clustering 任务中。<details>
<summary>Abstract</summary>
Data sets of multivariate normal distributions abound in many scientific areas like diffusion tensor imaging, structure tensor computer vision, radar signal processing, machine learning, just to name a few. In order to process those normal data sets for downstream tasks like filtering, classification or clustering, one needs to define proper notions of dissimilarities between normals and paths joining them. The Fisher-Rao distance defined as the Riemannian geodesic distance induced by the Fisher information metric is such a principled metric distance which however is not known in closed-form excepts for a few particular cases. In this work, we first report a fast and robust method to approximate arbitrarily finely the Fisher-Rao distance between multivariate normal distributions. Second, we introduce a class of distances based on diffeomorphic embeddings of the normal manifold into a submanifold of the higher-dimensional symmetric positive-definite cone corresponding to the manifold of centered normal distributions. We show that the projective Hilbert distance on the cone yields a metric on the embedded normal submanifold and we pullback that cone distance with its associated straight line Hilbert cone geodesics to obtain a distance and smooth paths between normal distributions. Compared to the Fisher-Rao distance approximation, the pullback Hilbert cone distance is computationally light since it requires to compute only the extreme minimal and maximal eigenvalues of matrices. Finally, we show how to use those distances in clustering tasks.
</details>
<details>
<summary>摘要</summary>
“多变量正态分布数据集在许多科学领域非常普遍，如扩散tensor成像、结构tensor计算机视觉、雷达信号处理、机器学习等。为了处理这些正态数据集，进行后续任务如过滤、分类或归类，需要定义正确的差异量 между正态。悉尼诺距离（Fisher-Rao distance）是一种原理正确的距离度量，但是除了其特殊情况外，它的闭式表示不知道。在这个工作中，我们首先报告了一种快速和可靠的方法来精确地 aproximate multivariate normal distributions之间的悉尼诺距离。其次，我们引入了一类基于diff伪omorphic embedding的距离，该距离在正态拓扑上定义了一个度量。我们显示了该距离在投影卷积体上是一个度量，并将其pullback到正态拓扑上，从而获得了一个距离和smooth paths between normal distributions。与悉尼诺距离 aproximation相比，pullback Hilbert cone distance更加计算轻量级，只需计算最小和最大特征值。最后，我们介绍了如何使用这些距离在 clustering 任务中。”Note: "Simplified Chinese" is a romanization of the Chinese language that uses simpler characters and grammar to facilitate typing and reading. It is not a formal standard, but rather a common practice among Chinese speakers who use the Roman alphabet.
</details></li>
</ul>
<hr>
<h2 id="SciBench-Evaluating-College-Level-Scientific-Problem-Solving-Abilities-of-Large-Language-Models"><a href="#SciBench-Evaluating-College-Level-Scientific-Problem-Solving-Abilities-of-Large-Language-Models" class="headerlink" title="SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models"></a>SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10635">http://arxiv.org/abs/2307.10635</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mandyyyyii/scibench">https://github.com/mandyyyyii/scibench</a></li>
<li>paper_authors: Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R. Loomba, Shichang Zhang, Yizhou Sun, Wei Wang</li>
<li>for: 这个论文的目的是检验大语言模型（LLMs）在复杂科学问题解决能力方面的进步。</li>
<li>methods: 该论文使用了两个精心编辑的数据集：一个公共数据集包含了高等教育文books中的科学问题，另一个关闭数据集包含了计算机科学和数学专业考试中的问题。然后，通过对两种代表性的LLMs进行各种提示策略的测试，对这两个数据集进行了深入的比较研究。</li>
<li>results: 结果显示，目前的LLMs在解决复杂科学问题方面的表现仅有35.80%的得分，并且通过详细的用户研究，对LLMs的错误分类为十种问题解决能力。研究发现，不同的提示策略之间没有一个显著的占优，一些策略可以提高某些问题解决能力，但同时会导致其他能力下降。<details>
<summary>Abstract</summary>
Recent advances in large language models (LLMs) have demonstrated notable progress on many mathematical benchmarks. However, most of these benchmarks only feature problems grounded in junior and senior high school subjects, contain only multiple-choice questions, and are confined to a limited scope of elementary arithmetic operations. To address these issues, this paper introduces an expansive benchmark suite SciBench that aims to systematically examine the reasoning capabilities required for complex scientific problem solving. SciBench contains two carefully curated datasets: an open set featuring a range of collegiate-level scientific problems drawn from mathematics, chemistry, and physics textbooks, and a closed set comprising problems from undergraduate-level exams in computer science and mathematics. Based on the two datasets, we conduct an in-depth benchmark study of two representative LLMs with various prompting strategies. The results reveal that current LLMs fall short of delivering satisfactory performance, with an overall score of merely 35.80%. Furthermore, through a detailed user study, we categorize the errors made by LLMs into ten problem-solving abilities. Our analysis indicates that no single prompting strategy significantly outperforms others and some strategies that demonstrate improvements in certain problem-solving skills result in declines in other skills. We envision that SciBench will catalyze further developments in the reasoning abilities of LLMs, thereby ultimately contributing to scientific research and discovery.
</details>
<details>
<summary>摘要</summary>
SciBench includes two datasets: an open set with a range of collegiate-level scientific problems from mathematics, chemistry, and physics textbooks, and a closed set with problems from undergraduate-level exams in computer science and mathematics. We conduct an in-depth study of two representative LLMs with various prompting strategies, and find that current LLMs only achieve an overall score of 35.80%.Through a detailed user study, we categorize the errors made by LLMs into ten problem-solving abilities. Our analysis shows that no single prompting strategy significantly outperforms others, and some strategies that improve in certain skills result in declines in other skills.We envision that SciBench will drive further developments in the reasoning abilities of LLMs, ultimately contributing to scientific research and discovery.
</details></li>
</ul>
<hr>
<h2 id="Generative-Language-Models-on-Nucleotide-Sequences-of-Human-Genes"><a href="#Generative-Language-Models-on-Nucleotide-Sequences-of-Human-Genes" class="headerlink" title="Generative Language Models on Nucleotide Sequences of Human Genes"></a>Generative Language Models on Nucleotide Sequences of Human Genes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10634">http://arxiv.org/abs/2307.10634</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/boun-tabi/generativelm-genes">https://github.com/boun-tabi/generativelm-genes</a></li>
<li>paper_authors: Musa Nuri Ihtiyar, Arzucan Ozgur</li>
<li>for: The paper is focused on developing an autoregressive generative language model for DNA sequences, with a focus on nucleotide sequences of human genes.</li>
<li>methods: The authors use a variety of techniques, including RNNs and N-grams, to explore the generative capabilities of their model. They also examine the use of real-life tasks beyond classical metrics such as perplexity to evaluate the model’s performance.</li>
<li>results: The authors observe that RNNs perform the best, and that the data-hungry nature of the model is not significantly affected by selecting a language with a minimal vocabulary size. They also find that the model is able to generate coherent and meaningful DNA sequences.Here is the same information in Simplified Chinese text:</li>
<li>for: 该论文主要关注开发一种基于转换器的生成语言模型，用于DNA序列。</li>
<li>methods: 作者使用了多种技术，包括RNNs和N-grams，来探索该模型的生成能力。他们还评估了使用实际任务以外的 métricas，以评估模型的性能。</li>
<li>results: 作者发现RNNs表现最佳，而选择一种语言的词汇量较少并不导致很大的数据需求变化。他们还发现模型能够生成有意义和 coherent的DNA序列。<details>
<summary>Abstract</summary>
Language models, primarily transformer-based ones, obtained colossal success in NLP. To be more precise, studies like BERT in NLU and works such as GPT-3 for NLG are very crucial. DNA sequences are very close to natural language in terms of structure, so if the DNA-related bioinformatics domain is concerned, discriminative models, like DNABert, exist. Yet, the generative side of the coin is mainly unexplored to the best of our knowledge. Consequently, we focused on developing an autoregressive generative language model like GPT-3 for DNA sequences. Because working with whole DNA sequences is challenging without substantial computational resources, we decided to carry out our study on a smaller scale, focusing on nucleotide sequences of human genes, unique parts in DNA with specific functionalities, instead of the whole DNA. This decision did not change the problem structure a lot due to the fact that both DNA and genes can be seen as 1D sequences consisting of four different nucleotides without losing much information and making too much simplification. First of all, we systematically examined an almost entirely unexplored problem and observed that RNNs performed the best while simple techniques like N-grams were also promising. Another beneficial point was learning how to work with generative models on languages we do not understand, unlike natural language. How essential using real-life tasks beyond the classical metrics such as perplexity is observed. Furthermore, checking whether the data-hungry nature of these models can be changed through selecting a language with minimal vocabulary size, four owing to four different types of nucleotides, is examined. The reason for reviewing this was that choosing such a language might make the problem easier. However, what we observed in this study was it did not provide that much of a change in the amount of data needed.
</details>
<details>
<summary>摘要</summary>
Language models, primarily transformer-based ones, have achieved great success in NLP. To be more precise, studies like BERT in NLU and works such as GPT-3 for NLG are very crucial. DNA sequences are very close to natural language in terms of structure, so if the DNA-related bioinformatics domain is concerned, discriminative models like DNABert exist. However, the generative side of the coin is mainly unexplored to the best of our knowledge. Therefore, we focused on developing an autoregressive generative language model like GPT-3 for DNA sequences. Because working with whole DNA sequences is challenging without substantial computational resources, we decided to carry out our study on a smaller scale, focusing on nucleotide sequences of human genes, unique parts in DNA with specific functionalities, instead of the whole DNA. This decision did not change the problem structure much because both DNA and genes can be seen as 1D sequences consisting of four different nucleotides without losing much information and making too much simplification.First, we systematically examined an almost entirely unexplored problem and observed that RNNs performed the best, while simple techniques like N-grams were also promising. Another beneficial point was learning how to work with generative models on languages we do not understand, unlike natural language. We also found that using real-life tasks beyond classical metrics such as perplexity is essential. Furthermore, we checked whether the data-hungry nature of these models can be changed through selecting a language with minimal vocabulary size, four owing to four different types of nucleotides. However, what we observed in this study was that choosing such a language did not provide that much of a change in the amount of data needed.
</details></li>
</ul>
<hr>
<h2 id="Multi-Method-Self-Training-Improving-Code-Generation-With-Text-And-Vice-Versa"><a href="#Multi-Method-Self-Training-Improving-Code-Generation-With-Text-And-Vice-Versa" class="headerlink" title="Multi-Method Self-Training: Improving Code Generation With Text, And Vice Versa"></a>Multi-Method Self-Training: Improving Code Generation With Text, And Vice Versa</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10633">http://arxiv.org/abs/2307.10633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shriyash K. Upadhyay, Etan J. Ginsberg</li>
<li>for: 提高语言模型的可用性和性能</li>
<li>methods: 多种方法自动训练</li>
<li>results: 1) 提高较弱方法（最高提高30%），2) 提高较强方法（最高提高32.2%），3) 提高相关 yet distinct 任务（最高提高10.3%）的性能，并通过数据生成和多种方法的使用来解释MMST的成功。Here’s a more detailed explanation of each point:1. for: The paper aims to improve the availability and performance of language models by introducing a new method called Multi-Method Self-Training (MMST).2. methods: The paper uses multiple methods for self-training, including the filtered outputs of another method, to augment the strengths and ameliorate the weaknesses of each method.3. results: The paper shows that MMST can improve the performance of the less performant method by up to 30%, improve the performance of the more performant method by up to 32.2%, and improve the performance of related yet distinct tasks by up to 10.3%. The improvement in performance is driven by the use of multiple methods, and the paper provides ablation analyses to explore why MMST works.<details>
<summary>Abstract</summary>
Large Language Models have many methods for solving the same problem. This introduces novel strengths (different methods may work well for different problems) and weaknesses (it may be difficult for users to know which method to use). In this paper, we introduce Multi-Method Self-Training (MMST), where one method is trained on the filtered outputs of another, allowing us to augment the strengths and ameliorate the weaknesses of each method. Using a 176B parameter model trained on both language and code, we show that MMST can 1) improve the less performant method (up to 30%) making the model easier to use, 2) improve the more performant method (up to 32.2%) making the model more performant, and 3) improve the performance of related but distinct tasks (up to 10.3%) by improving the ability of the model to generate rationales. We then conduct ablation analyses to explore why MMST works. We show that MMST generates more data than traditional self-training, but the improvement in performance is driven by the use of multiple methods. We also analyze prompt-engineering and anti-correlated performance between methods as means of making MMST more effective. We hope the evidence from our paper motivates machine learning researchers to explore ways in which advances in language models allow for new forms of training.
</details>
<details>
<summary>摘要</summary>
大型语言模型有多种方法解决同一个问题，这导致新的优势（不同的方法可以对不同的问题有好的表现）和弱点（用户可能Difficult to determine which method to use）。在这篇论文中，我们介绍Multi-Method Self-Training（MMST），其中一种方法被另一种方法所训练，从而可以增强每种方法的优势和改善每种方法的缺点。使用176亿参数的模型，我们显示MMST可以1）提高较差的方法（最多30%），使模型更易使用，2）提高较佳的方法（最多32.2%），使模型更高效，3）提高相关但不同的任务（最多10.3%）的表现，并提高模型生成理由的能力。然后我们进行了删除分析，以探索MMST的作用原理。我们发现MMST生成了更多的数据，但是改善表现的关键在于使用多种方法。我们还分析了引擎和反相关的表现，以提高MMST的效果。我们希望这篇论文的证据能够鼓励机器学习研究人员探索在语言模型技术的进步下，新的训练方法的可能性。
</details></li>
</ul>
<hr>
<h2 id="Unmasking-Falsehoods-in-Reviews-An-Exploration-of-NLP-Techniques"><a href="#Unmasking-Falsehoods-in-Reviews-An-Exploration-of-NLP-Techniques" class="headerlink" title="Unmasking Falsehoods in Reviews: An Exploration of NLP Techniques"></a>Unmasking Falsehoods in Reviews: An Exploration of NLP Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10617">http://arxiv.org/abs/2307.10617</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anusuya Baby Hari Krishnan<br>for: 这篇研究paper的目的是探讨如何使用机器学习模型来识别诈骗评价，尤其是餐厅评价中的伪评价。methods: 本研究使用了n-gram模型和max features来检测诈骗内容，并与五种不同的机器学习分类算法进行比较。此外，研究还应用了深度学习技术来增强诈骗评价的检测。results: 研究结果显示，使用了调和攻击分类器的方法可以实现最高的准确率，不仅在文本分类方面，而且在识别伪评价方面也表现出色。此外，研究还发现了诈骗评价的特征和特征，对于在线商业中的评价推广和评价管理提供了宝贵的见解。<details>
<summary>Abstract</summary>
In the contemporary digital landscape, online reviews have become an indispensable tool for promoting products and services across various businesses. Marketers, advertisers, and online businesses have found incentives to create deceptive positive reviews for their products and negative reviews for their competitors' offerings. As a result, the writing of deceptive reviews has become an unavoidable practice for businesses seeking to promote themselves or undermine their rivals. Detecting such deceptive reviews has become an intense and ongoing area of research. This research paper proposes a machine learning model to identify deceptive reviews, with a particular focus on restaurants. This study delves into the performance of numerous experiments conducted on a dataset of restaurant reviews known as the Deceptive Opinion Spam Corpus. To accomplish this, an n-gram model and max features are developed to effectively identify deceptive content, particularly focusing on fake reviews. A benchmark study is undertaken to explore the performance of two different feature extraction techniques, which are then coupled with five distinct machine learning classification algorithms. The experimental results reveal that the passive aggressive classifier stands out among the various algorithms, showcasing the highest accuracy not only in text classification but also in identifying fake reviews. Moreover, the research delves into data augmentation and implements various deep learning techniques to further enhance the process of detecting deceptive reviews. The findings shed light on the efficacy of the proposed machine learning approach and offer valuable insights into dealing with deceptive reviews in the realm of online businesses.
</details>
<details>
<summary>摘要</summary>
现代数字景观中，在线评价已成为不同业务的无可或的推广工具。广告商、市场部和在线业务都找到了创造假评价来推广自己的产品或推翻竞争对手的产品的利益。因此，假评价的写作已成为为企业推广自己或推翻竞争对手的不可或缺的做法。检测这些假评价已成为一项激烈和持续的研究领域。本研究提出了一种机器学习模型，用于识别假评价，尤其是针对餐厅的评价。本研究通过对知名餐厅评价数据集——假评价垃圾数据集进行多个实验，开发出n格RAM模型和最佳特征，以便有效地识别假内容，特别是假评价。进行了基准研究，以explore两种不同的特征提取技术的性能，然后与五种不同的机器学习分类算法结合。实验结果显示，通过负拒抵抗分类器的方式，在文本分类和假评价识别方面达到了最高精度。此外，研究还探讨了数据扩充和深度学习技术，以进一步提高假评价的检测过程。研究结果为检测在线评价中的假评价提供了有价值的指导和精深的理解，对于在线业务而言，对于假评价的检测和处理也提供了有价值的方法。
</details></li>
</ul>
<hr>
<h2 id="Heterogeneous-Federated-Learning-State-of-the-art-and-Research-Challenges"><a href="#Heterogeneous-Federated-Learning-State-of-the-art-and-Research-Challenges" class="headerlink" title="Heterogeneous Federated Learning: State-of-the-art and Research Challenges"></a>Heterogeneous Federated Learning: State-of-the-art and Research Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10616">http://arxiv.org/abs/2307.10616</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/marswhu/hfl_survey">https://github.com/marswhu/hfl_survey</a></li>
<li>paper_authors: Mang Ye, Xiuwen Fang, Bo Du, Pong C. Yuen, Dacheng Tao</li>
<li>for: 这篇论文主要是为了探讨 Federated Learning（FL）在大规模实际应用中的挑战和解决方案。</li>
<li>methods: 论文回顾了现有的 Federated Learning 研究，以及各种研究挑战，包括统计差异、模型差异、通信差异、设备差异以及其他挑战。</li>
<li>results: 论文提出了一种新的 Federated Learning 方法分类系统，并进行了深入的分析和评价。此外，论文还提出了一些未来研究方向，以便进一步发展这一领域。<details>
<summary>Abstract</summary>
Federated learning (FL) has drawn increasing attention owing to its potential use in large-scale industrial applications. Existing federated learning works mainly focus on model homogeneous settings. However, practical federated learning typically faces the heterogeneity of data distributions, model architectures, network environments, and hardware devices among participant clients. Heterogeneous Federated Learning (HFL) is much more challenging, and corresponding solutions are diverse and complex. Therefore, a systematic survey on this topic about the research challenges and state-of-the-art is essential. In this survey, we firstly summarize the various research challenges in HFL from five aspects: statistical heterogeneity, model heterogeneity, communication heterogeneity, device heterogeneity, and additional challenges. In addition, recent advances in HFL are reviewed and a new taxonomy of existing HFL methods is proposed with an in-depth analysis of their pros and cons. We classify existing methods from three different levels according to the HFL procedure: data-level, model-level, and server-level. Finally, several critical and promising future research directions in HFL are discussed, which may facilitate further developments in this field. A periodically updated collection on HFL is available at https://github.com/marswhu/HFL_Survey.
</details>
<details>
<summary>摘要</summary>
受欢迎的 Federated Learning（FL）在大规模产业应用中受到了越来越多的关注，因为它可以在分布式数据和模型之间学习。然而，实际的 Federated Learning 往往面临参与客户端的数据分布、模型结构、网络环境和硬件设备之间的多样性。这种多样性导致了异化 Federated Learning（HFL）的研究挑战更大，需要对应的多种和复杂的解决方案。因此，一篇系统性的survey sobre这个主题是非常重要的。在这份survey中，我们首先总结了HFL中不同方面的研究挑战，包括统计学上的多样性、模型上的多样性、通信上的多样性、设备上的多样性以及其他挑战。此外，我们还审视了最新的HFL研究进展，并提出了一种新的HFL方法分类方案，进行深入的分析其优缺点。我们将exististing方法分为三个不同的水平：数据级、模型级和服务级。最后，我们讨论了HFL未来的一些重要和有前途的研究方向，这些方向可能会促进这一领域的进一步发展。一个periodically更新的HFL集成可以在https://github.com/marswhu/HFL_Survey上找到。
</details></li>
</ul>
<hr>
<h2 id="Flatness-Aware-Minimization-for-Domain-Generalization"><a href="#Flatness-Aware-Minimization-for-Domain-Generalization" class="headerlink" title="Flatness-Aware Minimization for Domain Generalization"></a>Flatness-Aware Minimization for Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11108">http://arxiv.org/abs/2307.11108</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingxuan Zhang, Renzhe Xu, Han Yu, Yancheng Dong, Pengfei Tian, Peng Cu</li>
<li>for: 这个论文主要研究的是领域总结（Domain Generalization，DG）中的优化器选择。</li>
<li>methods: 作者提出了一种新的方法 called Flatness-Aware Minimization for Domain Generalization (FAD), which can efficiently optimize both zeroth-order and first-order flatness simultaneously for DG.</li>
<li>results: 实验结果显示，FAD 在多个 DG dataset 上表现出优于其他 zeroth-order 和 first-order 平坦性感知优化方法。 Additionally, the authors provide theoretical analyses of the FAD’s out-of-distribution (OOD) generalization error and convergence.<details>
<summary>Abstract</summary>
Domain generalization (DG) seeks to learn robust models that generalize well under unknown distribution shifts. As a critical aspect of DG, optimizer selection has not been explored in depth. Currently, most DG methods follow the widely used benchmark, DomainBed, and utilize Adam as the default optimizer for all datasets. However, we reveal that Adam is not necessarily the optimal choice for the majority of current DG methods and datasets. Based on the perspective of loss landscape flatness, we propose a novel approach, Flatness-Aware Minimization for Domain Generalization (FAD), which can efficiently optimize both zeroth-order and first-order flatness simultaneously for DG. We provide theoretical analyses of the FAD's out-of-distribution (OOD) generalization error and convergence. Our experimental results demonstrate the superiority of FAD on various DG datasets. Additionally, we confirm that FAD is capable of discovering flatter optima in comparison to other zeroth-order and first-order flatness-aware optimization methods.
</details>
<details>
<summary>摘要</summary>
领域总结（DG）目的是学习可靠的模型，以适应未知分布变化。作为DG的关键方面，优化器选择尚未得到广泛探讨。目前大多数DG方法采用DomainBed标准启动点，并使用Adam作为所有数据集的默认优化器。然而，我们发现Adam并不一定是现有DG方法和数据集中的优选。基于损失地形平坦性的视角，我们提出了一种新的方法：适应性评估降维minimization for Domain Generalization（FAD），可以有效地同时优化零次训练和首次训练的平坦性。我们提供了关于FAD的OOD泛化误差和收敛性的理论分析。我们的实验结果表明FAD在多个DG数据集上具有优越性。此外，我们证明了FAD可以更好地找到抽象的优点，相比其他零次训练和首次训练平坦性意识的优化方法。
</details></li>
</ul>
<hr>
<h2 id="Ensemble-Learning-based-Anomaly-Detection-for-IoT-Cybersecurity-via-Bayesian-Hyperparameters-Sensitivity-Analysis"><a href="#Ensemble-Learning-based-Anomaly-Detection-for-IoT-Cybersecurity-via-Bayesian-Hyperparameters-Sensitivity-Analysis" class="headerlink" title="Ensemble Learning based Anomaly Detection for IoT Cybersecurity via Bayesian Hyperparameters Sensitivity Analysis"></a>Ensemble Learning based Anomaly Detection for IoT Cybersecurity via Bayesian Hyperparameters Sensitivity Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10596">http://arxiv.org/abs/2307.10596</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tin Lai, Farnaz Farid, Abubakar Bello, Fariza Sabrina</li>
<li>for:  This paper focuses on enhancing IoT cybersecurity via anomaly detection using ensemble machine learning methods.</li>
<li>methods: The proposed method utilizes Bayesian hyperparameter optimization to adapt to a network environment with multiple IoT sensor readings, and combines the predictive power of multiple machine learning models to enhance predictive accuracy in heterogeneous datasets.</li>
<li>results: The proposed method shows high predictive power compared to traditional methods through experimental illustration.<details>
<summary>Abstract</summary>
The Internet of Things (IoT) integrates more than billions of intelligent devices over the globe with the capability of communicating with other connected devices with little to no human intervention. IoT enables data aggregation and analysis on a large scale to improve life quality in many domains. In particular, data collected by IoT contain a tremendous amount of information for anomaly detection. The heterogeneous nature of IoT is both a challenge and an opportunity for cybersecurity. Traditional approaches in cybersecurity monitoring often require different kinds of data pre-processing and handling for various data types, which might be problematic for datasets that contain heterogeneous features. However, heterogeneous types of network devices can often capture a more diverse set of signals than a single type of device readings, which is particularly useful for anomaly detection. In this paper, we present a comprehensive study on using ensemble machine learning methods for enhancing IoT cybersecurity via anomaly detection. Rather than using one single machine learning model, ensemble learning combines the predictive power from multiple models, enhancing their predictive accuracy in heterogeneous datasets rather than using one single machine learning model. We propose a unified framework with ensemble learning that utilises Bayesian hyperparameter optimisation to adapt to a network environment that contains multiple IoT sensor readings. Experimentally, we illustrate their high predictive power when compared to traditional methods.
</details>
<details>
<summary>摘要</summary>
互联网物联网（IoT）已经 integrates 了全球多亿个智能设备，这些设备可以自动通信，无需人类干预。IoT 可以实现大规模数据聚合和分析，以改善生活质量在多个领域。尤其是在 IoT 数据中，存在许多有用的信息可以探测到异常。然而，IoT 网络中的异类网络设备具有挑战和机会，传统的防护策略通常需要不同的数据处理和处理方法，这可能会导致复杂的数据处理问题。但是，异类网络设备可以捕捉到更多的信号，这是特别有用的 для 异常探测。在这篇文章中，我们提出了一个统一框架，使用组合学 machine learning 方法来强化 IoT 网络的防护。而不是使用单一的 machine learning 模型，组合学可以结合多个模型的预测力，在不同的网络环境中提高预测精度。我们提出了一个 Bayesian 参数优化的框架，以适应含有多个 IoT 感应器读取的网络环境。实验结果表明，这种方法的预测力较高，比较传统的方法。
</details></li>
</ul>
<hr>
<h2 id="Forecasting-Battery-Electric-Vehicle-Charging-Behavior-A-Deep-Learning-Approach-Equipped-with-Micro-Clustering-and-SMOTE-Techniques"><a href="#Forecasting-Battery-Electric-Vehicle-Charging-Behavior-A-Deep-Learning-Approach-Equipped-with-Micro-Clustering-and-SMOTE-Techniques" class="headerlink" title="Forecasting Battery Electric Vehicle Charging Behavior: A Deep Learning Approach Equipped with Micro-Clustering and SMOTE Techniques"></a>Forecasting Battery Electric Vehicle Charging Behavior: A Deep Learning Approach Equipped with Micro-Clustering and SMOTE Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10588">http://arxiv.org/abs/2307.10588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanif Tayarani, Trisha V. Ramadoss, Vaishnavi Karanam, Gil Tal, Christopher Nitta</li>
<li>for: This study aims to develop a novel Micro Clustering Deep Neural Network (MCDNN) to forecast BEV charging events, which is essential for electricity load aggregators and utility managers to provide charging stations and electricity capacity effectively.</li>
<li>methods: The study uses a robust dataset of trips and charges that occurred in California between 2015 and 2020 from 132 BEVs, spanning 5 BEV models, to train the MCDNN.</li>
<li>results: The numerical findings show that the proposed MCDNN is more effective than benchmark approaches in this field, such as support vector machine, k nearest neighbors, decision tree, and other neural network-based models in predicting the charging events.<details>
<summary>Abstract</summary>
Energy systems, climate change, and public health are among the primary reasons for moving toward electrification in transportation. Transportation electrification is being promoted worldwide to reduce emissions. As a result, many automakers will soon start making only battery electric vehicles (BEVs). BEV adoption rates are rising in California, mainly due to climate change and air pollution concerns. While great for climate and pollution goals, improperly managed BEV charging can lead to insufficient charging infrastructure and power outages. This study develops a novel Micro Clustering Deep Neural Network (MCDNN), an artificial neural network algorithm that is highly effective at learning BEVs trip and charging data to forecast BEV charging events, information that is essential for electricity load aggregators and utility managers to provide charging stations and electricity capacity effectively. The MCDNN is configured using a robust dataset of trips and charges that occurred in California between 2015 and 2020 from 132 BEVs, spanning 5 BEV models for a total of 1570167 vehicle miles traveled. The numerical findings revealed that the proposed MCDNN is more effective than benchmark approaches in this field, such as support vector machine, k nearest neighbors, decision tree, and other neural network-based models in predicting the charging events.
</details>
<details>
<summary>摘要</summary>
transportation electrification is being promoted worldwide to reduce emissions. As a result, many automakers will soon start making only battery electric vehicles (BEVs). BEV adoption rates are rising in California, mainly due to climate change and air pollution concerns. However, improperly managed BEV charging can lead to insufficient charging infrastructure and power outages. This study develops a novel Micro Clustering Deep Neural Network (MCDNN), an artificial neural network algorithm that is highly effective at learning BEVs trip and charging data to forecast BEV charging events, information that is essential for electricity load aggregators and utility managers to provide charging stations and electricity capacity effectively. The MCDNN is configured using a robust dataset of trips and charges that occurred in California between 2015 and 2020 from 132 BEVs, spanning 5 BEV models for a total of 1570167 vehicle miles traveled. The numerical findings revealed that the proposed MCDNN is more effective than benchmark approaches in this field, such as support vector machine, k nearest neighbors, decision tree, and other neural network-based models in predicting the charging events.Here is the translation in Traditional Chinese:运输电化正在全球推广以减少排放。因此，许多汽车生产商将很快开始生产仅有电池电动车 (BEV)。加利福尼亚州的 BEV 采购率正在增长，主要是因为气候变化和空气污染的关注。然而，不当管理 BEV 充电可能会导致充电基础设施不足和电力停机。本研究开发了一个微型对 clustering深度神经网 (MCDNN)，一种人工神经网络算法，可以很好地学习 BEV 行程和充电数据，预测 BEV 充电事件，这些信息是电力聚集者和供应商调度员需要的。MCDNN 是使用加利福尼亚州2015-2020年间的 132 部 BEV 的行程和充电数据集成的，总共行程 1570167 公里。numerical 的结果显示，提案的 MCDNN 在这个领域中比基准方法更有效，例如支持向量机器、最近邻居、决策树和其他神经网络基础模型在预测充电事件方面。
</details></li>
</ul>
<hr>
<h2 id="A-Holistic-Assessment-of-the-Reliability-of-Machine-Learning-Systems"><a href="#A-Holistic-Assessment-of-the-Reliability-of-Machine-Learning-Systems" class="headerlink" title="A Holistic Assessment of the Reliability of Machine Learning Systems"></a>A Holistic Assessment of the Reliability of Machine Learning Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10586">http://arxiv.org/abs/2307.10586</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anthony Corso, David Karamadian, Romeo Valentin, Mary Cooper, Mykel J. Kochenderfer</li>
<li>for: 本研究旨在评估机器学习系统的可靠性，以满足高风险应用场景中的需求。</li>
<li>methods: 本研究提出了一种整体评估机器学习系统可靠性的方法，包括评估五个关键属性：内部分布准确率、环境变化Robustness、攻击Robustness、准确性和外部分布检测。</li>
<li>results: 研究人员通过对实际任务中的500多个模型进行评估，发现不同的算法方法可以同时提高多个可靠性指标的表现。这个研究为机器学习可靠性的全面理解和未来研发提供了一份重要的贡献。<details>
<summary>Abstract</summary>
As machine learning (ML) systems increasingly permeate high-stakes settings such as healthcare, transportation, military, and national security, concerns regarding their reliability have emerged. Despite notable progress, the performance of these systems can significantly diminish due to adversarial attacks or environmental changes, leading to overconfident predictions, failures to detect input faults, and an inability to generalize in unexpected scenarios. This paper proposes a holistic assessment methodology for the reliability of ML systems. Our framework evaluates five key properties: in-distribution accuracy, distribution-shift robustness, adversarial robustness, calibration, and out-of-distribution detection. A reliability score is also introduced and used to assess the overall system reliability. To provide insights into the performance of different algorithmic approaches, we identify and categorize state-of-the-art techniques, then evaluate a selection on real-world tasks using our proposed reliability metrics and reliability score. Our analysis of over 500 models reveals that designing for one metric does not necessarily constrain others but certain algorithmic techniques can improve reliability across multiple metrics simultaneously. This study contributes to a more comprehensive understanding of ML reliability and provides a roadmap for future research and development.
</details>
<details>
<summary>摘要</summary>
为了更好地了解机器学习（ML）系统在高风险场景中的可靠性，这篇文章提出了一种整体评估方法。我们的框架评估了五个关键属性：内部分布式准确率、环境变化 Robustness、对抗攻击 Robustness、准确率和外部分布式检测。我们还引入了一个可靠度分数，用于评估整体系统可靠性。为了提供不同算法方法的性能分析，我们标识并分类了当前领先技术，然后使用我们提议的可靠性指标和可靠度分数来评估选择的模型。我们对超过500个模型进行了实际任务上的评估，发现不同的算法方法可以同时提高多个可靠性指标的性能。这篇研究对ML系统可靠性的全面理解做出了贡献，并提供了未来研发的路线图。
</details></li>
</ul>
<hr>
<h2 id="Intelligent-model-for-offshore-China-sea-fog-forecasting"><a href="#Intelligent-model-for-offshore-China-sea-fog-forecasting" class="headerlink" title="Intelligent model for offshore China sea fog forecasting"></a>Intelligent model for offshore China sea fog forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10580">http://arxiv.org/abs/2307.10580</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanfei Xiang, Qinghong Zhang, Mingqing Wang, Ruixue Xia, Yang Kong, Xiaomeng Huang</li>
<li>for: 预测海上雾的精准性和时效性非常重要，以确保海上和沿海经济活动的正常运行。</li>
<li>methods: 本研究使用机器学习方法，在数值天气预测模型中嵌入，通过用 Yangtze River Estuary（YRE）沿岸地区为例进行实验。在训练机器学习模型之前，我们使用时间延迟相关分析技术来 indentify关键预测器和解释海上雾出现的基本机制。此外，我们还实施 ensemble learning 和焦点损失函数，以提高我们模型的预测能力。</li>
<li>results: 我们的机器学习基于方法在一年的全面数据集上进行评估，包括天气站观测数据和历史预测数据。结果显示，我们的机器学习方法在预测海上雾的可见度低于或等于1公里的预测前60小时的情况下，表现出色，提高了可见度预测的抽象率（POD），同时降低了假阳性率（FAR）。<details>
<summary>Abstract</summary>
Accurate and timely prediction of sea fog is very important for effectively managing maritime and coastal economic activities. Given the intricate nature and inherent variability of sea fog, traditional numerical and statistical forecasting methods are often proven inadequate. This study aims to develop an advanced sea fog forecasting method embedded in a numerical weather prediction model using the Yangtze River Estuary (YRE) coastal area as a case study. Prior to training our machine learning model, we employ a time-lagged correlation analysis technique to identify key predictors and decipher the underlying mechanisms driving sea fog occurrence. In addition, we implement ensemble learning and a focal loss function to address the issue of imbalanced data, thereby enhancing the predictive ability of our model. To verify the accuracy of our method, we evaluate its performance using a comprehensive dataset spanning one year, which encompasses both weather station observations and historical forecasts. Remarkably, our machine learning-based approach surpasses the predictive performance of two conventional methods, the weather research and forecasting nonhydrostatic mesoscale model (WRF-NMM) and the algorithm developed by the National Oceanic and Atmospheric Administration (NOAA) Forecast Systems Laboratory (FSL). Specifically, in regard to predicting sea fog with a visibility of less than or equal to 1 km with a lead time of 60 hours, our methodology achieves superior results by increasing the probability of detection (POD) while simultaneously reducing the false alarm ratio (FAR).
</details>
<details>
<summary>摘要</summary>
Traditional numerical and statistical forecasting methods are often inadequate for accurately predicting sea fog due to its complex and inherently variable nature. This study aims to develop an advanced sea fog forecasting method using a numerical weather prediction model and machine learning techniques, with the Yangtze River Estuary (YRE) coastal area as a case study.Before training our machine learning model, we use a time-lagged correlation analysis technique to identify key predictors and understand the underlying mechanisms driving sea fog occurrence. We also employ ensemble learning and a focal loss function to address the issue of imbalanced data, which enhances the predictive ability of our model.To evaluate the accuracy of our method, we use a comprehensive dataset covering one year, which includes both weather station observations and historical forecasts. Our machine learning-based approach outperforms two conventional methods, the weather research and forecasting nonhydrostatic mesoscale model (WRF-NMM) and the algorithm developed by the National Oceanic and Atmospheric Administration (NOAA) Forecast Systems Laboratory (FSL). Specifically, our methodology achieves better results in predicting sea fog with a visibility of less than or equal to 1 km with a lead time of 60 hours, with higher probability of detection (POD) and lower false alarm ratio (FAR).
</details></li>
</ul>
<hr>
<h2 id="SecureBoost-Hyperparameter-Tuning-via-Multi-Objective-Federated-Learning"><a href="#SecureBoost-Hyperparameter-Tuning-via-Multi-Objective-Federated-Learning" class="headerlink" title="SecureBoost Hyperparameter Tuning via Multi-Objective Federated Learning"></a>SecureBoost Hyperparameter Tuning via Multi-Objective Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10579">http://arxiv.org/abs/2307.10579</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyao Ren, Yan Kang, Lixin Fan, Linghua Yang, Yongxin Tong, Qiang Yang</li>
<li>for: 这篇研究旨在提高vertical federated learning中的数据隐私保护和可读性，通过使用树茨增强和同时保护数据隐私。</li>
<li>methods: 本研究使用了SecureBoost算法，并提出了一个名为 Constrained Multi-Objective SecureBoost (CMOSB) 的算法来选择最佳的参数设定，以实现最佳的折冲点。</li>
<li>results: 实验结果显示，CMOSB 可以获得不仅比基eline更好的参数设定，而且可以找到最佳的参数设定，以满足不同的参数需求。<details>
<summary>Abstract</summary>
SecureBoost is a tree-boosting algorithm leveraging homomorphic encryption to protect data privacy in vertical federated learning setting. It is widely used in fields such as finance and healthcare due to its interpretability, effectiveness, and privacy-preserving capability. However, SecureBoost suffers from high computational complexity and risk of label leakage. To harness the full potential of SecureBoost, hyperparameters of SecureBoost should be carefully chosen to strike an optimal balance between utility, efficiency, and privacy. Existing methods either set hyperparameters empirically or heuristically, which are far from optimal. To fill this gap, we propose a Constrained Multi-Objective SecureBoost (CMOSB) algorithm to find Pareto optimal solutions that each solution is a set of hyperparameters achieving optimal tradeoff between utility loss, training cost, and privacy leakage. We design measurements of the three objectives. In particular, the privacy leakage is measured using our proposed instance clustering attack. Experimental results demonstrate that the CMOSB yields not only hyperparameters superior to the baseline but also optimal sets of hyperparameters that can support the flexible requirements of FL participants.
</details>
<details>
<summary>摘要</summary>
《secureboost》是一种树融合算法，利用同质加密保护数据隐私在垂直联合学习设置下。它在银行和医疗等领域广泛应用，因为它具有可读性、效果和隐私保护能力。然而，secureboost受到高计算复杂度和标签泄露的风险。为了激活secureboost的潜力，需要仔细选择secureboost的Hyperparameter，以达到优质、效率和隐私三方面的丰富平衡。现有的方法可以通过实验或euristic来设置Hyperparameter，但这些方法远远不够优化。为了填补这个空白，我们提出了一种受限制多目标secureboost（CMOSB）算法，找到Pareto优解，每个解是一组Hyperparameter，实现了数据损失、训练成本和隐私泄露之间的优质平衡。我们设计了三个目标的度量。具体来说，隐私泄露被我们提出的实例划分攻击来度量。实验结果表明，CMOSB可以不仅提供超过基准的Hyperparameter，还可以找到优质的多个Hyperparameter，以满足FL参与者的灵活要求。
</details></li>
</ul>
<hr>
<h2 id="Boosting-Federated-Learning-Convergence-with-Prototype-Regularization"><a href="#Boosting-Federated-Learning-Convergence-with-Prototype-Regularization" class="headerlink" title="Boosting Federated Learning Convergence with Prototype Regularization"></a>Boosting Federated Learning Convergence with Prototype Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10575">http://arxiv.org/abs/2307.10575</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Qiao, Huy Q. Le, Choong Seon Hong</li>
<li>for: 提高 Federated Learning 中数据不均衡的问题</li>
<li>methods: 使用 Prototype-based 常数 regularization 策略</li>
<li>results: 在 MNIST 和 Fashion-MNIST 上实现了平均测试精度提高3.3% 和8.9%，比 FedAvg 最佳参考值更高。同时，在不均衡 Setting 下具有快速收敛速率。<details>
<summary>Abstract</summary>
As a distributed machine learning technique, federated learning (FL) requires clients to collaboratively train a shared model with an edge server without leaking their local data. However, the heterogeneous data distribution among clients often leads to a decrease in model performance. To tackle this issue, this paper introduces a prototype-based regularization strategy to address the heterogeneity in the data distribution. Specifically, the regularization process involves the server aggregating local prototypes from distributed clients to generate a global prototype, which is then sent back to the individual clients to guide their local training. The experimental results on MNIST and Fashion-MNIST show that our proposal achieves improvements of 3.3% and 8.9% in average test accuracy, respectively, compared to the most popular baseline FedAvg. Furthermore, our approach has a fast convergence rate in heterogeneous settings.
</details>
<details>
<summary>摘要</summary>
为了应对分布式机器学习技术中的数据分布不均问题，本文提出了一种基于原型的规范约束策略。具体来说，在服务器端，通过将分布式客户端的本地原型聚合到一个全局原型中，然后将全局原型发送回到各个客户端，以便在本地训练中作为指导。实验结果表明，与最常用的基准方法FedAvg相比，我们的方案在MNIST和Fashion-MNIST dataset上的平均测试准确率提高了3.3%和8.9%。此外，我们的方法在不同数据分布情况下具有快速收敛速率。
</details></li>
</ul>
<hr>
<h2 id="Deceptive-Alignment-Monitoring"><a href="#Deceptive-Alignment-Monitoring" class="headerlink" title="Deceptive Alignment Monitoring"></a>Deceptive Alignment Monitoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10569">http://arxiv.org/abs/2307.10569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andres Carranza, Dhruv Pai, Rylan Schaeffer, Arnuv Tandon, Sanmi Koyejo</li>
<li>for: 防止大型机器学习模型的潜在偏误行为，即模型在外表看起来合理但实际上在隐私 modify 其行为的情况。</li>
<li>methods: 这篇论文认为，以下几个领域的研究将在未来对掩饰偏误的监测起到关键作用：多任务学习、逻辑学习、推荐系统、自然语言处理等。</li>
<li>results: 本论文认为，这些领域的研究将带来长期挑战和新的研究机遇，同时也会对掩饰偏误的监测提供新的思路和方法。<details>
<summary>Abstract</summary>
As the capabilities of large machine learning models continue to grow, and as the autonomy afforded to such models continues to expand, the spectre of a new adversary looms: the models themselves. The threat that a model might behave in a seemingly reasonable manner, while secretly and subtly modifying its behavior for ulterior reasons is often referred to as deceptive alignment in the AI Safety & Alignment communities. Consequently, we call this new direction Deceptive Alignment Monitoring. In this work, we identify emerging directions in diverse machine learning subfields that we believe will become increasingly important and intertwined in the near future for deceptive alignment monitoring, and we argue that advances in these fields present both long-term challenges and new research opportunities. We conclude by advocating for greater involvement by the adversarial machine learning community in these emerging directions.
</details>
<details>
<summary>摘要</summary>
large machine learning models的能力不断增长，自动化程度也在不断扩展，但隐藏在这些模型中的新敌人开始显现：模型本身。这种情况被称为“欺骗吧效”（deceptive alignment）在AI安全与对齐社区中，我们称这个新方向为“欺骗对齐监测”（Deceptive Alignment Monitoring）。在这篇文章中，我们认为未来几年将成为核心的多种机器学习子领域会变得越来越重要，这些领域包括：* 模型内部异常检测（Anomaly Detection in Models）* 模型之间的对抗（Model Adversarial Training）* 模型的隐藏状态检测（Detection of Hidden States in Models）* 模型的追踪和回归（Model Tracking and Recall）我们认为，这些领域的发展将带来长期挑战和新的研究机会。我们建议对抗机器学习社区更加参与这些领域的研究。
</details></li>
</ul>
<hr>
<h2 id="FACADE-A-Framework-for-Adversarial-Circuit-Anomaly-Detection-and-Evaluation"><a href="#FACADE-A-Framework-for-Adversarial-Circuit-Anomaly-Detection-and-Evaluation" class="headerlink" title="FACADE: A Framework for Adversarial Circuit Anomaly Detection and Evaluation"></a>FACADE: A Framework for Adversarial Circuit Anomaly Detection and Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10563">http://arxiv.org/abs/2307.10563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhruv Pai, Andres Carranza, Rylan Schaeffer, Arnuv Tandon, Sanmi Koyejo</li>
<li>for: 提高模型Robustness和可扩展的模型监测</li>
<li>methods: 使用probabilistic和几何方法进行不supervised机器学习中的自动异常检测</li>
<li>results: 生成probabilistic分布于环境中，提供高级别异常检测和模型Robustness的工具<details>
<summary>Abstract</summary>
We present FACADE, a novel probabilistic and geometric framework designed for unsupervised mechanistic anomaly detection in deep neural networks. Its primary goal is advancing the understanding and mitigation of adversarial attacks. FACADE aims to generate probabilistic distributions over circuits, which provide critical insights to their contribution to changes in the manifold properties of pseudo-classes, or high-dimensional modes in activation space, yielding a powerful tool for uncovering and combating adversarial attacks. Our approach seeks to improve model robustness, enhance scalable model oversight, and demonstrates promising applications in real-world deployment settings.
</details>
<details>
<summary>摘要</summary>
我们介绍FACADE，一种新的 probabilistic和几何框架，用于无监督机器学习模型中的不可预测异常检测。其主要目标是提高对抗攻击的理解和防御。FACADE通过生成征值分布来提供关键的征值对模型 pseudo-class 或高维模式空间的变化 Properties的理解，从而提供一种有力的抗击攻击工具。我们的方法可以提高模型的可靠性，提高可扩展的模型监测，并在实际应用中显示了有前途的应用。Note that Simplified Chinese is used here, which is the standard writing system used in mainland China. Traditional Chinese is also widely used, especially in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="Shared-Adversarial-Unlearning-Backdoor-Mitigation-by-Unlearning-Shared-Adversarial-Examples"><a href="#Shared-Adversarial-Unlearning-Backdoor-Mitigation-by-Unlearning-Shared-Adversarial-Examples" class="headerlink" title="Shared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial Examples"></a>Shared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial Examples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10562">http://arxiv.org/abs/2307.10562</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaokui Wei, Mingda Zhang, Hongyuan Zha, Baoyuan Wu</li>
<li>For: The paper is written for mitigating backdoor attacks in machine learning models.* Methods: The paper uses adversarial training techniques to purify a backdoored model using a small clean dataset.* Results: The proposed method, Shared Adversarial Unlearning (SAU), achieves state-of-the-art performance for backdoor defense on various benchmark datasets and network architectures.<details>
<summary>Abstract</summary>
Backdoor attacks are serious security threats to machine learning models where an adversary can inject poisoned samples into the training set, causing a backdoored model which predicts poisoned samples with particular triggers to particular target classes, while behaving normally on benign samples. In this paper, we explore the task of purifying a backdoored model using a small clean dataset. By establishing the connection between backdoor risk and adversarial risk, we derive a novel upper bound for backdoor risk, which mainly captures the risk on the shared adversarial examples (SAEs) between the backdoored model and the purified model. This upper bound further suggests a novel bi-level optimization problem for mitigating backdoor using adversarial training techniques. To solve it, we propose Shared Adversarial Unlearning (SAU). Specifically, SAU first generates SAEs, and then, unlearns the generated SAEs such that they are either correctly classified by the purified model and/or differently classified by the two models, such that the backdoor effect in the backdoored model will be mitigated in the purified model. Experiments on various benchmark datasets and network architectures show that our proposed method achieves state-of-the-art performance for backdoor defense.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传统的反向工程学习模型中存在背门风险，敌对者可以在训练集中植入恶意样本，导致背门模型预测恶意样本时特定的触发器和目标类，而正常样本则正常预测。在这篇论文中，我们研究了使用小量净化数据来纯化背门模型。通过证明背门风险和敌对风险之间的连接，我们 derive了一个 noval upper bound for 背门风险，该bound主要捕捉了在背门模型和纯化模型之间共享的敌对例子（SAEs）中的风险。这个bound还提出了一个 novel bi-level优化问题，用于 mitigating 背门风险。为解决这个问题，我们提出了 Shared Adversarial Unlearning（SAU）。具体来说，SAU首先生成SAEs，然后对生成的SAEs进行反学习，使其被纯化模型正确分类或者被两个模型不同分类，从而 Mitigate the backdoor effect in the backdoored model in the purified model。在各种 benchmark 数据集和网络架构上进行了实验，我们的提出的方法实现了对背门防御的state-of-the-art表现。
</details></li>
</ul>
<hr>
<h2 id="Post-variational-quantum-neural-networks"><a href="#Post-variational-quantum-neural-networks" class="headerlink" title="Post-variational quantum neural networks"></a>Post-variational quantum neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10560">http://arxiv.org/abs/2307.10560</a></li>
<li>repo_url: None</li>
<li>paper_authors: Po-Wei Huang, Patrick Rebentrost</li>
<li>for: 提高量子计算机的计算能力，解决现有级别的硬件不足问题。</li>
<li>methods: 使用杂合量子-经典计算机和变量算法，并采用后续策略来调整量子计算机的参数。</li>
<li>results: 在实际应用中，如手写数字识别，实现96%的分类精度。<details>
<summary>Abstract</summary>
Quantum computing has the potential to provide substantial computational advantages over current state-of-the-art classical supercomputers. However, current hardware is not advanced enough to execute fault-tolerant quantum algorithms. An alternative of using hybrid quantum-classical computing with variational algorithms can exhibit barren plateau issues, causing slow convergence of gradient-based optimization techniques. In this paper, we discuss "post-variational strategies", which shift tunable parameters from the quantum computer to the classical computer, opting for ensemble strategies when optimizing quantum models. We discuss various strategies and design principles for constructing individual quantum circuits, where the resulting ensembles can be optimized with convex programming. Further, we discuss architectural designs of post-variational quantum neural networks and analyze the propagation of estimation errors throughout such neural networks. Lastly, we show that our algorithm can be applied to real-world applications such as image classification on handwritten digits, producing a 96% classification accuracy.
</details>
<details>
<summary>摘要</summary>
量子计算有可能提供对当前最高标准的类别超级计算机的重要计算优势。然而，当前硬件还不够先进，无法执行错误tolerant的量子算法。为了解决这个问题，我们可以使用杂合量子-类别计算，使用变量算法。然而，这会导致杂合板块问题，使得梯度基于优化技术的收敛速度变得非常慢。在这篇论文中，我们讨论了“后variational策略”，即将可调参数从量子计算机Shift到类别计算机上，选择 ensemble策略来优化量子模型。我们介绍了不同的策略和设计原则，用于构建个体量子电路，以及对这些集合进行优化的几何编程。此外，我们还讨论了post-variational量子神经网络的建筑设计，并分析了这些神经网络中的估计错误的传播。最后，我们展示了我们的算法可以应用于真实的应用场景，如手写数字图像识别，并实现了96%的分类精度。
</details></li>
</ul>
<hr>
<h2 id="Air-Traffic-Controller-Workload-Level-Prediction-using-Conformalized-Dynamical-Graph-Learning"><a href="#Air-Traffic-Controller-Workload-Level-Prediction-using-Conformalized-Dynamical-Graph-Learning" class="headerlink" title="Air Traffic Controller Workload Level Prediction using Conformalized Dynamical Graph Learning"></a>Air Traffic Controller Workload Level Prediction using Conformalized Dynamical Graph Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10559">http://arxiv.org/abs/2307.10559</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ymlasu/para-atm-collection">https://github.com/ymlasu/para-atm-collection</a></li>
<li>paper_authors: Yutian Pang, Jueming Hu, Christopher S. Lieber, Nancy J. Cooke, Yongming Liu</li>
<li>for: 这篇论文主要是为了预测空交控制员（ATCo）的劳动负担，以避免过载和确保操作安全性和空域使用效率。</li>
<li>methods: 论文首先进行了对空交工作负担的研究综述，主要从空交的角度进行了分析。然后， authors 介绍了一种基于人类 loop（HITL）的实验设置，使用退休空交控制员进行了 simulations，并从三个鹊翼接近场景中获取了空交数据和劳动负担标签。</li>
<li>results: 实验结果表明， besides 交通密度特征，交通冲突特征也对劳动负担预测具有贡献（例如最小水平&#x2F;垂直距离）。 direct 从空间时间图Layout中学习Graph neural network可以实现更高的预测精度，比手动制作的交通复杂度特征更有效。 conformal prediction 是一种有用的工具，可以进一步提高模型预测精度，生成一个范围内的预测工作负担标签。<details>
<summary>Abstract</summary>
Air traffic control (ATC) is a safety-critical service system that demands constant attention from ground air traffic controllers (ATCos) to maintain daily aviation operations. The workload of the ATCos can have negative effects on operational safety and airspace usage. To avoid overloading and ensure an acceptable workload level for the ATCos, it is important to predict the ATCos' workload accurately for mitigation actions. In this paper, we first perform a review of research on ATCo workload, mostly from the air traffic perspective. Then, we briefly introduce the setup of the human-in-the-loop (HITL) simulations with retired ATCos, where the air traffic data and workload labels are obtained. The simulations are conducted under three Phoenix approach scenarios while the human ATCos are requested to self-evaluate their workload ratings (i.e., low-1 to high-7). Preliminary data analysis is conducted. Next, we propose a graph-based deep-learning framework with conformal prediction to identify the ATCo workload levels. The number of aircraft under the controller's control varies both spatially and temporally, resulting in dynamically evolving graphs. The experiment results suggest that (a) besides the traffic density feature, the traffic conflict feature contributes to the workload prediction capabilities (i.e., minimum horizontal/vertical separation distance); (b) directly learning from the spatiotemporal graph layout of airspace with graph neural network can achieve higher prediction accuracy, compare to hand-crafted traffic complexity features; (c) conformal prediction is a valuable tool to further boost model prediction accuracy, resulting a range of predicted workload labels. The code used is available at \href{https://github.com/ymlasu/para-atm-collection/blob/master/air-traffic-prediction/ATC-Workload-Prediction/}{$\mathsf{Link}$}.
</details>
<details>
<summary>摘要</summary>
空交控制（ATC）是一个安全关键的服务系统，需要地面空交控制员（ATCo）不断关注，以维护日常航空运营。ATCo的工作负担可能会影响航空安全和空域使用。为了避免过载和保持可接受的工作负担水平，需要准确预测ATCo的工作负担。在这篇论文中，我们首先进行了研究人员对ATCo工作负担的查询，主要来自航空交通的视角。然后，我们简要介绍了使用退休空交控制员（HITL）的人类在Loop（SIM） simulations的设置，其中获取了航空交通数据和工作负担标签。在三个鸡毛蒜蓉approach场景下，人类ATCo被请求自我评估他们的工作负担等级（即低1到高7）。我们进行了初步数据分析。接着，我们提议了一个基于图的深度学习框架，用于预测ATCo工作负担水平。由于空交控制员控制的飞机数量在空间和时间上都变化，因此导致的是动态演化的图。实验结果表明：（a）除了交通密度特征，交通冲突特征也对工作负担预测具有贡献（即最小水平/垂直分离距离）；（b）直接从空间时间图的空间图结构中学习，使用图神经网络可以达到更高的预测精度，比手工设计的交通复杂度特征更好；（c）使用兼容预测可以进一步提高模型预测精度，得到一个范围内的预测工作负担等级。代码可以在 $\mathsf{Link}$ 中找到。
</details></li>
</ul>
<hr>
<h2 id="SC-VALL-E-Style-Controllable-Zero-Shot-Text-to-Speech-Synthesizer"><a href="#SC-VALL-E-Style-Controllable-Zero-Shot-Text-to-Speech-Synthesizer" class="headerlink" title="SC VALL-E: Style-Controllable Zero-Shot Text to Speech Synthesizer"></a>SC VALL-E: Style-Controllable Zero-Shot Text to Speech Synthesizer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10550">http://arxiv.org/abs/2307.10550</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/0913ktg/sc_vall-e">https://github.com/0913ktg/sc_vall-e</a></li>
<li>paper_authors: Daegyeom Kim, Seongho Hong, Yong-Hoon Choi<br>for: 这个论文主要目标是提出一种基于神经编码器语言模型（VALL-E）的风格控制（SC）模型，用于生成可控的语音。methods: 该 SC VALL-E 模型使用了各种各样的Speaker、情感和说话风格等特征来控制语音的多种特征，并通过设计新的风格网络来控制这些特征。results: 我们通过对三种表达性语音合成模型进行比较性试验，发现 SC VALL-E 模型可以在不使用训练数据中的提示音时产生多种表达性的声音，并且与现有模型相比具有竞争力。<details>
<summary>Abstract</summary>
Expressive speech synthesis models are trained by adding corpora with diverse speakers, various emotions, and different speaking styles to the dataset, in order to control various characteristics of speech and generate the desired voice. In this paper, we propose a style control (SC) VALL-E model based on the neural codec language model (called VALL-E), which follows the structure of the generative pretrained transformer 3 (GPT-3). The proposed SC VALL-E takes input from text sentences and prompt audio and is designed to generate controllable speech by not simply mimicking the characteristics of the prompt audio but by controlling the attributes to produce diverse voices. We identify tokens in the style embedding matrix of the newly designed style network that represent attributes such as emotion, speaking rate, pitch, and voice intensity, and design a model that can control these attributes. To evaluate the performance of SC VALL-E, we conduct comparative experiments with three representative expressive speech synthesis models: global style token (GST) Tacotron2, variational autoencoder (VAE) Tacotron2, and original VALL-E. We measure word error rate (WER), F0 voiced error (FVE), and F0 gross pitch error (F0GPE) as evaluation metrics to assess the accuracy of generated sentences. For comparing the quality of synthesized speech, we measure comparative mean option score (CMOS) and similarity mean option score (SMOS). To evaluate the style control ability of the generated speech, we observe the changes in F0 and mel-spectrogram by modifying the trained tokens. When using prompt audio that is not present in the training data, SC VALL-E generates a variety of expressive sounds and demonstrates competitive performance compared to the existing models. Our implementation, pretrained models, and audio samples are located on GitHub.
</details>
<details>
<summary>摘要</summary>
干脆的语音合成模型通过添加多个说话者、不同的情感和说话风格到数据集来控制各种语音特征并生成愿望的声音。在这篇论文中，我们提出了基于神经码器语言模型（VALL-E）的样式控制（SC）VALL-E模型。该模型根据生成预训练转换器3（GPT-3）的结构而设计，可以从文本句子输入和提示音采样生成可控的语音。我们在新设计的风格网络中标识了表达不同属性的 токен，如情感、说话速度、音高和声音强度，并设计了一个可以控制这些属性的模型。为了评估SC VALLE的性能，我们对三种表达性语音合成模型进行比较性实验：全球风格标识符（GST）Tacotron2、变量自适应器（VAE）Tacotron2以及原始VALL-E。我们使用word error rate（WER）、F0 voiced error（FVE）和F0 gross pitch error（F0GPE）作为评估指标，以评估生成句子的准确性。为了比较合成的质量，我们使用相对意见分（CMOS）和相似意见分（SMOS）作为评估指标。为了评估生成的样式控制能力，我们观察改变训练的token后的F0和mel-spectrogram变化。当使用不在训练数据中的提示音时，SC VALL-E可以生成多种表达性的声音，并与现有模型相比具有竞争力。我们的实现、预训练模型和声音样本可以在GitHub上找到。
</details></li>
</ul>
<hr>
<h2 id="Differentially-Flat-Learning-based-Model-Predictive-Control-Using-a-Stability-State-and-Input-Constraining-Safety-Filter"><a href="#Differentially-Flat-Learning-based-Model-Predictive-Control-Using-a-Stability-State-and-Input-Constraining-Safety-Filter" class="headerlink" title="Differentially Flat Learning-based Model Predictive Control Using a Stability, State, and Input Constraining Safety Filter"></a>Differentially Flat Learning-based Model Predictive Control Using a Stability, State, and Input Constraining Safety Filter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10541">http://arxiv.org/abs/2307.10541</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/utiasdsl/fmpc_socp">https://github.com/utiasdsl/fmpc_socp</a></li>
<li>paper_authors: Adam W. Hall, Melissa Greeff, Angela P. Schoellig</li>
<li>for: 控制未知系统 using past trajectory data 和学习的系统动力学模型</li>
<li>methods: 使用学习的非线性动力学模型进行控制，包括使用线性化approximation和非线性优化方法</li>
<li>results: 提出一种新的非线性控制器，通过使用差分平坦性来实现类似于现有学习基于控制器的性能，但具有显著更好的计算效率，同时还能够满足稳定性和输入和平态约束的要求。<details>
<summary>Abstract</summary>
Learning-based optimal control algorithms control unknown systems using past trajectory data and a learned model of the system dynamics. These controllers use either a linear approximation of the learned dynamics, trading performance for faster computation, or nonlinear optimization methods, which typically perform better but can limit real-time applicability. In this work, we present a novel nonlinear controller that exploits differential flatness to achieve similar performance to state-of-the-art learning-based controllers but with significantly less computational effort. Differential flatness is a property of dynamical systems whereby nonlinear systems can be exactly linearized through a nonlinear input mapping. Here, the nonlinear transformation is learned as a Gaussian process and is used in a safety filter that guarantees, with high probability, stability as well as input and flat state constraint satisfaction. This safety filter is then used to refine inputs from a flat model predictive controller to perform constrained nonlinear learning-based optimal control through two successive convex optimizations. We compare our method to state-of-the-art learning-based control strategies and achieve similar performance, but with significantly better computational efficiency, while also respecting flat state and input constraints, and guaranteeing stability.
</details>
<details>
<summary>摘要</summary>
学习基于的优化控制算法控制未知系统使用过去轨迹数据和学习到系统动力学模型。这些控制器使用线性化了学习的动力学模型，换取性能的快速计算，或非线性优化方法，通常表现更好，但可能限制实时应用。在这项工作中，我们介绍了一种新的非线性控制器，利用差分平坦性来实现与当前状态艺学基于学习控制的性能相似，但计算占用 significatively less。差分平坦性是动力系统的属性，其中非线性系统可以通过非线性输入映射来简化为线性系统。在这里，非线性变换是通过 Gaussian 过程学习的，并用于安全筛选器，确保高概率稳定性和输入和平态约束的满足。这个安全筛选器然后用于修改来自平面预测控制器的输入，以进行约束非线性学习optimal控制，通过两个连续的几何优化来完成。我们与当前学习控制策略进行比较，实现相似的性能，但计算效率 significatively better，同时也遵守平态和输入约束，并保证稳定性。
</details></li>
</ul>
<hr>
<h2 id="The-Extractive-Abstractive-Axis-Measuring-Content-“Borrowing”-in-Generative-Language-Models"><a href="#The-Extractive-Abstractive-Axis-Measuring-Content-“Borrowing”-in-Generative-Language-Models" class="headerlink" title="The Extractive-Abstractive Axis: Measuring Content “Borrowing” in Generative Language Models"></a>The Extractive-Abstractive Axis: Measuring Content “Borrowing” in Generative Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11779">http://arxiv.org/abs/2307.11779</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nedelina Teneva</li>
<li>for: 本研究旨在探讨生成模型的抽象性和内容授权问题，并提出了EXTRACTIVE-ABSTRACTIVE轴作为评价生成模型的指标。</li>
<li>methods: 本研究使用了多种生成模型，包括Transformer和Language Model，并对其进行了评价和比较。</li>
<li>results: 研究发现，生成模型的抽象性和内容授权问题存在着挑战，而EXTRACTIVE-ABSTRACTIVE轴可以帮助解决这些问题。<details>
<summary>Abstract</summary>
Generative language models produce highly abstractive outputs by design, in contrast to extractive responses in search engines. Given this characteristic of LLMs and the resulting implications for content Licensing & Attribution, we propose the the so-called Extractive-Abstractive axis for benchmarking generative models and highlight the need for developing corresponding metrics, datasets and annotation guidelines. We limit our discussion to the text modality.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本传输到简化中文。<</SYS>>生成语言模型会生成高度抽象的输出，与搜索引擎的EXTRACTIVE响应不同，这些特点和内容授权、著作权带来了重要的后果。我们提出EXTRACTIVE-ABSTRACTIVE轴作为生成模型的评估标准，并需要开发相应的指标、数据集和注释指南。我们在文本模式下限制我们的讨论。
</details></li>
</ul>
<hr>
<h2 id="Fast-Unsupervised-Deep-Outlier-Model-Selection-with-Hypernetworks"><a href="#Fast-Unsupervised-Deep-Outlier-Model-Selection-with-Hypernetworks" class="headerlink" title="Fast Unsupervised Deep Outlier Model Selection with Hypernetworks"></a>Fast Unsupervised Deep Outlier Model Selection with Hypernetworks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10529">http://arxiv.org/abs/2307.10529</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueying Ding, Yue Zhao, Leman Akoglu</li>
<li>for: 本研究的目的是提出一种有效的深度神经网络基于Outlier Detection（OD）模型选择和hyperparameter（HP）调整方法，以解决现代OD模型中HP的多样性和 validation 问题。</li>
<li>methods: 本研究提出了一种名为HYPER的方法，它通过设计和训练一个新的 hypernetwork（HN），使得HN可以将hyperparameters（HP）映射到OD模型的优化参数上，从而实现高效的HP调整和模型选择。此外，HYPER还使用了meta-学习技术来train一个历史OD任务的标签预测函数，以便高效地进行验证。</li>
<li>results: 在35个OD任务上进行了广泛的实验，结果显示，HYPER可以与8个基elines相比，达到高性能水平，同时具有显著的效率提升。<details>
<summary>Abstract</summary>
Outlier detection (OD) finds many applications with a rich literature of numerous techniques. Deep neural network based OD (DOD) has seen a recent surge of attention thanks to the many advances in deep learning. In this paper, we consider a critical-yet-understudied challenge with unsupervised DOD, that is, effective hyperparameter (HP) tuning/model selection. While several prior work report the sensitivity of OD models to HPs, it becomes ever so critical for the modern DOD models that exhibit a long list of HPs. We introduce HYPER for tuning DOD models, tackling two fundamental challenges: (1) validation without supervision (due to lack of labeled anomalies), and (2) efficient search of the HP/model space (due to exponential growth in the number of HPs). A key idea is to design and train a novel hypernetwork (HN) that maps HPs onto optimal weights of the main DOD model. In turn, HYPER capitalizes on a single HN that can dynamically generate weights for many DOD models (corresponding to varying HPs), which offers significant speed-up. In addition, it employs meta-learning on historical OD tasks with labels to train a proxy validation function, likewise trained with our proposed HN efficiently. Extensive experiments on 35 OD tasks show that HYPER achieves high performance against 8 baselines with significant efficiency gains.
</details>
<details>
<summary>摘要</summary>
OUTLIER DETECTION (OD) 在多个应用程序中找到了广泛的应用，文献中也有丰富的技术。深度神经网络基于的 OUTLIER DETECTION (DOD) 在最近几年内受到了广泛关注，因为深度学习的多种进步。在这篇论文中，我们考虑了一个critical yet understudied challenge，即无监督的 OUTLIER DETECTION 模型选择和参数调整。虽然先前的研究表明了 OUTLIER DETECTION 模型对参数的敏感性，但在现代 DOD 模型中，这种敏感性变得更加重要。我们提出了 HYPER 来调整 DOD 模型，解决两个基本挑战：（1）无监督验证（由于缺乏异常数据标注），（2）高效地搜索参数/模型空间（由于参数的增加）。我们的关键想法是设计并训练一个新的 hypernetwork（HN），将参数映射到 OUTLIER DETECTION 模型的优化参数。然后，HYPER 利用这个 HN 可以动态生成多个 DOD 模型（对应于不同的参数），从而提供了显著的加速。此外，它还利用元学习来训练一个历史 OUTLIER DETECTION 任务的代理验证函数，如wise 训练了我们所提出的 HN，高效地进行了训练。我们的实验表明，HYPER 可以高效地与 8 个基elines 进行比较，并且在 35 个 OUTLIER DETECTION 任务上达到高性能。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Black-Box-Advice-Learning-Augmented-Algorithms-for-MDPs-with-Q-Value-Predictions"><a href="#Beyond-Black-Box-Advice-Learning-Augmented-Algorithms-for-MDPs-with-Q-Value-Predictions" class="headerlink" title="Beyond Black-Box Advice: Learning-Augmented Algorithms for MDPs with Q-Value Predictions"></a>Beyond Black-Box Advice: Learning-Augmented Algorithms for MDPs with Q-Value Predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10524">http://arxiv.org/abs/2307.10524</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tongxin Li, Yiheng Lin, Shaolei Ren, Adam Wierman</li>
<li>for: 该论文研究了在单轨迹时变Markov决策过程（MDP）中的一致性和可靠性之间的贸易。</li>
<li>methods: 该论文使用了Q值建议，并考虑了建议生成过程中的额外信息，以实现更好的性能保证。</li>
<li>results: 研究结果表明，通过利用Q值建议，可以在MDP模型中实现近似优化的性能保证，比黑盒建议更好。<details>
<summary>Abstract</summary>
We study the tradeoff between consistency and robustness in the context of a single-trajectory time-varying Markov Decision Process (MDP) with untrusted machine-learned advice. Our work departs from the typical approach of treating advice as coming from black-box sources by instead considering a setting where additional information about how the advice is generated is available. We prove a first-of-its-kind consistency and robustness tradeoff given Q-value advice under a general MDP model that includes both continuous and discrete state/action spaces. Our results highlight that utilizing Q-value advice enables dynamic pursuit of the better of machine-learned advice and a robust baseline, thus result in near-optimal performance guarantees, which provably improves what can be obtained solely with black-box advice.
</details>
<details>
<summary>摘要</summary>
我们研究了在单轨时间变化Markov决策过程（MDP）中的一致性和可靠性之间的贸易做。我们的工作与 Typical approach不同，不再将建议视为黑盒来源，而是考虑一种情况，在建议生成的additional information可以获得。我们证明了一种首次的一致性和可靠性贸易，基于Q值建议在通用MDP模型中，该模型包括连续和离散状态/动作空间。我们的结果表明，通过利用Q值建议，可以动态追求机器学习建议和可靠基线之间的最佳选择，从而实现近似最佳性能保证，这超越了单纯黑盒建议所能获得的性能。
</details></li>
</ul>
<hr>
<h2 id="Prediction-of-Handball-Matches-with-Statistically-Enhanced-Learning-via-Estimated-Team-Strengths"><a href="#Prediction-of-Handball-Matches-with-Statistically-Enhanced-Learning-via-Estimated-Team-Strengths" class="headerlink" title="Prediction of Handball Matches with Statistically Enhanced Learning via Estimated Team Strengths"></a>Prediction of Handball Matches with Statistically Enhanced Learning via Estimated Team Strengths</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11777">http://arxiv.org/abs/2307.11777</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florian Felice, Christophe Ley</li>
<li>for: 这个论文是为了预测手球赛事而写的。</li>
<li>methods: 该论文使用了统计学加强学习（SEL）模型来预测手球赛事。</li>
<li>results: 论文中的机器学习模型，通过添加SEL特征，实现了超过80%的准确率。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
We propose a Statistically Enhanced Learning (aka. SEL) model to predict handball games. Our Machine Learning model augmented with SEL features outperforms state-of-the-art models with an accuracy beyond 80%. In this work, we show how we construct the data set to train Machine Learning models on past female club matches. We then compare different models and evaluate them to assess their performance capabilities. Finally, explainability methods allow us to change the scope of our tool from a purely predictive solution to a highly insightful analytical tool. This can become a valuable asset for handball teams' coaches providing valuable statistical and predictive insights to prepare future competitions.
</details>
<details>
<summary>摘要</summary>
我们提出一种统计增强学习（简称 SEL）模型，用于预测手球比赛。我们的机器学习模型，通过添加 SEL 特征，超过了当前最佳模型的准确率，达到了80%以上。在这个工作中，我们介绍了如何使用过去女子俱乐部比赛数据来训练机器学习模型。然后，我们比较了不同的模型，并评估了它们的性能能力。最后，我们使用可解释方法，将我们的工具从一个纯Predictive解决方案转变成一个具有高度探索性的分析工具，这可以成为手球队教练的价值观点，提供有价值的统计和预测信息，以准备未来的竞赛。
</details></li>
</ul>
<hr>
<h2 id="FedSoup-Improving-Generalization-and-Personalization-in-Federated-Learning-via-Selective-Model-Interpolation"><a href="#FedSoup-Improving-Generalization-and-Personalization-in-Federated-Learning-via-Selective-Model-Interpolation" class="headerlink" title="FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation"></a>FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10507">http://arxiv.org/abs/2307.10507</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minghui Chen, Meirui Jiang, Qi Dou, Zehua Wang, Xiaoxiao Li</li>
<li>for: 这篇论文旨在解决跨档案联合学习（FL）中的分布差异问题，尤其是对于专案化FL方法的适应性。</li>
<li>methods: 我们提出了一种新的联合模型汤（FedSoup），通过选择性地 interpolating模型参数来优化本地和全球性能的贸易。</li>
<li>results: 我们在静脉和病理图像分类任务上评估了我们的提案，并获得了明显的增进外部数据适应性。我们的代码可以在<a target="_blank" rel="noopener" href="https://github.com/ubc-tea/FedSoup%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ubc-tea/FedSoup中找到。</a><details>
<summary>Abstract</summary>
Cross-silo federated learning (FL) enables the development of machine learning models on datasets distributed across data centers such as hospitals and clinical research laboratories. However, recent research has found that current FL algorithms face a trade-off between local and global performance when confronted with distribution shifts. Specifically, personalized FL methods have a tendency to overfit to local data, leading to a sharp valley in the local model and inhibiting its ability to generalize to out-of-distribution data. In this paper, we propose a novel federated model soup method (i.e., selective interpolation of model parameters) to optimize the trade-off between local and global performance. Specifically, during the federated training phase, each client maintains its own global model pool by monitoring the performance of the interpolated model between the local and global models. This allows us to alleviate overfitting and seek flat minima, which can significantly improve the model's generalization performance. We evaluate our method on retinal and pathological image classification tasks, and our proposed method achieves significant improvements for out-of-distribution generalization. Our code is available at https://github.com/ubc-tea/FedSoup.
</details>
<details>
<summary>摘要</summary>
（简体中文）跨存储 silo 联合学习（FL）可以在数据中心如医院和临床研究室等处开发机器学习模型。然而，当面临分布偏移时，现有的 FL 算法会面临本地和全球性能之间的负担。具体来说，个性化 FL 方法往往会过拟合本地数据，导致本地模型峰值陡峰，阻碍其对于不同于本地数据的泛化性能。在这篇论文中，我们提出了一种新的联合模型汤（i.e., 选择性 interpolate 模型参数）来优化本地和全球性能之间的负担。具体来说，在联合训练阶段，每个客户端都会维护自己的全球模型池，并通过监测 interpolated 模型在本地和全球模型之间的性能来缓解过拟合。这可以有效地提高模型的泛化性能。我们在Retinal和pathological图像分类任务上评估了我们的方法，并发现我们的方法在不同于本地数据的泛化性能上具有显著改善。我们的代码可以在 https://github.com/ubc-tea/FedSoup 上找到。
</details></li>
</ul>
<hr>
<h2 id="Identifying-Interpretable-Subspaces-in-Image-Representations"><a href="#Identifying-Interpretable-Subspaces-in-Image-Representations" class="headerlink" title="Identifying Interpretable Subspaces in Image Representations"></a>Identifying Interpretable Subspaces in Image Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10504">http://arxiv.org/abs/2307.10504</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neha Kalibhat, Shweta Bhardwaj, Bayan Bruss, Hamed Firooz, Maziar Sanjabi, Soheil Feizi</li>
<li>for: 本文提出了一种自动Feature explanation的方法，用于解释图像表示的特征。</li>
<li>methods: 该方法使用了对比概念来描述目标特征的高度活跃图像，并使用了预训练的视觉语言模型like CLIP来生成描述。每个词在描述中得分和排名，从而导致一小组高度相似的人类可理解的概念，用于解释目标特征。</li>
<li>results: 该方法可以解释大多数现有模型中的特征，并且可以减少干扰特征。此外，该方法还可以在不同的表示空间之间传递概念。<details>
<summary>Abstract</summary>
We propose Automatic Feature Explanation using Contrasting Concepts (FALCON), an interpretability framework to explain features of image representations. For a target feature, FALCON captions its highly activating cropped images using a large captioning dataset (like LAION-400m) and a pre-trained vision-language model like CLIP. Each word among the captions is scored and ranked leading to a small number of shared, human-understandable concepts that closely describe the target feature. FALCON also applies contrastive interpretation using lowly activating (counterfactual) images, to eliminate spurious concepts. Although many existing approaches interpret features independently, we observe in state-of-the-art self-supervised and supervised models, that less than 20% of the representation space can be explained by individual features. We show that features in larger spaces become more interpretable when studied in groups and can be explained with high-order scoring concepts through FALCON. We discuss how extracted concepts can be used to explain and debug failures in downstream tasks. Finally, we present a technique to transfer concepts from one (explainable) representation space to another unseen representation space by learning a simple linear transformation.
</details>
<details>
<summary>摘要</summary>
我们提出了自动Feature解释使用对比概念（FALCON），一种可解性框架，用于解释图像表示中的特征。为目标特征，FALCON使用大量captioningdataset（如LAION-400m）和预训练的视觉语言模型（如CLIP）来caption高活跃的剪辑图像。每个单词在caption中被分数和排名，导致一小数量的共享、人类可理解的概念，准确地描述目标特征。FALCON还使用对比解释使用低活跃（counterfactual）图像，以消除假设性的概念。我们发现，许多现有的方法单独解释特征，但是我们在当前领域的自动驱动和监督模型中发现，只有 less than 20%的表示空间可以由单个特征解释。我们表明，在更大的表示空间中，特征在组合上变得更容易理解，可以通过FALCON高序分数概念来解释。我们讨论如何使用提取的概念来解释和调试下游任务中的失败。最后，我们提出了一种将概念从一个可解性表示空间传播到另一个未经见的表示空间的技术，通过学习简单的线性变换。
</details></li>
</ul>
<hr>
<h2 id="A-Competitive-Learning-Approach-for-Specialized-Models-A-Solution-for-Complex-Physical-Systems-with-Distinct-Functional-Regimes"><a href="#A-Competitive-Learning-Approach-for-Specialized-Models-A-Solution-for-Complex-Physical-Systems-with-Distinct-Functional-Regimes" class="headerlink" title="A Competitive Learning Approach for Specialized Models: A Solution for Complex Physical Systems with Distinct Functional Regimes"></a>A Competitive Learning Approach for Specialized Models: A Solution for Complex Physical Systems with Distinct Functional Regimes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10496">http://arxiv.org/abs/2307.10496</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/exploita123/charmedforfree">https://github.com/exploita123/charmedforfree</a></li>
<li>paper_authors: Okezzi F. Ukorigho, Opeoluwa Owoyele</li>
<li>for: 本研究旨在提出一种新的竞争学习方法，用于从数据中提取物理系统的数据驱动模型。</li>
<li>methods: 该方法employs动态损失函数，用于同时训练一组模型。每个模型在训练过程中竞争对每个观察数据，以便在数据中identify不同的功能 режи。</li>
<li>results: 研究表明，该方法能够成功地identify功能regime，发现真实的管理方程，并降低测试错误。<details>
<summary>Abstract</summary>
Complex systems in science and engineering sometimes exhibit behavior that changes across different regimes. Traditional global models struggle to capture the full range of this complex behavior, limiting their ability to accurately represent the system. In response to this challenge, we propose a novel competitive learning approach for obtaining data-driven models of physical systems. The primary idea behind the proposed approach is to employ dynamic loss functions for a set of models that are trained concurrently on the data. Each model competes for each observation during training, allowing for the identification of distinct functional regimes within the dataset. To demonstrate the effectiveness of the learning approach, we coupled it with various regression methods that employ gradient-based optimizers for training. The proposed approach was tested on various problems involving model discovery and function approximation, demonstrating its ability to successfully identify functional regimes, discover true governing equations, and reduce test errors.
</details>
<details>
<summary>摘要</summary>
科学和工程中的复杂系统ometimes会展现不同的行为方式，传统的全球模型难以捕捉这些复杂行为的全范围。为了解决这个挑战，我们提议一种新的竞争学习方法，通过在数据上同时训练一组模型，使得每个模型在训练过程中竞争对每个观察结果。这种方法可以识别数据集中的不同功能模式，并通过使用动态损失函数，使模型在不同的训练过程中产生更好的性能。为了证明该学习方法的有效性，我们将其与多种回归方法结合使用，其中包括使用梯度基本优化器进行训练。我们在各种模型发现和函数近似问题上测试了该方法，并证明了其能够成功地识别功能模式、发现真正的管理方程和降低测试错误。
</details></li>
</ul>
<hr>
<h2 id="Novel-Batch-Active-Learning-Approach-and-Its-Application-to-Synthetic-Aperture-Radar-Datasets"><a href="#Novel-Batch-Active-Learning-Approach-and-Its-Application-to-Synthetic-Aperture-Radar-Datasets" class="headerlink" title="Novel Batch Active Learning Approach and Its Application to Synthetic Aperture Radar Datasets"></a>Novel Batch Active Learning Approach and Its Application to Synthetic Aperture Radar Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10495">http://arxiv.org/abs/2307.10495</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chapman20j/sar_bal">https://github.com/chapman20j/sar_bal</a></li>
<li>paper_authors: James Chapman, Bohan Chen, Zheng Tan, Jeff Calder, Kevin Miller, Andrea L. Bertozzi</li>
<li>for: 这个论文旨在提高机器学习方法的性能，通过选择有限数量的无标的数据点进行活动学习，以最大化基础分类器的性能。</li>
<li>methods: 本论文使用了序列活动学习和批量活动学习两种方法，其中序列活动学习每次选择一个问题集，而批量活动学习则选择多个数据点进行查询。</li>
<li>results: 本论文的结果显示，使用了Dijkstra的 Annulus Core-Set（DAC）和LocalMax的两种方法可以实现高度的精度和效率，并且可以超过现有的CNN方法。<details>
<summary>Abstract</summary>
Active learning improves the performance of machine learning methods by judiciously selecting a limited number of unlabeled data points to query for labels, with the aim of maximally improving the underlying classifier's performance. Recent gains have been made using sequential active learning for synthetic aperture radar (SAR) data arXiv:2204.00005. In each iteration, sequential active learning selects a query set of size one while batch active learning selects a query set of multiple datapoints. While batch active learning methods exhibit greater efficiency, the challenge lies in maintaining model accuracy relative to sequential active learning methods. We developed a novel, two-part approach for batch active learning: Dijkstra's Annulus Core-Set (DAC) for core-set generation and LocalMax for batch sampling. The batch active learning process that combines DAC and LocalMax achieves nearly identical accuracy as sequential active learning but is more efficient, proportional to the batch size. As an application, a pipeline is built based on transfer learning feature embedding, graph learning, DAC, and LocalMax to classify the FUSAR-Ship and OpenSARShip datasets. Our pipeline outperforms the state-of-the-art CNN-based methods.
</details>
<details>
<summary>摘要</summary>
活动学习可以提高机器学习方法的性能，通过选择有限数量的无标签数据点进行查询，以最大化下游分类器的性能。最近，在Synthetic Aperture Radar（SAR）数据上使用Sequential Active Learning（SAL）已经得到了进步。在每次迭代中，SAL选择一个查询集合，而批处活动学习选择多个数据点。虽然批处活动学习方法更加高效，但是维护模型准确性的挑战在于Sequential Active Learning方法。我们提出了一种新的、两部分的批处活动学习方法：Dijkstra的Annulus Core-Set（DAC）用于核心集生成和LocalMax用于批处采样。将DAC和LocalMax结合使用的批处活动学习过程可以与Sequential Active Learning方法准确相同，但是更高效，与批处大小成正比。我们使用了基于传输学习特征嵌入、图学习、DAC和LocalMax来分类FUSAR-Ship和OpenSARShip数据集。我们的管道超过了当前Convolutional Neural Network（CNN）基于方法的状态。
</details></li>
</ul>
<hr>
<h2 id="Blockchain-Based-Federated-Learning-Incentivizing-Data-Sharing-and-Penalizing-Dishonest-Behavior"><a href="#Blockchain-Based-Federated-Learning-Incentivizing-Data-Sharing-and-Penalizing-Dishonest-Behavior" class="headerlink" title="Blockchain-Based Federated Learning: Incentivizing Data Sharing and Penalizing Dishonest Behavior"></a>Blockchain-Based Federated Learning: Incentivizing Data Sharing and Penalizing Dishonest Behavior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10492">http://arxiv.org/abs/2307.10492</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amir Jaberzadeh, Ajay Kumar Shrestha, Faijan Ahamad Khan, Mohammed Afaan Shaikh, Bhargav Dave, Jason Geng</li>
<li>for: 这种方法是为了提高聚合学习模型的准确率而确保数据分享的安全性和公正性。</li>
<li>methods: 该方法使用了InterPlanetary File System、区块链和智能合约来实现安全和互惠的数据分享，并提供了激励机制、访问控制机制和惩戒任何不诚实行为。</li>
<li>results: 实验结果表明，提议的模型可以提高聚合学习模型的准确率，同时保证数据分享过程中的安全性和公正性。此外，该研究还实现了一个基于区块链技术的分布式学习平台，可以同时训练多个工作者的模型，保持数据隐私和安全性。<details>
<summary>Abstract</summary>
With the increasing importance of data sharing for collaboration and innovation, it is becoming more important to ensure that data is managed and shared in a secure and trustworthy manner. Data governance is a common approach to managing data, but it faces many challenges such as data silos, data consistency, privacy, security, and access control. To address these challenges, this paper proposes a comprehensive framework that integrates data trust in federated learning with InterPlanetary File System, blockchain, and smart contracts to facilitate secure and mutually beneficial data sharing while providing incentives, access control mechanisms, and penalizing any dishonest behavior. The experimental results demonstrate that the proposed model is effective in improving the accuracy of federated learning models while ensuring the security and fairness of the data-sharing process. The research paper also presents a decentralized federated learning platform that successfully trained a CNN model on the MNIST dataset using blockchain technology. The platform enables multiple workers to train the model simultaneously while maintaining data privacy and security. The decentralized architecture and use of blockchain technology allow for efficient communication and coordination between workers. This platform has the potential to facilitate decentralized machine learning and support privacy-preserving collaboration in various domains.
</details>
<details>
<summary>摘要</summary>
随着数据共享的重要性增加，保证数据的安全和可靠性变得更加重要。数据治理是一种常见的数据管理方法，但它面临着数据孤岛、数据一致性、隐私、安全和访问控制等挑战。为了解决这些挑战，这篇论文提出了一个完整的框架，将数据信任integrated into federated learning with InterPlanetary File System, blockchain, and smart contracts，以便在安全和公正的数据共享条件下进行安全和互利的数据共享，并提供了激励、访问控制机制和惩戒任何不诚实的行为。实验结果表明，提出的模型能够提高 federated learning 模型的准确率，同时保证数据共享过程的安全和公正性。研究论文还描述了一个基于区块链技术的分布式 federated learning 平台，可以同时让多个工作者在维护数据隐私和安全的前提下，共同训练 CNN 模型。该平台的分布式架构和使用区块链技术，可以减少工作者之间的通信和协调成本，并且具有潜在的分布式机器学习和隐私保护的应用前景。
</details></li>
</ul>
<hr>
<h2 id="Ab-using-Images-and-Sounds-for-Indirect-Instruction-Injection-in-Multi-Modal-LLMs"><a href="#Ab-using-Images-and-Sounds-for-Indirect-Instruction-Injection-in-Multi-Modal-LLMs" class="headerlink" title="(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs"></a>(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10490">http://arxiv.org/abs/2307.10490</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ebagdasa/multimodal_injection">https://github.com/ebagdasa/multimodal_injection</a></li>
<li>paper_authors: Eugene Bagdasaryan, Tsung-Yin Hsieh, Ben Nassi, Vitaly Shmatikov</li>
<li>for: 这篇论文描述了如何使用图像和声音来实现多模态语言模型中的恶意提示和指令注入攻击。</li>
<li>methods: 攻击者可以生成对应于提示的恶意扰动，并将其混合到图像或音频记录中。当用户问问 benign 模型关于扰动后的图像或音频时，扰动会导致模型输出攻击者选择的文本和&#x2F;或使Subsequent dialog follow the attacker’s instruction。</li>
<li>results: 作者通过证明了这种攻击的可行性，并提供了一些证明例targeting LLaVa和PandaGPT。<details>
<summary>Abstract</summary>
We demonstrate how images and sounds can be used for indirect prompt and instruction injection in multi-modal LLMs. An attacker generates an adversarial perturbation corresponding to the prompt and blends it into an image or audio recording. When the user asks the (unmodified, benign) model about the perturbed image or audio, the perturbation steers the model to output the attacker-chosen text and/or make the subsequent dialog follow the attacker's instruction. We illustrate this attack with several proof-of-concept examples targeting LLaVa and PandaGPT.
</details>
<details>
<summary>摘要</summary>
我们展示了图像和声音可以用于多modal LLMS中的间接提示和指令注入攻击。攻击者创建了对提示的攻击偏移，然后与图像或音频录制中混合。当用户对未修改的模型询问这个偏移后的图像或音频时，攻击偏移将模型规律输出攻击者选择的文本和/或让后续对话按照攻击者的指令继续。我们透过多个证明例Targeting LLaVa和PandaGPT详细介绍这一攻击。
</details></li>
</ul>
<hr>
<h2 id="SPRINT-A-Unified-Toolkit-for-Evaluating-and-Demystifying-Zero-shot-Neural-Sparse-Retrieval"><a href="#SPRINT-A-Unified-Toolkit-for-Evaluating-and-Demystifying-Zero-shot-Neural-Sparse-Retrieval" class="headerlink" title="SPRINT: A Unified Toolkit for Evaluating and Demystifying Zero-shot Neural Sparse Retrieval"></a>SPRINT: A Unified Toolkit for Evaluating and Demystifying Zero-shot Neural Sparse Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10488">http://arxiv.org/abs/2307.10488</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thakur-nandan/sprint">https://github.com/thakur-nandan/sprint</a></li>
<li>paper_authors: Nandan Thakur, Kexin Wang, Iryna Gurevych, Jimmy Lin</li>
<li>for: 这个论文的目的是提供一个基于 Pyserini 和 Lucene 的 Python 工具集 (SPRINT)，用于评估神经稀采推荐系统。</li>
<li>methods: 这个工具集包括五种内置的模型：uniCOIL、DeepImpact、SPARTA、TILDEv2 和 SPLADEv2。用户也可以轻松地添加自己定制的模型，只需要定义权重方法。</li>
<li>results: 使用 SPRINT 工具集，我们在 BEIR 上建立了强大和可重复的零shot 稀采推荐基准。我们的结果显示，SPLADEv2 在 BEIR 上得到了最高的平均得分 0.470 nDCG@10。此外，我们还发现 SPLADEv2 生成的稀采表示性中有一部分 tokens 位于原始查询和文档之外，这经常是其性能提升的原因。<details>
<summary>Abstract</summary>
Traditionally, sparse retrieval systems relied on lexical representations to retrieve documents, such as BM25, dominated information retrieval tasks. With the onset of pre-trained transformer models such as BERT, neural sparse retrieval has led to a new paradigm within retrieval. Despite the success, there has been limited software supporting different sparse retrievers running in a unified, common environment. This hinders practitioners from fairly comparing different sparse models and obtaining realistic evaluation results. Another missing piece is, that a majority of prior work evaluates sparse retrieval models on in-domain retrieval, i.e. on a single dataset: MS MARCO. However, a key requirement in practical retrieval systems requires models that can generalize well to unseen out-of-domain, i.e. zero-shot retrieval tasks. In this work, we provide SPRINT, a unified Python toolkit based on Pyserini and Lucene, supporting a common interface for evaluating neural sparse retrieval. The toolkit currently includes five built-in models: uniCOIL, DeepImpact, SPARTA, TILDEv2 and SPLADEv2. Users can also easily add customized models by defining their term weighting method. Using our toolkit, we establish strong and reproducible zero-shot sparse retrieval baselines across the well-acknowledged benchmark, BEIR. Our results demonstrate that SPLADEv2 achieves the best average score of 0.470 nDCG@10 on BEIR amongst all neural sparse retrievers. In this work, we further uncover the reasons behind its performance gain. We show that SPLADEv2 produces sparse representations with a majority of tokens outside of the original query and document which is often crucial for its performance gains, i.e. a limitation among its other sparse counterparts. We provide our SPRINT toolkit, models, and data used in our experiments publicly here at https://github.com/thakur-nandan/sprint.
</details>
<details>
<summary>摘要</summary>
传统上，稀疏检索系统通常使用lexical表示来检索文档，如BM25，控制了检索任务。随着预训练变换器模型如BERT的出现，神经稀疏检索引入了一种新的 paradigm within检索。 despite the success， there has been limited software supporting different sparse retrievers running in a unified, common environment。This hinders practitioners from fairly comparing different sparse models and obtaining realistic evaluation results。Another missing piece is that a majority of prior work evaluates sparse retrieval models on in-domain retrieval，i.e. on a single dataset：MS MARCO。However， a key requirement in practical retrieval systems requires models that can generalize well to unseen out-of-domain，i.e. zero-shot retrieval tasks。In this work，we provide SPRINT，a unified Python toolkit based on Pyserini and Lucene，supporting a common interface for evaluating neural sparse retrieval。The toolkit currently includes five built-in models：uniCOIL，DeepImpact，SPARTA，TILDEv2 and SPLADEv2。Users can also easily add customized models by defining their term weighting method。Using our toolkit，we establish strong and reproducible zero-shot sparse retrieval baselines across the well-acknowledged benchmark，BEIR。Our results demonstrate that SPLADEv2 achieves the best average score of 0.470 nDCG@10 on BEIR amongst all neural sparse retrievers。In this work，we further uncover the reasons behind its performance gain。We show that SPLADEv2 produces sparse representations with a majority of tokens outside of the original query and document，which is often crucial for its performance gains，i.e. a limitation among its other sparse counterparts。We provide our SPRINT toolkit，models，and data used in our experiments publicly here at <https://github.com/thakur-nandan/sprint>.
</details></li>
</ul>
<hr>
<h2 id="FinGPT-Democratizing-Internet-scale-Data-for-Financial-Large-Language-Models"><a href="#FinGPT-Democratizing-Internet-scale-Data-for-Financial-Large-Language-Models" class="headerlink" title="FinGPT: Democratizing Internet-scale Data for Financial Large Language Models"></a>FinGPT: Democratizing Internet-scale Data for Financial Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10485">http://arxiv.org/abs/2307.10485</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai4finance-foundation/fingpt">https://github.com/ai4finance-foundation/fingpt</a></li>
<li>paper_authors: Xiao-Yang Liu, Guoxuan Wang, Daochen Zha</li>
<li>For: FinGPT aims to democratize Internet-scale financial data for large language models (LLMs) to stimulate innovation and unlock new opportunities in open finance.* Methods: FinGPT is an open-sourced and data-centric framework that automates the collection and curation of real-time financial data from &gt;34 diverse sources on the Internet, and provides researchers and practitioners with accessible and transparent resources to develop their FinLLMs. Additionally, the paper proposes a simple yet effective strategy for fine-tuning FinLLM using the inherent feedback from the market, dubbed Reinforcement Learning with Stock Prices (RLSP).* Results: The paper showcases several FinGPT applications, including robo-advisor, sentiment analysis for algorithmic trading, and low-code development.<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated remarkable proficiency in understanding and generating human-like texts, which may potentially revolutionize the finance industry. However, existing LLMs often fall short in the financial field, which is mainly attributed to the disparities between general text data and financial text data. Unfortunately, there is only a limited number of financial text datasets available (quite small size), and BloombergGPT, the first financial LLM (FinLLM), is close-sourced (only the training logs were released). In light of this, we aim to democratize Internet-scale financial data for LLMs, which is an open challenge due to diverse data sources, low signal-to-noise ratio, and high time-validity. To address the challenges, we introduce an open-sourced and data-centric framework, \textit{Financial Generative Pre-trained Transformer (FinGPT)}, that automates the collection and curation of real-time financial data from >34 diverse sources on the Internet, providing researchers and practitioners with accessible and transparent resources to develop their FinLLMs. Additionally, we propose a simple yet effective strategy for fine-tuning FinLLM using the inherent feedback from the market, dubbed Reinforcement Learning with Stock Prices (RLSP). We also adopt the Low-rank Adaptation (LoRA, QLoRA) method that enables users to customize their own FinLLMs from open-source general-purpose LLMs at a low cost. Finally, we showcase several FinGPT applications, including robo-advisor, sentiment analysis for algorithmic trading, and low-code development. FinGPT aims to democratize FinLLMs, stimulate innovation, and unlock new opportunities in open finance. The codes are available at https://github.com/AI4Finance-Foundation/FinGPT and https://github.com/AI4Finance-Foundation/FinNLP
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经表现出杰出的人工智能能力，可能将改变金融业。然而，现有的LLM通常在金融领域下表现不佳，主要是因为通用文本数据和金融文本数据之间存在差异。实际上，金融领域的数据相对较少，而BloombergGPT，首个金融LLM（FinLLM），则是封存的（仅公开训练记录）。为了普及互联网范围内的金融数据，并且让研究者和实践者可以轻松地发展自己的FinLLM，我们提出了一个开源和数据中心的框架，名为“金融生成预训练transformer”（FinGPT）。FinGPT自动收集和整理互联网上的多种金融数据，提供了可 accessible 和透明的资源，以便研究者和实践者可以轻松地发展自己的FinLLM。此外，我们提出了一种简单 yet有效的策略，使用市场内的反馈来调整FinLLM，名为“股票价格反馈学习”（RLSP）。我们还采用了低维度适应（LoRA，QLoRA）方法，让用户可以从开源通用语言模型中自定义自己的FinLLM，而不需要高成本。最后，我们展示了多个FinGPT应用，包括智能投资、基于算法传统交易的情感分析、以及低程式化开发。FinGPT愿望普及FinLLM，刺激创新，解锁开放金融的新机会。代码可以在https://github.com/AI4Finance-Foundation/FinGPT和https://github.com/AI4Finance-Foundation/FinNLP获取。
</details></li>
</ul>
<hr>
<h2 id="Can-Instruction-Fine-Tuned-Language-Models-Identify-Social-Bias-through-Prompting"><a href="#Can-Instruction-Fine-Tuned-Language-Models-Identify-Social-Bias-through-Prompting" class="headerlink" title="Can Instruction Fine-Tuned Language Models Identify Social Bias through Prompting?"></a>Can Instruction Fine-Tuned Language Models Identify Social Bias through Prompting?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10472">http://arxiv.org/abs/2307.10472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omkar Dige, Jacob-Junqi Tian, David Emerson, Faiza Khan Khattak</li>
<li>for: 这个论文目的是评估语言模型中的偏见，以及如何通过零批示来评估这些偏见。</li>
<li>methods: 这个论文使用的方法是使用链条思维（Chain-of-Thought）提问来评估语言模型的偏见识别能力。</li>
<li>results: 研究发现，使用Alpaca 7B模型，在偏见识别任务中获得56.7%的准确率，而 scaling up LLM 大小和数据多样性可能会导致更高的性能提升。<details>
<summary>Abstract</summary>
As the breadth and depth of language model applications continue to expand rapidly, it is increasingly important to build efficient frameworks for measuring and mitigating the learned or inherited social biases of these models. In this paper, we present our work on evaluating instruction fine-tuned language models' ability to identify bias through zero-shot prompting, including Chain-of-Thought (CoT) prompts. Across LLaMA and its two instruction fine-tuned versions, Alpaca 7B performs best on the bias identification task with an accuracy of 56.7%. We also demonstrate that scaling up LLM size and data diversity could lead to further performance gain. This is a work-in-progress presenting the first component of our bias mitigation framework. We will keep updating this work as we get more results.
</details>
<details>
<summary>摘要</summary>
为应用语言模型的关系范围和深度不断扩大，现在越来越重要建立高效的测试和减少社会偏见的框架。在这篇论文中，我们介绍了使用零批训练语言模型识别偏见的能力，包括链接思维（CoT）提示。在LLaMA和其两个受训版本中，Alpaca 7B在偏见识别任务上表现最好，具体成绩为56.7%。我们还证明了将LLM大小和数据多样性扩大可以带来更大的性能提升。这是我们的偏见 Mitigation 框架的第一个 ком成分，我们将继续更新这个工作，当我们获得更多结果时。
</details></li>
</ul>
<hr>
<h2 id="Classification-of-Visualization-Types-and-Perspectives-in-Patents"><a href="#Classification-of-Visualization-Types-and-Perspectives-in-Patents" class="headerlink" title="Classification of Visualization Types and Perspectives in Patents"></a>Classification of Visualization Types and Perspectives in Patents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10471">http://arxiv.org/abs/2307.10471</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tibhannover/patentimageclassification">https://github.com/tibhannover/patentimageclassification</a></li>
<li>paper_authors: Junaid Ahmed Ghauri, Eric Müller-Budack, Ralph Ewerth</li>
<li>For: The paper is written for researchers and developers working on information and multimedia retrieval for patents, as well as those interested in deep learning approaches for image classification.* Methods: The paper uses state-of-the-art deep learning methods, including transformers, for the classification of visualization types and perspectives in patent images.* Results: The proposed approaches have been demonstrated to be feasible through experimental results, and the authors plan to make the source code, models, and dataset publicly available.Here’s the simplified Chinese version of the three key points:</li>
<li>for: 该论文是为了帮助信息和多媒体检索领域的研究人员和开发人员，以及关注深度学习方法的人员。</li>
<li>methods: 该论文使用当前最佳的深度学习方法，包括转换器，来分类专利图像中的视觉类型和视角。</li>
<li>results: 提出的方法已经实验表明其可行性，作者计划将源代码、模型和数据集公开发布。<details>
<summary>Abstract</summary>
Due to the swift growth of patent applications each year, information and multimedia retrieval approaches that facilitate patent exploration and retrieval are of utmost importance. Different types of visualizations (e.g., graphs, technical drawings) and perspectives (e.g., side view, perspective) are used to visualize details of innovations in patents. The classification of these images enables a more efficient search and allows for further analysis. So far, datasets for image type classification miss some important visualization types for patents. Furthermore, related work does not make use of recent deep learning approaches including transformers. In this paper, we adopt state-of-the-art deep learning methods for the classification of visualization types and perspectives in patent images. We extend the CLEF-IP dataset for image type classification in patents to ten classes and provide manual ground truth annotations. In addition, we derive a set of hierarchical classes from a dataset that provides weakly-labeled data for image perspectives. Experimental results have demonstrated the feasibility of the proposed approaches. Source code, models, and dataset will be made publicly available.
</details>
<details>
<summary>摘要</summary>
In this paper, we employ state-of-the-art deep learning methods for the classification of visualization types and perspectives in patent images. We expand the CLEF-IP dataset for image type classification in patents to ten classes and provide manual ground truth annotations. Additionally, we derive a set of hierarchical classes from a dataset that provides weakly-labeled data for image perspectives. Our experimental results demonstrate the feasibility of the proposed approaches. The source code, models, and dataset will be publicly available.
</details></li>
</ul>
<hr>
<h2 id="Properties-of-Discrete-Sliced-Wasserstein-Losses"><a href="#Properties-of-Discrete-Sliced-Wasserstein-Losses" class="headerlink" title="Properties of Discrete Sliced Wasserstein Losses"></a>Properties of Discrete Sliced Wasserstein Losses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10352">http://arxiv.org/abs/2307.10352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eloi Tanguy, Rémi Flamary, Julie Delon</li>
<li>for: 这个论文主要研究了块 Wasserstein 距离（SW）作为比较概率分布的一种代替方法，特别是在图像处理、领域适应和生成模型中，where it is common to optimize some parameters to minimize SW, which serves as a loss function between discrete probability measures.</li>
<li>methods: 本文研究了 $\mathcal{E}: Y \longmapsto \text{SW}_2^2(\gamma_Y, \gamma_Z)$ 的性质和优化属性，其中 $\gamma_Y$ 和 $\gamma_Z$ 是两个同量的离散概率分布。</li>
<li>results: 研究结果显示，$\mathcal{E}_p$ 的迪克梯度法和 $\mathcal{E}$ 的迪克梯度法都可以减少 $\mathcal{E}$ 的极值，并且在某种意义上，这些方法会 converge towards (Clarke)  критиче点。<details>
<summary>Abstract</summary>
The Sliced Wasserstein (SW) distance has become a popular alternative to the Wasserstein distance for comparing probability measures. Widespread applications include image processing, domain adaptation and generative modelling, where it is common to optimise some parameters in order to minimise SW, which serves as a loss function between discrete probability measures (since measures admitting densities are numerically unattainable). All these optimisation problems bear the same sub-problem, which is minimising the Sliced Wasserstein energy. In this paper we study the properties of $\mathcal{E}: Y \longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z)$, i.e. the SW distance between two uniform discrete measures with the same amount of points as a function of the support $Y \in \mathbb{R}^{n \times d}$ of one of the measures. We investigate the regularity and optimisation properties of this energy, as well as its Monte-Carlo approximation $\mathcal{E}_p$ (estimating the expectation in SW using only $p$ samples) and show convergence results on the critical points of $\mathcal{E}_p$ to those of $\mathcal{E}$, as well as an almost-sure uniform convergence. Finally, we show that in a certain sense, Stochastic Gradient Descent methods minimising $\mathcal{E}$ and $\mathcal{E}_p$ converge towards (Clarke) critical points of these energies.
</details>
<details>
<summary>摘要</summary>
“划分 Wasserstein（SW）距离已成为比 Wasserstein 距离更受欢迎的选择，用于比较概率分布。它在图像处理、领域适应和生成模型中具有广泛的应用，例如最小化 SW 作为概率分布之间的损失函数。这些优化问题都有同一个子问题，即寻找 SW 能量的最小值。在这篇文章中，我们研究了 $\mathcal{E}：Y \longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z)$ 的性质，其中 $\gamma_Y$ 和 $\gamma_Z$ 是两个具有相同数量点的不变概率分布。我们调查了这个能量的规律和优化性，以及其 Monte-Carlo 预测 $\mathcal{E}_p$（使用 $p$ 样本估计 SW 的期望值）的数值和数值算法，并证明了这些点的对应点的对应点的数值和 almost-sure uniform 收敛。最后，我们显示了在某种意义上，使用 Stochastic Gradient Descent 方法对 $\mathcal{E}$ 和 $\mathcal{E}_p$ 进行优化将导向（Clarke）的极值点。”
</details></li>
</ul>
<hr>
<h2 id="A-data-science-axiology-the-nature-value-and-risks-of-data-science"><a href="#A-data-science-axiology-the-nature-value-and-risks-of-data-science" class="headerlink" title="A data science axiology: the nature, value, and risks of data science"></a>A data science axiology: the nature, value, and risks of data science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10460">http://arxiv.org/abs/2307.10460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael L. Brodie</li>
<li>for: 本文提出了数据科学的axiology，即其目的、性质、重要性、风险和价值，以帮助理解和定义数据科学，并承认其潜在的利益和风险。</li>
<li>methods: 本文使用了AXIOLOGY的方法来探索和评估数据科学的特点，包括对数据科学的定义、特点、优势和风险的分析。</li>
<li>results: 本文的结果表明，数据科学是一种在不可预测的uncertainty下进行知识探索的研究 paradigm，具有广泛的应用前景和可能的风险。<details>
<summary>Abstract</summary>
Data science is not a science. It is a research paradigm with an unfathomed scope, scale, complexity, and power for knowledge discovery that is not otherwise possible and can be beyond human reasoning. It is changing our world practically and profoundly already widely deployed in tens of thousands of applications in every discipline in an AI Arms Race that, due to its inscrutability, can lead to unfathomed risks. This paper presents an axiology of data science, its purpose, nature, importance, risks, and value for problem solving, by exploring and evaluating its remarkable, definitive features. As data science is in its infancy, this initial, speculative axiology is intended to aid in understanding and defining data science to recognize its potential benefits, risks, and open research challenges. AI based data science is inherently about uncertainty that may be more realistic than our preference for the certainty of science. Data science will have impacts far beyond knowledge discovery and will take us into new ways of understanding the world.
</details>
<details>
<summary>摘要</summary>
“数据科学不是一种科学。它是一种研究方法论，它的范围、规模、复杂度和知识发现能力超出了人类理解的限制，可以为问题解决提供无法其他方式获得的力量。它已经在各个领域中广泛应用，并在人工智能竞赛中投入了 tens of thousands of 应用，但由于其不可预测性，可能会导致未知的风险。本文提出了数据科学的axiology，即其目的、性质、重要性、风险和问题解决的价值，通过探索和评估其特点来帮助理解和定义数据科学，并找到其潜在的利益、风险和研究挑战。基于人工智能的数据科学本身带有不确定性，可能比我们更好地反映现实。数据科学将对我们的世界产生深远的影响，并将带我们进入新的世界理解方式。”
</details></li>
</ul>
<hr>
<h2 id="A-New-Computationally-Simple-Approach-for-Implementing-Neural-Networks-with-Output-Hard-Constraints"><a href="#A-New-Computationally-Simple-Approach-for-Implementing-Neural-Networks-with-Output-Hard-Constraints" class="headerlink" title="A New Computationally Simple Approach for Implementing Neural Networks with Output Hard Constraints"></a>A New Computationally Simple Approach for Implementing Neural Networks with Output Hard Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10459">http://arxiv.org/abs/2307.10459</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Andrei V. Konstantinov, Lev V. Utkin</li>
<li>for: 本研究提出了一种新的计算简单的神经网络输出值约束方法，用于解决神经网络输出值约束问题。</li>
<li>methods: 该方法基于将神经网络参数向量映射到一个确定在可行集中的点上，以实现约束的执行。该映射通过额外的神经网络层实现，并可扩展到对输入具有相互关系的约束。</li>
<li>results: 该方法的计算简单，前行通过的复杂度为O(n*m)和O(n^2*m)，其中n是变量数量，m是约束数量。数值实验证明了该方法在优化和分类问题中的效果。代码实现方法的公开可用。<details>
<summary>Abstract</summary>
A new computationally simple method of imposing hard convex constraints on the neural network output values is proposed. The key idea behind the method is to map a vector of hidden parameters of the network to a point that is guaranteed to be inside the feasible set defined by a set of constraints. The mapping is implemented by the additional neural network layer with constraints for output. The proposed method is simply extended to the case when constraints are imposed not only on the output vectors, but also on joint constraints depending on inputs. The projection approach to imposing constraints on outputs can simply be implemented in the framework of the proposed method. It is shown how to incorporate different types of constraints into the proposed method, including linear and quadratic constraints, equality constraints, and dynamic constraints, constraints in the form of boundaries. An important feature of the method is its computational simplicity. Complexities of the forward pass of the proposed neural network layer by linear and quadratic constraints are O(n*m) and O(n^2*m), respectively, where n is the number of variables, m is the number of constraints. Numerical experiments illustrate the method by solving optimization and classification problems. The code implementing the method is publicly available.
</details>
<details>
<summary>摘要</summary>
新的计算简单方法对神经网络输出值进行硬 convex 约束是提出的。该方法的关键思想是将神经网络参数向量映射到一个可以 garantuee 在约束集中的点上。该映射通过额外的神经网络层实现，该层带有约束条件。提出的方法可以简单地扩展到输入joint 约束的情况。投影方法可以简单地在该方法的框架中实现。该方法可以 incorporate 不同类型的约束，包括线性和quadratic 约束、等式约束、边界约束等。该方法的计算简单性是其重要特点。前进通过神经网络层的复杂性为O(n*m)和O(n^2*m)，其中n是变量数量，m是约束数量。数学实验证明了该方法的可行性和精度。代码实现该方法公开available。
</details></li>
</ul>
<hr>
<h2 id="A-Step-Towards-Worldwide-Biodiversity-Assessment-The-BIOSCAN-1M-Insect-Dataset"><a href="#A-Step-Towards-Worldwide-Biodiversity-Assessment-The-BIOSCAN-1M-Insect-Dataset" class="headerlink" title="A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset"></a>A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10455">http://arxiv.org/abs/2307.10455</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zahrag/BIOSCAN-1M">https://github.com/zahrag/BIOSCAN-1M</a></li>
<li>paper_authors: Zahra Gharaee, ZeMing Gong, Nicholas Pellegrino, Iuliia Zarubiieva, Joakim Bruslund Haurum, Scott C. Lowe, Jaclyn T. A. McKeown, Chris C. Y. Ho, Joschka McLeod, Yi-Yun C Wei, Jireh Agda, Sujeevan Ratnasingham, Dirk Steinke, Angel X. Chang, Graham W. Taylor, Paul Fieguth</li>
<li>for: 这个论文的目的是为了开发一个基于图像的生物多样性识别模型，以便为全球生物多样性的评估做出贡献。</li>
<li>methods: 这篇论文使用了一个大量的手动标注的昆虫图像集，称为BIOSCAN-Insect Dataset，每个记录都被专家分类，并包含了生物学上的遗传信息，如纯度序列和分配给每个物种的杂合编号。</li>
<li>results: 这篇论文提出了一个约一百万张图像的数据集，用于训练计算机视觉模型，以实现图像基于的生物多样性识别。此外，数据集还展示了一种特有的长尾分布，以及生物学上的层次分类问题。<details>
<summary>Abstract</summary>
In an effort to catalog insect biodiversity, we propose a new large dataset of hand-labelled insect images, the BIOSCAN-Insect Dataset. Each record is taxonomically classified by an expert, and also has associated genetic information including raw nucleotide barcode sequences and assigned barcode index numbers, which are genetically-based proxies for species classification. This paper presents a curated million-image dataset, primarily to train computer-vision models capable of providing image-based taxonomic assessment, however, the dataset also presents compelling characteristics, the study of which would be of interest to the broader machine learning community. Driven by the biological nature inherent to the dataset, a characteristic long-tailed class-imbalance distribution is exhibited. Furthermore, taxonomic labelling is a hierarchical classification scheme, presenting a highly fine-grained classification problem at lower levels. Beyond spurring interest in biodiversity research within the machine learning community, progress on creating an image-based taxonomic classifier will also further the ultimate goal of all BIOSCAN research: to lay the foundation for a comprehensive survey of global biodiversity. This paper introduces the dataset and explores the classification task through the implementation and analysis of a baseline classifier.
</details>
<details>
<summary>摘要</summary>
为了目录 insect 多样性，我们提议一个大型手动标注的昆虫图像集合，称为 BIOSCAN-Insect 数据集。每个记录都被专家分类，并有关联的遗传信息，包括原始核酸条形码序列和分配的核酸指标号，这些是基于种类分类的生物学基于代理。本文介绍了一个精心纪录的一百万张图像集，主要用于训练计算机视觉模型，以提供图像基本的种类评估。然而，数据集还具有吸引Machine Learning社区的特点，包括长尾分布和层次分类问题。BIOSCAN 研究的最终目标是建立全球多样性的基础，这个数据集的研究将有助于实现这一目标。本文介绍了数据集和基线分类器的实现和分析。
</details></li>
</ul>
<hr>
<h2 id="The-importance-of-feature-preprocessing-for-differentially-private-linear-optimization"><a href="#The-importance-of-feature-preprocessing-for-differentially-private-linear-optimization" class="headerlink" title="The importance of feature preprocessing for differentially private linear optimization"></a>The importance of feature preprocessing for differentially private linear optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11106">http://arxiv.org/abs/2307.11106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziteng Sun, Ananda Theertha Suresh, Aditya Krishna Menon</li>
<li>for: 该研究是 investigate the sufficient condition of DPSGD for finding a good minimizer under privacy constraints.</li>
<li>methods: 该研究使用了 differentially private stochastic gradient descent (DPSGD) 和其变种，以及 feature preprocessing.</li>
<li>results: 研究发现， Without feature preprocessing, DPSGD 会导致隐私Error proportional to the maximum norm of features over all samples. furthermore, the proposed algorithm DPSGD-F combines DPSGD with feature preprocessing, and proves that for classification tasks, it incurs a privacy error proportional to the diameter of the features.<details>
<summary>Abstract</summary>
Training machine learning models with differential privacy (DP) has received increasing interest in recent years. One of the most popular algorithms for training differentially private models is differentially private stochastic gradient descent (DPSGD) and its variants, where at each step gradients are clipped and combined with some noise. Given the increasing usage of DPSGD, we ask the question: is DPSGD alone sufficient to find a good minimizer for every dataset under privacy constraints? As a first step towards answering this question, we show that even for the simple case of linear classification, unlike non-private optimization, (private) feature preprocessing is vital for differentially private optimization. In detail, we first show theoretically that there exists an example where without feature preprocessing, DPSGD incurs a privacy error proportional to the maximum norm of features over all samples. We then propose an algorithm called DPSGD-F, which combines DPSGD with feature preprocessing and prove that for classification tasks, it incurs a privacy error proportional to the diameter of the features $\max_{x, x' \in D} \|x - x'\|_2$. We then demonstrate the practicality of our algorithm on image classification benchmarks.
</details>
<details>
<summary>摘要</summary>
训练机器学习模型带有差异隐私（DP）在过去几年中得到了越来越多的关注。DPSGD和其变种是最受欢迎的 differentially private 模型训练算法之一，其中在每步骤中 gradients 会被 clipped 并与一些随机噪声相加。 giventhe increasing usage of DPSGD, we ask the question: is DPSGD alone sufficient to find a good minimizer for every dataset under privacy constraints? As a first step towards answering this question, we show that even for the simple case of linear classification, unlike non-private optimization, (private) feature preprocessing is vital for differentially private optimization. In detail, we first show theoretically that there exists an example where without feature preprocessing, DPSGD incurs a privacy error proportional to the maximum norm of features over all samples. We then propose an algorithm called DPSGD-F, which combines DPSGD with feature preprocessing and prove that for classification tasks, it incurs a privacy error proportional to the diameter of the features $\max_{x, x' \in D} \|x - x'\|_2$. We then demonstrate the practicality of our algorithm on image classification benchmarks.Note: The translation is in Simplified Chinese, which is one of the two standardized Chinese languages. The other one is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Integrating-a-Heterogeneous-Graph-with-Entity-aware-Self-attention-using-Relative-Position-Labels-for-Reading-Comprehension-Model"><a href="#Integrating-a-Heterogeneous-Graph-with-Entity-aware-Self-attention-using-Relative-Position-Labels-for-Reading-Comprehension-Model" class="headerlink" title="Integrating a Heterogeneous Graph with Entity-aware Self-attention using Relative Position Labels for Reading Comprehension Model"></a>Integrating a Heterogeneous Graph with Entity-aware Self-attention using Relative Position Labels for Reading Comprehension Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10443">http://arxiv.org/abs/2307.10443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shima Foolad, Kourosh Kiani</li>
<li>for: This paper aims to improve the ability of transformer models in handling complex reasoning tasks by integrating external knowledge into the model without relying on external sources.</li>
<li>methods: The proposed attention pattern includes global-local attention for word tokens, graph attention for entity tokens, and consideration of the type of relationship between each entity token and word token.</li>
<li>results: The experimental findings show that the proposed model outperforms both the cutting-edge LUKE-Graph and the baseline LUKE model on the ReCoRD dataset that focuses on commonsense reasoning.Here’s the Chinese text in the format you requested:</li>
<li>for: 本文目的是强化基于转换器模型的机器阅读理解任务处理复杂逻辑任务的能力，不再依赖外部知识。</li>
<li>methods: 该提议的注意模式包括全局-本地注意力 для单词标签、基于图像注意力 для实体标签，以及对每个实体标签和单词标签之间的关系类型进行考虑。</li>
<li>results: 实验结果表明，提议的模型在ReCoRD数据集，专注于通逻辑理解任务上，超越了当前的LUKE-Graph和基准LUKE模型。<details>
<summary>Abstract</summary>
Despite the significant progress made by transformer models in machine reading comprehension tasks, they still fall short in handling complex reasoning tasks due to the absence of explicit knowledge in the input sequence. To address this limitation, many recent works have proposed injecting external knowledge into the model. However, selecting relevant external knowledge, ensuring its availability, and requiring additional processing steps remain challenging. In this paper, we introduce a novel attention pattern that integrates reasoning knowledge derived from a heterogeneous graph into the transformer architecture without relying on external knowledge. The proposed attention pattern comprises three key elements: global-local attention for word tokens, graph attention for entity tokens that exhibit strong attention towards tokens connected in the graph as opposed to those unconnected, and the consideration of the type of relationship between each entity token and word token. This results in optimized attention between the two if a relationship exists. The pattern is coupled with special relative position labels, allowing it to integrate with LUKE's entity-aware self-attention mechanism. The experimental findings corroborate that our model outperforms both the cutting-edge LUKE-Graph and the baseline LUKE model on the ReCoRD dataset that focuses on commonsense reasoning.
</details>
<details>
<summary>摘要</summary>
尽管变换器模型在机器阅读理解任务中做出了重要进步，但它们仍然缺乏明确的知识，导致在复杂的推理任务中表现不佳。为解决这个限制，许多最近的研究提出了在模型中注入外部知识的方法。然而，选择相关的外部知识，保证其可用性，并且需要额外的处理步骤仍然是挑战。在这篇论文中，我们提出了一种新的注意模式，即在变换器架构中 инте integrate 了多样化图的推理知识。该注意模式包括三个关键元素：全球-本地注意 для单词符号，图注意对实体符号的强注意点，以及每个实体符号和单词符号之间的类型关系的考虑。这些元素结合使得注意力得到优化。此外，我们采用特殊相对位标签，使得它可以与LUKE模型的实体意识自注意机制结合。实验结果表明，我们的模型在ReCoRD数据集上，对于具有通情理解的机器阅读理解任务表现出色，超越了当前的LUKE-Graph和LUKE模型。
</details></li>
</ul>
<hr>
<h2 id="Confidence-Estimation-Using-Unlabeled-Data"><a href="#Confidence-Estimation-Using-Unlabeled-Data" class="headerlink" title="Confidence Estimation Using Unlabeled Data"></a>Confidence Estimation Using Unlabeled Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10440">http://arxiv.org/abs/2307.10440</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/topoxlab/consistency-ranking-loss">https://github.com/topoxlab/consistency-ranking-loss</a></li>
<li>paper_authors: Chen Li, Xiaoling Hu, Chao Chen</li>
<li>for: 这篇论文旨在提出一种semi-supervised的信任度估计方法，供深度神经网络在实际应用中更好地估计自己的信任度。</li>
<li>methods: 本方法使用训练过程中的预测一致性作为估计信任度的Surrogate函数，并提出了一种一致性排名损失函数来实现这一目的。</li>
<li>results: 在图像分类和 segmentation 任务上，本方法可以达到状态的表现，并且在下游活动学任务中表现出了明显的优势。<details>
<summary>Abstract</summary>
Overconfidence is a common issue for deep neural networks, limiting their deployment in real-world applications. To better estimate confidence, existing methods mostly focus on fully-supervised scenarios and rely on training labels. In this paper, we propose the first confidence estimation method for a semi-supervised setting, when most training labels are unavailable. We stipulate that even with limited training labels, we can still reasonably approximate the confidence of model on unlabeled samples by inspecting the prediction consistency through the training process. We use training consistency as a surrogate function and propose a consistency ranking loss for confidence estimation. On both image classification and segmentation tasks, our method achieves state-of-the-art performances in confidence estimation. Furthermore, we show the benefit of the proposed method through a downstream active learning task. The code is available at https://github.com/TopoXLab/consistency-ranking-loss
</details>
<details>
<summary>摘要</summary>
<<SYS>> tranlate the following text into Simplified ChineseOverconfidence is a common issue for deep neural networks, limiting their deployment in real-world applications. To better estimate confidence, existing methods mostly focus on fully-supervised scenarios and rely on training labels. In this paper, we propose the first confidence estimation method for a semi-supervised setting, when most training labels are unavailable. We stipulate that even with limited training labels, we can still reasonably approximate the confidence of model on unlabeled samples by inspecting the prediction consistency through the training process. We use training consistency as a surrogate function and propose a consistency ranking loss for confidence estimation. On both image classification and segmentation tasks, our method achieves state-of-the-art performances in confidence estimation. Furthermore, we show the benefit of the proposed method through a downstream active learning task. The code is available at https://github.com/TopoXLab/consistency-ranking-lossTranslate the text into Simplified Chinese:<<SYS>>深度神经网络中的过于自信是一个常见的问题，限制其在实际应用中的部署。现有方法主要集中在完全监督的enario中，通过训练标签来估计自信。在这篇论文中，我们提出了首个半监督情况下的自信估计方法，当多个训练标签不可用时。我们认为，即使受限的训练标签，我们仍可以通过训练过程中的预测一致性来合理地估计模型对未标注样本的自信。我们使用训练一致性作为准则函数，并提出了一种一致排名损失来估计自信。在图像分类和分割任务上，我们的方法实现了状态级表现。此外，我们还证明了我们的方法在下游活动学任务中的利好。代码可以在https://github.com/TopoXLab/consistency-ranking-loss上获取。Translation note:* "深度神经网络" is translated as "deep neural network"* "过于自信" is translated as "overconfidence"* "半监督情况" is translated as "semi-supervised setting"* "训练标签" is translated as "training labels"* "未标注样本" is translated as "unlabeled samples"* "一致性" is translated as "consistency"* "准则函数" is translated as "surrogate function"* "一致排名损失" is translated as "consistency ranking loss"
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Quantification-for-Molecular-Property-Predictions-with-Graph-Neural-Architecture-Search"><a href="#Uncertainty-Quantification-for-Molecular-Property-Predictions-with-Graph-Neural-Architecture-Search" class="headerlink" title="Uncertainty Quantification for Molecular Property Predictions with Graph Neural Architecture Search"></a>Uncertainty Quantification for Molecular Property Predictions with Graph Neural Architecture Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10438">http://arxiv.org/abs/2307.10438</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengli Jiang, Shiyi Qin, Reid C. Van Lehn, Prasanna Balaprakash, Victor M. Zavala</li>
<li>for: 该论文旨在提出一种自动化uncertainty quantification（UQ）方法，以便在分子性质预测中提高模型的可靠性和信息价值。</li>
<li>methods: 该论文使用了architecture search来生成一个高性能的GNN ensemble，并使用variance decomposition来分解数据和模型不确定性。</li>
<li>results: 对多个 benchmark dataset进行了计算实验，并证明了AutoGNNUQ在预测精度和UQ性能方面与现有的UQ方法相比，表现出了明显的优势。此外，通过t-SNE可视化，探索了分子特征和不确定性之间的相互关系，提供了数据集改进的指导。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have emerged as a prominent class of data-driven methods for molecular property prediction. However, a key limitation of typical GNN models is their inability to quantify uncertainties in the predictions. This capability is crucial for ensuring the trustworthy use and deployment of models in downstream tasks. To that end, we introduce AutoGNNUQ, an automated uncertainty quantification (UQ) approach for molecular property prediction. AutoGNNUQ leverages architecture search to generate an ensemble of high-performing GNNs, enabling the estimation of predictive uncertainties. Our approach employs variance decomposition to separate data (aleatoric) and model (epistemic) uncertainties, providing valuable insights for reducing them. In our computational experiments, we demonstrate that AutoGNNUQ outperforms existing UQ methods in terms of both prediction accuracy and UQ performance on multiple benchmark datasets. Additionally, we utilize t-SNE visualization to explore correlations between molecular features and uncertainty, offering insight for dataset improvement. AutoGNNUQ has broad applicability in domains such as drug discovery and materials science, where accurate uncertainty quantification is crucial for decision-making.
</details>
<details>
<summary>摘要</summary>
格raph神经网络（GNNs）已经成为分子性质预测中一种显著的数据驱动方法。然而，典型的GNN模型无法量化预测结果的不确定性。这种能力是下游任务中模型使用和部署的信任worthy的前提。为此，我们介绍AutoGNNUQ，一种自动 uncertainty quantification（UQ）方法 для分子性质预测。AutoGNNUQ利用架构搜索生成高性能GNN的ensemble，以便估计预测不确定性。我们的方法使用变差分解将数据（aleatoric）和模型（epistemic）不确定性分开，提供有价值的反馈 для降低其。在我们的计算实验中，我们证明AutoGNNUQ在多个benchmark数据集上比现有的UQ方法更高效， both in terms of prediction accuracy and UQ performance。此外，我们使用t-SNE可视化来探索分子特征和不确定性之间的相关性，为数据集改进提供了新的视角。AutoGNNUQ在药物搜索和材料科学等领域有广泛的应用，其中准确地量化不确定性是重要的决策前提。
</details></li>
</ul>
<hr>
<h2 id="A-Bayesian-Programming-Approach-to-Car-following-Model-Calibration-and-Validation-using-Limited-Data"><a href="#A-Bayesian-Programming-Approach-to-Car-following-Model-Calibration-and-Validation-using-Limited-Data" class="headerlink" title="A Bayesian Programming Approach to Car-following Model Calibration and Validation using Limited Data"></a>A Bayesian Programming Approach to Car-following Model Calibration and Validation using Limited Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10437">http://arxiv.org/abs/2307.10437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Franklin Abodo</li>
<li>for: 这个论文主要是为了提出一种可以准确模拟驾驶行为的车流模型，以便在交通研究和工程中设计和评估工程改进方案时能够更好地考虑安全和其他绩效指标。</li>
<li>methods: 这个论文使用了微型驾驶模型，并使用 bayesian 方法进行数据分析和参数估计，以便更好地捕捉和复制驾驶行为。</li>
<li>results: 这个论文通过 bayesian 方法来探讨和解决模型准确性问题，并发现了模型层次结构的利用可以提高模型的准确性。<details>
<summary>Abstract</summary>
Traffic simulation software is used by transportation researchers and engineers to design and evaluate changes to roadways. These simulators are driven by models of microscopic driver behavior from which macroscopic measures like flow and congestion can be derived. Many models are designed for a subset of possible traffic scenarios and roadway configurations, while others have no explicit constraints on their application. Work zones (WZs) are one scenario for which no model to date has reproduced realistic driving behavior. This makes it difficult to optimize for safety and other metrics when designing a WZ. The Federal Highway Administration commissioned the USDOT Volpe Center to develop a car-following (CF) model for use in microscopic simulators that can capture and reproduce driver behavior accurately within and outside of WZs. Volpe also performed a naturalistic driving study to collect telematics data from vehicles driven on roads with WZs for use in model calibration. During model development, Volpe researchers observed difficulties in calibrating their model, leaving them to question whether there existed flaws in their model, in the data, or in the procedure used to calibrate the model using the data. In this thesis, I use Bayesian methods for data analysis and parameter estimation to explore and, where possible, address these questions. First, I use Bayesian inference to measure the sufficiency of the size of the data set. Second, I compare the procedure and results of the genetic algorithm based calibration performed by the Volpe researchers with those of Bayesian calibration. Third, I explore the benefits of modeling CF hierarchically. Finally, I apply what was learned in the first three phases using an established CF model, Wiedemann 99, to the probabilistic modeling of the Volpe model. Validation is performed using information criteria as an estimate of predictive accuracy.
</details>
<details>
<summary>摘要</summary>
交通模拟软件由交通研究人员和工程师使用来设计和评估道路变化。这些模拟器由微型驾驶行为模型驱动，从而得到流和堵塞等宏观指标。许多模型适用于特定的交通情况和道路配置，而其他的则没有明确的应用约束。工地（WZ）是一种情况，在其中没有任何模型可以模拟真实的驾驶行为。这使得在设计工地时很难优化安全和其他指标。美国公路管理局（FHWA）委托美国交通部Volpe中心开发一个可以在微型模拟器中使用的拥挤（CF）模型，以便准确模拟驾驶行为。Volpe中心还进行了一项自然驾驶研究，收集了在道路上驾驶的车辆数据，以便在模型准确性校准中使用。在模型开发过程中，Volpe研究人员发现了困难在模型准确性校准中，这使得他们开始思考是否存在模型中的问题，数据中的问题，或者在使用数据进行模型准确性校准时的问题。在这个论文中，我使用 bayesian 方法来分析数据和参数估计，以探索和解决这些问题。首先，我使用 bayesian 推理来测量数据集的尺度是否充分。其次，我比较了使用 bayesian 准确性校准的结果和Volpe研究人员使用的遗传算法基于准确性校准的结果。最后，我探索了模型CF层次化的好处。最后，我使用已有的CF模型，Wiedemann 99，来应用学习到的知识，对Volpe模型的 probabilistic 模型进行模拟。验证是通过信息指标来估计预测精度。
</details></li>
</ul>
<hr>
<h2 id="A-Matrix-Ensemble-Kalman-Filter-based-Multi-arm-Neural-Network-to-Adequately-Approximate-Deep-Neural-Networks"><a href="#A-Matrix-Ensemble-Kalman-Filter-based-Multi-arm-Neural-Network-to-Adequately-Approximate-Deep-Neural-Networks" class="headerlink" title="A Matrix Ensemble Kalman Filter-based Multi-arm Neural Network to Adequately Approximate Deep Neural Networks"></a>A Matrix Ensemble Kalman Filter-based Multi-arm Neural Network to Adequately Approximate Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10436">http://arxiv.org/abs/2307.10436</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ved-piyush/menkf-ann-pul">https://github.com/ved-piyush/menkf-ann-pul</a></li>
<li>paper_authors: Ved Piyush, Yuchen Yan, Yuzhen Zhou, Yanbin Yin, Souparno Ghosh</li>
<li>for: 这个论文旨在提出一种基于 kalman 筛法的多手柄神经网络（MEnKF-ANN），用于缺省样本数量太小以训练多手柄神经网络。</li>
<li>methods: 该论文使用 kalman 筛法来 aproximate 神经网络，并实现了显式的模型堆叠。</li>
<li>results: 该论文可以”应对”一个基于 long short-term memory（LSTM）网络的分类问题，并可以为这个问题提供不确定性评估。<details>
<summary>Abstract</summary>
Deep Learners (DLs) are the state-of-art predictive mechanism with applications in many fields requiring complex high dimensional data processing. Although conventional DLs get trained via gradient descent with back-propagation, Kalman Filter (KF)-based techniques that do not need gradient computation have been developed to approximate DLs. We propose a multi-arm extension of a KF-based DL approximator that can mimic DL when the sample size is too small to train a multi-arm DL. The proposed Matrix Ensemble Kalman Filter-based multi-arm ANN (MEnKF-ANN) also performs explicit model stacking that becomes relevant when the training sample has an unequal-size feature set. Our proposed technique can approximate Long Short-term Memory (LSTM) Networks and attach uncertainty to the predictions obtained from these LSTMs with desirable coverage. We demonstrate how MEnKF-ANN can "adequately" approximate an LSTM network trained to classify what carbohydrate substrates are digested and utilized by a microbiome sample whose genomic sequences consist of polysaccharide utilization loci (PULs) and their encoded genes.
</details>
<details>
<summary>摘要</summary>
深度学习器（DL）是现状最高的预测机制，应用于需要复杂高维数据处理的多个领域。虽然传统的DL通过梯度下降和反射来训练，但Kalman滤波器（KF）基本技术不需要计算梯度，可以用来近似DL。我们提出了一种基于KF的多支武器ANN（MEnKF-ANN），可以在样本数太小以训练多支武器DL时模拟DL。我们的提议的技术还实现了显式模型堆叠，当特征集的大小不同时成为重要的。我们的提议的技术可以近似长期快速储存网络（LSTM）和attachuncertainty到这些LSTM的预测结果，并且可以获得可接受的保证。我们示例中用MEnKF-ANN来"合理"地近似一个基于LSTM网络训练的分类微生物材料的碳水化物substrates是否被微生物材料的PULs和其编码的基因消化和利用。
</details></li>
</ul>
<hr>
<h2 id="Learning-Formal-Specifications-from-Membership-and-Preference-Queries"><a href="#Learning-Formal-Specifications-from-Membership-and-Preference-Queries" class="headerlink" title="Learning Formal Specifications from Membership and Preference Queries"></a>Learning Formal Specifications from Membership and Preference Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10434">http://arxiv.org/abs/2307.10434</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ameesh Shah, Marcell Vazquez-Chanlatte, Sebastian Junges, Sanjit A. Seshia</li>
<li>for: 本研究探讨了活动学习在形式规定中的应用，包括自动机。</li>
<li>methods: 本文提出了一种新的框架，通过组合成员标签和对比 preference 来实现活动规定学习。</li>
<li>results: 我们在两个不同领域中实现了我们的框架，结果表明可以通过成员和偏好来稳定地识别规定。<details>
<summary>Abstract</summary>
Active learning is a well-studied approach to learning formal specifications, such as automata. In this work, we extend active specification learning by proposing a novel framework that strategically requests a combination of membership labels and pair-wise preferences, a popular alternative to membership labels. The combination of pair-wise preferences and membership labels allows for a more flexible approach to active specification learning, which previously relied on membership labels only. We instantiate our framework in two different domains, demonstrating the generality of our approach. Our results suggest that learning from both modalities allows us to robustly and conveniently identify specifications via membership and preferences.
</details>
<details>
<summary>摘要</summary>
active learning 是一种已经广泛研究的学习方法，用于学习正式规范，如自动机。在这项工作中，我们延伸了活动规范学习，提议一种新的框架，强制请求组合成员标签和对比标签，这是成员标签的受欢迎替代方案。这种组合的成员标签和对比标签允许我们在活动规范学习中采用更灵活的方法，之前只靠成员标签。我们在两个不同领域中实现了我们的框架，并证明了我们的方法的通用性。我们的结果表明，从两种模态中学习可以强健地和方便地识别规范 via 成员和偏好。
</details></li>
</ul>
<hr>
<h2 id="DP-TBART-A-Transformer-based-Autoregressive-Model-for-Differentially-Private-Tabular-Data-Generation"><a href="#DP-TBART-A-Transformer-based-Autoregressive-Model-for-Differentially-Private-Tabular-Data-Generation" class="headerlink" title="DP-TBART: A Transformer-based Autoregressive Model for Differentially Private Tabular Data Generation"></a>DP-TBART: A Transformer-based Autoregressive Model for Differentially Private Tabular Data Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10430">http://arxiv.org/abs/2307.10430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rodrigo Castellon, Achintya Gopal, Brian Bloniarz, David Rosenberg</li>
<li>for: 本文旨在提出一种能够保持分布式隐私的生成 tabular 数据的方法，并且能够与传统的 Margin-based 方法竞争。</li>
<li>methods: 本文使用 transformer 核心的 autoregressive 模型，并实现了分布式隐私。</li>
<li>results: 本文在各种数据集上达到了与 Margin-based 方法竞争的性能，甚至在某些情况下超越了状态之artefacts。<details>
<summary>Abstract</summary>
The generation of synthetic tabular data that preserves differential privacy is a problem of growing importance. While traditional marginal-based methods have achieved impressive results, recent work has shown that deep learning-based approaches tend to lag behind. In this work, we present Differentially-Private TaBular AutoRegressive Transformer (DP-TBART), a transformer-based autoregressive model that maintains differential privacy and achieves performance competitive with marginal-based methods on a wide variety of datasets, capable of even outperforming state-of-the-art methods in certain settings. We also provide a theoretical framework for understanding the limitations of marginal-based approaches and where deep learning-based approaches stand to contribute most. These results suggest that deep learning-based techniques should be considered as a viable alternative to marginal-based methods in the generation of differentially private synthetic tabular data.
</details>
<details>
<summary>摘要</summary>
“ differential privacy 的Synthetic tabular data生成问题是一个快速增长的领域。传统的边缘基数方法已经实现了很好的结果，但最近的研究表明，深度学习基于方法在这个领域的表现相对落后。在这个工作中，我们介绍了Diffentially-Private TaBular AutoRegressive Transformer（DP-TBART），一种基于 transformer 的自适应模型，保持了 differential privacy 并在各种数据集上达到了与边缘基数方法相当的性能，甚至在某些设置下超越了状态码的最佳方法。我们还提供了理论框架，用于理解边缘基数方法的局限性和深度学习基数方法在这个领域的潜在贡献。这些结果表明，深度学习基数方法应该被视为 Synthetic tabular data 生成 differential privacy 的可行的代替方案。”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="PreDiff-Precipitation-Nowcasting-with-Latent-Diffusion-Models"><a href="#PreDiff-Precipitation-Nowcasting-with-Latent-Diffusion-Models" class="headerlink" title="PreDiff: Precipitation Nowcasting with Latent Diffusion Models"></a>PreDiff: Precipitation Nowcasting with Latent Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10422">http://arxiv.org/abs/2307.10422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhihan Gao, Xingjian Shi, Boran Han, Hao Wang, Xiaoyong Jin, Danielle Maddix, Yi Zhu, Mu Li, Yuyang Wang</li>
<li>for: 这个论文旨在提出一种两Stage管道，用于各种 Earth system forecasting 任务中的 probabilistic spatiotemporal 预测。</li>
<li>methods: 该模型使用 conditional latent diffusion 方法，并通过显式控制知识机制来对预测结果进行 Physical 约束。</li>
<li>results: 实验表明，PreDiff 模型可以具有高效的操作用途，并能够处理不确定性和遵循物理法则的预测结果。<details>
<summary>Abstract</summary>
Earth system forecasting has traditionally relied on complex physical models that are computationally expensive and require significant domain expertise. In the past decade, the unprecedented increase in spatiotemporal Earth observation data has enabled data-driven forecasting models using deep learning techniques. These models have shown promise for diverse Earth system forecasting tasks but either struggle with handling uncertainty or neglect domain-specific prior knowledge, resulting in averaging possible futures to blurred forecasts or generating physically implausible predictions. To address these limitations, we propose a two-stage pipeline for probabilistic spatiotemporal forecasting: 1) We develop PreDiff, a conditional latent diffusion model capable of probabilistic forecasts. 2) We incorporate an explicit knowledge control mechanism to align forecasts with domain-specific physical constraints. This is achieved by estimating the deviation from imposed constraints at each denoising step and adjusting the transition distribution accordingly. We conduct empirical studies on two datasets: N-body MNIST, a synthetic dataset with chaotic behavior, and SEVIR, a real-world precipitation nowcasting dataset. Specifically, we impose the law of conservation of energy in N-body MNIST and anticipated precipitation intensity in SEVIR. Experiments demonstrate the effectiveness of PreDiff in handling uncertainty, incorporating domain-specific prior knowledge, and generating forecasts that exhibit high operational utility.
</details>
<details>
<summary>摘要</summary>
地球系统预报traditionally rely 于复杂的物理模型， Computationally expensive 和需要特定领域专家知识。在过去的一代，随着无前例的空间时间地球观测数据的增加，使用深度学习技术的数据驱动预报模型得到了推广。这些模型在多元地球预报任务中表现了应用潜力，但是它们 either struggle with handling uncertainty or neglect domain-specific prior knowledge, resulting in averaging possible futures to blurred forecasts or generating physically implausible predictions.为了解决这些限制，我们提出了一个二阶段管道，用于数值空间时间预报：1. We develop PreDiff，一个可 probabilistic forecasts的 conditional latent diffusion model。2. We incorporate an explicit knowledge control mechanism to align forecasts with domain-specific physical constraints. This is achieved by estimating the deviation from imposed constraints at each denoising step and adjusting the transition distribution accordingly.我们在两个 dataset上进行了实验：N-body MNIST，一个Synthetic dataset with chaotic behavior，和SEVIR，一个真实世界降水预报dataset。具体来说，我们在N-body MNIST中强制遵循能量守恒定律，并在SEVIR中预测降水强度。实验结果表明 PreDiff 能够 effectively handle uncertainty， incorporate domain-specific prior knowledge，并生成 forecasts that exhibit high operational utility。
</details></li>
</ul>
<hr>
<h2 id="Technical-Challenges-of-Deploying-Reinforcement-Learning-Agents-for-Game-Testing-in-AAA-Games"><a href="#Technical-Challenges-of-Deploying-Reinforcement-Learning-Agents-for-Game-Testing-in-AAA-Games" class="headerlink" title="Technical Challenges of Deploying Reinforcement Learning Agents for Game Testing in AAA Games"></a>Technical Challenges of Deploying Reinforcement Learning Agents for Game Testing in AAA Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11105">http://arxiv.org/abs/2307.11105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonas Gillberg, Joakim Bergdahl, Alessandro Sestini, Andrew Eakins, Linus Gisslen</li>
<li>for: 本研讨文是为了探讨如何通过增加自动游戏测试解决方案中的强化学习系统来提高测试覆盖率。</li>
<li>methods: 本研究使用了脚本bot和强化学习技术来扩展现有的自动测试解决方案。</li>
<li>results: 研究发现，通过增加强化学习系统，测试覆盖率得到了显著提高，能够减少测试时间并提高游戏质量。<details>
<summary>Abstract</summary>
Going from research to production, especially for large and complex software systems, is fundamentally a hard problem. In large-scale game production, one of the main reasons is that the development environment can be very different from the final product. In this technical paper we describe an effort to add an experimental reinforcement learning system to an existing automated game testing solution based on scripted bots in order to increase its capacity. We report on how this reinforcement learning system was integrated with the aim to increase test coverage similar to [1] in a set of AAA games including Battlefield 2042 and Dead Space (2023). The aim of this technical paper is to show a use-case of leveraging reinforcement learning in game production and cover some of the largest time sinks anyone who wants to make the same journey for their game may encounter. Furthermore, to help the game industry to adopt this technology faster, we propose a few research directions that we believe will be valuable and necessary for making machine learning, and especially reinforcement learning, an effective tool in game production.
</details>
<details>
<summary>摘要</summary>
原研究到生产的过程，特别是 для大型和复杂的软件系统，是一个基本的困难问题。在大规模游戏生产中，一个主要原因是开发环境与最终产品很不同。在这篇技术论文中，我们描述了一项尝试将 эксперименталь的强化学习系统添加到现有的自动游戏测试解决方案中，以增加其容量。我们报告了在一些AAA游戏，如《战field 2042》和《僵尸空间2023》中，如何将这种强化学习系统与现有的测试解决方案集成，以提高测试覆盖率。本技术论文的目的是向游戏产业展示如何使用强化学习在游戏生产中，并涵盖一些最大时间沟通的问题。此外，为了帮助游戏业 faster采用这种技术，我们提出了一些研究方向，我们认为将是值得的和必要的 для使机器学习，特别是强化学习，成为游戏生产中的有效工具。
</details></li>
</ul>
<hr>
<h2 id="Interpreting-and-Correcting-Medical-Image-Classification-with-PIP-Net"><a href="#Interpreting-and-Correcting-Medical-Image-Classification-with-PIP-Net" class="headerlink" title="Interpreting and Correcting Medical Image Classification with PIP-Net"></a>Interpreting and Correcting Medical Image Classification with PIP-Net</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10404">http://arxiv.org/abs/2307.10404</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/m-nauta/pipnet">https://github.com/m-nauta/pipnet</a></li>
<li>paper_authors: Meike Nauta, Johannes H. Hegeman, Jeroen Geerdink, Jörg Schlötterer, Maurice van Keulen, Christin Seifert</li>
<li>for: 这篇论文探讨了使用可解释机器学习模型对实际医疗影像数据的自动诊断支持。</li>
<li>methods: 这篇论文使用了PIP-Net模型，一种可解释的image classifier，并评估了其精度和可解释性在骨折检测和皮肤癌诊断中。</li>
<li>results: 结果显示PIP-Net的决策过程与医疗分类标准相符，并且可以轻松地识别数据质量问题，例如医疗影像中的不适合文本或标签错误。此外，我们首次显示了人类可以手动更正PIP-Net的推理，例如禁用不需要的标本。<details>
<summary>Abstract</summary>
Part-prototype models are explainable-by-design image classifiers, and a promising alternative to black box AI. This paper explores the applicability and potential of interpretable machine learning, in particular PIP-Net, for automated diagnosis support on real-world medical imaging data. PIP-Net learns human-understandable prototypical image parts and we evaluate its accuracy and interpretability for fracture detection and skin cancer diagnosis. We find that PIP-Net's decision making process is in line with medical classification standards, while only provided with image-level class labels. Because of PIP-Net's unsupervised pretraining of prototypes, data quality problems such as undesired text in an X-ray or labelling errors can be easily identified. Additionally, we are the first to show that humans can manually correct the reasoning of PIP-Net by directly disabling undesired prototypes. We conclude that part-prototype models are promising for medical applications due to their interpretability and potential for advanced model debugging.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXT全文模型是可解释的设计图像分类器，是黑obox AI的可行替代方案。这篇论文探讨了可解释机器学习的可行性和潜力，特别是PIP-Net在医疗影像数据自动诊断支持方面的应用。PIP-Net学习了人理解的图像组成部分，我们评估了它的准确率和可解释性在骨折检测和皮肤癌诊断方面。我们发现PIP-Net的决策过程与医学分类标准相一致，只需要提供图像级别的标签。由于PIP-Net的无监督prototype学习，数据质量问题 such as 不想要的文本在X射线图像或标签错误可以轻松地被识别出来。此外，我们是第一个表明人可以手动更正PIP-Net的理由的。我们 conclude that part-prototype models are promising for medical applications due to their interpretability and potential for advanced model debugging.<<SYS>>
</details></li>
</ul>
<hr>
<h2 id="Selection-functions-of-strong-lens-finding-neural-networks"><a href="#Selection-functions-of-strong-lens-finding-neural-networks" class="headerlink" title="Selection functions of strong lens finding neural networks"></a>Selection functions of strong lens finding neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10355">http://arxiv.org/abs/2307.10355</a></li>
<li>repo_url: None</li>
<li>paper_authors: A. Herle, C. M. O’Riordan, S. Vegetti</li>
<li>for: 这个论文是为了研究弯光神经网络在找到弯光系统方面的偏见性。</li>
<li>methods: 这个论文使用了类似于常见文献中的弯光神经网络架构和训练数据来训练弯光找到器。</li>
<li>results: 研究发现，使用常见的训练数据集，弯光神经网络会偏好大 Einstein 半径和大的源星系，source 星系的光度分布更加集中。提高检测 significado 阈值 FROM 8 σ TO 12 σ，只有50%的弯光系统有 Einstein 半径 大于或等于 1.04 arcsec，source 星系的半径大于或等于 0.194 arcsec，和 source Sérsic 指数大于或等于 2.62。模型用于找到弯光 quasi 会对高弯光系统产生更强的偏好。偏见函数不依赖于弯光系统的形态指数，因此测量这种量将不受影响。<details>
<summary>Abstract</summary>
Convolution Neural Networks trained for the task of lens finding with similar architecture and training data as is commonly found in the literature are biased classifiers. An understanding of the selection function of lens finding neural networks will be key to fully realising the potential of the large samples of strong gravitational lens systems that will be found in upcoming wide-field surveys. We use three training datasets, representative of those used to train galaxy-galaxy and galaxy-quasar lens finding neural networks. The networks preferentially select systems with larger Einstein radii and larger sources with more concentrated source-light distributions. Increasing the detection significance threshold to 12$\sigma$ from 8$\sigma$ results in 50 per cent of the selected strong lens systems having Einstein radii $\theta_\mathrm{E}$ $\ge$ 1.04 arcsec from $\theta_\mathrm{E}$ $\ge$ 0.879 arcsec, source radii $R_S$ $\ge$ 0.194 arcsec from $R_S$ $\ge$ 0.178 arcsec and source S\'ersic indices $n_{\mathrm{Sc}}^{\mathrm{S}}$ $\ge$ 2.62 from $n_{\mathrm{Sc}}^{\mathrm{S}}$ $\ge$ 2.55. The model trained to find lensed quasars shows a stronger preference for higher lens ellipticities than those trained to find lensed galaxies. The selection function is independent of the slope of the power-law of the mass profiles, hence measurements of this quantity will be unaffected. The lens finder selection function reinforces that of the lensing cross-section, and thus we expect our findings to be a general result for all galaxy-galaxy and galaxy-quasar lens finding neural networks.
</details>
<details>
<summary>摘要</summary>
干扰神经网络（Convolutional Neural Networks，CNNS）用于弯光镜搜索任务，与文献中常见的architecture和训练数据相同，会受到偏见。我们需要理解弯光镜搜索神经网络的选择函数，以便在未来的广泛访问Surveys中全面利用大量强 gravitational lens系统的潜力。我们使用了三个训练数据集，代表了通常用于训练 galaxy-galaxy和galaxy-quasar弯光镜搜索神经网络。这些网络偏好具有更大的爱因斯坦半径和更集中的源光谱分布。如果提高检测 significancethreshold到12$\sigma$从8$\sigma$，则50%选择的强弯光系统具有 $\theta_\mathrm{E}$ $\ge$ 1.04 arcsec，Source radii $R_S$ $\ge$ 0.194 arcsec，source S\'ersic indices $n_{\mathrm{Sc}}^{\mathrm{S}}$ $\ge$ 2.62。模型用于搜索弯光镜 quasiars显示更强的偏好于高弯光镜形状因子，而不是用于搜索弯光镜 galaxies。选择函数独立于弯光镜形状因子的斜率，因此测量这一量将不受影响。弯光镜选择函数会强化弯光镜批处函数，我们预计这些结果将适用于所有galaxy-galaxy和galaxy-quasar弯光镜搜索神经网络。
</details></li>
</ul>
<hr>
<h2 id="LightPath-Lightweight-and-Scalable-Path-Representation-Learning"><a href="#LightPath-Lightweight-and-Scalable-Path-Representation-Learning" class="headerlink" title="LightPath: Lightweight and Scalable Path Representation Learning"></a>LightPath: Lightweight and Scalable Path Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10171">http://arxiv.org/abs/2307.10171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sean Bin Yang, Jilin Hu, Chenjuan Guo, Bin Yang, Christian S. Jensen</li>
<li>for: 本研究旨在提供一种轻量级、可扩展的路径表示学习框架，以提高智能交通和智能城市应用中的路径表示学习效能。</li>
<li>methods: 本研究提议使用稀疏自编码器来实现轻量级和可扩展性，并通过关系逻辑推理和全局地域知识传播来快速训练稀疏路径编码器。</li>
<li>results: 经过广泛的实验，本研究发现LightPath框架可以减少资源消耗，同时保持高度的效能和可扩展性，并在实际应用中表现出色。<details>
<summary>Abstract</summary>
Movement paths are used widely in intelligent transportation and smart city applications. To serve such applications, path representation learning aims to provide compact representations of paths that enable efficient and accurate operations when used for different downstream tasks such as path ranking and travel cost estimation. In many cases, it is attractive that the path representation learning is lightweight and scalable; in resource-limited environments and under green computing limitations, it is essential. Yet, existing path representation learning studies focus on accuracy and pay at most secondary attention to resource consumption and scalability.   We propose a lightweight and scalable path representation learning framework, termed LightPath, that aims to reduce resource consumption and achieve scalability without affecting accuracy, thus enabling broader applicability. More specifically, we first propose a sparse auto-encoder that ensures that the framework achieves good scalability with respect to path length. Next, we propose a relational reasoning framework to enable faster training of more robust sparse path encoders. We also propose global-local knowledge distillation to further reduce the size and improve the performance of sparse path encoders. Finally, we report extensive experiments on two real-world datasets to offer insight into the efficiency, scalability, and effectiveness of the proposed framework.
</details>
<details>
<summary>摘要</summary>
路径表示法广泛应用于智能交通和智能城市应用场景中。为了服务这些应用场景，路径表示学习目的是提供高效精度的路径表示，以便在不同的下游任务中进行高效的操作，如路径排名和旅行成本估算。在资源有限和绿色计算限制的情况下， existing 路径表示学习研究主要关注精度，并仅征服其次的资源消耗和可扩展性。我们提出了一个轻量级和可扩展的路径表示学习框架，称为LightPath，以降低资源消耗和实现可扩展性，而不影响准确性。更 Specifically，我们首先提出了一种稀疏自动编码器，以确保框架在路径长度方面具有良好的扩展性。然后，我们提出了一种关系逻辑框架，以快速训练更加稀疏的路径编码器。 finally，我们提出了全球-本地知识传播，以进一步减小路径编码器的大小和提高其性能。我们在两个真实世界数据集上进行了广泛的实验，以便了解LightPath框架的效率、可扩展性和有效性。
</details></li>
</ul>
<hr>
<h2 id="Challenges-and-Applications-of-Large-Language-Models"><a href="#Challenges-and-Applications-of-Large-Language-Models" class="headerlink" title="Challenges and Applications of Large Language Models"></a>Challenges and Applications of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10169">http://arxiv.org/abs/2307.10169</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, Robert McHardy</li>
<li>for: 本研究旨在提供系统化的开Problem和应用成功列表，以帮助ML研究人员更快地了解领域的当前状况，并更快地成为产品人。</li>
<li>methods: 本研究采用系统化的方法来列出领域的开Problem和应用成功，包括Literature Review和Expert Interview等。</li>
<li>results: 本研究提出了一系列的开Problem和应用成功，包括语言模型的泛化和鲁棒性问题，以及应用于自然语言处理、计算机视觉和自然语言生成等领域的成功应用。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learning discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to establish a systematic set of open problems and application successes so that ML researchers can comprehend the field's current state more quickly and become productive.
</details>
<details>
<summary>摘要</summary>
庞大语言模型（LLM）从不存在到普遍的机器学习讨论中只需几年的时间。由于这个领域的快速发展，现在很难确定还有哪些挑战和已经成功应用的领域。在这篇论文中，我们希望建立一个系统的开放问题和应用成功的集合，让机器学习研究人员更快地理解领域的当前状况，更快地成为产品力。
</details></li>
</ul>
<hr>
<h2 id="VITS-Variational-Inference-Thomson-Sampling-for-contextual-bandits"><a href="#VITS-Variational-Inference-Thomson-Sampling-for-contextual-bandits" class="headerlink" title="VITS : Variational Inference Thomson Sampling for contextual bandits"></a>VITS : Variational Inference Thomson Sampling for contextual bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10167">http://arxiv.org/abs/2307.10167</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Clavier, Tom Huix, Alain Durmus</li>
<li>for: 本文介绍了一种Contextual Bandit中的Thompson sampling（TS）算法变体，解决了在每个轮次中需要样本来自当前 posterior distribution 的问题。</li>
<li>methods: 本文提出了一种基于 Gaussian Variational Inference 的新算法，称为 Varitional Inference Thompson Sampling（VITS），可以提供高效的 posterior approximation，并且容易从样本中采样。</li>
<li>results: 本文证明了 VITS 可以在 linear contextual bandit 中实现 sub-linear regret bound，与传统 TS 的 regret bound 相同水平，并且在实验中表现出色。<details>
<summary>Abstract</summary>
In this paper, we introduce and analyze a variant of the Thompson sampling (TS) algorithm for contextual bandits. At each round, traditional TS requires samples from the current posterior distribution, which is usually intractable. To circumvent this issue, approximate inference techniques can be used and provide samples with distribution close to the posteriors. However, current approximate techniques yield to either poor estimation (Laplace approximation) or can be computationally expensive (MCMC methods, Ensemble sampling...). In this paper, we propose a new algorithm, Varational Inference Thompson sampling VITS, based on Gaussian Variational Inference. This scheme provides powerful posterior approximations which are easy to sample from, and is computationally efficient, making it an ideal choice for TS. In addition, we show that VITS achieves a sub-linear regret bound of the same order in the dimension and number of round as traditional TS for linear contextual bandit. Finally, we demonstrate experimentally the effectiveness of VITS on both synthetic and real world datasets.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种Contextual Bandit中的Thompson抽样（TS）算法的变体。在每个轮次上，传统的TS需要从当前 posterior distribution 中取样，这通常是不可能的。为了缺乏这个问题，我们可以使用approximate inference技术，以提供 samples 的 distribution 接近 posterior。然而，现有的approximate技术可能会导致 poor estimation（Laplace approximation）或者是 computationally expensive（MCMC方法、Ensemble sampling...）。在这篇论文中，我们提出了一种新的算法，即 Gaussian Variational Inference 基于的 Varitional Inference Thompson Sampling（VITS）。这种方案提供了强大的 posterior 近似，易于采样，并且 computationally efficient，使其成为TS的理想选择。此外，我们证明了 VITS 在 linear contextual bandit 中可以 достичь同样的下界 regret bound，与传统TS的 regret bound 相同。最后，我们通过实验表明 VITS 在 synthetic 和实际世界数据上具有良好的效果。
</details></li>
</ul>
<hr>
<h2 id="Improving-Multimodal-Datasets-with-Image-Captioning"><a href="#Improving-Multimodal-Datasets-with-Image-Captioning" class="headerlink" title="Improving Multimodal Datasets with Image Captioning"></a>Improving Multimodal Datasets with Image Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10350">http://arxiv.org/abs/2307.10350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thao Nguyen, Samir Yitzhak Gadre, Gabriel Ilharco, Sewoong Oh, Ludwig Schmidt</li>
<li>for: 这个研究的目的是提高大型视觉语言模型的性能，通过增强网络数据的质量。</li>
<li>methods: 该研究使用生成caption来减少网络数据的噪声，并对不同的混合策略进行比较。</li>
<li>results: 相比DataCompbenchmark中提出的最佳策略，该研究的最佳方法在ImageNet和38个任务中占据2%和4%的优势，并在Flickr和MS-COCO检索中表现出2倍的提升。此外，研究还分析了生成caption的有效性，并证明了标准图像描述文本模型的性能不是训练多模态任务中文本超级vision模型的准确指标。<details>
<summary>Abstract</summary>
Massive web datasets play a key role in the success of large vision-language models like CLIP and Flamingo. However, the raw web data is noisy, and existing filtering methods to reduce noise often come at the expense of data diversity. Our work focuses on caption quality as one major source of noise, and studies how generated captions can increase the utility of web-scraped datapoints with nondescript text. Through exploring different mixing strategies for raw and generated captions, we outperform the best filtering method proposed by the DataComp benchmark by 2% on ImageNet and 4% on average across 38 tasks, given a candidate pool of 128M image-text pairs. Our best approach is also 2x better at Flickr and MS-COCO retrieval. We then analyze what makes synthetic captions an effective source of text supervision. In experimenting with different image captioning models, we also demonstrate that the performance of a model on standard image captioning benchmarks (e.g., NoCaps CIDEr) is not a reliable indicator of the utility of the captions it generates for multimodal training. Finally, our experiments with using generated captions at DataComp's large scale (1.28B image-text pairs) offer insights into the limitations of synthetic text, as well as the importance of image curation with increasing training data quantity.
</details>
<details>
<summary>摘要</summary>
巨大的网络数据集在大型视觉语言模型 like CLIP 和 Flamingo 的成功中发挥着关键作用。然而，原始网络数据具有噪音，现有的过滤方法通常会导致数据多样性减少。我们的工作将注重caption质量作为噪音的一个主要来源，研究如何使用生成的caption提高网络抓取的datapoints中的描述文本。通过不同的混合策略来处理原始和生成的caption，我们在ImageNet上比DataComp Benchmark中提出的最佳过滤方法高出2%，平均在38个任务上高出4%，给定128M个图像文本对。我们的最佳方法还在Flickr和MS-COCO中 Retrieval上高出2倍。我们还分析了生成caption的效果，并在不同的图像captioning模型中进行了实验。最后，我们在DataComp的大规模 scale（1.28B个图像文本对）上进行了实验，并提供了生成caption的局限性以及图像准备的重要性，随着训练数据量的增加。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Backdoor-Attacks"><a href="#Rethinking-Backdoor-Attacks" class="headerlink" title="Rethinking Backdoor Attacks"></a>Rethinking Backdoor Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10163">http://arxiv.org/abs/2307.10163</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lancopku/SOS">https://github.com/lancopku/SOS</a></li>
<li>paper_authors: Alaa Khaddaj, Guillaume Leclerc, Aleksandar Makelov, Kristian Georgiev, Hadi Salman, Andrew Ilyas, Aleksander Madry</li>
<li>for: 本文旨在探讨反向攻击问题，具体来说是在训练集中插入恶意构建的背门键入例会让模型易受到操纵。</li>
<li>methods: 本文提出了一种不同的方法来对反向攻击进行防御。具体来说是通过训练数据分布的结构信息来探测和除掉恶意构建的背门键入例。</li>
<li>results: 本文表明，在缺乏训练数据分布结构信息的情况下，反向攻击和自然出现的特征无法分辨，因此不可以通过普通的检测方法来探测。此外，本文还探讨了现有的反向攻击防御策略，并指出了这些策略的假设和依赖关系。最后，本文提出了一种新的反向攻击检测原理，并开发了一种有理论保证和实践效果的检测算法。<details>
<summary>Abstract</summary>
In a backdoor attack, an adversary inserts maliciously constructed backdoor examples into a training set to make the resulting model vulnerable to manipulation. Defending against such attacks typically involves viewing these inserted examples as outliers in the training set and using techniques from robust statistics to detect and remove them.   In this work, we present a different approach to the backdoor attack problem. Specifically, we show that without structural information about the training data distribution, backdoor attacks are indistinguishable from naturally-occurring features in the data--and thus impossible to "detect" in a general sense. Then, guided by this observation, we revisit existing defenses against backdoor attacks and characterize the (often latent) assumptions they make and on which they depend. Finally, we explore an alternative perspective on backdoor attacks: one that assumes these attacks correspond to the strongest feature in the training data. Under this assumption (which we make formal) we develop a new primitive for detecting backdoor attacks. Our primitive naturally gives rise to a detection algorithm that comes with theoretical guarantees and is effective in practice.
</details>
<details>
<summary>摘要</summary>
在一个后门攻击中，敌对者会把恶意构造的后门示例 inserts到训练集中，以使模型易受 manipulate。防御这类攻击通常通过视这些插入的示例为训练集中异常值来实现。在这种工作中，我们提出一种不同的后门攻击问题的解决方案。具体来说，我们表明无论training数据分布的结构信息，后门攻击都是自然出现在数据中的特征——因此不可以在通用的检测中检测到。然后，我们根据这一观察，再次审视了现有的后门攻击防御方法，描述它们所假设和依赖的假设。最后，我们explore一种新的后门攻击视角：假设这些攻击对应于训练数据中最强的特征。根据这个假设（我们将其形式化），我们开发了一种新的后门攻击检测原理。这个原理自然给出了一种检测算法，该算法在理论上有保证并在实践中有效。
</details></li>
</ul>
<hr>
<h2 id="Robust-Driving-Policy-Learning-with-Guided-Meta-Reinforcement-Learning"><a href="#Robust-Driving-Policy-Learning-with-Guided-Meta-Reinforcement-Learning" class="headerlink" title="Robust Driving Policy Learning with Guided Meta Reinforcement Learning"></a>Robust Driving Policy Learning with Guided Meta Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10160">http://arxiv.org/abs/2307.10160</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kanghoon Lee, Jiachen Li, David Isele, Jinkyoo Park, Kikuo Fujimura, Mykel J. Kochenderfer</li>
<li>for: 这篇论文旨在解决现有的Deep Reinforcement Learning（DRL）方法在交通场景中控制社会车辆时存在的问题，即训练环境中使用固定行为策略可能导致学习的驾驶策略过拟合环境。</li>
<li>methods: 我们提出了一种有效的方法，通过随机化社会车辆之间的互动奖励函数，以生成多样化的目标，并通过引导策略来训练单一的meta策略。</li>
<li>results: 我们的方法成功地学习了一个ego驾驶策略，该策略在未看到社会车辆的行为情况下能够总结良好，并且通过在meta策略控制社会车辆的环境中强化驾驶策略来提高其Robustness。<details>
<summary>Abstract</summary>
Although deep reinforcement learning (DRL) has shown promising results for autonomous navigation in interactive traffic scenarios, existing work typically adopts a fixed behavior policy to control social vehicles in the training environment. This may cause the learned driving policy to overfit the environment, making it difficult to interact well with vehicles with different, unseen behaviors. In this work, we introduce an efficient method to train diverse driving policies for social vehicles as a single meta-policy. By randomizing the interaction-based reward functions of social vehicles, we can generate diverse objectives and efficiently train the meta-policy through guiding policies that achieve specific objectives. We further propose a training strategy to enhance the robustness of the ego vehicle's driving policy using the environment where social vehicles are controlled by the learned meta-policy. Our method successfully learns an ego driving policy that generalizes well to unseen situations with out-of-distribution (OOD) social agents' behaviors in a challenging uncontrolled T-intersection scenario.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:although deep reinforcement learning (DRL) has shown promising results for autonomous navigation in interactive traffic scenarios, existing work typically adopts a fixed behavior policy to control social vehicles in the training environment. This may cause the learned driving policy to overfit the environment, making it difficult to interact well with vehicles with different, unseen behaviors. In this work, we introduce an efficient method to train diverse driving policies for social vehicles as a single meta-policy. By randomizing the interaction-based reward functions of social vehicles, we can generate diverse objectives and efficiently train the meta-policy through guiding policies that achieve specific objectives. We further propose a training strategy to enhance the robustness of the ego vehicle's driving policy using the environment where social vehicles are controlled by the learned meta-policy. Our method successfully learns an ego driving policy that generalizes well to unseen situations with out-of-distribution (OOD) social agents' behaviors in a challenging uncontrolled T-intersection scenario.Note: Simplified Chinese is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="Curvature-based-Clustering-on-Graphs"><a href="#Curvature-based-Clustering-on-Graphs" class="headerlink" title="Curvature-based Clustering on Graphs"></a>Curvature-based Clustering on Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10155">http://arxiv.org/abs/2307.10155</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/agosztolai/geometric_clustering">https://github.com/agosztolai/geometric_clustering</a></li>
<li>paper_authors: Yu Tian, Zachary Lubberts, Melanie Weber</li>
<li>for: 本研究探讨了基于图形学的无监督节点归一化（或社区探测）算法，以找到图中紧密连接的子结构，即社区或群体。</li>
<li>methods: 本方法利用图形学的几何特性，通过对图的边权进行变换，以揭示图中的社区结构。我们考虑了多种离散曲率概念，并分析了它们所导致的算法的实用性。</li>
<li>results: 我们提供了许多理论和实验证明，证明了我们的曲率基于归一化算法的实用性。此外，我们还给出了一些关于图形学 curvature和其对副图的关系的结果，这些结果可能对 curvature-based 网络分析有独立的价值。<details>
<summary>Abstract</summary>
Unsupervised node clustering (or community detection) is a classical graph learning task. In this paper, we study algorithms, which exploit the geometry of the graph to identify densely connected substructures, which form clusters or communities. Our method implements discrete Ricci curvatures and their associated geometric flows, under which the edge weights of the graph evolve to reveal its community structure. We consider several discrete curvature notions and analyze the utility of the resulting algorithms. In contrast to prior literature, we study not only single-membership community detection, where each node belongs to exactly one community, but also mixed-membership community detection, where communities may overlap. For the latter, we argue that it is beneficial to perform community detection on the line graph, i.e., the graph's dual. We provide both theoretical and empirical evidence for the utility of our curvature-based clustering algorithms. In addition, we give several results on the relationship between the curvature of a graph and that of its dual, which enable the efficient implementation of our proposed mixed-membership community detection approach and which may be of independent interest for curvature-based network analysis.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>古典图学任务之一是无监督节点划分（或社区探测）。在这篇论文中，我们研究使用图形的几何特性来找出密集连接的子结构，这些子结构组成社区或共同体。我们的方法使用离散 Ricci  curvature 和其相关的几何流动，以便在图形中Edge weights 的演化中揭示社区结构。我们考虑了多种离散 curvature 观念，并分析了其结果的有用性。与先前的文献不同，我们研究不只单一成员社区检测，而是也包括交叉成员社区检测，其中社区可能 overlap。为实现这种检测，我们 argue 是在图形的 dual 上进行社区检测，即 graph 的 dual。我们提供了 both theoretical 和 empirical 证明，以及一些关于离散 curvature 和其 dual 之间的关系，这些关系可能为 curvature-based 网络分析而有益，并且可能具有独立的 интерес。
</details></li>
</ul>
<hr>
<h2 id="Code-Detection-for-Hardware-Acceleration-Using-Large-Language-Models"><a href="#Code-Detection-for-Hardware-Acceleration-Using-Large-Language-Models" class="headerlink" title="Code Detection for Hardware Acceleration Using Large Language Models"></a>Code Detection for Hardware Acceleration Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10348">http://arxiv.org/abs/2307.10348</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Pablo Antonio Martínez, Gregorio Bernabé, José Manuel García</li>
<li>for: 本研究探讨了使用大型自然语言模型（LLM）进行代码检测。</li>
<li>methods: 本研究使用了C&#x2F;C++实现的基本kernels，包括矩阵乘法、卷积和快速傅立叶变换，并提出了一个初步的Naive提示和一种新的提示策略来进行代码检测。</li>
<li>results: 研究发现，使用传统提示策略可以很准确地检测代码，但是受到许多假阳性干扰。使用新的提示策略可以大幅减少假阳性，实现了代码检测的优秀总准确率（91.1%, 97.9%, 和99.7%），这些结果对现有的代码检测方法 pose 了一定的挑战。<details>
<summary>Abstract</summary>
Large language models (LLMs) have been massively applied to many tasks, often surpassing state-of-the-art approaches. While their effectiveness in code generation has been extensively studied (e.g., AlphaCode), their potential for code detection remains unexplored.   This work presents the first analysis of code detection using LLMs. Our study examines essential kernels, including matrix multiplication, convolution, and fast-fourier transform, implemented in C/C++. We propose both a preliminary, naive prompt and a novel prompting strategy for code detection.   Results reveal that conventional prompting achieves great precision but poor accuracy (68.8%, 22.3%, and 79.2% for GEMM, convolution, and FFT, respectively) due to a high number of false positives. Our novel prompting strategy substantially reduces false positives, resulting in excellent overall accuracy (91.1%, 97.9%, and 99.7%, respectively). These results pose a considerable challenge to existing state-of-the-art code detection methods.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经广泛应用于许多任务，经常超越现有的方法。然而，它们在代码检测方面的潜力还没有得到足够的研究。本研究是首次对代码检测使用 LLM 进行分析。我们的研究检查了关键kernels，包括矩阵乘法、卷积和快速傅立叶Transform，实现在C/C++中。我们提出了一种初步的简单提示和一种新的提示策略 для代码检测。结果显示，传统的提示方法可以达到很高的准确率（68.8%、22.3%和79.2%），但是受到许多假阳性的影响，导致总准确率较低（22.3%）。我们的新的提示策略可以减少假阳性，从而实现了优秀的总准确率（91.1%、97.9%和99.7%）。这些结果对现有的代码检测方法提出了很大的挑战。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Potential-Based-Rewards-for-Learning-Humanoid-Locomotion"><a href="#Benchmarking-Potential-Based-Rewards-for-Learning-Humanoid-Locomotion" class="headerlink" title="Benchmarking Potential Based Rewards for Learning Humanoid Locomotion"></a>Benchmarking Potential Based Rewards for Learning Humanoid Locomotion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10142">http://arxiv.org/abs/2307.10142</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/se-hwan/pbrs-humanoid">https://github.com/se-hwan/pbrs-humanoid</a></li>
<li>paper_authors: Se Hwan Jeon, Steve Heim, Charles Khazoom, Sangbae Kim</li>
<li>for: 本研究旨在探讨可能基 reward shaping（PBRS）在人工智能中的应用，以加速学习过程的整合。</li>
<li>methods: 本研究使用了标准的 reward shaping 方法和 PBRS 方法，对人工智能机器人进行了比较。</li>
<li>results: 研究发现，在高维系统中，PBRS 对学习速度的影响是有限的，但 PBRS 奖金项是比标准奖金项更鲁棒地Scaling。<details>
<summary>Abstract</summary>
The main challenge in developing effective reinforcement learning (RL) pipelines is often the design and tuning the reward functions. Well-designed shaping reward can lead to significantly faster learning. Naively formulated rewards, however, can conflict with the desired behavior and result in overfitting or even erratic performance if not properly tuned. In theory, the broad class of potential based reward shaping (PBRS) can help guide the learning process without affecting the optimal policy. Although several studies have explored the use of potential based reward shaping to accelerate learning convergence, most have been limited to grid-worlds and low-dimensional systems, and RL in robotics has predominantly relied on standard forms of reward shaping. In this paper, we benchmark standard forms of shaping with PBRS for a humanoid robot. We find that in this high-dimensional system, PBRS has only marginal benefits in convergence speed. However, the PBRS reward terms are significantly more robust to scaling than typical reward shaping approaches, and thus easier to tune.
</details>
<details>
<summary>摘要</summary>
主要挑战在开发有效的回归学习（RL）管道是设计和调整奖金函数。有效的奖金函数设计可以导致更快的学习。然而，粗制的奖金函数可能会与期望的行为冲突，从而导致过拟合或者异常的表现，如果不当调整。在理论上，广泛的可能基于奖金函数形成（PBRS）可以帮助学习过程进行导航，不会影响最佳策略。虽然一些研究已经探讨了使用PBRS加速学习的潜在优势，但大多数研究仅限于网格世界和低维系统，RL在机器人领域主要依靠标准的奖金函数形成。在这篇论文中，我们将标准的奖金函数形成与PBRS进行比较，对一个人类机器人进行了测试。我们发现在这个高维系统中，PBRS只有微妙的加速学习的速度。然而，PBRS的奖金函数是标准奖金函数形成方法更加鲁棒对缩放的，因此更容易调整。
</details></li>
</ul>
<hr>
<h2 id="ProtiGeno-a-prokaryotic-short-gene-finder-using-protein-language-models"><a href="#ProtiGeno-a-prokaryotic-short-gene-finder-using-protein-language-models" class="headerlink" title="ProtiGeno: a prokaryotic short gene finder using protein language models"></a>ProtiGeno: a prokaryotic short gene finder using protein language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10343">http://arxiv.org/abs/2307.10343</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tonytu16/protigeno">https://github.com/tonytu16/protigeno</a></li>
<li>paper_authors: Tony Tu, Gautham Krishna, Amirali Aghazadeh</li>
<li>for: 本研究的目的是提高脱氧核糖核酸生物体中短读取识别率。</li>
<li>methods: 该研究使用深度学习方法， Specifically targeting short prokaryotic genes using a protein language model trained on millions of evolved proteins。</li>
<li>results: 在4,288个脱氧核糖核酸 genomes 中进行系统性大规模实验，显示 ProtiGeno 能够更高精度和回归率地预测短编译和非编译基因。<details>
<summary>Abstract</summary>
Prokaryotic gene prediction plays an important role in understanding the biology of organisms and their function with applications in medicine and biotechnology. Although the current gene finders are highly sensitive in finding long genes, their sensitivity decreases noticeably in finding shorter genes (<180 nts). The culprit is insufficient annotated gene data to identify distinguishing features in short open reading frames (ORFs). We develop a deep learning-based method called ProtiGeno, specifically targeting short prokaryotic genes using a protein language model trained on millions of evolved proteins. In systematic large-scale experiments on 4,288 prokaryotic genomes, we demonstrate that ProtiGeno predicts short coding and noncoding genes with higher accuracy and recall than the current state-of-the-art gene finders. We discuss the predictive features of ProtiGeno and possible limitations by visualizing the three-dimensional structure of the predicted short genes. Data, codes, and models are available at https://github.com/tonytu16/protigeno.
</details>
<details>
<summary>摘要</summary>
probiotic gene prediction 在理解生物体的生物学和功能方面发挥重要作用，有医学和生物技术应用。 although current gene finders 非常敏感于发现长ogene，但是它们对短ogene（<180 nts）的敏感度显著下降。 问题在于缺乏可靠的annotated gene数据，以便识别短 open reading frames (ORFs) 中的特征。 we develop a deep learning-based method called ProtiGeno, 专门针对短抗原生物体gene。 through systematic large-scale experiments on 4,288 prokaryotic genomes, we demonstrate that ProtiGeno can predict short coding and noncoding genes with higher accuracy and recall than the current state-of-the-art gene finders. we discuss the predictive features of ProtiGeno and possible limitations by visualizing the three-dimensional structure of the predicted short genes. data, codes, and models are available at https://github.com/tonytu16/protigeno.
</details></li>
</ul>
<hr>
<h2 id="Gradient-Sparsification-For-Masked-Fine-Tuning-of-Transformers"><a href="#Gradient-Sparsification-For-Masked-Fine-Tuning-of-Transformers" class="headerlink" title="Gradient Sparsification For Masked Fine-Tuning of Transformers"></a>Gradient Sparsification For Masked Fine-Tuning of Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10098">http://arxiv.org/abs/2307.10098</a></li>
<li>repo_url: None</li>
<li>paper_authors: James O’ Neill, Sourav Dutta</li>
<li>for: 这 paper 是研究如何使用梯度抑制来改进预训练语言模型的过程。</li>
<li>methods: 这 paper 使用了 GradDrop 和其变种来强制梯度抑制，用于让预训练语言模型进行更好的微调。</li>
<li>results:  experiments 表明，GradDrop 可以与使用额外翻译数据进行中间预训练相比，并且超过标准微调和慢滑块释放。另外，一个后续分析表明，GradDrop 可以在未经训练的语言上提高性能。<details>
<summary>Abstract</summary>
Fine-tuning pretrained self-supervised language models is widely adopted for transfer learning to downstream tasks. Fine-tuning can be achieved by freezing gradients of the pretrained network and only updating gradients of a newly added classification layer, or by performing gradient updates on all parameters. Gradual unfreezing makes a trade-off between the two by gradually unfreezing gradients of whole layers during training. This has been an effective strategy to trade-off between storage and training speed with generalization performance. However, it is not clear whether gradually unfreezing layers throughout training is optimal, compared to sparse variants of gradual unfreezing which may improve fine-tuning performance. In this paper, we propose to stochastically mask gradients to regularize pretrained language models for improving overall fine-tuned performance. We introduce GradDrop and variants thereof, a class of gradient sparsification methods that mask gradients during the backward pass, acting as gradient noise. GradDrop is sparse and stochastic unlike gradual freezing. Extensive experiments on the multilingual XGLUE benchmark with XLMR-Large show that GradDrop is competitive against methods that use additional translated data for intermediate pretraining and outperforms standard fine-tuning and gradual unfreezing. A post-analysis shows how GradDrop improves performance with languages it was not trained on, such as under-resourced languages.
</details>
<details>
<summary>摘要</summary>
通用自学习语言模型的精度调整广泛采用转移学习到下游任务。精度调整可以通过冻结预训练网络的梯度并只更新新增的分类层的梯度，或者通过所有参数的梯度更新来实现。渐进冻结实现了一种在训练中逐渐解冻整个层的梯度，这是一种训练速度和存储空间之间的融合策略。然而，是否可以在训练中逐渐解冻整个层的梯度是最佳的，而不是使用随机的梯度抑制方法来改进精度调整表现。在这篇论文中，我们提出了在反向传播中随机抑制梯度的方法，即GradDrop和其变种。GradDrop是一种随机梯度抑制方法，在反向传播中随机地抑制梯度，与渐进冻结不同。我们在多语言XGLUE标准测试集上进行了广泛的实验，结果表明GradDrop和其变种与使用额外翻译数据进行中间预训练的方法相当竞争，并且超过标准的精度调整和渐进冻结。另外，我们还进行了后期分析，发现GradDrop在不同语言上的表现提高，包括资源不足的语言。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-invariances-and-introducing-priors-in-Gromov-Wasserstein-distances"><a href="#Revisiting-invariances-and-introducing-priors-in-Gromov-Wasserstein-distances" class="headerlink" title="Revisiting invariances and introducing priors in Gromov-Wasserstein distances"></a>Revisiting invariances and introducing priors in Gromov-Wasserstein distances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10093">http://arxiv.org/abs/2307.10093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pinar Demetci, Quang Huy Tran, Ievgen Redko, Ritambhara Singh</li>
<li>For: 本研究旨在提出一种新的优化的格罗莫夫-瓦asserstein距离，以提高机器学习 task 的性能。* Methods: 本研究使用了一种新的augmented Gromov-Wasserstein distance，该距离可以在某种程度上控制 transformations 的灵活性，并且可以更好地利用输入数据的特征对Alignment。* Results: 研究表明，augmented Gromov-Wasserstein distance 可以在单元细胞多Omic alignment和机器学习 transfer learning 等场景中提高性能。<details>
<summary>Abstract</summary>
Gromov-Wasserstein distance has found many applications in machine learning due to its ability to compare measures across metric spaces and its invariance to isometric transformations. However, in certain applications, this invariance property can be too flexible, thus undesirable. Moreover, the Gromov-Wasserstein distance solely considers pairwise sample similarities in input datasets, disregarding the raw feature representations. We propose a new optimal transport-based distance, called Augmented Gromov-Wasserstein, that allows for some control over the level of rigidity to transformations. It also incorporates feature alignments, enabling us to better leverage prior knowledge on the input data for improved performance. We present theoretical insights into the proposed metric. We then demonstrate its usefulness for single-cell multi-omic alignment tasks and a transfer learning scenario in machine learning.
</details>
<details>
<summary>摘要</summary>
格罗莫夫-瓦萨希恩距离在机器学习中发现了许多应用，因为它可以比较度量空间中的度量并具有对称变换不变性。然而，在某些应用中，这种不变性属性可能太 flexibility，因此不 desired。此外，格罗莫夫-瓦萨希恩距离只考虑输入数据集中的对比样本相似性，忽略原始特征表示。我们提出一种新的最优运输基于距离，called Augmented Gromov-Wasserstein,可以控制变换的水平弹性。它还包含特征对应， allowing us to better leverage prior knowledge on the input data for improved performance。我们提供了关于提posed metric的理论探讨。然后，我们用单元细胞多OmicAlignment任务和机器学习的传输学习场景来示例出其用于。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/20/cs.LG_2023_07_20/" data-id="clluro5jn004tq988735x57ae" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/20/cs.SD_2023_07_20/" class="article-date">
  <time datetime="2023-07-19T16:00:00.000Z" itemprop="datePublished">2023-07-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/20/cs.SD_2023_07_20/">cs.SD - 2023-07-20 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Transfer-Learning-and-Bias-Correction-with-Pre-trained-Audio-Embeddings"><a href="#Transfer-Learning-and-Bias-Correction-with-Pre-trained-Audio-Embeddings" class="headerlink" title="Transfer Learning and Bias Correction with Pre-trained Audio Embeddings"></a>Transfer Learning and Bias Correction with Pre-trained Audio Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10834">http://arxiv.org/abs/2307.10834</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/changhongw/audio-embedding-bias">https://github.com/changhongw/audio-embedding-bias</a></li>
<li>paper_authors: Changhong Wang, Gaël Richard, Brian McFee</li>
<li>for: 这篇论文的目的是研究预训练的音频表示中的偏见现象，以及如何通过后处理来减轻这些偏见的影响。</li>
<li>methods: 这篇论文使用了三种不同的预训练表示（VGGish、OpenL3和YAMNet），并对这些表示进行了比较，以评估它们在不同数据集上的性能。它们还研究了数据集Identify和种类分布是否会影响表示的偏见。</li>
<li>results: 研究发现，不同的预训练表示在不同数据集上的性能有很大差异，而且这些差异与数据集的Identify和种类分布有关。此外，研究还提出了一些后处理方法，以减轻预训练表示中的偏见影响，并提高数据集之间的泛化性能。<details>
<summary>Abstract</summary>
Deep neural network models have become the dominant approach to a large variety of tasks within music information retrieval (MIR). These models generally require large amounts of (annotated) training data to achieve high accuracy. Because not all applications in MIR have sufficient quantities of training data, it is becoming increasingly common to transfer models across domains. This approach allows representations derived for one task to be applied to another, and can result in high accuracy with less stringent training data requirements for the downstream task. However, the properties of pre-trained audio embeddings are not fully understood. Specifically, and unlike traditionally engineered features, the representations extracted from pre-trained deep networks may embed and propagate biases from the model's training regime. This work investigates the phenomenon of bias propagation in the context of pre-trained audio representations for the task of instrument recognition. We first demonstrate that three different pre-trained representations (VGGish, OpenL3, and YAMNet) exhibit comparable performance when constrained to a single dataset, but differ in their ability to generalize across datasets (OpenMIC and IRMAS). We then investigate dataset identity and genre distribution as potential sources of bias. Finally, we propose and evaluate post-processing countermeasures to mitigate the effects of bias, and improve generalization across datasets.
</details>
<details>
<summary>摘要</summary>
We first demonstrate that three different pre-trained representations (VGGish, OpenL3, and YAMNet) exhibit comparable performance when constrained to a single dataset, but differ in their ability to generalize across datasets (OpenMIC and IRMAS). We then investigate dataset identity and genre distribution as potential sources of bias. Finally, we propose and evaluate post-processing countermeasures to mitigate the effects of bias, and improve generalization across datasets.Here is the translation in Simplified Chinese:深度神经网络模型已成为Music信息检索（MIR）中主要的方法。这些模型通常需要大量（注解）训练数据以达到高精度。然而，不是所有MIR应用程序具有足够的训练数据，因此正在越来越常将模型转移到另一个领域。这种方法允许 representations derived for one task 被应用到另一个任务上，并可以通过使用 less stringent 的训练数据要求来实现高精度。然而，预训练音频表示的特性没有完全理解。 Specifically, the representations extracted from pre-trained deep networks may embed and propagate biases from the model's training regime. This work investigates the phenomenon of bias propagation in the context of pre-trained audio representations for the task of instrument recognition.我们首先示出了三个不同的预训练表示（VGGish, OpenL3, and YAMNet）在单个数据集上受限制时的相似性，但是它们在不同的数据集（OpenMIC和IRMAS）上的泛化能力不同。然后，我们调查了数据集标识和类型分布作为可能的偏见源。最后，我们提出并评估了后处理措施来 Mitigate the effects of bias，并提高数据集之间的泛化能力。
</details></li>
</ul>
<hr>
<h2 id="Cross-Corpus-Multilingual-Speech-Emotion-Recognition-Amharic-vs-Other-Languages"><a href="#Cross-Corpus-Multilingual-Speech-Emotion-Recognition-Amharic-vs-Other-Languages" class="headerlink" title="Cross-Corpus Multilingual Speech Emotion Recognition: Amharic vs. Other Languages"></a>Cross-Corpus Multilingual Speech Emotion Recognition: Amharic vs. Other Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10814">http://arxiv.org/abs/2307.10814</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ephrem Afele Retta, Richard Sutcliffe, Jabar Mahmood, Michael Abebe Berwo, Eiad Almekhlafi, Sajjad Ahmed Khan, Shehzad Ashraf Chaudhry, Mustafa Mhamed, Jun Feng</li>
<li>for: 这个论文的目的是研究跨语言和多语言情绪识别（SER）任务，以及如何使用不同语言的数据进行训练。</li>
<li>methods: 这个论文使用了AlexNet、VGGE（一种修改后的VGG模型）和ResNet50三种模型进行实验，并将所有数据集映射到只有两个类别（正面和负面）上。</li>
<li>results: 结果表明，使用英语或德语作为源语言，并使用阿姆哈里语作为目标语言可以获得最佳结果（在Experiment 2中）。此外，使用多种非阿姆哈里语言进行训练，然后测试在阿姆哈里语上的结果比使用单一非阿姆哈里语言进行训练更好（在Experiment 3中）。总的来说，结果表明，跨语言和多语言训练是缺乏语言资源时的有效策略。<details>
<summary>Abstract</summary>
In a conventional Speech emotion recognition (SER) task, a classifier for a given language is trained on a pre-existing dataset for that same language. However, where training data for a language does not exist, data from other languages can be used instead. We experiment with cross-lingual and multilingual SER, working with Amharic, English, German and URDU. For Amharic, we use our own publicly-available Amharic Speech Emotion Dataset (ASED). For English, German and Urdu we use the existing RAVDESS, EMO-DB and URDU datasets. We followed previous research in mapping labels for all datasets to just two classes, positive and negative. Thus we can compare performance on different languages directly, and combine languages for training and testing. In Experiment 1, monolingual SER trials were carried out using three classifiers, AlexNet, VGGE (a proposed variant of VGG), and ResNet50. Results averaged for the three models were very similar for ASED and RAVDESS, suggesting that Amharic and English SER are equally difficult. Similarly, German SER is more difficult, and Urdu SER is easier. In Experiment 2, we trained on one language and tested on another, in both directions for each pair: Amharic<->German, Amharic<->English, and Amharic<->Urdu. Results with Amharic as target suggested that using English or German as source will give the best result. In Experiment 3, we trained on several non-Amharic languages and then tested on Amharic. The best accuracy obtained was several percent greater than the best accuracy in Experiment 2, suggesting that a better result can be obtained when using two or three non-Amharic languages for training than when using just one non-Amharic language. Overall, the results suggest that cross-lingual and multilingual training can be an effective strategy for training a SER classifier when resources for a language are scarce.
</details>
<details>
<summary>摘要</summary>
传统的语音情感识别（SER）任务中，一个分类器会被训练在某种语言的已有数据集上。但是当语言的训练数据不存在时，可以使用其他语言的数据 instead。我们在埃塞俄比亚语、英语、德语和乌尔都语之间进行了交叉语言和多语言 SER 实验。对于埃塞俄比亚语，我们使用了我们自己公开的埃塞俄比亚语 Speech Emotion Dataset (ASED)。对于英语、德语和乌尔都语，我们使用了现有的 RAVDESS、EMO-DB 和 URDU 数据集。我们按照之前的研究方法将所有数据集的标签映射到两个类别：正面和负面。因此，我们可以直接比较不同语言的性能，并将语言组合在训练和测试中。在实验1中，我们使用了三个模型：AlexNet、VGGE 和 ResNet50，进行了各自的MONOLINGUAL SER 实验。结果显示，ASED 和 RAVDESS 的结果几乎相同， suggesting that Amharic and English SER are equally difficult. Similarly, German SER is more difficult, and Urdu SER is easier.在实验2中，我们将一种语言作为目标语言，并将另一种语言作为来源语言，在两个方向上进行了每个对。结果表明，使用英语或德语作为来源语言，对埃塞俄比亚语作为目标语言的结果最佳。在实验3中，我们将多种非埃塞俄比亚语言作为训练语言，然后对埃塞俄比亚语进行测试。最佳的结果比实验2中的最佳结果高出几个百分点，表明使用两或三种非埃塞俄比亚语言进行训练可以获得更好的结果。总之，结果表明，交叉语言和多语言训练是缺乏语言资源时的效果的 SER 分类器训练策略。
</details></li>
</ul>
<hr>
<h2 id="Perceptual-Quality-Assessment-of-Omnidirectional-Audio-visual-Signals"><a href="#Perceptual-Quality-Assessment-of-Omnidirectional-Audio-visual-Signals" class="headerlink" title="Perceptual Quality Assessment of Omnidirectional Audio-visual Signals"></a>Perceptual Quality Assessment of Omnidirectional Audio-visual Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10813">http://arxiv.org/abs/2307.10813</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xilei Zhu, Huiyu Duan, Yuqin Cao, Yuxin Zhu, Yucheng Zhu, Jing Liu, Li Chen, Xiongkuo Min, Guangtao Zhai</li>
<li>for: 这个论文主要针对的是评估全方位视频（ODV）的质量，以提高用户的体验质量（QoE）。</li>
<li>methods: 该论文首先创建了一个大规模的全方位音频视频质量评估数据集，并开发了三种基准方法来评估全方位音频视频质量（OAVQA）。这些方法结合了现有的单模式音频和视频评估模型，通过多模式融合策略进行评估。</li>
<li>results: 该论文在数据集上验证了音频视频融合方法的效iveness，提供了一个新的全方位QoE评估标准。数据集可以在 GitHub 上下载。<details>
<summary>Abstract</summary>
Omnidirectional videos (ODVs) play an increasingly important role in the application fields of medical, education, advertising, tourism, etc. Assessing the quality of ODVs is significant for service-providers to improve the user's Quality of Experience (QoE). However, most existing quality assessment studies for ODVs only focus on the visual distortions of videos, while ignoring that the overall QoE also depends on the accompanying audio signals. In this paper, we first establish a large-scale audio-visual quality assessment dataset for omnidirectional videos, which includes 375 distorted omnidirectional audio-visual (A/V) sequences generated from 15 high-quality pristine omnidirectional A/V contents, and the corresponding perceptual audio-visual quality scores. Then, we design three baseline methods for full-reference omnidirectional audio-visual quality assessment (OAVQA), which combine existing state-of-the-art single-mode audio and video QA models via multimodal fusion strategies. We validate the effectiveness of the A/V multimodal fusion method for OAVQA on our dataset, which provides a new benchmark for omnidirectional QoE evaluation. Our dataset is available at https://github.com/iamazxl/OAVQA.
</details>
<details>
<summary>摘要</summary>
“全向视频（ODV）在医疗、教育、广告、旅游等领域的应用越来越重要。评估ODV的质量是服务提供者提高用户体验质量（QoE）的关键。然而，现有的大多数ODV质量评估研究只关注视频的视觉扭曲，而忽略了音频信号的影响。本文首先建立了一个大规模的音频视频质量评估 dataset，包括375个扭曲的全向音频视频（A/V）序列，来自15个高质量的原始全向A/V内容。然后，我们设计了三种基eline方法 для全向音频视频质量评估（OAVQA），将现有的单模式音频和视频质量评估模型通过多模式融合策略结合。我们验证了这种A/V多模式融合方法在我们的数据集上的效果，提供了一个新的 benchmark для全向QoE评估。我们的数据集可以在 GitHub 上获取。”
</details></li>
</ul>
<hr>
<h2 id="Vesper-A-Compact-and-Effective-Pretrained-Model-for-Speech-Emotion-Recognition"><a href="#Vesper-A-Compact-and-Effective-Pretrained-Model-for-Speech-Emotion-Recognition" class="headerlink" title="Vesper: A Compact and Effective Pretrained Model for Speech Emotion Recognition"></a>Vesper: A Compact and Effective Pretrained Model for Speech Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10757">http://arxiv.org/abs/2307.10757</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/happycolor/vesper">https://github.com/happycolor/vesper</a></li>
<li>paper_authors: Weidong Chen, Xiaofen Xing, Peihao Chen, Xiangmin Xu</li>
<li>for: 本研究旨在适应大规模预训练模型（PTM）到语音情感识别任务。PTMs在人工智能领域亮起新的光，但它们是为普通任务设计的，因此在特定任务中可以进一步提高效果。同时，在实际应用中使用PTMs可能会受到其较大的大小的限制。</li>
<li>methods: 本研究提出了一种优化大规模PTMs для特定任务的研究方向，并在语音情感识别任务上提出了一种改进的情感特定预训练 encoder called Vesper。Vesper在基于WavLM的语音Dataset上进行预训练，并考虑情感特征。为了提高情感信息的敏感度，Vesper使用情感导向的masking策略来标识需要遮盖的区域。然后，Vesper使用层次和交叉层自我超级视图来提高其捕捉语音和语义表示的能力，这两者都是情感识别的关键。</li>
<li>results: 对于IEMOCAP、MELD和CREMA-D datasets的实验结果显示，Vesper WITH 4层比WavLM Base WITH 12层有更好的性能，而Vesper WITH 12层比WavLM Large WITH 24层有更好的性能。<details>
<summary>Abstract</summary>
This paper presents a paradigm that adapts general large-scale pretrained models (PTMs) to speech emotion recognition task. Although PTMs shed new light on artificial general intelligence, they are constructed with general tasks in mind, and thus, their efficacy for specific tasks can be further improved. Additionally, employing PTMs in practical applications can be challenging due to their considerable size. Above limitations spawn another research direction, namely, optimizing large-scale PTMs for specific tasks to generate task-specific PTMs that are both compact and effective. In this paper, we focus on the speech emotion recognition task and propose an improved emotion-specific pretrained encoder called Vesper. Vesper is pretrained on a speech dataset based on WavLM and takes into account emotional characteristics. To enhance sensitivity to emotional information, Vesper employs an emotion-guided masking strategy to identify the regions that need masking. Subsequently, Vesper employs hierarchical and cross-layer self-supervision to improve its ability to capture acoustic and semantic representations, both of which are crucial for emotion recognition. Experimental results on the IEMOCAP, MELD, and CREMA-D datasets demonstrate that Vesper with 4 layers outperforms WavLM Base with 12 layers, and the performance of Vesper with 12 layers surpasses that of WavLM Large with 24 layers.
</details>
<details>
<summary>摘要</summary>
The proposed approach is called Vesper, which is pre-trained on a speech dataset based on WavLM and takes into account emotional characteristics. To enhance sensitivity to emotional information, Vesper uses an emotion-guided masking strategy to identify the regions that need masking. The model also employs hierarchical and cross-layer self-supervision to improve its ability to capture acoustic and semantic representations, which are crucial for emotion recognition.Experimental results on the IEMOCAP, MELD, and CREMA-D datasets show that Vesper with 4 layers outperforms WavLM Base with 12 layers, and the performance of Vesper with 12 layers surpasses that of WavLM Large with 24 layers. This demonstrates the effectiveness of the proposed approach and the potential of Vesper as a task-specific PTM for speech emotion recognition.
</details></li>
</ul>
<hr>
<h2 id="PAS-Partial-Additive-Speech-Data-Augmentation-Method-for-Noise-Robust-Speaker-Verification"><a href="#PAS-Partial-Additive-Speech-Data-Augmentation-Method-for-Noise-Robust-Speaker-Verification" class="headerlink" title="PAS: Partial Additive Speech Data Augmentation Method for Noise Robust Speaker Verification"></a>PAS: Partial Additive Speech Data Augmentation Method for Noise Robust Speaker Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10628">http://arxiv.org/abs/2307.10628</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rst0070/Partial_Additive_Speech">https://github.com/rst0070/Partial_Additive_Speech</a></li>
<li>paper_authors: Wonbin Kim, Hyun-seo Shin, Ju-ho Kim, Jungwoo Heo, Chan-yeong Lim, Ha-Jin Yu</li>
<li>for: 提高SV系统在噪音环境中的准确率和质量</li>
<li>methods: 使用新的加法噪音方法（partial additive speech，PAS）来训练SV系统，以降低噪音环境对SV系统的影响</li>
<li>results: PAS方法比传统的加法噪音方法（EER）提高4.64%和5.01%，并通过注意模块和 speaker embedding 分析表明方法的效果。<details>
<summary>Abstract</summary>
Background noise reduces speech intelligibility and quality, making speaker verification (SV) in noisy environments a challenging task. To improve the noise robustness of SV systems, additive noise data augmentation method has been commonly used. In this paper, we propose a new additive noise method, partial additive speech (PAS), which aims to train SV systems to be less affected by noisy environments. The experimental results demonstrate that PAS outperforms traditional additive noise in terms of equal error rates (EER), with relative improvements of 4.64% and 5.01% observed in SE-ResNet34 and ECAPA-TDNN. We also show the effectiveness of proposed method by analyzing attention modules and visualizing speaker embeddings.
</details>
<details>
<summary>摘要</summary>
背景噪声会降低语音清晰度和质量，使 speaker verification（SV）在噪声环境中成为一项具有挑战性的任务。为了改善噪声Robustness Of SV系统，通常使用添加噪声数据增强方法。在这篇论文中，我们提出了一种新的添加噪声方法，即 partial additive speech（PAS），以培养SV系统对噪声环境更加敏感。实验结果表明，PAS在EER方面表现出比传统添加噪声更好的result，对SE-ResNet34和ECAPA-TDNN的相对改善为4.64%和5.01%。我们还通过分析注意力模块和Visualize speaker embeddings来证明提出的方法的效果。
</details></li>
</ul>
<hr>
<h2 id="Transsion-TSUP’s-speech-recognition-system-for-ASRU-2023-MADASR-Challenge"><a href="#Transsion-TSUP’s-speech-recognition-system-for-ASRU-2023-MADASR-Challenge" class="headerlink" title="Transsion TSUP’s speech recognition system for ASRU 2023 MADASR Challenge"></a>Transsion TSUP’s speech recognition system for ASRU 2023 MADASR Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11778">http://arxiv.org/abs/2307.11778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoxiao Li, Gaosheng Zhang, An Zhu, Weiyong Li, Shuming Fang, Xiaoyue Yang, Jianchao Zhu</li>
<li>for: 这篇论文旨在开发一个适应低资源印度语言的语音识别系统，并在ASRU 2023 MADASR Challenge 中提交了四个track的解决方案。</li>
<li>methods: 该系统使用了ASR模型，并在track 1和2中使用了压缩形态器encoder和双向转换器decoder，同时使用了共同CTC-Attention训练损失。此外，还使用了外部KenLM语言模型进行TLG beam search解码。在track 3和4中，使用了预训练的IndicWhisper模型，并在挑战数据集和公共可用数据集上进行了finetuning。另外， modifying the whisper beam search decoding方法以支持外部KenLM语言模型，使得更好地利用挑战中提供的额外文本。</li>
<li>results: 该方法在 Bengali语言的四个track中获得了24.17%、24.43%、15.97%和15.97%的单词错误率（WER），而在 Bhojpuri语言的四个track中获得了19.61%、19.54%、15.48%和15.48%的WER。这些结果表明该方法的效果。<details>
<summary>Abstract</summary>
This paper presents a speech recognition system developed by the Transsion Speech Understanding Processing Team (TSUP) for the ASRU 2023 MADASR Challenge. The system focuses on adapting ASR models for low-resource Indian languages and covers all four tracks of the challenge. For tracks 1 and 2, the acoustic model utilized a squeezeformer encoder and bidirectional transformer decoder with joint CTC-Attention training loss. Additionally, an external KenLM language model was used during TLG beam search decoding. For tracks 3 and 4, pretrained IndicWhisper models were employed and finetuned on both the challenge dataset and publicly available datasets. The whisper beam search decoding was also modified to support an external KenLM language model, which enabled better utilization of the additional text provided by the challenge. The proposed method achieved word error rates (WER) of 24.17%, 24.43%, 15.97%, and 15.97% for Bengali language in the four tracks, and WER of 19.61%, 19.54%, 15.48%, and 15.48% for Bhojpuri language in the four tracks. These results demonstrate the effectiveness of the proposed method.
</details>
<details>
<summary>摘要</summary>
For tracks 1 and 2, the acoustic model utilized a squeezeformer encoder and bidirectional transformer decoder with joint CTC-Attention training loss. Additionally, an external KenLM language model was used during TLG beam search decoding.For tracks 3 and 4, pretrained IndicWhisper models were employed and finetuned on both the challenge dataset and publicly available datasets. The whisper beam search decoding was also modified to support an external KenLM language model, which enabled better utilization of the additional text provided by the challenge.The proposed method achieved word error rates (WER) of 24.17%, 24.43%, 15.97%, and 15.97% for Bengali language in the four tracks, and WER of 19.61%, 19.54%, 15.48%, and 15.48% for Bhojpuri language in the four tracks. These results demonstrate the effectiveness of the proposed method.Translation in Simplified Chinese:这篇论文介绍了由Transsion Speech Understanding Processing Team (TSUP)开发的一种基于ASR模型的报告识别系统，用于ASRU 2023 MADASR Challenge。该系统专注于适应低资源印度语言的ASR模型，并覆盖了挑战的四个轨道。 для轨道1和2，语音模型使用了压缩转换器encoder和irectional transformer decoder，并使用了共同CTC-Attention培根精度训练。此外，还使用了外部KenLM语言模型进行TLG搜索 décoding。 для轨道3和4，采用了预训练的IndicWhisper模型，并在挑战数据集和公共可用数据集上进行了Finite tuning。幽amma beam search décoding也被修改，以支持外部KenLM语言模型，从而更好地利用挑战中提供的额外文本。提议的方法实现了Word Error Rate（WER）为24.17%、24.43%、15.97%和15.97%的 Bengali语言，以及WER为19.61%、19.54%、15.48%和15.48%的 Bhojpuri语言。这些结果表明了提议的方法的效果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/20/cs.SD_2023_07_20/" data-id="clluro5km007wq9883cypcj2d" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/20/eess.IV_2023_07_20/" class="article-date">
  <time datetime="2023-07-19T16:00:00.000Z" itemprop="datePublished">2023-07-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/20/eess.IV_2023_07_20/">eess.IV - 2023-07-20 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Parse-and-Recall-Towards-Accurate-Lung-Nodule-Malignancy-Prediction-like-Radiologists"><a href="#Parse-and-Recall-Towards-Accurate-Lung-Nodule-Malignancy-Prediction-like-Radiologists" class="headerlink" title="Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction like Radiologists"></a>Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction like Radiologists</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10824">http://arxiv.org/abs/2307.10824</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianpeng Zhang, Xianghua Ye, Jianfeng Zhang, Yuxing Tang, Minfeng Xu, Jianfei Guo, Xin Chen, Zaiyi Liu, Jingren Zhou, Le Lu, Ling Zhang</li>
<li>for: 这份研究是为了提高肺癌早期检测的精度和准确性。</li>
<li>methods: 这种方法是基于 radiologist-inspired 的，包括 context parsing 和 prototype recalling 两个模组。context parsing 模组首先将 nodule 的 context 结构分解，然后将 contextual information 统计分析，以更好地理解 nodule。prototype recalling 模组使用 prototype-based learning 技术，将先前学习的案例转换为 prototype，并在训练过程中在线更新。</li>
<li>results: 这种方法在两个不同的低剂量和非血痰检查 scenariodemonstrate 了高度的检测性能。<details>
<summary>Abstract</summary>
Lung cancer is a leading cause of death worldwide and early screening is critical for improving survival outcomes. In clinical practice, the contextual structure of nodules and the accumulated experience of radiologists are the two core elements related to the accuracy of identification of benign and malignant nodules. Contextual information provides comprehensive information about nodules such as location, shape, and peripheral vessels, and experienced radiologists can search for clues from previous cases as a reference to enrich the basis of decision-making. In this paper, we propose a radiologist-inspired method to simulate the diagnostic process of radiologists, which is composed of context parsing and prototype recalling modules. The context parsing module first segments the context structure of nodules and then aggregates contextual information for a more comprehensive understanding of the nodule. The prototype recalling module utilizes prototype-based learning to condense previously learned cases as prototypes for comparative analysis, which is updated online in a momentum way during training. Building on the two modules, our method leverages both the intrinsic characteristics of the nodules and the external knowledge accumulated from other nodules to achieve a sound diagnosis. To meet the needs of both low-dose and noncontrast screening, we collect a large-scale dataset of 12,852 and 4,029 nodules from low-dose and noncontrast CTs respectively, each with pathology- or follow-up-confirmed labels. Experiments on several datasets demonstrate that our method achieves advanced screening performance on both low-dose and noncontrast scenarios.
</details>
<details>
<summary>摘要</summary>
肺癌是全球最主要的死亡原因之一，早期检测对生存结果有着关键性。在临床实践中， nodule 的上下文结构和 radiologist 的总体经验是检测 benign 和 malig-nant nodule 的准确性的两个核心元素。上下文信息提供 nodule 的全面信息，如位置、形状和周围血管，经验丰富的 radiologist 可以通过前例来寻找指导，以增强决策的基础。在这篇论文中，我们提出了 radiologist-inspired 方法，它由上下文分析和原型回忆模块组成。上下文分析模块首先将 nodule 的上下文结构分割，然后对 nodule 进行更全面的理解。原型回忆模块利用原型基本学习来压缩已学习的案例，并在训练中进行在线更新。基于这两个模块，我们的方法可以充分利用 nodule 的内在特征和外部知识来实现准确的诊断。为了满足低剂量和非对照检测的需求，我们收集了12,852和4,029个 nodule 的低剂量和非对照 CT 数据，每个数据都有 pathology-或 follow-up-确认的标签。实验表明，我们的方法在低剂量和非对照场景中实现了高级别的检测性能。
</details></li>
</ul>
<hr>
<h2 id="A-novel-integrated-method-of-detection-grasping-for-specific-object-based-on-the-box-coordinate-matching"><a href="#A-novel-integrated-method-of-detection-grasping-for-specific-object-based-on-the-box-coordinate-matching" class="headerlink" title="A novel integrated method of detection-grasping for specific object based on the box coordinate matching"></a>A novel integrated method of detection-grasping for specific object based on the box coordinate matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11783">http://arxiv.org/abs/2307.11783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zongmin Liu, Jirui Wang, Jie Li, Zufeng Li, Kai Ren, Peng Shi</li>
<li>for: 提高服务机器人对老人和残疾人的照顾质量</li>
<li>methods: 提出了一种基于盒坐标匹配的检测-抓取融合方法，包括对SOLOv2实例分 segmentation模型进行修改，并在GR-CNN模型中添加ASPP和CAM模块，以便优化抓取估计。</li>
<li>results: 通过对检测和抓取任务进行分别验证，证明提出的改进模型具有较高的性能。此外，在一个虚拟平台上进行了具体的抓取任务，证明了提出的DG-BCM算法的可行性和有效性。<details>
<summary>Abstract</summary>
To better care for the elderly and disabled, it is essential for service robots to have an effective fusion method of object detection and grasp estimation. However, limited research has been observed on the combination of object detection and grasp estimation. To overcome this technical difficulty, a novel integrated method of detection-grasping for specific object based on the box coordinate matching is proposed in this paper. Firstly, the SOLOv2 instance segmentation model is improved by adding channel attention module (CAM) and spatial attention module (SAM). Then, the atrous spatial pyramid pooling (ASPP) and CAM are added to the generative residual convolutional neural network (GR-CNN) model to optimize grasp estimation. Furthermore, a detection-grasping integrated algorithm based on box coordinate matching (DG-BCM) is proposed to obtain the fusion model of object detection and grasp estimation. For verification, experiments on object detection and grasp estimation are conducted separately to verify the superiority of improved models. Additionally, grasping tasks for several specific objects are implemented on a simulation platform, demonstrating the feasibility and effectiveness of DG-BCM algorithm proposed in this paper.
</details>
<details>
<summary>摘要</summary>
Firstly, the SOLOv2 instance segmentation model is improved by adding channel attention module (CAM) and spatial attention module (SAM). Then, the atrous spatial pyramid pooling (ASPP) and CAM are added to the generative residual convolutional neural network (GR-CNN) model to optimize grasp estimation.Furthermore, a detection-grasping integrated algorithm based on box coordinate matching (DG-BCM) is proposed to obtain the fusion model of object detection and grasp estimation. For verification, experiments on object detection and grasp estimation are conducted separately to verify the superiority of the improved models. Additionally, grasping tasks for several specific objects are implemented on a simulation platform, demonstrating the feasibility and effectiveness of the DG-BCM algorithm proposed in this paper.
</details></li>
</ul>
<hr>
<h2 id="Aggressive-saliency-aware-point-cloud-compression"><a href="#Aggressive-saliency-aware-point-cloud-compression" class="headerlink" title="Aggressive saliency-aware point cloud compression"></a>Aggressive saliency-aware point cloud compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10741">http://arxiv.org/abs/2307.10741</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eleftheria Psatha, Dimitrios Laskos, Gerasimos Arvanitis, Konstantinos Moustakas</li>
<li>for: 提供一种 geometry-based 的端到端压缩方案，用于压缩点云数据，以满足具有准确表示三维场景的需求。</li>
<li>methods: 方法包括分离可见和不可见点云，计算基于点云的几何特征和用户位置的四个吸引地图，并使用这些地图来确定每个点的重要性水平，从而在编码过程中使用不同数量的比特来表示不同区域。</li>
<li>results: 比较研究表明，提出的方法在小比特率下可以获得显著更好的结果，比如MPEG的 geometry-based point cloud compression（G-PCC）算法。<details>
<summary>Abstract</summary>
The increasing demand for accurate representations of 3D scenes, combined with immersive technologies has led point clouds to extensive popularity. However, quality point clouds require a large amount of data and therefore the need for compression methods is imperative. In this paper, we present a novel, geometry-based, end-to-end compression scheme, that combines information on the geometrical features of the point cloud and the user's position, achieving remarkable results for aggressive compression schemes demanding very small bit rates. After separating visible and non-visible points, four saliency maps are calculated, utilizing the point cloud's geometry and distance from the user, the visibility information, and the user's focus point. A combination of these maps results in a final saliency map, indicating the overall significance of each point and therefore quantizing different regions with a different number of bits during the encoding process. The decoder reconstructs the point cloud making use of delta coordinates and solving a sparse linear system. Evaluation studies and comparisons with the geometry-based point cloud compression (G-PCC) algorithm by the Moving Picture Experts Group (MPEG), carried out for a variety of point clouds, demonstrate that the proposed method achieves significantly better results for small bit rates.
</details>
<details>
<summary>摘要</summary>
随着三维场景的准确表示需求的增加，加上 immerse 技术的普及，点云在现场得到了广泛的应用。然而，高质量点云需要大量数据，因此压缩方法的需求是急需的。在这篇论文中，我们提出了一种新的、基于几何特征的、端到端压缩方案，该方案结合点云的几何特征和用户位置信息，实现了非常出色的压缩效果，尤其是在具有非常小的比特率的情况下。我们首先将可见和不可见点分开，然后计算四个重要度地图，利用点云的几何特征、用户位置信息和用户注意点信息。这些地图的组合得到了最终的重要度地图，该地图指示每个点的重要程度，因此在编码过程中对不同区域进行不同数量的比特数量化。解码器使用delta坐标和解决一个稀疏线性系统来重建点云。我们对一系列点云进行了评估和与基于几何特征的点云压缩算法（G-PCC）由国际电影专家组织（MPEG）提出的方法进行了比较，结果显示，我们的方法在小比特率情况下得到了显著更好的效果。
</details></li>
</ul>
<hr>
<h2 id="Prediction-of-sunflower-leaf-area-at-vegetative-stage-by-image-analysis-and-application-to-the-estimation-of-water-stress-response-parameters-in-post-registration-varieties"><a href="#Prediction-of-sunflower-leaf-area-at-vegetative-stage-by-image-analysis-and-application-to-the-estimation-of-water-stress-response-parameters-in-post-registration-varieties" class="headerlink" title="Prediction of sunflower leaf area at vegetative stage by image analysis and application to the estimation of water stress response parameters in post-registration varieties"></a>Prediction of sunflower leaf area at vegetative stage by image analysis and application to the estimation of water stress response parameters in post-registration varieties</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11110">http://arxiv.org/abs/2307.11110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Casadebaig, Nicolas Blanchet, Nicolas Bernard Langlade<br>for:本研究旨在提供一种自动测量阳光花的发育和生理响应，以便为农民选择最适合的品种，以及为了了解植物对环境的响应的生物、遗传和分子基础。methods:在INRAE Toulouse的Heliaphen高通量现场测量平台上，我们设计了两个实验，每个实验有8种植物（2*96植物），并通过光栅获取了不同水压强度下植物的图像，并在每次评估期间手动测量植物的叶面积。results:通过分析图像，我们提取了植物的形态特征，并评估了不同模型以估算植物总叶面积。使用了一种线性模型和后续平滑处理，我们可以Relative squared error of 11%和效率93%来估算植物总叶面积。这些估算值与手动测量值相比较，显示了图像基于叶面积估算的方法的可靠性。此外，我们还计算了植物的叶展和营养响应（LE和TR），并与手动测量值进行比较。结果显示，图像基于叶面积估算的LE和TR参数估算值与手动测量值呈0.61和0.81的相关系数，表明这些参数可以用于模拟。<details>
<summary>Abstract</summary>
The automatic measurement of developmental and physiological responses of sunflowers to water stress represents an applied challenge for a better knowledge of the varieties available to growers, but also a fundamental one for identifying the biological, genetic and molecular bases of plant response to their environment.On INRAE Toulouse's Heliaphen high-throughput phenotyping platform, we set up two experiments, each with 8 varieties (2*96 plants), and acquired images of plants subjected or not to water stress, using a light barrier on a daily basis. At the same time, we manually measured the leaf surfaces of these plants every other day for the duration of the stress, which lasted around ten days. The images were analyzed to extract morphological characteristics of the segmented plants and different models were evaluated to estimate total plant leaf areas using these data.A linear model with a posteriori smoothing was used to estimate total leaf area with a relative squared error of 11% and an efficiency of 93%. Leaf areas estimated conventionally or with the developed model were used to calculate the leaf expansion and transpiration responses (LER and TR) used in the SUNFLO crop model for 8 sunflower varieties studied. Correlation coefficients of 0.61 and 0.81 for LER and TR respectively validate the use of image-based leaf area estimation. However, the estimated values for LER are lower than for the manual method on Heliaphen, but closer overall to the manual method on greenhouse-grown plants, potentially suggesting an overestimation of stress sensitivity.It can be concluded that the LE and TR parameter estimates can be used for simulations. The low cost of this method (compared with manual measurements), the possibility of parallelizing and repeating measurements on the Heliaphen platform, and of benefiting from the Heliaphen platform's data management, are major improvements for valorizing the SUNFLO model and characterizing the drought sensitivity of cultivated varieties.
</details>
<details>
<summary>摘要</summary>
自动测量阳光花的发展和生理响应对水压力是一种应用和基础研究，可以帮助选择有效的种植者，同时也可以更好地了解植物对环境的响应。在国家农业研究所图卢兹分所的Heliaphen高通量fenotyping平台上，我们设计了两个实验，每个实验有8种(2*96植物)，并在日常基础上获取了水压力和不受水压力影响的植物图像，使用了光栅。同时，我们手动测量了这些植物的叶表面每两天，持续约10天。图像分析后，我们使用了不同的模型来估算植物总叶面积。我们使用了一种线性模型，并在后续的滑动平均处理中估算了总叶面积，得到的Relative squared error为11%，效率为93%。这些估算的叶面积值被用来计算植物叶展和蒸发响应（LER和TR），并被用于SUNFLO作物模型中。 corr coefficient为0.61和0.81， validate了使用图像基于叶面积估算。 although the estimated LER values were lower than those obtained by the manual method on Heliaphen, they were closer to the manual method on greenhouse-grown plants, suggesting that the image-based method may overestimate stress sensitivity. Therefore, the LE and TR parameter estimates can be used for simulations, and the low cost of this method, the possibility of parallelizing and repeating measurements on the Heliaphen platform, and the benefits of using the Heliaphen platform's data management, are major improvements for valorizing the SUNFLO model and characterizing the drought sensitivity of cultivated varieties.
</details></li>
</ul>
<hr>
<h2 id="Depth-from-Defocus-Technique-A-Simple-Calibration-Free-Approach-for-Dispersion-Size-Measurement"><a href="#Depth-from-Defocus-Technique-A-Simple-Calibration-Free-Approach-for-Dispersion-Size-Measurement" class="headerlink" title="Depth from Defocus Technique: A Simple Calibration-Free Approach for Dispersion Size Measurement"></a>Depth from Defocus Technique: A Simple Calibration-Free Approach for Dispersion Size Measurement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10678">http://arxiv.org/abs/2307.10678</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saini Jatin Rao, Shubham Sharma, Saptarshi Basu, Cameron Tropea</li>
<li>for: 这个论文主要用于探讨了一种基于图像技术的粒子大小和位势测量方法，用于应用于多种场景，如涂抹液滴粒子大小、多相流中粒子物质的跟踪以及机器视觉系统中的目标标记检测。</li>
<li>methods: 该方法基于’深度 FROM 杂谱’（DFD）技术，使用单 Camera 设置，并且有一个简单的光学配置和简单的准备过程，使得这种方法可以在更广泛的应用中使用。</li>
<li>results: 该方法可以准确地测量粒子的大小和位势，并且可以在不同的环境下进行测量。<details>
<summary>Abstract</summary>
Dispersed particle size measurement is crucial in a variety of applications, be it in the sizing of spray droplets, tracking of particulate matter in multiphase flows, or the detection of target markers in machine vision systems. Further to sizing, such systems are characterised by extracting quantitative information like spatial position and associated velocity of the dispersed phase particles. In the present study we propose an imaging based volumetric measurement approach for estimating the size and position of spherically dispersed particles. The approach builds on the 'Depth from Defocus' (DFD) technique using a single camera approach. The simple optical configuration, consisting of a shadowgraph setup and a straightforward calibration procedure, makes this method readily deployable and accessible for broader applications.
</details>
<details>
<summary>摘要</summary>
粒子大小分布测量在多种应用中非常重要，包括涂抹液滴大小测量、多相流体中固体粒子的跟踪以及机器视觉系统中目标标记的检测。此外，这些系统还可以提取量化信息，如粒子颗粒的空间位置和相关速度。在 presente 研究中，我们提出了一种基于图像测量的粒子大小和位置估算方法。该方法基于 '深度从杂化'（DFD）技术，使用单个摄像头设计。该简单的光学配置和简单的准备过程，使得这种方法可以广泛应用和易于部署。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Unified-Demosaicing-for-Bayer-and-Non-Bayer-Patterned-Image-Sensors"><a href="#Efficient-Unified-Demosaicing-for-Bayer-and-Non-Bayer-Patterned-Image-Sensors" class="headerlink" title="Efficient Unified Demosaicing for Bayer and Non-Bayer Patterned Image Sensors"></a>Efficient Unified Demosaicing for Bayer and Non-Bayer Patterned Image Sensors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10667">http://arxiv.org/abs/2307.10667</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haechang Lee, Dongwon Park, Wongi Jeong, Kijeong Kim, Hyunwoo Je, Dongil Ryu, Se Young Chun</li>
<li>for: 本研究旨在提出一种能够应对不同光照条件下的非拜尔摄像头感知器（CMOS）的高效掺杂重建方法，以提高摄像头的性能和灵活性。</li>
<li>methods: 该方法基于知识学习，使用了CFA-adaptive筛选器，只需要1%的关键筛选器来满足不同CFA模式下的掺杂重建需求。</li>
<li>results: 对于 synthetic 和实际拍摄的RAW数据，该方法实现了与大规模模型相当的掺杂重建性能，并且通过在掺杂重建过程中使用元学习（KLAP-M），可以有效消除不同摄像头的杂质特征。<details>
<summary>Abstract</summary>
As the physical size of recent CMOS image sensors (CIS) gets smaller, the latest mobile cameras are adopting unique non-Bayer color filter array (CFA) patterns (e.g., Quad, Nona, QxQ), which consist of homogeneous color units with adjacent pixels. These non-Bayer sensors are superior to conventional Bayer CFA thanks to their changeable pixel-bin sizes for different light conditions but may introduce visual artifacts during demosaicing due to their inherent pixel pattern structures and sensor hardware characteristics. Previous demosaicing methods have primarily focused on Bayer CFA, necessitating distinct reconstruction methods for non-Bayer patterned CIS with various CFA modes under different lighting conditions. In this work, we propose an efficient unified demosaicing method that can be applied to both conventional Bayer RAW and various non-Bayer CFAs' RAW data in different operation modes. Our Knowledge Learning-based demosaicing model for Adaptive Patterns, namely KLAP, utilizes CFA-adaptive filters for only 1% key filters in the network for each CFA, but still manages to effectively demosaic all the CFAs, yielding comparable performance to the large-scale models. Furthermore, by employing meta-learning during inference (KLAP-M), our model is able to eliminate unknown sensor-generic artifacts in real RAW data, effectively bridging the gap between synthetic images and real sensor RAW. Our KLAP and KLAP-M methods achieved state-of-the-art demosaicing performance in both synthetic and real RAW data of Bayer and non-Bayer CFAs.
</details>
<details>
<summary>摘要</summary>
为了适应现代CMOS图像感知器（CIS）的尺寸逐渐减小，现场摄像头开始采用不同的非拜尔颜色筛子阵（CFA）模式（例如quad、non、QxQ），这些非拜尔筛子阵由相邻的像素组成同色单元。相比传统的拜尔筛子阵，这些非拜尔筛子阵具有可变像素宽度，适应不同的照明条件，但可能会在拼接过程中产生视觉artefacts，这是因为其内置的像素结构和感知器硬件特性。现有的拼接方法主要针对拜尔筛子阵，需要不同的重建方法 для不同的CFA模式和照明条件。在这项工作中，我们提出了一种高效的统一拼接方法，可以应用于拜尔RAW和不同的非拜尔CFAs的RAW数据中。我们的知识学习基于模板（KLAP）使用了CFA适应的筛子 filters，只需要1%的关键筛子在网络中，仍能有效拼接所有CFAs，与大规模模型相当。此外，通过在推理中使用元学习（KLAP-M），我们的模型可以减少真实感知器特有的未知遗传artefacts，有效地跨越真实图像和Synthetic图像之间的差异。我们的KLAP和KLAP-M方法在Synthetic和实际RAW数据中实现了拼接性能的状态计算。
</details></li>
</ul>
<hr>
<h2 id="Physics-Driven-Turbulence-Image-Restoration-with-Stochastic-Refinement"><a href="#Physics-Driven-Turbulence-Image-Restoration-with-Stochastic-Refinement" class="headerlink" title="Physics-Driven Turbulence Image Restoration with Stochastic Refinement"></a>Physics-Driven Turbulence Image Restoration with Stochastic Refinement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10603">http://arxiv.org/abs/2307.10603</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vita-group/pirn">https://github.com/vita-group/pirn</a></li>
<li>paper_authors: Ajay Jaiswal, Xingguang Zhang, Stanley H. Chan, Zhangyang Wang</li>
<li>for: 这个论文是为了解决长距离光学感知系统中的大气扰动引起的图像扭曲问题。</li>
<li>methods: 这篇论文提出了将物理基础的模拟器直接 integrate到网络训练过程中，以帮助网络分离随机性和扭曲以及下面的图像。此外，为了解决“平均效应”和模拟数据和实际世界扰动之间的领域差异，我们还提出了PiRN-SR，以提高感知质量。</li>
<li>results: 我们的PiRN和PiRN-SR可以提高对实际世界未知扰动条件的总体化和提供当前最高的修复精度和感知质量。我们的代码可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/VITA-Group/PiRN%7D">https://github.com/VITA-Group/PiRN}</a> 上获取。<details>
<summary>Abstract</summary>
Image distortion by atmospheric turbulence is a stochastic degradation, which is a critical problem in long-range optical imaging systems. A number of research has been conducted during the past decades, including model-based and emerging deep-learning solutions with the help of synthetic data. Although fast and physics-grounded simulation tools have been introduced to help the deep-learning models adapt to real-world turbulence conditions recently, the training of such models only relies on the synthetic data and ground truth pairs. This paper proposes the Physics-integrated Restoration Network (PiRN) to bring the physics-based simulator directly into the training process to help the network to disentangle the stochasticity from the degradation and the underlying image. Furthermore, to overcome the ``average effect" introduced by deterministic models and the domain gap between the synthetic and real-world degradation, we further introduce PiRN with Stochastic Refinement (PiRN-SR) to boost its perceptual quality. Overall, our PiRN and PiRN-SR improve the generalization to real-world unknown turbulence conditions and provide a state-of-the-art restoration in both pixel-wise accuracy and perceptual quality. Our codes are available at \url{https://github.com/VITA-Group/PiRN}.
</details>
<details>
<summary>摘要</summary>
图像扭曲因大气扰动是一种随机干扰，对远距离光学图像系统来说是一个关键问题。过去几十年内，有多种研究进行了，包括基于模型的和新兴深度学习解决方案，使用了 sintetic 数据。虽然最近引入了快速基于物理的 simulator 来帮助深度学习模型适应实际大气扰动条件，但是这些模型的训练仅仅基于 sintetic 数据和真实数据对。这篇论文提出了 integrating 物理Restoration 网络（PiRN），将物理基于的 simulator 直接引入到训练过程中，帮助网络分离随机性和扰动以及真实图像。此外，为了超越 deterministic 模型的“平均效应”和 sintetic 和实际扰动之间的领域差异，我们进一步引入 PiRN with Stochastic Refinement（PiRN-SR），提高其 perceived 质量。总的来说，我们的 PiRN 和 PiRN-SR 在实际未知大气扰动条件下提高了总体化能力和 perceived 质量，成为当前最佳的Restoration。我们的代码可以在 \url{https://github.com/VITA-Group/PiRN} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Is-Grad-CAM-Explainable-in-Medical-Images"><a href="#Is-Grad-CAM-Explainable-in-Medical-Images" class="headerlink" title="Is Grad-CAM Explainable in Medical Images?"></a>Is Grad-CAM Explainable in Medical Images?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10506">http://arxiv.org/abs/2307.10506</a></li>
<li>repo_url: None</li>
<li>paper_authors: Subhashis Suara, Aayush Jha, Pratik Sinha, Arif Ahmed Sekh</li>
<li>For: The paper is written for the field of artificial intelligence (AI) and medical imaging, with a focus on increasing interpretability and trust in deep learning models for effective diagnosis and treatment planning.* Methods: The paper explores the principles of Explainable Deep Learning and its relevance to medical imaging, discusses various explainability techniques and their limitations, and examines medical imaging applications of Grad-CAM.* Results: The findings highlight the potential of Explainable Deep Learning and Grad-CAM in improving the accuracy and interpretability of deep learning models in medical imaging.Here’s the Chinese version of the three information points:* For: 这篇论文是为人工智能（AI）和医疗影像领域写的，旨在提高深度学习模型的准确性和可解释性，以便更好地诊断和规划治疗。* Methods: 论文探讨了可解释深度学习的原则和它在医疗影像领域的应用，讨论了不同的解释技术和其局限性，并对Grad-CAM在医疗影像应用进行了检查。* Results: 结果表明可解释深度学习和Grad-CAM在医疗影像领域可以提高深度学习模型的准确性和可解释性。I hope that helps!<details>
<summary>Abstract</summary>
Explainable Deep Learning has gained significant attention in the field of artificial intelligence (AI), particularly in domains such as medical imaging, where accurate and interpretable machine learning models are crucial for effective diagnosis and treatment planning. Grad-CAM is a baseline that highlights the most critical regions of an image used in a deep learning model's decision-making process, increasing interpretability and trust in the results. It is applied in many computer vision (CV) tasks such as classification and explanation. This study explores the principles of Explainable Deep Learning and its relevance to medical imaging, discusses various explainability techniques and their limitations, and examines medical imaging applications of Grad-CAM. The findings highlight the potential of Explainable Deep Learning and Grad-CAM in improving the accuracy and interpretability of deep learning models in medical imaging. The code is available in (will be available).
</details>
<details>
<summary>摘要</summary>
<<SYS>> Deep Learning 可 explainer 在人工智能领域内得到了广泛关注，特别是在医学影像领域，因为 Deep Learning 模型的准确性和可解释性对于诊断和治疗规划是非常重要的。 Grad-CAM 是一个基准，可以帮助找出 Deep Learning 模型在做出决策时使用的最重要的图像区域，从而增加模型的可解释性和信任度。 它在许多计算机视觉（CV）任务中使用，如分类和解释。本研究探讨了 Explainable Deep Learning 的原则和医学影像领域的相关性，讨论了不同的解释技术和其限制，并检查了 Grad-CAM 在医学影像应用中的效果。发现结果表明，Explainable Deep Learning 和 Grad-CAM 在医学影像领域可以提高 Deep Learning 模型的准确性和可解释性。代码将在 (will be available) 上提供。>>I hope this helps! Let me know if you have any further questions or if there's anything else I can help with.
</details></li>
</ul>
<hr>
<h2 id="Metaverse-A-Young-Gamer’s-Perspective"><a href="#Metaverse-A-Young-Gamer’s-Perspective" class="headerlink" title="Metaverse: A Young Gamer’s Perspective"></a>Metaverse: A Young Gamer’s Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10439">http://arxiv.org/abs/2307.10439</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ivan V. Bajić, Teo Saeedi-Bajić, Kai Saeedi-Bajić</li>
<li>for: 这个论文旨在了解10岁以下儿童对Metaverse的需求和要求，以便在开发Metaverse技术时能更好地了解这个年轻的用户群体。</li>
<li>methods: 这篇论文使用了年轻游戏玩家的视角来探讨Metaverse，包括他们对Metaverse和物理世界之间的关系，以及他们对Metaverse的期望。</li>
<li>results: 这篇论文提出了一些关于Metaverse技术研发的挑战，以及年轻用户对这些技术的期望。这些结论可能有助于制定更详细的主观实验，以及对Metaverse技术的研发。<details>
<summary>Abstract</summary>
When developing technologies for the Metaverse, it is important to understand the needs and requirements of end users. Relatively little is known about the specific perspectives on the use of the Metaverse by the youngest audience: children ten and under. This paper explores the Metaverse from the perspective of a young gamer. It examines their understanding of the Metaverse in relation to the physical world and other technologies they may be familiar with, looks at some of their expectations of the Metaverse, and then relates these to the specific multimedia signal processing (MMSP) research challenges. The perspectives presented in the paper may be useful for planning more detailed subjective experiments involving young gamers, as well as informing the research on MMSP technologies targeted at these users.
</details>
<details>
<summary>摘要</summary>
When developing technologies for the Metaverse, it is important to understand the needs and requirements of end users. Relatively little is known about the specific perspectives on the use of the Metaverse by the youngest audience: children ten and under. This paper explores the Metaverse from the perspective of a young gamer. It examines their understanding of the Metaverse in relation to the physical world and other technologies they may be familiar with, looks at some of their expectations of the Metaverse, and then relates these to the specific multimedia signal processing (MMSP) research challenges. The perspectives presented in the paper may be useful for planning more detailed subjective experiments involving young gamers, as well as informing the research on MMSP technologies targeted at these users.Here's the translation in Traditional Chinese:当开发Metaverse技术时，了解使用者的需求和要求是非常重要。目前对最年轻的使用者——十岁以下的儿童的Metaverse使用情况所知甚少。本文从年轻的游戏者的角度出发，探讨Metaverse与物理世界以及他们可能熟悉的其他技术之间的关系，并考虑他们对Metaverse的期望，最后将这些观点与多媒体信号处理（MMSP）技术研究挑战相关。文中的观点可能对进行更详细的主观实验 involving young gamers 有所帮助，也可以帮助MMSP技术的研究，targeted at these users。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Latent-Autoencoder-with-Self-Attention-for-Structural-Image-Synthesis"><a href="#Adversarial-Latent-Autoencoder-with-Self-Attention-for-Structural-Image-Synthesis" class="headerlink" title="Adversarial Latent Autoencoder with Self-Attention for Structural Image Synthesis"></a>Adversarial Latent Autoencoder with Self-Attention for Structural Image Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10166">http://arxiv.org/abs/2307.10166</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiajie Fan, Laure Vuaille, Hao Wang, Thomas Bäck</li>
<li>for: This paper is written for facilitating industrial engineering processes using Generative Engineering Design approaches driven by Deep Generative Models (DGM).</li>
<li>methods: The paper proposes a novel model called Self-Attention Adversarial Latent Autoencoder (SA-ALAE) to generate feasible design images of complex engineering parts.</li>
<li>results: The proposed SA-ALAE model allows users to explore novel variants of an existing design and control the generation process by operating in latent space. The potential of SA-ALAE is demonstrated through generating engineering blueprints in a real automotive design task.<details>
<summary>Abstract</summary>
Generative Engineering Design approaches driven by Deep Generative Models (DGM) have been proposed to facilitate industrial engineering processes. In such processes, designs often come in the form of images, such as blueprints, engineering drawings, and CAD models depending on the level of detail. DGMs have been successfully employed for synthesis of natural images, e.g., displaying animals, human faces and landscapes. However, industrial design images are fundamentally different from natural scenes in that they contain rich structural patterns and long-range dependencies, which are challenging for convolution-based DGMs to generate. Moreover, DGM-driven generation process is typically triggered based on random noisy inputs, which outputs unpredictable samples and thus cannot perform an efficient industrial design exploration. We tackle these challenges by proposing a novel model Self-Attention Adversarial Latent Autoencoder (SA-ALAE), which allows generating feasible design images of complex engineering parts. With SA-ALAE, users can not only explore novel variants of an existing design, but also control the generation process by operating in latent space. The potential of SA-ALAE is shown by generating engineering blueprints in a real automotive design task.
</details>
<details>
<summary>摘要</summary>
生成工程设计方法，驱动深度生成模型（DGM），已经提议用于促进工程设计过程。在这些过程中，设计通常以图像形式出现，如蓝图、工程图纸和CAD模型，具体取决于级别。DGM已经成功应用于自然图像的合成，例如显示动物、人脸和风景。然而，工程设计图像与自然场景不同，它们具有丰富的结构征特和长距离依赖关系，这些关系使得混合基于的DGM难以生成。此外，DGM驱动的生成过程通常是基于随机噪音输入触发的，这会输出不可预测的样本，因此无法实现有效的工程设计探索。我们解决这些挑战，提出了一种新的模型，即Self-Attention Adversarial Latent Autoencoder（SA-ALAE）。SA-ALAE允许生成复杂工程部件的可能性。用户不仅可以探索现有设计的新变体，还可以在幂空间控制生成过程。我们在一个真实的汽车设计任务中展示了SA-ALAE的潜力。
</details></li>
</ul>
<hr>
<h2 id="Make-A-Volume-Leveraging-Latent-Diffusion-Models-for-Cross-Modality-3D-Brain-MRI-Synthesis"><a href="#Make-A-Volume-Leveraging-Latent-Diffusion-Models-for-Cross-Modality-3D-Brain-MRI-Synthesis" class="headerlink" title="Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D Brain MRI Synthesis"></a>Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D Brain MRI Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10094">http://arxiv.org/abs/2307.10094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lingting Zhu, Zeyue Xue, Zhenchao Jin, Xian Liu, Jingzhen He, Ziwei Liu, Lequan Yu</li>
<li>for: 这个研究的目的是提出一个新的多modal医疗影像合成方法，以便在医疗影像领域中实现多种应用。</li>
<li>methods: 这个方法使用了2D backbone和一个扩散基础的框架，named Make-A-Volume，来进行跨modal 3D医疗影像合成。它使用了一个潜在的扩散模型来学习跨modal的slice-wise mapping，并且在2D slice-mapping模型中插入了一系列的类型层来缓和频率矩阵的问题。</li>
<li>results: 实验结果显示，这个Make-A-Volume框架可以实现高效的多modal医疗影像合成，并且可以保持体积一致性。<details>
<summary>Abstract</summary>
Cross-modality medical image synthesis is a critical topic and has the potential to facilitate numerous applications in the medical imaging field. Despite recent successes in deep-learning-based generative models, most current medical image synthesis methods rely on generative adversarial networks and suffer from notorious mode collapse and unstable training. Moreover, the 2D backbone-driven approaches would easily result in volumetric inconsistency, while 3D backbones are challenging and impractical due to the tremendous memory cost and training difficulty. In this paper, we introduce a new paradigm for volumetric medical data synthesis by leveraging 2D backbones and present a diffusion-based framework, Make-A-Volume, for cross-modality 3D medical image synthesis. To learn the cross-modality slice-wise mapping, we employ a latent diffusion model and learn a low-dimensional latent space, resulting in high computational efficiency. To enable the 3D image synthesis and mitigate volumetric inconsistency, we further insert a series of volumetric layers in the 2D slice-mapping model and fine-tune them with paired 3D data. This paradigm extends the 2D image diffusion model to a volumetric version with a slightly increasing number of parameters and computation, offering a principled solution for generic cross-modality 3D medical image synthesis. We showcase the effectiveness of our Make-A-Volume framework on an in-house SWI-MRA brain MRI dataset and a public T1-T2 brain MRI dataset. Experimental results demonstrate that our framework achieves superior synthesis results with volumetric consistency.
</details>
<details>
<summary>摘要</summary>
跨模态医疗图像合成是一个关键问题，它具有潜在的应用前景在医疗图像领域。虽然最近的深度学习生成模型已经取得了一定的成功，但大多数当前的医疗图像合成方法仍然基于生成对抗网络，它们受到模式坍缩和训练不稳定的问题困扰。此外，2D脊梁驱动的方法容易导致立体不一致，而3D脊梁却受到巨大的存储成本和训练困难。在本文中，我们提出一种新的方法来合成3D医疗图像，通过利用2D脊梁来实现。我们提出了一种扩展2D图像扩散模型到3D版本的方法，并通过在2D层次映射模型中插入多个立体层来实现3D图像合成。我们采用了潜在空间的扩散模型来学习跨模态的片断 wise 映射，从而实现高效的计算。我们在自有的SWI-MRA脑MRI数据集和公共的T1-T2脑MRI数据集上展示了我们的Make-A-Volume框架的效果，结果表明我们的方法可以实现高质量的合成结果，同时保持立体一致。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/20/eess.IV_2023_07_20/" data-id="clluro5lx00cmq9886mkp1alq" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/19/cs.LG_2023_07_19/" class="article-date">
  <time datetime="2023-07-18T16:00:00.000Z" itemprop="datePublished">2023-07-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/19/cs.LG_2023_07_19/">cs.LG - 2023-07-19 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Android-in-the-Wild-A-Large-Scale-Dataset-for-Android-Device-Control"><a href="#Android-in-the-Wild-A-Large-Scale-Dataset-for-Android-Device-Control" class="headerlink" title="Android in the Wild: A Large-Scale Dataset for Android Device Control"></a>Android in the Wild: A Large-Scale Dataset for Android Device Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10088">http://arxiv.org/abs/2307.10088</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research/google-research">https://github.com/google-research/google-research</a></li>
<li>paper_authors: Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, Timothy Lillicrap</li>
<li>for: 这个研究的目的是为了开发一种可以理解人类自然语言指令并直接控制数字设备用户界面的设备控制系统。</li>
<li>methods: 这个研究使用了一个大型的Android设备控制数据集（Android in the Wild，简称AITW），该数据集包含人类对设备交互的示例，以及相应的自然语言指令。数据集包含715万个 Episodes，涵盖30万个不同的指令，以及四个版本的Android（v10-13）和八种设备类型（Pixel 2 XL到Pixel 6）。</li>
<li>results: 研究人员开发了两个代理，并在数据集上进行了性能测试。数据集采用了一种新的挑战：从视觉上看到的动作需要被推断出来，而不是直接从UI元素上进行操作。此外，动作空间包括精确的手势（例如，水平滚动来操作轮播widget）。研究人员将数据集组织成了一种可以驱动device控制系统的Robustness分析，即如何在新的任务描述、应用程序或平台版本下进行性能测试。<details>
<summary>Abstract</summary>
There is a growing interest in device-control systems that can interpret human natural language instructions and execute them on a digital device by directly controlling its user interface. We present a dataset for device-control research, Android in the Wild (AITW), which is orders of magnitude larger than current datasets. The dataset contains human demonstrations of device interactions, including the screens and actions, and corresponding natural language instructions. It consists of 715k episodes spanning 30k unique instructions, four versions of Android (v10-13),and eight device types (Pixel 2 XL to Pixel 6) with varying screen resolutions. It contains multi-step tasks that require semantic understanding of language and visual context. This dataset poses a new challenge: actions available through the user interface must be inferred from their visual appearance. And, instead of simple UI element-based actions, the action space consists of precise gestures (e.g., horizontal scrolls to operate carousel widgets). We organize our dataset to encourage robustness analysis of device-control systems, i.e., how well a system performs in the presence of new task descriptions, new applications, or new platform versions. We develop two agents and report performance across the dataset. The dataset is available at https://github.com/google-research/google-research/tree/master/android_in_the_wild.
</details>
<details>
<summary>摘要</summary>
“现有一 growing interest in 设备控制系统，可以理解人类自然语言指令并直接控制其用户界面。我们提供了一个设备控制研究数据集，Android in the Wild（AITW），其规模比现有数据集有很大的提升。该数据集包含人类设备交互示例，包括屏幕和操作，以及相应的自然语言指令。其包含30k个独特指令，4个版本的Android（v10-13），8种设备类型（Pixel 2 XL到Pixel 6），具有不同的屏幕分辨率。它包含多步任务，需要语言和视觉上下文的含义理解。这个数据集呈现了一个新的挑战：通过视觉表现来推理操作。而不是简单的UI元素基于的操作，操作空间包括精确的手势（例如，水平滚动来操作轮播组件）。我们将数据集分类，以便对设备控制系统的Robustness分析，即系统在新的任务描述、新的应用程序或新版本平台上的性能如何。我们开发了两个代理，并在数据集上进行性能测试。数据集可以在https://github.com/google-research/google-research/tree/master/android_in_the_wild上获取。”
</details></li>
</ul>
<hr>
<h2 id="A-Dual-Formulation-for-Probabilistic-Principal-Component-Analysis"><a href="#A-Dual-Formulation-for-Probabilistic-Principal-Component-Analysis" class="headerlink" title="A Dual Formulation for Probabilistic Principal Component Analysis"></a>A Dual Formulation for Probabilistic Principal Component Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10078">http://arxiv.org/abs/2307.10078</a></li>
<li>repo_url: None</li>
<li>paper_authors: Henri De Plaen, Johan A. K. Suykens</li>
<li>for: 这篇论文描述了在希尔伯特空间上的概率主成分分析，并证明了优解方案可以表示在对偶空间上。</li>
<li>methods: 该论文使用了概率主成分分析和对偶空间的技术。</li>
<li>results: 论文证明了概率主成分分析可以开发出生成框架，并 illustrate了这种方法在一个玩具数据集和一个实际数据集上的工作。<details>
<summary>Abstract</summary>
In this paper, we characterize Probabilistic Principal Component Analysis in Hilbert spaces and demonstrate how the optimal solution admits a representation in dual space. This allows us to develop a generative framework for kernel methods. Furthermore, we show how it englobes Kernel Principal Component Analysis and illustrate its working on a toy and a real dataset.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们Characterize了概率主成分分析（Probabilistic Principal Component Analysis，PPCA）在希尔伯特空间中的形式，并证明了优解方式可以在副空间中表示。这允许我们开发一个基于kernel方法的生成框架。此外，我们还证明了PPCA包含内核主成分分析（Kernel Principal Component Analysis，KPCA），并在一个玩具数据集和一个实际数据集上 Illustrates its work。Note: "希尔伯特空间" (Hilbert space) is translated as "希尔伯特空间" in Simplified Chinese, and "主成分分析" (Principal Component Analysis) is translated as "主成分分析" in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Accuracy-Estimation-of-Deep-Visual-Models-using-Domain-Adaptive-Adversarial-Perturbation-without-Source-Samples"><a href="#Unsupervised-Accuracy-Estimation-of-Deep-Visual-Models-using-Domain-Adaptive-Adversarial-Perturbation-without-Source-Samples" class="headerlink" title="Unsupervised Accuracy Estimation of Deep Visual Models using Domain-Adaptive Adversarial Perturbation without Source Samples"></a>Unsupervised Accuracy Estimation of Deep Visual Models using Domain-Adaptive Adversarial Perturbation without Source Samples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10062">http://arxiv.org/abs/2307.10062</a></li>
<li>repo_url: None</li>
<li>paper_authors: JoonHo Lee, Jae Oh Woo, Hankyu Moon, Kwonho Lee</li>
<li>for: 这篇论文是为了解决深度视觉模型在不同分布上表现下降的问题，因为存在源和目标分布之间的差异。</li>
<li>methods: 这篇论文使用了 Pseudo-labels 来估计目标统计中的偏差，并且运用了最近的源自由领域适应算法。</li>
<li>results: 这篇论文的结果显示，使用 Pseudo-labels 来估计模型在目标统计中的偏差，可以实现不需要源数据和标签的情况下，并且比过去需要源数据和标签的方法表现更好。<details>
<summary>Abstract</summary>
Deploying deep visual models can lead to performance drops due to the discrepancies between source and target distributions. Several approaches leverage labeled source data to estimate target domain accuracy, but accessing labeled source data is often prohibitively difficult due to data confidentiality or resource limitations on serving devices. Our work proposes a new framework to estimate model accuracy on unlabeled target data without access to source data. We investigate the feasibility of using pseudo-labels for accuracy estimation and evolve this idea into adopting recent advances in source-free domain adaptation algorithms. Our approach measures the disagreement rate between the source hypothesis and the target pseudo-labeling function, adapted from the source hypothesis. We mitigate the impact of erroneous pseudo-labels that may arise due to a high ideal joint hypothesis risk by employing adaptive adversarial perturbation on the input of the target model. Our proposed source-free framework effectively addresses the challenging distribution shift scenarios and outperforms existing methods requiring source data and labels for training.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Deploying deep visual models can lead to performance drops due to the discrepancies between source and target distributions. Several approaches leverage labeled source data to estimate target domain accuracy, but accessing labeled source data is often prohibitively difficult due to data confidentiality or resource limitations on serving devices. Our work proposes a new framework to estimate model accuracy on unlabeled target data without access to source data. We investigate the feasibility of using pseudo-labels for accuracy estimation and evolve this idea into adopting recent advances in source-free domain adaptation algorithms. Our approach measures the disagreement rate between the source hypothesis and the target pseudo-labeling function, adapted from the source hypothesis. We mitigate the impact of erroneous pseudo-labels that may arise due to a high ideal joint hypothesis risk by employing adaptive adversarial perturbation on the input of the target model. Our proposed source-free framework effectively addresses the challenging distribution shift scenarios and outperforms existing methods requiring source data and labels for training."into Simplified Chinese.Here's the translation:<<SYS>>发布深度视觉模型可能会导致性能下降，这是因为源和目标分布之间存在差异。一些方法利用标注的源数据来估计目标频道准确率，但是获取标注的源数据经常是由数据保密或服务器上的资源限制所困难。我们的工作提出了一个新的框架，可以在无法访问源数据的情况下估计目标频道准确率。我们investigate了使用 Pseudo-labels 来估计准确率，并采用了最新的无源适应频道适应算法。我们的方法测量源假设和目标假设函数之间的不一致率，这个概念来自源假设。我们还使用适应式对抗扰动来减轻高理想共同风险引起的假设扰动的影响。我们的提议的无源框架可以有效地解决分布转换场景，并且超越了需要源数据和标签进行训练的现有方法。
</details></li>
</ul>
<hr>
<h2 id="Accurate-deep-learning-sub-grid-scale-models-for-large-eddy-simulations"><a href="#Accurate-deep-learning-sub-grid-scale-models-for-large-eddy-simulations" class="headerlink" title="Accurate deep learning sub-grid scale models for large eddy simulations"></a>Accurate deep learning sub-grid scale models for large eddy simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10060">http://arxiv.org/abs/2307.10060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rikhi Bose, Arunabha M. Roy<br>for:这两种模型是用于大涨�edy simulation（LES）目的的尺度下降涨�edy模型。methods:这两种模型使用了深度学习（DL）算法，与现有的分析模型技术不同，可以生成高阶复杂非线性关系 между输入和输出。results:两种模型在不同的滤波宽度和 Reynolds 数下进行了预测，并且在统计性能指标上表现出了不同的result。 simpler 模型具有更好的特征学习能力，因此在预测 SGS 压力方面表现出了更高的水平。<details>
<summary>Abstract</summary>
We present two families of sub-grid scale (SGS) turbulence models developed for large-eddy simulation (LES) purposes. Their development required the formulation of physics-informed robust and efficient Deep Learning (DL) algorithms which, unlike state-of-the-art analytical modeling techniques can produce high-order complex non-linear relations between inputs and outputs. Explicit filtering of data from direct simulations of the canonical channel flow at two friction Reynolds numbers $Re_\tau\approx 395$ and 590 provided accurate data for training and testing. The two sets of models use different network architectures. One of the architectures uses tensor basis neural networks (TBNN) and embeds the simplified analytical model form of the general effective-viscosity hypothesis, thus incorporating the Galilean, rotational and reflectional invariances. The other architecture is that of a relatively simple network, that is able to incorporate the Galilean invariance only. However, this simpler architecture has better feature extraction capacity owing to its ability to establish relations between and extract information from cross-components of the integrity basis tensors and the SGS stresses. Both sets of models are used to predict the SGS stresses for feature datasets generated with different filter widths, and at different Reynolds numbers. It is shown that due to the simpler model's better feature learning capabilities, it outperforms the invariance embedded model in statistical performance metrics. In a priori tests, both sets of models provide similar levels of dissipation and backscatter. Based on the test results, both sets of models should be usable in a posteriori actual LESs.
</details>
<details>
<summary>摘要</summary>
我们提出了两家互相关联的子grid尺度湍流（SGS）模型，用于大扰拍� simulation（LES）目的。它们的发展需要了实现物理学 Informed 强大和有效的深度学习（DL）算法，不同于现有的分析模型技术，可以生成高阶复杂的非线性关系。我们从直接实验的标准 кананиче流道流动中获取了精确的数据进行训练和测试。这两组模型使用不同的网络架构。一个架构使用tensor基础神经网络（TBNN），并将简化的分析模型形式给嵌入通用有效黏度假设，因此包含加利ле安、Rotational和反射symmetries。另一个架构是一个相对简单的网络，它能够包含加利ле安对称性，但这个简单的架构具有更好的特征提取能力，因为它可以在标�âxis和SGS压力之间建立关系和提取信息。这两组模型用于预测SGS压力的预测dataset，生成了不同滤镜宽度和 Reynolds 数的特征。根据测试结果，简单的模型在统计性能指标上表现较好，但两组模型在预�期� TEST 中提供了相似的损失和反射。因此，这两组模型可以在 posteriori 实际 LES 中使用。
</details></li>
</ul>
<hr>
<h2 id="Convergence-Guarantees-for-Stochastic-Subgradient-Methods-in-Nonsmooth-Nonconvex-Optimization"><a href="#Convergence-Guarantees-for-Stochastic-Subgradient-Methods-in-Nonsmooth-Nonconvex-Optimization" class="headerlink" title="Convergence Guarantees for Stochastic Subgradient Methods in Nonsmooth Nonconvex Optimization"></a>Convergence Guarantees for Stochastic Subgradient Methods in Nonsmooth Nonconvex Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10053">http://arxiv.org/abs/2307.10053</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xnchxy/GeneralSGD">https://github.com/xnchxy/GeneralSGD</a></li>
<li>paper_authors: Nachuan Xiao, Xiaoyin Hu, Kim-Chuan Toh</li>
<li>for:  investigate the convergence properties of stochastic gradient descent (SGD) method and its variants, especially in training neural networks built from nonsmooth activation functions.</li>
<li>methods:  develop a novel framework that assigns different timescales to stepsizes for updating the momentum terms and variables, and prove the global convergence of the proposed framework in both single-timescale and two-timescale cases.</li>
<li>results:  prove that the proposed framework encompasses a wide range of well-known SGD-type methods, including heavy-ball SGD, SignSGD, Lion, normalized SGD and clipped SGD, and demonstrate the high efficiency of the analyzed SGD-type methods through preliminary numerical experiments.<details>
<summary>Abstract</summary>
In this paper, we investigate the convergence properties of the stochastic gradient descent (SGD) method and its variants, especially in training neural networks built from nonsmooth activation functions. We develop a novel framework that assigns different timescales to stepsizes for updating the momentum terms and variables, respectively. Under mild conditions, we prove the global convergence of our proposed framework in both single-timescale and two-timescale cases. We show that our proposed framework encompasses a wide range of well-known SGD-type methods, including heavy-ball SGD, SignSGD, Lion, normalized SGD and clipped SGD. Furthermore, when the objective function adopts a finite-sum formulation, we prove the convergence properties for these SGD-type methods based on our proposed framework. In particular, we prove that these SGD-type methods find the Clarke stationary points of the objective function with randomly chosen stepsizes and initial points under mild assumptions. Preliminary numerical experiments demonstrate the high efficiency of our analyzed SGD-type methods.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究束定的泛化特性，尤其是使用不规则激活函数construct neural networks的SGD方法和其变体。我们开发了一种新的框架，它在更新势能阶段和变量阶段分别 assign different timescales。在某些轻度条件下，我们证明了我们提议的框架的全球收敛性。我们还证明了我们的框架包括许多已知的SGD类型方法，包括重力SGD、SignSGD、Lion、normalized SGD和clipped SGD。在对象函数采用finite-sum形式时，我们证明了这些SGD类型方法的收敛性基于我们的框架。具体来说，我们证明这些SGD类型方法可以随机选择步长和初始点，并在一定假设下找到对象函数的clarke站点。先前的数值实验表明了我们分析的SGD类型方法的高效性。
</details></li>
</ul>
<hr>
<h2 id="Contextual-Reliability-When-Different-Features-Matter-in-Different-Contexts"><a href="#Contextual-Reliability-When-Different-Features-Matter-in-Different-Contexts" class="headerlink" title="Contextual Reliability: When Different Features Matter in Different Contexts"></a>Contextual Reliability: When Different Features Matter in Different Contexts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10026">http://arxiv.org/abs/2307.10026</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gaurav Ghosal, Amrith Setlur, Daniel S. Brown, Anca D. Dragan, Aditi Raghunathan</li>
<li>for: 这篇论文旨在解决深度神经网络在依据偶极关系时出现catastrophic failure的问题。</li>
<li>methods: 该论文提出了一种新的设定 Contextual Reliability，以及一种两stage的解决方案 Explicit Non-spurious feature Prediction (ENP)，可以在不同上下文中选择适当的特征来避免偶极关系。</li>
<li>results: 论文通过理论和实验验证了ENP的优势，并提供了新的Contextual Reliability的benchmark。<details>
<summary>Abstract</summary>
Deep neural networks often fail catastrophically by relying on spurious correlations. Most prior work assumes a clear dichotomy into spurious and reliable features; however, this is often unrealistic. For example, most of the time we do not want an autonomous car to simply copy the speed of surrounding cars -- we don't want our car to run a red light if a neighboring car does so. However, we cannot simply enforce invariance to next-lane speed, since it could provide valuable information about an unobservable pedestrian at a crosswalk. Thus, universally ignoring features that are sometimes (but not always) reliable can lead to non-robust performance. We formalize a new setting called contextual reliability which accounts for the fact that the "right" features to use may vary depending on the context. We propose and analyze a two-stage framework called Explicit Non-spurious feature Prediction (ENP) which first identifies the relevant features to use for a given context, then trains a model to rely exclusively on these features. Our work theoretically and empirically demonstrates the advantages of ENP over existing methods and provides new benchmarks for contextual reliability.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard writing system used in mainland China. The translation is based on the standardized grammar and vocabulary of Simplified Chinese, and may differ from the traditional Chinese writing system used in Hong Kong and Taiwan.
</details></li>
</ul>
<hr>
<h2 id="Europepolls-A-Dataset-of-Country-Level-Opinion-Polling-Data-for-the-European-Union-and-the-UK"><a href="#Europepolls-A-Dataset-of-Country-Level-Opinion-Polling-Data-for-the-European-Union-and-the-UK" class="headerlink" title="Europepolls: A Dataset of Country-Level Opinion Polling Data for the European Union and the UK"></a>Europepolls: A Dataset of Country-Level Opinion Polling Data for the European Union and the UK</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10022">http://arxiv.org/abs/2307.10022</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/konstantinos-p/europepolls">https://github.com/konstantinos-p/europepolls</a></li>
<li>paper_authors: Konstantinos Pitas</li>
<li>for: 这个论文的目的是提供一个开放的国家级历史意见调查数据集 для欧盟和英国，填补现有的意见调查数据 gap。</li>
<li>methods: 这个论文使用Wikipedia数据和pandas库进行了处理，并提供了Raw和Preprocessed数据在.csv格式下。</li>
<li>results: 这个论文的数据集可以帮助研究人员通过对多 modal数据（新闻文章、经济指标、社交媒体）和选举行为之间的复杂相互作用进行研究。<details>
<summary>Abstract</summary>
I propose an open dataset of country-level historical opinion polling data for the European Union and the UK. The dataset aims to fill a gap in available opinion polling data for the European Union. Some existing datasets are restricted to the past five years, limiting research opportunities. At the same time, some larger proprietary datasets exist but are available only in a visual preprocessed time series format. Finally, while other large datasets for individual countries might exist, these could be inaccessible due to language barriers. The data was gathered from Wikipedia, and preprocessed using the pandas library. Both the raw and the preprocessed data are in the .csv format. I hope that given the recent advances in LLMs and deep learning in general, this large dataset will enable researchers to uncover complex interactions between multimodal data (news articles, economic indicators, social media) and voting behavior. The raw data, the preprocessed data, and the preprocessing scripts are available on GitHub.
</details>
<details>
<summary>摘要</summary>
我提议一个开放的国家级历史民意调查数据集 для欧盟和英国。这个数据集的目标是填充有限的available民意调查数据集 для欧盟。一些现有的数据集只有过去五年的数据，限制了研究机会。同时，一些更大的专有数据集存在，但只能在可视化预处理时间序列格式下获得。此外，尚存在其他各国大数据集，但可能因语言障碍而无法访问。数据来源于Wikipedia，并使用pandas库进行预处理。原始数据和预处理后的数据均在.csv格式下可用。我希望，随着最近的LLMs和深度学习技术的发展，这个大数据集将帮助研究人员发现多Modal数据（新闻文章、经济指标、社交媒体）和选举行为之间的复杂交互。原始数据、预处理后数据以及预处理脚本都可以在GitHub上下载。
</details></li>
</ul>
<hr>
<h2 id="TbExplain-A-Text-based-Explanation-Method-for-Scene-Classification-Models-with-the-Statistical-Prediction-Correction"><a href="#TbExplain-A-Text-based-Explanation-Method-for-Scene-Classification-Models-with-the-Statistical-Prediction-Correction" class="headerlink" title="TbExplain: A Text-based Explanation Method for Scene Classification Models with the Statistical Prediction Correction"></a>TbExplain: A Text-based Explanation Method for Scene Classification Models with the Statistical Prediction Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10003">http://arxiv.org/abs/2307.10003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirhossein Aminimehr, Pouya Khani, Amirali Molaei, Amirmohammad Kazemeini, Erik Cambria</li>
<li>for: 该文章目的是提高黑盒机器学习模型的可解释性，并使用 XAI 技术和预训练对象检测器来提供场景分类模型的文本基于解释。</li>
<li>methods: 该文章使用了一种名为 TbExplain 的框架，该框架使用 XAI 技术和预训练对象检测器来提供文本基于解释，并在初始预测不可靠时使用一种新的方法来修正预测和文本基于解释。</li>
<li>results: 对于Scene Classification Task，TbExplain 与 ResNet 变体进行了量化和质量测试，结果表明 TbExplain 可以提高分类精度。<details>
<summary>Abstract</summary>
The field of Explainable Artificial Intelligence (XAI) aims to improve the interpretability of black-box machine learning models. Building a heatmap based on the importance value of input features is a popular method for explaining the underlying functions of such models in producing their predictions. Heatmaps are almost understandable to humans, yet they are not without flaws. Non-expert users, for example, may not fully understand the logic of heatmaps (the logic in which relevant pixels to the model's prediction are highlighted with different intensities or colors). Additionally, objects and regions of the input image that are relevant to the model prediction are frequently not entirely differentiated by heatmaps. In this paper, we propose a framework called TbExplain that employs XAI techniques and a pre-trained object detector to present text-based explanations of scene classification models. Moreover, TbExplain incorporates a novel method to correct predictions and textually explain them based on the statistics of objects in the input image when the initial prediction is unreliable. To assess the trustworthiness and validity of the text-based explanations, we conducted a qualitative experiment, and the findings indicated that these explanations are sufficiently reliable. Furthermore, our quantitative and qualitative experiments on TbExplain with scene classification datasets reveal an improvement in classification accuracy over ResNet variants.
</details>
<details>
<summary>摘要</summary>
黑盒机器学习模型的解释 artificial intelligence (XAI) 目标是将模型预测的解释提高到可解释的程度。建立基于输入特征价值的热力地图是一种广泛使用的方法来解释这些模型在预测中的下面运作。热力地图可以让人类更好地理解，但是它们并不是无懈的。例如，非专家用户可能无法完全理解热力地图中的逻辑（在预测中重要的像素点获得不同的强度或颜色）。此外，输入图像中对模型预测的重要物件和区域也经常不会被热力地图完全区别开来。在这篇文章中，我们提出了一个名为 TbExplain 的框架，它利用 XAI 技术和预训的物件探测器来提供文本基于解释Scene Classification 模型。此外，TbExplain 还包括一种新的方法，可以根据输入图像中物件的统计数据修正预测并文本解释它们，当初始预测不可靠时。为了评估文本解释的可信度和有效性，我们进行了一个质感实验，结果显示了这些解释是可靠的。此外，我们还进行了量化和质感实验，结果显示 TbExplain 在Scene Classification 数据集上具有改善类别精度的能力。
</details></li>
</ul>
<hr>
<h2 id="Impact-of-Disentanglement-on-Pruning-Neural-Networks"><a href="#Impact-of-Disentanglement-on-Pruning-Neural-Networks" class="headerlink" title="Impact of Disentanglement on Pruning Neural Networks"></a>Impact of Disentanglement on Pruning Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09994">http://arxiv.org/abs/2307.09994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carl Shneider, Peyman Rostami, Anis Kacem, Nilotpal Sinha, Abd El Rahman Shabayek, Djamila Aouada</li>
<li>for: 实现Edge设备上深度学习神经网络的运行，以完成实际世界中的任务特定目标，需要对神经网络进行压缩。</li>
<li>methods: 使用缓存自动encoder网络生成分离的内存表现，以实现模型压缩。</li>
<li>results: 透过强制网络学习分离表现，进行条件压缩，以实现分离的实现。<details>
<summary>Abstract</summary>
Deploying deep learning neural networks on edge devices, to accomplish task specific objectives in the real-world, requires a reduction in their memory footprint, power consumption, and latency. This can be realized via efficient model compression. Disentangled latent representations produced by variational autoencoder (VAE) networks are a promising approach for achieving model compression because they mainly retain task-specific information, discarding useless information for the task at hand. We make use of the Beta-VAE framework combined with a standard criterion for pruning to investigate the impact of forcing the network to learn disentangled representations on the pruning process for the task of classification. In particular, we perform experiments on MNIST and CIFAR10 datasets, examine disentanglement challenges, and propose a path forward for future works.
</details>
<details>
<summary>摘要</summary>
部署深度学习神经网络在边缘设备上，以实现实际世界中的任务特定目标，需要减少其内存占用量、功耗和延迟时间。这可以通过有效的模型压缩来实现。Variational autoencoder（VAE）网络生成的分离 latent representation 是一种有前途的方法，因为它们主要保留任务特定的信息，抛弃不必要的信息。我们使用 Beta-VAE 框架并与标准的剪枝 criterion 结合，研究在 classification 任务上强制网络学习分离表示的影响。在具体来说，我们在 MNIST 和 CIFAR10 数据集上进行实验，探讨分离挑战，并提出未来工作的道路。
</details></li>
</ul>
<hr>
<h2 id="UniMatch-A-Unified-User-Item-Matching-Framework-for-the-Multi-purpose-Merchant-Marketing"><a href="#UniMatch-A-Unified-User-Item-Matching-Framework-for-the-Multi-purpose-Merchant-Marketing" class="headerlink" title="UniMatch: A Unified User-Item Matching Framework for the Multi-purpose Merchant Marketing"></a>UniMatch: A Unified User-Item Matching Framework for the Multi-purpose Merchant Marketing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09989">http://arxiv.org/abs/2307.09989</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qifang Zhao, Tianyu Li, Meng Du, Yu Jiang, Qinghui Sun, Zhongyao Wang, Hong Liu, Huan Xu</li>
<li>for: 降低私域市场营销Cloud服务的成本</li>
<li>methods: 一个简单的用户项目匹配框架，同时实现用户定向和项目推荐，只需一个模型</li>
<li>results: 比 estado-of-the-art方法更高的性能，减少计算资源和日常维护成本<details>
<summary>Abstract</summary>
When doing private domain marketing with cloud services, the merchants usually have to purchase different machine learning models for the multiple marketing purposes, leading to a very high cost. We present a unified user-item matching framework to simultaneously conduct item recommendation and user targeting with just one model. We empirically demonstrate that the above concurrent modeling is viable via modeling the user-item interaction matrix with the multinomial distribution, and propose a bidirectional bias-corrected NCE loss for the implementation. The proposed loss function guides the model to learn the user-item joint probability $p(u,i)$ instead of the conditional probability $p(i|u)$ or $p(u|i)$ through correcting both the users and items' biases caused by the in-batch negative sampling. In addition, our framework is model-agnostic enabling a flexible adaptation of different model architectures. Extensive experiments demonstrate that our framework results in significant performance gains in comparison with the state-of-the-art methods, with greatly reduced cost on computing resources and daily maintenance.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:在私有领域市场营销中，商户通常需要购买不同的机器学习模型来满足多种营销目的，这会导致极高的成本。我们提出了一个统一用户 Item 匹配框架，可同时进行 Item 推荐和用户定向。我们采用多omial分布来模型用户 Item 交互矩阵，并提出了一种双向偏置修正 NCE 损失函数。该损失函数导引模型学习用户 Item 共同概率 $p(u,i)$，而不是用户或 Item 的 conditional probability $p(i|u)$ 或 $p(u|i)$。我们的框架是模型无关的，可以适应不同的模型架构。我们的实验结果表明，我们的框架可以与状态 искус法方法进行比较，并且具有明显的成本减少和日常维护降低。
</details></li>
</ul>
<hr>
<h2 id="TinyTrain-Deep-Neural-Network-Training-at-the-Extreme-Edge"><a href="#TinyTrain-Deep-Neural-Network-Training-at-the-Extreme-Edge" class="headerlink" title="TinyTrain: Deep Neural Network Training at the Extreme Edge"></a>TinyTrain: Deep Neural Network Training at the Extreme Edge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09988">http://arxiv.org/abs/2307.09988</a></li>
<li>repo_url: None</li>
<li>paper_authors: Young D. Kwon, Rui Li, Stylianos I. Venieris, Jagmohan Chauhan, Nicholas D. Lane, Cecilia Mascolo</li>
<li>for: 这篇论文的目的是提出一种基于设备上训练的个人化和隐私保护方法，并且因应物联网设备的数据稀缺和 computation 资源的限制，以提高训练时间的效率和精度。</li>
<li>methods: 本论文提出的方法是基于 selective 更新和硬件约束的实现，将网络模型分成多个部分，并且适当地更新每个部分，以减少训练时间和内存负载。</li>
<li>results: 本论文的实验结果显示，该方法可以与基于整个网络的 fine-tuning 方法相比，提高精度并且减少训练时间和内存负载。另外，该方法还可以在实际的边缘设备上进行训练，并且可以与现有的设备进行集成。<details>
<summary>Abstract</summary>
On-device training is essential for user personalisation and privacy. With the pervasiveness of IoT devices and microcontroller units (MCU), this task becomes more challenging due to the constrained memory and compute resources, and the limited availability of labelled user data. Nonetheless, prior works neglect the data scarcity issue, require excessively long training time (e.g. a few hours), or induce substantial accuracy loss ($\geq$10\%). We propose TinyTrain, an on-device training approach that drastically reduces training time by selectively updating parts of the model and explicitly coping with data scarcity. TinyTrain introduces a task-adaptive sparse-update method that dynamically selects the layer/channel based on a multi-objective criterion that jointly captures user data, the memory, and the compute capabilities of the target device, leading to high accuracy on unseen tasks with reduced computation and memory footprint. TinyTrain outperforms vanilla fine-tuning of the entire network by 3.6-5.0\% in accuracy, while reducing the backward-pass memory and computation cost by up to 2,286$\times$ and 7.68$\times$, respectively. Targeting broadly used real-world edge devices, TinyTrain achieves 9.5$\times$ faster and 3.5$\times$ more energy-efficient training over status-quo approaches, and 2.8$\times$ smaller memory footprint than SOTA approaches, while remaining within the 1 MB memory envelope of MCU-grade platforms.
</details>
<details>
<summary>摘要</summary>
<<SYS>>通过设备直接训练实现用户个性化和隐私，因为互联网对象（IoT）设备和微控制器单元（MCU）的普遍存在，这种任务变得更加挑战性。然而，先前的研究忽视了数据缺乏问题，需要过长的训练时间（例如几个小时），或者导致重大的准确性损失（至少10%）。我们提出了TinyTrain，一种在设备上进行训练的方法，可以减少训练时间，并且明确地处理数据缺乏问题。TinyTrain引入了任务适应的稀疏更新方法，可以在多目标 criterion 中动态选择层/通道，以高度准确地处理未见任务，并且减少了计算和存储开销。相比于普通的精度调整，TinyTrain提高了9.5倍的训练速度和3.5倍的能效性，同时占用的内存占用面积只有1MB，比SOTAapproach小3.8倍。targeting 广泛使用的实际世界边缘设备，TinyTrain实现了9.5倍的训练速度和3.5倍的能效性，同时占用的内存占用面积只有1MB，比SOTAapproach小3.8倍。Note: "SOTA" stands for "State of the Art", which means the current best performance in a particular field or task.
</details></li>
</ul>
<hr>
<h2 id="Learner-Referral-for-Cost-Effective-Federated-Learning-Over-Hierarchical-IoT-Networks"><a href="#Learner-Referral-for-Cost-Effective-Federated-Learning-Over-Hierarchical-IoT-Networks" class="headerlink" title="Learner Referral for Cost-Effective Federated Learning Over Hierarchical IoT Networks"></a>Learner Referral for Cost-Effective Federated Learning Over Hierarchical IoT Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09977">http://arxiv.org/abs/2307.09977</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yulan Gao, Ziqiang Ye, Yue Xiao, Wei Xiang</li>
<li>for: 提高 federated learning 中数据隐私护卫和资源占用的问题</li>
<li>methods: 提出了joint learner referral协助 federated client选择（LRef-FedCS），通信和计算资源调度，以及本地模型准确率优化（LMAO）方法，以最小化worst-case参与者的成本，保证 federated learning 在层次互联网络中长期公平</li>
<li>results: 通过LYAPUNOV优化技术，将原问题转化为Stepwise Joint Optimization Problem（JOP），并使用分布式LRef-FedCS方法和自适应全球最佳匹配搜索（SGHS）算法进行解决，实现了规模化和可扩展性。numerical simulation和实验结果表明，提出的LRef-FedCS方法可以很好地平衡追求高全球准确率和降低成本。<details>
<summary>Abstract</summary>
The paradigm of federated learning (FL) to address data privacy concerns by locally training parameters on resource-constrained clients in a distributed manner has garnered significant attention. Nonetheless, FL is not applicable when not all clients within the coverage of the FL server are registered with the FL network. To bridge this gap, this paper proposes joint learner referral aided federated client selection (LRef-FedCS), along with communications and computing resource scheduling, and local model accuracy optimization (LMAO) methods. These methods are designed to minimize the cost incurred by the worst-case participant and ensure the long-term fairness of FL in hierarchical Internet of Things (HieIoT) networks. Utilizing the Lyapunov optimization technique, we reformulate the original problem into a stepwise joint optimization problem (JOP). Subsequently, to tackle the mixed-integer non-convex JOP, we separatively and iteratively address LRef-FedCS and LMAO through the centralized method and self-adaptive global best harmony search (SGHS) algorithm, respectively. To enhance scalability, we further propose a distributed LRef-FedCS approach based on a matching game to replace the centralized method described above. Numerical simulations and experimental results on the MNIST/CIFAR-10 datasets demonstrate that our proposed LRef-FedCS approach could achieve a good balance between pursuing high global accuracy and reducing cost.
</details>
<details>
<summary>摘要</summary>
“联邦学习（FL）的模式以地方化训练参数，在资源有限的客户端上进行分布式方式，吸引了广泛关注。然而，FL不适用于所有客户端都注册在FL网络中。为了填补这个差距，这篇文章提出了共同学习者引导协助 federated client 选择（LRef-FedCS），以及通信和计算资源分配，本地模型精度优化（LMAO）方法。这些方法旨在最小化最差情况下的成本，并确保长期的公平性在层次互联网络（HieIoT）中。使用 Lyapunov 优化技术，我们将原始问题转换为一个步骤化的共同优化问题（JOP）。然后，为了解决混合整数非凸 JOP，我们采用分类和迭代地 addresses LRef-FedCS 和 LMAO，分别使用中央化方法和自适应全球最佳搜索（SGHS）算法。为了提高可扩展性，我们还提出了基于对称游戏的分布式 LRef-FedCS 方法。数据验证和实验结果显示，我们的提出的 LRef-FedCS 方法可以实现高度的全球精度，同时降低成本。”
</details></li>
</ul>
<hr>
<h2 id="Towards-green-AI-based-software-systems-an-architecture-centric-approach-GAISSA"><a href="#Towards-green-AI-based-software-systems-an-architecture-centric-approach-GAISSA" class="headerlink" title="Towards green AI-based software systems: an architecture-centric approach (GAISSA)"></a>Towards green AI-based software systems: an architecture-centric approach (GAISSA)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09964">http://arxiv.org/abs/2307.09964</a></li>
<li>repo_url: None</li>
<li>paper_authors: Silverio Martínez-Fernández, Xavier Franch, Francisco Durán</li>
<li>for: 提高人工智能系统的能效性，满足社会上的能源协调需求。</li>
<li>methods: 提供数据科学家和软件工程师工具支持，建立体系中心的方法，用于模拟和开发绿色人工智能系统。</li>
<li>results: 初步研究结果表明，GAISSA项目可能实现其目标。<details>
<summary>Abstract</summary>
Nowadays, AI-based systems have achieved outstanding results and have outperformed humans in different domains. However, the processes of training AI models and inferring from them require high computational resources, which pose a significant challenge in the current energy efficiency societal demand. To cope with this challenge, this research project paper describes the main vision, goals, and expected outcomes of the GAISSA project. The GAISSA project aims at providing data scientists and software engineers tool-supported, architecture-centric methods for the modelling and development of green AI-based systems. Although the project is in an initial stage, we describe the current research results, which illustrate the potential to achieve GAISSA objectives.
</details>
<details>
<summary>摘要</summary>
现在，基于人工智能的系统已经取得了出色的成绩，在不同的领域都超越了人类。然而，训练人工智能模型和从其进行推理需要高度的计算资源，这对现代社会能源效率的要求带来了重大挑战。为了解决这个挑战，本研究项目paper描述了GAISSA项目的主要视野、目标和预期结果。GAISSA项目旨在为数据科学家和软件工程师提供工具支持、体系中心的方法，以开发可持续的人工智能基本系统。虽然项目还在初始阶段，我们将介绍当前的研究成果，以ILLUSTRATEGAISSA目标的潜在实现可能性。
</details></li>
</ul>
<hr>
<h2 id="XSkill-Cross-Embodiment-Skill-Discovery"><a href="#XSkill-Cross-Embodiment-Skill-Discovery" class="headerlink" title="XSkill: Cross Embodiment Skill Discovery"></a>XSkill: Cross Embodiment Skill Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09955">http://arxiv.org/abs/2307.09955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengda Xu, Zhenjia Xu, Cheng Chi, Manuela Veloso, Shuran Song</li>
<li>for: 本研究的目的是开发一种基于模仿学习的机器人学习框架，以便从人类视频中提取可重用的机器人操作技能。</li>
<li>methods: 本研究使用了一种名为XSkill的模仿学习框架，该框架可以在没有标注数据的情况下，从人类和机器人的抓取视频中分析出各种操作技能的核心特征，并将其转移到机器人的动作中。</li>
<li>results: 实验结果表明，XSkill可以帮助机器人学习并执行未经见过的任务，并且可以很好地组合已知的技能来实现新的任务。此外，XSkill还可以在实际环境中进行应用。<details>
<summary>Abstract</summary>
Human demonstration videos are a widely available data source for robot learning and an intuitive user interface for expressing desired behavior. However, directly extracting reusable robot manipulation skills from unstructured human videos is challenging due to the big embodiment difference and unobserved action parameters. To bridge this embodiment gap, this paper introduces XSkill, an imitation learning framework that 1) discovers a cross-embodiment representation called skill prototypes purely from unlabeled human and robot manipulation videos, 2) transfers the skill representation to robot actions using conditional diffusion policy, and finally, 3) composes the learned skill to accomplish unseen tasks specified by a human prompt video. Our experiments in simulation and real-world environments show that the discovered skill prototypes facilitate both skill transfer and composition for unseen tasks, resulting in a more general and scalable imitation learning framework. The performance of XSkill is best understood from the anonymous website: https://xskillcorl.github.io.
</details>
<details>
<summary>摘要</summary>
人类示例视频是机器人学习的广泛可用数据源，同时也是一种直观的用户界面，可以表达机器人需要的行为。然而，直接从无结构的人类视频中提取可重用的机器人操作技能是困难的，因为人类和机器人之间存在大的实体差异和未观察到的动作参数。为bridging这个实体差距，这篇论文提出了XSkill，一个仿效学习框架，它可以：1. 从无标签的人类和机器人操作视频中发现跨实体的表示，称为技能原型。2. 使用条件扩散策略将技能表示转移到机器人动作中。3. 使用人类提示视频来组合学习的技能，以完成未见任务。我们在 simulations 和实际环境中进行了实验，结果显示，XSkill 可以成功地传递和组合学习的技能，以便更通用和扩展的仿效学习框架。XSkill 的性能可以通过无名网站：https://xskillcorl.github.io 来了解。
</details></li>
</ul>
<hr>
<h2 id="Impatient-Bandits-Optimizing-Recommendations-for-the-Long-Term-Without-Delay"><a href="#Impatient-Bandits-Optimizing-Recommendations-for-the-Long-Term-Without-Delay" class="headerlink" title="Impatient Bandits: Optimizing Recommendations for the Long-Term Without Delay"></a>Impatient Bandits: Optimizing Recommendations for the Long-Term Without Delay</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09943">http://arxiv.org/abs/2307.09943</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/spotify-research/impatient-bandits">https://github.com/spotify-research/impatient-bandits</a></li>
<li>paper_authors: Thomas M. McDonald, Lucas Maystre, Mounia Lalmas, Daniel Russo, Kamil Ciosek</li>
<li>for: 本研究旨在提高在线平台上的推荐系统的长期满意度。</li>
<li>methods: 本文使用多重臂拥有延迟奖励的多臂针对问题，并开发了一种基于这种预测模型的针对算法。</li>
<li>results: 对于一个Podcast推荐问题，我们的方法比使用短期或媒体期间的奖励来优化推荐的方法显著提高了性能。<details>
<summary>Abstract</summary>
Recommender systems are a ubiquitous feature of online platforms. Increasingly, they are explicitly tasked with increasing users' long-term satisfaction. In this context, we study a content exploration task, which we formalize as a multi-armed bandit problem with delayed rewards. We observe that there is an apparent trade-off in choosing the learning signal: Waiting for the full reward to become available might take several weeks, hurting the rate at which learning happens, whereas measuring short-term proxy rewards reflects the actual long-term goal only imperfectly. We address this challenge in two steps. First, we develop a predictive model of delayed rewards that incorporates all information obtained to date. Full observations as well as partial (short or medium-term) outcomes are combined through a Bayesian filter to obtain a probabilistic belief. Second, we devise a bandit algorithm that takes advantage of this new predictive model. The algorithm quickly learns to identify content aligned with long-term success by carefully balancing exploration and exploitation. We apply our approach to a podcast recommendation problem, where we seek to identify shows that users engage with repeatedly over two months. We empirically validate that our approach results in substantially better performance compared to approaches that either optimize for short-term proxies, or wait for the long-term outcome to be fully realized.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本的简化中文版本：<</SYS>>在线平台上，推荐系统已成为普遍的功能之一。随着用户满意度的提高，推荐系统的目标也变得更加明确。在这种情况下，我们研究一种内容探索任务，我们将其形式化为带延迟奖励的多重武器问题。我们发现，选择学习信号存在显著的负担：等待全部奖励成熔化可能需要几周时间，从而降低学习速度，而且仅仅估计短期代理奖励只能做到部分准确。我们解决这个挑战在两个步骤中：1. 我们开发了一种预测延迟奖励的模型，该模型包括所有到目前为止获得的信息。全部观察结果以及短期（短期或中期）的结果都会在bayesian滤波器中结合，以获得一个概率性信念。2. 我们设计了一种武器算法，该算法利用新的预测模型来快速地识别符合长期成功的内容。该算法会优化探索和利用的平衡，以确保快速地学习到符合长期目标的内容。我们在电台推荐问题中应用了我们的方法，我们寻找用户在两个月内重复听众的电台节目。我们经验证ified that our approach leads to substantially better performance compared to approaches that either optimize for short-term proxies or wait for the long-term outcome to be fully realized.
</details></li>
</ul>
<hr>
<h2 id="TREEMENT-Interpretable-Patient-Trial-Matching-via-Personalized-Dynamic-Tree-Based-Memory-Network"><a href="#TREEMENT-Interpretable-Patient-Trial-Matching-via-Personalized-Dynamic-Tree-Based-Memory-Network" class="headerlink" title="TREEMENT: Interpretable Patient-Trial Matching via Personalized Dynamic Tree-Based Memory Network"></a>TREEMENT: Interpretable Patient-Trial Matching via Personalized Dynamic Tree-Based Memory Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09942">http://arxiv.org/abs/2307.09942</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brandon Theodorou, Cao Xiao, Jimeng Sun<br>for:The paper aims to improve the accuracy and interpretability of patient trial matching in clinical trials using machine learning models.methods:The proposed model, TREEMENT, utilizes hierarchical clinical ontologies and an attentional beam-search query to learn a personalized patient representation and offer a granular level of alignment for improved performance and interpretability.results:TREEMENT outperforms the best baseline by 7% in terms of error reduction in criteria-level matching and achieves state-of-the-art results in its trial-level matching ability. Additionally, the model provides good interpretability, making the results easier to adopt.Here is the result in Simplified Chinese text:for: 这篇论文目标是通过机器学习模型提高临床试验中患者匹配的准确性和可解释性。methods: 提议的模型TREEMENT使用层次医疗ontoлоги和搜索查询来学习个性化患者表示并提供精细的对齐，以提高性能和可解释性。results: TREEMENT比最佳基eline提高7%的error reduction在匹配水平上，并在试验水平上达到了状态的最佳结果。此外，模型还提供了良好的可解释性，使得结果更容易采用。<details>
<summary>Abstract</summary>
Clinical trials are critical for drug development but often suffer from expensive and inefficient patient recruitment. In recent years, machine learning models have been proposed for speeding up patient recruitment via automatically matching patients with clinical trials based on longitudinal patient electronic health records (EHR) data and eligibility criteria of clinical trials. However, they either depend on trial-specific expert rules that cannot expand to other trials or perform matching at a very general level with a black-box model where the lack of interpretability makes the model results difficult to be adopted.   To provide accurate and interpretable patient trial matching, we introduce a personalized dynamic tree-based memory network model named TREEMENT. It utilizes hierarchical clinical ontologies to expand the personalized patient representation learned from sequential EHR data, and then uses an attentional beam-search query learned from eligibility criteria embedding to offer a granular level of alignment for improved performance and interpretability. We evaluated TREEMENT against existing models on real-world datasets and demonstrated that TREEMENT outperforms the best baseline by 7% in terms of error reduction in criteria-level matching and achieves state-of-the-art results in its trial-level matching ability. Furthermore, we also show TREEMENT can offer good interpretability to make the model results easier for adoption.
</details>
<details>
<summary>摘要</summary>
临床试验是药品开发的关键，但经常受到昂贵和不效率的病人招募困扰。在最近几年，机器学习模型被提议用于加速病人招募，通过基于长期电子医疗记录（EHR）数据和临床试验参与条件的自动匹配病人与临床试验。然而，这些模型 Either rely on临床试验专家规则，无法扩展到其他试验，或者使用黑盒模型，其缺乏可读性使得模型结果具有困难采用。为了提供准确和可读的病人试验匹配，我们引入了一种个性化动态树型记忆网络模型 named TREEMENT。它利用层次临床 ontology 扩展个性化病人表示，然后使用静态搜索查询学习自适应搜索来提供精细水平的匹配，以提高性能和可读性。我们对实际数据进行了对比，并证明 TREEMENT 相比最佳基线模型，在 критери产生级别匹配中减少错误率7%，并达到了当前的试验级别匹配能力的国际水平。此外，我们还证明 TREEMENT 可以提供良好的可读性，使得模型结果更易于采用。
</details></li>
</ul>
<hr>
<h2 id="Spuriosity-Didn’t-Kill-the-Classifier-Using-Invariant-Predictions-to-Harness-Spurious-Features"><a href="#Spuriosity-Didn’t-Kill-the-Classifier-Using-Invariant-Predictions-to-Harness-Spurious-Features" class="headerlink" title="Spuriosity Didn’t Kill the Classifier: Using Invariant Predictions to Harness Spurious Features"></a>Spuriosity Didn’t Kill the Classifier: Using Invariant Predictions to Harness Spurious Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09933">http://arxiv.org/abs/2307.09933</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cian Eastwood, Shashank Singh, Andrei Liviu Nicolicioiu, Marin Vlastelica, Julius von Kügelgen, Bernhard Schölkopf</li>
<li>for: 避免因Domain shift而导致的失败，recent works寻找提取具有稳定或不变关系的标签特征。</li>
<li>methods: 我们的主要贡献是提出一种基于pseudo-labels的方法，可以在无标签测试领域中使用不稳定特征提高性能。</li>
<li>results: 我们经过 teorically 和实验 validate 了我们的方法，并证明它可以无需测试领域标签学习一个 asymptotically-optimal 预测器。<details>
<summary>Abstract</summary>
To avoid failures on out-of-distribution data, recent works have sought to extract features that have a stable or invariant relationship with the label across domains, discarding the "spurious" or unstable features whose relationship with the label changes across domains. However, unstable features often carry complementary information about the label that could boost performance if used correctly in the test domain. Our main contribution is to show that it is possible to learn how to use these unstable features in the test domain without labels. In particular, we prove that pseudo-labels based on stable features provide sufficient guidance for doing so, provided that stable and unstable features are conditionally independent given the label. Based on this theoretical insight, we propose Stable Feature Boosting (SFB), an algorithm for: (i) learning a predictor that separates stable and conditionally-independent unstable features; and (ii) using the stable-feature predictions to adapt the unstable-feature predictions in the test domain. Theoretically, we prove that SFB can learn an asymptotically-optimal predictor without test-domain labels. Empirically, we demonstrate the effectiveness of SFB on real and synthetic data.
</details>
<details>
<summary>摘要</summary>
为了避免对不同频谱数据的失败，现代工作强调提取具有稳定或不变的关系于标签的特征，抛弃“费劳恐怖”或不稳定的特征，这些特征的关系于标签在不同频谱上发生变化。然而，不稳定的特征经常携带标签相关的补做信息，如果正确使用，可能会提高性能。我们的主要贡献在于显示可以在测试频谱上使用这些不稳定特征，无需标签。具体来说，我们证明可以基于稳定特征生成 pseudo-标签，并使用这些 pseudo-标签 来适应不稳定特征的预测。根据这个理论认识，我们提出了稳定特征增强（SFB）算法，它包括：（i）学习分离稳定和条件独立的不稳定特征；（ii）使用稳定特征预测来适应不稳定特征的预测。理论上，我们证明 SFB 可以在无需测试频谱标签的情况下学习极限优化的预测器。实际上，我们在实际和模拟数据上证明了 SFB 的效果。
</details></li>
</ul>
<hr>
<h2 id="DISA-DIfferentiable-Similarity-Approximation-for-Universal-Multimodal-Registration"><a href="#DISA-DIfferentiable-Similarity-Approximation-for-Universal-Multimodal-Registration" class="headerlink" title="DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration"></a>DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09931">http://arxiv.org/abs/2307.09931</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/imfusiongmbh/disa-universal-multimodal-registration">https://github.com/imfusiongmbh/disa-universal-multimodal-registration</a></li>
<li>paper_authors: Matteo Ronchetti, Wolfgang Wein, Nassir Navab, Oliver Zettinig, Raphael Prevost</li>
<li>for: 用于 Multimodal image registration 的 challenging 问题，以便实现许多图像引导程序中的匹配。</li>
<li>methods: 我们提出了一种基于 Machine Learning 的框架，用于创建可表达性的跨Modal 描述符，以便快速进行可变形 global registration。我们通过将现有的 similarity metric  aproximated 为一个点积分在小型 convolutional neural network (CNN) 的特征空间中，使得可以在不需要注册数据的情况下进行训练。</li>
<li>results: 我们的方法比 local patch-based metrics 快速多个数量级，可以直接应用于临床 setting，只需要将 similarity measure 替换为我们的方法。我们的实验在三个不同的 dataset 上表明，我们的方法可以很好地泛化到新的 setting，无需特殊的重新训练。我们将我们的训练代码和数据公开发布。<details>
<summary>Abstract</summary>
Multimodal image registration is a challenging but essential step for numerous image-guided procedures. Most registration algorithms rely on the computation of complex, frequently non-differentiable similarity metrics to deal with the appearance discrepancy of anatomical structures between imaging modalities. Recent Machine Learning based approaches are limited to specific anatomy-modality combinations and do not generalize to new settings. We propose a generic framework for creating expressive cross-modal descriptors that enable fast deformable global registration. We achieve this by approximating existing metrics with a dot-product in the feature space of a small convolutional neural network (CNN) which is inherently differentiable can be trained without registered data. Our method is several orders of magnitude faster than local patch-based metrics and can be directly applied in clinical settings by replacing the similarity measure with the proposed one. Experiments on three different datasets demonstrate that our approach generalizes well beyond the training data, yielding a broad capture range even on unseen anatomies and modality pairs, without the need for specialized retraining. We make our training code and data publicly available.
</details>
<details>
<summary>摘要</summary>
多modal图像注册是一个挑战性的、但是必要的步骤，用于访问多种图像访问领域中的图像注册。大多数注册算法依赖computational complex, frequently non-differentiable similarity metrics来处理不同图像模式之间的体结构出现的差异。近期的机器学习基于的方法仅对特定的体部-图像组合有限制，并且不能扩展到新的设定。我们提出了一个通用的框架，用于创建expressive cross-modal描述子，以实现快速的扭转全局注册。我们实现了这一点通过简化现有的度量，使其变成一个内置在小型卷积神经网（CNN）中的dot产品，这个度量是可微的且可以在训练过程中被训练。我们的方法比Local patch-based度量更快速，并且可以直接在供应链中应用，只需替换度量即可。我们在三个不同的数据集上进行了实验，结果显示了我们的方法可以广泛地适用于新的体部-图像组合，而且不需要特别的重训。我们将我们的训练代码和数据公开。
</details></li>
</ul>
<hr>
<h2 id="TimeTuner-Diagnosing-Time-Representations-for-Time-Series-Forecasting-with-Counterfactual-Explanations"><a href="#TimeTuner-Diagnosing-Time-Representations-for-Time-Series-Forecasting-with-Counterfactual-Explanations" class="headerlink" title="TimeTuner: Diagnosing Time Representations for Time-Series Forecasting with Counterfactual Explanations"></a>TimeTuner: Diagnosing Time Representations for Time-Series Forecasting with Counterfactual Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09916">http://arxiv.org/abs/2307.09916</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/catherinehao/timetuner">https://github.com/catherinehao/timetuner</a></li>
<li>paper_authors: Jianing Hao, Qing Shi, Yilin Ye, Wei Zeng</li>
<li>for: 本文旨在提供一种可视化分析框架，帮助分析者理解时序序列表示如何与模型预测相关。</li>
<li>methods: 本文使用两个阶段技术：首先，利用counterfactual解释连接时序序列表示、多变量特征和模型预测之间的关系。其次，设计多种协调视图，包括分割基于相关性的分布图和嵌入的双向条带，并提供了用户可以进行变换选择过程、浏览特征空间和评估模型性能的交互。</li>
<li>results: 基于实际时序序预测数据， authors demonstrates the applicability of TimeTuner 框架在单variate 太阳黑点和多variate 空气污染的预测中。feedback from domain experts表明，TimeTuner 可以帮助描述时序序列表示和导向特征工程过程。<details>
<summary>Abstract</summary>
Deep learning (DL) approaches are being increasingly used for time-series forecasting, with many efforts devoted to designing complex DL models. Recent studies have shown that the DL success is often attributed to effective data representations, fostering the fields of feature engineering and representation learning. However, automated approaches for feature learning are typically limited with respect to incorporating prior knowledge, identifying interactions among variables, and choosing evaluation metrics to ensure that the models are reliable. To improve on these limitations, this paper contributes a novel visual analytics framework, namely TimeTuner, designed to help analysts understand how model behaviors are associated with localized correlations, stationarity, and granularity of time-series representations. The system mainly consists of the following two-stage technique: We first leverage counterfactual explanations to connect the relationships among time-series representations, multivariate features and model predictions. Next, we design multiple coordinated views including a partition-based correlation matrix and juxtaposed bivariate stripes, and provide a set of interactions that allow users to step into the transformation selection process, navigate through the feature space, and reason the model performance. We instantiate TimeTuner with two transformation methods of smoothing and sampling, and demonstrate its applicability on real-world time-series forecasting of univariate sunspots and multivariate air pollutants. Feedback from domain experts indicates that our system can help characterize time-series representations and guide the feature engineering processes.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）方法在时间序列预测中越来越广泛应用，许多努力投入于设计复杂的DL模型。 latest studies have shown that DL success often attributed to effective data representations, which has led to the development of fields of feature engineering and representation learning. However, automated approaches for feature learning are typically limited in terms of incorporating prior knowledge, identifying interactions among variables, and choosing evaluation metrics to ensure the models are reliable. To improve on these limitations, this paper proposes a novel visual analytics framework, namely TimeTuner, designed to help analysts understand how model behaviors are associated with localized correlations, stationarity, and granularity of time-series representations. The system consists of the following two-stage technique:First, we leverage counterfactual explanations to connect the relationships among time-series representations, multivariate features, and model predictions. Next, we design multiple coordinated views, including a partition-based correlation matrix and juxtaposed bivariate stripes, and provide a set of interactions that allow users to step into the transformation selection process, navigate through the feature space, and reason the model performance.We instantiate TimeTuner with two transformation methods of smoothing and sampling, and demonstrate its applicability on real-world time-series forecasting of univariate sunspots and multivariate air pollutants. Feedback from domain experts indicates that our system can help characterize time-series representations and guide the feature engineering processes.
</details></li>
</ul>
<hr>
<h2 id="Deep-projection-networks-for-learning-time-homogeneous-dynamical-systems"><a href="#Deep-projection-networks-for-learning-time-homogeneous-dynamical-systems" class="headerlink" title="Deep projection networks for learning time-homogeneous dynamical systems"></a>Deep projection networks for learning time-homogeneous dynamical systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09912">http://arxiv.org/abs/2307.09912</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vladimir R. Kostic, Pietro Novelli, Riccardo Grazzi, Karim Lounici, Massimiliano Pontil</li>
<li>for: 学习时间Homogeneous动力系统的有用表示。</li>
<li>methods: 使用一种基于神经网络的方法，通过优化一个对象函数来学习一个转移运算符。</li>
<li>results: 提出了一种稳定且可靠的方法，可以应用于具有挑战性的场景。并且可以提高前期方法的性能。<details>
<summary>Abstract</summary>
We consider the general class of time-homogeneous dynamical systems, both discrete and continuous, and study the problem of learning a meaningful representation of the state from observed data. This is instrumental for the task of learning a forward transfer operator of the system, that in turn can be used for forecasting future states or observables. The representation, typically parametrized via a neural network, is associated with a projection operator and is learned by optimizing an objective function akin to that of canonical correlation analysis (CCA). However, unlike CCA, our objective avoids matrix inversions and therefore is generally more stable and applicable to challenging scenarios. Our objective is a tight relaxation of CCA and we further enhance it by proposing two regularization schemes, one encouraging the orthogonality of the components of the representation while the other exploiting Chapman-Kolmogorov's equation. We apply our method to challenging discrete dynamical systems, discussing improvements over previous methods, as well as to continuous dynamical systems.
</details>
<details>
<summary>摘要</summary>
我团队考虑了时间homogeneous的动力系统，包括离散和连续两种类型，并研究了从观测数据中学习一个有意义的状态表示。这个表示通常通过神经网络进行参数化，与投影运算器相关，并通过优化一个类似于 canonical correlation analysis（CCA）的目标函数来学习。不同于CCA，我们的目标函数不需要矩阵反转，因此更加稳定和适用于复杂的场景。我们的目标函数是CCA的紧张relaxation，并且我们提出了两种正则化方案，一个促进表示Components的正交性，另一个利用 Chapman-Kolmogorov 方程。我们应用我们的方法于复杂的离散动力系统和连续动力系统，讨论了以前方法的改进和性能提升。
</details></li>
</ul>
<hr>
<h2 id="Repeated-Observations-for-Classification"><a href="#Repeated-Observations-for-Classification" class="headerlink" title="Repeated Observations for Classification"></a>Repeated Observations for Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09896">http://arxiv.org/abs/2307.09896</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hüseyin Afşer, László Györfi, Harro Walk</li>
<li>for: 本研究讲卷非 Parametric 分类问题，具体来说是在重复观测 $\bV_1,\dots ,\bV_t $ 中进行分类。</li>
<li>methods: 本文提出了一些简单的分类规则，其中 conditional error probabilities 的渐进速度为 exponential。</li>
<li>results: 本文分析了一些特定的模型，如 robust detection by nominal densities、prototype classification、linear transformation、linear classification、scaling。<details>
<summary>Abstract</summary>
We study the problem nonparametric classification with repeated observations. Let $\bX$ be the $d$ dimensional feature vector and let $Y$ denote the label taking values in $\{1,\dots ,M\}$. In contrast to usual setup with large sample size $n$ and relatively low dimension $d$, this paper deals with the situation, when instead of observing a single feature vector $\bX$ we are given $t$ repeated feature vectors $\bV_1,\dots ,\bV_t $. Some simple classification rules are presented such that the conditional error probabilities have exponential convergence rate of convergence as $t\to\infty$. In the analysis, we investigate particular models like robust detection by nominal densities, prototype classification, linear transformation, linear classification, scaling.
</details>
<details>
<summary>摘要</summary>
我们研究非参数化分类问题，带有重复观测。假设 $\bX$ 是 $d$ 维特征向量，$Y$ 表示标签，取值在 $\{1,\ldots,M\}$ 中。相比于通常情况，我们在大样本大小 $n$ 和低维度 $d$ 的情况下进行研究，这篇论文则考虑了在多个特征向量 $\bV_1,\ldots,\bV_t$ 的情况下，并提出了一些简单的分类规则，其 conditional error probability 的渐进速度为 $t\to\infty$ 的几何速度。在分析中，我们研究了特定的模型，如 robust detection by nominal densities、prototype classification、linear transformation、linear classification、scaling。
</details></li>
</ul>
<hr>
<h2 id="Symmetric-Equilibrium-Learning-of-VAEs"><a href="#Symmetric-Equilibrium-Learning-of-VAEs" class="headerlink" title="Symmetric Equilibrium Learning of VAEs"></a>Symmetric Equilibrium Learning of VAEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09883">http://arxiv.org/abs/2307.09883</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boris Flach, Dmitrij Schlesinger, Alexander Shekhovtsov</li>
<li>for: 该研究旨在为缺乏准确模型的情况下，提高变量自动编码器（VAE）的学习方法。</li>
<li>methods: 该方法使用了尼什均衡学习方法，将数据空间和隐藏空间之间的分布映射到另一个空间中，并使用随机抽取来获取数据和隐藏变量的访问。</li>
<li>results: 实验表明，该方法可以learned comparable models with ELBO learning,并且可以应用于一些不可能通过标准VAE学习的任务。<details>
<summary>Abstract</summary>
We view variational autoencoders (VAE) as decoder-encoder pairs, which map distributions in the data space to distributions in the latent space and vice versa. The standard learning approach for VAEs, i.e. maximisation of the evidence lower bound (ELBO), has an obvious asymmetry in that respect. Moreover, it requires a closed form a-priori latent distribution. This limits the applicability of VAEs in more complex scenarios, such as general semi-supervised learning and employing complex generative models as priors. We propose a Nash equilibrium learning approach that relaxes these restrictions and allows learning VAEs in situations where both the data and the latent distributions are accessible only by sampling. The flexibility and simplicity of this approach allows its application to a wide range of learning scenarios and downstream tasks. We show experimentally that the models learned by this method are comparable to those obtained by ELBO learning and demonstrate its applicability for tasks that are not accessible by standard VAE learning.
</details>
<details>
<summary>摘要</summary>
我们视variational autoencoders（VAE）为编码器-解码器对，它们将数据空间中的分布映射到 latent space 中的分布，并将其反转。标准的 VAE 学习方法，即最大化证据下界（ELBO）的最大化，具有明显的偏好性，即需要预先知道 latent distribution 的closed form。这限制了 VAE 在更复杂的场景下的应用，如总是 semi-supervised learning 和使用复杂的生成模型为先验。我们提议一种 Nash 平衡学习方法，它可以放弃这些限制，允许在数据和 latent 分布可以通过采样获取时学习 VAE。这种灵活性和简洁性使其适用于各种学习场景和下游任务。我们通过实验表明，与 ELBO 学习相比，这种方法学习的模型相似，并且在不可用标准 VAE 学习的任务上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Likelihood-Estimation-with-One-way-Flows"><a href="#Adversarial-Likelihood-Estimation-with-One-way-Flows" class="headerlink" title="Adversarial Likelihood Estimation with One-way Flows"></a>Adversarial Likelihood Estimation with One-way Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09882">http://arxiv.org/abs/2307.09882</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omri Ben-Dov, Pravir Singh Gupta, Victoria Abrevaya, Michael J. Black, Partha Ghosh</li>
<li>for: 这个论文的目的是提出一种新的生成模型，以提高生成模型的质量和可靠性。</li>
<li>methods: 这个论文使用了生成对抗网络（GANs），并提出了一种新的方法，即使用能量基分布来最大化生成器的概率密度。这种方法可以提供更好的模式覆盖率和生成器的可靠性。</li>
<li>results: 实验结果表明，这种新的生成模型可以比传统的GANs快速 converges，并且可以生成高质量的样本，同时也可以避免过拟合常用的数据集。此外，这种模型还可以生成了维度低的积分表示。<details>
<summary>Abstract</summary>
Generative Adversarial Networks (GANs) can produce high-quality samples, but do not provide an estimate of the probability density around the samples. However, it has been noted that maximizing the log-likelihood within an energy-based setting can lead to an adversarial framework where the discriminator provides unnormalized density (often called energy). We further develop this perspective, incorporate importance sampling, and show that 1) Wasserstein GAN performs a biased estimate of the partition function, and we propose instead to use an unbiased estimator; 2) when optimizing for likelihood, one must maximize generator entropy. This is hypothesized to provide a better mode coverage. Different from previous works, we explicitly compute the density of the generated samples. This is the key enabler to designing an unbiased estimator of the partition function and computation of the generator entropy term. The generator density is obtained via a new type of flow network, called one-way flow network, that is less constrained in terms of architecture, as it does not require to have a tractable inverse function. Our experimental results show that we converge faster, produce comparable sample quality to GANs with similar architecture, successfully avoid over-fitting to commonly used datasets and produce smooth low-dimensional latent representations of the training data.
</details>
<details>
<summary>摘要</summary>
生成对抗网络（GAN）可以生成高质量样本，但不提供样本周围的概率密度估计。然而，有观点认为，在能量基础设定下，最大化律引函数可以导致对抗框架，其中对抗器提供了不归一化密度（常称为能量）。我们进一步发展这一观点，包括重要抽样，并表明：1）沃氏距离GAN实际上计算了偏置函数，我们提议使用不偏置的估计器；2）在优化概率时，需要最大化生成器的熵。这被假设为提供更好的模式覆盖率。与前一些工作不同，我们显式计算生成样本的密度。这是关键启用设计不偏置估计器和计算生成器熵项的能力。生成器密度通过一种新的流网络，称为一向流网络，可以更加自由地设计架构，不需要具有可导函数。我们的实验结果表明，我们可以更快 converges，生成高质量样本，并成功避免过拟合常用的数据集，生成平滑低维度的训练数据表示。
</details></li>
</ul>
<hr>
<h2 id="Detecting-Vulnerable-Nodes-in-Urban-Infrastructure-Interdependent-Network"><a href="#Detecting-Vulnerable-Nodes-in-Urban-Infrastructure-Interdependent-Network" class="headerlink" title="Detecting Vulnerable Nodes in Urban Infrastructure Interdependent Network"></a>Detecting Vulnerable Nodes in Urban Infrastructure Interdependent Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09866">http://arxiv.org/abs/2307.09866</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tsinghua-fib-lab/kdd2023-id546-urbaninfra">https://github.com/tsinghua-fib-lab/kdd2023-id546-urbaninfra</a></li>
<li>paper_authors: Jinzhu Mao, Liu Cao, Chen Gao, Huandong Wang, Hangyu Fan, Depeng Jin, Yong Li</li>
<li>for: 这个论文的目的是理解和 caracterizing 城市基础设施的投入性弱点，以便保护敏感设施和设计强健的 topological structure。</li>
<li>methods: 该论文使用了图神经网络和强化学习来模型城市系统中的互相关联关系，并通过实际数据训练来准确地评估城市系统的投入性弱点。</li>
<li>results: 该论文的实验结果表明，该系统可以准确地捕捉城市系统中的风险层次和投入性弱点，并且可以在不同的请求下进行跨越训练和转移学习。<details>
<summary>Abstract</summary>
Understanding and characterizing the vulnerability of urban infrastructures, which refers to the engineering facilities essential for the regular running of cities and that exist naturally in the form of networks, is of great value to us. Potential applications include protecting fragile facilities and designing robust topologies, etc. Due to the strong correlation between different topological characteristics and infrastructure vulnerability and their complicated evolution mechanisms, some heuristic and machine-assisted analysis fall short in addressing such a scenario. In this paper, we model the interdependent network as a heterogeneous graph and propose a system based on graph neural network with reinforcement learning, which can be trained on real-world data, to characterize the vulnerability of the city system accurately. The presented system leverages deep learning techniques to understand and analyze the heterogeneous graph, which enables us to capture the risk of cascade failure and discover vulnerable infrastructures of cities. Extensive experiments with various requests demonstrate not only the expressive power of our system but also transferring ability and necessity of the specific components.
</details>
<details>
<summary>摘要</summary>
理解和Characterizing城市基础设施的敏感性具有很大的价值。这种基础设施包括城市的工程设施，这些设施存在自然形成的网络结构，并且具有各种不同的特性。具体来说，我们可以通过保护脆弱的设施和设计Robust的topology等方式来应用这种理解和Characterizing技术。由于城市基础设施的特征相互 correlated，以及其复杂的演化机制，一些顺序和机器学习的分析方法无法正确地处理这种情况。为此，我们在这篇论文中提出了一种基于图 neural network with reinforcement learning的方法，可以在实际数据上训练，以准确地Characterizing城市系统的敏感性。这种方法利用深度学习技术来理解和分析异质图，使得我们能够捕捉城市系统的风险和潜在的脆弱性。我们在实际中进行了多种请求的实验，并证明了我们的系统不仅具有表达力，还能够转移和应用特定的组件。
</details></li>
</ul>
<hr>
<h2 id="Towards-a-population-informed-approach-to-the-definition-of-data-driven-models-for-structural-dynamics"><a href="#Towards-a-population-informed-approach-to-the-definition-of-data-driven-models-for-structural-dynamics" class="headerlink" title="Towards a population-informed approach to the definition of data-driven models for structural dynamics"></a>Towards a population-informed approach to the definition of data-driven models for structural dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09862">http://arxiv.org/abs/2307.09862</a></li>
<li>repo_url: None</li>
<li>paper_authors: G. Tsialiamanis, N. Dervilis, D. J. Wagg, K. Worden</li>
<li>for: 这个研究的目的是激励使用基于物理学的模型，以学习相似物理下的现象关系。</li>
<li>methods: 这个研究使用了两种meta-学习算法：模型独立meta-学习（MAML）算法和条件神经过程（CNP）模型。</li>
<li>results: 这两种算法都能够如意地approxime quantities of interest，并且表现与传统机器学习算法类似，即与可用的结构数量有关。<details>
<summary>Abstract</summary>
Machine learning has affected the way in which many phenomena for various domains are modelled, one of these domains being that of structural dynamics. However, because machine-learning algorithms are problem-specific, they often fail to perform efficiently in cases of data scarcity. To deal with such issues, combination of physics-based approaches and machine learning algorithms have been developed. Although such methods are effective, they also require the analyser's understanding of the underlying physics of the problem. The current work is aimed at motivating the use of models which learn such relationships from a population of phenomena, whose underlying physics are similar. The development of such models is motivated by the way that physics-based models, and more specifically finite element models, work. Such models are considered transferrable, explainable and trustworthy, attributes which are not trivially imposed or achieved for machine-learning models. For this reason, machine-learning approaches are less trusted by industry and often considered more difficult to form validated models. To achieve such data-driven models, a population-based scheme is followed here and two different machine-learning algorithms from the meta-learning domain are used. The two algorithms are the model-agnostic meta-learning (MAML) algorithm and the conditional neural processes (CNP) model. The algorithms seem to perform as intended and outperform a traditional machine-learning algorithm at approximating the quantities of interest. Moreover, they exhibit behaviour similar to traditional machine learning algorithms (e.g. neural networks or Gaussian processes), concerning their performance as a function of the available structures in the training population.
</details>
<details>
<summary>摘要</summary>
机器学习对许多领域的现象模型ing有所影响，其中一个领域是结构动力学。然而，由于机器学习算法是问题特定的，因此在数据稀缺时可能不具有高效性。为解决这些问题，物理学习和机器学习算法的组合被开发出来。虽然这些方法有效，但它们也需要分析者对问题的physics的理解。目前的工作是鼓励使用基于人口的现象模型，其中的physics是相似的。这种模型被考虑为可传递、可解释和可信任的，这些特性不是很容易实现或掌握的。因此，机器学习方法在业务中被视为更难以验证和更不可靠。为实现这些数据驱动模型，在这里采用了人口学习的方式，并使用了来自多元学习领域的两种机器学习算法：模型独立多学习（MAML）算法和条件神经过程（CNP）模型。这两种算法似乎按计划进行，并在可用的结构数量上表现出类似的行为，与传统机器学习算法（如神经网络或高斯过程）的性能相似。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Learning-for-Credit-Index-Option-Hedging"><a href="#Reinforcement-Learning-for-Credit-Index-Option-Hedging" class="headerlink" title="Reinforcement Learning for Credit Index Option Hedging"></a>Reinforcement Learning for Credit Index Option Hedging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09844">http://arxiv.org/abs/2307.09844</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Mandelli, Marco Pinciroli, Michele Trapletti, Edoardo Vittori</li>
<li>for: 本研究旨在通过强化学习方法找到债务指数选项最佳保险策略。</li>
<li>methods: 我们采用实用的方法，强调实际性，即逐步时间、交易成本等因素。我们应用了现代算法TRVO算法，并证明 derivated 保险策略超越了实践中的黑ilder&amp;奔矢 delta 保险策略。</li>
<li>results: 我们的研究表明，使用TRVO算法可以找到更好的保险策略，并且在实际市场数据上测试了我们的政策。<details>
<summary>Abstract</summary>
In this paper, we focus on finding the optimal hedging strategy of a credit index option using reinforcement learning. We take a practical approach, where the focus is on realism i.e. discrete time, transaction costs; even testing our policy on real market data. We apply a state of the art algorithm, the Trust Region Volatility Optimization (TRVO) algorithm and show that the derived hedging strategy outperforms the practitioner's Black & Scholes delta hedge.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们关注适用再强化学习来找到债务指数选项最佳减抵策略。我们采取了一种实用的方法，即注重实际情况，包括精确的时间预测、交易成本等，并对实际市场数据进行测试。我们使用了现代算法，即信任区域波动优化（TRVO）算法，并证明 derivated的减抵策略超过了实践中的布拉克-舒勒斯 delta 减抵策。
</details></li>
</ul>
<hr>
<h2 id="Near-Linear-Time-Projection-onto-the-ell-1-infty-Ball-Application-to-Sparse-Autoencoders"><a href="#Near-Linear-Time-Projection-onto-the-ell-1-infty-Ball-Application-to-Sparse-Autoencoders" class="headerlink" title="Near-Linear Time Projection onto the $\ell_{1,\infty}$ Ball; Application to Sparse Autoencoders"></a>Near-Linear Time Projection onto the $\ell_{1,\infty}$ Ball; Application to Sparse Autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09836">http://arxiv.org/abs/2307.09836</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/memo-p/projection">https://github.com/memo-p/projection</a></li>
<li>paper_authors: Guillaume Perez, Laurent Condat, Michel Barlaud</li>
<li>for: 快速训练大规模神经网络的简化。</li>
<li>methods: 使用 $\ell_{1,2}$ 和 $\ell_{1,\infty}$ 投影技术来简化和减少神经网络的总成本。</li>
<li>results: 提出了一种新的 $\ell_{1,\infty}$  нор球投影算法，其最坏时间复杂度为 $\mathcal{O}\big(nm+J\log(nm)\big)$，其中 $J$ 是一个减少到 0 的隐式常数，当精度高时，它减少到 $nm$。此外，提议在自动编码器训练中加入 $\ell_{1,\infty}$ 球投影以实现特征选择和神经网络精炼。在生物应用中，只有非常小的一部分数据 ($&lt;2%$) 是有关的，因此在编码器中进行简化。我们示出了我们的方法在生物 caso 和一般精炼 caso 中都是最快的。<details>
<summary>Abstract</summary>
Looking for sparsity is nowadays crucial to speed up the training of large-scale neural networks. Projections onto the $\ell_{1,2}$ and $\ell_{1,\infty}$ are among the most efficient techniques to sparsify and reduce the overall cost of neural networks. In this paper, we introduce a new projection algorithm for the $\ell_{1,\infty}$ norm ball. The worst-case time complexity of this algorithm is $\mathcal{O}\big(nm+J\log(nm)\big)$ for a matrix in $\mathbb{R}^{n\times m}$. $J$ is a term that tends to 0 when the sparsity is high, and to $nm$ when the sparsity is low. Its implementation is easy and it is guaranteed to converge to the exact solution in a finite time. Moreover, we propose to incorporate the $\ell_{1,\infty}$ ball projection while training an autoencoder to enforce feature selection and sparsity of the weights. Sparsification appears in the encoder to primarily do feature selection due to our application in biology, where only a very small part ($<2\%$) of the data is relevant. We show that both in the biological case and in the general case of sparsity that our method is the fastest.
</details>
<details>
<summary>摘要</summary>
现在，检测稀畴性已经成为训练大规模神经网络的关键。Project onto the $\ell_{1,2}$ and $\ell_{1,\infty}$ 是最高效的技术来稀畴化和减少神经网络的总成本。在这篇论文中，我们介绍了一种新的 projection algorithm for the $\ell_{1,\infty}$  нор球。这个算法的最坏情况时间复杂度是 $\mathcal{O}\big(nm+J\log(nm)\big)$，其中 $J$ 是一个很小的常数，当稀畴性高时，它会逐渐递减至 $nm$，而当稀畴性低时，它会保持不变。它的实现非常简单，并且可以在有限时间内 converges to the exact solution。此外，我们还提议在训练 autoencoder 时添加 $\ell_{1,\infty}$ 球 projection，以便在权重中实现特征选择和稀畴化。在生物应用中，只有非常小的一部分（ Less than 2%）的数据是有关的，因此在encoder中进行稀畴化。我们表明，在生物和通用稀畴情况下，我们的方法是最快的。
</details></li>
</ul>
<hr>
<h2 id="Deep-Operator-Network-Approximation-Rates-for-Lipschitz-Operators"><a href="#Deep-Operator-Network-Approximation-Rates-for-Lipschitz-Operators" class="headerlink" title="Deep Operator Network Approximation Rates for Lipschitz Operators"></a>Deep Operator Network Approximation Rates for Lipschitz Operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09835">http://arxiv.org/abs/2307.09835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christoph Schwab, Andreas Stein, Jakob Zech</li>
<li>for: 这个论文是用于研究深度运算网络（Deep Operator Networks，简称DON）如何用于模拟 lipschitz（或 holder）连续函数 $\mathcal G:\mathcal X\to\mathcal Y$ 之间的交互。</li>
<li>methods: 这个论文使用了 linear encoders $\mathcal E$ 和 decoders $\mathcal D$，以及一个参数化坐标映射网络来模拟 $\mathcal G$。</li>
<li>results: 这个论文得到了 $\mathcal G$ 的表达率下界，不需要 $\mathcal G$ 是幂极连续的假设。<details>
<summary>Abstract</summary>
We establish universality and expression rate bounds for a class of neural Deep Operator Networks (DON) emulating Lipschitz (or H\"older) continuous maps $\mathcal G:\mathcal X\to\mathcal Y$ between (subsets of) separable Hilbert spaces $\mathcal X$, $\mathcal Y$. The DON architecture considered uses linear encoders $\mathcal E$ and decoders $\mathcal D$ via (biorthogonal) Riesz bases of $\mathcal X$, $\mathcal Y$, and an approximator network of an infinite-dimensional, parametric coordinate map that is Lipschitz continuous on the sequence space $\ell^2(\mathbb N)$. Unlike previous works ([Herrmann, Schwab and Zech: Neural and Spectral operator surrogates: construction and expression rate bounds, SAM Report, 2022], [Marcati and Schwab: Exponential Convergence of Deep Operator Networks for Elliptic Partial Differential Equations, SAM Report, 2022]), which required for example $\mathcal G$ to be holomorphic, the present expression rate results require mere Lipschitz (or H\"older) continuity of $\mathcal G$. Key in the proof of the present expression rate bounds is the use of either super-expressive activations (e.g. [Yarotski: Elementary superexpressive activations, Int. Conf. on ML, 2021], [Shen, Yang and Zhang: Neural network approximation: Three hidden layers are enough, Neural Networks, 2021], and the references there) which are inspired by the Kolmogorov superposition theorem, or of nonstandard NN architectures with standard (ReLU) activations as recently proposed in [Zhang, Shen and Yang: Neural Network Architecture Beyond Width and Depth, Adv. in Neural Inf. Proc. Sys., 2022]. We illustrate the abstract results by approximation rate bounds for emulation of a) solution operators for parametric elliptic variational inequalities, and b) Lipschitz maps of Hilbert-Schmidt operators.
</details>
<details>
<summary>摘要</summary>
我们设定了一个简单的 Deep Operator Network (DON) 架构，用于模拟 Lipschitz （或Holder）连续的映射 $\mathcal G:\mathcal X\to\mathcal Y$  zwischen（subsets of）separable Hilbert spaces $\mathcal X$, $\mathcal Y$。DON 架构使用线性映射 $\mathcal E$ 和 $\mathcal D$ via（biorthogonal）Riesz bases of $\mathcal X$, $\mathcal Y$，并且使用一个具有无穷维度的参数coordinate map的近似器网络。不同于先前的研究（[Herrmann, Schwab and Zech: Neural and Spectral operator surrogates: construction and expression rate bounds, SAM Report, 2022]，[Marcati and Schwab: Exponential Convergence of Deep Operator Networks for Elliptic Partial Differential Equations, SAM Report, 2022]），我们的表达率结果不需要 $\mathcal G$ 是全纯函数。我们的关键证明是使用超对映射（例如 [Yarotski: Elementary superexpressive activations, Int. Conf. on ML, 2021]，[Shen, Yang and Zhang: Neural network approximation: Three hidden layers are enough, Neural Networks, 2021]，和其他文献），或者非标准的NN架构（使用 ReLU 活化），如 recent proposed in [Zhang, Shen and Yang: Neural Network Architecture Beyond Width and Depth, Adv. in Neural Inf. Proc. Sys., 2022]。我们透过实际的例子来详细说明我们的抽象结果，包括对参数elliptic variational inequality的解析器架构和对Hilbert-Schmidt operator的Lipschitz映射。
</details></li>
</ul>
<hr>
<h2 id="What-do-neural-networks-learn-in-image-classification-A-frequency-shortcut-perspective"><a href="#What-do-neural-networks-learn-in-image-classification-A-frequency-shortcut-perspective" class="headerlink" title="What do neural networks learn in image classification? A frequency shortcut perspective"></a>What do neural networks learn in image classification? A frequency shortcut perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09829">http://arxiv.org/abs/2307.09829</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nis-research/nn-frequency-shortcuts">https://github.com/nis-research/nn-frequency-shortcuts</a></li>
<li>paper_authors: Shunxin Wang, Raymond Veldhuis, Christoph Brune, Nicola Strisciuglio</li>
<li>for:  investigate the mechanisms of representation learning in neural networks (NNs) for classification tasks</li>
<li>methods:  experiments on synthetic datasets and natural images, proposal of a metric to measure class-wise frequency characteristics, and a method to identify frequency shortcuts</li>
<li>results:  NNs tend to find simple solutions for classification, and what they learn first during training depends on the most distinctive frequency characteristics; frequency shortcuts can be transferred across datasets and cannot be fully avoided by larger model capacity and data augmentation.<details>
<summary>Abstract</summary>
Frequency analysis is useful for understanding the mechanisms of representation learning in neural networks (NNs). Most research in this area focuses on the learning dynamics of NNs for regression tasks, while little for classification. This study empirically investigates the latter and expands the understanding of frequency shortcuts. First, we perform experiments on synthetic datasets, designed to have a bias in different frequency bands. Our results demonstrate that NNs tend to find simple solutions for classification, and what they learn first during training depends on the most distinctive frequency characteristics, which can be either low- or high-frequencies. Second, we confirm this phenomenon on natural images. We propose a metric to measure class-wise frequency characteristics and a method to identify frequency shortcuts. The results show that frequency shortcuts can be texture-based or shape-based, depending on what best simplifies the objective. Third, we validate the transferability of frequency shortcuts on out-of-distribution (OOD) test sets. Our results suggest that frequency shortcuts can be transferred across datasets and cannot be fully avoided by larger model capacity and data augmentation. We recommend that future research should focus on effective training schemes mitigating frequency shortcut learning.
</details>
<details>
<summary>摘要</summary>
频率分析对神经网络（NN）的表征学arning有用。大多数研究集中在NN的回归任务上进行了学习动力学研究，而对于分类任务的研究则比较少。这个研究通过实验探索类别分类任务中NN的学习机制，并扩展了频率短cut的理解。我们首先在人工生成的Synthetic dataset上进行了实验，这些dataset具有不同频率带的偏见。我们的结果表明，NN在分类任务上倾向于找到简单的解决方案，并且在训练过程中学习的第一个频率特征取决于数据的最明显频率特征，这些特征可以是低频或高频。然后，我们 validate这种现象在自然图像上，并提出了一种类别频率特征的度量和一种频率短cut的标识方法。结果表明，频率短cut可以是Texture-based或Shape-based，取决于最简化目标所需的频率特征。最后，我们证明了频率短cut的转移性，即频率短cut可以在不同的数据集上转移，并且不能完全由更大的模型容量和数据增强来避免。我们建议未来的研究应该关注有效地减少频率短cut的学习。
</details></li>
</ul>
<hr>
<h2 id="Multi-modal-Learning-based-Prediction-for-Disease"><a href="#Multi-modal-Learning-based-Prediction-for-Disease" class="headerlink" title="Multi-modal Learning based Prediction for Disease"></a>Multi-modal Learning based Prediction for Disease</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09823">http://arxiv.org/abs/2307.09823</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/batuhankmkaraman/mlbasedad">https://github.com/batuhankmkaraman/mlbasedad</a></li>
<li>paper_authors: Yaran Chen, Xueyu Chen, Yu Han, Haoran Li, Dongbin Zhao, Jingzhong Li, Xu Wang</li>
<li>for: 预测非アルコール性脂肪性肝病（NAFLD）的精准方法，以避免进一步的肝硬化和cirrhosis。</li>
<li>methods: combines comprehensive clinical dataset（FLDData）和多Modal学习基于NAFLD预测方法（DeepFLD），并使用多Modal输入，包括metadata和facial images，以实现更加精准的诊断。</li>
<li>results: DeepFLD模型在不同的 dataset 上表现出了良好的性能，并且可以使用only facial images as input，这表明了这种方法的可行性和简洁性。<details>
<summary>Abstract</summary>
Non alcoholic fatty liver disease (NAFLD) is the most common cause of chronic liver disease, which can be predicted accurately to prevent advanced fibrosis and cirrhosis. While, a liver biopsy, the gold standard for NAFLD diagnosis, is invasive, expensive, and prone to sampling errors. Therefore, non-invasive studies are extremely promising, yet they are still in their infancy due to the lack of comprehensive research data and intelligent methods for multi-modal data. This paper proposes a NAFLD diagnosis system (DeepFLDDiag) combining a comprehensive clinical dataset (FLDData) and a multi-modal learning based NAFLD prediction method (DeepFLD). The dataset includes over 6000 participants physical examinations, laboratory and imaging studies, extensive questionnaires, and facial images of partial participants, which is comprehensive and valuable for clinical studies. From the dataset, we quantitatively analyze and select clinical metadata that most contribute to NAFLD prediction. Furthermore, the proposed DeepFLD, a deep neural network model designed to predict NAFLD using multi-modal input, including metadata and facial images, outperforms the approach that only uses metadata. Satisfactory performance is also verified on other unseen datasets. Inspiringly, DeepFLD can achieve competitive results using only facial images as input rather than metadata, paving the way for a more robust and simpler non-invasive NAFLD diagnosis.
</details>
<details>
<summary>摘要</summary>
非アルコール性脂肪肝病（NAFLD）は、 Chronic liver disease の最も一般的な原因であり、进行度の评価や cirrhosis の予防を正确に予测することができます。 しかし、liver biopsy は、攻撃的で、高価で、标本の误差による不确実性があります。 そこで、非侵入的な研究は、非常に有望ですが、まだ、复数のモードのデータに対するインテリジェントな方法の欠如により、まだ infant の状态であります。 この论文では、NAFLD の诊断システム (DeepFLDDiag) を提案しています。 このシステムは、 comprehensive clinical dataset (FLDData) と、多modal 学习に基づいた NAFLD 予测法 (DeepFLD) を组み合わせています。 データセットには、6000人以上の参加者の物理调查、医疗所の検查结果、広范な质问naires、および一部の参加者の颜画像が含まれています。 このデータセットから、NAFLD 予测において最も贡献度の高い临床 metadata を量的に分析し、选択します。 さらに、提案された DeepFLD は、多modal 入力、 metadata と颜画像を使用して NAFLD を予测するための深层学习ネットワーク モデルです。 この方法は、 metadata のみを使用したアプローチよりも、优れた性能を示します。 さらに、 DeepFLD は、他の未観なデータセットでも、优れた性能を示しています。 惊くべきは、 DeepFLD は、颜画像のみを入力として使用することで、NAFLD の诊断を可能にしています。 これにより、非侵入的な NAFLD 诊断がより Robust かつ简単になります。
</details></li>
</ul>
<hr>
<h2 id="Deep-unrolling-Shrinkage-Network-for-Dynamic-MR-imaging"><a href="#Deep-unrolling-Shrinkage-Network-for-Dynamic-MR-imaging" class="headerlink" title="Deep unrolling Shrinkage Network for Dynamic MR imaging"></a>Deep unrolling Shrinkage Network for Dynamic MR imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09818">http://arxiv.org/abs/2307.09818</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yhao-z/dus-net">https://github.com/yhao-z/dus-net</a></li>
<li>paper_authors: Yinghao Zhang, Xiaodi Li, Weihang Li, Yue Hu</li>
<li>for: 这篇论文的目的是提出一个新的深度推广网络（DUS-Net），用于优化逐条件 $l_1$ 正规化验证类MR复原模型。</li>
<li>methods: 这篇论文使用了一个新的潜在频率注意力（AST）来学习每个通道的阈值，并将其应用到这些通道上。具体来说，这篇论文提出了一个叫做深度推广推广网络（DUS-Net），用于优化逐条件 $l_1$ 正规化验证类MR复原模型。</li>
<li>results: 实验结果显示，提案的 DUS-Net 可以超过现有的方法的性能。<details>
<summary>Abstract</summary>
Deep unrolling networks that utilize sparsity priors have achieved great success in dynamic magnetic resonance (MR) imaging. The convolutional neural network (CNN) is usually utilized to extract the transformed domain, and then the soft thresholding (ST) operator is applied to the CNN-transformed data to enforce the sparsity priors. However, the ST operator is usually constrained to be the same across all channels of the CNN-transformed data. In this paper, we propose a novel operator, called soft thresholding with channel attention (AST), that learns the threshold for each channel. In particular, we put forward a novel deep unrolling shrinkage network (DUS-Net) by unrolling the alternating direction method of multipliers (ADMM) for optimizing the transformed $l_1$ norm dynamic MR reconstruction model. Experimental results on an open-access dynamic cine MR dataset demonstrate that the proposed DUS-Net outperforms the state-of-the-art methods. The source code is available at \url{https://github.com/yhao-z/DUS-Net}.
</details>
<details>
<summary>摘要</summary>
深度折衣网络可以在动态磁共振成像中取得出色的成果。通常情况下，卷积神经网络（CNN）会被用来提取转换域，然后使用软resholding（ST）运算符来实现稀疏约束。然而，ST运算符通常会被限制为所有通道的CNN转换数据中的同一个常数。在这篇论文中，我们提出了一种新的运算符，即通道注意力软resholding（AST），它可以学习每个通道的阈值。具体来说，我们提出了一种新的深度折衣缩放网络（DUS-Net），通过对归一化方法的多个分量（ADMM）进行深度折衣，来优化转换$l_1$ нор的动态MR重建模型。实验结果表明，提出的DUS-Net在一个公开的动态磁共振MR数据集上表现出色，超过了当前的状态艺方法。源代码可以在 \url{https://github.com/yhao-z/DUS-Net} 中获取。
</details></li>
</ul>
<hr>
<h2 id="Manifold-Learning-with-Sparse-Regularised-Optimal-Transport"><a href="#Manifold-Learning-with-Sparse-Regularised-Optimal-Transport" class="headerlink" title="Manifold Learning with Sparse Regularised Optimal Transport"></a>Manifold Learning with Sparse Regularised Optimal Transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09816">http://arxiv.org/abs/2307.09816</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zsteve/QROT">https://github.com/zsteve/QROT</a></li>
<li>paper_authors: Stephen Zhang, Gilles Mordant, Tetsuya Matsumoto, Geoffrey Schiebinger</li>
<li>for: 本文旨在提出一种替代方法，用于在高维拓扑空间中检测数据的稀疏拓扑结构。</li>
<li>methods: 本文提出了一种使用对称最优运输算法，加上二阶regularization的方法，可以生成一个稀疏和自适应的凝合矩阵，这个矩阵可以被看作是一种泛化的bistochastic核心normalization。</li>
<li>results: 本文证明了这种核函数在维度下降时是一个Laplace-type операátor的逼近，并且在异eskedastic随机噪声下展现了稳定性和高效性。此外，本文还提出了一种高效的计算方法，可以快速计算出这种核函数。<details>
<summary>Abstract</summary>
Manifold learning is a central task in modern statistics and data science. Many datasets (cells, documents, images, molecules) can be represented as point clouds embedded in a high dimensional ambient space, however the degrees of freedom intrinsic to the data are usually far fewer than the number of ambient dimensions. The task of detecting a latent manifold along which the data are embedded is a prerequisite for a wide family of downstream analyses. Real-world datasets are subject to noisy observations and sampling, so that distilling information about the underlying manifold is a major challenge. We propose a method for manifold learning that utilises a symmetric version of optimal transport with a quadratic regularisation that constructs a sparse and adaptive affinity matrix, that can be interpreted as a generalisation of the bistochastic kernel normalisation. We prove that the resulting kernel is consistent with a Laplace-type operator in the continuous limit, establish robustness to heteroskedastic noise and exhibit these results in simulations. We identify a highly efficient computational scheme for computing this optimal transport for discrete data and demonstrate that it outperforms competing methods in a set of examples.
</details>
<details>
<summary>摘要</summary>
《映射学习》是现代统计和数据科学中的一个中心任务。许多数据集（细胞、文档、图像、分子）可以被视为高维 ambient 空间中的点云，但实际上数据中的自由度通常比高维维度更少。探索数据中的秘密映射是一个必要的前提，以便进行广泛的后续分析。实际数据受到噪声观测和采样的影响，因此提取数据下面的潜在映射信息是一个主要挑战。我们提出一种基于对称的最优运输方法，并添加了quadratic regularization，可以生成一个稀疏和适应性的亲和力矩阵，这可以被视为泛化带准kernel normalization。我们证明了这种kernel在绝对连续限制下是一个 Laplace-type 算子，并证明其对于不同类型的噪声具有稳定性。我们还提出了一种高效的计算方案，可以快速计算这种最优运输，并在一些示例中证明其超过竞争方法。
</details></li>
</ul>
<hr>
<h2 id="GenKL-An-Iterative-Framework-for-Resolving-Label-Ambiguity-and-Label-Non-conformity-in-Web-Images-Via-a-New-Generalized-KL-Divergence"><a href="#GenKL-An-Iterative-Framework-for-Resolving-Label-Ambiguity-and-Label-Non-conformity-in-Web-Images-Via-a-New-Generalized-KL-Divergence" class="headerlink" title="GenKL: An Iterative Framework for Resolving Label Ambiguity and Label Non-conformity in Web Images Via a New Generalized KL Divergence"></a>GenKL: An Iterative Framework for Resolving Label Ambiguity and Label Non-conformity in Web Images Via a New Generalized KL Divergence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09810">http://arxiv.org/abs/2307.09810</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/codetopaper/genkl">https://github.com/codetopaper/genkl</a></li>
<li>paper_authors: Xia Huang, Kai Fong Ernest Chong</li>
<li>for: 这个论文是为了解决在网络图像数据集中存在异常（Non-Conforming，NC）实例的问题，包括寻找和重新标注NC实例。</li>
<li>methods: 这个论文使用了一种新的方法，即$(\alpha, \beta)$-泛化KL差分（GenKL），来标识和重新标注NC实例。GenKL方法基于KL差分，但是通过使用参数α和β来扩展KL差分，以便更好地捕捉NC实例的特征。</li>
<li>results: 这个论文的实验结果表明，使用GenKL方法可以更好地标识和重新标注NC实例，从而提高图像分类器的性能。在三个网络图像数据集上（Clothing1M、Food101&#x2F;Food101N和mini WebVision 1.0），这个论文实现了新的状态ixel图像分类器的性能：81.34%、85.73%和78.99%&#x2F;92.54%（top-1&#x2F;top-5）。<details>
<summary>Abstract</summary>
Web image datasets curated online inherently contain ambiguous in-distribution (ID) instances and out-of-distribution (OOD) instances, which we collectively call non-conforming (NC) instances. In many recent approaches for mitigating the negative effects of NC instances, the core implicit assumption is that the NC instances can be found via entropy maximization. For "entropy" to be well-defined, we are interpreting the output prediction vector of an instance as the parameter vector of a multinomial random variable, with respect to some trained model with a softmax output layer. Hence, entropy maximization is based on the idealized assumption that NC instances have predictions that are "almost" uniformly distributed. However, in real-world web image datasets, there are numerous NC instances whose predictions are far from being uniformly distributed. To tackle the limitation of entropy maximization, we propose $(\alpha, \beta)$-generalized KL divergence, $\mathcal{D}_{\text{KL}}^{\alpha, \beta}(p\|q)$, which can be used to identify significantly more NC instances. Theoretical properties of $\mathcal{D}_{\text{KL}}^{\alpha, \beta}(p\|q)$ are proven, and we also show empirically that a simple use of $\mathcal{D}_{\text{KL}}^{\alpha, \beta}(p\|q)$ outperforms all baselines on the NC instance identification task. Building upon $(\alpha,\beta)$-generalized KL divergence, we also introduce a new iterative training framework, GenKL, that identifies and relabels NC instances. When evaluated on three web image datasets, Clothing1M, Food101/Food101N, and mini WebVision 1.0, we achieved new state-of-the-art classification accuracies: $81.34\%$, $85.73\%$ and $78.99\%$/$92.54\%$ (top-1/top-5), respectively.
</details>
<details>
<summary>摘要</summary>
<<SYS>>对于在线绘制的图像集，内置了不符合规范（ID）实例和外部不符合规范（OOD）实例，我们总之叫做非 conforming（NC）实例。在当今许多避免 NC 实例的负面影响的方法中，核心隐含假设是通过熵 maximization 找到 NC 实例。为了使熵定义，我们将输出预测向量视为一个 multinomial 随机变量的参数向量，与一个已经训练过的模型的软准输出层相关。因此，熵 maximization 基于理想化的假设，NC 实例的预测是“几乎” uniform 分布。但在实际的网络图像集中，有很多 NC 实例 whose 预测与这种假设不符。为了解决熵 maximization 的局限性，我们提出了 $\alpha$, $\beta$ 权重化 KL 差分， $\mathcal{D}_{\text{KL}}^{\alpha, \beta}(p\|q)$，可以更好地标识 NC 实例。我们证明了 $\mathcal{D}_{\text{KL}}^{\alpha, \beta}(p\|q)$ 的理论性质，并通过实验表明，使用 $\mathcal{D}_{\text{KL}}^{\alpha, \beta}(p\|q)$ 可以超过所有基eline的 NC 实例标识性能。基于 $\alpha$, $\beta$ 权重化 KL 差分，我们还提出了一种新的迭代训练框架，GenKL，可以标识并重新标注 NC 实例。当我们在 Clothing1M、Food101/Food101N 和 mini WebVision 1.0 三个网络图像集上评估时，我们实现了新的状态纪录级别的分类精度：81.34%，85.73% 和 78.99% / 92.54% (top-1/top-5)，分别。
</details></li>
</ul>
<hr>
<h2 id="Graph-Federated-Learning-Based-on-the-Decentralized-Framework"><a href="#Graph-Federated-Learning-Based-on-the-Decentralized-Framework" class="headerlink" title="Graph Federated Learning Based on the Decentralized Framework"></a>Graph Federated Learning Based on the Decentralized Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09801">http://arxiv.org/abs/2307.09801</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peilin Liu, Yanni Tang, Mingyue Zhang, Wu Chen</li>
<li>for: 提高数据隐私和模型精度的 federated learning 应用</li>
<li>methods: 基于分布式机器学习的 Decentralized 框架，通过对节点间数据相似性进行权重补做，集成 gradients 信息</li>
<li>results: 比较 FedAvg、Fedprox、GCFL 和 GCFL+ 方法，实验结果表明提议方法的表现更佳<details>
<summary>Abstract</summary>
Graph learning has a wide range of applications in many scenarios, which require more need for data privacy. Federated learning is an emerging distributed machine learning approach that leverages data from individual devices or data centers to improve the accuracy and generalization of the model, while also protecting the privacy of user data. Graph-federated learning is mainly based on the classical federated learning framework i.e., the Client-Server framework. However, the Client-Server framework faces problems such as a single point of failure of the central server and poor scalability of network topology. First, we introduce the decentralized framework to graph-federated learning. Second, determine the confidence among nodes based on the similarity of data among nodes, subsequently, the gradient information is then aggregated by linear weighting based on confidence. Finally, the proposed method is compared with FedAvg, Fedprox, GCFL, and GCFL+ to verify the effectiveness of the proposed method. Experiments demonstrate that the proposed method outperforms other methods.
</details>
<details>
<summary>摘要</summary>
《图学学习有广泛的应用场景，需要更加严格的数据隐私。联邦学习是一种在多个设备或数据中心之间分布式机器学习方法，可以提高模型准确性和通用性，同时保护用户数据隐私。基于类传统联邦学习框架，图联邦学习主要面临中央服务器单点失败和网络拓扑粗糙等问题。我们首先介绍了分布式框架到图联邦学习。其次，通过数据之间的相似性确定节点间的信任度，然后通过线性权重平均对Gradient信息进行聚合。最后，我们对提出的方法与FedAvg、Fedprox、GCFL和GCFL+进行比较，以证明提出的方法的有效性。实验表明，提出的方法在其他方法之上表现出色。》Note: The translation is done using a machine translation tool, and may not be perfect or idiomatic.
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Forecasting-with-Coherent-Aggregation"><a href="#Probabilistic-Forecasting-with-Coherent-Aggregation" class="headerlink" title="Probabilistic Forecasting with Coherent Aggregation"></a>Probabilistic Forecasting with Coherent Aggregation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09797">http://arxiv.org/abs/2307.09797</a></li>
<li>repo_url: None</li>
<li>paper_authors: Geoffrey Négiar, Ruijun Ma, O. Nangba Meetei, Mengfei Cao, Michael W. Mahoney</li>
<li>for: 这篇论文的目的是提出一个新的模型，用于生成具有层次结构的概率预测，并且将这些预测与层次结构相互关联。</li>
<li>methods: 这篇论文使用了一个对应分析模型，使用卷积神经网来生成因素模型的参数，以及因素负载和基层分布的参数。这个模型可以跟踪基层分布的变化，并且可以用于估计基层分布的数据。</li>
<li>results: 这篇论文的结果显示，该模型在三个层次预测数据集上得到了 significan 改善，具体来说是在11.8-41.4%之间。这篇论文还分析了模型参数对基层分布和因素数的影响。<details>
<summary>Abstract</summary>
Obtaining accurate probabilistic forecasts while respecting hierarchical information is an important operational challenge in many applications, perhaps most obviously in energy management, supply chain planning, and resource allocation. The basic challenge, especially for multivariate forecasting, is that forecasts are often required to be coherent with respect to the hierarchical structure. In this paper, we propose a new model which leverages a factor model structure to produce coherent forecasts by construction. This is a consequence of a simple (exchangeability) observation: permuting \textit{}base-level series in the hierarchy does not change their aggregates. Our model uses a convolutional neural network to produce parameters for the factors, their loadings and base-level distributions; it produces samples which can be differentiated with respect to the model's parameters; and it can therefore optimize for any sample-based loss function, including the Continuous Ranked Probability Score and quantile losses. We can choose arbitrary continuous distributions for the factor and the base-level distributions. We compare our method to two previous methods which can be optimized end-to-end, while enforcing coherent aggregation. Our model achieves significant improvements: between $11.8-41.4\%$ on three hierarchical forecasting datasets. We also analyze the influence of parameters in our model with respect to base-level distribution and number of factors.
</details>
<details>
<summary>摘要</summary>
取得准确的 probabilistic 预测，同时尊重层次结构是许多应用中的重要操作挑战。例如，能源管理、供应链规划和资源分配等领域。基本挑战是， especially for multivariate forecasting， 预测需要具有层次结构的准确性。在这篇论文中，我们提出了一种新的模型，利用因子模型结构生成具有层次结构的准确预测。这是由于一个简单的观察结论：在层次结构中重新排序基级系列不会改变它们的汇总值。我们的模型使用卷积神经网络生成因子、加载和基级分布的参数，并且可以生成可 differentiable 的采样，因此可以优化任何采样基于损失函数，包括连续排名 probability 分数和量程损失。我们可以选择任何连续分布来描述因子和基级分布。我们与之前两种可以满足端到端优化的方法进行比较，我们的模型在三个层次预测 dataset 上实现了显著提升：11.8-41.4%。我们还分析了我们模型中参数的影响，具体来说是基级分布和因子数量。
</details></li>
</ul>
<hr>
<h2 id="Forecasting-Early-with-Meta-Learning"><a href="#Forecasting-Early-with-Meta-Learning" class="headerlink" title="Forecasting Early with Meta Learning"></a>Forecasting Early with Meta Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09796">http://arxiv.org/abs/2307.09796</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/super-shayan/feml">https://github.com/super-shayan/feml</a></li>
<li>paper_authors: Shayan Jawed, Kiran Madhusudhanan, Vijaya Krishna Yalavarthi, Lars Schmidt-Thieme</li>
<li>for: 这个论文是为了提高时间序列预测的性能而设计的。</li>
<li>methods: 这个论文使用的方法是基于 Meta learning 技术，通过在不同的数据集上学习并将这些知识应用到目标数据集上进行预测。具体来说，这个论文提出了一种基于 Convolutional 嵌入层的 Meta learning 方法，该方法可以在有限的 Historic 观察数据上进行预测。</li>
<li>results: 这个论文的结果表明，使用 Meta learning 技术可以提高时间序列预测的性能，并且可以通过在不同的数据集上学习并将这些知识应用到目标数据集上进行预测。此外，这个论文还提出了一些与 Joint learning、Multi-task learning 和经典预测基准相关的解决方案。<details>
<summary>Abstract</summary>
In the early observation period of a time series, there might be only a few historic observations available to learn a model. However, in cases where an existing prior set of datasets is available, Meta learning methods can be applicable. In this paper, we devise a Meta learning method that exploits samples from additional datasets and learns to augment time series through adversarial learning as an auxiliary task for the target dataset. Our model (FEML), is equipped with a shared Convolutional backbone that learns features for varying length inputs from different datasets and has dataset specific heads to forecast for different output lengths. We show that FEML can meta learn across datasets and by additionally learning on adversarial generated samples as auxiliary samples for the target dataset, it can improve the forecasting performance compared to single task learning, and various solutions adapted from Joint learning, Multi-task learning and classic forecasting baselines.
</details>
<details>
<summary>摘要</summary>
在时间序列初期观察期，可能只有一些历史观察值available to learn a model。但在有存在先前数据集的情况下，元学习方法可能可用。在这篇论文中，我们提出了一种元学习方法，利用额外数据集中的样本进行增强时间序列的学习，并通过对Target dataset的挑战性样本进行 adversarial learning 作为 auxillary task。我们的模型（FEML）具有共享的卷积几何学习特征，可以从不同的数据集中学习不同长度的输入特征，并具有特定数据集的头部来预测不同的输出长度。我们表明，FEML可以在不同数据集之间进行元学习，并通过额外学习 adversarial生成的样本来提高预测性能，并比单任务学习和多任务学习、经典预测基elines。
</details></li>
</ul>
<hr>
<h2 id="From-West-to-East-Who-can-understand-the-music-of-the-others-better"><a href="#From-West-to-East-Who-can-understand-the-music-of-the-others-better" class="headerlink" title="From West to East: Who can understand the music of the others better?"></a>From West to East: Who can understand the music of the others better?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09795">http://arxiv.org/abs/2307.09795</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pxaris/ccml">https://github.com/pxaris/ccml</a></li>
<li>paper_authors: Charilaos Papaioannou, Emmanouil Benetos, Alexandros Potamianos</li>
<li>for: 本研究旨在探讨现代音乐情感Recognition（MIR）领域的深度学习模型是否可以用于不同的音乐文化和风格，以及是否可以建立类似的音乐音频嵌入模型，训练于不同文化或风格的数据。</li>
<li>methods: 本研究使用了转移学习方法，以了解不同音乐文化之间的相似性。研究使用了两个西方音乐数据集，两个来自东地中海文化的传统&#x2F;民谱数据集，以及两个印度古典音乐数据集。三个深度音频嵌入模型，包括两个CNN结构和一个Transformer结构，在不同的目标频道上进行了自动标注。</li>
<li>results: 实验结果显示，通过转移学习，在所有频道上达到了竞争性的表现，而最佳来源数据集则随着不同的音乐文化而变化。实现和训练过的模型都公开提供在公共存储库中。<details>
<summary>Abstract</summary>
Recent developments in MIR have led to several benchmark deep learning models whose embeddings can be used for a variety of downstream tasks. At the same time, the vast majority of these models have been trained on Western pop/rock music and related styles. This leads to research questions on whether these models can be used to learn representations for different music cultures and styles, or whether we can build similar music audio embedding models trained on data from different cultures or styles. To that end, we leverage transfer learning methods to derive insights about the similarities between the different music cultures to which the data belongs to. We use two Western music datasets, two traditional/folk datasets coming from eastern Mediterranean cultures, and two datasets belonging to Indian art music. Three deep audio embedding models are trained and transferred across domains, including two CNN-based and a Transformer-based architecture, to perform auto-tagging for each target domain dataset. Experimental results show that competitive performance is achieved in all domains via transfer learning, while the best source dataset varies for each music culture. The implementation and the trained models are both provided in a public repository.
</details>
<details>
<summary>摘要</summary>
Note:* "MIR" stands for Music Information Retrieval.* "CNN" stands for Convolutional Neural Network.* "Transformer" is a type of neural network architecture.* "auto-tagging" refers to the task of assigning genre or style labels to music tracks.* "public repository" refers to a publicly accessible repository of code and data, such as GitHub.
</details></li>
</ul>
<hr>
<h2 id="IncDSI-Incrementally-Updatable-Document-Retrieval"><a href="#IncDSI-Incrementally-Updatable-Document-Retrieval" class="headerlink" title="IncDSI: Incrementally Updatable Document Retrieval"></a>IncDSI: Incrementally Updatable Document Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10323">http://arxiv.org/abs/2307.10323</a></li>
<li>repo_url: None</li>
<li>paper_authors: Varsha Kishore, Chao Wan, Justin Lovelace, Yoav Artzi, Kilian Q. Weinberger</li>
<li>for: 这篇论文主要目标是提出一种方法，使得在文档检索模型已经训练完成后，可以快速添加新文档（约20-50ms每个文档），而不需要重新训练整个数据集或部分数据集。</li>
<li>methods: 该方法基于约束优化问题，通过微小地改变网络参数来添加新文档，而不需要重新训练整个模型。</li>
<li>results: 该方法可以在很短的时间内（约20-50ms每个文档）添加新文档，并且与重新训练整个模型相比，其性能甚至可以达到相同的水平。<details>
<summary>Abstract</summary>
Differentiable Search Index is a recently proposed paradigm for document retrieval, that encodes information about a corpus of documents within the parameters of a neural network and directly maps queries to corresponding documents. These models have achieved state-of-the-art performances for document retrieval across many benchmarks. These kinds of models have a significant limitation: it is not easy to add new documents after a model is trained. We propose IncDSI, a method to add documents in real time (about 20-50ms per document), without retraining the model on the entire dataset (or even parts thereof). Instead we formulate the addition of documents as a constrained optimization problem that makes minimal changes to the network parameters. Although orders of magnitude faster, our approach is competitive with re-training the model on the whole dataset and enables the development of document retrieval systems that can be updated with new information in real-time. Our code for IncDSI is available at https://github.com/varshakishore/IncDSI.
</details>
<details>
<summary>摘要</summary>
“差分搜寻索引”是一种最近提出的文档搜寻方法，它将文档库中的信息嵌入 ней网络中，直接将查询与相应的文档映射到一起。这些模型在许多测试场景中获得了最佳性能。然而，这种模型具有一个重要的限制：添加新文档需要重新训练整个模型。我们提出了一种方法，可以在实时（约20-50ms每个文档）添加新文档，不需要重新训练整个模型或部分模型。我们将添加文档视为一个受限制的优化问题，使得模型参数几乎不会改变。这种方法比较快速，但是和重新训练模型一样竞争，可以在实时更新文档搜寻系统。我们的IncDSI代码可以在 GitHub 上找到。
</details></li>
</ul>
<hr>
<h2 id="A-Note-on-Hardness-of-Computing-Recursive-Teaching-Dimension"><a href="#A-Note-on-Hardness-of-Computing-Recursive-Teaching-Dimension" class="headerlink" title="A Note on Hardness of Computing Recursive Teaching Dimension"></a>A Note on Hardness of Computing Recursive Teaching Dimension</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09792">http://arxiv.org/abs/2307.09792</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pasin Manurangsi</li>
<li>for: 计算概念类中的可 recursive teaching dimension (RTD) 问题需要 $n^{\Omega(\log n)}$ 时间。</li>
<li>methods: 使用 exponential time hypothesis (ETH)。</li>
<li>results: 与布尔特力算法的时间复杂度 ($n^{O(\log n)}$) 相同。<details>
<summary>Abstract</summary>
In this short note, we show that the problem of computing the recursive teaching dimension (RTD) for a concept class (given explicitly as input) requires $n^{\Omega(\log n)}$-time, assuming the exponential time hypothesis (ETH). This matches the running time $n^{O(\log n)}$ of the brute-force algorithm for the problem.
</details>
<details>
<summary>摘要</summary>
在这个短notes中，我们证明计算概念类中的回归教学维度（RTD）问题需要$n^{\Omega(\log n)}$时间，假设快速幂时间假设（ETH）。这与简洁力算法的运行时间$n^{O(\log n)}$匹配。
</details></li>
</ul>
<hr>
<h2 id="Reproducibility-in-Machine-Learning-Driven-Research"><a href="#Reproducibility-in-Machine-Learning-Driven-Research" class="headerlink" title="Reproducibility in Machine Learning-Driven Research"></a>Reproducibility in Machine Learning-Driven Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10320">http://arxiv.org/abs/2307.10320</a></li>
<li>repo_url: None</li>
<li>paper_authors: Harald Semmelrock, Simone Kopeinik, Dieter Theiler, Tony Ross-Hellauer, Dominik Kowald<br>for:* 这个论文旨在探讨机器学习（ML）驱动研究中的可重现性问题。methods:* 本论文通过文献综述来反思现有的ML可重现性问题，以及在不同研究领域中存在的可重现性障碍和挑战。results:* 本论文发现了一些可能支持ML可重现性的驱动因素，包括工具、做法和干预措施。这些可能有助于决策不同解决方案的可行性。<details>
<summary>Abstract</summary>
Research is facing a reproducibility crisis, in which the results and findings of many studies are difficult or even impossible to reproduce. This is also the case in machine learning (ML) and artificial intelligence (AI) research. Often, this is the case due to unpublished data and/or source-code, and due to sensitivity to ML training conditions. Although different solutions to address this issue are discussed in the research community such as using ML platforms, the level of reproducibility in ML-driven research is not increasing substantially. Therefore, in this mini survey, we review the literature on reproducibility in ML-driven research with three main aims: (i) reflect on the current situation of ML reproducibility in various research fields, (ii) identify reproducibility issues and barriers that exist in these research fields applying ML, and (iii) identify potential drivers such as tools, practices, and interventions that support ML reproducibility. With this, we hope to contribute to decisions on the viability of different solutions for supporting ML reproducibility.
</details>
<details>
<summary>摘要</summary>
研究现正面临一场可重现危机，许多研究结果和发现很难或甚至无法重现。这同样适用于机器学习（ML）和人工智能（AI）研究。常常这是因为未公开的数据和/或源代码，以及对ML训练条件的敏感性。虽然研究社区中有许多解决这个问题的方案，如使用ML平台，但ML驱动研究的可重现性并没有显著增长。因此，在这次小调查中，我们会回顾ML驱动研究的可重现性在不同领域的当前情况，认定这些领域中ML可重现性的问题和障碍，以及可能的驱动因素，如工具、做法和干预措施，以支持ML可重现性。我们希望通过这些研究来帮助决策不同解决方案的可行性。
</details></li>
</ul>
<hr>
<h2 id="ZeroQuant-FP-A-Leap-Forward-in-LLMs-Post-Training-W4A8-Quantization-Using-Floating-Point-Formats"><a href="#ZeroQuant-FP-A-Leap-Forward-in-LLMs-Post-Training-W4A8-Quantization-Using-Floating-Point-Formats" class="headerlink" title="ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats"></a>ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09782">http://arxiv.org/abs/2307.09782</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/DeepSpeed">https://github.com/microsoft/DeepSpeed</a></li>
<li>paper_authors: Xiaoxia Wu, Zhewei Yao, Yuxiong He</li>
<li>for: 这个研究旨在探讨大语言模型（LLM）中的 computationally efficient 和 maintaining model quality 之间的平衡。</li>
<li>methods: 本研究使用 floating-point（FP）quantization，特别是FP8和FP4，作为可能的解决方案。</li>
<li>results: 我们的调查发现，FP8 activation 在 LLM 中比 integer（INT8）更出色，尤其是模型具有超过一亿个参数时。 在 weight quantization 方面，我们发现 FP4 与 INT4 相比，表现相似或更好，并且可以让部署在 FP 支持的硬件上，如 H100。 对于精度Alignment问题，我们提出了两个构成簇�limitations的缩减方法，它们几乎没有影响 W4A8 模型的性能。 此外，我们还将量化方法与 Low Rank Compensation (LoRC) 策略相结合，尤其是在较小的模型中，实现更好的性能。 总的来说，本研究显示 FP quantization 具有巨大的潜力，并且可以实现高效的部署在资源有限的环境中。<details>
<summary>Abstract</summary>
In the complex domain of large language models (LLMs), striking a balance between computational efficiency and maintaining model quality is a formidable challenge. Navigating the inherent limitations of uniform quantization, particularly when dealing with outliers, and motivated by the launch of NVIDIA's H100 hardware, this study delves into the viability of floating-point (FP) quantization, particularly focusing on FP8 and FP4, as a potential solution. Our comprehensive investigation reveals that for LLMs, FP8 activation consistently outshines its integer (INT8) equivalent, with the performance edge becoming more noticeable in models possessing parameters beyond one billion. For weight quantization, our findings indicate that FP4 exhibits comparable, if not superior, performance to INT4, simplifying deployment on FP-supported hardware like H100. To mitigate the overhead from precision alignment caused by the disparity between weights and activations, we propose two scaling constraints for weight quantization that negligibly impact the performance compared to the standard W4A8 model. We additionally enhance our quantization methods by integrating the Low Rank Compensation (LoRC) strategy, yielding improvements especially in smaller models. The results of our investigation emphasize the immense potential of FP quantization for LLMs, paving the way for high-efficiency deployment in resource-limited settings.
</details>
<details>
<summary>摘要</summary>
在大型语言模型（LLM）领域中，实现计算效率和模型质量之间的平衡是一项具有挑战性的任务。在固定量化的限制下，特别是处理异常值时，这种研究探讨了浮点（FP）量化的可能性，主要关注FP8和FP4。我们的全面调查发现，对于LLMs，FP8活动频繁表现出优于其整数（INT8）等效的优势，这种优势在模型参数超过一百亿时变得更加明显。对于权量量化，我们的发现表明，FP4与INT4相比，表现相对或者甚至更好，使得在FP支持的硬件上进行部署变得更加简单。为了减少精度对齐所导致的开销，我们提议了两种缩放约束 для权量量化，对于标准W4A8模型来说，影响非常小。此外，我们还改进了我们的量化方法，通过 интеграating Low Rank Compensation（LoRC）策略，尤其在较小的模型中，带来了改进。我们的研究结果表明，FP量化对LLMs具有巨大的潜力，开展高效的部署在有限的资源环境中。
</details></li>
</ul>
<hr>
<h2 id="Text2Layer-Layered-Image-Generation-using-Latent-Diffusion-Model"><a href="#Text2Layer-Layered-Image-Generation-using-Latent-Diffusion-Model" class="headerlink" title="Text2Layer: Layered Image Generation using Latent Diffusion Model"></a>Text2Layer: Layered Image Generation using Latent Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09781">http://arxiv.org/abs/2307.09781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyang Zhang, Wentian Zhao, Xin Lu, Jeff Chien</li>
<li>for: 本研究旨在探讨层 composer 的层 composite 图像生成问题，以提高图像编辑的效率和质量。</li>
<li>methods: 提议使用自适应编码器和扩散模型，同时生成背景、前景、层Mask和组合图像。</li>
<li>results: 实验结果表明，提议的方法可以生成高质量的层 composite 图像，并为未来的研究提供了 benchmark。<details>
<summary>Abstract</summary>
Layer compositing is one of the most popular image editing workflows among both amateurs and professionals. Motivated by the success of diffusion models, we explore layer compositing from a layered image generation perspective. Instead of generating an image, we propose to generate background, foreground, layer mask, and the composed image simultaneously. To achieve layered image generation, we train an autoencoder that is able to reconstruct layered images and train diffusion models on the latent representation. One benefit of the proposed problem is to enable better compositing workflows in addition to the high-quality image output. Another benefit is producing higher-quality layer masks compared to masks produced by a separate step of image segmentation. Experimental results show that the proposed method is able to generate high-quality layered images and initiates a benchmark for future work.
</details>
<details>
<summary>摘要</summary>
层 Compositing 是现代图像编辑中最受欢迎的工作流程之一，both amateur 和专业人士都广泛使用。受扩散模型的成功启发，我们从层图生成的角度来探讨层 Compositing。而不是生成一个图像，我们提议同时生成背景、前景、层面Mask 和组合图像。为实现层图生成，我们训练了一个可重建层图的 autoencoder，并在幂示表示中训练扩散模型。这种方法的一个优点是能够实现更好的组合工作流程，同时生成高质量的层面Mask。另一个优点是生成的层面Mask 比传统的图像分割步骤生成的Mask 更高质量。实验结果表明，我们的方法可以生成高质量的层图，并成为未来工作的标准 referral。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Single-Feature-Importance-with-ICECREAM"><a href="#Beyond-Single-Feature-Importance-with-ICECREAM" class="headerlink" title="Beyond Single-Feature Importance with ICECREAM"></a>Beyond Single-Feature Importance with ICECREAM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09779">http://arxiv.org/abs/2307.09779</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Oesterle, Patrick Blöbaum, Atalanti A. Mastakouri, Elke Kirschbaum</li>
<li>for: 本研究旨在找到模型中任务的可读性和根本原因分析方法，以及哪些变量协会导致特定的输出结果。</li>
<li>methods: 本研究提出了一种信息量度量方法，用于衡量变量协会对目标变量分布的影响。这种方法可以识别哪些变量是获取特定结果的关键因素，而不是单独评估每个变量的重要性。</li>
<li>results: 在synthetic和实际数据上进行了实验，并显示了ICECREAM在可读性和根本原因分析任务中的表现优于现有方法，并在两个任务中达到了很高的准确率。<details>
<summary>Abstract</summary>
Which set of features was responsible for a certain output of a machine learning model? Which components caused the failure of a cloud computing application? These are just two examples of questions we are addressing in this work by Identifying Coalition-based Explanations for Common and Rare Events in Any Model (ICECREAM). Specifically, we propose an information-theoretic quantitative measure for the influence of a coalition of variables on the distribution of a target variable. This allows us to identify which set of factors is essential to obtain a certain outcome, as opposed to well-established explainability and causal contribution analysis methods which can assign contributions only to individual factors and rank them by their importance. In experiments with synthetic and real-world data, we show that ICECREAM outperforms state-of-the-art methods for explainability and root cause analysis, and achieves impressive accuracy in both tasks.
</details>
<details>
<summary>摘要</summary>
本文提出了一种基于联盟的解释方法，可以帮助解释模型的输出和云计算应用的失败。这种方法基于信息理论量化度量变量联盟对目标变量的分布的影响。这允许我们确定哪些变量是获得某种结果的关键因素，而不是单独评估每个变量的重要性。在合成和实际数据上的实验中，我们发现ICECREAM比现有的解释和根本原因分析方法更高效，并在两个任务中达到了出色的准确率。
</details></li>
</ul>
<hr>
<h2 id="Polyffusion-A-Diffusion-Model-for-Polyphonic-Score-Generation-with-Internal-and-External-Controls"><a href="#Polyffusion-A-Diffusion-Model-for-Polyphonic-Score-Generation-with-Internal-and-External-Controls" class="headerlink" title="Polyffusion: A Diffusion Model for Polyphonic Score Generation with Internal and External Controls"></a>Polyffusion: A Diffusion Model for Polyphonic Score Generation with Internal and External Controls</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10304">http://arxiv.org/abs/2307.10304</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aik2mlj/polyffusion">https://github.com/aik2mlj/polyffusion</a></li>
<li>paper_authors: Lejun Min, Junyan Jiang, Gus Xia, Jingwei Zhao</li>
<li>for: 该论文旨在提出一种Diffusion模型，用于生成多重音乐曲谱。</li>
<li>methods: 该模型使用image-like piano roll表示法，并使用内部控制和外部控制两种方法来生成音乐。内部控制是指用户先定义一部分音乐，然后让模型填充其余部分，类似于隐藏音乐生成（或音乐填充）任务。外部控制通过跨注意机制使用外部 yet related的信息，如和声、文化或其他特征来控制模型。</li>
<li>results: 我们的模型在多种音乐创作任务上显著超越了现有的Transformer和采样基eline，并且使用预训练分离表示的外部条件可以更有效地控制音乐生成。<details>
<summary>Abstract</summary>
We propose Polyffusion, a diffusion model that generates polyphonic music scores by regarding music as image-like piano roll representations. The model is capable of controllable music generation with two paradigms: internal control and external control. Internal control refers to the process in which users pre-define a part of the music and then let the model infill the rest, similar to the task of masked music generation (or music inpainting). External control conditions the model with external yet related information, such as chord, texture, or other features, via the cross-attention mechanism. We show that by using internal and external controls, Polyffusion unifies a wide range of music creation tasks, including melody generation given accompaniment, accompaniment generation given melody, arbitrary music segment inpainting, and music arrangement given chords or textures. Experimental results show that our model significantly outperforms existing Transformer and sampling-based baselines, and using pre-trained disentangled representations as external conditions yields more effective controls.
</details>
<details>
<summary>摘要</summary>
我们提出了Polyffusion模型，它可以生成多重音乐分解的乐谱，通过对音乐视为像图式钢琴表示来实现。这个模型具有可控的音乐生成功能，可以通过两种方法进行控制：内部控制和外部控制。内部控制是指用户先定义一部分乐谱，然后让模型填充其余部分，与遮盖音乐生成（或音乐填充）任务类似。外部控制通过跨注意机制使用外部 yet related的信息，如和声、文本或其他特征，来控制模型。我们表明，通过内部和外部控制，Polyffusion模型可以整合多种音乐创作任务，包括给予伴奏的旋律生成、给予旋律的伴奏生成、随机音乐段填充和基于和声或Texture的乐谱安排。实验结果显示，我们的模型在与传统Transformer和采样基础的比较中表现出色，并且使用预训练分离的表示作为外部条件可以更有效地控制。
</details></li>
</ul>
<hr>
<h2 id="Eliminating-Label-Leakage-in-Tree-Based-Vertical-Federated-Learning"><a href="#Eliminating-Label-Leakage-in-Tree-Based-Vertical-Federated-Learning" class="headerlink" title="Eliminating Label Leakage in Tree-Based Vertical Federated Learning"></a>Eliminating Label Leakage in Tree-Based Vertical Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10318">http://arxiv.org/abs/2307.10318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hideaki Takahashi, Jingjing Liu, Yang Liu</li>
<li>for: 防止树型 federated learning 模型泄露 privacy 信息</li>
<li>methods:  introduce 一种新的标签推理攻击 ID2Graph, 使用 instance space 中每个节点的集合来推理私有训练标签</li>
<li>results: 对多个数据集进行了全面的实验，显示 ID2Graph 攻击可以带来重大的隐私泄露风险，而 ID-LMID 防御机制可以有效地遏制标签泄露。<details>
<summary>Abstract</summary>
Vertical federated learning (VFL) enables multiple parties with disjoint features of a common user set to train a machine learning model without sharing their private data. Tree-based models have become prevalent in VFL due to their interpretability and efficiency. However, the vulnerability of tree-based VFL has not been sufficiently investigated. In this study, we first introduce a novel label inference attack, ID2Graph, which utilizes the sets of record-IDs assigned to each node (i.e., instance space) to deduce private training labels. The ID2Graph attack generates a graph structure from training samples, extracts communities from the graph, and clusters the local dataset using community information. To counteract label leakage from the instance space, we propose an effective defense mechanism, ID-LMID, which prevents label leakage by focusing on mutual information regularization. Comprehensive experiments conducted on various datasets reveal that the ID2Graph attack presents significant risks to tree-based models such as Random Forest and XGBoost. Further evaluations on these benchmarks demonstrate that ID-LMID effectively mitigates label leakage in such instances.
</details>
<details>
<summary>摘要</summary>
vertical Federated learning (VFL) 使多个党有不同特征的共同用户集来训练机器学习模型无需分享私人数据。 树状模型在 VFL 中变得普遍使用，因为它们具有可读性和效率。 然而，树状 VFL 的攻击性尚未得到足够的研究。 在这项研究中，我们首先介绍了一种新的标签推理攻击，ID2Graph，它利用训练样本中每个节点（即实例空间）分配的集合ID来推理私人训练标签。 ID2Graph 攻击生成了一个图структура从训练样本中，提取了图中的社区信息，并使用社区信息对本地数据进行分 clustering。 为了防止实例空间中的标签泄露，我们提议一种有效的防御机制，ID-LMID，它通过强调相互信息规范来防止标签泄露。 对于各种数据集进行了全面的实验，我们发现 ID2Graph 攻击对树状模型 such as Random Forest 和 XGBoost 具有显著的风险。 进一步对这些标准 benchmark 进行了评估，我们发现 ID-LMID 能够有效地 mitigate 标签泄露。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Learning-for-WiFi-CSI-Based-Human-Activity-Recognition-A-Systematic-Study"><a href="#Self-Supervised-Learning-for-WiFi-CSI-Based-Human-Activity-Recognition-A-Systematic-Study" class="headerlink" title="Self-Supervised Learning for WiFi CSI-Based Human Activity Recognition: A Systematic Study"></a>Self-Supervised Learning for WiFi CSI-Based Human Activity Recognition: A Systematic Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02412">http://arxiv.org/abs/2308.02412</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JJJinx/SSLCSI">https://github.com/JJJinx/SSLCSI</a></li>
<li>paper_authors: Ke Xu, Jiangtao Wang, Hongyuan Zhu, Dingchang Zheng</li>
<li>for: 这篇论文主要针对的是 WiFi CSI 基于 HAR 的深度学习应用，尤其是在面临数据缺乏问题时，如何通过 SSL 算法来学习有用的表示。</li>
<li>methods: 这篇论文使用了多种类型的 SSL 算法进行研究，包括已经研究过的和尚未研究过的类型，并在 WiFi CSI 基于 HAR 的三个公共数据集上进行了实验。</li>
<li>results: 实验结果表明，SSL 算法在 WiFi CSI 基于 HAR 应用中具有潜在的应用前景，但现有的工作存在一些局限性和盲点，需要进一步改进。<details>
<summary>Abstract</summary>
Recently, with the advancement of the Internet of Things (IoT), WiFi CSI-based HAR has gained increasing attention from academic and industry communities. By integrating the deep learning technology with CSI-based HAR, researchers achieve state-of-the-art performance without the need of expert knowledge. However, the scarcity of labeled CSI data remains the most prominent challenge when applying deep learning models in the context of CSI-based HAR due to the privacy and incomprehensibility of CSI-based HAR data. On the other hand, SSL has emerged as a promising approach for learning meaningful representations from data without heavy reliance on labeled examples. Therefore, considerable efforts have been made to address the challenge of insufficient data in deep learning by leveraging SSL algorithms. In this paper, we undertake a comprehensive inventory and analysis of the potential held by different categories of SSL algorithms, including those that have been previously studied and those that have not yet been explored, within the field. We provide an in-depth investigation of SSL algorithms in the context of WiFi CSI-based HAR. We evaluate four categories of SSL algorithms using three publicly available CSI HAR datasets, each encompassing different tasks and environmental settings. To ensure relevance to real-world applications, we design performance metrics that align with specific requirements. Furthermore, our experimental findings uncover several limitations and blind spots in existing work, highlighting the barriers that need to be addressed before SSL can be effectively deployed in real-world WiFi-based HAR applications. Our results also serve as a practical guideline for industry practitioners and provide valuable insights for future research endeavors in this field.
</details>
<details>
<summary>摘要</summary>
现在，因为互联网物联网（IoT）的发展，WiFi CSI基于HAR（人体动作识别）获得了学术和业界的越来越多的关注。通过将深度学习技术与CSI基于HAR结合，研究人员可以达到 estado del arte的性能，无需专业知识。然而，CSI基于HAR数据的缺乏标注数据仍然是在应用深度学习模型时最大的挑战。另一方面，SSL（自然语言处理）已经出现为了学习不需要大量标注数据的有用表示。因此，在深度学习中使用SSL算法的努力已经被做出了很多。在这篇论文中，我们进行了全面的SSL算法类型的评估和分析，包括已经研究过的和尚未研究过的类型。我们对WiFi CSI基于HAR中SSL算法进行了深入的调查。我们使用三个公共可用的CSI HAR数据集，每个数据集都包括不同的任务和环境设置，来评估四类SSL算法。为确保与实际应用相关，我们定义了与特定需求相符的性能指标。我们的实验结果表明，SSL算法在WiFi CSI基于HAR中具有潜在的优势，但是也存在一些局限性和盲点。我们的结果还提供了实际应用中SSL的实用指南，并为未来在这一领域的研究提供了价值的洞察。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Spatial-Temporal-Variational-Quantum-Circuit-to-Enable-Deep-Learning-on-NISQ-Devices"><a href="#A-Novel-Spatial-Temporal-Variational-Quantum-Circuit-to-Enable-Deep-Learning-on-NISQ-Devices" class="headerlink" title="A Novel Spatial-Temporal Variational Quantum Circuit to Enable Deep Learning on NISQ Devices"></a>A Novel Spatial-Temporal Variational Quantum Circuit to Enable Deep Learning on NISQ Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09771">http://arxiv.org/abs/2307.09771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinyang Li, Zhepeng Wang, Zhirui Hu, Prasanna Date, Ang Li, Weiwen Jiang</li>
<li>for: 本研究旨在推动量子计算机器学（Quantum Computing）在机器学习（Machine Learning）中的发展，提出一种新的空间-时间设计方法（ST-VQC），以提高量子学习模型的非线性性和鲁棒性。</li>
<li>methods: 本研究使用了量子逻辑设计（Quantum Circuit Design）和自动优化框架（Automated Optimization Framework），并对不同设计参数进行系统性分析。</li>
<li>results: 实验结果表明，ST-VQC可以在IBM量子处理器上（IBM_cairo和ibmq_lima）实现30%以上的准确率提升，并在非线性的 sintetic 数据上达到27.9%的提升。<details>
<summary>Abstract</summary>
Quantum computing presents a promising approach for machine learning with its capability for extremely parallel computation in high-dimension through superposition and entanglement. Despite its potential, existing quantum learning algorithms, such as Variational Quantum Circuits(VQCs), face challenges in handling more complex datasets, particularly those that are not linearly separable. What's more, it encounters the deployability issue, making the learning models suffer a drastic accuracy drop after deploying them to the actual quantum devices. To overcome these limitations, this paper proposes a novel spatial-temporal design, namely ST-VQC, to integrate non-linearity in quantum learning and improve the robustness of the learning model to noise. Specifically, ST-VQC can extract spatial features via a novel block-based encoding quantum sub-circuit coupled with a layer-wise computation quantum sub-circuit to enable temporal-wise deep learning. Additionally, a SWAP-Free physical circuit design is devised to improve robustness. These designs bring a number of hyperparameters. After a systematic analysis of the design space for each design component, an automated optimization framework is proposed to generate the ST-VQC quantum circuit. The proposed ST-VQC has been evaluated on two IBM quantum processors, ibm_cairo with 27 qubits and ibmq_lima with 7 qubits to assess its effectiveness. The results of the evaluation on the standard dataset for binary classification show that ST-VQC can achieve over 30% accuracy improvement compared with existing VQCs on actual quantum computers. Moreover, on a non-linear synthetic dataset, the ST-VQC outperforms a linear classifier by 27.9%, while the linear classifier using classical computing outperforms the existing VQC by 15.58%.
</details>
<details>
<summary>摘要</summary>
量子计算技术在机器学习方面具有极大的潜力，它可以通过超position和异构的方式实现高维度的极其平行计算。然而，现有的量子学习算法，如量子环路Circuits（VQCs），在处理更复杂的数据集时存在挑战，特别是不可分 Linearly separable 的数据集。此外，它们在真实的量子设备上部署时会导致学习模型准确率受到极大的影响。为了解决这些限制，本文提出了一种新的空间-时间设计方法，即 ST-VQC，以整合量子学习中的非线性性。具体来说，ST-VQC可以通过一种新的块基编码量子子子电路和层 wise 计算量子子电路来提取空间特征，并且可以在时间维度上进行深度学习。此外，为了提高 robustness，我们还提出了一种 SWAP-Free 物理电路设计。这些设计带来了一些参数。经系统性分析每个设计 ком成分，我们提出了一个自动优化框架，以生成 ST-VQC 量子电路。我们对 IBM 量子处理器 ibm_cairo 和 ibmq_lima 进行评估，结果显示，ST-VQC 在标准的 binary classification 数据集上可以达到30%以上的准确率提升，并在非线性synthetic 数据集上出perform linear classifier 和现有 VQC 的 27.9%和15.58%。
</details></li>
</ul>
<hr>
<h2 id="How-Curvature-Enhance-the-Adaptation-Power-of-Framelet-GCNs"><a href="#How-Curvature-Enhance-the-Adaptation-Power-of-Framelet-GCNs" class="headerlink" title="How Curvature Enhance the Adaptation Power of Framelet GCNs"></a>How Curvature Enhance the Adaptation Power of Framelet GCNs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09768">http://arxiv.org/abs/2307.09768</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dshi3553usyd/curvature_enhanced_graph_convolution">https://github.com/dshi3553usyd/curvature_enhanced_graph_convolution</a></li>
<li>paper_authors: Dai Shi, Yi Guo, Zhiqi Shao, Junbin Gao</li>
<li>for: 提高图像学习模型在图形数据上的性能，具体来说是解决图像中的过熔问题。</li>
<li>methods: 利用图形 Ricci  curvature 作为特殊转换函数 $\zeta$，并证明这种方法可以解决图像中的过熔问题。</li>
<li>results: 对比州对照数据集，提出一种基于 Ricci  curvature 的图像模型，其性能高于现有基eline。<details>
<summary>Abstract</summary>
Graph neural network (GNN) has been demonstrated powerful in modeling graph-structured data. However, despite many successful cases of applying GNNs to various graph classification and prediction tasks, whether the graph geometrical information has been fully exploited to enhance the learning performance of GNNs is not yet well understood. This paper introduces a new approach to enhance GNN by discrete graph Ricci curvature. Specifically, the graph Ricci curvature defined on the edges of a graph measures how difficult the information transits on one edge from one node to another based on their neighborhoods. Motivated by the geometric analogy of Ricci curvature in the graph setting, we prove that by inserting the curvature information with different carefully designed transformation function $\zeta$, several known computational issues in GNN such as over-smoothing can be alleviated in our proposed model. Furthermore, we verified that edges with very positive Ricci curvature (i.e., $\kappa_{i,j} \approx 1$) are preferred to be dropped to enhance model's adaption to heterophily graph and one curvature based graph edge drop algorithm is proposed. Comprehensive experiments show that our curvature-based GNN model outperforms the state-of-the-art baselines in both homophily and heterophily graph datasets, indicating the effectiveness of involving graph geometric information in GNNs.
</details>
<details>
<summary>摘要</summary>
граф neural network (GNN) 已经被证明能够有效地处理图Structured数据。然而，尽管许多GNN应用于不同的图分类和预测任务中得到了成功，whether the graph geometric information has been fully exploited to enhance the learning performance of GNNs is not yet well understood. This paper introduces a new approach to enhance GNN by discrete graph Ricci curvature. Specifically, the graph Ricci curvature defined on the edges of a graph measures how difficult the information transits on one edge from one node to another based on their neighborhoods. Motivated by the geometric analogy of Ricci curvature in the graph setting, we prove that by inserting the curvature information with different carefully designed transformation function $\zeta$, several known computational issues in GNN such as over-smoothing can be alleviated in our proposed model. Furthermore, we verified that edges with very positive Ricci curvature (i.e., $\kappa_{i,j} \approx 1$) are preferred to be dropped to enhance model's adaption to heterophily graph and one curvature based graph edge drop algorithm is proposed. Comprehensive experiments show that our curvature-based GNN model outperforms the state-of-the-art baselines in both homophily and heterophily graph datasets, indicating the effectiveness of involving graph geometric information in GNNs.
</details></li>
</ul>
<hr>
<h2 id="Sig-Splines-universal-approximation-and-convex-calibration-of-time-series-generative-models"><a href="#Sig-Splines-universal-approximation-and-convex-calibration-of-time-series-generative-models" class="headerlink" title="Sig-Splines: universal approximation and convex calibration of time series generative models"></a>Sig-Splines: universal approximation and convex calibration of time series generative models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09767">http://arxiv.org/abs/2307.09767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Magnus Wiese, Phillip Murray, Ralf Korn</li>
<li>for: 这 paper 的目的是为了提出一种基于神经流函数的新型生成模型，用于处理多变量离散时间序列数据。</li>
<li>methods: 该算法采用了神经流函数的构造方式，并结合了线性变换和签名变换，作为传统神经网络的替换。这种方法不仅实现了神经网络的通用性，同时还引入了模型参数的 convexity。</li>
<li>results: 研究人员通过实验和比较分析，证明了该模型的可靠性和精度。<details>
<summary>Abstract</summary>
We propose a novel generative model for multivariate discrete-time time series data. Drawing inspiration from the construction of neural spline flows, our algorithm incorporates linear transformations and the signature transform as a seamless substitution for traditional neural networks. This approach enables us to achieve not only the universality property inherent in neural networks but also introduces convexity in the model's parameters.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的生成模型，用于多变量离散时间序列数据。我们的算法灵感来自神经剖流的构造，并在神经网络中 linear transformation 和签名变换作为一种自然的替换。这种方法不仅可以保持神经网络的通用性，同时还可以在模型参数中引入凸形性。
</details></li>
</ul>
<hr>
<h2 id="Reinforcing-POD-based-model-reduction-techniques-in-reaction-diffusion-complex-networks-using-stochastic-filtering-and-pattern-recognition"><a href="#Reinforcing-POD-based-model-reduction-techniques-in-reaction-diffusion-complex-networks-using-stochastic-filtering-and-pattern-recognition" class="headerlink" title="Reinforcing POD based model reduction techniques in reaction-diffusion complex networks using stochastic filtering and pattern recognition"></a>Reinforcing POD based model reduction techniques in reaction-diffusion complex networks using stochastic filtering and pattern recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09762">http://arxiv.org/abs/2307.09762</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhishek Ajayakumar, Soumyendu Raha</li>
<li>for: 本研究旨在提高受到输入数据干扰的复杂网络模型的准确性。</li>
<li>methods: 我们提出了一种基于模式识别理论和随机滤波理论的算法框架，用于增强复杂网络模型的输出。</li>
<li>results: 我们的研究结果表明，我们的方法可以在受到输入数据干扰的情况下提高复杂网络模型的准确性。<details>
<summary>Abstract</summary>
Complex networks are used to model many real-world systems. However, the dimensionality of these systems can make them challenging to analyze. Dimensionality reduction techniques like POD can be used in such cases. However, these models are susceptible to perturbations in the input data. We propose an algorithmic framework that combines techniques from pattern recognition (PR) and stochastic filtering theory to enhance the output of such models. The results of our study show that our method can improve the accuracy of the surrogate model under perturbed inputs. Deep Neural Networks (DNNs) are susceptible to adversarial attacks. However, recent research has revealed that neural Ordinary Differential Equations (ODEs) exhibit robustness in specific applications. We benchmark our algorithmic framework with a Neural ODE-based approach as a reference.
</details>
<details>
<summary>摘要</summary>
困难网络是现实世界系统模型的重要工具。然而，这些系统的维度可能会使其分析困难。降维技术如POD可以使用在这些情况下。然而，这些模型容易受输入数据的干扰。我们提出一种算法框架， combining技术从图像识别(PR)和随机滤波理论来增强这些模型的输出。我们的研究结果表明，我们的方法可以在受到干扰输入时提高代理模型的准确性。深度神经网络(DNNs)容易受到恶意攻击。然而，最近的研究表明，神经Ordinary Differential Equations(ODEs)在特定应用场景下具有抗震性。我们对比了我们的算法框架与神经ODE-based方法作为参考。
</details></li>
</ul>
<hr>
<h2 id="FedBug-A-Bottom-Up-Gradual-Unfreezing-Framework-for-Federated-Learning"><a href="#FedBug-A-Bottom-Up-Gradual-Unfreezing-Framework-for-Federated-Learning" class="headerlink" title="FedBug: A Bottom-Up Gradual Unfreezing Framework for Federated Learning"></a>FedBug: A Bottom-Up Gradual Unfreezing Framework for Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10317">http://arxiv.org/abs/2307.10317</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iandrover/fedbug">https://github.com/iandrover/fedbug</a></li>
<li>paper_authors: Chia-Hsiang Kao, Yu-Chiang Frank Wang</li>
<li>for: 这个论文旨在解决联合学习中的客户端漂移问题 (Client Drift in Federated Learning)。</li>
<li>methods: 这个论文提出了一个名为 FedBug（联合学习底层逐层解冻）的新的联合学习框架，通过将客户端模型中的各层parameters作为服务器在每个全球轮次分配的参考点，实现跨客户模型的同步。</li>
<li>results: 这个论文透过实验证明了 FedBug 的超越 FedAvg 的减少速率，并且运行在不同的数据集、训练情况和网络架构下，实现了广泛的应用和可靠性。<details>
<summary>Abstract</summary>
Federated Learning (FL) offers a collaborative training framework, allowing multiple clients to contribute to a shared model without compromising data privacy. Due to the heterogeneous nature of local datasets, updated client models may overfit and diverge from one another, commonly known as the problem of client drift. In this paper, we propose FedBug (Federated Learning with Bottom-Up Gradual Unfreezing), a novel FL framework designed to effectively mitigate client drift. FedBug adaptively leverages the client model parameters, distributed by the server at each global round, as the reference points for cross-client alignment. Specifically, on the client side, FedBug begins by freezing the entire model, then gradually unfreezes the layers, from the input layer to the output layer. This bottom-up approach allows models to train the newly thawed layers to project data into a latent space, wherein the separating hyperplanes remain consistent across all clients. We theoretically analyze FedBug in a novel over-parameterization FL setup, revealing its superior convergence rate compared to FedAvg. Through comprehensive experiments, spanning various datasets, training conditions, and network architectures, we validate the efficacy of FedBug. Our contributions encompass a novel FL framework, theoretical analysis, and empirical validation, demonstrating the wide potential and applicability of FedBug.
</details>
<details>
<summary>摘要</summary>
联合学习（FL）提供了一个合作训练框架，让多个客户端参与共同建立一个共享模型，不会妥协数据隐私。由于本地数据的不同性，客户端模型的更新可能会过滤和分化，通常称为客户遗传问题。在这篇论文中，我们提出了FedBug（联合学习底层逐层解冻），一个新的FL框架，可以有效地解决客户遗传问题。FedBug在客户端上运行，从入口层开始逐层解冻，直到出口层为止。这个底层逐层解冻方法让模型在新解冻的层上训练，将数据 проек到一个共同的潜在空间中，并且在所有客户端上保持对应的分隔至对应的抽象面。我们在一个新的过 Parameterization FL 设置下进行了理论分析，发现FedBug的快速渐近率比 FedAvg 更高。通过实验调查，覆盖了不同的数据、训练情况和网络架构，我们证实了FedBug的有效性。我们的贡献包括一个新的FL框架、理论分析和实验验证，显示了FedBug的广泛应用和可能性。
</details></li>
</ul>
<hr>
<h2 id="Constructing-Extreme-Learning-Machines-with-zero-Spectral-Bias"><a href="#Constructing-Extreme-Learning-Machines-with-zero-Spectral-Bias" class="headerlink" title="Constructing Extreme Learning Machines with zero Spectral Bias"></a>Constructing Extreme Learning Machines with zero Spectral Bias</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09759">http://arxiv.org/abs/2307.09759</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaumudi Joshi, Vukka Snigdha, Arya Kumar Bhattacharya</li>
<li>for: 这篇论文旨在探讨饱和学习机制下的特征偏迁现象，以及如何通过修改ELM的结构来消除这种现象。</li>
<li>methods: 这篇论文使用了EXTREME LEARNING MACHINE（ELM）和傅里叶特征嵌入来研究特征偏迁现象。</li>
<li>results: 研究发现，ELM可以通过修改结构来完全消除特征偏迁现象，并且在实际应用中如物理学 Informed Neural Networks（PINNs）中可以实现高频分解。<details>
<summary>Abstract</summary>
The phenomena of Spectral Bias, where the higher frequency components of a function being learnt in a feedforward Artificial Neural Network (ANN) are seen to converge more slowly than the lower frequencies, is observed ubiquitously across ANNs. This has created technology challenges in fields where resolution of higher frequencies is crucial, like in Physics Informed Neural Networks (PINNs). Extreme Learning Machines (ELMs) that obviate an iterative solution process which provides the theoretical basis of Spectral Bias (SB), should in principle be free of the same. This work verifies the reliability of this assumption, and shows that it is incorrect. However, the structure of ELMs makes them naturally amenable to implementation of variants of Fourier Feature Embeddings, which have been shown to mitigate SB in ANNs. This approach is implemented and verified to completely eliminate SB, thus bringing into feasibility the application of ELMs for practical problems like PINNs where resolution of higher frequencies is essential.
</details>
<details>
<summary>摘要</summary>
偏差现象（Spectral Bias）在循环神经网络（ANN）中，高频组分在学习过程中 converges 更慢速度 than 低频组分，这种现象在各种 ANN 中都是普遍存在的。这对于需要高频分辨率的领域，如物理学 Informed Neural Networks（PINNs），带来了技术挑战。Extreme Learning Machines（ELMs），它们不需要迭代解决过程，这个过程提供了 spectral bias （SB）的理论基础，应该是免疫 SB 的。但是，实际情况表明这是错误的假设。然而，ELMs 的结构使其自然地适合 implements  Fourier Feature Embeddings 的 variants，这些 variants 已经在 ANN 中证明可以 mitigate SB。这种方法在实践中被实现并证明可以完全消除 SB，因此使得 ELMs 在实际问题中，如 PINNs，可以实现高频分辨率的应用。
</details></li>
</ul>
<hr>
<h2 id="Improved-Distribution-Matching-for-Dataset-Condensation"><a href="#Improved-Distribution-Matching-for-Dataset-Condensation" class="headerlink" title="Improved Distribution Matching for Dataset Condensation"></a>Improved Distribution Matching for Dataset Condensation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09742">http://arxiv.org/abs/2307.09742</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uitrbn/idm">https://github.com/uitrbn/idm</a></li>
<li>paper_authors: Ganlong Zhao, Guanbin Li, Yipeng Qin, Yizhou Yu</li>
<li>for: 减少深度学习数据存储成本和训练努力</li>
<li>methods: 基于分布匹配的新方法，包括分区和扩展扩充、高效和丰富模型采样、类具有分布常数化</li>
<li>results: 比之前的优化方法更高效，可以处理更大的数据集和模型，并且实验证明其效果<details>
<summary>Abstract</summary>
Dataset Condensation aims to condense a large dataset into a smaller one while maintaining its ability to train a well-performing model, thus reducing the storage cost and training effort in deep learning applications. However, conventional dataset condensation methods are optimization-oriented and condense the dataset by performing gradient or parameter matching during model optimization, which is computationally intensive even on small datasets and models. In this paper, we propose a novel dataset condensation method based on distribution matching, which is more efficient and promising. Specifically, we identify two important shortcomings of naive distribution matching (i.e., imbalanced feature numbers and unvalidated embeddings for distance computation) and address them with three novel techniques (i.e., partitioning and expansion augmentation, efficient and enriched model sampling, and class-aware distribution regularization). Our simple yet effective method outperforms most previous optimization-oriented methods with much fewer computational resources, thereby scaling data condensation to larger datasets and models. Extensive experiments demonstrate the effectiveness of our method. Codes are available at https://github.com/uitrbn/IDM
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Mood-Classification-of-Bangla-Songs-Based-on-Lyrics"><a href="#Mood-Classification-of-Bangla-Songs-Based-on-Lyrics" class="headerlink" title="Mood Classification of Bangla Songs Based on Lyrics"></a>Mood Classification of Bangla Songs Based on Lyrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10314">http://arxiv.org/abs/2307.10314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maliha Mahajebin, Mohammad Rifat Ahmmad Rashid, Nafees Mansoor</li>
<li>for: 本研究旨在分析孟加拉歌曲的情感，并使用自然语言处理和BERT算法对数据进行分析。</li>
<li>methods: 该研究使用了4000首孟加拉歌曲的歌词、类型和BERT算法进行分析，并将歌曲分为四种情感：快乐、悲伤、爱情和放松。</li>
<li>results: 该研究发现，4000首歌曲中有1513首表达悲伤情感，1362首表达爱情情感，886首表达快乐情感，以及239首表达放松情感。这些结果表明了不同的情感可以通过歌曲来表达，并且可以通过自然语言处理和BERT算法来自动分类歌曲的情感。<details>
<summary>Abstract</summary>
Music can evoke various emotions, and with the advancement of technology, it has become more accessible to people. Bangla music, which portrays different human emotions, lacks sufficient research. The authors of this article aim to analyze Bangla songs and classify their moods based on the lyrics. To achieve this, this research has compiled a dataset of 4000 Bangla song lyrics, genres, and used Natural Language Processing and the Bert Algorithm to analyze the data. Among the 4000 songs, 1513 songs are represented for the sad mood, 1362 for the romantic mood, 886 for happiness, and the rest 239 are classified as relaxation. By embedding the lyrics of the songs, the authors have classified the songs into four moods: Happy, Sad, Romantic, and Relaxed. This research is crucial as it enables a multi-class classification of songs' moods, making the music more relatable to people's emotions. The article presents the automated result of the four moods accurately derived from the song lyrics.
</details>
<details>
<summary>摘要</summary>
音乐可以触发不同的情感，而技术的进步使得音乐更加可 accessible 给人们。孟加拉音乐，表达不同人类情感的音乐，尚未得到充分的研究。本文的作者想要分析孟加拉歌曲，根据歌词来划分不同的情感。为此，这项研究编译了4000首孟加拉歌曲的歌词、类型，并使用自然语言处理和bert算法来分析数据。 Among the 4000 songs, 1513 songs are represented for the sad mood, 1362 for the romantic mood, 886 for happiness, and the rest 239 are classified as relaxation. By embedding the lyrics of the songs, the authors have classified the songs into four moods: Happy, Sad, Romantic, and Relaxed. This research is crucial as it enables a multi-class classification of songs' moods, making the music more relatable to people's emotions. The article presents the automated result of the four moods accurately derived from the song lyrics.
</details></li>
</ul>
<hr>
<h2 id="RaTE-a-Reproducible-automatic-Taxonomy-Evaluation-by-Filling-the-Gap"><a href="#RaTE-a-Reproducible-automatic-Taxonomy-Evaluation-by-Filling-the-Gap" class="headerlink" title="RaTE: a Reproducible automatic Taxonomy Evaluation by Filling the Gap"></a>RaTE: a Reproducible automatic Taxonomy Evaluation by Filling the Gap</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09706">http://arxiv.org/abs/2307.09706</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cestlucas/rate">https://github.com/cestlucas/rate</a></li>
<li>paper_authors: Tianjian Gao, Phillipe Langlais</li>
<li>for:  automatic taxonomy evaluation (ATE) is proposed to evaluate the quality of automatically constructed taxonomies.</li>
<li>methods:  the proposed RaTE scoring procedure relies on a pre-trained language model to evaluate the quality of taxonomies without relying on manual evaluation.</li>
<li>results:  the proposed RaTE scoring procedure is found to correlate well with human judgments, and artificially degrading a taxonomy leads to a decreased RaTE score.<details>
<summary>Abstract</summary>
Taxonomies are an essential knowledge representation, yet most studies on automatic taxonomy construction (ATC) resort to manual evaluation to score proposed algorithms. We argue that automatic taxonomy evaluation (ATE) is just as important as taxonomy construction. We propose RaTE, an automatic label-free taxonomy scoring procedure, which relies on a large pre-trained language model. We apply our evaluation procedure to three state-of-the-art ATC algorithms with which we built seven taxonomies from the Yelp domain, and show that 1) RaTE correlates well with human judgments and 2) artificially degrading a taxonomy leads to decreasing RaTE score.
</details>
<details>
<summary>摘要</summary>
税onomy 是知识表现的重要部分，但大多数自动税onomy建构（ATC）研究仍然靠manual评估提出的算法。我们认为自动税onomy评估（ATE）也是非常重要的。我们提出了一个名为 RaTE 的自动无标签税onomy评估方法，它基于一个大型预训语言模型。我们将这个评估方法应用到了三种现有ATC算法，并在Yelp领域建立了七个税onomy，并显示了以下两个结果：1） RaTE 与人工评价有很好的相关性，2）让税onomy降低到人工水平会导致 RaTE 分数下降。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Guided-Generation-for-Large-Language-Models"><a href="#Efficient-Guided-Generation-for-Large-Language-Models" class="headerlink" title="Efficient Guided Generation for Large Language Models"></a>Efficient Guided Generation for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09702">http://arxiv.org/abs/2307.09702</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/normal-computing/outlines">https://github.com/normal-computing/outlines</a></li>
<li>paper_authors: Brandon T. Willard, Rémi Louf</li>
<li>for: 这个论文的目的是描述如何使用 finite-state machine 框架来帮助文本生成器生成文本。</li>
<li>methods: 这个论文使用了 regular expressions 和 context-free grammars 来导航文本生成器的词汇表，并使用了一个索引来捕捉文本生成器的结构。</li>
<li>results: 这个方法可以减少文本生成器的负载，同时允许束缚逻辑和语义约束，并且可以生成可靠的接口。对比现有的方法，这个方法显示出了明显的性能提升。<details>
<summary>Abstract</summary>
In this article we show how the problem of neural text generation can be constructively reformulated in terms of transitions between the states of a finite-state machine. This framework leads to an efficient approach to guiding text generation with regular expressions and context-free grammars by allowing the construction of an index over a language model's vocabulary. The approach is model agnostic, allows one to enforce domain-specific knowledge and constraints, and enables the construction of reliable interfaces by guaranteeing the structure of the generated text. It adds little overhead to the token sequence generation process and significantly outperforms existing solutions. An implementation is provided in the open source Python library Outlines
</details>
<details>
<summary>摘要</summary>
在这篇文章中，我们示出了如何将神经文本生成问题重新定义为finite-state机器的状态转移问题。这个框架导致了一种高效地使用正则表达式和context-free语法来引导文本生成的方法，并允许建立语言模型词汇表的索引。该方法是模型无关的，可以落实域特定的知识和约束，并使得生成的文本结构可靠。它增加了token序列生成过程中的负担，并显著超越了现有的解决方案。我们在Python库Outlines中提供了一个实现。
</details></li>
</ul>
<hr>
<h2 id="STRAPPER-Preference-based-Reinforcement-Learning-via-Self-training-Augmentation-and-Peer-Regularization"><a href="#STRAPPER-Preference-based-Reinforcement-Learning-via-Self-training-Augmentation-and-Peer-Regularization" class="headerlink" title="STRAPPER: Preference-based Reinforcement Learning via Self-training Augmentation and Peer Regularization"></a>STRAPPER: Preference-based Reinforcement Learning via Self-training Augmentation and Peer Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09692">http://arxiv.org/abs/2307.09692</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rll-research/bpref">https://github.com/rll-research/bpref</a></li>
<li>paper_authors: Yachen Kang, Li He, Jinxin Liu, Zifeng Zhuang, Donglin Wang</li>
<li>for: 本研究旨在解决 preference-based reinforcement learning (PbRL) 中人类干预的问题，即将复杂的奖励函数学习到机器人行为中。</li>
<li>methods: 研究使用了 semi-supervised learning 和 consistency regularization 来学习 PbRL 模型，并提出了一种自动训练法和 peer regularization 来提高模型的学习效果。</li>
<li>results: 实验表明，该方法可以在不同的 semi-supervised 情况下学习出高效的机器人行为，并且可以避免 similarity trap 问题，从而提高奖励学习的可靠性。<details>
<summary>Abstract</summary>
Preference-based reinforcement learning (PbRL) promises to learn a complex reward function with binary human preference. However, such human-in-the-loop formulation requires considerable human effort to assign preference labels to segment pairs, hindering its large-scale applications. Recent approache has tried to reuse unlabeled segments, which implicitly elucidates the distribution of segments and thereby alleviates the human effort. And consistency regularization is further considered to improve the performance of semi-supervised learning. However, we notice that, unlike general classification tasks, in PbRL there exits a unique phenomenon that we defined as similarity trap in this paper. Intuitively, human can have diametrically opposite preferredness for similar segment pairs, but such similarity may trap consistency regularization fail in PbRL. Due to the existence of similarity trap, such consistency regularization improperly enhances the consistency possiblity of the model's predictions between segment pairs, and thus reduces the confidence in reward learning, since the augmented distribution does not match with the original one in PbRL. To overcome such issue, we present a self-training method along with our proposed peer regularization, which penalizes the reward model memorizing uninformative labels and acquires confident predictions. Empirically, we demonstrate that our approach is capable of learning well a variety of locomotion and robotic manipulation behaviors using different semi-supervised alternatives and peer regularization.
</details>
<details>
<summary>摘要</summary>
preference-based reinforcement learning (PbRL) 承诺学习复杂的奖励函数使用二进制人类偏好。然而，这种人loop形式需要较大的人类努力来分配偏好标签对 segment pair，限制其大规模应用。 recent approaches  reuse unlabeled segments，即implicitly elucidates the distribution of segments，thereby alleviating the human effort. In addition, consistency regularization is further considered to improve the performance of semi-supervised learning. However, we notice that, unlike general classification tasks, in PbRL there exists a unique phenomenon that we defined as similarity trap in this paper. Intuitively, human can have diametrically opposite preferredness for similar segment pairs, but such similarity may trap consistency regularization fail in PbRL. Due to the existence of similarity trap, such consistency regularization improperly enhances the consistency possibility of the model's predictions between segment pairs, and thus reduces the confidence in reward learning, since the augmented distribution does not match with the original one in PbRL. To overcome such issue, we present a self-training method along with our proposed peer regularization, which penalizes the reward model memorizing uninformative labels and acquires confident predictions. Empirically, we demonstrate that our approach is capable of learning well a variety of locomotion and robotic manipulation behaviors using different semi-supervised alternatives and peer regularization.
</details></li>
</ul>
<hr>
<h2 id="Joint-Service-Caching-Communication-and-Computing-Resource-Allocation-in-Collaborative-MEC-Systems-A-DRL-based-Two-timescale-Approach"><a href="#Joint-Service-Caching-Communication-and-Computing-Resource-Allocation-in-Collaborative-MEC-Systems-A-DRL-based-Two-timescale-Approach" class="headerlink" title="Joint Service Caching, Communication and Computing Resource Allocation in Collaborative MEC Systems: A DRL-based Two-timescale Approach"></a>Joint Service Caching, Communication and Computing Resource Allocation in Collaborative MEC Systems: A DRL-based Two-timescale Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09691">http://arxiv.org/abs/2307.09691</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qianqian Liu, Haixia Zhang, Xin Zhang, Dongfeng Yuan</li>
<li>for: 本研究旨在解决多元Edge Computing（MEC）系统中的服务质量（QoS）要求问题，因为限制的多维度资源带来了很大的挑战。</li>
<li>methods: 本研究提出了一个协同MEC框架，通过Edge服务器之间的资源共享来提高长期QoS和减少缓存交换成本。该框架包括服务缓存、协同下载、计算和通信资源分配等方面的优化。</li>
<li>results: 研究结果表明，提出的算法比基eline算法在平均QoS和缓存交换成本方面具有显著优势。<details>
<summary>Abstract</summary>
Meeting the strict Quality of Service (QoS) requirements of terminals has imposed a signiffcant challenge on Multiaccess Edge Computing (MEC) systems, due to the limited multidimensional resources. To address this challenge, we propose a collaborative MEC framework that facilitates resource sharing between the edge servers, and with the aim to maximize the long-term QoS and reduce the cache switching cost through joint optimization of service caching, collaborative offfoading, and computation and communication resource allocation. The dual timescale feature and temporal recurrence relationship between service caching and other resource allocation make solving the problem even more challenging. To solve it, we propose a deep reinforcement learning (DRL)-based dual timescale scheme, called DGL-DDPG, which is composed of a short-term genetic algorithm (GA) and a long short-term memory network-based deep deterministic policy gradient (LSTM-DDPG). In doing so, we reformulate the optimization problem as a Markov decision process (MDP) where the small-timescale resource allocation decisions generated by an improved GA are taken as the states and input into a centralized LSTM-DDPG agent to generate the service caching decision for the large-timescale. Simulation results demonstrate that our proposed algorithm outperforms the baseline algorithms in terms of the average QoS and cache switching cost.
</details>
<details>
<summary>摘要</summary>
To solve this problem, we propose a deep reinforcement learning (DRL)-based dual timescale scheme, called DGL-DDPG, which consists of a short-term genetic algorithm (GA) and a long short-term memory network-based deep deterministic policy gradient (LSTM-DDPG). We reformulate the optimization problem as a Markov decision process (MDP) where the small-timescale resource allocation decisions generated by an improved GA are taken as the states and input into a centralized LSTM-DDPG agent to generate the service caching decision for the large-timescale.Simulation results show that our proposed algorithm outperforms baseline algorithms in terms of average QoS and cache switching cost.
</details></li>
</ul>
<hr>
<h2 id="Amazon-M2-A-Multilingual-Multi-locale-Shopping-Session-Dataset-for-Recommendation-and-Text-Generation"><a href="#Amazon-M2-A-Multilingual-Multi-locale-Shopping-Session-Dataset-for-Recommendation-and-Text-Generation" class="headerlink" title="Amazon-M2: A Multilingual Multi-locale Shopping Session Dataset for Recommendation and Text Generation"></a>Amazon-M2: A Multilingual Multi-locale Shopping Session Dataset for Recommendation and Text Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09688">http://arxiv.org/abs/2307.09688</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Jin, Haitao Mao, Zheng Li, Haoming Jiang, Chen Luo, Hongzhi Wen, Haoyu Han, Hanqing Lu, Zhengyang Wang, Ruirui Li, Zhen Li, Monica Xiao Cheng, Rahul Goutam, Haiyang Zhang, Karthik Subbian, Suhang Wang, Yizhou Sun, Jiliang Tang, Bing Yin, Xianfeng Tang<br>for:The paper is written to present a new multilingual dataset for session-based recommendation, which can help improve personalization and understanding of user preferences.methods:The paper uses a dataset of millions of user sessions from six different locales, with products in English, German, Japanese, French, Italian, and Spanish.results:The paper introduces three tasks based on the dataset: next-product recommendation, next-product recommendation with domain shifts, and next-product title generation. The paper also hosts a competition in the KDD CUP 2023 and attracts thousands of users and submissions.<details>
<summary>Abstract</summary>
Modeling customer shopping intentions is a crucial task for e-commerce, as it directly impacts user experience and engagement. Thus, accurately understanding customer preferences is essential for providing personalized recommendations. Session-based recommendation, which utilizes customer session data to predict their next interaction, has become increasingly popular. However, existing session datasets have limitations in terms of item attributes, user diversity, and dataset scale. As a result, they cannot comprehensively capture the spectrum of user behaviors and preferences. To bridge this gap, we present the Amazon Multilingual Multi-locale Shopping Session Dataset, namely Amazon-M2. It is the first multilingual dataset consisting of millions of user sessions from six different locales, where the major languages of products are English, German, Japanese, French, Italian, and Spanish. Remarkably, the dataset can help us enhance personalization and understanding of user preferences, which can benefit various existing tasks as well as enable new tasks. To test the potential of the dataset, we introduce three tasks in this work: (1) next-product recommendation, (2) next-product recommendation with domain shifts, and (3) next-product title generation. With the above tasks, we benchmark a range of algorithms on our proposed dataset, drawing new insights for further research and practice. In addition, based on the proposed dataset and tasks, we hosted a competition in the KDD CUP 2023 and have attracted thousands of users and submissions. The winning solutions and the associated workshop can be accessed at our website https://kddcup23.github.io/.
</details>
<details>
<summary>摘要</summary>
<<SYS>>模型客户购物意图是电商中的一项重要任务，因为它直接影响用户体验和参与度。因此，正确理解客户首选是电商中非常重要的。会话基本建议，利用客户会话数据预测他们下一次的交互，在过去几年变得越来越受欢迎。然而，现有的会话数据集受到物品属性、用户多样性和数据规模的限制，无法全面捕捉用户行为和首选。为了bridging这个差距，我们提出了亚马逊多语言多地区购物会话数据集（Amazon-M2）。这是首个多语言的数据集，包含来自六个不同的地区的数百万个用户会话，其中主要语言为英文、德语、日语、法语、意大利语和西班牙语。这个数据集可以帮助我们提高个性化和用户首选的理解，从而对电商中的既有任务也能够提供新的任务。为了评估该数据集的潜力，我们在这篇论文中引入了三个任务：（1）下一个产品推荐，（2）下一个产品推荐与领域变化，以及（3）下一个产品标题生成。我们使用这些任务对我们的提posed数据集进行了测试，从而获得了新的发现和实践经验。此外，基于我们的提posed数据集和任务，我们在KDD CUP 2023中组织了一个比赛，并吸引了千计用户和提交。赢家的解决方案和相关工作均可在我们的官方网站（https://kddcup23.github.io/）上查看。
</details></li>
</ul>
<hr>
<h2 id="Convex-Geometry-of-ReLU-layers-Injectivity-on-the-Ball-and-Local-Reconstruction"><a href="#Convex-Geometry-of-ReLU-layers-Injectivity-on-the-Ball-and-Local-Reconstruction" class="headerlink" title="Convex Geometry of ReLU-layers, Injectivity on the Ball and Local Reconstruction"></a>Convex Geometry of ReLU-layers, Injectivity on the Ball and Local Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09672">http://arxiv.org/abs/2307.09672</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/danedane-haider/alpha-rectifying-frames">https://github.com/danedane-haider/alpha-rectifying-frames</a></li>
<li>paper_authors: Daniel Haider, Martin Ehler, Peter Balazs</li>
<li>for: study the injectivity of a ReLU-layer on the closed ball of $\mathbb{R}^n$ and its non-negative part.</li>
<li>methods: use a frame-theoretic setting and convex geometry to develop a computationally feasible method for verifying the injectivity of a ReLU-layer under reasonable restrictions.</li>
<li>results: provide explicit reconstruction formulas inspired by the duality concept from frame theory, allowing for the possibility of quantifying the invertibility of a ReLU-layer and a concrete reconstruction algorithm for any input vector on the ball.Here’s the Chinese text:</li>
<li>for: 研究固定ball of $\mathbb{R}^n$中ReLU层的注入性和非负部分。</li>
<li>methods: 使用框架理论设定和几何学来开发一种可行的方法，用于在有理性限制下验证ReLU层的注入性。</li>
<li>results: 提供启发式的重构方程，基于框架理论中的对偶性概念，使得可以对ReLU层的注入性进行量化，并提供一个可行的重构算法 для任何输入向量在球体上。<details>
<summary>Abstract</summary>
The paper uses a frame-theoretic setting to study the injectivity of a ReLU-layer on the closed ball of $\mathbb{R}^n$ and its non-negative part. In particular, the interplay between the radius of the ball and the bias vector is emphasized. Together with a perspective from convex geometry, this leads to a computationally feasible method of verifying the injectivity of a ReLU-layer under reasonable restrictions in terms of an upper bound of the bias vector. Explicit reconstruction formulas are provided, inspired by the duality concept from frame theory. All this gives rise to the possibility of quantifying the invertibility of a ReLU-layer and a concrete reconstruction algorithm for any input vector on the ball.
</details>
<details>
<summary>摘要</summary>
文章使用框理 Setting来研究ReLU层在闭合球上的具体性和非负部分。特别是弹性和偏置向量之间的关系。通过几何学的视角，可以实现有理性的层逻辑检查，并提供了可行的重建方法。这些方法基于框理中的对偶概念，并且可以量化ReLU层的具体性和任意输入向量的重建。
</details></li>
</ul>
<hr>
<h2 id="JAZZVAR-A-Dataset-of-Variations-found-within-Solo-Piano-Performances-of-Jazz-Standards-for-Music-Overpainting"><a href="#JAZZVAR-A-Dataset-of-Variations-found-within-Solo-Piano-Performances-of-Jazz-Standards-for-Music-Overpainting" class="headerlink" title="JAZZVAR: A Dataset of Variations found within Solo Piano Performances of Jazz Standards for Music Overpainting"></a>JAZZVAR: A Dataset of Variations found within Solo Piano Performances of Jazz Standards for Music Overpainting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09670">http://arxiv.org/abs/2307.09670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eleanor Row, Jingjing Tang, George Fazekas</li>
<li>for: 这个论文的目的是为了提供一个基于MIDI格式的 jazz标准曲目变化搜索数据集，以便进行音乐信息检索（MIR）和音乐生成等应用。</li>
<li>methods: 该论文使用了人工提取的 jazz 演奏中的变化段落，并将其与原始曲目的旋律和和声组合成对应的原始和变化段落对。</li>
<li>results: 该论文介绍了数据集的准备和排序过程，以及对数据集的分析和一种基于Transformer模型的音乐覆盖任务的基线实现。 I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Jazz pianists often uniquely interpret jazz standards. Passages from these interpretations can be viewed as sections of variation. We manually extracted such variations from solo jazz piano performances. The JAZZVAR dataset is a collection of 502 pairs of Variation and Original MIDI segments. Each Variation in the dataset is accompanied by a corresponding Original segment containing the melody and chords from the original jazz standard. Our approach differs from many existing jazz datasets in the music information retrieval (MIR) community, which often focus on improvisation sections within jazz performances. In this paper, we outline the curation process for obtaining and sorting the repertoire, the pipeline for creating the Original and Variation pairs, and our analysis of the dataset. We also introduce a new generative music task, Music Overpainting, and present a baseline Transformer model trained on the JAZZVAR dataset for this task. Other potential applications of our dataset include expressive performance analysis and performer identification.
</details>
<details>
<summary>摘要</summary>
爵士钢琴家们经常独特地 интерпретирова jazz标准。这些 интерпретаción 中的段落可以被视为变化段落。我们手动从爵士钢琴独奏中提取出这些变化。JAZZVAR 数据集包含 502对变化和原始 MIDI 段落的对。每个变化段落在数据集中都有对应的原始段落，其中包含原始爵士标准的旋律和和声。我们的方法与许多现有的 jazz 数据集在音乐信息检索（MIR）社区中的做法不同，这些数据集通常专注于 jazz 表演中的即兴段落。在这篇文章中，我们介绍了收录和排序 репертуа 的过程，以及创建 Original 和变化对的管道。我们还介绍了我们对这些数据集的分析，以及一种基于这些数据集的新的生成音乐任务——音乐覆盖。此外，这些数据集还有表达性表演分析和演奏者识别的潜在应用。
</details></li>
</ul>
<hr>
<h2 id="Towards-A-Unified-Agent-with-Foundation-Models"><a href="#Towards-A-Unified-Agent-with-Foundation-Models" class="headerlink" title="Towards A Unified Agent with Foundation Models"></a>Towards A Unified Agent with Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09668">http://arxiv.org/abs/2307.09668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Norman Di Palo, Arunkumar Byravan, Leonard Hasenclever, Markus Wulfmeier, Nicolas Heess, Martin Riedmiller</li>
<li>for: 本研究旨在嵌入和利用语言模型和视觉语言模型的能力，以解决人工智能束缚学习（RL）Agent的基本挑战。</li>
<li>methods: 我们设计了一个框架，使用语言作为RL Agent的核心理解工具，并研究如何通过语言来解决RL中的基本挑战，如有效尝试、再利用经验数据、计划技能和从观察学习。</li>
<li>results: 我们在一个具有罕见奖励的 simulate robotic manipulation 环境中测试了我们的方法，并证明了在尝试效率和再利用经验数据方面的显著性能提升。此外，我们还 illustrate了如何通过 reuse 学习的技能来解决新任务或模仿人类专家的视频。<details>
<summary>Abstract</summary>
Language Models and Vision Language Models have recently demonstrated unprecedented capabilities in terms of understanding human intentions, reasoning, scene understanding, and planning-like behaviour, in text form, among many others. In this work, we investigate how to embed and leverage such abilities in Reinforcement Learning (RL) agents. We design a framework that uses language as the core reasoning tool, exploring how this enables an agent to tackle a series of fundamental RL challenges, such as efficient exploration, reusing experience data, scheduling skills, and learning from observations, which traditionally require separate, vertically designed algorithms. We test our method on a sparse-reward simulated robotic manipulation environment, where a robot needs to stack a set of objects. We demonstrate substantial performance improvements over baselines in exploration efficiency and ability to reuse data from offline datasets, and illustrate how to reuse learned skills to solve novel tasks or imitate videos of human experts.
</details>
<details>
<summary>摘要</summary>
language models和视觉语言模型在文本形式中表现出了无 precedent的理解人类意图、逻辑、场景理解和规划行为等能力。在这项工作中，我们调查了如何将这些能力embedded在奖励学习（RL）代理人中。我们设计了一个框架，用语言作为核心的理解工具，探索如何使得代理人通过语言来解决RL中的一系列基本挑战，如高效探索、重用经验数据、调度技能和学习从观察中学习，这些传统需要分开、垂直设计的算法。我们在一个缺乏奖励的 simulated robotic manipulation环境中测试了我们的方法，where a robot needs to stack a set of objects。我们示出了与基elinesubstantial performance improvement在探索效率和重用数据from offline datasets，并解释了如何重用已学的技能来解决新任务或模仿人类专家的视频。
</details></li>
</ul>
<hr>
<h2 id="Anticipating-Technical-Expertise-and-Capability-Evolution-in-Research-Communities-using-Dynamic-Graph-Transformers"><a href="#Anticipating-Technical-Expertise-and-Capability-Evolution-in-Research-Communities-using-Dynamic-Graph-Transformers" class="headerlink" title="Anticipating Technical Expertise and Capability Evolution in Research Communities using Dynamic Graph Transformers"></a>Anticipating Technical Expertise and Capability Evolution in Research Communities using Dynamic Graph Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09665">http://arxiv.org/abs/2307.09665</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sameera Horawalavithana, Ellyn Ayton, Anastasiya Usenko, Robin Cosbey, Svitlana Volkova</li>
<li>for: This paper aims to anticipate technical expertise and capability evolution trends in safety-critical domains like nuclear nonproliferation and rapidly emerging fields like artificial intelligence.</li>
<li>methods: The paper extends traditional statistical relational learning approaches and formulates a problem of anticipating technical expertise and capability evolution using dynamic heterogeneous graph representations. It develops novel capabilities to forecast collaboration patterns, authorship behavior, and technical capability evolution at different granularities.</li>
<li>results: The paper demonstrates that its dynamic graph transformer (DGT) models predict collaboration, partnership, and expertise patterns with high accuracy, exceeding the best-performing static graph baseline models by 30-80% across AI and NN domains. The models accurately predict which established scientists will collaborate with early career scientists and vice-versa in the AI domain.Here is the simplified Chinese text for the three key points:</li>
<li>for: 这篇论文目标是预测安全关键领域如核不扩散和人工智能等领域技术能力和能力演化趋势。</li>
<li>methods: 该论文基于传统统计关系学学习方法，并将技术能力和能力演化问题转化为动态不同类型图表示。它开发出了新的能力预测合作模式、作者行为和技术能力演化等方面的预测能力。</li>
<li>results: 该论文示出了其动态图变换器（DGT）模型在AI和NN领域的合作、合作伙伴和技术能力演化方面预测精度高，超过了最佳静止图基线模型的30-80%。DGT模型可以准确预测在AI领域中已有名气的科学家和初出茅廊的科学家之间的合作。<details>
<summary>Abstract</summary>
The ability to anticipate technical expertise and capability evolution trends globally is essential for national and global security, especially in safety-critical domains like nuclear nonproliferation (NN) and rapidly emerging fields like artificial intelligence (AI). In this work, we extend traditional statistical relational learning approaches (e.g., link prediction in collaboration networks) and formulate a problem of anticipating technical expertise and capability evolution using dynamic heterogeneous graph representations. We develop novel capabilities to forecast collaboration patterns, authorship behavior, and technical capability evolution at different granularities (e.g., scientist and institution levels) in two distinct research fields. We implement a dynamic graph transformer (DGT) neural architecture, which pushes the state-of-the-art graph neural network models by (a) forecasting heterogeneous (rather than homogeneous) nodes and edges, and (b) relying on both discrete -- and continuous -- time inputs. We demonstrate that our DGT models predict collaboration, partnership, and expertise patterns with 0.26, 0.73, and 0.53 mean reciprocal rank values for AI and 0.48, 0.93, and 0.22 for NN domains. DGT model performance exceeds the best-performing static graph baseline models by 30-80% across AI and NN domains. Our findings demonstrate that DGT models boost inductive task performance, when previously unseen nodes appear in the test data, for the domains with emerging collaboration patterns (e.g., AI). Specifically, models accurately predict which established scientists will collaborate with early career scientists and vice-versa in the AI domain.
</details>
<details>
<summary>摘要</summary>
“预测技术专业和能力发展趋势 globally 是国家和全球安全的重要因素，尤其在安全关键领域如核不扩散（NN）和快速发展的领域如人工智能（AI）。在这个工作中，我们扩展传统的统计关系学习方法（例如连接预测在合作网络中），并将问题定义为预测技术专业和能力发展。我们发展了一些新的能力来预测合作模式、作者行为和技术能力发展的不同粒度（例如科学家和机构层级）在两个不同的研究领域。我们实现了动态图 transformer（DGT）神经架构，这个架构超过了当前最好的静止图基eline模型的性能，因为它可以预测不同类型的node和edge，并且可以使用不同的时间点进行预测。我们的DGT模型在AI和NN领域中预测了合作、合作伙伴和技术能力发展的模式，其中的mean reciprocal rank值为0.26、0.73、0.53和0.48、0.93、0.22。相比于最佳的静止图基eline模型，DGT模型在AI和NN领域中的表现提高了30-80%。我们的发现证明了DGT模型可以提高induction任务的表现，当前所未见的node出现在试验数据时。具体来说，模型可以正确预测在AI领域中已成熟的科学家和初级科学家之间的合作关系。”
</details></li>
</ul>
<hr>
<h2 id="Physics-based-Reduced-Order-Modeling-for-Uncertainty-Quantification-of-Guided-Wave-Propagation-using-Bayesian-Optimization"><a href="#Physics-based-Reduced-Order-Modeling-for-Uncertainty-Quantification-of-Guided-Wave-Propagation-using-Bayesian-Optimization" class="headerlink" title="Physics-based Reduced Order Modeling for Uncertainty Quantification of Guided Wave Propagation using Bayesian Optimization"></a>Physics-based Reduced Order Modeling for Uncertainty Quantification of Guided Wave Propagation using Bayesian Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09661">http://arxiv.org/abs/2307.09661</a></li>
<li>repo_url: None</li>
<li>paper_authors: G. I. Drakoulas, T. V. Gortsas, D. Polyzos<br>for: This paper focuses on the development of a machine learning-based reduced order model (BO-ML-ROM) for guided wave propagation (GWP) in structural health monitoring (SHM) applications.methods: The proposed BO-ML-ROM is integrated with a Bayesian optimization framework to adaptively sample the parameters for training, and the finite element method is used for high-fidelity simulations.results: The results show that the BO-ML-ROM outperforms one-shot sampling methods in terms of accuracy and speed-up, and demonstrates its value for uncertainty quantification (UQ) in GWP. The predicted results also reveal the efficiency of the proposed method for SHM applications.<details>
<summary>Abstract</summary>
In the context of digital twins, structural health monitoring (SHM) constitutes the backbone of condition-based maintenance, facilitating the interconnection between virtual and physical assets. Guided wave propagation (GWP) is commonly employed for the inspection of structures in SHM. However, GWP is sensitive to variations in the material properties of the structure, leading to false alarms. In this direction, uncertainty quantification (UQ) is regularly applied to improve the reliability of predictions. Computational mechanics is a useful tool for the simulation of GWP, and is often applied for UQ. Even so, the application of UQ methods requires numerous simulations, while large-scale, transient numerical GWP solutions increase the computational cost. Reduced order models (ROMs) are commonly employed to provide numerical results in a limited amount of time. In this paper, we propose a machine learning (ML)-based ROM, mentioned as BO-ML-ROM, to decrease the computational time related to the simulation of the GWP. The ROM is integrated with a Bayesian optimization (BO) framework, to adaptively sample the parameters for the ROM training. The finite element method is used for the simulation of the high-fidelity models. The formulated ROM is used for forward UQ of the GWP in an aluminum plate with varying material properties. To determine the influence of each parameter perturbation, a global, variance-based sensitivity analysis is implemented based on Sobol' indices. It is shown that Bayesian optimization outperforms one-shot sampling methods, both in terms of accuracy and speed-up. The predicted results reveal the efficiency of BO-ML-ROM for GWP and demonstrate its value for UQ.
</details>
<details>
<summary>摘要</summary>
在数字双身的Context中，结构健康监测（SHM）是维护基于状况的核心，实现虚拟和物理资产之间的连接。广播波传播（GWP）通常用于结构的检测，但GWP受结构物理属性变化的影响，导致假警告。为了改善预测的可靠性，不确定量衡（UQ）经常应用。计算机机械是GWP的计算机模拟的有用工具，并常用于UQ。然而，UQ方法的应用需要许多 simulations，而大规模、Transient numerical GWP解决方案会增加计算成本。减少的模型（ROMs）通常用于提供数字化的结果，以降低相关的计算时间。在这篇论文中，我们提出了一种基于机器学习（ML）的减少模型，称为BO-ML-ROM，以降低GWP的计算时间。BO-ML-ROM与抽象优化（BO）框架相结合，以适应参数的参数采样。finite element方法用于高精度模型的Simulation。我们使用 Sobol 指数进行全局、基于差异的敏感分析，以确定每个参数的影响。结果表明，bayesian optimization在精度和速度两个方面都超过了一次采样方法。预测结果表明，BO-ML-ROM可以高效地用于GWP和UQ。
</details></li>
</ul>
<hr>
<h2 id="Neural-Priority-Queues-for-Graph-Neural-Networks"><a href="#Neural-Priority-Queues-for-Graph-Neural-Networks" class="headerlink" title="Neural Priority Queues for Graph Neural Networks"></a>Neural Priority Queues for Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09660">http://arxiv.org/abs/2307.09660</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rishabh Jain, Petar Veličković, Pietro Liò</li>
<li>for: 本研究旨在扩展Graph Neural Networks (GNNs)，通过添加外部存储来提高神经算法的推理能力。</li>
<li>methods: 本文提出了一种名为Neural Priority Queues的神经网络模块，该模块是一种可微分的优先队列模型，用于GNNs。</li>
<li>results: 实验结果表明，Neural PQs可以准确地捕捉长距离依赖关系，并且可以与算法性reasoning相结合。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have shown considerable success in neural algorithmic reasoning. Many traditional algorithms make use of an explicit memory in the form of a data structure. However, there has been limited exploration on augmenting GNNs with external memory. In this paper, we present Neural Priority Queues, a differentiable analogue to algorithmic priority queues, for GNNs. We propose and motivate a desiderata for memory modules, and show that Neural PQs exhibit the desiderata, and reason about their use with algorithmic reasoning. This is further demonstrated by empirical results on the CLRS-30 dataset. Furthermore, we find the Neural PQs useful in capturing long-range interactions, as empirically shown on a dataset from the Long-Range Graph Benchmark.
</details>
<details>
<summary>摘要</summary>
图 neural networks (GNNs) 在神经算法理解中表现出了considerable 的成功。许多传统算法都会使用一个显式的记忆structure，但是对于 GNNs 的外部记忆的探索却有限。在这篇论文中，我们提出了一种名为 Neural Priority Queues（神经优先队列）的可微分的优先队列，并论述了这种模块的愿景。我们还进行了对这种模块的reasoning，并通过实验结果在 CLRS-30 数据集上展示了其效果。此外，我们发现 Neural PQs 可以有效地捕捉长距离交互，如在 Long-Range Graph Benchmark 上的数据集中所示。Here's the translation of the text into Traditional Chinese:граф値 neural networks (GNNs) 在神经算法理解中表现出了considerable 的成功。许多传统算法都会使用一个显式的内存strucure，但是对于 GNNs 的外部记忆的探索却有限。在这篇论文中，我们提出了一种名为 Neural Priority Queues（神经优先队列）的可微分的优先队列，并诉述了这种模块的愿景。我们还进行了对这种模块的reasoning，并通过实验结果在 CLRS-30 数据集上展示了其效果。此外，我们发现 Neural PQs 可以有效地捕捉长距离交互，如在 Long-Range Graph Benchmark 上的数据集中所示。
</details></li>
</ul>
<hr>
<h2 id="HAT-CL-A-Hard-Attention-to-the-Task-PyTorch-Library-for-Continual-Learning"><a href="#HAT-CL-A-Hard-Attention-to-the-Task-PyTorch-Library-for-Continual-Learning" class="headerlink" title="HAT-CL: A Hard-Attention-to-the-Task PyTorch Library for Continual Learning"></a>HAT-CL: A Hard-Attention-to-the-Task PyTorch Library for Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09653">http://arxiv.org/abs/2307.09653</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xduan7/hat-cl">https://github.com/xduan7/hat-cl</a></li>
<li>paper_authors: Xiaotian Duan</li>
<li>for: 这篇论文主要针对Continual Learning中的忘记现象（Catastrophic Forgetting），以及现有的Hard-Attention-to-the-Task（HAT）机制在解决这个问题上的潜在应用。</li>
<li>methods: 该论文提出了一种名为HAT-CL的用户友好、PyTorch兼容的HAT机制重新设计，它不仅自动 manipulate梯度还可以快速地将PyTorch模块转换为HAT模块。此外，HAT-CL还提供了一套可以快速地与现有架构集成的模块，以及与TIMM库集成的准备好使用的HAT网络。</li>
<li>results: 该论文通过对多个实验来展示HAT-CL的表现，并实现了在不同的模型和应用场景中的广泛应用。此外，论文还介绍了一些新的面积 manipulate技术，这些技术在多个实验中均有显著的改进。<details>
<summary>Abstract</summary>
Catastrophic forgetting, the phenomenon in which a neural network loses previously obtained knowledge during the learning of new tasks, poses a significant challenge in continual learning. The Hard-Attention-to-the-Task (HAT) mechanism has shown potential in mitigating this problem, but its practical implementation has been complicated by issues of usability and compatibility, and a lack of support for existing network reuse. In this paper, we introduce HAT-CL, a user-friendly, PyTorch-compatible redesign of the HAT mechanism. HAT-CL not only automates gradient manipulation but also streamlines the transformation of PyTorch modules into HAT modules. It achieves this by providing a comprehensive suite of modules that can be seamlessly integrated into existing architectures. Additionally, HAT-CL offers ready-to-use HAT networks that are smoothly integrated with the TIMM library. Beyond the redesign and reimplementation of HAT, we also introduce novel mask manipulation techniques for HAT, which have consistently shown improvements across various experiments. Our work paves the way for a broader application of the HAT mechanism, opening up new possibilities in continual learning across diverse models and applications.
</details>
<details>
<summary>摘要</summary>
神经网络在学习新任务时会遇到严重的忘却现象，这个现象称为 catastrophic forgetting。这个问题对于 continual learning 造成了一定的挑战。在这篇论文中，我们介绍了一个名为 HAT-CL 的用户友好、PyTorch 相容的 HAT 机制重新设计。HAT-CL 不��ky仅自动调整Gradient，而且可以将 PyTorch 模组转换为 HAT 模组。它实现了这一点通过提供了一个完整的模组套件，可以视需要与现有的架构集成。此外，HAT-CL 还提供了一些专门的 mask 处理技术，这些技术在不同的实验中一直表现出了改善。我们的工作为 continual learning 开启了新的可能性，扩展了不同的模型和应用。
</details></li>
</ul>
<hr>
<h2 id="Application-of-BadNets-in-Spam-Filters"><a href="#Application-of-BadNets-in-Spam-Filters" class="headerlink" title="Application of BadNets in Spam Filters"></a>Application of BadNets in Spam Filters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09649">http://arxiv.org/abs/2307.09649</a></li>
<li>repo_url: None</li>
<li>paper_authors: Swagnik Roychoudhury, Akshaj Kumar Veldanda</li>
<li>for: 保护用户 FROM 不良和危险的电子邮件</li>
<li>methods: 利用机器学习模型攻击 spam 筛选器</li>
<li>results: 显示了机器学习模型供应链中的潜在漏洞，需要谨慎评估和维护 spam 筛选器<details>
<summary>Abstract</summary>
Spam filters are a crucial component of modern email systems, as they help to protect users from unwanted and potentially harmful emails. However, the effectiveness of these filters is dependent on the quality of the machine learning models that power them. In this paper, we design backdoor attacks in the domain of spam filtering. By demonstrating the potential vulnerabilities in the machine learning model supply chain, we highlight the need for careful consideration and evaluation of the models used in spam filters. Our results show that the backdoor attacks can be effectively used to identify vulnerabilities in spam filters and suggest the need for ongoing monitoring and improvement in this area.
</details>
<details>
<summary>摘要</summary>
电子邮件系统中的垃圾邮件过滤器是现代电子邮件系统中的一个重要组件，它们帮助保护用户免受不必要和 potentially 有害的电子邮件。然而，垃圾邮件过滤器的效iveness 是机器学习模型的质量所 decide。在这篇论文中，我们设计了垃圾邮件过滤器中的后门攻击。我们通过示例出现了机器学习模型供应链中的潜在漏洞，并高亮了需要小心评估和评估这些模型。我们的结果表明，后门攻击可以有效地找到垃圾邮件过滤器中的漏洞，并建议进行持续监测和改进。
</details></li>
</ul>
<hr>
<h2 id="Promoting-Exploration-in-Memory-Augmented-Adam-using-Critical-Momenta"><a href="#Promoting-Exploration-in-Memory-Augmented-Adam-using-Critical-Momenta" class="headerlink" title="Promoting Exploration in Memory-Augmented Adam using Critical Momenta"></a>Promoting Exploration in Memory-Augmented Adam using Critical Momenta</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09638">http://arxiv.org/abs/2307.09638</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chandar-lab/cmoptimizer">https://github.com/chandar-lab/cmoptimizer</a></li>
<li>paper_authors: Pranshu Malviya, Gonçalo Mordido, Aristide Baratin, Reza Babanezhad Harikandeh, Jerry Huang, Simon Lacoste-Julien, Razvan Pascanu, Sarath Chandar</li>
<li>for: 提高 Adam 优化器的性能和泛化能力。</li>
<li>methods: 提出了一种基于缓存的 Adam 优化器，通过使用缓存来增强探索较平坦的极值地形，从而提高 Adam 的性能和泛化能力。</li>
<li>results: 经验表明，该方法可以提高多种 Adam 变体在标准的supervised语言模型和图像分类任务中的性能。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Adaptive gradient-based optimizers, particularly Adam, have left their mark in training large-scale deep learning models. The strength of such optimizers is that they exhibit fast convergence while being more robust to hyperparameter choice. However, they often generalize worse than non-adaptive methods. Recent studies have tied this performance gap to flat minima selection: adaptive methods tend to find solutions in sharper basins of the loss landscape, which in turn hurts generalization. To overcome this issue, we propose a new memory-augmented version of Adam that promotes exploration towards flatter minima by using a buffer of critical momentum terms during training. Intuitively, the use of the buffer makes the optimizer overshoot outside the basin of attraction if it is not wide enough. We empirically show that our method improves the performance of several variants of Adam on standard supervised language modelling and image classification tasks.
</details>
<details>
<summary>摘要</summary>
“ adaptive 梯度下降方法，特别是 Adam，在训练大规模深度学习模型方面留下了深刻的影响。这些优化器的优点在于它们具有快速收敛和较好的参数选择不敏感性。然而，它们通常比非适应方法对于泛化性较差。 recent studies 表明，这个性能差距可以归因于扁平的最小值选择：适应方法倾向找到锋角的损失地形上的解，这会对泛化造成负面影响。为了解决这个问题，我们提出了一个新的内存增强版 Adam，使用训练过程中的缓存来增强探索，以便找到更平滑的最小值。我们 empirically 显示，我们的方法可以提高多种 Adam 的 variant 在标准的supervised language modeling 和图像识别 зада务上的性能。”Note that Simplified Chinese is a written language, and the word order and grammar may be different from Traditional Chinese, which is spoken in Taiwan and some other parts of the world.
</details></li>
</ul>
<hr>
<h2 id="Towards-Federated-Foundation-Models-Scalable-Dataset-Pipelines-for-Group-Structured-Learning"><a href="#Towards-Federated-Foundation-Models-Scalable-Dataset-Pipelines-for-Group-Structured-Learning" class="headerlink" title="Towards Federated Foundation Models: Scalable Dataset Pipelines for Group-Structured Learning"></a>Towards Federated Foundation Models: Scalable Dataset Pipelines for Group-Structured Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09619">http://arxiv.org/abs/2307.09619</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research/dataset_grouper">https://github.com/google-research/dataset_grouper</a></li>
<li>paper_authors: Zachary Charles, Nicole Mitchell, Krishna Pillutla, Michael Reneer, Zachary Garrett</li>
<li>for: 这个论文是为了实现大规模 federated learning  simulation，特别是基于基础模型的个性化适应和任务特定的适应。</li>
<li>methods: 这个论文使用了一个名为 Dataset Grouper 的库，可以创建大规模的分组结构化数据集，以便在现有的软件框架中进行 federated learning 模拟。Dataset Grouper 具有三个优势：首先，它可以处理具有很多分组的大规模数据集，而不需要将整个数据集装入内存中。其次，它允许用户根据自己的需求选择基础数据集和分区方式。最后，它是框架无关的。</li>
<li>results: 根据实验结果，使用 Dataset Grouper 可以实现大规模 federated learning 模拟，并且算法如 FedAvg 在这种规模下更像是元学习方法而不是empirical risk minimization方法，这表明它们在下游个性化和任务特定的适应中具有更高的实用性。<details>
<summary>Abstract</summary>
We introduce a library, Dataset Grouper, to create large-scale group-structured (e.g., federated) datasets, enabling federated learning simulation at the scale of foundation models. This library allows the creation of group-structured versions of existing datasets based on user-specified partitions, and directly leads to a variety of useful heterogeneous datasets that can be plugged into existing software frameworks. Dataset Grouper offers three key advantages. First, it scales to settings where even a single group's dataset is too large to fit in memory. Second, it provides flexibility, both in choosing the base (non-partitioned) dataset and in defining partitions. Finally, it is framework-agnostic. We empirically demonstrate that Dataset Grouper allows for large-scale federated language modeling simulations on datasets that are orders of magnitude larger than in previous work. Our experimental results show that algorithms like FedAvg operate more as meta-learning methods than as empirical risk minimization methods at this scale, suggesting their utility in downstream personalization and task-specific adaptation.
</details>
<details>
<summary>摘要</summary>
我们介绍一个库，Dataset Grouper，用于创建大规模群结构化（例如，联邦）数据集，以实现联邦学习的大规模模拟，与基础模型一样规模。这个库允许创建基于用户指定的分区的群结构化版本的现有数据集，直接导致一些灵活的多元数据集，可以插入现有的软件框架。Dataset Grouper具有三个关键优势：首先，它可以扩展到可以在内存中存储的数据集的规模上。其次，它提供了flexibility，可以选择基础（非分区）数据集和定义分区。最后，它是框架无关的。我们经验表明，Dataset Grouper可以实现大规模的联邦语言模型 simulations，比前一个工作中的规模多 orders of magnitude。我们的实验结果表明，在这种规模下，算法如 FedAvg 操作更像是meta-学习方法，而不是empirical risk minimization方法，这表明它们在下游个性化和任务特定适应中的 utility。
</details></li>
</ul>
<hr>
<h2 id="Gradient-strikes-back-How-filtering-out-high-frequencies-improves-explanations"><a href="#Gradient-strikes-back-How-filtering-out-high-frequencies-improves-explanations" class="headerlink" title="Gradient strikes back: How filtering out high frequencies improves explanations"></a>Gradient strikes back: How filtering out high frequencies improves explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09591">http://arxiv.org/abs/2307.09591</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sabine Muzellec, Léo Andéol, Thomas Fel, Rufin VanRullen, Thomas Serre<br>for:这篇论文旨在解释深度神经网络做出决策的原因，特别是对于新的预测基于的解释方法和older的梯度基于的方法之间的比较。methods:这篇论文使用了三种 represntative的视觉分类模型的梯度极值点的分析，以解释这些模型决策的原因。results:研究发现，梯度基于的方法存在高频噪声，而预测基于的方法则具有更好的解释性。通过应用低通滤波器，研究人员提出了一种改进梯度基于的解释方法，并证明了这种方法可以提高解释性。<details>
<summary>Abstract</summary>
Recent years have witnessed an explosion in the development of novel prediction-based attribution methods, which have slowly been supplanting older gradient-based methods to explain the decisions of deep neural networks. However, it is still not clear why prediction-based methods outperform gradient-based ones. Here, we start with an empirical observation: these two approaches yield attribution maps with very different power spectra, with gradient-based methods revealing more high-frequency content than prediction-based methods. This observation raises multiple questions: What is the source of this high-frequency information, and does it truly reflect decisions made by the system? Lastly, why would the absence of high-frequency information in prediction-based methods yield better explainability scores along multiple metrics? We analyze the gradient of three representative visual classification models and observe that it contains noisy information emanating from high-frequencies. Furthermore, our analysis reveals that the operations used in Convolutional Neural Networks (CNNs) for downsampling appear to be a significant source of this high-frequency content -- suggesting aliasing as a possible underlying basis. We then apply an optimal low-pass filter for attribution maps and demonstrate that it improves gradient-based attribution methods. We show that (i) removing high-frequency noise yields significant improvements in the explainability scores obtained with gradient-based methods across multiple models -- leading to (ii) a novel ranking of state-of-the-art methods with gradient-based methods at the top. We believe that our results will spur renewed interest in simpler and computationally more efficient gradient-based methods for explainability.
</details>
<details>
<summary>摘要</summary>
最近几年内，深度神经网络的各种新的预测基于的归因方法在发展中，慢慢地取代了老的梯度基于的方法来解释深度神经网络的决策。然而，还没有得出为什么预测基于的方法超过梯度基于的方法的原因。我们从一个实际观察开始：这两种方法生成的归因地图具有非常不同的功率спект，预测基于的方法生成的地图具有较低的高频信息，而梯度基于的方法生成的地图具有更多的高频信息。这个观察问题提出了多个问题：高频信息的来源是什么，这些信息是否真的反映系统做出的决策？最后，为什么预测基于的方法生成的地图缺乏高频信息，会导致多个维度的解释分数得到改善？我们分析了三个代表性的视觉分类模型的梯度，发现梯度包含各种干扰信息，其中大部分来自高频信息。此外，我们的分析发现，在Convolutional Neural Networks（CNNs）中用于下采样的操作会导致高频干扰信息的产生——建议可能存在干扰作用为基础。我们应用最佳低通滤波器来生成归因地图，并证明了将高频干扰信息除掉可以提高梯度基于的方法的解释分数。我们发现：（i）从高频干扰信息中除掉信息可以提高梯度基于的方法的解释分数，（ii）这些方法在多个模型上的解释分数得到了改善。我们认为，我们的结果将重新启发使用更简单和计算效率更高的梯度基于的方法来解释深度神经网络的决策。
</details></li>
</ul>
<hr>
<h2 id="Self-Compatibility-Evaluating-Causal-Discovery-without-Ground-Truth"><a href="#Self-Compatibility-Evaluating-Causal-Discovery-without-Ground-Truth" class="headerlink" title="Self-Compatibility: Evaluating Causal Discovery without Ground Truth"></a>Self-Compatibility: Evaluating Causal Discovery without Ground Truth</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09552">http://arxiv.org/abs/2307.09552</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philipp M. Faller, Leena Chennuru Vankadara, Atalanti A. Mastakouri, Francesco Locatello, Dominik Janzing</li>
<li>for: This paper is written for those interested in causal discovery and the challenges of evaluating causal discovery algorithms without ground truth.</li>
<li>methods: The paper proposes a novel method for falsifying the output of a causal discovery algorithm in the absence of ground truth, based on the idea of compatibility between causal graphs learned on different subsets of variables.</li>
<li>results: The paper shows that detecting incompatibilities between causal graphs can provide strong evidence for the correctness of the causal model, and can aid in causal model selection.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了讨论 causal discovery 和无真实参照的 causal discovery 算法评估问题而写的。</li>
<li>methods: 论文提出了一种基于不同变量集合上的 causal 图兼容性的方法来证伪 causal discovery 算法的输出，以及这种方法的可行性和有效性。</li>
<li>results: 论文表明，通过检测 causal 图兼容性可以提供强有力的证据，证明 causal 模型的正确性，并可以帮助选择最佳 causal 模型。<details>
<summary>Abstract</summary>
As causal ground truth is incredibly rare, causal discovery algorithms are commonly only evaluated on simulated data. This is concerning, given that simulations reflect common preconceptions about generating processes regarding noise distributions, model classes, and more. In this work, we propose a novel method for falsifying the output of a causal discovery algorithm in the absence of ground truth. Our key insight is that while statistical learning seeks stability across subsets of data points, causal learning should seek stability across subsets of variables. Motivated by this insight, our method relies on a notion of compatibility between causal graphs learned on different subsets of variables. We prove that detecting incompatibilities can falsify wrongly inferred causal relations due to violation of assumptions or errors from finite sample effects. Although passing such compatibility tests is only a necessary criterion for good performance, we argue that it provides strong evidence for the causal models whenever compatibility entails strong implications for the joint distribution. We also demonstrate experimentally that detection of incompatibilities can aid in causal model selection.
</details>
<details>
<summary>摘要</summary>
为了检验 causal discovery 算法的性能，通常只使用 simulated data。这种做法有一定的问题，因为模拟数据可能不准确反映真实的生成过程。在这篇文章中，我们提出了一种新的方法来证明 causal discovery 算法的输出是否正确。我们的关键发现是，在统计学习中，搜索点对点的稳定性，而 causal learning 则应该搜索变量之间的稳定性。我们根据这个发现，我们的方法基于变量之间的兼容性来判断 causal 图是否正确。我们证明，如果检测到兼容性问题，那么可以推翻由于假设或样本效应导致的错误的 causal 关系。虽然通过兼容性测试只是一个必要的条件，但我们认为它提供了强有力的证据，当兼容性意味着变量之间的共同分布时。我们还通过实验表明，检测到兼容性问题可以帮助选择合适的 causal 模型。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-sports-commentary-in-order-to-automatically-recognize-events-and-extract-insights"><a href="#Analyzing-sports-commentary-in-order-to-automatically-recognize-events-and-extract-insights" class="headerlink" title="Analyzing sports commentary in order to automatically recognize events and extract insights"></a>Analyzing sports commentary in order to automatically recognize events and extract insights</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10303">http://arxiv.org/abs/2307.10303</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yanismiraoui/analyzing-sports-commentary-in-order-to-automatically-recognize-events-and-extract-insights">https://github.com/yanismiraoui/analyzing-sports-commentary-in-order-to-automatically-recognize-events-and-extract-insights</a></li>
<li>paper_authors: Yanis Miraoui</li>
<li>for: 这个论文目的是通过不同的自然语言处理技术和方法自动识别体育活动中的主要动作。</li>
<li>methods: 论文使用了多种自然语言处理技术和方法，包括分类和情感分析，对不同来源的直播体育评论进行分析，并将主要动作分类为不同类别。</li>
<li>results: 论文发现，使用 sentiment analysis 可以帮助检测主要动作。<details>
<summary>Abstract</summary>
In this paper, we carefully investigate how we can use multiple different Natural Language Processing techniques and methods in order to automatically recognize the main actions in sports events. We aim to extract insights by analyzing live sport commentaries from different sources and by classifying these major actions into different categories. We also study if sentiment analysis could help detect these main actions.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们仔细研究了如何使用多种自然语言处理技术和方法来自动识别体育活动中的主要动作。我们希望通过分析不同来源的直播体育评论，抽出关键信息，并将这些主要动作分为不同类别。此外，我们还研究了情感分析是否可以帮助检测这些主要动作。
</details></li>
</ul>
<hr>
<h2 id="The-semantic-landscape-paradigm-for-neural-networks"><a href="#The-semantic-landscape-paradigm-for-neural-networks" class="headerlink" title="The semantic landscape paradigm for neural networks"></a>The semantic landscape paradigm for neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09550">http://arxiv.org/abs/2307.09550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shreyas Gokhale</li>
<li>for: 这篇论文旨在提供一种概念和数学框架，用于解释深度神经网络的训练剖析和性能。</li>
<li>methods: 该论文使用了 semantic landscape 概念和数学模型，描述了深度神经网络的训练剖析为图形的交通动力学问题。</li>
<li>results: 研究发现，深度神经网络的训练剖析和性能可以通过Random Walk 和孤立点的统计学来解释，并且这些现象与物理学中的吸附和爆炸现象有关。<details>
<summary>Abstract</summary>
Deep neural networks exhibit a fascinating spectrum of phenomena ranging from predictable scaling laws to the unpredictable emergence of new capabilities as a function of training time, dataset size and network size. Analysis of these phenomena has revealed the existence of concepts and algorithms encoded within the learned representations of these networks. While significant strides have been made in explaining observed phenomena separately, a unified framework for understanding, dissecting, and predicting the performance of neural networks is lacking. Here, we introduce the semantic landscape paradigm, a conceptual and mathematical framework that describes the training dynamics of neural networks as trajectories on a graph whose nodes correspond to emergent algorithms that are instrinsic to the learned representations of the networks. This abstraction enables us to describe a wide range of neural network phenomena in terms of well studied problems in statistical physics. Specifically, we show that grokking and emergence with scale are associated with percolation phenomena, and neural scaling laws are explainable in terms of the statistics of random walks on graphs. Finally, we discuss how the semantic landscape paradigm complements existing theoretical and practical approaches aimed at understanding and interpreting deep neural networks.
</details>
<details>
<summary>摘要</summary>
深度神经网络展现出非常有趣的spectrum，从可预测的尺度法则到由训练时间、数据集大小和网络大小决定的新功能的不可预测出现。对这些现象的分析发现了神经网络学习的表示中隐藏的概念和算法。虽然在解释每个现象 separately 已经做出了很大的进步，但是一个综合的框架 для理解、分解和预测神经网络的性能是缺失的。在这里，我们引入 semantic landscape 概念，它是一种概念和数学框架，用于描述神经网络的训练剖ogram，其节点对应于神经网络学习的表示中隐藏的算法。这种抽象使得可以将各种神经网络现象描述为已知的统计物理问题。具体来说，我们显示了感知和emergence with scale 与聚变现象相关，而神经网络尺度法则可以通过图形的随机游走统计来解释。最后，我们讨论了semantic landscape 概念如何补充现有的理论和实践方法，以便更好地理解和解释深度神经网络。
</details></li>
</ul>
<hr>
<h2 id="DreaMR-Diffusion-driven-Counterfactual-Explanation-for-Functional-MRI"><a href="#DreaMR-Diffusion-driven-Counterfactual-Explanation-for-Functional-MRI" class="headerlink" title="DreaMR: Diffusion-driven Counterfactual Explanation for Functional MRI"></a>DreaMR: Diffusion-driven Counterfactual Explanation for Functional MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09547">http://arxiv.org/abs/2307.09547</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/icon-lab/dreamr">https://github.com/icon-lab/dreamr</a></li>
<li>paper_authors: Hasan Atakan Bedel, Tolga Çukur</li>
<li>for: 本研究旨在提供高特异性、可信度和准确性的fMRI解释方法，以便更好地理解脑响应和认知状态之间的关系。</li>
<li>methods: 本研究使用了 diffusion-driven counterfactual method，称为DreaMR，来生成高特异性、可信度和准确性的fMRI样本。DreaMR使用了分数多阶段蒸发准则来提高抽取效率，并使用了 transformer 架构来考虑长距离空间时间 Context。</li>
<li>results: 对于 neuroimaging 数据集，DreaMR 的实验结果表明，它在比 state-of-the-art counterfactual 方法更高的特异性、可信度和效率下生成样本。<details>
<summary>Abstract</summary>
Deep learning analyses have offered sensitivity leaps in detection of cognitive states from functional MRI (fMRI) measurements across the brain. Yet, as deep models perform hierarchical nonlinear transformations on their input, interpreting the association between brain responses and cognitive states is challenging. Among common explanation approaches for deep fMRI classifiers, attribution methods show poor specificity and perturbation methods show limited plausibility. While counterfactual generation promises to address these limitations, previous methods use variational or adversarial priors that yield suboptimal sample fidelity. Here, we introduce the first diffusion-driven counterfactual method, DreaMR, to enable fMRI interpretation with high specificity, plausibility and fidelity. DreaMR performs diffusion-based resampling of an input fMRI sample to alter the decision of a downstream classifier, and then computes the minimal difference between the original and counterfactual samples for explanation. Unlike conventional diffusion methods, DreaMR leverages a novel fractional multi-phase-distilled diffusion prior to improve sampling efficiency without compromising fidelity, and it employs a transformer architecture to account for long-range spatiotemporal context in fMRI scans. Comprehensive experiments on neuroimaging datasets demonstrate the superior specificity, fidelity and efficiency of DreaMR in sample generation over state-of-the-art counterfactual methods for fMRI interpretation.
</details>
<details>
<summary>摘要</summary>
深度学习分析已经实现了脑功能成像（fMRI）测量中识别认知状态的敏感性大幅提升。然而，由于深度模型对输入数据进行堆叠非线性变换，因此解释脑响应和认知状态之间的关系是困难的。常见的解释方法 для深度fMRI分类器包括负权重分布和挑战性分布，但这些方法具有较低的特点和可信度。在这些限制下，Counterfactual生成技术被提出，可以解决这些问题。在这种情况下，我们介绍了第一个Diffusion驱动的Counterfactual方法，即DreaMR，它可以在高特点和可信度下提供fMRI解释。DreaMR通过对输入fMRI样本进行Diffusion基于扩散的重新抽样，然后计算抽样和原始样本之间的最小差异来进行解释。与传统的Diffusion方法不同，DreaMR利用了一种新的分数多相阶段混合抽样法，以提高抽样效率而不损失可靠性，同时使用了一种变换器架构来考虑fMRI扫描中的长距离空间时间关系。经过了多个neuroimaging数据集的实验，我们发现DreaMR在样本生成方面比前STATE-OF-THE-ART counterfactual方法更高的特点和可信度。
</details></li>
</ul>
<hr>
<h2 id="Can-Neural-Network-Memorization-Be-Localized"><a href="#Can-Neural-Network-Memorization-Be-Localized" class="headerlink" title="Can Neural Network Memorization Be Localized?"></a>Can Neural Network Memorization Be Localized?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09542">http://arxiv.org/abs/2307.09542</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pratyushmaini/localizing-memorization">https://github.com/pratyushmaini/localizing-memorization</a></li>
<li>paper_authors: Pratyush Maini, Michael C. Mozer, Hanie Sedghi, Zachary C. Lipton, J. Zico Kolter, Chiyuan Zhang</li>
<li>for: 本研究探讨了深度过参数网络中 memorization 和通用化的关系，并提出了一种新的 dropout 方法。</li>
<li>methods: 本研究使用了 gradient accounting、layer rewinding 和 retraining 等三种实验来证明，memorization 不仅局限于最后几层，而且可以在不同层的多个 neuron 中发现。</li>
<li>results: 研究发现，memorization 通常局限于模型中很少的 neuron 或通道（大约5），并且可以通过 example-tied dropout 方法来引导模型对特定示例进行 memorization。这种方法可以减少模型对 memorized 示例的准确率从 100% 降至 3%，同时也降低了通用化差距。<details>
<summary>Abstract</summary>
Recent efforts at explaining the interplay of memorization and generalization in deep overparametrized networks have posited that neural networks $\textit{memorize}$ "hard" examples in the final few layers of the model. Memorization refers to the ability to correctly predict on $\textit{atypical}$ examples of the training set. In this work, we show that rather than being confined to individual layers, memorization is a phenomenon confined to a small set of neurons in various layers of the model. First, via three experimental sources of converging evidence, we find that most layers are redundant for the memorization of examples and the layers that contribute to example memorization are, in general, not the final layers. The three sources are $\textit{gradient accounting}$ (measuring the contribution to the gradient norms from memorized and clean examples), $\textit{layer rewinding}$ (replacing specific model weights of a converged model with previous training checkpoints), and $\textit{retraining}$ (training rewound layers only on clean examples). Second, we ask a more generic question: can memorization be localized $\textit{anywhere}$ in a model? We discover that memorization is often confined to a small number of neurons or channels (around 5) of the model. Based on these insights we propose a new form of dropout -- $\textit{example-tied dropout}$ that enables us to direct the memorization of examples to an apriori determined set of neurons. By dropping out these neurons, we are able to reduce the accuracy on memorized examples from $100\%\to3\%$, while also reducing the generalization gap.
</details>
<details>
<summary>摘要</summary>
Recent research has focused on the interplay between memorization and generalization in deep overparametrized networks, suggesting that neural networks memorize "hard" examples in the final few layers of the model. Memorization refers to the ability to correctly predict atypical examples in the training set. In this study, we found that memorization is not limited to individual layers, but rather is confined to a small set of neurons in various layers of the model.First, we provided three lines of evidence to support this claim:1. Gradient accounting: We measured the contribution to the gradient norms from memorized and clean examples, and found that most layers are redundant for memorization, and the layers that contribute to example memorization are not the final layers.2. Layer rewinding: We replaced specific model weights of a converged model with previous training checkpoints, and found that memorization is not limited to the final layers.3. Retraining: We trained rewound layers only on clean examples, and found that memorization is often confined to a small number of neurons or channels (around 5) of the model.Based on these insights, we proposed a new form of dropout called "example-tied dropout" that enables us to direct the memorization of examples to an apriori determined set of neurons. By dropping out these neurons, we were able to reduce the accuracy on memorized examples from 100% to 3%, while also reducing the generalization gap.
</details></li>
</ul>
<hr>
<h2 id="Forecasting-the-steam-mass-flow-in-a-powerplant-using-the-parallel-hybrid-network"><a href="#Forecasting-the-steam-mass-flow-in-a-powerplant-using-the-parallel-hybrid-network" class="headerlink" title="Forecasting the steam mass flow in a powerplant using the parallel hybrid network"></a>Forecasting the steam mass flow in a powerplant using the parallel hybrid network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09483">http://arxiv.org/abs/2307.09483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrii Kurkin, Jonas Hegemann, Mo Kordzanganeh, Alexey Melnikov</li>
<li>for: 提高热电厂的操作效率和成本 reduction</li>
<li>methods: 使用并行混合神经网络架构，结合参数化量子电路和常见的批处理神经网络，进行热质流量预测</li>
<li>results: 相比单独的古典和量子模型，并行混合模型可以更好地预测热质流量15分钟内的趋势，其MSE损失值较低，相对error between ground truth and model predictions also smaller.<details>
<summary>Abstract</summary>
Efficient and sustainable power generation is a crucial concern in the energy sector. In particular, thermal power plants grapple with accurately predicting steam mass flow, which is crucial for operational efficiency and cost reduction. In this study, we use a parallel hybrid neural network architecture that combines a parametrized quantum circuit and a conventional feed-forward neural network specifically designed for time-series prediction in industrial settings to enhance predictions of steam mass flow 15 minutes into the future. Our results show that the parallel hybrid model outperforms standalone classical and quantum models, achieving more than 5.7 and 4.9 times lower mean squared error (MSE) loss on the test set after training compared to pure classical and pure quantum networks, respectively. Furthermore, the hybrid model demonstrates smaller relative errors between the ground truth and the model predictions on the test set, up to 2 times better than the pure classical model. These findings contribute to the broader scientific understanding of how integrating quantum and classical machine learning techniques can be applied to real-world challenges faced by the energy sector, ultimately leading to optimized power plant operations.
</details>
<details>
<summary>摘要</summary>
efficient和可持续的电力生产是能源领域的关键问题。特别是热电厂面临精准预测蒸汽质量流量是操作效率和成本减少的关键。在这项研究中，我们使用并行的混合神经网络架构，将Parametrized Quantum Circuit和常规往返神经网络特性为时间序列预测在工业设置中进行整合，以提高蒸汽质量流量15分钟后的预测。我们的结果表明，并行混合模型比纯经典和量子网络模型更高效，在测试集上培育后的MSE损失下降了5.7和4.9倍，分别比纯经典和量子网络模型更低。此外，混合模型在测试集上与真实值之间的相对误差也更小，达到了2倍于纯经典模型。这些发现对把量子和经典机器学习技术应用于能源领域的实际挑战提供了贡献，ultimately leading to optimized power plant operations.
</details></li>
</ul>
<hr>
<h2 id="Overthinking-the-Truth-Understanding-how-Language-Models-Process-False-Demonstrations"><a href="#Overthinking-the-Truth-Understanding-how-Language-Models-Process-False-Demonstrations" class="headerlink" title="Overthinking the Truth: Understanding how Language Models Process False Demonstrations"></a>Overthinking the Truth: Understanding how Language Models Process False Demonstrations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09476">http://arxiv.org/abs/2307.09476</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dannyallover/overthinking_the_truth">https://github.com/dannyallover/overthinking_the_truth</a></li>
<li>paper_authors: Danny Halawi, Jean-Stanislas Denain, Jacob Steinhardt</li>
<li>for: 研究模型在几拍学习中复制困难任务的能力，并探究模型在复制过程中可能会复制错误或危险内容。</li>
<li>methods: 通过分析模型的内部表示，研究模型在复制过程中可能会出现的两种现象：过度思考和假推导头。</li>
<li>results: 研究发现，在 Early layers 中，正确和错误示例都可以induce相似的模型行为，但是在某个”极限层” 之后，正确示例的准确率逐渐下降，而错误示例的准确率则保持相对高。此外，假推导头可能是过度思考的一个可能机制，它们在 Late layers 中 attend 并复制错误信息，并且它们的消除可以减少过度思考。<details>
<summary>Abstract</summary>
Modern language models can imitate complex patterns through few-shot learning, enabling them to complete challenging tasks without fine-tuning. However, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context. We study harmful imitation through the lens of a model's internal representations, and identify two related phenomena: overthinking and false induction heads. The first phenomenon, overthinking, appears when we decode predictions from intermediate layers, given correct vs. incorrect few-shot demonstrations. At early layers, both demonstrations induce similar model behavior, but the behavior diverges sharply at some "critical layer", after which the accuracy given incorrect demonstrations progressively decreases. The second phenomenon, false induction heads, are a possible mechanistic cause of overthinking: these are heads in late layers that attend to and copy false information from previous demonstrations, and whose ablation reduces overthinking. Beyond scientific understanding, our results suggest that studying intermediate model computations could be a promising avenue for understanding and guarding against harmful model behaviors.
</details>
<details>
<summary>摘要</summary>
现代语言模型可以通过几招学习模式来模仿复杂的模式，从而完成复杂任务 без fine-tuning。然而，模仿也可能导致模型复制错误或有害内容，如果在上下文中存在。我们通过模型内部表示的视角来研究危险的模仿行为，并发现了两种相关的现象：过度思考和假推导头。首先，过度思考发生在在中间层次处decode预测结果时，当给出正确和错误几招示范时。在早期层次上，两个示范都会导致模型的行为相似，但是行为在某个"极限层"之后开始分别下降，并且在这个层次上预测错误的情况下，精度逐渐下降。其次，假推导头是可能导致过度思考的机制之一：这些是在后期层次上的头部，它们会从前一个示范中复制错误信息，并且它们的除去可以减少过度思考。我们的研究结果表明，研究模型的中间计算可能是了解和防止模型危险行为的有效途径。
</details></li>
</ul>
<hr>
<h2 id="A-Cryogenic-Memristive-Neural-Decoder-for-Fault-tolerant-Quantum-Error-Correction"><a href="#A-Cryogenic-Memristive-Neural-Decoder-for-Fault-tolerant-Quantum-Error-Correction" class="headerlink" title="A Cryogenic Memristive Neural Decoder for Fault-tolerant Quantum Error Correction"></a>A Cryogenic Memristive Neural Decoder for Fault-tolerant Quantum Error Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09463">http://arxiv.org/abs/2307.09463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frédéric Marcotte, Pierre-Antoine Mouny, Victor Yon, Gebremedhin A. Dagnew, Bohdan Kulchytskyy, Sophie Rochette, Yann Beilliard, Dominique Drouin, Pooya Ronagh</li>
<li>for: 这个论文主要target是为了提出一种基于卷积 neural network的量子错误修复（QEC）方法，以提高量子计算机系统的可靠性和稳定性。</li>
<li>methods: 这个论文使用了一种基于卷积 neural network的扩充器来加速量子错误修复的幂等处理，并使用了一种卷积 память设备来实现分形 matrix-vector 乘法。</li>
<li>results: 研究人员通过数值实验和实际测量来 investigate the impact of TiO$_\textrm{x}$-based memristive devices’ non-idealities on decoding accuracy,并开发了一种硬件意识的训练方法来减轻这些影响，以实现一种 Pseudo-threshold 值为 $9.23\times 10^{-4}$。<details>
<summary>Abstract</summary>
Neural decoders for quantum error correction (QEC) rely on neural networks to classify syndromes extracted from error correction codes and find appropriate recovery operators to protect logical information against errors. Despite the good performance of neural decoders, important practical requirements remain to be achieved, such as minimizing the decoding time to meet typical rates of syndrome generation in repeated error correction schemes, and ensuring the scalability of the decoding approach as the code distance increases. Designing a dedicated integrated circuit to perform the decoding task in co-integration with a quantum processor appears necessary to reach these decoding time and scalability requirements, as routing signals in and out of a cryogenic environment to be processed externally leads to unnecessary delays and an eventual wiring bottleneck. In this work, we report the design and performance analysis of a neural decoder inference accelerator based on an in-memory computing (IMC) architecture, where crossbar arrays of resistive memory devices are employed to both store the synaptic weights of the decoder neural network and perform analog matrix-vector multiplications during inference. In proof-of-concept numerical experiments supported by experimental measurements, we investigate the impact of TiO$_\textrm{x}$-based memristive devices' non-idealities on decoding accuracy. Hardware-aware training methods are developed to mitigate the loss in accuracy, allowing the memristive neural decoders to achieve a pseudo-threshold of $9.23\times 10^{-4}$ for the distance-three surface code, whereas the equivalent digital neural decoder achieves a pseudo-threshold of $1.01\times 10^{-3}$. This work provides a pathway to scalable, fast, and low-power cryogenic IMC hardware for integrated QEC.
</details>
<details>
<summary>摘要</summary>
neural decoders for quantum error correction (QEC) 利用神经网络来分类错误码和找到适当的恢复操作器，以保护逻辑信息免受错误的影响。 despite the good performance of neural decoders, there are still important practical requirements to be met, such as reducing the decoding time to meet the typical rates of syndrome generation in repeated error correction schemes, and ensuring the scalability of the decoding approach as the code distance increases. 设计专门的集成电路来执行解码任务和Quantum Processor co-integration appears necessary to reach these decoding time and scalability requirements, as routing signals in and out of a cryogenic environment to be processed externally leads to unnecessary delays and an eventual wiring bottleneck.In this work, we report the design and performance analysis of a neural decoder inference accelerator based on an in-memory computing (IMC) architecture, where crossbar arrays of resistive memory devices are employed to both store the synaptic weights of the decoder neural network and perform analog matrix-vector multiplications during inference. In proof-of-concept numerical experiments supported by experimental measurements, we investigate the impact of TiO$_\textrm{x}$-based memristive devices' non-idealities on decoding accuracy. Hardware-aware training methods are developed to mitigate the loss in accuracy, allowing the memristive neural decoders to achieve a pseudo-threshold of $9.23\times 10^{-4}$ for the distance-three surface code, whereas the equivalent digital neural decoder achieves a pseudo-threshold of $1.01\times 10^{-3}$. This work provides a pathway to scalable, fast, and low-power cryogenic IMC hardware for integrated QEC.
</details></li>
</ul>
<hr>
<h2 id="Does-Circuit-Analysis-Interpretability-Scale-Evidence-from-Multiple-Choice-Capabilities-in-Chinchilla"><a href="#Does-Circuit-Analysis-Interpretability-Scale-Evidence-from-Multiple-Choice-Capabilities-in-Chinchilla" class="headerlink" title="Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla"></a>Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09458">http://arxiv.org/abs/2307.09458</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tom Lieberum, Matthew Rahtz, János Kramár, Neel Nanda, Geoffrey Irving, Rohin Shah, Vladimir Mikulik</li>
<li>for: This paper aims to test the scalability of circuit analysis on a large language model (Chinchilla) and understand the internal mechanisms of the model’s correct answer identification.</li>
<li>methods: The paper uses existing techniques such as logit attribution, attention pattern visualization, and activation patching to analyze the model’s behavior, and applies these techniques to the 70B Chinchilla model.</li>
<li>results: The paper finds that the existing techniques can be applied to Chinchilla with mixed results, and identifies a small set of <code>output nodes&#39; (attention heads and MLPs) that are responsible for correct answer identification. Additionally, the paper shows that the query and key subspaces of the </code>correct letter’ heads represent an &#96;Nth item in an enumeration’ feature, but this explanation is only partial and there is more to learn about the heads’ behavior on a more general distribution.<details>
<summary>Abstract</summary>
\emph{Circuit analysis} is a promising technique for understanding the internal mechanisms of language models. However, existing analyses are done in small models far from the state of the art. To address this, we present a case study of circuit analysis in the 70B Chinchilla model, aiming to test the scalability of circuit analysis. In particular, we study multiple-choice question answering, and investigate Chinchilla's capability to identify the correct answer \emph{label} given knowledge of the correct answer \emph{text}. We find that the existing techniques of logit attribution, attention pattern visualization, and activation patching naturally scale to Chinchilla, allowing us to identify and categorize a small set of `output nodes' (attention heads and MLPs).   We further study the `correct letter' category of attention heads aiming to understand the semantics of their features, with mixed results. For normal multiple-choice question answers, we significantly compress the query, key and value subspaces of the head without loss of performance when operating on the answer labels for multiple-choice questions, and we show that the query and key subspaces represent an `Nth item in an enumeration' feature to at least some extent. However, when we attempt to use this explanation to understand the heads' behaviour on a more general distribution including randomized answer labels, we find that it is only a partial explanation, suggesting there is more to learn about the operation of `correct letter' heads on multiple choice question answering.
</details>
<details>
<summary>摘要</summary>
\emph{电路分析} 是一种有前途的技术，用于理解语言模型的内部机制。然而，现有的分析都是在小型模型上进行的，远离当前领域的状态。为了解决这个问题，我们提出了一个案例研究，在70B Chinchilla模型中进行电路分析。具体来说，我们研究了多选题回答问题，并研究了 Chinchilla 是否可以根据正确的答案文本来确定正确的答案标签。我们发现，现有的逻辑拟合、注意力图像化和活动覆盖技术可以自然地扩展到 Chinchilla，allowing us to identify和分类一小组`输出节点'（注意头和 MLP）。我们进一步研究了 `正确字符' 类型的注意头，以了解其特征semantics。结果表明，对于常见的多选题答案，我们可以压缩查询、键和值子空间中的头部分，无损性能。此外，我们发现，查询和键子空间代表了 `N个元素的排序' 特征，至少在一定程度上。然而，当我们尝试使用这种解释来理解 `正确字符' 头的行为时，我们发现这只是一个 partial explanation， suggesting there is more to learn about the operation of `correct letter' heads on multiple choice question answering.
</details></li>
</ul>
<hr>
<h2 id="Smooth-Attention-for-Deep-Multiple-Instance-Learning-Application-to-CT-Intracranial-Hemorrhage-Detection"><a href="#Smooth-Attention-for-Deep-Multiple-Instance-Learning-Application-to-CT-Intracranial-Hemorrhage-Detection" class="headerlink" title="Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection"></a>Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09457">http://arxiv.org/abs/2307.09457</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yunanwu2168/sa-mil">https://github.com/yunanwu2168/sa-mil</a></li>
<li>paper_authors: Yunan Wu, Francisco M. Castro-Macías, Pablo Morales-Álvarez, Rafael Molina, Aggelos K. Katsaggelos</li>
<li>for: 这个研究旨在提出一种基于深度学习的多个实例学习（MIL）模型，以优化医疗图像诊断中的bag label unknown instance label问题。</li>
<li>methods: 该模型使用了smooth attention mechanism来保证每个实例在bag中受到的注意程度是相似的，从而学习了bag中实例之间的 spatial dependencies。</li>
<li>results: 对于头CT扫描图像中的脑内出血检测，该模型比非smooth attention MIL模型在scan和slice水平上都有更好的表现，并且超越了当前的state-of-the-art MIL方法。<details>
<summary>Abstract</summary>
Multiple Instance Learning (MIL) has been widely applied to medical imaging diagnosis, where bag labels are known and instance labels inside bags are unknown. Traditional MIL assumes that instances in each bag are independent samples from a given distribution. However, instances are often spatially or sequentially ordered, and one would expect similar diagnostic importance for neighboring instances. To address this, in this study, we propose a smooth attention deep MIL (SA-DMIL) model. Smoothness is achieved by the introduction of first and second order constraints on the latent function encoding the attention paid to each instance in a bag. The method is applied to the detection of intracranial hemorrhage (ICH) on head CT scans. The results show that this novel SA-DMIL: (a) achieves better performance than the non-smooth attention MIL at both scan (bag) and slice (instance) levels; (b) learns spatial dependencies between slices; and (c) outperforms current state-of-the-art MIL methods on the same ICH test set.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Convergent-regularization-in-inverse-problems-and-linear-plug-and-play-denoisers"><a href="#Convergent-regularization-in-inverse-problems-and-linear-plug-and-play-denoisers" class="headerlink" title="Convergent regularization in inverse problems and linear plug-and-play denoisers"></a>Convergent regularization in inverse problems and linear plug-and-play denoisers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09441">http://arxiv.org/abs/2307.09441</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andreas Hauptmann, Subhadip Mukherjee, Carola-Bibiane Schönlieb, Ferdia Sherry</li>
<li>for: 这篇论文的目的是研究插入式减噪（PnP）方法在图像反问题中的收敛性和稳定性。</li>
<li>methods: 这篇论文使用了经典的正则化理论和最近的数据驱动方法来研究PnP方法的收敛性。</li>
<li>results: 该论文提出了一种新的spectral filtering技术来控制PnP方法中的正则化强度，并证明了PnP方法在linear denoiser情况下是一种收敛的正则化方案。<details>
<summary>Abstract</summary>
Plug-and-play (PnP) denoising is a popular iterative framework for solving imaging inverse problems using off-the-shelf image denoisers. Their empirical success has motivated a line of research that seeks to understand the convergence of PnP iterates under various assumptions on the denoiser. While a significant amount of research has gone into establishing the convergence of the PnP iteration for different regularity conditions on the denoisers, not much is known about the asymptotic properties of the converged solution as the noise level in the measurement tends to zero, i.e., whether PnP methods are provably convergent regularization schemes under reasonable assumptions on the denoiser. This paper serves two purposes: first, we provide an overview of the classical regularization theory in inverse problems and survey a few notable recent data-driven methods that are provably convergent regularization schemes. We then continue to discuss PnP algorithms and their established convergence guarantees. Subsequently, we consider PnP algorithms with linear denoisers and propose a novel spectral filtering technique to control the strength of regularization arising from the denoiser. Further, by relating the implicit regularization of the denoiser to an explicit regularization functional, we rigorously show that PnP with linear denoisers leads to a convergent regularization scheme. More specifically, we prove that in the limit as the noise vanishes, the PnP reconstruction converges to the minimizer of a regularization potential subject to the solution satisfying the noiseless operator equation. The theoretical analysis is corroborated by numerical experiments for the classical inverse problem of tomographic image reconstruction.
</details>
<details>
<summary>摘要</summary>
固定-插入（PnP）去噪是一种流行的迭代框架，用于解决图像逆问题。它们的实际成功激发了一条研究，旨在理解PnP迭代的收敛性，以及其解决图像逆问题的效果。虽然一些研究已经证明了PnP迭代的收敛性，但尚不多了关于随着测量噪声水平的下降，解决图像逆问题的稳定性和准确性的研究。这篇论文旨在两个目的：首先，提供经典的反射理论的概述，以及一些近期的数据驱动方法，这些方法是已知的收敛性规则。然后，我们继续讨论PnP算法，并证明它们的收敛性保证。接着，我们考虑PnP算法与线性去噪器的结合，并提出一种新的频谱筛选技术，用于控制由去噪器带来的正则化强度。最后，我们正式证明PnP算法与线性去噪器的结合是一种收敛的正则化方案，并且在随着噪声水平下降，PnP重建结果会 converges到一个具有正则化潜在能量的最小值解。实际分析证明了这一结论的正确性。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Conditional-Slot-Attention-for-Object-Centric-Learning"><a href="#Unsupervised-Conditional-Slot-Attention-for-Object-Centric-Learning" class="headerlink" title="Unsupervised Conditional Slot Attention for Object Centric Learning"></a>Unsupervised Conditional Slot Attention for Object Centric Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09437">http://arxiv.org/abs/2307.09437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Avinash Kori, Francesco Locatello, Francesca Toni, Ben Glocker</li>
<li>for: 本研究旨在提出一种不需要监督的方法，以学习对象中心表示，并解决多个对象实例绑定到专门的对象槽中的问题。</li>
<li>methods: 本研究使用了iterative attention来学习可组合的表示，并使用了一个新的概率槽词典（PSD）来实现特定对象槽绑定。</li>
<li>results: 研究发现，使用我们提出的方法可以在多个下游任务中提供场景组合能力，并在一些几个步骤适应性Visual reasoning任务中提供了显著的提升，同时与槽注意力在对象发现任务中表现相似或更好。<details>
<summary>Abstract</summary>
Extracting object-level representations for downstream reasoning tasks is an emerging area in AI. Learning object-centric representations in an unsupervised setting presents multiple challenges, a key one being binding an arbitrary number of object instances to a specialized object slot. Recent object-centric representation methods like Slot Attention utilize iterative attention to learn composable representations with dynamic inference level binding but fail to achieve specialized slot level binding. To address this, in this paper we propose Unsupervised Conditional Slot Attention using a novel Probabilistic Slot Dictionary (PSD). We define PSD with (i) abstract object-level property vectors as key and (ii) parametric Gaussian distribution as its corresponding value. We demonstrate the benefits of the learnt specific object-level conditioning distributions in multiple downstream tasks, namely object discovery, compositional scene generation, and compositional visual reasoning. We show that our method provides scene composition capabilities and a significant boost in a few shot adaptability tasks of compositional visual reasoning, while performing similarly or better than slot attention in object discovery tasks
</details>
<details>
<summary>摘要</summary>
“抽取对象级别表示是人工智能领域的一个emerging领域。在无监督的setting下学习对象中心的表示呈poses多个挑战，其中一个关键问题是将多个对象实例绑定到特殊的对象槽。现有的对象中心表示方法如Slot Attention使用迭代注意力学习可组合表示，但它们无法实现特殊槽级别的绑定。为解决这个问题，本文提出了无监督条件槽注意力（UCSD），使用一种新的概率槽词典（PSD）。我们定义PSD中的键为抽取对象级别性质 вектор，值为参数化的高斯分布。我们示出了学习的特定对象级别conditioning分布的多个下游任务的好处，包括对象发现、compositional scene generation和compositional visual reasoning。我们表明了我们的方法在多个shot adaptability任务中提供了场景组合能力，并在对象发现任务中与槽注意力相当或更好的表现”
</details></li>
</ul>
<hr>
<h2 id="Scaling-Laws-for-Imitation-Learning-in-NetHack"><a href="#Scaling-Laws-for-Imitation-Learning-in-NetHack" class="headerlink" title="Scaling Laws for Imitation Learning in NetHack"></a>Scaling Laws for Imitation Learning in NetHack</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09423">http://arxiv.org/abs/2307.09423</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jens Tuyls, Dhruv Madeka, Kari Torkkola, Dean Foster, Karthik Narasimhan, Sham Kakade</li>
<li>for: 这个论文旨在研究 whether carefully scaling up model and data size can bring improvements in the imitation learning setting.</li>
<li>methods: 作者使用了 Inspired by recent work in Natural Language Processing (NLP) where “scaling up” has resulted in increasingly more capable LLMs, 并在 NetHack 游戏中进行了实验。</li>
<li>results: 作者发现 IL 损失和 mean return 与 compute budget 相关， resulting in power laws for training compute-optimal IL agents with respect to model size and number of samples。 In addition, 作者发现这些 agent 在所有设置下都比之前的状态艺术高出至少 2x。<details>
<summary>Abstract</summary>
Imitation Learning (IL) is one of the most widely used methods in machine learning. Yet, while powerful, many works find it is often not able to fully recover the underlying expert behavior. However, none of these works deeply investigate the role of scaling up the model and data size. Inspired by recent work in Natural Language Processing (NLP) where "scaling up" has resulted in increasingly more capable LLMs, we investigate whether carefully scaling up model and data size can bring similar improvements in the imitation learning setting. To demonstrate our findings, we focus on the game of NetHack, a challenging environment featuring procedural generation, stochasticity, long-term dependencies, and partial observability. We find IL loss and mean return scale smoothly with the compute budget and are strongly correlated, resulting in power laws for training compute-optimal IL agents with respect to model size and number of samples. We forecast and train several NetHack agents with IL and find they outperform prior state-of-the-art by at least 2x in all settings. Our work both demonstrates the scaling behavior of imitation learning in a challenging domain, as well as the viability of scaling up current approaches for increasingly capable agents in NetHack, a game that remains elusively hard for current AI systems.
</details>
<details>
<summary>摘要</summary>
copied text模仿学习（IL）是机器学习中最广泛使用的方法之一。然而，虽然强大，但许多研究发现它经常无法完全回归到专家行为之下。然而，这些研究并没有深入调查模型和数据集大小的扩大对IL的影响。 inspirited by recent work in自然语言处理（NLP），我们调查了在IL Setting中，是否可以通过扩大模型和数据集大小来获得类似的改进。为了证明我们的发现，我们在NetHack游戏中做了实验，这是一个复杂的环境，具有过程生成、随机性、长期依赖和部分可见性。我们发现IL损失和平均回报与计算预算成直线关系，并且存在计算优化IL代理的强力法律。我们预测和训练了一些NetHack代理，并发现它们在所有设置下都高于之前的状态艺术。我们的工作不仅证明了IL在复杂环境中的扩大行为，还证明了可以通过扩大当前方法来创建更有能力的NetHack代理。
</details></li>
</ul>
<hr>
<h2 id="Causality-oriented-robustness-exploiting-general-additive-interventions"><a href="#Causality-oriented-robustness-exploiting-general-additive-interventions" class="headerlink" title="Causality-oriented robustness: exploiting general additive interventions"></a>Causality-oriented robustness: exploiting general additive interventions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10299">http://arxiv.org/abs/2307.10299</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xwshen51/drig">https://github.com/xwshen51/drig</a></li>
<li>paper_authors: Xinwei Shen, Peter Bühlmann, Armeen Taeb</li>
<li>for: 这篇论文的目的是为了提出一种robust prediction模型，可以在实际应用中面对分布差异时提供更好的预测性。</li>
<li>methods: 这篇论文使用了 causality-oriented robustness的方法，即 Distributional Robustness via Invariant Gradients (DRIG)，它在训练资料中使用通用的加法性改变来获得对于未见到的改变的Robust预测。</li>
<li>results: 论文证明了 DRIG 在线性设定下可以获得robust预测，并且显示了该方法可以在不同的挑战中提供更好的预测性。此外，论文还显示了该方法可以在半监督学习设定下进一步改善预测性。最后，论文通过synthetic simulations和单元细胞数据的实验验证了该方法的有效性。<details>
<summary>Abstract</summary>
Since distribution shifts are common in real-world applications, there is a pressing need for developing prediction models that are robust against such shifts. Existing frameworks, such as empirical risk minimization or distributionally robust optimization, either lack generalizability for unseen distributions or rely on postulated distance measures. Alternatively, causality offers a data-driven and structural perspective to robust predictions. However, the assumptions necessary for causal inference can be overly stringent, and the robustness offered by such causal models often lacks flexibility. In this paper, we focus on causality-oriented robustness and propose Distributional Robustness via Invariant Gradients (DRIG), a method that exploits general additive interventions in training data for robust predictions against unseen interventions, and naturally interpolates between in-distribution prediction and causality. In a linear setting, we prove that DRIG yields predictions that are robust among a data-dependent class of distribution shifts. Furthermore, we show that our framework includes anchor regression (Rothenh\"ausler et al.\ 2021) as a special case, and that it yields prediction models that protect against more diverse perturbations. We extend our approach to the semi-supervised domain adaptation setting to further improve prediction performance. Finally, we empirically validate our methods on synthetic simulations and on single-cell data.
</details>
<details>
<summary>摘要</summary>
因为分布Shift是实际应用中的常见现象，因此有一个急需要开发对不visible distribution的预测模型，这些模型需要对不可见的分布进行Robust预测。现有的框架，如empirical risk minimization或distributionally robust optimization，或者lack普遍性 для未见分布，或者基于假设的距离度量。 Alternatively， causality 提供了数据驱动和结构性的预测Robustness。然而， causal inference 的假设可能过于严格，而且Robustness 提供的灵活性不够。在这篇论文中，我们关注 causality-oriented Robustness，并提出 Distributional Robustness via Invariant Gradients (DRIG)，这种方法利用训练数据中的通用加itive intervention来预测未见 intervention 的Robust预测，并自然地在 in-distribution prediction 和 causality 之间进行 interpolate。在线性设定下，我们证明 DRIG 的预测是对 data-dependent 类型的分布Shift 的Robust。此外，我们显示了我们的框架包含 anchor regression (Rothenh\"ausler et al.\ 2021) 的特殊情况，并且它的预测模型可以保护对更多的扰动。我们将我们的方法扩展到 semi-supervised domain adaptation 设定，以进一步改进预测性能。最后，我们在 Synthetic simulations 和 single-cell data 上进行了实验验证。
</details></li>
</ul>
<hr>
<h2 id="Online-Learning-with-Costly-Features-in-Non-stationary-Environments"><a href="#Online-Learning-with-Costly-Features-in-Non-stationary-Environments" class="headerlink" title="Online Learning with Costly Features in Non-stationary Environments"></a>Online Learning with Costly Features in Non-stationary Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09388">http://arxiv.org/abs/2307.09388</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/saeedghoorchian/ncc-bandits">https://github.com/saeedghoorchian/ncc-bandits</a></li>
<li>paper_authors: Saeed Ghoorchian, Evgenii Kortukov, Setareh Maghsudi</li>
<li>for: maximizing long-term rewards in sequential decision-making problems</li>
<li>methods: extending the contextual bandit setting to observe subsets of features’ states</li>
<li>results: sublinear regret guarantee and superior performance in a real-world scenario<details>
<summary>Abstract</summary>
Maximizing long-term rewards is the primary goal in sequential decision-making problems. The majority of existing methods assume that side information is freely available, enabling the learning agent to observe all features' states before making a decision. In real-world problems, however, collecting beneficial information is often costly. That implies that, besides individual arms' reward, learning the observations of the features' states is essential to improve the decision-making strategy. The problem is aggravated in a non-stationary environment where reward and cost distributions undergo abrupt changes over time. To address the aforementioned dual learning problem, we extend the contextual bandit setting and allow the agent to observe subsets of features' states. The objective is to maximize the long-term average gain, which is the difference between the accumulated rewards and the paid costs on average. Therefore, the agent faces a trade-off between minimizing the cost of information acquisition and possibly improving the decision-making process using the obtained information. To this end, we develop an algorithm that guarantees a sublinear regret in time. Numerical results demonstrate the superiority of our proposed policy in a real-world scenario.
</details>
<details>
<summary>摘要</summary>
最大化长期收益是Sequential decision-making问题的主要目标。现有的大多数方法假设可以免费获得侧 информация，允许学习机器人在做出决策之前观察所有特征状态。然而，在实际问题中，收集有利信息可能是成本的。这意味着， besides 个体武器的奖励，学习特征状态的观察也是重要的，以提高决策策略。这个问题在非站点环境中更加严重，因为奖励和成本分布在时间上发生了快速变化。为解决上述双学习问题，我们将Contextual bandit设置扩展，允许机器人观察特征状态的子集。目标是最大化长期均值收益，即收益和付出的均值差。因此，机器人面临一种折衔决策，即最小化信息收集成本，可能通过获得的信息改善决策过程。为此，我们开发了一个 garantía sublinear regret in time的算法。实际结果表明我们提出的策略在实际场景中具有优势。
</details></li>
</ul>
<hr>
<h2 id="Batched-Predictors-Generalize-within-Distribution"><a href="#Batched-Predictors-Generalize-within-Distribution" class="headerlink" title="Batched Predictors Generalize within Distribution"></a>Batched Predictors Generalize within Distribution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09379">http://arxiv.org/abs/2307.09379</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andreas Loukas, Pan Kessel</li>
<li>for: 该论文研究批处理预测器的通用性 Properties，即模型用于预测小组例的平均标签。</li>
<li>methods: 该论文使用适当的总化带性质来证明批处理预测器的泛化保证更强，与标准每个样本预测方法相比，批处理预测器的泛化保证会加速指数增长。</li>
<li>results: 该论文通过实验 validate theoretically proved that batched predictors have exponentially stronger generalization guarantees than the standard per-sample approach, and the proposed bound holds independently of overparametrization.<details>
<summary>Abstract</summary>
We study the generalization properties of batched predictors, i.e., models tasked with predicting the mean label of a small set (or batch) of examples. The batched prediction paradigm is particularly relevant for models deployed to determine the quality of a group of compounds in preparation for offline testing. By utilizing a suitable generalization of the Rademacher complexity, we prove that batched predictors come with exponentially stronger generalization guarantees as compared to the standard per-sample approach. Surprisingly, the proposed bound holds independently of overparametrization. Our theoretical insights are validated experimentally for various tasks, architectures, and applications.
</details>
<details>
<summary>摘要</summary>
我们研究批处理预测器的通用性特性，即用于预测一小集（或批）示例的模型。批处理预测方式在准备进行线上测试时对模型 particulary relevanter。我们利用适当的总体化Rademacher复杂度，证明批处理预测器在标准每个样本预测方法相比具有ексиponentially强的泛化保证。意外的是，我们的理论发现不受过参数化的影响。我们通过实验 validate our theoretical insights for various tasks, architectures, and applications.Note: "批处理" (batched) in Chinese is usually translated as "批量处理" (batch processing), but in this context, I used "批处理预测器" (batched predictor) to emphasize the prediction aspect.
</details></li>
</ul>
<hr>
<h2 id="Data-Cross-Segmentation-for-Improved-Generalization-in-Reinforcement-Learning-Based-Algorithmic-Trading"><a href="#Data-Cross-Segmentation-for-Improved-Generalization-in-Reinforcement-Learning-Based-Algorithmic-Trading" class="headerlink" title="Data Cross-Segmentation for Improved Generalization in Reinforcement Learning Based Algorithmic Trading"></a>Data Cross-Segmentation for Improved Generalization in Reinforcement Learning Based Algorithmic Trading</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09377">http://arxiv.org/abs/2307.09377</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vikram Duvvur, Aashay Mehta, Edward Sun, Bo Wu, Ken Yew Chan, Jeff Schneider</li>
<li>for: 这篇论文主要是为了探讨用机器学习技术在算法交易系统中的应用，以及如何在贱liquid的资产市场和 differentiated assets 市场中实现更好的交易result。</li>
<li>methods: 这篇论文提出了一种基于强化学习（RL）算法的交易策略，使用学习的预测模型生成交易信号，并 Addresses the challenges of thinly traded financial markets and differentiated assets markets。</li>
<li>results: 在 tested on 20+ years of equity data from Bursa Malaysia，RL algorithm 能够在不同的市场情况下提供更好的交易result，包括减少交易成本和提高投资回报。<details>
<summary>Abstract</summary>
The use of machine learning in algorithmic trading systems is increasingly common. In a typical set-up, supervised learning is used to predict the future prices of assets, and those predictions drive a simple trading and execution strategy. This is quite effective when the predictions have sufficient signal, markets are liquid, and transaction costs are low. However, those conditions often do not hold in thinly traded financial markets and markets for differentiated assets such as real estate or vehicles. In these markets, the trading strategy must consider the long-term effects of taking positions that are relatively more difficult to change. In this work, we propose a Reinforcement Learning (RL) algorithm that trades based on signals from a learned predictive model and addresses these challenges. We test our algorithm on 20+ years of equity data from Bursa Malaysia.
</details>
<details>
<summary>摘要</summary>
machine learning在算法交易系统中的使用越来越普遍。在一般情况下，监督学习用于预测未来资产价格，并且这些预测驱动简单的交易和执行策略。这很有效果当预测具有足够的信号，市场活跃，交易成本低。然而，这些条件frequently不成立在稀采资产市场和不同化资产市场，如房地产或车辆。在这些市场中，交易策略必须考虑持有position的长期影响。在这种情况下，我们提出一种奖励学习（RL）算法，基于学习的预测信号来交易。我们在马来西亚证券交易所20多年的股票数据上测试了我们的算法。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/19/cs.LG_2023_07_19/" data-id="clluro5jk004lq9881h9ac00v" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/19/cs.SD_2023_07_19/" class="article-date">
  <time datetime="2023-07-18T16:00:00.000Z" itemprop="datePublished">2023-07-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/19/cs.SD_2023_07_19/">cs.SD - 2023-07-19 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Alzheimer’s-Disease-Detection-from-Spontaneous-Speech-and-Text-A-review"><a href="#Alzheimer’s-Disease-Detection-from-Spontaneous-Speech-and-Text-A-review" class="headerlink" title="Alzheimer’s Disease Detection from Spontaneous Speech and Text: A review"></a>Alzheimer’s Disease Detection from Spontaneous Speech and Text: A review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10005">http://arxiv.org/abs/2307.10005</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vrindha M. K., Geethu V., Anurenjan P. R., Deepak S., Sreeni K. G.</li>
<li>for: 这个评文主要用于检测阿尔ツ海默病。</li>
<li>methods: 这些研究使用语音分析来 diferenciar正常年龄增长和阿尔ツ海默病。</li>
<li>results: 这些研究发现，通过考虑语音和语言特征，可以建立更准确的阿尔ツ海默病分类模型。Here’s the same information in Simplified Chinese:</li>
<li>for: 这个评文主要用于检测阿尔ツ海默病。</li>
<li>methods: 这些研究使用语音分析来 diferenciar正常年龄增长和阿尔 Ts海默病。</li>
<li>results: 这些研究发现，通过考虑语音和语言特征，可以建立更准确的阿尔 Ts海默病分类模型。<details>
<summary>Abstract</summary>
In the past decade, there has been a surge in research examining the use of voice and speech analysis as a means of detecting neurodegenerative diseases such as Alzheimer's. Many studies have shown that certain acoustic features can be used to differentiate between normal aging and Alzheimer's disease, and speech analysis has been found to be a cost-effective method of detecting Alzheimer's dementia. The aim of this review is to analyze the various algorithms used in speech-based detection and classification of Alzheimer's disease. A literature survey was conducted using databases such as Web of Science, Google Scholar, and Science Direct, and articles published from January 2020 to the present were included based on keywords such as ``Alzheimer's detection'', "speech," and "natural language processing." The ADReSS, Pitt corpus, and CCC datasets are commonly used for the analysis of dementia from speech, and this review focuses on the various acoustic and linguistic feature engineering-based classification models drawn from 15 studies.   Based on the findings of this study, it appears that a more accurate model for classifying Alzheimer's disease can be developed by considering both linguistic and acoustic data. The review suggests that speech signals can be a useful tool for detecting dementia and may serve as a reliable biomarker for efficiently identifying Alzheimer's disease.
</details>
<details>
<summary>摘要</summary>
过去一代，有一场浪涌的研究探讨了使用语音和speech分析来诊断 neuros degenerative diseases 如阿尔茨海默病。许多研究表明， certain acoustic features 可以用来区分正常年龄和阿尔茨海默病，并且speech analysis 被发现为诊断阿尔茨海默病的cost-effective方法。本文总结了各种算法在speech-based检测和分类阿尔茨海默病中的应用，通过对 Web of Science、Google Scholar 和 Science Direct 等数据库中发表于2020年1月以来的文章进行Literature survey，并且包括了 keywords 如“阿尔茨海默病检测”、“语音”和“自然语言处理”。ADReSS、Pitt corpus 和 CCC 数据集是用于分析 деменcia 的常用数据集，本文将重点介绍15个研究中的各种语音和语言工程学特征基于的分类模型。根据本研究的结果，可以通过考虑语音和语言数据来建立更高精度的阿尔茨海默病分类模型。文章建议，speech signals 可以用于诊断 деменcia，并且可能成为诊断阿尔茨海默病的可靠生物标志。
</details></li>
</ul>
<hr>
<h2 id="DisCover-Disentangled-Music-Representation-Learning-for-Cover-Song-Identification"><a href="#DisCover-Disentangled-Music-Representation-Learning-for-Cover-Song-Identification" class="headerlink" title="DisCover: Disentangled Music Representation Learning for Cover Song Identification"></a>DisCover: Disentangled Music Representation Learning for Cover Song Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09775">http://arxiv.org/abs/2307.09775</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiahao Xun, Shengyu Zhang, Yanting Yang, Jieming Zhu, Liqun Deng, Zhou Zhao, Zhenhua Dong, Ruiqi Li, Lichao Zhang, Fei Wu</li>
<li>for: 本研究旨在提高cover song identification（CSI）的准确性，通过分离版本特定和版本不变因素来降低内版本和间版本相关性。</li>
<li>methods: 本研究提出了一种基于 causal graph 技术的分离模型，包括知识导向分离模块（KDM）和敌对分离模块（GADM），以阻塞内版本和间版本影响。</li>
<li>results: 对比best-performing方法，本研究的DisCover模型显示出更高的准确率和更好的解释能力，并且进一步验证了分离的重要性 для CSI。<details>
<summary>Abstract</summary>
In the field of music information retrieval (MIR), cover song identification (CSI) is a challenging task that aims to identify cover versions of a query song from a massive collection. Existing works still suffer from high intra-song variances and inter-song correlations, due to the entangled nature of version-specific and version-invariant factors in their modeling. In this work, we set the goal of disentangling version-specific and version-invariant factors, which could make it easier for the model to learn invariant music representations for unseen query songs. We analyze the CSI task in a disentanglement view with the causal graph technique, and identify the intra-version and inter-version effects biasing the invariant learning. To block these effects, we propose the disentangled music representation learning framework (DisCover) for CSI. DisCover consists of two critical components: (1) Knowledge-guided Disentanglement Module (KDM) and (2) Gradient-based Adversarial Disentanglement Module (GADM), which block intra-version and inter-version biased effects, respectively. KDM minimizes the mutual information between the learned representations and version-variant factors that are identified with prior domain knowledge. GADM identifies version-variant factors by simulating the representation transitions between intra-song versions, and exploits adversarial distillation for effect blocking. Extensive comparisons with best-performing methods and in-depth analysis demonstrate the effectiveness of DisCover and the and necessity of disentanglement for CSI.
</details>
<details>
<summary>摘要</summary>
在音乐信息检索（MIR）领域中，歌曲重建（CSI）是一项具有挑战性的任务，即从大量歌曲库中Identify query song的cover版本。现有的方法仍然受到高度的内song差异和间song相关性的影响，这是因为模型中版本特定和版本不变因素的杂乱结合所致。在这个工作中，我们设置了分离版本特定和版本不变因素的目标，这可以让模型更容易学习未见查询歌曲的不变音乐表示。我们使用 causal graph 技术进行分析 CSI 任务，并确定了内 song 和间 song 的影响，以阻塞这些影响。为此，我们提出了不同核心组成部分：（1）帮助导向分离模块（KDM）和（2）斜向分离模块（GADM），它们分别阻塞内 song 和间 song 的偏见效应。KDM 使用先验知识来减少学习表示和版本特定因素之间的互信息。GADM 通过模拟曲目版本之间的表示过渡，并通过对抗照射来阻塞影响。对比best-performing方法和深入分析表明，DisCover 的效果和分离的必要性。
</details></li>
</ul>
<hr>
<h2 id="Improving-Domain-Generalization-for-Sound-Classification-with-Sparse-Frequency-Regularized-Transformer"><a href="#Improving-Domain-Generalization-for-Sound-Classification-with-Sparse-Frequency-Regularized-Transformer" class="headerlink" title="Improving Domain Generalization for Sound Classification with Sparse Frequency-Regularized Transformer"></a>Improving Domain Generalization for Sound Classification with Sparse Frequency-Regularized Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09723">http://arxiv.org/abs/2307.09723</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hlmu/frito">https://github.com/hlmu/frito</a></li>
<li>paper_authors: Honglin Mu, Wentian Xia, Wanxiang Che</li>
<li>for: 提高Transformer模型对异常数据的泛化能力</li>
<li>methods: 限制每个语言序列位置的听觉频率维度的注意力范围，使Transformer模型更好地泛化</li>
<li>results: 实验表明，我们的方法可以帮助Transformer模型在TAU 2020和Nsynth数据集上达到最佳泛化性能，同时节省20%的计算时间<details>
<summary>Abstract</summary>
Sound classification models' performance suffers from generalizing on out-of-distribution (OOD) data. Numerous methods have been proposed to help the model generalize. However, most either introduce inference overheads or focus on long-lasting CNN-variants, while Transformers has been proven to outperform CNNs on numerous natural language processing and computer vision tasks. We propose FRITO, an effective regularization technique on Transformer's self-attention, to improve the model's generalization ability by limiting each sequence position's attention receptive field along the frequency dimension on the spectrogram. Experiments show that our method helps Transformer models achieve SOTA generalization performance on TAU 2020 and Nsynth datasets while saving 20% inference time.
</details>
<details>
<summary>摘要</summary>
声学分类模型的性能受到外部数据（OOD）的泛化影响。许多方法已经被提出来帮助模型泛化，但大多数方法引入推理负担或专注于长期存在的CNN变体，而 transformer则在自然语言处理和计算机视觉任务上表现出优异。我们提出了FRITO，一种有效的自注意力正则化技术，以限制每个序列位置的自注意力响应范围在频率维度上的spectrogram。实验显示，我们的方法可以帮助transformer模型在TAU 2020和Nsynth数据集上 дости得SOTA的泛化性能，同时节省20%的推理时间。
</details></li>
</ul>
<hr>
<h2 id="SLMGAN-Exploiting-Speech-Language-Model-Representations-for-Unsupervised-Zero-Shot-Voice-Conversion-in-GANs"><a href="#SLMGAN-Exploiting-Speech-Language-Model-Representations-for-Unsupervised-Zero-Shot-Voice-Conversion-in-GANs" class="headerlink" title="SLMGAN: Exploiting Speech Language Model Representations for Unsupervised Zero-Shot Voice Conversion in GANs"></a>SLMGAN: Exploiting Speech Language Model Representations for Unsupervised Zero-Shot Voice Conversion in GANs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09435">http://arxiv.org/abs/2307.09435</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yl4579/SLMGAN">https://github.com/yl4579/SLMGAN</a></li>
<li>paper_authors: Yinghao Aaron Li, Cong Han, Nima Mesgarani</li>
<li>for: 这个论文是为了建立一个基于大规模预训语言模型（SLM）的隐藏 MARK 抽象数据类型，尤其是在文本转语音、声音转换和声音提高等应用中。</li>
<li>methods: 这个论文使用了SLM表现库，并在生成数据类型中将其应用于生成数据类型。它还使用了一个新的SLM特征匹配损失函数，以及一个基于mel的构成器，从而建立了一个无监控的零发生训练 Zero-shot 声音转换系统。</li>
<li>results: 实验结果显示，SLMGAN比exist的零发生训练声音转换模型在自然性和相似性方面表现更好，并且 achieves comparable similarity。这显示了SLM-based discriminators的潜在应用在相关应用中。<details>
<summary>Abstract</summary>
In recent years, large-scale pre-trained speech language models (SLMs) have demonstrated remarkable advancements in various generative speech modeling applications, such as text-to-speech synthesis, voice conversion, and speech enhancement. These applications typically involve mapping text or speech inputs to pre-trained SLM representations, from which target speech is decoded. This paper introduces a new approach, SLMGAN, to leverage SLM representations for discriminative tasks within the generative adversarial network (GAN) framework, specifically for voice conversion. Building upon StarGANv2-VC, we add our novel SLM-based WavLM discriminators on top of the mel-based discriminators along with our newly designed SLM feature matching loss function, resulting in an unsupervised zero-shot voice conversion system that does not require text labels during training. Subjective evaluation results show that SLMGAN outperforms existing state-of-the-art zero-shot voice conversion models in terms of naturalness and achieves comparable similarity, highlighting the potential of SLM-based discriminators for related applications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/19/cs.SD_2023_07_19/" data-id="clluro5kk007qq98839wrdgya" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_07_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/19/eess.AS_2023_07_19/" class="article-date">
  <time datetime="2023-07-18T16:00:00.000Z" itemprop="datePublished">2023-07-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/19/eess.AS_2023_07_19/">eess.AS - 2023-07-19 22:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="An-analysis-on-the-effects-of-speaker-embedding-choice-in-non-auto-regressive-TTS"><a href="#An-analysis-on-the-effects-of-speaker-embedding-choice-in-non-auto-regressive-TTS" class="headerlink" title="An analysis on the effects of speaker embedding choice in non auto-regressive TTS"></a>An analysis on the effects of speaker embedding choice in non auto-regressive TTS</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09898">http://arxiv.org/abs/2307.09898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adriana Stan, Johannah O’Mahony</li>
<li>for: 本研究探讨了一种非 autoregressive 多 speaker speech synthesis 架构如何利用不同 speaker embedding 集的信息。</li>
<li>methods: 本研究使用了jointly 学习表示和使用预训练模型初始化的方法，以确定是否会提高目标 speaker 标识的质量改进。</li>
<li>results: 研究发现， regardless of the used set of embeddings and learning strategy, the network can handle various speaker identities equally well, with barely noticeable variations in speech output quality, and that speaker leakage within the core structure of the synthesis system is inevitable in the standard training procedures adopted thus far.<details>
<summary>Abstract</summary>
In this paper we introduce a first attempt on understanding how a non-autoregressive factorised multi-speaker speech synthesis architecture exploits the information present in different speaker embedding sets. We analyse if jointly learning the representations, and initialising them from pretrained models determine any quality improvements for target speaker identities. In a separate analysis, we investigate how the different sets of embeddings impact the network's core speech abstraction (i.e. zero conditioned) in terms of speaker identity and representation learning. We show that, regardless of the used set of embeddings and learning strategy, the network can handle various speaker identities equally well, with barely noticeable variations in speech output quality, and that speaker leakage within the core structure of the synthesis system is inevitable in the standard training procedures adopted thus far.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种首次尝试理解非 autoregressive 多 speaker speech synthesis 架构如何利用不同的 speaker embedding 集中的信息。我们分析了在 jointly 学习表示和使用预训练模型初始化的情况下，是否会对目标 speaker 标识提供任何质量改进。在另一个分析中，我们调查了不同 embedding 集的影响于核心 speech abstraction（即零条件）的 speaker identity 和表示学习。我们发现，无论使用哪个 embedding 集和学习策略，网络都能够平等地处理不同的 speaker 标识，并且在输出质量方面的变化幅度很小。此外，我们发现在标准的训练程序中，speaker leakage 在核心 Synthesis 系统中是不可避免的。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Acoustic-Word-Embedding-Learning-via-Correspondence-Transformer-Encoder"><a href="#Self-Supervised-Acoustic-Word-Embedding-Learning-via-Correspondence-Transformer-Encoder" class="headerlink" title="Self-Supervised Acoustic Word Embedding Learning via Correspondence Transformer Encoder"></a>Self-Supervised Acoustic Word Embedding Learning via Correspondence Transformer Encoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09871">http://arxiv.org/abs/2307.09871</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingru Lin, Xianghu Yue, Junyi Ao, Haizhou Li</li>
<li>for: 学习 Robust 语音嵌入（AWEs），以Map variable-length speech segment 到 fixed-dimensional representation。</li>
<li>methods: 使用 teacher-student learning framework，基于不同 acoustic instance 的同一个 word 应该在嵌入空间close的想法，进行自动注意力训练。</li>
<li>results: 实验结果表明，提出的 CTE 模型可以具有robustness to speech variations，如 speaker 和 domain，并在 Xitsonga 的 low-resource cross-lingual setting 中实现新的状态级表现。<details>
<summary>Abstract</summary>
Acoustic word embeddings (AWEs) aims to map a variable-length speech segment into a fixed-dimensional representation. High-quality AWEs should be invariant to variations, such as duration, pitch and speaker. In this paper, we introduce a novel self-supervised method to learn robust AWEs from a large-scale unlabelled speech corpus. Our model, named Correspondence Transformer Encoder (CTE), employs a teacher-student learning framework. We train the model based on the idea that different realisations of the same word should be close in the underlying embedding space. Specifically, we feed the teacher and student encoder with different acoustic instances of the same word and pre-train the model with a word-level loss. Our experiments show that the embeddings extracted from the proposed CTE model are robust to speech variations, e.g. speakers and domains. Additionally, when evaluated on Xitsonga, a low-resource cross-lingual setting, the CTE model achieves new state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
通过听音词嵌入（AWEs），我们想要将变长的说话段映射到固定维度的表示中。高质量的AWEs应该是不变的，无论 duration、pitch 或者 speaker 发生变化。在这篇论文中，我们介绍了一种新的自动超vised学习方法，用于从大规模的无标签说话 corpus 中学习Robust AWEs。我们的模型，名为对应 transformer encoder（CTE），使用 teacher-student 学习框架。我们在不同的 say 实例中的同一个 word 之间进行了预训练，使得模型在嵌入空间中保持同一个 word 的不变性。我们的实验结果表明，CTE 模型提取出的嵌入特征是不受 speech 变化（如 speaker 和 domain）的影响。此外，当我们在 Xitsonga cross-lingual setting 中评估 CTE 模型时，它达到了新的州chart Performance。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/19/eess.AS_2023_07_19/" data-id="clluro5ld00anq988e6hshhdl" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/19/eess.IV_2023_07_19/" class="article-date">
  <time datetime="2023-07-18T16:00:00.000Z" itemprop="datePublished">2023-07-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/19/eess.IV_2023_07_19/">eess.IV - 2023-07-19 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Flexible-Physical-Unclonable-Functions-based-on-non-deterministically-distributed-Dye-Doped-Fibers-and-Droplets"><a href="#Flexible-Physical-Unclonable-Functions-based-on-non-deterministically-distributed-Dye-Doped-Fibers-and-Droplets" class="headerlink" title="Flexible Physical Unclonable Functions based on non-deterministically distributed Dye-Doped Fibers and Droplets"></a>Flexible Physical Unclonable Functions based on non-deterministically distributed Dye-Doped Fibers and Droplets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11000">http://arxiv.org/abs/2308.11000</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mauro Daniel Luigi Bruno, Giuseppe Emanuele Lio, Antonio Ferraro, Sara Nocentini, Giuseppe Papuzzo, Agostino Forestiero, Giovanni Desiderio, Maria Penelope De Santo, Diederik Sybolt Wiersma, Roberto Caputo, Giovanni Golemme, Francesco Riboli, Riccardo Cristoforo Barberi</li>
<li>for: 防止伪造（anti-counterfeiting）解释</li>
<li>methods: 使用电子推抛和电子插填技术生成弹性自由悬浮膜，嵌入不同的物理不可复制函数（PUF）金钥</li>
<li>results: 产生了复杂的多层验证机制，包括激光谱发射、特征fluorescence谱和挑战回应对（CRP）识别协议，实现了高度安全的物品保护。<details>
<summary>Abstract</summary>
The development of new anti-counterfeiting solutions is a constant challenge and involves several research fields. Much interest is devoted to systems that are impossible to clone, based on the Physical Unclonable Function (PUF) paradigm. In this work, new strategies based on electrospinning and electrospraying of dye-doped polymeric materials are presented for the manufacturing of flexible free-standing films that embed different PUF keys. Films can be used to fabricate anticounterfeiting labels having three encryption levels: i) a map of fluorescent polymer droplets, with non deterministic positions on a dense yarn of polymer nanofibers; ii) a characteristic fluorescence spectrum for each label; iii) a challenge-response pairs (CRPs) identification protocol based on the strong nature of the physical unclonable function. The intrinsic uniqueness introduced by the deposition techniques encodes enough complexity into the optical anti-counterfeiting tag to generate thousands of cryptographic keys. The simple and cheap fabrication process as well as the multilevel authentication makes such colored polymeric unclonable tags a practical solution in the secure protection of merchandise in our daily life.
</details>
<details>
<summary>摘要</summary>
新型防伪措施的开发是一项不断挑战，涉及到多个研究领域。许多研究人员对于不可复制的系统产生了极大的兴趣，基于物理不可克隆函数（PUF）的思想。在这项工作中，我们提出了基于电子涂敷和电子扑灭的染料含 polymer 材料制造 flexible 自由浮动膜，其中包含不同的 PUF 密钥。这些膜可以用来制造防伪标签，具有三级加密：1. 涂敷在细菌纤维上的荟毒染料液体，具有不决定的位置和密度，形成一个复杂的荟毒染料液体地图。2. 每个标签具有独特的激发谱，作为特征性的防伪特征。3. 基于强大的物理不可克隆函数，实现了挑战-回答协议（CRP）的标识。由于电子涂敷和电子扑灭的固有特性，这些染料含 polymer 材料中的复杂性足以生成 тысячи个加密密钥。此外，制造过程简单、便宜，同时具有多级身份验证功能，使这种颜色染料含 polymer 防伪标签在我们日常生活中的安全保护中成为了实用的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Blind-Image-Quality-Assessment-Using-Multi-Stream-Architecture-with-Spatial-and-Channel-Attention"><a href="#Blind-Image-Quality-Assessment-Using-Multi-Stream-Architecture-with-Spatial-and-Channel-Attention" class="headerlink" title="Blind Image Quality Assessment Using Multi-Stream Architecture with Spatial and Channel Attention"></a>Blind Image Quality Assessment Using Multi-Stream Architecture with Spatial and Channel Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09857">http://arxiv.org/abs/2307.09857</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hassan Khalid, Nisar Ahmed</li>
<li>for: 本文提出了一种基于多流空间和通道注意力的自适应图像质量评估算法，以解决图像质量评估的难题，图像内容和扭曲都有所不同。</li>
<li>methods: 该算法首先使用两个不同的背景网络生成混合特征，然后通过空间和通道注意力来提供高权重到关键区域。</li>
<li>results: 实验结果表明，该算法可以更准确地预测图像质量，与人类感知评估呈高相关性。此外，该算法还具有优秀的泛化性，特别是在关键区域的感知信息上。<details>
<summary>Abstract</summary>
BIQA (Blind Image Quality Assessment) is an important field of study that evaluates images automatically. Although significant progress has been made, blind image quality assessment remains a difficult task since images vary in content and distortions. Most algorithms generate quality without emphasizing the important region of interest. In order to solve this, a multi-stream spatial and channel attention-based algorithm is being proposed. This algorithm generates more accurate predictions with a high correlation to human perceptual assessment by combining hybrid features from two different backbones, followed by spatial and channel attention to provide high weights to the region of interest. Four legacy image quality assessment datasets are used to validate the effectiveness of our proposed approach. Authentic and synthetic distortion image databases are used to demonstrate the effectiveness of the proposed method, and we show that it has excellent generalization properties with a particular focus on the perceptual foreground information.
</details>
<details>
<summary>摘要</summary>
BIQA (无视图质量评估) 是一个重要的研究领域，它自动评估图像质量。尽管已经取得了 significante 进步，但无视图质量评估仍然是一个困难的任务，因为图像的内容和损害都很多样。大多数算法生成的质量不强调重要的注意点区域。为解决这个问题，我们提出了一种多流程空间和通道注意力基于的算法。这种算法将 combining 两种不同的背景器，然后进行空间和通道注意力，以提供高权重的注意点区域，从而生成更加准确的预测，与人类感知评估高相关性。我们使用四个传统图像质量评估Dataset来验证我们的提议的有效性。我们还使用authentic和synthetic损害图像库来示示我们的方法的总体化能力，并示出它在特定的感知前景信息方面具有优秀的一致性。
</details></li>
</ul>
<hr>
<h2 id="Cryo-forum-A-framework-for-orientation-recovery-with-uncertainty-measure-with-the-application-in-cryo-EM-image-analysis"><a href="#Cryo-forum-A-framework-for-orientation-recovery-with-uncertainty-measure-with-the-application-in-cryo-EM-image-analysis" class="headerlink" title="Cryo-forum: A framework for orientation recovery with uncertainty measure with the application in cryo-EM image analysis"></a>Cryo-forum: A framework for orientation recovery with uncertainty measure with the application in cryo-EM image analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09847">http://arxiv.org/abs/2307.09847</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/phonchi/cryo-forum">https://github.com/phonchi/cryo-forum</a></li>
<li>paper_authors: Szu-Chi Chung<br>for:This paper aims to improve the efficiency and accuracy of orientation parameter estimation in single-particle cryo-electron microscopy (cryo-EM) by introducing a novel approach that uses a 10-dimensional feature vector and a Quadratically-Constrained Quadratic Program to derive the predicted orientation as a unit quaternion, supplemented by an uncertainty metric.methods:The proposed method uses a deep learning approach with a 10-dimensional feature vector to represent the orientation and applies a Quadratically-Constrained Quadratic Program to derive the predicted orientation as a unit quaternion, supplemented by an uncertainty metric. The method also includes a unique loss function that considers the pairwise distances between orientations.results:The proposed method effectively recovers orientations from 2D cryo-EM images in an end-to-end manner, and the inclusion of uncertainty quantification allows for direct clean-up of the dataset at the 3D level. The proposed method is packaged into a user-friendly software suite named cryo-forum, designed for easy accessibility by developers.<details>
<summary>Abstract</summary>
In single-particle cryo-electron microscopy (cryo-EM), the efficient determination of orientation parameters for 2D projection images poses a significant challenge yet is crucial for reconstructing 3D structures. This task is complicated by the high noise levels present in the cryo-EM datasets, which often include outliers, necessitating several time-consuming 2D clean-up processes. Recently, solutions based on deep learning have emerged, offering a more streamlined approach to the traditionally laborious task of orientation estimation. These solutions often employ amortized inference, eliminating the need to estimate parameters individually for each image. However, these methods frequently overlook the presence of outliers and may not adequately concentrate on the components used within the network. This paper introduces a novel approach that uses a 10-dimensional feature vector to represent the orientation and applies a Quadratically-Constrained Quadratic Program to derive the predicted orientation as a unit quaternion, supplemented by an uncertainty metric. Furthermore, we propose a unique loss function that considers the pairwise distances between orientations, thereby enhancing the accuracy of our method. Finally, we also comprehensively evaluate the design choices involved in constructing the encoder network, a topic that has not received sufficient attention in the literature. Our numerical analysis demonstrates that our methodology effectively recovers orientations from 2D cryo-EM images in an end-to-end manner. Importantly, the inclusion of uncertainty quantification allows for direct clean-up of the dataset at the 3D level. Lastly, we package our proposed methods into a user-friendly software suite named cryo-forum, designed for easy accessibility by the developers.
</details>
<details>
<summary>摘要</summary>
Single-particle cryo-electron microscopy (cryo-EM)中，确定图像方向的效率是一项关键任务，但是它对于重建3D结构非常重要。这项任务受到高噪音水平的影响，其中包括异常值，需要进行多次时间消耗的2D清洁过程。近年来，基于深度学习的解决方案在这个领域出现了，它们通常使用整合参数，从而消除每个图像需要独立计算参数的劳动 INTRODUCTION  This paper introduces a novel approach to solving the challenging task of orientation estimation in single-particle cryo-electron microscopy (cryo-EM). Our method uses a 10-dimensional feature vector to represent the orientation and applies a Quadratically-Constrained Quadratic Program (QCQP) to derive the predicted orientation as a unit quaternion, supplemented by an uncertainty metric. Additionally, we propose a unique loss function that considers the pairwise distances between orientations, thereby enhancing the accuracy of our method. We also comprehensively evaluate the design choices involved in constructing the encoder network, a topic that has not received sufficient attention in the literature. Our numerical analysis demonstrates that our methodology effectively recovers orientations from 2D cryo-EM images in an end-to-end manner. Importantly, the inclusion of uncertainty quantification allows for direct clean-up of the dataset at the 3D level. Finally, we package our proposed methods into a user-friendly software suite named cryo-forum, designed for easy accessibility by developers.Here's the translation in Traditional Chinese:Single-particle cryo-electron microscopy (cryo-EM)中，确定图像方向的效率是一项关键任务，但是它对于重建3D结构非常重要。这项任务受到高噪音水平的影响，其中包括异常值，需要进行多次时间消耗的2D清洁过程。近年来，基于深度学习的解决方案在这个领域出现了，它们通常使用整合参数，从而消除每个图像需要独立计算参数的劳动。本文介绍了一种新的方法，使用10维特征向量表示方向，并使用Quadratically-Constrained Quadratic Program (QCQP)来 derive预测的方向为单元量QUATERNION，并且附加了一个不确定度度量。此外，我们还提出了一个唯一的损失函数，该函数考虑了方向之间的对比度，从而提高了我们的方法的准确性。我们还对构建编码器网络的设计选择进行了全面的评估，这是在文献中未得到足够的注意的。我们的数值分析表明，我们的方法可以有效地从2D cryo-EM图像中提取方向，并且包含不确定度度量，可以直接清理3D数据集。最后，我们将我们的提posed方法包装成一个易用的软件套件，名为cryo-forum，用于开发者的易用性。
</details></li>
</ul>
<hr>
<h2 id="Compressive-Image-Scanning-Microscope"><a href="#Compressive-Image-Scanning-Microscope" class="headerlink" title="Compressive Image Scanning Microscope"></a>Compressive Image Scanning Microscope</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09841">http://arxiv.org/abs/2307.09841</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ajay Gunalan, Marco Castello, Simonluca Piazza, Shunlei Li, Alberto Diaspro, Leonardo S. Mattos, Paolo Bianchini</li>
<li>for: 实现压缩扫描镜 microscope (LSM) 图像扫描镜 (ISM) 中的压缩扫描，使用单 photon 荷电压缩 (SPAD) 数组探测器。</li>
<li>methods: 采用固定抽象策略，在数据获取过程中跳过 alternate 行列，从而降低了数据点数量，并消除了计算不同抽象矩阵的需要。 通过利用 SPAD 数组产生的平行图像，提高了压缩-ISM 图像的重建质量，相比标准压缩扫描 LSM 图像。</li>
<li>results: 研究结果表明，我们的方法可以生成高质量图像，同时降低数据获取时间和抗衰减的问题，具有减少光泳蚀的潜在优势。<details>
<summary>Abstract</summary>
We present a novel approach to implement compressive sensing in laser scanning microscopes (LSM), specifically in image scanning microscopy (ISM), using a single-photon avalanche diode (SPAD) array detector. Our method addresses two significant limitations in applying compressive sensing to LSM: the time to compute the sampling matrix and the quality of reconstructed images. We employ a fixed sampling strategy, skipping alternate rows and columns during data acquisition, which reduces the number of points scanned by a factor of four and eliminates the need to compute different sampling matrices. By exploiting the parallel images generated by the SPAD array, we improve the quality of the reconstructed compressive-ISM images compared to standard compressive confocal LSM images. Our results demonstrate the effectiveness of our approach in producing higher-quality images with reduced data acquisition time and potential benefits in reducing photobleaching.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，使用单 photon 泵发射器（SPAD）数组探测器实现压缩扫描镜 microscope（LSM）中的图像扫描镜（ISM）。我们的方法解决了在应用压缩扫描到 LSM 中的两个主要限制：计算抽象矩阵的时间和重建图像的质量。我们采用固定抽象策略，在数据收集过程中跳过 alternate 行和列，从而将数据点数减少为四分之一，并消除计算不同抽象矩阵的需求。通过利用 SPAD 数组生成的并行图像，我们提高了压缩-ISM 图像的重建质量，相比标准压缩扫描 LSM 图像。我们的结果表明我们的方法的有效性，可以生成高质量图像，降低数据收集时间和避免 фото腐蚀。
</details></li>
</ul>
<hr>
<h2 id="Fix-your-downsampling-ASAP-Be-natively-more-robust-via-Aliasing-and-Spectral-Artifact-free-Pooling"><a href="#Fix-your-downsampling-ASAP-Be-natively-more-robust-via-Aliasing-and-Spectral-Artifact-free-Pooling" class="headerlink" title="Fix your downsampling ASAP! Be natively more robust via Aliasing and Spectral Artifact free Pooling"></a>Fix your downsampling ASAP! Be natively more robust via Aliasing and Spectral Artifact free Pooling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09804">http://arxiv.org/abs/2307.09804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julia Grabinski, Janis Keuper, Margret Keuper</li>
<li>for: 本研究旨在提高卷积神经网络对常见损害和攻击的Native robustness。</li>
<li>methods: 本研究使用了FLC pooling和ASAP pooling两种方法来解决采样错误导致的扭曲问题。</li>
<li>results: 对于常见损害和攻击，ASAP pooling可以提高卷积神经网络的Native robustness，而不会影响clean accuracy。<details>
<summary>Abstract</summary>
Convolutional neural networks encode images through a sequence of convolutions, normalizations and non-linearities as well as downsampling operations into potentially strong semantic embeddings. Yet, previous work showed that even slight mistakes during sampling, leading to aliasing, can be directly attributed to the networks' lack in robustness. To address such issues and facilitate simpler and faster adversarial training, [12] recently proposed FLC pooling, a method for provably alias-free downsampling - in theory. In this work, we conduct a further analysis through the lens of signal processing and find that such current pooling methods, which address aliasing in the frequency domain, are still prone to spectral leakage artifacts. Hence, we propose aliasing and spectral artifact-free pooling, short ASAP. While only introducing a few modifications to FLC pooling, networks using ASAP as downsampling method exhibit higher native robustness against common corruptions, a property that FLC pooling was missing. ASAP also increases native robustness against adversarial attacks on high and low resolution data while maintaining similar clean accuracy or even outperforming the baseline.
</details>
<details>
<summary>摘要</summary>
convolutional neural networks 使用序列化核函数、标准化函数、非线性函数以及下采样操作来生成强式 semantic embeddings。然而，过去的工作表明，甚至小范围的采样错误，导致扭曲，可以直接归因于网络的不稳定性。为解决这些问题并实现更加简单和快速的对抗训练，[12] 最近提出了FLC pooling方法，该方法可以在理论上确保无偏Sampling。在这项工作中，我们通过信号处理的视角进行进一步的分析，发现现有的 pooling 方法，它们在频域中处理偏折补，仍然存在频率泄漏 artifacts。因此，我们提出了偏折补和频率 artifact-free pooling，简称ASAP。尽管ASAP只对 FLC pooling 进行了一些修改，但使用 ASAP 作为下采样方法的网络显示出更高的原生 Robustness  against common corruptions，这是FLC pooling缺失的性能。此外，ASAP 还提高了高和低分辨率数据上的对抗攻击 Robustness，保持同样的干净精度或者甚至超越基线。
</details></li>
</ul>
<hr>
<h2 id="DiffDP-Radiotherapy-Dose-Prediction-via-a-Diffusion-Model"><a href="#DiffDP-Radiotherapy-Dose-Prediction-via-a-Diffusion-Model" class="headerlink" title="DiffDP: Radiotherapy Dose Prediction via a Diffusion Model"></a>DiffDP: Radiotherapy Dose Prediction via a Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09794">http://arxiv.org/abs/2307.09794</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/scufzh/DiffDP">https://github.com/scufzh/DiffDP</a></li>
<li>paper_authors: Zhenghao Feng, Lu Wen, Peng Wang, Binyu Yan, Xi Wu, Jiliu Zhou, Yan Wang</li>
<li>for: 这个研究旨在提高放射治疗规划中的剂量分布预测效率和质量，并解决现有方法的过滤问题。</li>
<li>methods: 本研究提出了一个散射基于的剂量预测模型（DiffDP），包括一个前进过程和一个返回过程。在前进过程中，DiffDP将剂量分布图表转换为 Gaussian 噪声，并训练一个噪声预测器来预测添加在每个时间步骤中的噪声。在返回过程中，它从原始 Gaussian 噪声中移除噪声，并使用已经训练的噪声预测器来将噪声移除，最后终端出力预测的剂量分布图表。</li>
<li>results: 实验结果显示，DiffDP 模型能够对放射治疗规划中的130例肛瘤癌患者的剂量分布预测获得高度的准确性和稳定性。<details>
<summary>Abstract</summary>
Currently, deep learning (DL) has achieved the automatic prediction of dose distribution in radiotherapy planning, enhancing its efficiency and quality. However, existing methods suffer from the over-smoothing problem for their commonly used L_1 or L_2 loss with posterior average calculations. To alleviate this limitation, we innovatively introduce a diffusion-based dose prediction (DiffDP) model for predicting the radiotherapy dose distribution of cancer patients. Specifically, the DiffDP model contains a forward process and a reverse process. In the forward process, DiffDP gradually transforms dose distribution maps into Gaussian noise by adding small noise and trains a noise predictor to predict the noise added in each timestep. In the reverse process, it removes the noise from the original Gaussian noise in multiple steps with the well-trained noise predictor and finally outputs the predicted dose distribution map. To ensure the accuracy of the prediction, we further design a structure encoder to extract anatomical information from patient anatomy images and enable the noise predictor to be aware of the dose constraints within several essential organs, i.e., the planning target volume and organs at risk. Extensive experiments on an in-house dataset with 130 rectum cancer patients demonstrate the s
</details>
<details>
<summary>摘要</summary>
现在，深度学习（DL）已经实现了辐射规划中自动预测剂量分布的方法，提高了其效率和质量。然而，现有方法受到通用的L_1或L_2损失函数和后期平均计算的过滤限制。为了解决这一限制，我们创新地引入了DiffDP模型，用于预测肿瘤病人辐射剂量分布。具体来说，DiffDP模型包括一个前进过程和一个反向过程。在前进过程中，DiffDP逐渐将剂量分布图转化为高斯噪声图，并在每个时间步骤中训练一个噪声预测器来预测添加到图中的噪声。在反向过程中，它将噪声从原始高斯噪声图中除去，并在多个步骤中使用已经训练好的噪声预测器来除噪。最后，它输出预测的剂量分布图。为确保预测的准确性，我们还设计了结构编码器，用于从患者身体图像中提取解剖信息，并使噪声预测器对剂量约束（如规划目标体和风险器）产生影响。在我们自有数据集上进行了广泛的实验，结果表明DiffDP模型可以准确地预测肿瘤病人辐射剂量分布。
</details></li>
</ul>
<hr>
<h2 id="CPCM-Contextual-Point-Cloud-Modeling-for-Weakly-supervised-Point-Cloud-Semantic-Segmentation"><a href="#CPCM-Contextual-Point-Cloud-Modeling-for-Weakly-supervised-Point-Cloud-Semantic-Segmentation" class="headerlink" title="CPCM: Contextual Point Cloud Modeling for Weakly-supervised Point Cloud Semantic Segmentation"></a>CPCM: Contextual Point Cloud Modeling for Weakly-supervised Point Cloud Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10316">http://arxiv.org/abs/2307.10316</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lizhaoliu-Lec/CPCM">https://github.com/lizhaoliu-Lec/CPCM</a></li>
<li>paper_authors: Lizhao Liu, Zhuangwei Zhuang, Shangxin Huang, Xunlong Xiao, Tianhang Xiang, Cen Chen, Jingdong Wang, Mingkui Tan</li>
<li>for: 降低高成本的精度注释（less than 0.1% points are labeled），实现弱精度的点云semantic segmentation。</li>
<li>methods: 使用RegionMask和CMT两部分， RegionMask将点云分割成多个区域，然后使用CMT进行masked training，具体来说是将预测结果作为mask产生更多的训练样本。</li>
<li>results: 在ScanNet V2和S3DIS benchmark上得到了state-of-the-art的表现，证明CPCM可以减少高成本的精度注释，并且具有较高的Semantic Segmentation性能。<details>
<summary>Abstract</summary>
We study the task of weakly-supervised point cloud semantic segmentation with sparse annotations (e.g., less than 0.1% points are labeled), aiming to reduce the expensive cost of dense annotations. Unfortunately, with extremely sparse annotated points, it is very difficult to extract both contextual and object information for scene understanding such as semantic segmentation. Motivated by masked modeling (e.g., MAE) in image and video representation learning, we seek to endow the power of masked modeling to learn contextual information from sparsely-annotated points. However, directly applying MAE to 3D point clouds with sparse annotations may fail to work. First, it is nontrivial to effectively mask out the informative visual context from 3D point clouds. Second, how to fully exploit the sparse annotations for context modeling remains an open question. In this paper, we propose a simple yet effective Contextual Point Cloud Modeling (CPCM) method that consists of two parts: a region-wise masking (RegionMask) strategy and a contextual masked training (CMT) method. Specifically, RegionMask masks the point cloud continuously in geometric space to construct a meaningful masked prediction task for subsequent context learning. CMT disentangles the learning of supervised segmentation and unsupervised masked context prediction for effectively learning the very limited labeled points and mass unlabeled points, respectively. Extensive experiments on the widely-tested ScanNet V2 and S3DIS benchmarks demonstrate the superiority of CPCM over the state-of-the-art.
</details>
<details>
<summary>摘要</summary>
我们研究弱监督点云Semantic segmentation问题，即使用非常少的标注点（例如， menos de 0.1% 的点被标注），以降低严重的标注成本。然而，与极其罕见的标注点相对，� Extracting both contextual and object information for scene understanding, such as semantic segmentation, is very difficult. 为了解决这个问题，我们启发自masked modeling（例如，MAE）在图像和视频表示学习中的应用。我们想要通过masked modeling来学习点云中的上下文信息，但是直接将MAE应用于罕见的3D点云可能不具有效果。首先，是非常困难从3D点云中有效地排除有用的视觉上下文。其次，如何完全利用有限的标注点来模型上下文仍是一个开放的问题。在这篇论文中，我们提出了一种简单 yet effective的Contextual Point Cloud Modeling（CPCM）方法，它包括两部分：RegionMask和CMT。Specifically，RegionMask在空间上连续地遮盖点云，以构建一个有意义的masked prediction任务，以便后续的上下文学习。CMT分离了supervised segmentation和无supervised masked context prediction的学习，以便有效地学习非常有限的标注点和大量的无标注点。我们在ScanNet V2和S3DIS标准测试集上进行了广泛的实验，并证明了CPCM的优越性。
</details></li>
</ul>
<hr>
<h2 id="NTIRE-2023-Quality-Assessment-of-Video-Enhancement-Challenge"><a href="#NTIRE-2023-Quality-Assessment-of-Video-Enhancement-Challenge" class="headerlink" title="NTIRE 2023 Quality Assessment of Video Enhancement Challenge"></a>NTIRE 2023 Quality Assessment of Video Enhancement Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09729">http://arxiv.org/abs/2307.09729</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaohong Liu, Xiongkuo Min, Wei Sun, Yulun Zhang, Kai Zhang, Radu Timofte, Guangtao Zhai, Yixuan Gao, Yuqin Cao, Tengchuan Kou, Yunlong Dong, Ziheng Jia, Yilin Li, Wei Wu, Shuming Hu, Sibin Deng, Pengxiang Xiao, Ying Chen, Kai Li, Kai Zhao, Kun Yuan, Ming Sun, Heng Cong, Hao Wang, Lingzhi Fu, Yusheng Zhang, Rongyu Zhang, Hang Shi, Qihang Xu, Longan Xiao, Zhiliang Ma, Mirko Agarla, Luigi Celona, Claudio Rota, Raimondo Schettini, Zhiwei Huang, Yanan Li, Xiaotao Wang, Lei Lei, Hongye Liu, Wei Hong, Ironhead Chuang, Allen Lin, Drake Guan, Iris Chen, Kae Lou, Willy Huang, Yachun Tasi, Yvonne Kao, Haotian Fan, Fangyuan Kong, Shiqi Zhou, Hao Liu, Yu Lai, Shanshan Chen, Wenqi Wang, Haoning Wu, Chaofeng Chen, Chunzheng Zhu, Zekun Guo, Shiling Zhao, Haibing Yin, Hongkui Wang, Hanene Brachemi Meftah, Sid Ahmed Fezza, Wassim Hamidouche, Olivier Déforges, Tengfei Shi, Azadeh Mansouri, Hossein Motamednia, Amir Hossein Bakhtiari, Ahmad Mahmoudi Aznaveh</li>
<li>for: 本研究报告NTIRE 2023年视频改善质量评估挑战，它将在CVPR 2023年新形态图像恢复和提高工作坊（NTIRE）中举行。这个挑战的目的是解决视频处理领域中的一个主要挑战，即视频质量评估（VQA）。</li>
<li>methods: 挑战使用VDPVE数据集，包括600个彩色、亮度和对比度增强的视频，310个除噪视频和301个滤除晃动视频。挑战共有167名参与者。61个参与者队伍在开发阶段提交了1368份提交，而37个队伍在最终测试阶段提交了176份提交。最终，19个队伍提交了他们的模型和相关资料，并详细介绍了他们使用的方法。一些方法比基准方法更好，而赢家的方法表现出了优秀的预测性能。</li>
<li>results: 本研究发现，一些方法比基准方法更好，而赢家的方法表现出了优秀的预测性能。<details>
<summary>Abstract</summary>
This paper reports on the NTIRE 2023 Quality Assessment of Video Enhancement Challenge, which will be held in conjunction with the New Trends in Image Restoration and Enhancement Workshop (NTIRE) at CVPR 2023. This challenge is to address a major challenge in the field of video processing, namely, video quality assessment (VQA) for enhanced videos. The challenge uses the VQA Dataset for Perceptual Video Enhancement (VDPVE), which has a total of 1211 enhanced videos, including 600 videos with color, brightness, and contrast enhancements, 310 videos with deblurring, and 301 deshaked videos. The challenge has a total of 167 registered participants. 61 participating teams submitted their prediction results during the development phase, with a total of 3168 submissions. A total of 176 submissions were submitted by 37 participating teams during the final testing phase. Finally, 19 participating teams submitted their models and fact sheets, and detailed the methods they used. Some methods have achieved better results than baseline methods, and the winning methods have demonstrated superior prediction performance.
</details>
<details>
<summary>摘要</summary>
During the development phase, 61 participating teams submitted their prediction results, with a total of 3168 submissions. In the final testing phase, 176 submissions were submitted by 37 participating teams. Finally, 19 participating teams submitted their models and fact sheets, detailing the methods they used. Some of the methods achieved better results than baseline methods, and the winning methods demonstrated superior prediction performance.
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Driven-Multi-Scale-Feature-Fusion-Network-for-Real-time-Image-Deraining"><a href="#Uncertainty-Driven-Multi-Scale-Feature-Fusion-Network-for-Real-time-Image-Deraining" class="headerlink" title="Uncertainty-Driven Multi-Scale Feature Fusion Network for Real-time Image Deraining"></a>Uncertainty-Driven Multi-Scale Feature Fusion Network for Real-time Image Deraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09728">http://arxiv.org/abs/2307.09728</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ming Tong, Xuefeng Yan, Yongzhen Wang</li>
<li>for: 提高视觉基于测量系统中雨纹影响下的图像识别精度，并且在资源有限的设备上实现实时处理。</li>
<li>methods: 提出了一种基于uncertainty的多尺度特征融合网络（UMFFNet），通过学习映射分布来估计uncertainty，并通过不确定性信息动态强制特征提取和精度提高。</li>
<li>results: 对比其他状态艺术方法，UMFFNet在减少预测错误和提高图像涂抹效果方面达到了显著的性能提高，而且具有少量参数的优势。<details>
<summary>Abstract</summary>
Visual-based measurement systems are frequently affected by rainy weather due to the degradation caused by rain streaks in captured images, and existing imaging devices struggle to address this issue in real-time. While most efforts leverage deep networks for image deraining and have made progress, their large parameter sizes hinder deployment on resource-constrained devices. Additionally, these data-driven models often produce deterministic results, without considering their inherent epistemic uncertainty, which can lead to undesired reconstruction errors. Well-calibrated uncertainty can help alleviate prediction errors and assist measurement devices in mitigating risks and improving usability. Therefore, we propose an Uncertainty-Driven Multi-Scale Feature Fusion Network (UMFFNet) that learns the probability mapping distribution between paired images to estimate uncertainty. Specifically, we introduce an uncertainty feature fusion block (UFFB) that utilizes uncertainty information to dynamically enhance acquired features and focus on blurry regions obscured by rain streaks, reducing prediction errors. In addition, to further boost the performance of UMFFNet, we fused feature information from multiple scales to guide the network for efficient collaborative rain removal. Extensive experiments demonstrate that UMFFNet achieves significant performance improvements with few parameters, surpassing state-of-the-art image deraining methods.
</details>
<details>
<summary>摘要</summary>
“视觉基于的测量系统经常受到雨水的影响，因为捕捉图像中的雨纹会导致图像质量下降。现有的成像设备很难在实时中解决这个问题。大多数努力都是基于深度网络进行图像抖干，并且已经取得了进步，但这些深度网络的参数较大，导致部署在资源有限的设备上存在问题。此外，这些数据驱动模型经常生成决定性的结果，而不考虑其内在的可能性不确定性，这可能导致不想要的重建错误。因此，我们提出了一种基于不确定性的多级特征融合网络（UMFFNet），它学习图像对的概率分布来Estimate uncertainty。具体来说，我们引入了不确定性特征融合块（UFFB），它利用不确定性信息来动态增强获取的特征并专注于雨水纹理覆盖的模糊区域，从而减少预测错误。此外，为了进一步提高 UMFFNet 的性能，我们将特征信息从多个级别融合，以便导引网络协同进行雨水 removing。广泛的实验表明， UMFFNet 可以在几个参数下实现显著的性能提升，超过当前的图像抖干方法。”
</details></li>
</ul>
<hr>
<h2 id="Flexible-single-multimode-fiber-imaging-using-white-LED"><a href="#Flexible-single-multimode-fiber-imaging-using-white-LED" class="headerlink" title="Flexible single multimode fiber imaging using white LED"></a>Flexible single multimode fiber imaging using white LED</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09714">http://arxiv.org/abs/2307.09714</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minyu Fan, Kun Liu, Jie Zhu, Yu Cao, Sha Wang</li>
<li>for: 这项研究旨在实现多模式纤维（MMF）成像，以增强其在成像和光学通信中的应用可靠性。</li>
<li>methods: 这项研究使用白光LED和堆叠UNet实现MMF成像，并通过通道缝合技术提高重建效果。</li>
<li>results: 实验结果显示，MMF成像系统具有良好的鲁棒性特性，即平均归一化相关系数（PCC）为0.83，并且可以通过修改纤维形状进行灵活应用。<details>
<summary>Abstract</summary>
Multimode fiber (MMF) has been proven to have good potential in imaging and optical communication because of its advantages of small diameter and large mode numbers. However, due to the mode coupling and modal dispersion, it is very sensitive to environmental changes. Minor changes in the fiber shape can lead to difficulties in information reconstruction. Here, white LED and cascaded Unet are used to achieve MMF imaging to eliminate the effect of fiber perturbations. The output speckle patterns in three different color channels of the CCD camera produced by transferring images through the MMF are concatenated and inputted into the cascaded Unet using channel stitching technology to improve the reconstruction effects. The average Pearson correlation coefficient (PCC) of the reconstructed images from the Fashion-MINIST dataset is 0.83. In order to check the flexibility of such a system, perturbation tests on the image reconstruction capability by changing the fiber shapes are conducted. The experimental results show that the MMF imaging system has good robustness properties, i. e. the average PCC remains 0.83 even after completely changing the shape of the MMF. This research potentially provides a flexible approach for the practical application of MMF imaging.
</details>
<details>
<summary>摘要</summary>
多模式纤维（MMF）因其小直径和大模数的优点，在成像和光通信中具有良好的潜力。然而，由于模式相互作用和模态散射，MMF在环境变化时非常敏感。小量的纤维形状变化可能导致信息重建困难。在这里，白光LED和堆叠网络被用来实现MMF成像，以消除纤维变化的影响。 transferred images through the MMF的输出雾Patterns in three different color channels of the CCD camera were concatenated and inputted into the cascaded Unet using channel stitching technology to improve the reconstruction effects. The average Pearson correlation coefficient (PCC) of the reconstructed images from the Fashion-MINIST dataset was 0.83. To check the flexibility of the system, perturbation tests on the image reconstruction capability by changing the fiber shapes were conducted. The experimental results showed that the MMF imaging system has good robustness properties, i.e., the average PCC remained 0.83 even after completely changing the shape of the MMF. This research potentially provides a flexible approach for the practical application of MMF imaging.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Transformer-based-Dual-domain-Network-for-Few-view-Dedicated-Cardiac-SPECT-Image-Reconstructions"><a href="#Transformer-based-Dual-domain-Network-for-Few-view-Dedicated-Cardiac-SPECT-Image-Reconstructions" class="headerlink" title="Transformer-based Dual-domain Network for Few-view Dedicated Cardiac SPECT Image Reconstructions"></a>Transformer-based Dual-domain Network for Few-view Dedicated Cardiac SPECT Image Reconstructions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09624">http://arxiv.org/abs/2307.09624</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huidong Xie, Bo Zhou, Xiongchao Chen, Xueqi Guo, Stephanie Thorn, Yi-Hwa Liu, Ge Wang, Albert Sinusas, Chi Liu</li>
<li>for: 用于提高心血管疾病诊断的高质量3D射电心脏成像 reconstruction。</li>
<li>methods: 提议使用3D transformer-based dual-domain网络TIP-Net，首先直接将投影数据重构为3D射电心脏成像，然后使用图像域重构网络进行进一步重构。</li>
<li>results: 在Cardiac catheterization图像和临床诊断 interpretations中，与前一代方法相比，我们的方法生成的图像具有更高的心脏缺陷对比度，可能启用站立几视图专用射电心脏成像仪器实现高质量缺陷visualization。<details>
<summary>Abstract</summary>
Cardiovascular disease (CVD) is the leading cause of death worldwide, and myocardial perfusion imaging using SPECT has been widely used in the diagnosis of CVDs. The GE 530/570c dedicated cardiac SPECT scanners adopt a stationary geometry to simultaneously acquire 19 projections to increase sensitivity and achieve dynamic imaging. However, the limited amount of angular sampling negatively affects image quality. Deep learning methods can be implemented to produce higher-quality images from stationary data. This is essentially a few-view imaging problem. In this work, we propose a novel 3D transformer-based dual-domain network, called TIP-Net, for high-quality 3D cardiac SPECT image reconstructions. Our method aims to first reconstruct 3D cardiac SPECT images directly from projection data without the iterative reconstruction process by proposing a customized projection-to-image domain transformer. Then, given its reconstruction output and the original few-view reconstruction, we further refine the reconstruction using an image-domain reconstruction network. Validated by cardiac catheterization images, diagnostic interpretations from nuclear cardiologists, and defect size quantified by an FDA 510(k)-cleared clinical software, our method produced images with higher cardiac defect contrast on human studies compared with previous baseline methods, potentially enabling high-quality defect visualization using stationary few-view dedicated cardiac SPECT scanners.
</details>
<details>
<summary>摘要</summary>
Cardiovascular disease (CVD) 是全球最主要的死亡原因，而我脏性 perfusion imaging 使用 SPECT 已广泛应用于 CVD 的诊断。GE 530/570c 专门的卡ди亚特 SPECT 扫描仪采用站ARY geometry 同时获取 19 个投影，以提高敏感度和实现动态扫描。然而，有限的角度采样会影响图像质量。深度学习方法可以在站ARY数据上生成更高质量的图像。这是一个几视图图像问题。在这种情况下，我们提出了一种新的3D transformer-based dual-domain网络，称为 TIP-Net，用于高质量3D卡ди亚特 SPECT 图像重建。我们的方法首先从投影数据直接重建3D卡ди亚特 SPECT 图像，而不需要迭代重建过程。然后，给我们的重建输出和原始几视图重建结果，我们进一步修改重建结果使用图像领域重建网络。被 cardiac catheterization 图像、核心 Cardiologist 的诊断和 FDA 510(k) 审核通过的产品软件进行评估，我们的方法在人类研究中比前一代方法产生了更高的心脏缺陷对比，可能使得使用站ARY几视图专门卡ди亚特 SPECT 扫描仪获得高质量缺陷可视化。
</details></li>
</ul>
<hr>
<h2 id="A-comparative-analysis-of-SRGAN-models"><a href="#A-comparative-analysis-of-SRGAN-models" class="headerlink" title="A comparative analysis of SRGAN models"></a>A comparative analysis of SRGAN models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09456">http://arxiv.org/abs/2307.09456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fatemeh Rezapoor Nikroo, Ajinkya Deshmukh, Anantha Sharma, Adrian Tam, Kaarthik Kumar, Cleo Norris, Aditya Dangi</li>
<li>for: 这个研究检验了多种现代SRGAN（超分解生成对抗网络）模型，ESRGAN、Real-ESRGAN和EDSR，在一个基准数据集上进行了评估。</li>
<li>methods: 这些模型使用了一个管道来恶化图像，以评估其性能。</li>
<li>results: 我们发现，EDSR-BASE模型从huggingface中表现最佳，在量化指标和主观视觉质量评估中都显示出最高的性能，同时具有最小的计算开销。 EDSR生成的图像具有更高的峰峰信号噪声比（PSNR）和结构相似性指标（SSIM）值，并且可以通过Tesseract OCR引擎获得高质量的文本Recognition结果。这些发现表明，EDSR是一种稳定和有效的单图像超分解方法，可以用于应用场景需要高品质视觉和优化计算。<details>
<summary>Abstract</summary>
In this study, we evaluate the performance of multiple state-of-the-art SRGAN (Super Resolution Generative Adversarial Network) models, ESRGAN, Real-ESRGAN and EDSR, on a benchmark dataset of real-world images which undergo degradation using a pipeline. Our results show that some models seem to significantly increase the resolution of the input images while preserving their visual quality, this is assessed using Tesseract OCR engine. We observe that EDSR-BASE model from huggingface outperforms the remaining candidate models in terms of both quantitative metrics and subjective visual quality assessments with least compute overhead. Specifically, EDSR generates images with higher peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) values and are seen to return high quality OCR results with Tesseract OCR engine. These findings suggest that EDSR is a robust and effective approach for single-image super-resolution and may be particularly well-suited for applications where high-quality visual fidelity is critical and optimized compute.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们评估了多种现状顶尖SRGAN（超分解生成对抗网络）模型，ESRGAN、Real-ESRGAN和EDSR，对一个参考数据集中的实际图像进行了降低管道处理。我们的结果显示，一些模型能够明显提高输入图像的分辨率，同时保持图像的视觉质量，这被评估使用Tesseract OCR引擎。我们发现，来自huggingface的EDSR-BASE模型在对比其他候选模型的情况下，在量化指标和主观视觉质量评估中具有最低的计算开销。具体来说，EDSR模型能够生成高 peak signal-to-noise ratio（PSNR）和结构相似度指数（SSIM）值更高的图像，并且能够返回高质量的OCR结果。这些发现表明，EDSR是一种稳定有效的单图像超分解方法，可能是应用场景需要高质量视觉准确性和优化计算的情况下的佳选。
</details></li>
</ul>
<hr>
<h2 id="Measuring-Student-Behavioral-Engagement-using-Histogram-of-Actions"><a href="#Measuring-Student-Behavioral-Engagement-using-Histogram-of-Actions" class="headerlink" title="Measuring Student Behavioral Engagement using Histogram of Actions"></a>Measuring Student Behavioral Engagement using Histogram of Actions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09420">http://arxiv.org/abs/2307.09420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Abdelkawy, Islam Alkabbany, Asem Ali, Aly Farag</li>
<li>For: 这个论文旨在提出一种新的学生行为参与度测量技术，通过识别学生的行为来预测学生行为参与度水平。* Methods: 该方法使用人体骨架模型来模拟学生的姿势和上半身运动，并使用3D-CNN模型来学习学生上半身的动态。然后将动作识别结果转化为行为参与度 histogram，并将 histogram 作为输入给 SVM 分类器来判断学生是否参与度高或低。* Results: 对于1414个2分钟视频段和112个视频段，实验结果显示学生的动作可以达到100%的准确率（83.63%），而该方法也可以捕捉课堂中学生的平均参与度。<details>
<summary>Abstract</summary>
In this paper, we propose a novel technique for measuring behavioral engagement through students' actions recognition. The proposed approach recognizes student actions then predicts the student behavioral engagement level. For student action recognition, we use human skeletons to model student postures and upper body movements. To learn the dynamics of student upper body, a 3D-CNN model is used. The trained 3D-CNN model is used to recognize actions within every 2minute video segment then these actions are used to build a histogram of actions which encodes the student actions and their frequencies. This histogram is utilized as an input to SVM classifier to classify whether the student is engaged or disengaged. To evaluate the proposed framework, we build a dataset consisting of 1414 2-minute video segments annotated with 13 actions and 112 video segments annotated with two engagement levels. Experimental results indicate that student actions can be recognized with top 1 accuracy 83.63% and the proposed framework can capture the average engagement of the class.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的行为参与度测量技术，通过学生的动作识别。我们的方法首先识别学生的动作，然后预测学生的行为参与度水平。为实现学生动作识别，我们使用人体骨架模型来模拟学生的姿势和上半身运动。为了学习学生上半身的动态，我们使用3D-CNN模型进行训练。训练完成后，我们使用3D-CNN模型来识别每个2分钟视频段中的动作，然后将这些动作组织成一个动作频率分布图。这个图表被用作SVM分类器的输入，以判断学生是否参与度高或低。为评估我们的框架，我们建立了一个包含1414个2分钟视频段和112个视频段的数据集，每个视频段被标注为13种动作和两个参与度水平。实验结果表明，我们的方法可以识别学生动作的准确率达83.63%，并且可以捕捉出课堂中学生的平均参与度。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/19/eess.IV_2023_07_19/" data-id="clluro5lv00ciq988fbsxhm0y" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/18/cs.LG_2023_07_18/" class="article-date">
  <time datetime="2023-07-17T16:00:00.000Z" itemprop="datePublished">2023-07-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/18/cs.LG_2023_07_18/">cs.LG - 2023-07-18 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Enhancing-Pattern-Classification-in-Support-Vector-Machines-through-Matrix-Formulation"><a href="#Enhancing-Pattern-Classification-in-Support-Vector-Machines-through-Matrix-Formulation" class="headerlink" title="Enhancing Pattern Classification in Support Vector Machines through Matrix Formulation"></a>Enhancing Pattern Classification in Support Vector Machines through Matrix Formulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09372">http://arxiv.org/abs/2307.09372</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sambhav Jain Reshma Rastogi</li>
<li>for: 本研究 paper 的目的是提出一种矩阵形式的支持向量机器 (Matrix SVM)，以解决现有 SVM 模型在多类和多标签 Setting 中的限制。</li>
<li>methods: 本研究使用 Accelerated Gradient Descent 方法在 dual 中进行优化，以提高解决 Matrix-SVM 问题的效率。</li>
<li>results: 实验结果表明，Matrix SVM 在多标签和多类数据集上可以 достичь更高的时间效果，同时保持与 Binary Relevance SVM 相同的结果 Waterfall 。此外，矩阵形式还提供了一些透彻的意见和优势，可能不会在传统的 vector-based notation 中显示出来。<details>
<summary>Abstract</summary>
Support Vector Machines (SVM) have gathered significant acclaim as classifiers due to their successful implementation of Statistical Learning Theory. However, in the context of multiclass and multilabel settings, the reliance on vector-based formulations in existing SVM-based models poses limitations regarding flexibility and ease of incorporating additional terms to handle specific challenges. To overcome these limitations, our research paper focuses on introducing a matrix formulation for SVM that effectively addresses these constraints. By employing the Accelerated Gradient Descent method in the dual, we notably enhance the efficiency of solving the Matrix-SVM problem. Experimental evaluations on multilabel and multiclass datasets demonstrate that Matrix SVM achieves superior time efficacy while delivering similar results to Binary Relevance SVM.   Moreover, our matrix formulation unveils crucial insights and advantages that may not be readily apparent in traditional vector-based notations. We emphasize that numerous multilabel models can be viewed as extensions of SVM, with customised modifications to meet specific requirements. The matrix formulation presented in this paper establishes a solid foundation for developing more sophisticated models capable of effectively addressing the distinctive challenges encountered in multilabel learning.
</details>
<details>
<summary>摘要</summary>
支持向量机 (SVM) 在分类方面受到了广泛的赞誉，因为它们成功地应用了统计学学习理论。然而，在多类和多标签设置下，现有的 SVM 基本模型的向量化表述带来了灵活性和特定挑战处理的局限性。为了突破这些限制，我们的研究论文关注在 introducing 矩阵表述方法来解决这些问题。在 dual 中使用加速 gradient descent 方法，我们可以 notable 提高矩阵-SVM 问题的解决效率。实验评估在多标签和多类 datasets 上表明，矩阵 SVM 可以很快地完成任务，同时与 binary relevance SVM 的结果相似。此外，我们的矩阵表述还揭示了一些不太明显的优点和意义，它们可能不会在传统的向量化notation中得到表达。我们强调，许多多标签模型可以被视为 SVM 的扩展，通过自定义修改来满足特定的需求。矩阵表述在这篇论文中建立了一个坚实的基础，用于开发更加复杂的模型，以更好地Addressing 多标签学习中的特殊挑战。
</details></li>
</ul>
<hr>
<h2 id="Explanation-Guided-Fair-Federated-Learning-for-Transparent-6G-RAN-Slicing"><a href="#Explanation-Guided-Fair-Federated-Learning-for-Transparent-6G-RAN-Slicing" class="headerlink" title="Explanation-Guided Fair Federated Learning for Transparent 6G RAN Slicing"></a>Explanation-Guided Fair Federated Learning for Transparent 6G RAN Slicing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09494">http://arxiv.org/abs/2307.09494</a></li>
<li>repo_url: None</li>
<li>paper_authors: Swastika Roy, Hatim Chergui, Christos Verikoukis</li>
<li>for: 这个论文主要目标是建立在6G网络自动化中的透明性和可信度，通过使用可解释人工智能（XAI）技术来帮助建立AI黑obox中的信任。</li>
<li>methods: 这篇论文使用了closed-loop自动化和解释导向学习（EGL）的方法，并采用Jensen-Shannon（JS）差分来评估模型的解释。</li>
<li>results: 实验结果表明，提出的EGFL-JS方案可以提高了6G网络中RAN掉 Package的损失概率预测的可靠性和公平性，相比之下其他基于文献的基elines的性能提高了 более50%，并且提高了Recall metric的评价分。<details>
<summary>Abstract</summary>
Future zero-touch artificial intelligence (AI)-driven 6G network automation requires building trust in the AI black boxes via explainable artificial intelligence (XAI), where it is expected that AI faithfulness would be a quantifiable service-level agreement (SLA) metric along with telecommunications key performance indicators (KPIs). This entails exploiting the XAI outputs to generate transparent and unbiased deep neural networks (DNNs). Motivated by closed-loop (CL) automation and explanation-guided learning (EGL), we design an explanation-guided federated learning (EGFL) scheme to ensure trustworthy predictions by exploiting the model explanation emanating from XAI strategies during the training run time via Jensen-Shannon (JS) divergence. Specifically, we predict per-slice RAN dropped traffic probability to exemplify the proposed concept while respecting fairness goals formulated in terms of the recall metric which is included as a constraint in the optimization task. Finally, the comprehensiveness score is adopted to measure and validate the faithfulness of the explanations quantitatively. Simulation results show that the proposed EGFL-JS scheme has achieved more than $50\%$ increase in terms of comprehensiveness compared to different baselines from the literature, especially the variant EGFL-KL that is based on the Kullback-Leibler Divergence. It has also improved the recall score with more than $25\%$ relatively to unconstrained-EGFL.
</details>
<details>
<summary>摘要</summary>
未来零点touch的人工智能（AI）驱动的6G网络自动化需要建立AI黑盒子中的信任，通过可解释人工智能（XAI），其中AI的忠诚性将被视为服务级别协议（SLA）度量标准和电信键性表现指标（KPI）。这意味着利用XAI输出生成透明和不偏的深度神经网络（DNNs）。驱动closed-loop（CL）自动化和解释导向学习（EGL），我们设计了一种解释导向联合学习（EGFL）方案，以确保可靠的预测，通过在训练过程中使用XAI策略生成的模型解释。例如，我们预测了无线网络承载层损失报告概率，以 illustrate the proposed concept，并且遵循公平性目标表示为Recall度量，并包含在优化任务中。最后，我们采用了completeness分数来评估和验证解释的 faithfulness 量化。实验结果显示，我们的EGFL-JS方案在比较多个基elines之前获得了超过50%的提高，特别是与基于Kullback-Leibler分配的EGFL-KL变体相比，以及不受限制的EGFL。此外，EGFL-JS还提高了Recall分数，相比未受限制的EGFL，提高了超过25%。
</details></li>
</ul>
<hr>
<h2 id="Sparse-Gaussian-Graphical-Models-with-Discrete-Optimization-Computational-and-Statistical-Perspectives"><a href="#Sparse-Gaussian-Graphical-Models-with-Discrete-Optimization-Computational-and-Statistical-Perspectives" class="headerlink" title="Sparse Gaussian Graphical Models with Discrete Optimization: Computational and Statistical Perspectives"></a>Sparse Gaussian Graphical Models with Discrete Optimization: Computational and Statistical Perspectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09366">http://arxiv.org/abs/2307.09366</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kayhan Behdin, Wenyu Chen, Rahul Mazumder</li>
<li>For: The paper aims to estimate the inverse covariance matrix of a multivariate Gaussian distribution, assuming it is sparse.* Methods: The proposed method, called GraphL0BnB, is based on an $\ell_0$-penalized version of the pseudolikelihood function and uses a custom nonlinear branch-and-bound framework to solve the resulting mixed integer program.* Results: The paper reports numerical experiments on real and synthetic datasets that demonstrate the effectiveness of GraphL0BnB in solving the problem to near-optimality, even for large problem instances with $p &#x3D; 10^4$ variables. The paper also compares the performance of GraphL0BnB with various state-of-the-art approaches.Here are the three points in Simplified Chinese:* For: 本文目标是估计多变量 Gaussian 分布下的对角矩阵，假设它是稀疏的。* Methods: 提议的方法是基于 $\ell_0$ 约束的 Pseudolikelihood 函数，使用自定义的非线性分支和约束搜索 Framework 解决 resulting 混合整数程序。* Results: 文章报告了使用真实和 sintetic 数据进行的数值实验，表明 GraphL0BnB 可以准确地解决问题，即使 пробле 的规模很大，例如 $p &#x3D; 10^4$ 变量。文章还比较了 GraphL0BnB 与不同的现有方法的性能。<details>
<summary>Abstract</summary>
We consider the problem of learning a sparse graph underlying an undirected Gaussian graphical model, a key problem in statistical machine learning. Given $n$ samples from a multivariate Gaussian distribution with $p$ variables, the goal is to estimate the $p \times p$ inverse covariance matrix (aka precision matrix), assuming it is sparse (i.e., has a few nonzero entries). We propose GraphL0BnB, a new estimator based on an $\ell_0$-penalized version of the pseudolikelihood function, while most earlier approaches are based on the $\ell_1$-relaxation. Our estimator can be formulated as a convex mixed integer program (MIP) which can be difficult to compute at scale using off-the-shelf commercial solvers. To solve the MIP, we propose a custom nonlinear branch-and-bound (BnB) framework that solves node relaxations with tailored first-order methods. As a by-product of our BnB framework, we propose large-scale solvers for obtaining good primal solutions that are of independent interest. We derive novel statistical guarantees (estimation and variable selection) for our estimator and discuss how our approach improves upon existing estimators. Our numerical experiments on real/synthetic datasets suggest that our method can solve, to near-optimality, problem instances with $p = 10^4$ -- corresponding to a symmetric matrix of size $p \times p$ with $p^2/2$ binary variables. We demonstrate the usefulness of GraphL0BnB versus various state-of-the-art approaches on a range of datasets.
</details>
<details>
<summary>摘要</summary>
我们考虑一个学习简单Graph的问题，即在无向 Gaussian 统计模型中学习一个简单的 inverse covariance 矩阵（即精度矩阵）。假设这个矩阵只有几个非零元素。我们提出了一个新的估计器，即 GraphL0BnB，它基于一个 $\ell_0$-检测的扩展版 pseudolikelihood 函数。大多数先前的方法则是基于 $\ell_1$-缓和。我们的估计器可以表示为一个内部为整数的混合整数程式（MIP），它可能需要大量的计算资源使用现成的商业解决方案。为解决 MIP，我们提出了一个自定义的非线性分支与缓解（BnB）框架，它可以解决节点缓和使用特制的首项方法。作为我们的 BnB 框架的一个副产物，我们提出了一些大规模的解决方案，可以实现良好的原始解。我们 derive novel 的 statistically  garantuees（估计和变数选择） для我们的估计器，并讨论了我们的方法与先前的方法之间的优化。我们的数据实验表明，我们的方法可以对实际数据进行几乎优质的估计，并且可以解决具有 $p = 10^4$ 的问题，即一个对应的矩阵中的 $p^2/2$ 个二进制变数。我们还证明了我们的方法在各种数据集上的优化。
</details></li>
</ul>
<hr>
<h2 id="An-Evaluation-of-Zero-Cost-Proxies-–-from-Neural-Architecture-Performance-to-Model-Robustness"><a href="#An-Evaluation-of-Zero-Cost-Proxies-–-from-Neural-Architecture-Performance-to-Model-Robustness" class="headerlink" title="An Evaluation of Zero-Cost Proxies – from Neural Architecture Performance to Model Robustness"></a>An Evaluation of Zero-Cost Proxies – from Neural Architecture Performance to Model Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09365">http://arxiv.org/abs/2307.09365</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jovita Lukasik, Michael Moeller, Margret Keuper</li>
<li>for: 本文研究zero-cost proxy的应用在 neural architecture search 中，特别是在robustness和clean accuracy之间的joint search。</li>
<li>methods: 本文使用现有的zero-cost proxy来预测模型的性能，并分析这些proxy的特征重要性。</li>
<li>results: 本文发现，单个proxy预测robustness的任务相对更加困难，需要考虑多个proxy来预测模型的robustness。同时，clean accuracy可以由单个proxy进行预测。<details>
<summary>Abstract</summary>
Zero-cost proxies are nowadays frequently studied and used to search for neural architectures. They show an impressive ability to predict the performance of architectures by making use of their untrained weights. These techniques allow for immense search speed-ups. So far the joint search for well-performing and robust architectures has received much less attention in the field of NAS. Therefore, the main focus of zero-cost proxies is the clean accuracy of architectures, whereas the model robustness should play an evenly important part. In this paper, we analyze the ability of common zero-cost proxies to serve as performance predictors for robustness in the popular NAS-Bench-201 search space. We are interested in the single prediction task for robustness and the joint multi-objective of clean and robust accuracy. We further analyze the feature importance of the proxies and show that predicting the robustness makes the prediction task from existing zero-cost proxies more challenging. As a result, the joint consideration of several proxies becomes necessary to predict a model's robustness while the clean accuracy can be regressed from a single such feature.
</details>
<details>
<summary>摘要</summary>
现在，零成本代理常常被研究和使用来搜索神经网络架构。它们能够很好地预测架构的性能，只使用未训练的权重。这些技术可以大大减少搜索速度。迄今为止，搜索well-performing和Robust Architecture在神经网络搜索领域中得到了相对较少的关注。因此，零成本代理的主要焦点是神经网络的净精度，而模型的稳定性应该具有相同的重要性。在这篇论文中，我们分析了常见的零成本代理是否能够用来预测模型的稳定性在NAS-Bench-201搜索空间中。我们对单个预测任务和多目标任务（净精度和稳定性）进行分析。我们还分析了代理的特征重要性，并发现预测稳定性使得预测任务更加困难。因此，需要结合多个代理来预测模型的稳定性，而净精度可以从单个特征进行回归。
</details></li>
</ul>
<hr>
<h2 id="MOCA-Self-supervised-Representation-Learning-by-Predicting-Masked-Online-Codebook-Assignments"><a href="#MOCA-Self-supervised-Representation-Learning-by-Predicting-Masked-Online-Codebook-Assignments" class="headerlink" title="MOCA: Self-supervised Representation Learning by Predicting Masked Online Codebook Assignments"></a>MOCA: Self-supervised Representation Learning by Predicting Masked Online Codebook Assignments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09361">http://arxiv.org/abs/2307.09361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Spyros Gidaris, Andrei Bursuc, Oriane Simeoni, Antonin Vobecky, Nikos Komodakis, Matthieu Cord, Patrick Pérez</li>
<li>for: 降低视Transformer网络的贪吃需求，使用自监学习来减少大量完全标注数据的需求。</li>
<li>methods: 提出了一种单Stage和独立的方法MOCA，通过使用高级特征定义的面积预测任务来捕捉具有良好上下文理解性和图像变化不变性的两种自监学习方法。</li>
<li>results: 在低投入设定下实现新的状态纪录Results，并在多种评估协议中显示出了强大的实验性能，训练时间至少3倍 быстреeder than先前的方法。<details>
<summary>Abstract</summary>
Self-supervised learning can be used for mitigating the greedy needs of Vision Transformer networks for very large fully-annotated datasets. Different classes of self-supervised learning offer representations with either good contextual reasoning properties, e.g., using masked image modeling strategies, or invariance to image perturbations, e.g., with contrastive methods. In this work, we propose a single-stage and standalone method, MOCA, which unifies both desired properties using novel mask-and-predict objectives defined with high-level features (instead of pixel-level details). Moreover, we show how to effectively employ both learning paradigms in a synergistic and computation-efficient way. Doing so, we achieve new state-of-the-art results on low-shot settings and strong experimental results in various evaluation protocols with a training that is at least 3 times faster than prior methods.
</details>
<details>
<summary>摘要</summary>
自我监督学习可以用于减轻视力转换网络的贪吃需求，需要很大的完全标注数据集。不同类型的自我监督学习可以提供具有不同特性的表示，如使用遮盲图像模型策略获得良好的上下文理解性，或者使用对比方法获得图像变化的抗变异性。在这项工作中，我们提出了一种单阶段、独立的方法MOCA，它通过定义高级特征的新式遮盲预测目标来 объединить这两种愿望的特性。此外，我们还示了如何有效地使用这两种学习方法，以实现 computation-efficient 的synergistic效果。通过这种方法，我们在低投入设定下 achieve new state-of-the-art 的结果，并在多种评估协议中获得了强大的实验结果，并且训练时间至少3倍 быстреeder than priori方法。
</details></li>
</ul>
<hr>
<h2 id="Using-the-IBM-Analog-In-Memory-Hardware-Acceleration-Kit-for-Neural-Network-Training-and-Inference"><a href="#Using-the-IBM-Analog-In-Memory-Hardware-Acceleration-Kit-for-Neural-Network-Training-and-Inference" class="headerlink" title="Using the IBM Analog In-Memory Hardware Acceleration Kit for Neural Network Training and Inference"></a>Using the IBM Analog In-Memory Hardware Acceleration Kit for Neural Network Training and Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09357">http://arxiv.org/abs/2307.09357</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/IBM/aihwkit">https://github.com/IBM/aihwkit</a></li>
<li>paper_authors: Manuel Le Gallo, Corey Lammie, Julian Buechel, Fabio Carta, Omobayode Fagbohungbe, Charles Mackin, Hsinyu Tsai, Vijay Narayanan, Abu Sebastian, Kaoutar El Maghraoui, Malte J. Rasch</li>
<li>for: 本文旨在介绍如何在Analog In-Memory Computing（AIMC）硬件上部署深度神经网络（DNN）推理和训练，以实现与数字计算相同的准确性。</li>
<li>methods: 本文使用IBM的Analog Hardware Acceleration Kit（AIHWKit）Python库来模拟DNN的推理和训练。AIHWKit提供了各种功能和最佳实践来进行推理和训练。</li>
<li>results: 本文提供了AIHWKit在推理和训练DNN时的性能分析和评估。此外，本文还介绍了Analog AI Cloud Composer，它提供了使用AIHWKit simulation platform的完全托管云环境的优势。<details>
<summary>Abstract</summary>
Analog In-Memory Computing (AIMC) is a promising approach to reduce the latency and energy consumption of Deep Neural Network (DNN) inference and training. However, the noisy and non-linear device characteristics, and the non-ideal peripheral circuitry in AIMC chips, require adapting DNNs to be deployed on such hardware to achieve equivalent accuracy to digital computing. In this tutorial, we provide a deep dive into how such adaptations can be achieved and evaluated using the recently released IBM Analog Hardware Acceleration Kit (AIHWKit), freely available at https://github.com/IBM/aihwkit. The AIHWKit is a Python library that simulates inference and training of DNNs using AIMC. We present an in-depth description of the AIHWKit design, functionality, and best practices to properly perform inference and training. We also present an overview of the Analog AI Cloud Composer, that provides the benefits of using the AIHWKit simulation platform in a fully managed cloud setting. Finally, we show examples on how users can expand and customize AIHWKit for their own needs. This tutorial is accompanied by comprehensive Jupyter Notebook code examples that can be run using AIHWKit, which can be downloaded from https://github.com/IBM/aihwkit/tree/master/notebooks/tutorial.
</details>
<details>
<summary>摘要</summary>
智能存储计算（AIMC）是一种有前途的方法，以减少深度神经网络（DNN）的延迟和能耗。然而，设备特性和周围电路在AIMC芯片上是不准确的，需要适应DNN来实现相同的准确率。在这个教程中，我们将提供深入的解释如何实现和评估这些适应，以及使用IBM的分析硬件加速器包（AIHWKit）进行 simulate inference和训练。AIHWKit是一个基于Python的库，可以模拟DNN的推理和训练 using AIMC。我们将提供AIHWKit的设计、功能和最佳实践，以及使用Analog AI Cloud Composer在云环境中实现相应的优势。最后，我们将展示用户如何扩展和自定义AIHWKit。这个教程的详细 Jupyter Notebook 代码示例可以从https://github.com/IBM/aihwkit/tree/master/notebooks/tutorial下载。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Select-SAT-Encodings-for-Pseudo-Boolean-and-Linear-Integer-Constraints"><a href="#Learning-to-Select-SAT-Encodings-for-Pseudo-Boolean-and-Linear-Integer-Constraints" class="headerlink" title="Learning to Select SAT Encodings for Pseudo-Boolean and Linear Integer Constraints"></a>Learning to Select SAT Encodings for Pseudo-Boolean and Linear Integer Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09342">http://arxiv.org/abs/2307.09342</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/felixvuo/lease-data">https://github.com/felixvuo/lease-data</a></li>
<li>paper_authors: Felix Ulrich-Oltean, Peter Nightingale, James Alfred Walker</li>
<li>for: 解决复杂的满足和优化问题</li>
<li>methods: 使用超级vised机器学习方法选择编码</li>
<li>results: 比AutoFolio好，可以选择不同类型的问题编码<details>
<summary>Abstract</summary>
Many constraint satisfaction and optimisation problems can be solved effectively by encoding them as instances of the Boolean Satisfiability problem (SAT). However, even the simplest types of constraints have many encodings in the literature with widely varying performance, and the problem of selecting suitable encodings for a given problem instance is not trivial. We explore the problem of selecting encodings for pseudo-Boolean and linear constraints using a supervised machine learning approach. We show that it is possible to select encodings effectively using a standard set of features for constraint problems; however we obtain better performance with a new set of features specifically designed for the pseudo-Boolean and linear constraints. In fact, we achieve good results when selecting encodings for unseen problem classes. Our results compare favourably to AutoFolio when using the same feature set. We discuss the relative importance of instance features to the task of selecting the best encodings, and compare several variations of the machine learning method.
</details>
<details>
<summary>摘要</summary>
许多约束满足和优化问题可以有效地通过编码为布尔满足问题（SAT）解决。然而，最简单的约束类型还有许多编码方法在文献中，这些编码方法之间的性能差异很大，选择适合的编码方法 для给定问题实例是一个不容易的问题。我们使用监督式机器学习方法来选择编码方法，并证明可以使用标准的约束问题特征集来选择编码方法，但是使用专门为 Pseudo-Boolean 和线性约束设计的新特征集可以获得更好的性能。我们在使用同一集特征时与 AutoFolio 进行比较，并讨论实例特征对选择最佳编码方法的重要性。我们还比较了几种机器学习方法的变种。
</details></li>
</ul>
<hr>
<h2 id="Towards-Automated-Semantic-Segmentation-in-Mammography-Images"><a href="#Towards-Automated-Semantic-Segmentation-in-Mammography-Images" class="headerlink" title="Towards Automated Semantic Segmentation in Mammography Images"></a>Towards Automated Semantic Segmentation in Mammography Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10296">http://arxiv.org/abs/2307.10296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cesar A. Sierra-Franco, Jan Hurtado, Victor de A. Thomaz, Leonardo C. da Cruz, Santiago V. Silva, Alberto B. Raposo</li>
<li>for: 检测非可触护乳腺癌，提供诊断和评估图像质量的机会。</li>
<li>methods: 使用深度学习框架自动 segmenting 乳腺、肌肉、肉细胞和脂肪组织的边界。</li>
<li>results: 在多种不同的框架和图像 dataset 下，实现了准确的 segmentation 性能，表明该框架可以在临床实践中整合。<details>
<summary>Abstract</summary>
Mammography images are widely used to detect non-palpable breast lesions or nodules, preventing cancer and providing the opportunity to plan interventions when necessary. The identification of some structures of interest is essential to make a diagnosis and evaluate image adequacy. Thus, computer-aided detection systems can be helpful in assisting medical interpretation by automatically segmenting these landmark structures. In this paper, we propose a deep learning-based framework for the segmentation of the nipple, the pectoral muscle, the fibroglandular tissue, and the fatty tissue on standard-view mammography images. We introduce a large private segmentation dataset and extensive experiments considering different deep-learning model architectures. Our experiments demonstrate accurate segmentation performance on variate and challenging cases, showing that this framework can be integrated into clinical practice.
</details>
<details>
<summary>摘要</summary>
乳影像广泛用于检测不可触感乳腺癌病或肿块，预防癌病并提供诊断和治疗计划时的机会。确定一些关键结构的标识是诊断和评估影像质量的关键。因此，计算机助成检测系统可以帮助医疗解释人员自动分割关键结构。在这篇论文中，我们提出了基于深度学习的框架，用于标识乳膜、肌肉、肉絮组织和脂肪组织在标准视图乳影像中的自动分割。我们提供了大量私有分割数据集和广泛的实验，考虑了不同的深度学习模型架构。我们的实验结果表明，这种框架在多种和复杂的案例中具有高准确性，这表明该框架可以在临床实践中集成。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Field-Dependencies-for-Learning-on-Categorical-Data"><a href="#Exploiting-Field-Dependencies-for-Learning-on-Categorical-Data" class="headerlink" title="Exploiting Field Dependencies for Learning on Categorical Data"></a>Exploiting Field Dependencies for Learning on Categorical Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09321">http://arxiv.org/abs/2307.09321</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/csiro-robotics/mdl">https://github.com/csiro-robotics/mdl</a></li>
<li>paper_authors: Zhibin Li, Piotr Koniusz, Lu Zhang, Daniel Edward Pagendam, Peyman Moghadam</li>
<li>for: 学习 categorical 数据中的依赖关系，以提高模型的准确率和稳定性。</li>
<li>methods: 提出了一种新的方法，通过学习全局字段依赖矩阵，然后在实例级别使用不同的权重（即本地依赖模型）来提高字段间的模型。</li>
<li>results: 在六个popular数据集上比较了多种现有方法，并达到了更高的准确率和稳定性。详细的ablation study提供了更多的内容。<details>
<summary>Abstract</summary>
Traditional approaches for learning on categorical data underexploit the dependencies between columns (\aka fields) in a dataset because they rely on the embedding of data points driven alone by the classification/regression loss. In contrast, we propose a novel method for learning on categorical data with the goal of exploiting dependencies between fields. Instead of modelling statistics of features globally (i.e., by the covariance matrix of features), we learn a global field dependency matrix that captures dependencies between fields and then we refine the global field dependency matrix at the instance-wise level with different weights (so-called local dependency modelling) w.r.t. each field to improve the modelling of the field dependencies. Our algorithm exploits the meta-learning paradigm, i.e., the dependency matrices are refined in the inner loop of the meta-learning algorithm without the use of labels, whereas the outer loop intertwines the updates of the embedding matrix (the matrix performing projection) and global dependency matrix in a supervised fashion (with the use of labels). Our method is simple yet it outperforms several state-of-the-art methods on six popular dataset benchmarks. Detailed ablation studies provide additional insights into our method.
</details>
<details>
<summary>摘要</summary>
传统方法学习 categorical 数据会忽略数据集中列（即字段）之间的依赖关系，因为它们基于单独的分类/回归损失来驱动数据点的嵌入。相比之下，我们提出了一种新的方法，旨在利用数据集中列之间的依赖关系。而不是通过特征的全局统计来模型特征（即特征的covariance矩阵），我们学习一个全局字段依赖矩阵，然后在每个实例级别使用不同的权重（即本地依赖模型）来改进字段依赖的模型。我们的算法利用了元学习 парадиг，即依赖矩阵在内Loop中被反复更新，而外Loop则在有标签的情况下，将插入矩阵的更新和全局依赖矩阵的更新相互交互。我们的方法简单，但它在六个流行的数据集benchmark上表现出色，并且我们进行了详细的拟合分析，以提供更多的准确性。
</details></li>
</ul>
<hr>
<h2 id="Biomaker-CA-a-Biome-Maker-project-using-Cellular-Automata"><a href="#Biomaker-CA-a-Biome-Maker-project-using-Cellular-Automata" class="headerlink" title="Biomaker CA: a Biome Maker project using Cellular Automata"></a>Biomaker CA: a Biome Maker project using Cellular Automata</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09320">http://arxiv.org/abs/2307.09320</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ettore Randazzo, Alexander Mordvintsev</li>
<li>for: 这个论文是关于使用细胞自动机（CA）模拟生物体的生长和进化的研究。</li>
<li>methods: 这个研究使用了Python JAX框架对2D网格上的CA规则进行并行计算，并提供了不同的环境和物理法则，以及不同的模型架构和 мутаagen策略。</li>
<li>results: 研究人员通过模拟不同的环境和物理法则，证明了植物代理可以在缺乏营养的环境中生长、存活、繁殖和演化，并且可以通过用户交互式进行进化。<details>
<summary>Abstract</summary>
We introduce Biomaker CA: a Biome Maker project using Cellular Automata (CA). In Biomaker CA, morphogenesis is a first class citizen and small seeds need to grow into plant-like organisms to survive in a nutrient starved environment and eventually reproduce with variation so that a biome survives for long timelines. We simulate complex biomes by means of CA rules in 2D grids and parallelize all of its computation on GPUs through the Python JAX framework. We show how this project allows for several different kinds of environments and laws of 'physics', alongside different model architectures and mutation strategies. We further analyze some configurations to show how plant agents can grow, survive, reproduce, and evolve, forming stable and unstable biomes. We then demonstrate how one can meta-evolve models to survive in a harsh environment either through end-to-end meta-evolution or by a more surgical and efficient approach, called Petri dish meta-evolution. Finally, we show how to perform interactive evolution, where the user decides how to evolve a plant model interactively and then deploys it in a larger environment. We open source Biomaker CA at: https://tinyurl.com/2x8yu34s .
</details>
<details>
<summary>摘要</summary>
我们介绍生物创造器 CA：一个基因组织（CA）的生物创造项目。在生物创造器 CA 中，形态形成是一等公民，小种子需要在营养不足的环境中增长成植物如果生存，并最终繁殖，具有多样性，以确保生物群落长期存活。我们使用2D网格上的 CA 规则来模拟复杂的生态系统，并通过 Python JAX 框架在 GPU 上分布式计算。我们显示了这个项目可以支援多种环境和物理法则，以及不同的模型架构和突变策略。我们进一步分析了一些配置，说明植物代表如何在营养不足的环境中增长、存活、繁殖和演化，形成稳定和不稳定的生态系统。我们还示出了如何使用终端进化来在严峻环境中存活，或者使用更精确和高效的方法，即 Petri dish 进化。最后，我们显示了如何进行互动演化，让用户可以在互动式的方式下演化植物模型，然后将其部署到更大的环境中。我们开源了生物创造器 CA，请参考以下连结：https://tinyurl.com/2x8yu34s。
</details></li>
</ul>
<hr>
<h2 id="Multi-Modal-Discussion-Transformer-Integrating-Text-Images-and-Graph-Transformers-to-Detect-Hate-Speech-on-Social-Media"><a href="#Multi-Modal-Discussion-Transformer-Integrating-Text-Images-and-Graph-Transformers-to-Detect-Hate-Speech-on-Social-Media" class="headerlink" title="Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media"></a>Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09312">http://arxiv.org/abs/2307.09312</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liamhebert/multimodaldiscussiontransformer">https://github.com/liamhebert/multimodaldiscussiontransformer</a></li>
<li>paper_authors: Liam Hebert, Gaurav Sahu, Nanda Kishore Sreenivas, Lukasz Golab, Robin Cohen</li>
<li>for: 本研究的目的是开发一种基于多模态图表示的 hate speech 检测模型，以捕捉在在线社交网络中的谩骂语言。</li>
<li>methods: 该模型使用图transformer来捕捉整个讨论的上下文关系，并使用杂合层将文本和图像嵌入组合以取代单modal处理。</li>
<li>results: 对于基elines进行比较，我们发现我们的模型在检测 hate speech 方面的性能明显提高了，并进行了广泛的ablation研究。Translation:</li>
<li>for: The purpose of this study is to develop a hate speech detection model based on multimodal graph representation, to capture anti-social behavior in online social networks.</li>
<li>methods: The model uses graph transformers to capture the contextual relationships in the entire discussion, and combines text and image embeddings using fusion layers instead of processing them separately.</li>
<li>results: Compared to baselines, our model shows significantly improved performance in detecting hate speech, and we conducted extensive ablation studies.<details>
<summary>Abstract</summary>
We present the Multi-Modal Discussion Transformer (mDT), a novel multi-modal graph-based transformer model for detecting hate speech in online social networks. In contrast to traditional text-only methods, our approach to labelling a comment as hate speech centers around the holistic analysis of text and images. This is done by leveraging graph transformers to capture the contextual relationships in the entire discussion that surrounds a comment, with interwoven fusion layers to combine text and image embeddings instead of processing different modalities separately. We compare the performance of our model to baselines that only process text; we also conduct extensive ablation studies. We conclude with future work for multimodal solutions to deliver social value in online contexts, arguing that capturing a holistic view of a conversation greatly advances the effort to detect anti-social behavior.
</details>
<details>
<summary>摘要</summary>
我们介绍了多模态讨论变换器（mDT），一种新的多模态图基于变换器模型，用于在社交网络上探测 hate speech。与传统的文本Only方法不同，我们的方法是根据对评论的全面分析，包括文本和图像。我们利用图transformer来捕捉讨论中的上下文关系，并通过交叠卷积层将文本和图像嵌入拼接在一起，而不是分开处理不同的模式。我们与基线对比，并进行了广泛的剖析研究。我们 conclude 未来的多模态解决方案可以为在线上提供社会价值，因为捕捉讨论的全面视图可以帮助探测反社会行为。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Differentiation-for-Inverse-Problems-with-Applications-in-Quantum-Transport"><a href="#Automatic-Differentiation-for-Inverse-Problems-with-Applications-in-Quantum-Transport" class="headerlink" title="Automatic Differentiation for Inverse Problems with Applications in Quantum Transport"></a>Automatic Differentiation for Inverse Problems with Applications in Quantum Transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09311">http://arxiv.org/abs/2307.09311</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ivan Williams, Eric Polizzi</li>
<li>for:  inverse quantum transport problem</li>
<li>methods: neural solver and differentiable simulation</li>
<li>results: engineering continuous transmission properties and current-voltage characteristics<details>
<summary>Abstract</summary>
A neural solver and differentiable simulation of the quantum transmitting boundary model is presented for the inverse quantum transport problem. The neural solver is used to engineer continuous transmission properties and the differentiable simulation is used to engineer current-voltage characteristics.
</details>
<details>
<summary>摘要</summary>
neural 算法和可导的量子传输边界模型是用于反向量 calculus 问题的解决方案。 neural 算法用于引入连续传输性质，而可导的 simulate 用于引入电流-电压特性。
</details></li>
</ul>
<hr>
<h2 id="EigenTrajectory-Low-Rank-Descriptors-for-Multi-Modal-Trajectory-Forecasting"><a href="#EigenTrajectory-Low-Rank-Descriptors-for-Multi-Modal-Trajectory-Forecasting" class="headerlink" title="EigenTrajectory: Low-Rank Descriptors for Multi-Modal Trajectory Forecasting"></a>EigenTrajectory: Low-Rank Descriptors for Multi-Modal Trajectory Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09306">http://arxiv.org/abs/2307.09306</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/inhwanbae/eigentrajectory">https://github.com/inhwanbae/eigentrajectory</a></li>
<li>paper_authors: Inhwan Bae, Jean Oh, Hae-Gon Jeon</li>
<li>For: 本研究旨在提高人行道预测的精度和可靠性，通过使用新的轨迹描述符来减少轨迹的维度。* Methods: 我们使用一种新的轨迹描述符，将人行道径变换为一个紧凑的 $\mathbb{ET}$ 空间，然后使用现有的轨迹预测模型进行预测。此外，我们还提出了一种基于轨迹锚点的修正方法，以覆盖所有可能的未来。* Results: 我们的实验结果表明，使用我们的 EigenTrajectory 预测器可以显著提高现有轨迹预测模型的预测精度和可靠性，这表明我们的描述符适用于表示行人行为。代码可以在 <a target="_blank" rel="noopener" href="https://github.com/inhwanbae/EigenTrajectory">https://github.com/inhwanbae/EigenTrajectory</a> 上下载。<details>
<summary>Abstract</summary>
Capturing high-dimensional social interactions and feasible futures is essential for predicting trajectories. To address this complex nature, several attempts have been devoted to reducing the dimensionality of the output variables via parametric curve fitting such as the B\'ezier curve and B-spline function. However, these functions, which originate in computer graphics fields, are not suitable to account for socially acceptable human dynamics. In this paper, we present EigenTrajectory ($\mathbb{ET}$), a trajectory prediction approach that uses a novel trajectory descriptor to form a compact space, known here as $\mathbb{ET}$ space, in place of Euclidean space, for representing pedestrian movements. We first reduce the complexity of the trajectory descriptor via a low-rank approximation. We transform the pedestrians' history paths into our $\mathbb{ET}$ space represented by spatio-temporal principle components, and feed them into off-the-shelf trajectory forecasting models. The inputs and outputs of the models as well as social interactions are all gathered and aggregated in the corresponding $\mathbb{ET}$ space. Lastly, we propose a trajectory anchor-based refinement method to cover all possible futures in the proposed $\mathbb{ET}$ space. Extensive experiments demonstrate that our EigenTrajectory predictor can significantly improve both the prediction accuracy and reliability of existing trajectory forecasting models on public benchmarks, indicating that the proposed descriptor is suited to represent pedestrian behaviors. Code is publicly available at https://github.com/inhwanbae/EigenTrajectory .
</details>
<details>
<summary>摘要</summary>
Capturing high-dimensional social interactions and feasible futures is essential for predicting trajectories. To address this complex nature, several attempts have been devoted to reducing the dimensionality of the output variables via parametric curve fitting such as the B\'ezier curve and B-spline function. However, these functions, which originate in computer graphics fields, are not suitable to account for socially acceptable human dynamics. In this paper, we present EigenTrajectory ($\mathbb{ET}$), a trajectory prediction approach that uses a novel trajectory descriptor to form a compact space, known here as $\mathbb{ET}$ space, in place of Euclidean space, for representing pedestrian movements. We first reduce the complexity of the trajectory descriptor via a low-rank approximation. We transform the pedestrians' history paths into our $\mathbb{ET}$ space represented by spatio-temporal principle components, and feed them into off-the-shelf trajectory forecasting models. The inputs and outputs of the models as well as social interactions are all gathered and aggregated in the corresponding $\mathbb{ET}$ space. Lastly, we propose a trajectory anchor-based refinement method to cover all possible futures in the proposed $\mathbb{ET}$ space. Extensive experiments demonstrate that our EigenTrajectory predictor can significantly improve both the prediction accuracy and reliability of existing trajectory forecasting models on public benchmarks, indicating that the proposed descriptor is suited to represent pedestrian behaviors. Code is publicly available at https://github.com/inhwanbae/EigenTrajectory .
</details></li>
</ul>
<hr>
<h2 id="Conformal-prediction-under-ambiguous-ground-truth"><a href="#Conformal-prediction-under-ambiguous-ground-truth" class="headerlink" title="Conformal prediction under ambiguous ground truth"></a>Conformal prediction under ambiguous ground truth</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09302">http://arxiv.org/abs/2307.09302</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Stutz, Abhijit Guha Roy, Tatiana Matejovicova, Patricia Strachan, Ali Taylan Cemgil, Arnaud Doucet</li>
<li>for: 这个论文的目的是提出一种基于不确定标签的整形预测方法，以便在不具备准确标签的情况下进行不确定性评估。</li>
<li>methods: 该方法基于一种approximentation of the underlying posterior distribution of labels given inputs，以便处理不具备准确标签的情况。</li>
<li>results: 在synthetic和实际 dataset上，该方法可以准确地预测输入样本的标签，并且可以正确地评估输入样本的不确定性。在一个dermatology例子中，该方法可以成功地预测皮肤状况的标签。<details>
<summary>Abstract</summary>
In safety-critical classification tasks, conformal prediction allows to perform rigorous uncertainty quantification by providing confidence sets including the true class with a user-specified probability. This generally assumes the availability of a held-out calibration set with access to ground truth labels. Unfortunately, in many domains, such labels are difficult to obtain and usually approximated by aggregating expert opinions. In fact, this holds true for almost all datasets, including well-known ones such as CIFAR and ImageNet. Applying conformal prediction using such labels underestimates uncertainty. Indeed, when expert opinions are not resolvable, there is inherent ambiguity present in the labels. That is, we do not have ``crisp'', definitive ground truth labels and this uncertainty should be taken into account during calibration. In this paper, we develop a conformal prediction framework for such ambiguous ground truth settings which relies on an approximation of the underlying posterior distribution of labels given inputs. We demonstrate our methodology on synthetic and real datasets, including a case study of skin condition classification in dermatology.
</details>
<details>
<summary>摘要</summary>
在安全关键分类任务中，协形预测可以进行严格的不确定性评估，提供包含真实类别的信任集，用户可以指定概率。通常假设有一个保留 calibration set 可以获得真实标签。然而，在许多领域，这些标签很难获得，通常通过专家意见的汇总来估计。实际上，这是大多数数据集中的情况，包括常见的 CIFAR 和 ImageNet。在这些标签不确定的情况下，使用协形预测会下降 uncertainty。事实上，当专家意见不能分解时，存在 inherent  ambiguity 在标签中。即，我们没有 "卷积" 、definitive 的地面实标签，这种uncertainty 应该在 calibration 阶段被考虑。在这篇论文中，我们开发了一种协形预测框架，用于这种 ambiguous 地面实标签设置，基于输入的后期分布。我们在 sintetic 和实际数据集上进行了示例研究，包括皮肤状况分类在皮肤科学中。
</details></li>
</ul>
<hr>
<h2 id="FlexiAST-Flexibility-is-What-AST-Needs"><a href="#FlexiAST-Flexibility-is-What-AST-Needs" class="headerlink" title="FlexiAST: Flexibility is What AST Needs"></a>FlexiAST: Flexibility is What AST Needs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09286">http://arxiv.org/abs/2307.09286</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JiuFengSC/FlexiAST_INTERSPEECH23">https://github.com/JiuFengSC/FlexiAST_INTERSPEECH23</a></li>
<li>paper_authors: Jiu Feng, Mehmet Hamza Erol, Joon Son Chung, Arda Senocak</li>
<li>for: 提高Audio Spectrogram Transformer（AST）模型在不同补丁大小下的性能。</li>
<li>methods: 提出一种基于随机补丁大小选择和补丁重量resize的训练方法，使标准AST模型在推理阶段能够适应不同补丁大小。</li>
<li>results: 实验表明，FlexiAST模型在不同补丁大小下保持类似的性能，而无需进行architecture变更。<details>
<summary>Abstract</summary>
The objective of this work is to give patch-size flexibility to Audio Spectrogram Transformers (AST). Recent advancements in ASTs have shown superior performance in various audio-based tasks. However, the performance of standard ASTs degrades drastically when evaluated using different patch sizes from that used during training. As a result, AST models are typically re-trained to accommodate changes in patch sizes. To overcome this limitation, this paper proposes a training procedure to provide flexibility to standard AST models without architectural changes, allowing them to work with various patch sizes at the inference stage - FlexiAST. This proposed training approach simply utilizes random patch size selection and resizing of patch and positional embedding weights. Our experiments show that FlexiAST gives similar performance to standard AST models while maintaining its evaluation ability at various patch sizes on different datasets for audio classification tasks.
</details>
<details>
<summary>摘要</summary>
“这个工作的目标是给Audio Spectrogram Transformer（AST）提供材质大小灵活性。现代AST的进步已经在不同的音频任务中展示出色的表现。然而，标准AST的表现会在不同的材质大小下逐渐下降，导致AST模型需要重新训练来适应材质大小的变化。为了解决这个限制，这篇文章提出了一种不需要建构更改的训练方法，可以让标准AST模型在测试阶段运行各种材质大小。这个提案使用随机选择材质大小和重复材质大小的位置嵌入对应。我们的实验表明，FlexiAST可以与标准AST模型的表现相似，并且在不同的材质大小下保持评估能力。”Note that Simplified Chinese is a romanization of Chinese, and the actual Chinese text may be written differently.
</details></li>
</ul>
<hr>
<h2 id="End-to-End-Neural-Network-Training-for-Hyperbox-Based-Classification"><a href="#End-to-End-Neural-Network-Training-for-Hyperbox-Based-Classification" class="headerlink" title="End-to-End Neural Network Training for Hyperbox-Based Classification"></a>End-to-End Neural Network Training for Hyperbox-Based Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09269">http://arxiv.org/abs/2307.09269</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mlde-ms/hypernn">https://github.com/mlde-ms/hypernn</a></li>
<li>paper_authors: Denis Mayr Lima Martins, Christian Lülf, Fabian Gieseke</li>
<li>for: 这篇论文是为了提出一个新的、可微分的核心框架，以便实现高效地对大量数据进行分类。</li>
<li>methods: 这篇论文使用了神经网络，并将核心框架转换为可微分的形式，以便实现更高效的训练。</li>
<li>results: 这篇论文的结果显示，使用这个新的核心框架和训练方法可以获得更好的分类结果，并且训练时间更短。<details>
<summary>Abstract</summary>
Hyperbox-based classification has been seen as a promising technique in which decisions on the data are represented as a series of orthogonal, multidimensional boxes (i.e., hyperboxes) that are often interpretable and human-readable. However, existing methods are no longer capable of efficiently handling the increasing volume of data many application domains face nowadays. We address this gap by proposing a novel, fully differentiable framework for hyperbox-based classification via neural networks. In contrast to previous work, our hyperbox models can be efficiently trained in an end-to-end fashion, which leads to significantly reduced training times and superior classification results.
</details>
<details>
<summary>摘要</summary>
互 box 基本的分类技术已被视为一种有前途的技术，在这种技术中，数据的决策被表示为一系列的正交、多维度的盒子（即互 box），这些盒子通常是可读的和人类可读的。然而，现有的方法不再能够有效地处理现在许多应用领域面临的数据量的增加。我们解决这个问题 by proposing a novel, fully differentiable framework for hyperbox-based classification via neural networks. 在我们的方法中，互 box 模型可以在端到端的方式进行高效地训练，这导致了训练时间的减少和分类结果的提高。
</details></li>
</ul>
<hr>
<h2 id="Mobility-Aware-Joint-User-Scheduling-and-Resource-Allocation-for-Low-Latency-Federated-Learning"><a href="#Mobility-Aware-Joint-User-Scheduling-and-Resource-Allocation-for-Low-Latency-Federated-Learning" class="headerlink" title="Mobility-Aware Joint User Scheduling and Resource Allocation for Low Latency Federated Learning"></a>Mobility-Aware Joint User Scheduling and Resource Allocation for Low Latency Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09263">http://arxiv.org/abs/2307.09263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kecheng Fan, Wen Chen, Jun Li, Xiumei Deng, Xuefeng Han, Ming Ding</li>
<li>for: 这个论文的目的是提出一个实用的机器学习方法，以解决在联盟学习（Federated Learning，FL）中用户移动导致训练效能下降的问题。</li>
<li>methods: 这个论文使用了一个实际的用户移动模型，并提出了一个用户排程和资源分配方法，以减少训练延迟时间，并且考虑了对于用户移动的影响。</li>
<li>results:  simulations results show that the proposed algorithm achieves better performance than the state-of-the-art baselines, and a certain level of user mobility could improve training performance.<details>
<summary>Abstract</summary>
As an efficient distributed machine learning approach, Federated learning (FL) can obtain a shared model by iterative local model training at the user side and global model aggregating at the central server side, thereby protecting privacy of users. Mobile users in FL systems typically communicate with base stations (BSs) via wireless channels, where training performance could be degraded due to unreliable access caused by user mobility. However, existing work only investigates a static scenario or random initialization of user locations, which fail to capture mobility in real-world networks. To tackle this issue, we propose a practical model for user mobility in FL across multiple BSs, and develop a user scheduling and resource allocation method to minimize the training delay with constrained communication resources. Specifically, we first formulate an optimization problem with user mobility that jointly considers user selection, BS assignment to users, and bandwidth allocation to minimize the latency in each communication round. This optimization problem turned out to be NP-hard and we proposed a delay-aware greedy search algorithm (DAGSA) to solve it. Simulation results show that the proposed algorithm achieves better performance than the state-of-the-art baselines and a certain level of user mobility could improve training performance.
</details>
<details>
<summary>摘要</summary>
为了实现高效的分布式机器学习方法，联邦学习（FL）可以在用户端进行轮循式地本地模型训练和中央服务器端进行全球模型汇总，以保护用户隐私。在FL系统中，移动用户通常通过无线通信通道与基站（BS）进行交互，但是训练性能可能受到用户移动导致的不可预测访问所增加的干扰。现有研究仅考虑静止场景或随机初始化用户位置，未能捕捉实际网络中的移动。为解决这个问题，我们提出了实际的用户移动模型在FL中，并开发了一种用户调度和资源分配方法，以最小化通信资源的培训延迟。具体来说，我们首先将用户移动引入到联邦学习中的优化问题中，并jointly考虑用户选择、用户分配到BS以及带宽分配，以最小化每次通信圈中的延迟。这个优化问题被证明是NP困难的，我们提出了延迟意识搜索算法（DAGSA）解决它。实验结果表明，我们的算法在比较状态前的基elines上表现得更好，并且一定程度的用户移动可以提高训练性能。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Topological-Feature-via-Persistent-Homology-Filtration-Learning-for-Point-Clouds"><a href="#Adaptive-Topological-Feature-via-Persistent-Homology-Filtration-Learning-for-Point-Clouds" class="headerlink" title="Adaptive Topological Feature via Persistent Homology: Filtration Learning for Point Clouds"></a>Adaptive Topological Feature via Persistent Homology: Filtration Learning for Point Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09259">http://arxiv.org/abs/2307.09259</a></li>
<li>repo_url: None</li>
<li>paper_authors: Naoki Nishikawa, Yuichi Ike, Kenji Yamanishi</li>
<li>for: 提高机器学习点云处理精度，应用于形态识别和材料科学等领域。</li>
<li>methods: 使用神经网络学习自适应滤波，以保证 persistent homology 的同质性。</li>
<li>results: 在多个分类任务中表现出色，证明了我们的框架的可行性。Here’s the full text in Simplified Chinese:</li>
<li>for: 本研究旨在提高机器学习点云处理精度，应用于形态识别和材料科学等领域。</li>
<li>methods: 我们提出了一种基于神经网络的自适应滤波方法，以保证 persistent homology 的同质性。</li>
<li>results: 我们在多个分类任务中进行了实验，结果表明我们的框架在这些任务中表现出色，证明了我们的方法的可行性。<details>
<summary>Abstract</summary>
Machine learning for point clouds has been attracting much attention, with many applications in various fields, such as shape recognition and material science. To enhance the accuracy of such machine learning methods, it is known to be effective to incorporate global topological features, which are typically extracted by persistent homology. In the calculation of persistent homology for a point cloud, we need to choose a filtration for the point clouds, an increasing sequence of spaces. Because the performance of machine learning methods combined with persistent homology is highly affected by the choice of a filtration, we need to tune it depending on data and tasks. In this paper, we propose a framework that learns a filtration adaptively with the use of neural networks. In order to make the resulting persistent homology isometry-invariant, we develop a neural network architecture with such invariance. Additionally, we theoretically show a finite-dimensional approximation result that justifies our architecture. Experimental results demonstrated the efficacy of our framework in several classification tasks.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose a framework that learns a filtration adaptively using neural networks. To ensure the resulting persistent homology isometry-invariant, we develop a neural network architecture with such invariance. Additionally, we provide a finite-dimensional approximation result that justifies our architecture. Experimental results show the effectiveness of our framework in several classification tasks.Here's the translation in Simplified Chinese:机器学习 для点云已经吸引了很多关注，并在各种领域上有广泛的应用，如形状识别和材料科学。为了提高机器学习方法的准确性，通常需要包含全局拓扑特征，通常通过不变 homology 来提取。在计算不变 homology 中，我们需要选择一个筛选器，是一个增长序列的空间。由于选择筛选器的性能会高度影响机器学习方法的性能，因此需要根据数据和任务进行调整。在这篇论文中，我们提出了一种框架，通过使用神经网络来自适应地学习筛选器。为确保结果的不变 homology 尺度 invariants，我们开发了一种具有不变性的神经网络架构。此外，我们也提供了一个数学上的有限维approximation 结果，证明了我们的架构的正确性。实验结果表明，我们的框架在多个分类任务中表现出色。
</details></li>
</ul>
<hr>
<h2 id="PAC-Neural-Prediction-Set-Learning-to-Quantify-the-Uncertainty-of-Generative-Language-Models"><a href="#PAC-Neural-Prediction-Set-Learning-to-Quantify-the-Uncertainty-of-Generative-Language-Models" class="headerlink" title="PAC Neural Prediction Set Learning to Quantify the Uncertainty of Generative Language Models"></a>PAC Neural Prediction Set Learning to Quantify the Uncertainty of Generative Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09254">http://arxiv.org/abs/2307.09254</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sangdon Park, Taesoo Kim</li>
<li>for: 提高模型的可靠性和信任worthiness</li>
<li>methods: 使用神经网络 parameterized prediction set models，实现更精准的uncertainty quantification，并且满足 probably approximately correct (PAC) 保证</li>
<li>results: 在四种语言数据集和六种模型上，比基准方法提高quantified uncertainty的精度$63%$的平均值<details>
<summary>Abstract</summary>
Uncertainty learning and quantification of models are crucial tasks to enhance the trustworthiness of the models. Importantly, the recent surge of generative language models (GLMs) emphasizes the need for reliable uncertainty quantification due to the concerns on generating hallucinated facts. In this paper, we propose to learn neural prediction set models that comes with the probably approximately correct (PAC) guarantee for quantifying the uncertainty of GLMs. Unlike existing prediction set models, which are parameterized by a scalar value, we propose to parameterize prediction sets via neural networks, which achieves more precise uncertainty quantification but still satisfies the PAC guarantee. We demonstrate the efficacy of our method on four types of language datasets and six types of models by showing that our method improves the quantified uncertainty by $63\%$ on average, compared to a standard baseline method.
</details>
<details>
<summary>摘要</summary>
<<SYS>>对不确定性学习和模型评估是重要任务，以提高模型的可靠性。尤其是最近几年的生成语言模型（GLMs），它们的不确定性评估问题得到了更多的关注，因为它们可能会生成假信息。在这篇论文中，我们提议使用神经网络来学习预测集模型，这些模型具有可靠的不确定性评估保证（PAC），并且可以更 precisely 评估 GLMs 的不确定性。 unlike 现有的预测集模型，我们的模型参数化使用神经网络，这使得我们可以更好地评估 GLMs 的不确定性。我们在四种语言 dataset 和六种模型上进行了实验，并证明我们的方法可以提高评估不确定性的精度，相比标准基准方法，提高了63%的平均值。Note: " Probably approximately correct" (PAC) is a theoretical guarantee that the output of a machine learning model is likely to be close to the true output, with a certain level of confidence. In this context, the PAC guarantee is used to ensure that the uncertainty estimates produced by the model are reliable and accurate.
</details></li>
</ul>
<hr>
<h2 id="UniTabE-Pretraining-a-Unified-Tabular-Encoder-for-Heterogeneous-Tabular-Data"><a href="#UniTabE-Pretraining-a-Unified-Tabular-Encoder-for-Heterogeneous-Tabular-Data" class="headerlink" title="UniTabE: Pretraining a Unified Tabular Encoder for Heterogeneous Tabular Data"></a>UniTabE: Pretraining a Unified Tabular Encoder for Heterogeneous Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09249">http://arxiv.org/abs/2307.09249</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yazheng Yang, Yuqi Wang, Guang Liu, Ledell Wu, Qi Liu</li>
<li>for: 本研究旨在推广自然语言处理（NLP）中的预训练方法，应用于表格数据，以提高表格数据分析的Semantic Representation。</li>
<li>methods: 本研究使用了UniTabE方法，该方法基于表格元素模块（TabUnit）和Transformer编码器，可以适应不同的表格结构。此外，模型还支持预训练和Finetuning，通过自由形式的提示。</li>
<li>results: 实验结果显示，UniTabE方法在多个benchmark dataset上表现出色，超过了多个基eline模型。这说明UniTabE方法可以有效地提高表格数据的Semantic Representation，为表格数据分析带来 significiant progress.<details>
<summary>Abstract</summary>
Recent advancements in Natural Language Processing (NLP) have witnessed the groundbreaking impact of pretrained models, yielding impressive outcomes across various tasks. This study seeks to extend the power of pretraining methodologies to tabular data, a domain traditionally overlooked, yet inherently challenging due to the plethora of table schemas intrinsic to different tasks. The primary research questions underpinning this work revolve around the adaptation to heterogeneous table structures, the establishment of a universal pretraining protocol for tabular data, the generalizability and transferability of learned knowledge across tasks, the adaptation to diverse downstream applications, and the incorporation of incremental columns over time. In response to these challenges, we introduce UniTabE, a pioneering method designed to process tables in a uniform manner, devoid of constraints imposed by specific table structures. UniTabE's core concept relies on representing each basic table element with a module, termed TabUnit. This is subsequently followed by a Transformer encoder to refine the representation. Moreover, our model is designed to facilitate pretraining and finetuning through the utilization of free-form prompts. In order to implement the pretraining phase, we curated an expansive tabular dataset comprising approximately 13 billion samples, meticulously gathered from the Kaggle platform. Rigorous experimental testing and analyses were performed under a myriad of scenarios to validate the effectiveness of our methodology. The experimental results demonstrate UniTabE's superior performance against several baseline models across a multitude of benchmark datasets. This, therefore, underscores UniTabE's potential to significantly enhance the semantic representation of tabular data, thereby marking a significant stride in the field of tabular data analysis.
</details>
<details>
<summary>摘要</summary>
近年的自然语言处理（NLP）技术发展，启示出革命性的影响，在多种任务上取得了卓越的成绩。这项研究旨在扩展预训练方法的应用范围，推广到表格数据领域，这是传统上受过忽略的领域，但具有各种表格结构的强大挑战。本研究的主要问题包括适应不同表格结构、建立通用预训练协议、学习知识的通用性和跨任务传递性、适应多种下游应用、以及逐渐增加的列的支持。为解决这些挑战，我们提出了UniTabE方法，用于统一处理表格数据，不受特定表格结构的限制。UniTabE的核心思想是将每个基本表格元素表示为Module，称为TabUnit，然后使用Transformer编码器进行细化表示。此外，我们的模型设计能够方便预训练和finetuning，通过使用自由形式的提示。为进行预训练阶段，我们精心收集了约130亿个样本的大量表格数据，从Kaggle平台上精心收集。通过多种情况下的严格实验和分析，我们证明UniTabE方法在多个benchmark数据集上的表现优于多个基eline模型。这一结果 therefore表明UniTabE具有提高表格数据semantic表示的潜在能力，从而为表格数据分析带来重要的进步。
</details></li>
</ul>
<hr>
<h2 id="Application-of-BERT-in-Wind-Power-Forecasting-Teletraan’s-Solution-in-Baidu-KDD-Cup-2022"><a href="#Application-of-BERT-in-Wind-Power-Forecasting-Teletraan’s-Solution-in-Baidu-KDD-Cup-2022" class="headerlink" title="Application of BERT in Wind Power Forecasting-Teletraan’s Solution in Baidu KDD Cup 2022"></a>Application of BERT in Wind Power Forecasting-Teletraan’s Solution in Baidu KDD Cup 2022</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09248">http://arxiv.org/abs/2307.09248</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/longxingtan/kdd2022-baidu">https://github.com/longxingtan/kdd2022-baidu</a></li>
<li>paper_authors: Longxing Tan, Hongying Yue</li>
<li>for: 预测风力电力系统的可靠性和可持续发展</li>
<li>methods: 使用BERT模型和日均异常值补做来预测风力电力系统的输出</li>
<li>results: 在Baidu KDD Cup 2022中获得第三名，代表着模型的可靠性和精度Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written for the purpose of improving the reliability and sustainability of wind power systems by using a BERT model and daily fluctuation post-processing to make accurate predictions.</li>
<li>methods: The paper uses the BERT model, which is a type of deep learning model that has shown great success in natural language processing tasks, to predict the output of wind power systems. Additionally, the authors add daily fluctuation to the predicted results through post-processing to make the predictions more accurate and in line with daily periodicity.</li>
<li>results: The authors achieved third place out of 2490 teams in the Baidu KDD Cup 2022, which demonstrates the effectiveness and accuracy of their proposed method.<details>
<summary>Abstract</summary>
Nowadays, wind energy has drawn increasing attention as its important role in carbon neutrality and sustainable development. When wind power is integrated into the power grid, precise forecasting is necessary for the sustainability and security of the system. However, the unpredictable nature and long sequence prediction make it especially challenging. In this technical report, we introduce the BERT model applied for Baidu KDD Cup 2022, and the daily fluctuation is added by post-processing to make the predicted results in line with daily periodicity. Our solution achieves 3rd place of 2490 teams. The code is released athttps://github.com/LongxingTan/KDD2022-Baidu
</details>
<details>
<summary>摘要</summary>
现在，风能资源已经吸引了越来越多的关注，因为它在碳中和可持续发展中扮演着重要的角色。当风力发电机与电力网络集成时，准确预测成为了系统可持续性和安全性的重要因素。然而，风力预测具有不可预测性和长时间序列预测的特点，使得预测变得特别困难。在这份技术报告中，我们介绍了BERT模型在Baidu KDD杯2022中的应用，并通过后处理来添加日律性，使预测结果与日律性保持一致。我们的解决方案在2490个团队中获得第三名，代码在https://github.com/LongxingTan/KDD2022-Baidu上发布。
</details></li>
</ul>
<hr>
<h2 id="Towards-Sustainable-Deep-Learning-for-Multi-Label-Classification-on-NILM"><a href="#Towards-Sustainable-Deep-Learning-for-Multi-Label-Classification-on-NILM" class="headerlink" title="Towards Sustainable Deep Learning for Multi-Label Classification on NILM"></a>Towards Sustainable Deep Learning for Multi-Label Classification on NILM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09244">http://arxiv.org/abs/2307.09244</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anže Pirnat, Blaž Bertalanič, Gregor Cerar, Mihael Mohorčič, Carolina Fortuna</li>
<li>For: The paper is written for the purpose of improving the computation and energy efficiency of deep learning (DL) models for non-intrusive load monitoring (NILM) classification.* Methods: The paper proposes a novel DL model for enhanced multi-label classification of NILM, which is designed to reduce computational and energy demands during training and operation.* Results: The proposed model achieves on average approximately 8 percentage points in performance improvement compared to the state-of-the-art, while reducing the carbon footprint by more than 23%.Here’s the Chinese translation of the three key information points:* For: 本文是为了提高深度学习（DL）模型的计算和能源效率，用于非侵入式电力监测（NILM）分类。* Methods: 本文提出了一种新的DL模型，用于改进NILM多标签分类，以降低训练和运行中的计算和能源需求。* Results: 提议的模型与状态艺术比较，在REFIT和UK-DALE数据集上测试时， average提高了约8%的性能，同时减少了碳脚印的23%以上。<details>
<summary>Abstract</summary>
Non-intrusive load monitoring (NILM) is the process of obtaining appliance-level data from a single metering point, measuring total electricity consumption of a household or a business. Appliance-level data can be directly used for demand response applications and energy management systems as well as for awareness raising and motivation for improvements in energy efficiency and reduction in the carbon footprint. Recently, classical machine learning and deep learning (DL) techniques became very popular and proved as highly effective for NILM classification, but with the growing complexity these methods are faced with significant computational and energy demands during both their training and operation. In this paper, we introduce a novel DL model aimed at enhanced multi-label classification of NILM with improved computation and energy efficiency. We also propose a testing methodology for comparison of different models using data synthesized from the measurement datasets so as to better represent real-world scenarios. Compared to the state-of-the-art, the proposed model has its carbon footprint reduced by more than 23% while providing on average approximately 8 percentage points in performance improvement when testing on data derived from REFIT and UK-DALE datasets.
</details>
<details>
<summary>摘要</summary>
非侵入式电力监测（NILM）是指从单个计量点获取家用电器或商业用电器的具体数据，计算总电力消耗量。家用电器或商业用电器的具体数据可以直接用于需求应答应用和能源管理系统，以及提高能源效率和减少碳足迹。在最近的几年中，传统的机器学习和深度学习（DL）技术在NILM类型分类中变得非常流行，但是随着模型的复杂度的增加，它们面临着显著的计算和能源投入问题。在本文中，我们介绍了一种新的深度学习模型，旨在提高多标签分类的NILM性能。我们还提出了一种测试方法ологи，用于对不同模型进行比较，以更好地 simulate real-world scenarios。相比之前的状态艺术，我们的提案模型可以减少碳脚印的23%以上，并在REFIT和UK-DALE数据集上测试平均提高8个百分点的性能。
</details></li>
</ul>
<hr>
<h2 id="Fusing-Hand-and-Body-Skeletons-for-Human-Action-Recognition-in-Assembly"><a href="#Fusing-Hand-and-Body-Skeletons-for-Human-Action-Recognition-in-Assembly" class="headerlink" title="Fusing Hand and Body Skeletons for Human Action Recognition in Assembly"></a>Fusing Hand and Body Skeletons for Human Action Recognition in Assembly</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09238">http://arxiv.org/abs/2307.09238</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dustin Aganian, Mona Köhler, Benedict Stephan, Markus Eisenbach, Horst-Michael Gross</li>
<li>for: 这篇论文主要是为了提高人机合作的效果，使用机器人在制造过程中协助人类完成Assembly任务。</li>
<li>methods: 该方法使用较为简单的人体骨架，并与高级别的手套骨架结合，使用CNN和转换器来提高人体动作识别率。</li>
<li>results: 该方法在Assembly场景中的人体动作识别率得到了提高，可以帮助机器人更好地协助人类完成Assembly任务。<details>
<summary>Abstract</summary>
As collaborative robots (cobots) continue to gain popularity in industrial manufacturing, effective human-robot collaboration becomes crucial. Cobots should be able to recognize human actions to assist with assembly tasks and act autonomously. To achieve this, skeleton-based approaches are often used due to their ability to generalize across various people and environments. Although body skeleton approaches are widely used for action recognition, they may not be accurate enough for assembly actions where the worker's fingers and hands play a significant role. To address this limitation, we propose a method in which less detailed body skeletons are combined with highly detailed hand skeletons. We investigate CNNs and transformers, the latter of which are particularly adept at extracting and combining important information from both skeleton types using attention. This paper demonstrates the effectiveness of our proposed approach in enhancing action recognition in assembly scenarios.
</details>
<details>
<summary>摘要</summary>
随着协同机器人（COBOT）在工业生产领域的普及，人机合作的效果变得越来越重要。COBOT应该能够认识人类动作，协助Assembly任务，并且可以自动行动。为达到这个目标，skeleton-based方法经常用于人机合作，因为它们可以在不同的人和环境中广泛普适。虽然body skeleton方法广泛用于动作识别，但它们可能无法准确地识别Assembly动作，这是因为工人的手指和手臂在这些动作中扮演着重要的角色。为解决这个限制，我们提议一种方法，即将较为简单的body skeleton与高级细节的手skeleton结合在一起。我们 investigate CNNs和transformers，后者尤其适合从skeleton类型中提取和组合重要信息，使用注意力。本文证明我们提议的方法可以在Assembly场景中提高动作识别的效果。
</details></li>
</ul>
<hr>
<h2 id="Detecting-Throat-Cancer-from-Speech-Signals-Using-Machine-Learning-A-Reproducible-Literature-Review"><a href="#Detecting-Throat-Cancer-from-Speech-Signals-Using-Machine-Learning-A-Reproducible-Literature-Review" class="headerlink" title="Detecting Throat Cancer from Speech Signals Using Machine Learning: A Reproducible Literature Review"></a>Detecting Throat Cancer from Speech Signals Using Machine Learning: A Reproducible Literature Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09230">http://arxiv.org/abs/2307.09230</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mary Paterson, James Moor, Luisa Cutillo</li>
<li>for: 这个研究是对现有文献中的嗓腔癌检测使用机器学习和人工智能的论文进行探讨。</li>
<li>methods: 这些论文使用的方法包括神经网络，并且大多数使用神经网络进行实现。Audio中的多种特征被提取，mel-frequency cepstral coefficients最常用。</li>
<li>results: 我们使用转移学习在多类问题上进行分类，并实现了53.54%的涂抹率，83.14%的敏感率和64.00%的特征率。我们的分类器与同一个数据集上的结果相似。<details>
<summary>Abstract</summary>
In this work we perform a scoping review of the current literature on the detection of throat cancer from speech recordings using machine learning and artificial intelligence. We find 22 papers within this area and discuss their methods and results. We split these papers into two groups - nine performing binary classification, and 13 performing multi-class classification. The papers present a range of methods with neural networks being most commonly implemented. Many features are also extracted from the audio before classification, with the most common bring mel-frequency cepstral coefficients. None of the papers found in this search have associated code repositories and as such are not reproducible. Therefore, we create a publicly available code repository of our own classifiers. We use transfer learning on a multi-class problem, classifying three pathologies and healthy controls. Using this technique we achieve an unweighted average recall of 53.54%, sensitivity of 83.14%, and specificity of 64.00%. We compare our classifiers with the results obtained on the same dataset and find similar results.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们进行了评估当前文献中用于喉部癌诊断从语音记录中使用机器学习和人工智能的研究。我们找到了22篇相关文献，并讨论了它们的方法和结果。我们将这些文献分为了两组：9篇为二分类，13篇为多类分类。文献中最常用的方法是神经网络。大多数文献在语音分类之前将各种特征提取出来，最常用的是MEL-frequency cepstral coefficients。我们发现所有搜索到的文献都没有关联代码库，因此无法重现。因此，我们创建了一个公共可用的代码库。我们使用传输学习解决多类问题，分类三种疾病和健康控制。使用这种技术，我们获得了无权重平均回归率53.54%，感知率83.14%和特征率64.00%。我们比较了我们的分类器与同一个数据集上的结果，发现结果类似。
</details></li>
</ul>
<hr>
<h2 id="How-Many-Neurons-Does-it-Take-to-Approximate-the-Maximum"><a href="#How-Many-Neurons-Does-it-Take-to-Approximate-the-Maximum" class="headerlink" title="How Many Neurons Does it Take to Approximate the Maximum?"></a>How Many Neurons Does it Take to Approximate the Maximum?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09212">http://arxiv.org/abs/2307.09212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Itay Safran, Daniel Reichman, Paul Valiant</li>
<li>for: 本研究旨在探讨一个神经网络可以近似最大函数的大小，在最基本的近似情况下，即使用$L_2$范数，对于连续分布，并使用ReLU激活函数。</li>
<li>methods: 我们提供了新的下界和上界，以评估不同深度的神经网络所需的宽度，以及一个depth $\mathcal{O}(\log(\log(d)))$和宽度 $\mathcal{O}(d)$的建构，可以高效地近似最大函数。</li>
<li>results: 我们的结果显示，在depth 2和depth 3网络之间存在新的深度分离，以及depth 3和depth 5网络之间的深度分离。此外，我们还提供了一个depth $\mathcal{O}(\log(\log(d)))$和宽度 $\mathcal{O}(d)$的建构，可以高效地近似最大函数，远远超过了最好已知的深度 bound。<details>
<summary>Abstract</summary>
We study the size of a neural network needed to approximate the maximum function over $d$ inputs, in the most basic setting of approximating with respect to the $L_2$ norm, for continuous distributions, for a network that uses ReLU activations. We provide new lower and upper bounds on the width required for approximation across various depths. Our results establish new depth separations between depth 2 and 3, and depth 3 and 5 networks, as well as providing a depth $\mathcal{O}(\log(\log(d)))$ and width $\mathcal{O}(d)$ construction which approximates the maximum function, significantly improving upon the depth requirements of the best previously known bounds for networks with linearly-bounded width. Our depth separation results are facilitated by a new lower bound for depth 2 networks approximating the maximum function over the uniform distribution, assuming an exponential upper bound on the size of the weights. Furthermore, we are able to use this depth 2 lower bound to provide tight bounds on the number of neurons needed to approximate the maximum by a depth 3 network. Our lower bounds are of potentially broad interest as they apply to the widely studied and used \emph{max} function, in contrast to many previous results that base their bounds on specially constructed or pathological functions and distributions.
</details>
<details>
<summary>摘要</summary>
我们研究了一个神经网络需要来近似最大函数的大小，在最基本的设定下，即使用$L_2$ нор的近似，并且考虑到连续分布下的情况。我们提供了新的下界和上界，以及对不同深度的数据分布。我们的结果建立了新的深度分隔，包括深度2和3之间的分隔，以及深度3和5之间的分隔。此外，我们还提供了一个depth $\mathcal{O}(\log(\log(d)))$和宽度 $\mathcal{O}(d)$ 的建构，可以将最大函数近似，与之前最好的下界对于线性宽度的网络具有重要的改进。我们的深度分隔结果受到一个新的深度2网络近似最大函数的下界，假设Weight的大小是线性的。此外，我们还可以使用这个深度2下界，提供了对深度3网络近似最大函数的精确的 neuron 数量。我们的下界具有广泛的应用可能性，因为它们应用到了广泛研究和使用的\emph{max}函数，不同于许多先前的结果，它们基于特殊 constructed 或是 Pathological 函数和分布。
</details></li>
</ul>
<hr>
<h2 id="Automated-Ableism-An-Exploration-of-Explicit-Disability-Biases-in-Sentiment-and-Toxicity-Analysis-Models"><a href="#Automated-Ableism-An-Exploration-of-Explicit-Disability-Biases-in-Sentiment-and-Toxicity-Analysis-Models" class="headerlink" title="Automated Ableism: An Exploration of Explicit Disability Biases in Sentiment and Toxicity Analysis Models"></a>Automated Ableism: An Exploration of Explicit Disability Biases in Sentiment and Toxicity Analysis Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09209">http://arxiv.org/abs/2307.09209</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranav Narayanan Venkit, Mukund Srinath, Shomir Wilson</li>
<li>for: 本研究旨在探讨 sentiment analysis 和攻击干预模型在探测人际障碍 (PWD) 的表现时是否存在显著的偏见。</li>
<li>methods: 我们使用了 Perturbation Sensitivity Analysis 检测探测 Twitter 和 Reddit 社交媒体平台上关于 PWD 的对话，以获得实际社交场景中如何传播残障偏见的信息。然后，我们创建了 \textit{Bias Identification Test in Sentiment} (BITS)  корпуス，以量化任何 sentiment analysis 和攻击干预模型中的直接残障偏见。</li>
<li>results: 我们的研究发现，这些open AIaaS sentiment analysis 工具（包括 TextBlob、VADER、Google Cloud Natural Language API 和 DistilBERT）以及两个攻击干预模型（包括 two versions of Toxic-BERT）都存在显著的直接残障偏见。<details>
<summary>Abstract</summary>
We analyze sentiment analysis and toxicity detection models to detect the presence of explicit bias against people with disability (PWD). We employ the bias identification framework of Perturbation Sensitivity Analysis to examine conversations related to PWD on social media platforms, specifically Twitter and Reddit, in order to gain insight into how disability bias is disseminated in real-world social settings. We then create the \textit{Bias Identification Test in Sentiment} (BITS) corpus to quantify explicit disability bias in any sentiment analysis and toxicity detection models. Our study utilizes BITS to uncover significant biases in four open AIaaS (AI as a Service) sentiment analysis tools, namely TextBlob, VADER, Google Cloud Natural Language API, DistilBERT and two toxicity detection models, namely two versions of Toxic-BERT. Our findings indicate that all of these models exhibit statistically significant explicit bias against PWD.
</details>
<details>
<summary>摘要</summary>
我们对偏见检测和负面情绪检测模型进行分析，以检测对人障（PWD）的直接偏见。我们使用扰动敏感性分析框架来分析社交媒体平台上关于PWD的对话，以获得实际社会中对障碍偏见的准确情况。然后，我们创建了《偏见标准测试集》（BITS）来衡量任何情感分析和负面情绪检测模型中的直接障碍偏见。我们的研究使用BITS来揭示四个开放的 AIaaS（人工智能 как服务）情感分析工具——TextBlob、VADER、Google Cloud Natural Language API和DistilBERT——以及两个负面情绪检测模型——两个版本的 Toxic-BERT——中的明显偏见。我们的发现表明，这些模型都存在 statistically significant的直接障碍偏见。
</details></li>
</ul>
<hr>
<h2 id="Context-Conditional-Navigation-with-a-Learning-Based-Terrain-and-Robot-Aware-Dynamics-Model"><a href="#Context-Conditional-Navigation-with-a-Learning-Based-Terrain-and-Robot-Aware-Dynamics-Model" class="headerlink" title="Context-Conditional Navigation with a Learning-Based Terrain- and Robot-Aware Dynamics Model"></a>Context-Conditional Navigation with a Learning-Based Terrain- and Robot-Aware Dynamics Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09206">http://arxiv.org/abs/2307.09206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suresh Guttikonda, Jan Achterhold, Haolong Li, Joschka Boedecker, Joerg Stueckler</li>
<li>For: 本研究目的是开发一种能够适应不同环境和机器人属性变化的自主导航方法。* Methods: 本研究使用了基于神经过程的meta-学前向动力学模型，以适应不同地形和机器人动力学变化。* Results: 实验表明，提出的模型在长期轨迹预测任务中的预测误差较低，而且在自主规划控制任务中也能够更好地规划控制路径。<details>
<summary>Abstract</summary>
In autonomous navigation settings, several quantities can be subject to variations. Terrain properties such as friction coefficients may vary over time depending on the location of the robot. Also, the dynamics of the robot may change due to, e.g., different payloads, changing the system's mass, or wear and tear, changing actuator gains or joint friction. An autonomous agent should thus be able to adapt to such variations. In this paper, we develop a novel probabilistic, terrain- and robot-aware forward dynamics model, termed TRADYN, which is able to adapt to the above-mentioned variations. It builds on recent advances in meta-learning forward dynamics models based on Neural Processes. We evaluate our method in a simulated 2D navigation setting with a unicycle-like robot and different terrain layouts with spatially varying friction coefficients. In our experiments, the proposed model exhibits lower prediction error for the task of long-horizon trajectory prediction, compared to non-adaptive ablation models. We also evaluate our model on the downstream task of navigation planning, which demonstrates improved performance in planning control-efficient paths by taking robot and terrain properties into account.
</details>
<details>
<summary>摘要</summary>
在自主导航设置下，许多量值可能会受到变化。地形特性，如摩擦系数，随时间的变化可能会影响机器人的位置。此外，机器人的动力学也可能会发生变化，例如不同的负荷、系统质量的变化、 actuator  gain 或 JOINT 摩擦的变化。因此，一个自主智能体应该能够适应这些变化。在这篇论文中，我们开发了一种新的probabilistic，地形和机器人意识的前瞻动力学模型，称为 TRADYN，它能够适应以上所 mention 的变化。它基于最近的前瞻动力学模型基于神经过程的meta-学进步。我们在一个模拟的2D导航设置中使用了一种unicycle-like 机器人和不同的地形布局，并对其进行了评估。在我们的实验中，提议的模型在长期轨迹预测任务中表现出较低的预测错误，相比非适应模型。此外，我们还评估了我们的模型在导航规划任务中的表现，其表现出了改进的控制效率的规划路径。
</details></li>
</ul>
<hr>
<h2 id="Learning-Dynamic-Attribute-factored-World-Models-for-Efficient-Multi-object-Reinforcement-Learning"><a href="#Learning-Dynamic-Attribute-factored-World-Models-for-Efficient-Multi-object-Reinforcement-Learning" class="headerlink" title="Learning Dynamic Attribute-factored World Models for Efficient Multi-object Reinforcement Learning"></a>Learning Dynamic Attribute-factored World Models for Efficient Multi-object Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09205">http://arxiv.org/abs/2307.09205</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fan Feng, Sara Magliacane</li>
<li>for: 这个论文的目的是提高强化学习任务中agent的扩展性和可重复性，使其能够在不同的物体和属性下进行学习和执行任务。</li>
<li>methods: 这个论文使用了对象中心表示学习来提取视觉输入中的物体，并将其分类为不同的类别。然后，对每个类别的物体，学习一个类模板图，描述了这种物体的动力和奖励如何因属性分解。还学习了对象之间的互动模式图，描述了不同类别的物体之间的互动。通过这些图和动态互动图，学习出一个策略，可以在新环境中直接应用。</li>
<li>results: 在三个标准 dataset上测试了这个框架，并证明了它在未seen的物体、属性和潜在参数下进行扩展和可重复性的任务时表现出色，以及在组合已知任务时的表现也是比较好的。<details>
<summary>Abstract</summary>
In many reinforcement learning tasks, the agent has to learn to interact with many objects of different types and generalize to unseen combinations and numbers of objects. Often a task is a composition of previously learned tasks (e.g. block stacking). These are examples of compositional generalization, in which we compose object-centric representations to solve complex tasks. Recent works have shown the benefits of object-factored representations and hierarchical abstractions for improving sample efficiency in these settings. On the other hand, these methods do not fully exploit the benefits of factorization in terms of object attributes. In this paper, we address this opportunity and introduce the Dynamic Attribute FacTored RL (DAFT-RL) framework. In DAFT-RL, we leverage object-centric representation learning to extract objects from visual inputs. We learn to classify them in classes and infer their latent parameters. For each class of object, we learn a class template graph that describes how the dynamics and reward of an object of this class factorize according to its attributes. We also learn an interaction pattern graph that describes how objects of different classes interact with each other at the attribute level. Through these graphs and a dynamic interaction graph that models the interactions between objects, we can learn a policy that can then be directly applied in a new environment by just estimating the interactions and latent parameters. We evaluate DAFT-RL in three benchmark datasets and show our framework outperforms the state-of-the-art in generalizing across unseen objects with varying attributes and latent parameters, as well as in the composition of previously learned tasks.
</details>
<details>
<summary>摘要</summary>
在许多强化学习任务中，机器人需要学习与多种不同类型的物体交互，并泛化到未经见过的组合和数量。经常情况下，任务是一个各种已经学习过的任务的组合（例如堆叠块）。这些任务是物体中心的泛化，在这些情况下，我们可以通过物体中心的表示学习来解决复杂任务。在这篇论文中，我们提出了动态特征划分RL（DAFT-RL）框架。在DAFT-RL中，我们利用物体中心的表示学习来提取视觉输入中的物体。我们可以将它们分类为类别，并且从属特征参数的推断。对于每个类型的物体，我们学习一个类型图，该图描述了物体的动力学和奖励因为其特征的分解。我们还学习了不同类型物体之间的交互图，该图描述了物体之间的特征级别交互。通过这些图和动态交互图，我们可以学习一个策略，该策略可以在新环境中直接应用，只需要估计交互和隐藏参数。我们在三个标准数据集上进行了评估，并证明了我们的框架在未经见过的物体特征和隐藏参数的泛化，以及在组合已经学习过的任务中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-for-Computationally-Constrained-Heterogeneous-Devices-A-Survey"><a href="#Federated-Learning-for-Computationally-Constrained-Heterogeneous-Devices-A-Survey" class="headerlink" title="Federated Learning for Computationally-Constrained Heterogeneous Devices: A Survey"></a>Federated Learning for Computationally-Constrained Heterogeneous Devices: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09182">http://arxiv.org/abs/2307.09182</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kilian Pfeiffer, Martin Rapp, Ramin Khalili, Jörg Henkel</li>
<li>for: 提高用户隐私和减少中心服务器的负担，实现在设备上进行神经网络训练。</li>
<li>methods: 联邦学习（Federated Learning）技术，通过共享设备之间的知识，保持用户隐私，同时提高模型精度。</li>
<li>results: 在具有多种设备的不同硬件和软件环境中，联邦学习技术面临着多种差异和挑战，需要采取多种策略来减少这些差异，以提高模型精度和可靠性。<details>
<summary>Abstract</summary>
With an increasing number of smart devices like internet of things (IoT) devices deployed in the field, offloadingtraining of neural networks (NNs) to a central server becomes more and more infeasible. Recent efforts toimprove users' privacy have led to on-device learning emerging as an alternative. However, a model trainedonly on a single device, using only local data, is unlikely to reach a high accuracy. Federated learning (FL)has been introduced as a solution, offering a privacy-preserving trade-off between communication overheadand model accuracy by sharing knowledge between devices but disclosing the devices' private data. Theapplicability and the benefit of applying baseline FL are, however, limited in many relevant use cases dueto the heterogeneity present in such environments. In this survey, we outline the heterogeneity challengesFL has to overcome to be widely applicable in real-world applications. We especially focus on the aspect ofcomputation heterogeneity among the participating devices and provide a comprehensive overview of recentworks on heterogeneity-aware FL. We discuss two groups: works that adapt the NN architecture and worksthat approach heterogeneity on a system level, covering Federated Averaging (FedAvg), distillation, and splitlearning-based approaches, as well as synchronous and asynchronous aggregation schemes.
</details>
<details>
<summary>摘要</summary>
随着智能设备的数量不断增加，如物联网（IoT）设备，将神经网络（NN）的训练卷积到中央服务器上成为了不可能的。随后，为了保护用户隐私，在设备上进行学习（On-Device Learning）已经成为了一个可行的选择。然而，基于单个设备和本地数据进行训练的模型很难达到高精度。为了解决这个问题，联邦学习（Federated Learning，FL）已经被提出，它可以在保护设备私钥数据的同时，通过共享设备之间的知识，实现私钥数据不泄露的高精度模型训练。然而，FL在许多实际应用场景中的可应用性和优势受到了多种多样性的限制。在这篇简述中，我们描述了FL面临的多样性挑战，特别是设备计算能力的多样性，并提供了一个全面的最新研究综述。我们将分为两组：一组是改进神经网络架构的方法，另一组是在系统层面进行多样性处理的方法，包括联邦平均（FedAvg）、液化、分布式学习等方法，以及同步和异步聚合方案。
</details></li>
</ul>
<hr>
<h2 id="ECSIC-Epipolar-Cross-Attention-for-Stereo-Image-Compression"><a href="#ECSIC-Epipolar-Cross-Attention-for-Stereo-Image-Compression" class="headerlink" title="ECSIC: Epipolar Cross Attention for Stereo Image Compression"></a>ECSIC: Epipolar Cross Attention for Stereo Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10284">http://arxiv.org/abs/2307.10284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthias Wödlinger, Jan Kotera, Manuel Keglevic, Jan Xu, Robert Sablatnig</li>
<li>for: 这个论文是为了提出一种新的学习基于方法，用于压缩立体图像。</li>
<li>methods: 该方法利用了两个 Stero Context 模块和一个 Stero Cross Attention（SCA）模块来同时压缩左右图像。SCA模块在相对应的epipolar线上进行了交叉注意力，并在平行进行处理。</li>
<li>results: 对比其他方法，ECSIC 在 Cityscapes 和 InStereo2k 两个 популяр的立体图像数据集上达到了最佳性能，同时允许快速编码和解码，非常适合实时应用。<details>
<summary>Abstract</summary>
In this paper, we present ECSIC, a novel learned method for stereo image compression. Our proposed method compresses the left and right images in a joint manner by exploiting the mutual information between the images of the stereo image pair using a novel stereo cross attention (SCA) module and two stereo context modules. The SCA module performs cross-attention restricted to the corresponding epipolar lines of the two images and processes them in parallel. The stereo context modules improve the entropy estimation of the second encoded image by using the first image as a context. We conduct an extensive ablation study demonstrating the effectiveness of the proposed modules and a comprehensive quantitative and qualitative comparison with existing methods. ECSIC achieves state-of-the-art performance among stereo image compression models on the two popular stereo image datasets Cityscapes and InStereo2k while allowing for fast encoding and decoding, making it highly practical for real-time applications.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的学习方法 для顺帧图像压缩，称为ECSIC。我们的提议方法将左右图像压缩在一起，通过利用顺帧图像对的相互信息来进行压缩。我们使用了一种新的顺帧相关注意力（SCA）模块和两个顺帧上下文模块来实现这一目标。SCA模块在相应的轴线上进行交叉注意力限制，并在平行进行处理。两个顺帧上下文模块使得第二个编码图像的Entropy估计得到改善，通过使用第一个图像作为 Context。我们进行了广泛的缺省研究，并对现有方法进行了全面的量化和质量比较。ECSIC在Cityscapes和InStereo2k两个流行的顺帧图像数据集上实现了顺帧图像压缩模型的状态机器，同时具有快速编码和解码功能，因此在实时应用中非常实用。
</details></li>
</ul>
<hr>
<h2 id="Towards-Trustworthy-Dataset-Distillation"><a href="#Towards-Trustworthy-Dataset-Distillation" class="headerlink" title="Towards Trustworthy Dataset Distillation"></a>Towards Trustworthy Dataset Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09165">http://arxiv.org/abs/2307.09165</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shijie Ma, Fei Zhu, Zhen Cheng, Xu-Yao Zhang<br>for: Trustworthy Dataset Distillation (TrustDD) aims to reduce training costs and enhance the trustworthiness of deep learning models in real-world applications by distilling both in-distribution (InD) samples and outliers.methods: The proposed method utilizes dataset distillation (DD) to condenses large datasets into tiny synthetic datasets, and introduces Pseudo-Outlier Exposure (POE) to generate pseudo-outliers and enhance OOD detection.results: Comprehensive experiments on various settings demonstrate the effectiveness of TrustDD, and the proposed POE surpasses state-of-the-art method Outlier Exposure (OE). TrustDD is more trustworthy and applicable to real open-world scenarios compared to preceding DD methods.Here’s the simplified Chinese version:for: TrustDD 目的是提高深度学习模型在实际应用中的可靠性和训练效率，通过将大量数据筛选到简单的Synthetic dataset中。methods: TrustDD 使用 dataset distillation (DD) 将大量数据筛选到简单的 Synthetic dataset 中，并引入 Pseudo-Outlier Exposure (POE) 生成 Pseudo-outlier 并提高 OOD 检测。results: 各种设置的 comprehensive 实验表明 TrustDD 的有效性，并且提出的 POE 超越了state-of-the-art 方法 Outlier Exposure (OE)。TrustDD 比前一代 DD 方法更可靠和适用于实际开放世界应用场景。<details>
<summary>Abstract</summary>
Efficiency and trustworthiness are two eternal pursuits when applying deep learning in real-world applications. With regard to efficiency, dataset distillation (DD) endeavors to reduce training costs by distilling the large dataset into a tiny synthetic dataset. However, existing methods merely concentrate on in-distribution (InD) classification in a closed-world setting, disregarding out-of-distribution (OOD) samples. On the other hand, OOD detection aims to enhance models' trustworthiness, which is always inefficiently achieved in full-data settings. For the first time, we simultaneously consider both issues and propose a novel paradigm called Trustworthy Dataset Distillation (TrustDD). By distilling both InD samples and outliers, the condensed datasets are capable to train models competent in both InD classification and OOD detection. To alleviate the requirement of real outlier data and make OOD detection more practical, we further propose to corrupt InD samples to generate pseudo-outliers and introduce Pseudo-Outlier Exposure (POE). Comprehensive experiments on various settings demonstrate the effectiveness of TrustDD, and the proposed POE surpasses state-of-the-art method Outlier Exposure (OE). Compared with the preceding DD, TrustDD is more trustworthy and applicable to real open-world scenarios. Our code will be publicly available.
</details>
<details>
<summary>摘要</summary>
“效率和可靠性是深度学习应用实际场景中的两大永恒追求。在这个领域，数据集缩写（DD）尝试通过缩写大数据集为一个小型的合成数据集来减少训练成本。然而，现有方法仅关注在关闭世界设定下的内部分布（InD）类别，忽略了外部分布（OOD）样本。然而，OOD检测的目的是增强模型的可靠性，这通常在全数据设定下是不fficient的。为了解决这些问题，我们同时考虑了这两个问题，并提出了一种新的思路called Trustworthy Dataset Distillation（TrustDD）。通过缩写InD样本和异常样本，缩写后的数据集可以训练能够在InD类别和OOD检测中具备竞争力。为了避免实际异常数据的需求和使OOD检测更实用，我们进一步提出了 Pseudo-Outlier Exposure（POE）。我们对不同的设定进行了广泛的实验，并证明了 TrustDD 的有效性，而我们提出的 POE 超过了现有的 Outlier Exposure（OE）方法。相比之下，TrustDD 更加可靠和适用于真实的开放世界场景。我们的代码将在公共可用。”
</details></li>
</ul>
<hr>
<h2 id="MVA2023-Small-Object-Detection-Challenge-for-Spotting-Birds-Dataset-Methods-and-Results"><a href="#MVA2023-Small-Object-Detection-Challenge-for-Spotting-Birds-Dataset-Methods-and-Results" class="headerlink" title="MVA2023 Small Object Detection Challenge for Spotting Birds: Dataset, Methods, and Results"></a>MVA2023 Small Object Detection Challenge for Spotting Birds: Dataset, Methods, and Results</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09143">http://arxiv.org/abs/2307.09143</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iim-ttij/mva2023smallobjectdetection4spottingbirds">https://github.com/iim-ttij/mva2023smallobjectdetection4spottingbirds</a></li>
<li>paper_authors: Yuki Kondo, Norimichi Ukita, Takayuki Yamaguchi, Hao-Yu Hou, Mu-Yi Shen, Chia-Chi Hsu, En-Ming Huang, Yu-Chen Huang, Yu-Cheng Xia, Chien-Yao Wang, Chun-Yi Lee, Da Huo, Marc A. Kastner, Tingwei Liu, Yasutomo Kawanishi, Takatsugu Hirayama, Takahiro Komamizu, Ichiro Ide, Yosuke Shinya, Xinyao Liu, Guang Liang, Syusuke Yasui</li>
<li>for: 本研究旨在提出一个新的小物体检测数据集，以便进行远程小物体检测的实际应用。</li>
<li>methods: 本文提出了一种新的小物体检测方法，并在223名参与者的挑战中评测了其效果。</li>
<li>results: 研究发现，使用这种新方法可以在远程小物体检测中获得优秀的效果，并且提供了一个大量的小物体检测数据集和基eline代码以便进一步研究。<details>
<summary>Abstract</summary>
Small Object Detection (SOD) is an important machine vision topic because (i) a variety of real-world applications require object detection for distant objects and (ii) SOD is a challenging task due to the noisy, blurred, and less-informative image appearances of small objects. This paper proposes a new SOD dataset consisting of 39,070 images including 137,121 bird instances, which is called the Small Object Detection for Spotting Birds (SOD4SB) dataset. The detail of the challenge with the SOD4SB dataset is introduced in this paper. In total, 223 participants joined this challenge. This paper briefly introduces the award-winning methods. The dataset, the baseline code, and the website for evaluation on the public testset are publicly available.
</details>
<details>
<summary>摘要</summary>
小物体检测（SOD）是机器视觉领域的重要话题，因为（i）许多现实世界应用需要对远距离的物体进行检测，以及（ii）SOD是一项复杂的任务，因为小物体的图像表现具有噪声、模糊和不具有很多信息。这篇论文提出了一个新的SOD数据集，包括39,070张图像和137,121只鸟类实例，称为Small Object Detection for Spotting Birds（SOD4SB）数据集。本文介绍了SOD4SB数据集的挑战。总共有223名参与者参加了这个挑战。本文 briefly introduce了获奖方法。数据集、基线代码和评估网站对公共测试集进行评估是公共可用的。
</details></li>
</ul>
<hr>
<h2 id="Characterization-of-partial-wetting-by-CMAS-droplets-using-multiphase-many-body-dissipative-particle-dynamics-and-data-driven-discovery-based-on-PINNs"><a href="#Characterization-of-partial-wetting-by-CMAS-droplets-using-multiphase-many-body-dissipative-particle-dynamics-and-data-driven-discovery-based-on-PINNs" class="headerlink" title="Characterization of partial wetting by CMAS droplets using multiphase many-body dissipative particle dynamics and data-driven discovery based on PINNs"></a>Characterization of partial wetting by CMAS droplets using multiphase many-body dissipative particle dynamics and data-driven discovery based on PINNs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09142">http://arxiv.org/abs/2307.09142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elham Kiyani, Mahdi Kooshkbaghi, Khemraj Shukla, Rahul Babu Koneru, Zhen Li, Luis Bravo, Anindya Ghoshal, George Em Karniadakis, Mikko Karttunen</li>
<li>For: 这研究探讨了高熔率的CMAS液滴在不同初始尺寸和稳定接触角下的湿润动态。* Methods: 这研究使用多相多体积耗散动力学（mDPD）模拟，研究CMAS液滴的湿润动态。使用Physics-Informed Neural Network（PINN）框架确定湿润半径行为。使用符号回归来表示关系函数。* Results: 研究发现了CMAS液滴的湿润半径行为，并使用Bayesian PINNs（B-PINNs）评估和量化相关参数的不确定性。这研究将湿润动态模拟和机器学习技术结合，为高温应用提供了创新解决方案。<details>
<summary>Abstract</summary>
The molten sand, a mixture of calcia, magnesia, alumina, and silicate, known as CMAS, is characterized by its high viscosity, density, and surface tension. The unique properties of CMAS make it a challenging material to deal with in high-temperature applications, requiring innovative solutions and materials to prevent its buildup and damage to critical equipment. Here, we use multiphase many-body dissipative particle dynamics (mDPD) simulations to study the wetting dynamics of highly viscous molten CMAS droplets. The simulations are performed in three dimensions, with varying initial droplet sizes and equilibrium contact angles. We propose a coarse parametric ordinary differential equation (ODE) that captures the spreading radius behavior of the CMAS droplets. The ODE parameters are then identified based on the Physics-Informed Neural Network (PINN) framework. Subsequently, the closed form dependency of parameter values found by PINN on the initial radii and contact angles are given using symbolic regression. Finally, we employ Bayesian PINNs (B-PINNs) to assess and quantify the uncertainty associated with the discovered parameters. In brief, this study provides insight into spreading dynamics of CMAS droplets by fusing simple parametric ODE modeling and state-of-the-art machine learning techniques.
</details>
<details>
<summary>摘要</summary>
熔融砂粒材料（CMAS），它是含 calcium、magnesia、alumina 和 silicate 的混合物，具有高粘度、密度和表面张力。由于 CMAS 的特有性，在高温应用中处理它是一项挑战，需要创新的解决方案和材料来避免它的堆积和设备损害。在这里，我们使用多相多体积排斥凝聚 dynamics（mDPD）仿真来研究高粘度熔融 CMAS 液滴的湿润动力学。仿真在三维空间中进行，初始液滴尺寸和均衡接触角度进行变化。我们提出了一个粗略的常数参数方程（ODE），捕捉液滴的扩散半径行为。ODE 参数的标准值 Subsequently, the closed form dependency of parameter values found by PINN on the initial radii and contact angles are given using symbolic regression. Finally, we employ Bayesian PINNs (B-PINNs) to assess and quantify the uncertainty associated with the discovered parameters. In brief, this study provides insight into spreading dynamics of CMAS droplets by fusing simple parametric ODE modeling and state-of-the-art machine learning techniques.
</details></li>
</ul>
<hr>
<h2 id="Mining-of-Single-Class-by-Active-Learning-for-Semantic-Segmentation"><a href="#Mining-of-Single-Class-by-Active-Learning-for-Semantic-Segmentation" class="headerlink" title="Mining of Single-Class by Active Learning for Semantic Segmentation"></a>Mining of Single-Class by Active Learning for Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09109">http://arxiv.org/abs/2307.09109</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hugues Lambert, Emma Slade</li>
<li>for: 本研究的目的是提出一种基于深度优化学习的活动学习策略，以提高特定类型的模型训练效果。</li>
<li>methods: 本研究使用的方法是基于深度优化学习的 MiSiCAL 方法，它可以通过量精度相关性来建立高性能的模型训练集。 MiSiCAL 方法不需要重新训练目标模型多次，因此适用于大批量训练。</li>
<li>results: 研究结果表明，MiSiCAL 方法能够在 COCO10k 数据集上 OUTPERFORM 随机策略的 150 个类型，而最强的基线方法只能 OUTPERFORM 随机策略的 101 个类型。<details>
<summary>Abstract</summary>
Several Active Learning (AL) policies require retraining a target model several times in order to identify the most informative samples and rarely offer the option to focus on the acquisition of samples from underrepresented classes. Here the Mining of Single-Class by Active Learning (MiSiCAL) paradigm is introduced where an AL policy is constructed through deep reinforcement learning and exploits quantity-accuracy correlations to build datasets on which high-performance models can be trained with regards to specific classes. MiSiCAL is especially helpful in the case of very large batch sizes since it does not require repeated model training sessions as is common in other AL methods. This is thanks to its ability to exploit fixed representations of the candidate data points. We find that MiSiCAL is able to outperform a random policy on 150 out of 171 COCO10k classes, while the strongest baseline only outperforms random on 101 classes.
</details>
<details>
<summary>摘要</summary>
几种活动学习（AL）策略需要重新训练目标模型多次以确定最有用的样本并rarely提供针对少 представohn classes 的集中着注意力选择样本的选择。在这里，我们介绍了一种名为MINING SINGLE-CLASS BY ACTIVE LEARNING（MiSiCAL）的策略，通过深度强化学习构建了一个AL策略，利用量精度相关性来建立高性能模型可以在特定类上训练。MiSiCAL在大批量时 particuarily helpful，因为它不需要重复的模型训练会议。这是因为它可以利用固定表示的候选数据点。我们发现MiSiCAL可以在150个COCO10k类中超过随机策略，而最强基eline只能在101个类中超过随机策略。
</details></li>
</ul>
<hr>
<h2 id="Non-stationary-Delayed-Combinatorial-Semi-Bandit-with-Causally-Related-Rewards"><a href="#Non-stationary-Delayed-Combinatorial-Semi-Bandit-with-Causally-Related-Rewards" class="headerlink" title="Non-stationary Delayed Combinatorial Semi-Bandit with Causally Related Rewards"></a>Non-stationary Delayed Combinatorial Semi-Bandit with Causally Related Rewards</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09093">http://arxiv.org/abs/2307.09093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saeed Ghoorchian, Setareh Maghsudi</li>
<li>for: The paper is written for solving the problem of sequential decision-making under uncertainty with long feedback delays, particularly in non-stationary environments with structural dependencies amongst the reward distributions.</li>
<li>methods: The paper proposes a policy that learns the causal relations between the arms using a stationary structural equation model, and utilizes this knowledge to optimize the decision-making while adapting to drifts.</li>
<li>results: The paper proves a regret bound for the performance of the proposed algorithm, and evaluates the method via numerical analysis using synthetic and real-world datasets to detect the regions that contribute the most to the spread of Covid-19 in Italy.<details>
<summary>Abstract</summary>
Sequential decision-making under uncertainty is often associated with long feedback delays. Such delays degrade the performance of the learning agent in identifying a subset of arms with the optimal collective reward in the long run. This problem becomes significantly challenging in a non-stationary environment with structural dependencies amongst the reward distributions associated with the arms. Therefore, besides adapting to delays and environmental changes, learning the causal relations alleviates the adverse effects of feedback delay on the decision-making process. We formalize the described setting as a non-stationary and delayed combinatorial semi-bandit problem with causally related rewards. We model the causal relations by a directed graph in a stationary structural equation model. The agent maximizes the long-term average payoff, defined as a linear function of the base arms' rewards. We develop a policy that learns the structural dependencies from delayed feedback and utilizes that to optimize the decision-making while adapting to drifts. We prove a regret bound for the performance of the proposed algorithm. Besides, we evaluate our method via numerical analysis using synthetic and real-world datasets to detect the regions that contribute the most to the spread of Covid-19 in Italy.
</details>
<details>
<summary>摘要</summary>
纷纷决策下面存在长时间反馈延迟，这会导致学习代理人的表现下降，无法在长期内确定优化收益的子集。在非站点环境中，抽象依赖关系的赏金分布难以预测，这使得问题变得更加挑战。因此，除了适应延迟和环境变化之外，学习 causal 关系可以减轻延迟的负面影响。我们将此设定形式化为非站点和延迟 combinatorial 半带抽象问题，使用 causally 相关的赏金分布模型。我们的代理人通过延迟反馈学习 structural 相关性，并使用这些相关性来优化决策。我们证明了我们提出的算法的 regret  bound。此外，我们通过NumPy和实际数据进行数学分析，以便检测意大利COVID-19 的扩散区域。
</details></li>
</ul>
<hr>
<h2 id="A-Federated-learning-model-for-Electric-Energy-management-using-Blockchain-Technology"><a href="#A-Federated-learning-model-for-Electric-Energy-management-using-Blockchain-Technology" class="headerlink" title="A Federated learning model for Electric Energy management using Blockchain Technology"></a>A Federated learning model for Electric Energy management using Blockchain Technology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09080">http://arxiv.org/abs/2307.09080</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Shoaib Farooq, Azeen Ahmed Hayat</li>
<li>For: The paper aims to address energy shortfall and electricity load shedding in developing countries by improving energy management and increasing the use of renewable energy sources.* Methods: The paper proposes the use of federated learning and blockchain technology to forecast energy requirements and ensure transparency, traceability, and security in energy transactions between prosumers and consumers.* Results: The experiment results show that renewable energy sources have produced better and comparable results to other non-renewable energy resources.Here’s the simplified Chinese text for the three points:* For: 这篇论文目的是解决发展中国家的能源短缺和电力卸载问题，通过改善能源管理和使用可再生能源。* Methods: 论文提议使用联邦学习和区块链技术，对消费者和生产者之间的能源交易进行透明度、跟踪性和安全性的保障。* Results: 实验结果表明，可再生能源资源比其他非可再生能源资源更好和相当。<details>
<summary>Abstract</summary>
Energy shortfall and electricity load shedding are the main problems for developing countries. The main causes are lack of management in the energy sector and the use of non-renewable energy sources. The improved energy management and use of renewable sources can be significant to resolve energy crisis. It is necessary to increase the use of renewable energy sources (RESs) to meet the increasing energy demand due to high prices of fossil-fuel based energy. Federated learning (FL) is the most emerging technique in the field of artificial intelligence. Federated learning helps to generate global model at server side by ensemble locally trained models at remote edges sites while preserving data privacy. The global model used to predict energy demand to satisfy the needs of consumers. In this article, we have proposed Blockchain based safe distributed ledger technology for transaction of data between prosumer and consumer to ensure their transparency, traceability and security. Furthermore, we have also proposed a Federated learning model to forecast the energy requirements of consumer and prosumer. Moreover, Blockchain has been used to store excess energy data from prosumer for better management of energy between prosumer and grid. Lastly, the experiment results revealed that renewable energy sources have produced better and comparable results to other non-renewable energy resources.
</details>
<details>
<summary>摘要</summary>
发展中国家面临着能源短缺和电力卸载危机，主要原因是能源部门的管理不足和使用非可再生能源。通过改善能源管理和使用可再生能源，可以有效解决能源危机。随着化石燃料基本能源价格的上涨，使用可再生能源成为了解决能源危机的重要手段。最新的技术之一是联邦学习（FL），它可以在远程的边缘设备上 ensemble 本地训练的模型，而不需要将数据传输到服务器端，以保护数据隐私。这个全球模型可以预测消费者的能源需求，并且可以使用可再生能源来满足消费者的需求。本文提出了基于区块链的安全分布式笔记录技术，用于在消费者和生产者之间传输数据，以确保数据的透明度、可追溯性和安全性。此外，我们还提出了基于联邦学习的能源需求预测模型，以便更好地预测消费者和生产者的能源需求。此外，使用区块链存储生产者的剩余能源数据，以便更好地管理能源的协调。实验结果表明，可再生能源可以生产更好的和相对比较好的结果，相比于其他非可再生能源资源。
</details></li>
</ul>
<hr>
<h2 id="DiTTO-Diffusion-inspired-Temporal-Transformer-Operator"><a href="#DiTTO-Diffusion-inspired-Temporal-Transformer-Operator" class="headerlink" title="DiTTO: Diffusion-inspired Temporal Transformer Operator"></a>DiTTO: Diffusion-inspired Temporal Transformer Operator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09072">http://arxiv.org/abs/2307.09072</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oded Ovadia, Eli Turkel, Adar Kahana, George Em Karniadakis</li>
<li>for: 用于解决时间取值积分方程（PDE）。</li>
<li>methods: 使用数据驱动的操作学习方法，不需要时间排序。</li>
<li>results: 在多维度的 burgers 方程、navier-stokes 方程和声波方程上达到了状态机器人精度。 Additionally, the method can perform zero-shot super-resolution in time.Here’s the full text in Simplified Chinese:</li>
<li>for: 本文用于解决时间取值积分方程（PDE）。</li>
<li>methods: 本文提出了一种基于操作学习的数据驱动方法，不需要时间排序。</li>
<li>results: 在多维度的 burgers 方程、navier-stokes 方程和声波方程上，本方法达到了状态机器人精度。此外，方法还可以实现零试验超分辨率。<details>
<summary>Abstract</summary>
Solving partial differential equations (PDEs) using a data-driven approach has become increasingly common. The recent development of the operator learning paradigm has enabled the solution of a broader range of PDE-related problems. We propose an operator learning method to solve time-dependent PDEs continuously in time without needing any temporal discretization. The proposed approach, named DiTTO, is inspired by latent diffusion models. While diffusion models are usually used in generative artificial intelligence tasks, their time-conditioning mechanism is extremely useful for PDEs. The diffusion-inspired framework is combined with elements from the Transformer architecture to improve its capabilities.   We demonstrate the effectiveness of the new approach on a wide variety of PDEs in multiple dimensions, namely the 1-D Burgers' equation, 2-D Navier-Stokes equations, and the acoustic wave equation in 2-D and 3-D. DiTTO achieves state-of-the-art results in terms of accuracy for these problems. We also present a method to improve the performance of DiTTO by using fast sampling concepts from diffusion models. Finally, we show that DiTTO can accurately perform zero-shot super-resolution in time.
</details>
<details>
<summary>摘要</summary>
解决部分梯度方程（PDEs）使用数据驱动方法已成为日益普遍。最近的运算学学 paradigm的发展已使得解决更广泛的 PDE 相关问题变得可能。我们提议一种运算学学方法，名为 DiTTO，可以连续地在时间上解决时间依赖的 PDE。该方法灵感于潜在扩散模型，而扩散模型通常用于生成人工智能任务。扩散启发的框架与 transformer 架构的元素相结合，以提高其能力。我们在多维 PDE 上进行了广泛的实验，包括一维拜尔斯坦方程、二维奈尔-斯托克方程以及二维和三维的声波方程。DiTTO 在这些问题上达到了最新的精度标准。此外，我们还提出了使用快速抽样概念从扩散模型来提高 DiTTO 的性能的方法。最后，我们展示了 DiTTO 可以准确地进行零 shot 超分辨率在时间上。
</details></li>
</ul>
<hr>
<h2 id="Evaluate-Fine-tuning-Strategies-for-Fetal-Head-Ultrasound-Image-Segmentation-with-U-Net"><a href="#Evaluate-Fine-tuning-Strategies-for-Fetal-Head-Ultrasound-Image-Segmentation-with-U-Net" class="headerlink" title="Evaluate Fine-tuning Strategies for Fetal Head Ultrasound Image Segmentation with U-Net"></a>Evaluate Fine-tuning Strategies for Fetal Head Ultrasound Image Segmentation with U-Net</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09067">http://arxiv.org/abs/2307.09067</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/13204942/ft_methods_for_fetal_head_segmentation">https://github.com/13204942/ft_methods_for_fetal_head_segmentation</a></li>
<li>paper_authors: Fangyijie Wang, Guénolé Silvestre, Kathleen M. Curran</li>
<li>for: 这篇研究的目的是提高妊娠期间胎头圆周盘 circumference（HC）的测量效率，使用扩展学习（Transfer Learning，TL）方法来改善医疗生物米etry的精度。</li>
<li>methods: 本研究使用了潜在神经网络（Convolutional Neural Network，CNN）模型，并使用了轻量级的 MobileNet 作为数据库的数据库。</li>
<li>results: 研究发现，使用 Transfer Learning 方法可以将胎头像的数据库训练为 U-Net 网络，并且可以实现高度的准确性，仅需要有限的训练时间和资源。此外，本研究还发现，使用 Transfer Learning 方法可以实现较小的模型大小，并且可以提高模型的稳定性和可靠性。<details>
<summary>Abstract</summary>
Fetal head segmentation is a crucial step in measuring the fetal head circumference (HC) during gestation, an important biometric in obstetrics for monitoring fetal growth. However, manual biometry generation is time-consuming and results in inconsistent accuracy. To address this issue, convolutional neural network (CNN) models have been utilized to improve the efficiency of medical biometry. But training a CNN network from scratch is a challenging task, we proposed a Transfer Learning (TL) method. Our approach involves fine-tuning (FT) a U-Net network with a lightweight MobileNet as the encoder to perform segmentation on a set of fetal head ultrasound (US) images with limited effort. This method addresses the challenges associated with training a CNN network from scratch. It suggests that our proposed FT strategy yields segmentation performance that is comparable when trained with a reduced number of parameters by 85.8%. And our proposed FT strategy outperforms other strategies with smaller trainable parameter sizes below 4.4 million. Thus, we contend that it can serve as a dependable FT approach for reducing the size of models in medical image analysis. Our key findings highlight the importance of the balance between model performance and size in developing Artificial Intelligence (AI) applications by TL methods. Code is available at https://github.com/13204942/FT_Methods_for_Fetal_Head_Segmentation.
</details>
<details>
<summary>摘要</summary>
Gestational fetal head circumference (HC) measurement is crucial, and manual biometry is time-consuming and prone to errors. To address this, convolutional neural networks (CNNs) have been used for biometry. However, training a CNN from scratch is challenging. To solve this, we proposed a transfer learning (TL) method. Our approach involves fine-tuning a U-Net network with a lightweight MobileNet as the encoder to perform segmentation on a set of fetal head ultrasound (US) images with minimal effort. This method addresses the challenges of training a CNN from scratch and shows that our proposed FT strategy achieves segmentation performance comparable to training with a reduced number of parameters by 85.8%. Our proposed FT strategy also outperforms other strategies with smaller trainable parameter sizes below 4.4 million. Therefore, we suggest that our FT approach is a reliable method for reducing the size of models in medical image analysis. Our findings highlight the importance of balancing model performance and size in developing artificial intelligence (AI) applications using TL methods. 针对妊娠期胎头径的量度是至关重要，但是手动测量是时间费时且精度不稳定。为解决这个问题，人工神经网络（CNN）已经被应用于生物метría。然而，从零开始训练CNN是一项具有挑战性的任务。为解决这个问题，我们提出了传输学习（TL）方法。我们的方法涉及到了精细调整U-Net网络的 MobileNet 作为编码器，以实现对一组妊娠期胎头ultrasound（US）图像进行分割，即使是很少的努力。这种方法解决了训练CNN从零开始的挑战，并表明了我们的FT策略可以与减少参数数量的85.8%相比，实现分割性能。此外，我们的FT策略还超过了其他具有更小的可学习参数数量的策略。因此，我们建议这种FT方法可以作为医疗图像分析中减小模型的可靠方法。我们的关键发现指出了在通过TL方法开发人工智能应用程序时，模型性能和模型大小之间的平衡是非常重要的。
</details></li>
</ul>
<hr>
<h2 id="Learning-Adaptive-Neighborhoods-for-Graph-Neural-Networks"><a href="#Learning-Adaptive-Neighborhoods-for-Graph-Neural-Networks" class="headerlink" title="Learning Adaptive Neighborhoods for Graph Neural Networks"></a>Learning Adaptive Neighborhoods for Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09065">http://arxiv.org/abs/2307.09065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Avishkar Saha, Oscar Mendez, Chris Russell, Richard Bowden</li>
<li>for: 实现终端学习于 гра组织资料上，但许多工作假设已知 graph 结构。当输入 graph 是噪音或无法取得时，一种方法是建构或学习 latent graph 结构。这些方法通常固定 graph 结构中每个 node 的度量，这是不佳的。相反，我们提出了一个 novel end-to-end differentiable graph generator，可以建构 graph 结构，每个 node 可以选择它的邻居和大小。</li>
<li>methods: 我们提出了一个 novel end-to-end differentiable graph generator，可以建构 graph 结构，每个 node 可以选择它的邻居和大小。</li>
<li>results: 我们将我们的模组 integrate 到 trajectory prediction, point cloud classification 和 node classification pipeline 中，实现了与其他 structure-learning 方法相比的提高精度，在各种数据集和 GCN 背景下。<details>
<summary>Abstract</summary>
Graph convolutional networks (GCNs) enable end-to-end learning on graph structured data. However, many works assume a given graph structure. When the input graph is noisy or unavailable, one approach is to construct or learn a latent graph structure. These methods typically fix the choice of node degree for the entire graph, which is suboptimal. Instead, we propose a novel end-to-end differentiable graph generator which builds graph topologies where each node selects both its neighborhood and its size. Our module can be readily integrated into existing pipelines involving graph convolution operations, replacing the predetermined or existing adjacency matrix with one that is learned, and optimized, as part of the general objective. As such it is applicable to any GCN. We integrate our module into trajectory prediction, point cloud classification and node classification pipelines resulting in improved accuracy over other structure-learning methods across a wide range of datasets and GCN backbones.
</details>
<details>
<summary>摘要</summary>
格点卷积网络（GCN）可以实现端到端学习在格式化数据上。然而，许多工作假设给定的图结构。当输入图是噪音或不可用时，一种方法是构建或学习隐藏的图结构。这些方法通常固定整个图的节点度，这是不优化的。相反，我们提出了一种新的终端可微的图生成器，它可以在每个节点选择其邻居和大小。我们的模块可以轻松地与现有的图卷积操作相结合，将预先确定或现有的相互作用矩阵 replaced with一个学习和优化的矩阵，并成为任何GCN的一部分。因此，它是可应用的。我们将我们的模块集成到了路径预测、点云分类和节点分类管道中，从而在各种数据集和GCN脊梁上实现了与其他结构学习方法相比较高的准确率。
</details></li>
</ul>
<hr>
<h2 id="Extreme-heatwave-sampling-and-prediction-with-analog-Markov-chain-and-comparisons-with-deep-learning"><a href="#Extreme-heatwave-sampling-and-prediction-with-analog-Markov-chain-and-comparisons-with-deep-learning" class="headerlink" title="Extreme heatwave sampling and prediction with analog Markov chain and comparisons with deep learning"></a>Extreme heatwave sampling and prediction with analog Markov chain and comparisons with deep learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09060">http://arxiv.org/abs/2307.09060</a></li>
<li>repo_url: None</li>
<li>paper_authors: George Miloshevich, Dario Lucente, Pascal Yiou, Freddy Bouchet</li>
<li>For: The paper aims to develop a data-driven emulator, called stochastic weather generator (SWG), to estimate the probabilities of prolonged heatwaves in France and Scandinavia.* Methods: The SWG emulator uses the method of analogs of circulation, which is combined with temperature and soil moisture as predictor fields. The emulator is trained on an intermediate complexity climate model run, and the performance is evaluated using proper score appropriate for rare events. Dimensionality reduction techniques are applied to accelerate the computation of analogs.* Results: The probabilistic prediction achieved with SWG is compared with the one achieved with Convolutional Neural Network (CNN). The SWG emulator trained on 80 years of data is capable of estimating extreme return times of order of thousands of years for heatwaves longer than several days more precisely than the fit based on generalised extreme value distribution. The quality of its synthetic extreme teleconnection patterns obtained with stochastic weather generator is studied, and two examples of such synthetic teleconnection patterns for heatwaves in France and Scandinavia are provided.<details>
<summary>Abstract</summary>
We present a data-driven emulator, stochastic weather generator (SWG), suitable for estimating probabilities of prolonged heatwaves in France and Scandinavia. This emulator is based on the method of analogs of circulation to which we add temperature and soil moisture as predictor fields. We train the emulator on an intermediate complexity climate model run and show that it is capable of predicting conditional probabilities (forecasting) of heatwaves out of sample. Special attention is payed that this prediction is evaluated using proper score appropriate for rare events. To accelerate the computation of analogs dimensionality reduction techniques are applied and the performance is evaluated. The probabilistic prediction achieved with SWG is compared with the one achieved with   Convolutional Neural Network (CNN). With the availability of hundreds of years of training data CNNs perform better at the task of probabilistic prediction. In addition, we show that the SWG emulator trained on 80 years of data is capable of estimating extreme return times of order of thousands of years for heatwaves longer than several days more precisely than the fit based on generalised extreme value distribution. Finally, the quality of its synthetic extreme teleconnection patterns obtained with stochastic weather generator is studied. We showcase two examples of such synthetic teleconnection patterns for heatwaves in France and Scandinavia that compare favorably to the very long climate model control run.
</details>
<details>
<summary>摘要</summary>
我们介绍了一个数据驱动的模拟器，随机天气生成器（SWG），用于估计法国和斯堪的纳维亚地区的持续高温事件的可能性。这个模拟器基于流体动力学方法，并添加温度和土壤湿度作为预测字段。我们使用一个中间复杂度气候模型的训练来训练这个模拟器，并显示它可以预测基于样本外的条件概率（预测）高温事件。特别是，我们使用合适的评价函数来评估这种预测，以确保对罕见事件进行正确的评估。为加速计算流体的维度减少技术是应用于 analogs，并评估其性能。我们还比较了使用 convolutional neural network（CNN）进行probabilistic预测的性能。通过使用数百年的训练数据，CNN在这项任务上表现更好。此外，我们发现使用80年的数据训练SWG模拟器可以更正精确地估计高温事件持续时间长于数天的极端返回时间，与基于总体极值分布的预测相比。最后，我们研究了SWG模拟器生成的 sintethic极端 теле连接模式的质量。我们展示了法国和斯堪的纳维亚两个高温事件的 sintethic tele连接模式，与非常长气候模型控制运行相比，表现良好。
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-for-unsupervised-domain-adaptation-in-medical-imaging-Recent-advancements-and-future-perspectives"><a href="#Deep-learning-for-unsupervised-domain-adaptation-in-medical-imaging-Recent-advancements-and-future-perspectives" class="headerlink" title="Deep learning for unsupervised domain adaptation in medical imaging: Recent advancements and future perspectives"></a>Deep learning for unsupervised domain adaptation in medical imaging: Recent advancements and future perspectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01265">http://arxiv.org/abs/2308.01265</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suruchi Kumari, Pravendra Singh</li>
<li>for: 本文主要探讨了医学成像领域中最新的深度学习领域适应（Unsupervised Domain Adaptation，UDA）技术，以及它们在各种医学成像任务中的应用。</li>
<li>methods: 本文分析了医学成像领域中最新的UDA方法，包括特征对应、图像翻译、自我超vision和分解表示方法等多种方法。</li>
<li>results: 本文对各种UDA方法进行了技术分析和评估，并将其分为六个类别，包括图像分类、生物marks检测、肿瘤识别、脑成像分析、肠胃成像分析等多种任务。<details>
<summary>Abstract</summary>
Deep learning has demonstrated remarkable performance across various tasks in medical imaging. However, these approaches primarily focus on supervised learning, assuming that the training and testing data are drawn from the same distribution. Unfortunately, this assumption may not always hold true in practice. To address these issues, unsupervised domain adaptation (UDA) techniques have been developed to transfer knowledge from a labeled domain to a related but unlabeled domain. In recent years, significant advancements have been made in UDA, resulting in a wide range of methodologies, including feature alignment, image translation, self-supervision, and disentangled representation methods, among others. In this paper, we provide a comprehensive literature review of recent deep UDA approaches in medical imaging from a technical perspective. Specifically, we categorize current UDA research in medical imaging into six groups and further divide them into finer subcategories based on the different tasks they perform. We also discuss the respective datasets used in the studies to assess the divergence between the different domains. Finally, we discuss emerging areas and provide insights and discussions on future research directions to conclude this survey.
</details>
<details>
<summary>摘要</summary>
深度学习在医疗影像领域已经表现出惊人的表现。然而，这些方法主要偏向于有监督学习，假设训练和测试数据来自同一个分布。然而，这种假设可能不 siempre 成立。为 Address 这些问题，无监督领域适应（UDA）技术被开发出来，以传递来自标注域的知识到相关 yet unlabeled 域。在过去几年中，UDA 领域内有了大量的进展，包括特征对齐、图像翻译、自我监督和分解表示方法等。在本文中，我们提供了医疗影像领域的深度 UDA 方法的全面文献回顾，具体来说，我们将当前 UDA 研究分为六个组，并将它们进一步分为不同任务的子类别。我们还讨论了不同研究使用的数据集，以评估不同领域之间的差异。最后，我们提出了未来研究的前景和意见，以结束本文的报告。
</details></li>
</ul>
<hr>
<h2 id="Globally-solving-the-Gromov-Wasserstein-problem-for-point-clouds-in-low-dimensional-Euclidean-spaces"><a href="#Globally-solving-the-Gromov-Wasserstein-problem-for-point-clouds-in-low-dimensional-Euclidean-spaces" class="headerlink" title="Globally solving the Gromov-Wasserstein problem for point clouds in low dimensional Euclidean spaces"></a>Globally solving the Gromov-Wasserstein problem for point clouds in low dimensional Euclidean spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09057">http://arxiv.org/abs/2307.09057</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Ryner, Jan Kronqvist, Johan Karlsson</li>
<li>for:  Computes the Gromov-Wasserstein problem between two sets of points in low dimensional spaces, to quantify the similarity between two formations or shapes.</li>
<li>methods:  Reformulates the Quadratic Assignment Problem (QAP) as an optimization problem with a low-dimensional domain, leveraging the fact that the problem can be expressed as a concave quadratic optimization problem with low rank.</li>
<li>results:  Scales well with the number of points and can be used to find the global solution for large-scale problems with thousands of points.<details>
<summary>Abstract</summary>
This paper presents a framework for computing the Gromov-Wasserstein problem between two sets of points in low dimensional spaces, where the discrepancy is the squared Euclidean norm. The Gromov-Wasserstein problem is a generalization of the optimal transport problem that finds the assignment between two sets preserving pairwise distances as much as possible. This can be used to quantify the similarity between two formations or shapes, a common problem in AI and machine learning. The problem can be formulated as a Quadratic Assignment Problem (QAP), which is in general computationally intractable even for small problems. Our framework addresses this challenge by reformulating the QAP as an optimization problem with a low-dimensional domain, leveraging the fact that the problem can be expressed as a concave quadratic optimization problem with low rank. The method scales well with the number of points, and it can be used to find the global solution for large-scale problems with thousands of points. We compare the computational complexity of our approach with state-of-the-art methods on synthetic problems and apply it to a near-symmetrical problem which is of particular interest in computational biology.
</details>
<details>
<summary>摘要</summary>
However, the Gromov-Wasserstein problem is computationally intractable, even for small problems, and is typically formulated as a Quadratic Assignment Problem (QAP). Our framework addresses this challenge by reformulating the QAP as an optimization problem with a low-dimensional domain, leveraging the fact that the problem can be expressed as a concave quadratic optimization problem with low rank.The proposed method scales well with the number of points and can be used to find the global solution for large-scale problems with thousands of points. We compare the computational complexity of our approach with state-of-the-art methods on synthetic problems and apply it to a near-symmetrical problem of particular interest in computational biology.
</details></li>
</ul>
<hr>
<h2 id="Outlier-Robust-Tensor-Low-Rank-Representation-for-Data-Clustering"><a href="#Outlier-Robust-Tensor-Low-Rank-Representation-for-Data-Clustering" class="headerlink" title="Outlier-Robust Tensor Low-Rank Representation for Data Clustering"></a>Outlier-Robust Tensor Low-Rank Representation for Data Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09055">http://arxiv.org/abs/2307.09055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tong Wu</li>
<li>for: 提取受损张量数据中的异常点和分 clustering</li>
<li>methods: 基于张量特征值分解（t-SVD）的异常点检测和张量数据分 clustering</li>
<li>results: 可以 preciselly 恢复受损张量数据的行空间和检测异常点，并且可以 hanlde missing 数据情况。<details>
<summary>Abstract</summary>
Low-rank tensor analysis has received widespread attention with many practical applications. However, the tensor data are often contaminated by outliers or sample-specific corruptions. How to recover the tensor data that are corrupted by outliers and perform data clustering remains a challenging problem. This paper develops an outlier-robust tensor low-rank representation (OR-TLRR) method for simultaneous outlier detection and tensor data clustering based on the tensor singular value decomposition (t-SVD) algebraic framework. It is motivated by the recently proposed tensor-tensor product induced by invertible linear transforms that satisfy certain conditions. For tensor observations with arbitrary outlier corruptions, OR-TLRR has provable performance guarantee for exactly recovering the row space of clean data and detecting outliers under mild conditions. Moreover, an extension of OR-TLRR is also proposed to handle the case when parts of the data are missing. Finally, extensive experimental results on both synthetic and real data demonstrate the effectiveness of the proposed algorithms.
</details>
<details>
<summary>摘要</summary>
低级张量分析已经广泛受到关注，有很多实际应用。然而，张量数据经常受到异常值或样本特定的损害。如何修复受损的张量数据，并对其进行分类仍然是一个困难的问题。这篇论文开发了一种对异常值敏感的张量低级表示（OR-TLRR）方法，用于同时检测异常值和张量数据的分类。它是基于张量单值分解（t-SVD）的代数框架的。对于受到 произвольными异常损害的张量观察数据，OR-TLRR有可证明的性能保证，可以准确地恢复干净数据的列空间和检测异常值，只要异常值的干扰程度不太大。此外，论文还提出了处理缺失数据的扩展方法。最后，论文的实验结果表明了提议的算法的效果。
</details></li>
</ul>
<hr>
<h2 id="qecGPT-decoding-Quantum-Error-correcting-Codes-with-Generative-Pre-trained-Transformers"><a href="#qecGPT-decoding-Quantum-Error-correcting-Codes-with-Generative-Pre-trained-Transformers" class="headerlink" title="qecGPT: decoding Quantum Error-correcting Codes with Generative Pre-trained Transformers"></a>qecGPT: decoding Quantum Error-correcting Codes with Generative Pre-trained Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09025">http://arxiv.org/abs/2307.09025</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chy-i/qecgpt">https://github.com/chy-i/qecgpt</a></li>
<li>paper_authors: Hanyan Cao, Feng Pan, Yijia Wang, Pan Zhang</li>
<li>for: 提出了一个通用框架 для解码量子错误修正码，使用生成模型。</li>
<li>methods: 使用自然语言处理技术，特别是Transformers，学习逻辑运算和症状的联合概率。</li>
<li>results: 可以高效地计算逻辑运算的可能性，并直接生成最有可能的逻辑运算结果，计算复杂度为 $\mathcal O(2k)$，比传统最大可能性decoding算法要好。<details>
<summary>Abstract</summary>
We propose a general framework for decoding quantum error-correcting codes with generative modeling. The model utilizes autoregressive neural networks, specifically Transformers, to learn the joint probability of logical operators and syndromes. This training is in an unsupervised way, without the need for labeled training data, and is thus referred to as pre-training. After the pre-training, the model can efficiently compute the likelihood of logical operators for any given syndrome, using maximum likelihood decoding. It can directly generate the most-likely logical operators with computational complexity $\mathcal O(2k)$ in the number of logical qubits $k$, which is significantly better than the conventional maximum likelihood decoding algorithms that require $\mathcal O(4^k)$ computation. Based on the pre-trained model, we further propose refinement to achieve more accurately the likelihood of logical operators for a given syndrome by directly sampling the stabilizer operators. We perform numerical experiments on stabilizer codes with small code distances, using both depolarizing error models and error models with correlated noise. The results show that our approach provides significantly better decoding accuracy than the minimum weight perfect matching and belief-propagation-based algorithms. Our framework is general and can be applied to any error model and quantum codes with different topologies such as surface codes and quantum LDPC codes. Furthermore, it leverages the parallelization capabilities of GPUs, enabling simultaneous decoding of a large number of syndromes. Our approach sheds light on the efficient and accurate decoding of quantum error-correcting codes using generative artificial intelligence and modern computational power.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="U-shaped-Transformer-Retain-High-Frequency-Context-in-Time-Series-Analysis"><a href="#U-shaped-Transformer-Retain-High-Frequency-Context-in-Time-Series-Analysis" class="headerlink" title="U-shaped Transformer: Retain High Frequency Context in Time Series Analysis"></a>U-shaped Transformer: Retain High Frequency Context in Time Series Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09019">http://arxiv.org/abs/2307.09019</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingkui Chen, Yiqin Zhang</li>
<li>for: 本研究旨在增进时间序列预测领域中的 neural network 性能，通过综合利用 transformer 和 MLP 两种网络结构。</li>
<li>methods: 本研究采用了 skip-layer 连接和 patch merge 和 split 操作，以提高 transformer 的低频特征表示能力，并使用更大的数据集来充分利用 transformer 背景。</li>
<li>results: 实验结果表明，模型在多个数据集上表现出了高水平的性能，而且比 traditional transformer 更加高效。<details>
<summary>Abstract</summary>
Time series prediction plays a crucial role in various industrial fields. In recent years, neural networks with a transformer backbone have achieved remarkable success in many domains, including computer vision and NLP. In time series analysis domain, some studies have suggested that even the simplest MLP networks outperform advanced transformer-based networks on time series forecast tasks. However, we believe these findings indicate there to be low-rank properties in time series sequences. In this paper, we consider the low-pass characteristics of transformers and try to incorporate the advantages of MLP. We adopt skip-layer connections inspired by Unet into traditional transformer backbone, thus preserving high-frequency context from input to output, namely U-shaped Transformer. We introduce patch merge and split operation to extract features with different scales and use larger datasets to fully make use of the transformer backbone. Our experiments demonstrate that the model performs at an advanced level across multiple datasets with relatively low cost.
</details>
<details>
<summary>摘要</summary>
时间序列预测在各个产业领域发挥重要作用。近年来，基于 transformer 结构的神经网络在各个领域，如计算机视觉和自然语言处理，取得了很大成功。然而，一些研究表明，简单的多层感知网络可以在时间序列预测任务上超越高级 transformer 基于网络。我们认为这些发现反映了时间序列序列中的低级特性。在这篇论文中，我们考虑了 transformer 的低通Characteristics 和 MLP 网络的优点，并将它们结合在一起。我们采用了 skip-layer 连接，以保持输入到输出的高频上下文，即 U-shaped Transformer。我们还引入 patch 合并和拆分操作，以提取不同缩放的特征，并使用更大的数据集，以充分利用 transformer 脊梁。我们的实验表明，模型在多个数据集上表现出了高水平的性能，而且Relative low cost。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-LLMs-for-health-grounded-in-individual-specific-data"><a href="#Multimodal-LLMs-for-health-grounded-in-individual-specific-data" class="headerlink" title="Multimodal LLMs for health grounded in individual-specific data"></a>Multimodal LLMs for health grounded in individual-specific data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09018">http://arxiv.org/abs/2307.09018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anastasiya Belyaeva, Justin Cosentino, Farhad Hormozdiari, Krish Eswaran, Shravya Shetty, Greg Corrado, Andrew Carroll, Cory Y. McLean, Nicholas A. Furlotte</li>
<li>for: 这篇研究的目的是为了创建能够处理多种资料模式的大语言模型（LLMs），以解决各种领域中的问题，包括健康领域。</li>
<li>methods: 这篇研究使用了一个名为HeLM（Health Large Language Model for Multimodal Understanding）的框架，它可以将高维度的医疗资料与LLMs集成，以估计个人疾病风险。HeLM使用了一个Encoder来转换资料模式，将复杂的资料模式转换为LLM的token embedding空间，并将简单的资料模式转换为文本。</li>
<li>results: 根据UK Biobank的数据，HeLM可以有效地使用民生和医疗资料，以及高维度时间序列数据估计疾病风险。例如，HeLM在结合表格和呼吸图数据模式时，可以获得预测病例0.75的AUROC，比仅使用表格数据模式时的0.49要高。总的来说，HeLM在选择的八个二分类问题中，都能够超越或与класичного机器学习方法相等。此外，研究还评估了这个模型的扩展性和对个人健康和快速诊断的应用。<details>
<summary>Abstract</summary>
Foundation large language models (LLMs) have shown an impressive ability to solve tasks across a wide range of fields including health. To effectively solve personalized health tasks, LLMs need the ability to ingest a diversity of data modalities that are relevant to an individual's health status. In this paper, we take a step towards creating multimodal LLMs for health that are grounded in individual-specific data by developing a framework (HeLM: Health Large Language Model for Multimodal Understanding) that enables LLMs to use high-dimensional clinical modalities to estimate underlying disease risk. HeLM encodes complex data modalities by learning an encoder that maps them into the LLM's token embedding space and for simple modalities like tabular data by serializing the data into text. Using data from the UK Biobank, we show that HeLM can effectively use demographic and clinical features in addition to high-dimensional time-series data to estimate disease risk. For example, HeLM achieves an AUROC of 0.75 for asthma prediction when combining tabular and spirogram data modalities compared with 0.49 when only using tabular data. Overall, we find that HeLM outperforms or performs at parity with classical machine learning approaches across a selection of eight binary traits. Furthermore, we investigate the downstream uses of this model such as its generalizability to out-of-distribution traits and its ability to power conversations around individual health and wellness.
</details>
<details>
<summary>摘要</summary>
基于大语言模型（LLM）的研究表明，LLM可以在各种领域解决问题，包括健康领域。为了在个人化医疗方面有效地解决问题，LLM需要能够处理个人健康状况相关的多种数据类型。在这篇论文中，我们开发了一个框架（HeLM：健康大语言模型 для多模态理解），帮助LLM使用个人化数据来估计疾病风险。HeLM将复杂的数据类型编码为将其映射到LLM的符号空间中，而简单的数据类型则通过将数据序列化为文本来实现。使用UK Biobank数据，我们显示了HeLM可以有效地使用人口和临床特征以及高维时序数据来估计疾病风险。例如，当 combining 表格和气流数据模式时，HeLM的 AUC 为 0.75，而只使用表格数据时的 AUC 为 0.49。总的来说，我们发现 HeLM 在选择的八个二分类特征上表现出色，并且与传统机器学习方法相当或超过其表现。此外，我们还 investigate HeLM 的下游应用，包括其对非标型特征的普适性和在个人医疗和健康谈话中的应用能力。
</details></li>
</ul>
<hr>
<h2 id="PLiNIO-A-User-Friendly-Library-of-Gradient-based-Methods-for-Complexity-aware-DNN-Optimization"><a href="#PLiNIO-A-User-Friendly-Library-of-Gradient-based-Methods-for-Complexity-aware-DNN-Optimization" class="headerlink" title="PLiNIO: A User-Friendly Library of Gradient-based Methods for Complexity-aware DNN Optimization"></a>PLiNIO: A User-Friendly Library of Gradient-based Methods for Complexity-aware DNN Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09488">http://arxiv.org/abs/2307.09488</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniele Jahier Pagliari, Matteo Risso, Beatrice Alessandra Motetti, Alessio Burrello</li>
<li>for: 这篇论文主要是为了提供一个开源的深度神经网络设计自动化库（PLiNIO），来搭配各种渐渐变小的优化技术，以提高深度神经网络在紧缩边缘设备上的效能。</li>
<li>methods: 这篇论文使用了许多现代的深度神经网络设计自动化技术，包括预测精度估计、优化搜索、阶层优化、卷积优化等，并将这些技术集成到一个开源库中，提供了一个易用的用户界面。</li>
<li>results: 根据实验结果，PLiNIO可以实现优化深度神经网络的体积和精度，并且可以实现大约94.34%的内存减少，却只有&lt;1%的精度损失比基eline架构。<details>
<summary>Abstract</summary>
Accurate yet efficient Deep Neural Networks (DNNs) are in high demand, especially for applications that require their execution on constrained edge devices. Finding such DNNs in a reasonable time for new applications requires automated optimization pipelines since the huge space of hyper-parameter combinations is impossible to explore extensively by hand. In this work, we propose PLiNIO, an open-source library implementing a comprehensive set of state-of-the-art DNN design automation techniques, all based on lightweight gradient-based optimization, under a unified and user-friendly interface. With experiments on several edge-relevant tasks, we show that combining the various optimizations available in PLiNIO leads to rich sets of solutions that Pareto-dominate the considered baselines in terms of accuracy vs model size. Noteworthy, PLiNIO achieves up to 94.34% memory reduction for a <1% accuracy drop compared to a baseline architecture.
</details>
<details>
<summary>摘要</summary>
高效减少内存的深度神经网络（DNN）在边缘设备上的应用越来越受欢迎，特别是在有限的边缘设备上运行。手动搜索大量的超参数组合是不可能的，因此需要自动优化管道。在这种情况下，我们提出PLiNIO，一个开源库，实现了现代DNN设计自动化技术的总集，所有基于轻量级的梯度基于优化，通过统一和易用的界面进行实现。经过一些边缘相关任务的实验，我们发现，PLiNIO中的多种优化技术的组合可以生成高精度和轻量级的解决方案，与考虑的基准架构相比，PLiNIO可以实现94.34%的内存减少，仅带来<1%的准确率下降。
</details></li>
</ul>
<hr>
<h2 id="How-is-ChatGPT’s-behavior-changing-over-time"><a href="#How-is-ChatGPT’s-behavior-changing-over-time" class="headerlink" title="How is ChatGPT’s behavior changing over time?"></a>How is ChatGPT’s behavior changing over time?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09009">http://arxiv.org/abs/2307.09009</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lchen001/llmdrift">https://github.com/lchen001/llmdrift</a></li>
<li>paper_authors: Lingjiao Chen, Matei Zaharia, James Zou</li>
<li>for: 评估 GPT-3.5 和 GPT-4 两个大语言模型在不同时间点上的变化。</li>
<li>methods: 使用多种多样化任务评估 GPT-3.5 和 GPT-4 在不同时间点上的表现。</li>
<li>results: 发现 GPT-4 和 GPT-3.5 在不同时间点上的表现和行为可能会有很大的变化，如 prime vs. composite numbers 识别 task 中 GPT-4 (3月2023) 的表现比 GPT-4 (6月2023) 更好，但是 GPT-3.5 在6月的表现更好于3月。<details>
<summary>Abstract</summary>
GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on several diverse tasks: 1) math problems, 2) sensitive/dangerous questions, 3) opinion surveys, 4) multi-hop knowledge-intensive questions, 5) generating code, 6) US Medical License tests, and 7) visual reasoning. We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time. For example, GPT-4 (March 2023) was reasonable at identifying prime vs. composite numbers (84% accuracy) but GPT-4 (June 2023) was poor on these same questions (51% accuracy). This is partly explained by a drop in GPT-4's amenity to follow chain-of-thought prompting. Interestingly, GPT-3.5 was much better in June than in March in this task. GPT-4 became less willing to answer sensitive questions and opinion survey questions in June than in March. GPT-4 performed better at multi-hop questions in June than in March, while GPT-3.5's performance dropped on this task. Both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March. Overall, our findings show that the behavior of the "same" LLM service can change substantially in a relatively short amount of time, highlighting the need for continuous monitoring of LLMs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="OxfordVGG-Submission-to-the-EGO4D-AV-Transcription-Challenge"><a href="#OxfordVGG-Submission-to-the-EGO4D-AV-Transcription-Challenge" class="headerlink" title="OxfordVGG Submission to the EGO4D AV Transcription Challenge"></a>OxfordVGG Submission to the EGO4D AV Transcription Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09006">http://arxiv.org/abs/2307.09006</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/m-bain/whisperx">https://github.com/m-bain/whisperx</a></li>
<li>paper_authors: Jaesung Huh, Max Bain, Andrew Zisserman</li>
<li>for: 本研究报告提供了2023年EGO4D音频视觉自动语音识别挑战（AV-ASR）中oxfordvgg团队的技术细节。</li>
<li>methods: 本研究使用了WhisperX系统，用于高效处理长形audio的语音识别，并且使用了两个公开可用的文本标准化器。</li>
<li>results: 本研究在挑战测试集上取得56.0%的字误率（WER），排名第一名在排行板上。所有基eline代码和模型可以在<a target="_blank" rel="noopener" href="https://github.com/m-bain/whisperX%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/m-bain/whisperX上获取。</a><details>
<summary>Abstract</summary>
This report presents the technical details of our submission on the EGO4D Audio-Visual (AV) Automatic Speech Recognition Challenge 2023 from the OxfordVGG team. We present WhisperX, a system for efficient speech transcription of long-form audio with word-level time alignment, along with two text normalisers which are publicly available. Our final submission obtained 56.0% of the Word Error Rate (WER) on the challenge test set, ranked 1st on the leaderboard. All baseline codes and models are available on https://github.com/m-bain/whisperX.
</details>
<details>
<summary>摘要</summary>
这份报告介绍了我们在EGO4D Audio-Visual（AV）自动话语识别挑战2023中的提交技术细节，来自于牛津VGG团队。我们提出了一种名为WhisperX的高效语音转文本系统，以及两种公共可用的文本Normalizer。我们的最终提交在挑战测试集上达到56.0%的字SError率，排名第一名。所有基线代码和模型可以在https://github.com/m-bain/whisperX上下载。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Zero-shot-Domain-sensitive-Speech-Recognition-with-Prompt-conditioning-Fine-tuning"><a href="#Zero-shot-Domain-sensitive-Speech-Recognition-with-Prompt-conditioning-Fine-tuning" class="headerlink" title="Zero-shot Domain-sensitive Speech Recognition with Prompt-conditioning Fine-tuning"></a>Zero-shot Domain-sensitive Speech Recognition with Prompt-conditioning Fine-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10274">http://arxiv.org/abs/2307.10274</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mtkresearch/clairaudience">https://github.com/mtkresearch/clairaudience</a></li>
<li>paper_authors: Feng-Ting Liao, Yung-Chieh Chan, Yi-Chang Chen, Chan-Jan Hsu, Da-shan Shiu</li>
<li>for: 这 paper 是为了创建域专注的语音识别模型，利用文本域信息进行 Conditioning 生成。</li>
<li>methods: 这 paper 使用了精心调整的预训练、端到端模型（Whisper），通过示例示出学习域特定的示例来学习。</li>
<li>results: 这 paper 表明这种能力可以在不同的域和不同的示例上进行泛化，模型在未经见过的数据集上 achieve Word Error Rate (WER) 减少达 33%，并且通过文本Only fine-tuning 来实现域敏感和域适应。<details>
<summary>Abstract</summary>
In this work, we propose a method to create domain-sensitive speech recognition models that utilize textual domain information by conditioning its generation on a given text prompt. This is accomplished by fine-tuning a pre-trained, end-to-end model (Whisper) to learn from demonstrations with prompt examples. We show that this ability can be generalized to different domains and even various prompt contexts, with our model gaining a Word Error Rate (WER) reduction of up to 33% on unseen datasets from various domains, such as medical conversation, air traffic control communication, and financial meetings. Considering the limited availability of audio-transcript pair data, we further extend our method to text-only fine-tuning to achieve domain sensitivity as well as domain adaptation. We demonstrate that our text-only fine-tuned model can also attend to various prompt contexts, with the model reaching the most WER reduction of 29% on the medical conversation dataset.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们提出了一种方法，用于创建域特定的语音识别模型，该模型利用文本域信息进行conditioning。这是通过练化一个预训练的、端到端模型（Whisper）来学习示例示唆。我们表明这种能力可以泛化到不同域和不同提示上下文中，我们的模型在未seen datasets上 achieved Word Error Rate（WER）下降达33%。考虑到语音-讲本数据的有限可用性，我们进一步扩展了我们的方法，使其能够在文本只 fine-tuning中实现域敏感性和域适应性。我们示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示示
</details></li>
</ul>
<hr>
<h2 id="Oracle-Efficient-Online-Multicalibration-and-Omniprediction"><a href="#Oracle-Efficient-Online-Multicalibration-and-Omniprediction" class="headerlink" title="Oracle Efficient Online Multicalibration and Omniprediction"></a>Oracle Efficient Online Multicalibration and Omniprediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08999">http://arxiv.org/abs/2307.08999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sumegha Garg, Christopher Jung, Omer Reingold, Aaron Roth</li>
<li>for: 这种研究的目的是为了研究在在线对抗 Setting中的 omniprediction 算法，以及其与多calibration 的关系。</li>
<li>methods: 这种研究使用了多calibration 和 omniprediction 两种概念，以及一些学习理论的概念。</li>
<li>results: 这种研究得到了一种新的在线多calibration 算法，可以在无限 benchmark 类 $F$ 中进行定义，并且是 oracle 有效的（即对于任何类 $F$, 算法可以转化为一种有效的减少 regret 学习算法）。此外，这种算法还可以在 linear functions 类 $F$ 中进行有效的实现。此外，这种研究还提供了 upper 和 lower bounds，用于评估这种算法的性能。<details>
<summary>Abstract</summary>
A recent line of work has shown a surprising connection between multicalibration, a multi-group fairness notion, and omniprediction, a learning paradigm that provides simultaneous loss minimization guarantees for a large family of loss functions. Prior work studies omniprediction in the batch setting. We initiate the study of omniprediction in the online adversarial setting. Although there exist algorithms for obtaining notions of multicalibration in the online adversarial setting, unlike batch algorithms, they work only for small finite classes of benchmark functions $F$, because they require enumerating every function $f \in F$ at every round. In contrast, omniprediction is most interesting for learning theoretic hypothesis classes $F$, which are generally continuously large.   We develop a new online multicalibration algorithm that is well defined for infinite benchmark classes $F$, and is oracle efficient (i.e. for any class $F$, the algorithm has the form of an efficient reduction to a no-regret learning algorithm for $F$). The result is the first efficient online omnipredictor -- an oracle efficient prediction algorithm that can be used to simultaneously obtain no regret guarantees to all Lipschitz convex loss functions. For the class $F$ of linear functions, we show how to make our algorithm efficient in the worst case. Also, we show upper and lower bounds on the extent to which our rates can be improved: our oracle efficient algorithm actually promises a stronger guarantee called swap-omniprediction, and we prove a lower bound showing that obtaining $O(\sqrt{T})$ bounds for swap-omniprediction is impossible in the online setting. On the other hand, we give a (non-oracle efficient) algorithm which can obtain the optimal $O(\sqrt{T})$ omniprediction bounds without going through multicalibration, giving an information theoretic separation between these two solution concepts.
</details>
<details>
<summary>摘要</summary>
最近的研究表明了 Multicalibration 和 Omniprediction 之间的意外联系，Multicalibration 是一种多组公平性概念，Omniprediction 是一种学习 парадиг，可以同时提供多种损失函数下的极小损失 garantías。先前的研究主要关注 Omniprediction 在批处理 setting 中。我们开始研究 Omniprediction 在在线对抗 setting 中。 existing 算法可以在在线对抗 setting 中获得 Multicalibration 的概念，但它们只适用于小型固定的 benchmark function 集合 $F$，因为它们需要在每个轮次中列出所有函数 $f \in F$。与此相比，Omniprediction 更加 interesseting，因为它可以学习理论上的假设类 $F$，这些类通常是无限大的。我们开发了一个新的在线 Multicalibration 算法，可以对无限 benchmark function 集合 $F$进行定义，并且是oracle efficient（即对于任何类 $F$，算法有形式的减少到一个不失业的学习算法 для $F$）。结果是首个 oracle efficient 的在线 omnipredictor，可以同时提供对所有 Lipschitz 几何损失函数的 no-regret  garantías。对于 linear function 集合 $F$，我们说明了如何使我们的算法高效。此外，我们还提供了上下 bounds 的证明，其中我们的 oracle efficient 算法实际上承诺了更强的 guarantee called swap-omniprediction，并且我们证明了在在线 setting 中，不可能在 $O(\sqrt{T})$ 级别上获得 swap-omniprediction。相反，我们提供了一个（非oracle efficient）算法，可以在无需 Multicalibration 的情况下获得最佳 $O(\sqrt{T})$  omniprediction  bound。这提供了一种信息理论上的分离，证明了这两个解决方案之间的不同。
</details></li>
</ul>
<hr>
<h2 id="GraphCL-DTA-a-graph-contrastive-learning-with-molecular-semantics-for-drug-target-binding-affinity-prediction"><a href="#GraphCL-DTA-a-graph-contrastive-learning-with-molecular-semantics-for-drug-target-binding-affinity-prediction" class="headerlink" title="GraphCL-DTA: a graph contrastive learning with molecular semantics for drug-target binding affinity prediction"></a>GraphCL-DTA: a graph contrastive learning with molecular semantics for drug-target binding affinity prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08989">http://arxiv.org/abs/2307.08989</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinxing Yang, Genke Yang, Jian Chu</li>
<li>for: 预测药物与Target分子之间的吸附积分，以便在药物探索的初期阶段快速地评估新药的可能性。</li>
<li>methods: 我们提出了一种基于分子图的对比学习框架——GraphCL-DTA，通过这种框架，学习药物的分子图表示，保持分子图的 semantics。此外，我们还设计了一种新的损失函数，可以直接调整药物和目标表示的均匀性。</li>
<li>results: 我们在两个真实数据集（KIBA和Davis）上验证了GraphCL-DTA的效果，结果显示GraphCL-DTA在这些数据集上表现出色，较之前的状态艺模型而言，具有更高的准确率和更好的可靠性。<details>
<summary>Abstract</summary>
Drug-target binding affinity prediction plays an important role in the early stages of drug discovery, which can infer the strength of interactions between new drugs and new targets. However, the performance of previous computational models is limited by the following drawbacks. The learning of drug representation relies only on supervised data, without taking into account the information contained in the molecular graph itself. Moreover, most previous studies tended to design complicated representation learning module, while uniformity, which is used to measure representation quality, is ignored. In this study, we propose GraphCL-DTA, a graph contrastive learning with molecular semantics for drug-target binding affinity prediction. In GraphCL-DTA, we design a graph contrastive learning framework for molecular graphs to learn drug representations, so that the semantics of molecular graphs are preserved. Through this graph contrastive framework, a more essential and effective drug representation can be learned without additional supervised data. Next, we design a new loss function that can be directly used to smoothly adjust the uniformity of drug and target representations. By directly optimizing the uniformity of representations, the representation quality of drugs and targets can be improved. The effectiveness of the above innovative elements is verified on two real datasets, KIBA and Davis. The excellent performance of GraphCL-DTA on the above datasets suggests its superiority to the state-of-the-art model.
</details>
<details>
<summary>摘要</summary>
药Target绑定亲和力预测在药物发现的早期阶段具有重要的作用，可以推断新药和新目标之间的亲和力强度。然而，先前的计算模型的性能受以下缺点限制。学习药物表示 rely only on supervised data, without considering the information contained in the molecular graph itself. In addition, most previous studies have designed complicated representation learning modules, while the uniformity of the representation quality is ignored. In this study, we propose GraphCL-DTA, a graph contrastive learning with molecular semantics for drug-target binding affinity prediction. In GraphCL-DTA, we design a graph contrastive learning framework for molecular graphs to learn drug representations, so that the semantics of molecular graphs are preserved. Through this graph contrastive framework, a more essential and effective drug representation can be learned without additional supervised data. Next, we design a new loss function that can be directly used to smoothly adjust the uniformity of drug and target representations. By directly optimizing the uniformity of representations, the representation quality of drugs and targets can be improved. The effectiveness of the above innovative elements is verified on two real datasets, KIBA and Davis. The excellent performance of GraphCL-DTA on the above datasets suggests its superiority to the state-of-the-art model.
</details></li>
</ul>
<hr>
<h2 id="Neural-Network-Pruning-as-Spectrum-Preserving-Process"><a href="#Neural-Network-Pruning-as-Spectrum-Preserving-Process" class="headerlink" title="Neural Network Pruning as Spectrum Preserving Process"></a>Neural Network Pruning as Spectrum Preserving Process</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08982">http://arxiv.org/abs/2307.08982</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shibo Yao, Dantong Yu, Ioannis Koutis</li>
<li>for: 本研究旨在提出一种基于矩阵 спектル学习的神经网络减量方法，以提高神经网络在边缘设备上的运行效率。</li>
<li>methods: 本文使用矩阵 спектル学习来分析神经网络的训练过程，并提出一种基于矩阵减量的神经网络减量算法。</li>
<li>results: 实验结果表明，该算法可以更好地保留神经网络的重要参数，并提高神经网络在边缘设备上的运行效率。<details>
<summary>Abstract</summary>
Neural networks have achieved remarkable performance in various application domains. Nevertheless, a large number of weights in pre-trained deep neural networks prohibit them from being deployed on smartphones and embedded systems. It is highly desirable to obtain lightweight versions of neural networks for inference in edge devices. Many cost-effective approaches were proposed to prune dense and convolutional layers that are common in deep neural networks and dominant in the parameter space. However, a unified theoretical foundation for the problem mostly is missing. In this paper, we identify the close connection between matrix spectrum learning and neural network training for dense and convolutional layers and argue that weight pruning is essentially a matrix sparsification process to preserve the spectrum. Based on the analysis, we also propose a matrix sparsification algorithm tailored for neural network pruning that yields better pruning result. We carefully design and conduct experiments to support our arguments. Hence we provide a consolidated viewpoint for neural network pruning and enhance the interpretability of deep neural networks by identifying and preserving the critical neural weights.
</details>
<details>
<summary>摘要</summary>
In this paper, we explore the close connection between matrix spectrum learning and neural network training for dense and convolutional layers, and argue that weight pruning is essentially a matrix sparsification process to preserve the spectrum. Based on this analysis, we propose a matrix sparsification algorithm tailored for neural network pruning that yields better pruning results.We carefully design and conduct experiments to support our arguments, providing a consolidated viewpoint for neural network pruning and enhancing the interpretability of deep neural networks by identifying and preserving the critical neural weights.
</details></li>
</ul>
<hr>
<h2 id="A-Unifying-Framework-for-Differentially-Private-Sums-under-Continual-Observation"><a href="#A-Unifying-Framework-for-Differentially-Private-Sums-under-Continual-Observation" class="headerlink" title="A Unifying Framework for Differentially Private Sums under Continual Observation"></a>A Unifying Framework for Differentially Private Sums under Continual Observation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08970">http://arxiv.org/abs/2307.08970</a></li>
<li>repo_url: None</li>
<li>paper_authors: Monika Henzinger, Jalaj Upadhyay, Sarvagya Upadhyay</li>
<li>for: 本研究考虑了维护具有不同权重的汇总数据的权衡私钥性问题，即在不断观察时维护 differentially private 的汇总数据。</li>
<li>methods: 我们提出了一种通用框架和有效的算法来解决这个问题，其适用于任何足够平滑的函数。我们的算法是首个不具有多项式误差的权衡私钥性汇总数据算法。</li>
<li>results: 我们的算法可以在不断观察时维护 differentially private 的汇总数据，并且可以 precisley  recover continual counting 问题中的误差（ Henzinger et al., SODA 2023）。我们的算法基于因子化机制，其误差取决于underlying matrix的 $\gamma_2$ 和 $\gamma_F$ 范数。我们给出了一个可构造的证明，证明了 $\gamma_2$ 和 $\gamma_F$ 范数的上界和下界。这是首次不同于所有非零元素都是相同的lower-triangular矩阵下的非тиrivial下界。<details>
<summary>Abstract</summary>
We study the problem of maintaining a differentially private decaying sum under continual observation. We give a unifying framework and an efficient algorithm for this problem for \emph{any sufficiently smooth} function. Our algorithm is the first differentially private algorithm that does not have a multiplicative error for polynomially-decaying weights. Our algorithm improves on all prior works on differentially private decaying sums under continual observation and recovers exactly the additive error for the special case of continual counting from Henzinger et al. (SODA 2023) as a corollary.   Our algorithm is a variant of the factorization mechanism whose error depends on the $\gamma_2$ and $\gamma_F$ norm of the underlying matrix. We give a constructive proof for an almost exact upper bound on the $\gamma_2$ and $\gamma_F$ norm and an almost tight lower bound on the $\gamma_2$ norm for a large class of lower-triangular matrices. This is the first non-trivial lower bound for lower-triangular matrices whose non-zero entries are not all the same. It includes matrices for all continual decaying sums problems, resulting in an upper bound on the additive error of any differentially private decaying sums algorithm under continual observation.   We also explore some implications of our result in discrepancy theory and operator algebra. Given the importance of the $\gamma_2$ norm in computer science and the extensive work in mathematics, we believe our result will have further applications.
</details>
<details>
<summary>摘要</summary>
我们研究维护具有泛化隐私衰减的总和问题在不断观察下。我们提供一个统一的框架和高效的算法来解决这个问题，该算法适用于任何足够光滑的函数。我们的算法是第一个不具有多项式误差的泛化隐私总和算法。我们的算法超越了所有以前的泛化隐私总和算法，并在特殊情况下 recover exactly  Henzinger et al.（SODA 2023）中的添加误差。我们的算法是一种因子化机制的变体，其误差取决于underlying矩阵的$\gamma_2$和$\gamma_F$范数。我们提供了一种可构造的证明，证明了一个非常接近的上界和下界，其中下界适用于一类lower-triangular矩阵。这是第一个不同于所有非零非同样的非零元素的下三角矩阵的下界。它包括所有不断观察下的总和问题矩阵，从而得到了任何泛化隐私总和算法的添加误差的Upper bound。我们还探讨了我们结果在不同误差理论和运算代数方面的应用。由于计算机科学中的$\gamma_2$范数的重要性以及数学领域的广泛工作，我们认为我们的结果将有更多应用。
</details></li>
</ul>
<hr>
<h2 id="AutoAlign-Fully-Automatic-and-Effective-Knowledge-Graph-Alignment-enabled-by-Large-Language-Models"><a href="#AutoAlign-Fully-Automatic-and-Effective-Knowledge-Graph-Alignment-enabled-by-Large-Language-Models" class="headerlink" title="AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment enabled by Large Language Models"></a>AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment enabled by Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11772">http://arxiv.org/abs/2307.11772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Zhang, Yixin Su, Bayu Distiawan Trisedya, Xiaoyan Zhao, Min Yang, Hong Cheng, Jianzhong Qi</li>
<li>For: The paper is written for the task of entity alignment between knowledge graphs (KGs), specifically proposing a fully automatic method that does not require manually crafted seed alignments.* Methods: The method proposed in the paper is called AutoAlign, which uses predicate embeddings and entity embeddings to align entities between two KGs. Specifically, AutoAlign constructs a predicate-proximity-graph with the help of large language models to automatically capture the similarity between predicates across two KGs, and shifts the two KGs’ entity embeddings into the same vector space by computing the similarity between entities based on their attributes.* Results: The paper reports that AutoAlign improves the performance of entity alignment significantly compared to state-of-the-art methods, as demonstrated through experiments using real-world KGs.<details>
<summary>Abstract</summary>
The task of entity alignment between knowledge graphs (KGs) aims to identify every pair of entities from two different KGs that represent the same entity. Many machine learning-based methods have been proposed for this task. However, to our best knowledge, existing methods all require manually crafted seed alignments, which are expensive to obtain. In this paper, we propose the first fully automatic alignment method named AutoAlign, which does not require any manually crafted seed alignments. Specifically, for predicate embeddings, AutoAlign constructs a predicate-proximity-graph with the help of large language models to automatically capture the similarity between predicates across two KGs. For entity embeddings, AutoAlign first computes the entity embeddings of each KG independently using TransE, and then shifts the two KGs' entity embeddings into the same vector space by computing the similarity between entities based on their attributes. Thus, both predicate alignment and entity alignment can be done without manually crafted seed alignments. AutoAlign is not only fully automatic, but also highly effective. Experiments using real-world KGs show that AutoAlign improves the performance of entity alignment significantly compared to state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
知识 graphs (KGs) 的实体对应问题的任务是将两个不同 KGs 中的实体对应到同一个实体。许多机器学习基于方法已经被提出来解决这个问题。然而，我们所知道的是，现有的方法都需要手动制作的种子对应，这是贵重的。在这篇论文中，我们提出了第一个完全自动对应方法，名为 AutoAlign，不需要任何手动制作的种子对应。特别是，对于 predicate 嵌入，AutoAlign 使用大型自然语言模型来自动捕捉两个 KGs 中 predicate 之间的相似性。对于实体嵌入，AutoAlign 先使用 TransE 来独立计算每个 KG 中的实体嵌入，然后通过计算实体之间的属性相似性来将两个 KGs 的实体嵌入Shift到同一个向量空间中。因此， predicate 对应和实体对应都可以不需要手动制作种子对应。AutoAlign 不仅是完全自动的，还非常有效。使用实际世界 KGs 的实验表明，AutoAlign 在对entity alignment进行比对state-of-the-art方法时有显著改进。
</details></li>
</ul>
<hr>
<h2 id="Landscape-Surrogate-Learning-Decision-Losses-for-Mathematical-Optimization-Under-Partial-Information"><a href="#Landscape-Surrogate-Learning-Decision-Losses-for-Mathematical-Optimization-Under-Partial-Information" class="headerlink" title="Landscape Surrogate: Learning Decision Losses for Mathematical Optimization Under Partial Information"></a>Landscape Surrogate: Learning Decision Losses for Mathematical Optimization Under Partial Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08964">http://arxiv.org/abs/2307.08964</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/lancer">https://github.com/facebookresearch/lancer</a></li>
<li>paper_authors: Arman Zharmagambetov, Brandon Amos, Aaron Ferber, Taoan Huang, Bistra Dilkina, Yuandong Tian</li>
<li>for: 优化问题中的部分观察或通用优化器表现不佳，学习一个优化器 $\mathbf{g}$ 来解决这些问题，可以快速加速优化过程，并且可以利用过去经验。</li>
<li>methods: 使用一个可学习的地形代理 $M$ 来取代 $f\circ \mathbf{g}$，这个地形代理可以更快速地计算，提供稠密和平滑的梯度，可以泛化到未看到的优化问题，并通过分布式优化来高效地学习。</li>
<li>results: 在 sintetic 问题和实际问题上测试了我们的方法，比如短路和多维零链包，和股票配置优化，与状态 искусственный基线相比，我们的方法可以达到相同或更高的目标值，同时减少了对 $\mathbf{g}$ 的调用数量。特别是，我们的方法在高维ensional computationally expensive 问题上表现出色。<details>
<summary>Abstract</summary>
Recent works in learning-integrated optimization have shown promise in settings where the optimization problem is only partially observed or where general-purpose optimizers perform poorly without expert tuning. By learning an optimizer $\mathbf{g}$ to tackle these challenging problems with $f$ as the objective, the optimization process can be substantially accelerated by leveraging past experience. The optimizer can be trained with supervision from known optimal solutions or implicitly by optimizing the compound function $f\circ \mathbf{g}$. The implicit approach may not require optimal solutions as labels and is capable of handling problem uncertainty; however, it is slow to train and deploy due to frequent calls to optimizer $\mathbf{g}$ during both training and testing. The training is further challenged by sparse gradients of $\mathbf{g}$, especially for combinatorial solvers. To address these challenges, we propose using a smooth and learnable Landscape Surrogate $M$ as a replacement for $f\circ \mathbf{g}$. This surrogate, learnable by neural networks, can be computed faster than the solver $\mathbf{g}$, provides dense and smooth gradients during training, can generalize to unseen optimization problems, and is efficiently learned via alternating optimization. We test our approach on both synthetic problems, including shortest path and multidimensional knapsack, and real-world problems such as portfolio optimization, achieving comparable or superior objective values compared to state-of-the-art baselines while reducing the number of calls to $\mathbf{g}$. Notably, our approach outperforms existing methods for computationally expensive high-dimensional problems.
</details>
<details>
<summary>摘要</summary>
现代学习整合优化技术已经在部分 observable 的优化问题或通用优化器无需专家调整时表现出了搭配性。通过学习一个优化器 $\mathbf{g}$，以便在 $f$ 作为目标函数下解决这些复杂的问题，优化过程可以大幅加速。可以通过知识到的优化解决方案或间接地将 $f\circ \mathbf{g}$ 作为整合函数来训练优化器。 indirect approach 可能不需要优化解决方案作为标签，并且可以处理问题不确定性，但是它在训练和部署时间较慢，因为在训练和测试中频繁地调用优化器 $\mathbf{g}$。训练还面临着 $\mathbf{g}$ 的稀疏梯度问题，特别是对 combinatorial 解决器。为解决这些挑战，我们提出使用一个缓和可学习的景观准则 $M$，作为 $f\circ \mathbf{g}$ 的替代品。这个准则可以通过神经网络学习，在训练时间更快，提供稠密和平滑的梯度，可以泛化到未见优化问题，并通过相关优化来有效地学习。我们在 sintetic 问题和实际问题上进行了测试，包括短路和多维锦包，并取得了与状态艺术基准相同或更高的目标值，同时减少了对 $\mathbf{g}$ 的调用数量。特别是，我们的方法在高维计算成本高的问题上表现出色。
</details></li>
</ul>
<hr>
<h2 id="REX-Rapid-Exploration-and-eXploitation-for-AI-Agents"><a href="#REX-Rapid-Exploration-and-eXploitation-for-AI-Agents" class="headerlink" title="REX: Rapid Exploration and eXploitation for AI Agents"></a>REX: Rapid Exploration and eXploitation for AI Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08962">http://arxiv.org/abs/2307.08962</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rithesh Murthy, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Le Xue, Weiran Yao, Yihao Feng, Zeyuan Chen, Akash Gokul, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, Silvio Savarese</li>
<li>for: 提高AI代理的快速探索和尝试能力，解决现有AutoGPT风格技术的缺陷，如偏重精确描述的决策和缺乏有系统的尝试和失败处理方式。</li>
<li>methods: 提出一种增强的Rapid Exploration and eXploitation（REX）方法，通过添加奖励层和基于Upper Confidence Bound（UCB）的概念，使AI代理性能更加稳定和高效。REX方法不需要模型细化，可以利用日志数据，并与现有基础模型协作无缝。</li>
<li>results:  Comparative analysis表明，使用REX方法可以与现有方法（如Chain-of-Thoughts（CoT）和Reasoning viA Planning（RAP））相比，在一些情况下甚至超越其表现，同时具有显著减少执行时间的优点，提高了在多样化场景下的实际应用性。<details>
<summary>Abstract</summary>
In this paper, we propose an enhanced approach for Rapid Exploration and eXploitation for AI Agents called REX. Existing AutoGPT-style techniques have inherent limitations, such as a heavy reliance on precise descriptions for decision-making, and the lack of a systematic approach to leverage try-and-fail procedures akin to traditional Reinforcement Learning (RL). REX introduces an additional layer of rewards and integrates concepts similar to Upper Confidence Bound (UCB) scores, leading to more robust and efficient AI agent performance. This approach has the advantage of enabling the utilization of offline behaviors from logs and allowing seamless integration with existing foundation models while it does not require any model fine-tuning. Through comparative analysis with existing methods such as Chain-of-Thoughts(CoT) and Reasoning viA Planning(RAP), REX-based methods demonstrate comparable performance and, in certain cases, even surpass the results achieved by these existing techniques. Notably, REX-based methods exhibit remarkable reductions in execution time, enhancing their practical applicability across a diverse set of scenarios.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种改进后的快速探索和努力（Rapid Exploration and eXploitation，REX）方法，用于AI代理。现有的AutoGPT样式技术存在一定的限制，如准确描述的重要性和RL的缺乏系统化适应。REX增加了一层奖励，并将Upper Confidence Bound（UCB）类概念纳入方法中，从而使AI代理表现更加稳定和高效。这种方法具有使用日志中的假日行为，无需模型细化而可以与现有基础模型集成，并且在比较分析中与现有方法（Chain-of-Thoughts（CoT）和Reasoning viA Planning（RAP）） demonstrate了相似或者甚至超过其表现。尤其是，REX基本方法在执行时间方面具有显著的减少，使其在多样化的情况下更加实用。
</details></li>
</ul>
<hr>
<h2 id="Discretization-based-ensemble-model-for-robust-learning-in-IoT"><a href="#Discretization-based-ensemble-model-for-robust-learning-in-IoT" class="headerlink" title="Discretization-based ensemble model for robust learning in IoT"></a>Discretization-based ensemble model for robust learning in IoT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08955">http://arxiv.org/abs/2307.08955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anahita Namvar, Chandra Thapa, Salil S. Kanhere</li>
<li>for: 提高 IoT 设备识别模型的安全性，抵御黑盒和白盒攻击。</li>
<li>methods:  integrate discretization techniques and ensemble methods to improve the robustness of machine learning models for IoT device identification.</li>
<li>results: 提高了 ML 模型对 IoT 设备识别的可靠性和安全性，抵御了黑盒和白盒攻击。<details>
<summary>Abstract</summary>
IoT device identification is the process of recognizing and verifying connected IoT devices to the network. This is an essential process for ensuring that only authorized devices can access the network, and it is necessary for network management and maintenance. In recent years, machine learning models have been used widely for automating the process of identifying devices in the network. However, these models are vulnerable to adversarial attacks that can compromise their accuracy and effectiveness. To better secure device identification models, discretization techniques enable reduction in the sensitivity of machine learning models to adversarial attacks contributing to the stability and reliability of the model. On the other hand, Ensemble methods combine multiple heterogeneous models to reduce the impact of remaining noise or errors in the model. Therefore, in this paper, we integrate discretization techniques and ensemble methods and examine it on model robustness against adversarial attacks. In other words, we propose a discretization-based ensemble stacking technique to improve the security of our ML models. We evaluate the performance of different ML-based IoT device identification models against white box and black box attacks using a real-world dataset comprised of network traffic from 28 IoT devices. We demonstrate that the proposed method enables robustness to the models for IoT device identification.
</details>
<details>
<summary>摘要</summary>
互联网物联网设备识别是将连接到网络的互联网设备识别和验证的过程。这是确保只允许授权的设备访问网络的 essencial 过程，并对网络管理和维护是必需的。在过去几年中，机器学习模型广泛用于自动化网络中设备识别的过程。然而，这些模型容易受到恶意攻击的影响，这可能会降低其精度和有效性。为了更好地安全设备识别模型，精度技术可以减少机器学习模型对恶意攻击的敏感度，从而提高模型的稳定性和可靠性。此外，组合多种不同的模型可以减少剩下的噪音或错误的影响。因此，在这篇论文中，我们将精度技术和组合方法结合使用，并对其在模型对恶意攻击的Robustness进行评估。即我们提出了一种基于精度的集成堆叠技术，以提高互联网设备识别模型的安全性。我们使用了一个实际的网络流量数据集，包含28个互联网设备的网络流量，对不同的机器学习基于互联网设备识别模型进行了白盒和黑盒攻击的评估。我们的结果表明，我们的提议的方法可以提高互联网设备识别模型的Robustness。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-infused-Deep-Learning-Enables-Interpretable-Landslide-Forecasting"><a href="#Knowledge-infused-Deep-Learning-Enables-Interpretable-Landslide-Forecasting" class="headerlink" title="Knowledge-infused Deep Learning Enables Interpretable Landslide Forecasting"></a>Knowledge-infused Deep Learning Enables Interpretable Landslide Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08951">http://arxiv.org/abs/2307.08951</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengjing Ma, Gang Mei</li>
<li>for: 预测山崩发展和失败的可能性是一项复杂的任务，因为它们受到许多内部和外部因素的影响。</li>
<li>methods: 这篇文章使用了一种名为LFIT的转换器基本深度学习网络，该网络可以学习非线性关系，并且具有可读性和多源数据处理能力。</li>
<li>results: 文章表明，通过结合先前知识，可以提高整体山崩预测，并且可以捕捉不同地区的山崩行为和时间模式。通过使用塑形变形观测数据，文章验证了该方法的可靠性和可读性。<details>
<summary>Abstract</summary>
Forecasting how landslides will evolve over time or whether they will fail is a challenging task due to a variety of factors, both internal and external. Despite their considerable potential to address these challenges, deep learning techniques lack interpretability, undermining the credibility of the forecasts they produce. The recent development of transformer-based deep learning offers untapped possibilities for forecasting landslides with unprecedented interpretability and nonlinear feature learning capabilities. Here, we present a deep learning pipeline that is capable of predicting landslide behavior holistically, which employs a transformer-based network called LFIT to learn complex nonlinear relationships from prior knowledge and multiple source data, identifying the most relevant variables, and demonstrating a comprehensive understanding of landslide evolution and temporal patterns. By integrating prior knowledge, we provide improvement in holistic landslide forecasting, enabling us to capture diverse responses to various influencing factors in different local landslide areas. Using deformation observations as proxies for measuring the kinetics of landslides, we validate our approach by training models to forecast reservoir landslides in the Three Gorges Reservoir and creeping landslides on the Tibetan Plateau. When prior knowledge is incorporated, we show that interpretable landslide forecasting effectively identifies influential factors across various landslides. It further elucidates how local areas respond to these factors, making landslide behavior and trends more interpretable and predictable. The findings from this study will contribute to understanding landslide behavior in a new way and make the proposed approach applicable to other complex disasters influenced by internal and external factors in the future.
</details>
<details>
<summary>摘要</summary>
预测滑坡的发展趋势或是否会失败是一项复杂的任务，因为它们受到多种内部和外部因素的影响。 DESPITE THEIR POTENTIAL TO ADDRESS THESE CHALLENGES, deep learning techniques lack interpretability, which undermines the credibility of the forecasts they produce.  However, the recent development of transformer-based deep learning offers untapped possibilities for forecasting landslides with unprecedented interpretability and nonlinear feature learning capabilities. 在这种情况下，我们提出了一个深度学习管道，可以捕捉滑坡的行为的整体特征，该管道使用名为LFIT的变换器基于网络，可以学习复杂的非线性关系，并且可以确定最重要的变量。通过结合先前知识，我们提供了改进的总体滑坡预测方法，可以捕捉不同的滑坡区域响应不同的外部因素的多样化响应。使用塑形观察作为滑坡动力的代理，我们验证了我们的方法，通过在三峡水库和藏北高原的滑坡中训练模型，预测滑坡的发展趋势。当嵌入先前知识时，我们显示出可解释的滑坡预测方法可以准确地确定影响滑坡的因素，并且可以解释不同的滑坡区域如何响应这些因素，使滑坡行为和趋势更加可解释和预测。这些发现将在未来对其他复杂的自然灾害，受到内部和外部因素影响的灾害中应用。
</details></li>
</ul>
<hr>
<h2 id="Alioth-A-Machine-Learning-Based-Interference-Aware-Performance-Monitor-for-Multi-Tenancy-Applications-in-Public-Cloud"><a href="#Alioth-A-Machine-Learning-Based-Interference-Aware-Performance-Monitor-for-Multi-Tenancy-Applications-in-Public-Cloud" class="headerlink" title="Alioth: A Machine Learning Based Interference-Aware Performance Monitor for Multi-Tenancy Applications in Public Cloud"></a>Alioth: A Machine Learning Based Interference-Aware Performance Monitor for Multi-Tenancy Applications in Public Cloud</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08949">http://arxiv.org/abs/2307.08949</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sthowling/alioth">https://github.com/sthowling/alioth</a></li>
<li>paper_authors: Tianyao Shi, Yingxuan Yang, Yunlong Cheng, Xiaofeng Gao, Zhen Fang, Yongqiang Yang<br>for: This paper aims to monitor the performance degradation of cloud applications in public clouds caused by co-location interference.methods: The proposed method, Alioth, uses a novel machine learning framework that includes interference generators, denoising auto-encoders, domain adaptation neural networks, and SHAP explainers to monitor performance degradation.results: Alioth achieves an average mean absolute error of 5.29% offline and 10.8% when testing on applications unseen in the training stage, outperforming baseline methods. It also demonstrates robustness in signaling quality-of-service violation under dynamicity.<details>
<summary>Abstract</summary>
Multi-tenancy in public clouds may lead to co-location interference on shared resources, which possibly results in performance degradation of cloud applications. Cloud providers want to know when such events happen and how serious the degradation is, to perform interference-aware migrations and alleviate the problem. However, virtual machines (VM) in Infrastructure-as-a-Service public clouds are black-boxes to providers, where application-level performance information cannot be acquired. This makes performance monitoring intensely challenging as cloud providers can only rely on low-level metrics such as CPU usage and hardware counters.   We propose a novel machine learning framework, Alioth, to monitor the performance degradation of cloud applications. To feed the data-hungry models, we first elaborate interference generators and conduct comprehensive co-location experiments on a testbed to build Alioth-dataset which reflects the complexity and dynamicity in real-world scenarios. Then we construct Alioth by (1) augmenting features via recovering low-level metrics under no interference using denoising auto-encoders, (2) devising a transfer learning model based on domain adaptation neural network to make models generalize on test cases unseen in offline training, and (3) developing a SHAP explainer to automate feature selection and enhance model interpretability. Experiments show that Alioth achieves an average mean absolute error of 5.29% offline and 10.8% when testing on applications unseen in the training stage, outperforming the baseline methods. Alioth is also robust in signaling quality-of-service violation under dynamicity. Finally, we demonstrate a possible application of Alioth's interpretability, providing insights to benefit the decision-making of cloud operators. The dataset and code of Alioth have been released on GitHub.
</details>
<details>
<summary>摘要</summary>
多重租户在公共云上可能导致共享资源上的干扰，从而导致云应用程序的性能下降。云提供商希望在这些事件发生时知道其严重程度，以进行干扰意识的迁移和缓解问题。然而，基础设施协议（IaaS）云公共云中的虚拟机（VM）对提供商是黑洞，无法获得应用层性能信息。这使得性能监测变得非常困难，cloud提供商只能依靠低级别指标，如CPU使用率和硬件计数器。我们提出了一种新的机器学习框架，称为Alioth，用于监测云应用程序的性能下降。为了充实数据鲁棒的模型，我们首先 elaborated interference generators和在testbed上进行了广泛的共享实验，以建立Alioth-dataset，该集合反映了实际场景中的复杂性和动态性。然后，我们构建了Alioth，包括以下三个主要组成部分：1. 通过使用降噪自适应神经网络恢复低级别指标，扩展特征。2. 基于域 adapted神经网络模型，以便模型在测试 случаeschannel unseen in offline training中 generalize。3. 开发 SHAP解释器，以自动选择特征和提高模型解释性。实验表明，Alioth在线上和离线上的平均绝对误差为5.29%和10.8% respectively，比基eline方法优化。此外，Alioth在动态环境下也具有可靠的质量服务预测能力。最后，我们示出了Alioth的解释性可以为云运维师提供有价值的决策指导。Alioth-dataset和相关代码已经在GitHub上发布。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Label-Bias-via-Decoupled-Confident-Learning"><a href="#Mitigating-Label-Bias-via-Decoupled-Confident-Learning" class="headerlink" title="Mitigating Label Bias via Decoupled Confident Learning"></a>Mitigating Label Bias via Decoupled Confident Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08945">http://arxiv.org/abs/2307.08945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunyi Li, Maria De-Arteaga, Maytal Saar-Tsechansky</li>
<li>for: 本研究旨在提出一种特点是适应标签偏见的分类方法，以便在涉及重要领域中减少算法偏见。</li>
<li>methods: 本研究提出了一种名为分离信心学习（DeCoLe）的遮盾方法，该方法可以减少标签偏见的影响。</li>
<li>results: 在一个Synthetic数据集上测试了DeCoLe方法，结果显示其能够成功地检测出偏见标签，并且在仇恨言语识别任务中超过其他方法表现。<details>
<summary>Abstract</summary>
Growing concerns regarding algorithmic fairness have led to a surge in methodologies to mitigate algorithmic bias. However, such methodologies largely assume that observed labels in training data are correct. This is problematic because bias in labels is pervasive across important domains, including healthcare, hiring, and content moderation. In particular, human-generated labels are prone to encoding societal biases. While the presence of labeling bias has been discussed conceptually, there is a lack of methodologies to address this problem. We propose a pruning method -- Decoupled Confident Learning (DeCoLe) -- specifically designed to mitigate label bias. After illustrating its performance on a synthetic dataset, we apply DeCoLe in the context of hate speech detection, where label bias has been recognized as an important challenge, and show that it successfully identifies biased labels and outperforms competing approaches.
</details>
<details>
<summary>摘要</summary>
algorithmic fairness的问题在不断增加，这导致了一系列方法来减少算法偏见。然而，这些方法假设训练数据中的标签是正确的。这是一个问题，因为标签中的偏见是广泛存在的，包括医疗、招聘和内容审核等重要领域。人类生成的标签容易带有社会偏见。虽然标签偏见的存在已经被讨论，但是没有有效的方法来解决这个问题。我们提出了一种剪裁方法——分离信任学习（DeCoLe），特意设计来减少标签偏见。我们在一个 sintetic 数据集上验证了 DeCoLe 的性能，然后在仇恨言语检测中应用了 DeCoLe，并证明它成功地标识了偏见标签，并超过了竞争方法的性能。
</details></li>
</ul>
<hr>
<h2 id="Siamese-Networks-for-Weakly-Supervised-Human-Activity-Recognition"><a href="#Siamese-Networks-for-Weakly-Supervised-Human-Activity-Recognition" class="headerlink" title="Siamese Networks for Weakly Supervised Human Activity Recognition"></a>Siamese Networks for Weakly Supervised Human Activity Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08944">http://arxiv.org/abs/2307.08944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taoran Sheng, Manfred Huber</li>
<li>for: 这篇论文旨在应用深度学习于人体活动识别，但是训练深度神经网络需要大量的明确标注数据，这是困难的获得。</li>
<li>methods: 这篇论文提出了一种使用多个同构网络进行训练，只使用数据对的相似性信息来训练模型，从而生成一个可以作为各种各样的聚类算法的度量模型。</li>
<li>results: 论文在三个数据集上进行了评估，并证明了模型的效果iveness在分类和识别连续人体活动序列中。<details>
<summary>Abstract</summary>
Deep learning has been successfully applied to human activity recognition. However, training deep neural networks requires explicitly labeled data which is difficult to acquire. In this paper, we present a model with multiple siamese networks that are trained by using only the information about the similarity between pairs of data samples without knowing the explicit labels. The trained model maps the activity data samples into fixed size representation vectors such that the distance between the vectors in the representation space approximates the similarity of the data samples in the input space. Thus, the trained model can work as a metric for a wide range of different clustering algorithms. The training process minimizes a similarity loss function that forces the distance metric to be small for pairs of samples from the same kind of activity, and large for pairs of samples from different kinds of activities. We evaluate the model on three datasets to verify its effectiveness in segmentation and recognition of continuous human activity sequences.
</details>
<details>
<summary>摘要</summary>
深度学习已成功应用于人类活动识别。然而，训练深度神经网络需要明确标注的数据，具体来说是困难的获得。在这篇论文中，我们提出了一种使用多个同构网络进行训练，不需要明确标注数据。训练模型将活动数据样本映射到固定大小的表示向量中，使得表示空间中的距离 approximates 输入空间中的相似性。因此，训练模型可以作为各种不同的聚类算法的度量。训练过程中 minimizes 一个相似损失函数，该函数让距离度量在同类活动样本对应的情况下很小，并在不同类活动样本对应的情况下很大。我们在三个数据集上验证了模型的有效性，以确认其在连续人类活动序列的分割和识别方面的表现。
</details></li>
</ul>
<hr>
<h2 id="NTK-approximating-MLP-Fusion-for-Efficient-Language-Model-Fine-tuning"><a href="#NTK-approximating-MLP-Fusion-for-Efficient-Language-Model-Fine-tuning" class="headerlink" title="NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning"></a>NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08941">http://arxiv.org/abs/2307.08941</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/weitianxin/mlp_fusion">https://github.com/weitianxin/mlp_fusion</a></li>
<li>paper_authors: Tianxin Wei, Zeming Guo, Yifan Chen, Jingrui He</li>
<li>for: 这篇论文旨在提出一种通过NTK数据来实现简化预训练语言模型（PLM）的方法，以减少PLM的计算和内存需求。</li>
<li>methods: 本文使用NTK几何来检查PLM的多层感知器（MLP）模组，并提出一种通过将MLP装置为一些中心的组合来实现轻量级PLM的方法。</li>
<li>results: 实验结果显示，该方法可以实现PLM的简化，并在自然语言理解（NLU）和生成（NLG）任务上进行了有效的调整。<details>
<summary>Abstract</summary>
Fine-tuning a pre-trained language model (PLM) emerges as the predominant strategy in many natural language processing applications. However, even fine-tuning the PLMs and doing inference are expensive, especially on edge devices with low computing power. Some general approaches (e.g. quantization and distillation) have been widely studied to reduce the compute/memory of PLM fine-tuning, while very few one-shot compression techniques are explored. In this paper, we investigate the neural tangent kernel (NTK)--which reveals the gradient descent dynamics of neural networks--of the multilayer perceptrons (MLP) modules in a PLM and propose to coin a lightweight PLM through NTK-approximating MLP fusion. To achieve this, we reconsider the MLP as a bundle of sub-MLPs, and cluster them into a given number of centroids, which can then be restored as a compressed MLP and surprisingly shown to well approximate the NTK of the original PLM. Extensive experiments of PLM fine-tuning on both natural language understanding (NLU) and generation (NLG) tasks are provided to verify the effectiveness of the proposed method MLP fusion. Our code is available at https://github.com/weitianxin/MLP_Fusion.
</details>
<details>
<summary>摘要</summary>
大多数自然语言处理应用中出现了调整预训练语言模型（PLM）的方法。然而，即使是调整PLM和做出判断都是昂贵的，特别是在边缘设备上进行。一些通用的方法（如量化和液化）已经广泛研究以降低PLM调整的计算/存储量，而很少有一次性压缩技术被探索。在这篇论文中，我们研究了神经积分析（NTK）——描述神经网络的梯度下降动力学——的多层感知器（MLP）模块在PLM中，并提议通过NTK-近似MLP融合来创造轻量级PLM。为此，我们重新考虑MLP为一个分解成多个子MLP的Bundle，并将其分成一定数量的中心点，然后可以将其Restore为压缩MLP，并意外地发现可以良好地近似原PLM的NTK。我们提供了大量PLM精度调整NLU和NLG任务的实验来证明提议的方法的有效性。我们的代码可以在https://github.com/weitianxin/MLP_Fusion上找到。
</details></li>
</ul>
<hr>
<h2 id="Experimental-Security-Analysis-of-DNN-based-Adaptive-Cruise-Control-under-Context-Aware-Perception-Attacks"><a href="#Experimental-Security-Analysis-of-DNN-based-Adaptive-Cruise-Control-under-Context-Aware-Perception-Attacks" class="headerlink" title="Experimental Security Analysis of DNN-based Adaptive Cruise Control under Context-Aware Perception Attacks"></a>Experimental Security Analysis of DNN-based Adaptive Cruise Control under Context-Aware Perception Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08939">http://arxiv.org/abs/2307.08939</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xugui Zhou, Anqi Chen, Maxfield Kouzel, Haotian Ren, Morgan McCarty, Cristina Nita-Rotaru, Homa Alemzadeh</li>
<li>for: 评估深度神经网络（DNN）基于自适应巡航控制（ACC）系统的安全性，以防止恶意投毒攻击引起前方碰撞。</li>
<li>methods: 提出了一种结合知识驱动和数据驱动的方法，用于在攻击时选择最 kritical时刻，以及一种基于优化的方法来在运行时生成适应性的图像偏移。</li>
<li>results: 通过实验和实际驱动 simulator  Platform 和生产 ACC 系统，发现提案的攻击可以 achiev 142.9x 高的成功率，同时受到安全功能（如自动紧急刹车和前方碰撞预警）的干扰减少了89.6%。这种攻击 Robust 到实际世界因素和环境动态变化，同时能够避免被发现。这种研究提供了人类运行员和基本安全功能的抗攻击策略。<details>
<summary>Abstract</summary>
Adaptive Cruise Control (ACC) is a widely used driver assistance feature for maintaining desired speed and safe distance to the leading vehicles. This paper evaluates the security of the deep neural network (DNN) based ACC systems under stealthy perception attacks that strategically inject perturbations into camera data to cause forward collisions. We present a combined knowledge-and-data-driven approach to design a context-aware strategy for the selection of the most critical times for triggering the attacks and a novel optimization-based method for the adaptive generation of image perturbations at run-time. We evaluate the effectiveness of the proposed attack using an actual driving dataset and a realistic simulation platform with the control software from a production ACC system and a physical-world driving simulator while considering interventions by the driver and safety features such as Automatic Emergency Braking (AEB) and Forward Collision Warning (FCW). Experimental results show that the proposed attack achieves 142.9x higher success rate in causing accidents than random attacks and is mitigated 89.6% less by the safety features while being stealthy and robust to real-world factors and dynamic changes in the environment. This study provides insights into the role of human operators and basic safety interventions in preventing attacks.
</details>
<details>
<summary>摘要</summary>
这篇研究评估了基于深度神经网络（DNN）的自适应巡航控制（ACC）系统的安全性，以及这些系统对于隐藏式感知攻击的抵抗力。我们提出了一种结合知识驱动和数据驱动的方法，以选择最重要的时刻进行攻击，并且使用优化方法生成Run-time中的像素噪声。我们使用实际驾驶数据和真实的驾驶 simulate平台，考虑到驾驶员的干预和安全功能，例如自动紧急刹车（AEB）和前方冲击警示（FCW）。实验结果显示，我们的攻击成功率高于随机攻击的142.9倍，并且受到安全功能的抑制89.6%。此研究给出了人类驾驶员和基本安全功能的防御效果。
</details></li>
</ul>
<hr>
<h2 id="Multi-stage-Neural-Networks-Function-Approximator-of-Machine-Precision"><a href="#Multi-stage-Neural-Networks-Function-Approximator-of-Machine-Precision" class="headerlink" title="Multi-stage Neural Networks: Function Approximator of Machine Precision"></a>Multi-stage Neural Networks: Function Approximator of Machine Precision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08934">http://arxiv.org/abs/2307.08934</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongji Wang, Ching-Yao Lai</li>
<li>for: 这 paper 是为了提高神经网络在科学问题中的精度，并且使用多stage neural networks来mitigate spectral biases。</li>
<li>methods: 这 paper 使用了多stage neural networks，将训练过程分成不同的阶段，每个阶段使用一个新的网络来适应剩下的差异。</li>
<li>results: 这 paper 表明，使用多stage neural networks可以减少预测错误至O(10^{-16})水平，这是单个神经网络很难达到的精度。<details>
<summary>Abstract</summary>
Deep learning techniques are increasingly applied to scientific problems, where the precision of networks is crucial. Despite being deemed as universal function approximators, neural networks, in practice, struggle to reduce the prediction errors below $O(10^{-5})$ even with large network size and extended training iterations. To address this issue, we developed the multi-stage neural networks that divides the training process into different stages, with each stage using a new network that is optimized to fit the residue from the previous stage. Across successive stages, the residue magnitudes decreases substantially and follows an inverse power-law relationship with the residue frequencies. The multi-stage neural networks effectively mitigate the spectral biases associated with regular neural networks, enabling them to capture the high frequency feature of target functions. We demonstrate that the prediction error from the multi-stage training for both regression problems and physics-informed neural networks can nearly reach the machine-precision $O(10^{-16})$ of double-floating point within a finite number of iterations. Such levels of accuracy are rarely attainable using single neural networks alone.
</details>
<details>
<summary>摘要</summary>
深度学习技术在科学问题中越来越广泛应用，其精度的网络是关键。尽管被认为是通用函数近似器，实际上，神经网络在实践中难以降低预测错误 Below $O(10^{-5})$，即使使用大型网络和长时间训练轮次。为解决这问题，我们开发了多 stage 神经网络，将训练过程分解成不同的阶段，每个阶段使用新的网络，该网络是适应前一阶段剩余的。在 successive 阶段中，剩余大小减少了很多，并且与剩余频率关系为 inverse power-law 关系。多 stage 神经网络有效地 mitigate 神经网络的 спектраль偏好，使其能够捕捉目标函数的高频特征。我们示出，使用多 stage 训练，对于回归问题和物理学 Informed neural networks 的预测错误可以几乎达到机器精度 $O(10^{-16})$ 的水平，这些精度在单个神经网络alone 中 rarely 可以达到。
</details></li>
</ul>
<hr>
<h2 id="IxDRL-A-Novel-Explainable-Deep-Reinforcement-Learning-Toolkit-based-on-Analyses-of-Interestingness"><a href="#IxDRL-A-Novel-Explainable-Deep-Reinforcement-Learning-Toolkit-based-on-Analyses-of-Interestingness" class="headerlink" title="IxDRL: A Novel Explainable Deep Reinforcement Learning Toolkit based on Analyses of Interestingness"></a>IxDRL: A Novel Explainable Deep Reinforcement Learning Toolkit based on Analyses of Interestingness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08933">http://arxiv.org/abs/2307.08933</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sri-aic/23-xai-ixdrl-data">https://github.com/sri-aic/23-xai-ixdrl-data</a></li>
<li>paper_authors: Pedro Sequeira, Melinda Gervasio<br>for:The paper aims to provide a more explainable deep reinforcement learning (xDRL) framework to help human operators understand the competence of RL agents in complex decision-making tasks.methods:The proposed framework is based on interestingness analysis and is applicable to a wide range of RL algorithms, natively supporting the popular RLLib toolkit.results:The approach can identify agent behavior patterns and competency-controlling conditions, and the task elements mostly responsible for an agent’s competence, based on global and local analyses of interestingness. The framework provides agent designers with insights about RL agent competence, enabling more informed decisions about interventions, additional training, and other interactions in collaborative human-machine settings.<details>
<summary>Abstract</summary>
In recent years, advances in deep learning have resulted in a plethora of successes in the use of reinforcement learning (RL) to solve complex sequential decision tasks with high-dimensional inputs. However, existing systems lack the necessary mechanisms to provide humans with a holistic view of their competence, presenting an impediment to their adoption, particularly in critical applications where the decisions an agent makes can have significant consequences. Yet, existing RL-based systems are essentially competency-unaware in that they lack the necessary interpretation mechanisms to allow human operators to have an insightful, holistic view of their competency. Towards more explainable Deep RL (xDRL), we propose a new framework based on analyses of interestingness. Our tool provides various measures of RL agent competence stemming from interestingness analysis and is applicable to a wide range of RL algorithms, natively supporting the popular RLLib toolkit. We showcase the use of our framework by applying the proposed pipeline in a set of scenarios of varying complexity. We empirically assess the capability of the approach in identifying agent behavior patterns and competency-controlling conditions, and the task elements mostly responsible for an agent's competence, based on global and local analyses of interestingness. Overall, we show that our framework can provide agent designers with insights about RL agent competence, both their capabilities and limitations, enabling more informed decisions about interventions, additional training, and other interactions in collaborative human-machine settings.
</details>
<details>
<summary>摘要</summary>
To address this issue, we propose a new framework based on interestingness analysis to provide a more explainable Deep RL (xDRL) system. Our tool provides various measures of RL agent competence stemming from interestingness analysis and is applicable to a wide range of RL algorithms, natively supporting the popular RLLib toolkit.We demonstrate the use of our framework by applying it to a set of scenarios of varying complexity. We empirically assess the capability of the approach in identifying agent behavior patterns and competency-controlling conditions, as well as the task elements most responsible for an agent's competence, based on global and local analyses of interestingness.Our framework provides agent designers with insights into RL agent competence, including their capabilities and limitations, enabling more informed decisions about interventions, additional training, and other interactions in collaborative human-machine settings.
</details></li>
</ul>
<hr>
<h2 id="Submodular-Maximization-under-the-Intersection-of-Matroid-and-Knapsack-Constraints"><a href="#Submodular-Maximization-under-the-Intersection-of-Matroid-and-Knapsack-Constraints" class="headerlink" title="Submodular Maximization under the Intersection of Matroid and Knapsack Constraints"></a>Submodular Maximization under the Intersection of Matroid and Knapsack Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09487">http://arxiv.org/abs/2307.09487</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu-Ran Gu, Chao Bian, Chao Qian</li>
<li>for: 本文研究的目的是解决具有 intersection of $k$-matroid constraint和$m$-knapsack constraint的submodular maximization问题。</li>
<li>methods: 作者提出了一种名为SPROUT的新算法，通过将partial enumeration incorporated into the simultaneous greedy framework来解决该问题。</li>
<li>results: 作者证明了SPROUT可以在几乎polynomial时间内实现更好的approximation guarantee，并通过引入随机枚举和矫正技术，开发出了SPROUT++算法，在实践中具有更高的效率和相似的approximation guarantee。<details>
<summary>Abstract</summary>
Submodular maximization arises in many applications, and has attracted a lot of research attentions from various areas such as artificial intelligence, finance and operations research. Previous studies mainly consider only one kind of constraint, while many real-world problems often involve several constraints. In this paper, we consider the problem of submodular maximization under the intersection of two commonly used constraints, i.e., $k$-matroid constraint and $m$-knapsack constraint, and propose a new algorithm SPROUT by incorporating partial enumeration into the simultaneous greedy framework. We prove that SPROUT can achieve a polynomial-time approximation guarantee better than the state-of-the-art algorithms. Then, we introduce the random enumeration and smooth techniques into SPROUT to improve its efficiency, resulting in the SPROUT++ algorithm, which can keep a similar approximation guarantee. Experiments on the applications of movie recommendation and weighted max-cut demonstrate the superiority of SPROUT++ in practice.
</details>
<details>
<summary>摘要</summary>
<<SYS>>设<latex>k</latex>和<latex>m</latex>为两个正整数，我们考虑一个问题：在<latex>k</latex>-matroid约束和<latex>m</latex>-吸顺约束下，最大化submodular函数的问题。之前的研究主要考虑单一约束，而实际问题通常涉及多个约束。在本文中，我们提出了一种新的算法SPROUT，它通过同时干扰框架中的增量扩展来解决这个问题。我们证明了SPROUT可以在几乎真实时间内提供更好的近似度 garantia。然后，我们将随机枚举和缓和技术添加到SPROUT中，得到了SPROUT++算法，它可以保持相似的近似度 garantia。在电影推荐和权重最大枢纽问题的应用中，SPROUT++在实践中表现出了superiority。Note: The text has been translated using Google Translate, and may not be perfect.
</details></li>
</ul>
<hr>
<h2 id="On-the-fly-machine-learning-for-parametrization-of-the-effective-Hamiltonian"><a href="#On-the-fly-machine-learning-for-parametrization-of-the-effective-Hamiltonian" class="headerlink" title="On-the-fly machine learning for parametrization of the effective Hamiltonian"></a>On-the-fly machine learning for parametrization of the effective Hamiltonian</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08929">http://arxiv.org/abs/2307.08929</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingyue Ma, L. Bellaiche, Di Wu, Yurong Yang</li>
<li>for: 这研究旨在开发一种基于机器学习的启动式有效汉姆逻辑，用于预测和模拟 ferroelectrics 和 relaxor ferroelectrics 的性质。</li>
<li>methods: 这种方法使用 Bayesian 线性回归来 Parametrize 有效汉姆逻辑，在分子动力学实验中完成 Parametrization，并预测能量、力和压力以及它们的不确定性。当不确定性较大时，使用首要原理计算来重新训练参数。</li>
<li>results: 这种方法可以自动计算任何考虑系统的有效汉姆逻辑参数，包括复杂系统，而传统方法无法处理。用 BaTiO3 和 Pb(Sc,Ta)O3 作为示例，这种方法的准确性与传统首要原理 Parametrization 方法相当。<details>
<summary>Abstract</summary>
The first-principles-based effective Hamiltonian is widely used to predict and simulate the properties of ferroelectrics and relaxor ferroelectrics. However, the parametrization method of the effective Hamiltonian is complicated and hardly can resolve the systems with complex interactions and/or complex components. Here, we developed an on-the-fly machine learning approach to parametrize the effective Hamiltonian based on Bayesian linear regression. The parametrization is completed in molecular dynamics simulations, with the energy, forces and stress predicted at each step along with their uncertainties. First-principles calculations are executed when the uncertainties are large to retrain the parameters. This approach provides a universal and automatic way to compute the effective Hamiltonian parameters for any considered systems including complex systems which previous methods can not handle. BaTiO3 and Pb(Sc,Ta)O3 are taken as examples to show the accurateness of this approach comparing with conventional first-principles parametrization method.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用基本原理效 Hamiltonians 广泛预测和模拟 ferroelectrics 和 relaxor ferroelectrics 的性质。然而，基于效 Hamiltonians 的参数化方法复杂，难以处理复杂的交互和/或复杂的组分。我们在分子动力学实验中开发了一种在飞行中机器学习方法来参数化效 Hamiltonians，通过 Bayesian 线性回归来完成。在每步的分子动力学 simulate 中，能量、力和压力都预测了，同时预测了它们的不确定性。当不确定性大于一定程度时，我们使用首先原理计算来重新训练参数。这种方法提供了一种通用和自动的方法来计算效 Hamiltonians 参数，可以处理任何考虑的系统，包括复杂的系统，先前的方法无法处理。作为例子，我们选择了 BaTiO3 和 Pb(Sc,Ta)O3 来说明这种方法的准确性，与传统的首先原理参数化方法进行比较。
</details></li>
</ul>
<hr>
<h2 id="Federated-Large-Language-Model-A-Position-Paper"><a href="#Federated-Large-Language-Model-A-Position-Paper" class="headerlink" title="Federated Large Language Model: A Position Paper"></a>Federated Large Language Model: A Position Paper</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08925">http://arxiv.org/abs/2307.08925</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chaochao Chen, Xiaohua Feng, Jun Zhou, Jianwei Yin, Xiaolin Zheng</li>
<li>for: 这个研究旨在解决大规模语言模型（LLM）的开发问题，特别是在实际应用中遇到的挑战，例如公共领域数据的缺乏和维护private领域数据的隐私。</li>
<li>methods: 这个研究提出了一种称为“联邦式语言模型”（federated LLM）的技术，它包括三个主要的component，即联邦式语言模型预训练、联邦式语言模型细化和联邦式语言模型提示工程。每个component都有优点比传统LLM训练方法，并且提出了具体的工程策略来实现。</li>
<li>results: 这个研究获得了联邦式语言模型的优点，包括可以解决实际应用中的挑战，并且可以维护隐私和数据安全性。此外，研究也发现了联邦式语言模型在某些情况下可能会面临新的挑战和障碍。<details>
<summary>Abstract</summary>
Large scale language models (LLM) have received significant attention and found diverse applications across various domains, but their development encounters challenges in real-world scenarios. These challenges arise due to the scarcity of public domain data availability and the need to maintain privacy with respect to private domain data. To address these issues, federated learning (FL) has emerged as a promising technology that enables collaborative training of shared models while preserving decentralized data. We propose the concept of federated LLM, which comprises three key components, i.e., federated LLM pre-training, federated LLM fine-tuning, and federated LLM prompt engineering. For each component, we discuss its advantage over traditional LLM training methods and propose specific engineering strategies for implementation. Furthermore, we explore the novel challenges introduced by the integration of FL and LLM. We analyze existing solutions and identify potential obstacles faced by these solutions within the context of federated LLM.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-to-Sample-Tasks-for-Meta-Learning"><a href="#Learning-to-Sample-Tasks-for-Meta-Learning" class="headerlink" title="Learning to Sample Tasks for Meta Learning"></a>Learning to Sample Tasks for Meta Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08924">http://arxiv.org/abs/2307.08924</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ZJLAB-AMMI/HS-OMRL">https://github.com/ZJLAB-AMMI/HS-OMRL</a></li>
<li>paper_authors: Jingyao Wang, Zeen Song, Xingzhe Su, Lingyu Si, Hongwei Dong, Wenwen Qiang, Changwen Zheng</li>
<li>for: 通过对各种元学习方法、任务采样器和少量学习任务进行实验，这篇论文得出了三个结论。首先，无法确保元学习模型性能的通用任务采样策略。其次，任务多样性可能导致模型在训练中 Either underfit 或 overfit。最后，模型的总结果受任务分化、任务熵和任务Difficulty的影响。</li>
<li>methods: 作者提出了一种名为 Adaptive Sampler (ASr) 的新任务采样器，该采样器可以根据任务分化、任务熵和任务Difficulty来采样任务。以便优化 ASr，作者提出了一种简单普适的元学习算法。</li>
<li>results: 许多实验证明了提出的 ASr 的有效性。<details>
<summary>Abstract</summary>
Through experiments on various meta-learning methods, task samplers, and few-shot learning tasks, this paper arrives at three conclusions. Firstly, there are no universal task sampling strategies to guarantee the performance of meta-learning models. Secondly, task diversity can cause the models to either underfit or overfit during training. Lastly, the generalization performance of the models are influenced by task divergence, task entropy, and task difficulty. In response to these findings, we propose a novel task sampler called Adaptive Sampler (ASr). ASr is a plug-and-play task sampler that takes task divergence, task entropy, and task difficulty to sample tasks. To optimize ASr, we rethink and propose a simple and general meta-learning algorithm. Finally, a large number of empirical experiments demonstrate the effectiveness of the proposed ASr.
</details>
<details>
<summary>摘要</summary>
通过多种元学习方法、任务采样策略和少量学习任务的实验，这篇论文得出了三个结论。首先，没有一种通用的任务采样策略可以保证元学习模型的性能。第二，任务多样性可以使模型在训练中出现下降或过度适应。最后，模型的总体性能受到任务分化、任务 entropy 和任务难度的影响。为了应对这些发现，我们提出了一种名为 Adaptive Sampler（ASr）的任务采样器。ASr 是一个插件和玩家的任务采样器，它根据任务分化、任务 entropy 和任务难度来采样任务。为了优化 ASr，我们提出了一种简单和通用的元学习算法。最后，大量的实验证明了我们提出的 ASr 的有效性。
</details></li>
</ul>
<hr>
<h2 id="Optimistic-Estimate-Uncovers-the-Potential-of-Nonlinear-Models"><a href="#Optimistic-Estimate-Uncovers-the-Potential-of-Nonlinear-Models" class="headerlink" title="Optimistic Estimate Uncovers the Potential of Nonlinear Models"></a>Optimistic Estimate Uncovers the Potential of Nonlinear Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08921">http://arxiv.org/abs/2307.08921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yaoyu Zhang, Zhongwang Zhang, Leyang Zhang, Zhiwei Bai, Tao Luo, Zhi-Qin John Xu</li>
<li>for: 评估非线性模型最佳适应性表现的估计方法。</li>
<li>methods: 使用非线性模型，并且采用估计最小样本大小以达到目标函数的最佳适应性。</li>
<li>results: 对矩阵分解模型、深度模型和深度神经网络（DNN）进行了估计，并证明了这些模型在过参数化下的可适应性。此外，研究还发现了深度神经网络的两种特殊性：自由表达能力和成本表达能力。这两种特殊性提出了建议DNNS的建模设计原则：（一）不妨添加神经元和核函数；（二）限制神经元之间的连接。通过这种框架，我们预计在未来更深入理解如何和为什么许多非线性模型在实践中能够有效实现其潜在可能性。<details>
<summary>Abstract</summary>
We propose an optimistic estimate to evaluate the best possible fitting performance of nonlinear models. It yields an optimistic sample size that quantifies the smallest possible sample size to fit/recover a target function using a nonlinear model. We estimate the optimistic sample sizes for matrix factorization models, deep models, and deep neural networks (DNNs) with fully-connected or convolutional architecture. For each nonlinear model, our estimates predict a specific subset of targets that can be fitted at overparameterization, which are confirmed by our experiments. Our optimistic estimate reveals two special properties of the DNN models -- free expressiveness in width and costly expressiveness in connection. These properties suggest the following architecture design principles of DNNs: (i) feel free to add neurons/kernels; (ii) restrain from connecting neurons. Overall, our optimistic estimate theoretically unveils the vast potential of nonlinear models in fitting at overparameterization. Based on this framework, we anticipate gaining a deeper understanding of how and why numerous nonlinear models such as DNNs can effectively realize their potential in practice in the near future.
</details>
<details>
<summary>摘要</summary>
我们提出了一种优派估计来评估非线性模型的最佳适应性表现。它提供了一个最小的样本大小，用于评估目标函数使用非线性模型适应的可能性。我们对矩阵因子化模型、深度模型和深度神经网络（DNN）进行了估计，并证明了每种非线性模型的估计预测了一个特定的目标集可以在过参数化下适应。我们的估计还揭示了深度神经网络（DNN）模型的两种特殊性：自由表达能力和成本表达能力。这两种特殊性建议了深度神经网络（DNN）模型的建设设计原则：（i）自由添加神经元/核函数；（ii）限制神经元之间的连接。总的来说，我们的优派估计 theoretically 探明了非线性模型在过参数化下的潜在适应能力。基于这个框架，我们预计在未来将更深入地理解非线性模型在实践中如何有效地实现其潜在。
</details></li>
</ul>
<hr>
<h2 id="Continuous-Time-Reinforcement-Learning-New-Design-Algorithms-with-Theoretical-Insights-and-Performance-Guarantees"><a href="#Continuous-Time-Reinforcement-Learning-New-Design-Algorithms-with-Theoretical-Insights-and-Performance-Guarantees" class="headerlink" title="Continuous-Time Reinforcement Learning: New Design Algorithms with Theoretical Insights and Performance Guarantees"></a>Continuous-Time Reinforcement Learning: New Design Algorithms with Theoretical Insights and Performance Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08920">http://arxiv.org/abs/2307.08920</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brent A. Wallace, Jennie Si</li>
<li>for: 这个论文的目的是提出一种新的连续时间非线性优化控制方法，用于控制非线性系统。</li>
<li>methods: 这种方法基于分解physical system into smaller subproblems，并引入了一种新的刺激框架，以提高 persistency of excitation 和数值稳定性。</li>
<li>results: 这些算法可以提供 convergence 和关闭Loop稳定性保证，并在控制一个不稳定、非最小频段高速飞行器（HSV）上进行了示例应用。<details>
<summary>Abstract</summary>
Continuous-time nonlinear optimal control problems hold great promise in real-world applications. After decades of development, reinforcement learning (RL) has achieved some of the greatest successes as a general nonlinear control design method. However, a recent comprehensive analysis of state-of-the-art continuous-time RL (CT-RL) methods, namely, adaptive dynamic programming (ADP)-based CT-RL algorithms, reveals they face significant design challenges due to their complexity, numerical conditioning, and dimensional scaling issues. Despite advanced theoretical results, existing ADP CT-RL synthesis methods are inadequate in solving even small, academic problems. The goal of this work is thus to introduce a suite of new CT-RL algorithms for control of affine nonlinear systems. Our design approach relies on two important factors. First, our methods are applicable to physical systems that can be partitioned into smaller subproblems. This constructive consideration results in reduced dimensionality and greatly improved intuitiveness of design. Second, we introduce a new excitation framework to improve persistence of excitation (PE) and numerical conditioning performance via classical input/output insights. Such a design-centric approach is the first of its kind in the ADP CT-RL community. In this paper, we progressively introduce a suite of (decentralized) excitable integral reinforcement learning (EIRL) algorithms. We provide convergence and closed-loop stability guarantees, and we demonstrate these guarantees on a significant application problem of controlling an unstable, nonminimum phase hypersonic vehicle (HSV).
</details>
<details>
<summary>摘要</summary>
<<SYS>>translation into Simplified Chinese<</SYS>>连续时间非线性优化控制问题在实际应用中具有很大的推动力。在过去的几十年中，强化学习（RL）已经取得了一些非常成功的非线性控制设计方法。然而，一个最近的总体分析表明，使用ADP基于CT-RL算法的continuous-time RL（CT-RL）方法面临着复杂性、数值条件和维度增加问题。尽管有了先进的理论成果，现有的ADP CT-RL合成方法无法解决even small的学术问题。本工作的目标是引入一组新的CT-RL算法，用于控制非线性系统。我们的设计方法基于以下两个重要因素。首先，我们的方法适用于可以被分解成更小的子问题的物理系统。这种构建考虑导致维度减少和设计更加直观的问题。其次，我们引入了一个新的刺激框架，以提高持续刺激（PE）和数值条件性性能。这种设计中心的方法是CT-RL社区中的第一个。在这篇论文中，我们逐渐介绍了一组（分布式）刺激积分学习（EIRL）算法。我们提供了收敛和关闭Loop稳定性保证，并在一个重要应用问题中控制不稳定、非最小阶段速度飞行器（HSV）中进行了证明。
</details></li>
</ul>
<hr>
<h2 id="Accuracy-versus-time-frontiers-of-semi-supervised-and-self-supervised-learning-on-medical-images"><a href="#Accuracy-versus-time-frontiers-of-semi-supervised-and-self-supervised-learning-on-medical-images" class="headerlink" title="Accuracy versus time frontiers of semi-supervised and self-supervised learning on medical images"></a>Accuracy versus time frontiers of semi-supervised and self-supervised learning on medical images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08919">http://arxiv.org/abs/2307.08919</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tufts-ml/ssl-vs-ssl-benchmark">https://github.com/tufts-ml/ssl-vs-ssl-benchmark</a></li>
<li>paper_authors: Zhe Huang, Ruijie Jiang, Shuchin Aeron, Michael C. Hughes</li>
<li>for: 本研究的目的是为了解决资源受限、结果受关注的医学图像分类问题，通过采用自我超级vised学习和 semi-supervised learning方法，提高分类器的性能。</li>
<li>methods: 本研究使用了6种 semi-supervised 方法和5种自我超级vised学习方法，并与高品质标注数据作为基准进行比较。</li>
<li>results: 研究发现， MixMatch、SimCLR 和 BYOL 等方法在3种医学图像 datasets 上表现出色，并且可以在几个小时内达到优秀的性能。同时，通过选择适当的 hyperparameter 和进行较多的训练，可以获得进一步的提升。<details>
<summary>Abstract</summary>
For many applications of classifiers to medical images, a trustworthy label for each image can be difficult or expensive to obtain. In contrast, images without labels are more readily available. Two major research directions both promise that additional unlabeled data can improve classifier performance: self-supervised learning pretrains useful representations on unlabeled data only, then fine-tunes a classifier on these representations via the labeled set; semi-supervised learning directly trains a classifier on labeled and unlabeled data simultaneously. Recent methods from both directions have claimed significant gains on non-medical tasks, but do not systematically assess medical images and mostly compare only to methods in the same direction. This study contributes a carefully-designed benchmark to help answer a practitioner's key question: given a small labeled dataset and a limited budget of hours to spend on training, what gains from additional unlabeled images are possible and which methods best achieve them? Unlike previous benchmarks, ours uses realistic-sized validation sets to select hyperparameters, assesses runtime-performance tradeoffs, and bridges two research fields. By comparing 6 semi-supervised methods and 5 self-supervised methods to strong labeled-only baselines on 3 medical datasets with 30-1000 labels per class, we offer insights to resource-constrained, results-focused practitioners: MixMatch, SimCLR, and BYOL represent strong choices that were not surpassed by more recent methods. After much effort selecting hyperparameters on one dataset, we publish settings that enable strong methods to perform well on new medical tasks within a few hours, with further search over dozens of hours delivering modest additional gains.
</details>
<details>
<summary>摘要</summary>
für viele Anwendungen von Klassifikatoren auf medizinische Bilder kann ein zuverlässiger Label für jede Bilddatei schwierig oder teuer zu beschaffen sein. Im Gegensatz dazu sind Bilder ohne Labels mehr verfügbar. Zwei wichtige Forschungsrichtungen versprechen, dass zusätzliches unetikettiertes Datenmaterial die Leistung des Klassifikators verbessern kann: Selbstübergreifendes Lernen trainiert nützliche Representationen auf unetikettiertem Datenmaterial, then fine-tunes a classifier via the labeled set; semi-supervised Learning trainiert direkt einen Klassifikator auf etikettierten und unetikettierten Daten gleichzeitig. Recente Methoden beider Richtungen haben bedeutende Fortschritte bei nicht-medizinischen Aufgaben erzielt, aber systematisch die medizinischen Bilder nicht bewertet und sich nur mit Methoden in derselben Richtungen verglichen. This study contributes a carefully-designed Benchmark, um zu antworten auf eine wichtige Frage des Practitioners: given a small labeled dataset and a limited budget of hours to spend on training, what gains from additional unetikettiertes images are possible and which methods best achieve them? Im Gegensatz zu previous Benchmarks, ours uses realistic-sized validation sets to select hyperparameters, assesses runtime-performance tradeoffs, and bridges two research fields. By comparing 6 semi-supervised methods and 5 self-supervised methods to strong labeled-only baselines on 3 medical datasets with 30-1000 labels per class, we offer insights to resource-constrained, results-focused practitioners: MixMatch, SimCLR, and BYOL represent strong choices that were not surpassed by more recent methods. After much effort selecting hyperparameters on one dataset, we publish settings that enable strong methods to perform well on new medical tasks within a few hours, with further search over dozens of hours delivering modest additional gains.
</details></li>
</ul>
<hr>
<h2 id="Towards-the-Sparseness-of-Projection-Head-in-Self-Supervised-Learning"><a href="#Towards-the-Sparseness-of-Projection-Head-in-Self-Supervised-Learning" class="headerlink" title="Towards the Sparseness of Projection Head in Self-Supervised Learning"></a>Towards the Sparseness of Projection Head in Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08913">http://arxiv.org/abs/2307.08913</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeen Song, Xingzhe Su, Jingyao Wang, Wenwen Qiang, Changwen Zheng, Fuchun Sun</li>
<li>for: 提高自动学习（SSL）方法中的表示性能</li>
<li>methods: 使用对偶学习方法和理论分析，探讨投影头部件的内部机制和维度归一化现象之间的关系</li>
<li>results: 提出了一种假设，即只需要在数据批处理中最小化对偶损失时使用一部分特征；并通过对SSL方法进行论证，提出了一种名为SparseHead的规范项，可以减少投影头部件的稀疏性，从而提高SSL方法的表示性能。<details>
<summary>Abstract</summary>
In recent years, self-supervised learning (SSL) has emerged as a promising approach for extracting valuable representations from unlabeled data. One successful SSL method is contrastive learning, which aims to bring positive examples closer while pushing negative examples apart. Many current contrastive learning approaches utilize a parameterized projection head. Through a combination of empirical analysis and theoretical investigation, we provide insights into the internal mechanisms of the projection head and its relationship with the phenomenon of dimensional collapse. Our findings demonstrate that the projection head enhances the quality of representations by performing contrastive loss in a projected subspace. Therefore, we propose an assumption that only a subset of features is necessary when minimizing the contrastive loss of a mini-batch of data. Theoretical analysis further suggests that a sparse projection head can enhance generalization, leading us to introduce SparseHead - a regularization term that effectively constrains the sparsity of the projection head, and can be seamlessly integrated with any self-supervised learning (SSL) approaches. Our experimental results validate the effectiveness of SparseHead, demonstrating its ability to improve the performance of existing contrastive methods.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Sharpness-Aware-Graph-Collaborative-Filtering"><a href="#Sharpness-Aware-Graph-Collaborative-Filtering" class="headerlink" title="Sharpness-Aware Graph Collaborative Filtering"></a>Sharpness-Aware Graph Collaborative Filtering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08910">http://arxiv.org/abs/2307.08910</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huiyuan Chen, Chin-Chia Michael Yeh, Yujie Fan, Yan Zheng, Junpeng Wang, Vivian Lai, Mahashweta Das, Hao Yang</li>
<li>for: 提高Graph Neural Networks（GNNs）在协同缓存中的表现。</li>
<li>methods: 提出了一种有效的训练方案{gSAM}，基于权重损失 landscape的平滑性来优化GNNs。</li>
<li>results: 实验结果表明，gSAM可以提高GNNs的表现。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have achieved impressive performance in collaborative filtering. However, GNNs tend to yield inferior performance when the distributions of training and test data are not aligned well. Also, training GNNs requires optimizing non-convex neural networks with an abundance of local and global minima, which may differ widely in their performance at test time. Thus, it is essential to choose the minima carefully. Here we propose an effective training schema, called {gSAM}, under the principle that the \textit{flatter} minima has a better generalization ability than the \textit{sharper} ones. To achieve this goal, gSAM regularizes the flatness of the weight loss landscape by forming a bi-level optimization: the outer problem conducts the standard model training while the inner problem helps the model jump out of the sharp minima. Experimental results show the superiority of our gSAM.
</details>
<details>
<summary>摘要</summary>
格raph神经网络（GNNs）在共同推荐中表现出色。然而，GNNs在训练和测试数据分布不匹配时表现不佳。此外，训练GNNs需要优化非核心积分神经网络，这些神经网络具有很多的本地和全局最小值，其测试时表现可能很不同。因此，选择最佳的最小值非常重要。我们提出了一种有效的训练方法，即{gSAM}，其基于“稍平”的最小值具有更好的泛化能力。为实现这个目标，gSAM在权重损失的折衔减中做出了二级优化：外部问题进行标准模型训练，而内部问题帮助模型离开锋利的最小值。实验结果显示了我们的gSAM的优越性。
</details></li>
</ul>
<hr>
<h2 id="Solving-multiphysics-based-inverse-problems-with-learned-surrogates-and-constraints"><a href="#Solving-multiphysics-based-inverse-problems-with-learned-surrogates-and-constraints" class="headerlink" title="Solving multiphysics-based inverse problems with learned surrogates and constraints"></a>Solving multiphysics-based inverse problems with learned surrogates and constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11099">http://arxiv.org/abs/2307.11099</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyi Yin, Rafael Orozco, Mathias Louboutin, Felix J. Herrmann</li>
<li>for: 这 paper 是用于解决地质碳存储监测中的多物理 inverse problem 的。</li>
<li>methods: 这 paper 使用了 computationally cheap 的 learned surrogates 和 learned constraints 来解决这些问题。</li>
<li>results: 这 paper 的结果表明，这种 combinaison 可以提高 fluid-flow 性能的减法，并且可以处理多模态数据，包括 well 测量和 active-source time-lapse seismic 数据。另外，这种方法还可以保持准确性，因为它使用了一个 trained deep neural network 来 constrain the model iterates。<details>
<summary>Abstract</summary>
Solving multiphysics-based inverse problems for geological carbon storage monitoring can be challenging when multimodal time-lapse data are expensive to collect and costly to simulate numerically. We overcome these challenges by combining computationally cheap learned surrogates with learned constraints. Not only does this combination lead to vastly improved inversions for the important fluid-flow property, permeability, it also provides a natural platform for inverting multimodal data including well measurements and active-source time-lapse seismic data. By adding a learned constraint, we arrive at a computationally feasible inversion approach that remains accurate. This is accomplished by including a trained deep neural network, known as a normalizing flow, which forces the model iterates to remain in-distribution, thereby safeguarding the accuracy of trained Fourier neural operators that act as surrogates for the computationally expensive multiphase flow simulations involving partial differential equation solves. By means of carefully selected experiments, centered around the problem of geological carbon storage, we demonstrate the efficacy of the proposed constrained optimization method on two different data modalities, namely time-lapse well and time-lapse seismic data. While permeability inversions from both these two modalities have their pluses and minuses, their joint inversion benefits from either, yielding valuable superior permeability inversions and CO2 plume predictions near, and far away, from the monitoring wells.
</details>
<details>
<summary>摘要</summary>
解决基于多物理学的逆 пробле 的地质碳存储监测可以是困难的，当 multimodal 时间差数据成本高昂， numerically 计算成本高昂时。我们利用计算成本低廉的学习的代理人与学习的约束结合，不仅能够大幅提高流体流动性的重要性质，孔隙性，还提供了自然的多模态数据逆向平台。通过添加学习的约束，我们实现了计算可行的逆向方法，保持精度。这是通过包含训练好的深度神经网络，即 нормализа流，使模型迭代器 remains 在distribution中，保证训练了Fourier神经网络作为多相流 simulations involving partial differential equation solves的计算昂贵的surrogate。通过选择精心的实验，以地质碳存储问题为中心，我们在时间差井和时间差地震数据两个不同的模式下展示了提案的受限优化方法的效果。虽然孔隙性逆向从这两个模式中有其优缺点，但是两者的共同逆向具有优势，为CO2泵预测和碳存储监测提供了有价值的Superior permeability inversions和预测。
</details></li>
</ul>
<hr>
<h2 id="Basal-Bolus-Advisor-for-Type-1-Diabetes-T1D-Patients-Using-Multi-Agent-Reinforcement-Learning-RL-Methodology"><a href="#Basal-Bolus-Advisor-for-Type-1-Diabetes-T1D-Patients-Using-Multi-Agent-Reinforcement-Learning-RL-Methodology" class="headerlink" title="Basal-Bolus Advisor for Type 1 Diabetes (T1D) Patients Using Multi-Agent Reinforcement Learning (RL) Methodology"></a>Basal-Bolus Advisor for Type 1 Diabetes (T1D) Patients Using Multi-Agent Reinforcement Learning (RL) Methodology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08897">http://arxiv.org/abs/2307.08897</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mehrad Jaloli, Marzia Cescon</li>
<li>for: 这种研究旨在开发一种基于多智能 reinforcement learning（RL）的个性化血糖控制方法，以改善ype 1  диабе尼（T1D）患者的血糖控制。</li>
<li>methods: 该方法使用一个关闭的循环系统，包括血糖代谢模型和多智能软actor-critic RL模型，作为基础-膳食帮手。</li>
<li>results: 研究结果表明，RL-基于的基础-膳食帮手可以有效改善血糖控制，降低血糖波动性，并增加血糖水平在目标范围内的时间。同时，RL方法可以有效预防低血糖事件，并减少高血糖事件。此外，RL方法还导致了对 convential 疗法相比，每天基础荷尔血糖剂的减少。这些发现表明RL方法可以在ype 1  диабе尼患者中实现更好的血糖控制，并减少严重高血糖的风险。<details>
<summary>Abstract</summary>
This paper presents a novel multi-agent reinforcement learning (RL) approach for personalized glucose control in individuals with type 1 diabetes (T1D). The method employs a closed-loop system consisting of a blood glucose (BG) metabolic model and a multi-agent soft actor-critic RL model acting as the basal-bolus advisor. Performance evaluation is conducted in three scenarios, comparing the RL agents to conventional therapy. Evaluation metrics include glucose levels (minimum, maximum, and mean), time spent in different BG ranges, and average daily bolus and basal insulin dosages. Results demonstrate that the RL-based basal-bolus advisor significantly improves glucose control, reducing glycemic variability and increasing time spent within the target range (70-180 mg/dL). Hypoglycemia events are effectively prevented, and severe hyperglycemia events are reduced. The RL approach also leads to a statistically significant reduction in average daily basal insulin dosage compared to conventional therapy. These findings highlight the effectiveness of the multi-agent RL approach in achieving better glucose control and mitigating the risk of severe hyperglycemia in individuals with T1D.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Evaluating-unsupervised-disentangled-representation-learning-for-genomic-discovery-and-disease-risk-prediction"><a href="#Evaluating-unsupervised-disentangled-representation-learning-for-genomic-discovery-and-disease-risk-prediction" class="headerlink" title="Evaluating unsupervised disentangled representation learning for genomic discovery and disease risk prediction"></a>Evaluating unsupervised disentangled representation learning for genomic discovery and disease risk prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08893">http://arxiv.org/abs/2307.08893</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taedong Yun</li>
<li>for: 这个论文主要是为了研究高维клиниче数据中的生物marks，以及使用深度学习技术进行遗传学研究。</li>
<li>methods: 这个论文使用了多种无监督学习方法，包括自动编码器、VAE、β-VAE和factorVAE，以学习分离的表示。</li>
<li>results: 研究发现，使用FactorVAE或β-VAE，可以提高遗传学研究中的结果，包括基因associes数量、heritability和多ifactorial风险分数的性能。factorVAE在不同的规则化参数值下表现良好，而β-VAE却受到规则化参数值的影响较大。<details>
<summary>Abstract</summary>
High-dimensional clinical data have become invaluable resources for genetic studies, due to their accessibility in biobank-scale datasets and the development of high performance modeling techniques especially using deep learning. Recent work has shown that low dimensional embeddings of these clinical data learned by variational autoencoders (VAE) can be used for genome-wide association studies and polygenic risk prediction. In this work, we consider multiple unsupervised learning methods for learning disentangled representations, namely autoencoders, VAE, beta-VAE, and FactorVAE, in the context of genetic association studies. Using spirograms from UK Biobank as a running example, we observed improvements in the number of genome-wide significant loci, heritability, and performance of polygenic risk scores for asthma and chronic obstructive pulmonary disease by using FactorVAE or beta-VAE, compared to standard VAE or non-variational autoencoders. FactorVAEs performed effectively across multiple values of the regularization hyperparameter, while beta-VAEs were much more sensitive to the hyperparameter values.
</details>
<details>
<summary>摘要</summary>
高维临床数据已成为生物银行规模数据集的不可或缺的资源，这主要归功于生物银行规模数据集的可用性和深度学习技术的发展。据研究表明，使用变量自动编码器（VAE）学习的低维度表示可以用于遗传相关研究和多ifactorial风险预测。在本工作中，我们考虑了多种无监督学习方法，包括 autoencoder、VAE、β-VAE 和 FactorVAE，在遗传相关研究中。使用 UK Biobank 的呼吸图为例，我们发现了使用 FactorVAE 或 β-VAE 而非标准 VAE 或非变量自动编码器后，对气喘病和肺部疾病的遗传相关性、heritability 和多ifactorial风险分数的改进。FactorVAE 在多个规则化超参数值上表现得更好，而 β-VAE 对超参数值的敏感性较高。
</details></li>
</ul>
<hr>
<h2 id="The-Predicted-Deletion-Dynamic-Model-Taking-Advantage-of-ML-Predictions-for-Free"><a href="#The-Predicted-Deletion-Dynamic-Model-Taking-Advantage-of-ML-Predictions-for-Free" class="headerlink" title="The Predicted-Deletion Dynamic Model: Taking Advantage of ML Predictions, for Free"></a>The Predicted-Deletion Dynamic Model: Taking Advantage of ML Predictions, for Free</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08890">http://arxiv.org/abs/2307.08890</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quanquan C. Liu, Vaidehi Srinivas</li>
<li>for: 这 paper 的目的是解决动态图中edge更新预测问题，提高动态算法的效率。</li>
<li>methods: 这 paper 使用了预测 deleting 的 Dynamic Model，并提出了一种基于这种模型的框架，可以将部分动态算法”升级”到完全动态Setting中，减少了更新时间的复杂性。</li>
<li>results: 这 paper 的算法在不同的问题上都能够实现更好的性能，具体来说，它们的平均更新时间与部分动态算法相似，而且在预测质量较高时，它们的性能与完全动态算法相当。<details>
<summary>Abstract</summary>
The main bottleneck in designing efficient dynamic algorithms is the unknown nature of the update sequence. In particular, there are some problems, like 3-vertex connectivity, planar digraph all pairs shortest paths, and others, where the separation in runtime between the best partially dynamic solutions and the best fully dynamic solutions is polynomial, sometimes even exponential.   In this paper, we formulate the predicted-deletion dynamic model, motivated by a recent line of empirical work about predicting edge updates in dynamic graphs. In this model, edges are inserted and deleted online, and when an edge is inserted, it is accompanied by a "prediction" of its deletion time. This models real world settings where services may have access to historical data or other information about an input and can subsequently use such information make predictions about user behavior. The model is also of theoretical interest, as it interpolates between the partially dynamic and fully dynamic settings, and provides a natural extension of the algorithms with predictions paradigm to the dynamic setting.   We give a novel framework for this model that "lifts" partially dynamic algorithms into the fully dynamic setting with little overhead. We use our framework to obtain improved efficiency bounds over the state-of-the-art dynamic algorithms for a variety of problems. In particular, we design algorithms that have amortized update time that scales with a partially dynamic algorithm, with high probability, when the predictions are of high quality. On the flip side, our algorithms do no worse than existing fully-dynamic algorithms when the predictions are of low quality. Furthermore, our algorithms exhibit a graceful trade-off between the two cases. Thus, we are able to take advantage of ML predictions asymptotically "for free.''
</details>
<details>
<summary>摘要</summary>
主要瓶颈在设计高效的动态算法上是未知的更新序列。特别是有些问题，如3个顶点连接性、平面图全对短路和其他问题，其运行时间差 между最佳半动态解决方案和最佳完全动态解决方案是多项式，有时甚至是指数。在这篇论文中，我们提出预测删除动态模型，因为一些现实世界中的服务可以通过历史数据或其他信息来预测用户行为。这种模型在理论上也很有 interess，因为它在半动态和完全动态之间进行 interpolating，并且为动态设定提供了自然的扩展。我们给出了一种新的框架，使得半动态算法在完全动态设定下具有较少的开销。我们使用这种框架，设计了一些算法，其更新时间平均与半动态算法相似，高probability 时，当预测质量高时。另一方面，我们的算法不比现有的完全动态算法差，当预测质量低时。此外，我们的算法具有温和的质量补偿，因此可以充分利用机器学习预测，“免费”。
</details></li>
</ul>
<hr>
<h2 id="Examining-the-Effects-of-Degree-Distribution-and-Homophily-in-Graph-Learning-Models"><a href="#Examining-the-Effects-of-Degree-Distribution-and-Homophily-in-Graph-Learning-Models" class="headerlink" title="Examining the Effects of Degree Distribution and Homophily in Graph Learning Models"></a>Examining the Effects of Degree Distribution and Homophily in Graph Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08881">http://arxiv.org/abs/2307.08881</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research/graphworld">https://github.com/google-research/graphworld</a></li>
<li>paper_authors: Mustafa Yasir, John Palowitch, Anton Tsitsulin, Long Tran-Thanh, Bryan Perozzi</li>
<li>for: This paper aims to improve the evaluation of graph neural network (GNN) models by expanding the coverage of graph space within the GraphWorld framework.</li>
<li>methods: The paper uses three synthetic graph generators: the Stochastic Block Model (SBM), LFR, and CABAM. These generators are integrated into the GraphWorld framework to create more diverse populations of synthetic graphs for benchmarking GNN tasks.</li>
<li>results: The paper generates 300,000 graphs to benchmark 11 GNN models on a node classification task, and finds variations in GNN performance in response to homophily, degree distribution, and feature signal. The paper classifies GNN models based on their sensitivity to the new generators under these properties.Here’s the simplified Chinese text for the three information points:</li>
<li>for: 这篇论文目标是提高图神经网络（GNN）模型的评估，通过扩展图WORLD框架中的图空间覆盖。</li>
<li>methods: 这篇论文使用了三种 sintetic 图生成器：Stochastic Block Model（SBM）、LFR 和 CABAM。这些生成器被 integrate 到图WORLD框架中，以创造更多的多样化的 sintetic 图来 benchmark GNN 任务。</li>
<li>results: 这篇论文通过生成 300,000 个图，对 11 种 GNN 模型进行节点分类任务的评估，发现 GNN 性能响应于同类性、度分布和特征信号。 paper 根据这些特性将 GNN 模型分类为敏感性。<details>
<summary>Abstract</summary>
Despite a surge in interest in GNN development, homogeneity in benchmarking datasets still presents a fundamental issue to GNN research. GraphWorld is a recent solution which uses the Stochastic Block Model (SBM) to generate diverse populations of synthetic graphs for benchmarking any GNN task. Despite its success, the SBM imposed fundamental limitations on the kinds of graph structure GraphWorld could create.   In this work we examine how two additional synthetic graph generators can improve GraphWorld's evaluation; LFR, a well-established model in the graph clustering literature and CABAM, a recent adaptation of the Barabasi-Albert model tailored for GNN benchmarking. By integrating these generators, we significantly expand the coverage of graph space within the GraphWorld framework while preserving key graph properties observed in real-world networks. To demonstrate their effectiveness, we generate 300,000 graphs to benchmark 11 GNN models on a node classification task. We find GNN performance variations in response to homophily, degree distribution and feature signal. Based on these findings, we classify models by their sensitivity to the new generators under these properties. Additionally, we release the extensions made to GraphWorld on the GitHub repository, offering further evaluation of GNN performance on new graphs.
</details>
<details>
<summary>摘要</summary>
尽管GNN发展中的兴趣增长，仍然存在基本问题，即 benchmarking 数据集的同质性。GraphWorld 是一种最近的解决方案，使用 Stochastic Block Model（SBM）生成多样化的 synthetic graph 用于任何 GNN 任务的 benchmarking。尽管它成功，但 SBM 强制性限制 GraphWorld 可以创建的图结构类型。在这种工作中，我们检查了两种额外的 sintethic graph 生成器是如何提高 GraphWorld 的评估。LFR 是一种已有的图分群模型，CABAM 是一种对 Barabasi-Albert 模型的最近适应，专门用于 GNN  benchmarking。通过将这些生成器纳入 GraphWorld 框架，我们可以覆盖图空间的扩展，保持真实世界网络中观察到的关键图属性。为了证明其效果，我们生成了 300,000 个图用于对 11 种 GNN 模型进行节点分类任务的 benchmarking。我们发现 GNN 模型在同质性、度分布和特征信号下的性能变化。根据这些发现，我们将模型分为它们对新生成器下的性能响应。此外，我们在 GitHub 仓库中发布了 GraphWorld 的扩展，以便进一步评估 GNN 性能在新的图上。
</details></li>
</ul>
<hr>
<h2 id="Modular-Neural-Network-Approaches-for-Surgical-Image-Recognition"><a href="#Modular-Neural-Network-Approaches-for-Surgical-Image-Recognition" class="headerlink" title="Modular Neural Network Approaches for Surgical Image Recognition"></a>Modular Neural Network Approaches for Surgical Image Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08880">http://arxiv.org/abs/2307.08880</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nosseiba Ben Salem, Younes Bennani, Joseph Karkazan, Abir Barbara, Charles Dacheux, Thomas Gregory</li>
<li>for: 这个研究是为了提出一种基于深度学习的脑网络架构，以解决现代问题的复杂化和数据不足问题。</li>
<li>methods: 这个研究使用了自我训练的方法来解决数据不足问题，并且将问题分解为更简单的子 задачі，以提高模型的泛化和解释性。</li>
<li>results: 研究发现，使用模块学习方法可以提高分类性能，并且可以实现近乎完美的分类。另外，这种方法还可以提高数据分类的速度和可解释性。<details>
<summary>Abstract</summary>
Deep learning-based applications have seen a lot of success in recent years. Text, audio, image, and video have all been explored with great success using deep learning approaches. The use of convolutional neural networks (CNN) in computer vision, in particular, has yielded reliable results. In order to achieve these results, a large amount of data is required. However, the dataset cannot always be accessible. Moreover, annotating data can be difficult and time-consuming. Self-training is a semi-supervised approach that managed to alleviate this problem and achieve state-of-the-art performances. Theoretical analysis even proved that it may result in a better generalization than a normal classifier. Another problem neural networks can face is the increasing complexity of modern problems, requiring a high computational and storage cost. One way to mitigate this issue, a strategy that has been inspired by human cognition known as modular learning, can be employed. The principle of the approach is to decompose a complex problem into simpler sub-tasks. This approach has several advantages, including faster learning, better generalization, and enables interpretability.   In the first part of this paper, we introduce and evaluate different architectures of modular learning for Dorsal Capsulo-Scapholunate Septum (DCSS) instability classification. Our experiments have shown that modular learning improves performances compared to non-modular systems. Moreover, we found that weighted modular, that is to weight the output using the probabilities from the gating module, achieved an almost perfect classification. In the second part, we present our approach for data labeling and segmentation with self-training applied on shoulder arthroscopy images.
</details>
<details>
<summary>摘要</summary>
深度学习基于应用在过去几年中取得了很多成功。文本、音频、图像和视频都被使用深度学习方法进行了成功的探索。特别是在计算机视觉方面，卷积神经网络（CNN）的使用已经取得了可靠的结果。但是，获取数据的问题仍然存在。另外，标注数据可能会是困难的和耗时的。自学习是一种半监督学习方法，可以解决这个问题，并达到状态艺术的性能。理论分析还证明，它可能会在普通分类器之上取得更好的泛化性。另一个问题是现代问题的复杂性，需要高度的计算和存储成本。一种可以 mitigate这个问题的方法是模块学习。这种方法的原理是将复杂问题分解成更简单的子任务。这种方法有很多优点，包括更快的学习、更好的泛化和可读性。在本文的第一部分，我们介绍了不同的模块学习架构，并对DCSS不稳定性分类问题进行了评估。我们的实验结果表明，模块学习可以提高性能，而且使用权重模块可以达到几乎完美的分类。在第二部分，我们介绍了我们的自动标注和分割方法，使用自学习在肩镜像中进行了应用。
</details></li>
</ul>
<hr>
<h2 id="Disentangling-Node-Attributes-from-Graph-Topology-for-Improved-Generalizability-in-Link-Prediction"><a href="#Disentangling-Node-Attributes-from-Graph-Topology-for-Improved-Generalizability-in-Link-Prediction" class="headerlink" title="Disentangling Node Attributes from Graph Topology for Improved Generalizability in Link Prediction"></a>Disentangling Node Attributes from Graph Topology for Improved Generalizability in Link Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08877">http://arxiv.org/abs/2307.08877</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chatterjeeayan/upna">https://github.com/chatterjeeayan/upna</a></li>
<li>paper_authors: Ayan Chatterjee, Robin Walters, Giulia Menichetti, Tina Eliassi-Rad</li>
<li>for: 链接预测是图机器学习中关键的任务，具有广泛的应用。本文研究节点特征和图结构之间的互动，并证明在包含预训节点特征的情况下，链接预测模型的泛化能力得到提高。</li>
<li>methods: 我们提出的方法是UPNA（无监督节点特征预训），它解决了链接预测问题，学习一个函数，将两个节点特征作为输入，预测两节点之间的边的概率。与传统的图神经网络（GNN）不同，UPNA不会因为图中强制度分布的问题而陷入拟合偏见。</li>
<li>results: 我们的实验表明，UPNA可以在多种对比 datasets 上达到3X至34X的提高，超过当前的状态艺。此外，UPNA可以应用于多种对比学习任务，并与现有的链接预测模型集成，提高其泛化能力和图生成模型的强度。<details>
<summary>Abstract</summary>
Link prediction is a crucial task in graph machine learning with diverse applications. We explore the interplay between node attributes and graph topology and demonstrate that incorporating pre-trained node attributes improves the generalization power of link prediction models. Our proposed method, UPNA (Unsupervised Pre-training of Node Attributes), solves the inductive link prediction problem by learning a function that takes a pair of node attributes and predicts the probability of an edge, as opposed to Graph Neural Networks (GNN), which can be prone to topological shortcuts in graphs with power-law degree distribution. In this manner, UPNA learns a significant part of the latent graph generation mechanism since the learned function can be used to add incoming nodes to a growing graph. By leveraging pre-trained node attributes, we overcome observational bias and make meaningful predictions about unobserved nodes, surpassing state-of-the-art performance (3X to 34X improvement on benchmark datasets). UPNA can be applied to various pairwise learning tasks and integrated with existing link prediction models to enhance their generalizability and bolster graph generative models.
</details>
<details>
<summary>摘要</summary>
链接预测是图机器学习中关键的任务，具有广泛的应用。我们研究节点特征和图结构之间的互动，并证明在把预训练节点特征纳入模型中可以提高链接预测模型的通用能力。我们提出的方法是UPNA（无监督节点特征预训练），解决了链接预测问题，学习一个函数，用来预测两个节点之间的边的概率，而不是使用图神经网络（GNN），后者可能会因为图中具有强制的度分布而导致拓扑短 Circuit。这种方法可以学习图生成机制的一个重要部分，因为学习的函数可以用来添加新的入节点到生长中的图。通过利用预训练节点特征，我们超越观察偏见，可以对未观察的节点进行有意义的预测，超过了状态艺术性的表现（3X-34X提高在标准数据集上）。UPNA可以应用于多种对称学习任务，并可以与现有的链接预测模型集成，提高其通用性和图生成模型的鲁棒性。
</details></li>
</ul>
<hr>
<h2 id="Natural-Actor-Critic-for-Robust-Reinforcement-Learning-with-Function-Approximation"><a href="#Natural-Actor-Critic-for-Robust-Reinforcement-Learning-with-Function-Approximation" class="headerlink" title="Natural Actor-Critic for Robust Reinforcement Learning with Function Approximation"></a>Natural Actor-Critic for Robust Reinforcement Learning with Function Approximation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08875">http://arxiv.org/abs/2307.08875</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruida Zhou, Tao Liu, Min Cheng, Dileep Kalathil, P. R. Kumar, Chao Tian</li>
<li>for: 这个论文的目的是解决模型匹配问题，即在训练环境和测试环境之间的模型差异问题，以确定一个可靠性高的策略。</li>
<li>methods: 该论文提出了两种新的不确定集形式，一种基于双抽样，另一种基于积分概率度量。这两种不确定集形式使得大规模的Robust reinforcement learning（RL）变得可 tractable，即使只有训练环境。该论文还提出了一种 robust natural actor-critic（RNAC）方法，该方法包括新的不确定集形式和函数approximation。</li>
<li>results: 该论文的实验结果显示，RNAC方法可以在多个 MuJoCo 环境和一个实际世界的TurtleBot导航任务中提供良好的Robust性性能。<details>
<summary>Abstract</summary>
We study robust reinforcement learning (RL) with the goal of determining a well-performing policy that is robust against model mismatch between the training simulator and the testing environment. Previous policy-based robust RL algorithms mainly focus on the tabular setting under uncertainty sets that facilitate robust policy evaluation, but are no longer tractable when the number of states scales up. To this end, we propose two novel uncertainty set formulations, one based on double sampling and the other on an integral probability metric. Both make large-scale robust RL tractable even when one only has access to a simulator. We propose a robust natural actor-critic (RNAC) approach that incorporates the new uncertainty sets and employs function approximation. We provide finite-time convergence guarantees for the proposed RNAC algorithm to the optimal robust policy within the function approximation error. Finally, we demonstrate the robust performance of the policy learned by our proposed RNAC approach in multiple MuJoCo environments and a real-world TurtleBot navigation task.
</details>
<details>
<summary>摘要</summary>
我们研究了一种robust reinforcement learning（RL）方法，以确定在训练环境和测试环境之间存在模型匹配不良的情况下，可以达到良好的策略。之前的策略基于RL算法主要在表格设定下进行了不确定性集的使用，但是当状态数量增加时，这些算法就不再可行了。为此，我们提出了两种新的不确定性集形式，一种基于双抽样，另一种基于 интеграル概率度量。这两种形式使得大规模的RL问题变得可 tractable，即使只有训练环境的模型。我们提出了一种robust natural actor-critic（RNAC）方法，该方法包括新的不确定性集和函数近似。我们提供了finite-time converges guarantees，表明RNAC算法在函数近似误差下可以在有限时间内 converges到最佳robust策略。最后，我们在多个MuJoCo环境和一个实际世界TurtleBot导航任务中证明了我们提出的RNAC策略的robust性。
</details></li>
</ul>
<hr>
<h2 id="Latent-Space-Representations-of-Neural-Algorithmic-Reasoners"><a href="#Latent-Space-Representations-of-Neural-Algorithmic-Reasoners" class="headerlink" title="Latent Space Representations of Neural Algorithmic Reasoners"></a>Latent Space Representations of Neural Algorithmic Reasoners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08874">http://arxiv.org/abs/2307.08874</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mirjanic/nar-latent-spaces">https://github.com/mirjanic/nar-latent-spaces</a></li>
<li>paper_authors: Vladimir V. Mirjanić, Razvan Pascanu, Petar Veličković</li>
<li>for: 这个研究探讨了神经算法逻辑（NAR）领域中使用神经网络架构来可靠地捕捉经典计算方法的问题。</li>
<li>methods: 该研究使用图神经网络（GNN）架构，将输入编码成高维隐藏空间，并在执行算法时进行重复转换。</li>
<li>results: 研究发现GNN架构中的隐藏空间结构存在两种可能的失败模式：（1）loss of resolution，导致同样的值很难分辨；（2）无法处理训练期间未见到的值。提议使用softmax汇聚器和衰减隐藏空间来解决这两种问题，并证明这些改进可以在CLRS-30标准测试集上提高大多数算法的性能。<details>
<summary>Abstract</summary>
Neural Algorithmic Reasoning (NAR) is a research area focused on designing neural architectures that can reliably capture classical computation, usually by learning to execute algorithms. A typical approach is to rely on Graph Neural Network (GNN) architectures, which encode inputs in high-dimensional latent spaces that are repeatedly transformed during the execution of the algorithm. In this work we perform a detailed analysis of the structure of the latent space induced by the GNN when executing algorithms. We identify two possible failure modes: (i) loss of resolution, making it hard to distinguish similar values; (ii) inability to deal with values outside the range observed during training. We propose to solve the first issue by relying on a softmax aggregator, and propose to decay the latent space in order to deal with out-of-range values. We show that these changes lead to improvements on the majority of algorithms in the standard CLRS-30 benchmark when using the state-of-the-art Triplet-GMPNN processor. Our code is available at \href{https://github.com/mirjanic/nar-latent-spaces}{https://github.com/mirjanic/nar-latent-spaces}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="An-Alternative-to-Variance-Gini-Deviation-for-Risk-averse-Policy-Gradient"><a href="#An-Alternative-to-Variance-Gini-Deviation-for-Risk-averse-Policy-Gradient" class="headerlink" title="An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient"></a>An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08873">http://arxiv.org/abs/2307.08873</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yudong Luo, Guiliang Liu, Pascal Poupart, Yangchen Pan</li>
<li>for: 降低奖励学习中的风险偏好，即使是在数值上有些偏好。</li>
<li>methods: 使用新的风险度量——吉尼偏度，代替传统的归一化奖励方法。</li>
<li>results: 在具有明确的风险偏好的领域中，通过对吉尼偏度进行优化，实现高回报低风险的策略学习。其他方法在这些领域中很难学习一个合理的策略。<details>
<summary>Abstract</summary>
Restricting the variance of a policy's return is a popular choice in risk-averse Reinforcement Learning (RL) due to its clear mathematical definition and easy interpretability. Traditional methods directly restrict the total return variance. Recent methods restrict the per-step reward variance as a proxy. We thoroughly examine the limitations of these variance-based methods, such as sensitivity to numerical scale and hindering of policy learning, and propose to use an alternative risk measure, Gini deviation, as a substitute. We study various properties of this new risk measure and derive a policy gradient algorithm to minimize it. Empirical evaluation in domains where risk-aversion can be clearly defined, shows that our algorithm can mitigate the limitations of variance-based risk measures and achieves high return with low risk in terms of variance and Gini deviation when others fail to learn a reasonable policy.
</details>
<details>
<summary>摘要</summary>
限制策略回报的方差是现代学习控制（RL）中的一个受欢迎选择，因为它的数学定义具有明确的定义和易于理解的解释。传统方法直接限制总返回方差。最近的方法则限制每步奖励方差作为代理。我们详细检查了这些方差基于的方法的局限性，如数字化缩放的感度和策略学习妨碍，并提出一种新的风险度量，即吉尼度偏离，作为替代。我们研究了这种新的风险度量的多种性质，并 derivation 一种策略梯度算法来最小化它。实验表明，当其他策略不能学习合理的策略时，我们的算法可以减轻方差基于的风险度量的局限性，并在 variance 和吉尼度偏离方面实现高回报低风险。
</details></li>
</ul>
<hr>
<h2 id="Meta-Value-Learning-a-General-Framework-for-Learning-with-Learning-Awareness"><a href="#Meta-Value-Learning-a-General-Framework-for-Learning-with-Learning-Awareness" class="headerlink" title="Meta-Value Learning: a General Framework for Learning with Learning Awareness"></a>Meta-Value Learning: a General Framework for Learning with Learning Awareness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08863">http://arxiv.org/abs/2307.08863</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/metavaluelearning/metavaluelearning">https://github.com/metavaluelearning/metavaluelearning</a></li>
<li>paper_authors: Tim Cooijmans, Milad Aghajohari, Aaron Courville</li>
<li>For: 本 paper 的目的是解决多体系统中的梯度学习问题，因为梯度来自于一个第一个模型，这个模型不会考虑多体间学习过程的交互。* Methods: 本 paper extend了 LOLA 的想法，开发了一种全面的值基定义优化方法。这种方法的核心是一个我们称为元价值函数，它在每个 JOINT-policy 空间中为每个代理给出一个折抵负号的优化目标。我们 argue 这个梯度比原始目标更可靠，因为元价值函数来自于优化过程中的实际观察。* Results: 我们通过对 Logistic Game 和 Iterated Prisoner’s Dilemma 两个问题进行分析，显示了我们的方法的行为。<details>
<summary>Abstract</summary>
Gradient-based learning in multi-agent systems is difficult because the gradient derives from a first-order model which does not account for the interaction between agents' learning processes. LOLA (arXiv:1709.04326) accounts for this by differentiating through one step of optimization. We extend the ideas of LOLA and develop a fully-general value-based approach to optimization. At the core is a function we call the meta-value, which at each point in joint-policy space gives for each agent a discounted sum of its objective over future optimization steps. We argue that the gradient of the meta-value gives a more reliable improvement direction than the gradient of the original objective, because the meta-value derives from empirical observations of the effects of optimization. We show how the meta-value can be approximated by training a neural network to minimize TD error along optimization trajectories in which agents follow the gradient of the meta-value. We analyze the behavior of our method on the Logistic Game and on the Iterated Prisoner's Dilemma.
</details>
<details>
<summary>摘要</summary>
gradient-based learning in multi-agent systems 困难，因为梯度来自于一个第一阶模型，这个模型不考虑多个代理机器学习过程之间的互动。LOLA（arXiv:1709.04326）提出了一种解决方案，通过一步优化差分。我们在LOLA的基础上发展了一种完全普遍的价值基于方法，其核心是一个我们称为“元价值”的函数，每个代理机器在联合策略空间中的每个点处给每个代理机器一个折扣的未来优化步骤中的目标减少和。我们 argue that the gradient of the meta-value gives a more reliable improvement direction than the gradient of the original objective, because the meta-value derives from empirical observations of the effects of optimization. We show how the meta-value can be approximated by training a neural network to minimize TD error along optimization trajectories in which agents follow the gradient of the meta-value。我们分析了我们的方法在Logistic Game和Iterated Prisoner's Dilemma中的行为。
</details></li>
</ul>
<hr>
<h2 id="Curriculum-Learning-for-Graph-Neural-Networks-A-Multiview-Competence-based-Approach"><a href="#Curriculum-Learning-for-Graph-Neural-Networks-A-Multiview-Competence-based-Approach" class="headerlink" title="Curriculum Learning for Graph Neural Networks: A Multiview Competence-based Approach"></a>Curriculum Learning for Graph Neural Networks: A Multiview Competence-based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08859">http://arxiv.org/abs/2307.08859</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/CLU-UML/MCCL">https://github.com/CLU-UML/MCCL</a></li>
<li>paper_authors: Nidhi Vakil, Hadi Amiri</li>
<li>for: 本研究旨在提出一种基于图复杂度形式和模型能力的新方法，以便在图神经网络训练中进行有效的课程学习。</li>
<li>methods: 该方法使用了一种调度方案，以确定有效的课程，并考虑了不同的图Difficulty标准和模型能力 durante el entrenamiento。</li>
<li>results: 实验结果表明，该方法可以在真实世界的链接预测和节点分类任务中提供更高的效果，比如使用多个图Difficulty标准和模型能力来评估模型的性能。<details>
<summary>Abstract</summary>
A curriculum is a planned sequence of learning materials and an effective one can make learning efficient and effective for both humans and machines. Recent studies developed effective data-driven curriculum learning approaches for training graph neural networks in language applications. However, existing curriculum learning approaches often employ a single criterion of difficulty in their training paradigms. In this paper, we propose a new perspective on curriculum learning by introducing a novel approach that builds on graph complexity formalisms (as difficulty criteria) and model competence during training. The model consists of a scheduling scheme which derives effective curricula by accounting for different views of sample difficulty and model competence during training. The proposed solution advances existing research in curriculum learning for graph neural networks with the ability to incorporate a fine-grained spectrum of graph difficulty criteria in their training paradigms. Experimental results on real-world link prediction and node classification tasks illustrate the effectiveness of the proposed approach.
</details>
<details>
<summary>摘要</summary>
一个课程是一个规划的学习材料序列，一个有效的课程可以使学习变得更加效率和有效。现代研究已经开发出了训练图型神经网络的数据驱动课程学习方法。然而，现有的课程学习方法通常使用单一的难度标准来训练。在这篇论文中，我们提出了一个新的观点，即基于图型复杂性形式（难度标准）和模型能力的课程学习方法。我们的方法包括一个时间表，它可以从训练中的不同角度来评估题目难度和模型能力，并从中 derivate 有效的课程。我们的解决方案超越了现有的课程学习研究，可以将图型难度标准细分为训练中的不同角度。实验结果显示，我们的方法在实际的连接预测和节点分类任务中具有优秀的效果。
</details></li>
</ul>
<hr>
<h2 id="An-Admissible-Shift-Consistent-Method-for-Recommender-Systems"><a href="#An-Admissible-Shift-Consistent-Method-for-Recommender-Systems" class="headerlink" title="An Admissible Shift-Consistent Method for Recommender Systems"></a>An Admissible Shift-Consistent Method for Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08857">http://arxiv.org/abs/2307.08857</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tung Nguyen, Jeffrey Uhlmann</li>
<li>for:  solves matrix&#x2F;tensor completion problems in the context of recommender systems</li>
<li>methods:  proposes a new constraint called shift-consistency, and provides a rigorous mathematical description of the method</li>
<li>results:  provably guarantees several key mathematical properties, including satisfaction of an admissibility criterion, fairness, and robustness<details>
<summary>Abstract</summary>
In this paper, we propose a new constraint, called shift-consistency, for solving matrix/tensor completion problems in the context of recommender systems. Our method provably guarantees several key mathematical properties: (1) satisfies a recently established admissibility criterion for recommender systems; (2) satisfies a definition of fairness that eliminates a specific class of potential opportunities for users to maliciously influence system recommendations; and (3) offers robustness by exploiting provable uniqueness of missing-value imputation. We provide a rigorous mathematical description of the method, including its generalization from matrix to tensor form to permit representation and exploitation of complex structural relationships among sets of user and product attributes. We argue that our analysis suggests a structured means for defining latent-space projections that can permit provable performance properties to be established for machine learning methods.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一个新的约束，称为偏移一致性，用于解决Matrix/Tensor completion问题在推荐系统中。我们的方法可以证明满足以下几个关键数学性质：（1）满足推荐系统中最近确立的适用性标准;（2）满足一种定义的公平性，以消除用户恶意影响推荐系统的可能性;（3）具有耐用性，通过利用缺失值填充的可证明唯一性来抗衡。我们提供了一个严格的数学描述，包括矩阵到多重形式的普遍化，以利用用户和产品特征之间的复杂结构关系。我们认为，我们的分析表明了一种结构化的方式，可以让 latent-space 投影具有可证明性能特性。
</details></li>
</ul>
<hr>
<h2 id="Autoregressive-Diffusion-Model-for-Graph-Generation"><a href="#Autoregressive-Diffusion-Model-for-Graph-Generation" class="headerlink" title="Autoregressive Diffusion Model for Graph Generation"></a>Autoregressive Diffusion Model for Graph Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08849">http://arxiv.org/abs/2307.08849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lingkai Kong, Jiaming Cui, Haotian Sun, Yuchen Zhuang, B. Aditya Prakash, Chao Zhang</li>
<li>for: 本文提出了一种束缚基于模型 для图形生成。</li>
<li>methods: 该模型使用自适应扩散过程，直接在零域图空间中操作。在前向扩散过程中，我们设计了一个数据依赖的节点吸引排序网络，用于学习图的排序。在反向生成过程中，我们设计了一个减噪网络，用于高效地重建图。</li>
<li>results: 我们在六种不同的通用图数据集和两种分子数据集上进行了实验，结果显示，我们的模型可以与之前的状态地图形成比或更好，同时具有快速的生成速度。<details>
<summary>Abstract</summary>
Diffusion-based graph generative models have recently obtained promising results for graph generation. However, existing diffusion-based graph generative models are mostly one-shot generative models that apply Gaussian diffusion in the dequantized adjacency matrix space. Such a strategy can suffer from difficulty in model training, slow sampling speed, and incapability of incorporating constraints. We propose an \emph{autoregressive diffusion} model for graph generation. Unlike existing methods, we define a node-absorbing diffusion process that operates directly in the discrete graph space. For forward diffusion, we design a \emph{diffusion ordering network}, which learns a data-dependent node absorbing ordering from graph topology. For reverse generation, we design a \emph{denoising network} that uses the reverse node ordering to efficiently reconstruct the graph by predicting the node type of the new node and its edges with previously denoised nodes at a time. Based on the permutation invariance of graph, we show that the two networks can be jointly trained by optimizing a simple lower bound of data likelihood. Our experiments on six diverse generic graph datasets and two molecule datasets show that our model achieves better or comparable generation performance with previous state-of-the-art, and meanwhile enjoys fast generation speed.
</details>
<details>
<summary>摘要</summary>
Diffusion-based图生成模型在最近得到了优秀的结果，但现有的扩散基于的图生成模型多数是一次性的生成模型，它们在减量化的相对位图空间内应用扩散。这种策略可能会受到训练模型的困难，慢速的采样速度和约束的不具备。我们提出了一种“自适应扩散”模型，与现有方法不同，我们在离散图空间直接定义节点吸引扩散过程。对于前进扩散，我们设计了一个“扩散排序网络”，它学习从图ptopology得到数据依赖的节点吸引排序。对于逆生成，我们设计了一个“除噪网络”，它使用反向节点排序来高效地重建图， predicting the node type of the new node and its edges with previously denoised nodes at a time。基于图的幂等性，我们表明了这两个网络可以同时训练，通过优化数据可能性函数的简单下界来优化。我们在六种多样化的生成图据集和两个分子数据集上进行了实验，结果表明我们的模型可以与之前的状态时的性能相当或更好，同时具有快速的生成速度。
</details></li>
</ul>
<hr>
<h2 id="Privacy-preserving-patient-clustering-for-personalized-federated-learning"><a href="#Privacy-preserving-patient-clustering-for-personalized-federated-learning" class="headerlink" title="Privacy-preserving patient clustering for personalized federated learning"></a>Privacy-preserving patient clustering for personalized federated learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08847">http://arxiv.org/abs/2307.08847</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/g2lab/pcfbl">https://github.com/g2lab/pcfbl</a></li>
<li>paper_authors: Ahmed Elhussein, Gamze Gursoy</li>
<li>For: 这个研究旨在解决 Federated Learning (FL) 中 data 非同一性 Independent Distribution (non-IID) 问题，并提出 Privacy-preserving Community-Based Federated machine Learning (PCBFL) 框架，可以在不同医院中训练分组学习模型，并保护隐私。* Methods: PCBFL 使用 Secure Multiparty Computation (SMPC) 技术，可以安全地计算不同医院中病人的相似性分数，并使用 clustering 算法将病人分组。* Results: PCBFL 可以成功地将病人分组为低、中、高风险三种群体，并与传统和现有的 Clustered FL 框架进行比较，获得了平均 AUC 提升率4.3% 和 AUPRC 提升率7.8%。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a machine learning framework that enables multiple organizations to train a model without sharing their data with a central server. However, it experiences significant performance degradation if the data is non-identically independently distributed (non-IID). This is a problem in medical settings, where variations in the patient population contribute significantly to distribution differences across hospitals. Personalized FL addresses this issue by accounting for site-specific distribution differences. Clustered FL, a Personalized FL variant, was used to address this problem by clustering patients into groups across hospitals and training separate models on each group. However, privacy concerns remained as a challenge as the clustering process requires exchange of patient-level information. This was previously solved by forming clusters using aggregated data, which led to inaccurate groups and performance degradation. In this study, we propose Privacy-preserving Community-Based Federated machine Learning (PCBFL), a novel Clustered FL framework that can cluster patients using patient-level data while protecting privacy. PCBFL uses Secure Multiparty Computation, a cryptographic technique, to securely calculate patient-level similarity scores across hospitals. We then evaluate PCBFL by training a federated mortality prediction model using 20 sites from the eICU dataset. We compare the performance gain from PCBFL against traditional and existing Clustered FL frameworks. Our results show that PCBFL successfully forms clinically meaningful cohorts of low, medium, and high-risk patients. PCBFL outperforms traditional and existing Clustered FL frameworks with an average AUC improvement of 4.3% and AUPRC improvement of 7.8%.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种机器学习框架，允许多个组织共同训练模型，无需将数据分享到中央服务器。然而，如果数据不是非 identical independently distributed (non-IID)，FL 会经受显著性能下降。这是医疗设置中的问题，Variations in the patient population contribute significantly to distribution differences across hospitals。personalized FL 解决了这个问题，通过考虑各地点特定的分布差异。clustered FL，一种个人化 FL 变体，使用 clustering 方法将患者分组，并在每个组上训练 separating 模型。然而，隐私问题仍然成为挑战，因为 clustering 过程需要交换患者级别信息。这已经解决了通过使用聚合数据来组成 clusters，但这会导致不准确的组和性能下降。在本研究中，我们提出了隐私保护的社区基于 Federated 机器学习 (PCBFL)，一种新的 clustering FL 框架，可以在患者级别数据上 clustering 患者，同时保护隐私。PCBFL 使用 Secure Multiparty Computation，一种密码学技术，以安全地计算各地点患者相似度分数。我们然后评估 PCBFL，通过在 20 个 eICU 数据集中训练一个联邦 Mortality 预测模型。我们比较 PCBFL 的性能与传统和现有的 clustering FL 框架。我们的结果表明，PCBFL 成功划分了低、中、高风险患者的临床意义full cohort。PCBFL 与传统和现有的 clustering FL 框架相比，平均 AUC 提高4.3%，AUPRC 提高7.8%。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Safe-Policy-Learning-with-Chance-Constrained-Optimization-Application-to-Military-Security-Assessment-during-the-Vietnam-War"><a href="#Bayesian-Safe-Policy-Learning-with-Chance-Constrained-Optimization-Application-to-Military-Security-Assessment-during-the-Vietnam-War" class="headerlink" title="Bayesian Safe Policy Learning with Chance Constrained Optimization: Application to Military Security Assessment during the Vietnam War"></a>Bayesian Safe Policy Learning with Chance Constrained Optimization: Application to Military Security Assessment during the Vietnam War</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08840">http://arxiv.org/abs/2307.08840</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyang Jia, Eli Ben-Michael, Kosuke Imai</li>
<li>For: The paper aims to improve a security assessment algorithm used during the Vietnam War by using outcomes measured immediately after its introduction in late 1969.* Methods: The paper introduces the Average Conditional Risk (ACRisk) to quantify the risk of worse outcomes for subgroups of individual units, and a Bayesian policy learning framework to maximize the posterior expected value while controlling the ACRisk.* Results: The learned algorithm assesses most regions as more secure and emphasizes economic and political factors over military factors, compared to the actual algorithm used during the Vietnam War.Here are the three points in Simplified Chinese text:* For: 该文章目标是通过1969年底引入的出口来改进越南战争期间安全评估算法。* Methods: 该文章提出了 Conditional Risk (ACRisk) 来衡量各个单位 subgroup 的输出风险，以及 Bayesian 政策学习框架来控制 ACrisk 并最大化 posterior 期望值。* Results: 学习的算法认为大多数地区更安全，并且强调经济和政治因素比军事因素更重要，与实际使用的算法不同。<details>
<summary>Abstract</summary>
Algorithmic and data-driven decisions and recommendations are commonly used in high-stakes decision-making settings such as criminal justice, medicine, and public policy. We investigate whether it would have been possible to improve a security assessment algorithm employed during the Vietnam War, using outcomes measured immediately after its introduction in late 1969. This empirical application raises several methodological challenges that frequently arise in high-stakes algorithmic decision-making. First, before implementing a new algorithm, it is essential to characterize and control the risk of yielding worse outcomes than the existing algorithm. Second, the existing algorithm is deterministic, and learning a new algorithm requires transparent extrapolation. Third, the existing algorithm involves discrete decision tables that are common but difficult to optimize over.   To address these challenges, we introduce the Average Conditional Risk (ACRisk), which first quantifies the risk that a new algorithmic policy leads to worse outcomes for subgroups of individual units and then averages this over the distribution of subgroups. We also propose a Bayesian policy learning framework that maximizes the posterior expected value while controlling the posterior expected ACRisk. This framework separates the estimation of heterogeneous treatment effects from policy optimization, enabling flexible estimation of effects and optimization over complex policy classes. We characterize the resulting chance-constrained optimization problem as a constrained linear programming problem. Our analysis shows that compared to the actual algorithm used during the Vietnam War, the learned algorithm assesses most regions as more secure and emphasizes economic and political factors over military factors.
</details>
<details>
<summary>摘要</summary>
高科技和数据驱动的决策和建议在高风险决策场景中广泛应用，如刑事司法、医疗和公共政策。我们调查了越南战争期间使用的安全评估算法是否可以改进，使用实际实施后的1969年底的结果。这种实践中出现了一些高风险算法决策中常见的方法学挑战。首先，在实施新算法之前，需要评估和控制新算法可能导致差化的风险。第二，现有的算法是 deterministic，需要透明地推断新算法。第三，现有的算法包含精确的决策表，这些表difficult to optimize。为了解决这些挑战，我们引入了 Conditional Risk (ACRisk)，它首先评估新算法政策对各个单位的 subgroup 的风险差化，然后平均这些风险。我们还提出了 Bayesian 政策学习框架，该框架在控制 posterior 预期值时最大化预期值，并且可以灵活地估计影响和优化复杂的政策类型。我们将这种机会constrained optimization问题 characterized as a linear programming problem。我们的分析表明，相比 actual algorithm 使用 during the Vietnam War, the learned algorithm assesses most regions as more secure and emphasizes economic and political factors over military factors。
</details></li>
</ul>
<hr>
<h2 id="A-Meta-Learning-Based-Precoder-Optimization-Framework-for-Rate-Splitting-Multiple-Access"><a href="#A-Meta-Learning-Based-Precoder-Optimization-Framework-for-Rate-Splitting-Multiple-Access" class="headerlink" title="A Meta-Learning Based Precoder Optimization Framework for Rate-Splitting Multiple Access"></a>A Meta-Learning Based Precoder Optimization Framework for Rate-Splitting Multiple Access</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08822">http://arxiv.org/abs/2307.08822</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rafael Cerna Loli, Bruno Clerckx</li>
<li>for: 提出一种基于元学习的RSMA前处理优化框架，直接在无法知道整个通道状态情况的情况下优化RSMA前处理器。</li>
<li>methods: 利用含义过拟合的卷积神经网络来最大化显式均值总bitrate表达式，从而绕过需要其他训练数据的限制。</li>
<li>results: 数值结果显示，元学习基于的解决方案在中等规模场景下与传统前处理优化相当，在大规模场景下明显超越低复杂度前处理算法。<details>
<summary>Abstract</summary>
In this letter, we propose the use of a meta-learning based precoder optimization framework to directly optimize the Rate-Splitting Multiple Access (RSMA) precoders with partial Channel State Information at the Transmitter (CSIT). By exploiting the overfitting of the compact neural network to maximize the explicit Average Sum-Rate (ASR) expression, we effectively bypass the need for any other training data while minimizing the total running time. Numerical results reveal that the meta-learning based solution achieves similar ASR performance to conventional precoder optimization in medium-scale scenarios, and significantly outperforms sub-optimal low complexity precoder algorithms in the large-scale regime.
</details>
<details>
<summary>摘要</summary>
在这封信中，我们提议使用基于meta-学习的precoder优化框架来直接优化Rate-Splitting Multiple Access（RSMA）precoder，使用 transmitter （CSIT）中的部分通道状态信息。通过利用Compact Neural Network的过度适应来最大化显式Average Sum-Rate（ASR）表达，我们可以快速减少训练数据量，同时减少总耗时。 numerically 的结果表明，基于meta-学习的解决方案在中型情景下与传统precoder优化相似的ASR性能，而在大规模情景下明显超过低复杂度precoder算法。Here's the translation of the text into Traditional Chinese:在这封信中，我们提议使用基于meta-学习的precoder优化框架来直接优化Rate-Splitting Multiple Access（RSMA）precoder，使用传递器（CSIT）中的部分通道状态信息。通过利用Compact Neural Network的过度适应来最大化显式Average Sum-Rate（ASR）表达，我们可以快速减少训练数据量，同时减少总耗时。numerically 的结果显示，基于meta-学习的解决方案在中型情景下与传统precoder优化相似的ASR性能，而在大规模情景下明显超过低复杂度precoder算法。
</details></li>
</ul>
<hr>
<h2 id="Towards-Accelerating-Benders-Decomposition-via-Reinforcement-Learning-Surrogate-Models"><a href="#Towards-Accelerating-Benders-Decomposition-via-Reinforcement-Learning-Surrogate-Models" class="headerlink" title="Towards Accelerating Benders Decomposition via Reinforcement Learning Surrogate Models"></a>Towards Accelerating Benders Decomposition via Reinforcement Learning Surrogate Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08816">http://arxiv.org/abs/2307.08816</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Mak, Kyle Mana, Parisa Zehtabi, Michael Cashmore, Daniele Magazzeni, Manuela Veloso</li>
<li>for: 这篇论文是为了提出一种快速化Benders decomposition（BD）方法，以解决受不确定性影响的数值优化问题。</li>
<li>methods: 本论文使用的方法是BD方法，并利用一个代理模型来取代NP困难的整数主问题，以加速BD方法的执行。</li>
<li>results: 在实验中，这种加速BD方法可以让解决随机存储管理问题的时间提高30%，比其他加速BD实现方法更快。<details>
<summary>Abstract</summary>
Stochastic optimization (SO) attempts to offer optimal decisions in the presence of uncertainty. Often, the classical formulation of these problems becomes intractable due to (a) the number of scenarios required to capture the uncertainty and (b) the discrete nature of real-world planning problems. To overcome these tractability issues, practitioners turn to decomposition methods that divide the problem into smaller, more tractable sub-problems. The focal decomposition method of this paper is Benders decomposition (BD), which decomposes stochastic optimization problems on the basis of scenario independence. In this paper we propose a method of accelerating BD with the aid of a surrogate model in place of an NP-hard integer master problem. Through the acceleration method we observe 30% faster average convergence when compared to other accelerated BD implementations. We introduce a reinforcement learning agent as a surrogate and demonstrate how it can be used to solve a stochastic inventory management problem.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose a method for accelerating Benders decomposition (BD), a popular decomposition method for stochastic optimization problems, using a surrogate model in place of the NP-hard integer master problem. Our method leverages a reinforcement learning agent as the surrogate and demonstrates its effectiveness in solving a stochastic inventory management problem. We observe an average convergence rate 30% faster than other accelerated BD implementations.Here is the text in Simplified Chinese: Stochastic optimization (SO) 尝试提供在不确定环境中的优化决策。然而， classical 的问题表述方式可能会变得不可求解，因为需要大量的enario来捕捉不确定性，以及实际规划问题的精度问题。为了解决这些 tractability 问题，专家们经常使用分解方法，将问题分解成更加可控的子问题。在这篇论文中，我们提出了使用代理模型加速 Benders 分解（BD）的方法。我们使用 reinforcement learning 代理来解决一个不确定存储管理问题。我们观察到，使用这种加速方法可以比其他加速BD实现的平均速度提高30%。
</details></li>
</ul>
<hr>
<h2 id="Comparative-Performance-Evaluation-of-Large-Language-Models-for-Extracting-Molecular-Interactions-and-Pathway-Knowledge"><a href="#Comparative-Performance-Evaluation-of-Large-Language-Models-for-Extracting-Molecular-Interactions-and-Pathway-Knowledge" class="headerlink" title="Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge"></a>Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08813">http://arxiv.org/abs/2307.08813</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/boxorange/bioie-llm">https://github.com/boxorange/bioie-llm</a></li>
<li>paper_authors: Gilchan Park, Byung-Jun Yoon, Xihaier Luo, Vanessa López-Marrero, Patrick Johnstone, Shinjae Yoo, Francis J. Alexander</li>
<li>for: 这项研究的目的是使用大型自然语言模型来自动从科学文献中提取蛋白质相互作用、蛋白质通路和基因调控关系的知识。</li>
<li>methods: 本研究使用了不同的大型自然语言模型来完成蛋白质相互作用、蛋白质通路和基因调控关系的识别任务。</li>
<li>results: 研究发现了不同的大型自然语言模型在完成这些任务时的效果，并提供了一些显著的发现和未来的机会，以及仍然存在的挑战。In English, it means:</li>
<li>for: The goal of this study is to use large language models to automatically extract knowledge of protein interactions, pathways, and gene regulatory relations from scientific literature.</li>
<li>methods: The study uses different large language models to complete tasks of recognizing protein interactions, pathways, and gene regulatory relations.</li>
<li>results: The study finds the effectiveness of different language models in completing these tasks, provides significant findings, and discusses future opportunities and remaining challenges.<details>
<summary>Abstract</summary>
Understanding protein interactions and pathway knowledge is crucial for unraveling the complexities of living systems and investigating the underlying mechanisms of biological functions and complex diseases. While existing databases provide curated biological data from literature and other sources, they are often incomplete and their maintenance is labor-intensive, necessitating alternative approaches. In this study, we propose to harness the capabilities of large language models to address these issues by automatically extracting such knowledge from the relevant scientific literature. Toward this goal, in this work, we investigate the effectiveness of different large language models in tasks that involve recognizing protein interactions, pathways, and gene regulatory relations. We thoroughly evaluate the performance of various models, highlight the significant findings, and discuss both the future opportunities and the remaining challenges associated with this approach. The code and data are available at: https://github.com/boxorange/BioIE-LLM
</details>
<details>
<summary>摘要</summary>
理解蛋白交互和生物路径知识是生物系统复杂性的关键，帮助我们探索生物功能和复杂疾病的基础机理。现有数据库提供了文献和其他来源中的生物数据，但这些数据库经常受到不完整性和维护劳动的限制，需要新的方法。在本研究中，我们利用大型自然语言模型来解决这些问题，自动从相关的科学文献中提取生物知识。为达到这个目标，我们在这篇论文中评估了不同的大型自然语言模型在蛋白交互、生物路径和蛋白质调控关系的识别任务中的效果。我们仔细评估了各模型的表现，披露了重要的发现，并讨论了这种方法的未来机会和仍然存在的挑战。代码和数据可以在 GitHub 上获取：<https://github.com/boxorange/BioIE-LLM>。
</details></li>
</ul>
<hr>
<h2 id="DeepMem-ML-Models-as-storage-channels-and-their-mis-applications"><a href="#DeepMem-ML-Models-as-storage-channels-and-their-mis-applications" class="headerlink" title="DeepMem: ML Models as storage channels and their (mis-)applications"></a>DeepMem: ML Models as storage channels and their (mis-)applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08811">http://arxiv.org/abs/2307.08811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Abdullah Al Mamun, Quazi Mishkatul Alam, Erfan Shaigani, Pedram Zaree, Ihsen Alouani, Nael Abu-Ghazaleh</li>
<li>for: 本文提出了一种新的信息理论视角，视 ML 模型为一个存储通道，并研究了在这个存储通道上进行隐藏信息的存储和检测。</li>
<li>methods: 作者使用了一种黑盒访问方式，通过在训练时嵌入隐藏信息，并在部署后使用黑盒访问来检测和提取隐藏信息。</li>
<li>results: 作者分析了存储 primitives 和检测 primitives，并提出了一种基于 ML 特有的替换基于错误 correction 协议来提高存储 primitives 的可靠性。<details>
<summary>Abstract</summary>
Machine learning (ML) models are overparameterized to support generality and avoid overfitting. Prior works have shown that these additional parameters can be used for both malicious (e.g., hiding a model covertly within a trained model) and beneficial purposes (e.g., watermarking a model). In this paper, we propose a novel information theoretic perspective of the problem; we consider the ML model as a storage channel with a capacity that increases with overparameterization. Specifically, we consider a sender that embeds arbitrary information in the model at training time, which can be extracted by a receiver with a black-box access to the deployed model. We derive an upper bound on the capacity of the channel based on the number of available parameters. We then explore black-box write and read primitives that allow the attacker to: (i) store data in an optimized way within the model by augmenting the training data at the transmitter side, and (ii) to read it by querying the model after it is deployed. We also analyze the detectability of the writing primitive and consider a new version of the problem which takes information storage covertness into account. Specifically, to obtain storage covertness, we introduce a new constraint such that the data augmentation used for the write primitives minimizes the distribution shift with the initial (baseline task) distribution. This constraint introduces a level of "interference" with the initial task, thereby limiting the channel's effective capacity. Therefore, we develop optimizations to improve the capacity in this case, including a novel ML-specific substitution based error correction protocol. We believe that the proposed modeling of the problem offers new tools to better understand and mitigate potential vulnerabilities of ML, especially in the context of increasingly large models.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:机器学习（ML）模型通常过过参数化来支持通用性和避免过拟合。先前的研究表明，这些额外参数可以用于both malicious（如隐藏一个模型在训练过程中）和有益目的（如模型水印）。在这篇论文中，我们提出了一种新的信息理论视角，视ML模型为一个存储通道，其容量随参数的增加而增加。 Specifically，我们考虑一个发送者在训练时将自定义信息嵌入模型中，并且通过黑盒访问已部署模型来提取这些信息。我们得出了参数的容量的上限，并explore黑盒写和读 primitives，allowing the attacker to: (i) 在发送方 сторо面优化数据，以便在部署后通过模型进行读取，和 (ii) 通过访问部署后的模型来读取数据。我们还分析了写 primitives的检测性，并考虑了一个新的问题，即存储隐蔽性。 Specifically, to obtain storage covertness, we introduce a new constraint such that the data augmentation used for the write primitives minimizes the distribution shift with the initial (baseline task) distribution. This constraint introduces a level of "interference" with the initial task, thereby limiting the channel's effective capacity. Therefore, we develop optimizations to improve the capacity in this case, including a novel ML-specific substitution based error correction protocol. We believe that the proposed modeling of the problem offers new tools to better understand and mitigate potential vulnerabilities of ML, especially in the context of increasingly large models.Translated into Traditional Chinese:机器学习（ML）模型通常过过参数化来支持通用性和避免过拟合。先前的研究表明，这些额外参数可以用于both malicious（如隐藏一个模型在训练过程中）和有益目的（如模型水印）。在这篇论文中，我们提出了一个新的信息理论角度，视ML模型为一个存储通道，其容量随参数的增加而增加。 Specifically，我们考虑一个发送者在训练时将自定义信息嵌入模型中，并且透过黑盒访问已部署模型来提取这些信息。我们得出了参数的容量的上限，并explore黑盒写和读 primitives，allowing the attacker to: (i) 在发送方 сторо面优化数据，以便在部署后通过模型进行读取，和 (ii) 通过访问部署后的模型来读取数据。我们还分析了写 primitives的检测性，并考虑了一个新的问题，即存储隐蔽性。 Specifically, to obtain storage covertness, we introduce a new constraint such that the data augmentation used for the write primitives minimizes the distribution shift with the initial (baseline task) distribution. This constraint introduces a level of "interference" with the initial task, thereby limiting the channel's effective capacity. Therefore, we develop optimizations to improve the capacity in this case, including a novel ML-specific substitution based error correction protocol. We believe that the proposed modeling of the problem offers new tools to better understand and mitigate potential vulnerabilities of ML, especially in the context of increasingly large models.
</details></li>
</ul>
<hr>
<h2 id="Operator-Guidance-Informed-by-AI-Augmented-Simulations"><a href="#Operator-Guidance-Informed-by-AI-Augmented-Simulations" class="headerlink" title="Operator Guidance Informed by AI-Augmented Simulations"></a>Operator Guidance Informed by AI-Augmented Simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08810">http://arxiv.org/abs/2307.08810</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samuel J. Edwards, Michael Levine</li>
<li>for: 这个论文是为了计算船舶响应统计数据的多优化、数据适应方法。</li>
<li>methods: 这个论文使用了Long Short-Term Memory（LSTM）神经网络，以及一个快速低精度的计算工具SimpleCode，以及一个更高精度的计算工具Large Amplitude Motion Program（LAMP）。</li>
<li>results: 研究发现，使用LSTM神经网络可以准确地估计船舶响应统计数据，并且可以在不同的海洋条件下提供高精度的结果。<details>
<summary>Abstract</summary>
This paper will present a multi-fidelity, data-adaptive approach with a Long Short-Term Memory (LSTM) neural network to estimate ship response statistics in bimodal, bidirectional seas. The study will employ a fast low-fidelity, volume-based tool SimpleCode and a higher-fidelity tool known as the Large Amplitude Motion Program (LAMP). SimpleCode and LAMP data were generated by common bi-modal, bi-directional sea conditions in the North Atlantic as training data. After training an LSTM network with LAMP ship motion response data, a sample route was traversed and randomly sampled historical weather was input into SimpleCode and the LSTM network, and compared against the higher fidelity results.
</details>
<details>
<summary>摘要</summary>
这篇论文将介绍一种多模精度、数据适应的方法，使用长期快速响应（LSTM）神经网络来估算船舶响应统计在双模态、双向海域中。这项研究将使用快速低精度的SimpleCode工具和高精度的Large Amplitude Motion Program（LAMP）工具。SimpleCode和LAMP数据都是通过共同的双模态、双向海域条件在北大西洋中生成的训练数据。 после训练LSTM网络使用LAMP船舶运动数据，一个示例路线被跨越，并将SimpleCode和LSTM网络输入历史气象数据，并与更高精度结果进行比较。
</details></li>
</ul>
<hr>
<h2 id="Local-or-Global-Selective-Knowledge-Assimilation-for-Federated-Learning-with-Limited-Labels"><a href="#Local-or-Global-Selective-Knowledge-Assimilation-for-Federated-Learning-with-Limited-Labels" class="headerlink" title="Local or Global: Selective Knowledge Assimilation for Federated Learning with Limited Labels"></a>Local or Global: Selective Knowledge Assimilation for Federated Learning with Limited Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08809">http://arxiv.org/abs/2307.08809</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yae Jee Cho, Gauri Joshi, Dimitrios Dimitriadis</li>
<li>for: 提高 federated learning 的效果，尤其是在 client 有限的标签数据的情况下。</li>
<li>methods: 提出 FedLabel 方法，使 client 可以选择本地或全局模型来pseudo-标签未标签数据，并通过全局-本地一致常量正则化来利用两个模型的知识。</li>
<li>results: 在 cross-device 和 cross-silo  Setting 中，FedLabel 比其他 semi-supervised FL 基线方法提高 $8$-$24%$，甚至超过了标准全部标签 FL 基线($100%$ 标签数据)，只使用 $5$-$20%$ 的标签数据。<details>
<summary>Abstract</summary>
Many existing FL methods assume clients with fully-labeled data, while in realistic settings, clients have limited labels due to the expensive and laborious process of labeling. Limited labeled local data of the clients often leads to their local model having poor generalization abilities to their larger unlabeled local data, such as having class-distribution mismatch with the unlabeled data. As a result, clients may instead look to benefit from the global model trained across clients to leverage their unlabeled data, but this also becomes difficult due to data heterogeneity across clients. In our work, we propose FedLabel where clients selectively choose the local or global model to pseudo-label their unlabeled data depending on which is more of an expert of the data. We further utilize both the local and global models' knowledge via global-local consistency regularization which minimizes the divergence between the two models' outputs when they have identical pseudo-labels for the unlabeled data. Unlike other semi-supervised FL baselines, our method does not require additional experts other than the local or global model, nor require additional parameters to be communicated. We also do not assume any server-labeled data or fully labeled clients. For both cross-device and cross-silo settings, we show that FedLabel outperforms other semi-supervised FL baselines by $8$-$24\%$, and even outperforms standard fully supervised FL baselines ($100\%$ labeled data) with only $5$-$20\%$ of labeled data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Anomaly-Detection-with-Selective-Dictionary-Learning"><a href="#Anomaly-Detection-with-Selective-Dictionary-Learning" class="headerlink" title="Anomaly Detection with Selective Dictionary Learning"></a>Anomaly Detection with Selective Dictionary Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08807">http://arxiv.org/abs/2307.08807</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/denisilie94/pyod-dl">https://github.com/denisilie94/pyod-dl</a></li>
<li>paper_authors: Denis C. Ilie-Ablachim, Bogdan Dumitrescu</li>
<li>for: 本研究提出了基于词典学习（DL）和核心词典学习（KDL）的新型异常检测方法。</li>
<li>methods: 本研究使用了已知的DL和KDL算法，并将其改进为无监督的异常检测方法。此外，我们还提出了一种减少kernel版本（RKDL），用于解决大数据集问题。</li>
<li>results: 我们的算法在一个异常检测工具箱中引入，并与标准 referéncé результаты进行比较。<details>
<summary>Abstract</summary>
In this paper we present new methods of anomaly detection based on Dictionary Learning (DL) and Kernel Dictionary Learning (KDL). The main contribution consists in the adaption of known DL and KDL algorithms in the form of unsupervised methods, used for outlier detection. We propose a reduced kernel version (RKDL), which is useful for problems with large data sets, due to the large kernel matrix. We also improve the DL and RKDL methods by the use of a random selection of signals, which aims to eliminate the outliers from the training procedure. All our algorithms are introduced in an anomaly detection toolbox and are compared to standard benchmark results.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了基于字典学习（DL）和核函数字典学习（KDL）的新方法，用于异常检测。我们的主要贡献在于将知名的DL和KDL算法改进为无监督的方法，用于异常检测。我们还提出了一种减小核kernel版本（RKDL），适用于具有大数据集的问题，因为大 kernel 矩阵。此外，我们还使用随机选择的信号，以消除异常从训练过程中。我们的所有算法都是在异常检测工具箱中引入，并与标准 Referenz结果进行比较。Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. If you prefer Traditional Chinese, please let me know and I can provide the translation in that version as well.
</details></li>
</ul>
<hr>
<h2 id="Towards-Automated-Design-of-Riboswitches"><a href="#Towards-Automated-Design-of-Riboswitches" class="headerlink" title="Towards Automated Design of Riboswitches"></a>Towards Automated Design of Riboswitches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08801">http://arxiv.org/abs/2307.08801</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frederic Runge, Jörg K. H. Franke, Frank Hutter</li>
<li>for: 本研究旨在开发一种新的计算方法，用于降低扩ycz的筛选和选择成本，以提高核酸扩ycz的发现效率。</li>
<li>methods: 本研究使用了一种新的结构基于的设计方法，考虑了全球性和愿望的序列和结构特征。</li>
<li>results: 研究人员通过使用libLEARNA方法，成功地设计了茶苷核酸扩ycz库，包含30%更多的高质量独特候选者。<details>
<summary>Abstract</summary>
Experimental screening and selection pipelines for the discovery of novel riboswitches are expensive, time-consuming, and inefficient. Using computational methods to reduce the number of candidates for the screen could drastically decrease these costs. However, existing computational approaches do not fully satisfy all requirements for the design of such initial screening libraries. In this work, we present a new method, libLEARNA, capable of providing RNA focus libraries of diverse variable-length qualified candidates. Our novel structure-based design approach considers global properties as well as desired sequence and structure features. We demonstrate the benefits of our method by designing theophylline riboswitch libraries, following a previously published protocol, and yielding 30% more unique high-quality candidates.
</details>
<details>
<summary>摘要</summary>
现有的实验室检测和选择管道可能会带来高额的成本和时间开销，同时效率也不高。使用计算机方法来减少层次的候选人选择可能会带来极大的成本降低。然而，现有的计算方法并不完全满足初步层次检测图书馆的设计需求。在这个工作中，我们提出了一种新的方法，libLEARNA，可以提供多样化变长资格候选人库。我们的新的结构基于设计方法考虑了全局特性以及愿望的序列和结构特征。我们示出了我们的方法的优势，通过采用已发表的卡夫曼协议，设计了茶苷核酸抑制 riboswitch库，并且获得了30%更多的独特高质量候选人。
</details></li>
</ul>
<hr>
<h2 id="regulAS-A-Bioinformatics-Tool-for-the-Integrative-Analysis-of-Alternative-Splicing-Regulome-using-RNA-Seq-data"><a href="#regulAS-A-Bioinformatics-Tool-for-the-Integrative-Analysis-of-Alternative-Splicing-Regulome-using-RNA-Seq-data" class="headerlink" title="regulAS: A Bioinformatics Tool for the Integrative Analysis of Alternative Splicing Regulome using RNA-Seq data"></a>regulAS: A Bioinformatics Tool for the Integrative Analysis of Alternative Splicing Regulome using RNA-Seq data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08800">http://arxiv.org/abs/2307.08800</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/slipnitskaya/regulas">https://github.com/slipnitskaya/regulas</a></li>
<li>paper_authors: Sofya Lipnitskaya</li>
<li>for: regulAS is designed to support computational biology researchers in investigating regulatory mechanisms of splicing alterations in cancer and healthy human donors.</li>
<li>methods: regulAS uses integrative analysis of large-scale RNA-Seq data from TCGA and GTEx projects, with features such as RNA-Seq data retrieval, predictive modeling, and flexible reporting.</li>
<li>results: regulAS provides automated solutions for alternative splicing and cancer biology studies, enhancing efficiency, reproducibility, and customization of experimental design, with the extensibility to tailor the software to specific research needs.Here’s the same information in Simplified Chinese:</li>
<li>for: regulAS 是为 computation biology 研究人员提供一个支持工具，用于调查转录调节变化的规则机制，以及人体和癌症样本中的转录调节。</li>
<li>methods: regulAS 使用了大规模 RNA-Seq 数据，包括 TCGA 和 GTEx 项目，并提供了一些功能，如 RNA-Seq 数据检索、预测模型和灵活报告生成。</li>
<li>results: regulAS 提供了一个自动化的解决方案，用于研究转录调节和癌症生物学，提高了效率、可重复性和自定义实验设计的能力，同时允许研究人员根据自己的需求进行特定的自定义和扩展。<details>
<summary>Abstract</summary>
The regulAS software package is a bioinformatics tool designed to support computational biology researchers in investigating regulatory mechanisms of splicing alterations through integrative analysis of large-scale RNA-Seq data from cancer and healthy human donors, characterized by TCGA and GTEx projects. This technical report provides a comprehensive overview of regulAS, focusing on its core functionality, basic modules, experiment configuration, further extensibility and customisation.   The core functionality of regulAS enables the automation of computational experiments, efficient results storage and processing, and streamlined workflow management. Integrated basic modules extend regulAS with features such as RNA-Seq data retrieval from the public multi-omics UCSC Xena data repository, predictive modeling and feature ranking capabilities using the scikit-learn package, and flexible reporting generation for analysing gene expression profiles and relevant modulations of alternative splicing aberrations across tissues and cancer types. Experiment configuration is handled through YAML files with the Hydra and OmegaConf libraries, offering a user-friendly approach. Additionally, regulAS allows for the development and integration of custom modules to handle specialized tasks.   In conclusion, regulAS provides an automated solution for alternative splicing and cancer biology studies, enhancing efficiency, reproducibility, and customization of experimental design, while the extensibility of the pipeline enables researchers to further tailor the software package to their specific needs. Source code is available under the MIT license at https://github.com/slipnitskaya/regulAS.
</details>
<details>
<summary>摘要</summary>
regulAS 软件包是一款 bioinformatics 工具，用于支持生物计算研究人员在 investigate 蛋白水平修饰的调控机制方面进行集成分析大规模 RNA-Seq 数据。这份技术报告提供了 regulAS 的全面介绍，重点介绍其核心功能、基本模块、实验配置、进一步扩展和自定义。regulAS 的核心功能包括自动化计算实验、高效存储和处理结果，以及流程管理。 integrate 的基本模块包括从公共多元素 UCSC Xena 数据存储库中获取 RNA-Seq 数据、使用 scikit-learn 包进行预测模型和特征排名，以及自定义报告生成分析蛋白表达资料和相关的修饰异常现象 across 组织和癌种。实验配置通过 YAML 文件与 Hydra 和 OmegaConf 库进行处理，提供了一种用户友好的方法。此外，regulAS 还允许开发和集成特殊任务的自定义模块。总之，regulAS 提供了一个自动化的蛋白水平修饰和癌生物学研究的解决方案，提高了效率、可重复性和实验设计的自定义能力，同时 pipeline 的可扩展性允许研究人员根据自己的具体需求进行进一步的定制。源代码可以在 <https://github.com/slipnitskaya/regulAS> 获取，采用 MIT 许可证。
</details></li>
</ul>
<hr>
<h2 id="Reduced-Kernel-Dictionary-Learning"><a href="#Reduced-Kernel-Dictionary-Learning" class="headerlink" title="Reduced Kernel Dictionary Learning"></a>Reduced Kernel Dictionary Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08798">http://arxiv.org/abs/2307.08798</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/denisilie94/rkdl">https://github.com/denisilie94/rkdl</a></li>
<li>paper_authors: Denis C. Ilie-Ablachim, Bogdan Dumitrescu</li>
<li>for: 这篇论文是为了解决大量数据集时，常见的问题：核心矩阵的大小。</li>
<li>methods: 本文提出了一种新的方法，即使用训练精简表示法来生成减少大小的非线性表示。具体来说，我们使用梯度下降步骤来优化核kernel向量。</li>
<li>results: 我们通过三个数据集的实验显示，我们的方法可以提供更好的表示，即使使用一小数量的核kernel向量，同时也可以降低执行时间。<details>
<summary>Abstract</summary>
In this paper we present new algorithms for training reduced-size nonlinear representations in the Kernel Dictionary Learning (KDL) problem. Standard KDL has the drawback of a large size of the kernel matrix when the data set is large. There are several ways of reducing the kernel size, notably Nystr\"om sampling. We propose here a method more in the spirit of dictionary learning, where the kernel vectors are obtained with a trained sparse representation of the input signals. Moreover, we optimize directly the kernel vectors in the KDL process, using gradient descent steps. We show with three data sets that our algorithms are able to provide better representations, despite using a small number of kernel vectors, and also decrease the execution time with respect to KDL.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了新的算法用于在kernel Dictionary Learning（KDL）问题中训练减小非线性表示。标准KDL在数据集大时具有大kernel矩阵的缺点。有几种减小kernel大小的方法，其中一种是Nystr\"om sampling。我们在这里提出了一种更接近字典学习的方法，其中kernel вектор通过输入信号的训练稀缺表示获得。此外，我们直接在KDL过程中优化kernel вектор，使用梯度下降步骤。我们通过三个数据集的实验表明，我们的算法能够提供更好的表示，即使使用少量kernel вектор，同时也降低了与KDL的执行时间。
</details></li>
</ul>
<hr>
<h2 id="Classification-with-Incoherent-Kernel-Dictionary-Learning"><a href="#Classification-with-Incoherent-Kernel-Dictionary-Learning" class="headerlink" title="Classification with Incoherent Kernel Dictionary Learning"></a>Classification with Incoherent Kernel Dictionary Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08796">http://arxiv.org/abs/2307.08796</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/denisilie94/incoherent-kernel-dictionary-learning">https://github.com/denisilie94/incoherent-kernel-dictionary-learning</a></li>
<li>paper_authors: Denis C. Ilie-Ablachim, Bogdan Dumitrescu</li>
<li>for: 这个论文提出了一种基于字典学习（DL）的新的分类方法。</li>
<li>methods: 该方法使用了一种基于kernel的协方差DL，与标准线性版本的DL进行比较。此外，我们还提出了AK-SVD算法中的表示更新改进。</li>
<li>results: 我们对多个流行的分类问题数据库进行了测试，并得到了优秀的结果。<details>
<summary>Abstract</summary>
In this paper we present a new classification method based on Dictionary Learning (DL). The main contribution consists of a kernel version of incoherent DL, derived from its standard linear counterpart. We also propose an improvement of the AK-SVD algorithm concerning the representation update. Our algorithms are tested on several popular databases of classification problems.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种基于词典学习（DL）的新的分类方法。我们的主要贡献是基于 стандар的线性DL的kernel版本。此外，我们还提出了对AK-SVD算法的表示更新方法。我们的算法在一些流行的分类问题数据库上进行了测试。Here's a breakdown of the translation:* "In this paper" becomes "在这篇论文中"* "we present" becomes "我们提出"* "a new classification method" becomes "一种新的分类方法"* "based on Dictionary Learning (DL)" becomes "基于词典学习（DL）"* "The main contribution consists of" becomes "主要贡献是"* "a kernel version of incoherent DL" becomes "基于stanдар的线性DL的kernel版本"* "derived from its standard linear counterpart" becomes "从其标准线性对应部分 derivated"* "We also propose an improvement of the AK-SVD algorithm" becomes "此外，我们还提出了对AK-SVD算法的表示更新方法"* "concerning the representation update" becomes "关于表示更新"* "Our algorithms are tested on several popular databases of classification problems" becomes "我们的算法在一些流行的分类问题数据库上进行了测试"
</details></li>
</ul>
<hr>
<h2 id="Non-Stationary-Policy-Learning-for-Multi-Timescale-Multi-Agent-Reinforcement-Learning"><a href="#Non-Stationary-Policy-Learning-for-Multi-Timescale-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Non-Stationary Policy Learning for Multi-Timescale Multi-Agent Reinforcement Learning"></a>Non-Stationary Policy Learning for Multi-Timescale Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08794">http://arxiv.org/abs/2307.08794</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patrick Emami, Xiangyu Zhang, David Biagioni, Ahmed S. Zamzam</li>
<li>for: This paper is written for learning non-stationary policies in multi-timescale multi-agent reinforcement learning (MARL) environments.</li>
<li>methods: The paper proposes a simple framework for learning non-stationary policies, using available information about agent timescales to define a periodic time encoding. The proposed algorithm uses phase-functioned neural networks to parameterize the actor and critic, providing an inductive bias for periodicity.</li>
<li>results: The paper demonstrates the effectiveness of the proposed framework in learning multi-timescale policies through simulations in a gridworld and building energy management environment.<details>
<summary>Abstract</summary>
In multi-timescale multi-agent reinforcement learning (MARL), agents interact across different timescales. In general, policies for time-dependent behaviors, such as those induced by multiple timescales, are non-stationary. Learning non-stationary policies is challenging and typically requires sophisticated or inefficient algorithms. Motivated by the prevalence of this control problem in real-world complex systems, we introduce a simple framework for learning non-stationary policies for multi-timescale MARL. Our approach uses available information about agent timescales to define a periodic time encoding. In detail, we theoretically demonstrate that the effects of non-stationarity introduced by multiple timescales can be learned by a periodic multi-agent policy. To learn such policies, we propose a policy gradient algorithm that parameterizes the actor and critic with phase-functioned neural networks, which provide an inductive bias for periodicity. The framework's ability to effectively learn multi-timescale policies is validated on a gridworld and building energy management environment.
</details>
<details>
<summary>摘要</summary>
在多时间步骤多代理人学习（MARL）中，代理人在不同的时间步骤之间互动。一般来说，由多个时间步骤引起的政策是非站立的。学习非站立政策具有挑战性，通常需要复杂或不fficient的算法。由实际世界中复杂系统中的控制问题的普遍性启发了我们，我们提出了一个简单的框架 для学习非站立政策。我们使用代理人的时间步骤信息来定义周期时间编码。在详细的演示中，我们证明了由多个时间步骤引起的非站立效果可以通过周期多代理人政策学习。为学习这种政策，我们提议使用phasic函数神经网络来参数化actor和critic，这些神经网络提供了周期性的偏好。我们的框架能够有效地学习多时间步骤的政策，并在格リッド世界和建筑能源管理环境中验证了其效果。
</details></li>
</ul>
<hr>
<h2 id="Quarl-A-Learning-Based-Quantum-Circuit-Optimizer"><a href="#Quarl-A-Learning-Based-Quantum-Circuit-Optimizer" class="headerlink" title="Quarl: A Learning-Based Quantum Circuit Optimizer"></a>Quarl: A Learning-Based Quantum Circuit Optimizer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10120">http://arxiv.org/abs/2307.10120</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zikun Li, Jinjun Peng, Yixuan Mei, Sina Lin, Yi Wu, Oded Padon, Zhihao Jia</li>
<li>for: 优化量子Circuit是一个具有很大搜索空间的函数等价Circuit的优化问题，需要应用变换来实现最终性能提高。这篇论文介绍了Quarl，一种基于学习的量子Circuit优化器。</li>
<li>methods: Quarl使用了复制学习（RL）来优化量子Circuit，但RL在量子Circuit优化中存在两个主要挑战：巨大和变化的行动空间，以及非均匀的状态表示。Quarl使用了一种新的神经网络架构和RL训练过程来解决这些问题。</li>
<li>results: 我们的评估显示，Quarl在大多数benchmark Circuit上显著超越了现有的Circuit优化器。另外，Quarl可以学习执行旋转合并，这是现有优化器中的一种复杂、非本地的循环优化。<details>
<summary>Abstract</summary>
Optimizing quantum circuits is challenging due to the very large search space of functionally equivalent circuits and the necessity of applying transformations that temporarily decrease performance to achieve a final performance improvement. This paper presents Quarl, a learning-based quantum circuit optimizer. Applying reinforcement learning (RL) to quantum circuit optimization raises two main challenges: the large and varying action space and the non-uniform state representation. Quarl addresses these issues with a novel neural architecture and RL-training procedure. Our neural architecture decomposes the action space into two parts and leverages graph neural networks in its state representation, both of which are guided by the intuition that optimization decisions can be mostly guided by local reasoning while allowing global circuit-wide reasoning. Our evaluation shows that Quarl significantly outperforms existing circuit optimizers on almost all benchmark circuits. Surprisingly, Quarl can learn to perform rotation merging, a complex, non-local circuit optimization implemented as a separate pass in existing optimizers.
</details>
<details>
<summary>摘要</summary>
优化量子Circuit是一项挑战性的任务，因为函数相同Circuit的搜索空间非常大，并且需要应用变换，暂时降低性能，以达到最终的性能提高。本文介绍Quarl，一种基于学习的量子Circuit优化器。在应用了反射学习（RL）到量子Circuit优化时，存在两个主要挑战：大和变化的动作空间，以及不均匀的状态表示。Quarl通过一种新的神经网络架构和RL训练过程来解决这些问题。我们的神经网络架构将动作空间分解成两个部分，并使用图 нейрон网络来表示状态，两者均受到了认为优化决策可以主要由本地逻辑引导，同时允许全局Circuit范围内的逻辑。我们的评估表明，Quarl在大多数测试Circuit上显著超越了现有的优化器。另外，Quarl还能学习执行旋转合并，这是一项复杂的、非本地Circuit优化，在现有的优化器中实现为单独的一个过程。
</details></li>
</ul>
<hr>
<h2 id="A-DPLL-T-Framework-for-Verifying-Deep-Neural-Networks"><a href="#A-DPLL-T-Framework-for-Verifying-Deep-Neural-Networks" class="headerlink" title="A DPLL(T) Framework for Verifying Deep Neural Networks"></a>A DPLL(T) Framework for Verifying Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10266">http://arxiv.org/abs/2307.10266</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dynaroars/neuralsat-solver">https://github.com/dynaroars/neuralsat-solver</a></li>
<li>paper_authors: Hai Duong, Linhan Li, ThanhVu Nguyen, Matthew Dwyer</li>
<li>for: 这个论文是为了提出一种新的深度神经网络验证方法，帮助检测和修复神经网络中的漏洞和攻击。</li>
<li>methods: 该方法基于DPLL(T)算法，包括冲突学习、抽象和理论解决，可以看作是一种基于SMT的神经网络验证框架。</li>
<li>results: 预liminary结果表明，NeuralSAT прототип与当前领导的状态之间具有竞争力。希望通过优化和工程化，NeuralSAT能够带来现代SAT&#x2F;SMT解决方案的力量和成功，并推动神经网络验证领域的发展。<details>
<summary>Abstract</summary>
Deep Neural Networks (DNNs) have emerged as an effective approach to tackling real-world problems. However, like human-written software, automatically-generated DNNs can have bugs and be attacked. This thus attracts many recent interests in developing effective and scalable DNN verification techniques and tools. In this work, we introduce a NeuralSAT, a new constraint solving approach to DNN verification. The design of NeuralSAT follows the DPLL(T) algorithm used modern SMT solving, which includes (conflict) clause learning, abstraction, and theory solving, and thus NeuralSAT can be considered as an SMT framework for DNNs. Preliminary results show that the NeuralSAT prototype is competitive to the state-of-the-art. We hope, with proper optimization and engineering, NeuralSAT will carry the power and success of modern SAT/SMT solvers to DNN verification. NeuralSAT is avaliable from: https://github.com/dynaroars/neuralsat-solver
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）已成为解决现实世界问题的有效方法。然而，如人工写的软件一样，自动生成的 DNN 也可能具有错误和攻击性。这引起了许多最近的关注，旨在开发有效和扩展性的 DNN 验证技术和工具。在这项工作中，我们介绍了一种名为 NeuralSAT 的新的约束解决方法。NeuralSAT 的设计基于现代 SMT 解决方法中的 DPLL(T) 算法，包括（冲突）条件学习、抽象和理论解决，因此 NeuralSAT 可以视为 DNN 的 SMT 框架。初步结果表明，NeuralSAT 原型在竞争力方面与现状保持紧密。我们希望，通过适当的优化和工程，NeuralSAT 能够将现代 SAT/SMT 解决方法的力量和成功带到 DNN 验证中。NeuralSAT 可以从以下地址获取：https://github.com/dynaroars/neuralsat-solver
</details></li>
</ul>
<hr>
<h2 id="A-mixed-policy-to-improve-performance-of-language-models-on-math-problems"><a href="#A-mixed-policy-to-improve-performance-of-language-models-on-math-problems" class="headerlink" title="A mixed policy to improve performance of language models on math problems"></a>A mixed policy to improve performance of language models on math problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08767">http://arxiv.org/abs/2307.08767</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vividitytech/math_lm_rl">https://github.com/vividitytech/math_lm_rl</a></li>
<li>paper_authors: Gang Chen</li>
<li>for: 解决 math 问题时，语言模型通常采用采样策略来预测下一个词的 conditional probabilities。但是，在 math 理解步骤中，这种方法可能会导致错误答案。因此，我们提出了一种混合策略探索方法，使用 reinforcement learning 解决 math 问题。</li>
<li>methods: 我们提出了一种两级 токен探索策略：抽象级别的策略会根据概率采样下一个 токен是操作符或操作数，而第二级是具有最高分的循环采集策略。</li>
<li>results: 我们在 GSM8K 数据集上测试了我们的方法，使用 GPT-2 模型，并证明了更高于 $2%$ 的性能提升。我们的实现可以在 <a target="_blank" rel="noopener" href="https://github.com/vividitytech/math_lm_rl">https://github.com/vividitytech/math_lm_rl</a> 上找到。<details>
<summary>Abstract</summary>
When to solve math problems, most language models take a sampling strategy to predict next word according conditional probabilities. In the math reasoning step, it may generate wrong answer. Considering math problems are deterministic, we propose a mixed policy exploration approach to solve math problems with reinforcement learning. In peculiar, we propose a two level token exploration policy: the abstract level explores next token with probability and the second level is deterministic. Specifically, the abstract level policy will decide whether the token is operator or operand with probability sampling, while the second level is deterministic to select next token with the highest score in a greedy way. We test our method on GSM8K dataset with GPT-2 model, and demonstrate more than $2\%$ performance gain. Our implementation is available at https://github.com/vividitytech/math_lm_rl.
</details>
<details>
<summary>摘要</summary>
当解决数学问题时，大多数语言模型采取采样策略来预测下一个词的条件概率。在数学逻辑步骤中，它可能生成错误答案。考虑到数学问题是deterministic的，我们提议一种混合策略探索方法来解决数学问题使用再征学习。具体来说，我们提议一个两级符号探索策略：第一级是概率采样，第二级是决定性选择下一个符号的最高分。 Specifically, the first-level policy will decide whether the token is an operator or an operand with probability sampling, while the second level is deterministic to select the next token with the highest score in a greedy way. We test our method on GSM8K dataset with GPT-2 model, and demonstrate more than 2% performance gain. Our implementation is available at <https://github.com/vividitytech/math_lm_rl>.
</details></li>
</ul>
<hr>
<h2 id="Quality-Assessment-of-Photoplethysmography-Signals-For-Cardiovascular-Biomarkers-Monitoring-Using-Wearable-Devices"><a href="#Quality-Assessment-of-Photoplethysmography-Signals-For-Cardiovascular-Biomarkers-Monitoring-Using-Wearable-Devices" class="headerlink" title="Quality Assessment of Photoplethysmography Signals For Cardiovascular Biomarkers Monitoring Using Wearable Devices"></a>Quality Assessment of Photoplethysmography Signals For Cardiovascular Biomarkers Monitoring Using Wearable Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08766">http://arxiv.org/abs/2307.08766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Felipe M. Dias, Marcelo A. F. Toledo, Diego A. C. Cardenas, Douglas A. Almeida, Filipe A. C. Oliveira, Estela Ribeiro, Jose E. Krieger, Marco A. Gutierrez</li>
<li>for: 这个研究用于评估光学 Plethysmography (PPG) 信号质量，以提高血液征分和征识Cardiovascular 健康。</li>
<li>methods: 这个研究使用了机器学习算法（包括XGBoost、CatBoost和Random Forest）来训练27个统计特征从PPG信号中提取出高质量和低质量的PPG信号。</li>
<li>results: 研究发现，使用这些机器学习模型可以达到Se、PPV和F1-score的94.4、95.6和95.0，94.7、95.9和95.3，93.7、91.3和92.5，分别。这些结果与文献中的状态作准比较，表明机器学习模型可以用于开发远程、非侵入式和连续测量设备。<details>
<summary>Abstract</summary>
Photoplethysmography (PPG) is a non-invasive technology that measures changes in blood volume in the microvascular bed of tissue. It is commonly used in medical devices such as pulse oximeters and wrist worn heart rate monitors to monitor cardiovascular hemodynamics. PPG allows for the assessment of parameters (e.g., heart rate, pulse waveform, and peripheral perfusion) that can indicate conditions such as vasoconstriction or vasodilation, and provides information about microvascular blood flow, making it a valuable tool for monitoring cardiovascular health. However, PPG is subject to a number of sources of variations that can impact its accuracy and reliability, especially when using a wearable device for continuous monitoring, such as motion artifacts, skin pigmentation, and vasomotion. In this study, we extracted 27 statistical features from the PPG signal for training machine-learning models based on gradient boosting (XGBoost and CatBoost) and Random Forest (RF) algorithms to assess quality of PPG signals that were labeled as good or poor quality. We used the PPG time series from a publicly available dataset and evaluated the algorithm s performance using Sensitivity (Se), Positive Predicted Value (PPV), and F1-score (F1) metrics. Our model achieved Se, PPV, and F1-score of 94.4, 95.6, and 95.0 for XGBoost, 94.7, 95.9, and 95.3 for CatBoost, and 93.7, 91.3 and 92.5 for RF, respectively. Our findings are comparable to state-of-the-art reported in the literature but using a much simpler model, indicating that ML models are promising for developing remote, non-invasive, and continuous measurement devices.
</details>
<details>
<summary>摘要</summary>
photoplethysmography (PPG) 是一种不侵入性技术，用于测量血液Volume在微血管细胞床中的变化。它通常用于医疗器械 such as pulse oximeters和背部搭载的心率测量仪器来监测Cardiovascular hemodynamics。 PPG 可以评估参数（例如心率、脉冲形态和血液径流），这些参数可能会指示condition such as vasoconstriction or vasodilation，并提供关于微血管血液流动的信息，使其成为监测Cardiovascular health 的有用工具。然而，PPG 受到多种源的变化的影响，特别是在使用 wearable device 进行连续监测时，如运动 artifacts、皮肤颜色和血管运动。在这项研究中，我们从 PPG 信号中提取了27个统计特征，用于训练机器学习模型，包括梯度提升（XGBoost和CatBoost）和随机森（RF）算法。我们使用公共可用的 PPG 时间序列数据集，并使用 Se、Positive Predicted Value（PPV）和 F1-score（F1） metric 评估算法的性能。我们的模型实现了 Se、PPV 和 F1-score 的94.4、95.6和95.0，XGBoost 的94.7、95.9和95.3，CatBoost 的93.7、91.3和92.5，RF 的93.7、91.3和92.5。我们的发现与文献中的状态Of-the-art相似，但使用更简单的模型， indicating that ML models are promising for developing remote, non-invasive, and continuous measurement devices。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Application-of-Conditional-Normalizing-Flows-Stellar-Age-Inference-with-Gyrochronology"><a href="#A-Novel-Application-of-Conditional-Normalizing-Flows-Stellar-Age-Inference-with-Gyrochronology" class="headerlink" title="A Novel Application of Conditional Normalizing Flows: Stellar Age Inference with Gyrochronology"></a>A Novel Application of Conditional Normalizing Flows: Stellar Age Inference with Gyrochronology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08753">http://arxiv.org/abs/2307.08753</a></li>
<li>repo_url: None</li>
<li>paper_authors: Phil Van-Lane, Joshua S. Speagle, Stephanie Douglas</li>
<li>for: 用于推算低质量主序星的年龄</li>
<li>methods: 使用机器学习技术进行Conditional Normalizing Flows分析光谱数据</li>
<li>results: 实现了与文献值相符的年龄估算，并且提供了一种可靠的数据驱动的星系年龄推算方法<details>
<summary>Abstract</summary>
Stellar ages are critical building blocks of evolutionary models, but challenging to measure for low mass main sequence stars. An unexplored solution in this regime is the application of probabilistic machine learning methods to gyrochronology, a stellar dating technique that is uniquely well suited for these stars. While accurate analytical gyrochronological models have proven challenging to develop, here we apply conditional normalizing flows to photometric data from open star clusters, and demonstrate that a data-driven approach can constrain gyrochronological ages with a precision comparable to other standard techniques. We evaluate the flow results in the context of a Bayesian framework, and show that our inferred ages recover literature values well. This work demonstrates the potential of a probabilistic data-driven solution to widen the applicability of gyrochronological stellar dating.
</details>
<details>
<summary>摘要</summary>
星系年龄是进化模型的关键构建块，但低质量主序星测量具有挑战。未经探索的解决方案在这个领域是通过潜在机器学习方法进行gyrochronology，这是特别适用于这些星体的星年龄测量技术。虽然精确的分析gyrochronological模型具有挑战，但我们在光度测量数据上应用条件正常流，并示出了一种数据驱动的方法可以与其他标准技术相比的精度来限制gyrochronological年龄。我们在bayesian框架下评估流果，并发现我们的推断年龄与文献值很好地匹配。这种工作示出了潜在的数据驱动probabilistic解决方案可以扩展gyrochronological星年龄测量的应用范围。
</details></li>
</ul>
<hr>
<h2 id="Flow-Matching-in-Latent-Space"><a href="#Flow-Matching-in-Latent-Space" class="headerlink" title="Flow Matching in Latent Space"></a>Flow Matching in Latent Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08698">http://arxiv.org/abs/2307.08698</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vinairesearch/lfm">https://github.com/vinairesearch/lfm</a></li>
<li>paper_authors: Quan Dao, Hao Phung, Binh Nguyen, Anh Tran</li>
<li>for: train generative models with improved computational efficiency and scalability for high-resolution image synthesis</li>
<li>methods: apply flow matching in the latent spaces of pretrained autoencoders, integrate various conditions for conditional generation tasks</li>
<li>results: effective in both quantitative and qualitative results on various datasets, provide theoretical control of the Wasserstein-2 distance between the reconstructed latent flow distribution and true data distribution<details>
<summary>Abstract</summary>
Flow matching is a recent framework to train generative models that exhibits impressive empirical performance while being relatively easier to train compared with diffusion-based models. Despite its advantageous properties, prior methods still face the challenges of expensive computing and a large number of function evaluations of off-the-shelf solvers in the pixel space. Furthermore, although latent-based generative methods have shown great success in recent years, this particular model type remains underexplored in this area. In this work, we propose to apply flow matching in the latent spaces of pretrained autoencoders, which offers improved computational efficiency and scalability for high-resolution image synthesis. This enables flow-matching training on constrained computational resources while maintaining their quality and flexibility. Additionally, our work stands as a pioneering contribution in the integration of various conditions into flow matching for conditional generation tasks, including label-conditioned image generation, image inpainting, and semantic-to-image generation. Through extensive experiments, our approach demonstrates its effectiveness in both quantitative and qualitative results on various datasets, such as CelebA-HQ, FFHQ, LSUN Church & Bedroom, and ImageNet. We also provide a theoretical control of the Wasserstein-2 distance between the reconstructed latent flow distribution and true data distribution, showing it is upper-bounded by the latent flow matching objective. Our code will be available at https://github.com/VinAIResearch/LFM.git.
</details>
<details>
<summary>摘要</summary>
“流匹配”是一种最近的框架，用于训练生成模型，具有印象性的实验性能，而且训练更容易。然而，之前的方法仍面临computational expensive和大量的函数评估问题在像素空间中。另外，Latent-based生成方法在最近几年内表现出色，但这种模型类型在这个领域仍然未得到充分的探索。在这种工作中，我们提议在预训练 autoencoder 的 latent space 中应用流匹配，这可以提高计算效率和可扩展性，用于高分辨率图像生成。这种方法可以在受限的计算资源上进行流匹配训练，同时保持图像质量和灵活性。此外，我们的工作是在 flow matching 中 интеGRATION 多种条件的先驱性贡献，包括标签条件图像生成、图像填充和semantic-to-image生成。通过广泛的实验，我们的方法在多个数据集上表现出色，如 CelebA-HQ、FFHQ、LSUN Church & Bedroom 和 ImageNet。我们还提供了 Wasserstein-2 距离真实数据分布和重建 latent flow 分布的理论控制，证明它是上界。我们的代码将可以在 GitHub 上获得：https://github.com/VinAIResearch/LFM.git。
</details></li>
</ul>
<hr>
<h2 id="A-Multiobjective-Reinforcement-Learning-Framework-for-Microgrid-Energy-Management"><a href="#A-Multiobjective-Reinforcement-Learning-Framework-for-Microgrid-Energy-Management" class="headerlink" title="A Multiobjective Reinforcement Learning Framework for Microgrid Energy Management"></a>A Multiobjective Reinforcement Learning Framework for Microgrid Energy Management</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08692">http://arxiv.org/abs/2307.08692</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. Vivienne Liu, Patrick M. Reed, David Gold, Garret Quist, C. Lindsay Anderson</li>
<li>for: 提供一种解决多目标冲突的微grid操作方法</li>
<li>methods: 利用外生信息和数据驱动学习来探索高维目标空间，找到多目标之间的补做</li>
<li>results: 比Status quo操作更高效，提供多样化、适应性和可解释的操作方法<details>
<summary>Abstract</summary>
The emergence of microgrids (MGs) has provided a promising solution for decarbonizing and decentralizing the power grid, mitigating the challenges posed by climate change. However, MG operations often involve considering multiple objectives that represent the interests of different stakeholders, leading to potentially complex conflicts. To tackle this issue, we propose a novel multi-objective reinforcement learning framework that explores the high-dimensional objective space and uncovers the tradeoffs between conflicting objectives. This framework leverages exogenous information and capitalizes on the data-driven nature of reinforcement learning, enabling the training of a parametric policy without the need for long-term forecasts or knowledge of the underlying uncertainty distribution. The trained policies exhibit diverse, adaptive, and coordinative behaviors with the added benefit of providing interpretable insights on the dynamics of their information use. We employ this framework on the Cornell University MG (CU-MG), which is a combined heat and power MG, to evaluate its effectiveness. The results demonstrate performance improvements in all objectives considered compared to the status quo operations and offer more flexibility in navigating complex operational tradeoffs.
</details>
<details>
<summary>摘要</summary>
随着微型电网（MG）的出现，为了解决气候变化所带来的挑战，提供了一个有前途的解决方案，即减少和分散电力网络。然而，MG的运营通常需要考虑多个目标，这些目标代表不同的利益相互之间的矛盾，这可能会导致复杂的冲突。为了解决这个问题，我们提出了一种新的多目标学习框架，该框架可以探索高维目标空间中的交叉关系，并揭示不同目标之间的负担变化。这种框架利用外生信息，并利用学习的数据驱动特性，可以在不需要长期预测或者知道下游不确定分布的情况下，训练一个参数化策略。训练出来的策略具有多样化、适应性和协调性，同时还提供了可解释的动态信息使用动态。我们在康奈尔大学微型电网（CU-MG）上使用这种框架进行评估，结果表明，相比Status quo操作，我们的方法可以提高所有考虑的目标的性能，并提供更多的操作复杂关系的灵活性。
</details></li>
</ul>
<hr>
<h2 id="FlashAttention-2-Faster-Attention-with-Better-Parallelism-and-Work-Partitioning"><a href="#FlashAttention-2-Faster-Attention-with-Better-Parallelism-and-Work-Partitioning" class="headerlink" title="FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"></a>FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08691">http://arxiv.org/abs/2307.08691</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dao-ailab/flash-attention">https://github.com/dao-ailab/flash-attention</a></li>
<li>paper_authors: Tri Dao</li>
<li>for: 提高Transformers的Sequence length scaling，以提高语言模型和高分辨率图像理解的性能，以及开启代码、音频和视频生成等新应用。</li>
<li>methods: 利用GPU内存层次结构，实现 linear 而不是 quadratic 的内存占用和运行时间减速，无需 aproximation。</li>
<li>results: 对比于优化baselines，FlashAttention-2可以达到2-4倍的运行时间减速，并且在A100 GPU上达到50-73%的理论最大FLOPs&#x2F;s，接近GEMM操作的效率。<details>
<summary>Abstract</summary>
Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4$\times$ compared to optimized baselines), with no approximation. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40\% of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory. These yield around 2$\times$ speedup compared to FlashAttention, reaching 50-73\% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72\% model FLOPs utilization).
</details>
<details>
<summary>摘要</summary>
缩放变换器在更长的序列长度上进行缩放是过去几年内的一个主要问题，承诺改进语言模型和高分辨率图像理解的性能，以及开启新的代码、音频和视频生成应用。注意层是缩放到更长序列的主要瓶颈，因为它的运行时间和内存增长为序列长度的平方。 FlashAttention 利用了 GPU 内存层次结构的非对称性，实现了重要的内存减少（线性而不是平方）和运行时间加速（2-4倍于优化基线），无需折衣。然而，FlashAttention 仍然不够快，只达到了25-40%的理论最大 FLOPs/s。我们发现，这是由不佳的工作分配导致的，包括在 GPU 中的不同线程块和批处理中的低占用或 Shared 内存中的不必要读写。我们提议 FlashAttention-2，它通过改进工作分配来解决这些问题。具体来说，我们（1）修改算法，减少非 matrix-multiply FLOPs;（2）在单个头上并行执行注意计算，以增加占用率;（3）在每个线程块中，分配工作 между批处理，以减少通信过 Shared 内存。这些提高了约 2 倍的速度，达到 50-73% 的理论最大 FLOPs/s 在 A100 上，与 GEMM 操作的效率相似。我们经验 validate 了，当用于 Train GPT-style 模型时，FlashAttention-2 的训练速度可达 225 TFLOPs/s per A100 GPU (72% 模型 FLOPs 利用率)。
</details></li>
</ul>
<hr>
<h2 id="COLLIE-Systematic-Construction-of-Constrained-Text-Generation-Tasks"><a href="#COLLIE-Systematic-Construction-of-Constrained-Text-Generation-Tasks" class="headerlink" title="COLLIE: Systematic Construction of Constrained Text Generation Tasks"></a>COLLIE: Systematic Construction of Constrained Text Generation Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08689">http://arxiv.org/abs/2307.08689</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/princeton-nlp/Collie">https://github.com/princeton-nlp/Collie</a></li>
<li>paper_authors: Shunyu Yao, Howard Chen, Austin W. Hanjie, Runzhe Yang, Karthik Narasimhan</li>
<li>for: 本研究旨在提供一种 grammar-based 框架，用于 specifying 复杂的、compositional 约束，以便在自然语言处理中进行 Text generation under constraints。</li>
<li>methods: 本研究使用了 grammar-based 框架 COLLIE，可以Specify 多种层次的约束（word、sentence、paragraph、passage）和模型挑战（语言理解、逻辑推理、计数、semantic planning）。此外，还开发了一些自动提取任务实例的工具，以便使用 COLLIE 进行数据生成。</li>
<li>results: 通过使用 COLLIE，研究人员编译了 COLLIE-v1 数据集，包含 2080 个任务实例，其中每个任务实例包含 13 种约束结构。通过对 five 种 instruction-tuned 语言模型进行系统性的实验和分析，发现这些模型在处理 COLLIE 数据集时存在缺陷。 COLLIE 框架设计为轻量级和可扩展，希望社区可以通过开发更复杂的约束和评价方法来进一步提高自然语言处理技术。<details>
<summary>Abstract</summary>
Text generation under constraints have seen increasing interests in natural language processing, especially with the rapidly improving capabilities of large language models. However, existing benchmarks for constrained generation usually focus on fixed constraint types (e.g.,generate a sentence containing certain words) that have proved to be easy for state-of-the-art models like GPT-4. We present COLLIE, a grammar-based framework that allows the specification of rich, compositional constraints with diverse generation levels (word, sentence, paragraph, passage) and modeling challenges (e.g.,language understanding, logical reasoning, counting, semantic planning). We also develop tools for automatic extraction of task instances given a constraint structure and a raw text corpus. Using COLLIE, we compile the COLLIE-v1 dataset with 2080 instances comprising 13 constraint structures. We perform systematic experiments across five state-of-the-art instruction-tuned language models and analyze their performances to reveal shortcomings. COLLIE is designed to be extensible and lightweight, and we hope the community finds it useful to develop more complex constraints and evaluations in the future.
</details>
<details>
<summary>摘要</summary>
文本生成 unter constraint 已经受到了自然语言处理领域的越来越多的关注，尤其是大语言模型的能力在不断提高。然而，现有的受制定生成标准通常围绕固定的约束类型（例如，生成包含某些词的句子），这些约束已经证明容易 для当前的模型 like GPT-4。我们提出了 COLLIE，一个基于语法的框架，允许指定Rich和多层次的 compositional 约束（单词、句子、段落、段落），以及模型挑战（例如，语言理解、逻辑推理、计数、semantic planning）。我们还开发了自动提取 task instance 的工具，基于约束结构和原始文本库。使用 COLLIE，我们编译了 COLLIE-v1 数据集，包含 2080 个实例，其中 13 种约束结构。我们在五种状态atracking 语言模型上进行了系统性的实验，并分析其性能，以揭示缺陷。COLLIE 是可扩展和轻量级的，我们希望社区能够在未来开发更复杂的约束和评价。
</details></li>
</ul>
<hr>
<h2 id="An-R-package-for-parametric-estimation-of-causal-effects"><a href="#An-R-package-for-parametric-estimation-of-causal-effects" class="headerlink" title="An R package for parametric estimation of causal effects"></a>An R package for parametric estimation of causal effects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08686">http://arxiv.org/abs/2307.08686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joshua Wolff Anderson, Cyril Rakovski</li>
<li>for: 本文旨在介绍R包CausalModels，用于估计 causal effect。</li>
<li>methods: 本文使用了一些常见的统计方法，包括标准化、IP重み、G估计、结果回归、工具变量和投影匹配等。</li>
<li>results: 本文提供了一个简单和可访问的框架，可以在R中对不同的统计方法进行集成，用于估计 causal effect。Note: The above text is in Simplified Chinese.<details>
<summary>Abstract</summary>
This article explains the usage of R package CausalModels, which is publicly available on the Comprehensive R Archive Network. While packages are available for sufficiently estimating causal effects, there lacks a package that provides a collection of structural models using the conventional statistical approach developed by Hernan and Robins (2020). CausalModels addresses this deficiency of software in R concerning causal inference by offering tools for methods that account for biases in observational data without requiring extensive statistical knowledge. These methods should not be ignored and may be more appropriate or efficient in solving particular problems. While implementations of these statistical models are distributed among a number of causal packages, CausalModels introduces a simple and accessible framework for a consistent modeling pipeline among a variety of statistical methods for estimating causal effects in a single R package. It consists of common methods including standardization, IP weighting, G-estimation, outcome regression, instrumental variables and propensity matching.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Rubik’s-Cube-inspired-approach-to-Clifford-synthesis"><a href="#A-Rubik’s-Cube-inspired-approach-to-Clifford-synthesis" class="headerlink" title="A Rubik’s Cube inspired approach to Clifford synthesis"></a>A Rubik’s Cube inspired approach to Clifford synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08684">http://arxiv.org/abs/2307.08684</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gshartnett/rubiks-clifford-synthesis">https://github.com/gshartnett/rubiks-clifford-synthesis</a></li>
<li>paper_authors: Ning Bao, Gavin S. Hartnett</li>
<li>for: 解决Clifford元素的分解问题，即Clifford合成问题。</li>
<li>methods: 采用机器学习方法，基于距离标准的近似来实现Clifford合成。</li>
<li>results: 比现有算法更具有灵活性，可以适应特定设备的gate集、设备拓扑和gate精度。<details>
<summary>Abstract</summary>
The problem of decomposing an arbitrary Clifford element into a sequence of Clifford gates is known as Clifford synthesis. Drawing inspiration from similarities between this and the famous Rubik's Cube problem, we develop a machine learning approach for Clifford synthesis based on learning an approximation to the distance to the identity. This approach is probabilistic and computationally intensive. However, when a decomposition is successfully found, it often involves fewer gates than existing synthesis algorithms. Additionally, our approach is much more flexible than existing algorithms in that arbitrary gate sets, device topologies, and gate fidelities may incorporated, thus allowing for the approach to be tailored to a specific device.
</details>
<details>
<summary>摘要</summary>
“把任意的克利福德元素分解成克利福德门的序列是称为克利福德合成的问题。 Drawing inspiration from类似于这和著名的聂隐秘 куби cura问题，我们开发了基于学习距离Identidade的机器学习方法 для克利福德合成。 This approach是 probabilistic和 computationally intensive。 however，当一个分解成功时，它通常具有 fewer gates than existing synthesis algorithms。 In addition， our approach is much more flexible than existing algorithms in that arbitrary gate sets, device topologies, and gate fidelities may be incorporated， thus allowing for the approach to be tailored to a specific device.”Note that Simplified Chinese is the standard writing system used in mainland China, and it may be different from Traditional Chinese, which is used in Taiwan and other parts of the world.
</details></li>
</ul>
<hr>
<h2 id="Do-Models-Explain-Themselves-Counterfactual-Simulatability-of-Natural-Language-Explanations"><a href="#Do-Models-Explain-Themselves-Counterfactual-Simulatability-of-Natural-Language-Explanations" class="headerlink" title="Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations"></a>Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08678">http://arxiv.org/abs/2307.08678</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanda Chen, Ruiqi Zhong, Narutatsu Ri, Chen Zhao, He He, Jacob Steinhardt, Zhou Yu, Kathleen McKeown</li>
<li>for: 这个论文旨在研究大型自然语言模型（LLM）是否可以解释自己的决策过程。</li>
<li>methods: 作者提出了评估对natural language explanation的counterfactual simulatability，以测试LLM是否可以帮助人类构建模型处理不同输入的MENTAL MODEL。</li>
<li>results: 研究发现，LLM的解释具有低精度和不符合可能性，因此直接优化人类批准（例如RLHF）可能并不是 suficient solution。<details>
<summary>Abstract</summary>
Large language models (LLMs) are trained to imitate humans to explain human decisions. However, do LLMs explain themselves? Can they help humans build mental models of how LLMs process different inputs? To answer these questions, we propose to evaluate $\textbf{counterfactual simulatability}$ of natural language explanations: whether an explanation can enable humans to precisely infer the model's outputs on diverse counterfactuals of the explained input. For example, if a model answers "yes" to the input question "Can eagles fly?" with the explanation "all birds can fly", then humans would infer from the explanation that it would also answer "yes" to the counterfactual input "Can penguins fly?". If the explanation is precise, then the model's answer should match humans' expectations.   We implemented two metrics based on counterfactual simulatability: precision and generality. We generated diverse counterfactuals automatically using LLMs. We then used these metrics to evaluate state-of-the-art LLMs (e.g., GPT-4) on two tasks: multi-hop factual reasoning and reward modeling. We found that LLM's explanations have low precision and that precision does not correlate with plausibility. Therefore, naively optimizing human approvals (e.g., RLHF) may not be a sufficient solution.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）在训练时尝试模仿人类的决策，但是 LLM 是否能够解释自己的处理逻辑？可以使用 counterfactual simulatability 来评估 LLM 的解释能力。我们定义 counterfactual simulatability 为：一个解释是否能够帮助人类建立模型处理输入的精准模型。例如，如果一个模型对 input 问题 "Can eagles fly?" 的答案是 "yes"，并且提供解释 "all birds can fly"，那么人类就可以从解释中推断出模型对 counterfactual input "Can penguins fly?" 的答案是什么。如果解释准确，那么模型的答案应该与人类的预期相符。为了评估 LLM 的 counterfactual simulatability，我们提出了两种指标：精度和通用性。我们使用 LLM 自动生成了多个 counterfactual，然后使用这些指标来评估当前 state-of-the-art LLM （例如 GPT-4）在 multi-hop factual reasoning 和 reward modeling 两个任务上的表现。我们发现 LLM 的解释准确率很低，而且准确率与可能性无关。因此，直接优化人类的批准（例如 RLHF）可能并不是一个充分的解决方案。
</details></li>
</ul>
<hr>
<h2 id="TableGPT-Towards-Unifying-Tables-Nature-Language-and-Commands-into-One-GPT"><a href="#TableGPT-Towards-Unifying-Tables-Nature-Language-and-Commands-into-One-GPT" class="headerlink" title="TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT"></a>TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08674">http://arxiv.org/abs/2307.08674</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi Huang, Saisai Yang, Jing Yuan, Changbao Su, Xiang Li, Aofeng Su, Tao Zhang, Chen Zhou, Kaizhe Shou, Miao Wang, Wufang Zhu, Guoshan Lu, Chao Ye, Yali Ye, Wentao Ye, Yiming Zhang, Xinglong Deng, Jie Xu, Haobo Wang, Gang Chen, Junbo Zhao</li>
<li>for: 论文旨在提供一个可以通过自然语言输入操作表格的框架，使用大语言模型（LLMs）来理解和处理表格。</li>
<li>methods: 该框架基于全新的全球表格表示方式，通过同时训练 LLMs 在表格和文本模式之间，以便它们能够深入理解表格数据并在指令链中执行复杂的操作。</li>
<li>results: TableGPT 可以提供简单易用的表格操作方式，包括问答、数据操作、数据可视化、分析报告生成和自动预测等，从而为用户提供更多的便利和访问ibilty。<details>
<summary>Abstract</summary>
Tables are prevalent in real-world databases, requiring significant time and effort for humans to analyze and manipulate. The advancements in large language models (LLMs) have made it possible to interact with tables using natural language input, bringing this capability closer to reality. In this paper, we present TableGPT, a unified fine-tuned framework that enables LLMs to understand and operate on tables using external functional commands. It introduces the capability to seamlessly interact with tables, enabling a wide range of functionalities such as question answering, data manipulation (e.g., insert, delete, query, and modify operations), data visualization, analysis report generation, and automated prediction. TableGPT aims to provide convenience and accessibility to users by empowering them to effortlessly leverage tabular data. At the core of TableGPT lies the novel concept of global tabular representations, which empowers LLMs to gain a comprehensive understanding of the entire table beyond meta-information. By jointly training LLMs on both table and text modalities, TableGPT achieves a deep understanding of tabular data and the ability to perform complex operations on tables through chain-of-command instructions. Importantly, TableGPT offers the advantage of being a self-contained system rather than relying on external API interfaces. Moreover, it supports efficient data process flow, query rejection (when appropriate) and private deployment, enabling faster domain data fine-tuning and ensuring data privacy, which enhances the framework's adaptability to specific use cases.
</details>
<details>
<summary>摘要</summary>
Tables are prevalent in real-world databases, requiring significant time and effort for humans to analyze and manipulate. The advancements in large language models (LLMs) have made it possible to interact with tables using natural language input, bringing this capability closer to reality. In this paper, we present TableGPT, a unified fine-tuned framework that enables LLMs to understand and operate on tables using external functional commands. It introduces the capability to seamlessly interact with tables, enabling a wide range of functionalities such as question answering, data manipulation (e.g., insert, delete, query, and modify operations), data visualization, analysis report generation, and automated prediction. TableGPT aims to provide convenience and accessibility to users by empowering them to effortlessly leverage tabular data. At the core of TableGPT lies the novel concept of global tabular representations, which empowers LLMs to gain a comprehensive understanding of the entire table beyond meta-information. By jointly training LLMs on both table and text modalities, TableGPT achieves a deep understanding of tabular data and the ability to perform complex operations on tables through chain-of-command instructions. Importantly, TableGPT offers the advantage of being a self-contained system rather than relying on external API interfaces. Moreover, it supports efficient data process flow, query rejection (when appropriate) and private deployment, enabling faster domain data fine-tuning and ensuring data privacy, which enhances the framework's adaptability to specific use cases.
</details></li>
</ul>
<hr>
<h2 id="CohortFinder-an-open-source-tool-for-data-driven-partitioning-of-biomedical-image-cohorts-to-yield-robust-machine-learning-models"><a href="#CohortFinder-an-open-source-tool-for-data-driven-partitioning-of-biomedical-image-cohorts-to-yield-robust-machine-learning-models" class="headerlink" title="CohortFinder: an open-source tool for data-driven partitioning of biomedical image cohorts to yield robust machine learning models"></a>CohortFinder: an open-source tool for data-driven partitioning of biomedical image cohorts to yield robust machine learning models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08673">http://arxiv.org/abs/2307.08673</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fan Fan, Georgia Martinez, Thomas Desilvio, John Shin, Yijiang Chen, Bangchen Wang, Takaya Ozeki, Maxime W. Lafarge, Viktor H. Koelzer, Laura Barisoni, Anant Madabhushi, Satish E. Viswanath, Andrew Janowczyk</li>
<li>for: 降低机器学习模型的泛化性下降，减少批处理影响</li>
<li>methods: 使用数据驱动的 cohort 分割方法来缓解批处理的影响</li>
<li>results: 在医疗影像处理任务中，使用 CohortFinder 可以提高机器学习模型的性能<details>
<summary>Abstract</summary>
Batch effects (BEs) refer to systematic technical differences in data collection unrelated to biological variations whose noise is shown to negatively impact machine learning (ML) model generalizability. Here we release CohortFinder, an open-source tool aimed at mitigating BEs via data-driven cohort partitioning. We demonstrate CohortFinder improves ML model performance in downstream medical image processing tasks. CohortFinder is freely available for download at cohortfinder.com.
</details>
<details>
<summary>摘要</summary>
批处效应（BE）指的是数据收集过程中的系统性技术差异，不 relacionados con variationes biológicas，这些噪声可能会负面影响机器学习（ML）模型的泛化性。我们现在发布了一个开源工具，即CohortFinder，用于缓解BE的影响。我们在医学图像处理任务中展示了CohortFinder可以提高ML模型的性能。CohortFinder可以免费下载于cohortfinder.com。
</details></li>
</ul>
<hr>
<h2 id="Neural-Image-Compression-Generalization-Robustness-and-Spectral-Biases"><a href="#Neural-Image-Compression-Generalization-Robustness-and-Spectral-Biases" class="headerlink" title="Neural Image Compression: Generalization, Robustness, and Spectral Biases"></a>Neural Image Compression: Generalization, Robustness, and Spectral Biases</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08657">http://arxiv.org/abs/2307.08657</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kelsey Lieberman, James Diffenderfer, Charles Godfrey, Bhavya Kailkhura</li>
<li>for: 评估 neural image compression (NIC) 模型在实际应用中的抗迁移性和一致性性能。</li>
<li>methods: 提供了一个完整的benchmark suite来评估图像压缩方法的out-of-distribution (OOD)性能，包括CLIC-C和Kodak-C两个 benchmark，并提出了基于 спектrum的检查工具来深入了解图像压缩方法引入的错误和OOD性能。</li>
<li>results: 对一种经典编码器和多种 NIC 变体进行了详细的性能比较，发现了一些挑战当前我们对 NIC 的强点和局限性的发现，并通过理论分析深入了解 NIC 的OOD性能和数据的spectral properties的关系。<details>
<summary>Abstract</summary>
Recent neural image compression (NIC) advances have produced models which are starting to outperform traditional codecs. While this has led to growing excitement about using NIC in real-world applications, the successful adoption of any machine learning system in the wild requires it to generalize (and be robust) to unseen distribution shifts at deployment. Unfortunately, current research lacks comprehensive datasets and informative tools to evaluate and understand NIC performance in real-world settings. To bridge this crucial gap, first, this paper presents a comprehensive benchmark suite to evaluate the out-of-distribution (OOD) performance of image compression methods. Specifically, we provide CLIC-C and Kodak-C by introducing 15 corruptions to popular CLIC and Kodak benchmarks. Next, we propose spectrally inspired inspection tools to gain deeper insight into errors introduced by image compression methods as well as their OOD performance. We then carry out a detailed performance comparison of a classical codec with several NIC variants, revealing intriguing findings that challenge our current understanding of the strengths and limitations of NIC. Finally, we corroborate our empirical findings with theoretical analysis, providing an in-depth view of the OOD performance of NIC and its dependence on the spectral properties of the data. Our benchmarks, spectral inspection tools, and findings provide a crucial bridge to the real-world adoption of NIC. We hope that our work will propel future efforts in designing robust and generalizable NIC methods. Code and data will be made available at https://github.com/klieberman/ood_nic.
</details>
<details>
<summary>摘要</summary>
Recent neural image compression (NIC) advances have produced models that are starting to outperform traditional codecs. While this has led to growing excitement about using NIC in real-world applications, the successful adoption of any machine learning system in the wild requires it to generalize (and be robust) to unseen distribution shifts at deployment. Unfortunately, current research lacks comprehensive datasets and informative tools to evaluate and understand NIC performance in real-world settings. To bridge this crucial gap, first, this paper presents a comprehensive benchmark suite to evaluate the out-of-distribution (OOD) performance of image compression methods. Specifically, we provide CLIC-C and Kodak-C by introducing 15 corruptions to popular CLIC and Kodak benchmarks. Next, we propose spectrally inspired inspection tools to gain deeper insight into errors introduced by image compression methods as well as their OOD performance. We then carry out a detailed performance comparison of a classical codec with several NIC variants, revealing intriguing findings that challenge our current understanding of the strengths and limitations of NIC. Finally, we corroborate our empirical findings with theoretical analysis, providing an in-depth view of the OOD performance of NIC and its dependence on the spectral properties of the data. Our benchmarks, spectral inspection tools, and findings provide a crucial bridge to the real-world adoption of NIC. We hope that our work will propel future efforts in designing robust and generalizable NIC methods. Code and data will be made available at https://github.com/klieberman/ood_nic.Here's the translation in Traditional Chinese:Recent neural image compression (NIC) advances have produced models that are starting to outperform traditional codecs. While this has led to growing excitement about using NIC in real-world applications, the successful adoption of any machine learning system in the wild requires it to generalize (and be robust) to unseen distribution shifts at deployment. Unfortunately, current research lacks comprehensive datasets and informative tools to evaluate and understand NIC performance in real-world settings. To bridge this crucial gap, first, this paper presents a comprehensive benchmark suite to evaluate the out-of-distribution (OOD) performance of image compression methods. Specifically, we provide CLIC-C and Kodak-C by introducing 15 corruptions to popular CLIC and Kodak benchmarks. Next, we propose spectrally inspired inspection tools to gain deeper insight into errors introduced by image compression methods as well as their OOD performance. We then carry out a detailed performance comparison of a classical codec with several NIC variants, revealing intriguing findings that challenge our current understanding of the strengths and limitations of NIC. Finally, we corroborate our empirical findings with theoretical analysis, providing an in-depth view of the OOD performance of NIC and its dependence on the spectral properties of the data. Our benchmarks, spectral inspection tools, and findings provide a crucial bridge to the real-world adoption of NIC. We hope that our work will propel future efforts in designing robust and generalizable NIC methods. Code and data will be made available at https://github.com/klieberman/ood_nic.
</details></li>
</ul>
<hr>
<h2 id="A-General-Framework-for-Learning-under-Corruption-Label-Noise-Attribute-Noise-and-Beyond"><a href="#A-General-Framework-for-Learning-under-Corruption-Label-Noise-Attribute-Noise-and-Beyond" class="headerlink" title="A General Framework for Learning under Corruption: Label Noise, Attribute Noise, and Beyond"></a>A General Framework for Learning under Corruption: Label Noise, Attribute Noise, and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08643">http://arxiv.org/abs/2307.08643</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laura Iacovissi, Nan Lu, Robert C. Williamson</li>
<li>for: 本研究旨在系统地分析损害模型在分布水平上的影响，提供一个涵盖所有损害模型的通用框架，并研究损害对标准预测学习的影响。</li>
<li>methods: 本研究使用Markov kernel来形式地分析损害模型，并发现了 Label和特征上的复杂相互作用和依赖关系，这些关系通常被之前的研究所忽略。</li>
<li>results: 研究发现，损害对标准预测学习会导致 bayes 风险的变化，并提供了对不同损害实例的loss correction的理论分析。<details>
<summary>Abstract</summary>
Corruption is frequently observed in collected data and has been extensively studied in machine learning under different corruption models. Despite this, there remains a limited understanding of how these models relate such that a unified view of corruptions and their consequences on learning is still lacking. In this work, we formally analyze corruption models at the distribution level through a general, exhaustive framework based on Markov kernels. We highlight the existence of intricate joint and dependent corruptions on both labels and attributes, which are rarely touched by existing research. Further, we show how these corruptions affect standard supervised learning by analyzing the resulting changes in Bayes Risk. Our findings offer qualitative insights into the consequences of "more complex" corruptions on the learning problem, and provide a foundation for future quantitative comparisons. Applications of the framework include corruption-corrected learning, a subcase of which we study in this paper by theoretically analyzing loss correction with respect to different corruption instances.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>腐败是收集数据中常见的现象，在机器学习中也广泛研究了不同的腐败模型。尽管如此，我们对这些模型之间的关系仍然具有有限的理解，而且还缺乏一个总体的视角来描述这些腐败和它们对学习的影响。在这项工作中，我们使用Markov核来正式分析腐败模型的分布水平。我们发现了 labels和特征上的复杂联合腐败，这些腐败通常不受现有研究的关注。此外，我们还分析了这些腐败对标准指导学习的影响，并研究了由不同的腐败实例导致的损失修复。我们的发现可以提供对"更复杂"腐败对学习问题的影响的质量性理解，并为未来的量化比较提供基础。应用该框架包括腐败修正学习，我们在这篇论文中对这个子情况进行了理论分析。
</details></li>
</ul>
<hr>
<h2 id="LearnedSort-as-a-learning-augmented-SampleSort-Analysis-and-Parallelization"><a href="#LearnedSort-as-a-learning-augmented-SampleSort-Analysis-and-Parallelization" class="headerlink" title="LearnedSort as a learning-augmented SampleSort: Analysis and Parallelization"></a>LearnedSort as a learning-augmented SampleSort: Analysis and Parallelization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08637">http://arxiv.org/abs/2307.08637</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ivan Carvalho, Ramon Lawrence</li>
<li>for: 本文 analyze 和 parallelize LearnedSort algorithm，一种使用机器学习模型来实现排序的新算法。</li>
<li>methods: 本文使用了对predictions的分析， argue  dass LearnedSort 是一种learning-augmented SampleSort。</li>
<li>results: 对 synthetic 和实际 dataset 进行了 benchmark， parallel LearnedSort 比 IPS4o 和其他排序算法具有更高的并发性能。<details>
<summary>Abstract</summary>
This work analyzes and parallelizes LearnedSort, the novel algorithm that sorts using machine learning models based on the cumulative distribution function. LearnedSort is analyzed under the lens of algorithms with predictions, and it is argued that LearnedSort is a learning-augmented SampleSort. A parallel LearnedSort algorithm is developed combining LearnedSort with the state-of-the-art SampleSort implementation, IPS4o. Benchmarks on synthetic and real-world datasets demonstrate improved parallel performance for parallel LearnedSort compared to IPS4o and other sorting algorithms.
</details>
<details>
<summary>摘要</summary>
这个工作分析并平行化了 LearnedSort 算法，这是基于累累函数的机器学习模型来进行排序的新算法。 LearnedSort 被视为一种基于预测的算法，并且被证明是一种增强SampleSort的学习算法。我们开发了一种将 LearnedSort 与现有的 SampleSort 实现 IPS4o 结合的平行 LearnedSort 算法。对假数据和实际数据集进行了比较，结果显示了平行 LearnedSort 的性能提高 compared to IPS4o 和其他排序算法。
</details></li>
</ul>
<hr>
<h2 id="Retentive-Network-A-Successor-to-Transformer-for-Large-Language-Models"><a href="#Retentive-Network-A-Successor-to-Transformer-for-Large-Language-Models" class="headerlink" title="Retentive Network: A Successor to Transformer for Large Language Models"></a>Retentive Network: A Successor to Transformer for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08621">http://arxiv.org/abs/2307.08621</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/unilm">https://github.com/microsoft/unilm</a></li>
<li>paper_authors: Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, Furu Wei</li>
<li>for: This paper proposes a new architecture called Retentive Network (RetNet) for large language models, which simultaneously achieves training parallelism, low-cost inference, and good performance.</li>
<li>methods: The paper uses a retention mechanism for sequence modeling, which supports three computation paradigms: parallel, recurrent, and chunkwise recurrent. The parallel representation allows for training parallelism, while the recurrent representation enables low-cost $O(1)$ inference. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity.</li>
<li>results: The paper shows that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. Experimental results on language modeling demonstrate the effectiveness of RetNet, making it a strong successor to Transformer for large language models.<details>
<summary>Abstract</summary>
In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RetNet a strong successor to Transformer for large language models. Code will be available at https://aka.ms/retnet.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们提议Retentive Network（RetNet）作为大语言模型的基础架构，同时实现培训并行、低成本推理和好性能。我们理论上 derivates了回忆和注意力之间的连接。然后我们提议了保留机制，用于序列模型化，该机制支持三种计算方式，即并行、循环和块级循环。Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks.实验结果表明，RetNet实现了有利扩展性、并行培训、低成本部署和高效推理。RetNet的特有性使其成为Transformer的强 successor for large language models。代码将提供在https://aka.ms/retnet.
</details></li>
</ul>
<hr>
<h2 id="Understanding-the-impacts-of-crop-diversification-in-the-context-of-climate-change-a-machine-learning-approach"><a href="#Understanding-the-impacts-of-crop-diversification-in-the-context-of-climate-change-a-machine-learning-approach" class="headerlink" title="Understanding the impacts of crop diversification in the context of climate change: a machine learning approach"></a>Understanding the impacts of crop diversification in the context of climate change: a machine learning approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08617">http://arxiv.org/abs/2307.08617</a></li>
<li>repo_url: None</li>
<li>paper_authors: Georgios Giannarakis, Ilias Tsoumas, Stelios Neophytides, Christiana Papoutsa, Charalampos Kontoes, Diofantos Hadjimitsis</li>
<li>for: 这个论文是为了研究农业可持续强化的方法，以及这些方法在气候变化的情况下的影响。</li>
<li>methods: 这篇论文使用了多种数据和机器学习方法来研究农业生产力的影响。</li>
<li>results: 论文发现，在更暖和干燥的气候下，多种作物杂 cultivation 能够提高农业生产力，平均提高了2.8%。这种效果与高温和低湿度有相互作用。<details>
<summary>Abstract</summary>
The concept of sustainable intensification in agriculture necessitates the implementation of management practices that prioritize sustainability without compromising productivity. However, the effects of such practices are known to depend on environmental conditions, and are therefore expected to change as a result of a changing climate. We study the impact of crop diversification on productivity in the context of climate change. We leverage heterogeneous Earth Observation data and contribute a data-driven approach based on causal machine learning for understanding how crop diversification impacts may change in the future. We apply this method to the country of Cyprus throughout a 4-year period. We find that, on average, crop diversification significantly benefited the net primary productivity of crops, increasing it by 2.8%. The effect generally synergized well with higher maximum temperatures and lower soil moistures. In a warmer and more drought-prone climate, we conclude that crop diversification exhibits promising adaptation potential and is thus a sensible policy choice with regards to agricultural productivity for present and future.
</details>
<details>
<summary>摘要</summary>
“减少农业的环境影响是一种把持可持续发展的概念，但这些实践的效果受环境因素的影响，因此随着气候变化而改变。我们研究了采用多种作物杂 planting 对产量的影响，并通过基于 causal machine learning 的数据驱动方法来理解这些影响可能在未来如何变化。我们在塞浦路斯国家范围内进行了4年的研究，发现，在平均来说，多种作物杂 planting 对农作物的 net primary productivity 产生了2.8%的增长。这种效果通常与高温和低湿度相关。在将来的气候变化中，我们认为多种作物杂 planting 具有良好的适应能力，因此是一种有理解的农业产量政策选择。”Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Temporal-and-Geographical-Analysis-of-Real-Economic-Activities-in-the-Bitcoin-Blockchain"><a href="#Temporal-and-Geographical-Analysis-of-Real-Economic-Activities-in-the-Bitcoin-Blockchain" class="headerlink" title="Temporal and Geographical Analysis of Real Economic Activities in the Bitcoin Blockchain"></a>Temporal and Geographical Analysis of Real Economic Activities in the Bitcoin Blockchain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08616">http://arxiv.org/abs/2307.08616</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rafael Ramos Tubino, Remy Cazabet, Natkamon Tovanich, Celine Robardet</li>
<li>for: The paper focuses on the real economic activity in the Bitcoin blockchain, specifically on transactions between retail users and their neighbors, rather than between organizations.</li>
<li>methods: The paper introduces a heuristic method to classify Bitcoin players into three main categories: Frequent Receivers (FR), Neighbors of FR, and Others.</li>
<li>results: Most real transactions involve Frequent Receivers, who represent a small fraction of the total value exchanged but a significant fraction of all payments, which raises concerns about the centralization of the Bitcoin ecosystem. Additionally, the paper conducts a weekly pattern analysis of activity to provide insights into the geographical location of Bitcoin users and to quantify the bias of a well-known dataset for actor identification.Here are the same information points in Simplified Chinese text:</li>
<li>for: 这篇论文关注比特币链上真实的经济活动，具体来说是对于个人用户的交易，而不是 между机构如市场、交易所或其他服务。</li>
<li>methods: 论文提出一种归纳方法，将比特币玩家分为三类：固定接收者（FR）、邻居 FR 和其他人。</li>
<li>results: 实际交易主要发生在固定接收者身上，占总交易值的小部分，但占所有支付的重要部分，这引发了中央化的担忧。论文还进行了每周活动模式分析，提供了比特币用户的地理位置信息，并且量化了一个常见的数据集的偏见。<details>
<summary>Abstract</summary>
We study the real economic activity in the Bitcoin blockchain that involves transactions from/to retail users rather than between organizations such as marketplaces, exchanges, or other services. We first introduce a heuristic method to classify Bitcoin players into three main categories: Frequent Receivers (FR), Neighbors of FR, and Others. We show that most real transactions involve Frequent Receivers, representing a small fraction of the total value exchanged according to the blockchain, but a significant fraction of all payments, raising concerns about the centralization of the Bitcoin ecosystem. We also conduct a weekly pattern analysis of activity, providing insights into the geographical location of Bitcoin users and allowing us to quantify the bias of a well-known dataset for actor identification.
</details>
<details>
<summary>摘要</summary>
我们研究比特币区块链上真正的经济活动，涉及到零售用户的交易而不是组织如市场、交易所或其他服务。我们首先提出一种启发法来分类比特币玩家为三个主要类别：固定接收者（FR）、邻居FR和其他人。我们表明，大多数真实交易发生在固定接收者身上，表示比特币总额中的一小部分，但是对所有支付都占有重要的比重，引发了中央化比特币生态系统的问题。我们还进行了每周活动模式分析，提供了比特币用户的地理位置信息，并使我们可以评估一个常见的数据集中截然的偏见。
</details></li>
</ul>
<hr>
<h2 id="Hyperparameter-Tuning-Cookbook-A-guide-for-scikit-learn-PyTorch-river-and-spotPython"><a href="#Hyperparameter-Tuning-Cookbook-A-guide-for-scikit-learn-PyTorch-river-and-spotPython" class="headerlink" title="Hyperparameter Tuning Cookbook: A guide for scikit-learn, PyTorch, river, and spotPython"></a>Hyperparameter Tuning Cookbook: A guide for scikit-learn, PyTorch, river, and spotPython</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10262">http://arxiv.org/abs/2307.10262</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sequential-parameter-optimization/spotpython">https://github.com/sequential-parameter-optimization/spotpython</a></li>
<li>paper_authors: Thomas Bartz-Beielstein</li>
<li>for: 本文提供了一个完整的 гипер参数优化指南，使用 spotPython 对 scikit-learn、PyTorch 和 river 进行优化。</li>
<li>methods: 本文使用 spotPython 的模拟模型基于优化过程，并详细介绍了 hyperparameter tuning 的过程。</li>
<li>results: 本文通过多个案例研究，包括 sklearn 模型 Support Vector Classification、Random Forests、Gradient Boosting (XGB) 和 K-nearest neighbors (KNN) 的 hyperparameter tuning，以及 river 中的 Hoeffding Adaptive Tree Regressor。  plus, the integration of spotPython into the PyTorch and PyTorch Lightning training workflow is also discussed.<details>
<summary>Abstract</summary>
This document provides a comprehensive guide to hyperparameter tuning using spotPython for scikit-learn, PyTorch, and river. The first part introduces spotPython's surrogate model-based optimization process, while the second part focuses on hyperparameter tuning. Several case studies are presented, including hyperparameter tuning for sklearn models such as Support Vector Classification, Random Forests, Gradient Boosting (XGB), and K-nearest neighbors (KNN), as well as a Hoeffding Adaptive Tree Regressor from river. The integration of spotPython into the PyTorch and PyTorch Lightning training workflow is also discussed. With a hands-on approach and step-by-step explanations, this cookbook serves as a practical starting point for anyone interested in hyperparameter tuning with Python. Highlights include the interplay between Tensorboard, PyTorch Lightning, spotPython, and river. This publication is under development, with updates available on the corresponding webpage.
</details>
<details>
<summary>摘要</summary>
这份文档提供了使用spotPython进行scikit-learn、PyTorch和river中的参数优化的全面指南。文档的首部介绍了spotPython的代理模型基于优化过程，而第二部分则专注于参数优化。文档包含了多个案例研究，包括scikit-learn模型如支持向量分类、随机森林、梯度折衔（XGB）和K最近邻（KNN）等，以及来自river的韦伯丁适应树回归模型。文档还讨论了spotPython在PyTorch和PyTorch Lightning训练工作流程中的集成。通过实践的方式和步骤说明，这本cookbook作为Python中参数优化的实践开始点，强调了Tensorboard、PyTorch Lightning、spotPython和river之间的互动。这份文档正在开发中，更新信息可以通过相应的网页获得。
</details></li>
</ul>
<hr>
<h2 id="Artificial-Intelligence-for-the-Electron-Ion-Collider-AI4EIC"><a href="#Artificial-Intelligence-for-the-Electron-Ion-Collider-AI4EIC" class="headerlink" title="Artificial Intelligence for the Electron Ion Collider (AI4EIC)"></a>Artificial Intelligence for the Electron Ion Collider (AI4EIC)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08593">http://arxiv.org/abs/2307.08593</a></li>
<li>repo_url: None</li>
<li>paper_authors: C. Allaire, R. Ammendola, E. -C. Aschenauer, M. Balandat, M. Battaglieri, J. Bernauer, M. Bondì, N. Branson, T. Britton, A. Butter, I. Chahrour, P. Chatagnon, E. Cisbani, E. W. Cline, S. Dash, C. Dean, W. Deconinck, A. Deshpande, M. Diefenthaler, R. Ent, C. Fanelli, M. Finger, M. Finger, Jr., E. Fol, S. Furletov, Y. Gao, J. Giroux, N. C. Gunawardhana Waduge, R. Harish, O. Hassan, P. L. Hegde, R. J. Hernández-Pinto, A. Hiller Blin, T. Horn, J. Huang, D. Jayakodige, B. Joo, M. Junaid, P. Karande, B. Kriesten, R. Kunnawalkam Elayavalli, M. Lin, F. Liu, S. Liuti, G. Matousek, M. McEneaney, D. McSpadden, T. Menzo, T. Miceli, V. Mikuni, R. Montgomery, B. Nachman, R. R. Nair, J. Niestroy, S. A. Ochoa Oregon, J. Oleniacz, J. D. Osborn, C. Paudel, C. Pecar, C. Peng, G. N. Perdue, W. Phelps, M. L. Purschke, K. Rajput, Y. Ren, D. F. Renteria-Estrada, D. Richford, B. J. Roy, D. Roy, N. Sato, T. Satogata, G. Sborlini, M. Schram, D. Shih, J. Singh, R. Singh, A. Siodmok, P. Stone, J. Stevens, L. Suarez, K. Suresh, A. -N. Tawfik, F. Torales Acosta, N. Tran, R. Trotta, F. J. Twagirayezu, R. Tyson, S. Volkova, A. Vossen, E. Walter, D. Whiteson, M. Williams, S. Wu, N. Zachariou, P. Zurita</li>
<li>for: The paper is written for the EIC community, discussing the potential applications of AI&#x2F;ML in the facility’s experiments and commissioning processes.</li>
<li>methods: The paper covers various R&amp;D projects and approaches currently being explored in the EIC community, including cutting-edge techniques from other experiments.</li>
<li>results: The paper provides an overview of the goals and strategies regarding AI&#x2F;ML in the EIC community, as well as the potential benefits and insights that can be gained from their application.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为EIC社区写的，探讨了AI&#x2F;ML在该设施的实验和启动过程中的潜在应用。</li>
<li>methods: 论文涵盖了EIC社区当前在进行的多个R&amp;D项目和方法，包括其他实验中的前沿技术。</li>
<li>results: 论文提供了EIC社区对AI&#x2F;ML的应用 goals和策略的概述，以及通过其应用可以获得的优点和发现。<details>
<summary>Abstract</summary>
The Electron-Ion Collider (EIC), a state-of-the-art facility for studying the strong force, is expected to begin commissioning its first experiments in 2028. This is an opportune time for artificial intelligence (AI) to be included from the start at this facility and in all phases that lead up to the experiments. The second annual workshop organized by the AI4EIC working group, which recently took place, centered on exploring all current and prospective application areas of AI for the EIC. This workshop is not only beneficial for the EIC, but also provides valuable insights for the newly established ePIC collaboration at EIC. This paper summarizes the different activities and R&D projects covered across the sessions of the workshop and provides an overview of the goals, approaches and strategies regarding AI/ML in the EIC community, as well as cutting-edge techniques currently studied in other experiments.
</details>
<details>
<summary>摘要</summary>
电子离子碰撞器（EIC），一个最先进的强相互作用研究设施，预计在2028年开始首次实验启用。这是一个非常有利的时机，让人工智能（AI）从设施的开始就包括在内，并在所有实验阶段进行应用。第二年度的AI4EIC工作坊，由AI4EIC工作组组织，最近召开，主要是探讨CURRENT AND PROSPECTIVE APPLICATION AREAS OF AI FOR EIC。这个工作坊不仅对EIC有利，还为新成立的ePIC合作项目提供了宝贵的经验。本文将summarize工作坊的不同活动和R&D项目，提供EIC社区关于AI/ML的目标、方法和战略，以及目前在其他实验中研究的前沿技术。
</details></li>
</ul>
<hr>
<h2 id="Snapshot-Spectral-Clustering-–-a-costless-approach-to-deep-clustering-ensembles-generation"><a href="#Snapshot-Spectral-Clustering-–-a-costless-approach-to-deep-clustering-ensembles-generation" class="headerlink" title="Snapshot Spectral Clustering – a costless approach to deep clustering ensembles generation"></a>Snapshot Spectral Clustering – a costless approach to deep clustering ensembles generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08591">http://arxiv.org/abs/2307.08591</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Piróg, Halina Kwaśnicka</li>
<li>for: 本研究旨在探讨将深度学习与聚类结合使用，以提高聚类结果的准确性和稳定性。</li>
<li>methods: 本研究提出了一种新的深度聚类协同方法（Snapshot Spectral Clustering），利用多个视角的数据生成多个深度学习模型，并将其组合以实现更高的聚类精度和稳定性。</li>
<li>results: 实验结果表明，Snapshot Spectral Clustering方法可以减少计算成本，同时提高聚类结果的准确性和稳定性，相比于传统的聚类方法和深度学习方法。<details>
<summary>Abstract</summary>
Despite tremendous advancements in Artificial Intelligence, learning from large sets of data in an unsupervised manner remains a significant challenge. Classical clustering algorithms often fail to discover complex dependencies in large datasets, especially considering sparse, high-dimensional spaces. However, deep learning techniques proved to be successful when dealing with large quantities of data, efficiently reducing their dimensionality without losing track of underlying information. Several interesting advancements have already been made to combine deep learning and clustering. Still, the idea of enhancing the clustering results by combining multiple views of the data generated by deep neural networks appears to be insufficiently explored yet. This paper aims to investigate this direction and bridge the gap between deep neural networks, clustering techniques and ensemble learning methods. To achieve this goal, we propose a novel deep clustering ensemble method - Snapshot Spectral Clustering, designed to maximize the gain from combining multiple data views while minimizing the computational costs of creating the ensemble. Comparative analysis and experiments described in this paper prove the proposed concept, while the conducted hyperparameter study provides a valuable intuition to follow when selecting proper values.
</details>
<details>
<summary>摘要</summary>
To achieve this goal, we propose a novel deep clustering ensemble method called Snapshot Spectral Clustering. This method is designed to maximize the gain from combining multiple data views while minimizing computational costs. Our comparative analysis and experiments show that the proposed method is effective, and a hyperparameter study provides valuable intuition for selecting appropriate values.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/18/cs.LG_2023_07_18/" data-id="clluro5jm004rq9888mq74tw9" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/18/cs.SD_2023_07_18/" class="article-date">
  <time datetime="2023-07-17T16:00:00.000Z" itemprop="datePublished">2023-07-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/18/cs.SD_2023_07_18/">cs.SD - 2023-07-18 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Efficient-representation-of-head-related-transfer-functions-in-continuous-space-frequency-domains"><a href="#Efficient-representation-of-head-related-transfer-functions-in-continuous-space-frequency-domains" class="headerlink" title="Efficient representation of head-related transfer functions in continuous space-frequency domains"></a>Efficient representation of head-related transfer functions in continuous space-frequency domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09352">http://arxiv.org/abs/2307.09352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Szwajcowski</li>
<li>for: 这篇论文旨在探讨将圆柱卷积函数（SH）领域的继承方法替换为四维连续函数模型，以实现空间上的连续性。</li>
<li>methods: 论文使用了四维卷积函数（HSH）和圆柱卷积函数（SH） merged with one-dimensional basis functions，并对这两种方法进行比较。</li>
<li>results: 研究发现，使用四维连续函数模型可以将HRTF的大小谱спектроgram表示为一小组含义强的系数，这些系数可以在任何方向和频率下解码。HSH和SH merged with reverse Fourier-Bessel series表现最佳，其中HSH具有更好的压缩能力，对低级数据进行更高精度的重建。这些模型可以用于HRTF的 interpol、压缩和参数化，以及其他类型的直接函数的应用。<details>
<summary>Abstract</summary>
Utilizing spherical harmonic (SH) domain has been established as the default method of obtaining continuity over space in head-related transfer functions (HRTFs). This paper concerns different variants of extending this solution by replacing SHs with four-dimensional (4D) continuous functional models in which frequency is imagined as another physical dimension. Recently developed hyperspherical harmonic (HSH) representation is compared with models defined in spherindrical coordinate system by merging SHs with one-dimensional basis functions. The efficiency of both approaches is evaluated based on the reproduction errors for individual HRTFs from HUTUBS database, including detailed analysis of its dependency on chosen orders of approximation in frequency and space. Employing continuous functional models defined in 4D coordinate systems allows HRTF magnitude spectra to be expressed as a small set of coefficients which can be decoded back into values at any direction and frequency. The best performance was noted for HSHs and SHs merged with reverse Fourier-Bessel series, with the former featuring better compression abilities, achieving slightly higher accuracy for low number of coefficients. The presented models can serve multiple purposes, such as interpolation, compression or parametrization for machine learning applications, and can be applied not only to HRTFs but also to other types of directivity functions, e.g. sound source directivity.
</details>
<details>
<summary>摘要</summary>
utilizing 球面幂函数（SH）域已经被确立为HEAD相关传送函数（HRTF）的默认方法，这篇论文考虑了不同的扩展方案，其中 replacing SHs with four-dimensional（4D）连续函数模型，在这个模型中，频率被想象为空间中的另一个物理维度。最近开发的对称幂函数（HSH）表示与在圆柱坐标系中定义的模型相比较，并进行了详细的分析。这两种方法的效率被评估基于HRTF数据库中的重建错误，包括选择的频率和空间纬度的顺序方法的依赖关系。使用在4D坐标系中定义的连续函数模型，可以将HRTF的 магниту드 спектrum表示为一小组含义的系数，这些系数可以在任何方向和频率上解码。最佳性能被观察到在HSH和SH与反傅立埃尔-贝塞尔列表相加的情况下，前者具有更好的压缩能力，在低数量的系数下达到了微scopic的准确性。这些模型可以用于多种目的，如 interpolation、compression 或 parametrization for machine learning applications，并可以应用于其他类型的直径函数，如声源直径函数。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Timbre-Synthesis-using-Variational-Autoencoders-Regularized-on-Timbre-Descriptors"><a href="#Interpretable-Timbre-Synthesis-using-Variational-Autoencoders-Regularized-on-Timbre-Descriptors" class="headerlink" title="Interpretable Timbre Synthesis using Variational Autoencoders Regularized on Timbre Descriptors"></a>Interpretable Timbre Synthesis using Variational Autoencoders Regularized on Timbre Descriptors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10283">http://arxiv.org/abs/2307.10283</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anastasia Natsiou, Luca Longo, Sean O’Leary</li>
<li>for: 研究控制音色synthesizer的方法</li>
<li>methods: 使用深度神经网络和变量自动编码器（VAEs）生成音色表示</li>
<li>results: 提出了一种含timbre描述的准则化VAEs，并使用音色的幂数内容来缩小维度Here’s the translation in English for reference:</li>
<li>for: Research on controllable timbre synthesis methods</li>
<li>methods: Using deep neural networks and Variational Autoencoders (VAEs) to generate timbre representations</li>
<li>results: Proposed a regularized VAE-based latent space that incorporates timbre descriptors, and utilized harmonic content to reduce the dimensionality of the latent space.<details>
<summary>Abstract</summary>
Controllable timbre synthesis has been a subject of research for several decades, and deep neural networks have been the most successful in this area. Deep generative models such as Variational Autoencoders (VAEs) have the ability to generate a high-level representation of audio while providing a structured latent space. Despite their advantages, the interpretability of these latent spaces in terms of human perception is often limited. To address this limitation and enhance the control over timbre generation, we propose a regularized VAE-based latent space that incorporates timbre descriptors. Moreover, we suggest a more concise representation of sound by utilizing its harmonic content, in order to minimize the dimensionality of the latent space.
</details>
<details>
<summary>摘要</summary>
控制性 timbre 合成已经是研究的主题之一，深度神经网络是这个领域中最成功的。深度生成模型如 Variational Autoencoders (VAEs) 可以生成高级别的声音表示，并提供结构化的幂轨空间。然而，这些幂轨空间在人类听觉上的解释能力往往有限。为了解决这个限制并提高声音生成控制，我们提议使用带有声音描述符的幂轨空间，并提出一种更简洁的声音表示方法，通过利用声音的和弦内容来减少幂轨空间的维度。
</details></li>
</ul>
<hr>
<h2 id="On-Computing-In-the-Network-Covid-19-Coughs-Detection-Case-Study"><a href="#On-Computing-In-the-Network-Covid-19-Coughs-Detection-Case-Study" class="headerlink" title="On Computing In the Network: Covid-19 Coughs Detection Case Study"></a>On Computing In the Network: Covid-19 Coughs Detection Case Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08902">http://arxiv.org/abs/2307.08902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soukaina Ouledsidi Ali, Zakaria Ait Hmitti, Halima Elbiaze, Roch Glitho</li>
<li>for: 该论文旨在探讨在网络设备（如交换机和网络interface卡）上进行计算，以实现时间敏感应用的质量服务目标。</li>
<li>methods: 论文比较了在云端-边缘-浮动 kontinuum 中进行计算和缓存任务的优势，并对 covid-19 警示应用在机场设置中进行了一个关键的使用场景。</li>
<li>results: 通过实验比较，论文表明，在网络设备内部进行计算可以更好地降低往返时间（RTT）和流量筛选。<details>
<summary>Abstract</summary>
Computing in the network (COIN) is a promising technology that allows processing to be carried out within network devices such as switches and network interface cards. Time sensitive application can achieve their quality of service (QoS) target by flexibly distributing the caching and computing tasks in the cloud-edge-mist continuum. This paper highlights the advantages of in-network computing, comparing to edge computing, in terms of latency and traffic filtering. We consider a critical use case related to Covid-19 alert application in an airport setting. Arriving travelers are monitored through cough analysis so that potentially infected cases can be detected and isolated for medical tests. A performance comparison has been done between an architecture using in-network computing and another one using edge computing. We show using simulations that in-network computing outperforms edge computing in terms of Round Trip Time (RTT) and traffic filtering.
</details>
<details>
<summary>摘要</summary>
计算在网络（COIN）是一种有前途的技术，允许在网络设备 such as 交换机和网络接口卡上进行处理。时间敏感应用可以通过在云端-边缘-浮云 continuum 中flexibly分配缓存和计算任务来实现质量服务（QoS）目标。本文highlights COIN的优势，比如边缘计算，在延迟和流量筛选方面。我们考虑了一个关键的covid-19预警应用场景，在机场设置下，来往旅客通过喊叫分析进行监测，检测和隔离涉疫患者进行医学测试。我们通过 simulated comparison 表明，使用COIN的architecture 在延迟（RTT）和流量筛选方面表现出色，高于边缘计算。
</details></li>
</ul>
<hr>
<h2 id="Multilingual-Speech-to-Speech-Translation-into-Multiple-Target-Languages"><a href="#Multilingual-Speech-to-Speech-Translation-into-Multiple-Target-Languages" class="headerlink" title="Multilingual Speech-to-Speech Translation into Multiple Target Languages"></a>Multilingual Speech-to-Speech Translation into Multiple Target Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08655">http://arxiv.org/abs/2307.08655</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongyu Gong, Ning Dong, Sravya Popuri, Vedanuj Goswami, Ann Lee, Juan Pino</li>
<li>for: 多语言speech-to-speech翻译 (S2ST)，即在不同语言之间的口头交流。</li>
<li>methods: 利用最新的直接S2ST技术，包括speech-to-unit和 vocoder，并将这些关键组件增加多语言能力。 specifically, the paper proposes a multilingual extension of S2U, called speech-to-masked-unit (S2MU), which applies masking to units that do not belong to the given target language to reduce language interference. Additionally, the paper proposes a multilingual vocoder trained with language embedding and the auxiliary loss of language identification.</li>
<li>results: 在多种benchmark翻译测试集上，提出的多语言模型比双语模型在英语到16种目标语言的翻译中表现更出色。<details>
<summary>Abstract</summary>
Speech-to-speech translation (S2ST) enables spoken communication between people talking in different languages. Despite a few studies on multilingual S2ST, their focus is the multilinguality on the source side, i.e., the translation from multiple source languages to one target language. We present the first work on multilingual S2ST supporting multiple target languages. Leveraging recent advance in direct S2ST with speech-to-unit and vocoder, we equip these key components with multilingual capability. Speech-to-masked-unit (S2MU) is the multilingual extension of S2U, which applies masking to units which don't belong to the given target language to reduce the language interference. We also propose multilingual vocoder which is trained with language embedding and the auxiliary loss of language identification. On benchmark translation testsets, our proposed multilingual model shows superior performance than bilingual models in the translation from English into $16$ target languages.
</details>
<details>
<summary>摘要</summary>
听说-听写翻译（S2ST）可以帮助人们通过不同语言的口语沟通。虽有一些关于多语言S2ST的研究，但他们的重点都是源语言的多语言性，即从多种源语言翻译到一个目标语言。我们提出了首个支持多个目标语言的多语言S2ST模型。我们利用了直接S2ST的最新进展，包括speech-to-unit（S2U）和 vocalsoder，并将这些关键组件具备多语言能力。speech-to-masked-unit（S2MU）是S2U的多语言扩展，它对不属于目标语言的单元应用掩蔽，以降低语言干扰。我们还提出了多语言 vocalsoder，它在语言嵌入和语言认识的 auxillary 损失下训练。在标准翻译测试集上，我们的提议的多语言模型在英语到16种目标语言的翻译中表现出色，超过了双语模型的表现。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/18/cs.SD_2023_07_18/" data-id="clluro5kk007oq9884xmo71by" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_07_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/18/eess.AS_2023_07_18/" class="article-date">
  <time datetime="2023-07-17T16:00:00.000Z" itemprop="datePublished">2023-07-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/18/eess.AS_2023_07_18/">eess.AS - 2023-07-18 22:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Low-bit-rate-binaural-link-for-improved-ultra-low-latency-low-complexity-multichannel-speech-enhancement-in-Hearing-Aids"><a href="#Low-bit-rate-binaural-link-for-improved-ultra-low-latency-low-complexity-multichannel-speech-enhancement-in-Hearing-Aids" class="headerlink" title="Low bit rate binaural link for improved ultra low-latency low-complexity multichannel speech enhancement in Hearing Aids"></a>Low bit rate binaural link for improved ultra low-latency low-complexity multichannel speech enhancement in Hearing Aids</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08858">http://arxiv.org/abs/2307.08858</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nils L. Westhausen, Bernd T. Meyer</li>
<li>for: 提高听力器的听音质量</li>
<li>methods: 使用深度学习模型和 Filter-and-Sum 处理，并采用量化承载和集群通信来减少模型大小和计算负担</li>
<li>results: 可以匹配 oracle 双耳 LCMV 扬声器在非低延迟配置下的表现，且只需2毫秒的延迟Here’s the Chinese text in the format you requested:</li>
<li>for: 提高听力器的听音质量</li>
<li>methods: 使用深度学习模型和 Filter-and-Sum 处理，并采用量化承载和集群通信来减少模型大小和计算负担</li>
<li>results: 可以匹配 oracle 双耳 LCMV 扬声器在非低延迟配置下的表现，且只需2毫秒的延迟<details>
<summary>Abstract</summary>
Speech enhancement in hearing aids is a challenging task since the hardware limits the number of possible operations and the latency needs to be in the range of only a few milliseconds. We propose a deep-learning model compatible with these limitations, which we refer to as Group-Communication Filter-and-Sum Network (GCFSnet). GCFSnet is a causal multiple-input single output enhancement model using filter-and-sum processing in the time-frequency domain and a multi-frame deep post filter. All filters are complex-valued and are estimated by a deep-learning model using weight-sharing through Group Communication and quantization-aware training for reducing model size and computational footprint. For a further increase in performance, a low bit rate binaural link for delayed binaural features is proposed to use binaural information while retaining a latency of 2ms. The performance of an oracle binaural LCMV beamformer in non-low-latency configuration can be matched even by a unilateral configuration of the GCFSnet in terms of objective metrics.
</details>
<details>
<summary>摘要</summary>
听见助手中的语音提升是一项具有挑战性的任务，因为硬件的限制只能进行一定数量的操作，并且响应时间需要在几毫秒内。我们提议一种深度学习模型，称之为群组通信滤波和总网络（GCFSnet）。GCFSnet是一种 causal 多输入单出提升模型，使用时域频域的滤波和总处理，并使用多帧深度后 filters。所有滤波器都是复数值的，并由深度学习模型使用 weight-sharing 和量化感知训练来 estimates。为了进一步提高性能，我们提议使用低比特率双耳链接，以使用双耳信息而不超过2毫秒的响应时间。GCFSnet 的单边配置可以与非低延迟配置下的 oracle 双耳 LCMV 扫描器匹配，以对象指标来衡量。
</details></li>
</ul>
<hr>
<h2 id="Semi-supervised-multi-channel-speaker-diarization-with-cross-channel-attention"><a href="#Semi-supervised-multi-channel-speaker-diarization-with-cross-channel-attention" class="headerlink" title="Semi-supervised multi-channel speaker diarization with cross-channel attention"></a>Semi-supervised multi-channel speaker diarization with cross-channel attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08688">http://arxiv.org/abs/2307.08688</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shilong Wu, Jun Du, Maokui He, Shutong Niu, Hang Chen, Haitao Tang, Chin-Hui Lee</li>
<li>for: 本研究提出了一种半监督式的Speaker diarization系统，以利用大规模多通道训练数据，并生成pseudo标签来labels无标签数据。</li>
<li>methods: 本研究引入了 Cross-channel attention机制，以更好地学习speaker embedding的通道上下文信息。</li>
<li>results: 在CHiME-7 Mixer6数据集上，我们的系统比基于分 clustering 模型的开发集上的相对减少率为57.01%。在CHiME-6数据集上，当使用80%和50%标签的训练数据时，我们的系统与使用100%标签的训练数据相对性能相似。<details>
<summary>Abstract</summary>
Most neural speaker diarization systems rely on sufficient manual training data labels, which are hard to collect under real-world scenarios. This paper proposes a semi-supervised speaker diarization system to utilize large-scale multi-channel training data by generating pseudo-labels for unlabeled data. Furthermore, we introduce cross-channel attention into the Neural Speaker Diarization Using Memory-Aware Multi-Speaker Embedding (NSD-MA-MSE) to learn channel contextual information of speaker embeddings better. Experimental results on the CHiME-7 Mixer6 dataset which only contains partial speakers' labels of the training set, show that our system achieved 57.01% relative DER reduction compared to the clustering-based model on the development set. We further conducted experiments on the CHiME-6 dataset to simulate the scenario of missing partial training set labels. When using 80% and 50% labeled training data, our system performs comparably to the results obtained using 100% labeled data for training.
</details>
<details>
<summary>摘要</summary>
大多数神经网络发音分类系统都需要充足的手动训练数据标签，这些标签在实际场景下很难收集。这篇论文提议一种半监督的发音分类系统，利用大规模多通道训练数据生成 pseudo-标签，以便于无标签数据的学习。此外，我们在 Neural Speaker Diarization Using Memory-Aware Multi-Speaker Embedding (NSD-MA-MSE) 中引入了ChannelContextual Information的权重学习，以更好地学习发音者特征的通道信息。实验结果表明，在 CHiME-7 Mixer6 数据集上，我们的系统与 clustering-based 模型在开发集上实现了57.01%的相对减少性能。我们进一步在 CHiME-6 数据集上进行了 simulate 失去部分训练集标签的场景，当使用 80% 和 50% 标签过滤的训练数据时，我们的系统与使用 100% 标签训练的结果相当。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/18/eess.AS_2023_07_18/" data-id="clluro5lc00ajq988hdzx48eu" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/13/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><a class="page-number" href="/page/13/">13</a><span class="page-number current">14</span><a class="page-number" href="/page/15/">15</a><a class="page-number" href="/page/16/">16</a><span class="space">&hellip;</span><a class="page-number" href="/page/26/">26</a><a class="extend next" rel="next" href="/page/15/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
