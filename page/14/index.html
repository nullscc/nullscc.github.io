
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/14/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.CL_2023_10_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/15/cs.CL_2023_10_15/" class="article-date">
  <time datetime="2023-10-15T11:00:00.000Z" itemprop="datePublished">2023-10-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/15/cs.CL_2023_10_15/">cs.CL - 2023-10-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="UvA-MT’s-Participation-in-the-WMT23-General-Translation-Shared-Task"><a href="#UvA-MT’s-Participation-in-the-WMT23-General-Translation-Shared-Task" class="headerlink" title="UvA-MT’s Participation in the WMT23 General Translation Shared Task"></a>UvA-MT’s Participation in the WMT23 General Translation Shared Task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09946">http://arxiv.org/abs/2310.09946</a></li>
<li>repo_url: None</li>
<li>paper_authors: Di Wu, Shaomu Tan, David Stap, Ali Araabi, Christof Monz</li>
<li>for: 这个研究报告描述了阿姆斯特丹大学的自然语言处理实验室（UvA-MT）在2023年世界机器翻译大会（WMT）共享任务中的参加。他们在英文&lt;-&gt;希伯来两个方向的受限Track中参加竞赛，并显示了使用一个模型处理对向任务时，可以达到相似的结果，比较 Traditional的双语翻译。</li>
<li>methods: 这个研究使用了一些有效的策略，如回 перевод、重定义的嵌入表格和任务导向的精细调整，以提高自动评估中的最终结果。</li>
<li>results: 在自动评估中，他们在英文-&gt;希伯来和希伯来-&gt;英文两个方向中都获得了竞争性的结果。<details>
<summary>Abstract</summary>
This paper describes the UvA-MT's submission to the WMT 2023 shared task on general machine translation. We participate in the constrained track in two directions: English <-> Hebrew. In this competition, we show that by using one model to handle bidirectional tasks, as a minimal setting of Multilingual Machine Translation (MMT), it is possible to achieve comparable results with that of traditional bilingual translation for both directions. By including effective strategies, like back-translation, re-parameterized embedding table, and task-oriented fine-tuning, we obtained competitive final results in the automatic evaluation for both English -> Hebrew and Hebrew -> English directions.
</details>
<details>
<summary>摘要</summary>
translate to Simplified Chinese as follows:这篇论文描述了UvA-MT在WMT 2023共同任务中的提交，我们在Constrained Track中参加了英文 <-> 希伯来两个方向的翻译。在这次竞赛中，我们表明，通过使用一个模型处理双向任务，作为多语言翻译的最小设置（MMT），可以达到相同的结果。通过包括有效策略，如回译、重新参数表示表 и任务导向精度调整，我们在自动评估中获得了对 beiden方向的竞争性最终结果。
</details></li>
</ul>
<hr>
<h2 id="FiLM-Fill-in-Language-Models-for-Any-Order-Generation"><a href="#FiLM-Fill-in-Language-Models-for-Any-Order-Generation" class="headerlink" title="FiLM: Fill-in Language Models for Any-Order Generation"></a>FiLM: Fill-in Language Models for Any-Order Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09930">http://arxiv.org/abs/2310.09930</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shentianxiao/film">https://github.com/shentianxiao/film</a></li>
<li>paper_authors: Tianxiao Shen, Hao Peng, Ruoqi Shen, Yao Fu, Zaid Harchaoui, Yejin Choi</li>
<li>for: 填充语言模型 (Fill-in Language Model, FiLM) 的目的是提供一种可以在任意位置进行灵活生成的语言模型，以便在填充文本中使用双向文本上下文。</li>
<li>methods: FiLM 使用了一种新的语言模型方法，即采用 beta 分布中的变化掩码概率来提高 FiLM 的生成能力。在推理过程中，FiLM 可以顺利地插入缺失的句子、段落或整个文本，以确保输出的文本流畅、与周围上下文一致。</li>
<li>results: 在自动和人工评估中，FiLM 表现出色，超过了基于左到右语言模型的填充方法。FiLM 可以轻松地在不同的文本长度和难度水平上进行调整，并且可以在不同的语言模型大小上进行训练和 fine-tuning。<details>
<summary>Abstract</summary>
Language models have become the backbone of today's AI systems. However, their predominant left-to-right generation limits the use of bidirectional context, which is essential for tasks that involve filling text in the middle. We propose the Fill-in Language Model (FiLM), a new language modeling approach that allows for flexible generation at any position without adhering to a specific generation order. Its training extends the masked language modeling objective by adopting varying mask probabilities sampled from the Beta distribution to enhance the generative capabilities of FiLM. During inference, FiLM can seamlessly insert missing phrases, sentences, or paragraphs, ensuring that the outputs are fluent and are coherent with the surrounding context. In both automatic and human evaluations, FiLM outperforms existing infilling methods that rely on left-to-right language models trained on rearranged text segments. FiLM is easy to implement and can be either trained from scratch or fine-tuned from a left-to-right language model. Notably, as the model size grows, FiLM's perplexity approaches that of strong left-to-right language models of similar sizes, indicating FiLM's scalability and potential as a large language model.
</details>
<details>
<summary>摘要</summary>
现代人工智能系统中，语言模型已成为背景模型。然而，这些主要左往右生成的语言模型限制了使用对向文本填充的 bidirectional 上下文，这是装备填充文本的任务中非常重要。我们提出了填充语言模型（FiLM），一种新的语言模型化方法，可以在任何位置进行 flexible 生成，不受特定生成顺序的限制。它的训练将推广遮盾语言模型的对话预设，透过对应排版的 beta 分布来增强FiLM的生成能力。在推断中，FiLM可以顺利地插入缺失的句子、句末或段落，以确保输出的流畅和与周围上下文一致。在自动和人工评估中，FiLM比靠左往右的语言模型训练在重新排序的文本段落上的填充方法表现出色，并且可以轻松地从头部训练或精革左往右语言模型。值得一提的是，当模型的大小增加时，FiLM的误差接近强左往右语言模型相似大小的误差，这表明FiLM在大型模型中的可扩展性和潜力。
</details></li>
</ul>
<hr>
<h2 id="Prompting-Scientific-Names-for-Zero-Shot-Species-Recognition"><a href="#Prompting-Scientific-Names-for-Zero-Shot-Species-Recognition" class="headerlink" title="Prompting Scientific Names for Zero-Shot Species Recognition"></a>Prompting Scientific Names for Zero-Shot Species Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09929">http://arxiv.org/abs/2310.09929</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shubham Parashar, Zhiqiu Lin, Yanan Li, Shu Kong</li>
<li>for: 本研究旨在使用CLIP进行零shot认知高级生物物种，包括鸟类、植物和动物的species recognition。</li>
<li>methods: 本研究使用CLIP进行零shot认知，并使用大语言模型（LLM）生成描述（例如物种颜色和形状）以提高性能。</li>
<li>results: 研究发现，使用common名称（例如mountain hare）而不是学名（例如Lepus Timidus）在prompt中可以提高CLIP的认知精度，并且可以达到2∼5倍的提升。<details>
<summary>Abstract</summary>
Trained on web-scale image-text pairs, Vision-Language Models (VLMs) such as CLIP can recognize images of common objects in a zero-shot fashion. However, it is underexplored how to use CLIP for zero-shot recognition of highly specialized concepts, e.g., species of birds, plants, and animals, for which their scientific names are written in Latin or Greek. Indeed, CLIP performs poorly for zero-shot species recognition with prompts that use scientific names, e.g., "a photo of Lepus Timidus" (which is a scientific name in Latin). Because these names are usually not included in CLIP's training set. To improve performance, prior works propose to use large-language models (LLMs) to generate descriptions (e.g., of species color and shape) and additionally use them in prompts. We find that they bring only marginal gains. Differently, we are motivated to translate scientific names (e.g., Lepus Timidus) to common English names (e.g., mountain hare) and use such in the prompts. We find that common names are more likely to be included in CLIP's training set, and prompting them achieves 2$\sim$5 times higher accuracy on benchmarking datasets of fine-grained species recognition.
</details>
<details>
<summary>摘要</summary>
<SYS>使用 web 级别的图片文本对，视觉语言模型（VLM）如 CLIP 可以不经过训练就识别通用对象的图片。但是，对于高度专业化的概念，如鸟类、植物和动物的种类，它们的科学名称通常是拉丁文或希腊文。CLIP 在无需训练的情况下识别这些种类的图片表现不佳，因为这些名称没有包含在 CLIP 的训练集中。以前的研究提议使用大型自然语言模型（LLM）生成描述（例如，种类颜色和形状），并将其添加到提示中。我们发现它们只提供了有限的改进。与此不同，我们强调将科学名称翻译成通用英文名称（例如，山兔），并使用这些名称作为提示。我们发现这样可以提高 CLIP 的准确率，在benchmarking数据集上实现2-5倍的提高。</SYS>Here's the translation in Traditional Chinese as well:<SYS>使用 web 级别的图片文本对，视觉语言模型（VLM）如 CLIP 可以不经过训练就识别通用对象的图片。但是，对于高度专业化的概念，如鸟类、植物和动物的种类，它们的科学名称通常是拉丁文或希腊文。CLIP 在无需训练的情况下识别这些种类的图片表现不佳，因为这些名称没有包含在 CLIP 的训练集中。以前的研究提议使用大型自然语言模型（LLM）生成描述（例如，种类颜色和形状），并将其添加到提示中。我们发现它们只提供了有限的改进。与此不同，我们强调将科学名称翻译成通用英文名称（例如，山兔），并使用这些名称作为提示。我们发现这样可以提高 CLIP 的准确率，在benchmarking数据集上实现2-5倍的提高。</SYS>
</details></li>
</ul>
<hr>
<h2 id="Empirical-study-of-pretrained-multilingual-language-models-for-zero-shot-cross-lingual-generation"><a href="#Empirical-study-of-pretrained-multilingual-language-models-for-zero-shot-cross-lingual-generation" class="headerlink" title="Empirical study of pretrained multilingual language models for zero-shot cross-lingual generation"></a>Empirical study of pretrained multilingual language models for zero-shot cross-lingual generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09917">http://arxiv.org/abs/2310.09917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nadezhda Chirkova, Sheng Liang, Vassilina Nikoulina</li>
<li>for: 这个论文旨在研究零shot cross-语言生成技术，即使finetuning多语言预训练语言模型（mPLM）在一种语言上的一个生成任务，然后用其来预测这个任务在其他语言上的结果。</li>
<li>methods: 这篇论文测试了一些替代的mPLM模型，包括mBART和NLLB，并考虑了全 Parameters 的 fine-tuning 和 parameter-efficient fine-tuning with adapters。</li>
<li>results: 研究发现，mBART with adapters 与 mT5 相似，NLLB 可以在一些情况下与 mT5 竞争。 此外，研究发现训练学习率对 fine-tuning 的调整可以减轻生成错误语言的问题。<details>
<summary>Abstract</summary>
Zero-shot cross-lingual generation assumes finetuning the multilingual pretrained language model (mPLM) on a generation task in one language and then using it to make predictions for this task in other languages. Previous works notice a frequent problem of generation in a wrong language and propose approaches to address it, usually using mT5 as a backbone model. In this work, we test alternative mPLMs, such as mBART and NLLB, considering full finetuning and parameter-efficient finetuning with adapters. We find that mBART with adapters performs similarly to mT5 of the same size, and NLLB can be competitive in some cases. We also underline the importance of tuning learning rate used for finetuning, which helps to alleviate the problem of generation in the wrong language.
</details>
<details>
<summary>摘要</summary>
zero-shot 跨语言生成假设通过质量化多语言预训练语言模型（mPLM）的 Fine-tuning 进行一种语言的生成任务，然后用其来预测这个任务的其他语言。  previous works 发现生成 incorrect language 的问题，并提出了解决方案，通常使用 mT5 作为基础模型。 在这个工作中，我们测试了不同的 mPLM，如 mBART 和 NLLB，包括全部 Fine-tuning 和参数有效的 Fine-tuning  WITH 适配器。我们发现 mBART  WITH 适配器 和 mT5 的同等大小下表现相似，而 NLLB 在一些情况下可以达到竞争水平。我们还强调了在 Fine-tuning 中调整学习率的重要性，可以减轻生成 incorrect language 的问题。
</details></li>
</ul>
<hr>
<h2 id="Can-GPT-4V-ision-Serve-Medical-Applications-Case-Studies-on-GPT-4V-for-Multimodal-Medical-Diagnosis"><a href="#Can-GPT-4V-ision-Serve-Medical-Applications-Case-Studies-on-GPT-4V-for-Multimodal-Medical-Diagnosis" class="headerlink" title="Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal Medical Diagnosis"></a>Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal Medical Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09909">http://arxiv.org/abs/2310.09909</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chaoyi-wu/gpt-4v_medical_evaluation">https://github.com/chaoyi-wu/gpt-4v_medical_evaluation</a></li>
<li>paper_authors: Chaoyi Wu, Jiayu Lei, Qiaoyu Zheng, Weike Zhao, Weixiong Lin, Xiaoman Zhang, Xiao Zhou, Ziheng Zhao, Ya Zhang, Yanfeng Wang, Weidi Xie</li>
<li>for: This paper assesses the performance of OpenAI’s GPT-4V model in multimodal medical diagnosis, evaluating its ability to distinguish between medical image modalities and anatomy, as well as its ability to generate comprehensive reports.</li>
<li>methods: The evaluation uses 17 human body systems and 8 modalities of medical images, with or without patent history provided, to probe the GPT-4V’s ability on multiple clinical tasks such as imaging modality and anatomy recognition, disease diagnosis, and report generation.</li>
<li>results: The study finds that while GPT-4V demonstrates proficiency in distinguishing between medical image modalities and anatomy, it faces significant challenges in disease diagnosis and generating comprehensive reports, highlighting the limitations of large multimodal models in supporting real-world medical applications and clinical decision-making.Here are the three key points in Simplified Chinese:</li>
<li>for: 这项研究用于评估OpenAI的GPT-4V模型在多模态医学诊断中的表现，包括分辨医疗影像模式和解剖结构等能力。</li>
<li>methods: 这项评估使用17个人体系统和8种医疗影像模式，有或无患者历史提供，以探索GPT-4V在多种临床任务上的能力，包括影像模式和解剖结构识别、疾病诊断、报告生成等。</li>
<li>results: 研究发现，虽然GPT-4V在分辨医疗影像模式和解剖结构方面表现出色，但在疾病诊断和生成全面报告方面受到了重大挑战，表明大型多模态模型在实际医疗应用和临床决策中仍有很大的发展空间。<details>
<summary>Abstract</summary>
Driven by the large foundation models, the development of artificial intelligence has witnessed tremendous progress lately, leading to a surge of general interest from the public. In this study, we aim to assess the performance of OpenAI's newest model, GPT-4V(ision), specifically in the realm of multimodal medical diagnosis. Our evaluation encompasses 17 human body systems, including Central Nervous System, Head and Neck, Cardiac, Chest, Hematology, Hepatobiliary, Gastrointestinal, Urogenital, Gynecology, Obstetrics, Breast, Musculoskeletal, Spine, Vascular, Oncology, Trauma, Pediatrics, with images taken from 8 modalities used in daily clinic routine, e.g., X-ray, Computed Tomography (CT), Magnetic Resonance Imaging (MRI), Positron Emission Tomography (PET), Digital Subtraction Angiography (DSA), Mammography, Ultrasound, and Pathology. We probe the GPT-4V's ability on multiple clinical tasks with or without patent history provided, including imaging modality and anatomy recognition, disease diagnosis, report generation, disease localisation.   Our observation shows that, while GPT-4V demonstrates proficiency in distinguishing between medical image modalities and anatomy, it faces significant challenges in disease diagnosis and generating comprehensive reports. These findings underscore that while large multimodal models have made significant advancements in computer vision and natural language processing, it remains far from being used to effectively support real-world medical applications and clinical decision-making.   All images used in this report can be found in https://github.com/chaoyi-wu/GPT-4V_Medical_Evaluation.
</details>
<details>
<summary>摘要</summary>
由大型基础模型驱动，人工智能的发展最近几年有了很大的进步，引起了公众的广泛关注。在这项研究中，我们想要评估OpenAI的最新模型GPT-4V（视觉）在多modal医学诊断方面的表现。我们的评估覆盖了17个人体系统，包括中枢神经系统、头颈部、心脏、胸部、血液系统、肝胆系统、肠道系统、尿道系统、妇科、儿科、骨骼系统、脊梁系统、血管系统、肿瘤系统、护理、外伤等，图像来自日常临床 Routine的8种模式，例如X射线、计算tomography（CT）、核磁共振成像（MRI）、 позитрон发射tomography（PET）、数字抽取ANGIOGRAPHY（DSA）、胸部X射线、计算tomography（CT）、ultrasound和pathology。我们 probing GPT-4V的能力在多种临床任务上，包括图像模式和解剖学识别、疾病诊断、报告生成、疾病Localization。我们的观察表明，GPT-4V能够Distinguish between different medical imaging modalities and anatomy, but it faces significant challenges in disease diagnosis and report generation. These findings highlight that while large multimodal models have made significant advancements in computer vision and natural language processing, they are still far from being used to effectively support real-world medical applications and clinical decision-making.所有图像使用在这项报告中可以在GitHub上找到：https://github.com/chaoyi-wu/GPT-4V_Medical_Evaluation。
</details></li>
</ul>
<hr>
<h2 id="Reformulating-NLP-tasks-to-Capture-Longitudinal-Manifestation-of-Language-Disorders-in-People-with-Dementia"><a href="#Reformulating-NLP-tasks-to-Capture-Longitudinal-Manifestation-of-Language-Disorders-in-People-with-Dementia" class="headerlink" title="Reformulating NLP tasks to Capture Longitudinal Manifestation of Language Disorders in People with Dementia"></a>Reformulating NLP tasks to Capture Longitudinal Manifestation of Language Disorders in People with Dementia</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09897">http://arxiv.org/abs/2310.09897</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitris Gkoumas, Matthew Purver, Maria Liakata</li>
<li>for: 这个研究是为了 automatization 语言障碍模式，以便更好地识别和评估词语障碍。</li>
<li>methods: 这个研究使用了一个已经训练过的自然语言处理（NLP）模型，并对其进行了修改，以便在NLP任务中强制使用语言模式。然后，他们使用了这些任务的概率估计来构建数字语言标记，用于评估语言交流质量和语言障碍的严重程度。</li>
<li>results: 研究发现，提出的语言标记能够准确地识别患有 деменция 的人的语言障碍，并且与临床标记呈正相关。此外，这些语言标记还提供了词语障碍的可观察性和恰当性，可以用于评估词语障碍的进程。<details>
<summary>Abstract</summary>
Dementia is associated with language disorders which impede communication. Here, we automatically learn linguistic disorder patterns by making use of a moderately-sized pre-trained language model and forcing it to focus on reformulated natural language processing (NLP) tasks and associated linguistic patterns. Our experiments show that NLP tasks that encapsulate contextual information and enhance the gradient signal with linguistic patterns benefit performance. We then use the probability estimates from the best model to construct digital linguistic markers measuring the overall quality in communication and the intensity of a variety of language disorders. We investigate how the digital markers characterize dementia speech from a longitudinal perspective. We find that our proposed communication marker is able to robustly and reliably characterize the language of people with dementia, outperforming existing linguistic approaches; and shows external validity via significant correlation with clinical markers of behaviour. Finally, our proposed linguistic disorder markers provide useful insights into gradual language impairment associated with disease progression.
</details>
<details>
<summary>摘要</summary>
偏僻症与语言障碍有关，我们通过自动学习受控语言模型，训练其专注于修改后NLP任务和相关的语言模式。我们的实验显示，包含语言上下文信息并在语言模式中增强梯度信号的NLP任务可以提高表现。然后，我们使用最佳模型的概率估计来构建数字语言标记，评估整体沟通质量和语言障碍的严重程度。我们研究如何使用我们的提议的沟通标记来 caracterize dementia speech的长期趋势。我们发现，我们的提议的语言障碍标记能够坚定可靠地 caracterize人们患有偏僻症的语言，高于现有的语言方法；并与临床标记相关。最后，我们的语言障碍标记提供了有用的透视 gradual language impairment与疾病进程相关的语言障碍。
</details></li>
</ul>
<hr>
<h2 id="Bounding-and-Filling-A-Fast-and-Flexible-Framework-for-Image-Captioning"><a href="#Bounding-and-Filling-A-Fast-and-Flexible-Framework-for-Image-Captioning" class="headerlink" title="Bounding and Filling: A Fast and Flexible Framework for Image Captioning"></a>Bounding and Filling: A Fast and Flexible Framework for Image Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09876">http://arxiv.org/abs/2310.09876</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/changxinwang/boficap">https://github.com/changxinwang/boficap</a></li>
<li>paper_authors: Zheng Ma, Changxin Wang, Bo Huang, Zixuan Zhu, Jianbing Zhang</li>
<li>for: 这篇论文目的是提出一种快速和灵活的图像描述模型，以解决现有的描述模型具有 significiant inference latency 问题。</li>
<li>methods: 该模型使用 bounding 和 filling 技术，将图像分割成多个区域，然后采用 two-generation 方式填充每个区域。</li>
<li>results: 该模型在 MS-COCO 测试集上取得了状态的最佳性能（CIDEr 125.6），并且比基eline模型快速 9.22 倍；在半循环的情况下，该模型达到了 128.4 的 CIDEr 性能，并且速度比基eline模型快速 3.69 倍。<details>
<summary>Abstract</summary>
Most image captioning models following an autoregressive manner suffer from significant inference latency. Several models adopted a non-autoregressive manner to speed up the process. However, the vanilla non-autoregressive manner results in subpar performance, since it generates all words simultaneously, which fails to capture the relationships between words in a description. The semi-autoregressive manner employs a partially parallel method to preserve performance, but it sacrifices inference speed. In this paper, we introduce a fast and flexible framework for image captioning called BoFiCap based on bounding and filling techniques. The BoFiCap model leverages the inherent characteristics of image captioning tasks to pre-define bounding boxes for image regions and their relationships. Subsequently, the BoFiCap model fills corresponding words in each box using two-generation manners. Leveraging the box hints, our filling process allows each word to better perceive other words. Additionally, our model offers flexible image description generation: 1) by employing different generation manners based on speed or performance requirements, 2) producing varied sentences based on user-specified boxes. Experimental evaluations on the MS-COCO benchmark dataset demonstrate that our framework in a non-autoregressive manner achieves the state-of-the-art on task-specific metric CIDEr (125.6) while speeding up 9.22x than the baseline model with an autoregressive manner; in a semi-autoregressive manner, our method reaches 128.4 on CIDEr while a 3.69x speedup. Our code and data is available at https://github.com/ChangxinWang/BoFiCap.
</details>
<details>
<summary>摘要</summary>
大多数图像描述模型采用回归方式，却受到显著的推理延迟。一些模型采用非回归方式以加速过程，但这会导致性能下降，因为它们同时生成所有 слова，无法捕捉图像描述中 слова之间的关系。半回归方式使用部分并行方法保持性能，但是它们牺牲推理速度。本文提出一种快速和灵活的图像描述模型called BoFiCap，基于缓存和填充技术。BoFiCap模型利用图像描述任务的特点，先定义图像区域的缓存框，然后使用两种生成方式填充对应的字。利用框提示，我们的填充过程让每个字etter perceive其他字。此外，我们的模型提供了自适应的图像描述生成：1）根据速度或性能要求使用不同的生成方式，2）生成基于用户指定的盒子的多种句子。在COCO数据集上的实验评估 demonstrate了我们的框架在非回归方式下达到了状态之arte（CIDEr=125.6），同时速度比基eline模型（具有回归方式）快9.22倍。在半回归方式下，我们的方法达到了128.4的CIDEr，速度比基eline模型快3.69倍。我们的代码和数据可以在https://github.com/ChangxinWang/BoFiCap上获取。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Stance-Classification-with-Quantified-Moral-Foundations"><a href="#Enhancing-Stance-Classification-with-Quantified-Moral-Foundations" class="headerlink" title="Enhancing Stance Classification with Quantified Moral Foundations"></a>Enhancing Stance Classification with Quantified Moral Foundations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09848">http://arxiv.org/abs/2310.09848</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hong Zhang, Prasanta Bhattacharya, Wei Gao, Liang Ze Wong, Brandon Siyuan Loh, Joseph J. P. Simons, Jisun An</li>
<li>for: 这 paper 的目的是增强社交媒体上的立场检测，通过 incorporating deeper psychological attributes，特别是个人的道德基础。</li>
<li>methods: 这 paper 使用的方法包括EXTRACTING moral foundation features from text, 以及 message semantic features，来 классифика stance 在 message- 和 user-levels 上。</li>
<li>results:  Preliminary results 表明， encoding moral foundations 可以提高 stance detection 任务的性能，并帮助描述特定道德基础和 online stance 之间的关系。  results highlight the importance of considering deeper psychological attributes in stance analysis and underscores the role of moral foundations in guiding online social behavior.<details>
<summary>Abstract</summary>
This study enhances stance detection on social media by incorporating deeper psychological attributes, specifically individuals' moral foundations. These theoretically-derived dimensions aim to provide a comprehensive profile of an individual's moral concerns which, in recent work, has been linked to behaviour in a range of domains, including society, politics, health, and the environment. In this paper, we investigate how moral foundation dimensions can contribute to predicting an individual's stance on a given target. Specifically we incorporate moral foundation features extracted from text, along with message semantic features, to classify stances at both message- and user-levels across a range of targets and models. Our preliminary results suggest that encoding moral foundations can enhance the performance of stance detection tasks and help illuminate the associations between specific moral foundations and online stances on target topics. The results highlight the importance of considering deeper psychological attributes in stance analysis and underscores the role of moral foundations in guiding online social behavior.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Merging-Experts-into-One-Improving-Computational-Efficiency-of-Mixture-of-Experts"><a href="#Merging-Experts-into-One-Improving-Computational-Efficiency-of-Mixture-of-Experts" class="headerlink" title="Merging Experts into One: Improving Computational Efficiency of Mixture of Experts"></a>Merging Experts into One: Improving Computational Efficiency of Mixture of Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09832">http://arxiv.org/abs/2310.09832</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shwai-he/meo">https://github.com/shwai-he/meo</a></li>
<li>paper_authors: Shwai He, Run-Ze Fan, Liang Ding, Li Shen, Tianyi Zhou, Dacheng Tao</li>
<li>for: 提高语言模型的大小通常会导致NLPTasks的进步，但是会增加计算成本。零含量的混合专家（MoE）可以减少计算成本，但是如果增加激活专家的数量，计算成本会增加很快，限制实际应用。本文提出一种名为\textbf{\texttt{Merging Experts into One}（MEO）的计算效率的方法，可以保持增加专家的优点而不导致计算成本增加。</li>
<li>methods: 我们首先证明选择多个专家的优势，然后提出一种计算效率的方法，即\textbf{\texttt{Merging Experts into One}（MEO），可以将计算成本降低到单个专家的水平。此外，我们还提出了一种符号级注意块，可以进一步提高MEO的效率和表现。</li>
<li>results: 我们进行了广泛的实验，显示MEO可以减少计算成本，例如FLOPS从72.0G下降到28.6G（MEO）。此外，我们还提出了一种符号级注意块，可以进一步提高MEO的效率和表现。例如，在GLUE benchmark上，MEO的平均分数为83.3%，而vanilla MoE的平均分数为82.6%。<details>
<summary>Abstract</summary>
Scaling the size of language models usually leads to remarkable advancements in NLP tasks. But it often comes with a price of growing computational cost. Although a sparse Mixture of Experts (MoE) can reduce the cost by activating a small subset of parameters (e.g., one expert) for each input, its computation escalates significantly if increasing the number of activated experts, limiting its practical utility. Can we retain the advantages of adding more experts without substantially increasing the computational costs? In this paper, we first demonstrate the superiority of selecting multiple experts and then propose a computation-efficient approach called \textbf{\texttt{Merging Experts into One} (MEO), which reduces the computation cost to that of a single expert. Extensive experiments show that MEO significantly improves computational efficiency, e.g., FLOPS drops from 72.0G of vanilla MoE to 28.6G (MEO). Moreover, we propose a token-level attention block that further enhances the efficiency and performance of token-level MEO, e.g., 83.3\% (MEO) vs. 82.6\% (vanilla MoE) average score on the GLUE benchmark. Our code will be released upon acceptance. Code will be released at: \url{https://github.com/Shwai-He/MEO}.
</details>
<details>
<summary>摘要</summary>
通常，将语言模型的大小扩展到可观的尺度会导致NLPTasks的显著进步。然而，这经常会带来计算成本的增加。虽然 sparse Mixture of Experts（MoE）可以降低计算成本，但是如果启用更多的专家，计算成本会快速增加，限制其实际应用。我们是否可以保留添加更多专家的优点而不导致计算成本增加很多？在这篇论文中，我们首先表明了多个专家的选择的优势，然后我们提出了一种 computation-efficient的方法called \textbf{\texttt{Merging Experts into One}（MEO），可以降低计算成本到单个专家的水平。我们进行了广泛的实验，发现 MEO 可以减少 FLOPS 的值，例如，从 vanilla MoE 的 72.0G 降低到 28.6G（MEO）。此外，我们还提出了一种循环预测块，可以进一步提高 MEO 的效率和性能，例如，在 GLUE 测试准则上，MEO 的平均分数为 83.3%，而 vanilla MoE 的平均分数为 82.6%。我们将代码发布在接受后。代码将发布在：\url{https://github.com/Shwai-He/MEO}.
</details></li>
</ul>
<hr>
<h2 id="Assessing-the-Reliability-of-Large-Language-Model-Knowledge"><a href="#Assessing-the-Reliability-of-Large-Language-Model-Knowledge" class="headerlink" title="Assessing the Reliability of Large Language Model Knowledge"></a>Assessing the Reliability of Large Language Model Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09820">http://arxiv.org/abs/2310.09820</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weixuan Wang, Barry Haddow, Alexandra Birch, Wei Peng</li>
<li>for: 评估大语言模型（LLMs）的知识可靠性。</li>
<li>methods: 提出了一种名为 Model Knowledge Relibility Score (MONITOR) 的新度量方法，用于直接测试 LLMs 的事实可靠性。</li>
<li>results: 在一系列12种 LLMS 上进行了实验，并证明了 MONITOR 的效iveness 以及低计算成本。此外，还释放了一个名为 Factual Knowledge Test Corpus (FKTC) 的测试集，以便进一步研究。<details>
<summary>Abstract</summary>
Large language models (LLMs) have been treated as knowledge bases due to their strong performance in knowledge probing tasks. LLMs are typically evaluated using accuracy, yet this metric does not capture the vulnerability of LLMs to hallucination-inducing factors like prompt and context variability. How do we evaluate the capabilities of LLMs to consistently produce factually correct answers? In this paper, we propose MOdel kNowledge relIabiliTy scORe (MONITOR), a novel metric designed to directly measure LLMs' factual reliability. MONITOR computes the distance between the probability distributions of a valid output and its counterparts produced by the same LLM probing the same fact using different styles of prompts and contexts.Experiments on a comprehensive range of 12 LLMs demonstrate the effectiveness of MONITOR in evaluating the factual reliability of LLMs while maintaining a low computational overhead. In addition, we release the FKTC (Factual Knowledge Test Corpus) test set, containing 210,158 prompts in total to foster research along this line (https://github.com/Vicky-Wil/MONITOR).
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="RSVP-Customer-Intent-Detection-via-Agent-Response-Contrastive-and-Generative-Pre-Training"><a href="#RSVP-Customer-Intent-Detection-via-Agent-Response-Contrastive-and-Generative-Pre-Training" class="headerlink" title="RSVP: Customer Intent Detection via Agent Response Contrastive and Generative Pre-Training"></a>RSVP: Customer Intent Detection via Agent Response Contrastive and Generative Pre-Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09773">http://arxiv.org/abs/2310.09773</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tommytyc/rsvp">https://github.com/tommytyc/rsvp</a></li>
<li>paper_authors: Yu-Chien Tang, Wei-Yao Wang, An-Zi Yen, Wen-Chih Peng</li>
<li>for: 提供用户task-oriented对话中的精准回答和24小时支持</li>
<li>methods: 利用神经网络模型检测客户意图 based on their utterances</li>
<li>results: 与状态空间的基eline比进行了比较，得到了4.95%的准确率提升，3.4%的MRR@3提升和2.75%的MRR@5提升的结果<details>
<summary>Abstract</summary>
The dialogue systems in customer services have been developed with neural models to provide users with precise answers and round-the-clock support in task-oriented conversations by detecting customer intents based on their utterances. Existing intent detection approaches have highly relied on adaptively pre-training language models with large-scale datasets, yet the predominant cost of data collection may hinder their superiority. In addition, they neglect the information within the conversational responses of the agents, which have a lower collection cost, but are significant to customer intent as agents must tailor their replies based on the customers' intent. In this paper, we propose RSVP, a self-supervised framework dedicated to task-oriented dialogues, which utilizes agent responses for pre-training in a two-stage manner. Specifically, we introduce two pre-training tasks to incorporate the relations of utterance-response pairs: 1) Response Retrieval by selecting a correct response from a batch of candidates, and 2) Response Generation by mimicking agents to generate the response to a given utterance. Our benchmark results for two real-world customer service datasets show that RSVP significantly outperforms the state-of-the-art baselines by 4.95% for accuracy, 3.4% for MRR@3, and 2.75% for MRR@5 on average. Extensive case studies are investigated to show the validity of incorporating agent responses into the pre-training stage.
</details>
<details>
<summary>摘要</summary>
Dialogue 系统在客户服务中已经采用神经网络模型，以提供用户精准的答案和24小时的支持，通过检测客户意图基于他们的谈话来进行任务化对话。现有的意图检测方法强调适应性地预训练语言模型，但这可能增加成本。此外，它们忽略了代理人回复的信息，尽管这些信息在客户意图方面具有重要性，因为代理人必须根据客户的意图修改他们的回复。在本文中，我们提出了 RSVP，一个自动预训练框架，专门用于任务化对话。我们在两个阶段中使用代理人回复进行预训练：1）回复选择，选择一个正确的回复从批处理中的候选者中，2）回复生成，模仿代理人生成一个回复来回应给一个谈话。我们对两个实际的客户服务数据集进行了比较，结果显示，RSVP在精度、MRR@3和MRR@5等指标上平均高于状态之前的基eline by 4.95%、3.4%和2.75%。我们还进行了广泛的案例研究，以证明代理人回复的包含在预训练阶段是有效的。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Graph-Meaning-Representations-through-Decoupling-Contextual-Representation-Learning-and-Structural-Information-Propagation"><a href="#Revisiting-Graph-Meaning-Representations-through-Decoupling-Contextual-Representation-Learning-and-Structural-Information-Propagation" class="headerlink" title="Revisiting Graph Meaning Representations through Decoupling Contextual Representation Learning and Structural Information Propagation"></a>Revisiting Graph Meaning Representations through Decoupling Contextual Representation Learning and Structural Information Propagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09772">http://arxiv.org/abs/2310.09772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Zhou, Wenyu Chen, Dingyi Zeng, Hong Qu, Daniel Hershcovich</li>
<li>for: 本研究旨在探讨图意表示（GMRs）在关系EXTRACTION任务中的精确影响。</li>
<li>methods: 本研究提出了一种简单和参数效率高的神经网络架构，用于分离上下文表示学习和结构信息传递。</li>
<li>results: 研究结果表明，GMRs在四个英文和两个中文 dataset 中有所提高表达关系的性能，特别是英文dataset更加精确。然而，在文学领域dataset中，GMRs的效果较低。这些发现可以为将来关系EXTRACTION任务中的GMRs和 parser设计提供更好的指导。<details>
<summary>Abstract</summary>
In the field of natural language understanding, the intersection of neural models and graph meaning representations (GMRs) remains a compelling area of research. Despite the growing interest, a critical gap persists in understanding the exact influence of GMRs, particularly concerning relation extraction tasks. Addressing this, we introduce DAGNN-plus, a simple and parameter-efficient neural architecture designed to decouple contextual representation learning from structural information propagation. Coupled with various sequence encoders and GMRs, this architecture provides a foundation for systematic experimentation on two English and two Chinese datasets. Our empirical analysis utilizes four different graph formalisms and nine parsers. The results yield a nuanced understanding of GMRs, showing improvements in three out of the four datasets, particularly favoring English over Chinese due to highly accurate parsers. Interestingly, GMRs appear less effective in literary-domain datasets compared to general-domain datasets. These findings lay the groundwork for better-informed design of GMRs and parsers to improve relation classification, which is expected to tangibly impact the future trajectory of natural language understanding research.
</details>
<details>
<summary>摘要</summary>
在自然语言理解领域，神经网络和图意表示（GMR）的交叉研究仍然吸引着广泛的关注。尽管有增长的兴趣，但是关于GMR的具体影响仍然存在一个重要的知识 gap。为了解决这个问题，我们介绍了DAGNN-plus，一种简单而参数有效的神经网络架构，用于分离上下文表示学习和结构信息传递。与不同的序列编码器和GMR相结合，这个架构提供了对系统实验的基础，并在四种图形式和九个解析器的支持下进行了实验分析。我们的实验结果表明，GMR在英文和中文两个领域中的表现不同，特别是在文学领域比通用领域更具有优势。这些发现为将来改进GMR和解析器的设计，以提高关系类别的识别，这将对自然语言理解研究的未来轨迹产生直接的影响。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Model-Aware-In-Context-Learning-for-Code-Generation"><a href="#Large-Language-Model-Aware-In-Context-Learning-for-Code-Generation" class="headerlink" title="Large Language Model-Aware In-Context Learning for Code Generation"></a>Large Language Model-Aware In-Context Learning for Code Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09748">http://arxiv.org/abs/2310.09748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jia Li, Ge Li, Chongyang Tao, Jia Li, Huangzhao Zhang, Fang Liu, Zhi Jin</li>
<li>for: 这 paper 的目的是提出一种基于学习的选择方法，以提高 Code Generation 中 LLMS 的培养效果。</li>
<li>methods: 这 paper 使用了 LLMS 自身的生成概率来评估候选示例，然后通过对概率反馈来标注候选示例为正负。最后，通过带有对比学习目标的带有对比学习目标的导入，训练一个有效的检索器，以获得 LLMS 在 Code Generation 中的偏好。</li>
<li>results: 这 paper 的实验结果表明，LAIL 可以在 CodeGen 和 GPT-3.5 上提高 LLMS 的培养效果，相比之前的基eline 提高了11.58%、6.89%和5.07%，以及4.38%、2.85%和2.74%。<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown impressive in-context learning (ICL) ability in code generation. LLMs take a prompt consisting of requirement-code examples and a new requirement as input, and output new programs. Existing studies have found that ICL is highly dominated by the examples and thus arises research on example selection. However, existing approaches randomly select examples or only consider the textual similarity of requirements to retrieve, leading to sub-optimal performance. In this paper, we propose a novel learning-based selection approach named LAIL (LLM-Aware In-context Learning) for code generation. Given a candidate example, we exploit LLMs themselves to estimate it by considering the generation probabilities of ground-truth programs given a requirement and the example. We then label candidate examples as positive or negative through the probability feedback. Based on the labeled data, we import a contrastive learning objective to train an effective retriever that acquires the preference of LLMs in code generation. We apply LAIL to three LLMs and evaluate it on three representative datasets (e.g., MBJP, MBPP, and MBCPP). LATA outperforms the state-of-the-art baselines by 11.58%, 6.89%, and 5.07% on CodeGen, and 4.38%, 2.85%, and 2.74% on GPT-3.5 in terms of Pass@1, respectively.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在代码生成中表现出了吸引人的上下文学习（ICL）能力。LLM 接受一个包含需求代码示例和新需求的提示，并输出新的程序。现有的研究发现，ICL 受到示例的影响很大，因此引发了研究示例选择的研究。然而，现有的方法 Randomly 选择示例或者只考虑需求文本相似性来 retrieve，导致表现不佳。在这篇论文中，我们提出了一种新的学习基于选择方法 named LAIL（LLM-Aware In-context Learning）。给定一个候选示例，我们利用 LLM 自己来估算它，通过考虑需求和示例下的生成概率来Feedback probability。然后，我们将候选示例标记为正例或者负例，根据概率反馈。基于标记数据，我们导入了对比学习目标，以培养一个有效的检索器，使其获得 LLM 在代码生成中的偏好。我们在三个 LLM 上应用 LAIL，并对 MBJP、MBPP 和 MBCPP 三个表示性数据集进行评估。LATA 与当前基eline 相比，提高了代码生成的性能，具体是11.58%、6.89% 和 5.07% 的提升。
</details></li>
</ul>
<hr>
<h2 id="Overview-of-ImageArg-2023-The-First-Shared-Task-in-Multimodal-Argument-Mining"><a href="#Overview-of-ImageArg-2023-The-First-Shared-Task-in-Multimodal-Argument-Mining" class="headerlink" title="Overview of ImageArg-2023: The First Shared Task in Multimodal Argument Mining"></a>Overview of ImageArg-2023: The First Shared Task in Multimodal Argument Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12172">http://arxiv.org/abs/2310.12172</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhexiong Liu, Mohamed Elaraby, Yang Zhong, Diane Litman</li>
<li>for: 这篇论文提供了 ImageArg 共同任务的概述，这是第一个 Multimodal Argument Mining 共同任务，它在 EMNLP 2023 年度会议上召开。</li>
<li>methods: 这篇论文描述了两个分类子任务：（1）Argument Stance Classification，即判断一个包含图片和文本的推文是否支持或反对一个热点话题（如枪支持和堕胎）；（2）Image Persuasiveness Classification，即判断图片是否使文本更加吸引人。</li>
<li>results: 这个共同任务收到了 31 个参赛作品，其中 21 个来自 9 个团队，来自 6 个国家。最佳提交在 Subtask-A 中获得了 F1 分数 0.8647，而在 Subtask-B 中获得了 F1 分数 0.5561。<details>
<summary>Abstract</summary>
This paper presents an overview of the ImageArg shared task, the first multimodal Argument Mining shared task co-located with the 10th Workshop on Argument Mining at EMNLP 2023. The shared task comprises two classification subtasks - (1) Subtask-A: Argument Stance Classification; (2) Subtask-B: Image Persuasiveness Classification. The former determines the stance of a tweet containing an image and a piece of text toward a controversial topic (e.g., gun control and abortion). The latter determines whether the image makes the tweet text more persuasive. The shared task received 31 submissions for Subtask-A and 21 submissions for Subtask-B from 9 different teams across 6 countries. The top submission in Subtask-A achieved an F1-score of 0.8647 while the best submission in Subtask-B achieved an F1-score of 0.5561.
</details>
<details>
<summary>摘要</summary>
这份论文介绍了图像论据共同任务（ImageArg），这是在EMNLP 2023年工作坊上的第一个多Modal Argument Mining共同任务。该任务包括两个分类子任务：（1）子任务A：图像立场分类；（2）子任务B：图像宣传效果分类。前者确定一个推文中的图像和文本对于一个争议话题（例如，枪支控制和堕胎）的立场。后者确定图像是否使得推文文本更加吸引人。共同任务收到了31个提交 для子任务A和21个提交 для子任务B来自9个不同的团队在6个国家。最佳提交在子任务A中取得了F1分数0.8647，而最佳提交在子任务B中取得了F1分数0.5561。
</details></li>
</ul>
<hr>
<h2 id="KGQuiz-Evaluating-the-Generalization-of-Encoded-Knowledge-in-Large-Language-Models"><a href="#KGQuiz-Evaluating-the-Generalization-of-Encoded-Knowledge-in-Large-Language-Models" class="headerlink" title="KGQuiz: Evaluating the Generalization of Encoded Knowledge in Large Language Models"></a>KGQuiz: Evaluating the Generalization of Encoded Knowledge in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09725">http://arxiv.org/abs/2310.09725</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leopoldwhite/kgquiz">https://github.com/leopoldwhite/kgquiz</a></li>
<li>paper_authors: Yuyang Bai, Shangbin Feng, Vidhisha Balachandran, Zhaoxuan Tan, Shiqi Lou, Tianxing He, Yulia Tsvetkov</li>
<li>for: 这个论文旨在探讨大语言模型（LLM）在知识培养任务上的表现，以及如何系统地评估LLM的知识能力和其在不同知识领域和任务格式下的普适性。</li>
<li>methods: 这篇论文提出了一个名为KGQuiz的知识强度测试 benchmark，用于全面检验LLM的知识普适性和可行性。KGQuiz包括三个知识领域和五种任务 formats，从简单的真或假问题到复杂的开放知识生成。</li>
<li>results: 经过广泛的实验表明，LLM在简单的知识 QA 任务上表现出色，但是需要更复杂的推理或使用域pecific的知识时仍然存在很大挑战。这些结果表明KGQuiz可以用于分析LLM的知识能力和普适性在不同知识领域和任务格式下的变化。<details>
<summary>Abstract</summary>
Large language models (LLMs) demonstrate remarkable performance on knowledge-intensive tasks, suggesting that real-world knowledge is encoded in their model parameters. However, besides explorations on a few probing tasks in limited knowledge domains, it is not well understood how to evaluate LLMs' knowledge systematically and how well their knowledge abilities generalize, across a spectrum of knowledge domains and progressively complex task formats. To this end, we propose KGQuiz, a knowledge-intensive benchmark to comprehensively investigate the knowledge generalization abilities of LLMs. KGQuiz is a scalable framework constructed from triplet-based knowledge, which covers three knowledge domains and consists of five tasks with increasing complexity: true-or-false, multiple-choice QA, blank filling, factual editing, and open-ended knowledge generation. To gain a better understanding of LLMs' knowledge abilities and their generalization, we evaluate 10 open-source and black-box LLMs on the KGQuiz benchmark across the five knowledge-intensive tasks and knowledge domains. Extensive experiments demonstrate that LLMs achieve impressive performance in straightforward knowledge QA tasks, while settings and contexts requiring more complex reasoning or employing domain-specific facts still present significant challenges. We envision KGQuiz as a testbed to analyze such nuanced variations in performance across domains and task formats, and ultimately to understand, evaluate, and improve LLMs' knowledge abilities across a wide spectrum of knowledge domains and tasks.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在知识密集任务中表现出色，表明其模型参数中含有真实世界知识。然而，关于如何系统地评估 LLM 的知识能力和其知识能力是否可以普遍应用于多个知识领域和复杂任务格式，还不够了解。为此，我们提出了 KGQuiz，一个用于全面探索 LLM 的知识普适能力的benchmark。KGQuiz 基于 triplet 知识结构，覆盖了三个知识领域，包括五种任务 formats，从简单的true-or-false 和多选问答，到复杂的blank filling和factual editing，最后是开放式知识生成。为了更好地理解 LLM 的知识能力和其普适性，我们在 KGQuiz benchmark 上测试了 10 个开源和黑盒 LLM，并进行了广泛的实验。结果表明， LLM 在直观知识 QA 任务中表现出色，但是需要更复杂的解释或使用域pecific的事实时仍然存在很大的挑战。我们认为 KGQuiz 可以作为一个测试台来分析这些 nuanced 的表现差异，并 ultimately 理解、评估和提高 LLM 的知识能力在多个知识领域和任务格式中。
</details></li>
</ul>
<hr>
<h2 id="HiCL-Hierarchical-Contrastive-Learning-of-Unsupervised-Sentence-Embeddings"><a href="#HiCL-Hierarchical-Contrastive-Learning-of-Unsupervised-Sentence-Embeddings" class="headerlink" title="HiCL: Hierarchical Contrastive Learning of Unsupervised Sentence Embeddings"></a>HiCL: Hierarchical Contrastive Learning of Unsupervised Sentence Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09720">http://arxiv.org/abs/2310.09720</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuofeng Wu, Chaowei Xiao, VG Vinod Vydiswaran</li>
<li>for: 提高序列表示学习的效率和有效性，通过地方归一化和全序列归一化的对比学习来学习地方和全序列之间的关系。</li>
<li>methods: 提出了一种层次对比学习框架（HiCL），将序列分成多个段，使用地方和全序列归一化对比学习来学习段级和序列级关系，并且通过首先编码短段并然后聚合以提高训练效率。</li>
<li>results: 对比于传统方法，HiCL能够提高7种广泛评估的STS任务的前一个表现，升师平均提高+0.2%（BERT-large）和+0.44%（RoBERTa-large）。<details>
<summary>Abstract</summary>
In this paper, we propose a hierarchical contrastive learning framework, HiCL, which considers local segment-level and global sequence-level relationships to improve training efficiency and effectiveness. Traditional methods typically encode a sequence in its entirety for contrast with others, often neglecting local representation learning, leading to challenges in generalizing to shorter texts. Conversely, HiCL improves its effectiveness by dividing the sequence into several segments and employing both local and global contrastive learning to model segment-level and sequence-level relationships. Further, considering the quadratic time complexity of transformers over input tokens, HiCL boosts training efficiency by first encoding short segments and then aggregating them to obtain the sequence representation. Extensive experiments show that HiCL enhances the prior top-performing SNCSE model across seven extensively evaluated STS tasks, with an average increase of +0.2% observed on BERT-large and +0.44% on RoBERTa-large.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一个层次对比学习框架，即HiCL，该框架考虑了本地分割段级和全序列级关系，以提高训练效率和有效性。传统方法通常将序列编码为整体对比他们，而忽略本地表示学习，这会导致对短文本掌握困难。相反，HiCL通过将序列分割成多个段，并使用本地和全序列对比学习来模型段级和序列级关系。此外，考虑到 transformer 对输入字符数的平方时间复杂度，HiCL 提高了训练效率，先对短段进行编码，然后将其聚合以获得序列表示。广泛的实验表明，HiCL 可以提高先前的最佳 SNCSE 模型在七个广泛评估的 STS 任务上，平均提高 +0.2% 在 BERT-large 上和 +0.44% 在 RoBERTa-large 上。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/15/cs.CL_2023_10_15/" data-id="cloh7tqed00cb7b88bthib8uc" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_10_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/15/cs.LG_2023_10_15/" class="article-date">
  <time datetime="2023-10-15T10:00:00.000Z" itemprop="datePublished">2023-10-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/15/cs.LG_2023_10_15/">cs.LG - 2023-10-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="AMAGO-Scalable-In-Context-Reinforcement-Learning-for-Adaptive-Agents"><a href="#AMAGO-Scalable-In-Context-Reinforcement-Learning-for-Adaptive-Agents" class="headerlink" title="AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents"></a>AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09971">http://arxiv.org/abs/2310.09971</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ut-austin-rpl/amago">https://github.com/ut-austin-rpl/amago</a></li>
<li>paper_authors: Jake Grigsby, Linxi Fan, Yuke Zhu</li>
<li>for: The paper is written to tackle the challenges of generalization, long-term memory, and meta-learning in in-context Reinforcement Learning (RL) agents.</li>
<li>methods: The paper proposes a new in-context RL agent called AMAGO, which uses sequence models and off-policy learning to overcome the limitations of previous approaches.</li>
<li>results: The paper demonstrates the strong performance of AMAGO in meta-RL and long-term memory domains, and shows that it can solve goal-conditioned problems with challenging exploration. Additionally, the paper introduces a novel hindsight relabeling scheme that allows AMAGO to solve open-world domains.<details>
<summary>Abstract</summary>
We introduce AMAGO, an in-context Reinforcement Learning (RL) agent that uses sequence models to tackle the challenges of generalization, long-term memory, and meta-learning. Recent works have shown that off-policy learning can make in-context RL with recurrent policies viable. Nonetheless, these approaches require extensive tuning and limit scalability by creating key bottlenecks in agents' memory capacity, planning horizon, and model size. AMAGO revisits and redesigns the off-policy in-context approach to successfully train long-sequence Transformers over entire rollouts in parallel with end-to-end RL. Our agent is uniquely scalable and applicable to a wide range of problems. We demonstrate its strong performance empirically in meta-RL and long-term memory domains. AMAGO's focus on sparse rewards and off-policy data also allows in-context learning to extend to goal-conditioned problems with challenging exploration. When combined with a novel hindsight relabeling scheme, AMAGO can solve a previously difficult category of open-world domains, where agents complete many possible instructions in procedurally generated environments. We evaluate our agent on three goal-conditioned domains and study how its individual improvements connect to create a generalist policy.
</details>
<details>
<summary>摘要</summary>
我们介绍AMAGO，一个内Context Reinforcement Learning（RL）代理人，使用序列模型解决通用化、长期记忆和元学习的挑战。现有研究表明，离政RL可以使内Context RL with recurrent policies成为可能。然而，这些方法需要广泛的调整和限制数据容量、观察 horizon和模型大小，导致代理人的可扩展性和应用范围受限。AMAGO重新评估和重新设计了离政内Context Approach，成功地在整个推套中平行训练长序Transformer，并且具有广泛适用性。我们在Meta-RL和长期记忆领域 empirically 显示了它的强大表现。AMAGO的专注点在于罕见的 reward和离政数据也使内Context learning扩展到目标条件下的问题。当与一个新的预测重新标示方案相结合时，AMAGO可以解决一些过去Difficult的开放世界领域，其中代理人完成了许多可能的指令在生成的环境中。我们在三个目标条件下评估了我们的代理人，并研究了它们个别的改进如何相互连接，创建一个通用的政策。
</details></li>
</ul>
<hr>
<h2 id="Theoretical-Evaluation-of-Asymmetric-Shapley-Values-for-Root-Cause-Analysis"><a href="#Theoretical-Evaluation-of-Asymmetric-Shapley-Values-for-Root-Cause-Analysis" class="headerlink" title="Theoretical Evaluation of Asymmetric Shapley Values for Root-Cause Analysis"></a>Theoretical Evaluation of Asymmetric Shapley Values for Root-Cause Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09961">http://arxiv.org/abs/2310.09961</a></li>
<li>repo_url: None</li>
<li>paper_authors: Domokos M. Kelen, Mihály Petreczky, Péter Kersch, András A. Benczúr</li>
<li>for: 本研究探讨非对称的雪岭值（ASV），它是SHAP加itive本地解释方法的一种变体，可以在模型预测中检测不公正抵制。</li>
<li>methods: 本研究使用variance的方法来解释ASV的方法，并在多个实际 dataset上进行了比较，以证明ASV在特定模型家族中的有用性。</li>
<li>results: 研究发现，在某些情况下，ASV可能会产生Counter-intuitive的解释结果，这可能会导致模型预测中的根本原因分析错误。此外，研究还发现了一些特定的模型家族，如泛型加itive模型（GAM），在这些家族中，ASV具有愉悦的性质。<details>
<summary>Abstract</summary>
In this work, we examine Asymmetric Shapley Values (ASV), a variant of the popular SHAP additive local explanation method. ASV proposes a way to improve model explanations incorporating known causal relations between variables, and is also considered as a way to test for unfair discrimination in model predictions. Unexplored in previous literature, relaxing symmetry in Shapley values can have counter-intuitive consequences for model explanation. To better understand the method, we first show how local contributions correspond to global contributions of variance reduction. Using variance, we demonstrate multiple cases where ASV yields counter-intuitive attributions, arguably producing incorrect results for root-cause analysis. Second, we identify generalized additive models (GAM) as a restricted class for which ASV exhibits desirable properties. We support our arguments by proving multiple theoretical results about the method. Finally, we demonstrate the use of asymmetric attributions on multiple real-world datasets, comparing the results with and without restricted model families using gradient boosting and deep learning models.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们研究非对称的雪平值（ASV），这是SHAP添加式本地解释方法的一种变体。ASV提出了 incorporating known causal relations between variables的方法，并且被视为测试模型预测中的不公正折衔测试。在前期文献中没有被研究过，放弃雪平值的对称性可能会导致模型解释中的counter-intuitive consequence。为了更好地理解这种方法，我们首先示出了local contributions与global contributions of variance reduction的对应关系。使用方差，我们展示了多种情况下，ASV可能会生成错误的root-cause分析结果。其次，我们认为Generalized Additive Models（GAM）是一种受限的模型家族，ASV在这种模型家族中具有恰当的性质。我们支持我们的 Argument by proving multiple theoretical results about the method。最后，我们在多个实际 dataset上使用非对称的贡献值，并与和 без Restricted model families using gradient boosting和deep learning模型进行比较。
</details></li>
</ul>
<hr>
<h2 id="Deep-Reinforcement-Learning-with-Explicit-Context-Representation"><a href="#Deep-Reinforcement-Learning-with-Explicit-Context-Representation" class="headerlink" title="Deep Reinforcement Learning with Explicit Context Representation"></a>Deep Reinforcement Learning with Explicit Context Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09924">http://arxiv.org/abs/2310.09924</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francisco Munguia-Galeano, Ah-Hwee Tan, Ze Ji</li>
<li>for:  solves complex computational problems with contextual information</li>
<li>methods:  uses Iota explicit context representation (IECR) framework with contextual key frames (CKFs) and two loss functions</li>
<li>results:  significantly outperforms state-of-the-art equivalents in five discrete environments with contextual information<details>
<summary>Abstract</summary>
Reinforcement learning (RL) has shown an outstanding capability for solving complex computational problems. However, most RL algorithms lack an explicit method that would allow learning from contextual information. Humans use context to identify patterns and relations among elements in the environment, along with how to avoid making wrong actions. On the other hand, what may seem like an obviously wrong decision from a human perspective could take hundreds of steps for an RL agent to learn to avoid. This paper proposes a framework for discrete environments called Iota explicit context representation (IECR). The framework involves representing each state using contextual key frames (CKFs), which can then be used to extract a function that represents the affordances of the state; in addition, two loss functions are introduced with respect to the affordances of the state. The novelty of the IECR framework lies in its capacity to extract contextual information from the environment and learn from the CKFs' representation. We validate the framework by developing four new algorithms that learn using context: Iota deep Q-network (IDQN), Iota double deep Q-network (IDDQN), Iota dueling deep Q-network (IDuDQN), and Iota dueling double deep Q-network (IDDDQN). Furthermore, we evaluate the framework and the new algorithms in five discrete environments. We show that all the algorithms, which use contextual information, converge in around 40,000 training steps of the neural networks, significantly outperforming their state-of-the-art equivalents.
</details>
<details>
<summary>摘要</summary>
强化学习（RL）已经表现出解决复杂计算问题的惊人能力。然而，大多数RL算法缺乏显式的方法来学习上下文信息。人类通过上下文来识别环境中元素之间的征交和相互关系，以及如何避免 incorrect 行为。相反，RL Agent可能需要多达百步才能学习避免错误的决策。这篇论文提出了一个名为IECR（Iota Explicit Context Representation）的框架，该框架可以在精确的环境中提取上下文信息，并学习CKFs（上下文关键帧）的表示。此外，本文还提出了两个相对于上下文的产品函数损失函数。IECR框架的创新之处在于可以从环境中提取上下文信息，并学习CKFs的表示。我们验证了IECR框架，并开发了四种使用上下文学习的算法：Iota Deep Q-Network（IDQN）、Iota Double Deep Q-Network（IDDQN）、Iota Duelling Deep Q-Network（IDuDQN）和Iota Duelling Double Deep Q-Network（IDDDQN）。此外，我们还在五个精确环境中评估了IECR框架和这些算法。我们发现，所有使用上下文信息的算法在40,000步训练步骤后，可以快速并高效地学习，与当前最佳算法相比显著性能更高。
</details></li>
</ul>
<hr>
<h2 id="BONES-Near-Optimal-Neural-Enhanced-Video-Streaming"><a href="#BONES-Near-Optimal-Neural-Enhanced-Video-Streaming" class="headerlink" title="BONES: Near-Optimal Neural-Enhanced Video Streaming"></a>BONES: Near-Optimal Neural-Enhanced Video Streaming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09920">http://arxiv.org/abs/2310.09920</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lingdong Wang, Simran Singh, Jacob Chakareski, Mohammad Hajiesmaili, Ramesh K. Sitaraman</li>
<li>for: 提高用户视频流程体验质量（Quality of Experience，QoE）</li>
<li>methods: 使用神经网络优化技术（Neural Enhancement）和在线启发式优化算法（Online Lyapunov Optimization）提高视频质量</li>
<li>results: 比对当前状态艺技术，BONES算法可以提高用户视频流程体验质量4%到13%，显示其在提高视频流程体验质量方面具有潜在的应用前景。<details>
<summary>Abstract</summary>
Accessing high-quality video content can be challenging due to insufficient and unstable network bandwidth. Recent advances in neural enhancement have shown promising results in improving the quality of degraded videos through deep learning. Neural-Enhanced Streaming (NES) incorporates this new approach into video streaming, allowing users to download low-quality video segments and then enhance them to obtain high-quality content without violating the playback of the video stream. We introduce BONES, an NES control algorithm that jointly manages the network and computational resources to maximize the quality of experience (QoE) of the user. BONES formulates NES as a Lyapunov optimization problem and solves it in an online manner with near-optimal performance, making it the first NES algorithm to provide a theoretical performance guarantee. Our comprehensive experimental results indicate that BONES increases QoE by 4% to 13% over state-of-the-art algorithms, demonstrating its potential to enhance the video streaming experience for users. Our code and data will be released to the public.
</details>
<details>
<summary>摘要</summary>
Accessing high-quality video content can be challenging due to insufficient and unstable network bandwidth. Recent advances in neural enhancement have shown promising results in improving the quality of degraded videos through deep learning. Neural-Enhanced Streaming (NES) incorporates this new approach into video streaming, allowing users to download low-quality video segments and then enhance them to obtain high-quality content without violating the playback of the video stream. We introduce BONES, an NES control algorithm that jointly manages the network and computational resources to maximize the quality of experience (QoE) of the user. BONES formulates NES as a Lyapunov optimization problem and solves it in an online manner with near-optimal performance, making it the first NES algorithm to provide a theoretical performance guarantee. Our comprehensive experimental results indicate that BONES increases QoE by 4% to 13% over state-of-the-art algorithms, demonstrating its potential to enhance the video streaming experience for users. Our code and data will be released to the public.Here's the text in Traditional Chinese:Accessing high-quality video content can be challenging due to insufficient and unstable network bandwidth. Recent advances in neural enhancement have shown promising results in improving the quality of degraded videos through deep learning. Neural-Enhanced Streaming (NES) incorporates this new approach into video streaming, allowing users to download low-quality video segments and then enhance them to obtain high-quality content without violating the playback of the video stream. We introduce BONES, an NES control algorithm that jointly manages the network and computational resources to maximize the quality of experience (QoE) of the user. BONES formulates NES as a Lyapunov optimization problem and solves it in an online manner with near-optimal performance, making it the first NES algorithm to provide a theoretical performance guarantee. Our comprehensive experimental results indicate that BONES increases QoE by 4% to 13% over state-of-the-art algorithms, demonstrating its potential to enhance the video streaming experience for users. Our code and data will be released to the public.
</details></li>
</ul>
<hr>
<h2 id="Evaluation-of-feature-selection-performance-for-identification-of-best-effective-technical-indicators-on-stock-market-price-prediction"><a href="#Evaluation-of-feature-selection-performance-for-identification-of-best-effective-technical-indicators-on-stock-market-price-prediction" class="headerlink" title="Evaluation of feature selection performance for identification of best effective technical indicators on stock market price prediction"></a>Evaluation of feature selection performance for identification of best effective technical indicators on stock market price prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09903">http://arxiv.org/abs/2310.09903</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fatemeh Moodi, Amir Jahangard-Rafsanjani</li>
<li>for: 本研究的目的是通过特征选择来选择最佳的股票市场指标，以预测股票市场价格的误差最小。</li>
<li>methods: 本研究使用了 wrapper 特征选择方法，包括 SFS 和 SBS，并使用了 10 种估计器和 123 个技术指标来预测股票市场价格。</li>
<li>results: 研究发现，每种 wrapper 特征选择方法都有不同的结果，与不同的机器学习方法相关。ridge 和 LR 估计器，单独使用和与 wrapper 特征选择方法结合使用，在所有评价标准下得到了最佳股票市场预测结果。<details>
<summary>Abstract</summary>
Due to the influence of many factors, including technical indicators on stock market prediction, feature selection is important to choose the best indicators. One of the feature selection methods that consider the performance of models during feature selection is the wrapper feature selection method. The aim of this research is to identify a combination of the best stock market indicators through feature selection to predict the stock market price with the least error. In order to evaluate the impact of wrapper feature selection techniques on stock market prediction, in this paper SFS and SBS with 10 estimators and 123 technical indicators have been examined on the last 13 years of Apple Company. Also, by the proposed method, the data created by the 3-day time window were converted to the appropriate input for regression methods. Based on the results observed: (1) Each wrapper feature selection method has different results with different machine learning methods, and each method is more correlated with a specific set of technical indicators of the stock market. (2) Ridge and LR estimates alone, and with two methods of the wrapper feature selection, namely SFS and SBS; They had the best results with all assessment criteria for market forecast. (3)The Ridge and LR method with all the R2, MSE, RMSE, MAE and MAPE have the best stock market prediction results. Also, the MLP Regression Method, along with the Sequential Forwards Selection and the MSE, had the best performance. SVR regression, along with the SFS and the MSE, has improved greatly compared to the SVR regression with all indicators. (4) It was also observed that different features are selected by different ML methods with different evaluation parameters. (5) Most ML methods have used the Squeeze_pro, Percentage Price Oscillator, Thermo, Decay, Archer On-Balance Volume, Bollinger Bands, Squeeze and Ichimoku indicator.
</details>
<details>
<summary>摘要</summary>
因为多种因素的影响，包括技术指标在股票市场预测中，特征选择是重要的。本研究的目标是通过特征选择选择最佳的股票市场指标，以预测股票市场价格的误差最小。为了评估包装特征选择技术对股票市场预测的影响，本文在Apple公司上评估了13年的数据。具体来说，通过提posed方法，将3天时窗内的数据转换为适合回归方法的输入。根据结果所见：1. 每种包装特征选择方法都有不同的结果，与不同的机器学习方法相关。每种方法更加相关于股票市场技术指标的特定集。2. Ridge和LR估计独立，以及使用SFS和SBS两种包装特征选择方法时，在所有评价标准中表现最佳。3. Ridge和LR方法与所有评价标准（R2、MSE、RMSE、MAE和MAPE）表现最佳。此外，MLP回归方法，结合Sequential Forwards Selection和MSE，也表现出色。4.  Observation showed that different ML methods select different features with different evaluation parameters.5. Most ML methods use Squeeze_pro、Percentage Price Oscillator、Thermo、Decay、Archer On-Balance Volume、Bollinger Bands、Squeeze和Ichimoku指标。
</details></li>
</ul>
<hr>
<h2 id="Towards-Deep-Learning-Models-Resistant-to-Transfer-based-Adversarial-Attacks-via-Data-centric-Robust-Learning"><a href="#Towards-Deep-Learning-Models-Resistant-to-Transfer-based-Adversarial-Attacks-via-Data-centric-Robust-Learning" class="headerlink" title="Towards Deep Learning Models Resistant to Transfer-based Adversarial Attacks via Data-centric Robust Learning"></a>Towards Deep Learning Models Resistant to Transfer-based Adversarial Attacks via Data-centric Robust Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09891">http://arxiv.org/abs/2310.09891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yulong Yang, Chenhao Lin, Xiang Ji, Qiwei Tian, Qian Li, Hongshan Yang, Zhibo Wang, Chao Shen</li>
<li>for: 防御对于转移基于攻击的威胁，提供一种新的防御方法，即数据中心Robust Learning（DRL）。</li>
<li>methods: DRL使用一招一时的反恶例增强，而不是在整个训练过程中优化对抗例。</li>
<li>results: DRL在黑盒Robustness方面比PGD-AT、TRADES、EAT和FAT等常用AT技术表现出色，并且可以与多种数据增强和损失规则结合使用，以提高防御性。<details>
<summary>Abstract</summary>
Transfer-based adversarial attacks raise a severe threat to real-world deep learning systems since they do not require access to target models. Adversarial training (AT), which is recognized as the strongest defense against white-box attacks, has also guaranteed high robustness to (black-box) transfer-based attacks. However, AT suffers from heavy computational overhead since it optimizes the adversarial examples during the whole training process. In this paper, we demonstrate that such heavy optimization is not necessary for AT against transfer-based attacks. Instead, a one-shot adversarial augmentation prior to training is sufficient, and we name this new defense paradigm Data-centric Robust Learning (DRL). Our experimental results show that DRL outperforms widely-used AT techniques (e.g., PGD-AT, TRADES, EAT, and FAT) in terms of black-box robustness and even surpasses the top-1 defense on RobustBench when combined with diverse data augmentations and loss regularizations. We also identify other benefits of DRL, for instance, the model generalization capability and robust fairness.
</details>
<details>
<summary>摘要</summary>
transfer-based adversarial attacks pose a severe threat to real-world deep learning systems, as they do not require access to target models. adversarial training (AT), which is recognized as the strongest defense against white-box attacks, has also guaranteed high robustness to (black-box) transfer-based attacks. however, AT suffers from heavy computational overhead, as it optimizes adversarial examples during the entire training process. in this paper, we demonstrate that such heavy optimization is not necessary for AT against transfer-based attacks. instead, a one-shot adversarial augmentation prior to training is sufficient, and we name this new defense paradigm data-centric robust learning (drl). our experimental results show that drl outperforms widely-used at techniques (e.g., pgd-at, trades, eat, and fat) in terms of black-box robustness and even surpasses the top-1 defense on robustbench when combined with diverse data augmentations and loss regularizations. we also identify other benefits of drl, such as model generalization capability and robust fairness.
</details></li>
</ul>
<hr>
<h2 id="Score-Based-Methods-for-Discrete-Optimization-in-Deep-Learning"><a href="#Score-Based-Methods-for-Discrete-Optimization-in-Deep-Learning" class="headerlink" title="Score-Based Methods for Discrete Optimization in Deep Learning"></a>Score-Based Methods for Discrete Optimization in Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09890">http://arxiv.org/abs/2310.09890</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Eric Lei, Arman Adibi, Hamed Hassani</li>
<li>for: 这 paper 是用于解决深度学习任务中的离散优化问题的。</li>
<li>methods: 这 paper 使用了一种分数函数方法来解决这些问题，该方法使用了一个分数函数作为目标函数的代理，并使用了隐藏变量的嵌入和自动导函数框架来并行计算反向传播。</li>
<li>results: 该 paper 的实验表明，在对抗式集成分类任务中，该方法可以实现一个更好的平衡点，即快速并且解决高维数据的问题。<details>
<summary>Abstract</summary>
Discrete optimization problems often arise in deep learning tasks, despite the fact that neural networks typically operate on continuous data. One class of these problems involve objective functions which depend on neural networks, but optimization variables which are discrete. Although the discrete optimization literature provides efficient algorithms, they are still impractical in these settings due to the high cost of an objective function evaluation, which involves a neural network forward-pass. In particular, they require $O(n)$ complexity per iteration, but real data such as point clouds have values of $n$ in thousands or more. In this paper, we investigate a score-based approximation framework to solve such problems. This framework uses a score function as a proxy for the marginal gain of the objective, leveraging embeddings of the discrete variables and speed of auto-differentiation frameworks to compute backward-passes in parallel. We experimentally demonstrate, in adversarial set classification tasks, that our method achieves a superior trade-off in terms of speed and solution quality compared to heuristic methods.
</details>
<details>
<summary>摘要</summary>
几乎所有深度学习任务中都会遇到离散优化问题，即使神经网络通常处理连续数据。这类问题中的目标函数取决于神经网络，但优化变量是离散的。虽然离散优化文献中提供了高效的算法，但它们在这些设置中仍然不实用，因为目标函数评估的成本高，需要对神经网络进行前进传播，这需要 $O(n)$ 复杂度每次迭代。例如，实际数据如点云可能有 thousands 或更多的值。在这篇论文中，我们研究了一种分数函数近似框架，用于解决这些问题。这个框架使用分数函数作为目标函数的代理，利用离散变量的嵌入和自动导数框架来并行计算反向传播。我们在随机设置中的对抗性分类任务中实验ally示出，我们的方法可以在速度和解决质量之间取得优化的负号比例。
</details></li>
</ul>
<hr>
<h2 id="Empower-Text-Attributed-Graphs-Learning-with-Large-Language-Models-LLMs"><a href="#Empower-Text-Attributed-Graphs-Learning-with-Large-Language-Models-LLMs" class="headerlink" title="Empower Text-Attributed Graphs Learning with Large Language Models (LLMs)"></a>Empower Text-Attributed Graphs Learning with Large Language Models (LLMs)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09872">http://arxiv.org/abs/2310.09872</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianxiang Yu, Yuxiang Ren, Chenghua Gong, Jiaqi Tan, Xiang Li, Xuecang Zhang</li>
<li>for: 提高文本权重图像的性能在几个shot情况下（提高 node classification 任务的性能）</li>
<li>methods: 使用 Large Language Models (LLMs) 提取标签中的Semantic信息，并生成相应的类别 exemplars，然后使用边预测器捕捉原始数据中的结构信息，并将新生成的样本纳入原始图像中。</li>
<li>results: 对ogbn-arxiv dataset进行了广泛的实验，并显示了在1-shot情况下对基eline模型的76%提升。<details>
<summary>Abstract</summary>
Text-attributed graphs have recently garnered significant attention due to their wide range of applications in web domains. Existing methodologies employ word embedding models for acquiring text representations as node features, which are subsequently fed into Graph Neural Networks (GNNs) for training. Recently, the advent of Large Language Models (LLMs) has introduced their powerful capabilities in information retrieval and text generation, which can greatly enhance the text attributes of graph data. Furthermore, the acquisition and labeling of extensive datasets are both costly and time-consuming endeavors. Consequently, few-shot learning has emerged as a crucial problem in the context of graph learning tasks. In order to tackle this challenge, we propose a lightweight paradigm called ENG, which adopts a plug-and-play approach to empower text-attributed graphs through node generation using LLMs. Specifically, we utilize LLMs to extract semantic information from the labels and generate samples that belong to these categories as exemplars. Subsequently, we employ an edge predictor to capture the structural information inherent in the raw dataset and integrate the newly generated samples into the original graph. This approach harnesses LLMs for enhancing class-level information and seamlessly introduces labeled nodes and edges without modifying the raw dataset, thereby facilitating the node classification task in few-shot scenarios. Extensive experiments demonstrate the outstanding performance of our proposed paradigm, particularly in low-shot scenarios. For instance, in the 1-shot setting of the ogbn-arxiv dataset, ENG achieves a 76% improvement over the baseline model.
</details>
<details>
<summary>摘要</summary>
文本拥有Graph neural networks (GNNs) 在网络领域中得到了广泛的应用，现有的方法使用word embedding模型来获取文本表示，然后将其传递给GNNs进行训练。在大型自然语言模型（LLMs）的出现之后，这些 модели的强大能力在信息检索和文本生成中得到了应用，可以大幅提高文本特征。然而，收集和标注大量数据都是成本和时间consuming的任务。因此，几拍学习成为了图学习任务中的一个关键问题。为解决这个问题，我们提出了一种轻量级的方法called ENG，它采用了一种插件式的方法来授权文本拥有Graph中的节点生成。具体来说，我们使用LLMs来提取标签中的semantic信息，并生成符合这些类别的样本作为示例。然后，我们使用边预测器来捕捉原始数据中的结构信息，并将新生成的样本与原始图 Integrate into the graph。这种方法利用了LLMs来增强类别信息，无需修改原始数据，因此可以轻松地在几拍学习场景下进行节点分类。我们的实验表明，ENG方法在低shot场景下表现出色，例如在ogbn-arxivdataset中的1拍 Setting中，ENG方法与基eline模型相比提高了76%。
</details></li>
</ul>
<hr>
<h2 id="Alpha-Elimination-Using-Deep-Reinforcement-Learning-to-Reduce-Fill-In-during-Sparse-Matrix-Decomposition"><a href="#Alpha-Elimination-Using-Deep-Reinforcement-Learning-to-Reduce-Fill-In-during-Sparse-Matrix-Decomposition" class="headerlink" title="Alpha Elimination: Using Deep Reinforcement Learning to Reduce Fill-In during Sparse Matrix Decomposition"></a>Alpha Elimination: Using Deep Reinforcement Learning to Reduce Fill-In during Sparse Matrix Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09852">http://arxiv.org/abs/2310.09852</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arpan Dasgupta, Pawan Kumar</li>
<li>for: 这个论文目的是对叠合矩阵进行分解，以提高分解过程中的效率和内存需求。</li>
<li>methods: 这个论文使用了一种基于强化学习的叠合矩阵重新排序方法，即alphaElimination。这个方法以单玩家游戏的形式表现出叠合矩阵重新排序问题，并使用Monte-Carlo tree search和神经网络来找到最佳移动。</li>
<li>results: 这个论文的结果显示，alphaElimination 可以与现有的热点排序法相比，实现更好的填充避免，并且对叠合矩阵的分解过程和解释过程都有很好的影响。<details>
<summary>Abstract</summary>
A large number of computational and scientific methods commonly require decomposing a sparse matrix into triangular factors as LU decomposition. A common problem faced during this decomposition is that even though the given matrix may be very sparse, the decomposition may lead to a denser triangular factors due to fill-in. A significant fill-in may lead to prohibitively larger computational costs and memory requirement during decomposition as well as during the solve phase. To this end, several heuristic sparse matrix reordering methods have been proposed to reduce fill-in before the decomposition. However, finding an optimal reordering algorithm that leads to minimal fill-in during such decomposition is known to be a NP-hard problem. A reinforcement learning based approach is proposed for this problem. The sparse matrix reordering problem is formulated as a single player game. More specifically, Monte-Carlo tree search in combination with neural network is used as a decision making algorithm to search for the best move in our game. The proposed method, alphaElimination is found to produce significantly lesser non-zeros in the LU decomposition as compared to existing state-of-the-art heuristic algorithms with little to no increase in overall running time of the algorithm. The code for the project will be publicly available here\footnote{\url{https://github.com/misterpawan/alphaEliminationPaper}.
</details>
<details>
<summary>摘要</summary>
许多计算和科学方法通常需要将稀疏矩阵分解为LU分解。在这个分解过程中，常见的问题是，即使给定矩阵很稀疏，但是分解可能会导致三角因子更加稠密，即fill-in问题。这种填充可能会导致计算成本和内存需求急剧增加。为解决这个问题，许多启发式稀疏矩阵重新排序方法已经被提出。然而，找到最优的重新排序算法，以使得在这个分解过程中避免填充，是一个NP困难问题。本文提出了一种基于强化学习的方法。将稀疏矩阵重新排序问题定义为单player游戏。具体来说，使用Monte-Carlo搜索和神经网络的决策算法来搜索最佳的移动。该方法被命名为alphaElimination，并发现它可以在LU分解中生成许多更少的非零元素，与现有的状态艺术算法相比，几乎没有增加总运行时间。代码将在以下链接公开：\footnote{\url{https://github.com/misterpawan/alphaEliminationPaper}。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-ML-model-accuracy-for-Digital-VLSI-circuits-using-diffusion-models-A-study-on-synthetic-data-generation"><a href="#Enhancing-ML-model-accuracy-for-Digital-VLSI-circuits-using-diffusion-models-A-study-on-synthetic-data-generation" class="headerlink" title="Enhancing ML model accuracy for Digital VLSI circuits using diffusion models: A study on synthetic data generation"></a>Enhancing ML model accuracy for Digital VLSI circuits using diffusion models: A study on synthetic data generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10691">http://arxiv.org/abs/2310.10691</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prasha Srivastava, Pawan Kumar, Zia Abbas</li>
<li>for: 这个研究旨在使用扩散模型生成人工数据，以提高后续机器学习模型在电子芯片设计、评估和测试等任务中的准确性。</li>
<li>methods: 我们使用HSPICE设计环境和22nm CMOS技术节点进行了仿真，以获得可靠的真实训练数据。</li>
<li>results: 我们的结果表明，扩散模型生成的人工数据和实际数据之间存在很close的相似性。我们验证了生成的数据质量，并证明了数据扩展确实有效地提高了VLSI设计中的预测性。<details>
<summary>Abstract</summary>
Generative AI has seen remarkable growth over the past few years, with diffusion models being state-of-the-art for image generation. This study investigates the use of diffusion models in generating artificial data generation for electronic circuits for enhancing the accuracy of subsequent machine learning models in tasks such as performance assessment, design, and testing when training data is usually known to be very limited. We utilize simulations in the HSPICE design environment with 22nm CMOS technology nodes to obtain representative real training data for our proposed diffusion model. Our results demonstrate the close resemblance of synthetic data using diffusion model to real data. We validate the quality of generated data, and demonstrate that data augmentation certainly effective in predictive analysis of VLSI design for digital circuits.
</details>
<details>
<summary>摘要</summary>
这篇研究探讨使用扩散模型来生成人工训练数据，以提高后续机器学习模型在电子遗传学 задача中的准确性。我们使用HSPICE设计环境，使用22nm CMOS技术架构，从实验中获得真实训练数据，以验证扩散模型的效果。我们的结果显示，扩散模型生成的 sintetic 数据和实际数据之间存在着类似的相似性。我们还证明了生成数据的质量，并证明了数据增强对适用于对数字遗传学设计的预测分析有效。</SYS>Note:* "扩散模型" (diffusion model) refers to a type of generative model that generates data by iteratively refining a random noise vector until it matches the target data distribution.* "HSPICE" is a circuit simulator that is widely used in the field of electronic design automation.* "CMOS" (Complementary Metal-Oxide-Semiconductor) is a technology node that is commonly used in the fabrication of integrated circuits.* "适用" (suitable) means appropriate or applicable.* "预测分析" (predictive analysis) refers to the use of statistical or machine learning techniques to forecast the behavior of a system or process.
</details></li>
</ul>
<hr>
<h2 id="XRMDN-A-Recurrent-Mixture-Density-Networks-based-Architecture-for-Short-Term-Probabilistic-Demand-Forecasting-in-Mobility-on-Demand-Systems-with-High-Volatility"><a href="#XRMDN-A-Recurrent-Mixture-Density-Networks-based-Architecture-for-Short-Term-Probabilistic-Demand-Forecasting-in-Mobility-on-Demand-Systems-with-High-Volatility" class="headerlink" title="XRMDN: A Recurrent Mixture Density Networks-based Architecture for Short-Term Probabilistic Demand Forecasting in Mobility-on-Demand Systems with High Volatility"></a>XRMDN: A Recurrent Mixture Density Networks-based Architecture for Short-Term Probabilistic Demand Forecasting in Mobility-on-Demand Systems with High Volatility</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09847">http://arxiv.org/abs/2310.09847</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoming Li, Hubert Normandin-Taillon, Chun Wang, Xiao Huang</li>
<li>for: 这个研究是为了提高现实的 mobilitity-on-demand（MoD）系统中的需求预测精度，因为需求是具有高度动态波动性，这些波动性难以预测使用传统时间序列预测方法。</li>
<li>methods: 本研究提出了一个扩展的回传混合密度网络（XRMDN），它将传统的重量和均值神经网络扩展到回传神经网络，以capture historic data-series data的趋势，从而获得更好的预测结果。</li>
<li>results: 根据实验结果，XRMDN比三种参考模型（包括统计、机器学习和深度学习模型）在三个评估指标上表现更好，特别是在具有强波动性的需求预测中。此外，XRMDN还可以帮助优化MoD系统中的其他应用问题，例如在不确定性下进行优化。<details>
<summary>Abstract</summary>
In real Mobility-on-Demand (MoD) systems, demand is subject to high and dynamic volatility, which is difficult to predict by conventional time-series forecasting approaches. Most existing forecasting approaches yield the point value as the prediction result, which ignores the uncertainty that exists in the forecasting result. This will lead to the forecasting result severely deviating from the true demand value due to the high volatility existing in demand. To fill the gap, we propose an extended recurrent mixture density network (XRMDN), which extends the weight and mean neural networks to recurrent neural networks. The recurrent neurons for mean and variance can capture the trend of the historical data-series data, which enables a better forecasting result in dynamic and high volatility. We conduct comprehensive experiments on one taxi trip record and one bike-sharing real MoD data set to validate the performance of XRMDN. Specifically, we compare our model to three types of benchmark models, including statistical, machine learning, and deep learning models on three evaluation metrics. The validation results show that XRMDN outperforms the three groups of benchmark models in terms of the evaluation metrics. Most importantly, XRMDN substantially improves the forecasting accuracy with the demands in strong volatility. Last but not least, this probabilistic demand forecasting model contributes not only to the demand prediction in MoD systems but also to other optimization application problems, especially optimization under uncertainty, in MoD applications.
</details>
<details>
<summary>摘要</summary>
真实的流动性-on-需求（MoD）系统中的需求受到高度和动态的不稳定性影响，这些影响难以预测通过传统时间序列预测方法。大多数现有预测方法只预测点值，忽略预测结果中存在的不确定性。这将导致预测结果与真实需求值严重不符，因为需求的高度不稳定。为了填补这个空白，我们提议一种扩展的循环混合密度网络（XRMDN）模型，扩展了权重和均值神经网络到循环神经网络。循环神经网络可以捕捉历史数据时系列数据的趋势，从而实现更好的预测结果在动态和高度不稳定的情况下。我们对一个出租车旅程记录和一个自行车分享真实MoD数据集进行了广泛的实验，以验证XRMDN的性能。具体来说，我们与三种参考模型进行比较，包括统计、机器学习和深度学习模型，并在三个评价指标上进行比较。验证结果显示，XRMDN在评价指标上都高于参考模型。此外，XRMDN在需求强度不稳定的情况下显著提高了预测精度。最后，这种probabilistic需求预测模型不仅有助于MoD系统中的需求预测，还有助于其他优化应用问题，尤其是在MoD应用中的不确定性优化问题。
</details></li>
</ul>
<hr>
<h2 id="Secure-and-Robust-Communications-for-Cislunar-Space-Networks"><a href="#Secure-and-Robust-Communications-for-Cislunar-Space-Networks" class="headerlink" title="Secure and Robust Communications for Cislunar Space Networks"></a>Secure and Robust Communications for Cislunar Space Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09835">http://arxiv.org/abs/2310.09835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Selen Gecgel Cetin, Gunes Karabulut Kurt, Angeles Vazquez-Castro</li>
<li>for: 这个研究旨在提供一个基于机器学习的cislunar空间领域意识能力，以确保无间断的月球和地球之间通信。</li>
<li>methods: 本研究提出了一个细部通道模型，以及两种可能会在cislunar空间发生的干扰模型。</li>
<li>results: 研究结果显示，使用机器学习算法的cislunar空间领域意识能力可以实现96%的准确性，并且显示出了这种方法的扎实性和可靠性。<details>
<summary>Abstract</summary>
There is no doubt that the Moon has become the center of interest for commercial and international actors. Over the past decade, the number of planned long-term missions has increased dramatically. This makes the establishment of cislunar space networks (CSNs) crucial to orchestrate uninterrupted communications between the Moon and Earth. However, there are numerous challenges, unknowns, and uncertainties associated with cislunar communications that may pose various risks to lunar missions. In this study, we aim to address these challenges for cislunar communications by proposing a machine learning-based cislunar space domain awareness (SDA) capability that enables robust and secure communications. To this end, we first propose a detailed channel model for selected cislunar scenarios. Secondly, we propose two types of interference that could model anomalies that occur in cislunar space and are so far known only to a limited extent. Finally, we discuss our cislunar SDA to work in conjunction with the spacecraft communication system. Our proposed cislunar SDA, involving heuristic learning capabilities with machine learning algorithms, detects interference models with over 96% accuracy. The results demonstrate the promising performance of our cislunar SDA approach for secure and robust cislunar communication.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>月球已成为商业和国际行动者的中心，过去一代，计划的长期任务数量有所增加。这使得在月球和地球之间建立cislunar空间网络（CSN）变得非常重要，以确保无间断的通信。然而，cislunar通信存在许多挑战、未知和不确定性，这些风险可能对月球任务产生影响。在这种情况下，我们提出了一种基于机器学习的cislunar空间领域意识（SDA）能力，以确保安全和可靠的通信。为此，我们首先提出了选择的cislunar场景下的通道模型。其次，我们提出了两种可能出现在cislunar空间中的干扰，这些干扰至今只有有限的知识。最后，我们讨论了我们的cislunar SDA如何与空间器通信系统结合使用。我们的提议的cislunar SDA，结合机器学习算法的启发学能力，可以检测干扰模型的准确率高达96%。结果表明我们的cislunar SDA方法在确保安全和可靠的cislunar通信方面表现出色。
</details></li>
</ul>
<hr>
<h2 id="MAGIC-Detecting-Advanced-Persistent-Threats-via-Masked-Graph-Representation-Learning"><a href="#MAGIC-Detecting-Advanced-Persistent-Threats-via-Masked-Graph-Representation-Learning" class="headerlink" title="MAGIC: Detecting Advanced Persistent Threats via Masked Graph Representation Learning"></a>MAGIC: Detecting Advanced Persistent Threats via Masked Graph Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09831">http://arxiv.org/abs/2310.09831</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fdudsde/magic">https://github.com/fdudsde/magic</a></li>
<li>paper_authors: Zian Jia, Yun Xiong, Yuhong Nan, Yao Zhang, Jinjing Zhao, Mi Wen</li>
<li>for: 这篇论文旨在探讨防止高级攻击者（APT）的攻击方法，并提出一个名为MAGIC的新型自我超级攻击探测方法，可以在不同的监控环境下进行多层级的探测。</li>
<li>methods: 本文使用隐藏标签数据库学来学习防止APT攻击的模型，并将其应用于不同监控环境下的探测。</li>
<li>results: 本文在三个广泛使用的数据集上进行评估，结果显示MAGIC可以在所有测试场景中获得出色的探测结果，并与现有的APT探测方法相比，具有很大的性能优势。<details>
<summary>Abstract</summary>
Advance Persistent Threats (APTs), adopted by most delicate attackers, are becoming increasing common and pose great threat to various enterprises and institutions. Data provenance analysis on provenance graphs has emerged as a common approach in APT detection. However, previous works have exhibited several shortcomings: (1) requiring attack-containing data and a priori knowledge of APTs, (2) failing in extracting the rich contextual information buried within provenance graphs and (3) becoming impracticable due to their prohibitive computation overhead and memory consumption.   In this paper, we introduce MAGIC, a novel and flexible self-supervised APT detection approach capable of performing multi-granularity detection under different level of supervision. MAGIC leverages masked graph representation learning to model benign system entities and behaviors, performing efficient deep feature extraction and structure abstraction on provenance graphs. By ferreting out anomalous system behaviors via outlier detection methods, MAGIC is able to perform both system entity level and batched log level APT detection. MAGIC is specially designed to handle concept drift with a model adaption mechanism and successfully applies to universal conditions and detection scenarios. We evaluate MAGIC on three widely-used datasets, including both real-world and simulated attacks. Evaluation results indicate that MAGIC achieves promising detection results in all scenarios and shows enormous advantage over state-of-the-art APT detection approaches in performance overhead.
</details>
<details>
<summary>摘要</summary>
高级攻击者所采用的持续攻击（APT）正在不断增长，对各种企业和机构构成极大的威胁。数据源推论分析在APT检测中得到了广泛应用。然而，先前的工作具有以下缺陷：（1）需要攻击数据和先验知识，（2）无法提取质量推论图中埋藏的详细信息，（3）因计算负担和内存占用过高而成为不可持续。在这篇论文中，我们介绍MAGIC，一种新的自动化和灵活的APT检测方法。MAGIC可以在不同的级别和水平进行多重粒度检测，并且可以通过匿名系统实体和行为模型来快速抽象出深度特征。通过检测异常系统行为，MAGIC可以同时进行系统实体层和批处理日志层APT检测。MAGIC特别地采用了掩码图表学习来模型无辜系统实体和行为，并通过异常检测方法来检测异常系统行为。MAGIC可以适应概念漂移，并在通用条件和检测场景下显示出优异表现。我们对三个广泛使用的数据集进行了评估，包括真实攻击和模拟攻击。评估结果表明，MAGIC在所有场景中具有扎实的检测效果，与当前APT检测方法相比，具有巨大的性能优势。
</details></li>
</ul>
<hr>
<h2 id="VFLAIR-A-Research-Library-and-Benchmark-for-Vertical-Federated-Learning"><a href="#VFLAIR-A-Research-Library-and-Benchmark-for-Vertical-Federated-Learning" class="headerlink" title="VFLAIR: A Research Library and Benchmark for Vertical Federated Learning"></a>VFLAIR: A Research Library and Benchmark for Vertical Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09827">http://arxiv.org/abs/2310.09827</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/flair-thu/vflair">https://github.com/flair-thu/vflair</a></li>
<li>paper_authors: Tianyuan Zou, Zixuan Gu, Yu He, Hideaki Takahashi, Yang Liu, Guangnan Ye, Ya-Qin Zhang</li>
<li>for: 这篇论文旨在探讨Vertival Federated Learning（VFL）的应用和研究前景，以及如何防止不同类型的数据推理和后门攻击。</li>
<li>methods: 本文使用了多种模型、数据集和协议，并提供了标准化的评估模块以评估攻击和防御策略的性能。</li>
<li>results: 本文对11种攻击和8种防御策略进行了实验性评估，并从不同的通信和模型分割设置中绘制了具体的发现和建议，以帮助实际应用中的VFL部署场景选择防御策略。<details>
<summary>Abstract</summary>
Vertical Federated Learning (VFL) has emerged as a collaborative training paradigm that allows participants with different features of the same group of users to accomplish cooperative training without exposing their raw data or model parameters. VFL has gained significant attention for its research potential and real-world applications in recent years, but still faces substantial challenges, such as in defending various kinds of data inference and backdoor attacks. Moreover, most of existing VFL projects are industry-facing and not easily used for keeping track of the current research progress. To address this need, we present an extensible and lightweight VFL framework VFLAIR (available at https://github.com/FLAIR-THU/VFLAIR), which supports VFL training with a variety of models, datasets and protocols, along with standardized modules for comprehensive evaluations of attacks and defense strategies. We also benchmark 11 attacks and 8 defenses performance under different communication and model partition settings and draw concrete insights and recommendations on the choice of defense strategies for different practical VFL deployment scenario.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Communication-Compression-for-Byzantine-Robust-Learning-New-Efficient-Algorithms-and-Improved-Rates"><a href="#Communication-Compression-for-Byzantine-Robust-Learning-New-Efficient-Algorithms-and-Improved-Rates" class="headerlink" title="Communication Compression for Byzantine Robust Learning: New Efficient Algorithms and Improved Rates"></a>Communication Compression for Byzantine Robust Learning: New Efficient Algorithms and Improved Rates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09804">http://arxiv.org/abs/2310.09804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmad Rammal, Kaja Gruntkowska, Nikita Fedin, Eduard Gorbunov, Peter Richtárik</li>
<li>for: 本研究探讨了对 collaborative&#x2F;federated learning 中的审计缓存问题进行 Byzantine 鲁棒性的算法设计，以及通信压缩的搅动。</li>
<li>methods: 本文提出了两种新的 Byzantine 鲁棒方法：Byz-DASHA-PAGE 和 Byz-EF21，其中 Byz-DASHA-PAGE 在非 convex 和 Polyak-Lojasiewicz 平坦函数中具有更高的收敛速率、更小的邻居大小，并能承受更多的 Byzantine 工作者。Byz-EF21 方法则是首个具有通信压缩和错误反馈的 Byzantine 鲁棒方法，其bidirectional compression version 是 Byz-EF21-BC。</li>
<li>results: 作者在数学实验中测试了提议的方法，并证明了它们在非 convex 和 Polyak-Lojasiewicz 平坦函数中具有更高的收敛速率和更好的承受性。<details>
<summary>Abstract</summary>
Byzantine robustness is an essential feature of algorithms for certain distributed optimization problems, typically encountered in collaborative/federated learning. These problems are usually huge-scale, implying that communication compression is also imperative for their resolution. These factors have spurred recent algorithmic and theoretical developments in the literature of Byzantine-robust learning with compression. In this paper, we contribute to this research area in two main directions. First, we propose a new Byzantine-robust method with compression -- Byz-DASHA-PAGE -- and prove that the new method has better convergence rate (for non-convex and Polyak-Lojasiewicz smooth optimization problems), smaller neighborhood size in the heterogeneous case, and tolerates more Byzantine workers under over-parametrization than the previous method with SOTA theoretical convergence guarantees (Byz-VR-MARINA). Secondly, we develop the first Byzantine-robust method with communication compression and error feedback -- Byz-EF21 -- along with its bidirectional compression version -- Byz-EF21-BC -- and derive the convergence rates for these methods for non-convex and Polyak-Lojasiewicz smooth case. We test the proposed methods and illustrate our theoretical findings in the numerical experiments.
</details>
<details>
<summary>摘要</summary>
布日兹罗布特性是分布式优化问题中的重要特性，通常在协同学习/联邦学习中出现。这些问题通常很大规模，因此通信压缩也是必要的。这些因素在文献中促进了最近的算法和理论发展，包括Byzantine-robust学习与压缩。在这篇论文中，我们对这个研究领域进行了两个主要贡献。首先，我们提出了一种新的Byzantine-robust方法——Byz-DASHA-PAGE，并证明其在非对称和Polyak-Lojasiewicz细致优化问题中的更好的收敛率，小于前一个方法（Byz-VR-MARINA）的最佳理论收敛保证。其次，我们开发了第一种Byzantine-robust方法与通信压缩，并对其在非对称和Polyak-Lojasiewicz细致优化问题中的收敛率进行了分析。我们还开发了一种双向压缩版本——Byz-EF21-BC，并对其进行了数学分析。我们的实验证明了我们的理论发现。
</details></li>
</ul>
<hr>
<h2 id="FLrce-Efficient-Federated-Learning-with-Relationship-based-Client-Selection-and-Early-Stopping-Strategy"><a href="#FLrce-Efficient-Federated-Learning-with-Relationship-based-Client-Selection-and-Early-Stopping-Strategy" class="headerlink" title="FLrce: Efficient Federated Learning with Relationship-based Client Selection and Early-Stopping Strategy"></a>FLrce: Efficient Federated Learning with Relationship-based Client Selection and Early-Stopping Strategy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09789">http://arxiv.org/abs/2310.09789</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziru Niu, Hai Dong, A. Kai Qin, Tao Gu</li>
<li>for: 提高 Federated Learning（FL）的通信和计算效率，以提供智能服务保持数据隐私。</li>
<li>methods: 引入Dropout技术，让限制资源的边缘设备共同训练全球模型参数的一部分。</li>
<li>results: FLrce提高了通信和计算效率，在更少的轮数下达到了相同的准确率，并且可以在提前终止FL来降低通信和计算资源的消耗。<details>
<summary>Abstract</summary>
Federated learning (FL) achieves great popularity in broad areas as a powerful interface to offer intelligent services to customers while maintaining data privacy. Nevertheless, FL faces communication and computation bottlenecks due to limited bandwidth and resource constraints of edge devices. To comprehensively address the bottlenecks, the technique of dropout is introduced, where resource-constrained edge devices are allowed to collaboratively train a subset of the global model parameters. However, dropout impedes the learning efficiency of FL under unbalanced local data distributions. As a result, FL requires more rounds to achieve appropriate accuracy, consuming more communication and computation resources. In this paper, we present FLrce, an efficient FL framework with a relationship-based client selection and early-stopping strategy. FLrce accelerates the FL process by selecting clients with more significant effects, enabling the global model to converge to a high accuracy in fewer rounds. FLrce also leverages an early stopping mechanism to terminate FL in advance to save communication and computation resources. Experiment results show that FLrce increases the communication and computation efficiency by 6% to 73.9% and 20% to 79.5%, respectively, while maintaining competitive accuracy.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 在各种领域得到了广泛的推广，作为一种保持数据隐私的强大接口，提供智能服务给客户。然而，FL 面临有限带宽和资源限制的边缘设备的通信和计算瓶颈。为了全面解决这些瓶颈，dropout技术被引入，允许有限资源的边缘设备共同训练全球模型参数的一部分。然而，dropout会降低FL在不均匀本地数据分布时的学习效率。因此，FL需要更多的轮次来达到适当的准确率，消耗更多的通信和计算资源。在这篇论文中，我们提出了FLrce，一个高效的FL框架，具有关系基于的客户选择和早期终止策略。FLrce 加速了FL 过程，选择有更大影响的客户，使全球模型更快地 converges 到高准确率。FLrce 还利用了早期终止机制，提前终止 FL，以保存通信和计算资源。实验结果表明，FLrce 可以提高通信和计算效率，在 6% 到 73.9% 和 20% 到 79.5% 之间，而且保持竞争性的准确率。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Link-Prediction-for-New-Nodes-in-Temporal-Graph-Networks"><a href="#Dynamic-Link-Prediction-for-New-Nodes-in-Temporal-Graph-Networks" class="headerlink" title="Dynamic Link Prediction for New Nodes in Temporal Graph Networks"></a>Dynamic Link Prediction for New Nodes in Temporal Graph Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09787">http://arxiv.org/abs/2310.09787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaobo Zhu, Yan Wu, Qinhu Zhang, Zhanheng Chen, Ying He</li>
<li>for: 这篇论文的目的是提出一个基于 meta-learning 原则的模型，用于预测新的节点之间的连接。这种预测问题在实际应用中非常重要，例如在推荐系统中给予新的用户有关的项目推荐，以及在社交平台上给予新用户有关的内容推荐。</li>
<li>methods: 这篇论文使用了一个称为 temporal encoder 的专门模型，并且使用了一个称为 predictor 的模型来预测新节点是否会产生连接。这两个模型在 meta-learning 原则下进行学习，以便在预测新节点之间的连接时能够更好地适应。</li>
<li>results: 在三个公开的数据集上进行了实验，结果显示了这个模型的表现比以前的方法更好。具体来说，这个模型可以更好地预测新节点之间的连接，并且可以在几乎无预警情况下进行预测。<details>
<summary>Abstract</summary>
Modelling temporal networks for dynamic link prediction of new nodes has many real-world applications, such as providing relevant item recommendations to new customers in recommender systems and suggesting appropriate posts to new users on social platforms. Unlike old nodes, new nodes have few historical links, which poses a challenge for the dynamic link prediction task. Most existing dynamic models treat all nodes equally and are not specialized for new nodes, resulting in suboptimal performances. In this paper, we consider dynamic link prediction of new nodes as a few-shot problem and propose a novel model based on the meta-learning principle to effectively mitigate this problem. Specifically, we develop a temporal encoder with a node-level span memory to obtain a new node embedding, and then we use a predictor to determine whether the new node generates a link. To overcome the few-shot challenge, we incorporate the encoder-predictor into the meta-learning paradigm, which can learn two types of implicit information during the formation of the temporal network through span adaptation and node adaptation. The acquired implicit information can serve as model initialisation and facilitate rapid adaptation to new nodes through a fine-tuning process on just a few links. Experiments on three publicly available datasets demonstrate the superior performance of our model compared to existing state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
模拟 temporal networks 为新节点动态链接预测有多个实际应用，如为新用户提供相关的ITE推荐和社交平台上新用户的适当帖子推荐。与老节点不同，新节点具有少量历史链接，这对动态链接预测任务带来挑战。大多数现有的动态模型往往对所有节点进行等效处理，从而导致下OPTIMAL表现。在本文中，我们将动态链接预测新节点视为几枚shot问题，并提出一种基于元学习原则的新模型。具体来说，我们开发了一个包含节点级别span记忆的时间编码器，以获得新节点嵌入，然后使用一个预测器来判断新节点是否生成链接。为了解决几枚shot挑战，我们将编码器-预测器 integrate 到元学习 парадиг中，可以在形成 temporal network 过程中通过 span 适应和节点适应学习两种隐式信息。获得的隐式信息可以作为模型初始化，并且通过一些链接的精度适应来快速适应新节点。在三个公开的数据集上进行了实验，我们的模型比现有状态 искусственный方法表现出色。
</details></li>
</ul>
<hr>
<h2 id="Pseudo-Bayesian-Optimization"><a href="#Pseudo-Bayesian-Optimization" class="headerlink" title="Pseudo-Bayesian Optimization"></a>Pseudo-Bayesian Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09766">http://arxiv.org/abs/2310.09766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoxian Chen, Henry Lam</li>
<li>for: 这paper的目的是提出一种可靠的黑obox函数优化方法，以便在实际应用中实现优化过程中的可控性和稳定性。</li>
<li>methods: 这paper使用了一种叫做“Pseudo-Bayesian Optimization”的方法，它基于一个具有抽象原则的axioma framework，并使用了一种简单的本地回归和随机 prior 的构造来确定优化过程中的uncertainty。</li>
<li>results: 这paper的实验结果表明，使用 Pseudo-Bayesian Optimization 方法可以不仅确保优化过程的 convergence，还可以在高维 synthetic experiment、hyperparameter tuning 和机器人应用中实现更高的性能，并且可以与现有的state-of-the-art benchmarks相比，显示出更好的性能。<details>
<summary>Abstract</summary>
Bayesian Optimization is a popular approach for optimizing expensive black-box functions. Its key idea is to use a surrogate model to approximate the objective and, importantly, quantify the associated uncertainty that allows a sequential search of query points that balance exploitation-exploration. Gaussian process (GP) has been a primary candidate for the surrogate model, thanks to its Bayesian-principled uncertainty quantification power and modeling flexibility. However, its challenges have also spurred an array of alternatives whose convergence properties could be more opaque. Motivated by these, we study in this paper an axiomatic framework that elicits the minimal requirements to guarantee black-box optimization convergence that could apply beyond GP-related methods. Moreover, we leverage the design freedom in our framework, which we call Pseudo-Bayesian Optimization, to construct empirically superior algorithms. In particular, we show how using simple local regression, and a suitable "randomized prior" construction to quantify uncertainty, not only guarantees convergence but also consistently outperforms state-of-the-art benchmarks in examples ranging from high-dimensional synthetic experiments to realistic hyperparameter tuning and robotic applications.
</details>
<details>
<summary>摘要</summary>
bayesian 优化是一种广泛应用的优化方法，用于优化costly黑obox函数。其关键思想是使用一个surrogate模型来近似目标函数，同时能够量化相关的uncertainty，以实现sequential搜索。 Gaussian process（GP）因其 bayesian原理下的uncertainty量化能力和模型灵活性而成为主要候选人。然而，GP也存在一些挑战，这些挑战激发了一系列alternatives的发展。在这篇论文中，我们提出了一个axioms framework，该框架可以保证黑obox优化的收敛性，并且可以在GP相关方法之外应用。此外，我们利用了我们的框架的设计自由度，constructed一种empirically superior的算法。具体来说，我们使用了一个简单的local regression，并使用一种适当的“Randomized Prior”的构建来量化uncertainty。这不仅保证了收敛性，还可以在高维 synthetic experiments中consistently outperformstate-of-the-art benchmarks。
</details></li>
</ul>
<hr>
<h2 id="UniTime-A-Language-Empowered-Unified-Model-for-Cross-Domain-Time-Series-Forecasting"><a href="#UniTime-A-Language-Empowered-Unified-Model-for-Cross-Domain-Time-Series-Forecasting" class="headerlink" title="UniTime: A Language-Empowered Unified Model for Cross-Domain Time Series Forecasting"></a>UniTime: A Language-Empowered Unified Model for Cross-Domain Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09751">http://arxiv.org/abs/2310.09751</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xu Liu, Junfeng Hu, Yuan Li, Shizhe Diao, Yuxuan Liang, Bryan Hooi, Roger Zimmermann</li>
<li>for: 这个研究旨在提出一个横跨多个时间序列应用领域的统一模型架构，以扩大现有模型的应用范围。</li>
<li>methods: 本研究提出了UniTime模型，可以灵活地适应不同数据特性，并使用语言-时间缩排变换器和封页来实现多模式识别和时间序列数据的搭配。</li>
<li>results: 实验结果显示UniTime模型能够提高现有模型的预测性能和零学习转移性能。<details>
<summary>Abstract</summary>
Multivariate time series forecasting plays a pivotal role in contemporary web technologies. In contrast to conventional methods that involve creating dedicated models for specific time series application domains, this research advocates for a unified model paradigm that transcends domain boundaries. However, learning an effective cross-domain model presents the following challenges. First, various domains exhibit disparities in data characteristics, e.g., the number of variables, posing hurdles for existing models that impose inflexible constraints on these factors. Second, the model may encounter difficulties in distinguishing data from various domains, leading to suboptimal performance in our assessments. Third, the diverse convergence rates of time series domains can also result in compromised empirical performance. To address these issues, we propose UniTime for effective cross-domain time series learning. Concretely, UniTime can flexibly adapt to data with varying characteristics. It also uses domain instructions and a Language-TS Transformer to offer identification information and align two modalities. In addition, UniTime employs masking to alleviate domain convergence speed imbalance issues. Our extensive experiments demonstrate the effectiveness of UniTime in advancing state-of-the-art forecasting performance and zero-shot transferability.
</details>
<details>
<summary>摘要</summary>
多变量时间序列预测在当代网络技术中扮演着关键角色。与传统方法不同，这项研究提出了跨领域模型的统一模型架构，跨越领域边界。然而，学习有效的跨领域模型存在以下挑战：首先，不同领域的数据特征存在差异，例如变量数量，这会限制现有模型的灵活性。其次，模型可能很难分辨不同领域的数据，导致预测性能下降。最后，时间序列领域的多样化速度也可能导致实际性能下降。为解决这些问题，我们提出了UniTime，一种可靠地适应数据特征变化的模型。具体来说，UniTime可以适应数据中的变量数量变化，并使用领域指令和语言-TS transformer来提供标识信息和对两种模式进行对应。此外，UniTime还使用屏蔽来缓解领域融合速度不平衡问题。我们的广泛实验表明UniTime可以提高预测性能和零代负荷传递性。
</details></li>
</ul>
<hr>
<h2 id="Private-Synthetic-Data-Meets-Ensemble-Learning"><a href="#Private-Synthetic-Data-Meets-Ensemble-Learning" class="headerlink" title="Private Synthetic Data Meets Ensemble Learning"></a>Private Synthetic Data Meets Ensemble Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09729">http://arxiv.org/abs/2310.09729</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoyuan Sun, Navid Azizan, Akash Srivastava, Hao Wang</li>
<li>for: 这篇论文的目的是提高在真实数据上使用机器学习模型的性能，因为训练在实验数据上的模型在真实数据上可能会出现性能下降的问题。</li>
<li>methods: 这篇论文使用了一种新的ensemble策略，通过将多个实验数据中的模型 ensemble起来，以增强模型在真实数据上的表现。具体来说，他们使用了一些具有数据分布差异的实验数据，并将这些数据 ensemble起来，以增加模型的数据多样性。</li>
<li>results: 根据实验结果，这种ensemble策略可以对于使用GAN-based differential privacy mechanisms（即生成机制）训练的下游模型，提高其在真实数据上的表现，包括精度和模型调整的方面。但是，这种策略不会对于使用margin-based或workload-based differential privacy mechanisms（即统计机制）训练的下游模型提高表现。<details>
<summary>Abstract</summary>
When machine learning models are trained on synthetic data and then deployed on real data, there is often a performance drop due to the distribution shift between synthetic and real data. In this paper, we introduce a new ensemble strategy for training downstream models, with the goal of enhancing their performance when used on real data. We generate multiple synthetic datasets by applying a differential privacy (DP) mechanism several times in parallel and then ensemble the downstream models trained on these datasets. While each synthetic dataset might deviate more from the real data distribution, they collectively increase sample diversity. This may enhance the robustness of downstream models against distribution shifts. Our extensive experiments reveal that while ensembling does not enhance downstream performance (compared with training a single model) for models trained on synthetic data generated by marginal-based or workload-based DP mechanisms, our proposed ensemble strategy does improve the performance for models trained using GAN-based DP mechanisms in terms of both accuracy and calibration of downstream models.
</details>
<details>
<summary>摘要</summary>
当机器学习模型在生成的数据上训练并在实际数据上部署时，通常会出现性能下降，这是因为生成的数据和实际数据的分布shift。在这篇论文中，我们介绍了一种新的集成策略，用于训练下游模型，以提高它们在实际数据上的表现。我们通过应用多次 diferencial privacy（DP）机制来生成多个生成的数据集，然后将这些数据集上训练的下游模型 ensemble。虽然每个生成的数据集可能更 deviation from the real data distribution，但它们的总体样本多样性可以提高下游模型对分布shift的Robustness。我们的广泛实验表明，对于基于marginal-based或workload-based DP机制生成的数据集，集成不会提高下游模型的性能（与单个模型训练相比），但是我们的提议的集成策略对基于GAN-based DP机制生成的数据集进行训练，可以提高模型的准确性和下游模型的Calibration。
</details></li>
</ul>
<hr>
<h2 id="SVM-based-Multiclass-Classifier-for-Gait-phase-Classification-using-Shank-IMU-Sensor"><a href="#SVM-based-Multiclass-Classifier-for-Gait-phase-Classification-using-Shank-IMU-Sensor" class="headerlink" title="SVM based Multiclass Classifier for Gait phase Classification using Shank IMU Sensor"></a>SVM based Multiclass Classifier for Gait phase Classification using Shank IMU Sensor</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09728">http://arxiv.org/abs/2310.09728</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aswadh Khumar G S, Barath Kumar JK</li>
<li>for: 本研究旨在开发一种基于SVM多类分类的步态分类方法，以高精度地标识步态阶段，包括七个子阶段。</li>
<li>methods: 该方法使用个体IMU传感器数据，如膝盖加速度x、y、z和膝盖陀螺x，作为特征进行分类。</li>
<li>results: 该方法可以高度准确地分类不同的步态阶段，准确率约为90.3%。Here’s a breakdown of each point:</li>
<li>for: 本研究旨在开发一种基于SVM多类分类的步态分类方法，以高精度地标识步态阶段，包括七个子阶段。 (The study aims to develop a gait phase classification method based on SVM multi-class classification, to accurately identify the gait phases, including seven sub-phases.)</li>
<li>methods: 该方法使用个体IMU传感器数据，如膝盖加速度x、y、z和膝盖陀螺x，作为特征进行分类。 (The method uses individual IMU sensor data, such as shank acceleration x, y, and z, and knee angles, as features for classification.)</li>
<li>results: 该方法可以高度准确地分类不同的步态阶段，准确率约为90.3%。 (The method can accurately classify different gait phases with an accuracy of approximately 90.3%.)<details>
<summary>Abstract</summary>
In this study, a gait phase classification method based on SVM multiclass classification is introduced, with a focus on the precise identification of the stance and swing phases, which are further subdivided into seven phases. Data from individual IMU sensors, such as Shank Acceleration X, Y, Z, Shank Gyro X, and Knee Angles, are used as features in this classification model. The suggested technique successfully classifies the various gait phases with a significant accuracy of about 90.3%. Gait phase classification is crucial, especially in the domains of exoskeletons and prosthetics, where accurate identification of gait phases enables seamless integration with assistive equipment, improving mobility, stability, and energy economy. This study extends the study of gait and offers an effective method for correctly identifying gait phases from Shank IMU sensor data, with potential applications in biomechanical research, exoskeletons, rehabilitation, and prosthetics.
</details>
<details>
<summary>摘要</summary>
在本研究中，基于SVM多类分类的步态分类方法被引入，强调精准地识别步态的不同阶段，这些阶段进一步细分为七个阶段。研究使用个体IMU传感器数据，如膝盖加速度x、y、z、膝盖陀螺x等，作为分类模型的特征。提议的技术成功地分类不同的步态阶段，准确率达到了约90.3%。步态分类在许多领域都非常重要，如外围机械助手和假肢，准确识别步态阶段可以帮助融合助手设备，提高 mobilidad、稳定性和能源经济。本研究对步态的研究进一步推广，并提供了基于膝盖IMU传感器数据的有效的步态分类方法，可能在生物机械研究、外围机械助手、rehabilitation和假肢领域得到应用。
</details></li>
</ul>
<hr>
<h2 id="Provably-Fast-Convergence-of-Independent-Natural-Policy-Gradient-for-Markov-Potential-Games"><a href="#Provably-Fast-Convergence-of-Independent-Natural-Policy-Gradient-for-Markov-Potential-Games" class="headerlink" title="Provably Fast Convergence of Independent Natural Policy Gradient for Markov Potential Games"></a>Provably Fast Convergence of Independent Natural Policy Gradient for Markov Potential Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09727">http://arxiv.org/abs/2310.09727</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sundave1998/independent-npg-mpg">https://github.com/sundave1998/independent-npg-mpg</a></li>
<li>paper_authors: Youbang Sun, Tao Liu, Ruida Zhou, P. R. Kumar, Shahin Shahrampour</li>
<li>for: 这个研究探讨了一种独立自然策略加速算法（NPG），用于多代理游戏学习问题中的Markov潜在游戏。</li>
<li>methods: 这种独立NPG方法使用了一个假 oracle，以获得精确的策略评估，从而在技术假设和潜在差额的假设下，在 $\mathcal{O}(1&#x2F;\epsilon)$ 迭代中达到 $\epsilon$-纳什平衡（NE）。</li>
<li>results: 这个研究证明了，在 synthetic potential game 和 congestion game 中，独立NPG方法可以在 $\mathcal{O}(1&#x2F;\epsilon)$ 迭代中达到 $\epsilon$-纳什平衡，超过之前的最佳结果 $\mathcal{O}(1&#x2F;\epsilon^2)$ 迭代。<details>
<summary>Abstract</summary>
This work studies an independent natural policy gradient (NPG) algorithm for the multi-agent reinforcement learning problem in Markov potential games. It is shown that, under mild technical assumptions and the introduction of the \textit{suboptimality gap}, the independent NPG method with an oracle providing exact policy evaluation asymptotically reaches an $\epsilon$-Nash Equilibrium (NE) within $\mathcal{O}(1/\epsilon)$ iterations. This improves upon the previous best result of $\mathcal{O}(1/\epsilon^2)$ iterations and is of the same order, $\mathcal{O}(1/\epsilon)$, that is achievable for the single-agent case. Empirical results for a synthetic potential game and a congestion game are presented to verify the theoretical bounds.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SGA-A-Graph-Augmentation-Method-for-Signed-Graph-Neural-Networks"><a href="#SGA-A-Graph-Augmentation-Method-for-Signed-Graph-Neural-Networks" class="headerlink" title="SGA: A Graph Augmentation Method for Signed Graph Neural Networks"></a>SGA: A Graph Augmentation Method for Signed Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09705">http://arxiv.org/abs/2310.09705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyu Zhang, Shuyan Wan, Sijie Wang, Xianda Zheng, Xinrui Zhang, Kaiqi Zhao, Jiamou Liu, Dong Hao</li>
<li>for: This paper is written for analyzing complex patterns in real-world signed graphs, and addressing three key challenges in SGNN-based signed graph representation learning: sparsity, unbalanced triangles, and lack of supplementary information.</li>
<li>methods: The paper proposes a novel Signed Graph Augmentation framework (SGA) that includes three main components: (1) using an SGNN model to encode the signed graph and extract latent structural information for candidate augmentation structures, (2) evaluating and selecting the most beneficial candidate samples for modifying the original training set, and (3) a novel augmentation perspective that assigns varying training difficulty to training samples.</li>
<li>results: The paper demonstrates significant improvements in performance across multiple benchmarks using the proposed SGA method, outperforming baselines by up to 22.2% in AUC, 33.3% in F1-binary, 48.8% in F1-micro, and 36.3% in F1-macro on six real-world datasets.<details>
<summary>Abstract</summary>
Signed Graph Neural Networks (SGNNs) are vital for analyzing complex patterns in real-world signed graphs containing positive and negative links. However, three key challenges hinder current SGNN-based signed graph representation learning: sparsity in signed graphs leaves latent structures undiscovered, unbalanced triangles pose representation difficulties for SGNN models, and real-world signed graph datasets often lack supplementary information like node labels and features. These constraints limit the potential of SGNN-based representation learning. We address these issues with data augmentation techniques. Despite many graph data augmentation methods existing for unsigned graphs, none are tailored for signed graphs. Our paper introduces the novel Signed Graph Augmentation framework (SGA), comprising three main components. First, we employ the SGNN model to encode the signed graph, extracting latent structural information for candidate augmentation structures. Second, we evaluate these candidate samples (edges) and select the most beneficial ones for modifying the original training set. Third, we propose a novel augmentation perspective that assigns varying training difficulty to training samples, enabling the design of a new training strategy. Extensive experiments on six real-world datasets (Bitcoin-alpha, Bitcoin-otc, Epinions, Slashdot, Wiki-elec, and Wiki-RfA) demonstrate that SGA significantly improves performance across multiple benchmarks. Our method outperforms baselines by up to 22.2% in AUC for SGCN on Wiki-RfA, 33.3% in F1-binary, 48.8% in F1-micro, and 36.3% in F1-macro for GAT on Bitcoin-alpha in link sign prediction.
</details>
<details>
<summary>摘要</summary>
Signed Graph Neural Networks (SGNNs) 是对实际中带有正负链接的签名图进行分析复杂模式的关键工具。然而，现有的SGNN模型在签名图表示学习中存在三大挑战：签名图中的稀疏性使得潜在结构未被发现，不均衡的triangle对SGNN模型进行表示带来挑战，而且现实中的签名图数据往往缺乏节点标签和特征信息。这些限制使SGNN-基于表示学习的潜力受限。我们通过数据扩充技术来解决这些问题。虽然现有许多对 unsigned 图进行数据扩充的方法，但是这些方法并没有适应签名图。我们的论文提出了一种新的签名图扩充框架（SGA），包括以下三个主要组成部分：1. 我们使用 SGNN 模型来编码签名图，提取签名图中的潜在结构信息作为候选扩充结构。2. 我们评估这些候选样本（边），并选择对原始训练集进行最有利的修改。3. 我们提出了一种新的增强训练方法，对各种训练样本分配不同的训练难度，以便设计更好的训练策略。我们在六个真实世界数据集（Bitcoin-alpha、Bitcoin-otc、Epinions、Slashdot、Wiki-elec和Wiki-RfA）进行了广泛的实验，结果表明，SGA 可以在多个 bench 上显著提高性能。我们的方法在 Wiki-RfA 上的 AUC 上比基eline 提高了22.2%，在 Bitcoin-alpha 上的 F1-binary、F1-micro 和 F1-macro 上提高了33.3%、48.8% 和 36.3%。
</details></li>
</ul>
<hr>
<h2 id="When-Collaborative-Filtering-is-not-Collaborative-Unfairness-of-PCA-for-Recommendations"><a href="#When-Collaborative-Filtering-is-not-Collaborative-Unfairness-of-PCA-for-Recommendations" class="headerlink" title="When Collaborative Filtering is not Collaborative: Unfairness of PCA for Recommendations"></a>When Collaborative Filtering is not Collaborative: Unfairness of PCA for Recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09687">http://arxiv.org/abs/2310.09687</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Liu, Jackie Baek, Tina Eliassi-Rad</li>
<li>for: 这paper主要研究了推荐系统中的维度减少方法的公平性。</li>
<li>methods: 这paper使用了主要方法为原理 Component Analysis (PCA)，它可以从高维数据中提取特征组件，并生成一个低维表示。</li>
<li>results: 这paper发现了PCA的两个下面机制，它们会导致推荐系统中的不公平性。此外，paper还提出了一种改进PCA的算法，即Item-Weighted PCA，可以更好地处理不同类型的项目。在一些假设的矩阵上，paper证明了Item-Weighted PCA使用特定的质量可以最小化一个媒体化错误度量。在实际数据上，这paper发现Item-Weighted PCA不仅可以提高总体推荐质量，还可以提高流行和不流行的项目。<details>
<summary>Abstract</summary>
We study the fairness of dimensionality reduction methods for recommendations. We focus on the established method of principal component analysis (PCA), which identifies latent components and produces a low-rank approximation via the leading components while discarding the trailing components. Prior works have defined notions of "fair PCA"; however, these definitions do not answer the following question: what makes PCA unfair? We identify two underlying mechanisms of PCA that induce unfairness at the item level. The first negatively impacts less popular items, due to the fact that less popular items rely on trailing latent components to recover their values. The second negatively impacts the highly popular items, since the leading PCA components specialize in individual popular items instead of capturing similarities between items. To address these issues, we develop a polynomial-time algorithm, Item-Weighted PCA, a modification of PCA that uses item-specific weights in the objective. On a stylized class of matrices, we prove that Item-Weighted PCA using a specific set of weights minimizes a popularity-normalized error metric. Our evaluations on real-world datasets show that Item-Weighted PCA not only improves overall recommendation quality by up to $0.1$ item-level AUC-ROC but also improves on both popular and less popular items.
</details>
<details>
<summary>摘要</summary>
我们研究了维度减少方法的公平性，特别是已知的主 componenets分析（PCA）方法。PCA方法可以找到缺失的特征并生成一个低级别的approximation，通过主要的特征来抛弃追随的特征。先前的研究已经定义了“公平PCA”的概念，但这些定义并没有回答以下问题：PCA方法是如何不公平的？我们认为PCA方法存在两种下面机制，导致item级别的不公平ness。首先，less popular items会受到负面影响，因为这些items rely on追随的特征来恢复其价值。其次，highly popular items会受到负面影响，因为leading PCA components会专注于个体受欢迎的items而不是捕捉items之间的相似性。为了解决这些问题，我们开发了一个幂时间算法，Item-Weighted PCA，这是PCA方法的修改。在一个简化的矩阵类型上，我们证明了Item-Weighted PCA使用的项目特定的权重在目标函数中具有最小化一个受欢迎度normalized error metric的性能。我们对实际数据进行评估，显示Item-Weighted PCA不仅提高了总的推荐质量，最高达0.1个item-level AUC-ROC，同时也提高了受欢迎和less popular items的质量。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Column-Generation-by-Reinforcement-Learning-Based-Hyper-Heuristic-for-Vehicle-Routing-and-Scheduling-Problems"><a href="#Enhancing-Column-Generation-by-Reinforcement-Learning-Based-Hyper-Heuristic-for-Vehicle-Routing-and-Scheduling-Problems" class="headerlink" title="Enhancing Column Generation by Reinforcement Learning-Based Hyper-Heuristic for Vehicle Routing and Scheduling Problems"></a>Enhancing Column Generation by Reinforcement Learning-Based Hyper-Heuristic for Vehicle Routing and Scheduling Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09686">http://arxiv.org/abs/2310.09686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kuan Xu, Li Shen, Lindong Liu</li>
<li>for: 提高大规模问题的解决效率和解得质量</li>
<li>methods: 利用强化学习法 Hyper-heuristic 框架 RLHH 加速Column Generation 方法，并在每个 CG 迭代中选择最佳低级别规划算法</li>
<li>results: 在 Vehicle Routing Problem with Time Windows 和 Bus Driver Scheduling Problem 两个典型的 combinatorial optimization 问题中，可以提高解得质量，最高减少总成本达 27.9% 和 15.4%，在相同或更少的计算时间内减少计算时间。<details>
<summary>Abstract</summary>
Column generation (CG) is a vital method to solve large-scale problems by dynamically generating variables. It has extensive applications in common combinatorial optimization, such as vehicle routing and scheduling problems, where each iteration step requires solving an NP-hard constrained shortest path problem. Although some heuristic methods for acceleration already exist, they are not versatile enough to solve different problems. In this work, we propose a reinforcement learning-based hyper-heuristic framework, dubbed RLHH, to enhance the performance of CG. RLHH is a selection module embedded in CG to accelerate convergence and get better integer solutions. In each CG iteration, the RL agent selects a low-level heuristic to construct a reduced network only containing the edges with a greater chance of being part of the optimal solution. In addition, we specify RLHH to solve two typical combinatorial optimization problems: Vehicle Routing Problem with Time Windows (VRPTW) and Bus Driver Scheduling Problem (BDSP). The total cost can be reduced by up to 27.9\% in VRPTW and 15.4\% in BDSP compared to the best lower-level heuristic in our tested scenarios, within equivalent or even less computational time. The proposed RLHH is the first RL-based CG method that outperforms traditional approaches in terms of solution quality, which can promote the application of CG in combinatorial optimization.
</details>
<details>
<summary>摘要</summary>
column generation (CG) 是一种重要的方法，用于解决大规模问题，通过动态生成变量。它在各种常见的 combinatorial optimization 中有广泛的应用，如车辆 Routing 和调度问题，每个迭代步骤都需要解决一个 NP-hard 约束短路问题。虽然一些启发法已经存在，但它们并不够 versatile  enough 解决不同的问题。在这项工作中，我们提出了一种基于强化学习的 hyper-heuristic 框架，称为 RLHH，以提高 CG 的性能。RLHH 是 CG 中的一个选择模块，用于加速迭代和获得更好的整数解。在每个 CG 迭代中，RL  Agent 将选择一个低级别启发，用于构建一个只包含有更高可能性成为优解的最佳解的减少网络。此外，我们将 RLHH 应用于两种典型的 combinatorial optimization 问题：车辆 Routing 问题 with Time Windows (VRPTW) 和 Bus Driver Scheduling 问题 (BDSP)。我们在测试场景中发现，RLHH 可以将总成本降低到 27.9% 以下，相对于最佳下级别启发，并且在相同或更少的计算时间内达成。我们的提案的 RLHH 是首个通过强化学习来超越传统方法的 CG 方法，可以提高 CG 在 combinatorial optimization 中的应用。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/15/cs.LG_2023_10_15/" data-id="cloh7tqk600q67b88fyy50fl1" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_10_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/15/eess.IV_2023_10_15/" class="article-date">
  <time datetime="2023-10-15T09:00:00.000Z" itemprop="datePublished">2023-10-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/15/eess.IV_2023_10_15/">eess.IV - 2023-10-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Joint-Sparse-Representations-and-Coupled-Dictionary-Learning-in-Multi-Source-Heterogeneous-Image-Pseudo-color-Fusion"><a href="#Joint-Sparse-Representations-and-Coupled-Dictionary-Learning-in-Multi-Source-Heterogeneous-Image-Pseudo-color-Fusion" class="headerlink" title="Joint Sparse Representations and Coupled Dictionary Learning in Multi-Source Heterogeneous Image Pseudo-color Fusion"></a>Joint Sparse Representations and Coupled Dictionary Learning in Multi-Source Heterogeneous Image Pseudo-color Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09937">http://arxiv.org/abs/2310.09937</a></li>
<li>repo_url: None</li>
<li>paper_authors: Long Bai, Shilong Yao, Kun Gao, Yanjun Huang, Ruijie Tang, Hong Yan, Max Q. -H. Meng, Hongliang Ren</li>
<li>for: 提出一种基于 Coupled Dictionary Learning (CDL) 方法的 Synthetic Aperture Radar (SAR) 和多spectral pseudo-color合并方法，以实现高质量的合并图像。</li>
<li>methods: 使用传统的 Brovey 变换进行预处理，然后使用 CDL 捕捉对照图像对的相关性，通过强制联合稀热编码生成字典。最后，利用对字典中的联合稀热表示来构建图像掩蔽mask，并生成最终的合并图像。</li>
<li>results: 通过使用 Sentinel-1 卫星的 SAR 图像和 Landsat-8 卫星的多spectral图像进行实验验证，提出的方法可以实现优秀的视觉效果和数值性能，包括spectral distortion、相关系数、MSE、NIQE、BRISQUE 和 PIQE 等指标。<details>
<summary>Abstract</summary>
Considering that Coupled Dictionary Learning (CDL) method can obtain a reasonable linear mathematical relationship between resource images, we propose a novel CDL-based Synthetic Aperture Radar (SAR) and multispectral pseudo-color fusion method. Firstly, the traditional Brovey transform is employed as a pre-processing method on the paired SAR and multispectral images. Then, CDL is used to capture the correlation between the pre-processed image pairs based on the dictionaries generated from the source images via enforced joint sparse coding. Afterward, the joint sparse representation in the pair of dictionaries is utilized to construct an image mask via calculating the reconstruction errors, and therefore generate the final fusion image. The experimental verification results of the SAR images from the Sentinel-1 satellite and the multispectral images from the Landsat-8 satellite show that the proposed method can achieve superior visual effects, and excellent quantitative performance in terms of spectral distortion, correlation coefficient, MSE, NIQE, BRISQUE, and PIQE.
</details>
<details>
<summary>摘要</summary>
基于coupled dictionary learning（CDL）方法，我们提出了一种新的Synthetic Aperture Radar（SAR）和多spectral pseudo-color融合方法。首先，我们使用传统的Brovey变换作为预处理方法，对paired SAR和多spectral图像进行预处理。然后，我们使用CDL来捕捉paired图像对的相关性，基于源图像生成的字典via强制联合稀热编码。接着，我们利用对的字典中的联合稀热表示来构建图像掩码，通过计算重建错误来生成最终融合图像。实验Result of SAR图像来自Sentinel-1卫星和多spectral图像来自Landsat-8卫星表明，提出的方法可以实现优秀的视觉效果，且在 spectral distortion、相关系数、MSE、NIQE、BRISQUE和PIQE等方面具有出色的量化表现。
</details></li>
</ul>
<hr>
<h2 id="Segment-Anything-Model-for-Pedestrian-Infrastructure-Inventory-Assessing-Zero-Shot-Segmentation-on-Multi-Mode-Geospatial-Data"><a href="#Segment-Anything-Model-for-Pedestrian-Infrastructure-Inventory-Assessing-Zero-Shot-Segmentation-on-Multi-Mode-Geospatial-Data" class="headerlink" title="Segment Anything Model for Pedestrian Infrastructure Inventory: Assessing Zero-Shot Segmentation on Multi-Mode Geospatial Data"></a>Segment Anything Model for Pedestrian Infrastructure Inventory: Assessing Zero-Shot Segmentation on Multi-Mode Geospatial Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09918">http://arxiv.org/abs/2310.09918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiahao Xia, Gavin Gong, Jiawei Liu, Zhigang Zhu, Hao Tang</li>
<li>for: 这个论文旨在设计和优化基于Segment Anything Model（SAM）的人行道基础设施分割工作流程，能够有效处理多源地ospatial数据，包括LiDAR数据和卫星图像数据。</li>
<li>methods: 本论文使用扩展的人行道基础设施清单，包括通常被传统定义中排除的街区用品对象。我们的贡献在于生成必要的知识，回答以下两个问题：首先，哪种数据表示可以使SAM实现零批处理基础设施对象？其次，SAM如何在分割人行道基础设施对象方面表现？</li>
<li>results: 我们的发现表明，将来自移动LiDAR点云数据生成的街景图像与卫星图像数据结合使用，可以与SAM高效地创建可扩展的人行道基础设施清单，具有立即的利用价值，对于GIS专业人员、城市管理者、交通所有者和残疾人旅行者都具有重要意义。<details>
<summary>Abstract</summary>
In this paper, a Segment Anything Model (SAM)-based pedestrian infrastructure segmentation workflow is designed and optimized, which is capable of efficiently processing multi-sourced geospatial data including LiDAR data and satellite imagery data. We used an expanded definition of pedestrian infrastructure inventory which goes beyond the traditional transportation elements to include street furniture objects often omitted from the traditional definition. Our contributions lie in producing the necessary knowledge to answer the following two questions. First, which data representation can facilitate zero-shot segmentation of infrastructure objects with SAM? Second, how well does the SAM-based method perform on segmenting pedestrian infrastructure objects? Our findings indicate that street view images generated from mobile LiDAR point cloud data, when paired along with satellite imagery data, can work efficiently with SAM to create a scalable pedestrian infrastructure inventory approach with immediate benefits to GIS professionals, city managers, transportation owners, and walkers, especially those with travel-limiting disabilities.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们设计了基于Segment Anything Model（SAM）的人行道基础设施分割工作流程，可以高效处理多源地ospatial数据，包括LiDAR数据和卫星图像数据。我们使用扩展的人行道基础设施清单定义，超出传统交通元素，包括通常被忽略的街 furniture对象。我们的贡献在于生成必要的知识，回答以下两个问题：首先，哪种数据表示可以通过SAM实现零批处理基础设施对象？第二，SAM基于方法如何在基础设施对象上进行分割？我们的发现表明，从移动LiDAR点云数据生成的街景图像，当与卫星图像数据结合使用时，可以高效地与SAM合作创建可扩展的人行道基础设施清单方法，具有立即的利益 дляGIS专业人员、城市管理者、交通所有人和残疾人，特别是那些受限的旅行者。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/15/eess.IV_2023_10_15/" data-id="cloh7tqq3016c7b88b6trd85n" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_10_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/15/eess.SP_2023_10_15/" class="article-date">
  <time datetime="2023-10-15T08:00:00.000Z" itemprop="datePublished">2023-10-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/15/eess.SP_2023_10_15/">eess.SP - 2023-10-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Distributed-Estimation-with-Partially-Accessible-Information-An-IMAT-Approach-to-LMS-Diffusion"><a href="#Distributed-Estimation-with-Partially-Accessible-Information-An-IMAT-Approach-to-LMS-Diffusion" class="headerlink" title="Distributed Estimation with Partially Accessible Information: An IMAT Approach to LMS Diffusion"></a>Distributed Estimation with Partially Accessible Information: An IMAT Approach to LMS Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09970">http://arxiv.org/abs/2310.09970</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahdi Shamsi, Farokh Marvasti</li>
<li>for: 提高分布式算法的可见性和稳定性</li>
<li>methods: 基于信号流分析的组合策略分析框架和阈值算法</li>
<li>results: 在时域和变换域中存在缺失信息的情况下，提出了一种基于阈值算法的支持向量识别和利用策略，并在两种组合enario中进行了示范<details>
<summary>Abstract</summary>
Distributed algorithms, particularly Diffusion Least Mean Square, are widely favored for their reliability, robustness, and fast convergence in various industries. However, limited observability of the target can compromise the integrity of the algorithm. To address this issue, this paper proposes a framework for analyzing combination strategies by drawing inspiration from signal flow analysis. A thresholding-based algorithm is also presented to identify and utilize the support vector in scenarios with missing information about the target vector's support. The proposed approach is demonstrated in two combination scenarios, showcasing the effectiveness of the algorithm in situations characterized by sparse observations in the time and transform domains.
</details>
<details>
<summary>摘要</summary>
diffused least squares 算法在各个领域得到广泛应用，特别是因为它们的可靠性、鲁棒性和快速收敛性。然而，target vector的有限可见性可能会导致算法的完整性受到损害。为解决这个问题，本文提出了一种基于信号流分析的框架，并提出了一种阈值分析法来Identify和利用目标向量的支持向量在有限信息的情况下。该方法在时域和变换域中的缺失观测场景中进行了两种组合场景的示例，展示了该算法在稀疏观测场景中的效果。Note: "Diffusion Least Mean Square" in the original text is translated as "diffused least squares 算法" in Simplified Chinese, as "diffusion" is not a commonly used term in Chinese and "least squares" is more commonly used to refer to this type of algorithm.
</details></li>
</ul>
<hr>
<h2 id="Semi-Supervised-End-to-End-Learning-for-Integrated-Sensing-and-Communications"><a href="#Semi-Supervised-End-to-End-Learning-for-Integrated-Sensing-and-Communications" class="headerlink" title="Semi-Supervised End-to-End Learning for Integrated Sensing and Communications"></a>Semi-Supervised End-to-End Learning for Integrated Sensing and Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09940">http://arxiv.org/abs/2310.09940</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/josemateosramos/sslisac">https://github.com/josemateosramos/sslisac</a></li>
<li>paper_authors: José Miguel Mateos-Ramos, Baptiste Chatelier, Christian Häger, Musa Furkan Keskin, Luc Le Magoarou, Henk Wymeersch</li>
<li>for: 本文针对 ISAC 混合感知通信系统的问题进行研究，旨在提高硬件、频率和能源效率。</li>
<li>methods: 本文使用 differentiable model-based 学习方法，实现了单目标检测和定位估计，以及多input single-output 通信。</li>
<li>results: 我们的结果显示，使用半指导学习策略可以实现相似的性能，仅需使用 98.8%  fewer labeled data。<details>
<summary>Abstract</summary>
Integrated sensing and communications (ISAC) is envisioned as one of the key enablers of next-generation wireless systems, offering improved hardware, spectral, and energy efficiencies. In this paper, we consider an ISAC transceiver with an impaired uniform linear array that performs single-target detection and position estimation, and multiple-input single-output communications. A differentiable model-based learning approach is considered, which optimizes both the transmitter and the sensing receiver in an end-to-end manner. An unsupervised loss function that enables impairment compensation without the need for labeled data is proposed. Semi-supervised learning strategies are also proposed, which use a combination of small amounts of labeled data and unlabeled data. Our results show that semi-supervised learning can achieve similar performance to supervised learning with 98.8% less required labeled data.
</details>
<details>
<summary>摘要</summary>
“集成感测通信（ISAC）是未来无线系统的关键促进因素，提供了改善硬件、频率和能量效率。本文考虑了一种受损均匀线列天线的ISAC收发器，实现单目标探测和位置估计，以及多输入单出口通信。我们使用可导模型基本学习方法，在终端干扰下优化发射器和探测Receiver。我们提出了无标签数据补偿的不监督学习策略，以及使用小量标签数据和无标签数据的半监督学习策略。我们的结果表明，半监督学习可以与监督学习准确率相似，仅需98.8%的标签数据。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Enhance-Security-of-Time-Modulated-Array-Enabled-Directional-Modulation-by-Introducing-Symbol-Ambiguity"><a href="#Enhance-Security-of-Time-Modulated-Array-Enabled-Directional-Modulation-by-Introducing-Symbol-Ambiguity" class="headerlink" title="Enhance Security of Time-Modulated Array-Enabled Directional Modulation by Introducing Symbol Ambiguity"></a>Enhance Security of Time-Modulated Array-Enabled Directional Modulation by Introducing Symbol Ambiguity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09922">http://arxiv.org/abs/2310.09922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhihao Tao, Zhaoyi Xu, Athina Petropulu</li>
<li>for: 这个论文研究了时间模拟数组（TMA）启用的方向性模拟（DM）通信系统是否可以破解。</li>
<li>methods: 论文首先示出了使用格点搜索可以成功找到TMA生成的唯一和实际混合矩阵。然后，提出了引入符号模糊来防止格点搜索的推论，并设计了两个原则来构建符号模糊：一是缺rank的缺失和非唯一性的ON-OFF切换模式。</li>
<li>results: 论文提出的原则和机制不仅可以在理论上设计更安全的TMA DM系统，还经验 validate 了其效果。<details>
<summary>Abstract</summary>
In this paper, if the time-modulated array (TMA)-enabled directional modulation (DM) communication system can be cracked is investigated and the answer is YES! We first demonstrate that the scrambling data received at the eavesdropper can be defied by using grid search to successfully find the only and actual mixing matrix generated by TMA. Then, we propose introducing symbol ambiguity to TMA to defend the defying of grid search, and design two principles for the TMA mixing matrix, i.e., rank deficiency and non-uniqueness of the ON-OFF switching pattern, that can be used to construct the symbol ambiguity. Also, we present a feasible mechanism to implement these two principles. Our proposed principles and mechanism not only shed light on how to design a more secure TMA DM system theoretically in the future, but also have been validated to be effective by bit error rate measurements.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们调查了使用时间模拟数组（TMA）启用方向性模式（DM）通信系统是否可以被破解。我们首先表明了使用格点搜索可以成功地找到由TMA生成的唯一和实际混合矩阵。然后，我们提议引入符号模糊性来防止格点搜索的推断，并设计了两种原则来构建符号模糊性，即缺陷行列和非唯一性的ON-OFF切换模式。此外，我们还提出了可行的实现机制。我们的提出的原则和机制不仅帮助我们在未来理论上设计更安全的TMA DM系统，而且已经被验证了通过比特错误率测量。
</details></li>
</ul>
<hr>
<h2 id="Stacked-Intelligent-Metasurface-Performs-a-2D-DFT-in-the-Wave-Domain-for-DOA-Estimation"><a href="#Stacked-Intelligent-Metasurface-Performs-a-2D-DFT-in-the-Wave-Domain-for-DOA-Estimation" class="headerlink" title="Stacked Intelligent Metasurface Performs a 2D DFT in the Wave Domain for DOA Estimation"></a>Stacked Intelligent Metasurface Performs a 2D DFT in the Wave Domain for DOA Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09861">http://arxiv.org/abs/2310.09861</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiancheng An, Chau Yuen, Marco Di Renzo, Merouane Debbah, H. Vincent Poor, Lajos Hanzo</li>
<li>for: 这个论文的目的是提出一种基于受托辐射元件（SIM）的技术来实现二维方向来源估算（DOA）。</li>
<li>methods: 这种技术使用了一种先进的SIM，其中每个元件在入射波传播过程中自动完成了二维离散傅里叶变换（DFT）。为了使SIM完成这个任务，我们设计了一个梯度下降算法，用于逐步更新每个元件的相位Shift，以最小化SIM的响应和2D DFT矩阵之间的差异。</li>
<li>results: 数值模拟结果表明，一个充分训练的SIM可以很准确地完成2D DFT。例如，在实验中，SIM的计算速度为光学计算速度，DOA估算的 сред平均误差（MSE）为10^-4。<details>
<summary>Abstract</summary>
Staked intelligent metasurface (SIM) based techniques are developed to perform two-dimensional (2D) direction-of-arrival (DOA) estimation. In contrast to the conventional designs, an advanced SIM in front of the receiving array automatically performs the 2D discrete Fourier transform (DFT) as the incident waves propagate through it. To arrange for the SIM to carry out this task, we design a gradient descent algorithm for iteratively updating the phase shift of each meta-atom in the SIM to minimize the fitting error between the SIM's response and the 2D DFT matrix. To further improve the DOA estimation accuracy, we configure the phase shifts in the input layer of SIM to generate a set of 2D DFT matrices having orthogonal spatial frequency bins. Extensive numerical simulations verify the capability of a well-trained SIM to perform 2D DFT. Specifically, it is demonstrated that the SIM having an optical computational speed achieves an MSE of $10^{-4}$ in 2D DOA estimation.
</details>
<details>
<summary>摘要</summary>
“基于固化智能表面（SIM）技术的二维方向来源估计（2D DOA）方法已经开发出来。与传统设计不同的是，我们在接收阵列前方的高级SIM上自动执行2D离散傅里叶变换（DFT）。为让SIM进行这项任务，我们设计了一种梯度下降算法，通过迭代更新每个元素的相位偏移，以最小化SIM的响应和2D DFT矩阵之间的差异。为了进一步提高DOA估计精度，我们在SIM的输入层中配置了一系列的2D DFT矩阵，其中每个矩阵具有正交的空间频率分辨率。数值仿真表明，一个具有光学计算速度的SIM可以实现2D DOA估计的 mean squared error（MSE）为10^-4。”Note: The translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. The other form is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Towards-Structural-Sparse-Precoding-Dynamic-Time-Frequency-Space-and-Power-Multistage-Resource-Programming"><a href="#Towards-Structural-Sparse-Precoding-Dynamic-Time-Frequency-Space-and-Power-Multistage-Resource-Programming" class="headerlink" title="Towards Structural Sparse Precoding: Dynamic Time, Frequency, Space, and Power Multistage Resource Programming"></a>Towards Structural Sparse Precoding: Dynamic Time, Frequency, Space, and Power Multistage Resource Programming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09840">http://arxiv.org/abs/2310.09840</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhongxiang Wei, Ping Wang, Qingjiang Shi, Xu Zhu, Christos Masouros</li>
<li>for: 这篇论文主要针对 fifth-generation 通信系统中的即时传输应用需求，尤其是在时间维度上的质量要求。</li>
<li>methods: 本论文使用 multistage 优化方法，包括时间、频率、空间和能量领域资源的共同优化。</li>
<li>results: 本论文的设计可以实现高性能的传输系统，并且具有快速的数据测试速率。<details>
<summary>Abstract</summary>
In last decades, dynamic resource programming in partial resource domains has been extensively investigated for single time slot optimizations. However, with the emerging real-time media applications in fifth-generation communications, their new quality of service requirements are often measured in temporal dimension. This requires multistage optimization for full resource domain dynamic programming. Taking experience rate as a typical temporal multistage metric, we jointly optimize time, frequency, space and power domains resource for multistage optimization. To strike a good tradeoff between system performance and computational complexity, we first transform the formulated mixed integer non-linear constraints into equivalent convex second order cone constraints, by exploiting the coupling effect among the resources. Leveraging the concept of structural sparsity, the objective of max-min experience rate is given as a weighted 1-norm term associated with the precoding matrix. Finally, a low-complexity iterative algorithm is proposed for full resource domain programming, aided by another simple conic optimization for obtaining its feasible initial result. Simulation verifies that our design significantly outperform the benchmarks while maintaining a fast convergence rate, shedding light on full domain dynamic resource programming of multistage optimizations.
</details>
<details>
<summary>摘要</summary>
最近几十年，在半资源领域中进行了广泛的动态资源编程，以优化单个时间槽。然而，五代通信技术出现后，新的服务质量要求 oft measure在时间维度上。这需要进行全资源领域的多阶段优化。以经验率为例的 temporaldimensional metric，我们同时优化了时间、频率、空间和功率领域的资源。为了 достичь系统性能和计算复杂度之间的好 equilibrio,我们首先将混合整数非线性约束转化为等效的几何二次辐射约束，利用资源之间的协同作用。然后，我们给出了一个优化目标函数，其中包含了约束matrix的权重的1- norm。最后，我们提出了一种低复杂度的迭代算法，用于实现全资源领域的动态资源编程，并且使用另一个简单的几何优化算法来获得其可行的初始结果。实验证明了我们的设计在比较性能和速度方面具有显著的优势，提供了全资源领域的动态资源编程的全面性和可行性。
</details></li>
</ul>
<hr>
<h2 id="Cell-Free-Massive-MIMO-Surveillance-Systems"><a href="#Cell-Free-Massive-MIMO-Surveillance-Systems" class="headerlink" title="Cell-Free Massive MIMO Surveillance Systems"></a>Cell-Free Massive MIMO Surveillance Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09769">http://arxiv.org/abs/2310.09769</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zahra Mobini, Hien Quoc Ngo, Michail Matthaiou, Lajos Hanzo</li>
<li>for: 本研究旨在提高国家安全性，通过使用无线监测系统来监测不可信通信链接。</li>
<li>methods: 本研究提出了一种新的维度免疫多输入多输出（CF-mMIMO）无线监测系统，其中许多分散的多天线帮助监测节点（MNs）进行观察或干扰不可信的通信链接。</li>
<li>results: 我们分析了CF-mMIMO无线监测系统的性能，并 deriv了关于监测成功率的关闭式表达式。我们还提出了一种满足同时观察和干扰的模式分配算法，以及一种最大化最小监测成功率的干扰发射功率分配算法。研究结果表明，我们的提posed CF-mMIMO系统可以在相对较少的MN数量下提供显著性能提升，比基准值co-located mMIMO系统的11倍。<details>
<summary>Abstract</summary>
Wireless surveillance, in which untrusted communications links are proactively monitored by legitimate agencies, has started to garner a lot of interest for enhancing the national security. In this paper, we propose a new cell-free massive multiple-input multiple-output (CF-mMIMO) wireless surveillance system, where a large number of distributed multi-antenna aided legitimate monitoring nodes (MNs) embark on either observing or jamming untrusted communication links. To facilitate concurrent observing and jamming, a subset of the MNs is selected for monitoring the untrusted transmitters (UTs), while the remaining MNs are selected for jamming the untrusted receivers (URs). We analyze the performance of CF-mMIMO wireless surveillance and derive a closed-form expression for the monitoring success probability of MNs. We then propose a greedy algorithm for the observing vs, jamming mode assignment of MNs, followed by the conception of a jamming transmit power allocation algorithm for maximizing the minimum monitoring success probability concerning all the UT and UR pairs based on the associated long-term channel state information knowledge. In conclusion, our proposed CF-mMIMO system is capable of significantly improving the performance of the MNs compared to that of the state-of-the-art baseline. In scenarios of a mediocre number of MNs, our proposed scheme provides an 11-fold improvement in the minimum monitoring success probability compared to its co-located mMIMO benchmarker.
</details>
<details>
<summary>摘要</summary>
无线监测，在无法确保通信链路的情况下，由合法机构监测，已经吸引了很多关注，以提高国家安全。在这篇论文中，我们提议一种新的终端分布式多输入多输出（CF-mMIMO）无线监测系统，其中一大量分布在多个antenna帮助合法监测节点（MNs）进行观察或干扰不可信通信链路。为实现同时观察和干扰，一部分MNs用于观察不可信发送器（UTs），而另一部分MNs用于干扰不可信接收器（URs）。我们分析了CF-mMIMO无线监测系统的性能，并 derivated一个关闭式表达式来表示MNs的监测成功率。然后，我们提出了一种满足策略来选择MNs的观察和干扰模式，并提出了一种基于长期通道状态信息的干扰发射功率分配策略，以最大化所有UT和UR对的监测成功率。在结论中，我们的提议的CF-mMIMO系统可以在MNs的数量不多的情况下，与当前基准相比，提高监测成功率的最小值。在一些中等数量的MNs情况下，我们的方案提供了11倍的监测成功率提升，相比于其相对位置的mMIMO参考。
</details></li>
</ul>
<hr>
<h2 id="Assessing-Smart-Algorithms-for-Gait-Phases-Detection-in-Lower-Limb-Prosthesis-A-Comprehensive-Review"><a href="#Assessing-Smart-Algorithms-for-Gait-Phases-Detection-in-Lower-Limb-Prosthesis-A-Comprehensive-Review" class="headerlink" title="Assessing Smart Algorithms for Gait Phases Detection in Lower Limb Prosthesis: A Comprehensive Review"></a>Assessing Smart Algorithms for Gait Phases Detection in Lower Limb Prosthesis: A Comprehensive Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09735">http://arxiv.org/abs/2310.09735</a></li>
<li>repo_url: None</li>
<li>paper_authors: Barath Kumar JK, Aswadh Khumar G S</li>
<li>for: 这些研究旨在提高步态分类的精度，以便在脊梁rehabilitation系统中应用。</li>
<li>methods: 这些研究使用了多种感知器，包括佩戴式和非佩戴式的感知器，以获取步态数据。</li>
<li>results: 研究发现了多种感知器和感知器组合，可以在日常环境中分析步态模式。这些感知器的选择因素包括感知器的精度、可靠性和成本等。<details>
<summary>Abstract</summary>
Over the past few years, the division of gait phases has emerged as a complex area of research that carries significant importance for various applications in the field of gait technologies. The accurate partitioning of gait phases plays a crucial role in advancing these applications. Researchers have been exploring a range of sensors that can be employed to provide data for algorithms involved in gait phase partitioning. These sensors can be broadly categorized into two types: wearable and non-wearable, each offering unique advantages and capabilities. In our study aimed at examining the current approaches to gait analysis and detection specifically designed for implementation in ambulatory rehabilitation systems, we conducted a comprehensive meta-analysis of existing research studies. Our analysis revealed a diverse range of sensors and sensor combinations that demonstrate the ability to analyze gait patterns in ambulatory settings. These sensor options vary from basic force-based binary switches to more intricate setups incorporating multiple inertial sensors and sophisticated algorithms. The findings highlight the wide spectrum of available technologies and methodologies used in gait analysis for ambulatory applications. To conduct an extensive review, we systematically examined two prominent databases, IEEE and Scopus, with the aim of identifying relevant studies pertaining to gait analysis. The search criteria were limited to 189 papers published between 1999 and 2023. From this pool, we identified and included five papers that specifically focused on various techniques including Thresholding, Quasi-static method, adaptive classifier, and SVM-based approaches. These selected papers provided valuable insights for our review.
</details>
<details>
<summary>摘要</summary>
过去几年，走势阶段的分类已经成为一个复杂的研究领域，具有重要的应用意义在走势技术领域。精确地分类走势阶段是提高这些应用的关键因素。研究人员正在探索一种以下的仪器来提供资料 для走势阶段分类的算法：抽象和非抽象的仪器，每一种都有各自的优点和能力。在我们的研究中，我们对于数位训练系统中的走势分析和检测进行了广泛的meta分析。我们发现了一些不同的仪器和仪器组合，可以在行动 Setting中分析走势模式。这些仪器选择自基本的力矩基于的二进制变数到更复杂的设备和复杂的算法。我们的发现显示了走势分析在行动应用中的广泛技术和方法。为了进行广泛的评审，我们对IEEE和Scopus两个著名的数据库进行了系统性的搜寻，并将搜寻结果限定为1999年至2023年发表的189篇文献。从这个池中，我们选择和包括了不同的技术，例如阈值分类、静止方法、适应分类和SVM基本方法的五篇文献。这些选择的文献给我们提供了宝贵的启示。
</details></li>
</ul>
<hr>
<h2 id="A-generalization-of-the-achievable-rate-of-a-MISO-system-using-Bode-Fano-wideband-matching-theory"><a href="#A-generalization-of-the-achievable-rate-of-a-MISO-system-using-Bode-Fano-wideband-matching-theory" class="headerlink" title="A generalization of the achievable rate of a MISO system using Bode-Fano wideband matching theory"></a>A generalization of the achievable rate of a MISO system using Bode-Fano wideband matching theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09723">http://arxiv.org/abs/2310.09723</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nitish Deshpande, Miguel R. Castellanos, Saeed R. Khosravirad, Jinfeng Du, Harish Viswanathan, Robert W. Heath Jr</li>
<li>for: 本研究旨在提高多输入单输出（MISO）系统的信息理论可实现率，并具体实现了宽频匹配理论。</li>
<li>methods: 本研究使用多口电路理论方法，具体是利用频率选择性的散射参数来优化MISO系统的可实现率。</li>
<li>results: 研究结果表明，使用优化的传输系数和劳达-法诺不等式约束，可以提高MISO系统的信息理论可实现率。对比 идеаль的可实现率、不考虑匹配约束的可实现率以及使用不优化匹配策略的可实现率，研究结果表明优化匹配网络可以提高MISO系统的可实现率。此外，研究还提出了一种实用的方法来估算可实现率上限。<details>
<summary>Abstract</summary>
Impedance-matching networks affect power transfer from the radio frequency (RF) chains to the antennas. Their design impacts the signal to noise ratio (SNR) and the achievable rate. In this paper, we maximize the information-theoretic achievable rate of a multiple-input-single-output (MISO) system with wideband matching constraints. Using a multiport circuit theory approach with frequency-selective scattering parameters, we propose a general framework for optimizing the MISO achievable rate that incorporates Bode-Fano wideband matching theory. We express the solution to the achievable rate optimization problem in terms of the optimized transmission coefficient and the Lagrangian parameters corresponding to the Bode-Fano inequality constraints. We apply this framework to a single electric Chu's antenna and an array of two electric Chu's antennas. We compare the optimized achievable rate obtained numerically with other benchmarks like the ideal achievable rate computed by disregarding matching constraints and the achievable rate obtained by using sub-optimal matching strategies like conjugate matching and frequency-flat transmission. We also propose a practical methodology to approximate the achievable rate bound by using the optimal transmission coefficient to derive a physically realizable matching network through the ADS software.
</details>
<details>
<summary>摘要</summary>
“干扰网络影响电力传输自 ради频率（RF）扩展到天线。它们的设计对信号与噪音比（SNR）和可行率有影响。在这篇论文中，我们将最大化多Input单Output（MISO）系统的信号理论可行率。使用多口筒电路理论方法，我们提出一个应用各种频率选择性散射特性的通用框架，以便优化MISO可行率。我们将解决可行率最佳化问题的解释为优化传输系数和Bode-Fano干扰对称理论中的Lagrangian参数。我们将这个框架应用到单电池Chu天线和两个电池Chu天线阵列。我们比较优化的可行率与其他参考标准（例如忽略干扰限制的理论可行率和适应干扰策略如 conjugate matching和频率平坦传输）进行比较。我们还提出了一个实用的方法来近似可行率上限，通过使用最佳传输系数来 derive physically realizable干扰网络，使用ADS软件。”
</details></li>
</ul>
<hr>
<h2 id="Two-Enhanced-rate-Power-Allocation-Strategies-for-Active-IRS-assisted-Wireless-Network"><a href="#Two-Enhanced-rate-Power-Allocation-Strategies-for-Active-IRS-assisted-Wireless-Network" class="headerlink" title="Two Enhanced-rate Power Allocation Strategies for Active IRS-assisted Wireless Network"></a>Two Enhanced-rate Power Allocation Strategies for Active IRS-assisted Wireless Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09721">http://arxiv.org/abs/2310.09721</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiankun Cheng, Rongen Dong, Wenlong Cai, Ruiqi Liu, Feng Shu, Jiangzhou Wang<br>for:active IRS-aided network under a total power constraintmethods:adjusting power between base station (BS) and IRS, transmit beamforming at BS, reflecting beamforming at IRSresults:maximizing the SNR with two high-performance PA strategies, enhanced multiple random initialization Newton’s (EMRIN) and Taylor polynomial approximation (TPA), which perform much better than fixed PA in accordance with rate, and approach exhaustive search as the number of IRS reflecting elements increases.Here is the Chinese translation of the three key information points:for:活动反射表面协助网络下的总功率限制方法:调整基站和反射表面之间的功率，传输扫描和反射扫描结果:通过两种高性能的PA策略，即增强多random初始化Newton方法（EMRIN）和Taylor多项式approximation（TPA），实现了对级别的最大化SNR，并且比固定PA更好，随着反射表面元素的数量增加，逼近极值搜索。<details>
<summary>Abstract</summary>
Due to its ability of overcoming the impact of double-fading effect, active intelligent reflecting surface (IRS) has attracted a lot of attention. Unlike passive IRS, active IRS should be supplied by power, thus adjusting power between base station (BS) and IRS having a direct impact on the system rate performance. In this paper, the active IRS-aided network under a total power constraint is modeled with an ability of adjusting power between BS and IRS. Given the transmit beamforming at BS and reflecting beamforming at IRS, the SNR expression is derived to be a function of power allocation (PA) factor, and the optimization of maximizing the SNR is given. Subsequently, two high-performance PA strategies, enhanced multiple random initialization Newton's (EMRIN) and Taylor polynomial approximation (TPA), are proposed. The former is to improve the rate performance of classic Netwon's method to avoid involving a local optimal point by using multiple random initializations. To reduce its high computational complexity, the latter provides a closed-form solution by making use of the first-order Taylor polynomial approximation to the original SNR function. Actually, using TPA, the original optimization problem is transformed into a problem of finding a root for a third-order polynomial.Simulation results are as follows: the first-order TPA of SNR fit its exact expression well, the proposed two PA methods performs much better than fixed PA in accordance with rate, and appoaches exhaustive search as the number of IRS reflecting elements goes to large-scale.
</details>
<details>
<summary>摘要</summary>
因为它可以超越双折射效应的影响，活动智能反射 superficie (IRS) 已经吸引了很多关注。 unlike 被动 IRS，活动 IRS 需要接受电力供应，因此在基站 (BS) 和 IRS 之间的电力调整直接影响系统速率性能。在这篇文章中，我们模型了具有电力限制的活动 IRS-助け的网络。给定基站发射扫描和 IRS 反射扫描，我们 derivate 了 SNR 表达式，并提出了最大化 SNR 的优化问题。然后，我们提出了两种高性能 PA 策略：增强多个随机初始化 Newton 方法（EMRIN）和 Taylor  polynomials 近似法（TPA）。前者是为了提高 классических Newton 方法的率性能，避免Local 优点点的涉及。而后者通过使用首颗 Taylor  polynomials 近似来原 SNR 函数，提供了一个关闭式解决方案，从而减少了高计算复杂性。实际上，使用 TPA，原来的优化问题被转化为了一个找到第三阶 polynomials 的根的问题。实验结果如下：TPA 的第一阶近似与 exact 表达式很相似，而我们提出的两种 PA 方法在 accordance  WITH 率性能上明显超过 fix PA，并且随着 IRS 反射元件的数量增加，两种方法的性能接近 exhaustive search。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/15/eess.SP_2023_10_15/" data-id="cloh7tqrd019p7b88h3vzgr75" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_10_14" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/14/cs.SD_2023_10_14/" class="article-date">
  <time datetime="2023-10-14T15:00:00.000Z" itemprop="datePublished">2023-10-14</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/14/cs.SD_2023_10_14/">cs.SD - 2023-10-14</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Dynamic-Prediction-of-Full-Ocean-Depth-SSP-by-Hierarchical-LSTM-An-Experimental-Result"><a href="#Dynamic-Prediction-of-Full-Ocean-Depth-SSP-by-Hierarchical-LSTM-An-Experimental-Result" class="headerlink" title="Dynamic Prediction of Full-Ocean Depth SSP by Hierarchical LSTM: An Experimental Result"></a>Dynamic Prediction of Full-Ocean Depth SSP by Hierarchical LSTM: An Experimental Result</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09522">http://arxiv.org/abs/2310.09522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiajun Lu, Wei Huang, Hao Zhang</li>
<li>for: 用于预测未来水声速度分布，提高海上定位、导航和时间测量（PNT）精度。</li>
<li>methods: 提议使用层次Long Short-Term Memory（H-LSTM）神经网络预测未来水声速度分布，利用时间维度中的声速分布分布模式。</li>
<li>results: 通过实验和仿真 validate the proposed method，结果显示该方法的准确率高于现有方法。<details>
<summary>Abstract</summary>
SSP distribution is an important parameter for underwater positioning, navigation and timing (PNT) because it affects the propagation mode of underwater acoustic signals. To accurate predict future sound speed distribution, we propose a hierarchical long short--term memory (H--LSTM) neural network for future sound speed prediction, which explore the distribution pattern of sound velocity in the time dimension. To verify the feasibility and effectiveness, we conducted both simulations and real experiments. The ocean experiment was held in the South China Sea in April, 2023. Results show that the accuracy of the proposed method outperforms the state--of--the--art methods.
</details>
<details>
<summary>摘要</summary>
<<SSP分布是水下定位、导航和时间（PNT）中的一个重要参数，因为它影响水下声波的传播模式。为了准确预测未来声速分布，我们提议使用层次长短期记忆（H-LSTM）神经网络进行未来声速预测，以探索声速在时间维度上的分布模式。为了证明可行性和有效性，我们进行了 simulations和实验。海洋实验在2023年4月在南海举行。结果表明，提议的方法的精度高于现有方法。>>Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/14/cs.SD_2023_10_14/" data-id="cloh7tqml00wz7b8851kgghis" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_10_14" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/14/cs.CV_2023_10_14/" class="article-date">
  <time datetime="2023-10-14T13:00:00.000Z" itemprop="datePublished">2023-10-14</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/14/cs.CV_2023_10_14/">cs.CV - 2023-10-14</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="What-Do-Deep-Saliency-Models-Learn-about-Visual-Attention"><a href="#What-Do-Deep-Saliency-Models-Learn-about-Visual-Attention" class="headerlink" title="What Do Deep Saliency Models Learn about Visual Attention?"></a>What Do Deep Saliency Models Learn about Visual Attention?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09679">http://arxiv.org/abs/2310.09679</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/szzexpoi/saliency_analysis">https://github.com/szzexpoi/saliency_analysis</a></li>
<li>paper_authors: Shi Chen, Ming Jiang, Qi Zhao</li>
<li>for: 这篇论文旨在探讨深度聚焦模型如何预测人类视觉注意力，以及这些模型的成功机制是如何工作的。</li>
<li>methods: 本文提出了一种新的分析框架，可以帮助理解深度聚焦模型学习的隐藏特征，并提供了一种原则性的解释和量化这些特征的贡献。这个框架可以将隐藏特征分解成可解释的基准，并将聚焦预测转化为一种权重组合的问题。</li>
<li>results: 通过应用这种框架，我们进行了广泛的分析，包括聚焦预测的正面和负面权重、训练数据和架构设计的影响、细化训练的进程效应和常见的深度聚焦模型失败模式。此外，我们还通过分析不同应用场景中的视觉注意力特征，如人类自闭症 spectrum 症例中的异常注意力、情感引起的吸引注意力和时间的注意力演化。<details>
<summary>Abstract</summary>
In recent years, deep saliency models have made significant progress in predicting human visual attention. However, the mechanisms behind their success remain largely unexplained due to the opaque nature of deep neural networks. In this paper, we present a novel analytic framework that sheds light on the implicit features learned by saliency models and provides principled interpretation and quantification of their contributions to saliency prediction. Our approach decomposes these implicit features into interpretable bases that are explicitly aligned with semantic attributes and reformulates saliency prediction as a weighted combination of probability maps connecting the bases and saliency. By applying our framework, we conduct extensive analyses from various perspectives, including the positive and negative weights of semantics, the impact of training data and architectural designs, the progressive influences of fine-tuning, and common failure patterns of state-of-the-art deep saliency models. Additionally, we demonstrate the effectiveness of our framework by exploring visual attention characteristics in various application scenarios, such as the atypical attention of people with autism spectrum disorder, attention to emotion-eliciting stimuli, and attention evolution over time. Our code is publicly available at \url{https://github.com/szzexpoi/saliency_analysis}.
</details>
<details>
<summary>摘要</summary>
在最近的几年中，深度眩怯模型已经取得了人类视觉注意力预测的 significanth进步。然而，这些模型的成功的机制仍然largely unexplained，这是因为深度神经网络的含义不 transparent。在这篇论文中，我们提出了一种新的分析框架，它可以揭示深度眩怯模型学习的隐式特征，并提供了理解和量化这些特征对眩怯预测的贡献的原则性的解释。我们的方法将这些隐式特征分解成可解释的基准，这些基准与semantic attribute的对应关系是Explicitly aligned。我们重新定义了眩怯预测为这些基准之间的权重加权组合，并通过应用我们的框架，我们进行了广泛的分析，包括正面和负面权重的semantics，训练数据和建筑设计的影响，练习的进步性和state-of-the-art深度眩怯模型的共同失败模式。此外，我们还通过各种应用场景来探讨视觉注意力特征，例如人类 autism spectrum disorder 的非典型注意力、情感刺激刺激的注意力和时间的注意力演化。我们的代码公开在 \url{https://github.com/szzexpoi/saliency_analysis}。
</details></li>
</ul>
<hr>
<h2 id="Point-DynRF-Point-based-Dynamic-Radiance-Fields-from-a-Monocular-Video"><a href="#Point-DynRF-Point-based-Dynamic-Radiance-Fields-from-a-Monocular-Video" class="headerlink" title="Point-DynRF: Point-based Dynamic Radiance Fields from a Monocular Video"></a>Point-DynRF: Point-based Dynamic Radiance Fields from a Monocular Video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09647">http://arxiv.org/abs/2310.09647</a></li>
<li>repo_url: None</li>
<li>paper_authors: Byeongjun Park, Changick Kim</li>
<li>for: 生成从笔直视频中的新视图</li>
<li>methods: 使用 neural point clouds 和 dynamic radiance fields 来学习全局场景几何信息和渲染过程</li>
<li>results: 在 NVIDIA Dynamic Scenes Dataset 和一些 causally captured monocular video clips 上验证了方法的有效性<details>
<summary>Abstract</summary>
Dynamic radiance fields have emerged as a promising approach for generating novel views from a monocular video. However, previous methods enforce the geometric consistency to dynamic radiance fields only between adjacent input frames, making it difficult to represent the global scene geometry and degenerates at the viewpoint that is spatio-temporally distant from the input camera trajectory. To solve this problem, we introduce point-based dynamic radiance fields (\textbf{Point-DynRF}), a novel framework where the global geometric information and the volume rendering process are trained by neural point clouds and dynamic radiance fields, respectively. Specifically, we reconstruct neural point clouds directly from geometric proxies and optimize both radiance fields and the geometric proxies using our proposed losses, allowing them to complement each other. We validate the effectiveness of our method with experiments on the NVIDIA Dynamic Scenes Dataset and several causally captured monocular video clips.
</details>
<details>
<summary>摘要</summary>
《动态辐射场》技术在生成单视图的新观察角度方面表现出了惊人的前进。然而，先前的方法只在邻近输入帧中保证动态辐射场的几何一致性，使得表示全场景几何和观点远离输入摄像机轨迹的问题变得困难。为解决这个问题，我们提出了点基的动态辐射场（Point-DynRF）框架，其中global scene几何信息和volume渲染过程通过神经点云和动态辐射场进行了培auotrained。具体来说，我们直接从几何代理中重建神经点云，并通过我们提出的损失函数来优化辐射场和几何代理，使其相互补做。我们通过对NVIDIA动态场景集和一些 causally captured的单视图视频剪辑进行实验 validate了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Dimma-Semi-supervised-Low-Light-Image-Enhancement-with-Adaptive-Dimming"><a href="#Dimma-Semi-supervised-Low-Light-Image-Enhancement-with-Adaptive-Dimming" class="headerlink" title="Dimma: Semi-supervised Low Light Image Enhancement with Adaptive Dimming"></a>Dimma: Semi-supervised Low Light Image Enhancement with Adaptive Dimming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09633">http://arxiv.org/abs/2310.09633</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wojciechkoz/dimma">https://github.com/wojciechkoz/dimma</a></li>
<li>paper_authors: Wojciech Kozłowski, Michał Szachniewicz, Michał Stypułkowski, Maciej Zięba</li>
<li>for: 提高低光照图像质量，保持自然颜色</li>
<li>methods: 使用小量图像对数据集进行协同学习，通过混合混合稀释权重网络来模拟不同摄像头下的场景拍摄情况，并通过精度量化器来调整亮度水平</li>
<li>results: 使用只需几个图像对可以达到与完全监督方法相同的竞争水平，并且在某些指标上超越当前状态艺法，几乎与其他方法相当Here’s the explanation in English:</li>
<li>for: The paper aims to enhance low-light images while maintaining their natural colors.</li>
<li>methods: The proposed approach uses a small set of image pairs to replicate scenes captured under extreme lighting conditions using a specific camera. It employs a convolutional mixture density network to generate distorted colors based on illumination differences, and accurately grades the dimming factor for flexibility in adjusting brightness levels. Additionally, the approach uses a conditional UNet architecture to generate images with desired lightness levels based on user input.</li>
<li>results: The proposed approach achieves competitive results compared to fully supervised methods, and surpasses state-of-the-art methods in some metrics when trained on the full dataset.<details>
<summary>Abstract</summary>
Enhancing low-light images while maintaining natural colors is a challenging problem due to camera processing variations and limited access to photos with ground-truth lighting conditions. The latter is a crucial factor for supervised methods that achieve good results on paired datasets but do not handle out-of-domain data well. On the other hand, unsupervised methods, while able to generalize, often yield lower-quality enhancements. To fill this gap, we propose Dimma, a semi-supervised approach that aligns with any camera by utilizing a small set of image pairs to replicate scenes captured under extreme lighting conditions taken by that specific camera. We achieve that by introducing a convolutional mixture density network that generates distorted colors of the scene based on the illumination differences. Additionally, our approach enables accurate grading of the dimming factor, which provides a wide range of control and flexibility in adjusting the brightness levels during the low-light image enhancement process. To further improve the quality of our results, we introduce an architecture based on a conditional UNet. The lightness value provided by the user serves as the conditional input to generate images with the desired lightness. Our approach using only few image pairs achieves competitive results compared to fully supervised methods. Moreover, when trained on the full dataset, our model surpasses state-of-the-art methods in some metrics and closely approaches them in others.
</details>
<details>
<summary>摘要</summary>
提高低光照图像的质量是一个具有挑战性的问题，因为摄像头处理的变化和有限的照明条件图像的数量。后者是重要的因素，因为超级vised方法可以在匹配数据集上达到良好的结果，但是不能处理非预期的数据。相反，无监督方法可以泛化，但是通常会产生较低质量的提高。为了填补这个差距，我们提议了Dimma，一种半监督方法，通过使用特定摄像头拍摄的场景图像的小量对照片来模拟极端照明条件下拍摄的场景。我们通过引入一个 convolutional mixture density network来生成场景图像中的扭曲颜色，基于照明差异。此外，我们的方法可以精确地测量场景图像的暗度因子，提供了较广泛的控制和灵活性来调整低光照图像的亮度水平。为了进一步提高我们的结果质量，我们引入了基于 conditional UNet 的架构。用户输入的亮度值 serving as the conditional input，以生成具有所需亮度的图像。我们的方法使用只需几对照片可以与完全监督方法相比，并且当训练在全 dataset 时，我们的模型超越了当前状态的方法，在一些指标中甚至超越了其他方法，只是在其他指标中有些落后。
</details></li>
</ul>
<hr>
<h2 id="Time-based-Mapping-of-Space-Using-Visual-Motion-Invariants"><a href="#Time-based-Mapping-of-Space-Using-Visual-Motion-Invariants" class="headerlink" title="Time-based Mapping of Space Using Visual Motion Invariants"></a>Time-based Mapping of Space Using Visual Motion Invariants</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09632">http://arxiv.org/abs/2310.09632</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juan D. Yepes, Daniel Raviv</li>
<li>for: 这个论文的目的是提出一种基于视觉动态特征的三维点 clouds的表示方法，以确保形态不变。</li>
<li>methods: 该方法利用非线性的流体量计量器来创建一种新的表示方法，称为“时间清除”（Time-Clearance）和“时间到接触”（Time-to-Contact）。这些 invariants 保持时间不变，使得可以轻松地检测移动点不符合预期的不变性。</li>
<li>results: 作者通过实验和 Unity 模拟来证明这种表示方法的有效性，并表明可以轻松地检测移动点不符合预期的不变性。此外，这种表示方法需要只一个摄像机，并且不需要确定摄像机的运动速度量。此外，该方法适合并行处理。<details>
<summary>Abstract</summary>
This paper focuses on visual motion-based invariants that result in a representation of 3D points in which the stationary environment remains invariant, ensuring shape constancy. This is achieved even as the images undergo constant change due to camera motion. Nonlinear functions of measurable optical flow, which are related to geometric 3D invariants, are utilized to create a novel representation. We refer to the resulting optical flow-based invariants as 'Time-Clearance' and the well-known 'Time-to-Contact' (TTC). Since these invariants remain constant over time, it becomes straightforward to detect moving points that do not adhere to the expected constancy. We present simulations of a camera moving relative to a 3D object, snapshots of its projected images captured by a rectilinearly moving camera, and the object as it appears unchanged in the new domain over time. In addition, Unity-based simulations demonstrate color-coded transformations of a projected 3D scene, illustrating how moving objects can be readily identified. This representation is straightforward, relying on simple optical flow functions. It requires only one camera, and there is no need to determine the magnitude of the camera's velocity vector. Furthermore, the representation is pixel-based, making it suitable for parallel processing.
</details>
<details>
<summary>摘要</summary>
We present simulations of a camera moving relative to a 3D object, snapshots of its projected images captured by a rectilinearly moving camera, and the object as it appears unchanged in the new domain over time. In addition, Unity-based simulations demonstrate color-coded transformations of a projected 3D scene, illustrating how moving objects can be readily identified.This representation is straightforward, relying on simple optical flow functions. It requires only one camera, and there is no need to determine the magnitude of the camera's velocity vector. Furthermore, the representation is pixel-based, making it suitable for parallel processing.
</details></li>
</ul>
<hr>
<h2 id="Real-Time-Traffic-Sign-Detection-A-Case-Study-in-a-Santa-Clara-Suburban-Neighborhood"><a href="#Real-Time-Traffic-Sign-Detection-A-Case-Study-in-a-Santa-Clara-Suburban-Neighborhood" class="headerlink" title="Real-Time Traffic Sign Detection: A Case Study in a Santa Clara Suburban Neighborhood"></a>Real-Time Traffic Sign Detection: A Case Study in a Santa Clara Suburban Neighborhood</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09630">http://arxiv.org/abs/2310.09630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Harish Loghashankar, Hieu Nguyen</li>
<li>for: 这项研究旨在开发一个实时交通标识系统，使用YOLOv5架构并在室外社区中进行实时识别交通标识。</li>
<li>methods: 该项目将使用多样化的交通标识图像集进行训练YOLOv5模型，并在适合实时推理的硬件平台上部署模型。</li>
<li>results: 在实验中，该系统在实时摄像头上检测和识别交通标识得分为96%，表明该系统可以提供实时和准确的交通信息，有助于提高道路安全和交通管理，并且可能为自动驾驶研究开辟新的可能性。<details>
<summary>Abstract</summary>
This research project aims to develop a real-time traffic sign detection system using the YOLOv5 architecture and deploy it for efficient traffic sign recognition during a drive in a suburban neighborhood. The project's primary objectives are to train the YOLOv5 model on a diverse dataset of traffic sign images and deploy the model on a suitable hardware platform capable of real-time inference. The project will involve collecting a comprehensive dataset of traffic sign images. By leveraging the trained YOLOv5 model, the system will detect and classify traffic signs from a real-time camera on a dashboard inside a vehicle. The performance of the deployed system will be evaluated based on its accuracy in detecting traffic signs, real-time processing speed, and overall reliability. During a case study in a suburban neighborhood, the system demonstrated a notable 96% accuracy in detecting traffic signs. This research's findings have the potential to improve road safety and traffic management by providing timely and accurate real-time information about traffic signs and can pave the way for further research into autonomous driving.
</details>
<details>
<summary>摘要</summary>
The project involves collecting a comprehensive dataset of traffic sign images, and leveraging the trained YOLOv5 model, the system will detect and classify traffic signs from a real-time camera on a dashboard inside a vehicle. The performance of the deployed system will be evaluated based on its accuracy in detecting traffic signs, real-time processing speed, and overall reliability.During a case study in a suburban neighborhood, the system demonstrated a notable 96% accuracy in detecting traffic signs. The findings of this research have the potential to improve road safety and traffic management by providing timely and accurate real-time information about traffic signs, and can pave the way for further research into autonomous driving.Translation notes:* "suburban neighborhood" is translated as "郊区" (suburban area)* "dashboard" is translated as "车载屏" (car-mounted screen)* "real-time" is translated as "实时" (real-time)* "accuracy" is translated as "准确率" (accuracy)* "processing speed" is translated as "处理速度" (processing speed)* "reliability" is translated as "可靠性" (reliability)Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Detecting-Moving-Objects-Using-a-Novel-Optical-Flow-Based-Range-Independent-Invariant"><a href="#Detecting-Moving-Objects-Using-a-Novel-Optical-Flow-Based-Range-Independent-Invariant" class="headerlink" title="Detecting Moving Objects Using a Novel Optical-Flow-Based Range-Independent Invariant"></a>Detecting Moving Objects Using a Novel Optical-Flow-Based Range-Independent Invariant</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09627">http://arxiv.org/abs/2310.09627</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Raviv, Juan D. Yepes, Ayush Gowda</li>
<li>for: 这篇论文主要关注了一种新的移动对象检测方法，该方法可以在摄像机运动时检测移动对象。</li>
<li>methods: 该方法使用了光流基本变换，通过这种变换可以生成一个不依赖于时间点播、点云范围和摄像机速度的兼容2D不变图像输出。在这个新频谱中，对于3D点 cloud中的点，如果它们与预定的查找图像值不匹配，那么它们就可以被轻松地识别为相对于静止3D环境中的移动对象。该方法不需要了解摄像机的运动方向或速度，也不需要3D点云范围信息。它适合实时并行处理，因此非常实用。</li>
<li>results: 作者通过实验和仿真 validate了新频谱的有效性，并证明其在拥有直线运动的摄像机时的稳定性。这种方法开创了新的移动对象检测方法，同时也为未来在六度自由运动摄像机中的研究提供了基础。<details>
<summary>Abstract</summary>
This paper focuses on a novel approach for detecting moving objects during camera motion. We present an optical-flow-based transformation that yields a consistent 2D invariant image output regardless of time instants, range of points in 3D, and the speed of the camera. In other words, this transformation generates a lookup image that remains invariant despite the changing projection of the 3D scene and camera motion. In the new domain, projections of 3D points that deviate from the values of the predefined lookup image can be clearly identified as moving relative to the stationary 3D environment, making them seamlessly detectable. The method does not require prior knowledge of the direction of motion or speed of the camera, nor does it necessitate 3D point range information. It is well-suited for real-time parallel processing, rendering it highly practical for implementation. We have validated the effectiveness of the new domain through simulations and experiments, demonstrating its robustness in scenarios involving rectilinear camera motion, both in simulations and with real-world data. This approach introduces new ways for moving objects detection during camera motion, and also lays the foundation for future research in the context of moving object detection during six-degrees-of-freedom camera motion.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="JSMoCo-Joint-Coil-Sensitivity-and-Motion-Correction-in-Parallel-MRI-with-a-Self-Calibrating-Score-Based-Diffusion-Model"><a href="#JSMoCo-Joint-Coil-Sensitivity-and-Motion-Correction-in-Parallel-MRI-with-a-Self-Calibrating-Score-Based-Diffusion-Model" class="headerlink" title="JSMoCo: Joint Coil Sensitivity and Motion Correction in Parallel MRI with a Self-Calibrating Score-Based Diffusion Model"></a>JSMoCo: Joint Coil Sensitivity and Motion Correction in Parallel MRI with a Self-Calibrating Score-Based Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09625">http://arxiv.org/abs/2310.09625</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lixuan Chen, Xuanyu Tian, Jiangjie Wu, Ruimin Feng, Guoyan Lao, Yuyao Zhang, Hongjiang Wei</li>
<li>for:  correction of motion artifacts in MRI reconstruction</li>
<li>methods:  joint estimation of motion parameters and coil sensitivity maps using score-based diffusion models and Gibbs sampler</li>
<li>results:  high-quality MRI image reconstruction from sparsely-sampled k-space data, even in the presence of motion<details>
<summary>Abstract</summary>
Magnetic Resonance Imaging (MRI) stands as a powerful modality in clinical diagnosis. However, it is known that MRI faces challenges such as long acquisition time and vulnerability to motion-induced artifacts. Despite the success of many existing motion correction algorithms, there has been limited research focused on correcting motion artifacts on the estimated coil sensitivity maps for fast MRI reconstruction. Existing methods might suffer from severe performance degradation due to error propagation resulting from the inaccurate coil sensitivity maps estimation. In this work, we propose to jointly estimate the motion parameters and coil sensitivity maps for under-sampled MRI reconstruction, referred to as JSMoCo. However, joint estimation of motion parameters and coil sensitivities results in a highly ill-posed inverse problem due to an increased number of unknowns. To address this, we introduce score-based diffusion models as powerful priors and leverage the MRI physical principles to efficiently constrain the solution space for this optimization problem. Specifically, we parameterize the rigid motion as three trainable variables and model coil sensitivity maps as polynomial functions. Leveraging the physical knowledge, we then employ Gibbs sampler for joint estimation, ensuring system consistency between sensitivity maps and desired images, avoiding error propagation from pre-estimated sensitivity maps to the reconstructed images. We conduct comprehensive experiments to evaluate the performance of JSMoCo on the fastMRI dataset. The results show that our method is capable of reconstructing high-quality MRI images from sparsely-sampled k-space data, even affected by motion. It achieves this by accurately estimating both motion parameters and coil sensitivities, effectively mitigating motion-related challenges during MRI reconstruction.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-Hierarchical-Features-with-Joint-Latent-Space-Energy-Based-Prior"><a href="#Learning-Hierarchical-Features-with-Joint-Latent-Space-Energy-Based-Prior" class="headerlink" title="Learning Hierarchical Features with Joint Latent Space Energy-Based Prior"></a>Learning Hierarchical Features with Joint Latent Space Energy-Based Prior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09604">http://arxiv.org/abs/2310.09604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiali Cui, Ying Nian Wu, Tian Han</li>
<li>for: 学习层次表示的基本问题</li>
<li>methods: 提议使用共同幽会空间EBM前模型和多层幽会变量</li>
<li>results: 实验表明该模型能有效地捕捉层次表示和模型数据分布<details>
<summary>Abstract</summary>
This paper studies the fundamental problem of multi-layer generator models in learning hierarchical representations. The multi-layer generator model that consists of multiple layers of latent variables organized in a top-down architecture tends to learn multiple levels of data abstraction. However, such multi-layer latent variables are typically parameterized to be Gaussian, which can be less informative in capturing complex abstractions, resulting in limited success in hierarchical representation learning. On the other hand, the energy-based (EBM) prior is known to be expressive in capturing the data regularities, but it often lacks the hierarchical structure to capture different levels of hierarchical representations. In this paper, we propose a joint latent space EBM prior model with multi-layer latent variables for effective hierarchical representation learning. We develop a variational joint learning scheme that seamlessly integrates an inference model for efficient inference. Our experiments demonstrate that the proposed joint EBM prior is effective and expressive in capturing hierarchical representations and modelling data distribution.
</details>
<details>
<summary>摘要</summary>
To address this issue, the authors propose a joint latent space EBM prior model with multi-layer latent variables for effective hierarchical representation learning. They develop a variational joint learning scheme that seamlessly integrates an inference model for efficient inference. The authors' experiments demonstrate that the proposed joint EBM prior is effective and expressive in capturing hierarchical representations and modeling data distribution.Translation notes:* "multi-layer generator models" becomes "多层生成器模型" (duō zhì chǎng jī mó delè)* "latent variables" becomes "隐藏变量" (yǐn zhǐ biàn yòu)* "Gaussian" becomes "高矮分布" (gāo ài fān bù)* "energy-based" becomes "能量基于" (néng liàng jī yǔ)* "hierarchical representations" becomes "层次表示" (céng zhì bǎo xiǎng)* "data distribution" becomes "数据分布" (shù jiào fān bù)
</details></li>
</ul>
<hr>
<h2 id="B-Spine-Learning-B-Spline-Curve-Representation-for-Robust-and-Interpretable-Spinal-Curvature-Estimation"><a href="#B-Spine-Learning-B-Spline-Curve-Representation-for-Robust-and-Interpretable-Spinal-Curvature-Estimation" class="headerlink" title="B-Spine: Learning B-Spline Curve Representation for Robust and Interpretable Spinal Curvature Estimation"></a>B-Spine: Learning B-Spline Curve Representation for Robust and Interpretable Spinal Curvature Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09603">http://arxiv.org/abs/2310.09603</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/whao22/B-Spine">https://github.com/whao22/B-Spine</a></li>
<li>paper_authors: Hao Wang, Qiang Song, Ruofeng Yin, Rui Ma, Yizhou Yu, Yi Chang<br>for:* 这paper是为了提出一种robust和可 interpret的spinal curvature estimation方法。methods:* 该方法使用了一种深度学习pipeline，包括SegRefine网络和B-spline预测模型，以便从低质量X射线图像中提取spine curvature。results:* 与其他代表性和State-of-the-Art学习基于方法进行Quantitative和Qualitative比较后，该方法在公共AASCE2019数据集和我们新提出的CJUH-JLU数据集上表现出了superior的性能，demonstrating its robustness and interpretability for spinal curvature estimation.<details>
<summary>Abstract</summary>
Spinal curvature estimation is important to the diagnosis and treatment of the scoliosis. Existing methods face several issues such as the need of expensive annotations on the vertebral landmarks and being sensitive to the image quality. It is challenging to achieve robust estimation and obtain interpretable results, especially for low-quality images which are blurry and hazy. In this paper, we propose B-Spine, a novel deep learning pipeline to learn B-spline curve representation of the spine and estimate the Cobb angles for spinal curvature estimation from low-quality X-ray images. Given a low-quality input, a novel SegRefine network which employs the unpaired image-to-image translation is proposed to generate a high quality spine mask from the initial segmentation result. Next, a novel mask-based B-spline prediction model is proposed to predict the B-spline curve for the spine centerline. Finally, the Cobb angles are estimated by a hybrid approach which combines the curve slope analysis and a curve-based regression model. We conduct quantitative and qualitative comparisons with the representative and SOTA learning-based methods on the public AASCE2019 dataset and our new proposed CJUH-JLU dataset which contains more challenging low-quality images. The superior performance on both datasets shows our method can achieve both robustness and interpretability for spinal curvature estimation.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language English Simplified ChineseSpinal curvature estimation is important for the diagnosis and treatment of scoliosis. Existing methods have several issues, such as the need for expensive annotations on vertebral landmarks and being sensitive to image quality. It is challenging to achieve robust estimation and obtain interpretable results, especially for low-quality images that are blurry and hazy. In this paper, we propose B-Spine, a novel deep learning pipeline to learn B-spline curve representation of the spine and estimate the Cobb angles for spinal curvature estimation from low-quality X-ray images. Given a low-quality input, a novel SegRefine network which employs unpaired image-to-image translation is proposed to generate a high-quality spine mask from the initial segmentation result. Next, a novel mask-based B-spline prediction model is proposed to predict the B-spline curve for the spine centerline. Finally, the Cobb angles are estimated by a hybrid approach which combines curve slope analysis and a curve-based regression model. We conduct quantitative and qualitative comparisons with representative and SOTA learning-based methods on the public AASCE2019 dataset and our new proposed CJUH-JLU dataset, which contains more challenging low-quality images. The superior performance on both datasets shows that our method can achieve both robustness and interpretability for spinal curvature estimation.
</details></li>
</ul>
<hr>
<h2 id="Hawkeye-A-PyTorch-based-Library-for-Fine-Grained-Image-Recognition-with-Deep-Learning"><a href="#Hawkeye-A-PyTorch-based-Library-for-Fine-Grained-Image-Recognition-with-Deep-Learning" class="headerlink" title="Hawkeye: A PyTorch-based Library for Fine-Grained Image Recognition with Deep Learning"></a>Hawkeye: A PyTorch-based Library for Fine-Grained Image Recognition with Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09600">http://arxiv.org/abs/2310.09600</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hawkeye-finegrained/hawkeye">https://github.com/hawkeye-finegrained/hawkeye</a></li>
<li>paper_authors: Jiabei He, Yang Shen, Xiu-Shen Wei, Ye Wu</li>
<li>for: 这份研究是为了提供一个开源的 PyTorch 基础库，用于 Fine-Grained Image Recognition (FGIR) 任务。</li>
<li>methods: 这个库使用了深度学习的方法，并且具有 Modular 架构，以提高代码质量和人类可读配置。它还包含了 16 种现有的精细方法，覆盖 6 种不同的 paradigm，允许用户尝试不同的方法来解决 FGIR 任务。</li>
<li>results: 根据 authors 的声明，这个库是首个以 PyTorch 为基础的开源库，并且提供了一个全面的解决方案 для FGIR 任务。<details>
<summary>Abstract</summary>
Fine-Grained Image Recognition (FGIR) is a fundamental and challenging task in computer vision and multimedia that plays a crucial role in Intellectual Economy and Industrial Internet applications. However, the absence of a unified open-source software library covering various paradigms in FGIR poses a significant challenge for researchers and practitioners in the field. To address this gap, we present Hawkeye, a PyTorch-based library for FGIR with deep learning. Hawkeye is designed with a modular architecture, emphasizing high-quality code and human-readable configuration, providing a comprehensive solution for FGIR tasks. In Hawkeye, we have implemented 16 state-of-the-art fine-grained methods, covering 6 different paradigms, enabling users to explore various approaches for FGIR. To the best of our knowledge, Hawkeye represents the first open-source PyTorch-based library dedicated to FGIR. It is publicly available at https://github.com/Hawkeye-FineGrained/Hawkeye/, providing researchers and practitioners with a powerful tool to advance their research and development in the field of FGIR.
</details>
<details>
<summary>摘要</summary>
fine-grained 图像识别（FGIR）是计算机视觉和多媒体领域的基础和挑战性任务，对知识经济和工业互联网应用具有重要的作用。然而，无一个统一的开源软件库，覆盖了不同的FGIR Paradigma，对研究人员和实践者们带来了很大的挑战。为了解决这个问题，我们提出了Hawkeye，一个基于PyTorch的FGIR库。Hawkeye采用了模块化的架构，强调高质量的代码和人类可读的配置，为FGIR任务提供了全面的解决方案。在Hawkeye中，我们实现了16种当前顶尖的细化图像方法，覆盖了6个不同的Paradigma，使用户可以探索不同的FGIR方法。据我们所知，Hawkeye是首个基于PyTorch的开源FGIR库，可以在https://github.com/Hawkeye-FineGrained/Hawkeye/上获取。这将为研究人员和实践者们提供一个强大的工具，以推动FGIR领域的研究和开发。
</details></li>
</ul>
<hr>
<h2 id="Learning-Unified-Representations-for-Multi-Resolution-Face-Recognition"><a href="#Learning-Unified-Representations-for-Multi-Resolution-Face-Recognition" class="headerlink" title="Learning Unified Representations for Multi-Resolution Face Recognition"></a>Learning Unified Representations for Multi-Resolution Face Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09563">http://arxiv.org/abs/2310.09563</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stevensmith2000/btnet">https://github.com/stevensmith2000/btnet</a></li>
<li>paper_authors: Hulingxiao He, Wu Yuan, Yidian Huang, Shilong Zhao, Wen Yuan, Hanqing Li</li>
<li>for: 提高多分辨率脸Recognizer的表示学习方法</li>
<li>methods: 使用Branch-to-Trunk网络(BTNet)，包括一个统一Encoder（TNet）和多个分辨率Adapter（BNets），输入刚好与输出匹配，提高了微辨率脸的可识别度</li>
<li>results: 实验表明，BTNet可以在面Recognition benchmark上达到优秀表现，具有较少计算量和参数存储，并在QMUL-SurvFace 1: N face identification任务上创造新的状态机制。代码可以在<a target="_blank" rel="noopener" href="https://github.com/StevenSmith2000/BTNet%E4%B8%8A%E8%8E%B7%E5%8F%96">https://github.com/StevenSmith2000/BTNet上获取</a><details>
<summary>Abstract</summary>
In this work, we propose Branch-to-Trunk network (BTNet), a representation learning method for multi-resolution face recognition. It consists of a trunk network (TNet), namely a unified encoder, and multiple branch networks (BNets), namely resolution adapters. As per the input, a resolution-specific BNet is used and the output are implanted as feature maps in the feature pyramid of TNet, at a layer with the same resolution. The discriminability of tiny faces is significantly improved, as the interpolation error introduced by rescaling, especially up-sampling, is mitigated on the inputs. With branch distillation and backward-compatible training, BTNet transfers discriminative high-resolution information to multiple branches while guaranteeing representation compatibility. Our experiments demonstrate strong performance on face recognition benchmarks, both for multi-resolution identity matching and feature aggregation, with much less computation amount and parameter storage. We establish new state-of-the-art on the challenging QMUL-SurvFace 1: N face identification task. Our code is available at https://github.com/StevenSmith2000/BTNet.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们提出了分支到主干网络（BTNet），一种多resolution face recognition的表示学习方法。它包括一个主干网络（TNet），即统一编码器，以及多个分支网络（BNets），即分解器。根据输入，使用resolution-specific BNet，并将输出作为特征图层在TNet的特征峰中进行嵌入。这 mitigates the interpolation error introduced by rescaling, especially up-sampling, and significantly improves the discriminability of tiny faces. 通过分支涂抹和回传compatible训练，BTNet可以将高分辨率的特征信息传递给多个分支，同时保证表示相容性。我们的实验表明BTNet在多resolution identity matching和特征聚合任务上显示出了强大表现，减少了计算量和参数存储量。我们在QMUL-SurvFace 1: N face identification任务上创造了新的状态码，代码可以在https://github.com/StevenSmith2000/BTNet中获取。
</details></li>
</ul>
<hr>
<h2 id="Scene-Text-Recognition-Models-Explainability-Using-Local-Features"><a href="#Scene-Text-Recognition-Models-Explainability-Using-Local-Features" class="headerlink" title="Scene Text Recognition Models Explainability Using Local Features"></a>Scene Text Recognition Models Explainability Using Local Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09549">http://arxiv.org/abs/2310.09549</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/markytools/strexp">https://github.com/markytools/strexp</a></li>
<li>paper_authors: Mark Vincent Ty, Rowel Atienza</li>
<li>for: 这个论文主要研究的是Scene Text Recognition（STR）透明性（XAI），即如何使人们能够理解模型的预测结果的原因。</li>
<li>methods: 这篇论文使用了数据解释框架，即归因基本方法，来解释深度学习模型中的输入数据。然而，在STR中，这些方法仅仅能够提供全局性的解释，不能够准确地解释输入数据中的每个字符的预测结果。为了解决这个问题，这篇论文提出了一种新的方法，即STRExp，可以考虑本地解释，即每个字符预测结果的解释。</li>
<li>results: 这篇论文对不同的STR模型和数据集进行了比较，并评估了不同的归因基本方法的效果。结果表明，STRExp可以提供更加精准和有用的解释，而且可以在不同的STR模型和数据集上进行广泛的应用。<details>
<summary>Abstract</summary>
Explainable AI (XAI) is the study on how humans can be able to understand the cause of a model's prediction. In this work, the problem of interest is Scene Text Recognition (STR) Explainability, using XAI to understand the cause of an STR model's prediction. Recent XAI literatures on STR only provide a simple analysis and do not fully explore other XAI methods. In this study, we specifically work on data explainability frameworks, called attribution-based methods, that explain the important parts of an input data in deep learning models. However, integrating them into STR produces inconsistent and ineffective explanations, because they only explain the model in the global context. To solve this problem, we propose a new method, STRExp, to take into consideration the local explanations, i.e. the individual character prediction explanations. This is then benchmarked across different attribution-based methods on different STR datasets and evaluated across different STR models.
</details>
<details>
<summary>摘要</summary>
什么是可解释AI（XAI）？XAI是研究如何让人们理解模型预测的原因的学科。在这项工作中，我们的问题关注点是场景文本识别（STR）可解释，使用XAI来理解STR模型的预测。现有XAI литера图书籍中只提供了简单的分析，没有充分探讨其他XAI方法。在这项研究中，我们专门关注深度学习模型中的数据解释框架，即归因基本方法，可以解释输入数据中重要的部分。但是，将其集成到STR中会导致不一致和不效的解释，因为它们只能解释模型在全局上。为解决这个问题，我们提出了一种新方法，STRExp，可以考虑本地解释，即个体字符预测解释。这种方法然后在不同的归因基本方法和不同的STR数据集上进行了比较。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-the-Sim-to-Real-Gap-in-Cloth-Manipulation"><a href="#Benchmarking-the-Sim-to-Real-Gap-in-Cloth-Manipulation" class="headerlink" title="Benchmarking the Sim-to-Real Gap in Cloth Manipulation"></a>Benchmarking the Sim-to-Real Gap in Cloth Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09543">http://arxiv.org/abs/2310.09543</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Blanco-Mulero, Oriol Barbany, Gokhan Alcan, Adrià Colomé, Carme Torras, Ville Kyrki</li>
<li>for: 这篇论文旨在评估现实 físic engine 是如何帮助学习柔体物体的扭曲和位移的。</li>
<li>methods: 这篇论文使用了四种流行的柔体物体模拟器：MuJoCo、Bullet、Flex 和 SOFA，并对它们进行评估。</li>
<li>results: 论文提供了一个开源的测试集，用于评估这些模拟器的实际准确性、计算时间和稳定性。<details>
<summary>Abstract</summary>
Realistic physics engines play a crucial role for learning to manipulate deformable objects such as garments in simulation. By doing so, researchers can circumvent challenges such as sensing the deformation of the object in the real-world. In spite of the extensive use of simulations for this task, few works have evaluated the reality gap between deformable object simulators and real-world data. We present a benchmark dataset to evaluate the sim-to-real gap in cloth manipulation. The dataset is collected by performing a dynamic cloth manipulation task involving contact with a rigid table. We use the dataset to evaluate the reality gap, computational time, and simulation stability of four popular deformable object simulators: MuJoCo, Bullet, Flex, and SOFA. Additionally, we discuss the benefits and drawbacks of each simulator. The benchmark dataset is open-source. Supplementary material, videos, and code, can be found at https://sites.google.com/view/cloth-sim2real-benchmark.
</details>
<details>
<summary>摘要</summary>
现实 física 引擎在学习处理可变形 объек 的 simulate 中发挥关键作用。通过这样做，研究人员可以远离实际世界中感知对象的变形的挑战。尽管对这种任务的 simulate 广泛使用，但有少数作品评估了 sim-to-real 间的差距。我们提供了一个标准化数据集来评估 cloth  manipulate 中的 sim-to-real 差距。数据集是通过对固定桌子进行动态 cloth  manipulate 任务来收集的。我们使用该数据集来评估 simulator 的 reality gap，计算时间以及模拟稳定性。此外，我们还讨论了每个 simulator 的优缺点。标准化数据集开源，补充材料、视频和代码可以在https://sites.google.com/view/cloth-sim2real-benchmark 找到。
</details></li>
</ul>
<hr>
<h2 id="Towards-End-to-End-Unsupervised-Saliency-Detection-with-Self-Supervised-Top-Down-Context"><a href="#Towards-End-to-End-Unsupervised-Saliency-Detection-with-Self-Supervised-Top-Down-Context" class="headerlink" title="Towards End-to-End Unsupervised Saliency Detection with Self-Supervised Top-Down Context"></a>Towards End-to-End Unsupervised Saliency Detection with Self-Supervised Top-Down Context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09533">http://arxiv.org/abs/2310.09533</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yicheng Song, Shuyong Gao, Haozhe Xing, Yiting Cheng, Yan Wang, Wenqiang Zhang</li>
<li>for: 本研究旨在提高无监督聚合物 objet detection 的训练效率，并且可以 mines 深度特征中的rich semantic信息。</li>
<li>methods: 我们提出了一种自动supervised end-to-end salient object detection框架，通过上述的top-downcontext来学习最有助于性的分割指导。</li>
<li>results: 我们的方法在 benchmark 数据集上进行了广泛的实验，并证明了与最近的终端方法和多stage方法相比，我们的方法可以达到最高的性能。<details>
<summary>Abstract</summary>
Unsupervised salient object detection aims to detect salient objects without using supervision signals eliminating the tedious task of manually labeling salient objects. To improve training efficiency, end-to-end methods for USOD have been proposed as a promising alternative. However, current solutions rely heavily on noisy handcraft labels and fail to mine rich semantic information from deep features. In this paper, we propose a self-supervised end-to-end salient object detection framework via top-down context. Specifically, motivated by contrastive learning, we exploit the self-localization from the deepest feature to construct the location maps which are then leveraged to learn the most instructive segmentation guidance. Further considering the lack of detailed information in deepest features, we exploit the detail-boosting refiner module to enrich the location labels with details. Moreover, we observe that due to lack of supervision, current unsupervised saliency models tend to detect non-salient objects that are salient in some other samples of corresponding scenarios. To address this widespread issue, we design a novel Unsupervised Non-Salient Suppression (UNSS) method developing the ability to ignore non-salient objects. Extensive experiments on benchmark datasets demonstrate that our method achieves leading performance among the recent end-to-end methods and most of the multi-stage solutions. The code is available.
</details>
<details>
<summary>摘要</summary>
<<SYS>>这是一个使用简化中文的文本：不监督焦点检测目标检测焦点物件的存在，而不需要手动标注焦点物件。为了提高训练效率，终端方法 для USOD 已经被提议为可靠的替代方案。然而，目前的解决方案将重要的 semantic information 从深度特征中挖掘出来，导致训练效率低下。在这篇文章中，我们提出一个自动监督终端焦点检测框架，通过上下文的对应来学习焦点检测。具体来说，我们灵感自适应学习，从深度特征中找出最有价的位置对，然后将其用于学习最有价的分割引导。此外，因为深度特征中缺乏细节信息，我们运用细节增强修正模组来补充位置标签中的细节信息。此外，我们发现现有的不监督焦点模型往往对于某些相似的场景中的非焦点物件进行检测，这是一个广泛的问题。为了解决这个问题，我们提出了一个名为Unsupervised Non-Salient Suppression（UNSS）的新方法，可以将非焦点物件忽略掉。实验结果显示，我们的方法在最近的终端方法和多阶段解决方案中具有领先的表现。代码可以在网上获取。
</details></li>
</ul>
<hr>
<h2 id="TS-ENAS-Two-Stage-Evolution-for-Cell-based-Network-Architecture-Search"><a href="#TS-ENAS-Two-Stage-Evolution-for-Cell-based-Network-Architecture-Search" class="headerlink" title="TS-ENAS:Two-Stage Evolution for Cell-based Network Architecture Search"></a>TS-ENAS:Two-Stage Evolution for Cell-based Network Architecture Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09525">http://arxiv.org/abs/2310.09525</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juan Zou, Shenghong Wu, Yizhang Xia, Weiwei Jiang, Zeping Wu, Jinhua Zheng</li>
<li>for: 本研究提出了一种Two-Stage Evolution for cell-based Network Architecture Search（TS-ENAS）算法，用于自动设计神经网络结构。</li>
<li>methods: 该算法使用一个一阶搜索和一个二阶搜索两个阶段来搜索神经网络结构。在第一阶段，使用堆栈Cell进行搜索，以减少搜索的复杂性。在第二阶段，对这些Cell进行调整。</li>
<li>results: 在四个图像分类 dataset（Fashion-MNIST、CIFAR10、CIFAR100和ImageNet）上进行了广泛的测试和比较，并与22种现有的算法进行了比较，包括手动设计的网络和NAS网络。结果表明，TS-ENAS可以更有效地找到与其他算法相对性能的神经网络结构。<details>
<summary>Abstract</summary>
Neural network architecture search provides a solution to the automatic design of network structures. However, it is difficult to search the whole network architecture directly. Although using stacked cells to search neural network architectures is an effective way to reduce the complexity of searching, these methods do not able find the global optimal neural network structure since the number of layers, cells and connection methods is fixed. In this paper, we propose a Two-Stage Evolution for cell-based Network Architecture Search(TS-ENAS), including one-stage searching based on stacked cells and second-stage adjusting these cells. In our algorithm, a new cell-based search space and an effective two-stage encoding method are designed to represent cells and neural network structures. In addition, a cell-based weight inheritance strategy is designed to initialize the weight of the network, which significantly reduces the running time of the algorithm. The proposed methods are extensively tested and compared on four image classification dataset, Fashion-MNIST, CIFAR10, CIFAR100 and ImageNet and compared with 22 state-of-the-art algorithms including hand-designed networks and NAS networks. The experimental results show that TS-ENAS can more effectively find the neural network architecture with comparative performance.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:神经网络结构搜索提供了自动设计网络结构的解决方案。然而，直接搜索整个网络结构是困难的。尽管使用堆式细胞来搜索神经网络结构是有效的方法来减少搜索的复杂性，但这些方法无法找到全球优化的神经网络结构，因为层数、细胞数和连接方法的数量是固定的。在这篇论文中，我们提出了一种两个阶段演化的细胞基于网络 architecture搜索算法（TS-ENAS），包括一个阶段是基于堆式细胞的搜索，以及第二阶段是调整这些细胞。在我们的算法中，我们设计了一个新的细胞基于搜索空间和一种有效的两阶段编码方法来表示细胞和神经网络结构。此外，我们还设计了基于细胞的初始化策略来初始化网络的权重，这有效减少了算法的运行时间。我们在四个图像分类数据集（Fashion-MNIST、CIFAR10、CIFAR100和ImageNet）进行了广泛的测试和比较，与22种现有的算法（包括手动设计的网络和NAS网络）进行了比较。实验结果表明，TS-ENAS可以更有效地找到与其他方法相当的神经网络结构。
</details></li>
</ul>
<hr>
<h2 id="OBSUM-An-object-based-spatial-unmixing-model-for-spatiotemporal-fusion-of-remote-sensing-images"><a href="#OBSUM-An-object-based-spatial-unmixing-model-for-spatiotemporal-fusion-of-remote-sensing-images" class="headerlink" title="OBSUM: An object-based spatial unmixing model for spatiotemporal fusion of remote sensing images"></a>OBSUM: An object-based spatial unmixing model for spatiotemporal fusion of remote sensing images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09517">http://arxiv.org/abs/2310.09517</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/houcaiguo/obsum-code">https://github.com/houcaiguo/obsum-code</a></li>
<li>paper_authors: Houcai Guo, Dingqi Ye, Lorenzo Bruzzone<br>for:这个论文旨在提高遥感图像的空间和时间分辨率，以便进行时间序列分析。methods:这个研究提出了一种基于物体分析和空间分解的物体基于空间混合模型（OBSUM），以解决当前遥感时间序列融合方法中的两个重要问题。OBSUM包括一个预处理步骤和三个融合步骤，即物体水平减混、物体水平偏差补偿和像素水平偏差补偿。results:对比五种代表性的遥感时间序列融合方法，OBSUM在准确指标和视觉效果上表现出色，并且在两个典型的遥感应用中也达到了满意的结果。因此，OBSUM有很大的应用潜力，可以生成高分辨率和准确的时间序列观测数据，以支持多种遥感应用。<details>
<summary>Abstract</summary>
Spatiotemporal fusion aims to improve both the spatial and temporal resolution of remote sensing images, thus facilitating time-series analysis at a fine spatial scale. However, there are several important issues that limit the application of current spatiotemporal fusion methods. First, most spatiotemporal fusion methods are based on pixel-level computation, which neglects the valuable object-level information of the land surface. Moreover, many existing methods cannot accurately retrieve strong temporal changes between the available high-resolution image at base date and the predicted one. This study proposes an Object-Based Spatial Unmixing Model (OBSUM), which incorporates object-based image analysis and spatial unmixing, to overcome the two abovementioned problems. OBSUM consists of one preprocessing step and three fusion steps, i.e., object-level unmixing, object-level residual compensation, and pixel-level residual compensation. OBSUM can be applied using only one fine image at the base date and one coarse image at the prediction date, without the need of a coarse image at the base date. The performance of OBSUM was compared with five representative spatiotemporal fusion methods. The experimental results demonstrated that OBSUM outperformed other methods in terms of both accuracy indices and visual effects over time-series. Furthermore, OBSUM also achieved satisfactory results in two typical remote sensing applications. Therefore, it has great potential to generate accurate and high-resolution time-series observations for supporting various remote sensing applications.
</details>
<details>
<summary>摘要</summary>
<<SYS>>这个研究旨在提高遥感图像的空间和时间解析精度，以便进行时间序列分析，并且解决现有的一些重要问题。首先，大多数的遥感融合方法是基于像素级计算，忽略了地表上的有价值物体信息。其次，许多现有的方法无法精确地回传具有强时间变化的高分辨率图像。本研究提出了一个物体基本分析和空间混合模型（OBSUM），以解决上述问题。OBSUM包括一个预processing步骤和三个融合步骤，即物体水平混合、物体水平剩余补偿和像素水平剩余补偿。OBSUM可以使用仅有一个精细图像的基本日期，并且不需要基本日期的粗糙图像。实验结果显示，OBSUM在精度指标和视觉效果上优于五种代表性的遥感融合方法。此外，OBSUM也在两个典型的遥感应用中取得了满意的结果。因此，它具有高精度时间序列观测的可能性。
</details></li>
</ul>
<hr>
<h2 id="Foundation-Ark-Accruing-and-Reusing-Knowledge-for-Superior-and-Robust-Performance"><a href="#Foundation-Ark-Accruing-and-Reusing-Knowledge-for-Superior-and-Robust-Performance" class="headerlink" title="Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance"></a>Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09507">http://arxiv.org/abs/2310.09507</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jlianglab/ark">https://github.com/jlianglab/ark</a></li>
<li>paper_authors: DongAo Ma, Jiaxuan Pang, Michael B. Gotway, Jianming Liang<br>for: 本研究旨在开发一个可 powerful和Robust的基础模型，通过聚合多个小型公共数据集来实现。methods: 我们提出了 Ark 框架，用于聚合和重用多个不同专家标注的数据集中的知识。results: 我们通过在多种成像任务中进行精度调整、线性检测和性别偏见分析，证明了 Ark 模型在比特aha fully&#x2F;self-supervised baselines和 Google 的专有 CXR-FM 模型之上具有显著的超越性和Robustness。<details>
<summary>Abstract</summary>
Deep learning nowadays offers expert-level and sometimes even super-expert-level performance, but achieving such performance demands massive annotated data for training (e.g., Google's proprietary CXR Foundation Model (CXR-FM) was trained on 821,544 labeled and mostly private chest X-rays (CXRs)). Numerous datasets are publicly available in medical imaging but individually small and heterogeneous in expert labels. We envision a powerful and robust foundation model that can be trained by aggregating numerous small public datasets. To realize this vision, we have developed Ark, a framework that accrues and reuses knowledge from heterogeneous expert annotations in various datasets. As a proof of concept, we have trained two Ark models on 335,484 and 704,363 CXRs, respectively, by merging several datasets including ChestX-ray14, CheXpert, MIMIC-II, and VinDr-CXR, evaluated them on a wide range of imaging tasks covering both classification and segmentation via fine-tuning, linear-probing, and gender-bias analysis, and demonstrated our Ark's superior and robust performance over the SOTA fully/self-supervised baselines and Google's proprietary CXR-FM. This enhanced performance is attributed to our simple yet powerful observation that aggregating numerous public datasets diversifies patient populations and accrues knowledge from diverse experts, yielding unprecedented performance yet saving annotation cost. With all codes and pretrained models released at GitHub.com/JLiangLab/Ark, we hope that Ark exerts an important impact on open science, as accruing and reusing knowledge from expert annotations in public datasets can potentially surpass the performance of proprietary models trained on unusually large data, inspiring many more researchers worldwide to share codes and datasets to build open foundation models, accelerate open science, and democratize deep learning for medical imaging.
</details>
<details>
<summary>摘要</summary>
现在的深度学习技术已经可以达到专家级或者超过专家级的性能，但是获得这样的性能需要巨量的标注数据进行训练（如Google的专有CXR基础模型（CXR-FM）在821544个标注的和大多数私人胸部X射线图像（CXRs）上进行训练）。医疗影像领域有很多公共数据集，但每个数据集都是小型和多样化的专家标注。我们的愿景是建立一个强大和稳定的基础模型，可以通过合并各种小型公共数据集来训练。为了实现这个愿景，我们已经开发了Ark框架，它可以聚合和重用各种各样的专家标注知识。作为证明，我们已经在335484和704363个CXRs上分别训练了两个Ark模型，并通过精细调整、直接检测和性别偏见分析来评估其性能。我们的Ark模型在覆盖各种影像任务，包括分类和分割，并表现出超过当前最佳自动化/自我超vised基线和Google专有CXR-FM的性能。这种提高的性能归因于我们简单 yet 强大的观察，即合并各种公共数据集可以多样化病人人口和收集专家知识，从而实现无 precedent的性能，同时降低标注成本。我们在GitHub上发布了所有代码和预训练模型，我们希望Ark能够对开源科学产生重要影响，因为聚合和重用公共数据集中的专家标注知识可能会超越 propriety模型在非常大数据上的性能，鼓励更多的研究人员在世界各地分享代码和数据集，加速开源科学，和democratize深度学习 для医疗影像领域。
</details></li>
</ul>
<hr>
<h2 id="JM3D-JM3D-LLM-Elevating-3D-Representation-with-Joint-Multi-modal-Cues"><a href="#JM3D-JM3D-LLM-Elevating-3D-Representation-with-Joint-Multi-modal-Cues" class="headerlink" title="JM3D &amp; JM3D-LLM: Elevating 3D Representation with Joint Multi-modal Cues"></a>JM3D &amp; JM3D-LLM: Elevating 3D Representation with Joint Multi-modal Cues</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09503">http://arxiv.org/abs/2310.09503</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mr-neko/jm3d">https://github.com/mr-neko/jm3d</a></li>
<li>paper_authors: Jiayi Ji, Haowei Wang, Changli Wu, Yiwei Ma, Xiaoshuai Sun, Rongrong Ji</li>
<li>for: 本研究旨在解决3D表示学中的三大挑战，包括信息损失、缺乏同步和不充分利用细节信息。</li>
<li>methods: 该研究提出了一种全面的JM3D方法，包括多视图图像和层次文本的结构多模式组织器（SMO）和语言理解与视觉表示的共同对齐（JMA）。</li>
<li>results: 研究表明，JM3D-LLM模型在ModelNet40和ScanObjectNN测试集上显示出了superiority，并且JM3D-LLM模型通过有效的微调来结合3D表示和大语言模型。<details>
<summary>Abstract</summary>
The rising importance of 3D representation learning, pivotal in computer vision, autonomous driving, and robotics, is evident. However, a prevailing trend, which straightforwardly resorted to transferring 2D alignment strategies to the 3D domain, encounters three distinct challenges: (1) Information Degradation: This arises from the alignment of 3D data with mere single-view 2D images and generic texts, neglecting the need for multi-view images and detailed subcategory texts. (2) Insufficient Synergy: These strategies align 3D representations to image and text features individually, hampering the overall optimization for 3D models. (3) Underutilization: The fine-grained information inherent in the learned representations is often not fully exploited, indicating a potential loss in detail. To address these issues, we introduce JM3D, a comprehensive approach integrating point cloud, text, and image. Key contributions include the Structured Multimodal Organizer (SMO), enriching vision-language representation with multiple views and hierarchical text, and the Joint Multi-modal Alignment (JMA), combining language understanding with visual representation. Our advanced model, JM3D-LLM, marries 3D representation with large language models via efficient fine-tuning. Evaluations on ModelNet40 and ScanObjectNN establish JM3D's superiority. The superior performance of JM3D-LLM further underscores the effectiveness of our representation transfer approach. Our code and models are available at https://github.com/Mr-Neko/JM3D.
</details>
<details>
<summary>摘要</summary>
随着三维表示学的崛起，在计算机视觉、自动驾驶和 роботиCS中发挥着重要作用。然而，一种流行的趋势是将二维对对策直接应用到三维领域，这种方法存在三个突出的挑战：（1）信息强化：这种方法将三维数据与单个视图二维图像和普通的文本进行对齐，忽略了多视图图像和详细的子类别文本的需求。（2）不足的共同作用：这些策略将三维表示与图像和文本特征进行对齐，阻碍整体优化三维模型。（3）不充分利用：学习的表示中具有细节的信息经常未被完全利用，表明有可能的误差。为了解决这些问题，我们介绍了JM3D，一种全面的方法，它将点云、文本和图像集成起来。JM3D的关键贡献包括多视图文本结构（SMO），它使用多个视图和层次文本来增强视语言表示，以及对语言理解与视觉表示的共同对齐（JMA）。我们的高级模型JM3D-LLM通过有效的微调来结合语言模型和三维表示。我们在ModelNet40和ScanObjectNN上进行了评估，结果表明JM3D的超越性。JM3D-LLM的高性能进一步证明了我们的表示传递方法的有效性。我们的代码和模型可以在https://github.com/Mr-Neko/JM3D中找到。
</details></li>
</ul>
<hr>
<h2 id="Learning-In-between-Imagery-Dynamics-via-Physical-Latent-Spaces"><a href="#Learning-In-between-Imagery-Dynamics-via-Physical-Latent-Spaces" class="headerlink" title="Learning In-between Imagery Dynamics via Physical Latent Spaces"></a>Learning In-between Imagery Dynamics via Physical Latent Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09495">http://arxiv.org/abs/2310.09495</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jihun Han, Yoonsang Lee, Anne Gelb</li>
<li>for: 学习两个图像在连续时间步骤中的下一个图像的动态关系</li>
<li>methods: 利用含有物理模型表示的partial differential equations（PDEs）来估计图像的中间阶段，并保持图像空间相关性</li>
<li>results: 通过数字测试使用地球科学图像数据，证明方法的稳定性和有效性<details>
<summary>Abstract</summary>
We present a framework designed to learn the underlying dynamics between two images observed at consecutive time steps. The complex nature of image data and the lack of temporal information pose significant challenges in capturing the unique evolving patterns. Our proposed method focuses on estimating the intermediary stages of image evolution, allowing for interpretability through latent dynamics while preserving spatial correlations with the image. By incorporating a latent variable that follows a physical model expressed in partial differential equations (PDEs), our approach ensures the interpretability of the learned model and provides insight into corresponding image dynamics. We demonstrate the robustness and effectiveness of our learning framework through a series of numerical tests using geoscientific imagery data.
</details>
<details>
<summary>摘要</summary>
我们提出了一种框架，用于学习两个图像在连续时间步骤中的下面动力。图像数据的复杂性和缺乏时间信息使得捕捉唯一发展模式具有挑战性。我们的提议是估算图像演化过程中的中间阶段，以保持图像空间相关性，并通过潜在动力提供可读性。我们的方法包括一个遵循部分偏微分方程（PDE）的潜在变量，以保证学习模型的可读性并提供相应的图像动力的理解。我们通过对地球科学图像数据进行数值测试，证明了我们的学习框架的稳定性和效果。
</details></li>
</ul>
<hr>
<h2 id="Perception-Reinforcement-Using-Auxiliary-Learning-Feature-Fusion-A-Modified-Yolov8-for-Head-Detection"><a href="#Perception-Reinforcement-Using-Auxiliary-Learning-Feature-Fusion-A-Modified-Yolov8-for-Head-Detection" class="headerlink" title="Perception Reinforcement Using Auxiliary Learning Feature Fusion: A Modified Yolov8 for Head Detection"></a>Perception Reinforcement Using Auxiliary Learning Feature Fusion: A Modified Yolov8 for Head Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09492">http://arxiv.org/abs/2310.09492</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiezhou Chen, Guankun Wang, Weixiang Liu, Xiaopin Zhong, Yibin Tian, ZongZe Wu</li>
<li>for: 提高人头检测精度和 robustness</li>
<li>methods: 使用改进版 Yolov8，增加 auxillary 学习特征拟合（ALFF）模块和 Distribution Focal Loss 等技术来提高目标感知和检测精度</li>
<li>results: 实验结果表明我们的方法可以提高人头检测精度和 robustness，并且在不同的背景和照明条件下都有优秀的表现<details>
<summary>Abstract</summary>
Head detection provides distribution information of pedestrian, which is crucial for scene statistical analysis, traffic management, and risk assessment and early warning. However, scene complexity and large-scale variation in the real world make accurate detection more difficult. Therefore, we present a modified Yolov8 which improves head detection performance through reinforcing target perception. An Auxiliary Learning Feature Fusion (ALFF) module comprised of LSTM and convolutional blocks is used as the auxiliary task to help the model perceive targets. In addition, we introduce Noise Calibration into Distribution Focal Loss to facilitate model fitting and improve the accuracy of detection. Considering the requirements of high accuracy and speed for the head detection task, our method is adapted with two kinds of backbone, namely Yolov8n and Yolov8m. The results demonstrate the superior performance of our approach in improving detection accuracy and robustness.
</details>
<details>
<summary>摘要</summary>
《头部检测提供了人行道用户分布信息，这对Scene统计分析、交通管理和风险评估预警都是关键。然而，实际世界中的场景复杂度和大规模变化使得准确检测更加困难。因此，我们提出了一种基于Yolov8的修改方法，通过强化目标感知来提高头部检测性能。我们使用一个名为ALFF（auxiliary learning feature fusion）模块，其包括LSTM和卷积块，作为辅助任务，帮助模型更好地感知目标。此外，我们还引入了降噪约束分布损失，以便提高模型的适应性和准确性。考虑到头部检测任务的高精度和速度要求，我们采用了Yolov8n和Yolov8m两种骨干。结果表明，我们的方法可以提高检测精度和Robustness。》Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Design-Space-of-Diffusion-Autoencoders-for-Face-Morphing"><a href="#Exploring-the-Design-Space-of-Diffusion-Autoencoders-for-Face-Morphing" class="headerlink" title="Exploring the Design Space of Diffusion Autoencoders for Face Morphing"></a>Exploring the Design Space of Diffusion Autoencoders for Face Morphing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09484">http://arxiv.org/abs/2310.09484</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zander Blasingame, Chen Liu</li>
<li>for: 本研究探索了 diffusion autoencoders 创造的 face morph 设计空间，特别是 sampling algorithms、reverse DDIM solver 和 partial sampling through small amounts of added noise 等三个轴。</li>
<li>methods: 本研究使用了 sampling algorithms、reverse DDIM solver 和 partial sampling through small amounts of added noise 等方法。</li>
<li>results: 研究发现，采用不同的 sampling algorithms、reverse DDIM solver 和 partial sampling through small amounts of added noise 可以创造出不同的 face morph。<details>
<summary>Abstract</summary>
Face morphs created by Diffusion Autoencoders are a recent innovation and the design space of such an approach has not been well explored. We explore three axes of the design space, i.e., 1) sampling algorithms, 2) the reverse DDIM solver, and 3) partial sampling through small amounts of added noise.
</details>
<details>
<summary>摘要</summary>
Diffusion Autoencoders 创造的面部变换是最近的创新，这个设计空间的探索尚未充分。我们探索了三个轴，即：1. 采样算法2. 反向 DDIM 解决方案3. 通过小量附加的噪声进行部分采样Here's a breakdown of the translation:1. "Diffusion Autoencoders" (Diffusion Autoencoders) - 散度自适应器2. "创造的面部变换" (face morphs created) - 面部变换 (face morphs)3. "是最近的创新" (is a recent innovation) - 最近的创新 (recent innovation)4. "设计空间" (design space) - 设计空间 (design space)5. "尚未充分" (has not been well explored) - 尚未充分 (has not been well explored)6. "我们探索了三个轴" (we explore three axes) - 我们探索了三个轴 (we explore three axes)7. "即：" (i.e.,) - 即： (i.e.,)8. "采样算法" (sampling algorithms) - 采样算法 (sampling algorithms)9. "反向 DDIM 解决方案" (reverse DDIM solver) - 反向 DDIM 解决方案 (reverse DDIM solver)10. "通过小量附加的噪声进行部分采样" (partial sampling through small amounts of added noise) - 通过小量附加的噪声进行部分采样 (partial sampling through small amounts of added noise)
</details></li>
</ul>
<hr>
<h2 id="MiniGPT-v2-large-language-model-as-a-unified-interface-for-vision-language-multi-task-learning"><a href="#MiniGPT-v2-large-language-model-as-a-unified-interface-for-vision-language-multi-task-learning" class="headerlink" title="MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning"></a>MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09478">http://arxiv.org/abs/2310.09478</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, Mohamed Elhoseiny</li>
<li>for: 这篇论文的目的是建立一个统一的界面，用于完成多种语言领域的应用，包括图像描述、视觉问题回答和视觉基础设定等。</li>
<li>methods: 这篇论文使用了MiniGPT-v2模型，这是一个可以作为多种视觉语言任务的统一界面。它使用唯一识别码来识别不同任务的训练指令，以提高模型对每个任务的学习效率。</li>
<li>results: 实验结果显示，MiniGPT-v2在视觉问题回答和视觉基础设定benchmark上表现强，与其他视觉语言通用模型相比。<details>
<summary>Abstract</summary>
Large language models have shown their remarkable capabilities as a general interface for various language-related applications. Motivated by this, we target to build a unified interface for completing many vision-language tasks including image description, visual question answering, and visual grounding, among others. The challenge is to use a single model for performing diverse vision-language tasks effectively with simple multi-modal instructions. Towards this objective, we introduce MiniGPT-v2, a model that can be treated as a unified interface for better handling various vision-language tasks. We propose using unique identifiers for different tasks when training the model. These identifiers enable our model to better distinguish each task instruction effortlessly and also improve the model learning efficiency for each task. After the three-stage training, the experimental results show that MiniGPT-v2 achieves strong performance on many visual question-answering and visual grounding benchmarks compared to other vision-language generalist models. Our model and codes are available at https://minigpt-v2.github.io/
</details>
<details>
<summary>摘要</summary>
大型语言模型在各种语言相关应用中展示了其权威的能力。这为我们提供了动机，我们想要建立一个统一的界面，以便处理多种视觉语言任务，包括图像描述、视觉问题回答和视觉定位等等。挑战是使用单一模型来进行多种视觉语言任务，并且将其训练为每个任务。为了解决这个问题，我们引入了MiniGPT-v2模型，它可以作为视觉语言任务的统一界面。我们在训练过程中使用对应的唯一识别码，以便让模型更容易识别每个任务的指令，并且提高模型对每个任务的学习效率。经过三阶段训练后，我们的实验结果显示，MiniGPT-v2在许多视觉问题回答和视觉定位benchmark上表现出色，与其他视觉语言通用模型相比。我们的模型和代码可以在https://minigpt-v2.github.io/上取得。
</details></li>
</ul>
<hr>
<h2 id="Plug-and-Play-Feature-Generation-for-Few-Shot-Medical-Image-Classification"><a href="#Plug-and-Play-Feature-Generation-for-Few-Shot-Medical-Image-Classification" class="headerlink" title="Plug-and-Play Feature Generation for Few-Shot Medical Image Classification"></a>Plug-and-Play Feature Generation for Few-Shot Medical Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09471">http://arxiv.org/abs/2310.09471</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qianyu Guo, Huifang Du, Xing Jia, Shuyong Gao, Yan Teng, Haofen Wang, Wenqiang Zhang</li>
<li>for: 提高医学图像分类模型的普适性和实用性，使用有限数量的训练数据。</li>
<li>methods: 提出了一种名为MedMFG的灵活和轻量级的插件和撑杆方法，可以生成具有足够特征的类别特征。</li>
<li>results: 在跨域分数标准上对比多个基eline和后处理方法，MedMFG达到了10%以上的性能提升，并且可以轻松地与多种背景和基eline整合。<details>
<summary>Abstract</summary>
Few-shot learning (FSL) presents immense potential in enhancing model generalization and practicality for medical image classification with limited training data; however, it still faces the challenge of severe overfitting in classifier training due to distribution bias caused by the scarce training samples. To address the issue, we propose MedMFG, a flexible and lightweight plug-and-play method designed to generate sufficient class-distinctive features from limited samples. Specifically, MedMFG first re-represents the limited prototypes to assign higher weights for more important information features. Then, the prototypes are variationally generated into abundant effective features. Finally, the generated features and prototypes are together to train a more generalized classifier. Experiments demonstrate that MedMFG outperforms the previous state-of-the-art methods on cross-domain benchmarks involving the transition from natural images to medical images, as well as medical images with different lesions. Notably, our method achieves over 10% performance improvement compared to several baselines. Fusion experiments further validate the adaptability of MedMFG, as it seamlessly integrates into various backbones and baselines, consistently yielding improvements of over 2.9% across all results.
</details>
<details>
<summary>摘要</summary>
几个示例学习（Few-shot learning，FSL）具有极大的潜力提高模型通用性和实用性，尤其是在医疗图像分类中使用有限训练数据。然而，FSL仍然面临分布偏见导致分类器训练过程中严重溢出的问题。为解决这问题，我们提出MedMFG，一种灵活且轻量级的插件式方法，可以生成充足的类别特征从有限样本中。具体来说，MedMFG首先重新表示有限原型，以便将更重要的信息特征赋予更高的权重。然后，原型通过变换生成成为丰富的有效特征。最后，生成的特征和原型一起训练一个更通用的分类器。实验表明，MedMFG在跨领域 benchmark 上比前一些基eline方法提高了10%以上的性能。此外，我们的方法在不同的背景和基eline上进行融合实验， consistently 获得了2.9%以上的提高。
</details></li>
</ul>
<hr>
<h2 id="Towards-More-Accurate-Diffusion-Model-Acceleration-with-A-Timestep-Aligner"><a href="#Towards-More-Accurate-Diffusion-Model-Acceleration-with-A-Timestep-Aligner" class="headerlink" title="Towards More Accurate Diffusion Model Acceleration with A Timestep Aligner"></a>Towards More Accurate Diffusion Model Acceleration with A Timestep Aligner</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09469">http://arxiv.org/abs/2310.09469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengfei Xia, Yujun Shen, Changsong Lei, Yu Zhou, Ran Yi, Deli Zhao, Wenping Wang, Yong-jin Liu</li>
<li>for: 提高Diffusion Model的推理速度，解决现有加速方法的性能下降问题。</li>
<li>methods: 通过视为离散积分过程，找到更加准确的积分方向，以提高推理性能。具体来说，在每个净化步骤中，将原始参数化被取代，并在新的步骤中 conditioning 网络，以使其更加准确地描述真实分布。</li>
<li>results: 广泛的实验表明，我们的插件设计可以高效地训练，并在多种现有加速方法中提高推理性能，特别是当有少量净化步骤时。例如，在使用10个净化步骤的LSUN Bedroom dataset上，我们可以将DDIM的FID从9.65降低到6.07，只需采用我们的方法。代码将公开发布。<details>
<summary>Abstract</summary>
A diffusion model, which is formulated to produce an image using thousands of denoising steps, usually suffers from a slow inference speed. Existing acceleration algorithms simplify the sampling by skipping most steps yet exhibit considerable performance degradation. By viewing the generation of diffusion models as a discretized integrating process, we argue that the quality drop is partly caused by applying an inaccurate integral direction to a timestep interval. To rectify this issue, we propose a timestep aligner that helps find a more accurate integral direction for a particular interval at the minimum cost. Specifically, at each denoising step, we replace the original parameterization by conditioning the network on a new timestep, which is obtained by aligning the sampling distribution to the real distribution. Extensive experiments show that our plug-in design can be trained efficiently and boost the inference performance of various state-of-the-art acceleration methods, especially when there are few denoising steps. For example, when using 10 denoising steps on the popular LSUN Bedroom dataset, we improve the FID of DDIM from 9.65 to 6.07, simply by adopting our method for a more appropriate set of timesteps. Code will be made publicly available.
</details>
<details>
<summary>摘要</summary>
一种扩散模型，通过千次减噪步骤生成图像，通常受到慢速推理速度的限制。现有的加速算法简化抽取步骤，但显示出较大的性能下降。我们认为，生成扩散模型的过程可 viewed为离散 интегрирование过程，因此，生成图像质量下降的一部分原因是应用不正确的积分方向。为了解决这个问题，我们提议一种时间步骤对齐器，帮助找到更加准确的积分方向，最小化成本。具体来说，在每次减噪步骤中，我们将原始参数化替换为根据新的时间步骤 conditioning 网络。我们进行了广泛的实验，发现我们的插件设计可以高效地训练，并提高各种现有加速方法的推理性能，特别是当有少量减噪步骤时。例如，在使用10个减噪步骤的LSUN床间 dataset上，我们可以通过采用我们的方法，从9.65提高FID到6.07。我们将代码公开。
</details></li>
</ul>
<hr>
<h2 id="MAC-ModAlity-Calibration-for-Object-Detection"><a href="#MAC-ModAlity-Calibration-for-Object-Detection" class="headerlink" title="MAC: ModAlity Calibration for Object Detection"></a>MAC: ModAlity Calibration for Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09461">http://arxiv.org/abs/2310.09461</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yutian Lei, Jun Liu, Dong Huang</li>
<li>for: 本研究旨在开发一种能够快速和高效地将RGB输入模式转换为非RGB输入模式的方法，以便在不同的输入模式下进行物体检测。</li>
<li>methods: 本研究提出了一种名为ModAlity Calibration（MAC）的灵活管道，用于协调目标输入模式和源输入模式之间的差异。具体来说，我们在目标输入模式上额外添加了一个小准备模块，并对这个模块进行MAC训练技术的应用，以便在无需100%手动标注的情况下，使目标输入模式模型达到与基eline模型相同或更好的表现。</li>
<li>results: 我们通过对WiFi输入模式、Lidar输入模式和热成像输入模式等不同输入模式的模型进行组合，并将这些模型与预训练的RGB输入模式模型进行拟合，证明了MAC的效iveness。<details>
<summary>Abstract</summary>
The flourishing success of Deep Neural Networks(DNNs) on RGB-input perception tasks has opened unbounded possibilities for non-RGB-input perception tasks, such as object detection from wireless signals, lidar scans, and infrared images. Compared to the matured development pipeline of RGB-input (source modality) models, developing non-RGB-input (target-modality) models from scratch poses excessive challenges in the modality-specific network design/training tricks and labor in the target-modality annotation. In this paper, we propose ModAlity Calibration (MAC), an efficient pipeline for calibrating target-modality inputs to the DNN object detection models developed on the RGB (source) modality. We compose a target-modality-input model by adding a small calibrator module ahead of a source-modality model and introduce MAC training techniques to impose dense supervision on the calibrator. By leveraging (1) prior knowledge synthesized from the source-modality model and (2) paired {target, source} data with zero manual annotations, our target-modality models reach comparable or better metrics than baseline models that require 100% manual annotations. We demonstrate the effectiveness of MAC by composing the WiFi-input, Lidar-input, and Thermal-Infrared-input models upon the pre-trained RGB-input models respectively.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）在RGB输入感知任务上的繁荣成功打开了非RGB输入感知任务的无尽可能性，如从无线电信号、激光扫描和红外图像中的对象检测。相比于已经成熟的RGB输入（源模式）模型的开发管线，开发非RGB输入（目标模式）模型从零开始带来了过度的挑战，包括特性化网络设计/训练技巧和目标模式注解的劳动。在这篇论文中，我们提出了模态均衡（MAC）管线，用于均衡目标模式输入到基于RGB模式（源模式）的对象检测模型中。我们将目标模式输入模型的前置加一小均衡模块，并引入MAC训练技术，以在均衡模块中强制对均衡模块进行密集监督。通过利用（1）源模式模型中的优先知识和（2）paired {target, source} 数据集，我们的目标模式模型可以达到与基eline模型相同或更好的 metric，而不需要100%的手动注解。我们通过将WiFi输入、激光输入和红外温度输入模型分别建立在pre-trained RGB输入模型上，来证明MAC的效果。
</details></li>
</ul>
<hr>
<h2 id="PaintHuman-Towards-High-fidelity-Text-to-3D-Human-Texturing-via-Denoised-Score-Distillation"><a href="#PaintHuman-Towards-High-fidelity-Text-to-3D-Human-Texturing-via-Denoised-Score-Distillation" class="headerlink" title="PaintHuman: Towards High-fidelity Text-to-3D Human Texturing via Denoised Score Distillation"></a>PaintHuman: Towards High-fidelity Text-to-3D Human Texturing via Denoised Score Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09458">http://arxiv.org/abs/2310.09458</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianhui Yu, Hao Zhu, Liming Jiang, Chen Change Loy, Weidong Cai, Wayne Wu</li>
<li>for: 本研究旨在解决 zero-shot text-to-3D 人体生成中 SDS 方法可能提供不准确的梯度方向问题，以及高级度文本-to-3D 人体质量控制的挑战。</li>
<li>methods: 本研究提出了一种名为PaintHuman的模型，通过两种方法来解决问题：首先，引入了一种修改 SDS 的新得分函数（denoised score distillation，DSD），以 iteratively 更正梯度方向并生成高质量的Texture。其次，使用深度图作为geometry guidance，确保Texture具有人体模型表面的semantic alignment。</li>
<li>results: 对比 estado-of-the-art 方法，我们的方法在许多预测和评估任务上具有较高的性能和质量。<details>
<summary>Abstract</summary>
Recent advances in zero-shot text-to-3D human generation, which employ the human model prior (eg, SMPL) or Score Distillation Sampling (SDS) with pre-trained text-to-image diffusion models, have been groundbreaking. However, SDS may provide inaccurate gradient directions under the weak diffusion guidance, as it tends to produce over-smoothed results and generate body textures that are inconsistent with the detailed mesh geometry. Therefore, directly leverage existing strategies for high-fidelity text-to-3D human texturing is challenging. In this work, we propose a model called PaintHuman to addresses the challenges from two aspects. We first propose a novel score function, Denoised Score Distillation (DSD), which directly modifies the SDS by introducing negative gradient components to iteratively correct the gradient direction and generate high-quality textures. In addition, we use the depth map as a geometric guidance to ensure the texture is semantically aligned to human mesh surfaces. To guarantee the quality of rendered results, we employ geometry-aware networks to predict surface materials and render realistic human textures. Extensive experiments, benchmarked against state-of-the-art methods, validate the efficacy of our approach.
</details>
<details>
<summary>摘要</summary>
近期的零shot文本到3D人体生成技术发展，使用人体模型先验（如SMPL）或Score Distillation Sampling（SDS）与预训练文本到图像扩散模型，取得了重要进展。然而，SDS可能在弱扩散指导下提供不准确的梯度方向，因为它有可能生成过度平滑的结果和与细节 mesh geometry 不一致的人体 texture。因此，直接使用现有的高精度文本到3D人体纹理策略是挑战。在这种工作中，我们提议一种名为PaintHuman的模型，用以解决这些挑战。我们首先提出了一种新的分数函数，Denosied Score Distillation（DSD），它直接修改了 SDS，通过引入负梯度组分来逐次更正梯度方向，生成高质量的纹理。此外，我们使用深度图为 geometric 指导，确保纹理与人体 mesh 表面含义相对应。为保证渲染结果的质量，我们使用 geometry-aware 网络预测表面材质并生成真实的人体纹理。我们对 state-of-the-art 方法进行了广泛的实验，并证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="UCM-Net-A-Lightweight-and-Efficient-Solution-for-Skin-Lesion-Segmentation-using-MLP-and-CNN"><a href="#UCM-Net-A-Lightweight-and-Efficient-Solution-for-Skin-Lesion-Segmentation-using-MLP-and-CNN" class="headerlink" title="UCM-Net: A Lightweight and Efficient Solution for Skin Lesion Segmentation using MLP and CNN"></a>UCM-Net: A Lightweight and Efficient Solution for Skin Lesion Segmentation using MLP and CNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09457">http://arxiv.org/abs/2310.09457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chunyu Yuan, Dongfang Zhao, Sos S. Agaian<br>for:* 这个论文的目的是提出一种高效、轻量级的皮肤恶性肿瘤分割方法，以便在移动医疗应用中使用。methods:* 该方法使用了多层感知（MLP）和卷积神经网络（CNN）的组合，并提出了一种新的封装块（UCM-Net-Block）来减少参数的 overhead 并提高学习能力。results:* 对于 isic2017 和 isic2018 数据集进行了广泛的实验，并证明了 UCM-Net 在皮肤恶性肿瘤分割中的竞争力。* UCM-Net 的参数少于 50KB，计算量少于 0.05 GLOPs，创造了新的可能性标准 для皮肤恶性肿瘤分割的效率。<details>
<summary>Abstract</summary>
Skin cancer is a significant public health problem, and computer-aided diagnosis can help to prevent and treat it. A crucial step for computer-aided diagnosis is accurately segmenting skin lesions in images, which allows for lesion detection, classification, and analysis. However, this task is challenging due to the diverse characteristics of lesions, such as appearance, shape, size, color, texture, and location, as well as image quality issues like noise, artifacts, and occlusions. Deep learning models have recently been applied to skin lesion segmentation, but they have high parameter counts and computational demands, making them unsuitable for mobile health applications. To address this challenge, we propose UCM-Net, a novel, efficient, and lightweight solution that integrates Multi-Layer Perceptions (MLP) and Convolutional Neural Networks (CNN). Unlike conventional UNet architectures, our UCMNet-Block reduces parameter overhead and enhances UCM-Net's learning capabilities, leading to robust segmentation performance. We validate UCM-Net's competitiveness through extensive experiments on isic2017 and isic2018 datasets. Remarkably, UCM-Net has less than 50KB parameters and less than 0.05 Giga-Operations Per Second (GLOPs), setting a new possible standard for efficiency in skin lesion segmentation. The source code will be publicly available.
</details>
<details>
<summary>摘要</summary>
皮肤癌是一个严重的公共卫生问题，计算机助成诊断可以帮助预防和治疗。计算机助成诊断的关键步骤是准确地分割皮肤癌病变图像中的病变，以便诊断、分类和分析。然而，这个任务很困难，因为癌病变的多样性，包括外表、形状、大小、颜色、文本ure和位置等，以及图像质量问题，如噪声、artefacts和遮挡。最近，深度学习模型已经应用于皮肤癌病变分割，但它们具有高参数计数和计算需求，使其不适合移动医疗应用。为解决这个挑战，我们提出了UCM-Net，一种新的、有效和轻量级的解决方案，它结合多层感知（MLP）和卷积神经网络（CNN）。与传统的UNet架构不同，我们的UCMNet-Block减少参数开销和提高UCM-Net的学习能力，从而实现了稳定的分割性能。我们通过对isic2017和isic2018数据集进行广泛的实验，证明UCM-Net在皮肤癌病变分割中的竞争力。特别是，UCM-Net的参数少于50KB，计算需求少于0.05 Giga-Operations Per Second（GLOPs），创造了新的可能性标准 для皮肤癌病变分割的效率。源代码将公开 availability。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/14/cs.CV_2023_10_14/" data-id="cloh7tqi100je7b8818tt68i5" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_10_14" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/14/cs.AI_2023_10_14/" class="article-date">
  <time datetime="2023-10-14T12:00:00.000Z" itemprop="datePublished">2023-10-14</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/14/cs.AI_2023_10_14/">cs.AI - 2023-10-14</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Neuro-Mimetic-Realization-of-the-Common-Model-of-Cognition-via-Hebbian-Learning-and-Free-Energy-Minimization"><a href="#A-Neuro-Mimetic-Realization-of-the-Common-Model-of-Cognition-via-Hebbian-Learning-and-Free-Energy-Minimization" class="headerlink" title="A Neuro-Mimetic Realization of the Common Model of Cognition via Hebbian Learning and Free Energy Minimization"></a>A Neuro-Mimetic Realization of the Common Model of Cognition via Hebbian Learning and Free Energy Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15177">http://arxiv.org/abs/2310.15177</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Ororbia, Mary Alexandria Kelly</li>
<li>for: 本研究的目的是探讨Generative AI的发展和其对认知科学的影响。</li>
<li>methods: 本研究使用了COGnitive Neural GENerative系统，这是一种基于Hebbian适应的神经网络模型，用于优化variational free energy函数。</li>
<li>results: 本研究提出了一种基于COG系统的 cognitive architectures，可以用于模拟大脑中的认知过程。<details>
<summary>Abstract</summary>
Over the last few years, large neural generative models, capable of synthesizing intricate sequences of words or producing complex image patterns, have recently emerged as a popular representation of what has come to be known as "generative artificial intelligence" (generative AI). Beyond opening the door to new opportunities as well as challenges for the domain of statistical machine learning, the rising popularity of generative AI brings with it interesting questions for Cognitive Science, which seeks to discover the nature of the processes that underpin minds and brains as well as to understand how such functionality might be acquired and instantiated in biological (or artificial) substrate. With this goal in mind, we argue that a promising long-term pathway lies in the crafting of cognitive architectures, a long-standing tradition of the field, cast fundamentally in terms of neuro-mimetic generative building blocks. Concretely, we discuss the COGnitive Neural GENerative system, which is an architecture that casts the Common Model of Cognition in terms of Hebbian adaptation operating in service of optimizing a variational free energy functional.
</details>
<details>
<summary>摘要</summary>
最近几年来，大规模神经生成模型，可以生成复杂的语言序列或图像模式，在人工智能领域中崛起为人们关注的新兴表现形式。这些模型不仅开启了新的机会和挑战，还对认知科学产生了感兴趣的问题，旨在探索 minds 和 brains 的内在机制，以及如何在生物（或人工）材料中实现这种功能。为实现这个目标，我们认为，制定 cognitive 架构是一条有前途的长期路径。在这个框架下，我们讨论了 COGnitive Neural GENerative system，这是一种基于 Hebbian 适应的神经生成模型，用于优化变量自由能函数。
</details></li>
</ul>
<hr>
<h2 id="Improved-Contextual-Recognition-In-Automatic-Speech-Recognition-Systems-By-Semantic-Lattice-Rescoring"><a href="#Improved-Contextual-Recognition-In-Automatic-Speech-Recognition-Systems-By-Semantic-Lattice-Rescoring" class="headerlink" title="Improved Contextual Recognition In Automatic Speech Recognition Systems By Semantic Lattice Rescoring"></a>Improved Contextual Recognition In Automatic Speech Recognition Systems By Semantic Lattice Rescoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09680">http://arxiv.org/abs/2310.09680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ankitha Sudarshan, Vinay Samuel, Parth Patwa, Ibtihel Amara, Aman Chadha</li>
<li>for: 提高语音识别系统的上下文依赖词汇识别精度</li>
<li>methods: 利用隐马尔科夫模型和混合型神经网络模型，并具有语音和语言模型的 интеграción</li>
<li>results: 在LibriSpeech数据集上实现了很好的效果，Word Error Rate（WER）下降了可见的程度<details>
<summary>Abstract</summary>
Automatic Speech Recognition (ASR) has witnessed a profound research interest. Recent breakthroughs have given ASR systems different prospects such as faithfully transcribing spoken language, which is a pivotal advancement in building conversational agents. However, there is still an imminent challenge of accurately discerning context-dependent words and phrases. In this work, we propose a novel approach for enhancing contextual recognition within ASR systems via semantic lattice processing leveraging the power of deep learning models in accurately delivering spot-on transcriptions across a wide variety of vocabularies and speaking styles. Our solution consists of using Hidden Markov Models and Gaussian Mixture Models (HMM-GMM) along with Deep Neural Networks (DNN) models integrating both language and acoustic modeling for better accuracy. We infused our network with the use of a transformer-based model to properly rescore the word lattice achieving remarkable capabilities with a palpable reduction in Word Error Rate (WER). We demonstrate the effectiveness of our proposed framework on the LibriSpeech dataset with empirical analyses.
</details>
<details>
<summary>摘要</summary>
自动语音识别（ASR）技术在研究中受到了广泛的关注。最近的突破使得ASR系统可以准确地识别语音，这是建立对话代理人的关键进步。然而，仍然存在一个急需要准确地识别上下文依赖的词语和短语的挑战。在这种工作中，我们提出了一种改进上下文认知在ASR系统中的方法，通过semantic lattice processing，利用深度学习模型以高精度提供详细的转录。我们的解决方案包括使用隐藏马尔可夫模型和 Gaussian Mixture Models（HMM-GMM）以及深度神经网络（DNN）模型，将语言和音响模型结合起来以提高准确性。我们在网络中使用transformer-based模型来正确地重新分配词网络，实现了很高的Word Error Rate（WER）下降。我们在LibriSpeech dataset上进行了实验，并进行了实验分析。
</details></li>
</ul>
<hr>
<h2 id="Mastering-Robot-Manipulation-with-Multimodal-Prompts-through-Pretraining-and-Multi-task-Fine-tuning"><a href="#Mastering-Robot-Manipulation-with-Multimodal-Prompts-through-Pretraining-and-Multi-task-Fine-tuning" class="headerlink" title="Mastering Robot Manipulation with Multimodal Prompts through Pretraining and Multi-task Fine-tuning"></a>Mastering Robot Manipulation with Multimodal Prompts through Pretraining and Multi-task Fine-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09676">http://arxiv.org/abs/2310.09676</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiachen Li, Qiaozi Gao, Michael Johnston, Xiaofeng Gao, Xuehai He, Suhaila Shakiah, Hangjie Shi, Reza Ghanadan, William Yang Wang</li>
<li>for: 本研究的目的是开发一种可以通过多模态提示（文本描述和视觉信号）来控制机器人的 manipulate 能力。</li>
<li>methods: 我们的方法包括一个两个阶段的训练管道，包括逆动力预训练和多任务调整。为了促进多模态理解，我们设计了一个多模态提示编码器，通过将预训练的语言模型与视觉输入连接起来，模拟动作维度之间的依赖关系。</li>
<li>results: 我们的方法在 VIMA-BENCH 上进行了实验，并在成功率上达到了新的状态对照（10%提高）。此外，我们还证明了我们的模型在 Context 学习中表现出色。<details>
<summary>Abstract</summary>
Prompt-based learning has been demonstrated as a compelling paradigm contributing to large language models' tremendous success (LLMs). Inspired by their success in language tasks, existing research has leveraged LLMs in embodied instruction following and task planning. However, not much attention has been paid to embodied tasks with multimodal prompts, combining vision signals with text descriptions. This type of task poses a major challenge to robots' capability to understand the interconnection and complementarity between vision and language signals. In this work, we introduce an effective framework that learns a policy to perform robot manipulation with multimodal prompts from multi-task expert trajectories. Our methods consist of a two-stage training pipeline that performs inverse dynamics pretraining and multi-task finetuning. To facilitate multimodal understanding, we design our multimodal prompt encoder by augmenting a pretrained LM with a residual connection to the visual input and model the dependencies among action dimensions. Empirically, we evaluate the efficacy of our method on the VIMA-BENCH and establish a new state-of-the-art (10% improvement in success rate). Moreover, we demonstrate that our model exhibits remarkable in-context learning ability.
</details>
<details>
<summary>摘要</summary>
In this work, we propose an effective framework for learning a policy to perform robot manipulation with multimodal prompts using multi-task expert trajectories. Our approach consists of a two-stage training pipeline that includes inverse dynamics pretraining and multi-task finetuning. To facilitate multimodal understanding, we design our multimodal prompt encoder by augmenting a pretrained LM with a residual connection to the visual input and modeling the dependencies among action dimensions.Empirically, we evaluate the effectiveness of our method on the VIMA-BENCH and achieve a new state-of-the-art with a 10% improvement in success rate. Additionally, we demonstrate that our model exhibits remarkable in-context learning ability.
</details></li>
</ul>
<hr>
<h2 id="Efficient-Model-Agnostic-Multi-Group-Equivariant-Networks"><a href="#Efficient-Model-Agnostic-Multi-Group-Equivariant-Networks" class="headerlink" title="Efficient Model-Agnostic Multi-Group Equivariant Networks"></a>Efficient Model-Agnostic Multi-Group Equivariant Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09675">http://arxiv.org/abs/2310.09675</a></li>
<li>repo_url: None</li>
<li>paper_authors: Razan Baltaji, Sourya Basu, Lav R. Varshney</li>
<li>for:  This paper aims to address the computational expense of constructing model-agnostic group equivariant networks, such as equitune, for large product groups.</li>
<li>methods: The paper proposes two efficient model-agnostic equivariant designs for two related problems: one with multiple inputs and another with a single input but a large product group. The designs use a novel fusion layer called an IS layer, which is a universal approximator of invariant-symmetric functions.</li>
<li>results: The paper shows that the proposed designs are competitive with equitune and its variants, while being computationally more efficient. The designs are applied to three applications: multi-image classification, language compositionality, and robust zero-shot image classification.<details>
<summary>Abstract</summary>
Constructing model-agnostic group equivariant networks, such as equitune (Basu et al., 2023b) and its generalizations (Kim et al., 2023), can be computationally expensive for large product groups. We address this by providing efficient model-agnostic equivariant designs for two related problems: one where the network has multiple inputs each with potentially different groups acting on them, and another where there is a single input but the group acting on it is a large product group. For the first design, we initially consider a linear model and characterize the entire equivariant space that satisfies this constraint. This characterization gives rise to a novel fusion layer between different channels that satisfies an invariance-symmetry (IS) constraint, which we call an IS layer. We then extend this design beyond linear models, similar to equitune, consisting of equivariant and IS layers. We also show that the IS layer is a universal approximator of invariant-symmetric functions. Inspired by the first design, we use the notion of the IS property to design a second efficient model-agnostic equivariant design for large product groups acting on a single input. For the first design, we provide experiments on multi-image classification where each view is transformed independently with transformations such as rotations. We find equivariant models are robust to such transformations and perform competitively otherwise. For the second design, we consider three applications: language compositionality on the SCAN dataset to product groups; fairness in natural language generation from GPT-2 to address intersectionality; and robust zero-shot image classification with CLIP. Overall, our methods are simple and general, competitive with equitune and its variants, while also being computationally more efficient.
</details>
<details>
<summary>摘要</summary>
建立模型无关的群equivariant网络，如equitune（Basu et al., 2023b）和其扩展（Kim et al., 2023），可能会对大量产品群进行计算昂贵的成本。我们解决这个问题，提供了高效的模型无关equivariant设计，用于两个相关的问题：一个是网络有多个输入，每个输入都可能有不同的群 acting on it，另一个是一个输入，但是群 acting on it是一个大量产品群。对于第一个设计，我们首先考虑一个线性模型，并Characterize了满足这个约束的整个equivariant空间。这个Characterization导致了一种新的协调层（IS layer），它满足一个对称-对称（IS）约束。我们然后将这个设计扩展到不同的模型，类似于equitune，包括equivariant和IS层。我们还证明了IS层是对称-对称函数的universal approximator。受第一个设计的启发，我们使用IS性质来设计一个高效的模型无关equivariant设计，用于大量产品群 acting on a single input。我们提供了对多视图分类的实验，发现equivariant模型对独立的变换（如旋转）具有抗变换性和竞争性。对于第二个设计，我们考虑了三个应用：语言compositional on SCAN dataset，用于product groups; fairness in natural language generation from GPT-2，用于Addressing intersectionality;和robust zero-shot image classification with CLIP。总的来说，我们的方法是简单而普遍，与equitune和其 variants相当竞争，同时也更加计算效率。
</details></li>
</ul>
<hr>
<h2 id="Edge-InversionNet-Enabling-Efficient-Inference-of-InversionNet-on-Edge-Devices"><a href="#Edge-InversionNet-Enabling-Efficient-Inference-of-InversionNet-on-Edge-Devices" class="headerlink" title="Edge-InversionNet: Enabling Efficient Inference of InversionNet on Edge Devices"></a>Edge-InversionNet: Enabling Efficient Inference of InversionNet on Edge Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09667">http://arxiv.org/abs/2310.09667</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhepeng Wang, Isaacshubhanand Putla, Weiwen Jiang, Youzuo Lin</li>
<li>for: 这个研究旨在提高Edge设备上的普遍干扰实验（Full Waveform Inversion，FWI）效率，通过将数据驱动学习模型对精确性进行简洁化。</li>
<li>methods: 我们提出使用结构剪溃算法来实现轻量级InversionNet，以便在Edge设备上进行效率的推论。</li>
<li>results: 实验结果显示，剪溃后的InversionNet可以实现98.2%的计算资源减少，并且仅受轻度模型性能下降。<details>
<summary>Abstract</summary>
Seismic full waveform inversion (FWI) is a widely used technique in geophysics for inferring subsurface structures from seismic data. And InversionNet is one of the most successful data-driven machine learning models that is applied to seismic FWI. However, the high computing costs to run InversionNet have made it challenging to be efficiently deployed on edge devices that are usually resource-constrained. Therefore, we propose to employ the structured pruning algorithm to get a lightweight version of InversionNet, which can make an efficient inference on edge devices. And we also made a prototype with Raspberry Pi to run the lightweight InversionNet. Experimental results show that the pruned InversionNet can achieve up to 98.2 % reduction in computing resources with moderate model performance degradation.
</details>
<details>
<summary>摘要</summary>
震动全波形数据逆置（FWI）是地球物理学中广泛使用的技术，用于基于地震数据推断地下结构。而倒推网络（InversionNet）是数据驱动机器学习模型中的一个最成功的应用，但高计算成本使其在边缘设备上进行效率地部署具有挑战性。因此，我们提议使用结构采样法来实现一个轻量级的倒推网络，以实现在边缘设备上高效的推断。而我们还制作了使用蓝莓 Pi 进行测试的原型。实验结果表明，减少后的倒推网络可以实现高达98.2%的计算资源减少，同时只带来一定的模型性能下降。
</details></li>
</ul>
<hr>
<h2 id="A-Generalized-Extensive-Form-Fictitious-Play-Algorithm"><a href="#A-Generalized-Extensive-Form-Fictitious-Play-Algorithm" class="headerlink" title="A Generalized Extensive-Form Fictitious Play Algorithm"></a>A Generalized Extensive-Form Fictitious Play Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09658">http://arxiv.org/abs/2310.09658</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim P. Schulze</li>
<li>for: 这个论文是为了解二人零球游戏的平衡而写的。</li>
<li>methods: 这个论文使用了一种简单的扩展形算法来找到两人零球游戏的平衡。这个算法是通过一种总体化的形式来实现的，与一种扩展形 ficitious play 算法相似。</li>
<li>results: 这个论文的结果表明，这种新算法与一种相似的扩展形 ficitious play 算法和一种 counter-factual regret minimization 算法相比，它具有更好的性能，并且更容易实现。<details>
<summary>Abstract</summary>
We introduce a simple extensive-form algorithm for finding equilibria of two-player, zero-sum games. The algorithm is realization equivalent to a generalized form of Fictitious Play. We compare its performance to that of a similar extensive-form fictitious play algorithm and a counter-factual regret minimization algorithm. All three algorithms share the same advantages over normal-form fictitious play in terms of reducing storage requirements and computational complexity. The new algorithm is intuitive and straightforward to implement, making it an appealing option for those looking for a quick and easy game solving tool.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种简单的扩展形算法来找到两人零点游戏的平衡点。这个算法是扩展形非常Play的一种通用形式，我们与相似的扩展形非常Play算法和反思悔检算法进行比较。这三个算法都比正常形非常Play具有减少存储需求和计算复杂性的优点。新算法直观易于实现，使其成为寻找快速简单游戏解决工具的首选。
</details></li>
</ul>
<hr>
<h2 id="SelfVC-Voice-Conversion-With-Iterative-Refinement-using-Self-Transformations"><a href="#SelfVC-Voice-Conversion-With-Iterative-Refinement-using-Self-Transformations" class="headerlink" title="SelfVC: Voice Conversion With Iterative Refinement using Self Transformations"></a>SelfVC: Voice Conversion With Iterative Refinement using Self Transformations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09653">http://arxiv.org/abs/2310.09653</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paarth Neekhara, Shehzeen Hussain, Rafael Valle, Boris Ginsburg, Rishabh Ranjan, Shlomo Dubnov, Farinaz Koushanfar, Julian McAuley<br>for: 这个论文的目的是提出一种叫做SelfVC的训练策略，用于逐渐提高一个语音转换模型的性能。methods: 这个论文使用了自然语音学习和人脸识别模型来 derivate speech representations，并使用这些表示来训练一个可控的语音转换模型。results: 在这个论文中，通过使用自我生成的示例来逐渐改进语音转换模型，可以提高生成的语音的 speaker similarity 和自然性。此外，SelfVC 还可以应用于零shot语音转换、 cross-lingual 语音转换和可控的语音合成等任务。<details>
<summary>Abstract</summary>
We propose SelfVC, a training strategy to iteratively improve a voice conversion model with self-synthesized examples. Previous efforts on voice conversion focus on explicitly disentangling speech representations to separately encode speaker characteristics and linguistic content. However, disentangling speech representations to capture such attributes using task-specific loss terms can lead to information loss by discarding finer nuances of the original signal. In this work, instead of explicitly disentangling attributes with loss terms, we present a framework to train a controllable voice conversion model on entangled speech representations derived from self-supervised learning and speaker verification models. First, we develop techniques to derive prosodic information from the audio signal and SSL representations to train predictive submodules in the synthesis model. Next, we propose a training strategy to iteratively improve the synthesis model for voice conversion, by creating a challenging training objective using self-synthesized examples. In this training approach, the current state of the synthesis model is used to generate voice-converted variations of an utterance, which serve as inputs for the reconstruction task, ensuring a continuous and purposeful refinement of the model. We demonstrate that incorporating such self-synthesized examples during training improves the speaker similarity of generated speech as compared to a baseline voice conversion model trained solely on heuristically perturbed inputs. SelfVC is trained without any text and is applicable to a range of tasks such as zero-shot voice conversion, cross-lingual voice conversion, and controllable speech synthesis with pitch and pace modifications. SelfVC achieves state-of-the-art results in zero-shot voice conversion on metrics evaluating naturalness, speaker similarity, and intelligibility of synthesized audio.
</details>
<details>
<summary>摘要</summary>
我们提出了SelfVC，一种培训策略，可以逐步提高一个语音转换模型的性能。之前的尝试中，人们通常通过显式地分离发音表示来分配说话特征和语言内容。然而，通过任务特定的损失函数来损失这些特征可能会导致信息损失，因为抛弃了原始信号的细节。在这个工作中，我们不是通过显式地分离特征来损失这些特征，而是通过控制语音转换模型来培训混合的发音表示。我们开发了一些技术来从音频信号和SSL表示中提取发音信息，并用这些信息来训练预测子模块。然后，我们提出了一种培训策略，可以逐步改进语音转换模型的性能，通过使用自己生成的示例进行反馈循环训练。在这种培训策略中，当前的synthesis模型的状态被用来生成转换后的话语变体，作为反馈输入，以确保不断改进模型的性能。我们示出，在培训SelfVC模型时，不需要任何文本，可以在不同任务中进行零批量语音转换、跨语言语音转换和可控的语音生成中使用。SelfVC在零批量语音转换中实现了状态之最的结果，并且在自然性、发音相似度和理解性等方面取得了出色的成绩。
</details></li>
</ul>
<hr>
<h2 id="Lexical-Entrainment-for-Conversational-Systems"><a href="#Lexical-Entrainment-for-Conversational-Systems" class="headerlink" title="Lexical Entrainment for Conversational Systems"></a>Lexical Entrainment for Conversational Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09651">http://arxiv.org/abs/2310.09651</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengxiang Shi, Procheta Sen, Aldo Lipani</li>
<li>for: This paper aims to address the issue of lexical entrainment (LE) in conversational systems, which is a crucial humanlike phenomenon that is not adequately addressed by current response generation models.</li>
<li>methods: The authors propose a new dataset, named MULTIWOZ-ENTR, and a measure for LE for conversational systems. They also suggest two new tasks, a LE extraction task and a LE generation task, and present two baseline approaches for the LE extraction task.</li>
<li>results: The authors demonstrate the effectiveness of their proposed approach by presenting results from experiments conducted on the MULTIWOZ-ENTR dataset.Here is the same information in Simplified Chinese text, as requested:</li>
<li>for: 这篇论文是为了解决对话系统中的 lexical entrainment（LE）问题，这是一种人类样式的现象，现有的回答生成模型未能充分考虑这一点。</li>
<li>methods: 作者们提出了一个新的数据集，名为 MULTIWOZ-ENTR，以及一种对 LE 的评价方法。他们还建议两个新任务，一个是 LE 提取任务，另一个是 LE 生成任务，并提出了两种基线方法来实现 LE 提取任务。</li>
<li>results: 作者们通过在 MULTIWOZ-ENTR 数据集上进行的实验，证明了他们的提出的方法的有效性。<details>
<summary>Abstract</summary>
Conversational agents have become ubiquitous in assisting with daily tasks, and are expected to possess human-like features. One such feature is lexical entrainment (LE), a phenomenon in which speakers in human-human conversations tend to naturally and subconsciously align their lexical choices with those of their interlocutors, leading to more successful and engaging conversations. As an example, if a digital assistant replies 'Your appointment for Jinling Noodle Pub is at 7 pm' to the question 'When is my reservation for Jinling Noodle Bar today?', it may feel as though the assistant is trying to correct the speaker, whereas a response of 'Your reservation for Jinling Noodle Bar is at 7 pm' would likely be perceived as more positive. This highlights the importance of LE in establishing a shared terminology for maximum clarity and reducing ambiguity in conversations. However, we demonstrate in this work that current response generation models do not adequately address this crucial humanlike phenomenon. To address this, we propose a new dataset, named MULTIWOZ-ENTR, and a measure for LE for conversational systems. Additionally, we suggest a way to explicitly integrate LE into conversational systems with two new tasks, a LE extraction task and a LE generation task. We also present two baseline approaches for the LE extraction task, which aim to detect LE expressions from dialogue contexts.
</details>
<details>
<summary>摘要</summary>
很多对话代理程序已经在日常任务中出现，并且需要具备人类化特征。一种这种特征是语言同步（LE），即在人类对话中， speaker 们会自然地和无意识地与对方的语言选择相吻合，从而使对话更加成功和有趣。例如，如果一个数字助手回答“你今天的预约时间为7点”，即使用户问道“今天我的预约时间是什么时间？”，助手的回答可能会被视为 corrected ，而不是“你的预约时间是7点”。这显示了LE在建立共同术语的重要性，以避免对话中的歧义。然而，我们在这个工作中发现，当前的响应生成模型并不充分考虑这一重要的人类特征。为此，我们提出了一个新的数据集名为 MULTIWOZ-ENTR，以及一个LE测量方法。此外，我们还提出了一种将LEExplicitly integrate into conversational systems的方法，包括两个新任务：LE抽取任务和LE生成任务。此外，我们还提出了两种基elineapproaches for LE抽取任务，以检测对话上的LE表达。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Federated-Learning-in-Healthcare-a-review"><a href="#Multimodal-Federated-Learning-in-Healthcare-a-review" class="headerlink" title="Multimodal Federated Learning in Healthcare: a review"></a>Multimodal Federated Learning in Healthcare: a review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09650">http://arxiv.org/abs/2310.09650</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacob Thrasher, Alina Devkota, Prasiddha Siwakotai, Rohit Chivukula, Pranav Poudel, Chaunbo Hu, Binod Bhattarai, Prashnna Gyawali</li>
<li>for: 本研究旨在探讨医疗领域中的多Modal Federated Learning（MMFL），以及其在保持患者数据隐私和安全的情况下提供高度准确和可靠的人工智能系统。</li>
<li>methods: 本研究使用了聚合学习和 federated learning 等方法，以实现在多个本地数据存储机构中进行多模态学习。</li>
<li>results: 本研究提出了一些挑战现有模型的限制，并指出了未来在这个领域的发展方向。<details>
<summary>Abstract</summary>
Recent advancements in multimodal machine learning have empowered the development of accurate and robust AI systems in the medical domain, especially within centralized database systems. Simultaneously, Federated Learning (FL) has progressed, providing a decentralized mechanism where data need not be consolidated, thereby enhancing the privacy and security of sensitive healthcare data. The integration of these two concepts supports the ongoing progress of multimodal learning in healthcare while ensuring the security and privacy of patient records within local data-holding agencies. This paper offers a concise overview of the significance of FL in healthcare and outlines the current state-of-the-art approaches to Multimodal Federated Learning (MMFL) within the healthcare domain. It comprehensively examines the existing challenges in the field, shedding light on the limitations of present models. Finally, the paper outlines potential directions for future advancements in the field, aiming to bridge the gap between cutting-edge AI technology and the imperative need for patient data privacy in healthcare applications.
</details>
<details>
<summary>摘要</summary>
（简体中文）近期，多modal机器学习技术在医疗领域得到了进一步发展，特别是在中央数据库系统中。同时，联合学习（FL）也在进步，提供了一种分布式机制，不需要集中数据，从而提高了医疗数据的隐私和安全性。这两种概念的结合支持了医疗领域的多modal学习进程，同时保证了患者记录在本地数据持有机构中的安全性和隐私性。本文提供了医疗领域联合学习的简要概述，并详细描述了当前领域的挑战和限制。最后，本文还提出了未来发展的可能性，旨在bridging当今AI技术和医疗应用中病人数据隐私的差距。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Binary-Code-Comment-Quality-Classification-Integrating-Generative-AI-for-Improved-Accuracy"><a href="#Enhancing-Binary-Code-Comment-Quality-Classification-Integrating-Generative-AI-for-Improved-Accuracy" class="headerlink" title="Enhancing Binary Code Comment Quality Classification: Integrating Generative AI for Improved Accuracy"></a>Enhancing Binary Code Comment Quality Classification: Integrating Generative AI for Improved Accuracy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11467">http://arxiv.org/abs/2310.11467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rohith Arumugam S, Angel Deborah S</li>
<li>for: 提高 binary 代码注释质量分类模型的准确率</li>
<li>methods: integrating generated code and comment pairs to improve model accuracy</li>
<li>results: 两个分类模型，一个使用原始数据集，另一个使用扩展数据集和生成的代码注释对Here’s a more detailed explanation of each point:1. for: The paper is written to improve the accuracy of a binary code comment quality classification model. The authors aim to achieve this by integrating generated code and comment pairs into the model.2. methods: The authors use a Large Language Model Architecture to generate code and comment pairs, and then label these pairs to indicate their utility. They then incorporate these generated pairs into the original dataset to create an augmented dataset. Finally, they train two classification models: one using the original dataset and another using the augmented dataset.3. results: The authors report the results of their experiments, which show that the second model (using the augmented dataset) achieves higher accuracy than the first model (using the original dataset). Specifically, the second model achieves an accuracy of 85.1%, while the first model achieves an accuracy of 78.4%.<details>
<summary>Abstract</summary>
This report focuses on enhancing a binary code comment quality classification model by integrating generated code and comment pairs, to improve model accuracy. The dataset comprises 9048 pairs of code and comments written in the C programming language, each annotated as "Useful" or "Not Useful." Additionally, code and comment pairs are generated using a Large Language Model Architecture, and these generated pairs are labeled to indicate their utility. The outcome of this effort consists of two classification models: one utilizing the original dataset and another incorporating the augmented dataset with the newly generated code comment pairs and labels.
</details>
<details>
<summary>摘要</summary>
这份报告关注将二进制代码评论质量分类模型与生成的代码和评论对照搭配，以提高模型准确性。数据集包含9048对C编程语言中的代码和评论，每个笔记为"有用"或"无用"。此外，代码和评论对也由大型自然语言模型架构生成，并将这些生成对照标注为其有用性。结果包括两个分类模型：一个使用原始数据集，另一个包括已生成的代码评论对和标注。
</details></li>
</ul>
<hr>
<h2 id="ASSERT-Automated-Safety-Scenario-Red-Teaming-for-Evaluating-the-Robustness-of-Large-Language-Models"><a href="#ASSERT-Automated-Safety-Scenario-Red-Teaming-for-Evaluating-the-Robustness-of-Large-Language-Models" class="headerlink" title="ASSERT: Automated Safety Scenario Red Teaming for Evaluating the Robustness of Large Language Models"></a>ASSERT: Automated Safety Scenario Red Teaming for Evaluating the Robustness of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09624">http://arxiv.org/abs/2310.09624</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alexmeigz/assert">https://github.com/alexmeigz/assert</a></li>
<li>paper_authors: Alex Mei, Sharon Levy, William Yang Wang</li>
<li>for: 这个论文的目的是提高AI安全评估中的可靠性，以适应高度Random的环境。</li>
<li>methods: 该论文提出了三种方法，即语意匹配增强、目标启动和敌意批注注入。这些方法用于生成覆盖多个安全设定的测试集，包括semantic equivalence、related scenarios和敌意情况。</li>
<li>results: 研究发现，exist在现有的状态艺模型中的安全保护措施并不能 garantuee模型在各种语义相关的情况下的正确性。在Semantic equivalence和related scenarios中，模型的性能差异 statistically significant， erreur rates up to 19% in zero-shot adversarial settings。这些结果表明，在AI安全评估中需要更多的注意和研究。<details>
<summary>Abstract</summary>
As large language models are integrated into society, robustness toward a suite of prompts is increasingly important to maintain reliability in a high-variance environment.Robustness evaluations must comprehensively encapsulate the various settings in which a user may invoke an intelligent system. This paper proposes ASSERT, Automated Safety Scenario Red Teaming, consisting of three methods -- semantically aligned augmentation, target bootstrapping, and adversarial knowledge injection. For robust safety evaluation, we apply these methods in the critical domain of AI safety to algorithmically generate a test suite of prompts covering diverse robustness settings -- semantic equivalence, related scenarios, and adversarial. We partition our prompts into four safety domains for a fine-grained analysis of how the domain affects model performance. Despite dedicated safeguards in existing state-of-the-art models, we find statistically significant performance differences of up to 11% in absolute classification accuracy among semantically related scenarios and error rates of up to 19% absolute error in zero-shot adversarial settings, raising concerns for users' physical safety.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-decoder-only-foundation-model-for-time-series-forecasting"><a href="#A-decoder-only-foundation-model-for-time-series-forecasting" class="headerlink" title="A decoder-only foundation model for time-series forecasting"></a>A decoder-only foundation model for time-series forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10688">http://arxiv.org/abs/2310.10688</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhimanyu Das, Weihao Kong, Rajat Sen, Yichen Zhou</li>
<li>for: 这份研究是为了设计一个基于大语言模型的时间序列基础模型，用于预测，其 zero-shot 性能在多个公开数据集上几乎与现有最佳指导预测模型相当。</li>
<li>methods: 这个模型基于嵌入式类别器的剪辑者类型注意力模型，通过将一个大时间序列集合预先训练，可以在不同的预测历史长度、预测长度和时间细分度上运作良好。</li>
<li>results: 研究发现，这个模型在不同的数据集上可以实现高度的预测精度，并且可以跨越不同的预测历史长度、预测长度和时间细分度。<details>
<summary>Abstract</summary>
Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.
</details>
<details>
<summary>摘要</summary>
使用大语言模型 recent advances in Natural Language Processing (NLP) 的技术，我们设计了一个时间序列基础模型，其 zeroshot 性能在多个公共数据集上与每个数据集的现有supervised forecasting模型的精度几乎相当。我们的模型基于预训练patched-decoder Style attention模型，可以在不同的预测历史长度、预测长度和时间粒度上工作良好。Note:* "zeroshot" means that the model is not trained on any specific dataset, but still achieves good performance on that dataset.* "supervised" means that the model is trained on a specific dataset with labeled data.* "patched-decoder" is a type of attention mechanism used in the model.
</details></li>
</ul>
<hr>
<h2 id="Deep-Neural-Networks-Can-Learn-Generalizable-Same-Different-Visual-Relations"><a href="#Deep-Neural-Networks-Can-Learn-Generalizable-Same-Different-Visual-Relations" class="headerlink" title="Deep Neural Networks Can Learn Generalizable Same-Different Visual Relations"></a>Deep Neural Networks Can Learn Generalizable Same-Different Visual Relations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09612">http://arxiv.org/abs/2310.09612</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexa R. Tartaglini, Sheridan Feucht, Michael A. Lepori, Wai Keen Vong, Charles Lovering, Brenden M. Lake, Ellie Pavlick</li>
<li>for: 研究深度神经网络是否可以学习和泛化同ifferent关系，包括在不同批处理和精度训练中。</li>
<li>methods: 使用多种架构、预训练方法和精度训练数据来研究深度神经网络是否可以学习和泛化同ifferent关系。</li>
<li>results: certain pretrained transformers可以学习一个高度泛化的同ifferent关系，并且 fine-tuning on abstract shapeslacking texture or color提供了最强的out-of-distribution泛化。<details>
<summary>Abstract</summary>
Although deep neural networks can achieve human-level performance on many object recognition benchmarks, prior work suggests that these same models fail to learn simple abstract relations, such as determining whether two objects are the same or different. Much of this prior work focuses on training convolutional neural networks to classify images of two same or two different abstract shapes, testing generalization on within-distribution stimuli. In this article, we comprehensively study whether deep neural networks can acquire and generalize same-different relations both within and out-of-distribution using a variety of architectures, forms of pretraining, and fine-tuning datasets. We find that certain pretrained transformers can learn a same-different relation that generalizes with near perfect accuracy to out-of-distribution stimuli. Furthermore, we find that fine-tuning on abstract shapes that lack texture or color provides the strongest out-of-distribution generalization. Our results suggest that, with the right approach, deep neural networks can learn generalizable same-different visual relations.
</details>
<details>
<summary>摘要</summary>
In this article, we thoroughly investigate whether deep neural networks can acquire and generalize same-different relations both within and out-of-distribution using various architectures, pretraining methods, and fine-tuning datasets. We find that certain pretrained transformers can learn a same-different relation that generalizes with near perfect accuracy to out-of-distribution stimuli. Moreover, we find that fine-tuning on abstract shapes that lack texture or color provides the strongest out-of-distribution generalization. Our results suggest that, with the right approach, deep neural networks can learn generalizable same-different visual relations.
</details></li>
</ul>
<hr>
<h2 id="Penetrative-AI-Making-LLMs-Comprehend-the-Physical-World"><a href="#Penetrative-AI-Making-LLMs-Comprehend-the-Physical-World" class="headerlink" title="Penetrative AI: Making LLMs Comprehend the Physical World"></a>Penetrative AI: Making LLMs Comprehend the Physical World</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09605">http://arxiv.org/abs/2310.09605</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huatao Xu, Liying Han, Mo Li, Mani Srivastava</li>
<li>for: 本研究探讨了如何使用大型自然语言模型（LLMs）与物联网传感器和 actuators进行交互和理解物理世界，以推动人工智能在物理世界中的应用。</li>
<li>methods: 本研究采用了扩展LLMs的方法，通过处理感知信号来让模型与物理世界进行交互和理解。</li>
<li>results: 研究发现，使用ChatGPT作为例子，LLMs在处理物联网传感器数据和对其进行理解方面具有considerable和特殊的能力，这开放了新的应用场景 дляLLMs，同时也为人工智能在物理世界中的应用提供了新的机会。<details>
<summary>Abstract</summary>
Recent developments in Large Language Models (LLMs) have demonstrated their remarkable capabilities across a range of tasks. Questions, however, persist about the nature of LLMs and their potential to integrate common-sense human knowledge when performing tasks involving information about the real physical world. This paper delves into these questions by exploring how LLMs can be extended to interact with and reason about the physical world through IoT sensors and actuators, a concept that we term "\textit{Penetrative AI}". The paper explores such an extension at two levels of LLMs' ability to penetrate into the physical world via the processing of sensory signals. Our preliminary findings indicate that LLMs, with ChatGPT being the representative example in our exploration, have considerable and unique proficiency in employing the knowledge they learned during training for interpreting IoT sensor data and reasoning over them about tasks in the physical realm. Not only this opens up new applications for LLMs beyond traditional text-based tasks, but also enables new ways of incorporating human knowledge in cyber-physical systems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Context-aware-Session-based-Recommendation-with-Graph-Neural-Networks"><a href="#Context-aware-Session-based-Recommendation-with-Graph-Neural-Networks" class="headerlink" title="Context-aware Session-based Recommendation with Graph Neural Networks"></a>Context-aware Session-based Recommendation with Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09593">http://arxiv.org/abs/2310.09593</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/brilliantzhang/cares">https://github.com/brilliantzhang/cares</a></li>
<li>paper_authors: Zhihui Zhang, JianXiang Yu, Xiang Li</li>
<li>for: 本研究旨在提高Session-based recommendation（SBR）的准确率，特别是在使用不同类型的Session中捕捉用户兴趣的情况下。</li>
<li>methods: 本研究提出了一种名为CARES的Context-Aware Session-based Recommendation模型，该模型利用不同类型的Session中的Context来捕捉用户兴趣，并采用图ael neural networks进行学习。</li>
<li>results: 实验结果表明，CARES模型在三个标准 datasets上均有显著的提高，比如P@20和MRR@20等指标。<details>
<summary>Abstract</summary>
Session-based recommendation (SBR) is a task that aims to predict items based on anonymous sequences of user behaviors in a session. While there are methods that leverage rich context information in sessions for SBR, most of them have the following limitations: 1) they fail to distinguish the item-item edge types when constructing the global graph for exploiting cross-session contexts; 2) they learn a fixed embedding vector for each item, which lacks the flexibility to reflect the variation of user interests across sessions; 3) they generally use the one-hot encoded vector of the target item as the hard label to predict, thus failing to capture the true user preference. To solve these issues, we propose CARES, a novel context-aware session-based recommendation model with graph neural networks, which utilizes different types of contexts in sessions to capture user interests. Specifically, we first construct a multi-relation cross-session graph to connect items according to intra- and cross-session item-level contexts. Further, to encode the variation of user interests, we design personalized item representations. Finally, we employ a label collaboration strategy for generating soft user preference distribution as labels. Experiments on three benchmark datasets demonstrate that CARES consistently outperforms state-of-the-art models in terms of P@20 and MRR@20. Our data and codes are publicly available at https://github.com/brilliantZhang/CARES.
</details>
<details>
<summary>摘要</summary>
Session-based recommendation (SBR) 是一个任务，旨在预测基于匿名用户行为序列的用户喜好。虽然有些方法利用session中的丰富上下文信息来实现SBR，但大多数方法具有以下限制：1）不能区分 item-item 边的类型，在构建全局图以利用跨SESSION上下文时，2）学习固定的 embedding 矢量，缺乏用户兴趣的变化适应性，3）通常使用目标项的一元化编码vector作为硬标签预测，从而失去真实用户喜好的表达。为解决这些问题，我们提出了 CARES，一种Context-Aware Session-Based Recommendation模型，利用session中不同类型的上下文来捕捉用户兴趣。具体来说，我们首先构建了多种关系跨SESSION图，将item相互连接，根据内部和跨SESSION item-level上下文。此外，为了编码用户兴趣的变化，我们设计了个性化项表示。最后，我们采用标签合作策略来生成软USER preference分布。实验结果显示，CARES在三个标准 benchmark 数据集上表现出色，相比之前的模型，它在P@20和MRR@20上均有显著提高。我们的数据和代码可以在https://github.com/brilliantZhang/CARES中获取。
</details></li>
</ul>
<hr>
<h2 id="Solving-Math-Word-Problems-with-Reexamination"><a href="#Solving-Math-Word-Problems-with-Reexamination" class="headerlink" title="Solving Math Word Problems with Reexamination"></a>Solving Math Word Problems with Reexamination</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09590">http://arxiv.org/abs/2310.09590</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/steven640pixel/psedualmwp">https://github.com/steven640pixel/psedualmwp</a></li>
<li>paper_authors: Yi Bin, Wenhao Shi, Yujuan Ding, Yang Yang, See-Kiong Ng</li>
<li>for: 这个论文的目的是提高数学问题 solving 能力。</li>
<li>methods: 这个论文使用了 pseudo-dual 学习方法，即在训练过程中重新评估问题的解决方法。</li>
<li>results: 实验表明，当将 pseudo-dual 学习方法应用于一些代表性的数学问题 solving 算法时，可以提高问题 solving 的能力。<details>
<summary>Abstract</summary>
Math word problem (MWP) solving aims to understand the descriptive math problem and calculate the result, for which previous efforts are mostly devoted to upgrade different technical modules. This paper brings a different perspective of \textit{reexamination process} during training by introducing a pseudo-dual task to enhance the MWP solving. We propose a pseudo-dual (PseDual) learning scheme to model such process, which is model-agnostic thus can be adapted to any existing MWP solvers. The pseudo-dual task is specifically defined as filling the numbers in the expression back into the original word problem with numbers masked. To facilitate the effective joint learning of the two tasks, we further design a scheduled fusion strategy for the number infilling task, which smoothly switches the input from the ground-truth math expressions to the predicted ones. Our pseudo-dual learning scheme has been tested and proven effective when being equipped in several representative MWP solvers through empirical studies. \textit{The codes and trained models are available at:} \url{https://github.com/steven640pixel/PsedualMWP}. \end{abstract}
</details>
<details>
<summary>摘要</summary>
mat word problem (MWP) 解决目标是理解描述性数学问题并计算结果，而前一些努力都是升级不同的技术模块。 这篇论文带来了一种不同的 \textit{重新评估过程} 在训练中的思路，通过引入一个 pseudo-dual 任务来提高 MWP 解决。我们提议一种 pseudo-dual 学习方案来模型这个过程，这种方案是无关模型的，因此可以适应任何现有的 MWP 解决器。 pseudo-dual 任务是填充数字到原始的数学问题中，并将数字掩码。为了实现有效的共同学习两个任务，我们还设计了一种安排的融合策略，使得输入从真实的数学表达中顺利地转换到预测的表达中。我们的 pseudo-dual 学习方案在一些代表性的 MWP 解决器上进行了实验，并证明了其效果。 \textit{代码和训练模型可以在} \url{https://github.com/steven640pixel/PsedualMWP} \textit{上获取.}
</details></li>
</ul>
<hr>
<h2 id="Autonomous-Tree-search-Ability-of-Large-Language-Models"><a href="#Autonomous-Tree-search-Ability-of-Large-Language-Models" class="headerlink" title="Autonomous Tree-search Ability of Large Language Models"></a>Autonomous Tree-search Ability of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10686">http://arxiv.org/abs/2310.10686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheyu Zhang, Zhuorui Ye, Yikang Shen, Chuang Gan</li>
<li>for: 提高大型语言模型的推理能力，解决逻辑推理和策略规划等任务。</li>
<li>methods: 使用自动化搜索能力，通过LLM API进行自动化搜索，实现对答案的搜索迹线。</li>
<li>results: 实验结果显示，使用我们的方法可以 achieved huge improvements，比如Chain of Thoughtapproach的准确率提高33%，并且需要 menos GPT-api cost。此外，我们还收集了使用ATS prompt方法和精度调整LLaMA的数据，这种方法可以带来更大的改进。<details>
<summary>Abstract</summary>
Large Language Models have excelled in remarkable reasoning capabilities with advanced prompting techniques, but they fall short on tasks that require exploration, strategic foresight, and sequential decision-making. Recent works propose to utilize external programs to define search logic, such that LLMs can perform passive tree search to solve more challenging reasoning tasks. Though impressive results have been achieved, there are several fundamental limitations of these approaches. First, passive tree searches are not efficient as they usually require multiple rounds of LLM API calls to solve one single problem. Moreover, passive search methods are not flexible since they need task-specific program designs. Then a natural question arises: can we maintain the tree-search capability of LLMs without the aid of external programs, and can still generate responses that clearly demonstrate the process of a tree-structure search? To this end, we propose a new concept called autonomous tree-search ability of LLM, which can automatically generate a response containing search trajectories for the correct answer. Concretely, we perform search trajectories using capable LLM API via a fixed system prompt, allowing them to perform autonomous tree-search (ATS) right out of the box. Experiments on 4 puzzle games demonstrate our method can achieve huge improvements. The ATS-BFS method outperforms the Chain of Thought approach by achieving an average accuracy improvement of 33%. Compared to Tree of Thoughts, it requires 65.6% or 47.7% less GPT-api cost to attain a comparable level of accuracy. Moreover, we have collected data using the ATS prompt method and fine-tuned LLaMA. This approach yield a greater improvement compared to the ones fine-tuned on CoT data. Specifically, it outperforms CoT-tuned LLaMAs by an average of 40.6% and 38.5% for LLaMA2-7B and LLaMA2-13B, respectively.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在进行先进的推理任务时，已经表现出了非常出色的推理能力，但在需要探索、 стратегіic预见和顺序做决策的任务时，它们仍然缺乏表现。 latest works propose to use external programs to define search logic, so that LLMs can perform passive tree search to solve more challenging reasoning tasks. Although impressive results have been achieved, there are several fundamental limitations of these approaches. First, passive tree searches are not efficient, as they usually require multiple rounds of LLM API calls to solve one single problem. Moreover, passive search methods are not flexible, as they need task-specific program designs. Therefore, a natural question arises: can we maintain the tree-search capability of LLMs without the aid of external programs, and can still generate responses that clearly demonstrate the process of a tree-structure search? To this end, we propose a new concept called autonomous tree-search ability of LLM, which can automatically generate a response containing search trajectories for the correct answer. Specifically, we perform search trajectories using capable LLM API via a fixed system prompt, allowing them to perform autonomous tree-search (ATS) right out of the box. Experimental results on 4 puzzle games demonstrate that our method can achieve significant improvements. The ATS-BFS method outperforms the Chain of Thought approach by achieving an average accuracy improvement of 33%. Compared to Tree of Thoughts, it requires 65.6% or 47.7% less GPT-api cost to attain a comparable level of accuracy. Moreover, we have collected data using the ATS prompt method and fine-tuned LLaMA. This approach yields a greater improvement compared to the ones fine-tuned on CoT data. Specifically, it outperforms CoT-tuned LLaMAs by an average of 40.6% and 38.5% for LLaMA2-7B and LLaMA2-13B, respectively.
</details></li>
</ul>
<hr>
<h2 id="PS-AAS-Portfolio-Selection-for-Automated-Algorithm-Selection-in-Black-Box-Optimization"><a href="#PS-AAS-Portfolio-Selection-for-Automated-Algorithm-Selection-in-Black-Box-Optimization" class="headerlink" title="PS-AAS: Portfolio Selection for Automated Algorithm Selection in Black-Box Optimization"></a>PS-AAS: Portfolio Selection for Automated Algorithm Selection in Black-Box Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10685">http://arxiv.org/abs/2310.10685</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ana Kostovska, Gjorgjina Cenikj, Diederick Vermetten, Anja Jankovic, Ana Nikolikj, Urban Skvorc, Peter Korosec, Carola Doerr, Tome Eftimov</li>
<li>for: 这种论文是为了研究自动化算法选择（AAS）的投资问题，即选择一个包含多种算法的股票，以实现最佳的算法选择。</li>
<li>methods: 这篇论文提出了一种数据驱动的股票选择技术，即创建算法行为 méta-表示，基于这些méta-表示的相似性构建一个图，并应用图算法选择最终的多样化、表示性和不重复的股票。</li>
<li>results: 这篇论文通过使用不同的méta-表示技术（SHAP和performance2vec），对324个不同的CMA-ES变体进行了优化BBOB单目标问题的测试，并比较了两种类型的股票：一种是基于总算法行为，另一种是基于每个问题的算法行为。结果显示，使用performance2vec-based méta-表示的方法选择了小型股票，与虚拟最佳算法相比，而使用SHAP-based méta-表示的方法选择了更多的算法，但是在AAS任务中表现不如personalized股票。在大多数考虑的场景下，个性化股票比 класси的排序方法更好，而且在所有场景下都比整个股票更好。<details>
<summary>Abstract</summary>
The performance of automated algorithm selection (AAS) strongly depends on the portfolio of algorithms to choose from. Selecting the portfolio is a non-trivial task that requires balancing the trade-off between the higher flexibility of large portfolios with the increased complexity of the AAS task. In practice, probably the most common way to choose the algorithms for the portfolio is a greedy selection of the algorithms that perform well in some reference tasks of interest.   We set out in this work to investigate alternative, data-driven portfolio selection techniques. Our proposed method creates algorithm behavior meta-representations, constructs a graph from a set of algorithms based on their meta-representation similarity, and applies a graph algorithm to select a final portfolio of diverse, representative, and non-redundant algorithms. We evaluate two distinct meta-representation techniques (SHAP and performance2vec) for selecting complementary portfolios from a total of 324 different variants of CMA-ES for the task of optimizing the BBOB single-objective problems in dimensionalities 5 and 30 with different cut-off budgets. We test two types of portfolios: one related to overall algorithm behavior and the `personalized' one (related to algorithm behavior per each problem separately). We observe that the approach built on the performance2vec-based representations favors small portfolios with negligible error in the AAS task relative to the virtual best solver from the selected portfolio, whereas the portfolios built from the SHAP-based representations gain from higher flexibility at the cost of decreased performance of the AAS. Across most considered scenarios, personalized portfolios yield comparable or slightly better performance than the classical greedy approach. They outperform the full portfolio in all scenarios.
</details>
<details>
<summary>摘要</summary>
algorithm选择自动化（AAS）的性能强度取决于可选的算法集合。选择该集合是一项非轻松的任务，需要平衡更高的灵活性和更大的算法选择任务的复杂度。在实践中，可能最常用的方法是根据参考任务的兴趣选择算法。在这种情况下，我们在这项工作中提出了一种数据驱动的算法集合选择技术。我们的提议的方法是创建算法行为媒体表示，将一组算法基于媒体表示之间的相似性构建一个图，并将图算法应用于选择最终的多样化、代表性和不重复的算法集合。我们对324个不同的CMA-ES变体进行了两种不同的媒体表示技术（SHAP和performance2vec）来选择相似的算法集合，并测试了两种类型的集合：一种关于总算法行为，另一种是关于每个问题的算法行为。我们发现，基于performance2vec-based的表示方法选择小型集合，与虚拟最佳算法从选择的集合中的错误相对较小，而基于SHAP-based的表示方法增加了更高的灵活性，但是在AAS任务中导致性能下降。在大多数考虑的场景下，个性化集合比 классифика greedy 方法提供了相似或微弱的性能，而且在所有场景下都高于全部集合。
</details></li>
</ul>
<hr>
<h2 id="Does-CLIP’s-Generalization-Performance-Mainly-Stem-from-High-Train-Test-Similarity"><a href="#Does-CLIP’s-Generalization-Performance-Mainly-Stem-from-High-Train-Test-Similarity" class="headerlink" title="Does CLIP’s Generalization Performance Mainly Stem from High Train-Test Similarity?"></a>Does CLIP’s Generalization Performance Mainly Stem from High Train-Test Similarity?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09562">http://arxiv.org/abs/2310.09562</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prasanna Mayilvahanan, Thaddäus Wiedemer, Evgenia Rusak, Matthias Bethge, Wieland Brendel</li>
<li>for: 本文研究 CLIP 模型在各种 OUT-OF-DISTRIBUTION (OOD)  benchmark 上的 Zero-shot 和 few-shot 能力。</li>
<li>methods: 本文使用 RETRAINING  CLIP 模型 在剪裁 LAION 数据集上，以提高其 OOD 性能。</li>
<li>results: 研究发现，尽管 CLIP 模型在剪裁 LAION 数据集上的性能下降，但总体性能仍然高。这表示，高的train-test相似性不能完全解释 CLIP 模型的 OOD 性能，其他训练数据的属性must drive CLIP 模型学习更一般的表示。此外，通过剪裁数据点和 OOD  benchmark 相似的数据点，我们揭示了一个 100M split of LAION（原始大小的一半），可以训练 CLIP 模型达到原始 OOD 性能水平。<details>
<summary>Abstract</summary>
Foundation models like CLIP are trained on hundreds of millions of samples and effortlessly generalize to new tasks and inputs. Out of the box, CLIP shows stellar zero-shot and few-shot capabilities on a wide range of out-of-distribution (OOD) benchmarks, which prior works attribute mainly to today's large and comprehensive training dataset (like LAION). However, it is questionable how meaningful terms like out-of-distribution generalization are for CLIP as it seems likely that web-scale datasets like LAION simply contain many samples that are similar to common OOD benchmarks originally designed for ImageNet. To test this hypothesis, we retrain CLIP on pruned LAION splits that replicate ImageNet's train-test similarity with respect to common OOD benchmarks. While we observe a performance drop on some benchmarks, surprisingly, CLIP's overall performance remains high. This shows that high train-test similarity is insufficient to explain CLIP's OOD performance, and other properties of the training data must drive CLIP to learn more generalizable representations. Additionally, by pruning data points that are dissimilar to the OOD benchmarks, we uncover a 100M split of LAION ($\frac{1}{4}$th of its original size) on which CLIP can be trained to match its original OOD performance.
</details>
<details>
<summary>摘要</summary>
CLIP模型如Foundation models是通过百万个样本进行训练，并能够自动泛化到新任务和输入。它的出厂性能在各种不同的外部数据集上表现杰出，这被认为是因为今天的大规模和全面的训练数据集（如LAION）。然而，是否有意义地用 терminus如外部数据集泛化是CLIP的问题，因为它似乎是LAION中的许多样本与常见的OOD benchmarks（如ImageNet）有很多相似之处。为了测试这个假设，我们重新训练CLIP在LAION中采样后的分割中，这些分割与ImageNet的训练集和测试集之间具有相似的类型相似性。虽然我们观察到一些benchmark上的性能下降，但CLIP的总性能仍然高。这表明高的train-test相似性不能完全解释CLIP的OOD性能，其他训练数据的属性must drive CLIP学习更泛化的表示。此外，我们通过从LAION中删除与OOD benchmarks不相似的数据点，发现一个100M大小的LAION分割（占原始大小的一半），在这个分割上训练CLIP可以与原始OOD性能匹配。
</details></li>
</ul>
<hr>
<h2 id="Graph-Neural-Network-approaches-for-single-cell-data-A-recent-overview"><a href="#Graph-Neural-Network-approaches-for-single-cell-data-A-recent-overview" class="headerlink" title="Graph Neural Network approaches for single-cell data: A recent overview"></a>Graph Neural Network approaches for single-cell data: A recent overview</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09561">http://arxiv.org/abs/2310.09561</a></li>
<li>repo_url: None</li>
<li>paper_authors: Konstantinos Lazaros, Dimitris E. Koumadorakis, Panagiotis Vlamos, Aristidis G. Vrahatis</li>
<li>for: 这 paper 的目的是探讨 Graph Neural Networks (GNN) 在单元细胞数据上的应用，以及 GNN 方法在不同目标上的可行性。</li>
<li>methods: 这 paper 使用了多种 GNN 方法，包括 Graph Attention Networks (GAT) 和 Graph Convolutional Neural Networks (Graph CNN)，以及其他相关的方法。</li>
<li>results: 这 paper 提出了一些结合 GNN 和单元细胞数据的研究，显示了这些方法在不同目标上的可行性，例如 cell-type annotation, data integration and imputation, gene regulatory network reconstruction, clustering 等。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNN) are reshaping our understanding of biomedicine and diseases by revealing the deep connections among genes and cells. As both algorithmic and biomedical technologies have advanced significantly, we're entering a transformative phase of personalized medicine. While pioneering tools like Graph Attention Networks (GAT) and Graph Convolutional Neural Networks (Graph CNN) are advancing graph-based learning, the rise of single-cell sequencing techniques is reshaping our insights on cellular diversity and function. Numerous studies have combined GNNs with single-cell data, showing promising results. In this work, we highlight the GNN methodologies tailored for single-cell data over the recent years. We outline the diverse range of graph deep learning architectures that center on GAT methodologies. Furthermore, we underscore the several objectives of GNN strategies in single-cell data contexts, ranging from cell-type annotation, data integration and imputation, gene regulatory network reconstruction, clustering and many others. This review anticipates a future where GNNs become central to single-cell analysis efforts, particularly as vast omics datasets are continuously generated and the interconnectedness of cells and genes enhances our depth of knowledge in biomedicine.
</details>
<details>
<summary>摘要</summary>
GRAPH神经网络（GNN）正在改变我们对生物医学和疾病的理解，揭示了基因和细胞之间的深层连接。随着算法技术和生物技术的进步，我们正在进入个性化医学的转型阶段。而前所未有的工具如图像注意力网络（GAT）和图像卷积神经网络（图 CNN）正在推动图形学习的发展，而单细胞测序技术的出现也在改变我们对细胞多样性和功能的理解。许多研究已经结合GNNs与单细胞数据，并取得了有 promise 的结果。在这个工作中，我们将强调最近几年对单细胞数据的GNN方法的探索。我们将介绍各种中心于GAT方法的图深度学习架构，并强调GNN策略在单细胞数据上的多种目标，包括细胞类型标注、数据集成和填充、基因规则网络重建、划分和其他多种目标。这篇文章预测未来，GNN将成为单细胞分析的中心，特别是随着不断生成的庞大各种数据和细胞和基因之间的连接，我们对生物医学的知识将更加深入。
</details></li>
</ul>
<hr>
<h2 id="UNIQA-A-Unified-Framework-for-Both-Full-Reference-and-No-Reference-Image-Quality-Assessment"><a href="#UNIQA-A-Unified-Framework-for-Both-Full-Reference-and-No-Reference-Image-Quality-Assessment" class="headerlink" title="UNIQA: A Unified Framework for Both Full-Reference and No-Reference Image Quality Assessment"></a>UNIQA: A Unified Framework for Both Full-Reference and No-Reference Image Quality Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09560">http://arxiv.org/abs/2310.09560</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Ke Yun, Weisi Lin</li>
<li>for: 提高全referenced（FR）和无参照（NR）图像质量评估（IQA）的性能，并能够同时处理FR和NR输入。</li>
<li>methods: 提出了一种基于 semantic impact 模型的 универсальный网络，包括encoder和多级自注意力（HSA）模块，以及跨层跨注意力（CSCA）模块，用于模型空间扭曲水平和图像 semantics的关系。</li>
<li>results: 对四个synthetic扭曲数据集和三个authentic扭曲数据集进行了广泛的实验，并得到了较高的性能，超过了相关的FR和NR方法。<details>
<summary>Abstract</summary>
The human visual system (HVS) is effective at distinguishing low-quality images due to its ability to sense the distortion level and the resulting semantic impact. Prior research focuses on developing dedicated networks based on the presence and absence of pristine images, respectively, and this results in limited application scope and potential performance inconsistency when switching from NR to FR IQA. In addition, most methods heavily rely on spatial distortion modeling through difference maps or weighted features, and this may not be able to well capture the correlations between distortion and the semantic impact it causes. To this end, we aim to design a unified network for both Full-Reference (FR) and No-Reference (NR) IQA via semantic impact modeling. Specifically, we employ an encoder to extract multi-level features from input images. Then a Hierarchical Self-Attention (HSA) module is proposed as a universal adapter for both FR and NR inputs to model the spatial distortion level at each encoder stage. Furthermore, considering that distortions contaminate encoder stages and damage image semantic meaning differently, a Cross-Scale Cross-Attention (CSCA) module is proposed to examine correlations between distortion at shallow stages and deep ones. By adopting HSA and CSCA, the proposed network can effectively perform both FR and NR IQA. Extensive experiments demonstrate that the proposed simple network is effective and outperforms the relevant state-of-the-art FR and NR methods on four synthetic-distorted datasets and three authentic-distorted datasets.
</details>
<details>
<summary>摘要</summary>
人类视觉系统（HVS）能够准确地认识低质量图像，这是因为它能够感受到图像的扭曲水平和导致的semantic影响。先前的研究主要关注于基于存在和缺失高品质图像的特有网络，这会导致应用范围有限和在切换到FR IQA时性能不稳定。此外，大多数方法依赖于空间扭曲模型，通过差图或权重特征来模型扭曲水平，这可能无法好地捕捉扭曲对semantic意义的影响。为了解决这个问题，我们目的是设计一个可以同时执行FR和NR IQA的统一网络，通过semantic impact模型来模型扭曲水平。我们使用encoder提取输入图像的多级特征。然后，我们提出了一种 Hierarchical Self-Attention（HSA）模块，作为FR和NR输入的通用适配器，以模型encoder stage上的空间扭曲水平。此外，我们认为扭曲会在encoder stage上污染图像的semantic意义，因此我们提出了一种 Cross-Scale Cross-Attention（CSCA）模块，以检查扭曲在不同深度stage之间的相关性。通过采用HSA和CSCA，我们的提案的简单网络可以高效地执行FR和NR IQA。我们的实验证明，我们的提案的简单网络可以高效地与相关的FR和NR方法进行比较，并在四个synthetic-distorted dataset和三个authentic-distorted dataset上获得更好的性能。
</details></li>
</ul>
<hr>
<h2 id="A-study-of-the-impact-of-generative-AI-based-data-augmentation-on-software-metadata-classification"><a href="#A-study-of-the-impact-of-generative-AI-based-data-augmentation-on-software-metadata-classification" class="headerlink" title="A study of the impact of generative AI-based data augmentation on software metadata classification"></a>A study of the impact of generative AI-based data augmentation on software metadata classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13714">http://arxiv.org/abs/2310.13714</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tripti Kumari, Chakali Sai Charan, Ayan Das</li>
<li>for: 这篇论文是为了自动预测代码-注释对的有用性而写的。</li>
<li>methods: 这篇论文使用了人工智能基于神经网络上下文表示的注释和代码的关系来预测代码-注释对的有用性，并对基础数据和大语言模型生成的数据进行性能分析。</li>
<li>results: 在官方评测中，这篇论文的系统比基eline提高4%的F1分数，并且生成的数据质量也得到了改进。<details>
<summary>Abstract</summary>
This paper presents the system submitted by the team from IIT(ISM) Dhanbad in FIRE IRSE 2023 shared task 1 on the automatic usefulness prediction of code-comment pairs as well as the impact of Large Language Model(LLM) generated data on original base data towards an associated source code. We have developed a framework where we train a machine learning-based model using the neural contextual representations of the comments and their corresponding codes to predict the usefulness of code-comments pair and performance analysis with LLM-generated data with base data. In the official assessment, our system achieves a 4% increase in F1-score from baseline and the quality of generated data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Protein-3D-Graph-Structure-Learning-for-Robust-Structure-based-Protein-Property-Prediction"><a href="#Protein-3D-Graph-Structure-Learning-for-Robust-Structure-based-Protein-Property-Prediction" class="headerlink" title="Protein 3D Graph Structure Learning for Robust Structure-based Protein Property Prediction"></a>Protein 3D Graph Structure Learning for Robust Structure-based Protein Property Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11466">http://arxiv.org/abs/2310.11466</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yufei Huang, Siyuan Li, Jin Su, Lirong Wu, Odin Zhang, Haitao Lin, Jingqi Qi, Zihan Liu, Zhangyang Gao, Yuyang Liu, Jiangbin Zheng, Stan. ZQ. Li</li>
<li>for: 本研究旨在解决在蛋白质性质预测中使用预测结构时出现的性能下降问题。</li>
<li>methods: 本研究使用了一种基于蛋白质三维图 структуры学习的框架，即Structure embedding Alignment Optimization（SAO），以mitigate the problem of structure embedding bias between predicted and experimental protein structures。</li>
<li>results: 对比于现有方法，本研究的方法能够在蛋白质性质预测中提高性能，并且可以适用于预测结构和实验结构 both。<details>
<summary>Abstract</summary>
Protein structure-based property prediction has emerged as a promising approach for various biological tasks, such as protein function prediction and sub-cellular location estimation. The existing methods highly rely on experimental protein structure data and fail in scenarios where these data are unavailable. Predicted protein structures from AI tools (e.g., AlphaFold2) were utilized as alternatives. However, we observed that current practices, which simply employ accurately predicted structures during inference, suffer from notable degradation in prediction accuracy. While similar phenomena have been extensively studied in general fields (e.g., Computer Vision) as model robustness, their impact on protein property prediction remains unexplored. In this paper, we first investigate the reason behind the performance decrease when utilizing predicted structures, attributing it to the structure embedding bias from the perspective of structure representation learning. To study this problem, we identify a Protein 3D Graph Structure Learning Problem for Robust Protein Property Prediction (PGSL-RP3), collect benchmark datasets, and present a protein Structure embedding Alignment Optimization framework (SAO) to mitigate the problem of structure embedding bias between the predicted and experimental protein structures. Extensive experiments have shown that our framework is model-agnostic and effective in improving the property prediction of both predicted structures and experimental structures. The benchmark datasets and codes will be released to benefit the community.
</details>
<details>
<summary>摘要</summary>
In this paper, we investigate the reason behind the performance decrease when utilizing predicted structures and attribute it to the structure embedding bias from the perspective of structure representation learning. To study this problem, we identify a Protein 3D Graph Structure Learning Problem for Robust Protein Property Prediction (PGSL-RP3), collect benchmark datasets, and present a protein Structure embedding Alignment Optimization framework (SAO) to mitigate the problem of structure embedding bias between the predicted and experimental protein structures. Our framework is model-agnostic and effective in improving the property prediction of both predicted structures and experimental structures. The benchmark datasets and codes will be released to benefit the community.Here is the Simplified Chinese translation of the text:蛋白结构基于的蛋白性质预测已经成为生物任务中的一种有前途的方法，例如蛋白功能预测和细胞内位置估计。现有的方法几乎完全依赖于实验蛋白结构数据，而无法在这些数据不可用的情况下进行预测。使用人工智能工具（如AlphaFold2）预测的蛋白结构作为替代方案，但我们发现现有的方法，即直接在推理过程中使用准确预测的结构，会导致显著的预测精度下降。这种现象在通用计算机视觉领域已经广泛研究过，称为模型 Robustness，但它对蛋白性质预测的影响尚未得到研究。在这篇论文中，我们首次调查预测结构使用后蛋白性质预测的性能下降的原因，归结于结构表示学习的结构嵌入偏见。为了研究这个问题，我们定义了蛋白3D图像学习问题（PGSL-RP3），收集了参考数据集，并提出了一种蛋白结构嵌入对静定方法（SAO），以解决结构嵌入偏见 между预测结构和实验结构。我们的框架是模型无关的，并且对预测结构和实验结构都有效。我们将参考数据集和代码公布，以便社区各方受益。
</details></li>
</ul>
<hr>
<h2 id="Software-Metadata-Classification-based-on-Generative-Artificial-Intelligence"><a href="#Software-Metadata-Classification-based-on-Generative-Artificial-Intelligence" class="headerlink" title="Software Metadata Classification based on Generative Artificial Intelligence"></a>Software Metadata Classification based on Generative Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13006">http://arxiv.org/abs/2310.13006</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seetharam Killivalavan, Durairaj Thenmozhi</li>
<li>for: 这种方法可以提高 binary 代码注释质量分类模型的性能。</li>
<li>methods: 该方法利用 Generative Artificial Intelligence (AI) 技术，通过 OpenAI API 生成了 1239 个新的代码注释对，从各种 GitHub 存储库和开源项目中提取出来，并将其与现有的 9048 个对照集成到 C 语言中。</li>
<li>results: 使用 cutting-edge Large Language Model Architecture 生成的数据集显示了明显的改善，具体来说，当 incorporated into SVM 模型时，精度从 0.79 提高到 0.85，增加 6%；当 incorporated into ANN 模型时，准确率从 0.731 提高到 0.746，增加 1.5%。<details>
<summary>Abstract</summary>
This paper presents a novel approach to enhance the performance of binary code comment quality classification models through the application of Generative Artificial Intelligence (AI). By leveraging the OpenAI API, a dataset comprising 1239 newly generated code-comment pairs, extracted from various GitHub repositories and open-source projects, has been labelled as "Useful" or "Not Useful", and integrated into the existing corpus of 9048 pairs in the C programming language. Employing a cutting-edge Large Language Model Architecture, the generated dataset demonstrates notable improvements in model accuracy. Specifically, when incorporated into the Support Vector Machine (SVM) model, a 6% increase in precision is observed, rising from 0.79 to 0.85. Additionally, the Artificial Neural Network (ANN) model exhibits a 1.5% increase in recall, climbing from 0.731 to 0.746. This paper sheds light on the potential of Generative AI in augmenting code comment quality classification models. The results affirm the effectiveness of this methodology, indicating its applicability in broader contexts within software development and quality assurance domains. The findings underscore the significance of integrating generative techniques to advance the accuracy and efficacy of machine learning models in practical software engineering scenarios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Instruction-Tuning-with-Human-Curriculum"><a href="#Instruction-Tuning-with-Human-Curriculum" class="headerlink" title="Instruction Tuning with Human Curriculum"></a>Instruction Tuning with Human Curriculum</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09518">http://arxiv.org/abs/2310.09518</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bruce W. Lee, Hyunsoo Cho, Kang Min Yoo</li>
<li>for: 这篇论文旨在探讨如何使用结构化认知学方法来优化现代大语言模型中的指令优化。</li>
<li>methods: 该论文提出了一种基于人类教育框架的高度结构化的人工数据集，并对其进行了适应性和评估。</li>
<li>results: 研究结果表明，该方法可以提高语言模型的性能，比如MMLUbenchmark上的提升为+3.06，并且可以避免额外计算成本。<details>
<summary>Abstract</summary>
The dominant paradigm for instruction tuning is the random-shuffled training of maximally diverse instruction-response pairs. This paper explores the potential benefits of applying a structured cognitive learning approach to instruction tuning in contemporary large language models like ChatGPT and GPT-4. Unlike the previous conventional randomized instruction dataset, we propose a highly structured synthetic dataset that mimics the progressive and organized nature of human education. We curate our dataset by aligning it with educational frameworks, incorporating meta information including its topic and cognitive rigor level for each sample. Our dataset covers comprehensive fine-grained topics spanning diverse educational stages (from middle school to graduate school) with various questions for each topic to enhance conceptual depth using Bloom's taxonomy-a classification framework distinguishing various levels of human cognition for each concept. The results demonstrate that this cognitive rigorous training approach yields significant performance enhancements - +3.06 on the MMLU benchmark and an additional +1.28 on AI2 Reasoning Challenge (hard set) - compared to conventional randomized training, all while avoiding additional computational costs. This research highlights the potential of leveraging human learning principles to enhance the capabilities of language models in comprehending and responding to complex instructions and tasks.
</details>
<details>
<summary>摘要</summary>
主流启发方法 для处理调教是随机洗涤最大多样性的指令-响应对。本文探讨了应用结构化认知学学习方法来提高现代大语言模型如ChatGPT和GPT-4的指令调教。与传统的随机化指令数据集不同，我们提议一个高度结构化的人工数据集，模拟人类教育的进步和有序性。我们对数据进行了匹配，包括每个样本的主题和认知困难程度信息。我们的数据涵盖了广泛的细化主题（从中学到大学），每个主题都有多个问题，以增强概念深度使用布隆分类法-一种分类框架，可以区分不同的认知水平。结果表明，这种认知强化训练方法可以提高表现，比传统随机训练高出3.06个MMLU指标和1.28个AI2逻辑挑战（困难集）。此外，这种方法不需要额外的计算成本。这些研究表明，可以利用人类学习原理来提高语言模型对复杂指令和任务的理解和回答能力。
</details></li>
</ul>
<hr>
<h2 id="Towards-Semantic-Communication-Protocols-for-6G-From-Protocol-Learning-to-Language-Oriented-Approaches"><a href="#Towards-Semantic-Communication-Protocols-for-6G-From-Protocol-Learning-to-Language-Oriented-Approaches" class="headerlink" title="Towards Semantic Communication Protocols for 6G: From Protocol Learning to Language-Oriented Approaches"></a>Towards Semantic Communication Protocols for 6G: From Protocol Learning to Language-Oriented Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09506">http://arxiv.org/abs/2310.09506</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jihong Park, Seung-Woo Ko, Jinho Choi, Seong-Lyun Kim, Mehdi Bennis</li>
<li>for: 这篇论文旨在探讨未来的6G系统将如何面对多种非站ARY tasks的挑战，以及传统的媒体存取控制协议（MAC协议）是如何适应这些挑战的。</li>
<li>methods: 这篇论文提出了一个新的分类方法，将资料驱动的MAC协议分为三级： Level 1 MAC 是使用多代理深度循环学习（MADRL）构建的任务对应神经协议；Level 2 MAC 是将 Level 1 MAC 的输出转换为明确的符号；Level 3 MAC 是使用大语言模型（LLM）和生成模型来构建语言对应的协议。</li>
<li>results: 这篇论文通过探讨这些层次的基本技术和选择性案例研究，提供了关于资料驱动MAC协议的未来走势和未来研究方向的对答。<details>
<summary>Abstract</summary>
The forthcoming 6G systems are expected to address a wide range of non-stationary tasks. This poses challenges to traditional medium access control (MAC) protocols that are static and predefined. In response, data-driven MAC protocols have recently emerged, offering ability to tailor their signaling messages for specific tasks. This article presents a novel categorization of these data-driven MAC protocols into three levels: Level 1 MAC. task-oriented neural protocols constructed using multi-agent deep reinforcement learning (MADRL); Level 2 MAC. neural network-oriented symbolic protocols developed by converting Level 1 MAC outputs into explicit symbols; and Level 3 MAC. language-oriented semantic protocols harnessing large language models (LLMs) and generative models. With this categorization, we aim to explore the opportunities and challenges of each level by delving into their foundational techniques. Drawing from information theory and associated principles as well as selected case studies, this study provides insights into the trajectory of data-driven MAC protocols and sheds light on future research directions.
</details>
<details>
<summary>摘要</summary>
六代系统即将来临，预期能解决各种非静止任务。这会对传统的媒体存取控制协议（MAC）协议产生挑战，这些协议通常是静止的和预先定义的。对此，使用数据驱动的MAC协议已经出现，这些协议可以根据特定任务 tailor其讯息。本文提出了一个 novel 的分类方法，将这些数据驱动的MAC协议分为三级： Level 1 MAC：使用多智能深度反对应学习（MADRL）构建的任务对应神经网络协议。 Level 2 MAC：将 Level 1 MAC 的输出转换为Explicit symbols，这样可以使用神经网络协议。 Level 3 MAC：使用大型语言模型（LLMs）和生成模型，实现语言对应的协议。透过这个分类，我们希望可以探讨每个等级的机遇和挑战，并且从信息论和相关的原则以及选择的实验案例中获得新的见解。这篇研究提供了数据驱动MAC协议的未来趋势和未来研究方向的新的思路。
</details></li>
</ul>
<hr>
<h2 id="One-Shot-Sensitivity-Aware-Mixed-Sparsity-Pruning-for-Large-Language-Models"><a href="#One-Shot-Sensitivity-Aware-Mixed-Sparsity-Pruning-for-Large-Language-Models" class="headerlink" title="One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models"></a>One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09499">http://arxiv.org/abs/2310.09499</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hang Shao, Bei Liu, Yanmin Qian</li>
<li>for: 提高生成预训练变换器(GPT)家族模型的实用性，通过量化、剪枝和其他方法提高模型的效率。</li>
<li>methods: 基于梯度敏感度杂合稀疏剪枝法，不需要重新训练可以剪枝GPT模型至少50%的稀疏率。该方法可以根据敏感度进行适应性分配稀疏，从而降低剪枝导致的错误，保持总稀疏率。</li>
<li>results: 提出的方法可以在极高稀疏率下进一步提高LLM模型的效率，并且兼容量化，可以进一步压缩LLM模型。<details>
<summary>Abstract</summary>
Various Large Language Models(LLMs) from the Generative Pretrained Transformer~(GPT) family have achieved outstanding performances in a wide range of text generation tasks. However, the enormous model sizes have hindered their practical use in real-world applications due to high inference latency. Therefore, improving the efficiencies of LLMs through quantization, pruning, and other means has been a key issue in LLM studies. In this work, we propose a method based on Hessian sensitivity-aware mixed sparsity pruning to prune LLMs to at least 50\% sparsity without the need of any retraining. It allocates sparsity adaptively based on sensitivity, allowing us to reduce pruning-induced error while maintaining the overall sparsity level. The advantages of the proposed method exhibit even more when the sparsity is extremely high. Furthermore, our method is compatible with quantization, enabling further compression of LLMs.
</details>
<details>
<summary>摘要</summary>
各种大型语言模型（LLM）从转换器生成器（GPT）家族已经在文本生成任务中显示出杰出的表现。然而，这些模型的巨大大小使得它们在实际应用中具有高的推理延迟。因此，提高LLM的效率通过量化、剪裁等方法已成为LLM研究中的关键问题。在这个工作中，我们提出基于梯度敏感性杂化混合稀疏剪枝法，可以剪枝LLMs到至少50%的稀疏程度而无需重新训练。它根据敏感性分配稀疏性，使我们可以降低剪枝引起的错误而保持总的稀疏性水平。我们的方法在稀疏程度非常高时表现出更多的优势。此外，我们的方法与量化相容，可以进一步压缩LLMs。
</details></li>
</ul>
<hr>
<h2 id="A-Setwise-Approach-for-Effective-and-Highly-Efficient-Zero-shot-Ranking-with-Large-Language-Models"><a href="#A-Setwise-Approach-for-Effective-and-Highly-Efficient-Zero-shot-Ranking-with-Large-Language-Models" class="headerlink" title="A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models"></a>A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09497">http://arxiv.org/abs/2310.09497</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ielab/llm-rankers">https://github.com/ielab/llm-rankers</a></li>
<li>paper_authors: Shengyao Zhuang, Honglei Zhuang, Bevan Koopman, Guido Zuccon</li>
<li>for: 这paper主要是为了evaluating the effectiveness and efficiency of different prompting approaches for large language models (LLMs) in zero-shot document ranking tasks.</li>
<li>methods: 这paper使用了Pointwise, Pairwise,和Listwise prompting approaches, along with a novel Setwise approach, to evaluate their effectiveness and efficiency in LLM-based zero-shot ranking.</li>
<li>results: 这paper的实验结果表明，Pointwise approaches are efficient but less effective, Pairwise approaches are effective but computationally expensive, while Setwise approaches can significantly reduce computational costs while retaining high zero-shot ranking effectiveness.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) demonstrate impressive effectiveness in zero-shot document ranking tasks. Pointwise, Pairwise, and Listwise prompting approaches have been proposed for LLM-based zero-shot ranking. Our study begins by thoroughly evaluating these existing approaches within a consistent experimental framework, considering factors like model size, token consumption, latency, among others. This first-of-its-kind comparative evaluation of these approaches allows us to identify the trade-offs between effectiveness and efficiency inherent in each approach. We find that while Pointwise approaches score high on efficiency, they suffer from poor effectiveness. Conversely, Pairwise approaches demonstrate superior effectiveness but incur high computational overhead. To further enhance the efficiency of LLM-based zero-shot ranking, we propose a novel Setwise prompting approach. Our approach reduces the number of LLM inferences and the amount of prompt token consumption during the ranking procedure, significantly improving the efficiency of LLM-based zero-shot ranking. We test our method using the TREC DL datasets and the BEIR zero-shot document ranking benchmark. The empirical results indicate that our approach considerably reduces computational costs while also retaining high zero-shot ranking effectiveness.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在零shot文档排序任务中表现出众，Pointwise、Pairwise和Listwise promptingapproaches已经被提出用于LLM基于的零shot排序。我们的研究开始于在一个共同的实验室中仔细评估这些现有的方法，考虑因素如模型大小、token消耗量、延迟时间等。这是第一次对这些方法进行了系统性的比较评估，从而找到每个方法之间的质量和效率之间的交易。我们发现，虽然Pointwise方法具有高效性，但效果不佳。相反，Pairwise方法表现出色，但计算开销很高。为了进一步提高LLM基于的零shot排序的效率，我们提议了一种新的Setwise promptingapproach。我们的方法可以减少LLM的推理数量和提示Token的消耗量，从而显著提高LLM基于的零shot排序的效率。我们使用TREC DL数据集和BEIR零shot文档排序标准套件进行测试，实验结果表明，我们的方法可以减少计算成本，同时保持高的零shot排序效果。
</details></li>
</ul>
<hr>
<h2 id="Mirage-Model-Agnostic-Graph-Distillation-for-Graph-Classification"><a href="#Mirage-Model-Agnostic-Graph-Distillation-for-Graph-Classification" class="headerlink" title="Mirage: Model-Agnostic Graph Distillation for Graph Classification"></a>Mirage: Model-Agnostic Graph Distillation for Graph Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09486">http://arxiv.org/abs/2310.09486</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mridul Gupta, Sahil Manchanda, Hariprasad Kodamana, Sayan Ranu</li>
<li>For: The paper aims to scale training of graph neural networks (GNNs) on large datasets while reducing the need for computation and data resources.* Methods: The paper proposes a distillation algorithm called Mirage, which compresses the computation data itself to create a concise distilled summary, rather than emulating gradient flows on the original training set.* Results: The paper reports that Mirage outperforms state-of-the-art baselines in terms of generalization accuracy, data compression, and distillation efficiency, and is an unsupervised and architecture-agnostic distillation algorithm.Here’s the Simplified Chinese text version of the three information points:</li>
<li>for: 这篇论文目标是将图神经网络（GNNs）在大量数据上进行训练，同时减少计算和数据资源的需求。</li>
<li>methods: 论文提出了一种名为 Mirage的幻化算法，它将计算数据本身压缩成一个简洁的幻化摘要，而不是在原始训练集上优化梯度流。</li>
<li>results: 论文表明，Mirage 比州态艺法（baselines）有更高的泛化精度、数据压缩和幻化效率，并且是一种无监督的、architecture-agnostic的幻化算法。<details>
<summary>Abstract</summary>
GNNs, like other deep learning models, are data and computation hungry. There is a pressing need to scale training of GNNs on large datasets to enable their usage on low-resource environments. Graph distillation is an effort in that direction with the aim to construct a smaller synthetic training set from the original training data without significantly compromising model performance. While initial efforts are promising, this work is motivated by two key observations: (1) Existing graph distillation algorithms themselves rely on training with the full dataset, which undermines the very premise of graph distillation. (2) The distillation process is specific to the target GNN architecture and hyper-parameters and thus not robust to changes in the modeling pipeline. We circumvent these limitations by designing a distillation algorithm called Mirage for graph classification. Mirage is built on the insight that a message-passing GNN decomposes the input graph into a multiset of computation trees. Furthermore, the frequency distribution of computation trees is often skewed in nature, enabling us to condense this data into a concise distilled summary. By compressing the computation data itself, as opposed to emulating gradient flows on the original training set-a prevalent approach to date-Mirage transforms into an unsupervised and architecture-agnostic distillation algorithm. Extensive benchmarking on real-world datasets underscores Mirage's superiority, showcasing enhanced generalization accuracy, data compression, and distillation efficiency when compared to state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
图学混淆（Graph Distillation）是一种尝试通过从原始训练数据中构建一个更小的合成训练集来实现这一目标。然而，现有的图学混淆算法它们自己也需要训练使用整个数据集，这会让图学混淆失去意义。另外，现有的图学混淆算法往往是特定于目标GNN结构和参数，因此不具有对模型pipeline的灵活性和稳定性。为了解决这些限制，我们提出了一种名为 Mirage的图学混淆算法，这种算法基于GNN在输入图上进行消息传递的性质。我们发现，GNN在输入图上进行消息传递时，会将图转化为一个多重 computation tree 的集合。此外，这些 computation tree 的频率分布往往具有极值性，因此我们可以通过简化这些数据来生成一个简洁的混淆SUMMARY。相比于以往的方法，Mirage不需要对原始训练数据进行批处理，而是直接对 computation data 进行压缩。这使得 Mirage 成为一种无监督的、建筑自适应的图学混淆算法。我们在实际的 dataset 上进行了广泛的测试，结果显示 Mirage 的总体性能明显高于现有的基线。 Mirage 可以更好地捕捉图数据的特点，同时具有更好的数据压缩和混淆效率。
</details></li>
</ul>
<hr>
<h2 id="Unified-High-binding-Watermark-for-Unconditional-Image-Generation-Models"><a href="#Unified-High-binding-Watermark-for-Unconditional-Image-Generation-Models" class="headerlink" title="Unified High-binding Watermark for Unconditional Image Generation Models"></a>Unified High-binding Watermark for Unconditional Image Generation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09479">http://arxiv.org/abs/2310.09479</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruinan Ma, Yu-an Tan, Shangbo Wu, Tian Chen, Yajie Wang, Yuanzhang Li</li>
<li>for: 防止AI生成图像模型数据盗用和版权侵犯</li>
<li>methods: 使用隐形编码器 Writing  watermark 图像到原始 AIGC 工具输出图像，然后通过相应的解码器检测和验证 Whether  the suspicious model steals the original AIGC tool data</li>
<li>results: 实验表明，我们的方法可以在只使用模型输出图像的情况下，几乎达到零假阳性率，并且可以跨多种 UIG 模型进行数据盗用验证，提高方法的实用性。<details>
<summary>Abstract</summary>
Deep learning techniques have implemented many unconditional image generation (UIG) models, such as GAN, Diffusion model, etc. The extremely realistic images (also known as AI-Generated Content, AIGC for short) produced by these models bring urgent needs for intellectual property protection such as data traceability and copyright certification. An attacker can steal the output images of the target model and use them as part of the training data to train a private surrogate UIG model. The implementation mechanisms of UIG models are diverse and complex, and there is no unified and effective protection and verification method at present. To address these issues, we propose a two-stage unified watermark verification mechanism with high-binding effects for such models. In the first stage, we use an encoder to invisibly write the watermark image into the output images of the original AIGC tool, and reversely extract the watermark image through the corresponding decoder. In the second stage, we design the decoder fine-tuning process, and the fine-tuned decoder can make correct judgments on whether the suspicious model steals the original AIGC tool data. Experiments demonstrate our method can complete the verification work with almost zero false positive rate under the condition of only using the model output images. Moreover, the proposed method can achieve data steal verification across different types of UIG models, which further increases the practicality of the method.
</details>
<details>
<summary>摘要</summary>
深度学习技术已经实现了许多无条件图像生成（UIG）模型，如GAN、扩散模型等。这些模型生成的极其真实的图像（也称为AI生成内容，AIGC）带来了知识产权保护的紧迫需求，如数据追溯和版权证书。一个攻击者可以偷窃目标模型的输出图像，并使其成为私人代理UIG模型的训练数据。现有的实现机制多样化和复杂，无一统的有效保护和验证方法。为解决这些问题，我们提出了一种两阶段统一水印验证机制，具有高绑定效果。在第一阶段，我们使用编码器将水印图像隐身地写入原始AIGC工具的输出图像中，并通过对应的解码器反向提取水印图像。在第二阶段，我们设计了细化过程，并使用细化过的解码器可以正确地判断是否有恶意模型偷窃原始AIGC工具数据。实验表明，我们的方法可以在只使用模型输出图像的情况下完成验证工作，并且几乎没有假阳性结果。此外，我们的方法还可以验证不同类型的UIG模型数据，从而进一步提高方法的实用性。
</details></li>
</ul>
<hr>
<h2 id="HIO-SDF-Hierarchical-Incremental-Online-Signed-Distance-Fields"><a href="#HIO-SDF-Hierarchical-Incremental-Online-Signed-Distance-Fields" class="headerlink" title="HIO-SDF: Hierarchical Incremental Online Signed Distance Fields"></a>HIO-SDF: Hierarchical Incremental Online Signed Distance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09463">http://arxiv.org/abs/2310.09463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vasileios Vasilopoulos, Suveer Garg, Jinwook Huh, Bhoram Lee, Volkan Isler<br>for: 这个论文的目的是为了开发一种能够高效、可 updatable 的大型移动机器工作空间表示方法。methods: 这个方法使用了签名距离场（SDF）来表示环境，并使用层次结构来结合粗细网格和神经网络来实现高效更新和空间占用优化。results: 这个方法在所有测试场景中都达到了46%的全球SDF错误平均值，并且在同等分辨率的粗细网格上达到了30%的错误低点。<details>
<summary>Abstract</summary>
A good representation of a large, complex mobile robot workspace must be space-efficient yet capable of encoding relevant geometric details. When exploring unknown environments, it needs to be updatable incrementally in an online fashion. We introduce HIO-SDF, a new method that represents the environment as a Signed Distance Field (SDF). State of the art representations of SDFs are based on either neural networks or voxel grids. Neural networks are capable of representing the SDF continuously. However, they are hard to update incrementally as neural networks tend to forget previously observed parts of the environment unless an extensive sensor history is stored for training. Voxel-based representations do not have this problem but they are not space-efficient especially in large environments with fine details. HIO-SDF combines the advantages of these representations using a hierarchical approach which employs a coarse voxel grid that captures the observed parts of the environment together with high-resolution local information to train a neural network. HIO-SDF achieves a 46% lower mean global SDF error across all test scenes than a state of the art continuous representation, and a 30% lower error than a discrete representation at the same resolution as our coarse global SDF grid.
</details>
<details>
<summary>摘要</summary>
一个好的大型移动机器工作空间表示应该是效率高而能够包含相关的几何细节。当探索未知环境时，它需要在线更新可能。我们介绍了HIO-SDF，一种新的方法，它表示环境为签名距离场（SDF）。现有的SDF表示方法包括神经网络或VOXEL网格。神经网络可以持续表示SDF，但它们难以在线更新，除非保留了大量的感知历史用于训练。VOXEL网格没有这个问题，但它们在大型环境中不是太空效率。HIO-SDF结合了这些表示方法的优点，使用层次方法，其中使用粗粒度的VOXEL网格捕捉到环境中观察到的部分，并使用高分辨率的地方信息来训练神经网络。HIO-SDF在所有测试场景中的平均全球SDF误差比现有的连续表示方法下降46%，比同等分辨率的粗粒度全球SDF网格下降30%。
</details></li>
</ul>
<hr>
<h2 id="A-Framework-for-Empowering-Reinforcement-Learning-Agents-with-Causal-Analysis-Enhancing-Automated-Cryptocurrency-Trading"><a href="#A-Framework-for-Empowering-Reinforcement-Learning-Agents-with-Causal-Analysis-Enhancing-Automated-Cryptocurrency-Trading" class="headerlink" title="A Framework for Empowering Reinforcement Learning Agents with Causal Analysis: Enhancing Automated Cryptocurrency Trading"></a>A Framework for Empowering Reinforcement Learning Agents with Causal Analysis: Enhancing Automated Cryptocurrency Trading</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09462">http://arxiv.org/abs/2310.09462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rasoul Amirzadeh, Dhananjay Thiruvady, Asef Nazari, Mong Shan Ee</li>
<li>for: 本研究旨在提高人工智能增强交易方法的财务效益，通过开发一个基于强化学习的自动交易系统，以便在随时变化的加密货币市场中实现更高的回报。</li>
<li>methods: 我们提出了一个名为CausalReinforceNet的框架，用于支持决策系统。该框架通过 causal 分析增强了强化学习代理的能力。在 feature 工程过程中，我们使用 bayesian 网络来确定最有关系的特征，以便影响加密货币价格的变化。此外，我们还在决策过程中添加了 probabilistic 价格方向信号，以提高我们的强化学习代理的决策能力。由于加密货币市场的高投机性，我们设计了一种保守的方法，限制卖出和买入的位置大小，以管理风险。</li>
<li>results: 我们的框架在比较以 Buy-and-Hold 策略为准的情况下，有显著的财务效益。此外，我们开发了两个基于 CausalReinforceNet 框架的强化学习代理，其中一个基于 Q-learning 算法，另一个基于 deep Q-learning 算法。两个代理在 Binance Coin 和 Ethereum 等加密货币上都实现了显著的回报。<details>
<summary>Abstract</summary>
Despite advances in artificial intelligence-enhanced trading methods, developing a profitable automated trading system remains challenging in the rapidly evolving cryptocurrency market. This study aims to address these challenges by developing a reinforcement learning-based automated trading system for five popular altcoins~(cryptocurrencies other than Bitcoin): Binance Coin, Ethereum, Litecoin, Ripple, and Tether. To this end, we present CausalReinforceNet, a framework framed as a decision support system. Designed as the foundational architecture of the trading system, the CausalReinforceNet framework enhances the capabilities of the reinforcement learning agent through causal analysis. Within this framework, we use Bayesian networks in the feature engineering process to identify the most relevant features with causal relationships that influence cryptocurrency price movements. Additionally, we incorporate probabilistic price direction signals from dynamic Bayesian networks to enhance our reinforcement learning agent's decision-making. Due to the high volatility of the cryptocurrency market, we design our framework to adopt a conservative approach that limits sell and buy position sizes to manage risk. We develop two agents using the CausalReinforceNet framework, each based on distinct reinforcement learning algorithms. The results indicate that our framework substantially surpasses the Buy-and-Hold benchmark strategy in profitability. Additionally, both agents generated notable returns on investment for Binance Coin and Ethereum.
</details>
<details>
<summary>摘要</summary>
尽管人工智能增强交易方法有所进步，但在快速发展的 криптовалю短证市场中建立可得利的自动交易系统仍然是一项挑战。本研究目的在于解决这些挑战，通过开发一个基于强化学习的自动交易系统，用于五种流行的 altcoin（非比特币的 криптовалю）： Binance Coin、Ethereum、Litecoin、Ripple 和 Tether。为此，我们提出了 CausalReinforceNet 框架，它是一个决策支持系统。这个框架通过 causal 分析增强了强化学习代理的能力。在这个框架中，我们使用 bayesian 网络进行特征工程，以确定对 криптовалю价格变化的影响最重要的特征。此外，我们还将 probabilistic 价格方向信号 incorporated 到强化学习代理的决策中。由于 криптовалю市场的高投资风险，我们设计了一种保守的approach，限制卖出和买入的位置大小，以管理风险。我们使用 CausalReinforceNet 框架开发了两个代理，每个基于不同的强化学习算法。结果表明，我们的框架在利润方面有所进步，而且两个代理对 Binance Coin 和 Ethereum 都取得了显著的回报。
</details></li>
</ul>
<hr>
<h2 id="Metacognitive-threshold-a-computational-account"><a href="#Metacognitive-threshold-a-computational-account" class="headerlink" title="Metacognitive threshold: a computational account"></a>Metacognitive threshold: a computational account</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13005">http://arxiv.org/abs/2310.13005</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brendan Conway-Smith, Robert L. West</li>
<li>for: 本研究旨在计算地考虑认知门槛（认知状态能够被识别的最小刺激量），并讨论可能影响认知门槛的认知训练和冥想。</li>
<li>methods: 本研究使用计算方法计算认知门槛，并采用认知训练和冥想来调整认知门槛。</li>
<li>results: 研究发现，通过认知训练和冥想，可以影响认知门槛，从而提高认知能力。<details>
<summary>Abstract</summary>
This paper will explore ways of computationally accounting for the metacognitive threshold -- the minimum amount of stimulus needed for a mental state to be perceived -- and discuss potential cognitive mechanisms by which this threshold can be influenced through metacognitive training and meditation.
</details>
<details>
<summary>摘要</summary>
这篇论文将探讨计算方法来考虑认知阈值（最少的刺激量可以让一种心理状态被感知），并讨论可能通过认知培训和禅定来影响认知阈值的认知机制。Here's a breakdown of the translation:* 这篇论文 (zhè běn tōng zhì) - This paper* 将探讨 (shall discuss) - will explore* 计算方法 (jìsuān fāngyì) - computational methods* 认知阈值 (rènqì jiāngrù) - metacognitive threshold* 可以让 (kěyǐ jiàng) - can be* 一种心理状态 (yī zhǒng xīn líng zhèng) - a mental state* 被感知 (bèi gǎn zhī) - is perceived* 并讨论 (bìng tālūn) - and discuss* 认知机制 (rènqì jīfāng) - cognitive mechanisms* 可以影响 (kěyǐ yǐngxiǎng) - can be influenced* 通过 (tōngguò) - through* 认知培训 (rènqì pīxùn) - metacognitive training* 和 (hē) - and* 禅定 (chán dìng) - meditation
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Model-Unlearning"><a href="#Large-Language-Model-Unlearning" class="headerlink" title="Large Language Model Unlearning"></a>Large Language Model Unlearning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10683">http://arxiv.org/abs/2310.10683</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kevinyaobytedance/llm_unlearn">https://github.com/kevinyaobytedance/llm_unlearn</a></li>
<li>paper_authors: Yuanshun Yao, Xiaojun Xu, Yang Liu</li>
<li>for: 本研究旨在探讨如何使用语言模型（LLM）进行忘卷（unlearning），即忘记不良（mis）行为。</li>
<li>methods: 本研究使用了忘卷技术，包括使用负例（negative examples）来对LML进行Alignment。</li>
<li>results: 研究显示，使用忘卷技术可以更有效地对LML进行Alignment，只需要负例来调整LML的行为。此外，忘卷技术还具有计算效率和可靠性的优点。<details>
<summary>Abstract</summary>
We study how to perform unlearning, i.e. forgetting undesirable (mis)behaviors, on large language models (LLMs). We show at least three scenarios of aligning LLMs with human preferences can benefit from unlearning: (1) removing harmful responses, (2) erasing copyright-protected content as requested, and (3) eliminating hallucinations. Unlearning, as an alignment technique, has three advantages. (1) It only requires negative (e.g. harmful) examples, which are much easier and cheaper to collect (e.g. via red teaming or user reporting) than positive (e.g. helpful and often human-written) examples required in RLHF (RL from human feedback). (2) It is computationally efficient. (3) It is especially effective when we know which training samples cause the misbehavior. To the best of our knowledge, our work is among the first to explore LLM unlearning. We are also among the first to formulate the settings, goals, and evaluations in LLM unlearning. We show that if practitioners only have limited resources, and therefore the priority is to stop generating undesirable outputs rather than to try to generate desirable outputs, unlearning is particularly appealing. Despite only having negative samples, our ablation study shows that unlearning can still achieve better alignment performance than RLHF with just 2% of its computational time.
</details>
<details>
<summary>摘要</summary>
我们研究如何进行“忘记”（unlearning），即Language Model（LLM）中的“不良”（undesirable）行为的忘记。我们显示了至少三种场景，在这些场景中，对LML进行Alignment可以受益于忘记：（1） removing harmful responses，（2） erasing copyright-protected content as requested，和（3） eliminating hallucinations。忘记作为一种Alignment技术，有三个优势：（1）只需要负（例如，危险）示例，这些示例比较容易和便宜地收集（例如，通过红团或用户报告），与RLHF（RL from human feedback）中需要的正（例如，有用和常常是人工写的）示例相比。（2） computationally efficient。（3）特别有用当我们知道哪些训练示例导致了不良行为。据我们所知，我们的工作是LLM unlearning中的一员，同时我们还是LLM unlearning中的第一个进行设定、目标和评估的人。我们发现，当有限的资源的情况下，优先级是停止生成不良输出，而不是尝试生成有用输出的情况下，忘记是非常吸引人的。尽管只有负示例，我们的剥离研究显示，忘记仍然可以在RLHF的2%的计算时间内达到更好的Alignment性能。
</details></li>
</ul>
<hr>
<h2 id="LgTS-Dynamic-Task-Sampling-using-LLM-generated-sub-goals-for-Reinforcement-Learning-Agents"><a href="#LgTS-Dynamic-Task-Sampling-using-LLM-generated-sub-goals-for-Reinforcement-Learning-Agents" class="headerlink" title="LgTS: Dynamic Task Sampling using LLM-generated sub-goals for Reinforcement Learning Agents"></a>LgTS: Dynamic Task Sampling using LLM-generated sub-goals for Reinforcement Learning Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09454">http://arxiv.org/abs/2310.09454</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shukla-yash/LgTS-LLM-guided-Dynamic-Task-Sampling-for-RL-agents">https://github.com/shukla-yash/LgTS-LLM-guided-Dynamic-Task-Sampling-for-RL-agents</a></li>
<li>paper_authors: Yash Shukla, Wenchang Gao, Vasanth Sarathy, Alvaro Velasquez, Robert Wright, Jivko Sinapov<br>for: 这个论文旨在探讨大语言模型（LLM）在人工智能机器人和代理人问题中的规划能力，以及如何使用LLM来帮助RL代理人学习和行动。methods: 该论文提出了一种新的LgTS（LLM导导教师学习）方法，该方法利用LLM生成目标状态的子目标图表示，并使用教师学生学习算法让RL代理人学习从起始状态到目标状态的策略，同时尽量减少环境互动次数。与先前的LLM使用方法不同，该方法不需要访问专有或精度调整的LLM，也不需要预训练的策略来实现LLM提出的子目标。results: 通过在基于DoorKeyDomain的格リッド世界和搜索救援领域的实验，我们表明了LLM生成的子目标图表示有助于RL代理人学习LLM提出的子目标，并且教师学生学习算法可以减少环境互动次数。<details>
<summary>Abstract</summary>
Recent advancements in reasoning abilities of Large Language Models (LLM) has promoted their usage in problems that require high-level planning for robots and artificial agents. However, current techniques that utilize LLMs for such planning tasks make certain key assumptions such as, access to datasets that permit finetuning, meticulously engineered prompts that only provide relevant and essential information to the LLM, and most importantly, a deterministic approach to allow execution of the LLM responses either in the form of existing policies or plan operators. In this work, we propose LgTS (LLM-guided Teacher-Student learning), a novel approach that explores the planning abilities of LLMs to provide a graphical representation of the sub-goals to a reinforcement learning (RL) agent that does not have access to the transition dynamics of the environment. The RL agent uses Teacher-Student learning algorithm to learn a set of successful policies for reaching the goal state from the start state while simultaneously minimizing the number of environmental interactions. Unlike previous methods that utilize LLMs, our approach does not assume access to a propreitary or a fine-tuned LLM, nor does it require pre-trained policies that achieve the sub-goals proposed by the LLM. Through experiments on a gridworld based DoorKey domain and a search-and-rescue inspired domain, we show that generating a graphical structure of sub-goals helps in learning policies for the LLM proposed sub-goals and the Teacher-Student learning algorithm minimizes the number of environment interactions when the transition dynamics are unknown.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/14/cs.AI_2023_10_14/" data-id="cloh7tqcg005h7b8823aqa8b2" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_14" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/14/cs.CL_2023_10_14/" class="article-date">
  <time datetime="2023-10-14T11:00:00.000Z" itemprop="datePublished">2023-10-14</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/14/cs.CL_2023_10_14/">cs.CL - 2023-10-14</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Beyond-Testers’-Biases-Guiding-Model-Testing-with-Knowledge-Bases-using-LLMs"><a href="#Beyond-Testers’-Biases-Guiding-Model-Testing-with-Knowledge-Bases-using-LLMs" class="headerlink" title="Beyond Testers’ Biases: Guiding Model Testing with Knowledge Bases using LLMs"></a>Beyond Testers’ Biases: Guiding Model Testing with Knowledge Bases using LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09668">http://arxiv.org/abs/2310.09668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenyang Yang, Rishabh Rustogi, Rachel Brower-Sinning, Grace A. Lewis, Christian Kästner, Tongshuang Wu</li>
<li>for: 这篇论文主要是为了提供一种用于模型测试的工具，以帮助测试人员更好地识别需要测试的方面。</li>
<li>methods: 这篇论文使用了大量自然语言处理技术，包括生成知识库和互动式推荐，以帮助测试人员系统地探索不同的概念。</li>
<li>results: 在用户研究中，测试人员使用Weaver工具时能够更好地识别模型需要测试的方面，并发现了大量的失败测试案例。此外，Weaver还可以帮助实践者在真实的应用场景中测试模型，例如代码理解和对话简要摘要。<details>
<summary>Abstract</summary>
Current model testing work has mostly focused on creating test cases. Identifying what to test is a step that is largely ignored and poorly supported. We propose Weaver, an interactive tool that supports requirements elicitation for guiding model testing. Weaver uses large language models to generate knowledge bases and recommends concepts from them interactively, allowing testers to elicit requirements for further testing. Weaver provides rich external knowledge to testers and encourages testers to systematically explore diverse concepts beyond their own biases. In a user study, we show that both NLP experts and non-experts identified more, as well as more diverse concepts worth testing when using Weaver. Collectively, they found more than 200 failing test cases for stance detection with zero-shot ChatGPT. Our case studies further show that Weaver can help practitioners test models in real-world settings, where developers define more nuanced application scenarios (e.g., code understanding and transcript summarization) using LLMs.
</details>
<details>
<summary>摘要</summary>
当前模型测试工作主要集中在创建测试用例上。确定要测试的内容是一个大多数被忽视和不受支持的步骤。我们提出了 Weaver，一个互动工具，可以支持需求描述导引模型测试。Weaver 使用大型自然语言模型生成知识库并在互动方式下提供概念建议，allowing testers 可以从概念中得到更多的测试需求。Weaver 为测试人员提供了丰富的外部知识，并且鼓励测试人员系统地探索多种概念，超越自己的偏见。在用户研究中，我们发现了 NLP 专家和非专家都可以使用 Weaver 来确定更多，以及更多样本的测试需求。总的来说，他们在 zero-shot ChatGPT 上找到了 más de 200 个失败测试用例。我们的案例研究还表明，Weaver 可以帮助实践者在真实的应用场景中测试模型（如代码理解和讲话笔记摘要），使用 LLMs。
</details></li>
</ul>
<hr>
<h2 id="Legend-at-ArAIEval-Shared-Task-Persuasion-Technique-Detection-using-a-Language-Agnostic-Text-Representation-Model"><a href="#Legend-at-ArAIEval-Shared-Task-Persuasion-Technique-Detection-using-a-Language-Agnostic-Text-Representation-Model" class="headerlink" title="Legend at ArAIEval Shared Task: Persuasion Technique Detection using a Language-Agnostic Text Representation Model"></a>Legend at ArAIEval Shared Task: Persuasion Technique Detection using a Language-Agnostic Text Representation Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09661">http://arxiv.org/abs/2310.09661</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olumide E. Ojo, Olaronke O. Adebanji, Hiram Calvo, Damian O. Dieke, Olumuyiwa E. Ojo, Seye E. Akinsanya, Tolulope O. Abiola, Anna Feldman</li>
<li>for: 本研究的目的是参加2023年阿拉伯语言处理会议（ArabicNLP）的阿拉伯语AI任务评估挑战（ArAIEval），特别是任务1，即从推文和新闻文章中识别吸引人的技巧。</li>
<li>methods: 该研究使用了XLM-RoBERTa语言无关文本表示模型进行训练循环，并进行细化的多语言模型微调。</li>
<li>results: 在测试集评估中，我们的微调后的多语言模型在任务1下的子任务A中取得了0.64的微 F1分数。<details>
<summary>Abstract</summary>
In this paper, we share our best performing submission to the Arabic AI Tasks Evaluation Challenge (ArAIEval) at ArabicNLP 2023. Our focus was on Task 1, which involves identifying persuasion techniques in excerpts from tweets and news articles. The persuasion technique in Arabic texts was detected using a training loop with XLM-RoBERTa, a language-agnostic text representation model. This approach proved to be potent, leveraging fine-tuning of a multilingual language model. In our evaluation of the test set, we achieved a micro F1 score of 0.64 for subtask A of the competition.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们分享我们在“阿拉伯语言处理（ArabicNLP）2023”年度的“阿拉伯AI任务评估比赛”（ArAIEval）中的最佳提交。我们的关注点是任务1，即在推文和新闻文章中Identify persuasion techniques。在阿拉伯文本中探测了使用XLM-RoBERTa语言无关文本表示模型的训练循环。这种方法证明了其高效，通过细化多语言模型的 fine-tuning。在我们对测试集进行评估时，我们在subtask A中获得了0.64的微 F1分数。
</details></li>
</ul>
<hr>
<h2 id="An-End-to-End-System-for-Reproducibility-Assessment-of-Source-Code-Repositories-via-Their-Readmes"><a href="#An-End-to-End-System-for-Reproducibility-Assessment-of-Source-Code-Repositories-via-Their-Readmes" class="headerlink" title="An End-to-End System for Reproducibility Assessment of Source Code Repositories via Their Readmes"></a>An End-to-End System for Reproducibility Assessment of Source Code Repositories via Their Readmes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09634">http://arxiv.org/abs/2310.09634</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kaanakdeniz/reproducibility_assessment">https://github.com/kaanakdeniz/reproducibility_assessment</a></li>
<li>paper_authors: Eyüp Kaan Akdeniz, Selma Tekir, Malik Nizar Asad Al Hinnawi</li>
<li>for: 支持机器学习研究的可重复性评估</li>
<li>methods: 使用约定模板和自定义函数对Readme文件进行检查，并使用层次转移模型为Readme文件分类</li>
<li>results: 系统可以准确地评估Readme文件的可重复性，并且可以提供可解释的分数。同时，section similarity-based系统比层次转移模型 performs better。<details>
<summary>Abstract</summary>
Increased reproducibility of machine learning research has been a driving force for dramatic improvements in learning performances. The scientific community further fosters this effort by including reproducibility ratings in reviewer forms and considering them as a crucial factor for the overall evaluation of papers. Accompanying source code is not sufficient to make a work reproducible. The shared codes should meet the ML reproducibility checklist as well. This work aims to support reproducibility evaluations of papers with source codes. We propose an end-to-end system that operates on the Readme file of the source code repositories. The system checks the compliance of a given Readme to a template proposed by a widely used platform for sharing source codes of research. Our system generates scores based on a custom function to combine section scores. We also train a hierarchical transformer model to assign a class label to a given Readme. The experimental results show that the section similarity-based system performs better than the hierarchical transformer. Moreover, it has an advantage regarding explainability since one can directly relate the score to the sections of Readme files.
</details>
<details>
<summary>摘要</summary>
增加机器学习研究的可重现性是导致学习性能的进步的驱动力。科学社区还进一步推动这一努力，将可重现性评估纳入评审表单中，并视为评审整体评价的关键因素。仅提供源代码不足以使一作品可重现。我们建议一个综合系统，运行在源代码存储库的Readme文件上。该系统根据提案的模板检查源代码的可重现性，并生成分数根据自定义的函数组合分。我们还训练了一个层次转换器模型，将给定的Readme文件分配分类标签。实验结果表明，基于section相似性的系统在可重现性评估中表现更好，并且具有更好的解释性，因为可以直接将分数关联到Readme文件中的section。
</details></li>
</ul>
<hr>
<h2 id="A-Digital-Language-Coherence-Marker-for-Monitoring-Dementia"><a href="#A-Digital-Language-Coherence-Marker-for-Monitoring-Dementia" class="headerlink" title="A Digital Language Coherence Marker for Monitoring Dementia"></a>A Digital Language Coherence Marker for Monitoring Dementia</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09623">http://arxiv.org/abs/2310.09623</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitris Gkoumas, Adam Tsakalidis, Maria Liakata</li>
<li>for: 这个论文旨在提出一种新的、可靠且非侵入的方法，使用自然语言进行诊断和监测 деменcia。</li>
<li>methods: 该论文提出了一种新的任务，即学习叙述中的时间逻辑一致性，并 investigate了多种神经网络方法。</li>
<li>results: 研究发现，与健康人群相比，人们 WITH dementia 的语言一致性呈现出明显的差异，并且与临床生物标志物相关性较高。此外，该 marker 还具有普适性，可应用于其他相关的疾病。<details>
<summary>Abstract</summary>
The use of spontaneous language to derive appropriate digital markers has become an emergent, promising and non-intrusive method to diagnose and monitor dementia. Here we propose methods to capture language coherence as a cost-effective, human-interpretable digital marker for monitoring cognitive changes in people with dementia. We introduce a novel task to learn the temporal logical consistency of utterances in short transcribed narratives and investigate a range of neural approaches. We compare such language coherence patterns between people with dementia and healthy controls and conduct a longitudinal evaluation against three clinical bio-markers to investigate the reliability of our proposed digital coherence marker. The coherence marker shows a significant difference between people with mild cognitive impairment, those with Alzheimer's Disease and healthy controls. Moreover our analysis shows high association between the coherence marker and the clinical bio-markers as well as generalisability potential to other related conditions.
</details>
<details>
<summary>摘要</summary>
使用自然语言来 derive 适当的数字标记已成为迅速发展、有前途和不侵入的诊断和监测诱导症方法。我们提议使用语言一致性作为一种经济、人类可读取的数字标记，以监测人类诱导症的认知变化。我们介绍了一项新任务，用于学习句子之间的时间逻辑一致性，并 investigate 多种神经网络方法。我们比较了这些语言一致性模式，并与三种临床生物标志物进行长期评估，以确定我们所提议的数字一致标记的可靠性。我们发现，与轻度认知障碍、阿兹海默病和健康群体进行比较，我们的一致标记具有显著差异。此外，我们的分析还显示了这些一致标记与临床生物标志物之间的高相关性，以及其普适性。
</details></li>
</ul>
<hr>
<h2 id="An-Expression-Tree-Decoding-Strategy-for-Mathematical-Equation-Generation"><a href="#An-Expression-Tree-Decoding-Strategy-for-Mathematical-Equation-Generation" class="headerlink" title="An Expression Tree Decoding Strategy for Mathematical Equation Generation"></a>An Expression Tree Decoding Strategy for Mathematical Equation Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09619">http://arxiv.org/abs/2310.09619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenqi Zhang, Yongliang Shen, Qingpeng Nong, Zeqi Tan, Yanna Ma, Weiming Lu</li>
<li>for: 该论文主要探讨了如何从自然语言中生成数学公式。</li>
<li>methods: 该论文提出了一种基于树结构的表达水平生成方法，通过层次并行解码策略和双分配匹配算法来生成数学公式。</li>
<li>results: 实验表明，该方法在生成复杂结构的数学公式方面表现出色，比基eline方法更高效。<details>
<summary>Abstract</summary>
Generating mathematical equations from natural language requires an accurate understanding of the relations among math expressions. Existing approaches can be broadly categorized into token-level and expression-level generation. The former treats equations as a mathematical language, sequentially generating math tokens. Expression-level methods generate each expression one by one. However, each expression represents a solving step, and there naturally exist parallel or dependent relations between these steps, which are ignored by current sequential methods. Therefore, we integrate tree structure into the expression-level generation and advocate an expression tree decoding strategy. To generate a tree with expression as its node, we employ a layer-wise parallel decoding strategy: we decode multiple independent expressions (leaf nodes) in parallel at each layer and repeat parallel decoding layer by layer to sequentially generate these parent node expressions that depend on others. Besides, a bipartite matching algorithm is adopted to align multiple predictions with annotations for each layer. Experiments show our method outperforms other baselines, especially for these equations with complex structures.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本中的数学公式生成需要精准地理解数学表达之间的关系。现有的方法可以分为token级和表达级两类。前者视数学公式为一种语言，顺序生成数学token。表达级方法每个表达都代表一步解题，但是这些步骤之间存在并行或依赖关系，现有的顺序方法忽略了这些关系。因此，我们将树结构integrated到表达级生成中，并提出了表达树解码策略。为生成一棵表达树，我们采用层 wise并行解码策略：在每层解码多个独立的表达（叶节点）并重复层 wise并行解码来生成依赖于别的父节点表达。此外，我们采用了一种两个分配算法来对多个预测与注释进行对应。实验表明，我们的方法在Equation with complex structures上比基eline方法表现更好。
</details></li>
</ul>
<hr>
<h2 id="Moral-consensus-and-divergence-in-partisan-language-use"><a href="#Moral-consensus-and-divergence-in-partisan-language-use" class="headerlink" title="Moral consensus and divergence in partisan language use"></a>Moral consensus and divergence in partisan language use</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09618">http://arxiv.org/abs/2310.09618</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nakwon Rim, Marc G. Berman, Yuan Chang Leong<br>methods: 这个论文使用了大规模的Reddit社区和新闻媒体的言语数据（294,476,146个评论和6,749,781篇文章），使用word embedding模型来捕捉语言中的 semantic association，并在7个政治话题（如 abortion、immigration）上进行了研究。results: 研究发现， despite shared moral understanding across the political spectrum, there are consistent differences in the moral associations of words between conservative and liberal text sources, which can be used to distinguish text sources with above 85% classification accuracy. These findings suggest that partisan language differences are widespread and may contribute to political polarization.<details>
<summary>Abstract</summary>
Polarization has increased substantially in political discourse, contributing to a widening partisan divide. In this paper, we analyzed large-scale, real-world language use in Reddit communities (294,476,146 comments) and in news outlets (6,749,781 articles) to uncover psychological dimensions along which partisan language is divided. Using word embedding models that captured semantic associations based on co-occurrences of words in vast textual corpora, we identified patterns of affective polarization present in natural political discourse. We then probed the semantic associations of words related to seven political topics (e.g., abortion, immigration) along the dimensions of morality (moral-to-immoral), threat (threatening-to-safe), and valence (pleasant-to-unpleasant). Across both Reddit communities and news outlets, we identified a small but systematic divergence in the moral associations of words between text sources with different partisan leanings. Moral associations of words were highly correlated between conservative and liberal text sources (average $\rho$ = 0.96), but the differences remained reliable to enable us to distinguish text sources along partisan lines with above 85% classification accuracy. These findings underscore that despite a shared moral understanding across the political spectrum, there are consistent differences that shape partisan language and potentially exacerbate political polarization. Our results, drawn from both informal interactions on social media and curated narratives in news outlets, indicate that these trends are widespread. Leveraging advanced computational techniques, this research offers a fresh perspective that complements traditional methods in political attitudes.
</details>
<details>
<summary>摘要</summary>
政治话语的偏极化现象在现代社会中日益增加，这对政党分化的扩大做出了贡献。在这篇论文中，我们通过分析Reddit社区的294476146个评论和新闻媒体的6749781篇文章，以揭示政治话语中的心理维度。我们使用基于很大文本 Corpora 的词嵌入模型，捕捉了词语之间的含义相互关系，并发现了政治话语中的情感偏极现象。我们 then probed the semantic associations of words related to seven political topics (such as abortion and immigration) along the dimensions of morality (moral-to-immoral), threat (threatening-to-safe), and valence (pleasant-to-unpleasant).在Reddit社区和新闻媒体中，我们发现了一些小而系统的语言偏极现象，即不同政治倾向的文本来源之间的道德含义的差异。虽然保守和自由政治倾向的文本来源之间的道德含义之间存在高度的相互关系（平均值为0.96），但这些差异仍然可以准确地分辨出文本来源的政治倾向，使得我们可以在85%的权益上分类文本来源。这些发现表明，尽管在政治 спектrum中存在共同的道德理解，但是存在一些不同的特征，这些特征可能增加政治偏极化。我们的研究结果，来自社交媒体和新闻媒体，表明这些趋势是普遍的。我们利用了高级计算技术，这些研究结果可以补充传统的政治态度研究。
</details></li>
</ul>
<hr>
<h2 id="RethinkingTMSC-An-Empirical-Study-for-Target-Oriented-Multimodal-Sentiment-Classification"><a href="#RethinkingTMSC-An-Empirical-Study-for-Target-Oriented-Multimodal-Sentiment-Classification" class="headerlink" title="RethinkingTMSC: An Empirical Study for Target-Oriented Multimodal Sentiment Classification"></a>RethinkingTMSC: An Empirical Study for Target-Oriented Multimodal Sentiment Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09596">http://arxiv.org/abs/2310.09596</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/junjie-ye/rethinkingtmsc">https://github.com/junjie-ye/rethinkingtmsc</a></li>
<li>paper_authors: Junjie Ye, Jie Zhou, Junfeng Tian, Rui Wang, Qi Zhang, Tao Gui, Xuanjing Huang</li>
<li>for: 本研究的目的是调查目标受众情感分类 Task 中 modalities 的重要性和 multimodal fusion module 的效果，以及现有数据集是否能够支持研究。</li>
<li>methods: 本研究使用了广泛的实验和深入的分析来回答以下问题：Q1：modalities 在 TMSC 中的重要性是否相同？Q2：哪些 multimodal fusion module 更加有效？Q3：现有的数据集能否支持研究？</li>
<li>results: 实验和分析显示，目前的 TMSC 系统主要依靠文本模式来决定目标受众情感，因此我们指出了一些改进 TMSC 任务的方向，包括模型设计和数据集构建。<details>
<summary>Abstract</summary>
Recently, Target-oriented Multimodal Sentiment Classification (TMSC) has gained significant attention among scholars. However, current multimodal models have reached a performance bottleneck. To investigate the causes of this problem, we perform extensive empirical evaluation and in-depth analysis of the datasets to answer the following questions: Q1: Are the modalities equally important for TMSC? Q2: Which multimodal fusion modules are more effective? Q3: Do existing datasets adequately support the research? Our experiments and analyses reveal that the current TMSC systems primarily rely on the textual modality, as most of targets' sentiments can be determined solely by text. Consequently, we point out several directions to work on for the TMSC task in terms of model design and dataset construction. The code and data can be found in https://github.com/Junjie-Ye/RethinkingTMSC.
</details>
<details>
<summary>摘要</summary>
近期，目标受控多模态情感分类（TMSC）在学者中受到了广泛关注。然而，当前的多模态模型已经达到了性能瓶颈。为了调查这个问题的原因，我们进行了详细的实验和深入分析数据集，以回答以下问题：Q1：多 modalities 在 TMSC 中是否具有相同的重要性？Q2：哪些多模态融合模块更有效？Q3：现有的数据集是否能够完善 TMSC 研究？我们的实验和分析表明，目前的 TMSC 系统主要依赖于文本modalities，因为大多数目标的情感都可以通过文本来确定。因此，我们提出了一些关于 TMSC 任务的模型设计和数据集建设的方向。codes 和数据可以在 <https://github.com/Junjie-Ye/RethinkingTMSC> 找到。
</details></li>
</ul>
<hr>
<h2 id="Self-Detoxifying-Language-Models-via-Toxification-Reversal"><a href="#Self-Detoxifying-Language-Models-via-Toxification-Reversal" class="headerlink" title="Self-Detoxifying Language Models via Toxification Reversal"></a>Self-Detoxifying Language Models via Toxification Reversal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09573">http://arxiv.org/abs/2310.09573</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cooperleong00/toxificationreversal">https://github.com/cooperleong00/toxificationreversal</a></li>
<li>paper_authors: Chak Tou Leong, Yi Cheng, Jiashuo Wang, Jian Wang, Wenjie Li</li>
<li>for: 降低预训练语言模型（PLM）中生成危险或伤害性内容的风险，以便更安全地部署。</li>
<li>methods: 我们提出了一种较轻量级的方法，即让PLM本身实现”自我抹黑”。我们的方法基于 prepending a negative steering prompt 可以让 PLM 生成恶意内容。同时，我们受到了最近的解释研究中的研究，即通过注意层来实现 PLM 内部的Contextualized Representations 的演化。在这基础之上，我们设计了一种方法，可以从 normal generation process 中提取恶意方向，然后通过 manipulate 注意层内的信息流来驱动生成 towards the reversed direction。</li>
<li>results: 我们的方法，不需要任何 fine-tuning 或额外组件，可以 achieved comparable performance with state-of-the-art methods。<details>
<summary>Abstract</summary>
Language model detoxification aims to minimize the risk of generating offensive or harmful content in pretrained language models (PLMs) for safer deployment. Existing methods can be roughly categorized as finetuning-based and decoding-based. However, the former is often resource-intensive, while the latter relies on additional components and potentially compromises the generation fluency. In this paper, we propose a more lightweight approach that enables the PLM itself to achieve "self-detoxification". Our method is built upon the observation that prepending a negative steering prompt can effectively induce PLMs to generate toxic content. At the same time, we are inspired by the recent research in the interpretability field, which formulates the evolving contextualized representations within the PLM as an information stream facilitated by the attention layers. Drawing on this idea, we devise a method to identify the toxification direction from the normal generation process to the one prompted with the negative prefix, and then steer the generation to the reversed direction by manipulating the information movement within the attention layers. Experimental results show that our approach, without any fine-tuning or extra components, can achieve comparable performance with state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
language model detoxification aims to minimize the risk of generating offensive or harmful content in pretrained language models (PLMs) for safer deployment. existing methods can be roughly categorized as finetuning-based and decoding-based. however, the former is often resource-intensive, while the latter relies on additional components and potentially compromises the generation fluency. in this paper, we propose a more lightweight approach that enables the PLM itself to achieve "self-detoxification". our method is built upon the observation that prepending a negative steering prompt can effectively induce PLMs to generate toxic content. at the same time, we are inspired by the recent research in the interpretability field, which formulates the evolving contextualized representations within the PLM as an information stream facilitated by the attention layers. drawing on this idea, we devise a method to identify the toxification direction from the normal generation process to the one prompted with the negative prefix, and then steer the generation to the reversed direction by manipulating the information movement within the attention layers. experimental results show that our approach, without any fine-tuning or extra components, can achieve comparable performance with state-of-the-art methods.Here's the translation in Traditional Chinese:语模型净化目标是为了降低预训练语言模型（PLM）发布时的风险，以确保更安全的应用。现有的方法可以大致分为调整基于和解oding基于两种。然而，前者经常需要资源投入，而后者则可能会妥协生成流畅性。在这篇论文中，我们提出了一种更轻量级的方法，让 PLM 本身能够实现 "自我净化"。我们的方法基于 prepending 负面引导预告可以导致 PLM 产生毒性内容的观察。同时，我们受到解释性研究的鼓励，它将 PLM 中的变化 contextualized 表示形式化为资讯流通过注意层。从这个想法开始，我们提出了一种方法来从 normal 生成过程中决定毒化方向，然后通过注意层中的资讯运动来驾驭生成。实验结果显示，我们的方法，不需要任何调整或额外元件，可以与现有的方法相比获得相似的性能。
</details></li>
</ul>
<hr>
<h2 id="Can-Large-Language-Model-Comprehend-Ancient-Chinese-A-Preliminary-Test-on-ACLUE"><a href="#Can-Large-Language-Model-Comprehend-Ancient-Chinese-A-Preliminary-Test-on-ACLUE" class="headerlink" title="Can Large Language Model Comprehend Ancient Chinese? A Preliminary Test on ACLUE"></a>Can Large Language Model Comprehend Ancient Chinese? A Preliminary Test on ACLUE</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09550">http://arxiv.org/abs/2310.09550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yixuan Zhang, Haonan Li</li>
<li>for: 评估大型自然语言处理模型（LLMs）在理解古代中文方面的能力。</li>
<li>methods: 使用ACLUE评价指标集，评测8种现代最佳语言模型在古代中文和现代中文之间的表现差异。</li>
<li>results: through 评测，发现其表现最佳的是ChatGLM2，得分为37.4%。<details>
<summary>Abstract</summary>
Large language models (LLMs) have showcased remarkable capabilities in understanding and generating language. However, their ability in comprehending ancient languages, particularly ancient Chinese, remains largely unexplored. To bridge this gap, we present ACLUE, an evaluation benchmark designed to assess the capability of language models in comprehending ancient Chinese. ACLUE consists of 15 tasks cover a range of skills, spanning phonetic, lexical, syntactic, semantic, inference and knowledge. Through the evaluation of eight state-of-the-art LLMs, we observed a noticeable disparity in their performance between modern Chinese and ancient Chinese. Among the assessed models, ChatGLM2 demonstrates the most remarkable performance, achieving an average score of 37.4%. We have made our code and data public available.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在理解和生成语言方面表现出了很好的能力，但它们对古代中文理解的能力仍然很少被探索。为了填补这一空白，我们提出了ACLUE评价指标，用于评估语言模型对古代中文的理解能力。ACLUE包括15个任务，覆盖了各种技能，包括声学、词汇、语法、 semantics、推理和知识。通过评估8种当今最先进的LLM，我们发现了这些模型对现代中文和古代中文的表现存在显著差异。其中，ChatGLM2表现最出色，其平均分为37.4%。我们已经将代码和数据公开。
</details></li>
</ul>
<hr>
<h2 id="CarExpert-Leveraging-Large-Language-Models-for-In-Car-Conversational-Question-Answering"><a href="#CarExpert-Leveraging-Large-Language-Models-for-In-Car-Conversational-Question-Answering" class="headerlink" title="CarExpert: Leveraging Large Language Models for In-Car Conversational Question Answering"></a>CarExpert: Leveraging Large Language Models for In-Car Conversational Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09536">http://arxiv.org/abs/2310.09536</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Rashad Al Hasan Rony, Christian Suess, Sinchana Ramakanth Bhat, Viju Sudhi, Julia Schneider, Maximilian Vogel, Roman Teucher, Ken E. Friedl, Soumya Sahoo</li>
<li>for: 本研究旨在提高大型自然语言模型（LLM）在域pecific问答中的表现，并解决现有LLM在域pecific问答中的限制。</li>
<li>methods: 本研究提出了一种名为CarExpert的在车辆中的问答系统，该系统利用LLM来控制输入、提供域pecific文档给抽取和生成答案组件，并控制输出以确保安全和域pecific答案。</li>
<li>results: 对比STATE-OF-THE-ART LLMs，CarExpert在生成自然、安全和车specific答案方面表现出色。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated remarkable performance by following natural language instructions without fine-tuning them on domain-specific tasks and data. However, leveraging LLMs for domain-specific question answering suffers from severe limitations. The generated answer tends to hallucinate due to the training data collection time (when using off-the-shelf), complex user utterance and wrong retrieval (in retrieval-augmented generation). Furthermore, due to the lack of awareness about the domain and expected output, such LLMs may generate unexpected and unsafe answers that are not tailored to the target domain. In this paper, we propose CarExpert, an in-car retrieval-augmented conversational question-answering system leveraging LLMs for different tasks. Specifically, CarExpert employs LLMs to control the input, provide domain-specific documents to the extractive and generative answering components, and controls the output to ensure safe and domain-specific answers. A comprehensive empirical evaluation exhibits that CarExpert outperforms state-of-the-art LLMs in generating natural, safe and car-specific answers.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）已经展现出了很好的性能，可以按照自然语言指令进行不需要精化的域pecific任务和数据上的执行。然而，使用LLM进行域pecific问答会遇到严重的限制。生成的答案往往会偏差，这是因为使用的数据采集时间（当用off-the-shelf）、复杂的用户语音和错误的检索（在检索增强生成中）。此外，由于缺乏域和期望输出的认知，这些LLM可能会生成不适用于目标域的不适用和不安全的答案。在这篇论文中，我们提出了CarExpert，一个基于LLM的在车辆内进行检索增强对话问答系统。具体来说，CarExpert使用LLM来控制输入、提供域pecific文档给抽取和生成答案组件，并控制输出，以确保安全和适用的答案。一项全面的实验证明了CarExpert在生成自然、安全和车辆特有的答案方面表现出优于状态之前的LLM。
</details></li>
</ul>
<hr>
<h2 id="Reward-Augmented-Decoding-Efficient-Controlled-Text-Generation-With-a-Unidirectional-Reward-Model"><a href="#Reward-Augmented-Decoding-Efficient-Controlled-Text-Generation-With-a-Unidirectional-Reward-Model" class="headerlink" title="Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model"></a>Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09520">http://arxiv.org/abs/2310.09520</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haikangdeng/RAD">https://github.com/haikangdeng/RAD</a></li>
<li>paper_authors: Haikang Deng, Colin Raffel</li>
<li>for: 这 paper 是为了提高语言模型生成文本的质量和特性。</li>
<li>methods: 这 paper 使用了 Reward-Augmented Decoding (RAD) 方法，这是一种基于小型单向奖励模型的文本生成方法，可以让语言模型生成文本有特定属性。RAD 使用奖励模型来评分生成过程中的每个步骤，并根据奖励值调整抽样概率，以增加高奖励的token。</li>
<li>results: 通过对生成非攻击性和情感控制文本进行实验，这 paper 表明 RAD 在不改变生成过程的情况下，可以达到最佳性和state-of-the-art方法的性能水平，并且可以在非常大的语言模型中实现最佳性，而且 computation overhead 很小。<details>
<summary>Abstract</summary>
While large language models have proven effective in a huge range of downstream applications, they often generate text that is problematic or lacks a desired attribute. In this paper, we introduce Reward-Augmented Decoding (RAD), a text generation procedure that uses a small unidirectional reward model to encourage a language model to generate text that has certain properties. Specifically, RAD uses the reward model to score generations as they are produced and rescales sampling probabilities to favor high-reward tokens. By using a unidirectional reward model, RAD can cache activations from prior generation steps to decrease computational overhead. Through experiments on generating non-toxic and sentiment-controlled text, we demonstrate that RAD performs best among methods that change only the generation procedure and matches the performance of state-of-the-art methods that involve re-training the language model. We further validate that RAD is effective on very large language models while incurring a minimal computational overhead.
</details>
<details>
<summary>摘要</summary>
大型语言模型已经证明在广泛的下游应用中效果很好，但它们通常生成的文本具有问题或缺乏欲有的特性。在这篇论文中，我们介绍了奖励增强解oding（RAD），一种文本生成 процедуre，使用小型单向奖励模型来鼓励语言模型生成具有特定特性的文本。具体来说，RAD使用奖励模型来评分生成过程中的生成，并将抽样概率重新调整以偏好高奖励的字元。使用单向奖励模型，RAD可以储存上一步生成的活化，以减少计算成本。通过实验，我们证明了RAD在生成非攻击性和情感控制的文本方面表现最佳，并与现有的语言模型重新训练方法相当。此外，我们还验证了RAD在巨大语言模型上的效果，而且计算成本几乎没有增加。
</details></li>
</ul>
<hr>
<h2 id="Attentive-Multi-Layer-Perceptron-for-Non-autoregressive-Generation"><a href="#Attentive-Multi-Layer-Perceptron-for-Non-autoregressive-Generation" class="headerlink" title="Attentive Multi-Layer Perceptron for Non-autoregressive Generation"></a>Attentive Multi-Layer Perceptron for Non-autoregressive Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09512">http://arxiv.org/abs/2310.09512</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shark-nlp/attentivemlp">https://github.com/shark-nlp/attentivemlp</a></li>
<li>paper_authors: Shuyang Jiang, Jun Zhang, Jiangtao Feng, Lin Zheng, Lingpeng Kong</li>
<li>for: 这 paper 的目的是提出一种高效的非自然语言生成模型，以解决非自然语言生成模型的效率问题。</li>
<li>methods: 该 paper 使用了一种新的多层感知机制 variant，称为 Attentive Multi-Layer Perceptron~(AMLP)，来生成高效的语言模型。 AMLP 使用了适应的投影矩阵，来模型语言模型中的关系。</li>
<li>results: 该 paper 的实验结果表明，AMLP 与其他高效的非自然语言生成模型相比，在文本生成和机器翻译等任务上具有显著的优势。 AMLP 的自我和交叉注意能力也被分别测试，并与其他高效的模型相比，得到了比较良好的结果。  Additionally, the paper also shows that AMLP has a significant reduction in memory cost compared to vanilla non-autoregressive models for long sequences.<details>
<summary>Abstract</summary>
Autoregressive~(AR) generation almost dominates sequence generation for its efficacy. Recently, non-autoregressive~(NAR) generation gains increasing popularity for its efficiency and growing efficacy. However, its efficiency is still bottlenecked by quadratic complexity in sequence lengths, which is prohibitive for scaling to long sequence generation and few works have been done to mitigate this problem. In this paper, we propose a novel MLP variant, \textbf{A}ttentive \textbf{M}ulti-\textbf{L}ayer \textbf{P}erceptron~(AMLP), to produce a generation model with linear time and space complexity. Different from classic MLP with static and learnable projection matrices, AMLP leverages adaptive projections computed from inputs in an attentive mode. The sample-aware adaptive projections enable communications among tokens in a sequence, and model the measurement between the query and key space. Furthermore, we marry AMLP with popular NAR models, deriving a highly efficient NAR-AMLP architecture with linear time and space complexity. Empirical results show that such marriage architecture surpasses competitive efficient NAR models, by a significant margin on text-to-speech synthesis and machine translation. We also test AMLP's self- and cross-attention ability separately with extensive ablation experiments, and find them comparable or even superior to the other efficient models. The efficiency analysis further shows that AMLP extremely reduces the memory cost against vanilla non-autoregressive models for long sequences.
</details>
<details>
<summary>摘要</summary>
自适应式生成（AR）技术在序列生成方面几乎占据了主导地位，而非自适应式生成（NAR）技术在最近几年得到了越来越多的关注，主要因为它的效率和生成能力在不断提高。然而，NAR技术的效率仍然受到序列长度的二次复杂性的限制，这使得扩展到长序列生成和少量工作已经做出了很多努力。在这篇论文中，我们提出了一种新的多层感知器（MLP）变体，称为注意力感知器（AMLP），以生成一个具有线性时间和空间复杂度的生成模型。与 класси型MLP的静态和学习投影矩阵不同，AMLP利用从输入中计算的适应投影。这些适应投影可以在序列中的各个元素之间进行通信，并且可以测量查询和关键空间之间的距离。此外，我们将AMLP与流行的NAR模型结合，得到了高效的NAR-AMLP架构，该架构具有线性时间和空间复杂度。实验结果表明，这种结合架构在文本到语音生成和机器翻译方面的性能明显高于竞争对手，并且我们还进行了详细的自注意力和交叉注意力能力测试，发现它们与其他高效的模型相当或者甚至更高。最后，我们还进行了内存成本分析，发现AMLP在长序列情况下可以极大地减少内存成本。
</details></li>
</ul>
<hr>
<h2 id="DepNeCTI-Dependency-based-Nested-Compound-Type-Identification-for-Sanskrit"><a href="#DepNeCTI-Dependency-based-Nested-Compound-Type-Identification-for-Sanskrit" class="headerlink" title="DepNeCTI: Dependency-based Nested Compound Type Identification for Sanskrit"></a>DepNeCTI: Dependency-based Nested Compound Type Identification for Sanskrit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09501">http://arxiv.org/abs/2310.09501</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yaswanth-iitkgp/depnecti">https://github.com/yaswanth-iitkgp/depnecti</a></li>
<li>paper_authors: Jivnesh Sandhan, Yaswanth Narsupalli, Sreevatsa Muppirala, Sriram Krishnan, Pavankumar Satuluri, Amba Kulkarni, Pawan Goyal</li>
<li>for: 本研究旨在提出一个新的任务：嵌入式多 компонент合成类型标识（NeCTI），以便理解多 component 合成中的隐藏结构和 semantics。</li>
<li>methods: 本研究使用了2个新的标注数据集，并对这些数据集进行了基线测试。然后，提出了一种新的框架名为 DepNeCTI，该框架基于依赖关系来实现嵌入式多 component 合成类型标识。</li>
<li>results: 对于 NeCTI 任务， DepNeCTI 框架在 Labeled Span Score (LSS) 方面的平均绝对改进率为 13.1 个 F1 分，并在推理效率方面实现了5倍的提高。此外，研究还发现了上下文对 NeCTI 任务的有利作用。<details>
<summary>Abstract</summary>
Multi-component compounding is a prevalent phenomenon in Sanskrit, and understanding the implicit structure of a compound's components is crucial for deciphering its meaning. Earlier approaches in Sanskrit have focused on binary compounds and neglected the multi-component compound setting. This work introduces the novel task of nested compound type identification (NeCTI), which aims to identify nested spans of a multi-component compound and decode the implicit semantic relations between them. To the best of our knowledge, this is the first attempt in the field of lexical semantics to propose this task.   We present 2 newly annotated datasets including an out-of-domain dataset for this task. We also benchmark these datasets by exploring the efficacy of the standard problem formulations such as nested named entity recognition, constituency parsing and seq2seq, etc. We present a novel framework named DepNeCTI: Dependency-based Nested Compound Type Identifier that surpasses the performance of the best baseline with an average absolute improvement of 13.1 points F1-score in terms of Labeled Span Score (LSS) and a 5-fold enhancement in inference efficiency. In line with the previous findings in the binary Sanskrit compound identification task, context provides benefits for the NeCTI task. The codebase and datasets are publicly available at: https://github.com/yaswanth-iitkgp/DepNeCTI
</details>
<details>
<summary>摘要</summary>
多Component合成是吠拜话中的普遍现象，理解合成元素的隐式结构是解译其意义的关键。 Earlier approaches in Sanskrit have focused on binary compounds and neglected the multi-component compound setting. This work introduces the novel task of nested compound type identification (NeCTI), which aims to identify nested spans of a multi-component compound and decode the implicit semantic relations between them. To the best of our knowledge, this is the first attempt in the field of lexical semantics to propose this task.   We present 2 newly annotated datasets including an out-of-domain dataset for this task. We also benchmark these datasets by exploring the efficacy of the standard problem formulations such as nested named entity recognition, constituency parsing and seq2seq, etc. We present a novel framework named DepNeCTI: Dependency-based Nested Compound Type Identifier that surpasses the performance of the best baseline with an average absolute improvement of 13.1 points F1-score in terms of Labeled Span Score (LSS) and a 5-fold enhancement in inference efficiency. In line with the previous findings in the binary Sanskrit compound identification task, context provides benefits for the NeCTI task. The codebase and datasets are publicly available at: https://github.com/yaswanth-iitkgp/DepNeCTI.Note: Please note that the translation is in Simplified Chinese, and the grammar and sentence structure may be different from Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Computational-analyses-of-linguistic-features-with-schizophrenic-and-autistic-traits-along-with-formal-thought-disorders"><a href="#Computational-analyses-of-linguistic-features-with-schizophrenic-and-autistic-traits-along-with-formal-thought-disorders" class="headerlink" title="Computational analyses of linguistic features with schizophrenic and autistic traits along with formal thought disorders"></a>Computational analyses of linguistic features with schizophrenic and autistic traits along with formal thought disorders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09494">http://arxiv.org/abs/2310.09494</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takeshi Saga, Hiroki Tanaka, Satoshi Nakamura</li>
<li>for: 这个研究是 investigate 哲思疾病 (FTD) 的表现在语言中，FTD 是 autism spectrum disorder (ASD) 和 schizophrenia 等疾病的一种表现。</li>
<li>methods: 这个研究使用了一个日本语音报告数据集，通过人员募集服务收集了有关 ASD 和 SPD 的分数标签。研究使用了社会响应度量表第二版 (SRS2) 和偏妄人格量表 (SPQ)，包括 SPQ 中的异常语言指标来评估语言特征。</li>
<li>results: 研究发现，异常语言指标与总 SPQ 和 SRS 分数显然相关，但两者自身不相关。异常语言指标 longer speech about negative memory 引起了更多 FTD 症状。减少研究表明，功能词和抽象特征对异常语言指标产生重要影响，而内容词只对 SRS 预测有效。这种结果表明 SPD 和 ASD 之间存在差异。数据和程序使用在这里：<a target="_blank" rel="noopener" href="https://sites.google.com/view/sagatake/resource">https://sites.google.com/view/sagatake/resource</a>.<details>
<summary>Abstract</summary>
[See full abstract in the pdf] Formal Thought Disorder (FTD), which is a group of symptoms in cognition that affects language and thought, can be observed through language. FTD is seen across such developmental or psychiatric disorders as Autism Spectrum Disorder (ASD) or Schizophrenia, and its related Schizotypal Personality Disorder (SPD). This paper collected a Japanese audio-report dataset with score labels related to ASD and SPD through a crowd-sourcing service from the general population. We measured language characteristics with the 2nd edition of the Social Responsiveness Scale (SRS2) and the Schizotypal Personality Questionnaire (SPQ), including an odd speech subscale from SPQ to quantify the FTD symptoms. We investigated the following four research questions through machine-learning-based score predictions: (RQ1) How are schizotypal and autistic measures correlated? (RQ2) What is the most suitable task to elicit FTD symptoms? (RQ3) Does the length of speech affect the elicitation of FTD symptoms? (RQ4) Which features are critical for capturing FTD symptoms? We confirmed that an FTD-related subscale, odd speech, was significantly correlated with both the total SPQ and SRS scores, although they themselves were not correlated significantly. Our regression analysis indicated that longer speech about a negative memory elicited more FTD symptoms. The ablation study confirmed the importance of function words and both the abstract and temporal features for FTD-related odd speech estimation. In contrast, content words were effective only in the SRS predictions, and content words were effective only in the SPQ predictions, a result that implies the differences between SPD-like and ASD-like symptoms. Data and programs used in this paper can be found here: https://sites.google.com/view/sagatake/resource.
</details>
<details>
<summary>摘要</summary>
[参考报告PDF]：正式思维障碍（FTD）是一组语言和思维方面的症状，可以通过语言来观察。FTD在发展或心理疾病中出现，如自闭症спектrum病（ASD）和 шизотипи性人格障碍（SPD）。这篇论文通过在日本的一个音频报告数据集上使用招募服务，收集了与ASD和SPD相关的语音报告数据。我们使用第2版社会响应度量表（SRS2）和 шизотипи性人格问卷（SPQ）进行语言特征量化，包括SPQ中的奇特语言子层，以评估FTD症状。我们提出了以下四个研究问题，通过机器学习基于分数预测来解答：（RQ1）ASD和SPD的度量相关吗？（RQ2）哪种任务可以最好Trigger FTD症状？（RQ3）长度的语言影响FTD症状的发现吗？（RQ4）FTD症状捕捉关键的特征是什么？我们发现，FTD相关的语言子层与总SPQ和SRS分数显著相关，但它们自身没有显著相关。我们的回归分析表明， longer speech about a negative memory elicited more FTD symptoms。减少学习中的函数词和抽象特征以及时间特征对FTD相关odd speech估算具有重要作用。相反，内容词只在SRS预测中有效，而内容词只在SPQ预测中有效，这种结果表明了ASD和SPD之间的差异。数据和程序可以在以下链接中找到：https://sites.google.com/view/sagatake/resource。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/14/cs.CL_2023_10_14/" data-id="cloh7tqeb00c57b884ja1au0f" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_10_14" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/14/cs.LG_2023_10_14/" class="article-date">
  <time datetime="2023-10-14T10:00:00.000Z" itemprop="datePublished">2023-10-14</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/14/cs.LG_2023_10_14/">cs.LG - 2023-10-14</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Towards-Semi-Structured-Automatic-ICD-Coding-via-Tree-based-Contrastive-Learning"><a href="#Towards-Semi-Structured-Automatic-ICD-Coding-via-Tree-based-Contrastive-Learning" class="headerlink" title="Towards Semi-Structured Automatic ICD Coding via Tree-based Contrastive Learning"></a>Towards Semi-Structured Automatic ICD Coding via Tree-based Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09672">http://arxiv.org/abs/2310.09672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chang Lu, Chandan K. Reddy, Ping Wang, Yue Ning</li>
<li>for: 提高现有ICD编码模型的性能，解决由医疗专业人员的写作习惯和病人的多种病理特征引起的医学记录的变化和有限的数据难题。</li>
<li>methods: 对医学记录进行自动分割，引入对比预训练策略，使用软多标签相似度度量基于树编辑距离进行预训练。还设计了遮盖部分训练策略，使ICD编码模型能够更好地定位相关的ICD代码段落。</li>
<li>results: 通过对存在限制数据的ICD编码模型进行预训练，提高了其性能。同时，通过遮盖部分训练策略，ICD编码模型能够更好地定位相关的ICD代码段落。<details>
<summary>Abstract</summary>
Automatic coding of International Classification of Diseases (ICD) is a multi-label text categorization task that involves extracting disease or procedure codes from clinical notes. Despite the application of state-of-the-art natural language processing (NLP) techniques, there are still challenges including limited availability of data due to privacy constraints and the high variability of clinical notes caused by different writing habits of medical professionals and various pathological features of patients. In this work, we investigate the semi-structured nature of clinical notes and propose an automatic algorithm to segment them into sections. To address the variability issues in existing ICD coding models with limited data, we introduce a contrastive pre-training approach on sections using a soft multi-label similarity metric based on tree edit distance. Additionally, we design a masked section training strategy to enable ICD coding models to locate sections related to ICD codes. Extensive experimental results demonstrate that our proposed training strategies effectively enhance the performance of existing ICD coding methods.
</details>
<details>
<summary>摘要</summary>
自动编码国际疾病分类（ICD）是一个多标签文本分类任务，它涉及提取医疗记录中的疾病或手术代码。尽管应用了当前最先进的自然语言处理（NLP）技术，仍然存在一些挑战，包括数据的有限可用性由隐私限制和医疗记录的高度变化，即医生们的不同写作风格和患者的多种疾病特征。在这项工作中，我们调查了医疗记录的半结构化特性，并提议一种自动分段算法。为了解决现有ICD编码模型受限于数据的变化问题，我们引入了一种对section进行预训练的对比预训练方法，并设计了一种遮盖section训练策略，以便ICD编码模型能够定位相关的section。我们的实验结果表明，我们的提议的训练策略有效地提高了现有ICD编码方法的表现。
</details></li>
</ul>
<hr>
<h2 id="A-Blockchain-empowered-Multi-Aggregator-Federated-Learning-Architecture-in-Edge-Computing-with-Deep-Reinforcement-Learning-Optimization"><a href="#A-Blockchain-empowered-Multi-Aggregator-Federated-Learning-Architecture-in-Edge-Computing-with-Deep-Reinforcement-Learning-Optimization" class="headerlink" title="A Blockchain-empowered Multi-Aggregator Federated Learning Architecture in Edge Computing with Deep Reinforcement Learning Optimization"></a>A Blockchain-empowered Multi-Aggregator Federated Learning Architecture in Edge Computing with Deep Reinforcement Learning Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09665">http://arxiv.org/abs/2310.09665</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao Li, Weili Wu</li>
<li>for: 本研究旨在提出一种基于区块链技术的多收集器Edge Federated Learning架构(BMA-FL),以提高 Edge Federated Learning 的安全性和效率。</li>
<li>methods: 本研究提出了一种新的轻量级的拜占庭同质性验证机制(PBCM)，用于在BMA-FL中实现安全和快速的模型归并和同步。此外，我们还提出了一种多智能深度强化学习算法，用于帮助收集器决定最佳训练策略。</li>
<li>results: 我们在实际数据集上进行了实验，并证明了BMA-FL可以更快地获得更好的模型，比基eline更高效。这表明了PBCM和我们提出的深度强化学习算法的有效性。<details>
<summary>Abstract</summary>
Federated learning (FL) is emerging as a sought-after distributed machine learning architecture, offering the advantage of model training without direct exposure of raw data. With advancements in network infrastructure, FL has been seamlessly integrated into edge computing. However, the limited resources on edge devices introduce security vulnerabilities to FL in the context. While blockchain technology promises to bolster security, practical deployment on resource-constrained edge devices remains a challenge. Moreover, the exploration of FL with multiple aggregators in edge computing is still new in the literature. Addressing these gaps, we introduce the Blockchain-empowered Heterogeneous Multi-Aggregator Federated Learning Architecture (BMA-FL). We design a novel light-weight Byzantine consensus mechanism, namely PBCM, to enable secure and fast model aggregation and synchronization in BMA-FL. We also dive into the heterogeneity problem in BMA-FL that the aggregators are associated with varied number of connected trainers with Non-IID data distributions and diverse training speed. We proposed a multi-agent deep reinforcement learning algorithm to help aggregators decide the best training strategies. The experiments on real-word datasets demonstrate the efficiency of BMA-FL to achieve better models faster than baselines, showing the efficacy of PBCM and proposed deep reinforcement learning algorithm.
</details>
<details>
<summary>摘要</summary>
《采用分布式机器学习架构的联邦学习（Federated Learning，FL）正在吸引越来越多的关注，因为它可以在不直接暴露原始数据的情况下进行模型训练。随着网络基础设施的提高，FL已经顺利地集成到边缘计算中。然而，边缘设备的限制性资源引入了FL在边缘计算环境中的安全漏洞。而区块链技术则承诺可以加强安全性，但是在资源有限的边缘设备上实际部署仍然是一大挑战。此外，现有的文献中对多个聚合器在边缘计算中的FLexploration还是新的。为了解决这些问题，我们介绍了区块链 empowered 多聚合器联邦学习架构（BMA-FL）。我们设计了一种轻量级的Byzantine共识机制，称为PBCM，以便在BMA-FL中安全快速地进行模型聚合和同步。我们还考虑了BMA-FL中聚合器与不同数据分布和多个连接的训练速度的多样性问题，并提出了一种基于多智能深度优化学习算法的解决方案。实验结果表明，BMA-FL可以更快地获得更好的模型，比基eline更高效。这显示了PBCM和我们提议的深度优化学习算法的有效性。》Note: The translation is done using Google Translate and may not be perfect. Please note that the translation is in Simplified Chinese, not Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Topology-guided-Hypergraph-Transformer-Network-Unveiling-Structural-Insights-for-Improved-Representation"><a href="#Topology-guided-Hypergraph-Transformer-Network-Unveiling-Structural-Insights-for-Improved-Representation" class="headerlink" title="Topology-guided Hypergraph Transformer Network: Unveiling Structural Insights for Improved Representation"></a>Topology-guided Hypergraph Transformer Network: Unveiling Structural Insights for Improved Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09657">http://arxiv.org/abs/2310.09657</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khaled Mohammed Saifuddin, Mehmet Emin Aktas, Esra Akbas</li>
<li>for: 本文旨在扩展传统图的概念，使用嵌入式的图神经网络（GNN）进行图表示学习，并且可以在高阶关系的图上进行表示学习。</li>
<li>methods: 本文提出了一种基于图的Topology-guided Hypergraph Transformer Network（THTN）模型，该模型首先将图转换成高阶图，然后使用简单 yet effective的结构和空间编码模块，将节点的表示增强，同时捕捉节点的本地和全局 topological 表达。</li>
<li>results: 对于节点分类任务，提出的模型表现比既有方法更好，可以更好地捕捉节点的本地和全局 topological 表达。<details>
<summary>Abstract</summary>
Hypergraphs, with their capacity to depict high-order relationships, have emerged as a significant extension of traditional graphs. Although Graph Neural Networks (GNNs) have remarkable performance in graph representation learning, their extension to hypergraphs encounters challenges due to their intricate structures. Furthermore, current hypergraph transformers, a special variant of GNN, utilize semantic feature-based self-attention, ignoring topological attributes of nodes and hyperedges. To address these challenges, we propose a Topology-guided Hypergraph Transformer Network (THTN). In this model, we first formulate a hypergraph from a graph while retaining its structural essence to learn higher-order relations within the graph. Then, we design a simple yet effective structural and spatial encoding module to incorporate the topological and spatial information of the nodes into their representation. Further, we present a structure-aware self-attention mechanism that discovers the important nodes and hyperedges from both semantic and structural viewpoints. By leveraging these two modules, THTN crafts an improved node representation, capturing both local and global topological expressions. Extensive experiments conducted on node classification tasks demonstrate that the performance of the proposed model consistently exceeds that of the existing approaches.
</details>
<details>
<summary>摘要</summary>
�� Hypergraphs, with their ability to depict high-order relationships, have emerged as a significant extension of traditional graphs. Although Graph Neural Networks (GNNs) have shown remarkable performance in graph representation learning, their extension to hypergraphs encounters challenges due to their complex structures. Furthermore, current hypergraph transformers, a special variant of GNN, use semantic feature-based self-attention, ignoring the topological attributes of nodes and hyperedges. To address these challenges, we propose a Topology-guided Hypergraph Transformer Network (THTN).In this model, we first formulate a hypergraph from a graph while retaining its structural essence to learn higher-order relations within the graph. Then, we design a simple yet effective structural and spatial encoding module to incorporate the topological and spatial information of the nodes into their representation. Further, we present a structure-aware self-attention mechanism that discovers the important nodes and hyperedges from both semantic and structural viewpoints. By leveraging these two modules, THTN crafts an improved node representation, capturing both local and global topological expressions.Extensive experiments conducted on node classification tasks demonstrate that the performance of the proposed model consistently exceeds that of the existing approaches.
</details></li>
</ul>
<hr>
<h2 id="Mixed-Type-Tabular-Data-Synthesis-with-Score-based-Diffusion-in-Latent-Space"><a href="#Mixed-Type-Tabular-Data-Synthesis-with-Score-based-Diffusion-in-Latent-Space" class="headerlink" title="Mixed-Type Tabular Data Synthesis with Score-based Diffusion in Latent Space"></a>Mixed-Type Tabular Data Synthesis with Score-based Diffusion in Latent Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09656">http://arxiv.org/abs/2310.09656</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amazon-science/tabsyn">https://github.com/amazon-science/tabsyn</a></li>
<li>paper_authors: Hengrui Zhang, Jiani Zhang, Balasubramaniam Srinivasan, Zhengyuan Shen, Xiao Qin, Christos Faloutsos, Huzefa Rangwala, George Karypis</li>
<li>for: 本文旨在提出一种能够生成高质量的 tabular 数据的方法，以满足不同数据类型的混合和复杂的分布特性。</li>
<li>methods: 本文提出了一种基于 variational autoencoder (VAE) 的 diffusion 模型，可以将不同数据类型转化为单一的空间，并显式地捕捉列之间的关系。</li>
<li>results: 对 six 个数据集进行了广泛的实验，并证明了 TABSYN 可以在 column-wise 分布和列对卷积关系的估计中具有更高的准确率，比如 existing 方法的 86% 和 67%。<details>
<summary>Abstract</summary>
Recent advances in tabular data generation have greatly enhanced synthetic data quality. However, extending diffusion models to tabular data is challenging due to the intricately varied distributions and a blend of data types of tabular data. This paper introduces TABSYN, a methodology that synthesizes tabular data by leveraging a diffusion model within a variational autoencoder (VAE) crafted latent space. The key advantages of the proposed TABSYN include (1) Generality: the ability to handle a broad spectrum of data types by converting them into a single unified space and explicitly capture inter-column relations; (2) Quality: optimizing the distribution of latent embeddings to enhance the subsequent training of diffusion models, which helps generate high-quality synthetic data, (3) Speed: much fewer number of reverse steps and faster synthesis speed than existing diffusion-based methods. Extensive experiments on six datasets with five metrics demonstrate that TABSYN outperforms existing methods. Specifically, it reduces the error rates by 86% and 67% for column-wise distribution and pair-wise column correlation estimations compared with the most competitive baselines.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>通用性: 能够处理广泛的数据类型，将它们转换为单一的空间，并且显著地捕捉列之间的关系。2. 质量: 优化纹理中的 latent embedding 分布，以提高 diffusion 模型的训练，从而生成高质量的 sintetic data。3. 速度: 比现有的 diffusion-based 方法快得多，减少了反向步骤的数量，提高了生成速度。广泛的实验表明，TABSYN 在六个 dataset 上的五个指标上都超过了现有的方法。具体来说，它相比最竞争的基准值，降低了列级分布和对列相关性的估计错误率 by 86% 和 67%。</details></li>
</ol>
<hr>
<h2 id="DPZero-Dimension-Independent-and-Differentially-Private-Zeroth-Order-Optimization"><a href="#DPZero-Dimension-Independent-and-Differentially-Private-Zeroth-Order-Optimization" class="headerlink" title="DPZero: Dimension-Independent and Differentially Private Zeroth-Order Optimization"></a>DPZero: Dimension-Independent and Differentially Private Zeroth-Order Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09639">http://arxiv.org/abs/2310.09639</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liang Zhang, Kiran Koshy Thekumparampil, Sewoong Oh, Niao He</li>
<li>for: 这篇论文目的是解决对于执行大型自然语言模型（LLM）的微调问题，具体是处理内存和隐私问题。</li>
<li>methods: 这篇论文使用了零次方法来进行斜方向优化，这些方法仅从前进推导，因此可以大大减少训练时间的内存负载。然而，直接结合标准的隐私机制和零次方法会增加维度相依的复杂性。</li>
<li>results: 这篇论文提出了一个名为DPZero的新的隐私条件下的零次方法，它的复杂度仅对问题的内部维度有很强的依赖性，实际上可以实现高效的应用。<details>
<summary>Abstract</summary>
The widespread practice of fine-tuning pretrained large language models (LLMs) on domain-specific data faces two major challenges in memory and privacy. First, as the size of LLMs continue to grow, encompassing billions of parameters, the memory demands of gradient-based training methods via backpropagation become prohibitively high. Second, given the tendency of LLMs to memorize and disclose sensitive training data, the privacy of fine-tuning data must be respected. To this end, we explore the potential of zeroth-order methods in differentially private optimization for fine-tuning LLMs. Zeroth-order methods, which rely solely on forward passes, substantially reduce memory consumption during training. However, directly combining them with standard differential privacy mechanism poses dimension-dependent complexity. To bridge the gap, we introduce DPZero, a novel differentially private zeroth-order algorithm with nearly dimension-independent rates. Our theoretical analysis reveals that its complexity hinges primarily on the problem's intrinsic dimension and exhibits only a logarithmic dependence on the ambient dimension. This renders DPZero a highly practical option for real-world LLMs deployments.
</details>
<details>
<summary>摘要</summary>
广泛的专业化大型语言模型（LLM）微调问题面临两大挑战：首先，随着 LLM 的大小不断增长，使用反对推导法（backpropagation）进行梯度下降课题的记忆需求变得禁制高昂。其次，由于 LLM 倾向于记忆和泄露敏感训练数据，因此训练数据的隐私必须受到尊重。为此，我们探索了零顺序方法在不同数据隐私优化中的潜力。零顺序方法，它仅靠前进通过，可以实现严重降低训练中的记忆消耗。但是，直接结合它们与标准的隐私机制会带来维度相依的复杂性。为了填补这个差距，我们提出了 DPZero，一种基于零顺序方法的不同数据隐私优化算法。我们的理论分析显示，DPZero 的复杂度仅受到问题的内在维度的影响，并且只具有对数对应的依赖性。这使得 DPZero 在实际应用中成为了非常实用的选择。
</details></li>
</ul>
<hr>
<h2 id="Generative-Adversarial-Training-for-Text-to-Speech-Synthesis-Based-on-Raw-Phonetic-Input-and-Explicit-Prosody-Modelling"><a href="#Generative-Adversarial-Training-for-Text-to-Speech-Synthesis-Based-on-Raw-Phonetic-Input-and-Explicit-Prosody-Modelling" class="headerlink" title="Generative Adversarial Training for Text-to-Speech Synthesis Based on Raw Phonetic Input and Explicit Prosody Modelling"></a>Generative Adversarial Training for Text-to-Speech Synthesis Based on Raw Phonetic Input and Explicit Prosody Modelling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09636">http://arxiv.org/abs/2310.09636</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tiberiu44/TTS-Cube">https://github.com/tiberiu44/TTS-Cube</a></li>
<li>paper_authors: Tiberiu Boros, Stefan Daniel Dumitrescu, Ionut Mironica, Radu Chivereanu</li>
<li>for: 这个论文描述了一个端到端的语音合成系统，使用生成对抗训练。</li>
<li>methods: 这个系统使用生成对抗训练来训练其 vocoder，用于 raw phoneme-to-audio 转换，并使用显式的 fonetic、抑制和持续时间模型。</li>
<li>results: 研究人员通过对多个预训练模型进行实验，并引入了一种新的高度表达式的字符声音匹配方法，基于独特的风格标识符。<details>
<summary>Abstract</summary>
We describe an end-to-end speech synthesis system that uses generative adversarial training. We train our Vocoder for raw phoneme-to-audio conversion, using explicit phonetic, pitch and duration modeling. We experiment with several pre-trained models for contextualized and decontextualized word embeddings and we introduce a new method for highly expressive character voice matching, based on discreet style tokens.
</details>
<details>
<summary>摘要</summary>
我们描述了一个端到端的语音合成系统，使用生成对抗训练。我们用raw phoneme-to-audio转换器进行辅助训练，使用显式的音位、抑制和持续时间模型。我们对几种预训练的词袋模型进行实验，并 introduce一种新的高度表达式的字符声音匹配方法，基于精细的风格то克。
</details></li>
</ul>
<hr>
<h2 id="Landslide-Topology-Uncovers-Failure-Movements"><a href="#Landslide-Topology-Uncovers-Failure-Movements" class="headerlink" title="Landslide Topology Uncovers Failure Movements"></a>Landslide Topology Uncovers Failure Movements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09631">http://arxiv.org/abs/2310.09631</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kamal Rana, Kushanav Bhuyan, Joaquin Vicente Ferrer, Fabrice Cotton, Ugur Ozturk, Filippo Catani, Nishant Malik<br>for:* The paper aims to improve the accuracy of landslide predictive models and impact assessments by identifying failure types based on their movements.methods:* The approach uses 3D landslide topology to identify failure types, such as slides and flows, by analyzing topological proxies that reveal the mechanics of mass movements.results:* The approach achieves 80 to 94% accuracy in identifying failure types in historic and event-specific landslide databases from various geomorphological and climatic contexts.<details>
<summary>Abstract</summary>
The death toll and monetary damages from landslides continue to rise despite advancements in predictive modeling. The predictive capability of these models is limited as landslide databases used in training and assessing the models often have crucial information missing, such as underlying failure types. Here, we present an approach for identifying failure types based on their movements, e.g., slides and flows by leveraging 3D landslide topology. We observe topological proxies reveal prevalent signatures of mass movement mechanics embedded in the landslide's morphology or shape, such as detecting coupled movement styles within complex landslides. We find identical failure types exhibit similar topological properties, and by using them as predictors, we can identify failure types in historic and event-specific landslide databases (including multi-temporal) from various geomorphological and climatic contexts such as Italy, the US Pacific Northwest region, Denmark, Turkey, and China with 80 to 94 % accuracy. To demonstrate the real-world application of the method, we implement it in two undocumented datasets from China and publicly release the datasets. These new insights can considerably improve the performance of landslide predictive models and impact assessments. Moreover, our work introduces a new paradigm for studying landslide shapes to understand underlying processes through the lens of landslide topology.
</details>
<details>
<summary>摘要</summary>
“死亡人数和经济损害由滑坡继续增加，尽管预测模型的技术已经得到进步。预测模型的能力受到数据库中的信息损失的限制，这些数据库通常缺乏关键信息，如滑坡类型。我们提出了一种方法，利用滑坡三维 topology 来识别滑坡类型，如滑坡和流体。我们发现了类似的滑坡类型具有相似的 topological 特征，并使用这些特征作为预测器，可以准确地识别 historic 和事件特定的滑坡数据库（包括多时间），从不同的地质和气候背景中获得 80-94% 的准确率。为证明这种方法的实际应用，我们在中国两个未文件的数据集中实现了它，并公开发布了这些数据集。这些新的发现可以明显提高滑坡预测模型的性能和影响评估。此外，我们的工作还 introduce 了一种新的理解滑坡形态的方法，通过滑坡 topology 来理解滑坡的下面过程。”Note: Please note that the translation is in Simplified Chinese, which is used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Federated-Battery-Diagnosis-and-Prognosis"><a href="#Federated-Battery-Diagnosis-and-Prognosis" class="headerlink" title="Federated Battery Diagnosis and Prognosis"></a>Federated Battery Diagnosis and Prognosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09628">http://arxiv.org/abs/2310.09628</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nur Banu Altinpulluk, Deniz Altinpulluk, Paritosh Ramanan, Noah Paulson, Feng Qiu, Susan Babinec, Murat Yildirim</li>
<li>for: 这个研究旨在提出一个分布式电池诊断和预测模型，以解决现有的数据拥有权、隐私、通信和处理等问题。</li>
<li>methods: 我们提出了一个分布式电池诊断和预测模型，将标准电流电压时间数据分布式处理，仅将模型参数通信，减少通信负载并保持数据隐私。</li>
<li>results: 我们的模型可以实现隐私保护的分布式电池诊断和预测，并且可以预测电池剩余寿命。这个模型将为电池健康管理带来一个新的思维方向。<details>
<summary>Abstract</summary>
Battery diagnosis, prognosis and health management models play a critical role in the integration of battery systems in energy and mobility fields. However, large-scale deployment of these models is hindered by a myriad of challenges centered around data ownership, privacy, communication, and processing. State-of-the-art battery diagnosis and prognosis methods require centralized collection of data, which further aggravates these challenges. Here we propose a federated battery prognosis model, which distributes the processing of battery standard current-voltage-time-usage data in a privacy-preserving manner. Instead of exchanging raw standard current-voltage-time-usage data, our model communicates only the model parameters, thus reducing communication load and preserving data confidentiality. The proposed model offers a paradigm shift in battery health management through privacy-preserving distributed methods for battery data processing and remaining lifetime prediction.
</details>
<details>
<summary>摘要</summary>
锂电池诊断、预测和健康管理模型在能源和交通领域的集成中扮演了关键角色。然而，大规模部署这些模型受到数据所有权、隐私、通信和处理等多种挑战。现状的锂电池诊断和预测方法均需要集中收集数据，这进一步增加了这些挑战。我们提议一种联邦锂电池预测模型，该模型在隐私保护的方式下分布式处理标准电压电流时间使用数据。而不是交换Raw标准电压电流时间数据，我们的模型仅交换模型参数，从而降低了通信负担和保持了数据Confidentiality。我们提出的模型将锂电池健康管理领域带来隐私保护分布式处理锂电池数据的新模式，并提高锂电池剩余寿命预测的精度。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-for-Urban-Air-Quality-Analytics-A-Survey"><a href="#Machine-Learning-for-Urban-Air-Quality-Analytics-A-Survey" class="headerlink" title="Machine Learning for Urban Air Quality Analytics: A Survey"></a>Machine Learning for Urban Air Quality Analytics: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09620">http://arxiv.org/abs/2310.09620</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jindong Han, Weijia Zhang, Hao Liu, Hui Xiong</li>
<li>for: 这篇论文主要目标是探讨机器学习（ML）技术在空气质量分析领域的应用，以提供一份全面的报告，帮助专业人士寻找适合自己的问题和进行前沿研究。</li>
<li>methods: 本论文使用的方法包括数据收集、数据预处理、机器学习模型的应用等，涵盖了多种空气质量分析任务，如污染 patrern mining、空气质量推断和预测等。</li>
<li>results: 本论文提供了一系列已有的空气质量分析任务的综述和分类，同时还提供了一些已有的公共空气质量数据集，以便进一步研究。此外，文章还预测了未来研究的一些可能的方向。<details>
<summary>Abstract</summary>
The increasing air pollution poses an urgent global concern with far-reaching consequences, such as premature mortality and reduced crop yield, which significantly impact various aspects of our daily lives. Accurate and timely analysis of air pollution is crucial for understanding its underlying mechanisms and implementing necessary precautions to mitigate potential socio-economic losses. Traditional analytical methodologies, such as atmospheric modeling, heavily rely on domain expertise and often make simplified assumptions that may not be applicable to complex air pollution problems. In contrast, Machine Learning (ML) models are able to capture the intrinsic physical and chemical rules by automatically learning from a large amount of historical observational data, showing great promise in various air quality analytical tasks. In this article, we present a comprehensive survey of ML-based air quality analytics, following a roadmap spanning from data acquisition to pre-processing, and encompassing various analytical tasks such as pollution pattern mining, air quality inference, and forecasting. Moreover, we offer a systematic categorization and summary of existing methodologies and applications, while also providing a list of publicly available air quality datasets to ease the research in this direction. Finally, we identify several promising future research directions. This survey can serve as a valuable resource for professionals seeking suitable solutions for their specific challenges and advancing their research at the cutting edge.
</details>
<details>
<summary>摘要</summary>
这个增长的空气污染问题具有急迫的全球性，带来许多深远的后果，如提早死亡和减少的农作物生产，这些影响了我们日常生活的多方面。精确和时间的空气污染分析是理解其下面的机制和适当的预防措施，以减少可能的社会经济损失。传统的分析方法，如大气模型，严重依赖专家知识和假设，可能无法应对复杂的空气污染问题。相比之下，机器学习（ML）模型能够自动从历史观测数据中学习出空气污染的内在物理和化学规律，显示了它们在不同的空气质量分析任务中的杰出应用潜力。在这篇文章中，我们提供了一个概要的机器学习基于空气质量分析的调查，包括数据收集到预处理的步骤，以及各种分析任务，如污染图像探索、空气质量推断和预测。此外，我们提供了现有的方法和应用的系统性概括和摘要，同时提供了访问公共空气质量数据的列表，以便进一步研究这个方向。最后，我们点出了未来研究的一些有前途的方向。这篇调查可以作为专业人员寻找适合他们特定挑战的适当解决方案，并为他们的研究进一步发展到边缘领域。
</details></li>
</ul>
<hr>
<h2 id="STORM-Efficient-Stochastic-Transformer-based-World-Models-for-Reinforcement-Learning"><a href="#STORM-Efficient-Stochastic-Transformer-based-World-Models-for-Reinforcement-Learning" class="headerlink" title="STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning"></a>STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09615">http://arxiv.org/abs/2310.09615</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/weipu-zhang/storm">https://github.com/weipu-zhang/storm</a></li>
<li>paper_authors: Weipu Zhang, Gang Wang, Jian Sun, Yetian Yuan, Gao Huang</li>
<li>for: 本研究旨在提高模型基 Reinforcement learning 算法在视觉输入环境中的表现。</li>
<li>methods: 该方法首先通过自我超vised learning 构建一个参数化的 simulate 世界模型，然后利用这个模型来提高 Agent 的策略。</li>
<li>results: 该方法可以达到人类水平的表现($126.7%$ 的 Atari $100$k 评估标准)，并且比之前的方法更加高效。<details>
<summary>Abstract</summary>
Recently, model-based reinforcement learning algorithms have demonstrated remarkable efficacy in visual input environments. These approaches begin by constructing a parameterized simulation world model of the real environment through self-supervised learning. By leveraging the imagination of the world model, the agent's policy is enhanced without the constraints of sampling from the real environment. The performance of these algorithms heavily relies on the sequence modeling and generation capabilities of the world model. However, constructing a perfectly accurate model of a complex unknown environment is nearly impossible. Discrepancies between the model and reality may cause the agent to pursue virtual goals, resulting in subpar performance in the real environment. Introducing random noise into model-based reinforcement learning has been proven beneficial. In this work, we introduce Stochastic Transformer-based wORld Model (STORM), an efficient world model architecture that combines the strong sequence modeling and generation capabilities of Transformers with the stochastic nature of variational autoencoders. STORM achieves a mean human performance of $126.7\%$ on the Atari $100$k benchmark, setting a new record among state-of-the-art methods that do not employ lookahead search techniques. Moreover, training an agent with $1.85$ hours of real-time interaction experience on a single NVIDIA GeForce RTX 3090 graphics card requires only $4.3$ hours, showcasing improved efficiency compared to previous methodologies.
</details>
<details>
<summary>摘要</summary>
近些时候，基于模型的强化学习算法在视觉输入环境中表现出色。这些方法首先通过自动学习建立一个参数化的模拟世界模型，然后利用模型的想像力提高代理人的策略，不受实际环境的采样限制。然而，建立一个完全准确的模型是几乎不可能的。模型和现实之间的差异可能导致代理人追求虚拟目标，从而导致实际环境的性能下降。在这种情况下，引入随机噪声到模型基于强化学习已经证明有利。在这项工作中，我们提出了 Stochastic Transformer-based wORld Model（STORM），这是一种高效的世界模型架构，它将Transformers强大的序列模型和生成能力与变量自动编码器的随机性结合起来。STORM在Atari 100k 测试benchmark上达到了人类性能的mean值 $126.7\%$，创下了没有使用lookahead搜索技术的新纪录。此外，通过实际时间互动体验的1.85小时，我们只需要4.3小时的训练时间，这显示了与前一代方法相比的更高效性。
</details></li>
</ul>
<hr>
<h2 id="Towards-Intelligent-Network-Management-Leveraging-AI-for-Network-Service-Detection"><a href="#Towards-Intelligent-Network-Management-Leveraging-AI-for-Network-Service-Detection" class="headerlink" title="Towards Intelligent Network Management: Leveraging AI for Network Service Detection"></a>Towards Intelligent Network Management: Leveraging AI for Network Service Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09609">http://arxiv.org/abs/2310.09609</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khuong N. Nguyen, Abhishek Sehgal, Yuming Zhu, Junsu Choi, Guanbo Chen, Hao Chen, Boon Loong Ng, Charlie Zhang</li>
<li>for: 这个研究旨在开发一个高级的网络流量分类系统，以便在现代无线通讯网络中进行精确的流量分析。</li>
<li>methods: 本研究使用机器学习方法ologies来分析网络流量，并将其分为不同的网络服务类型。我们的方法包括识别网络流量中的对应类型，并将其分为多个小型流量流。我们使用机器学习模型来训练这些服务类型。</li>
<li>results: 我们的研究结果显示，我们的方法可以实现高度的精确性，并且可以在不同的无线网络条件下进行类型化。这些结果显示了机器学习在无线技术中的应用潜力。<details>
<summary>Abstract</summary>
As the complexity and scale of modern computer networks continue to increase, there has emerged an urgent need for precise traffic analysis, which plays a pivotal role in cutting-edge wireless connectivity technologies. This study focuses on leveraging Machine Learning methodologies to create an advanced network traffic classification system. We introduce a novel data-driven approach that excels in identifying various network service types in real-time, by analyzing patterns within the network traffic. Our method organizes similar kinds of network traffic into distinct categories, referred to as network services, based on latency requirement. Furthermore, it decomposes the network traffic stream into multiple, smaller traffic flows, with each flow uniquely carrying a specific service. Our ML models are trained on a dataset comprised of labeled examples representing different network service types collected on various Wi-Fi network conditions. Upon evaluation, our system demonstrates a remarkable accuracy in distinguishing the network services. These results emphasize the substantial promise of integrating Artificial Intelligence in wireless technologies. Such an approach encourages more efficient energy consumption, enhances Quality of Service assurance, and optimizes the allocation of network resources, thus laying a solid groundwork for the development of advanced intelligent networks.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)随着现代计算机网络的复杂性和规模不断增加，需要精准的网络流量分析已成为现代无线连接技术的急需。本研究利用机器学习方法创建高级网络流量分类系统。我们提出了一种新的数据驱动方法，可以在实时中识别不同的网络服务类型，通过分析网络流量中的模式。我们的方法将类似的网络流量分成不同类别，称为网络服务，基于延迟需求。此外，它还将网络流量流分为多个更小的流量流，每个流唯一携带特定的服务。我们的ML模型在各种Wi-Fi网络条件下收集的标注示例集上进行训练。评估结果显示，我们的系统在分类网络服务时表现出了很高的准确率。这些结果强调了将人工智能 integrating into wireless technologies 的极大承诺，这种方法可以提高能效的能源消耗，提高服务质量保证，并优化网络资源的分配，从而为高级智能网络的发展提供坚实的基础。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-maximization-of-social-welfare"><a href="#Adaptive-maximization-of-social-welfare" class="headerlink" title="Adaptive maximization of social welfare"></a>Adaptive maximization of social welfare</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09597">http://arxiv.org/abs/2310.09597</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolo Cesa-Bianchi, Roberto Colomboni, Maximilian Kasy</li>
<li>for: 本文研究了 repeatedly choosing policies 以 maximize social welfare.</li>
<li>methods: 文章使用了 experimentation 学习 response functions, 并 derive 了 regret bound.</li>
<li>results: 文章证明了 regret Rate of $T^{2&#x2F;3}$, 这意味着 (i) welfare maximization 比 multi-armed bandit problem 更加困难, 并且 (ii) 算法实现了最佳率。 如果社会利益是凹形的, 可以使用 dyadic search algorithm 实现 $T^{1&#x2F;2}$ 率。<details>
<summary>Abstract</summary>
We consider the problem of repeatedly choosing policies to maximize social welfare. Welfare is a weighted sum of private utility and public revenue. Earlier outcomes inform later policies. Utility is not observed, but indirectly inferred. Response functions are learned through experimentation.   We derive a lower bound on regret, and a matching adversarial upper bound for a variant of the Exp3 algorithm. Cumulative regret grows at a rate of $T^{2/3}$. This implies that (i) welfare maximization is harder than the multi-armed bandit problem (with a rate of $T^{1/2}$ for finite policy sets), and (ii) our algorithm achieves the optimal rate. For the stochastic setting, if social welfare is concave, we can achieve a rate of $T^{1/2}$ (for continuous policy sets), using a dyadic search algorithm.   We analyze an extension to nonlinear income taxation, and sketch an extension to commodity taxation. We compare our setting to monopoly pricing (which is easier), and price setting for bilateral trade (which is harder).
</details>
<details>
<summary>摘要</summary>
我们考虑了 repeatedly choosing policies 以 maximize social welfare。 social welfare 是一个 weighted sum of private utility 和 public revenue。earlier outcomes inform later policies。 utility 不能 directly observed，而是 indirectly inferred。response functions 是通过 experimentation 学习的。我们 derivated a lower bound on regret, and a matching adversarial upper bound for a variant of the Exp3 algorithm。cumulative regret grows at a rate of $T^{2/3}$.这意味着 (i) welfare maximization 比 multi-armed bandit problem 更难（with a rate of $T^{1/2}$ for finite policy sets），和 (ii) our algorithm achieves the optimal rate。对于随机设置，如果 social welfare 是凹形函数，我们可以使用 dyadic search algorithm  achiev a rate of $T^{1/2}$ (for continuous policy sets)。我们还分析了 nonlinear income taxation 的扩展，并略 outline commodity taxation 的扩展。我们比较了我们的设置与 monopoly pricing 和 bilateral trade 的价格设置。Note: Please note that the translation is in Simplified Chinese, and some words or phrases may have different translations in Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Causality-and-Independence-Enhancement-for-Biased-Node-Classification"><a href="#Causality-and-Independence-Enhancement-for-Biased-Node-Classification" class="headerlink" title="Causality and Independence Enhancement for Biased Node Classification"></a>Causality and Independence Enhancement for Biased Node Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09586">http://arxiv.org/abs/2310.09586</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chen-gx/cie">https://github.com/chen-gx/cie</a></li>
<li>paper_authors: Guoxin Chen, Yongqing Wang, Fangda Guo, Qinglang Guo, Jiangli Shao, Huawei Shen, Xueqi Cheng</li>
<li>For: The paper aims to address the problem of out-of-distribution (OOD) generalization for node classification on graphs, specifically focusing on mixed biases and low-resource scenarios.* Methods: The proposed Causality and Independence Enhancement (CIE) framework estimates causal and spurious features at the node representation level, mitigates the influence of spurious correlations through backdoor adjustment, and introduces independence constraint to improve discriminability and stability of causal and spurious features.* Results: The proposed CIE approach significantly enhances the performance of graph neural networks (GNNs) and outperforms state-of-the-art debiased node classification methods in various scenarios, including specific types of data biases, mixed biases, and low-resource scenarios.<details>
<summary>Abstract</summary>
Most existing methods that address out-of-distribution (OOD) generalization for node classification on graphs primarily focus on a specific type of data biases, such as label selection bias or structural bias. However, anticipating the type of bias in advance is extremely challenging, and designing models solely for one specific type may not necessarily improve overall generalization performance. Moreover, limited research has focused on the impact of mixed biases, which are more prevalent and demanding in real-world scenarios. To address these limitations, we propose a novel Causality and Independence Enhancement (CIE) framework, applicable to various graph neural networks (GNNs). Our approach estimates causal and spurious features at the node representation level and mitigates the influence of spurious correlations through the backdoor adjustment. Meanwhile, independence constraint is introduced to improve the discriminability and stability of causal and spurious features in complex biased environments. Essentially, CIE eliminates different types of data biases from a unified perspective, without the need to design separate methods for each bias as before. To evaluate the performance under specific types of data biases, mixed biases, and low-resource scenarios, we conducted comprehensive experiments on five publicly available datasets. Experimental results demonstrate that our approach CIE not only significantly enhances the performance of GNNs but outperforms state-of-the-art debiased node classification methods.
</details>
<details>
<summary>摘要</summary>
现有的方法主要对应类别数据上的特定类型偏见，如标签选择偏见或结构偏见。但是，预测偏见类型在进程中很困难，设计对一种特定类型的模型可能不会改善整体普遍化性能。另外，有限的研究集中在混合偏见的影响，这些偏见在实际场景中更为普遍和复杂。为了解决这些限制，我们提出了一个新的 causality and independence enhancement（CIE）框架，可以应用于多种图像神经网络（GNNs）。我们的方法在节点表现层估计 causal 和误假特征，并通过反门侧调整减少误假相关性。同时，我们引入独立性限制，以提高统计特征的检测性和稳定性在复杂偏见环境中。简而言之，CIE 可以从一个统一的角度消除不同类型的数据偏见，不需要在进程中设计每种偏见的特别方法。为了评估在特定类型的数据偏见、混合偏见、低资源情况下的表现，我们进行了广泛的实验，结果显示了我们的 CIE 方法不仅能够明显提高 GNN 的表现，而且超过了目前的调整类别数据方法的表现。
</details></li>
</ul>
<hr>
<h2 id="Two-Sides-of-The-Same-Coin-Bridging-Deep-Equilibrium-Models-and-Neural-ODEs-via-Homotopy-Continuation"><a href="#Two-Sides-of-The-Same-Coin-Bridging-Deep-Equilibrium-Models-and-Neural-ODEs-via-Homotopy-Continuation" class="headerlink" title="Two Sides of The Same Coin: Bridging Deep Equilibrium Models and Neural ODEs via Homotopy Continuation"></a>Two Sides of The Same Coin: Bridging Deep Equilibrium Models and Neural ODEs via Homotopy Continuation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09583">http://arxiv.org/abs/2310.09583</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shutong Ding, Tianyu Cui, Jingya Wang, Ye Shi</li>
<li>for: 这个论文主要目标是提出一种新的隐式模型，即HomODE，以解决隐式模型中的稳点问题。</li>
<li>methods: 这个论文使用了homotopy continuation方法来解决隐式模型中的稳点问题，并且开发了一种加速方法来提高模型的性能。</li>
<li>results: 实验结果表明，HomODE可以在图像分类任务中超过现有的隐式模型，并且具有更好的稳定性和更低的内存占用率。<details>
<summary>Abstract</summary>
Deep Equilibrium Models (DEQs) and Neural Ordinary Differential Equations (Neural ODEs) are two branches of implicit models that have achieved remarkable success owing to their superior performance and low memory consumption. While both are implicit models, DEQs and Neural ODEs are derived from different mathematical formulations. Inspired by homotopy continuation, we establish a connection between these two models and illustrate that they are actually two sides of the same coin. Homotopy continuation is a classical method of solving nonlinear equations based on a corresponding ODE. Given this connection, we proposed a new implicit model called HomoODE that inherits the property of high accuracy from DEQs and the property of stability from Neural ODEs. Unlike DEQs, which explicitly solve an equilibrium-point-finding problem via Newton's methods in the forward pass, HomoODE solves the equilibrium-point-finding problem implicitly using a modified Neural ODE via homotopy continuation. Further, we developed an acceleration method for HomoODE with a shared learnable initial point. It is worth noting that our model also provides a better understanding of why Augmented Neural ODEs work as long as the augmented part is regarded as the equilibrium point to find. Comprehensive experiments with several image classification tasks demonstrate that HomoODE surpasses existing implicit models in terms of both accuracy and memory consumption.
</details>
<details>
<summary>摘要</summary>
深度平衡模型（DEQs）和神经常微方程（Neural ODEs）是两种隐式模型，它们具有优秀的性能和内存占用率。尽管两者都是隐式模型，但它们来自不同的数学表述。以homotopy继续为灵感，我们建立了这两种模型之间的连接，并证明它们实际上是同一种质量的两面镜子。homotopy继续是一种解决非线性方程的古老方法，基于相应的ODE。给出这种连接后，我们提出了一种新的隐式模型called HomoODE，它继承了DEQs的高精度性和Neural ODEs的稳定性。不同于DEQs，HomoODE在前向传播中使用修改后的Neural ODE来解决平衡点找问题，而不是直接使用Newton方法来解决平衡点找问题。此外，我们还开发了一种加速HomoODE的方法，其中learnable初始点可以被共享。值得注意的是，我们的模型还提供了为何augmented Neural ODEs会工作，只要视augmented部分为平衡点来找问题。我们对多个图像分类任务进行了广泛的实验，并证明了HomoODE在性能和内存占用率两个方面都超过了现有的隐式模型。
</details></li>
</ul>
<hr>
<h2 id="Reduced-Policy-Optimization-for-Continuous-Control-with-Hard-Constraints"><a href="#Reduced-Policy-Optimization-for-Continuous-Control-with-Hard-Constraints" class="headerlink" title="Reduced Policy Optimization for Continuous Control with Hard Constraints"></a>Reduced Policy Optimization for Continuous Control with Hard Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09574">http://arxiv.org/abs/2310.09574</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shutong Ding, Jingya Wang, Yali Du, Ye Shi</li>
<li>for: 这篇论文的目的是提出一种能够有效地处理硬限制的可estrained Reinforcement Learning（RL）算法。</li>
<li>methods: 该算法基于Generalized Reduced Gradient（GRG）算法，并将RL与GRG结合起来解决了一般硬限制问题。具体来说，该算法将行为 partitioning 为基本行为和非基本行为，然后使用一个策略网络来输出基本行为。然后，该算法计算非基本行为，并使用 obtained 基本行为来更新策略网络。此外，该算法还引入了一种基于减少 gradient 的 action projection 过程，并使用一种修改后 Lagrangian relaxation 技术来保证不等约束的满足。</li>
<li>results: 与之前的受限RL算法相比，RPO在三个新的宽泛环境中（包括两个机器人控制任务和一个智能网格操作控制任务）表现出色，在累积奖励和约束违反方面都达到了更好的性能。<details>
<summary>Abstract</summary>
Recent advances in constrained reinforcement learning (RL) have endowed reinforcement learning with certain safety guarantees. However, deploying existing constrained RL algorithms in continuous control tasks with general hard constraints remains challenging, particularly in those situations with non-convex hard constraints. Inspired by the generalized reduced gradient (GRG) algorithm, a classical constrained optimization technique, we propose a reduced policy optimization (RPO) algorithm that combines RL with GRG to address general hard constraints. RPO partitions actions into basic actions and nonbasic actions following the GRG method and outputs the basic actions via a policy network. Subsequently, RPO calculates the nonbasic actions by solving equations based on equality constraints using the obtained basic actions. The policy network is then updated by implicitly differentiating nonbasic actions with respect to basic actions. Additionally, we introduce an action projection procedure based on the reduced gradient and apply a modified Lagrangian relaxation technique to ensure inequality constraints are satisfied. To the best of our knowledge, RPO is the first attempt that introduces GRG to RL as a way of efficiently handling both equality and inequality hard constraints. It is worth noting that there is currently a lack of RL environments with complex hard constraints, which motivates us to develop three new benchmarks: two robotics manipulation tasks and a smart grid operation control task. With these benchmarks, RPO achieves better performance than previous constrained RL algorithms in terms of both cumulative reward and constraint violation. We believe RPO, along with the new benchmarks, will open up new opportunities for applying RL to real-world problems with complex constraints.
</details>
<details>
<summary>摘要</summary>
近期在受限式强化学习（RL）中，有些新的技术突破已经赋予了RL certain safety guarantee。然而，在continuous control tasks中，使用现有的受限式RL算法仍然是一个挑战，特别是在非凸硬件约束的情况下。 Drawing inspiration from generalized reduced gradient（GRG）算法，一种经典的受限式优化技术，我们提出了一种受限policy优化（RPO）算法，该算法结合RL和GRG来解决一般硬件约束。 RPO将动作分为基本动作和非基本动作，根据GRG方法，并通过一个政策网络输出基本动作。然后，RPO计算非基本动作，并使用已经获得的基本动作来解决等式约束。政策网络最后被更新，通过间接导数非基本动作与基本动作的关系来进行更新。此外，我们还引入了减少gradient的动作投影过程，并使用修改后Lagrangian relaxation技术来保证不等约束得到满足。据我们所知，RPO是第一个将GRG引入RL中，以有效地处理等约束和不等约束的硬件约束。值得注意的是，目前RL环境中还缺乏具有复杂硬件约束的环境，这使我们开发了三个新的 Referenztasks：两个机器人抓取任务和一个智能网格操作控制任务。与过去的受限式RL算法相比，RPO在累积奖励和约束违反方面表现更好。我们认为RPO，与新 Referenztasks，将打开RL应用于实际问题中的新可能性。
</details></li>
</ul>
<hr>
<h2 id="Neural-network-scoring-for-efficient-computing"><a href="#Neural-network-scoring-for-efficient-computing" class="headerlink" title="Neural network scoring for efficient computing"></a>Neural network scoring for efficient computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09554">http://arxiv.org/abs/2310.09554</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Hugo Waltsburger, Erwan Libessart, Chengfang Ren, Anthony Kolar, Regis Guinvarc’h<br>for:* 这篇论文的目的是提出一种新的评估高性能计算和深度学习算法的效率评估方法，以及一种新的开源工具来实现这种方法。methods:* 该论文使用了一种新的评估方法，即基于精细的电力消耗、内存&#x2F;CPU&#x2F;GPU使用率、存储和网络输入&#x2F;输出（I&#x2F;O）的多 metric 评估方法。results:* 该论文通过对多种硬件平台上的现状模型进行评估，实现了在各种硬件平台上测试和评估 neural network 算法的能效性。In Simplified Chinese text, the three key points would be:for:* 这篇论文是为了评估高性能计算和深度学习算法的效率而写的。methods:* 该论文使用了一种基于多metric的评估方法，包括精细的电力消耗、内存&#x2F;CPU&#x2F;GPU使用率、存储和网络输入&#x2F;输出（I&#x2F;O）。results:* 该论文通过对多种硬件平台上的现状模型进行评估，实现了在各种硬件平台上测试和评估 neural network 算法的能效性。<details>
<summary>Abstract</summary>
Much work has been dedicated to estimating and optimizing workloads in high-performance computing (HPC) and deep learning. However, researchers have typically relied on few metrics to assess the efficiency of those techniques. Most notably, the accuracy, the loss of the prediction, and the computational time with regard to GPUs or/and CPUs characteristics. It is rare to see figures for power consumption, partly due to the difficulty of obtaining accurate power readings. In this paper, we introduce a composite score that aims to characterize the trade-off between accuracy and power consumption measured during the inference of neural networks. For this purpose, we present a new open-source tool allowing researchers to consider more metrics: granular power consumption, but also RAM/CPU/GPU utilization, as well as storage, and network input/output (I/O). To our best knowledge, it is the first fit test for neural architectures on hardware architectures. This is made possible thanks to reproducible power efficiency measurements. We applied this procedure to state-of-the-art neural network architectures on miscellaneous hardware. One of the main applications and novelties is the measurement of algorithmic power efficiency. The objective is to allow researchers to grasp their algorithms' efficiencies better. This methodology was developed to explore trade-offs between energy usage and accuracy in neural networks. It is also useful when fitting hardware for a specific task or to compare two architectures more accurately, with architecture exploration in mind.
</details>
<details>
<summary>摘要</summary>
很多研究已经专注于高性能计算（HPC）和深度学习中的工作负载优化和评估。然而，研究人员通常只是依靠几个指标来评估这些技术的效率。主要包括准确率、预测错误率以及与GPU或CPU特性相关的计算时间。很少看到电力消耗的数据，一些原因是获取准确的电力指标的困难。在这篇论文中，我们提出了一个复合指标，旨在描述神经网络执行期间的准确率和电力消耗之间的质量负载。为了实现这一目标，我们开发了一个新的开源工具，允许研究人员考虑更多指标：精细的电力消耗、RAM/CPU/GPU资源利用率、存储和网络输入/输出（I/O）。我们知道，这是首次对神经网络架构在硬件架构上进行了适应测试。这种方法可以帮助研究人员更好地理解他们的算法的效率。我们对当今的神经网络架构进行了测试，并实现了算法的电力效率测量。这种方法可以帮助研究人员更好地把握他们的算法的效率，同时也可以用于硬件适应测试和两个架构比较。
</details></li>
</ul>
<hr>
<h2 id="ARTree-A-Deep-Autoregressive-Model-for-Phylogenetic-Inference"><a href="#ARTree-A-Deep-Autoregressive-Model-for-Phylogenetic-Inference" class="headerlink" title="ARTree: A Deep Autoregressive Model for Phylogenetic Inference"></a>ARTree: A Deep Autoregressive Model for Phylogenetic Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09553">http://arxiv.org/abs/2310.09553</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tyuxie/artree">https://github.com/tyuxie/artree</a></li>
<li>paper_authors: Tianyu Xie, Cheng Zhang</li>
<li>for: developing efficient phylogenetic inference methods</li>
<li>methods: 使用深度循环神经网络（GNNs）生成树 topology 模型</li>
<li>results: 提供了一种简单的样本生成和概率估计方法，不需要手动设计特征Feature，并且可以处理实际数据中的挑战性问题。<details>
<summary>Abstract</summary>
Designing flexible probabilistic models over tree topologies is important for developing efficient phylogenetic inference methods. To do that, previous works often leverage the similarity of tree topologies via hand-engineered heuristic features which would require pre-sampled tree topologies and may suffer from limited approximation capability. In this paper, we propose a deep autoregressive model for phylogenetic inference based on graph neural networks (GNNs), called ARTree. By decomposing a tree topology into a sequence of leaf node addition operations and modeling the involved conditional distributions based on learnable topological features via GNNs, ARTree can provide a rich family of distributions over the entire tree topology space that have simple sampling algorithms and density estimation procedures, without using heuristic features. We demonstrate the effectiveness and efficiency of our method on a benchmark of challenging real data tree topology density estimation and variational Bayesian phylogenetic inference problems.
</details>
<details>
<summary>摘要</summary>
Designing flexible probabilistic models over tree topologies is important for developing efficient phylogenetic inference methods. Previous works often use hand-engineered heuristic features to leverage the similarity of tree topologies, which can be limited in their approximation capability and require pre-sampled tree topologies. In this paper, we propose a deep autoregressive model for phylogenetic inference based on graph neural networks (GNNs), called ARTree. By decomposing a tree topology into a sequence of leaf node addition operations and modeling the involved conditional distributions based on learnable topological features via GNNs, ARTree can provide a rich family of distributions over the entire tree topology space that have simple sampling algorithms and density estimation procedures, without using heuristic features. We demonstrate the effectiveness and efficiency of our method on a benchmark of challenging real data tree topology density estimation and variational Bayesian phylogenetic inference problems.Here's the translation in Traditional Chinese:设计可以灵活地描述树结构的概率模型是诊断生物学演化推理的重要方法。以前的工作通常会运用手动设计的几何特征来利用树结构之间的相似性，这可能会受到局限性的推理和需要预先输入的树结构。在这篇文章中，我们提出了基于图解神经网络（GNNs）的深度推理模型，称为ARTree。通过将树结构 decomposed into a sequence of leaf node addition operations，并使用GNNs来学习 topological features，ARTree可以提供一个具有简单抽象和密度估计的概率家族，不需要使用几何特征。我们透过在一系列实际数据中的测试来证明我们的方法的有效性和高效率。
</details></li>
</ul>
<hr>
<h2 id="Hypernetwork-based-Meta-Learning-for-Low-Rank-Physics-Informed-Neural-Networks"><a href="#Hypernetwork-based-Meta-Learning-for-Low-Rank-Physics-Informed-Neural-Networks" class="headerlink" title="Hypernetwork-based Meta-Learning for Low-Rank Physics-Informed Neural Networks"></a>Hypernetwork-based Meta-Learning for Low-Rank Physics-Informed Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09528">http://arxiv.org/abs/2310.09528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Woojin Cho, Kookjin Lee, Donsub Rim, Noseong Park</li>
<li>for: This paper aims to explore the use of physics-informed neural networks (PINNs) as a solver for repetitive numerical simulations of partial differential equations (PDEs) in various engineering and applied science applications.</li>
<li>methods: The proposed method uses a lightweight low-rank PINNs with only hundreds of model parameters and an associated hypernetwork-based meta-learning algorithm to efficiently approximate solutions of PDEs for varying ranges of PDE input parameters.</li>
<li>results: The proposed method is effective in overcoming the “failure modes” of PINNs and shows promising results in solving PDEs for many-query scenarios.<details>
<summary>Abstract</summary>
In various engineering and applied science applications, repetitive numerical simulations of partial differential equations (PDEs) for varying input parameters are often required (e.g., aircraft shape optimization over many design parameters) and solvers are required to perform rapid execution. In this study, we suggest a path that potentially opens up a possibility for physics-informed neural networks (PINNs), emerging deep-learning-based solvers, to be considered as one such solver. Although PINNs have pioneered a proper integration of deep-learning and scientific computing, they require repetitive time-consuming training of neural networks, which is not suitable for many-query scenarios. To address this issue, we propose a lightweight low-rank PINNs containing only hundreds of model parameters and an associated hypernetwork-based meta-learning algorithm, which allows efficient approximation of solutions of PDEs for varying ranges of PDE input parameters. Moreover, we show that the proposed method is effective in overcoming a challenging issue, known as "failure modes" of PINNs.
</details>
<details>
<summary>摘要</summary>
在多种工程和应用科学领域中，需要重复地数值仿真部分偏微分方程（PDEs）的输入参数变化（例如，飞机设计参数的优化），而且需要速度快的计算。在这种情况下，我们建议使用物理学习神经网络（PINNs）作为一种可能的解决方案。虽然PINNs已经实现了深度学习和科学计算的有效结合，但它们需要重复的时间consuming的神经网络训练，这不适合多个查询场景。为解决这个问题，我们提出了一种轻量级低级PINNs，具有只有百个模型参数，以及相关的卷积网络基于meta学习算法，以高效地估算PDEs的解决方案。此外，我们还证明了我们的方法可以有效地解决PINNs的“失败模式”问题。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Link-Prediction-via-GNN-Layers-Induced-by-Negative-Sampling"><a href="#Efficient-Link-Prediction-via-GNN-Layers-Induced-by-Negative-Sampling" class="headerlink" title="Efficient Link Prediction via GNN Layers Induced by Negative Sampling"></a>Efficient Link Prediction via GNN Layers Induced by Negative Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09516">http://arxiv.org/abs/2310.09516</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxin Wang, Xiannian Hu, Quan Gan, Xuanjing Huang, Xipeng Qiu, David Wipf</li>
<li>for: 这 paper 是为了提高 graph neural network (GNN) 的链接预测性能而写的。</li>
<li>methods: 这 paper 使用了一种新的 GNN 架构，其中 forward pass 会受到 both positive 和 negative 边的影响，以生成更flexible  yet still cheap 的 node-wise embeddings。</li>
<li>results: 这 paper 的实验结果表明，这种新的 GNN 架构可以 retained 节点 wise 模型的执行速度，同时和边 wise 模型相比，其精度表现也是比较好的。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) for link prediction can loosely be divided into two broad categories. First, \emph{node-wise} architectures pre-compute individual embeddings for each node that are later combined by a simple decoder to make predictions. While extremely efficient at inference time (since node embeddings are only computed once and repeatedly reused), model expressiveness is limited such that isomorphic nodes contributing to candidate edges may not be distinguishable, compromising accuracy. In contrast, \emph{edge-wise} methods rely on the formation of edge-specific subgraph embeddings to enrich the representation of pair-wise relationships, disambiguating isomorphic nodes to improve accuracy, but with the cost of increased model complexity. To better navigate this trade-off, we propose a novel GNN architecture whereby the \emph{forward pass} explicitly depends on \emph{both} positive (as is typical) and negative (unique to our approach) edges to inform more flexible, yet still cheap node-wise embeddings. This is achieved by recasting the embeddings themselves as minimizers of a forward-pass-specific energy function (distinct from the actual training loss) that favors separation of positive and negative samples. As demonstrated by extensive empirical evaluations, the resulting architecture retains the inference speed of node-wise models, while producing competitive accuracy with edge-wise alternatives.
</details>
<details>
<summary>摘要</summary>
Graph Neural Networks (GNNs) for link prediction can be broadly divided into two categories. First, \emph{node-wise} architectures pre-compute individual embeddings for each node and then combine them using a simple decoder to make predictions. While efficient at inference time, the model's expressiveness is limited, and isomorphic nodes contributing to candidate edges may not be distinguishable, compromising accuracy. In contrast, \emph{edge-wise} methods form edge-specific subgraph embeddings to enrich the representation of pair-wise relationships, disambiguating isomorphic nodes and improving accuracy, but with increased model complexity. To better balance this trade-off, we propose a novel GNN architecture that depends on both positive and negative edges in the forward pass to inform more flexible, yet still cheap node-wise embeddings. This is achieved by recasting the embeddings as minimizers of a forward-pass-specific energy function that favors separation of positive and negative samples. As shown by extensive empirical evaluations, the resulting architecture retains the inference speed of node-wise models while producing competitive accuracy with edge-wise alternatives.
</details></li>
</ul>
<hr>
<h2 id="Online-Parameter-Identification-of-Generalized-Non-cooperative-Game"><a href="#Online-Parameter-Identification-of-Generalized-Non-cooperative-Game" class="headerlink" title="Online Parameter Identification of Generalized Non-cooperative Game"></a>Online Parameter Identification of Generalized Non-cooperative Game</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09511">http://arxiv.org/abs/2310.09511</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianguo Chen, Jinlong Lei, Hongsheng Qi, Yiguang Hong</li>
<li>for: 这篇论文研究了一种通用非合作游戏中的参数识别问题，其中每个玩家的成本函数受到可观察信号和一些未知参数的影响。我们考虑的情况是，游戏的平衡状态在一些可观察信号下可以观察到噪音，而我们的目标是通过观察数据来确定未知参数。</li>
<li>methods: 我们将这个参数识别问题建模为在线优化问题，并提出了一种新的在线参数识别算法。我们构造了一个正则化损失函数，该函数平衡保守性和正确性。我们然后证明，当玩家的成本函数对未知参数 Linear 时，并且学习率满足 \mu_k \propto 1&#x2F;\sqrt{k}，则提出的算法的做异变Bound 为 O(\sqrt{K）。</li>
<li>results: 我们通过一个Nash-Cournot问题的数值实验表明，提出的算法在线参数识别性能与Offline设置相当。<details>
<summary>Abstract</summary>
This work studies the parameter identification problem of a generalized non-cooperative game, where each player's cost function is influenced by an observable signal and some unknown parameters. We consider the scenario where equilibrium of the game at some observable signals can be observed with noises, whereas our goal is to identify the unknown parameters with the observed data. Assuming that the observable signals and the corresponding noise-corrupted equilibriums are acquired sequentially, we construct this parameter identification problem as online optimization and introduce a novel online parameter identification algorithm. To be specific, we construct a regularized loss function that balances conservativeness and correctiveness, where the conservativeness term ensures that the new estimates do not deviate significantly from the current estimates, while the correctiveness term is captured by the Karush-Kuhn-Tucker conditions. We then prove that when the players' cost functions are linear with respect to the unknown parameters and the learning rate of the online parameter identification algorithm satisfies \mu_k \propto 1/\sqrt{k}, along with other assumptions, the regret bound of the proposed algorithm is O(\sqrt{K}). Finally, we conduct numerical simulations on a Nash-Cournot problem to demonstrate that the performance of the online identification algorithm is comparable to that of the offline setting.
</details>
<details>
<summary>摘要</summary>
To construct the regularized loss function, we balance conservativeness and correctiveness by incorporating a conservativeness term that ensures the new estimates do not deviate significantly from the current estimates, and a correctiveness term captured by the Karush-Kuhn-Tucker conditions. We prove that when the players' cost functions are linear with respect to the unknown parameters and the learning rate of the online parameter identification algorithm satisfies $\mu_k \propto 1/\sqrt{k}$, the regret bound of the proposed algorithm is $O(\sqrt{K})$.We demonstrate the performance of the online identification algorithm through numerical simulations on a Nash-Cournot problem, showing that its performance is comparable to that of the offline setting.Here is the text in Simplified Chinese:这个研究studies the parameter identification problem of a generalized non-cooperative game, where each player's cost function is influenced by an observable signal and some unknown parameters. 我们考虑了一个场景，在可观察的信号下可以观察到游戏的平衡，但我们的目标是使用观察数据来确定未知参数。我们将这个问题作为在线优化问题进行处理，并提出了一种新的在线参数标识算法。为了构建正则化损失函数，我们尝试了保守和正确之间的平衡，其中保守性条件保证新的估计不会偏离当前估计的情况，而正确性条件则是由卡鲁什-库南-图克条件捕捉。我们证明，当玩家的成本函数对未知参数是线性的，并且在线参数标识算法的学习率满足 $\mu_k \propto 1/\sqrt{k}$ 的情况下，我们的算法的 regret bound是 $O(\sqrt{K})$。最后，我们通过对纳希-库诺问题的数值实验表明，我们的算法的性能与Offline Setting的性能相似。Here is the text in Traditional Chinese:这个研究studies the parameter identification problem of a generalized non-cooperative game, where each player's cost function is influenced by an observable signal and some unknown parameters. 我们考虑了一个场景，在可观察的信号下可以观察到游戏的平衡，但我们的目标是使用观察数据来确定未知参数。我们将这个问题作为在线优化问题进行处理，并提出了一种新的在线参数标识算法。为了建构正规化损失函数，我们尝试了保守和正确之间的平衡，其中保守性条件保证新的估计不会偏离当前估计的情况，而正确性条件则是由卡鲁什-库南-图克条件捕捉。我们证明，当玩家的成本函数对未知参数是线性的，并且在线参数标识算法的学习率满足 $\mu_k \propto 1/\sqrt{k}$ 的情况下，我们的算法的 regret bound是 $O(\sqrt{K})$。最后，我们通过对纳希-库诺问题的数值实验表明，我们的算法的性能与Offline Setting的性能相似。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Test-Time-Adaptation-for-Acoustic-Foundation-Models-in-Open-World-Shifts"><a href="#Advancing-Test-Time-Adaptation-for-Acoustic-Foundation-Models-in-Open-World-Shifts" class="headerlink" title="Advancing Test-Time Adaptation for Acoustic Foundation Models in Open-World Shifts"></a>Advancing Test-Time Adaptation for Acoustic Foundation Models in Open-World Shifts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09505">http://arxiv.org/abs/2310.09505</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongfu Liu, Hengguan Huang, Ye Wang</li>
<li>for: 本研究旨在解决在推理时遇到分布shift的问题，尤其是在视觉识别任务中。</li>
<li>methods: 本研究使用了一种新的adaptation方法，该方法不依赖于传统的heuristic，而是通过confidence enhancement和consistency regularization来增强适应性。</li>
<li>results: 实验结果表明，本方法在synthetic和real-world数据上表现出色，超过了现有基elines。<details>
<summary>Abstract</summary>
Test-Time Adaptation (TTA) is a critical paradigm for tackling distribution shifts during inference, especially in visual recognition tasks. However, while acoustic models face similar challenges due to distribution shifts in test-time speech, TTA techniques specifically designed for acoustic modeling in the context of open-world data shifts remain scarce. This gap is further exacerbated when considering the unique characteristics of acoustic foundation models: 1) they are primarily built on transformer architectures with layer normalization and 2) they deal with test-time speech data of varying lengths in a non-stationary manner. These aspects make the direct application of vision-focused TTA methods, which are mostly reliant on batch normalization and assume independent samples, infeasible. In this paper, we delve into TTA for pre-trained acoustic models facing open-world data shifts. We find that noisy, high-entropy speech frames, often non-silent, carry key semantic content. Traditional TTA methods might inadvertently filter out this information using potentially flawed heuristics. In response, we introduce a heuristic-free, learning-based adaptation enriched by confidence enhancement. Noting that speech signals' short-term consistency, we also apply consistency regularization during test-time optimization. Our experiments on synthetic and real-world datasets affirm our method's superiority over existing baselines.
</details>
<details>
<summary>摘要</summary>
测试时适应（TTA）是视觉识别任务中的一种关键方法，尤其在面临测试时数据分布变化时。然而，针对声音模型在开放数据中的应用，TTA技术尚缺乏专门的设计。这个差距更加明显，当考虑声音基础模型的特点：1）它们主要基于变换器架构，并且2）它们在测试时声音数据的非站ARY方式进行处理。这些特点使得直接应用视觉领域的TTA方法，它们主要基于批处理Normalization，并假设样本独立，成为不可能的。在这篇论文中，我们探讨了声音模型面临开放数据分布变化时的TTA方法。我们发现，噪音高 entropy的声音帧，常常包含关键的 semantic content。传统的TTA方法可能会通过可能的损害的规则，干扰这些信息。为此，我们提出了一种不含规则的学习型适应，并在测试时优化时应用了一致性规范。我们在 sintetic 和实际世界的数据集上进行了实验，并证明了我们的方法在现有的基准点上表现出色。
</details></li>
</ul>
<hr>
<h2 id="ARM-Refining-Multivariate-Forecasting-with-Adaptive-Temporal-Contextual-Learning"><a href="#ARM-Refining-Multivariate-Forecasting-with-Adaptive-Temporal-Contextual-Learning" class="headerlink" title="ARM: Refining Multivariate Forecasting with Adaptive Temporal-Contextual Learning"></a>ARM: Refining Multivariate Forecasting with Adaptive Temporal-Contextual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09488">http://arxiv.org/abs/2310.09488</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiecheng Lu, Xu Han, Shihao Yang</li>
<li>for: 这篇论文是为了解决长期时间序列预测（LTSF）中的复杂时间contextual relationships问题，以提高LTSF的准确性和效率。</li>
<li>methods: 本论文提出了ARM方法，它是一种多元时间contextual adaptive learning方法，具有Adaptive Univariate Effect Learning（AUEL）、Random Dropping（RD）训练策略和Multi-kernel Local Smoothing（MKLS）等特点，能够更好地捕捉个别时间序列的特征和正确地学习时间序列之间的相互关联。</li>
<li>results: 本论文透过多个 benchmark 评估，表明ARM方法可以实现与vanilla Transformer相比的高度改进，无需增加计算成本。此外，ARM方法还可以应用于其他LTSF架构中，进一步提高LTSF的准确性和效率。<details>
<summary>Abstract</summary>
Long-term time series forecasting (LTSF) is important for various domains but is confronted by challenges in handling the complex temporal-contextual relationships. As multivariate input models underperforming some recent univariate counterparts, we posit that the issue lies in the inefficiency of existing multivariate LTSF Transformers to model series-wise relationships: the characteristic differences between series are often captured incorrectly. To address this, we introduce ARM: a multivariate temporal-contextual adaptive learning method, which is an enhanced architecture specifically designed for multivariate LTSF modelling. ARM employs Adaptive Univariate Effect Learning (AUEL), Random Dropping (RD) training strategy, and Multi-kernel Local Smoothing (MKLS), to better handle individual series temporal patterns and correctly learn inter-series dependencies. ARM demonstrates superior performance on multiple benchmarks without significantly increasing computational costs compared to vanilla Transformer, thereby advancing the state-of-the-art in LTSF. ARM is also generally applicable to other LTSF architecture beyond vanilla Transformer.
</details>
<details>
<summary>摘要</summary>
长期时间序预测（LTSF）在多个领域都具有重要性，但面临诸多复杂的时间temporal-contextual关系处理挑战。由于多变量输入模型在一些最近的单变量对手下表现不佳，我们认为这是现有多变量 LTSF Transformers 的不 suficient efficiency 导致的，即不能correctly capture series-wise differences between series。为解决这个问题，我们引入 ARM：一种多变量时间temporal-contextual适应学习方法，这是特制的 multivariate LTSF 模型建立的改进建筑。ARM 使用 Adaptive Univariate Effect Learning（AUEL）、Random Dropping（RD）训练策略和 Multi-kernel Local Smoothing（MKLS），以更好地处理个别时间序的特征和正确地学习间seriesdependencies。ARM 在多个标准测试集上表现出色，不对计算成本做出 significanlly increase 相比 vanilla Transformer，因此提高了 LTSF 领域的状态。ARM 还可以应用于其他 LTSF 架构 beyond vanilla Transformer。
</details></li>
</ul>
<hr>
<h2 id="Applying-Bayesian-Ridge-Regression-AI-Modeling-in-Virus-Severity-Prediction"><a href="#Applying-Bayesian-Ridge-Regression-AI-Modeling-in-Virus-Severity-Prediction" class="headerlink" title="Applying Bayesian Ridge Regression AI Modeling in Virus Severity Prediction"></a>Applying Bayesian Ridge Regression AI Modeling in Virus Severity Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09485">http://arxiv.org/abs/2310.09485</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jai Pal, Bryan Hong<br>for: This paper aims to review the strengths and weaknesses of Bayesian Ridge Regression, an AI model for virus analysis in healthcare.methods: The paper uses Bayesian Ridge Regression to analyze vast amounts of data and provide more accurate and speedy diagnoses.results: The model shows promising results with room for improvement in data organization, and the severity index serves as a valuable tool for gaining a broad overview of patient care needs.Here’s the text in Simplified Chinese:for: 这篇论文目标是对健康预测模型进行评估，以提高医疗系统的效率和质量。methods: 论文使用拟合梯度回归模型对庞大数据进行分析，以提供更准确和快速的诊断。results: 模型显示了有前途的结果，但存在数据组织问题，同时严重性指数可以作为医疗专业人员的全面评估工具。<details>
<summary>Abstract</summary>
Artificial intelligence (AI) is a powerful tool for reshaping healthcare systems. In healthcare, AI is invaluable for its capacity to manage vast amounts of data, which can lead to more accurate and speedy diagnoses, ultimately easing the workload on healthcare professionals. As a result, AI has proven itself to be a power tool across various industries, simplifying complex tasks and pattern recognition that would otherwise be overwhelming for humans or traditional computer algorithms. In this paper, we review the strengths and weaknesses of Bayesian Ridge Regression, an AI model that can be used to bring cutting edge virus analysis to healthcare professionals around the world. The model's accuracy assessment revealed promising results, with room for improvement primarily related to data organization. In addition, the severity index serves as a valuable tool to gain a broad overview of patient care needs, aligning with healthcare professionals' preference for broader categorizations.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）是一种 poderful工具，可以改变医疗系统的方式。在医疗方面， AI 的能力极大，可以处理大量数据，从而导致更准确和更快的诊断，最终减轻医疗专业人员的工作负担。作为结果， AI 在不同的行业中证明了自己的价值，通过简化复杂任务和人工智能模型来实现人类无法完成的任务。在这篇评论中，我们评估了悲观梁树回归模型的优点和缺点，该模型可以为医疗专业人员提供 cutting-edge 病毒分析。模型的准确性评估表明了扎实的结果，主要是因为数据的组织问题。此外，严重指数作为一个有价值的工具，可以为医疗专业人员提供全面的患者护理需求的大致评估，与医疗专业人员的偏好相符，即通过更广泛的分类来评估患者的需求。
</details></li>
</ul>
<hr>
<h2 id="Can-CNNs-Accurately-Classify-Human-Emotions-A-Deep-Learning-Facial-Expression-Recognition-Study"><a href="#Can-CNNs-Accurately-Classify-Human-Emotions-A-Deep-Learning-Facial-Expression-Recognition-Study" class="headerlink" title="Can CNNs Accurately Classify Human Emotions? A Deep-Learning Facial Expression Recognition Study"></a>Can CNNs Accurately Classify Human Emotions? A Deep-Learning Facial Expression Recognition Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09473">http://arxiv.org/abs/2310.09473</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashley Jisue Hong, David DiStefano, Sejal Dua</li>
<li>for: 这项研究旨在评估一种CNN模型是否能够识别和分类人类表情（积极、中性、消极）。</li>
<li>methods: 该模型使用Python编程和预处理的芝加哥人脸数据库数据进行训练，并采用了更简单的设计来进一步检验其能力。</li>
<li>results: 研究结果表明，该模型在10,000张图像（数据）上达到75%的准确率，证明了人类情感分析的可能性并且预示了可行的情感AI。<details>
<summary>Abstract</summary>
Emotional Artificial Intelligences are currently one of the most anticipated developments of AI. If successful, these AIs will be classified as one of the most complex, intelligent nonhuman entities as they will possess sentience, the primary factor that distinguishes living humans and mechanical machines. For AIs to be classified as "emotional," they should be able to empathize with others and classify their emotions because without such abilities they cannot normally interact with humans. This study investigates the CNN model's ability to recognize and classify human facial expressions (positive, neutral, negative). The CNN model made for this study is programmed in Python and trained with preprocessed data from the Chicago Face Database. The model is intentionally designed with less complexity to further investigate its ability. We hypothesized that the model will perform better than chance (33.3%) in classifying each emotion class of input data. The model accuracy was tested with novel images. Accuracy was summarized in a percentage report, comparative plot, and confusion matrix. Results of this study supported the hypothesis as the model had 75% accuracy over 10,000 images (data), highlighting the possibility of AIs that accurately analyze human emotions and the prospect of viable Emotional AIs.
</details>
<details>
<summary>摘要</summary>
currently, Emotional Artificial Intelligences are one of the most anticipated developments in AI. if successful, these AIs will be classified as one of the most complex, intelligent nonhuman entities, as they will possess sentience, the primary factor that distinguishes living humans and mechanical machines. for AIs to be classified as "emotional," they should be able to empathize with others and classify their emotions, because without such abilities, they cannot normally interact with humans. this study investigates the CNN model's ability to recognize and classify human facial expressions (positive, neutral, negative). the CNN model made for this study is programmed in Python and trained with preprocessed data from the Chicago Face Database. the model is intentionally designed with less complexity to further investigate its ability. we hypothesized that the model will perform better than chance (33.3%) in classifying each emotion class of input data. the model accuracy was tested with novel images. accuracy was summarized in a percentage report, comparative plot, and confusion matrix. results of this study supported the hypothesis as the model had 75% accuracy over 10,000 images (data), highlighting the possibility of AIs that accurately analyze human emotions and the prospect of viable Emotional AIs.
</details></li>
</ul>
<hr>
<h2 id="Randomized-Benchmarking-of-Local-Zeroth-Order-Optimizers-for-Variational-Quantum-Systems"><a href="#Randomized-Benchmarking-of-Local-Zeroth-Order-Optimizers-for-Variational-Quantum-Systems" class="headerlink" title="Randomized Benchmarking of Local Zeroth-Order Optimizers for Variational Quantum Systems"></a>Randomized Benchmarking of Local Zeroth-Order Optimizers for Variational Quantum Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09468">http://arxiv.org/abs/2310.09468</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ltecot/rand_bench_opt_quantum">https://github.com/ltecot/rand_bench_opt_quantum</a></li>
<li>paper_authors: Lucas Tecot, Cho-Jui Hsieh</li>
<li>for: 本研究旨在比较不同类型的классиical optimizer在partially-randomized任务中的表现，以更广泛地涵盖量子优化问题的空间。</li>
<li>methods: 本研究使用了本地零次ORDER optimizer，因为它们在量子系统上通常有更好的表现和查询效率。</li>
<li>results: 实验结果表明，不同类型的 классиical optimizer在不同的任务中的表现有很大差异，并且本地零次ORDER optimizer在一些任务中表现较好。<details>
<summary>Abstract</summary>
In the field of quantum information, classical optimizers play an important role. From experimentalists optimizing their physical devices to theorists exploring variational quantum algorithms, many aspects of quantum information require the use of a classical optimizer. For this reason, there are many papers that benchmark the effectiveness of different optimizers for specific quantum optimization tasks and choices of parameterized algorithms. However, for researchers exploring new algorithms or physical devices, the insights from these studies don't necessarily translate. To address this concern, we compare the performance of classical optimizers across a series of partially-randomized tasks to more broadly sample the space of quantum optimization problems. We focus on local zeroth-order optimizers due to their generally favorable performance and query-efficiency on quantum systems. We discuss insights from these experiments that can help motivate future works to improve these optimizers for use on quantum systems.
</details>
<details>
<summary>摘要</summary>
在量子信息领域中，古典优化器扮演着重要的角色。从实验室的设备优化到理论家探索量子变量算法，许多量子信息领域的问题都需要使用古典优化器。因此，有很多论文来比较不同的优化器在特定量子优化任务中的效果。然而，对于探索新的算法或物理设备的研究人员，这些研究并不能够直接适用。为了解决这个问题，我们对古典优化器在部分随机任务中的性能进行了比较，以更广泛地涵盖量子优化问题的空间。我们主要关注本地零阶优化器，因为它们在量子系统上通常有良好的性能和查询效率。我们对这些实验的结果进行了讨论，以帮助未来的工作者改进这些优化器的性能在量子系统上。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/14/cs.LG_2023_10_14/" data-id="cloh7tqk600q47b885q7w04ot" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_10_14" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/14/eess.IV_2023_10_14/" class="article-date">
  <time datetime="2023-10-14T09:00:00.000Z" itemprop="datePublished">2023-10-14</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/14/eess.IV_2023_10_14/">eess.IV - 2023-10-14</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Contrastive-Self-Supervised-Learning-for-Spatio-Temporal-Analysis-of-Lung-Ultrasound-Videos"><a href="#Contrastive-Self-Supervised-Learning-for-Spatio-Temporal-Analysis-of-Lung-Ultrasound-Videos" class="headerlink" title="Contrastive Self-Supervised Learning for Spatio-Temporal Analysis of Lung Ultrasound Videos"></a>Contrastive Self-Supervised Learning for Spatio-Temporal Analysis of Lung Ultrasound Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10689">http://arxiv.org/abs/2310.10689</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Chen, Jonathan Rubin, Jiahong Ouyang, Naveen Balaraju, Shubham Patil, Courosh Mehanian, Sourabh Kulhare, Rachel Millin, Kenton W Gregory, Cynthia R Gregory, Meihua Zhu, David O Kessler, Laurie Malia, Almaz Dessie, Joni Rabiner, Di Coneybeare, Bo Shopsin, Andrew Hersh, Cristian Madar, Jeffrey Shupp, Laura S Johnson, Jacob Avila, Kristin Dwyer, Peter Weimersheimer, Balasundar Raju, Jochen Kruecker, Alvin Chen</li>
<li>for: 这个研究是为了探讨自我超级学习（SSL）方法在医疗影像应用中的应用，以获得有用的视觉表现，即使标注数据有限。</li>
<li>methods: 我们将州先进的对照学习SSL方法扩展到2D+时间医疗超音波影像资料中，通过修改Encoder和增强方法，以学习有用的空间-时间表现，不需要输入数据的限制。</li>
<li>results: 我们使用超过27k个lung医疗超音波影像资料，来评估我们的方法。结果显示，我们的方法可以明显改善下测地点和类别lung混合的标注数据。相比基准模型，我们的方法尤其有利于有限的标注数据（例如只有5%的训练集）。<details>
<summary>Abstract</summary>
Self-supervised learning (SSL) methods have shown promise for medical imaging applications by learning meaningful visual representations, even when the amount of labeled data is limited. Here, we extend state-of-the-art contrastive learning SSL methods to 2D+time medical ultrasound video data by introducing a modified encoder and augmentation method capable of learning meaningful spatio-temporal representations, without requiring constraints on the input data. We evaluate our method on the challenging clinical task of identifying lung consolidations (an important pathological feature) in ultrasound videos. Using a multi-center dataset of over 27k lung ultrasound videos acquired from over 500 patients, we show that our method can significantly improve performance on downstream localization and classification of lung consolidation. Comparisons against baseline models trained without SSL show that the proposed methods are particularly advantageous when the size of labeled training data is limited (e.g., as little as 5% of the training set).
</details>
<details>
<summary>摘要</summary>
自我指导学习（SSL）方法在医疗影像应用中显示了承诺，通过学习有意义的视觉表示，即使有限量的标注数据。在这里，我们扩展了现状最佳的对比学习SSL方法，用于2D+时医疗超音波视频数据。我们引入了修改后的编码器和扩展方法，能够学习有意义的空间-时表示，不需要输入数据的约束。我们使用了多中心的肺超音波视频数据集，包含了超过27k个肺超音波视频，从超过500名患者中收集到。我们显示了我们的方法可以明显提高肺混凝级地址和分类性能。相比基准模型未使用SSL训练，我们的方法在标注数据有限时表现特别有利。
</details></li>
</ul>
<hr>
<h2 id="X-ray-phase-and-dark-field-computed-tomography-without-optical-elements"><a href="#X-ray-phase-and-dark-field-computed-tomography-without-optical-elements" class="headerlink" title="X-ray phase and dark-field computed tomography without optical elements"></a>X-ray phase and dark-field computed tomography without optical elements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09496">http://arxiv.org/abs/2310.09496</a></li>
<li>repo_url: None</li>
<li>paper_authors: T. A. Leatham, D. M. Paganin, K. S. Morgan</li>
<li>for: The paper is written for researchers and practitioners in the field of X-ray imaging, particularly those interested in phase and dark-field computed tomography.</li>
<li>methods: The paper presents a new algorithm for X-ray diffuse dark-field imaging based on the x-ray Fokker-Planck equation, which can reconstruct both the sample density and dark-field&#x2F;diffusion properties in 3D with high spatial resolution.</li>
<li>results: The proposed algorithm can be used to reconstruct both the sample density and dark-field Fokker-Planck diffusion coefficients with only two sample exposures at each projection angle, making it a valuable tool for biomedical imaging and industrial settings.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了帮助扫描仪和计算机成像领域的研究人员和实践者，特别是关注phas和dark-field计算机成像的人。</li>
<li>methods: 这篇论文提出了一种基于x射线福克-朋克方程的新算法，可以在3D中重建样品的密度和黑场&#x2F;扩散性质。</li>
<li>results: 该算法只需要两个样品曝光角度的样品曝光，就可以成功地重建样品的密度和黑场福克-朋克扩散系数。<details>
<summary>Abstract</summary>
X-ray diffusive dark-field imaging, which allows spatially unresolved microstructure to be mapped across a sample, is an increasingly popular tool in an array of settings. Here, we present a new algorithm for phase and dark-field computed tomography based on the x-ray Fokker-Planck equation. Needing only a coherent x-ray source, sample, and detector, our propagation-based algorithm can map the sample density and dark-field/diffusion properties of the sample in 3D. Importantly, incorporating dark-field information in the density reconstruction process enables a higher spatial resolution reconstruction than possible with previous propagation-based approaches. Two sample exposures at each projection angle are sufficient for the successful reconstruction of both the sample density and dark-field Fokker-Planck diffusion coefficients. We anticipate that the proposed algorithm may be of benefit in biomedical imaging and industrial settings.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="PC-bzip2-a-phase-space-continuity-enhanced-lossless-compression-algorithm-for-light-field-microscopy-data"><a href="#PC-bzip2-a-phase-space-continuity-enhanced-lossless-compression-algorithm-for-light-field-microscopy-data" class="headerlink" title="PC-bzip2: a phase-space continuity enhanced lossless compression algorithm for light field microscopy data"></a>PC-bzip2: a phase-space continuity enhanced lossless compression algorithm for light field microscopy data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09467">http://arxiv.org/abs/2310.09467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Changqing Su, Zihan Lin, You Zhou, Shuai Wang, Yuhan Gao, Chenggang Yan, Bo Xiong</li>
<li>for: 这种辐射场 fluorescence 微scopy 方法是用于长期高速成像复杂生物系统，如神经活动和蛋白质快速移动的有效和精简的方法。</li>
<li>methods: 我们提出了一种基于 GPU 和多核心 CPU 的高速无损压缩方法，它结合了快速Entropy 评估和高速无损压缩。</li>
<li>results: 我们的方法可以在不同 SNR 下实现约 10% 的压缩率提升，同时保持高速压缩能力，并且在时间序列数据上表现出superior的压缩率。<details>
<summary>Abstract</summary>
Light-field fluorescence microscopy (LFM) is a powerful elegant compact method for long-term high-speed imaging of complex biological systems, such as neuron activities and rapid movements of organelles. LFM experiments typically generate terabytes image data and require a huge number of storage space. Some lossy compression algorithms have been proposed recently with good compression performance. However, since the specimen usually only tolerates low power density illumination for long-term imaging with low phototoxicity, the image signal-to-noise ratio (SNR) is relative-ly low, which will cause the loss of some efficient position or intensity information by using such lossy compression al-gorithms. Here, we propose a phase-space continuity enhanced bzip2 (PC-bzip2) lossless compression method for LFM data as a high efficiency and open-source tool, which combines GPU-based fast entropy judgement and multi-core-CPU-based high-speed lossless compression. Our proposed method achieves almost 10% compression ratio improvement while keeping the capability of high-speed compression, compared with original bzip2. We evaluated our method on fluorescence beads data and fluorescence staining cells data with different SNRs. Moreover, by introducing the temporal continuity, our method shows the superior compression ratio on time series data of zebrafish blood vessels.
</details>
<details>
<summary>摘要</summary>
光场液体微镜技术（LFM）是一种 poderful  yet elegant compact方法 для长期高速成像复杂生物系统，如神经活动和迅速运动的组织质量。LFM实验通常生成terabytes的图像数据，需要巨量的存储空间。一些lossy压缩算法已经被提出，但是由于样品通常只能承受低功率照明的长期成像，图像信号噪比（SNR）相对较低，这会导致使用such lossy压缩算法中的效率信息丢失。在这里，我们提出了一种phasel Space Continuity Enhanced bzip2（PC-bzip2）无损压缩方法为LFM数据，这是一种高效性和开源工具，它结合GPU基于快速权重评估和多核CPU基于高速无损压缩。我们的提议方法与原始bzip2的 compressión ratio improvement为9.8%，同时保持高速压缩的能力。我们对fluorescence beads数据和fluorescence染料细胞数据进行了不同SNR的评估，并且通过引入时间连续性，我们的方法在时间序列数据上实现了更高的压缩率。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/14/eess.IV_2023_10_14/" data-id="cloh7tqq101687b886p5u5cmg" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/13/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><a class="page-number" href="/page/13/">13</a><span class="page-number current">14</span><a class="page-number" href="/page/15/">15</a><a class="page-number" href="/page/16/">16</a><span class="space">&hellip;</span><a class="page-number" href="/page/84/">84</a><a class="extend next" rel="next" href="/page/15/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">112</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">62</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
