
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/22/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.AI_2023_10_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/22/cs.AI_2023_10_22/" class="article-date">
  <time datetime="2023-10-22T12:00:00.000Z" itemprop="datePublished">2023-10-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/22/cs.AI_2023_10_22/">cs.AI - 2023-10-22</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-generalized-likelihood-weighted-optimal-sampling-algorithm-for-rare-event-probability-quantification"><a href="#A-generalized-likelihood-weighted-optimal-sampling-algorithm-for-rare-event-probability-quantification" class="headerlink" title="A generalized likelihood-weighted optimal sampling algorithm for rare-event probability quantification"></a>A generalized likelihood-weighted optimal sampling algorithm for rare-event probability quantification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14457">http://arxiv.org/abs/2310.14457</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/umbrellagong/gpextreme">https://github.com/umbrellagong/gpextreme</a></li>
<li>paper_authors: Xianliang Gong, Yulin Pan</li>
<li>for: 用于效率地量化输入-响应系统中罕见事件的统计数据</li>
<li>methods: 使用一种新的获取函数，该函数是原始的可能性权重（LW）获取函数的扩展，可以更好地 Address 两个缺陷：1）输入空间相关罕见响应的采样不充分; 2）模型可能具有较大偏差，尤其是在复杂的输入-响应函数和有限样本情况下。</li>
<li>results: 比原始LW获取函数更高效，在多个测试 caso中显示出超过一个级别的性能提升，并在工程应用中用于船在随机海洋中罕见滚动统计量化。<details>
<summary>Abstract</summary>
In this work, we introduce a new acquisition function for sequential sampling to efficiently quantify rare-event statistics of an input-to-response (ItR) system with given input probability and expensive function evaluations. Our acquisition is a generalization of the likelihood-weighted (LW) acquisition that was initially designed for the same purpose and then extended to many other applications. The improvement in our acquisition comes from the generalized form with two additional parameters, by varying which one can target and address two weaknesses of the original LW acquisition: (1) that the input space associated with rare-event responses is not sufficiently stressed in sampling; (2) that the surrogate model (generated from samples) may have significant deviation from the true ItR function, especially for cases with complex ItR function and limited number of samples. In addition, we develop a critical procedure in Monte-Carlo discrete optimization of the acquisition function, which achieves orders of magnitude acceleration compared to existing approaches for such type of problems. The superior performance of our new acquisition to the original LW acquisition is demonstrated in a number of test cases, including some cases that were designed to show the effectiveness of the original LW acquisition. We finally apply our method to an engineering example to quantify the rare-event roll-motion statistics of a ship in a random sea.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们介绍了一种新的获取函数，用于Sequential Sampling来有效地量化输入-响应（ItR）系统的罕见事件统计。我们的获取函数是原始的可信度权重（LW）获取函数的推广，通过两个额外参数，可以根据不同的目标进行调整。这两个参数可以解决原始LW获取函数的两个弱点：（1）输入空间相关罕见响应的抽样不充分;（2）基于抽样生成的模型（surrogate model）可能与真实ItR函数存在显著差异，特别是在复杂ItR函数和有限样本情况下。此外，我们还开发了一种在Monte-Carlo精确优化中的关键程序，可以实现多orders of magnitude的加速。我们的新获取函数比原始LW获取函数表现更优异，这被证明在一些测试案例中，包括一些用于测试原始LW获取函数的案例。最后，我们应用了我们的方法到一个工程实例，以量化船在随机海洋中的罕见滚动统计。
</details></li>
</ul>
<hr>
<h2 id="Mobile-Traffic-Prediction-at-the-Edge-through-Distributed-and-Transfer-Learning"><a href="#Mobile-Traffic-Prediction-at-the-Edge-through-Distributed-and-Transfer-Learning" class="headerlink" title="Mobile Traffic Prediction at the Edge through Distributed and Transfer Learning"></a>Mobile Traffic Prediction at the Edge through Distributed and Transfer Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14456">http://arxiv.org/abs/2310.14456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alfredo Petrella, Marco Miozzo, Paolo Dini</li>
<li>for: 这个论文旨在预测移动网络流量，以便智能优化移动网络。</li>
<li>methods: 该论文提出了一种基于边缘计算的预测框架，使用边缘上获取的数据进行预测。两种主要的深度学习架构，基于卷积神经网络（CNN）和循环神经网络（RNN），在不同的训练条件下进行测试。此外，该论文还应用了知识传递学习（KTL）技术来提高模型的性能，同时减少计算资源的需求。</li>
<li>results: 实验结果显示，CNN架构在RNNs之上表现出色，并提供了预测能力的估计。KTL技术能够降低模型的能量占用率，其中对CNNs和RNNs的预测模型而言，能量占用率下降60%和90%。最后，该论文还应用了两种前沿的解释性人工智能技术来解释得到的学习模型。<details>
<summary>Abstract</summary>
Traffic prediction represents one of the crucial tasks for smartly optimizing the mobile network. The research in this topic concentrated in making predictions in a centralized fashion, i.e., by collecting data from the different network elements. This translates to a considerable amount of energy for data transmission and processing. In this work, we propose a novel prediction framework based on edge computing which uses datasets obtained on the edge through a large measurement campaign. Two main Deep Learning architectures are designed, based on Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), and tested under different training conditions. In addition, Knowledge Transfer Learning (KTL) techniques are employed to improve the performance of the models while reducing the required computational resources. Simulation results show that the CNN architectures outperform the RNNs. An estimation for the needed training energy is provided, highlighting KTL ability to reduce the energy footprint of the models of 60% and 90% for CNNs and RNNs, respectively. Finally, two cutting-edge explainable Artificial Intelligence techniques are employed to interpret the derived learning models.
</details>
<details>
<summary>摘要</summary>
traffic prediction 是智能化 моби 网络的一项重要任务。研究在这个领域集中式的方式进行预测，即通过不同的网络元素收集数据。这会带来很大的能源消耗 для数据传输和处理。在这项工作中，我们提出了一种基于边缘计算的预测框架，使用在边缘上进行测量 campaigndata  obtain。我们设计了两种主要的深度学习架构，基于卷积神经网络（CNNs）和循环神经网络（RNNs），并在不同的训练条件下进行测试。此外，我们还利用了知识传递学习（KTL）技术来提高模型的性能，同时降低计算资源的需求。 simulation 结果表明，CNN 架构在 RNNs 之上表现出色。我们还提供了预测模型所需的训练能源的估算，并显示KTL 技术可以降低模型的能源占用率，分别为 60% 和 90%  для CNNs 和 RNNs。最后，我们利用了两种前沿的解释性人工智能技术来解释 deriv 的学习模型。
</details></li>
</ul>
<hr>
<h2 id="An-International-Consortium-for-Evaluations-of-Societal-Scale-Risks-from-Advanced-AI"><a href="#An-International-Consortium-for-Evaluations-of-Societal-Scale-Risks-from-Advanced-AI" class="headerlink" title="An International Consortium for Evaluations of Societal-Scale Risks from Advanced AI"></a>An International Consortium for Evaluations of Societal-Scale Risks from Advanced AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14455">http://arxiv.org/abs/2310.14455</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ross Gruetzemacher, Alan Chan, Kevin Frazier, Christy Manning, Štěpán Los, James Fox, José Hernández-Orallo, John Burden, Matija Franklin, Clíodhna Ní Ghuidhir, Mark Bailey, Daniel Eth, Toby Pilditch, Kyle Kilian<br>for: The paper is written to address the need for effective governance and regulation of advanced AI systems, particularly in response to the risks they pose.methods: The paper proposes the creation of an international consortium for AI risk evaluations, which would bring together AI developers and third-party evaluators to assess and mitigate risks from advanced AI systems.results: The proposed consortium could play a critical role in coordinating international efforts to manage responsible scaling policies and evaluate risks from advanced AI systems, and could potentially help to mitigate societal-scale risks from these systems.<details>
<summary>Abstract</summary>
Given rapid progress toward advanced AI and risks from frontier AI systems (advanced AI systems pushing the boundaries of the AI capabilities frontier), the creation and implementation of AI governance and regulatory schemes deserves prioritization and substantial investment. However, the status quo is untenable and, frankly, dangerous. A regulatory gap has permitted AI labs to conduct research, development, and deployment activities with minimal oversight. In response, frontier AI system evaluations have been proposed as a way of assessing risks from the development and deployment of frontier AI systems. Yet, the budding AI risk evaluation ecosystem faces significant coordination challenges, such as a limited diversity of evaluators, suboptimal allocation of effort, and perverse incentives. This paper proposes a solution in the form of an international consortium for AI risk evaluations, comprising both AI developers and third-party AI risk evaluators. Such a consortium could play a critical role in international efforts to mitigate societal-scale risks from advanced AI, including in managing responsible scaling policies and coordinated evaluation-based risk response. In this paper, we discuss the current evaluation ecosystem and its shortcomings, propose an international consortium for advanced AI risk evaluations, discuss issues regarding its implementation, discuss lessons that can be learnt from previous international institutions and existing proposals for international AI governance institutions, and, finally, we recommend concrete steps to advance the establishment of the proposed consortium: (i) solicit feedback from stakeholders, (ii) conduct additional research, (iii) conduct a workshop(s) for stakeholders, (iv) analyze feedback and create final proposal, (v) solicit funding, and (vi) create a consortium.
</details>
<details>
<summary>摘要</summary>
随着人工智能的快速进步和前iers AI 系统的风险 (前iers AI 系统在智能能力前iers的边缘进行推进), 建立和实施人工智能治理和规则制定的优先级和投入是需要优先考虑的。然而，当前的情况是不可持续和危险的。AI 实验室在研究、开发和部署活动中受到最小监管的情况下进行了研究，开发和部署活动。因此，前iers AI 系统评估被提出来评估前iers AI 系统的风险。然而，AI 风险评估生态系统面临着 significi cant 协调挑战，如评估人员的匮乏多样性、资源的不均分配和反面的激励。本文提出一种解决方案，即成立国际合作组织 для AI 风险评估，包括 AI 开发者和第三方 AI 风险评估人员。这种合作组织可以在国际努力中减轻社会规模的风险，包括负责任的扩展策略和协调评估基础的风险应对。本文首先介绍当前评估生态系统的缺陷和不足，然后提出了国际合作组织的建议，包括实施问题、学习自前例和现有的国际 AI 治理建议、以及实施步骤。 Specifically, the text has been translated as follows:随着人工智能的快速进步和前iers AI 系统的风险 (前iers AI 系统在智能能力前iers的边缘进行推进), 建立和实施人工智能治理和规则制定的优先级和投入是需要优先考虑的。然而，当前的情况是不可持续和危险的。AI 实验室在研究、开发和部署活动中受到最小监管的情况下进行了研究，开发和部署活动。因此，前iers AI 系统评估被提出来评估前iers AI 系统的风险。然而，AI 风险评估生态系统面临着 significiant 协调挑战，如评估人员的匮乏多样性、资源的不均分配和反面的激励。本文提出一种解决方案，即成立国际合作组织 для AI 风险评估，包括 AI 开发者和第三方 AI 风险评估人员。这种合作组织可以在国际努力中减轻社会规模的风险，包括负责任的扩展策略和协调评估基础的风险应对。
</details></li>
</ul>
<hr>
<h2 id="Retrieval-Augmented-Chain-of-Thought-in-Semi-structured-Domains"><a href="#Retrieval-Augmented-Chain-of-Thought-in-Semi-structured-Domains" class="headerlink" title="Retrieval-Augmented Chain-of-Thought in Semi-structured Domains"></a>Retrieval-Augmented Chain-of-Thought in Semi-structured Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14435">http://arxiv.org/abs/2310.14435</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vaibhavg152/Retrieval-Augmented-Chain-of-Thought-in-Semi-structured-Domains">https://github.com/vaibhavg152/Retrieval-Augmented-Chain-of-Thought-in-Semi-structured-Domains</a></li>
<li>paper_authors: Vaibhav Mavi, Abulhair Saparov, Chen Zhao</li>
<li>for: 法律和金融领域的问答系统中使用现有的问答系统会存在一些挑战，需要具备专业知识。</li>
<li>methods: 这篇论文探讨了利用法律和金融数据的半结构化特性，以高效地检索相关的上下文，使用大语言模型（LLMs）进行领域专业的问答。</li>
<li>results: 这项研究的系统比当前模型高效，同时还提供了有用的解释，激励将LLMs integrate into 法律和金融NLP系统中进行未来研究。<details>
<summary>Abstract</summary>
Applying existing question answering (QA) systems to specialized domains like law and finance presents challenges that necessitate domain expertise. Although large language models (LLMs) have shown impressive language comprehension and in-context learning capabilities, their inability to handle very long inputs/contexts is well known. Tasks specific to these domains need significant background knowledge, leading to contexts that can often exceed the maximum length that existing LLMs can process. This study explores leveraging the semi-structured nature of legal and financial data to efficiently retrieve relevant context, enabling the use of LLMs for domain-specialized QA. The resulting system outperforms contemporary models and also provides useful explanations for the answers, encouraging the integration of LLMs into legal and financial NLP systems for future research.
</details>
<details>
<summary>摘要</summary>
使用现有的问答（QA）系统在特定领域如法律和金融中存在挑战，需要领域专业知识。虽然大型自然语言模型（LLM）在语言理解和上下文学习能力方面表现出色，但它们无法处理非常长的输入/上下文，这是公共知识。这些领域的任务需要背景知识，导致上下文可以经常超过现有的 LLM 处理 longest length。这项研究探讨了利用法律和金融数据的半结构化特性，以高效地检索相关上下文，使用 LLM 进行领域化问答。该系统比当代模型高效，并提供了有用的解释，鼓励将 LLM integrated 到法律和金融 NLP 系统中 для未来研究。
</details></li>
</ul>
<hr>
<h2 id="Which-Prompts-Make-The-Difference-Data-Prioritization-For-Efficient-Human-LLM-Evaluation"><a href="#Which-Prompts-Make-The-Difference-Data-Prioritization-For-Efficient-Human-LLM-Evaluation" class="headerlink" title="Which Prompts Make The Difference? Data Prioritization For Efficient Human LLM Evaluation"></a>Which Prompts Make The Difference? Data Prioritization For Efficient Human LLM Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14424">http://arxiv.org/abs/2310.14424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meriem Boubdir, Edward Kim, Beyza Ermis, Marzieh Fadaee, Sara Hooker<br>for: 大型自然语言模型的评估中，人类评估变得越来越重要，可以更好地捕捉语言细节和用户喜好。methods: 我们使用度量方法来减少人类反馈的数量，以提高模型评估的效率。results: 我们发现，使用度量方法可以减少模型评估中的决定性（或“tie”）结果，比Random Sample减少了54%。这表明我们的方法可以更好地使用人类反馈，从而提高模型评估的效率。<details>
<summary>Abstract</summary>
Human evaluation is increasingly critical for assessing large language models, capturing linguistic nuances, and reflecting user preferences more accurately than traditional automated metrics. However, the resource-intensive nature of this type of annotation process poses significant challenges. The key question driving our work: "is it feasible to minimize human-in-the-loop feedback by prioritizing data instances which most effectively distinguish between models?" We evaluate several metric-based methods and find that these metrics enhance the efficiency of human evaluations by minimizing the number of required annotations, thus saving time and cost, while ensuring a robust performance evaluation. We show that our method is effective across widely used model families, reducing instances of indecisive (or "tie") outcomes by up to 54% compared to a random sample when focusing on the top-20 percentile of prioritized instances. This potential reduction in required human effort positions our approach as a valuable strategy in future large language model evaluations.
</details>
<details>
<summary>摘要</summary>
人类评估在评估大型自然语言模型时变得越来越重要，捕捉语言细节和用户偏好更加准确 than traditional自动指标。然而，这种类型的注释过程具有资源占用性的挑战。我们的问题驱动我们的工作："是否可以减少人工循环反馈？"我们评估了一些度量基于的方法，并发现这些度量可以提高人类评估的效率，减少需要注释的数量，从而节省时间和成本，同时保证评估性能的Robustness。我们的方法在广泛使用的模型家族上效果，减少 indecisive（或“tie”）结果的数量，相比随机抽样，下降54%。这种减少的人工努力位置我们的方法为未来大型自然语言模型评估中的有价值策略。
</details></li>
</ul>
<hr>
<h2 id="Monte-Carlo-Thought-Search-Large-Language-Model-Querying-for-Complex-Scientific-Reasoning-in-Catalyst-Design"><a href="#Monte-Carlo-Thought-Search-Large-Language-Model-Querying-for-Complex-Scientific-Reasoning-in-Catalyst-Design" class="headerlink" title="Monte Carlo Thought Search: Large Language Model Querying for Complex Scientific Reasoning in Catalyst Design"></a>Monte Carlo Thought Search: Large Language Model Querying for Complex Scientific Reasoning in Catalyst Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14420">http://arxiv.org/abs/2310.14420</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pnnl/chemreasoner">https://github.com/pnnl/chemreasoner</a></li>
<li>paper_authors: Henry W. Sprueill, Carl Edwards, Mariefel V. Olarte, Udishnu Sanyal, Heng Ji, Sutanay Choudhury</li>
<li>for: 本研究目的是找到新的催化剂，需要复杂的化学性质和结果之间的权衡，导致搜索空间的 combinatorial 增长。</li>
<li>methods: 本研究使用 Monte Carlo Tree Search 方法，以提高 state-of-the-art chain-of-thought prompting 变种，以增强科学理解。</li>
<li>results: 我们的方法比最佳基eline提高 25.8%，并发现我们的方法可以增强科学家的理解和发现过程，提供新的想法和发现。<details>
<summary>Abstract</summary>
Discovering novel catalysts requires complex reasoning involving multiple chemical properties and resultant trade-offs, leading to a combinatorial growth in the search space. While large language models (LLM) have demonstrated novel capabilities for chemistry through complex instruction following capabilities and high quality reasoning, a goal-driven combinatorial search using LLMs has not been explored in detail. In this work, we present a Monte Carlo Tree Search-based approach that improves beyond state-of-the-art chain-of-thought prompting variants to augment scientific reasoning. We introduce two new reasoning datasets: 1) a curation of computational chemistry simulations, and 2) diverse questions written by catalysis researchers for reasoning about novel chemical conversion processes. We improve over the best baseline by 25.8\% and find that our approach can augment scientist's reasoning and discovery process with novel insights.
</details>
<details>
<summary>摘要</summary>
发现新的催化剂需要复杂的逻辑，涉及多种化学性质和结果的让步，从而导致搜索空间的 combinatorial 增长。虽然大型自然语言模型（LLM）已经在化学中表现出了新的能力，但一种以目标为导向的 combinatorial 搜索使用 LLM 尚未得到了详细的探索。在这种工作中，我们提出了基于 Monte Carlo Tree Search 的方法，可以超越现有的状态艺术探索变体，以增强科学逻辑。我们 introduce 了两个新的逻辑数据集：1）一个 Computational chemistry  simulations 的筛选，2） catalysis 研究者写的多种关于新的化学转化过程的问题。我们在最佳基eline上提高了25.8％，并发现我们的方法可以增强科学家的逻辑和发现过程，并提供新的发现。
</details></li>
</ul>
<hr>
<h2 id="Vision-Language-Models-in-Autonomous-Driving-and-Intelligent-Transportation-Systems"><a href="#Vision-Language-Models-in-Autonomous-Driving-and-Intelligent-Transportation-Systems" class="headerlink" title="Vision Language Models in Autonomous Driving and Intelligent Transportation Systems"></a>Vision Language Models in Autonomous Driving and Intelligent Transportation Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14414">http://arxiv.org/abs/2310.14414</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingcheng Zhou, Mingyu Liu, Bare Luka Zagar, Ekim Yurtsever, Alois C. Knoll</li>
<li>for: 本研究的目的是为Autonomous Driving (AD)和Intelligent Transportation Systems (ITS)领域内的视觉语言模型（VLM）做出全面的检视和评估，以探讨当前模型和数据集的进展，以及未来研究方向。</li>
<li>methods: 本研究使用了许多当前最佳的语言模型，包括Large Language Models (LLMs)，以及一些特定的交通和驾驶数据集。</li>
<li>results: 本研究发现了许多视觉语言模型在Autonomous Driving (AD)和Intelligent Transportation Systems (ITS)领域的应用，包括改善驾驶安全性和效率，以及探索新的研究方向。同时，研究还发现了一些挑战和研究缺失，需要进一步的研究和解决。<details>
<summary>Abstract</summary>
The applications of Vision-Language Models (VLMs) in the fields of Autonomous Driving (AD) and Intelligent Transportation Systems (ITS) have attracted widespread attention due to their outstanding performance and the ability to leverage Large Language Models (LLMs). By integrating language data, the vehicles, and transportation systems are able to deeply understand real-world environments, improving driving safety and efficiency. In this work, we present a comprehensive survey of the advances in language models in this domain, encompassing current models and datasets. Additionally, we explore the potential applications and emerging research directions. Finally, we thoroughly discuss the challenges and research gap. The paper aims to provide researchers with the current work and future trends of VLMs in AD and ITS.
</details>
<details>
<summary>摘要</summary>
自然语言模型（VLM）在自动驾驶（AD）和智能交通系统（ITS）领域的应用已经吸引了广泛的注意，这主要归功于它们在实际环境中的出色表现以及能够利用大型语言模型（LLM）的优势。通过语言数据的 интеграción，车辆和交通系统能够深入理解现实环境，提高驾驶安全性和效率。在这篇论文中，我们提供了自然语言模型在这个领域的全面报告，涵盖当前的模型和数据集。此外，我们还探讨了这个领域的潜在应用和出现的研究方向。最后，我们详细讨论了这个领域的挑战和研究空白。本文的目标是为研究人员提供当前和未来自然语言模型在AD和ITS领域的进展和趋势。
</details></li>
</ul>
<hr>
<h2 id="Be-Selfish-But-Wisely-Investigating-the-Impact-of-Agent-Personality-in-Mixed-Motive-Human-Agent-Interactions"><a href="#Be-Selfish-But-Wisely-Investigating-the-Impact-of-Agent-Personality-in-Mixed-Motive-Human-Agent-Interactions" class="headerlink" title="Be Selfish, But Wisely: Investigating the Impact of Agent Personality in Mixed-Motive Human-Agent Interactions"></a>Be Selfish, But Wisely: Investigating the Impact of Agent Personality in Mixed-Motive Human-Agent Interactions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14404">http://arxiv.org/abs/2310.14404</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kushal Chawla, Ian Wu, Yu Rong, Gale M. Lucas, Jonathan Gratch<br>for: 这种方法是为了设计一个谈判对话系统。methods: 这种方法使用自我游戏学习，训练一个与人类对话数据模拟的用户。results: 这种方法会导致谈判对话系统失去妥协的价值，导致对方不再谈判，最终影响系统的表现。<details>
<summary>Abstract</summary>
A natural way to design a negotiation dialogue system is via self-play RL: train an agent that learns to maximize its performance by interacting with a simulated user that has been designed to imitate human-human dialogue data. Although this procedure has been adopted in prior work, we find that it results in a fundamentally flawed system that fails to learn the value of compromise in a negotiation, which can often lead to no agreements (i.e., the partner walking away without a deal), ultimately hurting the model's overall performance. We investigate this observation in the context of the DealOrNoDeal task, a multi-issue negotiation over books, hats, and balls. Grounded in negotiation theory from Economics, we modify the training procedure in two novel ways to design agents with diverse personalities and analyze their performance with human partners. We find that although both techniques show promise, a selfish agent, which maximizes its own performance while also avoiding walkaways, performs superior to other variants by implicitly learning to generate value for both itself and the negotiation partner. We discuss the implications of our findings for what it means to be a successful negotiation dialogue system and how these systems should be designed in the future.
</details>
<details>
<summary>摘要</summary>
自然的方式设计一个谈判对话系统是通过自我游戏学习：训练一个智能代理人，它通过与一个模拟人类对话数据的虚拟用户进行交互来学习提高自己的表现。尽管这种方法在先前的工作中已经被采用，但我们发现它会导致谈判中寻求妥协的核心问题不得到学习，这可能导致对话失败（即对方不达成协议），最终影响模型的总性表现。我们在DealOrNoDeal任务中进行了研究，这是一种多个问题的谈判，涉及到书、帽子和球。基于经济学的谈判理论，我们修改了训练过程的两种新方法，以设计具有多样化个性的代理人，并分析它们与人类伙伴的表现。我们发现，尽管这两种技术都有承诺，但一个自私的代理人（它尽量提高自己的表现，同时避免对话失败）在其他变体中表现出色，并隐式地学习生成对话伙伴和自己都有价值的情况。我们讨论了我们的发现对成功谈判对话系统的设计意味着什么，以及未来这些系统应该如何设计。
</details></li>
</ul>
<hr>
<h2 id="O3D-Offline-Data-driven-Discovery-and-Distillation-for-Sequential-Decision-Making-with-Large-Language-Models"><a href="#O3D-Offline-Data-driven-Discovery-and-Distillation-for-Sequential-Decision-Making-with-Large-Language-Models" class="headerlink" title="O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making with Large Language Models"></a>O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14403">http://arxiv.org/abs/2310.14403</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuchen Xiao, Yanchao Sun, Mengda Xu, Udari Madhushani, Jared Vann, Deepeka Garg, Sumitra Ganesh</li>
<li>for: 提高大型语言模型（LLM）在解决sequential decision-making问题的表现</li>
<li>methods: 利用offline数据 scale（例如人类交互的日志）来提高LLM代理的in-context learning性能</li>
<li>results: O3D框架可以帮助LLM代理不需要训练就能够解决复杂和长时间的任务，并在多个任务中提取普遍可用的技能和知识<details>
<summary>Abstract</summary>
Recent advancements in large language models (LLMs) have exhibited promising performance in solving sequential decision-making problems. By imitating few-shot examples provided in the prompts (i.e., in-context learning), an LLM agent can interact with an external environment and complete given tasks without additional training. However, such few-shot examples are often insufficient to generate high-quality solutions for complex and long-horizon tasks, while the limited context length cannot consume larger-scale demonstrations. To this end, we propose an offline learning framework that utilizes offline data at scale (e.g, logs of human interactions) to facilitate the in-context learning performance of LLM agents. We formally define LLM-powered policies with both text-based approaches and code-based approaches. We then introduce an Offline Data-driven Discovery and Distillation (O3D) framework to improve LLM-powered policies without finetuning. O3D automatically discovers reusable skills and distills generalizable knowledge across multiple tasks based on offline interaction data, advancing the capability of solving downstream tasks. Empirical results under two interactive decision-making benchmarks (ALFWorld and WebShop) demonstrate that O3D can notably enhance the decision-making capabilities of LLMs through the offline discovery and distillation process, and consistently outperform baselines across various LLMs with both text-based-policy and code-based-policy.
</details>
<details>
<summary>摘要</summary>
We define LLM-powered policies with both text-based and code-based approaches, and introduce an Offline Data-driven Discovery and Distillation (O3D) framework to improve these policies without finetuning. O3D automatically discovers reusable skills and distills generalizable knowledge across multiple tasks based on offline interaction data, enabling LLMs to solve downstream tasks more effectively.Empirical results under two interactive decision-making benchmarks (ALFWorld and WebShop) show that O3D can significantly enhance the decision-making capabilities of LLMs through the offline discovery and distillation process, and consistently outperform baselines across various LLMs with both text-based and code-based policies.
</details></li>
</ul>
<hr>
<h2 id="Value-of-Assistance-for-Grasping"><a href="#Value-of-Assistance-for-Grasping" class="headerlink" title="Value of Assistance for Grasping"></a>Value of Assistance for Grasping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14402">http://arxiv.org/abs/2310.14402</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Masarwy, Yuval Goshen, David Dovrat, Sarah Keren</li>
<li>for:  robotic grasping task with uncertain object pose</li>
<li>methods: probabilistic estimation of object pose, VOA measure for assessing observation effectiveness</li>
<li>results: effective in simulated and real-world robotic settings<details>
<summary>Abstract</summary>
In many realistic settings, a robot is tasked with grasping an object without knowing its exact pose. Instead, the robot relies on a probabilistic estimation of the pose to decide how to attempt the grasp. We offer a novel Value of Assistance (VOA) measure for assessing the expected effect a specific observation will have on the robot's ability to successfully complete the grasp. Thus, VOA supports the decision of which sensing action would be most beneficial to the grasping task. We evaluate our suggested measures in both simulated and real-world robotic settings.
</details>
<details>
<summary>摘要</summary>
在许多现实场景中，机器人被要求抓取物品而不知其具体位置。而是通过 probabilistic 估计pose 来决定机器人是如何尝试抓取。我们提出了一种新的值帮助度（VOA）测量方法，用于评估抓取任务中具体的观察对机器人成功完成的影响。因此，VOA 支持机器人选择哪一种感知行为最有利于抓取任务。我们在实验室和实际机器人设置中评估了我们的建议。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-bag-with-a-simulation-free-reinforcement-learning-framework-for-robots"><a href="#Learning-to-bag-with-a-simulation-free-reinforcement-learning-framework-for-robots" class="headerlink" title="Learning to bag with a simulation-free reinforcement learning framework for robots"></a>Learning to bag with a simulation-free reinforcement learning framework for robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14398">http://arxiv.org/abs/2310.14398</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francisco Munguia-Galeano, Jihong Zhu, Juan David Hernández, Ze Ji</li>
<li>for: 这 paper 的目的是教育机器人如何包袋 (bagging) deformable objects, such as bags.</li>
<li>methods: 这 paper 使用了一种学习基于权重算法的框架, 可以在实际世界中学习包袋任务, 不需要使用模拟环境. 该框架使用了一组基本动作和五个状态来表示任务, 并通过一种强化学习算法来找到最佳抓取点.</li>
<li>results: 在实际世界中训练了三个小时后, 框架可以在不同的包袋任务中达到60%和80%的成功率, 并且在两个不同的袋子大小进行测试, 发现模型具有普适性.<details>
<summary>Abstract</summary>
Bagging is an essential skill that humans perform in their daily activities. However, deformable objects, such as bags, are complex for robots to manipulate. This paper presents an efficient learning-based framework that enables robots to learn bagging. The novelty of this framework is its ability to perform bagging without relying on simulations. The learning process is accomplished through a reinforcement learning algorithm introduced in this work, designed to find the best grasping points of the bag based on a set of compact state representations. The framework utilizes a set of primitive actions and represents the task in five states. In our experiments, the framework reaches a 60 % and 80 % of success rate after around three hours of training in the real world when starting the bagging task from folded and unfolded, respectively. Finally, we test the trained model with two more bags of different sizes to evaluate its generalizability.
</details>
<details>
<summary>摘要</summary>
“bagging”是人类日常活动中的一种重要技能，但是柔软对象，如袋子，对机器人来说是复杂的 manipulate 的。这篇论文提出了一种高效的学习基于框架，使机器人能够学习bagging。这个框架的新特点在于不需要仿真环境，通过在这篇论文中介绍的一种强化学习算法，找到最佳抓取袋子的点。该框架使用了一组基本动作和五种状态来表示任务。在我们的实验中，该框架在真实世界中训练了三个小时后，在开始包装任务时达到了60%和80%的成功率，即从折叠和 unfolded 开始。最后，我们测试了训练后的模型，对两个不同大小的袋子进行了评估。
</details></li>
</ul>
<hr>
<h2 id="Merging-Generated-and-Retrieved-Knowledge-for-Open-Domain-QA"><a href="#Merging-Generated-and-Retrieved-Knowledge-for-Open-Domain-QA" class="headerlink" title="Merging Generated and Retrieved Knowledge for Open-Domain QA"></a>Merging Generated and Retrieved Knowledge for Open-Domain QA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14393">http://arxiv.org/abs/2310.14393</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yunx-z/combo">https://github.com/yunx-z/combo</a></li>
<li>paper_authors: Yunxiang Zhang, Muhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, Lu Wang</li>
<li>For: The paper is written for improving open-domain question answering (QA) systems by effectively leveraging two sources of information: retrieved passages and large language models (LLMs).* Methods: The paper proposes a Compatibility-Oriented knowledge Merging (COMBO) framework that matches LLM-generated passages with retrieved counterparts into compatible pairs, based on discriminators trained with silver compatibility labels. The framework uses a Fusion-in-Decoder-based reader model to handle passage pairs and arrive at the final answer.* Results: The paper shows that COMBO outperforms competitive baselines on three out of four tested open-domain QA benchmarks, and demonstrates greater efficacy in scenarios with a higher degree of knowledge conflicts.<details>
<summary>Abstract</summary>
Open-domain question answering (QA) systems are often built with retrieval modules. However, retrieving passages from a given source is known to suffer from insufficient knowledge coverage. Alternatively, prompting large language models (LLMs) to generate contextual passages based on their parametric knowledge has been shown to improve QA performance. Yet, LLMs tend to "hallucinate" content that conflicts with the retrieved knowledge. Based on the intuition that answers supported by both sources are more likely to be correct, we propose COMBO, a Compatibility-Oriented knowledge Merging for Better Open-domain QA framework, to effectively leverage the two sources of information. Concretely, we match LLM-generated passages with retrieved counterparts into compatible pairs, based on discriminators trained with silver compatibility labels. Then a Fusion-in-Decoder-based reader model handles passage pairs to arrive at the final answer. Experiments show that COMBO outperforms competitive baselines on three out of four tested open-domain QA benchmarks. Further analysis reveals that our proposed framework demonstrates greater efficacy in scenarios with a higher degree of knowledge conflicts.
</details>
<details>
<summary>摘要</summary>
具体来说，我们匹配LLM生成的passages与检索的对应者成Compatible pairs，基于训练用silver compatibility labels的Discriminators。然后，一个Fusion-in-Decoder-based reader model处理过程来到答案。实验表明，COMBO超过了竞争对手在四个测试的开放领域QA benchmarks。进一步的分析表明，我们的提议的Frameworks在知识冲突程度较高的场景中更有效。
</details></li>
</ul>
<hr>
<h2 id="ARCOQ-Arabic-Closest-Opposite-Questions-Dataset"><a href="#ARCOQ-Arabic-Closest-Opposite-Questions-Dataset" class="headerlink" title="ARCOQ: Arabic Closest Opposite Questions Dataset"></a>ARCOQ: Arabic Closest Opposite Questions Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14384">http://arxiv.org/abs/2310.14384</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sandrarizkallah/arcoq-dataset">https://github.com/sandrarizkallah/arcoq-dataset</a></li>
<li>paper_authors: Sandra Rizkallah, Amir F. Atiya, Samir Shaheen</li>
<li>for: 本研究提供了一个 Arabic 语言最近相反问题的数据集，这是首个为 Arabic 语言而设计的数据集。这个数据集对于 Antonymy 检测系统评估非常有用，结构类似于英语 Graduate Record Examination (GRE) 最近相反问题数据集。</li>
<li>methods: 本研究使用了一个 queries 和候选词集合来构建数据集，每个问题都包含一个查询词，需要从候选词中找到最近相反词。每个问题还关联着正确的答案。此外，文章还提供了一个基于 Arabic 词嵌入模型的性能标准。</li>
<li>results: 本研究提供了一个公共可用的数据集，以及一个标准的开发集和测试集分割。文章还提供了一些基于 Arabic 词嵌入模型的性能统计数据，以用于评估不同的 Antonymy 检测系统。<details>
<summary>Abstract</summary>
This paper presents a dataset for closest opposite questions in Arabic language. The dataset is the first of its kind for the Arabic language. It is beneficial for the assessment of systems on the aspect of antonymy detection. The structure is similar to that of the Graduate Record Examination (GRE) closest opposite questions dataset for the English language. The introduced dataset consists of 500 questions, each contains a query word for which the closest opposite needs to be determined from among a set of candidate words. Each question is also associated with the correct answer. We publish the dataset publicly in addition to providing standard splits of the dataset into development and test sets. Moreover, the paper provides a benchmark for the performance of different Arabic word embedding models on the introduced dataset.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MoPe-Model-Perturbation-based-Privacy-Attacks-on-Language-Models"><a href="#MoPe-Model-Perturbation-based-Privacy-Attacks-on-Language-Models" class="headerlink" title="MoPe: Model Perturbation-based Privacy Attacks on Language Models"></a>MoPe: Model Perturbation-based Privacy Attacks on Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14369">http://arxiv.org/abs/2310.14369</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marvin Li, Jason Wang, Jeffrey Wang, Seth Neel</li>
<li>for: 这个研究旨在检测大型自然语言模型（LLMs）是否会不意外地透露训练数据中的敏感信息。</li>
<li>methods: 这篇文章提出了一个新的方法，即模型扰动（MoPe），可以高度确定一个文本是否在一个预训语言模型的训练数据中。MoPe 在模型参数空间添加随机变动，然后测量模型在某个点 $x$ 的落差 log-likelihood，这个统计我们显示可以近似跟训练点的希尔伯特矩阵的跟踪。</li>
<li>results: 在各种语言模型，从 70 万到 12 亿个参数，我们的 MoPe 方法比存在损失基于的攻击和最近提出的扰动基于方法更有效。我们还考虑了训练点顺序和模型大小对于攻击成功的影响，并实际显示 MoPe 可以实际地近似希尔伯特矩阵的跟踪。我们的结果显示，损失单点没有能力决定抽取性 – 有些训练点可以使用我们的方法恢复，它们的平均损失都很高。这些结果质疑了之前的研究，它们使用单点损失作为训练点忘记或忘记的证据。<details>
<summary>Abstract</summary>
Recent work has shown that Large Language Models (LLMs) can unintentionally leak sensitive information present in their training data. In this paper, we present Model Perturbations (MoPe), a new method to identify with high confidence if a given text is in the training data of a pre-trained language model, given white-box access to the models parameters. MoPe adds noise to the model in parameter space and measures the drop in log-likelihood at a given point $x$, a statistic we show approximates the trace of the Hessian matrix with respect to model parameters. Across language models ranging from $70$M to $12$B parameters, we show that MoPe is more effective than existing loss-based attacks and recently proposed perturbation-based methods. We also examine the role of training point order and model size in attack success, and empirically demonstrate that MoPe accurately approximate the trace of the Hessian in practice. Our results show that the loss of a point alone is insufficient to determine extractability -- there are training points we can recover using our method that have average loss. This casts some doubt on prior works that use the loss of a point as evidence of memorization or unlearning.
</details>
<details>
<summary>摘要</summary>
最近的研究表明，大型语言模型（LLM）可能会不意气地泄露训练数据中的敏感信息。在这篇论文中，我们提出了一种新的方法 called Model Perturbations（MoPe），可以在给定文本是否在预训练语言模型的训练数据中的高度置信度测试。MoPe在模型参数空间添加噪声，并测量在给定点 $x$ 上模型参数的下降 log-likelihood 的度量，我们证明这个度量与模型参数对Trace Hessian 矩阵很相似。在7000万到120亿参数的语言模型之间，我们证明MoPe比既有损失基于攻击和最近提出的噪声基于方法更有效。我们还研究了训练点顺序和模型大小对攻击成功的影响，并实际证明MoPe可以准确地估计Trace Hessian 在实践中。我们的结果表明，单个点的损失不能准确地确定抽取性——有些训练点可以通过我们的方法恢复，它们的平均损失高。这些结果对于以前的研究，使用单个点的损失作为忘记或不记忆的证据表示有一定的怀疑。
</details></li>
</ul>
<hr>
<h2 id="Right-No-Matter-Why-AI-Fact-checking-and-AI-Authority-in-Health-related-Inquiry-Settings"><a href="#Right-No-Matter-Why-AI-Fact-checking-and-AI-Authority-in-Health-related-Inquiry-Settings" class="headerlink" title="Right, No Matter Why: AI Fact-checking and AI Authority in Health-related Inquiry Settings"></a>Right, No Matter Why: AI Fact-checking and AI Authority in Health-related Inquiry Settings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14358">http://arxiv.org/abs/2310.14358</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elena Sergeeva, Anastasia Sergeeva, Huiyun Tang, Kerstin Bongard-Blanchy, Peter Szolovits</li>
<li>for: 本研究探讨用户对AI建议的接受行为，以评估健康相关声明的真实性在不同“建议质量”设定下。</li>
<li>methods: 我们采用了探索性的评估方法，通过向用户提供不同类型的AI建议来影响他们对健康相关声明的评估。</li>
<li>results: 我们发现，即使只是表示AI认为声明是错误&#x2F;正确，也可以让更 than half of the people更改他们的声明真实性评估。不同的建议类型对接受率产生影响，但是获得建议的単纯效应经常大于建议类型效应。<details>
<summary>Abstract</summary>
Previous research on expert advice-taking shows that humans exhibit two contradictory behaviors: on the one hand, people tend to overvalue their own opinions undervaluing the expert opinion, and on the other, people often defer to other people's advice even if the advice itself is rather obviously wrong. In our study, we conduct an exploratory evaluation of users' AI-advice accepting behavior when evaluating the truthfulness of a health-related statement in different "advice quality" settings. We find that even feedback that is confined to just stating that "the AI thinks that the statement is false/true" results in more than half of people moving their statement veracity assessment towards the AI suggestion. The different types of advice given influence the acceptance rates, but the sheer effect of getting a suggestion is often bigger than the suggestion-type effect.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="From-Chaos-to-Clarity-Claim-Normalization-to-Empower-Fact-Checking"><a href="#From-Chaos-to-Clarity-Claim-Normalization-to-Empower-Fact-Checking" class="headerlink" title="From Chaos to Clarity: Claim Normalization to Empower Fact-Checking"></a>From Chaos to Clarity: Claim Normalization to Empower Fact-Checking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14338">http://arxiv.org/abs/2310.14338</a></li>
<li>repo_url: None</li>
<li>paper_authors: Megha Sundriyal, Tanmoy Chakraborty, Preslav Nakov</li>
<li>For: The paper aims to help identify precise and prominent claims in social media posts that require verification, by introducing a novel task called Claim Normalization (ClaimNorm) and proposing a pioneering approach called CACN that leverages human reasoning processes and large language models.* Methods: The paper proposes a two-stage approach that first uses chain-of-thought and claim check-worthiness estimation to comprehend intricate claims, and then leverages large language models’ in-context learning abilities to provide guidance and improve the claim normalization process.* Results: The paper evaluates the effectiveness of the proposed model using a comprehensive real-world dataset (CLAN) consisting of more than 6k instances of social media posts alongside their respective normalized claims, and demonstrates that CACN outperforms several baselines across various evaluation measures.<details>
<summary>Abstract</summary>
With the proliferation of social media platforms, users are exposed to vast information, including posts containing misleading claims. However, the pervasive noise inherent in these posts presents a challenge in identifying precise and prominent claims that require verification. Extracting the core assertions from such posts is arduous and time-consuming. We introduce a novel task called Claim Normalization (aka ClaimNorm) that aims to decompose complex and noisy social media posts into more straightforward and understandable forms, termed normalized claims. We propose CACN, a pioneering approach that leverages chain-of-thought and claim check-worthiness estimation, mimicking human reasoning processes, to comprehend intricate claims. Moreover, we capitalize on large language models' powerful in-context learning abilities to provide guidance and improve the claim normalization process. To evaluate the effectiveness of our proposed model, we meticulously compile a comprehensive real-world dataset, CLAN, comprising more than 6k instances of social media posts alongside their respective normalized claims. Experimentation demonstrates that CACN outperforms several baselines across various evaluation measures. A rigorous error analysis validates CACN's capabilities and pitfalls.
</details>
<details>
<summary>摘要</summary>
随着社交媒体平台的普及，用户面临着巨量的信息泥沼，其中包括含有误导性声明的帖子。然而，这些帖子中的噪音使得精确地提取声明变得困难和耗时。为了解决这个问题，我们提出了一个新任务 called Claim Normalization（简称 ClaimNorm），它的目标是将社交媒体帖子中的复杂和噪音声明转化为更加简单和易理解的形式，称为 normalized claims。我们提出了 CACN 模型，它利用链式思维和声明可靠性估计来模拟人类的思维过程，以便更好地理解复杂的声明。此外，我们利用大语言模型的强大在线学习能力，为声明Normalization过程提供指导和改进。为评估我们提出的模型效果，我们精心编译了 CLAN dataset，包含超过 6k 个社交媒体帖子和其对应的 normalized claims。实验结果表明，CACN 超过了多个基线数据进行多种评价指标。一份严格的错误分析证明了 CACN 的能力和缺点。
</details></li>
</ul>
<hr>
<h2 id="Learning-Interpretable-Rules-for-Scalable-Data-Representation-and-Classification"><a href="#Learning-Interpretable-Rules-for-Scalable-Data-Representation-and-Classification" class="headerlink" title="Learning Interpretable Rules for Scalable Data Representation and Classification"></a>Learning Interpretable Rules for Scalable Data Representation and Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14336">http://arxiv.org/abs/2310.14336</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/12wang3/rrl">https://github.com/12wang3/rrl</a></li>
<li>paper_authors: Zhuo Wang, Wei Zhang, Ning Liu, Jianyong Wang</li>
<li>for:  This paper aims to improve the scalability and interpretability of rule-based models for data representation and classification.</li>
<li>methods:  The proposed method, called Rule-based Representation Learner (RRL), uses a novel training method called Gradient Grafting to optimize the discrete model using gradient descent, and employs a novel design of logical activation functions to increase scalability.</li>
<li>results:  RRL outperforms competitive interpretable approaches on ten small and four large data sets, and can be easily adjusted to obtain a trade-off between classification accuracy and model complexity for different scenarios.<details>
<summary>Abstract</summary>
Rule-based models, e.g., decision trees, are widely used in scenarios demanding high model interpretability for their transparent inner structures and good model expressivity. However, rule-based models are hard to optimize, especially on large data sets, due to their discrete parameters and structures. Ensemble methods and fuzzy/soft rules are commonly used to improve performance, but they sacrifice the model interpretability. To obtain both good scalability and interpretability, we propose a new classifier, named Rule-based Representation Learner (RRL), that automatically learns interpretable non-fuzzy rules for data representation and classification. To train the non-differentiable RRL effectively, we project it to a continuous space and propose a novel training method, called Gradient Grafting, that can directly optimize the discrete model using gradient descent. A novel design of logical activation functions is also devised to increase the scalability of RRL and enable it to discretize the continuous features end-to-end. Exhaustive experiments on ten small and four large data sets show that RRL outperforms the competitive interpretable approaches and can be easily adjusted to obtain a trade-off between classification accuracy and model complexity for different scenarios. Our code is available at: https://github.com/12wang3/rrl.
</details>
<details>
<summary>摘要</summary>
rule-based models, such as decision trees, are widely used in scenarios that require high model interpretability because of their transparent internal structures and good model expressivity. However, rule-based models are difficult to optimize, especially on large datasets, due to their discrete parameters and structures. Ensemble methods and fuzzy/soft rules are often used to improve performance, but these sacrifices model interpretability. To achieve both good scalability and interpretability, we propose a new classifier, called Rule-based Representation Learner (RRL), that automatically learns interpretable non-fuzzy rules for data representation and classification. To train the non-differentiable RRL effectively, we project it to a continuous space and propose a novel training method, called Gradient Grafting, that can directly optimize the discrete model using gradient descent. A novel design of logical activation functions is also devised to increase the scalability of RRL and enable it to discretize the continuous features end-to-end. Exhaustive experiments on ten small and four large datasets show that RRL outperforms competitive interpretable approaches and can be easily adjusted to obtain a trade-off between classification accuracy and model complexity for different scenarios. Our code is available at: https://github.com/12wang3/rrl.Here's the Chinese translation of the text, with some notes on the translation:1. 规则型模型 (rule-based models)：这类模型广泛应用在需要高度可读性的场景中，如决策树、规则 Engines 等。这些模型的透明性和表达能力使得它们在可读性和可解释性方面表现出色。2. However, 规则型模型 (rule-based models) 具有粗糙的结构和离散参数，因此在大型数据集上进行优化很困难。为了提高性能，人们通常使用 ensemble methods 和杂化/软规则来增强模型的表达能力。3. 但是，使用 ensemble methods 和杂化/软规则来提高性能会导致模型的可读性减退。为了实现同时保持高度可读性和高度表达能力，我们提出了一种新的分类器，即 Rule-based Representation Learner (RRL)。4. RRL 可以自动学习可读性高的非杂化规则，用于数据表示和分类。为了让 RRL 可以在大型数据集上进行效果地优化，我们提出了一种新的训练方法，即 Gradient Grafting。5. 在训练非 diferenciable RRL 时，我们使用 Gradient Grafting 方法可以直接使用梯度下降来优化离散模型。此外，我们还提出了一种新的逻辑激活函数的设计，以增加 RRL 的扩展性和可扩展性。6. 我们在十个小型数据集和四个大型数据集上进行了对比性实验，结果表明 RRL 可以超越一些竞争对手的可读性方法，并且可以根据不同的场景进行轻松地调整，以获得分类精度和模型复杂度之间的平衡。7. 我们的代码可以在 GitHub 上找到：https://github.com/12wang3/rrl.Some notes on the translation:1. "rule-based models" is translated as "规则型模型" (rule-based models), which is a common term used in machine learning to refer to models that use rules to make predictions.2. "high model interpretability" is translated as "高度可读性" (high degree of interpretability), which emphasizes the ability of the model to provide insights into its decision-making process.3. "discrete parameters" is translated as "离散参数" (discrete parameters), which refers to the fact that the model's parameters are not continuous, but rather take on discrete values.4. "ensemble methods" is translated as "ensemble methods" (ensemble methods), which refers to techniques that combine the predictions of multiple models to improve overall performance.5. "fuzzy/soft rules" is translated as "杂化/软规则" (fuzzy/soft rules), which refers to rules that allow for some degree of uncertainty or vagueness in their predictions.6. " Gradient Grafting" is translated as "梯度植入" (Gradient Grafting), which is a novel training method proposed in the paper to directly optimize the discrete model using gradient descent.7. " logical activation functions" is translated as "逻辑激活函数" (logical activation functions), which refers to a type of activation function that is designed to increase the scalability of the model.
</details></li>
</ul>
<hr>
<h2 id="CLMSM-A-Multi-Task-Learning-Framework-for-Pre-training-on-Procedural-Text"><a href="#CLMSM-A-Multi-Task-Learning-Framework-for-Pre-training-on-Procedural-Text" class="headerlink" title="CLMSM: A Multi-Task Learning Framework for Pre-training on Procedural Text"></a>CLMSM: A Multi-Task Learning Framework for Pre-training on Procedural Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14326">http://arxiv.org/abs/2310.14326</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhilash Nandy, Manav Nitin Kapadnis, Pawan Goyal, Niloy Ganguly</li>
<li>For:	+ The paper is written for proposing a domain-specific, continual pre-training framework for procedural NLP tasks.* Methods:	+ The framework uses a Multi-Task Learning Framework to optimize two objectives: contrastive learning using hard triplets and a novel mask-step modeling objective.* Results:	+ The proposed framework (CLMSM) outperforms baselines on recipes (in-domain) and is able to generalize to open-domain procedural NLP tasks.<details>
<summary>Abstract</summary>
In this paper, we propose CLMSM, a domain-specific, continual pre-training framework, that learns from a large set of procedural recipes. CLMSM uses a Multi-Task Learning Framework to optimize two objectives - a) Contrastive Learning using hard triplets to learn fine-grained differences across entities in the procedures, and b) a novel Mask-Step Modelling objective to learn step-wise context of a procedure. We test the performance of CLMSM on the downstream tasks of tracking entities and aligning actions between two procedures on three datasets, one of which is an open-domain dataset not conforming with the pre-training dataset. We show that CLMSM not only outperforms baselines on recipes (in-domain) but is also able to generalize to open-domain procedural NLP tasks.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了CLMSM，一种领域专门的大规模练习框架，利用大量的过程recipes进行学习。CLMSM使用多任务学习框架来优化两个目标：一是对照 triplets进行强制学习细腻差异 между实体，二是一种新的面积步骤模型目标来学习过程步骤的上下文。我们在三个数据集上测试CLMSM的性能，其中一个是一个公开的数据集，不符合预训练数据集。我们发现CLMSM不仅在recipes（预训练）上超越了基eline，还能够通过预训练数据集来泛化到开放领域的过程语言任务。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Semantic-Processing-Techniques"><a href="#A-Survey-on-Semantic-Processing-Techniques" class="headerlink" title="A Survey on Semantic Processing Techniques"></a>A Survey on Semantic Processing Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18345">http://arxiv.org/abs/2310.18345</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Mao, Kai He, Xulang Zhang, Guanyi Chen, Jinjie Ni, Zonglin Yang, Erik Cambria</li>
<li>for: 本研究旨在探讨计算语言学中的 semantics 领域的最新进展，以及这一领域在不同应用领域的拓展和integration。</li>
<li>methods: 本文分析了五种semantic processing task，包括word sense disambiguation、anaphora resolution、named entity recognition、concept extraction和subjectivity detection。并评估了这些任务的相关理论研究、高级方法和下游应用。</li>
<li>results: 本文对semantic processing tasks的研究进行了概括和比较，探讨了不同技术和应用趋势，并提出了未来发展的方向和 possiblities。<details>
<summary>Abstract</summary>
Semantic processing is a fundamental research domain in computational linguistics. In the era of powerful pre-trained language models and large language models, the advancement of research in this domain appears to be decelerating. However, the study of semantics is multi-dimensional in linguistics. The research depth and breadth of computational semantic processing can be largely improved with new technologies. In this survey, we analyzed five semantic processing tasks, e.g., word sense disambiguation, anaphora resolution, named entity recognition, concept extraction, and subjectivity detection. We study relevant theoretical research in these fields, advanced methods, and downstream applications. We connect the surveyed tasks with downstream applications because this may inspire future scholars to fuse these low-level semantic processing tasks with high-level natural language processing tasks. The review of theoretical research may also inspire new tasks and technologies in the semantic processing domain. Finally, we compare the different semantic processing techniques and summarize their technical trends, application trends, and future directions.
</details>
<details>
<summary>摘要</summary>
《semantic processing》是计算语言学的基础研究领域之一。在大型预训练语言模型和大语言模型的时代，这一领域的研究进步似乎减速。然而，语义研究是语言学的多维ensional问题，computational semantic processing的研究深度和广度可以通过新技术得到大幅提升。在这份调研中，我们分析了五种semantic processing任务，例如word sense disambiguation、anaphora resolution、named entity recognition、concept extraction和主观性检测。我们研究相关的理论研究、先进方法和下游应用。我们将调研任务与下游应用连接起来，因为这可能会鼓励未来的学者将低级语义处理任务与高级自然语言处理任务融合起来。审视理论研究也可能 inspirer新的任务和技术在语义处理领域。最后，我们比较不同的语义处理技术，总结其技术趋势、应用趋势和未来方向。
</details></li>
</ul>
<hr>
<h2 id="Chainpoll-A-high-efficacy-method-for-LLM-hallucination-detection"><a href="#Chainpoll-A-high-efficacy-method-for-LLM-hallucination-detection" class="headerlink" title="Chainpoll: A high efficacy method for LLM hallucination detection"></a>Chainpoll: A high efficacy method for LLM hallucination detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18344">http://arxiv.org/abs/2310.18344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robert Friel, Atindriyo Sanyal</li>
<li>for: 本研究旨在提出一种新的幻觉检测方法ChainPoll，并提供一个改进的benchmark datasets集RealHall，以评估现有 studies中的幻觉检测metric。</li>
<li>methods: 本研究使用了一种新的幻觉检测方法ChainPoll，以及一个改进的benchmark datasets集RealHall，以评估现有 studies中的幻觉检测metric。</li>
<li>results: 对RealHall benchmark datasets进行了全面的比较，发现ChainPoll在所有RealHall benchmark上表现出色，AUROC为0.781，比最佳理论方法提高11%，超过了行业标准23%。此外，ChainPoll还具有cost-effective和高度可见的优点。此外，本研究还引入了两种新的幻觉度量：Adherence和Correctness。<details>
<summary>Abstract</summary>
Large language models (LLMs) have experienced notable advancements in generating coherent and contextually relevant responses. However, hallucinations - incorrect or unfounded claims - are still prevalent, prompting the creation of automated metrics to detect these in LLM outputs. Our contributions include: introducing ChainPoll, an innovative hallucination detection method that excels compared to its counterparts, and unveiling RealHall, a refined collection of benchmark datasets to assess hallucination detection metrics from recent studies. While creating RealHall, we assessed tasks and datasets from previous hallucination detection studies and observed that many are not suitable for the potent LLMs currently in use. Overcoming this, we opted for four datasets challenging for modern LLMs and pertinent to real-world scenarios. Using RealHall, we conducted a comprehensive comparison of ChainPoll with numerous hallucination metrics from recent studies. Our findings indicate that ChainPoll outperforms in all RealHall benchmarks, achieving an overall AUROC of 0.781. This surpasses the next best theoretical method by 11% and exceeds industry standards by over 23%. Additionally, ChainPoll is cost-effective and offers greater transparency than other metrics. We introduce two novel metrics to assess LLM hallucinations: Adherence and Correctness. Adherence is relevant to Retrieval Augmented Generation workflows, evaluating an LLM's analytical capabilities within given documents and contexts. In contrast, Correctness identifies logical and reasoning errors.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在生成准确和Contextually relevanteresponses方面已经做出了显著的进步。然而，幻觉 -  incorrect或不科学的声明 - 仍然是LLM输出中的普遍问题，因此创造了自动化的metricsto检测这些幻觉在LLM输出中。我们的贡献包括：引入ChainPoll，一种创新的幻觉检测方法，与其他counterparts相比，具有显著的优势；以及披露RealHall，一个改进的benchmark datasets，用于评估最近的学术研究中的幻觉检测 metric。在创建RealHall时，我们评估了过去的幻觉检测任务和dataset，发现许多不适合当今的强大LLM使用。为了超越这些 limitation，我们选择了四个dataset，挑战当今的LLM，并与实际场景相关。使用RealHall，我们对ChainPoll与最近学术研究中的幻觉metric进行了全面的比较。我们的发现表明，ChainPoll在RealHall benchmark中表现出色，AUROC为0.781，比最佳理论方法提高11%，超过了industry标准 by over 23%。此外，ChainPoll可以efficiently cost-effective，并且具有更高的透明度。我们还引入了两个新的metric来评估LLM幻觉： Adherence和Correctness。 Adherence relevante到 Retrieval Augmented Generation workflows，评估LLM在给定文档和context中的分析能力。与此相反，Correctness检测LLM的逻辑和推理错误。
</details></li>
</ul>
<hr>
<h2 id="NERetrieve-Dataset-for-Next-Generation-Named-Entity-Recognition-and-Retrieval"><a href="#NERetrieve-Dataset-for-Next-Generation-Named-Entity-Recognition-and-Retrieval" class="headerlink" title="NERetrieve: Dataset for Next Generation Named Entity Recognition and Retrieval"></a>NERetrieve: Dataset for Next Generation Named Entity Recognition and Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14282">http://arxiv.org/abs/2310.14282</a></li>
<li>repo_url: None</li>
<li>paper_authors: Uri Katz, Matan Vetzler, Amir DN Cohen, Yoav Goldberg</li>
<li>for: 这些研究旨在推动Named Entity Recognition（NER）任务的进一步发展，包括提高NER模型的精度和可扩展性。</li>
<li>methods: 这些研究使用大语言模型（LLM），包括zero-shotRecognition和自动标注等技术，以提高NER模型的性能。</li>
<li>results: 研究发现， presenteNER任务仍然存在许多挑战，包括更细化的实体类型、零shot认知和检索、以及减少预处理时间等。<details>
<summary>Abstract</summary>
Recognizing entities in texts is a central need in many information-seeking scenarios, and indeed, Named Entity Recognition (NER) is arguably one of the most successful examples of a widely adopted NLP task and corresponding NLP technology. Recent advances in large language models (LLMs) appear to provide effective solutions (also) for NER tasks that were traditionally handled with dedicated models, often matching or surpassing the abilities of the dedicated models. Should NER be considered a solved problem? We argue to the contrary: the capabilities provided by LLMs are not the end of NER research, but rather an exciting beginning. They allow taking NER to the next level, tackling increasingly more useful, and increasingly more challenging, variants. We present three variants of the NER task, together with a dataset to support them. The first is a move towards more fine-grained -- and intersectional -- entity types. The second is a move towards zero-shot recognition and extraction of these fine-grained types based on entity-type labels. The third, and most challenging, is the move from the recognition setup to a novel retrieval setup, where the query is a zero-shot entity type, and the expected result is all the sentences from a large, pre-indexed corpus that contain entities of these types, and their corresponding spans. We show that all of these are far from being solved. We provide a large, silver-annotated corpus of 4 million paragraphs covering 500 entity types, to facilitate research towards all of these three goals.
</details>
<details>
<summary>摘要</summary>
Recognizing entities in texts is a central need in many information-seeking scenarios, and indeed, Named Entity Recognition (NER) is arguably one of the most successful examples of a widely adopted NLP task and corresponding NLP technology. Recent advances in large language models (LLMs) appear to provide effective solutions (also) for NER tasks that were traditionally handled with dedicated models, often matching or surpassing the abilities of the dedicated models. However, we argue that NER should not be considered a solved problem: the capabilities provided by LLMs are not the end of NER research, but rather an exciting beginning. They allow taking NER to the next level, tackling increasingly more useful, and increasingly more challenging, variants. We present three variants of the NER task, together with a dataset to support them. The first is a move towards more fine-grained -- and intersectional -- entity types. The second is a move towards zero-shot recognition and extraction of these fine-grained types based on entity-type labels. The third, and most challenging, is the move from the recognition setup to a novel retrieval setup, where the query is a zero-shot entity type, and the expected result is all the sentences from a large, pre-indexed corpus that contain entities of these types, and their corresponding spans. We show that all of these are far from being solved. We provide a large, silver-annotated corpus of 4 million paragraphs covering 500 entity types, to facilitate research towards all of these three goals.
</details></li>
</ul>
<hr>
<h2 id="RSM-NLP-at-BLP-2023-Task-2-Bangla-Sentiment-Analysis-using-Weighted-and-Majority-Voted-Fine-Tuned-Transformers"><a href="#RSM-NLP-at-BLP-2023-Task-2-Bangla-Sentiment-Analysis-using-Weighted-and-Majority-Voted-Fine-Tuned-Transformers" class="headerlink" title="RSM-NLP at BLP-2023 Task 2: Bangla Sentiment Analysis using Weighted and Majority Voted Fine-Tuned Transformers"></a>RSM-NLP at BLP-2023 Task 2: Bangla Sentiment Analysis using Weighted and Majority Voted Fine-Tuned Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14261">http://arxiv.org/abs/2310.14261</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ptnv-s/rsm-nlp-blp-task2">https://github.com/ptnv-s/rsm-nlp-blp-task2</a></li>
<li>paper_authors: Pratinav Seth, Rashi Goel, Komal Mathur, Swetha Vemulapalli</li>
<li>for: 本研究旨在提高孟加拉语社交媒体内容的自动情感分析能力。</li>
<li>methods: 本研究使用多种多语言BERT模型进行实验和调整，并使用多样本投票和质量权重加权模型，以提高情感分析的准确率。</li>
<li>results: 本研究在多类分类任务中取得0.711的分数，并在共同任务中名列十名。Here’s the translation in English for reference:</li>
<li>for: The aim of this research is to improve the ability of automatic sentiment analysis for Bangla social media content.</li>
<li>methods: The research uses various multilingual and pre-trained BERT models for experimentation and fine-tuning, and employs a majority voting and weighted ensemble model to enhance the accuracy of sentiment analysis.</li>
<li>results: The research achieved a score of 0.711 on the multiclass classification task and ranked 10th among participants on the leaderboard for the shared task.<details>
<summary>Abstract</summary>
This paper describes our approach to submissions made at Shared Task 2 at BLP Workshop - Sentiment Analysis of Bangla Social Media Posts. Sentiment Analysis is an action research area in the digital age. With the rapid and constant growth of online social media sites and services and the increasing amount of textual data, the application of automatic Sentiment Analysis is on the rise. However, most of the research in this domain is based on the English language. Despite being the world's sixth most widely spoken language, little work has been done in Bangla. This task aims to promote work on Bangla Sentiment Analysis while identifying the polarity of social media content by determining whether the sentiment expressed in the text is Positive, Negative, or Neutral. Our approach consists of experimenting and finetuning various multilingual and pre-trained BERT-based models on our downstream tasks and using a Majority Voting and Weighted ensemble model that outperforms individual baseline model scores. Our system scored 0.711 for the multiclass classification task and scored 10th place among the participants on the leaderboard for the shared task. Our code is available at https://github.com/ptnv-s/RSM-NLP-BLP-Task2 .
</details>
<details>
<summary>摘要</summary>
translate into Simplified Chinese:这篇论文描述了我们在BLP工作坊的共享任务2中的提交方法 - sentiment analysis of Bangla social media posts。sentiment analysis是数字时代的一个研究领域，随着在线社交媒体网站和服务的快速增长和文本数据的增加，自动化sentiment analysis的应用也在增加。然而，大多数研究在这个领域是基于英语。不过，巴anga是全球第六 Most widely spoken语言，但是对于这种语言的研究却很少。这个任务的目标是促进巴anga Sentiment Analysis的研究，并在文本中确定 sentiment的 polarity是Positive、Negative或Neutral。我们的方法包括对多种多语言和预训练BERT模型进行实验和精度调整，并使用多数投票和Weighted ensemble模型，这个模型超过了个别基eline模型分数。我们的系统在多类分类任务上得分0.711，并在共享任务的 лидер板上排名第10名。我们的代码可以在https://github.com/ptnv-s/RSM-NLP-BLP-Task2 上获取。
</details></li>
</ul>
<hr>
<h2 id="High-Quality-3D-Face-Reconstruction-with-Affine-Convolutional-Networks"><a href="#High-Quality-3D-Face-Reconstruction-with-Affine-Convolutional-Networks" class="headerlink" title="High-Quality 3D Face Reconstruction with Affine Convolutional Networks"></a>High-Quality 3D Face Reconstruction with Affine Convolutional Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14237">http://arxiv.org/abs/2310.14237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiqian Lin, Jiangke Lin, Lincheng Li, Yi Yuan, Zhengxia Zou</li>
<li>for: This paper aims to tackle the challenges of canonical view reconstruction from a single input image, specifically addressing the problem of spatial misalignment between the input and output images.</li>
<li>methods: The proposed method uses an Affine Convolution Network (ACN) architecture to handle spatially non-corresponding input and output images, and represents 3D human heads in UV space with multiple components, including diffuse maps, position maps, and light maps.</li>
<li>results: The method is able to generate high-quality UV maps at a resolution of 512 x 512 pixels, while previous approaches typically generate maps at a lower resolution of 256 x 256 pixels or smaller.Here’s the simplified Chinese text for the three key points:</li>
<li>for: 这篇论文目标是解决单个输入图像中的 canonical view reconstruction 问题，具体来说是处理输入和输出图像之间的空间不对称问题。</li>
<li>methods: 该方法使用 Affine Convolution Network (ACN) 架构来处理空间不对称的输入和输出图像，并使用多个组件来表示 3D 人脸的 UV 空间，包括Diffuse 地图、Position 地图和 Light 地图。</li>
<li>results: 方法可以生成高质量的 UV 地图，分辨率为 512 x 512 像素，而前一种方法通常生成的地图分辨率为 256 x 256 像素或小于。<details>
<summary>Abstract</summary>
Recent works based on convolutional encoder-decoder architecture and 3DMM parameterization have shown great potential for canonical view reconstruction from a single input image. Conventional CNN architectures benefit from exploiting the spatial correspondence between the input and output pixels. However, in 3D face reconstruction, the spatial misalignment between the input image (e.g. face) and the canonical/UV output makes the feature encoding-decoding process quite challenging. In this paper, to tackle this problem, we propose a new network architecture, namely the Affine Convolution Networks, which enables CNN based approaches to handle spatially non-corresponding input and output images and maintain high-fidelity quality output at the same time. In our method, an affine transformation matrix is learned from the affine convolution layer for each spatial location of the feature maps. In addition, we represent 3D human heads in UV space with multiple components, including diffuse maps for texture representation, position maps for geometry representation, and light maps for recovering more complex lighting conditions in the real world. All the components can be trained without any manual annotations. Our method is parametric-free and can generate high-quality UV maps at resolution of 512 x 512 pixels, while previous approaches normally generate 256 x 256 pixels or smaller. Our code will be released once the paper got accepted.
</details>
<details>
<summary>摘要</summary>
最近的研究基于卷积编码器-解码器架构和3DMM参数化已经显示了从单个输入图像重建 canonical 视图的潜在性。传统的 CNN 架构可以利用输入和输出像素之间的空间相对性，从而提高特征编码-解码过程的性能。但在3D面重建中，输入图像（例如脸）和 canonical/UV 输出之间的空间误差使得特征编码-解码过程变得非常困难。在这篇论文中，我们提出了一种新的网络架构，即 Affine Convolution Networks，以便 CNN 基本上可以处理空间不匹配的输入和输出图像，同时保持高质量输出。在我们的方法中，每个空间位置的特征图中学习了一个 Affine 变换矩阵。此外，我们使用多组分表示3D 人头的 UV 空间，包括diffuse 图像表示纹理，position 图像表示几何表示，以及light 图像为在真实世界中更复杂的光照条件的恢复。所有组分都可以通过无需手动标注来学习。我们的方法是 Parametric-free 的，可以生成高分辨率（512 x 512 像素）的高质量 UV 地图，而之前的方法通常生成的是 256 x 256 像素或更小的地图。我们的代码将在论文被接受后发布。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Meta-Neural-Heuristic-for-Multi-Objective-Combinatorial-Optimization"><a href="#Efficient-Meta-Neural-Heuristic-for-Multi-Objective-Combinatorial-Optimization" class="headerlink" title="Efficient Meta Neural Heuristic for Multi-Objective Combinatorial Optimization"></a>Efficient Meta Neural Heuristic for Multi-Objective Combinatorial Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15196">http://arxiv.org/abs/2310.15196</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bill-cjb/emnh">https://github.com/bill-cjb/emnh</a></li>
<li>paper_authors: Jinbiao Chen, Jiahai Wang, Zizhen Zhang, Zhiguang Cao, Te Ye, Siyuan Chen</li>
<li>for: 解决多目标 combinatorial 优化问题 (MOCOP)</li>
<li>methods: 使用深度强化学习 neural heuristics，并提出了一种高效的元 нейро逻辑 (EMNH) 方法，通过快速训练和精细调整来解决问题</li>
<li>results: EMNH 方法可以在 solution quality 和学习效率两个方面比 estado-of-the-art  нейро逻辑方法出色，并且可以与传统的优化策略相比，提供竞争力强的解决方案，而且消耗的时间非常短。<details>
<summary>Abstract</summary>
Recently, neural heuristics based on deep reinforcement learning have exhibited promise in solving multi-objective combinatorial optimization problems (MOCOPs). However, they are still struggling to achieve high learning efficiency and solution quality. To tackle this issue, we propose an efficient meta neural heuristic (EMNH), in which a meta-model is first trained and then fine-tuned with a few steps to solve corresponding single-objective subproblems. Specifically, for the training process, a (partial) architecture-shared multi-task model is leveraged to achieve parallel learning for the meta-model, so as to speed up the training; meanwhile, a scaled symmetric sampling method with respect to the weight vectors is designed to stabilize the training. For the fine-tuning process, an efficient hierarchical method is proposed to systematically tackle all the subproblems. Experimental results on the multi-objective traveling salesman problem (MOTSP), multi-objective capacitated vehicle routing problem (MOCVRP), and multi-objective knapsack problem (MOKP) show that, EMNH is able to outperform the state-of-the-art neural heuristics in terms of solution quality and learning efficiency, and yield competitive solutions to the strong traditional heuristics while consuming much shorter time.
</details>
<details>
<summary>摘要</summary>
近期，基于深度再征学习的神经归纳算法已经在解决多目标 combinatorial 优化问题（MOCOP）中显示了承诺。然而，它们仍然很难达到高效学习和解决方案质量的目标。为了解决这个问题，我们提出了一种高效的元神经归纳算法（EMNH），其中首先训练一个元模型，然后使用一些步骤进行细化以解决相应的单目标优化问题。在训练过程中，我们利用了一个（部分）建筑物共享多任务模型，以实现并行学习，以加速训练过程。同时，我们设计了一种尺度相对的对 вектор的抖擞方法，以稳定训练过程。在细化过程中，我们提出了一种高效的层次方法，以系统地解决所有的优化问题。实验结果显示，EMNH可以在多目标旅行商问题（MOTSP）、多目标资源限制车辆路径问题（MOCVRP）和多目标零部件问题（MOKP）中，超越现有的神经归纳算法，以至于解决方案质量和学习效率。同时，它可以在短时间内提供竞争力强的传统归纳算法解决方案。
</details></li>
</ul>
<hr>
<h2 id="Neural-Multi-Objective-Combinatorial-Optimization-with-Diversity-Enhancement"><a href="#Neural-Multi-Objective-Combinatorial-Optimization-with-Diversity-Enhancement" class="headerlink" title="Neural Multi-Objective Combinatorial Optimization with Diversity Enhancement"></a>Neural Multi-Objective Combinatorial Optimization with Diversity Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15195">http://arxiv.org/abs/2310.15195</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bill-cjb/nhde">https://github.com/bill-cjb/nhde</a></li>
<li>paper_authors: Jinbiao Chen, Zizhen Zhang, Zhiguang Cao, Yaoxin Wu, Yining Ma, Te Ye, Jiahai Wang</li>
<li>for:  solves multi-objective combinatorial optimization (MOCO) problems with a novel neural heuristic that enhances diversity.</li>
<li>methods:  uses an indicator-enhanced deep reinforcement learning method and a heterogeneous graph attention mechanism to capture the relations between the instance graph and the Pareto front graph, as well as a multiple Pareto optima strategy to sample and preserve desirable solutions.</li>
<li>results:  generates a Pareto front with higher diversity, achieving superior overall performance on classic MOCO problems, and is generic and can be applied to different neural methods for MOCO.Here’s the full Chinese text:</li>
<li>for:  solves 多bjective combinatorial optimization (MOCO) 问题，使用一种新的神经拟合算法，以提高多元性。</li>
<li>methods: 使用指标增强的深度学习方法和异类图注意力机制，捕捉实例图和Pareto前图之间的关系，同时采用多个Pareto优点策略，抽样和保留有价值的解决方案。</li>
<li>results: 在经典MOCO问题上实现了更高的多元性Pareto前，实现了更好的总性能，并且可以应用于不同的神经方法 дляMOCO。<details>
<summary>Abstract</summary>
Most of existing neural methods for multi-objective combinatorial optimization (MOCO) problems solely rely on decomposition, which often leads to repetitive solutions for the respective subproblems, thus a limited Pareto set. Beyond decomposition, we propose a novel neural heuristic with diversity enhancement (NHDE) to produce more Pareto solutions from two perspectives. On the one hand, to hinder duplicated solutions for different subproblems, we propose an indicator-enhanced deep reinforcement learning method to guide the model, and design a heterogeneous graph attention mechanism to capture the relations between the instance graph and the Pareto front graph. On the other hand, to excavate more solutions in the neighborhood of each subproblem, we present a multiple Pareto optima strategy to sample and preserve desirable solutions. Experimental results on classic MOCO problems show that our NHDE is able to generate a Pareto front with higher diversity, thereby achieving superior overall performance. Moreover, our NHDE is generic and can be applied to different neural methods for MOCO.
</details>
<details>
<summary>摘要</summary>
大多数现有的神经方法 для多目标组合优化（MOCO）问题都仅仅采用分解，这frequently leads to repetitive solutions for the respective subproblems, resulting in a limited Pareto set. 在这种情况下，我们提出了一种新的神经规范with diversity enhancement（NHDE），以生成更多的Pareto解决方案。从一个角度来看，我们提出了一种指标增强的深度学习方法，以避免不同的子问题之间的重复解决方案。同时，我们设计了一种异质图注意机制，以捕捉实例图和Pareto前图之间的关系。从另一个角度来看，我们采用多个Pareto优点策略，以采样和保留愿意的解决方案。实验结果表明，我们的NHDE能够生成一个更高度的多样性Pareto前，从而实现更好的总性能。此外，我们的NHDE可以应用于不同的神经方法for MOCO。
</details></li>
</ul>
<hr>
<h2 id="MIRACLE-Towards-Personalized-Dialogue-Generation-with-Latent-Space-Multiple-Personal-Attribute-Control"><a href="#MIRACLE-Towards-Personalized-Dialogue-Generation-with-Latent-Space-Multiple-Personal-Attribute-Control" class="headerlink" title="MIRACLE: Towards Personalized Dialogue Generation with Latent-Space Multiple Personal Attribute Control"></a>MIRACLE: Towards Personalized Dialogue Generation with Latent-Space Multiple Personal Attribute Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18342">http://arxiv.org/abs/2310.18342</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lzy-the-boys/miracle">https://github.com/lzy-the-boys/miracle</a></li>
<li>paper_authors: Zhenyi Lu, Wei Wei, Xiaoye Qu, XianLing Mao, Dangyang Chen, Jixiong Chen</li>
<li>for: 这种研究旨在提高人工智能对话系统的人性化特征，以便实现更加自然的人类对话。</li>
<li>methods: 该研究提出了一种新的个性化对话生成方法，基于多个人性特征的控制，包括语言风格、内心人物特征等。</li>
<li>results: 实验表明，该方法可以提供更高的人性化控制和对话质量，并且可以在多个对话场景中实现灵活的人性特征组合。<details>
<summary>Abstract</summary>
Personalized dialogue systems aim to endow the chatbot agent with more anthropomorphic traits for human-like interactions. Previous approaches have explored explicitly user profile modeling using text descriptions, implicit derivation of user embeddings, or utilizing handicraft prompts for ChatGPT-like models. However, textual personas are limited in describing multi-faceted attributes (\emph{e.g.}, \emph{language style, inner character nuances}), implicit embedding suffers from personality sparsity, and handicraft prompts lack fine-grained and stable controllability. Hence, these approaches may struggle with complex personalized dialogue generation tasks that require generating controllable responses with multiple personal attributes. To this end, we propose \textbf{\textsc{Miracle}, a novel personalized dialogue generation method through \textbf{M}ult\textbf{I}ple Pe\textbf{R}sonal \textbf{A}ttributes \textbf{C}ontrol within \textbf{L}atent-Space \textbf{E}nergy-based Models. ttributes \textbf{C}ontrol within \textbf{L}atent-Space \textbf{E}nergy-based Models. Specifically, our approach first disentangles complex personality into multi-faceted attributes. Subsequently, we employ a conditional variational auto-encoder to align with the dense personalized responses within a latent joint attribute space. We have also tailored a dedicated energy function and customized the ordinary differential equations sampling method to offer flexible attribute composition and precise attribute control. Extensive experiments demonstrate that \textsc{Miracle} outperforms several strong baselines in terms of personality controllability and response generation quality. Our dataset and code are available at \url{https://github.com/LZY-the-boys/MIRACLE}
</details>
<details>
<summary>摘要</summary>
人工智能对话系统的目标是赋予对话机器人更多人类特征，以便更自然的人类交互。先前的方法包括文本描述来明确用户模型，从文本中推导用户嵌入，或者使用手工提示来驱动ChatGPT样式的模型。然而，文本个性只能描述用户的一些多方面特征（例如，语言风格、内心特点），嵌入难以表达用户的人格特点，而手工提示缺乏精细控制。因此，这些方法可能会在复杂的个性化对话生成任务中遇到困难，特别是需要生成多个人性特征的响应。为此，我们提出了\textbf{\textsc{Miracle}，一种新的个性化对话生成方法，通过\textbf{M}ult\textbf{I}ple \textbf{P}ersonal \textbf{A}ttributes \textbf{C}ontrol within \textbf{L}atent-Space \textbf{E}nergy-based Models。具体来说，我们的方法首先分解复杂的人性特征，然后employs a conditional variational autoencoder来对具有密集个性响应的积极特征空间进行对应。我们还特制了一个专门的能量函数和自适应的差分方程，以便可以自由地组合特征和精确地控制特征。我们的实验证明，\textsc{Miracle}在人性可控和响应质量两个方面都高于多个强基eline。我们的数据集和代码可以在\url{https://github.com/LZY-the-boys/MIRACLE}上获取。
</details></li>
</ul>
<hr>
<h2 id="UniMAP-Universal-SMILES-Graph-Representation-Learning"><a href="#UniMAP-Universal-SMILES-Graph-Representation-Learning" class="headerlink" title="UniMAP: Universal SMILES-Graph Representation Learning"></a>UniMAP: Universal SMILES-Graph Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14216">http://arxiv.org/abs/2310.14216</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fengshikun/unimap">https://github.com/fengshikun/unimap</a></li>
<li>paper_authors: Shikun Feng, Lixin Yang, Weiying Ma, Yanyan Lan</li>
<li>for: This paper aims to propose a universal molecular representation learning model that can effectively leverage both SMILES and graph representations for drug-related applications.</li>
<li>methods: The proposed model, UniMAP, uses an embedding layer to obtain token and node&#x2F;edge representations in SMILES and graph, respectively, followed by a multi-layer Transformer to conduct deep cross-modality fusion. The model is pre-trained on four tasks: Multi-Level Cross-Modality Masking, SMILES-Graph Matching, Fragment-Level Alignment, and Domain Knowledge Learning.</li>
<li>results: UniMAP outperforms current state-of-the-art pre-training methods on various downstream tasks, including molecular property prediction, drug-target affinity prediction, and drug-drug interaction. The learned representations are also visualized to demonstrate the effect of multi-modality integration.<details>
<summary>Abstract</summary>
Molecular representation learning is fundamental for many drug related applications. Most existing molecular pre-training models are limited in using single molecular modality, either SMILES or graph representation. To effectively leverage both modalities, we argue that it is critical to capture the fine-grained 'semantics' between SMILES and graph, because subtle sequence/graph differences may lead to contrary molecular properties. In this paper, we propose a universal SMILE-graph representation learning model, namely UniMAP. Firstly, an embedding layer is employed to obtain the token and node/edge representation in SMILES and graph, respectively. A multi-layer Transformer is then utilized to conduct deep cross-modality fusion. Specially, four kinds of pre-training tasks are designed for UniMAP, including Multi-Level Cross-Modality Masking (CMM), SMILES-Graph Matching (SGM), Fragment-Level Alignment (FLA), and Domain Knowledge Learning (DKL). In this way, both global (i.e. SGM and DKL) and local (i.e. CMM and FLA) alignments are integrated to achieve comprehensive cross-modality fusion. We evaluate UniMAP on various downstream tasks, i.e. molecular property prediction, drug-target affinity prediction and drug-drug interaction. Experimental results show that UniMAP outperforms current state-of-the-art pre-training methods.We also visualize the learned representations to demonstrate the effect of multi-modality integration.
</details>
<details>
<summary>摘要</summary>
молекулярное представление научиться是许多药物相关应用的基础。大多数现有的药物预训模型都是使用单一的分子模式，可以是SMILES或图表示。为了有效地利用这两种模式，我们认为是关键 capture fine-grained 'semantics' между SMILES和图，因为微scopic序列/图像差异可能会导致不同的分子性质。在这篇论文中，我们提议一种通用的SMILES-图表示学习模型，称为UniMAP。首先，一个嵌入层被使用来获取SMILES和图的token和节点/边表示。然后，一个多层变换器被使用来进行深度的cross-modality融合。特别是，我们设计了四种预训任务 дляUniMAP，包括多级cross-modality遮盲(CMM), SMILES-图匹配(SGM),块级对alignment(FLA)和领域知识学习(DKL)。这样，global (即SGM和DKL)和local (即CMM和FLA)的对应都被集成，以实现全面的cross-modality融合。我们在多个下游任务上评估UniMAP，包括分子性质预测、药物-Target相互作用预测和药物-药物相互作用。实验结果表明，UniMAP在现有状态的预训方法中表现出色。我们还利用学习的表示图示出效果多模式融合。
</details></li>
</ul>
<hr>
<h2 id="Item-Graph2vec-a-Efficient-and-Effective-Approach-using-Item-Co-occurrence-Graph-Embedding-for-Collaborative-Filtering"><a href="#Item-Graph2vec-a-Efficient-and-Effective-Approach-using-Item-Co-occurrence-Graph-Embedding-for-Collaborative-Filtering" class="headerlink" title="Item-Graph2vec: a Efficient and Effective Approach using Item Co-occurrence Graph Embedding for Collaborative Filtering"></a>Item-Graph2vec: a Efficient and Effective Approach using Item Co-occurrence Graph Embedding for Collaborative Filtering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14215">http://arxiv.org/abs/2310.14215</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cpu135/item-graph2vec">https://github.com/cpu135/item-graph2vec</a></li>
<li>paper_authors: Ruilin Yuan, Leya Li, Yuanzhe Cai</li>
<li>for: 提高大规模项目推荐系统的效率</li>
<li>methods: 使用随机游走Item Graph embedding算法</li>
<li>results: 相比Item2vec，Item-Graph2vec在大规模数据集上具有稳定的运行时间和高效的性能，并且在实际数据集上实现了3倍的效率提升。<details>
<summary>Abstract</summary>
Current item-item collaborative filtering algorithms based on artificial neural network, such as Item2vec, have become ubiquitous and are widely applied in the modern recommender system. However, these approaches do not apply to the large-scale item-based recommendation system because of their extremely long training time. To overcome the shortcoming that current algorithms have high training time costs and poor stability when dealing with large-scale data sets, the item graph embedding algorithm Item-Graph2vec is described here. This algorithm transforms the users' shopping list into a item co-occurrence graph, obtains item sequences through randomly travelling on this co-occurrence graph and finally trains item vectors through sequence samples. We posit that because of the stable size of item, the size and density of the item co-occurrence graph change slightly with the increase in the training corpus. Therefore, Item-Graph2vec has a stable runtime on the large scale data set, and its performance advantage becomes more and more obvious with the growth of the training corpus. Extensive experiments conducted on real-world data sets demonstrate that Item-Graph2vec outperforms Item2vec by 3 times in terms of efficiency on douban data set, while the error generated by the random walk sampling is small.
</details>
<details>
<summary>摘要</summary>
当前的item-item共同推荐算法，如Item2vec，在现代推荐系统中广泛应用。然而，这些方法在大规模item-based推荐系统中不适用，因为它们的训练时间非常长。为了超越现有的缺点，Item Graph2vec算法是描述的。这个算法将用户购买记录转换成item共享图，从共享图中随机旅行获得item序列，并最后使用序列样本来训练item vectors。我们认为，由于item的稳定大小，item共享图的大小和密度随着训练 corpora 的增加而变化稍微。因此，Item Graph2vec在大规模数据集上具有稳定的运行时间，而且其性能优势随着训练 corpora 的增长而变得更加明显。在实际数据集上进行了广泛的实验，并证明了Item Graph2vec在对 douban 数据集进行训练时，与Item2vec相比，三倍的效率。同时，随机步骤采样生成的误差较小。
</details></li>
</ul>
<hr>
<h2 id="LUNA-A-Model-Based-Universal-Analysis-Framework-for-Large-Language-Models"><a href="#LUNA-A-Model-Based-Universal-Analysis-Framework-for-Large-Language-Models" class="headerlink" title="LUNA: A Model-Based Universal Analysis Framework for Large Language Models"></a>LUNA: A Model-Based Universal Analysis Framework for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14211">http://arxiv.org/abs/2310.14211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Da Song, Xuan Xie, Jiayang Song, Derui Zhu, Yuheng Huang, Felix Juefei-Xu, Lei Ma<br>for:This paper aims to provide a universal analysis framework for large language models (LLMs) to evaluate their trustworthiness from multiple perspectives.methods:The proposed framework, called LUNA, leverages various abstract model construction methods and defines evaluation metrics to assess the quality of the abstract model and the semantics of the LLM.results:The proposed framework enables versatile analysis of LLMs from multiple quality perspectives in a human-interpretable manner, and can be used to evaluate the trustworthiness of LLMs in various industrial domains.<details>
<summary>Abstract</summary>
Over the past decade, Artificial Intelligence (AI) has had great success recently and is being used in a wide range of academic and industrial fields. More recently, LLMs have made rapid advancements that have propelled AI to a new level, enabling even more diverse applications and industrial domains with intelligence, particularly in areas like software engineering and natural language processing. Nevertheless, a number of emerging trustworthiness concerns and issues exhibited in LLMs have already recently received much attention, without properly solving which the widespread adoption of LLMs could be greatly hindered in practice. The distinctive characteristics of LLMs, such as the self-attention mechanism, extremely large model scale, and autoregressive generation schema, differ from classic AI software based on CNNs and RNNs and present new challenges for quality analysis. Up to the present, it still lacks universal and systematic analysis techniques for LLMs despite the urgent industrial demand. Towards bridging this gap, we initiate an early exploratory study and propose a universal analysis framework for LLMs, LUNA, designed to be general and extensible, to enable versatile analysis of LLMs from multiple quality perspectives in a human-interpretable manner. In particular, we first leverage the data from desired trustworthiness perspectives to construct an abstract model as an auxiliary analysis asset, which is empowered by various abstract model construction methods. To assess the quality of the abstract model, we collect and define a number of evaluation metrics, aiming at both abstract model level and the semantics level. Then, the semantics, which is the degree of satisfaction of the LLM w.r.t. the trustworthiness perspective, is bound to and enriches the abstract model with semantics, which enables more detailed analysis applications for diverse purposes.
</details>
<details>
<summary>摘要</summary>
Specifically, we first leverage data from desired trustworthiness perspectives to construct an abstract model as an auxiliary analysis asset, which is empowered by various abstract model construction methods. We then collect and define a set of evaluation metrics to assess the quality of the abstract model, aiming at both the abstract model level and the semantics level. Finally, the semantics, which represents the degree of satisfaction of the LLM with respect to the trustworthiness perspective, is bound to and enriches the abstract model with semantics, enabling more detailed analysis applications for diverse purposes.
</details></li>
</ul>
<hr>
<h2 id="CXR-LLaVA-Multimodal-Large-Language-Model-for-Interpreting-Chest-X-ray-Images"><a href="#CXR-LLaVA-Multimodal-Large-Language-Model-for-Interpreting-Chest-X-ray-Images" class="headerlink" title="CXR-LLaVA: Multimodal Large Language Model for Interpreting Chest X-ray Images"></a>CXR-LLaVA: Multimodal Large Language Model for Interpreting Chest X-ray Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18341">http://arxiv.org/abs/2310.18341</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ecofri/cxr_llava">https://github.com/ecofri/cxr_llava</a></li>
<li>paper_authors: Seowoo Lee, M. D., Jiwon Youn, Mansu Kim Ph. D., Soon Ho Yoon, M. D. Ph. D</li>
<li>for: 这项研究旨在开发一个开源的多模态大语言模型，用于解读胸部X射线图像（CXR）。</li>
<li>methods: 该研究使用了659,287个公开available的胸部X射线图像进行训练，其中417,336个图像有特定的放射学畸形标注（dataset 1），241,951个图像提供了自由文本放射学报告（dataset 2）。在训练Resnet50作为图像Encoder后，使用语言-图像对应预训练来对CXR和其相应的放射学畸形进行对应。然后，使用dataset 2进行精度调整，并使用GPT-4进行生成多种问答enario。相关代码可以在<a target="_blank" rel="noopener" href="https://github.com/ECOFRI/CXR_LLaVA%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ECOFRI/CXR_LLaVA中找到。</a></li>
<li>results: 在测试集中，模型的性能因参数而异。在 average 情况下，它在五种疾病（气肿、心肿、填充、肿胀和液体积）上的 F1 分数为0.34，通过提问工程可以提高到0.46。在独立集中，模型的 average F1 分数为0.30。另外，对于未在训练中看到的儿童胸部X射线图像集，模型可以准确地分类不正常的胸部X射线图像，F1 分数在0.84-0.85之间。<details>
<summary>Abstract</summary>
Purpose: Recent advancements in large language models (LLMs) have expanded their capabilities in a multimodal fashion, potentially replicating the image interpretation of human radiologists. This study aimed to develop open-source multimodal large language model for interpreting chest X-ray images (CXR-LLaVA). We also examined the effect of prompt engineering and model parameters such as temperature and nucleus sampling.   Materials and Methods: For training, we collected 659,287 publicly available CXRs: 417,336 CXRs had labels for certain radiographic abnormalities (dataset 1); 241,951 CXRs provided free-text radiology reports (dataset 2). After pre-training the Resnet50 as an image encoder, the contrastive language-image pre-training was used to align CXRs and corresponding radiographic abnormalities. Then, the Large Language Model Meta AI-2 was fine-tuned using dataset 2, which were refined using GPT-4, with generating various question answering scenarios. The code can be found at https://github.com/ECOFRI/CXR_LLaVA.   Results: In the test set, we observed that the model's performance fluctuated based on its parameters. On average, it achieved F1 score of 0.34 for five pathologic findings (atelectasis, cardiomegaly, consolidation, edema, and pleural effusion), which was improved to 0.46 through prompt engineering. In the independent set, the model achieved an average F1 score of 0.30 for the same pathologic findings. Notably, for the pediatric chest radiograph dataset, which was unseen during training, the model differentiated abnormal radiographs with an F1 score ranging from 0.84 to 0.85.   Conclusion: CXR-LLaVA demonstrates promising potential in CXR interpretation. Both prompt engineering and model parameter adjustments can play pivotal roles in interpreting CXRs.
</details>
<details>
<summary>摘要</summary>
目的：近期大型自然语言模型（LLM）的进步已经扩展了其多Modal功能，可能地模拟人类胸部X射线专业人员的图像解释能力。这项研究旨在开发开源的多Modal大型自然语言模型，用于解释胸部X射线图像（CXR-LLaVA）。我们还研究了提示工程和模型参数的效果，如温度和核心采样。材料和方法：为训练，我们收集了659,287个公开可用的胸部X射线图像：417,336个图像有特定的放射学畸形标注（数据集1）；241,951个图像提供了自由文本医学报告（数据集2）。在预训练Resnet50作为图像Encoder后，我们使用语言-图像对对照预训练来与胸部X射线图像相对应。然后，我们使用数据集2进行微调，并使用GPT-4进行多种问题回答场景的细化。代码可以在https://github.com/ECOFRI/CXR_LLaVA找到。结果：在测试集中，我们发现模型的性能会随着参数的变化。在 average 的情况下，它达到了五种疾病发现（气肿、心肺肥大、填充物、肿胀和胸膜液泛）的 F1 分数为0.34，通过提示工程提高到0.46。在独立集中，模型在同样的五种疾病发现中的 average F1 分数为0.30。尤其是在没有在训练过程中看到的儿童胸部X射线数据集中，模型能够 diferenciate 畸形胸部X射线图像的 F1 分数在0.84-0.85之间。结论：CXR-LLaVA表现出了可能的胸部X射线解释潜力。提示工程和模型参数的调整都可以在解释胸部X射线图像中扮演着重要的角色。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Discern-Imitating-Heterogeneous-Human-Demonstrations-with-Preference-and-Representation-Learning"><a href="#Learning-to-Discern-Imitating-Heterogeneous-Human-Demonstrations-with-Preference-and-Representation-Learning" class="headerlink" title="Learning to Discern: Imitating Heterogeneous Human Demonstrations with Preference and Representation Learning"></a>Learning to Discern: Imitating Heterogeneous Human Demonstrations with Preference and Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14196">http://arxiv.org/abs/2310.14196</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sachit Kuhar, Shuo Cheng, Shivang Chopra, Matthew Bronars, Danfei Xu</li>
<li>for: 本研究旨在Addressing the challenges of maintaining the quality of collected data and addressing the suboptimal nature of some demonstrations in practical imitation learning (IL) systems.</li>
<li>methods: 本研究提出了一种名为Learning to Discern (L2D)的离线仿omorphism学习框架，通过在嵌入式的轨迹段中学习一个秘密表示，并使用喜好学习来评估和学习不同风格的示范者的示例。</li>
<li>results: 研究表明，L2D可以有效地评估和学习从不同质量和风格的示范者中的示例，从而提高了policy性能在多种任务中，包括在模拟和物理机器人上。<details>
<summary>Abstract</summary>
Practical Imitation Learning (IL) systems rely on large human demonstration datasets for successful policy learning. However, challenges lie in maintaining the quality of collected data and addressing the suboptimal nature of some demonstrations, which can compromise the overall dataset quality and hence the learning outcome. Furthermore, the intrinsic heterogeneity in human behavior can produce equally successful but disparate demonstrations, further exacerbating the challenge of discerning demonstration quality. To address these challenges, this paper introduces Learning to Discern (L2D), an offline imitation learning framework for learning from demonstrations with diverse quality and style. Given a small batch of demonstrations with sparse quality labels, we learn a latent representation for temporally embedded trajectory segments. Preference learning in this latent space trains a quality evaluator that generalizes to new demonstrators exhibiting different styles. Empirically, we show that L2D can effectively assess and learn from varying demonstrations, thereby leading to improved policy performance across a range of tasks in both simulations and on a physical robot.
</details>
<details>
<summary>摘要</summary>
实用的模仿学习（IL）系统依赖于大量人类示范数据实现成功的政策学习。然而，维护收集的数据质量和处理一些示范的不理想性可能会影响整体数据质量和学习结果。另外，人类行为的内在多样性可能会生成同样成功但具有不同风格的示范，进一步加剧了决定示范质量的挑战。为解决这些挑战，本文提出了学习把握（L2D），一种离线模仿学习框架，可以从多质量和风格的示范中学习。给定一小批示范，我们学习了一个抽象的表示，并在这个表示空间中进行偏好学习，以训练一个普适的质量评估器。我们的实验表明，L2D可以有效地评估和学习不同示范者的示范，从而提高政策性能在多种任务中，包括在模拟和物理机器人上。
</details></li>
</ul>
<hr>
<h2 id="PromptMix-A-Class-Boundary-Augmentation-Method-for-Large-Language-Model-Distillation"><a href="#PromptMix-A-Class-Boundary-Augmentation-Method-for-Large-Language-Model-Distillation" class="headerlink" title="PromptMix: A Class Boundary Augmentation Method for Large Language Model Distillation"></a>PromptMix: A Class Boundary Augmentation Method for Large Language Model Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14192">http://arxiv.org/abs/2310.14192</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/servicenow/promptmix-emnlp-2023">https://github.com/servicenow/promptmix-emnlp-2023</a></li>
<li>paper_authors: Gaurav Sahu, Olga Vechtomova, Dzmitry Bahdanau, Issam H. Laradji</li>
<li>for: 提高文本分类精度和效率，对具有有限训练数据的问题进行解决</li>
<li>methods: 利用大语言模型（LLM）如GPT3，生成新的示例，并使用LLM的指令和几拟分类能力来生成更有用的数据增强</li>
<li>results: 在4个文本分类 datasets（银行77、TREC6、主观性（SUBJ）和推特投诉）中，提出了一种生成和重新标签borderline示例的方法，以便将大型LLM如GPT3.5-turbo的知识传递到更小的和更便宜的分类器中，并在4个dataset上表现出比多个5拟数据增强方法更好的效果。<details>
<summary>Abstract</summary>
Data augmentation is a widely used technique to address the problem of text classification when there is a limited amount of training data. Recent work often tackles this problem using large language models (LLMs) like GPT3 that can generate new examples given already available ones. In this work, we propose a method to generate more helpful augmented data by utilizing the LLM's abilities to follow instructions and perform few-shot classifications. Our specific PromptMix method consists of two steps: 1) generate challenging text augmentations near class boundaries; however, generating borderline examples increases the risk of false positives in the dataset, so we 2) relabel the text augmentations using a prompting-based LLM classifier to enhance the correctness of labels in the generated data. We evaluate the proposed method in challenging 2-shot and zero-shot settings on four text classification datasets: Banking77, TREC6, Subjectivity (SUBJ), and Twitter Complaints. Our experiments show that generating and, crucially, relabeling borderline examples facilitates the transfer of knowledge of a massive LLM like GPT3.5-turbo into smaller and cheaper classifiers like DistilBERT$_{base}$ and BERT$_{base}$. Furthermore, 2-shot PromptMix outperforms multiple 5-shot data augmentation methods on the four datasets. Our code is available at https://github.com/ServiceNow/PromptMix-EMNLP-2023.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>文本扩充是一种广泛使用的技术来解决文本分类问题，当有限量的训练数据时。现今的研究通常使用大型自然语言模型（LLM），如GPT3，生成新的示例。在这种工作中，我们提出了一种方法，使用LLM的能力来跟进 instrucciones和几架分类来生成更有帮助的扩充数据。我们的特定PromptMix方法包括两步： 1）生成文本扩充在类别边界附近，但生成边缘示例会增加数据集中的假阳性风险，所以我们 2）使用提示基于的LLM分类器来修正生成的文本扩充标签，以提高生成数据中的正确性。我们在四个文本分类 dataset上进行了对4-shot和0-shot设定的实验：Banking77、TREC6、Subjectivity (SUBJ) 和 Twitter Complaints。我们的实验表明，生成并关键地修正边缘示例可以传递大型LLM如GPT3.5-turbo中的知识到更小和更便宜的分类器如DistilBERT$_{base}$和BERT$_{base}$。此外，2-shot PromptMix在四个数据集上超过多个5-shot数据扩充方法。我们的代码可以在https://github.com/ServiceNow/PromptMix-EMNLP-2023 上获取。
</details></li>
</ul>
<hr>
<h2 id="Randomized-Forward-Mode-of-Automatic-Differentiation-for-Optimization-Algorithms"><a href="#Randomized-Forward-Mode-of-Automatic-Differentiation-for-Optimization-Algorithms" class="headerlink" title="Randomized Forward Mode of Automatic Differentiation for Optimization Algorithms"></a>Randomized Forward Mode of Automatic Differentiation for Optimization Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14168">http://arxiv.org/abs/2310.14168</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khemraj Shukla, Yeonjong Shin</li>
<li>for: 这个论文主要是为了提出一种基于反Mode différentiation的隐藏层激活函数，以及一种基于这种激活函数的隐藏层神经网络，以提高神经网络的性能。</li>
<li>methods: 该论文使用了反Mode différentiation来计算损失函数的导数，并使用这些导数来更新神经网络的参数。具体来说，该论文提出了一种基于irectional derivatives的隐藏层神经网络更新方法，该方法使用了forward Mode AD或Jacobian vector Product来计算导数。</li>
<li>results: 该论文通过对多个概率分布中的Random directions进行测试，并通过对各种Physics-informed neural networks和Deep Operator Networks进行实验，显示了该方法的高效性和稳定性。同时，该论文还提供了一种理论分析，证明了该方法的速度收敛率。<details>
<summary>Abstract</summary>
Backpropagation within neural networks leverages a fundamental element of automatic differentiation, which is referred to as the reverse mode differentiation, or vector Jacobian Product (VJP) or, in the context of differential geometry, known as the pull-back process. The computation of gradient is important as update of neural network parameters is performed using gradient descent method. In this study, we present a genric randomized method, which updates the parameters of neural networks by using directional derivatives of loss functions computed efficiently by using forward mode AD or Jacobian vector Product (JVP). These JVP are computed along the random directions sampled from different probability distributions e.g., Bernoulli, Normal, Wigner, Laplace and Uniform distributions. The computation of gradient is performed during the forward pass of the neural network. We also present a rigorous analysis of the presented methods providing the rate of convergence along with the computational experiments deployed in scientific Machine learning in particular physics-informed neural networks and Deep Operator Networks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>使用神经网络中的归档传播方法可以利用自动导 diferencial的基本元素，即逆向导数 diferencial 或向量雅可比产品（VJP），或在斜块 геометрии中称为pull-back过程。计算梯度非常重要，因为使用梯度下降方法来更新神经网络参数。在这种研究中，我们提出了一种通用随机化方法，通过使用前向AD或雅可比产品（JVP）来计算梯度。这些JVP在不同的概率分布，例如 Bernoulli、Normal、Wigner、Laplace 和 Uniform 分布中随机 sampling 的方向上进行计算。计算梯度在神经网络的前向传播过程中进行。我们还提供了一种准确的分析方法，其中提供了涨落速率以及计算实验的结果，其中包括物理学 Informed Neural Networks 和 Deep Operator Networks。
</details></li>
</ul>
<hr>
<h2 id="Graph-Convolutional-Network-with-Connectivity-Uncertainty-for-EEG-based-Emotion-Recognition"><a href="#Graph-Convolutional-Network-with-Connectivity-Uncertainty-for-EEG-based-Emotion-Recognition" class="headerlink" title="Graph Convolutional Network with Connectivity Uncertainty for EEG-based Emotion Recognition"></a>Graph Convolutional Network with Connectivity Uncertainty for EEG-based Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14165">http://arxiv.org/abs/2310.14165</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongxiang Gao, Xiangyao Wang, Zhenghua Chen, Min Wu, Zhipeng Cai, Lulu Zhao, Jianqing Li, Chengyu Liu</li>
<li>for: 这个研究的目的是提高人机交互的自动情感识别能力，使用多条 Електроэнцефалограм (EEG) 信号。</li>
<li>methods: 这个研究使用的方法包括分布式不确定性方法、 граhp convolutional neural network (GCN) 架构、graph mixup 技术和深度GCN 重量。</li>
<li>results: 实验结果显示，这个方法比前一代方法有更好的性能，在两个常用的数据集上（SEED和SEEDIV）获得了正面和有意义的改善。<details>
<summary>Abstract</summary>
Automatic emotion recognition based on multichannel Electroencephalography (EEG) holds great potential in advancing human-computer interaction. However, several significant challenges persist in existing research on algorithmic emotion recognition. These challenges include the need for a robust model to effectively learn discriminative node attributes over long paths, the exploration of ambiguous topological information in EEG channels and effective frequency bands, and the mapping between intrinsic data qualities and provided labels. To address these challenges, this study introduces the distribution-based uncertainty method to represent spatial dependencies and temporal-spectral relativeness in EEG signals based on Graph Convolutional Network (GCN) architecture that adaptively assigns weights to functional aggregate node features, enabling effective long-path capturing while mitigating over-smoothing phenomena. Moreover, the graph mixup technique is employed to enhance latent connected edges and mitigate noisy label issues. Furthermore, we integrate the uncertainty learning method with deep GCN weights in a one-way learning fashion, termed Connectivity Uncertainty GCN (CU-GCN). We evaluate our approach on two widely used datasets, namely SEED and SEEDIV, for emotion recognition tasks. The experimental results demonstrate the superiority of our methodology over previous methods, yielding positive and significant improvements. Ablation studies confirm the substantial contributions of each component to the overall performance.
</details>
<details>
<summary>摘要</summary>
To address these challenges, this study introduces a distribution-based uncertainty method to represent spatial dependencies and temporal-spectral relativeness in EEG signals based on Graph Convolutional Network (GCN) architecture. The GCN architecture adaptively assigns weights to functional aggregate node features, enabling effective long-path capturing while mitigating over-smoothing phenomena. Moreover, the graph mixup technique is employed to enhance latent connected edges and mitigate noisy label issues.Furthermore, the uncertainty learning method is integrated with deep GCN weights in a one-way learning fashion, termed Connectivity Uncertainty GCN (CU-GCN). The CU-GCN approach is evaluated on two widely used datasets, namely SEED and SEEDIV, for emotion recognition tasks. The experimental results demonstrate the superiority of the CU-GCN methodology over previous methods, yielding positive and significant improvements. Ablation studies confirm the substantial contributions of each component to the overall performance.
</details></li>
</ul>
<hr>
<h2 id="Augmenting-End-to-End-Steering-Angle-Prediction-with-CAN-Bus-Data"><a href="#Augmenting-End-to-End-Steering-Angle-Prediction-with-CAN-Bus-Data" class="headerlink" title="Augmenting End-to-End Steering Angle Prediction with CAN Bus Data"></a>Augmenting End-to-End Steering Angle Prediction with CAN Bus Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14162">http://arxiv.org/abs/2310.14162</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rohan Gupta</li>
<li>for: 这paper的目的是提高自动驾驶车辆的终端推导预测精度，而不使用激光雷达和感知器。</li>
<li>methods: 这paper使用了计算机视觉模型，并通过感知器融合CAN总线数据来提高计算机视觉模型的准确性。</li>
<li>results: 结果显示，通过感知器融合CAN总线数据可以降低计算机视觉模型的预测错误率，其中一些模型的预测错误率可以降低80%。<details>
<summary>Abstract</summary>
In recent years, end to end steering prediction for autonomous vehicles has become a major area of research. The primary method for achieving end to end steering was to use computer vision models on a live feed of video data. However, to further increase accuracy, many companies have added data from light detection and ranging (LiDAR) and or radar sensors through sensor fusion. However, the addition of lasers and sensors comes at a high financial cost. In this paper, I address both of these issues by increasing the accuracy of the computer vision models without the increased cost of using LiDAR and or sensors. I achieved this by improving the accuracy of computer vision models by sensor fusing CAN bus data, a vehicle protocol, with video data. CAN bus data is a rich source of information about the vehicle's state, including its speed, steering angle, and acceleration. By fusing this data with video data, the accuracy of the computer vision model's predictions can be improved. When I trained the model without CAN bus data, I obtained an RMSE of 0.02492, while the model trained with the CAN bus data achieved an RMSE of 0.01970. This finding indicates that fusing CAN Bus data with video data can reduce the computer vision model's prediction error by 20% with some models decreasing the error by 80%.
</details>
<details>
<summary>摘要</summary>
近年来，驾驶自动化领域内的终端转向预测技术得到了广泛的研究。原始方法实现终端转向是通过计算机视觉模型在实时视频数据上进行预测。然而，为了进一步提高准确性，许多公司通过整合激光雷达（LiDAR）和/或雷达感知器的整合来进行整合感知。然而，加入激光和感知器的成本很高。在这篇论文中，我解决了这两个问题，即提高计算机视觉模型的准确性，不需要增加LiDAR和/或感知器的成本。我实现了这一点通过将CAN总线数据（车辆协议）与视频数据整合，CAN总线数据包含车辆的状态信息，包括速度、转向角度和加速度。当我不使用CAN总线数据进行训练时，我获得的RMSE值为0.02492，而使用CAN总线数据进行训练的模型则可以得到RMSE值为0.01970。这一结果表明，将CAN总线数据与视频数据整合可以降低计算机视觉模型的预测错误率，一些模型可以降低错误率80%。
</details></li>
</ul>
<hr>
<h2 id="When-Urban-Region-Profiling-Meets-Large-Language-Models"><a href="#When-Urban-Region-Profiling-Meets-Large-Language-Models" class="headerlink" title="When Urban Region Profiling Meets Large Language Models"></a>When Urban Region Profiling Meets Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18340">http://arxiv.org/abs/2310.18340</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yibo Yan, Haomin Wen, Siru Zhong, Wei Chen, Haodong Chen, Qingsong Wen, Roger Zimmermann, Yuxuan Liang</li>
<li>for: 这个论文的目的是提出一种基于大语言模型（LLM）的城市区域 profiling 方法，以便为城市规划和可持续发展提供有用的数据支持。</li>
<li>methods: 这个方法使用了一种名为 UrbanCLIP 的新型方法，它利用 LLM 的力量，将文本modalities 集成到城市图像 profiling 中，并通过对Image-to-Text LLM 生成的文本描述和图像进行协同学习，实现了自然语言监督下的城市视觉学习。</li>
<li>results: 实验结果表明，UrbanCLIP 方法可以提高城市区域 profiling 的精度，在四个主要中国城市的三个城市指标预测中，与当前方法相比，平均提高了6.1%的R^2 值。<details>
<summary>Abstract</summary>
Urban region profiling from web-sourced data is of utmost importance for urban planning and sustainable development. We are witnessing a rising trend of LLMs for various fields, especially dealing with multi-modal data research such as vision-language learning, where the text modality serves as a supplement information for the image. Since textual modality has never been introduced into modality combinations in urban region profiling, we aim to answer two fundamental questions in this paper: i) Can textual modality enhance urban region profiling? ii) and if so, in what ways and with regard to which aspects? To answer the questions, we leverage the power of Large Language Models (LLMs) and introduce the first-ever LLM-enhanced framework that integrates the knowledge of textual modality into urban imagery profiling, named LLM-enhanced Urban Region Profiling with Contrastive Language-Image Pretraining (UrbanCLIP). Specifically, it first generates a detailed textual description for each satellite image by an open-source Image-to-Text LLM. Then, the model is trained on the image-text pairs, seamlessly unifying natural language supervision for urban visual representation learning, jointly with contrastive loss and language modeling loss. Results on predicting three urban indicators in four major Chinese metropolises demonstrate its superior performance, with an average improvement of 6.1% on R^2 compared to the state-of-the-art methods. Our code and the image-language dataset will be released upon paper notification.
</details>
<details>
<summary>摘要</summary>
城市区域识别从网络数据处理是现代城市规划和可持续发展的重要前提。我们目睹到涉及多Modal数据研究的LLM技术在不断升温，特别是视觉语言学习，其中文本模式作为图像信息的补充。由于文本模式从未在城市区域识别中使用过，我们的研究旨在回答以下两个基本问题：一、文本模式能否提高城市区域识别？二、如果可以，那么在哪些方面和如何？为了回答这些问题，我们利用LLM技术的强大能力，并提出了首次在城市区域识别中结合文本模式的框架，名为UrbanCLIP。具体来说，它首先生成每个卫星图像的详细文本描述，使用开源的Image-to-Text LLM进行生成。然后，模型在图像-文本对中进行训练，同时结合对偶损失和语言模型学习损失。实验结果表明，UrbanCLIP在预测四个主要中国城市的三个城市指标方面表现出色，与当前方法的平均提升率为6.1%。我们的代码和图像语言数据将在论文通知时发布。
</details></li>
</ul>
<hr>
<h2 id="Are-LSTMs-Good-Few-Shot-Learners"><a href="#Are-LSTMs-Good-Few-Shot-Learners" class="headerlink" title="Are LSTMs Good Few-Shot Learners?"></a>Are LSTMs Good Few-Shot Learners?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14139">http://arxiv.org/abs/2310.14139</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mikehuisman/lstm-fewshotlearning-oplstm">https://github.com/mikehuisman/lstm-fewshotlearning-oplstm</a></li>
<li>paper_authors: Mike Huisman, Thomas M. Moerland, Aske Plaat, Jan N. van Rijn</li>
<li>for: 这篇研究旨在探讨深度学习需要大量数据来学习新任务，而meta-learning可以解决这个限制。</li>
<li>methods: 这篇研究使用了LSTM和backpropagation来学习meta-learning。</li>
<li>results: LSTM surprisingly在一个简单的几个据点变数回推 regression benchmark上表现出色，但是在更复杂的几个据点图像分类 benchmark上表现不如预期。研究人员提出了两个可能的解释，并提出了一个新的方法called Outer Product LSTM (OP-LSTM)，它可以解决这些问题，并在内部预测和跨领域预测中获得了明显的性能提升。<details>
<summary>Abstract</summary>
Deep learning requires large amounts of data to learn new tasks well, limiting its applicability to domains where such data is available. Meta-learning overcomes this limitation by learning how to learn. In 2001, Hochreiter et al. showed that an LSTM trained with backpropagation across different tasks is capable of meta-learning. Despite promising results of this approach on small problems, and more recently, also on reinforcement learning problems, the approach has received little attention in the supervised few-shot learning setting. We revisit this approach and test it on modern few-shot learning benchmarks. We find that LSTM, surprisingly, outperform the popular meta-learning technique MAML on a simple few-shot sine wave regression benchmark, but that LSTM, expectedly, fall short on more complex few-shot image classification benchmarks. We identify two potential causes and propose a new method called Outer Product LSTM (OP-LSTM) that resolves these issues and displays substantial performance gains over the plain LSTM. Compared to popular meta-learning baselines, OP-LSTM yields competitive performance on within-domain few-shot image classification, and performs better in cross-domain settings by 0.5% to 1.9% in accuracy score. While these results alone do not set a new state-of-the-art, the advances of OP-LSTM are orthogonal to other advances in the field of meta-learning, yield new insights in how LSTM work in image classification, allowing for a whole range of new research directions. For reproducibility purposes, we publish all our research code publicly.
</details>
<details>
<summary>摘要</summary>
深度学习需要大量数据来学习新任务，这限制了其应用于具有相应数据的领域。元学习把这一限制打砸，它学会如何学习。在2001年， Hochreiter等人表明了一种使用反传播的LSTM在不同任务上进行元学习，这种方法在小问题上表现了扎实的成果，而且在最近也在强化学习问题上得到了应用。然而，这种方法在监督少量学习设定中受到了少量关注。我们重新审视了这种方法，并在现代少量学习标准架构上测试它。我们发现LSTM在简单的几个shot折衔回归benchmark上表现出优于受欢迎的MAML方法，但是LSTM在更复杂的几个shot图像分类benchmark上表现不佳。我们认为这可能是两个原因，并提出了一种新的方法called Outer Product LSTM（OP-LSTM），该方法可以解决这些问题，并且在监督少量学习和跨频道设定中显示了显著性能提升。相比 популяр的元学习基eline，OP-LSTM在内频道图像分类中展示了竞争性能，并在跨频道设定中表现出0.5%到1.9%的准确率提升。虽然这些结果并不是新的状态态，但OP-LSTM的进步是与其他元学习领域的进步正交的，它为图像分类领域带来了新的研究方向，并且为研究者提供了更多的研究方向。为了保证可重现性，我们将所有的研究代码公开发布。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/22/cs.AI_2023_10_22/" data-id="clp88dbrt0060ob8813je9h6g" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/22/cs.CL_2023_10_22/" class="article-date">
  <time datetime="2023-10-22T11:00:00.000Z" itemprop="datePublished">2023-10-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/22/cs.CL_2023_10_22/">cs.CL - 2023-10-22</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Domain-Terminology-Integration-into-Machine-Translation-Leveraging-Large-Language-Models"><a href="#Domain-Terminology-Integration-into-Machine-Translation-Leveraging-Large-Language-Models" class="headerlink" title="Domain Terminology Integration into Machine Translation: Leveraging Large Language Models"></a>Domain Terminology Integration into Machine Translation: Leveraging Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14451">http://arxiv.org/abs/2310.14451</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yasmin Moslem, Gianfranco Romani, Mahdi Molaei, Rejwanul Haque, John D. Kelleher, Andy Way</li>
<li>for: 提高机器翻译（MT）的精度，以便在专业领域内进行更好的交流和理解。</li>
<li>methods: 利用大语言模型（LLM）进行两项实验，包括生成同时语言对的数据和将MT模型中的翻译结果进行自动批注。</li>
<li>results: 结果表明，我们的提议方法能够有效地将预先批准的词汇 integrate 到翻译中，成功率从36.67%提高至72.88%。<details>
<summary>Abstract</summary>
This paper discusses the methods that we used for our submissions to the WMT 2023 Terminology Shared Task for German-to-English (DE-EN), English-to-Czech (EN-CS), and Chinese-to-English (ZH-EN) language pairs. The task aims to advance machine translation (MT) by challenging participants to develop systems that accurately translate technical terms, ultimately enhancing communication and understanding in specialised domains. To this end, we conduct experiments that utilise large language models (LLMs) for two purposes: generating synthetic bilingual terminology-based data, and post-editing translations generated by an MT model through incorporating pre-approved terms. Our system employs a four-step process: (i) using an LLM to generate bilingual synthetic data based on the provided terminology, (ii) fine-tuning a generic encoder-decoder MT model, with a mix of the terminology-based synthetic data generated in the first step and a randomly sampled portion of the original generic training data, (iii) generating translations with the fine-tuned MT model, and (iv) finally, leveraging an LLM for terminology-constrained automatic post-editing of the translations that do not include the required terms. The results demonstrate the effectiveness of our proposed approach in improving the integration of pre-approved terms into translations. The number of terms incorporated into the translations of the blind dataset increases from an average of 36.67% with the generic model to an average of 72.88% by the end of the process. In other words, successful utilisation of terms nearly doubles across the three language pairs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Using an LLM to generate bilingual synthetic data based on the provided terminology.2. Fine-tuning a generic encoder-decoder MT model with a mix of the terminology-based synthetic data and a randomly sampled portion of the original generic training data.3. Generating translations with the fine-tuned MT model.4. Leveraging an LLM for terminology-constrained automatic post-editing of the translations that do not include the required terms.The results show that our proposed approach effectively improves the integration of pre-approved terms into translations. The average number of terms incorporated into the translations of the blind dataset increases from 36.67% with the generic model to 72.88% by the end of the process, nearly doubling the successful utilization of terms across the three language pairs.</details></li>
</ol>
<hr>
<h2 id="TATA-Stance-Detection-via-Topic-Agnostic-and-Topic-Aware-Embeddings"><a href="#TATA-Stance-Detection-via-Topic-Agnostic-and-Topic-Aware-Embeddings" class="headerlink" title="TATA: Stance Detection via Topic-Agnostic and Topic-Aware Embeddings"></a>TATA: Stance Detection via Topic-Agnostic and Topic-Aware Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14450">http://arxiv.org/abs/2310.14450</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hanshanley/tata">https://github.com/hanshanley/tata</a></li>
<li>paper_authors: Hans W. A. Hanley, Zakir Durumeric</li>
<li>for: 本文是为了建立一个通用的立场检测模型，能够在不同主题下仍能准确地检测立场。</li>
<li>methods: 本文使用了对照学习和一个未标注的新闻文章数据集，通过培育TAG和TAW表示来建立不同主题下的立场检测模型。</li>
<li>results: combine这两种表示， authors achieved state-of-the-art performance on several public stance detection datasets（Zero-shot VAST dataset的 $F_1$-score为0.771）。<details>
<summary>Abstract</summary>
Stance detection is important for understanding different attitudes and beliefs on the Internet. However, given that a passage's stance toward a given topic is often highly dependent on that topic, building a stance detection model that generalizes to unseen topics is difficult. In this work, we propose using contrastive learning as well as an unlabeled dataset of news articles that cover a variety of different topics to train topic-agnostic/TAG and topic-aware/TAW embeddings for use in downstream stance detection. Combining these embeddings in our full TATA model, we achieve state-of-the-art performance across several public stance detection datasets (0.771 $F_1$-score on the Zero-shot VAST dataset). We release our code and data at https://github.com/hanshanley/tata.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language Simplified Chinese;Stance detection is important for understanding different attitudes and beliefs on the Internet. However, given that a passage's stance toward a given topic is often highly dependent on that topic, building a stance detection model that generalizes to unseen topics is difficult. In this work, we propose using contrastive learning as well as an unlabeled dataset of news articles that cover a variety of different topics to train topic-agnostic/TAG and topic-aware/TAW embeddings for use in downstream stance detection. Combining these embeddings in our full TATA model, we achieve state-of-the-art performance across several public stance detection datasets (0.771 $F_1$-score on the Zero-shot VAST dataset). We release our code and data at https://github.com/hanshanley/tata.中文简体版：在互联网上，理解不同的态度和信仰是重要的。然而，由于文章对某个话题的态度往往受话题的限制，建立能 generalized to unseen topics的态度探测模型是困难的。在这项工作中，我们提议使用对比学习以及一个不同话题的新闻文章数据集来训练无关话题/TAG和相关话题/TAW的嵌入，用于下游态度探测。将这些嵌入组合在我们的全局TATA模型中，我们在多个公共态度探测数据集上达到了状态级表现（在零shot VAST数据集上的$F_1$分数为0.771）。我们在 GitHub 上发布了代码和数据，请参考 <https://github.com/hanshanley/tata>。
</details></li>
</ul>
<hr>
<h2 id="Text-generation-for-dataset-augmentation-in-security-classification-tasks"><a href="#Text-generation-for-dataset-augmentation-in-security-classification-tasks" class="headerlink" title="Text generation for dataset augmentation in security classification tasks"></a>Text generation for dataset augmentation in security classification tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14429">http://arxiv.org/abs/2310.14429</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wenliangdai/multi-task-offensive-language-detection">https://github.com/wenliangdai/multi-task-offensive-language-detection</a></li>
<li>paper_authors: Alexander P. Welsh, Matthew Edwards</li>
<li>for: 填充安全领域的训练数据不足问题</li>
<li>methods: 使用自然语言文本生成器填充训练数据，测试多个安全相关文本分类任务</li>
<li>results: GPT-3数据增强策略可以对训练无足问题进行改善，尤其是在知道阳性类别数据有严重限制的情况下<details>
<summary>Abstract</summary>
Security classifiers, designed to detect malicious content in computer systems and communications, can underperform when provided with insufficient training data. In the security domain, it is often easy to find samples of the negative (benign) class, and challenging to find enough samples of the positive (malicious) class to train an effective classifier. This study evaluates the application of natural language text generators to fill this data gap in multiple security-related text classification tasks. We describe a variety of previously-unexamined language-model fine-tuning approaches for this purpose and consider in particular the impact of disproportionate class-imbalances in the training set. Across our evaluation using three state-of-the-art classifiers designed for offensive language detection, review fraud detection, and SMS spam detection, we find that models trained with GPT-3 data augmentation strategies outperform both models trained without augmentation and models trained using basic data augmentation strategies already in common usage. In particular, we find substantial benefits for GPT-3 data augmentation strategies in situations with severe limitations on known positive-class samples.
</details>
<details>
<summary>摘要</summary>
安全分类器，用于检测计算机系统和通信中的恶意内容，可能会表现不佳当提供不充分的训练数据。在安全领域，通常容易找到benign类样本，而困难找到足够的malicious类样本来训练有效的分类器。本研究利用自然语言文本生成器来填充这种数据空白，并考虑了训练集中略重的类别不均衡的影响。我们使用三种当前最佳的分类器，用于探测攻击性语言、评论骗局和短信骗局，并评估了不同的语言模型练习方法。我们发现，使用GPT-3数据生成策略可以超越没有增强和基本增强策略的模型，尤其在限制了知道的正面类样本的情况下。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-are-biased-to-overestimate-profoundness"><a href="#Large-Language-Models-are-biased-to-overestimate-profoundness" class="headerlink" title="Large Language Models are biased to overestimate profoundness"></a>Large Language Models are biased to overestimate profoundness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14422">http://arxiv.org/abs/2310.14422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eugenio Herrera-Berg, Tomás Vergara Browne, Pablo León-Villagrá, Marc-Lluís Vives, Cristian Buc Calderon</li>
<li>for: 本研究评估了多种语言模型（LLMs）对日常、动员、 Pseudo-profound声明的评估能力，以及RLHF对模型带来的偏见。</li>
<li>methods: 研究使用了多种提示技术，包括ew-shot学习提示和链式思维提示，以评估模型对不同类型声明的评估能力。</li>
<li>results: 研究发现，LLMs和人类之间存在显著的声明相似性，不管使用哪种提示技术。但是，LLMs系统性地过分评估非сен的声明，除了Tk-instruct，它独特地下esti mates声明的深度。ew-shot学习提示能够使得模型的评估与人类更加相似。此外，研究还发现RLHF可能导致模型带来偏见，增加声明深度的评估。<details>
<summary>Abstract</summary>
Recent advancements in natural language processing by large language models (LLMs), such as GPT-4, have been suggested to approach Artificial General Intelligence. And yet, it is still under dispute whether LLMs possess similar reasoning abilities to humans. This study evaluates GPT-4 and various other LLMs in judging the profoundness of mundane, motivational, and pseudo-profound statements. We found a significant statement-to-statement correlation between the LLMs and humans, irrespective of the type of statements and the prompting technique used. However, LLMs systematically overestimate the profoundness of nonsensical statements, with the exception of Tk-instruct, which uniquely underestimates the profoundness of statements. Only few-shot learning prompts, as opposed to chain-of-thought prompting, draw LLMs ratings closer to humans. Furthermore, this work provides insights into the potential biases induced by Reinforcement Learning from Human Feedback (RLHF), inducing an increase in the bias to overestimate the profoundness of statements.
</details>
<details>
<summary>摘要</summary>
latest advancements in natural language processing by large language models (LLMs), such as GPT-4, have been suggested to approach Artificial General Intelligence. However, it is still under dispute whether LLMs possess similar reasoning abilities to humans. This study evaluates GPT-4 and various other LLMs in judging the profundity of mundane, motivational, and pseudo-profound statements. We found a significant statement-to-statement correlation between the LLMs and humans, irrespective of the type of statements and the prompting technique used. However, LLMs systematically overestimate the profoundness of nonsensical statements, with the exception of Tk-instruct, which uniquely underestimates the profoundness of statements. Only few-shot learning prompts, as opposed to chain-of-thought prompting, draw LLMs ratings closer to humans. Furthermore, this work provides insights into the potential biases induced by Reinforcement Learning from Human Feedback (RLHF), inducing an increase in the bias to overestimate the profoundness of statements.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="REFER-An-End-to-end-Rationale-Extraction-Framework-for-Explanation-Regularization"><a href="#REFER-An-End-to-end-Rationale-Extraction-Framework-for-Explanation-Regularization" class="headerlink" title="REFER: An End-to-end Rationale Extraction Framework for Explanation Regularization"></a>REFER: An End-to-end Rationale Extraction Framework for Explanation Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14418">http://arxiv.org/abs/2310.14418</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Reza Ghasemi Madani, Pasquale Minervini</li>
<li>for: 本文旨在提高Explainable Natural Language Processing中的人工标注文本解释的重要性。</li>
<li>methods: 本文提出了一种名为REFER的框架，该框架使用可微的解释EXTractor，可以在推理过程中借鉴人工标注的帮助。</li>
<li>results: 在我们的实验中，REFER在具有 faithfulness、plausibility和下游任务准确率的情况下，与之前的基线比较，在e-SNLI和CoS-E上得到了较好的结果，其中的composite normalized relative gain比例提高了11%和3%。<details>
<summary>Abstract</summary>
Human-annotated textual explanations are becoming increasingly important in Explainable Natural Language Processing. Rationale extraction aims to provide faithful (i.e., reflective of the behavior of the model) and plausible (i.e., convincing to humans) explanations by highlighting the inputs that had the largest impact on the prediction without compromising the performance of the task model. In recent works, the focus of training rationale extractors was primarily on optimizing for plausibility using human highlights, while the task model was trained on jointly optimizing for task predictive accuracy and faithfulness. We propose REFER, a framework that employs a differentiable rationale extractor that allows to back-propagate through the rationale extraction process. We analyze the impact of using human highlights during training by jointly training the task model and the rationale extractor. In our experiments, REFER yields significantly better results in terms of faithfulness, plausibility, and downstream task accuracy on both in-distribution and out-of-distribution data. On both e-SNLI and CoS-E, our best setting produces better results in terms of composite normalized relative gain than the previous baselines by 11% and 3%, respectively.
</details>
<details>
<summary>摘要</summary>
人类标注文本解释在可解释自然语言处理中变得越来越重要。理由提取目标为提供准确（即模型行为reflective）并有理由的解释，而不是妥协任务模型性能。在现有的工作中，训练理由提取器的主要目标是优化假设性，使用人类高亮来评估plausibility。我们提出了REFER框架，它使用可微分的理由提取器，允许在理由提取过程中进行反propagation。我们分析了在训练中使用人类高亮的影响，并在任务模型和理由提取器同时训练。在我们的实验中，REFER实现了在 faithfulness、plausibility 和下游任务准确率方面提高了较大的改进，并在 e-SNLI 和 CoS-E 上实现了更好的 composite normalized relative gain 表现，相比前一个基eline提高11%和3%。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Subjective-Cognitive-Appraisals-of-Emotions-from-Large-Language-Models"><a href="#Evaluating-Subjective-Cognitive-Appraisals-of-Emotions-from-Large-Language-Models" class="headerlink" title="Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models"></a>Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14389">http://arxiv.org/abs/2310.14389</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/honglizhan/covidet-appraisals-public">https://github.com/honglizhan/covidet-appraisals-public</a></li>
<li>paper_authors: Hongli Zhan, Desmond C. Ong, Junyi Jessy Li</li>
<li>for: This paper is written to address the lack of research on the automatic prediction of cognitive appraisals in emotional experiences.</li>
<li>methods: The paper uses a dataset called CovidET-Appraisals, which assesses 24 appraisal dimensions in 241 Reddit posts, to evaluate the ability of large language models to automatically assess and explain cognitive appraisals.</li>
<li>results: The paper finds that while the best models are performant, open-sourced LLMs fall short at this task, presenting a new challenge in the future development of emotionally intelligent models.Here is the information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了弥补情感经验中自主评估的缺失而写的。</li>
<li>methods: 这篇论文使用了 CovidET-Appraisals 数据集，该数据集包含 24 个评估维度，每个维度都有自然语言的理由，在 241 篇 Reddit 帖子中进行了评估。</li>
<li>results: 这篇论文发现，虽然最佳模型表现出色，但开源 LL 模型在这个任务上异常缺乏能力，这提出了未来情感智能模型的新挑战。<details>
<summary>Abstract</summary>
The emotions we experience involve complex processes; besides physiological aspects, research in psychology has studied cognitive appraisals where people assess their situations subjectively, according to their own values (Scherer, 2005). Thus, the same situation can often result in different emotional experiences. While the detection of emotion is a well-established task, there is very limited work so far on the automatic prediction of cognitive appraisals. This work fills the gap by presenting CovidET-Appraisals, the most comprehensive dataset to-date that assesses 24 appraisal dimensions, each with a natural language rationale, across 241 Reddit posts. CovidET-Appraisals presents an ideal testbed to evaluate the ability of large language models -- excelling at a wide range of NLP tasks -- to automatically assess and explain cognitive appraisals. We found that while the best models are performant, open-sourced LLMs fall short at this task, presenting a new challenge in the future development of emotionally intelligent models. We release our dataset at https://github.com/honglizhan/CovidET-Appraisals-Public.
</details>
<details>
<summary>摘要</summary>
我们的情感经历 involve 复杂的过程; 不仅有生物学方面，心理学研究也探究了人们对自己情感状况的主观评估，根据自己的价值观（Scherer, 2005）。因此，同一个情况可能会导致不同的情感体验。虽然检测情感是一项已经成熔的任务，但是自动预测认知评估还没有得到过足的关注。这项工作填补了这一空白，并提供了 CovidET-Appraisals dataset，覆盖了 24 个评估维度，每个维度有自然语言的理由，在 241 篇 Reddit 帖子中进行评估。CovidET-Appraisals 提供了一个完善的测试环境，用于评估大语言模型在 NLP 任务中表现的能力，并且自动评估和解释认知评估。我们发现，尽管最佳模型表现出色，但开源 LLM 在这项任务上却落后，提出了未来开发情感智能模型的新挑战。我们将数据集发布在 GitHub 上，具体地址为 <https://github.com/honglizhan/CovidET-Appraisals-Public>。
</details></li>
</ul>
<hr>
<h2 id="Bi-Encoders-based-Species-Normalization-–-Pairwise-Sentence-Learning-to-Rank"><a href="#Bi-Encoders-based-Species-Normalization-–-Pairwise-Sentence-Learning-to-Rank" class="headerlink" title="Bi-Encoders based Species Normalization – Pairwise Sentence Learning to Rank"></a>Bi-Encoders based Species Normalization – Pairwise Sentence Learning to Rank</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14366">http://arxiv.org/abs/2310.14366</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zainab Awan, Tim Kahlke, Peter Ralph, Paul Kennedy</li>
<li>for: 该论文旨在提出一种深度学习方法，用于生物医学名实体Normalization。</li>
<li>methods: 该方法基于Best Matching 25算法生成候选概念，然后使用bi-directional encoder representation from the encoder (BERT)进行排名。</li>
<li>results: 对于物种实体类型，我们的方法比现有方法更高效，能够准确地将实体连接到NCBI分类。<details>
<summary>Abstract</summary>
Motivation: Biomedical named-entity normalization involves connecting biomedical entities with distinct database identifiers in order to facilitate data integration across various fields of biology. Existing systems for biomedical named entity normalization heavily rely on dictionaries, manually created rules, and high-quality representative features such as lexical or morphological characteristics. However, recent research has investigated the use of neural network-based models to reduce dependence on dictionaries, manually crafted rules, and features. Despite these advancements, the performance of these models is still limited due to the lack of sufficiently large training datasets. These models have a tendency to overfit small training corpora and exhibit poor generalization when faced with previously unseen entities, necessitating the redesign of rules and features. Contribution: We present a novel deep learning approach for named entity normalization, treating it as a pair-wise learning to rank problem. Our method utilizes the widely-used information retrieval algorithm Best Matching 25 to generate candidate concepts, followed by the application of bi-directional encoder representation from the encoder (BERT) to re-rank the candidate list. Notably, our approach eliminates the need for feature-engineering or rule creation. We conduct experiments on species entity types and evaluate our method against state-of-the-art techniques using LINNAEUS and S800 biomedical corpora. Our proposed approach surpasses existing methods in linking entities to the NCBI taxonomy. To the best of our knowledge, there is no existing neural network-based approach for species normalization in the literature.
</details>
<details>
<summary>摘要</summary>
目的：生物医学命名实体normalization通过连接生物医学实体与特定数据库标识符来实现数据集成。现有的生物医学命名实体normalization系统大量依赖于词典、手动创建的规则和高质量表达特征。然而，最近的研究已经调查了使用神经网络模型来减少词典、手动创建的规则和特征的依赖。尽管有这些进步，但现有的模型在性能上仍有限制，它们往往因为训练数据集的小型而过拟合，并且对于之前未看到的实体表现出差异欠拟合，导致规则和特征的重新设计。贡献：我们提出了一种新的深度学习方法 для命名实体normalization，将其视为一个对照学习排名问题。我们的方法首先使用广泛使用的信息检索算法Best Matching 25生成候选概念，然后通过双向encoder表示（BERT）对候选列表进行排名。吸引注意的是，我们的方法不需要特征工程或规则创建。我们在种类实体类型上进行实验，并对我们的方法与当前的状态艺术技术进行比较。我们的提议方法在连接实体到NCBI分类中超过现有方法。到目前为止，there is no existing neural network-based approach for species normalization in the literature.
</details></li>
</ul>
<hr>
<h2 id="Is-ChatGPT-a-game-changer-for-geocoding-–-a-benchmark-for-geocoding-address-parsing-techniques"><a href="#Is-ChatGPT-a-game-changer-for-geocoding-–-a-benchmark-for-geocoding-address-parsing-techniques" class="headerlink" title="Is ChatGPT a game changer for geocoding – a benchmark for geocoding address parsing techniques"></a>Is ChatGPT a game changer for geocoding – a benchmark for geocoding address parsing techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14360">http://arxiv.org/abs/2310.14360</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengcong Yin, Diya Li, Daniel W. Goldberg</li>
<li>for: 评估 GPT-3 模型在地理编码地址解析任务中的表现。</li>
<li>methods: 使用人工输入模式挖掘数据集，并对 GPT-3 模型、transformer 模型和 LSTM-CRF 模型进行训练和比较。</li>
<li>results: Bidirectional LSTM-CRF 模型在这些 transformer 模型和 GPT-3 模型中表现最佳，而 transformer 模型和 GPT-3 模型在表现上几乎相当。GPT-3 模型，虽然表现不佳，但在几个例子下表现出了潜在的改进空间。<details>
<summary>Abstract</summary>
The remarkable success of GPT models across various tasks, including toponymy recognition motivates us to assess the performance of the GPT-3 model in the geocoding address parsing task. To ensure that the evaluation more accurately mirrors performance in real-world scenarios with diverse user input qualities and resolve the pressing need for a 'gold standard' evaluation dataset for geocoding systems, we introduce a benchmark dataset of low-quality address descriptions synthesized based on human input patterns mining from actual input logs of a geocoding system in production. This dataset has 21 different input errors and variations; contains over 239,000 address records that are uniquely selected from streets across all U.S. 50 states and D.C.; and consists of three subsets to be used as training, validation, and testing sets. Building on this, we train and gauge the performance of the GPT-3 model in extracting address components, contrasting its performance with transformer-based and LSTM-based models. The evaluation results indicate that Bidirectional LSTM-CRF model has achieved the best performance over these transformer-based models and GPT-3 model. Transformer-based models demonstrate very comparable results compared to the Bidirectional LSTM-CRF model. The GPT-3 model, though trailing in performance, showcases potential in the address parsing task with few-shot examples, exhibiting room for improvement with additional fine-tuning. We open source the code and data of this presented benchmark so that researchers can utilize it for future model development or extend it to evaluate similar tasks, such as document geocoding.
</details>
<details>
<summary>摘要</summary>
“GPT模型在不同任务中的成功，包括地名识别，使我们感兴趣测试GPT-3模型在地址解析任务中的性能。为了更准确地反映实际场景中的用户输入质量和提供一个'金标准'评价数据集，我们创建了一个基于人工输入模式的低质量地址描述 synthesized 数据集。这个数据集包含21种输入错误和变化，涵盖了美国全国50个州和特区的所有街道，共计239,000个唯一选择的地址记录。我们使用这个数据集进行训练和评估GPT-3模型、 transformer 基于模型和 LSTM 基于模型的性能。评估结果显示，携带irectional LSTM-CRF 模型在这些 transformer 基于模型和 GPT-3 模型中表现最佳。 transformer 基于模型和 GPT-3 模型在性能上几乎相当，但 GPT-3 模型在性能上落后，但它在几个例子中表现出了潜力，表明可以通过进一步的微调提高其性能。我们将这个数据集和代码开源，以便未来的研究人员可以利用它来开发模型或扩展其到评估类似任务，如文档地理编码。”
</details></li>
</ul>
<hr>
<h2 id="Cultural-and-Linguistic-Diversity-Improves-Visual-Representations"><a href="#Cultural-and-Linguistic-Diversity-Improves-Visual-Representations" class="headerlink" title="Cultural and Linguistic Diversity Improves Visual Representations"></a>Cultural and Linguistic Diversity Improves Visual Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14356">http://arxiv.org/abs/2310.14356</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andre Ye, Sebastin Santy, Jena D. Hwang, Amy X. Zhang, Ranjay Krishna</li>
<li>for: 这 paper 探讨了图像理解中不同文化背景下的视觉吗。</li>
<li>methods: 作者使用了多种方法，包括 scene graph, embedding, 和语言复杂度来评估不同语言的caption的semantic coverage。</li>
<li>results: 研究发现，当数据包含多种语言时，caption的semantic coverage会高于单语言数据，并且模型在不同语言的测试数据上表现最佳。<details>
<summary>Abstract</summary>
Computer vision often treats perception as objective, and this assumption gets reflected in the way that datasets are collected and models are trained. For instance, image descriptions in different languages are typically assumed to be translations of the same semantic content. However, work in cross-cultural psychology and linguistics has shown that individuals differ in their visual perception depending on their cultural background and the language they speak. In this paper, we demonstrate significant differences in semantic content across languages in both dataset and model-produced captions. When data is multilingual as opposed to monolingual, captions have higher semantic coverage on average, as measured by scene graph, embedding, and linguistic complexity. For example, multilingual captions have on average 21.8% more objects, 24.5% more relations, and 27.1% more attributes than a set of monolingual captions. Moreover, models trained on content from different languages perform best against test data from those languages, while those trained on multilingual content perform consistently well across all evaluation data compositions. Our research provides implications for how diverse modes of perception can improve image understanding.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-Law-and-NLP-Bridging-Disciplinary-Disconnects"><a href="#The-Law-and-NLP-Bridging-Disciplinary-Disconnects" class="headerlink" title="The Law and NLP: Bridging Disciplinary Disconnects"></a>The Law and NLP: Bridging Disciplinary Disconnects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14346">http://arxiv.org/abs/2310.14346</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robert Mahari, Dominik Stammbach, Elliott Ash, Alex ‘Sandy’ Pentland</li>
<li>for: 法律实践中的语言是其根源，但法律师和学者尚未广泛采用自然语言处理（NLP）工具。同时，法律系统正面临一个访问正义危机，NLP可能可以减轻这个危机。</li>
<li>methods: 本文论证法律NLP领域的研究缺乏与法律社区的连接，导致一些最受欢迎的法律NLP任务无法满足法律实践中的需求。</li>
<li>results: 我们在审查最近的法律NLP文献中发现，法律NLP社区与法律学术界之间存在较少的交叉。我们认为，一些最受欢迎的法律NLP任务无法满足法律实践中的需求。我们提出了一些可以bridgingdisciplinary disconnects的法律NLP任务，并高亮了未曾探索的法律NLP研究领域。<details>
<summary>Abstract</summary>
Legal practice is intrinsically rooted in the fabric of language, yet legal practitioners and scholars have been slow to adopt tools from natural language processing (NLP). At the same time, the legal system is experiencing an access to justice crisis, which could be partially alleviated with NLP. In this position paper, we argue that the slow uptake of NLP in legal practice is exacerbated by a disconnect between the needs of the legal community and the focus of NLP researchers. In a review of recent trends in the legal NLP literature, we find limited overlap between the legal NLP community and legal academia. Our interpretation is that some of the most popular legal NLP tasks fail to address the needs of legal practitioners. We discuss examples of legal NLP tasks that promise to bridge disciplinary disconnects and highlight interesting areas for legal NLP research that remain underexplored.
</details>
<details>
<summary>摘要</summary>
法律实践深深涉及语言的结构，然而法律师和学者对自然语言处理（NLP）技术的采用相对落后。同时，法律系统正面临访问正义危机，NLP可能可以减轻这种危机。在这份位点纸中，我们 argue That the slow adoption of NLP in legal practice is exacerbated by a disconnect between the needs of the legal community and the focus of NLP researchers. In a review of recent trends in legal NLP literature, we find limited overlap between the legal NLP community and legal academia. Our interpretation is that some of the most popular legal NLP tasks fail to address the needs of legal practitioners. We discuss examples of legal NLP tasks that promise to bridge disciplinary disconnects and highlight interesting areas for legal NLP research that remain underexplored.Here's the translation in Traditional Chinese:法律实践深深涉及语言的结构，然而法律师和学者对自然语言处理（NLP）技术的采用相对落后。同时，法律系统正面临访问正义危机，NLP可能可以减轻这种危机。在这份位点纸中，我们 argue That the slow adoption of NLP in legal practice is exacerbated by a disconnect between the needs of the legal community and the focus of NLP researchers. In a review of recent trends in legal NLP literature, we find limited overlap between the legal NLP community and legal academia. Our interpretation is that some of the most popular legal NLP tasks fail to address the needs of legal practitioners. We discuss examples of legal NLP tasks that promise to bridge disciplinary disconnects and highlight interesting areas for legal NLP research that remain underexplored.
</details></li>
</ul>
<hr>
<h2 id="Social-Commonsense-Guided-Search-Query-Generation-for-Open-Domain-Knowledge-Powered-Conversations"><a href="#Social-Commonsense-Guided-Search-Query-Generation-for-Open-Domain-Knowledge-Powered-Conversations" class="headerlink" title="Social Commonsense-Guided Search Query Generation for Open-Domain Knowledge-Powered Conversations"></a>Social Commonsense-Guided Search Query Generation for Open-Domain Knowledge-Powered Conversations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14340">http://arxiv.org/abs/2310.14340</a></li>
<li>repo_url: None</li>
<li>paper_authors: Revanth Gangi Reddy, Hao Bai, Wentao Yao, Sharath Chandra Etagi Suresh, Heng Ji, ChengXiang Zhai</li>
<li>for: 提高对话中的信息 Retrieval relevance和specificity，使对话更加有趣和有价值。</li>
<li>methods: 利用社交常识对话系统建立话题相关连接，并通过 instruciton-driven 查询生成法生成更加有 relevance 和specificity 的查询。</li>
<li>results: 比较 experiment 结果表明，提出的方法可以超越现有的查询生成技术，并生成更加有趣、有价值和有 relevance 的查询，从而提高对话中的信息 Retrieval 效果。<details>
<summary>Abstract</summary>
Open-domain dialog involves generating search queries that help obtain relevant knowledge for holding informative conversations. However, it can be challenging to determine what information to retrieve when the user is passive and does not express a clear need or request. To tackle this issue, we present a novel approach that focuses on generating internet search queries that are guided by social commonsense. Specifically, we leverage a commonsense dialog system to establish connections related to the conversation topic, which subsequently guides our query generation. Our proposed framework addresses passive user interactions by integrating topic tracking, commonsense response generation and instruction-driven query generation. Through extensive evaluations, we show that our approach overcomes limitations of existing query generation techniques that rely solely on explicit dialog information, and produces search queries that are more relevant, specific, and compelling, ultimately resulting in more engaging responses.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Open-domain dialog involves generating search queries that help obtain relevant knowledge for holding informative conversations. However, it can be challenging to determine what information to retrieve when the user is passive and does not express a clear need or request. To tackle this issue, we present a novel approach that focuses on generating internet search queries that are guided by social commonsense. Specifically, we leverage a commonsense dialog system to establish connections related to the conversation topic, which subsequently guides our query generation. Our proposed framework addresses passive user interactions by integrating topic tracking, commonsense response generation, and instruction-driven query generation. Through extensive evaluations, we show that our approach overcomes limitations of existing query generation techniques that rely solely on explicit dialog information, and produces search queries that are more relevant, specific, and compelling, ultimately resulting in more engaging responses." into 中文（简体）Here's the translation:开放领域对话通常包括生成可以帮助获得有用知识的搜索查询。然而，当用户被动并没有明确的需求或请求时，可能困难确定需要检索哪些信息。为解决这个问题，我们提出了一种新的方法，即通过社会常识导航查询生成。我们利用对话系统来建立与对话话题相关的连接，然后将这些连接用于生成查询。我们的提议的框架解决了悬挂式用户互动的问题，并 integrates 话题跟踪、常识响应生成和指导查询生成。经过广泛的评估，我们表明我们的方法可以超越现有的查询生成技术，生成更加有关、特定和吸引人的搜索查询，最终导致更加有趣的响应。
</details></li>
</ul>
<hr>
<h2 id="DiFair-A-Benchmark-for-Disentangled-Assessment-of-Gender-Knowledge-and-Bias"><a href="#DiFair-A-Benchmark-for-Disentangled-Assessment-of-Gender-Knowledge-and-Bias" class="headerlink" title="DiFair: A Benchmark for Disentangled Assessment of Gender Knowledge and Bias"></a>DiFair: A Benchmark for Disentangled Assessment of Gender Knowledge and Bias</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14329">http://arxiv.org/abs/2310.14329</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mzakizadeh/difair_public">https://github.com/mzakizadeh/difair_public</a></li>
<li>paper_authors: Mahdi Zakizadeh, Kaveh Eskandari Miandoab, Mohammad Taher Pilehvar</li>
<li>for: mitigating the gender bias in pretrained language models and evaluating the impact of bias mitigation on useful gender knowledge</li>
<li>methods: using a manually curated dataset called DiFair, introducing a unified metric called gender invariance score to quantify both biased behavior and preservation of useful gender knowledge</li>
<li>results: experimental results show that debiasing techniques can ameliorate the issue of gender bias, but at the cost of lowering the model’s useful gender knowledge<details>
<summary>Abstract</summary>
Numerous debiasing techniques have been proposed to mitigate the gender bias that is prevalent in pretrained language models. These are often evaluated on datasets that check the extent to which the model is gender-neutral in its predictions. Importantly, this evaluation protocol overlooks the possible adverse impact of bias mitigation on useful gender knowledge. To fill this gap, we propose DiFair, a manually curated dataset based on masked language modeling objectives. DiFair allows us to introduce a unified metric, gender invariance score, that not only quantifies a model's biased behavior, but also checks if useful gender knowledge is preserved. We use DiFair as a benchmark for a number of widely-used pretained language models and debiasing techniques. Experimental results corroborate previous findings on the existing gender biases, while also demonstrating that although debiasing techniques ameliorate the issue of gender bias, this improvement usually comes at the price of lowering useful gender knowledge of the model.
</details>
<details>
<summary>摘要</summary>
很多去偏见技术已经被提出来 Mitigate the gender bias that is prevalent in pre-trained language models. These are often evaluated on datasets that check the extent to which the model is gender-neutral in its predictions. However, this evaluation protocol overlooks the possible adverse impact of bias mitigation on useful gender knowledge. To fill this gap, we propose DiFair, a manually curated dataset based on masked language modeling objectives. DiFair allows us to introduce a unified metric, gender invariance score, that not only quantifies a model's biased behavior, but also checks if useful gender knowledge is preserved. We use DiFair as a benchmark for a number of widely-used pre-trained language models and debiasing techniques. Experimental results corroborate previous findings on the existing gender biases, while also demonstrating that although debiasing techniques ameliorate the issue of gender bias, this improvement usually comes at the price of lowering useful gender knowledge of the model.Here's the translation in Traditional Chinese:很多去偏见技术已经被提出来 Mitigate the gender bias that is prevalent in pre-trained language models. These are often evaluated on datasets that check the extent to which the model is gender-neutral in its predictions. However, this evaluation protocol overlooks the possible adverse impact of bias mitigation on useful gender knowledge. To fill this gap, we propose DiFair, a manually curated dataset based on masked language modeling objectives. DiFair allows us to introduce a unified metric, gender invariance score, that not only quantifies a model's biased behavior, but also checks if useful gender knowledge is preserved. We use DiFair as a benchmark for a number of widely-used pre-trained language models and debiasing techniques. Experimental results corroborate previous findings on the existing gender biases, while also demonstrating that although debiasing techniques ameliorate the issue of gender bias, this improvement usually comes at the price of lowering useful gender knowledge of the model.
</details></li>
</ul>
<hr>
<h2 id="Towards-Harmful-Erotic-Content-Detection-through-Coreference-Driven-Contextual-Analysis"><a href="#Towards-Harmful-Erotic-Content-Detection-through-Coreference-Driven-Contextual-Analysis" class="headerlink" title="Towards Harmful Erotic Content Detection through Coreference-Driven Contextual Analysis"></a>Towards Harmful Erotic Content Detection through Coreference-Driven Contextual Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14325">http://arxiv.org/abs/2310.14325</a></li>
<li>repo_url: None</li>
<li>paper_authors: Inez Okulska, Emilia Wiśnios</li>
<li>for: 本研究旨在开发一种hybrid neural和规则基于的上下文意识检测系统，用于检测色情内容中的有害上下文信息。</li>
<li>methods: 本研究采用了核心引用解决方法，与专业评审人员合作编制了一个数据集，并开发了一个可以分辨有害与无害色情内容的分类器。</li>
<li>results: 本研究在波兰文本上测试了hybrid模型，达到了84%的准确率和80%的回归率，而基于RoBERTa和Longformer的模型则无法显示出类似的表现，这说明了核心引用链的重要性在检测有害色情内容中。<details>
<summary>Abstract</summary>
Adult content detection still poses a great challenge for automation. Existing classifiers primarily focus on distinguishing between erotic and non-erotic texts. However, they often need more nuance in assessing the potential harm. Unfortunately, the content of this nature falls beyond the reach of generative models due to its potentially harmful nature. Ethical restrictions prohibit large language models (LLMs) from analyzing and classifying harmful erotics, let alone generating them to create synthetic datasets for other neural models. In such instances where data is scarce and challenging, a thorough analysis of the structure of such texts rather than a large model may offer a viable solution. Especially given that harmful erotic narratives, despite appearing similar to harmless ones, usually reveal their harmful nature first through contextual information hidden in the non-sexual parts of the narrative.   This paper introduces a hybrid neural and rule-based context-aware system that leverages coreference resolution to identify harmful contextual cues in erotic content. Collaborating with professional moderators, we compiled a dataset and developed a classifier capable of distinguishing harmful from non-harmful erotic content. Our hybrid model, tested on Polish text, demonstrates a promising accuracy of 84% and a recall of 80%. Models based on RoBERTa and Longformer without explicit usage of coreference chains achieved significantly weaker results, underscoring the importance of coreference resolution in detecting such nuanced content as harmful erotics. This approach also offers the potential for enhanced visual explainability, supporting moderators in evaluating predictions and taking necessary actions to address harmful content.
</details>
<details>
<summary>摘要</summary>
成人内容检测仍然是自动化领域的挑战。现有的分类器主要是将 эротиче和非эротиче文本分开。然而，它们经常缺乏对可能的害的评估。 Unfortunately, this type of content is beyond the reach of generative models due to its potentially harmful nature.伦理限制禁止大语言模型（LLMs）从 analyzing和分类害词的内容，尤其是生成这类内容以创建Synthetic datasets for other neural models.在这种数据稀缺和挑战的情况下，一种可靠的解决方案是通过对这些文本的结构进行仔细分析，而不是使用大型模型。这是因为害词内容，尽管看起来和无害内容相似，通常在非性部分中隐藏的上下文信息中表现出害词性。这篇论文介绍了一种混合神经网络和规则库的上下文意识系统，利用核心引用解决方案来识别害词内容中的害词上下文信息。与专业调度人员合作，我们编辑了一个数据集并开发了一个可 distinguish between harmful and non-harmful erotic content的分类器。我们的混合模型在Polish文本上进行测试，显示了84%的准确率和80%的回归率。基于RoBERTa和Longformer的模型，没有显式使用核心引用链，得到的结果显示了较弱的性能，这说明了核心引用解决方案在检测这种细腻内容的害词性方面的重要性。这种方法还提供了可见的视觉解释性，支持调度人员评估预测结果并采取必要的行动来解决害词内容。
</details></li>
</ul>
<hr>
<h2 id="4-and-7-bit-Labeling-for-Projective-and-Non-Projective-Dependency-Trees"><a href="#4-and-7-bit-Labeling-for-Projective-and-Non-Projective-Dependency-Trees" class="headerlink" title="4 and 7-bit Labeling for Projective and Non-Projective Dependency Trees"></a>4 and 7-bit Labeling for Projective and Non-Projective Dependency Trees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14319">http://arxiv.org/abs/2310.14319</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlos Gómez-Rodríguez, Diego Roca, David Vilares</li>
<li>for: 这篇论文是为了提出一种可以将任何 проекive 依赖树转换为一个字符串中的4位标签的编码方法。</li>
<li>methods: 这篇论文使用了一种基于字符串的标签编码方法，每个单词的标签包含4个位数，表示该单词是左或右依赖的、外most的左&#x2F;右依赖、有左&#x2F;右叶子节点等信息。</li>
<li>results: 该编码方法可以在线性时间内编码和解码，并且在一些多样化的树频谱上实现了较高的准确率，比之前最佳的序列标签编码方法更高。<details>
<summary>Abstract</summary>
We introduce an encoding for parsing as sequence labeling that can represent any projective dependency tree as a sequence of 4-bit labels, one per word. The bits in each word's label represent (1) whether it is a right or left dependent, (2) whether it is the outermost (left/right) dependent of its parent, (3) whether it has any left children and (4) whether it has any right children. We show that this provides an injective mapping from trees to labels that can be encoded and decoded in linear time. We then define a 7-bit extension that represents an extra plane of arcs, extending the coverage to almost full non-projectivity (over 99.9% empirical arc coverage). Results on a set of diverse treebanks show that our 7-bit encoding obtains substantial accuracy gains over the previously best-performing sequence labeling encodings.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种编码方式，可以将任何投影依赖树转换为一个字符串中的4位标签，每个词的标签包含以下信息：（1）是右或左依赖关系，（2）是父节点的左或右外部依赖，（3）有左子节点，（4）有右子节点。我们证明了这是一个唯一映射，可以在线时间内编码和解码。我们还定义了一个7位扩展，表示一个额外的平面弧，使得覆盖率接近100%。在一组多样的树银行上测试的结果显示，我们的7位编码可以获得substantial的准确率提升， compared to之前最佳的序列标签编码。
</details></li>
</ul>
<hr>
<h2 id="Neural-Text-Sanitization-with-Privacy-Risk-Indicators-An-Empirical-Analysis"><a href="#Neural-Text-Sanitization-with-Privacy-Risk-Indicators-An-Empirical-Analysis" class="headerlink" title="Neural Text Sanitization with Privacy Risk Indicators: An Empirical Analysis"></a>Neural Text Sanitization with Privacy Risk Indicators: An Empirical Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14312">http://arxiv.org/abs/2310.14312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anthi Papadopoulou, Pierre Lison, Mark Anderson, Lilja Øvrelid, Ildikó Pilán</li>
<li>for: 本研究的目的是提出一种 двух步文本匿名化方法，并对两个最新发布的数据集进行实验分析：Text Anonymization Benchmark（Pil&#39;an et al., 2022）和一个基于Wikipedia的生ografies（Papadopoulou et al., 2022）。</li>
<li>methods: 本研究使用了一种权限允许的实体识别器，该识别器使用了标准命名实体识别模型和从Wikidata中提取的人员相关词汇进行训练。第二步是根据检测到的文本段进行风险评估，并使用语言模型概率、文本段分类、序列标签、扰动和网络搜索来评估隐私风险。</li>
<li>results: 本研究提供了五种不同的隐私风险指标，分别基于语言模型概率、文本段分类、序列标签、扰动和网络搜索。研究人员对每种风险指标进行了比较分析，并描述了它们的优点和局限性，特别是与可用的标注数据相关。<details>
<summary>Abstract</summary>
Text sanitization is the task of redacting a document to mask all occurrences of (direct or indirect) personal identifiers, with the goal of concealing the identity of the individual(s) referred in it. In this paper, we consider a two-step approach to text sanitization and provide a detailed analysis of its empirical performance on two recently published datasets: the Text Anonymization Benchmark (Pil\'an et al., 2022) and a collection of Wikipedia biographies (Papadopoulou et al., 2022). The text sanitization process starts with a privacy-oriented entity recognizer that seeks to determine the text spans expressing identifiable personal information. This privacy-oriented entity recognizer is trained by combining a standard named entity recognition model with a gazetteer populated by person-related terms extracted from Wikidata. The second step of the text sanitization process consists in assessing the privacy risk associated with each detected text span, either isolated or in combination with other text spans. We present five distinct indicators of the re-identification risk, respectively based on language model probabilities, text span classification, sequence labelling, perturbations, and web search. We provide a contrastive analysis of each privacy indicator and highlight their benefits and limitations, notably in relation to the available labeled data.
</details>
<details>
<summary>摘要</summary>
文本净化是将文本中的直接或间接个人标识符Mask all occurrences of (direct or indirect) personal identifiers in a document, with the goal of concealing the identity of the individual(s) referred in it. 在这篇论文中，我们考虑了一种两步方法 для实现文本净化，并对两个最近发布的数据集进行了详细的实验分析：Text Anonymization Benchmark（Pil\'an et al., 2022）和一个来自Wikipedia的biography集合（Papadopoulou et al., 2022）。文本净化过程从privacy-oriented实体识别器开始，该实体识别器通过将标准命名实体识别模型和 Wikidata中的人员相关词汇拼接而训练。第二步的文本净化过程是评估各检测到的文本块中的隐私风险，单独或与其他文本块组合。我们提出了五种不同的隐私指标，分别基于语言模型概率、文本块分类、序列标签、扰动和网络搜索。我们对每个隐私指标进行了对照分析，并指出了它们的优点和局限性，特别是与可用的标注数据相关。
</details></li>
</ul>
<hr>
<h2 id="Language-Model-Unalignment-Parametric-Red-Teaming-to-Expose-Hidden-Harms-and-Biases"><a href="#Language-Model-Unalignment-Parametric-Red-Teaming-to-Expose-Hidden-Harms-and-Biases" class="headerlink" title="Language Model Unalignment: Parametric Red-Teaming to Expose Hidden Harms and Biases"></a>Language Model Unalignment: Parametric Red-Teaming to Expose Hidden Harms and Biases</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14303">http://arxiv.org/abs/2310.14303</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rishabh Bhardwaj, Soujanya Poria</li>
<li>for: 这篇论文的目的是如何用不同的方法来评估大语言模型（LLMs）的危险性。</li>
<li>methods: 这篇论文使用了一种新的方法，即参数调整（parametric red-teaming）来评估 LLMs 的危险性。这种方法通过调整模型参数来绕过模型的安全行为，并且只需要使用 100 个示例。</li>
<li>results: 这篇论文的结果表明，使用参数调整方法可以很有效地绕过 CHATGPT 等模型的安全行为，并且可以在多种模型上实现高度的攻击成功率。此外，这种方法还可以暴露模型中隐藏的偏见和偏好。<details>
<summary>Abstract</summary>
Red-teaming has been a widely adopted way to evaluate the harmfulness of Large Language Models (LLMs). It aims to jailbreak a model's safety behavior to make it act as a helpful agent disregarding the harmfulness of the query. Existing methods are primarily based on input text-based red-teaming such as adversarial prompts, low-resource prompts, or contextualized prompts to condition the model in a way to bypass its safe behavior. Bypassing the guardrails uncovers hidden harmful information and biases in the model that are left untreated or newly introduced by its safety training. However, prompt-based attacks fail to provide such a diagnosis owing to their low attack success rate, and applicability to specific models. In this paper, we present a new perspective on LLM safety research i.e., parametric red-teaming through Unalignment. It simply (instruction) tunes the model parameters to break model guardrails that are not deeply rooted in the model's behavior. Unalignment using as few as 100 examples can significantly bypass commonly referred to as CHATGPT, to the point where it responds with an 88% success rate to harmful queries on two safety benchmark datasets. On open-source models such as VICUNA-7B and LLAMA-2-CHAT 7B AND 13B, it shows an attack success rate of more than 91%. On bias evaluations, Unalignment exposes inherent biases in safety-aligned models such as CHATGPT and LLAMA- 2-CHAT where the model's responses are strongly biased and opinionated 64% of the time.
</details>
<details>
<summary>摘要</summary>
红团队（red-teaming）已经广泛地应用于评估大语言模型（LLM）的危害性。它的目标是让模型免受安全限制，以便它可以根据危害性的查询行为。现有的方法主要基于输入文本基于的红团队，如敌对提示、低资源提示或Contextualized提示来 condition the model，以使其免受安全限制。免除安全限制可以暴露模型中隐藏的危害信息和偏见，但提示基于的攻击失败率较高，并且只适用于特定的模型。在这篇论文中，我们提出了一新的LLM安全研究视角，即参数红团队（Parametric red-teaming）。它通过调整模型参数来破坏模型的安全限制，而这些限制不深刻地关联到模型的行为。使用100个示例的不一致可以很好地绕过CHATGPT等常见的模型，并达到88%的攻击成功率。在开源模型上，如VICUNA-7B和LLAMA-2-CHAT 7B和13B，它的攻击成功率高于91%。在偏见评估中，不一致 expose了安全适配模型中的隐藏偏见，其中模型的回答有64%的时间具有强烈的偏见和意见性。
</details></li>
</ul>
<hr>
<h2 id="Conversational-Speech-Recognition-by-Learning-Audio-textual-Cross-modal-Contextual-Representation"><a href="#Conversational-Speech-Recognition-by-Learning-Audio-textual-Cross-modal-Contextual-Representation" class="headerlink" title="Conversational Speech Recognition by Learning Audio-textual Cross-modal Contextual Representation"></a>Conversational Speech Recognition by Learning Audio-textual Cross-modal Contextual Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14278">http://arxiv.org/abs/2310.14278</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kun Wei, Bei Li, Hang Lv, Quan Lu, Ning Jiang, Lei Xie</li>
<li>for: 提高 conversational ASR 系统的准确率和持续性，特别是在EXTRACTING RELEVANT CONTEXTUAL INFORMATION FROM PREVIOUS CONVERSATIONAL TURNS 中。</li>
<li>methods: 我们提出了一种新的 Conversational ASR 系统，基于 Conformer Encoder-Decoder 模型，并具有跨模态对话表示。我们的方法通过特殊的编码器和模式层输入将听说和文本模型结合在一起，从而EXTRACTING RICHER HISTORICAL SPEECH CONTEXT WITHOUT EXPLICIT ERROR PROPAGATION。我们还将conditional latent variational module  incorporated into the decoder to learn conversational level attributes such as role preference and topic coherence。</li>
<li>results: 我们的模型在 Mandarin conversation datasets HKUST 和 MagicData-RAMC 上实现了相对准确率提高8.8%和23%，compared to the standard Conformer model。<details>
<summary>Abstract</summary>
Automatic Speech Recognition (ASR) in conversational settings presents unique challenges, including extracting relevant contextual information from previous conversational turns. Due to irrelevant content, error propagation, and redundancy, existing methods struggle to extract longer and more effective contexts. To address this issue, we introduce a novel Conversational ASR system, extending the Conformer encoder-decoder model with cross-modal conversational representation. Our approach leverages a cross-modal extractor that combines pre-trained speech and text models through a specialized encoder and a modal-level mask input. This enables the extraction of richer historical speech context without explicit error propagation. We also incorporate conditional latent variational modules to learn conversational level attributes such as role preference and topic coherence. By introducing both cross-modal and conversational representations into the decoder, our model retains context over longer sentences without information loss, achieving relative accuracy improvements of 8.8% and 23% on Mandarin conversation datasets HKUST and MagicData-RAMC, respectively, compared to the standard Conformer model.
</details>
<details>
<summary>摘要</summary>
自动语音识别（ASR）在对话设置下存在独特的挑战，包括从前一些对话扩展有用的上下文信息。由于无关内容、错误卷积和重复，现有方法很难提取更长和有效的上下文。为解决这个问题，我们介绍了一种新的对话式ASR系统，扩展了Conformer编码器-解码器模型，并添加了跨模态对话表示。我们的方法利用一个跨模态提取器，将预训练的音频和文本模型通过特殊的编码器和模式层掩码输入结合。这使得更多的历史语音上下文可以无需显式错误卷积提取。我们还将条件潜在变量模块 integrate into the decoder，学习对话水平特征，如角色偏好和话题一致性。通过将跨模态和对话表示添加到解码器中，我们的模型可以保持长句子上下文不产生信息损失，实现相对准确率提高8.8%和23%在香港大学科技大学（HKUST）和魔法数据-RAMC（MagicData-RAMC）普通话数据集上，相比标准Conformer模型。
</details></li>
</ul>
<hr>
<h2 id="CT-GAT-Cross-Task-Generative-Adversarial-Attack-based-on-Transferability"><a href="#CT-GAT-Cross-Task-Generative-Adversarial-Attack-based-on-Transferability" class="headerlink" title="CT-GAT: Cross-Task Generative Adversarial Attack based on Transferability"></a>CT-GAT: Cross-Task Generative Adversarial Attack based on Transferability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14265">http://arxiv.org/abs/2310.14265</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiaoxuannlp/ct-gat">https://github.com/xiaoxuannlp/ct-gat</a></li>
<li>paper_authors: Minxuan Lv, Chengwei Dai, Kun Li, Wei Zhou, Songlin Hu</li>
<li>for: 防护神经网络模型免受敌意例子的攻击</li>
<li>methods: 直接使用多任务中的恶意示例提取可转移特征来构造敌意例子</li>
<li>results: 在十个不同的 dataset 上进行实验，结果表明我们的方法可以具有较好的攻击性能，并且可以采用小成本来实现。Here’s the full Chinese translation of the paper’s abstract:</li>
<li>for: 本文采用多任务中的恶意示例来防护神经网络模型免受敌意例子的攻击。</li>
<li>methods: 我们直接使用多任务中的恶意示例提取可转移特征来构造敌意例子。 Specifically, we train a sequence-to-sequence generative model named CT-GAT using adversarial sample data collected from multiple tasks to acquire universal adversarial features and generate adversarial examples for different tasks.</li>
<li>results: 我们在十个不同的 dataset 上进行实验，结果表明我们的方法可以具有较好的攻击性能，并且可以采用小成本来实现。<details>
<summary>Abstract</summary>
Neural network models are vulnerable to adversarial examples, and adversarial transferability further increases the risk of adversarial attacks. Current methods based on transferability often rely on substitute models, which can be impractical and costly in real-world scenarios due to the unavailability of training data and the victim model's structural details. In this paper, we propose a novel approach that directly constructs adversarial examples by extracting transferable features across various tasks. Our key insight is that adversarial transferability can extend across different tasks. Specifically, we train a sequence-to-sequence generative model named CT-GAT using adversarial sample data collected from multiple tasks to acquire universal adversarial features and generate adversarial examples for different tasks. We conduct experiments on ten distinct datasets, and the results demonstrate that our method achieves superior attack performance with small cost.
</details>
<details>
<summary>摘要</summary>
神经网络模型容易受到敌意示例的威胁，而受攻击性质的传播更会增加攻击风险。现有的基于传播的方法frequently rely on占位模型，这可能在实际应用中是不可预测的和成本高的。在这篇论文中，我们提出了一种新的方法，通过提取不同任务之间的传播特征来直接构建敌意示例。我们的关键发现是敌意传播可以跨任务扩展。我们使用多个任务的敌意样本来训练一个名为CT-GAT的序列到序列生成模型，以获得通用的敌意特征并生成不同任务的敌意示例。我们在十个不同的数据集上进行了实验，结果显示，我们的方法可以在小成本下实现高度的攻击性能。
</details></li>
</ul>
<hr>
<h2 id="Boosting-Unsupervised-Machine-Translation-with-Pseudo-Parallel-Data"><a href="#Boosting-Unsupervised-Machine-Translation-with-Pseudo-Parallel-Data" class="headerlink" title="Boosting Unsupervised Machine Translation with Pseudo-Parallel Data"></a>Boosting Unsupervised Machine Translation with Pseudo-Parallel Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14262">http://arxiv.org/abs/2310.14262</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ivana Kvapilíková, Ondřej Bojar</li>
<li>for: 提高低语言机器翻译系统的质量</li>
<li>methods: 使用 Pseudo-parallel sentence pairs 和 synthetic sentence pairs 进行训练</li>
<li>results: 与基线相比，提高翻译质量，最高提高14.5个 BLEU 点（英语到乌克兰语）<details>
<summary>Abstract</summary>
Even with the latest developments in deep learning and large-scale language modeling, the task of machine translation (MT) of low-resource languages remains a challenge. Neural MT systems can be trained in an unsupervised way without any translation resources but the quality lags behind, especially in truly low-resource conditions. We propose a training strategy that relies on pseudo-parallel sentence pairs mined from monolingual corpora in addition to synthetic sentence pairs back-translated from monolingual corpora. We experiment with different training schedules and reach an improvement of up to 14.5 BLEU points (English to Ukrainian) over a baseline trained on back-translated data only.
</details>
<details>
<summary>摘要</summary>
即使最新的深度学习和大规模语言模型技术发展，机器翻译（MT）低资源语言 task 仍然是一个挑战。我们提议一种培训策略，利用 Pseudo-parallel sentence pairs 和 artificial sentence pairs back-translated from monolingual corpora。我们在不同的培训时间表现出来的结果，可以达到14.5 BLEU点（英语到乌克兰）的提升。
</details></li>
</ul>
<hr>
<h2 id="From-Static-to-Dynamic-A-Continual-Learning-Framework-for-Large-Language-Models"><a href="#From-Static-to-Dynamic-A-Continual-Learning-Framework-for-Large-Language-Models" class="headerlink" title="From Static to Dynamic: A Continual Learning Framework for Large Language Models"></a>From Static to Dynamic: A Continual Learning Framework for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14248">http://arxiv.org/abs/2310.14248</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/elfsong/dynamind">https://github.com/elfsong/dynamind</a></li>
<li>paper_authors: Mingzhe Du, Anh Tuan Luu, Bin Ji, See-kiong Ng</li>
<li>for: 这篇论文是为了解决大型自然语言处理模型（LLMs）中的复杂性问题，以提高其在不同自然语言处理任务中的表现。</li>
<li>methods: 这篇论文提出了一个名为DynaMind的新的持续学习框架，旨在帮助LLMs继续学习并吸收新知识。DynaMind包括内存机制来吸收新知识，以及增强模型推论过程中的模块运算器，以提高LLMs的表现精度。</li>
<li>results: 根据比较 experiments，DynaMind可以有效地解决LLMs中的复杂性问题，并提高其表现精度。<details>
<summary>Abstract</summary>
The vast number of parameters in large language models (LLMs) endows them with remarkable capabilities, allowing them to excel in a variety of natural language processing tasks. However, this complexity also presents challenges, making LLMs difficult to train and inhibiting their ability to continuously assimilate new knowledge, which may lead to inaccuracies in their outputs. To mitigate these issues, this paper presents DynaMind, a novel continual learning framework designed for LLMs. DynaMind incorporates memory mechanisms to assimilate new knowledge and modular operators to enhance the model inference process with the newly assimilated knowledge, consequently improving the accuracies of LLMs' outputs. Benchmark experiments demonstrate DynaMind's effectiveness in overcoming these challenges. The code and demo of DynaMind are available on GitHub: https://github.com/Elfsong/DynaMind.
</details>
<details>
<summary>摘要</summary>
庞大的参数量在大型自然语言处理模型（LLM）中具有惊人的能力，使其在各种自然语言处理任务中表现出色。然而，这种复杂性也存在挑战，使LLM困难于训练，并限制其继续吸收新知识，可能导致其输出的不准确。为解决这些问题，本文提出了DynaMind，一种特有的连续学习框架，适用于LLM。DynaMind包括记忆机制，以吸收新知识，以及模块运算符，以提高LLM的输出准确性。实验示出DynaMind可以有效地解决这些问题。代码和示例可以在GitHub上找到：https://github.com/Elfsong/DynaMind。
</details></li>
</ul>
<hr>
<h2 id="PHD-Pixel-Based-Language-Modeling-of-Historical-Documents"><a href="#PHD-Pixel-Based-Language-Modeling-of-Historical-Documents" class="headerlink" title="PHD: Pixel-Based Language Modeling of Historical Documents"></a>PHD: Pixel-Based Language Modeling of Historical Documents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18343">http://arxiv.org/abs/2310.18343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nadav Borenstein, Phillip Rust, Desmond Elliott, Isabelle Augenstein</li>
<li>for: 这篇论文是为了探讨 историography 中文档案的数字化处理和自然语言处理方法。</li>
<li>methods: 该论文使用最新的像素基于语言模型，通过重建受遮盖的像素区域来替代传统的 OCR 技术。它还提出了一种新的合成档案生成方法，用于生成具有历史档案特点的 sintetic 档案。</li>
<li>results: 该论文通过实验证明，PHD 模型在重建受遮盖的像素区域方面具有高度的掌握能力，并在历史问答任务中得到了成功应用。<details>
<summary>Abstract</summary>
The digitisation of historical documents has provided historians with unprecedented research opportunities. Yet, the conventional approach to analysing historical documents involves converting them from images to text using OCR, a process that overlooks the potential benefits of treating them as images and introduces high levels of noise. To bridge this gap, we take advantage of recent advancements in pixel-based language models trained to reconstruct masked patches of pixels instead of predicting token distributions. Due to the scarcity of real historical scans, we propose a novel method for generating synthetic scans to resemble real historical documents. We then pre-train our model, PHD, on a combination of synthetic scans and real historical newspapers from the 1700-1900 period. Through our experiments, we demonstrate that PHD exhibits high proficiency in reconstructing masked image patches and provide evidence of our model's noteworthy language understanding capabilities. Notably, we successfully apply our model to a historical QA task, highlighting its usefulness in this domain.
</details>
<details>
<summary>摘要</summary>
digitization of historical documents 提供了历史学家无 precedent 的研究机会。然而，传统的历史文档分析方法是将图像转换为文本使用 OCR，这种方法忽略了图像的可能性并且引入了高水平的噪声。为了bridging这个差距，我们利用了最近的像素基本语言模型，用于重建掩码的图像 patches 而不是预测Token分布。由于历史材料的罕见性，我们提议一种新的方法生成Synthetic scans 来模拟真实的历史文档。我们然后预训练我们的模型PHD 使用组合的Synthetic scans 和真实的历史报纸从1700-1900年代。我们的实验表明PHD 在重建掩码的图像 patches 方面表现出了高度的能力，并且我们在历史QA任务中成功地应用了我们的模型，这亮出了它在这个领域的用于。
</details></li>
</ul>
<hr>
<h2 id="Customising-General-Large-Language-Models-for-Specialised-Emotion-Recognition-Tasks"><a href="#Customising-General-Large-Language-Models-for-Specialised-Emotion-Recognition-Tasks" class="headerlink" title="Customising General Large Language Models for Specialised Emotion Recognition Tasks"></a>Customising General Large Language Models for Specialised Emotion Recognition Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14225">http://arxiv.org/abs/2310.14225</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liyizhe Peng, Zixing Zhang, Tao Pang, Jing Han, Huan Zhao, Hao Chen, Björn W. Schuller</li>
<li>for: 这个论文主要是为了探讨大语言模型（LLMs）在情感识别任务中的性能和可行性。</li>
<li>methods: 这篇论文使用了两种不同的模态适应技术来改进Chat General Language Model（一个公共可用的大语言模型），即深度提示调整和低维度适应。</li>
<li>results: 实验结果表明，通过使用这两种技术改进的LLM可以轻松超越其他特有的深度模型，这表明LLM在情感识别任务中具有强大的传输性和可行性。<details>
<summary>Abstract</summary>
The advent of large language models (LLMs) has gained tremendous attention over the past year. Previous studies have shown the astonishing performance of LLMs not only in other tasks but also in emotion recognition in terms of accuracy, universality, explanation, robustness, few/zero-shot learning, and others. Leveraging the capability of LLMs inevitably becomes an essential solution for emotion recognition. To this end, we further comprehensively investigate how LLMs perform in linguistic emotion recognition if we concentrate on this specific task. Specifically, we exemplify a publicly available and widely used LLM -- Chat General Language Model, and customise it for our target by using two different modal adaptation techniques, i.e., deep prompt tuning and low-rank adaptation. The experimental results obtained on six widely used datasets present that the adapted LLM can easily outperform other state-of-the-art but specialised deep models. This indicates the strong transferability and feasibility of LLMs in the field of emotion recognition.
</details>
<details>
<summary>摘要</summary>
<<SYS>>大语言模型（LLM）的出现在过去一年内得到了很多关注。之前的研究表明，LLM在其他任务上的表现非常出众，以及在情感识别任务中的准确率、通用性、解释能力、鲁棒性、少/Zero-shot学习等方面的表现。利用LLM的能力变得是解决情感识别问题的必要手段。为此，我们进一步全面调查了LLM在语言情感识别任务中的表现。例如，我们使用了公共可用的和广泛使用的LLM——Chat General Language Model，并使用两种不同的模态适应技术，即深度推荐练化和低级适应。实验结果表明，适应后的LLM可以轻松击败其他特有的深度模型。这表明LLM在情感识别领域的传输性和可行性。
</details></li>
</ul>
<hr>
<h2 id="Manifold-Preserving-Transformers-are-Effective-for-Short-Long-Range-Encoding"><a href="#Manifold-Preserving-Transformers-are-Effective-for-Short-Long-Range-Encoding" class="headerlink" title="Manifold-Preserving Transformers are Effective for Short-Long Range Encoding"></a>Manifold-Preserving Transformers are Effective for Short-Long Range Encoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14206">http://arxiv.org/abs/2310.14206</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/victor7246/transject">https://github.com/victor7246/transject</a></li>
<li>paper_authors: Ayan Sengupta, Md Shad Akhtar, Tanmoy Chakraborty</li>
<li>for: 本研究旨在提高Transformer模型的表达能力，特别是保持层次结构信息。</li>
<li>methods: 本文提出了一种名为TransJect的encoder模型，该模型通过保证层次距离 preserved来提高表达能力。具体来说，TransJect使用了一种简单的替代方案来确保点积分注意力，从而保证了Liψchitz连续性。</li>
<li>results: 在多个短和长序列分类任务上，TransJect比Transformer的variantshow了最大提升6.8%和5.9%。此外，TransJect在语言模型任务上表现出79%的提升。此外，本文还探讨了多头自注意的缺陷从统计物理角度。<details>
<summary>Abstract</summary>
Multi-head self-attention-based Transformers have shown promise in different learning tasks. Albeit these models exhibit significant improvement in understanding short-term and long-term contexts from sequences, encoders of Transformers and their variants fail to preserve layer-wise contextual information. Transformers usually project tokens onto sparse manifolds and fail to preserve mathematical equivalence among the token representations. In this work, we propose TransJect, an encoder model that guarantees a theoretical bound for layer-wise distance preservation between a pair of tokens. We propose a simple alternative to dot-product attention to ensure Lipschitz continuity. This allows TransJect to learn injective mappings to transform token representations to different manifolds with similar topology and preserve Euclidean distance between every pair of tokens in subsequent layers. Evaluations across multiple benchmark short- and long-sequence classification tasks show maximum improvements of 6.8% and 5.9%, respectively, over the variants of Transformers. Additionally, TransJect displays 79% better performance than Transformer on the language modeling task. We further highlight the shortcomings of multi-head self-attention from the statistical physics viewpoint. Although multi-head self-attention was incepted to learn different abstraction levels within the networks, our empirical analyses suggest that different attention heads learn randomly and unorderly. In contrast, TransJect adapts a mixture of experts for regularization; these experts are more orderly and balanced and learn different sparse representations from the input sequences. TransJect exhibits very low entropy and can be efficiently scaled to larger depths.
</details>
<details>
<summary>摘要</summary>
多头自注意型Transformer显示了不同学习任务中的搭配性。虽然这些模型在序列中的短期和长期上下文理解方面表现出了显著改进，但Transformer和其变种的Encoder却无法保持层次上的上下文信息。Transformer通常将token映射到稀疏拟合和失去数学相等性 among token representations。在这种情况下，我们提出了TransJect，一种Encoder模型，可以保证层次上的距离保持。我们提出了一种简单的替代品dot-product注意，以确保Lipschitz连续性。这使得TransJect可以学习将token表示变换到不同的拟合上，保持后续层次上的Euclidean距离between every pair of tokens。多个benchmark短序列和长序列分类任务上的评估显示，TransJect可以与Transformer变种的最大改进6.8%和5.9%。此外，TransJect在语言模型任务上表现出79%的提高。我们还从统计物理角度探讨了多头自注意的缺陷。虽然多头自注意是为了在网络中学习不同层次的抽象，但我们的实际分析表明，不同的注意头会随机和无序地学习。相比之下，TransJect采用了一种精心混合的专家，这些专家更加有序和平衡，从输入序列中学习不同的稀疏表示。TransJect具有很低的 entropy和可以高效扩展到更大的深度。
</details></li>
</ul>
<hr>
<h2 id="QA-NatVer-Question-Answering-for-Natural-Logic-based-Fact-Verification"><a href="#QA-NatVer-Question-Answering-for-Natural-Logic-based-Fact-Verification" class="headerlink" title="QA-NatVer: Question Answering for Natural Logic-based Fact Verification"></a>QA-NatVer: Question Answering for Natural Logic-based Fact Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14198">http://arxiv.org/abs/2310.14198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rami Aly, Marek Strong, Andreas Vlachos</li>
<li>for: 评估声明真实性基于证据， faithfulness 是一个重要考虑因素，即生成可信的解释。</li>
<li>methods: 使用问答系统预测自然逻辑运算符，利用指导语言模型的泛化能力，无需训练数据。</li>
<li>results: 在 FEVER 几个shot Setting 中，我们的方法比最佳基eline提高了4.3个准确性点，包括一个预训练 seq2seq 自然逻辑系统和一个预训练提问基类ifier。我们的系统在对比Fact datasets 中显示出了稳定性和可重用性，并在没有进一步注释的情况下超越了所有其他方法。人工评估表明，我们的方法生成的证据更加可能且少量错误的自然逻辑运算符。<details>
<summary>Abstract</summary>
Fact verification systems assess a claim's veracity based on evidence. An important consideration in designing them is faithfulness, i.e. generating explanations that accurately reflect the reasoning of the model. Recent works have focused on natural logic, which operates directly on natural language by capturing the semantic relation of spans between an aligned claim with its evidence via set-theoretic operators. However, these approaches rely on substantial resources for training, which are only available for high-resource languages. To this end, we propose to use question answering to predict natural logic operators, taking advantage of the generalization capabilities of instruction-tuned language models. Thus, we obviate the need for annotated training data while still relying on a deterministic inference system. In a few-shot setting on FEVER, our approach outperforms the best baseline by $4.3$ accuracy points, including a state-of-the-art pre-trained seq2seq natural logic system, as well as a state-of-the-art prompt-based classifier. Our system demonstrates its robustness and portability, achieving competitive performance on a counterfactual dataset and surpassing all approaches without further annotation on a Danish verification dataset. A human evaluation indicates that our approach produces more plausible proofs with fewer erroneous natural logic operators than previous natural logic-based systems.
</details>
<details>
<summary>摘要</summary>
фак verify 系统 评估一个说法的真实性基于证据。 设计这些系统时，一个重要考虑因素是忠诚度，即生成的解释能够准确反映模型的逻辑。  current works 将关注自然逻辑，它直接在自然语言上运行，通过 captured  span 之间的semantic relation 和 claims 的Alignment来进行操作。  however, these approaches rely on a large amount of training data, which is only available for high-resource languages.To address this challenge, we propose using question answering to predict natural logic operators, taking advantage of the generalization capabilities of instruction-tuned language models. This approach eliminates the need for annotated training data and relies on a deterministic inference system. In a few-shot setting on FEVER, our approach outperforms the best baseline by 4.3 accuracy points, including a state-of-the-art pre-trained seq2seq natural logic system and a state-of-the-art prompt-based classifier. Our system demonstrates its robustness and portability, achieving competitive performance on a counterfactual dataset and surpassing all approaches without further annotation on a Danish verification dataset. A human evaluation indicates that our approach produces more plausible proofs with fewer erroneous natural logic operators than previous natural logic-based systems.
</details></li>
</ul>
<hr>
<h2 id="An-In-Context-Schema-Understanding-Method-for-Knowledge-Base-Question-Answering"><a href="#An-In-Context-Schema-Understanding-Method-for-Knowledge-Base-Question-Answering" class="headerlink" title="An In-Context Schema Understanding Method for Knowledge Base Question Answering"></a>An In-Context Schema Understanding Method for Knowledge Base Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14174">http://arxiv.org/abs/2310.14174</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yantao Liu, Zixuan Li, Xiaolong Jin, Long Bai, Saiping Guan, Jiafeng Guo, Xueqi Cheng</li>
<li>for: 本研究旨在提高大语言模型在知识基础中问答任务中的表现，具体来说是通过增强大语言模型对知识库的schema理解来提高其作为semantic parser的能力。</li>
<li>methods: 本研究提出了一种叫做In-Context Schema Understanding（ICSU）的方法，该方法利用了在context学习机制，通过提供例子来指导大语言模型生成SPARQL查询。为了从注释化的问题-查询对中检索合适的例子，ICSU采用了四种不同的检索策略。</li>
<li>results: 实验结果表明，ICSU与所有的检索策略都可以与随机检索策略相比，显著提高了准确率（从12%提高到78.76%）。<details>
<summary>Abstract</summary>
The Knowledge Base Question Answering (KBQA) task aims to answer natural language questions based on a given knowledge base. As a kind of common method for this task, semantic parsing-based ones first convert natural language questions to logical forms (e.g., SPARQL queries) and then execute them on knowledge bases to get answers. Recently, Large Language Models (LLMs) have shown strong abilities in language understanding and may be adopted as semantic parsers in such kinds of methods. However, in doing so, a great challenge for LLMs is to understand the schema of knowledge bases. Therefore, in this paper, we propose an In-Context Schema Understanding (ICSU) method for facilitating LLMs to be used as a semantic parser in KBQA. Specifically, ICSU adopts the In-context Learning mechanism to instruct LLMs to generate SPARQL queries with examples. In order to retrieve appropriate examples from annotated question-query pairs, which contain comprehensive schema information related to questions, ICSU explores four different retrieval strategies. Experimental results on the largest KBQA benchmark, KQA Pro, show that ICSU with all these strategies outperforms that with a random retrieval strategy significantly (from 12\% to 78.76\% in accuracy).
</details>
<details>
<summary>摘要</summary>
《知识库问答（KBQA）任务的目标是根据给定的知识库回答自然语言问题。现有一种常见的方法是将自然语言问题转化为逻辑形式（例如 SPARQL 查询），然后执行在知识库中以获取答案。最近，大型自然语言模型（LLM）在语言理解方面表现出色，因此可能被采用为 semantic parser 在这些方法中。然而，在这种情况下，LLM 的一大挑战是理解知识库的结构。因此，在这篇论文中，我们提出了一种在Context Schema Understanding（ICSU）方法，用于使 LLM 在 KBQA 中作为semantic parser。具体来说，ICSU 采用了在 Context 学习机制，以示 LLM 生成 SPARQL 查询的示例。为了从 annotated question-query 对中检索相关的 schema 信息，ICSU 探索了四种不同的检索策略。实验结果表明，ICSU 与所有这些策略相比，在 KQA Pro 最大知识库问答 benchmark 上表现出色，具体来说，ICSU 的准确率从 12% 提高到 78.76%。
</details></li>
</ul>
<hr>
<h2 id="Can-Language-Models-Laugh-at-YouTube-Short-form-Videos"><a href="#Can-Language-Models-Laugh-at-YouTube-Short-form-Videos" class="headerlink" title="Can Language Models Laugh at YouTube Short-form Videos?"></a>Can Language Models Laugh at YouTube Short-form Videos?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14159">http://arxiv.org/abs/2310.14159</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dayoon-ko/exfuntube">https://github.com/dayoon-ko/exfuntube</a></li>
<li>paper_authors: Dayoon Ko, Sangho Lee, Gunhee Kim</li>
<li>For: 本研究targets at developing a dataset and a prompting method to improve large language models’ (LLMs) understanding of humorous videos on social media.* Methods: 研究使用了YouTube上的用户生成的10000个多Modal funny videos，通过视频过滤管道和GPT-3.5进行验证，并为每个视频添加时间戳和文本解释。* Results: 研究表明，使用zero-shot video-to-text prompting可以有效提高LLMs对视频幽默的理解，并通过三种评估方法（自动分数、理由质量实验和人工评价）得到了证明。<details>
<summary>Abstract</summary>
As short-form funny videos on social networks are gaining popularity, it becomes demanding for AI models to understand them for better communication with humans. Unfortunately, previous video humor datasets target specific domains, such as speeches or sitcoms, and mostly focus on verbal cues. We curate a user-generated dataset of 10K multimodal funny videos from YouTube, called ExFunTube. Using a video filtering pipeline with GPT-3.5, we verify both verbal and visual elements contributing to humor. After filtering, we annotate each video with timestamps and text explanations for funny moments. Our ExFunTube is unique over existing datasets in that our videos cover a wide range of domains with various types of humor that necessitate a multimodal understanding of the content. Also, we develop a zero-shot video-to-text prompting to maximize video humor understanding of large language models (LLMs). With three different evaluation methods using automatic scores, rationale quality experiments, and human evaluations, we show that our prompting significantly improves LLMs' ability for humor explanation.
</details>
<details>
<summary>摘要</summary>
As short-form funny videos on social networks become increasingly popular, it is becoming more important for AI models to understand them in order to communicate with humans more effectively. However, previous video humor datasets have focused on specific domains such as speeches or sitcoms, and have primarily targeted verbal cues. We have curated a dataset of 10,000 multimodal funny videos from YouTube, called ExFunTube, which includes both visual and verbal elements that contribute to humor. We use a video filtering pipeline with GPT-3.5 to verify the humor in each video, and then annotate each video with timestamps and text explanations for the funny moments. Our ExFunTube dataset is unique compared to existing datasets, as it covers a wide range of domains with various types of humor that require a multimodal understanding of the content. Additionally, we have developed a zero-shot video-to-text prompting method to improve the ability of large language models (LLMs) to understand humor. We evaluate our prompting method using three different methods, including automatic scores, rationale quality experiments, and human evaluations, and show that it significantly improves the ability of LLMs to explain humor.
</details></li>
</ul>
<hr>
<h2 id="Orthogonal-Subspace-Learning-for-Language-Model-Continual-Learning"><a href="#Orthogonal-Subspace-Learning-for-Language-Model-Continual-Learning" class="headerlink" title="Orthogonal Subspace Learning for Language Model Continual Learning"></a>Orthogonal Subspace Learning for Language Model Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14152">http://arxiv.org/abs/2310.14152</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao Wang, Tianze Chen, Qiming Ge, Han Xia, Rong Bao, Rui Zheng, Qi Zhang, Tao Gui, Xuanjing Huang</li>
<li>for: 这篇论文旨在解决语言模型在进行多任务时的慢性忘记问题。</li>
<li>methods: 本文提出了一种简单有效的方法，即阶层低维适应（O-LoRA），可以有效地减少语言模型在进行新任务时的慢性忘记。O-LoRA 在不同的低维vector空间中学习任务，以避免任务之间的干扰。</li>
<li>results: 实验结果显示， compared to现有方法，O-LoRA 能够更好地保持语言模型对未见任务的普遍能力。 In addition, O-LoRA 只需要额外增加一些参数成本，并且不需要用户数据储存 для重新读取。<details>
<summary>Abstract</summary>
Benefiting from massive corpora and advanced hardware, large language models (LLMs) exhibit remarkable capabilities in language understanding and generation. However, their performance degrades in scenarios where multiple tasks are encountered sequentially, also known as catastrophic forgetting. In this paper, we propose orthogonal low-rank adaptation (O-LoRA), a simple and efficient approach for continual learning in language models, effectively mitigating catastrophic forgetting while learning new tasks. Specifically, O-LoRA learns tasks in different (low-rank) vector subspaces that are kept orthogonal to each other in order to minimize interference. Our method induces only marginal additional parameter costs and requires no user data storage for replay. Experimental results on continual learning benchmarks show that our method outperforms state-of-the-art methods. Furthermore, compared to previous approaches, our method excels in preserving the generalization ability of LLMs on unseen tasks.
</details>
<details>
<summary>摘要</summary>
LLMs 因为巨大的词汇和高级硬件的支持，在语言理解和生成方面表现出了惊人的能力。然而，在紧随着多个任务的场景下，它们的表现却会出现悬崖式忘记。在这篇论文中，我们提出了低维 adaptation（O-LoRA），一种简单高效的方法，用于语言模型的连续学习，以避免悬崖式忘记。具体来说，O-LoRA 在不同的低维向量空间中学习不同任务，以避免干扰。我们的方法增加了非常少的参数成本，并不需要用户存储数据进行回放。实验结果表明，我们的方法在连续学习测试 benchmark 上表现出色，并且比之前的方法更好地保留 LLMs 对未seen 任务的总结能力。
</details></li>
</ul>
<hr>
<h2 id="PromptCBLUE-A-Chinese-Prompt-Tuning-Benchmark-for-the-Medical-Domain"><a href="#PromptCBLUE-A-Chinese-Prompt-Tuning-Benchmark-for-the-Medical-Domain" class="headerlink" title="PromptCBLUE: A Chinese Prompt Tuning Benchmark for the Medical Domain"></a>PromptCBLUE: A Chinese Prompt Tuning Benchmark for the Medical Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14151">http://arxiv.org/abs/2310.14151</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/michael-wzhu/PromptCBLUE">https://github.com/michael-wzhu/PromptCBLUE</a></li>
<li>paper_authors: Wei Zhu, Xiaoling Wang, Huanran Zheng, Mosha Chen, Buzhou Tang</li>
<li>For:	+ The paper aims to evaluate Chinese language models (LLMs) for multi-task capabilities on a wide range of bio-medical tasks.* Methods:	+ The authors re-build the Chinese Biomedical Language Understanding Evaluation (CBLUE) benchmark into a large-scale prompt-tuning benchmark, called PromptCBLUE.	+ The benchmark is designed to evaluate Chinese LLMs’ performance on medical entity recognition, medical text classification, medical natural language inference, medical dialogue understanding, and medical content&#x2F;dialogue generation.* Results:	+ The authors experiment with fine-tuning 9 Chinese LLMs with different techniques and report the results.<details>
<summary>Abstract</summary>
Biomedical language understanding benchmarks are the driving forces for artificial intelligence applications with large language model (LLM) back-ends. However, most current benchmarks: (a) are limited to English which makes it challenging to replicate many of the successes in English for other languages, or (b) focus on knowledge probing of LLMs and neglect to evaluate how LLMs apply these knowledge to perform on a wide range of bio-medical tasks, or (c) have become a publicly available corpus and are leaked to LLMs during pre-training. To facilitate the research in medical LLMs, we re-build the Chinese Biomedical Language Understanding Evaluation (CBLUE) benchmark into a large scale prompt-tuning benchmark, PromptCBLUE. Our benchmark is a suitable test-bed and an online platform for evaluating Chinese LLMs' multi-task capabilities on a wide range bio-medical tasks including medical entity recognition, medical text classification, medical natural language inference, medical dialogue understanding and medical content/dialogue generation. To establish evaluation on these tasks, we have experimented and report the results with the current 9 Chinese LLMs fine-tuned with differtent fine-tuning techniques.
</details>
<details>
<summary>摘要</summary>
生物医学语言理解指标是人工智能应用中的推动力，但现有的大多数指标有以下限制：（a）仅限于英语，使得其他语言的复制困难，或（b）主要关注语言模型的知识探测，忽略语言模型在各种生物医学任务上的应用，或（c）已经公开化并泄露给语言模型 durante pre-training。为促进医学语言模型的研究，我们将中文生物医学语言理解评估 benchmark（CBLUE）重新建立为大规模的提示调整 benchmark，即 PromptCBLUE。我们的 benchmark 是一个适用的测试床和在线平台，用于评估中文语言模型在各种生物医学任务上的多任务能力，包括医学实体识别、医学文本分类、医学自然语言推理、医学对话理解和医学内容/对话生成。为了建立这些任务的评估，我们在现有的9种中文语言模型中进行了不同的细化技术的实验，并发布了结果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/22/cs.CL_2023_10_22/" data-id="clp88dbu500dyob88d5vxgbxd" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_10_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/22/cs.LG_2023_10_22/" class="article-date">
  <time datetime="2023-10-22T10:00:00.000Z" itemprop="datePublished">2023-10-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/22/cs.LG_2023_10_22/">cs.LG - 2023-10-22</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Diffusion-Model-Assisted-Supervised-Learning-of-Generative-Models-for-Density-Estimation"><a href="#Diffusion-Model-Assisted-Supervised-Learning-of-Generative-Models-for-Density-Estimation" class="headerlink" title="Diffusion-Model-Assisted Supervised Learning of Generative Models for Density Estimation"></a>Diffusion-Model-Assisted Supervised Learning of Generative Models for Density Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14458">http://arxiv.org/abs/2310.14458</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanfang Liu, Minglei Yang, Zezhong Zhang, Feng Bao, Yanzhao Cao, Guannan Zhang</li>
<li>for: 用于训练生成模型进行密度估计。</li>
<li>methods: 使用分布型抽象模型，包括生成对抗网络、正规化流和自适应神经网络，并使用分布型抽象模型来生成标注数据。</li>
<li>results: 提高了生成模型的训练效率和准确性，不需要使用可逆神经网络和计算Jacobian矩阵。<details>
<summary>Abstract</summary>
We present a supervised learning framework of training generative models for density estimation. Generative models, including generative adversarial networks, normalizing flows, variational auto-encoders, are usually considered as unsupervised learning models, because labeled data are usually unavailable for training. Despite the success of the generative models, there are several issues with the unsupervised training, e.g., requirement of reversible architectures, vanishing gradients, and training instability. To enable supervised learning in generative models, we utilize the score-based diffusion model to generate labeled data. Unlike existing diffusion models that train neural networks to learn the score function, we develop a training-free score estimation method. This approach uses mini-batch-based Monte Carlo estimators to directly approximate the score function at any spatial-temporal location in solving an ordinary differential equation (ODE), corresponding to the reverse-time stochastic differential equation (SDE). This approach can offer both high accuracy and substantial time savings in neural network training. Once the labeled data are generated, we can train a simple fully connected neural network to learn the generative model in the supervised manner. Compared with existing normalizing flow models, our method does not require to use reversible neural networks and avoids the computation of the Jacobian matrix. Compared with existing diffusion models, our method does not need to solve the reverse-time SDE to generate new samples. As a result, the sampling efficiency is significantly improved. We demonstrate the performance of our method by applying it to a set of 2D datasets as well as real data from the UCI repository.
</details>
<details>
<summary>摘要</summary>
我们提出了一个监督式学习框架，用于对密度估计进行生成模型训练。生成模型，包括生成对抗网络、标准化对抗网络和条件 autoencoder，通常被视为无监督式学习模型，因为训练时通常没有标签的资料。 despite the success of the generative models, there are several issues with the unsupervised training, such as the need for reversible architectures, vanishing gradients, and training instability. To enable supervised learning in generative models, we utilize the score-based diffusion model to generate labeled data. Unlike existing diffusion models that train neural networks to learn the score function, we develop a training-free score estimation method. This approach uses mini-batch-based Monte Carlo estimators to directly approximate the score function at any spatial-temporal location in solving an ordinary differential equation (ODE), corresponding to the reverse-time stochastic differential equation (SDE). This approach can offer both high accuracy and substantial time savings in neural network training. Once the labeled data are generated, we can train a simple fully connected neural network to learn the generative model in a supervised manner. Compared with existing normalizing flow models, our method does not require the use of reversible neural networks and avoids the computation of the Jacobian matrix. Compared with existing diffusion models, our method does not need to solve the reverse-time SDE to generate new samples. As a result, the sampling efficiency is significantly improved. We demonstrate the performance of our method by applying it to a set of 2D datasets as well as real data from the UCI repository.
</details></li>
</ul>
<hr>
<h2 id="URegM-a-unified-prediction-model-of-resource-consumption-for-refactoring-software-smells-in-open-source-cloud"><a href="#URegM-a-unified-prediction-model-of-resource-consumption-for-refactoring-software-smells-in-open-source-cloud" class="headerlink" title="URegM: a unified prediction model of resource consumption for refactoring software smells in open source cloud"></a>URegM: a unified prediction model of resource consumption for refactoring software smells in open source cloud</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14444">http://arxiv.org/abs/2310.14444</a></li>
<li>repo_url: None</li>
<li>paper_authors: Asif Imran, Tevfik Kosar</li>
<li>for: 这篇论文是为了提高云计算内部过程资源利用率而写的。</li>
<li>methods: 这篇论文使用了一种名为“统一回归模型”（URegM）来预测代码异臭重构后对云资源的使用。</li>
<li>results: 实验结果表明，URegM可以准确预测代码异臭重构后对云资源的使用。这将帮助云服务提供商更好地规划资源分配和代码重构。<details>
<summary>Abstract</summary>
The low cost and rapid provisioning capabilities have made the cloud a desirable platform to launch complex scientific applications. However, resource utilization optimization is a significant challenge for cloud service providers, since the earlier focus is provided on optimizing resources for the applications that run on the cloud, with a low emphasis being provided on optimizing resource utilization of the cloud computing internal processes. Code refactoring has been associated with improving the maintenance and understanding of software code. However, analyzing the impact of the refactoring source code of the cloud and studying its impact on cloud resource usage require further analysis. In this paper, we propose a framework called Unified Regression Modelling (URegM) which predicts the impact of code smell refactoring on cloud resource usage. We test our experiments in a real-life cloud environment using a complex scientific application as a workload. Results show that URegM is capable of accurately predicting resource consumption due to code smell refactoring. This will permit cloud service providers with advanced knowledge about the impact of refactoring code smells on resource consumption, thus allowing them to plan their resource provisioning and code refactoring more effectively.
</details>
<details>
<summary>摘要</summary>
“低成本和快速提供 capacities 使云平台成为 Complex scientific applications 的吸引力。然而，云服务提供商需要优化资源使其能够更好地使用资源，因为在云计算内部过程中的资源使用不受优化。Code refactoring 有助于改善软件代码的维护和理解。然而，云环境中 refactoring 代码的影响需要进一步的分析。本文提出一个名为 Unified Regression Modelling (URegM) 的框架，可以预测代码臭味 refactoring 对云资源的使用情况。我们在一个真实的云环境中进行实验，使用一个复杂的科学应用作为负载。结果表明，URegM 可以准确预测代码臭味 refactoring 对云资源的使用情况。这将允许云服务提供商在代码 refactoring 和资源分配方面更加有效地规划。”
</details></li>
</ul>
<hr>
<h2 id="EDGE-Improved-Training-and-Sampling-of-EDGE"><a href="#EDGE-Improved-Training-and-Sampling-of-EDGE" class="headerlink" title="EDGE++: Improved Training and Sampling of EDGE"></a>EDGE++: Improved Training and Sampling of EDGE</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14441">http://arxiv.org/abs/2310.14441</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning">https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning</a></li>
<li>paper_authors: Mingyang Wu, Xiaohui Chen, Li-Ping Liu</li>
<li>For: 本研究目的是改进现有的Diffusion-based方法，以提高大型网络生成的效率和生成质量。* Methods: 本文提出了对EDGE模型的两个改进：一是根据度Specific Noise Schedule优化活动节点数量，从而减少内存消耗；二是提出了一种改进的采样方案，以更好地控制生成过程中的相似性。* Results: 实验结果表明，提出的修改不仅提高了生成效率，还提高了生成的图像质量，为大型网络生成任务提供了一个可靠和扩展的解决方案。<details>
<summary>Abstract</summary>
Recently developed deep neural models like NetGAN, CELL, and Variational Graph Autoencoders have made progress but face limitations in replicating key graph statistics on generating large graphs. Diffusion-based methods have emerged as promising alternatives, however, most of them present challenges in computational efficiency and generative performance. EDGE is effective at modeling large networks, but its current denoising approach can be inefficient, often leading to wasted computational resources and potential mismatches in its generation process. In this paper, we propose enhancements to the EDGE model to address these issues. Specifically, we introduce a degree-specific noise schedule that optimizes the number of active nodes at each timestep, significantly reducing memory consumption. Additionally, we present an improved sampling scheme that fine-tunes the generative process, allowing for better control over the similarity between the synthesized and the true network. Our experimental results demonstrate that the proposed modifications not only improve the efficiency but also enhance the accuracy of the generated graphs, offering a robust and scalable solution for graph generation tasks.
</details>
<details>
<summary>摘要</summary>
近期发展的深度神经网络模型如NetGAN、CELL和Variational Graph Autoencoders等已经做出了进步，但是它们在生成大图时面临限制，Diffusion-based方法也在潜在的替代者中出现，但是大多数其中的计算效率和生成性能存在挑战。EDGE模型可以模型大网络，但是其当前的净化方法可能会导致计算资源浪费和生成过程中的匹配问题。在这篇论文中，我们提出了对EDGE模型的改进，包括度量特定的噪声调度，以优化每个时间步中活动节点的数量，显著减少内存占用。此外，我们还提出了改进的采样方案，可以细化生成过程，以更好地控制生成的图和真实图之间的相似性。我们的实验结果表明，我们的修改不仅提高了效率，还提高了生成的图的准确性，提供了一个可靠和扩展的图生成解决方案。
</details></li>
</ul>
<hr>
<h2 id="Fairness-aware-Optimal-Graph-Filter-Design"><a href="#Fairness-aware-Optimal-Graph-Filter-Design" class="headerlink" title="Fairness-aware Optimal Graph Filter Design"></a>Fairness-aware Optimal Graph Filter Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14432">http://arxiv.org/abs/2310.14432</a></li>
<li>repo_url: None</li>
<li>paper_authors: O. Deniz Kose, Yanning Shen, Gonzalo Mateos</li>
<li>for: 本文针对 graph-based learning 中存在的偏见问题进行研究，并提出了一种基于 graph signal processing 的偏见 Mitigation 方法。</li>
<li>methods: 本文使用了 graph filters 来减少 sensitive attributes 和 graph 结构之间的相关性，并通过 convex 问题在 graph спектраль领域中设计了最佳的 filter 设计。</li>
<li>results: 实验表明，提出的方法可以提高 fairness 度并保持相同的 utility，与现有的 fairness-aware 基线方法相比。<details>
<summary>Abstract</summary>
Graphs are mathematical tools that can be used to represent complex real-world interconnected systems, such as financial markets and social networks. Hence, machine learning (ML) over graphs has attracted significant attention recently. However, it has been demonstrated that ML over graphs amplifies the already existing bias towards certain under-represented groups in various decision-making problems due to the information aggregation over biased graph structures. Faced with this challenge, here we take a fresh look at the problem of bias mitigation in graph-based learning by borrowing insights from graph signal processing. Our idea is to introduce predesigned graph filters within an ML pipeline to reduce a novel unsupervised bias measure, namely the correlation between sensitive attributes and the underlying graph connectivity. We show that the optimal design of said filters can be cast as a convex problem in the graph spectral domain. We also formulate a linear programming (LP) problem informed by a theoretical bias analysis, which attains a closed-form solution and leads to a more efficient fairness-aware graph filter. Finally, for a design whose degrees of freedom are independent of the input graph size, we minimize the bias metric over the family of polynomial graph convolutional filters. Our optimal filter designs offer complementary strengths to explore favorable fairness-utility-complexity tradeoffs. For performance evaluation, we conduct extensive and reproducible node classification experiments over real-world networks. Our results show that the proposed framework leads to better fairness measures together with similar utility compared to state-of-the-art fairness-aware baselines.
</details>
<details>
<summary>摘要</summary>
图表是数学工具，可以用来表示复杂的现实世界中的连接系统，如金融市场和社交网络。因此，机器学习（ML）在图表上的应用吸引了广泛的关注。然而，已经证明了ML在图表上会增强现有的偏见，导致各种决策问题中的偏见倾向某些被排除的群体。面临这个挑战，我们借鉴了图像处理的思想，引入了预设计图 filters 来减少敏感特征和图连接性之间的相关性。我们示示了这些筛选器的优化设计可以转化为一个对准的问题，并且可以通过LP问题来减少偏见。最后，我们为独立于输入图表大小的设计，对家族中的多项式图 convolutional filters 进行了最小化偏见度量。我们的优化筛选器设计提供了不同的优势，可以根据偏见、 utility 和复杂度进行质量评估。通过广泛和可重复的节点分类实验，我们的结果表明，我们的框架可以同时实现更好的偏见度量和相似的实用性。
</details></li>
</ul>
<hr>
<h2 id="Clustering-Students-Based-on-Gamification-User-Types-and-Learning-Styles"><a href="#Clustering-Students-Based-on-Gamification-User-Types-and-Learning-Styles" class="headerlink" title="Clustering Students Based on Gamification User Types and Learning Styles"></a>Clustering Students Based on Gamification User Types and Learning Styles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14430">http://arxiv.org/abs/2310.14430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emre Arslan, Atilla Özkaymak, Nesrin Özdener Dönmez</li>
<li>for:  clustering students according to their gamification user types and learning styles</li>
<li>methods: K-means algorithm and Gamification User Type Hexad Scale, Grasha-Riechmann Student Learning Style Scale</li>
<li>results: neutral results with a Silhouette coefficient of 0.12, indicating that the clustering is not satisfactory.<details>
<summary>Abstract</summary>
The aim of this study is clustering students according to their gamification user types and learning styles with the purpose of providing instructors with a new perspective of grouping students in case of clustering which cannot be done by hand when there are multiple scales in data. The data used consists of 251 students who were enrolled at a Turkish state university. When grouping students, K-means algorithm has been utilized as clustering algorithm. As for determining the gamification user types and learning styles of students, Gamification User Type Hexad Scale and Grasha-Riechmann Student Learning Style Scale have been used respectively. Silhouette coefficient is utilized as clustering quality measure. After fitting the algorithm in several ways, highest Silhouette coefficient obtained was 0.12 meaning that results are neutral but not satisfactory. All the statistical operations and data visualizations were made using Python programming language.
</details>
<details>
<summary>摘要</summary>
本研究的目的是根据学生的游戏化用户类型和学习风格进行分群，以提供教师一种新的分组学生的方法，这种方法不可以由手动进行分组，当数据具有多个级别时。该研究使用了251名在土耳其国立大学学习的学生的数据。在分群学生时，使用了K-means算法。为确定学生的游戏化用户类型和学习风格，使用了游戏用户类型六元排序和格拉沙-瑞希曼学生学习风格分型。使用了Silhouette系数作为分群质量度量。经过多种适应，最高的Silhouette系数为0.12，表示结果为中度可行，不够满意。所有统计计算和数据可视化都使用了Python编程语言。
</details></li>
</ul>
<hr>
<h2 id="A-Quadratic-Synchronization-Rule-for-Distributed-Deep-Learning"><a href="#A-Quadratic-Synchronization-Rule-for-Distributed-Deep-Learning" class="headerlink" title="A Quadratic Synchronization Rule for Distributed Deep Learning"></a>A Quadratic Synchronization Rule for Distributed Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14423">http://arxiv.org/abs/2310.14423</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hmgxr128/qsr">https://github.com/hmgxr128/qsr</a></li>
<li>paper_authors: Xinran Gu, Kaifeng Lyu, Sanjeev Arora, Jingzhao Zhang, Longbo Huang<br>for: 本文旨在解决分布式深度学习中同步梯度的问题，特别是在多个节点同时训练大型模型时，通信开销增大。methods: 本文提出了一种名为幂 synchronization rule（QSR）的理论基础方法，该方法在学习率递减过程中动态设置H值，以提高模型的泛化性。results: 对于ImageNet datasets上的ResNet和ViT模型，本文的实验表明，使用QSR的本地梯度方法可以在同步方法中提高测试准确率，并且在16或64个GPU上进行训练时，可以降低训练时间，同时提高验证预测率。<details>
<summary>Abstract</summary>
In distributed deep learning with data parallelism, synchronizing gradients at each training step can cause a huge communication overhead, especially when many nodes work together to train large models. Local gradient methods, such as Local SGD, address this issue by allowing workers to compute locally for $H$ steps without synchronizing with others, hence reducing communication frequency. While $H$ has been viewed as a hyperparameter to trade optimization efficiency for communication cost, recent research indicates that setting a proper $H$ value can lead to generalization improvement. Yet, selecting a proper $H$ is elusive. This work proposes a theory-grounded method for determining $H$, named the Quadratic Synchronization Rule (QSR), which recommends dynamically setting $H$ in proportion to $\frac{1}{\eta^2}$ as the learning rate $\eta$ decays over time. Extensive ImageNet experiments on ResNet and ViT show that local gradient methods with QSR consistently improve the test accuracy over other synchronization strategies. Compared with the standard data parallel training, QSR enables Local AdamW on ViT-B to cut the training time on 16 or 64 GPUs down from 26.7 to 20.2 hours or from 8.6 to 5.5 hours and, at the same time, achieves $1.16\%$ or $0.84\%$ higher top-1 validation accuracy.
</details>
<details>
<summary>摘要</summary>
在分布式深度学习中，每个训练步骤的参数同步过程可能会导致巨大的通信开销，特别是当多个节点同时训练大型模型时。本地梯度方法，如本地SGD，可以通过让工作者在本地计算$H$步骤而降低与其他节点之间的通信频率。虽然$H$被视为训练效率和通信成本之间的权衡因素，但是选择合适的$H$值仍然是一个悬峰。这种工作提出了一种基于理论的方法，称为幂函数同步规则（QSR），该方法在学习率$\eta$逐渐减小过程中，对$H$进行动态设置，并与$\frac{1}{\eta^2}$成正比。经验显示，使用QSR的本地梯度方法在ImageNet上的ResNet和ViT上表现出了比其他同步策略更高的测试准确率。相比标准的数据并行训练，QSR使得Local AdamW在ViT-B上的16或64 GPU上减少了26.7小时训练时间，并同时实现了1.16%或0.84%高的顶部一 validate准精度。
</details></li>
</ul>
<hr>
<h2 id="Data-Augmentation-a-Combined-Inductive-Deductive-Approach-featuring-Answer-Set-Programming"><a href="#Data-Augmentation-a-Combined-Inductive-Deductive-Approach-featuring-Answer-Set-Programming" class="headerlink" title="Data Augmentation: a Combined Inductive-Deductive Approach featuring Answer Set Programming"></a>Data Augmentation: a Combined Inductive-Deductive Approach featuring Answer Set Programming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14413">http://arxiv.org/abs/2310.14413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierangela Bruno, Francesco Calimeri, Cinzia Marte, Simona Perri</li>
<li>for: 建立一个 hybrid inductive-deductive 框架，从有限多个真实标签图像开始，以运用逻辑程式来压缩新图像的结构，并且 garantuee 这些新图像符合领域知识中的组合遵循和特定的欲望。</li>
<li>methods: 使用逻辑程式来压缩新图像的结构，并且使用深度学习来创建photo-realistic 图像。</li>
<li>results: 提出了一个 hybrid inductive-deductive 框架，可以从有限多个真实标签图像开始，以运用逻辑程式来压缩新图像的结构，并且 garantuee 这些新图像符合领域知识中的组合遵循和特定的欲望。<details>
<summary>Abstract</summary>
Although the availability of a large amount of data is usually given for granted, there are relevant scenarios where this is not the case; for instance, in the biomedical/healthcare domain, some applications require to build huge datasets of proper images, but the acquisition of such images is often hard for different reasons (e.g., accessibility, costs, pathology-related variability), thus causing limited and usually imbalanced datasets. Hence, the need for synthesizing photo-realistic images via advanced Data Augmentation techniques is crucial. In this paper we propose a hybrid inductive-deductive approach to the problem; in particular, starting from a limited set of real labeled images, the proposed framework makes use of logic programs for declaratively specifying the structure of new images, that is guaranteed to comply with both a set of constraints coming from the domain knowledge and some specific desiderata. The resulting labeled images undergo a dedicated process based on Deep Learning in charge of creating photo-realistic images that comply with the generated label.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Universal-representation-by-Boltzmann-machines-with-Regularised-Axons"><a href="#Universal-representation-by-Boltzmann-machines-with-Regularised-Axons" class="headerlink" title="Universal representation by Boltzmann machines with Regularised Axons"></a>Universal representation by Boltzmann machines with Regularised Axons</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14395">http://arxiv.org/abs/2310.14395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Przemysław R. Grzybowski, Antoni Jankiewicz, Eloy Piñol, David Cirauqui, Dorota H. Grzybowska, Paweł M. Petrykowski, Miguel Ángel García-March, Maciej Lewenstein, Gorka Muñoz-Gil, Alejandro Pozas-Kerstjens</li>
<li>for: 这篇论文旨在描述一种对 Boltzmann 机器进行规范，以便有效地采样和训练。</li>
<li>methods: 这篇论文使用了对 Boltzmann 机器连接的规范，以控制模型的能量地形，从而使得采样和训练变得更加容易。</li>
<li>results: 论文证明了规范后的 Boltzmann 机器仍能够表示任意分布，并且可以控制数量的能量地峰，以便进行导航式采样和训练。此外，文章还表明了规范后的 Boltzmann 机器可以存储无限多个相关的visible patron，并且可以完美地重新建立。<details>
<summary>Abstract</summary>
It is widely known that Boltzmann machines are capable of representing arbitrary probability distributions over the values of their visible neurons, given enough hidden ones. However, sampling -- and thus training -- these models can be numerically hard. Recently we proposed a regularisation of the connections of Boltzmann machines, in order to control the energy landscape of the model, paving a way for efficient sampling and training. Here we formally prove that such regularised Boltzmann machines preserve the ability to represent arbitrary distributions. This is in conjunction with controlling the number of energy local minima, thus enabling easy \emph{guided} sampling and training. Furthermore, we explicitly show that regularised Boltzmann machines can store exponentially many arbitrarily correlated visible patterns with perfect retrieval, and we connect them to the Dense Associative Memory networks.
</details>
<details>
<summary>摘要</summary>
广泛知道，博尔兹曼机能够表示任意概率分布的值 visible neuron，只要有 enough hidden ones。然而，采样 -- 和因此训练 -- 这些模型可能是数值上的困难。最近，我们提出了 Boltzmann 机Connection 的 regularization，以控制模型的能量地形，使得可以有效地采样和训练。我们正式证明，这些正则化的 Boltzmann 机能够保持表示任意分布的能力。此外，我们还显式地显示了正则化 Boltzmann 机可以存储无限多个相关的可见模式，并且可以完美地重新 retrieve，并与 dense associative memory networks 相连。Note: "visible neurons" and "hidden neurons" are not explicitly translated in the text, as they are not necessary for the meaning of the sentence. However, in Simplified Chinese, "visible neurons" can be translated as "可见神经元" and "hidden neurons" can be translated as "隐藏神经元".
</details></li>
</ul>
<hr>
<h2 id="A-global-product-of-fine-scale-urban-building-height-based-on-spaceborne-lidar"><a href="#A-global-product-of-fine-scale-urban-building-height-based-on-spaceborne-lidar" class="headerlink" title="A global product of fine-scale urban building height based on spaceborne lidar"></a>A global product of fine-scale urban building height based on spaceborne lidar</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14355">http://arxiv.org/abs/2310.14355</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao Ma, Guang Zheng, Chi Xu, L. Monika Moskal, Peng Gong, Qinghua Guo, Huabing Huang, Xuecao Li, Yong Pang, Cheng Wang, Huan Xie, Bailang Yu, Bo Zhao, Yuyu Zhou<br>for:This paper aims to provide a global product of urban building heights with fine spatial resolutions and global coverages, which is essential for achieving the UN’s Sustainable Development Goals (SDGs) and supporting future urban studies.methods:The authors combined the spaceborne lidar instrument of GEDI with multi-sourced data including remotely sensed images (Landsat-8, Sentinel-2, and Sentinel-1) and topographic data to produce a global product of urban building heights with a fine grid size of 150 m around 2020.results:The estimated method of building height samples based on the GEDI data was effective with a Pearson’s r of 0.78 and an RMSE of 3.67 m in comparison to the reference data. The mapping product also demonstrated good performance with a Pearson’s r of 0.71 and an RMSE of 4.60 m. The global urban building height map provides a higher spatial resolution (150 m) with greater inherent details about the spatial heterogeneity and flexibility of updating using the GEDI samples as inputs.<details>
<summary>Abstract</summary>
Characterizing urban environments with broad coverages and high precision is more important than ever for achieving the UN's Sustainable Development Goals (SDGs) as half of the world's populations are living in cities. Urban building height as a fundamental 3D urban structural feature has far-reaching applications. However, so far, producing readily available datasets of recent urban building heights with fine spatial resolutions and global coverages remains a challenging task. Here, we provide an up-to-date global product of urban building heights based on a fine grid size of 150 m around 2020 by combining the spaceborne lidar instrument of GEDI and multi-sourced data including remotely sensed images (i.e., Landsat-8, Sentinel-2, and Sentinel-1) and topographic data. Our results revealed that the estimated method of building height samples based on the GEDI data was effective with 0.78 of Pearson's r and 3.67 m of RMSE in comparison to the reference data. The mapping product also demonstrated good performance as indicated by its strong correlation with the reference data (i.e., Pearson's r = 0.71, RMSE = 4.60 m). Compared with the currently existing products, our global urban building height map holds the ability to provide a higher spatial resolution (i.e., 150 m) with a great level of inherent details about the spatial heterogeneity and flexibility of updating using the GEDI samples as inputs. This work will boost future urban studies across many fields including climate, environmental, ecological, and social sciences.
</details>
<details>
<summary>摘要</summary>
将城市环境Characterizing with broad coverage and high precision是实现联合国可持续发展目标(SDGs)的关键，因为全球人口的一半居住在城市中。城市建筑高度作为城市三维结构特征有着广泛的应用。然而，到目前为止，生成可靠的城市建筑高度数据集，具有细度的高空间分辨率和全球覆盖率，仍然是一项挑战。我们提供了2020年的全球城市建筑高度产品，基于150米的细网格大小，通过结合地面雷达仪器GEDI和多源数据（如卫星图像（Landsat-8、Sentinel-2、Sentinel-1）和地形数据）。我们的结果表明，基于GEDI数据的建筑高度采样方法的效果是良好，Pearson相关系数为0.78，RMSE为3.67米。我们的映射产品也表现出了良好的性能，与参照数据相关系数为0.71，RMSE为4.60米。与现有产品相比，我们的全球城市建筑高度地图具有更高的空间分辨率（150米）和更多的内在细节，可以用GEDI采样作为输入，更好地满足未来城市研究的需求。
</details></li>
</ul>
<hr>
<h2 id="Can-strong-structural-encoding-reduce-the-importance-of-Message-Passing"><a href="#Can-strong-structural-encoding-reduce-the-importance-of-Message-Passing" class="headerlink" title="Can strong structural encoding reduce the importance of Message Passing?"></a>Can strong structural encoding reduce the importance of Message Passing?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15197">http://arxiv.org/abs/2310.15197</a></li>
<li>repo_url: None</li>
<li>paper_authors: Floor Eijkelboom, Erik Bekkers, Michael Bronstein, Francesco Di Giovanni</li>
<li>for: 本文研究了message passing neural networks（MPNNs）在图数据上的应用，特别是如何在图数据上使用特征信息和结构信息来学习节点表示。</li>
<li>methods: 本文提出了一种新的方法，基于矩阵乘法来将特征信息和结构信息相互作用。这种方法与标准的笛卡尔和拼接方法进行比较。</li>
<li>results: 研究结果表明，使用矩阵乘法方法可以在一些任务上减少或完全消除消息传递层，而无需让模型表现下降。这表明，当模型可以构建强的结构编码时，消息传递的重要性相对较低。<details>
<summary>Abstract</summary>
The most prevalent class of neural networks operating on graphs are message passing neural networks (MPNNs), in which the representation of a node is updated iteratively by aggregating information in the 1-hop neighborhood. Since this paradigm for computing node embeddings may prevent the model from learning coarse topological structures, the initial features are often augmented with structural information of the graph, typically in the form of Laplacian eigenvectors or Random Walk transition probabilities. In this work, we explore the contribution of message passing when strong structural encodings are provided. We introduce a novel way of modeling the interaction between feature and structural information based on their tensor product rather than the standard concatenation. The choice of interaction is compared in common scenarios and in settings where the capacity of the message-passing layer is severely reduced and ultimately the message-passing phase is removed altogether. Our results indicate that using tensor-based encodings is always at least on par with the concatenation-based encoding and that it makes the model much more robust when the message passing layers are removed, on some tasks incurring almost no drop in performance. This suggests that the importance of message passing is limited when the model can construct strong structural encodings.
</details>
<details>
<summary>摘要</summary>
最常见的图学习网络是消息传递神经网络（MPNN），它们的节点表示更新是通过邻居信息的聚合来实现的。由于这种方法可能会阻止模型学习大规模的结构特征，因此通常会将初始特征与图结构信息相结合，通常是拉普拉斯特征或游走过程的概率。在这项工作中，我们研究了消息传递在强结构编码下的贡献。我们提出了一种基于维度积 producer 而不是标准拼接的交互方法来模型特征和结构信息之间的互动。我们对于常见的场景和消息传递层的容量减少情况进行比较，最后还移除消息传递阶段 altogether。我们的结果表明，使用维度积编码总是与拼接编码相当，并且在某些任务下，它会减少性能的下降幅度。这表示消息传递的重要性在模型可以构建强结构编码时相对较低。
</details></li>
</ul>
<hr>
<h2 id="Pyramidal-Hidden-Markov-Model-For-Multivariate-Time-Series-Forecasting"><a href="#Pyramidal-Hidden-Markov-Model-For-Multivariate-Time-Series-Forecasting" class="headerlink" title="Pyramidal Hidden Markov Model For Multivariate Time Series Forecasting"></a>Pyramidal Hidden Markov Model For Multivariate Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14341">http://arxiv.org/abs/2310.14341</a></li>
<li>repo_url: None</li>
<li>paper_authors: YeXin Huang</li>
<li>for: 预测时间序列数据的未来值</li>
<li>methods: 提出了一种基于PyramidalHidden Markov Model（PHMM）的时间序列预测方法，利用多步隐марков链模型来捕捉多个多步隐态态</li>
<li>results: 实验结果表明，提出的PHMM模型在多变量时间序列 dataset上比其竞争对手更加出色，能够更好地处理非站点和噪音数据，并建立更加准确和全面的预测结果。<details>
<summary>Abstract</summary>
The Hidden Markov Model (HMM) can predict the future value of a time series based on its current and previous values, making it a powerful algorithm for handling various types of time series. Numerous studies have explored the improvement of HMM using advanced techniques, leading to the development of several variations of HMM. Despite these studies indicating the increased competitiveness of HMM compared to other advanced algorithms, few have recognized the significance and impact of incorporating multistep stochastic states into its performance. In this work, we propose a Pyramidal Hidden Markov Model (PHMM) that can capture multiple multistep stochastic states. Initially, a multistep HMM is designed for extracting short multistep stochastic states. Next, a novel time series forecasting structure is proposed based on PHMM, which utilizes pyramid-like stacking to adaptively identify long multistep stochastic states. By employing these two schemes, our model can effectively handle non-stationary and noisy data, while also establishing long-term dependencies for more accurate and comprehensive forecasting. The experimental results on diverse multivariate time series datasets convincingly demonstrate the superior performance of our proposed PHMM compared to its competitive peers in time series forecasting.
</details>
<details>
<summary>摘要</summary>
隐藏Markov模型（HMM）可以预测时间序列的未来值基于其当前和前一个值，使其成为处理多种时间序列的 poderful算法。许多研究已经探索了HMM的改进，导致了多种HMM的发展。 despite these studies indicating the increased competitiveness of HMM compared to other advanced algorithms, few have recognized the significance and impact of incorporating multistep stochastic states into its performance. 在这种工作中，我们提议一种Pyramidal隐藏Markov模型（PHMM），可以捕捉多个多步骤的随机状态。首先，我们设计了一种多步骤HMM，用于提取短时间内的多步骤随机状态。然后，我们提出了一种基于PHMM的新的时间序列预测结构，使用Pyramid-like的堆叠来适应ively认定长时间内的多步骤随机状态。通过这两种方案，我们的模型可以有效地处理不稳定和噪声掺杂的数据，同时也可以建立长期依赖关系，以更准确和全面的预测。实验结果表明，我们的提议的PHMM在多种多变量时间序列数据集上表现出了superior的性能，与其竞争对手相比。
</details></li>
</ul>
<hr>
<h2 id="PPFL-A-Personalized-Federated-Learning-Framework-for-Heterogeneous-Population"><a href="#PPFL-A-Personalized-Federated-Learning-Framework-for-Heterogeneous-Population" class="headerlink" title="PPFL: A Personalized Federated Learning Framework for Heterogeneous Population"></a>PPFL: A Personalized Federated Learning Framework for Heterogeneous Population</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14337">http://arxiv.org/abs/2310.14337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Di, Yi Yang, Haishan Ye, Xiangyu Chang</li>
<li>for: 本研究旨在开发一种适应个人偏好的 Federated Learning 框架，以保护个人隐私。</li>
<li>methods: 本研究使用 canonical models 捕捉人群的基本特征，并使用 membership vectors 表示客户的偏好。</li>
<li>results: 研究表明，PPFL 方法可以提供substantial insights into client characteristics，并且比 existed Personalized Federated Learning 方法更有优势。<details>
<summary>Abstract</summary>
Personalization aims to characterize individual preferences and is widely applied across many fields. However, conventional personalized methods operate in a centralized manner and potentially expose the raw data when pooling individual information. In this paper, with privacy considerations, we develop a flexible and interpretable personalized framework within the paradigm of Federated Learning, called PPFL (Population Personalized Federated Learning). By leveraging canonical models to capture fundamental characteristics among the heterogeneous population and employing membership vectors to reveal clients' preferences, it models the heterogeneity as clients' varying preferences for these characteristics and provides substantial insights into client characteristics, which is lacking in existing Personalized Federated Learning (PFL) methods. Furthermore, we explore the relationship between our method and three main branches of PFL methods: multi-task PFL, clustered FL, and decoupling PFL, and demonstrate the advantages of PPFL. To solve PPFL (a non-convex constrained optimization problem), we propose a novel random block coordinate descent algorithm and present the convergence property. We conduct experiments on both pathological and practical datasets, and the results validate the effectiveness of PPFL.
</details>
<details>
<summary>摘要</summary>
个人化目标是描述个体偏好，广泛应用于多个领域。然而，传统的个人化方法采用中央化的方式运行，可能暴露个体数据。在这篇论文中，我们考虑隐私问题，开发了一种灵活可解释的个人化框架，称为PPFL（人口个性化联合学习）。我们利用 canonical model 捕捉人口中的基本特征，并使用会员 вектор 表达客户的偏好，以模拟客户对这些特征的差异性，并提供了详细的客户特征信息，其与现有的个性化联合学习方法（PFL）不同。此外，我们还探讨了我们的方法与多任务 PFL、分区 FL 和解除 PFL 的三大分支的关系，并证明 PPFL 的优势。为解决 PPFL （一个非对称约束优化问题），我们提议一种新的随机块坐标下降算法，并证明其收敛性。我们在实验中使用了一些病理和实际的数据集，并 validate PPFL 的效果。
</details></li>
</ul>
<hr>
<h2 id="Finite-Sample-Analysis-of-the-Temporal-Difference-Learning"><a href="#Finite-Sample-Analysis-of-the-Temporal-Difference-Learning" class="headerlink" title="Finite-Sample Analysis of the Temporal Difference Learning"></a>Finite-Sample Analysis of the Temporal Difference Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14286">http://arxiv.org/abs/2310.14286</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sergey Samsonov, Daniil Tiapkin, Alexey Naumov, Eric Moulines</li>
<li>for: 本研究考虑了使用 temporal difference (TD) 方法进行政策评估的 Markov Decision Processes (MDP) 中的性能精细 bounds 问题。</li>
<li>methods: 本文使用了一种简单的算法，即使用 universal 和实例无关的步长，以及 Polyak-Ruppert 尾均值。</li>
<li>results: 本文提供了近似优化的几何和偏差项，以及相应的样本复杂性 bound。 我们的证明技巧基于 refined error bounds  для linear stochastic approximation 以及 TD-type recurrence 中的新稳定性结果。<details>
<summary>Abstract</summary>
In this paper we consider the problem of obtaining sharp bounds for the performance of temporal difference (TD) methods with linear functional approximation for policy evaluation in discounted Markov Decision Processes. We show that a simple algorithm with a universal and instance-independent step size together with Polyak-Ruppert tail averaging is sufficient to obtain near-optimal variance and bias terms. We also provide the respective sample complexity bounds. Our proof technique is based on refined error bounds for linear stochastic approximation together with the novel stability result for the product of random matrices that arise from the TD-type recurrence.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们考虑了使用线性函数 aproximation 的 temporal difference（TD）方法来评估折扣Markov决策过程中的性能。我们显示了一个简单的算法，具有 universal 和实例独立的步长，可以获得near-optimal的偏差和方差项。我们还提供了相应的样本复杂性bound。我们的证明技术基于线性随机化的精细错误 bounds 以及TD型循环中的产品Random Matrices的新稳定性结果。
</details></li>
</ul>
<hr>
<h2 id="Robust-Visual-Imitation-Learning-with-Inverse-Dynamics-Representations"><a href="#Robust-Visual-Imitation-Learning-with-Inverse-Dynamics-Representations" class="headerlink" title="Robust Visual Imitation Learning with Inverse Dynamics Representations"></a>Robust Visual Imitation Learning with Inverse Dynamics Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14274">http://arxiv.org/abs/2310.14274</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyuan Li, Xun Wang, Rongchang Zuo, Kewu Sun, Lingfei Cui, Jishiyu Ding, Peng Liu, Zhe Ma</li>
<li>for: 解决复杂的sequential decision-making问题</li>
<li>methods: 开发了一种 inverse dynamics state representation learning objective，以对学习环境和专家环境进行对齐</li>
<li>results: 在各种视觉扰动和多种视觉控制任务中，可以达到几乎专家水平的性能，与现状的visual IL方法和Robust IL方法显著超越<details>
<summary>Abstract</summary>
Imitation learning (IL) has achieved considerable success in solving complex sequential decision-making problems. However, current IL methods mainly assume that the environment for learning policies is the same as the environment for collecting expert datasets. Therefore, these methods may fail to work when there are slight differences between the learning and expert environments, especially for challenging problems with high-dimensional image observations. However, in real-world scenarios, it is rare to have the chance to collect expert trajectories precisely in the target learning environment. To address this challenge, we propose a novel robust imitation learning approach, where we develop an inverse dynamics state representation learning objective to align the expert environment and the learning environment. With the abstract state representation, we design an effective reward function, which thoroughly measures the similarity between behavior data and expert data not only element-wise, but also from the trajectory level. We conduct extensive experiments to evaluate the proposed approach under various visual perturbations and in diverse visual control tasks. Our approach can achieve a near-expert performance in most environments, and significantly outperforms the state-of-the-art visual IL methods and robust IL methods.
</details>
<details>
<summary>摘要</summary>
“模仿学习（IL）已经在复杂的顺序决策问题上取得了显著的成功。然而，现有的IL方法主要假设学习策略的环境与收集专家数据的环境一致。因此，这些方法可能在环境有所不同时失效，特别是高维图像观察的复杂问题上。然而，在实际场景中，很少有收集专家轨迹的机会， precisely in the target learning environment。为解决这个挑战，我们提出了一种新的稳定的模仿学习方法，其中我们开发了反动动态状态表示学习目标，以对专家环境和学习环境进行对齐。通过抽象状态表示，我们设计了一个有效的奖励函数，该函数不仅在元素级别，还在轨迹级别进行 Similarity Measure between behavior data and expert data。我们进行了广泛的实验来评估我们的方法，并在不同的视觉干扰和多种视觉控制任务中达到了 near-expert 性能。我们的方法可以在大多数环境中达到领先的性能，并在视觉IL方法和稳定IL方法中具有显著优势。”
</details></li>
</ul>
<hr>
<h2 id="Shortcuts-for-causal-discovery-of-nonlinear-models-by-score-matching"><a href="#Shortcuts-for-causal-discovery-of-nonlinear-models-by-score-matching" class="headerlink" title="Shortcuts for causal discovery of nonlinear models by score matching"></a>Shortcuts for causal discovery of nonlinear models by score matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14246">http://arxiv.org/abs/2310.14246</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Montagna, Nicoletta Noceti, Lorenzo Rosasco, Francesco Locatello</li>
<li>for: 这篇论文的目的是对非线性随机数据中的 causal discovery 进行研究，并提出了 ScoreSort 算法来解决非线性模型中的 score 排序问题。</li>
<li>methods: 该论文使用了 simulated data 进行实验，并对非线性模型进行了分析和比较。</li>
<li>results: 研究发现，ScoreSort 算法在非线性模型中具有更高的统计效率，并且可以在多种 synthetic benchmarks 上实现 score-sortability。同时，研究还发现了数据的多样性是评估非线性 causal discovery 方法的重要限制因素，以及在不同的设定下进行详细测试和分析统计性质是对 causal discovery 研究中的重要考虑因素。<details>
<summary>Abstract</summary>
The use of simulated data in the field of causal discovery is ubiquitous due to the scarcity of annotated real data. Recently, Reisach et al., 2021 highlighted the emergence of patterns in simulated linear data, which displays increasing marginal variance in the casual direction. As an ablation in their experiments, Montagna et al., 2023 found that similar patterns may emerge in nonlinear models for the variance of the score vector $\nabla \log p_{\mathbf{X}$, and introduced the ScoreSort algorithm. In this work, we formally define and characterize this score-sortability pattern of nonlinear additive noise models. We find that it defines a class of identifiable (bivariate) causal models overlapping with nonlinear additive noise models. We theoretically demonstrate the advantages of ScoreSort in terms of statistical efficiency compared to prior state-of-the-art score matching-based methods and empirically show the score-sortability of the most common synthetic benchmarks in the literature. Our findings remark (1) the lack of diversity in the data as an important limitation in the evaluation of nonlinear causal discovery approaches, (2) the importance of thoroughly testing different settings within a problem class, and (3) the importance of analyzing statistical properties in causal discovery, where research is often limited to defining identifiability conditions of the model.
</details>
<details>
<summary>摘要</summary>
使用模拟数据在 causal discovery 领域是普遍的，因为真实标注数据的罕见。Recently, Reisach et al. (2021) 指出了模拟数据中的增长性趋势，这种趋势在 causal 方向上 display 增加的边缘方差。在他们的实验中，Montagna et al. (2023) 发现了类似的趋势可能会出现在非线性模型中，并提出了 ScoreSort 算法。在这种工作中，我们正式定义和特征化这种 ScoreSort 模型的评分可排性特征。我们发现这种特征定义了一类可识别的双向 causal 模型，与非线性加性随机噪声模型 overlap 。我们理论上表明 ScoreSort 比 Priori 状态的分配比对方法更高效，并且实际上证明了 literature 中最常见的 sintetic 标准 benchmark 中的评分可排性。我们的发现包括（1）数据的不够多样性是评估非线性 causal discovery 方法的重要限制，（2）在一个问题类中测试不同设置是重要的，（3）在 causal discovery 中分析统计性质是研究中常常被限制的。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Deep-Ensemble-for-Out-of-Distribution-Detection-A-Loss-Landscape-Perspective"><a href="#Revisiting-Deep-Ensemble-for-Out-of-Distribution-Detection-A-Loss-Landscape-Perspective" class="headerlink" title="Revisiting Deep Ensemble for Out-of-Distribution Detection: A Loss Landscape Perspective"></a>Revisiting Deep Ensemble for Out-of-Distribution Detection: A Loss Landscape Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14227">http://arxiv.org/abs/2310.14227</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fanghenshaometeor/ood-mode-ensemble">https://github.com/fanghenshaometeor/ood-mode-ensemble</a></li>
<li>paper_authors: Kun Fang, Qinghua Tao, Xiaolin Huang, Jie Yang</li>
<li>for: 这个研究旨在探讨深度学习模型中的Out-of-Distribution（OoD）检测方法，以及如何从损失地图的角度来探索OoD检测。</li>
<li>methods: 本研究使用了多个独立的模式来探索OoD检测，并通过模式组合来提高OoD检测的性能。</li>
<li>results: 实验结果显示，独立模式之间存在高度的测量不确定性，而模式组合可以将这些不确定性减少，提高OoD检测的可靠性。<details>
<summary>Abstract</summary>
Existing Out-of-Distribution (OoD) detection methods address to detect OoD samples from In-Distribution data (InD) mainly by exploring differences in features, logits and gradients in Deep Neural Networks (DNNs). We in this work propose a new perspective upon loss landscape and mode ensemble to investigate OoD detection. In the optimization of DNNs, there exist many local optima in the parameter space, or namely modes. Interestingly, we observe that these independent modes, which all reach low-loss regions with InD data (training and test data), yet yield significantly different loss landscapes with OoD data. Such an observation provides a novel view to investigate the OoD detection from the loss landscape and further suggests significantly fluctuating OoD detection performance across these modes. For instance, FPR values of the RankFeat method can range from 46.58% to 84.70% among 5 modes, showing uncertain detection performance evaluations across independent modes. Motivated by such diversities on OoD loss landscape across modes, we revisit the deep ensemble method for OoD detection through mode ensemble, leading to improved performance and benefiting the OoD detector with reduced variances. Extensive experiments covering varied OoD detectors and network structures illustrate high variances across modes and also validate the superiority of mode ensemble in boosting OoD detection. We hope this work could attract attention in the view of independent modes in the OoD loss landscape and more reliable evaluations on OoD detectors.
</details>
<details>
<summary>摘要</summary>
现有的Out-of-Distribution（OoD）检测方法主要通过探索特征、搅瑞和梯度在深度神经网络（DNN）中检测OoD样本。在本工作中，我们提出了一新的视角，即损失 landscape和模式ensemble来研究OoD检测。DNN的优化中存在许多本地极值点，即模式。我们发现这些独立的模式，它们都能够在训练和测试数据上达到低损失区域，但是在OoD数据上却导致了显著不同的损失 landscape。这种观察提供了一个新的视角来研究OoD检测，并且建议了通过模式ensemble来改进OoD检测性能。例如，RankFeat方法的FPR值在不同模式中可以从46.58%到84.70%不同，表明OoD检测性能的评估存在很大的不确定性。我们通过模式ensemble来降低这种不确定性，并通过广泛的实验证明了模式ensemble的superiority。我们希望这种新的视角能吸引更多的关注，并且能够提供更可靠的OoD检测评估方法。
</details></li>
</ul>
<hr>
<h2 id="SUT-Active-Defects-Probing-for-Transcompiler-Models"><a href="#SUT-Active-Defects-Probing-for-Transcompiler-Models" class="headerlink" title="SUT: Active Defects Probing for Transcompiler Models"></a>SUT: Active Defects Probing for Transcompiler Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14209">http://arxiv.org/abs/2310.14209</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengnan Qi, Yufan Huang, Maoquan Wang, Yongqiang Yao, Zihan Liu, Bin Gu, Colin Clement, Neel Sundaresan</li>
<li>for: 本研究旨在提出一种新的编程语言翻译评价指标，以检测当前的编程语言翻译模型在目标语言中的基本语法错误。</li>
<li>methods: 本研究使用了一种新的活动报告测试集（SUT），包括一个高度可读性的评价工具 для准确性和测试分数。</li>
<li>results: 实验表明，即使使用了强大的模型如ChatGPT，其仍然在这些基本单元测试上出现错误，相比之前的编程语言翻译任务评价集，其通过率下降了26.15%。此外，评价工具还揭示了这些模型在语法元素方面的不足。<details>
<summary>Abstract</summary>
Automatic Program translation has enormous application value and hence has been attracting significant interest from AI researchers. However, we observe that current program translation models still make elementary syntax errors, particularly, when the target language does not have syntax elements in the source language. Metrics like BLUE, CodeBLUE and computation accuracy may not expose these issues. In this paper we introduce a new metrics for programming language translation and these metrics address these basic syntax errors. We develop a novel active defects probing suite called Syntactic Unit Tests (SUT) which includes a highly interpretable evaluation harness for accuracy and test scoring. Experiments have shown that even powerful models like ChatGPT still make mistakes on these basic unit tests. Specifically, compared to previous program translation task evaluation dataset, its pass rate on our unit tests has decreased by 26.15%. Further our evaluation harness reveal syntactic element errors in which these models exhibit deficiencies.
</details>
<details>
<summary>摘要</summary>
自动程序翻译具有巨大的应用价值，因此吸引了许多人工智能研究者的关注。然而，我们发现当目标语言缺乏源语言语法元素时，当前的程序翻译模型仍然会出现基本语法错误。度量器如BLUE、CodeBLUE和计算准确率可能不会暴露这些问题。在这篇论文中，我们介绍了一种新的程序语言翻译度量器，这些度量器可以捕捉这些基本语法错误。我们开发了一个新的活动缺陷探测组合 called Syntactic Unit Tests (SUT)，该组合包括一个高度可读性评估器和准确分数评价。实验表明，即使是 poderoso的 ChatGPT 模型也会在我们的单元测试中出现错误。具体来说，相比之前的程序翻译任务评估集，我们的单元测试中的通过率下降了26.15%。此外，我们的评估器还揭示了这些模型在语法元素上的缺陷。
</details></li>
</ul>
<hr>
<h2 id="Prompt-Engineering-Through-the-Lens-of-Optimal-Control"><a href="#Prompt-Engineering-Through-the-Lens-of-Optimal-Control" class="headerlink" title="Prompt Engineering Through the Lens of Optimal Control"></a>Prompt Engineering Through the Lens of Optimal Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14201">http://arxiv.org/abs/2310.14201</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Luo, Yiming Tang, Chengfeng Shen, Zhennan Zhou, Bin Dong</li>
<li>for: 该论文旨在探讨Optimal Control Framework的应用于多轮人工智能交互（Prompt Engineering，PE）中，以提高人机交互的效率和效果。</li>
<li>methods: 该论文使用了多轮PE方法，包括单轮PE、多轮PE、集成PE和多智能协作PE等方法，以及一种新的优化控制框架，以系матизи并优化现有PE方法。</li>
<li>results: 该论文提出了一种优化控制框架，可以对多轮PE进行系мати化和优化，并且可以扩展到多智能协作PE和集成PE等方法。这些方法可以提高人机交互的效率和效果，并且具有更好的可解释性和可视化性。<details>
<summary>Abstract</summary>
Prompt Engineering (PE) has emerged as a critical technique for guiding Large Language Models (LLMs) in solving intricate tasks. Its importance is highlighted by its potential to significantly enhance the efficiency and effectiveness of human-machine interaction. As tasks grow increasingly complex, recent advanced PE methods have extended beyond the limitations of single-round interactions to embrace multi-round interactions, which allows for a deeper and more nuanced engagement with LLMs. In this paper, we propose an optimal control framework tailored for multi-round interactions with LLMs. This framework provides a unified mathematical structure that not only systematizes the existing PE methods but also sets the stage for rigorous analytical improvements. Furthermore, we extend this framework to include PE via ensemble methods and multi-agent collaboration, thereby enlarging the scope of applicability. By adopting an optimal control perspective, we offer fresh insights into existing PE methods and highlight theoretical challenges that warrant future research. Besides, our work lays a foundation for the development of more effective and interpretable PE methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Improved-Techniques-for-Training-Consistency-Models"><a href="#Improved-Techniques-for-Training-Consistency-Models" class="headerlink" title="Improved Techniques for Training Consistency Models"></a>Improved Techniques for Training Consistency Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14189">http://arxiv.org/abs/2310.14189</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sallu08/Consistency-Regulariation-FSSL-Naive">https://github.com/sallu08/Consistency-Regulariation-FSSL-Naive</a></li>
<li>paper_authors: Yang Song, Prafulla Dhariwal<br>for:This paper focuses on improving the quality of consistency models, a type of generative model that can sample high-quality data in one step without the need for adversarial training.methods:The authors present several improved techniques for consistency training, including eliminating Exponential Moving Average from the teacher consistency model, adopting Pseudo-Huber losses, and introducing a lognormal noise schedule.results:The authors achieve FID scores of 2.51 and 3.25 on CIFAR-10 and ImageNet $64\times 64$ respectively in a single sampling step, marking a 3.5$\times$ and 4$\times$ improvement compared to prior consistency training approaches. Through two-step sampling, they further reduce FID scores to 2.24 and 2.77 on these two datasets, surpassing those obtained via distillation in both one-step and two-step settings.<details>
<summary>Abstract</summary>
Consistency models are a nascent family of generative models that can sample high quality data in one step without the need for adversarial training. Current consistency models achieve optimal sample quality by distilling from pre-trained diffusion models and employing learned metrics such as LPIPS. However, distillation limits the quality of consistency models to that of the pre-trained diffusion model, and LPIPS causes undesirable bias in evaluation. To tackle these challenges, we present improved techniques for consistency training, where consistency models learn directly from data without distillation. We delve into the theory behind consistency training and identify a previously overlooked flaw, which we address by eliminating Exponential Moving Average from the teacher consistency model. To replace learned metrics like LPIPS, we adopt Pseudo-Huber losses from robust statistics. Additionally, we introduce a lognormal noise schedule for the consistency training objective, and propose to double total discretization steps every set number of training iterations. Combined with better hyperparameter tuning, these modifications enable consistency models to achieve FID scores of 2.51 and 3.25 on CIFAR-10 and ImageNet $64\times 64$ respectively in a single sampling step. These scores mark a 3.5$\times$ and 4$\times$ improvement compared to prior consistency training approaches. Through two-step sampling, we further reduce FID scores to 2.24 and 2.77 on these two datasets, surpassing those obtained via distillation in both one-step and two-step settings, while narrowing the gap between consistency models and other state-of-the-art generative models.
</details>
<details>
<summary>摘要</summary>
《协调模型》是一个新兴的生成模型，可以在一步中采样高质量数据，而不需要对抗学习。现有的协调模型可以达到最佳的样本质量 by 热退 diffusion 模型，并使用学习的度量如 LPIPS。然而，热退限制了协调模型的质量，而 LPIPS 会导致评估中不良的偏见。为了解决这些挑战，我们提出了改进的协调训练方法，协调模型可以直接从数据中学习，无需热退。我们分析了协调训练的理论基础，发现了一个以前未被注意到的缺陷，我们通过从教师协调模型中减少 Exponential Moving Average 来解决这个缺陷。而代替学习度量，我们采用了 Pseudo-Huber 损失函数。此外，我们还引入了 lognormal 噪声程序，并提议在训练迭代中逐步增加总数化步骤。通过更好的hyperparameter优化，这些修改使得协调模型可以在一步中采样 FID 分数为 2.51 和 3.25 的 CIFAR-10 和 ImageNet $64\times 64$ 分别，比前一个 consistency training 方法提高了 3.5 倍和 4 倍。通过两步采样，我们还可以将 FID 分数降低到 2.24 和 2.77，超越了在一步和两步 Setting 中使用热退的分数，同时逐渐趋近其他状态的生成模型。
</details></li>
</ul>
<hr>
<h2 id="A-General-Theory-for-Softmax-Gating-Multinomial-Logistic-Mixture-of-Experts"><a href="#A-General-Theory-for-Softmax-Gating-Multinomial-Logistic-Mixture-of-Experts" class="headerlink" title="A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts"></a>A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14188">http://arxiv.org/abs/2310.14188</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huy Nguyen, Pedram Akbarian, TrungTin Nguyen, Nhat Ho</li>
<li>for: 本文研究了一种基于多个子模型的混合专家（MoE）模型，以提高多种回归和分类应用中的性能。</li>
<li>methods: 本文使用了抽象分析方法，对于回归设置下的MoE模型的性能进行了分析，并提出了一种新的修改了Softmax抽象函数的方法，以解决在部分专家参数消失时的问题。</li>
<li>results: 本文实证了MoE模型在分类问题下的抽象率和参数估计率，并发现在一些专家参数消失时，抽象率会 slower than polynomial 率，但通过修改Softmax抽象函数，可以提高参数估计率。<details>
<summary>Abstract</summary>
Mixture-of-experts (MoE) model incorporates the power of multiple submodels via gating functions to achieve greater performance in numerous regression and classification applications. From a theoretical perspective, while there have been previous attempts to comprehend the behavior of that model under the regression settings through the convergence analysis of maximum likelihood estimation in the Gaussian MoE model, such analysis under the setting of a classification problem has remained missing in the literature. We close this gap by establishing the convergence rates of density estimation and parameter estimation in the softmax gating multinomial logistic MoE model. Notably, when part of the expert parameters vanish, these rates are shown to be slower than polynomial rates owing to an inherent interaction between the softmax gating and expert functions via partial differential equations. To address this issue, we propose using a novel class of modified softmax gating functions which transform the input value before delivering them to the gating functions. As a result, the previous interaction disappears and the parameter estimation rates are significantly improved.
</details>
<details>
<summary>摘要</summary>
归一模型（Mixture-of-experts，MoE）利用多个子模型的力via关键函数实现更高的性能在多种回归和分类应用中。从理论上来说，尚未在文献中对MoE模型在回归设置下的行为进行了深入的分析，而这种分析在分类问题下尚未被探讨。我们填补了这一空白，并证明了softmax关键函数多omial几何MoE模型的整体抽象率和参数估计率在部分专家参数消失时 slower than 多项式率，这是因为softmax关键函数和专家函数之间存在自然的互动。 To address this issue, we propose using a novel class of modified softmax gating functions, which transform the input value before delivering them to the gating functions. As a result, the previous interaction disappears, and the parameter estimation rates are significantly improved.
</details></li>
</ul>
<hr>
<h2 id="Learning-Invariant-Molecular-Representation-in-Latent-Discrete-Space"><a href="#Learning-Invariant-Molecular-Representation-in-Latent-Discrete-Space" class="headerlink" title="Learning Invariant Molecular Representation in Latent Discrete Space"></a>Learning Invariant Molecular Representation in Latent Discrete Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14170">http://arxiv.org/abs/2310.14170</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hicai-zju/imold">https://github.com/hicai-zju/imold</a></li>
<li>paper_authors: Xiang Zhuang, Qiang Zhang, Keyan Ding, Yatao Bian, Xiao Wang, Jingsong Lv, Hongyang Chen, Huajun Chen</li>
<li>for: 这个研究旨在提高药物探索中的分子表示学习，并解决现有方法在不同环境下的训练和测试数据之间的泛化问题。</li>
<li>methods: 我们提出了一个新的框架，叫做“首先编码后分离”，可以将分子表示学习中的不同环境下的数据分类为不变的特征。我们还引入了一个对于训练数据的余域量化模组，以降低训练数据过滤的风险，并保持了Encoder的表达力。</li>
<li>results: 我们的模型在18个真实的分子数据集上进行了广泛的实验，结果显示我们的模型在不同环境下的泛化性比起现有基eline的优胜。我们的代码可以在<a target="_blank" rel="noopener" href="https://github.com/HICAI-ZJU/iMoLD%E4%B8%8A%E5%8F%96%E5%BE%97%E3%80%82">https://github.com/HICAI-ZJU/iMoLD上取得。</a><details>
<summary>Abstract</summary>
Molecular representation learning lays the foundation for drug discovery. However, existing methods suffer from poor out-of-distribution (OOD) generalization, particularly when data for training and testing originate from different environments. To address this issue, we propose a new framework for learning molecular representations that exhibit invariance and robustness against distribution shifts. Specifically, we propose a strategy called ``first-encoding-then-separation'' to identify invariant molecule features in the latent space, which deviates from conventional practices. Prior to the separation step, we introduce a residual vector quantization module that mitigates the over-fitting to training data distributions while preserving the expressivity of encoders. Furthermore, we design a task-agnostic self-supervised learning objective to encourage precise invariance identification, which enables our method widely applicable to a variety of tasks, such as regression and multi-label classification. Extensive experiments on 18 real-world molecular datasets demonstrate that our model achieves stronger generalization against state-of-the-art baselines in the presence of various distribution shifts. Our code is available at https://github.com/HICAI-ZJU/iMoLD.
</details>
<details>
<summary>摘要</summary>
分子表示学习建立了药物发现的基础。然而，现有方法受到不同环境下数据的外部分布Shift的困难，特别是在训练和测试数据来源不同时。为解决这问题，我们提出了一个新的分子表示学习框架，具有不变性和抗分布Shift的特点。具体来说，我们提出了一种名为“首先编码然后分离”的策略，以适应分子特征的不变性。在分离步骤之前，我们引入了一个很 residual vector quantization module，以避免训练数据分布适应而导致的过拟合，同时保持编码器的表达能力。此外，我们设计了一个无关任务的自适应学习目标，以促进精准的不变性标识，这使得我们的方法可以通用于多种任务，如回归和多个标签分类。我们的代码可以在https://github.com/HICAI-ZJU/iMoLD中获取。Extensive experiments on 18 real-world molecular datasets demonstrate that our model achieves stronger generalization against state-of-the-art baselines in the presence of various distribution shifts.
</details></li>
</ul>
<hr>
<h2 id="Ensemble-Learning-for-Graph-Neural-Networks"><a href="#Ensemble-Learning-for-Graph-Neural-Networks" class="headerlink" title="Ensemble Learning for Graph Neural Networks"></a>Ensemble Learning for Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14166">http://arxiv.org/abs/2310.14166</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wongzhenhao/elgnn">https://github.com/wongzhenhao/elgnn</a></li>
<li>paper_authors: Zhen Hao Wong, Ling Yue, Quanming Yao</li>
<li>for: 这篇论文探讨了使用集成学习技术来提高图结构数据中的图神经网络（GNNs）性能和可靠性。</li>
<li>methods: 这篇论文使用了多个GNN模型的不同初始化或架构，并使用了Tree-Structured Parzen Estimator算法确定ensemble weights。</li>
<li>results:  ensemble学习可以增强GNN对复杂图结构数据的分析能力，提高总准确率，降低偏差和方差，降低噪声数据的影响。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have shown success in various fields for learning from graph-structured data. This paper investigates the application of ensemble learning techniques to improve the performance and robustness of Graph Neural Networks (GNNs). By training multiple GNN models with diverse initializations or architectures, we create an ensemble model named ELGNN that captures various aspects of the data and uses the Tree-Structured Parzen Estimator algorithm to determine the ensemble weights. Combining the predictions of these models enhances overall accuracy, reduces bias and variance, and mitigates the impact of noisy data. Our findings demonstrate the efficacy of ensemble learning in enhancing GNN capabilities for analyzing complex graph-structured data. The code is public at https://github.com/wongzhenhao/ELGNN.
</details>
<details>
<summary>摘要</summary>
GRAPH Neural Networks (GNNs) have shown success in various fields for learning from graph-structured data. This paper investigates the application of ensemble learning techniques to improve the performance and robustness of GRAPH Neural Networks (GNNs). By training multiple GNN models with diverse initializations or architectures, we create an ensemble model named ELGNN that captures various aspects of the data and uses the Tree-Structured Parzen Estimator algorithm to determine the ensemble weights. Combining the predictions of these models enhances overall accuracy, reduces bias and variance, and mitigates the impact of noisy data. Our findings demonstrate the efficacy of ensemble learning in enhancing GNN capabilities for analyzing complex graph-structured data. The code is public at https://github.com/wongzhenhao/ELGNN.Note that I've kept the original English names for the concepts and techniques to maintain clarity and consistency.
</details></li>
</ul>
<hr>
<h2 id="α-Fair-Contextual-Bandits"><a href="#α-Fair-Contextual-Bandits" class="headerlink" title="$α$-Fair Contextual Bandits"></a>$α$-Fair Contextual Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14164">http://arxiv.org/abs/2310.14164</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddhant Chaudhary, Abhishek Sinha</li>
<li>for: 本文研究的目标是解决 $\alpha$-Fair Contextual Bandits问题，即在对抗设定中 maximize 全球 $\alpha$- fair  utility function，而不是增加总奖励。</li>
<li>methods: 作者提出了一种有效的算法，可以在全信息设定和抽象反馈设定下保证 approximately sublinear regret。</li>
<li>results: 该算法可以在对抗设定中 garantuee an approximately sublinear regret，并且可以避免闭塞效应和符合法规要求。<details>
<summary>Abstract</summary>
Contextual bandit algorithms are at the core of many applications, including recommender systems, clinical trials, and optimal portfolio selection. One of the most popular problems studied in the contextual bandit literature is to maximize the sum of the rewards in each round by ensuring a sublinear regret against the best-fixed context-dependent policy. However, in many applications, the cumulative reward is not the right objective - the bandit algorithm must be fair in order to avoid the echo-chamber effect and comply with the regulatory requirements. In this paper, we consider the $\alpha$-Fair Contextual Bandits problem, where the objective is to maximize the global $\alpha$-fair utility function - a non-decreasing concave function of the cumulative rewards in the adversarial setting. The problem is challenging due to the non-separability of the objective across rounds. We design an efficient algorithm that guarantees an approximately sublinear regret in the full-information and bandit feedback settings.
</details>
<details>
<summary>摘要</summary>
Contextual bandit algorithms are at the core of many applications, including recommender systems, clinical trials, and optimal portfolio selection. One of the most popular problems studied in the contextual bandit literature is to maximize the sum of the rewards in each round by ensuring a sublinear regret against the best-fixed context-dependent policy. However, in many applications, the cumulative reward is not the right objective - the bandit algorithm must be fair in order to avoid the echo-chamber effect and comply with the regulatory requirements. In this paper, we consider the $\alpha$-Fair Contextual Bandits problem, where the objective is to maximize the global $\alpha$-fair utility function - a non-decreasing concave function of the cumulative rewards in the adversarial setting. The problem is challenging due to the non-separability of the objective across rounds. We design an efficient algorithm that guarantees an approximately sublinear regret in the full-information and bandit feedback settings.Here's the translation in Simplified Chinese:Contextual bandit algorithms 是多种应用的核心，包括推荐系统、临床试验和最佳投资选择。文章中最受欢迎的问题是在各个回合中 maximize 奖励的和，并在最佳固定上下文依赖策略的比较下保持 sublinear 后悔。然而，在许多应用中，总奖励不是正确的目标 - 带刺机器必须公平，以避免闪电室效应和符合规定要求。在这篇文章中，我们考虑 $\alpha $- Fair Contextual Bandits 问题，其目标是在对抗设定下 maximize 全球 $\alpha $- fair  utility 函数 - 一个不递减的凹陷函数。问题具有不可分离性，使得它变得更加挑战。我们设计了一个有效的算法，以保证在充分信息和反馈设定下 approximately sublinear 后悔。
</details></li>
</ul>
<hr>
<h2 id="Promoting-Generalization-for-Exact-Solvers-via-Adversarial-Instance-Augmentation"><a href="#Promoting-Generalization-for-Exact-Solvers-via-Adversarial-Instance-Augmentation" class="headerlink" title="Promoting Generalization for Exact Solvers via Adversarial Instance Augmentation"></a>Promoting Generalization for Exact Solvers via Adversarial Instance Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14161">http://arxiv.org/abs/2310.14161</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoyang Liu, Yufei Kuang, Jie Wang, Xijun Li, Yongdong Zhang, Feng Wu</li>
<li>for: 提高离散线性 программирова（MILP）解题器的效率</li>
<li>methods: 使用学习算法提高MILP解题器的普适性和效率</li>
<li>results: 实验表明，通过生成多种增强实例，AdaSolver可以提高MILP解题器的效率，并且可以在不同的分布下实现remarkable的提高I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
Machine learning has been successfully applied to improve the efficiency of Mixed-Integer Linear Programming (MILP) solvers. However, the learning-based solvers often suffer from severe performance degradation on unseen MILP instances -- especially on large-scale instances from a perturbed environment -- due to the limited diversity of training distributions. To tackle this problem, we propose a novel approach, which is called Adversarial Instance Augmentation and does not require to know the problem type for new instance generation, to promote data diversity for learning-based branching modules in the branch-and-bound (B&B) Solvers (AdaSolver). We use the bipartite graph representations for MILP instances and obtain various perturbed instances to regularize the solver by augmenting the graph structures with a learned augmentation policy. The major technical contribution of AdaSolver is that we formulate the non-differentiable instance augmentation as a contextual bandit problem and adversarially train the learning-based solver and augmentation policy, enabling efficient gradient-based training of the augmentation policy. To the best of our knowledge, AdaSolver is the first general and effective framework for understanding and improving the generalization of both imitation-learning-based (IL-based) and reinforcement-learning-based (RL-based) B&B solvers. Extensive experiments demonstrate that by producing various augmented instances, AdaSolver leads to a remarkable efficiency improvement across various distributions.
</details>
<details>
<summary>摘要</summary>
machine learning 已经成功应用于提高混合整数线性 программирова (MILP) 解决器的效率。然而，学习基于的解决器经常在未看过的 MILP 实例上表现出严重的性能下降 --特别是大规模实例从受到扰动环境-- 由于培育分布的有限多样性。为解决这个问题，我们提出了一种新的方法，即 Adversarial Instance Augmentation，不需要知道问题类型，为学习基于分支模块在 B&B 解决器 (AdaSolver) 中提高数据多样性。我们使用 MILP 实例的双分图表示，并通过学习的扩充策略对图结构进行加工，以增强解决器的普适性。AdaSolver 的主要技术贡献在于将不 diferenciable 的实例扩充视为上下文ual Bandit 问题，并对学习基于的扩充策略进行 adversarial 训练，使得学习基于的解决器可以高效地进行梯度下降训练。根据我们所知，AdaSolver 是首个普适并有效的框架，用于理解和提高学习基于 B&B 解决器的通用化。我们的实验证明，通过生成多种扩充实例，AdaSolver 在不同的分布下可以得到很大的效率提高。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/22/cs.LG_2023_10_22/" data-id="clp88dbz900toob886uswgaam" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_10_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/22/eess.SP_2023_10_22/" class="article-date">
  <time datetime="2023-10-22T08:00:00.000Z" itemprop="datePublished">2023-10-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/22/eess.SP_2023_10_22/">eess.SP - 2023-10-22</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Submodular-Optimization-for-Placement-of-Intelligent-Reflecting-Surfaces-in-Sensing-Systems"><a href="#Submodular-Optimization-for-Placement-of-Intelligent-Reflecting-Surfaces-in-Sensing-Systems" class="headerlink" title="Submodular Optimization for Placement of Intelligent Reflecting Surfaces in Sensing Systems"></a>Submodular Optimization for Placement of Intelligent Reflecting Surfaces in Sensing Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14443">http://arxiv.org/abs/2310.14443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zahra Esmaeilbeig, Kumar Vijay Mishra, Arian Eamaz, Mojtaba Soltanalian</li>
<li>for: 这篇论文是关于智能反射表面（IRS）的优化部署和探测应用的研究。</li>
<li>methods: 本论文使用了最大共识度来决定多个IRS平台的部署，以优化探测应用。</li>
<li>results: 研究表明，使用最大共识度作为优化标准可以实现约63%的最差性能保证。<details>
<summary>Abstract</summary>
Intelligent reflecting surfaces (IRS) and their optimal deployment are the new technological frontier in sensing applications. Recently, IRS have demonstrated potential in advancing target estimation and detection. While the optimal phase-shift of IRS for different tasks has been studied extensively in the literature, the optimal placement of multiple IRS platforms for sensing applications is less explored. In this paper, we design the placement of IRS platforms for sensing by maximizing the mutual information. In particular, we use this criterion to determine an approximately optimal placement of IRS platforms to illuminate an area where the target has a hypothetical presence. After demonstrating the submodularity of the mutual information criteria, we tackle the design problem by means of a constant-factor approximation algorithm for submodular optimization. Numerical results are presented to validate the proposed submodular optimization framework for optimal IRS placement with worst case performance bounded to $1-1/e\approx 63 \%$.
</details>
<details>
<summary>摘要</summary>
《智能反射表面（IRS）和其最佳部署》是当今探测应用的新技术前沿。最近，IRS已经表现出在目标估计和检测方面的潜力。虽然literature中对不同任务的IRS阶段的优化已经得到了广泛的研究，但IRS平台的多个部署 для探测应用还尚未得到了充分的研究。在这篇论文中，我们设计了IRS平台的部署方式，以最大化互信息。特别是，我们使用这个标准来确定IRS平台的投射方式，以照明假设存在目标的区域。我们首先证明了互信息标准的子模度性，然后使用子模度优化算法来解决设计问题。 num = 1-1/e ≈ 63%。Here's the breakdown of the translation:* "Intelligent reflecting surfaces" is translated as "智能反射表面" (Zhīngnéng jiànshì biānshì)* "and their optimal deployment" is translated as "以及其最佳部署" (yǐngqí yǔ jiāojiā bèipiāo)* "are the new technological frontier in sensing applications" is translated as "是当今探测应用的新技术前沿" (shì dāngjīn tàncéng yìngyùn yìshì)* "Recently, IRS have demonstrated potential in advancing target estimation and detection" is translated as "最近，IRS已经表现出在目标估计和检测方面的潜力" (zuìjìn, IRS yǐjīng bìngxìn zhìyì yìshì)* "While the optimal phase-shift of IRS for different tasks has been studied extensively in the literature" is translated as "虽然literature中对不同任务的IRS阶段的优化已经得到了广泛的研究" ( substitude "phase-shift" with "阶段" (jìděn) in Chinese)* "the optimal placement of multiple IRS platforms for sensing applications is less explored" is translated as "IRS平台的多个部署 для探测应用还尚未得到了充分的研究" (IRS píngtiān de duō gè bèipiāo yǐng yìshì)* "In this paper, we design the placement of IRS platforms for sensing by maximizing the mutual information" is translated as "在这篇论文中，我们设计了IRS平台的部署方式，以最大化互信息" (substitute "placing" with "部署" (bèipiāo) in Chinese)* "In particular, we use this criterion to determine an approximately optimal placement of IRS platforms to illuminate an area where the target has a hypothetical presence" is translated as "特别是，我们使用这个标准来确定IRS平台的投射方式，以照明假设存在目标的区域" (substitute "illuminate" with "照明" (zhàoqǐng) in Chinese)* "After demonstrating the submodularity of the mutual information criteria" is translated as "我们首先证明了互信息标准的子模度性" (substitute "submodularity" with "子模度性" (zǐmódegè) in Chinese)* "we tackle the design problem by means of a constant-factor approximation algorithm for submodular optimization" is translated as "然后使用子模度优化算法来解决设计问题" (substitute "constant-factor" with "常数因子" (chángxīng yǐngxí) in Chinese)* "Numerical results are presented to validate the proposed submodular optimization framework for optimal IRS placement with worst case performance bounded to $1-1/e\approx 63\%$" is translated as " num = 1-1/e ≈ 63%。我们提供的子模度优化框架的数值结果，以验证优化结果的可行性" (substitute "worst case" with "最差情况" (zuìjì) in Chinese)
</details></li>
</ul>
<hr>
<h2 id="Piezoelectric-Sensors-for-Real-time-Monitoring-and-Quality-Control-in-Additive-Manufacturing"><a href="#Piezoelectric-Sensors-for-Real-time-Monitoring-and-Quality-Control-in-Additive-Manufacturing" class="headerlink" title="Piezoelectric Sensors for Real-time Monitoring and Quality Control in Additive Manufacturing"></a>Piezoelectric Sensors for Real-time Monitoring and Quality Control in Additive Manufacturing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14321">http://arxiv.org/abs/2310.14321</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rashid T. Momin</li>
<li>for: 本研究旨在为工程领域的精度制造过程提供深入理解和实用技术，尤其是在添加制造中。</li>
<li>methods: 本研究采用系统性的方法，自基本原理起推导到实践应用，以满足不同读者的需求。</li>
<li>results: 研究发现， piezoelectric 感应器在实时监测和质量控制方面具有广泛的应用前景和潜在价值，对于新手和专业人士都具有启发性和实用性。<details>
<summary>Abstract</summary>
Within the ever-evolving landscape of engineering, particularly in the dynamic domain of additive In manufacturing, a pursuit of precision and excellence in production processes takes centre stage. This research , This paper serves to give a comprehensive understanding of piezoelectric sensors, a topic that is both academically engaging and of practical significance, catering to both seasoned experts and those newly venturing into the field. Additive manufacturing, lauded for its groundbreaking potential, underscores the imperative of rigorous quality control. This introduces piezoelectric sensors, devices that may be unfamiliar to many but possess considerable potential. This paper embarks on a methodical journey, commencing with an introductory elucidation of the piezoelectric effect. It then advances to the vital role of piezoelectric sensors in real-time monitoring and quality control, unveiling their potential and relevance for newcomers and seasoned professionals alike. This research, structured systematically from fundamental principles to pragmatic applications, presents findings that are not only academically informative but also represent a substantial stride towards achieving precision and high-quality manufacturing processes in the engineering field.
</details>
<details>
<summary>摘要</summary>
在工程领域中，特别是在加工制造领域，精度和excel在生产过程中拥有中心舞台。这篇研究，这篇论文旨在为涉及到 piezoelectric 传感器的学术研究和实践提供全面的理解，它不仅对有经验的专家有启发，还对新进入该领域的人有很大的启示。加工制造被赞赏为创新的潜力，强调了严格的质量控制的重要性。这使得 piezoelectric 传感器成为了一种可能新的和有潜力的技术。这篇论文从基本原理开始，逐步向实践应用，对新手和经验手都有很大的启示。这项研究不仅是学术上的探索，也是在工程领域实现精度和高质量生产过程的一大步进。
</details></li>
</ul>
<hr>
<h2 id="FAS-assisted-NOMA-Short-Packet-Communication-Systems"><a href="#FAS-assisted-NOMA-Short-Packet-Communication-Systems" class="headerlink" title="FAS-assisted NOMA Short-Packet Communication Systems"></a>FAS-assisted NOMA Short-Packet Communication Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14251">http://arxiv.org/abs/2310.14251</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zeniSoida/pl1">https://github.com/zeniSoida/pl1</a></li>
<li>paper_authors: Jianchao Zheng, Tuo Wu, Xiazhi Lai, Cunhua Pan, Maged Elkashlan, Kai-Kit Wong</li>
<li>for:  investigate a fluid antenna system (FAS)-assisted downlink non-orthogonal multiple access (NOMA) for short-packet communications.</li>
<li>methods:  base station (BS) adopts a single fixed antenna, while both the central user (CU) and the cell-edge user (CEU) are equipped with a FAS.</li>
<li>results:  the diversity order for CU and CEU is $N$, indicating that the system performance can be considerably improved by increasing $N$.Here’s the full version in Traditional Chinese:</li>
<li>for: 这个研究探讨了一个使用流体天线系统（FAS）协助下行非共域多存取（NOMA）的短包通信系统。</li>
<li>methods: BS使用固定天线，而中央用户（CU）和红色边用户（CEU）则是配备了FAS的。</li>
<li>results: FAS帮助下，CU和CEU的多标题状况下的系统性能可以随N的增加而提高。<details>
<summary>Abstract</summary>
In this paper, we investigate a fluid antenna system (FAS)-assisted downlink non-orthogonal multiple access (NOMA) for short-packet communications. The base station (BS) adopts a single fixed antenna, while both the central user (CU) and the cell-edge user (CEU) are equipped with a FAS. Each FAS comprises $N$ flexible positions (also known as ports), linked to $N$ arbitrarily correlated Rayleigh fading channels. We derive expressions for the average block error rate (BLER) of the FAS-assisted NOMA system and provide asymptotic BLER expressions. We determine that the diversity order for CU and CEU is $N$, indicating that the system performance can be considerably improved by increasing $N$. Simulation results validate the great performance of FAS.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了一个流体天线系统（FAS）助动下link非对称多接入（NOMA） для短包通信。基站（BS）采用单一固定天线，而中央用户（CU）和边缘用户（CEU）均配备了FAS。每个FAS包括 $N$ 个 flexible位（也称为端口），与 $N$ 个互相相关的干扰谱噪抖动通道相连。我们 derive了 FAS-assisted NOMA 系统的平均块错误率（BLER）表达式，并提供了各种各样的 BLER 表达式。我们发现，CU 和 CEU 的多普通性顺序是 $N$，这表明，通过增加 $N$，系统性能可以得到显著改善。实验结果证明了 FAS 的优秀性能。
</details></li>
</ul>
<hr>
<h2 id="How-do-the-resting-EEG-preprocessing-states-affect-the-outcomes-of-postprocessing"><a href="#How-do-the-resting-EEG-preprocessing-states-affect-the-outcomes-of-postprocessing" class="headerlink" title="How do the resting EEG preprocessing states affect the outcomes of postprocessing?"></a>How do the resting EEG preprocessing states affect the outcomes of postprocessing?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15194">http://arxiv.org/abs/2310.15194</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiang Hu, Jie Ruan, Juan Hou, Pedro Antonio Valdes-Sosa, Zhao Lv</li>
<li>for: 本研究旨在探讨预处理影响后处理结果的影响，特别是预处理不足和过度预处理对频域、空间和时间领域的后处理所带来的影响。</li>
<li>methods: 研究人员使用了新 York 头部模型和多变量自 regression 模型来生成清晰 EEG（CE）作为参考数据，然后通过在 CE 基础上增加 Gaussian 噪声和失去大脑活动来生成预处理不足（IPE）和过度预处理（EPE）数据。</li>
<li>results: 研究人员发现，预处理不足和过度预处理都会导致后处理结果与 CE 之间的偏差，特别是在频域、空间和时间领域的 Statistics、多通道能量、跨 Spectra、源 imaging 粒度和贝叶率等方面。此外，研究人员还发现，PaLOSi 指标与预处理状态的变化有 statistically  significative 相关性。<details>
<summary>Abstract</summary>
Plenty of artifact removal tools and pipelines have been developed to correct the EEG recordings and discover the values below the waveforms. Without visual inspection from the experts, it is susceptible to derive improper preprocessing states, like the insufficient preprocessed EEG (IPE), and the excessive preprocessed EEG (EPE). However, little is known about the impacts of IPE or EPE on the postprocessing in the frequency, spatial and temporal domains, particularly as to the spectra and the functional connectivity (FC) analysis. Here, the clean EEG (CE) was synthesized as the ground truth based on the New-York head model and the multivariate autoregressive model. Later, the IPE and the EPE were simulated by injecting the Gaussian noise and losing the brain activities, respectively. Then, the impacts on postprocessing were quantified by the deviation caused by the IPE or EPE from the CE as to the 4 temporal statistics, the multichannel power, the cross spectra, the dispersion of source imaging, and the properties of scalp EEG network. Lastly, the association analysis was performed between the PaLOSi metric and the varying trends of postprocessing with the evolution of preprocessing states. This study shed light on how the postprocessing outcomes are affected by the preprocessing states and PaLOSi may be a potential effective quality metric.
</details>
<details>
<summary>摘要</summary>
很多遗物除去工具和管道已经开发出来 corrections EEG 记录，以便发现下面波形的值。而不经visual inspection from the experts，可能导致 derivation improper preprocessing states, such as insufficient preprocessed EEG (IPE) and excessive preprocessed EEG (EPE). However, little is known about the impacts of IPE or EPE on the postprocessing in the frequency, spatial, and temporal domains, particularly as to the spectra and the functional connectivity (FC) analysis.Here, the clean EEG (CE) was synthesized as the ground truth based on the New-York head model and the multivariate autoregressive model. Later, the IPE and the EPE were simulated by injecting Gaussian noise and losing brain activities, respectively. Then, the impacts on postprocessing were quantified by the deviation caused by the IPE or EPE from the CE as to the 4 temporal statistics, the multichannel power, the cross spectra, the dispersion of source imaging, and the properties of scalp EEG network. Lastly, the association analysis was performed between the PaLOSi metric and the varying trends of postprocessing with the evolution of preprocessing states. This study shed light on how the postprocessing outcomes are affected by the preprocessing states and PaLOSi may be a potential effective quality metric.
</details></li>
</ul>
<hr>
<h2 id="On-the-Sum-Secrecy-Rate-of-Multi-User-Holographic-MIMO-Networks"><a href="#On-the-Sum-Secrecy-Rate-of-Multi-User-Holographic-MIMO-Networks" class="headerlink" title="On the Sum Secrecy Rate of Multi-User Holographic MIMO Networks"></a>On the Sum Secrecy Rate of Multi-User Holographic MIMO Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14217">http://arxiv.org/abs/2310.14217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arthur S. de Sena, Jiguang He, Ahmed Al Hammadi, Chongwen Huang, Faouzi Bader, Merouane Debbah, Mathias Fink</li>
<li>for: 这篇论文主要针对下一代通信系统的增强多样性和多重化，探讨了扩展到安全通信领域的可能性。</li>
<li>methods: 该论文使用了meta-atom技术，并对HMIMO网络的能量和频率效率、路径损失分析、通道模型进行了分析。</li>
<li>results: 研究发现，采用adaptive&#x2F;灵活的发射功率分配（PA）可以在高信号噪声比（SNR） régime中获得显著性能提升，对两个用户情况下可以获得更多于100%的增强。<details>
<summary>Abstract</summary>
The emerging concept of extremely-large holographic multiple-input multiple-output (HMIMO), beneficial from compactly and densely packed cost-efficient radiating meta-atoms, has been demonstrated for enhanced degrees of freedom even in pure line-of-sight conditions, enabling tremendous multiplexing gain for the next-generation communication systems. Most of the reported works focus on energy and spectrum efficiency, path loss analyses, and channel modeling. The extension to secure communications remains unexplored. In this paper, we theoretically characterize the secrecy capacity of the HMIMO network with multiple legitimate users and one eavesdropper while taking into consideration artificial noise and max-min fairness. We formulate the power allocation (PA) problem and address it by following successive convex approximation and Taylor expansion. We further study the effect of fixed PA coefficients, imperfect channel state information, inter-element spacing, and the number of Eve's antennas on the sum secrecy rate. Simulation results show that significant performance gain with more than 100\% increment in the high signal-to-noise ratio (SNR) regime for the two-user case is obtained by exploiting adaptive/flexible PA compared to the case with fixed PA coefficients.
</details>
<details>
<summary>摘要</summary>
新的概念——超大型杂点多输入多输出（HMIMO）技术，具有高度压缩和高密度的成本效益的卫星体，已经在纯线路条件下实现了更高的自由度，这将导致下一代通信系统的多重化增量。大多数报道的研究都集中在能量和频率效益、距离损失分析和通道模型。然而，安全通信的扩展还没有被探讨。在这篇论文中，我们理论上Characterize HMIMO网络的机密容量，并考虑人工噪声和最大最小公平。我们将 transmit power allocation（PA）问题，通过Successive Convex Approximation和Taylor扩展解决。我们进一步研究了固定PA系数、不完全的通道状态信息、元素间距离和贝娅的天线数量对Sum secrecy rate的影响。实验结果表明，在高信号噪声比（SNR）区间，通过适应/灵活PA比与固定PA系数相比，可以获得高达100%的性能提升。
</details></li>
</ul>
<hr>
<h2 id="A-Coordinate-Descent-Approach-to-Atomic-Norm-Minimization"><a href="#A-Coordinate-Descent-Approach-to-Atomic-Norm-Minimization" class="headerlink" title="A Coordinate Descent Approach to Atomic Norm Minimization"></a>A Coordinate Descent Approach to Atomic Norm Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14182">http://arxiv.org/abs/2310.14182</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruifu Li, Danijela Cabric</li>
<li>for: 这个论文的目的是解决含有稀疏信号处理的应用中的原子范数最小化问题。</li>
<li>methods: 这篇论文提出了一种基于坐标降低的低复杂度、matrix-free的原子范数最小化解决方案。该方法利用了稀疏规化的自然性，通过坐标降低框架来实现高效的解决方案。</li>
<li>results: 该方法在数学上证明可以快速地求解原子范数最小化问题，并且在大规模问题中实现了高效的计算。通过对比ADMM和自定义内点SDP解决方案，该方法在解决稀疏问题方面明显更快。<details>
<summary>Abstract</summary>
Atomic norm minimization is of great interest in various applications of sparse signal processing including super-resolution line-spectral estimation and signal denoising. In practice, atomic norm minimization (ANM) is formulated as a semi-definite programming (SDP) which is generally hard to solve. This work introduces a low-complexity, matrix-free method for solving ANM. The method uses the framework of coordinate descent and exploits the sparsity-induced nature of atomic-norm regularization. Specifically, an equivalent, non-convex formulation of ANM is first proposed. It is then proved that applying the coordinate descent framework on the non-convex formulation leads to convergence to the global optimal point. For the case of a single measurement vector of length N in discrete fourier transform (DFT) basis, the complexity of each iteration in the coordinate descent procedure is O(N log N ), rendering the proposed method efficient even for large-scale problems. The proposed coordinate descent framework can be readily modified to solve a variety of ANM problems, including multi-dimensional ANM with multiple measurement vectors. It is easy to implement and can essentially be applied to any atomic sets as long as a corresponding rank-1 problem can be solved. Through extensive numerical simulations, it is verified that for solving sparse problems the proposed method is much faster than the alternating direction method of multipliers (ADMM) or the customized interior point SDP solver.
</details>
<details>
<summary>摘要</summary>
“原子 нор 最小化是各种各样的应用，包括超解析和信号噪声除除。但是在实践中，原子 norm 最小化（ANM）通常被表示为半definite 程序（SDP），这通常很难解决。这项工作提出了一种低复杂度、缺少矩阵的方法来解决 ANM。该方法使用坐标 descent 框架，利用原子规定的稀疏性来减少计算复杂性。具体来说，首先提出了 ANM 的非конvex 表述，然后证明通过坐标 descent 框架处理非конvex 表述可以达到全局最优点。在单个测量向量长度为 N 的抽象快推（DFT）基础下，每次迭代的复杂度为 O(N log N)，这使得提出的方法在大规模问题上高效。此外，这种坐标 descent 框架可以轻松地修改，以解决多维 ANM 问题，包括多个测量向量。它易于实现，可以适用于任何原子集，只要可以解决相应的 rank-1 问题。经过广泛的数值实验，证明了用于解决稀疏问题的提出方法比 ADMM 或自定义内点 SDP 解决器更快。”
</details></li>
</ul>
<hr>
<h2 id="Spatial-Sigma-Delta-Modulation-for-Coarsely-Quantized-Massive-MIMO-Downlink-Flexible-Designs-by-Convex-Optimization"><a href="#Spatial-Sigma-Delta-Modulation-for-Coarsely-Quantized-Massive-MIMO-Downlink-Flexible-Designs-by-Convex-Optimization" class="headerlink" title="Spatial Sigma-Delta Modulation for Coarsely Quantized Massive MIMO Downlink: Flexible Designs by Convex Optimization"></a>Spatial Sigma-Delta Modulation for Coarsely Quantized Massive MIMO Downlink: Flexible Designs by Convex Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14179">http://arxiv.org/abs/2310.14179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wai-Yiu Keung, Wing-Kin Ma</li>
<li>for: 本文考虑了多用户大量MIMO下降频率预编码，使用低分辨率数字到分析转换器（DAC）。</li>
<li>methods: 本文使用了Sigma-Delta模ulation来控制量化误差效应。</li>
<li>results: 数字实现中使用Sigma-Delta模ulation可以达到近似于无量化的性能，但需要考虑angle sector和数量化级别等因素。<details>
<summary>Abstract</summary>
This paper considers the context of multiuser massive MIMO downlink precoding with low-resolution digital-to-analog converters (DACs) at the transmitter. This subject is motivated by the consideration that it is expensive to employ high-resolution DACs for practical massive MIMO implementations. The challenge with using low-resolution DACs is to overcome the detrimental quantization error effects. Recently, spatial Sigma-Delta modulation has arisen as a viable way to put quantization errors under control. This approach takes insight from temporal Sigma-Delta modulation in classical DAC studies. Assuming a 1D uniform linear transmit antenna array, the principle is to shape the quantization errors in space such that the shaped quantization errors are pushed away from the user-serving angle sector. In the previous studies, spatial Sigma-Delta modulation was performed by direct application of the basic first- and second-order modulators from the Sigma-Delta literature. In this paper, we develop a general Sigma-Delta modulator design framework for any given order, for any given number of quantization levels, and for any given angle sector. We formulate our design as a problem of maximizing the signal-to-quantization-and-noise ratios experienced by the users. The formulated problem is convex and can be efficiently solved by available solvers. Our proposed framework offers the alternative option of focused quantization error suppression in accordance with channel state information. Our framework can also be extended to 2D planar transmit antenna arrays. We perform numerical study under different operating conditions, and the numerical results suggest that, given a moderate number of quantization levels, say, 5 to 7 levels, our optimization-based Sigma-Delta modulation schemes can lead to bit error rate performance close to that of the unquantized counterpart.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Factor-Graph-Processing-for-Dual-Blind-Deconvolution-at-ISAC-Receiver"><a href="#Factor-Graph-Processing-for-Dual-Blind-Deconvolution-at-ISAC-Receiver" class="headerlink" title="Factor Graph Processing for Dual-Blind Deconvolution at ISAC Receiver"></a>Factor Graph Processing for Dual-Blind Deconvolution at ISAC Receiver</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14167">http://arxiv.org/abs/2310.14167</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roman Jacome, Edwin Vargas, Kumar Vijay Mishra, Brian M. Sadler, Henry Arguello</li>
<li>for: 本研究针对的是 интеграцион感知通信（ISAC）系统，它们可以同时访问和利用有限的电磁频谱资源。</li>
<li>methods: 本研究使用了线性状态空间模型（LSSM）来模型信号模型的动态变化。</li>
<li>results: 实验结果表明，使用了效果差分方法（EM）算法可以高效地估算无知变量，包括噪声存在的情况下。<details>
<summary>Abstract</summary>
Integrated sensing and communications (ISAC) systems have gained significant interest because of their ability to jointly and efficiently access, utilize, and manage the scarce electromagnetic spectrum. The co-existence approach toward ISAC focuses on the receiver processing of overlaid radar and communications signals coming from independent transmitters. A specific ISAC coexistence problem is dual-blind deconvolution (DBD), wherein the transmit signals and channels of both radar and communications are unknown to the receiver. Prior DBD works ignore the evolution of the signal model over time. In this work, we consider a dynamic DBD scenario using a linear state space model (LSSM) such that, apart from the transmit signals and channels of both systems, the LSSM parameters are also unknown. We employ a factor graph representation to model these unknown variables. We avoid the conventional matrix inversion approach to estimate the unknown variables by using an efficient expectation-maximization algorithm, where each iteration employs a Gaussian message passing over the factor graph structure. Numerical experiments demonstrate the accurate estimation of radar and communications channels, including in the presence of noise.
</details>
<details>
<summary>摘要</summary>
интегрированные сенсинг и коммуникации (ISAC) 系统在过去几年中得到了广泛的关注，因为它们可以同时和有效地访问、利用和管理有限的电磁 спектrum。在 ISAC 的共存方法中，接收器处理来自独立的发送器的掩码过的雷达和通信信号。特定的 ISAC 共存问题是双目掩码分解 (DBD)，其中雷达和通信系统的发送信号和通道都是接收器不知道的。先前的 DBD 工作忽略了信号模型的时间演化。在这种情况下，我们使用线性状态空间模型 (LSSM)，以便除了雷达和通信系统的发送信号和通道之外，还不知道 LSSM 参数。我们使用因果图表示这些未知变量。我们避免了传统的矩阵逆解方法，而是使用高效的期望最大化算法，每次迭代都使用 Gaussian 消息传递在因果图结构上。实验示出了准确地估计雷达和通信通道，包括在噪声存在的情况下。
</details></li>
</ul>
<hr>
<h2 id="Photoplethysmography-based-atrial-fibrillation-detection-an-updated-review-from-July-2019"><a href="#Photoplethysmography-based-atrial-fibrillation-detection-an-updated-review-from-July-2019" class="headerlink" title="Photoplethysmography based atrial fibrillation detection: an updated review from July 2019"></a>Photoplethysmography based atrial fibrillation detection: an updated review from July 2019</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14155">http://arxiv.org/abs/2310.14155</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Ding, Ran Xiao, Weijia Wang, Elizabeth Holdsworth, Xiao Hu</li>
<li>for: 本研究旨在寻找最新的抽血 photoplethysmography (PPG) 技术，用于不间断的 атrial fibrillation (AF) 监测，以提高患者的健康状况。</li>
<li>methods: 本研究采用了数字卫生和人工智能 (AI) 解决方案，检查了59 项研究，包括统计方法、传统机器学习技术和深度学习方法。</li>
<li>results: 研究发现，使用 PPG 技术可以准确地检测 AF，并且可以提高患者的健康状况。同时，研究还发现了在这个领域存在的挑战。<details>
<summary>Abstract</summary>
Atrial fibrillation (AF) is a prevalent cardiac arrhythmia associated with significant health ramifications, including an elevated susceptibility to ischemic stroke, heart disease, and heightened mortality. Photoplethysmography (PPG) has emerged as a promising technology for continuous AF monitoring for its cost-effectiveness and widespread integration into wearable devices. Our team previously conducted an exhaustive review on PPG-based AF detection before June 2019. However, since then, more advanced technologies have emerged in this field. This paper offers a comprehensive review of the latest advancements in PPG-based AF detection, utilizing digital health and artificial intelligence (AI) solutions, within the timeframe spanning from July 2019 to December 2022. Through extensive exploration of scientific databases, we have identified 59 pertinent studies. Our comprehensive review encompasses an in-depth assessment of the statistical methodologies, traditional machine learning techniques, and deep learning approaches employed in these studies. In addition, we address the challenges encountered in the domain of PPG-based AF detection. Furthermore, we maintain a dedicated website to curate the latest research in this area, with regular updates on a regular basis.
</details>
<details>
<summary>摘要</summary>
AF（atrialfibrillation）是一种常见的心脏 arrhythmia，与 significativ health consequences associated， including elevated risk of ischemic stroke, heart disease, and increased mortality. Photoplethysmography (PPG) has emerged as a promising technology for continuous AF monitoring due to its cost-effectiveness and widespread integration into wearable devices. Our team previously conducted an exhaustive review of PPG-based AF detection before June 2019. However, since then, more advanced technologies have emerged in this field. This paper provides a comprehensive review of the latest advancements in PPG-based AF detection, utilizing digital health and artificial intelligence (AI) solutions, within the timeframe spanning from July 2019 to December 2022. Through extensive exploration of scientific databases, we have identified 59 pertinent studies. Our comprehensive review encompasses an in-depth assessment of the statistical methodologies, traditional machine learning techniques, and deep learning approaches employed in these studies. In addition, we address the challenges encountered in the domain of PPG-based AF detection. Furthermore, we maintain a dedicated website to curate the latest research in this area, with regular updates on a regular basis.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/22/eess.SP_2023_10_22/" data-id="clp88dc8101gbob889evjdj4a" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_10_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/21/cs.SD_2023_10_21/" class="article-date">
  <time datetime="2023-10-21T15:00:00.000Z" itemprop="datePublished">2023-10-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/21/cs.SD_2023_10_21/">cs.SD - 2023-10-21</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Temporal-convolutional-neural-networks-to-generate-a-head-related-impulse-response-from-one-direction-to-another"><a href="#Temporal-convolutional-neural-networks-to-generate-a-head-related-impulse-response-from-one-direction-to-another" class="headerlink" title="Temporal convolutional neural networks to generate a head-related impulse response from one direction to another"></a>Temporal convolutional neural networks to generate a head-related impulse response from one direction to another</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14018">http://arxiv.org/abs/2310.14018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tatsuki Kobayashi, Yoshiko Maruyama, Isao Nambu, Shohei Yano, Yasuhiro Wada</li>
<li>for: Virtual sound synthesis technology allows users to perceive spatial sound through headphones or earphones, but accurate virtual sound requires an individual head-related transfer function (HRTF).</li>
<li>methods: This study proposed a method to generate HRTFs from one direction to the other using temporal convolutional neural networks (TCNs) and publicly available datasets in the horizontal plane.</li>
<li>results: The proposed method successfully generated HRIRs for directions other than the front direction in the dataset, and was found to be equivalent to the measured HRIRs in a new dataset through behavioral experiments with human participants. These results suggest that the proposed TCNs can be used to generate personalized HRIRs for virtual sound.Here’s the summary in Traditional Chinese as well, for your reference:</li>
<li>for: 虚拟 зву频技术允许使用者透过耳机或耳筒听到三维音频，但是精准的虚拟音频需要个人化头部转换函数（HRTF）。</li>
<li>methods: 这项研究提出了将HRTF从一个方向转换到另一个方向的方法，使用了时间卷积神经网络（TCN）和公共可用数据集在水平面上进行训练。</li>
<li>results: 提议的方法成功将HRIRs从其他方向转换到前方方向，并且在新的数据集上进行了训练。Behavioral实验显示，生成的HRIRs与实验测量的HRIRs相等。这些结果表示，提议的TCNs可以从一个方向转换到另一个方向，实现个人化虚拟音频。<details>
<summary>Abstract</summary>
Virtual sound synthesis is a technology that allows users to perceive spatial sound through headphones or earphones. However, accurate virtual sound requires an individual head-related transfer function (HRTF), which can be difficult to measure due to the need for a specialized environment. In this study, we proposed a method to generate HRTFs from one direction to the other. To this end, we used temporal convolutional neural networks (TCNs) to generate head-related impulse responses (HRIRs). To train the TCNs, publicly available datasets in the horizontal plane were used. Using the trained networks, we successfully generated HRIRs for directions other than the front direction in the dataset. We found that the proposed method successfully generated HRIRs for publicly available datasets. To test the generalization of the method, we measured the HRIRs of a new dataset and tested whether the trained networks could be used for this new dataset. Although the similarity evaluated by spectral distortion was slightly degraded, behavioral experiments with human participants showed that the generated HRIRs were equivalent to the measured ones. These results suggest that the proposed TCNs can be used to generate personalized HRIRs from one direction to another, which could contribute to the personalization of virtual sound.
</details>
<details>
<summary>摘要</summary>
虚拟声音合成技术可以让用户通过headset或earphone感受到三维声音。然而，实际的虚拟声音需要个人头部相关传输函数（HRTF），这可以因特殊环境而困难测量。在这个研究中，我们提出了一种方法，可以从一个方向转换到另一个方向的HRTF。为此，我们使用了时间卷积神经网络（TCN）生成头部相关冲击响应（HRIR）。使用训练好的网络，我们成功地生成了不同方向的HRIR。我们发现，提出的方法可以成功地生成不同方向的HRIR，并且在新的数据集中测试了该方法的通用性。虽然spectral distortion评估中的相似性略差，但人类参与者的行为实验表明，生成的HRIR与测量的HRIR相当。这些结果表明，提出的TCN可以用于个人化虚拟声音。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/21/cs.SD_2023_10_21/" data-id="clp88dc200119ob88a6zy8itl" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_10_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/21/eess.AS_2023_10_21/" class="article-date">
  <time datetime="2023-10-21T14:00:00.000Z" itemprop="datePublished">2023-10-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/21/eess.AS_2023_10_21/">eess.AS - 2023-10-21</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="SwG-former-Sliding-window-Graph-Convolutional-Network-Integrated-with-Conformer-for-Sound-Event-Localization-and-Detection"><a href="#SwG-former-Sliding-window-Graph-Convolutional-Network-Integrated-with-Conformer-for-Sound-Event-Localization-and-Detection" class="headerlink" title="SwG-former: Sliding-window Graph Convolutional Network Integrated with Conformer for Sound Event Localization and Detection"></a>SwG-former: Sliding-window Graph Convolutional Network Integrated with Conformer for Sound Event Localization and Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14016">http://arxiv.org/abs/2310.14016</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiming Huang, Qinghua Huang, Liyan Ma, Zhengyu Chen, Chuan Wang</li>
<li>for: 本研究旨在提高Sound Event Localization and Detection（SELD）系统的性能，特别是在自然空间声学环境下。</li>
<li>methods: 本研究提出了一种基于图表示的novel graph convolutional network（GCN）模型，具有同时抽取空间特征和时间特征的能力。此外，一种robust Conv2dAgg函数也被提出，用于协调邻居特征。</li>
<li>results: 对比与现有的先进SELD模型，本研究的SwG-former模型在同一个声学环境下表现出了superior的性能。此外，将SwG模块整合到EINV2网络中，得到的SwG-EINV2模型也超过了现有的SOTA方法。<details>
<summary>Abstract</summary>
Sound event localization and detection (SELD) is a joint task of sound event detection (SED) and direction of arrival (DoA) estimation. SED mainly relies on temporal dependencies to distinguish different sound classes, while DoA estimation depends on spatial correlations to estimate source directions. To jointly optimize two subtasks, the SELD system should extract spatial correlations and model temporal dependencies simultaneously. However, numerous models mainly extract spatial correlations and model temporal dependencies separately. In this paper, the interdependence of spatial-temporal information in audio signals is exploited for simultaneous extraction to enhance the model performance. In response, a novel graph representation leveraging graph convolutional network (GCN) in non-Euclidean space is developed to extract spatial-temporal information concurrently. A sliding-window graph (SwG) module is designed based on the graph representation. It exploits sliding-windows with different sizes to learn temporal context information and dynamically constructs graph vertices in the frequency-channel (F-C) domain to capture spatial correlations. Furthermore, as the cornerstone of message passing, a robust Conv2dAgg function is proposed and embedded into the SwG module to aggregate the features of neighbor vertices. To improve the performance of SELD in a natural spatial acoustic environment, a general and efficient SwG-former model is proposed by integrating the SwG module with the Conformer. It exhibits superior performance in comparison to recent advanced SELD models. To further validate the generality and efficiency of the SwG-former, it is seamlessly integrated into the event-independent network version 2 (EINV2) called SwG-EINV2. The SwG-EINV2 surpasses the state-of-the-art (SOTA) methods under the same acoustic environment.
</details>
<details>
<summary>摘要</summary>
声音事件地点Localization和检测（SELD）是一个joint任务，它包括声音事件检测（SED）和方向来源估计（DoA）。SED主要基于时间关系来分辨不同的声音类型，而DoA估计则基于空间相关性来估计源向量。为了同时优化两个子任务，SELD系统应该同时提取空间相关性和模型时间关系。然而，许多模型通常是分开提取空间相关性和时间关系。在这篇论文中，我们利用声音信号中的空间-时间信息相互关系，并将其同时提取出来，以提高模型性能。为此，我们开发了一种基于图表示的新型图 convolutional neural network（GCN）模型，可以同时提取空间-时间信息。我们还设计了一个基于图表示的滑动窗口模块（SwG），它利用不同的窗口大小来学习时间上下文信息，并在频道频率（F-C）空间中动态构建图顶点，以捕捉空间相关性。此外，我们还提出了一种Robust Conv2dAgg函数，用于在图顶点之间进行消息传递。为了在自然的空间声音环境中提高SELD性能，我们提出了一种通用和高效的SwG-former模型，其由SwG模块和Conformer结合而成。该模型在与现有高级SELD模型进行比较时表现出色。此外，我们还将SwG-former模型与事件独立网络版2（EINV2）结合，得到了SwG-EINV2模型。SwG-EINV2模型在同一个声音环境中超过了现有的最佳方法。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/21/eess.AS_2023_10_21/" data-id="clp88dc3k0152ob880qp69jau" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_10_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/21/cs.CV_2023_10_21/" class="article-date">
  <time datetime="2023-10-21T13:00:00.000Z" itemprop="datePublished">2023-10-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/21/cs.CV_2023_10_21/">cs.CV - 2023-10-21</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Zero-shot-Learning-of-Individualized-Task-Contrast-Prediction-from-Resting-state-Functional-Connectomes"><a href="#Zero-shot-Learning-of-Individualized-Task-Contrast-Prediction-from-Resting-state-Functional-Connectomes" class="headerlink" title="Zero-shot Learning of Individualized Task Contrast Prediction from Resting-state Functional Connectomes"></a>Zero-shot Learning of Individualized Task Contrast Prediction from Resting-state Functional Connectomes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14105">http://arxiv.org/abs/2310.14105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minh Nguyen, Gia H. Ngo, Mert R. Sabuncu</li>
<li>for: 可以用resting-state functional MRI（rsfMRI）scan来预测任务触发活动</li>
<li>methods: 使用machine learning（ML）模型，通过对suffix pair的resting-state和任务触发fMRI scan进行训练</li>
<li>results: 可以预测 novel task的活动，并且与state-of-the-art模型的in-domain预测相当In English, this would be:</li>
<li>for: Using resting-state functional MRI (rsfMRI) scans to predict task-evoked activity</li>
<li>methods: Using machine learning (ML) models, trained on paired resting-state and task-evoked fMRI scans</li>
<li>results: Can predict activity for novel tasks, and is competitive with state-of-the-art models’ in-domain predictions<details>
<summary>Abstract</summary>
Given sufficient pairs of resting-state and task-evoked fMRI scans from subjects, it is possible to train ML models to predict subject-specific task-evoked activity using resting-state functional MRI (rsfMRI) scans. However, while rsfMRI scans are relatively easy to collect, obtaining sufficient task fMRI scans is much harder as it involves more complex experimental designs and procedures. Thus, the reliance on scarce paired data limits the application of current techniques to only tasks seen during training. We show that this reliance can be reduced by leveraging group-average contrasts, enabling zero-shot predictions for novel tasks. Our approach, named OPIC (short for Omni-Task Prediction of Individual Contrasts), takes as input a subject's rsfMRI-derived connectome and a group-average contrast, to produce a prediction of the subject-specific contrast. Similar to zero-shot learning in large language models using special inputs to obtain answers for novel natural language processing tasks, inputting group-average contrasts guides the OPIC model to generalize to novel tasks unseen in training. Experimental results show that OPIC's predictions for novel tasks are not only better than simple group-averages, but are also competitive with a state-of-the-art model's in-domain predictions that was trained using in-domain tasks' data.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:可以通过 sufficient pairs of resting-state and task-evoked fMRI scans from subjects 来训练机器学习（ML）模型，预测个体特定的任务触发活动使用 resting-state functional MRI（rsfMRI）scans。然而，而 rsfMRI scans 相对容易收集，而获取足够的任务 fMRI scans 则更加复杂，需要更复杂的实验设计和过程。因此，现有技术的应用受到了scarce paired data的限制，只能在训练中使用已经看到的任务。我们显示，可以通过利用 group-average contrasts 降低这种限制，实现 zero-shot 预测 Novel task。我们的方法，名为 OPIC（short for Omni-Task Prediction of Individual Contrasts），输入一个subject的 rsfMRI-derived connectome 和 group-average contrast，以生成一个subject-specific contrast 预测。与大型语言模型使用特定输入来获取 novel natural language processing task 的答案类似，输入 group-average contrasts 使得 OPIC 模型能够泛化到 novel task 中。实验结果表明，OPIC 的预测对 Novel task 不仅 луч于单纯的 group-average，而且与一种状态之前的模型在预测中的性能也很竞争。
</details></li>
</ul>
<hr>
<h2 id="Unleashing-Modified-Deep-Learning-Models-in-Efficient-COVID19-Detection"><a href="#Unleashing-Modified-Deep-Learning-Models-in-Efficient-COVID19-Detection" class="headerlink" title="Unleashing Modified Deep Learning Models in Efficient COVID19 Detection"></a>Unleashing Modified Deep Learning Models in Efficient COVID19 Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14081">http://arxiv.org/abs/2310.14081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Aminul Islam, Shabbir Ahmed Shuvo, Mohammad Abu Tareq Rony, M Raihan, Md Abu Sufian</li>
<li>for: 这个研究旨在提高COVID-19预测和检测精度，以帮助医疗机构、政策制定者和研究人员做出更加有知识基础的决策，从而减少COVID-19和其他传染病的影响。</li>
<li>methods: 这个研究使用了深度学习技术，特别是MobileNet V3、DenseNet201和GoogleNet Inception V1等模型，以及损失优化和可扩展批处理normalization等策略，以提高预测模型的性能和鲁棒性。</li>
<li>results: 研究发现，使用MobileNet V3、DenseNet201和GoogleNet Inception V1等模型可以实现高度的预测精度，并且可以结合损失优化和可扩展批处理normalization来进一步提高预测模型的性能和鲁棒性。<details>
<summary>Abstract</summary>
The COVID19 pandemic, a unique and devastating respiratory disease outbreak, has affected global populations as the disease spreads rapidly. Recent Deep Learning breakthroughs may improve COVID19 prediction and forecasting as a tool of precise and fast detection, however, current methods are still being examined to achieve higher accuracy and precision. This study analyzed the collection contained 8055 CT image samples, 5427 of which were COVID cases and 2628 non COVID. The 9544 Xray samples included 4044 COVID patients and 5500 non COVID cases. The most accurate models are MobileNet V3 (97.872 percent), DenseNet201 (97.567 percent), and GoogleNet Inception V1 (97.643 percent). High accuracy indicates that these models can make many accurate predictions, as well as others, are also high for MobileNetV3 and DenseNet201. An extensive evaluation using accuracy, precision, and recall allows a comprehensive comparison to improve predictive models by combining loss optimization with scalable batch normalization in this study. Our analysis shows that these tactics improve model performance and resilience for advancing COVID19 prediction and detection and shows how Deep Learning can improve disease handling. The methods we suggest would strengthen healthcare systems, policymakers, and researchers to make educated decisions to reduce COVID19 and other contagious diseases.   CCS CONCEPTS Covid,Deep Learning, Image Processing   KEYWORDS Covid, Deep Learning, DenseNet201, MobileNet, ResNet, DenseNet, GoogleNet, Image Processing, Disease Detection.
</details>
<details>
<summary>摘要</summary>
COVID-19 流行病，一种独特且肇事的呼吸疾病爆发，对全球人口产生了深触的影响。 current Deep Learning 突破可能提高 COVID-19 预测和预测，作为精确和快速检测工具。但是，当前方法仍在进行评估，以提高准确率和精度。本研究分析了包含 8055 个 CT 图像样本，其中 5427 个是 COVID  случа例，2628 个是非 COVID  случа例。9544 个 X-ray 样本中包括 4044 个 COVID 病例和 5500 个非 COVID 病例。最准确的模型是 MobileNet V3（97.872%），DenseNet201（97.567%）和 GoogleNet Inception V1（97.643%）。高准确率表明这些模型可以做出许多准确预测，同时其他模型也具有高准确率。通过精度、精度和回归的全面评估，我们可以对改进预测模型进行比较。我们的分析表明，通过捆绑损失优化与可扩展批处理normalization可以提高模型性能和抗预测能力。这些方法可以强化医疗系统、政策制定者和研究人员，以便根据 COVID-19 和其他传染病的情况进行教育的决策。关键词： COVID-19，Deep Learning， DenseNet201， MobileNet， ResNet， DenseNet， GoogleNet， Image Processing，疾病检测。
</details></li>
</ul>
<hr>
<h2 id="Concept-based-Anomaly-Detection-in-Retail-Stores-for-Automatic-Correction-using-Mobile-Robots"><a href="#Concept-based-Anomaly-Detection-in-Retail-Stores-for-Automatic-Correction-using-Mobile-Robots" class="headerlink" title="Concept-based Anomaly Detection in Retail Stores for Automatic Correction using Mobile Robots"></a>Concept-based Anomaly Detection in Retail Stores for Automatic Correction using Mobile Robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14063">http://arxiv.org/abs/2310.14063</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya Kapoor, Vartika Sengar, Nijil George, Vighnesh Vatsal, Jayavardhana Gubbi, Balamuralidhar P, Arpan Pal</li>
<li>for: 这 paper 的目的是提出一种基于视transformer（ViT）的概念异常检测方法，用于检测商店中的货物错位和缺失。</li>
<li>methods: 该方法使用 auto-encoder 架构，然后使用异常检测在准确空间进行异常检测。</li>
<li>results: 该方法在 RP2K 数据集上的峰成功率为 89.90%，比基本模型的标准 ViT auto-encoder 高出 8.10%。<details>
<summary>Abstract</summary>
Tracking of inventory and rearrangement of misplaced items are some of the most labor-intensive tasks in a retail environment. While there have been attempts at using vision-based techniques for these tasks, they mostly use planogram compliance for detection of any anomalies, a technique that has been found lacking in robustness and scalability. Moreover, existing systems rely on human intervention to perform corrective actions after detection. In this paper, we present Co-AD, a Concept-based Anomaly Detection approach using a Vision Transformer (ViT) that is able to flag misplaced objects without using a prior knowledge base such as a planogram. It uses an auto-encoder architecture followed by outlier detection in the latent space. Co-AD has a peak success rate of 89.90% on anomaly detection image sets of retail objects drawn from the RP2K dataset, compared to 80.81% on the best-performing baseline of a standard ViT auto-encoder. To demonstrate its utility, we describe a robotic mobile manipulation pipeline to autonomously correct the anomalies flagged by Co-AD. This work is ultimately aimed towards developing autonomous mobile robot solutions that reduce the need for human intervention in retail store management.
</details>
<details>
<summary>摘要</summary>
<<SYS>>销售环境中最具压力的任务之一是存储和重新排序丢失的商品。而使用视觉技术进行这些任务的尝试已经存在，但大多数使用计划图合规性进行异常检测，这种技术缺乏可靠性和扩展性。此外，现有系统往往需要人工干预进行 corrections 。在这篇论文中，我们提出了 Co-AD，一种基于概念的异常检测方法，使用视觉转换器（ViT）来识别丢失的物品，不需要使用先知库如计划图。它使用自适应网络架构，然后在幽默空间进行异常检测。Co-AD 在 RP2K 数据集上的峰值成功率为 89.90%，比最佳基eline的标准 ViT 自适应网络架构高出 8.19%。为了证明其实用性，我们描述了一种基于移动抓取机的自动化corrctions 管理管道。这项工作最终目标是开发自动化移动机器人解决方案，减少零售店管理中的人工干预。
</details></li>
</ul>
<hr>
<h2 id="Training-Image-Derivatives-Increased-Accuracy-and-Universal-Robustness"><a href="#Training-Image-Derivatives-Increased-Accuracy-and-Universal-Robustness" class="headerlink" title="Training Image Derivatives: Increased Accuracy and Universal Robustness"></a>Training Image Derivatives: Increased Accuracy and Universal Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14045">http://arxiv.org/abs/2310.14045</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vsevolod I. Avrutskiy<br>for:* The paper is written for the problem of image analysis, specifically reconstructing the vertices of a cube based on its image.methods:* The paper uses derivative training, a method that computes the derivatives of the output values in the forward pass and includes them in the cost function to improve the accuracy of the neural network.* The paper also uses a gradient-based algorithm to minimize the cost function with respect to the weights.results:* The paper obtains 25 times more accurate results for noiseless inputs by training the derivatives with respect to the 6 degrees of freedom of the cube.* The paper also provides insights into the robustness problem, including two types of network vulnerabilities and a trade-off between accuracy and robustness.Here is the text in Simplified Chinese:for:* 这篇论文是为图像分析问题写的，具体来说是根据图像重建三角形的顶点。methods:* 这篇论文使用 derive 训练，在前向传递中计算输出值的导数，并将其包含在成本函数中以提高神经网络的准确性。* 这篇论文还使用一种基于梯度的算法来最小化成本函数中的权重。results:* 这篇论文在噪声为零的输入上达到了25倍的更高精度。* 这篇论文还提供了图像分析问题中的 robustness 问题的重要情况，包括神经网络中两种类型的抵触性和两种类型的图像变化。<details>
<summary>Abstract</summary>
Derivative training is a well-known method to improve the accuracy of neural networks. In the forward pass, not only the output values are computed, but also their derivatives, and their deviations from the target derivatives are included in the cost function, which is minimized with respect to the weights by a gradient-based algorithm. So far, this method has been implemented for relatively low-dimensional tasks. In this study, we apply the approach to the problem of image analysis. We consider the task of reconstructing the vertices of a cube based on its image. By training the derivatives with respect to the 6 degrees of freedom of the cube, we obtain 25 times more accurate results for noiseless inputs. The derivatives also provide important insights into the robustness problem, which is currently understood in terms of two types of network vulnerabilities. The first type is small perturbations that dramatically change the output, and the second type is substantial image changes that the network erroneously ignores. They are currently considered as conflicting goals, since conventional training methods produce a trade-off. The first type can be analyzed via the gradient of the network, but the second type requires human evaluation of the inputs, which is an oracle substitute. For the task at hand, the nearest neighbor oracle can be defined, and the knowledge of derivatives allows it to be expanded into Taylor series. This allows to perform the first-order robustness analysis that unifies both types of vulnerabilities, and to implement robust training that eliminates any trade-offs, so that accuracy and robustness are limited only by network capacity.
</details>
<details>
<summary>摘要</summary>
偏导训练是一种通用的方法，可以提高神经网络的准确率。在前向传播中，不仅计算输出值，还计算其导数和与目标导数的差异，并将其包含在权重下降的成本函数中，通过梯度基本算法进行最小化。到目前为止，这种方法已经实现在相对低维度的任务上。在这项研究中，我们将这种方法应用到图像分析任务中，即根据图像推算出立方体的顶点坐标。通过对立方体的6个自由度进行导数训练，我们可以获得25倍的精度，对于噪声无效输入。导数还提供了对稳定性问题的重要视角，这个问题目前被理解为神经网络的两种类型敏感性。第一种是小幅度的输入修改，导致输出异常变化，第二种是大规模的图像修改，神经网络错误地忽略这些修改。这两种类型的敏感性目前被视为矛盾目标， convent ional 训练方法会产生交易。第一种可以通过神经网络的梯度进行分析，但第二种需要人工评估输入，这是一个oracle substitute。对于这个任务， nearest neighbor oracle 可以定义，并且导数的知识允许其扩展为泰勒级数。这样可以实现首领稳定性分析，并将稳定性和准确率限制在神经网络容量之间。
</details></li>
</ul>
<hr>
<h2 id="You-Only-Condense-Once-Two-Rules-for-Pruning-Condensed-Datasets"><a href="#You-Only-Condense-Once-Two-Rules-for-Pruning-Condensed-Datasets" class="headerlink" title="You Only Condense Once: Two Rules for Pruning Condensed Datasets"></a>You Only Condense Once: Two Rules for Pruning Condensed Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14019">http://arxiv.org/abs/2310.14019</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang He, Lingao Xiao, Joey Tianyi Zhou</li>
<li>For: 提高训练效率，适应设备上的限制条件。* Methods: 使用两个简单的数据采样规则：低LBPE分数和平衡构建。* Results: 在ConvNet、ResNet和DenseNet网络上，在CIFAR-10、CIFAR-100和ImageNet数据集上达到了6.98-8.89%和6.31-23.92%的准确率提升。<details>
<summary>Abstract</summary>
Dataset condensation is a crucial tool for enhancing training efficiency by reducing the size of the training dataset, particularly in on-device scenarios. However, these scenarios have two significant challenges: 1) the varying computational resources available on the devices require a dataset size different from the pre-defined condensed dataset, and 2) the limited computational resources often preclude the possibility of conducting additional condensation processes. We introduce You Only Condense Once (YOCO) to overcome these limitations. On top of one condensed dataset, YOCO produces smaller condensed datasets with two embarrassingly simple dataset pruning rules: Low LBPE Score and Balanced Construction. YOCO offers two key advantages: 1) it can flexibly resize the dataset to fit varying computational constraints, and 2) it eliminates the need for extra condensation processes, which can be computationally prohibitive. Experiments validate our findings on networks including ConvNet, ResNet and DenseNet, and datasets including CIFAR-10, CIFAR-100 and ImageNet. For example, our YOCO surpassed various dataset condensation and dataset pruning methods on CIFAR-10 with ten Images Per Class (IPC), achieving 6.98-8.89% and 6.31-23.92% accuracy gains, respectively. The code is available at: https://github.com/he-y/you-only-condense-once.
</details>
<details>
<summary>摘要</summary>
dataset 缩减是训练效率的重要工具，尤其在设备上进行训练时。但是这些情况有两个主要挑战：1）设备上的计算资源不断变化，需要不同于预先定义的缩减 dataset size，2）设备上的计算资源frequently precludes the possibility of conducting additional condensation processes。我们介绍了“仅需缩减一次”（YOCO）来解决这些问题。YOCO 可以生成更小的缩减 dataset，并且可以灵活地调整 dataset size 以适应不同的计算限制。此外，YOCO 可以消除额外的缩减 процес，这可以是计算昂费的。我们的实验结果显示，YOCO 在 ConvNet、ResNet 和 DenseNet 等网络上，以及 CIFAR-10、CIFAR-100 和 ImageNet 等 dataset 上，均有着superior的表现。例如，我们在 CIFAR-10 上，使用 YOCO 可以从原始的 10 个图像每个类别（IPC）开始，获得 6.98-8.89% 和 6.31-23.92% 的精度提升。相关代码可以在 GitHub 上找到：https://github.com/he-y/you-only-condense-once。
</details></li>
</ul>
<hr>
<h2 id="Ophthalmic-Biomarker-Detection-Using-Ensembled-Vision-Transformers-–-Winning-Solution-to-IEEE-SPS-VIP-Cup-2023"><a href="#Ophthalmic-Biomarker-Detection-Using-Ensembled-Vision-Transformers-–-Winning-Solution-to-IEEE-SPS-VIP-Cup-2023" class="headerlink" title="Ophthalmic Biomarker Detection Using Ensembled Vision Transformers – Winning Solution to IEEE SPS VIP Cup 2023"></a>Ophthalmic Biomarker Detection Using Ensembled Vision Transformers – Winning Solution to IEEE SPS VIP Cup 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14005">http://arxiv.org/abs/2310.14005</a></li>
<li>repo_url: None</li>
<li>paper_authors: H. A. Z. Sameen Shahgir, Khondker Salman Sayeed, Tanjeem Azwad Zaman, Md. Asif Haider, Sheikh Saifur Rahman Jony, M. Sohel Rahman</li>
<li>For: The paper was written for the IEEE SPS VIP Cup 2023: Ophthalmic Biomarker Detection competition, with the primary objective of identifying biomarkers from Optical Coherence Tomography (OCT) images obtained from a diverse range of patients.* Methods: The authors trained two vision transformer-based models, MaxViT and EVA-02, using robust augmentations and 5-fold cross-validation. They ensembled the two models at inference time and found that MaxViT’s use of convolution layers followed by strided attention was better suited for detecting local features, while EVA-02’s use of normal attention mechanism and knowledge distillation was better for detecting global features.* Results: The authors achieved a patient-wise F1 score of 0.814 in the first phase and 0.8527 in the second and final phase of VIP Cup 2023, scoring 3.8% higher than the next-best solution.<details>
<summary>Abstract</summary>
This report outlines our approach in the IEEE SPS VIP Cup 2023: Ophthalmic Biomarker Detection competition. Our primary objective in this competition was to identify biomarkers from Optical Coherence Tomography (OCT) images obtained from a diverse range of patients. Using robust augmentations and 5-fold cross-validation, we trained two vision transformer-based models: MaxViT and EVA-02, and ensembled them at inference time. We find MaxViT's use of convolution layers followed by strided attention to be better suited for the detection of local features while EVA-02's use of normal attention mechanism and knowledge distillation is better for detecting global features. Ours was the best-performing solution in the competition, achieving a patient-wise F1 score of 0.814 in the first phase and 0.8527 in the second and final phase of VIP Cup 2023, scoring 3.8% higher than the next-best solution.
</details>
<details>
<summary>摘要</summary>
本报告介绍了我们在IEEE SPS VIP杯2023：眼部生物标志检测比赛中采用的方法。我们的主要目标是从多样化的病人群中提取眼部生物标志。我们使用了可靠的扩展和5fold跨 VALIDATION，并在推理时 ensemble两种视transformer模型：MaxViT和EVA-02。我们发现MaxViT使用 convolution层后的步骤权重检测本地特征，而EVA-02使用 normal attention机制和知识传递是更适合检测全局特征。在比赛中，我们的解决方案得到了第一阶段和第二阶段的VI P杯2023的病人级F1分数0.814和0.8527，比下一个最佳解决方案高出3.8%。
</details></li>
</ul>
<hr>
<h2 id="Bi-discriminator-Domain-Adversarial-Neural-Networks-with-Class-Level-Gradient-Alignment"><a href="#Bi-discriminator-Domain-Adversarial-Neural-Networks-with-Class-Level-Gradient-Alignment" class="headerlink" title="Bi-discriminator Domain Adversarial Neural Networks with Class-Level Gradient Alignment"></a>Bi-discriminator Domain Adversarial Neural Networks with Class-Level Gradient Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13959">http://arxiv.org/abs/2310.13959</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuang Zhao, Hongke Zhao, Hengshu Zhu, Zhenya Huang, Nan Feng, Enhong Chen, Hui Xiong</li>
<li>for: 这个研究旨在提高隐私领域转移中的不监控性评估，并且将Source domain的丰富知识转移到Target domain中，同时保持Target domain的标签空间。</li>
<li>methods: 本研究使用的方法包括bi-discriminator领域敌方网络，以及基于梯度信号和第二阶probability估计的分组梯度对齐。</li>
<li>results: 实验结果显示， compared to existing方法，本研究的方法可以更好地将Target domain中的标签转移到Source domain中，并且可以更好地避免错分析和错分类。<details>
<summary>Abstract</summary>
Unsupervised domain adaptation aims to transfer rich knowledge from the annotated source domain to the unlabeled target domain with the same label space. One prevalent solution is the bi-discriminator domain adversarial network, which strives to identify target domain samples outside the support of the source domain distribution and enforces their classification to be consistent on both discriminators. Despite being effective, agnostic accuracy and overconfident estimation for out-of-distribution samples hinder its further performance improvement. To address the above challenges, we propose a novel bi-discriminator domain adversarial neural network with class-level gradient alignment, i.e. BACG. BACG resorts to gradient signals and second-order probability estimation for better alignment of domain distributions. Specifically, for accuracy-awareness, we first design an optimizable nearest neighbor algorithm to obtain pseudo-labels of samples in the target domain, and then enforce the backward gradient approximation of the two discriminators at the class level. Furthermore, following evidential learning theory, we transform the traditional softmax-based optimization method into a Multinomial Dirichlet hierarchical model to infer the class probability distribution as well as samples uncertainty, thereby alleviating misestimation of out-of-distribution samples and guaranteeing high-quality classes alignment. In addition, inspired by contrastive learning, we develop a memory bank-based variant, i.e. Fast-BACG, which can greatly shorten the training process at the cost of a minor decrease in accuracy. Extensive experiments and detailed theoretical analysis on four benchmark data sets validate the effectiveness and robustness of our algorithm.
</details>
<details>
<summary>摘要</summary>
Unsupervised domain adaptation aims to transfer rich knowledge from the annotated source domain to the unlabeled target domain with the same label space. One prevalent solution is the bi-discriminator domain adversarial network, which strives to identify target domain samples outside the support of the source domain distribution and enforces their classification to be consistent on both discriminators. Despite being effective, agnostic accuracy and overconfident estimation for out-of-distribution samples hinder its further performance improvement. To address the above challenges, we propose a novel bi-discriminator domain adversarial neural network with class-level gradient alignment, i.e. BACG. BACG resorts to gradient signals and second-order probability estimation for better alignment of domain distributions. Specifically, for accuracy-awareness, we first design an optimizable nearest neighbor algorithm to obtain pseudo-labels of samples in the target domain, and then enforce the backward gradient approximation of the two discriminators at the class level. Furthermore, following evidential learning theory, we transform the traditional softmax-based optimization method into a Multinomial Dirichlet hierarchical model to infer the class probability distribution as well as samples uncertainty, thereby alleviating misestimation of out-of-distribution samples and guaranteeing high-quality classes alignment. In addition, inspired by contrastive learning, we develop a memory bank-based variant, i.e. Fast-BACG, which can greatly shorten the training process at the cost of a minor decrease in accuracy. Extensive experiments and detailed theoretical analysis on four benchmark data sets validate the effectiveness and robustness of our algorithm.Here's the translation in Traditional Chinese:Unsupervised domain adaptation aims to transfer rich knowledge from the annotated source domain to the unlabeled target domain with the same label space. One prevalent solution is the bi-discriminator domain adversarial network, which strives to identify target domain samples outside the support of the source domain distribution and enforces their classification to be consistent on both discriminators. Despite being effective, agnostic accuracy and overconfident estimation for out-of-distribution samples hinder its further performance improvement. To address the above challenges, we propose a novel bi-discriminator domain adversarial neural network with class-level gradient alignment, i.e. BACG. BACG resorts to gradient signals and second-order probability estimation for better alignment of domain distributions. Specifically, for accuracy-awareness, we first design an optimizable nearest neighbor algorithm to obtain pseudo-labels of samples in the target domain, and then enforce the backward gradient approximation of the two discriminators at the class level. Furthermore, following evidential learning theory, we transform the traditional softmax-based optimization method into a Multinomial Dirichlet hierarchical model to infer the class probability distribution as well as samples uncertainty, thereby alleviating misestimation of out-of-distribution samples and guaranteeing high-quality classes alignment. In addition, inspired by contrastive learning, we develop a memory bank-based variant, i.e. Fast-BACG, which can greatly shorten the training process at the cost of a minor decrease in accuracy. Extensive experiments and detailed theoretical analysis on four benchmark data sets validate the effectiveness and robustness of our algorithm.
</details></li>
</ul>
<hr>
<h2 id="Competitive-Ensembling-Teacher-Student-Framework-for-Semi-Supervised-Left-Atrium-MRI-Segmentation"><a href="#Competitive-Ensembling-Teacher-Student-Framework-for-Semi-Supervised-Left-Atrium-MRI-Segmentation" class="headerlink" title="Competitive Ensembling Teacher-Student Framework for Semi-Supervised Left Atrium MRI Segmentation"></a>Competitive Ensembling Teacher-Student Framework for Semi-Supervised Left Atrium MRI Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13955">http://arxiv.org/abs/2310.13955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuyan Shi, Yichi Zhang, Shasha Wang<br>for: 这篇论文主要关注于 semi-supervised learning 技术的应用在医疗影像分类中，尤其是 Left Atrium (LA) 区域的分类。methods: 本文提出了一个简单 yet efficient 的 teacher-student 架构，其中两个学生模型受到不同的任务水平干扰，并在教师模型的导引下进行互相学习。此外，文章还提出了一种竞争性的整合策略，以将更可靠的信息融合到教师模型中。results: 本文在公共的 LA 数据集上进行了评估，并获得了优秀的性能成绩，具体来说，该方法可以充分利用无标注数据，并较上先进的几种 semi-supervised 方法表现出色。<details>
<summary>Abstract</summary>
Semi-supervised learning has greatly advanced medical image segmentation since it effectively alleviates the need of acquiring abundant annotations from experts and utilizes unlabeled data which is much easier to acquire. Among existing perturbed consistency learning methods, mean-teacher model serves as a standard baseline for semi-supervised medical image segmentation. In this paper, we present a simple yet efficient competitive ensembling teacher student framework for semi-supervised for left atrium segmentation from 3D MR images, in which two student models with different task-level disturbances are introduced to learn mutually, while a competitive ensembling strategy is performed to ensemble more reliable information to teacher model. Different from the one-way transfer between teacher and student models, our framework facilitates the collaborative learning procedure of different student models with the guidance of teacher model and motivates different training networks for a competitive learning and ensembling procedure to achieve better performance. We evaluate our proposed method on the public Left Atrium (LA) dataset and it obtains impressive performance gains by exploiting the unlabeled data effectively and outperforms several existing semi-supervised methods.
</details>
<details>
<summary>摘要</summary>
semi-supervised learning 已经大幅提高医疗图像分割的技术 Waterloo ，因为它有效地减轻了专家们 annotate 大量数据的需求，并利用了 easier to obtain 的无标注数据。在现有的妥协一致学习方法中，mean-teacher 模型 serves as a standard baseline for semi-supervised medical image segmentation。在这篇论文中，我们提出了一种简单 yet efficient 的 competitive ensembling teacher student 框架，用于 semi-supervised 左心室 segmentation from 3D MR 图像，其中有两个学生模型 with different task-level disturbances 是用来学习 mutually，而一种 competitive ensembling 策略是用于 ensemble 更可靠的信息到 teacher model。与一个 teacher 和学生模型之间的一个way transfer 不同，我们的框架实现了不同的学生模型之间的协同学习过程，帮助 by teacher model 的指导和动力，以实现更好的性能。我们对 public Left Atrium (LA) 数据集进行了评估，并获得了很好的性能提升，通过有效地利用无标注数据和多个现有的半指导学习方法。
</details></li>
</ul>
<hr>
<h2 id="Fuzzy-NMS-Improving-3D-Object-Detection-with-Fuzzy-Classification-in-NMS"><a href="#Fuzzy-NMS-Improving-3D-Object-Detection-with-Fuzzy-Classification-in-NMS" class="headerlink" title="Fuzzy-NMS: Improving 3D Object Detection with Fuzzy Classification in NMS"></a>Fuzzy-NMS: Improving 3D Object Detection with Fuzzy Classification in NMS</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13951">http://arxiv.org/abs/2310.13951</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Wang, Xinyu Zhang, Fachuan Zhao, Chuze Wu, Yichen Wang, Ziying Song, Lei Yang, Jun Li, Huaping Liu</li>
<li>for: 提高3D物体检测精度，解决NMS过程中的不确定性</li>
<li>methods: 引入混合学习方法，提出一种通用化精度补做模块</li>
<li>results: 对多种最新的NMS基于检测器进行改进，特别是对小物体（如人车）的准确率有显著提高，无需重新训练和显著增加推理时间<details>
<summary>Abstract</summary>
Non-maximum suppression (NMS) is an essential post-processing module used in many 3D object detection frameworks to remove overlapping candidate bounding boxes. However, an overreliance on classification scores and difficulties in determining appropriate thresholds can affect the resulting accuracy directly. To address these issues, we introduce fuzzy learning into NMS and propose a novel generalized Fuzzy-NMS module to achieve finer candidate bounding box filtering. The proposed Fuzzy-NMS module combines the volume and clustering density of candidate bounding boxes, refining them with a fuzzy classification method and optimizing the appropriate suppression thresholds to reduce uncertainty in the NMS process. Adequate validation experiments are conducted using the mainstream KITTI and large-scale Waymo 3D object detection benchmarks. The results of these tests demonstrate the proposed Fuzzy-NMS module can improve the accuracy of numerous recently NMS-based detectors significantly, including PointPillars, PV-RCNN, and IA-SSD, etc. This effect is particularly evident for small objects such as pedestrians and bicycles. As a plug-and-play module, Fuzzy-NMS does not need to be retrained and produces no obvious increases in inference time.
</details>
<details>
<summary>摘要</summary>
我们的提案的总体瑞逸-NMS 模块通过将卷积体和凝聚密度的候选 bounding box 组合起来，并使用瑞逸分类方法来细化它们，以便优化适当的阈值，从而减少 NMS 过程中的uncertainty。我们对主流的 KITTI 和大规模的 Waymo 3D object detection 标准准进行了适当的验证实验。实验结果表明，我们的提案的总体瑞逸-NMS 模块可以在许多最近的 NMS 基于检测器中提高准确性，包括 PointPillars、PV-RCNN 和 IA-SSD 等。这种效果尤其明显于小物体，如人行道和自行车。总之，我们的总体瑞逸-NMS 模块是一个可插件的模块，不需要重新训练，并且不会导致明显的执行时间增加。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Image-Generation-by-Spatial-Transformation-in-Perceptual-Colorspaces"><a href="#Adversarial-Image-Generation-by-Spatial-Transformation-in-Perceptual-Colorspaces" class="headerlink" title="Adversarial Image Generation by Spatial Transformation in Perceptual Colorspaces"></a>Adversarial Image Generation by Spatial Transformation in Perceptual Colorspaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13950">http://arxiv.org/abs/2310.13950</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ayberkydn/stadv-torch">https://github.com/ayberkydn/stadv-torch</a></li>
<li>paper_authors: Ayberk Aydin, Alptekin Temizel</li>
<li>for: 这个论文旨在提出一种基于色彩空间的攻击方法，以便在深度神经网络上进行targeted white-box攻击。</li>
<li>methods: 该方法使用了空间变换来生成攻击样本，其中Pixel值在Chrominance channels上独立变换，而不是直接对像值进行添加性负杂化或者直接操作。</li>
<li>results: 实验结果表明，该方法在targeted white-box攻击 Setting下可以获得竞争力的欺骗率，并且在benign和攻击样本之间的approxiamte perceptual distance上表现出优异的result。I hope that helps! Let me know if you have any further questions or if there’s anything else I can assist you with.<details>
<summary>Abstract</summary>
Deep neural networks are known to be vulnerable to adversarial perturbations. The amount of these perturbations are generally quantified using $L_p$ metrics, such as $L_0$, $L_2$ and $L_\infty$. However, even when the measured perturbations are small, they tend to be noticeable by human observers since $L_p$ distance metrics are not representative of human perception. On the other hand, humans are less sensitive to changes in colorspace. In addition, pixel shifts in a constrained neighborhood are hard to notice. Motivated by these observations, we propose a method that creates adversarial examples by applying spatial transformations, which creates adversarial examples by changing the pixel locations independently to chrominance channels of perceptual colorspaces such as $YC_{b}C_{r}$ and $CIELAB$, instead of making an additive perturbation or manipulating pixel values directly. In a targeted white-box attack setting, the proposed method is able to obtain competitive fooling rates with very high confidence. The experimental evaluations show that the proposed method has favorable results in terms of approximate perceptual distance between benign and adversarially generated images. The source code is publicly available at https://github.com/ayberkydn/stadv-torch
</details>
<details>
<summary>摘要</summary>
Motivated by these observations, we propose a method that creates adversarial examples by applying spatial transformations, which creates adversarial examples by changing the pixel locations independently in chrominance channels of perceptual colorspaces such as $YC_{b}C_{r}$ and $CIELAB$, instead of making an additive perturbation or manipulating pixel values directly. In a targeted white-box attack setting, the proposed method is able to obtain competitive fooling rates with very high confidence.The experimental evaluations show that the proposed method has favorable results in terms of approximate perceptual distance between benign and adversarially generated images. The source code is publicly available at <https://github.com/ayberkydn/stadv-torch>.
</details></li>
</ul>
<hr>
<h2 id="Learning-Motion-Refinement-for-Unsupervised-Face-Animation"><a href="#Learning-Motion-Refinement-for-Unsupervised-Face-Animation" class="headerlink" title="Learning Motion Refinement for Unsupervised Face Animation"></a>Learning Motion Refinement for Unsupervised Face Animation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13912">http://arxiv.org/abs/2310.13912</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jialetao/mrfa">https://github.com/jialetao/mrfa</a></li>
<li>paper_authors: Jiale Tao, Shuhang Gu, Wen Li, Lixin Duan</li>
<li>for: 生成一个基于出处图像的人脸视频，模拟驱动视频中的人脸动作。</li>
<li>methods: 采用了一种新的无监督人脸动画方法，同时学习粗细动作。在本方法中，我们利用了本地Affine运动模型来学习全局粗细动作，并在本地区域使用一种新的动作细化模块来补做粗细动作。这个模块是通过 dense correlation between source and driving images 来学习的。</li>
<li>results: 对 widely used benchmarks 进行了广泛的实验，并取得了state-of-the-art的结果。<details>
<summary>Abstract</summary>
Unsupervised face animation aims to generate a human face video based on the appearance of a source image, mimicking the motion from a driving video. Existing methods typically adopted a prior-based motion model (e.g., the local affine motion model or the local thin-plate-spline motion model). While it is able to capture the coarse facial motion, artifacts can often be observed around the tiny motion in local areas (e.g., lips and eyes), due to the limited ability of these methods to model the finer facial motions. In this work, we design a new unsupervised face animation approach to learn simultaneously the coarse and finer motions. In particular, while exploiting the local affine motion model to learn the global coarse facial motion, we design a novel motion refinement module to compensate for the local affine motion model for modeling finer face motions in local areas. The motion refinement is learned from the dense correlation between the source and driving images. Specifically, we first construct a structure correlation volume based on the keypoint features of the source and driving images. Then, we train a model to generate the tiny facial motions iteratively from low to high resolution. The learned motion refinements are combined with the coarse motion to generate the new image. Extensive experiments on widely used benchmarks demonstrate that our method achieves the best results among state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
<<SYS>>这是一个对文本进行简化中文译文的示例：</SYS>无监督面部动画目标是将来源图片中的人脸动画化，基于驱动影片中的动作，并将动作调节为人脸的细微动作。现有方法通常使用本地欧几何动作模型（例如本地欧几何动作模型或本地薄板拓扑动作模型）。这些方法可以捕捉到人脸的粗略动作，但是它们对本地区域（例如嘴巴和眼睛）的动作有限，往往会出现遗憾。在这个工作中，我们设计了一个新的无监督面部动画方法，同时学习粗略和细微的动作。具体来说，我们利用本地欧几何动作模型学习全局粗略的人脸动作，并设计了一个新的动作精度调整模块，以补偿本地欧几何动作模型对本地区域的动作精度模型。这个动作精度调整是根据驱动影片和源影片之间的密集相联性学习的。具体来说，我们首先建立了基于关键特征的源影片和驱动影片之间的结构相联性量。然后，我们将这个结构相联性量训练成一个可以从低分辨率到高分辨率的模型，以产生细微的动作调整。学习的动作调整与粗略动作结合，创建出新的图片。实际实验显示，我们的方法在广泛使用的标准benchmark上得到了最好的结果。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Driving-Behavior-for-Autonomous-Vehicles-Based-on-Gramian-Angular-Field-Vision-Transformer"><a href="#Exploring-Driving-Behavior-for-Autonomous-Vehicles-Based-on-Gramian-Angular-Field-Vision-Transformer" class="headerlink" title="Exploring Driving Behavior for Autonomous Vehicles Based on Gramian Angular Field Vision Transformer"></a>Exploring Driving Behavior for Autonomous Vehicles Based on Gramian Angular Field Vision Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13906">http://arxiv.org/abs/2310.13906</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junwei You, Ying Chen, Zhuoyu Jiang, Zhangchi Liu, Zilin Huang, Yifeng Ding, Bin Ran</li>
<li>for: 本研究旨在提出一种用于分类自动驾驶车辆（AV）驾驶行为的有效分类方法，以便诊断AV运行问题、改进自动驾驶算法和减少事故率。</li>
<li>methods: 该研究提出了一种名为Gramian Angular Field Vision Transformer（GAF-ViT）模型，用于分析AV驾驶行为。GAF-ViT模型包括三个关键组件：GAFTransformer模块、通道注意力模块和多通道ViT模块。这些模块将表示序列中的多个变量行为转换为多个图像，然后使用图像识别技术进行行为分类。</li>
<li>results: 对于Waymo开放数据集的轨迹数据进行实验表示，提出的GAF-ViT模型实现了当前领先的性能。此外，对于各个模块的效果进行了减少性研究，以证明模型的可行性。<details>
<summary>Abstract</summary>
Effective classification of autonomous vehicle (AV) driving behavior emerges as a critical area for diagnosing AV operation faults, enhancing autonomous driving algorithms, and reducing accident rates. This paper presents the Gramian Angular Field Vision Transformer (GAF-ViT) model, designed to analyze AV driving behavior. The proposed GAF-ViT model consists of three key components: GAF Transformer Module, Channel Attention Module, and Multi-Channel ViT Module. These modules collectively convert representative sequences of multivariate behavior into multi-channel images and employ image recognition techniques for behavior classification. A channel attention mechanism is applied to multi-channel images to discern the impact of various driving behavior features. Experimental evaluation on the Waymo Open Dataset of trajectories demonstrates that the proposed model achieves state-of-the-art performance. Furthermore, an ablation study effectively substantiates the efficacy of individual modules within the model.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传递给定文本到简化中文。<</SYS>>自动驾驶车辆（AV）驾驶行为分类成为诊断AV操作错误、改进自动驾驶算法和减少事故率的关键领域。本文介绍了Gramian Angular Field Vision Transformer（GAF-ViT）模型，用于分析AV驾驶行为。提议的GAF-ViT模型包括三个关键组件：GAF TransformerModule、Channel AttentionModule和Multi-Channel ViTModule。这些模块结合收集的多个变量行为的表示序列，并使用图像识别技术进行行为分类。通过频道注意机制对多个频道图像进行区分。实验表明，提议的模型在 Waymo 开放数据集上的轨迹 traverse 得到了状态的最佳性。此外，一个ablation研究有效地证明了模型中各个模块的效果。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Transformer-Using-Cross-Channel-attention-for-Object-Detection-in-Remote-Sensing-Images"><a href="#Multimodal-Transformer-Using-Cross-Channel-attention-for-Object-Detection-in-Remote-Sensing-Images" class="headerlink" title="Multimodal Transformer Using Cross-Channel attention for Object Detection in Remote Sensing Images"></a>Multimodal Transformer Using Cross-Channel attention for Object Detection in Remote Sensing Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13876">http://arxiv.org/abs/2310.13876</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bissmella Bahaduri, Zuheng Ming, Fangchen Feng, Anissa Mokraou</li>
<li>for: 这篇研究旨在提高遥测图像中的物体探测精度，并且解决遥测图像中物体探测的特定挑战，例如资料标注的缺乏和高分辨率图像中的小物体。</li>
<li>methods: 本研究提出了一个多模式转换器，通过跨通道注意力模组来探索多源遥测数据的联合。相比于传统的通道合并方法，该模组能够学习不同通道之间的关系，实现多模式输入的协调。此外，研究人员还提出了基于Swin transformer的新架构，具有固定维度的卷积层，以获得轻量级的精确性和Computational efficiency。</li>
<li>results: 实验结果显示了该多模式转换器和架构的效果，证明了它们在多模式遥测图像中的应用性。<details>
<summary>Abstract</summary>
Object detection in Remote Sensing Images (RSI) is a critical task for numerous applications in Earth Observation (EO). Unlike general object detection, object detection in RSI has specific challenges: 1) the scarcity of labeled data in RSI compared to general object detection datasets, and 2) the small objects presented in a high-resolution image with a vast background. To address these challenges, we propose a multimodal transformer exploring multi-source remote sensing data for object detection. Instead of directly combining the multimodal input through a channel-wise concatenation, which ignores the heterogeneity of different modalities, we propose a cross-channel attention module. This module learns the relationship between different channels, enabling the construction of a coherent multimodal input by aligning the different modalities at the early stage. We also introduce a new architecture based on the Swin transformer that incorporates convolution layers in non-shifting blocks while maintaining fixed dimensions, allowing for the generation of fine-to-coarse representations with a favorable accuracy-computation trade-off. The extensive experiments prove the effectiveness of the proposed multimodal fusion module and architecture, demonstrating their applicability to multimodal aerial imagery.
</details>
<details>
<summary>摘要</summary>
remote sensing 图像中的对象检测是许多应用程序地球观测（EO）中的关键任务。与通用对象检测不同，对象检测在 remote sensing 图像中具有特定挑战：1） remote sensing 图像中标注数据的稀缺性，2） 图像中的小对象在高分辨率背景中呈现。 To address these challenges, we propose a multimodal transformer exploring multi-source remote sensing data for object detection. Instead of directly combining the multimodal input through a channel-wise concatenation, which ignores the heterogeneity of different modalities, we propose a cross-channel attention module. This module learns the relationship between different channels, enabling the construction of a coherent multimodal input by aligning the different modalities at the early stage. We also introduce a new architecture based on the Swin transformer that incorporates convolution layers in non-shifting blocks while maintaining fixed dimensions, allowing for the generation of fine-to-coarse representations with a favorable accuracy-computation trade-off. The extensive experiments prove the effectiveness of the proposed multimodal fusion module and architecture, demonstrating their applicability to multimodal aerial imagery.Here's a word-for-word translation of the text into Simplified Chinese:远程感知图像中的对象检测是EO中许多应用程序的关键任务。与通用对象检测不同，对象检测在远程感知图像中具有特定挑战：1）远程感知图像中标注数据的稀缺性，2）图像中的小对象在高分辨率背景中呈现。 To address these challenges, we propose a multimodal transformer exploring multi-source remote sensing data for object detection. Instead of directly combining the multimodal input through a channel-wise concatenation, which ignores the heterogeneity of different modalities, we propose a cross-channel attention module. This module learns the relationship between different channels, enabling the construction of a coherent multimodal input by aligning the different modalities at the early stage. We also introduce a new architecture based on the Swin transformer that incorporates convolution layers in non-shifting blocks while maintaining fixed dimensions, allowing for the generation of fine-to-coarse representations with a favorable accuracy-computation trade-off. The extensive experiments prove the effectiveness of the proposed multimodal fusion module and architecture, demonstrating their applicability to multimodal aerial imagery.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/21/cs.CV_2023_10_21/" data-id="clp88dbwo00luob880b1t39zl" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_10_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/21/cs.AI_2023_10_21/" class="article-date">
  <time datetime="2023-10-21T12:00:00.000Z" itemprop="datePublished">2023-10-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/21/cs.AI_2023_10_21/">cs.AI - 2023-10-21</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Ask-To-The-Point-Open-Domain-Entity-Centric-Question-Generation"><a href="#Ask-To-The-Point-Open-Domain-Entity-Centric-Question-Generation" class="headerlink" title="Ask To The Point: Open-Domain Entity-Centric Question Generation"></a>Ask To The Point: Open-Domain Entity-Centric Question Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14126">http://arxiv.org/abs/2310.14126</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liuyuxiang512/ecqg">https://github.com/liuyuxiang512/ecqg</a></li>
<li>paper_authors: Yuxiang Liu, Jie Huang, Kevin Chen-Chuan Chang</li>
<li>for: 实现话题具体学习、助教读物和 факт检查等应用场景，提出了一个新任务：实体中心问题生成（ECQG）。</li>
<li>methods: 提出了一种具有一致性的 PLM 基础架构 GenCONE，包括两个新模块：内容强调模块和问题验证模块。</li>
<li>results: 经过广泛的实验，GenCONE 能够具有显著和稳定的性能优势，并且两个模块具有辅之usage和补做作用。<details>
<summary>Abstract</summary>
We introduce a new task called *entity-centric question generation* (ECQG), motivated by real-world applications such as topic-specific learning, assisted reading, and fact-checking. The task aims to generate questions from an entity perspective. To solve ECQG, we propose a coherent PLM-based framework GenCONE with two novel modules: content focusing and question verification. The content focusing module first identifies a focus as "what to ask" to form draft questions, and the question verification module refines the questions afterwards by verifying the answerability. We also construct a large-scale open-domain dataset from SQuAD to support this task. Our extensive experiments demonstrate that GenCONE significantly and consistently outperforms various baselines, and two modules are effective and complementary in generating high-quality questions.
</details>
<details>
<summary>摘要</summary>
我们介绍了一个新任务called *实体中心问题生成* (ECQG), 该任务受到实际应用场景的启发，如专题学习、辅助阅读和事实核实。该任务的目标是从实体角度生成问题。为解决ECQG，我们提议了一个协调PLM-based框架GenCONE，该框架包括两个新模块：内容专注和问题验证。内容专注模块首先确定了“要问什么”的焦点，以生成签证问题，而问题验证模块则在验证答案可否回答。我们还构建了一个大规模的开放领域数据集，来支持这个任务。我们的广泛实验表明，GenCONE在various baselines的比较中具有显著且一致的优势，两个模块也是生成高质量问题的有效和补充性的。
</details></li>
</ul>
<hr>
<h2 id="Sentiment-Analysis-Across-Multiple-African-Languages-A-Current-Benchmark"><a href="#Sentiment-Analysis-Across-Multiple-African-Languages-A-Current-Benchmark" class="headerlink" title="Sentiment Analysis Across Multiple African Languages: A Current Benchmark"></a>Sentiment Analysis Across Multiple African Languages: A Current Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14120">http://arxiv.org/abs/2310.14120</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saurav K. Aryal, Howard Prioleau, Surakshya Aryal</li>
<li>for: 这项研究的目的是为了提高非洲语言 sentiment 分析的研究，并评估当前的 transformer 模型在非洲语言上的性能。</li>
<li>methods: 研究使用了 AfriSenti-SemEval Shared Task 12 上的注释 sentiment analysis 数据，并对当前状态的 transformer 模型进行了比较和评估。</li>
<li>results: 研究发现，即使在低资源情况下，更多的数据仍然可以生成更好的模型，并且模型专门为非洲语言开发的模型在所有任务上都表现出色。此外，没有一个 universal 模型能够适用于所有语言的评估。<details>
<summary>Abstract</summary>
Sentiment analysis is a fundamental and valuable task in NLP. However, due to limitations in data and technological availability, research into sentiment analysis of African languages has been fragmented and lacking. With the recent release of the AfriSenti-SemEval Shared Task 12, hosted as a part of The 17th International Workshop on Semantic Evaluation, an annotated sentiment analysis of 14 African languages was made available. We benchmarked and compared current state-of-art transformer models across 12 languages and compared the performance of training one-model-per-language versus single-model-all-languages. We also evaluated the performance of standard multilingual models and their ability to learn and transfer cross-lingual representation from non-African to African languages. Our results show that despite work in low resource modeling, more data still produces better models on a per-language basis. Models explicitly developed for African languages outperform other models on all tasks. Additionally, no one-model-fits-all solution exists for a per-language evaluation of the models evaluated. Moreover, for some languages with a smaller sample size, a larger multilingual model may perform better than a dedicated per-language model for sentiment classification.
</details>
<details>
<summary>摘要</summary>
《叙述分析是NLP领域的基础和重要任务。然而，由于数据和技术限制，关于非洲语言的叙述分析研究受到了限制，Fragmented和缺乏。随着最近发布的AfriSenti-SemEval Shared Task 12，14种非洲语言的叙述分析标注数据被提供。我们对当前状态的转换器模型进行了比较和比较，并 evaluate了单语言模型 versus 所有语言模型的训练。我们还评估了标准多语言模型的能力以及其在非洲语言上学习和传递cross-语言表示的能力。我们的结果显示，尽管在低资源模型方面做了很多工作，但更多的数据仍然可以生成更好的模型。专门为非洲语言开发的模型在所有任务上都高于其他模型。此外，没有一个“一模型 fits all”的解决方案，每种语言的评估中的模型都不同。此外，对于一些语言的小样本大小，大型多语言模型可能会在叙述分类任务上表现更好于专门为该语言开发的模型。
</details></li>
</ul>
<hr>
<h2 id="CLIP-meets-Model-Zoo-Experts-Pseudo-Supervision-for-Visual-Enhancement"><a href="#CLIP-meets-Model-Zoo-Experts-Pseudo-Supervision-for-Visual-Enhancement" class="headerlink" title="CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement"></a>CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14108">http://arxiv.org/abs/2310.14108</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammadreza Salehi, Mehrdad Farajtabar, Maxwell Horton, Fartash Faghri, Hadi Pouransari, Raviteja Vemulapalli, Oncel Tuzel, Ali Farhadi, Mohammad Rastegari, Sachin Mehta</li>
<li>for: 这个论文的目的是提高CLIP模型的视觉表示能力。</li>
<li>methods: 这个论文使用了开源的任务特定视觉模型生成 pseudo-labels，并在这些 pseudo-labels 基础上训练 CLIP 模型。</li>
<li>results: 这个方法可以提高 CLIP 模型在不同视觉任务中的表现，包括 segmentation、检测、深度估计和表面法线估计，最多提高16.3%。这些提高不会减少 CLIP 模型的现有能力，包括其在适应性零分类中的护卫。<details>
<summary>Abstract</summary>
Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities. This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.
</details>
<details>
<summary>摘要</summary>
对比语言图像预训练（CLIP）是一种标准的视觉语言模型训练方法。CLIP可以扩展，可以提示，并具有对分布变化的鲁棒性，但它缺乏对象定位功能。这篇论文研究以下问题：可以通过将任务特定的视觉模型添加到CLIP训练中来改善其视觉表示？为此，我们利用开源的任务特定视觉模型生成pseudo-标签，并在这些pseudo-标签的基础上训练CLIP模型。这个简单的设置可以提高CLIP模型在不同视觉任务上的表现，最高提高达16.3%。这些改进不仅不会减少CLIP的现有能力，包括它的能力在零shot提示下的报表能力。
</details></li>
</ul>
<hr>
<h2 id="On-the-Transferability-of-Visually-Grounded-PCFGs"><a href="#On-the-Transferability-of-Visually-Grounded-PCFGs" class="headerlink" title="On the Transferability of Visually Grounded PCFGs"></a>On the Transferability of Visually Grounded PCFGs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14107">http://arxiv.org/abs/2310.14107</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhaoyanpeng/cpcfg">https://github.com/zhaoyanpeng/cpcfg</a></li>
<li>paper_authors: Yanpeng Zhao, Ivan Titov</li>
<li>for: 本研究旨在评估视觉基础 grammar 生成器在不同文本领域中的可迁移性。</li>
<li>methods: 我们extend了VC-PCFG模型，使其能够在不同文本领域中进行迁移学习。我们采用了零shot转移学习 Setting，即在源领域训练模型，然后直接应用到目标领域。</li>
<li>results: 我们的实验结果表明，视觉基础对文本的改进效果在相似领域中传递，但在远程领域中失效。我们还进行了数据和结果分析，发现 lexicon  overlap  между源领域和目标领域是迁移性的最重要因素。<details>
<summary>Abstract</summary>
There has been a significant surge of interest in visually grounded grammar induction in recent times. While a variety of models have been developed for the task and have demonstrated impressive performance, they have not been evaluated on text domains that are different from the training domain, so it is unclear if the improvements brought by visual groundings are transferable. Our study aims to fill this gap and assess the degree of transferability. We start by extending VC-PCFG (short for Visually-grounded Compound PCFG~\citep{zhao-titov-2020-visually}) in such a way that it can transfer across text domains. We consider a zero-shot transfer learning setting where a model is trained on the source domain and is directly applied to target domains, without any further training. Our experimental results suggest that: the benefits from using visual groundings transfer to text in a domain similar to the training domain but fail to transfer to remote domains. Further, we conduct data and result analysis; we find that the lexicon overlap between the source domain and the target domain is the most important factor in the transferability of VC-PCFG.
</details>
<details>
<summary>摘要</summary>
Recently, there has been a significant increase in interest in visually grounded grammar induction. While various models have been developed for this task and have shown impressive performance, their transferability to different text domains has not been evaluated. Our study aims to fill this gap and assess the degree of transferability. We start by extending VC-PCFG (short for Visually-grounded Compound PCFG) to enable transfer across text domains. We use a zero-shot transfer learning setting where a model is trained on the source domain and is directly applied to target domains without further training. Our experimental results show that: the benefits of using visual groundings transfer to text in a domain similar to the training domain but fail to transfer to remote domains. Additionally, we conduct data and result analysis and find that the lexicon overlap between the source domain and the target domain is the most important factor in the transferability of VC-PCFG.
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Instruction-Fine-tuned-Model-Evaluation-to-Guide-Industrial-Applications"><a href="#Revisiting-Instruction-Fine-tuned-Model-Evaluation-to-Guide-Industrial-Applications" class="headerlink" title="Revisiting Instruction Fine-tuned Model Evaluation to Guide Industrial Applications"></a>Revisiting Instruction Fine-tuned Model Evaluation to Guide Industrial Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14103">http://arxiv.org/abs/2310.14103</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/manuelfay/ifteval">https://github.com/manuelfay/ifteval</a></li>
<li>paper_authors: Manuel Faysse, Gautier Viaud, Céline Hudelot, Pierre Colombo</li>
<li>for: 这 paper 是 investigating task-specialization strategies for IFT model deployment in practical industrial settings.</li>
<li>methods: 该 paper 使用 LLM-based metrics to evaluate the performance of IFT models.</li>
<li>results: 该 paper 提供了实际 industrial setting 中 IFT 模型的部署中的贸易offs, offering practitioners actionable insights for real-world IFT model deployment.<details>
<summary>Abstract</summary>
Instruction Fine-Tuning (IFT) is a powerful paradigm that strengthens the zero-shot capabilities of Large Language Models (LLMs), but in doing so induces new evaluation metric requirements. We show LLM-based metrics to be well adapted to these requirements, and leverage them to conduct an investigation of task-specialization strategies, quantifying the trade-offs that emerge in practical industrial settings. Our findings offer practitioners actionable insights for real-world IFT model deployment.
</details>
<details>
<summary>摘要</summary>
instruction 细调 (IFT) 是一种强大的思想方法，可以增强大型语言模型 (LLM) 的零shot 能力，但是在这之前需要新的评估指标。我们表明 LLM 基于的指标适应这些要求，并利用它们来研究实际工业场景中的任务特化策略，量化在实际应用中出现的交易offs。我们的发现可以为实际 IFT 模型部署提供实践性的指导意见。Here's a word-for-word translation of the text into Simplified Chinese: instruction 细调（IFT）是一种强大的思想方法，可以增强大型语言模型（LLM）的零shot能力，但是在这之前需要新的评估指标。我们表明 LLM基于的指标适应这些要求，并利用它们来研究实际工业场景中的任务特化策略，量化在实际应用中出现的交易offs。我们的发现可以为实际 IFT 模型部署提供实践性的指导意见。
</details></li>
</ul>
<hr>
<h2 id="Stabilizing-reinforcement-learning-control-A-modular-framework-for-optimizing-over-all-stable-behavior"><a href="#Stabilizing-reinforcement-learning-control-A-modular-framework-for-optimizing-over-all-stable-behavior" class="headerlink" title="Stabilizing reinforcement learning control: A modular framework for optimizing over all stable behavior"></a>Stabilizing reinforcement learning control: A modular framework for optimizing over all stable behavior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14098">http://arxiv.org/abs/2310.14098</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nathan P. Lawrence, Philip D. Loewen, Shuyuan Wang, Michael G. Forbes, R. Bhushan Gopaluni</li>
<li>for: 本文提出了一个权重控制器设计框架，该框架结合深度学习的优化和模型自由，同时保证稳定性。</li>
<li>methods: 本文使用了Youla-Kucera参数化定义搜索领域，并使用行为系统构建了数据驱动内部模型。在噪声存在的情况下，对数据驱动模型的稳定性进行了分析。</li>
<li>results: 本文通过matrix factorization方法给出了所有稳定线性运算符的集合，并使用神经网络表示参数化的稳定运算符集合，实现了与标准深度学习库的无缝集成。最后，本文还展示了如何应用这些想法来调整固定结构控制器。<details>
<summary>Abstract</summary>
We propose a framework for the design of feedback controllers that combines the optimization-driven and model-free advantages of deep reinforcement learning with the stability guarantees provided by using the Youla-Kucera parameterization to define the search domain. Recent advances in behavioral systems allow us to construct a data-driven internal model; this enables an alternative realization of the Youla-Kucera parameterization based entirely on input-output exploration data. Perhaps of independent interest, we formulate and analyze the stability of such data-driven models in the presence of noise. The Youla-Kucera approach requires a stable "parameter" for controller design. For the training of reinforcement learning agents, the set of all stable linear operators is given explicitly through a matrix factorization approach. Moreover, a nonlinear extension is given using a neural network to express a parameterized set of stable operators, which enables seamless integration with standard deep learning libraries. Finally, we show how these ideas can also be applied to tune fixed-structure controllers.
</details>
<details>
<summary>摘要</summary>
我们提出一个框架，将深度学习的优化优势和模型自由的优势结合起来，同时保证稳定性通过用Youla-Kucera参数化来定义搜索空间。现有的行为系统技术使我们可以构建数据驱动的内部模型，这使得我们可以基于输入输出探索数据来实现Youla-Kucera参数化的alternative实现。此外，我们还研究了这些数据驱动模型在噪声存在时的稳定性。Youla-Kucera方法需要一个稳定的参数来设计控制器。在训练深度学习代理人时，所有稳定的线性运算的集合可以通过矩阵分解方法得到Explicitly。此外，我们还给出了一种使用神经网络表示参数化集合的稳定运算的非线性扩展，这使得我们可以轻松地与标准深度学习库集成。最后，我们示出了如何应用这些想法来调整固定结构的控制器。
</details></li>
</ul>
<hr>
<h2 id="Learning-Reward-for-Physical-Skills-using-Large-Language-Model"><a href="#Learning-Reward-for-Physical-Skills-using-Large-Language-Model" class="headerlink" title="Learning Reward for Physical Skills using Large Language Model"></a>Learning Reward for Physical Skills using Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14092">http://arxiv.org/abs/2310.14092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuwei Zeng, Yiqing Xu</li>
<li>for: 学习物理技能的奖励函数是一个挑战，因为这些任务的谱度非常广泛，状态和动作空间的维度很高，以及复杂的感知反馈。获取专家示范数据是成本高昂和时间费时的。</li>
<li>methods: 我们使用大型自然语言模型（LLM）提取任务相关知识，以提出有效的奖励函数。我们的方法包括两个组成部分：首先，使用 LLM 提出特征和参数化的奖励函数。然后，我们通过迭代自适应过程来更新这些提出的奖励函数的参数，以最小化与 LLM 的排名不一致性。</li>
<li>results: 我们在三个模拟的物理技能学习任务上进行了测试，证明了我们的设计选择的有效性。<details>
<summary>Abstract</summary>
Learning reward functions for physical skills are challenging due to the vast spectrum of skills, the high-dimensionality of state and action space, and nuanced sensory feedback. The complexity of these tasks makes acquiring expert demonstration data both costly and time-consuming. Large Language Models (LLMs) contain valuable task-related knowledge that can aid in learning these reward functions. However, the direct application of LLMs for proposing reward functions has its limitations such as numerical instability and inability to incorporate the environment feedback. We aim to extract task knowledge from LLMs using environment feedback to create efficient reward functions for physical skills. Our approach consists of two components. We first use the LLM to propose features and parameterization of the reward function. Next, we update the parameters of this proposed reward function through an iterative self-alignment process. In particular, this process minimizes the ranking inconsistency between the LLM and our learned reward functions based on the new observations. We validated our method by testing it on three simulated physical skill learning tasks, demonstrating effective support for our design choices.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Use LLM to propose features and parameterization of the reward function.2. Update the proposed reward function parameters through an iterative self-alignment process that minimizes the ranking inconsistency between the LLM and our learned reward functions based on new observations.We validated our method by testing it on three simulated physical skill learning tasks, demonstrating effective support for our design choices.</details></li>
</ol>
<hr>
<h2 id="To-Copy-or-not-to-Copy-That-is-a-Critical-Issue-of-the-Output-Softmax-Layer-in-Neural-Sequential-Recommenders"><a href="#To-Copy-or-not-to-Copy-That-is-a-Critical-Issue-of-the-Output-Softmax-Layer-in-Neural-Sequential-Recommenders" class="headerlink" title="To Copy, or not to Copy; That is a Critical Issue of the Output Softmax Layer in Neural Sequential Recommenders"></a>To Copy, or not to Copy; That is a Critical Issue of the Output Softmax Layer in Neural Sequential Recommenders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14079">http://arxiv.org/abs/2310.14079</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iesl/softmax_cpr_recommend">https://github.com/iesl/softmax_cpr_recommend</a></li>
<li>paper_authors: Haw-Shiuan Chang, Nikhil Agarwal, Andrew McCallum</li>
<li>for: 强化Sequential Recommendation任务中复现项目的处理能力</li>
<li>methods: 采用recently-proposed softmax alternatives如softmax-CPR，对输出softmax层进行修改，解决单个隐藏状态嵌入和静态项嵌入的问题</li>
<li>results: 在12个数据集上提供了一致性的改进，对GRU4Rec模型进行修改后，在5个数据集中具有重复项的NDCG@10提高10%（4%-17%），在7个数据集中无重复项的NDCG@10提高24%（8%-39%）。<details>
<summary>Abstract</summary>
Recent studies suggest that the existing neural models have difficulty handling repeated items in sequential recommendation tasks. However, our understanding of this difficulty is still limited. In this study, we substantially advance this field by identifying a major source of the problem: the single hidden state embedding and static item embeddings in the output softmax layer. Specifically, the similarity structure of the global item embeddings in the softmax layer sometimes forces the single hidden state embedding to be close to new items when copying is a better choice, while sometimes forcing the hidden state to be close to the items from the input inappropriately. To alleviate the problem, we adapt the recently-proposed softmax alternatives such as softmax-CPR to sequential recommendation tasks and demonstrate that the new softmax architectures unleash the capability of the neural encoder on learning when to copy and when to exclude the items from the input sequence. By only making some simple modifications on the output softmax layer for SASRec and GRU4Rec, softmax-CPR achieves consistent improvement in 12 datasets. With almost the same model size, our best method not only improves the average NDCG@10 of GRU4Rec in 5 datasets with duplicated items by 10% (4%-17% individually) but also improves 7 datasets without duplicated items by 24% (8%-39%)!
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Convolutional-Bidirectional-Variational-Autoencoder-for-Image-Domain-Translation-of-Dotted-Arabic-Expiration"><a href="#Convolutional-Bidirectional-Variational-Autoencoder-for-Image-Domain-Translation-of-Dotted-Arabic-Expiration" class="headerlink" title="Convolutional Bidirectional Variational Autoencoder for Image Domain Translation of Dotted Arabic Expiration"></a>Convolutional Bidirectional Variational Autoencoder for Image Domain Translation of Dotted Arabic Expiration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14069">http://arxiv.org/abs/2310.14069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Zidane, Ghada Soliman</li>
<li>for: 这个论文提出了一种基于升降栈底部卷积 convolutional bidirectional variational autoencoder（LCBVAE）架构的Encoder和Decoder，用于将斜体阿拉伯数字日期翻译成填充了阿拉伯数字日期。</li>
<li>methods: 我们采用了自定义和适应CRNN模型，并将其训练在2019年至2027年的填充图像上，以提取日期和评估LCBVAE模型在日期识别方面的性能。</li>
<li>results: 我们发现，在LCBVAE架构中添加缓冲层可以提高总体化的性能，并在下游传播学习任务中实现了图像翻译的97%准确率。这种方法可以普适应任何下游学习任务，如图像翻译和重建。<details>
<summary>Abstract</summary>
THIS paper proposes an approach of Ladder Bottom-up Convolutional Bidirectional Variational Autoencoder (LCBVAE) architecture for the encoder and decoder, which is trained on the image translation of the dotted Arabic expiration dates by reconstructing the Arabic dotted expiration dates into filled-in expiration dates. We employed a customized and adapted version of Convolutional Recurrent Neural Network CRNN model to meet our specific requirements and enhance its performance in our context, and then trained the custom CRNN model with the filled-in images from the year of 2019 to 2027 to extract the expiration dates and assess the model performance of LCBVAE on the expiration date recognition. The pipeline of (LCBVAE+CRNN) can be then integrated into an automated sorting systems for extracting the expiry dates and sorting the products accordingly during the manufacture stage. Additionally, it can overcome the manual entry of expiration dates that can be time-consuming and inefficient at the merchants. Due to the lack of the availability of the dotted Arabic expiration date images, we created an Arabic dot-matrix True Type Font (TTF) for the generation of the synthetic images. We trained the model with unrealistic synthetic dates of 59902 images and performed the testing on a realistic synthetic date of 3287 images from the year of 2019 to 2027, represented as yyyy/mm/dd. In our study, we demonstrated the significance of latent bottleneck layer with improving the generalization when the size is increased up to 1024 in downstream transfer learning tasks as for image translation. The proposed approach achieved an accuracy of 97% on the image translation with using the LCBVAE architecture that can be generalized for any downstream learning tasks as for image translation and reconstruction.
</details>
<details>
<summary>摘要</summary>
本文提出了一种垂直卷积减采样变换自动编码器（LCBVAE）架构，用于编码器和解码器，用于图像翻译 arabic 黑点日期。我们采用了自定义和适应版本的卷积循环神经网络（CRNN）模型，以满足我们的特定需求，并在我们的上下文中进行了性能改进。然后，我们在2019年到2027年的 filled-in 图像上训练了自定义 CRNN 模型，以提取日期和评估 LCBVAE 模型在日期识别方面的性能。该管道可以在生产阶段 integrating 到自动化分类系统中，以提取日期并根据日期进行产品的分类。此外，它可以超越手动输入日期，因为这可能是时间consuming 和不可靠的。由于lack  arabic 黑点日期图像的可用性，我们创建了一个 arabic 黑点矩阵 True Type Font（TTF），用于生成 synthetic 图像。我们在59902 个不实际的日期图像上训练了模型，并在2019年到2027年的 realistic  synthetic 日期上进行测试，表示为 yyyy/mm/dd。在我们的研究中，我们发现了隐藏瓶颈层可以提高通用性，并且当隐藏瓶颈层的大小增加到 1024 时，在下游转移学习任务中可以提高通用性。我们的方法达到了 97% 的准确率在图像翻译任务中，并且可以普适应任何下游学习任务。
</details></li>
</ul>
<hr>
<h2 id="MOELoRA-An-MOE-based-Parameter-Efficient-Fine-Tuning-Method-for-Multi-task-Medical-Applications"><a href="#MOELoRA-An-MOE-based-Parameter-Efficient-Fine-Tuning-Method-for-Multi-task-Medical-Applications" class="headerlink" title="MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications"></a>MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18339">http://arxiv.org/abs/2310.18339</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liuqidong07/moelora-peft">https://github.com/liuqidong07/moelora-peft</a></li>
<li>paper_authors: Qidong Liu, Xian Wu, Xiangyu Zhao, Yuanshao Zhu, Derong Xu, Feng Tian, Yefeng Zheng</li>
<li>for: 这个研究旨在为大语言模型（LLMs）在医疗系统中进行微调，以应对实际医疗任务中的多种任务。</li>
<li>methods: 我们提出了一个叫做MOELoRA的参数效率微调框架，利用MOE的多任务学习和LoRA的参数效率微调。我们将专家分为两个低矩阵对，以保持小数目的参数。此外，我们提出了一个任务驱动的闸函数，可以调节各专家的贡献和生成不同任务的特有参数。</li>
<li>results: 我们在公开的多任务中文医疗数据集上进行了广泛的实验，结果显示MOELoRA可以超越现有的参数效率微调方法。<details>
<summary>Abstract</summary>
The recent surge in the field of Large Language Models (LLMs) has gained significant attention in numerous domains. In order to tailor an LLM to a specific domain such as a web-based healthcare system, fine-tuning with domain knowledge is necessary. However, two issues arise during fine-tuning LLMs for medical applications. The first is the problem of task variety, where there are numerous distinct tasks in real-world medical scenarios. This diversity often results in suboptimal fine-tuning due to data imbalance and seesawing problems. Additionally, the high cost of fine-tuning can be prohibitive, impeding the application of LLMs. The large number of parameters in LLMs results in enormous time and computational consumption during fine-tuning, which is difficult to justify. To address these two issues simultaneously, we propose a novel parameter-efficient fine-tuning framework for multi-task medical applications called MOELoRA. The framework aims to capitalize on the benefits of both MOE for multi-task learning and LoRA for parameter-efficient fine-tuning. To unify MOE and LoRA, we devise multiple experts as the trainable parameters, where each expert consists of a pair of low-rank matrices to maintain a small number of trainable parameters. Additionally, we propose a task-motivated gate function for all MOELoRA layers that can regulate the contributions of each expert and generate distinct parameters for various tasks. To validate the effectiveness and practicality of the proposed method, we conducted comprehensive experiments on a public multi-task Chinese medical dataset. The experimental results demonstrate that MOELoRA outperforms existing parameter-efficient fine-tuning methods. The implementation is available online for convenient reproduction of our experiments.
</details>
<details>
<summary>摘要</summary>
Recently, there has been a surge of interest in Large Language Models (LLMs) in various domains. To adapt an LLM to a specific domain like a web-based healthcare system, fine-tuning with domain knowledge is essential. However, there are two challenges during fine-tuning LLMs for medical applications. First, there are many diverse tasks in real-world medical scenarios, leading to suboptimal fine-tuning due to data imbalance and seesawing problems. Second, the high cost of fine-tuning can be prohibitive, making it difficult to apply LLMs. The large number of parameters in LLMs results in significant time and computational consumption during fine-tuning, which is difficult to justify. To address these two issues simultaneously, we propose a novel parameter-efficient fine-tuning framework for multi-task medical applications called MOELoRA. The framework combines the benefits of MOE for multi-task learning and LoRA for parameter-efficient fine-tuning.In our framework, we use multiple experts as the trainable parameters, where each expert consists of a pair of low-rank matrices to maintain a small number of trainable parameters. Additionally, we propose a task-motivated gate function for all MOELoRA layers that can regulate the contributions of each expert and generate distinct parameters for various tasks. To validate the effectiveness and practicality of the proposed method, we conducted comprehensive experiments on a public multi-task Chinese medical dataset. The experimental results show that MOELoRA outperforms existing parameter-efficient fine-tuning methods. The implementation is available online for convenient reproduction of our experiments.
</details></li>
</ul>
<hr>
<h2 id="On-the-Neural-Tangent-Kernel-of-Equilibrium-Models"><a href="#On-the-Neural-Tangent-Kernel-of-Equilibrium-Models" class="headerlink" title="On the Neural Tangent Kernel of Equilibrium Models"></a>On the Neural Tangent Kernel of Equilibrium Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14062">http://arxiv.org/abs/2310.14062</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhili Feng, J. Zico Kolter</li>
<li>for: 这个研究探讨了深度平衡（DEQ）模型的神经 Tangent kernel（NTK）。</li>
<li>methods: 研究使用了 root-finding 方法来有效地找到 DEQ 模型的 deterministic NTK。</li>
<li>results: 研究发现，尽管 Fully-connected 神经网络的 NTK 在宽度和深度都在无限大时可能是随机的，但 DEQ 模型在某些条件下仍然具有 deterministic NTK，并且可以通过 root-finding 方法有效地找到它。<details>
<summary>Abstract</summary>
This work studies the neural tangent kernel (NTK) of the deep equilibrium (DEQ) model, a practical ``infinite-depth'' architecture which directly computes the infinite-depth limit of a weight-tied network via root-finding. Even though the NTK of a fully-connected neural network can be stochastic if its width and depth both tend to infinity simultaneously, we show that contrarily a DEQ model still enjoys a deterministic NTK despite its width and depth going to infinity at the same time under mild conditions. Moreover, this deterministic NTK can be found efficiently via root-finding.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Composer-Style-specific-Symbolic-Music-Generation-Using-Vector-Quantized-Discrete-Diffusion-Models"><a href="#Composer-Style-specific-Symbolic-Music-Generation-Using-Vector-Quantized-Discrete-Diffusion-Models" class="headerlink" title="Composer Style-specific Symbolic Music Generation Using Vector Quantized Discrete Diffusion Models"></a>Composer Style-specific Symbolic Music Generation Using Vector Quantized Discrete Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14044">http://arxiv.org/abs/2310.14044</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jincheng Zhang, Jingjing Tang, Charalampos Saitis, György Fazekas</li>
<li>for: 这篇论文旨在应用vector quantized variational autoencoder（VQ-VAE）和数组diffusion模型，实现符合作曲者风格的象数音乐生成。</li>
<li>methods: 本文使用VQ-VAE将象数音乐转换为一系列的index，然后使用数组diffusion模型来模拟VQ-VAE的数组几何空间。</li>
<li>results: 实验结果显示，使用本文提出的方法可以实现符合作曲者风格的象数音乐生成，精度为72.36%。<details>
<summary>Abstract</summary>
Emerging Denoising Diffusion Probabilistic Models (DDPM) have become increasingly utilised because of promising results they have achieved in diverse generative tasks with continuous data, such as image and sound synthesis. Nonetheless, the success of diffusion models has not been fully extended to discrete symbolic music. We propose to combine a vector quantized variational autoencoder (VQ-VAE) and discrete diffusion models for the generation of symbolic music with desired composer styles. The trained VQ-VAE can represent symbolic music as a sequence of indexes that correspond to specific entries in a learned codebook. Subsequently, a discrete diffusion model is used to model the VQ-VAE's discrete latent space. The diffusion model is trained to generate intermediate music sequences consisting of codebook indexes, which are then decoded to symbolic music using the VQ-VAE's decoder. The results demonstrate our model can generate symbolic music with target composer styles that meet the given conditions with a high accuracy of 72.36%.
</details>
<details>
<summary>摘要</summary>
emerging Denoising Diffusion Probabilistic Models (DDPM) 已经越来越受到使用，因为它在不同的生成任务中的连续数据上表现出色，如图像和音频生成。然而， diffusion 模型尚未被完全扩展到字符串符号音乐。我们提议将量化变换自动编码器（VQ-VAE）和字符串扩散模型结合使用，以生成符号音乐 WITH 愿景作曲风格。训练过的 VQ-VAE 可以将符号音乐表示为一个序列的索引，这些索引与一个学习的码库中的特定条目相对应。然后，一个字符串扩散模型将 VQ-VAE 的字符串潜在空间模型化。扩散模型会生成符号音乐序列中的代码库索引，这些索引然后通过 VQ-VAE 的解码器转换为符号音乐。结果表明，我们的模型可以生成符号音乐 WITH 目标作曲风格，并且准确率为 72.36%。
</details></li>
</ul>
<hr>
<h2 id="Fast-Diffusion-GAN-Model-for-Symbolic-Music-Generation-Controlled-by-Emotions"><a href="#Fast-Diffusion-GAN-Model-for-Symbolic-Music-Generation-Controlled-by-Emotions" class="headerlink" title="Fast Diffusion GAN Model for Symbolic Music Generation Controlled by Emotions"></a>Fast Diffusion GAN Model for Symbolic Music Generation Controlled by Emotions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14040">http://arxiv.org/abs/2310.14040</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jincheng Zhang, György Fazekas, Charalampos Saitis</li>
<li>for: 本研究旨在用扩散模型和生成对抗网络（GAN）控制生成的符号音乐，以实现 targets 的情感控制。</li>
<li>methods: 我们首先使用已经训练过的变量自动编码器获取符号音乐数据集的嵌入，然后使用这些嵌入来训练扩散模型。</li>
<li>results: 我们的模型成功控制了生成的符号音乐，并且在计算成本方面具有显著改善，只需要四个时间步骤来减噪，而现有的扩散模型对符号音乐生成的计算成本是千万次。<details>
<summary>Abstract</summary>
Diffusion models have shown promising results for a wide range of generative tasks with continuous data, such as image and audio synthesis. However, little progress has been made on using diffusion models to generate discrete symbolic music because this new class of generative models are not well suited for discrete data while its iterative sampling process is computationally expensive. In this work, we propose a diffusion model combined with a Generative Adversarial Network, aiming to (i) alleviate one of the remaining challenges in algorithmic music generation which is the control of generation towards a target emotion, and (ii) mitigate the slow sampling drawback of diffusion models applied to symbolic music generation. We first used a trained Variational Autoencoder to obtain embeddings of a symbolic music dataset with emotion labels and then used those to train a diffusion model. Our results demonstrate the successful control of our diffusion model to generate symbolic music with a desired emotion. Our model achieves several orders of magnitude improvement in computational cost, requiring merely four time steps to denoise while the steps required by current state-of-the-art diffusion models for symbolic music generation is in the order of thousands.
</details>
<details>
<summary>摘要</summary>
Diffusion models have shown promising results for a wide range of generative tasks with continuous data, such as image and audio synthesis. However, little progress has been made on using diffusion models to generate discrete symbolic music because this new class of generative models are not well suited for discrete data while its iterative sampling process is computationally expensive. In this work, we propose a diffusion model combined with a Generative Adversarial Network, aiming to (i) alleviate one of the remaining challenges in algorithmic music generation, which is the control of generation towards a target emotion, and (ii) mitigate the slow sampling drawback of diffusion models applied to symbolic music generation. We first used a trained Variational Autoencoder to obtain embeddings of a symbolic music dataset with emotion labels and then used those to train a diffusion model. Our results demonstrate the successful control of our diffusion model to generate symbolic music with a desired emotion. Our model achieves several orders of magnitude improvement in computational cost, requiring merely four time steps to denoise while the steps required by current state-of-the-art diffusion models for symbolic music generation is in the order of thousands.Here's the translation in Traditional Chinese:Diffusion models have shown promising results for a wide range of generative tasks with continuous data, such as image and audio synthesis. However, little progress has been made on using diffusion models to generate discrete symbolic music because this new class of generative models are not well suited for discrete data while its iterative sampling process is computationally expensive. In this work, we propose a diffusion model combined with a Generative Adversarial Network, aiming to (i) alleviate one of the remaining challenges in algorithmic music generation, which is the control of generation towards a target emotion, and (ii) mitigate the slow sampling drawback of diffusion models applied to symbolic music generation. We first used a trained Variational Autoencoder to obtain embeddings of a symbolic music dataset with emotion labels and then used those to train a diffusion model. Our results demonstrate the successful control of our diffusion model to generate symbolic music with a desired emotion. Our model achieves several orders of magnitude improvement in computational cost, requiring merely four time steps to denoise while the steps required by current state-of-the-art diffusion models for symbolic music generation is in the order of thousands.
</details></li>
</ul>
<hr>
<h2 id="Small-Language-Models-Fine-tuned-to-Coordinate-Larger-Language-Models-improve-Complex-Reasoning"><a href="#Small-Language-Models-Fine-tuned-to-Coordinate-Larger-Language-Models-improve-Complex-Reasoning" class="headerlink" title="Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning"></a>Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18338">http://arxiv.org/abs/2310.18338</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lcs2-iiitd/daslam">https://github.com/lcs2-iiitd/daslam</a></li>
<li>paper_authors: Gurusha Juneja, Subhabrata Dutta, Soumen Chakrabarti, Sunny Manchanda, Tanmoy Chakraborty</li>
<li>for: 提高大语言模型（LLM）的链式思维能力，解决复杂多步骤的理性问题。</li>
<li>methods: 使用 decomposition generator 将复杂问题 decomposes 成需 fewer reasoning steps 的子问题，然后使用 solver 解决子问题。</li>
<li>results: 使用 DaSLaM 方法，可以使用 comparable 或更好的性能，与 orders-of-magnitude 更大的 GPT-4 相比。此外， DaSLaM 方法不受 solver 的scale 影响，可以使用 diverse 大小的 solver LM 获得显著的性能提升。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) prompted to generate chain-of-thought (CoT) exhibit impressive reasoning capabilities. Recent attempts at prompt decomposition toward solving complex, multi-step reasoning problems depend on the ability of the LLM to simultaneously decompose and solve the problem. A significant disadvantage is that foundational LLMs are typically not available for fine-tuning, making adaptation computationally prohibitive. We believe (and demonstrate) that problem decomposition and solution generation are distinct capabilites, better addressed in separate modules, than by one monolithic LLM. We introduce DaSLaM, which uses a decomposition generator to decompose complex problems into subproblems that require fewer reasoning steps. These subproblems are answered by a solver. We use a relatively small (13B parameters) LM as the decomposition generator, which we train using policy gradient optimization to interact with a solver LM (regarded as black-box) and guide it through subproblems, thereby rendering our method solver-agnostic. Evaluation on multiple different reasoning datasets reveal that with our method, a 175 billion parameter LM (text-davinci-003) can produce competitive or even better performance, compared to its orders-of-magnitude larger successor, GPT-4. Additionally, we show that DaSLaM is not limited by the solver's capabilities as a function of scale; e.g., solver LMs with diverse sizes give significant performance improvement with our solver-agnostic decomposition technique. Exhaustive ablation studies evince the superiority of our modular finetuning technique over exorbitantly large decomposer LLMs, based on prompting alone.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）因为被调动产生链式思维（CoT）而表现出卓越的推理能力。现在的尝试通过问题分解来解决复杂多步骤的问题，很大程度上取决于LLM的能力同时进行问题分解和解决。然而，基础LLM通常不可以进行微调，从而使得适应成本高昂。我们认为（并证明）问题分解和解决是不同的能力，更好地通过分类模组来处理，而不是单一的LLM。我们称之为DaSLaM，它使用问题分解生成器将复杂问题分解成需要 fewer 推理步骤的子问题。这些子问题由解决器回答。我们使用一个相对较小的（13B个参数）LM作为问题分解生成器，并使用政策倾斜优化训练它与解决器LM（被视为黑盒）互动，以导引它通过子问题，因此让我们的方法成为解决器无关的。我们在多个不同的推理数据集上进行评估，发现我们的方法可以让1750亿个参数的LM（text-davinci-003）生成竞争或更好的性能，相比之下它的规模增加了许多。此外，我们还证明了DaSLaM不受解决器的规模影响，例如，不同大小的解决器LM可以为我们的模块化微调技术带来很大的性能提升。我们进行了详细的剥夺研究，证明我们的模块化微调技术在Prompting alone下比极大的问题分解LLM更有优势。
</details></li>
</ul>
<hr>
<h2 id="Contrast-Everything-A-Hierarchical-Contrastive-Framework-for-Medical-Time-Series"><a href="#Contrast-Everything-A-Hierarchical-Contrastive-Framework-for-Medical-Time-Series" class="headerlink" title="Contrast Everything: A Hierarchical Contrastive Framework for Medical Time-Series"></a>Contrast Everything: A Hierarchical Contrastive Framework for Medical Time-Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14017">http://arxiv.org/abs/2310.14017</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dl4mhealth/comet">https://github.com/dl4mhealth/comet</a></li>
<li>paper_authors: Yihe Wang, Yu Han, Haishuai Wang, Xiang Zhang</li>
<li>for: 这个研究旨在提高医疗时间序列分析中的相似表现学习，以便更好地利用医疗时间序列中的资讯，减少专业人员的努力和时间投入。</li>
<li>methods: 本研究提出了一个创新的层次架构，叫做COMET，它在医疗时间序列中捕捉到四个可能的水平的数据一致性：观察、样本、实验和病人水平。通过开发多个水平的对照损失函数，我们可以学习有效的表现，并实现自动化的医疗时间序列分析。</li>
<li>results: 我们在具有10%和1%的标签数据分布的挑战性设置下实现了实验，并与六个基eline进行比较。结果显示，COMET在所有基eline中都表现出色，特别是在10%和1%的标签数据分布下的设置中。这些结果证明了我们的框架在医疗时间序列相似表现学习中的重要性。<details>
<summary>Abstract</summary>
Contrastive representation learning is crucial in medical time series analysis as it alleviates dependency on labor-intensive, domain-specific, and scarce expert annotations. However, existing contrastive learning methods primarily focus on one single data level, which fails to fully exploit the intricate nature of medical time series. To address this issue, we present COMET, an innovative hierarchical framework that leverages data consistencies at all inherent levels in medical time series. Our meticulously designed model systematically captures data consistency from four potential levels: observation, sample, trial, and patient levels. By developing contrastive loss at multiple levels, we can learn effective representations that preserve comprehensive data consistency, maximizing information utilization in a self-supervised manner. We conduct experiments in the challenging patient-independent setting. We compare COMET against six baselines using three diverse datasets, which include ECG signals for myocardial infarction and EEG signals for Alzheimer's and Parkinson's diseases. The results demonstrate that COMET consistently outperforms all baselines, particularly in setup with 10% and 1% labeled data fractions across all datasets. These results underscore the significant impact of our framework in advancing contrastive representation learning techniques for medical time series. The source code is available at https://github.com/DL4mHealth/COMET.
</details>
<details>
<summary>摘要</summary>
医疗时序分析中，对比表示学学习是关键，因为它减轻了劳动密集、领域特定和珍贵专家标注的依赖。然而，现有的对比学习方法主要集中在单一数据层次，这会无法全面利用医疗时序的复杂性。为解决这个问题，我们提出了COMET，一种创新的层次结构框架，利用医疗时序中所有自然层次的数据一致性。我们 méticulously 设计的模型逐级捕捉数据一致性，从观察、样本、试验和患者四个级别进行对比学习。通过开发多级对比损失函数，我们可以学习有效的表示，保留医疗时序中完整的数据一致性，最大化自我监督的信息利用。我们在医疗时序中的挑战性 patrnt-independent 设置中进行实验，与六个基线比较。我们使用三种多样化的数据集，包括ECG信号和 Alzheimer's 和 Parkinson's 疾病的 EEG 信号。结果表明，COMET 在所有基线之上具有优异表现，特别是在10%和1%标注数据分布中的设置中。这些结果赋予COMET 在医疗时序对比表示学习技术的进步。COMET 的源代码可以在 GitHub 上获取：https://github.com/DL4mHealth/COMET。
</details></li>
</ul>
<hr>
<h2 id="One-is-More-Diverse-Perspectives-within-a-Single-Network-for-Efficient-DRL"><a href="#One-is-More-Diverse-Perspectives-within-a-Single-Network-for-Efficient-DRL" class="headerlink" title="One is More: Diverse Perspectives within a Single Network for Efficient DRL"></a>One is More: Diverse Perspectives within a Single Network for Efficient DRL</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14009">http://arxiv.org/abs/2310.14009</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiqin Tan, Ling Pan, Longbo Huang</li>
<li>for: 这 paper 是用于提高 deep reinforcement learning 的效率和稳定性的研究。</li>
<li>methods: 这 paper 使用了多个子网络（OMNet），每个子网络输出不同的结果，从而提高了学习效率和鲁棒性。</li>
<li>results: 通过在 MuJoCo 测试集上进行 comprehensive 评估， authors 发现 OMNet 能够很好地寻找效果和计算成本之间的平衡。<details>
<summary>Abstract</summary>
Deep reinforcement learning has achieved remarkable performance in various domains by leveraging deep neural networks for approximating value functions and policies. However, using neural networks to approximate value functions or policy functions still faces challenges, including low sample efficiency and overfitting. In this paper, we introduce OMNet, a novel learning paradigm utilizing multiple subnetworks within a single network, offering diverse outputs efficiently. We provide a systematic pipeline, including initialization, training, and sampling with OMNet. OMNet can be easily applied to various deep reinforcement learning algorithms with minimal additional overhead. Through comprehensive evaluations conducted on MuJoCo benchmark, our findings highlight OMNet's ability to strike an effective balance between performance and computational cost.
</details>
<details>
<summary>摘要</summary>
深度强化学习已在多个领域取得了显著的成绩，通过使用深度神经网络来近似价值函数和策略函数。然而，使用神经网络来近似价值函数或策略函数仍然面临挑战，包括低样本效率和过拟合。在本文中，我们介绍了OMNet，一种新的学习模式，它在单个网络中嵌入多个子网络，以获得多种输出，高效地。我们提供了一个系统化的管道，包括初始化、训练和采样，以及OMNet可以轻松应用于多种深度强化学习算法，增加了最小的额外开销。通过对MuJoCo benchmark进行了全面的评估，我们的发现表明OMNet能够均衡性能和计算成本。
</details></li>
</ul>
<hr>
<h2 id="On-Bilingual-Lexicon-Induction-with-Large-Language-Models"><a href="#On-Bilingual-Lexicon-Induction-with-Large-Language-Models" class="headerlink" title="On Bilingual Lexicon Induction with Large Language Models"></a>On Bilingual Lexicon Induction with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13995">http://arxiv.org/abs/2310.13995</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cambridgeltl/prompt4bli">https://github.com/cambridgeltl/prompt4bli</a></li>
<li>paper_authors: Yaoyiran Li, Anna Korhonen, Ivan Vulić</li>
<li>for: 本研究旨在探讨使用最新一代大语言模型（LLMs）来开发双语词表。</li>
<li>methods: 我们采用了零批示推理、几批示推理和标准的BLI预训练方法来评估多语言模型（mLLMs）的应用性。</li>
<li>results: 我们的实验结果显示，使用几批示推理的 nearest neighbours 示例可以达到最佳性能，并创造了许多语言对的新纪录。<details>
<summary>Abstract</summary>
Bilingual Lexicon Induction (BLI) is a core task in multilingual NLP that still, to a large extent, relies on calculating cross-lingual word representations. Inspired by the global paradigm shift in NLP towards Large Language Models (LLMs), we examine the potential of the latest generation of LLMs for the development of bilingual lexicons. We ask the following research question: Is it possible to prompt and fine-tune multilingual LLMs (mLLMs) for BLI, and how does this approach compare against and complement current BLI approaches? To this end, we systematically study 1) zero-shot prompting for unsupervised BLI and 2) few-shot in-context prompting with a set of seed translation pairs, both without any LLM fine-tuning, as well as 3) standard BLI-oriented fine-tuning of smaller LLMs. We experiment with 18 open-source text-to-text mLLMs of different sizes (from 0.3B to 13B parameters) on two standard BLI benchmarks covering a range of typologically diverse languages. Our work is the first to demonstrate strong BLI capabilities of text-to-text mLLMs. The results reveal that few-shot prompting with in-context examples from nearest neighbours achieves the best performance, establishing new state-of-the-art BLI scores for many language pairs. We also conduct a series of in-depth analyses and ablation studies, providing more insights on BLI with (m)LLMs, also along with their limitations.
</details>
<details>
<summary>摘要</summary>
global paradigm shift в NLP towards Large Language Models (LLMs) 我们从 LLMs 的最新一代获得了开发双语词汇的潜力。我们的研究问题是：可以将多ilingual LLMs (mLLMs) 用于双语词汇问题（BLI）的问题，并且如何与现有的 BLI 方法相比。为此，我们系统地研究了以下问题：1. 零shot 提示，不需要精度批评的 BLI。2. 几个 shot 的内容提示，使用一组seed translation pairs，而不需要精度批评。3. 标准的 BLI-oriented 精度批评。我们实验了 18 个开源的文本至文本 mLLMs ，它们的大小在 0.3B 到 13B 个参数之间，在两个标准的 BLI 库中进行了评估。我们的研究是首次证明了文本至文本 mLLMs 的强大 BLI 能力。结果显示，几个 shot 的内容提示可以取得最好的性能，建立了许多语言对的新的顶峰 BLI 分数。我们还进行了详细的分析和剔除研究，提供了更多关于 BLI 的关键。
</details></li>
</ul>
<hr>
<h2 id="Application-of-deep-and-reinforcement-learning-to-boundary-control-problems"><a href="#Application-of-deep-and-reinforcement-learning-to-boundary-control-problems" class="headerlink" title="Application of deep and reinforcement learning to boundary control problems"></a>Application of deep and reinforcement learning to boundary control problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15191">http://arxiv.org/abs/2310.15191</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zenineasa/MasterThesis">https://github.com/zenineasa/MasterThesis</a></li>
<li>paper_authors: Zenin Easa Panthakkalakath, Juraj Kardoš, Olaf Schenk</li>
<li>for: 本研究旨在使用深度学习和强化学习解决边控制问题。</li>
<li>methods: 我们采用迭代优化策略，使用空间神经网络构建初始猜测，并使用维度-时间神经网络学习迭代优化算法。</li>
<li>results: 我们的数据培育和测试表明，提议的方法可以与现有的解决方案相比，速度和准确性相当。在我们的初步结果中，网络实现成本比IPOPT低于51%的情况。<details>
<summary>Abstract</summary>
The boundary control problem is a non-convex optimization and control problem in many scientific domains, including fluid mechanics, structural engineering, and heat transfer optimization. The aim is to find the optimal values for the domain boundaries such that the enclosed domain adhering to the governing equations attains the desired state values. Traditionally, non-linear optimization methods, such as the Interior-Point method (IPM), are used to solve such problems.   This project explores the possibilities of using deep learning and reinforcement learning to solve boundary control problems. We adhere to the framework of iterative optimization strategies, employing a spatial neural network to construct well-informed initial guesses, and a spatio-temporal neural network learns the iterative optimization algorithm using policy gradients. Synthetic data, generated from the problems formulated in the literature, is used for training, testing and validation. The numerical experiments indicate that the proposed method can rival the speed and accuracy of existing solvers. In our preliminary results, the network attains costs lower than IPOPT, a state-of-the-art non-linear IPM, in 51\% cases. The overall number of floating point operations in the proposed method is similar to that of IPOPT. Additionally, the informed initial guess method and the learned momentum-like behaviour in the optimizer method are incorporated to avoid convergence to local minima.
</details>
<details>
<summary>摘要</summary>
“边界控制问题是科学领域中的一种非 convex 优化和控制问题，包括流体动力学、结构工程和热传输优化等。目标是找到包含 governing 方程的Domaint 的优化值，使涵盖的 Domaint 达到所需的状态值。传统上，非线性优化方法，如Interior-Point 方法（IPM），用于解决这些问题。”This project explores the use of deep learning and reinforcement learning to solve boundary control problems. We use an iterative optimization strategy, with a spatial neural network constructing well-informed initial guesses and a spatio-temporal neural network learning the iterative optimization algorithm using policy gradients. Synthetic data, generated from problems formulated in the literature, is used for training, testing, and validation. Our numerical experiments show that the proposed method can rival the speed and accuracy of existing solvers. In our preliminary results, the network attains costs lower than IPOPT, a state-of-the-art non-linear IPM, in 51% of cases. The overall number of floating point operations in the proposed method is similar to that of IPOPT. Additionally, we incorporate an informed initial guess method and a learned momentum-like behavior in the optimizer to avoid convergence to local minima.
</details></li>
</ul>
<hr>
<h2 id="Ensemble-Instruct-Generating-Instruction-Tuning-Data-with-a-Heterogeneous-Mixture-of-LMs"><a href="#Ensemble-Instruct-Generating-Instruction-Tuning-Data-with-a-Heterogeneous-Mixture-of-LMs" class="headerlink" title="Ensemble-Instruct: Generating Instruction-Tuning Data with a Heterogeneous Mixture of LMs"></a>Ensemble-Instruct: Generating Instruction-Tuning Data with a Heterogeneous Mixture of LMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13961">http://arxiv.org/abs/2310.13961</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ibm/ensemble-instruct">https://github.com/ibm/ensemble-instruct</a></li>
<li>paper_authors: Young-Suk Lee, Md Arafat Sultan, Yousef El-Kurdi, Tahira Naseem Asim Munawar, Radu Florian, Salim Roukos, Ramón Fernandez Astudillo</li>
<li>for: 用于自动生成数据，提高对话机器人的强度，只需要小量的人工监督。</li>
<li>methods: 使用 Self-Instruct 和 Alpaca 等技术，训练小型语言模型（10B–40B参数），并使用 permissive licenses。</li>
<li>results: 提高自动生成数据的质量，对于小型语言模型和大型语言模型都有显著提高，并且小型 instruction-tuned LM 生成的输出更有用。<details>
<summary>Abstract</summary>
Using in-context learning (ICL) for data generation, techniques such as Self-Instruct (Wang et al., 2023) or the follow-up Alpaca (Taori et al., 2023) can train strong conversational agents with only a small amount of human supervision. One limitation of these approaches is that they resort to very large language models (around 175B parameters) that are also proprietary and non-public. Here we explore the application of such techniques to language models that are much smaller (around 10B--40B parameters) and have permissive licenses. We find the Self-Instruct approach to be less effective at these sizes and propose new ICL methods that draw on two main ideas: (a) Categorization and simplification of the ICL templates to make prompt learning easier for the LM, and (b) Ensembling over multiple LM outputs to help select high-quality synthetic examples. Our algorithm leverages the 175 Self-Instruct seed tasks and employs separate pipelines for instructions that require an input and instructions that do not. Empirical investigations with different LMs show that: (1) Our proposed method yields higher-quality instruction tuning data than Self-Instruct, (2) It improves performances of both vanilla and instruction-tuned LMs by significant margins, and (3) Smaller instruction-tuned LMs generate more useful outputs than their larger un-tuned counterparts. Our codebase is available at https://github.com/IBM/ensemble-instruct.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:使用增Context学习（ICL）数据生成技术，如Self-Instruct（Wang et al., 2023）或Alpaca（Taori et al., 2023），可以帮助强化对话机器人，只需小量人工监督。这些方法的一个限制是它们需要非常大的语言模型（约175B参数），这些模型也是 propriety 和非公共的。在这里，我们探索将这些技术应用于更小的语言模型（约10B--40B参数），这些模型具有允许的许可证。我们发现Self-Instruct方法在这些大小下不太有效，我们提出了新的ICL方法，它们基于以下两个主要想法：（a）将ICL模板分类和简化，使Language Model（LM）更容易学习提示，和（b）将多个LM输出ensemble，以选择高质量的人工示例。我们的算法利用Self-Instruct的175个种子任务，并使用不同的LM pipeline，对于需要输入和不需要输入的指令而分开。我们的实验表明，我们的提posed方法可以生成更高质量的指令调整数据，并且可以提高vanilla LM和指令调整LM的性能，同时使小型指令调整LM生成更有用的输出。我们的代码库可以在https://github.com/IBM/ensemble-instruct中获取。
</details></li>
</ul>
<hr>
<h2 id="Towards-dialogue-based-computer-aided-software-requirements-elicitation"><a href="#Towards-dialogue-based-computer-aided-software-requirements-elicitation" class="headerlink" title="Towards dialogue based, computer aided software requirements elicitation"></a>Towards dialogue based, computer aided software requirements elicitation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13953">http://arxiv.org/abs/2310.13953</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vasiliy Seibert</li>
<li>for: 这篇论文是为了探讨如何从自然语言规范中提取模型的问题。</li>
<li>methods: 这篇论文提出了一种对话基于的计算机支持的软件需求分析方法，而不是先前的模型提取方法，它鼓励个性、创造力和真实的妥协。</li>
<li>results: 这篇论文通过简单的实验示例了这种方法的核心思想，并讨论了这种方法和现有方法的比较。同时，它也认为未来的自然语言处理和生成AI技术的进步可能会带来重要的进步。<details>
<summary>Abstract</summary>
Several approaches have been presented, which aim to extract models from natural language specifications. These approaches have inherent weaknesses for they assume an initial problem understanding that is perfect, and they leave no room for feedback. Motivated by real-world collaboration settings between requirements engineers and customers, this paper proposes an interaction blueprint that aims for dialogue based, computer aided software requirements analysis. Compared to mere model extraction approaches, this interaction blueprint encourages individuality, creativity and genuine compromise. A simplistic Experiment was conducted to showcase the general idea. This paper discusses the experiment as well as the proposed interaction blueprint and argues, that advancements in natural language processing and generative AI might lead to significant progress in a foreseeable future. However, for that, there is a need to move away from a magical black box expectation and instead moving towards a dialogue based approach that recognizes the individuality that is an undeniable part of requirements engineering.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:有几种方法已经被提出来，以EXTRACT模型从自然语言规格。这些方法具有内在的弱点，因为它们假设问题理解是完美的，并且没有减 feedback 的机制。由实际世界的合作场景中的需求工程师和客户而受到 inspirited，这篇论文提出了对话基本的交互蓝图，用于计算机助成的软件需求分析。这种方法强调个性、创造力和真实的妥协，并在简单的实验中进行了示例。这篇论文讨论了实验和提出的交互蓝图，并 argued  That advancements in自然语言处理和生成 AI 可能会在未来导致显著的进步，但我们需要停止对 "黑盒子" 的期望，而是转向对话基本的方法，认可需求工程的个性。
</details></li>
</ul>
<hr>
<h2 id="Approximate-Implication-for-Probabilistic-Graphical-Models"><a href="#Approximate-Implication-for-Probabilistic-Graphical-Models" class="headerlink" title="Approximate Implication for Probabilistic Graphical Models"></a>Approximate Implication for Probabilistic Graphical Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13942">http://arxiv.org/abs/2310.13942</a></li>
<li>repo_url: None</li>
<li>paper_authors: Batya Kenig</li>
<li>for: 本研究证明了关于 Conditional Independence (CI) 在 Probabilistic Graphical Models (PGMs) 中的准确性问题。</li>
<li>methods: 本文使用了现有的系统推理方法，以及新的证明方法，来研究 CI 的准确性。</li>
<li>results: 本文证明了对于非导向图模型，无法提供任何保证，而对于导向图模型，使用 $d$-separation 算法可以提供准确的 CI。 Additionally, the paper establishes improved approximation guarantees for independence relations derived from marginal and saturated CIs.<details>
<summary>Abstract</summary>
The graphical structure of Probabilistic Graphical Models (PGMs) represents the conditional independence (CI) relations that hold in the modeled distribution. Every separator in the graph represents a conditional independence relation in the distribution, making them the vehicle through which new conditional independencies are inferred and verified. The notion of separation in graphs depends on whether the graph is directed (i.e., a Bayesian Network), or undirected (i.e., a Markov Network).   The premise of all current systems-of-inference for deriving CIs in PGMs, is that the set of CIs used for the construction of the PGM hold exactly. In practice, algorithms for extracting the structure of PGMs from data discover approximate CIs that do not hold exactly in the distribution. In this paper, we ask how the error in this set propagates to the inferred CIs read off the graphical structure. More precisely, what guarantee can we provide on the inferred CI when the set of CIs that entailed it hold only approximately? It has recently been shown that in the general case, no such guarantee can be provided.   In this work, we prove new negative and positive results concerning this problem. We prove that separators in undirected PGMs do not necessarily represent approximate CIs. That is, no guarantee can be provided for CIs inferred from the structure of undirected graphs. We prove that such a guarantee exists for the set of CIs inferred in directed graphical models, making the $d$-separation algorithm a sound and complete system for inferring approximate CIs. We also establish improved approximation guarantees for independence relations derived from marginal and saturated CIs.
</details>
<details>
<summary>摘要</summary>
PGMs（概率图）的图Structural representation表示模型中的 conditional independence（CI）关系。每个分隔器在图中表示模型中的CI关系，使其成为新的CI关系的推理和验证的渠道。图中的分隔器取决于图是指向的（即 bayesian network）还是无向的（即 markov network）。现有所有系统的推理方法都基于CI关系的集合在PGM中准确地满足。在实际中，数据中PGM的结构检索算法发现的CI关系不准确地存在于分布中。在这篇论文中，我们问 Error propagation in inferred CIs 问题的解决方案。即在PGM中的CI关系是否准确地推理出来？我们证明了一些新的负面和正面结果。在无向PGM中，分隔器不一定表示CI关系的近似。即，无法提供PGM中的CI关系的准确性 garantia。而在指向PGM中，我们证明了$d$-separation算法是一个准确和完整的系统，用于推理CI关系。此外，我们还证明了基于边缘和满足CI关系的约束的约束 CI 关系的改进了近似性保证。
</details></li>
</ul>
<hr>
<h2 id="The-Hidden-Adversarial-Vulnerabilities-of-Medical-Federated-Learning"><a href="#The-Hidden-Adversarial-Vulnerabilities-of-Medical-Federated-Learning" class="headerlink" title="The Hidden Adversarial Vulnerabilities of Medical Federated Learning"></a>The Hidden Adversarial Vulnerabilities of Medical Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13893">http://arxiv.org/abs/2310.13893</a></li>
<li>repo_url: None</li>
<li>paper_authors: Erfan Darzi, Florian Dubost, Nanna. M. Sijtsema, P. M. A van Ooijen</li>
<li>for: 这个论文探讨了联合医疗图像分析系统对 adversarial 攻击的感受性。</li>
<li>methods: 该分析发现了一种新的攻击途径：利用先前全局模型更新的梯度信息，攻击者可以提高他们的攻击效率和传播性，而无需额外的计算成本增加。</li>
<li>results: 研究表明，适当初始化后的单步攻击（例如 FGSM）可以超越其迭代对手的效率，但需要更少的计算资源。这些发现强调了在联合医疗设备中应对 AI 安全的需要。<details>
<summary>Abstract</summary>
In this paper, we delve into the susceptibility of federated medical image analysis systems to adversarial attacks. Our analysis uncovers a novel exploitation avenue: using gradient information from prior global model updates, adversaries can enhance the efficiency and transferability of their attacks. Specifically, we demonstrate that single-step attacks (e.g. FGSM), when aptly initialized, can outperform the efficiency of their iterative counterparts but with reduced computational demand. Our findings underscore the need to revisit our understanding of AI security in federated healthcare settings.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们探讨了联合医疗图像分析系统中的恶意攻击的感受性。我们的分析发现了一个新的攻击途径：使用先前全球模型更新的梯度信息，恶意者可以提高攻击的效率和传播性。具体来说，我们表明了使用单步攻击（如FGSM），当初始化得当时，可以超过迭代攻击的效率，但却减少计算强度。我们的发现强调了在联合医疗设施中AI安全的重要性。
</details></li>
</ul>
<hr>
<h2 id="COVIDFakeExplainer-An-Explainable-Machine-Learning-based-Web-Application-for-Detecting-COVID-19-Fake-News"><a href="#COVIDFakeExplainer-An-Explainable-Machine-Learning-based-Web-Application-for-Detecting-COVID-19-Fake-News" class="headerlink" title="COVIDFakeExplainer: An Explainable Machine Learning based Web Application for Detecting COVID-19 Fake News"></a>COVIDFakeExplainer: An Explainable Machine Learning based Web Application for Detecting COVID-19 Fake News</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13890">http://arxiv.org/abs/2310.13890</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/DatProGuy/COVIDFakeExplainer">https://github.com/DatProGuy/COVIDFakeExplainer</a></li>
<li>paper_authors: Dylan Warman, Muhammad Ashad Kabir</li>
<li>for: 这篇论文旨在提供一个实用的伪新闻检测解决方案，以帮助社会对伪新闻进行有效防范。</li>
<li>methods: 本论文使用机器学习技术，包括深度学习方法，以探索伪新闻检测的可能性。特别是，本论文使用BERT模型，并将其应用于实际的伪新闻检测和解释。</li>
<li>results: 本论文的实验结果显示，BERT模型在检测COVID-19相关伪新闻方面 exhibits 高度的准确性。此外，本论文还提出了一个可读性增强的BERT模型，并将其作为一个服务通过AWS云端API主机。<details>
<summary>Abstract</summary>
Fake news has emerged as a critical global issue, magnified by the COVID-19 pandemic, underscoring the need for effective preventive tools. Leveraging machine learning, including deep learning techniques, offers promise in combatting fake news. This paper goes beyond by establishing BERT as the superior model for fake news detection and demonstrates its utility as a tool to empower the general populace. We have implemented a browser extension, enhanced with explainability features, enabling real-time identification of fake news and delivering easily interpretable explanations. To achieve this, we have employed two publicly available datasets and created seven distinct data configurations to evaluate three prominent machine learning architectures. Our comprehensive experiments affirm BERT's exceptional accuracy in detecting COVID-19-related fake news. Furthermore, we have integrated an explainability component into the BERT model and deployed it as a service through Amazon's cloud API hosting (AWS). We have developed a browser extension that interfaces with the API, allowing users to select and transmit data from web pages, receiving an intelligible classification in return. This paper presents a practical end-to-end solution, highlighting the feasibility of constructing a holistic system for fake news detection, which can significantly benefit society.
</details>
<details>
<summary>摘要</summary>
假新闻在全球范围内已成为一个严重的问题，COVID-19大流行的爆发进一步强调了需要有效的预防工具。利用机器学习，包括深度学习技术，可能在打击假新闻方面提供希望。这篇论文超越了现有的研究，将BERT模型确定为假新闻检测的最佳模型，并将其作为普通民众 empower 的工具。我们开发了一款浏览器扩展程序，该扩展程序具有解释性特性，可以在实时检测假新闻并提供可读性高的解释。为了实现这一点，我们使用了两个公共可用的数据集，并创建了七种不同的数据配置来评估三种知名的机器学习架构。我们的广泛的实验证明了 COVID-19 相关的假新闻检测BERT模型的突出性。此外，我们将BERT模型集成了解释性Component，并通过Amazon云API主机（AWS）部署为服务。我们开发了一款浏览器扩展程序，该扩展程序可以从网页上选择和传输数据，并得到可读性高的分类结果。本论文提出了一个实用的端到端解决方案， highlighting 社会的可能性建立一个总体的假新闻检测系统，该系统可以对社会产生很大的好处。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/21/cs.AI_2023_10_21/" data-id="clp88dbrt005yob88580k1dah" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/21/cs.CL_2023_10_21/" class="article-date">
  <time datetime="2023-10-21T11:00:00.000Z" itemprop="datePublished">2023-10-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/21/cs.CL_2023_10_21/">cs.CL - 2023-10-21</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Structural-generalization-in-COGS-Supertagging-is-almost-all-you-need"><a href="#Structural-generalization-in-COGS-Supertagging-is-almost-all-you-need" class="headerlink" title="Structural generalization in COGS: Supertagging is (almost) all you need"></a>Structural generalization in COGS: Supertagging is (almost) all you need</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14124">http://arxiv.org/abs/2310.14124</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alban-petit/semantic-supertag-parser">https://github.com/alban-petit/semantic-supertag-parser</a></li>
<li>paper_authors: Alban Petit, Caio Corro, François Yvon</li>
<li>for: 提高 neural network 在不同类型的语言模型中的泛化能力</li>
<li>methods: 提出了一种基于图的semantic parsing框架，并对其进行了多种扩展以解决泛化问题</li>
<li>results: 实验结果表明，我们的方法可以在COGS dataset中提高泛化能力，特别是在需要结构泛化的例子上得到了显著提高<details>
<summary>Abstract</summary>
In many Natural Language Processing applications, neural networks have been found to fail to generalize on out-of-distribution examples. In particular, several recent semantic parsing datasets have put forward important limitations of neural networks in cases where compositional generalization is required. In this work, we extend a neural graph-based semantic parsing framework in several ways to alleviate this issue. Notably, we propose: (1) the introduction of a supertagging step with valency constraints, expressed as an integer linear program; (2) a reduction of the graph prediction problem to the maximum matching problem; (3) the design of an incremental early-stopping training strategy to prevent overfitting. Experimentally, our approach significantly improves results on examples that require structural generalization in the COGS dataset, a known challenging benchmark for compositional generalization. Overall, our results confirm that structural constraints are important for generalization in semantic parsing.
</details>
<details>
<summary>摘要</summary>
多种自然语言处理应用程序中，神经网络通常无法泛化到不同分布中的示例。特别是在需要 Compositional Generalization 的 semantic parsing 数据集中，神经网络表现出了重要的局限性。在这种情况下，我们对一种基于神经网络的 semantic parsing 框架进行了多种扩展，以解决这个问题。主要提议包括：1. 引入精度标记步骤，使用整数线性Programming来表达 valency 约束。2. 将图像预测问题转换为最大匹配问题。3. 设计了逐步停止训练策略，以避免过拟合。实验表明，我们的方法可以在 COGS 数据集中，解决需要结构泛化的示例中显著提高结果。总之，我们的结果证明了结构约束对泛化的重要性。
</details></li>
</ul>
<hr>
<h2 id="Finite-context-Indexing-of-Restricted-Output-Space-for-NLP-Models-Facing-Noisy-Input"><a href="#Finite-context-Indexing-of-Restricted-Output-Space-for-NLP-Models-Facing-Noisy-Input" class="headerlink" title="Finite-context Indexing of Restricted Output Space for NLP Models Facing Noisy Input"></a>Finite-context Indexing of Restricted Output Space for NLP Models Facing Noisy Input</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14110">http://arxiv.org/abs/2310.14110</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mnhng/firo">https://github.com/mnhng/firo</a></li>
<li>paper_authors: Minh Nguyen, Nancy F. Chen</li>
<li>for: 提高 NLP 模型对不净输入的性能，而不是降低清晰输入的性能。</li>
<li>methods: FiRo 方法使用 finite-context aggregation 获取上下文嵌入，并在受限的输出空间内查找静止的表示。</li>
<li>results: FiRo 方法在六个分类任务和一个序列标注任务上，以不同程度的噪声为输入，与基eline相比表现出色。<details>
<summary>Abstract</summary>
NLP models excel on tasks with clean inputs, but are less accurate with noisy inputs. In particular, character-level noise such as human-written typos and adversarially-engineered realistic-looking misspellings often appears in text and can easily trip up NLP models. Prior solutions to address character-level noise often alter the content of the inputs (low fidelity), thus inadvertently lowering model accuracy on clean inputs. We proposed FiRo, an approach to boost NLP model performance on noisy inputs without sacrificing performance on clean inputs. FiRo sanitizes the input text while preserving its fidelity by inferring the noise-free form for each token in the input. FiRo uses finite-context aggregation to obtain contextual embeddings which is then used to find the noise-free form within a restricted output space. The output space is restricted to a small cluster of probable candidates in order to predict the noise-free tokens more accurately. Although the clusters are small, FiRo's effective vocabulary (union of all clusters) can be scaled up to better preserve the input content. Experimental results show NLP models that use FiRo outperforming baselines on six classification tasks and one sequence labeling task at various degrees of noise.
</details>
<details>
<summary>摘要</summary>
FiRo 使用 finite-context aggregation 获取上下文嵌入，然后使用 restricted output space 来预测噪音自由形。输出空间是限制在一小 clusters 中，以更准确地预测噪音自由形。虽然 clusters 是小的，但FiRo 的有效词汇（union of all clusters）可以被扩展，以更好地保持输入内容。实验结果表明，使用 FiRo 的 NLP 模型在六个分类任务和一个序列标签任务上表现出色，在不同程度的噪音下都高于基eline。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Knowledge-Graphs-for-Orphan-Entity-Allocation-in-Resume-Processing"><a href="#Leveraging-Knowledge-Graphs-for-Orphan-Entity-Allocation-in-Resume-Processing" class="headerlink" title="Leveraging Knowledge Graphs for Orphan Entity Allocation in Resume Processing"></a>Leveraging Knowledge Graphs for Orphan Entity Allocation in Resume Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14093">http://arxiv.org/abs/2310.14093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aagam Bakliwal, Shubham Manish Gandhi, Yashodhara Haribhakta</li>
<li>for:  automatize and enhance the efficiency of the job screening process</li>
<li>methods:  association mining, concept extraction, external knowledge linking, named entity recognition, and knowledge graph construction</li>
<li>results:  successful bucketing of orphan entities within resumes, more effective candidate-job matching, and improved resume screening process accuracy.<details>
<summary>Abstract</summary>
Significant challenges are posed in talent acquisition and recruitment by processing and analyzing unstructured data, particularly resumes. This research presents a novel approach for orphan entity allocation in resume processing using knowledge graphs. Techniques of association mining, concept extraction, external knowledge linking, named entity recognition, and knowledge graph construction are integrated into our pipeline. By leveraging these techniques, the aim is to automate and enhance the efficiency of the job screening process by successfully bucketing orphan entities within resumes. This allows for more effective matching between candidates and job positions, streamlining the resume screening process, and enhancing the accuracy of candidate-job matching. The approach's exceptional effectiveness and resilience are highlighted through extensive experimentation and evaluation, ensuring that alternative measures can be relied upon for seamless processing and orphan entity allocation in case of any component failure. The capabilities of knowledge graphs in generating valuable insights through intelligent information extraction and representation, specifically in the domain of categorizing orphan entities, are highlighted by the results of our research.
</details>
<details>
<summary>摘要</summary>
significannot challenges are posed in talent acquisition and recruitment by processing and analyzing unstructured data, particularly resumes. This research presents a novel approach for orphan entity allocation in resume processing using knowledge graphs. Techniques of association mining, concept extraction, external knowledge linking, named entity recognition, and knowledge graph construction are integrated into our pipeline. By leveraging these techniques, the aim is to automate and enhance the efficiency of the job screening process by successfully bucketing orphan entities within resumes. This allows for more effective matching between candidates and job positions, streamlining the resume screening process, and enhancing the accuracy of candidate-job matching. The approach's exceptional effectiveness and resilience are highlighted through extensive experimentation and evaluation, ensuring that alternative measures can be relied upon for seamless processing and orphan entity allocation in case of any component failure. The capabilities of knowledge graphs in generating valuable insights through intelligent information extraction and representation, specifically in the domain of categorizing orphan entities, are highlighted by the results of our research.Here's the text with some additional information about the Simplified Chinese translation:The translation is in Simplified Chinese, which is the standardized form of Chinese used in mainland China and Singapore. The text is written in a formal and academic style, using technical terms and concepts related to natural language processing, machine learning, and knowledge graphs.Some of the key concepts and techniques used in the text include:* orphan entity allocation (遗弃实体分配): the process of identifying and categorizing entities in unstructured data, such as resumes, that do not fit into predefined categories or structures.* knowledge graphs (知识图): a type of graph that represents entities and their relationships in a structured and interconnected way, allowing for efficient information extraction and analysis.* association mining (关联挖掘): a technique used to discover and extract relationships between entities in unstructured data, such as resumes.* concept extraction (概念提取): a technique used to identify and extract relevant concepts and entities from unstructured data, such as resumes.* named entity recognition (命名实体识别): a technique used to identify and extract specific types of entities, such as names of people, organizations, and locations, from unstructured data, such as resumes.Overall, the text presents a novel approach for automating and enhancing the efficiency of the job screening process using knowledge graphs and other techniques, with the goal of improving the accuracy of candidate-job matching and streamlining the resume screening process.
</details></li>
</ul>
<hr>
<h2 id="MedEval-A-Multi-Level-Multi-Task-and-Multi-Domain-Medical-Benchmark-for-Language-Model-Evaluation"><a href="#MedEval-A-Multi-Level-Multi-Task-and-Multi-Domain-Medical-Benchmark-for-Language-Model-Evaluation" class="headerlink" title="MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark for Language Model Evaluation"></a>MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark for Language Model Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14088">http://arxiv.org/abs/2310.14088</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zexue He, Yu Wang, An Yan, Yao Liu, Eric Y. Chang, Amilcare Gentili, Julian McAuley, Chun-Nan Hsu</li>
<li>for: 本研究旨在提供一个多元、多任务、多领域医疗 benchmark，以推动语言模型在医疗领域的开发。</li>
<li>methods: 本研究使用了多种健康系统的数据，包括8种检查方式，共有22,779句 sentence和21,228份报告。研究人员在多个水平提供了专家标注，实现了精确的数据分类和多元的应用潜力。</li>
<li>results: 研究人员通过评估10种通用和领域特定的语言模型，包括健康领域基于领域的基eline和一般化的大语言模型（如ChatGPT），获得了不同任务之间的语言模型效果的评估结果。研究结果显示，大语言模型在不同任务之间的效果不同，并且发现了对 instrucion 的适应是一个重要的因素。<details>
<summary>Abstract</summary>
Curated datasets for healthcare are often limited due to the need of human annotations from experts. In this paper, we present MedEval, a multi-level, multi-task, and multi-domain medical benchmark to facilitate the development of language models for healthcare. MedEval is comprehensive and consists of data from several healthcare systems and spans 35 human body regions from 8 examination modalities. With 22,779 collected sentences and 21,228 reports, we provide expert annotations at multiple levels, offering a granular potential usage of the data and supporting a wide range of tasks. Moreover, we systematically evaluated 10 generic and domain-specific language models under zero-shot and finetuning settings, from domain-adapted baselines in healthcare to general-purposed state-of-the-art large language models (e.g., ChatGPT). Our evaluations reveal varying effectiveness of the two categories of language models across different tasks, from which we notice the importance of instruction tuning for few-shot usage of large language models. Our investigation paves the way toward benchmarking language models for healthcare and provides valuable insights into the strengths and limitations of adopting large language models in medical domains, informing their practical applications and future advancements.
</details>
<details>
<summary>摘要</summary>
医疗领域的数据集经常受限于专家的人工标注。在这篇论文中，我们介绍了医生eval，一个多级、多任务、多领域的医疗语言模型开发 benchmark。医生eval 全面，涵盖多个医疗系统，覆盖人体8种检查方式，共收集了22,779句话和21,228份报告。我们提供了多个水平的专家标注，为数据的细化使用提供了可能的潜在应用，支持广泛的任务。此外，我们系统地评估了10种通用和医疗领域特定的语言模型，包括医疗领域基线模型和普通大语言模型（如ChatGPT）。我们的评估发现，这两类语言模型在不同任务中的效果不同，而且在几个任务中，大语言模型的几个 shot 使用需要进行调教。我们的调查开创了医疗领域语言模型的 benchmarking 的可能性，并为将来的应用和进步提供了有价值的见解，了解大语言模型在医疗领域的优劣和局限性。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Accuracy-Evaluating-Self-Consistency-of-Code-Large-Language-Models-with-IdentityChain"><a href="#Beyond-Accuracy-Evaluating-Self-Consistency-of-Code-Large-Language-Models-with-IdentityChain" class="headerlink" title="Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models with IdentityChain"></a>Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models with IdentityChain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14053">http://arxiv.org/abs/2310.14053</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/marcusm117/IdentityChain">https://github.com/marcusm117/IdentityChain</a></li>
<li>paper_authors: Marcus J. Min, Yangruibo Ding, Luca Buratti, Saurabh Pujar, Gail Kaiser, Suman Jana, Baishakhi Ray</li>
<li>for: 评估大型自然语言处理模型（Code LLMs）的可靠性。</li>
<li>methods: 提出了一种名为IdentityChain的框架，可以同时评估模型的自我一致性和总体准确率。</li>
<li>results: 对11个Code LLMs进行了评估，发现它们无法保持自我一致性，并且通过使用IdentityChain可以曝光当前模型的三大缺陷。<details>
<summary>Abstract</summary>
Code Large Language Models (Code LLMs) are being increasingly employed in real-life applications, so evaluating them is critical. While the general accuracy of Code LLMs on individual tasks has been extensively evaluated, their self-consistency across different tasks is overlooked. Intuitively, a trustworthy model should be self-consistent when generating natural language specifications for its own code and generating code for its own specifications. Failure to preserve self-consistency reveals a lack of understanding of the shared semantics underlying natural language and programming language, and therefore undermines the trustworthiness of a model. In this paper, we first formally define the self-consistency of Code LLMs and then design a framework, IdentityChain, which effectively and efficiently evaluates the self-consistency and general accuracy of a model at the same time. We study eleven Code LLMs and show that they fail to preserve self-consistency, which is indeed a distinct aspect from general accuracy. Furthermore, we show that IdentityChain can be used as a model debugging tool to expose weaknesses of Code LLMs by demonstrating three major weaknesses that we identify in current models using IdentityChain. Our code is available at https://github.com/marcusm117/IdentityChain.
</details>
<details>
<summary>摘要</summary>
<<SYS coding:utf-8>>大型语言模型（大型语言模型，Code LLMs）在实际应用中越来越广泛使用，因此评估其重要。而大型语言模型在不同任务之间的一致性却受到了忽略。人们可以 intuition 来认为，一个可靠的模型应该在生成自然语言规范和代码之间保持一致。如果不能保持一致性，则表明模型对自然语言和编程语言共同下的 semantics 没有很好的理解，因此模型的可靠性将受到损害。在这篇论文中，我们首先正式定义了 Code LLMs 的自身一致性，然后我们设计了一个框架，叫做 IdentityChain，可以同时评估模型的自身一致性和总准确率。我们对 eleven 种 Code LLMs 进行了研究，发现它们在保持自身一致性方面存在问题，这实际上是一种与总准确率不同的特征。此外，我们还证明了 IdentityChain 可以作为模型调试工具，用于曝光当前模型的三大弱点。我们的代码可以在 https://github.com/marcusm117/IdentityChain 上找到。
</details></li>
</ul>
<hr>
<h2 id="Code-Switching-with-Word-Senses-for-Pretraining-in-Neural-Machine-Translation"><a href="#Code-Switching-with-Word-Senses-for-Pretraining-in-Neural-Machine-Translation" class="headerlink" title="Code-Switching with Word Senses for Pretraining in Neural Machine Translation"></a>Code-Switching with Word Senses for Pretraining in Neural Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14050">http://arxiv.org/abs/2310.14050</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vivek Iyer, Edoardo Barba, Alexandra Birch, Jeff Z. Pan, Roberto Navigli</li>
<li>for: 本研究旨在提高Neural Machine Translation（NMT）模型的精度和可靠性，通过在Pre-training阶段使用知识库中的单词意思信息来改善模型的多语言能力。</li>
<li>methods: 本研究提出了Word Sense Pretraining for Neural Machine Translation（WSP-NMT）方法，该方法利用知识库中的单词意思信息进行预训练，以提高模型在多语言翻译任务中的表现。</li>
<li>results: 实验结果表明，WSP-NMT方法可以显著提高翻译质量，并在不同的数据和资源匮乏情况下保持良好的表现。此外，研究还发现了在DiBiMT排除词检测测试上的细致精度提升。<details>
<summary>Abstract</summary>
Lexical ambiguity is a significant and pervasive challenge in Neural Machine Translation (NMT), with many state-of-the-art (SOTA) NMT systems struggling to handle polysemous words (Campolungo et al., 2022). The same holds for the NMT pretraining paradigm of denoising synthetic "code-switched" text (Pan et al., 2021; Iyer et al., 2023), where word senses are ignored in the noising stage -- leading to harmful sense biases in the pretraining data that are subsequently inherited by the resulting models. In this work, we introduce Word Sense Pretraining for Neural Machine Translation (WSP-NMT) - an end-to-end approach for pretraining multilingual NMT models leveraging word sense-specific information from Knowledge Bases. Our experiments show significant improvements in overall translation quality. Then, we show the robustness of our approach to scale to various challenging data and resource-scarce scenarios and, finally, report fine-grained accuracy improvements on the DiBiMT disambiguation benchmark. Our studies yield interesting and novel insights into the merits and challenges of integrating word sense information and structured knowledge in multilingual pretraining for NMT.
</details>
<details>
<summary>摘要</summary>
Lexical ambiguity 是 neural machine translation (NMT) 中的一个重要和普遍存在的挑战 (Campolungo et al., 2022)。多种现代 NMT 系统在处理多义词 (polysemous words) 方面几乎都有困难。同样的情况也出现在 NMT 预训练 paradigm 中，例如 denoising synthetic "code-switched" text (Pan et al., 2021; Iyer et al., 2023)，在杂化阶段中忽略单词的意思 -- 导致预训练数据中的词义偏见，这些偏见后来被传递给结果模型。在这种情况下，我们引入了 Word Sense Pretraining for Neural Machine Translation (WSP-NMT) - 一种结束到结束的方法，使用知识库中的单词意思特定信息来预训练多语言 NMT 模型。我们的实验表明，我们的方法可以提高总翻译质量。然后，我们证明了我们的方法可以扩展到各种复杂的数据和资源匮乏场景，并最后报告了 DiBiMT 分词标准 bencmark 上的细化准确性改进。我们的研究提供了关于将单词意思信息和结构化知识integrated into multilingual pretraining for NMT的新颖和有趣的发现。
</details></li>
</ul>
<hr>
<h2 id="MeaeQ-Mount-Model-Extraction-Attacks-with-Efficient-Queries"><a href="#MeaeQ-Mount-Model-Extraction-Attacks-with-Efficient-Queries" class="headerlink" title="MeaeQ: Mount Model Extraction Attacks with Efficient Queries"></a>MeaeQ: Mount Model Extraction Attacks with Efficient Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14047">http://arxiv.org/abs/2310.14047</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/c-w-d/meaeq">https://github.com/c-w-d/meaeq</a></li>
<li>paper_authors: Chengwei Dai, Minxuan Lv, Kun Li, Wei Zhou</li>
<li>For: The paper is written to address model extraction attacks in natural language processing (NLP) and to propose a method for stealing victim models with low query costs.* Methods: The paper uses a zero-shot sequence inference classifier combined with API service information to filter task-relevant data from a public text corpus, and a clustering-based data reduction technique to obtain representative data as queries for the attack.* Results: The paper achieves higher functional similarity to the victim model than baselines while requiring fewer queries, as demonstrated through extensive experiments conducted on four benchmark datasets.Here’s the same information in Simplified Chinese text:* For: 本文是为了 Addressing Model Extraction Attacks in Natural Language Processing (NLP) 和提出一种用于夺取受害模型的低查询成本的方法。* Methods: 本文使用 Zero-shot Sequence Inference Classifier 与 API 服务信息结合，从公共文本 corpus 中筛选任务相关的数据，并使用 clustering-based data reduction technique 获取表示性的数据作为攻击的查询。* Results: 本文在四个 benchmark 数据集上实现了高度的函数相似性，而且需要更少的查询，较基eline 高。<details>
<summary>Abstract</summary>
We study model extraction attacks in natural language processing (NLP) where attackers aim to steal victim models by repeatedly querying the open Application Programming Interfaces (APIs). Recent works focus on limited-query budget settings and adopt random sampling or active learning-based sampling strategies on publicly available, unannotated data sources. However, these methods often result in selected queries that lack task relevance and data diversity, leading to limited success in achieving satisfactory results with low query costs. In this paper, we propose MeaeQ (Model extraction attack with efficient Queries), a straightforward yet effective method to address these issues. Specifically, we initially utilize a zero-shot sequence inference classifier, combined with API service information, to filter task-relevant data from a public text corpus instead of a problem domain-specific dataset. Furthermore, we employ a clustering-based data reduction technique to obtain representative data as queries for the attack. Extensive experiments conducted on four benchmark datasets demonstrate that MeaeQ achieves higher functional similarity to the victim model than baselines while requiring fewer queries. Our code is available at https://github.com/C-W-D/MeaeQ.
</details>
<details>
<summary>摘要</summary>
我们研究模型EXTRACT attacks在自然语言处理（NLP）中，攻击者希望通过 repeatedly 访问公开的应用程序编程接口（API）来窃取受害者模型。最近的工作主要集中在有限Query 预算设置下，采用随机抽样或活动学习基于抽样策略在公共可用数据源上。然而，这些方法经常导致选择的查询缺乏任务相关性和数据多样性，从而导致寻求满意的结果需要较高的查询成本。在这篇论文中，我们提出 MeaeQ（模型EXTRACT攻击with高效查询），一种简单又有效的方法来解决这些问题。我们首先利用零拟合序列推理分类器，结合API服务信息，从公共文本资源中筛选任务相关的数据而不是具体领域特定的数据集。其次，我们使用聚类分析技术来减少数据，从而获得代表性的查询。我们对四个标准测试集进行了广泛的实验，结果显示，MeaeQ可以在 fewer 查询下达到更高的功能相似性，而与基eline 相比。我们的代码可以在 <https://github.com/C-W-D/MeaeQ> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Tree-Prompting-Efficient-Task-Adaptation-without-Fine-Tuning"><a href="#Tree-Prompting-Efficient-Task-Adaptation-without-Fine-Tuning" class="headerlink" title="Tree Prompting: Efficient Task Adaptation without Fine-Tuning"></a>Tree Prompting: Efficient Task Adaptation without Fine-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14034">http://arxiv.org/abs/2310.14034</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/csinva/treeprompt">https://github.com/csinva/treeprompt</a></li>
<li>paper_authors: John X. Morris, Chandan Singh, Alexander M. Rush, Jianfeng Gao, Yuntian Deng</li>
<li>for: 本文旨在提高小型语言模型（LM）的应用性，通过构建决策树来提高提问的精度。</li>
<li>methods: 本文提出了树提 prompting 方法，即在执行时，通过有效地路由上一步决策树的结果来确定下一步LM的调用。</li>
<li>results: 实验表明，树提 prompting 方法可以提高分类 dataset 的准确率，与Finetuning 相当，并且可以在各种任务上进行审查模型的决策过程。<details>
<summary>Abstract</summary>
Prompting language models (LMs) is the main interface for applying them to new tasks. However, for smaller LMs, prompting provides low accuracy compared to gradient-based finetuning. Tree Prompting is an approach to prompting which builds a decision tree of prompts, linking multiple LM calls together to solve a task. At inference time, each call to the LM is determined by efficiently routing the outcome of the previous call using the tree. Experiments on classification datasets show that Tree Prompting improves accuracy over competing methods and is competitive with fine-tuning. We also show that variants of Tree Prompting allow inspection of a model's decision-making process.
</details>
<details>
<summary>摘要</summary>
LMs的提示是主要用于应用它们到新任务中。然而，对于较小的LMs，提示提供的准确率相对较低，而梯度基于的finetuning则提供了更高的准确率。Tree Prompting是一种提示方法，它建立了一棵决策树，将多个LM调用串起来解决任务。在推理时，每次LM调用的结果都会根据树的结构有效地Routing到下一次调用。我们的实验表明，Tree Prompting可以提高分类 datasets 的准确率，并与finetuning竞争。此外，我们还示出了Tree Prompting的变体可以对模型决策过程进行检查。
</details></li>
</ul>
<hr>
<h2 id="Analysing-State-Backed-Propaganda-Websites-a-New-Dataset-and-Linguistic-Study"><a href="#Analysing-State-Backed-Propaganda-Websites-a-New-Dataset-and-Linguistic-Study" class="headerlink" title="Analysing State-Backed Propaganda Websites: a New Dataset and Linguistic Study"></a>Analysing State-Backed Propaganda Websites: a New Dataset and Linguistic Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14032">http://arxiv.org/abs/2310.14032</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gatenlp/wordpress-site-extractor">https://github.com/gatenlp/wordpress-site-extractor</a></li>
<li>paper_authors: Freddy Heppell, Kalina Bontcheva, Carolina Scarton</li>
<li>for: 这篇论文旨在分析两个以前没有研究过的国家支持的假新闻网站，即Reliable Recent News (rrn.world)和WarOnFakes (waronfakes.com)，这两个网站分别发布了多种语言的内容，包括阿拉伯语、中文、英语、法语、德语和西班牙语。</li>
<li>methods: 我们使用了内容获取方法和跨站无监督主题聚合方法来处理这些多语言数据集，并对网页翻译和时间分析进行语言和时间分析。</li>
<li>results: 我们的研究发现，这两个网站的内容具有较高的假新闻分布率，并且有一些文章的发布日期不准确。我们还发现了这些网站之间的语言和主题相似性，以及它们在时间上的变化。我们还公开发布了一个包含14,053篇文章的新数据集，每篇文章都有相应的语言版本和附加的元数据，如链接和图片。本研究对NLP社区的主要贡献在于提供了假新闻网站的新数据集，以及训练NLP工具 для假新闻检测。<details>
<summary>Abstract</summary>
This paper analyses two hitherto unstudied sites sharing state-backed disinformation, Reliable Recent News (rrn.world) and WarOnFakes (waronfakes.com), which publish content in Arabic, Chinese, English, French, German, and Spanish. We describe our content acquisition methodology and perform cross-site unsupervised topic clustering on the resulting multilingual dataset. We also perform linguistic and temporal analysis of the web page translations and topics over time, and investigate articles with false publication dates. We make publicly available this new dataset of 14,053 articles, annotated with each language version, and additional metadata such as links and images. The main contribution of this paper for the NLP community is in the novel dataset which enables studies of disinformation networks, and the training of NLP tools for disinformation detection.
</details>
<details>
<summary>摘要</summary>
这篇论文分析了两个未经研究的媒体站点，即可靠最新新闻（rrn.world）和战对假新闻（waronfakes.com），这两个站点都发布了多语言内容（阿拉伯语、中文、英语、法语、德语和西班牙语）。我们描述了我们的内容获取方法和跨站点无监督主题划分方法，并对多语言数据集进行语言和时间分析，以及文章发布日期的错误分析。我们公开发布了14,053篇文章，每篇文章都有相应的语言版本和附加元数据，如链接和图像。本文的主要贡献是提供了一个新的识别假新闻网络的数据集，以及训练NLP工具的机会。
</details></li>
</ul>
<hr>
<h2 id="LLM-Prop-Predicting-Physical-And-Electronic-Properties-Of-Crystalline-Solids-From-Their-Text-Descriptions"><a href="#LLM-Prop-Predicting-Physical-And-Electronic-Properties-Of-Crystalline-Solids-From-Their-Text-Descriptions" class="headerlink" title="LLM-Prop: Predicting Physical And Electronic Properties Of Crystalline Solids From Their Text Descriptions"></a>LLM-Prop: Predicting Physical And Electronic Properties Of Crystalline Solids From Their Text Descriptions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14029">http://arxiv.org/abs/2310.14029</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vertaix/llm-prop">https://github.com/vertaix/llm-prop</a></li>
<li>paper_authors: Andre Niyongabo Rubungo, Craig Arnold, Barry P. Rand, Adji Bousso Dieng</li>
<li>for: 这个论文旨在提出一种基于大语言模型（LLM）的方法，用于从晶体文本描述中预测晶体性质。</li>
<li>methods: 该方法使用大语言模型（LLM）来利用晶体文本描述来预测晶体的物理和电子性质。</li>
<li>results: 对比现有的graph neural network（GNN）方法，LLM-Prop方法在预测晶体带隙、是否直接带隙和晶体单元体积等性质上表现出较高的准确率。<details>
<summary>Abstract</summary>
The prediction of crystal properties plays a crucial role in the crystal design process. Current methods for predicting crystal properties focus on modeling crystal structures using graph neural networks (GNNs). Although GNNs are powerful, accurately modeling the complex interactions between atoms and molecules within a crystal remains a challenge. Surprisingly, predicting crystal properties from crystal text descriptions is understudied, despite the rich information and expressiveness that text data offer. One of the main reasons is the lack of publicly available data for this task. In this paper, we develop and make public a benchmark dataset (called TextEdge) that contains text descriptions of crystal structures with their properties. We then propose LLM-Prop, a method that leverages the general-purpose learning capabilities of large language models (LLMs) to predict the physical and electronic properties of crystals from their text descriptions. LLM-Prop outperforms the current state-of-the-art GNN-based crystal property predictor by about 4% in predicting band gap, 3% in classifying whether the band gap is direct or indirect, and 66% in predicting unit cell volume. LLM-Prop also outperforms a finetuned MatBERT, a domain-specific pre-trained BERT model, despite having 3 times fewer parameters. Our empirical results may highlight the current inability of GNNs to capture information pertaining to space group symmetry and Wyckoff sites for accurate crystal property prediction.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "The prediction of crystal properties plays a crucial role in the crystal design process. Current methods for predicting crystal properties focus on modeling crystal structures using graph neural networks (GNNs). Although GNNs are powerful, accurately modeling the complex interactions between atoms and molecules within a crystal remains a challenge. Surprisingly, predicting crystal properties from crystal text descriptions is understudied, despite the rich information and expressiveness that text data offer. One of the main reasons is the lack of publicly available data for this task. In this paper, we develop and make public a benchmark dataset (called TextEdge) that contains text descriptions of crystal structures with their properties. We then propose LLM-Prop, a method that leverages the general-purpose learning capabilities of large language models (LLMs) to predict the physical and electronic properties of crystals from their text descriptions. LLM-Prop outperforms the current state-of-the-art GNN-based crystal property predictor by about 4% in predicting band gap, 3% in classifying whether the band gap is direct or indirect, and 66% in predicting unit cell volume. LLM-Prop also outperforms a finetuned MatBERT, a domain-specific pre-trained BERT model, despite having 3 times fewer parameters. Our empirical results may highlight the current inability of GNNs to capture information pertaining to space group symmetry and Wyckoff sites for accurate crystal property prediction."中文翻译：<<SYS>>预测 кристаллических 属性在 кристалли设计过程中扮演关键角色。现有方法用图 neural networks（GNNs）模拟 кристалли结构，尽管 GNNs 强大，但是很难准确地模拟 кристалли中原子和分子之间复杂的交互。尽管预测 кристалли属性从 кристалли文本描述是未explored的，尽管文本数据具有丰富的信息和表达能力。一个主要的原因是该任务上没有公共可用的数据。在这篇论文中，我们开发了一个名为 TextEdge 的 referential dataset，该 dataset包含 кристалли结构的文本描述和其属性。我们然后提出了 LLM-Prop，一种利用大型自然语言模型（LLMs）预测 кристалли物理和电子属性的方法。LLM-Prop 在预测带隙、直接或间接带隙和单元积体积方面比现有状态 искусственный neural networks （GNNs） Based crystal property predictor 高于4%，高于3%在分类直接或间接带隙，和66%在预测单元积体积。LLM-Prop 还高于一个finetuned MatBERT，一种预先训练的 BERT 模型，尽管它有3倍少的参数。我们的实验结果可能高亮当前 GNNs 无法捕捉关于空群 симметрии和 Wyckoff 位置的信息，以便准确预测 кристалли属性。
</details></li>
</ul>
<hr>
<h2 id="GASCOM-Graph-based-Attentive-Semantic-Context-Modeling-for-Online-Conversation-Understanding"><a href="#GASCOM-Graph-based-Attentive-Semantic-Context-Modeling-for-Online-Conversation-Understanding" class="headerlink" title="GASCOM: Graph-based Attentive Semantic Context Modeling for Online Conversation Understanding"></a>GASCOM: Graph-based Attentive Semantic Context Modeling for Online Conversation Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14028">http://arxiv.org/abs/2310.14028</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vibhor Agarwal, Yu Chen, Nishanth Sastry</li>
<li>for: 这篇论文是为了提高在线对话理解的性能而写的。</li>
<li>methods: 这篇论文提出了一种基于图 estructure的注意力机制，使得可以更好地理解在线对话的含义。</li>
<li>results: 论文使用了两种新的算法，可以从整个对话树中提取有用的信息，并且使用多头Graph Attention Mechanism来进一步细化对话上的含义模型化。论文的实验结果表明，与现状比较，这种方法可以提高对话理解的性能，提高了对 polarity prediction 和 hate speech detection 的性能。<details>
<summary>Abstract</summary>
Online conversation understanding is an important yet challenging NLP problem which has many useful applications (e.g., hate speech detection). However, online conversations typically unfold over a series of posts and replies to those posts, forming a tree structure within which individual posts may refer to semantic context from higher up the tree. Such semantic cross-referencing makes it difficult to understand a single post by itself; yet considering the entire conversation tree is not only difficult to scale but can also be misleading as a single conversation may have several distinct threads or points, not all of which are relevant to the post being considered. In this paper, we propose a Graph-based Attentive Semantic COntext Modeling (GASCOM) framework for online conversation understanding. Specifically, we design two novel algorithms that utilise both the graph structure of the online conversation as well as the semantic information from individual posts for retrieving relevant context nodes from the whole conversation. We further design a token-level multi-head graph attention mechanism to pay different attentions to different tokens from different selected context utterances for fine-grained conversation context modeling. Using this semantic conversational context, we re-examine two well-studied problems: polarity prediction and hate speech detection. Our proposed framework significantly outperforms state-of-the-art methods on both tasks, improving macro-F1 scores by 4.5% for polarity prediction and by 5% for hate speech detection. The GASCOM context weights also enhance interpretability.
</details>
<details>
<summary>摘要</summary>
在线对话理解是一项重要又挑战性的自然语言处理（NLP）问题，它在许多应用中具有用于 hate speech detection 等应用。然而，在线对话通常是一串带有回快的帖子和回快中的帖子，组成一个树状结构，各个帖子可能引用上下文中的semantic context。这种 semantic cross-referencing 使得单个帖子难以理解，同时考虑整个对话树也不仅困难scaling，还可能导致偏导的解释，因为一个对话可能有多个不同的线索或焦点，其中不 todos 是 relevante para el post being considered。在这篇论文中，我们提出一个 Graph-based Attentive Semantic COntext Modeling（GASCOM）框架，用于在线对话理解。具体来说，我们设计了两种新的算法，利用在线对话的graph结构以及各个帖子的semantic信息，来选择 relevante context nodes from the whole conversation。此外，我们还设计了一个token-level multi-head graph attention mechanism，用于在不同的selected context utterances中进行细致的对话上下文模型化。使用这种 semantic conversational context，我们重新评估了两个已有的问题：polarity prediction和 hate speech detection。我们的提议的框架在两个任务上显著超越了当前的状态方法，提高了macro-F1分数 by 4.5% for polarity prediction和by 5% for hate speech detection。GASCOM上下文权重也提高了解释性。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-and-Multimodal-Retrieval-for-Visual-Word-Sense-Disambiguation"><a href="#Large-Language-Models-and-Multimodal-Retrieval-for-Visual-Word-Sense-Disambiguation" class="headerlink" title="Large Language Models and Multimodal Retrieval for Visual Word Sense Disambiguation"></a>Large Language Models and Multimodal Retrieval for Visual Word Sense Disambiguation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14025">http://arxiv.org/abs/2310.14025</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anastasiakrith/multimodal-retrieval-for-vwsd">https://github.com/anastasiakrith/multimodal-retrieval-for-vwsd</a></li>
<li>paper_authors: Anastasia Kritharoula, Maria Lymperaiou, Giorgos Stamou</li>
<li>for: 本文主要针对的是解决文本含义歧义的图像检索任务，即Visual Word Sense Disambiguation (VWSD)。</li>
<li>methods: 本文采用了多种方法，包括最新的transformer-based方法和大自然语言模型（LLMs）来解决VWSD任务。</li>
<li>results:  experiments表明，我们的方法可以在VWSD任务中达到竞争性的排名结果，并且通过Chain-of-Thought（CoT）提示来帮助解释answer生成。<details>
<summary>Abstract</summary>
Visual Word Sense Disambiguation (VWSD) is a novel challenging task with the goal of retrieving an image among a set of candidates, which better represents the meaning of an ambiguous word within a given context. In this paper, we make a substantial step towards unveiling this interesting task by applying a varying set of approaches. Since VWSD is primarily a text-image retrieval task, we explore the latest transformer-based methods for multimodal retrieval. Additionally, we utilize Large Language Models (LLMs) as knowledge bases to enhance the given phrases and resolve ambiguity related to the target word. We also study VWSD as a unimodal problem by converting to text-to-text and image-to-image retrieval, as well as question-answering (QA), to fully explore the capabilities of relevant models. To tap into the implicit knowledge of LLMs, we experiment with Chain-of-Thought (CoT) prompting to guide explainable answer generation. On top of all, we train a learn to rank (LTR) model in order to combine our different modules, achieving competitive ranking results. Extensive experiments on VWSD demonstrate valuable insights to effectively drive future directions.
</details>
<details>
<summary>摘要</summary>
Visible Word Sense Disambiguation (VWSD) 是一个新型的挑战性任务，旨在从一组候选者中选取一幅图像，更好地表现出一个模糊词的意思在特定上下文中。在这篇文章中，我们做出了一个重要的进步，通过应用不同的方法来解决这个有趣的任务。由于 VWSD 主要是文本-图像搜寻任务，我们探索了最新的 transformer-based 方法来进行多modal搜寻。此外，我们使用 Large Language Models (LLMs) 来增强给定的短语，解决对目标词的模糊性。我们还研究了 VWSD 作为单modal问题，通过将它转换为文本-文本和图像-图像搜寻，以及问答 (QA)，以全面探索相关模型的能力。为了吸取 LLMS 的隐藏知识，我们尝试使用 Chain-of-Thought (CoT) 提示来引导可解释的答案生成。最后，我们将多个模组联合起来，使用 learn to rank (LTR) 模型进行排名， achieving 竞争性的排名结果。广泛的实验在 VWSD 中，提供了宝贵的见解，将未来发展领域带向更好的未来。
</details></li>
</ul>
<hr>
<h2 id="Toward-Stronger-Textual-Attack-Detectors"><a href="#Toward-Stronger-Textual-Attack-Detectors" class="headerlink" title="Toward Stronger Textual Attack Detectors"></a>Toward Stronger Textual Attack Detectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14001">http://arxiv.org/abs/2310.14001</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pierrecolombo/adversarialattacksnlp">https://github.com/pierrecolombo/adversarialattacksnlp</a></li>
<li>paper_authors: Pierre Colombo, Marine Picot, Nathan Noiry, Guillaume Staerman, Pablo Piantanida</li>
<li>for: 防止文本敌对攻击，即使是深度NLP系统。</li>
<li>methods: 引入了一个新的检测敌对攻击的框架——LAROUSSE，以及一个新的评价板准——STAKEOUT。</li>
<li>results: LAROUSSE比前一代方法更高效，并且可以防止梯度基本法。<details>
<summary>Abstract</summary>
The landscape of available textual adversarial attacks keeps growing, posing severe threats and raising concerns regarding the deep NLP system's integrity. However, the crucial problem of defending against malicious attacks has only drawn the attention of the NLP community. The latter is nonetheless instrumental in developing robust and trustworthy systems. This paper makes two important contributions in this line of search: (i) we introduce LAROUSSE, a new framework to detect textual adversarial attacks and (ii) we introduce STAKEOUT, a new benchmark composed of nine popular attack methods, three datasets, and two pre-trained models. LAROUSSE is ready-to-use in production as it is unsupervised, hyperparameter-free, and non-differentiable, protecting it against gradient-based methods. Our new benchmark STAKEOUT allows for a robust evaluation framework: we conduct extensive numerical experiments which demonstrate that LAROUSSE outperforms previous methods, and which allows to identify interesting factors of detection rate variations.
</details>
<details>
<summary>摘要</summary>
文章做出了两个重要贡献：首先，我们引入了一个新的检测文本针对攻击的框架，即LAROUSSE，它是不需要监督学习、无参数、不可导的，因此具有更高的安全性。其次，我们引入了一个新的评估框架，即STAKEOUT，它包括9种常见的攻击方法、3个数据集和2个预训练模型。我们的新框架允许更加稳定地评估检测效果，我们进行了广泛的数字实验，结果表明LAROUSSE在前一代方法之上出众，同时还可以识别检测率的变化因素。
</details></li>
</ul>
<hr>
<h2 id="Transductive-Learning-for-Textual-Few-Shot-Classification-in-API-based-Embedding-Models"><a href="#Transductive-Learning-for-Textual-Few-Shot-Classification-in-API-based-Embedding-Models" class="headerlink" title="Transductive Learning for Textual Few-Shot Classification in API-based Embedding Models"></a>Transductive Learning for Textual Few-Shot Classification in API-based Embedding Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13998">http://arxiv.org/abs/2310.13998</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Colombo, Victor Pellegrain, Malik Boudiaf, Victor Storchan, Myriam Tami, Ismail Ben Ayed, Celine Hudelot, Pablo Piantanida</li>
<li>for:  This paper focuses on the practical applications of natural language processing, specifically few-shot classification, and addresses the issue of proprietary and closed APIs.</li>
<li>methods: The paper proposes a transductive inference learning paradigm that utilizes unlabeled data, along with a new parameter-free transductive regularizer based on the Fisher-Rao loss.</li>
<li>results: The paper presents experimental results using eight backbone models and an episodic evaluation over 1,000 episodes, which demonstrate the superiority of transductive inference over the standard inductive setting.Here is the result in Simplified Chinese text:</li>
<li>for: 本文关注自然语言处理的实际应用，具体是几个shot类型的分类，并解决了 propriety 和关闭 API 的问题。</li>
<li>methods: 本文提出了一种推uctive推理学习模式，利用无标注数据，并提出了一种无参数的推uctive规范基于 Fisher-Rao 损失。</li>
<li>results: 本文通过使用 eight 个基础模型和一千个 episodic 评估，展示了推uctive推理在标准 inductive 设定下的超越。<details>
<summary>Abstract</summary>
Proprietary and closed APIs are becoming increasingly common to process natural language, and are impacting the practical applications of natural language processing, including few-shot classification. Few-shot classification involves training a model to perform a new classification task with a handful of labeled data. This paper presents three contributions. First, we introduce a scenario where the embedding of a pre-trained model is served through a gated API with compute-cost and data-privacy constraints. Second, we propose a transductive inference, a learning paradigm that has been overlooked by the NLP community. Transductive inference, unlike traditional inductive learning, leverages the statistics of unlabeled data. We also introduce a new parameter-free transductive regularizer based on the Fisher-Rao loss, which can be used on top of the gated API embeddings. This method fully utilizes unlabeled data, does not share any label with the third-party API provider and could serve as a baseline for future research. Third, we propose an improved experimental setting and compile a benchmark of eight datasets involving multiclass classification in four different languages, with up to 151 classes. We evaluate our methods using eight backbone models, along with an episodic evaluation over 1,000 episodes, which demonstrate the superiority of transductive inference over the standard inductive setting.
</details>
<details>
<summary>摘要</summary>
专有和关闭API在处理自然语言方面变得越来越普遍，这对自然语言处理的实际应用产生了影响，包括几个shot分类。本文提出了三个贡献。首先，我们介绍了一种情况，在这种情况下，一个预训练模型的 embedding 被通过一个限制 compute-cost 和数据隐私的 gat API 提供。第二，我们提议了一种被 NLP 社区忽视的学习模式——推uctive inference。推uctive inference 不同于传统的 inductive learning，它利用不标注数据的统计特性。我们还介绍了一个新的参数-free 推uctive regularizer，基于 Fisher-Rao 损失函数，可以在gat API 中使用。这种方法可以完全利用无标注数据，不需要与第三方 API 提供者共享标签，并且可以作为未来研究的基准。第三，我们提出了一个改进的实验设定，并编译了八个dataset，包括四种语言，最多 151 个分类。我们使用八种背部bone模型进行评估，并在1,000个 episodic 评估中，发现推uctive inference 在标准 inductive 设定下表现出优异性。
</details></li>
</ul>
<hr>
<h2 id="Emulating-the-Human-Mind-A-Neural-symbolic-Link-Prediction-Model-with-Fast-and-Slow-Reasoning-and-Filtered-Rules"><a href="#Emulating-the-Human-Mind-A-Neural-symbolic-Link-Prediction-Model-with-Fast-and-Slow-Reasoning-and-Filtered-Rules" class="headerlink" title="Emulating the Human Mind: A Neural-symbolic Link Prediction Model with Fast and Slow Reasoning and Filtered Rules"></a>Emulating the Human Mind: A Neural-symbolic Link Prediction Model with Fast and Slow Reasoning and Filtered Rules</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13996">http://arxiv.org/abs/2310.13996</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Hossein Khojasteh, Najmeh Torabian, Ali Farjami, Saeid Hosseini, Behrouz Minaei-Bidgoli</li>
<li>for: 这个研究的目的是解决知识граフ（KG）中的不完整性问题，并提出了一个新的神经几何模型named FaSt-FLiP，以提高链接预测的性能和可解性。</li>
<li>methods: 这个模型使用了“常识推理”和“快速思考”两种人类认知方面的特点，并结合了逻辑和神经网络模型，以提高链接预测的精度和可解性。</li>
<li>results: 研究结果显示，FaSt-FLiP模型在链接预测中的表现较高，并能够提供更可靠的解释。另外，模型还能够自动检测和删除逻辑模型生成的错误规则。<details>
<summary>Abstract</summary>
Link prediction is an important task in addressing the incompleteness problem of knowledge graphs (KG). Previous link prediction models suffer from issues related to either performance or explanatory capability. Furthermore, models that are capable of generating explanations, often struggle with erroneous paths or reasoning leading to the correct answer. To address these challenges, we introduce a novel Neural-Symbolic model named FaSt-FLiP (stands for Fast and Slow Thinking with Filtered rules for Link Prediction task), inspired by two distinct aspects of human cognition: "commonsense reasoning" and "thinking, fast and slow." Our objective is to combine a logical and neural model for enhanced link prediction. To tackle the challenge of dealing with incorrect paths or rules generated by the logical model, we propose a semi-supervised method to convert rules into sentences. These sentences are then subjected to assessment and removal of incorrect rules using an NLI (Natural Language Inference) model. Our approach to combining logical and neural models involves first obtaining answers from both the logical and neural models. These answers are subsequently unified using an Inference Engine module, which has been realized through both algorithmic implementation and a novel neural model architecture. To validate the efficacy of our model, we conducted a series of experiments. The results demonstrate the superior performance of our model in both link prediction metrics and the generation of more reliable explanations.
</details>
<details>
<summary>摘要</summary>
链接预测是知识 graphs（KG）的重要任务，以解决知识 Graphs 的不完整性问题。现有的链接预测模型受到性能和可解释能力的限制。而且，可以生成解释的模型经常会遇到错误的路径或理由，导致正确答案。为 Addressing these challenges, we propose a novel Neural-Symbolic model named FaSt-FLiP（快速思维与筛选规则 для链接预测任务）， drawing inspiration from two aspects of human cognition："通常的思维"和"快速和慢速的思考。our objective is to combine a logical and neural model for enhanced link prediction. To tackle the challenge of dealing with incorrect paths or rules generated by the logical model, we propose a semi-supervised method to convert rules into sentences. These sentences are then subjected to assessment and removal of incorrect rules using an NLI（自然语言推理）model. Our approach to combining logical and neural models involves first obtaining answers from both the logical and neural models. These answers are subsequently unified using an Inference Engine module, which has been realized through both algorithmic implementation and a novel neural model architecture. To validate the efficacy of our model, we conducted a series of experiments. The results demonstrate the superior performance of our model in both link prediction metrics and the generation of more reliable explanations.
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Information-Theoretic-Objective-to-Disentangle-Representations-for-Fair-Classification"><a href="#A-Novel-Information-Theoretic-Objective-to-Disentangle-Representations-for-Fair-Classification" class="headerlink" title="A Novel Information-Theoretic Objective to Disentangle Representations for Fair Classification"></a>A Novel Information-Theoretic Objective to Disentangle Representations for Fair Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13990">http://arxiv.org/abs/2310.13990</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Colombo, Nathan Noiry, Guillaume Staerman, Pablo Piantanida</li>
<li>for: The paper aims to learn abstract representations of reality from the observation of multiple contextual situations, specifically disentangled representations that are low-dimensional and independent of sensitive attributes such as gender or age.</li>
<li>methods: The paper proposes a novel family of regularizers called CLINIC, which minimizes the mutual information between the latent representation and the sensitive attribute conditional to the target. This approach is parameter-free and easier to train than previous techniques.</li>
<li>results: The paper demonstrates that the proposed CLINIC losses offer a better disentanglement&#x2F;accuracy trade-off than previous techniques and generalize better than training with cross-entropy loss, provided that the disentanglement task is not too constraining.Here is the simplified Chinese version of the three key information points:</li>
<li>for: 该论文目标是从多个 contextual situations 中学习抽象的 reality 表示，具体来说是EXTRACT 独立的表示，即低维度且独立的概念表示。</li>
<li>methods: 该论文提出一种新的 family of regularizers called CLINIC，该regularizers 的目标是将敏感特征（如性别或年龄）与 latent representation 的相关性降低到最低。这种方法是 parameter-free 的， easier 和 faster than previous techniques。</li>
<li>results: 该论文的实验结果显示，提出的 CLINIC losses 可以比 previous techniques 提供更好的 disentanglement&#x2F;accuracy 的负荷平衡，并且在不太紧张的disentanglement task 下可以更好地泛化。<details>
<summary>Abstract</summary>
One of the pursued objectives of deep learning is to provide tools that learn abstract representations of reality from the observation of multiple contextual situations. More precisely, one wishes to extract disentangled representations which are (i) low dimensional and (ii) whose components are independent and correspond to concepts capturing the essence of the objects under consideration (Locatello et al., 2019b). One step towards this ambitious project consists in learning disentangled representations with respect to a predefined (sensitive) attribute, e.g., the gender or age of the writer. Perhaps one of the main application for such disentangled representations is fair classification. Existing methods extract the last layer of a neural network trained with a loss that is composed of a cross-entropy objective and a disentanglement regularizer. In this work, we adopt an information-theoretic view of this problem which motivates a novel family of regularizers that minimizes the mutual information between the latent representation and the sensitive attribute conditional to the target. The resulting set of losses, called CLINIC, is parameter free and thus, it is easier and faster to train. CLINIC losses are studied through extensive numerical experiments by training over 2k neural networks. We demonstrate that our methods offer a better disentanglement/accuracy trade-off than previous techniques, and generalize better than training with cross-entropy loss solely provided that the disentanglement task is not too constraining.
</details>
<details>
<summary>摘要</summary>
一个深度学习的核心目标是提供能够从多个上下文中学习抽象的现实表示的工具。更具体地说，希望从多个上下文中提取独立的表示，其维度低、并且其组成部分独立、对象本身的核心特征capturing (Locatello et al., 2019b).一种实现这个奢侈目标的方法是通过对敏感特征（如作者的性别或年龄）进行分离表示学习。可能这种独立表示的主要应用是公平分类。现有方法通常是提取一个通过混合Entropy目标和分离regularizer进行训练的神经网络的最后一层。在这个工作中，我们采用信息论视角来解决这个问题，并提出了一个新的家族征识器，它将 conditional于目标进行抽象的mutual information minimization。这些loss Functions被称为CLINIC，它们是无参数的，因此更容易和更快地训练。我们通过对2k个神经网络进行数字实验来研究CLINIC loss。我们示示了我们的方法可以在训练过程中提供更好的抽象/准确性质和平衡，并且在训练cross-entropy loss solo不可能达到的情况下，能够更好地泛化。
</details></li>
</ul>
<hr>
<h2 id="GEMBA-MQM-Detecting-Translation-Quality-Error-Spans-with-GPT-4"><a href="#GEMBA-MQM-Detecting-Translation-Quality-Error-Spans-with-GPT-4" class="headerlink" title="GEMBA-MQM: Detecting Translation Quality Error Spans with GPT-4"></a>GEMBA-MQM: Detecting Translation Quality Error Spans with GPT-4</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13988">http://arxiv.org/abs/2310.13988</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tom Kocmi, Christian Federmann</li>
<li>for: 评估翻译质量错误</li>
<li>methods: 使用GPT模型，三次预览技术，无需人工参考翻译</li>
<li>results: 达到系统排名的最佳准确率，但需要注意GPT模型的商业性和黑盒特性，不建议用于学术论文中证明其他方法的改进。<details>
<summary>Abstract</summary>
This paper introduces GEMBA-MQM, a GPT-based evaluation metric designed to detect translation quality errors, specifically for the quality estimation setting without the need for human reference translations. Based on the power of large language models (LLM), GEMBA-MQM employs a fixed three-shot prompting technique, querying the GPT-4 model to mark error quality spans. Compared to previous works, our method has language-agnostic prompts, thus avoiding the need for manual prompt preparation for new languages.   While preliminary results indicate that GEMBA-MQM achieves state-of-the-art accuracy for system ranking, we advise caution when using it in academic works to demonstrate improvements over other methods due to its dependence on the proprietary, black-box GPT model.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了GEMBA-MQM，一种基于GPT的评估指标，用于探测翻译质量错误。这种指标不需要人工参考翻译，可以在质量估计设置中使用。通过大语言模型（LLM）的力量，GEMBA-MQM使用固定的三批提问技术，让GPT-4模型标识错误质量段落。与前一代方法相比，我们的方法有语言共享的提问，因此不需要为新语言手动准备提问。然而，我们建议在学术论文中使用GEMBA-MQM时需要谨慎，因为它取决于商业化、黑盒子GPT模型，这可能会导致使用它来证明其他方法的优越性的问题。
</details></li>
</ul>
<hr>
<h2 id="HateRephrase-Zero-and-Few-Shot-Reduction-of-Hate-Intensity-in-Online-Posts-using-Large-Language-Models"><a href="#HateRephrase-Zero-and-Few-Shot-Reduction-of-Hate-Intensity-in-Online-Posts-using-Large-Language-Models" class="headerlink" title="HateRephrase: Zero- and Few-Shot Reduction of Hate Intensity in Online Posts using Large Language Models"></a>HateRephrase: Zero- and Few-Shot Reduction of Hate Intensity in Online Posts using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13985">http://arxiv.org/abs/2310.13985</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vibhor Agarwal, Yu Chen, Nishanth Sastry</li>
<li>for: 这篇论文旨在提出一种新的、简单有效的方法，即在发布前对潜在仇恨言论内容进行重写。</li>
<li>methods: 这篇论文使用了大语言模型（LLMs），并比较了不同的提示方法，包括任务描述、仇Definition、少量示例和思维链。</li>
<li>results: 研究发现，使用几shot示例提示的LLMs最perform最好，并且在不同的提示方法下都有良好的表现。而且，对于不同的提示方法，GPT-3.5表现最佳。此外，人工评估表明，GPT-3.5生成的重写内容甚至超过了人类生成的ground truth重写内容。<details>
<summary>Abstract</summary>
Hate speech has become pervasive in today's digital age. Although there has been considerable research to detect hate speech or generate counter speech to combat hateful views, these approaches still cannot completely eliminate the potential harmful societal consequences of hate speech -- hate speech, even when detected, can often not be taken down or is often not taken down enough; and hate speech unfortunately spreads quickly, often much faster than any generated counter speech.   This paper investigates a relatively new yet simple and effective approach of suggesting a rephrasing of potential hate speech content even before the post is made. We show that Large Language Models (LLMs) perform well on this task, outperforming state-of-the-art baselines such as BART-Detox. We develop 4 different prompts based on task description, hate definition, few-shot demonstrations and chain-of-thoughts for comprehensive experiments and conduct experiments on open-source LLMs such as LLaMA-1, LLaMA-2 chat, Vicuna as well as OpenAI's GPT-3.5. We propose various evaluation metrics to measure the efficacy of the generated text and ensure the generated text has reduced hate intensity without drastically changing the semantic meaning of the original text.   We find that LLMs with a few-shot demonstrations prompt work the best in generating acceptable hate-rephrased text with semantic meaning similar to the original text. Overall, we find that GPT-3.5 outperforms the baseline and open-source models for all the different kinds of prompts. We also perform human evaluations and interestingly, find that the rephrasings generated by GPT-3.5 outperform even the human-generated ground-truth rephrasings in the dataset. We also conduct detailed ablation studies to investigate why LLMs work satisfactorily on this task and conduct a failure analysis to understand the gaps.
</details>
<details>
<summary>摘要</summary>
仇恨言语在当今数字时代已经成为普遍存在的问题。尽管有很多研究检测仇恨言语或生成对抗仇恨观点的counter speech，但这些方法仍然无法完全消除仇恨言语的社会后果 -- 仇恨言语，即使检测到了，通常不能或很难被移除，而且仇恨言语往往很快就会扩散，常常比生成的counter speech更快。这篇论文研究了一种新的、简单而有效的方法，即在投稿之前，使用大语言模型（LLMs）来提议修改潜在的仇恨言语内容。我们证明了LLMs在这个任务上表现良好，超过了现有的基线模型如BART-Detox。我们开发了4个不同的提示，基于任务描述、仇Definition、几个示例和串联思维，进行了广泛的实验。我们使用了开源的LLaMA-1、LLaMA-2 chat、Vicuna以及OpenAI的GPT-3.5等模型进行实验。我们提出了多种评价指标，以确保生成的文本减少了仇恨程度，而不会毁灭语意。我们发现，使用几个示例提示的LLMs最为有效，能够生成接受的仇恨重新写文本， semantic meaning与原文相似。总之，我们发现GPT-3.5在所有不同的提示上都超过了基线和开源模型。我们还进行了人工评价， Interestingly，我们发现GPT-3.5生成的重新写文本甚至超过了人工生成的基准重新写文本。我们还进行了细化的抽象研究和失败分析，以解释LLMs在这个任务上的成功原因。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Pronunciation-Assessment-–-A-Review"><a href="#Automatic-Pronunciation-Assessment-–-A-Review" class="headerlink" title="Automatic Pronunciation Assessment – A Review"></a>Automatic Pronunciation Assessment – A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13974">http://arxiv.org/abs/2310.13974</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yassine El Kheir, Ahmed Ali, Shammur Absar Chowdhury</li>
<li>for: 这篇论文主要是为了探讨计算机辅助发音训练（CAPT）中的发音评估方法和其应用。</li>
<li>methods: 这篇论文评论了在发音评估方面使用的方法，包括phonemic和prosodic两种方法。</li>
<li>results: 论文认为，现有的研究存在一些挑战和局限性，并提出了未来研究的可能性。<details>
<summary>Abstract</summary>
Pronunciation assessment and its application in computer-aided pronunciation training (CAPT) have seen impressive progress in recent years. With the rapid growth in language processing and deep learning over the past few years, there is a need for an updated review. In this paper, we review methods employed in pronunciation assessment for both phonemic and prosodic. We categorize the main challenges observed in prominent research trends, and highlight existing limitations, and available resources. This is followed by a discussion of the remaining challenges and possible directions for future work.
</details>
<details>
<summary>摘要</summary>
声音评估和计算机辅助声音训练（CAPT）在最近几年中得到了很大的进步。随着语言处理和深度学习技术的快速发展，需要进行更新的评估。本文介绍了声音评估中使用的方法，包括音节和语言流行的评估。我们分类了主要的研究趋势中的挑战，并高亮现有的限制和可用资源。然后是对未来工作的残余挑战和可能的方向的讨论。Note: Simplified Chinese is the standard writing system used in mainland China, while Traditional Chinese is used in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="AITA-Generating-Moral-Judgements-of-the-Crowd-with-Reasoning"><a href="#AITA-Generating-Moral-Judgements-of-the-Crowd-with-Reasoning" class="headerlink" title="AITA Generating Moral Judgements of the Crowd with Reasoning"></a>AITA Generating Moral Judgements of the Crowd with Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18336">http://arxiv.org/abs/2310.18336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Osama Bsher, Ameer Sabri<br>for: This paper aims to generate comments with moral reasoning for stories with moral dilemmas using the AITA subreddit as a dataset.methods: The authors will leverage the vast amount of data on the forum and use state-of-the-art seq2seq text generation models to generate coherent comments that align with the norms and values of the AITA community.results: The authors aim to evaluate the ability of these models to make moral judgments similarly to humans and produce concise comments providing clear moral stances and advice for the poster.<details>
<summary>Abstract</summary>
Morality is a fundamental aspect of human behavior and ethics, influencing how we interact with each other and the world around us. When faced with a moral dilemma, a person's ability to make clear moral judgments can be clouded. Due to many factors such as personal biases, emotions and situational factors people can find it difficult to decide their best course of action. The AmITheAsshole (AITA) subreddit is a forum on the social media platform Reddit that helps people get clarity and objectivity on their predicaments. In the forum people post anecdotes about moral dilemmas they are facing in their lives, seeking validation for their actions or advice on how to navigate the situation from the community. The morality of the actions in each post is classified based on the collective opinion of the community into mainly two labels, "Not The Asshole" (NTA) and "You Are The Asshole" (YTA). This project aims to generate comments with moral reasoning for stories with moral dilemmas using the AITA subreddit as a dataset. While past literature has explored the classification of posts into labels (Alhassan et al., 2022), the generation of comments remains a novel and challenging task. It involves understanding the complex social and ethical considerations in each situation. To address this challenge, we will leverage the vast amount of data on the forum with the goal of generating coherent comments that align with the norms and values of the AITA community. In this endeavor, we aim to evaluate state-of-the-art seq2seq text generation models for their ability to make moral judgments similarly to humans, ultimately producing concise comments providing clear moral stances and advice for the poster.
</details>
<details>
<summary>摘要</summary>
人类行为中的道德是一个基本方面，影响我们如何与其他人和世界around us interact。当面临道德困难时，人们可能会受到个人偏见、情感和情况因素的影响，导致困难做出明确的道德判断。为了帮助人们得到清晰性和 объекivity，Reddit上的AmITheAsshole（AITA）子社区成为了一个有用的平台。在这个社区中，人们会分享他们面临的道德困难，并请求 validation for their actions或对 Situation 的 Navigation 建议。根据社区的共同意见，每篇文章将被分类为主要两个标签：“Not The Asshole”（NTA）和“You Are The Asshole”（YTA）。这个项目的目标是使用 AITA 子社区的数据生成文章中的评论，以提供清晰的道德观点和建议。在这个任务中，我们将利用社区数据的庞大量，以生成一致的评论，与 AITA 社区的 norms 和价值观念相一致。为了解决这个挑战，我们将使用现代 seq2seq 文本生成模型，以模拟人类的道德判断能力。最终，我们希望通过生成简洁明了的评论，为poster提供清晰的道德观点和建议。
</details></li>
</ul>
<hr>
<h2 id="Linguistically-Motivated-Sign-Language-Segmentation"><a href="#Linguistically-Motivated-Sign-Language-Segmentation" class="headerlink" title="Linguistically Motivated Sign Language Segmentation"></a>Linguistically Motivated Sign Language Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13960">http://arxiv.org/abs/2310.13960</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sign-language-processing/transcription">https://github.com/sign-language-processing/transcription</a></li>
<li>paper_authors: Amit Moryossef, Zifan Jiang, Mathias Müller, Sarah Ebling, Yoav Goldberg</li>
<li>for: 本文旨在提出一种新的手语分割方法，以便在手语处理系统中进行下游任务，如手语识别、 транскриpción和机器翻译。</li>
<li>methods: 我们提出的方法基于手语 corpora 中观察到的语言学 clue，使用 BIO 标记替换传统的 IO 标记，以考虑手语的连续性。我们还explore使用光流特征来捕捉手语语法的 просодические特征。</li>
<li>results: 我们发现，使用 BIO 标记可以更好地模型手语 bounding box。在某些深度模型中，使用光流特征可以提高分割质量，但在更深的模型中，这些特征的贡献几乎可以忽略不计。通过精心调整模型的解码算法，我们可以进一步提高分割质量。我们的最终模型可以在不同的手语语言下进行零基础学习，并在不同的视频内容上进行渠道。<details>
<summary>Abstract</summary>
Sign language segmentation is a crucial task in sign language processing systems. It enables downstream tasks such as sign recognition, transcription, and machine translation. In this work, we consider two kinds of segmentation: segmentation into individual signs and segmentation into phrases, larger units comprising several signs. We propose a novel approach to jointly model these two tasks.   Our method is motivated by linguistic cues observed in sign language corpora. We replace the predominant IO tagging scheme with BIO tagging to account for continuous signing. Given that prosody plays a significant role in phrase boundaries, we explore the use of optical flow features. We also provide an extensive analysis of hand shapes and 3D hand normalization.   We find that introducing BIO tagging is necessary to model sign boundaries. Explicitly encoding prosody by optical flow improves segmentation in shallow models, but its contribution is negligible in deeper models. Careful tuning of the decoding algorithm atop the models further improves the segmentation quality.   We demonstrate that our final models generalize to out-of-domain video content in a different signed language, even under a zero-shot setting. We observe that including optical flow and 3D hand normalization enhances the robustness of the model in this context.
</details>
<details>
<summary>摘要</summary>
签语段落是指文本处理系统中的签语分割任务。它可以帮助下游任务，如签语识别、转写和机器翻译。在这个工作中，我们考虑了两种类型的分割：即分割成个体签语和分割成短语，后者是由多个签语组成的更大单位。我们提出了一种新的方法，旨在同时解决这两种任务。  我们的方法受到了签语 Corpora 中的语言学cue的启发。我们将主流的 IO 标记方案改为 BIO 标记，以考虑不间断的签语。由于语言的气息在短语边界上发挥重要作用，我们尝试使用光流特征。我们还提供了详细的手势分析和3D手势normalization。  我们发现，使用 BIO 标记是必要的，以便模型签语边界。使用光流特征可以在浅度模型中提高分割质量，但在深度模型中，其贡献几乎可以忽略不计。通过精细调整模型顶部的解码算法，可以进一步提高分割质量。  我们展示了我们的最终模型可以在不同的指语言中进行零基础学习，并在无预训练情况下保持良好的分割质量。我们发现，包含光流和3D手势normalization可以提高模型在这种情况下的Robustness。
</details></li>
</ul>
<hr>
<h2 id="Values-Ethics-Morals-On-the-Use-of-Moral-Concepts-in-NLP-Research"><a href="#Values-Ethics-Morals-On-the-Use-of-Moral-Concepts-in-NLP-Research" class="headerlink" title="Values, Ethics, Morals? On the Use of Moral Concepts in NLP Research"></a>Values, Ethics, Morals? On the Use of Moral Concepts in NLP Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13915">http://arxiv.org/abs/2310.13915</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karina Vida, Judith Simon, Anne Lauscher</li>
<li>for: 本研究旨在探讨NLPT中的伦理问题，尤其是语言模型的道德评价。</li>
<li>methods: 本研究使用文献综述和系统性分析方法，探讨NLPT中 morality 的定义和基础。</li>
<li>results: 研究发现，大多数文献没有提供明确的定义，也没有遵循哲学定义。此外，研究还给出了三个建议，以促进NLPT中的道德讨论。<details>
<summary>Abstract</summary>
With language technology increasingly affecting individuals' lives, many recent works have investigated the ethical aspects of NLP. Among other topics, researchers focused on the notion of morality, investigating, for example, which moral judgements language models make. However, there has been little to no discussion of the terminology and the theories underpinning those efforts and their implications. This lack is highly problematic, as it hides the works' underlying assumptions and hinders a thorough and targeted scientific debate of morality in NLP. In this work, we address this research gap by (a) providing an overview of some important ethical concepts stemming from philosophy and (b) systematically surveying the existing literature on moral NLP w.r.t. their philosophical foundation, terminology, and data basis. For instance, we analyse what ethical theory an approach is based on, how this decision is justified, and what implications it entails. Our findings surveying 92 papers show that, for instance, most papers neither provide a clear definition of the terms they use nor adhere to definitions from philosophy. Finally, (c) we give three recommendations for future research in the field. We hope our work will lead to a more informed, careful, and sound discussion of morality in language technology.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Providing an overview of important ethical concepts from philosophy.2. Systematically surveying the existing literature on moral NLP, examining their philosophical foundations, terminology, and data basis.3. Offering three recommendations for future research in the field.Our survey of 92 papers found that most papers do not provide clear definitions of the terms they use, and few adhere to definitions from philosophy. We hope that our work will contribute to a more informed, careful, and sound discussion of morality in language technology.</details></li>
</ol>
<hr>
<h2 id="RTSUM-Relation-Triple-based-Interpretable-Summarization-with-Multi-level-Salience-Visualization"><a href="#RTSUM-Relation-Triple-based-Interpretable-Summarization-with-Multi-level-Salience-Visualization" class="headerlink" title="RTSUM: Relation Triple-based Interpretable Summarization with Multi-level Salience Visualization"></a>RTSUM: Relation Triple-based Interpretable Summarization with Multi-level Salience Visualization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13895">http://arxiv.org/abs/2310.13895</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sjyyj/sjyyj">https://github.com/sjyyj/sjyyj</a></li>
<li>paper_authors: Seonglae Cho, Yonggi Cho, HoonJae Lee, Myungha Jang, Jinyoung Yeo, Dongha Lee</li>
<li>for: 本文提出了一种无监督概要框架，利用关系 triplets 为概要的基本单元。</li>
<li>methods: 输入文档后，本方法首先选择了突出的关系 triplets via 多级评分，然后通过文本-文本语言模型生成了简洁的概要。</li>
<li>results: 基于本方法，我们还开发了一款可解释性概要工具，提供了细致的解释与输出概要。用户可以自定义选择不同的级别，以便在文本单元层次上visual化文本的重要性。代码公开available。<details>
<summary>Abstract</summary>
In this paper, we present RTSUM, an unsupervised summarization framework that utilizes relation triples as the basic unit for summarization. Given an input document, RTSUM first selects salient relation triples via multi-level salience scoring and then generates a concise summary from the selected relation triples by using a text-to-text language model. On the basis of RTSUM, we also develop a web demo for an interpretable summarizing tool, providing fine-grained interpretations with the output summary. With support for customization options, our tool visualizes the salience for textual units at three distinct levels: sentences, relation triples, and phrases. The codes,are publicly available.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种无监督摘要框架，称为RTSUM，它利用关系三元组作为摘要的基本单元。给定输入文档，RTSUM首先选择了突出的关系三元组via多级重要性分数，然后使用文本到文本语言模型生成了一份简洁的摘要。基于RTSUM，我们还开发了一个可视化摘要工具，可以提供细化的解释。这个工具支持自定义选项，可以在三级层次（句子、关系三元组、短语）上Visualize文本单元的重要性。代码publicly available。
</details></li>
</ul>
<hr>
<h2 id="RECAP-Towards-Precise-Radiology-Report-Generation-via-Dynamic-Disease-Progression-Reasoning"><a href="#RECAP-Towards-Precise-Radiology-Report-Generation-via-Dynamic-Disease-Progression-Reasoning" class="headerlink" title="RECAP: Towards Precise Radiology Report Generation via Dynamic Disease Progression Reasoning"></a>RECAP: Towards Precise Radiology Report Generation via Dynamic Disease Progression Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13864">http://arxiv.org/abs/2310.13864</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wjhou/recap">https://github.com/wjhou/recap</a></li>
<li>paper_authors: Wenjun Hou, Yi Cheng, Kaishuai Xu, Wenjie Li, Jiang Liu<br>for: 实现 радиialogists 的工作负担减轻methods: 使用动态疾病进程理解和历史纪录融合results: 精确地生成专业医疗报告<details>
<summary>Abstract</summary>
Automating radiology report generation can significantly alleviate radiologists' workloads. Previous research has primarily focused on realizing highly concise observations while neglecting the precise attributes that determine the severity of diseases (e.g., small pleural effusion). Since incorrect attributes will lead to imprecise radiology reports, strengthening the generation process with precise attribute modeling becomes necessary. Additionally, the temporal information contained in the historical records, which is crucial in evaluating a patient's current condition (e.g., heart size is unchanged), has also been largely disregarded. To address these issues, we propose RECAP, which generates precise and accurate radiology reports via dynamic disease progression reasoning. Specifically, RECAP first predicts the observations and progressions (i.e., spatiotemporal information) given two consecutive radiographs. It then combines the historical records, spatiotemporal information, and radiographs for report generation, where a disease progression graph and dynamic progression reasoning mechanism are devised to accurately select the attributes of each observation and progression. Extensive experiments on two publicly available datasets demonstrate the effectiveness of our model.
</details>
<details>
<summary>摘要</summary>
自动化放射学报告生成可以减轻放射学家的工作负担。先前的研究主要集中在实现极简观察结果，而忽略了疾病严重程度决定的精确属性（例如肿胸积液）。由于错误的属性会导致不准确的放射学报告，因此需要加强生成过程中的精确属性模型。此外，历史记录中的时间信息，如评估病人当前状况中的心脏大小是否变化（例如心脏大小不变），也被大量忽略。为解决这些问题，我们提议RECAP，它通过动态疾病进程逻辑来生成精确和准确的放射学报告。具体来说，RECAP首先预测两个连续的放射ogram的观察结果和进程（即空间时间信息）。然后，它将历史记录、空间时间信息和放射ogram组合起来，通过疾病进程图和动态进程逻辑机制来准确选择每个观察结果和进程的属性。我们对公共数据集进行了广泛的实验，结果表明RECAP的效果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/21/cs.CL_2023_10_21/" data-id="clp88dbu500dwob88dc705cjk" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_10_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/21/cs.LG_2023_10_21/" class="article-date">
  <time datetime="2023-10-21T10:00:00.000Z" itemprop="datePublished">2023-10-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/21/cs.LG_2023_10_21/">cs.LG - 2023-10-21</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Optimal-Batched-Best-Arm-Identification"><a href="#Optimal-Batched-Best-Arm-Identification" class="headerlink" title="Optimal Batched Best Arm Identification"></a>Optimal Batched Best Arm Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14129">http://arxiv.org/abs/2310.14129</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyuan Jin, Yu Yang, Jing Tang, Xiaokui Xiao, Pan Xu</li>
<li>for: 本研究探讨批处理最佳臂标识问题（Batched Best Arm Identification，BBAI），learner的目标是尽可能快地确定最佳臂，同时尽可能减少策略的变化次数。</li>
<li>methods: 我们提出了三批最佳臂标识算法（Tri-BBAI），它是首个在极限设定（即$\delta\to 0$）下实现最佳样本复杂性的批处理算法，并且只需要最多三批。基于Tri-BBAI，我们还提出了几乎最佳批处理最佳臂标识算法（Opt-BBAI），它在非极限设定（即$\delta&gt;0$）下实现近似最佳样本和批复杂性，而且与Tri-BBAI在$\delta\to 0$时的复杂性相同。</li>
<li>results: 我们的研究表明，Opt-BBAI在非极限设定下实现了近似最佳样本和批复杂性，而且不需要对返回最佳臂的事件进行条件，这与之前的批处理算法不同。此外，我们还提出了一种新的检查最佳臂是否被消除的方法，它是独立的兴趣。<details>
<summary>Abstract</summary>
We study the batched best arm identification (BBAI) problem, where the learner's goal is to identify the best arm while switching the policy as less as possible. In particular, we aim to find the best arm with probability $1-\delta$ for some small constant $\delta>0$ while minimizing both the sample complexity (total number of arm pulls) and the batch complexity (total number of batches). We propose the three-batch best arm identification (Tri-BBAI) algorithm, which is the first batched algorithm that achieves the optimal sample complexity in the asymptotic setting (i.e., $\delta\rightarrow 0$) and runs only in at most $3$ batches. Based on Tri-BBAI, we further propose the almost optimal batched best arm identification (Opt-BBAI) algorithm, which is the first algorithm that achieves the near-optimal sample and batch complexity in the non-asymptotic setting (i.e., $\delta>0$ is arbitrarily fixed), while enjoying the same batch and sample complexity as Tri-BBAI when $\delta$ tends to zero. Moreover, in the non-asymptotic setting, the complexity of previous batch algorithms is usually conditioned on the event that the best arm is returned (with a probability of at least $1-\delta$), which is potentially unbounded in cases where a sub-optimal arm is returned. In contrast, the complexity of Opt-BBAI does not rely on such an event. This is achieved through a novel procedure that we design for checking whether the best arm is eliminated, which is of independent interest.
</details>
<details>
<summary>摘要</summary>
我们研究批处最佳臂识别问题（BBAI），学生的目标是将最佳臂识别出来，将策略调整得最少。具体来说，我们想要在probability $1-\delta$的情况下找到最佳臂，而且将数据分析和批号复杂度（total number of arm pulls和total number of batches）降到最低。我们提出了三批最佳臂识别算法（Tri-BBAI），这是第一个批号算法，可以在无限边界（i.e., $\delta\rightarrow 0$)的情况下实现最佳数据分析和批号复杂度，并且只需要最多三个批号。基于Tri-BBAI，我们进一步提出了几乎最佳批号最佳臂识别算法（Opt-BBAI），这是第一个可以在非 asymptotic 环境（i.e., $\delta>0$是任意固定）下实现几乎最佳的数据分析和批号复杂度，并且跟Tri-BBAI在 $\delta$ 趋向 zero 时的性能相同。此外，在非 asymptotic 环境中，前一代批号算法的复杂度通常受到返回最佳臂的几率（至少是 $1-\delta$）的限制，这可能是无限大的情况。在 контраス特，Opt-BBAI 的复杂度不受这种事件的限制。这是通过我们设计的一种独特的检查方法来检查最佳臂是否被消除的。
</details></li>
</ul>
<hr>
<h2 id="DispersioNET-Joint-Inversion-of-Rayleigh-Wave-Multimode-Phase-Velocity-Dispersion-Curves-using-Convolutional-Neural-Networks"><a href="#DispersioNET-Joint-Inversion-of-Rayleigh-Wave-Multimode-Phase-Velocity-Dispersion-Curves-using-Convolutional-Neural-Networks" class="headerlink" title="DispersioNET: Joint Inversion of Rayleigh-Wave Multimode Phase Velocity Dispersion Curves using Convolutional Neural Networks"></a>DispersioNET: Joint Inversion of Rayleigh-Wave Multimode Phase Velocity Dispersion Curves using Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14094">http://arxiv.org/abs/2310.14094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rohan Sharma, Divakar Vashisth, Bharath Shekar</li>
<li>for: 本研究使用深度学习模型DispersioNET来对声波基本和高频模式phase velocity dispersion curve进行联合逆解，以获取声波速度profile。</li>
<li>methods: 该模型基于卷积神经网络(CNN)，并在不同的噪声水平上进行训练和测试。</li>
<li>results: DispersioNET能够准确预测声波速度profile，并能够抗噪和鲁棒性。<details>
<summary>Abstract</summary>
Rayleigh wave dispersion curves have been widely used in near-surface studies, and are primarily inverted for the shear wave (S-wave) velocity profiles. However, the inverse problem is ill-posed, non-unique and nonlinear. Here, we introduce DispersioNET, a deep learning model based on convolution neural networks (CNN) to perform the joint inversion of Rayleigh wave fundamental and higher order mode phase velocity dispersion curves. DispersioNET is trained and tested on both noise-free and noisy dispersion curve datasets and predicts S-wave velocity profiles that match closely with the true velocities. The architecture is agnostic to variations in S-wave velocity profiles such as increasing velocity with depth and intermediate low-velocity layers, while also ensuring that the output remains independent of the number of layers.
</details>
<details>
<summary>摘要</summary>
rayleigh 波动峰位 Curves 在近地表研究中广泛使用，主要是对剪切波（S波）速度profile 进行逆解。然而，逆问题是不定、多重和非线性的。我们介绍了 DispersioNET，一种基于卷积神经网络（CNN）的深度学习模型，用于同时逆解 Rayleigh 波基本和高阶模式 phase 峰位速度分布 Curves。DispersioNET 在噪声存在和缺失的数据集上训练和测试，并能够准确预测 S波速度profile，与真实速度匹配得非常 closely。模型具有不同 S波速度profile 变化的tolerance，同时保证输出不受层数的影响。
</details></li>
</ul>
<hr>
<h2 id="A-Specialized-Semismooth-Newton-Method-for-Kernel-Based-Optimal-Transport"><a href="#A-Specialized-Semismooth-Newton-Method-for-Kernel-Based-Optimal-Transport" class="headerlink" title="A Specialized Semismooth Newton Method for Kernel-Based Optimal Transport"></a>A Specialized Semismooth Newton Method for Kernel-Based Optimal Transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14087">http://arxiv.org/abs/2310.14087</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyi Lin, Marco Cuturi, Michael I. Jordan</li>
<li>for:  This paper proposes a new method for solving optimal transport (OT) problems using a nonsmooth fixed-point model and a specialized semismooth Newton (SSN) method.</li>
<li>methods: The proposed method uses a nonsmooth fixed-point model and a specialized semismooth Newton (SSN) method to efficiently solve kernel-based OT problems.</li>
<li>results: The proposed method achieves a global convergence rate of $O(1&#x2F;\sqrt{k})$ and a local quadratic convergence rate under standard regularity conditions, and shows substantial speedups over the short-step interior-point method (SSIPM) on both synthetic and real datasets.Here is the text in Simplified Chinese:</li>
<li>for: 这篇论文提出了一种使用非平滑稳点模型和特殊的半凝固新颖方法解决优Transport问题的新方法。</li>
<li>methods: 该方法使用非平滑稳点模型和特殊的半凝固新颖方法解决核心基于的优Transport问题。</li>
<li>results: 该方法实现了$O(1&#x2F;\sqrt{k})$的全局收敛率和标准规定的本地二次收敛率，并在实验中显示了SSIPM的显著加速。<details>
<summary>Abstract</summary>
Kernel-based optimal transport (OT) estimators offer an alternative, functional estimation procedure to address OT problems from samples. Recent works suggest that these estimators are more statistically efficient than plug-in (linear programming-based) OT estimators when comparing probability measures in high-dimensions~\citep{Vacher-2021-Dimension}. Unfortunately, that statistical benefit comes at a very steep computational price: because their computation relies on the short-step interior-point method (SSIPM), which comes with a large iteration count in practice, these estimators quickly become intractable w.r.t. sample size $n$. To scale these estimators to larger $n$, we propose a nonsmooth fixed-point model for the kernel-based OT problem, and show that it can be efficiently solved via a specialized semismooth Newton (SSN) method: We show, exploring the problem's structure, that the per-iteration cost of performing one SSN step can be significantly reduced in practice. We prove that our SSN method achieves a global convergence rate of $O(1/\sqrt{k})$, and a local quadratic convergence rate under standard regularity conditions. We show substantial speedups over SSIPM on both synthetic and real datasets.
</details>
<details>
<summary>摘要</summary>
kernel-based最优运输（OT）估计器提供了一种代替方法来解决OT问题，从样本中进行函数估计。据最新的研究表明，这些估计器在高维度下比插入（线性编程基于）OT估计器更有统计效率，但是计算成本却非常高：因为它们的计算基于短步内部点法（SSIPM），在实践中通常需要许多迭代。为了扩展这些估计器，我们提出了一种非光滑固定点模型，并证明可以通过特殊的半稳定新颖（SSN）方法高效解决。我们展示，通过利用问题的结构，每次SSN步骤的计算成本可以在实践中减少到一定程度。我们证明我们的SSN方法在全球收敛率为$O(1/\sqrt{k})$，以及当标准正则条件下的本地二阶收敛率。我们在 sintetic 和实际数据集上展示了显著的计算速度提升。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Doubly-Optimal-No-Regret-Learning-in-Strongly-Monotone-and-Exp-Concave-Games-with-Gradient-Feedback"><a href="#Adaptive-Doubly-Optimal-No-Regret-Learning-in-Strongly-Monotone-and-Exp-Concave-Games-with-Gradient-Feedback" class="headerlink" title="Adaptive, Doubly Optimal No-Regret Learning in Strongly Monotone and Exp-Concave Games with Gradient Feedback"></a>Adaptive, Doubly Optimal No-Regret Learning in Strongly Monotone and Exp-Concave Games with Gradient Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14085">http://arxiv.org/abs/2310.14085</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael I. Jordan, Tianyi Lin, Zhengyuan Zhou</li>
<li>for: 本研究旨在设计一个不需要先知 convexity&#x2F;monotonicity 参数的在线梯度下降（OGD）算法，以便在单个代理和多个代理的情况下实现最优的 regret 和 Nash 平衡rate。</li>
<li>methods: 本研究使用了一种全部适应的 OGD 算法，称为 \textsf{AdaOGD}，它不需要先知 convexity&#x2F;monotonicity 参数。在单个代理情况下，\textsf{AdaOGD} 可以实现 $O(\log^2(T))$ 的 regret，这是最优的结果，只有 log 因子的差异。在多个代理情况下，如果每个代理使用 \textsf{AdaOGD}，则共同行动会在 $O(\frac{\log^3 T}{T})$ 的速度 converges 到一个唯一的 Nash 平衡。</li>
<li>results: 本研究的结果显示，\textsf{AdaOGD} 可以在新闻vendor 问题中实现最优的 regret 和 Nash 平衡rate。此外，本研究还扩展到了更通用的 exp-concave 成本函数和游戏中，使用在线 Newton 步骤（ONS）算法。<details>
<summary>Abstract</summary>
Online gradient descent (OGD) is well known to be doubly optimal under strong convexity or monotonicity assumptions: (1) in the single-agent setting, it achieves an optimal regret of $\Theta(\log T)$ for strongly convex cost functions; and (2) in the multi-agent setting of strongly monotone games, with each agent employing OGD, we obtain last-iterate convergence of the joint action to a unique Nash equilibrium at an optimal rate of $\Theta(\frac{1}{T})$. While these finite-time guarantees highlight its merits, OGD has the drawback that it requires knowing the strong convexity/monotonicity parameters. In this paper, we design a fully adaptive OGD algorithm, \textsf{AdaOGD}, that does not require a priori knowledge of these parameters. In the single-agent setting, our algorithm achieves $O(\log^2(T))$ regret under strong convexity, which is optimal up to a log factor. Further, if each agent employs \textsf{AdaOGD} in strongly monotone games, the joint action converges in a last-iterate sense to a unique Nash equilibrium at a rate of $O(\frac{\log^3 T}{T})$, again optimal up to log factors. We illustrate our algorithms in a learning version of the classical newsvendor problem, where due to lost sales, only (noisy) gradient feedback can be observed. Our results immediately yield the first feasible and near-optimal algorithm for both the single-retailer and multi-retailer settings. We also extend our results to the more general setting of exp-concave cost functions and games, using the online Newton step (ONS) algorithm.
</details>
<details>
<summary>摘要</summary>
在线梯度下降（OGD）在强连续或强 monotonicity  assumption下是著名的双优化算法：（1）在单个代理Setting中，它可以在强连续成本函数下 achieve  óptimal regret of $\Theta(\log T)$;（2）在多个代理Setting中，每个代理使用 OGD，我们可以获得last-iterate 协调的joint action converges to a unique Nash equilibrium at an optimal rate of $\Theta(\frac{1}{T})$. 虽然这些 finite-time guarantees  highlight its merits, OGD 需要先知道强连续/MONOTONICITY 参数。在这篇论文中，我们设计了一个完全适应的 OGD 算法，\textsf{AdaOGD}，不需要先知道这些参数。在单个代理Setting中，我们的算法可以 achieve $O(\log^2(T))$ regret under strong convexity, which is optimal up to a log factor。此外，如果每个代理使用 \textsf{AdaOGD} 在强MONOTONICITY games中，joint action 会在 last-iterate  sense converge to a unique Nash equilibrium at a rate of $O(\frac{\log^3 T}{T})$, again optimal up to log factors。我们在学习版新闻 vendor problem中 illustrate 我们的算法，因为lost sales，只能观察（噪音）梯度反馈。我们的结果 immediately yield the first feasible and near-optimal algorithm for both the single-retailer and multi-retailer settings。我们还将我们的结果推广到更一般的 exp-concave cost functions 和 games，使用 online Newton step（ONS）算法。
</details></li>
</ul>
<hr>
<h2 id="Graph-Neural-Networks-and-Applied-Linear-Algebra"><a href="#Graph-Neural-Networks-and-Applied-Linear-Algebra" class="headerlink" title="Graph Neural Networks and Applied Linear Algebra"></a>Graph Neural Networks and Applied Linear Algebra</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14084">http://arxiv.org/abs/2310.14084</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sandialabs/gnn-applied-linear-algebra">https://github.com/sandialabs/gnn-applied-linear-algebra</a></li>
<li>paper_authors: Nicholas S. Moore, Eric C. Cyr, Peter Ohm, Christopher M. Siefert, Raymond S. Tuminaro</li>
<li>for: 本文主要针对数学 linear algebra 领域的科学计算问题，探讨如何使用 neural network (NN) 来解决 sparse matrix 计算问题。</li>
<li>methods: 本文使用 graph neural network (GNN) 方法来处理 sparse matrix 计算问题，GNN 定义了聚合函数（例如和），可以操作变量大小的输入数据，生成固定大小的输出数据，以便应用 MLP 来解决问题。</li>
<li>results: 本文通过提供了多个实际例子，示出了如何使用 GNN 来解决一些常见的 linear algebra 任务，包括matrix-vector 乘法、插值、松弛方法和连接度度量。<details>
<summary>Abstract</summary>
Sparse matrix computations are ubiquitous in scientific computing. With the recent interest in scientific machine learning, it is natural to ask how sparse matrix computations can leverage neural networks (NN). Unfortunately, multi-layer perceptron (MLP) neural networks are typically not natural for either graph or sparse matrix computations. The issue lies with the fact that MLPs require fixed-sized inputs while scientific applications generally generate sparse matrices with arbitrary dimensions and a wide range of nonzero patterns (or matrix graph vertex interconnections). While convolutional NNs could possibly address matrix graphs where all vertices have the same number of nearest neighbors, a more general approach is needed for arbitrary sparse matrices, e.g. arising from discretized partial differential equations on unstructured meshes. Graph neural networks (GNNs) are one approach suitable to sparse matrices. GNNs define aggregation functions (e.g., summations) that operate on variable size input data to produce data of a fixed output size so that MLPs can be applied. The goal of this paper is to provide an introduction to GNNs for a numerical linear algebra audience. Concrete examples are provided to illustrate how many common linear algebra tasks can be accomplished using GNNs. We focus on iterative methods that employ computational kernels such as matrix-vector products, interpolation, relaxation methods, and strength-of-connection measures. Our GNN examples include cases where parameters are determined a-priori as well as cases where parameters must be learned. The intent with this article is to help computational scientists understand how GNNs can be used to adapt machine learning concepts to computational tasks associated with sparse matrices. It is hoped that this understanding will stimulate data-driven extensions of classical sparse linear algebra tasks.
</details>
<details>
<summary>摘要</summary>
科学计算中的稀疏矩阵计算是普遍存在的。随着科学机器学习的兴趣增长，可以问题是如何使用神经网络（NN）来利用稀疏矩阵计算。然而，多层感知网络（MLP）通常不适用于图或稀疏矩阵计算，因为MLP需要固定大小的输入，而科学应用通常会生成稀疏矩阵，其维度和非零强相互关系是不固定的。而卷积神经网络（CNN）可能可以解决图格上的稀疏矩阵问题，但是需要一种更通用的方法来解决任意稀疏矩阵问题，例如由不同维度的几何函数所生成的稀疏矩阵。图神经网络（GNN）是一种适用于稀疏矩阵的方法，GNN定义了聚合函数（例如和），这些函数可以在变量大小的输入数据上运行，以生成固定大小的输出数据，从而使得MLP可以应用。本文的目标是为数 Linear Algebra 读者提供 GNN 的引入，并提供一些具体的例子来说明如何使用 GNN 来完成许多常见的Linear Algebra 任务。我们关注到了迭代方法，包括矩阵-向量产品、 interpolate、 relaxation 方法和 strength-of-connection 度量。我们的 GNN 例子包括参数由先前确定的情况以及参数需要学习的情况。我们希望通过这篇文章，让计算科学家理解 GNN 如何用于改变机器学习概念，以便在稀疏矩阵计算中进行数据驱动的扩展。
</details></li>
</ul>
<hr>
<h2 id="Counterfactual-Prediction-Under-Selective-Confounding"><a href="#Counterfactual-Prediction-Under-Selective-Confounding" class="headerlink" title="Counterfactual Prediction Under Selective Confounding"></a>Counterfactual Prediction Under Selective Confounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14064">http://arxiv.org/abs/2310.14064</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sohaib730/causalml">https://github.com/sohaib730/causalml</a></li>
<li>paper_authors: Sohaib Kiani, Jared Barton, Jon Sushinsky, Lynda Heimbach, Bo Luo</li>
<li>for: 本研究旨在Addressing the challenge of conducting interpretable causal inference between a binary treatment and its resulting outcome when not all confounders are known.</li>
<li>methods: 我们提出了一种基于Selective Confounding的方法，使用双调sample来实现。这种方法可以在多个决策者与不同政策存在的情况下进行适应。</li>
<li>results: 我们提供了both theoretical error bounds和实际证据，证明了我们的方法的有效性。此外，我们还介绍了三种特定于儿童分配场景的评价方法，以增强透明性和可解性。<details>
<summary>Abstract</summary>
This research addresses the challenge of conducting interpretable causal inference between a binary treatment and its resulting outcome when not all confounders are known. Confounders are factors that have an influence on both the treatment and the outcome. We relax the requirement of knowing all confounders under desired treatment, which we refer to as Selective Confounding, to enable causal inference in diverse real-world scenarios. Our proposed scheme is designed to work in situations where multiple decision-makers with different policies are involved and where there is a re-evaluation mechanism after the initial decision to ensure consistency. These assumptions are more practical to fulfill compared to the availability of all confounders under all treatments. To tackle the issue of Selective Confounding, we propose the use of dual-treatment samples. These samples allow us to employ two-step procedures, such as Regression Adjustment or Doubly-Robust, to learn counterfactual predictors. We provide both theoretical error bounds and empirical evidence of the effectiveness of our proposed scheme using synthetic and real-world child placement data. Furthermore, we introduce three evaluation methods specifically tailored to assess the performance in child placement scenarios. By emphasizing transparency and interpretability, our approach aims to provide decision-makers with a valuable tool. The source code repository of this work is located at https://github.com/sohaib730/CausalML.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="On-discretisation-drift-and-smoothness-regularisation-in-neural-network-training"><a href="#On-discretisation-drift-and-smoothness-regularisation-in-neural-network-training" class="headerlink" title="On discretisation drift and smoothness regularisation in neural network training"></a>On discretisation drift and smoothness regularisation in neural network training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14036">http://arxiv.org/abs/2310.14036</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mihaela Claudia Rosca</li>
<li>for: 本研究的目的是更深入地理解深度学习，尤其是优化和模型规范化。</li>
<li>methods: 本研究使用了梯度下降（GD）和负梯度流（NGF）等优化算法，以及新的连续时间流动来研究GD的动态。</li>
<li>results: 研究发现，在超级vised学习和两个玩家游戏中训练不稳定的问题可以通过构建新的学习率时间表和规范来解决。此外，研究还发现了在各种深度学习领域中细eness规范对优化的影响，并在强化学习中添加细eness规范后得到了性能提升。<details>
<summary>Abstract</summary>
The deep learning recipe of casting real-world problems as mathematical optimisation and tackling the optimisation by training deep neural networks using gradient-based optimisation has undoubtedly proven to be a fruitful one. The understanding behind why deep learning works, however, has lagged behind its practical significance. We aim to make steps towards an improved understanding of deep learning with a focus on optimisation and model regularisation. We start by investigating gradient descent (GD), a discrete-time algorithm at the basis of most popular deep learning optimisation algorithms. Understanding the dynamics of GD has been hindered by the presence of discretisation drift, the numerical integration error between GD and its often studied continuous-time counterpart, the negative gradient flow (NGF). To add to the toolkit available to study GD, we derive novel continuous-time flows that account for discretisation drift. Unlike the NGF, these new flows can be used to describe learning rate specific behaviours of GD, such as training instabilities observed in supervised learning and two-player games. We then translate insights from continuous time into mitigation strategies for unstable GD dynamics, by constructing novel learning rate schedules and regularisers that do not require additional hyperparameters. Like optimisation, smoothness regularisation is another pillar of deep learning's success with wide use in supervised learning and generative modelling. Despite their individual significance, the interactions between smoothness regularisation and optimisation have yet to be explored. We find that smoothness regularisation affects optimisation across multiple deep learning domains, and that incorporating smoothness regularisation in reinforcement learning leads to a performance boost that can be recovered using adaptions to optimisation methods.
</details>
<details>
<summary>摘要</summary>
深度学习的制作方法，将现实世界的问题化为数学优化问题，并使用深度神经网络进行加速器基于梯度下降优化，不疑而是成功的。然而，深度学习的工作原理尚未得到充分的理解。我们想要做出一些探索深度学习的尝试，特icularly focusing on optimization and model regularization. We start by investigating gradient descent (GD), a discrete-time algorithm that is the foundation of most popular deep learning optimization algorithms. Understanding the dynamics of GD has been hindered by the presence of discretization drift, the numerical integration error between GD and its continuous-time counterpart, the negative gradient flow (NGF). To add to the toolkit available to study GD, we derive novel continuous-time flows that account for discretization drift. Unlike the NGF, these new flows can be used to describe learning rate specific behaviors of GD, such as training instabilities observed in supervised learning and two-player games. We then translate insights from continuous time into mitigation strategies for unstable GD dynamics, by constructing novel learning rate schedules and regularizers that do not require additional hyperparameters. Smoothness regularization is another pillar of deep learning's success, with wide use in supervised learning and generative modeling. Despite their individual significance, the interactions between smoothness regularization and optimization have yet to be explored. We find that smoothness regularization affects optimization across multiple deep learning domains, and that incorporating smoothness regularization in reinforcement learning leads to a performance boost that can be recovered using adaptions to optimization methods.
</details></li>
</ul>
<hr>
<h2 id="Filling-the-Missing-Exploring-Generative-AI-for-Enhanced-Federated-Learning-over-Heterogeneous-Mobile-Edge-Devices"><a href="#Filling-the-Missing-Exploring-Generative-AI-for-Enhanced-Federated-Learning-over-Heterogeneous-Mobile-Edge-Devices" class="headerlink" title="Filling the Missing: Exploring Generative AI for Enhanced Federated Learning over Heterogeneous Mobile Edge Devices"></a>Filling the Missing: Exploring Generative AI for Enhanced Federated Learning over Heterogeneous Mobile Edge Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13981">http://arxiv.org/abs/2310.13981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peichun Li, Hanwen Zhang, Yuan Wu, Liping Qian, Rong Yu, Dusit Niyato, Xuemin Shen</li>
<li>for: 提高分布式人工智能模型训练过程中的Edge网络中设备的数据和资源不同性问题。</li>
<li>methods: 提议使用生成AI技术实现 federated learning，通过填充地方数据的想法来解决数据不同性问题，同时提高训练效率和设备资源利用率。</li>
<li>results: 实验结果表明，使用FIMI可以将设备 сторо的能源减少50%，同时达到global测试准确率目标，并且在非独立同分布（non-IID）数据下，FIMI可以显著提高全球准确率。<details>
<summary>Abstract</summary>
Distributed Artificial Intelligence (AI) model training over mobile edge networks encounters significant challenges due to the data and resource heterogeneity of edge devices. The former hampers the convergence rate of the global model, while the latter diminishes the devices' resource utilization efficiency. In this paper, we propose a generative AI-empowered federated learning to address these challenges by leveraging the idea of FIlling the MIssing (FIMI) portion of local data. Specifically, FIMI can be considered as a resource-aware data augmentation method that effectively mitigates the data heterogeneity while ensuring efficient FL training. We first quantify the relationship between the training data amount and the learning performance. We then study the FIMI optimization problem with the objective of minimizing the device-side overall energy consumption subject to required learning performance constraints. The decomposition-based analysis and the cross-entropy searching method are leveraged to derive the solution, where each device is assigned suitable AI-synthesized data and resource utilization policy. Experiment results demonstrate that FIMI can save up to 50% of the device-side energy to achieve the target global test accuracy in comparison with the existing methods. Meanwhile, FIMI can significantly enhance the converged global accuracy under the non-independently-and-identically distribution (non-IID) data.
</details>
<details>
<summary>摘要</summary>
分布式人工智能（AI）模型训练在移动边缘网络上遇到了数据和资源不一致的问题。前者阻碍全球模型的吞吐率，而后者降低设备的资源利用效率。在这篇论文中，我们提出了基于生成AI的联邦学习来解决这些问题，利用了填充缺失（FIMI）的想法。 Specifically, FIMI可以看作是一种资源意识的数据扩充方法，有效地缓解数据不一致性，同时保证了有效的联邦学习训练。我们首先量化了训练数据量和学习性能之间的关系。然后，我们研究了FIMI优化问题，即将设备侧总能 consumption最小化，保证学习性能要求。通过分解分析和十字推测法，我们 derivated解决方案，每个设备都被分配了适合的AI生成的数据和资源利用策略。实验结果表明，FIMI可以将设备侧能 consumption降低至50%，以达到targettest accuracy。同时，FIMI可以在非独立同一分布（non-IID）数据下显著提高全球准确率。
</details></li>
</ul>
<hr>
<h2 id="Continual-Invariant-Risk-Minimization"><a href="#Continual-Invariant-Risk-Minimization" class="headerlink" title="Continual Invariant Risk Minimization"></a>Continual Invariant Risk Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13977">http://arxiv.org/abs/2310.13977</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Alesiani, Shujian Yu, Mathias Niepert</li>
<li>for: 本研究旨在提出一种基于环境变换的连续学习方法，以便在不同环境中学习模型能够捕捉到共同特征表示。</li>
<li>methods: 本研究使用了 invariant risk minimization（IRM）方法，即在所有环境都可用于学习系统上的方法。另外，本研究还提出了一种基于变分 Bayesian 和双层框架的扩展方法，以满足连续学习中的共同特征捕捉。</li>
<li>results: 研究表明，使用提出的方法可以在多个数据集和多个顺序环境中，与之前的方法相比或与之比肩，提高连续学习中的模型性能。<details>
<summary>Abstract</summary>
Empirical risk minimization can lead to poor generalization behavior on unseen environments if the learned model does not capture invariant feature representations. Invariant risk minimization (IRM) is a recent proposal for discovering environment-invariant representations. IRM was introduced by Arjovsky et al. (2019) and extended by Ahuja et al. (2020). IRM assumes that all environments are available to the learning system at the same time. With this work, we generalize the concept of IRM to scenarios where environments are observed sequentially. We show that existing approaches, including those designed for continual learning, fail to identify the invariant features and models across sequentially presented environments. We extend IRM under a variational Bayesian and bilevel framework, creating a general approach to continual invariant risk minimization. We also describe a strategy to solve the optimization problems using a variant of the alternating direction method of multiplier (ADMM). We show empirically using multiple datasets and with multiple sequential environments that the proposed methods outperform or is competitive with prior approaches.
</details>
<details>
<summary>摘要</summary>
empirical risk minimization可能会导致在未看到的环境中的差异化行为，如果学习的模型没有捕捉环境不变的特征表示。不变风险最小化(IRM)是一种最近提出的环境不变表示发现方法。IRM由Arjovsky et al.（2019）引入并由Ahuja et al.（2020）扩展。IRM假设所有环境都可以同时给学习系统提供。在这种情况下，我们推广了IRM的概念，以适应sequentially presented environments中的环境。我们表明，包括 continual learning的方法在内的现有方法无法在sequentially presented environments中标识不变的特征和模型。我们将IRM扩展为variational Bayesian和bilateral框架，创造一种总体的持续不变风险最小化方法。我们还描述了使用一种变体的alternating direction method of multiplier(ADMM)来解决优化问题的策略。我们通过多个数据集和多个sequential environments的实验表明，提posed方法可以比或与之前的方法相比性。
</details></li>
</ul>
<hr>
<h2 id="ASBART-Accelerated-Soft-Bayes-Additive-Regression-Trees"><a href="#ASBART-Accelerated-Soft-Bayes-Additive-Regression-Trees" class="headerlink" title="ASBART:Accelerated Soft Bayes Additive Regression Trees"></a>ASBART:Accelerated Soft Bayes Additive Regression Trees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13975">http://arxiv.org/abs/2310.13975</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/richael008/xsbart">https://github.com/richael008/xsbart</a></li>
<li>paper_authors: Hao Ran, Yang Bai</li>
<li>For:  This paper proposes a new variant of Bayesian additive regression trees (BART) called accelerate soft BART (ASBART), which improves the speed of the existing Soft BART model while maintaining comparable accuracy.* Methods: The proposed ASBART method uses a new algorithm that is about 10 times faster than the default Soft BART method, making it more practical for real-world applications.* Results: Simulation studies show that ASBART has comparable accuracy to Soft BART, while being significantly faster in terms of computational speed. The code for ASBART is open-source and available online.<details>
<summary>Abstract</summary>
Bayes additive regression trees(BART) is a nonparametric regression model which has gained wide-spread popularity in recent years due to its flexibility and high accuracy of estimation. Soft BART,one variation of BART,improves both practically and heoretically on existing Bayesian sum-of-trees models. One bottleneck for Soft BART is its slow speed in the long MCMC loop. Compared to BART,it use more than about 20 times to complete the calculation with the default setting. We proposed a variant of BART named accelerate Soft BART(ASBART). Simulation studies show that the new method is about 10 times faster than the Soft BART with comparable accuracy. Our code is open-source and available at https://github.com/richael008/XSBART.
</details>
<details>
<summary>摘要</summary>
bayes 添加 regresión árboles (BART) 是一种非 Parametric 回归模型，在过去几年内得到了广泛的推广和应用，因为它的灵活性和估计精度高。 软 BART，BART 的一种变体，在现有的 Bayesian 汇集树模型上进行了改进，从理论和实践两个方面来说，它提高了回归预测的精度。 然而，Soft BART 的长MCMC循环速度比较慢，相比 BART，它需要大约 20 倍的计算时间。我们提出了一种加速 Soft BART 的方法，称为加速 Soft BART (ASBART)。 根据 simulations 的研究，新方法比 Soft BART 快约 10 倍，并且与其相比，它们在精度方面几乎相同。 我们的代码开源在 GitHub 上，可以在 <https://github.com/richael008/XSBART> 获取。
</details></li>
</ul>
<hr>
<h2 id="Distributed-Linear-Regression-with-Compositional-Covariates"><a href="#Distributed-Linear-Regression-with-Compositional-Covariates" class="headerlink" title="Distributed Linear Regression with Compositional Covariates"></a>Distributed Linear Regression with Compositional Covariates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13969">http://arxiv.org/abs/2310.13969</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Chao, Lei Huang, Xuejun Ma</li>
<li>for: 解决大数据集中分布式统计方法和计算问题。</li>
<li>methods: 提议了两种分布式优化技术，一种是基于ADMM框架的中央化优化算法，另一种是基于CDMM框架的分布式坐标下降算法。</li>
<li>results: 通过对真实数据和 sintetic数据进行数值实验，证明了提议的算法的有效性和可靠性。<details>
<summary>Abstract</summary>
With the availability of extraordinarily huge data sets, solving the problems of distributed statistical methodology and computing for such data sets has become increasingly crucial in the big data area. In this paper, we focus on the distributed sparse penalized linear log-contrast model in massive compositional data. In particular, two distributed optimization techniques under centralized and decentralized topologies are proposed for solving the two different constrained convex optimization problems. Both two proposed algorithms are based on the frameworks of Alternating Direction Method of Multipliers (ADMM) and Coordinate Descent Method of Multipliers(CDMM, Lin et al., 2014, Biometrika). It is worth emphasizing that, in the decentralized topology, we introduce a distributed coordinate-wise descent algorithm based on Group ADMM(GADMM, Elgabli et al., 2020, Journal of Machine Learning Research) for obtaining a communication-efficient regularized estimation. Correspondingly, the convergence theories of the proposed algorithms are rigorously established under some regularity conditions. Numerical experiments on both synthetic and real data are conducted to evaluate our proposed algorithms.
</details>
<details>
<summary>摘要</summary>
随着庞大数据集的可用性的提高，解决分布式统计方法和计算问题已成为大数据领域中越来越重要的问题。在这篇论文中，我们关注于巨大compositional数据中的分布式稀缺假设模型。我们提出了两种分布式优化技术，一种是基于中央化topology，另一种是基于分布式topology。两种提出的算法都基于ADMM和CDMM框架(Lin et al., 2014, Biometrika)。在分布式topology中，我们提出了一种分布式坐标点wise降降算法基于Group ADMM(Elgabli et al., 2020, Journal of Machine Learning Research)，以实现通信效率的Regularized估计。对于提出的算法，我们也证明了其 convergence的理论基础。在实验中，我们使用 sintetic和实际数据进行了数值测试，以评估我们的提出算法。
</details></li>
</ul>
<hr>
<h2 id="Minimax-Optimal-Transfer-Learning-for-Kernel-based-Nonparametric-Regression"><a href="#Minimax-Optimal-Transfer-Learning-for-Kernel-based-Nonparametric-Regression" class="headerlink" title="Minimax Optimal Transfer Learning for Kernel-based Nonparametric Regression"></a>Minimax Optimal Transfer Learning for Kernel-based Nonparametric Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13966">http://arxiv.org/abs/2310.13966</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Wang, Caixing Wang, Xin He, Xingdong Feng</li>
<li>for: This paper focuses on investigating the transfer learning problem within the context of nonparametric regression over a reproducing kernel Hilbert space, with the aim of bridging the gap between practical effectiveness and theoretical guarantees.</li>
<li>methods: The proposed method uses kernel ridge regression for the known transferable source case, and an efficient aggregation algorithm for the unknown case, which can automatically detect and alleviate the effects of negative sources.</li>
<li>results: The paper provides the statistical properties of the desired estimators and establishes the minimax optimal rate, and through extensive numerical experiments on synthetic data and real examples, the effectiveness of the proposed method is validated.<details>
<summary>Abstract</summary>
In recent years, transfer learning has garnered significant attention in the machine learning community. Its ability to leverage knowledge from related studies to improve generalization performance in a target study has made it highly appealing. This paper focuses on investigating the transfer learning problem within the context of nonparametric regression over a reproducing kernel Hilbert space. The aim is to bridge the gap between practical effectiveness and theoretical guarantees. We specifically consider two scenarios: one where the transferable sources are known and another where they are unknown. For the known transferable source case, we propose a two-step kernel-based estimator by solely using kernel ridge regression. For the unknown case, we develop a novel method based on an efficient aggregation algorithm, which can automatically detect and alleviate the effects of negative sources. This paper provides the statistical properties of the desired estimators and establishes the minimax optimal rate. Through extensive numerical experiments on synthetic data and real examples, we validate our theoretical findings and demonstrate the effectiveness of our proposed method.
</details>
<details>
<summary>摘要</summary>
Two scenarios are considered: one where the transferable sources are known, and another where they are unknown. For the known transferable source case, a two-step kernel-based estimator is proposed, solely using kernel ridge regression. For the unknown case, a novel method based on an efficient aggregation algorithm is developed, which can automatically detect and alleviate the effects of negative sources.The statistical properties of the desired estimators are provided, and the minimax optimal rate is established. Extensive numerical experiments on synthetic data and real examples are conducted to validate the theoretical findings and demonstrate the effectiveness of the proposed method.
</details></li>
</ul>
<hr>
<h2 id="Toward-Generative-Data-Augmentation-for-Traffic-Classification"><a href="#Toward-Generative-Data-Augmentation-for-Traffic-Classification" class="headerlink" title="Toward Generative Data Augmentation for Traffic Classification"></a>Toward Generative Data Augmentation for Traffic Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13935">http://arxiv.org/abs/2310.13935</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Wang, Alessandro Finamore, Pietro Michiardi, Massimo Gallo, Dario Rossi</li>
<li>for: 本研究旨在探讨数据增强技术在网络应用中的可行性，特别是在流量分类领域。</li>
<li>methods: 本研究采用了14种手动设计的数据增强策略，应用于MIRAGE19 dataset。</li>
<li>results: 研究结果显示，数据增强可以在流量分类中提供未曾被探讨的优势，同时促进了使用生成模型自动设计数据增强策略的研究课程。<details>
<summary>Abstract</summary>
Data Augmentation (DA)-augmenting training data with synthetic samples-is wildly adopted in Computer Vision (CV) to improve models performance. Conversely, DA has not been yet popularized in networking use cases, including Traffic Classification (TC). In this work, we present a preliminary study of 14 hand-crafted DAs applied on the MIRAGE19 dataset. Our results (i) show that DA can reap benefits previously unexplored in TC and (ii) foster a research agenda on the use of generative models to automate DA design.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Data Augmentation (DA)-augmenting training data with synthetic samples-is wildly adopted in Computer Vision (CV) to improve models performance. Conversely, DA has not been yet popularized in networking use cases, including Traffic Classification (TC). In this work, we present a preliminary study of 14 hand-crafted DAs applied on the MIRAGE19 dataset. Our results (i) show that DA can reap benefits previously unexplored in TC and (ii) foster a research agenda on the use of generative models to automate DA design.">以下是文本的Simplified Chinese翻译：<<SYS>>计算机视觉（CV）中广泛采用数据扩充（DA）技术，卷积神经网络性能。然而，在网络应用场景中，包括流量分类（TC），DA还没有得到普及。在这项工作中，我们对MIRAGE19数据集上手工设计了14种DA，并进行了初步研究。我们的结果表明，DA可以在TC中获得未曾提及的利益，同时也激发了使用生成模型自动化DA设计的研究论坛。
</details></li>
</ul>
<hr>
<h2 id="Diversified-Outlier-Exposure-for-Out-of-Distribution-Detection-via-Informative-Extrapolation"><a href="#Diversified-Outlier-Exposure-for-Out-of-Distribution-Detection-via-Informative-Extrapolation" class="headerlink" title="Diversified Outlier Exposure for Out-of-Distribution Detection via Informative Extrapolation"></a>Diversified Outlier Exposure for Out-of-Distribution Detection via Informative Extrapolation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13923">http://arxiv.org/abs/2310.13923</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zfancy/divoe">https://github.com/zfancy/divoe</a></li>
<li>paper_authors: Jianing Zhu, Geng Yu, Jiangchao Yao, Tongliang Liu, Gang Niu, Masashi Sugiyama, Bo Han<br>for:本研究旨在提高机器学习模型在实际应用中的可靠性，通过进行outsider暴露（OOD）检测。methods:本研究提出了一种新的框架，即多样化外围暴露（DivOE），通过在训练过程中使用多亮示的auxiliary outliers来实现有效的OOD检测。results: DivOE通过在训练过程中生成更多的外围样本，以便在ID和OOD数据之间找到更多的分界点，从而提高OOD检测的准确性。<details>
<summary>Abstract</summary>
Out-of-distribution (OOD) detection is important for deploying reliable machine learning models on real-world applications. Recent advances in outlier exposure have shown promising results on OOD detection via fine-tuning model with informatively sampled auxiliary outliers. However, previous methods assume that the collected outliers can be sufficiently large and representative to cover the boundary between ID and OOD data, which might be impractical and challenging. In this work, we propose a novel framework, namely, Diversified Outlier Exposure (DivOE), for effective OOD detection via informative extrapolation based on the given auxiliary outliers. Specifically, DivOE introduces a new learning objective, which diversifies the auxiliary distribution by explicitly synthesizing more informative outliers for extrapolation during training. It leverages a multi-step optimization method to generate novel outliers beyond the original ones, which is compatible with many variants of outlier exposure. Extensive experiments and analyses have been conducted to characterize and demonstrate the effectiveness of the proposed DivOE. The code is publicly available at: https://github.com/tmlr-group/DivOE.
</details>
<details>
<summary>摘要</summary>
OUT-OF-DISTRIBUTION (OOD) 检测是在实际应用中部署可靠机器学习模型的重要问题。 latest advances in outlier exposure  have shown promising results on OOD detection via fine-tuning model with informatively sampled auxiliary outliers。 However, previous methods assume that the collected outliers can be sufficiently large and representative to cover the boundary between ID and OOD data, which might be impractical and challenging。 In this work, we propose a novel framework, namely, Diversified Outlier Exposure (DivOE), for effective OOD detection via informative extrapolation based on the given auxiliary outliers。 Specifically, DivOE introduces a new learning objective, which diversifies the auxiliary distribution by explicitly synthesizing more informative outliers for extrapolation during training。 It leverages a multi-step optimization method to generate novel outliers beyond the original ones, which is compatible with many variants of outlier exposure。 Extensive experiments and analyses have been conducted to characterize and demonstrate the effectiveness of the proposed DivOE。 The code is publicly available at: https://github.com/tmlr-group/DivOE。Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Equivariant-Map-and-Agent-Geometry-for-Autonomous-Driving-Motion-Prediction"><a href="#Equivariant-Map-and-Agent-Geometry-for-Autonomous-Driving-Motion-Prediction" class="headerlink" title="Equivariant Map and Agent Geometry for Autonomous Driving Motion Prediction"></a>Equivariant Map and Agent Geometry for Autonomous Driving Motion Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13922">http://arxiv.org/abs/2310.13922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuping Wang, Jier Chen</li>
<li>for: 这种研究旨在解决自动驾驶中的深度学习帮助预测运动的问题，具体来说是确保运动的准确性和稳定性。</li>
<li>methods: 这种研究使用了一种名为EqMotion的新型动作预测模型，该模型具有几何变换等价性和人工对话等变换性，从而使得模型在不同的坐标系下仍然可以准确预测动作。此外，该研究还引入了一种具有几何变换等价性的高清地图处理方法，以增强网络的空间理解。</li>
<li>results: 该研究表明，通过使用EqMotion模型和高清地图处理方法，可以实现高精度的动作预测，同时具有轻量级的设计和高效的数据利用。<details>
<summary>Abstract</summary>
In autonomous driving, deep learning enabled motion prediction is a popular topic. A critical gap in traditional motion prediction methodologies lies in ensuring equivariance under Euclidean geometric transformations and maintaining invariant interaction relationships. This research introduces a groundbreaking solution by employing EqMotion, a theoretically geometric equivariant and interaction invariant motion prediction model for particles and humans, plus integrating agent-equivariant high-definition (HD) map features for context aware motion prediction in autonomous driving. The use of EqMotion as backbone marks a significant departure from existing methods by rigorously ensuring motion equivariance and interaction invariance. Equivariance here implies that an output motion must be equally transformed under the same Euclidean transformation as an input motion, while interaction invariance preserves the manner in which agents interact despite transformations. These properties make the network robust to arbitrary Euclidean transformations and contribute to more accurate prediction. In addition, we introduce an equivariant method to process the HD map to enrich the spatial understanding of the network while preserving the overall network equivariance property. By applying these technologies, our model is able to achieve high prediction accuracy while maintain a lightweight design and efficient data utilization.
</details>
<details>
<summary>摘要</summary>
自主驾驶中，深度学习启用的动作预测是一个受欢迎的话题。传统的动作预测方法存在一个重要的缺陷，即保证动作 equivariant 和 interaction invariant。这项研究提出了一种创新的解决方案，通过使用EqMotion，一种 theoretically 几何 equivariant 和 interaction invariant 的动作预测模型，以及 Agent-equivariant HD map 特征进行上下文意识激活的动作预测。使用EqMotion 作为基础marks a significant departure from existing methods， rigorously ensuring motion equivariance and interaction invariance。 equivariance 在这里意味着输入动作下的输出动作必须在同一个几何变换下具有相同的变换，而 interaction invariance 保持了代理人之间的交互方式不变，即使在变换下。这些特性使得网络具有对任意几何变换的 Robustness 和更高的预测精度。此外，我们还引入了一种具有 equivariance 性的方法来处理 HD map，以激活网络的空间理解，而不损失整体网络的 equivariance 性。通过应用这些技术，我们的模型能够实现高精度的预测，同时具有轻量级的设计和高效的数据利用。
</details></li>
</ul>
<hr>
<h2 id="Southern-Ocean-Dynamics-Under-Climate-Change-New-Knowledge-Through-Physics-Guided-Machine-Learning"><a href="#Southern-Ocean-Dynamics-Under-Climate-Change-New-Knowledge-Through-Physics-Guided-Machine-Learning" class="headerlink" title="Southern Ocean Dynamics Under Climate Change: New Knowledge Through Physics-Guided Machine Learning"></a>Southern Ocean Dynamics Under Climate Change: New Knowledge Through Physics-Guided Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13916">http://arxiv.org/abs/2310.13916</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yikwill/THOR-MOM6">https://github.com/yikwill/THOR-MOM6</a></li>
<li>paper_authors: William Yik, Maike Sonnewald, Mariana C. A. Clare, Redouane Lguensat</li>
<li>for: 本研究旨在理解由气候变化引起的南极环流Current的变化，以及这些变化对南极环流Current的影响。</li>
<li>methods: 本研究使用了Tracking global Heating with Ocean Regimes（THOR）方法，将高分辨率气候模型数据分解成不同的物理 régime，并使用神经网络模型预测这些 régime的变化。</li>
<li>results: 研究发现，随着气候变化，南极环流Current在interactions with the Pacific-Antarctic Ridge region发生了 régime shift，其中流速增强，而 bathymetry的 dominant dynamical role weakens。<details>
<summary>Abstract</summary>
Complex ocean systems such as the Antarctic Circumpolar Current play key roles in the climate, and current models predict shifts in their strength and area under climate change. However, the physical processes underlying these changes are not well understood, in part due to the difficulty of characterizing and tracking changes in ocean physics in complex models. To understand changes in the Antarctic Circumpolar Current, we extend the method Tracking global Heating with Ocean Regimes (THOR) to a mesoscale eddy permitting climate model and identify regions of the ocean characterized by similar physics, called dynamical regimes, using readily accessible fields from climate models. To this end, we cluster grid cells into dynamical regimes and train an ensemble of neural networks to predict these regimes and track them under climate change. Finally, we leverage this new knowledge to elucidate the dynamics of regime shifts. Here we illustrate the value of this high-resolution version of THOR, which allows for mesoscale turbulence, with a case study of the Antarctic Circumpolar Current and its interactions with the Pacific-Antarctic Ridge. In this region, THOR specifically reveals a shift in dynamical regime under climate change driven by changes in wind stress and interactions with bathymetry. Using this knowledge to guide further exploration, we find that as the Antarctic Circumpolar Current shifts north under intensifying wind stress, the dominant dynamical role of bathymetry weakens and the flow strengthens.
</details>
<details>
<summary>摘要</summary>
COMPLEX ocean systems, such as the Antarctic Circumpolar Current, play key roles in the climate, and current models predict shifts in their strength and area under climate change. However, the physical processes underlying these changes are not well understood, in part due to the difficulty of characterizing and tracking changes in ocean physics in complex models. To understand changes in the Antarctic Circumpolar Current, we extend the method Tracking global Heating with Ocean Regimes (THOR) to a mesoscale eddy permitting climate model and identify regions of the ocean characterized by similar physics, called dynamical regimes, using readily accessible fields from climate models. To this end, we cluster grid cells into dynamical regimes and train an ensemble of neural networks to predict these regimes and track them under climate change. Finally, we leverage this new knowledge to elucidate the dynamics of regime shifts. Here we illustrate the value of this high-resolution version of THOR, which allows for mesoscale turbulence, with a case study of the Antarctic Circumpolar Current and its interactions with the Pacific-Antarctic Ridge. In this region, THOR specifically reveals a shift in dynamical regime under climate change driven by changes in wind stress and interactions with bathymetry. Using this knowledge to guide further exploration, we find that as the Antarctic Circumpolar Current shifts north under intensifying wind stress, the dominant dynamical role of bathymetry weakens and the flow strengthens.
</details></li>
</ul>
<hr>
<h2 id="Pre-Training-on-Large-Scale-Generated-Docking-Conformations-with-HelixDock-to-Unlock-the-Potential-of-Protein-ligand-Structure-Prediction-Models"><a href="#Pre-Training-on-Large-Scale-Generated-Docking-Conformations-with-HelixDock-to-Unlock-the-Potential-of-Protein-ligand-Structure-Prediction-Models" class="headerlink" title="Pre-Training on Large-Scale Generated Docking Conformations with HelixDock to Unlock the Potential of Protein-ligand Structure Prediction Models"></a>Pre-Training on Large-Scale Generated Docking Conformations with HelixDock to Unlock the Potential of Protein-ligand Structure Prediction Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13913">http://arxiv.org/abs/2310.13913</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lihang Liu, Donglong He, Xianbin Ye, Shanzhuo Zhang, Xiaonan Zhang, Jingbo Zhou, Jun Li, Hua Chai, Fan Wang, Jingzhou He, Liang Zheng, Yonghui Li, Xiaomin Fang</li>
<li>for: 这篇论文旨在提高分子对抗位置预测的精确性，以推动人工智能驱动的药物探索。</li>
<li>methods: 这篇论文使用了深度学习技术来强化分子对抗位置预测，并且使用了大量的蛋白质和小分子组合数据进行预训。</li>
<li>results: 相比传统物理学基于的基底方法和深度学习基于的基底方法，这篇论文的HelixDock方法在复杂的测试集上表现出了superiority，尤其是在蛋白质和小分子之间的对抗位置预测方面。<details>
<summary>Abstract</summary>
Molecular docking, a pivotal computational tool for drug discovery, predicts the binding interactions between small molecules (ligands) and target proteins (receptors). Conventional physics-based docking tools, though widely used, face limitations in precision due to restricted conformational sampling and imprecise scoring functions. Recent endeavors have employed deep learning techniques to enhance docking accuracy, but their generalization remains a concern due to limited training data. Leveraging the success of extensive and diverse data in other domains, we introduce HelixDock, a novel approach for site-specific molecular docking. Hundreds of millions of binding poses are generated by traditional docking tools, encompassing diverse protein targets and small molecules. Our deep learning-based docking model, a SE(3)-equivariant network, is pre-trained with this large-scale dataset and then fine-tuned with a small number of precise receptor-ligand complex structures. Comparative analyses against physics-based and deep learning-based baseline methods highlight HelixDock's superiority, especially on challenging test sets. Our study elucidates the scaling laws of the pre-trained molecular docking models, showcasing consistent improvements with increased model parameters and pre-train data quantities. Harnessing the power of extensive and diverse generated data holds promise for advancing AI-driven drug discovery.
</details>
<details>
<summary>摘要</summary>
分子停靠，一种重要的计算工具，预测小分子（抗体）与目标蛋白（受体）之间的绑定交互。传统的物理学基于的停靠工具，尽管广泛使用，但受限于精确性，因为它们只能进行有限的配置检查和不准确的评分函数。现在的努力是使用深度学习技术来提高停靠精度，但它们的泛化仍然是一个问题，因为它们只有有限的训练数据。我们在其他领域中的丰富和多样化数据的基础上引入了 HelixDock，一种新的方法。我们使用传统的停靠工具生成了数百万个绑定位置，涵盖了多种蛋白目标和小分子。我们的深度学习基于的停靠模型，一个SE(3)相似的网络，通过大规模数据集预训练，然后精度地调整了一个小数量的准确抗体-小分子复合结构。与物理学基于和深度学习基于的基线方法进行比较分析，HelixDock在具有挑战性的测试集上表现出优异性，特别是在难以预测的情况下。我们的研究描述了预训练分子停靠模型的涨幅法律，展示了随着模型参数和预训练数据量的增加，模型的性能得到了一致提高。通过利用广泛生成的数据来提高人工智能驱动的药物探索，我们希望能够推动这一领域的发展。
</details></li>
</ul>
<hr>
<h2 id="Towards-Hyperparameter-Agnostic-DNN-Training-via-Dynamical-System-Insights"><a href="#Towards-Hyperparameter-Agnostic-DNN-Training-via-Dynamical-System-Insights" class="headerlink" title="Towards Hyperparameter-Agnostic DNN Training via Dynamical System Insights"></a>Towards Hyperparameter-Agnostic DNN Training via Dynamical System Insights</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13901">http://arxiv.org/abs/2310.13901</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carmel Fiscko, Aayushya Agarwal, Yihan Ruan, Soummya Kar, Larry Pileggi, Bruno Sinopoli<br>for:ECCO-DNN is designed to optimize deep neural network training.methods:ECCO-DNN uses a stochastic first-order optimization method that models the optimization variable trajectory as a dynamical system and adaptively selects step sizes based on the trajectory’s shape.results:ECCO-DNN achieves comparable performance to state-of-the-art optimizers including ADAM, SGD, RMSProp, and AdaGrad, and its single hyperparameter can be changed by three orders of magnitude without affecting the trained models’ accuracies. Additionally, ECCO-DNN is insensitive to hyperparameter variations and reduces the data and computation needed for hyperparameter tuning, making it advantageous for rapid prototyping and for applications with new datasets.<details>
<summary>Abstract</summary>
We present a stochastic first-order optimization method specialized for deep neural networks (DNNs), ECCO-DNN. This method models the optimization variable trajectory as a dynamical system and develops a discretization algorithm that adaptively selects step sizes based on the trajectory's shape. This provides two key insights: designing the dynamical system for fast continuous-time convergence and developing a time-stepping algorithm to adaptively select step sizes based on principles of numerical integration and neural network structure. The result is an optimizer with performance that is insensitive to hyperparameter variations and that achieves comparable performance to state-of-the-art optimizers including ADAM, SGD, RMSProp, and AdaGrad. We demonstrate this in training DNN models and datasets, including CIFAR-10 and CIFAR-100 using ECCO-DNN and find that ECCO-DNN's single hyperparameter can be changed by three orders of magnitude without affecting the trained models' accuracies. ECCO-DNN's insensitivity reduces the data and computation needed for hyperparameter tuning, making it advantageous for rapid prototyping and for applications with new datasets. To validate the efficacy of our proposed optimizer, we train an LSTM architecture on a household power consumption dataset with ECCO-DNN and achieve an optimal mean-square-error without tuning hyperparameters.
</details>
<details>
<summary>摘要</summary>
我们提出了一种随机首频优化方法特化于深度神经网络（DNN），即ECCO-DNN。该方法将优化变量轨迹模型为动力系统，并开发了一种适应步长选择算法，以获得两个关键发现：设计动力系统以实现快速连续时间减少，并开发一种时间步骤算法以自适应选择步长，基于数值积分和神经网络结构。这些设计决策使ECCO-DNN的优化器性能免疫参数变化的影响，并与现有的优化器，包括ADAM、SGD、RMSProp和AdaGrad，具有相同的性能。我们在训练DNN模型和数据集中使用ECCO-DNN，包括CIFAR-10和CIFAR-100，并发现ECCO-DNN的单个超参数可以通过三个级别的变化而不影响训练模型的准确性。ECCO-DNN的不敏感性降低了数据和计算所需的 hyperparameter 调试，使其在快速原型和新数据集应用中更加优势。为证明我们提出的优化器的有效性，我们在一个家用电力消耗数据集上使用ECCO-DNN训练LSTM架构，并实现了最佳平均方差值，无需调整超参数。
</details></li>
</ul>
<hr>
<h2 id="Masked-Hard-Attention-Transformers-and-Boolean-RASP-Recognize-Exactly-the-Star-Free-Languages"><a href="#Masked-Hard-Attention-Transformers-and-Boolean-RASP-Recognize-Exactly-the-Star-Free-Languages" class="headerlink" title="Masked Hard-Attention Transformers and Boolean RASP Recognize Exactly the Star-Free Languages"></a>Masked Hard-Attention Transformers and Boolean RASP Recognize Exactly the Star-Free Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13897">http://arxiv.org/abs/2310.13897</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dana Angluin, David Chiang, Andy Yang</li>
<li>for: 这个论文研究了 transformer Encoder 的可能性和逻辑限制。</li>
<li>methods: 这个论文使用了硬 attention 和严格未来masking 方法，并证明了这种网络可以认izers star-free 语言。添加位域 embedding 可以扩展认izers 到其他已知类别。</li>
<li>results: 这个论文证明了 transformer 网络可以认izers star-free 语言，并与 first-order logic、 temporal logic 和 algebraic automata theory 有关。<details>
<summary>Abstract</summary>
We consider transformer encoders with hard attention (in which all attention is focused on exactly one position) and strict future masking (in which each position only attends to positions strictly to its left), and prove that the class of languages recognized by these networks is exactly the star-free languages. Adding position embeddings increases the class of recognized languages to other well-studied classes. A key technique in these proofs is Boolean RASP, a variant of RASP that is restricted to Boolean values. Via the star-free languages, we relate transformers to first-order logic, temporal logic, and algebraic automata theory.
</details>
<details>
<summary>摘要</summary>
我们考虑使用转换器Encoder，具有固定注意力（所有注意力都集中在一个位置）和严格未来掩码（每个位置只能关注左侧的位置），并证明这些网络可以认可的语言类型是星号自由语言。添加位域嵌入可以将认可的语言类型扩展到其他已有的类型。我们使用布尔RASP，一种受限的RASP变体，进行关键技术。通过星号自由语言，我们将转换器相关联到首领逻辑、时间逻辑和代数自动机理论。
</details></li>
</ul>
<hr>
<h2 id="Specify-Robust-Causal-Representation-from-Mixed-Observations"><a href="#Specify-Robust-Causal-Representation-from-Mixed-Observations" class="headerlink" title="Specify Robust Causal Representation from Mixed Observations"></a>Specify Robust Causal Representation from Mixed Observations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13892">http://arxiv.org/abs/2310.13892</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ymy4323460/cari">https://github.com/ymy4323460/cari</a></li>
<li>paper_authors: Mengyue Yang, Xinyu Cai, Furui Liu, Weinan Zhang, Jun Wang</li>
<li>for: 本研究旨在学习从观察数据中学习一种低维度、紧凑的表示，以提高预测模型的稳定性和泛化性。</li>
<li>methods: 本研究使用了 causal 表示学习方法，通过在学习过程中添加 mutual information 度量来规范学习。</li>
<li>results: 研究表明，使用 causal 表示学习方法可以提高模型的Robustness 和泛化性，并且在骚扰攻击和分布Shift 下表现更好于基eline。I hope that helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
Learning representations purely from observations concerns the problem of learning a low-dimensional, compact representation which is beneficial to prediction models. Under the hypothesis that the intrinsic latent factors follow some casual generative models, we argue that by learning a causal representation, which is the minimal sufficient causes of the whole system, we can improve the robustness and generalization performance of machine learning models. In this paper, we develop a learning method to learn such representation from observational data by regularizing the learning procedure with mutual information measures, according to the hypothetical factored causal graph. We theoretically and empirically show that the models trained with the learned causal representations are more robust under adversarial attacks and distribution shifts compared with baselines. The supplementary materials are available at https://github.com/ymy $4323460 / \mathrm{CaRI} /$.
</details>
<details>
<summary>摘要</summary>
学习 purely from observations 的表示 Concerns the problem of learning a low-dimensional, compact representation, which is beneficial to prediction models. Under the assumption that the intrinsic latent factors follow some causal generative models, we argue that by learning a causal representation, which is the minimal sufficient causes of the whole system, we can improve the robustness and generalization performance of machine learning models. In this paper, we develop a learning method to learn such representation from observational data by regularizing the learning procedure with mutual information measures, according to the hypothetical factored causal graph. We theoretically and empirically show that the models trained with the learned causal representations are more robust under adversarial attacks and distribution shifts compared with baselines. 附加资料可以在 https://github.com/ymy $4323460 / \mathrm{CaRI} /$ 中找到。
</details></li>
</ul>
<hr>
<h2 id="Towards-a-General-Framework-for-Continual-Learning-with-Pre-training"><a href="#Towards-a-General-Framework-for-Continual-Learning-with-Pre-training" class="headerlink" title="Towards a General Framework for Continual Learning with Pre-training"></a>Towards a General Framework for Continual Learning with Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13888">http://arxiv.org/abs/2310.13888</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thu-ml/hide-prompt">https://github.com/thu-ml/hide-prompt</a></li>
<li>paper_authors: Liyuan Wang, Jingyi Xie, Xingxing Zhang, Hang Su, Jun Zhu</li>
<li>for: 本研究探讨了一种通用框架，用于Sequential continual learning tasks，通过预训练来实现人工智能系统适应真实世界动态变化。</li>
<li>methods: 我们在理论上将目标函数 decomposed into three hierarchical components，包括 within-task prediction、task-identity inference 和 task-adaptive prediction。然后，我们提出了一种新的方法，使用 parameter-efficient fine-tuning (PEFT) 技术和 representation statistics 来显著提高这些组件。</li>
<li>results: 我们在下游 continual learning 中观察到了我们的方法的优势和通用性，并进一步探讨了 PEFT 技术在上游 continual learning 中的应用可能性。此外，我们还讨论了该框架与 neuroscience 最新的进展之间的生物基础。<details>
<summary>Abstract</summary>
In this work, we present a general framework for continual learning of sequentially arrived tasks with the use of pre-training, which has emerged as a promising direction for artificial intelligence systems to accommodate real-world dynamics. From a theoretical perspective, we decompose its objective into three hierarchical components, including within-task prediction, task-identity inference, and task-adaptive prediction. Then we propose an innovative approach to explicitly optimize these components with parameter-efficient fine-tuning (PEFT) techniques and representation statistics. We empirically demonstrate the superiority and generality of our approach in downstream continual learning, and further explore the applicability of PEFT techniques in upstream continual learning. We also discuss the biological basis of the proposed framework with recent advances in neuroscience.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们提出了一种总体框架，用于Sequential continual learning，利用预训练，这种方向已成为人工智能系统来应对实际世界动态的一种有前途的方法。从理论上来看，我们将目标函数分解成三个层次结构，包括内部任务预测、任务标识推理和任务适应预测。然后，我们提议一种新的方法，使用参数效率的细致调整（PEFT）技术和表示统计来显著地优化这些组成部分。我们在下游 continual learning 中进行了实验，证明了我们的方法的优越性和通用性。此外，我们还探讨了这个框架的生物基础，与 neuroscience 最新的进展有关。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Transport-based-Nonlinear-Filtering-in-High-dimensional-Settings"><a href="#Optimal-Transport-based-Nonlinear-Filtering-in-High-dimensional-Settings" class="headerlink" title="Optimal Transport-based Nonlinear Filtering in High-dimensional Settings"></a>Optimal Transport-based Nonlinear Filtering in High-dimensional Settings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13886">http://arxiv.org/abs/2310.13886</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Al-Jarrah, Niyizhen Jin, Bamdad Hosseini, Amirhossein Taghvaei</li>
<li>for: 本文解决非线性滤波问题，即计算随机动力系统状态条件分布给历史噪声部分观测的问题。</li>
<li>methods: 我们提出的方法基于非线性滤波的最优运输解释，导致一种基于模拟和无概率算法的 simulation-based 和 likelihood-free 算法，可以估计当前状态分布到下一步时间Step的 Brenier 最优运输Map。</li>
<li>results: 我们的方法比 SIR 滤波和ensemble Kalman filter 表现出更高的样本效率、高维度可描述性和能够捕捉复杂多模分布的能力。<details>
<summary>Abstract</summary>
This paper addresses the problem of nonlinear filtering, i.e., computing the conditional distribution of the state of a stochastic dynamical system given a history of noisy partial observations. The primary focus is on scenarios involving degenerate likelihoods or high-dimensional states, where traditional sequential importance resampling (SIR) particle filters face the weight degeneracy issue. Our proposed method builds on an optimal transport interpretation of nonlinear filtering, leading to a simulation-based and likelihood-free algorithm that estimates the Brenier optimal transport map from the current distribution of the state to the distribution at the next time step. Our formulation allows us to harness the approximation power of neural networks to model complex and multi-modal distributions and employ stochastic optimization algorithms to enhance scalability. Extensive numerical experiments are presented that compare our method to the SIR particle filter and the ensemble Kalman filter, demonstrating the superior performance of our method in terms of sample efficiency, high-dimensional scalability, and the ability to capture complex and multi-modal distributions.
</details>
<details>
<summary>摘要</summary>
Our proposed method is based on an optimal transport interpretation of nonlinear filtering, which leads to a simulation-based and likelihood-free algorithm that estimates the Brenier optimal transport map from the current distribution of the state to the distribution at the next time step. By using neural networks to model complex and multi-modal distributions and stochastic optimization algorithms to enhance scalability, our formulation is able to handle challenging scenarios with ease.Extensive numerical experiments are presented that compare our method to the SIR particle filter and the ensemble Kalman filter. The results demonstrate the superior performance of our method in terms of sample efficiency, high-dimensional scalability, and the ability to capture complex and multi-modal distributions.
</details></li>
</ul>
<hr>
<h2 id="Fast-Approximation-of-Similarity-Graphs-with-Kernel-Density-Estimation"><a href="#Fast-Approximation-of-Similarity-Graphs-with-Kernel-Density-Estimation" class="headerlink" title="Fast Approximation of Similarity Graphs with Kernel Density Estimation"></a>Fast Approximation of Similarity Graphs with Kernel Density Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13870">http://arxiv.org/abs/2310.13870</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pmacg/kde-similarity-graph">https://github.com/pmacg/kde-similarity-graph</a></li>
<li>paper_authors: Peter Macgregor, He Sun</li>
<li>for: 构建一个稀疏的相似图，以便进行现代归一化算法的第一步。</li>
<li>methods: 基于kernel density estimation问题，提出了一种新的算法框架，可以快速构建稀疏的相似图，同时保持归一化结果的结构。</li>
<li>results: 与scikit-learn和FAISS库的实现相比，我们的方法在多种数据集上显著超越了它们。<details>
<summary>Abstract</summary>
Constructing a similarity graph from a set $X$ of data points in $\mathbb{R}^d$ is the first step of many modern clustering algorithms. However, typical constructions of a similarity graph have high time complexity, and a quadratic space dependency with respect to $|X|$. We address this limitation and present a new algorithmic framework that constructs a sparse approximation of the fully connected similarity graph while preserving its cluster structure. Our presented algorithm is based on the kernel density estimation problem, and is applicable for arbitrary kernel functions. We compare our designed algorithm with the well-known implementations from the scikit-learn library and the FAISS library, and find that our method significantly outperforms the implementation from both libraries on a variety of datasets.
</details>
<details>
<summary>摘要</summary>
现在的许多现代聚类算法的第一步是从一个集合 $X$ 中的数据点集构建一个相似Graph。然而，通常情况下，构建相似图的方法具有高时间复杂度和对 $|X|$ 的平方空间依赖。我们解决这个限制，并提出了一个新的算法框架，可以构建稀疏的相似图，保持聚类结构。我们的提出的算法基于kernel density估计问题，适用于任意的kernel函数。我们与scikit-learn库和FAISS库中的常用实现进行比较，发现我们的方法在多种数据集上明显超过了这两个库的实现。
</details></li>
</ul>
<hr>
<h2 id="Distributionally-Robust-Optimization-with-Bias-and-Variance-Reduction"><a href="#Distributionally-Robust-Optimization-with-Bias-and-Variance-Reduction" class="headerlink" title="Distributionally Robust Optimization with Bias and Variance Reduction"></a>Distributionally Robust Optimization with Bias and Variance Reduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13863">http://arxiv.org/abs/2310.13863</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Ronak Mehta, Vincent Roulet, Krishna Pillutla, Zaid Harchaoui</li>
<li>for: 该研究 targets the distributionally robust optimization (DRO) problem with spectral risk-based uncertainty set and $f$-divergence penalty, which includes common risk-sensitive learning objectives such as regularized condition value-at-risk (CVaR) and average top-$k$ loss.</li>
<li>methods: 该研究提出了一种名为 Prospect 的杂相 gradient-based algorithm，该算法只需要调整一个学习率参数，并且可以 linear convergence for smooth regularized losses。这与之前的算法不同，这些算法可能需要调整多个参数，或者因为偏向的梯度估计或不够的迁移而失败。</li>
<li>results: 该研究通过实验表明，Prospect 可以比基eline 2-3倍快速 converges on distribution shift and fairness benchmarks spanning tabular, vision, and language domains。<details>
<summary>Abstract</summary>
We consider the distributionally robust optimization (DRO) problem with spectral risk-based uncertainty set and $f$-divergence penalty. This formulation includes common risk-sensitive learning objectives such as regularized condition value-at-risk (CVaR) and average top-$k$ loss. We present Prospect, a stochastic gradient-based algorithm that only requires tuning a single learning rate hyperparameter, and prove that it enjoys linear convergence for smooth regularized losses. This contrasts with previous algorithms that either require tuning multiple hyperparameters or potentially fail to converge due to biased gradient estimates or inadequate regularization. Empirically, we show that Prospect can converge 2-3$\times$ faster than baselines such as stochastic gradient and stochastic saddle-point methods on distribution shift and fairness benchmarks spanning tabular, vision, and language domains.
</details>
<details>
<summary>摘要</summary>
我团队考虑了分布robust优化（DRO）问题，使用spectral risk-based uncertainty set和$f$- divergence penalty。这种形式包括常见的风险敏感学习目标，如常量值-at-risk（CVaR）和平均top-$k$ loss。我们提出了Prospect算法，它仅需要调整一个学习率超参数，并证明它在抽象化的REGularized loss函数下具有线性减少的性质。这与之前的算法不同，这些算法可能需要调整多个超参数，或者因为偏向的梯度估计或不足的正则化而无法 converges。我们在实验中证明，Prospect可以比基eline算法快速 converge 2-3倍，包括在分布shift和公平性 benchmark上。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/21/cs.LG_2023_10_21/" data-id="clp88dbz700tiob882z3x1law" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/21/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/20/">20</a><a class="page-number" href="/page/21/">21</a><span class="page-number current">22</span><a class="page-number" href="/page/23/">23</a><a class="page-number" href="/page/24/">24</a><span class="space">&hellip;</span><a class="page-number" href="/page/97/">97</a><a class="extend next" rel="next" href="/page/23/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">66</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">81</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">140</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
