
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/9/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-eess.AS_2023_07_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/24/eess.AS_2023_07_24/" class="article-date">
  <time datetime="2023-07-23T16:00:00.000Z" itemprop="datePublished">2023-07-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/24/eess.AS_2023_07_24/">eess.AS - 2023-07-24 22:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Adaptation-of-Whisper-models-to-child-speech-recognition"><a href="#Adaptation-of-Whisper-models-to-child-speech-recognition" class="headerlink" title="Adaptation of Whisper models to child speech recognition"></a>Adaptation of Whisper models to child speech recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13008">http://arxiv.org/abs/2307.13008</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/c3imaging/whisper_child_speech">https://github.com/c3imaging/whisper_child_speech</a></li>
<li>paper_authors: Rishabh Jain, Andrei Barcovschi, Mariam Yiwere, Peter Corcoran, Horia Cucu</li>
<li>for: 改善儿童语音识别系统的表现</li>
<li>methods: 使用 Whisper 模型的自适应和 wav2vec2 模型的自适应训练</li>
<li>results: 比非自适应 Whisper 模型有更好的 ASR 表现，并且 utilizing self-supervised wav2vec2 模型的自适应训练可以超越 Whisper 模型的自适应训练。<details>
<summary>Abstract</summary>
Automatic Speech Recognition (ASR) systems often struggle with transcribing child speech due to the lack of large child speech datasets required to accurately train child-friendly ASR models. However, there are huge amounts of annotated adult speech datasets which were used to create multilingual ASR models, such as Whisper. Our work aims to explore whether such models can be adapted to child speech to improve ASR for children. In addition, we compare Whisper child-adaptations with finetuned self-supervised models, such as wav2vec2. We demonstrate that finetuning Whisper on child speech yields significant improvements in ASR performance on child speech, compared to non finetuned Whisper models. Additionally, utilizing self-supervised Wav2vec2 models that have been finetuned on child speech outperforms Whisper finetuning.
</details>
<details>
<summary>摘要</summary>
自动语音识别（ASR）系统经常在儿童语音转译中遇到困难，因为缺乏大量儿童语音数据来准确训练适合儿童的 ASR 模型。然而，有巨量的注释 adult speech 数据，用于创建多种语言的 ASR 模型，如 Whisper。我们的工作目的是探索这些模型是否可以适应儿童语音，以提高 ASR 性能。此外，我们还比较了 Whisper 儿童化与自动学习 wav2vec2 模型，并证明了后者在儿童语音上的表现较好。Here's the translation in Traditional Chinese:自动语音识别（ASR）系统经常在儿童语音转换中遇到困难，因为缺乏大量儿童语音数据来精确训练适合儿童的 ASR 模型。然而，有巨量的注释 adult speech 数据，用于创建多种语言的 ASR 模型，如 Whisper。我们的工作目的是探索这些模型是否可以适应儿童语音，以提高 ASR 性能。此外，我们还比较了 Whisper 儿童化与自动学习 wav2vec2 模型，并证明了后者在儿童语音上的表现较好。
</details></li>
</ul>
<hr>
<h2 id="Performance-Comparison-Between-VoLTE-and-non-VoLTE-Voice-Calls-During-Mobility-in-Commercial-Deployment-A-Drive-Test-Based-Analysis"><a href="#Performance-Comparison-Between-VoLTE-and-non-VoLTE-Voice-Calls-During-Mobility-in-Commercial-Deployment-A-Drive-Test-Based-Analysis" class="headerlink" title="Performance Comparison Between VoLTE and non-VoLTE Voice Calls During Mobility in Commercial Deployment: A Drive Test-Based Analysis"></a>Performance Comparison Between VoLTE and non-VoLTE Voice Calls During Mobility in Commercial Deployment: A Drive Test-Based Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12397">http://arxiv.org/abs/2307.12397</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rashed Hasan Ratul, Muhammad Iqbal, Jen-Yi Pan, Mohammad Mahadi Al Deen, Mohammad Tawhid Kawser, Mohammad Masum Billah</li>
<li>for: 这个研究的目的是为了提高无线通信网络的性能，特别是在于对于无线电话服务的配送。</li>
<li>methods: 这个研究使用了 XCAL 驱动测试工具来收集真实时间网络参数资料，以进行 VoLTE 和非 VoLTE 语音号的比较。</li>
<li>results: 研究发现 VoLTE 可以提供更快的设定时间和更好的电池储存性，相比非 VoLTE。具体来说，VoLTE 可以储存约 60.76% 的能源在服务请求之前，和约 38.97% 的能源在服务请求之后。此外，VoLTE 到 VoLTE 呼叫的设定时间比非 VoLTE 基于 LTE 到 LTE 呼叫的设定时间快了约 72.6%。<details>
<summary>Abstract</summary>
The optimization of network performance is vital for the delivery of services using standard cellular technologies for mobile communications. Call setup delay and User Equipment (UE) battery savings significantly influence network performance. Improving these factors is vital for ensuring optimal service delivery. In comparison to traditional circuit-switched voice calls, VoLTE (Voice over LTE) technology offers faster call setup durations and better battery-saving performance. To validate these claims, a drive test was carried out using the XCAL drive test tool to collect real-time network parameter details in VoLTE and non-VoLTE voice calls. The findings highlight the analysis of real-time network characteristics, such as the call setup delay calculation, battery-saving performance, and DRX mechanism. The study contributes to the understanding of network optimization strategies and provides insights for enhancing the quality of service (QoS) in mobile communication networks. Examining VoLTE and non-VoLTE operations, this research highlights the substantial energy savings obtained by VoLTE. Specifically, VoLTE saves approximately 60.76% of energy before the Service Request and approximately 38.97% of energy after the Service Request. Moreover, VoLTE to VoLTE calls have a 72.6% faster call setup delay than non-VoLTE-based LTE to LTE calls, because of fewer signaling messages required. Furthermore, as compared to non-VoLTE to non-VoLTE calls, VoLTE to non-VoLTE calls offer an 18.6% faster call setup delay. These results showcase the performance advantages of VoLTE and reinforce its potential for offering better services in wireless communication networks.
</details>
<details>
<summary>摘要</summary>
网络性能优化对于基于标准移动通信技术提供服务的实现非常重要。启动延迟和用户设备（UE）电池寿命对网络性能有着重要影响。提高这些因素对于确保优质服务提供是非常重要。相比传统的固定呼叫，VoLTE（语音 sobre LTE）技术可以提供更快的启动延迟和更好的电池寿命性能。为验证这些声明，我们使用XCAL驱动测试工具进行了实时网络参数详细情况的收集，并对VoLTE和非VoLTE语音呼叫进行了比较。研究发现，VoLTE比非VoLTE实现更快的启动延迟和更好的电池寿命性能。具体来说，VoLTE在呼叫设置之前可以节省约60.76%的能量，并在呼叫设置之后可以节省约38.97%的能量。此外，VoLTE到VoLTE呼叫的启动延迟比非VoLTE-based LTE到LTE呼叫更快，因为需要 fewer signaling messages。此外，VoLTE到非VoLTE呼叫的启动延迟比非VoLTE到非VoLTE呼叫更快。这些结果表明VoLTE的性能优势，并证明它在无线通信网络中可以提供更好的服务。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/24/eess.AS_2023_07_24/" data-id="cllsj9wzo006auv88h963cdpd" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/24/eess.IV_2023_07_24/" class="article-date">
  <time datetime="2023-07-23T16:00:00.000Z" itemprop="datePublished">2023-07-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/24/eess.IV_2023_07_24/">eess.IV - 2023-07-24 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Conditional-Residual-Coding-A-Remedy-for-Bottleneck-Problems-in-Conditional-Inter-Frame-Coding"><a href="#Conditional-Residual-Coding-A-Remedy-for-Bottleneck-Problems-in-Conditional-Inter-Frame-Coding" class="headerlink" title="Conditional Residual Coding: A Remedy for Bottleneck Problems in Conditional Inter Frame Coding"></a>Conditional Residual Coding: A Remedy for Bottleneck Problems in Conditional Inter Frame Coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12864">http://arxiv.org/abs/2307.12864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabian Brand, Jürgen Seiler, André Kaup</li>
<li>for: 这篇论文是关于新型的视频编码方法，即基于神经网络的压缩方法。</li>
<li>methods: 这篇论文使用的方法是基于神经网络的 conditional coding，它可以在理论上比传统的剩余编码（如 HEVC 或 VVC 等标准）更好。</li>
<li>results: 但是， conditional coding 可能会受到预测信号处理的数据瓶颈，即不能将所有预测信号中的信息传递到重建信号中，从而降低编码器性能。这篇论文提出了 conditional residual coding 概念，它是基于 conditional coding 的信息学性质，可以减少瓶颈的影响，同时保持 conditional coding 的理论性能。<details>
<summary>Abstract</summary>
Conditional coding is a new video coding paradigm enabled by neural-network-based compression. It can be shown that conditional coding is in theory better than the traditional residual coding, which is widely used in video compression standards like HEVC or VVC. However, on closer inspection, it becomes clear that conditional coders can suffer from information bottlenecks in the prediction path, i.e., that due to the data processing inequality not all information from the prediction signal can be passed to the reconstructed signal, thereby impairing the coder performance. In this paper we propose the conditional residual coding concept, which we derive from information theoretical properties of the conditional coder. This coder significantly reduces the influence of bottlenecks, while maintaining the theoretical performance of the conditional coder. We provide a theoretical analysis of the coding paradigm and demonstrate the performance of the conditional residual coder in a practical example. We show that conditional residual coders alleviate the disadvantages of conditional coders while being able to maintain their advantages over residual coders. In the spectrum of residual and conditional coding, we can therefore consider them as ``the best from both worlds''.
</details>
<details>
<summary>摘要</summary>
conditional coding是一种新的视频编码方法，它基于神经网络压缩而实现。理论上来说，conditional coding比传统的差分编码（如HEVC或VVC中的差分编码）更好。然而，在更加仔细的分析中，我们发现conditional coders可能会在预测路径中遇到信息瓶颈，即由数据处理不均衡而导致的信息不能够从预测信号传递到重建信号中，从而影响编码器性能。在这篇论文中，我们提出了conditional residual coding概念，它基于condition coding的信息理论性质。这种编码器可以减少预测路径中的瓶颈影响，同时保持condition coding的理论性能。我们对 coding  парадиг进行了理论分析，并在实践中证明了conditional residual coder的性能。我们发现，conditional residual coders可以消除condition coding的缺点，同时保持它们在差分编码中的优势。因此，在差分和condition coding之间的spectrum中，我们可以视conditional residual coders为“最佳之选”。
</details></li>
</ul>
<hr>
<h2 id="Spatiotemporal-Modeling-Encounters-3D-Medical-Image-Analysis-Slice-Shift-UNet-with-Multi-View-Fusion"><a href="#Spatiotemporal-Modeling-Encounters-3D-Medical-Image-Analysis-Slice-Shift-UNet-with-Multi-View-Fusion" class="headerlink" title="Spatiotemporal Modeling Encounters 3D Medical Image Analysis: Slice-Shift UNet with Multi-View Fusion"></a>Spatiotemporal Modeling Encounters 3D Medical Image Analysis: Slice-Shift UNet with Multi-View Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12853">http://arxiv.org/abs/2307.12853</a></li>
<li>repo_url: None</li>
<li>paper_authors: C. I. Ugwu, S. Casarin, O. Lanz</li>
<li>for:  This paper aims to develop a new 2D-based model for 3D image analysis, which can extract three-dimensional features at the complexity of 2D CNNs.</li>
<li>methods:  The proposed model, called Slice SHift UNet (SSH-UNet), uses multi-view features that are collaboratively learned by performing 2D convolutions along the three orthogonal planes of a volume, and imposing a weights-sharing mechanism. The third dimension is reincorporated by shifting a portion of the feature maps along the slices’ axis.</li>
<li>results:  The effectiveness of the SSH-UNet model is validated in two datasets, Multi-Modality Abdominal Multi-Organ Segmentation (AMOS) and Multi-Atlas Labeling Beyond the Cranial Vault (BTCV), showing that it is more efficient while on par in performance with state-of-the-art architectures.<details>
<summary>Abstract</summary>
As a fundamental part of computational healthcare, Computer Tomography (CT) and Magnetic Resonance Imaging (MRI) provide volumetric data, making the development of algorithms for 3D image analysis a necessity. Despite being computationally cheap, 2D Convolutional Neural Networks can only extract spatial information. In contrast, 3D CNNs can extract three-dimensional features, but they have higher computational costs and latency, which is a limitation for clinical practice that requires fast and efficient models. Inspired by the field of video action recognition we propose a new 2D-based model dubbed Slice SHift UNet (SSH-UNet) which encodes three-dimensional features at 2D CNN's complexity. More precisely multi-view features are collaboratively learned by performing 2D convolutions along the three orthogonal planes of a volume and imposing a weights-sharing mechanism. The third dimension, which is neglected by the 2D convolution, is reincorporated by shifting a portion of the feature maps along the slices' axis. The effectiveness of our approach is validated in Multi-Modality Abdominal Multi-Organ Segmentation (AMOS) and Multi-Atlas Labeling Beyond the Cranial Vault (BTCV) datasets, showing that SSH-UNet is more efficient while on par in performance with state-of-the-art architectures.
</details>
<details>
<summary>摘要</summary>
computational healthcare的基础部分是计算机成像（CT）和核磁共振成像（MRI），它们提供了三维数据，因此开发三维图像分析算法是必要的。然而，两维卷积神经网络（2D CNN）只能提取空间信息，而三维 CNN 可以提取三维特征，但它们具有更高的计算成本和延迟，这限制了临床实践中需要快速和高效的模型。 Drawing inspiration from the field of video action recognition, we propose a new 2D-based model called Slice SHift UNet (SSH-UNet) that encodes three-dimensional features at the complexity of 2D CNNs. Specifically, we perform 2D convolutions along the three orthogonal planes of a volume and share weights to collaboratively learn multi-view features. The third dimension, which is neglected by the 2D convolution, is reincorporated by shifting a portion of the feature maps along the slices' axis. Our approach is validated on the Multi-Modality Abdominal Multi-Organ Segmentation (AMOS) and Multi-Atlas Labeling Beyond the Cranial Vault (BTCV) datasets, showing that SSH-UNet is more efficient while on par in performance with state-of-the-art architectures.
</details></li>
</ul>
<hr>
<h2 id="Multi-View-Vertebra-Localization-and-Identification-from-CT-Images"><a href="#Multi-View-Vertebra-Localization-and-Identification-from-CT-Images" class="headerlink" title="Multi-View Vertebra Localization and Identification from CT Images"></a>Multi-View Vertebra Localization and Identification from CT Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12845">http://arxiv.org/abs/2307.12845</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shanghaitech-impact/multi-view-vertebra-localization-and-identification-from-ct-images">https://github.com/shanghaitech-impact/multi-view-vertebra-localization-and-identification-from-ct-images</a></li>
<li>paper_authors: Han Wu, Jiadong Zhang, Yu Fang, Zhentao Liu, Nizhuan Wang, Zhiming Cui, Dinggang Shen</li>
<li>for: 验证CT图像中vertebra的准确位置和识别</li>
<li>methods: 多视图vertebra本地化和识别，转化3D问题为2D本地化和识别任务，不受3D裁剪覆盖的限制，能够自然地学习多视图全局信息</li>
<li>results: 使用仅两个2D网络，可以准确地位置和识别CT图像中的vertebra，并且在比较状态方法上占据优势<details>
<summary>Abstract</summary>
Accurately localizing and identifying vertebrae from CT images is crucial for various clinical applications. However, most existing efforts are performed on 3D with cropping patch operation, suffering from the large computation costs and limited global information. In this paper, we propose a multi-view vertebra localization and identification from CT images, converting the 3D problem into a 2D localization and identification task on different views. Without the limitation of the 3D cropped patch, our method can learn the multi-view global information naturally. Moreover, to better capture the anatomical structure information from different view perspectives, a multi-view contrastive learning strategy is developed to pre-train the backbone. Additionally, we further propose a Sequence Loss to maintain the sequential structure embedded along the vertebrae. Evaluation results demonstrate that, with only two 2D networks, our method can localize and identify vertebrae in CT images accurately, and outperforms the state-of-the-art methods consistently. Our code is available at https://github.com/ShanghaiTech-IMPACT/Multi-View-Vertebra-Localization-and-Identification-from-CT-Images.
</details>
<details>
<summary>摘要</summary>
精准地在CT图像上本地化和识别脊椎是各种临床应用的关键。然而，现有的大多数努力都是基于3D的割裁补丁操作，它们受到大量计算成本和局部信息的限制。在这篇论文中，我们提出了基于多视图的脊椎本地化和识别方法，将3D问题转化为2D本地化和识别任务。不同于局部补丁的3D方法，我们的方法可以自然地学习多视图的全局信息。此外，为了更好地捕捉不同视角的解剖结构信息，我们还提出了一种多视图对比学习策略来预训练后iony。此外，我们还提出了一种序列损失来维护脊椎中的顺序结构。评估结果表明，只需要两个2D网络，我们的方法可以在CT图像上准确地本地化和识别脊椎，并在比较前一些方法中占据领先地位。我们的代码可以在<https://github.com/ShanghaiTech-IMPACT/Multi-View-Vertebra-Localization-and-Identification-from-CT-Images>中找到。
</details></li>
</ul>
<hr>
<h2 id="Deep-Homography-Prediction-for-Endoscopic-Camera-Motion-Imitation-Learning"><a href="#Deep-Homography-Prediction-for-Endoscopic-Camera-Motion-Imitation-Learning" class="headerlink" title="Deep Homography Prediction for Endoscopic Camera Motion Imitation Learning"></a>Deep Homography Prediction for Endoscopic Camera Motion Imitation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12792">http://arxiv.org/abs/2307.12792</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Huber, Sebastien Ourselin, Christos Bergeles, Tom Vercauteren</li>
<li>for: 这 paper  investigate laparoscopic camera motion automation through imitation learning from retrospective videos of laparoscopic interventions.</li>
<li>methods: 该 paper  introduce a novel method that learns to augment a surgeon’s behavior in image space through object motion invariant image registration via homographies, without making any geometric assumptions or requiring depth information.</li>
<li>results:  compared to two baselines, the proposed method demonstrates significant improvements on the Cholec80 and HeiChole datasets, with an improvement of 47% over camera motion continuation. Additionally, the method is shown to correctly predict camera motion on the public motion classification labels of the AutoLaparo dataset.<details>
<summary>Abstract</summary>
In this work, we investigate laparoscopic camera motion automation through imitation learning from retrospective videos of laparoscopic interventions. A novel method is introduced that learns to augment a surgeon's behavior in image space through object motion invariant image registration via homographies. Contrary to existing approaches, no geometric assumptions are made and no depth information is necessary, enabling immediate translation to a robotic setup. Deviating from the dominant approach in the literature which consist of following a surgical tool, we do not handcraft the objective and no priors are imposed on the surgical scene, allowing the method to discover unbiased policies. In this new research field, significant improvements are demonstrated over two baselines on the Cholec80 and HeiChole datasets, showcasing an improvement of 47% over camera motion continuation. The method is further shown to indeed predict camera motion correctly on the public motion classification labels of the AutoLaparo dataset. All code is made accessible on GitHub.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们 investigate Laparoscopic camera motion automation through imitation learning from retrospective videos of laparoscopic interventions. 我们引入了一种新的方法，该方法可以在图像空间增强外科医生的行为，不需要任何几何假设和深度信息，因此可以直接应用于机器人设置。相比于文献中主流的方法，我们不会手工设置目标，也不会假设外科场景，因此方法可以自由发现不受干扰的策略。在新的研究领域中，我们展示了对Cholec80和HeiChole数据集的显著改进，提高了相对Camera Motion Continuation的47%。此外，我们还证明了该方法可以正确预测 Camera Motion 的公共运动分类标签AutoLaparo数据集。所有代码都可以在GitHub上获取。
</details></li>
</ul>
<hr>
<h2 id="Synthetic-white-balancing-for-intra-operative-hyperspectral-imaging"><a href="#Synthetic-white-balancing-for-intra-operative-hyperspectral-imaging" class="headerlink" title="Synthetic white balancing for intra-operative hyperspectral imaging"></a>Synthetic white balancing for intra-operative hyperspectral imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12791">http://arxiv.org/abs/2307.12791</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anisha Bahl, Conor C. Horgan, Mirek Janatka, Oscar J. MacCormac, Philip Noonan, Yijing Xie, Jianrong Qiu, Nicola Cavalcanti, Philipp Fürnstahl, Michael Ebner, Mads S. Bergholt, Jonathan Shapey, Tom Vercauteren</li>
<li>for: 该研究旨在开发一种可靠的、抗菌的白色参照图像，用于医学应用中的非侵入式规则分辨。</li>
<li>methods: 该研究提出了一种基于视频中的标准静止测量的新参照图像生成算法，使用了纯净的 sintetic reference construction 方法。</li>
<li>results: 研究表明，使用该算法可以实现高度准确的 spectral and color reconstruction，并且在实验中表现良好， median pixel-by-pixel errors 低于 6.5%。<details>
<summary>Abstract</summary>
Hyperspectral imaging shows promise for surgical applications to non-invasively provide spatially-resolved, spectral information. For calibration purposes, a white reference image of a highly-reflective Lambertian surface should be obtained under the same imaging conditions. Standard white references are not sterilizable, and so are unsuitable for surgical environments. We demonstrate the necessity for in situ white references and address this by proposing a novel, sterile, synthetic reference construction algorithm. The use of references obtained at different distances and lighting conditions to the subject were examined. Spectral and color reconstructions were compared with standard measurements qualitatively and quantitatively, using $\Delta E$ and normalised RMSE respectively. The algorithm forms a composite image from a video of a standard sterile ruler, whose imperfect reflectivity is compensated for. The reference is modelled as the product of independent spatial and spectral components, and a scalar factor accounting for gain, exposure, and light intensity. Evaluation of synthetic references against ideal but non-sterile references is performed using the same metrics alongside pixel-by-pixel errors. Finally, intraoperative integration is assessed though cadaveric experiments. Improper white balancing leads to increases in all quantitative and qualitative errors. Synthetic references achieve median pixel-by-pixel errors lower than 6.5% and produce similar reconstructions and errors to an ideal reference. The algorithm integrated well into surgical workflow, achieving median pixel-by-pixel errors of 4.77%, while maintaining good spectral and color reconstruction.
</details>
<details>
<summary>摘要</summary>
高spectral成像显示在手术应用中的推荐作用，可以不侵入性地提供空间分解的spectral信息。为了 calibration purposes，需要获得一张白色参照图像，该图像是一个高度反射的 Lambertian 表面在同一个捕集条件下获得的。标准的白色参照图像不可 sterilizable，因此不适用于手术环境。我们证明了需要场景中的白色参照图像，并提出了一种新的、可靠、Synthetic 参照图像生成算法。我们 исследова了不同距离和照明条件下的参照图像的使用情况，并对spectral和色彩重建进行了质量和量度的比较，使用 $\Delta E$ 和 normalized RMSE 两种指标。该算法生成了一张从标准 sterile 尺Rule 视频中获得的复合图像，其中偏差的反射率被补做。参照图像被模型为独立的空间和spectral 分量的乘积，以及一个映射矩阵 accounting for gain, exposure, and light intensity。我们对Synthetic 参照图像与理想 pero 非 sterile 参照图像进行了比较，并使用相同的指标进行评价。最后，我们通过 cadaveric 实验评估了 intraoperative 集成。不当的白平衡会导致所有量化和质量错误的增加。Synthetic 参照图像实现了 median 像素误差低于 6.5%，并且生成了与理想参照图像相似的重建和误差。该算法在手术工作流中集成 well，实现了 median 像素误差为 4.77%，并保持了好的spectral和色彩重建。
</details></li>
</ul>
<hr>
<h2 id="ICF-SRSR-Invertible-scale-Conditional-Function-for-Self-Supervised-Real-world-Single-Image-Super-Resolution"><a href="#ICF-SRSR-Invertible-scale-Conditional-Function-for-Self-Supervised-Real-world-Single-Image-Super-Resolution" class="headerlink" title="ICF-SRSR: Invertible scale-Conditional Function for Self-Supervised Real-world Single Image Super-Resolution"></a>ICF-SRSR: Invertible scale-Conditional Function for Self-Supervised Real-world Single Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12751">http://arxiv.org/abs/2307.12751</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reyhaneh Neshatavar, Mohsen Yavartanoo, Sanghyun Son, Kyoung Mu Lee</li>
<li>for: 提高单图超分解问题中的精度和实用性，无需使用任何对&#x2F;无对训练数据。</li>
<li>methods: 提出一种归一化可逆函数（ICF），可以扩大输入图像的比例，然后将原始输入图像还原到不同比例条件下。通过这种ICF，我们构建了一种新的自助学习SR框架（ICF-SRSR）。</li>
<li>results: 经验表明，我们的ICF-SRSR可以在无需使用任何对&#x2F;无对训练数据的情况下处理SR任务，并且在实际世界 scenarios中表现出色，与现有的超级vised&#x2F;无对训练方法相当。<details>
<summary>Abstract</summary>
Single image super-resolution (SISR) is a challenging ill-posed problem that aims to up-sample a given low-resolution (LR) image to a high-resolution (HR) counterpart. Due to the difficulty in obtaining real LR-HR training pairs, recent approaches are trained on simulated LR images degraded by simplified down-sampling operators, e.g., bicubic. Such an approach can be problematic in practice because of the large gap between the synthesized and real-world LR images. To alleviate the issue, we propose a novel Invertible scale-Conditional Function (ICF), which can scale an input image and then restore the original input with different scale conditions. By leveraging the proposed ICF, we construct a novel self-supervised SISR framework (ICF-SRSR) to handle the real-world SR task without using any paired/unpaired training data. Furthermore, our ICF-SRSR can generate realistic and feasible LR-HR pairs, which can make existing supervised SISR networks more robust. Extensive experiments demonstrate the effectiveness of the proposed method in handling SISR in a fully self-supervised manner. Our ICF-SRSR demonstrates superior performance compared to the existing methods trained on synthetic paired images in real-world scenarios and exhibits comparable performance compared to state-of-the-art supervised/unsupervised methods on public benchmark datasets.
</details>
<details>
<summary>摘要</summary>
<<SYS>>单张图像超分辨 (SISR) 是一个具有挑战性的缺失定理问题，旨在将给定的低分辨 (LR) 图像提升到高分辨 (HR) 对应的图像。由于获得实际LR-HR训练对的困难，现有的方法通常是通过简化的下采样操作生成模拟LR图像进行训练，如比立方体抠杆。这种方法在实际应用中存在一定的问题，因为生成的模拟LR图像与实际世界中的LR图像之间存在很大的差异。为了解决这问题，我们提出了一种新的归一化Scale-Conditional函数 (ICF)，可以将输入图像扩大，然后使用不同的扩展级别来恢复原始输入图像。通过利用我们提出的ICF，我们建立了一种新的无监督SISR框架 (ICF-SRSR)，可以在实际世界中进行SR任务无需使用任何配偶/非配偶训练数据。此外，我们的ICF-SRSR可以生成真实可行的LR-HR对，这可以使现有的监督SISR网络更加Robust。我们进行了广泛的实验， demonstarted the effectiveness of the proposed method in handling SISR in a fully self-supervised manner. Our ICF-SRSR demonstrates superior performance compared to existing methods trained on synthetic paired images in real-world scenarios and exhibits comparable performance compared to state-of-the-art supervised/unsupervised methods on public benchmark datasets.Note: The translation is done using Google Translate, and some minor adjustments may be needed to make it more idiomatic.
</details></li>
</ul>
<hr>
<h2 id="Dense-Transformer-based-Enhanced-Coding-Network-for-Unsupervised-Metal-Artifact-Reduction"><a href="#Dense-Transformer-based-Enhanced-Coding-Network-for-Unsupervised-Metal-Artifact-Reduction" class="headerlink" title="Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction"></a>Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12717">http://arxiv.org/abs/2307.12717</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wangduo Xie, Matthew B. Blaschko</li>
<li>for: 针对 CT 图像受到金属残余的抑制，提高临床诊断的精度。</li>
<li>methods: 基于 dense transformer 的增强编码网络 (DTEC-Net)，包括层次分解编码器和高级密度处理，以获取长距离匹配的紧密编码序列。然后，提出第二阶段分离方法，改进密集序列的解码过程。</li>
<li>results: 对一个标准测试集进行了广泛的实验和模型讨论，证明 DTEC-Net 的有效性，在对比前一个状态艺术方法时，大幅减少了金属残余，同时保留了更多的纹理细节。<details>
<summary>Abstract</summary>
CT images corrupted by metal artifacts have serious negative effects on clinical diagnosis. Considering the difficulty of collecting paired data with ground truth in clinical settings, unsupervised methods for metal artifact reduction are of high interest. However, it is difficult for previous unsupervised methods to retain structural information from CT images while handling the non-local characteristics of metal artifacts. To address these challenges, we proposed a novel Dense Transformer based Enhanced Coding Network (DTEC-Net) for unsupervised metal artifact reduction. Specifically, we introduce a Hierarchical Disentangling Encoder, supported by the high-order dense process, and transformer to obtain densely encoded sequences with long-range correspondence. Then, we present a second-order disentanglement method to improve the dense sequence's decoding process. Extensive experiments and model discussions illustrate DTEC-Net's effectiveness, which outperforms the previous state-of-the-art methods on a benchmark dataset, and greatly reduces metal artifacts while restoring richer texture details.
</details>
<details>
<summary>摘要</summary>
《CT图像损坏了金属artefacts会产生严重的负面影响于临床诊断。由于在临床设置中采集对照数据的困难，不supervised方法 для降低金属artefacts的抗 corr 感兴趣。然而，previous unsupervised方法难以保留CT图像的结构信息while处理金属artefacts的非本地特性。为了解决这些挑战，我们提出了一种基于Dense Transformer的增强编码网络（DTEC-Net）。specifically，我们提出了一种层次分解编码器，得到高级密度过程支持的密集编码序列，并使用trasnformer来获得长距离匹配。然后，我们提出了第二个分解方法，以改进密集序列的解码过程。广泛的实验和模型讨论表明DTEC-Net的效果，superior于前一个状态的艺术方法，在一个标准数据集上具有更好的降低金属artefacts的能力，同时保留了更加丰富的текxture细节。》Note: Simplified Chinese is used here, as it is the most widely used variety of Chinese in mainland China. However, if you prefer Traditional Chinese, I can provide that version as well.
</details></li>
</ul>
<hr>
<h2 id="Low-complexity-Overfitted-Neural-Image-Codec"><a href="#Low-complexity-Overfitted-Neural-Image-Codec" class="headerlink" title="Low-complexity Overfitted Neural Image Codec"></a>Low-complexity Overfitted Neural Image Codec</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12706">http://arxiv.org/abs/2307.12706</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Orange-OpenSource/Cool-Chic">https://github.com/Orange-OpenSource/Cool-Chic</a></li>
<li>paper_authors: Thomas Leguay, Théo Ladune, Pierrick Philippe, Gordon Clare, Félix Henry</li>
<li>for: 这个论文是为了提出一种减少复杂度的神经网络图像编码器，使得每个输入图像的解码器参数可以被过度适应。</li>
<li>methods: 这种方法使用了过度适应的方法，并且使用了低复杂度的模块和改进的训练过程，以实现更高的编码效率。</li>
<li>results: 这种方法可以与Autoencoder的性能竞争，并且在不同的编码条件下超过HEVC的性能，同时具有较低的复杂度。<details>
<summary>Abstract</summary>
We propose a neural image codec at reduced complexity which overfits the decoder parameters to each input image. While autoencoders perform up to a million multiplications per decoded pixel, the proposed approach only requires 2300 multiplications per pixel. Albeit low-complexity, the method rivals autoencoder performance and surpasses HEVC performance under various coding conditions. Additional lightweight modules and an improved training process provide a 14% rate reduction with respect to previous overfitted codecs, while offering a similar complexity. This work is made open-source at https://orange-opensource.github.io/Cool-Chic/
</details>
<details>
<summary>摘要</summary>
我们提出了一种带有减少复杂度的神经网络图像编码器，其中编码器参数被每个输入图像进行过拟合。而传统的 autoencoder 可能需要多达一百万次乘法运算每个解码像素，我们的方法只需要每个像素进行 2300 次乘法运算。尽管具有低复杂度，我们的方法可以与 autoencoder 的性能竞争，并在不同的编码条件下超越 HEVC 的性能。此外，我们还提出了一些轻量级的模块和改进的训练过程，使得对于先前的过拟合编码器而言，可以实现14%的比例减少，同时保持相似的复杂度。这个工作将在 <https://orange-opensource.github.io/Cool-Chic/> 上发布为开源项目。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Based-Unrolling-for-Reconstruction-and-Super-resolution-of-Single-Photon-Lidar-Systems"><a href="#Bayesian-Based-Unrolling-for-Reconstruction-and-Super-resolution-of-Single-Photon-Lidar-Systems" class="headerlink" title="Bayesian Based Unrolling for Reconstruction and Super-resolution of Single-Photon Lidar Systems"></a>Bayesian Based Unrolling for Reconstruction and Super-resolution of Single-Photon Lidar Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12700">http://arxiv.org/abs/2307.12700</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abderrahim Halimi, Jakeoung Koo, Stephen McLaughlin</li>
<li>for: 本文旨在应对实际应用中3D单 photon Lidar成像遇到的噪声环境和探测器有限分辨率的挑战，通过deep learning算法来进行重建和超分辨。</li>
<li>methods: 本文提出了基于 Bayesian 模型的深度学习算法，可以利用统计和学习两种不同的框架之优点，提供最佳估计和改善网络可读性。与现有学习算法相比，提议的架构具有更少的可训练参数、更高的噪声和系统响应函数模型化缺陷的Robustness，并提供了更详细的估计信息，包括uncertainty度量。</li>
<li>results: 对于 synthetic 和实际数据，提议的算法可以与现有算法相比，提供类似的推断质量和计算复杂度，同时具有更好的网络解释性。<details>
<summary>Abstract</summary>
Deploying 3D single-photon Lidar imaging in real world applications faces several challenges due to imaging in high noise environments and with sensors having limited resolution. This paper presents a deep learning algorithm based on unrolling a Bayesian model for the reconstruction and super-resolution of 3D single-photon Lidar. The resulting algorithm benefits from the advantages of both statistical and learning based frameworks, providing best estimates with improved network interpretability. Compared to existing learning-based solutions, the proposed architecture requires a reduced number of trainable parameters, is more robust to noise and mismodelling of the system impulse response function, and provides richer information about the estimates including uncertainty measures. Results on synthetic and real data show competitive results regarding the quality of the inference and computational complexity when compared to state-of-the-art algorithms. This short paper is based on contributions published in [1] and [2].
</details>
<details>
<summary>摘要</summary>
在实际应用中部署3D单 photon探测影像遇到许多挑战，主要是因为探测环境噪声高，探测器分辨率有限。本文提出了基于深度学习算法的bayesian模型卷积 reconstruction和超分辨3D单 photon探测。该算法利用了统计和学习框架的优点，提供了最佳估计，同时具有改进的网络解释性。与现有的学习基于算法相比，提议的架构需要较少的可训练参数，更加鲁棒对噪和系统响应函数的误差模型，并提供了更多的估计信息，包括不确定度测量。synthetic和实际数据的结果表明，与当前状态的算法相比，提议的算法可以达到竞争力的结果质量和计算复杂度。这篇短文基于[1]和[2]的贡献。
</details></li>
</ul>
<hr>
<h2 id="Automatic-lobe-segmentation-using-attentive-cross-entropy-and-end-to-end-fissure-generation"><a href="#Automatic-lobe-segmentation-using-attentive-cross-entropy-and-end-to-end-fissure-generation" class="headerlink" title="Automatic lobe segmentation using attentive cross entropy and end-to-end fissure generation"></a>Automatic lobe segmentation using attentive cross entropy and end-to-end fissure generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12634">http://arxiv.org/abs/2307.12634</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/htytewx/softcam">https://github.com/htytewx/softcam</a></li>
<li>paper_authors: Qi Su, Na Wang, Jiawen Xie, Yinan Chen, Xiaofan Zhang</li>
<li>for:  automatic lung lobe segmentation algorithm for the diagnosis and treatment of lung diseases</li>
<li>methods:  task-specific loss function to pay attention to the area around the pulmonary fissure, end-to-end pulmonary fissure generation method, registration-based loss function to alleviate convergence difficulty</li>
<li>results:  achieved 97.83% and 94.75% dice scores on private dataset STLB and public LUNA16 dataset respectively<details>
<summary>Abstract</summary>
The automatic lung lobe segmentation algorithm is of great significance for the diagnosis and treatment of lung diseases, however, which has great challenges due to the incompleteness of pulmonary fissures in lung CT images and the large variability of pathological features. Therefore, we propose a new automatic lung lobe segmentation framework, in which we urge the model to pay attention to the area around the pulmonary fissure during the training process, which is realized by a task-specific loss function. In addition, we introduce an end-to-end pulmonary fissure generation method in the auxiliary pulmonary fissure segmentation task, without any additional network branch. Finally, we propose a registration-based loss function to alleviate the convergence difficulty of the Dice loss supervised pulmonary fissure segmentation task. We achieve 97.83% and 94.75% dice scores on our private dataset STLB and public LUNA16 dataset respectively.
</details>
<details>
<summary>摘要</summary>
“自动lung lobe分 segmentation算法具有诊断和治疗肺病的重要意义，但受到肺CT图像中肺裂的不完整性和病理特征的大幅度变化所困扰。因此，我们提出了一个新的自动lung lobe分 segmentation框架，其中我们强调模型在训练过程中对肺裂附近区域的注意。此外，我们引入了一个终端到终端的肺裂生成方法，不需要额外的网络分支。最后，我们提出了一个注册基准的损失函数，以解决由Dice损失函数监督的肺裂分 segmentation任务中的对准问题。我们在私人数据集STLB和公开的LUNA16数据集上实现了97.83%和94.75%的Dice分数。”Note that Simplified Chinese is used here, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Sparse-annotation-strategies-for-segmentation-of-short-axis-cardiac-MRI"><a href="#Sparse-annotation-strategies-for-segmentation-of-short-axis-cardiac-MRI" class="headerlink" title="Sparse annotation strategies for segmentation of short axis cardiac MRI"></a>Sparse annotation strategies for segmentation of short axis cardiac MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12619">http://arxiv.org/abs/2307.12619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Josh Stein, Maxime Di Folco, Julia Schnabel<br>for:这种研究旨在调查使用少量标注数据（48个标注量）和少量slice标注数据（每个案例）来达到高精度心脏MRI分割结果。methods:研究使用了state-of-the-art nnU-Net模型，并运用了减少数据量和减少slice标注数据的方法来降低标注量。results:研究发现，使用更少的slice标注数据可以达到高精度分割结果，而使用更多的volume标注数据并不一定能够提高分割结果。此外，中部区域的slice标注更有价值，而腹部区域的slice标注最差。因此，对于心脏MRI分割任务，更好的方法是尽量标注多个slice而不是标注更多的volume。<details>
<summary>Abstract</summary>
Short axis cardiac MRI segmentation is a well-researched topic, with excellent results achieved by state-of-the-art models in a supervised setting. However, annotating MRI volumes is time-consuming and expensive. Many different approaches (e.g. transfer learning, data augmentation, few-shot learning, etc.) have emerged in an effort to use fewer annotated data and still achieve similar performance as a fully supervised model. Nevertheless, to the best of our knowledge, none of these works focus on which slices of MRI volumes are most important to annotate for yielding the best segmentation results. In this paper, we investigate the effects of training with sparse volumes, i.e. reducing the number of cases annotated, and sparse annotations, i.e. reducing the number of slices annotated per case. We evaluate the segmentation performance using the state-of-the-art nnU-Net model on two public datasets to identify which slices are the most important to annotate. We have shown that training on a significantly reduced dataset (48 annotated volumes) can give a Dice score greater than 0.85 and results comparable to using the full dataset (160 and 240 volumes for each dataset respectively). In general, training on more slice annotations provides more valuable information compared to training on more volumes. Further, annotating slices from the middle of volumes yields the most beneficial results in terms of segmentation performance, and the apical region the worst. When evaluating the trade-off between annotating volumes against slices, annotating as many slices as possible instead of annotating more volumes is a better strategy.
</details>
<details>
<summary>摘要</summary>
短轴心脏MRI分割是已经有很多研究的话题，现有的最新模型在监督环境下已经 дости得了极佳的结果。然而，annotating MRI卷积是时间consuming和昂贵的。许多不同的方法（例如传播学习、数据增强、几据学习等）在努力使用更少的annotated data并 still achieve相似的性能，但是，到目前为止，none of these works focus on which slices of MRI卷积是最重要的annotate，以获得最好的分割结果。在这篇论文中，我们investigate the effects of training with sparse volumes和sparse annotations，specifically, reducing the number of cases annotated and the number of slices annotated per case。我们使用了state-of-the-art nnU-Net模型在两个公共数据集上评估分割性能，以找出最重要的slice要annotate。我们发现，使用仅具有48个annotated volume的dataset可以获得Dice分数大于0.85，并且和使用完整的dataset（160和240个volume for each dataset）的结果相似。总体而言，training on more slice annotations provides more valuable information compared to training on more volumes。此外， annotating slices from the middle of volumes yields the most beneficial results in terms of segmentation performance, while the apical region is the worst. When evaluating the trade-off between annotating volumes against slices, annotating as many slices as possible instead of annotating more volumes is a better strategy.
</details></li>
</ul>
<hr>
<h2 id="Attribute-Regularized-Soft-Introspective-VAE-Towards-Cardiac-Attribute-Regularization-Through-MRI-Domains"><a href="#Attribute-Regularized-Soft-Introspective-VAE-Towards-Cardiac-Attribute-Regularization-Through-MRI-Domains" class="headerlink" title="Attribute Regularized Soft Introspective VAE: Towards Cardiac Attribute Regularization Through MRI Domains"></a>Attribute Regularized Soft Introspective VAE: Towards Cardiac Attribute Regularization Through MRI Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12618">http://arxiv.org/abs/2307.12618</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maxime Di Folco, Cosmin Bercea, Julia A. Schnabel<br>for:* 这个论文主要目标是提高深度生成模型的控制性，使其能够更好地捕捉隐藏的特征。methods:* 该论文提出了一种叫做Attributed Soft Introspective VAE（Attri-SIVAE）的新方法，它将 attribute  regularized loss  incorporated into Soft-Intro VAE 框架中。results:* 实验表明，该方法在不同的 MRI 数据集上都能够达到类似的重建和Regularization性能，与 state-of-the-art Attributed regularized VAE 相比，该方法还能够在不同的数据集上保持同样的Regularization水平。<details>
<summary>Abstract</summary>
Deep generative models have emerged as influential instruments for data generation and manipulation. Enhancing the controllability of these models by selectively modifying data attributes has been a recent focus. Variational Autoencoders (VAEs) have shown promise in capturing hidden attributes but often produce blurry reconstructions. Controlling these attributes through different imaging domains is difficult in medical imaging. Recently, Soft Introspective VAE leverage the benefits of both VAEs and Generative Adversarial Networks (GANs), which have demonstrated impressive image synthesis capabilities, by incorporating an adversarial loss into VAE training. In this work, we propose the Attributed Soft Introspective VAE (Attri-SIVAE) by incorporating an attribute regularized loss, into the Soft-Intro VAE framework. We evaluate experimentally the proposed method on cardiac MRI data from different domains, such as various scanner vendors and acquisition centers. The proposed method achieves similar performance in terms of reconstruction and regularization compared to the state-of-the-art Attributed regularized VAE but additionally also succeeds in keeping the same regularization level when tested on a different dataset, unlike the compared method.
</details>
<details>
<summary>摘要</summary>
深度生成模型已成为数据生成和修改的重要工具。提高这些模型的可控性，通过选择性地修改数据属性，是最近的焦点。变量自动编码器（VAEs）表现出在捕捉隐藏属性的潜力，但经常生成模糊的重建。在医学成像中，控制这些属性通过不同的成像频谱是困难的。最近，软 introspective VAE（Soft-Intro VAE）利用了 VAEs 和生成对抗网络（GANs）的优点，通过在 VAE 训练中添加对抗损失来产生出色的图像 sintesis 能力。在这项工作中，我们提出了具有属性规范损失的 Attributed Soft Introspective VAE（Attri-SIVAE）。我们通过实验评估了提议的方法在不同频谱的各种心脏 MRI 数据上。提议的方法与 state-of-the-art 带有属性规范损失的 VAE 相当，但同时也能够在不同的数据集上保持同样的规范化水平，与比较方法不同。
</details></li>
</ul>
<hr>
<h2 id="AMaizeD-An-End-to-End-Pipeline-for-Automatic-Maize-Disease-Detection"><a href="#AMaizeD-An-End-to-End-Pipeline-for-Automatic-Maize-Disease-Detection" class="headerlink" title="AMaizeD: An End to End Pipeline for Automatic Maize Disease Detection"></a>AMaizeD: An End to End Pipeline for Automatic Maize Disease Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03766">http://arxiv.org/abs/2308.03766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anish Mall, Sanchit Kabra, Ankur Lhila, Pawan Ajmera</li>
<li>for: 这个研究论文旨在提供一个自动化的丰盈麦病识别框架，用于早期发现农场麦谷植物病变。</li>
<li>methods: 本研究使用多spectral imaging技术，combined with deep learning algorithms和segmenation techniques，以提取植物病变特征。</li>
<li>results: 实验结果显示，本框架能够实时检测多种麦病变，包括粉刺菌、anthracnose和叶萎病。<details>
<summary>Abstract</summary>
This research paper presents AMaizeD: An End to End Pipeline for Automatic Maize Disease Detection, an automated framework for early detection of diseases in maize crops using multispectral imagery obtained from drones. A custom hand-collected dataset focusing specifically on maize crops was meticulously gathered by expert researchers and agronomists. The dataset encompasses a diverse range of maize varieties, cultivation practices, and environmental conditions, capturing various stages of maize growth and disease progression. By leveraging multispectral imagery, the framework benefits from improved spectral resolution and increased sensitivity to subtle changes in plant health. The proposed framework employs a combination of convolutional neural networks (CNNs) as feature extractors and segmentation techniques to identify both the maize plants and their associated diseases. Experimental results demonstrate the effectiveness of the framework in detecting a range of maize diseases, including powdery mildew, anthracnose, and leaf blight. The framework achieves state-of-the-art performance on the custom hand-collected dataset and contributes to the field of automated disease detection in agriculture, offering a practical solution for early identification of diseases in maize crops advanced machine learning techniques and deep learning architectures.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Development-Of-Automated-Cardiac-Arrhythmia-Detection-Methods-Using-Single-Channel-ECG-Signal"><a href="#Development-Of-Automated-Cardiac-Arrhythmia-Detection-Methods-Using-Single-Channel-ECG-Signal" class="headerlink" title="Development Of Automated Cardiac Arrhythmia Detection Methods Using Single Channel ECG Signal"></a>Development Of Automated Cardiac Arrhythmia Detection Methods Using Single Channel ECG Signal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02405">http://arxiv.org/abs/2308.02405</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arpita Paul, Avik Kumar Das, Manas Rakshit, Ankita Ray Chowdhury, Susmita Saha, Hrishin Roy, Sajal Sarkar, Dongiri Prasanth, Eravelli Saicharan</li>
<li>For: 这个研究旨在开发一个基于单通道电子心脏ogram (ECG) 信号的多类型过节损检测算法，以减少对心脏疾病的死亡。* Methods: 本研究使用心跳时间径特征、形态特征和wavelet几何特征，联合使用Statistical、Entropy和能量基于的特征，并应用机器学习基于Random Forest的分类器。* Results: 使用HRV和时域形态特征，取得85.11%的精度、85.11%的敏感性、85.07%的精确性和85.00%的F1分数，使用HRV和wavelet几何特征，取得90.91%的精度、90.91%的敏感性、90.96%的精确性和90.87%的F1分数。<details>
<summary>Abstract</summary>
Arrhythmia, an abnormal cardiac rhythm, is one of the most common types of cardiac disease. Automatic detection and classification of arrhythmia can be significant in reducing deaths due to cardiac diseases. This work proposes a multi-class arrhythmia detection algorithm using single channel electrocardiogram (ECG) signal. In this work, heart rate variability (HRV) along with morphological features and wavelet coefficient features are utilized for detection of 9 classes of arrhythmia. Statistical, entropy and energy-based features are extracted and applied to machine learning based random forest classifiers. Data used in both works is taken from 4 broad databases (CPSC and CPSC extra, PTB-XL, G12EC and Chapman-Shaoxing and Ningbo Database) made available by Physionet. With HRV and time domain morphological features, an average accuracy of 85.11%, sensitivity of 85.11%, precision of 85.07% and F1 score of 85.00% is obtained whereas with HRV and wavelet coefficient features, the performance obtained is 90.91% accuracy, 90.91% sensitivity, 90.96% precision and 90.87% F1 score. The detailed analysis of simulation results affirms that the presented scheme effectively detects broad categories of arrhythmia from single-channel ECG records. In the last part of the work, the proposed classification schemes are implemented on hardware using Raspberry Pi for real time ECG signal classification.
</details>
<details>
<summary>摘要</summary>
心Rate变化（HRV）和形态特征以及wavelet幂特征的组合使用，可以有效地检测单通道ECG信号中的9类cardiac arrhythmia。在这种方法中，使用Physionet提供的4个广泛数据库（CPSC和CPSC extra、PTB-XL、G12EC和Chapman-Shaoxing和Ningbo数据库）的数据，通过机器学习基于Random Forest的分类器进行检测。在实验结果的详细分析中，我们发现，提出的方案可以准确地从单通道ECG记录中检测出广泛的cardiac arrhythmia类型。最后，我们在实时ECG信号分类中实现了提出的分类方案，使用Raspberry Pi硬件实现。Here's the translation of the text into Traditional Chinese:心率变化（HRV）和形态特征以及wavelet幂特征的组合使用，可以有效地检测单通道ECG信号中的9类cardiac arrhythmia。在这种方法中，使用Physionet提供的4个广泛数据库（CPSC和CPSC extra、PTB-XL、G12EC和Chapman-Shaoxing和Ningbo数据库）的数据，通过机器学习基于Random Forest的分类器进行检测。在实验结果的详细分析中，我们发现，提出的方案可以准确地从单通道ECG记录中检测出广泛的cardiac arrhythmia类型。最后，我们在实时ECG信号分类中实现了提出的分类方案，使用Raspberry Pi硬件实现。
</details></li>
</ul>
<hr>
<h2 id="4D-Feet-Registering-Walking-Foot-Shapes-Using-Attention-Enhanced-Dynamic-Synchronized-Graph-Convolutional-LSTM-Network"><a href="#4D-Feet-Registering-Walking-Foot-Shapes-Using-Attention-Enhanced-Dynamic-Synchronized-Graph-Convolutional-LSTM-Network" class="headerlink" title="4D Feet: Registering Walking Foot Shapes Using Attention Enhanced Dynamic-Synchronized Graph Convolutional LSTM Network"></a>4D Feet: Registering Walking Foot Shapes Using Attention Enhanced Dynamic-Synchronized Graph Convolutional LSTM Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12377">http://arxiv.org/abs/2307.12377</a></li>
<li>repo_url: None</li>
<li>paper_authors: Farzam Tajdari, Toon Huysmans, Xinhe Yao, Jun Xu, Yu Song<br>for:这个论文的目的是为了研究4D扫描技术，帮助研究人员更好地理解人体动态变形的特征。methods:这个论文使用的方法包括非固定iterative closest-farthest points算法来找到和对准不同摄像头捕捉的动态特征，以及一种新型的ADGC-LSTM网络来同步不同摄像头捕捉的扫描结果。另外，论文还使用非固定注册方法将同步的扫描结果与时间线相匹配。results:论文的结果表明，提出的框架能够有效地同步不同摄像头捕捉的4D扫描结果，并且能够生成高质量的3D矫正模型。在使用新开发的4D脚部扫描仪时，论文创造了首个开放数据集，名为4D脚部（15 fps），包括58名参与者的右和左脚的4D形状（共116个脚，包括5147帧3D帧），覆盖了走姿周期中重要的阶段。<details>
<summary>Abstract</summary>
4D scans of dynamic deformable human body parts help researchers have a better understanding of spatiotemporal features. However, reconstructing 4D scans based on multiple asynchronous cameras encounters two main challenges: 1) finding the dynamic correspondences among different frames captured by each camera at the timestamps of the camera in terms of dynamic feature recognition, and 2) reconstructing 3D shapes from the combined point clouds captured by different cameras at asynchronous timestamps in terms of multi-view fusion. In this paper, we introduce a generic framework that is able to 1) find and align dynamic features in the 3D scans captured by each camera using the nonrigid iterative closest-farthest points algorithm; 2) synchronize scans captured by asynchronous cameras through a novel ADGC-LSTM-based network, which is capable of aligning 3D scans captured by different cameras to the timeline of a specific camera; and 3) register a high-quality template to synchronized scans at each timestamp to form a high-quality 3D mesh model using a non-rigid registration method. With a newly developed 4D foot scanner, we validate the framework and create the first open-access data-set, namely the 4D feet. It includes 4D shapes (15 fps) of the right and left feet of 58 participants (116 feet in total, including 5147 3D frames), covering significant phases of the gait cycle. The results demonstrate the effectiveness of the proposed framework, especially in synchronizing asynchronous 4D scans using the proposed ADGC-LSTM network.
</details>
<details>
<summary>摘要</summary>
4D扫描技术为研究人体动态可变部分带来了更好的理解空间特征。然而，基于多个异步相机拍摄的4D扫描重建又存在两大挑战：1）在不同相机的时间戳上找到动态相关性，并在不同相机的时间戳上匹配动态特征；2）通过多视图融合来重建3D形状。在本文中，我们提出了一个通用的框架，可以1）使用非定剑迭代最近最远点对算法来找到和对准3D扫描中的动态特征；2）使用一种新型的ADGC-LSTM网络将不同相机拍摄的4D扫描同步到特定相机的时间轴上；3）使用非定剑注册方法将同步化后的4D扫描与高质量模板进行匹配，以生成高质量3D mesh模型。我们采用了一种新开发的4D脚部扫描仪，并验证了该框架。我们创建了首个开放数据集，即4D脚。该数据集包括15帧/秒的4D形状（包括右脚和左脚），共计116个脚，包括5147帧的3D扫描数据，覆盖了人 gaits 周期中的重要阶段。结果表明提出的框架具有较高的效果，特别是在同步异步4D扫描中使用我们提出的ADGC-LSTM网络。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/24/eess.IV_2023_07_24/" data-id="cllsj9x0e008kuv88b27w24r2" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/23/cs.LG_2023_07_23/" class="article-date">
  <time datetime="2023-07-22T16:00:00.000Z" itemprop="datePublished">2023-07-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/23/cs.LG_2023_07_23/">cs.LG - 2023-07-23 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Right-for-the-Wrong-Reason-Can-Interpretable-ML-Techniques-Detect-Spurious-Correlations"><a href="#Right-for-the-Wrong-Reason-Can-Interpretable-ML-Techniques-Detect-Spurious-Correlations" class="headerlink" title="Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?"></a>Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12344">http://arxiv.org/abs/2307.12344</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ss-sun/right-for-the-wrong-reason">https://github.com/ss-sun/right-for-the-wrong-reason</a></li>
<li>paper_authors: Susu Sun, Lisa M. Koch, Christian F. Baumgartner</li>
<li>for: 本文旨在评估一种解释技术的能力，以确定它是否能够正确地检测模型中的假 correlate。</li>
<li>methods: 本文使用了五种后处解释技术和一种自然 interpretable 的方法来检测一个胸部 X-ray 诊断任务中的三种人工添加的干扰因素。</li>
<li>results: 结果显示，使用 SHAP 和 Attri-Net 方法可以准确地检测模型中的假 correlate，并且这些方法可以被用来可靠地检测模型的错误行为。<details>
<summary>Abstract</summary>
While deep neural network models offer unmatched classification performance, they are prone to learning spurious correlations in the data. Such dependencies on confounding information can be difficult to detect using performance metrics if the test data comes from the same distribution as the training data. Interpretable ML methods such as post-hoc explanations or inherently interpretable classifiers promise to identify faulty model reasoning. However, there is mixed evidence whether many of these techniques are actually able to do so. In this paper, we propose a rigorous evaluation strategy to assess an explanation technique's ability to correctly identify spurious correlations. Using this strategy, we evaluate five post-hoc explanation techniques and one inherently interpretable method for their ability to detect three types of artificially added confounders in a chest x-ray diagnosis task. We find that the post-hoc technique SHAP, as well as the inherently interpretable Attri-Net provide the best performance and can be used to reliably identify faulty model behavior.
</details>
<details>
<summary>摘要</summary>
深度神经网络模型具有无可比的分类性能，但它们容易学习数据中的假 correlate。这些依赖干扰信息的相互作用可以通过性能指标难以探测，尤其如果测试数据来自同一个分布。可解释的机器学习方法，如后期解释或自然解释的分类器，承诺能够识别模型的恶劣逻辑。然而，有证据表明，许多这些技术并不能够做到这一点。在这篇论文中，我们提出了一种充分的评估策略，用于评估一种解释技术的能力，并可以准确地识别模型中的假 correlate。使用这种策略，我们评估了五种后期解释技术和一种自然解释的方法，并发现这些技术可以可靠地识别模型的假 correlate。特别是，SHAP技术和自然解释的Attri-Net具有最好的表现，可以用于可靠地识别模型的恶劣逻辑。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Learning-for-Audio-Based-Emotion-Recognition"><a href="#Self-Supervised-Learning-for-Audio-Based-Emotion-Recognition" class="headerlink" title="Self-Supervised Learning for Audio-Based Emotion Recognition"></a>Self-Supervised Learning for Audio-Based Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12343">http://arxiv.org/abs/2307.12343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peranut Nimitsurachat, Peter Washington</li>
<li>for: 这个论文的目的是提高 audio 输入数据 的情感识别模型性能，以便在心理健康、市场营销、游戏和社交媒体分析等领域中建立交互式系统。</li>
<li>methods: 这个论文使用了自动学习（SSL）方法，通过预测数据自身的特性来学习，而不需要大量的指导标签。</li>
<li>results: 研究发现，使用 SSL 预训练后 fine-tune 使得模型性能在所有评价指标上都提高，特别是在较易分类的情绪（如快乐、悲伤和愤怒）方面。此外，研究还发现，当输入数据具有嵌入特征表示时，自动学习更加有用。<details>
<summary>Abstract</summary>
Emotion recognition models using audio input data can enable the development of interactive systems with applications in mental healthcare, marketing, gaming, and social media analysis. While the field of affective computing using audio data is rich, a major barrier to achieve consistently high-performance models is the paucity of available training labels. Self-supervised learning (SSL) is a family of methods which can learn despite a scarcity of supervised labels by predicting properties of the data itself. To understand the utility of self-supervised learning for audio-based emotion recognition, we have applied self-supervised learning pre-training to the classification of emotions from the CMU- MOSEI's acoustic modality. Unlike prior papers that have experimented with raw acoustic data, our technique has been applied to encoded acoustic data. Our model is first pretrained to uncover the randomly-masked timestamps of the acoustic data. The pre-trained model is then fine-tuned using a small sample of annotated data. The performance of the final model is then evaluated via several evaluation metrics against a baseline deep learning model with an identical backbone architecture. We find that self-supervised learning consistently improves the performance of the model across all metrics. This work shows the utility of self-supervised learning for affective computing, demonstrating that self-supervised learning is most useful when the number of training examples is small, and that the effect is most pronounced for emotions which are easier to classify such as happy, sad and anger. This work further demonstrates that self-supervised learning works when applied to embedded feature representations rather than the traditional approach of pre-training on the raw input space.
</details>
<details>
<summary>摘要</summary>
“情感识别模型使用音频输入数据可以实现互动系统的开发，应用于心理健康、市场营销、游戏和社交媒体分析。然而，使得模型表现优秀的主要障碍是标注数据的稀缺。基于这点，我们应用了自我超vised学习（SSL）方法，使得模型能够学习，即使标注数据稀缺。为了了解Audio-based情感识别中自我超vised学习的 utility，我们在CMU-MOSEI的音频数据中进行了自我超vised学习预训练。与先前的研究不同，我们的技术将encoded acoustic data应用于模型。我们的模型首先预训练，以找出随机遮盖的时间戳。然后，预训练模型被精度调整使用一小样本的注释数据。最终模型的性能被评估使用多个评价指标，并与基eline深度学习模型进行比较。我们发现，自我超vised学习可以一直提高模型的性能，并且效果最 pronounced  для易于分类的情感，如快乐、悲伤和愤怒。此外，我们发现自我超vised学习在嵌入特征表示中应用得更加有效，而不是传统的预训练在原始输入空间。”
</details></li>
</ul>
<hr>
<h2 id="Rapid-detection-of-soil-carbonates-by-means-of-NIR-spectroscopy-deep-learning-methods-and-phase-quantification-by-powder-Xray-diffraction"><a href="#Rapid-detection-of-soil-carbonates-by-means-of-NIR-spectroscopy-deep-learning-methods-and-phase-quantification-by-powder-Xray-diffraction" class="headerlink" title="Rapid detection of soil carbonates by means of NIR spectroscopy, deep learning methods and phase quantification by powder Xray diffraction"></a>Rapid detection of soil carbonates by means of NIR spectroscopy, deep learning methods and phase quantification by powder Xray diffraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12341">http://arxiv.org/abs/2307.12341</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lykourgos Chiniadis, Petros Tamvakis</li>
<li>for: 这个研究旨在提高农业生产和土壤属性分析，以实现生态平衡和环境可持续性。</li>
<li>methods: 本研究使用FT NIR reflectanceспектроскопия和深度学习方法来预测土壤中碳酸含量。</li>
<li>results: 研究获得了很好的预测结果，能够快速和高精度地预测土壤中碳酸含量，并且在未前seen的土壤标本上还能够获得良好的预测结果。<details>
<summary>Abstract</summary>
Soil NIR spectral absorbance/reflectance libraries are utilized towards improving agricultural production and analysis of soil properties which are key prerequisite for agroecological balance and environmental sustainability. Carbonates in particular, represent a soil property which is mostly affected even by mild, let alone extreme, changes of environmental conditions during climate change. In this study we propose a rapid and efficient way to predict carbonates content in soil by means of FT NIR reflectance spectroscopy and by use of deep learning methods. We exploited multiple machine learning methods, such as: 1) a MLP Regressor and 2) a CNN and compare their performance with other traditional ML algorithms such as PLSR, Cubist and SVM on the combined dataset of two NIR spectral libraries: KSSL (USDA), a dataset of soil samples reflectance spectra collected nationwide, and LUCAS TopSoil (European Soil Library) which contains soil sample absorbance spectra from all over the European Union, and use them to predict carbonate content on never before seen soil samples. Soil samples in KSSL and in TopSoil spectral libraries were acquired in the spectral region of visNIR, however in this study, only the NIR spectral region was utilized. Quantification of carbonates by means of Xray Diffraction is in good agreement with the volumetric method and the MLP prediction. Our work contributes to rapid carbonates content prediction in soil samples in cases where: 1) no volumetric method is available and 2) only NIR spectra absorbance data are available. Up till now and to the best of our knowledge, there exists no other study, that presents a prediction model trained on such an extensive dataset with such promising results on unseen data, undoubtedly supporting the notion that deep learning models present excellent prediction tools for soil carbonates content.
</details>
<details>
<summary>摘要</summary>
soil NIR spectral absorbance/reflectance 图书馆是用于改进农业生产和土壤性质分析，这些特性是生态平衡和环境可持续发展的关键前提。碳酸盐尤其是，它们在气候变化中，即使是轻度的环境变化，都会受到影响。本研究提出了一种快速和高效的碳酸盐含量预测方法，使用FT NIR反射спектроскопия和深度学习方法。我们利用了多种机器学习方法，如：1）多层感知网络（MLP）回归器和2）卷积神经网络（CNN），并与其他传统的机器学习算法，如：PLSR、Cubist和SVM，进行比较。我们使用了两个NIR光谱图书馆：KSSL（美国农业部）和LUCAS TopSoil（欧盟土壤图书馆）的共同数据集，这两个数据集包含了不同地区的土壤样本吸收和反射光谱数据。我们只使用了NIR光谱区域。我们通过X射Diffraction测量碳酸盐含量和MLP预测值进行比较，结果表明我们的预测模型具有良好的准确性。我们的工作可以帮助在无法使用液体方法和只有NIR光谱吸收数据时预测土壤中碳酸盐含量。到目前为止，我们没有发现任何一项研究，可以在类似的大规模数据集上提出类似的预测模型，这证明了深度学习模型在土壤碳酸盐含量预测中具有极高的预测精度。
</details></li>
</ul>
<hr>
<h2 id="TabADM-Unsupervised-Tabular-Anomaly-Detection-with-Diffusion-Models"><a href="#TabADM-Unsupervised-Tabular-Anomaly-Detection-with-Diffusion-Models" class="headerlink" title="TabADM: Unsupervised Tabular Anomaly Detection with Diffusion Models"></a>TabADM: Unsupervised Tabular Anomaly Detection with Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12336">http://arxiv.org/abs/2307.12336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guy Zamberg, Moshe Salhov, Ofir Lindenbaum, Amir Averbuch</li>
<li>for: 本研究旨在提出一种基于扩散的概率模型，用于不监督型异常检测。</li>
<li>methods: 该模型通过unique reject scheme来减弱异常样本对密度估计的影响，并在推理阶段通过确定低密度区域来识别异常样本。</li>
<li>results: 实验结果表明，该方法能够提高异常检测的能力，并且相比基eline方法具有更好的稳定性和较少的参数调整。<details>
<summary>Abstract</summary>
Tables are an abundant form of data with use cases across all scientific fields. Real-world datasets often contain anomalous samples that can negatively affect downstream analysis. In this work, we only assume access to contaminated data and present a diffusion-based probabilistic model effective for unsupervised anomaly detection. Our model is trained to learn the density of normal samples by utilizing a unique rejection scheme to attenuate the influence of anomalies on the density estimation. At inference, we identify anomalies as samples in low-density regions. We use real data to demonstrate that our method improves detection capabilities over baselines. Furthermore, our method is relatively stable to the dimension of the data and does not require extensive hyperparameter tuning.
</details>
<details>
<summary>摘要</summary>
tables是一种非常常见的数据形式，在各个科学领域中有各种应用场景。实际数据中经常存在异常样本，这些异常样本可能会对后续分析产生负面影响。在这种情况下，我们只假设有杂质数据访问，并提出了一种基于扩散的 probabilistic 模型，用于不监督的异常检测。我们的模型通过利用异常样本的独特拒绝方案来减少异常样本对density估计的影响。在推理阶段，我们将异常样本标识为density低区域中的样本。我们使用实际数据来证明我们的方法可以提高异常检测的能力，并且我们的方法相对稳定于数据维度和不需要大量的 гипер参数调整。
</details></li>
</ul>
<hr>
<h2 id="An-axiomatized-PDE-model-of-deep-neural-networks"><a href="#An-axiomatized-PDE-model-of-deep-neural-networks" class="headerlink" title="An axiomatized PDE model of deep neural networks"></a>An axiomatized PDE model of deep neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12333">http://arxiv.org/abs/2307.12333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tangjun Wang, Wenqi Tao, Chenglong Bao, Zuoqiang Shi</li>
<li>for: 研究深度神经网络（DNN）与 partiall differential equation（PDE）之间的关系，以推导DNN的普遍型PDE模型。</li>
<li>methods: 将DNN视为一个演化算符，从简单的基模型出发，通过一些合理的假设，证明演化算符实际上由湍涨-散射方程推导出来。</li>
<li>results: 根据湍涨-散射方程，提出了一种新的训练方法 для ResNet，实验验证了该方法的性能。<details>
<summary>Abstract</summary>
Inspired by the relation between deep neural network (DNN) and partial differential equations (PDEs), we study the general form of the PDE models of deep neural networks. To achieve this goal, we formulate DNN as an evolution operator from a simple base model. Based on several reasonable assumptions, we prove that the evolution operator is actually determined by convection-diffusion equation. This convection-diffusion equation model gives mathematical explanation for several effective networks. Moreover, we show that the convection-diffusion model improves the robustness and reduces the Rademacher complexity. Based on the convection-diffusion equation, we design a new training method for ResNets. Experiments validate the performance of the proposed method.
</details>
<details>
<summary>摘要</summary>
受深度神经网络（DNN）和偏微分方程（PDE）之间的关系启发，我们研究深度神经网络的总体形式PDE模型。为 достичь这个目标，我们将DNN视为一个基本模型的演化运算器。基于一些合理的假设，我们证明了演化运算器实际上是受扩散-演化方程的决定。这个扩散-演化方程模型为各种有效网络提供了数学解释。此外，我们还证明了这个模型可以提高robustness并降低Rademacher复杂度。基于扩散-演化方程，我们设计了一种新的训练方法 дляResNets。实验证明了我们的提案的性能。
</details></li>
</ul>
<hr>
<h2 id="Tackling-the-Curse-of-Dimensionality-with-Physics-Informed-Neural-Networks"><a href="#Tackling-the-Curse-of-Dimensionality-with-Physics-Informed-Neural-Networks" class="headerlink" title="Tackling the Curse of Dimensionality with Physics-Informed Neural Networks"></a>Tackling the Curse of Dimensionality with Physics-Informed Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12306">http://arxiv.org/abs/2307.12306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheyuan Hu, Khemraj Shukla, George Em Karniadakis, Kenji Kawaguchi</li>
<li>for: 解决高维partial differential equations (PDEs)问题，提高计算效率和扩展性。</li>
<li>methods: 使用Stochastic Dimension Gradient Descent (SDGD)方法，将梯度分解成不同维度的部分，随机选择每个维度的部分进行训练physics-informed neural networks (PINNs)。</li>
<li>results: 实验表明，SDGD可以快速解决许多高维PDEs问题，如Hamilton-Jacobi-Bellman (HJB)和Schrödinger方程在千个维度中的解决。例如，在单个GPU上使用SDGD和PINNs mesh-free方法解决了100,000维的非线性PDEs问题。<details>
<summary>Abstract</summary>
The curse-of-dimensionality (CoD) taxes computational resources heavily with exponentially increasing computational cost as the dimension increases. This poses great challenges in solving high-dimensional PDEs as Richard Bellman first pointed out over 60 years ago. While there has been some recent success in solving numerically partial differential equations (PDEs) in high dimensions, such computations are prohibitively expensive, and true scaling of general nonlinear PDEs to high dimensions has never been achieved. In this paper, we develop a new method of scaling up physics-informed neural networks (PINNs) to solve arbitrary high-dimensional PDEs. The new method, called Stochastic Dimension Gradient Descent (SDGD), decomposes a gradient of PDEs into pieces corresponding to different dimensions and samples randomly a subset of these dimensional pieces in each iteration of training PINNs. We theoretically prove the convergence guarantee and other desired properties of the proposed method. We experimentally demonstrate that the proposed method allows us to solve many notoriously hard high-dimensional PDEs, including the Hamilton-Jacobi-Bellman (HJB) and the Schr\"{o}dinger equations in thousands of dimensions very fast on a single GPU using the PINNs mesh-free approach. For instance, we solve nontrivial nonlinear PDEs (one HJB equation and one Black-Scholes equation) in 100,000 dimensions in 6 hours on a single GPU using SDGD with PINNs. Since SDGD is a general training methodology of PINNs, SDGD can be applied to any current and future variants of PINNs to scale them up for arbitrary high-dimensional PDEs.
</details>
<details>
<summary>摘要</summary>
科学家们长期面临的困难之一是维度的束缚，即维度的数量的增加会导致计算成本呈指数增长。这使得解决高维度偏微分方程（PDE）变得非常困难，尤其是当维度增加时。虽然在最近的几年中，有些研究人员已经成功地使用数值方法来解决高维度PDE，但这些计算仍然是非常昂贵的，而且真正的维度扩展到高维度PDE还未得到解决。在这篇论文中，我们提出了一种新的方法，即随机维度梯度下降（SDGD），用于扩展物理学 Informed Neural Networks（PINNs）来解决任意高维度PDE。SDGD方法将PDE的梯度分解为不同维度的部分，然后随机选择这些维度的部分进行每次训练PINNs。我们证明了该方法的收敛保证和其他愿望的性质。我们通过实验示例，SDGD方法可以很快地解决许多困难高维度PDE，包括汉密尔顿-雅各布-贝尔曼（HJB）和施罗德Equation在千个维度中的解决。例如，我们在6个GPU上使用SDGD方法和PINNs无格法解决100,000维度的非线性PDE，需要6个小时。由于SDGD是PINNs的一种普适的训练方法，因此SDGD可以应用于任何当前和未来的PINNs变体，以扩展它们到任意高维度PDE。
</details></li>
</ul>
<hr>
<h2 id="Physics-Informed-Machine-Learning-of-Argon-Gas-Driven-Melt-Pool-Dynamics"><a href="#Physics-Informed-Machine-Learning-of-Argon-Gas-Driven-Melt-Pool-Dynamics" class="headerlink" title="Physics-Informed Machine Learning of Argon Gas-Driven Melt Pool Dynamics"></a>Physics-Informed Machine Learning of Argon Gas-Driven Melt Pool Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12304">http://arxiv.org/abs/2307.12304</a></li>
<li>repo_url: None</li>
<li>paper_authors: R. Sharma, W. Grace Guo, M. Raissi, Y. B. Guo</li>
<li>For: 这篇论文旨在提出一种基于物理定律的机器学习方法（Physics-Informed Machine Learning，PIML），用于预测鋳件运动动力学（melt pool dynamics），并且不需要训练资料。* Methods: 这篇论文使用了一种整合神经网络与物理定律的方法，即physics-informed neural network（PINN），并且将模型常数通过数据验证。* Results: 这篇论文获得了一个高效的预测模型，可以预测鋳件运动动力学的温度、流速和压力等变量，而且不需要解决非线性的奈特-斯托克斯方程。<details>
<summary>Abstract</summary>
Melt pool dynamics in metal additive manufacturing (AM) is critical to process stability, microstructure formation, and final properties of the printed materials. Physics-based simulation including computational fluid dynamics (CFD) is the dominant approach to predict melt pool dynamics. However, the physics-based simulation approaches suffer from the inherent issue of very high computational cost. This paper provides a physics-informed machine learning (PIML) method by integrating neural networks with the governing physical laws to predict the melt pool dynamics such as temperature, velocity, and pressure without using any training data on velocity. This approach avoids solving the highly non-linear Navier-Stokes equation numerically, which significantly reduces the computational cost. The difficult-to-determine model constants of the governing equations of the melt pool can also be inferred through data-driven discovery. In addition, the physics-informed neural network (PINN) architecture has been optimized for efficient model training. The data-efficient PINN model is attributed to the soft penalty by incorporating governing partial differential equations (PDEs), initial conditions, and boundary conditions in the PINN model.
</details>
<details>
<summary>摘要</summary>
metal 添加制造（AM）中的熔Pool动力学是关键的过程稳定性、微structure形成和打印材料的最终性能。物理基础的计算 fluid dynamics（CFD）是预测熔Pool动力学的主要方法。然而，物理基础的计算方法受到非常高的计算成本的限制。本文提出了一种基于物理学习（PIML）方法，通过结合神经网络和管理物理法律来预测熔Pool动力学特性，包括温度、速度和压力，不需要使用任何速度训练数据。这种方法可以避免解决非线性的 Navier-Stokes 方程 numerically，从而减少计算成本。此外，通过数据驱动发现，可以通过迁移学习来推导Difficult-to-determine模型常数。此外， physics-informed neural network（PINN）的建立也得到了优化。数据效率的 PINN 模型归功于软罚 penalty，该 penalty包括 governing partial differential equations（PDEs）、初始条件和边界条件。
</details></li>
</ul>
<hr>
<h2 id="RANSAC-NN-Unsupervised-Image-Outlier-Detection-using-RANSAC"><a href="#RANSAC-NN-Unsupervised-Image-Outlier-Detection-using-RANSAC" class="headerlink" title="RANSAC-NN: Unsupervised Image Outlier Detection using RANSAC"></a>RANSAC-NN: Unsupervised Image Outlier Detection using RANSAC</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12301">http://arxiv.org/abs/2307.12301</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mxtsai/ransac-nn">https://github.com/mxtsai/ransac-nn</a></li>
<li>paper_authors: Chen-Han Tsai, Yu-Shao Peng</li>
<li>For: 本文提出了一种特有的无监督图像异常点检测算法RANSAC-NN，用于确保计算机视觉任务中图像数据的质量和准确性。* Methods: 本文使用了RANSAC方法，对图像进行比较，自动预测每个图像的异常分数，无需额外训练或标签信息。* Results: 对于15种多样化的数据集，RANSAC-NN在与状态对照算法进行比较中，无需调整任何超参数，一直表现出优异的成绩。此外，文章还提供了对每个RANSAC-NN组件的详细分析，以及其应用于图像涂抹检测的可能性。<details>
<summary>Abstract</summary>
Image outlier detection (OD) is crucial for ensuring the quality and accuracy of image datasets used in computer vision tasks. The majority of OD algorithms, however, have not been targeted toward image data. Consequently, the results of applying such algorithms to images are often suboptimal. In this work, we propose RANSAC-NN, a novel unsupervised OD algorithm specifically designed for images. By comparing images in a RANSAC-based approach, our algorithm automatically predicts the outlier score of each image without additional training or label information. We evaluate RANSAC-NN against state-of-the-art OD algorithms on 15 diverse datasets. Without any hyperparameter tuning, RANSAC-NN consistently performs favorably in contrast to other algorithms in almost every dataset category. Furthermore, we provide a detailed analysis to understand each RANSAC-NN component, and we demonstrate its potential applications in image mislabeled detection. Code for RANSAC-NN is provided at https://github.com/mxtsai/ransac-nn
</details>
<details>
<summary>摘要</summary>
Image outlier detection (OD) 是计算机视觉任务中关键的质量和准确性的保证。大多数OD算法却没有专门针对图像数据进行设计，因此在应用这些算法到图像时，结果通常不佳。在这种工作中，我们提出了RANSAC-NN，一种新的无监督OD算法，专门为图像设计。我们通过比较图像在RANSAC基于的方法中，自动地预测每个图像的异常分数，无需其他训练或标签信息。我们对RANSAC-NN与当前最佳OD算法进行比较，在15个多样化的数据集上进行评估。无需任何超参数调整，RANSAC-NN在大多数数据集类别中一直表现出优于其他算法。此外，我们还提供了每个RANSAC-NN组件的详细分析，并证明其在图像涂抹检测中的潜在应用。RANSAC-NN的代码可以在https://github.com/mxtsai/ransac-nn中找到。
</details></li>
</ul>
<hr>
<h2 id="ResWCAE-Biometric-Pattern-Image-Denoising-Using-Residual-Wavelet-Conditioned-Autoencoder"><a href="#ResWCAE-Biometric-Pattern-Image-Denoising-Using-Residual-Wavelet-Conditioned-Autoencoder" class="headerlink" title="ResWCAE: Biometric Pattern Image Denoising Using Residual Wavelet-Conditioned Autoencoder"></a>ResWCAE: Biometric Pattern Image Denoising Using Residual Wavelet-Conditioned Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12255">http://arxiv.org/abs/2307.12255</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youzhi Liang, Wen Liang</li>
<li>For: 这种论文主要用于提高Internet of Things（IoT）设备中的生物 metric验证系统的可靠性，特别是在高噪音水平下。* Methods: 该论文提出了一种轻量级和可靠的深度学习架构，即Residual Wavelet-Conditioned Convolutional Autoencoder（Res-WCAE），用于生物指纹图像的降噪。Res-WCAE包括两个编码器和一个解码器，其中一个编码器是图像编码器，另一个编码器是波峰编码器，用于在卷积变换Domain中使用抽象和细节子图像来取得特征表示。* Results: 实验结果表明，Res-WCAE比其他state-of-the-art降噪方法更高效，特别是在高噪音水平下的 heavily degraded fingerprint images。总的来说，Res-WCAE显示出在compact IoT设备中的潜在应用价值。<details>
<summary>Abstract</summary>
The utilization of biometric authentication with pattern images is increasingly popular in compact Internet of Things (IoT) devices. However, the reliability of such systems can be compromised by image quality issues, particularly in the presence of high levels of noise. While state-of-the-art deep learning algorithms designed for generic image denoising have shown promise, their large number of parameters and lack of optimization for unique biometric pattern retrieval make them unsuitable for these devices and scenarios. In response to these challenges, this paper proposes a lightweight and robust deep learning architecture, the Residual Wavelet-Conditioned Convolutional Autoencoder (Res-WCAE) with a Kullback-Leibler divergence (KLD) regularization, designed specifically for fingerprint image denoising. Res-WCAE comprises two encoders - an image encoder and a wavelet encoder - and one decoder. Residual connections between the image encoder and decoder are leveraged to preserve fine-grained spatial features, where the bottleneck layer conditioned on the compressed representation of features obtained from the wavelet encoder using approximation and detail subimages in the wavelet-transform domain. The effectiveness of Res-WCAE is evaluated against several state-of-the-art denoising methods, and the experimental results demonstrate that Res-WCAE outperforms these methods, particularly for heavily degraded fingerprint images in the presence of high levels of noise. Overall, Res-WCAE shows promise as a solution to the challenges faced by biometric authentication systems in compact IoT devices.
</details>
<details>
<summary>摘要</summary>
“在 compact 互联网络设备中，使用生物特征识别技术的应用正在增加。然而，这些系统的可靠性可能受到图像质量问题的影响，特别是在高水平的噪声存在下。现有的深度学习算法，特别是适应器的深度学习算法，在涉及到通用图像颜色化的应用中已经显示了损害。这些算法的问题在于它们的参数数量过多，并且不适合专门针对生物特征图像恢复。面对这些挑战，本文提出了一个轻量级和可靠的深度学习架构——差异条件条件径缩对称卷积推断器（Res-WCAE）。Res-WCAE 包括两个Encoder——图像Encoder和wavelet Encoder——以及一个解oder。在图像Encoder和解oder之间的径缩连接，使得精细空间特征保留了，并且使用径缩后的特征表示进行条件径缩。实验结果显示，Res-WCAE 在高水平的噪声下进行图像恢复时表现更好，特别是针对严重损害的生物特征图像。总的来说，Res-WCAE 显示出在 compact IoT 设备中的应用潜力。”
</details></li>
</ul>
<hr>
<h2 id="Explainable-Depression-Detection-via-Head-Motion-Patterns"><a href="#Explainable-Depression-Detection-via-Head-Motion-Patterns" class="headerlink" title="Explainable Depression Detection via Head Motion Patterns"></a>Explainable Depression Detection via Head Motion Patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12241">http://arxiv.org/abs/2307.12241</a></li>
<li>repo_url: None</li>
<li>paper_authors: Monika Gahalawat, Raul Fernandez Rojas, Tanaya Guha, Ramanathan Subramanian, Roland Goecke</li>
<li>for: 检测抑郁症状</li>
<li>methods: 基于基本头部动作单元（kinemes）的方法，包括从头部动作数据中挖掘kinemes以及从健康控制人群中学习kineme Patterns，并使用机器学习方法进行评估</li>
<li>results: 研究发现，头部动作 patrterns 可以作为抑郁症状的生物标志，并且可以通过分析重建错误的统计来进行识别。在 BlackDog 和 AVEC2013 数据集上，我们实现了最高的 F1 分数为 0.79 和 0.82，分别为二分类 episodic 薄片和视频中的抑郁症状识别。<details>
<summary>Abstract</summary>
While depression has been studied via multimodal non-verbal behavioural cues, head motion behaviour has not received much attention as a biomarker. This study demonstrates the utility of fundamental head-motion units, termed \emph{kinemes}, for depression detection by adopting two distinct approaches, and employing distinctive features: (a) discovering kinemes from head motion data corresponding to both depressed patients and healthy controls, and (b) learning kineme patterns only from healthy controls, and computing statistics derived from reconstruction errors for both the patient and control classes. Employing machine learning methods, we evaluate depression classification performance on the \emph{BlackDog} and \emph{AVEC2013} datasets. Our findings indicate that: (1) head motion patterns are effective biomarkers for detecting depressive symptoms, and (2) explanatory kineme patterns consistent with prior findings can be observed for the two classes. Overall, we achieve peak F1 scores of 0.79 and 0.82, respectively, over BlackDog and AVEC2013 for binary classification over episodic \emph{thin-slices}, and a peak F1 of 0.72 over videos for AVEC2013.
</details>
<details>
<summary>摘要</summary>
“而对于抑郁症的研究，头部运动讯号并未获得很多注意。本研究表明了抑郁检测的有用性，通过使用头部运动中的基本单元（称为“运动单元”），并运用了不同的方法和特征。我们使用机器学习方法进行评估，在“BlackDog”和“AVEC2013”数据集上进行分类。我们的结果显示：（1）头部运动模式是抑郁症的有效生物 marker，（2）可以获得解释性的运动单元模式，与先前的发现一致。总的来说，我们在“BlackDog”和“AVEC2013”上取得了最高的 F1 分数，分别为 0.79 和 0.82，以及“AVEC2013”上的影片分类中的最高 F1 分数为 0.72。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. The traditional Chinese form of the text is also provided for reference:“而对于抑郁症的研究，头部运动讯号并未获得很多注意。本研究表明了抑郁检测的有用性，通过使用头部运动中的基本单元（称为“运动单元”），并运用了不同的方法和特征。我们使用机器学习方法进行评估，在“BlackDog”和“AVEC2013”数据集上进行分类。我们的结果显示：（1）头部运动模式是抑郁症的有效生物 marker，（2）可以获得解释性的运动单元模式，与先前的发现一致。总的来说，我们在“BlackDog”和“AVEC2013”上取得了最高的 F1 分数，分别为 0.79 和 0.82，以及“AVEC2013”上的影片分类中的最高 F1 分数为 0.72。”
</details></li>
</ul>
<hr>
<h2 id="Demonstration-of-a-Response-Time-Based-Remaining-Useful-Life-RUL-Prediction-for-Software-Systems"><a href="#Demonstration-of-a-Response-Time-Based-Remaining-Useful-Life-RUL-Prediction-for-Software-Systems" class="headerlink" title="Demonstration of a Response Time Based Remaining Useful Life (RUL) Prediction for Software Systems"></a>Demonstration of a Response Time Based Remaining Useful Life (RUL) Prediction for Software Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12237">http://arxiv.org/abs/2307.12237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ray Islam, Peter Sandborn</li>
<li>for: 这个论文旨在应用PHM概念来预测软件系统的故障和寿命。</li>
<li>methods: 该论文使用了usage参数（如发布数量和类型）和性能参数（如响应时间）来预测软件系统的RUL。</li>
<li>results: 实际数据和预测模型的比较表明，PHM概念可以成功地应用于软件系统，并且可以计算出系统的寿命来做管理决策。<details>
<summary>Abstract</summary>
Prognostic and Health Management (PHM) has been widely applied to hardware systems in the electronics and non-electronics domains but has not been explored for software. While software does not decay over time, it can degrade over release cycles. Software health management is confined to diagnostic assessments that identify problems, whereas prognostic assessment potentially indicates when in the future a problem will become detrimental. Relevant research areas such as software defect prediction, software reliability prediction, predictive maintenance of software, software degradation, and software performance prediction, exist, but all of these represent diagnostic models built upon historical data, none of which can predict an RUL for software. This paper addresses the application of PHM concepts to software systems for fault predictions and RUL estimation. Specifically, this paper addresses how PHM can be used to make decisions for software systems such as version update and upgrade, module changes, system reengineering, rejuvenation, maintenance scheduling, budgeting, and total abandonment. This paper presents a method to prognostically and continuously predict the RUL of a software system based on usage parameters (e.g., the numbers and categories of releases) and performance parameters (e.g., response time). The model developed has been validated by comparing actual data, with the results that were generated by predictive models. Statistical validation (regression validation, and k-fold cross validation) has also been carried out. A case study, based on publicly available data for the Bugzilla application is presented. This case study demonstrates that PHM concepts can be applied to software systems and RUL can be calculated to make system management decisions.
</details>
<details>
<summary>摘要</summary>
《预测和软件健康管理（PHM）在硬件系统中广泛应用，但尚未探讨软件。虽然软件不会逐渐衰老，但可能会逐渐下降。软件健康管理仅仅是诊断评估，而预测评估可能可以预测未来何时会出现问题。相关的研究领域包括软件缺陷预测、软件可靠性预测、软件维护预测、软件衰老和软件性能预测，但这些都是基于历史数据建立的诊断模型，无法预测软件的寿命。本文应用PHM概念到软件系统，以预测问题和寿命。本文讨论了如何使用PHM来做软件系统的决策，包括版本更新和升级、模块更改、系统重新设计、重生、维护计划、预算和完全抛弃。本文提出了一种基于使用量和性能参数（例如版本数和响应时间）的软件系统的RUL预测方法。该模型已经验证了实际数据，并通过比较预测模型生成的结果进行了统计验证（回归验证和kfold横断验证）。一个基于公共可用数据的 Bugzilla 应用的案例研究也被提供，这种案例展示了PHM概念可以应用于软件系统，并且可以计算出软件系统的寿命以进行系统管理决策。
</details></li>
</ul>
<hr>
<h2 id="Multi-Modal-Machine-Learning-for-Assessing-Gaming-Skills-in-Online-Streaming-A-Case-Study-with-CS-GO"><a href="#Multi-Modal-Machine-Learning-for-Assessing-Gaming-Skills-in-Online-Streaming-A-Case-Study-with-CS-GO" class="headerlink" title="Multi-Modal Machine Learning for Assessing Gaming Skills in Online Streaming: A Case Study with CS:GO"></a>Multi-Modal Machine Learning for Assessing Gaming Skills in Online Streaming: A Case Study with CS:GO</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12236">http://arxiv.org/abs/2307.12236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Longxiang Zhang, Wenping Wang</li>
<li>for: 本研究旨在提供个性化推荐和服务促销给在线流媒体平台的用户。</li>
<li>methods: 本研究使用最新的端到端模型学习joint representation of multiple modalities，并进行了大量的实验 validate其效果。</li>
<li>results: 研究发现，提议的模型能够学习有意义的表示，但同时也具有识别用户的问题。未来工作将集中在解决这一问题上。<details>
<summary>Abstract</summary>
Online streaming is an emerging market that address much attention. Assessing gaming skills from videos is an important task for streaming service providers to discover talented gamers. Service providers require the information to offer customized recommendation and service promotion to their customers. Meanwhile, this is also an important multi-modal machine learning tasks since online streaming combines vision, audio and text modalities. In this study we begin by identifying flaws in the dataset and proceed to clean it manually. Then we propose several variants of latest end-to-end models to learn joint representation of multiple modalities. Through our extensive experimentation, we demonstrate the efficacy of our proposals. Moreover, we identify that our proposed models is prone to identifying users instead of learning meaningful representations. We purpose future work to address the issue in the end.
</details>
<details>
<summary>摘要</summary>
互联网直播是一个崛起的市场，吸引了很多注意。从视频中评估玩家技巧是直播服务提供商必须的重要任务，以便为客户提供个性化推荐和服务推广。同时，这也是一个重要的多modal机器学习任务，因为在线直播结合了视觉、音频和文本modalities。在这种研究中，我们开始由 dataset 中的欠点进行识别，然后手动清理它。然后，我们提出了一些最新的终端模型，以学习多modalities 的共同表示。通过我们的广泛实验，我们证明了我们的建议的有效性。此外，我们发现我们的建议模型容易被用户认出，而不是学习有意义的表示。我们未来的工作是解决这个问题。
</details></li>
</ul>
<hr>
<h2 id="EchoGLAD-Hierarchical-Graph-Neural-Networks-for-Left-Ventricle-Landmark-Detection-on-Echocardiograms"><a href="#EchoGLAD-Hierarchical-Graph-Neural-Networks-for-Left-Ventricle-Landmark-Detection-on-Echocardiograms" class="headerlink" title="EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms"></a>EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12229">http://arxiv.org/abs/2307.12229</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/masoudmo/echoglad">https://github.com/masoudmo/echoglad</a></li>
<li>paper_authors: Masoud Mokhtari, Mobina Mahdavi, Hooman Vaseli, Christina Luong, Purang Abolmaesumi, Teresa S. M. Tsang, Renjie Liao</li>
<li>for: 本研究旨在自动检测心脏左心室的四个标志位置以及计算左心室内部积分和周围肌肉约重。</li>
<li>methods: 我们提出了一种基于电子心室图像的层次图神经网络（GNN）方法，即EchoGLAD，以实现左心室标志位置检测。我们还引入了多层级指导学习框架，以实现多resolution的标志位置检测。</li>
<li>results: 我们在公共和私人数据集上进行了测试，并在ID和OOD两种设定下测试了我们的模型。在ID设定下，我们实现了左心室标志位置检测的最佳约错值（MAE）为1.46毫米和1.86毫米。而在OOD设定下，我们的模型表现比之前的方法更好，测试MAE为4.3毫米。<details>
<summary>Abstract</summary>
The functional assessment of the left ventricle chamber of the heart requires detecting four landmark locations and measuring the internal dimension of the left ventricle and the approximate mass of the surrounding muscle. The key challenge of automating this task with machine learning is the sparsity of clinical labels, i.e., only a few landmark pixels in a high-dimensional image are annotated, leading many prior works to heavily rely on isotropic label smoothing. However, such a label smoothing strategy ignores the anatomical information of the image and induces some bias. To address this challenge, we introduce an echocardiogram-based, hierarchical graph neural network (GNN) for left ventricle landmark detection (EchoGLAD). Our main contributions are: 1) a hierarchical graph representation learning framework for multi-resolution landmark detection via GNNs; 2) induced hierarchical supervision at different levels of granularity using a multi-level loss. We evaluate our model on a public and a private dataset under the in-distribution (ID) and out-of-distribution (OOD) settings. For the ID setting, we achieve the state-of-the-art mean absolute errors (MAEs) of 1.46 mm and 1.86 mm on the two datasets. Our model also shows better OOD generalization than prior works with a testing MAE of 4.3 mm.
</details>
<details>
<summary>摘要</summary>
心脏左心室功能评估需要检测四个标志点和测量左心室内部维度以及周围肌肉的大约质量。主要挑战是通过机器学习自动化这项任务的挑战是仅有一些临床标签，即只有一些标志点像素在高维度图像上被标注，导致许多先前的工作都是依赖于均匀标签平滑化。然而，这种标签平滑化策略忽略了图像的解剖学信息并且带来一定的偏见。为解决这个挑战，我们提出了一种用电声图像基于的层次图像神经网络（EchoGLAD） для左心室标志点检测。我们的主要贡献包括：1. 一种层次图像表示学习框架，通过图像神经网络来实现多分辨率标志点检测。2. 通过多级层次权重来实现层次supervision，以提高模型的精度和一致性。我们在公共和私人数据集上进行了测试，在ID设定下，我们实现了左心室标志点检测的状态各orden MaE值为1.46毫米和1.86毫米。我们的模型还在OOD设定下表现出了更好的泛化性，测试MaE值为4.3毫米。
</details></li>
</ul>
<hr>
<h2 id="The-identification-of-garbage-dumps-in-the-rural-areas-of-Cyprus-through-the-application-of-deep-learning-to-satellite-imagery"><a href="#The-identification-of-garbage-dumps-in-the-rural-areas-of-Cyprus-through-the-application-of-deep-learning-to-satellite-imagery" class="headerlink" title="The identification of garbage dumps in the rural areas of Cyprus through the application of deep learning to satellite imagery"></a>The identification of garbage dumps in the rural areas of Cyprus through the application of deep learning to satellite imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02502">http://arxiv.org/abs/2308.02502</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew Keith Wilkinson</li>
<li>For: The paper aims to address the issue of illegal garbage dumping in rural areas of Cyprus by using artificial intelligence techniques and satellite imagery to identify garbage dumps.* Methods: The authors collected a novel dataset of images and used data augmentation techniques to increase the size of the dataset. They trained an artificial neural network, specifically a convolutional neural network, to recognize the presence or absence of garbage in new images.* Results: The resulting model was able to correctly identify images containing garbage in approximately 90% of cases, demonstrating the efficacy of the approach. The authors envision that this model could form the basis of a future system that could systematically analyze the entire landscape of Cyprus to build a comprehensive “garbage” map of the island.Here’s the same information in Simplified Chinese:*  FOR: 这篇论文目标是解决Cyprus遍布的非法垃圾排放问题，使用人工智能技术和卫星影像来识别垃圾场。*  METHODS: 作者收集了一个新的图像集，并使用数据扩展技术来增加该集的大小。他们使用人工神经网络，特别是卷积神经网络，来识别新图像中是否存在垃圾。*  RESULTS: 结果表明，该模型可以正确地识别新图像中是否存在垃圾，达到了约90%的准确率。作者认为，这种方法可以成为未来系统性地分析Cyprus岛全域的垃圾地图的基础。<details>
<summary>Abstract</summary>
Garbage disposal is a challenging problem throughout the developed world. In Cyprus, as elsewhere, illegal ``fly-tipping" is a significant issue, especially in rural areas where few legal garbage disposal options exist. However, there is a lack of studies that attempt to measure the scale of this problem, and few resources available to address it. A method of automating the process of identifying garbage dumps would help counter this and provide information to the relevant authorities. The aim of this study was to investigate the degree to which artificial intelligence techniques, together with satellite imagery, can be used to identify illegal garbage dumps in the rural areas of Cyprus. This involved collecting a novel dataset of images that could be categorised as either containing, or not containing, garbage. The collection of such datasets in sufficient raw quantities is time consuming and costly. Therefore a relatively modest baseline set of images was collected, then data augmentation techniques used to increase the size of this dataset to a point where useful machine learning could occur. From this set of images an artificial neural network was trained to recognise the presence or absence of garbage in new images. A type of neural network especially suited to this task known as ``convolutional neural networks" was used. The efficacy of the resulting model was evaluated using an independently collected dataset of test images. The result was a deep learning model that could correctly identify images containing garbage in approximately 90\% of cases. It is envisaged that this model could form the basis of a future system that could systematically analyse the entire landscape of Cyprus to build a comprehensive ``garbage" map of the island.
</details>
<details>
<summary>摘要</summary>
垃圾处理是发达国家的挑战之一，特别是在贫困地区，因为有限的法定垃圾处理选择。然而，没有多少研究尝试度量这个问题的规模，以及有效的解决方案。这项研究的目的是使用人工智能技术和卫星影像来识别Cyprus郊区非法抛弃垃圾。这包括收集一个新的图像集，可以分为含有或无垃圾。收集这些图像集的时间和成本很高，因此只收集了一个有限的基线集。然后使用数据扩充技术来增加这个集的大小，使其具有可行的机器学习。通过这个集，我们训练了一个人工神经网络，可以识别新图像中是否含有垃圾。我们使用了适合这个任务的特殊类型的神经网络，即卷积神经网络。我们评估了这个模型的效果，使用独立收集的测试集。结果是，这个模型可以在90%的情况下正确地识别含有垃圾的图像。我们想象，这个模型可以成为未来对Cyprus岛全景进行系统性分析，建立一个“垃圾”地图。
</details></li>
</ul>
<hr>
<h2 id="Geometry-Aware-Adaptation-for-Pretrained-Models"><a href="#Geometry-Aware-Adaptation-for-Pretrained-Models" class="headerlink" title="Geometry-Aware Adaptation for Pretrained Models"></a>Geometry-Aware Adaptation for Pretrained Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12226">http://arxiv.org/abs/2307.12226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicholas Roberts, Xintong Li, Dyah Adila, Sonia Cromp, Tzu-Heng Huang, Jitian Zhao, Frederic Sala</li>
<li>for: 这个论文是为了提高零下学习模型的性能而设计的，特别是在标签空间中的类别数量很大的情况下。</li>
<li>methods: 这个论文使用了一种简单的方法，即在标签空间中使用Fréchetmean来代替标准的argmax预测规则，以提高模型的预测性能。</li>
<li>results: 该方法可以在ImageNet上获得最高达29.7%的相对提升，并且可以扩展到数万个类别。在没有外部度量的情况下，使用自动生成的metric自Embeddings中可以获得10.5%的提升。<details>
<summary>Abstract</summary>
Machine learning models -- including prominent zero-shot models -- are often trained on datasets whose labels are only a small proportion of a larger label space. Such spaces are commonly equipped with a metric that relates the labels via distances between them. We propose a simple approach to exploit this information to adapt the trained model to reliably predict new classes -- or, in the case of zero-shot prediction, to improve its performance -- without any additional training. Our technique is a drop-in replacement of the standard prediction rule, swapping argmax with the Fr\'echet mean. We provide a comprehensive theoretical analysis for this approach, studying (i) learning-theoretic results trading off label space diameter, sample complexity, and model dimension, (ii) characterizations of the full range of scenarios in which it is possible to predict any unobserved class, and (iii) an optimal active learning-like next class selection procedure to obtain optimal training classes for when it is not possible to predict the entire range of unobserved classes. Empirically, using easily-available external metrics, our proposed approach, Loki, gains up to 29.7% relative improvement over SimCLR on ImageNet and scales to hundreds of thousands of classes. When no such metric is available, Loki can use self-derived metrics from class embeddings and obtains a 10.5% improvement on pretrained zero-shot models such as CLIP.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Improving-Out-of-Distribution-Robustness-of-Classifiers-via-Generative-Interpolation"><a href="#Improving-Out-of-Distribution-Robustness-of-Classifiers-via-Generative-Interpolation" class="headerlink" title="Improving Out-of-Distribution Robustness of Classifiers via Generative Interpolation"></a>Improving Out-of-Distribution Robustness of Classifiers via Generative Interpolation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12219">http://arxiv.org/abs/2307.12219</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoyue Bai, Ceyuan Yang, Yinghao Xu, S. -H. Gary Chan, Bolei Zhou</li>
<li>for: 该研究旨在提高神经网络模型对不同分布数据的鲁棒性。</li>
<li>methods: 该研究使用生成模型作为数据增强来源，通过混合多个领域生成器来生成多样化的异常样本。</li>
<li>results: 实验显示，提posed方法可以Explicitly increases the diversity of training domains and achieves consistent improvements over baselines across datasets and multiple different distribution shifts.<details>
<summary>Abstract</summary>
Deep neural networks achieve superior performance for learning from independent and identically distributed (i.i.d.) data. However, their performance deteriorates significantly when handling out-of-distribution (OoD) data, where the training and test are drawn from different distributions. In this paper, we explore utilizing the generative models as a data augmentation source for improving out-of-distribution robustness of neural classifiers. Specifically, we develop a simple yet effective method called Generative Interpolation to fuse generative models trained from multiple domains for synthesizing diverse OoD samples. Training a generative model directly on the source domains tends to suffer from mode collapse and sometimes amplifies the data bias. Instead, we first train a StyleGAN model on one source domain and then fine-tune it on the other domains, resulting in many correlated generators where their model parameters have the same initialization thus are aligned. We then linearly interpolate the model parameters of the generators to spawn new sets of generators. Such interpolated generators are used as an extra data augmentation source to train the classifiers. The interpolation coefficients can flexibly control the augmentation direction and strength. In addition, a style-mixing mechanism is applied to further improve the diversity of the generated OoD samples. Our experiments show that the proposed method explicitly increases the diversity of training domains and achieves consistent improvements over baselines across datasets and multiple different distribution shifts.
</details>
<details>
<summary>摘要</summary>
Training a generative model directly on the source domains can suffer from mode collapse and amplify data bias. Instead, we first train a StyleGAN model on one source domain and then fine-tune it on the other domains, resulting in many correlated generators with aligned model parameters. We then linearly interpolate the model parameters of the generators to spawn new sets of generators. Such interpolated generators are used as an extra data augmentation source to train the classifiers. The interpolation coefficients can flexibly control the augmentation direction and strength.In addition, we apply a style-mixing mechanism to further improve the diversity of the generated OoD samples. Our experiments show that the proposed method explicitly increases the diversity of training domains and achieves consistent improvements over baselines across datasets and multiple different distribution shifts.
</details></li>
</ul>
<hr>
<h2 id="Mental-Workload-Estimation-with-Electroencephalogram-Signals-by-Combining-Multi-Space-Deep-Models"><a href="#Mental-Workload-Estimation-with-Electroencephalogram-Signals-by-Combining-Multi-Space-Deep-Models" class="headerlink" title="Mental Workload Estimation with Electroencephalogram Signals by Combining Multi-Space Deep Models"></a>Mental Workload Estimation with Electroencephalogram Signals by Combining Multi-Space Deep Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02409">http://arxiv.org/abs/2308.02409</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hong-Hai Nguyen, Ngumimi Karen Iyortsuun, Hyung-Jeong Yang, Guee-Sang Lee, Soo-Hyung Kim</li>
<li>for: This paper aims to classify mental workload into three states and estimate continuum levels to improve early detection of mental health problems.</li>
<li>methods: The method combines multiple dimensions of space and uses Temporal Convolutional Networks in the time domain, as well as a new architecture called the Multi-Dimensional Residual Block in the frequency domain.</li>
<li>results: The proposed method is expected to provide accurate estimates of mental workload and improve the early detection of mental health problems.Here’s the text in Simplified Chinese:</li>
<li>for: 这篇论文目标是分类心理工作负荷为三种状态并估算连续水平，以提高早期心理健康问题的检测。</li>
<li>methods: 该方法结合了多个空间维度，使用时域卷积神经网络和频域中的新架构——多维度剩余块，以获得最佳的心理估算结果。</li>
<li>results: 该方法预期能够提供准确的心理工作负荷估算结果，并且有助于早期检测心理健康问题。<details>
<summary>Abstract</summary>
The human brain is in a continuous state of activity during both work and rest. Mental activity is a daily process, and when the brain is overworked, it can have negative effects on human health. In recent years, great attention has been paid to early detection of mental health problems because it can help prevent serious health problems and improve quality of life. Several signals are used to assess mental state, but the electroencephalogram (EEG) is widely used by researchers because of the large amount of information it provides about the brain. This paper aims to classify mental workload into three states and estimate continuum levels. Our method combines multiple dimensions of space to achieve the best results for mental estimation. In the time domain approach, we use Temporal Convolutional Networks, and in the frequency domain, we propose a new architecture called the Multi-Dimensional Residual Block, which combines residual blocks.
</details>
<details>
<summary>摘要</summary>
人脑在工作和休息时都处于不断活动的状态。 mental activity是每天的过程，当脑部过度劳累时，可能会对人类健康产生负面影响。 recent years，大量关注 Early Detection of mental health problems，因为这可以帮助预防严重的健康问题并提高生活质量。 Several signals are used to assess mental state，但是 electroencephalogram (EEG) 是研究人员最广泛使用的，因为它可以提供大量关于脑部的信息。 This paper aims to classify mental workload into three states and estimate continuum levels. Our method combines multiple dimensions of space to achieve the best results for mental estimation. In the time domain approach, we use Temporal Convolutional Networks，and in the frequency domain, we propose a new architecture called the Multi-Dimensional Residual Block，which combines residual blocks.
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Agents-For-Attacking-Inaudible-Voice-Activated-Devices"><a href="#Adversarial-Agents-For-Attacking-Inaudible-Voice-Activated-Devices" class="headerlink" title="Adversarial Agents For Attacking Inaudible Voice Activated Devices"></a>Adversarial Agents For Attacking Inaudible Voice Activated Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12204">http://arxiv.org/abs/2307.12204</a></li>
<li>repo_url: None</li>
<li>paper_authors: Forrest McKee, David Noever</li>
<li>For: The paper focuses on the risk of inaudible attacks on voice-activated devices and the need for new cybersecurity measures to address this emerging threat.* Methods: The authors use reinforcement learning to analyze the vulnerability of novel Internet of Things (IoT) configurations to inaudible attacks, and they evaluate six reinforcement learning algorithms to determine the most effective approach.* Results: The authors find that Deep-Q learning with exploitation is the most effective algorithm for rapidly owning all nodes in a baseline network model, highlighting the critical need for understanding non-conventional networks and new cybersecurity measures to address the growing threat of inaudible attacks.<details>
<summary>Abstract</summary>
The paper applies reinforcement learning to novel Internet of Thing configurations. Our analysis of inaudible attacks on voice-activated devices confirms the alarming risk factor of 7.6 out of 10, underlining significant security vulnerabilities scored independently by NIST National Vulnerability Database (NVD). Our baseline network model showcases a scenario in which an attacker uses inaudible voice commands to gain unauthorized access to confidential information on a secured laptop. We simulated many attack scenarios on this baseline network model, revealing the potential for mass exploitation of interconnected devices to discover and own privileged information through physical access without adding new hardware or amplifying device skills. Using Microsoft's CyberBattleSim framework, we evaluated six reinforcement learning algorithms and found that Deep-Q learning with exploitation proved optimal, leading to rapid ownership of all nodes in fewer steps. Our findings underscore the critical need for understanding non-conventional networks and new cybersecurity measures in an ever-expanding digital landscape, particularly those characterized by mobile devices, voice activation, and non-linear microphones susceptible to malicious actors operating stealth attacks in the near-ultrasound or inaudible ranges. By 2024, this new attack surface might encompass more digital voice assistants than people on the planet yet offer fewer remedies than conventional patching or firmware fixes since the inaudible attacks arise inherently from the microphone design and digital signal processing.
</details>
<details>
<summary>摘要</summary>
文章应用再强化学习来解决新型互联网设备的配置问题。我们对无声攻击的voice-activated设备进行分析，并确认了攻击性能isk因子为7.6/10，这标识了严重的安全漏洞。我们的基线网络模型示例了一个攻击者通过无声语音命令获取未授权访问 laptop 上的机密信息的场景。我们在这个基线网络模型上模拟了许多攻击场景，发现了可以通过物理访问而不需要新硬件或提高设备技能的攻击的潜在性。使用Microsoft的CyberBattleSim框架，我们评估了六种再强化学习算法，发现了使用攻击的 Deep-Q 学习算法，可以在 fewer steps 中获得所有节点的权限。我们的发现强调了理解非传统网络和新的安全措施在不断扩展的数字景观中的重要性，特别是包括移动设备、voice activation和非线性 microphone 在内的攻击 surface。到2024年，这个新的攻击表面可能会包括更多的数字voice助手than人类在地球上， yet offer fewer remedies than conventional patching or firmware fixes since the inaudible attacks arise inherently from the microphone design and digital signal processing。
</details></li>
</ul>
<hr>
<h2 id="NCART-Neural-Classification-and-Regression-Tree-for-Tabular-Data"><a href="#NCART-Neural-Classification-and-Regression-Tree-for-Tabular-Data" class="headerlink" title="NCART: Neural Classification and Regression Tree for Tabular Data"></a>NCART: Neural Classification and Regression Tree for Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12198">http://arxiv.org/abs/2307.12198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaqi Luo, Shixin Xu</li>
<li>for: 本研究旨在提出一种可解释性神经网络模型，以增强深度学习模型在表格数据分析中的可靠性和可读性。</li>
<li>methods: 本研究提出了一种名为神经分类和回归树（NCART）的新型可解释性神经网络模型，它将多个可导式无知树纳入神经网络架构中，以维持神经网络的可读性，同时充分利用神经网络的端到端能力。</li>
<li>results: 对于不同的数据集和深度学习模型，NCART模型在可靠性和可读性两个方面具有突出的优势，并且在大规模数据处理和小规模数据处理中均有出色的表现，比较稳定和可靠。<details>
<summary>Abstract</summary>
Deep learning models have become popular in the analysis of tabular data, as they address the limitations of decision trees and enable valuable applications like semi-supervised learning, online learning, and transfer learning. However, these deep-learning approaches often encounter a trade-off. On one hand, they can be computationally expensive when dealing with large-scale or high-dimensional datasets. On the other hand, they may lack interpretability and may not be suitable for small-scale datasets. In this study, we propose a novel interpretable neural network called Neural Classification and Regression Tree (NCART) to overcome these challenges. NCART is a modified version of Residual Networks that replaces fully-connected layers with multiple differentiable oblivious decision trees. By integrating decision trees into the architecture, NCART maintains its interpretability while benefiting from the end-to-end capabilities of neural networks. The simplicity of the NCART architecture makes it well-suited for datasets of varying sizes and reduces computational costs compared to state-of-the-art deep learning models. Extensive numerical experiments demonstrate the superior performance of NCART compared to existing deep learning models, establishing it as a strong competitor to tree-based models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Monadic-Deep-Learning"><a href="#Monadic-Deep-Learning" class="headerlink" title="Monadic Deep Learning"></a>Monadic Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12187">http://arxiv.org/abs/2307.12187</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ThoughtWorksInc/monadic-deep-learning">https://github.com/ThoughtWorksInc/monadic-deep-learning</a></li>
<li>paper_authors: Bo Yang, Zhihao Zhang Kirisame Marisa, Kai Shi</li>
<li>for: 这篇论文是为了解决在静态类型语言中运行神经网络模型时，拥有自动演算的问题。</li>
<li>methods: 这篇论文使用了一种新的自动演算方法，可以在静态类型函数中包含多个可训练变量，并且可以与元语言进行兼容。它还使用了一些幺征和幺征转换，使用者可以创建具有卷积的表达式，表示动态神经网络。</li>
<li>results: 该论文的实验结果表明，使用DeepLearning.scala可以创建复杂的神经网络表达式，并且仍然保持类型安全。<details>
<summary>Abstract</summary>
The Java and Scala community has built a very successful big data ecosystem. However, most of neural networks running on it are modeled in dynamically typed programming languages. These dynamically typed deep learning frameworks treat neural networks as differentiable expressions that contain many trainable variable, and perform automatic differentiation on those expressions when training them.   Until 2019, none of the learning frameworks in statically typed languages provided the expressive power of traditional frameworks. Their users are not able to use custom algorithms unless creating plenty of boilerplate code for hard-coded back-propagation.   We solved this problem in DeepLearning.scala 2. Our contributions are:   1. We discovered a novel approach to perform automatic differentiation in reverse mode for statically typed functions that contain multiple trainable variable, and can interoperate freely with the metalanguage.   2. We designed a set of monads and monad transformers, which allow users to create monadic expressions that represent dynamic neural networks.   3. Along with these monads, we provide some applicative functors, to perform multiple calculations in parallel.   With these features, users of DeepLearning.scala were able to create complex neural networks in an intuitive and concise way, and still maintain type safety.
</details>
<details>
<summary>摘要</summary>
Java和Scala社区已经建立了一个非常成功的大数据生态系统。然而，大多数在其上运行的神经网络是使用动态类型编程语言进行模型化的。这些动态类型深度学习框架将神经网络视为可导数学表达，并在训练时自动进行差分。 until 2019, 在静态类型语言中的学习框架都没有提供了传统框架的表达力。他们的用户无法使用自定义算法，除非创建大量的硬编码反射分布。我们解决了这个问题，在 DeepLearning.scala 2 中我们的贡献包括：1. 我们发现了一种新的方法，可以在静态类型函数中自动进行差分，并且可以与金属语言进行自由交互。2. 我们设计了一组幂道和幂道变换，允许用户创建幂道表达式，表示动态神经网络。3. 同时，我们提供了一些应用程序赋值，以进行多个计算并行执行。与这些特性相结合，DeepLearning.scala 的用户可以在直观和简洁的方式创建复杂的神经网络，同时保持类型安全性。
</details></li>
</ul>
<hr>
<h2 id="Machine-learning-discovers-invariants-of-braids-and-flat-braids"><a href="#Machine-learning-discovers-invariants-of-braids-and-flat-braids" class="headerlink" title="Machine learning discovers invariants of braids and flat braids"></a>Machine learning discovers invariants of braids and flat braids</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12185">http://arxiv.org/abs/2307.12185</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexei Lisitsa, Mateo Salles, Alexei Vernitski</li>
<li>for: 这个论文用机器学习分类布里耳（或平面布里耳）的例子，并将其分为不重要和重要两类。</li>
<li>methods: 这个论文使用了指导学习，使用神经网络（多层感知器）进行超级vised学习。</li>
<li>results: 通过这个方法，我们能够解释布里耳的结构，并证明它们的结构为真理。最终，我们发现了新的便利的布里耳 invariants，包括完全的平面布里耳 invariants。<details>
<summary>Abstract</summary>
We use machine learning to classify examples of braids (or flat braids) as trivial or non-trivial. Our ML takes form of supervised learning using neural networks (multilayer perceptrons). When they achieve good results in classification, we are able to interpret their structure as mathematical conjectures and then prove these conjectures as theorems. As a result, we find new convenient invariants of braids, including a complete invariant of flat braids.
</details>
<details>
<summary>摘要</summary>
我们使用机器学习来分类拥有别种辫的示例（或平面辫）为轻量级或非轻量级。我们的ML形式为指导学习使用神经网络（多层感知器）。当它们在分类中获得良好的结果时，我们可以解释它们的结构为数学 conjecture，然后证明这些推论为定理。因此，我们发现新的便利的辫 invariants，包括完整的平面辫 invariants。Note: "辫" (braids) is a word that is commonly used in Chinese to refer to braids, but it can also have other meanings depending on the context. In this translation, I have used the more specific term "平面辫" (flat braids) to refer to the specific type of braids being discussed in the text.
</details></li>
</ul>
<hr>
<h2 id="Prototype-Driven-and-Multi-Expert-Integrated-Multi-Modal-MR-Brain-Tumor-Image-Segmentation"><a href="#Prototype-Driven-and-Multi-Expert-Integrated-Multi-Modal-MR-Brain-Tumor-Image-Segmentation" class="headerlink" title="Prototype-Driven and Multi-Expert Integrated Multi-Modal MR Brain Tumor Image Segmentation"></a>Prototype-Driven and Multi-Expert Integrated Multi-Modal MR Brain Tumor Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12180">http://arxiv.org/abs/2307.12180</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/linzy0227/pdminet">https://github.com/linzy0227/pdminet</a></li>
<li>paper_authors: Yafei Zhang, Zhiyuan Li, Huafeng Li, Dapeng Tao</li>
<li>for: 这种研究旨在提高多模式磁共振（MR）脑肿瘤图像分割方法，以便更好地确定和地理位置脑肿瘤子区域。</li>
<li>methods: 该方法使用脑肿瘤prototype驱动的多专家融合，以高亮每个脑肿瘤子区域的特征。具体来说，我们提出了一种互传机制，将不同模式特征传输给每个另一个，以解决单模式特征缺乏信息的问题。此外，我们还提出了一种基于学习的脑肿瘤代表特征表示和融合方法，将代表特征与脑肿瘤特征相结合，生成相应的活化地图。</li>
<li>results: 对三个竞赛脑肿瘤分割数据集进行实验，研究结果表明，该方法的性能较高，可以更好地确定和地理位置脑肿瘤子区域。<details>
<summary>Abstract</summary>
For multi-modal magnetic resonance (MR) brain tumor image segmentation, current methods usually directly extract the discriminative features from input images for tumor sub-region category determination and localization. However, the impact of information aliasing caused by the mutual inclusion of tumor sub-regions is often ignored. Moreover, existing methods usually do not take tailored efforts to highlight the single tumor sub-region features. To this end, a multi-modal MR brain tumor segmentation method with tumor prototype-driven and multi-expert integration is proposed. It could highlight the features of each tumor sub-region under the guidance of tumor prototypes. Specifically, to obtain the prototypes with complete information, we propose a mutual transmission mechanism to transfer different modal features to each other to address the issues raised by insufficient information on single-modal features. Furthermore, we devise a prototype-driven feature representation and fusion method with the learned prototypes, which implants the prototypes into tumor features and generates corresponding activation maps. With the activation maps, the sub-region features consistent with the prototype category can be highlighted. A key information enhancement and fusion strategy with multi-expert integration is designed to further improve the segmentation performance. The strategy can integrate the features from different layers of the extra feature extraction network and the features highlighted by the prototypes. Experimental results on three competition brain tumor segmentation datasets prove the superiority of the proposed method.
</details>
<details>
<summary>摘要</summary>
为了实现多模态核磁共振（MR）脑肿瘤图像分割，当前方法通常直接从输入图像中提取特征来确定和地理化肿瘤子区域。然而，现有方法通常忽略了信息干扰，这是由肿瘤子区域之间的共包含引起的。此外，现有方法通常不会为单个肿瘤子区域特征进行特化努力。为此，我们提出了一种多模态MR脑肿瘤分割方法，该方法可以根据肿瘤prototype驱动并进行多专家集成。它可以在肿瘤prototype的指导下高亮每个肿瘤子区域的特征。具体来说，为了获得完整信息的肿瘤prototype，我们提出了一种相互传输机制，将不同模态特征传输给每个模式，以解决单模态特征不具备完整信息的问题。此外，我们还提出了一种基于学习的肿瘤prototype驱动特征表示和融合方法，将肿瘤prototype直接植入到肿瘤特征中，并生成相应的活化地图。通过活化地图，可以高亮与肿瘤prototype类别相符的子区域特征。为了进一步提高分割性能，我们还设计了一种多专家集成和信息增强策略，该策略可以集成不同层次的特性提取网络和肿瘤prototype中的特征。实验结果表明，提出的方法在三个竞赛脑肿瘤分割数据集上表现出优异性。
</details></li>
</ul>
<hr>
<h2 id="Learn-to-Compress-LtC-Efficient-Learning-based-Streaming-Video-Analytics"><a href="#Learn-to-Compress-LtC-Efficient-Learning-based-Streaming-Video-Analytics" class="headerlink" title="Learn to Compress (LtC): Efficient Learning-based Streaming Video Analytics"></a>Learn to Compress (LtC): Efficient Learning-based Streaming Video Analytics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12171">http://arxiv.org/abs/2307.12171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quazi Mishkatul Alam, Israat Haque, Nael Abu-Ghazaleh</li>
<li>for: 这篇论文旨在建立一个高效的对当地视频分析管道，以节省网络带宽和电力耗用。</li>
<li>methods: 论文提出了一个名为LtC的协同框架，它在视频来源和分析服务器之间实现了高效的视频流减少。LtC使用了全功能的分析算法作为教师，将视频流分解为不同区域，并将这些区域节省为高质量，而其他区域则进行攻击性压缩。此外，LtC还具有一个基于特征差分的时间滤波器，以删除不具新讯息的帧。</li>
<li>results: LtC比最近发表的流行框架使用28-35% less 网络带宽，具有45%短的响应延迟，而且与最近发表的流行框架相比，LtC的分析性能相似。<details>
<summary>Abstract</summary>
Video analytics are often performed as cloud services in edge settings, mainly to offload computation, and also in situations where the results are not directly consumed at the video sensors. Sending high-quality video data from the edge devices can be expensive both in terms of bandwidth and power use. In order to build a streaming video analytics pipeline that makes efficient use of these resources, it is therefore imperative to reduce the size of the video stream. Traditional video compression algorithms are unaware of the semantics of the video, and can be both inefficient and harmful for the analytics performance. In this paper, we introduce LtC, a collaborative framework between the video source and the analytics server, that efficiently learns to reduce the video streams within an analytics pipeline. Specifically, LtC uses the full-fledged analytics algorithm at the server as a teacher to train a lightweight student neural network, which is then deployed at the video source. The student network is trained to comprehend the semantic significance of various regions within the videos, which is used to differentially preserve the crucial regions in high quality while the remaining regions undergo aggressive compression. Furthermore, LtC also incorporates a novel temporal filtering algorithm based on feature-differencing to omit transmitting frames that do not contribute new information. Overall, LtC is able to use 28-35% less bandwidth and has up to 45% shorter response delay compared to recently published state of the art streaming frameworks while achieving similar analytics performance.
</details>
<details>
<summary>摘要</summary>
视频分析通常作为云服务在边缘设置下进行，主要是为了减轻计算负担，以及在视频感知不直接在视频传感器上进行处理。从边缘设备传输高质量视频数据可能会产生巨大的带宽和电力成本。为建立高效的流动视频分析管道，因此需要减小视频流。传统的视频压缩算法不理解视频的 semantics，可能会导致不具有效果和对分析性能有害。在本文中，我们介绍了 LtC，一个在视频源和分析服务器之间合作的框架。Specifically，LtC 使用全功能的分析算法作为教师，将轻量级神经网络部署到视频源上，以提高分析管道中视频流的压缩率。此外，LtC 还包括一种基于特征差分的新的时间滤波算法，以便忽略不含新信息的帧。总的来说，LtC 可以使用28-35%的带宽和45%的响应延迟，相比最新的流动框架，而达到类似的分析性能。
</details></li>
</ul>
<hr>
<h2 id="Optimized-Network-Architectures-for-Large-Language-Model-Training-with-Billions-of-Parameters"><a href="#Optimized-Network-Architectures-for-Large-Language-Model-Training-with-Billions-of-Parameters" class="headerlink" title="Optimized Network Architectures for Large Language Model Training with Billions of Parameters"></a>Optimized Network Architectures for Large Language Model Training with Billions of Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12169">http://arxiv.org/abs/2307.12169</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiyang Wang, Manya Ghobadi, Kayvon Shakeri, Ying Zhang, Naader Hasani</li>
<li>for: 这个论文挑战了任何到任何网络的训练大语言模型（LLM）的传统模式。</li>
<li>methods: 我们提出了一种新的网络架构，它准确地反映了 LLM 的通信需求。我们将集群分成多个 GPU 之间的非阻塞any-to-any高速连接，称为 HB Domain。只有在 HB Domain 内部的 GPU 之间进行通信。</li>
<li>results: 我们的提议可以减少网络成本达到 75%，而无需妥协 LLM 训练性能。<details>
<summary>Abstract</summary>
This paper challenges the well-established paradigm for building any-to-any networks for training Large Language Models (LLMs). We show that LLMs exhibit a unique communication pattern where only small groups of GPUs require high-bandwidth any-to-any communication within them, to achieve near-optimal training performance. Across these groups of GPUs, the communication is insignificant, sparse, and homogeneous. We propose a new network architecture that closely resembles the communication requirement of LLMs. Our architecture partitions the cluster into sets of GPUs interconnected with non-blocking any-to-any high-bandwidth interconnects that we call HB domains. Across the HB domains, the network only connects GPUs with communication demands. We call this network a "rail-only" connection, and show that our proposed architecture reduces the network cost by up to 75% compared to the state-of-the-art any-to-any Clos networks without compromising the performance of LLM training.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Facial-Point-Graphs-for-Amyotrophic-Lateral-Sclerosis-Identification"><a href="#Facial-Point-Graphs-for-Amyotrophic-Lateral-Sclerosis-Identification" class="headerlink" title="Facial Point Graphs for Amyotrophic Lateral Sclerosis Identification"></a>Facial Point Graphs for Amyotrophic Lateral Sclerosis Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12159">http://arxiv.org/abs/2307.12159</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nícolas Barbosa Gomes, Arissa Yoshida, Mateus Roder, Guilherme Camargo de Oliveira, João Paulo Papa</li>
<li>for: 旨在早期诊断amyotrophic lateral sclerosis (ALS)，以提供更好的治疗机会，提高病人的生活质量。</li>
<li>methods: 利用计算机方法分析病人的面部表情，通过学习面部图形信息，自动识别ALS。</li>
<li>results: 在渥太华Neuroface数据集上，提议的方法比前一代结果更高，表现出优异的潜力。<details>
<summary>Abstract</summary>
Identifying Amyotrophic Lateral Sclerosis (ALS) in its early stages is essential for establishing the beginning of treatment, enriching the outlook, and enhancing the overall well-being of those affected individuals. However, early diagnosis and detecting the disease's signs is not straightforward. A simpler and cheaper way arises by analyzing the patient's facial expressions through computational methods. When a patient with ALS engages in specific actions, e.g., opening their mouth, the movement of specific facial muscles differs from that observed in a healthy individual. This paper proposes Facial Point Graphs to learn information from the geometry of facial images to identify ALS automatically. The experimental outcomes in the Toronto Neuroface dataset show the proposed approach outperformed state-of-the-art results, fostering promising developments in the area.
</details>
<details>
<summary>摘要</summary>
早期诊断阿勒阵性肌萎缩综合征（ALS）是非常重要的，可以为患者提供更好的诊断、改善结果和全面健康状况。然而，早期诊断和识别病种的征象不是一件容易的事情。这项研究提出使用计算机方法分析患者的面部表情，以自动识别ALS。当患者进行特定动作时，如打开嘴巴，健康人的面部肌肉运动与ALS患者不同。这项研究使用面部点云图来学习面部图像的几何特征，并在多伦多神经面数据集上进行实验，结果表明该方法在比较前一些现有方法的情况下，表现出了更好的成果，这些成果有助于推动该领域的发展。
</details></li>
</ul>
<hr>
<h2 id="DIP-RL-Demonstration-Inferred-Preference-Learning-in-Minecraft"><a href="#DIP-RL-Demonstration-Inferred-Preference-Learning-in-Minecraft" class="headerlink" title="DIP-RL: Demonstration-Inferred Preference Learning in Minecraft"></a>DIP-RL: Demonstration-Inferred Preference Learning in Minecraft</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12158">http://arxiv.org/abs/2307.12158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ellen Novoseller, Vinicius G. Goecks, David Watkins, Josh Miller, Nicholas Waytowich</li>
<li>for: 本研究旨在解决在无结构的真实世界中，RL算法学习Sequential Decision-Making时，因为奖励信号不明确而无法正确地捕捉所需行为的问题。</li>
<li>methods: 本研究提出了Demonstration-Inferred Preference Reinforcement Learning（DIP-RL）算法，利用人类示范在三种不同的方式，包括训练自动编码器、在RL训练批处理中使用示范数据，以及从示范数据中推断行为偏好来学习一个奖励函数，以 guidRL算法学习。</li>
<li>results: 结果表明，DIP-RL可以引导RL算法学习一个奖励函数，该奖励函数反映人类的偏好，并且DIP-RL相比基eline表现竞争力强。<details>
<summary>Abstract</summary>
In machine learning for sequential decision-making, an algorithmic agent learns to interact with an environment while receiving feedback in the form of a reward signal. However, in many unstructured real-world settings, such a reward signal is unknown and humans cannot reliably craft a reward signal that correctly captures desired behavior. To solve tasks in such unstructured and open-ended environments, we present Demonstration-Inferred Preference Reinforcement Learning (DIP-RL), an algorithm that leverages human demonstrations in three distinct ways, including training an autoencoder, seeding reinforcement learning (RL) training batches with demonstration data, and inferring preferences over behaviors to learn a reward function to guide RL. We evaluate DIP-RL in a tree-chopping task in Minecraft. Results suggest that the method can guide an RL agent to learn a reward function that reflects human preferences and that DIP-RL performs competitively relative to baselines. DIP-RL is inspired by our previous work on combining demonstrations and pairwise preferences in Minecraft, which was awarded a research prize at the 2022 NeurIPS MineRL BASALT competition, Learning from Human Feedback in Minecraft. Example trajectory rollouts of DIP-RL and baselines are located at https://sites.google.com/view/dip-rl.
</details>
<details>
<summary>摘要</summary>
机器学习的决策行为中，一个算法式代理人 learns to interact with环境，收到一个奖励信号作为反馈。但在许多未知的实际环境中，这个奖励信号是未知的，人类无法可靠地设计一个正确地捕捉 desired behavior的奖励信号。为解决这类未知和开放的环境中的任务，我们提出了 Demonstration-Inferred Preference Reinforcement Learning (DIP-RL)，一个算法，它利用人类示范的方式，包括训练 autoencoder、将示范资料用于RL 训练批次的seed、以及对行为偏好进行推断，以学习一个奖励函数，导引RL。我们在 Minecraft 中进行了树割任务的评估，结果表明 DI P-RL 可以将RL代理人学习一个与人类偏好相符的奖励函数，并且 DI P-RL 与基准相比表现竞争。DIP-RL 是我们之前的 Combining Demonstrations and Pairwise Preferences in Minecraft 研究的推展，该研究在2022年 NeurIPS MineRL BASALT 竞赛中获奖。具体的示例 Rollout 可以在 https://sites.google.com/view/dip-rl 获取。
</details></li>
</ul>
<hr>
<h2 id="Identifying-contributors-to-supply-chain-outcomes-in-a-multi-echelon-setting-a-decentralised-approach"><a href="#Identifying-contributors-to-supply-chain-outcomes-in-a-multi-echelon-setting-a-decentralised-approach" class="headerlink" title="Identifying contributors to supply chain outcomes in a multi-echelon setting: a decentralised approach"></a>Identifying contributors to supply chain outcomes in a multi-echelon setting: a decentralised approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12157">http://arxiv.org/abs/2307.12157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Schoepf, Jack Foster, Alexandra Brintrup</li>
<li>for: 这篇论文目的是解决多层供应链中 metric 变化的原因难以确定的问题，尤其是在供应链内部分不可见的情况下。</li>
<li>methods: 该论文提议使用可解释人工智能 для分布式计算产品质量变化的原因。该方法不需要供应链成员分享数据，因为所有计算都发生在分布式环境中。</li>
<li>results: 实验结果表明，使用该方法可以更好地检测产品质量变化的原因，比起中央化使用Shapley添加тив解释法。<details>
<summary>Abstract</summary>
Organisations often struggle to identify the causes of change in metrics such as product quality and delivery duration. This task becomes increasingly challenging when the cause lies outside of company borders in multi-echelon supply chains that are only partially observable. Although traditional supply chain management has advocated for data sharing to gain better insights, this does not take place in practice due to data privacy concerns. We propose the use of explainable artificial intelligence for decentralised computing of estimated contributions to a metric of interest in a multi-stage production process. This approach mitigates the need to convince supply chain actors to share data, as all computations occur in a decentralised manner. Our method is empirically validated using data collected from a real multi-stage manufacturing process. The results demonstrate the effectiveness of our approach in detecting the source of quality variations compared to a centralised approach using Shapley additive explanations.
</details>
<details>
<summary>摘要</summary>
企业们经常难以确定生产质量和交付时间的变化的原因。这个任务在多层供应链中，尤其是在只有部分可见的情况下，变得更加困难。传统的供应链管理强调了数据分享，以获得更好的洞察，但在实践中，这并不总是可行的，因为数据隐私问题。我们建议使用可解释人工智能 для分布式计算估算变量的贡献，以避免需要供应链 Actors 分享数据。我们的方法在实验 validate 中被证明，可以准确地检测质量变化的来源，比中央化使用 Shapley 添加тив性解释更有效。
</details></li>
</ul>
<hr>
<h2 id="Real-Time-Neural-Video-Recovery-and-Enhancement-on-Mobile-Devices"><a href="#Real-Time-Neural-Video-Recovery-and-Enhancement-on-Mobile-Devices" class="headerlink" title="Real-Time Neural Video Recovery and Enhancement on Mobile Devices"></a>Real-Time Neural Video Recovery and Enhancement on Mobile Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12152">http://arxiv.org/abs/2307.12152</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Zhaoyuan He, Yifan Yang, Lili Qiu, Kyoungjun Park</li>
<li>for: 这个论文是为了优化移动设备上的视频流处理而写的。</li>
<li>methods: 这个论文使用了一种新的视频帧恢复算法、一种新的超解算法和一种接收器增强视频比特率适应算法。</li>
<li>results: 该论文的实验表明，该approach可以在不同的网络环境下支持30帧&#x2F;秒的实时增强，并且可以提高视频流经验质量（Quality of Experience，QoE）24%-82%。<details>
<summary>Abstract</summary>
As mobile devices become increasingly popular for video streaming, it's crucial to optimize the streaming experience for these devices. Although deep learning-based video enhancement techniques are gaining attention, most of them cannot support real-time enhancement on mobile devices. Additionally, many of these techniques are focused solely on super-resolution and cannot handle partial or complete loss or corruption of video frames, which is common on the Internet and wireless networks.   To overcome these challenges, we present a novel approach in this paper. Our approach consists of (i) a novel video frame recovery scheme, (ii) a new super-resolution algorithm, and (iii) a receiver enhancement-aware video bit rate adaptation algorithm. We have implemented our approach on an iPhone 12, and it can support 30 frames per second (FPS). We have evaluated our approach in various networks such as WiFi, 3G, 4G, and 5G networks. Our evaluation shows that our approach enables real-time enhancement and results in a significant increase in video QoE (Quality of Experience) of 24\% - 82\% in our video streaming system.
</details>
<details>
<summary>摘要</summary>
As mobile devices become increasingly popular for video streaming, it's crucial to optimize the streaming experience for these devices. Although deep learning-based video enhancement techniques are gaining attention, most of them cannot support real-time enhancement on mobile devices. Additionally, many of these techniques are focused solely on super-resolution and cannot handle partial or complete loss or corruption of video frames, which is common on the Internet and wireless networks.  To overcome these challenges, we present a novel approach in this paper. Our approach consists of (i) a novel video frame recovery scheme, (ii) a new super-resolution algorithm, and (iii) a receiver enhancement-aware video bit rate adaptation algorithm. We have implemented our approach on an iPhone 12, and it can support 30 frames per second (FPS). We have evaluated our approach in various networks such as WiFi, 3G, 4G, and 5G networks. Our evaluation shows that our approach enables real-time enhancement and results in a significant increase in video QoE (Quality of Experience) of 24% - 82% in our video streaming system.
</details></li>
</ul>
<hr>
<h2 id="Learned-Gridification-for-Efficient-Point-Cloud-Processing"><a href="#Learned-Gridification-for-Efficient-Point-Cloud-Processing" class="headerlink" title="Learned Gridification for Efficient Point Cloud Processing"></a>Learned Gridification for Efficient Point Cloud Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14354">http://arxiv.org/abs/2307.14354</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/computri/gridifier">https://github.com/computri/gridifier</a></li>
<li>paper_authors: Putri A. van der Linden, David W. Romero, Erik J. Bekkers</li>
<li>for: 这 paper 的目的是解决点云处理中的可插义问题，使得点云处理可以更加有效率。</li>
<li>methods: 这 paper 使用了学习的 gridification 技术来将点云转换成一个紧凑的、规则的网格，然后使用这个网格来进行后续的操作。</li>
<li>results: 这 paper 通过理论和实验分析，证明 gridified 网络可以更好地扩展到大规模点云数据，同时保持竞争性的 результа们。<details>
<summary>Abstract</summary>
Neural operations that rely on neighborhood information are much more expensive when deployed on point clouds than on grid data due to the irregular distances between points in a point cloud. In a grid, on the other hand, we can compute the kernel only once and reuse it for all query positions. As a result, operations that rely on neighborhood information scale much worse for point clouds than for grid data, specially for large inputs and large neighborhoods.   In this work, we address the scalability issue of point cloud methods by tackling its root cause: the irregularity of the data. We propose learnable gridification as the first step in a point cloud processing pipeline to transform the point cloud into a compact, regular grid. Thanks to gridification, subsequent layers can use operations defined on regular grids, e.g., Conv3D, which scale much better than native point cloud methods. We then extend gridification to point cloud to point cloud tasks, e.g., segmentation, by adding a learnable de-gridification step at the end of the point cloud processing pipeline to map the compact, regular grid back to its original point cloud form. Through theoretical and empirical analysis, we show that gridified networks scale better in terms of memory and time than networks directly applied on raw point cloud data, while being able to achieve competitive results. Our code is publicly available at https://github.com/computri/gridifier.
</details>
<details>
<summary>摘要</summary>
神经操作依赖 neighboorhood 信息在点云上比在格点数据上更加昂贵，因为点云中点的距离不规则。而在格点数据上，我们可以一次计算核函数，然后重用其于所有查询位置。因此，基于 neighborhood 信息的操作在点云上尤其是大输入和大 neighboorhood 时会扩散很快。在这种情况下，我们解决点云处理的扩散问题的根本原因——点云数据的不规则性。我们提出了可学习的 gridification 来将点云转换成一个紧凑、规则的网格。由于 gridification，后续层可以使用基于网格的操作，例如 Conv3D，这些操作在规模上能够更好地扩展。我们还扩展了 gridification 到点云到点云任务，例如分割，通过添加一个可学习的 de-gridification 步骤，将紧凑的网格还原回原始的点云形式。我们通过理论和实验分析表明，gridified 网络在内存和时间上比直接应用于原始点云数据更加高效，同时能够达到竞争性的结果。我们的代码可以在 https://github.com/computri/gridifier 上获取。
</details></li>
</ul>
<hr>
<h2 id="CorrFL-Correlation-Based-Neural-Network-Architecture-for-Unavailability-Concerns-in-a-Heterogeneous-IoT-Environment"><a href="#CorrFL-Correlation-Based-Neural-Network-Architecture-for-Unavailability-Concerns-in-a-Heterogeneous-IoT-Environment" class="headerlink" title="CorrFL: Correlation-Based Neural Network Architecture for Unavailability Concerns in a Heterogeneous IoT Environment"></a>CorrFL: Correlation-Based Neural Network Architecture for Unavailability Concerns in a Heterogeneous IoT Environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12149">http://arxiv.org/abs/2307.12149</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Western-OC2-Lab/CorrFL">https://github.com/Western-OC2-Lab/CorrFL</a></li>
<li>paper_authors: Ibrahim Shaer, Abdallah Shami</li>
<li>for: 解决 Federated Learning 中模型结构不同和 IoT 节点不可用的问题</li>
<li>methods: 提出了 Correlation-based Federated Learning（CorrFL）方法，通过 проекting 模型权重到共同 latent space 来Address 模型不同性，并最小化缺 absent 模型的重建loss</li>
<li>results: 通过对一个实际用 caso 进行评估，对比 CorrFL 模型和不同用 caso 的 referential model，得到 CorrFL 模型在预测性能和数据交换量的影响下表现更优于 referential model。<details>
<summary>Abstract</summary>
The Federated Learning (FL) paradigm faces several challenges that limit its application in real-world environments. These challenges include the local models' architecture heterogeneity and the unavailability of distributed Internet of Things (IoT) nodes due to connectivity problems. These factors posit the question of "how can the available models fill the training gap of the unavailable models?". This question is referred to as the "Oblique Federated Learning" problem. This problem is encountered in the studied environment that includes distributed IoT nodes responsible for predicting CO2 concentrations. This paper proposes the Correlation-based FL (CorrFL) approach influenced by the representational learning field to address this problem. CorrFL projects the various model weights to a common latent space to address the model heterogeneity. Its loss function minimizes the reconstruction loss when models are absent and maximizes the correlation between the generated models. The latter factor is critical because of the intersection of the feature spaces of the IoT devices. CorrFL is evaluated on a realistic use case, involving the unavailability of one IoT device and heightened activity levels that reflect occupancy. The generated CorrFL models for the unavailable IoT device from the available ones trained on the new environment are compared against models trained on different use cases, referred to as the benchmark model. The evaluation criteria combine the mean absolute error (MAE) of predictions and the impact of the amount of exchanged data on the prediction performance improvement. Through a comprehensive experimental procedure, the CorrFL model outperformed the benchmark model in every criterion.
</details>
<details>
<summary>摘要</summary>
联邦学习（FL）模式面临多个挑战，限制其在实际环境中的应用。这些挑战包括本地模型的架构多样性和分布式互联物联网（IoT）节点的无法访问问题。这个问题被称为“偏置联邦学习”问题。这个问题在包括分布式IoT节点，负责预测二氧化碳浓度的环境中被研究。这篇文章提出了基于相关学习（CorrFL）方法，以解决这个问题。CorrFL方法将不同模型的 weights 投射到共同的潜在空间，以解决模型多样性。它的损失函数最小化缺失模型时的重建损失，并 maximizes 模型之间的相互相关性。这个第二个因素是critical，因为IoT设备的特征空间之间的交汇。CorrFL方法被评估在实际的使用案例中，包括一个缺失IoT设备和高水平的活动水平，反映occupancy。生成的CorrFL模型在缺失IoT设备上培育的新环境中，与不同使用案例中的参考模型（benchmark model）进行比较。评估标准包括预测误差的总平均误差（MAE）和预测性能改善的资料交换量的影响。通过充分的实验程序，CorrFL模型在每个标准中都高于参考模型。
</details></li>
</ul>
<hr>
<h2 id="Applications-of-Machine-Learning-to-Modelling-and-Analysing-Dynamical-Systems"><a href="#Applications-of-Machine-Learning-to-Modelling-and-Analysing-Dynamical-Systems" class="headerlink" title="Applications of Machine Learning to Modelling and Analysing Dynamical Systems"></a>Applications of Machine Learning to Modelling and Analysing Dynamical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03763">http://arxiv.org/abs/2308.03763</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vedanta Thapar</li>
<li>for: 这个论文主要研究了使用物理学 Informed Neural Networks 分析非线性汉密尔顿动力系统，具有一个运动量的第一 интеграル。</li>
<li>methods: 该论文提出了一种结合现有汉密尔顿神经网络结构的 Adaptable Symplectic Recurrent Neural Networks 架构，该架构可以保持汉密尔顿方程和相对空间的 симплектиче结构，同时预测动力学行为的整个参数空间。</li>
<li>results: 该论文表明，该架构在预测汉密尔顿动力学中，特别是在含有多个参数的潜能中表现出色，显著超过了之前的神经网络预测。furthermore, the authors demonstrate the robustness of their method by applying it to the nonlinear Henon-Heiles potential under chaotic, quasiperiodic and periodic conditions.<details>
<summary>Abstract</summary>
We explore the use of Physics Informed Neural Networks to analyse nonlinear Hamiltonian Dynamical Systems with a first integral of motion. In this work, we propose an architecture which combines existing Hamiltonian Neural Network structures into Adaptable Symplectic Recurrent Neural Networks which preserve Hamilton's equations as well as the symplectic structure of phase space while predicting dynamics for the entire parameter space. This architecture is found to significantly outperform previously proposed neural networks when predicting Hamiltonian dynamics especially in potentials which contain multiple parameters. We demonstrate its robustness using the nonlinear Henon-Heiles potential under chaotic, quasiperiodic and periodic conditions.   The second problem we tackle is whether we can use the high dimensional nonlinear capabilities of neural networks to predict the dynamics of a Hamiltonian system given only partial information of the same. Hence we attempt to take advantage of Long Short Term Memory networks to implement Takens' embedding theorem and construct a delay embedding of the system followed by mapping the topologically invariant attractor to the true form. This architecture is then layered with Adaptable Symplectic nets to allow for predictions which preserve the structure of Hamilton's equations. We show that this method works efficiently for single parameter potentials and provides accurate predictions even over long periods of time.
</details>
<details>
<summary>摘要</summary>
我们探讨使用物理 Informed Neural Networks（PINNs）分析非线性哈密顿动力系统，并研究一种可适应射步器的 Hamiltonian Neural Network 结构，以保持哈密顿方程和相似性结构的阶段空间。我们提议一种结合现有 Hamiltonian Neural Network 结构的 Adaptable Symplectic Recurrent Neural Networks（ASRNNs），可以在整个参数空间预测动力学。我们发现这种结构在多参数potential中表现出色， especialy in potentials containing multiple parameters.我们通过非线性 Henon-Heiles potential的混沌、 quasi-periodic 和 periodic 条件进行了 robustness 测试。第二个问题是可以使用高维非线性神经网络预测哈密顿系统的动力学，即使只有部分系统的信息。因此，我们尝试使用 Long Short Term Memory 网络实现 Takens 嵌入定理，并将系统的延迟嵌入映射到真实的形式。然后，我们将 Adaptable Symplectic nets 层在这个嵌入中，以保持哈密顿方程的结构。我们发现这种方法可以高效地预测单参数 potentials，并提供了精度的预测，即使在长时间内。
</details></li>
</ul>
<hr>
<h2 id="A-Vision-for-Cleaner-Rivers-Harnessing-Snapshot-Hyperspectral-Imaging-to-Detect-Macro-Plastic-Litter"><a href="#A-Vision-for-Cleaner-Rivers-Harnessing-Snapshot-Hyperspectral-Imaging-to-Detect-Macro-Plastic-Litter" class="headerlink" title="A Vision for Cleaner Rivers: Harnessing Snapshot Hyperspectral Imaging to Detect Macro-Plastic Litter"></a>A Vision for Cleaner Rivers: Harnessing Snapshot Hyperspectral Imaging to Detect Macro-Plastic Litter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12145">http://arxiv.org/abs/2307.12145</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/river-lab/hyperspectral_macro_plastic_detection">https://github.com/river-lab/hyperspectral_macro_plastic_detection</a></li>
<li>paper_authors: Nathaniel Hanson, Ahmet Demirkaya, Deniz Erdoğmuş, Aron Stubbins, Taşkın Padır, Tales Imbiriba</li>
<li>For: 本研究旨在提出一种计算成像方法来快速、自动地检测废弃塑料杂物在河流环境中的扩散。* Methods: 该研究使用可见短波infrared hyperspectral成像技术进行废弃塑料杂物的检测，并采用机器学习分类方法来提高检测精度。* Results: 实验结果表明，使用Snapshot Visible-Shortwave Infrared hyperspectral成像技术可以在实时Tracking废弃塑料杂物中实现高检测精度，特别是在复杂的场景下。 Code、数据和模型都可以在线获取：<a target="_blank" rel="noopener" href="https://github.com/RIVeR-Lab/hyperspectral_macro_plastic_detection%E3%80%82">https://github.com/RIVeR-Lab/hyperspectral_macro_plastic_detection。</a><details>
<summary>Abstract</summary>
Plastic waste entering the riverine harms local ecosystems leading to negative ecological and economic impacts. Large parcels of plastic waste are transported from inland to oceans leading to a global scale problem of floating debris fields. In this context, efficient and automatized monitoring of mismanaged plastic waste is paramount. To address this problem, we analyze the feasibility of macro-plastic litter detection using computational imaging approaches in river-like scenarios. We enable near-real-time tracking of partially submerged plastics by using snapshot Visible-Shortwave Infrared hyperspectral imaging. Our experiments indicate that imaging strategies associated with machine learning classification approaches can lead to high detection accuracy even in challenging scenarios, especially when leveraging hyperspectral data and nonlinear classifiers. All code, data, and models are available online: https://github.com/RIVeR-Lab/hyperspectral_macro_plastic_detection.
</details>
<details>
<summary>摘要</summary>
пластик废弃进入河流系统会对当地生态系统造成负面影响，导致生态和经济上的负面影响。大量废弃 пласти克材料从内陆运输到海洋，导致全球范围内漂浮垃圾场景。在这种情况下，高效和自动化的废弃 пласти克碎屑监测是非常重要。为解决这个问题，我们分析了使用计算成像方法检测废弃 macro  пласти克碎屑的可行性。我们通过使用快照 Visible-Shortwave Infrared  hyperspectral成像获得了近实时检测部分抛光的废弃 пласти克的能力。我们的实验表明，使用计算机视觉分类方法可以在复杂的场景中获得高检测精度，特别是当使用 hyperspectral 数据和非线性分类器时。所有代码、数据和模型都可以在 GitHub 上获取：https://github.com/RIVeR-Lab/hyperspectral_macro_plastic_detection。
</details></li>
</ul>
<hr>
<h2 id="Emergence-of-Adaptive-Circadian-Rhythms-in-Deep-Reinforcement-Learning"><a href="#Emergence-of-Adaptive-Circadian-Rhythms-in-Deep-Reinforcement-Learning" class="headerlink" title="Emergence of Adaptive Circadian Rhythms in Deep Reinforcement Learning"></a>Emergence of Adaptive Circadian Rhythms in Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12143">http://arxiv.org/abs/2307.12143</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aqeel13932/mn_project">https://github.com/aqeel13932/mn_project</a></li>
<li>paper_authors: Aqeel Labash, Florian Fletzer, Daniel Majoral, Raul Vicente</li>
<li>for: 这项研究旨在研究深度学习Agent中的 circadian-like 征性rhythm的出现。</li>
<li>methods: 作者在一个可靠 periodic variation 的环境中部署了 Agent，并通过解决一个搜寻任务来学习。作者系统地描述了 Agent 的行为 während learning, 并证明了 Agent 内部的 rhythm 的出现是内在的和可调整的。</li>
<li>results: 作者通过分析 bifurcation 和 phase response 曲线表示了人工神经元发展出 dynamics 以支持环境 rhythm 的内化。从动力系统视角来看， adapting 进程由神经元动力学中的稳定 periodic orbit 的出现和相应的 phase response 响应而进行。<details>
<summary>Abstract</summary>
Adapting to regularities of the environment is critical for biological organisms to anticipate events and plan. A prominent example is the circadian rhythm corresponding to the internalization by organisms of the $24$-hour period of the Earth's rotation. In this work, we study the emergence of circadian-like rhythms in deep reinforcement learning agents. In particular, we deployed agents in an environment with a reliable periodic variation while solving a foraging task. We systematically characterize the agent's behavior during learning and demonstrate the emergence of a rhythm that is endogenous and entrainable. Interestingly, the internal rhythm adapts to shifts in the phase of the environmental signal without any re-training. Furthermore, we show via bifurcation and phase response curve analyses how artificial neurons develop dynamics to support the internalization of the environmental rhythm. From a dynamical systems view, we demonstrate that the adaptation proceeds by the emergence of a stable periodic orbit in the neuron dynamics with a phase response that allows an optimal phase synchronisation between the agent's dynamics and the environmental rhythm.
</details>
<details>
<summary>摘要</summary>
适应环境的规律是生物体存活的关键，以预测事件和规划。一个典型的例子是生物体内部的Circadian rhythm，即通过生物体内部的$24$-hour期的地球旋转的内化。在这项工作中，我们研究了深度学习代理人在环境中的Circadian-like rhythm的出现。特别是，我们在一个可靠的 periodic variation 环境中部署了代理人，并在学习过程中系统地描述代理人的行为。我们发现，代理人在学习过程中发展了一种内生的rhythm，该rhythm可以适应环境的阶段的变化，而无需重新训练。此外，我们通过分支和相位回快分析，证明了人工神经元的动力学发展了以支持内化环境的rhythm。从动力学视角来看，适应过程是通过神经元动力学中的稳定 periodic orbit 的出现，并且该 periodic orbit 的相位响应允许生物体动力学和环境的相位同步。
</details></li>
</ul>
<hr>
<h2 id="Unlocking-Carbon-Reduction-Potential-with-Reinforcement-Learning-for-the-Three-Dimensional-Loading-Capacitated-Vehicle-Routing-Problem"><a href="#Unlocking-Carbon-Reduction-Potential-with-Reinforcement-Learning-for-the-Three-Dimensional-Loading-Capacitated-Vehicle-Routing-Problem" class="headerlink" title="Unlocking Carbon Reduction Potential with Reinforcement Learning for the Three-Dimensional Loading Capacitated Vehicle Routing Problem"></a>Unlocking Carbon Reduction Potential with Reinforcement Learning for the Three-Dimensional Loading Capacitated Vehicle Routing Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12136">http://arxiv.org/abs/2307.12136</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Schoepf, Stephen Mak, Julian Senoner, Liming Xu, Netland Torbjörn, Alexandra Brintrup</li>
<li>For: The paper aims to improve the efficiency of heavy goods vehicle routing in the supply chain delivery system, with a focus on increasing the loading efficiency and reducing carbon emissions.* Methods: The paper proposes using reinforcement learning to solve the three-dimensional loading capacitated vehicle routing problem, which has not been previously studied in the literature. The authors claim that their method scales approximately linearly with the problem size and can handle large-scale logistics optimization.* Results: The authors demonstrate the effectiveness of their reinforcement learning model by benchmarking it against state-of-the-art methods and showing that it performs within an average gap of 3.83% to 8.10% compared to established methods. They also claim that their model lays the foundation for this research stream and represents a promising first step towards large-scale logistics optimization with reinforcement learning.Here is the simplified Chinese text for the three information points:* 用途：文章目的是提高庞大货物运输系统中的卡车路径规划效率，尤其是提高荷载效率和减少碳排放。* 方法：文章提出使用强化学习解决三维荷载限制的卡车路径规划问题，这问题在操作研究领域已经广泛研究，但是没有使用强化学习来解决这个问题。作者们称其方法的时间复杂度接近线性增长，可以承受大规模的物流优化。* 结果：作者们通过对state-of-the-art方法进行比较，证明其模型在三维荷载限制卡车路径规划问题上的性能准确性在3.83%到8.10%之间。他们还称，其模型不仅表现出了扩大物流优化的潜力，还为这一研究流程提供了基础。<details>
<summary>Abstract</summary>
Heavy goods vehicles are vital backbones of the supply chain delivery system but also contribute significantly to carbon emissions with only 60% loading efficiency in the United Kingdom. Collaborative vehicle routing has been proposed as a solution to increase efficiency, but challenges remain to make this a possibility. One key challenge is the efficient computation of viable solutions for co-loading and routing. Current operations research methods suffer from non-linear scaling with increasing problem size and are therefore bound to limited geographic areas to compute results in time for day-to-day operations. This only allows for local optima in routing and leaves global optimisation potential untouched. We develop a reinforcement learning model to solve the three-dimensional loading capacitated vehicle routing problem in approximately linear time. While this problem has been studied extensively in operations research, no publications on solving it with reinforcement learning exist. We demonstrate the favourable scaling of our reinforcement learning model and benchmark our routing performance against state-of-the-art methods. The model performs within an average gap of 3.83% to 8.10% compared to established methods. Our model not only represents a promising first step towards large-scale logistics optimisation with reinforcement learning but also lays the foundation for this research stream.
</details>
<details>
<summary>摘要</summary>
厉害货物车辆是供应链交通系统的重要脊梁，但它们也对碳排放做出了重要贡献。在英国，厉害货物车辆的加载效率只有60%， Collaborative vehicle routing 被提议为一种解决方案，但是还有一些挑战需要解决。一个关键的挑战是计算可行的协同加载和路由解决方案的有效性。现有的运筹学方法受到增加的问题大小的非线性缩放，因此只能在有限的地理区域内计算结果，这只能确定本地最佳解决方案，留下全球优化潜力。我们开发了一个强化学习模型，以解决三维协同加载货物车辆路由问题。这个问题在运筹学中已经广泛研究，但是没有关于使用强化学习解决这个问题的出版物。我们示出了我们模型的有利扩展性，并将其比较了现有的路由性能。我们的模型与现有方法的性能差距在3.83%到8.10%之间。我们的模型不仅表现出了有 promise的首步，也为这个研究流量开创了基础。
</details></li>
</ul>
<hr>
<h2 id="The-Sample-Complexity-of-Multi-Distribution-Learning-for-VC-Classes"><a href="#The-Sample-Complexity-of-Multi-Distribution-Learning-for-VC-Classes" class="headerlink" title="The Sample Complexity of Multi-Distribution Learning for VC Classes"></a>The Sample Complexity of Multi-Distribution Learning for VC Classes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12135">http://arxiv.org/abs/2307.12135</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranjal Awasthi, Nika Haghtalab, Eric Zhao</li>
<li>for: 这篇论文旨在探讨多分布学习的自然推广，即在多个数据分布下学习的情况。</li>
<li>methods: 该论文使用了游戏动力学的思想来解决多分布学习中的一些挑战。</li>
<li>results: 研究人员通过提出新的算法和分析方法，成功地降低了多分布学习中的样本复杂性下界。<details>
<summary>Abstract</summary>
Multi-distribution learning is a natural generalization of PAC learning to settings with multiple data distributions. There remains a significant gap between the known upper and lower bounds for PAC-learnable classes. In particular, though we understand the sample complexity of learning a VC dimension d class on $k$ distributions to be $O(\epsilon^{-2} \ln(k)(d + k) + \min\{\epsilon^{-1} dk, \epsilon^{-4} \ln(k) d\})$, the best lower bound is $\Omega(\epsilon^{-2}(d + k \ln(k)))$. We discuss recent progress on this problem and some hurdles that are fundamental to the use of game dynamics in statistical learning.
</details>
<details>
<summary>摘要</summary>
多分布学习是自然推广的PAC学习设置中的多个数据分布。尚未知道PAC可学习的类的差距。特别是，虽然我们理解了学习VCdimension d类型在k个分布上的样本复杂度为O（ε^{-2} ln(k)(d + k) + мин\{\ε^{-1} dk, ε^{-4} ln(k) d\}），但最好的下界为Ω（ε^{-2}(d + k ln(k))）。我们讨论了这个问题的最新进展和使用游戏动力学在统计学习中的核心障碍。
</details></li>
</ul>
<hr>
<h2 id="AI-on-the-Road-A-Comprehensive-Analysis-of-Traffic-Accidents-and-Accident-Detection-System-in-Smart-Cities"><a href="#AI-on-the-Road-A-Comprehensive-Analysis-of-Traffic-Accidents-and-Accident-Detection-System-in-Smart-Cities" class="headerlink" title="AI on the Road: A Comprehensive Analysis of Traffic Accidents and Accident Detection System in Smart Cities"></a>AI on the Road: A Comprehensive Analysis of Traffic Accidents and Accident Detection System in Smart Cities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12128">http://arxiv.org/abs/2307.12128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Victor Adewopo, Nelly Elsayed, Zag Elsayed, Murat Ozer, Victoria Wangia-Anderson, Ahmed Abdelgawad</li>
<li>for: 提高交通管理和交通事故预防</li>
<li>methods: 使用交通监控摄像头和动作识别系统检测和响应交通事故</li>
<li>results: 提高交通管理和交通事故严重性<details>
<summary>Abstract</summary>
Accident detection and traffic analysis is a critical component of smart city and autonomous transportation systems that can reduce accident frequency, severity and improve overall traffic management. This paper presents a comprehensive analysis of traffic accidents in different regions across the United States using data from the National Highway Traffic Safety Administration (NHTSA) Crash Report Sampling System (CRSS). To address the challenges of accident detection and traffic analysis, this paper proposes a framework that uses traffic surveillance cameras and action recognition systems to detect and respond to traffic accidents spontaneously. Integrating the proposed framework with emergency services will harness the power of traffic cameras and machine learning algorithms to create an efficient solution for responding to traffic accidents and reducing human errors. Advanced intelligence technologies, such as the proposed accident detection systems in smart cities, will improve traffic management and traffic accident severity. Overall, this study provides valuable insights into traffic accidents in the US and presents a practical solution to enhance the safety and efficiency of transportation systems.
</details>
<details>
<summary>摘要</summary>
智能城市和自主交通系统中的意外检测和交通分析是一项重要 комponet，可以降低意外频率、严重性和改善总体交通管理。这篇论文对美国各地不同地区的交通意外进行了全面的分析，使用国家公路交通安全管理局（NHTSA）的交通事故报告采样系统（CRSS）的数据。为了解决意外检测和交通分析的挑战，这篇论文提出了一个框架，该框架使用交通把握摄像头和行为识别系统来自动检测和应对交通意外。将该框架与急救服务集成，将让交通摄像头和机器学习算法帮助创造一种高效的交通意外应急应对解决方案。智能技术，如提出的交通意外检测系统，将改善交通管理和交通意外严重性。总的来说，这篇论文对美国交通意外进行了价值的分析，并提出了实用的解决方案，以提高交通系统的安全和效率。
</details></li>
</ul>
<hr>
<h2 id="Synthesis-of-Batik-Motifs-using-a-Diffusion-–-Generative-Adversarial-Network"><a href="#Synthesis-of-Batik-Motifs-using-a-Diffusion-–-Generative-Adversarial-Network" class="headerlink" title="Synthesis of Batik Motifs using a Diffusion – Generative Adversarial Network"></a>Synthesis of Batik Motifs using a Diffusion – Generative Adversarial Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12122">http://arxiv.org/abs/2307.12122</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/octadion/diffusion-stylegan2-ada-pytorch">https://github.com/octadion/diffusion-stylegan2-ada-pytorch</a></li>
<li>paper_authors: One Octadion, Novanto Yudistira, Diva Kurnianingtyas</li>
<li>For: 协助batik设计师或手工艺术家创造独特和高品质的batik模样，以减少生产时间和成本。* Methods: 使用StyleGAN2-Ada和Diffusion技术生成真实和高品质的synthetic batik模样，并对模型架构进行调整，使用了高品质的batik数据集。* Results: 根据质量和量itative评估，模型能够生成authentic和高品质的batik模样，具有细部艺术变化。<details>
<summary>Abstract</summary>
Batik, a unique blend of art and craftsmanship, is a distinct artistic and technological creation for Indonesian society. Research on batik motifs is primarily focused on classification. However, further studies may extend to the synthesis of batik patterns. Generative Adversarial Networks (GANs) have been an important deep learning model for generating synthetic data, but often face challenges in the stability and consistency of results. This research focuses on the use of StyleGAN2-Ada and Diffusion techniques to produce realistic and high-quality synthetic batik patterns. StyleGAN2-Ada is a variation of the GAN model that separates the style and content aspects in an image, whereas diffusion techniques introduce random noise into the data. In the context of batik, StyleGAN2-Ada and Diffusion are used to produce realistic synthetic batik patterns. This study also made adjustments to the model architecture and used a well-curated batik dataset. The main goal is to assist batik designers or craftsmen in producing unique and quality batik motifs with efficient production time and costs. Based on qualitative and quantitative evaluations, the results show that the model tested is capable of producing authentic and quality batik patterns, with finer details and rich artistic variations. The dataset and code can be accessed here:https://github.com/octadion/diffusion-stylegan2-ada-pytorch
</details>
<details>
<summary>摘要</summary>
batik，一种独特的艺术和手工艺术，是印度尼西亚社会的独特艺术和技术创作。研究 batik 图案主要集中在分类方面，但可能会扩展到batik 图案的合成。生成 adversarial 网络（GANs）是深度学习模型，可以生成合成数据，但常面临稳定性和一致性的挑战。本研究使用 StyleGAN2-Ada 和扩散技术生成高质量和真实的合成 batik 图案。StyleGAN2-Ada 是 GAN 模型中的一种变体，可以将图像中的风格和内容分开，而扩散技术则是在数据中引入随机噪音。在 batik 的 context中，StyleGAN2-Ada 和扩散被用来生成真实的合成 batik 图案。本研究还对模型结构进行了调整，使用了高质量的 batik 数据集。目标是帮助 batik 设计师或手工艺师生成唯一和高质量的 batik 图案，降低生产时间和成本。根据 качеitative 和量化的评价，结果表明模型测试可以生成authentic 和高质量的 batik 图案，具有细节和艺术变化。数据集和代码可以在以下链接中获取：https://github.com/octadion/diffusion-stylegan2-ada-pytorch
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/23/cs.LG_2023_07_23/" data-id="cllsj9wye001ruv8872s2hj16" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/23/cs.SD_2023_07_23/" class="article-date">
  <time datetime="2023-07-22T16:00:00.000Z" itemprop="datePublished">2023-07-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/23/cs.SD_2023_07_23/">cs.SD - 2023-07-23 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-meta-learning-scheme-for-fast-accent-domain-expansion-in-Mandarin-speech-recognition"><a href="#A-meta-learning-scheme-for-fast-accent-domain-expansion-in-Mandarin-speech-recognition" class="headerlink" title="A meta learning scheme for fast accent domain expansion in Mandarin speech recognition"></a>A meta learning scheme for fast accent domain expansion in Mandarin speech recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12262">http://arxiv.org/abs/2307.12262</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziwei Zhu, Changhao Shan, Bihong Zhang, Jian Yu</li>
<li>for: 这篇论文的目的是实现中文自动识别（ASR）中的腔调领域扩展，并且不会对中文ASR的性能造成损害。</li>
<li>methods: 这篇论文使用了元学习技术来实现快速的腔调领域扩展，这种技术可以在多个领域中学习通用的关系，而不是仅对特定领域进行适材化。</li>
<li>results: 这篇论文的方法在腔调领域扩展任务中表现出色，相比基准模型，它的改进率为37%，并且在大量数据下也显示了4%的改进率。<details>
<summary>Abstract</summary>
Spoken languages show significant variation across mandarin and accent. Despite the high performance of mandarin automatic speech recognition (ASR), accent ASR is still a challenge task. In this paper, we introduce meta-learning techniques for fast accent domain expansion in mandarin speech recognition, which expands the field of accents without deteriorating the performance of mandarin ASR. Meta-learning or learn-to-learn can learn general relation in multi domains not only for over-fitting a specific domain. So we select meta-learning in the domain expansion task. This more essential learning will cause improved performance on accent domain extension tasks. We combine the methods of meta learning and freeze of model parameters, which makes the recognition performance more stable in different cases and the training faster about 20%. Our approach significantly outperforms other methods about 3% relatively in the accent domain expansion task. Compared to the baseline model, it improves relatively 37% under the condition that the mandarin test set remains unchanged. In addition, it also proved this method to be effective on a large amount of data with a relative performance improvement of 4% on the accent test set.
</details>
<details>
<summary>摘要</summary>
《 spoken languages show significant variation across mandarin and accent. despite the high performance of mandarin automatic speech recognition (ASR), accent ASR is still a challenge task. in this paper, we introduce meta-learning techniques for fast accent domain expansion in mandarin speech recognition, which expands the field of accents without deteriorating the performance of mandarin ASR. meta-learning or learn-to-learn can learn general relations in multi-domains not only for over-fitting a specific domain. so we select meta-learning in the domain expansion task. this more essential learning will cause improved performance on accent domain extension tasks. we combine the methods of meta-learning and freeze of model parameters, which makes the recognition performance more stable in different cases and the training faster about 20%. our approach significantly outperforms other methods about 3% relatively in the accent domain expansion task. compared to the baseline model, it improves relatively 37% under the condition that the mandarin test set remains unchanged. in addition, it also proved this method to be effective on a large amount of data with a relative performance improvement of 4% on the accent test set.》Note that Simplified Chinese is the official standard for written Chinese in mainland China and is used in this translation. Traditional Chinese is also widely used, especially in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="MyVoice-Arabic-Speech-Resource-Collaboration-Platform"><a href="#MyVoice-Arabic-Speech-Resource-Collaboration-Platform" class="headerlink" title="MyVoice: Arabic Speech Resource Collaboration Platform"></a>MyVoice: Arabic Speech Resource Collaboration Platform</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02503">http://arxiv.org/abs/2308.02503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yousseif Elshahawy, Yassine El Kheir, Shammur Absar Chowdhury, Ahmed Ali</li>
<li>for: 增强阿拉伯语言 dialectal 技术	+ The paper is written to improve Arabic dialectal speech technologies.</li>
<li>methods: 使用 crowdsource 平台收集阿拉伯语言Speech	+ The paper uses a crowdsourcing platform to collect Arabic speech data.</li>
<li>results: 提供大量的阿拉伯语言 диалект Speech 数据	+ The paper provides a large amount of Arabic dialect speech data.Here’s the simplified Chinese text in the format you requested:</li>
<li>for: 增强阿拉伯语言 диалект speech 技术</li>
<li>methods: 使用 crowdsource 平台收集阿拉伯语言Speech</li>
<li>results: 提供大量的阿拉伯语言 диалект Speech 数据<details>
<summary>Abstract</summary>
We introduce MyVoice, a crowdsourcing platform designed to collect Arabic speech to enhance dialectal speech technologies. This platform offers an opportunity to design large dialectal speech datasets; and makes them publicly available. MyVoice allows contributors to select city/country-level fine-grained dialect and record the displayed utterances. Users can switch roles between contributors and annotators. The platform incorporates a quality assurance system that filters out low-quality and spurious recordings before sending them for validation. During the validation phase, contributors can assess the quality of recordings, annotate them, and provide feedback which is then reviewed by administrators. Furthermore, the platform offers flexibility to admin roles to add new data or tasks beyond dialectal speech and word collection, which are displayed to contributors. Thus, enabling collaborative efforts in gathering diverse and large Arabic speech data.
</details>
<details>
<summary>摘要</summary>
我们介绍MyVoice，一个人员征集平台，旨在收集阿拉伯语言的口语，以提高方言技术。这个平台为参与者提供了设置城市/国家精细方言的机会，并将这些大量方言数据公开提供。MyVoice让参与者可以选择城市/国家精细方言，并记录显示的言语。用户可以在角色之间切换，从参与者转为注释者。平台内置了质量保证系统，过滤出低质量和假记录，然后将其发送给验证。在验证阶段，参与者可以评估录音质量，注释和提供反馈，该反馈会被管理员审核。此外，平台允许管理角色添加新的数据或任务，这些数据和任务将被显示给参与者。因此，MyVoiceEnable了多方合作的集合各种多样的阿拉伯语言数据。
</details></li>
</ul>
<hr>
<h2 id="Signal-Reconstruction-from-Mel-spectrogram-Based-on-Bi-level-Consistency-of-Full-band-Magnitude-and-Phase"><a href="#Signal-Reconstruction-from-Mel-spectrogram-Based-on-Bi-level-Consistency-of-Full-band-Magnitude-and-Phase" class="headerlink" title="Signal Reconstruction from Mel-spectrogram Based on Bi-level Consistency of Full-band Magnitude and Phase"></a>Signal Reconstruction from Mel-spectrogram Based on Bi-level Consistency of Full-band Magnitude and Phase</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12232">http://arxiv.org/abs/2307.12232</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/YoshikiMas/signal-reconstruction-from-mel-spectrogram">https://github.com/YoshikiMas/signal-reconstruction-from-mel-spectrogram</a></li>
<li>paper_authors: Yoshiki Masuyama, Natsuki Ueno, Nobutaka Ono</li>
<li>for: 重建时域信号从低维 спектрограм中</li>
<li>methods: 利用低维 STFT矩阵和mel-spectrogram之间的双层关系进行优化算法</li>
<li>results: 对话、音乐和环境信号进行了实验，并得到了有效的重建结果<details>
<summary>Abstract</summary>
We propose an optimization-based method for reconstructing a time-domain signal from a low-dimensional spectral representation such as a mel-spectrogram. Phase reconstruction has been studied to reconstruct a time-domain signal from the full-band short-time Fourier transform (STFT) magnitude. The Griffin-Lim algorithm (GLA) has been widely used because it relies only on the redundancy of STFT and is applicable to various audio signals. In this paper, we jointly reconstruct the full-band magnitude and phase by considering the bi-level relationships among the time-domain signal, its STFT coefficients, and its mel-spectrogram. The proposed method is formulated as a rigorous optimization problem and estimates the full-band magnitude based on the criterion used in GLA. Our experiments demonstrate the effectiveness of the proposed method on speech, music, and environmental signals.
</details>
<details>
<summary>摘要</summary>
我们提出了一种优化方法来重建时域信号从低维 спектрограм中。 phase reconstruction 已经研究过来重建时域信号从全带宽短时域傅立叙恩（STFT）大小。格里夫金-林算法（GLA）广泛使用，因为它仅仅利用 STFT 的重复性，适用于多种音频信号。在这篇论文中，我们同时重建全带大小和相位，通过考虑时域信号、其 STFT 系数和mel-spectrogram之间的双层关系。我们的方法形式为严格的优化问题，根据 GLA 使用的标准 criterion 来估算全带大小。我们的实验表明，我们的方法对语音、乐队和环境信号都有效。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Integration-of-Speech-Separation-and-Recognition-with-Self-Supervised-Learning-Representation"><a href="#Exploring-the-Integration-of-Speech-Separation-and-Recognition-with-Self-Supervised-Learning-Representation" class="headerlink" title="Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation"></a>Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12231">http://arxiv.org/abs/2307.12231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yoshiki Masuyama, Xuankai Chang, Wangyou Zhang, Samuele Cornell, Zhong-Qiu Wang, Nobutaka Ono, Yanmin Qian, Shinji Watanabe</li>
<li>for: 这 paper 的目的是提高 reverberant 和噪音混合的多 speaker ASR 性能。</li>
<li>methods: 这 paper 使用多核频谱映射（TF-GridNet）和自动学习表征（SSLR）来实现 speech separation，并在 ASR 后续模型中使用最佳特征。</li>
<li>results: 该 paper 在 reverberant WHAMR! 测试集上达到了 2.5% 单词错误率，相比之下 existing mask-based MVDR 扩散抑制和 filterbank 集成（28.9%），显著提高了多 speaker ASR 性能。<details>
<summary>Abstract</summary>
Neural speech separation has made remarkable progress and its integration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we explore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recognition performance from the case with filterbank features. To further improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separation and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%).
</details>
<details>
<summary>摘要</summary>
neural speech separation 已经取得了很大的进步，与自动语音识别（ASR）的结合是一种重要的方向，可以实现多个说话人ASR。本文提供了具有深度探索的speech separation在噪音混叠和噪音混叠的scenario中的研究，作为ASR前端。在详细的实践中，我们探索了多通道分离方法、面积基于的束缚映射和复杂的spectral mapping，以及最佳的ASR后端模型中的特征。我们使用了最近的自动学习表示（SSLR）作为特征，并提高了使用filterbank特征的认识性能。为了进一步提高多个说话人认识性能，我们提出了一种特殊的训练策略，用于将speech separation和认识结合使用SSLR。我们的提议使用TF-GridNet基于的复杂的spectral mapping和WavLM基于的SSLR实现了WHAMR！测试集的2.5%词错率，与使用面积基于的MVDR束缚映射和filterbank结合（28.9%）相比，有所显著提高。
</details></li>
</ul>
<hr>
<h2 id="Backdoor-Attacks-against-Voice-Recognition-Systems-A-Survey"><a href="#Backdoor-Attacks-against-Voice-Recognition-Systems-A-Survey" class="headerlink" title="Backdoor Attacks against Voice Recognition Systems: A Survey"></a>Backdoor Attacks against Voice Recognition Systems: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13643">http://arxiv.org/abs/2307.13643</a></li>
<li>repo_url: None</li>
<li>paper_authors: Baochen Yan, Jiahe Lan, Zheng Yan</li>
<li>for: 本研究旨在探讨voice recognition systems (VRSs)受到后门攻击的漏洞，以提高VRSs的安全性和隐私性。</li>
<li>methods: 本文使用了深度学习对VRSs进行漏洞探测和分析，并提出了一组评价标准来评估后门攻击方法的性能。</li>
<li>results: 本文对VRSs受到的后门攻击进行了全面的检查和分类，并对现有的攻击方法进行了分析和评价。同时，本文还介绍了经典的后门防御方法和通用音频防御技术，并评估了它们在VRSs上的可行性。<details>
<summary>Abstract</summary>
Voice Recognition Systems (VRSs) employ deep learning for speech recognition and speaker recognition. They have been widely deployed in various real-world applications, from intelligent voice assistance to telephony surveillance and biometric authentication. However, prior research has revealed the vulnerability of VRSs to backdoor attacks, which pose a significant threat to the security and privacy of VRSs. Unfortunately, existing literature lacks a thorough review on this topic. This paper fills this research gap by conducting a comprehensive survey on backdoor attacks against VRSs. We first present an overview of VRSs and backdoor attacks, elucidating their basic knowledge. Then we propose a set of evaluation criteria to assess the performance of backdoor attack methods. Next, we present a comprehensive taxonomy of backdoor attacks against VRSs from different perspectives and analyze the characteristic of different categories. After that, we comprehensively review existing attack methods and analyze their pros and cons based on the proposed criteria. Furthermore, we review classic backdoor defense methods and generic audio defense techniques. Then we discuss the feasibility of deploying them on VRSs. Finally, we figure out several open issues and further suggest future research directions to motivate the research of VRSs security.
</details>
<details>
<summary>摘要</summary>
声认系统（VRS）通过深度学习实现语音识别和speaker识别。它们在不同的实际应用中广泛应用，从智能声助到电信监测和生物特征验证。然而，先前的研究发现VRS受到后门攻击的潜在威胁，这种威胁对VRS的安全性和隐私具有重要性。然而，现有的文献缺乏对这个话题的全面回顾。这篇论文填补了这个研究空白，通过对后门攻击VRS的全面评估。我们首先提供VRS和后门攻击的概述，然后提出评估后门攻击方法的评价标准。接着，我们提出了对VRS后门攻击的全面分类，分析不同类别的特点。然后，我们对现有的攻击方法进行了全面的回顾和分析，并评估它们的优缺点。此外，我们还评估了经典的后门防御方法和通用音频防御技术，以及它们在VRS上的可行性。最后，我们提出了一些未解决的问题和未来研究方向，以促进VRS的安全研究。
</details></li>
</ul>
<hr>
<h2 id="Modality-Confidence-Aware-Training-for-Robust-End-to-End-Spoken-Language-Understanding"><a href="#Modality-Confidence-Aware-Training-for-Robust-End-to-End-Spoken-Language-Understanding" class="headerlink" title="Modality Confidence Aware Training for Robust End-to-End Spoken Language Understanding"></a>Modality Confidence Aware Training for Robust End-to-End Spoken Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12134">http://arxiv.org/abs/2307.12134</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suyoun Kim, Akshat Shrivastava, Duc Le, Ju Lin, Ozlem Kalinli, Michael L. Seltzer</li>
<li>for: 这个论文目的是提高端到端语言理解（SLU）系统的Robustness，使其在语音识别错误时仍能准确地理解语音。</li>
<li>methods: 这个论文使用了一种单一模型，利用预训练的语音识别模型（ASR）的音频和文本表示，并且在设备流式enario下超过传统的管道SLU系统。但是，E2E SLU系统仍然在文本表示质量低时表现弱。为了解决这个问题，我们提出了一种新的E2E SLU系统，利用音频和文本表示的协同级别来增强ASR错误的Robustness。</li>
<li>results: 我们在STOP数据集上进行了实验，并发现了我们的方法可以提高SLU系统的准确率。此外，我们还进行了分析，以证明我们的方法的有效性。<details>
<summary>Abstract</summary>
End-to-end (E2E) spoken language understanding (SLU) systems that generate a semantic parse from speech have become more promising recently. This approach uses a single model that utilizes audio and text representations from pre-trained speech recognition models (ASR), and outperforms traditional pipeline SLU systems in on-device streaming scenarios. However, E2E SLU systems still show weakness when text representation quality is low due to ASR transcription errors. To overcome this issue, we propose a novel E2E SLU system that enhances robustness to ASR errors by fusing audio and text representations based on the estimated modality confidence of ASR hypotheses. We introduce two novel techniques: 1) an effective method to encode the quality of ASR hypotheses and 2) an effective approach to integrate them into E2E SLU models. We show accuracy improvements on STOP dataset and share the analysis to demonstrate the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
最近，端到端（E2E）的语言理解系统（SLU）在使用抽象语音识别模型（ASR）时得到了更多的承诺。这种方法使用单一的模型，利用来自ASR预训练模型的音频和文本表示，并在设备上流处理方式上超越传统的管道SLU系统。然而，E2E SLU系统仍然在文本表示质量低下时显示弱点，即ASR译写错误。为了解决这个问题，我们提议一种新的E2E SLU系统，通过将音频和文本表示 fusion 以提高对ASR译写错误的抗锋性。我们介绍了两种新技术：1）一种有效的ASR假设质量编码方法，和2）一种有效的将其集成到E2E SLU模型中的方法。我们在STOP数据集上进行了实验，并分享分析结果，以证明我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Estimating-speaker-direction-on-a-humanoid-robot-with-binaural-acoustic-signals"><a href="#Estimating-speaker-direction-on-a-humanoid-robot-with-binaural-acoustic-signals" class="headerlink" title="Estimating speaker direction on a humanoid robot with binaural acoustic signals"></a>Estimating speaker direction on a humanoid robot with binaural acoustic signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12129">http://arxiv.org/abs/2307.12129</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranav Barot, Katja Mombaur, Ewen MacDonald</li>
<li>for: 这个论文是为了实现人类与机器人之间的语音互动，计算机器人需要估计人类对话者的位置。</li>
<li>methods: 这个论文使用了一种方法来优化DOA估计参数，同时考虑了实时应用场景。这个方法是基于人工头部的双耳声源定位框架。收集了实际数据，并对其进行了注释。</li>
<li>results: 这个论文通过了一种简单的搜索方法和一种 bayesian 模型来优化参数，并对实时应用场景的延迟效应进行了研究。结果被验证和讨论。<details>
<summary>Abstract</summary>
To achieve human-like behaviour during speech interactions, it is necessary for a humanoid robot to estimate the location of a human talker. Here, we present a method to optimize the parameters used for the direction of arrival (DOA) estimation, while also considering real-time applications for human-robot interaction scenarios. This method is applied to binaural sound source localization framework on a humanoid robotic head. Real data is collected and annotated for this work. Optimizations are performed via a brute force method and a Bayesian model based method, results are validated and discussed, and effects on latency for real-time use are also explored.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)为实现人类语音交流中的人类行为，Robot需要估算人类说话者的位置。在这里，我们提出了一种优化DOA估计参数的方法，同时考虑了人机交互场景中的实时应用。这种方法是应用于人类机械头上的双耳声源定位框架。收集了实际数据并对其进行了注释。我们通过粗暴方法和 bayesian模型基于方法进行优化，并对结果进行验证和讨论，还研究了实时使用时的延迟效应。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/23/cs.SD_2023_07_23/" data-id="cllsj9wz30046uv88hd3haghb" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/23/eess.IV_2023_07_23/" class="article-date">
  <time datetime="2023-07-22T16:00:00.000Z" itemprop="datePublished">2023-07-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/23/eess.IV_2023_07_23/">eess.IV - 2023-07-23 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ES2Net-An-Efficient-Spectral-Spatial-Network-for-Hyperspectral-Image-Change-Detection"><a href="#ES2Net-An-Efficient-Spectral-Spatial-Network-for-Hyperspectral-Image-Change-Detection" class="headerlink" title="ES2Net: An Efficient Spectral-Spatial Network for Hyperspectral Image Change Detection"></a>ES2Net: An Efficient Spectral-Spatial Network for Hyperspectral Image Change Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12327">http://arxiv.org/abs/2307.12327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingren Yao, Yuan Zhou, Wei Xiang</li>
<li>for: 这个研究旨在提高干扰异常检测（HSI-CD）的精度和效率，通过自动选择适合检测的带段和对带段之间的非线性关系进行干扰异常检测。</li>
<li>methods: 本研究提出了一个终端能力优化的 spectral-spatial 变化检测网络（ES2Net），包括一个可学习的带段选择模块和一个对带段之间的非线性关系进行干扰异常检测。</li>
<li>results: 实验结果显示，ES2Net 比其他现有方法更有效率和更高精度地进行 HSI-CD。<details>
<summary>Abstract</summary>
Hyperspectral image change detection (HSI-CD) aims to identify the differences in bitemporal HSIs. To mitigate spectral redundancy and improve the discriminativeness of changing features, some methods introduced band selection technology to select bands conducive for CD. However, these methods are limited by the inability to end-to-end training with the deep learning-based feature extractor and lack considering the complex nonlinear relationship among bands. In this paper, we propose an end-to-end efficient spectral-spatial change detection network (ES2Net) to address these issues. Specifically, we devised a learnable band selection module to automatically select bands conducive to CD. It can be jointly optimized with a feature extraction network and capture the complex nonlinear relationships among bands. Moreover, considering the large spatial feature distribution differences among different bands, we design the cluster-wise spatial attention mechanism that assigns a spatial attention factor to each individual band to individually improve the feature discriminativeness for each band. Experiments on three widely used HSI-CD datasets demonstrate the effectiveness and superiority of this method compared with other state-of-the-art methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Development-of-pericardial-fat-count-images-using-a-combination-of-three-different-deep-learning-models"><a href="#Development-of-pericardial-fat-count-images-using-a-combination-of-three-different-deep-learning-models" class="headerlink" title="Development of pericardial fat count images using a combination of three different deep-learning models"></a>Development of pericardial fat count images using a combination of three different deep-learning models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12316">http://arxiv.org/abs/2307.12316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takaaki Matsunaga, Atsushi Kono, Hidetoshi Matsuo, Kaoru Kitagawa, Mizuho Nishio, Hiromi Hashimura, Yu Izawa, Takayoshi Toba, Kazuki Ishikawa, Akie Katsuki, Kazuyuki Ohmura, Takamichi Murakami</li>
<li>For: The paper aims to generate pericardial fat count images (PFCIs) from chest radiographs (CXRs) using a dedicated deep-learning model, in order to evaluate pericardial fat (PF) and its relationship with coronary artery disease.* Methods: The proposed method uses three different deep-learning models, including CycleGAN, to generate PFCIs from CXRs. The method involves projecting three-dimensional CT images to generate PFCIs, where fat accumulation is represented by high pixel values.* Results: The proposed model showed better performance than a single CycleGAN-based model in generating PFCIs, with higher structural similarity index measure (SSIM), lower mean squared error (MSE), and lower mean absolute error (MAE). The results suggest that PFCI evaluation without CT may be possible with the proposed method.<details>
<summary>Abstract</summary>
Rationale and Objectives: Pericardial fat (PF), the thoracic visceral fat surrounding the heart, promotes the development of coronary artery disease by inducing inflammation of the coronary arteries. For evaluating PF, this study aimed to generate pericardial fat count images (PFCIs) from chest radiographs (CXRs) using a dedicated deep-learning model.   Materials and Methods: The data of 269 consecutive patients who underwent coronary computed tomography (CT) were reviewed. Patients with metal implants, pleural effusion, history of thoracic surgery, or that of malignancy were excluded. Thus, the data of 191 patients were used. PFCIs were generated from the projection of three-dimensional CT images, where fat accumulation was represented by a high pixel value. Three different deep-learning models, including CycleGAN, were combined in the proposed method to generate PFCIs from CXRs. A single CycleGAN-based model was used to generate PFCIs from CXRs for comparison with the proposed method. To evaluate the image quality of the generated PFCIs, structural similarity index measure (SSIM), mean squared error (MSE), and mean absolute error (MAE) of (i) the PFCI generated using the proposed method and (ii) the PFCI generated using the single model were compared.   Results: The mean SSIM, MSE, and MAE were as follows: 0.856, 0.0128, and 0.0357, respectively, for the proposed model; and 0.762, 0.0198, and 0.0504, respectively, for the single CycleGAN-based model.   Conclusion: PFCIs generated from CXRs with the proposed model showed better performance than those with the single model. PFCI evaluation without CT may be possible with the proposed method.
</details>
<details>
<summary>摘要</summary>
目的和目标：胸膜脂肪（PF），脊梁内脂肪细胞附近心脏，Promotes the development of coronary artery disease by inducing inflammation of the coronary arteries。为评估PF，本研究旨在通过专门的深度学习模型生成胸膜脂肪计数图像（PFCIs）从胸部X射线图像（CXRs）中。材料和方法：本研究审查了269例 consecutive patients underwent coronary computed tomography（CT）的数据。排除了金属设备、肿瘤、胸部手术历史或肿瘤的患者。因此，数据中的191例被用于分析。PFCIs是基于三维CT图像的投影，其中脂肪储存表示高像素值。本研究使用了三种不同的深度学习模型，包括CycleGAN，来生成PFCIs从CXRs。单个CycleGAN基本模型被用来生成PFCIs从CXRs，以便与提案方法进行比较。为评估生成的PFCIs的图像质量，使用了结构相似度度量（SSIM）、平均平方误差（MSE）和平均绝对误差（MAE）进行比较。结果：生成PFCIs的平均SSIM、MSE和MAE分别为：0.856、0.0128和0.0357，用于提案模型；和0.762、0.0198和0.0504，用于单个CycleGAN基本模型。结论：由提案模型生成的PFCIs表现更好于单个CycleGAN基本模型生成的PFCIs。PFCI评估可能不需要CT。
</details></li>
</ul>
<hr>
<h2 id="Simultaneous-temperature-estimation-and-nonuniformity-correction-from-multiple-frames"><a href="#Simultaneous-temperature-estimation-and-nonuniformity-correction-from-multiple-frames" class="headerlink" title="Simultaneous temperature estimation and nonuniformity correction from multiple frames"></a>Simultaneous temperature estimation and nonuniformity correction from multiple frames</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12297">http://arxiv.org/abs/2307.12297</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navot Oz, Omri Berman, Nir Sochen, David Mendelovich, Iftach Klapp<br>for: 这个研究旨在提出一种同时进行温度估计和非均匀调正的方法，以提高低成本的红外摄像机在不同应用中的精度和可靠性。methods: 本研究使用了kernel估计网络（KPN）deep learning架构，并将物理摄像机获取模型 incorporated into the network，以便结合多帧照片。另外，我们也提出了一个新的偏移对象，将 ambient temperature 组み入模型中，以便估计摄像机的偏移。results: 我们发现了多帧照片的数量对温度估计和非均匀调正的精度有着重要的影响。此外，我们的方法与vanilla KPN相比，有着明显的改善，归功于偏移对象。实验结果显示，使用我们的方法，可以在低成本的红外摄像机上 achieve high accuracy 温度估计和非均匀调正，与costly scientific-grade radiometric cameras 相比，只有小数字的average error。<details>
<summary>Abstract</summary>
Infrared (IR) cameras are widely used for temperature measurements in various applications, including agriculture, medicine, and security. Low-cost IR camera have an immense potential to replace expansive radiometric cameras in these applications, however low-cost microbolometer-based IR cameras are prone to spatially-variant nonuniformity and to drift in temperature measurements, which limits their usability in practical scenarios.   To address these limitations, we propose a novel approach for simultaneous temperature estimation and nonuniformity correction from multiple frames captured by low-cost microbolometer-based IR cameras. We leverage the physical image acquisition model of the camera and incorporate it into a deep learning architecture called kernel estimation networks (KPN), which enables us to combine multiple frames despite imperfect registration between them. We also propose a novel offset block that incorporates the ambient temperature into the model and enables us to estimate the offset of the camera, which is a key factor in temperature estimation.   Our findings demonstrate that the number of frames has a significant impact on the accuracy of temperature estimation and nonuniformity correction. Moreover, our approach achieves a significant improvement in performance compared to vanilla KPN, thanks to the offset block. The method was tested on real data collected by a low-cost IR camera mounted on a UAV, showing only a small average error of $0.27^\circ C-0.54^\circ C$ relative to costly scientific-grade radiometric cameras.   Our method provides an accurate and efficient solution for simultaneous temperature estimation and nonuniformity correction, which has important implications for a wide range of practical applications.
</details>
<details>
<summary>摘要</summary>
INFRARED（IR）摄像机广泛应用于温度测量多种应用领域，包括农业、医学和安全。低成本IR摄像机有很大的潜力取代昂贵的辐射测量摄像机，但是它们受到了空间不均和温度测量偏差的限制，这限制了它们在实际场景中的应用。为了解决这些限制，我们提出了一种新的方法，即同时进行温度估计和非uniformity correction，使用低成本微博лом特基于IR摄像机的多帧图像。我们利用摄像机物理图像获取模型，并将其integrated into一种深度学习架构 called kernel estimation networks（KPN），以便将多帧图像结合，即使它们之间不完美地对齐。我们还提出了一个新的偏移块，即 ambient temperature 的偏移，它使得我们可以在模型中 estimates the camera offset，这是温度估计中的关键因素。我们的发现表明，图像数量对温度估计和非uniformity correction的精度有很大的影响。此外，我们的方法在比 vanilla KPN 更好的表现，感谢偏移块。我们的方法在实际数据中进行测试，用一架低成本IR摄像机在无人机上收集的数据，显示只有0.27-0.54°C的平均偏差相比高科技质量的 radiometric 摄像机。我们的方法提供了一种准确和高效的同时温度估计和非uniformity correction的解决方案，这对各种实际应用有重要意义。
</details></li>
</ul>
<hr>
<h2 id="ASCON-Anatomy-aware-Supervised-Contrastive-Learning-Framework-for-Low-dose-CT-Denoising"><a href="#ASCON-Anatomy-aware-Supervised-Contrastive-Learning-Framework-for-Low-dose-CT-Denoising" class="headerlink" title="ASCON: Anatomy-aware Supervised Contrastive Learning Framework for Low-dose CT Denoising"></a>ASCON: Anatomy-aware Supervised Contrastive Learning Framework for Low-dose CT Denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12225">http://arxiv.org/abs/2307.12225</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hao1635/ASCON">https://github.com/hao1635/ASCON</a></li>
<li>paper_authors: Zhihao Chen, Qi Gao, Yi Zhang, Hongming Shan</li>
<li>for: 这篇论文是为了提出一种新的精度控制方法，用于低剂量 computed tomography（CT）的雷达清洁。</li>
<li>methods: 该方法使用了两种新的设计：一个高效的自我注意力基于 U-Net（ESAU-Net），以及一个多尺度的解剖学对比网络（MAC-Net）。</li>
<li>results: 对两个公共的低剂量 CT 雷达清洁数据集进行了广泛的实验，并证明了 ASCON 在对比之前的模型性能的超越。特别是，ASCON 可以为低剂量 CT 雷达清洁提供解剖学可读性，这是首次实现的。<details>
<summary>Abstract</summary>
While various deep learning methods have been proposed for low-dose computed tomography (CT) denoising, most of them leverage the normal-dose CT images as the ground-truth to supervise the denoising process. These methods typically ignore the inherent correlation within a single CT image, especially the anatomical semantics of human tissues, and lack the interpretability on the denoising process. In this paper, we propose a novel Anatomy-aware Supervised CONtrastive learning framework, termed ASCON, which can explore the anatomical semantics for low-dose CT denoising while providing anatomical interpretability. The proposed ASCON consists of two novel designs: an efficient self-attention-based U-Net (ESAU-Net) and a multi-scale anatomical contrastive network (MAC-Net). First, to better capture global-local interactions and adapt to the high-resolution input, an efficient ESAU-Net is introduced by using a channel-wise self-attention mechanism. Second, MAC-Net incorporates a patch-wise non-contrastive module to capture inherent anatomical information and a pixel-wise contrastive module to maintain intrinsic anatomical consistency. Extensive experimental results on two public low-dose CT denoising datasets demonstrate superior performance of ASCON over state-of-the-art models. Remarkably, our ASCON provides anatomical interpretability for low-dose CT denoising for the first time. Source code is available at https://github.com/hao1635/ASCON.
</details>
<details>
<summary>摘要</summary>
“Various deep learning methods have been proposed for low-dose computed tomography (CT) denoising, but most of them rely on normal-dose CT images as ground truth to supervise the denoising process. These methods typically ignore the inherent correlation within a single CT image, especially the anatomical semantics of human tissues, and lack interpretability of the denoising process. In this paper, we propose a novel Anatomy-aware Supervised CONtrastive learning framework, termed ASCON, which can explore the anatomical semantics for low-dose CT denoising while providing anatomical interpretability. The proposed ASCON consists of two novel designs: an efficient self-attention-based U-Net (ESAU-Net) and a multi-scale anatomical contrastive network (MAC-Net). First, to better capture global-local interactions and adapt to high-resolution input, an efficient ESAU-Net is introduced by using a channel-wise self-attention mechanism. Second, MAC-Net incorporates a patch-wise non-contrastive module to capture inherent anatomical information and a pixel-wise contrastive module to maintain intrinsic anatomical consistency. Extensive experimental results on two public low-dose CT denoising datasets demonstrate superior performance of ASCON over state-of-the-art models. Remarkably, our ASCON provides anatomical interpretability for low-dose CT denoising for the first time. Source code is available at https://github.com/hao1635/ASCON.”Note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese used in mainland China. The other version is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="SCPAT-GAN-Structural-Constrained-and-Pathology-Aware-Convolutional-Transformer-GAN-for-Virtual-Histology-Staining-of-Human-Coronary-OCT-images"><a href="#SCPAT-GAN-Structural-Constrained-and-Pathology-Aware-Convolutional-Transformer-GAN-for-Virtual-Histology-Staining-of-Human-Coronary-OCT-images" class="headerlink" title="SCPAT-GAN: Structural Constrained and Pathology Aware Convolutional Transformer-GAN for Virtual Histology Staining of Human Coronary OCT images"></a>SCPAT-GAN: Structural Constrained and Pathology Aware Convolutional Transformer-GAN for Virtual Histology Staining of Human Coronary OCT images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12138">http://arxiv.org/abs/2307.12138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueshen Li, Hongshan Liu, Xiaoyu Song, Brigitta C. Brott, Silvio H. Litovsky, Yu Gan</li>
<li>for: 用于生成 coronary optical coherence tomography（OCT）图像中的虚拟染色 Histology，以更好地导航 coronary artery disease 的治疗。</li>
<li>methods: 使用 transformer 生成 adversarial network（GAN），并在网络结构中添加 pathological guidance，以便在 OCT 图像上生成虚拟染色 H&amp;E histology。</li>
<li>results: SCPAT-GAN 可以提供高质量的虚拟染色 Histology，并且可以准确地映射 OCT 图像中的疾病区域。<details>
<summary>Abstract</summary>
There is a significant need for the generation of virtual histological information from coronary optical coherence tomography (OCT) images to better guide the treatment of coronary artery disease. However, existing methods either require a large pixel-wisely paired training dataset or have limited capability to map pathological regions. To address these issues, we proposed a structural constrained, pathology aware, transformer generative adversarial network, namely SCPAT-GAN, to generate virtual stained H&E histology from OCT images. The proposed SCPAT-GAN advances existing methods via a novel design to impose pathological guidance on structural layers using transformer-based network.
</details>
<details>
<summary>摘要</summary>
有一个重要的需求是从心血管成像扫描仪（OCT）图像生成虚拟 Histological信息，以更好地指导心血管疾病的治疗。然而，现有的方法可能需要大量的像素对齐训练集或具有有限的病理区域映射能力。为解决这些问题，我们提出了一种结构受限、病理意识的变换生成对抗网络（SCPAT-GAN），用于从OCT图像生成虚拟染色H&E histology。我们的提议的SCPAT-GAN在现有方法的基础之上增加了一种新的设计，通过 transformer-based 网络将结构层受到病理指导。
</details></li>
</ul>
<hr>
<h2 id="Improving-temperature-estimation-in-low-cost-infrared-cameras-using-deep-neural-networks"><a href="#Improving-temperature-estimation-in-low-cost-infrared-cameras-using-deep-neural-networks" class="headerlink" title="Improving temperature estimation in low-cost infrared cameras using deep neural networks"></a>Improving temperature estimation in low-cost infrared cameras using deep neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12130">http://arxiv.org/abs/2307.12130</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navot Oz, Nir Sochen, David Mendelovich, Iftach Klapp</li>
<li>for: 提高低成本温度摄像机的温度准确性和纹理不均性。</li>
<li>methods: 开发了一种考虑 ambient temperature 的非对称不均性模拟器，并提出了一种基于批处理神经网络的方法，使用单张图像和摄像机自身测得的 ambient temperature 来估算对象温度并修正不均性。</li>
<li>results: 比前 works 低了约 $1^\circ C$ 的平均温度误差，并且通过Physical constraint 下降了误差4%。 验证数据集中的平均温度误差为 $0.37^\circ C$，并在实际场景中得到了相当的结果。<details>
<summary>Abstract</summary>
Low-cost thermal cameras are inaccurate (usually $\pm 3^\circ C$) and have space-variant nonuniformity across their detector. Both inaccuracy and nonuniformity are dependent on the ambient temperature of the camera. The main goal of this work was to improve the temperature accuracy of low-cost cameras and rectify the nonuniformity.   A nonuniformity simulator that accounts for the ambient temperature was developed. An end-to-end neural network that incorporates the ambient temperature at image acquisition was introduced. The neural network was trained with the simulated nonuniformity data to estimate the object's temperature and correct the nonuniformity, using only a single image and the ambient temperature measured by the camera itself. Results show that the proposed method lowered the mean temperature error by approximately $1^\circ C$ compared to previous works. In addition, applying a physical constraint on the network lowered the error by an additional $4\%$.   The mean temperature error over an extensive validation dataset was $0.37^\circ C$. The method was verified on real data in the field and produced equivalent results.
</details>
<details>
<summary>摘要</summary>
Note: The Simplified Chinese translation is written in the Mandarin dialect, which is the most widely spoken and accepted form of Chinese.Translation Notes:1. "Low-cost thermal cameras" is translated as "低成本热图像仪" (dīn chéng běn rè tè yǐng)2. "inaccurate" is translated as "不准确" (bù zhèng qiú)3. "space-variant nonuniformity" is translated as "空间不均匀的非准确" (kōng jiān bù jí chū de fēi zhèng qiú)4. "ambient temperature" is translated as "环境温度" (huán jìn wēn dù)5. "nonuniformity simulator" is translated as "非均匀性模拟器" (fēi jí chū xìng mó xì)6. "end-to-end neural network" is translated as "端到端神经网络" (dían dào diàn xīn líng wǎng wǎn)7. "physical constraint" is translated as "物理约束" (wù lǐ jiè shòu)8. "mean temperature error" is translated as "平均温度误差" (píng jìn wēn dù huì chá)9. "validation dataset" is translated as "验证数据集" (yàn zhèng xù xiǎng)10. "real data in the field" is translated as "实际数据在场" (shí jì xù xiǎng)Note that the translation is written in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese may be used in other regions, such as Taiwan and Hong Kong.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/23/eess.IV_2023_07_23/" data-id="cllsj9x0b0088uv88enwhgwp0" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/22/cs.LG_2023_07_22/" class="article-date">
  <time datetime="2023-07-21T16:00:00.000Z" itemprop="datePublished">2023-07-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/22/cs.LG_2023_07_22/">cs.LG - 2023-07-22 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Revolution-of-Personalized-Healthcare-Enabling-Human-Digital-Twin-with-Mobile-AIGC"><a href="#A-Revolution-of-Personalized-Healthcare-Enabling-Human-Digital-Twin-with-Mobile-AIGC" class="headerlink" title="A Revolution of Personalized Healthcare: Enabling Human Digital Twin with Mobile AIGC"></a>A Revolution of Personalized Healthcare: Enabling Human Digital Twin with Mobile AIGC</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12115">http://arxiv.org/abs/2307.12115</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayuan Chen, Changyan Yi, Hongyang Du, Dusit Niyato, Jiawen Kang, Jun Cai, Xuemin, Shen</li>
<li>for: 本研究旨在探讨移动 искусственный智能生成内容（AIGC）技术在人工智能驱动人工对应（HDT）领域的应用。</li>
<li>methods: 本文提出了移动AIGC驱动HDT的系统架构，并详细介绍了相关的设计要求和挑战。</li>
<li>results: 经过实验研究，移动AIGC驱动HDT解决方案显示出在虚拟物理治疗教学平台中的效果。<details>
<summary>Abstract</summary>
Mobile Artificial Intelligence-Generated Content (AIGC) technology refers to the adoption of AI algorithms deployed at mobile edge networks to automate the information creation process while fulfilling the requirements of end users. Mobile AIGC has recently attracted phenomenal attentions and can be a key enabling technology for an emerging application, called human digital twin (HDT). HDT empowered by the mobile AIGC is expected to revolutionize the personalized healthcare by generating rare disease data, modeling high-fidelity digital twin, building versatile testbeds, and providing 24/7 customized medical services. To promote the development of this new breed of paradigm, in this article, we propose a system architecture of mobile AIGC-driven HDT and highlight the corresponding design requirements and challenges. Moreover, we illustrate two use cases, i.e., mobile AIGC-driven HDT in customized surgery planning and personalized medication. In addition, we conduct an experimental study to prove the effectiveness of the proposed mobile AIGC-driven HDT solution, which shows a particular application in a virtual physical therapy teaching platform. Finally, we conclude this article by briefly discussing several open issues and future directions.
</details>
<details>
<summary>摘要</summary>
移动人工智能生成内容（AIGC）技术指的是在移动边缘网络上部署AI算法，以自动生成信息，同时满足用户需求。移动AIGC在最近吸引了非常多的关注，可以成为人类数字双（HDT）的关键启用技术。HDT通过移动AIGC得到强化，预计将改变个性化医疗，生成罕见疾病数据，建立高准确度数字双，创建多样化测试床，提供24/7个性化医疗服务。为推动这种新的 paradigma 的发展，本文提出了移动AIGC驱动HDT的系统架构，并 highlight了相应的设计要求和挑战。此外，我们还提出了两个使用情景，即移动AIGC驱动HDT在自定义手术规划和个性化药物。此外，我们进行了实验研究，证明了我们提议的移动AIGC驱动HDT解决方案的效果，其在虚拟物理治疗教学平台中有特定的应用。最后，我们 briefly discuss了一些开放问题和未来方向。
</details></li>
</ul>
<hr>
<h2 id="A-Zero-shot-and-Few-shot-Study-of-Instruction-Finetuned-Large-Language-Models-Applied-to-Clinical-and-Biomedical-Tasks"><a href="#A-Zero-shot-and-Few-shot-Study-of-Instruction-Finetuned-Large-Language-Models-Applied-to-Clinical-and-Biomedical-Tasks" class="headerlink" title="A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks"></a>A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12114">http://arxiv.org/abs/2307.12114</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanis Labrak, Mickael Rouvier, Richard Dufour</li>
<li>for: 这些论文是为了评估四种现状最佳大语言模型（LLMs）在临床和生物医学自然语言处理（NLP）任务中的性能。</li>
<li>methods: 这些论文使用了四种实际应用中最佳的 instruction-tuned LLMs，包括 ChatGPT、Flan-T5 UL2、Tk-Instruct 和 Alpaca，在英语的13种实际临床和生物医学 NLP 任务上进行了评估。</li>
<li>results: 结果表明，评估的 LLMs 在零和几个采样enario下对大多数任务的性能已经接近了状态之前模型的性能，尤其是在问答任务上表现非常出色，即使它们没有在这些任务上看到过示例。但是，我们发现了分类和关系抽取任务的性能较低，相比特别训练的医疗领域模型，如 PubMedBERT。此外，我们注意到没有任何 LLM 在所有任务上超过其他模型的性能，一些模型更适合某些任务。<details>
<summary>Abstract</summary>
We evaluate four state-of-the-art instruction-tuned large language models (LLMs) -- ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca -- on a set of 13 real-world clinical and biomedical natural language processing (NLP) tasks in English, such as named-entity recognition (NER), question-answering (QA), relation extraction (RE), etc. Our overall results demonstrate that the evaluated LLMs begin to approach performance of state-of-the-art models in zero- and few-shot scenarios for most tasks, and particularly well for the QA task, even though they have never seen examples from these tasks before. However, we observed that the classification and RE tasks perform below what can be achieved with a specifically trained model for the medical field, such as PubMedBERT. Finally, we noted that no LLM outperforms all the others on all the studied tasks, with some models being better suited for certain tasks than others.
</details>
<details>
<summary>摘要</summary>
我们评估了四种现场最佳大语言模型（LLM）——ChatGPT、Flan-T5 UL2、Tk-Instruct和Alpaca——在英语的13种实际医疗和生物医学自然语言处理（NLP）任务上，如命名实体识别（NER）、问答（QA）、关系提取（RE）等。我们的总结结果表明，评估的LLMs在零或几次示例enario下对大多数任务的性能逐渐接近了现场最佳模型的水平，尤其是在QA任务上表现特别好，即使它们没有在这些任务上看到过示例。然而，我们发现了分类和RE任务的性能比专门为医学领域训练的模型，如PubMedBERT，还下。最后，我们注意到没有LLM能够在所有任务上表现更好，一些模型更适合某些任务。
</details></li>
</ul>
<hr>
<h2 id="Active-Control-of-Flow-over-Rotating-Cylinder-by-Multiple-Jets-using-Deep-Reinforcement-Learning"><a href="#Active-Control-of-Flow-over-Rotating-Cylinder-by-Multiple-Jets-using-Deep-Reinforcement-Learning" class="headerlink" title="Active Control of Flow over Rotating Cylinder by Multiple Jets using Deep Reinforcement Learning"></a>Active Control of Flow over Rotating Cylinder by Multiple Jets using Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12083">http://arxiv.org/abs/2307.12083</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kamyar Dobakhti, Jafar Ghazanfarian</li>
<li>For: This paper aims to optimize the use of rotation and deep reinforcement learning (DRL) for drag reduction on blunt bodies.* Methods: The paper uses multiple controlled jets and a DRL algorithm to reach maximum possible drag suppression. The paper also explores the optimization of the number and positions of the jets, sensor location, and maximum allowed flow rate.* Results: The combination of rotation and DRL reduces the drag coefficient by up to 49.75%, and the agent can keep the lift coefficient at a value near zero or stabilize it at a smaller number. Additionally, the paper finds that having more sensors at more locations is not always beneficial and should be determined based on the need of the user and corresponding configuration.<details>
<summary>Abstract</summary>
The real power of artificial intelligence appears in reinforcement learning, which is computationally and physically more sophisticated due to its dynamic nature. Rotation and injection are some of the proven ways in active flow control for drag reduction on blunt bodies. In this paper, rotation will be added to the cylinder alongside the deep reinforcement learning (DRL) algorithm, which uses multiple controlled jets to reach the maximum possible drag suppression. Characteristics of the DRL code, including controlling parameters, their limitations, and optimization of the DRL network for use with rotation will be presented. This work will focus on optimizing the number and positions of the jets, the sensors location, and the maximum allowed flow rate to jets in the form of the maximum allowed flow rate of each actuation and the total number of them per episode. It is found that combining the rotation and DRL is promising since it suppresses the vortex shedding, stabilizes the Karman vortex street, and reduces the drag coefficient by up to 49.75%. Also, it will be shown that having more sensors at more locations is not always a good choice and the sensor number and location should be determined based on the need of the user and corresponding configuration. Also, allowing the agent to have access to higher flow rates, mostly reduces the performance, except when the cylinder rotates. In all cases, the agent can keep the lift coefficient at a value near zero, or stabilize it at a smaller number.
</details>
<details>
<summary>摘要</summary>
真正的人工智能能量在回归学习中表现出真正的力量，因为它的动态性使得计算和物理上更加复杂。在活流控制中，旋转和注射是已知的方法来降低碰撞体的阻力。在这篇论文中，我们将在圆柱体上添加旋转，并使用深度回归学习（DRL）算法，使用多个控制的气流来达到最大可能的阻力减少。我们将介绍DRL代码中控制参数的限制和优化，以及在旋转下使用DRL网络的优化。我们发现，将旋转和DRL相结合，能够有效地降低阻力系数，最高达49.75%。此外，我们还发现，在不同的配置下，有更多的感测器并不总是有利，感测器的数量和位置应该根据用户的需求进行定制。同时，允许代理人访问更高的流速，通常不会提高性能，除非圆柱体在旋转。在所有情况下，代理人都能保持升力系数在零附近，或者稳定其为较小的数字。
</details></li>
</ul>
<hr>
<h2 id="Spectral-Normalized-Cut-Graph-Partitioning-with-Fairness-Constraints"><a href="#Spectral-Normalized-Cut-Graph-Partitioning-with-Fairness-Constraints" class="headerlink" title="Spectral Normalized-Cut Graph Partitioning with Fairness Constraints"></a>Spectral Normalized-Cut Graph Partitioning with Fairness Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12065">http://arxiv.org/abs/2307.12065</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiali2000/fnm">https://github.com/jiali2000/fnm</a></li>
<li>paper_authors: Jia Li, Yanhao Wang, Arpit Merchant</li>
<li>for: 这个论文的目的是提出一种基于分类敏感特征的图分区算法，以确保图中每个群组中的分布相对均衡，同时最小化正规化距离值。</li>
<li>methods: 本文提出的方法包括一个两阶段spectral算法，首先添加了一个基于公平性 criterion的扩展拉格朗日函数，然后在第二阶段使用一种轮换方案来从公平embedding中生成$k$个群组。</li>
<li>results: 经过对九个 benchmark dataset的实验表明，FNM方法与三个基eline方法相比，在公平性和分区质量之间能够更好地平衡。<details>
<summary>Abstract</summary>
Normalized-cut graph partitioning aims to divide the set of nodes in a graph into $k$ disjoint clusters to minimize the fraction of the total edges between any cluster and all other clusters. In this paper, we consider a fair variant of the partitioning problem wherein nodes are characterized by a categorical sensitive attribute (e.g., gender or race) indicating membership to different demographic groups. Our goal is to ensure that each group is approximately proportionally represented in each cluster while minimizing the normalized cut value. To resolve this problem, we propose a two-phase spectral algorithm called FNM. In the first phase, we add an augmented Lagrangian term based on our fairness criteria to the objective function for obtaining a fairer spectral node embedding. Then, in the second phase, we design a rounding scheme to produce $k$ clusters from the fair embedding that effectively trades off fairness and partition quality. Through comprehensive experiments on nine benchmark datasets, we demonstrate the superior performance of FNM compared with three baseline methods.
</details>
<details>
<summary>摘要</summary>
通常化 cut 图分割目标是将图中的节点集分成 $k$ 个不 overlap 的集群，以最小化任何两个集群之间的总边数。在这篇论文中，我们考虑了一种公平的变种图分割问题，其中节点被 categorical 敏感特征（例如性别或种族）分类为不同的社会组别。我们的目标是确保每个组别在每个集群中都有约等的表现，同时最小化 normalized cut 值。为解决这个问题，我们提出了一种两个阶段的spectral算法 called FNM。在第一阶段，我们添加了基于我们的公平准则的扩展拉格朗日函数到目标函数中，以获得更公平的spectral节点嵌入。然后，在第二阶段，我们设计了一种轮换方案，以生成 $k$ 个集群从公平嵌入中，并且有效地考虑了公平和分割质量之间的衡量。通过对九个基准数据集进行了广泛的实验，我们展示了 FNM 的超过基准方法的性能。
</details></li>
</ul>
<hr>
<h2 id="Balancing-Exploration-and-Exploitation-in-Hierarchical-Reinforcement-Learning-via-Latent-Landmark-Graphs"><a href="#Balancing-Exploration-and-Exploitation-in-Hierarchical-Reinforcement-Learning-via-Latent-Landmark-Graphs" class="headerlink" title="Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs"></a>Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12063">http://arxiv.org/abs/2307.12063</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/papercode2022/hill">https://github.com/papercode2022/hill</a></li>
<li>paper_authors: Qingyang Zhang, Yiming Yang, Jingqing Ruan, Xuantang Xiong, Dengpeng Xing, Bo Xu</li>
<li>for: This paper aims to address the exploration-exploitation dilemma in reinforcement learning by proposing a hierarchical reinforcement learning method called HILL, which learns latent subgoal representations that satisfy temporal coherence and dynamically builds latent landmark graphs to balance exploration and exploitation.</li>
<li>methods: The HILL method uses a contrastive representation learning objective to learn latent subgoal representations, and employs a novelty measure on nodes and a utility measure on edges to balance exploration and exploitation.</li>
<li>results: The experimental results show that HILL outperforms state-of-the-art baselines on continuous control tasks with sparse rewards in sample efficiency and asymptotic performance.Here’s the Chinese translation of the three key information points:</li>
<li>for: 这篇论文目标是解决 reinforcement learning 中的冒险奖励矛盾，提出一种层次游戏学习方法 called HILL，该方法学习具有时间准确性的隐藏目标表示。</li>
<li>methods: HILL 方法使用对比表示学习目标来学习隐藏目标表示，并使用节点新鲜度和边利用度来均衡冒险和利用。</li>
<li>results: 实验结果显示，HILL 方法在缺乏奖励的连续控制任务上超过了现状权限的基线。<details>
<summary>Abstract</summary>
Goal-Conditioned Hierarchical Reinforcement Learning (GCHRL) is a promising paradigm to address the exploration-exploitation dilemma in reinforcement learning. It decomposes the source task into subgoal conditional subtasks and conducts exploration and exploitation in the subgoal space. The effectiveness of GCHRL heavily relies on subgoal representation functions and subgoal selection strategy. However, existing works often overlook the temporal coherence in GCHRL when learning latent subgoal representations and lack an efficient subgoal selection strategy that balances exploration and exploitation. This paper proposes HIerarchical reinforcement learning via dynamically building Latent Landmark graphs (HILL) to overcome these limitations. HILL learns latent subgoal representations that satisfy temporal coherence using a contrastive representation learning objective. Based on these representations, HILL dynamically builds latent landmark graphs and employs a novelty measure on nodes and a utility measure on edges. Finally, HILL develops a subgoal selection strategy that balances exploration and exploitation by jointly considering both measures. Experimental results demonstrate that HILL outperforms state-of-the-art baselines on continuous control tasks with sparse rewards in sample efficiency and asymptotic performance. Our code is available at https://github.com/papercode2022/HILL.
</details>
<details>
<summary>摘要</summary>
“对于受挑战的问题，Goal-Conditioned Hierarchical Reinforcement Learning（GCHRL）是一种有前途的方法。它将源任务分解为子目标对应的子任务，并在子任务空间进行探索和实现。然而，现有的作品往往忽略GCHRL中的时间一致性，对于学习隐藏的子目标表示而言，没有有效的时间一致性评估方法。这篇论文提出了HIerarchical reinforcement learning via dynamically building Latent Landmark graphs（HILL），以解决这些限制。HILL使用了一个对称的描述学习目标，以learn隐藏的子目标表示，并且 dynamically builds latent landmark graphs，使用了一个新的novelty measure和一个utility measure。最后，HILL发展了一个具有平衡探索和实现的子目标选择策略。实验结果显示，HILL在缺乏对象奖励的连续控制任务上，在样本效率和长期性方面，与现有的基eline相比，表现出色。我们的代码可以在https://github.com/papercode2022/HILL上获取。”
</details></li>
</ul>
<hr>
<h2 id="Game-Theoretic-Robust-Reinforcement-Learning-Handles-Temporally-Coupled-Perturbations"><a href="#Game-Theoretic-Robust-Reinforcement-Learning-Handles-Temporally-Coupled-Perturbations" class="headerlink" title="Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations"></a>Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12062">http://arxiv.org/abs/2307.12062</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongyuan Liang, Yanchao Sun, Ruijie Zheng, Xiangyu Liu, Tuomas Sandholm, Furong Huang, Stephen McAleer</li>
<li>for:  Robust reinforcement learning (RL) seeks to train policies that can perform well under environment perturbations or adversarial attacks.</li>
<li>methods: GRAD, a novel game-theoretic approach that treats the temporally-coupled robust RL problem as a partially-observable two-player zero-sum game.</li>
<li>results: The proposed approach exhibits significant robustness advantages compared to baselines against both standard and temporally-coupled attacks, in both state and action spaces.Here’s the text in Simplified Chinese:</li>
<li>for:  Robust reinforcement learning (RL) 实现训练策略，能够在环境干扰或敌方攻击下表现良好。</li>
<li>methods: GRAD，一种新的游戏理论方法，将当前时间步骤中的干扰视为一个部分可观察的零余游戏。</li>
<li>results: 提案的方法在多个连续控制任务上展现了与基准相比的显著强健优势，包括标准和时间相依的干扰攻击。<details>
<summary>Abstract</summary>
Robust reinforcement learning (RL) seeks to train policies that can perform well under environment perturbations or adversarial attacks. Existing approaches typically assume that the space of possible perturbations remains the same across timesteps. However, in many settings, the space of possible perturbations at a given timestep depends on past perturbations. We formally introduce temporally-coupled perturbations, presenting a novel challenge for existing robust RL methods. To tackle this challenge, we propose GRAD, a novel game-theoretic approach that treats the temporally-coupled robust RL problem as a partially-observable two-player zero-sum game. By finding an approximate equilibrium in this game, GRAD ensures the agent's robustness against temporally-coupled perturbations. Empirical experiments on a variety of continuous control tasks demonstrate that our proposed approach exhibits significant robustness advantages compared to baselines against both standard and temporally-coupled attacks, in both state and action spaces.
</details>
<details>
<summary>摘要</summary>
Robust reinforcement learning（RL）寻求训练策略，以便在环境干扰或敌意攻击时表现出色。现有方法通常假设每个时间步骤的可能的干扰空间保持不变。然而，在许多情况下，每个时间步骤的可能的干扰空间取决于过去的干扰。我们正式引入了时间相关的干扰，并提出了一种新的挑战 для现有的robust RL方法。为解决这个挑战，我们提议GRAD，一种基于游戏理论的新方法，将时间相关的Robust RL问题视为一个部分可见的两个玩家零加游戏。通过在这个游戏中找到一个近似平衡，GRAD确保了机器人的对时间相关干扰的 Robustness。empirical experiments表明，我们的提议方法在许多连续控制任务上具有显著的Robustness优势，比基准方法更能抵抗标准和时间相关的攻击，包括状态和动作空间。
</details></li>
</ul>
<hr>
<h2 id="Fast-Knowledge-Graph-Completion-using-Graphics-Processing-Units"><a href="#Fast-Knowledge-Graph-Completion-using-Graphics-Processing-Units" class="headerlink" title="Fast Knowledge Graph Completion using Graphics Processing Units"></a>Fast Knowledge Graph Completion using Graphics Processing Units</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12059">http://arxiv.org/abs/2307.12059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chun-Hee Lee, Dong-oh Kang, Hwa Jeon Song</li>
<li>for: 提高知识图的完善性，增加新的关系</li>
<li>methods: 使用GPU加速知识图完善框架，将知识图完善问题转化为相似Join问题，并使用度量空间的性质 derivate关键公式，实现快速的知识图完善</li>
<li>results: 实验表明，提出的框架可以高效处理知识图完善问题<details>
<summary>Abstract</summary>
Knowledge graphs can be used in many areas related to data semantics such as question-answering systems, knowledge based systems. However, the currently constructed knowledge graphs need to be complemented for better knowledge in terms of relations. It is called knowledge graph completion. To add new relations to the existing knowledge graph by using knowledge graph embedding models, we have to evaluate $N\times N \times R$ vector operations, where $N$ is the number of entities and $R$ is the number of relation types. It is very costly.   In this paper, we provide an efficient knowledge graph completion framework on GPUs to get new relations using knowledge graph embedding vectors. In the proposed framework, we first define "transformable to a metric space" and then provide a method to transform the knowledge graph completion problem into the similarity join problem for a model which is "transformable to a metric space". After that, to efficiently process the similarity join problem, we derive formulas using the properties of a metric space. Based on the formulas, we develop a fast knowledge graph completion algorithm. Finally, we experimentally show that our framework can efficiently process the knowledge graph completion problem.
</details>
<details>
<summary>摘要</summary>
知识图可以在数据 semantics 中的多个领域应用，如问答系统、知识基于系统。然而，目前构建的知识图需要进一步补充以获得更好的知识，这被称为知识图完成。为在现有知识图中添加新关系，我们需要评估 $N\times N \times R$ 矩阵操作，其中 $N$ 是实体的数量，$R$ 是关系类型的数量。这是非常昂贵的。在这篇论文中，我们提供了一种高效的知识图完成框架，使用 GPU 进行加速。我们首先定义 "可转换到度量空间"，然后将知识图完成问题转换成度量空间中的相似Join问题。然后，我们使用度量空间的性质 derive  formulas，并根据这些方程开发了一种快速的知识图完成算法。最后，我们通过实验证明了我们的框架可以高效处理知识图完成问题。
</details></li>
</ul>
<hr>
<h2 id="Exploring-MLOps-Dynamics-An-Experimental-Analysis-in-a-Real-World-Machine-Learning-Project"><a href="#Exploring-MLOps-Dynamics-An-Experimental-Analysis-in-a-Real-World-Machine-Learning-Project" class="headerlink" title="Exploring MLOps Dynamics: An Experimental Analysis in a Real-World Machine Learning Project"></a>Exploring MLOps Dynamics: An Experimental Analysis in a Real-World Machine Learning Project</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13473">http://arxiv.org/abs/2307.13473</a></li>
<li>repo_url: None</li>
<li>paper_authors: Awadelrahman M. A. Ahmed</li>
<li>For: This paper aims to optimize the MLOps process to enhance the efficiency and effectiveness of machine learning projects.* Methods: The paper employs a comprehensive MLOps workflow that covers essential phases like problem definition, data acquisition, data preparation, model development, model deployment, monitoring, management, scalability, and governance and compliance. The study also utilizes a systematic tracking approach to document revisits to specific phases and constructs a matrix to quantify the degree of overlap between phases.* Results: The paper provides valuable insights into the dynamic and iterative nature of the MLOps workflow, offering practical tips and recommendations for optimizing the workflow. The resulting data provides visual representations of the interdependencies and iterative characteristics of the MLOps process, contributing to enhancing the efficiency and effectiveness of machine learning projects.Here’s the simplified Chinese text for the three key points:* For: 这篇论文目标是优化机器学习操作（MLOps）过程，以提高机器学习项目的效率和效果。* Methods: 这篇论文采用了一个全面的 MLOps 工作流程，覆盖了重要的阶段，如问题定义、数据获取、数据准备、模型开发、模型部署、监控、管理、扩展性和合规性。研究还采用了系统的跟踪方法来记录特定阶段的重新访问，并构建了一个矩阵来衡量阶段之间的重叠度。* Results: 这篇论文提供了有价值的实践建议和技术指南，以便优化 MLOps 过程。研究还提供了可见的数据表示MLOps 过程的相互关系和迭代特性，为实际应用提供了有价值的指导。<details>
<summary>Abstract</summary>
This article presents an experiment focused on optimizing the MLOps (Machine Learning Operations) process, a crucial aspect of efficiently implementing machine learning projects. The objective is to identify patterns and insights to enhance the MLOps workflow, considering its iterative and interdependent nature in real-world model development scenarios.   The experiment involves a comprehensive MLOps workflow, covering essential phases like problem definition, data acquisition, data preparation, model development, model deployment, monitoring, management, scalability, and governance and compliance. Practical tips and recommendations are derived from the results, emphasizing proactive planning and continuous improvement for the MLOps workflow.   The experimental investigation was strategically integrated within a real-world ML project which followed essential phases of the MLOps process in a production environment, handling large-scale structured data. A systematic tracking approach was employed to document revisits to specific phases from a main phase under focus, capturing the reasons for such revisits. By constructing a matrix to quantify the degree of overlap between phases, the study unveils the dynamic and iterative nature of the MLOps workflow.   The resulting data provides visual representations of the MLOps process's interdependencies and iterative characteristics within the experimental framework, offering valuable insights for optimizing the workflow and making informed decisions in real-world scenarios. This analysis contributes to enhancing the efficiency and effectiveness of machine learning projects through an improved MLOps process.   Keywords: MLOps, Machine Learning Operations, Optimization, Experimental Analysis, Iterative Process, Pattern Identification.
</details>
<details>
<summary>摘要</summary>
本文描述了一项实验，旨在优化机器学习操作（MLOps）过程，这是实现机器学习项目的关键一环。实验的目标是识别MLOps工作流程中的模式和洞察，以提高实际场景中模型开发的MLOps工作流程。实验包括了MLOps工作流程的全面覆盖，从问题定义、数据收集、数据准备、模型开发、模型部署、监测、管理、可扩展性和合规性等主要阶段。实验结果提供了实践的建议和推荐，强调了积极的规划和不断改进，以提高MLOps工作流程的效率和效iveness。实验中使用了一个真实的机器学习项目，该项目遵循了MLOps工作流程的主要阶段，处理大规模结构化数据。通过系统地跟踪 revisits 到特定阶段，记录了 revisits 的原因，并由此构建了度量MLOps工作流程中各阶段之间的重叠度的矩阵。这种方法显示了MLOps工作流程的动态和融合特性。实验结果提供了 ML Ops 过程中各阶段之间的相互关联和迭代特性的可见表示，为实际场景中优化 ML Ops 过程提供了有价值的洞察。这种分析对于提高机器学习项目的效率和效iveness有很大的贡献。关键词：MLOps, 机器学习操作, 优化, 实验分析, 迭代过程, 模式识别.
</details></li>
</ul>
<hr>
<h2 id="Extracting-Molecular-Properties-from-Natural-Language-with-Multimodal-Contrastive-Learning"><a href="#Extracting-Molecular-Properties-from-Natural-Language-with-Multimodal-Contrastive-Learning" class="headerlink" title="Extracting Molecular Properties from Natural Language with Multimodal Contrastive Learning"></a>Extracting Molecular Properties from Natural Language with Multimodal Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12996">http://arxiv.org/abs/2307.12996</a></li>
<li>repo_url: None</li>
<li>paper_authors: Romain Lacombe, Andrew Gaut, Jeff He, David Lüdeke, Kateryna Pistunova</li>
<li>for: 本研究旨在将科学知识从文本中提取到分子图表示中。</li>
<li>methods: 研究使用对比学习将神经图表示与文本描述的特征进行对应，并使用神经相关性分配策略提高文本检索。此外，我们还提出了一种基于有机反应的新型分子图生成策略。</li>
<li>results: 研究表明，在下游分子网络Property Classification任务中，我们的模型表现出了+4.26% AUROC的提升和+1.54%的提升 compared to MoMu模型（Su et al. 2022）。<details>
<summary>Abstract</summary>
Deep learning in computational biochemistry has traditionally focused on molecular graphs neural representations; however, recent advances in language models highlight how much scientific knowledge is encoded in text. To bridge these two modalities, we investigate how molecular property information can be transferred from natural language to graph representations. We study property prediction performance gains after using contrastive learning to align neural graph representations with representations of textual descriptions of their characteristics. We implement neural relevance scoring strategies to improve text retrieval, introduce a novel chemically-valid molecular graph augmentation strategy inspired by organic reactions, and demonstrate improved performance on downstream MoleculeNet property classification tasks. We achieve a +4.26% AUROC gain versus models pre-trained on the graph modality alone, and a +1.54% gain compared to recently proposed molecular graph/text contrastively trained MoMu model (Su et al. 2022).
</details>
<details>
<summary>摘要</summary>
深度学习在计算生物化学中传统上专注于分子图神经表示;然而，最近的语言模型发展显示了科学知识如何被编码在文本中。为了融合这两种模式，我们研究如何从自然语言中提取分子性质信息，并将其转换为图表示。我们使用对比学习对神经图表示和文本描述的特征进行对齐，并使用神经相关性分配策略来提高文本检索。我们还开发了一种基于有机反应的新型化学图像增强策略，并在下游的分子网络性质分类任务上实现了+4.26%的AUROC提升和+1.54%的提升，相比之前的图模式和文本对照训练的MoMu模型（Su et al. 2022）。
</details></li>
</ul>
<hr>
<h2 id="Flight-Contrail-Segmentation-via-Augmented-Transfer-Learning-with-Novel-SR-Loss-Function-in-Hough-Space"><a href="#Flight-Contrail-Segmentation-via-Augmented-Transfer-Learning-with-Novel-SR-Loss-Function-in-Hough-Space" class="headerlink" title="Flight Contrail Segmentation via Augmented Transfer Learning with Novel SR Loss Function in Hough Space"></a>Flight Contrail Segmentation via Augmented Transfer Learning with Novel SR Loss Function in Hough Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12032">http://arxiv.org/abs/2307.12032</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/junzis/contrail-net">https://github.com/junzis/contrail-net</a></li>
<li>paper_authors: Junzi Sun, Esther Roosenbrand</li>
<li>for: 检测飞行 contrails 的卫星图像中的 contrails，减少环境影响和气候变化。</li>
<li>methods: 使用加强转移学习模型和 SR Loss 函数，从 varying 的图像条件下准确地检测 contrails。</li>
<li>results: 提供了一种新的 contrail 检测模型，可以在 minimal 数据下准确地检测 contrails，并且开辟了新的机器学习在航空研究中的应用前景。<details>
<summary>Abstract</summary>
Air transport poses significant environmental challenges, particularly the contribution of flight contrails to climate change due to their potential global warming impact. Detecting contrails from satellite images has been a long-standing challenge. Traditional computer vision techniques have limitations under varying image conditions, and machine learning approaches using typical convolutional neural networks are hindered by the scarcity of hand-labeled contrail datasets and contrail-tailored learning processes. In this paper, we introduce an innovative model based on augmented transfer learning that accurately detects contrails with minimal data. We also propose a novel loss function, SR Loss, which improves contrail line detection by transforming the image space into Hough space. Our research opens new avenues for machine learning-based contrail detection in aviation research, offering solutions to the lack of large hand-labeled datasets, and significantly enhancing contrail detection models.
</details>
<details>
<summary>摘要</summary>
飞行交通对环境造成重要挑战，特别是飞机烟尘对气候变化的贡献，这些烟尘可能对全球暖化产生影响。传统的计算机视觉技术在不同的图像条件下有限制，机器学习方法使用Typical Convolutional Neural Networks (CNNs) 受到手动标注烟尘数据的罕见性和烟尘特化学习过程的限制。在这篇论文中，我们介绍了一种创新的模型，基于增强转移学习，可以准确地检测烟尘。我们还提出了一个新的损失函数，SR损失，该函数在图像空间转换到抽象空间，从而提高烟尘线检测。我们的研究开启了新的机器学习基于烟尘检测的可能性，为航空研究提供了新的解决方案，并对烟尘检测模型进行了重要改进。
</details></li>
</ul>
<hr>
<h2 id="FinPT-Financial-Risk-Prediction-with-Profile-Tuning-on-Pretrained-Foundation-Models"><a href="#FinPT-Financial-Risk-Prediction-with-Profile-Tuning-on-Pretrained-Foundation-Models" class="headerlink" title="FinPT: Financial Risk Prediction with Profile Tuning on Pretrained Foundation Models"></a>FinPT: Financial Risk Prediction with Profile Tuning on Pretrained Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00065">http://arxiv.org/abs/2308.00065</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuweiyin/finpt">https://github.com/yuweiyin/finpt</a></li>
<li>paper_authors: Yuwei Yin, Yazheng Yang, Jian Yang, Qi Liu</li>
<li>for: 该论文旨在提出一种新的金融风险预测方法，以及一个开源的金融标准benchmark。</li>
<li>methods: 该方法使用Profile Tuning技术，将大型预训模型与自然语言客户 profiling 结合，以便更好地预测金融风险。</li>
<li>results: 实验表明，该方法可以与一系列代表性的强基线进行比较，并且可以更好地预测金融风险。further analysis也深入了解了LLMs在金融风险预测方面的性能。<details>
<summary>Abstract</summary>
Financial risk prediction plays a crucial role in the financial sector. Machine learning methods have been widely applied for automatically detecting potential risks and thus saving the cost of labor. However, the development in this field is lagging behind in recent years by the following two facts: 1) the algorithms used are somewhat outdated, especially in the context of the fast advance of generative AI and large language models (LLMs); 2) the lack of a unified and open-sourced financial benchmark has impeded the related research for years. To tackle these issues, we propose FinPT and FinBench: the former is a novel approach for financial risk prediction that conduct Profile Tuning on large pretrained foundation models, and the latter is a set of high-quality datasets on financial risks such as default, fraud, and churn. In FinPT, we fill the financial tabular data into the pre-defined instruction template, obtain natural-language customer profiles by prompting LLMs, and fine-tune large foundation models with the profile text to make predictions. We demonstrate the effectiveness of the proposed FinPT by experimenting with a range of representative strong baselines on FinBench. The analytical studies further deepen the understanding of LLMs for financial risk prediction.
</details>
<details>
<summary>摘要</summary>
金融风险预测在金融领域扮演着关键的角色。机器学习技术已广泛应用于自动检测potential risks，从而节省劳动成本。然而，在这一领域的发展受到了两个因素的压缩：1）使用的算法相对落后，尤其是在生成AI和大语言模型（LLM）的快速发展的背景下；2）缺乏一个统一的开源金融标准 benchmark，对相关研究造成了年月的妨碍。为解决这些问题，我们提出了FinPT和FinBench：前者是一种新的金融风险预测方法，通过在大型预训模型上进行 Profile Tuning，并使用自然语言客户 profiling 来提高预测精度；后者是一个高质量的金融风险数据集，包括落后、诈骗和迁移等风险。在FinPT中，我们将金融表格数据填充到预定的指示模板中，通过LLM提取自然语言客户 profiling，并使用这些 profiling 来练化大型基础模型以进行预测。我们通过对一系列代表强基eline进行实验，证明了FinPT的效果。分析研究还深入了解LLMs在金融风险预测方面的性能。
</details></li>
</ul>
<hr>
<h2 id="A-Flexible-Framework-for-Incorporating-Patient-Preferences-Into-Q-Learning"><a href="#A-Flexible-Framework-for-Incorporating-Patient-Preferences-Into-Q-Learning" class="headerlink" title="A Flexible Framework for Incorporating Patient Preferences Into Q-Learning"></a>A Flexible Framework for Incorporating Patient Preferences Into Q-Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12022">http://arxiv.org/abs/2307.12022</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joshua P. Zitovsky, Leslie Wilson, Michael R. Kosorok</li>
<li>for: 这篇研究旨在解决现实医疗问题中的多重竞争结果（例如治疗效果和副作用严重程度），现有的统计方法 для预测动态治疗方案（DTR）通常假设单一结果的 interess，而几 Methods 则受到重要的限制。</li>
<li>methods: 我们提出了一新的方法，即潜在价值Q学习（LUQ-Learning），它使用潜在模型方法来自然地扩展Q学习到多元结果设定下，并遵循每名病人的理想交易。不同于先前的方法，我们的框架允许无限多个时间点和结果，并将自愿报告的病人偏好纳入考虑。</li>
<li>results: 我们在基于低背痛试验和已经完成的治疗试验的 simulated experiments 中，与多个替代基准点进行比较，结果显示我们的方法在调节多元结果下具有高度竞争的实验性表现。<details>
<summary>Abstract</summary>
In real-world healthcare problems, there are often multiple competing outcomes of interest, such as treatment efficacy and side effect severity. However, statistical methods for estimating dynamic treatment regimes (DTRs) usually assume a single outcome of interest, and the few methods that deal with composite outcomes suffer from important limitations. This includes restrictions to a single time point and two outcomes, the inability to incorporate self-reported patient preferences and limited theoretical guarantees. To this end, we propose a new method to address these limitations, which we dub Latent Utility Q-Learning (LUQ-Learning). LUQ-Learning uses a latent model approach to naturally extend Q-learning to the composite outcome setting and adopt the ideal trade-off between outcomes to each patient. Unlike previous approaches, our framework allows for an arbitrary number of time points and outcomes, incorporates stated preferences and achieves strong asymptotic performance with realistic assumptions on the data. We conduct simulation experiments based on an ongoing trial for low back pain as well as a well-known completed trial for schizophrenia. In all experiments, our method achieves highly competitive empirical performance compared to several alternative baselines.
</details>
<details>
<summary>摘要</summary>
在现实医疗问题中，经常存在多个竞争的目标结果，如治疗效果和生化效果的严重程度。然而，统计方法用于估计动态治疗方案（DTR）通常假设单一的目标结果，而其中几种方法都受到重要的限制。这包括单一时间点和两个结果的限制，无法 интеグ勋自报告病人偏好以及有限的理论保证。为此，我们提出了一种新的方法，我们称之为潜在用户Q学习（LUQ-Learning）。LUQ-Learning使用潜在模型方法来自然地扩展Q学习到复合结果设定下，采取每个患者的理想的质量平衡。与之前的方法不同，我们的框架允许任意的时间点和结果数量，并包括自报告偏好，实现了实际数据下有理的假设下的强 asymptotic performance。我们在一个低背疼患者试验中进行了 simulate实验，以及一个已经完成的试验数据集。在所有实验中，我们的方法与多个基准方法进行了高度竞争性的实验性表现。
</details></li>
</ul>
<hr>
<h2 id="Model-Predictive-Control-MPC-of-an-Artificial-Pancreas-with-Data-Driven-Learning-of-Multi-Step-Ahead-Blood-Glucose-Predictors"><a href="#Model-Predictive-Control-MPC-of-an-Artificial-Pancreas-with-Data-Driven-Learning-of-Multi-Step-Ahead-Blood-Glucose-Predictors" class="headerlink" title="Model Predictive Control (MPC) of an Artificial Pancreas with Data-Driven Learning of Multi-Step-Ahead Blood Glucose Predictors"></a>Model Predictive Control (MPC) of an Artificial Pancreas with Data-Driven Learning of Multi-Step-Ahead Blood Glucose Predictors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12015">http://arxiv.org/abs/2307.12015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eleonora Maria Aiello, Mehrad Jaloli, Marzia Cescon</li>
<li>For: The paper is written for treating type 1 diabetes (T1D) using a closed-loop insulin delivery algorithm that incorporates a data-driven multi-step-ahead blood glucose (BG) predictor and a Linear Time-Varying (LTV) Model Predictive Control (MPC) framework.* Methods: The paper proposes a nonlinear function of past input-output data and an affine function of future insulin control inputs to predict future BG concentrations. Specifically, a Long Short-Term Memory (LSTM) network is used for the nonlinear part, and a linear regression model is used for the affine component.* Results: The proposed LSTM-MPC controller outperformed a traditional linear MPC based on an auto-regressive with exogenous (ARX) input model in terms of accuracy of future glucose concentrations and closed-loop performance. Specifically, the LSTM-MPC controller achieved a mean $\pm$ standard deviation percent time in the range 70-180 [mg&#x2F;dL] of 74.99 $\pm$ 7.09, compared to 54.15 $\pm$ 14.89 for the ARX-MPC controller. Additionally, the LSTM-MPC controller had less severe hypoglycemia, with a mean $\pm$ standard deviation percent time in the range 70-140 [mg&#x2F;dL] of 47.78$\pm$8.55, compared to 34.62 $\pm$9.04 for the ARX-MPC controller.<details>
<summary>Abstract</summary>
We present the design and \textit{in-silico} evaluation of a closed-loop insulin delivery algorithm to treat type 1 diabetes (T1D) consisting in a data-driven multi-step-ahead blood glucose (BG) predictor integrated into a Linear Time-Varying (LTV) Model Predictive Control (MPC) framework. Instead of identifying an open-loop model of the glucoregulatory system from available data, we propose to directly fit the entire BG prediction over a predefined prediction horizon to be used in the MPC, as a nonlinear function of past input-ouput data and an affine function of future insulin control inputs. For the nonlinear part, a Long Short-Term Memory (LSTM) network is proposed, while for the affine component a linear regression model is chosen. To assess benefits and drawbacks when compared to a traditional linear MPC based on an auto-regressive with exogenous (ARX) input model identified from data, we evaluated the proposed LSTM-MPC controller in three simulation scenarios: a nominal case with 3 meals per day, a random meal disturbances case where meals were generated with a recently published meal generator, and a case with 25$\%$ decrease in the insulin sensitivity. Further, in all the scenarios, no feedforward meal bolus was administered. For the more challenging random meal generation scenario, the mean $\pm$ standard deviation percent time in the range 70-180 [mg/dL] was 74.99 $\pm$ 7.09 vs. 54.15 $\pm$ 14.89, the mean $\pm$ standard deviation percent time in the tighter range 70-140 [mg/dL] was 47.78$\pm$8.55 vs. 34.62 $\pm$9.04, while the mean $\pm$ standard deviation percent time in sever hypoglycemia, i.e., $<$ 54 [mg/dl] was 1.00$\pm$3.18 vs. 9.45$\pm$11.71, for our proposed LSTM-MPC controller and the traditional ARX-MPC, respectively. Our approach provided accurate predictions of future glucose concentrations and good closed-loop performances of the overall MPC controller.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种闭环式血糖采集算法，用于治疗一型糖尿病（T1D），该算法包括一个基于数据驱动的多步预测血糖（BG）预测器，并与线性时间变化（LTV）模型预测控制（MPC）框架集成。而不是直接从可用数据中Identify一个开环模型，我们提议直接使用前一个预测 horizon的整个BG预测作为一个非线性函数，并用未来药物控制输入作为一个线性函数。为非线性部分，我们提议使用一个Long Short-Term Memory（LSTM）网络，而为线性部分，我们选择了一个线性回归模型。为了评估我们的LSTM-MPC控制器和传统的ARX-MPC控制器之间的优劣点，我们在三个模拟场景中进行了评估：一个标准情况下，每天有3顿饭，一个随机饭物干扰情况，饭物是通过最近发布的饭物生成器生成的，以及一个25%的药物敏感度下降情况。此外，在所有场景下，没有前向补偿饭物背包。在更加具有挑战性的随机饭物生成场景下， mean ± 标准差%时间在70-180[mg/dL]范围内为74.99 ± 7.09 vs. 54.15 ± 14.89， mean ± 标准差%时间在70-140[mg/dL]范围内为47.78 ± 8.55 vs. 34.62 ± 9.04，而mean ± 标准差%时间在严重低血糖（<54[mg/dL]）下为1.00 ± 3.18 vs. 9.45 ± 11.71。我们的方法提供了准确的未来血糖浓度预测和闭环式MPC控制器的全面性能。
</details></li>
</ul>
<hr>
<h2 id="NLCUnet-Single-Image-Super-Resolution-Network-with-Hairline-Details"><a href="#NLCUnet-Single-Image-Super-Resolution-Network-with-Hairline-Details" class="headerlink" title="NLCUnet: Single-Image Super-Resolution Network with Hairline Details"></a>NLCUnet: Single-Image Super-Resolution Network with Hairline Details</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12014">http://arxiv.org/abs/2307.12014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiancong Feng, Yuan-Gen Wang, Fengchuang Xing</li>
<li>For: 这篇论文旨在提高单图超解像的精度。* Methods: 论文提出了三个核心设计：首先，引入非本地注意力机制，以恢复图像地方的局部 Piece;然后，发现现有工作中的模糊核心可以忽略，因此提出了一种新的网络架构， integrate depth-wise convolution 和通道注意力，而不需要模糊核心估计。最后，为了让剪辑区域尽可能包含semantic信息，提出了随机64×64剪辑在中心512×512剪辑中 instead of direct random crop inside the whole image of 2K size。* Results: 对DF2K数据集进行了许多实验，并证明了我们的NLCUnet在PSNR和SSIM指标上比 estado-of-the-art 性能更好，并且在视觉上也更有优势。<details>
<summary>Abstract</summary>
Pursuing the precise details of super-resolution images is challenging for single-image super-resolution tasks. This paper presents a single-image super-resolution network with hairline details (termed NLCUnet), including three core designs. Specifically, a non-local attention mechanism is first introduced to restore local pieces by learning from the whole image region. Then, we find that the blur kernel trained by the existing work is unnecessary. Based on this finding, we create a new network architecture by integrating depth-wise convolution with channel attention without the blur kernel estimation, resulting in a performance improvement instead. Finally, to make the cropped region contain as much semantic information as possible, we propose a random 64$\times$64 crop inside the central 512$\times$512 crop instead of a direct random crop inside the whole image of 2K size. Numerous experiments conducted on the benchmark DF2K dataset demonstrate that our NLCUnet performs better than the state-of-the-art in terms of the PSNR and SSIM metrics and yields visually favorable hairline details.
</details>
<details>
<summary>摘要</summary>
追求超解像的细节精度在单图超解像 задании中是一项挑战。这篇论文提出了一种单图超解像网络（NLCUnet），包括三个核心设计。具体来说，我们首先引入非本地注意力机制，用于在整个图像区域学习并复原本地精度。然后，我们发现现有工作中用于训练模糊核的权重是不必要的，因此我们创建了一个新的网络架构，将深度wise核论并 Channel Attention 结合使用，从而实现性能提升。最后，我们提议在中心 512x512 像素区域内随机选择 64x64 像素的区域，以便尽可能多地保留图像中的semantic信息。经过在 DF2K 数据集上进行了多次实验，我们发现我们的 NLCUnet 在 PSNR 和 SSIM 指标上表现出色，并且可以提供更加有吸引力的毛细膨胀细节。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Self-Supervised-Learning-Based-Approach-for-Patient-Similarity-A-Case-Study-on-Atrial-Fibrillation-Detection-from-PPG-Signal"><a href="#Contrastive-Self-Supervised-Learning-Based-Approach-for-Patient-Similarity-A-Case-Study-on-Atrial-Fibrillation-Detection-from-PPG-Signal" class="headerlink" title="Contrastive Self-Supervised Learning Based Approach for Patient Similarity: A Case Study on Atrial Fibrillation Detection from PPG Signal"></a>Contrastive Self-Supervised Learning Based Approach for Patient Similarity: A Case Study on Atrial Fibrillation Detection from PPG Signal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02433">http://arxiv.org/abs/2308.02433</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/subangkar/simsig">https://github.com/subangkar/simsig</a></li>
<li>paper_authors: Subangkar Karmaker Shanto, Shoumik Saha, Atif Hasan Rahman, Mohammad Mehedy Masud, Mohammed Eunus Ali</li>
<li>for: 用于搜索类似病人，使用生物信号physiological signal进行深度学习搜索。</li>
<li>methods: 使用异构学习方法学习类似病人的生物信号数据，并提出了一些邻居选择算法来确定最相似的病人。</li>
<li>results: 对于检测心脏病Atrial Fibrillation（AF），通过光敏电生物学（PPG）信号来评估病人类似性，并在大量实验中证明了其效果。<details>
<summary>Abstract</summary>
In this paper, we propose a novel contrastive learning based deep learning framework for patient similarity search using physiological signals. We use a contrastive learning based approach to learn similar embeddings of patients with similar physiological signal data. We also introduce a number of neighbor selection algorithms to determine the patients with the highest similarity on the generated embeddings. To validate the effectiveness of our framework for measuring patient similarity, we select the detection of Atrial Fibrillation (AF) through photoplethysmography (PPG) signals obtained from smartwatch devices as our case study. We present extensive experimentation of our framework on a dataset of over 170 individuals and compare the performance of our framework with other baseline methods on this dataset.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种基于对比学习的深度学习框架，用于基于生物physiological signal的患者相似性搜索。我们使用对比学习方法来学习患者的生物信号数据中的相似性 embedding。我们还介绍了一些邻居选择算法，用于确定生成的 embedding 中的最高相似度患者。为了证明我们的框架对患者相似性的评估效果，我们选择了基于 photoplethysmography (PPG) 信号的 Atrial Fibrillation (AF) 检测作为我们的案例研究。我们对一个包含超过 170 人的数据集进行了广泛的实验，并与其他基线方法进行比较。
</details></li>
</ul>
<hr>
<h2 id="Expert-Knowledge-Aware-Image-Difference-Graph-Representation-Learning-for-Difference-Aware-Medical-Visual-Question-Answering"><a href="#Expert-Knowledge-Aware-Image-Difference-Graph-Representation-Learning-for-Difference-Aware-Medical-Visual-Question-Answering" class="headerlink" title="Expert Knowledge-Aware Image Difference Graph Representation Learning for Difference-Aware Medical Visual Question Answering"></a>Expert Knowledge-Aware Image Difference Graph Representation Learning for Difference-Aware Medical Visual Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11986">http://arxiv.org/abs/2307.11986</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/holipori/mimic-diff-vqa">https://github.com/holipori/mimic-diff-vqa</a></li>
<li>paper_authors: Xinyue Hu, Lin Gu, Qiyuan An, Mengliang Zhang, Liangchen Liu, Kazuma Kobayashi, Tatsuya Harada, Ronald M. Summers, Yingying Zhu</li>
<li>for: 提高医疗机器人视觉语言模型的自动化水平，并且更好地识别医疗图像的差异。</li>
<li>methods: 提出了一种新的胸部X射线图像差异视觉问答任务，并收集了700,703个问答对，来评估医疗图像的差异。</li>
<li>results: 通过利用专家知识来构建多关系图表，提高了医疗图像差异问答任务的性能。<details>
<summary>Abstract</summary>
To contribute to automating the medical vision-language model, we propose a novel Chest-Xray Difference Visual Question Answering (VQA) task. Given a pair of main and reference images, this task attempts to answer several questions on both diseases and, more importantly, the differences between them. This is consistent with the radiologist's diagnosis practice that compares the current image with the reference before concluding the report. We collect a new dataset, namely MIMIC-Diff-VQA, including 700,703 QA pairs from 164,324 pairs of main and reference images. Compared to existing medical VQA datasets, our questions are tailored to the Assessment-Diagnosis-Intervention-Evaluation treatment procedure used by clinical professionals. Meanwhile, we also propose a novel expert knowledge-aware graph representation learning model to address this task. The proposed baseline model leverages expert knowledge such as anatomical structure prior, semantic, and spatial knowledge to construct a multi-relationship graph, representing the image differences between two images for the image difference VQA task. The dataset and code can be found at https://github.com/Holipori/MIMIC-Diff-VQA. We believe this work would further push forward the medical vision language model.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:为推动医疗视力语言模型的自动化，我们提出了一个新的胸部X射影异常视觉问答任务（VQA）。给定两个主要和参考图像，这个任务试图回答几个疾病和两个图像之间的差异问题。这与诊断医生在比较当前图像和参考图像后作出报告的实践相符。我们收集了一个新的数据集，名为MIMIC-Diff-VQA，包含700,703个问题对从164,324对主要和参考图像中。相比现有的医学VQA数据集，我们的问题更加适应诊断、诊断、治疗和评估过程中的临床专业人员的做法。同时，我们还提出了一种基于专家知识的图像差异图表学习模型来解决这个任务。我们的基eline模型利用了专家知识，如生物结构先天知识、semantic知识和空间知识，构建多关系图，表示两个图像之间的差异。数据集和代码可以在https://github.com/Holipori/MIMIC-Diff-VQA中找到。我们认为这项工作将会驱动医疗视力语言模型的进一步发展。
</details></li>
</ul>
<hr>
<h2 id="Collaborative-Graph-Neural-Networks-for-Attributed-Network-Embedding"><a href="#Collaborative-Graph-Neural-Networks-for-Attributed-Network-Embedding" class="headerlink" title="Collaborative Graph Neural Networks for Attributed Network Embedding"></a>Collaborative Graph Neural Networks for Attributed Network Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11981">http://arxiv.org/abs/2307.11981</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qiaoyut/conn">https://github.com/qiaoyut/conn</a></li>
<li>paper_authors: Qiaoyu Tan, Xin Zhang, Xiao Huang, Hao Chen, Jundong Li, Xia Hu</li>
<li>for: 这篇论文的目的是提出一种适应属性网络嵌入的Graph Neural Networks（GNN）架构，以提高模型的容量和表现。</li>
<li>methods: 这篇论文使用的方法包括：1) 选择性地传递邻近节点和受到属性类别的消息，2) 同时重建节点到节点和节点到属性类别的互动。</li>
<li>results: 实验结果表明，这篇论文提出的CONN架构在真实的网络上表现出了明显的优势，与现有的 embedding 算法相比，具有较大的margin。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have shown prominent performance on attributed network embedding. However, existing efforts mainly focus on exploiting network structures, while the exploitation of node attributes is rather limited as they only serve as node features at the initial layer. This simple strategy impedes the potential of node attributes in augmenting node connections, leading to limited receptive field for inactive nodes with few or even no neighbors. Furthermore, the training objectives (i.e., reconstructing network structures) of most GNNs also do not include node attributes, although studies have shown that reconstructing node attributes is beneficial. Thus, it is encouraging to deeply involve node attributes in the key components of GNNs, including graph convolution operations and training objectives. However, this is a nontrivial task since an appropriate way of integration is required to maintain the merits of GNNs. To bridge the gap, in this paper, we propose COllaborative graph Neural Networks--CONN, a tailored GNN architecture for attribute network embedding. It improves model capacity by 1) selectively diffusing messages from neighboring nodes and involved attribute categories, and 2) jointly reconstructing node-to-node and node-to-attribute-category interactions via cross-correlation. Experiments on real-world networks demonstrate that CONN excels state-of-the-art embedding algorithms with a great margin.
</details>
<details>
<summary>摘要</summary>
Graph Neural Networks (GNNs) have shown prominent performance on attributed network embedding. However, existing efforts mainly focus on exploiting network structures, while the exploitation of node attributes is rather limited as they only serve as node features at the initial layer. This simple strategy impedes the potential of node attributes in augmenting node connections, leading to limited receptive field for inactive nodes with few or even no neighbors. Furthermore, the training objectives (i.e., reconstructing network structures) of most GNNs also do not include node attributes, although studies have shown that reconstructing node attributes is beneficial. Thus, it is encouraging to deeply involve node attributes in the key components of GNNs, including graph convolution operations and training objectives. However, this is a nontrivial task since an appropriate way of integration is required to maintain the merits of GNNs. To bridge the gap, in this paper, we propose COllaborative graph Neural Networks--CONN, a tailored GNN architecture for attribute network embedding. It improves model capacity by 1) selectively diffusing messages from neighboring nodes and involved attribute categories, and 2) jointly reconstructing node-to-node and node-to-attribute-category interactions via cross-correlation. Experiments on real-world networks demonstrate that CONN excels state-of-the-art embedding algorithms with a great margin.
</details></li>
</ul>
<hr>
<h2 id="Simulation-of-Arbitrary-Level-Contrast-Dose-in-MRI-Using-an-Iterative-Global-Transformer-Model"><a href="#Simulation-of-Arbitrary-Level-Contrast-Dose-in-MRI-Using-an-Iterative-Global-Transformer-Model" class="headerlink" title="Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model"></a>Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11980">http://arxiv.org/abs/2307.11980</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dayang Wang, Srivathsa Pasumarthi, Greg Zaharchuk, Ryan Chamberlain</li>
<li>for: 本研究旨在提出一种基于 transformer 的迭代模型方法，用于Synthesizing 具有不同增强程度的图像，以满足不同类型的 GBCAs 和疾病的需求。</li>
<li>methods: 该方法基于 sub-sampling based attention 机制和旋转 shift 模块，能够 capture 不同增强相关特征。</li>
<li>results: 比较测试表明，提出的模型表现比其他当前状态的方法更好，并在下游任务 such as 剂量减少和肿瘤分 segmentation 中表现出优异的临床实用性。<details>
<summary>Abstract</summary>
Deep learning (DL) based contrast dose reduction and elimination in MRI imaging is gaining traction, given the detrimental effects of Gadolinium-based Contrast Agents (GBCAs). These DL algorithms are however limited by the availability of high quality low dose datasets. Additionally, different types of GBCAs and pathologies require different dose levels for the DL algorithms to work reliably. In this work, we formulate a novel transformer (Gformer) based iterative modelling approach for the synthesis of images with arbitrary contrast enhancement that corresponds to different dose levels. The proposed Gformer incorporates a sub-sampling based attention mechanism and a rotational shift module that captures the various contrast related features. Quantitative evaluation indicates that the proposed model performs better than other state-of-the-art methods. We further perform quantitative evaluation on downstream tasks such as dose reduction and tumor segmentation to demonstrate the clinical utility.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）基于对比剂减少和消除在MRI成像中得到了进展，由于质子剂（GBCAs）的负面影响。这些DL算法然而受到高质量低剂量数据的有效性的限制。此外，不同类型的GBCAs和疾病需要不同的剂量水平以确保DL算法的可靠性。在这种工作中，我们提出了一种基于变换器（Gformer）的迭代模型方法，用于synthesize图像的任意对比强化，与不同的剂量水平相对应。提案中的Gformer包括一种子抽样基于注意机制和一种旋转shift模块，以捕捉不同的对比相关特征。量化评估表明，提案的模型比其他现状顶尖方法性能更好。我们进一步进行了下游任务 such as 剂量减少和肿瘤分 segmentation，以证明临床实用性。
</details></li>
</ul>
<hr>
<h2 id="Why-Is-Prompt-Tuning-for-Vision-Language-Models-Robust-to-Noisy-Labels"><a href="#Why-Is-Prompt-Tuning-for-Vision-Language-Models-Robust-to-Noisy-Labels" class="headerlink" title="Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?"></a>Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11978">http://arxiv.org/abs/2307.11978</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cewu/ptnl">https://github.com/cewu/ptnl</a></li>
<li>paper_authors: Cheng-En Wu, Yu Tian, Haichao Yu, Heng Wang, Pedro Morgado, Yu Hen Hu, Linjie Yang</li>
<li>for: 这个论文主要是研究如何使用CLIP进行几何预处理，以提高预测的准确率。</li>
<li>methods: 这个论文使用了CLIP来学习一个通用的文本-图像嵌入空间，然后通过几何预处理来适应新的分类任务。</li>
<li>results: 这个论文发现，使用几何预处理可以很好地鲁应对标签噪音，并且可以使用不精确的预测来更新预处理。<details>
<summary>Abstract</summary>
Vision-language models such as CLIP learn a generic text-image embedding from large-scale training data. A vision-language model can be adapted to a new classification task through few-shot prompt tuning. We find that such a prompt tuning process is highly robust to label noises. This intrigues us to study the key reasons contributing to the robustness of the prompt tuning paradigm. We conducted extensive experiments to explore this property and find the key factors are: 1) the fixed classname tokens provide a strong regularization to the optimization of the model, reducing gradients induced by the noisy samples; 2) the powerful pre-trained image-text embedding that is learned from diverse and generic web data provides strong prior knowledge for image classification. Further, we demonstrate that noisy zero-shot predictions from CLIP can be used to tune its own prompt, significantly enhancing prediction accuracy in the unsupervised setting. The code is available at https://github.com/CEWu/PTNL.
</details>
<details>
<summary>摘要</summary>
视觉语模型如CLIP通过大规模训练学习一个通用的文本图像嵌入。一个视觉语模型可以通过几张图片推理来适应新的分类任务。我们发现这种推理过程具有很高的鲁棒性，这使我们感兴趣研究这种鲁棒性的关键原因。我们进行了广泛的实验研究，并发现关键因素有：1）固定的类名token提供了模型优化的强大正则化，减少了噪声样本引起的梯度; 2）通过多样化和通用的网络数据学习的强大预训练图像文本嵌入，为图像分类提供了强大的先验知识。此外，我们示出了CLIP的噪声零shot预测可以用来调整其自己的提示，显著提高了无监督设定下的预测精度。代码可以在https://github.com/CEWu/PTNL中找到。
</details></li>
</ul>
<hr>
<h2 id="Out-of-Distribution-Optimality-of-Invariant-Risk-Minimization"><a href="#Out-of-Distribution-Optimality-of-Invariant-Risk-Minimization" class="headerlink" title="Out-of-Distribution Optimality of Invariant Risk Minimization"></a>Out-of-Distribution Optimality of Invariant Risk Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11972">http://arxiv.org/abs/2307.11972</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shoji Toyota, Kenji Fukumizu</li>
<li>for: 这篇论文是为了解释 invariant risk minimization (IRM) 方法的理论基础，并提供了一个准确的证明，证明 IRM 方法可以减少 out-of-distribution (o.o.d.) 风险。</li>
<li>methods: 这篇论文使用了 bi-level 优化问题来解释 IRM 方法，并提供了一个准确的证明，证明 bi-level 优化问题的解决方案可以减少 o.o.d. 风险。</li>
<li>results: 这篇论文提供了一个准确的证明，证明 IRM 方法可以减少 o.o.d. 风险，并提供了一些条件，以确保 bi-level 优化问题的解决方案可以减少 o.o.d. 风险。<details>
<summary>Abstract</summary>
Deep Neural Networks often inherit spurious correlations embedded in training data and hence may fail to generalize to unseen domains, which have different distributions from the domain to provide training data. M. Arjovsky et al. (2019) introduced the concept out-of-distribution (o.o.d.) risk, which is the maximum risk among all domains, and formulated the issue caused by spurious correlations as a minimization problem of the o.o.d. risk. Invariant Risk Minimization (IRM) is considered to be a promising approach to minimize the o.o.d. risk: IRM estimates a minimum of the o.o.d. risk by solving a bi-level optimization problem. While IRM has attracted considerable attention with empirical success, it comes with few theoretical guarantees. Especially, a solid theoretical guarantee that the bi-level optimization problem gives the minimum of the o.o.d. risk has not yet been established. Aiming at providing a theoretical justification for IRM, this paper rigorously proves that a solution to the bi-level optimization problem minimizes the o.o.d. risk under certain conditions. The result also provides sufficient conditions on distributions providing training data and on a dimension of feature space for the bi-leveled optimization problem to minimize the o.o.d. risk.
</details>
<details>
<summary>摘要</summary>
Note:* "Deep Neural Networks" is 深度神经网络 (shēn dì shén zhì wǎng) in Simplified Chinese.* "out-of-distribution" is 外部风险 (wài bù fēng xióng) in Simplified Chinese.* "Invariant Risk Minimization" is 风险规范化 (fēng xióng guī fāng huà) in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="DHC-Dual-debiased-Heterogeneous-Co-training-Framework-for-Class-imbalanced-Semi-supervised-Medical-Image-Segmentation"><a href="#DHC-Dual-debiased-Heterogeneous-Co-training-Framework-for-Class-imbalanced-Semi-supervised-Medical-Image-Segmentation" class="headerlink" title="DHC: Dual-debiased Heterogeneous Co-training Framework for Class-imbalanced Semi-supervised Medical Image Segmentation"></a>DHC: Dual-debiased Heterogeneous Co-training Framework for Class-imbalanced Semi-supervised Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11960">http://arxiv.org/abs/2307.11960</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/dhc">https://github.com/xmed-lab/dhc</a></li>
<li>paper_authors: Haonan Wang, Xiaomeng Li<br>for:这篇论文主要targets semi-supervised learning (SSL) for 3D medical image segmentation, with a focus on addressing the class imbalance problem.methods:The proposed method, called Dual-debiased Heterogeneous Co-training (DHC), leverages two sub-models with different strengths to improve the segmentation accuracy. The method also introduces two loss weighting strategies, DistDW和DiffDW, to dynamically adjust the pseudo labels and alleviate the class imbalance issue.results:Experiments show that the proposed DHC framework significantly improves the segmentation accuracy compared to the state-of-the-art SSL methods, demonstrating its potential for more challenging SSL settings. Additionally, the proposed method outperforms other class-imbalance designs, providing a more robust solution for real-world applications.<details>
<summary>Abstract</summary>
The volume-wise labeling of 3D medical images is expertise-demanded and time-consuming; hence semi-supervised learning (SSL) is highly desirable for training with limited labeled data. Imbalanced class distribution is a severe problem that bottlenecks the real-world application of these methods but was not addressed much. Aiming to solve this issue, we present a novel Dual-debiased Heterogeneous Co-training (DHC) framework for semi-supervised 3D medical image segmentation. Specifically, we propose two loss weighting strategies, namely Distribution-aware Debiased Weighting (DistDW) and Difficulty-aware Debiased Weighting (DiffDW), which leverage the pseudo labels dynamically to guide the model to solve data and learning biases. The framework improves significantly by co-training these two diverse and accurate sub-models. We also introduce more representative benchmarks for class-imbalanced semi-supervised medical image segmentation, which can fully demonstrate the efficacy of the class-imbalance designs. Experiments show that our proposed framework brings significant improvements by using pseudo labels for debiasing and alleviating the class imbalance problem. More importantly, our method outperforms the state-of-the-art SSL methods, demonstrating the potential of our framework for the more challenging SSL setting. Code and models are available at: https://github.com/xmed-lab/DHC.
</details>
<details>
<summary>摘要</summary>
《三维医疗图像分割预测中的卷积批处理是专业需求占用时间，因此半supervised learning（SSL）是非常有优点的。然而，实际应用中存在很严重的分类不均衡问题，这限制了这些方法的应用。为解决这个问题，我们提出了一个新的双向偏置多样化合成（DHC）框架，用于 semi-supervised 三维医疗图像分割。我们提出了两种损失补偿策略，namely Distribution-aware Debiased Weighting（DistDW）和 Difficulty-aware Debiased Weighting（DiffDW），这些策略在运动pseudo标签来导引模型解决数据和学习偏见。该框架在多个子模型之间协同训练，从而提高了性能。我们还引入了更加代表的 semi-supervised 医疗图像分割benchmark，可以完全展示class-imbalance设计的效果。实验表明，我们提出的方法可以通过使用pseudo标签进行偏置和平衡分类不均衡问题，并且超过了状态当前的SSL方法，示出了我们的框架在更加挑战的SSL设置中的潜在能力。代码和模型可以在https://github.com/xmed-lab/DHC上获取。》
</details></li>
</ul>
<hr>
<h2 id="Multi-representations-Space-Separation-based-Graph-level-Anomaly-aware-Detection"><a href="#Multi-representations-Space-Separation-based-Graph-level-Anomaly-aware-Detection" class="headerlink" title="Multi-representations Space Separation based Graph-level Anomaly-aware Detection"></a>Multi-representations Space Separation based Graph-level Anomaly-aware Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12994">http://arxiv.org/abs/2307.12994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fu Lin, Haonan Gong, Mingkang Li, Zitong Wang, Yue Zhang, Xuexiong Luo</li>
<li>for: 本研究的目的是检测图像中的异常图。</li>
<li>methods: 我们提出了一种基于多 Representation Space 的异常检测方法，包括一个异常感知模块，用于评估异常图的重要性。我们还使用四种不同的加权图表示来学习正常和异常图的表示空间。</li>
<li>results: 我们对基线方法进行了广泛的比较，并在十个公共图像dataset上进行了广泛的测试。结果表明，我们的方法具有高效性。<details>
<summary>Abstract</summary>
Graph structure patterns are widely used to model different area data recently. How to detect anomalous graph information on these graph data has become a popular research problem. The objective of this research is centered on the particular issue that how to detect abnormal graphs within a graph set. The previous works have observed that abnormal graphs mainly show node-level and graph-level anomalies, but these methods equally treat two anomaly forms above in the evaluation of abnormal graphs, which is contrary to the fact that different types of abnormal graph data have different degrees in terms of node-level and graph-level anomalies. Furthermore, abnormal graphs that have subtle differences from normal graphs are easily escaped detection by the existing methods. Thus, we propose a multi-representations space separation based graph-level anomaly-aware detection framework in this paper. To consider the different importance of node-level and graph-level anomalies, we design an anomaly-aware module to learn the specific weight between them in the abnormal graph evaluation process. In addition, we learn strictly separate normal and abnormal graph representation spaces by four types of weighted graph representations against each other including anchor normal graphs, anchor abnormal graphs, training normal graphs, and training abnormal graphs. Based on the distance error between the graph representations of the test graph and both normal and abnormal graph representation spaces, we can accurately determine whether the test graph is anomalous. Our approach has been extensively evaluated against baseline methods using ten public graph datasets, and the results demonstrate its effectiveness.
</details>
<details>
<summary>摘要</summary>
近些年来，图structure模式广泛应用于不同领域数据模型中。但是如何检测图数据中的异常信息已成为一个热门研究问题。我们的研究目标是在图集中检测异常图。前期研究发现，异常图主要表现为节点级别和图级别异常，但是现有方法均视这两种异常形态为一样，这与实际情况不符。此外，异常图具有微妙异常特征，容易被现有方法排除。因此，我们提出了一个基于多个表示空间分离的图级别异常检测框架。为了考虑节点级别和图级别异常的不同重要性，我们设计了一个异常感知模块，用于在异常图评估过程中学习特定权重。此外，我们学习严格分别的正常和异常图表示空间，通过四种权重图表示对彼此进行比较。基于测试图与正常和异常图表示空间之间的距离错误，我们可以准确地判断测试图是否异常。我们的方法在基于基线方法的比较下进行了广泛的评估，结果表明其效果高效。
</details></li>
</ul>
<hr>
<h2 id="High-performance-real-world-optical-computing-trained-by-in-situ-model-free-optimization"><a href="#High-performance-real-world-optical-computing-trained-by-in-situ-model-free-optimization" class="headerlink" title="High-performance real-world optical computing trained by in situ model-free optimization"></a>High-performance real-world optical computing trained by in situ model-free optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11957">http://arxiv.org/abs/2307.11957</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangyuan Zhao, Xin Shu, Renjie Zhou</li>
<li>for: 提高光学计算系统的高速和低能耗数据处理能力，但面临 computationally demanding 训练和实际 gap。</li>
<li>methods: 提出了一种无模型解决方案，基于分布式扩散算法进行光学计算系统的增量优化。这种方法不需要 computation-heavy 和偏见的系统模拟，直接借鉴系统的黑盒特性，将损失反射到光学权重的概率分布中。</li>
<li>results: 通过在单层折射光学计算系统上进行实验，达到了在 MNIST 和 FMNIST 数据集上的高精度分类result。此外，还展示了其在无图像和高速细胞分析中的潜在应用 potential。<details>
<summary>Abstract</summary>
Optical computing systems can provide high-speed and low-energy data processing but face deficiencies in computationally demanding training and simulation-to-reality gap. We propose a model-free solution for lightweight in situ optimization of optical computing systems based on the score gradient estimation algorithm. This approach treats the system as a black box and back-propagates loss directly to the optical weights' probabilistic distributions, hence circumventing the need for computation-heavy and biased system simulation. We demonstrate a superior classification accuracy on the MNIST and FMNIST datasets through experiments on a single-layer diffractive optical computing system. Furthermore, we show its potential for image-free and high-speed cell analysis. The inherent simplicity of our proposed method, combined with its low demand for computational resources, expedites the transition of optical computing from laboratory demonstrations to real-world applications.
</details>
<details>
<summary>摘要</summary>
OPTICAL计算系统可提供高速和低能耗数据处理，但面临计算挑战和实际和模拟之间的差距。我们提出了一种无模型解决方案，基于分数跑分布预测算法，用于轻量级在处理器上进行位置优化。这种方法将系统看作黑盒，直接倒退损失到光学权重的概率分布上，因此不需要 computation-intensive和偏见的系统模拟。我们通过实验示出，在单层折射光计算系统上实现了MNIST和FMNIST数据集上的高精度分类。此外，我们还展示了它的潜在应用于无图像和高速细胞分析。我们的提议的简单性和计算资源的低需求，使得光学计算从实验室示范到实际应用的过渡变得更加容易。
</details></li>
</ul>
<hr>
<h2 id="Puioio-On-device-Real-Time-Smartphone-Based-Automated-Exercise-Repetition-Counting-System"><a href="#Puioio-On-device-Real-Time-Smartphone-Based-Automated-Exercise-Repetition-Counting-System" class="headerlink" title="Pūioio: On-device Real-Time Smartphone-Based Automated Exercise Repetition Counting System"></a>Pūioio: On-device Real-Time Smartphone-Based Automated Exercise Repetition Counting System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02420">http://arxiv.org/abs/2308.02420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Sinclair, Kayla Kautai, Seyed Reza Shahamiri<br>for: 这个研究的目的是为了开发一种能够在智能手机上实时计数运动 répétitions的系统，以便提高人们的 физи健身和rehabilitation 效果。methods: 这个研究使用了深度学习技术，包括pose estimation、thresholding、optical flow和状态机制，以实现智能手机上的运动 répétitions计数。results: 研究表明，这个系统在真实的测试中达到了98.89%的准确率，而在预录的视频数据集上测试时的准确率为98.85%。这表明这个系统是一种有效、低成本、便捷的解决方案，不需要特殊的硬件或敏感器，也不需要网络连接。<details>
<summary>Abstract</summary>
Automated exercise repetition counting has applications across the physical fitness realm, from personal health to rehabilitation. Motivated by the ubiquity of mobile phones and the benefits of tracking physical activity, this study explored the feasibility of counting exercise repetitions in real-time, using only on-device inference, on smartphones. In this work, after providing an extensive overview of the state-of-the-art automatic exercise repetition counting methods, we introduce a deep learning based exercise repetition counting system for smartphones consisting of five components: (1) Pose estimation, (2) Thresholding, (3) Optical flow, (4) State machine, and (5) Counter. The system is then implemented via a cross-platform mobile application named P\=uioio that uses only the smartphone camera to track repetitions in real time for three standard exercises: Squats, Push-ups, and Pull-ups. The proposed system was evaluated via a dataset of pre-recorded videos of individuals exercising as well as testing by subjects exercising in real time. Evaluation results indicated the system was 98.89% accurate in real-world tests and up to 98.85% when evaluated via the pre-recorded dataset. This makes it an effective, low-cost, and convenient alternative to existing solutions since the proposed system has minimal hardware requirements without requiring any wearable or specific sensors or network connectivity.
</details>
<details>
<summary>摘要</summary>
自动化运动重复计数有应用于身体健康和重建领域，由于移动电话的普遍和跟踪物理活动的好处，这项研究探索了使用只有移动设备逻辑进行实时计数的可能性。本文首先提供了 automatic exercise repetition counting方法的现状报告，然后引入了基于深度学习的运动重复计数系统，包括五个组件：（1）姿势估计，（2）阈值处理，（3）光流计算，（4）状态机制，（5）计数器。该系统然后通过一款跨平台移动应用程序名为P\=uioio实现，该应用程序使用移动设备摄像头实时跟踪重复进行三种标准运动：蹲squats、推手push-ups和拔手pull-ups。进行测试时，提出的系统在实际测试中达到98.89%的准确率，并且在预录视频中测试时达到98.85%的准确率。这使得该系统成为一种有效、低成本、便捷的替代方案，因为它具有最低硬件要求，没有需要佩戴式或特殊的感应器或网络连接。
</details></li>
</ul>
<hr>
<h2 id="Implicit-Interpretation-of-Importance-Weight-Aware-Updates"><a href="#Implicit-Interpretation-of-Importance-Weight-Aware-Updates" class="headerlink" title="Implicit Interpretation of Importance Weight Aware Updates"></a>Implicit Interpretation of Importance Weight Aware Updates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11955">http://arxiv.org/abs/2307.11955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keyi Chen, Francesco Orabona</li>
<li>for: 这篇论文主要应用于 convex machine learning 中的优化问题，尤其是对于 subgradient descent 的学习率调整。</li>
<li>methods: 这篇论文使用了 Importance Weight Aware (IWA) 更新方法，这是一种基于 infinitely many infinitesimal updates 的方法，可以对于每个损失函数进行无限多次微更新。</li>
<li>results: 这篇论文首次显示了 IWA 更新方法在 online learning  Setting 中的 strictly better regret upper bound，与 plain gradient updates 相比，这是由新的框架 generalized implicit Follow-the-Regularized-Leader (FTRL) 所支持。<details>
<summary>Abstract</summary>
Due to its speed and simplicity, subgradient descent is one of the most used optimization algorithms in convex machine learning algorithms. However, tuning its learning rate is probably its most severe bottleneck to achieve consistent good performance. A common way to reduce the dependency on the learning rate is to use implicit/proximal updates. One such variant is the Importance Weight Aware (IWA) updates, which consist of infinitely many infinitesimal updates on each loss function. However, IWA updates' empirical success is not completely explained by their theory. In this paper, we show for the first time that IWA updates have a strictly better regret upper bound than plain gradient updates in the online learning setting. Our analysis is based on the new framework, generalized implicit Follow-the-Regularized-Leader (FTRL) (Chen and Orabona, 2023), to analyze generalized implicit updates using a dual formulation. In particular, our results imply that IWA updates can be considered as approximate implicit/proximal updates.
</details>
<details>
<summary>摘要</summary>
由于它的速度和简洁性，梯度下降是 convex 机器学习算法中最常用的优化算法之一。然而，调整它的学习率是它最大的瓶颈，以实现一致良好的性能。一种常见的减少学习率依赖性的方法是使用隐式/ proximal 更新。一种such variant是强迫权重 aware（IWA）更新，它包括无限多个infinitesimal更新。然而， IWA 更新的实际成功并不完全由其理论 explain。在这篇论文中，我们表明，IWA 更新在在线学习设定下具有 strictly better regret upper bound than plain gradient updates。我们的分析基于新的框架，Generalized Implicit Follow-the-Regularized-Leader（FTRL）（Chen and Orabona, 2023），用于分析通用隐式更新。具体来说，我们的结果表明，IWA 更新可以被视为approximate隐式/ proximal 更新。
</details></li>
</ul>
<hr>
<h2 id="On-Robot-Bayesian-Reinforcement-Learning-for-POMDPs"><a href="#On-Robot-Bayesian-Reinforcement-Learning-for-POMDPs" class="headerlink" title="On-Robot Bayesian Reinforcement Learning for POMDPs"></a>On-Robot Bayesian Reinforcement Learning for POMDPs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11954">http://arxiv.org/abs/2307.11954</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hai Nguyen, Sammie Katt, Yuchen Xiao, Christopher Amato</li>
<li>for: 这个论文的目的是提出一种专门针对物理系统的 bayesian 强化学习方法，以解决人工智能学习 robotics 中的数据成本问题。</li>
<li>methods: 该方法使用了一种专门的 factored 表示方法，将专家知识captured，然后证明 posterior 会factorize 成相似的形式，并最终将其формализова为 bayesian 框架。它还提出了一种基于 Monte-Carlo tree search 和 particle filtering 的在线解决方法，可以利用typical low-level robot simulators 和处理未知环境的不确定性。</li>
<li>results: 该方法在两个人机交互任务中实现了near-optimal 性能，只需要一些实际世界的 episodenumbers。一个视频 displaying learned policies 可以在 <a target="_blank" rel="noopener" href="https://youtu.be/H9xp60ngOes">https://youtu.be/H9xp60ngOes</a> 找到。<details>
<summary>Abstract</summary>
Robot learning is often difficult due to the expense of gathering data. The need for large amounts of data can, and should, be tackled with effective algorithms and leveraging expert information on robot dynamics. Bayesian reinforcement learning (BRL), thanks to its sample efficiency and ability to exploit prior knowledge, is uniquely positioned as such a solution method. Unfortunately, the application of BRL has been limited due to the difficulties of representing expert knowledge as well as solving the subsequent inference problem. This paper advances BRL for robotics by proposing a specialized framework for physical systems. In particular, we capture this knowledge in a factored representation, then demonstrate the posterior factorizes in a similar shape, and ultimately formalize the model in a Bayesian framework. We then introduce a sample-based online solution method, based on Monte-Carlo tree search and particle filtering, specialized to solve the resulting model. This approach can, for example, utilize typical low-level robot simulators and handle uncertainty over unknown dynamics of the environment. We empirically demonstrate its efficiency by performing on-robot learning in two human-robot interaction tasks with uncertainty about human behavior, achieving near-optimal performance after only a handful of real-world episodes. A video of learned policies is at https://youtu.be/H9xp60ngOes.
</details>
<details>
<summary>摘要</summary>
瑞博特学习（BRL）通常受到数据成本的限制，因为需要大量数据来训练。然而，BRL因其效率和利用专家知识的能力而成为一个有效的解决方案。然而，BRL的应用受到专家知识表达和后续推理问题的限制。这篇论文推广BRL在 робо特领域，通过特殊的框架来捕捉专家知识。具体来说，我们将知识表示为一个分解表示，并证明 posterior 会分解成类似的形式。最后，我们将模型归纳到 Bayesian 框架中。然后，我们引入一种基于 Monte-Carlo 搜索和粒子筛选的在线解决方法，特化用于解决 resulting 模型。这种方法可以利用常见的低级 robot 模拟器，并处理不确定的环境动力学的不确定性。我们在两个人机交互任务中进行了实验，并达到了几乎最佳性能，只需要几个真实世界的回合。视频展示学习政策的详细信息请参考 <https://youtu.be/H9xp60ngOes>。
</details></li>
</ul>
<hr>
<h2 id="HIQL-Offline-Goal-Conditioned-RL-with-Latent-States-as-Actions"><a href="#HIQL-Offline-Goal-Conditioned-RL-with-Latent-States-as-Actions" class="headerlink" title="HIQL: Offline Goal-Conditioned RL with Latent States as Actions"></a>HIQL: Offline Goal-Conditioned RL with Latent States as Actions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11949">http://arxiv.org/abs/2307.11949</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/seohongpark/hiql">https://github.com/seohongpark/hiql</a></li>
<li>paper_authors: Seohong Park, Dibya Ghosh, Benjamin Eysenbach, Sergey Levine</li>
<li>for: 该论文旨在提出一种基于未经过supervision的自适应RL算法，可以直接从大量无标签数据中学习。</li>
<li>methods: 该算法使用一个不含操作的价值函数，通过分层的 decomposición 学习两个策略：一个高级策略使用状态作为动作，预测子目标，以及一个低级策略预测达到子目标所需的操作。</li>
<li>results: 通过分析和示例，该方法可以减少估计价值函数中的噪声，并应用于Offline goal-reaching benchmark，解决远程目标的问题，可以扩展到高维图像观察数据，并可以使用无操作数据进行学习。<details>
<summary>Abstract</summary>
Unsupervised pre-training has recently become the bedrock for computer vision and natural language processing. In reinforcement learning (RL), goal-conditioned RL can potentially provide an analogous self-supervised approach for making use of large quantities of unlabeled (reward-free) data. However, building effective algorithms for goal-conditioned RL that can learn directly from diverse offline data is challenging, because it is hard to accurately estimate the exact value function for faraway goals. Nonetheless, goal-reaching problems exhibit structure, such that reaching distant goals entails first passing through closer subgoals. This structure can be very useful, as assessing the quality of actions for nearby goals is typically easier than for more distant goals. Based on this idea, we propose a hierarchical algorithm for goal-conditioned RL from offline data. Using one action-free value function, we learn two policies that allow us to exploit this structure: a high-level policy that treats states as actions and predicts (a latent representation of) a subgoal and a low-level policy that predicts the action for reaching this subgoal. Through analysis and didactic examples, we show how this hierarchical decomposition makes our method robust to noise in the estimated value function. We then apply our method to offline goal-reaching benchmarks, showing that our method can solve long-horizon tasks that stymie prior methods, can scale to high-dimensional image observations, and can readily make use of action-free data. Our code is available at https://seohong.me/projects/hiql/
</details>
<details>
<summary>摘要</summary>
Recently, unsupervised pre-training has become the foundation for computer vision and natural language processing. In reinforcement learning (RL), goal-conditioned RL has the potential to provide a self-supervised approach that can utilize large amounts of unlabeled (reward-free) data. However, building effective algorithms for goal-conditioned RL that can learn directly from diverse offline data is challenging, as it is difficult to accurately estimate the exact value function for faraway goals. Nonetheless, goal-reaching problems exhibit structure, such that reaching distant goals involves first passing through closer subgoals. This structure can be very useful, as assessing the quality of actions for nearby goals is typically easier than for more distant goals. Based on this idea, we propose a hierarchical algorithm for goal-conditioned RL from offline data. Using one action-free value function, we learn two policies: a high-level policy that treats states as actions and predicts (a latent representation of) a subgoal, and a low-level policy that predicts the action for reaching this subgoal. Through analysis and didactic examples, we show how this hierarchical decomposition makes our method robust to noise in the estimated value function. We then apply our method to offline goal-reaching benchmarks, showing that our method can solve long-horizon tasks that stymie prior methods, can scale to high-dimensional image observations, and can readily make use of action-free data. Our code is available at <https://seohong.me/projects/hiql/>.Here's the translation in Traditional Chinese:最近，无监督预训学（pre-training）已经成为计算机视觉和自然语言处理的基础。在征募学习（RL）中，目标受控RL有可能提供一个自我指导的方法，可以对大量的无条件数据进行学习。然而，建立有效的目标受控RL算法，可以从多元的过去数据中直接学习，是一个挑战。这是因为，评估远方目标的价值函数是很难精确地估计的。然而，目标实现问题会展示结构，例如，要到达更远的目标时，通常需要先通过更近的子目标。这种结构可以非常有用，因为评估近距离的目标是比较容易的。基于这个想法，我们提出了一个层次化的算法，从过去数据中学习目标受控RL。使用一个无动作的值函数，我们学习了两个政策：一个高层政策，将状态视为动作，预测（一个隐藏表示）子目标，以及一个低层政策，预测用于到达子目标的动作。通过分析和示例，我们显示了这个层次化分解的优点，使我们的方法更准确地处理错误估计的值函数。然后，我们将我们的方法应用到过去目标实现的参考数据上，展示了我们的方法可以解决长时间任务，超越先前的方法，可以扩展到高维影像观察，并且可以快速地使用无动作数据。我们的代码可以在 <https://seohong.me/projects/hiql/> 获取。
</details></li>
</ul>
<hr>
<h2 id="The-instabilities-of-large-learning-rate-training-a-loss-landscape-view"><a href="#The-instabilities-of-large-learning-rate-training-a-loss-landscape-view" class="headerlink" title="The instabilities of large learning rate training: a loss landscape view"></a>The instabilities of large learning rate training: a loss landscape view</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11948">http://arxiv.org/abs/2307.11948</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lawrence Wang, Stephen Roberts</li>
<li>for: 研究深度学习网络训练中大学习率较大的稳定性问题。</li>
<li>methods: 使用Hessian矩阵来描述损失函数的梯度场，描述梯度搅拌的不稳定性。</li>
<li>results: 发现训练过程中存在“梯度场平整”和“梯度场转移”两种现象，这两种现象与训练不稳定性有直接关系。<details>
<summary>Abstract</summary>
Modern neural networks are undeniably successful. Numerous works study how the curvature of loss landscapes can affect the quality of solutions. In this work we study the loss landscape by considering the Hessian matrix during network training with large learning rates - an attractive regime that is (in)famously unstable. We characterise the instabilities of gradient descent, and we observe the striking phenomena of \textit{landscape flattening} and \textit{landscape shift}, both of which are intimately connected to the instabilities of training.
</details>
<details>
<summary>摘要</summary>
现代神经网络确实非常成功。许多研究证明损失函数的曲率对解决方案质量有很大影响。在这个工作中，我们研究损失函数的 landscape，通过考虑大学习率时的希尔бер特矩阵。我们描述了梯度下降的不稳定性，并观察了“ landscape flattening”和“ landscape shift”这两种phenomena，它们与训练不稳定性有着密切的关系。
</details></li>
</ul>
<hr>
<h2 id="Collaboratively-Learning-Linear-Models-with-Structured-Missing-Data"><a href="#Collaboratively-Learning-Linear-Models-with-Structured-Missing-Data" class="headerlink" title="Collaboratively Learning Linear Models with Structured Missing Data"></a>Collaboratively Learning Linear Models with Structured Missing Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11947">http://arxiv.org/abs/2307.11947</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Cheng, Gary Cheng, John Duchi</li>
<li>for: 本文研究了多个Agent共同学习最小二乘估计问题，每个Agent观察不同的特征子数据（例如感知器的分解）。目标是为每个Agent生成最佳估计器。</li>
<li>methods: 我们提出了一种分布式、半监督的算法Collab，包括三个步骤：本地训练、聚合和分布。我们的方法不需要传输标注数据，因此是通信效率高的。</li>
<li>results: 我们的方法可以在实际数据和synthetic数据上测试，并且nearly asymptotically local minimax优秀，即在标注数据不可用的情况下，我们的方法可以与可以通信标注数据的估计器相比。<details>
<summary>Abstract</summary>
We study the problem of collaboratively learning least squares estimates for $m$ agents. Each agent observes a different subset of the features$\unicode{x2013}$e.g., containing data collected from sensors of varying resolution. Our goal is to determine how to coordinate the agents in order to produce the best estimator for each agent. We propose a distributed, semi-supervised algorithm Collab, consisting of three steps: local training, aggregation, and distribution. Our procedure does not require communicating the labeled data, making it communication efficient and useful in settings where the labeled data is inaccessible. Despite this handicap, our procedure is nearly asymptotically local minimax optimal$\unicode{x2013}$even among estimators allowed to communicate the labeled data such as imputation methods. We test our method on real and synthetic data.
</details>
<details>
<summary>摘要</summary>
我们研究多个代理人共同学习最小二乘估计问题。每个代理人都观察不同的特征子集合，例如感应器的分解度不同。我们的目标是让代理人们如何协调，以生成每个代理人最好的估计器。我们提出了分布式、半监督的算法Collab，包括三个步骤：本地训练、聚合和分布。我们的程序不需要通过实际资料进行通信，因此它具有通信效率的优点，特别在资料权限严格的情况下。尽管如此，我们的程序仍然几乎在极限情况下对局部最小最佳，甚至在允许交流实际资料的情况下，如填充方法。我们在实际数据和 sintetic 数据上进行测试。
</details></li>
</ul>
<hr>
<h2 id="Batch-Clipping-and-Adaptive-Layerwise-Clipping-for-Differential-Private-Stochastic-Gradient-Descent"><a href="#Batch-Clipping-and-Adaptive-Layerwise-Clipping-for-Differential-Private-Stochastic-Gradient-Descent" class="headerlink" title="Batch Clipping and Adaptive Layerwise Clipping for Differential Private Stochastic Gradient Descent"></a>Batch Clipping and Adaptive Layerwise Clipping for Differential Private Stochastic Gradient Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11939">http://arxiv.org/abs/2307.11939</a></li>
<li>repo_url: None</li>
<li>paper_authors: Toan N. Nguyen, Phuong Ha Nguyen, Lam M. Nguyen, Marten Van Dijk</li>
<li>for: 这 paper 是为了研究 differential private stochastic gradient descent (DPSGD) 的改进方法，以及其在深度神经网络中的应用。</li>
<li>methods: 这 paper 使用了一种称为批量剪裁 (Batch Clipping, BC) 的方法，以及一种称为层次剪裁 (Adaptive Layerwise Clipping, ALC) 的方法。这两种方法都是为了使得 DPSGD 可以在深度神经网络中使用批处理 (batch normalization layers, BNL)。</li>
<li>results: 实验表明，使用这 two 种方法可以使得 DPSGD 在 CIFAR-$10 $ 上的 resnet-$18 $ 模型进行训练，而原始 DPSGD 则不能进行训练。<details>
<summary>Abstract</summary>
Each round in Differential Private Stochastic Gradient Descent (DPSGD) transmits a sum of clipped gradients obfuscated with Gaussian noise to a central server which uses this to update a global model which often represents a deep neural network. Since the clipped gradients are computed separately, which we call Individual Clipping (IC), deep neural networks like resnet-18 cannot use Batch Normalization Layers (BNL) which is a crucial component in deep neural networks for achieving a high accuracy. To utilize BNL, we introduce Batch Clipping (BC) where, instead of clipping single gradients as in the orginal DPSGD, we average and clip batches of gradients. Moreover, the model entries of different layers have different sensitivities to the added Gaussian noise. Therefore, Adaptive Layerwise Clipping methods (ALC), where each layer has its own adaptively finetuned clipping constant, have been introduced and studied, but so far without rigorous DP proofs. In this paper, we propose {\em a new ALC and provide rigorous DP proofs for both BC and ALC}. Experiments show that our modified DPSGD with BC and ALC for CIFAR-$10$ with resnet-$18$ converges while DPSGD with IC and ALC does not.
</details>
<details>
<summary>摘要</summary>
每回轮在差分私人随机梯度下降（DPSGD）中都会将折叠后的梯度进行干扰加速，并将其发送到中央服务器，以更新一个全球模型，通常是深度神经网络。由于折叠后的梯度在不同层中 computing  separately，因此深度神经网络如 ResNet-18 不能使用批 normalization layer（BNL），这是深度神经网络实现高精度的一个关键组件。为了使用 BNL，我们引入批 clipping（BC），其中，而不是单个梯度的 clipping，我们将批量梯度进行平均和折叠。此外，不同层的模型元素对添加的 Gaussian 噪声有不同的感itivity。因此，我们引入 adaptive layerwise clipping 方法（ALC），其中每个层有自适应地调整的 clipping 常量。在这篇论文中，我们提出了一种新的 ALC，并提供了准确的 DP 证明。实验表明，我们修改后的 DPSGD  WITH BC 和 ALC 可以在 CIFAR-10 上 WITH ResNet-18  converges，而 DPSGD  WITH IC 和 ALC 则不可以。
</details></li>
</ul>
<hr>
<h2 id="Mercer-Large-Scale-Kernel-Machines-from-Ridge-Function-Perspective"><a href="#Mercer-Large-Scale-Kernel-Machines-from-Ridge-Function-Perspective" class="headerlink" title="Mercer Large-Scale Kernel Machines from Ridge Function Perspective"></a>Mercer Large-Scale Kernel Machines from Ridge Function Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11925">http://arxiv.org/abs/2307.11925</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karol Dziedziul, Sergey Kryzhevich</li>
<li>for: 本文提出了一种基于ridge函数的大规模kernel机器学习方法，并通过林-毕肖斯 theorem的推广来解释其效果。</li>
<li>methods: 本文使用了Random features方法，并通过近似理论来研究可以通过权重的积分来approxmiate各种kernel函数。</li>
<li>results: 本文的结果表明，可以通过权重积分来approxmiate各种kernel函数，但是存在一些障碍，如果要在大规模数据集上使用这种方法。这些结果可能在深度学习中有各种应用，特别是在图像处理领域。<details>
<summary>Abstract</summary>
To present Mercer large-scale kernel machines from a ridge function perspective, we recall the results by Lin and Pinkus from Fundamentality of ridge functions. We consider the main theorem of the recent paper by Rachimi and Recht, 2008, Random features for large-scale kernel machines in terms of the Approximation Theory. We study which kernels can be approximated by a sum of cosine function products with arguments depending on $x$ and $y$ and present the obstacles of such an approach. The results of this article may have various applications in Deep Learning, especially in problems related to Image Processing.
</details>
<details>
<summary>摘要</summary>
将梅瑞较大批量机器学习从ridge函数角度提出，我们回忆了林和毕普斯的诸果。我们考虑了2008年rachimi和recht的论文《随机特征 для大规模kernel机器学习》中的主定理，我们研究了可以通过cosine函数乘积来近似哪些kernels，并描述了这种方法的障碍。这些结果可能在深度学习中有各种应用，特别是在图像处理问题中。
</details></li>
</ul>
<hr>
<h2 id="Selective-Perception-Optimizing-State-Descriptions-with-Reinforcement-Learning-for-Language-Model-Actors"><a href="#Selective-Perception-Optimizing-State-Descriptions-with-Reinforcement-Learning-for-Language-Model-Actors" class="headerlink" title="Selective Perception: Optimizing State Descriptions with Reinforcement Learning for Language Model Actors"></a>Selective Perception: Optimizing State Descriptions with Reinforcement Learning for Language Model Actors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11922">http://arxiv.org/abs/2307.11922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kolby Nottingham, Yasaman Razeghi, Kyungmin Kim, JB Lanier, Pierre Baldi, Roy Fox, Sameer Singh</li>
<li>for: 这个论文主要针对的是使用大语言模型（LLM）进行序列决策任务，特别是在 робо控和游戏等领域。</li>
<li>methods: 这篇论文提出了一种自动选择简洁状态描述的方法，名叫“简洁语言输入 для决策响应”（BLINDER）。该方法通过学习任务条件下的状态描述价值函数来自动选择简洁的状态描述。</li>
<li>results: 在 NetHack 游戏和 robotic manipulation 任务中，BLINDER 方法能够提高任务成功率，降低输入大小和计算成本，并在不同的 LLM actor 之间进行泛化。<details>
<summary>Abstract</summary>
Large language models (LLMs) are being applied as actors for sequential decision making tasks in domains such as robotics and games, utilizing their general world knowledge and planning abilities. However, previous work does little to explore what environment state information is provided to LLM actors via language. Exhaustively describing high-dimensional states can impair performance and raise inference costs for LLM actors. Previous LLM actors avoid the issue by relying on hand-engineered, task-specific protocols to determine which features to communicate about a state and which to leave out. In this work, we propose Brief Language INputs for DEcision-making Responses (BLINDER), a method for automatically selecting concise state descriptions by learning a value function for task-conditioned state descriptions. We evaluate BLINDER on the challenging video game NetHack and a robotic manipulation task. Our method improves task success rate, reduces input size and compute costs, and generalizes between LLM actors.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Poverty-rate-prediction-using-multi-modal-survey-and-earth-observation-data"><a href="#Poverty-rate-prediction-using-multi-modal-survey-and-earth-observation-data" class="headerlink" title="Poverty rate prediction using multi-modal survey and earth observation data"></a>Poverty rate prediction using multi-modal survey and earth observation data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11921">http://arxiv.org/abs/2307.11921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simone Fobi, Manuel Cardona, Elliott Collins, Caleb Robinson, Anthony Ortiz, Tina Sederholm, Rahul Dodhia, Juan Lavista Ferres</li>
<li>For: 这种方法用于将家庭民调数据和卫星影像特征结合以预测地区贫困率。* Methods: 该方法使用单步特征提取方法生成10m&#x2F;px Sentinal-2表面反射卫星影像的视觉特征，然后与民调问题相结合以实现贫困率的估计。* Results: 包含卫星影像特征的代表测试集上的贫困率估计 error 从4.09%降低至3.88%，而包含小问题的选择方法可以further reduce error to 3.71%。这些方法还显示出卫星影像特征具有地区和城市化差异的特征。<details>
<summary>Abstract</summary>
This work presents an approach for combining household demographic and living standards survey questions with features derived from satellite imagery to predict the poverty rate of a region. Our approach utilizes visual features obtained from a single-step featurization method applied to freely available 10m/px Sentinel-2 surface reflectance satellite imagery. These visual features are combined with ten survey questions in a proxy means test (PMT) to estimate whether a household is below the poverty line. We show that the inclusion of visual features reduces the mean error in poverty rate estimates from 4.09% to 3.88% over a nationally representative out-of-sample test set. In addition to including satellite imagery features in proxy means tests, we propose an approach for selecting a subset of survey questions that are complementary to the visual features extracted from satellite imagery. Specifically, we design a survey variable selection approach guided by the full survey and image features and use the approach to determine the most relevant set of small survey questions to include in a PMT. We validate the choice of small survey questions in a downstream task of predicting the poverty rate using the small set of questions. This approach results in the best performance -- errors in poverty rate decrease from 4.09% to 3.71%. We show that extracted visual features encode geographic and urbanization differences between regions.
</details>
<details>
<summary>摘要</summary>
In addition to including satellite imagery features in the PMT, the approach proposes a method for selecting a subset of survey questions that are complementary to the visual features extracted from satellite imagery. The approach uses a survey variable selection method guided by the full survey and image features to determine the most relevant set of small survey questions to include in the PMT. The choice of small survey questions is validated in a downstream task of predicting the poverty rate, resulting in the best performance with errors in poverty rate decreasing from 4.09% to 3.71%.The approach shows that the extracted visual features encode geographic and urbanization differences between regions, providing a more accurate estimate of poverty rates. By combining survey questions with satellite imagery features, the approach provides a more comprehensive and accurate assessment of poverty rates, particularly in areas with limited survey data.
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Vulnerabilities-in-Interpretable-Deep-Learning-Systems-with-Query-Efficient-Black-box-Attacks"><a href="#Unveiling-Vulnerabilities-in-Interpretable-Deep-Learning-Systems-with-Query-Efficient-Black-box-Attacks" class="headerlink" title="Unveiling Vulnerabilities in Interpretable Deep Learning Systems with Query-Efficient Black-box Attacks"></a>Unveiling Vulnerabilities in Interpretable Deep Learning Systems with Query-Efficient Black-box Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11906">http://arxiv.org/abs/2307.11906</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eldor Abdukhamidov, Mohammed Abuhamad, Simon S. Woo, Eric Chan-Tin, Tamer Abuhmed</li>
<li>For: This paper aims to demonstrate a novel black-box attack against Interpretable Deep Learning Systems (IDLSes), which can compromise the integrity and reliability of these systems.* Methods: The proposed attack uses a microbial genetic algorithm-based approach that combines transfer-based and score-based methods, and requires no prior knowledge of the target model or its interpretation model.* Results: The proposed attack achieves high attack success rates using adversarial examples with attribution maps that are highly similar to those of benign samples, making it difficult to detect even by human analysts. The results highlight the need for improved IDLS security to ensure their practical reliability.Here’s the summary in Traditional Chinese:* For: 这篇论文旨在展示一种黑盒攻击 against Interpretable Deep Learning Systems (IDLSes)，以确保这些系统的可靠性和可信度。* Methods: 提案的攻击使用了微生物遗传学算法基本的方法，融合了转移基本和分数基本方法，并不需要对目标模型和其解释模型的内容有任何专业知识。* Results: 提案的攻击得到了高的攻击成功率，使用了对抗例显示对应图的高相似性，让人类分析师难以探测。结果显示了IDLS安全性的需求，以确保它们在实际应用中的可靠性。<details>
<summary>Abstract</summary>
Deep learning has been rapidly employed in many applications revolutionizing many industries, but it is known to be vulnerable to adversarial attacks. Such attacks pose a serious threat to deep learning-based systems compromising their integrity, reliability, and trust. Interpretable Deep Learning Systems (IDLSes) are designed to make the system more transparent and explainable, but they are also shown to be susceptible to attacks. In this work, we propose a novel microbial genetic algorithm-based black-box attack against IDLSes that requires no prior knowledge of the target model and its interpretation model. The proposed attack is a query-efficient approach that combines transfer-based and score-based methods, making it a powerful tool to unveil IDLS vulnerabilities. Our experiments of the attack show high attack success rates using adversarial examples with attribution maps that are highly similar to those of benign samples which makes it difficult to detect even by human analysts. Our results highlight the need for improved IDLS security to ensure their practical reliability.
</details>
<details>
<summary>摘要</summary>
深度学习已经广泛应用在许多领域，但它们知道易受到敌意攻击。这些攻击会对深度学习基于系统造成严重的威胁，打乱它们的完整性、可靠性和信任worth。可读性深度学习系统（IDLS）是为了让系统更加透明和可解释的，但它们也被示出可能受到攻击。在这项工作中，我们提出了一种基于微生物遗传算法的黑盒攻击方法，不需要攻击目标模型和其解释模型的先前知识。我们的攻击方法是一种高效的查询方法，它结合了传输基于方法和分数基于方法，使其成为对 IDLS 的攻击工具。我们的实验表明，使用挑战性示例和归属地图，可以得到高攻击成功率，而且这些示例与正常样本具有高度相似性，使其具有极具潜在攻击力。我们的结果表明，为了确保 IDLS 的实际可靠性，需要进一步加强 IDLS 的安全性。
</details></li>
</ul>
<hr>
<h2 id="Model-Compression-Methods-for-YOLOv5-A-Review"><a href="#Model-Compression-Methods-for-YOLOv5-A-Review" class="headerlink" title="Model Compression Methods for YOLOv5: A Review"></a>Model Compression Methods for YOLOv5: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11904">http://arxiv.org/abs/2307.11904</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Jani, Jamil Fayyad, Younes Al-Younes, Homayoun Najjaran</li>
<li>for: 本文主要针对Resource-constrained edge devices上部署YOLOv5对象检测器，以提高其精度和效率。</li>
<li>methods: 本文主要采用Network pruning和Quantization两种方法来压缩YOLOv5模型，以降低内存使用量和执行时间。</li>
<li>results: 经过实践和分析，我们发现在YOLOv5上应用Network pruning和Quantization方法可以降低内存使用量和执行时间，但是还存在一些问题需要进一步解决。<details>
<summary>Abstract</summary>
Over the past few years, extensive research has been devoted to enhancing YOLO object detectors. Since its introduction, eight major versions of YOLO have been introduced with the purpose of improving its accuracy and efficiency. While the evident merits of YOLO have yielded to its extensive use in many areas, deploying it on resource-limited devices poses challenges. To address this issue, various neural network compression methods have been developed, which fall under three main categories, namely network pruning, quantization, and knowledge distillation. The fruitful outcomes of utilizing model compression methods, such as lowering memory usage and inference time, make them favorable, if not necessary, for deploying large neural networks on hardware-constrained edge devices. In this review paper, our focus is on pruning and quantization due to their comparative modularity. We categorize them and analyze the practical results of applying those methods to YOLOv5. By doing so, we identify gaps in adapting pruning and quantization for compressing YOLOv5, and provide future directions in this area for further exploration. Among several versions of YOLO, we specifically choose YOLOv5 for its excellent trade-off between recency and popularity in literature. This is the first specific review paper that surveys pruning and quantization methods from an implementation point of view on YOLOv5. Our study is also extendable to newer versions of YOLO as implementing them on resource-limited devices poses the same challenges that persist even today. This paper targets those interested in the practical deployment of model compression methods on YOLOv5, and in exploring different compression techniques that can be used for subsequent versions of YOLO.
</details>
<details>
<summary>摘要</summary>
过去几年，对 YOLO 对象检测器进行了广泛的研究，以提高其精度和效率。自其引入以来，共有八个主要版本的 YOLO 发布，以提高其精度和效率。虽然 YOLO 的优点已经得到了广泛的应用，但在资源有限的设备上部署它存在挑战。为解决这个问题，多种神经网络压缩方法被开发出来，这些方法可以分为三个主要类别：网络剪辑、量化和知识传递。使用这些压缩方法可以降低内存使用量和执行时间，因此在硬件限制的边缘设备上部署大型神经网络变得可能。在本文中，我们将关注剪辑和量化，因为它们在模型压缩方面具有相对的可模块性。我们将这些方法进行分类和分析，并通过应用这些方法于 YOLOv5 来确定它们的实际效果。通过这些研究，我们可以了解剪辑和量化在 YOLOv5 上的应用存在哪些挑战，并提供未来的探索方向。在多个 YOLO 版本中，我们选择了 YOLOv5，因为它在文献中的最新和最受欢迎性兼有。这是对 YOLOv5 模型压缩方法的实现点视图的第一篇评论文。我们的研究也可以扩展到 newer 版本的 YOLO，因为在资源有限的设备上部署它们也存在同样的挑战。本文针对那些关注实际部署模型压缩方法的人，以及愿意探索不同的压缩技术，以便应用于未来的 YOLO 版本。
</details></li>
</ul>
<hr>
<h2 id="Project-Florida-Federated-Learning-Made-Easy"><a href="#Project-Florida-Federated-Learning-Made-Easy" class="headerlink" title="Project Florida: Federated Learning Made Easy"></a>Project Florida: Federated Learning Made Easy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11899">http://arxiv.org/abs/2307.11899</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Madrigal Diaz, Andre Manoel, Jialei Chen, Nalin Singal, Robert Sim</li>
<li>for: 该论文目的是推介一种基于 Federated Learning（FL）的大规模分布式学习解决方案，以便在不同设备和存储系统之间实现数据privacy和安全性。</li>
<li>methods: 该论文使用了分布式学习的技术，包括模型分布式训练、客户端代码更新和中央整理器的集成，以实现在不同设备和存储系统之间进行大规模分布式学习。</li>
<li>results: 该论文通过提供云主机的基础设施和相关任务管理界面，以及支持多种编程语言的多平台SDK，实现了跨设备的分布式学习解决方案，并在多种操作系统和硬件特性下进行了示例实验，以证明系统的能力。<details>
<summary>Abstract</summary>
We present Project Florida, a system architecture and software development kit (SDK) enabling deployment of large-scale Federated Learning (FL) solutions across a heterogeneous device ecosystem. Federated learning is an approach to machine learning based on a strong data sovereignty principle, i.e., that privacy and security of data is best enabled by storing it at its origin, whether on end-user devices or in segregated cloud storage silos. Federated learning enables model training across devices and silos while the training data remains within its security boundary, by distributing a model snapshot to a client running inside the boundary, running client code to update the model, and then aggregating updated snapshots across many clients in a central orchestrator. Deploying a FL solution requires implementation of complex privacy and security mechanisms as well as scalable orchestration infrastructure. Scale and performance is a paramount concern, as the model training process benefits from full participation of many client devices, which may have a wide variety of performance characteristics. Project Florida aims to simplify the task of deploying cross-device FL solutions by providing cloud-hosted infrastructure and accompanying task management interfaces, as well as a multi-platform SDK supporting most major programming languages including C++, Java, and Python, enabling FL training across a wide range of operating system (OS) and hardware specifications. The architecture decouples service management from the FL workflow, enabling a cloud service provider to deliver FL-as-a-service (FLaaS) to ML engineers and application developers. We present an overview of Florida, including a description of the architecture, sample code, and illustrative experiments demonstrating system capabilities.
</details>
<details>
<summary>摘要</summary>
我们现在介绍Project Florida，一个系统架构和软件开发工具包（SDK），允许在多种设备生态系统上部署大规模联合学习（FL）解决方案。联合学习是一种基于强大数据主权原则的机器学习方法，即数据的隐私和安全性最好是在数据的原始位置（在客户端设备或分隔的云存储囊中）保持。联合学习允许在客户端设备和存储囊之间进行模型训练，而不需要将数据传输到外部，只需将模型快照分发到客户端上，让客户端运行代码来更新模型，然后将更新后的快照集中在中央抽象器中。实施联合学习解决方案需要实施复杂的隐私和安全机制，以及可扩展的集成基础设施。因为模型训练过程具有全面参与的客户端设备，这些设备可能具有多种性能特点，因此缩放和性能是一个关键问题。Project Florida goal is to simplify the task of deploying cross-device FL solutions by providing cloud-hosted infrastructure and accompanying task management interfaces, as well as a multi-platform SDK supporting most major programming languages including C++, Java, and Python, enabling FL training across a wide range of operating system (OS) and hardware specifications. The architecture decouples service management from the FL workflow, enabling a cloud service provider to deliver FL-as-a-service (FLaaS) to ML engineers and application developers. We present an overview of Florida, including a description of the architecture, sample code, and illustrative experiments demonstrating system capabilities.
</details></li>
</ul>
<hr>
<h2 id="Hindsight-DICE-Stable-Credit-Assignment-for-Deep-Reinforcement-Learning"><a href="#Hindsight-DICE-Stable-Credit-Assignment-for-Deep-Reinforcement-Learning" class="headerlink" title="Hindsight-DICE: Stable Credit Assignment for Deep Reinforcement Learning"></a>Hindsight-DICE: Stable Credit Assignment for Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11897">http://arxiv.org/abs/2307.11897</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/skandavaidyanath/credit-assignment">https://github.com/skandavaidyanath/credit-assignment</a></li>
<li>paper_authors: Akash Velu, Skanda Vaidyanath, Dilip Arumugam</li>
<li>for: 解决缺乏评价反馈的环境下的决策问题</li>
<li>methods: 使用已有的重要性评估比例估计技术来稳定化和改善基eline方法</li>
<li>results: 提高了稳定性和效率，适用于各种困难的环境中Here’s a breakdown of each point:</li>
<li>for: The paper is written to solve the problem of sequential decision-making in environments with limited evaluative feedback, specifically the challenge of credit assignment.</li>
<li>methods: The paper uses existing importance-sampling ratio estimation techniques to improve the stability and efficiency of hindsight policy methods.</li>
<li>results: The paper shows that the proposed method improves the stability and efficiency of hindsight policy methods, and is applicable to a broad range of environments.<details>
<summary>Abstract</summary>
Oftentimes, environments for sequential decision-making problems can be quite sparse in the provision of evaluative feedback to guide reinforcement-learning agents. In the extreme case, long trajectories of behavior are merely punctuated with a single terminal feedback signal, leading to a significant temporal delay between the observation of a non-trivial reward and the individual steps of behavior culpable for achieving said reward. Coping with such a credit assignment challenge is one of the hallmark characteristics of reinforcement learning. While prior work has introduced the concept of hindsight policies to develop a theoretically moxtivated method for reweighting on-policy data by impact on achieving the observed trajectory return, we show that these methods experience instabilities which lead to inefficient learning in complex environments. In this work, we adapt existing importance-sampling ratio estimation techniques for off-policy evaluation to drastically improve the stability and efficiency of these so-called hindsight policy methods. Our hindsight distribution correction facilitates stable, efficient learning across a broad range of environments where credit assignment plagues baseline methods.
</details>
<details>
<summary>摘要</summary>
frequently, decision-making environments lack evaluative feedback to guide reinforcement-learning agents. In extreme cases, long behavior trajectories are only punctuated by a single terminal feedback signal, resulting in a significant delay between observing a non-trivial reward and identifying the specific actions responsible for earning said reward. This credit assignment challenge is a hallmark of reinforcement learning. Prior work has introduced hindsight policies to develop a theoretically motivated method for reweighting on-policy data based on its impact on achieving the observed trajectory return. However, these methods experience instabilities that lead to inefficient learning in complex environments. In this work, we adapt existing importance-sampling ratio estimation techniques for off-policy evaluation to significantly improve the stability and efficiency of hindsight policy methods. Our hindsight distribution correction enables stable, efficient learning across a broad range of environments where credit assignment plagues baseline methods.
</details></li>
</ul>
<hr>
<h2 id="On-the-Vulnerability-of-Fairness-Constrained-Learning-to-Malicious-Noise"><a href="#On-the-Vulnerability-of-Fairness-Constrained-Learning-to-Malicious-Noise" class="headerlink" title="On the Vulnerability of Fairness Constrained Learning to Malicious Noise"></a>On the Vulnerability of Fairness Constrained Learning to Malicious Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11892">http://arxiv.org/abs/2307.11892</a></li>
<li>repo_url: None</li>
<li>paper_authors: Avrim Blum, Princewill Okoroafor, Aadirupa Saha, Kevin Stangl</li>
<li>for: 这些研究探讨了对小量恶意噪声的敏感性，具体来说是对具有公平约束的学习系统的抗性。</li>
<li>methods: 这篇论文使用了随机分类器，以减少恶意噪声的影响。</li>
<li>results: 研究发现，允许随机分类器时，敏感性可以降低到$\Theta(\alpha)$，$\alpha$是噪声率。此外，研究还发现，对于等机会和平衡约束，敏感性可以降低到$O(\sqrt{\alpha})$和$O(1)$。这些结果提供了对敏感性噪声的 более细致的视图。<details>
<summary>Abstract</summary>
We consider the vulnerability of fairness-constrained learning to small amounts of malicious noise in the training data. Konstantinov and Lampert (2021) initiated the study of this question and presented negative results showing there exist data distributions where for several fairness constraints, any proper learner will exhibit high vulnerability when group sizes are imbalanced. Here, we present a more optimistic view, showing that if we allow randomized classifiers, then the landscape is much more nuanced. For example, for Demographic Parity we show we can incur only a $\Theta(\alpha)$ loss in accuracy, where $\alpha$ is the malicious noise rate, matching the best possible even without fairness constraints. For Equal Opportunity, we show we can incur an $O(\sqrt{\alpha})$ loss, and give a matching $\Omega(\sqrt{\alpha})$lower bound. In contrast, Konstantinov and Lampert (2021) showed for proper learners the loss in accuracy for both notions is $\Omega(1)$. The key technical novelty of our work is how randomization can bypass simple "tricks" an adversary can use to amplify his power. We also consider additional fairness notions including Equalized Odds and Calibration. For these fairness notions, the excess accuracy clusters into three natural regimes $O(\alpha)$,$O(\sqrt{\alpha})$ and $O(1)$. These results provide a more fine-grained view of the sensitivity of fairness-constrained learning to adversarial noise in training data.
</details>
<details>
<summary>摘要</summary>
我们考虑了对公平性限制学习的不可靠性问题，特别是对小量邪恶噪声的影响。 konstantinov和Lampert（2021）开始了这个研究，并提出了负结果，证明了在某些公平性限制下，任何合法的学习者将具有高度的不可靠性，尤其是当群体大小不均匀时。在这里，我们提供了一个更optimistic的见解，表明如果允许随机分类器，则这个景象会变得非常复杂。例如，对于人口均值的公平性，我们显示了仅具有$\Theta(\alpha)$的损失率，与不具有公平性限制的情况相同。对于平等机会的公平性，我们显示了仅具有$O(\sqrt{\alpha})$的损失率，并提供了匹配的$\Omega(\sqrt{\alpha})$下界。与 Konstantinov和Lampert（2021）的结果不同，我们显示在合法学习者中，两者的损失率皆为$\Omega(1)$。我们还考虑了其他的公平性条件，包括平等机会和准确。这些公平性条件下，损失率分布在三个自然的范围之间：$O(\alpha)$, $O(\sqrt{\alpha})$ 和 $O(1)$。这些结果给出了对公平性限制学习对噪声训练数据的敏感度的更细节的观察。
</details></li>
</ul>
<hr>
<h2 id="On-the-Universality-of-Linear-Recurrences-Followed-by-Nonlinear-Projections"><a href="#On-the-Universality-of-Linear-Recurrences-Followed-by-Nonlinear-Projections" class="headerlink" title="On the Universality of Linear Recurrences Followed by Nonlinear Projections"></a>On the Universality of Linear Recurrences Followed by Nonlinear Projections</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11888">http://arxiv.org/abs/2307.11888</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antonio Orvieto, Soham De, Caglar Gulcehre, Razvan Pascanu, Samuel L. Smith</li>
<li>for: 这篇论文目的是提出一种基于回归线性层的序列模型，包括S4、S5和LRU等，可以将任何足够 régulier sequence-to-sequence 映射精确地模拟。</li>
<li>methods: 该论文使用了回归层和位置层的多层感知器（MLP）组合来实现序列模型，并将回归层看作是压缩算法，可以准确地存储输入序列的信息在内部状态中，然后由高度表达的 MLP 进行处理。</li>
<li>results: 该论文的结果表明，这种基于回归线性层的序列模型可以将任何足够 régulier sequence-to-sequence 映射精确地模拟，无需额外的训练或特殊设计。<details>
<summary>Abstract</summary>
In this note (work in progress towards a full-length paper) we show that a family of sequence models based on recurrent linear layers~(including S4, S5, and the LRU) interleaved with position-wise multi-layer perceptrons~(MLPs) can approximate arbitrarily well any sufficiently regular non-linear sequence-to-sequence map. The main idea behind our result is to see recurrent layers as compression algorithms that can faithfully store information about the input sequence into an inner state, before it is processed by the highly expressive MLP.
</details>
<details>
<summary>摘要</summary>
在这个笔记（ towards a full-length paper 的工作进展）中，我们显示了一家序列模型，基于回归线性层（包括 S4、S5 和 LRU）和位置层叠多层感知器（MLP），可以将任何充分 régulié sequence-to-sequence 映射逼近到任何很好的非线性映射。我们的主要想法是看待回归层为压缩算法，可以准确地将输入序列存储在内部状态中，然后由高度表达的 MLP 处理。
</details></li>
</ul>
<hr>
<h2 id="MORE-Measurement-and-Correlation-Based-Variational-Quantum-Circuit-for-Multi-classification"><a href="#MORE-Measurement-and-Correlation-Based-Variational-Quantum-Circuit-for-Multi-classification" class="headerlink" title="MORE: Measurement and Correlation Based Variational Quantum Circuit for Multi-classification"></a>MORE: Measurement and Correlation Based Variational Quantum Circuit for Multi-classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11875">http://arxiv.org/abs/2307.11875</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jindi0/more">https://github.com/jindi0/more</a></li>
<li>paper_authors: Jindi Wu, Tianjie Hu, Qun Li</li>
<li>for: 这 paper 的目的是提出一种高效的量子多类分类器，即 measurement and correlation based variational quantum multi-classifier (MORE)。</li>
<li>methods: 该方法使用了同 binary 分类器一样的变量 ansatz，并在完全利用单个读取 qubit 的量子信息上进行多类分类。具体来说，选择了三个观察量来形成二维希尔бер特空间的基态，然后使用量子状态探测技术来重建读取态。接着，通过量子变量 clustering 方法来确定类别之间的相关性，并使用量子标签 based 监督学习来确定输入数据与其相应的量子标签之间的映射。</li>
<li>results: 我们通过使用 Qiskit Python 库进行实现并对干扰量子系统和干扰系统进行广泛的实验评估，发现 MORE  DESPITE 使用简单的 ansatz 和有限的量子资源，可以达到先进的性能。<details>
<summary>Abstract</summary>
Quantum computing has shown considerable promise for compute-intensive tasks in recent years. For instance, classification tasks based on quantum neural networks (QNN) have garnered significant interest from researchers and have been evaluated in various scenarios. However, the majority of quantum classifiers are currently limited to binary classification tasks due to either constrained quantum computing resources or the need for intensive classical post-processing. In this paper, we propose an efficient quantum multi-classifier called MORE, which stands for measurement and correlation based variational quantum multi-classifier. MORE adopts the same variational ansatz as binary classifiers while performing multi-classification by fully utilizing the quantum information of a single readout qubit. To extract the complete information from the readout qubit, we select three observables that form the basis of a two-dimensional Hilbert space. We then use the quantum state tomography technique to reconstruct the readout state from the measurement results. Afterward, we explore the correlation between classes to determine the quantum labels for classes using the variational quantum clustering approach. Next, quantum label-based supervised learning is performed to identify the mapping between the input data and their corresponding quantum labels. Finally, the predicted label is determined by its closest quantum label when using the classifier. We implement this approach using the Qiskit Python library and evaluate it through extensive experiments on both noise-free and noisy quantum systems. Our evaluation results demonstrate that MORE, despite using a simple ansatz and limited quantum resources, achieves advanced performance.
</details>
<details>
<summary>摘要</summary>
量子计算在最近几年内已经表现出了较大的搭配能力。例如，基于量子神经网络（QNN）的分类任务已经吸引了研究者的广泛关注并在各种场景中进行了评估。然而，现在大多数量子分类器都受到了限制的量子计算资源或需要大量的经典后处理。在这篇论文中，我们提出了一种高效的量子多分类器，称为MORE，它 stands for measurement and correlation based variational quantum multi-classifier。MORE采用了同 binary 分类器一样的变量 ansatz，而在完全利用单个读取量子 bits 的量子信息进行多分类。为了从读取量子 bits 中提取完整的信息，我们选择了三个观测量，它们构成了一个二维希尔бер特空间的基。然后，我们使用量子状态探测技术来重建读取状态从测量结果中。接着，我们研究分类关系来确定类别的量子标签使用变量量子 clustering 方法。然后，我们使用量子标签基于的超vised 学习方法来确定输入数据和其对应的量子标签之间的映射。最后，我们使用类ifier 来预测输入数据的标签。我们使用 Qiskit Python 库实现这种方法，并通过对噪声量子系统和噪声free 量子系统进行了广泛的实验来评估其性能。我们的评估结果表明，MORE，即使使用简单的 ansatz 和有限的量子资源，仍然可以达到先进的性能。
</details></li>
</ul>
<hr>
<h2 id="The-Looming-Threat-of-Fake-and-LLM-generated-LinkedIn-Profiles-Challenges-and-Opportunities-for-Detection-and-Prevention"><a href="#The-Looming-Threat-of-Fake-and-LLM-generated-LinkedIn-Profiles-Challenges-and-Opportunities-for-Detection-and-Prevention" class="headerlink" title="The Looming Threat of Fake and LLM-generated LinkedIn Profiles: Challenges and Opportunities for Detection and Prevention"></a>The Looming Threat of Fake and LLM-generated LinkedIn Profiles: Challenges and Opportunities for Detection and Prevention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11864">http://arxiv.org/abs/2307.11864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navid Ayoobi, Sadat Shahriar, Arjun Mukherjee<br>for: 这个研究旨在检测 LinkedIn 在注册时 already 生成的假 profiles，以保持平台的完整性，防止假用户从获取真正用户的个人信息和敏感信息，并防止假用户在未来的骗财和骗取活动中增加假信望。methods: 这个研究使用 LinkedIn  Profil 中提供的文本信息，并引入 Section and Subsection Tag Embedding (SSTE) 方法，以增强这些数据的分类特征，从而分辨真实 profil 和假 profil。results: 研究表明，使用 static 和 contextualized word embeddings，包括 GloVe、Flair、BERT 和 RoBERTa，可以将 legitimate 和假 profil 分辨出来，准确率大约为 95%。此外，研究还表明，SSTE 在训练集中没有使用 LLM 生成的 profil 时，可以达到约 90% 的准确率。这是一个重要的发现，因为未来几年内，多种 LLM 将在普及，设计一个可以分辨多种 LLM 生成的 profil 的系统会变得非常困难。<details>
<summary>Abstract</summary>
In this paper, we present a novel method for detecting fake and Large Language Model (LLM)-generated profiles in the LinkedIn Online Social Network immediately upon registration and before establishing connections. Early fake profile identification is crucial to maintaining the platform's integrity since it prevents imposters from acquiring the private and sensitive information of legitimate users and from gaining an opportunity to increase their credibility for future phishing and scamming activities. This work uses textual information provided in LinkedIn profiles and introduces the Section and Subsection Tag Embedding (SSTE) method to enhance the discriminative characteristics of these data for distinguishing between legitimate profiles and those created by imposters manually or by using an LLM. Additionally, the dearth of a large publicly available LinkedIn dataset motivated us to collect 3600 LinkedIn profiles for our research. We will release our dataset publicly for research purposes. This is, to the best of our knowledge, the first large publicly available LinkedIn dataset for fake LinkedIn account detection. Within our paradigm, we assess static and contextualized word embeddings, including GloVe, Flair, BERT, and RoBERTa. We show that the suggested method can distinguish between legitimate and fake profiles with an accuracy of about 95% across all word embeddings. In addition, we show that SSTE has a promising accuracy for identifying LLM-generated profiles, despite the fact that no LLM-generated profiles were employed during the training phase, and can achieve an accuracy of approximately 90% when only 20 LLM-generated profiles are added to the training set. It is a significant finding since the proliferation of several LLMs in the near future makes it extremely challenging to design a single system that can identify profiles created with various LLMs.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的方法，用于在 LinkedIn 在线社交平台上立即 региSTRATION 后识别假 profil和大语言模型（LLM）生成的 profil。 Early 识别假 profil是维护平台的完整性非常重要，因为它防止了假者从获得真实用户的私人和敏感信息，并从获得诈骗和骗子活动的机会。这种工作使用 LinkedIn  profil 中提供的文本信息，并引入了分段和子分段标签嵌入（SSTE）方法，以增强这些数据的区分性，用于分辨真实 profil 和由假者或 LLM 手动或机器生成的 profil。此外，由于没有大规模公开可用的 LinkedIn 数据集，我们自己收集了 3600 个 LinkedIn profil 进行研究。我们将在研究中发布我们的数据集。这是，我们知道的情况下，首个大规模公开 LinkedIn 数据集，用于假 LinkedIn 账户检测。在我们的 paradigm 中，我们评估了静态和 contextualized 单词嵌入，包括 GloVe、Flair、BERT 和 RoBERTa。我们发现，建议的方法可以在所有单词嵌入上分辨真实 profil 和假 profil，准确率大约为 95%。此外，我们发现，SSTE 在 LLM 生成 profil 上有承诺的准确率，即使在训练阶段没有使用 LLM 生成 profil，可以达到约 90% 的准确率，只需要将 20 个 LLM 生成 profil 添加到训练集中。这是一项重要发现，因为未来几年，许多 LLM 将在未来普及，设计一个系统可以identify 由多种 LLM 生成的 profil 是极其困难的。
</details></li>
</ul>
<hr>
<h2 id="Data-Induced-Interactions-of-Sparse-Sensors"><a href="#Data-Induced-Interactions-of-Sparse-Sensors" class="headerlink" title="Data-Induced Interactions of Sparse Sensors"></a>Data-Induced Interactions of Sparse Sensors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11838">http://arxiv.org/abs/2307.11838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrei A. Klishin, J. Nathan Kutz, Krithika Manohar</li>
<li>for: 这篇论文是为了探讨大型实验数据在科学和工程中的低维结构，以及如何使用少量的感知器来重建完整的系统状态。</li>
<li>methods: 论文使用了受限 interpolation 和 QR 分解算法来优化感知器的布局。</li>
<li>results: 论文通过统计物理的热力学视角计算出感知器与训练数据之间的互动场景，并可以结合外部选择 критери估算感知器更换的影响。<details>
<summary>Abstract</summary>
Large-dimensional empirical data in science and engineering frequently has low-rank structure and can be represented as a combination of just a few eigenmodes. Because of this structure, we can use just a few spatially localized sensor measurements to reconstruct the full state of a complex system. The quality of this reconstruction, especially in the presence of sensor noise, depends significantly on the spatial configuration of the sensors. Multiple algorithms based on gappy interpolation and QR factorization have been proposed to optimize sensor placement. Here, instead of an algorithm that outputs a singular "optimal" sensor configuration, we take a thermodynamic view to compute the full landscape of sensor interactions induced by the training data. The landscape takes the form of the Ising model in statistical physics, and accounts for both the data variance captured at each sensor location and the crosstalk between sensors. Mapping out these data-induced sensor interactions allows combining them with external selection criteria and anticipating sensor replacement impacts.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="eXplainable-Artificial-Intelligence-XAI-in-age-prediction-A-systematic-review"><a href="#eXplainable-Artificial-Intelligence-XAI-in-age-prediction-A-systematic-review" class="headerlink" title="eXplainable Artificial Intelligence (XAI) in age prediction: A systematic review"></a>eXplainable Artificial Intelligence (XAI) in age prediction: A systematic review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13704">http://arxiv.org/abs/2307.13704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alena Kalyakulina, Igor Yusipov</li>
<li>for: 这项研究旨在探讨使用可解释人工智能（XAI）方法进行年龄预测任务的应用。</li>
<li>methods: 该研究使用了多种XAI方法，包括体系层次分析、可读性分析和模型解释等方法，对年龄预测任务进行了系统性的梳理和分析。</li>
<li>results: 研究发现，XAI方法可以帮助提高年龄预测的准确率和可解释性，并且可以帮助降低年龄预测模型中的风险和不确定性。<details>
<summary>Abstract</summary>
eXplainable Artificial Intelligence (XAI) is now an important and essential part of machine learning, allowing to explain the predictions of complex models. XAI is especially required in risky applications, particularly in health care, where human lives depend on the decisions of AI systems. One area of medical research is age prediction and identification of biomarkers of aging and age-related diseases. However, the role of XAI in the age prediction task has not previously been explored directly. In this review, we discuss the application of XAI approaches to age prediction tasks. We give a systematic review of the works organized by body systems, and discuss the benefits of XAI in medical applications and, in particular, in the age prediction domain.
</details>
<details>
<summary>摘要</summary>
互解的人工智能（XAI）现在成为机器学习中重要和必需的一部分，允许解释模型预测的结果。XAI特别在危险应用中需要，如医疗领域，人工智能系统的决策直接影响人们的生命。一个医学研究领域是年龄预测和年龄相关疾病的识别 биомarkers。然而，XAI在年龄预测任务中的角色尚未直接探讨。在这篇评论中，我们讨论了XAI方法在年龄预测任务中的应用。我们按照身体系统进行了系统性的回顾，并讨论了医疗应用中XAI的优点，特别是在年龄预测领域。
</details></li>
</ul>
<hr>
<h2 id="PINNsFormer-A-Transformer-Based-Framework-For-Physics-Informed-Neural-Networks"><a href="#PINNsFormer-A-Transformer-Based-Framework-For-Physics-Informed-Neural-Networks" class="headerlink" title="PINNsFormer: A Transformer-Based Framework For Physics-Informed Neural Networks"></a>PINNsFormer: A Transformer-Based Framework For Physics-Informed Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11833">http://arxiv.org/abs/2307.11833</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adityalab/pinnsformer">https://github.com/adityalab/pinnsformer</a></li>
<li>paper_authors: Leo Zhiyuan Zhao, Xueying Ding, B. Aditya Prakash</li>
<li>for: 这个论文是为了解决深度学习框架中的数学方程问题，特别是解决偏微分方程（PDEs）的数值解。</li>
<li>methods: 该论文提出了一种基于Transformer的新框架，称为PINNsFormer，它可以准确地 aproximate PDEs的解决方案，通过捕捉时间相关性使用多头注意力机制。</li>
<li>results: 该论文通过实验表明，PINNsFormer可以在多种场景中更好地学习PDEs的解决方案，比如传统PINNs无法学习的场景。此外，PINNsFormer还可以在计算和存储成本下降时，与传统PINNs相比，减少约10%的计算和存储成本。<details>
<summary>Abstract</summary>
Physics-Informed Neural Networks (PINNs) have emerged as a promising deep learning framework for approximating numerical solutions for partial differential equations (PDEs). While conventional PINNs and most related studies adopt fully-connected multilayer perceptrons (MLP) as the backbone structure, they have neglected the temporal relations in PDEs and failed to approximate the true solution. In this paper, we propose a novel Transformer-based framework, namely PINNsFormer, that accurately approximates PDEs' solutions by capturing the temporal dependencies with multi-head attention mechanisms in Transformer-based models. Instead of approximating point predictions, PINNsFormer adapts input vectors to pseudo sequences and point-wise PINNs loss to a sequential PINNs loss. In addition, PINNsFormer is equipped with a novel activation function, namely Wavelet, which anticipates the Fourier decomposition through deep neural networks. We empirically demonstrate PINNsFormer's ability to capture the PDE solutions for various scenarios, in which conventional PINNs have failed to learn. We also show that PINNsFormer achieves superior approximation accuracy on such problems than conventional PINNs with non-sensitive hyperparameters, in trade of marginal computational and memory costs, with extensive experiments.
</details>
<details>
<summary>摘要</summary>
物理 Informed Neural Networks (PINNs) 已经出现为解析数学方程 partial differential equations (PDEs) 的深度学习框架。  although conventional PINNs and most related studies adopt fully-connected multilayer perceptrons (MLP) as the backbone structure, they have neglected the temporal relations in PDEs and failed to approximate the true solution. 在这篇论文中，我们提出了一种新的 Transformer 基于的框架，即 PINNsFormer，可以准确地 approximates PDEs 的解决方案，通过捕捉 Transformer 中的多头注意机制来捕捉 PDEs 中的时间相关性。 而不是通过点预测来 Approximation，PINNsFormer 将输入向量映射到 pseudo 序列上，并将点级 PINNs 损失转换为顺序 PINNs 损失。 此外，PINNsFormer 还配备了一种新的活动函数，即 Wavelet，可以预测 Fourier 分解在深度神经网络中。 我们在实验中证明了 PINNsFormer 可以在各种情况下 capture PDEs 的解决方案，而 conventional PINNs 无法学习。 此外，PINNsFormer 在这些问题上的 Approximation 精度高于 conventional PINNs ，尽管计算和内存成本增加了一些，通过广泛的实验证明。
</details></li>
</ul>
<hr>
<h2 id="Differentially-Private-Heavy-Hitter-Detection-using-Federated-Analytics"><a href="#Differentially-Private-Heavy-Hitter-Detection-using-Federated-Analytics" class="headerlink" title="Differentially Private Heavy Hitter Detection using Federated Analytics"></a>Differentially Private Heavy Hitter Detection using Federated Analytics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11749">http://arxiv.org/abs/2307.11749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karan Chadha, Junye Chen, John Duchi, Vitaly Feldman, Hanieh Hashemi, Omid Javidbakht, Audra McMillan, Kunal Talwar</li>
<li>for: 本文提出了一种基于前缀树的差分private大量访问检测算法，以实现每个用户的多个数据点的学习和差分隐私保护。</li>
<li>methods: 本文提出了一种自适应hyperparameter调整算法，以提高算法性能，同时满足计算、通信和隐私约束。此外，本文还 explore了不同的数据选择方案以及引入拒绝列表的影响。</li>
<li>results: 经过EXTENSIVE EXPERIMENTATION ON THE REDDIT DATASET，本文发现这些改进都能够提高算法性能，同时满足差分隐私和计算、通信约束。<details>
<summary>Abstract</summary>
In this work, we study practical heuristics to improve the performance of prefix-tree based algorithms for differentially private heavy hitter detection. Our model assumes each user has multiple data points and the goal is to learn as many of the most frequent data points as possible across all users' data with aggregate and local differential privacy. We propose an adaptive hyperparameter tuning algorithm that improves the performance of the algorithm while satisfying computational, communication and privacy constraints. We explore the impact of different data-selection schemes as well as the impact of introducing deny lists during multiple runs of the algorithm. We test these improvements using extensive experimentation on the Reddit dataset~\cite{caldas2018leaf} on the task of learning the most frequent words.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们研究了可行的启发式法则，以提高基于 prefix-tree 算法的差分隐私极大热点检测性能。我们的模型假设每名用户有多个数据点，目标是通过聚合和本地差分隐私来学习所有用户数据中的最多热点数据点。我们提议一种适应性hyperparameter调整算法，可以提高算法的性能，同时满足计算、通信和隐私约束。我们还研究了不同的数据选择方案以及在多次运行算法时引入拒绝列表的影响。我们通过对 Reddit 数据集（Caldas et al., 2018）的 Word 学习任务进行了广泛的实验来评估这些改进。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Ad-Auction-Realism-Practical-Insights-Modeling-Implications"><a href="#Advancing-Ad-Auction-Realism-Practical-Insights-Modeling-Implications" class="headerlink" title="Advancing Ad Auction Realism: Practical Insights &amp; Modeling Implications"></a>Advancing Ad Auction Realism: Practical Insights &amp; Modeling Implications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11732">http://arxiv.org/abs/2307.11732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ming Chen, Sareh Nabi, Marciano Siniscalchi</li>
<li>for: 本文提出了一种在网络广告拍卖中学习模型，以允许现代网络拍卖中的四个关键现实特征：（1）广告插槽的价值和点击率因用户搜索词而变化，（2）投标商的数量和身份在每次拍卖中是不确定的，（3）广告主只收到部分、汇总的反馈，（4）支付规则只有部分指定。</li>
<li>methods: 作者们使用了一种对抗强制算法来模型广告主的行为，不受拍卖机制细节的影响。</li>
<li>results: 研究发现，在更加复杂的环境下，“软底”可以提高关键性能指标，即使投标商来自同一个人口。此外，研究还证明了如何从观察拍卖价格中推断广告主价值分布，从而证明了这种方法在更真实的拍卖 Setting 中的实际可行性。<details>
<summary>Abstract</summary>
This paper proposes a learning model of online ad auctions that allows for the following four key realistic characteristics of contemporary online auctions: (1) ad slots can have different values and click-through rates depending on users' search queries, (2) the number and identity of competing advertisers are unobserved and change with each auction, (3) advertisers only receive partial, aggregated feedback, and (4) payment rules are only partially specified. We model advertisers as agents governed by an adversarial bandit algorithm, independent of auction mechanism intricacies. Our objective is to simulate the behavior of advertisers for counterfactual analysis, prediction, and inference purposes. Our findings reveal that, in such richer environments, "soft floors" can enhance key performance metrics even when bidders are drawn from the same population. We further demonstrate how to infer advertiser value distributions from observed bids, thereby affirming the practical efficacy of our approach even in a more realistic auction setting.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Ad slots can have different values and click-through rates depending on users’ search queries.2. The number and identity of competing advertisers are unobserved and change with each auction.3. Advertisers only receive partial, aggregated feedback.4. Payment rules are only partially specified.We model advertisers as agents governed by an adversarial bandit algorithm, independent of auction mechanism intricacies. Our objective is to simulate the behavior of advertisers for counterfactual analysis, prediction, and inference purposes.Our findings show that, in such richer environments, “soft floors” can enhance key performance metrics even when bidders are drawn from the same population. We also demonstrate how to infer advertiser value distributions from observed bids, thereby confirming the practical efficacy of our approach in a more realistic auction setting.</details></li>
</ol>
<hr>
<h2 id="Mitigating-Communications-Threats-in-Decentralized-Federated-Learning-through-Moving-Target-Defense"><a href="#Mitigating-Communications-Threats-in-Decentralized-Federated-Learning-through-Moving-Target-Defense" class="headerlink" title="Mitigating Communications Threats in Decentralized Federated Learning through Moving Target Defense"></a>Mitigating Communications Threats in Decentralized Federated Learning through Moving Target Defense</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11730">http://arxiv.org/abs/2307.11730</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/enriquetomasmb/fedstellar">https://github.com/enriquetomasmb/fedstellar</a></li>
<li>paper_authors: Enrique Tomás Martínez Beltrán, Pedro Miguel Sánchez Sánchez, Sergio López Bernal, Gérôme Bovet, Manuel Gil Pérez, Gregorio Martínez Pérez, Alberto Huertas Celdrán</li>
<li>for: This paper focuses on addressing communication security challenges in Decentralized Federated Learning (DFL) to ensure the privacy and integrity of data during model aggregation.</li>
<li>methods: The paper introduces a security module that combines symmetric and asymmetric encryption with Moving Target Defense (MTD) techniques, including random neighbor selection and IP&#x2F;port switching, to protect DFL communications.</li>
<li>results: The security module is validated through experiments with the MNIST dataset and eclipse attacks, showing an average F1 score of 95% with moderate increases in CPU usage (up to 63.2% +-3.5%) and network traffic (230 MB +-15 MB) under the most secure configuration, mitigating the risks posed by eavesdropping or eclipse attacks.Here’s the Chinese version of the three key points:</li>
<li>for: 这篇论文关注Decentralized Federated Learning (DFL) 中通信安全问题，以确保数据隐私和完整性 durante 模型聚合。</li>
<li>methods: 这篇论文提出了一个安全模块，该模块结合 симметриック和非对称加密技术，以及移动目标防御 (MTD) 技术，包括随机邻居选择和 IP&#x2F;端口 switching，以保护 DFL 通信。</li>
<li>results: 安全模块通过 MNIST 数据集和 Eclipse 攻击进行了验证，结果显示，在最安全配置下，平均 F1 分数达到 95%，CPU 使用率最高达 63.2% +-3.5%，网络流量最高达 230 MB +-15 MB，成功 mitigate 隐私攻击和 Eclipse 攻击的风险。<details>
<summary>Abstract</summary>
The rise of Decentralized Federated Learning (DFL) has enabled the training of machine learning models across federated participants, fostering decentralized model aggregation and reducing dependence on a server. However, this approach introduces unique communication security challenges that have yet to be thoroughly addressed in the literature. These challenges primarily originate from the decentralized nature of the aggregation process, the varied roles and responsibilities of the participants, and the absence of a central authority to oversee and mitigate threats. Addressing these challenges, this paper first delineates a comprehensive threat model, highlighting the potential risks of DFL communications. In response to these identified risks, this work introduces a security module designed for DFL platforms to counter communication-based attacks. The module combines security techniques such as symmetric and asymmetric encryption with Moving Target Defense (MTD) techniques, including random neighbor selection and IP/port switching. The security module is implemented in a DFL platform called Fedstellar, allowing the deployment and monitoring of the federation. A DFL scenario has been deployed, involving eight physical devices implementing three security configurations: (i) a baseline with no security, (ii) an encrypted configuration, and (iii) a configuration integrating both encryption and MTD techniques. The effectiveness of the security module is validated through experiments with the MNIST dataset and eclipse attacks. The results indicated an average F1 score of 95%, with moderate increases in CPU usage (up to 63.2% +-3.5%) and network traffic (230 MB +-15 MB) under the most secure configuration, mitigating the risks posed by eavesdropping or eclipse attacks.
</details>
<details>
<summary>摘要</summary>
德中 federated learning (DFL) 的出现已经允许机器学习模型在联合参与者之间训练，从而实现了分布式模型集成和减少服务器依赖。然而，这种方法引入了一些独特的通信安全挑战，在文献中尚未得到充分研究。这些挑战主要来自分布式集成过程中的各种角色和责任，以及缺乏中央权威来监督和 Mitigate 威胁。为了解决这些挑战，本文首先提出了一个完整的威胁模型，描述了 DFL 通信中的潜在风险。作为回应，本工作提出了一种针对 DFL 平台的安全模块，该模块结合了Symmetric and Asymmetric Encryption 技术以及 Move Target Defense (MTD) 技术，包括随机邻居选择和 IP/port 转换。该安全模块在 Fedstellar 平台上实现，allowing the deployment and monitoring of the federation。在一个 DFL 场景中，涉及到八个物理设备，实现了三种安全配置：（i）无安全配置，（ii）加密配置，和（iii）集成加密和 MTD 技术的配置。通过使用 MNIST 数据集和 Eclipse 攻击进行实验， validate 了安全模块的效iveness。结果表明，在最安全配置下，模型的 F1 分数平均为 95%，CPU 使用率（最高）为 63.2% ± 3.5%，网络流量（最高）为 230 MB ± 15 MB。这些结果表明，通过加密和 MTD 技术，可以有效地 mitigate 防止窃听或 Eclipse 攻击。
</details></li>
</ul>
<hr>
<h2 id="Local-Kernel-Renormalization-as-a-mechanism-for-feature-learning-in-overparametrized-Convolutional-Neural-Networks"><a href="#Local-Kernel-Renormalization-as-a-mechanism-for-feature-learning-in-overparametrized-Convolutional-Neural-Networks" class="headerlink" title="Local Kernel Renormalization as a mechanism for feature learning in overparametrized Convolutional Neural Networks"></a>Local Kernel Renormalization as a mechanism for feature learning in overparametrized Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11807">http://arxiv.org/abs/2307.11807</a></li>
<li>repo_url: None</li>
<li>paper_authors: R. Aiudi, R. Pacelli, A. Vezzani, R. Burioni, P. Rotondo<br>for:This paper explores the differences in feature learning between fully-connected (FC) and convolutional architectures (CNNs) in deep neural networks.methods:The paper uses a simple theoretical framework to provide a rationale for the differences in performance between FC and CNN architectures. The authors derive a finite-width effective action for an architecture with one convolutional hidden layer and compare it with the result available for FC networks.results:The paper shows that the kernel of the CNN architecture undergoes a local renormalization, meaning that the network can select the local components that will contribute to the final prediction in a data-dependent way. This finding highlights a simple mechanism for feature learning that can take place in overparametrized shallow CNNs, but not in shallow FC architectures or in locally connected neural networks without weight sharing.<details>
<summary>Abstract</summary>
Feature learning, or the ability of deep neural networks to automatically learn relevant features from raw data, underlies their exceptional capability to solve complex tasks. However, feature learning seems to be realized in different ways in fully-connected (FC) or convolutional architectures (CNNs). Empirical evidence shows that FC neural networks in the infinite-width limit eventually outperform their finite-width counterparts. Since the kernel that describes infinite-width networks does not evolve during training, whatever form of feature learning occurs in deep FC architectures is not very helpful in improving generalization. On the other hand, state-of-the-art architectures with convolutional layers achieve optimal performances in the finite-width regime, suggesting that an effective form of feature learning emerges in this case. In this work, we present a simple theoretical framework that provides a rationale for these differences, in one hidden layer networks. First, we show that the generalization performance of a finite-width FC network can be obtained by an infinite-width network, with a suitable choice of the Gaussian priors. Second, we derive a finite-width effective action for an architecture with one convolutional hidden layer and compare it with the result available for FC networks. Remarkably, we identify a completely different form of kernel renormalization: whereas the kernel of the FC architecture is just globally renormalized by a single scalar parameter, the CNN kernel undergoes a local renormalization, meaning that the network can select the local components that will contribute to the final prediction in a data-dependent way. This finding highlights a simple mechanism for feature learning that can take place in overparametrized shallow CNNs, but not in shallow FC architectures or in locally connected neural networks without weight sharing.
</details>
<details>
<summary>摘要</summary>
FC neural networks 和 CNNs 都有不同的方式来实现特征学习。empirical evidence 表明，在无穷宽度限制下，FC neural networks 的性能会超过其有限宽度对应的性能。然而，由于无穷宽度网络的核函数在训练过程中不会发展，因此FC neural networks 中的特征学习不会对泛化性能产生帮助。相反，当前的状态机器学习架构中的 convolutional layers 可以在有限宽度限制下达到最佳性能，这 Suggests 在这种情况下可以发现有效的特征学习方式。在这项工作中，我们提出了一个简单的理论框架，以解释这些差异。首先，我们表明了一个有限宽度 FC 网络的泛化性能可以通过无穷宽度网络来实现，并且可以通过适当的 Gaussian priors 来选择。其次，我们 derivated 一个有限宽度效果动作，并与 FC 网络的结果进行比较。意外地，我们发现了一种完全不同的kernel renormalization：FC 网络的核函数只是全局地 renormalized 一个整数参数，而 CNN 的核函数则会在数据依存的方式下 renormalized，这意味着网络可以在数据依存的情况下选择当地组件，以便在最终预测中做出贡献。这一发现高光了一种简单的特征学习机制，可以在过 parametrization 的 shallow CNN 中发生，但不可以在 shallow FC 网络或者 Without weight sharing 的本地连接神经网络中发生。
</details></li>
</ul>
<hr>
<h2 id="Convergence-of-SGD-for-Training-Neural-Networks-with-Sliced-Wasserstein-Losses"><a href="#Convergence-of-SGD-for-Training-Neural-Networks-with-Sliced-Wasserstein-Losses" class="headerlink" title="Convergence of SGD for Training Neural Networks with Sliced Wasserstein Losses"></a>Convergence of SGD for Training Neural Networks with Sliced Wasserstein Losses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11714">http://arxiv.org/abs/2307.11714</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eloi Tanguy</li>
<li>for: 这 paper 是关于 Optimal Transport 和 Neural Networks 的研究，具体来说是关于 Stochastic Gradient Descent (SGD) 在 Non-smooth 和 Non-convex 函数上的收敛性。</li>
<li>methods: 这 paper 使用了 Sliced Wasserstein (SW) 距离作为比较概率分布的方法，并使用了 Bianchi et al. (2022) 所提出的一些新的结果来证明 SGD 的收敛性。</li>
<li>results: 这 paper 显示了 fixed-step SGD 轨迹在 NN 参数上的收敛性，并且在更严格的假设下，显示了 noised 和 projected SGD 的收敛性。 Specifically, the paper shows that the trajectories of fixed-step SGD approach the set of (sub)-gradient flow equations as the step decreases, and under stricter assumptions, the long-run limits of the trajectories approach a set of generalised critical points of the loss function.<details>
<summary>Abstract</summary>
Optimal Transport has sparked vivid interest in recent years, in particular thanks to the Wasserstein distance, which provides a geometrically sensible and intuitive way of comparing probability measures. For computational reasons, the Sliced Wasserstein (SW) distance was introduced as an alternative to the Wasserstein distance, and has seen uses for training generative Neural Networks (NNs). While convergence of Stochastic Gradient Descent (SGD) has been observed practically in such a setting, there is to our knowledge no theoretical guarantee for this observation. Leveraging recent works on convergence of SGD on non-smooth and non-convex functions by Bianchi et al. (2022), we aim to bridge that knowledge gap, and provide a realistic context under which fixed-step SGD trajectories for the SW loss on NN parameters converge. More precisely, we show that the trajectories approach the set of (sub)-gradient flow equations as the step decreases. Under stricter assumptions, we show a much stronger convergence result for noised and projected SGD schemes, namely that the long-run limits of the trajectories approach a set of generalised critical points of the loss function.
</details>
<details>
<summary>摘要</summary>
最近年来，最优运输（Optimal Transport）已经引起了广泛的关注，尤其是通过瓦asserstein距离（Wasserstein distance）来比较概率分布的 geometrically sensible 和直观的方法。由于计算原因，人工智能神经网络（Neural Networks，NNs）的训练中使用了截面瓦asserstein（SW）距离，并且在实践中观察到了SGD的收敛。然而，我们知道的理论保证是不够。基于 Bianchi et al. (2022) 的最近研究，我们想要填补这个知识隔口，并在NN参数的 SW 损失下实现了SGD的收敛。更加准确地说，我们证明了SGD的流体动向逐步逼近（sub-gradient flow）的设置，并在更加严格的假设下证明了SGD的杂化和投影后的收敛结果。
</details></li>
</ul>
<hr>
<h2 id="JoinGym-An-Efficient-Query-Optimization-Environment-for-Reinforcement-Learning"><a href="#JoinGym-An-Efficient-Query-Optimization-Environment-for-Reinforcement-Learning" class="headerlink" title="JoinGym: An Efficient Query Optimization Environment for Reinforcement Learning"></a>JoinGym: An Efficient Query Optimization Environment for Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11704">http://arxiv.org/abs/2307.11704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaiwen Wang, Junxiong Wang, Yueying Li, Nathan Kallus, Immanuel Trummer, Wen Sun</li>
<li>For: 这篇论文旨在提供一个高效和轻量级的查询优化环境，用于应用强化学习（RL）在数据管理问题中。* Methods: 论文使用Markov决策过程（MDP）形式表述了左深和树状变体的Join order selection（JOS）问题，并提供了遵循标准Gymnasium API的实现。* Results: 论文发现，使用RL算法可以在培训集查询中near-优秀表现，但是在测试集查询中表现下降多个数量级。这个差距驱动了进一步研究RL算法在多任务 combinatorial optimization问题中的泛化能力。<details>
<summary>Abstract</summary>
In this paper, we present \textsc{JoinGym}, an efficient and lightweight query optimization environment for reinforcement learning (RL). Join order selection (JOS) is a classic NP-hard combinatorial optimization problem from database query optimization and can serve as a practical testbed for the generalization capabilities of RL algorithms. We describe how to formulate each of the left-deep and bushy variants of the JOS problem as a Markov Decision Process (MDP), and we provide an implementation adhering to the standard Gymnasium API. We highlight that our implementation \textsc{JoinGym} is completely based on offline traces of all possible joins, which enables RL practitioners to easily and quickly test their methods on a realistic data management problem without needing to setup any systems. Moreover, we also provide all possible join traces on $3300$ novel SQL queries generated from the IMDB dataset. Upon benchmarking popular RL algorithms, we find that at least one method can obtain near-optimal performance on train-set queries but their performance degrades by several orders of magnitude on test-set queries. This gap motivates further research for RL algorithms that generalize well in multi-task combinatorial optimization problems.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一个名为\textsc{JoinGym}的高效和轻量级查询优化环境，用于应用束缚学习（RL）。Join顺序选择（JOS）是一个经典的NP困难的 combinatorial optimization问题，来自数据库查询优化，可以作为RL算法的通用化能力的实验室。我们介绍了如何将左深和叠缩 variant of JOS问题转换为Markov决策过程（MDP），并提供了遵循标准Gymnasium API的实现。我们指出，我们的实现\textsc{JoinGym}完全基于所有可能的 joins 的离线轨迹，这使得RL实践者可以很容易地和快速地在真实的数据管理问题上测试他们的方法，无需设置任何系统。此外，我们还提供了 $3300$ 个新的 SQL 查询，生成自 IMDB 数据集。在 benchmarking 流行的 RL 算法时，我们发现至少有一种方法可以在训练集查询上达到 Near-optimal 性能，但它们在测试集查询上表现下降多个ORDERS。这个差距激励我们进一步研究 RL 算法在多任务 combinatorial optimization 问题中的泛化能力。
</details></li>
</ul>
<hr>
<h2 id="Using-simulation-to-calibrate-real-data-acquisition-in-veterinary-medicine"><a href="#Using-simulation-to-calibrate-real-data-acquisition-in-veterinary-medicine" class="headerlink" title="Using simulation to calibrate real data acquisition in veterinary medicine"></a>Using simulation to calibrate real data acquisition in veterinary medicine</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11695">http://arxiv.org/abs/2307.11695</a></li>
<li>repo_url: None</li>
<li>paper_authors: Krystian Strzałka, Szymon Mazurek, Maciej Wielgosz, Paweł Russek, Jakub Caputa, Daria Łukasik, Jan Krupiński, Jakub Grzeszczyk, Michał Karwatowski, Rafał Frączek, Ernest Jamro, Marcin Pietroń, Sebastian Koryciak, Agnieszka Dąbrowska-Boruch, Kazimierz Wiatr</li>
<li>for: 这paper探讨了使用模拟环境增强兽医数据获取和诊断的创新方法, 专注于狗的步态分析。</li>
<li>methods: 这paper使用Blender和Blenderproc库生成 simulate diverse anatomical, environmental, and behavioral conditions的数据集, 并标准化 Graph形式进行优化分析。</li>
<li>results: 初步结果表明, 这种基于模拟的方法可能会提高兽医诊断的精度和效率, 通过结合实际和synthetic数据来提高整体效果。<details>
<summary>Abstract</summary>
This paper explores the innovative use of simulation environments to enhance data acquisition and diagnostics in veterinary medicine, focusing specifically on gait analysis in dogs. The study harnesses the power of Blender and the Blenderproc library to generate synthetic datasets that reflect diverse anatomical, environmental, and behavioral conditions. The generated data, represented in graph form and standardized for optimal analysis, is utilized to train machine learning algorithms for identifying normal and abnormal gaits. Two distinct datasets with varying degrees of camera angle granularity are created to further investigate the influence of camera perspective on model accuracy. Preliminary results suggest that this simulation-based approach holds promise for advancing veterinary diagnostics by enabling more precise data acquisition and more effective machine learning models. By integrating synthetic and real-world patient data, the study lays a robust foundation for improving overall effectiveness and efficiency in veterinary medicine.
</details>
<details>
<summary>摘要</summary>
这篇论文探讨了使用模拟环境增强畜牧医学数据收集和诊断的创新方法，特地关注了狗的步态分析。研究使用Blender和Blenderproc库生成了具有多样化 анатомиче、环境和行为条件的 sintetic数据集。生成的数据表示为图形形式标准化，用于训练机器学习算法来识别正常和异常的步态。研究创建了两个不同的摄像头角度精细度的数据集，以更好地调查摄像头角度对模型准确性的影响。初步结果表明，这种基于模拟的方法可能对畜牧医学诊断带来进步，使得数据收集更加精准，机器学习模型更加有效。通过将 sintetic和实际病人数据集 integrate，研究为畜牧医学的整体效果和效率提供了一个坚实的基础。
</details></li>
</ul>
<hr>
<h2 id="Fast-Adaptive-Test-Time-Defense-with-Robust-Features"><a href="#Fast-Adaptive-Test-Time-Defense-with-Robust-Features" class="headerlink" title="Fast Adaptive Test-Time Defense with Robust Features"></a>Fast Adaptive Test-Time Defense with Robust Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11672">http://arxiv.org/abs/2307.11672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anurag Singh, Mahalakshmi Sabanayagam, Krikamol Muandet, Debarghya Ghoshdastidar</li>
<li>for: 本研究旨在提高深度神经网络的对抗性能，并且提出了一种新的适应测试时防御策略，可以轻松地与任何现有的鲁棒训练过程结合使用，无需额外的测试时计算。</li>
<li>methods: 本研究使用了一种基于特征feature的稳定性概念，将训练过程中的模型参数 проекed到最稳定的特征空间中，从而减少了对抗性攻击的脆弱性。我们 theoretically 展示了一般加法模型中的顶层特征空间更加稳定，并且通过NTK相等性证明了这一点。</li>
<li>results: 我们在CIFAR-10和CIFAR-100数据集上进行了广泛的实验，包括RobustBench中的州际方法，并观察到了提出的方法在计算成本下较低时即使能够超越现有的适应测试时防御策略。<details>
<summary>Abstract</summary>
Adaptive test-time defenses are used to improve the robustness of deep neural networks to adversarial examples. However, existing methods significantly increase the inference time due to additional optimization on the model parameters or the input at test time. In this work, we propose a novel adaptive test-time defense strategy that is easy to integrate with any existing (robust) training procedure without additional test-time computation. Based on the notion of robustness of features that we present, the key idea is to project the trained models to the most robust feature space, thereby reducing the vulnerability to adversarial attacks in non-robust directions. We theoretically show that the top eigenspace of the feature matrix are more robust for a generalized additive model and support our argument for a large width neural network with the Neural Tangent Kernel (NTK) equivalence. We conduct extensive experiments on CIFAR-10 and CIFAR-100 datasets for several robustness benchmarks, including the state-of-the-art methods in RobustBench, and observe that the proposed method outperforms existing adaptive test-time defenses at much lower computation costs.
</details>
<details>
<summary>摘要</summary>
使用可靠测试时防御技术提高深度神经网络对攻击性例子的Robustness。然而，现有方法会增加测试时间，因为它们需要在测试时进行额外的优化模型参数或输入。在这种工作中，我们提出了一种新的可靠测试时防御策略，可以轻松地与现有的可靠训练方法结合使用，无需额外的测试时间计算。基于我们提出的特征Robustness的概念，我们的关键思想是将训练模型投影到最Robust特征空间中，以降低不Robust方向的攻击。我们理论上显示，通过一般加法模型，特征矩阵的Top eigenvector更Robust，并且支持我们对大宽神经网络的NTK相等性。我们在CIFAR-10和CIFAR-100数据集上进行了广泛的实验，包括RobustBench状态OF艺术方法，并发现提议方法在计算成本下较低的情况下，超过现有的可靠测试时防御方法。
</details></li>
</ul>
<hr>
<h2 id="An-Efficient-Interior-Point-Method-for-Online-Convex-Optimization"><a href="#An-Efficient-Interior-Point-Method-for-Online-Convex-Optimization" class="headerlink" title="An Efficient Interior-Point Method for Online Convex Optimization"></a>An Efficient Interior-Point Method for Online Convex Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11668">http://arxiv.org/abs/2307.11668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elad Hazan, Nimrod Megiddo</li>
<li>for: 这 paper 的目的是减少在线几何优化中的遗弃。</li>
<li>methods: 该 paper 使用了一种新的 adaptive 算法，可以减少遗弃的规模。</li>
<li>results: 该 paper 证明了该算法可以在 $O(\sqrt{T \log T})$ 的时间内减少遗弃，这是最小可能的，只有一个 logarithmic 项。<details>
<summary>Abstract</summary>
A new algorithm for regret minimization in online convex optimization is described. The regret of the algorithm after $T$ time periods is $O(\sqrt{T \log T})$ - which is the minimum possible up to a logarithmic term. In addition, the new algorithm is adaptive, in the sense that the regret bounds hold not only for the time periods $1,\ldots,T$ but also for every sub-interval $s,s+1,\ldots,t$. The running time of the algorithm matches that of newly introduced interior point algorithms for regret minimization: in $n$-dimensional space, during each iteration the new algorithm essentially solves a system of linear equations of order $n$, rather than solving some constrained convex optimization problem in $n$ dimensions and possibly many constraints.
</details>
<details>
<summary>摘要</summary>
新算法可以减少 regret 在在线凸优化中。该算法在 $T$ 个时间段后的 regret 是 $O(\sqrt{T \log T})$，这是最小可能的，即使带有对数项。此外，该算法是可适应的，即在每个子时间段 $s,s+1,\ldots,t$ 中， regret 约束也成立。算法的运行时间与新引入的内部点算法一样，即在 $n$ 维空间中每次迭代时解决一个线性方程组件，而不是解决一个凸优化问题并可能有多个约束。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/22/cs.LG_2023_07_22/" data-id="cllsj9wye001tuv8870ji8ojd" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/22/eess.IV_2023_07_22/" class="article-date">
  <time datetime="2023-07-21T16:00:00.000Z" itemprop="datePublished">2023-07-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/22/eess.IV_2023_07_22/">eess.IV - 2023-07-22 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Direct-atomic-number-reconstruction-of-dual-energy-cargo-radiographs-using-a-semiempirical-transparency-model"><a href="#Direct-atomic-number-reconstruction-of-dual-energy-cargo-radiographs-using-a-semiempirical-transparency-model" class="headerlink" title="Direct atomic number reconstruction of dual energy cargo radiographs using a semiempirical transparency model"></a>Direct atomic number reconstruction of dual energy cargo radiographs using a semiempirical transparency model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12099">http://arxiv.org/abs/2307.12099</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Lalor, Areg Danagoulian</li>
<li>for: 这个论文旨在提高货物检测系统的检测能力，特别是在检测涉猛物时。</li>
<li>methods: 该论文使用了一种高精度的原子数预测方法，该方法基于测量图像透射率值的 chi-squared 误差来减少误差。</li>
<li>results: 该论文通过使用这种方法，可以在噪声图像上获得准确的材料预测结果，并且可以适应屏障物的检测。<details>
<summary>Abstract</summary>
Dual energy cargo inspection systems are sensitive to both the area density and the atomic number of an imaged container due to the Z dependence of photon attenuation. The ability to identify cargo contents by their atomic number enables improved detection capabilities of illicit materials. Existing methods typically classify materials into a few material classes using an empirical calibration step. However, such a coarse label discretization limits atomic number selectivity and can yield inaccurate results if a material is near the midpoint of two bins. This work introduces a high resolution atomic number prediction method by minimizing the chi-squared error between measured transparency values and a semiempirical transparency model. Our previous work showed that by incorporating calibration step, the semiempirical transparency model can capture second order effects such as scattering. This method is benchmarked using two simulated radiographic phantoms, demonstrating the ability to obtain accurate material predictions on noisy input images by incorporating an image segmentation step. Furthermore, we show that this approach can be adapted to identify shielded objects after first determining the properties of the shielding, taking advantage of the closed-form nature of the transparency model.
</details>
<details>
<summary>摘要</summary>
双能量货物检测系统具有区域密度和原子数的敏感性，由于光子吸收的Z依赖性。能够根据物质的原子数进行识别，可以提高损害物检测的精度。现有方法通常通过一个Empirical calibration步骤来分类材料，但这会限制原子数选择性并可能导致不准确的结果，如果材料在两个分类中的中点。这项工作介绍了一种高分辨率原子数预测方法，通过最小化χ²错误值 между测量的透射值和一种半employmaterial模型来实现。我们之前的工作表明，通过包含Calibration步骤，半employmaterial模型可以捕捉二次效应，如散射。这种方法在使用两个模拟的放射学phantom中进行了 benchmarkevaluation，表明可以在噪声输入图像上获得准确的材料预测结果，通过包含图像分割步骤。此外，我们还示出了这种方法可以适应标定障碍物，首先确定障碍物的属性，利用透射模型的关闭形式性。
</details></li>
</ul>
<hr>
<h2 id="On-the-Effectiveness-of-Spectral-Discriminators-for-Perceptual-Quality-Improvement"><a href="#On-the-Effectiveness-of-Spectral-Discriminators-for-Perceptual-Quality-Improvement" class="headerlink" title="On the Effectiveness of Spectral Discriminators for Perceptual Quality Improvement"></a>On the Effectiveness of Spectral Discriminators for Perceptual Quality Improvement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12027">http://arxiv.org/abs/2307.12027</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luciennnnnnn/dualformer">https://github.com/luciennnnnnn/dualformer</a></li>
<li>paper_authors: Xin Luo, Yunan Zhu, Shunxin Xu, Dong Liu</li>
<li>for: 这个研究旨在探讨spectral discriminator在图像生成模型中的应用，以提高SR图像质量。</li>
<li>methods: 该研究使用了spectral discriminator和ordinary discriminator，并对比了它们的效果。同时， authors提出了一种使用Transformer对spectral discriminator进行聚合的方法，以提高spectral discriminator的性能。</li>
<li>results: 研究发现，spectral discriminator在高频范围内表现更好，而ordinary discriminator在低频范围内表现更好。因此，authors建议同时使用spectral discriminator和ordinary discriminator。此外， authors verify了该方法的效果，通过PD质量评价和无参图像质量评价任务。<details>
<summary>Abstract</summary>
Several recent studies advocate the use of spectral discriminators, which evaluate the Fourier spectra of images for generative modeling. However, the effectiveness of the spectral discriminators is not well interpreted yet. We tackle this issue by examining the spectral discriminators in the context of perceptual image super-resolution (i.e., GAN-based SR), as SR image quality is susceptible to spectral changes. Our analyses reveal that the spectral discriminator indeed performs better than the ordinary (a.k.a. spatial) discriminator in identifying the differences in the high-frequency range; however, the spatial discriminator holds an advantage in the low-frequency range. Thus, we suggest that the spectral and spatial discriminators shall be used simultaneously. Moreover, we improve the spectral discriminators by first calculating the patch-wise Fourier spectrum and then aggregating the spectra by Transformer. We verify the effectiveness of the proposed method twofold. On the one hand, thanks to the additional spectral discriminator, our obtained SR images have their spectra better aligned to those of the real images, which leads to a better PD tradeoff. On the other hand, our ensembled discriminator predicts the perceptual quality more accurately, as evidenced in the no-reference image quality assessment task.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:Recent studies have proposed using spectral discriminators, which evaluate the Fourier spectra of images for generative modeling. However, the effectiveness of the spectral discriminators is not well understood. We investigate the spectral discriminators in the context of perceptual image super-resolution (i.e., GAN-based SR), as SR image quality is sensitive to spectral changes. Our analysis shows that the spectral discriminator performs better than the ordinary (a.k.a. spatial) discriminator in identifying differences in the high-frequency range, but the spatial discriminator has an advantage in the low-frequency range. Therefore, we suggest using both the spectral and spatial discriminators simultaneously. Moreover, we improve the spectral discriminators by first calculating the patch-wise Fourier spectrum and then aggregating the spectra using Transformer. We verify the effectiveness of the proposed method through twofold experiments. On the one hand, the additional spectral discriminator helps align the spectra of the obtained SR images with those of the real images, leading to a better PD tradeoff. On the other hand, our ensembled discriminator predicts perceptual quality more accurately, as shown in the no-reference image quality assessment task.
</details></li>
</ul>
<hr>
<h2 id="A-Cascade-Transformer-based-Model-for-3D-Dose-Distribution-Prediction-in-Head-and-Neck-Cancer-Radiotherapy"><a href="#A-Cascade-Transformer-based-Model-for-3D-Dose-Distribution-Prediction-in-Head-and-Neck-Cancer-Radiotherapy" class="headerlink" title="A Cascade Transformer-based Model for 3D Dose Distribution Prediction in Head and Neck Cancer Radiotherapy"></a>A Cascade Transformer-based Model for 3D Dose Distribution Prediction in Head and Neck Cancer Radiotherapy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12005">http://arxiv.org/abs/2307.12005</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ghtara/dose_prediction">https://github.com/ghtara/dose_prediction</a></li>
<li>paper_authors: Tara Gheshlaghi, Shahabedin Nabavi, Samire Shirzadikia, Mohsen Ebrahimi Moghaddam, Nima Rostampour</li>
<li>For: This paper aims to improve the accuracy and efficiency of radiation therapy planning for cancer treatment by using deep learning methods to predict dose distribution maps and segment organs at risk.* Methods: The proposed model consists of two cascade encoder-decoder networks: one for organs at risk segmentation and the other for dose distribution prediction. The segmentation network uses transformer blocks and multi-scale convolutional blocks, while the dose distribution prediction network employs a pyramid architecture.* Results: The proposed model outperformed state-of-the-art methods in terms of dose and DVH scores, especially in regions with low prescribed doses. The predicted dose maps showed good coincidence with ground truth, and the segmentation subnet achieved high Dice and HD95 scores.<details>
<summary>Abstract</summary>
Radiation therapy is the primary method used to treat cancer in the clinic. Its goal is to deliver a precise dose to the planning target volume (PTV) while protecting the surrounding organs at risk (OARs). However, the traditional workflow used by dosimetrists to plan the treatment is time-consuming and subjective, requiring iterative adjustments based on their experience. Deep learning methods can be used to predict dose distribution maps to address these limitations. The study proposes a cascade model for organs at risk segmentation and dose distribution prediction. An encoder-decoder network has been developed for the segmentation task, in which the encoder consists of transformer blocks, and the decoder uses multi-scale convolutional blocks. Another cascade encoder-decoder network has been proposed for dose distribution prediction using a pyramid architecture. The proposed model has been evaluated using an in-house head and neck cancer dataset of 96 patients and OpenKBP, a public head and neck cancer dataset of 340 patients. The segmentation subnet achieved 0.79 and 2.71 for Dice and HD95 scores, respectively. This subnet outperformed the existing baselines. The dose distribution prediction subnet outperformed the winner of the OpenKBP2020 competition with 2.77 and 1.79 for dose and DVH scores, respectively. The predicted dose maps showed good coincidence with ground truth, with a superiority after linking with the auxiliary segmentation task. The proposed model outperformed state-of-the-art methods, especially in regions with low prescribed doses.
</details>
<details>
<summary>摘要</summary>
医学中常用的放射疗法是用于治疗癌症的主要方法。其目标是尽可能准确地向规划目标体积（PTV）中注入精准的剂量，同时保护周围的有害组织（OARs）。然而，传统的规划工作流程由数 metrician 进行，是时间consuming 和主观的，需要不断的调整基于他们的经验。深度学习方法可以用来预测剂量分布图，以解决这些限制。本研究提出了顺序模型，用于组织致癌症和剂量分布预测。一个编码器-解码器网络已经为 segmentation 任务开发，其中编码器由 transformer 块组成，解码器使用多尺度 convolutional 块。另一个顺序编码器-解码器网络已经为剂量分布预测使用 pyramid 架构。提案的模型已经在自有的头颈癌 dataset 上进行了评估，包括 96 例的患者数据和 OpenKBP 公共头颈癌 dataset 上的 340 例患者数据。 segmentation 子网络在 Dice 和 HD95 分数上达到 0.79 和 2.71，分别高于现有的基线。剂量分布预测子网络在 OpenKBP2020 比赛中的赢家上达到 2.77 和 1.79，分别高于现有的基线。预测的剂量图与实际值有good coincidence，链接 auxiliary segmentation 任务后显示了superiority。提案的模型超越了当前的状态艺术方法，特别是在低剂量区域。
</details></li>
</ul>
<hr>
<h2 id="ELiOT-End-to-end-Lidar-Odometry-using-Transformer-Framework"><a href="#ELiOT-End-to-end-Lidar-Odometry-using-Transformer-Framework" class="headerlink" title="ELiOT : End-to-end Lidar Odometry using Transformer Framework"></a>ELiOT : End-to-end Lidar Odometry using Transformer Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11998">http://arxiv.org/abs/2307.11998</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daegyu Lee, Hyunwoo Nam, D. Hyunchul Shim</li>
<li>for: 本 paper 是为了提出一种基于 transformer 架构的 LiDAR 附近推测方法，用于实现精准的 LiDAR Scene 跟踪。</li>
<li>methods: 该方法使用了 Self-attention flow embedding network，通过强调 LiDAR Scene 的自动关注，避免了传统的 3D-2D 投影方法。网络架构包括 3D transformer encoder-decoder，可以准确地预测 LiDAR Scene 的 pose。</li>
<li>results: 在 urbans 数据集上，该方法得到了鼓舞人的结果，平均翻译错误率为 7.59%，旋转错误率为 2.67%。这表明该方法可以准确地跟踪 LiDAR Scene，无需使用传统的 geometric 概念。<details>
<summary>Abstract</summary>
In recent years, deep-learning-based point cloud registration methods have shown significant promise. Furthermore, learning-based 3D detectors have demonstrated their effectiveness in encoding semantic information from LiDAR data. In this paper, we introduce ELiOT, an end-to-end LiDAR odometry framework built on a transformer architecture. Our proposed Self-attention flow embedding network implicitly represents the motion of sequential LiDAR scenes, bypassing the need for 3D-2D projections traditionally used in such tasks. The network pipeline, composed of a 3D transformer encoder-decoder, has shown effectiveness in predicting poses on urban datasets. In terms of translational and rotational errors, our proposed method yields encouraging results, with 7.59% and 2.67% respectively on the KITTI odometry dataset. This is achieved with an end-to-end approach that foregoes the need for conventional geometric concepts.
</details>
<details>
<summary>摘要</summary>
近年来，深度学习基于点云注册方法已经表现出了明显的承诺。此外，学习基于3D探测器已经证明可以从激光数据中提取 semantic 信息。在这篇文章中，我们介绍了 ELiOT，一种基于 transformer 架构的端到端 LiDAR 速度框架。我们提议的 Self-attention flow embedding 网络可以避免传统的 3D-2D 投影，并将Sequential LiDAR 场景中的运动嵌入到网络中。这个网络结构由3D transformer 编码器-解码器组成，在城市数据集上显示了Predict pose 的效果。在翻译和旋转错误方面，我们的提议方法实现了鼓舞人的结果，即7.59%和2.67% 分别在 KITTI 速度数据集上。这是一种端到端的方法，不需要传统的几何学概念。
</details></li>
</ul>
<hr>
<h2 id="Topology-Preserving-Automatic-Labeling-of-Coronary-Arteries-via-Anatomy-aware-Connection-Classifier"><a href="#Topology-Preserving-Automatic-Labeling-of-Coronary-Arteries-via-Anatomy-aware-Connection-Classifier" class="headerlink" title="Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-aware Connection Classifier"></a>Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-aware Connection Classifier</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11959">http://arxiv.org/abs/2307.11959</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zutsusemi/miccai2023-topolab-labels">https://github.com/zutsusemi/miccai2023-topolab-labels</a></li>
<li>paper_authors: Zhixing Zhang, Ziwei Zhao, Dong Wang, Shishuang Zhao, Yuhang Liu, Jia Liu, Liwei Wang</li>
<li>for: 本研究旨在提高自动脉络标注的精度，以便更好地诊断心血管疾病。</li>
<li>methods: 该研究提出了一种新的TopoLab框架，其利用了脉络的解剖连接来帮助准确地标注脉络段。具体来说，研究者提出了一种层次特征汇集策略和多个脉络连接类别标注方法。</li>
<li>results: 实验结果表明，TopoLab在orCaScore数据集和一个内部数据集上都达到了领先性的表现。<details>
<summary>Abstract</summary>
Automatic labeling of coronary arteries is an essential task in the practical diagnosis process of cardiovascular diseases. For experienced radiologists, the anatomically predetermined connections are important for labeling the artery segments accurately, while this prior knowledge is barely explored in previous studies. In this paper, we present a new framework called TopoLab which incorporates the anatomical connections into the network design explicitly. Specifically, the strategies of intra-segment feature aggregation and inter-segment feature interaction are introduced for hierarchical segment feature extraction. Moreover, we propose the anatomy-aware connection classifier to enable classification for each connected segment pair, which effectively exploits the prior topology among the arteries with different categories. To validate the effectiveness of our method, we contribute high-quality annotations of artery labeling to the public orCaScore dataset. The experimental results on both the orCaScore dataset and an in-house dataset show that our TopoLab has achieved state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
自动标注 coronary artery 是cardiovascular disease 诊断过程中的一项重要任务。经验丰富的 radiologist 知道， precisely determining the anatomical connections 是标注 artery segment 的关键，而这一点在前一 studies 中几乎没有被探讨。在这篇论文中，我们提出了一种新的框架called TopoLab，它Explicitly incorporates the anatomical connections into the network design. Specifically, we introduce the strategies of intra-segment feature aggregation and inter-segment feature interaction for hierarchical segment feature extraction. 更over, we propose the anatomy-aware connection classifier to enable classification for each connected segment pair, which effectively exploits the prior topology among the arteries with different categories. To validate the effectiveness of our method, we contribute high-quality annotations of artery labeling to the public orCaScore dataset. The experimental results on both the orCaScore dataset and an in-house dataset show that our TopoLab has achieved state-of-the-art performance.
</details></li>
</ul>
<hr>
<h2 id="PartDiff-Image-Super-resolution-with-Partial-Diffusion-Models"><a href="#PartDiff-Image-Super-resolution-with-Partial-Diffusion-Models" class="headerlink" title="PartDiff: Image Super-resolution with Partial Diffusion Models"></a>PartDiff: Image Super-resolution with Partial Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11926">http://arxiv.org/abs/2307.11926</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Zhao, Alex Ling Yu Hung, Kaifeng Pang, Haoxin Zheng, Kyunghyun Sung</li>
<li>for: 本研究旨在提高 diffusion-based generative models 的计算效率，以便在各种图像生成任务中使用。</li>
<li>methods: 作者提出了 Partial Diffusion Model (PartDiff)，它在 diffusion 过程中直接往 intermediate latent state diffuse，而不是往 pure random noise Diffusion。在生成过程中，PartDiff 只执行部分的 denoising 步骤。此外，作者还引入了 “latent alignment”，以协调 low-resolution 和高resolution 图像的 latent 空间。</li>
<li>results: 对于 MRI 和自然图像，PartDiff 比 plain diffusion-based super-resolution methods 减少了 denoising 步骤数量，同时不 sacrificing 生成质量。<details>
<summary>Abstract</summary>
Denoising diffusion probabilistic models (DDPMs) have achieved impressive performance on various image generation tasks, including image super-resolution. By learning to reverse the process of gradually diffusing the data distribution into Gaussian noise, DDPMs generate new data by iteratively denoising from random noise. Despite their impressive performance, diffusion-based generative models suffer from high computational costs due to the large number of denoising steps.In this paper, we first observed that the intermediate latent states gradually converge and become indistinguishable when diffusing a pair of low- and high-resolution images. This observation inspired us to propose the Partial Diffusion Model (PartDiff), which diffuses the image to an intermediate latent state instead of pure random noise, where the intermediate latent state is approximated by the latent of diffusing the low-resolution image. During generation, Partial Diffusion Models start denoising from the intermediate distribution and perform only a part of the denoising steps. Additionally, to mitigate the error caused by the approximation, we introduce "latent alignment", which aligns the latent between low- and high-resolution images during training. Experiments on both magnetic resonance imaging (MRI) and natural images show that, compared to plain diffusion-based super-resolution methods, Partial Diffusion Models significantly reduce the number of denoising steps without sacrificing the quality of generation.
</details>
<details>
<summary>摘要</summary>
diffusion probabilistic models (DDPMs) 有 achieved impressive performance 在 various image generation tasks, including image super-resolution. By learning to reverse the process of gradually diffusing the data distribution into Gaussian noise, DDPMs generate new data by iteratively denoising from random noise. Despite their impressive performance, diffusion-based generative models suffer from high computational costs due to the large number of denoising steps.在这篇论文中，我们首先注意到，在 diffusing 一对 low-resolution 和 high-resolution 图像的 pairs, 中间的 latent states 逐渐凝固并变得不可分辨。这一观察点我们 inspirited 我们提出 Partial Diffusion Model (PartDiff), which diffuses the image to an intermediate latent state instead of pure random noise, where the intermediate latent state is approximated by the latent of diffusing the low-resolution image. During generation, Partial Diffusion Models start denoising from the intermediate distribution and perform only a part of the denoising steps.此外，为了缓解由 approximation 所引起的错误，我们引入 "latent alignment", 在训练中对 low-resolution 和 high-resolution 图像的 latent 进行 align. Experiments on both magnetic resonance imaging (MRI) 和 natural images show that, compared to plain diffusion-based super-resolution methods, Partial Diffusion Models significantly reduce the number of denoising steps without sacrificing the quality of generation.
</details></li>
</ul>
<hr>
<h2 id="Conditional-Temporal-Attention-Networks-for-Neonatal-Cortical-Surface-Reconstruction"><a href="#Conditional-Temporal-Attention-Networks-for-Neonatal-Cortical-Surface-Reconstruction" class="headerlink" title="Conditional Temporal Attention Networks for Neonatal Cortical Surface Reconstruction"></a>Conditional Temporal Attention Networks for Neonatal Cortical Surface Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11870">http://arxiv.org/abs/2307.11870</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/m-qiang/cotan">https://github.com/m-qiang/cotan</a></li>
<li>paper_authors: Qiang Ma, Liu Li, Vanessa Kyriakopoulou, Joseph Hajnal, Emma C. Robinson, Bernhard Kainz, Daniel Rueckert</li>
<li>for: 这个论文的目的是为了提出一种高速、端到端的 Cortical surface reconstruction 方法，以模型新生儿脑发育的快速进程。</li>
<li>methods: 这个方法使用 Conditional Temporal Attention Network (CoTAN)，一种能够预测多分辨率 stationary velocity fields (SVF) 的端到端框架。CoTAN 通过计算每个 SVF 的重要性，并通过学习的注意力地図来学习一个 conditional time-varying velocity field (CTVF)。</li>
<li>results: CoTAN 可以减少 mesh 自交错错误，并且只需 0.21 秒可以将初始模板 mesh 扭曲到 cortical white matter 和 pial surfaces 上。与现有基线相比，CoTAN 可以达到优秀的性能，具有仅 0.12mm 的几何错误和 0.07% 的自交错错误。<details>
<summary>Abstract</summary>
Cortical surface reconstruction plays a fundamental role in modeling the rapid brain development during the perinatal period. In this work, we propose Conditional Temporal Attention Network (CoTAN), a fast end-to-end framework for diffeomorphic neonatal cortical surface reconstruction. CoTAN predicts multi-resolution stationary velocity fields (SVF) from neonatal brain magnetic resonance images (MRI). Instead of integrating multiple SVFs, CoTAN introduces attention mechanisms to learn a conditional time-varying velocity field (CTVF) by computing the weighted sum of all SVFs at each integration step. The importance of each SVF, which is estimated by learned attention maps, is conditioned on the age of the neonates and varies with the time step of integration. The proposed CTVF defines a diffeomorphic surface deformation, which reduces mesh self-intersection errors effectively. It only requires 0.21 seconds to deform an initial template mesh to cortical white matter and pial surfaces for each brain hemisphere. CoTAN is validated on the Developing Human Connectome Project (dHCP) dataset with 877 3D brain MR images acquired from preterm and term born neonates. Compared to state-of-the-art baselines, CoTAN achieves superior performance with only 0.12mm geometric error and 0.07% self-intersecting faces. The visualization of our attention maps illustrates that CoTAN indeed learns coarse-to-fine surface deformations automatically without intermediate supervision.
</details>
<details>
<summary>摘要</summary>
cortical surface reconstruction 在模型新生儿大脑发育的快速进程中扮演着基本的角色。在这项工作中，我们提议了 Conditional Temporal Attention Network（CoTAN），一种快速、端到端的杜尼诺瓦尔扩散表面重建框架。CoTAN 预测了多resolution stationary velocity field（SVF），从新生儿大脑磁共振成像图像（MRI）中预测。而不是将多个 SVF  интегра，CoTAN 引入了注意力机制，以学习一个 conditioned time-varying velocity field（CTVF），通过在每个集成步骤中计算所有 SVF 的权重的和。每个 SVF 的重要性，由学习的注意力地图所评估，与新生儿的年龄相关，随着集成步骤的变化而变化。提议的 CTVF 定义了一个 diffeomorphic surface deformation，可以有效减少缓冲自交错错误。它只需0.21秒可以将初始模板网格变换成 cortical white matter 和 pial surface  для每个脑半球。CoTAN 在 dHCP 数据集上进行了验证，与州流行基eline 相比，CoTAN 具有较好的性能，只有0.12mm的准确性和0.07%的自交错错误。我们的注意力地图可视化显示，CoTAN 实际上自动学习了粗细到细节的表面变换，无需中间监督。
</details></li>
</ul>
<hr>
<h2 id="Digital-Modeling-on-Large-Kernel-Metamaterial-Neural-Network"><a href="#Digital-Modeling-on-Large-Kernel-Metamaterial-Neural-Network" class="headerlink" title="Digital Modeling on Large Kernel Metamaterial Neural Network"></a>Digital Modeling on Large Kernel Metamaterial Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11862">http://arxiv.org/abs/2307.11862</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quan Liu, Hanyu Zheng, Brandon T. Swartz, Ho hin Lee, Zuhayr Asad, Ivan Kravchenko, Jason G. Valentine, Yuankai Huo</li>
<li>for: 这篇论文的目的是提出一种基于光学计算单元的大型kernels neural network（LMNN），以最大化现代meta-optic neural network（MNN）的数字能力，同时考虑光学限制。</li>
<li>methods: 该论文使用了模型重新参数化和网络压缩技术，以maximize the learning capacity of MNN while modeling the physical restrictions of meta-optic。</li>
<li>results: 实验结果表明，提出的LMNN可以提高分类精度，同时降低计算延迟。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) utilized recently are physically deployed with computational units (e.g., CPUs and GPUs). Such a design might lead to a heavy computational burden, significant latency, and intensive power consumption, which are critical limitations in applications such as the Internet of Things (IoT), edge computing, and the usage of drones. Recent advances in optical computational units (e.g., metamaterial) have shed light on energy-free and light-speed neural networks. However, the digital design of the metamaterial neural network (MNN) is fundamentally limited by its physical limitations, such as precision, noise, and bandwidth during fabrication. Moreover, the unique advantages of MNN's (e.g., light-speed computation) are not fully explored via standard 3x3 convolution kernels. In this paper, we propose a novel large kernel metamaterial neural network (LMNN) that maximizes the digital capacity of the state-of-the-art (SOTA) MNN with model re-parametrization and network compression, while also considering the optical limitation explicitly. The new digital learning scheme can maximize the learning capacity of MNN while modeling the physical restrictions of meta-optic. With the proposed LMNN, the computation cost of the convolutional front-end can be offloaded into fabricated optical hardware. The experimental results on two publicly available datasets demonstrate that the optimized hybrid design improved classification accuracy while reducing computational latency. The development of the proposed LMNN is a promising step towards the ultimate goal of energy-free and light-speed AI.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）最近都是通过计算单元（例如CPU和GPU）进行物理部署。这种设计可能会导致重大的计算占用资源，显著的延迟和吃力的电源消耗，这些限制在互联网关系（IoT）、边缘计算和无人机应用中是关键的。最近，光学计算单元（例如元material）的进步带来了无源电能和光速神经网络。然而，光学设计的物理限制（例如精度、雷达和带宽）会限制MNN的数字设计。此外，MNN的独特优势（例如光速计算）没有通过标准3x3卷积核被完全探索。在本文中，我们提出了一种新的大kernel元material神经网络（LMNN），该方法可以最大化MNN的数字能力，同时考虑光学限制。新的数字学习方案可以在MNN中最大化学习能力，同时模拟元optic的物理限制。通过我们的提议的LMNN，计算前端的计算成本可以被卷积到制造过的光学硬件上。实验结果表明，通过优化hybrid设计，在两个公共可用的数据集上提高分类精度，降低计算延迟。开发LMNN是实现无源电能和光速AI的前景之一。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Hyperspectral-Pansharpening-on-large-scale-PRISMA-dataset"><a href="#Deep-Learning-Hyperspectral-Pansharpening-on-large-scale-PRISMA-dataset" class="headerlink" title="Deep Learning Hyperspectral Pansharpening on large scale PRISMA dataset"></a>Deep Learning Hyperspectral Pansharpening on large scale PRISMA dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11666">http://arxiv.org/abs/2307.11666</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simone Zini, Mirko Paolo Barbato, Flavio Piccoli, Paolo Napoletano</li>
<li>for: 这个研究旨在评估多种深度学习策略对高spectral排比进行抗锈。</li>
<li>methods: 这个研究使用了多种现有的深度学习方法，并将其适应PRISMA高spectral数据集。</li>
<li>results: 研究发现，使用深度学习方法可以在高spectral排比 task 中表现更好，并且在 Reduced Resolution 和 Full Resolution 两种情况下都表现出色。<details>
<summary>Abstract</summary>
In this work, we assess several deep learning strategies for hyperspectral pansharpening. First, we present a new dataset with a greater extent than any other in the state of the art. This dataset, collected using the ASI PRISMA satellite, covers about 262200 km2, and its heterogeneity is granted by randomly sampling the Earth's soil. Second, we adapted several state of the art approaches based on deep learning to fit PRISMA hyperspectral data and then assessed, quantitatively and qualitatively, the performance in this new scenario. The investigation has included two settings: Reduced Resolution (RR) to evaluate the techniques in a supervised environment and Full Resolution (FR) for a real-world evaluation. The main purpose is the evaluation of the reconstruction fidelity of the considered methods. In both scenarios, for the sake of completeness, we also included machine-learning-free approaches. From this extensive analysis has emerged that data-driven neural network methods outperform machine-learning-free approaches and adapt better to the task of hyperspectral pansharpening, both in RR and FR protocols.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们评估了数字深度学习策略对光谱扫描图像进行缩进。首先，我们提供了一个新的数据集，其覆盖率比现有的状态 искусственный卫星PRISMA的数据集更大，约262200 km2。这个数据集通过随机采样地球的土壤来保证其多样性。然后，我们适应了一些现有的深度学习方法，以适应PRISMA的光谱数据。我们Then quantitatively and qualitatively evaluated the performance of these methods in two settings: Reduced Resolution (RR) and Full Resolution (FR).我们的主要目标是评估这些方法的重建准确性。在这种情况下，为了完整性，我们还包括了不含机器学习的方法。从这项广泛的分析中，我们发现 dass data-driven neural network methods outperform machine-learning-free approaches and adapt better to the task of hyperspectral pansharpening, both in RR and FR protocols.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/22/eess.IV_2023_07_22/" data-id="cllsj9x060084uv88cqpe2dl5" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/21/cs.LG_2023_07_21/" class="article-date">
  <time datetime="2023-07-20T16:00:00.000Z" itemprop="datePublished">2023-07-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/21/cs.LG_2023_07_21/">cs.LG - 2023-07-21 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Enhancing-CLIP-with-GPT-4-Harnessing-Visual-Descriptions-as-Prompts"><a href="#Enhancing-CLIP-with-GPT-4-Harnessing-Visual-Descriptions-as-Prompts" class="headerlink" title="Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts"></a>Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11661">http://arxiv.org/abs/2307.11661</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mayug/vdt-adapter">https://github.com/mayug/vdt-adapter</a></li>
<li>paper_authors: Mayug Maniparambil, Chris Vorster, Derek Molloy, Noel Murphy, Kevin McGuinness, Noel E. O’Connor</li>
<li>for: 这篇论文主要是为了提高CLIP的下游任务表现，特别是在特定的精细任务上。</li>
<li>methods: 这篇论文使用了GPT-4作为生成模型，通过设计可视描述文本来适应CLIP。</li>
<li>results:  compared to CLIP的默认提问，这篇论文的方法可以提高0 shot转移精度（EuroSAT、DTD、SUN397和CUB等精细任务的改进率都超过4%），并且设计了一种简单的几架adapter，可以选择最佳的句子来构建通用的分类器，超过了CoCoOP的平均提高率（精细任务上的提高率超过4%）。<details>
<summary>Abstract</summary>
Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have revolutionized visual representation learning by providing good performance on downstream datasets. VLMs are 0-shot adapted to a downstream dataset by designing prompts that are relevant to the dataset. Such prompt engineering makes use of domain expertise and a validation dataset. Meanwhile, recent developments in generative pretrained models like GPT-4 mean they can be used as advanced internet search tools. They can also be manipulated to provide visual information in any structure. In this work, we show that GPT-4 can be used to generate text that is visually descriptive and how this can be used to adapt CLIP to downstream tasks. We show considerable improvements in 0-shot transfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD (~7%), SUN397 (~4.6%), and CUB (~3.3%) when compared to CLIP's default prompt. We also design a simple few-shot adapter that learns to choose the best possible sentences to construct generalizable classifiers that outperform the recently proposed CoCoOP by ~2% on average and by over 4% on 4 specialized fine-grained datasets. The code, prompts, and auxiliary text dataset is available at https://github.com/mayug/VDT-Adapter.
</details>
<details>
<summary>摘要</summary>
带有对比性预训的大规模视力语言模型（VLM），如CLIP，已经革命化视觉表示学习。VLM可以通过设计相关的提示来适应下游数据集，这种提示工程利用域专业知识和验证数据集。同时，最近的生成预训模型如GPT-4可以用作高级网络搜索工具，也可以被修改以提供任意结构的视觉信息。在这个工作中，我们展示了GPT-4可以生成可见描述性文本，并使用这种文本来适应CLIP下游任务。我们发现，在特殊精细数据集上（如EuroSAT、DTD、SUN397和CUB），使用GPT-4生成的提示可以提高0-shot传输精度（大约7%），比CLIP的默认提示更高。此外，我们设计了一个简单的几招适应器，可以选择最佳的句子来构建通用分类器，超过CoCoOP的最新提posed的4%的平均提高和4个特殊精细数据集的4%的提高。代码、提示和辅助文本数据集可以在https://github.com/mayug/VDT-Adapter中下载。
</details></li>
</ul>
<hr>
<h2 id="Bandits-with-Deterministically-Evolving-States"><a href="#Bandits-with-Deterministically-Evolving-States" class="headerlink" title="Bandits with Deterministically Evolving States"></a>Bandits with Deterministically Evolving States</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11655">http://arxiv.org/abs/2307.11655</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khashayar Khosravi, Renato Paes Leme, Chara Podimata, Apostolis Tsorvantzis</li>
<li>for: 本文提出了一种学习带有抽象状态的带刺抽象bandit模型，用于推荐系统和在线广告等应用。</li>
<li>methods: 本文使用了多臂抽象bandit算法，并分析了线上学习算法的 regret 率。</li>
<li>results: 本文提出的 regret 率为：对于 $\lambda \in [0, 1&#x2F;T^2]$， regret 率为 $\widetilde O(\sqrt{KT})$；对于 $\lambda &#x3D; T^{-a&#x2F;b}$ with $b &lt; a &lt; 2b$， regret 率为 $\widetilde O (T^{b&#x2F;a})$；对于 $\lambda \in (1&#x2F;T, 1 - 1&#x2F;\sqrt{T}]$， regret 率为 $\widetilde O (K^{1&#x2F;3}T^{2&#x2F;3})$；对于 $\lambda \in [1 - 1&#x2F;\sqrt{T}, 1]$， regret 率为 $\widetilde O (K\sqrt{T})$。<details>
<summary>Abstract</summary>
We propose a model for learning with bandit feedback while accounting for deterministically evolving and unobservable states that we call Bandits with Deterministically Evolving States. The workhorse applications of our model are learning for recommendation systems and learning for online ads. In both cases, the reward that the algorithm obtains at each round is a function of the short-term reward of the action chosen and how ``healthy'' the system is (i.e., as measured by its state). For example, in recommendation systems, the reward that the platform obtains from a user's engagement with a particular type of content depends not only on the inherent features of the specific content, but also on how the user's preferences have evolved as a result of interacting with other types of content on the platform. Our general model accounts for the different rate $\lambda \in [0,1]$ at which the state evolves (e.g., how fast a user's preferences shift as a result of previous content consumption) and encompasses standard multi-armed bandits as a special case. The goal of the algorithm is to minimize a notion of regret against the best-fixed sequence of arms pulled. We analyze online learning algorithms for any possible parametrization of the evolution rate $\lambda$. Specifically, the regret rates obtained are: for $\lambda \in [0, 1/T^2]$: $\widetilde O(\sqrt{KT})$; for $\lambda = T^{-a/b}$ with $b < a < 2b$: $\widetilde O (T^{b/a})$; for $\lambda \in (1/T, 1 - 1/\sqrt{T}): \widetilde O (K^{1/3}T^{2/3})$; and for $\lambda \in [1 - 1/\sqrt{T}, 1]: \widetilde O (K\sqrt{T})$.
</details>
<details>
<summary>摘要</summary>
我们提出了一种学习模型，称为带有束定态态域的强化学习（Bandits with Deterministically Evolving States，简称为BDES）。这种模型的应用场景包括推荐系统和在线广告学习。在这两种情况下，算法在每个轮次获得的奖励是函数的短期奖励和系统的状态（例如，用户在平台上消费的内容的吸引力）。我们的通用模型考虑了不同的演化速率 $\lambda \in [0,1]$，并包括标准多重武器的特例。算法的目标是对于任何可能的参数化 $\lambda$ 来减少对最佳固定武器拔取序列的 regret。我们分析了在线学习算法，并取得了不同的 regret 率：* $\lambda \in [0, 1/T^2]$：$\widetilde O(\sqrt{KT})$* $\lambda = T^{-a/b}$ with $b < a < 2b$：$\widetilde O (T^{b/a})$* $\lambda \in (1/T, 1 - 1/\sqrt{T})$：$\widetilde O (K^{1/3}T^{2/3})$* $\lambda \in [1 - 1/\sqrt{T}, 1]$：$\widetilde O (K\sqrt{T})$
</details></li>
</ul>
<hr>
<h2 id="Scalable-Multi-agent-Covering-Option-Discovery-based-on-Kronecker-Graphs"><a href="#Scalable-Multi-agent-Covering-Option-Discovery-based-on-Kronecker-Graphs" class="headerlink" title="Scalable Multi-agent Covering Option Discovery based on Kronecker Graphs"></a>Scalable Multi-agent Covering Option Discovery based on Kronecker Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11629">http://arxiv.org/abs/2307.11629</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayu Chen, Jingdi Chen, Tian Lan, Vaneet Aggarwal<br>for: 多智能体RL探索技术methods: 基于Kronecker图的Fiedler矩阵估计，以及基于神经网络的表示学习技术来预测多智能体技能results: 在Mujoco simulate器上进行的评估表明，提出的算法可以成功地发现多智能体技能，并与当前最佳性能做比较。<details>
<summary>Abstract</summary>
Covering skill (a.k.a., option) discovery has been developed to improve the exploration of RL in single-agent scenarios with sparse reward signals, through connecting the most distant states in the embedding space provided by the Fiedler vector of the state transition graph. Given that joint state space grows exponentially with the number of agents in multi-agent systems, existing researches still relying on single-agent skill discovery either become prohibitive or fail to directly discover joint skills that improve the connectivity of the joint state space. In this paper, we propose multi-agent skill discovery which enables the ease of decomposition. Our key idea is to approximate the joint state space as a Kronecker graph, based on which we can directly estimate its Fiedler vector using the Laplacian spectrum of individual agents' transition graphs. Further, considering that directly computing the Laplacian spectrum is intractable for tasks with infinite-scale state spaces, we further propose a deep learning extension of our method by estimating eigenfunctions through NN-based representation learning techniques. The evaluation on multi-agent tasks built with simulators like Mujoco, shows that the proposed algorithm can successfully identify multi-agent skills, and significantly outperforms the state-of-the-art. Codes are available at: https://github.itap.purdue.edu/Clan-labs/Scalable_MAOD_via_KP.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate_language: zh-CNCovering skill (即选项) 发现技术已经开发来改进单机RL中的探索，通过连接状态转移图中的最远两点来提高embedding空间中的连接性。由于多机系统中的联合状态空间随着参与者数量的增加而呈指数增长，现有的研究仍然采用单机技术来探索单机技能，这会变得不可持或直接探索多机技能来提高联合状态空间的连接性。在这篇论文中，我们提出了多机技能发现技术，允许拆分。我们的关键思想是将联合状态空间 aproximated 为Kronecker图，基于这个图可以直接估计其Fiedler вектор使用个体参与者的转移图laplacian спектル。此外，由于直接计算laplacian спектル是对任务 state space 的无限规模 зада务中不可能实现，我们进一步提出了基于深度学习技术的扩展方法，通过NN-based representation learning技术来估计eigenfunctions。在使用Mujoco等模拟器建立的多机任务上，我们的算法能够成功 Identify multi-agent skills, 并显著超越当前的状态。代码可以在以下地址获取：https://github.itap.purdue.edu/Clan-labs/Scalable_MAOD_via_KP。
</details></li>
</ul>
<hr>
<h2 id="Offline-Multi-Agent-Reinforcement-Learning-with-Implicit-Global-to-Local-Value-Regularization"><a href="#Offline-Multi-Agent-Reinforcement-Learning-with-Implicit-Global-to-Local-Value-Regularization" class="headerlink" title="Offline Multi-Agent Reinforcement Learning with Implicit Global-to-Local Value Regularization"></a>Offline Multi-Agent Reinforcement Learning with Implicit Global-to-Local Value Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11620">http://arxiv.org/abs/2307.11620</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangsen Wang, Haoran Xu, Yinan Zheng, Xianyuan Zhan</li>
<li>for: 本研究旨在解决多智能体RL中的离线学习问题，提出了一种基于全局值规范的离线多智能体RL算法（OMIGA），可以很好地处理大型多智能体状态空间的问题。</li>
<li>methods: OMIGA使用了离线数据相关的正则化，将全局值规范转化为等价的本地值规范，并同时实现在样本中学习，从而协调多智能体的价值评估和策略学习。</li>
<li>results: 根据在多智能体MuJoCo和StarCraft II微管理任务上的实验结果，OMIGA在大多数任务中表现出了较好的性能，超过了现有的离线多智能体RL方法。<details>
<summary>Abstract</summary>
Offline reinforcement learning (RL) has received considerable attention in recent years due to its attractive capability of learning policies from offline datasets without environmental interactions. Despite some success in the single-agent setting, offline multi-agent RL (MARL) remains to be a challenge. The large joint state-action space and the coupled multi-agent behaviors pose extra complexities for offline policy optimization. Most existing offline MARL studies simply apply offline data-related regularizations on individual agents, without fully considering the multi-agent system at the global level. In this work, we present OMIGA, a new offline m ulti-agent RL algorithm with implicit global-to-local v alue regularization. OMIGA provides a principled framework to convert global-level value regularization into equivalent implicit local value regularizations and simultaneously enables in-sample learning, thus elegantly bridging multi-agent value decomposition and policy learning with offline regularizations. Based on comprehensive experiments on the offline multi-agent MuJoCo and StarCraft II micro-management tasks, we show that OMIGA achieves superior performance over the state-of-the-art offline MARL methods in almost all tasks.
</details>
<details>
<summary>摘要</summary>
请求：将给定文本翻译成简化中文。</sys>响应：近年来，无线强化学习（RL）在无需环境互动的情况下学习策略得到了广泛关注。虽然在单机器情况下有一定成功，但多机器人RL（MARL）在线上仍然是一项挑战。因为多机器人系统的共同状态-动作空间和相互关联的行为增加了多机器人RL的复杂性。大多数现有的多机器人RL研究只是对单机器人的数据相关正则化应用了限制，没有充分考虑多机器人系统的全局水平。在这项工作中，我们提出了OMIGA，一种新的无线多机器人RL算法，其中包含了隐式全局值正则化。OMIGA可以将全局水平的值正则化转化为等价的隐式本地值正则化，并同时允许在样本中学习，因此简洁地结合多机器人值分解和策略学习，并且使用无线正则化。基于多机器人 MuJoCo 和 StarCraft II 微管理任务的广泛实验，我们表明OMIGA可以在大多数任务中超越现有的无线多机器人RL方法的性能。
</details></li>
</ul>
<hr>
<h2 id="Robust-Fully-Asynchronous-Methods-for-Distributed-Training-over-General-Architecture"><a href="#Robust-Fully-Asynchronous-Methods-for-Distributed-Training-over-General-Architecture" class="headerlink" title="Robust Fully-Asynchronous Methods for Distributed Training over General Architecture"></a>Robust Fully-Asynchronous Methods for Distributed Training over General Architecture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11617">http://arxiv.org/abs/2307.11617</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zehan Zhu, Ye Tian, Yan Huang, Jinming Xu, Shibo He</li>
<li>for: 这个论文旨在解决分布式机器学习中的同步问题，因为存在延迟、数据丢失和慢节点等问题，同步方法成本过高。</li>
<li>methods: 该论文提出了一种Robust Fully-Asynchronous Stochastic Gradient Tracking方法（R-FAST），每个设备都可以按照自己的节奏进行本地计算和通信，不需要任何同步。这种方法可以消除设备间数据不同的问题，并且能够承受数据丢失，通过设计合适的辅助变量来跟踪和缓存总导数向量。</li>
<li>results: 该论文表明，R-FAST方法在平均情况下可以在拥有轮廓树的情况下 converge to a neighborhood of the optimum with a geometric rate for smooth and strongly convex objectives; and to a stationary point with a sublinear rate for general non-convex settings. 实验表明，R-FAST方法比同步标准算法（如环形AllReduce和D-PSGD）快1.5-2倍，同时仍然保持相同的准确率，并且在存在慢节点时比存在 asynchronous SOTA algorithms（如AD-PSGD和OSGP）更高效。<details>
<summary>Abstract</summary>
Perfect synchronization in distributed machine learning problems is inefficient and even impossible due to the existence of latency, package losses and stragglers. We propose a Robust Fully-Asynchronous Stochastic Gradient Tracking method (R-FAST), where each device performs local computation and communication at its own pace without any form of synchronization. Different from existing asynchronous distributed algorithms, R-FAST can eliminate the impact of data heterogeneity across devices and allow for packet losses by employing a robust gradient tracking strategy that relies on properly designed auxiliary variables for tracking and buffering the overall gradient vector. More importantly, the proposed method utilizes two spanning-tree graphs for communication so long as both share at least one common root, enabling flexible designs in communication architectures. We show that R-FAST converges in expectation to a neighborhood of the optimum with a geometric rate for smooth and strongly convex objectives; and to a stationary point with a sublinear rate for general non-convex settings. Extensive experiments demonstrate that R-FAST runs 1.5-2 times faster than synchronous benchmark algorithms, such as Ring-AllReduce and D-PSGD, while still achieving comparable accuracy, and outperforms existing asynchronous SOTA algorithms, such as AD-PSGD and OSGP, especially in the presence of stragglers.
</details>
<details>
<summary>摘要</summary>
完美同步在分布式机器学习问题上是不可避免的和甚至是不可能的，因为存在延迟、包裹丢失和偏移。我们提出了一种稳健全 asynchronous stochastic gradient tracking方法（R-FAST），其中每个设备在自己的 tempo 上进行本地计算和通信，不需要任何同步。与现有的异步分布式算法不同，R-FAST可以消除设备间数据不一致的影响，并且在使用适当设计的辅助变量时，可以承受包裹丢失。此外，我们使用两个拓扑图进行通信，只要这两个拓扑图至少有一个共同的根，就可以实现flexible的通信架构。我们证明R-FAST在满足某些条件下 converges in expectation to a neighborhood of the optimum with a geometric rate for smooth and strongly convex objectives; and to a stationary point with a sublinear rate for general non-convex settings。实验表明R-FAST比同步标准算法（如环形AllReduce和D-PSGD）快1.5-2倍，同时保持相同的准确率，并且超过现有的异步SOTA算法（如AD-PSGD和OSGP），特别是在存在偏移者时。
</details></li>
</ul>
<hr>
<h2 id="Persistent-Ballistic-Entanglement-Spreading-with-Optimal-Control-in-Quantum-Spin-Chains"><a href="#Persistent-Ballistic-Entanglement-Spreading-with-Optimal-Control-in-Quantum-Spin-Chains" class="headerlink" title="Persistent Ballistic Entanglement Spreading with Optimal Control in Quantum Spin Chains"></a>Persistent Ballistic Entanglement Spreading with Optimal Control in Quantum Spin Chains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11609">http://arxiv.org/abs/2307.11609</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ying Lu, Pei Shi, Xiao-Han Wang, Jie Hu, Shi-Ju Ran</li>
<li>for: 本研究探讨了量子多体系统在和稳定态下的动力学性质。</li>
<li>methods: 本文使用了“可变性增强”频率场（VEEF）来诱导量子磁链上的固有强隐式扩散。</li>
<li>results: 研究发现，在使用VEEF时，量子磁链上的两个体系的互相关联度（EE）会在时间方向上 linear 增长，直到EE reaches 真实极限。此外，EE 的增长速率与交互强度之间存在关系。<details>
<summary>Abstract</summary>
Entanglement propagation provides a key routine to understand quantum many-body dynamics in and out of equilibrium. In this work, we uncover that the ``variational entanglement-enhancing'' field (VEEF) robustly induces a persistent ballistic spreading of entanglement in quantum spin chains. The VEEF is time dependent, and is optimally controlled to maximize the bipartite entanglement entropy (EE) of the final state. Such a linear growth persists till the EE reaches the genuine saturation $\tilde{S} = - \log_{2} 2^{-\frac{N}{2}}=\frac{N}{2}$ with $N$ the total number of spins. The EE satisfies $S(t) = v t$ for the time $t \leq \frac{N}{2v}$, with $v$ the velocity. These results are in sharp contrast with the behaviors without VEEF, where the EE generally approaches a sub-saturation known as the Page value $\tilde{S}_{P} =\tilde{S} - \frac{1}{2\ln{2}}$ in the long-time limit, and the entanglement growth deviates from being linear before the Page value is reached. The dependence between the velocity and interactions is explored, with $v \simeq 2.76$, $4.98$, and $5.75$ for the spin chains with Ising, XY, and Heisenberg interactions, respectively. We further show that the nonlinear growth of EE emerges with the presence of long-range interactions.
</details>
<details>
<summary>摘要</summary>
Entanglement 传播提供了一个关键的 Routine 来理解量子多体动态在和稳态下的行为。在这个工作中，我们发现了“可变式强联结”（VEEF）可以强制量子萤幕中的潜在强联结持续传播。VEEF 是时间相依的，并可以最佳化以 Maximize 统计量子萤幕的对称强联结 entropy（EE）。这个线性增长持续到EE 到达真正的极限 $\tilde{S} = - \log_{2} 2^{-\frac{N}{2}}=\frac{N}{2}$ ，其中 $N$ 是总共有几个萤幕。EE 满足 $S(t) = v t$ ，其中 $v$ 是速度。这些结果与未使用 VEEF 的情况不同，其中 EE 通常在长时间 limit 下接近一个低于极限的对称强联结值 $\tilde{S}_{P} =\tilde{S} - \frac{1}{2\ln{2}}$，并且强联结增长不同于 linear 前 Page value 是 reached。我们还考虑了互动与速度之间的相互关联，其中 $v \simeq 2.76$, $4.98$, 和 $5.75$ 分别为 Ising、XY 和 Heisenberg 互动的萤幕中的速度。我们进一步显示了对萤幕的长距离互动对对称强联结的发展具有影响。
</details></li>
</ul>
<hr>
<h2 id="Learning-minimal-representations-of-stochastic-processes-with-variational-autoencoders"><a href="#Learning-minimal-representations-of-stochastic-processes-with-variational-autoencoders" class="headerlink" title="Learning minimal representations of stochastic processes with variational autoencoders"></a>Learning minimal representations of stochastic processes with variational autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11608">http://arxiv.org/abs/2307.11608</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gabrielfernandezfernandez/spivae">https://github.com/gabrielfernandezfernandez/spivae</a></li>
<li>paper_authors: Gabriel Fernández-Fernández, Carlo Manzo, Maciej Lewenstein, Alexandre Dauphin, Gorka Muñoz-Gil</li>
<li>For: The paper is written for understanding and modeling stochastic processes, which are widely used in various fields to describe natural phenomena with randomness and uncertainty.* Methods: The paper introduces an unsupervised machine learning approach based on an extended $\beta$-variational autoencoder architecture to determine the minimal set of parameters required to effectively describe the dynamics of a stochastic process.* Results: The method is shown to be effective in extracting the minimal relevant parameters that accurately describe the dynamics of paradigmatic diffusion models, and can also generate new trajectories that faithfully replicate the expected stochastic behavior.<details>
<summary>Abstract</summary>
Stochastic processes have found numerous applications in science, as they are broadly used to model a variety of natural phenomena. Due to their intrinsic randomness and uncertainty, they are however difficult to characterize. Here, we introduce an unsupervised machine learning approach to determine the minimal set of parameters required to effectively describe the dynamics of a stochastic process. Our method builds upon an extended $\beta$-variational autoencoder architecture. By means of simulated datasets corresponding to paradigmatic diffusion models, we showcase its effectiveness in extracting the minimal relevant parameters that accurately describe these dynamics. Furthermore, the method enables the generation of new trajectories that faithfully replicate the expected stochastic behavior. Overall, our approach enables for the autonomous discovery of unknown parameters describing stochastic processes, hence enhancing our comprehension of complex phenomena across various fields.
</details>
<details>
<summary>摘要</summary>
Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. The translation is based on the given text and may not capture all the nuances and subtleties of the original text.
</details></li>
</ul>
<hr>
<h2 id="Finding-Optimal-Diverse-Feature-Sets-with-Alternative-Feature-Selection"><a href="#Finding-Optimal-Diverse-Feature-Sets-with-Alternative-Feature-Selection" class="headerlink" title="Finding Optimal Diverse Feature Sets with Alternative Feature Selection"></a>Finding Optimal Diverse Feature Sets with Alternative Feature Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11607">http://arxiv.org/abs/2307.11607</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jakob-bach/alternative-feature-selection">https://github.com/jakob-bach/alternative-feature-selection</a></li>
<li>paper_authors: Jakob Bach</li>
<li>for: 本文提出了一种新的特征选择方法，可以为用户提供多个可解释的预测模型。</li>
<li>methods: 本文使用了约束来定义备用特征集，并允许用户控制备用集的数量和不同程度。</li>
<li>results: 在30种分类 dataset 上测试了该方法，发现备用特征集可以具有高预测质量，并分析了一些影响这个结果的因素。<details>
<summary>Abstract</summary>
Feature selection is popular for obtaining small, interpretable, yet highly accurate prediction models. Conventional feature-selection methods typically yield one feature set only, which might not suffice in some scenarios. For example, users might be interested in finding alternative feature sets with similar prediction quality, offering different explanations of the data. In this article, we introduce alternative feature selection and formalize it as an optimization problem. In particular, we define alternatives via constraints and enable users to control the number and dissimilarity of alternatives. Next, we analyze the complexity of this optimization problem and show NP-hardness. Further, we discuss how to integrate conventional feature-selection methods as objectives. Finally, we evaluate alternative feature selection with 30 classification datasets. We observe that alternative feature sets may indeed have high prediction quality, and we analyze several factors influencing this outcome.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Feature selection is popular for obtaining small, interpretable, yet highly accurate prediction models. Conventional feature-selection methods typically yield one feature set only, which might not suffice in some scenarios. For example, users might be interested in finding alternative feature sets with similar prediction quality, offering different explanations of the data. In this article, we introduce alternative feature selection and formalize it as an optimization problem. In particular, we define alternatives via constraints and enable users to control the number and dissimilarity of alternatives. Next, we analyze the complexity of this optimization problem and show NP-hardness. Further, we discuss how to integrate conventional feature-selection methods as objectives. Finally, we evaluate alternative feature selection with 30 classification datasets. We observe that alternative feature sets may indeed have high prediction quality, and we analyze several factors influencing this outcome." into 简化中文。Here's the translation:feature 选择受欢迎，因为它可以获得小、可读性高、准确预测模型。传统的特征选择方法通常只会生成一个特征集，这可能不足以在某些情况下。例如，用户可能感兴趣找到不同的特征集，它们具有相似的预测质量，但是对数据的解释不同。在这篇文章中，我们介绍了备用特征选择，并将其формализова为优化问题。我们通过约束定义备用特征，并让用户控制备用特征的数量和不同程度。然后，我们分析了这个优化问题的复杂性，并证明其NP困难。此外，我们讨论了如何集成传统的特征选择方法为目标。最后，我们对30个分类 dataset进行了评估，发现备用特征集可能indeed具有高预测质量，并分析了一些影响这种结果的因素。
</details></li>
</ul>
<hr>
<h2 id="Transferability-of-Convolutional-Neural-Networks-in-Stationary-Learning-Tasks"><a href="#Transferability-of-Convolutional-Neural-Networks-in-Stationary-Learning-Tasks" class="headerlink" title="Transferability of Convolutional Neural Networks in Stationary Learning Tasks"></a>Transferability of Convolutional Neural Networks in Stationary Learning Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11588">http://arxiv.org/abs/2307.11588</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/damowerko/mtt">https://github.com/damowerko/mtt</a></li>
<li>paper_authors: Damian Owerko, Charilaos I. Kanatsoulis, Jennifer Bondarchuk, Donald J. Bucci Jr, Alejandro Ribeiro</li>
<li>For: This paper is written for researchers and practitioners working on deep learning techniques, particularly in the context of large-scale spatial problems such as multi-target tracking and mobile infrastructure on demand.* Methods: The paper introduces a novel framework for efficient training of convolutional neural networks (CNNs) for large-scale spatial problems, which involves investigating the properties of CNNs for tasks where the underlying signals are stationary. The authors also provide a theoretical analysis that provides a bound on the performance degradation.* Results: The authors conduct thorough experimental analysis on two tasks and show that the CNN is able to tackle problems with many hundreds of agents after being trained with fewer than ten. The results demonstrate that CNN architectures provide solutions to these problems at previously computationally intractable scales.Here is the same information in Simplified Chinese text:* For: 这篇论文是为了研究深度学习技术，特别是在大规模空间问题上的应用，如多标目追踪和移动基础设施按需。* Methods: 论文引入了一种新的深度学习框架，用于高效地训练卷积神经网络（CNN），以解决大规模空间问题。作者们还提供了一种理论分析，以确定性能下降的上限。* Results: 作者们在两个任务上进行了广泛的实验分析，并显示了CNN可以在很多Agent后被训练的情况下，解决多达百计的问题。这些结果表明，CNN架构可以在前些计算上不可能的规模上提供解决方案。<details>
<summary>Abstract</summary>
Recent advances in hardware and big data acquisition have accelerated the development of deep learning techniques. For an extended period of time, increasing the model complexity has led to performance improvements for various tasks. However, this trend is becoming unsustainable and there is a need for alternative, computationally lighter methods. In this paper, we introduce a novel framework for efficient training of convolutional neural networks (CNNs) for large-scale spatial problems. To accomplish this we investigate the properties of CNNs for tasks where the underlying signals are stationary. We show that a CNN trained on small windows of such signals achieves a nearly performance on much larger windows without retraining. This claim is supported by our theoretical analysis, which provides a bound on the performance degradation. Additionally, we conduct thorough experimental analysis on two tasks: multi-target tracking and mobile infrastructure on demand. Our results show that the CNN is able to tackle problems with many hundreds of agents after being trained with fewer than ten. Thus, CNN architectures provide solutions to these problems at previously computationally intractable scales.
</details>
<details>
<summary>摘要</summary>
Note:* "硬件" (hùnéng) means "hardware" in Chinese.* "大数据" (dàxù) means "big data" in Chinese.* " convolutional neural networks" (CNNs) is translated as "卷积神经网络" (juéshòu xīnǎo wǎngwǎng) in Chinese.* "stationary" (stationary) is translated as "站立" (zhànzhí) in Chinese.* "multi-target tracking" is translated as "多目标跟踪" (duōmùzhì gēngcháng) in Chinese.* "mobile infrastructure on demand" is translated as "需要的移动基础设施" (xūyào de qiàngdòng jībèi shèshi) in Chinese.
</details></li>
</ul>
<hr>
<h2 id="A-Change-of-Heart-Improving-Speech-Emotion-Recognition-through-Speech-to-Text-Modality-Conversion"><a href="#A-Change-of-Heart-Improving-Speech-Emotion-Recognition-through-Speech-to-Text-Modality-Conversion" class="headerlink" title="A Change of Heart: Improving Speech Emotion Recognition through Speech-to-Text Modality Conversion"></a>A Change of Heart: Improving Speech Emotion Recognition through Speech-to-Text Modality Conversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11584">http://arxiv.org/abs/2307.11584</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iclr2023achangeofheart/meld-modality-conversion">https://github.com/iclr2023achangeofheart/meld-modality-conversion</a></li>
<li>paper_authors: Zeinab Sadat Taghavi, Ali Satvaty, Hossein Sameti</li>
<li>for: 本研究旨在提高MELD数据集上的情感识别性能。</li>
<li>methods: 本研究提出了一种模态转换概念，通过自动语音识别（ASR）系统和文本分类器进行实现。</li>
<li>results: 研究发现，首先使用ASR系统和文本分类器进行模态转换后，可以获得显著的提高；而在假设ASR输出为完美的情况下，进一步研究模态转换对情感识别的影响，结果显示模态转换++方法在MELD数据集上的情感识别WF1分数中超过了当前speech模式下的最佳方法。<details>
<summary>Abstract</summary>
Speech Emotion Recognition (SER) is a challenging task. In this paper, we introduce a modality conversion concept aimed at enhancing emotion recognition performance on the MELD dataset. We assess our approach through two experiments: first, a method named Modality-Conversion that employs automatic speech recognition (ASR) systems, followed by a text classifier; second, we assume perfect ASR output and investigate the impact of modality conversion on SER, this method is called Modality-Conversion++. Our findings indicate that the first method yields substantial results, while the second method outperforms state-of-the-art (SOTA) speech-based approaches in terms of SER weighted-F1 (WF1) score on the MELD dataset. This research highlights the potential of modality conversion for tasks that can be conducted in alternative modalities.
</details>
<details>
<summary>摘要</summary>
干预 Emotion Recognition (ER) 是一项复杂的任务。在这篇论文中，我们提出了一种模态转换概念，以提高 MELD 数据集 上 ER 性能。我们通过两个实验评估了我们的方法：首先，我们使用自动语音识别 (ASR) 系统，然后使用文本分类器；第二，我们假设 ASR 输出为完美，并研究模态转换对 ER 的影响，这个方法被称为 Modality-Conversion++。我们的发现表明第一种方法具有显著的效果，而 Modality-Conversion++ 方法在 MELD 数据集上的 ER 权重 F1 分数（WF1）上超越了现有的speech-based方法。这种研究强调了模态转换的潜在价值，对于可以在不同模态上进行的任务。
</details></li>
</ul>
<hr>
<h2 id="Design-Space-Exploration-on-Efficient-and-Accurate-Human-Pose-Estimation-from-Sparse-IMU-Sensing"><a href="#Design-Space-Exploration-on-Efficient-and-Accurate-Human-Pose-Estimation-from-Sparse-IMU-Sensing" class="headerlink" title="Design Space Exploration on Efficient and Accurate Human Pose Estimation from Sparse IMU-Sensing"></a>Design Space Exploration on Efficient and Accurate Human Pose Estimation from Sparse IMU-Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02397">http://arxiv.org/abs/2308.02397</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/itiv-kit/dse-sparse-imu">https://github.com/itiv-kit/dse-sparse-imu</a></li>
<li>paper_authors: Iris Fürst-Walter, Antonio Nappi, Tanja Harbaum, Jürgen Becker<br>for:本研究旨在提供一种准确地评估人体运动的方法，而不需要折衣人体隐私数据。因此，本研究使用陀螺仪测量单元（IMU）进行本地处理，而不是常见的摄像头捕获。methods:本研究使用 simulate 的设计空间探索（DSE）工具，对不同的陀螺仪数量和位置进行评估。首先，我们从公开 disponibles 的人体模型数据集中生成 IMU 数据，并使用深度学习模型进行训练。此外，我们提出了一种联合指标，以评估精度和资源之间的质量负担。results:根据 DSE 的评估结果，我们得出了一种优化的陀螺仪配置，其中使用 4 个陀螺仪， mesh 误差为 6.03 cm，相比之前的状态法具有32.7% 的提高和两个陀螺仪的减少硬件努力。这种配置可以用于设计健康应用程序，并且注意到数据隐私和资源感知。<details>
<summary>Abstract</summary>
Human Pose Estimation (HPE) to assess human motion in sports, rehabilitation or work safety requires accurate sensing without compromising the sensitive underlying personal data. Therefore, local processing is necessary and the limited energy budget in such systems can be addressed by Inertial Measurement Units (IMU) instead of common camera sensing. The central trade-off between accuracy and efficient use of hardware resources is rarely discussed in research. We address this trade-off by a simulative Design Space Exploration (DSE) of a varying quantity and positioning of IMU-sensors. First, we generate IMU-data from a publicly available body model dataset for different sensor configurations and train a deep learning model with this data. Additionally, we propose a combined metric to assess the accuracy-resource trade-off. We used the DSE as a tool to evaluate sensor configurations and identify beneficial ones for a specific use case. Exemplary, for a system with equal importance of accuracy and resources, we identify an optimal sensor configuration of 4 sensors with a mesh error of 6.03 cm, increasing the accuracy by 32.7% and reducing the hardware effort by two sensors compared to state of the art. Our work can be used to design health applications with well-suited sensor positioning and attention to data privacy and resource-awareness.
</details>
<details>
<summary>摘要</summary>
人体姿势估计（HPE）在运动、rehabilitation或工作安全领域需要准确探测人体运动，而不需要侵犯人体敏感的个人数据。因此，本地处理是必要的，而且由于限制的能源预算，相比于常见的摄像头感知，使用各种测量单元（IMU）更加合适。我们在研究中 rarely discussed的中心权衡是 между准确和硬件资源的有效使用。我们通过 simulate Design Space Exploration（DSE）来解决这个权衡。首先，我们从公共可用的人体模型数据集中生成IMU数据，并用这些数据训练深度学习模型。此外，我们提出了一种组合度量来评估准确性和硬件资源之间的权衡。我们使用DSE作为工具来评估感知器配置，并为特定应用场景标识有利的感知器配置。例如，对于需要准确性和硬件资源均匀的系统，我们Identify an optimal sensor configuration of 4 sensors with a mesh error of 6.03 cm, which increases accuracy by 32.7% and reduces hardware effort by two sensors compared to the state of the art.我们的工作可以用来设计健康应用程序，并遵循数据隐私和硬件资源的注意事项。
</details></li>
</ul>
<hr>
<h2 id="FMT-Removing-Backdoor-Feature-Maps-via-Feature-Map-Testing-in-Deep-Neural-Networks"><a href="#FMT-Removing-Backdoor-Feature-Maps-via-Feature-Map-Testing-in-Deep-Neural-Networks" class="headerlink" title="FMT: Removing Backdoor Feature Maps via Feature Map Testing in Deep Neural Networks"></a>FMT: Removing Backdoor Feature Maps via Feature Map Testing in Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11565">http://arxiv.org/abs/2307.11565</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ase2023paper/fmt">https://github.com/ase2023paper/fmt</a></li>
<li>paper_authors: Dong Huang, Qingwen Bu, Yahao Qing, Yichao Fu, Heming Cui</li>
<li>for: 防范深度神经网络中的后门攻击</li>
<li>methods: Feature Map Testing（FMT），检测和移除后门特征图</li>
<li>results: 相比现有防御策略，FMT可以更好地降低后门攻击成功率（ASR），并保持模型性能高，进一步提高模型的安全性。<details>
<summary>Abstract</summary>
Deep neural networks have been widely used in many critical applications, such as autonomous vehicles and medical diagnosis. However, their security is threatened by backdoor attack, which is achieved by adding artificial patterns to specific training data. Existing defense strategies primarily focus on using reverse engineering to reproduce the backdoor trigger generated by attackers and subsequently repair the DNN model by adding the trigger into inputs and fine-tuning the model with ground-truth labels. However, once the trigger generated by the attackers is complex and invisible, the defender can not successfully reproduce the trigger. Consequently, the DNN model will not be repaired since the trigger is not effectively removed.   In this work, we propose Feature Map Testing~(FMT). Different from existing defense strategies, which focus on reproducing backdoor triggers, FMT tries to detect the backdoor feature maps, which are trained to extract backdoor information from the inputs. After detecting these backdoor feature maps, FMT will erase them and then fine-tune the model with a secure subset of training data. Our experiments demonstrate that, compared to existing defense strategies, FMT can effectively reduce the Attack Success Rate (ASR) even against the most complex and invisible attack triggers. Second, unlike conventional defense methods that tend to exhibit low Robust Accuracy (i.e., the model's accuracy on the poisoned data), FMT achieves higher RA, indicating its superiority in maintaining model performance while mitigating the effects of backdoor attacks~(e.g., FMT obtains 87.40\% RA in CIFAR10). Third, compared to existing feature map pruning techniques, FMT can cover more backdoor feature maps~(e.g., FMT removes 83.33\% of backdoor feature maps from the model in the CIFAR10 \& BadNet scenario).
</details>
<details>
<summary>摘要</summary>
深度神经网络在许多关键应用中广泛使用，如自动驾驶和医疗诊断。然而，它们的安全性受到后门攻击的威胁，后门攻击者通过添加到特定训练数据中的人工模式来实现。现有的防御策略主要集中在使用反工程来复制攻击者生成的后门触发器，并在使用真实标签来修复DNN模型。然而，如果攻击者生成的后门触发器复杂且不可见，则防御者无法成功复制它。因此，DNN模型不会被修复，因为触发器不能有效地除除。在这种情况下，我们提出了特征地图测试（Feature Map Testing，FMT）。与现有防御策略不同，FMT不是 Trying to reproduce backdoor triggers，而是寻找被训练以提取后门信息的特征地图。一旦检测到这些后门特征地图，FMT会将它们删除，然后使用安全的训练数据来练化模型。我们的实验表明，相比现有防御策略，FMT可以更有效地减少攻击成功率（ASR），即使攻击者生成的触发器非常复杂且不可见。其次，与传统防御方法一样，FMT可以保持模型的性能，而不是降低模型的准确率（例如，FMT在CIFAR10中获得87.40%的特性精度）。最后，相比现有的特征地图剔除技术，FMT可以覆盖更多的后门特征地图（例如，FMT在CIFAR10 & BadNet场景中删除了83.33%的后门特征地图）。
</details></li>
</ul>
<hr>
<h2 id="A-multi-modal-representation-of-El-Nino-Southern-Oscillation-Diversity"><a href="#A-multi-modal-representation-of-El-Nino-Southern-Oscillation-Diversity" class="headerlink" title="A multi-modal representation of El Niño Southern Oscillation Diversity"></a>A multi-modal representation of El Niño Southern Oscillation Diversity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11552">http://arxiv.org/abs/2307.11552</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jakob-schloer/latentgmm">https://github.com/jakob-schloer/latentgmm</a></li>
<li>paper_authors: Jakob Schlör, Felix Strnad, Antonietta Capotondi, Bedartha Goswami</li>
<li>For: 本研究用来描述赤道太平洋海面温度偏差（SSTA）的多样性，并提出了一种新的分类方法来描述El Niño-南方抗抗温层（ENSO）事件的多样性。* Methods: 本研究使用低维度的太平洋海面温度表示方法，使用不精确的 clustering 方法来描述ENSO事件的多样性。* Results: 研究发现，ENSO事件不仅有 longitudinal 位置的差异，还有其他特征之间的差异。五种已知的ENSO类别中，有一个新的类别：极端的El Niño。此外，CP La Niña、EP El Niño和极端El Niño在interdecadal ENSO variability 中发挥了重要作用。<details>
<summary>Abstract</summary>
The El Ni\~no-Southern Oscillation (ENSO) is characterized by alternating periods of warm (El Ni\~no) and cold (La Ni\~na) sea surface temperature anomalies (SSTA) in the equatorial Pacific. Although El Ni\~no and La Ni\~na are well-defined climate patterns, no two events are alike. To date, ENSO diversity has been described primarily in terms of the longitudinal location of peak SSTA, used to define a bimodal classification of events in Eastern Pacific (EP) and Central Pacific (CP) types. Here, we use low-dimensional representations of Pacific SSTAs to argue that binary categorical memberships are unsuitable to describe ENSO events. Using fuzzy unsupervised clustering, we recover the four known ENSO categories, along with a fifth category: an Extreme El Ni\~no. We show that Extreme El Ni\~nos differ both in their intensity and temporal evolution from canonical EP El Ni\~nos. We also find that CP La Ni\~nas, EP El Ni\~nos, and Extreme El Ni\~nos contribute the most to interdecadal ENSO variability.
</details>
<details>
<summary>摘要</summary>
“El Niño-南方气压 осцилляции（ENSO）是指在赤道太平洋的海水温度异常（SSTA） periodic alternate between暖（El Niño）和冷（La Niña）。虽然El Niño和La Niña是明确定义的气候模式，但每个事件都不同。到目前为止，ENSO多样性主要是通过 longitudinal 位置的最高SSTA值来定义为东部太平洋（EP）和中部太平洋（CP）类型进行描述。本文使用低维度表示太平洋SSTAs， argue that binary categorical memberships are unsuitable to describe ENSO events。使用不确定无监督聚类，我们回归了四种已知ENSO类别，以及一种极端El Niño。我们发现极端El Niños在其强度和时间演化方面与常规EP El Niños有所不同。我们还发现CP La Niñas、EP El Niños和极端El Niños在Interdecadal ENSO变化中发挥了最大作用。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and widely used in other countries as well. The Traditional Chinese form of the text would be slightly different.
</details></li>
</ul>
<hr>
<h2 id="Towards-practical-reinforcement-learning-for-tokamak-magnetic-control"><a href="#Towards-practical-reinforcement-learning-for-tokamak-magnetic-control" class="headerlink" title="Towards practical reinforcement learning for tokamak magnetic control"></a>Towards practical reinforcement learning for tokamak magnetic control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11546">http://arxiv.org/abs/2307.11546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brendan D. Tracey, Andrea Michi, Yuri Chervonyi, Ian Davies, Cosmin Paduraru, Nevena Lazic, Federico Felici, Timo Ewalds, Craig Donner, Cristian Galperti, Jonas Buchli, Michael Neunert, Andrea Huber, Jonathan Evens, Paula Kurylowicz, Daniel J. Mankowitz, Martin Riedmiller, The TCV Team</li>
<li>for: 这个研究旨在解决实时控制系统中的激增学习（RL）方法的缺陷，包括磁激态控制领域中的磁激态稳定和稳定性。</li>
<li>methods: 本研究使用RL方法，并提出了几个算法改进以提高控制精度、缓速错误和学习新任务的时间。</li>
<li>results:  simulations 显示，该方法可以提高形状精度达65%，降低长期电流偏差，并缩短学习新任务所需的时间。新的TCV Tokamak实验也验证了这些结果，并预示了在RL方法下可以 Routinely  Achieve 精度的燃料装载。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) has shown promising results for real-time control systems, including the domain of plasma magnetic control. However, there are still significant drawbacks compared to traditional feedback control approaches for magnetic confinement. In this work, we address key drawbacks of the RL method; achieving higher control accuracy for desired plasma properties, reducing the steady-state error, and decreasing the required time to learn new tasks. We build on top of \cite{degrave2022magnetic}, and present algorithmic improvements to the agent architecture and training procedure. We present simulation results that show up to 65\% improvement in shape accuracy, achieve substantial reduction in the long-term bias of the plasma current, and additionally reduce the training time required to learn new tasks by a factor of 3 or more. We present new experiments using the upgraded RL-based controllers on the TCV tokamak, which validate the simulation results achieved, and point the way towards routinely achieving accurate discharges using the RL approach.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Training-Latency-Minimization-for-Model-Splitting-Allowed-Federated-Edge-Learning"><a href="#Training-Latency-Minimization-for-Model-Splitting-Allowed-Federated-Edge-Learning" class="headerlink" title="Training Latency Minimization for Model-Splitting Allowed Federated Edge Learning"></a>Training Latency Minimization for Model-Splitting Allowed Federated Edge Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11532">http://arxiv.org/abs/2307.11532</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Wen, Guopeng Zhang, Kezhi Wang, Kun Yang</li>
<li>for: 提高深度学习模型训练时的计算能力，使用边缘计算和分解学习来降低训练延迟。</li>
<li>methods: 提出了一种基于模型分解的允许 federated learning（SFL）框架，使用了拟合分层和计算资源分配来解决训练延迟最小化问题。</li>
<li>results: 通过对efficientnetv2模型和mnist数据集进行广泛的实验，证明了提案的SFL框架可以提高训练效率和精度。<details>
<summary>Abstract</summary>
To alleviate the shortage of computing power faced by clients in training deep neural networks (DNNs) using federated learning (FL), we leverage the edge computing and split learning to propose a model-splitting allowed FL (SFL) framework, with the aim to minimize the training latency without loss of test accuracy. Under the synchronized global update setting, the latency to complete a round of global training is determined by the maximum latency for the clients to complete a local training session. Therefore, the training latency minimization problem (TLMP) is modelled as a minimizing-maximum problem. To solve this mixed integer nonlinear programming problem, we first propose a regression method to fit the quantitative-relationship between the cut-layer and other parameters of an AI-model, and thus, transform the TLMP into a continuous problem. Considering that the two subproblems involved in the TLMP, namely, the cut-layer selection problem for the clients and the computing resource allocation problem for the parameter-server are relative independence, an alternate-optimization-based algorithm with polynomial time complexity is developed to obtain a high-quality solution to the TLMP. Extensive experiments are performed on a popular DNN-model EfficientNetV2 using dataset MNIST, and the results verify the validity and improved performance of the proposed SFL framework.
</details>
<details>
<summary>摘要</summary>
To solve this mixed integer nonlinear programming problem, we first propose a regression method to fit the quantitative relationship between the cut-layer and other parameters of an AI model, transforming the TLMP into a continuous problem. Considering that the two subproblems involved in the TLMP, namely, the cut-layer selection problem for clients and the computing resource allocation problem for the parameter server, are relatively independent, we develop an alternate optimization-based algorithm with polynomial time complexity to obtain a high-quality solution to the TLMP.Extensive experiments are performed on a popular DNN model EfficientNetV2 using the MNIST dataset, and the results verify the validity and improved performance of the proposed SFL framework.
</details></li>
</ul>
<hr>
<h2 id="General-regularization-in-covariate-shift-adaptation"><a href="#General-regularization-in-covariate-shift-adaptation" class="headerlink" title="General regularization in covariate shift adaptation"></a>General regularization in covariate shift adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11503">http://arxiv.org/abs/2307.11503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duc Hoan Nguyen, Sergei V. Pereverzyev, Werner Zellinger</li>
<li>for: 这篇论文主要是为了解决最小二乘学习算法在 reprojection kernel 空间（RKHS）中出现的错误，即未来数据分布与训练数据分布不同导致的错误。</li>
<li>methods: 该论文使用了一种广泛使用的重要方法，即使用样本权重来修正最小二乘学习算法的错误。在实际情况下，样本权重是根据估计的卷积-尼科德梯度的值来确定的。</li>
<li>results: 该论文通过对重新权重 kernel 回归在 RKHS 中的错误约束来证明，在弱细分条件下，需要的样本数量比标准监督学习无论数据分布差异而言较少。<details>
<summary>Abstract</summary>
Sample reweighting is one of the most widely used methods for correcting the error of least squares learning algorithms in reproducing kernel Hilbert spaces (RKHS), that is caused by future data distributions that are different from the training data distribution. In practical situations, the sample weights are determined by values of the estimated Radon-Nikod\'ym derivative, of the future data distribution w.r.t.~the training data distribution. In this work, we review known error bounds for reweighted kernel regression in RKHS and obtain, by combination, novel results. We show under weak smoothness conditions, that the amount of samples, needed to achieve the same order of accuracy as in the standard supervised learning without differences in data distributions, is smaller than proven by state-of-the-art analyses.
</details>
<details>
<summary>摘要</summary>
样本重量是 reproduce kernel Hilbert space（RKHS）中最常用的方法之一，用于纠正最小二乘学习算法的错误，即未来数据分布与训练数据分布不同导致的错误。在实际应用中，样本重量通常由估计的Radon-Nikodym derivate的值决定，这个 derivate是未来数据分布对训练数据分布的射影。在这项工作中，我们回顾了已知的重量化kernel regression的错误上限，并通过组合，获得了新的结果。我们在弱约束条件下表明，需要的样本数量，以达到标准监督学习无论数据分布差异的同等精度水平，比现有分析所证明的要少。
</details></li>
</ul>
<hr>
<h2 id="Predict-Refine-Synthesize-Self-Guiding-Diffusion-Models-for-Probabilistic-Time-Series-Forecasting"><a href="#Predict-Refine-Synthesize-Self-Guiding-Diffusion-Models-for-Probabilistic-Time-Series-Forecasting" class="headerlink" title="Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting"></a>Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11494">http://arxiv.org/abs/2307.11494</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcel Kollovieh, Abdul Fatir Ansari, Michael Bohlke-Schneider, Jasper Zschiegner, Hao Wang, Yuyang Wang</li>
<li>For: 本研究探讨了无条件时间序列扩散模型在不同应用领域的潜在应用前提下。* Methods: 我们提出了一种无需辅助网络或改变训练过程的自我引导机制，以便在推理时对下游任务进行条件化。* Results: 我们在三个不同的时间序列任务上进行了实验，并证明了我们的方法可以与多种任务特定的条件预测方法竞争，同时也可以在下游预测器训练中提高预测性能。<details>
<summary>Abstract</summary>
Diffusion models have achieved state-of-the-art performance in generative modeling tasks across various domains. Prior works on time series diffusion models have primarily focused on developing conditional models tailored to specific forecasting or imputation tasks. In this work, we explore the potential of task-agnostic, unconditional diffusion models for several time series applications. We propose TSDiff, an unconditionally trained diffusion model for time series. Our proposed self-guidance mechanism enables conditioning TSDiff for downstream tasks during inference, without requiring auxiliary networks or altering the training procedure. We demonstrate the effectiveness of our method on three different time series tasks: forecasting, refinement, and synthetic data generation. First, we show that TSDiff is competitive with several task-specific conditional forecasting methods (predict). Second, we leverage the learned implicit probability density of TSDiff to iteratively refine the predictions of base forecasters with reduced computational overhead over reverse diffusion (refine). Notably, the generative performance of the model remains intact -- downstream forecasters trained on synthetic samples from TSDiff outperform forecasters that are trained on samples from other state-of-the-art generative time series models, occasionally even outperforming models trained on real data (synthesize).
</details>
<details>
<summary>摘要</summary>
Diffusion models 已经在不同领域的生成模型任务中实现了 estado del arte 的性能。先前的时间序列扩散模型研究主要集中在发展特定预测或填充任务的条件模型上。在这种工作中，我们探索了无条件的时间序列扩散模型在多个应用领域的潜在优势。我们提出了 TSDiff，一个未经条件训练的时间序列扩散模型。我们的提议的自适应机制使得 TSDiff 在推理过程中可以conditioning 到下游任务，不需要附加网络或改变训练过程。我们在三个不同的时间序列任务上展示了 TSDiff 的效果：预测、修正和生成数据。首先，我们表明了 TSDiff 与多种任务特定的conditional预测方法相比，具有竞争力。其次，我们利用 TSDiff 学习的隐式概率密度来Iteratively 修正基础预测器的预测结果，降低计算开销，超过反扩散（修正）。值得一提的是，生成模型的性能具有坚实性——在 TSDiff 生成的synthetic 样本上训练的下游预测器， occasional 会超过基于其他现代生成时间序列模型的预测器，甚至 occasional 会超过基于实际数据的预测器。
</details></li>
</ul>
<hr>
<h2 id="A-New-Deep-State-Space-Analysis-Framework-for-Patient-Latent-State-Estimation-and-Classification-from-EHR-Time-Series-Data"><a href="#A-New-Deep-State-Space-Analysis-Framework-for-Patient-Latent-State-Estimation-and-Classification-from-EHR-Time-Series-Data" class="headerlink" title="A New Deep State-Space Analysis Framework for Patient Latent State Estimation and Classification from EHR Time Series Data"></a>A New Deep State-Space Analysis Framework for Patient Latent State Estimation and Classification from EHR Time Series Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11487">http://arxiv.org/abs/2307.11487</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aya Nakamura, Ryosuke Kojima, Yuji Okamoto, Eiichiro Uchino, Yohei Mineharu, Yohei Harada, Mayumi Kamada, Manabu Muto, Motoko Yanagita, Yasushi Okuno</li>
<li>for: 这种研究旨在使用电子健康记录（EHRs）中的机器学习和人工智能技术，为抗癌和chronic condition的长期治疗提供更好的方法。</li>
<li>methods: 这种研究使用了时间序列无监督学习，将EHRs中的时间序列数据模型化为深度状态空间模型，从而实现了对患者内部状态的可解释和临床解释的学习、可见化和归类。</li>
<li>results: 研究使用了12,695名癌症患者的时间序列实验室数据，成功地找到了与诊断相关的潜在状态，并通过可视化和归类分析，揭示了患者的时间序列变化特征和测试项的相关性。这种框架超过了现有方法，可以帮助我们更好地从EHRs中理解疾病进程，并且可以用于调整治疗和诊断决策。<details>
<summary>Abstract</summary>
Many diseases, including cancer and chronic conditions, require extended treatment periods and long-term strategies. Machine learning and AI research focusing on electronic health records (EHRs) have emerged to address this need. Effective treatment strategies involve more than capturing sequential changes in patient test values. It requires an explainable and clinically interpretable model by capturing the patient's internal state over time.   In this study, we propose the "deep state-space analysis framework," using time-series unsupervised learning of EHRs with a deep state-space model. This framework enables learning, visualizing, and clustering of temporal changes in patient latent states related to disease progression.   We evaluated our framework using time-series laboratory data from 12,695 cancer patients. By estimating latent states, we successfully discover latent states related to prognosis. By visualization and cluster analysis, the temporal transition of patient status and test items during state transitions characteristic of each anticancer drug were identified. Our framework surpasses existing methods in capturing interpretable latent space. It can be expected to enhance our comprehension of disease progression from EHRs, aiding treatment adjustments and prognostic determinations.
</details>
<details>
<summary>摘要</summary>
许多疾病，包括癌症和慢性疾病，需要长期的治疗时间和战略。机器学习和人工智能研究专注于电子医疗记录（EHRs）以解决这个需求。有效的治疗策略需要更多的than just 捕捉病人测试值的时间序列变化。它需要一个可解释的和临床可解释的模型，可以捕捉病人的内部状态的变化逐渐。在这项研究中，我们提出了“深度状态空间分析框架”，使用时间序列无监督学习EHRs中的深度状态空间模型。这个框架允许学习、可见化和归类时间序列中病人潜在状态的变化。我们对12,695名癌症患者的时间序列实验室数据进行评估。通过估计潜在状态，我们成功地发现了与诊断相关的潜在状态。通过视觉和归类分析，我们可以看到病人状态和测试项在状态转移特征性的抗癌药物时的时间变化特征。我们的框架比现有方法更好地捕捉可解释的潜在空间，可以预期帮助我们从EHRs中更好地理解疾病的发展，并且可以帮助调整治疗和诊断决策。
</details></li>
</ul>
<hr>
<h2 id="A-Deep-Learning-Approach-for-Overall-Survival-Prediction-in-Lung-Cancer-with-Missing-Values"><a href="#A-Deep-Learning-Approach-for-Overall-Survival-Prediction-in-Lung-Cancer-with-Missing-Values" class="headerlink" title="A Deep Learning Approach for Overall Survival Prediction in Lung Cancer with Missing Values"></a>A Deep Learning Approach for Overall Survival Prediction in Lung Cancer with Missing Values</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11465">http://arxiv.org/abs/2307.11465</a></li>
<li>repo_url: None</li>
<li>paper_authors: Camillo Maria Caruso, Valerio Guarrasi, Sara Ramella, Paolo Soda</li>
<li>for: 本研究的目的是提出一种能够有效利用每个病人的信息，包括both censored（存活）和uncensored（死亡）病人，同时考虑事件的时间信息的AI模型，以提高Non-small cell lung cancer（NSCLC）患者的生存率。</li>
<li>methods: 本研究使用了变换器建模，不需要任何填充策略，直接使用可用的特征进行学习，并使用特定的损失函数来考虑both censored和uncensored病人，以及时间上的变化。</li>
<li>results: 相比之前的模型，本研究的模型在6年的评估期间，使用不同的时间粒度（1个月、1年和2年），得到了Ct-index的评估值为71.97、77.58和80.72，分别超过了所有state-of-the-art方法，无论使用哪种填充策略。<details>
<summary>Abstract</summary>
One of the most challenging fields where Artificial Intelligence (AI) can be applied is lung cancer research, specifically non-small cell lung cancer (NSCLC). In particular, overall survival (OS), the time between diagnosis and death, is a vital indicator of patient status, enabling tailored treatment and improved OS rates. In this analysis, there are two challenges to take into account. First, few studies effectively exploit the information available from each patient, leveraging both uncensored (i.e., dead) and censored (i.e., survivors) patients, considering also the events' time. Second, the handling of incomplete data is a common issue in the medical field. This problem is typically tackled through the use of imputation methods. Our objective is to present an AI model able to overcome these limits, effectively learning from both censored and uncensored patients and their available features, for the prediction of OS for NSCLC patients. We present a novel approach to survival analysis with missing values in the context of NSCLC, which exploits the strengths of the transformer architecture to account only for available features without requiring any imputation strategy. By making use of ad-hoc losses for OS, it is able to account for both censored and uncensored patients, as well as changes in risks over time. We compared our method with state-of-the-art models for survival analysis coupled with different imputation strategies. We evaluated the results obtained over a period of 6 years using different time granularities obtaining a Ct-index, a time-dependent variant of the C-index, of 71.97, 77.58 and 80.72 for time units of 1 month, 1 year and 2 years, respectively, outperforming all state-of-the-art methods regardless of the imputation method used.
</details>
<details>
<summary>摘要</summary>
一个非常挑战性的领域是论lung cancer研究，尤其是非小细胞肺癌（NSCLC）。在这种分析中，有两个挑战需要考虑。首先，很少的研究能充分利用每个病人提供的信息，挖掘bothuncensored（即死亡）和censored（即存活）病人，同时考虑事件的时间。其次，医疗领域中的数据缺失是一个常见的问题。通常通过使用插值方法来解决这个问题。我们的目标是开发一个能够超越这些限制的人工智能模型，可以从bothuncensored和censored病人中学习可用的特征，预测NSCLC患者的生存时间。我们提出了一种新的生存分析方法，利用变换器架构，不需要任何插值策略，只需考虑可用的特征。通过使用特殊的损失函数来考虑bothuncensored和censored病人，以及时间变化的风险。我们与现有的生存分析模型进行比较，并使用不同的插值策略。我们在6年的时间内对不同的时间粒度进行评估，得到了71.97、77.58和80.72的Ct指数，分别对应1月、1年和2年时间粒度。这些结果超过了所有现有的方法，无论使用哪种插值策略。
</details></li>
</ul>
<hr>
<h2 id="Improve-Long-term-Memory-Learning-Through-Rescaling-the-Error-Temporally"><a href="#Improve-Long-term-Memory-Learning-Through-Rescaling-the-Error-Temporally" class="headerlink" title="Improve Long-term Memory Learning Through Rescaling the Error Temporally"></a>Improve Long-term Memory Learning Through Rescaling the Error Temporally</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11462">http://arxiv.org/abs/2307.11462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shida Wang, Zhanglu Yan</li>
<li>for: 本文研究sequence模型中长期记忆学习中的错误度量选择。我们发现通常使用的错误都偏向短期记忆，包括平均绝对&#x2F;平方Error。</li>
<li>methods: 我们提议使用时间折算错误来减少这种偏向短期记忆的偏误，同时可以解决vanishing gradient问题。</li>
<li>results: 我们在不同的长期任务和序列模型上进行了数学实验，结果证明适当的时间折算错误对长期记忆学习是必要的。这是我们知道的首个对sequence模型中错误度量偏向短期记忆的量化分析。<details>
<summary>Abstract</summary>
This paper studies the error metric selection for long-term memory learning in sequence modelling. We examine the bias towards short-term memory in commonly used errors, including mean absolute/squared error. Our findings show that all temporally positive-weighted errors are biased towards short-term memory in learning linear functionals. To reduce this bias and improve long-term memory learning, we propose the use of a temporally rescaled error. In addition to reducing the bias towards short-term memory, this approach can also alleviate the vanishing gradient issue. We conduct numerical experiments on different long-memory tasks and sequence models to validate our claims. Numerical results confirm the importance of appropriate temporally rescaled error for effective long-term memory learning. To the best of our knowledge, this is the first work that quantitatively analyzes different errors' memory bias towards short-term memory in sequence modelling.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Neural-Operators-for-Delay-Compensating-Control-of-Hyperbolic-PIDEs"><a href="#Neural-Operators-for-Delay-Compensating-Control-of-Hyperbolic-PIDEs" class="headerlink" title="Neural Operators for Delay-Compensating Control of Hyperbolic PIDEs"></a>Neural Operators for Delay-Compensating Control of Hyperbolic PIDEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11436">http://arxiv.org/abs/2307.11436</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jingzhang-jz/no_hyperbolic_delay">https://github.com/jingzhang-jz/no_hyperbolic_delay</a></li>
<li>paper_authors: Jie Qi, Jing Zhang, Miroslav Krstic</li>
<li>for: 这篇论文旨在扩展DeepONet操作学框架，用于解决延迟型PDE控制问题。</li>
<li>methods: 该论文使用PDE倒逆设计生成积分函数，并使用深度神经网络approximate这些积分函数。</li>
<li>results: 该论文提出了一种约等于逼近误差的approximation-theoretic result，并证明了这种approximation可以保证系统的稳定性。此外，论文还提出了使用DeepONet-approximated observers和输出反馈法，并证明了这些方法的稳定性。<details>
<summary>Abstract</summary>
The recently introduced DeepONet operator-learning framework for PDE control is extended from the results for basic hyperbolic and parabolic PDEs to an advanced hyperbolic class that involves delays on both the state and the system output or input. The PDE backstepping design produces gain functions that are outputs of a nonlinear operator, mapping functions on a spatial domain into functions on a spatial domain, and where this gain-generating operator's inputs are the PDE's coefficients. The operator is approximated with a DeepONet neural network to a degree of accuracy that is provably arbitrarily tight. Once we produce this approximation-theoretic result in infinite dimension, with it we establish stability in closed loop under feedback that employs approximate gains. In addition to supplying such results under full-state feedback, we also develop DeepONet-approximated observers and output-feedback laws and prove their own stabilizing properties under neural operator approximations. With numerical simulations we illustrate the theoretical results and quantify the numerical effort savings, which are of two orders of magnitude, thanks to replacing the numerical PDE solving with the DeepONet.
</details>
<details>
<summary>摘要</summary>
Recently, the DeepONet operator-learning framework for PDE control has been extended from basic hyperbolic and parabolic PDEs to an advanced hyperbolic class that involves delays on both the state and the system output or input. The PDE backstepping design produces gain functions that are outputs of a nonlinear operator, mapping functions on a spatial domain into functions on a spatial domain, and where this gain-generating operator's inputs are the PDE's coefficients. The operator is approximated with a DeepONet neural network to a degree of accuracy that is provably arbitrarily tight. Once we produce this approximation-theoretic result in infinite dimension, with it we establish stability in closed loop under feedback that employs approximate gains. In addition to supplying such results under full-state feedback, we also develop DeepONet-approximated observers and output-feedback laws and prove their own stabilizing properties under neural operator approximations. With numerical simulations we illustrate the theoretical results and quantify the numerical effort savings, which are of two orders of magnitude, thanks to replacing the numerical PDE solving with the DeepONet.Here's the word-for-word translation of the text into Simplified Chinese:最近，DeepONet操作学框架在Hyperbolic PDE控制中被扩展到包括状态和系统输出或输入延迟的高级Hyperbolic类型。PDE反馈设计生成的 gain 函数是一个非线性操作的输出，将函数空间中的函数映射到另一个函数空间中，其中这个 gain-生成器的输入是PDE的系数。这个操作被 aproximated 用 DeepONet神经网络，以达到可靠的、无穷维度的精度。我们通过这种近似理论结果，在无穷维度中Establish 稳定性，并且在反馈 employed  approximate gains 下，实现了稳定性。此外，我们还开发了 DeepONet 近似的观察器和输出反馈法则，并证明它们在神经操作近似下具有稳定性。通过数值仿真，我们证明了理论结果，并量化了数值努力的减少，这是两个数量级的。
</details></li>
</ul>
<hr>
<h2 id="Batching-for-Green-AI-–-An-Exploratory-Study-on-Inference"><a href="#Batching-for-Green-AI-–-An-Exploratory-Study-on-Inference" class="headerlink" title="Batching for Green AI – An Exploratory Study on Inference"></a>Batching for Green AI – An Exploratory Study on Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11434">http://arxiv.org/abs/2307.11434</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim Yarally, Luís Cruz, Daniel Feitosa, June Sallou, Arie van Deursen</li>
<li>for: 本研究探讨了批处理对计算机视觉领域中已经训练完成的五种深度学习模型的能耗和响应时间影响。</li>
<li>methods: 本研究使用了五种已经发表的计算机视觉领域的深度学习模型，并对这些模型进行了批处理测试。</li>
<li>results: 研究发现，批处理对计算机视觉领域中已经训练完成的五种深度学习模型都有显著的影响，并且能耗和响应时间都随着批处理大小的增加而增加。此外，研究还发现了过去十年中神经网络的能耗和精度的发展趋势，发现能耗的增长速度远远高于精度的增长，并且提出了一些可能的解释。最后，研究还探讨了一种名为ShuffleNetV2（2018）的网络，它在其时间内实现了竞争性性能而具有远低的能耗水平。<details>
<summary>Abstract</summary>
The batch size is an essential parameter to tune during the development of new neural networks. Amongst other quality indicators, it has a large degree of influence on the model's accuracy, generalisability, training times and parallelisability. This fact is generally known and commonly studied. However, during the application phase of a deep learning model, when the model is utilised by an end-user for inference, we find that there is a disregard for the potential benefits of introducing a batch size. In this study, we examine the effect of input batching on the energy consumption and response times of five fully-trained neural networks for computer vision that were considered state-of-the-art at the time of their publication. The results suggest that batching has a significant effect on both of these metrics. Furthermore, we present a timeline of the energy efficiency and accuracy of neural networks over the past decade. We find that in general, energy consumption rises at a much steeper pace than accuracy and question the necessity of this evolution. Additionally, we highlight one particular network, ShuffleNetV2(2018), that achieved a competitive performance for its time while maintaining a much lower energy consumption. Nevertheless, we highlight that the results are model dependent.
</details>
<details>
<summary>摘要</summary>
批处大小是深度学习模型开发中一个重要的参数，其影响模型的准确率、通用性、训练时间和并行性等质量指标。这一点通常已被广泛研究。然而，在深度学习模型应用阶段，当模型被用户进行推理时，批处大小的影响并未受到足够的重视。本研究检查了五种完全训练的计算机视觉神经网络在批处大小的影响下的能量消耗和响应时间。结果表明，批处大小有着显著的影响。此外，我们还提供了过去十年内神经网络能效和准确率的时间线，发现能效消耗在准确率的提升后逐渐增加，而不是同步增加。此外，我们还探讨了一个特定的网络，即ShuffleNetV2（2018），它在其时代达到了竞争性性能，而且具有较低的能效消耗。然而，我们注意到结果受模型影响。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Embedding-Learning-for-Human-Activity-Recognition-Using-Wearable-Sensor-Data"><a href="#Unsupervised-Embedding-Learning-for-Human-Activity-Recognition-Using-Wearable-Sensor-Data" class="headerlink" title="Unsupervised Embedding Learning for Human Activity Recognition Using Wearable Sensor Data"></a>Unsupervised Embedding Learning for Human Activity Recognition Using Wearable Sensor Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11796">http://arxiv.org/abs/2307.11796</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taoran Sheng, Manfred Huber</li>
<li>for:  recognize human activities from wearable sensor data</li>
<li>methods: unsupervised approach based on nature of human activity, embedding space, clustering algorithms</li>
<li>results: improved performance in identifying and categorizing underlying human activities compared to unsupervised techniques applied directly to original data set<details>
<summary>Abstract</summary>
The embedded sensors in widely used smartphones and other wearable devices make the data of human activities more accessible. However, recognizing different human activities from the wearable sensor data remains a challenging research problem in ubiquitous computing. One of the reasons is that the majority of the acquired data has no labels. In this paper, we present an unsupervised approach, which is based on the nature of human activity, to project the human activities into an embedding space in which similar activities will be located closely together. Using this, subsequent clustering algorithms can benefit from the embeddings, forming behavior clusters that represent the distinct activities performed by a person. Results of experiments on three labeled benchmark datasets demonstrate the effectiveness of the framework and show that our approach can help the clustering algorithm achieve improved performance in identifying and categorizing the underlying human activities compared to unsupervised techniques applied directly to the original data set.
</details>
<details>
<summary>摘要</summary>
《嵌入式感知器在广泛使用的智能手机和其他搭载设备中的数据使得人类活动更加可访问。然而，从嵌入式感知器数据中识别不同的人类活动仍然是艰难的研究问题。一个原因是大多数获得的数据没有标签。在这篇论文中，我们提出了一种无监督的方法，基于人类活动的自然特征，将人类活动 проек到一个嵌入空间中，在该空间中相似的活动将会受到较近的位置。使用这些嵌入，后续的聚类算法可以从嵌入中受益，形成人类活动的行为团集，表示人类在不同场景下的不同活动。实验结果表明，我们的框架可以帮助聚类算法更好地识别和分类人类活动，比较传统的无监督方法直接应用于原始数据集。》Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="An-Analysis-of-Multi-Agent-Reinforcement-Learning-for-Decentralized-Inventory-Control-Systems"><a href="#An-Analysis-of-Multi-Agent-Reinforcement-Learning-for-Decentralized-Inventory-Control-Systems" class="headerlink" title="An Analysis of Multi-Agent Reinforcement Learning for Decentralized Inventory Control Systems"></a>An Analysis of Multi-Agent Reinforcement Learning for Decentralized Inventory Control Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11432">http://arxiv.org/abs/2307.11432</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marwan Mousa, Damien van de Berg, Niki Kotecha, Ehecatl Antonio del Rio-Chanona, Max Mowbray</li>
<li>for: 解决供应链内部存储管理问题，即找到最佳重新订购策略。</li>
<li>methods: 使用多智能体强化学习，每个实体由一个智能体控制。</li>
<li>results: 与中央数据驱动解决方案相当的性能，并在大多数情况下超越分布式模型驱动解决方案，同时尊重系统信息约束。<details>
<summary>Abstract</summary>
Most solutions to the inventory management problem assume a centralization of information that is incompatible with organisational constraints in real supply chain networks. The inventory management problem is a well-known planning problem in operations research, concerned with finding the optimal re-order policy for nodes in a supply chain. While many centralized solutions to the problem exist, they are not applicable to real-world supply chains made up of independent entities. The problem can however be naturally decomposed into sub-problems, each associated with an independent entity, turning it into a multi-agent system. Therefore, a decentralized data-driven solution to inventory management problems using multi-agent reinforcement learning is proposed where each entity is controlled by an agent. Three multi-agent variations of the proximal policy optimization algorithm are investigated through simulations of different supply chain networks and levels of uncertainty. The centralized training decentralized execution framework is deployed, which relies on offline centralization during simulation-based policy identification, but enables decentralization when the policies are deployed online to the real system. Results show that using multi-agent proximal policy optimization with a centralized critic leads to performance very close to that of a centralized data-driven solution and outperforms a distributed model-based solution in most cases while respecting the information constraints of the system.
</details>
<details>
<summary>摘要</summary>
大多数解决存储管理问题的解决方案假设了中央化信息的可用性，这与实际供应链网络中的组织结构不兼容。存储管理问题是操作研究中的一个著名规划问题，旨在找到最优的重新订购策略 для供应链中的节点。虽然许多中央化解决方案存在，但它们不适用于实际的独立实体组成的供应链网络中。问题可以自然划分为子问题，每个子问题与独立实体相关。因此，一种基于多智能体学习的数据驱动的分布式存储管理解决方案被提议，其中每个实体由一个智能体控制。三种多智能体变体的距离降低策略算法在不同的供应链网络和不确定程度下进行了 simulations。中央训练并行执行框架被部署，该框架在在实际系统中部署策略时依靠了Offline中央化，但在部署策略时允许了分布式执行。结果显示，使用多智能体距离降低策略与中央数据驱动解决方案的性能几乎相同，而且在大多数情况下超过分布式模型基于解决方案的性能，同时尊重系统中信息的限制。
</details></li>
</ul>
<hr>
<h2 id="Prompting-Large-Language-Models-with-Speech-Recognition-Abilities"><a href="#Prompting-Large-Language-Models-with-Speech-Recognition-Abilities" class="headerlink" title="Prompting Large Language Models with Speech Recognition Abilities"></a>Prompting Large Language Models with Speech Recognition Abilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11795">http://arxiv.org/abs/2307.11795</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Junteng Jia, Yuan Shangguan, Ke Li, Jinxi Guo, Wenhan Xiong, Jay Mahadeokar, Ozlem Kalinli, Christian Fuegen, Mike Seltzer</li>
<li>for: 本研究旨在扩展大语言模型（LLM）的能力，通过直接附加小audio编码器，使其可以进行语音识别（ASR）。</li>
<li>methods: 本研究使用了开源的LLaMA-7B模型，并附加了一个小的audio编码器，通过 prepending 一个序列的audial嵌入来将文本嵌入与audio嵌入相连接。</li>
<li>results: 实验表明，将LLaMA-7B模型与audio编码器结合使用可以在多语言MLS dataset上提高18%，并且可以在LLaMA模型被冻结或audio编码器缩放时进行多语言语音识别。<details>
<summary>Abstract</summary>
Large language models have proven themselves highly flexible, able to solve a wide range of generative tasks, such as abstractive summarization and open-ended question answering. In this paper we extend the capabilities of LLMs by directly attaching a small audio encoder allowing it to perform speech recognition. By directly prepending a sequence of audial embeddings to the text token embeddings, the LLM can be converted to an automatic speech recognition (ASR) system, and be used in the exact same manner as its textual counterpart. Experiments on Multilingual LibriSpeech (MLS) show that incorporating a conformer encoder into the open sourced LLaMA-7B allows it to outperform monolingual baselines by 18% and perform multilingual speech recognition despite LLaMA being trained overwhelmingly on English text. Furthermore, we perform ablation studies to investigate whether the LLM can be completely frozen during training to maintain its original capabilities, scaling up the audio encoder, and increasing the audio encoder striding to generate fewer embeddings. The results from these studies show that multilingual ASR is possible even when the LLM is frozen or when strides of almost 1 second are used in the audio encoder opening up the possibility for LLMs to operate on long-form audio.
</details>
<details>
<summary>摘要</summary>
大型语言模型已经证明了它们在多种生成任务上表现出色，如抽象摘要和开终问答。在这篇论文中，我们将 LLMS 的能力扩展到可以进行语音识别。我们直接将语音编码器附加到文本对象码 embedding 上，将 LLMS 转换为自动语音识别系统，并且可以与文本版本一样使用。实验结果显示，将 Conformer 编码器 incorporated 到 open sourced LLaMA-7B 中，可以比较英文单语基eline 高18%，并且可以进行多种语言语音识别，即使 LLMA 被训练超过英文文本。此外，我们进行了ablation 研究，以investigate 是否可以将 LLM 完全冻结在训练中，推广音频编码器，并将音频编码器步长增加，以生成 fewer embeddings。研究结果显示，多种语言语音识别是可能的，即使 LLM 被冻结或音频编码器步长接近1秒。
</details></li>
</ul>
<hr>
<h2 id="Attention-to-Entropic-Communication"><a href="#Attention-to-Entropic-Communication" class="headerlink" title="Attention to Entropic Communication"></a>Attention to Entropic Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11423">http://arxiv.org/abs/2307.11423</a></li>
<li>repo_url: None</li>
<li>paper_authors: Torsten Enßlin, Carolin Weidinger, Philipp Frank</li>
<li>for: 本文旨在探讨关于注意力的概念，即人工智能中的数字归一化，以及通信理论中的相对 entropy（RE）的结合。</li>
<li>methods: 本文使用了RE来导引优化的编码和解码方法，包括最大熵原则（MEP）。RE从四个要求中 derivable，namely being analytical, local, proper, and calibrated。</li>
<li>results: 研究发现，使用Weighted RE来引导注意力导航可能不正确。在一个消息发送者想要确保接收者可以做出 Informed actions 的场景下，可以使用Entropic attention communication来实现合适的通信协议。这种通信协议可以帮助设计优化的通信协议，以及更好地理解人类communication。例如，我们的分析表明，在其他正直的通信伙伴之间存在偏移的利益下，可以预期到哪种水平的合作。<details>
<summary>Abstract</summary>
The concept of attention, numerical weights that emphasize the importance of particular data, has proven to be very relevant in artificial intelligence. Relative entropy (RE, aka Kullback-Leibler divergence) plays a central role in communication theory. Here we combine these concepts, attention and RE. RE guides optimal encoding of messages in bandwidth-limited communication as well as optimal message decoding via the maximum entropy principle (MEP). In the coding scenario, RE can be derived from four requirements, namely being analytical, local, proper, and calibrated. Weighted RE, used for attention steering in communications, turns out to be improper. To see how proper attention communication can emerge, we analyze a scenario of a message sender who wants to ensure that the receiver of the message can perform well-informed actions. If the receiver decodes the message using the MEP, the sender only needs to know the receiver's utility function to inform optimally, but not the receiver's initial knowledge state. In case only the curvature of the utility function maxima are known, it becomes desirable to accurately communicate an attention function, in this case a by this curvature weighted and re-normalized probability function. Entropic attention communication is here proposed as the desired generalization of entropic communication that permits weighting while being proper, thereby aiding the design of optimal communication protocols in technical applications and helping to understand human communication. For example, our analysis shows how to derive the level of cooperation expected under misaligned interests of otherwise honest communication partners.
</details>
<details>
<summary>摘要</summary>
人工智能中的注意力概念（numerical weights that emphasize the importance of particular data）已经得到了广泛的应用。对于通信理论来说，相对Entropy（RE，又称降低利比得 divergence）扮演了中心角色。在这里，我们将注意力和RE结合起来。RE在带宽有限的通信中用于最佳编码消息以及最佳消息解码via最大Entropy原理（MEP）。在编码场景中，RE可以从四个需求 derivation，namely being analytical, local, proper, and calibrated。weighted RE，用于注意力导航在通信中，实际上是不正确的。要如何实现正确的注意力通信，我们分析了一个消息发送者希望确保接收者可以做出高效的行动的场景。如果接收者使用MEP解码消息，那么发送者只需知道接收者的利用函数，以便 Informed Optimally，而不需要知道接收者的初始知识状态。在只知道利用函数的拐点的 curvature 的情况下，则变得愿意将注意力函数正确地权重和 нормализа，即Entropic attention communication。我们提出了这种总的扩展，即Entropic communication，允许权重，从而帮助设计最佳通信协议，以及帮助理解人类通信。例如，我们的分析表明，在不同兴趣的情况下，预计的合作水平是多少。
</details></li>
</ul>
<hr>
<h2 id="Direct-and-inverse-modeling-of-soft-robots-by-learning-a-condensed-FEM-model"><a href="#Direct-and-inverse-modeling-of-soft-robots-by-learning-a-condensed-FEM-model" class="headerlink" title="Direct and inverse modeling of soft robots by learning a condensed FEM model"></a>Direct and inverse modeling of soft robots by learning a condensed FEM model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11408">http://arxiv.org/abs/2307.11408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Etienne Ménager, Tanguy Navez, Olivier Goury, Christian Duriez</li>
<li>for: 这篇论文旨在提出一种学习基于方法来获得一个具有充分表征力的机械表述，以便在实时控制软体机器人时使用。</li>
<li>methods: 该论文使用了学习方法，特别是基于非线性弹性数据在活动器&#x2F;效器空间提供的condensation方法，以获得一个具有可重要性和高效性的机械表述。</li>
<li>results: 该论文表明，这种压缩模型可以通过一个合理的数据量来学习，同时具有高效的模型描述能力，可以直接地推导软体机器人的直接和反直接干涉。此外，该论文还比较了基于完整FEM模型和压缩学习模型的反向模型，并显示了两者之间的区别。<details>
<summary>Abstract</summary>
The Finite Element Method (FEM) is a powerful modeling tool for predicting the behavior of soft robots. However, its use for control can be difficult for non-specialists of numerical computation: it requires an optimization of the computation to make it real-time. In this paper, we propose a learning-based approach to obtain a compact but sufficiently rich mechanical representation. Our choice is based on nonlinear compliance data in the actuator/effector space provided by a condensation of the FEM model. We demonstrate that this compact model can be learned with a reasonable amount of data and, at the same time, be very efficient in terms of modeling, since we can deduce the direct and inverse kinematics of the robot. We also show how to couple some models learned individually in particular on an example of a gripper composed of two soft fingers. Other results are shown by comparing the inverse model derived from the full FEM model and the one from the compact learned version. This work opens new perspectives, namely for the embedded control of soft robots, but also for their design. These perspectives are also discussed in the paper.
</details>
<details>
<summary>摘要</summary>
finite element method (FEM) 是软体机器人预测行为的强大工具。然而，它在控制方面可能会对非专家 numerics computation 提供挑战。在这篇文章中，我们提出了一种基于学习的方法，以获得实时计算的简洁且具有充分的机械表示。我们的选择基于 FEM 模型中的非线性弹性数据在 actuator/effector 空间。我们证明了这个简洁模型可以通过有限的数据学习，同时具有高效的计算模型特性，因为我们可以从 robot 的直接和反直接运动方程中提取机械参数。此外，我们还示出了将一些个体学习在 particualr 的示例中，例如一个由两个软脚 compose 的抓取机。其他结果还是通过对全 FEM 模型 derive 的反模型和学习版本进行比较来展示。这项工作开启了新的视野，包括软体机器人的嵌入控制和设计。这些视野也在文章中进行了讨论。
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Modeling-of-Inter-and-Intra-observer-Variability-in-Medical-Image-Segmentation"><a href="#Probabilistic-Modeling-of-Inter-and-Intra-observer-Variability-in-Medical-Image-Segmentation" class="headerlink" title="Probabilistic Modeling of Inter- and Intra-observer Variability in Medical Image Segmentation"></a>Probabilistic Modeling of Inter- and Intra-observer Variability in Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11397">http://arxiv.org/abs/2307.11397</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arne Schmidt, Pablo Morales-Álvarez, Rafael Molina</li>
<li>for: 这个论文是为了解决医学图像分割 tasks 中的干扰和变化问题，特别是 Between medical experts 之间和内部的差异。</li>
<li>methods: 该论文提出了一种新的模型，即 Probabilistic Inter-Observer and iNtra-Observer variation NetwOrk (Pionono)，它捕捉每名评分员的标注行为，并将其与图像特征图呈现出probabilistic segmentation 预测结果。该模型通过变量推理优化，可以进行端到端训练。</li>
<li>results: 实验表明，Pionono 模型在实际悉数据上达到了 state-of-the-art 水平，并且可以生成多个协调的 segmentation 图，这些图呈现出评分员的专家意见，这些信息可以用于诊断过程中。<details>
<summary>Abstract</summary>
Medical image segmentation is a challenging task, particularly due to inter- and intra-observer variability, even between medical experts. In this paper, we propose a novel model, called Probabilistic Inter-Observer and iNtra-Observer variation NetwOrk (Pionono). It captures the labeling behavior of each rater with a multidimensional probability distribution and integrates this information with the feature maps of the image to produce probabilistic segmentation predictions. The model is optimized by variational inference and can be trained end-to-end. It outperforms state-of-the-art models such as STAPLE, Probabilistic U-Net, and models based on confusion matrices. Additionally, Pionono predicts multiple coherent segmentation maps that mimic the rater's expert opinion, which provides additional valuable information for the diagnostic process. Experiments on real-world cancer segmentation datasets demonstrate the high accuracy and efficiency of Pionono, making it a powerful tool for medical image analysis.
</details>
<details>
<summary>摘要</summary>
医学图像分割是一项复杂的任务，尤其是由于间观察者和内观察者的变化，即使是医疗专家。在这篇论文中，我们提出了一种新的模型，即可能性间观察者和内观察者变化网络（Pionono）。该模型捕捉每个评分者的标签行为，并将其与图像特征图表进行集成，以生成可能性分割预测。该模型通过变量推断优化，可以进行端到端训练。与现有的模型，如STAPLE、概率U-Net和基于冲突矩阵的模型相比，Pionono表现更高精度和效率。此外，Pionono预测多个协调的分割图，这些图像具有医生专家的专业意见，可以为诊断过程提供价值的信息。实验表明，Pionono在实际悉数癌症分割 dataset 上表现出高精度和效率，使其成为医学图像分析的有力工具。
</details></li>
</ul>
<hr>
<h2 id="Towards-Better-Fairness-Utility-Trade-off-A-Comprehensive-Measurement-Based-Reinforcement-Learning-Framework"><a href="#Towards-Better-Fairness-Utility-Trade-off-A-Comprehensive-Measurement-Based-Reinforcement-Learning-Framework" class="headerlink" title="Towards Better Fairness-Utility Trade-off: A Comprehensive Measurement-Based Reinforcement Learning Framework"></a>Towards Better Fairness-Utility Trade-off: A Comprehensive Measurement-Based Reinforcement Learning Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11379">http://arxiv.org/abs/2307.11379</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simiao Zhang, Jitao Bai, Menghong Guan, Yihao Huang, Yueling Zhang, Jun Sun, Geguang Pu</li>
<li>for:  This paper aims to improve the fairness-utility trade-off in machine learning classifiers, addressing the challenge of ensuring fairness while maintaining utility.</li>
<li>methods: The proposed method, CFU (Comprehensive Fairness-Utility), is a reinforcement learning-based framework that considers multiple fairness notions and utility simultaneously. It introduces new metrics based on an in-depth analysis of the relationship between different fairness metrics.</li>
<li>results: The paper reports extensive experimental results demonstrating that CFU outperforms state-of-the-art techniques and improves the classifier on multiple fairness metrics without sacrificing utility, with an average improvement of 37.5%.<details>
<summary>Abstract</summary>
Machine learning is widely used to make decisions with societal impact such as bank loan approving, criminal sentencing, and resume filtering. How to ensure its fairness while maintaining utility is a challenging but crucial issue. Fairness is a complex and context-dependent concept with over 70 different measurement metrics. Since existing regulations are often vague in terms of which metric to use and different organizations may prefer different fairness metrics, it is important to have means of improving fairness comprehensively. Existing mitigation techniques often target at one specific fairness metric and have limitations in improving multiple notions of fairness simultaneously. In this work, we propose CFU (Comprehensive Fairness-Utility), a reinforcement learning-based framework, to efficiently improve the fairness-utility trade-off in machine learning classifiers. A comprehensive measurement that can simultaneously consider multiple fairness notions as well as utility is established, and new metrics are proposed based on an in-depth analysis of the relationship between different fairness metrics. The reward function of CFU is constructed with comprehensive measurement and new metrics. We conduct extensive experiments to evaluate CFU on 6 tasks, 3 machine learning models, and 15 fairness-utility measurements. The results demonstrate that CFU can improve the classifier on multiple fairness metrics without sacrificing its utility. It outperforms all state-of-the-art techniques and has witnessed a 37.5% improvement on average.
</details>
<details>
<summary>摘要</summary>
机器学习广泛应用于决策中有社会影响，如银行贷款批准、刑事判决和简历过滤。保证其公平性是一个挑战性的 yet crucial 问题。公平性是一个复杂的和上下文依赖的概念，有超过 70 个不同的衡量指标。由于现有的法规通常是对哪个指标使用的抽象的，不同的组织可能会选择不同的公平性指标，因此有必要有一种改进公平性的全面的方法。现有的缓解技术通常只针对一个公平性指标，有限制性在同时改进多个公平性指标上。在这种工作中，我们提出了 CFU（全面公平性-利用度）框架，用于改进机器学习分类器的公平性-利用度贸易。我们建立了一个涵盖多个公平性指标以及利用度的全面测量方法，并基于这些指标进行新的衡量。 CFU 的奖励函数是基于全面测量和新的衡量。我们对 CFU 进行了广泛的实验，评估了 6 个任务、3 种机器学习模型和 15 个公平性-利用度测量。结果显示，CFU 可以同时改进多个公平性指标，而不是牺牲利用度。它超过了所有现有的技术，并在平均上提高了 37.5%。
</details></li>
</ul>
<hr>
<h2 id="LatentAugment-Data-Augmentation-via-Guided-Manipulation-of-GAN’s-Latent-Space"><a href="#LatentAugment-Data-Augmentation-via-Guided-Manipulation-of-GAN’s-Latent-Space" class="headerlink" title="LatentAugment: Data Augmentation via Guided Manipulation of GAN’s Latent Space"></a>LatentAugment: Data Augmentation via Guided Manipulation of GAN’s Latent Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11375">http://arxiv.org/abs/2307.11375</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ltronchin/latentaugment">https://github.com/ltronchin/latentaugment</a></li>
<li>paper_authors: Lorenzo Tronchin, Minh H. Vu, Paolo Soda, Tommy Löfstedt</li>
<li>for: 提高训练数据的量和多样性，降低过拟合和提高泛化</li>
<li>methods: 使用生成对抗网络（GANs）生成高质量样本，并通过修改幽默向量来提高样本的多样性和准确性</li>
<li>results: 对从MRI到CT的深度模型进行翻译，使用LatentAugment DA策略，可以超越标准DA和GAN-based sampling，提高模型的泛化能力和 Synthetic samples的多样性和准确性。<details>
<summary>Abstract</summary>
Data Augmentation (DA) is a technique to increase the quantity and diversity of the training data, and by that alleviate overfitting and improve generalisation. However, standard DA produces synthetic data for augmentation with limited diversity. Generative Adversarial Networks (GANs) may unlock additional information in a dataset by generating synthetic samples having the appearance of real images. However, these models struggle to simultaneously address three key requirements: fidelity and high-quality samples; diversity and mode coverage; and fast sampling. Indeed, GANs generate high-quality samples rapidly, but have poor mode coverage, limiting their adoption in DA applications. We propose LatentAugment, a DA strategy that overcomes the low diversity of GANs, opening up for use in DA applications. Without external supervision, LatentAugment modifies latent vectors and moves them into latent space regions to maximise the synthetic images' diversity and fidelity. It is also agnostic to the dataset and the downstream task. A wide set of experiments shows that LatentAugment improves the generalisation of a deep model translating from MRI-to-CT beating both standard DA as well GAN-based sampling. Moreover, still in comparison with GAN-based sampling, LatentAugment synthetic samples show superior mode coverage and diversity. Code is available at: https://github.com/ltronchin/LatentAugment.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the following text into Simplified Chinese.<</SYS>>数据扩充（DA）是一种技术来增加训练数据的量和多样性，从而避免过拟合和提高泛化。然而，标准的DA仅生成有限多样性的Synthetic数据。生成 adversarial networks（GANs）可能会从数据中提取更多的信息，通过生成具有真实图像的样式的Synthetic样本。然而，这些模型很难同时满足三个关键要求：准确性和高质量样本；多样性和模式覆盖率；和快速采样。实际上，GANs可以快速生成高质量样本，但它们的模式覆盖率很低，限制了它们在DA应用中的采用。我们提出了LatentAugment，一种DA策略，可以在GANs中解决低多样性的问题，并在DA应用中使用。无需外部监督，LatentAugment可以在latent空间中修改latent вектор，将其移动到latent空间中的最佳多样性和准确性的区域。它还是数据aset和下游任务无关的。一系列实验表明，LatentAugment可以提高一个深度模型从MRI到CT的翻译性能，超过标准DA和GAN-based sampling。此外，相比GAN-based sampling，LatentAugment的Synthetic样本还显示出更高的多样性和模式覆盖率。代码可以在：https://github.com/ltronchin/LatentAugment。
</details></li>
</ul>
<hr>
<h2 id="Diverse-Offline-Imitation-via-Fenchel-Duality"><a href="#Diverse-Offline-Imitation-via-Fenchel-Duality" class="headerlink" title="Diverse Offline Imitation via Fenchel Duality"></a>Diverse Offline Imitation via Fenchel Duality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11373">http://arxiv.org/abs/2307.11373</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marin Vlastelica, Pavel Kolev, Jin Cheng, Georg Martius</li>
<li>for: 本研究旨在开发一种无监督技能发现算法，它可以在没有在线环境访问的情况下自动学习多种精巧的技能。</li>
<li>methods: 本算法使用了共识信息目标函数，并采用了KL散度约束来保证每种技能在不同状态下的行为占据率与专家的占据率相似。</li>
<li>results: 本研究提出了一种基于 Fenchel duality、再养学习和无监督技能发现的简单算法，可以在无线上学习多种与专家相似的精巧技能。<details>
<summary>Abstract</summary>
There has been significant recent progress in the area of unsupervised skill discovery, with various works proposing mutual information based objectives, as a source of intrinsic motivation. Prior works predominantly focused on designing algorithms that require online access to the environment. In contrast, we develop an \textit{offline} skill discovery algorithm. Our problem formulation considers the maximization of a mutual information objective constrained by a KL-divergence. More precisely, the constraints ensure that the state occupancy of each skill remains close to the state occupancy of an expert, within the support of an offline dataset with good state-action coverage. Our main contribution is to connect Fenchel duality, reinforcement learning and unsupervised skill discovery, and to give a simple offline algorithm for learning diverse skills that are aligned with an expert.
</details>
<details>
<summary>摘要</summary>
近期在无监督技能发现领域有了显著的进步，许多研究提出了基于共识信息的目标函数，作为内在动机。先前的工作主要关注设计需要在环境上线访问的算法。相比之下，我们开发了一种OFFLINE技能发现算法。我们的问题定义是基于共识信息目标函数均衡化KL抖度的约束，以保证每个技能的状态占据率保持在专家的状态占据率附近，在具有良好状态动作覆盖率的OFFLINE数据集上。我们的主要贡献是将 fenchel duality、再征学习和无监督技能发现联系起来，并提供一种简单的OFFLINE算法，用于学习与专家Alignment的多样化技能。
</details></li>
</ul>
<hr>
<h2 id="Random-Separating-Hyperplane-Theorem-and-Learning-Polytopes"><a href="#Random-Separating-Hyperplane-Theorem-and-Learning-Polytopes" class="headerlink" title="Random Separating Hyperplane Theorem and Learning Polytopes"></a>Random Separating Hyperplane Theorem and Learning Polytopes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11371">http://arxiv.org/abs/2307.11371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chiranjib Bhattacharyya, Ravindran Kannan, Amit Kumar</li>
<li>for: 这个论文主要关注的是 convex geometry 领域中的一个基本结果，即 Separating Hyperplane theorem，以及其在多规体上的应用。</li>
<li>methods: 这个论文使用了 Random Separating Hyperplane Theorem (RSH)，这是 Separating Hyperplane theorem 的强化版本，用于多规体上的分离。</li>
<li>results: 这个论文的结果包括：1) 使用 RSH 可以在高probability 下，将一个点与多规体之间的距离分离出来，并且margin 可以达到 O（δ&#x2F;√d）; 2) 使用 RSH 可以在 polynomially many  queries 下，approximate 一个 unit diameter 多规体的 Hausdorff distance  dentro δ; 3) 如果多规体的顶点受到很好地分离，那么可以使用 optimization oracle 来生成一个点集合，每个点都在 Hausdorff distance O（δ）内的多规体上，并且这个点集合中含有一个点 Close 于每个顶点。<details>
<summary>Abstract</summary>
The Separating Hyperplane theorem is a fundamental result in Convex Geometry with myriad applications. Our first result, Random Separating Hyperplane Theorem (RSH), is a strengthening of this for polytopes. $\rsh$ asserts that if the distance between $a$ and a polytope $K$ with $k$ vertices and unit diameter in $\Re^d$ is at least $\delta$, where $\delta$ is a fixed constant in $(0,1)$, then a randomly chosen hyperplane separates $a$ and $K$ with probability at least $1/poly(k)$ and margin at least $\Omega \left(\delta/\sqrt{d} \right)$. An immediate consequence of our result is the first near optimal bound on the error increase in the reduction from a Separation oracle to an Optimization oracle over a polytope.   RSH has algorithmic applications in learning polytopes. We consider a fundamental problem, denoted the ``Hausdorff problem'', of learning a unit diameter polytope $K$ within Hausdorff distance $\delta$, given an optimization oracle for $K$. Using RSH, we show that with polynomially many random queries to the optimization oracle, $K$ can be approximated within error $O(\delta)$. To our knowledge this is the first provable algorithm for the Hausdorff Problem. Building on this result, we show that if the vertices of $K$ are well-separated, then an optimization oracle can be used to generate a list of points, each within Hausdorff distance $O(\delta)$ of $K$, with the property that the list contains a point close to each vertex of $K$. Further, we show how to prune this list to generate a (unique) approximation to each vertex of the polytope. We prove that in many latent variable settings, e.g., topic modeling, LDA, optimization oracles do exist provided we project to a suitable SVD subspace. Thus, our work yields the first efficient algorithm for finding approximations to the vertices of the latent polytope under the well-separatedness assumption.
</details>
<details>
<summary>摘要</summary>
“凹陷函数定理”是凹陷几何中的基本结果，具有广泛的应用。我们的第一个结果是随机凹陷函数定理（RSH），它是凹陷函数定理的强化版本，应用于多面体。RSH assert that if the distance between $a$ and a polytope $K$ with $k$ vertices and unit diameter in $\Re^d$ is at least $\delta$, where $\delta$ is a fixed constant in $(0,1)$, then a randomly chosen hyperplane separates $a$ and $K$ with probability at least $1/poly(k)$ and margin at least $\Omega \left(\delta/\sqrt{d} \right)$.这个结果的直接后果是对多面体的减少错误增长在减少测试到估计项中的首先几何问题中的近似解。RSH有着算法应用，包括学习多面体。我们考虑一个基本问题，称为“ Hausdorff 问题”，是学习单位径多面体 $K$ 的 Hausdorff 距离 $\delta$， givien an optimization oracle for $K$. 使用 RSH，我们表明了可以使用几何问题中的减少错误增长，通过几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以几何问题中的减少错误增长，以�
</details></li>
</ul>
<hr>
<h2 id="Bridging-the-Reality-Gap-of-Reinforcement-Learning-based-Traffic-Signal-Control-using-Domain-Randomization-and-Meta-Learning"><a href="#Bridging-the-Reality-Gap-of-Reinforcement-Learning-based-Traffic-Signal-Control-using-Domain-Randomization-and-Meta-Learning" class="headerlink" title="Bridging the Reality Gap of Reinforcement Learning based Traffic Signal Control using Domain Randomization and Meta Learning"></a>Bridging the Reality Gap of Reinforcement Learning based Traffic Signal Control using Domain Randomization and Meta Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11357">http://arxiv.org/abs/2307.11357</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arthur Müller, Matthia Sabatelli</li>
<li>for: 本研究旨在 Addressing the reality gap in Reinforcement Learning (RL) based Traffic Signal Control (TSC) systems.</li>
<li>methods: 本研究使用了 Domain Randomization (DR) 和 Model-Agnostic Meta-Learning (MAML) 两种策略来 bridge the reality gap.</li>
<li>results: 实验结果显示，DR 和 MAML 比 state-of-the-art RL 算法高效，因此有潜力用于 mitigating the reality gap in RLbased TSC systems.<details>
<summary>Abstract</summary>
Reinforcement Learning (RL) has been widely explored in Traffic Signal Control (TSC) applications, however, still no such system has been deployed in practice. A key barrier to progress in this area is the reality gap, the discrepancy that results from differences between simulation models and their real-world equivalents. In this paper, we address this challenge by first presenting a comprehensive analysis of potential simulation parameters that contribute to this reality gap. We then also examine two promising strategies that can bridge this gap: Domain Randomization (DR) and Model-Agnostic Meta-Learning (MAML). Both strategies were trained with a traffic simulation model of an intersection. In addition, the model was embedded in LemgoRL, a framework that integrates realistic, safety-critical requirements into the control system. Subsequently, we evaluated the performance of the two methods on a separate model of the same intersection that was developed with a different traffic simulator. In this way, we mimic the reality gap. Our experimental results show that both DR and MAML outperform a state-of-the-art RL algorithm, therefore highlighting their potential to mitigate the reality gap in RLbased TSC systems.
</details>
<details>
<summary>摘要</summary>
强化学习（RL）在交通信号控制（TSC）应用中广泛探索，然而到目前为止还没有实际部署过这类系统。一个关键的障碍是现实差距，即模拟和实际世界之间的差异。在本文中，我们首先对可能导致这个现实差距的模拟参数进行了全面的分析。然后我们还检查了两种可能bridging这个差距的策略：领域随机化（DR）和模型无关元学习（MAML）。这两种策略在交通 simulateModel of an intersection 上进行了训练。然后，我们将这两种方法应用于交通 simulateModel of the same intersection 上，这个模型使用了不同的交通 simulate器。这种方式可以模拟现实差距。我们的实验结果表明，DR和MAML在比较一种现有RL算法时表现出色，因此highlighted their potential to mitigate the reality gap in RL-based TSC systems。
</details></li>
</ul>
<hr>
<h2 id="What-can-a-Single-Attention-Layer-Learn-A-Study-Through-the-Random-Features-Lens"><a href="#What-can-a-Single-Attention-Layer-Learn-A-Study-Through-the-Random-Features-Lens" class="headerlink" title="What can a Single Attention Layer Learn? A Study Through the Random Features Lens"></a>What can a Single Attention Layer Learn? A Study Through the Random Features Lens</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11353">http://arxiv.org/abs/2307.11353</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hengyu Fu, Tianyu Guo, Yu Bai, Song Mei</li>
<li>for: 本研究は、Transformer架构中的注意层（Attention Layer）的学习和泛化に関する理论的研究である。</li>
<li>methods: 本研究使用了随机特征设定，其中注意层有大量的头，并使用随机冻结查询矩阵和随机冻结关键矩阵，以及可调值矩阵。</li>
<li>results: 我们的研究结果表明，随机特征注意层可以表示具有排序不变性的广泛的目标函数。我们还提供了来自finite samples的过程复杂度下的额外风险下界，用于学习这些目标函数。我们的结果还表明，随机特征注意层在学习某些自然目标函数时可以有更好的样本复杂度。实验结果证实了我们的理论发现，并进一步阐明了样本大小和目标函数复杂度之间的交互。<details>
<summary>Abstract</summary>
Attention layers -- which map a sequence of inputs to a sequence of outputs -- are core building blocks of the Transformer architecture which has achieved significant breakthroughs in modern artificial intelligence. This paper presents a rigorous theoretical study on the learning and generalization of a single multi-head attention layer, with a sequence of key vectors and a separate query vector as input. We consider the random feature setting where the attention layer has a large number of heads, with randomly sampled frozen query and key matrices, and trainable value matrices. We show that such a random-feature attention layer can express a broad class of target functions that are permutation invariant to the key vectors. We further provide quantitative excess risk bounds for learning these target functions from finite samples, using random feature attention with finitely many heads.   Our results feature several implications unique to the attention structure compared with existing random features theory for neural networks, such as (1) Advantages in the sample complexity over standard two-layer random-feature networks; (2) Concrete and natural classes of functions that can be learned efficiently by a random-feature attention layer; and (3) The effect of the sampling distribution of the query-key weight matrix (the product of the query and key matrix), where Gaussian random weights with a non-zero mean result in better sample complexities over the zero-mean counterpart for learning certain natural target functions. Experiments on simulated data corroborate our theoretical findings and further illustrate the interplay between the sample size and the complexity of the target function.
</details>
<details>
<summary>摘要</summary>
注意层 -- 将输入序列映射到输出序列 -- 是Transformer架构的核心构件，在现代人工智能中取得了重要突破。这篇论文提供了对单个多头注意层学习和泛化的严格理论研究，输入包括一系列键向量和独立的查询向量。我们考虑随机特征设置，其中注意层有大量的头，查询和键矩阵随机冻结，值矩阵可变。我们显示这种随机特征注意层可以表示一类具有排序不变性的目标函数。我们还提供了来自finite samples的过剩风险下界，用于学习这些目标函数。我们的结果包括几个对注意结构的特点的特殊Implications，如：1. 与标准两层随机特征网络相比，随机特征注意层的样本复杂性优于。2. 随机特征注意层可以高效地学习一类自然的目标函数。3. 查询-键权重矩阵（查询和键矩阵乘积）的采样分布对学习某些自然目标函数的性能产生了影响。在某些情况下，随机采样 weights的非零均值可以使学习性能更好。实验结果证实了我们的理论发现，并进一步阐明了样本大小和目标函数的复杂度之间的交互关系。
</details></li>
</ul>
<hr>
<h2 id="Model-based-Offline-Reinforcement-Learning-with-Count-based-Conservatism"><a href="#Model-based-Offline-Reinforcement-Learning-with-Count-based-Conservatism" class="headerlink" title="Model-based Offline Reinforcement Learning with Count-based Conservatism"></a>Model-based Offline Reinforcement Learning with Count-based Conservatism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11352">http://arxiv.org/abs/2307.11352</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/oh-lab/count-morl">https://github.com/oh-lab/count-morl</a></li>
<li>paper_authors: Byeongchan Kim, Min-hwan Oh</li>
<li>for: 本研究提出了一种基于模型的离线再强化学习方法，称为$\texttt{Count-MORL}$。这种方法利用状态动作对的计数估计来衡量模型估计误差，是现有知识之最初的一种实现 counts-based conservatism 的算法。</li>
<li>methods: 我们首先证明了计数估计误差与状态动作对的频率之间是倒数相关关系。其次，我们示出了在使用 count-based 保守模型下学习的策略具有优化性能保证。</li>
<li>results: 通过广泛的数字实验，我们证明了 $\texttt{Count-MORL}$ 与哈希码实现在 D4RL 数据集上表现出色，至少比现有的离线RL算法更高。代码可以在 $\href{<a target="_blank" rel="noopener" href="https://github.com/oh-lab/Count-MORL%7D%7Bhttps://github.com/oh-lab/Count-MORL%7D$">https://github.com/oh-lab/Count-MORL}{https://github.com/oh-lab/Count-MORL}$</a> 上获取。<details>
<summary>Abstract</summary>
In this paper, we propose a model-based offline reinforcement learning method that integrates count-based conservatism, named $\texttt{Count-MORL}$. Our method utilizes the count estimates of state-action pairs to quantify model estimation error, marking the first algorithm of demonstrating the efficacy of count-based conservatism in model-based offline deep RL to the best of our knowledge. For our proposed method, we first show that the estimation error is inversely proportional to the frequency of state-action pairs. Secondly, we demonstrate that the learned policy under the count-based conservative model offers near-optimality performance guarantees. Through extensive numerical experiments, we validate that $\texttt{Count-MORL}$ with hash code implementation significantly outperforms existing offline RL algorithms on the D4RL benchmark datasets. The code is accessible at $\href{https://github.com/oh-lab/Count-MORL}{https://github.com/oh-lab/Count-MORL}$.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了基于模型的离线再强化学习方法，称为 $\texttt{Count-MORL}$。我们的方法利用状态动作对的计数估计来衡量模型估计误差，这是我们知道的离线深度学习中首个满足count-based conservatism的算法。我们首先示出了估计误差与状态动作对的频率之间是倒数相关的关系。其次，我们证明了我们所学习的策略在基于计数保守的模型下提供了近似优化性能保证。通过广泛的数值实验，我们证明了 $\texttt{Count-MORL}$ 在 D4RL  benchmark 数据集上与哈希码实现显著超越了现有的离线RL算法。代码可以在 $\href{https://github.com/oh-lab/Count-MORL}{https://github.com/oh-lab/Count-MORL}$ 中获取。
</details></li>
</ul>
<hr>
<h2 id="Bounded-P-values-in-Parametric-Programming-based-Selective-Inference"><a href="#Bounded-P-values-in-Parametric-Programming-based-Selective-Inference" class="headerlink" title="Bounded P-values in Parametric Programming-based Selective Inference"></a>Bounded P-values in Parametric Programming-based Selective Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11351">http://arxiv.org/abs/2307.11351</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shirara1016/bounded_p_values_in_si">https://github.com/shirara1016/bounded_p_values_in_si</a></li>
<li>paper_authors: Tomohiro Shiraishi, Daiki Miwa, Vo Nguyen Le Duy, Ichiro Takeuchi</li>
<li>for: 这个研究探讨了选择性推论（SI）作为 Statistical hypothesis testing 的框架，并提出了一种可靠地实现 SI 的方法。</li>
<li>methods: 本研究使用 Parametric programming-based SI (PP-based SI)，并提出了一种方法来计算 p-value 的上下限，以及三种搜寻策略来有效地提高这些上下限。</li>
<li>results: 研究显示，提出的方法可以实现高精度的选择性推论，并且可以实现高效的搜寻。<details>
<summary>Abstract</summary>
Selective inference (SI) has been actively studied as a promising framework for statistical hypothesis testing for data-driven hypotheses. The basic idea of SI is to make inferences conditional on an event that a hypothesis is selected. In order to perform SI, this event must be characterized in a traceable form. When selection event is too difficult to characterize, additional conditions are introduced for tractability. This additional conditions often causes the loss of power, and this issue is referred to as over-conditioning. Parametric programming-based SI (PP-based SI) has been proposed as one way to address the over-conditioning issue. The main problem of PP-based SI is its high computational cost due to the need to exhaustively explore the data space. In this study, we introduce a procedure to reduce the computational cost while guaranteeing the desired precision, by proposing a method to compute the upper and lower bounds of p-values. We also proposed three types of search strategies that efficiently improve these bounds. We demonstrate the effectiveness of the proposed method in hypothesis testing problems for feature selection in linear models and attention region identification in deep neural networks.
</details>
<details>
<summary>摘要</summary>
选择性推理（SI）已被 актив地研究，作为数据驱动假设测试的可能性框架。SI的基本想法是根据假设选择条件下进行推理。为了进行SI，这个事件必须能够可追溯性地表述。当选择事件太难以表述时，通常会引入额外条件以提高可行性。然而，这些额外条件通常会导致损失精度，这被称为过度条件。 Parametric programming-based SI (PP-based SI) 已被提议作为解决过度条件问题的方法。然而，PP-based SI 的主要问题是其高计算成本，因为需要枚举数据空间。在本研究中，我们介绍了一种方法来降低计算成本，保证所需的精度，通过计算 p-值 的上下限。我们还提出了三种搜索策略，可以有效地提高这些上下限。我们在线性模型中的特征选择和深度神经网络中的注意区域标识问题中示出了方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Improving-Transferability-of-Adversarial-Examples-via-Bayesian-Attacks"><a href="#Improving-Transferability-of-Adversarial-Examples-via-Bayesian-Attacks" class="headerlink" title="Improving Transferability of Adversarial Examples via Bayesian Attacks"></a>Improving Transferability of Adversarial Examples via Bayesian Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11334">http://arxiv.org/abs/2307.11334</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qizhang Li, Yiwen Guo, Xiaochen Yang, Wangmeng Zuo, Hao Chen</li>
<li>for: 提高防御机器学习模型的抗性性，使其能够更好地鲁棒性考验和抗击攻击。</li>
<li>methods:  incorporating Bayesian formulation into model parameters and input, 采用权重学习方法来权重考虑模型参数和输入的 posterior distribution。</li>
<li>results: 1) 将 Bayesian formulation扩展到输入和参数都能够joint地多样化，得到了显著提高的抗性性; 2) 通过高级近似方法来估计 posterior distribution over input，进一步提高了抗性性; 3) 提出了一种理性的方法来微调模型参数。<details>
<summary>Abstract</summary>
This paper presents a substantial extension of our work published at ICLR. Our ICLR work advocated for enhancing transferability in adversarial examples by incorporating a Bayesian formulation into model parameters, which effectively emulates the ensemble of infinitely many deep neural networks, while, in this paper, we introduce a novel extension by incorporating the Bayesian formulation into the model input as well, enabling the joint diversification of both the model input and model parameters. Our empirical findings demonstrate that: 1) the combination of Bayesian formulations for both the model input and model parameters yields significant improvements in transferability; 2) by introducing advanced approximations of the posterior distribution over the model input, adversarial transferability achieves further enhancement, surpassing all state-of-the-arts when attacking without model fine-tuning. Moreover, we propose a principled approach to fine-tune model parameters in such an extended Bayesian formulation. The derived optimization objective inherently encourages flat minima in the parameter space and input space. Extensive experiments demonstrate that our method achieves a new state-of-the-art on transfer-based attacks, improving the average success rate on ImageNet and CIFAR-10 by 19.14% and 2.08%, respectively, when comparing with our ICLR basic Bayesian method. We will make our code publicly available.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>The combination of Bayesian formulations for both the model input and model parameters leads to significant improvements in transferability.2. By introducing advanced approximations of the posterior distribution over the model input, adversarial transferability is further enhanced, surpassing all state-of-the-art results when attacking without model fine-tuning.Moreover, we propose a principled approach to fine-tune model parameters in this extended Bayesian formulation. The derived optimization objective inherently encourages flat minima in both the parameter space and input space. Extensive experiments demonstrate that our method achieves a new state-of-the-art on transfer-based attacks, improving the average success rate on ImageNet and CIFAR-10 by 19.14% and 2.08%, respectively, compared to our ICLR basic Bayesian method. Our code will be publicly available.</details></li>
</ol>
<hr>
<h2 id="Demystifying-Local-and-Global-Fairness-Trade-offs-in-Federated-Learning-Using-Partial-Information-Decomposition"><a href="#Demystifying-Local-and-Global-Fairness-Trade-offs-in-Federated-Learning-Using-Partial-Information-Decomposition" class="headerlink" title="Demystifying Local and Global Fairness Trade-offs in Federated Learning Using Partial Information Decomposition"></a>Demystifying Local and Global Fairness Trade-offs in Federated Learning Using Partial Information Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11333">http://arxiv.org/abs/2307.11333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Faisal Hamman, Sanghamitra Dutta</li>
<li>for: 本研究提供了一种信息理论视角来探讨联合学习（FL）中敏感属性（如性别、种族等）的公平负担。现有的研究主要关注全局公平（总模型差异）或本地公平（每个客户端模型差异），而不一定考虑这两者之间的负担。本研究缺乏全局和本地公平之间的交互关系的理解，以及全局和本地公平之间的负担是否存在。</li>
<li>methods: 本研究使用了一种名为 partial information decomposition（PID）的信息理论工具，first identify 联合学习中的三种不公平来源：唯一差异（Unique Disparity）、重复差异（Redundant Disparity）和遮盖差异（Masked Disparity）。使用可读性示例，我们示出了这三种差异如何导致全局和本地公平。这种剖析帮助我们 derive 全局和本地公平的基本限制和负担之间的交互关系，特别是在数据不均衡情况下。</li>
<li>results: 本研究通过实验使用标准 benchmark 数据集支持我们的理论发现。这项工作为联合学习中不公平负担问题提供了一种更加细致的理解，可以帮助选择本地不公平缓减技术，以及其在实践中的协调和效果。<details>
<summary>Abstract</summary>
In this paper, we present an information-theoretic perspective to group fairness trade-offs in federated learning (FL) with respect to sensitive attributes, such as gender, race, etc. Existing works mostly focus on either \emph{global fairness} (overall disparity of the model across all clients) or \emph{local fairness} (disparity of the model at each individual client), without always considering their trade-offs. There is a lack of understanding of the interplay between global and local fairness in FL, and if and when one implies the other. To address this gap, we leverage a body of work in information theory called partial information decomposition (PID) which first identifies three sources of unfairness in FL, namely, \emph{Unique Disparity}, \emph{Redundant Disparity}, and \emph{Masked Disparity}. Using canonical examples, we demonstrate how these three disparities contribute to global and local fairness. This decomposition helps us derive fundamental limits and trade-offs between global or local fairness, particularly under data heterogeneity, as well as, derive conditions under which one implies the other. We also present experimental results on benchmark datasets to support our theoretical findings. This work offers a more nuanced understanding of the sources of disparity in FL that can inform the use of local disparity mitigation techniques, and their convergence and effectiveness when deployed in practice.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种信息学方面的视角，用于描述 Federated Learning（FL）中敏感属性（如性别、种族等）的公平负担关系。现有的工作主要集中在全局公平（总模型差异）或本地公平（每个客户端模型差异），而未经常考虑这些关系的交互。lack of understanding of the interplay between global and local fairness in FL, and if and when one implies the other. To address this gap, we leverage a body of work in information theory called partial information decomposition (PID) which first identifies three sources of unfairness in FL, namely, Unique Disparity, Redundant Disparity, and Masked Disparity. Using canonical examples, we demonstrate how these three disparities contribute to global and local fairness. This decomposition helps us derive fundamental limits and trade-offs between global or local fairness, particularly under data heterogeneity, as well as, derive conditions under which one implies the other. We also present experimental results on benchmark datasets to support our theoretical findings. This work offers a more nuanced understanding of the sources of disparity in FL that can inform the use of local disparity mitigation techniques, and their convergence and effectiveness when deployed in practice.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Beyond-Convergence-Identifiability-of-Machine-Learning-and-Deep-Learning-Models"><a href="#Beyond-Convergence-Identifiability-of-Machine-Learning-and-Deep-Learning-Models" class="headerlink" title="Beyond Convergence: Identifiability of Machine Learning and Deep Learning Models"></a>Beyond Convergence: Identifiability of Machine Learning and Deep Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11332">http://arxiv.org/abs/2307.11332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reza Sameni</li>
<li>for:  investigate the notion of model parameter identifiability through a case study</li>
<li>methods:  utilize a deep neural network to attempt to estimate subject-wise parameters from motion sensor data</li>
<li>results:  certain parameters can be identified from the observation data, while others remain unidentifiable due to limitations of the experimental setup<details>
<summary>Abstract</summary>
Machine learning (ML) and deep learning models are extensively used for parameter optimization and regression problems. However, not all inverse problems in ML are ``identifiable,'' indicating that model parameters may not be uniquely determined from the available data and the data model's input-output relationship. In this study, we investigate the notion of model parameter identifiability through a case study focused on parameter estimation from motion sensor data. Utilizing a bipedal-spring mass human walk dynamics model, we generate synthetic data representing diverse gait patterns and conditions. Employing a deep neural network, we attempt to estimate subject-wise parameters, including mass, stiffness, and equilibrium leg length. The results show that while certain parameters can be identified from the observation data, others remain unidentifiable, highlighting that unidentifiability is an intrinsic limitation of the experimental setup, necessitating a change in data collection and experimental scenarios. Beyond this specific case study, the concept of identifiability has broader implications in ML and deep learning. Addressing unidentifiability requires proven identifiable models (with theoretical support), multimodal data fusion techniques, and advancements in model-based machine learning. Understanding and resolving unidentifiability challenges will lead to more reliable and accurate applications across diverse domains, transcending mere model convergence and enhancing the reliability of machine learning models.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）和深度学习模型在参数优化和回归问题中广泛应用。然而，不是所有机器学习 inverse problem 都是可识别的，表示模型参数可能不是数据和输入输出关系的唯一确定。在这项研究中，我们研究了机器学习模型参数可识别性的概念，通过人行徒步动力学模型的 случа study 来 investigate。我们生成了多种步态和条件的Synthetic数据，并使用深度神经网络来估算每个参与者的参数，包括质量、刚性和平衡脚长。结果表明，一些参数可以从观察数据中被确定，而其他参数则无法确定，这 highlights  эксперименталь设置中的内在限制，需要改变数据采集和实验方案。这种特定案例中的结论也有更广泛的应用在机器学习和深度学习中。解决不可识别性需要确定可识别模型（具有理论支持）、多Modal 数据融合技术和模型基于机器学习的进步。理解和解决不可识别性挑战将导致更可靠和准确的应用在多个领域，超越模型的极限并提高机器学习模型的可靠性。
</details></li>
</ul>
<hr>
<h2 id="Methodologies-for-Improving-Modern-Industrial-Recommender-Systems"><a href="#Methodologies-for-Improving-Modern-Industrial-Recommender-Systems" class="headerlink" title="Methodologies for Improving Modern Industrial Recommender Systems"></a>Methodologies for Improving Modern Industrial Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01204">http://arxiv.org/abs/2308.01204</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shusen Wang</li>
<li>for: 这篇论文是为了提高现代工业 recommender systems（RS）的方法ology而写的，特意是为了提高RS的适用率和持续时间。</li>
<li>methods: 论文使用了一些现代RS技术和策略，如物联网（IoT）、大数据、人工智能（AI）等，以提高RS的准确率和个性化程度。</li>
<li>results: 论文通过实践和优化现代RS技术和策略，提高了一些销售量、用户活跃率和持续时间等销售指标。<details>
<summary>Abstract</summary>
Recommender system (RS) is an established technology with successful applications in social media, e-commerce, entertainment, and more. RSs are indeed key to the success of many popular APPs, such as YouTube, Tik Tok, Xiaohongshu, Bilibili, and others. This paper explores the methodology for improving modern industrial RSs. It is written for experienced RS engineers who are diligently working to improve their key performance indicators, such as retention and duration. The experiences shared in this paper have been tested in some real industrial RSs and are likely to be generalized to other RSs as well. Most contents in this paper are industry experience without publicly available references.
</details>
<details>
<summary>摘要</summary>
“推荐系统（RS）是一种已经发展成熟的技术，在社交媒体、电子商务、娱乐等领域都有成功应用。RS是许多受欢迎的APP的关键，如 YouTube、Tik Tok、Xiaohongshu 和 Bilibili 等。本文将探讨现代工业RS的改进方法。这篇文章是为了经验丰富的 RS 工程师，以提高他们的关键性表现指标，如退货率和使用时间。文章中的经验都是在真实的工业RS中进行试验和验证的，可能会应用于其他RS中。大多数内容都是无公开的业界经验。”Note: Please keep in mind that the translation is in Simplified Chinese, which is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and other parts of the world.
</details></li>
</ul>
<hr>
<h2 id="Systematic-Adaptation-of-Communication-focused-Machine-Learning-Models-from-Real-to-Virtual-Environments-for-Human-Robot-Collaboration"><a href="#Systematic-Adaptation-of-Communication-focused-Machine-Learning-Models-from-Real-to-Virtual-Environments-for-Human-Robot-Collaboration" class="headerlink" title="Systematic Adaptation of Communication-focused Machine Learning Models from Real to Virtual Environments for Human-Robot Collaboration"></a>Systematic Adaptation of Communication-focused Machine Learning Models from Real to Virtual Environments for Human-Robot Collaboration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11327">http://arxiv.org/abs/2307.11327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Debasmita Mukherjee, Ritwik Singhai, Homayoun Najjaran</li>
<li>for: 这个论文的目的是如何将真实世界中的手势识别模型适应虚拟世界中，以便在虚拟境中使用手势来控制协作机器人。</li>
<li>methods: 这篇论文提出了一种系统性的框架，用于将真实世界中的手势识别模型适应虚拟世界中，并提供了一些指导方针 для创建受过约束的虚拟数据集。</li>
<li>results: 这篇论文通过实验表明，可以使用有限的虚拟数据集来适应手势识别模型，并且提供了一些适用于其他模式，如身体姿势和表情的指导方针。<details>
<summary>Abstract</summary>
Virtual reality has proved to be useful in applications in several fields ranging from gaming, medicine, and training to development of interfaces that enable human-robot collaboration. It empowers designers to explore applications outside of the constraints posed by the real world environment and develop innovative solutions and experiences. Hand gestures recognition which has been a topic of much research and subsequent commercialization in the real world has been possible because of the creation of large, labelled datasets. In order to utilize the power of natural and intuitive hand gestures in the virtual domain for enabling embodied teleoperation of collaborative robots, similarly large datasets must be created so as to keep the working interface easy to learn and flexible enough to add more gestures. Depending on the application, this may be computationally or economically prohibitive. Thus, the adaptation of trained deep learning models that perform well in the real environment to the virtual may be a solution to this challenge. This paper presents a systematic framework for the real to virtual adaptation using limited size of virtual dataset along with guidelines for creating a curated dataset. Finally, while hand gestures have been considered as the communication mode, the guidelines and recommendations presented are generic. These are applicable to other modes such as body poses and facial expressions which have large datasets available in the real domain which must be adapted to the virtual one.
</details>
<details>
<summary>摘要</summary>
虚拟现实已经在多个领域展示了其使用价值，包括游戏、医疗、训练和人机合作 интерфей斯的开发。它让设计师可以在虚拟环境中探索不受实际环境限制的应用程序，并开发创新的解决方案和经验。手势认识是许多研究和商业化的话题之一，在真实世界中已经实现了。为了在虚拟域中使用自然和直观的手势，需要创建大量标注的数据集。这可能是计算机或经济上的瓶颈。因此，将已经在真实环境中训练好的深度学习模型到虚拟环境中进行适应可能是一个解决方案。这篇文章提出了一个系统化的框架，用于在有限大小的虚拟数据集上进行真实到虚拟的适应，以及创建标注数据集的指南。最后，手势被视为通信模式，但是这些指南和建议是通用的，适用于其他模式，如身体姿态和表情，它们在真实世界中有大量数据集可以进行适应。
</details></li>
</ul>
<hr>
<h2 id="Analysis-of-Elephant-Movement-in-Sub-Saharan-Africa-Ecological-Climatic-and-Conservation-Perspectives"><a href="#Analysis-of-Elephant-Movement-in-Sub-Saharan-Africa-Ecological-Climatic-and-Conservation-Perspectives" class="headerlink" title="Analysis of Elephant Movement in Sub-Saharan Africa: Ecological, Climatic, and Conservation Perspectives"></a>Analysis of Elephant Movement in Sub-Saharan Africa: Ecological, Climatic, and Conservation Perspectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11325">http://arxiv.org/abs/2307.11325</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew Hines, Gregory Glatzer, Shreya Ghosh, Prasenjit Mitra</li>
<li>for: 这项研究旨在更好地理解非洲亚洲大陆的大象移动行为，以便为保护大象和人类之间的共存提供有效的策略。</li>
<li>methods: 该研究使用分析方法来探索大象移动行为的复杂关系，包括季节变化和降水征 cycles。</li>
<li>results: 研究发现了大象移动行为受到季节变化和降水征 cycles的影响，并提供了一种可预测大象移动路径的方法，这些结果可以用于规划有效的保护策略，降低人类和大象之间的冲突，有效管理土地使用，加强反贼活动。<details>
<summary>Abstract</summary>
The interaction between elephants and their environment has profound implications for both ecology and conservation strategies. This study presents an analytical approach to decipher the intricate patterns of elephant movement in Sub-Saharan Africa, concentrating on key ecological drivers such as seasonal variations and rainfall patterns. Despite the complexities surrounding these influential factors, our analysis provides a holistic view of elephant migratory behavior in the context of the dynamic African landscape. Our comprehensive approach enables us to predict the potential impact of these ecological determinants on elephant migration, a critical step in establishing informed conservation strategies. This projection is particularly crucial given the impacts of global climate change on seasonal and rainfall patterns, which could substantially influence elephant movements in the future. The findings of our work aim to not only advance the understanding of movement ecology but also foster a sustainable coexistence of humans and elephants in Sub-Saharan Africa. By predicting potential elephant routes, our work can inform strategies to minimize human-elephant conflict, effectively manage land use, and enhance anti-poaching efforts. This research underscores the importance of integrating movement ecology and climatic variables for effective wildlife management and conservation planning.
</details>
<details>
<summary>摘要</summary>
elephants 和它们的环境之间的互动对生态和保护策略有深刻的影响。这项研究提出了一种分析方法，用于揭示非洲亚洲地区 elephant 的移动模式，关注关键的生态因素，如季节变化和降水模式。尽管这些因素具有复杂的交互，但我们的分析提供了一个整体的视角，用于解释非洲风景中 elephant 的移动行为。我们的全面方法可以预测 elephant 移动的可能影响，这是为建立有知识基础的保护策略提供了关键的一步。这些预测对于未来由全球气候变化引起的季节和降水模式的变化具有重要意义。我们的研究目标是不仅提高生态运动学的理解，而且促进人类和 elephant 之间的可持续共生。我们的工作可以预测可能的 elephant 路线，以便为人类- elephant 冲突的避免、地用规划和反贼斗斗提供有用的信息。这项研究强调了将生态运动学和气候变化 integrate 到野生动物管理和保护规划中的重要性。
</details></li>
</ul>
<hr>
<h2 id="XLDA-Linear-Discriminant-Analysis-for-Scaling-Continual-Learning-to-Extreme-Classification-at-the-Edge"><a href="#XLDA-Linear-Discriminant-Analysis-for-Scaling-Continual-Learning-to-Extreme-Classification-at-the-Edge" class="headerlink" title="XLDA: Linear Discriminant Analysis for Scaling Continual Learning to Extreme Classification at the Edge"></a>XLDA: Linear Discriminant Analysis for Scaling Continual Learning to Extreme Classification at the Edge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11317">http://arxiv.org/abs/2307.11317</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karan Shah, Vishruth Veerendranath, Anushka Hebbar, Raghavendra Bhat</li>
<li>for: 这篇论文目的是在边缘部署中进行分类学习，特别是在极端分类场景下进行测试。</li>
<li>methods: 这篇论文使用了Streaming Linear Discriminant Analysis（LDA），并且在边缘部署中进行了证明，以确保LDA可以在极端分类场景下进行正确的分类。</li>
<li>results: 论文的结果显示，使用XLDA框架可以在边缘部署中实现极端分类场景下的高效分类，并且可以在有限的计算资源下进行训练和执行。在 AliProducts 和 Google Landmarks V2 等极端分类 dataset 上，可以达到42倍的训练速度和5倍的执行速度。<details>
<summary>Abstract</summary>
Streaming Linear Discriminant Analysis (LDA) while proven in Class-incremental Learning deployments at the edge with limited classes (upto 1000), has not been proven for deployment in extreme classification scenarios. In this paper, we present: (a) XLDA, a framework for Class-IL in edge deployment where LDA classifier is proven to be equivalent to FC layer including in extreme classification scenarios, and (b) optimizations to enable XLDA-based training and inference for edge deployment where there is a constraint on available compute resources. We show up to 42x speed up using a batched training approach and up to 5x inference speedup with nearest neighbor search on extreme datasets like AliProducts (50k classes) and Google Landmarks V2 (81k classes)
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Streaming Linear Discriminant Analysis (LDA) while proven in Class-incremental Learning deployments at the edge with limited classes (upto 1000), has not been proven for deployment in extreme classification scenarios. In this paper, we present: (a) XLDA, a framework for Class-IL in edge deployment where LDA classifier is proven to be equivalent to FC layer including in extreme classification scenarios, and (b) optimizations to enable XLDA-based training and inference for edge deployment where there is a constraint on available compute resources. We show up to 42x speed up using a batched training approach and up to 5x inference speedup with nearest neighbor search on extreme datasets like AliProducts (50k classes) and Google Landmarks V2 (81k classes)" into Simplified Chinese.翻译文本 "Streaming Linear Discriminant Analysis (LDA) 在边缘部署中证明了限制类数（最多1000）的Class-incremental Learning场景中的可靠性，但在极端分类场景中尚未证明。本文提出了：(a) XLDA，一个用于边缘部署的 Class-IL 框架，其中LDA类ifier与极端分类场景中的FC层等价；以及(b) 用于实现 XLDA 的训练和推理的优化，其中包括限制可用计算资源。我们在极端数据集如 AliProducts（50k 类）和 Google Landmarks V2（81k 类）上达到了最高的42倍快速训练方法和最高的5倍快速推理 nearest neighbor search。
</details></li>
</ul>
<hr>
<h2 id="Making-Pre-trained-Language-Models-both-Task-solvers-and-Self-calibrators"><a href="#Making-Pre-trained-Language-Models-both-Task-solvers-and-Self-calibrators" class="headerlink" title="Making Pre-trained Language Models both Task-solvers and Self-calibrators"></a>Making Pre-trained Language Models both Task-solvers and Self-calibrators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11316">http://arxiv.org/abs/2307.11316</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yangyi-chen/lm-toast">https://github.com/yangyi-chen/lm-toast</a></li>
<li>paper_authors: Yangyi Chen, Xingyao Wang, Heng Ji</li>
<li>for: 本研究想要让语言模型（PLM）在高风险应用中具有合理的信任度估计，而不是PLM的默认信任分数。</li>
<li>methods: 本研究提出了一种LM-TOAST训练算法，用于解决有限训练样本、数据不均衡和分布转移等三个挑战。</li>
<li>results: 实验结果表明，LM-TOAST可以有效地利用训练数据，使PLM具有合理的信任度估计，同时保持原始任务性能。此外，研究还应用了LM-TOAST在选择性分类、防御式攻击和模型堆叠等三个下游应用中。<details>
<summary>Abstract</summary>
Pre-trained language models (PLMs) serve as backbones for various real-world systems. For high-stake applications, it's equally essential to have reasonable confidence estimations in predictions. While the vanilla confidence scores of PLMs can already be effectively utilized, PLMs consistently become overconfident in their wrong predictions, which is not desirable in practice. Previous work shows that introducing an extra calibration task can mitigate this issue. The basic idea involves acquiring additional data to train models in predicting the confidence of their initial predictions. However, it only demonstrates the feasibility of this kind of method, assuming that there are abundant extra available samples for the introduced calibration task. In this work, we consider the practical scenario that we need to effectively utilize training samples to make PLMs both task-solvers and self-calibrators. Three challenges are presented, including limited training samples, data imbalance, and distribution shifts. We first conduct pilot experiments to quantify various decisive factors in the calibration task. Based on the empirical analysis results, we propose a training algorithm LM-TOAST to tackle the challenges. Experimental results show that LM-TOAST can effectively utilize the training data to make PLMs have reasonable confidence estimations while maintaining the original task performance. Further, we consider three downstream applications, namely selective classification, adversarial defense, and model cascading, to show the practical usefulness of LM-TOAST. The code will be made public at \url{https://github.com/Yangyi-Chen/LM-TOAST}.
</details>
<details>
<summary>摘要</summary>
预训言语模型（PLM）作为各种实际系统的基础，在高资产应用中也非常重要。在预测中，保持合理的自信度是非常重要的。然而，PLM通常在错误预测时变得过自信，这并不是实际应用中的理想情况。前一些研究表明，通过添加额外的核心任务可以解决这个问题。基本思路是通过训练模型预测其初始预测的自信度。然而，这只是假设有充足的额外样本可以进行这种类型的方法。在这种实践中，我们需要有效地利用训练样本，使PLM同时成为任务解决器和自我调整器。我们提出了三个挑战，包括有限的训练样本、数据不均衡和分布shift。我们首先进行了预测实验，以量化各种决定性因素。基于实验分析结果，我们提出了一种训练算法LM-TOAST，以解决这些挑战。实验结果表明，LM-TOAST可以有效地利用训练数据，使PLM有合理的自信度估计，同时保持原始任务性能。此外，我们考虑了三个下游应用，包括选择性分类、对抗防御和模型堆叠，以示LM-TOAST的实际用途。代码将于 \url{https://github.com/Yangyi-Chen/LM-TOAST} 公开。
</details></li>
</ul>
<hr>
<h2 id="Artificial-Intelligence-Generated-Terahertz-Multi-Resonant-Metasurfaces-via-Improved-Transformer-and-CGAN-Neural-Networks"><a href="#Artificial-Intelligence-Generated-Terahertz-Multi-Resonant-Metasurfaces-via-Improved-Transformer-and-CGAN-Neural-Networks" class="headerlink" title="Artificial Intelligence-Generated Terahertz Multi-Resonant Metasurfaces via Improved Transformer and CGAN Neural Networks"></a>Artificial Intelligence-Generated Terahertz Multi-Resonant Metasurfaces via Improved Transformer and CGAN Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11794">http://arxiv.org/abs/2307.11794</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yangpeng Huang, Naixing Feng, Yijun Cai</li>
<li>for: 本研究旨在提高深度学习网络（DNN）为tera兆HERTZ（THz）多晶质graphene表面设计的 inverse design 能力。</li>
<li>methods: 本研究提出了改进的Transformer和条件生成敌对网络（CGAN）来实现THz多晶质graphene表面设计基于吸收谱。改进的Transformer可以在StoV（spectrum to vector）设计中获得更高的准确率和泛化性能，而StoI（spectrum to image）设计通过CGAN可以提供更全面的信息和更高的准确率than MLP。</li>
<li>results: 本研究发现，改进的CGAN可以直接从愿望的多晶质graphene表面吸收谱中设计图像。这种方法可以大大提高人工智能生成的表面设计（AIGM）的设计过程，并提供了开发复杂THz表面 based on 2D材料使用生成神经网络的有用指南。<details>
<summary>Abstract</summary>
It is well known that the inverse design of terahertz (THz) multi-resonant graphene metasurfaces by using traditional deep neural networks (DNNs) has limited generalization ability. In this paper, we propose improved Transformer and conditional generative adversarial neural networks (CGAN) for the inverse design of graphene metasurfaces based upon THz multi-resonant absorption spectra. The improved Transformer can obtain higher accuracy and generalization performance in the StoV (Spectrum to Vector) design compared to traditional multilayer perceptron (MLP) neural networks, while the StoI (Spectrum to Image) design achieved through CGAN can provide more comprehensive information and higher accuracy than the StoV design obtained by MLP. Moreover, the improved CGAN can achieve the inverse design of graphene metasurface images directly from the desired multi-resonant absorption spectra. It is turned out that this work can finish facilitating the design process of artificial intelligence-generated metasurfaces (AIGM), and even provide a useful guide for developing complex THz metasurfaces based on 2D materials using generative neural networks.
</details>
<details>
<summary>摘要</summary>
znscr.com/i/1537085489818328Please note that the above link is a simplified Chinese text and not the actual translation of the given text. To see the actual translation, please copy the text and paste it into a Simplified Chinese text reader or translator.
</details></li>
</ul>
<hr>
<h2 id="Neuromorphic-Online-Learning-for-Spatiotemporal-Patterns-with-a-Forward-only-Timeline"><a href="#Neuromorphic-Online-Learning-for-Spatiotemporal-Patterns-with-a-Forward-only-Timeline" class="headerlink" title="Neuromorphic Online Learning for Spatiotemporal Patterns with a Forward-only Timeline"></a>Neuromorphic Online Learning for Spatiotemporal Patterns with a Forward-only Timeline</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11314">http://arxiv.org/abs/2307.11314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenhang Zhang, Jingang Jin, Haowen Fang, Qinru Qiu</li>
<li>for: 这个论文的目的是为了在适用于嵌入式应用的在线学习中提高神经网络模型的性能。</li>
<li>methods: 这个论文使用的方法是一种名为Spatiotemporal Online Learning for Synaptic Adaptation（SOLSA），它是特地为神经网络模型（LIF neurons）和其相关的突触Synaptic Adaptation进行在线学习的。</li>
<li>results: 相比BPTT算法，SOLSA具有较低的内存需求，并且可以更好地平衡时间工作负荷。此外，SOLSA还包括了加强技术，如域内约束、早期停止训练和自适应突触筛选器，这些技术可以加速启示速度和提高学习性能。相比之下，SOLSA与其他非BPTT基于的SNN学习方法之间的平均学习精度提高为14.2%，而与BPTT相比，SOLSA的平均学习精度高于BPTT的5%，同时内存成本下降72%。<details>
<summary>Abstract</summary>
Spiking neural networks (SNNs) are bio-plausible computing models with high energy efficiency. The temporal dynamics of neurons and synapses enable them to detect temporal patterns and generate sequences. While Backpropagation Through Time (BPTT) is traditionally used to train SNNs, it is not suitable for online learning of embedded applications due to its high computation and memory cost as well as extended latency. Previous works have proposed online learning algorithms, but they often utilize highly simplified spiking neuron models without synaptic dynamics and reset feedback, resulting in subpar performance. In this work, we present Spatiotemporal Online Learning for Synaptic Adaptation (SOLSA), specifically designed for online learning of SNNs composed of Leaky Integrate and Fire (LIF) neurons with exponentially decayed synapses and soft reset. The algorithm not only learns the synaptic weight but also adapts the temporal filters associated to the synapses. Compared to the BPTT algorithm, SOLSA has much lower memory requirement and achieves a more balanced temporal workload distribution. Moreover, SOLSA incorporates enhancement techniques such as scheduled weight update, early stop training and adaptive synapse filter, which speed up the convergence and enhance the learning performance. When compared to other non-BPTT based SNN learning, SOLSA demonstrates an average learning accuracy improvement of 14.2%. Furthermore, compared to BPTT, SOLSA achieves a 5% higher average learning accuracy with a 72% reduction in memory cost.
</details>
<details>
<summary>摘要</summary>
神经网络（SNN）是生物可能的计算模型，具有高能效性。neurons和synapses的时间动态让它们能够检测时间序列和生成序列。而传统的Backpropagation Through Time（BPTT）被用来训练SNN，但它不适用于在嵌入式应用程序上线学习因为其高计算和内存成本以及延迟。先前的工作已经提出了在线学习算法，但它们通常使用简化的神经元模型和synaptic dynamics，导致性能不佳。在这项工作中，我们提出了Spatiotemporal Online Learning for Synaptic Adaptation（SOLSA），特意设计为在线学习SNN，其中包括泄漏 integrate and Fire（LIF）神经元和快速衰减的synapses。算法不仅学习synaptic weight，还适应相应的时间滤波器。相比BPTT算法，SOLSA具有远低的内存需求，并实现了更平衡的时间工作负荷分布。此外，SOLSA还包括了提高技术，例如计划的weight更新、早停训练和自适应synapse滤波器，这些技术能够加速启示和提高学习性能。与其他非BPTT基于SNN学习相比，SOLSA表现出14.2%的均值学习精度提高。此外，与BPTT相比，SOLSA实现了5%更高的均值学习精度，但内存成本减少了72%。
</details></li>
</ul>
<hr>
<h2 id="Who-should-I-Collaborate-with-A-Comparative-Study-of-Academia-and-Industry-Research-Collaboration-in-NLP"><a href="#Who-should-I-Collaborate-with-A-Comparative-Study-of-Academia-and-Industry-Research-Collaboration-in-NLP" class="headerlink" title="Who should I Collaborate with? A Comparative Study of Academia and Industry Research Collaboration in NLP"></a>Who should I Collaborate with? A Comparative Study of Academia and Industry Research Collaboration in NLP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04524">http://arxiv.org/abs/2308.04524</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hussain Sadiq Abuwala, Bohan Zhang, Mushi Wang</li>
<li>for: 研究了学术界与产业界合作对自然语言处理（NLP）的影响。</li>
<li>methods: 创建了一个管道来提取 NLP 论文中的姓名和引用，并将其分为三类：学术界、产业界和 hybrid（学术界与产业界合作）。</li>
<li>results: 发现了协作类论文的发表量在增加趋势，同时这些论文在影响力上也高于solely在学术界发表的论文。<details>
<summary>Abstract</summary>
The goal of our research was to investigate the effects of collaboration between academia and industry on Natural Language Processing (NLP). To do this, we created a pipeline to extract affiliations and citations from NLP papers and divided them into three categories: academia, industry, and hybrid (collaborations between academia and industry). Our empirical analysis found that there is a trend towards an increase in industry and academia-industry collaboration publications and that these types of publications tend to have a higher impact compared to those produced solely within academia.
</details>
<details>
<summary>摘要</summary>
我们的研究目标是研究学术和产业合作对自然语言处理（NLP）的影响。为此，我们创建了一个管道，EXTRACT afilliliations和引用 FROM NLP论文，并将其分为三类：学术、产业和 hybrid（学术和产业合作）。我们的实证分析发现，有一个趋势，即学术和产业合作出版物的数量在增加，而且这些类型的出版物具有较高的影响力，比 Solo 在学术中发表的论文更高。
</details></li>
</ul>
<hr>
<h2 id="PI-VEGAN-Physics-Informed-Variational-Embedding-Generative-Adversarial-Networks-for-Stochastic-Differential-Equations"><a href="#PI-VEGAN-Physics-Informed-Variational-Embedding-Generative-Adversarial-Networks-for-Stochastic-Differential-Equations" class="headerlink" title="PI-VEGAN: Physics Informed Variational Embedding Generative Adversarial Networks for Stochastic Differential Equations"></a>PI-VEGAN: Physics Informed Variational Embedding Generative Adversarial Networks for Stochastic Differential Equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11289">http://arxiv.org/abs/2307.11289</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruisong Gao, Yufeng Wang, Min Yang, Chuanjun Chen</li>
<li>for: 解决随机 diffe方程的前向、反向、混合问题，只有部分系统参数的感知数据available。</li>
<li>methods:  integrate governing physical laws into PI-VEGAN with automatic differentiation, introduce variational encoder to approximate latent variables, use generator to learn characteristics of stochastic partial equations.</li>
<li>results:  compared with previous PI-WGAN, PI-VEGAN achieves satisfactory stability and accuracy in solving forward, inverse, and mixed problems of stochastic differential equations.<details>
<summary>Abstract</summary>
We present a new category of physics-informed neural networks called physics informed variational embedding generative adversarial network (PI-VEGAN), that effectively tackles the forward, inverse, and mixed problems of stochastic differential equations. In these scenarios, the governing equations are known, but only a limited number of sensor measurements of the system parameters are available. We integrate the governing physical laws into PI-VEGAN with automatic differentiation, while introducing a variational encoder for approximating the latent variables of the actual distribution of the measurements. These latent variables are integrated into the generator to facilitate accurate learning of the characteristics of the stochastic partial equations. Our model consists of three components, namely the encoder, generator, and discriminator, each of which is updated alternatively employing the stochastic gradient descent algorithm. We evaluate the effectiveness of PI-VEGAN in addressing forward, inverse, and mixed problems that require the concurrent calculation of system parameters and solutions. Numerical results demonstrate that the proposed method achieves satisfactory stability and accuracy in comparison with the previous physics-informed generative adversarial network (PI-WGAN).
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的物理学 Informed neural network，即物理学 Informed Variational Embedding Generative Adversarial Network (PI-VEGAN)，可以有效地解决 Stochastic Differential Equations 中的前向、反向和混合问题。在这些情况下，系统参数的 governing 方程是知道的，但只有一个有限多少感知器的系统参数是可用的。我们将物理学 Informed PI-VEGAN 中的物理法律与自动梯度法相结合，并引入了一个变量编码器来近似实际分布中的测量变量。这些测量变量被引入到生成器中，以便准确地学习 Stochastic Partial Equations 中的特征。我们的模型由三部分组成：编码器、生成器和检测器，每个部分都在使用随机梯度下降算法进行更新。我们通过对前向、反向和混合问题进行同时计算系统参数和解的方法进行评估。numerical 结果表明，我们的方法可以与之前的物理学 Informed WGAN (PI-WGAN) 相比，在稳定性和准确性方面达到了满意的效果。
</details></li>
</ul>
<hr>
<h2 id="Kernelized-Offline-Contextual-Dueling-Bandits"><a href="#Kernelized-Offline-Contextual-Dueling-Bandits" class="headerlink" title="Kernelized Offline Contextual Dueling Bandits"></a>Kernelized Offline Contextual Dueling Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11288">http://arxiv.org/abs/2307.11288</a></li>
<li>repo_url: None</li>
<li>paper_authors: Viraj Mehta, Ojash Neopane, Vikramjeet Das, Sen Lin, Jeff Schneider, Willie Neiswanger</li>
<li>for: 这个论文主要针对哪个问题？	+ answer: 这个论文主要针对 preference-based 反馈的应用问题，例如人工智能学习从人类反馈中获得奖励函数的问题。</li>
<li>methods: 这个论文使用了哪些方法？	+ answer: 这个论文使用了上下文选择的方法，即在获取人类反馈时选择合适的上下文，以最大化Policy的评估。具体来说，这个论文使用了上下文战略和上下文战略的组合。</li>
<li>results: 这个论文获得了哪些结果？	+ answer: 这个论文提供了一种基于上下文选择的上下文战略，并证明了这种方法可以减少对人类反馈的成本。此外，这个论文还提供了一些实验证明，表明这种方法可以在实际应用中提高效率。<details>
<summary>Abstract</summary>
Preference-based feedback is important for many applications where direct evaluation of a reward function is not feasible. A notable recent example arises in reinforcement learning from human feedback on large language models. For many of these applications, the cost of acquiring the human feedback can be substantial or even prohibitive. In this work, we take advantage of the fact that often the agent can choose contexts at which to obtain human feedback in order to most efficiently identify a good policy, and introduce the offline contextual dueling bandit setting. We give an upper-confidence-bound style algorithm for this setting and prove a regret bound. We also give empirical confirmation that this method outperforms a similar strategy that uses uniformly sampled contexts.
</details>
<details>
<summary>摘要</summary>
preference-based 反馈是许多应用程序中非常重要的。一个最近的例子是从人类反馈中学习大语言模型。许多这些应用程序中，人类反馈的成本可能很高或甚至是不可接受的。在这种情况下，我们利用agent可以选择收集人类反馈的上下文，以便最有效地确定一个好策略，并引入了线上上下文战斗式带宽设定。我们提供了一种上限信息级别样式的算法，并证明了一个违和 bound。我们还提供了实验证明，这种方法在比用 uniformly 随机上下文时表现更好。
</details></li>
</ul>
<hr>
<h2 id="MAS-Towards-Resource-Efficient-Federated-Multiple-Task-Learning"><a href="#MAS-Towards-Resource-Efficient-Federated-Multiple-Task-Learning" class="headerlink" title="MAS: Towards Resource-Efficient Federated Multiple-Task Learning"></a>MAS: Towards Resource-Efficient Federated Multiple-Task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11285">http://arxiv.org/abs/2307.11285</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiming Zhuang, Yonggang Wen, Lingjuan Lyu, Shuai Zhang</li>
<li>for: 这个研究旨在开发一个可以同时训练多个分布式机器学习任务的系统，以提高边缘设备上的模型训练效能。</li>
<li>methods: 这个系统使用了一种新的merge和split方法来协调多个同时进行的分布式机器学习任务，包括将多个任务融合为一个所有任务的多任务架构，然后在训练后分成两个或更多的任务。</li>
<li>results: 实验结果显示，这个系统可以比其他方法更高效地训练多个分布式机器学习任务，具体来说，可以降低训练时间2倍，并降低能源消耗40%。<details>
<summary>Abstract</summary>
Federated learning (FL) is an emerging distributed machine learning method that empowers in-situ model training on decentralized edge devices. However, multiple simultaneous FL tasks could overload resource-constrained devices. In this work, we propose the first FL system to effectively coordinate and train multiple simultaneous FL tasks. We first formalize the problem of training simultaneous FL tasks. Then, we present our new approach, MAS (Merge and Split), to optimize the performance of training multiple simultaneous FL tasks. MAS starts by merging FL tasks into an all-in-one FL task with a multi-task architecture. After training for a few rounds, MAS splits the all-in-one FL task into two or more FL tasks by using the affinities among tasks measured during the all-in-one training. It then continues training each split of FL tasks based on model parameters from the all-in-one training. Extensive experiments demonstrate that MAS outperforms other methods while reducing training time by 2x and reducing energy consumption by 40%. We hope this work will inspire the community to further study and optimize training simultaneous FL tasks.
</details>
<details>
<summary>摘要</summary>
协同学习（FL）是一种在分布式机器学习方法中训练 Edge 设备上的模型的新趋势。然而，多个同时进行 FL 任务可能会过载具有限制的设备资源。在这项工作中，我们提出了首个能够有效地协调和训练多个同时进行 FL 任务的 FL 系统。我们首先正式定义同时进行 FL 任务的训练问题。然后，我们提出了我们的新方法（MAS），它可以优化训练多个同时进行 FL 任务的性能。MAS 开始是将多个 FL 任务合并为一个总体 FL 任务，并使用多任务架构进行训练。在训练几回后，MAS 会将总体 FL 任务分解成两个或更多的 FL 任务，基于在总体训练中测量的任务之间的相互关系。然后，它会继续基于每个分解的 FL 任务进行训练。我们的实验证明，MAS 可以比其他方法提高性能，同时降低训练时间和能耗。我们希望这项工作能够激励社区更加深入研究和优化同时进行 FL 任务的训练。
</details></li>
</ul>
<hr>
<h2 id="Epsilon-Privacy-Metric-for-Machine-Learning-Models"><a href="#Epsilon-Privacy-Metric-for-Machine-Learning-Models" class="headerlink" title="Epsilon*: Privacy Metric for Machine Learning Models"></a>Epsilon*: Privacy Metric for Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11280">http://arxiv.org/abs/2307.11280</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diana M. Negoescu, Humberto Gonzalez, Saad Eddin Al Orjany, Jilei Yang, Yuliia Lut, Rahul Tandra, Xiaowen Zhang, Xinyi Zheng, Zach Douglas, Vidita Nolkha, Parvez Ahammad, Gennady Samorodnitsky</li>
<li>For: This paper introduces a new privacy metric called Epsilon* to measure the privacy risk of a single model instance before, during, or after deployment of privacy mitigation strategies.* Methods: The metric does not require access to the training data or model training algorithm, and is based on a hypothesis test used by an adversary in a membership inference attack.* Results: The paper shows that Epsilon* is sensitive to privacy risk mitigation by training with differential privacy (DP), and can reduce the value of Epsilon* by up to 800% compared to non-DP trained baseline models. This allows privacy auditors to be independent of model owners and enables decision-makers to visualize the privacy-utility landscape to make informed decisions.In Simplified Chinese text, the three points would be:*  для: 这篇论文提出了一种新的隐私度量标准called Epsilon<em>，用于在模型实例之前、 durante或者后部署隐私控制策略时测量隐私风险。</em> 方法: Epsilon* metric不需要训练数据或模型训练算法的存在，基于一种可能攻击者在成员推测攻击中使用的假设测试。* 结果: 论文显示，在使用权限隐私（DP）训练时，Epsilon* 可以减少至基eline模型的800%，这使得隐私审计人可以独立于模型所有者，并让决策者可以查看隐私Utility 图表，以便做出了知情的决策。<details>
<summary>Abstract</summary>
We introduce Epsilon*, a new privacy metric for measuring the privacy risk of a single model instance prior to, during, or after deployment of privacy mitigation strategies. The metric does not require access to the training data sampling or model training algorithm. Epsilon* is a function of true positive and false positive rates in a hypothesis test used by an adversary in a membership inference attack. We distinguish between quantifying the privacy loss of a trained model instance and quantifying the privacy loss of the training mechanism which produces this model instance. Existing approaches in the privacy auditing literature provide lower bounds for the latter, while our metric provides a lower bound for the former by relying on an (${\epsilon}$,${\delta}$)-type of quantification of the privacy of the trained model instance. We establish a relationship between these lower bounds and show how to implement Epsilon* to avoid numerical and noise amplification instability. We further show in experiments on benchmark public data sets that Epsilon* is sensitive to privacy risk mitigation by training with differential privacy (DP), where the value of Epsilon* is reduced by up to 800% compared to the Epsilon* values of non-DP trained baseline models. This metric allows privacy auditors to be independent of model owners, and enables all decision-makers to visualize the privacy-utility landscape to make informed decisions regarding the trade-offs between model privacy and utility.
</details>
<details>
<summary>摘要</summary>
我们引入ε*，一个新的隐私度量表，用于在模型部署前、部署期间或部署后衡量模型实例的隐私风险。ε*不需要训练数据抽样或模型训练算法的存取。ε*是一个基于伪阳性率和伪阴性率的假设测试中的一个隐私度量。我们区分了衡量训练模型实例的隐私损失和衡量训练机制生成此模型实例的隐私损失。现有的隐私审核文献提供了训练机制生成此模型实例的隐私损失的下限，而我们的度量则提供了训练模型实例的隐私损失的下限，通过($\epsilon$,$\delta$)-型的隐私评估。我们建立了这两个下限之间的关系，并示出如何实现ε*以避免数据和杂音增幅不稳定。我们还在使用数据加密（DP）训练的公共数据集上进行实验，发现ε*值可以降低至800%以上 compared to非DP训练基eline模型的ε*值。这个度量使得隐私审核人员可以独立于模型所有者，并让所有决策者可以在隐私和用途之间做出了 Informed Decision。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Segment-from-Noisy-Annotations-A-Spatial-Correction-Approach"><a href="#Learning-to-Segment-from-Noisy-Annotations-A-Spatial-Correction-Approach" class="headerlink" title="Learning to Segment from Noisy Annotations: A Spatial Correction Approach"></a>Learning to Segment from Noisy Annotations: A Spatial Correction Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02498">http://arxiv.org/abs/2308.02498</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/michaelofsbu/spatialcorrection">https://github.com/michaelofsbu/spatialcorrection</a></li>
<li>paper_authors: Jiachen Yao, Yikai Zhang, Songzhu Zheng, Mayank Goswami, Prateek Prasanna, Chao Chen</li>
<li>for: 本研究旨在提高深度神经网络（DNNs）在医学图像分割任务中的表现，通过处理精度低的注释。</li>
<li>methods: 本研究提出了一种基于Markov模型的图像分割注释隐含抑制方法，该方法可以考虑图像分割注释中的空间相关性和偏见。</li>
<li>results: 实验表明，本研究的方法在synthetic和实际噪声注释上都有出色的表现，比现有的状态之势法要好。<details>
<summary>Abstract</summary>
Noisy labels can significantly affect the performance of deep neural networks (DNNs). In medical image segmentation tasks, annotations are error-prone due to the high demand in annotation time and in the annotators' expertise. Existing methods mostly assume noisy labels in different pixels are \textit{i.i.d}. However, segmentation label noise usually has strong spatial correlation and has prominent bias in distribution. In this paper, we propose a novel Markov model for segmentation noisy annotations that encodes both spatial correlation and bias. Further, to mitigate such label noise, we propose a label correction method to recover true label progressively. We provide theoretical guarantees of the correctness of the proposed method. Experiments show that our approach outperforms current state-of-the-art methods on both synthetic and real-world noisy annotations.
</details>
<details>
<summary>摘要</summary>
噪音标签可以很大程度上影响深度神经网络（DNNs）的性能。在医疗图像分割任务中，注释具有较高的错误率，主要因为注释时间的限制和注释者的专业知识需求。现有方法大多都假设不同像素的噪音标签是独立的。然而，分割标签噪音通常具有强的空间相关性和明显的偏见。在这篇论文中，我们提出了一种新的Markov模型，用于捕捉分割噪音注释中的空间相关性和偏见。此外，我们还提出了一种标签修正方法，可以逐渐更正真实标签。我们提供了对方法的理论保证。实验表明，我们的方法可以在Synthetic和实际噪音注释上超过当前状态的前iers的性能。
</details></li>
</ul>
<hr>
<h2 id="Screening-Mammography-Breast-Cancer-Detection"><a href="#Screening-Mammography-Breast-Cancer-Detection" class="headerlink" title="Screening Mammography Breast Cancer Detection"></a>Screening Mammography Breast Cancer Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11274">http://arxiv.org/abs/2307.11274</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chakrabortyde/rsna-breast-cancer">https://github.com/chakrabortyde/rsna-breast-cancer</a></li>
<li>paper_authors: Debajyoti Chakraborty</li>
<li>for:  automated breast cancer detection</li>
<li>methods:  different methodologies were tested against the RSNA dataset of radiographic breast images</li>
<li>results:  average validation case pF1 score of 0.56 across methods<details>
<summary>Abstract</summary>
Breast cancer is a leading cause of cancer-related deaths, but current programs are expensive and prone to false positives, leading to unnecessary follow-up and patient anxiety. This paper proposes a solution to automated breast cancer detection, to improve the efficiency and accuracy of screening programs. Different methodologies were tested against the RSNA dataset of radiographic breast images of roughly 20,000 female patients and yielded an average validation case pF1 score of 0.56 across methods.
</details>
<details>
<summary>摘要</summary>
乳癌是癌症相关死亡的主要原因，但现有的计划具有高成本和假阳性的问题，导致无需的追踪和患者焦虑。这篇论文提出了自动乳癌检测的解决方案，以提高检测计划的效率和准确率。不同的方法在RSNA数据集中测试了约20,000名女性患者的骨盔影像，并获得了0.56的平均验证案例pF1分数。
</details></li>
</ul>
<hr>
<h2 id="On-the-Fisher-Rao-Gradient-of-the-Evidence-Lower-Bound"><a href="#On-the-Fisher-Rao-Gradient-of-the-Evidence-Lower-Bound" class="headerlink" title="On the Fisher-Rao Gradient of the Evidence Lower Bound"></a>On the Fisher-Rao Gradient of the Evidence Lower Bound</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11249">http://arxiv.org/abs/2307.11249</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nihat Ay, Jesse van Oostrum</li>
<li>for: studies the Fisher-Rao gradient of the evidence lower bound (ELBO)</li>
<li>methods: uses invariance properties of gradients within information geometry</li>
<li>results: establishes the equivalence of minimizing the prime objective function and maximizing the ELBO<details>
<summary>Abstract</summary>
This article studies the Fisher-Rao gradient, also referred to as the natural gradient, of the evidence lower bound, the ELBO, which plays a crucial role within the theory of the Variational Autonecoder, the Helmholtz Machine and the Free Energy Principle. The natural gradient of the ELBO is related to the natural gradient of the Kullback-Leibler divergence from a target distribution, the prime objective function of learning. Based on invariance properties of gradients within information geometry, conditions on the underlying model are provided that ensure the equivalence of minimising the prime objective function and the maximisation of the ELBO.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Leveraging-arbitrary-mobile-sensor-trajectories-with-shallow-recurrent-decoder-networks-for-full-state-reconstruction"><a href="#Leveraging-arbitrary-mobile-sensor-trajectories-with-shallow-recurrent-decoder-networks-for-full-state-reconstruction" class="headerlink" title="Leveraging arbitrary mobile sensor trajectories with shallow recurrent decoder networks for full-state reconstruction"></a>Leveraging arbitrary mobile sensor trajectories with shallow recurrent decoder networks for full-state reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11793">http://arxiv.org/abs/2307.11793</a></li>
<li>repo_url: None</li>
<li>paper_authors: Megan R. Ebers, Jan P. Williams, Katherine M. Steele, J. Nathan Kutz</li>
<li>for: 这篇论文主要用于探讨如何使用移动感知器来监测和估计复杂的空间时间系统。</li>
<li>methods: 该论文使用了深度学习架构，特别是LSTM网络和解码器网络，将动态轨迹信息映射到全状态空间中。</li>
<li>results: 研究表明，使用移动感知器和 shallow recurrent decoder networks 可以高精度地重建全状态空间，并且该架构可以快速适应不同的动力学轨迹。此外，该架构还可以训练在不同的数据集上进行快速普适化。<details>
<summary>Abstract</summary>
Sensing is one of the most fundamental tasks for the monitoring, forecasting and control of complex, spatio-temporal systems. In many applications, a limited number of sensors are mobile and move with the dynamics, with examples including wearable technology, ocean monitoring buoys, and weather balloons. In these dynamic systems (without regions of statistical-independence), the measurement time history encodes a significant amount of information that can be extracted for critical tasks. Most model-free sensing paradigms aim to map current sparse sensor measurements to the high-dimensional state space, ignoring the time-history all together. Using modern deep learning architectures, we show that a sequence-to-vector model, such as an LSTM (long, short-term memory) network, with a decoder network, dynamic trajectory information can be mapped to full state-space estimates. Indeed, we demonstrate that by leveraging mobile sensor trajectories with shallow recurrent decoder networks, we can train the network (i) to accurately reconstruct the full state space using arbitrary dynamical trajectories of the sensors, (ii) the architecture reduces the variance of the mean-square error of the reconstruction error in comparison with immobile sensors, and (iii) the architecture also allows for rapid generalization (parameterization of dynamics) for data outside the training set. Moreover, the path of the sensor can be chosen arbitrarily, provided training data for the spatial trajectory of the sensor is available. The exceptional performance of the network architecture is demonstrated on three applications: turbulent flows, global sea-surface temperature data, and human movement biomechanics.
</details>
<details>
<summary>摘要</summary>
感知是复杂系统监测、预测和控制中的一项基本任务。在许多应用中，有限数量的感知器是移动的，例如着装式技术、海洋监测浮标和气象气球。在这些动态系统中（无区域独立统计），测量时间历史记录了大量信息，可以提取到关键任务中。大多数无模型感知方法尝试将当前稀疏测量映射到高维状态空间中，忽略时间历史 altogether。使用现代深度学习架构，我们显示了一种序列vector模型，例如LSTM（长短期记忆）网络，与动态轨迹信息相结合，可以将动态轨迹信息映射到全状态空间估计。实际上，我们证明了以下三点：（i）通过使用移动感知器的 shallow recurrent decoder网络，可以准确地重建全状态空间估计，不管感知器的动态轨迹是什么样的。（ii）该架构可以将变量的均方误差减少到较低水平，相比静止感知器。（iii）该架构还允许快速通用化（参数化动力学），可以在训练集外进行数据处理。此外，感知器的路径可以随意选择，只要有相应的空间轨迹训练数据。我们在三个应用中展示了 exceptional performance：湍流、全球海面温度数据和人体运动生物力学。
</details></li>
</ul>
<hr>
<h2 id="On-Sensor-Data-Filtering-using-Neuromorphic-Computing-for-High-Energy-Physics-Experiments"><a href="#On-Sensor-Data-Filtering-using-Neuromorphic-Computing-for-High-Energy-Physics-Experiments" class="headerlink" title="On-Sensor Data Filtering using Neuromorphic Computing for High Energy Physics Experiments"></a>On-Sensor Data Filtering using Neuromorphic Computing for High Energy Physics Experiments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11242">http://arxiv.org/abs/2307.11242</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shruti R. Kulkarni, Aaron Young, Prasanna Date, Narasinga Rao Miniskar, Jeffrey S. Vetter, Farah Fahim, Benjamin Parpillon, Jennet Dickinson, Nhan Tran, Jieun Yoo, Corrinne Mills, Morris Swartz, Petar Maksimovic, Catherine D. Schuman, Alice Bean</li>
<li>for: 这个研究探讨了基于神经omorphic计算的脉冲神经网络（SNN）模型在高能物理实验中滤除感器电子数据的问题。</li>
<li>methods: 我们采用了一种压缩型神经omorphic模型，将探测器数据转换为二进制值事件流，然后由SNN进行处理。我们通过许多系统设计选择，从数据编码到优化训练算法的hyperparameters，实现了一个准确且压缩的SNN模型。</li>
<li>results: 我们的结果显示，使用进化算法和优化的hyperparameters，SNN模型可以达到约91%的信号效率，只需占用半数量的参数。<details>
<summary>Abstract</summary>
This work describes the investigation of neuromorphic computing-based spiking neural network (SNN) models used to filter data from sensor electronics in high energy physics experiments conducted at the High Luminosity Large Hadron Collider. We present our approach for developing a compact neuromorphic model that filters out the sensor data based on the particle's transverse momentum with the goal of reducing the amount of data being sent to the downstream electronics. The incoming charge waveforms are converted to streams of binary-valued events, which are then processed by the SNN. We present our insights on the various system design choices - from data encoding to optimal hyperparameters of the training algorithm - for an accurate and compact SNN optimized for hardware deployment. Our results show that an SNN trained with an evolutionary algorithm and an optimized set of hyperparameters obtains a signal efficiency of about 91% with nearly half as many parameters as a deep neural network.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Edgewise-outliers-of-network-indexed-signals"><a href="#Edgewise-outliers-of-network-indexed-signals" class="headerlink" title="Edgewise outliers of network indexed signals"></a>Edgewise outliers of network indexed signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11239">http://arxiv.org/abs/2307.11239</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kristats/spout">https://github.com/kristats/spout</a></li>
<li>paper_authors: Christopher Rieser, Anne Ruiz-Gazen, Christine Thomas-Agnan</li>
<li>for: 本研究探讨了网络索引多变量数据中变量之间和图节点之间的依赖关系模型。</li>
<li>methods: 作者首先计算了一些平方误差的分布，以便确定检测规则和阈值 для异常检测。然后，他们提出了一种robust版本的决定性MCD算法，称为边缘MCD。</li>
<li>results: 在模拟数据上的应用和实际数据集上的应用显示了考虑依赖结构的利用。<details>
<summary>Abstract</summary>
We consider models for network indexed multivariate data involving a dependence between variables as well as across graph nodes.   In the framework of these models, we focus on outliers detection and introduce the concept of edgewise outliers. For this purpose, we first derive the distribution of some sums of squares, in particular squared Mahalanobis distances that can be used to fix detection rules and thresholds for outlier detection. We then propose a robust version of the deterministic MCD algorithm that we call edgewise MCD. An application on simulated data shows the interest of taking the dependence structure into account. We also illustrate the utility of the proposed method with a real data set.
</details>
<details>
<summary>摘要</summary>
我们考虑了网络索引多变量数据中变量之间以及图节点之间的相互依赖关系。在这个框架下，我们关注异常检测，并引入了边缘异常概念。为此，我们首先计算了某些和平方的分布，特别是方差距离的平方，可以用于定制检测规则和阈值。然后，我们提出了一种robust版本的决定性MCD算法，我们称之为边缘MCD。在 simulate 数据上的应用显示了考虑依赖结构的利用。我们还使用了一个实际数据集来证明方法的实用性。Note: "网络索引" in the original text refers to the fact that the data is indexed by a network, meaning that each data point is associated with a set of nodes in the network. In the translation, I translated it as "网络索引多变量数据" to emphasize that the data is both indexed by a network and involves multiple variables.
</details></li>
</ul>
<hr>
<h2 id="QDC-Quantum-Diffusion-Convolution-Kernels-on-Graphs"><a href="#QDC-Quantum-Diffusion-Convolution-Kernels-on-Graphs" class="headerlink" title="QDC: Quantum Diffusion Convolution Kernels on Graphs"></a>QDC: Quantum Diffusion Convolution Kernels on Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11234">http://arxiv.org/abs/2307.11234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Markovich</li>
<li>For: The paper is written for improving the predictive accuracy of graph convolutional neural networks (GCNs) by introducing a new convolution kernel called the Quantum Diffusion Convolution (QDC) operator.* Methods: The QDC operator effectively rewires the graph according to the occupation correlations of the vertices by trading on the generalized diffusion paradigm for the propagation of a quantum particle over the graph. The paper also introduces a multiscale variant that combines messages from the QDC operator and the traditional combinatorial Laplacian.* Results: The paper shows that QDC improves predictive performance on widely used benchmark datasets when compared to similar methods, as demonstrated through experiments and spectral studies.<details>
<summary>Abstract</summary>
Graph convolutional neural networks (GCNs) operate by aggregating messages over local neighborhoods given the prediction task under interest. Many GCNs can be understood as a form of generalized diffusion of input features on the graph, and significant work has been dedicated to improving predictive accuracy by altering the ways of message passing. In this work, we propose a new convolution kernel that effectively rewires the graph according to the occupation correlations of the vertices by trading on the generalized diffusion paradigm for the propagation of a quantum particle over the graph. We term this new convolution kernel the Quantum Diffusion Convolution (QDC) operator. In addition, we introduce a multiscale variant that combines messages from the QDC operator and the traditional combinatorial Laplacian. To understand our method, we explore the spectral dependence of homophily and the importance of quantum dynamics in the construction of a bandpass filter. Through these studies, as well as experiments on a range of datasets, we observe that QDC improves predictive performance on the widely used benchmark datasets when compared to similar methods.
</details>
<details>
<summary>摘要</summary>
几何傅立曼 нейрон网络（GCN）通过聚合地方邻域内的讯息进行预测任务。许多GCN可以理解为一种通用扩散的输入特征在几何上的扩散，并有很多研究旨在提高预测精度的方法。在这个工作中，我们提出了一个新的散射核心（QDC）算子，它可以根据顶点间的聚集相互作用来重新排列几何。此外，我们介绍了一种多尺度版本，它结合了QDC算子和传统的 combinatorial Laplacian。为了理解我们的方法，我们研究了同优化和几何动态的spectral dependence，以及实际上的几何构造。通过这些研究和许多数据集上的实验，我们发现QDC可以在相似的方法上提高预测性能。
</details></li>
</ul>
<hr>
<h2 id="From-Adaptive-Query-Release-to-Machine-Unlearning"><a href="#From-Adaptive-Query-Release-to-Machine-Unlearning" class="headerlink" title="From Adaptive Query Release to Machine Unlearning"></a>From Adaptive Query Release to Machine Unlearning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11228">http://arxiv.org/abs/2307.11228</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enayat Ullah, Raman Arora</li>
<li>for: 这 paper 的目的是强化机器学习模型的“忘记”能力，即在已经训练过的模型上进行不可逆的更新，以提高模型的泛化能力。</li>
<li>methods: 这 paper 使用了一种基于查询的方法，即采用适应查询来实现机器学习模型的“忘记”。具体来说，这 paper 提出了一种基于线性和预处理查询的“忘记”算法，可以快速地进行更新。</li>
<li>results: 这 paper 的结果表明，使用这种“忘记”算法可以在各种问题上提高机器学习模型的泛化能力，包括泛化优化问题和Generalized Linear Models (GLMs) 等。具体来说，这 paper 的结果表明，在某些情况下，使用“忘记”算法可以实现更好的泛化能力，比如在 GLMs 中可以达到维度独立的泛化率。<details>
<summary>Abstract</summary>
We formalize the problem of machine unlearning as design of efficient unlearning algorithms corresponding to learning algorithms which perform a selection of adaptive queries from structured query classes. We give efficient unlearning algorithms for linear and prefix-sum query classes. As applications, we show that unlearning in many problems, in particular, stochastic convex optimization (SCO), can be reduced to the above, yielding improved guarantees for the problem. In particular, for smooth Lipschitz losses and any $\rho>0$, our results yield an unlearning algorithm with excess population risk of $\tilde O\big(\frac{1}{\sqrt{n}}+\frac{\sqrt{d}}{n\rho}\big)$ with unlearning query (gradient) complexity $\tilde O(\rho \cdot \text{Retraining Complexity})$, where $d$ is the model dimensionality and $n$ is the initial number of samples. For non-smooth Lipschitz losses, we give an unlearning algorithm with excess population risk $\tilde O\big(\frac{1}{\sqrt{n}}+\big(\frac{\sqrt{d}}{n\rho}\big)^{1/2}\big)$ with the same unlearning query (gradient) complexity. Furthermore, in the special case of Generalized Linear Models (GLMs), such as those in linear and logistic regression, we get dimension-independent rates of $\tilde O\big(\frac{1}{\sqrt{n}} +\frac{1}{(n\rho)^{2/3}}\big)$ and $\tilde O\big(\frac{1}{\sqrt{n}} +\frac{1}{(n\rho)^{1/3}}\big)$ for smooth Lipschitz and non-smooth Lipschitz losses respectively. Finally, we give generalizations of the above from one unlearning request to \textit{dynamic} streams consisting of insertions and deletions.
</details>
<details>
<summary>摘要</summary>
我们正式化机器学习忘却问题，设计高效的忘却算法，与学习算法相似，可以进行适应的查询。我们提供了高效的忘却算法，用于线性和前缀和算法。我们还证明了在许多问题中，例如测度数据分析（SCO）中，忘却可以实现改进的保证。具体来说，对于具有对称损失函数和任意 $\rho>0$ 的情况，我们的结果提供了一个忘却算法，其具有对称损失函数的优化强度误差 $\tilde O\left(\frac{1}{\sqrt{n}}+\frac{\sqrt{d}}{n\rho}\right)$，并且具有忘却查询（梯度）复杂度 $\tilde O\left(\rho \cdot \text{Retraining Complexity}\right)$，其中 $d$ 是模型维度，$n$ 是初始样本数量。对非对称损失函数，我们提供了一个忘却算法，其具有对称损失函数的优化强度误差 $\tilde O\left(\frac{1}{\sqrt{n}}+\left(\frac{\sqrt{d}}{n\rho}\right)^{1/2}\right)$，并且具有相同的忘却查询（梯度）复杂度。尤其是，在一般化线性模型（GLM）中，例如线性回传和логисти回传，我们得到了维度独立的误差率 $\tilde O\left(\frac{1}{\sqrt{n}}+\frac{1}{(n\rho)^{2/3}}\right)$ 和 $\tilde O\left(\frac{1}{\sqrt{n}}+\frac{1}{(n\rho)^{1/3}}\right)$  для对称损失函数和非对称损失函数。最后，我们将这些结果扩展到多个忘却请求，以及\textit{动态}流中的插入和删除。
</details></li>
</ul>
<hr>
<h2 id="Quantum-Convolutional-Neural-Networks-with-Interaction-Layers-for-Classification-of-Classical-Data"><a href="#Quantum-Convolutional-Neural-Networks-with-Interaction-Layers-for-Classification-of-Classical-Data" class="headerlink" title="Quantum Convolutional Neural Networks with Interaction Layers for Classification of Classical Data"></a>Quantum Convolutional Neural Networks with Interaction Layers for Classification of Classical Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11792">http://arxiv.org/abs/2307.11792</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jishnu Mahmud, Raisa Mashtura, Shaikh Anowarul Fattah, Mohammad Saquib</li>
<li>for: 研究多比特交互对量子神经网络的影响，以提高表达能力和积极性。</li>
<li>methods: 提出了一种量子卷积网络，利用三比特交互层，提高网络的表达能力和积极性，并应用于图像和一维数据分类。</li>
<li>results: 对三个公共可用数据集（MNIST、Fashion MNIST、Iris）进行了二分和多分类测试，和现有状态的方法进行比较，并被发现性能超过现有方法。<details>
<summary>Abstract</summary>
Quantum Machine Learning (QML) has come into the limelight due to the exceptional computational abilities of quantum computers. With the promises of near error-free quantum computers in the not-so-distant future, it is important that the effect of multi-qubit interactions on quantum neural networks is studied extensively. This paper introduces a Quantum Convolutional Network with novel Interaction layers exploiting three-qubit interactions increasing the network's expressibility and entangling capability, for classifying both image and one-dimensional data. The proposed approach is tested on three publicly available datasets namely MNIST, Fashion MNIST, and Iris datasets, to perform binary and multiclass classifications and is found to supersede the performance of the existing state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
量子机器学习（QML）因量子计算机的特殊计算能力而受到关注。随着近代错误率几乎为零的量子计算机的未来途径，研究多量子bits之间的交互效果对量子神经网络的影响非常重要。这篇论文提出了一种具有新型交互层的量子卷积网络，利用三量子bits之间的交互提高网络的表达能力和混合能力，用于图像和一维数据的分类。该方法在MNIST、Fashion MNIST和芳香Datasets三个公共可用数据集上进行了二分和多分类测试，并被证明超越了现有状态的方法。
</details></li>
</ul>
<hr>
<h2 id="Jina-Embeddings-A-Novel-Set-of-High-Performance-Sentence-Embedding-Models"><a href="#Jina-Embeddings-A-Novel-Set-of-High-Performance-Sentence-Embedding-Models" class="headerlink" title="Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models"></a>Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11224">http://arxiv.org/abs/2307.11224</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Günther, Louis Milliken, Jonathan Geuter, Georgios Mastrapas, Bo Wang, Han Xiao</li>
<li>for: 本研究开发了一个高性能的句子嵌入模型，用于转换不同文本输入为数据表示，并实现文本含义的捕捉。</li>
<li>methods: 本研究使用了高质量的对称和三重数据集，进行了详细的数据清洁和模型训练过程，并使用了大规模文本嵌入评估 benchmarck (MTEB) 进行了广泛的性能评估。</li>
<li>results: 本研究发现，这些模型在 dense retrieval 和 semantic textual similarity 等应用中表现出色，并且透过建立了一个新的负数陈述和非负数陈述的训练和评估数据集，以提高模型对负数陈述的认识。<details>
<summary>Abstract</summary>
Jina Embeddings constitutes a set of high-performance sentence embedding models adept at translating various textual inputs into numerical representations, thereby capturing the semantic essence of the text. The models excel in applications such as dense retrieval and semantic textual similarity. This paper details the development of Jina Embeddings, starting with the creation of high-quality pairwise and triplet datasets. It underlines the crucial role of data cleaning in dataset preparation, gives in-depth insights into the model training process, and concludes with a comprehensive performance evaluation using the Massive Textual Embedding Benchmark (MTEB). To increase the model's awareness of negations, we constructed a novel training and evaluation dataset of negated and non-negated statements, which we make publicly available to the community.
</details>
<details>
<summary>摘要</summary>
简凝嵌入模型是一组高性能句子嵌入模型，能够将不同的文本输入转化为数值表示，从而捕捉文本 semantic essence。这些模型在 dense retrieval 和 semantic textual similarity 等应用中表现出色。本文介绍了简凝嵌入模型的开发，从高质量的 pairwise 和 triplet 数据集的创建开始，并强调数据清洁的重要性。文中还提供了嵌入模型训练过程的深入解释，并通过 Massive Textual Embedding Benchmark (MTEB) 进行了全面性的性能评估。为提高模型对否定语言的识别能力，我们还构建了一个新的训练和评估集，包括否定和非否定句子，并将其公开给社区。
</details></li>
</ul>
<hr>
<h2 id="FairMobi-Net-A-Fairness-aware-Deep-Learning-Model-for-Urban-Mobility-Flow-Generation"><a href="#FairMobi-Net-A-Fairness-aware-Deep-Learning-Model-for-Urban-Mobility-Flow-Generation" class="headerlink" title="FairMobi-Net: A Fairness-aware Deep Learning Model for Urban Mobility Flow Generation"></a>FairMobi-Net: A Fairness-aware Deep Learning Model for Urban Mobility Flow Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11214">http://arxiv.org/abs/2307.11214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhewei Liu, Lipai Huang, Chao Fan, Ali Mostafavi</li>
<li>for: 用于更好地理解城市结构和人口活动模式，促进城市规划和管理。</li>
<li>methods: 使用一种新的、具有公平性的深度学习模型（FairMobi-Net），通过纳入公平损失函数和混合binary分类和数值回归技术来预测人口流动。</li>
<li>results: 对四个美国城市的人口流动数据进行验证，显示FairMobi-Net模型在不同地区之间的人口流动预测中具有更高的准确率和公平性，并且可以 Addressing the previous fairness concern，提供了一个有效的工具 для准确地生成人口流动 across regions。<details>
<summary>Abstract</summary>
Generating realistic human flows across regions is essential for our understanding of urban structures and population activity patterns, enabling important applications in the fields of urban planning and management. However, a notable shortcoming of most existing mobility generation methodologies is neglect of prediction fairness, which can result in underestimation of mobility flows across regions with vulnerable population groups, potentially resulting in inequitable resource distribution and infrastructure development. To overcome this limitation, our study presents a novel, fairness-aware deep learning model, FairMobi-Net, for inter-region human flow prediction. The FairMobi-Net model uniquely incorporates fairness loss into the loss function and employs a hybrid approach, merging binary classification and numerical regression techniques for human flow prediction. We validate the FairMobi-Net model using comprehensive human mobility datasets from four U.S. cities, predicting human flow at the census-tract level. Our findings reveal that the FairMobi-Net model outperforms state-of-the-art models (such as the DeepGravity model) in producing more accurate and equitable human flow predictions across a variety of region pairs, regardless of regional income differences. The model maintains a high degree of accuracy consistently across diverse regions, addressing the previous fairness concern. Further analysis of feature importance elucidates the impact of physical distances and road network structures on human flows across regions. With fairness as its touchstone, the model and results provide researchers and practitioners across the fields of urban sciences, transportation engineering, and computing with an effective tool for accurate generation of human mobility flows across regions.
</details>
<details>
<summary>摘要</summary>
generating realistic human flows across regions is essential for our understanding of urban structures and population activity patterns, enabling important applications in the fields of urban planning and management. However, a notable shortcoming of most existing mobility generation methodologies is neglect of prediction fairness, which can result in underestimation of mobility flows across regions with vulnerable population groups, potentially resulting in inequitable resource distribution and infrastructure development. To overcome this limitation, our study presents a novel, fairness-aware deep learning model, FairMobi-Net, for inter-region human flow prediction. The FairMobi-Net model uniquely incorporates fairness loss into the loss function and employs a hybrid approach, merging binary classification and numerical regression techniques for human flow prediction. We validate the FairMobi-Net model using comprehensive human mobility datasets from four U.S. cities, predicting human flow at the census-tract level. Our findings reveal that the FairMobi-Net model outperforms state-of-the-art models (such as the DeepGravity model) in producing more accurate and equitable human flow predictions across a variety of region pairs, regardless of regional income differences. The model maintains a high degree of accuracy consistently across diverse regions, addressing the previous fairness concern. Further analysis of feature importance elucidates the impact of physical distances and road network structures on human flows across regions. With fairness as its touchstone, the model and results provide researchers and practitioners across the fields of urban sciences, transportation engineering, and computing with an effective tool for accurate generation of human mobility flows across regions.
</details></li>
</ul>
<hr>
<h2 id="The-Effect-of-Epidemiological-Cohort-Creation-on-the-Machine-Learning-Prediction-of-Homelessness-and-Police-Interaction-Outcomes-Using-Administrative-Health-Care-Data"><a href="#The-Effect-of-Epidemiological-Cohort-Creation-on-the-Machine-Learning-Prediction-of-Homelessness-and-Police-Interaction-Outcomes-Using-Administrative-Health-Care-Data" class="headerlink" title="The Effect of Epidemiological Cohort Creation on the Machine Learning Prediction of Homelessness and Police Interaction Outcomes Using Administrative Health Care Data"></a>The Effect of Epidemiological Cohort Creation on the Machine Learning Prediction of Homelessness and Police Interaction Outcomes Using Administrative Health Care Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11211">http://arxiv.org/abs/2307.11211</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Fuzzy-sh/Machine-Learning-Risk-Estimation-and-Prediction-of-Homelessness-and-Police-interaction">https://github.com/Fuzzy-sh/Machine-Learning-Risk-Estimation-and-Prediction-of-Homelessness-and-Police-interaction</a></li>
<li>paper_authors: Faezehsadat Shahidi, M. Ethan MacDonald, Dallas Seitz, Geoffrey Messier</li>
<li>for: This study aimed to identify key features associated with initial homelessness and police interaction among individuals with addiction or mental health diagnoses, and to demonstrate the benefits of using flexible windows in predictive modeling.</li>
<li>methods: The study used an administrative healthcare dataset from Calgary, Alberta, Canada, comprising 240,219 individuals diagnosed with addiction or mental health issues between April 2013 and March 2018. The cohort was followed for 2 years to identify factors associated with homelessness and police interactions. The study compared logistic regression (LR) and machine learning (ML) models, including random forests (RF) and extreme gradient boosting (XGBoost), in two cohorts with different methods.</li>
<li>results: The study found that male sex, substance disorder, psychiatrist visits, and drug abuse were associated with initial homelessness and police interaction. The flexible window method used in XGBoost showed superior performance in predicting initial homelessness and police interaction, with sensitivity and AUC values of 91% and 90%, respectively.Here is the information in Simplified Chinese text:</li>
<li>for: 这个研究的目的是确定初次无家者和警察互动的关键特征，以及使用 flexible 窗口方法可以提高预测模型的性能。</li>
<li>methods: 这个研究使用了加拿大阿尔伯塔省卡尔加里市的行政医疗数据集，包括240,219名被诊断为有添iction或心理健康问题的人 between 2013年4月1日和2018年3月31日。研究followed这个 cohort for 2年以确定因素和警察互动。研究比较了 logistic regression（LR）和机器学习（ML）模型，包括Random Forests（RF）和极限梯度提升（XGBoost），在两个 cohort 中进行比较。</li>
<li>results: 研究发现，♂性（AORs：H&#x3D;1.51，P&#x3D;2.52）、substance disorder（AORs：H&#x3D;3.70，P&#x3D;2.83）、心理医生访问（AORs：H&#x3D;1.44，P&#x3D;1.49）和药物滥用（AORs：H&#x3D;2.67，P&#x3D;1.83）与初次无家者和警察互动有关。 XGBoost 使用 flexible 窗口方法显示在预测初次无家者和警察互动时的性能更高，sensitivity 为91%，AUC 为90%。<details>
<summary>Abstract</summary>
Background: Mental illness can lead to adverse outcomes such as homelessness and police interaction and understanding of the events leading up to these adverse outcomes is important. Predictive models may help identify individuals at risk of such adverse outcomes. Using a fixed observation window cohort with logistic regression (LR) or machine learning (ML) models can result in lower performance when compared with adaptive and parcellated windows.   Method: An administrative healthcare dataset was used, comprising of 240,219 individuals in Calgary, Alberta, Canada who were diagnosed with addiction or mental health (AMH) between April 1, 2013, and March 31, 2018. The cohort was followed for 2 years to identify factors associated with homelessness and police interactions. To understand the benefit of flexible windows to predictive models, an alternative cohort was created. Then LR and ML models, including random forests (RF), and extreme gradient boosting (XGBoost) were compared in the two cohorts.   Results: Among 237,602 individuals, 0.8% (1,800) experienced first homelessness, while 0.32% (759) reported initial police interaction among 237,141 individuals. Male sex (AORs: H=1.51, P=2.52), substance disorder (AORs: H=3.70, P=2.83), psychiatrist visits (AORs: H=1.44, P=1.49), and drug abuse (AORs: H=2.67, P=1.83) were associated with initial homelessness (H) and police interaction (P). XGBoost showed superior performance using the flexible method (sensitivity =91%, AUC =90% for initial homelessness, and sensitivity =90%, AUC=89% for initial police interaction)   Conclusion: This study identified key features associated with initial homelessness and police interaction and demonstrated that flexible windows can improve predictive modeling.
</details>
<details>
<summary>摘要</summary>
背景：心理疾病可能会导致不良结果，如失Homelessness和警察互动，了解这些不良结果的发展过程是重要的。预测模型可能能够 identificational individuals at risk of such adverse outcomes。使用 fixes observation window cohort with logistic regression (LR) or machine learning (ML) models可能会导致性能下降。方法：我们使用了一个行政医疗数据集，包括2013年4月1日至2018年3月31日在加拿大阿尔伯塔省的240,219名患有情绪或神经疾病（AMH）患者。这个群体被跟踪了2年，以便 Identify factors associated with homelessness and police interactions。为了了解 flexible windows 对预测模型的好处，我们创建了一个alternative cohort。然后，我们使用了LR和ML模型，包括Random Forests (RF)和极限梯度提升 (XGBoost)，在这两个cohort中进行了比较。结果： Among 237,602 individuals, 0.8% (1,800) experienced first homelessness, while 0.32% (759) reported initial police interaction among 237,141 individuals. male sex (AORs: H=1.51, P=2.52), substance disorder (AORs: H=3.70, P=2.83), psychiatrist visits (AORs: H=1.44, P=1.49), and drug abuse (AORs: H=2.67, P=1.83) were associated with initial homelessness (H) and police interaction (P). XGBoost showed superior performance using the flexible method (sensitivity =91%, AUC =90% for initial homelessness, and sensitivity =90%, AUC=89% for initial police interaction)。结论：这个研究Identified key features associated with initial homelessness and police interaction, and demonstrated that flexible windows can improve predictive modeling.
</details></li>
</ul>
<hr>
<h2 id="Clinical-Trial-Active-Learning"><a href="#Clinical-Trial-Active-Learning" class="headerlink" title="Clinical Trial Active Learning"></a>Clinical Trial Active Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11209">http://arxiv.org/abs/2307.11209</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/olivesgatech/clinical-trial-active-learning">https://github.com/olivesgatech/clinical-trial-active-learning</a></li>
<li>paper_authors: Zoe Fowler, Kiran Kokilepersaud, Mohit Prabhushankar, Ghassan AlRegib</li>
<li>for: 这篇论文旨在解决临床试验中非独立同分布（non-i.i.d）结构的问题，并提出了一种新的活动学习方法。</li>
<li>methods: 本论文使用了一种基于时间conditioning的 prospectively Active Learning方法，即在收集数据时conditioning on the time an image was collected，以维护i.i.d.假设。</li>
<li>results: 对比传统的retrospective Active Learning方法，prospective Active Learning在两种不同的测试环境中表现出了更好的性能。<details>
<summary>Abstract</summary>
This paper presents a novel approach to active learning that takes into account the non-independent and identically distributed (non-i.i.d.) structure of a clinical trial setting. There exists two types of clinical trials: retrospective and prospective. Retrospective clinical trials analyze data after treatment has been performed; prospective clinical trials collect data as treatment is ongoing. Typically, active learning approaches assume the dataset is i.i.d. when selecting training samples; however, in the case of clinical trials, treatment results in a dependency between the data collected at the current and past visits. Thus, we propose prospective active learning to overcome the limitations present in traditional active learning methods and apply it to disease detection in optical coherence tomography (OCT) images, where we condition on the time an image was collected to enforce the i.i.d. assumption. We compare our proposed method to the traditional active learning paradigm, which we refer to as retrospective in nature. We demonstrate that prospective active learning outperforms retrospective active learning in two different types of test settings.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Heuristic-Hyperparameter-Choice-for-Image-Anomaly-Detection"><a href="#Heuristic-Hyperparameter-Choice-for-Image-Anomaly-Detection" class="headerlink" title="Heuristic Hyperparameter Choice for Image Anomaly Detection"></a>Heuristic Hyperparameter Choice for Image Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11197">http://arxiv.org/abs/2307.11197</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyu Jiang, João P. C. Bertoldo, Etienne Decencière</li>
<li>for: 这个论文目的是为了提高图像潜在问题检测（AD）中使用深度学习神经网络的精度和效率。</li>
<li>methods: 本论文使用了预训练的深度学习模型，并将其中EXTRACTED的深度特征进行缩减，使用NPCA算法进行维度缩减，以减少计算成本并提高效能。</li>
<li>results: 本论文通过实验表明，使用NPCA缩减法可以将维度缩减至少量化，同时维持较好的检测性能。<details>
<summary>Abstract</summary>
Anomaly detection (AD) in images is a fundamental computer vision problem by deep learning neural network to identify images deviating significantly from normality. The deep features extracted from pretrained models have been proved to be essential for AD based on multivariate Gaussian distribution analysis. However, since models are usually pretrained on a large dataset for classification tasks such as ImageNet, they might produce lots of redundant features for AD, which increases computational cost and degrades the performance. We aim to do the dimension reduction of Negated Principal Component Analysis (NPCA) for these features. So we proposed some heuristic to choose hyperparameter of NPCA algorithm for getting as fewer components of features as possible while ensuring a good performance.
</details>
<details>
<summary>摘要</summary>
安全检测（AD）在图像上是一个基本的计算机视觉问题，使用深度学习神经网络来确定图像与正常情况之间的差异。深度特征从预训练模型中提取出来有助于AD，根据多变量泊松分布分析。但是，由于模型通常在大量数据集上进行类别任务such as ImageNet的预训练，它们可能生成大量 redundant features для AD，这会提高计算成本并降低性能。我们想使用NPCA算法进行维度减少。因此，我们提出了一些启发来选择NPCA算法中的超参数，以获得最少的特征组件，同时保证良好的性能。
</details></li>
</ul>
<hr>
<h2 id="Exploring-reinforcement-learning-techniques-for-discrete-and-continuous-control-tasks-in-the-MuJoCo-environment"><a href="#Exploring-reinforcement-learning-techniques-for-discrete-and-continuous-control-tasks-in-the-MuJoCo-environment" class="headerlink" title="Exploring reinforcement learning techniques for discrete and continuous control tasks in the MuJoCo environment"></a>Exploring reinforcement learning techniques for discrete and continuous control tasks in the MuJoCo environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11166">http://arxiv.org/abs/2307.11166</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vaddadi Sai Rahul, Debajyoti Chakraborty</li>
<li>for: 本研究使用MuJoCo快速物理渲染器进行连续控制任务，并公布任务的观察空间、动作空间、奖励等细节。</li>
<li>methods: 本研究使用Q-学习和SARSA作为基准，并通过离散方法进行比较。然后，使用DDPG进行深度政策梯度法，并在大量话数episode中评估其性能。</li>
<li>results: Q学习在大量话数episode中评估得分高于SARSA，但DDPG在一些话数episode中评估得分高于两者。此外，通过微调模型参数，我们预期可以提高性能，但需要更多时间和资源。<details>
<summary>Abstract</summary>
We leverage the fast physics simulator, MuJoCo to run tasks in a continuous control environment and reveal details like the observation space, action space, rewards, etc. for each task. We benchmark value-based methods for continuous control by comparing Q-learning and SARSA through a discretization approach, and using them as baselines, progressively moving into one of the state-of-the-art deep policy gradient method DDPG. Over a large number of episodes, Qlearning outscored SARSA, but DDPG outperformed both in a small number of episodes. Lastly, we also fine-tuned the model hyper-parameters expecting to squeeze more performance but using lesser time and resources. We anticipated that the new design for DDPG would vastly improve performance, yet after only a few episodes, we were able to achieve decent average rewards. We expect to improve the performance provided adequate time and computational resources.
</details>
<details>
<summary>摘要</summary>
我们利用快速物理 simulator MuJoCo 来运行任务在连续控制环境中，揭示任务的观察空间、动作空间、奖励等详细信息。我们通过对值基方法进行比较，使用 Q-学习和 SARSA 作为基准，逐步提高到当今顶尖的深度策略梯度方法 DDPG。经过大量的episode，Q学习超越 SARSA，但 DDPG 在一些episode中超越了两者。最后，我们也进行了模型超参数的微调，希望通过更小的时间和资源来提高性能。然而，我们在只需几集episode后就能够获得了良好的均值奖励。我们预计通过更多的时间和资源来提高性能。
</details></li>
</ul>
<hr>
<h2 id="Data-driven-criteria-for-quantum-correlations"><a href="#Data-driven-criteria-for-quantum-correlations" class="headerlink" title="Data-driven criteria for quantum correlations"></a>Data-driven criteria for quantum correlations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11091">http://arxiv.org/abs/2307.11091</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mateusz Krawczyk, Jarosław Pawłowski, Maciej M. Maśka, Katarzyna Roszak</li>
<li>for: 检测三量子系统中的相关性 using 无监督的神经网络</li>
<li>methods: 使用无监督的神经网络训练，强制网络认可分离态，检测相关态为异常</li>
<li>results: 检测到的相关态是量子谱度，而不是互信息，并且网络的识别率比基线模型高得多<details>
<summary>Abstract</summary>
We build a machine learning model to detect correlations in a three-qubit system using a neural network trained in an unsupervised manner on randomly generated states. The network is forced to recognize separable states, and correlated states are detected as anomalies. Quite surprisingly, we find that the proposed detector performs much better at distinguishing a weaker form of quantum correlations, namely, the quantum discord, than entanglement. In fact, it has a tendency to grossly overestimate the set of entangled states even at the optimal threshold for entanglement detection, while it underestimates the set of discordant states to a much lesser extent. In order to illustrate the nature of states classified as quantum-correlated, we construct a diagram containing various types of states -- entangled, as well as separable, both discordant and non-discordant. We find that the near-zero value of the recognition loss reproduces the shape of the non-discordant separable states with high accuracy, especially considering the non-trivial shape of this set on the diagram. The network architecture is designed carefully: it preserves separability, and its output is equivariant with respect to qubit permutations. We show that the choice of architecture is important to get the highest detection accuracy, much better than for a baseline model that just utilizes a partial trace operation.
</details>
<details>
<summary>摘要</summary>
我们建立了一个机器学习模型，用于检测三量子系统中的相关性，使用一个无监督的神经网络，训练在随机生成的态上。网络被迫认可分离态，相关态被检测为异常。我们发现，我们提议的检测器在识别量子相关性的能力方面表现出优于束缚相关性的能力，并且具有较强的精度。实际上，它在最佳检测率下对束缚态进行检测时会大幅度上估计束缚态集，而对分离态集进行检测时则减少较少。为了 illustrate 检测器对不同类型的态的行为，我们构建了一个包含各种态的 diagram。我们发现，near-zero值的认知损失能够准确地重建非分离态的形态，尤其是考虑到这些集的非rivalry形态。我们设计了网络 architecture 仔细，以保持分离性，并且网络输出是对 qubit Permutations 的 equivariant。我们显示，选择合适的 architecture 对检测精度具有重要作用，可以达到比基eline模型（只使用 partial trace 操作）更高的检测精度。
</details></li>
</ul>
<hr>
<h2 id="PAPR-Proximity-Attention-Point-Rendering"><a href="#PAPR-Proximity-Attention-Point-Rendering" class="headerlink" title="PAPR: Proximity Attention Point Rendering"></a>PAPR: Proximity Attention Point Rendering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11086">http://arxiv.org/abs/2307.11086</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanshu Zhang, Shichong Peng, Alireza Moazeni, Ke Li</li>
<li>for: 本文旨在提出一种新的点云表示方法，能够准确地从零开始学习场景表面的简洁表示。</li>
<li>methods: 该方法包括点云表示和可微分渲染器。点云表示使用点云，每个点拥有空间位置、前景分数和视图独立特征向量。渲染器选择每条光栅的相关点并生成精确的颜色使用其关联特征。</li>
<li>results: 本方法可以准确地学习点云位置来表示正确的场景几何结构，即使初始化和目标几何结构有很大差异。此外，该方法还能够捕捉细节 texture 详细信息，使用只需要一小部分的点。作者还展示了该方法在四个实际应用中的成果：geometry editing、object manipulation、texture transfer和exposure control。更多结果和代码可以通过作者的项目网站<a target="_blank" rel="noopener" href="https://zvict.github.io/papr/">https://zvict.github.io/papr/</a> obtain。<details>
<summary>Abstract</summary>
Learning accurate and parsimonious point cloud representations of scene surfaces from scratch remains a challenge in 3D representation learning. Existing point-based methods often suffer from the vanishing gradient problem or require a large number of points to accurately model scene geometry and texture. To address these limitations, we propose Proximity Attention Point Rendering (PAPR), a novel method that consists of a point-based scene representation and a differentiable renderer. Our scene representation uses a point cloud where each point is characterized by its spatial position, foreground score, and view-independent feature vector. The renderer selects the relevant points for each ray and produces accurate colours using their associated features. PAPR effectively learns point cloud positions to represent the correct scene geometry, even when the initialization drastically differs from the target geometry. Notably, our method captures fine texture details while using only a parsimonious set of points. We also demonstrate four practical applications of our method: geometry editing, object manipulation, texture transfer, and exposure control. More results and code are available on our project website at https://zvict.github.io/papr/.
</details>
<details>
<summary>摘要</summary>
学习精准而含括的点云表示方法仍然是3D表示学习中的挑战。现有的点云方法经常受到消失梯度问题或需要很多点云来准确地模型场景的几何结构和文字URE。为解决这些限制，我们提出了距离注意点渲染（PAPR）方法，它包括点云表示和可微分渲染器。我们的场景表示使用点云，每个点被定义为其空间位置、前景分数和视图独立的特征向量。渲染器选择每个光栅中的相关点云，并使用它们相关的特征来生成准确的颜色。PAPR有效地学习点云位置来表示正确的场景几何结构，即使初始化与目标几何结构有很大差异。此外，我们的方法可以capture细 texture detail，只需使用一个含括的点云集。我们还展示了我们的方法在四个实际应用中的成果：几何编辑、物体操作、xture传输和曝光控制。更多结果和代码可以在我们项目网站（https://zvict.github.io/papr/）上找到。
</details></li>
</ul>
<hr>
<h2 id="Representation-Learning-in-Anomaly-Detection-Successes-Limits-and-a-Grand-Challenge"><a href="#Representation-Learning-in-Anomaly-Detection-Successes-Limits-and-a-Grand-Challenge" class="headerlink" title="Representation Learning in Anomaly Detection: Successes, Limits and a Grand Challenge"></a>Representation Learning in Anomaly Detection: Successes, Limits and a Grand Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11085">http://arxiv.org/abs/2307.11085</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yedid Hoshen</li>
<li>for: 这篇论文写于异常检测领域，主要讨论了异常检测领域中的普遍性Limitations和如何突破这些限制。</li>
<li>methods: 这篇论文使用了一种名为”no free lunch principle”的概念， argue that there is no single algorithm or approach that can work well for all anomaly detection tasks, and that strong task priors are necessary to overcome these limitations.</li>
<li>results: 论文提出了两个”grand challenges” для异常检测，一个是科学发现的异常检测，另一个是在ImageNet dataset中检测最异常的图像。 论文认为，为了解决这些挑战，需要开发新的异常检测工具和想法。<details>
<summary>Abstract</summary>
In this perspective paper, we argue that the dominant paradigm in anomaly detection cannot scale indefinitely and will eventually hit fundamental limits. This is due to the a no free lunch principle for anomaly detection. These limitations can be overcome when there are strong tasks priors, as is the case for many industrial tasks. When such priors do not exists, the task is much harder for anomaly detection. We pose two such tasks as grand challenges for anomaly detection: i) scientific discovery by anomaly detection ii) a "mini-grand" challenge of detecting the most anomalous image in the ImageNet dataset. We believe new anomaly detection tools and ideas would need to be developed to overcome these challenges.
</details>
<details>
<summary>摘要</summary>
在这篇视角论文中，我们认为主流的异常检测模式无法无限扩展，最终会遇到基本的限制。这是由于异常检测中无免卡 principleno free lunch的原理。这些限制可以通过强大的任务优先顺序来缓解，例如许多工业任务中的情况。当这些优先顺序不存在时，任务变得更加困难。我们提出了两个如此任务作为异常检测的大挑战：一是科学发现通过异常检测ii）ImageNet数据集中最突出的异常图像检测的“小大挑战”。我们认为需要开发新的异常检测工具和想法来超越这些挑战。
</details></li>
</ul>
<hr>
<h2 id="GLSFormer-Gated-Long-Short-Sequence-Transformer-for-Step-Recognition-in-Surgical-Videos"><a href="#GLSFormer-Gated-Long-Short-Sequence-Transformer-for-Step-Recognition-in-Surgical-Videos" class="headerlink" title="GLSFormer: Gated - Long, Short Sequence Transformer for Step Recognition in Surgical Videos"></a>GLSFormer: Gated - Long, Short Sequence Transformer for Step Recognition in Surgical Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11081">http://arxiv.org/abs/2307.11081</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nisargshah1999/glsformer">https://github.com/nisargshah1999/glsformer</a></li>
<li>paper_authors: Nisarg A. Shah, Shameema Sikder, S. Swaroop Vedula, Vishal M. Patel</li>
<li>for: Automated surgical step recognition, aiming to improve patient safety and decision-making during surgeries.</li>
<li>methods: Vision transformer-based approach, jointly learning spatio-temporal features directly from sequence of frame-level patches, with a gated-temporal attention mechanism to combine short-term and long-term representations.</li>
<li>results: Superior performance compared to various state-of-the-art methods, validating the suitability of the proposed approach for automated surgical step recognition.Here’s the full text in Simplified Chinese:</li>
<li>for: 本文提出的目的是提高手术过程中的患者安全和决策，通过自动化手术步骤识别。</li>
<li>methods: 本文提出的方法是基于视力变换器， direkt地从框级别的图像序列中学习空间-时间特征，并使用关闭时间注意力机制将短期和长期空间-时间特征相结合。</li>
<li>results: 对于两个眼肠病手术视频数据集（Cataract-101和D99）的测试，本文的方法表现出比先前的状态 искусственный方法更好的性能，这证明了提出的方法适用于自动化手术步骤识别。I hope that helps!<details>
<summary>Abstract</summary>
Automated surgical step recognition is an important task that can significantly improve patient safety and decision-making during surgeries. Existing state-of-the-art methods for surgical step recognition either rely on separate, multi-stage modeling of spatial and temporal information or operate on short-range temporal resolution when learned jointly. However, the benefits of joint modeling of spatio-temporal features and long-range information are not taken in account. In this paper, we propose a vision transformer-based approach to jointly learn spatio-temporal features directly from sequence of frame-level patches. Our method incorporates a gated-temporal attention mechanism that intelligently combines short-term and long-term spatio-temporal feature representations. We extensively evaluate our approach on two cataract surgery video datasets, namely Cataract-101 and D99, and demonstrate superior performance compared to various state-of-the-art methods. These results validate the suitability of our proposed approach for automated surgical step recognition. Our code is released at: https://github.com/nisargshah1999/GLSFormer
</details>
<details>
<summary>摘要</summary>
自动化手术步骤识别是一项重要的任务，可以有效提高手术过程中的患者安全性和决策。现有的状态之arteMethods for surgical step recognition either rely on separate, multi-stage modeling of spatial and temporal information or operate on short-range temporal resolution when learned jointly. However, the benefits of joint modeling of spatio-temporal features and long-range information are not taken into account. In this paper, we propose a vision transformer-based approach to jointly learn spatio-temporal features directly from sequence of frame-level patches. Our method incorporates a gated-temporal attention mechanism that intelligently combines short-term and long-term spatio-temporal feature representations. We extensively evaluate our approach on two cataract surgery video datasets, namely Cataract-101 and D99, and demonstrate superior performance compared to various state-of-the-art methods. These results validate the suitability of our proposed approach for automated surgical step recognition. Our code is released at: https://github.com/nisargshah1999/GLSFormer.Here's the text with some notes on the translation:* "Automated surgical step recognition" is translated as "自动化手术步骤识别" (Zì huò zhì yì xiǎng zhì yì bù qiǎo yì)* "state-of-the-art methods" is translated as "状态之arteMethods" (zhì yì bù qiǎo yì)* "spatial and temporal information" is translated as "空间和时间信息" (kōng jiān yǔ shí jiān xìn xīng)* "short-range temporal resolution" is translated as "短范围的时间分辨率" (duǎn fāng yuè de shí jiān fēn xiǎn)* "long-range information" is translated as "长范围的信息" (cháng fāng yuè de xìn xīng)* "gated-temporal attention" is translated as "关闭时间注意" (guān bì shí jiān zhù yì)* "short-term and long-term spatio-temporal feature representations" is translated as "短期和长期的空间时间特征表示" (duǎn qī yǔ cháng qī de kōng jiān shí jiān tè biǎo xiǎng)* "Cataract-101 and D99" is translated as "Cataract-101和D99" (Katāract-101 yǔ D99)* "superior performance" is translated as "优秀的表现" (yù xiù de biǎo xiǎng)Note that the translation is done using Simplified Chinese, which is the most widely used Chinese script in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Brain2Music-Reconstructing-Music-from-Human-Brain-Activity"><a href="#Brain2Music-Reconstructing-Music-from-Human-Brain-Activity" class="headerlink" title="Brain2Music: Reconstructing Music from Human Brain Activity"></a>Brain2Music: Reconstructing Music from Human Brain Activity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11078">http://arxiv.org/abs/2307.11078</a></li>
<li>repo_url: None</li>
<li>paper_authors: Timo I. Denk, Yu Takagi, Takuya Matsuyama, Andrea Agostinelli, Tomoya Nakai, Christian Frank, Shinji Nishimoto</li>
<li>for: 本研究旨在利用Functional Magnetic Resonance Imaging（fMRI）记录的大脑活动，重建音乐。</li>
<li>methods: 该方法使用音乐检索或MusicLM音乐生成模型， Conditioned on fMRI数据中的嵌入。</li>
<li>results: 生成的音乐与人类主体经验的音乐相似，具有semantic property如种类、乐器编制和情感。<details>
<summary>Abstract</summary>
The process of reconstructing experiences from human brain activity offers a unique lens into how the brain interprets and represents the world. In this paper, we introduce a method for reconstructing music from brain activity, captured using functional magnetic resonance imaging (fMRI). Our approach uses either music retrieval or the MusicLM music generation model conditioned on embeddings derived from fMRI data. The generated music resembles the musical stimuli that human subjects experienced, with respect to semantic properties like genre, instrumentation, and mood. We investigate the relationship between different components of MusicLM and brain activity through a voxel-wise encoding modeling analysis. Furthermore, we discuss which brain regions represent information derived from purely textual descriptions of music stimuli. We provide supplementary material including examples of the reconstructed music at https://google-research.github.io/seanet/brain2music
</details>
<details>
<summary>摘要</summary>
人脑活动重建经验提供了一种独特的视角，可以帮助我们理解脑如何解释和表示世界。在这篇论文中，我们介绍了一种利用功能核磁共振成像（fMRI）记录的脑动活动，并将其转换为乐曲。我们的方法使用 either 乐曲检索或 MusicLM 音乐生成模型，conditional 在 embeddings  derived from fMRI 数据上。生成的乐曲具有人类试验者所经历的乐曲Semantic 性质，如乐曲类型、乐器演奏和情感。我们通过 voxel-wise 编码模型分析了不同 MusicLM 组件和脑动活动之间的关系。此外，我们还讨论了脑区划分表示来自文本描述的乐曲 stimuli 的信息。我们在 [https://google-research.github.io/seanet/brain2music](https://google-research.github.io/seanet/brain2music) 提供了补充材料，包括重建的乐曲示例。
</details></li>
</ul>
<hr>
<h2 id="AlignDet-Aligning-Pre-training-and-Fine-tuning-in-Object-Detection"><a href="#AlignDet-Aligning-Pre-training-and-Fine-tuning-in-Object-Detection" class="headerlink" title="AlignDet: Aligning Pre-training and Fine-tuning in Object Detection"></a>AlignDet: Aligning Pre-training and Fine-tuning in Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11077">http://arxiv.org/abs/2307.11077</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liming-ai/AlignDet">https://github.com/liming-ai/AlignDet</a></li>
<li>paper_authors: Ming Li, Jie Wu, Xionghui Wang, Chen Chen, Jie Qin, Xuefeng Xiao, Rui Wang, Min Zheng, Xin Pan<br>for: 这个 paper 的目的是要提出一个单一的预训框架，以便将它适应到不同的检测器，以解决现有的预训与下测试过程中的不一致问题，以提高检测器的性能、通用度和调整速度。methods: 这个 paper 使用了一个名为 AlignDet 的架构，它将预训过程分为两个阶段：图像领域的预训和盒子领域的预训。图像领域的预训将检测器的背景抽象获得到整体的见解，而盒子领域的预训则将获得实例水平的 semantics 和任务相关的概念，以初始化检测器的部件。这个架构可以在无监督的模式下预训所有模组。results: 实验结果显示，AlignDet 可以在不同的协议、模型背景、数据设定和训练程式中实现显著的改善，例如，对 FCOS 的改善为 5.3 mAP，对 RetinaNet 的改善为 2.1 mAP，对 Faster R-CNN 的改善为 3.3 mAP，对 DETR 的改善为 2.3 mAP，并且需要 fewer epochs。<details>
<summary>Abstract</summary>
The paradigm of large-scale pre-training followed by downstream fine-tuning has been widely employed in various object detection algorithms. In this paper, we reveal discrepancies in data, model, and task between the pre-training and fine-tuning procedure in existing practices, which implicitly limit the detector's performance, generalization ability, and convergence speed. To this end, we propose AlignDet, a unified pre-training framework that can be adapted to various existing detectors to alleviate the discrepancies. AlignDet decouples the pre-training process into two stages, i.e., image-domain and box-domain pre-training. The image-domain pre-training optimizes the detection backbone to capture holistic visual abstraction, and box-domain pre-training learns instance-level semantics and task-aware concepts to initialize the parts out of the backbone. By incorporating the self-supervised pre-trained backbones, we can pre-train all modules for various detectors in an unsupervised paradigm. As depicted in Figure 1, extensive experiments demonstrate that AlignDet can achieve significant improvements across diverse protocols, such as detection algorithm, model backbone, data setting, and training schedule. For example, AlignDet improves FCOS by 5.3 mAP, RetinaNet by 2.1 mAP, Faster R-CNN by 3.3 mAP, and DETR by 2.3 mAP under fewer epochs.
</details>
<details>
<summary>摘要</summary>
大量预训练后下渠化已成为各种物体检测算法的广泛采用方法。在这篇论文中，我们发现了预训练和下渠化过程中数据、模型和任务之间的不一致，这些不一致隐藏了检测器的性能、通用能力和收敛速度。为此，我们提出了AlignDet，一个可以与不同的检测器结合使用的统一预训练框架。AlignDet将预训练过程分为两个阶段：图像领域预训练和盒子领域预训练。图像领域预训练可以使检测后缘捕捉整体视觉抽象，而盒子领域预训练可以学习实例水平 semantics和任务相关概念，从而初始化检测器的各部分。通过 incorporating 自动预训练后缘，我们可以在无监督模式下预训练各种检测器的所有模块。根据 Figure 1 所示，我们进行了广泛的实验，显示 AlignDet 可以在不同的协议下实现显著的提升，例如检测算法、模型后缘、数据设置和训练计划。例如，AlignDet 可以提高 FCOS 的 mAP 值 5.3，RetinaNet 的 mAP 值 2.1，Faster R-CNN 的 mAP 值 3.3，和 DETR 的 mAP 值 2.3，并且在更少的训练 epoch 下达到这些提升。
</details></li>
</ul>
<hr>
<h2 id="Effectiveness-and-predictability-of-in-network-storage-cache-for-scientific-workflows"><a href="#Effectiveness-and-predictability-of-in-network-storage-cache-for-scientific-workflows" class="headerlink" title="Effectiveness and predictability of in-network storage cache for scientific workflows"></a>Effectiveness and predictability of in-network storage cache for scientific workflows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11069">http://arxiv.org/abs/2307.11069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Caitlin Sim, Kesheng Wu, Alex Sim, Inder Monga, Chin Guok, Frank Wurthwein, Diego Davila, Harvey Newman, Justas Balcas</li>
<li>for: 这个论文是研究如何通过地区数据存储缓存来减少广域网络流量和数据访问延迟，以提高科学应用中数据访问效率的。</li>
<li>methods: 这个论文使用了对南加利福尼亚 petabyte 级缓存系统的操作记录进行分析，并通过建立机器学习模型来研究缓存行为的预测性。</li>
<li>results: 研究发现，该缓存系统已经将67.6%的文件请求从广域网络中移除，并将每天的网络流量减少了12.3TB（或35.4%）。然而，由于数据访问模式的差异，缓存系统已经实施了不会将小文件逐出缓存的策略。此外，建立的机器学习模型能够准确预测缓存访问、缓存失败和网络吞吐量，这将对未来的资源配置和规划研究提供有用的信息。<details>
<summary>Abstract</summary>
Large scientific collaborations often have multiple scientists accessing the same set of files while doing different analyses, which create repeated accesses to the large amounts of shared data located far away. These data accesses have long latency due to distance and occupy the limited bandwidth available over the wide-area network. To reduce the wide-area network traffic and the data access latency, regional data storage caches have been installed as a new networking service. To study the effectiveness of such a cache system in scientific applications, we examine the Southern California Petabyte Scale Cache for a high-energy physics experiment. By examining about 3TB of operational logs, we show that this cache removed 67.6% of file requests from the wide-area network and reduced the traffic volume on wide-area network by 12.3TB (or 35.4%) an average day. The reduction in the traffic volume (35.4%) is less than the reduction in file counts (67.6%) because the larger files are less likely to be reused. Due to this difference in data access patterns, the cache system has implemented a policy to avoid evicting smaller files when processing larger files. We also build a machine learning model to study the predictability of the cache behavior. Tests show that this model is able to accurately predict the cache accesses, cache misses, and network throughput, making the model useful for future studies on resource provisioning and planning.
</details>
<details>
<summary>摘要</summary>
大型科学合作项目经常有多名科学家访问同一组文件进行不同分析，导致远程数据的重复访问，这些数据访问具有较长延迟时间和占用有限的宽带网络带宽。为了减少宽带网络流量和数据访问延迟时间，我们在科学应用中引入了区域数据存储缓存服务。我们通过分析南加利福尼亚州 petabyte 缓存系统的操作日志约 3TB，发现该缓存系统可以将宽带网络流量减少 35.4%，并将文件请求从宽带网络中除除 67.6%。由于数据访问模式的不同，缓存系统实施了不要把小文件从缓存中移除的策略。我们还建立了一个机器学习模型来研究缓存行为的预测性。测试显示该模型能准确预测缓存访问、缓存失败和网络吞吐量，使其成为未来资源配置和规划研究的有用工具。
</details></li>
</ul>
<hr>
<h2 id="A-LLM-Assisted-Exploitation-of-AI-Guardian"><a href="#A-LLM-Assisted-Exploitation-of-AI-Guardian" class="headerlink" title="A LLM Assisted Exploitation of AI-Guardian"></a>A LLM Assisted Exploitation of AI-Guardian</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15008">http://arxiv.org/abs/2307.15008</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicholas Carlini</li>
<li>for: 这篇论文是为了研究GPT-4是否能够帮助研究人员在针对式机器学习领域进行研究。</li>
<li>methods: 这篇论文使用了GPT-4语言模型来实现攻击AI-Guardian防御机制的方法。</li>
<li>results: 研究发现，使用GPT-4语言模型可以快速和有效地实现攻击AI-Guardian防御机制，并且在某些情况下，GPT-4可以更快速地生成攻击代码于作者本人。<details>
<summary>Abstract</summary>
Large language models (LLMs) are now highly capable at a diverse range of tasks. This paper studies whether or not GPT-4, one such LLM, is capable of assisting researchers in the field of adversarial machine learning. As a case study, we evaluate the robustness of AI-Guardian, a recent defense to adversarial examples published at IEEE S&P 2023, a top computer security conference. We completely break this defense: the proposed scheme does not increase robustness compared to an undefended baseline.   We write none of the code to attack this model, and instead prompt GPT-4 to implement all attack algorithms following our instructions and guidance. This process was surprisingly effective and efficient, with the language model at times producing code from ambiguous instructions faster than the author of this paper could have done. We conclude by discussing (1) the warning signs present in the evaluation that suggested to us AI-Guardian would be broken, and (2) our experience with designing attacks and performing novel research using the most recent advances in language modeling.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）现在可以执行多种任务，这篇研究是否GPT-4，一个这些LLM，可以帮助研究人员在反攻击机器学习领域中工作。作为一个案例研究，我们评估了IEEE S&P 2023年会上发表的AI-Guardian防御反攻击例程的可靠性。我们发现：提案的方案不会增加防御性，相比于无防御基准。我们不写任何攻击代码，而是让GPT-4透过我们的指导和指令来实现攻击算法。这个过程发现很有效率和高效，GPT-4在某些情况下可以从模糊的指令中输出代码比我们这篇文章的作者更快。我们结论是：在评估中发现AI-Guardian将被破坏的警示signals，以及我们在使用最新的语言模型技术进行设计攻击和进行原创研究的经验。
</details></li>
</ul>
<hr>
<h2 id="Breadcrumbs-to-the-Goal-Goal-Conditioned-Exploration-from-Human-in-the-Loop-Feedback"><a href="#Breadcrumbs-to-the-Goal-Goal-Conditioned-Exploration-from-Human-in-the-Loop-Feedback" class="headerlink" title="Breadcrumbs to the Goal: Goal-Conditioned Exploration from Human-in-the-Loop Feedback"></a>Breadcrumbs to the Goal: Goal-Conditioned Exploration from Human-in-the-Loop Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11049">http://arxiv.org/abs/2307.11049</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/improbable-ai/human-guided-exploration">https://github.com/improbable-ai/human-guided-exploration</a></li>
<li>paper_authors: Marcel Torne, Max Balsells, Zihan Wang, Samedh Desai, Tao Chen, Pulkit Agrawal, Abhishek Gupta</li>
<li>for:  solve sequential decision-making tasks requiring expansive exploration without careful design of reward functions or use of novelty-seeking exploration bonuses</li>
<li>methods: use low-quality feedback from non-expert users that may be sporadic, asynchronous, and noisy to guide exploration, bifurcating human feedback and policy learning</li>
<li>results: learn policies with no hand-crafted reward design or exploration bonuses, and scale to learning directly on real-world robots using occasional, asynchronous feedback from human supervisors<details>
<summary>Abstract</summary>
Exploration and reward specification are fundamental and intertwined challenges for reinforcement learning. Solving sequential decision-making tasks requiring expansive exploration requires either careful design of reward functions or the use of novelty-seeking exploration bonuses. Human supervisors can provide effective guidance in the loop to direct the exploration process, but prior methods to leverage this guidance require constant synchronous high-quality human feedback, which is expensive and impractical to obtain. In this work, we present a technique called Human Guided Exploration (HuGE), which uses low-quality feedback from non-expert users that may be sporadic, asynchronous, and noisy. HuGE guides exploration for reinforcement learning not only in simulation but also in the real world, all without meticulous reward specification. The key concept involves bifurcating human feedback and policy learning: human feedback steers exploration, while self-supervised learning from the exploration data yields unbiased policies. This procedure can leverage noisy, asynchronous human feedback to learn policies with no hand-crafted reward design or exploration bonuses. HuGE is able to learn a variety of challenging multi-stage robotic navigation and manipulation tasks in simulation using crowdsourced feedback from non-expert users. Moreover, this paradigm can be scaled to learning directly on real-world robots, using occasional, asynchronous feedback from human supervisors.
</details>
<details>
<summary>摘要</summary>
文本翻译为简化中文：探索和奖励规定是机器学习中的基本和紧密相关挑战。解决需要广泛探索的序列决策任务需要 either 精心设计奖励函数或使用新鲜事物奖励。人工指导可以提供有效的导航，但先前的方法需要高质量的同步人工反馈，这是昂贵和实际不可能获得的。在这项工作中，我们提出了一种技术called HuGE（人类导航探索），它使用低质量的非专家用户反馈，该反馈可能是间歇的、异步和噪音。HuGE将人类反馈与策略学习分离，人类反馈导引探索，而自我超vised学习从探索数据获得无偏度策略。这种方法可以利用不精确、异步的人类反馈来学习无需手动设计奖励或探索奖励。HuGE可以在模拟环境中学习多个复杂的机器人导航和抓取任务，并且可以扩展到直接在实际世界中学习，使用 occasional、异步的人类反馈。</SYS>
</details></li>
</ul>
<hr>
<h2 id="A-Definition-of-Continual-Reinforcement-Learning"><a href="#A-Definition-of-Continual-Reinforcement-Learning" class="headerlink" title="A Definition of Continual Reinforcement Learning"></a>A Definition of Continual Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11046">http://arxiv.org/abs/2307.11046</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Abel, André Barreto, Benjamin Van Roy, Doina Precup, Hado van Hasselt, Satinder Singh</li>
<li>for: 本研究旨在开发一种基础 continual reinforcement learning 方法。</li>
<li>methods: 本文使用的方法包括 &lt;insert methods used in the paper, e.g. experience replay, memory-based methods, etc.&gt;。</li>
<li>results: 本研究实现了 &lt;insert main results of the paper, e.g. improved performance on benchmark tasks, etc.&gt;。<details>
<summary>Abstract</summary>
In this paper we develop a foundation for continual reinforcement learning.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们开发了一个基础 для连续强化学习。Here's the word-for-word translation:在这篇论文中，我们开发了一个基础于连续强化学习。Note that "连续强化学习" (liánxù qiángxīn xuéxí) is a compound phrase in Chinese, where "连续" (liánxù) means "continuous" and "强化学习" (qiángxīn xuéxí) means "reinforcement learning".
</details></li>
</ul>
<hr>
<h2 id="On-the-Convergence-of-Bounded-Agents"><a href="#On-the-Convergence-of-Bounded-Agents" class="headerlink" title="On the Convergence of Bounded Agents"></a>On the Convergence of Bounded Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11044">http://arxiv.org/abs/2307.11044</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Abel, André Barreto, Hado van Hasselt, Benjamin Van Roy, Doina Precup, Satinder Singh</li>
<li>for: 本文研究了agent convergence的定义和性质。</li>
<li>methods: 作者提出了两种补做agent convergence的方法，即基于环境状态的方法和基于代理器内部状态的方法。</li>
<li>results: 作者证明了这两种方法的基本性质和关系，并在标准设置中验证了它们的正确性。<details>
<summary>Abstract</summary>
When has an agent converged? Standard models of the reinforcement learning problem give rise to a straightforward definition of convergence: An agent converges when its behavior or performance in each environment state stops changing. However, as we shift the focus of our learning problem from the environment's state to the agent's state, the concept of an agent's convergence becomes significantly less clear. In this paper, we propose two complementary accounts of agent convergence in a framing of the reinforcement learning problem that centers around bounded agents. The first view says that a bounded agent has converged when the minimal number of states needed to describe the agent's future behavior cannot decrease. The second view says that a bounded agent has converged just when the agent's performance only changes if the agent's internal state changes. We establish basic properties of these two definitions, show that they accommodate typical views of convergence in standard settings, and prove several facts about their nature and relationship. We take these perspectives, definitions, and analysis to bring clarity to a central idea of the field.
</details>
<details>
<summary>摘要</summary>
（简体中文）当一个代理人（agent）已经 converges 时，标准的学习问题模型会提供一个直观的定义：一个代理人 converges 当它在每个环境状态下的行为或性能停止变化。但是，当我们将学习问题的关注点从环境状态转移到代理人的状态时，代理人的 converges 概念变得非常模糊。在这篇论文中，我们提出了两种可 complementary 的代理人 converges 观点，它们都是在缩小代理人（bounded agent）的框架下进行学习问题定义。第一种观点是，一个缩小代理人 converges 当其未来行为所需的最小状态数量不能减少。第二种观点是，一个缩小代理人 converges 当代理人的性能只有在代理人的内部状态发生变化时才会发生变化。我们证明了这两个定义的基本性质，并证明它们能够满足标准设置中的常见看法，以及它们之间的关系。我们通过这些观点、定义和分析来为学习问题中的一个中心概念带来清晰。
</details></li>
</ul>
<hr>
<h2 id="Embroid-Unsupervised-Prediction-Smoothing-Can-Improve-Few-Shot-Classification"><a href="#Embroid-Unsupervised-Prediction-Smoothing-Can-Improve-Few-Shot-Classification" class="headerlink" title="Embroid: Unsupervised Prediction Smoothing Can Improve Few-Shot Classification"></a>Embroid: Unsupervised Prediction Smoothing Can Improve Few-Shot Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11031">http://arxiv.org/abs/2307.11031</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HazyResearch/embroid">https://github.com/HazyResearch/embroid</a></li>
<li>paper_authors: Neel Guha, Mayee F. Chen, Kush Bhatia, Azalia Mirhoseini, Frederic Sala, Christopher Ré</li>
<li>for: 提高语言模型（LM）的推荐学习能力，以便自动化数据标注，尤其是在手动标注是昂贵的领域。</li>
<li>methods: 使用Embroid方法，通过计算不同嵌入函数下的多个表示来改进预言，并使用预测错误的相邻样本来生成额外的预测。</li>
<li>results: Embroid方法可以substantially提高LM的性能（例如，GPT-JT中的平均提高7.3分），同时也可以实现更复杂的提示策略（如链式思维），并可以根据嵌入函数来特化到法律领域。<details>
<summary>Abstract</summary>
Recent work has shown that language models' (LMs) prompt-based learning capabilities make them well suited for automating data labeling in domains where manual annotation is expensive. The challenge is that while writing an initial prompt is cheap, improving a prompt is costly -- practitioners often require significant labeled data in order to evaluate the impact of prompt modifications. Our work asks whether it is possible to improve prompt-based learning without additional labeled data. We approach this problem by attempting to modify the predictions of a prompt, rather than the prompt itself. Our intuition is that accurate predictions should also be consistent: samples which are similar under some feature representation should receive the same prompt prediction. We propose Embroid, a method which computes multiple representations of a dataset under different embedding functions, and uses the consistency between the LM predictions for neighboring samples to identify mispredictions. Embroid then uses these neighborhoods to create additional predictions for each sample, and combines these predictions with a simple latent variable graphical model in order to generate a final corrected prediction. In addition to providing a theoretical analysis of Embroid, we conduct a rigorous empirical evaluation across six different LMs and up to 95 different tasks. We find that (1) Embroid substantially improves performance over original prompts (e.g., by an average of 7.3 points on GPT-JT), (2) also realizes improvements for more sophisticated prompting strategies (e.g., chain-of-thought), and (3) can be specialized to domains like law through the embedding functions.
</details>
<details>
<summary>摘要</summary>
最近的研究表明，语言模型（LM）的提示基本学习能力使其成为自动化数据标注的便利工具。问题在于，虽然写出初始提示便宜，但是改进提示却需要大量标注数据来评估提示修改的影响。我们的工作想知道是否可以不使用更多的标注数据来改进提示。我们的思路是，准确的预测应该也是一致的：样本在某些特征表示下应该有相似的预测。我们提出了一种方法 called Embroid，它使用不同的嵌入函数计算不同的数据集表示，并使用LM的预测差异来identify mispredictions。 Embroid使用这些邻居样本来生成每个样本的额外预测，并将这些预测与一个简单的隐藏变量图模型相结合，以生成最终的修正预测。除了对Embroid进行了理论分析，我们还进行了六个不同的LM和95个任务的严格的实验评估。我们发现：1. Embroid在原始提示基础上大幅提高性能（例如，GPT-JT上的平均提高7.3点）。2. Embroid也可以提高更复杂的提示策略（例如，链条思维）。3. Embroid可以根据嵌入函数特定化到领域，如法律领域。
</details></li>
</ul>
<hr>
<h2 id="Cluster-aware-Semi-supervised-Learning-Relational-Knowledge-Distillation-Provably-Learns-Clustering"><a href="#Cluster-aware-Semi-supervised-Learning-Relational-Knowledge-Distillation-Provably-Learns-Clustering" class="headerlink" title="Cluster-aware Semi-supervised Learning: Relational Knowledge Distillation Provably Learns Clustering"></a>Cluster-aware Semi-supervised Learning: Relational Knowledge Distillation Provably Learns Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11030">http://arxiv.org/abs/2307.11030</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yijun Dong, Kevin Miller, Qi Lei, Rachel Ward</li>
<li>for: 这个论文的目的是提供一种理论基础 для关系知识储存（Relational Knowledge Distillation，RKD），尤其是在 semi-supervised classification 问题上。</li>
<li>methods: 这篇论文使用了 spectral clustering 来解释 RKD，并提供了一个基于 teacher model 生成的人口诱导图来解释 RKD 的效果。</li>
<li>results: 研究人员发现，RKD 可以降低 clustering error，并且提供了一个采样复杂度 bound 以确保 RKD 的效果。此外，研究人员还展示了 RKD 在 semi-supervised learning 中的标签效率，并将数据扩展一致性 regularization  интегрирова到了这个框架中，发现 RKD 可以提供一个 “全局” 的视角，而 consistency regularization 则是一个 “本地” 的视角。<details>
<summary>Abstract</summary>
Despite the empirical success and practical significance of (relational) knowledge distillation that matches (the relations of) features between teacher and student models, the corresponding theoretical interpretations remain limited for various knowledge distillation paradigms. In this work, we take an initial step toward a theoretical understanding of relational knowledge distillation (RKD), with a focus on semi-supervised classification problems. We start by casting RKD as spectral clustering on a population-induced graph unveiled by a teacher model. Via a notion of clustering error that quantifies the discrepancy between the predicted and ground truth clusterings, we illustrate that RKD over the population provably leads to low clustering error. Moreover, we provide a sample complexity bound for RKD with limited unlabeled samples. For semi-supervised learning, we further demonstrate the label efficiency of RKD through a general framework of cluster-aware semi-supervised learning that assumes low clustering errors. Finally, by unifying data augmentation consistency regularization into this cluster-aware framework, we show that despite the common effect of learning accurate clusterings, RKD facilitates a "global" perspective through spectral clustering, whereas consistency regularization focuses on a "local" perspective via expansion.
</details>
<details>
<summary>摘要</summary>
尽管Relational Knowledge Distillation（RKD）在实际应用中取得了成功和实际意义，但对其理论解释仍然有限。在这项工作中，我们首先将RKD视为spectral clustering在教师模型所induced的人口图中进行解释。通过量化预测和真实分类的差异，我们证明RKD在人口上实际下导致低分类错误率。此外，我们还提供了有限无标样本数的样本复杂度下RKD的下界。为semi-supervised learning，我们进一步验证了RKD的标签效率，并在假设低分类错误率的cluster-aware semi-supervised learning框架中展示了RKD的效果。最后，我们将数据扩展一致性正则化纳入这个框架中，并证明RKD可以帮助学习准确的分类，但是consistency regularization则会导致学习"local"的分类。
</details></li>
</ul>
<hr>
<h2 id="Amortized-Variational-Inference-When-and-Why"><a href="#Amortized-Variational-Inference-When-and-Why" class="headerlink" title="Amortized Variational Inference: When and Why?"></a>Amortized Variational Inference: When and Why?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11018">http://arxiv.org/abs/2307.11018</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/charlesm93/amortized_vi">https://github.com/charlesm93/amortized_vi</a></li>
<li>paper_authors: Charles C. Margossian, David M. Blei</li>
<li>for: 这篇论文主要针对 approximate posterior inference 的问题，尤其是在 deep generative models 中使用 amortized variational inference (A-VI) 来加速推理。</li>
<li>methods: 这篇论文研究了 A-VI 作为一种可能的替代方法，以及其在不同模型中的性能。具体来说， authors 提出了一些条件，确定在哪些模型中 A-VI 可以达到 F-VI 的优秀解。同时，他们也研究了 A-VI 的计算复杂度，并发现在某些模型中，A-VI 可以快速 converge 并且不需要与观察数量成正比。</li>
<li>results: 这篇论文的实验结果表明，在某些模型中，A-VI 可以很好地适应推理，并且可以达到 F-VI 的优秀解。然而，在某些模型中，A-VI 无法达到 F-VI 的优秀解，尤其是在 hidden Markov models 和 Gaussian processes 中。<details>
<summary>Abstract</summary>
Amortized variational inference (A-VI) is a method for approximating the intractable posterior distributions that arise in probabilistic models. The defining feature of A-VI is that it learns a global inference function that maps each observation to its local latent variable's approximate posterior. This stands in contrast to the more classical factorized (or mean-field) variational inference (F-VI), which directly learns the parameters of the approximating distribution for each latent variable. In deep generative models, A-VI is used as a computational trick to speed up inference for local latent variables. In this paper, we study A-VI as a general alternative to F-VI for approximate posterior inference. A-VI cannot produce an approximation with a lower Kullback-Leibler divergence than F-VI's optimal solution, because the amortized family is a subset of the factorized family. Thus a central theoretical problem is to characterize when A-VI still attains F-VI's optimal solution. We derive conditions on both the model and the inference function under which A-VI can theoretically achieve F-VI's optimum. We show that for a broad class of hierarchical models, including deep generative models, it is possible to close the gap between A-VI and F-VI. Further, for an even broader class of models, we establish when and how to expand the domain of the inference function to make amortization a feasible strategy. Finally, we prove that for certain models -- including hidden Markov models and Gaussian processes -- A-VI cannot match F-VI's solution, no matter how expressive the inference function is. We also study A-VI empirically. On several examples, we corroborate our theoretical results and investigate the performance of A-VI when varying the complexity of the inference function. When the gap between A-VI and F-VI can be closed, we find that the required complexity of the function need not scale with the number of observations, and that A-VI often converges faster than F-VI.
</details>
<details>
<summary>摘要</summary>
通用简化的变量推理（A-VI）是一种方法，用于估计不可算的 posterior 分布，这些分布出现在概率模型中。A-VI 的特点是学习一个全局的推理函数，该函数将每个观察值映射到其本地隐藏变量的approximate posterior。与经典的分解（或平均场）变量推理（F-VI）不同，A-VI 直接学习每个隐藏变量的参数。在深度生成模型中，A-VI 作为计算技巧，以加速 мест latent 变量的推理。在这篇论文中，我们研究 A-VI 作为一种通用的近似 posterior 推理方法。由于 A-VI 的权化家族是 F-VI 的权化家族的子集，因此 A-VI 不能生成一个与 F-VI 最佳解的更低 Kullback-Leibler 差。我们Derive 了模型和推理函数的条件，以及在哪些情况下 A-VI 可以达到 F-VI 的最佳解。我们证明了在很多层次模型中，包括深度生成模型，可以减小 A-VI 和 F-VI 之间的差距。此外，我们还证明了在一些模型中，扩展推理函数的域可以使得权化成为可能的策略。最后，我们证明了某些模型（如隐藏 Markov 模型和 Gaussian 过程）中，A-VI 无法与 F-VI 的最佳解匹配，无论推理函数如何强大。我们还对 A-VI 进行了实验研究，在几个示例中，我们证明了我们的理论结果，并研究了 A-VI 在不同观察数量时的性能。当可以减小 A-VI 和 F-VI 之间的差距时，我们发现推理函数的复杂度不必与观察数量成正比，并且 A-VI 通常更快 converge 于 F-VI。
</details></li>
</ul>
<hr>
<h2 id="Multi-objective-point-cloud-autoencoders-for-explainable-myocardial-infarction-prediction"><a href="#Multi-objective-point-cloud-autoencoders-for-explainable-myocardial-infarction-prediction" class="headerlink" title="Multi-objective point cloud autoencoders for explainable myocardial infarction prediction"></a>Multi-objective point cloud autoencoders for explainable myocardial infarction prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11017">http://arxiv.org/abs/2307.11017</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcel Beetz, Abhirup Banerjee, Vicente Grau</li>
<li>for: 预测心肺病变（Myocardial Infarction，MI）的技术。</li>
<li>methods: 使用多目标点云自动编码器，一种基于多类3D点云表示心脏解剖和功能的几何深度学习方法，以实现可解释的MI预测。</li>
<li>results: 在大型UK Biobank数据集上，该方法可以准确地重建多个时间点的3D形状，并且在MI预测任务上超过多种机器学习和深度学习参考值 by 19%。同时，该方法的任务特定紧凑的秘密空间可以明显地分离控制和MI群集，并且可以在3D形状和相关的主体编码之间找到严格的相关性，这表明了预测的可解释性。<details>
<summary>Abstract</summary>
Myocardial infarction (MI) is one of the most common causes of death in the world. Image-based biomarkers commonly used in the clinic, such as ejection fraction, fail to capture more complex patterns in the heart's 3D anatomy and thus limit diagnostic accuracy. In this work, we present the multi-objective point cloud autoencoder as a novel geometric deep learning approach for explainable infarction prediction, based on multi-class 3D point cloud representations of cardiac anatomy and function. Its architecture consists of multiple task-specific branches connected by a low-dimensional latent space to allow for effective multi-objective learning of both reconstruction and MI prediction, while capturing pathology-specific 3D shape information in an interpretable latent space. Furthermore, its hierarchical branch design with point cloud-based deep learning operations enables efficient multi-scale feature learning directly on high-resolution anatomy point clouds. In our experiments on a large UK Biobank dataset, the multi-objective point cloud autoencoder is able to accurately reconstruct multi-temporal 3D shapes with Chamfer distances between predicted and input anatomies below the underlying images' pixel resolution. Our method outperforms multiple machine learning and deep learning benchmarks for the task of incident MI prediction by 19% in terms of Area Under the Receiver Operating Characteristic curve. In addition, its task-specific compact latent space exhibits easily separable control and MI clusters with clinically plausible associations between subject encodings and corresponding 3D shapes, thus demonstrating the explainability of the prediction.
</details>
<details>
<summary>摘要</summary>
我occlusion infarction (MI)是全球最常见的死亡原因之一。医学上常用的图像基准标志，如舒张率，无法捕捉心脏三维结构更复杂的模式，因此限制了诊断准确性。在这项工作中，我们介绍了一种新的多目标点云自编码器，作为基于三维心脏形态和功能的可解释梢肢预测的新型几何深度学方法。其架构包括多个任务特定分支，连接了低维度的潜在空间，以实现有效的多目标学习重建和梢肢预测，同时捕捉疾病特定的三维形态信息。此外，其层次分支设计和点云深度运算使得可以直接在高分辨率的生物学点云上进行高效的多级特征学习。在我们对大型UK Biobank数据集进行实验时，多目标点云自编码器能够准确重建多个时间点的三维形态，Chamfer距离输入和预测的形态之间低于图像的像素分辨率。我们的方法比多种机器学习和深度学习标准准降19%以上，在 incident MI 预测任务上。此外，我们的任务特定紧凑的潜在空间可以轻松地分离控制和MI群集，并且与对应的三维形态之间存在严格的相关性，这表明预测的解释性强。
</details></li>
</ul>
<hr>
<h2 id="Flow-Map-Learning-for-Unknown-Dynamical-Systems-Overview-Implementation-and-Benchmarks"><a href="#Flow-Map-Learning-for-Unknown-Dynamical-Systems-Overview-Implementation-and-Benchmarks" class="headerlink" title="Flow Map Learning for Unknown Dynamical Systems: Overview, Implementation, and Benchmarks"></a>Flow Map Learning for Unknown Dynamical Systems: Overview, Implementation, and Benchmarks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11013">http://arxiv.org/abs/2307.11013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Victor Churchill, Dongbin Xiu</li>
<li>for: 本文旨在介绍流Map学习（FML）框架，以及与深度神经网络（DNN）结合使用的数据驱动模型化方法，以便对未知动力系统进行精确预测。</li>
<li>methods: 本文使用了FML框架，并结合深度神经网络进行数据驱动模型化。</li>
<li>results: 本文提供了一组可重复的数字细节和FML结果，以便评估和验证模型的准确性。<details>
<summary>Abstract</summary>
Flow map learning (FML), in conjunction with deep neural networks (DNNs), has shown promises for data driven modeling of unknown dynamical systems. A remarkable feature of FML is that it is capable of producing accurate predictive models for partially observed systems, even when their exact mathematical models do not exist. In this paper, we present an overview of the FML framework, along with the important computational details for its successful implementation. We also present a set of well defined benchmark problems for learning unknown dynamical systems. All the numerical details of these problems are presented, along with their FML results, to ensure that the problems are accessible for cross-examination and the results are reproducible.
</details>
<details>
<summary>摘要</summary>
流图学习（FML），与深度神经网络（DNNs）结合使用，已经显示出对未知动力系统数据驱动模型的承诺。FML可以生成准确预测模型，即使系统的准确数学模型没有存在。在这篇论文中，我们提供FML框架的概述，以及实现成功的计算细节。我们还提供一组已定义的标准问题，以便学习未知动力系统。这些问题的数值细节和FML结果都被详细介绍，以便让问题可以进行检验和结果可以重新生成。
</details></li>
</ul>
<hr>
<h2 id="Neuron-Sensitivity-Guided-Test-Case-Selection-for-Deep-Learning-Testing"><a href="#Neuron-Sensitivity-Guided-Test-Case-Selection-for-Deep-Learning-Testing" class="headerlink" title="Neuron Sensitivity Guided Test Case Selection for Deep Learning Testing"></a>Neuron Sensitivity Guided Test Case Selection for Deep Learning Testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11011">http://arxiv.org/abs/2307.11011</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ase2023paper/nss">https://github.com/ase2023paper/nss</a></li>
<li>paper_authors: Dong Huang, Qingwen Bu, Yichao Fu, Yuhao Qing, Bocheng Xiao, Heming Cui</li>
<li>for: 这篇论文主要应用于测试深度神经网络（DNN）模型中的错误行为，以提高DNN模型的可靠性和安全性。</li>
<li>methods: 本论文提出了一个名为NSS的方法，即Neural Network Sensitivity guided test case Selection，可以从大量的无标的数据中选择有价的测试案例，以减少标注时间和成本。NSS利用内部神经元的信息来选择有价的测试案例，这些测试案例有高度的错误识别和模型改进能力。</li>
<li>results: 根据四个广泛使用的数据集和四个优秀设计的DNN模型进行评估，NSS比基eline方法更好地评估测试案例的错误识别和模型改进能力。具体来说，在MNIST &amp; LeNet1实验中，当选择5%的测试案例时，NSS可以获得81.8%的错误检测率，高于基eline方法20%。<details>
<summary>Abstract</summary>
Deep Neural Networks~(DNNs) have been widely deployed in software to address various tasks~(e.g., autonomous driving, medical diagnosis). However, they could also produce incorrect behaviors that result in financial losses and even threaten human safety. To reveal the incorrect behaviors in DNN and repair them, DNN developers often collect rich unlabeled datasets from the natural world and label them to test the DNN models. However, properly labeling a large number of unlabeled datasets is a highly expensive and time-consuming task.   To address the above-mentioned problem, we propose NSS, Neuron Sensitivity guided test case Selection, which can reduce the labeling time by selecting valuable test cases from unlabeled datasets. NSS leverages the internal neuron's information induced by test cases to select valuable test cases, which have high confidence in causing the model to behave incorrectly. We evaluate NSS with four widely used datasets and four well-designed DNN models compared to SOTA baseline methods. The results show that NSS performs well in assessing the test cases' probability of fault triggering and model improvement capabilities. Specifically, compared with baseline approaches, NSS obtains a higher fault detection rate~(e.g., when selecting 5\% test case from the unlabeled dataset in MNIST \& LeNet1 experiment, NSS can obtain 81.8\% fault detection rate, 20\% higher than baselines).
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）在软件中广泛应用，用于解决多种任务（如自动驾驶和医疗诊断）。然而，它们也可能产生错误行为，导致金融损失和威胁人类安全。为揭示DNN中的错误行为并修复它们，DNN开发者们经常从自然世界中收集丰富的无标注数据集和将其标注，以测试DNN模型。然而，对于大量无标注数据集的正确标注是一项昂贵的时间消耗项。为解决上述问题，我们提出了NSS，即神经元敏感度引导测试案例选择。NSS可以减少标注时间，通过选择值得测试案例来减少标注时间。NSS利用测试案例内神经元信息来选择值得测试案例，这些测试案例具有高自信心使模型行为错误。我们对四个广泛使用的数据集和四种特别设计的DNN模型进行评估，与现有基线方法进行比较。结果表明，NSS在评估测试案例的报错诱发概率和模型改进能力方面表现良好。具体来说，相比基线方法，NSS在MNIST & LeNet1实验中选择5%的无标注数据集时可以获得81.8%的报错检测率，高于基线方法20%。
</details></li>
</ul>
<hr>
<h2 id="Sharpness-Minimization-Algorithms-Do-Not-Only-Minimize-Sharpness-To-Achieve-Better-Generalization"><a href="#Sharpness-Minimization-Algorithms-Do-Not-Only-Minimize-Sharpness-To-Achieve-Better-Generalization" class="headerlink" title="Sharpness Minimization Algorithms Do Not Only Minimize Sharpness To Achieve Better Generalization"></a>Sharpness Minimization Algorithms Do Not Only Minimize Sharpness To Achieve Better Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11007">http://arxiv.org/abs/2307.11007</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaiyue Wen, Zhiyuan Li, Tengyu Ma</li>
<li>for: 这篇论文探讨了为什么过参量神经网络可以泛化的原因，尽管有很多研究。</li>
<li>methods: 这篇论文使用了理论和实验方法来检查现有的解释，发现有三种情况：（1）抽象曲线上的平坦性确实会导致泛化；（2）存在不泛化的最平坦模型和锐度最小化算法，但这些模型并不泛化；（3）有些不泛化的最平坦模型，但锐度最小化算法仍然可以泛化。</li>
<li>results: 这些结果表明，锐度和泛化之间的关系不寻常复杂，不仅取决于数据分布和模型结构，而且锐度最小化算法不仅是为了降低锐度来实现更好的泛化。这证明了现有的解释并不能完全解释泛化神经网络的性能。<details>
<summary>Abstract</summary>
Despite extensive studies, the underlying reason as to why overparameterized neural networks can generalize remains elusive. Existing theory shows that common stochastic optimizers prefer flatter minimizers of the training loss, and thus a natural potential explanation is that flatness implies generalization. This work critically examines this explanation. Through theoretical and empirical investigation, we identify the following three scenarios for two-layer ReLU networks: (1) flatness provably implies generalization; (2) there exist non-generalizing flattest models and sharpness minimization algorithms fail to generalize, and (3) perhaps most surprisingly, there exist non-generalizing flattest models, but sharpness minimization algorithms still generalize. Our results suggest that the relationship between sharpness and generalization subtly depends on the data distributions and the model architectures and sharpness minimization algorithms do not only minimize sharpness to achieve better generalization. This calls for the search for other explanations for the generalization of over-parameterized neural networks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Flatness does imply generalization.2. There exist non-generalizing flattest models, and sharpness minimization algorithms fail to generalize.3. There exist non-generalizing flattest models, but sharpness minimization algorithms still generalize.Our findings suggest that the relationship between sharpness and generalization is complex and depends on the data distribution and model architecture. Simply minimizing sharpness does not guarantee better generalization. This highlights the need for alternative explanations for the generalization of overparameterized neural networks.</details></li>
</ol>
<hr>
<h2 id="Private-Federated-Learning-with-Autotuned-Compression"><a href="#Private-Federated-Learning-with-Autotuned-Compression" class="headerlink" title="Private Federated Learning with Autotuned Compression"></a>Private Federated Learning with Autotuned Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10999">http://arxiv.org/abs/2307.10999</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research/federated">https://github.com/google-research/federated</a></li>
<li>paper_authors: Enayat Ullah, Christopher A. Choquette-Choo, Peter Kairouz, Sewoong Oh</li>
<li>for: 降低private federated learning中的通信量，不需要设置或调整压缩率。</li>
<li>methods: 使用在线方法自动调整压缩率，以保证安全的聚合和差分隐私。</li>
<li>results: 在实际 datasets 上达到了有利的压缩率， без需要调整。<details>
<summary>Abstract</summary>
We propose new techniques for reducing communication in private federated learning without the need for setting or tuning compression rates. Our on-the-fly methods automatically adjust the compression rate based on the error induced during training, while maintaining provable privacy guarantees through the use of secure aggregation and differential privacy. Our techniques are provably instance-optimal for mean estimation, meaning that they can adapt to the ``hardness of the problem" with minimal interactivity. We demonstrate the effectiveness of our approach on real-world datasets by achieving favorable compression rates without the need for tuning.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的技术来减少Private Federated Learning中的通信量，无需设置或调整压缩率。我们的在线方法会根据训练过程中引入的错误自动调整压缩率，同时保持安全的积分和渐变隐私保证。我们的技术可以实时适应问题的难度，并且可以在实际应用中 достичь DESirable的压缩率而无需调整。我们在真实的数据集上进行了实验，并取得了良好的压缩率。
</details></li>
</ul>
<hr>
<h2 id="DREAM-Domain-free-Reverse-Engineering-Attributes-of-Black-box-Model"><a href="#DREAM-Domain-free-Reverse-Engineering-Attributes-of-Black-box-Model" class="headerlink" title="DREAM: Domain-free Reverse Engineering Attributes of Black-box Model"></a>DREAM: Domain-free Reverse Engineering Attributes of Black-box Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10997">http://arxiv.org/abs/2307.10997</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rongqing Li, Jiaqi Yu, Changsheng Li, Wenhan Luo, Ye Yuan, Guoren Wang</li>
<li>for: 这篇论文是为了探讨黑obox神经网络模型的特性揭示问题，即无法获取黑obox模型的训练数据集时，是否可以仍然揭示模型的特性。</li>
<li>methods: 该论文提出了一种基于Out of Distribution（OOD）泛化问题的框架，通过学习一个适应所有域的增强模型，以反向推理黑obox模型的特性。</li>
<li>results: 实验结果表明，该方法在与基elines进行比较时具有显著的优势，能够准确揭示黑obox模型的特性。<details>
<summary>Abstract</summary>
Deep learning models are usually black boxes when deployed on machine learning platforms. Prior works have shown that the attributes ($e.g.$, the number of convolutional layers) of a target black-box neural network can be exposed through a sequence of queries. There is a crucial limitation: these works assume the dataset used for training the target model to be known beforehand and leverage this dataset for model attribute attack. However, it is difficult to access the training dataset of the target black-box model in reality. Therefore, whether the attributes of a target black-box model could be still revealed in this case is doubtful. In this paper, we investigate a new problem of Domain-agnostic Reverse Engineering the Attributes of a black-box target Model, called DREAM, without requiring the availability of the target model's training dataset, and put forward a general and principled framework by casting this problem as an out of distribution (OOD) generalization problem. In this way, we can learn a domain-agnostic model to inversely infer the attributes of a target black-box model with unknown training data. This makes our method one of the kinds that can gracefully apply to an arbitrary domain for model attribute reverse engineering with strong generalization ability. Extensive experimental studies are conducted and the results validate the superiority of our proposed method over the baselines.
</details>
<details>
<summary>摘要</summary>
深度学习模型在机器学习平台上通常是黑obox。先前的研究表明，可以通过一系列查询暴露目标黑obox神经网络的特征（例如，卷积层数）。然而，这些研究假设了已知目标模型的训练集，并利用这个集合进行模型特征攻击。然而，在实际情况中，获取目标模型的训练集是困难的。因此，目标模型的特征是否可以在这种情况下暴露出来是有很大的uncertainty。在这篇论文中，我们研究了一个新的问题：针对黑obox目标模型的域无关特征抽取问题（DREAM），不需要目标模型的训练集。我们提出了一种普遍和原理性的框架，将这个问题划为过去 Distribution（OOD）泛化问题。因此，我们可以通过学习一个域无关模型，反向推断目标黑obox模型的特征。这使得我们的方法可以在任意域上进行模型特征抽取，并且具有强大的泛化能力。我们进行了广泛的实验研究，结果证明了我们的提出方法的优越性。
</details></li>
</ul>
<hr>
<h2 id="Progressive-distillation-diffusion-for-raw-music-generation"><a href="#Progressive-distillation-diffusion-for-raw-music-generation" class="headerlink" title="Progressive distillation diffusion for raw music generation"></a>Progressive distillation diffusion for raw music generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10994">http://arxiv.org/abs/2307.10994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Svetlana Pavlova</li>
<li>for: 这个论文的目的是应用一种新的深度学习方法来生成原始的音频文件。</li>
<li>methods: 这种新的方法基于扩散模型，是最近几年内对图像生成 task 取得出色的一种深度生成模型。</li>
<li>results: 这个论文实现了一种不可onditional 生成模型， Progressive distillation diffusion with 1D U-Net，并对不同参数的影响进行了比较。这种方法能够处理进行逐步的音频处理和生成，并且可以进行 looped generation。<details>
<summary>Abstract</summary>
This paper aims to apply a new deep learning approach to the task of generating raw audio files. It is based on diffusion models, a recent type of deep generative model. This new type of method has recently shown outstanding results with image generation. A lot of focus has been given to those models by the computer vision community. On the other hand, really few have been given for other types of applications such as music generation in waveform domain.   In this paper the model for unconditional generating applied to music is implemented: Progressive distillation diffusion with 1D U-Net. Then, a comparison of different parameters of diffusion and their value in a full result is presented. One big advantage of the methods implemented through this work is the fact that the model is able to deal with progressing audio processing and generating , using transformation from 1-channel 128 x 384 to 3-channel 128 x 128 mel-spectrograms and looped generation. The empirical comparisons are realized across different self-collected datasets.
</details>
<details>
<summary>摘要</summary>
这篇论文旨在应用一种新的深度学习方法来生成原始音频文件。它基于扩散模型，这是最近几年内的深度生成模型。这种新的方法在计算机视觉社区中得到了很多关注。然而，对于其他应用领域，如音乐生成，却受到了非常少的关注。在这篇论文中，我们实现了无条件生成的模型：进步滤波扩散（1D U-Net）。然后，我们对不同扩散参数的值进行了比较，并对全部结果进行了评估。这种方法的一个重要优点是，它可以处理进行进程audio处理和生成，通过将1个通道的128x384转换为3个通道的128x128mel-spectrograms。我们在不同的自收集数据集上进行了实验性比较。
</details></li>
</ul>
<hr>
<h2 id="LLM-Cognitive-Judgements-Differ-From-Human"><a href="#LLM-Cognitive-Judgements-Differ-From-Human" class="headerlink" title="LLM Cognitive Judgements Differ From Human"></a>LLM Cognitive Judgements Differ From Human</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11787">http://arxiv.org/abs/2307.11787</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sotlampr/llm-cognitive-judgements">https://github.com/sotlampr/llm-cognitive-judgements</a></li>
<li>paper_authors: Sotiris Lamprinidis</li>
<li>for:  investigate the cognitive capabilities of Large Language Models (LLMs)</li>
<li>methods:  using GPT-3 and ChatGPT on an limited-data inductive reasoning task</li>
<li>results:  the models’ cognitive judgements are not human-like<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have lately been on the spotlight of researchers, businesses, and consumers alike. While the linguistic capabilities of such models have been studied extensively, there is growing interest in investigating them as cognitive subjects. In the present work I examine GPT-3 and ChatGPT capabilities on an limited-data inductive reasoning task from the cognitive science literature. The results suggest that these models' cognitive judgements are not human-like.
</details>
<details>
<summary>摘要</summary>
大语言模型（LLM）在最近几年来已经受到研究人员、企业和消费者的关注。虽然这些模型的语言能力已经得到了广泛的研究，但是有越来越多的人想 Investigate these models as cognitive subjects。在 presente 的工作中，我 examine GPT-3 和 ChatGPT 在有限数据 inductive reasoning 任务上的能力。结果表明这些模型的认知判断不如人类的。Note: "presente" is a typo, it should be "present work" (现在的工作) in Chinese.
</details></li>
</ul>
<hr>
<h2 id="Investigating-minimizing-the-training-set-fill-distance-in-machine-learning-regression"><a href="#Investigating-minimizing-the-training-set-fill-distance-in-machine-learning-regression" class="headerlink" title="Investigating minimizing the training set fill distance in machine learning regression"></a>Investigating minimizing the training set fill distance in machine learning regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10988">http://arxiv.org/abs/2307.10988</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paolo Climaco, Jochen Garcke</li>
<li>for: 本研究旨在提出一种采样方法，以最大化模型性能而减少计算成本。</li>
<li>methods: 该方法基于采样小训练集从大量未标注数据点中采样，并 derive了一个上界，表示预测误差与训练集填充距离之间的线性关系。</li>
<li>results: 通过两种回归模型在两个 dataset 上进行实验，研究证明了选择填充距离最小化的训练集可以显著减少不同回归模型的最大预测误差，比既有 sampling 方法有很大的优势。<details>
<summary>Abstract</summary>
Many machine learning regression methods leverage large datasets for training predictive models. However, using large datasets may not be feasible due to computational limitations or high labelling costs. Therefore, sampling small training sets from large pools of unlabelled data points is essential to maximize model performance while maintaining computational efficiency. In this work, we study a sampling approach aimed to minimize the fill distance of the selected set. We derive an upper bound for the maximum expected prediction error that linearly depends on the training set fill distance, conditional to the knowledge of data features. For empirical validation, we perform experiments using two regression models on two datasets. We empirically show that selecting a training set by aiming to minimize the fill distance, thereby minimizing the bound, significantly reduces the maximum prediction error of various regression models, outperforming existing sampling approaches by a large margin.
</details>
<details>
<summary>摘要</summary>
很多机器学习回归方法利用大量数据进行训练预测模型。然而，使用大量数据可能并不是可行的，因为计算限制或高标注成本。因此，从大量未标注数据点中采样小训练集是重要的，以最大化模型性能而减少计算成本。在这种情况下，我们研究了一种采样方法，旨在最小化训练集填充距离。我们 derive了一个对各种回归模型的最大预测误差的上界，这个上界与训练集填充距离相互关系。为 empirical 验证，我们在两个回归模型上进行了两个数据集的实验。我们发现，通过选择尽可能少的训练集，以尽可能小的 bound 为目标，可以减少各种回归模型的最大预测误差，并且超过现有采样方法的表现。
</details></li>
</ul>
<hr>
<h2 id="MASR-Metadata-Aware-Speech-Representation"><a href="#MASR-Metadata-Aware-Speech-Representation" class="headerlink" title="MASR: Metadata Aware Speech Representation"></a>MASR: Metadata Aware Speech Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10982">http://arxiv.org/abs/2307.10982</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anjali Raj, Shikhar Bharadwaj, Sriram Ganapathy, Min Ma, Shikhar Vashishth</li>
<li>for: 这篇论文的目的是提出一种Metadata Aware Speech Representation learning框架，以增强使用 metadata 信息来改进语音表示学习。</li>
<li>methods: 该框架使用多种外部知识来增强使用 metadata 信息，包括样本级别的对比相似性矩阵，并可与任何选择的 SSL 方法结合使用。</li>
<li>results: 在多个下游任务中，如语言识别、语音识别和非语义任务中，MASR 表现出显著的性能提升，并提供了深入的语言识别任务分析，以解释如何使用提案的损失函数使 representations 分离 closely 相似的语言。<details>
<summary>Abstract</summary>
In the recent years, speech representation learning is constructed primarily as a self-supervised learning (SSL) task, using the raw audio signal alone, while ignoring the side-information that is often available for a given speech recording. In this paper, we propose MASR, a Metadata Aware Speech Representation learning framework, which addresses the aforementioned limitations. MASR enables the inclusion of multiple external knowledge sources to enhance the utilization of meta-data information. The external knowledge sources are incorporated in the form of sample-level pair-wise similarity matrices that are useful in a hard-mining loss. A key advantage of the MASR framework is that it can be combined with any choice of SSL method. Using MASR representations, we perform evaluations on several downstream tasks such as language identification, speech recognition and other non-semantic tasks such as speaker and emotion recognition. In these experiments, we illustrate significant performance improvements for the MASR over other established benchmarks. We perform a detailed analysis on the language identification task to provide insights on how the proposed loss function enables the representations to separate closely related languages.
</details>
<details>
<summary>摘要</summary>
MASR可以包含多个外部知识源，以增强meta-data信息的利用。这些外部知识源以sample-level对比相似矩阵的形式被integrated，对hard-mining损失提供了有用的信息。MASR框架的一个关键优点是可以与任何SSL方法结合使用。使用MASR表示，我们在多个下游任务中进行了评估，包括语言识别、speech recognition和其他非语义任务，如speaker和情感识别。在这些实验中，我们发现MASR表示有较高的性能提升。为了提供language identification任务中MASR表示的分析，我们进行了详细的分析。我们发现，由于MASR表示可以更好地分离 closely related languages，因此提高了语言识别的性能。
</details></li>
</ul>
<hr>
<h2 id="PATROL-Privacy-Oriented-Pruning-for-Collaborative-Inference-Against-Model-Inversion-Attacks"><a href="#PATROL-Privacy-Oriented-Pruning-for-Collaborative-Inference-Against-Model-Inversion-Attacks" class="headerlink" title="PATROL: Privacy-Oriented Pruning for Collaborative Inference Against Model Inversion Attacks"></a>PATROL: Privacy-Oriented Pruning for Collaborative Inference Against Model Inversion Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10981">http://arxiv.org/abs/2307.10981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiwei Ding, Lan Zhang, Miao Pan, Xiaoyong Yuan</li>
<li>for: 这篇论文的目的是提出一个名为PATROL的隐私导向剪裁方法，以保护在协同推导中的数据隐私。</li>
<li>methods: 这篇论文使用了一种名为协同推导的方法，其中 edge device 首先在本地进行部分深度神经网络 (DNN) 的推导，然后将中间结果上传到云端进行完成推导。此外，这篇论文还使用了两个关键的 ком成分：Lipschitz 规范和反推训练，以增加预测错误和增强目标推导模型。</li>
<li>results: 这篇论文的结果显示，PATROL 可以实现隐私导向剪裁，并且可以增加推导的精度和安全性。实验结果显示，PATROL 可以对数据隐私进行有效的保护，同时仍能保持推导的精度。<details>
<summary>Abstract</summary>
Collaborative inference has been a promising solution to enable resource-constrained edge devices to perform inference using state-of-the-art deep neural networks (DNNs). In collaborative inference, the edge device first feeds the input to a partial DNN locally and then uploads the intermediate result to the cloud to complete the inference. However, recent research indicates model inversion attacks (MIAs) can reconstruct input data from intermediate results, posing serious privacy concerns for collaborative inference. Existing perturbation and cryptography techniques are inefficient and unreliable in defending against MIAs while performing accurate inference. This paper provides a viable solution, named PATROL, which develops privacy-oriented pruning to balance privacy, efficiency, and utility of collaborative inference. PATROL takes advantage of the fact that later layers in a DNN can extract more task-specific features. Given limited local resources for collaborative inference, PATROL intends to deploy more layers at the edge based on pruning techniques to enforce task-specific features for inference and reduce task-irrelevant but sensitive features for privacy preservation. To achieve privacy-oriented pruning, PATROL introduces two key components: Lipschitz regularization and adversarial reconstruction training, which increase the reconstruction errors by reducing the stability of MIAs and enhance the target inference model by adversarial training, respectively.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:合作推理已经是资源有限的边缘设备进行推理使用当前最佳深度神经网络（DNN）的有力解决方案。在合作推理中，边缘设备首先在本地部署partial DNN，然后上传 intermediate result到云中完成推理。然而，最近的研究表明，模型反推攻击（MIAs）可以从intermediate result中重建输入数据，对于合作推理 pose serious privacy concerns。现有的扰动和加密技术是不可靠和不fficient的，无法防止 MIAs 的进行准确的推理。这篇文章提供了一个可行的解决方案，名为 PATROL，该方案通过采用隐私意识的剪辑来平衡隐私、效率和 Utility 的推理。PATROL 利用 DNN  later layer 可以更好地提取任务特定的特征。由于边缘资源有限，PATROL 计划在 Edge 部署更多的层，通过剪辑技术来强制执行任务特定的推理和降低任务不相关 yet sensitive 的特征来保护隐私。为了实现隐私意识的剪辑，PATROL 引入了两个关键组成部分：Lipschitz 常数化和对抗重建训练，这两个部分可以通过减少 MIAs 的稳定性来增加重建错误，并通过对抗训练来提高目标推理模型。
</details></li>
</ul>
<hr>
<h2 id="Globally-Normalising-the-Transducer-for-Streaming-Speech-Recognition"><a href="#Globally-Normalising-the-Transducer-for-Streaming-Speech-Recognition" class="headerlink" title="Globally Normalising the Transducer for Streaming Speech Recognition"></a>Globally Normalising the Transducer for Streaming Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10975">http://arxiv.org/abs/2307.10975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rogier van Dalen</li>
<li>for: 该论文主要探讨了在流处理模式下使用转导器（如RNN-转导器或Conformer-转导器）进行语音识别时的问题。</li>
<li>methods: 该论文提出了一种解决在流处理模式下转导器的数学问题，即使用全局Normalization来更正地评估模型的性能。</li>
<li>results: 根据实验结果，全局Normalization可以将流处理模式下的单词错误率降低9-11%，相当于减少了近半的差距与预先读取模式之间。<details>
<summary>Abstract</summary>
The Transducer (e.g. RNN-Transducer or Conformer-Transducer) generates an output label sequence as it traverses the input sequence. It is straightforward to use in streaming mode, where it generates partial hypotheses before the complete input has been seen. This makes it popular in speech recognition. However, in streaming mode the Transducer has a mathematical flaw which, simply put, restricts the model's ability to change its mind. The fix is to replace local normalisation (e.g. a softmax) with global normalisation, but then the loss function becomes impossible to evaluate exactly. A recent paper proposes to solve this by approximating the model, severely degrading performance. Instead, this paper proposes to approximate the loss function, allowing global normalisation to apply to a state-of-the-art streaming model. Global normalisation reduces its word error rate by 9-11% relative, closing almost half the gap between streaming and lookahead mode.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>> трансдусер（例如 RNN-Transducer 或 Conformer-Transducer）生成输出标签序列，遍历输入序列时 генериate 部分假设。这使得它受欢迎于语音识别中的流式模式下使用。然而，在流式模式下，trasducer 具有一个数学上的缺陷，即限制模型更改意见的能力。这可以通过全局 нормализацию（如 softmax）的替换来解决，但是这会使损失函数无法准确评估。一篇最近的论文提议通过模型的 Approximation 来解决这个问题，但这会严重降低性能。这篇论文提议通过 Approximation 来评估损失函数，使得全局 нормализация可以应用于流式模型，从而降低字错率Relative 9-11%，将流式和预测模式之间的差距减少到一半。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/21/cs.LG_2023_07_21/" data-id="cllsj9wyf001vuv88fm973dhz" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/21/cs.SD_2023_07_21/" class="article-date">
  <time datetime="2023-07-20T16:00:00.000Z" itemprop="datePublished">2023-07-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/21/cs.SD_2023_07_21/">cs.SD - 2023-07-21 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Integrating-Pretrained-ASR-and-LM-to-Perform-Sequence-Generation-for-Spoken-Language-Understanding"><a href="#Integrating-Pretrained-ASR-and-LM-to-Perform-Sequence-Generation-for-Spoken-Language-Understanding" class="headerlink" title="Integrating Pretrained ASR and LM to Perform Sequence Generation for Spoken Language Understanding"></a>Integrating Pretrained ASR and LM to Perform Sequence Generation for Spoken Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11005">http://arxiv.org/abs/2307.11005</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddhant Arora, Hayato Futami, Yosuke Kashiwagi, Emiru Tsunoo, Brian Yan, Shinji Watanabe</li>
<li>for: 这项研究旨在提出一种三阶段端到端 SLU 系统，将预训练的语音识别（ASR）和语言模型（LM）集成到 SLU 框架中，以提高序列生成任务的性能。</li>
<li>methods: 该研究提出了一种三阶段结构，首先使用 ASR 子网络预测 ASR 讯息，然后使用 LM 子网络进行初步的 SLU 预测，最后使用决策子网络 Conditioning 在 ASR 和 LM 子网络的表示上进行最终预测。</li>
<li>results: 研究结果表明，该三阶段 SLU 系统在 SLURP 和 SLUE 两个 benchmark 数据集上表现出色，特别是在声音挑战性较高的词语上。<details>
<summary>Abstract</summary>
There has been an increased interest in the integration of pretrained speech recognition (ASR) and language models (LM) into the SLU framework. However, prior methods often struggle with a vocabulary mismatch between pretrained models, and LM cannot be directly utilized as they diverge from its NLU formulation. In this study, we propose a three-pass end-to-end (E2E) SLU system that effectively integrates ASR and LM subnetworks into the SLU formulation for sequence generation tasks. In the first pass, our architecture predicts ASR transcripts using the ASR subnetwork. This is followed by the LM subnetwork, which makes an initial SLU prediction. Finally, in the third pass, the deliberation subnetwork conditions on representations from the ASR and LM subnetworks to make the final prediction. Our proposed three-pass SLU system shows improved performance over cascaded and E2E SLU models on two benchmark SLU datasets, SLURP and SLUE, especially on acoustically challenging utterances.
</details>
<details>
<summary>摘要</summary>
受欢迎的 интеграciónpretrained speech recognition（ASR）和语言模型（LM）into SLU框架已经引起了更多的关注。然而，先前的方法经常会遇到pretrained模型和LM之间的词汇差异，LM无法直接使用，因为它与NLU表述不匹配。在本研究中，我们提议一种三个过程的端到端（E2E）SLU系统，可以有效地将ASR和LM子网络 integrate into SLU表述。在第一个过程中，我们的架构预测ASR转cripts使用ASR子网络。接着，LM子网络会初步预测SLU。最后，在第三个过程中，妥协子网络通过使用ASR和LM子网络的表示来做最终预测。我们的提议的三个过程SLU系统在两个标准SLU数据集上（SLURP和SLUE）的表现更好，特别是在听力挑战性的词汇上。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/21/cs.SD_2023_07_21/" data-id="cllsj9wz30044uv881dtfe15j" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_07_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/21/eess.AS_2023_07_21/" class="article-date">
  <time datetime="2023-07-20T16:00:00.000Z" itemprop="datePublished">2023-07-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/21/eess.AS_2023_07_21/">eess.AS - 2023-07-21 22:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Topic-Identification-For-Spontaneous-Speech-Enriching-Audio-Features-With-Embedded-Linguistic-Information"><a href="#Topic-Identification-For-Spontaneous-Speech-Enriching-Audio-Features-With-Embedded-Linguistic-Information" class="headerlink" title="Topic Identification For Spontaneous Speech: Enriching Audio Features With Embedded Linguistic Information"></a>Topic Identification For Spontaneous Speech: Enriching Audio Features With Embedded Linguistic Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11450">http://arxiv.org/abs/2307.11450</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aalto-speech/Topic-identification-for-spontaneous-Finnish-speech">https://github.com/aalto-speech/Topic-identification-for-spontaneous-Finnish-speech</a></li>
<li>paper_authors: Dejan Porjazovski, Tamás Grósz, Mikko Kurimo</li>
<li>for: 本研究旨在探讨非标准文本解决方案，以寻找可以在低资源场景中使用的解决方案。</li>
<li>methods: 本研究使用了一系列的音频只和多模态方法，并对这些方法进行了评估。</li>
<li>results: 研究发现，尽管ASR系统不可用，但是听频只的方法仍然可以取得可靠的结果，而多模态方法则可以达到最佳的结果。<details>
<summary>Abstract</summary>
Traditional topic identification solutions from audio rely on an automatic speech recognition system (ASR) to produce transcripts used as input to a text-based model. These approaches work well in high-resource scenarios, where there are sufficient data to train both components of the pipeline. However, in low-resource situations, the ASR system, even if available, produces low-quality transcripts, leading to a bad text-based classifier. Moreover, spontaneous speech containing hesitations can further degrade the performance of the ASR model. In this paper, we investigate alternatives to the standard text-only solutions by comparing audio-only and hybrid techniques of jointly utilising text and audio features. The models evaluated on spontaneous Finnish speech demonstrate that purely audio-based solutions are a viable option when ASR components are not available, while the hybrid multi-modal solutions achieve the best results.
</details>
<details>
<summary>摘要</summary>
传统的话题识别解决方案从音频依赖于自动语音识别系统（ASR）生成讲解用于输入文本模型。这些方法在高资源场景下工作良好，因为有 sufficient数据来训练两个组件的管道。然而，在低资源情况下，ASR系统，即使可用，生成的讲解质量很低，导致文本基于的分类器表现不佳。此外，不可控的自由说话中的停顿可以进一步降低ASR模型的性能。在这篇论文中，我们研究了标准文本仅解决方案的alternatives，比较了听音仅和多模态融合方法的性能。在无法控制的自由 фин兰语言中评估模型，我们发现了没有ASR组件的听音基本解决方案是一个可行的选择，而多模态融合方法在最佳情况下实现了最好的结果。
</details></li>
</ul>
<hr>
<h2 id="MeetEval-A-Toolkit-for-Computation-of-Word-Error-Rates-for-Meeting-Transcription-Systems"><a href="#MeetEval-A-Toolkit-for-Computation-of-Word-Error-Rates-for-Meeting-Transcription-Systems" class="headerlink" title="MeetEval: A Toolkit for Computation of Word Error Rates for Meeting Transcription Systems"></a>MeetEval: A Toolkit for Computation of Word Error Rates for Meeting Transcription Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11394">http://arxiv.org/abs/2307.11394</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thilo von Neumann, Christoph Boeddeker, Marc Delcroix, Reinhold Haeb-Umbach</li>
<li>for: 评估会议笔记系统的评估工具kit</li>
<li>methods: 使用一个统一的接口计算常用的单词错误率(WER)，包括cpWER、ORC WER 和 MIMO WER 等定义，并通过时间约束来确保假设字符串与参照字符串的匹配更加准确</li>
<li>results: 通过增加时间约束来提高匹配算法的质量，并且对于word-level timing信息不可用的情况，提供了一种将segment-level timing信息简化为word-level timing信息的方法，该方法导致的WER与exact word-level annotations匹配的WER相似<details>
<summary>Abstract</summary>
MeetEval is an open-source toolkit to evaluate all kinds of meeting transcription systems. It provides a unified interface for the computation of commonly used Word Error Rates (WERs), specifically cpWER, ORC WER and MIMO WER along other WER definitions. We extend the cpWER computation by a temporal constraint to ensure that only words are identified as correct when the temporal alignment is plausible. This leads to a better quality of the matching of the hypothesis string to the reference string that more closely resembles the actual transcription quality, and a system is penalized if it provides poor time annotations. Since word-level timing information is often not available, we present a way to approximate exact word-level timings from segment-level timings (e.g., a sentence) and show that the approximation leads to a similar WER as a matching with exact word-level annotations. At the same time, the time constraint leads to a speedup of the matching algorithm, which outweighs the additional overhead caused by processing the time stamps.
</details>
<details>
<summary>摘要</summary>
MeetEval 是一个开源工具kit，用于评估各种会议录音系统。它提供一个统一的 interfacel  для计算通用的 Word Error Rates（WER），包括 cpWER、ORC WER 和 MIMO WER 等其他 WER 定义。我们将 cpWER 的计算扩展到加入时间限制，以确保只有在时间对错是可能的情况下才认为字串是正确的。这导致了对假设字串和参考字串的匹配更加精确，并且罚款系统如果它将提供了差劣的时间标签。由于单词水平的时间信息通常不可用，我们提出了一种将 sentence 级时间标签转换为单词级时间标签的方法，并证明这个方法对于 WER 的影响相当于对于实际录音质量的匹配。同时，时间限制导致了匹配算法的速度增加，这个增加的速度超过了对时间标签的处理所增加的额外负载。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/21/eess.AS_2023_07_21/" data-id="cllsj9wzp006cuv884rl00n1e" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/8/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="page-number" href="/page/8/">8</a><span class="page-number current">9</span><a class="page-number" href="/page/10/">10</a><a class="page-number" href="/page/11/">11</a><span class="space">&hellip;</span><a class="page-number" href="/page/17/">17</a><a class="extend next" rel="next" href="/page/10/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'', root:''}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
