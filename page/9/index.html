
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/9/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.LG_2023_10_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/22/cs.LG_2023_10_22/" class="article-date">
  <time datetime="2023-10-22T10:00:00.000Z" itemprop="datePublished">2023-10-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/22/cs.LG_2023_10_22/">cs.LG - 2023-10-22</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Diffusion-Model-Assisted-Supervised-Learning-of-Generative-Models-for-Density-Estimation"><a href="#Diffusion-Model-Assisted-Supervised-Learning-of-Generative-Models-for-Density-Estimation" class="headerlink" title="Diffusion-Model-Assisted Supervised Learning of Generative Models for Density Estimation"></a>Diffusion-Model-Assisted Supervised Learning of Generative Models for Density Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14458">http://arxiv.org/abs/2310.14458</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanfang Liu, Minglei Yang, Zezhong Zhang, Feng Bao, Yanzhao Cao, Guannan Zhang</li>
<li>for: 用于训练生成模型进行密度估计。</li>
<li>methods: 使用分布型抽象模型，包括生成对抗网络、正规化流和自适应神经网络，并使用分布型抽象模型来生成标注数据。</li>
<li>results: 提高了生成模型的训练效率和准确性，不需要使用可逆神经网络和计算Jacobian矩阵。<details>
<summary>Abstract</summary>
We present a supervised learning framework of training generative models for density estimation. Generative models, including generative adversarial networks, normalizing flows, variational auto-encoders, are usually considered as unsupervised learning models, because labeled data are usually unavailable for training. Despite the success of the generative models, there are several issues with the unsupervised training, e.g., requirement of reversible architectures, vanishing gradients, and training instability. To enable supervised learning in generative models, we utilize the score-based diffusion model to generate labeled data. Unlike existing diffusion models that train neural networks to learn the score function, we develop a training-free score estimation method. This approach uses mini-batch-based Monte Carlo estimators to directly approximate the score function at any spatial-temporal location in solving an ordinary differential equation (ODE), corresponding to the reverse-time stochastic differential equation (SDE). This approach can offer both high accuracy and substantial time savings in neural network training. Once the labeled data are generated, we can train a simple fully connected neural network to learn the generative model in the supervised manner. Compared with existing normalizing flow models, our method does not require to use reversible neural networks and avoids the computation of the Jacobian matrix. Compared with existing diffusion models, our method does not need to solve the reverse-time SDE to generate new samples. As a result, the sampling efficiency is significantly improved. We demonstrate the performance of our method by applying it to a set of 2D datasets as well as real data from the UCI repository.
</details>
<details>
<summary>摘要</summary>
我们提出了一个监督式学习框架，用于对密度估计进行生成模型训练。生成模型，包括生成对抗网络、标准化对抗网络和条件 autoencoder，通常被视为无监督式学习模型，因为训练时通常没有标签的资料。 despite the success of the generative models, there are several issues with the unsupervised training, such as the need for reversible architectures, vanishing gradients, and training instability. To enable supervised learning in generative models, we utilize the score-based diffusion model to generate labeled data. Unlike existing diffusion models that train neural networks to learn the score function, we develop a training-free score estimation method. This approach uses mini-batch-based Monte Carlo estimators to directly approximate the score function at any spatial-temporal location in solving an ordinary differential equation (ODE), corresponding to the reverse-time stochastic differential equation (SDE). This approach can offer both high accuracy and substantial time savings in neural network training. Once the labeled data are generated, we can train a simple fully connected neural network to learn the generative model in a supervised manner. Compared with existing normalizing flow models, our method does not require the use of reversible neural networks and avoids the computation of the Jacobian matrix. Compared with existing diffusion models, our method does not need to solve the reverse-time SDE to generate new samples. As a result, the sampling efficiency is significantly improved. We demonstrate the performance of our method by applying it to a set of 2D datasets as well as real data from the UCI repository.
</details></li>
</ul>
<hr>
<h2 id="URegM-a-unified-prediction-model-of-resource-consumption-for-refactoring-software-smells-in-open-source-cloud"><a href="#URegM-a-unified-prediction-model-of-resource-consumption-for-refactoring-software-smells-in-open-source-cloud" class="headerlink" title="URegM: a unified prediction model of resource consumption for refactoring software smells in open source cloud"></a>URegM: a unified prediction model of resource consumption for refactoring software smells in open source cloud</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14444">http://arxiv.org/abs/2310.14444</a></li>
<li>repo_url: None</li>
<li>paper_authors: Asif Imran, Tevfik Kosar</li>
<li>for: 这篇论文是为了提高云计算内部过程资源利用率而写的。</li>
<li>methods: 这篇论文使用了一种名为“统一回归模型”（URegM）来预测代码异臭重构后对云资源的使用。</li>
<li>results: 实验结果表明，URegM可以准确预测代码异臭重构后对云资源的使用。这将帮助云服务提供商更好地规划资源分配和代码重构。<details>
<summary>Abstract</summary>
The low cost and rapid provisioning capabilities have made the cloud a desirable platform to launch complex scientific applications. However, resource utilization optimization is a significant challenge for cloud service providers, since the earlier focus is provided on optimizing resources for the applications that run on the cloud, with a low emphasis being provided on optimizing resource utilization of the cloud computing internal processes. Code refactoring has been associated with improving the maintenance and understanding of software code. However, analyzing the impact of the refactoring source code of the cloud and studying its impact on cloud resource usage require further analysis. In this paper, we propose a framework called Unified Regression Modelling (URegM) which predicts the impact of code smell refactoring on cloud resource usage. We test our experiments in a real-life cloud environment using a complex scientific application as a workload. Results show that URegM is capable of accurately predicting resource consumption due to code smell refactoring. This will permit cloud service providers with advanced knowledge about the impact of refactoring code smells on resource consumption, thus allowing them to plan their resource provisioning and code refactoring more effectively.
</details>
<details>
<summary>摘要</summary>
“低成本和快速提供 capacities 使云平台成为 Complex scientific applications 的吸引力。然而，云服务提供商需要优化资源使其能够更好地使用资源，因为在云计算内部过程中的资源使用不受优化。Code refactoring 有助于改善软件代码的维护和理解。然而，云环境中 refactoring 代码的影响需要进一步的分析。本文提出一个名为 Unified Regression Modelling (URegM) 的框架，可以预测代码臭味 refactoring 对云资源的使用情况。我们在一个真实的云环境中进行实验，使用一个复杂的科学应用作为负载。结果表明，URegM 可以准确预测代码臭味 refactoring 对云资源的使用情况。这将允许云服务提供商在代码 refactoring 和资源分配方面更加有效地规划。”
</details></li>
</ul>
<hr>
<h2 id="EDGE-Improved-Training-and-Sampling-of-EDGE"><a href="#EDGE-Improved-Training-and-Sampling-of-EDGE" class="headerlink" title="EDGE++: Improved Training and Sampling of EDGE"></a>EDGE++: Improved Training and Sampling of EDGE</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14441">http://arxiv.org/abs/2310.14441</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning">https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning</a></li>
<li>paper_authors: Mingyang Wu, Xiaohui Chen, Li-Ping Liu</li>
<li>For: 本研究目的是改进现有的Diffusion-based方法，以提高大型网络生成的效率和生成质量。* Methods: 本文提出了对EDGE模型的两个改进：一是根据度Specific Noise Schedule优化活动节点数量，从而减少内存消耗；二是提出了一种改进的采样方案，以更好地控制生成过程中的相似性。* Results: 实验结果表明，提出的修改不仅提高了生成效率，还提高了生成的图像质量，为大型网络生成任务提供了一个可靠和扩展的解决方案。<details>
<summary>Abstract</summary>
Recently developed deep neural models like NetGAN, CELL, and Variational Graph Autoencoders have made progress but face limitations in replicating key graph statistics on generating large graphs. Diffusion-based methods have emerged as promising alternatives, however, most of them present challenges in computational efficiency and generative performance. EDGE is effective at modeling large networks, but its current denoising approach can be inefficient, often leading to wasted computational resources and potential mismatches in its generation process. In this paper, we propose enhancements to the EDGE model to address these issues. Specifically, we introduce a degree-specific noise schedule that optimizes the number of active nodes at each timestep, significantly reducing memory consumption. Additionally, we present an improved sampling scheme that fine-tunes the generative process, allowing for better control over the similarity between the synthesized and the true network. Our experimental results demonstrate that the proposed modifications not only improve the efficiency but also enhance the accuracy of the generated graphs, offering a robust and scalable solution for graph generation tasks.
</details>
<details>
<summary>摘要</summary>
近期发展的深度神经网络模型如NetGAN、CELL和Variational Graph Autoencoders等已经做出了进步，但是它们在生成大图时面临限制，Diffusion-based方法也在潜在的替代者中出现，但是大多数其中的计算效率和生成性能存在挑战。EDGE模型可以模型大网络，但是其当前的净化方法可能会导致计算资源浪费和生成过程中的匹配问题。在这篇论文中，我们提出了对EDGE模型的改进，包括度量特定的噪声调度，以优化每个时间步中活动节点的数量，显著减少内存占用。此外，我们还提出了改进的采样方案，可以细化生成过程，以更好地控制生成的图和真实图之间的相似性。我们的实验结果表明，我们的修改不仅提高了效率，还提高了生成的图的准确性，提供了一个可靠和扩展的图生成解决方案。
</details></li>
</ul>
<hr>
<h2 id="Fairness-aware-Optimal-Graph-Filter-Design"><a href="#Fairness-aware-Optimal-Graph-Filter-Design" class="headerlink" title="Fairness-aware Optimal Graph Filter Design"></a>Fairness-aware Optimal Graph Filter Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14432">http://arxiv.org/abs/2310.14432</a></li>
<li>repo_url: None</li>
<li>paper_authors: O. Deniz Kose, Yanning Shen, Gonzalo Mateos</li>
<li>for: 本文针对 graph-based learning 中存在的偏见问题进行研究，并提出了一种基于 graph signal processing 的偏见 Mitigation 方法。</li>
<li>methods: 本文使用了 graph filters 来减少 sensitive attributes 和 graph 结构之间的相关性，并通过 convex 问题在 graph спектраль领域中设计了最佳的 filter 设计。</li>
<li>results: 实验表明，提出的方法可以提高 fairness 度并保持相同的 utility，与现有的 fairness-aware 基线方法相比。<details>
<summary>Abstract</summary>
Graphs are mathematical tools that can be used to represent complex real-world interconnected systems, such as financial markets and social networks. Hence, machine learning (ML) over graphs has attracted significant attention recently. However, it has been demonstrated that ML over graphs amplifies the already existing bias towards certain under-represented groups in various decision-making problems due to the information aggregation over biased graph structures. Faced with this challenge, here we take a fresh look at the problem of bias mitigation in graph-based learning by borrowing insights from graph signal processing. Our idea is to introduce predesigned graph filters within an ML pipeline to reduce a novel unsupervised bias measure, namely the correlation between sensitive attributes and the underlying graph connectivity. We show that the optimal design of said filters can be cast as a convex problem in the graph spectral domain. We also formulate a linear programming (LP) problem informed by a theoretical bias analysis, which attains a closed-form solution and leads to a more efficient fairness-aware graph filter. Finally, for a design whose degrees of freedom are independent of the input graph size, we minimize the bias metric over the family of polynomial graph convolutional filters. Our optimal filter designs offer complementary strengths to explore favorable fairness-utility-complexity tradeoffs. For performance evaluation, we conduct extensive and reproducible node classification experiments over real-world networks. Our results show that the proposed framework leads to better fairness measures together with similar utility compared to state-of-the-art fairness-aware baselines.
</details>
<details>
<summary>摘要</summary>
图表是数学工具，可以用来表示复杂的现实世界中的连接系统，如金融市场和社交网络。因此，机器学习（ML）在图表上的应用吸引了广泛的关注。然而，已经证明了ML在图表上会增强现有的偏见，导致各种决策问题中的偏见倾向某些被排除的群体。面临这个挑战，我们借鉴了图像处理的思想，引入了预设计图 filters 来减少敏感特征和图连接性之间的相关性。我们示示了这些筛选器的优化设计可以转化为一个对准的问题，并且可以通过LP问题来减少偏见。最后，我们为独立于输入图表大小的设计，对家族中的多项式图 convolutional filters 进行了最小化偏见度量。我们的优化筛选器设计提供了不同的优势，可以根据偏见、 utility 和复杂度进行质量评估。通过广泛和可重复的节点分类实验，我们的结果表明，我们的框架可以同时实现更好的偏见度量和相似的实用性。
</details></li>
</ul>
<hr>
<h2 id="Clustering-Students-Based-on-Gamification-User-Types-and-Learning-Styles"><a href="#Clustering-Students-Based-on-Gamification-User-Types-and-Learning-Styles" class="headerlink" title="Clustering Students Based on Gamification User Types and Learning Styles"></a>Clustering Students Based on Gamification User Types and Learning Styles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14430">http://arxiv.org/abs/2310.14430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emre Arslan, Atilla Özkaymak, Nesrin Özdener Dönmez</li>
<li>for:  clustering students according to their gamification user types and learning styles</li>
<li>methods: K-means algorithm and Gamification User Type Hexad Scale, Grasha-Riechmann Student Learning Style Scale</li>
<li>results: neutral results with a Silhouette coefficient of 0.12, indicating that the clustering is not satisfactory.<details>
<summary>Abstract</summary>
The aim of this study is clustering students according to their gamification user types and learning styles with the purpose of providing instructors with a new perspective of grouping students in case of clustering which cannot be done by hand when there are multiple scales in data. The data used consists of 251 students who were enrolled at a Turkish state university. When grouping students, K-means algorithm has been utilized as clustering algorithm. As for determining the gamification user types and learning styles of students, Gamification User Type Hexad Scale and Grasha-Riechmann Student Learning Style Scale have been used respectively. Silhouette coefficient is utilized as clustering quality measure. After fitting the algorithm in several ways, highest Silhouette coefficient obtained was 0.12 meaning that results are neutral but not satisfactory. All the statistical operations and data visualizations were made using Python programming language.
</details>
<details>
<summary>摘要</summary>
本研究的目的是根据学生的游戏化用户类型和学习风格进行分群，以提供教师一种新的分组学生的方法，这种方法不可以由手动进行分组，当数据具有多个级别时。该研究使用了251名在土耳其国立大学学习的学生的数据。在分群学生时，使用了K-means算法。为确定学生的游戏化用户类型和学习风格，使用了游戏用户类型六元排序和格拉沙-瑞希曼学生学习风格分型。使用了Silhouette系数作为分群质量度量。经过多种适应，最高的Silhouette系数为0.12，表示结果为中度可行，不够满意。所有统计计算和数据可视化都使用了Python编程语言。
</details></li>
</ul>
<hr>
<h2 id="A-Quadratic-Synchronization-Rule-for-Distributed-Deep-Learning"><a href="#A-Quadratic-Synchronization-Rule-for-Distributed-Deep-Learning" class="headerlink" title="A Quadratic Synchronization Rule for Distributed Deep Learning"></a>A Quadratic Synchronization Rule for Distributed Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14423">http://arxiv.org/abs/2310.14423</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hmgxr128/qsr">https://github.com/hmgxr128/qsr</a></li>
<li>paper_authors: Xinran Gu, Kaifeng Lyu, Sanjeev Arora, Jingzhao Zhang, Longbo Huang<br>for: 本文旨在解决分布式深度学习中同步梯度的问题，特别是在多个节点同时训练大型模型时，通信开销增大。methods: 本文提出了一种名为幂 synchronization rule（QSR）的理论基础方法，该方法在学习率递减过程中动态设置H值，以提高模型的泛化性。results: 对于ImageNet datasets上的ResNet和ViT模型，本文的实验表明，使用QSR的本地梯度方法可以在同步方法中提高测试准确率，并且在16或64个GPU上进行训练时，可以降低训练时间，同时提高验证预测率。<details>
<summary>Abstract</summary>
In distributed deep learning with data parallelism, synchronizing gradients at each training step can cause a huge communication overhead, especially when many nodes work together to train large models. Local gradient methods, such as Local SGD, address this issue by allowing workers to compute locally for $H$ steps without synchronizing with others, hence reducing communication frequency. While $H$ has been viewed as a hyperparameter to trade optimization efficiency for communication cost, recent research indicates that setting a proper $H$ value can lead to generalization improvement. Yet, selecting a proper $H$ is elusive. This work proposes a theory-grounded method for determining $H$, named the Quadratic Synchronization Rule (QSR), which recommends dynamically setting $H$ in proportion to $\frac{1}{\eta^2}$ as the learning rate $\eta$ decays over time. Extensive ImageNet experiments on ResNet and ViT show that local gradient methods with QSR consistently improve the test accuracy over other synchronization strategies. Compared with the standard data parallel training, QSR enables Local AdamW on ViT-B to cut the training time on 16 or 64 GPUs down from 26.7 to 20.2 hours or from 8.6 to 5.5 hours and, at the same time, achieves $1.16\%$ or $0.84\%$ higher top-1 validation accuracy.
</details>
<details>
<summary>摘要</summary>
在分布式深度学习中，每个训练步骤的参数同步过程可能会导致巨大的通信开销，特别是当多个节点同时训练大型模型时。本地梯度方法，如本地SGD，可以通过让工作者在本地计算$H$步骤而降低与其他节点之间的通信频率。虽然$H$被视为训练效率和通信成本之间的权衡因素，但是选择合适的$H$值仍然是一个悬峰。这种工作提出了一种基于理论的方法，称为幂函数同步规则（QSR），该方法在学习率$\eta$逐渐减小过程中，对$H$进行动态设置，并与$\frac{1}{\eta^2}$成正比。经验显示，使用QSR的本地梯度方法在ImageNet上的ResNet和ViT上表现出了比其他同步策略更高的测试准确率。相比标准的数据并行训练，QSR使得Local AdamW在ViT-B上的16或64 GPU上减少了26.7小时训练时间，并同时实现了1.16%或0.84%高的顶部一 validate准精度。
</details></li>
</ul>
<hr>
<h2 id="Data-Augmentation-a-Combined-Inductive-Deductive-Approach-featuring-Answer-Set-Programming"><a href="#Data-Augmentation-a-Combined-Inductive-Deductive-Approach-featuring-Answer-Set-Programming" class="headerlink" title="Data Augmentation: a Combined Inductive-Deductive Approach featuring Answer Set Programming"></a>Data Augmentation: a Combined Inductive-Deductive Approach featuring Answer Set Programming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14413">http://arxiv.org/abs/2310.14413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierangela Bruno, Francesco Calimeri, Cinzia Marte, Simona Perri</li>
<li>for: 建立一个 hybrid inductive-deductive 框架，从有限多个真实标签图像开始，以运用逻辑程式来压缩新图像的结构，并且 garantuee 这些新图像符合领域知识中的组合遵循和特定的欲望。</li>
<li>methods: 使用逻辑程式来压缩新图像的结构，并且使用深度学习来创建photo-realistic 图像。</li>
<li>results: 提出了一个 hybrid inductive-deductive 框架，可以从有限多个真实标签图像开始，以运用逻辑程式来压缩新图像的结构，并且 garantuee 这些新图像符合领域知识中的组合遵循和特定的欲望。<details>
<summary>Abstract</summary>
Although the availability of a large amount of data is usually given for granted, there are relevant scenarios where this is not the case; for instance, in the biomedical/healthcare domain, some applications require to build huge datasets of proper images, but the acquisition of such images is often hard for different reasons (e.g., accessibility, costs, pathology-related variability), thus causing limited and usually imbalanced datasets. Hence, the need for synthesizing photo-realistic images via advanced Data Augmentation techniques is crucial. In this paper we propose a hybrid inductive-deductive approach to the problem; in particular, starting from a limited set of real labeled images, the proposed framework makes use of logic programs for declaratively specifying the structure of new images, that is guaranteed to comply with both a set of constraints coming from the domain knowledge and some specific desiderata. The resulting labeled images undergo a dedicated process based on Deep Learning in charge of creating photo-realistic images that comply with the generated label.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Universal-representation-by-Boltzmann-machines-with-Regularised-Axons"><a href="#Universal-representation-by-Boltzmann-machines-with-Regularised-Axons" class="headerlink" title="Universal representation by Boltzmann machines with Regularised Axons"></a>Universal representation by Boltzmann machines with Regularised Axons</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14395">http://arxiv.org/abs/2310.14395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Przemysław R. Grzybowski, Antoni Jankiewicz, Eloy Piñol, David Cirauqui, Dorota H. Grzybowska, Paweł M. Petrykowski, Miguel Ángel García-March, Maciej Lewenstein, Gorka Muñoz-Gil, Alejandro Pozas-Kerstjens</li>
<li>for: 这篇论文旨在描述一种对 Boltzmann 机器进行规范，以便有效地采样和训练。</li>
<li>methods: 这篇论文使用了对 Boltzmann 机器连接的规范，以控制模型的能量地形，从而使得采样和训练变得更加容易。</li>
<li>results: 论文证明了规范后的 Boltzmann 机器仍能够表示任意分布，并且可以控制数量的能量地峰，以便进行导航式采样和训练。此外，文章还表明了规范后的 Boltzmann 机器可以存储无限多个相关的visible patron，并且可以完美地重新建立。<details>
<summary>Abstract</summary>
It is widely known that Boltzmann machines are capable of representing arbitrary probability distributions over the values of their visible neurons, given enough hidden ones. However, sampling -- and thus training -- these models can be numerically hard. Recently we proposed a regularisation of the connections of Boltzmann machines, in order to control the energy landscape of the model, paving a way for efficient sampling and training. Here we formally prove that such regularised Boltzmann machines preserve the ability to represent arbitrary distributions. This is in conjunction with controlling the number of energy local minima, thus enabling easy \emph{guided} sampling and training. Furthermore, we explicitly show that regularised Boltzmann machines can store exponentially many arbitrarily correlated visible patterns with perfect retrieval, and we connect them to the Dense Associative Memory networks.
</details>
<details>
<summary>摘要</summary>
广泛知道，博尔兹曼机能够表示任意概率分布的值 visible neuron，只要有 enough hidden ones。然而，采样 -- 和因此训练 -- 这些模型可能是数值上的困难。最近，我们提出了 Boltzmann 机Connection 的 regularization，以控制模型的能量地形，使得可以有效地采样和训练。我们正式证明，这些正则化的 Boltzmann 机能够保持表示任意分布的能力。此外，我们还显式地显示了正则化 Boltzmann 机可以存储无限多个相关的可见模式，并且可以完美地重新 retrieve，并与 dense associative memory networks 相连。Note: "visible neurons" and "hidden neurons" are not explicitly translated in the text, as they are not necessary for the meaning of the sentence. However, in Simplified Chinese, "visible neurons" can be translated as "可见神经元" and "hidden neurons" can be translated as "隐藏神经元".
</details></li>
</ul>
<hr>
<h2 id="A-global-product-of-fine-scale-urban-building-height-based-on-spaceborne-lidar"><a href="#A-global-product-of-fine-scale-urban-building-height-based-on-spaceborne-lidar" class="headerlink" title="A global product of fine-scale urban building height based on spaceborne lidar"></a>A global product of fine-scale urban building height based on spaceborne lidar</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14355">http://arxiv.org/abs/2310.14355</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao Ma, Guang Zheng, Chi Xu, L. Monika Moskal, Peng Gong, Qinghua Guo, Huabing Huang, Xuecao Li, Yong Pang, Cheng Wang, Huan Xie, Bailang Yu, Bo Zhao, Yuyu Zhou<br>for:This paper aims to provide a global product of urban building heights with fine spatial resolutions and global coverages, which is essential for achieving the UN’s Sustainable Development Goals (SDGs) and supporting future urban studies.methods:The authors combined the spaceborne lidar instrument of GEDI with multi-sourced data including remotely sensed images (Landsat-8, Sentinel-2, and Sentinel-1) and topographic data to produce a global product of urban building heights with a fine grid size of 150 m around 2020.results:The estimated method of building height samples based on the GEDI data was effective with a Pearson’s r of 0.78 and an RMSE of 3.67 m in comparison to the reference data. The mapping product also demonstrated good performance with a Pearson’s r of 0.71 and an RMSE of 4.60 m. The global urban building height map provides a higher spatial resolution (150 m) with greater inherent details about the spatial heterogeneity and flexibility of updating using the GEDI samples as inputs.<details>
<summary>Abstract</summary>
Characterizing urban environments with broad coverages and high precision is more important than ever for achieving the UN's Sustainable Development Goals (SDGs) as half of the world's populations are living in cities. Urban building height as a fundamental 3D urban structural feature has far-reaching applications. However, so far, producing readily available datasets of recent urban building heights with fine spatial resolutions and global coverages remains a challenging task. Here, we provide an up-to-date global product of urban building heights based on a fine grid size of 150 m around 2020 by combining the spaceborne lidar instrument of GEDI and multi-sourced data including remotely sensed images (i.e., Landsat-8, Sentinel-2, and Sentinel-1) and topographic data. Our results revealed that the estimated method of building height samples based on the GEDI data was effective with 0.78 of Pearson's r and 3.67 m of RMSE in comparison to the reference data. The mapping product also demonstrated good performance as indicated by its strong correlation with the reference data (i.e., Pearson's r = 0.71, RMSE = 4.60 m). Compared with the currently existing products, our global urban building height map holds the ability to provide a higher spatial resolution (i.e., 150 m) with a great level of inherent details about the spatial heterogeneity and flexibility of updating using the GEDI samples as inputs. This work will boost future urban studies across many fields including climate, environmental, ecological, and social sciences.
</details>
<details>
<summary>摘要</summary>
将城市环境Characterizing with broad coverage and high precision是实现联合国可持续发展目标(SDGs)的关键，因为全球人口的一半居住在城市中。城市建筑高度作为城市三维结构特征有着广泛的应用。然而，到目前为止，生成可靠的城市建筑高度数据集，具有细度的高空间分辨率和全球覆盖率，仍然是一项挑战。我们提供了2020年的全球城市建筑高度产品，基于150米的细网格大小，通过结合地面雷达仪器GEDI和多源数据（如卫星图像（Landsat-8、Sentinel-2、Sentinel-1）和地形数据）。我们的结果表明，基于GEDI数据的建筑高度采样方法的效果是良好，Pearson相关系数为0.78，RMSE为3.67米。我们的映射产品也表现出了良好的性能，与参照数据相关系数为0.71，RMSE为4.60米。与现有产品相比，我们的全球城市建筑高度地图具有更高的空间分辨率（150米）和更多的内在细节，可以用GEDI采样作为输入，更好地满足未来城市研究的需求。
</details></li>
</ul>
<hr>
<h2 id="Can-strong-structural-encoding-reduce-the-importance-of-Message-Passing"><a href="#Can-strong-structural-encoding-reduce-the-importance-of-Message-Passing" class="headerlink" title="Can strong structural encoding reduce the importance of Message Passing?"></a>Can strong structural encoding reduce the importance of Message Passing?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15197">http://arxiv.org/abs/2310.15197</a></li>
<li>repo_url: None</li>
<li>paper_authors: Floor Eijkelboom, Erik Bekkers, Michael Bronstein, Francesco Di Giovanni</li>
<li>for: 本文研究了message passing neural networks（MPNNs）在图数据上的应用，特别是如何在图数据上使用特征信息和结构信息来学习节点表示。</li>
<li>methods: 本文提出了一种新的方法，基于矩阵乘法来将特征信息和结构信息相互作用。这种方法与标准的笛卡尔和拼接方法进行比较。</li>
<li>results: 研究结果表明，使用矩阵乘法方法可以在一些任务上减少或完全消除消息传递层，而无需让模型表现下降。这表明，当模型可以构建强的结构编码时，消息传递的重要性相对较低。<details>
<summary>Abstract</summary>
The most prevalent class of neural networks operating on graphs are message passing neural networks (MPNNs), in which the representation of a node is updated iteratively by aggregating information in the 1-hop neighborhood. Since this paradigm for computing node embeddings may prevent the model from learning coarse topological structures, the initial features are often augmented with structural information of the graph, typically in the form of Laplacian eigenvectors or Random Walk transition probabilities. In this work, we explore the contribution of message passing when strong structural encodings are provided. We introduce a novel way of modeling the interaction between feature and structural information based on their tensor product rather than the standard concatenation. The choice of interaction is compared in common scenarios and in settings where the capacity of the message-passing layer is severely reduced and ultimately the message-passing phase is removed altogether. Our results indicate that using tensor-based encodings is always at least on par with the concatenation-based encoding and that it makes the model much more robust when the message passing layers are removed, on some tasks incurring almost no drop in performance. This suggests that the importance of message passing is limited when the model can construct strong structural encodings.
</details>
<details>
<summary>摘要</summary>
最常见的图学习网络是消息传递神经网络（MPNN），它们的节点表示更新是通过邻居信息的聚合来实现的。由于这种方法可能会阻止模型学习大规模的结构特征，因此通常会将初始特征与图结构信息相结合，通常是拉普拉斯特征或游走过程的概率。在这项工作中，我们研究了消息传递在强结构编码下的贡献。我们提出了一种基于维度积 producer 而不是标准拼接的交互方法来模型特征和结构信息之间的互动。我们对于常见的场景和消息传递层的容量减少情况进行比较，最后还移除消息传递阶段 altogether。我们的结果表明，使用维度积编码总是与拼接编码相当，并且在某些任务下，它会减少性能的下降幅度。这表示消息传递的重要性在模型可以构建强结构编码时相对较低。
</details></li>
</ul>
<hr>
<h2 id="Pyramidal-Hidden-Markov-Model-For-Multivariate-Time-Series-Forecasting"><a href="#Pyramidal-Hidden-Markov-Model-For-Multivariate-Time-Series-Forecasting" class="headerlink" title="Pyramidal Hidden Markov Model For Multivariate Time Series Forecasting"></a>Pyramidal Hidden Markov Model For Multivariate Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14341">http://arxiv.org/abs/2310.14341</a></li>
<li>repo_url: None</li>
<li>paper_authors: YeXin Huang</li>
<li>for: 预测时间序列数据的未来值</li>
<li>methods: 提出了一种基于PyramidalHidden Markov Model（PHMM）的时间序列预测方法，利用多步隐марков链模型来捕捉多个多步隐态态</li>
<li>results: 实验结果表明，提出的PHMM模型在多变量时间序列 dataset上比其竞争对手更加出色，能够更好地处理非站点和噪音数据，并建立更加准确和全面的预测结果。<details>
<summary>Abstract</summary>
The Hidden Markov Model (HMM) can predict the future value of a time series based on its current and previous values, making it a powerful algorithm for handling various types of time series. Numerous studies have explored the improvement of HMM using advanced techniques, leading to the development of several variations of HMM. Despite these studies indicating the increased competitiveness of HMM compared to other advanced algorithms, few have recognized the significance and impact of incorporating multistep stochastic states into its performance. In this work, we propose a Pyramidal Hidden Markov Model (PHMM) that can capture multiple multistep stochastic states. Initially, a multistep HMM is designed for extracting short multistep stochastic states. Next, a novel time series forecasting structure is proposed based on PHMM, which utilizes pyramid-like stacking to adaptively identify long multistep stochastic states. By employing these two schemes, our model can effectively handle non-stationary and noisy data, while also establishing long-term dependencies for more accurate and comprehensive forecasting. The experimental results on diverse multivariate time series datasets convincingly demonstrate the superior performance of our proposed PHMM compared to its competitive peers in time series forecasting.
</details>
<details>
<summary>摘要</summary>
隐藏Markov模型（HMM）可以预测时间序列的未来值基于其当前和前一个值，使其成为处理多种时间序列的 poderful算法。许多研究已经探索了HMM的改进，导致了多种HMM的发展。 despite these studies indicating the increased competitiveness of HMM compared to other advanced algorithms, few have recognized the significance and impact of incorporating multistep stochastic states into its performance. 在这种工作中，我们提议一种Pyramidal隐藏Markov模型（PHMM），可以捕捉多个多步骤的随机状态。首先，我们设计了一种多步骤HMM，用于提取短时间内的多步骤随机状态。然后，我们提出了一种基于PHMM的新的时间序列预测结构，使用Pyramid-like的堆叠来适应ively认定长时间内的多步骤随机状态。通过这两种方案，我们的模型可以有效地处理不稳定和噪声掺杂的数据，同时也可以建立长期依赖关系，以更准确和全面的预测。实验结果表明，我们的提议的PHMM在多种多变量时间序列数据集上表现出了superior的性能，与其竞争对手相比。
</details></li>
</ul>
<hr>
<h2 id="PPFL-A-Personalized-Federated-Learning-Framework-for-Heterogeneous-Population"><a href="#PPFL-A-Personalized-Federated-Learning-Framework-for-Heterogeneous-Population" class="headerlink" title="PPFL: A Personalized Federated Learning Framework for Heterogeneous Population"></a>PPFL: A Personalized Federated Learning Framework for Heterogeneous Population</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14337">http://arxiv.org/abs/2310.14337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Di, Yi Yang, Haishan Ye, Xiangyu Chang</li>
<li>for: 本研究旨在开发一种适应个人偏好的 Federated Learning 框架，以保护个人隐私。</li>
<li>methods: 本研究使用 canonical models 捕捉人群的基本特征，并使用 membership vectors 表示客户的偏好。</li>
<li>results: 研究表明，PPFL 方法可以提供substantial insights into client characteristics，并且比 existed Personalized Federated Learning 方法更有优势。<details>
<summary>Abstract</summary>
Personalization aims to characterize individual preferences and is widely applied across many fields. However, conventional personalized methods operate in a centralized manner and potentially expose the raw data when pooling individual information. In this paper, with privacy considerations, we develop a flexible and interpretable personalized framework within the paradigm of Federated Learning, called PPFL (Population Personalized Federated Learning). By leveraging canonical models to capture fundamental characteristics among the heterogeneous population and employing membership vectors to reveal clients' preferences, it models the heterogeneity as clients' varying preferences for these characteristics and provides substantial insights into client characteristics, which is lacking in existing Personalized Federated Learning (PFL) methods. Furthermore, we explore the relationship between our method and three main branches of PFL methods: multi-task PFL, clustered FL, and decoupling PFL, and demonstrate the advantages of PPFL. To solve PPFL (a non-convex constrained optimization problem), we propose a novel random block coordinate descent algorithm and present the convergence property. We conduct experiments on both pathological and practical datasets, and the results validate the effectiveness of PPFL.
</details>
<details>
<summary>摘要</summary>
个人化目标是描述个体偏好，广泛应用于多个领域。然而，传统的个人化方法采用中央化的方式运行，可能暴露个体数据。在这篇论文中，我们考虑隐私问题，开发了一种灵活可解释的个人化框架，称为PPFL（人口个性化联合学习）。我们利用 canonical model 捕捉人口中的基本特征，并使用会员 вектор 表达客户的偏好，以模拟客户对这些特征的差异性，并提供了详细的客户特征信息，其与现有的个性化联合学习方法（PFL）不同。此外，我们还探讨了我们的方法与多任务 PFL、分区 FL 和解除 PFL 的三大分支的关系，并证明 PPFL 的优势。为解决 PPFL （一个非对称约束优化问题），我们提议一种新的随机块坐标下降算法，并证明其收敛性。我们在实验中使用了一些病理和实际的数据集，并 validate PPFL 的效果。
</details></li>
</ul>
<hr>
<h2 id="Finite-Sample-Analysis-of-the-Temporal-Difference-Learning"><a href="#Finite-Sample-Analysis-of-the-Temporal-Difference-Learning" class="headerlink" title="Finite-Sample Analysis of the Temporal Difference Learning"></a>Finite-Sample Analysis of the Temporal Difference Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14286">http://arxiv.org/abs/2310.14286</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sergey Samsonov, Daniil Tiapkin, Alexey Naumov, Eric Moulines</li>
<li>for: 本研究考虑了使用 temporal difference (TD) 方法进行政策评估的 Markov Decision Processes (MDP) 中的性能精细 bounds 问题。</li>
<li>methods: 本文使用了一种简单的算法，即使用 universal 和实例无关的步长，以及 Polyak-Ruppert 尾均值。</li>
<li>results: 本文提供了近似优化的几何和偏差项，以及相应的样本复杂性 bound。 我们的证明技巧基于 refined error bounds  для linear stochastic approximation 以及 TD-type recurrence 中的新稳定性结果。<details>
<summary>Abstract</summary>
In this paper we consider the problem of obtaining sharp bounds for the performance of temporal difference (TD) methods with linear functional approximation for policy evaluation in discounted Markov Decision Processes. We show that a simple algorithm with a universal and instance-independent step size together with Polyak-Ruppert tail averaging is sufficient to obtain near-optimal variance and bias terms. We also provide the respective sample complexity bounds. Our proof technique is based on refined error bounds for linear stochastic approximation together with the novel stability result for the product of random matrices that arise from the TD-type recurrence.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们考虑了使用线性函数 aproximation 的 temporal difference（TD）方法来评估折扣Markov决策过程中的性能。我们显示了一个简单的算法，具有 universal 和实例独立的步长，可以获得near-optimal的偏差和方差项。我们还提供了相应的样本复杂性bound。我们的证明技术基于线性随机化的精细错误 bounds 以及TD型循环中的产品Random Matrices的新稳定性结果。
</details></li>
</ul>
<hr>
<h2 id="Robust-Visual-Imitation-Learning-with-Inverse-Dynamics-Representations"><a href="#Robust-Visual-Imitation-Learning-with-Inverse-Dynamics-Representations" class="headerlink" title="Robust Visual Imitation Learning with Inverse Dynamics Representations"></a>Robust Visual Imitation Learning with Inverse Dynamics Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14274">http://arxiv.org/abs/2310.14274</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyuan Li, Xun Wang, Rongchang Zuo, Kewu Sun, Lingfei Cui, Jishiyu Ding, Peng Liu, Zhe Ma</li>
<li>for: 解决复杂的sequential decision-making问题</li>
<li>methods: 开发了一种 inverse dynamics state representation learning objective，以对学习环境和专家环境进行对齐</li>
<li>results: 在各种视觉扰动和多种视觉控制任务中，可以达到几乎专家水平的性能，与现状的visual IL方法和Robust IL方法显著超越<details>
<summary>Abstract</summary>
Imitation learning (IL) has achieved considerable success in solving complex sequential decision-making problems. However, current IL methods mainly assume that the environment for learning policies is the same as the environment for collecting expert datasets. Therefore, these methods may fail to work when there are slight differences between the learning and expert environments, especially for challenging problems with high-dimensional image observations. However, in real-world scenarios, it is rare to have the chance to collect expert trajectories precisely in the target learning environment. To address this challenge, we propose a novel robust imitation learning approach, where we develop an inverse dynamics state representation learning objective to align the expert environment and the learning environment. With the abstract state representation, we design an effective reward function, which thoroughly measures the similarity between behavior data and expert data not only element-wise, but also from the trajectory level. We conduct extensive experiments to evaluate the proposed approach under various visual perturbations and in diverse visual control tasks. Our approach can achieve a near-expert performance in most environments, and significantly outperforms the state-of-the-art visual IL methods and robust IL methods.
</details>
<details>
<summary>摘要</summary>
“模仿学习（IL）已经在复杂的顺序决策问题上取得了显著的成功。然而，现有的IL方法主要假设学习策略的环境与收集专家数据的环境一致。因此，这些方法可能在环境有所不同时失效，特别是高维图像观察的复杂问题上。然而，在实际场景中，很少有收集专家轨迹的机会， precisely in the target learning environment。为解决这个挑战，我们提出了一种新的稳定的模仿学习方法，其中我们开发了反动动态状态表示学习目标，以对专家环境和学习环境进行对齐。通过抽象状态表示，我们设计了一个有效的奖励函数，该函数不仅在元素级别，还在轨迹级别进行 Similarity Measure between behavior data and expert data。我们进行了广泛的实验来评估我们的方法，并在不同的视觉干扰和多种视觉控制任务中达到了 near-expert 性能。我们的方法可以在大多数环境中达到领先的性能，并在视觉IL方法和稳定IL方法中具有显著优势。”
</details></li>
</ul>
<hr>
<h2 id="Shortcuts-for-causal-discovery-of-nonlinear-models-by-score-matching"><a href="#Shortcuts-for-causal-discovery-of-nonlinear-models-by-score-matching" class="headerlink" title="Shortcuts for causal discovery of nonlinear models by score matching"></a>Shortcuts for causal discovery of nonlinear models by score matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14246">http://arxiv.org/abs/2310.14246</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Montagna, Nicoletta Noceti, Lorenzo Rosasco, Francesco Locatello</li>
<li>for: 这篇论文的目的是对非线性随机数据中的 causal discovery 进行研究，并提出了 ScoreSort 算法来解决非线性模型中的 score 排序问题。</li>
<li>methods: 该论文使用了 simulated data 进行实验，并对非线性模型进行了分析和比较。</li>
<li>results: 研究发现，ScoreSort 算法在非线性模型中具有更高的统计效率，并且可以在多种 synthetic benchmarks 上实现 score-sortability。同时，研究还发现了数据的多样性是评估非线性 causal discovery 方法的重要限制因素，以及在不同的设定下进行详细测试和分析统计性质是对 causal discovery 研究中的重要考虑因素。<details>
<summary>Abstract</summary>
The use of simulated data in the field of causal discovery is ubiquitous due to the scarcity of annotated real data. Recently, Reisach et al., 2021 highlighted the emergence of patterns in simulated linear data, which displays increasing marginal variance in the casual direction. As an ablation in their experiments, Montagna et al., 2023 found that similar patterns may emerge in nonlinear models for the variance of the score vector $\nabla \log p_{\mathbf{X}$, and introduced the ScoreSort algorithm. In this work, we formally define and characterize this score-sortability pattern of nonlinear additive noise models. We find that it defines a class of identifiable (bivariate) causal models overlapping with nonlinear additive noise models. We theoretically demonstrate the advantages of ScoreSort in terms of statistical efficiency compared to prior state-of-the-art score matching-based methods and empirically show the score-sortability of the most common synthetic benchmarks in the literature. Our findings remark (1) the lack of diversity in the data as an important limitation in the evaluation of nonlinear causal discovery approaches, (2) the importance of thoroughly testing different settings within a problem class, and (3) the importance of analyzing statistical properties in causal discovery, where research is often limited to defining identifiability conditions of the model.
</details>
<details>
<summary>摘要</summary>
使用模拟数据在 causal discovery 领域是普遍的，因为真实标注数据的罕见。Recently, Reisach et al. (2021) 指出了模拟数据中的增长性趋势，这种趋势在 causal 方向上 display 增加的边缘方差。在他们的实验中，Montagna et al. (2023) 发现了类似的趋势可能会出现在非线性模型中，并提出了 ScoreSort 算法。在这种工作中，我们正式定义和特征化这种 ScoreSort 模型的评分可排性特征。我们发现这种特征定义了一类可识别的双向 causal 模型，与非线性加性随机噪声模型 overlap 。我们理论上表明 ScoreSort 比 Priori 状态的分配比对方法更高效，并且实际上证明了 literature 中最常见的 sintetic 标准 benchmark 中的评分可排性。我们的发现包括（1）数据的不够多样性是评估非线性 causal discovery 方法的重要限制，（2）在一个问题类中测试不同设置是重要的，（3）在 causal discovery 中分析统计性质是研究中常常被限制的。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Deep-Ensemble-for-Out-of-Distribution-Detection-A-Loss-Landscape-Perspective"><a href="#Revisiting-Deep-Ensemble-for-Out-of-Distribution-Detection-A-Loss-Landscape-Perspective" class="headerlink" title="Revisiting Deep Ensemble for Out-of-Distribution Detection: A Loss Landscape Perspective"></a>Revisiting Deep Ensemble for Out-of-Distribution Detection: A Loss Landscape Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14227">http://arxiv.org/abs/2310.14227</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fanghenshaometeor/ood-mode-ensemble">https://github.com/fanghenshaometeor/ood-mode-ensemble</a></li>
<li>paper_authors: Kun Fang, Qinghua Tao, Xiaolin Huang, Jie Yang</li>
<li>for: 这个研究旨在探讨深度学习模型中的Out-of-Distribution（OoD）检测方法，以及如何从损失地图的角度来探索OoD检测。</li>
<li>methods: 本研究使用了多个独立的模式来探索OoD检测，并通过模式组合来提高OoD检测的性能。</li>
<li>results: 实验结果显示，独立模式之间存在高度的测量不确定性，而模式组合可以将这些不确定性减少，提高OoD检测的可靠性。<details>
<summary>Abstract</summary>
Existing Out-of-Distribution (OoD) detection methods address to detect OoD samples from In-Distribution data (InD) mainly by exploring differences in features, logits and gradients in Deep Neural Networks (DNNs). We in this work propose a new perspective upon loss landscape and mode ensemble to investigate OoD detection. In the optimization of DNNs, there exist many local optima in the parameter space, or namely modes. Interestingly, we observe that these independent modes, which all reach low-loss regions with InD data (training and test data), yet yield significantly different loss landscapes with OoD data. Such an observation provides a novel view to investigate the OoD detection from the loss landscape and further suggests significantly fluctuating OoD detection performance across these modes. For instance, FPR values of the RankFeat method can range from 46.58% to 84.70% among 5 modes, showing uncertain detection performance evaluations across independent modes. Motivated by such diversities on OoD loss landscape across modes, we revisit the deep ensemble method for OoD detection through mode ensemble, leading to improved performance and benefiting the OoD detector with reduced variances. Extensive experiments covering varied OoD detectors and network structures illustrate high variances across modes and also validate the superiority of mode ensemble in boosting OoD detection. We hope this work could attract attention in the view of independent modes in the OoD loss landscape and more reliable evaluations on OoD detectors.
</details>
<details>
<summary>摘要</summary>
现有的Out-of-Distribution（OoD）检测方法主要通过探索特征、搅瑞和梯度在深度神经网络（DNN）中检测OoD样本。在本工作中，我们提出了一新的视角，即损失 landscape和模式ensemble来研究OoD检测。DNN的优化中存在许多本地极值点，即模式。我们发现这些独立的模式，它们都能够在训练和测试数据上达到低损失区域，但是在OoD数据上却导致了显著不同的损失 landscape。这种观察提供了一个新的视角来研究OoD检测，并且建议了通过模式ensemble来改进OoD检测性能。例如，RankFeat方法的FPR值在不同模式中可以从46.58%到84.70%不同，表明OoD检测性能的评估存在很大的不确定性。我们通过模式ensemble来降低这种不确定性，并通过广泛的实验证明了模式ensemble的superiority。我们希望这种新的视角能吸引更多的关注，并且能够提供更可靠的OoD检测评估方法。
</details></li>
</ul>
<hr>
<h2 id="SUT-Active-Defects-Probing-for-Transcompiler-Models"><a href="#SUT-Active-Defects-Probing-for-Transcompiler-Models" class="headerlink" title="SUT: Active Defects Probing for Transcompiler Models"></a>SUT: Active Defects Probing for Transcompiler Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14209">http://arxiv.org/abs/2310.14209</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengnan Qi, Yufan Huang, Maoquan Wang, Yongqiang Yao, Zihan Liu, Bin Gu, Colin Clement, Neel Sundaresan</li>
<li>for: 本研究旨在提出一种新的编程语言翻译评价指标，以检测当前的编程语言翻译模型在目标语言中的基本语法错误。</li>
<li>methods: 本研究使用了一种新的活动报告测试集（SUT），包括一个高度可读性的评价工具 для准确性和测试分数。</li>
<li>results: 实验表明，即使使用了强大的模型如ChatGPT，其仍然在这些基本单元测试上出现错误，相比之前的编程语言翻译任务评价集，其通过率下降了26.15%。此外，评价工具还揭示了这些模型在语法元素方面的不足。<details>
<summary>Abstract</summary>
Automatic Program translation has enormous application value and hence has been attracting significant interest from AI researchers. However, we observe that current program translation models still make elementary syntax errors, particularly, when the target language does not have syntax elements in the source language. Metrics like BLUE, CodeBLUE and computation accuracy may not expose these issues. In this paper we introduce a new metrics for programming language translation and these metrics address these basic syntax errors. We develop a novel active defects probing suite called Syntactic Unit Tests (SUT) which includes a highly interpretable evaluation harness for accuracy and test scoring. Experiments have shown that even powerful models like ChatGPT still make mistakes on these basic unit tests. Specifically, compared to previous program translation task evaluation dataset, its pass rate on our unit tests has decreased by 26.15%. Further our evaluation harness reveal syntactic element errors in which these models exhibit deficiencies.
</details>
<details>
<summary>摘要</summary>
自动程序翻译具有巨大的应用价值，因此吸引了许多人工智能研究者的关注。然而，我们发现当目标语言缺乏源语言语法元素时，当前的程序翻译模型仍然会出现基本语法错误。度量器如BLUE、CodeBLUE和计算准确率可能不会暴露这些问题。在这篇论文中，我们介绍了一种新的程序语言翻译度量器，这些度量器可以捕捉这些基本语法错误。我们开发了一个新的活动缺陷探测组合 called Syntactic Unit Tests (SUT)，该组合包括一个高度可读性评估器和准确分数评价。实验表明，即使是 poderoso的 ChatGPT 模型也会在我们的单元测试中出现错误。具体来说，相比之前的程序翻译任务评估集，我们的单元测试中的通过率下降了26.15%。此外，我们的评估器还揭示了这些模型在语法元素上的缺陷。
</details></li>
</ul>
<hr>
<h2 id="Prompt-Engineering-Through-the-Lens-of-Optimal-Control"><a href="#Prompt-Engineering-Through-the-Lens-of-Optimal-Control" class="headerlink" title="Prompt Engineering Through the Lens of Optimal Control"></a>Prompt Engineering Through the Lens of Optimal Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14201">http://arxiv.org/abs/2310.14201</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Luo, Yiming Tang, Chengfeng Shen, Zhennan Zhou, Bin Dong</li>
<li>for: 该论文旨在探讨Optimal Control Framework的应用于多轮人工智能交互（Prompt Engineering，PE）中，以提高人机交互的效率和效果。</li>
<li>methods: 该论文使用了多轮PE方法，包括单轮PE、多轮PE、集成PE和多智能协作PE等方法，以及一种新的优化控制框架，以系матизи并优化现有PE方法。</li>
<li>results: 该论文提出了一种优化控制框架，可以对多轮PE进行系мати化和优化，并且可以扩展到多智能协作PE和集成PE等方法。这些方法可以提高人机交互的效率和效果，并且具有更好的可解释性和可视化性。<details>
<summary>Abstract</summary>
Prompt Engineering (PE) has emerged as a critical technique for guiding Large Language Models (LLMs) in solving intricate tasks. Its importance is highlighted by its potential to significantly enhance the efficiency and effectiveness of human-machine interaction. As tasks grow increasingly complex, recent advanced PE methods have extended beyond the limitations of single-round interactions to embrace multi-round interactions, which allows for a deeper and more nuanced engagement with LLMs. In this paper, we propose an optimal control framework tailored for multi-round interactions with LLMs. This framework provides a unified mathematical structure that not only systematizes the existing PE methods but also sets the stage for rigorous analytical improvements. Furthermore, we extend this framework to include PE via ensemble methods and multi-agent collaboration, thereby enlarging the scope of applicability. By adopting an optimal control perspective, we offer fresh insights into existing PE methods and highlight theoretical challenges that warrant future research. Besides, our work lays a foundation for the development of more effective and interpretable PE methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Improved-Techniques-for-Training-Consistency-Models"><a href="#Improved-Techniques-for-Training-Consistency-Models" class="headerlink" title="Improved Techniques for Training Consistency Models"></a>Improved Techniques for Training Consistency Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14189">http://arxiv.org/abs/2310.14189</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sallu08/Consistency-Regulariation-FSSL-Naive">https://github.com/sallu08/Consistency-Regulariation-FSSL-Naive</a></li>
<li>paper_authors: Yang Song, Prafulla Dhariwal<br>for:This paper focuses on improving the quality of consistency models, a type of generative model that can sample high-quality data in one step without the need for adversarial training.methods:The authors present several improved techniques for consistency training, including eliminating Exponential Moving Average from the teacher consistency model, adopting Pseudo-Huber losses, and introducing a lognormal noise schedule.results:The authors achieve FID scores of 2.51 and 3.25 on CIFAR-10 and ImageNet $64\times 64$ respectively in a single sampling step, marking a 3.5$\times$ and 4$\times$ improvement compared to prior consistency training approaches. Through two-step sampling, they further reduce FID scores to 2.24 and 2.77 on these two datasets, surpassing those obtained via distillation in both one-step and two-step settings.<details>
<summary>Abstract</summary>
Consistency models are a nascent family of generative models that can sample high quality data in one step without the need for adversarial training. Current consistency models achieve optimal sample quality by distilling from pre-trained diffusion models and employing learned metrics such as LPIPS. However, distillation limits the quality of consistency models to that of the pre-trained diffusion model, and LPIPS causes undesirable bias in evaluation. To tackle these challenges, we present improved techniques for consistency training, where consistency models learn directly from data without distillation. We delve into the theory behind consistency training and identify a previously overlooked flaw, which we address by eliminating Exponential Moving Average from the teacher consistency model. To replace learned metrics like LPIPS, we adopt Pseudo-Huber losses from robust statistics. Additionally, we introduce a lognormal noise schedule for the consistency training objective, and propose to double total discretization steps every set number of training iterations. Combined with better hyperparameter tuning, these modifications enable consistency models to achieve FID scores of 2.51 and 3.25 on CIFAR-10 and ImageNet $64\times 64$ respectively in a single sampling step. These scores mark a 3.5$\times$ and 4$\times$ improvement compared to prior consistency training approaches. Through two-step sampling, we further reduce FID scores to 2.24 and 2.77 on these two datasets, surpassing those obtained via distillation in both one-step and two-step settings, while narrowing the gap between consistency models and other state-of-the-art generative models.
</details>
<details>
<summary>摘要</summary>
《协调模型》是一个新兴的生成模型，可以在一步中采样高质量数据，而不需要对抗学习。现有的协调模型可以达到最佳的样本质量 by 热退 diffusion 模型，并使用学习的度量如 LPIPS。然而，热退限制了协调模型的质量，而 LPIPS 会导致评估中不良的偏见。为了解决这些挑战，我们提出了改进的协调训练方法，协调模型可以直接从数据中学习，无需热退。我们分析了协调训练的理论基础，发现了一个以前未被注意到的缺陷，我们通过从教师协调模型中减少 Exponential Moving Average 来解决这个缺陷。而代替学习度量，我们采用了 Pseudo-Huber 损失函数。此外，我们还引入了 lognormal 噪声程序，并提议在训练迭代中逐步增加总数化步骤。通过更好的hyperparameter优化，这些修改使得协调模型可以在一步中采样 FID 分数为 2.51 和 3.25 的 CIFAR-10 和 ImageNet $64\times 64$ 分别，比前一个 consistency training 方法提高了 3.5 倍和 4 倍。通过两步采样，我们还可以将 FID 分数降低到 2.24 和 2.77，超越了在一步和两步 Setting 中使用热退的分数，同时逐渐趋近其他状态的生成模型。
</details></li>
</ul>
<hr>
<h2 id="A-General-Theory-for-Softmax-Gating-Multinomial-Logistic-Mixture-of-Experts"><a href="#A-General-Theory-for-Softmax-Gating-Multinomial-Logistic-Mixture-of-Experts" class="headerlink" title="A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts"></a>A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14188">http://arxiv.org/abs/2310.14188</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huy Nguyen, Pedram Akbarian, TrungTin Nguyen, Nhat Ho</li>
<li>for: 本文研究了一种基于多个子模型的混合专家（MoE）模型，以提高多种回归和分类应用中的性能。</li>
<li>methods: 本文使用了抽象分析方法，对于回归设置下的MoE模型的性能进行了分析，并提出了一种新的修改了Softmax抽象函数的方法，以解决在部分专家参数消失时的问题。</li>
<li>results: 本文实证了MoE模型在分类问题下的抽象率和参数估计率，并发现在一些专家参数消失时，抽象率会 slower than polynomial 率，但通过修改Softmax抽象函数，可以提高参数估计率。<details>
<summary>Abstract</summary>
Mixture-of-experts (MoE) model incorporates the power of multiple submodels via gating functions to achieve greater performance in numerous regression and classification applications. From a theoretical perspective, while there have been previous attempts to comprehend the behavior of that model under the regression settings through the convergence analysis of maximum likelihood estimation in the Gaussian MoE model, such analysis under the setting of a classification problem has remained missing in the literature. We close this gap by establishing the convergence rates of density estimation and parameter estimation in the softmax gating multinomial logistic MoE model. Notably, when part of the expert parameters vanish, these rates are shown to be slower than polynomial rates owing to an inherent interaction between the softmax gating and expert functions via partial differential equations. To address this issue, we propose using a novel class of modified softmax gating functions which transform the input value before delivering them to the gating functions. As a result, the previous interaction disappears and the parameter estimation rates are significantly improved.
</details>
<details>
<summary>摘要</summary>
归一模型（Mixture-of-experts，MoE）利用多个子模型的力via关键函数实现更高的性能在多种回归和分类应用中。从理论上来说，尚未在文献中对MoE模型在回归设置下的行为进行了深入的分析，而这种分析在分类问题下尚未被探讨。我们填补了这一空白，并证明了softmax关键函数多omial几何MoE模型的整体抽象率和参数估计率在部分专家参数消失时 slower than 多项式率，这是因为softmax关键函数和专家函数之间存在自然的互动。 To address this issue, we propose using a novel class of modified softmax gating functions, which transform the input value before delivering them to the gating functions. As a result, the previous interaction disappears, and the parameter estimation rates are significantly improved.
</details></li>
</ul>
<hr>
<h2 id="Learning-Invariant-Molecular-Representation-in-Latent-Discrete-Space"><a href="#Learning-Invariant-Molecular-Representation-in-Latent-Discrete-Space" class="headerlink" title="Learning Invariant Molecular Representation in Latent Discrete Space"></a>Learning Invariant Molecular Representation in Latent Discrete Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14170">http://arxiv.org/abs/2310.14170</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hicai-zju/imold">https://github.com/hicai-zju/imold</a></li>
<li>paper_authors: Xiang Zhuang, Qiang Zhang, Keyan Ding, Yatao Bian, Xiao Wang, Jingsong Lv, Hongyang Chen, Huajun Chen</li>
<li>for: 这个研究旨在提高药物探索中的分子表示学习，并解决现有方法在不同环境下的训练和测试数据之间的泛化问题。</li>
<li>methods: 我们提出了一个新的框架，叫做“首先编码后分离”，可以将分子表示学习中的不同环境下的数据分类为不变的特征。我们还引入了一个对于训练数据的余域量化模组，以降低训练数据过滤的风险，并保持了Encoder的表达力。</li>
<li>results: 我们的模型在18个真实的分子数据集上进行了广泛的实验，结果显示我们的模型在不同环境下的泛化性比起现有基eline的优胜。我们的代码可以在<a target="_blank" rel="noopener" href="https://github.com/HICAI-ZJU/iMoLD%E4%B8%8A%E5%8F%96%E5%BE%97%E3%80%82">https://github.com/HICAI-ZJU/iMoLD上取得。</a><details>
<summary>Abstract</summary>
Molecular representation learning lays the foundation for drug discovery. However, existing methods suffer from poor out-of-distribution (OOD) generalization, particularly when data for training and testing originate from different environments. To address this issue, we propose a new framework for learning molecular representations that exhibit invariance and robustness against distribution shifts. Specifically, we propose a strategy called ``first-encoding-then-separation'' to identify invariant molecule features in the latent space, which deviates from conventional practices. Prior to the separation step, we introduce a residual vector quantization module that mitigates the over-fitting to training data distributions while preserving the expressivity of encoders. Furthermore, we design a task-agnostic self-supervised learning objective to encourage precise invariance identification, which enables our method widely applicable to a variety of tasks, such as regression and multi-label classification. Extensive experiments on 18 real-world molecular datasets demonstrate that our model achieves stronger generalization against state-of-the-art baselines in the presence of various distribution shifts. Our code is available at https://github.com/HICAI-ZJU/iMoLD.
</details>
<details>
<summary>摘要</summary>
分子表示学习建立了药物发现的基础。然而，现有方法受到不同环境下数据的外部分布Shift的困难，特别是在训练和测试数据来源不同时。为解决这问题，我们提出了一个新的分子表示学习框架，具有不变性和抗分布Shift的特点。具体来说，我们提出了一种名为“首先编码然后分离”的策略，以适应分子特征的不变性。在分离步骤之前，我们引入了一个很 residual vector quantization module，以避免训练数据分布适应而导致的过拟合，同时保持编码器的表达能力。此外，我们设计了一个无关任务的自适应学习目标，以促进精准的不变性标识，这使得我们的方法可以通用于多种任务，如回归和多个标签分类。我们的代码可以在https://github.com/HICAI-ZJU/iMoLD中获取。Extensive experiments on 18 real-world molecular datasets demonstrate that our model achieves stronger generalization against state-of-the-art baselines in the presence of various distribution shifts.
</details></li>
</ul>
<hr>
<h2 id="Ensemble-Learning-for-Graph-Neural-Networks"><a href="#Ensemble-Learning-for-Graph-Neural-Networks" class="headerlink" title="Ensemble Learning for Graph Neural Networks"></a>Ensemble Learning for Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14166">http://arxiv.org/abs/2310.14166</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wongzhenhao/elgnn">https://github.com/wongzhenhao/elgnn</a></li>
<li>paper_authors: Zhen Hao Wong, Ling Yue, Quanming Yao</li>
<li>for: 这篇论文探讨了使用集成学习技术来提高图结构数据中的图神经网络（GNNs）性能和可靠性。</li>
<li>methods: 这篇论文使用了多个GNN模型的不同初始化或架构，并使用了Tree-Structured Parzen Estimator算法确定ensemble weights。</li>
<li>results:  ensemble学习可以增强GNN对复杂图结构数据的分析能力，提高总准确率，降低偏差和方差，降低噪声数据的影响。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have shown success in various fields for learning from graph-structured data. This paper investigates the application of ensemble learning techniques to improve the performance and robustness of Graph Neural Networks (GNNs). By training multiple GNN models with diverse initializations or architectures, we create an ensemble model named ELGNN that captures various aspects of the data and uses the Tree-Structured Parzen Estimator algorithm to determine the ensemble weights. Combining the predictions of these models enhances overall accuracy, reduces bias and variance, and mitigates the impact of noisy data. Our findings demonstrate the efficacy of ensemble learning in enhancing GNN capabilities for analyzing complex graph-structured data. The code is public at https://github.com/wongzhenhao/ELGNN.
</details>
<details>
<summary>摘要</summary>
GRAPH Neural Networks (GNNs) have shown success in various fields for learning from graph-structured data. This paper investigates the application of ensemble learning techniques to improve the performance and robustness of GRAPH Neural Networks (GNNs). By training multiple GNN models with diverse initializations or architectures, we create an ensemble model named ELGNN that captures various aspects of the data and uses the Tree-Structured Parzen Estimator algorithm to determine the ensemble weights. Combining the predictions of these models enhances overall accuracy, reduces bias and variance, and mitigates the impact of noisy data. Our findings demonstrate the efficacy of ensemble learning in enhancing GNN capabilities for analyzing complex graph-structured data. The code is public at https://github.com/wongzhenhao/ELGNN.Note that I've kept the original English names for the concepts and techniques to maintain clarity and consistency.
</details></li>
</ul>
<hr>
<h2 id="α-Fair-Contextual-Bandits"><a href="#α-Fair-Contextual-Bandits" class="headerlink" title="$α$-Fair Contextual Bandits"></a>$α$-Fair Contextual Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14164">http://arxiv.org/abs/2310.14164</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddhant Chaudhary, Abhishek Sinha</li>
<li>for: 本文研究的目标是解决 $\alpha$-Fair Contextual Bandits问题，即在对抗设定中 maximize 全球 $\alpha$- fair  utility function，而不是增加总奖励。</li>
<li>methods: 作者提出了一种有效的算法，可以在全信息设定和抽象反馈设定下保证 approximately sublinear regret。</li>
<li>results: 该算法可以在对抗设定中 garantuee an approximately sublinear regret，并且可以避免闭塞效应和符合法规要求。<details>
<summary>Abstract</summary>
Contextual bandit algorithms are at the core of many applications, including recommender systems, clinical trials, and optimal portfolio selection. One of the most popular problems studied in the contextual bandit literature is to maximize the sum of the rewards in each round by ensuring a sublinear regret against the best-fixed context-dependent policy. However, in many applications, the cumulative reward is not the right objective - the bandit algorithm must be fair in order to avoid the echo-chamber effect and comply with the regulatory requirements. In this paper, we consider the $\alpha$-Fair Contextual Bandits problem, where the objective is to maximize the global $\alpha$-fair utility function - a non-decreasing concave function of the cumulative rewards in the adversarial setting. The problem is challenging due to the non-separability of the objective across rounds. We design an efficient algorithm that guarantees an approximately sublinear regret in the full-information and bandit feedback settings.
</details>
<details>
<summary>摘要</summary>
Contextual bandit algorithms are at the core of many applications, including recommender systems, clinical trials, and optimal portfolio selection. One of the most popular problems studied in the contextual bandit literature is to maximize the sum of the rewards in each round by ensuring a sublinear regret against the best-fixed context-dependent policy. However, in many applications, the cumulative reward is not the right objective - the bandit algorithm must be fair in order to avoid the echo-chamber effect and comply with the regulatory requirements. In this paper, we consider the $\alpha$-Fair Contextual Bandits problem, where the objective is to maximize the global $\alpha$-fair utility function - a non-decreasing concave function of the cumulative rewards in the adversarial setting. The problem is challenging due to the non-separability of the objective across rounds. We design an efficient algorithm that guarantees an approximately sublinear regret in the full-information and bandit feedback settings.Here's the translation in Simplified Chinese:Contextual bandit algorithms 是多种应用的核心，包括推荐系统、临床试验和最佳投资选择。文章中最受欢迎的问题是在各个回合中 maximize 奖励的和，并在最佳固定上下文依赖策略的比较下保持 sublinear 后悔。然而，在许多应用中，总奖励不是正确的目标 - 带刺机器必须公平，以避免闪电室效应和符合规定要求。在这篇文章中，我们考虑 $\alpha $- Fair Contextual Bandits 问题，其目标是在对抗设定下 maximize 全球 $\alpha $- fair  utility 函数 - 一个不递减的凹陷函数。问题具有不可分离性，使得它变得更加挑战。我们设计了一个有效的算法，以保证在充分信息和反馈设定下 approximately sublinear 后悔。
</details></li>
</ul>
<hr>
<h2 id="Promoting-Generalization-for-Exact-Solvers-via-Adversarial-Instance-Augmentation"><a href="#Promoting-Generalization-for-Exact-Solvers-via-Adversarial-Instance-Augmentation" class="headerlink" title="Promoting Generalization for Exact Solvers via Adversarial Instance Augmentation"></a>Promoting Generalization for Exact Solvers via Adversarial Instance Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14161">http://arxiv.org/abs/2310.14161</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoyang Liu, Yufei Kuang, Jie Wang, Xijun Li, Yongdong Zhang, Feng Wu</li>
<li>for: 提高离散线性 программирова（MILP）解题器的效率</li>
<li>methods: 使用学习算法提高MILP解题器的普适性和效率</li>
<li>results: 实验表明，通过生成多种增强实例，AdaSolver可以提高MILP解题器的效率，并且可以在不同的分布下实现remarkable的提高I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
Machine learning has been successfully applied to improve the efficiency of Mixed-Integer Linear Programming (MILP) solvers. However, the learning-based solvers often suffer from severe performance degradation on unseen MILP instances -- especially on large-scale instances from a perturbed environment -- due to the limited diversity of training distributions. To tackle this problem, we propose a novel approach, which is called Adversarial Instance Augmentation and does not require to know the problem type for new instance generation, to promote data diversity for learning-based branching modules in the branch-and-bound (B&B) Solvers (AdaSolver). We use the bipartite graph representations for MILP instances and obtain various perturbed instances to regularize the solver by augmenting the graph structures with a learned augmentation policy. The major technical contribution of AdaSolver is that we formulate the non-differentiable instance augmentation as a contextual bandit problem and adversarially train the learning-based solver and augmentation policy, enabling efficient gradient-based training of the augmentation policy. To the best of our knowledge, AdaSolver is the first general and effective framework for understanding and improving the generalization of both imitation-learning-based (IL-based) and reinforcement-learning-based (RL-based) B&B solvers. Extensive experiments demonstrate that by producing various augmented instances, AdaSolver leads to a remarkable efficiency improvement across various distributions.
</details>
<details>
<summary>摘要</summary>
machine learning 已经成功应用于提高混合整数线性 программирова (MILP) 解决器的效率。然而，学习基于的解决器经常在未看过的 MILP 实例上表现出严重的性能下降 --特别是大规模实例从受到扰动环境-- 由于培育分布的有限多样性。为解决这个问题，我们提出了一种新的方法，即 Adversarial Instance Augmentation，不需要知道问题类型，为学习基于分支模块在 B&B 解决器 (AdaSolver) 中提高数据多样性。我们使用 MILP 实例的双分图表示，并通过学习的扩充策略对图结构进行加工，以增强解决器的普适性。AdaSolver 的主要技术贡献在于将不 diferenciable 的实例扩充视为上下文ual Bandit 问题，并对学习基于的扩充策略进行 adversarial 训练，使得学习基于的解决器可以高效地进行梯度下降训练。根据我们所知，AdaSolver 是首个普适并有效的框架，用于理解和提高学习基于 B&B 解决器的通用化。我们的实验证明，通过生成多种扩充实例，AdaSolver 在不同的分布下可以得到很大的效率提高。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/22/cs.LG_2023_10_22/" data-id="cloimipcp00qks488d09lhpuy" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_10_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/22/eess.SP_2023_10_22/" class="article-date">
  <time datetime="2023-10-22T08:00:00.000Z" itemprop="datePublished">2023-10-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/22/eess.SP_2023_10_22/">eess.SP - 2023-10-22</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Submodular-Optimization-for-Placement-of-Intelligent-Reflecting-Surfaces-in-Sensing-Systems"><a href="#Submodular-Optimization-for-Placement-of-Intelligent-Reflecting-Surfaces-in-Sensing-Systems" class="headerlink" title="Submodular Optimization for Placement of Intelligent Reflecting Surfaces in Sensing Systems"></a>Submodular Optimization for Placement of Intelligent Reflecting Surfaces in Sensing Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14443">http://arxiv.org/abs/2310.14443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zahra Esmaeilbeig, Kumar Vijay Mishra, Arian Eamaz, Mojtaba Soltanalian</li>
<li>for: 这篇论文是关于智能反射表面（IRS）的优化部署和探测应用的研究。</li>
<li>methods: 本论文使用了最大共识度来决定多个IRS平台的部署，以优化探测应用。</li>
<li>results: 研究表明，使用最大共识度作为优化标准可以实现约63%的最差性能保证。<details>
<summary>Abstract</summary>
Intelligent reflecting surfaces (IRS) and their optimal deployment are the new technological frontier in sensing applications. Recently, IRS have demonstrated potential in advancing target estimation and detection. While the optimal phase-shift of IRS for different tasks has been studied extensively in the literature, the optimal placement of multiple IRS platforms for sensing applications is less explored. In this paper, we design the placement of IRS platforms for sensing by maximizing the mutual information. In particular, we use this criterion to determine an approximately optimal placement of IRS platforms to illuminate an area where the target has a hypothetical presence. After demonstrating the submodularity of the mutual information criteria, we tackle the design problem by means of a constant-factor approximation algorithm for submodular optimization. Numerical results are presented to validate the proposed submodular optimization framework for optimal IRS placement with worst case performance bounded to $1-1/e\approx 63 \%$.
</details>
<details>
<summary>摘要</summary>
《智能反射表面（IRS）和其最佳部署》是当今探测应用的新技术前沿。最近，IRS已经表现出在目标估计和检测方面的潜力。虽然literature中对不同任务的IRS阶段的优化已经得到了广泛的研究，但IRS平台的多个部署 для探测应用还尚未得到了充分的研究。在这篇论文中，我们设计了IRS平台的部署方式，以最大化互信息。特别是，我们使用这个标准来确定IRS平台的投射方式，以照明假设存在目标的区域。我们首先证明了互信息标准的子模度性，然后使用子模度优化算法来解决设计问题。 num = 1-1/e ≈ 63%。Here's the breakdown of the translation:* "Intelligent reflecting surfaces" is translated as "智能反射表面" (Zhīngnéng jiànshì biānshì)* "and their optimal deployment" is translated as "以及其最佳部署" (yǐngqí yǔ jiāojiā bèipiāo)* "are the new technological frontier in sensing applications" is translated as "是当今探测应用的新技术前沿" (shì dāngjīn tàncéng yìngyùn yìshì)* "Recently, IRS have demonstrated potential in advancing target estimation and detection" is translated as "最近，IRS已经表现出在目标估计和检测方面的潜力" (zuìjìn, IRS yǐjīng bìngxìn zhìyì yìshì)* "While the optimal phase-shift of IRS for different tasks has been studied extensively in the literature" is translated as "虽然literature中对不同任务的IRS阶段的优化已经得到了广泛的研究" ( substitude "phase-shift" with "阶段" (jìděn) in Chinese)* "the optimal placement of multiple IRS platforms for sensing applications is less explored" is translated as "IRS平台的多个部署 для探测应用还尚未得到了充分的研究" (IRS píngtiān de duō gè bèipiāo yǐng yìshì)* "In this paper, we design the placement of IRS platforms for sensing by maximizing the mutual information" is translated as "在这篇论文中，我们设计了IRS平台的部署方式，以最大化互信息" (substitute "placing" with "部署" (bèipiāo) in Chinese)* "In particular, we use this criterion to determine an approximately optimal placement of IRS platforms to illuminate an area where the target has a hypothetical presence" is translated as "特别是，我们使用这个标准来确定IRS平台的投射方式，以照明假设存在目标的区域" (substitute "illuminate" with "照明" (zhàoqǐng) in Chinese)* "After demonstrating the submodularity of the mutual information criteria" is translated as "我们首先证明了互信息标准的子模度性" (substitute "submodularity" with "子模度性" (zǐmódegè) in Chinese)* "we tackle the design problem by means of a constant-factor approximation algorithm for submodular optimization" is translated as "然后使用子模度优化算法来解决设计问题" (substitute "constant-factor" with "常数因子" (chángxīng yǐngxí) in Chinese)* "Numerical results are presented to validate the proposed submodular optimization framework for optimal IRS placement with worst case performance bounded to $1-1/e\approx 63\%$" is translated as " num = 1-1/e ≈ 63%。我们提供的子模度优化框架的数值结果，以验证优化结果的可行性" (substitute "worst case" with "最差情况" (zuìjì) in Chinese)
</details></li>
</ul>
<hr>
<h2 id="Piezoelectric-Sensors-for-Real-time-Monitoring-and-Quality-Control-in-Additive-Manufacturing"><a href="#Piezoelectric-Sensors-for-Real-time-Monitoring-and-Quality-Control-in-Additive-Manufacturing" class="headerlink" title="Piezoelectric Sensors for Real-time Monitoring and Quality Control in Additive Manufacturing"></a>Piezoelectric Sensors for Real-time Monitoring and Quality Control in Additive Manufacturing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14321">http://arxiv.org/abs/2310.14321</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rashid T. Momin</li>
<li>for: 本研究旨在为工程领域的精度制造过程提供深入理解和实用技术，尤其是在添加制造中。</li>
<li>methods: 本研究采用系统性的方法，自基本原理起推导到实践应用，以满足不同读者的需求。</li>
<li>results: 研究发现， piezoelectric 感应器在实时监测和质量控制方面具有广泛的应用前景和潜在价值，对于新手和专业人士都具有启发性和实用性。<details>
<summary>Abstract</summary>
Within the ever-evolving landscape of engineering, particularly in the dynamic domain of additive In manufacturing, a pursuit of precision and excellence in production processes takes centre stage. This research , This paper serves to give a comprehensive understanding of piezoelectric sensors, a topic that is both academically engaging and of practical significance, catering to both seasoned experts and those newly venturing into the field. Additive manufacturing, lauded for its groundbreaking potential, underscores the imperative of rigorous quality control. This introduces piezoelectric sensors, devices that may be unfamiliar to many but possess considerable potential. This paper embarks on a methodical journey, commencing with an introductory elucidation of the piezoelectric effect. It then advances to the vital role of piezoelectric sensors in real-time monitoring and quality control, unveiling their potential and relevance for newcomers and seasoned professionals alike. This research, structured systematically from fundamental principles to pragmatic applications, presents findings that are not only academically informative but also represent a substantial stride towards achieving precision and high-quality manufacturing processes in the engineering field.
</details>
<details>
<summary>摘要</summary>
在工程领域中，特别是在加工制造领域，精度和excel在生产过程中拥有中心舞台。这篇研究，这篇论文旨在为涉及到 piezoelectric 传感器的学术研究和实践提供全面的理解，它不仅对有经验的专家有启发，还对新进入该领域的人有很大的启示。加工制造被赞赏为创新的潜力，强调了严格的质量控制的重要性。这使得 piezoelectric 传感器成为了一种可能新的和有潜力的技术。这篇论文从基本原理开始，逐步向实践应用，对新手和经验手都有很大的启示。这项研究不仅是学术上的探索，也是在工程领域实现精度和高质量生产过程的一大步进。
</details></li>
</ul>
<hr>
<h2 id="FAS-assisted-NOMA-Short-Packet-Communication-Systems"><a href="#FAS-assisted-NOMA-Short-Packet-Communication-Systems" class="headerlink" title="FAS-assisted NOMA Short-Packet Communication Systems"></a>FAS-assisted NOMA Short-Packet Communication Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14251">http://arxiv.org/abs/2310.14251</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zeniSoida/pl1">https://github.com/zeniSoida/pl1</a></li>
<li>paper_authors: Jianchao Zheng, Tuo Wu, Xiazhi Lai, Cunhua Pan, Maged Elkashlan, Kai-Kit Wong</li>
<li>for:  investigate a fluid antenna system (FAS)-assisted downlink non-orthogonal multiple access (NOMA) for short-packet communications.</li>
<li>methods:  base station (BS) adopts a single fixed antenna, while both the central user (CU) and the cell-edge user (CEU) are equipped with a FAS.</li>
<li>results:  the diversity order for CU and CEU is $N$, indicating that the system performance can be considerably improved by increasing $N$.Here’s the full version in Traditional Chinese:</li>
<li>for: 这个研究探讨了一个使用流体天线系统（FAS）协助下行非共域多存取（NOMA）的短包通信系统。</li>
<li>methods: BS使用固定天线，而中央用户（CU）和红色边用户（CEU）则是配备了FAS的。</li>
<li>results: FAS帮助下，CU和CEU的多标题状况下的系统性能可以随N的增加而提高。<details>
<summary>Abstract</summary>
In this paper, we investigate a fluid antenna system (FAS)-assisted downlink non-orthogonal multiple access (NOMA) for short-packet communications. The base station (BS) adopts a single fixed antenna, while both the central user (CU) and the cell-edge user (CEU) are equipped with a FAS. Each FAS comprises $N$ flexible positions (also known as ports), linked to $N$ arbitrarily correlated Rayleigh fading channels. We derive expressions for the average block error rate (BLER) of the FAS-assisted NOMA system and provide asymptotic BLER expressions. We determine that the diversity order for CU and CEU is $N$, indicating that the system performance can be considerably improved by increasing $N$. Simulation results validate the great performance of FAS.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了一个流体天线系统（FAS）助动下link非对称多接入（NOMA） для短包通信。基站（BS）采用单一固定天线，而中央用户（CU）和边缘用户（CEU）均配备了FAS。每个FAS包括 $N$ 个 flexible位（也称为端口），与 $N$ 个互相相关的干扰谱噪抖动通道相连。我们 derive了 FAS-assisted NOMA 系统的平均块错误率（BLER）表达式，并提供了各种各样的 BLER 表达式。我们发现，CU 和 CEU 的多普通性顺序是 $N$，这表明，通过增加 $N$，系统性能可以得到显著改善。实验结果证明了 FAS 的优秀性能。
</details></li>
</ul>
<hr>
<h2 id="How-do-the-resting-EEG-preprocessing-states-affect-the-outcomes-of-postprocessing"><a href="#How-do-the-resting-EEG-preprocessing-states-affect-the-outcomes-of-postprocessing" class="headerlink" title="How do the resting EEG preprocessing states affect the outcomes of postprocessing?"></a>How do the resting EEG preprocessing states affect the outcomes of postprocessing?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15194">http://arxiv.org/abs/2310.15194</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiang Hu, Jie Ruan, Juan Hou, Pedro Antonio Valdes-Sosa, Zhao Lv</li>
<li>for: 本研究旨在探讨预处理影响后处理结果的影响，特别是预处理不足和过度预处理对频域、空间和时间领域的后处理所带来的影响。</li>
<li>methods: 研究人员使用了新 York 头部模型和多变量自 regression 模型来生成清晰 EEG（CE）作为参考数据，然后通过在 CE 基础上增加 Gaussian 噪声和失去大脑活动来生成预处理不足（IPE）和过度预处理（EPE）数据。</li>
<li>results: 研究人员发现，预处理不足和过度预处理都会导致后处理结果与 CE 之间的偏差，特别是在频域、空间和时间领域的 Statistics、多通道能量、跨 Spectra、源 imaging 粒度和贝叶率等方面。此外，研究人员还发现，PaLOSi 指标与预处理状态的变化有 statistically  significative 相关性。<details>
<summary>Abstract</summary>
Plenty of artifact removal tools and pipelines have been developed to correct the EEG recordings and discover the values below the waveforms. Without visual inspection from the experts, it is susceptible to derive improper preprocessing states, like the insufficient preprocessed EEG (IPE), and the excessive preprocessed EEG (EPE). However, little is known about the impacts of IPE or EPE on the postprocessing in the frequency, spatial and temporal domains, particularly as to the spectra and the functional connectivity (FC) analysis. Here, the clean EEG (CE) was synthesized as the ground truth based on the New-York head model and the multivariate autoregressive model. Later, the IPE and the EPE were simulated by injecting the Gaussian noise and losing the brain activities, respectively. Then, the impacts on postprocessing were quantified by the deviation caused by the IPE or EPE from the CE as to the 4 temporal statistics, the multichannel power, the cross spectra, the dispersion of source imaging, and the properties of scalp EEG network. Lastly, the association analysis was performed between the PaLOSi metric and the varying trends of postprocessing with the evolution of preprocessing states. This study shed light on how the postprocessing outcomes are affected by the preprocessing states and PaLOSi may be a potential effective quality metric.
</details>
<details>
<summary>摘要</summary>
很多遗物除去工具和管道已经开发出来 corrections EEG 记录，以便发现下面波形的值。而不经visual inspection from the experts，可能导致 derivation improper preprocessing states, such as insufficient preprocessed EEG (IPE) and excessive preprocessed EEG (EPE). However, little is known about the impacts of IPE or EPE on the postprocessing in the frequency, spatial, and temporal domains, particularly as to the spectra and the functional connectivity (FC) analysis.Here, the clean EEG (CE) was synthesized as the ground truth based on the New-York head model and the multivariate autoregressive model. Later, the IPE and the EPE were simulated by injecting Gaussian noise and losing brain activities, respectively. Then, the impacts on postprocessing were quantified by the deviation caused by the IPE or EPE from the CE as to the 4 temporal statistics, the multichannel power, the cross spectra, the dispersion of source imaging, and the properties of scalp EEG network. Lastly, the association analysis was performed between the PaLOSi metric and the varying trends of postprocessing with the evolution of preprocessing states. This study shed light on how the postprocessing outcomes are affected by the preprocessing states and PaLOSi may be a potential effective quality metric.
</details></li>
</ul>
<hr>
<h2 id="On-the-Sum-Secrecy-Rate-of-Multi-User-Holographic-MIMO-Networks"><a href="#On-the-Sum-Secrecy-Rate-of-Multi-User-Holographic-MIMO-Networks" class="headerlink" title="On the Sum Secrecy Rate of Multi-User Holographic MIMO Networks"></a>On the Sum Secrecy Rate of Multi-User Holographic MIMO Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14217">http://arxiv.org/abs/2310.14217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arthur S. de Sena, Jiguang He, Ahmed Al Hammadi, Chongwen Huang, Faouzi Bader, Merouane Debbah, Mathias Fink</li>
<li>for: 这篇论文主要针对下一代通信系统的增强多样性和多重化，探讨了扩展到安全通信领域的可能性。</li>
<li>methods: 该论文使用了meta-atom技术，并对HMIMO网络的能量和频率效率、路径损失分析、通道模型进行了分析。</li>
<li>results: 研究发现，采用adaptive&#x2F;灵活的发射功率分配（PA）可以在高信号噪声比（SNR） régime中获得显著性能提升，对两个用户情况下可以获得更多于100%的增强。<details>
<summary>Abstract</summary>
The emerging concept of extremely-large holographic multiple-input multiple-output (HMIMO), beneficial from compactly and densely packed cost-efficient radiating meta-atoms, has been demonstrated for enhanced degrees of freedom even in pure line-of-sight conditions, enabling tremendous multiplexing gain for the next-generation communication systems. Most of the reported works focus on energy and spectrum efficiency, path loss analyses, and channel modeling. The extension to secure communications remains unexplored. In this paper, we theoretically characterize the secrecy capacity of the HMIMO network with multiple legitimate users and one eavesdropper while taking into consideration artificial noise and max-min fairness. We formulate the power allocation (PA) problem and address it by following successive convex approximation and Taylor expansion. We further study the effect of fixed PA coefficients, imperfect channel state information, inter-element spacing, and the number of Eve's antennas on the sum secrecy rate. Simulation results show that significant performance gain with more than 100\% increment in the high signal-to-noise ratio (SNR) regime for the two-user case is obtained by exploiting adaptive/flexible PA compared to the case with fixed PA coefficients.
</details>
<details>
<summary>摘要</summary>
新的概念——超大型杂点多输入多输出（HMIMO）技术，具有高度压缩和高密度的成本效益的卫星体，已经在纯线路条件下实现了更高的自由度，这将导致下一代通信系统的多重化增量。大多数报道的研究都集中在能量和频率效益、距离损失分析和通道模型。然而，安全通信的扩展还没有被探讨。在这篇论文中，我们理论上Characterize HMIMO网络的机密容量，并考虑人工噪声和最大最小公平。我们将 transmit power allocation（PA）问题，通过Successive Convex Approximation和Taylor扩展解决。我们进一步研究了固定PA系数、不完全的通道状态信息、元素间距离和贝娅的天线数量对Sum secrecy rate的影响。实验结果表明，在高信号噪声比（SNR）区间，通过适应/灵活PA比与固定PA系数相比，可以获得高达100%的性能提升。
</details></li>
</ul>
<hr>
<h2 id="A-Coordinate-Descent-Approach-to-Atomic-Norm-Minimization"><a href="#A-Coordinate-Descent-Approach-to-Atomic-Norm-Minimization" class="headerlink" title="A Coordinate Descent Approach to Atomic Norm Minimization"></a>A Coordinate Descent Approach to Atomic Norm Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14182">http://arxiv.org/abs/2310.14182</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruifu Li, Danijela Cabric</li>
<li>for: 这个论文的目的是解决含有稀疏信号处理的应用中的原子范数最小化问题。</li>
<li>methods: 这篇论文提出了一种基于坐标降低的低复杂度、matrix-free的原子范数最小化解决方案。该方法利用了稀疏规化的自然性，通过坐标降低框架来实现高效的解决方案。</li>
<li>results: 该方法在数学上证明可以快速地求解原子范数最小化问题，并且在大规模问题中实现了高效的计算。通过对比ADMM和自定义内点SDP解决方案，该方法在解决稀疏问题方面明显更快。<details>
<summary>Abstract</summary>
Atomic norm minimization is of great interest in various applications of sparse signal processing including super-resolution line-spectral estimation and signal denoising. In practice, atomic norm minimization (ANM) is formulated as a semi-definite programming (SDP) which is generally hard to solve. This work introduces a low-complexity, matrix-free method for solving ANM. The method uses the framework of coordinate descent and exploits the sparsity-induced nature of atomic-norm regularization. Specifically, an equivalent, non-convex formulation of ANM is first proposed. It is then proved that applying the coordinate descent framework on the non-convex formulation leads to convergence to the global optimal point. For the case of a single measurement vector of length N in discrete fourier transform (DFT) basis, the complexity of each iteration in the coordinate descent procedure is O(N log N ), rendering the proposed method efficient even for large-scale problems. The proposed coordinate descent framework can be readily modified to solve a variety of ANM problems, including multi-dimensional ANM with multiple measurement vectors. It is easy to implement and can essentially be applied to any atomic sets as long as a corresponding rank-1 problem can be solved. Through extensive numerical simulations, it is verified that for solving sparse problems the proposed method is much faster than the alternating direction method of multipliers (ADMM) or the customized interior point SDP solver.
</details>
<details>
<summary>摘要</summary>
“原子 нор 最小化是各种各样的应用，包括超解析和信号噪声除除。但是在实践中，原子 norm 最小化（ANM）通常被表示为半definite 程序（SDP），这通常很难解决。这项工作提出了一种低复杂度、缺少矩阵的方法来解决 ANM。该方法使用坐标 descent 框架，利用原子规定的稀疏性来减少计算复杂性。具体来说，首先提出了 ANM 的非конvex 表述，然后证明通过坐标 descent 框架处理非конvex 表述可以达到全局最优点。在单个测量向量长度为 N 的抽象快推（DFT）基础下，每次迭代的复杂度为 O(N log N)，这使得提出的方法在大规模问题上高效。此外，这种坐标 descent 框架可以轻松地修改，以解决多维 ANM 问题，包括多个测量向量。它易于实现，可以适用于任何原子集，只要可以解决相应的 rank-1 问题。经过广泛的数值实验，证明了用于解决稀疏问题的提出方法比 ADMM 或自定义内点 SDP 解决器更快。”
</details></li>
</ul>
<hr>
<h2 id="Spatial-Sigma-Delta-Modulation-for-Coarsely-Quantized-Massive-MIMO-Downlink-Flexible-Designs-by-Convex-Optimization"><a href="#Spatial-Sigma-Delta-Modulation-for-Coarsely-Quantized-Massive-MIMO-Downlink-Flexible-Designs-by-Convex-Optimization" class="headerlink" title="Spatial Sigma-Delta Modulation for Coarsely Quantized Massive MIMO Downlink: Flexible Designs by Convex Optimization"></a>Spatial Sigma-Delta Modulation for Coarsely Quantized Massive MIMO Downlink: Flexible Designs by Convex Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14179">http://arxiv.org/abs/2310.14179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wai-Yiu Keung, Wing-Kin Ma</li>
<li>for: 本文考虑了多用户大量MIMO下降频率预编码，使用低分辨率数字到分析转换器（DAC）。</li>
<li>methods: 本文使用了Sigma-Delta模ulation来控制量化误差效应。</li>
<li>results: 数字实现中使用Sigma-Delta模ulation可以达到近似于无量化的性能，但需要考虑angle sector和数量化级别等因素。<details>
<summary>Abstract</summary>
This paper considers the context of multiuser massive MIMO downlink precoding with low-resolution digital-to-analog converters (DACs) at the transmitter. This subject is motivated by the consideration that it is expensive to employ high-resolution DACs for practical massive MIMO implementations. The challenge with using low-resolution DACs is to overcome the detrimental quantization error effects. Recently, spatial Sigma-Delta modulation has arisen as a viable way to put quantization errors under control. This approach takes insight from temporal Sigma-Delta modulation in classical DAC studies. Assuming a 1D uniform linear transmit antenna array, the principle is to shape the quantization errors in space such that the shaped quantization errors are pushed away from the user-serving angle sector. In the previous studies, spatial Sigma-Delta modulation was performed by direct application of the basic first- and second-order modulators from the Sigma-Delta literature. In this paper, we develop a general Sigma-Delta modulator design framework for any given order, for any given number of quantization levels, and for any given angle sector. We formulate our design as a problem of maximizing the signal-to-quantization-and-noise ratios experienced by the users. The formulated problem is convex and can be efficiently solved by available solvers. Our proposed framework offers the alternative option of focused quantization error suppression in accordance with channel state information. Our framework can also be extended to 2D planar transmit antenna arrays. We perform numerical study under different operating conditions, and the numerical results suggest that, given a moderate number of quantization levels, say, 5 to 7 levels, our optimization-based Sigma-Delta modulation schemes can lead to bit error rate performance close to that of the unquantized counterpart.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Factor-Graph-Processing-for-Dual-Blind-Deconvolution-at-ISAC-Receiver"><a href="#Factor-Graph-Processing-for-Dual-Blind-Deconvolution-at-ISAC-Receiver" class="headerlink" title="Factor Graph Processing for Dual-Blind Deconvolution at ISAC Receiver"></a>Factor Graph Processing for Dual-Blind Deconvolution at ISAC Receiver</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14167">http://arxiv.org/abs/2310.14167</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roman Jacome, Edwin Vargas, Kumar Vijay Mishra, Brian M. Sadler, Henry Arguello</li>
<li>for: 本研究针对的是 интеграцион感知通信（ISAC）系统，它们可以同时访问和利用有限的电磁频谱资源。</li>
<li>methods: 本研究使用了线性状态空间模型（LSSM）来模型信号模型的动态变化。</li>
<li>results: 实验结果表明，使用了效果差分方法（EM）算法可以高效地估算无知变量，包括噪声存在的情况下。<details>
<summary>Abstract</summary>
Integrated sensing and communications (ISAC) systems have gained significant interest because of their ability to jointly and efficiently access, utilize, and manage the scarce electromagnetic spectrum. The co-existence approach toward ISAC focuses on the receiver processing of overlaid radar and communications signals coming from independent transmitters. A specific ISAC coexistence problem is dual-blind deconvolution (DBD), wherein the transmit signals and channels of both radar and communications are unknown to the receiver. Prior DBD works ignore the evolution of the signal model over time. In this work, we consider a dynamic DBD scenario using a linear state space model (LSSM) such that, apart from the transmit signals and channels of both systems, the LSSM parameters are also unknown. We employ a factor graph representation to model these unknown variables. We avoid the conventional matrix inversion approach to estimate the unknown variables by using an efficient expectation-maximization algorithm, where each iteration employs a Gaussian message passing over the factor graph structure. Numerical experiments demonstrate the accurate estimation of radar and communications channels, including in the presence of noise.
</details>
<details>
<summary>摘要</summary>
интегрированные сенсинг и коммуникации (ISAC) 系统在过去几年中得到了广泛的关注，因为它们可以同时和有效地访问、利用和管理有限的电磁 спектrum。在 ISAC 的共存方法中，接收器处理来自独立的发送器的掩码过的雷达和通信信号。特定的 ISAC 共存问题是双目掩码分解 (DBD)，其中雷达和通信系统的发送信号和通道都是接收器不知道的。先前的 DBD 工作忽略了信号模型的时间演化。在这种情况下，我们使用线性状态空间模型 (LSSM)，以便除了雷达和通信系统的发送信号和通道之外，还不知道 LSSM 参数。我们使用因果图表示这些未知变量。我们避免了传统的矩阵逆解方法，而是使用高效的期望最大化算法，每次迭代都使用 Gaussian 消息传递在因果图结构上。实验示出了准确地估计雷达和通信通道，包括在噪声存在的情况下。
</details></li>
</ul>
<hr>
<h2 id="Photoplethysmography-based-atrial-fibrillation-detection-an-updated-review-from-July-2019"><a href="#Photoplethysmography-based-atrial-fibrillation-detection-an-updated-review-from-July-2019" class="headerlink" title="Photoplethysmography based atrial fibrillation detection: an updated review from July 2019"></a>Photoplethysmography based atrial fibrillation detection: an updated review from July 2019</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14155">http://arxiv.org/abs/2310.14155</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Ding, Ran Xiao, Weijia Wang, Elizabeth Holdsworth, Xiao Hu</li>
<li>for: 本研究旨在寻找最新的抽血 photoplethysmography (PPG) 技术，用于不间断的 атrial fibrillation (AF) 监测，以提高患者的健康状况。</li>
<li>methods: 本研究采用了数字卫生和人工智能 (AI) 解决方案，检查了59 项研究，包括统计方法、传统机器学习技术和深度学习方法。</li>
<li>results: 研究发现，使用 PPG 技术可以准确地检测 AF，并且可以提高患者的健康状况。同时，研究还发现了在这个领域存在的挑战。<details>
<summary>Abstract</summary>
Atrial fibrillation (AF) is a prevalent cardiac arrhythmia associated with significant health ramifications, including an elevated susceptibility to ischemic stroke, heart disease, and heightened mortality. Photoplethysmography (PPG) has emerged as a promising technology for continuous AF monitoring for its cost-effectiveness and widespread integration into wearable devices. Our team previously conducted an exhaustive review on PPG-based AF detection before June 2019. However, since then, more advanced technologies have emerged in this field. This paper offers a comprehensive review of the latest advancements in PPG-based AF detection, utilizing digital health and artificial intelligence (AI) solutions, within the timeframe spanning from July 2019 to December 2022. Through extensive exploration of scientific databases, we have identified 59 pertinent studies. Our comprehensive review encompasses an in-depth assessment of the statistical methodologies, traditional machine learning techniques, and deep learning approaches employed in these studies. In addition, we address the challenges encountered in the domain of PPG-based AF detection. Furthermore, we maintain a dedicated website to curate the latest research in this area, with regular updates on a regular basis.
</details>
<details>
<summary>摘要</summary>
AF（atrialfibrillation）是一种常见的心脏 arrhythmia，与 significativ health consequences associated， including elevated risk of ischemic stroke, heart disease, and increased mortality. Photoplethysmography (PPG) has emerged as a promising technology for continuous AF monitoring due to its cost-effectiveness and widespread integration into wearable devices. Our team previously conducted an exhaustive review of PPG-based AF detection before June 2019. However, since then, more advanced technologies have emerged in this field. This paper provides a comprehensive review of the latest advancements in PPG-based AF detection, utilizing digital health and artificial intelligence (AI) solutions, within the timeframe spanning from July 2019 to December 2022. Through extensive exploration of scientific databases, we have identified 59 pertinent studies. Our comprehensive review encompasses an in-depth assessment of the statistical methodologies, traditional machine learning techniques, and deep learning approaches employed in these studies. In addition, we address the challenges encountered in the domain of PPG-based AF detection. Furthermore, we maintain a dedicated website to curate the latest research in this area, with regular updates on a regular basis.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/22/eess.SP_2023_10_22/" data-id="cloimipjq019ys488f247gmyu" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_10_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/21/cs.SD_2023_10_21/" class="article-date">
  <time datetime="2023-10-21T15:00:00.000Z" itemprop="datePublished">2023-10-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/21/cs.SD_2023_10_21/">cs.SD - 2023-10-21</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Temporal-convolutional-neural-networks-to-generate-a-head-related-impulse-response-from-one-direction-to-another"><a href="#Temporal-convolutional-neural-networks-to-generate-a-head-related-impulse-response-from-one-direction-to-another" class="headerlink" title="Temporal convolutional neural networks to generate a head-related impulse response from one direction to another"></a>Temporal convolutional neural networks to generate a head-related impulse response from one direction to another</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14018">http://arxiv.org/abs/2310.14018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tatsuki Kobayashi, Yoshiko Maruyama, Isao Nambu, Shohei Yano, Yasuhiro Wada</li>
<li>for: Virtual sound synthesis technology allows users to perceive spatial sound through headphones or earphones, but accurate virtual sound requires an individual head-related transfer function (HRTF).</li>
<li>methods: This study proposed a method to generate HRTFs from one direction to the other using temporal convolutional neural networks (TCNs) and publicly available datasets in the horizontal plane.</li>
<li>results: The proposed method successfully generated HRIRs for directions other than the front direction in the dataset, and was found to be equivalent to the measured HRIRs in a new dataset through behavioral experiments with human participants. These results suggest that the proposed TCNs can be used to generate personalized HRIRs for virtual sound.Here’s the summary in Traditional Chinese as well, for your reference:</li>
<li>for: 虚拟 зву频技术允许使用者透过耳机或耳筒听到三维音频，但是精准的虚拟音频需要个人化头部转换函数（HRTF）。</li>
<li>methods: 这项研究提出了将HRTF从一个方向转换到另一个方向的方法，使用了时间卷积神经网络（TCN）和公共可用数据集在水平面上进行训练。</li>
<li>results: 提议的方法成功将HRIRs从其他方向转换到前方方向，并且在新的数据集上进行了训练。Behavioral实验显示，生成的HRIRs与实验测量的HRIRs相等。这些结果表示，提议的TCNs可以从一个方向转换到另一个方向，实现个人化虚拟音频。<details>
<summary>Abstract</summary>
Virtual sound synthesis is a technology that allows users to perceive spatial sound through headphones or earphones. However, accurate virtual sound requires an individual head-related transfer function (HRTF), which can be difficult to measure due to the need for a specialized environment. In this study, we proposed a method to generate HRTFs from one direction to the other. To this end, we used temporal convolutional neural networks (TCNs) to generate head-related impulse responses (HRIRs). To train the TCNs, publicly available datasets in the horizontal plane were used. Using the trained networks, we successfully generated HRIRs for directions other than the front direction in the dataset. We found that the proposed method successfully generated HRIRs for publicly available datasets. To test the generalization of the method, we measured the HRIRs of a new dataset and tested whether the trained networks could be used for this new dataset. Although the similarity evaluated by spectral distortion was slightly degraded, behavioral experiments with human participants showed that the generated HRIRs were equivalent to the measured ones. These results suggest that the proposed TCNs can be used to generate personalized HRIRs from one direction to another, which could contribute to the personalization of virtual sound.
</details>
<details>
<summary>摘要</summary>
虚拟声音合成技术可以让用户通过headset或earphone感受到三维声音。然而，实际的虚拟声音需要个人头部相关传输函数（HRTF），这可以因特殊环境而困难测量。在这个研究中，我们提出了一种方法，可以从一个方向转换到另一个方向的HRTF。为此，我们使用了时间卷积神经网络（TCN）生成头部相关冲击响应（HRIR）。使用训练好的网络，我们成功地生成了不同方向的HRIR。我们发现，提出的方法可以成功地生成不同方向的HRIR，并且在新的数据集中测试了该方法的通用性。虽然spectral distortion评估中的相似性略差，但人类参与者的行为实验表明，生成的HRIR与测量的HRIR相当。这些结果表明，提出的TCN可以用于个人化虚拟声音。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/21/cs.SD_2023_10_21/" data-id="cloimipeu00wws488hoawc9q6" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_10_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/21/eess.AS_2023_10_21/" class="article-date">
  <time datetime="2023-10-21T14:00:00.000Z" itemprop="datePublished">2023-10-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/21/eess.AS_2023_10_21/">eess.AS - 2023-10-21</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="SwG-former-Sliding-window-Graph-Convolutional-Network-Integrated-with-Conformer-for-Sound-Event-Localization-and-Detection"><a href="#SwG-former-Sliding-window-Graph-Convolutional-Network-Integrated-with-Conformer-for-Sound-Event-Localization-and-Detection" class="headerlink" title="SwG-former: Sliding-window Graph Convolutional Network Integrated with Conformer for Sound Event Localization and Detection"></a>SwG-former: Sliding-window Graph Convolutional Network Integrated with Conformer for Sound Event Localization and Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14016">http://arxiv.org/abs/2310.14016</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiming Huang, Qinghua Huang, Liyan Ma, Zhengyu Chen, Chuan Wang</li>
<li>for: 本研究旨在提高Sound Event Localization and Detection（SELD）系统的性能，特别是在自然空间声学环境下。</li>
<li>methods: 本研究提出了一种基于图表示的novel graph convolutional network（GCN）模型，具有同时抽取空间特征和时间特征的能力。此外，一种robust Conv2dAgg函数也被提出，用于协调邻居特征。</li>
<li>results: 对比与现有的先进SELD模型，本研究的SwG-former模型在同一个声学环境下表现出了superior的性能。此外，将SwG模块整合到EINV2网络中，得到的SwG-EINV2模型也超过了现有的SOTA方法。<details>
<summary>Abstract</summary>
Sound event localization and detection (SELD) is a joint task of sound event detection (SED) and direction of arrival (DoA) estimation. SED mainly relies on temporal dependencies to distinguish different sound classes, while DoA estimation depends on spatial correlations to estimate source directions. To jointly optimize two subtasks, the SELD system should extract spatial correlations and model temporal dependencies simultaneously. However, numerous models mainly extract spatial correlations and model temporal dependencies separately. In this paper, the interdependence of spatial-temporal information in audio signals is exploited for simultaneous extraction to enhance the model performance. In response, a novel graph representation leveraging graph convolutional network (GCN) in non-Euclidean space is developed to extract spatial-temporal information concurrently. A sliding-window graph (SwG) module is designed based on the graph representation. It exploits sliding-windows with different sizes to learn temporal context information and dynamically constructs graph vertices in the frequency-channel (F-C) domain to capture spatial correlations. Furthermore, as the cornerstone of message passing, a robust Conv2dAgg function is proposed and embedded into the SwG module to aggregate the features of neighbor vertices. To improve the performance of SELD in a natural spatial acoustic environment, a general and efficient SwG-former model is proposed by integrating the SwG module with the Conformer. It exhibits superior performance in comparison to recent advanced SELD models. To further validate the generality and efficiency of the SwG-former, it is seamlessly integrated into the event-independent network version 2 (EINV2) called SwG-EINV2. The SwG-EINV2 surpasses the state-of-the-art (SOTA) methods under the same acoustic environment.
</details>
<details>
<summary>摘要</summary>
声音事件地点Localization和检测（SELD）是一个joint任务，它包括声音事件检测（SED）和方向来源估计（DoA）。SED主要基于时间关系来分辨不同的声音类型，而DoA估计则基于空间相关性来估计源向量。为了同时优化两个子任务，SELD系统应该同时提取空间相关性和模型时间关系。然而，许多模型通常是分开提取空间相关性和时间关系。在这篇论文中，我们利用声音信号中的空间-时间信息相互关系，并将其同时提取出来，以提高模型性能。为此，我们开发了一种基于图表示的新型图 convolutional neural network（GCN）模型，可以同时提取空间-时间信息。我们还设计了一个基于图表示的滑动窗口模块（SwG），它利用不同的窗口大小来学习时间上下文信息，并在频道频率（F-C）空间中动态构建图顶点，以捕捉空间相关性。此外，我们还提出了一种Robust Conv2dAgg函数，用于在图顶点之间进行消息传递。为了在自然的空间声音环境中提高SELD性能，我们提出了一种通用和高效的SwG-former模型，其由SwG模块和Conformer结合而成。该模型在与现有高级SELD模型进行比较时表现出色。此外，我们还将SwG-former模型与事件独立网络版2（EINV2）结合，得到了SwG-EINV2模型。SwG-EINV2模型在同一个声音环境中超过了现有的最佳方法。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/21/eess.AS_2023_10_21/" data-id="cloimipg2010fs4888ktr3slg" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_10_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/21/cs.CV_2023_10_21/" class="article-date">
  <time datetime="2023-10-21T13:00:00.000Z" itemprop="datePublished">2023-10-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/21/cs.CV_2023_10_21/">cs.CV - 2023-10-21</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Zero-shot-Learning-of-Individualized-Task-Contrast-Prediction-from-Resting-state-Functional-Connectomes"><a href="#Zero-shot-Learning-of-Individualized-Task-Contrast-Prediction-from-Resting-state-Functional-Connectomes" class="headerlink" title="Zero-shot Learning of Individualized Task Contrast Prediction from Resting-state Functional Connectomes"></a>Zero-shot Learning of Individualized Task Contrast Prediction from Resting-state Functional Connectomes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14105">http://arxiv.org/abs/2310.14105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minh Nguyen, Gia H. Ngo, Mert R. Sabuncu</li>
<li>for: 可以用resting-state functional MRI（rsfMRI）scan来预测任务触发活动</li>
<li>methods: 使用machine learning（ML）模型，通过对suffix pair的resting-state和任务触发fMRI scan进行训练</li>
<li>results: 可以预测 novel task的活动，并且与state-of-the-art模型的in-domain预测相当In English, this would be:</li>
<li>for: Using resting-state functional MRI (rsfMRI) scans to predict task-evoked activity</li>
<li>methods: Using machine learning (ML) models, trained on paired resting-state and task-evoked fMRI scans</li>
<li>results: Can predict activity for novel tasks, and is competitive with state-of-the-art models’ in-domain predictions<details>
<summary>Abstract</summary>
Given sufficient pairs of resting-state and task-evoked fMRI scans from subjects, it is possible to train ML models to predict subject-specific task-evoked activity using resting-state functional MRI (rsfMRI) scans. However, while rsfMRI scans are relatively easy to collect, obtaining sufficient task fMRI scans is much harder as it involves more complex experimental designs and procedures. Thus, the reliance on scarce paired data limits the application of current techniques to only tasks seen during training. We show that this reliance can be reduced by leveraging group-average contrasts, enabling zero-shot predictions for novel tasks. Our approach, named OPIC (short for Omni-Task Prediction of Individual Contrasts), takes as input a subject's rsfMRI-derived connectome and a group-average contrast, to produce a prediction of the subject-specific contrast. Similar to zero-shot learning in large language models using special inputs to obtain answers for novel natural language processing tasks, inputting group-average contrasts guides the OPIC model to generalize to novel tasks unseen in training. Experimental results show that OPIC's predictions for novel tasks are not only better than simple group-averages, but are also competitive with a state-of-the-art model's in-domain predictions that was trained using in-domain tasks' data.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:可以通过 sufficient pairs of resting-state and task-evoked fMRI scans from subjects 来训练机器学习（ML）模型，预测个体特定的任务触发活动使用 resting-state functional MRI（rsfMRI）scans。然而，而 rsfMRI scans 相对容易收集，而获取足够的任务 fMRI scans 则更加复杂，需要更复杂的实验设计和过程。因此，现有技术的应用受到了scarce paired data的限制，只能在训练中使用已经看到的任务。我们显示，可以通过利用 group-average contrasts 降低这种限制，实现 zero-shot 预测 Novel task。我们的方法，名为 OPIC（short for Omni-Task Prediction of Individual Contrasts），输入一个subject的 rsfMRI-derived connectome 和 group-average contrast，以生成一个subject-specific contrast 预测。与大型语言模型使用特定输入来获取 novel natural language processing task 的答案类似，输入 group-average contrasts 使得 OPIC 模型能够泛化到 novel task 中。实验结果表明，OPIC 的预测对 Novel task 不仅 луч于单纯的 group-average，而且与一种状态之前的模型在预测中的性能也很竞争。
</details></li>
</ul>
<hr>
<h2 id="Unleashing-Modified-Deep-Learning-Models-in-Efficient-COVID19-Detection"><a href="#Unleashing-Modified-Deep-Learning-Models-in-Efficient-COVID19-Detection" class="headerlink" title="Unleashing Modified Deep Learning Models in Efficient COVID19 Detection"></a>Unleashing Modified Deep Learning Models in Efficient COVID19 Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14081">http://arxiv.org/abs/2310.14081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Aminul Islam, Shabbir Ahmed Shuvo, Mohammad Abu Tareq Rony, M Raihan, Md Abu Sufian</li>
<li>for: 这个研究旨在提高COVID-19预测和检测精度，以帮助医疗机构、政策制定者和研究人员做出更加有知识基础的决策，从而减少COVID-19和其他传染病的影响。</li>
<li>methods: 这个研究使用了深度学习技术，特别是MobileNet V3、DenseNet201和GoogleNet Inception V1等模型，以及损失优化和可扩展批处理normalization等策略，以提高预测模型的性能和鲁棒性。</li>
<li>results: 研究发现，使用MobileNet V3、DenseNet201和GoogleNet Inception V1等模型可以实现高度的预测精度，并且可以结合损失优化和可扩展批处理normalization来进一步提高预测模型的性能和鲁棒性。<details>
<summary>Abstract</summary>
The COVID19 pandemic, a unique and devastating respiratory disease outbreak, has affected global populations as the disease spreads rapidly. Recent Deep Learning breakthroughs may improve COVID19 prediction and forecasting as a tool of precise and fast detection, however, current methods are still being examined to achieve higher accuracy and precision. This study analyzed the collection contained 8055 CT image samples, 5427 of which were COVID cases and 2628 non COVID. The 9544 Xray samples included 4044 COVID patients and 5500 non COVID cases. The most accurate models are MobileNet V3 (97.872 percent), DenseNet201 (97.567 percent), and GoogleNet Inception V1 (97.643 percent). High accuracy indicates that these models can make many accurate predictions, as well as others, are also high for MobileNetV3 and DenseNet201. An extensive evaluation using accuracy, precision, and recall allows a comprehensive comparison to improve predictive models by combining loss optimization with scalable batch normalization in this study. Our analysis shows that these tactics improve model performance and resilience for advancing COVID19 prediction and detection and shows how Deep Learning can improve disease handling. The methods we suggest would strengthen healthcare systems, policymakers, and researchers to make educated decisions to reduce COVID19 and other contagious diseases.   CCS CONCEPTS Covid,Deep Learning, Image Processing   KEYWORDS Covid, Deep Learning, DenseNet201, MobileNet, ResNet, DenseNet, GoogleNet, Image Processing, Disease Detection.
</details>
<details>
<summary>摘要</summary>
COVID-19 流行病，一种独特且肇事的呼吸疾病爆发，对全球人口产生了深触的影响。 current Deep Learning 突破可能提高 COVID-19 预测和预测，作为精确和快速检测工具。但是，当前方法仍在进行评估，以提高准确率和精度。本研究分析了包含 8055 个 CT 图像样本，其中 5427 个是 COVID  случа例，2628 个是非 COVID  случа例。9544 个 X-ray 样本中包括 4044 个 COVID 病例和 5500 个非 COVID 病例。最准确的模型是 MobileNet V3（97.872%），DenseNet201（97.567%）和 GoogleNet Inception V1（97.643%）。高准确率表明这些模型可以做出许多准确预测，同时其他模型也具有高准确率。通过精度、精度和回归的全面评估，我们可以对改进预测模型进行比较。我们的分析表明，通过捆绑损失优化与可扩展批处理normalization可以提高模型性能和抗预测能力。这些方法可以强化医疗系统、政策制定者和研究人员，以便根据 COVID-19 和其他传染病的情况进行教育的决策。关键词： COVID-19，Deep Learning， DenseNet201， MobileNet， ResNet， DenseNet， GoogleNet， Image Processing，疾病检测。
</details></li>
</ul>
<hr>
<h2 id="Concept-based-Anomaly-Detection-in-Retail-Stores-for-Automatic-Correction-using-Mobile-Robots"><a href="#Concept-based-Anomaly-Detection-in-Retail-Stores-for-Automatic-Correction-using-Mobile-Robots" class="headerlink" title="Concept-based Anomaly Detection in Retail Stores for Automatic Correction using Mobile Robots"></a>Concept-based Anomaly Detection in Retail Stores for Automatic Correction using Mobile Robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14063">http://arxiv.org/abs/2310.14063</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya Kapoor, Vartika Sengar, Nijil George, Vighnesh Vatsal, Jayavardhana Gubbi, Balamuralidhar P, Arpan Pal</li>
<li>for: 这 paper 的目的是提出一种基于视transformer（ViT）的概念异常检测方法，用于检测商店中的货物错位和缺失。</li>
<li>methods: 该方法使用 auto-encoder 架构，然后使用异常检测在准确空间进行异常检测。</li>
<li>results: 该方法在 RP2K 数据集上的峰成功率为 89.90%，比基本模型的标准 ViT auto-encoder 高出 8.10%。<details>
<summary>Abstract</summary>
Tracking of inventory and rearrangement of misplaced items are some of the most labor-intensive tasks in a retail environment. While there have been attempts at using vision-based techniques for these tasks, they mostly use planogram compliance for detection of any anomalies, a technique that has been found lacking in robustness and scalability. Moreover, existing systems rely on human intervention to perform corrective actions after detection. In this paper, we present Co-AD, a Concept-based Anomaly Detection approach using a Vision Transformer (ViT) that is able to flag misplaced objects without using a prior knowledge base such as a planogram. It uses an auto-encoder architecture followed by outlier detection in the latent space. Co-AD has a peak success rate of 89.90% on anomaly detection image sets of retail objects drawn from the RP2K dataset, compared to 80.81% on the best-performing baseline of a standard ViT auto-encoder. To demonstrate its utility, we describe a robotic mobile manipulation pipeline to autonomously correct the anomalies flagged by Co-AD. This work is ultimately aimed towards developing autonomous mobile robot solutions that reduce the need for human intervention in retail store management.
</details>
<details>
<summary>摘要</summary>
<<SYS>>销售环境中最具压力的任务之一是存储和重新排序丢失的商品。而使用视觉技术进行这些任务的尝试已经存在，但大多数使用计划图合规性进行异常检测，这种技术缺乏可靠性和扩展性。此外，现有系统往往需要人工干预进行 corrections 。在这篇论文中，我们提出了 Co-AD，一种基于概念的异常检测方法，使用视觉转换器（ViT）来识别丢失的物品，不需要使用先知库如计划图。它使用自适应网络架构，然后在幽默空间进行异常检测。Co-AD 在 RP2K 数据集上的峰值成功率为 89.90%，比最佳基eline的标准 ViT 自适应网络架构高出 8.19%。为了证明其实用性，我们描述了一种基于移动抓取机的自动化corrctions 管理管道。这项工作最终目标是开发自动化移动机器人解决方案，减少零售店管理中的人工干预。
</details></li>
</ul>
<hr>
<h2 id="Training-Image-Derivatives-Increased-Accuracy-and-Universal-Robustness"><a href="#Training-Image-Derivatives-Increased-Accuracy-and-Universal-Robustness" class="headerlink" title="Training Image Derivatives: Increased Accuracy and Universal Robustness"></a>Training Image Derivatives: Increased Accuracy and Universal Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14045">http://arxiv.org/abs/2310.14045</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vsevolod I. Avrutskiy<br>for:* The paper is written for the problem of image analysis, specifically reconstructing the vertices of a cube based on its image.methods:* The paper uses derivative training, a method that computes the derivatives of the output values in the forward pass and includes them in the cost function to improve the accuracy of the neural network.* The paper also uses a gradient-based algorithm to minimize the cost function with respect to the weights.results:* The paper obtains 25 times more accurate results for noiseless inputs by training the derivatives with respect to the 6 degrees of freedom of the cube.* The paper also provides insights into the robustness problem, including two types of network vulnerabilities and a trade-off between accuracy and robustness.Here is the text in Simplified Chinese:for:* 这篇论文是为图像分析问题写的，具体来说是根据图像重建三角形的顶点。methods:* 这篇论文使用 derive 训练，在前向传递中计算输出值的导数，并将其包含在成本函数中以提高神经网络的准确性。* 这篇论文还使用一种基于梯度的算法来最小化成本函数中的权重。results:* 这篇论文在噪声为零的输入上达到了25倍的更高精度。* 这篇论文还提供了图像分析问题中的 robustness 问题的重要情况，包括神经网络中两种类型的抵触性和两种类型的图像变化。<details>
<summary>Abstract</summary>
Derivative training is a well-known method to improve the accuracy of neural networks. In the forward pass, not only the output values are computed, but also their derivatives, and their deviations from the target derivatives are included in the cost function, which is minimized with respect to the weights by a gradient-based algorithm. So far, this method has been implemented for relatively low-dimensional tasks. In this study, we apply the approach to the problem of image analysis. We consider the task of reconstructing the vertices of a cube based on its image. By training the derivatives with respect to the 6 degrees of freedom of the cube, we obtain 25 times more accurate results for noiseless inputs. The derivatives also provide important insights into the robustness problem, which is currently understood in terms of two types of network vulnerabilities. The first type is small perturbations that dramatically change the output, and the second type is substantial image changes that the network erroneously ignores. They are currently considered as conflicting goals, since conventional training methods produce a trade-off. The first type can be analyzed via the gradient of the network, but the second type requires human evaluation of the inputs, which is an oracle substitute. For the task at hand, the nearest neighbor oracle can be defined, and the knowledge of derivatives allows it to be expanded into Taylor series. This allows to perform the first-order robustness analysis that unifies both types of vulnerabilities, and to implement robust training that eliminates any trade-offs, so that accuracy and robustness are limited only by network capacity.
</details>
<details>
<summary>摘要</summary>
偏导训练是一种通用的方法，可以提高神经网络的准确率。在前向传播中，不仅计算输出值，还计算其导数和与目标导数的差异，并将其包含在权重下降的成本函数中，通过梯度基本算法进行最小化。到目前为止，这种方法已经实现在相对低维度的任务上。在这项研究中，我们将这种方法应用到图像分析任务中，即根据图像推算出立方体的顶点坐标。通过对立方体的6个自由度进行导数训练，我们可以获得25倍的精度，对于噪声无效输入。导数还提供了对稳定性问题的重要视角，这个问题目前被理解为神经网络的两种类型敏感性。第一种是小幅度的输入修改，导致输出异常变化，第二种是大规模的图像修改，神经网络错误地忽略这些修改。这两种类型的敏感性目前被视为矛盾目标， convent ional 训练方法会产生交易。第一种可以通过神经网络的梯度进行分析，但第二种需要人工评估输入，这是一个oracle substitute。对于这个任务， nearest neighbor oracle 可以定义，并且导数的知识允许其扩展为泰勒级数。这样可以实现首领稳定性分析，并将稳定性和准确率限制在神经网络容量之间。
</details></li>
</ul>
<hr>
<h2 id="You-Only-Condense-Once-Two-Rules-for-Pruning-Condensed-Datasets"><a href="#You-Only-Condense-Once-Two-Rules-for-Pruning-Condensed-Datasets" class="headerlink" title="You Only Condense Once: Two Rules for Pruning Condensed Datasets"></a>You Only Condense Once: Two Rules for Pruning Condensed Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14019">http://arxiv.org/abs/2310.14019</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang He, Lingao Xiao, Joey Tianyi Zhou</li>
<li>For: 提高训练效率，适应设备上的限制条件。* Methods: 使用两个简单的数据采样规则：低LBPE分数和平衡构建。* Results: 在ConvNet、ResNet和DenseNet网络上，在CIFAR-10、CIFAR-100和ImageNet数据集上达到了6.98-8.89%和6.31-23.92%的准确率提升。<details>
<summary>Abstract</summary>
Dataset condensation is a crucial tool for enhancing training efficiency by reducing the size of the training dataset, particularly in on-device scenarios. However, these scenarios have two significant challenges: 1) the varying computational resources available on the devices require a dataset size different from the pre-defined condensed dataset, and 2) the limited computational resources often preclude the possibility of conducting additional condensation processes. We introduce You Only Condense Once (YOCO) to overcome these limitations. On top of one condensed dataset, YOCO produces smaller condensed datasets with two embarrassingly simple dataset pruning rules: Low LBPE Score and Balanced Construction. YOCO offers two key advantages: 1) it can flexibly resize the dataset to fit varying computational constraints, and 2) it eliminates the need for extra condensation processes, which can be computationally prohibitive. Experiments validate our findings on networks including ConvNet, ResNet and DenseNet, and datasets including CIFAR-10, CIFAR-100 and ImageNet. For example, our YOCO surpassed various dataset condensation and dataset pruning methods on CIFAR-10 with ten Images Per Class (IPC), achieving 6.98-8.89% and 6.31-23.92% accuracy gains, respectively. The code is available at: https://github.com/he-y/you-only-condense-once.
</details>
<details>
<summary>摘要</summary>
dataset 缩减是训练效率的重要工具，尤其在设备上进行训练时。但是这些情况有两个主要挑战：1）设备上的计算资源不断变化，需要不同于预先定义的缩减 dataset size，2）设备上的计算资源frequently precludes the possibility of conducting additional condensation processes。我们介绍了“仅需缩减一次”（YOCO）来解决这些问题。YOCO 可以生成更小的缩减 dataset，并且可以灵活地调整 dataset size 以适应不同的计算限制。此外，YOCO 可以消除额外的缩减 процес，这可以是计算昂费的。我们的实验结果显示，YOCO 在 ConvNet、ResNet 和 DenseNet 等网络上，以及 CIFAR-10、CIFAR-100 和 ImageNet 等 dataset 上，均有着superior的表现。例如，我们在 CIFAR-10 上，使用 YOCO 可以从原始的 10 个图像每个类别（IPC）开始，获得 6.98-8.89% 和 6.31-23.92% 的精度提升。相关代码可以在 GitHub 上找到：https://github.com/he-y/you-only-condense-once。
</details></li>
</ul>
<hr>
<h2 id="Ophthalmic-Biomarker-Detection-Using-Ensembled-Vision-Transformers-–-Winning-Solution-to-IEEE-SPS-VIP-Cup-2023"><a href="#Ophthalmic-Biomarker-Detection-Using-Ensembled-Vision-Transformers-–-Winning-Solution-to-IEEE-SPS-VIP-Cup-2023" class="headerlink" title="Ophthalmic Biomarker Detection Using Ensembled Vision Transformers – Winning Solution to IEEE SPS VIP Cup 2023"></a>Ophthalmic Biomarker Detection Using Ensembled Vision Transformers – Winning Solution to IEEE SPS VIP Cup 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14005">http://arxiv.org/abs/2310.14005</a></li>
<li>repo_url: None</li>
<li>paper_authors: H. A. Z. Sameen Shahgir, Khondker Salman Sayeed, Tanjeem Azwad Zaman, Md. Asif Haider, Sheikh Saifur Rahman Jony, M. Sohel Rahman</li>
<li>For: The paper was written for the IEEE SPS VIP Cup 2023: Ophthalmic Biomarker Detection competition, with the primary objective of identifying biomarkers from Optical Coherence Tomography (OCT) images obtained from a diverse range of patients.* Methods: The authors trained two vision transformer-based models, MaxViT and EVA-02, using robust augmentations and 5-fold cross-validation. They ensembled the two models at inference time and found that MaxViT’s use of convolution layers followed by strided attention was better suited for detecting local features, while EVA-02’s use of normal attention mechanism and knowledge distillation was better for detecting global features.* Results: The authors achieved a patient-wise F1 score of 0.814 in the first phase and 0.8527 in the second and final phase of VIP Cup 2023, scoring 3.8% higher than the next-best solution.<details>
<summary>Abstract</summary>
This report outlines our approach in the IEEE SPS VIP Cup 2023: Ophthalmic Biomarker Detection competition. Our primary objective in this competition was to identify biomarkers from Optical Coherence Tomography (OCT) images obtained from a diverse range of patients. Using robust augmentations and 5-fold cross-validation, we trained two vision transformer-based models: MaxViT and EVA-02, and ensembled them at inference time. We find MaxViT's use of convolution layers followed by strided attention to be better suited for the detection of local features while EVA-02's use of normal attention mechanism and knowledge distillation is better for detecting global features. Ours was the best-performing solution in the competition, achieving a patient-wise F1 score of 0.814 in the first phase and 0.8527 in the second and final phase of VIP Cup 2023, scoring 3.8% higher than the next-best solution.
</details>
<details>
<summary>摘要</summary>
本报告介绍了我们在IEEE SPS VIP杯2023：眼部生物标志检测比赛中采用的方法。我们的主要目标是从多样化的病人群中提取眼部生物标志。我们使用了可靠的扩展和5fold跨 VALIDATION，并在推理时 ensemble两种视transformer模型：MaxViT和EVA-02。我们发现MaxViT使用 convolution层后的步骤权重检测本地特征，而EVA-02使用 normal attention机制和知识传递是更适合检测全局特征。在比赛中，我们的解决方案得到了第一阶段和第二阶段的VI P杯2023的病人级F1分数0.814和0.8527，比下一个最佳解决方案高出3.8%。
</details></li>
</ul>
<hr>
<h2 id="Bi-discriminator-Domain-Adversarial-Neural-Networks-with-Class-Level-Gradient-Alignment"><a href="#Bi-discriminator-Domain-Adversarial-Neural-Networks-with-Class-Level-Gradient-Alignment" class="headerlink" title="Bi-discriminator Domain Adversarial Neural Networks with Class-Level Gradient Alignment"></a>Bi-discriminator Domain Adversarial Neural Networks with Class-Level Gradient Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13959">http://arxiv.org/abs/2310.13959</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuang Zhao, Hongke Zhao, Hengshu Zhu, Zhenya Huang, Nan Feng, Enhong Chen, Hui Xiong</li>
<li>for: 这个研究旨在提高隐私领域转移中的不监控性评估，并且将Source domain的丰富知识转移到Target domain中，同时保持Target domain的标签空间。</li>
<li>methods: 本研究使用的方法包括bi-discriminator领域敌方网络，以及基于梯度信号和第二阶probability估计的分组梯度对齐。</li>
<li>results: 实验结果显示， compared to existing方法，本研究的方法可以更好地将Target domain中的标签转移到Source domain中，并且可以更好地避免错分析和错分类。<details>
<summary>Abstract</summary>
Unsupervised domain adaptation aims to transfer rich knowledge from the annotated source domain to the unlabeled target domain with the same label space. One prevalent solution is the bi-discriminator domain adversarial network, which strives to identify target domain samples outside the support of the source domain distribution and enforces their classification to be consistent on both discriminators. Despite being effective, agnostic accuracy and overconfident estimation for out-of-distribution samples hinder its further performance improvement. To address the above challenges, we propose a novel bi-discriminator domain adversarial neural network with class-level gradient alignment, i.e. BACG. BACG resorts to gradient signals and second-order probability estimation for better alignment of domain distributions. Specifically, for accuracy-awareness, we first design an optimizable nearest neighbor algorithm to obtain pseudo-labels of samples in the target domain, and then enforce the backward gradient approximation of the two discriminators at the class level. Furthermore, following evidential learning theory, we transform the traditional softmax-based optimization method into a Multinomial Dirichlet hierarchical model to infer the class probability distribution as well as samples uncertainty, thereby alleviating misestimation of out-of-distribution samples and guaranteeing high-quality classes alignment. In addition, inspired by contrastive learning, we develop a memory bank-based variant, i.e. Fast-BACG, which can greatly shorten the training process at the cost of a minor decrease in accuracy. Extensive experiments and detailed theoretical analysis on four benchmark data sets validate the effectiveness and robustness of our algorithm.
</details>
<details>
<summary>摘要</summary>
Unsupervised domain adaptation aims to transfer rich knowledge from the annotated source domain to the unlabeled target domain with the same label space. One prevalent solution is the bi-discriminator domain adversarial network, which strives to identify target domain samples outside the support of the source domain distribution and enforces their classification to be consistent on both discriminators. Despite being effective, agnostic accuracy and overconfident estimation for out-of-distribution samples hinder its further performance improvement. To address the above challenges, we propose a novel bi-discriminator domain adversarial neural network with class-level gradient alignment, i.e. BACG. BACG resorts to gradient signals and second-order probability estimation for better alignment of domain distributions. Specifically, for accuracy-awareness, we first design an optimizable nearest neighbor algorithm to obtain pseudo-labels of samples in the target domain, and then enforce the backward gradient approximation of the two discriminators at the class level. Furthermore, following evidential learning theory, we transform the traditional softmax-based optimization method into a Multinomial Dirichlet hierarchical model to infer the class probability distribution as well as samples uncertainty, thereby alleviating misestimation of out-of-distribution samples and guaranteeing high-quality classes alignment. In addition, inspired by contrastive learning, we develop a memory bank-based variant, i.e. Fast-BACG, which can greatly shorten the training process at the cost of a minor decrease in accuracy. Extensive experiments and detailed theoretical analysis on four benchmark data sets validate the effectiveness and robustness of our algorithm.Here's the translation in Traditional Chinese:Unsupervised domain adaptation aims to transfer rich knowledge from the annotated source domain to the unlabeled target domain with the same label space. One prevalent solution is the bi-discriminator domain adversarial network, which strives to identify target domain samples outside the support of the source domain distribution and enforces their classification to be consistent on both discriminators. Despite being effective, agnostic accuracy and overconfident estimation for out-of-distribution samples hinder its further performance improvement. To address the above challenges, we propose a novel bi-discriminator domain adversarial neural network with class-level gradient alignment, i.e. BACG. BACG resorts to gradient signals and second-order probability estimation for better alignment of domain distributions. Specifically, for accuracy-awareness, we first design an optimizable nearest neighbor algorithm to obtain pseudo-labels of samples in the target domain, and then enforce the backward gradient approximation of the two discriminators at the class level. Furthermore, following evidential learning theory, we transform the traditional softmax-based optimization method into a Multinomial Dirichlet hierarchical model to infer the class probability distribution as well as samples uncertainty, thereby alleviating misestimation of out-of-distribution samples and guaranteeing high-quality classes alignment. In addition, inspired by contrastive learning, we develop a memory bank-based variant, i.e. Fast-BACG, which can greatly shorten the training process at the cost of a minor decrease in accuracy. Extensive experiments and detailed theoretical analysis on four benchmark data sets validate the effectiveness and robustness of our algorithm.
</details></li>
</ul>
<hr>
<h2 id="Competitive-Ensembling-Teacher-Student-Framework-for-Semi-Supervised-Left-Atrium-MRI-Segmentation"><a href="#Competitive-Ensembling-Teacher-Student-Framework-for-Semi-Supervised-Left-Atrium-MRI-Segmentation" class="headerlink" title="Competitive Ensembling Teacher-Student Framework for Semi-Supervised Left Atrium MRI Segmentation"></a>Competitive Ensembling Teacher-Student Framework for Semi-Supervised Left Atrium MRI Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13955">http://arxiv.org/abs/2310.13955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuyan Shi, Yichi Zhang, Shasha Wang<br>for: 这篇论文主要关注于 semi-supervised learning 技术的应用在医疗影像分类中，尤其是 Left Atrium (LA) 区域的分类。methods: 本文提出了一个简单 yet efficient 的 teacher-student 架构，其中两个学生模型受到不同的任务水平干扰，并在教师模型的导引下进行互相学习。此外，文章还提出了一种竞争性的整合策略，以将更可靠的信息融合到教师模型中。results: 本文在公共的 LA 数据集上进行了评估，并获得了优秀的性能成绩，具体来说，该方法可以充分利用无标注数据，并较上先进的几种 semi-supervised 方法表现出色。<details>
<summary>Abstract</summary>
Semi-supervised learning has greatly advanced medical image segmentation since it effectively alleviates the need of acquiring abundant annotations from experts and utilizes unlabeled data which is much easier to acquire. Among existing perturbed consistency learning methods, mean-teacher model serves as a standard baseline for semi-supervised medical image segmentation. In this paper, we present a simple yet efficient competitive ensembling teacher student framework for semi-supervised for left atrium segmentation from 3D MR images, in which two student models with different task-level disturbances are introduced to learn mutually, while a competitive ensembling strategy is performed to ensemble more reliable information to teacher model. Different from the one-way transfer between teacher and student models, our framework facilitates the collaborative learning procedure of different student models with the guidance of teacher model and motivates different training networks for a competitive learning and ensembling procedure to achieve better performance. We evaluate our proposed method on the public Left Atrium (LA) dataset and it obtains impressive performance gains by exploiting the unlabeled data effectively and outperforms several existing semi-supervised methods.
</details>
<details>
<summary>摘要</summary>
semi-supervised learning 已经大幅提高医疗图像分割的技术 Waterloo ，因为它有效地减轻了专家们 annotate 大量数据的需求，并利用了 easier to obtain 的无标注数据。在现有的妥协一致学习方法中，mean-teacher 模型 serves as a standard baseline for semi-supervised medical image segmentation。在这篇论文中，我们提出了一种简单 yet efficient 的 competitive ensembling teacher student 框架，用于 semi-supervised 左心室 segmentation from 3D MR 图像，其中有两个学生模型 with different task-level disturbances 是用来学习 mutually，而一种 competitive ensembling 策略是用于 ensemble 更可靠的信息到 teacher model。与一个 teacher 和学生模型之间的一个way transfer 不同，我们的框架实现了不同的学生模型之间的协同学习过程，帮助 by teacher model 的指导和动力，以实现更好的性能。我们对 public Left Atrium (LA) 数据集进行了评估，并获得了很好的性能提升，通过有效地利用无标注数据和多个现有的半指导学习方法。
</details></li>
</ul>
<hr>
<h2 id="Fuzzy-NMS-Improving-3D-Object-Detection-with-Fuzzy-Classification-in-NMS"><a href="#Fuzzy-NMS-Improving-3D-Object-Detection-with-Fuzzy-Classification-in-NMS" class="headerlink" title="Fuzzy-NMS: Improving 3D Object Detection with Fuzzy Classification in NMS"></a>Fuzzy-NMS: Improving 3D Object Detection with Fuzzy Classification in NMS</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13951">http://arxiv.org/abs/2310.13951</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Wang, Xinyu Zhang, Fachuan Zhao, Chuze Wu, Yichen Wang, Ziying Song, Lei Yang, Jun Li, Huaping Liu</li>
<li>for: 提高3D物体检测精度，解决NMS过程中的不确定性</li>
<li>methods: 引入混合学习方法，提出一种通用化精度补做模块</li>
<li>results: 对多种最新的NMS基于检测器进行改进，特别是对小物体（如人车）的准确率有显著提高，无需重新训练和显著增加推理时间<details>
<summary>Abstract</summary>
Non-maximum suppression (NMS) is an essential post-processing module used in many 3D object detection frameworks to remove overlapping candidate bounding boxes. However, an overreliance on classification scores and difficulties in determining appropriate thresholds can affect the resulting accuracy directly. To address these issues, we introduce fuzzy learning into NMS and propose a novel generalized Fuzzy-NMS module to achieve finer candidate bounding box filtering. The proposed Fuzzy-NMS module combines the volume and clustering density of candidate bounding boxes, refining them with a fuzzy classification method and optimizing the appropriate suppression thresholds to reduce uncertainty in the NMS process. Adequate validation experiments are conducted using the mainstream KITTI and large-scale Waymo 3D object detection benchmarks. The results of these tests demonstrate the proposed Fuzzy-NMS module can improve the accuracy of numerous recently NMS-based detectors significantly, including PointPillars, PV-RCNN, and IA-SSD, etc. This effect is particularly evident for small objects such as pedestrians and bicycles. As a plug-and-play module, Fuzzy-NMS does not need to be retrained and produces no obvious increases in inference time.
</details>
<details>
<summary>摘要</summary>
我们的提案的总体瑞逸-NMS 模块通过将卷积体和凝聚密度的候选 bounding box 组合起来，并使用瑞逸分类方法来细化它们，以便优化适当的阈值，从而减少 NMS 过程中的uncertainty。我们对主流的 KITTI 和大规模的 Waymo 3D object detection 标准准进行了适当的验证实验。实验结果表明，我们的提案的总体瑞逸-NMS 模块可以在许多最近的 NMS 基于检测器中提高准确性，包括 PointPillars、PV-RCNN 和 IA-SSD 等。这种效果尤其明显于小物体，如人行道和自行车。总之，我们的总体瑞逸-NMS 模块是一个可插件的模块，不需要重新训练，并且不会导致明显的执行时间增加。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Image-Generation-by-Spatial-Transformation-in-Perceptual-Colorspaces"><a href="#Adversarial-Image-Generation-by-Spatial-Transformation-in-Perceptual-Colorspaces" class="headerlink" title="Adversarial Image Generation by Spatial Transformation in Perceptual Colorspaces"></a>Adversarial Image Generation by Spatial Transformation in Perceptual Colorspaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13950">http://arxiv.org/abs/2310.13950</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ayberkydn/stadv-torch">https://github.com/ayberkydn/stadv-torch</a></li>
<li>paper_authors: Ayberk Aydin, Alptekin Temizel</li>
<li>for: 这个论文旨在提出一种基于色彩空间的攻击方法，以便在深度神经网络上进行targeted white-box攻击。</li>
<li>methods: 该方法使用了空间变换来生成攻击样本，其中Pixel值在Chrominance channels上独立变换，而不是直接对像值进行添加性负杂化或者直接操作。</li>
<li>results: 实验结果表明，该方法在targeted white-box攻击 Setting下可以获得竞争力的欺骗率，并且在benign和攻击样本之间的approxiamte perceptual distance上表现出优异的result。I hope that helps! Let me know if you have any further questions or if there’s anything else I can assist you with.<details>
<summary>Abstract</summary>
Deep neural networks are known to be vulnerable to adversarial perturbations. The amount of these perturbations are generally quantified using $L_p$ metrics, such as $L_0$, $L_2$ and $L_\infty$. However, even when the measured perturbations are small, they tend to be noticeable by human observers since $L_p$ distance metrics are not representative of human perception. On the other hand, humans are less sensitive to changes in colorspace. In addition, pixel shifts in a constrained neighborhood are hard to notice. Motivated by these observations, we propose a method that creates adversarial examples by applying spatial transformations, which creates adversarial examples by changing the pixel locations independently to chrominance channels of perceptual colorspaces such as $YC_{b}C_{r}$ and $CIELAB$, instead of making an additive perturbation or manipulating pixel values directly. In a targeted white-box attack setting, the proposed method is able to obtain competitive fooling rates with very high confidence. The experimental evaluations show that the proposed method has favorable results in terms of approximate perceptual distance between benign and adversarially generated images. The source code is publicly available at https://github.com/ayberkydn/stadv-torch
</details>
<details>
<summary>摘要</summary>
Motivated by these observations, we propose a method that creates adversarial examples by applying spatial transformations, which creates adversarial examples by changing the pixel locations independently in chrominance channels of perceptual colorspaces such as $YC_{b}C_{r}$ and $CIELAB$, instead of making an additive perturbation or manipulating pixel values directly. In a targeted white-box attack setting, the proposed method is able to obtain competitive fooling rates with very high confidence.The experimental evaluations show that the proposed method has favorable results in terms of approximate perceptual distance between benign and adversarially generated images. The source code is publicly available at <https://github.com/ayberkydn/stadv-torch>.
</details></li>
</ul>
<hr>
<h2 id="Learning-Motion-Refinement-for-Unsupervised-Face-Animation"><a href="#Learning-Motion-Refinement-for-Unsupervised-Face-Animation" class="headerlink" title="Learning Motion Refinement for Unsupervised Face Animation"></a>Learning Motion Refinement for Unsupervised Face Animation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13912">http://arxiv.org/abs/2310.13912</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jialetao/mrfa">https://github.com/jialetao/mrfa</a></li>
<li>paper_authors: Jiale Tao, Shuhang Gu, Wen Li, Lixin Duan</li>
<li>for: 生成一个基于出处图像的人脸视频，模拟驱动视频中的人脸动作。</li>
<li>methods: 采用了一种新的无监督人脸动画方法，同时学习粗细动作。在本方法中，我们利用了本地Affine运动模型来学习全局粗细动作，并在本地区域使用一种新的动作细化模块来补做粗细动作。这个模块是通过 dense correlation between source and driving images 来学习的。</li>
<li>results: 对 widely used benchmarks 进行了广泛的实验，并取得了state-of-the-art的结果。<details>
<summary>Abstract</summary>
Unsupervised face animation aims to generate a human face video based on the appearance of a source image, mimicking the motion from a driving video. Existing methods typically adopted a prior-based motion model (e.g., the local affine motion model or the local thin-plate-spline motion model). While it is able to capture the coarse facial motion, artifacts can often be observed around the tiny motion in local areas (e.g., lips and eyes), due to the limited ability of these methods to model the finer facial motions. In this work, we design a new unsupervised face animation approach to learn simultaneously the coarse and finer motions. In particular, while exploiting the local affine motion model to learn the global coarse facial motion, we design a novel motion refinement module to compensate for the local affine motion model for modeling finer face motions in local areas. The motion refinement is learned from the dense correlation between the source and driving images. Specifically, we first construct a structure correlation volume based on the keypoint features of the source and driving images. Then, we train a model to generate the tiny facial motions iteratively from low to high resolution. The learned motion refinements are combined with the coarse motion to generate the new image. Extensive experiments on widely used benchmarks demonstrate that our method achieves the best results among state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
<<SYS>>这是一个对文本进行简化中文译文的示例：</SYS>无监督面部动画目标是将来源图片中的人脸动画化，基于驱动影片中的动作，并将动作调节为人脸的细微动作。现有方法通常使用本地欧几何动作模型（例如本地欧几何动作模型或本地薄板拓扑动作模型）。这些方法可以捕捉到人脸的粗略动作，但是它们对本地区域（例如嘴巴和眼睛）的动作有限，往往会出现遗憾。在这个工作中，我们设计了一个新的无监督面部动画方法，同时学习粗略和细微的动作。具体来说，我们利用本地欧几何动作模型学习全局粗略的人脸动作，并设计了一个新的动作精度调整模块，以补偿本地欧几何动作模型对本地区域的动作精度模型。这个动作精度调整是根据驱动影片和源影片之间的密集相联性学习的。具体来说，我们首先建立了基于关键特征的源影片和驱动影片之间的结构相联性量。然后，我们将这个结构相联性量训练成一个可以从低分辨率到高分辨率的模型，以产生细微的动作调整。学习的动作调整与粗略动作结合，创建出新的图片。实际实验显示，我们的方法在广泛使用的标准benchmark上得到了最好的结果。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Driving-Behavior-for-Autonomous-Vehicles-Based-on-Gramian-Angular-Field-Vision-Transformer"><a href="#Exploring-Driving-Behavior-for-Autonomous-Vehicles-Based-on-Gramian-Angular-Field-Vision-Transformer" class="headerlink" title="Exploring Driving Behavior for Autonomous Vehicles Based on Gramian Angular Field Vision Transformer"></a>Exploring Driving Behavior for Autonomous Vehicles Based on Gramian Angular Field Vision Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13906">http://arxiv.org/abs/2310.13906</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junwei You, Ying Chen, Zhuoyu Jiang, Zhangchi Liu, Zilin Huang, Yifeng Ding, Bin Ran</li>
<li>for: 本研究旨在提出一种用于分类自动驾驶车辆（AV）驾驶行为的有效分类方法，以便诊断AV运行问题、改进自动驾驶算法和减少事故率。</li>
<li>methods: 该研究提出了一种名为Gramian Angular Field Vision Transformer（GAF-ViT）模型，用于分析AV驾驶行为。GAF-ViT模型包括三个关键组件：GAFTransformer模块、通道注意力模块和多通道ViT模块。这些模块将表示序列中的多个变量行为转换为多个图像，然后使用图像识别技术进行行为分类。</li>
<li>results: 对于Waymo开放数据集的轨迹数据进行实验表示，提出的GAF-ViT模型实现了当前领先的性能。此外，对于各个模块的效果进行了减少性研究，以证明模型的可行性。<details>
<summary>Abstract</summary>
Effective classification of autonomous vehicle (AV) driving behavior emerges as a critical area for diagnosing AV operation faults, enhancing autonomous driving algorithms, and reducing accident rates. This paper presents the Gramian Angular Field Vision Transformer (GAF-ViT) model, designed to analyze AV driving behavior. The proposed GAF-ViT model consists of three key components: GAF Transformer Module, Channel Attention Module, and Multi-Channel ViT Module. These modules collectively convert representative sequences of multivariate behavior into multi-channel images and employ image recognition techniques for behavior classification. A channel attention mechanism is applied to multi-channel images to discern the impact of various driving behavior features. Experimental evaluation on the Waymo Open Dataset of trajectories demonstrates that the proposed model achieves state-of-the-art performance. Furthermore, an ablation study effectively substantiates the efficacy of individual modules within the model.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传递给定文本到简化中文。<</SYS>>自动驾驶车辆（AV）驾驶行为分类成为诊断AV操作错误、改进自动驾驶算法和减少事故率的关键领域。本文介绍了Gramian Angular Field Vision Transformer（GAF-ViT）模型，用于分析AV驾驶行为。提议的GAF-ViT模型包括三个关键组件：GAF TransformerModule、Channel AttentionModule和Multi-Channel ViTModule。这些模块结合收集的多个变量行为的表示序列，并使用图像识别技术进行行为分类。通过频道注意机制对多个频道图像进行区分。实验表明，提议的模型在 Waymo 开放数据集上的轨迹 traverse 得到了状态的最佳性。此外，一个ablation研究有效地证明了模型中各个模块的效果。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Transformer-Using-Cross-Channel-attention-for-Object-Detection-in-Remote-Sensing-Images"><a href="#Multimodal-Transformer-Using-Cross-Channel-attention-for-Object-Detection-in-Remote-Sensing-Images" class="headerlink" title="Multimodal Transformer Using Cross-Channel attention for Object Detection in Remote Sensing Images"></a>Multimodal Transformer Using Cross-Channel attention for Object Detection in Remote Sensing Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13876">http://arxiv.org/abs/2310.13876</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bissmella Bahaduri, Zuheng Ming, Fangchen Feng, Anissa Mokraou</li>
<li>for: 这篇研究旨在提高遥测图像中的物体探测精度，并且解决遥测图像中物体探测的特定挑战，例如资料标注的缺乏和高分辨率图像中的小物体。</li>
<li>methods: 本研究提出了一个多模式转换器，通过跨通道注意力模组来探索多源遥测数据的联合。相比于传统的通道合并方法，该模组能够学习不同通道之间的关系，实现多模式输入的协调。此外，研究人员还提出了基于Swin transformer的新架构，具有固定维度的卷积层，以获得轻量级的精确性和Computational efficiency。</li>
<li>results: 实验结果显示了该多模式转换器和架构的效果，证明了它们在多模式遥测图像中的应用性。<details>
<summary>Abstract</summary>
Object detection in Remote Sensing Images (RSI) is a critical task for numerous applications in Earth Observation (EO). Unlike general object detection, object detection in RSI has specific challenges: 1) the scarcity of labeled data in RSI compared to general object detection datasets, and 2) the small objects presented in a high-resolution image with a vast background. To address these challenges, we propose a multimodal transformer exploring multi-source remote sensing data for object detection. Instead of directly combining the multimodal input through a channel-wise concatenation, which ignores the heterogeneity of different modalities, we propose a cross-channel attention module. This module learns the relationship between different channels, enabling the construction of a coherent multimodal input by aligning the different modalities at the early stage. We also introduce a new architecture based on the Swin transformer that incorporates convolution layers in non-shifting blocks while maintaining fixed dimensions, allowing for the generation of fine-to-coarse representations with a favorable accuracy-computation trade-off. The extensive experiments prove the effectiveness of the proposed multimodal fusion module and architecture, demonstrating their applicability to multimodal aerial imagery.
</details>
<details>
<summary>摘要</summary>
remote sensing 图像中的对象检测是许多应用程序地球观测（EO）中的关键任务。与通用对象检测不同，对象检测在 remote sensing 图像中具有特定挑战：1） remote sensing 图像中标注数据的稀缺性，2） 图像中的小对象在高分辨率背景中呈现。 To address these challenges, we propose a multimodal transformer exploring multi-source remote sensing data for object detection. Instead of directly combining the multimodal input through a channel-wise concatenation, which ignores the heterogeneity of different modalities, we propose a cross-channel attention module. This module learns the relationship between different channels, enabling the construction of a coherent multimodal input by aligning the different modalities at the early stage. We also introduce a new architecture based on the Swin transformer that incorporates convolution layers in non-shifting blocks while maintaining fixed dimensions, allowing for the generation of fine-to-coarse representations with a favorable accuracy-computation trade-off. The extensive experiments prove the effectiveness of the proposed multimodal fusion module and architecture, demonstrating their applicability to multimodal aerial imagery.Here's a word-for-word translation of the text into Simplified Chinese:远程感知图像中的对象检测是EO中许多应用程序的关键任务。与通用对象检测不同，对象检测在远程感知图像中具有特定挑战：1）远程感知图像中标注数据的稀缺性，2）图像中的小对象在高分辨率背景中呈现。 To address these challenges, we propose a multimodal transformer exploring multi-source remote sensing data for object detection. Instead of directly combining the multimodal input through a channel-wise concatenation, which ignores the heterogeneity of different modalities, we propose a cross-channel attention module. This module learns the relationship between different channels, enabling the construction of a coherent multimodal input by aligning the different modalities at the early stage. We also introduce a new architecture based on the Swin transformer that incorporates convolution layers in non-shifting blocks while maintaining fixed dimensions, allowing for the generation of fine-to-coarse representations with a favorable accuracy-computation trade-off. The extensive experiments prove the effectiveness of the proposed multimodal fusion module and architecture, demonstrating their applicability to multimodal aerial imagery.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/21/cs.CV_2023_10_21/" data-id="cloimip9q00jis488goj7g7nb" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_10_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/21/cs.AI_2023_10_21/" class="article-date">
  <time datetime="2023-10-21T12:00:00.000Z" itemprop="datePublished">2023-10-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/21/cs.AI_2023_10_21/">cs.AI - 2023-10-21</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Ask-To-The-Point-Open-Domain-Entity-Centric-Question-Generation"><a href="#Ask-To-The-Point-Open-Domain-Entity-Centric-Question-Generation" class="headerlink" title="Ask To The Point: Open-Domain Entity-Centric Question Generation"></a>Ask To The Point: Open-Domain Entity-Centric Question Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14126">http://arxiv.org/abs/2310.14126</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liuyuxiang512/ecqg">https://github.com/liuyuxiang512/ecqg</a></li>
<li>paper_authors: Yuxiang Liu, Jie Huang, Kevin Chen-Chuan Chang</li>
<li>for: 实现话题具体学习、助教读物和 факт检查等应用场景，提出了一个新任务：实体中心问题生成（ECQG）。</li>
<li>methods: 提出了一种具有一致性的 PLM 基础架构 GenCONE，包括两个新模块：内容强调模块和问题验证模块。</li>
<li>results: 经过广泛的实验，GenCONE 能够具有显著和稳定的性能优势，并且两个模块具有辅之usage和补做作用。<details>
<summary>Abstract</summary>
We introduce a new task called *entity-centric question generation* (ECQG), motivated by real-world applications such as topic-specific learning, assisted reading, and fact-checking. The task aims to generate questions from an entity perspective. To solve ECQG, we propose a coherent PLM-based framework GenCONE with two novel modules: content focusing and question verification. The content focusing module first identifies a focus as "what to ask" to form draft questions, and the question verification module refines the questions afterwards by verifying the answerability. We also construct a large-scale open-domain dataset from SQuAD to support this task. Our extensive experiments demonstrate that GenCONE significantly and consistently outperforms various baselines, and two modules are effective and complementary in generating high-quality questions.
</details>
<details>
<summary>摘要</summary>
我们介绍了一个新任务called *实体中心问题生成* (ECQG), 该任务受到实际应用场景的启发，如专题学习、辅助阅读和事实核实。该任务的目标是从实体角度生成问题。为解决ECQG，我们提议了一个协调PLM-based框架GenCONE，该框架包括两个新模块：内容专注和问题验证。内容专注模块首先确定了“要问什么”的焦点，以生成签证问题，而问题验证模块则在验证答案可否回答。我们还构建了一个大规模的开放领域数据集，来支持这个任务。我们的广泛实验表明，GenCONE在various baselines的比较中具有显著且一致的优势，两个模块也是生成高质量问题的有效和补充性的。
</details></li>
</ul>
<hr>
<h2 id="Sentiment-Analysis-Across-Multiple-African-Languages-A-Current-Benchmark"><a href="#Sentiment-Analysis-Across-Multiple-African-Languages-A-Current-Benchmark" class="headerlink" title="Sentiment Analysis Across Multiple African Languages: A Current Benchmark"></a>Sentiment Analysis Across Multiple African Languages: A Current Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14120">http://arxiv.org/abs/2310.14120</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saurav K. Aryal, Howard Prioleau, Surakshya Aryal</li>
<li>for: 这项研究的目的是为了提高非洲语言 sentiment 分析的研究，并评估当前的 transformer 模型在非洲语言上的性能。</li>
<li>methods: 研究使用了 AfriSenti-SemEval Shared Task 12 上的注释 sentiment analysis 数据，并对当前状态的 transformer 模型进行了比较和评估。</li>
<li>results: 研究发现，即使在低资源情况下，更多的数据仍然可以生成更好的模型，并且模型专门为非洲语言开发的模型在所有任务上都表现出色。此外，没有一个 universal 模型能够适用于所有语言的评估。<details>
<summary>Abstract</summary>
Sentiment analysis is a fundamental and valuable task in NLP. However, due to limitations in data and technological availability, research into sentiment analysis of African languages has been fragmented and lacking. With the recent release of the AfriSenti-SemEval Shared Task 12, hosted as a part of The 17th International Workshop on Semantic Evaluation, an annotated sentiment analysis of 14 African languages was made available. We benchmarked and compared current state-of-art transformer models across 12 languages and compared the performance of training one-model-per-language versus single-model-all-languages. We also evaluated the performance of standard multilingual models and their ability to learn and transfer cross-lingual representation from non-African to African languages. Our results show that despite work in low resource modeling, more data still produces better models on a per-language basis. Models explicitly developed for African languages outperform other models on all tasks. Additionally, no one-model-fits-all solution exists for a per-language evaluation of the models evaluated. Moreover, for some languages with a smaller sample size, a larger multilingual model may perform better than a dedicated per-language model for sentiment classification.
</details>
<details>
<summary>摘要</summary>
《叙述分析是NLP领域的基础和重要任务。然而，由于数据和技术限制，关于非洲语言的叙述分析研究受到了限制，Fragmented和缺乏。随着最近发布的AfriSenti-SemEval Shared Task 12，14种非洲语言的叙述分析标注数据被提供。我们对当前状态的转换器模型进行了比较和比较，并 evaluate了单语言模型 versus 所有语言模型的训练。我们还评估了标准多语言模型的能力以及其在非洲语言上学习和传递cross-语言表示的能力。我们的结果显示，尽管在低资源模型方面做了很多工作，但更多的数据仍然可以生成更好的模型。专门为非洲语言开发的模型在所有任务上都高于其他模型。此外，没有一个“一模型 fits all”的解决方案，每种语言的评估中的模型都不同。此外，对于一些语言的小样本大小，大型多语言模型可能会在叙述分类任务上表现更好于专门为该语言开发的模型。
</details></li>
</ul>
<hr>
<h2 id="CLIP-meets-Model-Zoo-Experts-Pseudo-Supervision-for-Visual-Enhancement"><a href="#CLIP-meets-Model-Zoo-Experts-Pseudo-Supervision-for-Visual-Enhancement" class="headerlink" title="CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement"></a>CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14108">http://arxiv.org/abs/2310.14108</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammadreza Salehi, Mehrdad Farajtabar, Maxwell Horton, Fartash Faghri, Hadi Pouransari, Raviteja Vemulapalli, Oncel Tuzel, Ali Farhadi, Mohammad Rastegari, Sachin Mehta</li>
<li>for: 这个论文的目的是提高CLIP模型的视觉表示能力。</li>
<li>methods: 这个论文使用了开源的任务特定视觉模型生成 pseudo-labels，并在这些 pseudo-labels 基础上训练 CLIP 模型。</li>
<li>results: 这个方法可以提高 CLIP 模型在不同视觉任务中的表现，包括 segmentation、检测、深度估计和表面法线估计，最多提高16.3%。这些提高不会减少 CLIP 模型的现有能力，包括其在适应性零分类中的护卫。<details>
<summary>Abstract</summary>
Contrastive language image pretraining (CLIP) is a standard method for training vision-language models. While CLIP is scalable, promptable, and robust to distribution shifts on image classification tasks, it lacks object localization capabilities. This paper studies the following question: Can we augment CLIP training with task-specific vision models from model zoos to improve its visual representations? Towards this end, we leverage open-source task-specific vision models to generate pseudo-labels for an uncurated and noisy image-text dataset. Subsequently, we train CLIP models on these pseudo-labels in addition to the contrastive training on image and text pairs. This simple setup shows substantial improvements of up to 16.3% across different vision tasks, including segmentation, detection, depth estimation, and surface normal estimation. Importantly, these enhancements are achieved without compromising CLIP's existing capabilities, including its proficiency in promptable zero-shot classification.
</details>
<details>
<summary>摘要</summary>
对比语言图像预训练（CLIP）是一种标准的视觉语言模型训练方法。CLIP可以扩展，可以提示，并具有对分布变化的鲁棒性，但它缺乏对象定位功能。这篇论文研究以下问题：可以通过将任务特定的视觉模型添加到CLIP训练中来改善其视觉表示？为此，我们利用开源的任务特定视觉模型生成pseudo-标签，并在这些pseudo-标签的基础上训练CLIP模型。这个简单的设置可以提高CLIP模型在不同视觉任务上的表现，最高提高达16.3%。这些改进不仅不会减少CLIP的现有能力，包括它的能力在零shot提示下的报表能力。
</details></li>
</ul>
<hr>
<h2 id="On-the-Transferability-of-Visually-Grounded-PCFGs"><a href="#On-the-Transferability-of-Visually-Grounded-PCFGs" class="headerlink" title="On the Transferability of Visually Grounded PCFGs"></a>On the Transferability of Visually Grounded PCFGs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14107">http://arxiv.org/abs/2310.14107</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhaoyanpeng/cpcfg">https://github.com/zhaoyanpeng/cpcfg</a></li>
<li>paper_authors: Yanpeng Zhao, Ivan Titov</li>
<li>for: 本研究旨在评估视觉基础 grammar 生成器在不同文本领域中的可迁移性。</li>
<li>methods: 我们extend了VC-PCFG模型，使其能够在不同文本领域中进行迁移学习。我们采用了零shot转移学习 Setting，即在源领域训练模型，然后直接应用到目标领域。</li>
<li>results: 我们的实验结果表明，视觉基础对文本的改进效果在相似领域中传递，但在远程领域中失效。我们还进行了数据和结果分析，发现 lexicon  overlap  между源领域和目标领域是迁移性的最重要因素。<details>
<summary>Abstract</summary>
There has been a significant surge of interest in visually grounded grammar induction in recent times. While a variety of models have been developed for the task and have demonstrated impressive performance, they have not been evaluated on text domains that are different from the training domain, so it is unclear if the improvements brought by visual groundings are transferable. Our study aims to fill this gap and assess the degree of transferability. We start by extending VC-PCFG (short for Visually-grounded Compound PCFG~\citep{zhao-titov-2020-visually}) in such a way that it can transfer across text domains. We consider a zero-shot transfer learning setting where a model is trained on the source domain and is directly applied to target domains, without any further training. Our experimental results suggest that: the benefits from using visual groundings transfer to text in a domain similar to the training domain but fail to transfer to remote domains. Further, we conduct data and result analysis; we find that the lexicon overlap between the source domain and the target domain is the most important factor in the transferability of VC-PCFG.
</details>
<details>
<summary>摘要</summary>
Recently, there has been a significant increase in interest in visually grounded grammar induction. While various models have been developed for this task and have shown impressive performance, their transferability to different text domains has not been evaluated. Our study aims to fill this gap and assess the degree of transferability. We start by extending VC-PCFG (short for Visually-grounded Compound PCFG) to enable transfer across text domains. We use a zero-shot transfer learning setting where a model is trained on the source domain and is directly applied to target domains without further training. Our experimental results show that: the benefits of using visual groundings transfer to text in a domain similar to the training domain but fail to transfer to remote domains. Additionally, we conduct data and result analysis and find that the lexicon overlap between the source domain and the target domain is the most important factor in the transferability of VC-PCFG.
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Instruction-Fine-tuned-Model-Evaluation-to-Guide-Industrial-Applications"><a href="#Revisiting-Instruction-Fine-tuned-Model-Evaluation-to-Guide-Industrial-Applications" class="headerlink" title="Revisiting Instruction Fine-tuned Model Evaluation to Guide Industrial Applications"></a>Revisiting Instruction Fine-tuned Model Evaluation to Guide Industrial Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14103">http://arxiv.org/abs/2310.14103</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/manuelfay/ifteval">https://github.com/manuelfay/ifteval</a></li>
<li>paper_authors: Manuel Faysse, Gautier Viaud, Céline Hudelot, Pierre Colombo</li>
<li>for: 这 paper 是 investigating task-specialization strategies for IFT model deployment in practical industrial settings.</li>
<li>methods: 该 paper 使用 LLM-based metrics to evaluate the performance of IFT models.</li>
<li>results: 该 paper 提供了实际 industrial setting 中 IFT 模型的部署中的贸易offs, offering practitioners actionable insights for real-world IFT model deployment.<details>
<summary>Abstract</summary>
Instruction Fine-Tuning (IFT) is a powerful paradigm that strengthens the zero-shot capabilities of Large Language Models (LLMs), but in doing so induces new evaluation metric requirements. We show LLM-based metrics to be well adapted to these requirements, and leverage them to conduct an investigation of task-specialization strategies, quantifying the trade-offs that emerge in practical industrial settings. Our findings offer practitioners actionable insights for real-world IFT model deployment.
</details>
<details>
<summary>摘要</summary>
instruction 细调 (IFT) 是一种强大的思想方法，可以增强大型语言模型 (LLM) 的零shot 能力，但是在这之前需要新的评估指标。我们表明 LLM 基于的指标适应这些要求，并利用它们来研究实际工业场景中的任务特化策略，量化在实际应用中出现的交易offs。我们的发现可以为实际 IFT 模型部署提供实践性的指导意见。Here's a word-for-word translation of the text into Simplified Chinese: instruction 细调（IFT）是一种强大的思想方法，可以增强大型语言模型（LLM）的零shot能力，但是在这之前需要新的评估指标。我们表明 LLM基于的指标适应这些要求，并利用它们来研究实际工业场景中的任务特化策略，量化在实际应用中出现的交易offs。我们的发现可以为实际 IFT 模型部署提供实践性的指导意见。
</details></li>
</ul>
<hr>
<h2 id="Stabilizing-reinforcement-learning-control-A-modular-framework-for-optimizing-over-all-stable-behavior"><a href="#Stabilizing-reinforcement-learning-control-A-modular-framework-for-optimizing-over-all-stable-behavior" class="headerlink" title="Stabilizing reinforcement learning control: A modular framework for optimizing over all stable behavior"></a>Stabilizing reinforcement learning control: A modular framework for optimizing over all stable behavior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14098">http://arxiv.org/abs/2310.14098</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nathan P. Lawrence, Philip D. Loewen, Shuyuan Wang, Michael G. Forbes, R. Bhushan Gopaluni</li>
<li>for: 本文提出了一个权重控制器设计框架，该框架结合深度学习的优化和模型自由，同时保证稳定性。</li>
<li>methods: 本文使用了Youla-Kucera参数化定义搜索领域，并使用行为系统构建了数据驱动内部模型。在噪声存在的情况下，对数据驱动模型的稳定性进行了分析。</li>
<li>results: 本文通过matrix factorization方法给出了所有稳定线性运算符的集合，并使用神经网络表示参数化的稳定运算符集合，实现了与标准深度学习库的无缝集成。最后，本文还展示了如何应用这些想法来调整固定结构控制器。<details>
<summary>Abstract</summary>
We propose a framework for the design of feedback controllers that combines the optimization-driven and model-free advantages of deep reinforcement learning with the stability guarantees provided by using the Youla-Kucera parameterization to define the search domain. Recent advances in behavioral systems allow us to construct a data-driven internal model; this enables an alternative realization of the Youla-Kucera parameterization based entirely on input-output exploration data. Perhaps of independent interest, we formulate and analyze the stability of such data-driven models in the presence of noise. The Youla-Kucera approach requires a stable "parameter" for controller design. For the training of reinforcement learning agents, the set of all stable linear operators is given explicitly through a matrix factorization approach. Moreover, a nonlinear extension is given using a neural network to express a parameterized set of stable operators, which enables seamless integration with standard deep learning libraries. Finally, we show how these ideas can also be applied to tune fixed-structure controllers.
</details>
<details>
<summary>摘要</summary>
我们提出一个框架，将深度学习的优化优势和模型自由的优势结合起来，同时保证稳定性通过用Youla-Kucera参数化来定义搜索空间。现有的行为系统技术使我们可以构建数据驱动的内部模型，这使得我们可以基于输入输出探索数据来实现Youla-Kucera参数化的alternative实现。此外，我们还研究了这些数据驱动模型在噪声存在时的稳定性。Youla-Kucera方法需要一个稳定的参数来设计控制器。在训练深度学习代理人时，所有稳定的线性运算的集合可以通过矩阵分解方法得到Explicitly。此外，我们还给出了一种使用神经网络表示参数化集合的稳定运算的非线性扩展，这使得我们可以轻松地与标准深度学习库集成。最后，我们示出了如何应用这些想法来调整固定结构的控制器。
</details></li>
</ul>
<hr>
<h2 id="Learning-Reward-for-Physical-Skills-using-Large-Language-Model"><a href="#Learning-Reward-for-Physical-Skills-using-Large-Language-Model" class="headerlink" title="Learning Reward for Physical Skills using Large Language Model"></a>Learning Reward for Physical Skills using Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14092">http://arxiv.org/abs/2310.14092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuwei Zeng, Yiqing Xu</li>
<li>for: 学习物理技能的奖励函数是一个挑战，因为这些任务的谱度非常广泛，状态和动作空间的维度很高，以及复杂的感知反馈。获取专家示范数据是成本高昂和时间费时的。</li>
<li>methods: 我们使用大型自然语言模型（LLM）提取任务相关知识，以提出有效的奖励函数。我们的方法包括两个组成部分：首先，使用 LLM 提出特征和参数化的奖励函数。然后，我们通过迭代自适应过程来更新这些提出的奖励函数的参数，以最小化与 LLM 的排名不一致性。</li>
<li>results: 我们在三个模拟的物理技能学习任务上进行了测试，证明了我们的设计选择的有效性。<details>
<summary>Abstract</summary>
Learning reward functions for physical skills are challenging due to the vast spectrum of skills, the high-dimensionality of state and action space, and nuanced sensory feedback. The complexity of these tasks makes acquiring expert demonstration data both costly and time-consuming. Large Language Models (LLMs) contain valuable task-related knowledge that can aid in learning these reward functions. However, the direct application of LLMs for proposing reward functions has its limitations such as numerical instability and inability to incorporate the environment feedback. We aim to extract task knowledge from LLMs using environment feedback to create efficient reward functions for physical skills. Our approach consists of two components. We first use the LLM to propose features and parameterization of the reward function. Next, we update the parameters of this proposed reward function through an iterative self-alignment process. In particular, this process minimizes the ranking inconsistency between the LLM and our learned reward functions based on the new observations. We validated our method by testing it on three simulated physical skill learning tasks, demonstrating effective support for our design choices.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Use LLM to propose features and parameterization of the reward function.2. Update the proposed reward function parameters through an iterative self-alignment process that minimizes the ranking inconsistency between the LLM and our learned reward functions based on new observations.We validated our method by testing it on three simulated physical skill learning tasks, demonstrating effective support for our design choices.</details></li>
</ol>
<hr>
<h2 id="To-Copy-or-not-to-Copy-That-is-a-Critical-Issue-of-the-Output-Softmax-Layer-in-Neural-Sequential-Recommenders"><a href="#To-Copy-or-not-to-Copy-That-is-a-Critical-Issue-of-the-Output-Softmax-Layer-in-Neural-Sequential-Recommenders" class="headerlink" title="To Copy, or not to Copy; That is a Critical Issue of the Output Softmax Layer in Neural Sequential Recommenders"></a>To Copy, or not to Copy; That is a Critical Issue of the Output Softmax Layer in Neural Sequential Recommenders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14079">http://arxiv.org/abs/2310.14079</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iesl/softmax_cpr_recommend">https://github.com/iesl/softmax_cpr_recommend</a></li>
<li>paper_authors: Haw-Shiuan Chang, Nikhil Agarwal, Andrew McCallum</li>
<li>for: 强化Sequential Recommendation任务中复现项目的处理能力</li>
<li>methods: 采用recently-proposed softmax alternatives如softmax-CPR，对输出softmax层进行修改，解决单个隐藏状态嵌入和静态项嵌入的问题</li>
<li>results: 在12个数据集上提供了一致性的改进，对GRU4Rec模型进行修改后，在5个数据集中具有重复项的NDCG@10提高10%（4%-17%），在7个数据集中无重复项的NDCG@10提高24%（8%-39%）。<details>
<summary>Abstract</summary>
Recent studies suggest that the existing neural models have difficulty handling repeated items in sequential recommendation tasks. However, our understanding of this difficulty is still limited. In this study, we substantially advance this field by identifying a major source of the problem: the single hidden state embedding and static item embeddings in the output softmax layer. Specifically, the similarity structure of the global item embeddings in the softmax layer sometimes forces the single hidden state embedding to be close to new items when copying is a better choice, while sometimes forcing the hidden state to be close to the items from the input inappropriately. To alleviate the problem, we adapt the recently-proposed softmax alternatives such as softmax-CPR to sequential recommendation tasks and demonstrate that the new softmax architectures unleash the capability of the neural encoder on learning when to copy and when to exclude the items from the input sequence. By only making some simple modifications on the output softmax layer for SASRec and GRU4Rec, softmax-CPR achieves consistent improvement in 12 datasets. With almost the same model size, our best method not only improves the average NDCG@10 of GRU4Rec in 5 datasets with duplicated items by 10% (4%-17% individually) but also improves 7 datasets without duplicated items by 24% (8%-39%)!
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Convolutional-Bidirectional-Variational-Autoencoder-for-Image-Domain-Translation-of-Dotted-Arabic-Expiration"><a href="#Convolutional-Bidirectional-Variational-Autoencoder-for-Image-Domain-Translation-of-Dotted-Arabic-Expiration" class="headerlink" title="Convolutional Bidirectional Variational Autoencoder for Image Domain Translation of Dotted Arabic Expiration"></a>Convolutional Bidirectional Variational Autoencoder for Image Domain Translation of Dotted Arabic Expiration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14069">http://arxiv.org/abs/2310.14069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Zidane, Ghada Soliman</li>
<li>for: 这个论文提出了一种基于升降栈底部卷积 convolutional bidirectional variational autoencoder（LCBVAE）架构的Encoder和Decoder，用于将斜体阿拉伯数字日期翻译成填充了阿拉伯数字日期。</li>
<li>methods: 我们采用了自定义和适应CRNN模型，并将其训练在2019年至2027年的填充图像上，以提取日期和评估LCBVAE模型在日期识别方面的性能。</li>
<li>results: 我们发现，在LCBVAE架构中添加缓冲层可以提高总体化的性能，并在下游传播学习任务中实现了图像翻译的97%准确率。这种方法可以普适应任何下游学习任务，如图像翻译和重建。<details>
<summary>Abstract</summary>
THIS paper proposes an approach of Ladder Bottom-up Convolutional Bidirectional Variational Autoencoder (LCBVAE) architecture for the encoder and decoder, which is trained on the image translation of the dotted Arabic expiration dates by reconstructing the Arabic dotted expiration dates into filled-in expiration dates. We employed a customized and adapted version of Convolutional Recurrent Neural Network CRNN model to meet our specific requirements and enhance its performance in our context, and then trained the custom CRNN model with the filled-in images from the year of 2019 to 2027 to extract the expiration dates and assess the model performance of LCBVAE on the expiration date recognition. The pipeline of (LCBVAE+CRNN) can be then integrated into an automated sorting systems for extracting the expiry dates and sorting the products accordingly during the manufacture stage. Additionally, it can overcome the manual entry of expiration dates that can be time-consuming and inefficient at the merchants. Due to the lack of the availability of the dotted Arabic expiration date images, we created an Arabic dot-matrix True Type Font (TTF) for the generation of the synthetic images. We trained the model with unrealistic synthetic dates of 59902 images and performed the testing on a realistic synthetic date of 3287 images from the year of 2019 to 2027, represented as yyyy/mm/dd. In our study, we demonstrated the significance of latent bottleneck layer with improving the generalization when the size is increased up to 1024 in downstream transfer learning tasks as for image translation. The proposed approach achieved an accuracy of 97% on the image translation with using the LCBVAE architecture that can be generalized for any downstream learning tasks as for image translation and reconstruction.
</details>
<details>
<summary>摘要</summary>
本文提出了一种垂直卷积减采样变换自动编码器（LCBVAE）架构，用于编码器和解码器，用于图像翻译 arabic 黑点日期。我们采用了自定义和适应版本的卷积循环神经网络（CRNN）模型，以满足我们的特定需求，并在我们的上下文中进行了性能改进。然后，我们在2019年到2027年的 filled-in 图像上训练了自定义 CRNN 模型，以提取日期和评估 LCBVAE 模型在日期识别方面的性能。该管道可以在生产阶段 integrating 到自动化分类系统中，以提取日期并根据日期进行产品的分类。此外，它可以超越手动输入日期，因为这可能是时间consuming 和不可靠的。由于lack  arabic 黑点日期图像的可用性，我们创建了一个 arabic 黑点矩阵 True Type Font（TTF），用于生成 synthetic 图像。我们在59902 个不实际的日期图像上训练了模型，并在2019年到2027年的 realistic  synthetic 日期上进行测试，表示为 yyyy/mm/dd。在我们的研究中，我们发现了隐藏瓶颈层可以提高通用性，并且当隐藏瓶颈层的大小增加到 1024 时，在下游转移学习任务中可以提高通用性。我们的方法达到了 97% 的准确率在图像翻译任务中，并且可以普适应任何下游学习任务。
</details></li>
</ul>
<hr>
<h2 id="MOELoRA-An-MOE-based-Parameter-Efficient-Fine-Tuning-Method-for-Multi-task-Medical-Applications"><a href="#MOELoRA-An-MOE-based-Parameter-Efficient-Fine-Tuning-Method-for-Multi-task-Medical-Applications" class="headerlink" title="MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications"></a>MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18339">http://arxiv.org/abs/2310.18339</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liuqidong07/moelora-peft">https://github.com/liuqidong07/moelora-peft</a></li>
<li>paper_authors: Qidong Liu, Xian Wu, Xiangyu Zhao, Yuanshao Zhu, Derong Xu, Feng Tian, Yefeng Zheng</li>
<li>for: 这个研究旨在为大语言模型（LLMs）在医疗系统中进行微调，以应对实际医疗任务中的多种任务。</li>
<li>methods: 我们提出了一个叫做MOELoRA的参数效率微调框架，利用MOE的多任务学习和LoRA的参数效率微调。我们将专家分为两个低矩阵对，以保持小数目的参数。此外，我们提出了一个任务驱动的闸函数，可以调节各专家的贡献和生成不同任务的特有参数。</li>
<li>results: 我们在公开的多任务中文医疗数据集上进行了广泛的实验，结果显示MOELoRA可以超越现有的参数效率微调方法。<details>
<summary>Abstract</summary>
The recent surge in the field of Large Language Models (LLMs) has gained significant attention in numerous domains. In order to tailor an LLM to a specific domain such as a web-based healthcare system, fine-tuning with domain knowledge is necessary. However, two issues arise during fine-tuning LLMs for medical applications. The first is the problem of task variety, where there are numerous distinct tasks in real-world medical scenarios. This diversity often results in suboptimal fine-tuning due to data imbalance and seesawing problems. Additionally, the high cost of fine-tuning can be prohibitive, impeding the application of LLMs. The large number of parameters in LLMs results in enormous time and computational consumption during fine-tuning, which is difficult to justify. To address these two issues simultaneously, we propose a novel parameter-efficient fine-tuning framework for multi-task medical applications called MOELoRA. The framework aims to capitalize on the benefits of both MOE for multi-task learning and LoRA for parameter-efficient fine-tuning. To unify MOE and LoRA, we devise multiple experts as the trainable parameters, where each expert consists of a pair of low-rank matrices to maintain a small number of trainable parameters. Additionally, we propose a task-motivated gate function for all MOELoRA layers that can regulate the contributions of each expert and generate distinct parameters for various tasks. To validate the effectiveness and practicality of the proposed method, we conducted comprehensive experiments on a public multi-task Chinese medical dataset. The experimental results demonstrate that MOELoRA outperforms existing parameter-efficient fine-tuning methods. The implementation is available online for convenient reproduction of our experiments.
</details>
<details>
<summary>摘要</summary>
Recently, there has been a surge of interest in Large Language Models (LLMs) in various domains. To adapt an LLM to a specific domain like a web-based healthcare system, fine-tuning with domain knowledge is essential. However, there are two challenges during fine-tuning LLMs for medical applications. First, there are many diverse tasks in real-world medical scenarios, leading to suboptimal fine-tuning due to data imbalance and seesawing problems. Second, the high cost of fine-tuning can be prohibitive, making it difficult to apply LLMs. The large number of parameters in LLMs results in significant time and computational consumption during fine-tuning, which is difficult to justify. To address these two issues simultaneously, we propose a novel parameter-efficient fine-tuning framework for multi-task medical applications called MOELoRA. The framework combines the benefits of MOE for multi-task learning and LoRA for parameter-efficient fine-tuning.In our framework, we use multiple experts as the trainable parameters, where each expert consists of a pair of low-rank matrices to maintain a small number of trainable parameters. Additionally, we propose a task-motivated gate function for all MOELoRA layers that can regulate the contributions of each expert and generate distinct parameters for various tasks. To validate the effectiveness and practicality of the proposed method, we conducted comprehensive experiments on a public multi-task Chinese medical dataset. The experimental results show that MOELoRA outperforms existing parameter-efficient fine-tuning methods. The implementation is available online for convenient reproduction of our experiments.
</details></li>
</ul>
<hr>
<h2 id="On-the-Neural-Tangent-Kernel-of-Equilibrium-Models"><a href="#On-the-Neural-Tangent-Kernel-of-Equilibrium-Models" class="headerlink" title="On the Neural Tangent Kernel of Equilibrium Models"></a>On the Neural Tangent Kernel of Equilibrium Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14062">http://arxiv.org/abs/2310.14062</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhili Feng, J. Zico Kolter</li>
<li>for: 这个研究探讨了深度平衡（DEQ）模型的神经 Tangent kernel（NTK）。</li>
<li>methods: 研究使用了 root-finding 方法来有效地找到 DEQ 模型的 deterministic NTK。</li>
<li>results: 研究发现，尽管 Fully-connected 神经网络的 NTK 在宽度和深度都在无限大时可能是随机的，但 DEQ 模型在某些条件下仍然具有 deterministic NTK，并且可以通过 root-finding 方法有效地找到它。<details>
<summary>Abstract</summary>
This work studies the neural tangent kernel (NTK) of the deep equilibrium (DEQ) model, a practical ``infinite-depth'' architecture which directly computes the infinite-depth limit of a weight-tied network via root-finding. Even though the NTK of a fully-connected neural network can be stochastic if its width and depth both tend to infinity simultaneously, we show that contrarily a DEQ model still enjoys a deterministic NTK despite its width and depth going to infinity at the same time under mild conditions. Moreover, this deterministic NTK can be found efficiently via root-finding.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Composer-Style-specific-Symbolic-Music-Generation-Using-Vector-Quantized-Discrete-Diffusion-Models"><a href="#Composer-Style-specific-Symbolic-Music-Generation-Using-Vector-Quantized-Discrete-Diffusion-Models" class="headerlink" title="Composer Style-specific Symbolic Music Generation Using Vector Quantized Discrete Diffusion Models"></a>Composer Style-specific Symbolic Music Generation Using Vector Quantized Discrete Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14044">http://arxiv.org/abs/2310.14044</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jincheng Zhang, Jingjing Tang, Charalampos Saitis, György Fazekas</li>
<li>for: 这篇论文旨在应用vector quantized variational autoencoder（VQ-VAE）和数组diffusion模型，实现符合作曲者风格的象数音乐生成。</li>
<li>methods: 本文使用VQ-VAE将象数音乐转换为一系列的index，然后使用数组diffusion模型来模拟VQ-VAE的数组几何空间。</li>
<li>results: 实验结果显示，使用本文提出的方法可以实现符合作曲者风格的象数音乐生成，精度为72.36%。<details>
<summary>Abstract</summary>
Emerging Denoising Diffusion Probabilistic Models (DDPM) have become increasingly utilised because of promising results they have achieved in diverse generative tasks with continuous data, such as image and sound synthesis. Nonetheless, the success of diffusion models has not been fully extended to discrete symbolic music. We propose to combine a vector quantized variational autoencoder (VQ-VAE) and discrete diffusion models for the generation of symbolic music with desired composer styles. The trained VQ-VAE can represent symbolic music as a sequence of indexes that correspond to specific entries in a learned codebook. Subsequently, a discrete diffusion model is used to model the VQ-VAE's discrete latent space. The diffusion model is trained to generate intermediate music sequences consisting of codebook indexes, which are then decoded to symbolic music using the VQ-VAE's decoder. The results demonstrate our model can generate symbolic music with target composer styles that meet the given conditions with a high accuracy of 72.36%.
</details>
<details>
<summary>摘要</summary>
emerging Denoising Diffusion Probabilistic Models (DDPM) 已经越来越受到使用，因为它在不同的生成任务中的连续数据上表现出色，如图像和音频生成。然而， diffusion 模型尚未被完全扩展到字符串符号音乐。我们提议将量化变换自动编码器（VQ-VAE）和字符串扩散模型结合使用，以生成符号音乐 WITH 愿景作曲风格。训练过的 VQ-VAE 可以将符号音乐表示为一个序列的索引，这些索引与一个学习的码库中的特定条目相对应。然后，一个字符串扩散模型将 VQ-VAE 的字符串潜在空间模型化。扩散模型会生成符号音乐序列中的代码库索引，这些索引然后通过 VQ-VAE 的解码器转换为符号音乐。结果表明，我们的模型可以生成符号音乐 WITH 目标作曲风格，并且准确率为 72.36%。
</details></li>
</ul>
<hr>
<h2 id="Fast-Diffusion-GAN-Model-for-Symbolic-Music-Generation-Controlled-by-Emotions"><a href="#Fast-Diffusion-GAN-Model-for-Symbolic-Music-Generation-Controlled-by-Emotions" class="headerlink" title="Fast Diffusion GAN Model for Symbolic Music Generation Controlled by Emotions"></a>Fast Diffusion GAN Model for Symbolic Music Generation Controlled by Emotions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14040">http://arxiv.org/abs/2310.14040</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jincheng Zhang, György Fazekas, Charalampos Saitis</li>
<li>for: 本研究旨在用扩散模型和生成对抗网络（GAN）控制生成的符号音乐，以实现 targets 的情感控制。</li>
<li>methods: 我们首先使用已经训练过的变量自动编码器获取符号音乐数据集的嵌入，然后使用这些嵌入来训练扩散模型。</li>
<li>results: 我们的模型成功控制了生成的符号音乐，并且在计算成本方面具有显著改善，只需要四个时间步骤来减噪，而现有的扩散模型对符号音乐生成的计算成本是千万次。<details>
<summary>Abstract</summary>
Diffusion models have shown promising results for a wide range of generative tasks with continuous data, such as image and audio synthesis. However, little progress has been made on using diffusion models to generate discrete symbolic music because this new class of generative models are not well suited for discrete data while its iterative sampling process is computationally expensive. In this work, we propose a diffusion model combined with a Generative Adversarial Network, aiming to (i) alleviate one of the remaining challenges in algorithmic music generation which is the control of generation towards a target emotion, and (ii) mitigate the slow sampling drawback of diffusion models applied to symbolic music generation. We first used a trained Variational Autoencoder to obtain embeddings of a symbolic music dataset with emotion labels and then used those to train a diffusion model. Our results demonstrate the successful control of our diffusion model to generate symbolic music with a desired emotion. Our model achieves several orders of magnitude improvement in computational cost, requiring merely four time steps to denoise while the steps required by current state-of-the-art diffusion models for symbolic music generation is in the order of thousands.
</details>
<details>
<summary>摘要</summary>
Diffusion models have shown promising results for a wide range of generative tasks with continuous data, such as image and audio synthesis. However, little progress has been made on using diffusion models to generate discrete symbolic music because this new class of generative models are not well suited for discrete data while its iterative sampling process is computationally expensive. In this work, we propose a diffusion model combined with a Generative Adversarial Network, aiming to (i) alleviate one of the remaining challenges in algorithmic music generation, which is the control of generation towards a target emotion, and (ii) mitigate the slow sampling drawback of diffusion models applied to symbolic music generation. We first used a trained Variational Autoencoder to obtain embeddings of a symbolic music dataset with emotion labels and then used those to train a diffusion model. Our results demonstrate the successful control of our diffusion model to generate symbolic music with a desired emotion. Our model achieves several orders of magnitude improvement in computational cost, requiring merely four time steps to denoise while the steps required by current state-of-the-art diffusion models for symbolic music generation is in the order of thousands.Here's the translation in Traditional Chinese:Diffusion models have shown promising results for a wide range of generative tasks with continuous data, such as image and audio synthesis. However, little progress has been made on using diffusion models to generate discrete symbolic music because this new class of generative models are not well suited for discrete data while its iterative sampling process is computationally expensive. In this work, we propose a diffusion model combined with a Generative Adversarial Network, aiming to (i) alleviate one of the remaining challenges in algorithmic music generation, which is the control of generation towards a target emotion, and (ii) mitigate the slow sampling drawback of diffusion models applied to symbolic music generation. We first used a trained Variational Autoencoder to obtain embeddings of a symbolic music dataset with emotion labels and then used those to train a diffusion model. Our results demonstrate the successful control of our diffusion model to generate symbolic music with a desired emotion. Our model achieves several orders of magnitude improvement in computational cost, requiring merely four time steps to denoise while the steps required by current state-of-the-art diffusion models for symbolic music generation is in the order of thousands.
</details></li>
</ul>
<hr>
<h2 id="Small-Language-Models-Fine-tuned-to-Coordinate-Larger-Language-Models-improve-Complex-Reasoning"><a href="#Small-Language-Models-Fine-tuned-to-Coordinate-Larger-Language-Models-improve-Complex-Reasoning" class="headerlink" title="Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning"></a>Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18338">http://arxiv.org/abs/2310.18338</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lcs2-iiitd/daslam">https://github.com/lcs2-iiitd/daslam</a></li>
<li>paper_authors: Gurusha Juneja, Subhabrata Dutta, Soumen Chakrabarti, Sunny Manchanda, Tanmoy Chakraborty</li>
<li>for: 提高大语言模型（LLM）的链式思维能力，解决复杂多步骤的理性问题。</li>
<li>methods: 使用 decomposition generator 将复杂问题 decomposes 成需 fewer reasoning steps 的子问题，然后使用 solver 解决子问题。</li>
<li>results: 使用 DaSLaM 方法，可以使用 comparable 或更好的性能，与 orders-of-magnitude 更大的 GPT-4 相比。此外， DaSLaM 方法不受 solver 的scale 影响，可以使用 diverse 大小的 solver LM 获得显著的性能提升。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) prompted to generate chain-of-thought (CoT) exhibit impressive reasoning capabilities. Recent attempts at prompt decomposition toward solving complex, multi-step reasoning problems depend on the ability of the LLM to simultaneously decompose and solve the problem. A significant disadvantage is that foundational LLMs are typically not available for fine-tuning, making adaptation computationally prohibitive. We believe (and demonstrate) that problem decomposition and solution generation are distinct capabilites, better addressed in separate modules, than by one monolithic LLM. We introduce DaSLaM, which uses a decomposition generator to decompose complex problems into subproblems that require fewer reasoning steps. These subproblems are answered by a solver. We use a relatively small (13B parameters) LM as the decomposition generator, which we train using policy gradient optimization to interact with a solver LM (regarded as black-box) and guide it through subproblems, thereby rendering our method solver-agnostic. Evaluation on multiple different reasoning datasets reveal that with our method, a 175 billion parameter LM (text-davinci-003) can produce competitive or even better performance, compared to its orders-of-magnitude larger successor, GPT-4. Additionally, we show that DaSLaM is not limited by the solver's capabilities as a function of scale; e.g., solver LMs with diverse sizes give significant performance improvement with our solver-agnostic decomposition technique. Exhaustive ablation studies evince the superiority of our modular finetuning technique over exorbitantly large decomposer LLMs, based on prompting alone.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）因为被调动产生链式思维（CoT）而表现出卓越的推理能力。现在的尝试通过问题分解来解决复杂多步骤的问题，很大程度上取决于LLM的能力同时进行问题分解和解决。然而，基础LLM通常不可以进行微调，从而使得适应成本高昂。我们认为（并证明）问题分解和解决是不同的能力，更好地通过分类模组来处理，而不是单一的LLM。我们称之为DaSLaM，它使用问题分解生成器将复杂问题分解成需要 fewer 推理步骤的子问题。这些子问题由解决器回答。我们使用一个相对较小的（13B个参数）LM作为问题分解生成器，并使用政策倾斜优化训练它与解决器LM（被视为黑盒）互动，以导引它通过子问题，因此让我们的方法成为解决器无关的。我们在多个不同的推理数据集上进行评估，发现我们的方法可以让1750亿个参数的LM（text-davinci-003）生成竞争或更好的性能，相比之下它的规模增加了许多。此外，我们还证明了DaSLaM不受解决器的规模影响，例如，不同大小的解决器LM可以为我们的模块化微调技术带来很大的性能提升。我们进行了详细的剥夺研究，证明我们的模块化微调技术在Prompting alone下比极大的问题分解LLM更有优势。
</details></li>
</ul>
<hr>
<h2 id="Contrast-Everything-A-Hierarchical-Contrastive-Framework-for-Medical-Time-Series"><a href="#Contrast-Everything-A-Hierarchical-Contrastive-Framework-for-Medical-Time-Series" class="headerlink" title="Contrast Everything: A Hierarchical Contrastive Framework for Medical Time-Series"></a>Contrast Everything: A Hierarchical Contrastive Framework for Medical Time-Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14017">http://arxiv.org/abs/2310.14017</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dl4mhealth/comet">https://github.com/dl4mhealth/comet</a></li>
<li>paper_authors: Yihe Wang, Yu Han, Haishuai Wang, Xiang Zhang</li>
<li>for: 这个研究旨在提高医疗时间序列分析中的相似表现学习，以便更好地利用医疗时间序列中的资讯，减少专业人员的努力和时间投入。</li>
<li>methods: 本研究提出了一个创新的层次架构，叫做COMET，它在医疗时间序列中捕捉到四个可能的水平的数据一致性：观察、样本、实验和病人水平。通过开发多个水平的对照损失函数，我们可以学习有效的表现，并实现自动化的医疗时间序列分析。</li>
<li>results: 我们在具有10%和1%的标签数据分布的挑战性设置下实现了实验，并与六个基eline进行比较。结果显示，COMET在所有基eline中都表现出色，特别是在10%和1%的标签数据分布下的设置中。这些结果证明了我们的框架在医疗时间序列相似表现学习中的重要性。<details>
<summary>Abstract</summary>
Contrastive representation learning is crucial in medical time series analysis as it alleviates dependency on labor-intensive, domain-specific, and scarce expert annotations. However, existing contrastive learning methods primarily focus on one single data level, which fails to fully exploit the intricate nature of medical time series. To address this issue, we present COMET, an innovative hierarchical framework that leverages data consistencies at all inherent levels in medical time series. Our meticulously designed model systematically captures data consistency from four potential levels: observation, sample, trial, and patient levels. By developing contrastive loss at multiple levels, we can learn effective representations that preserve comprehensive data consistency, maximizing information utilization in a self-supervised manner. We conduct experiments in the challenging patient-independent setting. We compare COMET against six baselines using three diverse datasets, which include ECG signals for myocardial infarction and EEG signals for Alzheimer's and Parkinson's diseases. The results demonstrate that COMET consistently outperforms all baselines, particularly in setup with 10% and 1% labeled data fractions across all datasets. These results underscore the significant impact of our framework in advancing contrastive representation learning techniques for medical time series. The source code is available at https://github.com/DL4mHealth/COMET.
</details>
<details>
<summary>摘要</summary>
医疗时序分析中，对比表示学学习是关键，因为它减轻了劳动密集、领域特定和珍贵专家标注的依赖。然而，现有的对比学习方法主要集中在单一数据层次，这会无法全面利用医疗时序的复杂性。为解决这个问题，我们提出了COMET，一种创新的层次结构框架，利用医疗时序中所有自然层次的数据一致性。我们 méticulously 设计的模型逐级捕捉数据一致性，从观察、样本、试验和患者四个级别进行对比学习。通过开发多级对比损失函数，我们可以学习有效的表示，保留医疗时序中完整的数据一致性，最大化自我监督的信息利用。我们在医疗时序中的挑战性 patrnt-independent 设置中进行实验，与六个基线比较。我们使用三种多样化的数据集，包括ECG信号和 Alzheimer's 和 Parkinson's 疾病的 EEG 信号。结果表明，COMET 在所有基线之上具有优异表现，特别是在10%和1%标注数据分布中的设置中。这些结果赋予COMET 在医疗时序对比表示学习技术的进步。COMET 的源代码可以在 GitHub 上获取：https://github.com/DL4mHealth/COMET。
</details></li>
</ul>
<hr>
<h2 id="One-is-More-Diverse-Perspectives-within-a-Single-Network-for-Efficient-DRL"><a href="#One-is-More-Diverse-Perspectives-within-a-Single-Network-for-Efficient-DRL" class="headerlink" title="One is More: Diverse Perspectives within a Single Network for Efficient DRL"></a>One is More: Diverse Perspectives within a Single Network for Efficient DRL</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14009">http://arxiv.org/abs/2310.14009</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiqin Tan, Ling Pan, Longbo Huang</li>
<li>for: 这 paper 是用于提高 deep reinforcement learning 的效率和稳定性的研究。</li>
<li>methods: 这 paper 使用了多个子网络（OMNet），每个子网络输出不同的结果，从而提高了学习效率和鲁棒性。</li>
<li>results: 通过在 MuJoCo 测试集上进行 comprehensive 评估， authors 发现 OMNet 能够很好地寻找效果和计算成本之间的平衡。<details>
<summary>Abstract</summary>
Deep reinforcement learning has achieved remarkable performance in various domains by leveraging deep neural networks for approximating value functions and policies. However, using neural networks to approximate value functions or policy functions still faces challenges, including low sample efficiency and overfitting. In this paper, we introduce OMNet, a novel learning paradigm utilizing multiple subnetworks within a single network, offering diverse outputs efficiently. We provide a systematic pipeline, including initialization, training, and sampling with OMNet. OMNet can be easily applied to various deep reinforcement learning algorithms with minimal additional overhead. Through comprehensive evaluations conducted on MuJoCo benchmark, our findings highlight OMNet's ability to strike an effective balance between performance and computational cost.
</details>
<details>
<summary>摘要</summary>
深度强化学习已在多个领域取得了显著的成绩，通过使用深度神经网络来近似价值函数和策略函数。然而，使用神经网络来近似价值函数或策略函数仍然面临挑战，包括低样本效率和过拟合。在本文中，我们介绍了OMNet，一种新的学习模式，它在单个网络中嵌入多个子网络，以获得多种输出，高效地。我们提供了一个系统化的管道，包括初始化、训练和采样，以及OMNet可以轻松应用于多种深度强化学习算法，增加了最小的额外开销。通过对MuJoCo benchmark进行了全面的评估，我们的发现表明OMNet能够均衡性能和计算成本。
</details></li>
</ul>
<hr>
<h2 id="On-Bilingual-Lexicon-Induction-with-Large-Language-Models"><a href="#On-Bilingual-Lexicon-Induction-with-Large-Language-Models" class="headerlink" title="On Bilingual Lexicon Induction with Large Language Models"></a>On Bilingual Lexicon Induction with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13995">http://arxiv.org/abs/2310.13995</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cambridgeltl/prompt4bli">https://github.com/cambridgeltl/prompt4bli</a></li>
<li>paper_authors: Yaoyiran Li, Anna Korhonen, Ivan Vulić</li>
<li>for: 本研究旨在探讨使用最新一代大语言模型（LLMs）来开发双语词表。</li>
<li>methods: 我们采用了零批示推理、几批示推理和标准的BLI预训练方法来评估多语言模型（mLLMs）的应用性。</li>
<li>results: 我们的实验结果显示，使用几批示推理的 nearest neighbours 示例可以达到最佳性能，并创造了许多语言对的新纪录。<details>
<summary>Abstract</summary>
Bilingual Lexicon Induction (BLI) is a core task in multilingual NLP that still, to a large extent, relies on calculating cross-lingual word representations. Inspired by the global paradigm shift in NLP towards Large Language Models (LLMs), we examine the potential of the latest generation of LLMs for the development of bilingual lexicons. We ask the following research question: Is it possible to prompt and fine-tune multilingual LLMs (mLLMs) for BLI, and how does this approach compare against and complement current BLI approaches? To this end, we systematically study 1) zero-shot prompting for unsupervised BLI and 2) few-shot in-context prompting with a set of seed translation pairs, both without any LLM fine-tuning, as well as 3) standard BLI-oriented fine-tuning of smaller LLMs. We experiment with 18 open-source text-to-text mLLMs of different sizes (from 0.3B to 13B parameters) on two standard BLI benchmarks covering a range of typologically diverse languages. Our work is the first to demonstrate strong BLI capabilities of text-to-text mLLMs. The results reveal that few-shot prompting with in-context examples from nearest neighbours achieves the best performance, establishing new state-of-the-art BLI scores for many language pairs. We also conduct a series of in-depth analyses and ablation studies, providing more insights on BLI with (m)LLMs, also along with their limitations.
</details>
<details>
<summary>摘要</summary>
global paradigm shift в NLP towards Large Language Models (LLMs) 我们从 LLMs 的最新一代获得了开发双语词汇的潜力。我们的研究问题是：可以将多ilingual LLMs (mLLMs) 用于双语词汇问题（BLI）的问题，并且如何与现有的 BLI 方法相比。为此，我们系统地研究了以下问题：1. 零shot 提示，不需要精度批评的 BLI。2. 几个 shot 的内容提示，使用一组seed translation pairs，而不需要精度批评。3. 标准的 BLI-oriented 精度批评。我们实验了 18 个开源的文本至文本 mLLMs ，它们的大小在 0.3B 到 13B 个参数之间，在两个标准的 BLI 库中进行了评估。我们的研究是首次证明了文本至文本 mLLMs 的强大 BLI 能力。结果显示，几个 shot 的内容提示可以取得最好的性能，建立了许多语言对的新的顶峰 BLI 分数。我们还进行了详细的分析和剔除研究，提供了更多关于 BLI 的关键。
</details></li>
</ul>
<hr>
<h2 id="Application-of-deep-and-reinforcement-learning-to-boundary-control-problems"><a href="#Application-of-deep-and-reinforcement-learning-to-boundary-control-problems" class="headerlink" title="Application of deep and reinforcement learning to boundary control problems"></a>Application of deep and reinforcement learning to boundary control problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15191">http://arxiv.org/abs/2310.15191</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zenineasa/MasterThesis">https://github.com/zenineasa/MasterThesis</a></li>
<li>paper_authors: Zenin Easa Panthakkalakath, Juraj Kardoš, Olaf Schenk</li>
<li>for: 本研究旨在使用深度学习和强化学习解决边控制问题。</li>
<li>methods: 我们采用迭代优化策略，使用空间神经网络构建初始猜测，并使用维度-时间神经网络学习迭代优化算法。</li>
<li>results: 我们的数据培育和测试表明，提议的方法可以与现有的解决方案相比，速度和准确性相当。在我们的初步结果中，网络实现成本比IPOPT低于51%的情况。<details>
<summary>Abstract</summary>
The boundary control problem is a non-convex optimization and control problem in many scientific domains, including fluid mechanics, structural engineering, and heat transfer optimization. The aim is to find the optimal values for the domain boundaries such that the enclosed domain adhering to the governing equations attains the desired state values. Traditionally, non-linear optimization methods, such as the Interior-Point method (IPM), are used to solve such problems.   This project explores the possibilities of using deep learning and reinforcement learning to solve boundary control problems. We adhere to the framework of iterative optimization strategies, employing a spatial neural network to construct well-informed initial guesses, and a spatio-temporal neural network learns the iterative optimization algorithm using policy gradients. Synthetic data, generated from the problems formulated in the literature, is used for training, testing and validation. The numerical experiments indicate that the proposed method can rival the speed and accuracy of existing solvers. In our preliminary results, the network attains costs lower than IPOPT, a state-of-the-art non-linear IPM, in 51\% cases. The overall number of floating point operations in the proposed method is similar to that of IPOPT. Additionally, the informed initial guess method and the learned momentum-like behaviour in the optimizer method are incorporated to avoid convergence to local minima.
</details>
<details>
<summary>摘要</summary>
“边界控制问题是科学领域中的一种非 convex 优化和控制问题，包括流体动力学、结构工程和热传输优化等。目标是找到包含 governing 方程的Domaint 的优化值，使涵盖的 Domaint 达到所需的状态值。传统上，非线性优化方法，如Interior-Point 方法（IPM），用于解决这些问题。”This project explores the use of deep learning and reinforcement learning to solve boundary control problems. We use an iterative optimization strategy, with a spatial neural network constructing well-informed initial guesses and a spatio-temporal neural network learning the iterative optimization algorithm using policy gradients. Synthetic data, generated from problems formulated in the literature, is used for training, testing, and validation. Our numerical experiments show that the proposed method can rival the speed and accuracy of existing solvers. In our preliminary results, the network attains costs lower than IPOPT, a state-of-the-art non-linear IPM, in 51% of cases. The overall number of floating point operations in the proposed method is similar to that of IPOPT. Additionally, we incorporate an informed initial guess method and a learned momentum-like behavior in the optimizer to avoid convergence to local minima.
</details></li>
</ul>
<hr>
<h2 id="Ensemble-Instruct-Generating-Instruction-Tuning-Data-with-a-Heterogeneous-Mixture-of-LMs"><a href="#Ensemble-Instruct-Generating-Instruction-Tuning-Data-with-a-Heterogeneous-Mixture-of-LMs" class="headerlink" title="Ensemble-Instruct: Generating Instruction-Tuning Data with a Heterogeneous Mixture of LMs"></a>Ensemble-Instruct: Generating Instruction-Tuning Data with a Heterogeneous Mixture of LMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13961">http://arxiv.org/abs/2310.13961</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ibm/ensemble-instruct">https://github.com/ibm/ensemble-instruct</a></li>
<li>paper_authors: Young-Suk Lee, Md Arafat Sultan, Yousef El-Kurdi, Tahira Naseem Asim Munawar, Radu Florian, Salim Roukos, Ramón Fernandez Astudillo</li>
<li>for: 用于自动生成数据，提高对话机器人的强度，只需要小量的人工监督。</li>
<li>methods: 使用 Self-Instruct 和 Alpaca 等技术，训练小型语言模型（10B–40B参数），并使用 permissive licenses。</li>
<li>results: 提高自动生成数据的质量，对于小型语言模型和大型语言模型都有显著提高，并且小型 instruction-tuned LM 生成的输出更有用。<details>
<summary>Abstract</summary>
Using in-context learning (ICL) for data generation, techniques such as Self-Instruct (Wang et al., 2023) or the follow-up Alpaca (Taori et al., 2023) can train strong conversational agents with only a small amount of human supervision. One limitation of these approaches is that they resort to very large language models (around 175B parameters) that are also proprietary and non-public. Here we explore the application of such techniques to language models that are much smaller (around 10B--40B parameters) and have permissive licenses. We find the Self-Instruct approach to be less effective at these sizes and propose new ICL methods that draw on two main ideas: (a) Categorization and simplification of the ICL templates to make prompt learning easier for the LM, and (b) Ensembling over multiple LM outputs to help select high-quality synthetic examples. Our algorithm leverages the 175 Self-Instruct seed tasks and employs separate pipelines for instructions that require an input and instructions that do not. Empirical investigations with different LMs show that: (1) Our proposed method yields higher-quality instruction tuning data than Self-Instruct, (2) It improves performances of both vanilla and instruction-tuned LMs by significant margins, and (3) Smaller instruction-tuned LMs generate more useful outputs than their larger un-tuned counterparts. Our codebase is available at https://github.com/IBM/ensemble-instruct.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:使用增Context学习（ICL）数据生成技术，如Self-Instruct（Wang et al., 2023）或Alpaca（Taori et al., 2023），可以帮助强化对话机器人，只需小量人工监督。这些方法的一个限制是它们需要非常大的语言模型（约175B参数），这些模型也是 propriety 和非公共的。在这里，我们探索将这些技术应用于更小的语言模型（约10B--40B参数），这些模型具有允许的许可证。我们发现Self-Instruct方法在这些大小下不太有效，我们提出了新的ICL方法，它们基于以下两个主要想法：（a）将ICL模板分类和简化，使Language Model（LM）更容易学习提示，和（b）将多个LM输出ensemble，以选择高质量的人工示例。我们的算法利用Self-Instruct的175个种子任务，并使用不同的LM pipeline，对于需要输入和不需要输入的指令而分开。我们的实验表明，我们的提posed方法可以生成更高质量的指令调整数据，并且可以提高vanilla LM和指令调整LM的性能，同时使小型指令调整LM生成更有用的输出。我们的代码库可以在https://github.com/IBM/ensemble-instruct中获取。
</details></li>
</ul>
<hr>
<h2 id="Towards-dialogue-based-computer-aided-software-requirements-elicitation"><a href="#Towards-dialogue-based-computer-aided-software-requirements-elicitation" class="headerlink" title="Towards dialogue based, computer aided software requirements elicitation"></a>Towards dialogue based, computer aided software requirements elicitation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13953">http://arxiv.org/abs/2310.13953</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vasiliy Seibert</li>
<li>for: 这篇论文是为了探讨如何从自然语言规范中提取模型的问题。</li>
<li>methods: 这篇论文提出了一种对话基于的计算机支持的软件需求分析方法，而不是先前的模型提取方法，它鼓励个性、创造力和真实的妥协。</li>
<li>results: 这篇论文通过简单的实验示例了这种方法的核心思想，并讨论了这种方法和现有方法的比较。同时，它也认为未来的自然语言处理和生成AI技术的进步可能会带来重要的进步。<details>
<summary>Abstract</summary>
Several approaches have been presented, which aim to extract models from natural language specifications. These approaches have inherent weaknesses for they assume an initial problem understanding that is perfect, and they leave no room for feedback. Motivated by real-world collaboration settings between requirements engineers and customers, this paper proposes an interaction blueprint that aims for dialogue based, computer aided software requirements analysis. Compared to mere model extraction approaches, this interaction blueprint encourages individuality, creativity and genuine compromise. A simplistic Experiment was conducted to showcase the general idea. This paper discusses the experiment as well as the proposed interaction blueprint and argues, that advancements in natural language processing and generative AI might lead to significant progress in a foreseeable future. However, for that, there is a need to move away from a magical black box expectation and instead moving towards a dialogue based approach that recognizes the individuality that is an undeniable part of requirements engineering.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:有几种方法已经被提出来，以EXTRACT模型从自然语言规格。这些方法具有内在的弱点，因为它们假设问题理解是完美的，并且没有减 feedback 的机制。由实际世界的合作场景中的需求工程师和客户而受到 inspirited，这篇论文提出了对话基本的交互蓝图，用于计算机助成的软件需求分析。这种方法强调个性、创造力和真实的妥协，并在简单的实验中进行了示例。这篇论文讨论了实验和提出的交互蓝图，并 argued  That advancements in自然语言处理和生成 AI 可能会在未来导致显著的进步，但我们需要停止对 "黑盒子" 的期望，而是转向对话基本的方法，认可需求工程的个性。
</details></li>
</ul>
<hr>
<h2 id="Approximate-Implication-for-Probabilistic-Graphical-Models"><a href="#Approximate-Implication-for-Probabilistic-Graphical-Models" class="headerlink" title="Approximate Implication for Probabilistic Graphical Models"></a>Approximate Implication for Probabilistic Graphical Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13942">http://arxiv.org/abs/2310.13942</a></li>
<li>repo_url: None</li>
<li>paper_authors: Batya Kenig</li>
<li>for: 本研究证明了关于 Conditional Independence (CI) 在 Probabilistic Graphical Models (PGMs) 中的准确性问题。</li>
<li>methods: 本文使用了现有的系统推理方法，以及新的证明方法，来研究 CI 的准确性。</li>
<li>results: 本文证明了对于非导向图模型，无法提供任何保证，而对于导向图模型，使用 $d$-separation 算法可以提供准确的 CI。 Additionally, the paper establishes improved approximation guarantees for independence relations derived from marginal and saturated CIs.<details>
<summary>Abstract</summary>
The graphical structure of Probabilistic Graphical Models (PGMs) represents the conditional independence (CI) relations that hold in the modeled distribution. Every separator in the graph represents a conditional independence relation in the distribution, making them the vehicle through which new conditional independencies are inferred and verified. The notion of separation in graphs depends on whether the graph is directed (i.e., a Bayesian Network), or undirected (i.e., a Markov Network).   The premise of all current systems-of-inference for deriving CIs in PGMs, is that the set of CIs used for the construction of the PGM hold exactly. In practice, algorithms for extracting the structure of PGMs from data discover approximate CIs that do not hold exactly in the distribution. In this paper, we ask how the error in this set propagates to the inferred CIs read off the graphical structure. More precisely, what guarantee can we provide on the inferred CI when the set of CIs that entailed it hold only approximately? It has recently been shown that in the general case, no such guarantee can be provided.   In this work, we prove new negative and positive results concerning this problem. We prove that separators in undirected PGMs do not necessarily represent approximate CIs. That is, no guarantee can be provided for CIs inferred from the structure of undirected graphs. We prove that such a guarantee exists for the set of CIs inferred in directed graphical models, making the $d$-separation algorithm a sound and complete system for inferring approximate CIs. We also establish improved approximation guarantees for independence relations derived from marginal and saturated CIs.
</details>
<details>
<summary>摘要</summary>
PGMs（概率图）的图Structural representation表示模型中的 conditional independence（CI）关系。每个分隔器在图中表示模型中的CI关系，使其成为新的CI关系的推理和验证的渠道。图中的分隔器取决于图是指向的（即 bayesian network）还是无向的（即 markov network）。现有所有系统的推理方法都基于CI关系的集合在PGM中准确地满足。在实际中，数据中PGM的结构检索算法发现的CI关系不准确地存在于分布中。在这篇论文中，我们问 Error propagation in inferred CIs 问题的解决方案。即在PGM中的CI关系是否准确地推理出来？我们证明了一些新的负面和正面结果。在无向PGM中，分隔器不一定表示CI关系的近似。即，无法提供PGM中的CI关系的准确性 garantia。而在指向PGM中，我们证明了$d$-separation算法是一个准确和完整的系统，用于推理CI关系。此外，我们还证明了基于边缘和满足CI关系的约束的约束 CI 关系的改进了近似性保证。
</details></li>
</ul>
<hr>
<h2 id="The-Hidden-Adversarial-Vulnerabilities-of-Medical-Federated-Learning"><a href="#The-Hidden-Adversarial-Vulnerabilities-of-Medical-Federated-Learning" class="headerlink" title="The Hidden Adversarial Vulnerabilities of Medical Federated Learning"></a>The Hidden Adversarial Vulnerabilities of Medical Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13893">http://arxiv.org/abs/2310.13893</a></li>
<li>repo_url: None</li>
<li>paper_authors: Erfan Darzi, Florian Dubost, Nanna. M. Sijtsema, P. M. A van Ooijen</li>
<li>for: 这个论文探讨了联合医疗图像分析系统对 adversarial 攻击的感受性。</li>
<li>methods: 该分析发现了一种新的攻击途径：利用先前全局模型更新的梯度信息，攻击者可以提高他们的攻击效率和传播性，而无需额外的计算成本增加。</li>
<li>results: 研究表明，适当初始化后的单步攻击（例如 FGSM）可以超越其迭代对手的效率，但需要更少的计算资源。这些发现强调了在联合医疗设备中应对 AI 安全的需要。<details>
<summary>Abstract</summary>
In this paper, we delve into the susceptibility of federated medical image analysis systems to adversarial attacks. Our analysis uncovers a novel exploitation avenue: using gradient information from prior global model updates, adversaries can enhance the efficiency and transferability of their attacks. Specifically, we demonstrate that single-step attacks (e.g. FGSM), when aptly initialized, can outperform the efficiency of their iterative counterparts but with reduced computational demand. Our findings underscore the need to revisit our understanding of AI security in federated healthcare settings.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们探讨了联合医疗图像分析系统中的恶意攻击的感受性。我们的分析发现了一个新的攻击途径：使用先前全球模型更新的梯度信息，恶意者可以提高攻击的效率和传播性。具体来说，我们表明了使用单步攻击（如FGSM），当初始化得当时，可以超过迭代攻击的效率，但却减少计算强度。我们的发现强调了在联合医疗设施中AI安全的重要性。
</details></li>
</ul>
<hr>
<h2 id="COVIDFakeExplainer-An-Explainable-Machine-Learning-based-Web-Application-for-Detecting-COVID-19-Fake-News"><a href="#COVIDFakeExplainer-An-Explainable-Machine-Learning-based-Web-Application-for-Detecting-COVID-19-Fake-News" class="headerlink" title="COVIDFakeExplainer: An Explainable Machine Learning based Web Application for Detecting COVID-19 Fake News"></a>COVIDFakeExplainer: An Explainable Machine Learning based Web Application for Detecting COVID-19 Fake News</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13890">http://arxiv.org/abs/2310.13890</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/DatProGuy/COVIDFakeExplainer">https://github.com/DatProGuy/COVIDFakeExplainer</a></li>
<li>paper_authors: Dylan Warman, Muhammad Ashad Kabir</li>
<li>for: 这篇论文旨在提供一个实用的伪新闻检测解决方案，以帮助社会对伪新闻进行有效防范。</li>
<li>methods: 本论文使用机器学习技术，包括深度学习方法，以探索伪新闻检测的可能性。特别是，本论文使用BERT模型，并将其应用于实际的伪新闻检测和解释。</li>
<li>results: 本论文的实验结果显示，BERT模型在检测COVID-19相关伪新闻方面 exhibits 高度的准确性。此外，本论文还提出了一个可读性增强的BERT模型，并将其作为一个服务通过AWS云端API主机。<details>
<summary>Abstract</summary>
Fake news has emerged as a critical global issue, magnified by the COVID-19 pandemic, underscoring the need for effective preventive tools. Leveraging machine learning, including deep learning techniques, offers promise in combatting fake news. This paper goes beyond by establishing BERT as the superior model for fake news detection and demonstrates its utility as a tool to empower the general populace. We have implemented a browser extension, enhanced with explainability features, enabling real-time identification of fake news and delivering easily interpretable explanations. To achieve this, we have employed two publicly available datasets and created seven distinct data configurations to evaluate three prominent machine learning architectures. Our comprehensive experiments affirm BERT's exceptional accuracy in detecting COVID-19-related fake news. Furthermore, we have integrated an explainability component into the BERT model and deployed it as a service through Amazon's cloud API hosting (AWS). We have developed a browser extension that interfaces with the API, allowing users to select and transmit data from web pages, receiving an intelligible classification in return. This paper presents a practical end-to-end solution, highlighting the feasibility of constructing a holistic system for fake news detection, which can significantly benefit society.
</details>
<details>
<summary>摘要</summary>
假新闻在全球范围内已成为一个严重的问题，COVID-19大流行的爆发进一步强调了需要有效的预防工具。利用机器学习，包括深度学习技术，可能在打击假新闻方面提供希望。这篇论文超越了现有的研究，将BERT模型确定为假新闻检测的最佳模型，并将其作为普通民众 empower 的工具。我们开发了一款浏览器扩展程序，该扩展程序具有解释性特性，可以在实时检测假新闻并提供可读性高的解释。为了实现这一点，我们使用了两个公共可用的数据集，并创建了七种不同的数据配置来评估三种知名的机器学习架构。我们的广泛的实验证明了 COVID-19 相关的假新闻检测BERT模型的突出性。此外，我们将BERT模型集成了解释性Component，并通过Amazon云API主机（AWS）部署为服务。我们开发了一款浏览器扩展程序，该扩展程序可以从网页上选择和传输数据，并得到可读性高的分类结果。本论文提出了一个实用的端到端解决方案， highlighting 社会的可能性建立一个总体的假新闻检测系统，该系统可以对社会产生很大的好处。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/21/cs.AI_2023_10_21/" data-id="cloimip5o005vs488hcencgew" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/21/cs.CL_2023_10_21/" class="article-date">
  <time datetime="2023-10-21T11:00:00.000Z" itemprop="datePublished">2023-10-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/21/cs.CL_2023_10_21/">cs.CL - 2023-10-21</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Structural-generalization-in-COGS-Supertagging-is-almost-all-you-need"><a href="#Structural-generalization-in-COGS-Supertagging-is-almost-all-you-need" class="headerlink" title="Structural generalization in COGS: Supertagging is (almost) all you need"></a>Structural generalization in COGS: Supertagging is (almost) all you need</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14124">http://arxiv.org/abs/2310.14124</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alban-petit/semantic-supertag-parser">https://github.com/alban-petit/semantic-supertag-parser</a></li>
<li>paper_authors: Alban Petit, Caio Corro, François Yvon</li>
<li>for: 提高 neural network 在不同类型的语言模型中的泛化能力</li>
<li>methods: 提出了一种基于图的semantic parsing框架，并对其进行了多种扩展以解决泛化问题</li>
<li>results: 实验结果表明，我们的方法可以在COGS dataset中提高泛化能力，特别是在需要结构泛化的例子上得到了显著提高<details>
<summary>Abstract</summary>
In many Natural Language Processing applications, neural networks have been found to fail to generalize on out-of-distribution examples. In particular, several recent semantic parsing datasets have put forward important limitations of neural networks in cases where compositional generalization is required. In this work, we extend a neural graph-based semantic parsing framework in several ways to alleviate this issue. Notably, we propose: (1) the introduction of a supertagging step with valency constraints, expressed as an integer linear program; (2) a reduction of the graph prediction problem to the maximum matching problem; (3) the design of an incremental early-stopping training strategy to prevent overfitting. Experimentally, our approach significantly improves results on examples that require structural generalization in the COGS dataset, a known challenging benchmark for compositional generalization. Overall, our results confirm that structural constraints are important for generalization in semantic parsing.
</details>
<details>
<summary>摘要</summary>
多种自然语言处理应用程序中，神经网络通常无法泛化到不同分布中的示例。特别是在需要 Compositional Generalization 的 semantic parsing 数据集中，神经网络表现出了重要的局限性。在这种情况下，我们对一种基于神经网络的 semantic parsing 框架进行了多种扩展，以解决这个问题。主要提议包括：1. 引入精度标记步骤，使用整数线性Programming来表达 valency 约束。2. 将图像预测问题转换为最大匹配问题。3. 设计了逐步停止训练策略，以避免过拟合。实验表明，我们的方法可以在 COGS 数据集中，解决需要结构泛化的示例中显著提高结果。总之，我们的结果证明了结构约束对泛化的重要性。
</details></li>
</ul>
<hr>
<h2 id="Finite-context-Indexing-of-Restricted-Output-Space-for-NLP-Models-Facing-Noisy-Input"><a href="#Finite-context-Indexing-of-Restricted-Output-Space-for-NLP-Models-Facing-Noisy-Input" class="headerlink" title="Finite-context Indexing of Restricted Output Space for NLP Models Facing Noisy Input"></a>Finite-context Indexing of Restricted Output Space for NLP Models Facing Noisy Input</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14110">http://arxiv.org/abs/2310.14110</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mnhng/firo">https://github.com/mnhng/firo</a></li>
<li>paper_authors: Minh Nguyen, Nancy F. Chen</li>
<li>for: 提高 NLP 模型对不净输入的性能，而不是降低清晰输入的性能。</li>
<li>methods: FiRo 方法使用 finite-context aggregation 获取上下文嵌入，并在受限的输出空间内查找静止的表示。</li>
<li>results: FiRo 方法在六个分类任务和一个序列标注任务上，以不同程度的噪声为输入，与基eline相比表现出色。<details>
<summary>Abstract</summary>
NLP models excel on tasks with clean inputs, but are less accurate with noisy inputs. In particular, character-level noise such as human-written typos and adversarially-engineered realistic-looking misspellings often appears in text and can easily trip up NLP models. Prior solutions to address character-level noise often alter the content of the inputs (low fidelity), thus inadvertently lowering model accuracy on clean inputs. We proposed FiRo, an approach to boost NLP model performance on noisy inputs without sacrificing performance on clean inputs. FiRo sanitizes the input text while preserving its fidelity by inferring the noise-free form for each token in the input. FiRo uses finite-context aggregation to obtain contextual embeddings which is then used to find the noise-free form within a restricted output space. The output space is restricted to a small cluster of probable candidates in order to predict the noise-free tokens more accurately. Although the clusters are small, FiRo's effective vocabulary (union of all clusters) can be scaled up to better preserve the input content. Experimental results show NLP models that use FiRo outperforming baselines on six classification tasks and one sequence labeling task at various degrees of noise.
</details>
<details>
<summary>摘要</summary>
FiRo 使用 finite-context aggregation 获取上下文嵌入，然后使用 restricted output space 来预测噪音自由形。输出空间是限制在一小 clusters 中，以更准确地预测噪音自由形。虽然 clusters 是小的，但FiRo 的有效词汇（union of all clusters）可以被扩展，以更好地保持输入内容。实验结果表明，使用 FiRo 的 NLP 模型在六个分类任务和一个序列标签任务上表现出色，在不同程度的噪音下都高于基eline。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Knowledge-Graphs-for-Orphan-Entity-Allocation-in-Resume-Processing"><a href="#Leveraging-Knowledge-Graphs-for-Orphan-Entity-Allocation-in-Resume-Processing" class="headerlink" title="Leveraging Knowledge Graphs for Orphan Entity Allocation in Resume Processing"></a>Leveraging Knowledge Graphs for Orphan Entity Allocation in Resume Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14093">http://arxiv.org/abs/2310.14093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aagam Bakliwal, Shubham Manish Gandhi, Yashodhara Haribhakta</li>
<li>for:  automatize and enhance the efficiency of the job screening process</li>
<li>methods:  association mining, concept extraction, external knowledge linking, named entity recognition, and knowledge graph construction</li>
<li>results:  successful bucketing of orphan entities within resumes, more effective candidate-job matching, and improved resume screening process accuracy.<details>
<summary>Abstract</summary>
Significant challenges are posed in talent acquisition and recruitment by processing and analyzing unstructured data, particularly resumes. This research presents a novel approach for orphan entity allocation in resume processing using knowledge graphs. Techniques of association mining, concept extraction, external knowledge linking, named entity recognition, and knowledge graph construction are integrated into our pipeline. By leveraging these techniques, the aim is to automate and enhance the efficiency of the job screening process by successfully bucketing orphan entities within resumes. This allows for more effective matching between candidates and job positions, streamlining the resume screening process, and enhancing the accuracy of candidate-job matching. The approach's exceptional effectiveness and resilience are highlighted through extensive experimentation and evaluation, ensuring that alternative measures can be relied upon for seamless processing and orphan entity allocation in case of any component failure. The capabilities of knowledge graphs in generating valuable insights through intelligent information extraction and representation, specifically in the domain of categorizing orphan entities, are highlighted by the results of our research.
</details>
<details>
<summary>摘要</summary>
significannot challenges are posed in talent acquisition and recruitment by processing and analyzing unstructured data, particularly resumes. This research presents a novel approach for orphan entity allocation in resume processing using knowledge graphs. Techniques of association mining, concept extraction, external knowledge linking, named entity recognition, and knowledge graph construction are integrated into our pipeline. By leveraging these techniques, the aim is to automate and enhance the efficiency of the job screening process by successfully bucketing orphan entities within resumes. This allows for more effective matching between candidates and job positions, streamlining the resume screening process, and enhancing the accuracy of candidate-job matching. The approach's exceptional effectiveness and resilience are highlighted through extensive experimentation and evaluation, ensuring that alternative measures can be relied upon for seamless processing and orphan entity allocation in case of any component failure. The capabilities of knowledge graphs in generating valuable insights through intelligent information extraction and representation, specifically in the domain of categorizing orphan entities, are highlighted by the results of our research.Here's the text with some additional information about the Simplified Chinese translation:The translation is in Simplified Chinese, which is the standardized form of Chinese used in mainland China and Singapore. The text is written in a formal and academic style, using technical terms and concepts related to natural language processing, machine learning, and knowledge graphs.Some of the key concepts and techniques used in the text include:* orphan entity allocation (遗弃实体分配): the process of identifying and categorizing entities in unstructured data, such as resumes, that do not fit into predefined categories or structures.* knowledge graphs (知识图): a type of graph that represents entities and their relationships in a structured and interconnected way, allowing for efficient information extraction and analysis.* association mining (关联挖掘): a technique used to discover and extract relationships between entities in unstructured data, such as resumes.* concept extraction (概念提取): a technique used to identify and extract relevant concepts and entities from unstructured data, such as resumes.* named entity recognition (命名实体识别): a technique used to identify and extract specific types of entities, such as names of people, organizations, and locations, from unstructured data, such as resumes.Overall, the text presents a novel approach for automating and enhancing the efficiency of the job screening process using knowledge graphs and other techniques, with the goal of improving the accuracy of candidate-job matching and streamlining the resume screening process.
</details></li>
</ul>
<hr>
<h2 id="MedEval-A-Multi-Level-Multi-Task-and-Multi-Domain-Medical-Benchmark-for-Language-Model-Evaluation"><a href="#MedEval-A-Multi-Level-Multi-Task-and-Multi-Domain-Medical-Benchmark-for-Language-Model-Evaluation" class="headerlink" title="MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark for Language Model Evaluation"></a>MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark for Language Model Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14088">http://arxiv.org/abs/2310.14088</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zexue He, Yu Wang, An Yan, Yao Liu, Eric Y. Chang, Amilcare Gentili, Julian McAuley, Chun-Nan Hsu</li>
<li>for: 本研究旨在提供一个多元、多任务、多领域医疗 benchmark，以推动语言模型在医疗领域的开发。</li>
<li>methods: 本研究使用了多种健康系统的数据，包括8种检查方式，共有22,779句 sentence和21,228份报告。研究人员在多个水平提供了专家标注，实现了精确的数据分类和多元的应用潜力。</li>
<li>results: 研究人员通过评估10种通用和领域特定的语言模型，包括健康领域基于领域的基eline和一般化的大语言模型（如ChatGPT），获得了不同任务之间的语言模型效果的评估结果。研究结果显示，大语言模型在不同任务之间的效果不同，并且发现了对 instrucion 的适应是一个重要的因素。<details>
<summary>Abstract</summary>
Curated datasets for healthcare are often limited due to the need of human annotations from experts. In this paper, we present MedEval, a multi-level, multi-task, and multi-domain medical benchmark to facilitate the development of language models for healthcare. MedEval is comprehensive and consists of data from several healthcare systems and spans 35 human body regions from 8 examination modalities. With 22,779 collected sentences and 21,228 reports, we provide expert annotations at multiple levels, offering a granular potential usage of the data and supporting a wide range of tasks. Moreover, we systematically evaluated 10 generic and domain-specific language models under zero-shot and finetuning settings, from domain-adapted baselines in healthcare to general-purposed state-of-the-art large language models (e.g., ChatGPT). Our evaluations reveal varying effectiveness of the two categories of language models across different tasks, from which we notice the importance of instruction tuning for few-shot usage of large language models. Our investigation paves the way toward benchmarking language models for healthcare and provides valuable insights into the strengths and limitations of adopting large language models in medical domains, informing their practical applications and future advancements.
</details>
<details>
<summary>摘要</summary>
医疗领域的数据集经常受限于专家的人工标注。在这篇论文中，我们介绍了医生eval，一个多级、多任务、多领域的医疗语言模型开发 benchmark。医生eval 全面，涵盖多个医疗系统，覆盖人体8种检查方式，共收集了22,779句话和21,228份报告。我们提供了多个水平的专家标注，为数据的细化使用提供了可能的潜在应用，支持广泛的任务。此外，我们系统地评估了10种通用和医疗领域特定的语言模型，包括医疗领域基线模型和普通大语言模型（如ChatGPT）。我们的评估发现，这两类语言模型在不同任务中的效果不同，而且在几个任务中，大语言模型的几个 shot 使用需要进行调教。我们的调查开创了医疗领域语言模型的 benchmarking 的可能性，并为将来的应用和进步提供了有价值的见解，了解大语言模型在医疗领域的优劣和局限性。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Accuracy-Evaluating-Self-Consistency-of-Code-Large-Language-Models-with-IdentityChain"><a href="#Beyond-Accuracy-Evaluating-Self-Consistency-of-Code-Large-Language-Models-with-IdentityChain" class="headerlink" title="Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models with IdentityChain"></a>Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models with IdentityChain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14053">http://arxiv.org/abs/2310.14053</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/marcusm117/IdentityChain">https://github.com/marcusm117/IdentityChain</a></li>
<li>paper_authors: Marcus J. Min, Yangruibo Ding, Luca Buratti, Saurabh Pujar, Gail Kaiser, Suman Jana, Baishakhi Ray</li>
<li>for: 评估大型自然语言处理模型（Code LLMs）的可靠性。</li>
<li>methods: 提出了一种名为IdentityChain的框架，可以同时评估模型的自我一致性和总体准确率。</li>
<li>results: 对11个Code LLMs进行了评估，发现它们无法保持自我一致性，并且通过使用IdentityChain可以曝光当前模型的三大缺陷。<details>
<summary>Abstract</summary>
Code Large Language Models (Code LLMs) are being increasingly employed in real-life applications, so evaluating them is critical. While the general accuracy of Code LLMs on individual tasks has been extensively evaluated, their self-consistency across different tasks is overlooked. Intuitively, a trustworthy model should be self-consistent when generating natural language specifications for its own code and generating code for its own specifications. Failure to preserve self-consistency reveals a lack of understanding of the shared semantics underlying natural language and programming language, and therefore undermines the trustworthiness of a model. In this paper, we first formally define the self-consistency of Code LLMs and then design a framework, IdentityChain, which effectively and efficiently evaluates the self-consistency and general accuracy of a model at the same time. We study eleven Code LLMs and show that they fail to preserve self-consistency, which is indeed a distinct aspect from general accuracy. Furthermore, we show that IdentityChain can be used as a model debugging tool to expose weaknesses of Code LLMs by demonstrating three major weaknesses that we identify in current models using IdentityChain. Our code is available at https://github.com/marcusm117/IdentityChain.
</details>
<details>
<summary>摘要</summary>
<<SYS coding:utf-8>>大型语言模型（大型语言模型，Code LLMs）在实际应用中越来越广泛使用，因此评估其重要。而大型语言模型在不同任务之间的一致性却受到了忽略。人们可以 intuition 来认为，一个可靠的模型应该在生成自然语言规范和代码之间保持一致。如果不能保持一致性，则表明模型对自然语言和编程语言共同下的 semantics 没有很好的理解，因此模型的可靠性将受到损害。在这篇论文中，我们首先正式定义了 Code LLMs 的自身一致性，然后我们设计了一个框架，叫做 IdentityChain，可以同时评估模型的自身一致性和总准确率。我们对 eleven 种 Code LLMs 进行了研究，发现它们在保持自身一致性方面存在问题，这实际上是一种与总准确率不同的特征。此外，我们还证明了 IdentityChain 可以作为模型调试工具，用于曝光当前模型的三大弱点。我们的代码可以在 https://github.com/marcusm117/IdentityChain 上找到。
</details></li>
</ul>
<hr>
<h2 id="Code-Switching-with-Word-Senses-for-Pretraining-in-Neural-Machine-Translation"><a href="#Code-Switching-with-Word-Senses-for-Pretraining-in-Neural-Machine-Translation" class="headerlink" title="Code-Switching with Word Senses for Pretraining in Neural Machine Translation"></a>Code-Switching with Word Senses for Pretraining in Neural Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14050">http://arxiv.org/abs/2310.14050</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vivek Iyer, Edoardo Barba, Alexandra Birch, Jeff Z. Pan, Roberto Navigli</li>
<li>for: 本研究旨在提高Neural Machine Translation（NMT）模型的精度和可靠性，通过在Pre-training阶段使用知识库中的单词意思信息来改善模型的多语言能力。</li>
<li>methods: 本研究提出了Word Sense Pretraining for Neural Machine Translation（WSP-NMT）方法，该方法利用知识库中的单词意思信息进行预训练，以提高模型在多语言翻译任务中的表现。</li>
<li>results: 实验结果表明，WSP-NMT方法可以显著提高翻译质量，并在不同的数据和资源匮乏情况下保持良好的表现。此外，研究还发现了在DiBiMT排除词检测测试上的细致精度提升。<details>
<summary>Abstract</summary>
Lexical ambiguity is a significant and pervasive challenge in Neural Machine Translation (NMT), with many state-of-the-art (SOTA) NMT systems struggling to handle polysemous words (Campolungo et al., 2022). The same holds for the NMT pretraining paradigm of denoising synthetic "code-switched" text (Pan et al., 2021; Iyer et al., 2023), where word senses are ignored in the noising stage -- leading to harmful sense biases in the pretraining data that are subsequently inherited by the resulting models. In this work, we introduce Word Sense Pretraining for Neural Machine Translation (WSP-NMT) - an end-to-end approach for pretraining multilingual NMT models leveraging word sense-specific information from Knowledge Bases. Our experiments show significant improvements in overall translation quality. Then, we show the robustness of our approach to scale to various challenging data and resource-scarce scenarios and, finally, report fine-grained accuracy improvements on the DiBiMT disambiguation benchmark. Our studies yield interesting and novel insights into the merits and challenges of integrating word sense information and structured knowledge in multilingual pretraining for NMT.
</details>
<details>
<summary>摘要</summary>
Lexical ambiguity 是 neural machine translation (NMT) 中的一个重要和普遍存在的挑战 (Campolungo et al., 2022)。多种现代 NMT 系统在处理多义词 (polysemous words) 方面几乎都有困难。同样的情况也出现在 NMT 预训练 paradigm 中，例如 denoising synthetic "code-switched" text (Pan et al., 2021; Iyer et al., 2023)，在杂化阶段中忽略单词的意思 -- 导致预训练数据中的词义偏见，这些偏见后来被传递给结果模型。在这种情况下，我们引入了 Word Sense Pretraining for Neural Machine Translation (WSP-NMT) - 一种结束到结束的方法，使用知识库中的单词意思特定信息来预训练多语言 NMT 模型。我们的实验表明，我们的方法可以提高总翻译质量。然后，我们证明了我们的方法可以扩展到各种复杂的数据和资源匮乏场景，并最后报告了 DiBiMT 分词标准 bencmark 上的细化准确性改进。我们的研究提供了关于将单词意思信息和结构化知识integrated into multilingual pretraining for NMT的新颖和有趣的发现。
</details></li>
</ul>
<hr>
<h2 id="MeaeQ-Mount-Model-Extraction-Attacks-with-Efficient-Queries"><a href="#MeaeQ-Mount-Model-Extraction-Attacks-with-Efficient-Queries" class="headerlink" title="MeaeQ: Mount Model Extraction Attacks with Efficient Queries"></a>MeaeQ: Mount Model Extraction Attacks with Efficient Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14047">http://arxiv.org/abs/2310.14047</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/c-w-d/meaeq">https://github.com/c-w-d/meaeq</a></li>
<li>paper_authors: Chengwei Dai, Minxuan Lv, Kun Li, Wei Zhou</li>
<li>For: The paper is written to address model extraction attacks in natural language processing (NLP) and to propose a method for stealing victim models with low query costs.* Methods: The paper uses a zero-shot sequence inference classifier combined with API service information to filter task-relevant data from a public text corpus, and a clustering-based data reduction technique to obtain representative data as queries for the attack.* Results: The paper achieves higher functional similarity to the victim model than baselines while requiring fewer queries, as demonstrated through extensive experiments conducted on four benchmark datasets.Here’s the same information in Simplified Chinese text:* For: 本文是为了 Addressing Model Extraction Attacks in Natural Language Processing (NLP) 和提出一种用于夺取受害模型的低查询成本的方法。* Methods: 本文使用 Zero-shot Sequence Inference Classifier 与 API 服务信息结合，从公共文本 corpus 中筛选任务相关的数据，并使用 clustering-based data reduction technique 获取表示性的数据作为攻击的查询。* Results: 本文在四个 benchmark 数据集上实现了高度的函数相似性，而且需要更少的查询，较基eline 高。<details>
<summary>Abstract</summary>
We study model extraction attacks in natural language processing (NLP) where attackers aim to steal victim models by repeatedly querying the open Application Programming Interfaces (APIs). Recent works focus on limited-query budget settings and adopt random sampling or active learning-based sampling strategies on publicly available, unannotated data sources. However, these methods often result in selected queries that lack task relevance and data diversity, leading to limited success in achieving satisfactory results with low query costs. In this paper, we propose MeaeQ (Model extraction attack with efficient Queries), a straightforward yet effective method to address these issues. Specifically, we initially utilize a zero-shot sequence inference classifier, combined with API service information, to filter task-relevant data from a public text corpus instead of a problem domain-specific dataset. Furthermore, we employ a clustering-based data reduction technique to obtain representative data as queries for the attack. Extensive experiments conducted on four benchmark datasets demonstrate that MeaeQ achieves higher functional similarity to the victim model than baselines while requiring fewer queries. Our code is available at https://github.com/C-W-D/MeaeQ.
</details>
<details>
<summary>摘要</summary>
我们研究模型EXTRACT attacks在自然语言处理（NLP）中，攻击者希望通过 repeatedly 访问公开的应用程序编程接口（API）来窃取受害者模型。最近的工作主要集中在有限Query 预算设置下，采用随机抽样或活动学习基于抽样策略在公共可用数据源上。然而，这些方法经常导致选择的查询缺乏任务相关性和数据多样性，从而导致寻求满意的结果需要较高的查询成本。在这篇论文中，我们提出 MeaeQ（模型EXTRACT攻击with高效查询），一种简单又有效的方法来解决这些问题。我们首先利用零拟合序列推理分类器，结合API服务信息，从公共文本资源中筛选任务相关的数据而不是具体领域特定的数据集。其次，我们使用聚类分析技术来减少数据，从而获得代表性的查询。我们对四个标准测试集进行了广泛的实验，结果显示，MeaeQ可以在 fewer 查询下达到更高的功能相似性，而与基eline 相比。我们的代码可以在 <https://github.com/C-W-D/MeaeQ> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Tree-Prompting-Efficient-Task-Adaptation-without-Fine-Tuning"><a href="#Tree-Prompting-Efficient-Task-Adaptation-without-Fine-Tuning" class="headerlink" title="Tree Prompting: Efficient Task Adaptation without Fine-Tuning"></a>Tree Prompting: Efficient Task Adaptation without Fine-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14034">http://arxiv.org/abs/2310.14034</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/csinva/treeprompt">https://github.com/csinva/treeprompt</a></li>
<li>paper_authors: John X. Morris, Chandan Singh, Alexander M. Rush, Jianfeng Gao, Yuntian Deng</li>
<li>for: 本文旨在提高小型语言模型（LM）的应用性，通过构建决策树来提高提问的精度。</li>
<li>methods: 本文提出了树提 prompting 方法，即在执行时，通过有效地路由上一步决策树的结果来确定下一步LM的调用。</li>
<li>results: 实验表明，树提 prompting 方法可以提高分类 dataset 的准确率，与Finetuning 相当，并且可以在各种任务上进行审查模型的决策过程。<details>
<summary>Abstract</summary>
Prompting language models (LMs) is the main interface for applying them to new tasks. However, for smaller LMs, prompting provides low accuracy compared to gradient-based finetuning. Tree Prompting is an approach to prompting which builds a decision tree of prompts, linking multiple LM calls together to solve a task. At inference time, each call to the LM is determined by efficiently routing the outcome of the previous call using the tree. Experiments on classification datasets show that Tree Prompting improves accuracy over competing methods and is competitive with fine-tuning. We also show that variants of Tree Prompting allow inspection of a model's decision-making process.
</details>
<details>
<summary>摘要</summary>
LMs的提示是主要用于应用它们到新任务中。然而，对于较小的LMs，提示提供的准确率相对较低，而梯度基于的finetuning则提供了更高的准确率。Tree Prompting是一种提示方法，它建立了一棵决策树，将多个LM调用串起来解决任务。在推理时，每次LM调用的结果都会根据树的结构有效地Routing到下一次调用。我们的实验表明，Tree Prompting可以提高分类 datasets 的准确率，并与finetuning竞争。此外，我们还示出了Tree Prompting的变体可以对模型决策过程进行检查。
</details></li>
</ul>
<hr>
<h2 id="Analysing-State-Backed-Propaganda-Websites-a-New-Dataset-and-Linguistic-Study"><a href="#Analysing-State-Backed-Propaganda-Websites-a-New-Dataset-and-Linguistic-Study" class="headerlink" title="Analysing State-Backed Propaganda Websites: a New Dataset and Linguistic Study"></a>Analysing State-Backed Propaganda Websites: a New Dataset and Linguistic Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14032">http://arxiv.org/abs/2310.14032</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gatenlp/wordpress-site-extractor">https://github.com/gatenlp/wordpress-site-extractor</a></li>
<li>paper_authors: Freddy Heppell, Kalina Bontcheva, Carolina Scarton</li>
<li>for: 这篇论文旨在分析两个以前没有研究过的国家支持的假新闻网站，即Reliable Recent News (rrn.world)和WarOnFakes (waronfakes.com)，这两个网站分别发布了多种语言的内容，包括阿拉伯语、中文、英语、法语、德语和西班牙语。</li>
<li>methods: 我们使用了内容获取方法和跨站无监督主题聚合方法来处理这些多语言数据集，并对网页翻译和时间分析进行语言和时间分析。</li>
<li>results: 我们的研究发现，这两个网站的内容具有较高的假新闻分布率，并且有一些文章的发布日期不准确。我们还发现了这些网站之间的语言和主题相似性，以及它们在时间上的变化。我们还公开发布了一个包含14,053篇文章的新数据集，每篇文章都有相应的语言版本和附加的元数据，如链接和图片。本研究对NLP社区的主要贡献在于提供了假新闻网站的新数据集，以及训练NLP工具 для假新闻检测。<details>
<summary>Abstract</summary>
This paper analyses two hitherto unstudied sites sharing state-backed disinformation, Reliable Recent News (rrn.world) and WarOnFakes (waronfakes.com), which publish content in Arabic, Chinese, English, French, German, and Spanish. We describe our content acquisition methodology and perform cross-site unsupervised topic clustering on the resulting multilingual dataset. We also perform linguistic and temporal analysis of the web page translations and topics over time, and investigate articles with false publication dates. We make publicly available this new dataset of 14,053 articles, annotated with each language version, and additional metadata such as links and images. The main contribution of this paper for the NLP community is in the novel dataset which enables studies of disinformation networks, and the training of NLP tools for disinformation detection.
</details>
<details>
<summary>摘要</summary>
这篇论文分析了两个未经研究的媒体站点，即可靠最新新闻（rrn.world）和战对假新闻（waronfakes.com），这两个站点都发布了多语言内容（阿拉伯语、中文、英语、法语、德语和西班牙语）。我们描述了我们的内容获取方法和跨站点无监督主题划分方法，并对多语言数据集进行语言和时间分析，以及文章发布日期的错误分析。我们公开发布了14,053篇文章，每篇文章都有相应的语言版本和附加元数据，如链接和图像。本文的主要贡献是提供了一个新的识别假新闻网络的数据集，以及训练NLP工具的机会。
</details></li>
</ul>
<hr>
<h2 id="LLM-Prop-Predicting-Physical-And-Electronic-Properties-Of-Crystalline-Solids-From-Their-Text-Descriptions"><a href="#LLM-Prop-Predicting-Physical-And-Electronic-Properties-Of-Crystalline-Solids-From-Their-Text-Descriptions" class="headerlink" title="LLM-Prop: Predicting Physical And Electronic Properties Of Crystalline Solids From Their Text Descriptions"></a>LLM-Prop: Predicting Physical And Electronic Properties Of Crystalline Solids From Their Text Descriptions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14029">http://arxiv.org/abs/2310.14029</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vertaix/llm-prop">https://github.com/vertaix/llm-prop</a></li>
<li>paper_authors: Andre Niyongabo Rubungo, Craig Arnold, Barry P. Rand, Adji Bousso Dieng</li>
<li>for: 这个论文旨在提出一种基于大语言模型（LLM）的方法，用于从晶体文本描述中预测晶体性质。</li>
<li>methods: 该方法使用大语言模型（LLM）来利用晶体文本描述来预测晶体的物理和电子性质。</li>
<li>results: 对比现有的graph neural network（GNN）方法，LLM-Prop方法在预测晶体带隙、是否直接带隙和晶体单元体积等性质上表现出较高的准确率。<details>
<summary>Abstract</summary>
The prediction of crystal properties plays a crucial role in the crystal design process. Current methods for predicting crystal properties focus on modeling crystal structures using graph neural networks (GNNs). Although GNNs are powerful, accurately modeling the complex interactions between atoms and molecules within a crystal remains a challenge. Surprisingly, predicting crystal properties from crystal text descriptions is understudied, despite the rich information and expressiveness that text data offer. One of the main reasons is the lack of publicly available data for this task. In this paper, we develop and make public a benchmark dataset (called TextEdge) that contains text descriptions of crystal structures with their properties. We then propose LLM-Prop, a method that leverages the general-purpose learning capabilities of large language models (LLMs) to predict the physical and electronic properties of crystals from their text descriptions. LLM-Prop outperforms the current state-of-the-art GNN-based crystal property predictor by about 4% in predicting band gap, 3% in classifying whether the band gap is direct or indirect, and 66% in predicting unit cell volume. LLM-Prop also outperforms a finetuned MatBERT, a domain-specific pre-trained BERT model, despite having 3 times fewer parameters. Our empirical results may highlight the current inability of GNNs to capture information pertaining to space group symmetry and Wyckoff sites for accurate crystal property prediction.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "The prediction of crystal properties plays a crucial role in the crystal design process. Current methods for predicting crystal properties focus on modeling crystal structures using graph neural networks (GNNs). Although GNNs are powerful, accurately modeling the complex interactions between atoms and molecules within a crystal remains a challenge. Surprisingly, predicting crystal properties from crystal text descriptions is understudied, despite the rich information and expressiveness that text data offer. One of the main reasons is the lack of publicly available data for this task. In this paper, we develop and make public a benchmark dataset (called TextEdge) that contains text descriptions of crystal structures with their properties. We then propose LLM-Prop, a method that leverages the general-purpose learning capabilities of large language models (LLMs) to predict the physical and electronic properties of crystals from their text descriptions. LLM-Prop outperforms the current state-of-the-art GNN-based crystal property predictor by about 4% in predicting band gap, 3% in classifying whether the band gap is direct or indirect, and 66% in predicting unit cell volume. LLM-Prop also outperforms a finetuned MatBERT, a domain-specific pre-trained BERT model, despite having 3 times fewer parameters. Our empirical results may highlight the current inability of GNNs to capture information pertaining to space group symmetry and Wyckoff sites for accurate crystal property prediction."中文翻译：<<SYS>>预测 кристаллических 属性在 кристалли设计过程中扮演关键角色。现有方法用图 neural networks（GNNs）模拟 кристалли结构，尽管 GNNs 强大，但是很难准确地模拟 кристалли中原子和分子之间复杂的交互。尽管预测 кристалли属性从 кристалли文本描述是未explored的，尽管文本数据具有丰富的信息和表达能力。一个主要的原因是该任务上没有公共可用的数据。在这篇论文中，我们开发了一个名为 TextEdge 的 referential dataset，该 dataset包含 кристалли结构的文本描述和其属性。我们然后提出了 LLM-Prop，一种利用大型自然语言模型（LLMs）预测 кристалли物理和电子属性的方法。LLM-Prop 在预测带隙、直接或间接带隙和单元积体积方面比现有状态 искусственный neural networks （GNNs） Based crystal property predictor 高于4%，高于3%在分类直接或间接带隙，和66%在预测单元积体积。LLM-Prop 还高于一个finetuned MatBERT，一种预先训练的 BERT 模型，尽管它有3倍少的参数。我们的实验结果可能高亮当前 GNNs 无法捕捉关于空群 симметрии和 Wyckoff 位置的信息，以便准确预测 кристалли属性。
</details></li>
</ul>
<hr>
<h2 id="GASCOM-Graph-based-Attentive-Semantic-Context-Modeling-for-Online-Conversation-Understanding"><a href="#GASCOM-Graph-based-Attentive-Semantic-Context-Modeling-for-Online-Conversation-Understanding" class="headerlink" title="GASCOM: Graph-based Attentive Semantic Context Modeling for Online Conversation Understanding"></a>GASCOM: Graph-based Attentive Semantic Context Modeling for Online Conversation Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14028">http://arxiv.org/abs/2310.14028</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vibhor Agarwal, Yu Chen, Nishanth Sastry</li>
<li>for: 这篇论文是为了提高在线对话理解的性能而写的。</li>
<li>methods: 这篇论文提出了一种基于图 estructure的注意力机制，使得可以更好地理解在线对话的含义。</li>
<li>results: 论文使用了两种新的算法，可以从整个对话树中提取有用的信息，并且使用多头Graph Attention Mechanism来进一步细化对话上的含义模型化。论文的实验结果表明，与现状比较，这种方法可以提高对话理解的性能，提高了对 polarity prediction 和 hate speech detection 的性能。<details>
<summary>Abstract</summary>
Online conversation understanding is an important yet challenging NLP problem which has many useful applications (e.g., hate speech detection). However, online conversations typically unfold over a series of posts and replies to those posts, forming a tree structure within which individual posts may refer to semantic context from higher up the tree. Such semantic cross-referencing makes it difficult to understand a single post by itself; yet considering the entire conversation tree is not only difficult to scale but can also be misleading as a single conversation may have several distinct threads or points, not all of which are relevant to the post being considered. In this paper, we propose a Graph-based Attentive Semantic COntext Modeling (GASCOM) framework for online conversation understanding. Specifically, we design two novel algorithms that utilise both the graph structure of the online conversation as well as the semantic information from individual posts for retrieving relevant context nodes from the whole conversation. We further design a token-level multi-head graph attention mechanism to pay different attentions to different tokens from different selected context utterances for fine-grained conversation context modeling. Using this semantic conversational context, we re-examine two well-studied problems: polarity prediction and hate speech detection. Our proposed framework significantly outperforms state-of-the-art methods on both tasks, improving macro-F1 scores by 4.5% for polarity prediction and by 5% for hate speech detection. The GASCOM context weights also enhance interpretability.
</details>
<details>
<summary>摘要</summary>
在线对话理解是一项重要又挑战性的自然语言处理（NLP）问题，它在许多应用中具有用于 hate speech detection 等应用。然而，在线对话通常是一串带有回快的帖子和回快中的帖子，组成一个树状结构，各个帖子可能引用上下文中的semantic context。这种 semantic cross-referencing 使得单个帖子难以理解，同时考虑整个对话树也不仅困难scaling，还可能导致偏导的解释，因为一个对话可能有多个不同的线索或焦点，其中不 todos 是 relevante para el post being considered。在这篇论文中，我们提出一个 Graph-based Attentive Semantic COntext Modeling（GASCOM）框架，用于在线对话理解。具体来说，我们设计了两种新的算法，利用在线对话的graph结构以及各个帖子的semantic信息，来选择 relevante context nodes from the whole conversation。此外，我们还设计了一个token-level multi-head graph attention mechanism，用于在不同的selected context utterances中进行细致的对话上下文模型化。使用这种 semantic conversational context，我们重新评估了两个已有的问题：polarity prediction和 hate speech detection。我们的提议的框架在两个任务上显著超越了当前的状态方法，提高了macro-F1分数 by 4.5% for polarity prediction和by 5% for hate speech detection。GASCOM上下文权重也提高了解释性。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-and-Multimodal-Retrieval-for-Visual-Word-Sense-Disambiguation"><a href="#Large-Language-Models-and-Multimodal-Retrieval-for-Visual-Word-Sense-Disambiguation" class="headerlink" title="Large Language Models and Multimodal Retrieval for Visual Word Sense Disambiguation"></a>Large Language Models and Multimodal Retrieval for Visual Word Sense Disambiguation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14025">http://arxiv.org/abs/2310.14025</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anastasiakrith/multimodal-retrieval-for-vwsd">https://github.com/anastasiakrith/multimodal-retrieval-for-vwsd</a></li>
<li>paper_authors: Anastasia Kritharoula, Maria Lymperaiou, Giorgos Stamou</li>
<li>for: 本文主要针对的是解决文本含义歧义的图像检索任务，即Visual Word Sense Disambiguation (VWSD)。</li>
<li>methods: 本文采用了多种方法，包括最新的transformer-based方法和大自然语言模型（LLMs）来解决VWSD任务。</li>
<li>results:  experiments表明，我们的方法可以在VWSD任务中达到竞争性的排名结果，并且通过Chain-of-Thought（CoT）提示来帮助解释answer生成。<details>
<summary>Abstract</summary>
Visual Word Sense Disambiguation (VWSD) is a novel challenging task with the goal of retrieving an image among a set of candidates, which better represents the meaning of an ambiguous word within a given context. In this paper, we make a substantial step towards unveiling this interesting task by applying a varying set of approaches. Since VWSD is primarily a text-image retrieval task, we explore the latest transformer-based methods for multimodal retrieval. Additionally, we utilize Large Language Models (LLMs) as knowledge bases to enhance the given phrases and resolve ambiguity related to the target word. We also study VWSD as a unimodal problem by converting to text-to-text and image-to-image retrieval, as well as question-answering (QA), to fully explore the capabilities of relevant models. To tap into the implicit knowledge of LLMs, we experiment with Chain-of-Thought (CoT) prompting to guide explainable answer generation. On top of all, we train a learn to rank (LTR) model in order to combine our different modules, achieving competitive ranking results. Extensive experiments on VWSD demonstrate valuable insights to effectively drive future directions.
</details>
<details>
<summary>摘要</summary>
Visible Word Sense Disambiguation (VWSD) 是一个新型的挑战性任务，旨在从一组候选者中选取一幅图像，更好地表现出一个模糊词的意思在特定上下文中。在这篇文章中，我们做出了一个重要的进步，通过应用不同的方法来解决这个有趣的任务。由于 VWSD 主要是文本-图像搜寻任务，我们探索了最新的 transformer-based 方法来进行多modal搜寻。此外，我们使用 Large Language Models (LLMs) 来增强给定的短语，解决对目标词的模糊性。我们还研究了 VWSD 作为单modal问题，通过将它转换为文本-文本和图像-图像搜寻，以及问答 (QA)，以全面探索相关模型的能力。为了吸取 LLMS 的隐藏知识，我们尝试使用 Chain-of-Thought (CoT) 提示来引导可解释的答案生成。最后，我们将多个模组联合起来，使用 learn to rank (LTR) 模型进行排名， achieving 竞争性的排名结果。广泛的实验在 VWSD 中，提供了宝贵的见解，将未来发展领域带向更好的未来。
</details></li>
</ul>
<hr>
<h2 id="Toward-Stronger-Textual-Attack-Detectors"><a href="#Toward-Stronger-Textual-Attack-Detectors" class="headerlink" title="Toward Stronger Textual Attack Detectors"></a>Toward Stronger Textual Attack Detectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14001">http://arxiv.org/abs/2310.14001</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pierrecolombo/adversarialattacksnlp">https://github.com/pierrecolombo/adversarialattacksnlp</a></li>
<li>paper_authors: Pierre Colombo, Marine Picot, Nathan Noiry, Guillaume Staerman, Pablo Piantanida</li>
<li>for: 防止文本敌对攻击，即使是深度NLP系统。</li>
<li>methods: 引入了一个新的检测敌对攻击的框架——LAROUSSE，以及一个新的评价板准——STAKEOUT。</li>
<li>results: LAROUSSE比前一代方法更高效，并且可以防止梯度基本法。<details>
<summary>Abstract</summary>
The landscape of available textual adversarial attacks keeps growing, posing severe threats and raising concerns regarding the deep NLP system's integrity. However, the crucial problem of defending against malicious attacks has only drawn the attention of the NLP community. The latter is nonetheless instrumental in developing robust and trustworthy systems. This paper makes two important contributions in this line of search: (i) we introduce LAROUSSE, a new framework to detect textual adversarial attacks and (ii) we introduce STAKEOUT, a new benchmark composed of nine popular attack methods, three datasets, and two pre-trained models. LAROUSSE is ready-to-use in production as it is unsupervised, hyperparameter-free, and non-differentiable, protecting it against gradient-based methods. Our new benchmark STAKEOUT allows for a robust evaluation framework: we conduct extensive numerical experiments which demonstrate that LAROUSSE outperforms previous methods, and which allows to identify interesting factors of detection rate variations.
</details>
<details>
<summary>摘要</summary>
文章做出了两个重要贡献：首先，我们引入了一个新的检测文本针对攻击的框架，即LAROUSSE，它是不需要监督学习、无参数、不可导的，因此具有更高的安全性。其次，我们引入了一个新的评估框架，即STAKEOUT，它包括9种常见的攻击方法、3个数据集和2个预训练模型。我们的新框架允许更加稳定地评估检测效果，我们进行了广泛的数字实验，结果表明LAROUSSE在前一代方法之上出众，同时还可以识别检测率的变化因素。
</details></li>
</ul>
<hr>
<h2 id="Transductive-Learning-for-Textual-Few-Shot-Classification-in-API-based-Embedding-Models"><a href="#Transductive-Learning-for-Textual-Few-Shot-Classification-in-API-based-Embedding-Models" class="headerlink" title="Transductive Learning for Textual Few-Shot Classification in API-based Embedding Models"></a>Transductive Learning for Textual Few-Shot Classification in API-based Embedding Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13998">http://arxiv.org/abs/2310.13998</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Colombo, Victor Pellegrain, Malik Boudiaf, Victor Storchan, Myriam Tami, Ismail Ben Ayed, Celine Hudelot, Pablo Piantanida</li>
<li>for:  This paper focuses on the practical applications of natural language processing, specifically few-shot classification, and addresses the issue of proprietary and closed APIs.</li>
<li>methods: The paper proposes a transductive inference learning paradigm that utilizes unlabeled data, along with a new parameter-free transductive regularizer based on the Fisher-Rao loss.</li>
<li>results: The paper presents experimental results using eight backbone models and an episodic evaluation over 1,000 episodes, which demonstrate the superiority of transductive inference over the standard inductive setting.Here is the result in Simplified Chinese text:</li>
<li>for: 本文关注自然语言处理的实际应用，具体是几个shot类型的分类，并解决了 propriety 和关闭 API 的问题。</li>
<li>methods: 本文提出了一种推uctive推理学习模式，利用无标注数据，并提出了一种无参数的推uctive规范基于 Fisher-Rao 损失。</li>
<li>results: 本文通过使用 eight 个基础模型和一千个 episodic 评估，展示了推uctive推理在标准 inductive 设定下的超越。<details>
<summary>Abstract</summary>
Proprietary and closed APIs are becoming increasingly common to process natural language, and are impacting the practical applications of natural language processing, including few-shot classification. Few-shot classification involves training a model to perform a new classification task with a handful of labeled data. This paper presents three contributions. First, we introduce a scenario where the embedding of a pre-trained model is served through a gated API with compute-cost and data-privacy constraints. Second, we propose a transductive inference, a learning paradigm that has been overlooked by the NLP community. Transductive inference, unlike traditional inductive learning, leverages the statistics of unlabeled data. We also introduce a new parameter-free transductive regularizer based on the Fisher-Rao loss, which can be used on top of the gated API embeddings. This method fully utilizes unlabeled data, does not share any label with the third-party API provider and could serve as a baseline for future research. Third, we propose an improved experimental setting and compile a benchmark of eight datasets involving multiclass classification in four different languages, with up to 151 classes. We evaluate our methods using eight backbone models, along with an episodic evaluation over 1,000 episodes, which demonstrate the superiority of transductive inference over the standard inductive setting.
</details>
<details>
<summary>摘要</summary>
专有和关闭API在处理自然语言方面变得越来越普遍，这对自然语言处理的实际应用产生了影响，包括几个shot分类。本文提出了三个贡献。首先，我们介绍了一种情况，在这种情况下，一个预训练模型的 embedding 被通过一个限制 compute-cost 和数据隐私的 gat API 提供。第二，我们提议了一种被 NLP 社区忽视的学习模式——推uctive inference。推uctive inference 不同于传统的 inductive learning，它利用不标注数据的统计特性。我们还介绍了一个新的参数-free 推uctive regularizer，基于 Fisher-Rao 损失函数，可以在gat API 中使用。这种方法可以完全利用无标注数据，不需要与第三方 API 提供者共享标签，并且可以作为未来研究的基准。第三，我们提出了一个改进的实验设定，并编译了八个dataset，包括四种语言，最多 151 个分类。我们使用八种背部bone模型进行评估，并在1,000个 episodic 评估中，发现推uctive inference 在标准 inductive 设定下表现出优异性。
</details></li>
</ul>
<hr>
<h2 id="Emulating-the-Human-Mind-A-Neural-symbolic-Link-Prediction-Model-with-Fast-and-Slow-Reasoning-and-Filtered-Rules"><a href="#Emulating-the-Human-Mind-A-Neural-symbolic-Link-Prediction-Model-with-Fast-and-Slow-Reasoning-and-Filtered-Rules" class="headerlink" title="Emulating the Human Mind: A Neural-symbolic Link Prediction Model with Fast and Slow Reasoning and Filtered Rules"></a>Emulating the Human Mind: A Neural-symbolic Link Prediction Model with Fast and Slow Reasoning and Filtered Rules</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13996">http://arxiv.org/abs/2310.13996</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Hossein Khojasteh, Najmeh Torabian, Ali Farjami, Saeid Hosseini, Behrouz Minaei-Bidgoli</li>
<li>for: 这个研究的目的是解决知识граフ（KG）中的不完整性问题，并提出了一个新的神经几何模型named FaSt-FLiP，以提高链接预测的性能和可解性。</li>
<li>methods: 这个模型使用了“常识推理”和“快速思考”两种人类认知方面的特点，并结合了逻辑和神经网络模型，以提高链接预测的精度和可解性。</li>
<li>results: 研究结果显示，FaSt-FLiP模型在链接预测中的表现较高，并能够提供更可靠的解释。另外，模型还能够自动检测和删除逻辑模型生成的错误规则。<details>
<summary>Abstract</summary>
Link prediction is an important task in addressing the incompleteness problem of knowledge graphs (KG). Previous link prediction models suffer from issues related to either performance or explanatory capability. Furthermore, models that are capable of generating explanations, often struggle with erroneous paths or reasoning leading to the correct answer. To address these challenges, we introduce a novel Neural-Symbolic model named FaSt-FLiP (stands for Fast and Slow Thinking with Filtered rules for Link Prediction task), inspired by two distinct aspects of human cognition: "commonsense reasoning" and "thinking, fast and slow." Our objective is to combine a logical and neural model for enhanced link prediction. To tackle the challenge of dealing with incorrect paths or rules generated by the logical model, we propose a semi-supervised method to convert rules into sentences. These sentences are then subjected to assessment and removal of incorrect rules using an NLI (Natural Language Inference) model. Our approach to combining logical and neural models involves first obtaining answers from both the logical and neural models. These answers are subsequently unified using an Inference Engine module, which has been realized through both algorithmic implementation and a novel neural model architecture. To validate the efficacy of our model, we conducted a series of experiments. The results demonstrate the superior performance of our model in both link prediction metrics and the generation of more reliable explanations.
</details>
<details>
<summary>摘要</summary>
链接预测是知识 graphs（KG）的重要任务，以解决知识 Graphs 的不完整性问题。现有的链接预测模型受到性能和可解释能力的限制。而且，可以生成解释的模型经常会遇到错误的路径或理由，导致正确答案。为 Addressing these challenges, we propose a novel Neural-Symbolic model named FaSt-FLiP（快速思维与筛选规则 для链接预测任务）， drawing inspiration from two aspects of human cognition："通常的思维"和"快速和慢速的思考。our objective is to combine a logical and neural model for enhanced link prediction. To tackle the challenge of dealing with incorrect paths or rules generated by the logical model, we propose a semi-supervised method to convert rules into sentences. These sentences are then subjected to assessment and removal of incorrect rules using an NLI（自然语言推理）model. Our approach to combining logical and neural models involves first obtaining answers from both the logical and neural models. These answers are subsequently unified using an Inference Engine module, which has been realized through both algorithmic implementation and a novel neural model architecture. To validate the efficacy of our model, we conducted a series of experiments. The results demonstrate the superior performance of our model in both link prediction metrics and the generation of more reliable explanations.
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Information-Theoretic-Objective-to-Disentangle-Representations-for-Fair-Classification"><a href="#A-Novel-Information-Theoretic-Objective-to-Disentangle-Representations-for-Fair-Classification" class="headerlink" title="A Novel Information-Theoretic Objective to Disentangle Representations for Fair Classification"></a>A Novel Information-Theoretic Objective to Disentangle Representations for Fair Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13990">http://arxiv.org/abs/2310.13990</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Colombo, Nathan Noiry, Guillaume Staerman, Pablo Piantanida</li>
<li>for: The paper aims to learn abstract representations of reality from the observation of multiple contextual situations, specifically disentangled representations that are low-dimensional and independent of sensitive attributes such as gender or age.</li>
<li>methods: The paper proposes a novel family of regularizers called CLINIC, which minimizes the mutual information between the latent representation and the sensitive attribute conditional to the target. This approach is parameter-free and easier to train than previous techniques.</li>
<li>results: The paper demonstrates that the proposed CLINIC losses offer a better disentanglement&#x2F;accuracy trade-off than previous techniques and generalize better than training with cross-entropy loss, provided that the disentanglement task is not too constraining.Here is the simplified Chinese version of the three key information points:</li>
<li>for: 该论文目标是从多个 contextual situations 中学习抽象的 reality 表示，具体来说是EXTRACT 独立的表示，即低维度且独立的概念表示。</li>
<li>methods: 该论文提出一种新的 family of regularizers called CLINIC，该regularizers 的目标是将敏感特征（如性别或年龄）与 latent representation 的相关性降低到最低。这种方法是 parameter-free 的， easier 和 faster than previous techniques。</li>
<li>results: 该论文的实验结果显示，提出的 CLINIC losses 可以比 previous techniques 提供更好的 disentanglement&#x2F;accuracy 的负荷平衡，并且在不太紧张的disentanglement task 下可以更好地泛化。<details>
<summary>Abstract</summary>
One of the pursued objectives of deep learning is to provide tools that learn abstract representations of reality from the observation of multiple contextual situations. More precisely, one wishes to extract disentangled representations which are (i) low dimensional and (ii) whose components are independent and correspond to concepts capturing the essence of the objects under consideration (Locatello et al., 2019b). One step towards this ambitious project consists in learning disentangled representations with respect to a predefined (sensitive) attribute, e.g., the gender or age of the writer. Perhaps one of the main application for such disentangled representations is fair classification. Existing methods extract the last layer of a neural network trained with a loss that is composed of a cross-entropy objective and a disentanglement regularizer. In this work, we adopt an information-theoretic view of this problem which motivates a novel family of regularizers that minimizes the mutual information between the latent representation and the sensitive attribute conditional to the target. The resulting set of losses, called CLINIC, is parameter free and thus, it is easier and faster to train. CLINIC losses are studied through extensive numerical experiments by training over 2k neural networks. We demonstrate that our methods offer a better disentanglement/accuracy trade-off than previous techniques, and generalize better than training with cross-entropy loss solely provided that the disentanglement task is not too constraining.
</details>
<details>
<summary>摘要</summary>
一个深度学习的核心目标是提供能够从多个上下文中学习抽象的现实表示的工具。更具体地说，希望从多个上下文中提取独立的表示，其维度低、并且其组成部分独立、对象本身的核心特征capturing (Locatello et al., 2019b).一种实现这个奢侈目标的方法是通过对敏感特征（如作者的性别或年龄）进行分离表示学习。可能这种独立表示的主要应用是公平分类。现有方法通常是提取一个通过混合Entropy目标和分离regularizer进行训练的神经网络的最后一层。在这个工作中，我们采用信息论视角来解决这个问题，并提出了一个新的家族征识器，它将 conditional于目标进行抽象的mutual information minimization。这些loss Functions被称为CLINIC，它们是无参数的，因此更容易和更快地训练。我们通过对2k个神经网络进行数字实验来研究CLINIC loss。我们示示了我们的方法可以在训练过程中提供更好的抽象/准确性质和平衡，并且在训练cross-entropy loss solo不可能达到的情况下，能够更好地泛化。
</details></li>
</ul>
<hr>
<h2 id="GEMBA-MQM-Detecting-Translation-Quality-Error-Spans-with-GPT-4"><a href="#GEMBA-MQM-Detecting-Translation-Quality-Error-Spans-with-GPT-4" class="headerlink" title="GEMBA-MQM: Detecting Translation Quality Error Spans with GPT-4"></a>GEMBA-MQM: Detecting Translation Quality Error Spans with GPT-4</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13988">http://arxiv.org/abs/2310.13988</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tom Kocmi, Christian Federmann</li>
<li>for: 评估翻译质量错误</li>
<li>methods: 使用GPT模型，三次预览技术，无需人工参考翻译</li>
<li>results: 达到系统排名的最佳准确率，但需要注意GPT模型的商业性和黑盒特性，不建议用于学术论文中证明其他方法的改进。<details>
<summary>Abstract</summary>
This paper introduces GEMBA-MQM, a GPT-based evaluation metric designed to detect translation quality errors, specifically for the quality estimation setting without the need for human reference translations. Based on the power of large language models (LLM), GEMBA-MQM employs a fixed three-shot prompting technique, querying the GPT-4 model to mark error quality spans. Compared to previous works, our method has language-agnostic prompts, thus avoiding the need for manual prompt preparation for new languages.   While preliminary results indicate that GEMBA-MQM achieves state-of-the-art accuracy for system ranking, we advise caution when using it in academic works to demonstrate improvements over other methods due to its dependence on the proprietary, black-box GPT model.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了GEMBA-MQM，一种基于GPT的评估指标，用于探测翻译质量错误。这种指标不需要人工参考翻译，可以在质量估计设置中使用。通过大语言模型（LLM）的力量，GEMBA-MQM使用固定的三批提问技术，让GPT-4模型标识错误质量段落。与前一代方法相比，我们的方法有语言共享的提问，因此不需要为新语言手动准备提问。然而，我们建议在学术论文中使用GEMBA-MQM时需要谨慎，因为它取决于商业化、黑盒子GPT模型，这可能会导致使用它来证明其他方法的优越性的问题。
</details></li>
</ul>
<hr>
<h2 id="HateRephrase-Zero-and-Few-Shot-Reduction-of-Hate-Intensity-in-Online-Posts-using-Large-Language-Models"><a href="#HateRephrase-Zero-and-Few-Shot-Reduction-of-Hate-Intensity-in-Online-Posts-using-Large-Language-Models" class="headerlink" title="HateRephrase: Zero- and Few-Shot Reduction of Hate Intensity in Online Posts using Large Language Models"></a>HateRephrase: Zero- and Few-Shot Reduction of Hate Intensity in Online Posts using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13985">http://arxiv.org/abs/2310.13985</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vibhor Agarwal, Yu Chen, Nishanth Sastry</li>
<li>for: 这篇论文旨在提出一种新的、简单有效的方法，即在发布前对潜在仇恨言论内容进行重写。</li>
<li>methods: 这篇论文使用了大语言模型（LLMs），并比较了不同的提示方法，包括任务描述、仇Definition、少量示例和思维链。</li>
<li>results: 研究发现，使用几shot示例提示的LLMs最perform最好，并且在不同的提示方法下都有良好的表现。而且，对于不同的提示方法，GPT-3.5表现最佳。此外，人工评估表明，GPT-3.5生成的重写内容甚至超过了人类生成的ground truth重写内容。<details>
<summary>Abstract</summary>
Hate speech has become pervasive in today's digital age. Although there has been considerable research to detect hate speech or generate counter speech to combat hateful views, these approaches still cannot completely eliminate the potential harmful societal consequences of hate speech -- hate speech, even when detected, can often not be taken down or is often not taken down enough; and hate speech unfortunately spreads quickly, often much faster than any generated counter speech.   This paper investigates a relatively new yet simple and effective approach of suggesting a rephrasing of potential hate speech content even before the post is made. We show that Large Language Models (LLMs) perform well on this task, outperforming state-of-the-art baselines such as BART-Detox. We develop 4 different prompts based on task description, hate definition, few-shot demonstrations and chain-of-thoughts for comprehensive experiments and conduct experiments on open-source LLMs such as LLaMA-1, LLaMA-2 chat, Vicuna as well as OpenAI's GPT-3.5. We propose various evaluation metrics to measure the efficacy of the generated text and ensure the generated text has reduced hate intensity without drastically changing the semantic meaning of the original text.   We find that LLMs with a few-shot demonstrations prompt work the best in generating acceptable hate-rephrased text with semantic meaning similar to the original text. Overall, we find that GPT-3.5 outperforms the baseline and open-source models for all the different kinds of prompts. We also perform human evaluations and interestingly, find that the rephrasings generated by GPT-3.5 outperform even the human-generated ground-truth rephrasings in the dataset. We also conduct detailed ablation studies to investigate why LLMs work satisfactorily on this task and conduct a failure analysis to understand the gaps.
</details>
<details>
<summary>摘要</summary>
仇恨言语在当今数字时代已经成为普遍存在的问题。尽管有很多研究检测仇恨言语或生成对抗仇恨观点的counter speech，但这些方法仍然无法完全消除仇恨言语的社会后果 -- 仇恨言语，即使检测到了，通常不能或很难被移除，而且仇恨言语往往很快就会扩散，常常比生成的counter speech更快。这篇论文研究了一种新的、简单而有效的方法，即在投稿之前，使用大语言模型（LLMs）来提议修改潜在的仇恨言语内容。我们证明了LLMs在这个任务上表现良好，超过了现有的基线模型如BART-Detox。我们开发了4个不同的提示，基于任务描述、仇Definition、几个示例和串联思维，进行了广泛的实验。我们使用了开源的LLaMA-1、LLaMA-2 chat、Vicuna以及OpenAI的GPT-3.5等模型进行实验。我们提出了多种评价指标，以确保生成的文本减少了仇恨程度，而不会毁灭语意。我们发现，使用几个示例提示的LLMs最为有效，能够生成接受的仇恨重新写文本， semantic meaning与原文相似。总之，我们发现GPT-3.5在所有不同的提示上都超过了基线和开源模型。我们还进行了人工评价， Interestingly，我们发现GPT-3.5生成的重新写文本甚至超过了人工生成的基准重新写文本。我们还进行了细化的抽象研究和失败分析，以解释LLMs在这个任务上的成功原因。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Pronunciation-Assessment-–-A-Review"><a href="#Automatic-Pronunciation-Assessment-–-A-Review" class="headerlink" title="Automatic Pronunciation Assessment – A Review"></a>Automatic Pronunciation Assessment – A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13974">http://arxiv.org/abs/2310.13974</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yassine El Kheir, Ahmed Ali, Shammur Absar Chowdhury</li>
<li>for: 这篇论文主要是为了探讨计算机辅助发音训练（CAPT）中的发音评估方法和其应用。</li>
<li>methods: 这篇论文评论了在发音评估方面使用的方法，包括phonemic和prosodic两种方法。</li>
<li>results: 论文认为，现有的研究存在一些挑战和局限性，并提出了未来研究的可能性。<details>
<summary>Abstract</summary>
Pronunciation assessment and its application in computer-aided pronunciation training (CAPT) have seen impressive progress in recent years. With the rapid growth in language processing and deep learning over the past few years, there is a need for an updated review. In this paper, we review methods employed in pronunciation assessment for both phonemic and prosodic. We categorize the main challenges observed in prominent research trends, and highlight existing limitations, and available resources. This is followed by a discussion of the remaining challenges and possible directions for future work.
</details>
<details>
<summary>摘要</summary>
声音评估和计算机辅助声音训练（CAPT）在最近几年中得到了很大的进步。随着语言处理和深度学习技术的快速发展，需要进行更新的评估。本文介绍了声音评估中使用的方法，包括音节和语言流行的评估。我们分类了主要的研究趋势中的挑战，并高亮现有的限制和可用资源。然后是对未来工作的残余挑战和可能的方向的讨论。Note: Simplified Chinese is the standard writing system used in mainland China, while Traditional Chinese is used in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="AITA-Generating-Moral-Judgements-of-the-Crowd-with-Reasoning"><a href="#AITA-Generating-Moral-Judgements-of-the-Crowd-with-Reasoning" class="headerlink" title="AITA Generating Moral Judgements of the Crowd with Reasoning"></a>AITA Generating Moral Judgements of the Crowd with Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18336">http://arxiv.org/abs/2310.18336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Osama Bsher, Ameer Sabri<br>for: This paper aims to generate comments with moral reasoning for stories with moral dilemmas using the AITA subreddit as a dataset.methods: The authors will leverage the vast amount of data on the forum and use state-of-the-art seq2seq text generation models to generate coherent comments that align with the norms and values of the AITA community.results: The authors aim to evaluate the ability of these models to make moral judgments similarly to humans and produce concise comments providing clear moral stances and advice for the poster.<details>
<summary>Abstract</summary>
Morality is a fundamental aspect of human behavior and ethics, influencing how we interact with each other and the world around us. When faced with a moral dilemma, a person's ability to make clear moral judgments can be clouded. Due to many factors such as personal biases, emotions and situational factors people can find it difficult to decide their best course of action. The AmITheAsshole (AITA) subreddit is a forum on the social media platform Reddit that helps people get clarity and objectivity on their predicaments. In the forum people post anecdotes about moral dilemmas they are facing in their lives, seeking validation for their actions or advice on how to navigate the situation from the community. The morality of the actions in each post is classified based on the collective opinion of the community into mainly two labels, "Not The Asshole" (NTA) and "You Are The Asshole" (YTA). This project aims to generate comments with moral reasoning for stories with moral dilemmas using the AITA subreddit as a dataset. While past literature has explored the classification of posts into labels (Alhassan et al., 2022), the generation of comments remains a novel and challenging task. It involves understanding the complex social and ethical considerations in each situation. To address this challenge, we will leverage the vast amount of data on the forum with the goal of generating coherent comments that align with the norms and values of the AITA community. In this endeavor, we aim to evaluate state-of-the-art seq2seq text generation models for their ability to make moral judgments similarly to humans, ultimately producing concise comments providing clear moral stances and advice for the poster.
</details>
<details>
<summary>摘要</summary>
人类行为中的道德是一个基本方面，影响我们如何与其他人和世界around us interact。当面临道德困难时，人们可能会受到个人偏见、情感和情况因素的影响，导致困难做出明确的道德判断。为了帮助人们得到清晰性和 объекivity，Reddit上的AmITheAsshole（AITA）子社区成为了一个有用的平台。在这个社区中，人们会分享他们面临的道德困难，并请求 validation for their actions或对 Situation 的 Navigation 建议。根据社区的共同意见，每篇文章将被分类为主要两个标签：“Not The Asshole”（NTA）和“You Are The Asshole”（YTA）。这个项目的目标是使用 AITA 子社区的数据生成文章中的评论，以提供清晰的道德观点和建议。在这个任务中，我们将利用社区数据的庞大量，以生成一致的评论，与 AITA 社区的 norms 和价值观念相一致。为了解决这个挑战，我们将使用现代 seq2seq 文本生成模型，以模拟人类的道德判断能力。最终，我们希望通过生成简洁明了的评论，为poster提供清晰的道德观点和建议。
</details></li>
</ul>
<hr>
<h2 id="Linguistically-Motivated-Sign-Language-Segmentation"><a href="#Linguistically-Motivated-Sign-Language-Segmentation" class="headerlink" title="Linguistically Motivated Sign Language Segmentation"></a>Linguistically Motivated Sign Language Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13960">http://arxiv.org/abs/2310.13960</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sign-language-processing/transcription">https://github.com/sign-language-processing/transcription</a></li>
<li>paper_authors: Amit Moryossef, Zifan Jiang, Mathias Müller, Sarah Ebling, Yoav Goldberg</li>
<li>for: 本文旨在提出一种新的手语分割方法，以便在手语处理系统中进行下游任务，如手语识别、 транскриpción和机器翻译。</li>
<li>methods: 我们提出的方法基于手语 corpora 中观察到的语言学 clue，使用 BIO 标记替换传统的 IO 标记，以考虑手语的连续性。我们还explore使用光流特征来捕捉手语语法的 просодические特征。</li>
<li>results: 我们发现，使用 BIO 标记可以更好地模型手语 bounding box。在某些深度模型中，使用光流特征可以提高分割质量，但在更深的模型中，这些特征的贡献几乎可以忽略不计。通过精心调整模型的解码算法，我们可以进一步提高分割质量。我们的最终模型可以在不同的手语语言下进行零基础学习，并在不同的视频内容上进行渠道。<details>
<summary>Abstract</summary>
Sign language segmentation is a crucial task in sign language processing systems. It enables downstream tasks such as sign recognition, transcription, and machine translation. In this work, we consider two kinds of segmentation: segmentation into individual signs and segmentation into phrases, larger units comprising several signs. We propose a novel approach to jointly model these two tasks.   Our method is motivated by linguistic cues observed in sign language corpora. We replace the predominant IO tagging scheme with BIO tagging to account for continuous signing. Given that prosody plays a significant role in phrase boundaries, we explore the use of optical flow features. We also provide an extensive analysis of hand shapes and 3D hand normalization.   We find that introducing BIO tagging is necessary to model sign boundaries. Explicitly encoding prosody by optical flow improves segmentation in shallow models, but its contribution is negligible in deeper models. Careful tuning of the decoding algorithm atop the models further improves the segmentation quality.   We demonstrate that our final models generalize to out-of-domain video content in a different signed language, even under a zero-shot setting. We observe that including optical flow and 3D hand normalization enhances the robustness of the model in this context.
</details>
<details>
<summary>摘要</summary>
签语段落是指文本处理系统中的签语分割任务。它可以帮助下游任务，如签语识别、转写和机器翻译。在这个工作中，我们考虑了两种类型的分割：即分割成个体签语和分割成短语，后者是由多个签语组成的更大单位。我们提出了一种新的方法，旨在同时解决这两种任务。  我们的方法受到了签语 Corpora 中的语言学cue的启发。我们将主流的 IO 标记方案改为 BIO 标记，以考虑不间断的签语。由于语言的气息在短语边界上发挥重要作用，我们尝试使用光流特征。我们还提供了详细的手势分析和3D手势normalization。  我们发现，使用 BIO 标记是必要的，以便模型签语边界。使用光流特征可以在浅度模型中提高分割质量，但在深度模型中，其贡献几乎可以忽略不计。通过精细调整模型顶部的解码算法，可以进一步提高分割质量。  我们展示了我们的最终模型可以在不同的指语言中进行零基础学习，并在无预训练情况下保持良好的分割质量。我们发现，包含光流和3D手势normalization可以提高模型在这种情况下的Robustness。
</details></li>
</ul>
<hr>
<h2 id="Values-Ethics-Morals-On-the-Use-of-Moral-Concepts-in-NLP-Research"><a href="#Values-Ethics-Morals-On-the-Use-of-Moral-Concepts-in-NLP-Research" class="headerlink" title="Values, Ethics, Morals? On the Use of Moral Concepts in NLP Research"></a>Values, Ethics, Morals? On the Use of Moral Concepts in NLP Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13915">http://arxiv.org/abs/2310.13915</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karina Vida, Judith Simon, Anne Lauscher</li>
<li>for: 本研究旨在探讨NLPT中的伦理问题，尤其是语言模型的道德评价。</li>
<li>methods: 本研究使用文献综述和系统性分析方法，探讨NLPT中 morality 的定义和基础。</li>
<li>results: 研究发现，大多数文献没有提供明确的定义，也没有遵循哲学定义。此外，研究还给出了三个建议，以促进NLPT中的道德讨论。<details>
<summary>Abstract</summary>
With language technology increasingly affecting individuals' lives, many recent works have investigated the ethical aspects of NLP. Among other topics, researchers focused on the notion of morality, investigating, for example, which moral judgements language models make. However, there has been little to no discussion of the terminology and the theories underpinning those efforts and their implications. This lack is highly problematic, as it hides the works' underlying assumptions and hinders a thorough and targeted scientific debate of morality in NLP. In this work, we address this research gap by (a) providing an overview of some important ethical concepts stemming from philosophy and (b) systematically surveying the existing literature on moral NLP w.r.t. their philosophical foundation, terminology, and data basis. For instance, we analyse what ethical theory an approach is based on, how this decision is justified, and what implications it entails. Our findings surveying 92 papers show that, for instance, most papers neither provide a clear definition of the terms they use nor adhere to definitions from philosophy. Finally, (c) we give three recommendations for future research in the field. We hope our work will lead to a more informed, careful, and sound discussion of morality in language technology.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Providing an overview of important ethical concepts from philosophy.2. Systematically surveying the existing literature on moral NLP, examining their philosophical foundations, terminology, and data basis.3. Offering three recommendations for future research in the field.Our survey of 92 papers found that most papers do not provide clear definitions of the terms they use, and few adhere to definitions from philosophy. We hope that our work will contribute to a more informed, careful, and sound discussion of morality in language technology.</details></li>
</ol>
<hr>
<h2 id="RTSUM-Relation-Triple-based-Interpretable-Summarization-with-Multi-level-Salience-Visualization"><a href="#RTSUM-Relation-Triple-based-Interpretable-Summarization-with-Multi-level-Salience-Visualization" class="headerlink" title="RTSUM: Relation Triple-based Interpretable Summarization with Multi-level Salience Visualization"></a>RTSUM: Relation Triple-based Interpretable Summarization with Multi-level Salience Visualization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13895">http://arxiv.org/abs/2310.13895</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sjyyj/sjyyj">https://github.com/sjyyj/sjyyj</a></li>
<li>paper_authors: Seonglae Cho, Yonggi Cho, HoonJae Lee, Myungha Jang, Jinyoung Yeo, Dongha Lee</li>
<li>for: 本文提出了一种无监督概要框架，利用关系 triplets 为概要的基本单元。</li>
<li>methods: 输入文档后，本方法首先选择了突出的关系 triplets via 多级评分，然后通过文本-文本语言模型生成了简洁的概要。</li>
<li>results: 基于本方法，我们还开发了一款可解释性概要工具，提供了细致的解释与输出概要。用户可以自定义选择不同的级别，以便在文本单元层次上visual化文本的重要性。代码公开available。<details>
<summary>Abstract</summary>
In this paper, we present RTSUM, an unsupervised summarization framework that utilizes relation triples as the basic unit for summarization. Given an input document, RTSUM first selects salient relation triples via multi-level salience scoring and then generates a concise summary from the selected relation triples by using a text-to-text language model. On the basis of RTSUM, we also develop a web demo for an interpretable summarizing tool, providing fine-grained interpretations with the output summary. With support for customization options, our tool visualizes the salience for textual units at three distinct levels: sentences, relation triples, and phrases. The codes,are publicly available.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种无监督摘要框架，称为RTSUM，它利用关系三元组作为摘要的基本单元。给定输入文档，RTSUM首先选择了突出的关系三元组via多级重要性分数，然后使用文本到文本语言模型生成了一份简洁的摘要。基于RTSUM，我们还开发了一个可视化摘要工具，可以提供细化的解释。这个工具支持自定义选项，可以在三级层次（句子、关系三元组、短语）上Visualize文本单元的重要性。代码publicly available。
</details></li>
</ul>
<hr>
<h2 id="RECAP-Towards-Precise-Radiology-Report-Generation-via-Dynamic-Disease-Progression-Reasoning"><a href="#RECAP-Towards-Precise-Radiology-Report-Generation-via-Dynamic-Disease-Progression-Reasoning" class="headerlink" title="RECAP: Towards Precise Radiology Report Generation via Dynamic Disease Progression Reasoning"></a>RECAP: Towards Precise Radiology Report Generation via Dynamic Disease Progression Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13864">http://arxiv.org/abs/2310.13864</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wjhou/recap">https://github.com/wjhou/recap</a></li>
<li>paper_authors: Wenjun Hou, Yi Cheng, Kaishuai Xu, Wenjie Li, Jiang Liu<br>for: 实现 радиialogists 的工作负担减轻methods: 使用动态疾病进程理解和历史纪录融合results: 精确地生成专业医疗报告<details>
<summary>Abstract</summary>
Automating radiology report generation can significantly alleviate radiologists' workloads. Previous research has primarily focused on realizing highly concise observations while neglecting the precise attributes that determine the severity of diseases (e.g., small pleural effusion). Since incorrect attributes will lead to imprecise radiology reports, strengthening the generation process with precise attribute modeling becomes necessary. Additionally, the temporal information contained in the historical records, which is crucial in evaluating a patient's current condition (e.g., heart size is unchanged), has also been largely disregarded. To address these issues, we propose RECAP, which generates precise and accurate radiology reports via dynamic disease progression reasoning. Specifically, RECAP first predicts the observations and progressions (i.e., spatiotemporal information) given two consecutive radiographs. It then combines the historical records, spatiotemporal information, and radiographs for report generation, where a disease progression graph and dynamic progression reasoning mechanism are devised to accurately select the attributes of each observation and progression. Extensive experiments on two publicly available datasets demonstrate the effectiveness of our model.
</details>
<details>
<summary>摘要</summary>
自动化放射学报告生成可以减轻放射学家的工作负担。先前的研究主要集中在实现极简观察结果，而忽略了疾病严重程度决定的精确属性（例如肿胸积液）。由于错误的属性会导致不准确的放射学报告，因此需要加强生成过程中的精确属性模型。此外，历史记录中的时间信息，如评估病人当前状况中的心脏大小是否变化（例如心脏大小不变），也被大量忽略。为解决这些问题，我们提议RECAP，它通过动态疾病进程逻辑来生成精确和准确的放射学报告。具体来说，RECAP首先预测两个连续的放射ogram的观察结果和进程（即空间时间信息）。然后，它将历史记录、空间时间信息和放射ogram组合起来，通过疾病进程图和动态进程逻辑机制来准确选择每个观察结果和进程的属性。我们对公共数据集进行了广泛的实验，结果表明RECAP的效果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/21/cs.CL_2023_10_21/" data-id="cloimip7m00cps4888ldm0992" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_10_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/21/cs.LG_2023_10_21/" class="article-date">
  <time datetime="2023-10-21T10:00:00.000Z" itemprop="datePublished">2023-10-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/21/cs.LG_2023_10_21/">cs.LG - 2023-10-21</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Optimal-Batched-Best-Arm-Identification"><a href="#Optimal-Batched-Best-Arm-Identification" class="headerlink" title="Optimal Batched Best Arm Identification"></a>Optimal Batched Best Arm Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14129">http://arxiv.org/abs/2310.14129</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyuan Jin, Yu Yang, Jing Tang, Xiaokui Xiao, Pan Xu</li>
<li>for: 本研究探讨批处理最佳臂标识问题（Batched Best Arm Identification，BBAI），learner的目标是尽可能快地确定最佳臂，同时尽可能减少策略的变化次数。</li>
<li>methods: 我们提出了三批最佳臂标识算法（Tri-BBAI），它是首个在极限设定（即$\delta\to 0$）下实现最佳样本复杂性的批处理算法，并且只需要最多三批。基于Tri-BBAI，我们还提出了几乎最佳批处理最佳臂标识算法（Opt-BBAI），它在非极限设定（即$\delta&gt;0$）下实现近似最佳样本和批复杂性，而且与Tri-BBAI在$\delta\to 0$时的复杂性相同。</li>
<li>results: 我们的研究表明，Opt-BBAI在非极限设定下实现了近似最佳样本和批复杂性，而且不需要对返回最佳臂的事件进行条件，这与之前的批处理算法不同。此外，我们还提出了一种新的检查最佳臂是否被消除的方法，它是独立的兴趣。<details>
<summary>Abstract</summary>
We study the batched best arm identification (BBAI) problem, where the learner's goal is to identify the best arm while switching the policy as less as possible. In particular, we aim to find the best arm with probability $1-\delta$ for some small constant $\delta>0$ while minimizing both the sample complexity (total number of arm pulls) and the batch complexity (total number of batches). We propose the three-batch best arm identification (Tri-BBAI) algorithm, which is the first batched algorithm that achieves the optimal sample complexity in the asymptotic setting (i.e., $\delta\rightarrow 0$) and runs only in at most $3$ batches. Based on Tri-BBAI, we further propose the almost optimal batched best arm identification (Opt-BBAI) algorithm, which is the first algorithm that achieves the near-optimal sample and batch complexity in the non-asymptotic setting (i.e., $\delta>0$ is arbitrarily fixed), while enjoying the same batch and sample complexity as Tri-BBAI when $\delta$ tends to zero. Moreover, in the non-asymptotic setting, the complexity of previous batch algorithms is usually conditioned on the event that the best arm is returned (with a probability of at least $1-\delta$), which is potentially unbounded in cases where a sub-optimal arm is returned. In contrast, the complexity of Opt-BBAI does not rely on such an event. This is achieved through a novel procedure that we design for checking whether the best arm is eliminated, which is of independent interest.
</details>
<details>
<summary>摘要</summary>
我们研究批处最佳臂识别问题（BBAI），学生的目标是将最佳臂识别出来，将策略调整得最少。具体来说，我们想要在probability $1-\delta$的情况下找到最佳臂，而且将数据分析和批号复杂度（total number of arm pulls和total number of batches）降到最低。我们提出了三批最佳臂识别算法（Tri-BBAI），这是第一个批号算法，可以在无限边界（i.e., $\delta\rightarrow 0$)的情况下实现最佳数据分析和批号复杂度，并且只需要最多三个批号。基于Tri-BBAI，我们进一步提出了几乎最佳批号最佳臂识别算法（Opt-BBAI），这是第一个可以在非 asymptotic 环境（i.e., $\delta>0$是任意固定）下实现几乎最佳的数据分析和批号复杂度，并且跟Tri-BBAI在 $\delta$ 趋向 zero 时的性能相同。此外，在非 asymptotic 环境中，前一代批号算法的复杂度通常受到返回最佳臂的几率（至少是 $1-\delta$）的限制，这可能是无限大的情况。在 контраス特，Opt-BBAI 的复杂度不受这种事件的限制。这是通过我们设计的一种独特的检查方法来检查最佳臂是否被消除的。
</details></li>
</ul>
<hr>
<h2 id="DispersioNET-Joint-Inversion-of-Rayleigh-Wave-Multimode-Phase-Velocity-Dispersion-Curves-using-Convolutional-Neural-Networks"><a href="#DispersioNET-Joint-Inversion-of-Rayleigh-Wave-Multimode-Phase-Velocity-Dispersion-Curves-using-Convolutional-Neural-Networks" class="headerlink" title="DispersioNET: Joint Inversion of Rayleigh-Wave Multimode Phase Velocity Dispersion Curves using Convolutional Neural Networks"></a>DispersioNET: Joint Inversion of Rayleigh-Wave Multimode Phase Velocity Dispersion Curves using Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14094">http://arxiv.org/abs/2310.14094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rohan Sharma, Divakar Vashisth, Bharath Shekar</li>
<li>for: 本研究使用深度学习模型DispersioNET来对声波基本和高频模式phase velocity dispersion curve进行联合逆解，以获取声波速度profile。</li>
<li>methods: 该模型基于卷积神经网络(CNN)，并在不同的噪声水平上进行训练和测试。</li>
<li>results: DispersioNET能够准确预测声波速度profile，并能够抗噪和鲁棒性。<details>
<summary>Abstract</summary>
Rayleigh wave dispersion curves have been widely used in near-surface studies, and are primarily inverted for the shear wave (S-wave) velocity profiles. However, the inverse problem is ill-posed, non-unique and nonlinear. Here, we introduce DispersioNET, a deep learning model based on convolution neural networks (CNN) to perform the joint inversion of Rayleigh wave fundamental and higher order mode phase velocity dispersion curves. DispersioNET is trained and tested on both noise-free and noisy dispersion curve datasets and predicts S-wave velocity profiles that match closely with the true velocities. The architecture is agnostic to variations in S-wave velocity profiles such as increasing velocity with depth and intermediate low-velocity layers, while also ensuring that the output remains independent of the number of layers.
</details>
<details>
<summary>摘要</summary>
rayleigh 波动峰位 Curves 在近地表研究中广泛使用，主要是对剪切波（S波）速度profile 进行逆解。然而，逆问题是不定、多重和非线性的。我们介绍了 DispersioNET，一种基于卷积神经网络（CNN）的深度学习模型，用于同时逆解 Rayleigh 波基本和高阶模式 phase 峰位速度分布 Curves。DispersioNET 在噪声存在和缺失的数据集上训练和测试，并能够准确预测 S波速度profile，与真实速度匹配得非常 closely。模型具有不同 S波速度profile 变化的tolerance，同时保证输出不受层数的影响。
</details></li>
</ul>
<hr>
<h2 id="A-Specialized-Semismooth-Newton-Method-for-Kernel-Based-Optimal-Transport"><a href="#A-Specialized-Semismooth-Newton-Method-for-Kernel-Based-Optimal-Transport" class="headerlink" title="A Specialized Semismooth Newton Method for Kernel-Based Optimal Transport"></a>A Specialized Semismooth Newton Method for Kernel-Based Optimal Transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14087">http://arxiv.org/abs/2310.14087</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyi Lin, Marco Cuturi, Michael I. Jordan</li>
<li>for:  This paper proposes a new method for solving optimal transport (OT) problems using a nonsmooth fixed-point model and a specialized semismooth Newton (SSN) method.</li>
<li>methods: The proposed method uses a nonsmooth fixed-point model and a specialized semismooth Newton (SSN) method to efficiently solve kernel-based OT problems.</li>
<li>results: The proposed method achieves a global convergence rate of $O(1&#x2F;\sqrt{k})$ and a local quadratic convergence rate under standard regularity conditions, and shows substantial speedups over the short-step interior-point method (SSIPM) on both synthetic and real datasets.Here is the text in Simplified Chinese:</li>
<li>for: 这篇论文提出了一种使用非平滑稳点模型和特殊的半凝固新颖方法解决优Transport问题的新方法。</li>
<li>methods: 该方法使用非平滑稳点模型和特殊的半凝固新颖方法解决核心基于的优Transport问题。</li>
<li>results: 该方法实现了$O(1&#x2F;\sqrt{k})$的全局收敛率和标准规定的本地二次收敛率，并在实验中显示了SSIPM的显著加速。<details>
<summary>Abstract</summary>
Kernel-based optimal transport (OT) estimators offer an alternative, functional estimation procedure to address OT problems from samples. Recent works suggest that these estimators are more statistically efficient than plug-in (linear programming-based) OT estimators when comparing probability measures in high-dimensions~\citep{Vacher-2021-Dimension}. Unfortunately, that statistical benefit comes at a very steep computational price: because their computation relies on the short-step interior-point method (SSIPM), which comes with a large iteration count in practice, these estimators quickly become intractable w.r.t. sample size $n$. To scale these estimators to larger $n$, we propose a nonsmooth fixed-point model for the kernel-based OT problem, and show that it can be efficiently solved via a specialized semismooth Newton (SSN) method: We show, exploring the problem's structure, that the per-iteration cost of performing one SSN step can be significantly reduced in practice. We prove that our SSN method achieves a global convergence rate of $O(1/\sqrt{k})$, and a local quadratic convergence rate under standard regularity conditions. We show substantial speedups over SSIPM on both synthetic and real datasets.
</details>
<details>
<summary>摘要</summary>
kernel-based最优运输（OT）估计器提供了一种代替方法来解决OT问题，从样本中进行函数估计。据最新的研究表明，这些估计器在高维度下比插入（线性编程基于）OT估计器更有统计效率，但是计算成本却非常高：因为它们的计算基于短步内部点法（SSIPM），在实践中通常需要许多迭代。为了扩展这些估计器，我们提出了一种非光滑固定点模型，并证明可以通过特殊的半稳定新颖（SSN）方法高效解决。我们展示，通过利用问题的结构，每次SSN步骤的计算成本可以在实践中减少到一定程度。我们证明我们的SSN方法在全球收敛率为$O(1/\sqrt{k})$，以及当标准正则条件下的本地二阶收敛率。我们在 sintetic 和实际数据集上展示了显著的计算速度提升。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Doubly-Optimal-No-Regret-Learning-in-Strongly-Monotone-and-Exp-Concave-Games-with-Gradient-Feedback"><a href="#Adaptive-Doubly-Optimal-No-Regret-Learning-in-Strongly-Monotone-and-Exp-Concave-Games-with-Gradient-Feedback" class="headerlink" title="Adaptive, Doubly Optimal No-Regret Learning in Strongly Monotone and Exp-Concave Games with Gradient Feedback"></a>Adaptive, Doubly Optimal No-Regret Learning in Strongly Monotone and Exp-Concave Games with Gradient Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14085">http://arxiv.org/abs/2310.14085</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael I. Jordan, Tianyi Lin, Zhengyuan Zhou</li>
<li>for: 本研究旨在设计一个不需要先知 convexity&#x2F;monotonicity 参数的在线梯度下降（OGD）算法，以便在单个代理和多个代理的情况下实现最优的 regret 和 Nash 平衡rate。</li>
<li>methods: 本研究使用了一种全部适应的 OGD 算法，称为 \textsf{AdaOGD}，它不需要先知 convexity&#x2F;monotonicity 参数。在单个代理情况下，\textsf{AdaOGD} 可以实现 $O(\log^2(T))$ 的 regret，这是最优的结果，只有 log 因子的差异。在多个代理情况下，如果每个代理使用 \textsf{AdaOGD}，则共同行动会在 $O(\frac{\log^3 T}{T})$ 的速度 converges 到一个唯一的 Nash 平衡。</li>
<li>results: 本研究的结果显示，\textsf{AdaOGD} 可以在新闻vendor 问题中实现最优的 regret 和 Nash 平衡rate。此外，本研究还扩展到了更通用的 exp-concave 成本函数和游戏中，使用在线 Newton 步骤（ONS）算法。<details>
<summary>Abstract</summary>
Online gradient descent (OGD) is well known to be doubly optimal under strong convexity or monotonicity assumptions: (1) in the single-agent setting, it achieves an optimal regret of $\Theta(\log T)$ for strongly convex cost functions; and (2) in the multi-agent setting of strongly monotone games, with each agent employing OGD, we obtain last-iterate convergence of the joint action to a unique Nash equilibrium at an optimal rate of $\Theta(\frac{1}{T})$. While these finite-time guarantees highlight its merits, OGD has the drawback that it requires knowing the strong convexity/monotonicity parameters. In this paper, we design a fully adaptive OGD algorithm, \textsf{AdaOGD}, that does not require a priori knowledge of these parameters. In the single-agent setting, our algorithm achieves $O(\log^2(T))$ regret under strong convexity, which is optimal up to a log factor. Further, if each agent employs \textsf{AdaOGD} in strongly monotone games, the joint action converges in a last-iterate sense to a unique Nash equilibrium at a rate of $O(\frac{\log^3 T}{T})$, again optimal up to log factors. We illustrate our algorithms in a learning version of the classical newsvendor problem, where due to lost sales, only (noisy) gradient feedback can be observed. Our results immediately yield the first feasible and near-optimal algorithm for both the single-retailer and multi-retailer settings. We also extend our results to the more general setting of exp-concave cost functions and games, using the online Newton step (ONS) algorithm.
</details>
<details>
<summary>摘要</summary>
在线梯度下降（OGD）在强连续或强 monotonicity  assumption下是著名的双优化算法：（1）在单个代理Setting中，它可以在强连续成本函数下 achieve  óptimal regret of $\Theta(\log T)$;（2）在多个代理Setting中，每个代理使用 OGD，我们可以获得last-iterate 协调的joint action converges to a unique Nash equilibrium at an optimal rate of $\Theta(\frac{1}{T})$. 虽然这些 finite-time guarantees  highlight its merits, OGD 需要先知道强连续/MONOTONICITY 参数。在这篇论文中，我们设计了一个完全适应的 OGD 算法，\textsf{AdaOGD}，不需要先知道这些参数。在单个代理Setting中，我们的算法可以 achieve $O(\log^2(T))$ regret under strong convexity, which is optimal up to a log factor。此外，如果每个代理使用 \textsf{AdaOGD} 在强MONOTONICITY games中，joint action 会在 last-iterate  sense converge to a unique Nash equilibrium at a rate of $O(\frac{\log^3 T}{T})$, again optimal up to log factors。我们在学习版新闻 vendor problem中 illustrate 我们的算法，因为lost sales，只能观察（噪音）梯度反馈。我们的结果 immediately yield the first feasible and near-optimal algorithm for both the single-retailer and multi-retailer settings。我们还将我们的结果推广到更一般的 exp-concave cost functions 和 games，使用 online Newton step（ONS）算法。
</details></li>
</ul>
<hr>
<h2 id="Graph-Neural-Networks-and-Applied-Linear-Algebra"><a href="#Graph-Neural-Networks-and-Applied-Linear-Algebra" class="headerlink" title="Graph Neural Networks and Applied Linear Algebra"></a>Graph Neural Networks and Applied Linear Algebra</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14084">http://arxiv.org/abs/2310.14084</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sandialabs/gnn-applied-linear-algebra">https://github.com/sandialabs/gnn-applied-linear-algebra</a></li>
<li>paper_authors: Nicholas S. Moore, Eric C. Cyr, Peter Ohm, Christopher M. Siefert, Raymond S. Tuminaro</li>
<li>for: 本文主要针对数学 linear algebra 领域的科学计算问题，探讨如何使用 neural network (NN) 来解决 sparse matrix 计算问题。</li>
<li>methods: 本文使用 graph neural network (GNN) 方法来处理 sparse matrix 计算问题，GNN 定义了聚合函数（例如和），可以操作变量大小的输入数据，生成固定大小的输出数据，以便应用 MLP 来解决问题。</li>
<li>results: 本文通过提供了多个实际例子，示出了如何使用 GNN 来解决一些常见的 linear algebra 任务，包括matrix-vector 乘法、插值、松弛方法和连接度度量。<details>
<summary>Abstract</summary>
Sparse matrix computations are ubiquitous in scientific computing. With the recent interest in scientific machine learning, it is natural to ask how sparse matrix computations can leverage neural networks (NN). Unfortunately, multi-layer perceptron (MLP) neural networks are typically not natural for either graph or sparse matrix computations. The issue lies with the fact that MLPs require fixed-sized inputs while scientific applications generally generate sparse matrices with arbitrary dimensions and a wide range of nonzero patterns (or matrix graph vertex interconnections). While convolutional NNs could possibly address matrix graphs where all vertices have the same number of nearest neighbors, a more general approach is needed for arbitrary sparse matrices, e.g. arising from discretized partial differential equations on unstructured meshes. Graph neural networks (GNNs) are one approach suitable to sparse matrices. GNNs define aggregation functions (e.g., summations) that operate on variable size input data to produce data of a fixed output size so that MLPs can be applied. The goal of this paper is to provide an introduction to GNNs for a numerical linear algebra audience. Concrete examples are provided to illustrate how many common linear algebra tasks can be accomplished using GNNs. We focus on iterative methods that employ computational kernels such as matrix-vector products, interpolation, relaxation methods, and strength-of-connection measures. Our GNN examples include cases where parameters are determined a-priori as well as cases where parameters must be learned. The intent with this article is to help computational scientists understand how GNNs can be used to adapt machine learning concepts to computational tasks associated with sparse matrices. It is hoped that this understanding will stimulate data-driven extensions of classical sparse linear algebra tasks.
</details>
<details>
<summary>摘要</summary>
科学计算中的稀疏矩阵计算是普遍存在的。随着科学机器学习的兴趣增长，可以问题是如何使用神经网络（NN）来利用稀疏矩阵计算。然而，多层感知网络（MLP）通常不适用于图或稀疏矩阵计算，因为MLP需要固定大小的输入，而科学应用通常会生成稀疏矩阵，其维度和非零强相互关系是不固定的。而卷积神经网络（CNN）可能可以解决图格上的稀疏矩阵问题，但是需要一种更通用的方法来解决任意稀疏矩阵问题，例如由不同维度的几何函数所生成的稀疏矩阵。图神经网络（GNN）是一种适用于稀疏矩阵的方法，GNN定义了聚合函数（例如和），这些函数可以在变量大小的输入数据上运行，以生成固定大小的输出数据，从而使得MLP可以应用。本文的目标是为数 Linear Algebra 读者提供 GNN 的引入，并提供一些具体的例子来说明如何使用 GNN 来完成许多常见的Linear Algebra 任务。我们关注到了迭代方法，包括矩阵-向量产品、 interpolate、 relaxation 方法和 strength-of-connection 度量。我们的 GNN 例子包括参数由先前确定的情况以及参数需要学习的情况。我们希望通过这篇文章，让计算科学家理解 GNN 如何用于改变机器学习概念，以便在稀疏矩阵计算中进行数据驱动的扩展。
</details></li>
</ul>
<hr>
<h2 id="Counterfactual-Prediction-Under-Selective-Confounding"><a href="#Counterfactual-Prediction-Under-Selective-Confounding" class="headerlink" title="Counterfactual Prediction Under Selective Confounding"></a>Counterfactual Prediction Under Selective Confounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14064">http://arxiv.org/abs/2310.14064</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sohaib730/causalml">https://github.com/sohaib730/causalml</a></li>
<li>paper_authors: Sohaib Kiani, Jared Barton, Jon Sushinsky, Lynda Heimbach, Bo Luo</li>
<li>for: 本研究旨在Addressing the challenge of conducting interpretable causal inference between a binary treatment and its resulting outcome when not all confounders are known.</li>
<li>methods: 我们提出了一种基于Selective Confounding的方法，使用双调sample来实现。这种方法可以在多个决策者与不同政策存在的情况下进行适应。</li>
<li>results: 我们提供了both theoretical error bounds和实际证据，证明了我们的方法的有效性。此外，我们还介绍了三种特定于儿童分配场景的评价方法，以增强透明性和可解性。<details>
<summary>Abstract</summary>
This research addresses the challenge of conducting interpretable causal inference between a binary treatment and its resulting outcome when not all confounders are known. Confounders are factors that have an influence on both the treatment and the outcome. We relax the requirement of knowing all confounders under desired treatment, which we refer to as Selective Confounding, to enable causal inference in diverse real-world scenarios. Our proposed scheme is designed to work in situations where multiple decision-makers with different policies are involved and where there is a re-evaluation mechanism after the initial decision to ensure consistency. These assumptions are more practical to fulfill compared to the availability of all confounders under all treatments. To tackle the issue of Selective Confounding, we propose the use of dual-treatment samples. These samples allow us to employ two-step procedures, such as Regression Adjustment or Doubly-Robust, to learn counterfactual predictors. We provide both theoretical error bounds and empirical evidence of the effectiveness of our proposed scheme using synthetic and real-world child placement data. Furthermore, we introduce three evaluation methods specifically tailored to assess the performance in child placement scenarios. By emphasizing transparency and interpretability, our approach aims to provide decision-makers with a valuable tool. The source code repository of this work is located at https://github.com/sohaib730/CausalML.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="On-discretisation-drift-and-smoothness-regularisation-in-neural-network-training"><a href="#On-discretisation-drift-and-smoothness-regularisation-in-neural-network-training" class="headerlink" title="On discretisation drift and smoothness regularisation in neural network training"></a>On discretisation drift and smoothness regularisation in neural network training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14036">http://arxiv.org/abs/2310.14036</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mihaela Claudia Rosca</li>
<li>for: 本研究的目的是更深入地理解深度学习，尤其是优化和模型规范化。</li>
<li>methods: 本研究使用了梯度下降（GD）和负梯度流（NGF）等优化算法，以及新的连续时间流动来研究GD的动态。</li>
<li>results: 研究发现，在超级vised学习和两个玩家游戏中训练不稳定的问题可以通过构建新的学习率时间表和规范来解决。此外，研究还发现了在各种深度学习领域中细eness规范对优化的影响，并在强化学习中添加细eness规范后得到了性能提升。<details>
<summary>Abstract</summary>
The deep learning recipe of casting real-world problems as mathematical optimisation and tackling the optimisation by training deep neural networks using gradient-based optimisation has undoubtedly proven to be a fruitful one. The understanding behind why deep learning works, however, has lagged behind its practical significance. We aim to make steps towards an improved understanding of deep learning with a focus on optimisation and model regularisation. We start by investigating gradient descent (GD), a discrete-time algorithm at the basis of most popular deep learning optimisation algorithms. Understanding the dynamics of GD has been hindered by the presence of discretisation drift, the numerical integration error between GD and its often studied continuous-time counterpart, the negative gradient flow (NGF). To add to the toolkit available to study GD, we derive novel continuous-time flows that account for discretisation drift. Unlike the NGF, these new flows can be used to describe learning rate specific behaviours of GD, such as training instabilities observed in supervised learning and two-player games. We then translate insights from continuous time into mitigation strategies for unstable GD dynamics, by constructing novel learning rate schedules and regularisers that do not require additional hyperparameters. Like optimisation, smoothness regularisation is another pillar of deep learning's success with wide use in supervised learning and generative modelling. Despite their individual significance, the interactions between smoothness regularisation and optimisation have yet to be explored. We find that smoothness regularisation affects optimisation across multiple deep learning domains, and that incorporating smoothness regularisation in reinforcement learning leads to a performance boost that can be recovered using adaptions to optimisation methods.
</details>
<details>
<summary>摘要</summary>
深度学习的制作方法，将现实世界的问题化为数学优化问题，并使用深度神经网络进行加速器基于梯度下降优化，不疑而是成功的。然而，深度学习的工作原理尚未得到充分的理解。我们想要做出一些探索深度学习的尝试，特icularly focusing on optimization and model regularization. We start by investigating gradient descent (GD), a discrete-time algorithm that is the foundation of most popular deep learning optimization algorithms. Understanding the dynamics of GD has been hindered by the presence of discretization drift, the numerical integration error between GD and its continuous-time counterpart, the negative gradient flow (NGF). To add to the toolkit available to study GD, we derive novel continuous-time flows that account for discretization drift. Unlike the NGF, these new flows can be used to describe learning rate specific behaviors of GD, such as training instabilities observed in supervised learning and two-player games. We then translate insights from continuous time into mitigation strategies for unstable GD dynamics, by constructing novel learning rate schedules and regularizers that do not require additional hyperparameters. Smoothness regularization is another pillar of deep learning's success, with wide use in supervised learning and generative modeling. Despite their individual significance, the interactions between smoothness regularization and optimization have yet to be explored. We find that smoothness regularization affects optimization across multiple deep learning domains, and that incorporating smoothness regularization in reinforcement learning leads to a performance boost that can be recovered using adaptions to optimization methods.
</details></li>
</ul>
<hr>
<h2 id="Filling-the-Missing-Exploring-Generative-AI-for-Enhanced-Federated-Learning-over-Heterogeneous-Mobile-Edge-Devices"><a href="#Filling-the-Missing-Exploring-Generative-AI-for-Enhanced-Federated-Learning-over-Heterogeneous-Mobile-Edge-Devices" class="headerlink" title="Filling the Missing: Exploring Generative AI for Enhanced Federated Learning over Heterogeneous Mobile Edge Devices"></a>Filling the Missing: Exploring Generative AI for Enhanced Federated Learning over Heterogeneous Mobile Edge Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13981">http://arxiv.org/abs/2310.13981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peichun Li, Hanwen Zhang, Yuan Wu, Liping Qian, Rong Yu, Dusit Niyato, Xuemin Shen</li>
<li>for: 提高分布式人工智能模型训练过程中的Edge网络中设备的数据和资源不同性问题。</li>
<li>methods: 提议使用生成AI技术实现 federated learning，通过填充地方数据的想法来解决数据不同性问题，同时提高训练效率和设备资源利用率。</li>
<li>results: 实验结果表明，使用FIMI可以将设备 сторо的能源减少50%，同时达到global测试准确率目标，并且在非独立同分布（non-IID）数据下，FIMI可以显著提高全球准确率。<details>
<summary>Abstract</summary>
Distributed Artificial Intelligence (AI) model training over mobile edge networks encounters significant challenges due to the data and resource heterogeneity of edge devices. The former hampers the convergence rate of the global model, while the latter diminishes the devices' resource utilization efficiency. In this paper, we propose a generative AI-empowered federated learning to address these challenges by leveraging the idea of FIlling the MIssing (FIMI) portion of local data. Specifically, FIMI can be considered as a resource-aware data augmentation method that effectively mitigates the data heterogeneity while ensuring efficient FL training. We first quantify the relationship between the training data amount and the learning performance. We then study the FIMI optimization problem with the objective of minimizing the device-side overall energy consumption subject to required learning performance constraints. The decomposition-based analysis and the cross-entropy searching method are leveraged to derive the solution, where each device is assigned suitable AI-synthesized data and resource utilization policy. Experiment results demonstrate that FIMI can save up to 50% of the device-side energy to achieve the target global test accuracy in comparison with the existing methods. Meanwhile, FIMI can significantly enhance the converged global accuracy under the non-independently-and-identically distribution (non-IID) data.
</details>
<details>
<summary>摘要</summary>
分布式人工智能（AI）模型训练在移动边缘网络上遇到了数据和资源不一致的问题。前者阻碍全球模型的吞吐率，而后者降低设备的资源利用效率。在这篇论文中，我们提出了基于生成AI的联邦学习来解决这些问题，利用了填充缺失（FIMI）的想法。 Specifically, FIMI可以看作是一种资源意识的数据扩充方法，有效地缓解数据不一致性，同时保证了有效的联邦学习训练。我们首先量化了训练数据量和学习性能之间的关系。然后，我们研究了FIMI优化问题，即将设备侧总能 consumption最小化，保证学习性能要求。通过分解分析和十字推测法，我们 derivated解决方案，每个设备都被分配了适合的AI生成的数据和资源利用策略。实验结果表明，FIMI可以将设备侧能 consumption降低至50%，以达到targettest accuracy。同时，FIMI可以在非独立同一分布（non-IID）数据下显著提高全球准确率。
</details></li>
</ul>
<hr>
<h2 id="Continual-Invariant-Risk-Minimization"><a href="#Continual-Invariant-Risk-Minimization" class="headerlink" title="Continual Invariant Risk Minimization"></a>Continual Invariant Risk Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13977">http://arxiv.org/abs/2310.13977</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Alesiani, Shujian Yu, Mathias Niepert</li>
<li>for: 本研究旨在提出一种基于环境变换的连续学习方法，以便在不同环境中学习模型能够捕捉到共同特征表示。</li>
<li>methods: 本研究使用了 invariant risk minimization（IRM）方法，即在所有环境都可用于学习系统上的方法。另外，本研究还提出了一种基于变分 Bayesian 和双层框架的扩展方法，以满足连续学习中的共同特征捕捉。</li>
<li>results: 研究表明，使用提出的方法可以在多个数据集和多个顺序环境中，与之前的方法相比或与之比肩，提高连续学习中的模型性能。<details>
<summary>Abstract</summary>
Empirical risk minimization can lead to poor generalization behavior on unseen environments if the learned model does not capture invariant feature representations. Invariant risk minimization (IRM) is a recent proposal for discovering environment-invariant representations. IRM was introduced by Arjovsky et al. (2019) and extended by Ahuja et al. (2020). IRM assumes that all environments are available to the learning system at the same time. With this work, we generalize the concept of IRM to scenarios where environments are observed sequentially. We show that existing approaches, including those designed for continual learning, fail to identify the invariant features and models across sequentially presented environments. We extend IRM under a variational Bayesian and bilevel framework, creating a general approach to continual invariant risk minimization. We also describe a strategy to solve the optimization problems using a variant of the alternating direction method of multiplier (ADMM). We show empirically using multiple datasets and with multiple sequential environments that the proposed methods outperform or is competitive with prior approaches.
</details>
<details>
<summary>摘要</summary>
empirical risk minimization可能会导致在未看到的环境中的差异化行为，如果学习的模型没有捕捉环境不变的特征表示。不变风险最小化(IRM)是一种最近提出的环境不变表示发现方法。IRM由Arjovsky et al.（2019）引入并由Ahuja et al.（2020）扩展。IRM假设所有环境都可以同时给学习系统提供。在这种情况下，我们推广了IRM的概念，以适应sequentially presented environments中的环境。我们表明，包括 continual learning的方法在内的现有方法无法在sequentially presented environments中标识不变的特征和模型。我们将IRM扩展为variational Bayesian和bilateral框架，创造一种总体的持续不变风险最小化方法。我们还描述了使用一种变体的alternating direction method of multiplier(ADMM)来解决优化问题的策略。我们通过多个数据集和多个sequential environments的实验表明，提posed方法可以比或与之前的方法相比性。
</details></li>
</ul>
<hr>
<h2 id="ASBART-Accelerated-Soft-Bayes-Additive-Regression-Trees"><a href="#ASBART-Accelerated-Soft-Bayes-Additive-Regression-Trees" class="headerlink" title="ASBART:Accelerated Soft Bayes Additive Regression Trees"></a>ASBART:Accelerated Soft Bayes Additive Regression Trees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13975">http://arxiv.org/abs/2310.13975</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/richael008/xsbart">https://github.com/richael008/xsbart</a></li>
<li>paper_authors: Hao Ran, Yang Bai</li>
<li>For:  This paper proposes a new variant of Bayesian additive regression trees (BART) called accelerate soft BART (ASBART), which improves the speed of the existing Soft BART model while maintaining comparable accuracy.* Methods: The proposed ASBART method uses a new algorithm that is about 10 times faster than the default Soft BART method, making it more practical for real-world applications.* Results: Simulation studies show that ASBART has comparable accuracy to Soft BART, while being significantly faster in terms of computational speed. The code for ASBART is open-source and available online.<details>
<summary>Abstract</summary>
Bayes additive regression trees(BART) is a nonparametric regression model which has gained wide-spread popularity in recent years due to its flexibility and high accuracy of estimation. Soft BART,one variation of BART,improves both practically and heoretically on existing Bayesian sum-of-trees models. One bottleneck for Soft BART is its slow speed in the long MCMC loop. Compared to BART,it use more than about 20 times to complete the calculation with the default setting. We proposed a variant of BART named accelerate Soft BART(ASBART). Simulation studies show that the new method is about 10 times faster than the Soft BART with comparable accuracy. Our code is open-source and available at https://github.com/richael008/XSBART.
</details>
<details>
<summary>摘要</summary>
bayes 添加 regresión árboles (BART) 是一种非 Parametric 回归模型，在过去几年内得到了广泛的推广和应用，因为它的灵活性和估计精度高。 软 BART，BART 的一种变体，在现有的 Bayesian 汇集树模型上进行了改进，从理论和实践两个方面来说，它提高了回归预测的精度。 然而，Soft BART 的长MCMC循环速度比较慢，相比 BART，它需要大约 20 倍的计算时间。我们提出了一种加速 Soft BART 的方法，称为加速 Soft BART (ASBART)。 根据 simulations 的研究，新方法比 Soft BART 快约 10 倍，并且与其相比，它们在精度方面几乎相同。 我们的代码开源在 GitHub 上，可以在 <https://github.com/richael008/XSBART> 获取。
</details></li>
</ul>
<hr>
<h2 id="Distributed-Linear-Regression-with-Compositional-Covariates"><a href="#Distributed-Linear-Regression-with-Compositional-Covariates" class="headerlink" title="Distributed Linear Regression with Compositional Covariates"></a>Distributed Linear Regression with Compositional Covariates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13969">http://arxiv.org/abs/2310.13969</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Chao, Lei Huang, Xuejun Ma</li>
<li>for: 解决大数据集中分布式统计方法和计算问题。</li>
<li>methods: 提议了两种分布式优化技术，一种是基于ADMM框架的中央化优化算法，另一种是基于CDMM框架的分布式坐标下降算法。</li>
<li>results: 通过对真实数据和 sintetic数据进行数值实验，证明了提议的算法的有效性和可靠性。<details>
<summary>Abstract</summary>
With the availability of extraordinarily huge data sets, solving the problems of distributed statistical methodology and computing for such data sets has become increasingly crucial in the big data area. In this paper, we focus on the distributed sparse penalized linear log-contrast model in massive compositional data. In particular, two distributed optimization techniques under centralized and decentralized topologies are proposed for solving the two different constrained convex optimization problems. Both two proposed algorithms are based on the frameworks of Alternating Direction Method of Multipliers (ADMM) and Coordinate Descent Method of Multipliers(CDMM, Lin et al., 2014, Biometrika). It is worth emphasizing that, in the decentralized topology, we introduce a distributed coordinate-wise descent algorithm based on Group ADMM(GADMM, Elgabli et al., 2020, Journal of Machine Learning Research) for obtaining a communication-efficient regularized estimation. Correspondingly, the convergence theories of the proposed algorithms are rigorously established under some regularity conditions. Numerical experiments on both synthetic and real data are conducted to evaluate our proposed algorithms.
</details>
<details>
<summary>摘要</summary>
随着庞大数据集的可用性的提高，解决分布式统计方法和计算问题已成为大数据领域中越来越重要的问题。在这篇论文中，我们关注于巨大compositional数据中的分布式稀缺假设模型。我们提出了两种分布式优化技术，一种是基于中央化topology，另一种是基于分布式topology。两种提出的算法都基于ADMM和CDMM框架(Lin et al., 2014, Biometrika)。在分布式topology中，我们提出了一种分布式坐标点wise降降算法基于Group ADMM(Elgabli et al., 2020, Journal of Machine Learning Research)，以实现通信效率的Regularized估计。对于提出的算法，我们也证明了其 convergence的理论基础。在实验中，我们使用 sintetic和实际数据进行了数值测试，以评估我们的提出算法。
</details></li>
</ul>
<hr>
<h2 id="Minimax-Optimal-Transfer-Learning-for-Kernel-based-Nonparametric-Regression"><a href="#Minimax-Optimal-Transfer-Learning-for-Kernel-based-Nonparametric-Regression" class="headerlink" title="Minimax Optimal Transfer Learning for Kernel-based Nonparametric Regression"></a>Minimax Optimal Transfer Learning for Kernel-based Nonparametric Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13966">http://arxiv.org/abs/2310.13966</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Wang, Caixing Wang, Xin He, Xingdong Feng</li>
<li>for: This paper focuses on investigating the transfer learning problem within the context of nonparametric regression over a reproducing kernel Hilbert space, with the aim of bridging the gap between practical effectiveness and theoretical guarantees.</li>
<li>methods: The proposed method uses kernel ridge regression for the known transferable source case, and an efficient aggregation algorithm for the unknown case, which can automatically detect and alleviate the effects of negative sources.</li>
<li>results: The paper provides the statistical properties of the desired estimators and establishes the minimax optimal rate, and through extensive numerical experiments on synthetic data and real examples, the effectiveness of the proposed method is validated.<details>
<summary>Abstract</summary>
In recent years, transfer learning has garnered significant attention in the machine learning community. Its ability to leverage knowledge from related studies to improve generalization performance in a target study has made it highly appealing. This paper focuses on investigating the transfer learning problem within the context of nonparametric regression over a reproducing kernel Hilbert space. The aim is to bridge the gap between practical effectiveness and theoretical guarantees. We specifically consider two scenarios: one where the transferable sources are known and another where they are unknown. For the known transferable source case, we propose a two-step kernel-based estimator by solely using kernel ridge regression. For the unknown case, we develop a novel method based on an efficient aggregation algorithm, which can automatically detect and alleviate the effects of negative sources. This paper provides the statistical properties of the desired estimators and establishes the minimax optimal rate. Through extensive numerical experiments on synthetic data and real examples, we validate our theoretical findings and demonstrate the effectiveness of our proposed method.
</details>
<details>
<summary>摘要</summary>
Two scenarios are considered: one where the transferable sources are known, and another where they are unknown. For the known transferable source case, a two-step kernel-based estimator is proposed, solely using kernel ridge regression. For the unknown case, a novel method based on an efficient aggregation algorithm is developed, which can automatically detect and alleviate the effects of negative sources.The statistical properties of the desired estimators are provided, and the minimax optimal rate is established. Extensive numerical experiments on synthetic data and real examples are conducted to validate the theoretical findings and demonstrate the effectiveness of the proposed method.
</details></li>
</ul>
<hr>
<h2 id="Toward-Generative-Data-Augmentation-for-Traffic-Classification"><a href="#Toward-Generative-Data-Augmentation-for-Traffic-Classification" class="headerlink" title="Toward Generative Data Augmentation for Traffic Classification"></a>Toward Generative Data Augmentation for Traffic Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13935">http://arxiv.org/abs/2310.13935</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Wang, Alessandro Finamore, Pietro Michiardi, Massimo Gallo, Dario Rossi</li>
<li>for: 本研究旨在探讨数据增强技术在网络应用中的可行性，特别是在流量分类领域。</li>
<li>methods: 本研究采用了14种手动设计的数据增强策略，应用于MIRAGE19 dataset。</li>
<li>results: 研究结果显示，数据增强可以在流量分类中提供未曾被探讨的优势，同时促进了使用生成模型自动设计数据增强策略的研究课程。<details>
<summary>Abstract</summary>
Data Augmentation (DA)-augmenting training data with synthetic samples-is wildly adopted in Computer Vision (CV) to improve models performance. Conversely, DA has not been yet popularized in networking use cases, including Traffic Classification (TC). In this work, we present a preliminary study of 14 hand-crafted DAs applied on the MIRAGE19 dataset. Our results (i) show that DA can reap benefits previously unexplored in TC and (ii) foster a research agenda on the use of generative models to automate DA design.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Data Augmentation (DA)-augmenting training data with synthetic samples-is wildly adopted in Computer Vision (CV) to improve models performance. Conversely, DA has not been yet popularized in networking use cases, including Traffic Classification (TC). In this work, we present a preliminary study of 14 hand-crafted DAs applied on the MIRAGE19 dataset. Our results (i) show that DA can reap benefits previously unexplored in TC and (ii) foster a research agenda on the use of generative models to automate DA design.">以下是文本的Simplified Chinese翻译：<<SYS>>计算机视觉（CV）中广泛采用数据扩充（DA）技术，卷积神经网络性能。然而，在网络应用场景中，包括流量分类（TC），DA还没有得到普及。在这项工作中，我们对MIRAGE19数据集上手工设计了14种DA，并进行了初步研究。我们的结果表明，DA可以在TC中获得未曾提及的利益，同时也激发了使用生成模型自动化DA设计的研究论坛。
</details></li>
</ul>
<hr>
<h2 id="Diversified-Outlier-Exposure-for-Out-of-Distribution-Detection-via-Informative-Extrapolation"><a href="#Diversified-Outlier-Exposure-for-Out-of-Distribution-Detection-via-Informative-Extrapolation" class="headerlink" title="Diversified Outlier Exposure for Out-of-Distribution Detection via Informative Extrapolation"></a>Diversified Outlier Exposure for Out-of-Distribution Detection via Informative Extrapolation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13923">http://arxiv.org/abs/2310.13923</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zfancy/divoe">https://github.com/zfancy/divoe</a></li>
<li>paper_authors: Jianing Zhu, Geng Yu, Jiangchao Yao, Tongliang Liu, Gang Niu, Masashi Sugiyama, Bo Han<br>for:本研究旨在提高机器学习模型在实际应用中的可靠性，通过进行outsider暴露（OOD）检测。methods:本研究提出了一种新的框架，即多样化外围暴露（DivOE），通过在训练过程中使用多亮示的auxiliary outliers来实现有效的OOD检测。results: DivOE通过在训练过程中生成更多的外围样本，以便在ID和OOD数据之间找到更多的分界点，从而提高OOD检测的准确性。<details>
<summary>Abstract</summary>
Out-of-distribution (OOD) detection is important for deploying reliable machine learning models on real-world applications. Recent advances in outlier exposure have shown promising results on OOD detection via fine-tuning model with informatively sampled auxiliary outliers. However, previous methods assume that the collected outliers can be sufficiently large and representative to cover the boundary between ID and OOD data, which might be impractical and challenging. In this work, we propose a novel framework, namely, Diversified Outlier Exposure (DivOE), for effective OOD detection via informative extrapolation based on the given auxiliary outliers. Specifically, DivOE introduces a new learning objective, which diversifies the auxiliary distribution by explicitly synthesizing more informative outliers for extrapolation during training. It leverages a multi-step optimization method to generate novel outliers beyond the original ones, which is compatible with many variants of outlier exposure. Extensive experiments and analyses have been conducted to characterize and demonstrate the effectiveness of the proposed DivOE. The code is publicly available at: https://github.com/tmlr-group/DivOE.
</details>
<details>
<summary>摘要</summary>
OUT-OF-DISTRIBUTION (OOD) 检测是在实际应用中部署可靠机器学习模型的重要问题。 latest advances in outlier exposure  have shown promising results on OOD detection via fine-tuning model with informatively sampled auxiliary outliers。 However, previous methods assume that the collected outliers can be sufficiently large and representative to cover the boundary between ID and OOD data, which might be impractical and challenging。 In this work, we propose a novel framework, namely, Diversified Outlier Exposure (DivOE), for effective OOD detection via informative extrapolation based on the given auxiliary outliers。 Specifically, DivOE introduces a new learning objective, which diversifies the auxiliary distribution by explicitly synthesizing more informative outliers for extrapolation during training。 It leverages a multi-step optimization method to generate novel outliers beyond the original ones, which is compatible with many variants of outlier exposure。 Extensive experiments and analyses have been conducted to characterize and demonstrate the effectiveness of the proposed DivOE。 The code is publicly available at: https://github.com/tmlr-group/DivOE。Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Equivariant-Map-and-Agent-Geometry-for-Autonomous-Driving-Motion-Prediction"><a href="#Equivariant-Map-and-Agent-Geometry-for-Autonomous-Driving-Motion-Prediction" class="headerlink" title="Equivariant Map and Agent Geometry for Autonomous Driving Motion Prediction"></a>Equivariant Map and Agent Geometry for Autonomous Driving Motion Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13922">http://arxiv.org/abs/2310.13922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuping Wang, Jier Chen</li>
<li>for: 这种研究旨在解决自动驾驶中的深度学习帮助预测运动的问题，具体来说是确保运动的准确性和稳定性。</li>
<li>methods: 这种研究使用了一种名为EqMotion的新型动作预测模型，该模型具有几何变换等价性和人工对话等变换性，从而使得模型在不同的坐标系下仍然可以准确预测动作。此外，该研究还引入了一种具有几何变换等价性的高清地图处理方法，以增强网络的空间理解。</li>
<li>results: 该研究表明，通过使用EqMotion模型和高清地图处理方法，可以实现高精度的动作预测，同时具有轻量级的设计和高效的数据利用。<details>
<summary>Abstract</summary>
In autonomous driving, deep learning enabled motion prediction is a popular topic. A critical gap in traditional motion prediction methodologies lies in ensuring equivariance under Euclidean geometric transformations and maintaining invariant interaction relationships. This research introduces a groundbreaking solution by employing EqMotion, a theoretically geometric equivariant and interaction invariant motion prediction model for particles and humans, plus integrating agent-equivariant high-definition (HD) map features for context aware motion prediction in autonomous driving. The use of EqMotion as backbone marks a significant departure from existing methods by rigorously ensuring motion equivariance and interaction invariance. Equivariance here implies that an output motion must be equally transformed under the same Euclidean transformation as an input motion, while interaction invariance preserves the manner in which agents interact despite transformations. These properties make the network robust to arbitrary Euclidean transformations and contribute to more accurate prediction. In addition, we introduce an equivariant method to process the HD map to enrich the spatial understanding of the network while preserving the overall network equivariance property. By applying these technologies, our model is able to achieve high prediction accuracy while maintain a lightweight design and efficient data utilization.
</details>
<details>
<summary>摘要</summary>
自主驾驶中，深度学习启用的动作预测是一个受欢迎的话题。传统的动作预测方法存在一个重要的缺陷，即保证动作 equivariant 和 interaction invariant。这项研究提出了一种创新的解决方案，通过使用EqMotion，一种 theoretically 几何 equivariant 和 interaction invariant 的动作预测模型，以及 Agent-equivariant HD map 特征进行上下文意识激活的动作预测。使用EqMotion 作为基础marks a significant departure from existing methods， rigorously ensuring motion equivariance and interaction invariance。 equivariance 在这里意味着输入动作下的输出动作必须在同一个几何变换下具有相同的变换，而 interaction invariance 保持了代理人之间的交互方式不变，即使在变换下。这些特性使得网络具有对任意几何变换的 Robustness 和更高的预测精度。此外，我们还引入了一种具有 equivariance 性的方法来处理 HD map，以激活网络的空间理解，而不损失整体网络的 equivariance 性。通过应用这些技术，我们的模型能够实现高精度的预测，同时具有轻量级的设计和高效的数据利用。
</details></li>
</ul>
<hr>
<h2 id="Southern-Ocean-Dynamics-Under-Climate-Change-New-Knowledge-Through-Physics-Guided-Machine-Learning"><a href="#Southern-Ocean-Dynamics-Under-Climate-Change-New-Knowledge-Through-Physics-Guided-Machine-Learning" class="headerlink" title="Southern Ocean Dynamics Under Climate Change: New Knowledge Through Physics-Guided Machine Learning"></a>Southern Ocean Dynamics Under Climate Change: New Knowledge Through Physics-Guided Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13916">http://arxiv.org/abs/2310.13916</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yikwill/THOR-MOM6">https://github.com/yikwill/THOR-MOM6</a></li>
<li>paper_authors: William Yik, Maike Sonnewald, Mariana C. A. Clare, Redouane Lguensat</li>
<li>for: 本研究旨在理解由气候变化引起的南极环流Current的变化，以及这些变化对南极环流Current的影响。</li>
<li>methods: 本研究使用了Tracking global Heating with Ocean Regimes（THOR）方法，将高分辨率气候模型数据分解成不同的物理 régime，并使用神经网络模型预测这些 régime的变化。</li>
<li>results: 研究发现，随着气候变化，南极环流Current在interactions with the Pacific-Antarctic Ridge region发生了 régime shift，其中流速增强，而 bathymetry的 dominant dynamical role weakens。<details>
<summary>Abstract</summary>
Complex ocean systems such as the Antarctic Circumpolar Current play key roles in the climate, and current models predict shifts in their strength and area under climate change. However, the physical processes underlying these changes are not well understood, in part due to the difficulty of characterizing and tracking changes in ocean physics in complex models. To understand changes in the Antarctic Circumpolar Current, we extend the method Tracking global Heating with Ocean Regimes (THOR) to a mesoscale eddy permitting climate model and identify regions of the ocean characterized by similar physics, called dynamical regimes, using readily accessible fields from climate models. To this end, we cluster grid cells into dynamical regimes and train an ensemble of neural networks to predict these regimes and track them under climate change. Finally, we leverage this new knowledge to elucidate the dynamics of regime shifts. Here we illustrate the value of this high-resolution version of THOR, which allows for mesoscale turbulence, with a case study of the Antarctic Circumpolar Current and its interactions with the Pacific-Antarctic Ridge. In this region, THOR specifically reveals a shift in dynamical regime under climate change driven by changes in wind stress and interactions with bathymetry. Using this knowledge to guide further exploration, we find that as the Antarctic Circumpolar Current shifts north under intensifying wind stress, the dominant dynamical role of bathymetry weakens and the flow strengthens.
</details>
<details>
<summary>摘要</summary>
COMPLEX ocean systems, such as the Antarctic Circumpolar Current, play key roles in the climate, and current models predict shifts in their strength and area under climate change. However, the physical processes underlying these changes are not well understood, in part due to the difficulty of characterizing and tracking changes in ocean physics in complex models. To understand changes in the Antarctic Circumpolar Current, we extend the method Tracking global Heating with Ocean Regimes (THOR) to a mesoscale eddy permitting climate model and identify regions of the ocean characterized by similar physics, called dynamical regimes, using readily accessible fields from climate models. To this end, we cluster grid cells into dynamical regimes and train an ensemble of neural networks to predict these regimes and track them under climate change. Finally, we leverage this new knowledge to elucidate the dynamics of regime shifts. Here we illustrate the value of this high-resolution version of THOR, which allows for mesoscale turbulence, with a case study of the Antarctic Circumpolar Current and its interactions with the Pacific-Antarctic Ridge. In this region, THOR specifically reveals a shift in dynamical regime under climate change driven by changes in wind stress and interactions with bathymetry. Using this knowledge to guide further exploration, we find that as the Antarctic Circumpolar Current shifts north under intensifying wind stress, the dominant dynamical role of bathymetry weakens and the flow strengthens.
</details></li>
</ul>
<hr>
<h2 id="Pre-Training-on-Large-Scale-Generated-Docking-Conformations-with-HelixDock-to-Unlock-the-Potential-of-Protein-ligand-Structure-Prediction-Models"><a href="#Pre-Training-on-Large-Scale-Generated-Docking-Conformations-with-HelixDock-to-Unlock-the-Potential-of-Protein-ligand-Structure-Prediction-Models" class="headerlink" title="Pre-Training on Large-Scale Generated Docking Conformations with HelixDock to Unlock the Potential of Protein-ligand Structure Prediction Models"></a>Pre-Training on Large-Scale Generated Docking Conformations with HelixDock to Unlock the Potential of Protein-ligand Structure Prediction Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13913">http://arxiv.org/abs/2310.13913</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lihang Liu, Donglong He, Xianbin Ye, Shanzhuo Zhang, Xiaonan Zhang, Jingbo Zhou, Jun Li, Hua Chai, Fan Wang, Jingzhou He, Liang Zheng, Yonghui Li, Xiaomin Fang</li>
<li>for: 这篇论文旨在提高分子对抗位置预测的精确性，以推动人工智能驱动的药物探索。</li>
<li>methods: 这篇论文使用了深度学习技术来强化分子对抗位置预测，并且使用了大量的蛋白质和小分子组合数据进行预训。</li>
<li>results: 相比传统物理学基于的基底方法和深度学习基于的基底方法，这篇论文的HelixDock方法在复杂的测试集上表现出了superiority，尤其是在蛋白质和小分子之间的对抗位置预测方面。<details>
<summary>Abstract</summary>
Molecular docking, a pivotal computational tool for drug discovery, predicts the binding interactions between small molecules (ligands) and target proteins (receptors). Conventional physics-based docking tools, though widely used, face limitations in precision due to restricted conformational sampling and imprecise scoring functions. Recent endeavors have employed deep learning techniques to enhance docking accuracy, but their generalization remains a concern due to limited training data. Leveraging the success of extensive and diverse data in other domains, we introduce HelixDock, a novel approach for site-specific molecular docking. Hundreds of millions of binding poses are generated by traditional docking tools, encompassing diverse protein targets and small molecules. Our deep learning-based docking model, a SE(3)-equivariant network, is pre-trained with this large-scale dataset and then fine-tuned with a small number of precise receptor-ligand complex structures. Comparative analyses against physics-based and deep learning-based baseline methods highlight HelixDock's superiority, especially on challenging test sets. Our study elucidates the scaling laws of the pre-trained molecular docking models, showcasing consistent improvements with increased model parameters and pre-train data quantities. Harnessing the power of extensive and diverse generated data holds promise for advancing AI-driven drug discovery.
</details>
<details>
<summary>摘要</summary>
分子停靠，一种重要的计算工具，预测小分子（抗体）与目标蛋白（受体）之间的绑定交互。传统的物理学基于的停靠工具，尽管广泛使用，但受限于精确性，因为它们只能进行有限的配置检查和不准确的评分函数。现在的努力是使用深度学习技术来提高停靠精度，但它们的泛化仍然是一个问题，因为它们只有有限的训练数据。我们在其他领域中的丰富和多样化数据的基础上引入了 HelixDock，一种新的方法。我们使用传统的停靠工具生成了数百万个绑定位置，涵盖了多种蛋白目标和小分子。我们的深度学习基于的停靠模型，一个SE(3)相似的网络，通过大规模数据集预训练，然后精度地调整了一个小数量的准确抗体-小分子复合结构。与物理学基于和深度学习基于的基线方法进行比较分析，HelixDock在具有挑战性的测试集上表现出优异性，特别是在难以预测的情况下。我们的研究描述了预训练分子停靠模型的涨幅法律，展示了随着模型参数和预训练数据量的增加，模型的性能得到了一致提高。通过利用广泛生成的数据来提高人工智能驱动的药物探索，我们希望能够推动这一领域的发展。
</details></li>
</ul>
<hr>
<h2 id="Towards-Hyperparameter-Agnostic-DNN-Training-via-Dynamical-System-Insights"><a href="#Towards-Hyperparameter-Agnostic-DNN-Training-via-Dynamical-System-Insights" class="headerlink" title="Towards Hyperparameter-Agnostic DNN Training via Dynamical System Insights"></a>Towards Hyperparameter-Agnostic DNN Training via Dynamical System Insights</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13901">http://arxiv.org/abs/2310.13901</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carmel Fiscko, Aayushya Agarwal, Yihan Ruan, Soummya Kar, Larry Pileggi, Bruno Sinopoli<br>for:ECCO-DNN is designed to optimize deep neural network training.methods:ECCO-DNN uses a stochastic first-order optimization method that models the optimization variable trajectory as a dynamical system and adaptively selects step sizes based on the trajectory’s shape.results:ECCO-DNN achieves comparable performance to state-of-the-art optimizers including ADAM, SGD, RMSProp, and AdaGrad, and its single hyperparameter can be changed by three orders of magnitude without affecting the trained models’ accuracies. Additionally, ECCO-DNN is insensitive to hyperparameter variations and reduces the data and computation needed for hyperparameter tuning, making it advantageous for rapid prototyping and for applications with new datasets.<details>
<summary>Abstract</summary>
We present a stochastic first-order optimization method specialized for deep neural networks (DNNs), ECCO-DNN. This method models the optimization variable trajectory as a dynamical system and develops a discretization algorithm that adaptively selects step sizes based on the trajectory's shape. This provides two key insights: designing the dynamical system for fast continuous-time convergence and developing a time-stepping algorithm to adaptively select step sizes based on principles of numerical integration and neural network structure. The result is an optimizer with performance that is insensitive to hyperparameter variations and that achieves comparable performance to state-of-the-art optimizers including ADAM, SGD, RMSProp, and AdaGrad. We demonstrate this in training DNN models and datasets, including CIFAR-10 and CIFAR-100 using ECCO-DNN and find that ECCO-DNN's single hyperparameter can be changed by three orders of magnitude without affecting the trained models' accuracies. ECCO-DNN's insensitivity reduces the data and computation needed for hyperparameter tuning, making it advantageous for rapid prototyping and for applications with new datasets. To validate the efficacy of our proposed optimizer, we train an LSTM architecture on a household power consumption dataset with ECCO-DNN and achieve an optimal mean-square-error without tuning hyperparameters.
</details>
<details>
<summary>摘要</summary>
我们提出了一种随机首频优化方法特化于深度神经网络（DNN），即ECCO-DNN。该方法将优化变量轨迹模型为动力系统，并开发了一种适应步长选择算法，以获得两个关键发现：设计动力系统以实现快速连续时间减少，并开发一种时间步骤算法以自适应选择步长，基于数值积分和神经网络结构。这些设计决策使ECCO-DNN的优化器性能免疫参数变化的影响，并与现有的优化器，包括ADAM、SGD、RMSProp和AdaGrad，具有相同的性能。我们在训练DNN模型和数据集中使用ECCO-DNN，包括CIFAR-10和CIFAR-100，并发现ECCO-DNN的单个超参数可以通过三个级别的变化而不影响训练模型的准确性。ECCO-DNN的不敏感性降低了数据和计算所需的 hyperparameter 调试，使其在快速原型和新数据集应用中更加优势。为证明我们提出的优化器的有效性，我们在一个家用电力消耗数据集上使用ECCO-DNN训练LSTM架构，并实现了最佳平均方差值，无需调整超参数。
</details></li>
</ul>
<hr>
<h2 id="Masked-Hard-Attention-Transformers-and-Boolean-RASP-Recognize-Exactly-the-Star-Free-Languages"><a href="#Masked-Hard-Attention-Transformers-and-Boolean-RASP-Recognize-Exactly-the-Star-Free-Languages" class="headerlink" title="Masked Hard-Attention Transformers and Boolean RASP Recognize Exactly the Star-Free Languages"></a>Masked Hard-Attention Transformers and Boolean RASP Recognize Exactly the Star-Free Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13897">http://arxiv.org/abs/2310.13897</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dana Angluin, David Chiang, Andy Yang</li>
<li>for: 这个论文研究了 transformer Encoder 的可能性和逻辑限制。</li>
<li>methods: 这个论文使用了硬 attention 和严格未来masking 方法，并证明了这种网络可以认izers star-free 语言。添加位域 embedding 可以扩展认izers 到其他已知类别。</li>
<li>results: 这个论文证明了 transformer 网络可以认izers star-free 语言，并与 first-order logic、 temporal logic 和 algebraic automata theory 有关。<details>
<summary>Abstract</summary>
We consider transformer encoders with hard attention (in which all attention is focused on exactly one position) and strict future masking (in which each position only attends to positions strictly to its left), and prove that the class of languages recognized by these networks is exactly the star-free languages. Adding position embeddings increases the class of recognized languages to other well-studied classes. A key technique in these proofs is Boolean RASP, a variant of RASP that is restricted to Boolean values. Via the star-free languages, we relate transformers to first-order logic, temporal logic, and algebraic automata theory.
</details>
<details>
<summary>摘要</summary>
我们考虑使用转换器Encoder，具有固定注意力（所有注意力都集中在一个位置）和严格未来掩码（每个位置只能关注左侧的位置），并证明这些网络可以认可的语言类型是星号自由语言。添加位域嵌入可以将认可的语言类型扩展到其他已有的类型。我们使用布尔RASP，一种受限的RASP变体，进行关键技术。通过星号自由语言，我们将转换器相关联到首领逻辑、时间逻辑和代数自动机理论。
</details></li>
</ul>
<hr>
<h2 id="Specify-Robust-Causal-Representation-from-Mixed-Observations"><a href="#Specify-Robust-Causal-Representation-from-Mixed-Observations" class="headerlink" title="Specify Robust Causal Representation from Mixed Observations"></a>Specify Robust Causal Representation from Mixed Observations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13892">http://arxiv.org/abs/2310.13892</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ymy4323460/cari">https://github.com/ymy4323460/cari</a></li>
<li>paper_authors: Mengyue Yang, Xinyu Cai, Furui Liu, Weinan Zhang, Jun Wang</li>
<li>for: 本研究旨在学习从观察数据中学习一种低维度、紧凑的表示，以提高预测模型的稳定性和泛化性。</li>
<li>methods: 本研究使用了 causal 表示学习方法，通过在学习过程中添加 mutual information 度量来规范学习。</li>
<li>results: 研究表明，使用 causal 表示学习方法可以提高模型的Robustness 和泛化性，并且在骚扰攻击和分布Shift 下表现更好于基eline。I hope that helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
Learning representations purely from observations concerns the problem of learning a low-dimensional, compact representation which is beneficial to prediction models. Under the hypothesis that the intrinsic latent factors follow some casual generative models, we argue that by learning a causal representation, which is the minimal sufficient causes of the whole system, we can improve the robustness and generalization performance of machine learning models. In this paper, we develop a learning method to learn such representation from observational data by regularizing the learning procedure with mutual information measures, according to the hypothetical factored causal graph. We theoretically and empirically show that the models trained with the learned causal representations are more robust under adversarial attacks and distribution shifts compared with baselines. The supplementary materials are available at https://github.com/ymy $4323460 / \mathrm{CaRI} /$.
</details>
<details>
<summary>摘要</summary>
学习 purely from observations 的表示 Concerns the problem of learning a low-dimensional, compact representation, which is beneficial to prediction models. Under the assumption that the intrinsic latent factors follow some causal generative models, we argue that by learning a causal representation, which is the minimal sufficient causes of the whole system, we can improve the robustness and generalization performance of machine learning models. In this paper, we develop a learning method to learn such representation from observational data by regularizing the learning procedure with mutual information measures, according to the hypothetical factored causal graph. We theoretically and empirically show that the models trained with the learned causal representations are more robust under adversarial attacks and distribution shifts compared with baselines. 附加资料可以在 https://github.com/ymy $4323460 / \mathrm{CaRI} /$ 中找到。
</details></li>
</ul>
<hr>
<h2 id="Towards-a-General-Framework-for-Continual-Learning-with-Pre-training"><a href="#Towards-a-General-Framework-for-Continual-Learning-with-Pre-training" class="headerlink" title="Towards a General Framework for Continual Learning with Pre-training"></a>Towards a General Framework for Continual Learning with Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13888">http://arxiv.org/abs/2310.13888</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thu-ml/hide-prompt">https://github.com/thu-ml/hide-prompt</a></li>
<li>paper_authors: Liyuan Wang, Jingyi Xie, Xingxing Zhang, Hang Su, Jun Zhu</li>
<li>for: 本研究探讨了一种通用框架，用于Sequential continual learning tasks，通过预训练来实现人工智能系统适应真实世界动态变化。</li>
<li>methods: 我们在理论上将目标函数 decomposed into three hierarchical components，包括 within-task prediction、task-identity inference 和 task-adaptive prediction。然后，我们提出了一种新的方法，使用 parameter-efficient fine-tuning (PEFT) 技术和 representation statistics 来显著提高这些组件。</li>
<li>results: 我们在下游 continual learning 中观察到了我们的方法的优势和通用性，并进一步探讨了 PEFT 技术在上游 continual learning 中的应用可能性。此外，我们还讨论了该框架与 neuroscience 最新的进展之间的生物基础。<details>
<summary>Abstract</summary>
In this work, we present a general framework for continual learning of sequentially arrived tasks with the use of pre-training, which has emerged as a promising direction for artificial intelligence systems to accommodate real-world dynamics. From a theoretical perspective, we decompose its objective into three hierarchical components, including within-task prediction, task-identity inference, and task-adaptive prediction. Then we propose an innovative approach to explicitly optimize these components with parameter-efficient fine-tuning (PEFT) techniques and representation statistics. We empirically demonstrate the superiority and generality of our approach in downstream continual learning, and further explore the applicability of PEFT techniques in upstream continual learning. We also discuss the biological basis of the proposed framework with recent advances in neuroscience.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们提出了一种总体框架，用于Sequential continual learning，利用预训练，这种方向已成为人工智能系统来应对实际世界动态的一种有前途的方法。从理论上来看，我们将目标函数分解成三个层次结构，包括内部任务预测、任务标识推理和任务适应预测。然后，我们提议一种新的方法，使用参数效率的细致调整（PEFT）技术和表示统计来显著地优化这些组成部分。我们在下游 continual learning 中进行了实验，证明了我们的方法的优越性和通用性。此外，我们还探讨了这个框架的生物基础，与 neuroscience 最新的进展有关。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Transport-based-Nonlinear-Filtering-in-High-dimensional-Settings"><a href="#Optimal-Transport-based-Nonlinear-Filtering-in-High-dimensional-Settings" class="headerlink" title="Optimal Transport-based Nonlinear Filtering in High-dimensional Settings"></a>Optimal Transport-based Nonlinear Filtering in High-dimensional Settings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13886">http://arxiv.org/abs/2310.13886</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Al-Jarrah, Niyizhen Jin, Bamdad Hosseini, Amirhossein Taghvaei</li>
<li>for: 本文解决非线性滤波问题，即计算随机动力系统状态条件分布给历史噪声部分观测的问题。</li>
<li>methods: 我们提出的方法基于非线性滤波的最优运输解释，导致一种基于模拟和无概率算法的 simulation-based 和 likelihood-free 算法，可以估计当前状态分布到下一步时间Step的 Brenier 最优运输Map。</li>
<li>results: 我们的方法比 SIR 滤波和ensemble Kalman filter 表现出更高的样本效率、高维度可描述性和能够捕捉复杂多模分布的能力。<details>
<summary>Abstract</summary>
This paper addresses the problem of nonlinear filtering, i.e., computing the conditional distribution of the state of a stochastic dynamical system given a history of noisy partial observations. The primary focus is on scenarios involving degenerate likelihoods or high-dimensional states, where traditional sequential importance resampling (SIR) particle filters face the weight degeneracy issue. Our proposed method builds on an optimal transport interpretation of nonlinear filtering, leading to a simulation-based and likelihood-free algorithm that estimates the Brenier optimal transport map from the current distribution of the state to the distribution at the next time step. Our formulation allows us to harness the approximation power of neural networks to model complex and multi-modal distributions and employ stochastic optimization algorithms to enhance scalability. Extensive numerical experiments are presented that compare our method to the SIR particle filter and the ensemble Kalman filter, demonstrating the superior performance of our method in terms of sample efficiency, high-dimensional scalability, and the ability to capture complex and multi-modal distributions.
</details>
<details>
<summary>摘要</summary>
Our proposed method is based on an optimal transport interpretation of nonlinear filtering, which leads to a simulation-based and likelihood-free algorithm that estimates the Brenier optimal transport map from the current distribution of the state to the distribution at the next time step. By using neural networks to model complex and multi-modal distributions and stochastic optimization algorithms to enhance scalability, our formulation is able to handle challenging scenarios with ease.Extensive numerical experiments are presented that compare our method to the SIR particle filter and the ensemble Kalman filter. The results demonstrate the superior performance of our method in terms of sample efficiency, high-dimensional scalability, and the ability to capture complex and multi-modal distributions.
</details></li>
</ul>
<hr>
<h2 id="Fast-Approximation-of-Similarity-Graphs-with-Kernel-Density-Estimation"><a href="#Fast-Approximation-of-Similarity-Graphs-with-Kernel-Density-Estimation" class="headerlink" title="Fast Approximation of Similarity Graphs with Kernel Density Estimation"></a>Fast Approximation of Similarity Graphs with Kernel Density Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13870">http://arxiv.org/abs/2310.13870</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pmacg/kde-similarity-graph">https://github.com/pmacg/kde-similarity-graph</a></li>
<li>paper_authors: Peter Macgregor, He Sun</li>
<li>for: 构建一个稀疏的相似图，以便进行现代归一化算法的第一步。</li>
<li>methods: 基于kernel density estimation问题，提出了一种新的算法框架，可以快速构建稀疏的相似图，同时保持归一化结果的结构。</li>
<li>results: 与scikit-learn和FAISS库的实现相比，我们的方法在多种数据集上显著超越了它们。<details>
<summary>Abstract</summary>
Constructing a similarity graph from a set $X$ of data points in $\mathbb{R}^d$ is the first step of many modern clustering algorithms. However, typical constructions of a similarity graph have high time complexity, and a quadratic space dependency with respect to $|X|$. We address this limitation and present a new algorithmic framework that constructs a sparse approximation of the fully connected similarity graph while preserving its cluster structure. Our presented algorithm is based on the kernel density estimation problem, and is applicable for arbitrary kernel functions. We compare our designed algorithm with the well-known implementations from the scikit-learn library and the FAISS library, and find that our method significantly outperforms the implementation from both libraries on a variety of datasets.
</details>
<details>
<summary>摘要</summary>
现在的许多现代聚类算法的第一步是从一个集合 $X$ 中的数据点集构建一个相似Graph。然而，通常情况下，构建相似图的方法具有高时间复杂度和对 $|X|$ 的平方空间依赖。我们解决这个限制，并提出了一个新的算法框架，可以构建稀疏的相似图，保持聚类结构。我们的提出的算法基于kernel density估计问题，适用于任意的kernel函数。我们与scikit-learn库和FAISS库中的常用实现进行比较，发现我们的方法在多种数据集上明显超过了这两个库的实现。
</details></li>
</ul>
<hr>
<h2 id="Distributionally-Robust-Optimization-with-Bias-and-Variance-Reduction"><a href="#Distributionally-Robust-Optimization-with-Bias-and-Variance-Reduction" class="headerlink" title="Distributionally Robust Optimization with Bias and Variance Reduction"></a>Distributionally Robust Optimization with Bias and Variance Reduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13863">http://arxiv.org/abs/2310.13863</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Ronak Mehta, Vincent Roulet, Krishna Pillutla, Zaid Harchaoui</li>
<li>for: 该研究 targets the distributionally robust optimization (DRO) problem with spectral risk-based uncertainty set and $f$-divergence penalty, which includes common risk-sensitive learning objectives such as regularized condition value-at-risk (CVaR) and average top-$k$ loss.</li>
<li>methods: 该研究提出了一种名为 Prospect 的杂相 gradient-based algorithm，该算法只需要调整一个学习率参数，并且可以 linear convergence for smooth regularized losses。这与之前的算法不同，这些算法可能需要调整多个参数，或者因为偏向的梯度估计或不够的迁移而失败。</li>
<li>results: 该研究通过实验表明，Prospect 可以比基eline 2-3倍快速 converges on distribution shift and fairness benchmarks spanning tabular, vision, and language domains。<details>
<summary>Abstract</summary>
We consider the distributionally robust optimization (DRO) problem with spectral risk-based uncertainty set and $f$-divergence penalty. This formulation includes common risk-sensitive learning objectives such as regularized condition value-at-risk (CVaR) and average top-$k$ loss. We present Prospect, a stochastic gradient-based algorithm that only requires tuning a single learning rate hyperparameter, and prove that it enjoys linear convergence for smooth regularized losses. This contrasts with previous algorithms that either require tuning multiple hyperparameters or potentially fail to converge due to biased gradient estimates or inadequate regularization. Empirically, we show that Prospect can converge 2-3$\times$ faster than baselines such as stochastic gradient and stochastic saddle-point methods on distribution shift and fairness benchmarks spanning tabular, vision, and language domains.
</details>
<details>
<summary>摘要</summary>
我团队考虑了分布robust优化（DRO）问题，使用spectral risk-based uncertainty set和$f$- divergence penalty。这种形式包括常见的风险敏感学习目标，如常量值-at-risk（CVaR）和平均top-$k$ loss。我们提出了Prospect算法，它仅需要调整一个学习率超参数，并证明它在抽象化的REGularized loss函数下具有线性减少的性质。这与之前的算法不同，这些算法可能需要调整多个超参数，或者因为偏向的梯度估计或不足的正则化而无法 converges。我们在实验中证明，Prospect可以比基eline算法快速 converge 2-3倍，包括在分布shift和公平性 benchmark上。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/21/cs.LG_2023_10_21/" data-id="cloimipco00qis4883vev8p3k" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_10_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/21/eess.SP_2023_10_21/" class="article-date">
  <time datetime="2023-10-21T08:00:00.000Z" itemprop="datePublished">2023-10-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/21/eess.SP_2023_10_21/">eess.SP - 2023-10-21</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Non-sliced-Optical-Arbitrary-Waveform-Measurement-OAWM-Using-a-Silicon-Photonic-Receiver-Chip"><a href="#Non-sliced-Optical-Arbitrary-Waveform-Measurement-OAWM-Using-a-Silicon-Photonic-Receiver-Chip" class="headerlink" title="Non-sliced Optical Arbitrary Waveform Measurement (OAWM) Using a Silicon Photonic Receiver Chip"></a>Non-sliced Optical Arbitrary Waveform Measurement (OAWM) Using a Silicon Photonic Receiver Chip</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17662">http://arxiv.org/abs/2310.17662</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Drayss, Dengyang Fang, Christoph Füllner, Wolfgang Freude, Sebastian Randel, Christian Koos</li>
<li>for: 这篇论文是为了探讨 comb-based optical arbitrary waveform measurement (OAWM) 技术的可能性和应用前景而写的。</li>
<li>methods: 该论文使用了一种基于 silicon photonic chip 的 OAWM 前端，不需要 slice filters 也不需要活动控制。它包含四个 IQ 接收器，通过使用 femtosecond 模糊化 Laser 进行精确准化，可以达到 170 GHz 的总采集带宽。</li>
<li>results: 该 OAWM 系统可以成功地测量 sinusoidal 测试信号，并达到 SINAD 30 dB 和 ENOB 4.7 bit。它还可以接收 64QAM 数据信号，并在 symbol rates 达到 100 GBd 时实现 constellation signal-to-noise ratios (CSNR) 与 conventional coherent receivers 相当。在理论上，我们发现可以通过增加非分割 OAWM 系统的通道数来提高捕获带宽和信号质量。<details>
<summary>Abstract</summary>
Comb-based optical arbitrary waveform measurement (OAWM) techniques can overcome the bandwidth limitations of conventional coherent detection schemes and may have disruptive impact on a wide range of scientific and industrial applications. Over the previous years, different OAWM schemes have been demonstrated, showing the performance and the application potential of the concept in laboratory experiments. However, these demonstrations still relied on discrete fiber-optic components or on combinations of discrete coherent receivers with integrated optical slicing filters that require complex tuning procedures to achieve the desired performance. In this paper, we demonstrate the first wavelength-agnostic OAWM front-end that is integrated on a compact silicon photonic chip and that neither requires slicing filters nor active controls. Our OAWM system comprises four IQ receivers, which are accurately calibrated using a femtosecond mode-locked laser and which offer a total acquisition bandwidth of 170 GHz. Using sinusoidal test signals, we measure a signal-to-noise-and-distortion ratio (SINAD) of 30 dB for the reconstructed signal, which corresponds to an effective number of bits (ENOB) of 4.7 bit, where the underlying electronic analog-to-digital converters (ADC) turn out to be the main limitation. The performance of the OAWM system is further demonstrated by receiving 64QAM data signals at symbol rates of up to 100 GBd, achieving constellation signal-to-noise ratios (CSNR) that are on par with those obtained for conventional coherent receivers. In a theoretical scalability analysis, we show that increasing the channel count of non-sliced OAWM systems can improve both the acquisition bandwidth and the signal quality. We believe that our work represents a key step towards out-of-lab use of highly compact OAWM systems that rely on chip-scale integrated optical front-ends.
</details>
<details>
<summary>摘要</summary>
optical Arbitrary Waveform Measurement（OAWM）技术可以超越传统同步探测方案的频率限制，并可能在广泛的科学和工业应用中产生干扰性的影响。过去几年，不同的OAWM方案在实验室中得到了证明，并显示了这种概念的性能和应用潜力。但是，这些实验仍然基于离散的纤维仪件或者离散的同步接收器和集成仪件的组合，需要复杂的调试过程来实现所需的性能。在这篇论文中，我们展示了首个不需要探针过滤器，也不需要活动控制的OAWM前端。我们的OAWM系统包括四个IQ接收器，它们通过使用 Femtosecond 模拟激光来精准寻定，并具有170 GHz的总获得带宽。使用正弦测试信号，我们测量到了重建信号的噪声比为30 dB，相应的有效数 bits（ENOB）为4.7 bit，其中下面的电子分析数字转换器（ADC）是主要的限制。我们的OAWM系统的性能进一步得到了证明，可以接收64QAM数据信号，并达到与传统同步接收器相同的符号信号噪声比（CSNR）。在理论上，我们发现了不需要探针过滤器的OAWM系统的频率Count的提高可以提高获得带宽和信号质量。我们认为，我们的工作代表了同步探测系统的快速出场的关键一步。
</details></li>
</ul>
<hr>
<h2 id="Green-Beamforming-Design-for-Integrated-Sensing-and-Communication-Systems-A-Practical-Approach-Using-Beam-Matching-Error-Metrics"><a href="#Green-Beamforming-Design-for-Integrated-Sensing-and-Communication-Systems-A-Practical-Approach-Using-Beam-Matching-Error-Metrics" class="headerlink" title="Green Beamforming Design for Integrated Sensing and Communication Systems: A Practical Approach Using Beam-Matching Error Metrics"></a>Green Beamforming Design for Integrated Sensing and Communication Systems: A Practical Approach Using Beam-Matching Error Metrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13993">http://arxiv.org/abs/2310.13993</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luping Xiang, Ke Xu, Jie Hu, Kun Yang</li>
<li>for: 本研究提出了一种绿色扫描设计方案，用于 интеграted sensing and communication（ISAC）系统，使用扫描性能评估中的扫描差异来评估雷达性能。</li>
<li>methods: 本研究使用了semidefinite relaxation（SDR）方法和iterative rank minimization algorithm（IRM）来解决扫描设计中的非对称挑战，并且应用了扫描模式匹配错误来评估雷达性能。</li>
<li>results: 实验结果表明，提出的优化扫描设计方案具有非常出色的扫描性能，强调了雷达组件在探测任务中的特出表现。<details>
<summary>Abstract</summary>
In this paper, we propose a green beamforming design for the integrated sensing and communication (ISAC) system, using beam-matching error to assess radar performance. The beam-matching error metric, which considers the mean square error between the desired and designed beam patterns, provides a more practical evaluation approach. To tackle the non-convex challenge inherent in beamforming design, we apply semidefinite relaxation (SDR) to address the rank-one relaxation issue, followed by the iterative rank minimization algorithm (IRM) for rank-one recovery. The simulation results showcase the effectiveness of our proposed optimal beamforming design, emphasizing the exceptional performance of the radar component in sensing tasks.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种绿色扩散设计方案，用于 интегрирован的感测和通信（ISAC）系统，使用束匹配错误来评估雷达性能。束匹配错误指标，考虑了想要的束 pattern和设计的束 pattern之间的平均方差，提供了更实用的评估方法。为了解决扩散设计中的非 convex 挑战，我们使用半definite relaxation（SDR）来解决级数一relaxation问题，然后使用迭代rank minimization algorithm（IRM）来实现级数一回复。实验结果显示了我们提出的优化扩散设计的效iveness，强调了雷达组件在感测任务中的出色性能。
</details></li>
</ul>
<hr>
<h2 id="Robust-NOMA-assisted-OTFS-ISAC-Network-Design-with-3D-Motion-Prediction-Topology"><a href="#Robust-NOMA-assisted-OTFS-ISAC-Network-Design-with-3D-Motion-Prediction-Topology" class="headerlink" title="Robust NOMA-assisted OTFS-ISAC Network Design with 3D Motion Prediction Topology"></a>Robust NOMA-assisted OTFS-ISAC Network Design with 3D Motion Prediction Topology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13984">http://arxiv.org/abs/2310.13984</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luping Xiang, Ke Xu, Jie Hu, Christos Masouros, Kun Yang</li>
<li>for: 提供一个基于非正交多存取（NOMA）和时频空间（OTFS）的数据通信和感知（ISAC）网络，以支持多个用户。</li>
<li>methods: 使用无人航空车（UAV）作为空中基站，并使用ISAC来提取用户的位置和速度信息，并实现非正交功率分配以获得更高的可达速率。</li>
<li>results: 透过3D动态预测拓扑导引NOMA传输，并提出一个可靠的功率分配解决方案，以解决做为MMF和SR问题。 simulation结果显示，提案的NOMA-assisted OTFS-ISAC系统在具有3D动态预测拓扑的情况下，在完美和不完美频道条件下具有较高的可达速率。<details>
<summary>Abstract</summary>
This paper proposes a novel non-orthogonal multiple access (NOMA)-assisted orthogonal time-frequency space (OTFS)-integrated sensing and communication (ISAC) network, which uses unmanned aerial vehicles (UAVs) as air base stations to support multiple users. By employing ISAC, the UAV extracts position and velocity information from the user's echo signals, and non-orthogonal power allocation is conducted to achieve a superior achievable rate. A 3D motion prediction topology is used to guide the NOMA transmission for multiple users, and a robust power allocation solution is proposed under perfect and imperfect channel estimation for Maxi-min Fairness (MMF) and Maximum sum-Rate (SR) problems. Simulation results demonstrate the superiority of the proposed NOMA-assisted OTFS-ISAC system over other systems in terms of achievable rate under both perfect and imperfect channel conditions with the aid of 3D motion prediction topology.
</details>
<details>
<summary>摘要</summary>
To guide the NOMA transmission for multiple users, a 3D motion prediction topology is used. The network also proposes a robust power allocation solution under both perfect and imperfect channel estimation for Maximum Minimum Fairness (MMF) and Maximum Sum-Rate (SR) problems.Simulation results show that the proposed NOMA-assisted OTFS-ISAC system outperforms other systems in terms of achievable rate under both perfect and imperfect channel conditions, with the aid of the 3D motion prediction topology.
</details></li>
</ul>
<hr>
<h2 id="Cloud-Connected-Wireless-Holter-Monitor-Machine-with-Neural-Networks-Based-ECG-Analysis-for-Remote-Health-Monitoring"><a href="#Cloud-Connected-Wireless-Holter-Monitor-Machine-with-Neural-Networks-Based-ECG-Analysis-for-Remote-Health-Monitoring" class="headerlink" title="Cloud-Connected Wireless Holter Monitor Machine with Neural Networks Based ECG Analysis for Remote Health Monitoring"></a>Cloud-Connected Wireless Holter Monitor Machine with Neural Networks Based ECG Analysis for Remote Health Monitoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13965">http://arxiv.org/abs/2310.13965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Azlaan Ranjha, Laiba Jabbar, Osaid Ahmed</li>
<li>for: 提高心血管疾病诊断的准确性</li>
<li>methods: 使用无线电心电听器、WIFI数据传输、人工神经网络分类模型</li>
<li>results: 实现心血管疾病诊断的准确率超过88%，提供了一种快速、准确且cost-effective的心血管诊断方案<details>
<summary>Abstract</summary>
This study describes the creation of a wireless, transportable Holter monitor to improve the accuracy of cardiac disease diagnosis. The main goal of this study is to develop a low-cost cardiac screening system suited explicitly for underprivileged areas, addressing the rising rates of cardiovascular death. The suggested system includes a wireless Electrocardiogram (ECG) module for real-time cardiac signal gathering using attached electrodes, with data transfer made possible by WiFi to a cloud server for archival and analysis. The system uses a neural network model for automated ECG classification, concentrating on the identification of cardiac anomalies. The diagnostic performance of cardiologist-level ECG analysis is surpassed by our upgraded deep neural network architecture, which underwent thorough evaluation and showed a stunning accuracy rate of more than 88\%. A quick, accurate, and reasonably priced option for cardiac screening is provided by this ground-breaking technology, which smoothly merges wireless data transfer with AI-assisted diagnostics. In addition to providing a thorough overview of the development process, this paper also highlights methods used to improve model accuracy, such as data preparation, class imbalance correction using oversampling, and model fine-tuning. The work shows the viability of a comprehensive remote cardiac screening system powered by AI and maximising the use of wearable and cloud computing resources. Such cutting-edge remote health monitoring technologies have great promise for improved health outcomes and early identification, especially in resource-constrained countries.
</details>
<details>
<summary>摘要</summary>
这项研究描述了一种无线传输式适用于诊断心血管疾病的哨杀器监测系统。研究的主要目标是开发一种低成本的心血管检测系统，特意针对贫困地区，以应对心血管疾病的增长。提议的系统包括一个无线电心电响（ECG）模块，通过附加的电极收集心电信号，并通过WiFi传输数据到云服务器进行存储和分析。系统使用一种升级的深度神经网络模型进行自动ECG分类，主要关注心血管疾病的识别。研究人员通过对模型进行优化，包括数据准备、数据偏置纠正和模型细化，提高了模型的准确率，达到了超过88%。这项创新技术为心血管疾病检测提供了快速、准确、便宜的选择，并将无线数据传输和人工智能助动诊断融合在一起。此外，研究人员还提出了使用云计算和佩戴式设备来实现远程响应心血管疾病检测的可能性。这种前于顶峰的远程卫生监测技术具有很大的推动健康结果和早期识别潜力，特别是在资源受限的国家。
</details></li>
</ul>
<hr>
<h2 id="Goal-oriented-Communications-for-the-IoT-System-Design-and-Adaptive-Resource-Optimization"><a href="#Goal-oriented-Communications-for-the-IoT-System-Design-and-Adaptive-Resource-Optimization" class="headerlink" title="Goal-oriented Communications for the IoT: System Design and Adaptive Resource Optimization"></a>Goal-oriented Communications for the IoT: System Design and Adaptive Resource Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13948">http://arxiv.org/abs/2310.13948</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paolo Di Lorenzo, Mattia Merluzzi, Francesco Binucci, Claudio Battiloro, Paolo Banelli, Emilio Calvanese Strinati, Sergio Barbarossa</li>
<li>for: 本文旨在探讨Internet of Things（IoT）应用程序中的资源限制问题，包括频率、能源、计算、学习和推理能力等。</li>
<li>methods: 本文提出了一种新的目标带动（GO）IoT系统设计方法，弃察传输数据的精度为主，直接关注数据交换的目标实现。该方法通过系统优化，结合感知、无线通信、计算、学习和控制等方面，提高IoT系统的效果。</li>
<li>results: 本文通过数据示例表明，GO IoT系统设计方法可以在边缘推理、合作感知和联合学习等场景中实现显著的提高。这些示例表明了该方法的实际应用和现实意义，有potentiality to revolutionize IoT systems。<details>
<summary>Abstract</summary>
Internet of Things (IoT) applications combine sensing, wireless communication, intelligence, and actuation, enabling the interaction among heterogeneous devices that collect and process considerable amounts of data. However, the effectiveness of IoT applications needs to face the limitation of available resources, including spectrum, energy, computing, learning and inference capabilities. This paper challenges the prevailing approach to IoT communication, which prioritizes the usage of resources in order to guarantee perfect recovery, at the bit level, of the data transmitted by the sensors to the central unit. We propose a novel approach, called goal-oriented (GO) IoT system design, that transcends traditional bit-related metrics and focuses directly on the fulfillment of the goal motivating the exchange of data. The improvement is then achieved through a comprehensive system optimization, integrating sensing, communication, computation, learning, and control. We provide numerical results demonstrating the practical applications of our methodology in compelling use cases such as edge inference, cooperative sensing, and federated learning. These examples highlight the effectiveness and real-world implications of our proposed approach, with the potential to revolutionize IoT systems.
</details>
<details>
<summary>摘要</summary>
互联网物联网（IoT）应用程序将感知、无线通信、智能和动作结合起来，使不同设备之间进行数据交换和处理。然而，IoT应用程序的效iveness需要面临有限的资源，包括频率带、能源、计算、学习和推理能力。本文挑战了现有的IoT通信方法，它强调使用资源以确保数据传输的精度。我们提出了一种新的目标对齐（GO）IoT系统设计方法，它不仅仅是关注传输数据的位数据精度，而是直接关注数据传输的目标。我们通过了一种整体系统优化方法，整合感知、通信、计算、学习和控制，提高系统的效iveness。我们提供了数字结果，证明我们的方法在Edge推理、合作感知和联邦学习等实用应用中的实际应用。这些例子说明了我们的提议的效果和现实意义，它有可能革命化IoT系统。
</details></li>
</ul>
<hr>
<h2 id="Joint-Network-Function-Placement-and-Routing-Optimization-in-Dynamic-Software-defined-Satellite-Terrestrial-Integrated-Networks"><a href="#Joint-Network-Function-Placement-and-Routing-Optimization-in-Dynamic-Software-defined-Satellite-Terrestrial-Integrated-Networks" class="headerlink" title="Joint Network Function Placement and Routing Optimization in Dynamic Software-defined Satellite-Terrestrial Integrated Networks"></a>Joint Network Function Placement and Routing Optimization in Dynamic Software-defined Satellite-Terrestrial Integrated Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13940">http://arxiv.org/abs/2310.13940</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuo Yuan, Yaohua Sun, Mugen Peng</li>
<li>for: 提高软件定义卫星地球Integrated网络（SDSTN）的延迟服务提供，以提高资源灵活性和全球通信覆盖率。</li>
<li>methods: 通过对虚拟网络功能（VNF）布局和路由规划进行集成优化，以适应网络动态特性和低轨道卫星的有限资源。</li>
<li>results: 比基eline方案提高完成服务数量超过58%，并降低服务延迟率大于17%。<details>
<summary>Abstract</summary>
Software-defined satellite-terrestrial integrated networks (SDSTNs) are seen as a promising paradigm for achieving high resource flexibility and global communication coverage. However, low latency service provisioning is still challenging due to the fast variation of network topology and limited onboard resource at low earth orbit satellites. To address this issue, we study service provisioning in SDSTNs via joint optimization of virtual network function (VNF) placement and routing planning with network dynamics characterized by a time-evolving graph. Aiming at minimizing average service latency, the corresponding problem is formulated as an integer nonlinear programming under resource, VNF deployment, and time-slotted flow constraints. Since exhaustive search is intractable, we transform the primary problem into an integer linear programming by involving auxiliary variables and then propose a Benders decomposition based branch-and-cut (BDBC) algorithm. Towards practical use, a time expansion-based decoupled greedy (TEDG) algorithm is further designed with rigorous complexity analysis. Extensive experiments demonstrate the optimality of BDBC algorithm and the low complexity of TEDG algorithm. Meanwhile, it is indicated that they can improve the number of completed services within a configuration period by up to 58% and reduce the average service latency by up to 17% compared to baseline schemes.
</details>
<details>
<summary>摘要</summary>
软件定义卫星地面Integrated networks (SDSTNs) 被看作是一种有前途的方法，以实现高资源灵活性和全球通信覆盖。然而，为了提供低延迟服务，由于网络结构的快速变化和低轨道卫星上的限制资源，仍然是一个挑战。为解决这个问题，我们研究了在SDSTNs中提供服务通过虚拟网络功能（VNF）的分配和路由规划的 JOINT优化。我们的目标是最小化服务延迟，并且将问题转化为一个整数非线性程序，受到资源、VNF部署和时钟分配的约束限制。由于枚举搜索是不可行的，我们将 primal problem 转化为整数线性程序，并提出了基于Benders decomposition的分支和裁剪（BDBC）算法。为实际应用，我们还提出了一种基于时间扩展的分解蜕蜕（TEDG）算法，并进行了严格的复杂性分析。实验结果表明，BDBC算法是优化的，而TEDG算法的复杂性很低。此外，我们发现，它们可以在配置期内完成更多的服务，并将服务延迟降低到17%以上，相比基eline schemes。
</details></li>
</ul>
<hr>
<h2 id="Wideband-Beamforming-for-STAR-RIS-assisted-THz-Communications-with-Three-Side-Beam-Split"><a href="#Wideband-Beamforming-for-STAR-RIS-assisted-THz-Communications-with-Three-Side-Beam-Split" class="headerlink" title="Wideband Beamforming for STAR-RIS-assisted THz Communications with Three-Side Beam Split"></a>Wideband Beamforming for STAR-RIS-assisted THz Communications with Three-Side Beam Split</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13933">http://arxiv.org/abs/2310.13933</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wencai Yan, Wanming Hao, Gangcan Sun, Chongwen Huang, Qingqing Wu</li>
<li>for: 这篇论文研究了同时发射和反射智能表面（STAR-RIS）帮助的THz通信系统，包括三个方向的折射。这篇论文首次分析了STAR-RIS的双面折射效应。</li>
<li>methods: 作者提出了一种基于时钟延迟（TD）的完全连接结构，以减轻双面折射效应。此外，作者还提出了一种具有低硬件复杂度和低功耗的半连接结构，其中多个STAR-RIS元素共享一个TD。</li>
<li>results: 作者通过数值结果验证了提出的方案的有效性。<details>
<summary>Abstract</summary>
In this paper, we consider the simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS)-assisted THz communications with three-side beam split. Except for the beam split at the base station (BS), we analyze the double-side beam split at the STAR-RIS for the first time. To relieve the double-side beam split effect, we propose a time delayer (TD)-based fully-connected structure at the STAR-RIS. As a further advance, a low-hardware complexity and low-power consumption sub-connected structure is developed, where multiple STAR-RIS elements share one TD. Meanwhile, considering the practical scenario, we investigate a multi-STAR-RIS and multi-user communication system, and a sum rate maximization problem is formulated by jointly optimizing the hybrid analog/digital beamforming, time delays at the BS as well as the double-layer phase-shift coefficients, time delays and amplitude coefficients at the STAR-RISs. Based on this, we first allocate users for each STAR-RIS, and then derive the analog beamforming, time delays at the BS, and the double-layer phase-shift coefficients, time delays at each STAR-RIS. Next, we develop an alternative optimization algorithm to calculate the digital beamforming at the BS and amplitude coefficients at the STAR-RISs. Finally, the numerical results verify the effectiveness of the proposed schemes.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们考虑了同时传输和反射可配置智能表面（STAR-RIS）帮助的 THz 通信系统中的三面扫描分。除了基站（BS）的扫描分，我们对 STAR-RIS 中的双面扫描分进行了分析。为了减轻双面扫描分的影响，我们提议在 STAR-RIS 中使用时延（TD）基于完全连接结构。此外，我们还提出了一种具有低硬件复杂度和低能耗的半连接结构，其中多个 STAR-RIS 元素共享一个 TD。此外，我们还考虑了实际场景，研究了多个 STAR-RIS 和多用户通信系统，并对 hybrid 分析/数字 beamforming、时延在 BS 以及 DOUBLE-LAYER 相位偏移 coefficients、时延和幅偏移在每个 STAR-RIS 进行了最大化Sum rate 问题。基于这，我们首先将用户分配给每个 STAR-RIS，然后 derivation analog beamforming、时延在 BS 和 DOUBLE-LAYER 相位偏移 coefficients、时延在每个 STAR-RIS。接着，我们开发了一种代替优化算法，用于计算数字 beamforming 在 BS 和每个 STAR-RIS 的幅偏移。最后，数值结果证明了我们提出的方案的有效性。
</details></li>
</ul>
<hr>
<h2 id="Trajectory-and-Power-Design-for-Aerial-Multi-User-Covert-Communications"><a href="#Trajectory-and-Power-Design-for-Aerial-Multi-User-Covert-Communications" class="headerlink" title="Trajectory and Power Design for Aerial Multi-User Covert Communications"></a>Trajectory and Power Design for Aerial Multi-User Covert Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13932">http://arxiv.org/abs/2310.13932</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongjiang Lei, Jiacheng Jiang, Imran Shafique Ansari, Gaofeng Pan, Mohamed-Slim Alouini</li>
<li>for: 这个论文是研究了一种多用户下链 dual-UAV 掩饰通信系统，以提高未来通信系统的安全性和可靠性。</li>
<li>methods: 该论文使用了安全信息传输、人工干扰信号和位置不确定性等方法来实现掩饰通信。</li>
<li>results: 研究发现，采用合适的 trajectory 和功率分配策略可以提高掩饰率，并且在多天线 Wardens 情况下，可以采用类似的策略来提高掩饰率。<details>
<summary>Abstract</summary>
Unmanned aerial vehicles (UAVs) can provide wireless access to terrestrial users, regardless of geographical constraints, and will be an important part of future communication systems. In this paper, a multi-user downlink dual-UAVs enabled covert communication system was investigated, in which a UAV transmits secure information to ground users in the presence of multiple wardens as well as a friendly jammer UAV transmits artificial jamming signals to fight with the wardens. The scenario of wardens being outfitted with a single antenna is considered, and the detection error probability (DEP) of wardens with finite observations is researched. Then, considering the uncertainty of wardens' location, a robust optimization problem with worst-case covertness constraint is formulated to maximize the average covert rate by jointly optimizing power allocation and trajectory. To cope with the optimization problem, an algorithm based on successive convex approximation methods is proposed. Thereafter, the results are extended to the case where all the wardens are equipped with multiple antennas. After analyzing the DEP in this scenario, a tractable lower bound of the DEP is obtained by utilizing Pinsker's inequality. Subsequently, the non-convex optimization problem was established and efficiently coped by utilizing a similar algorithm as in the single-antenna scenario. Numerical results indicate the effectiveness of our proposed algorithm.
</details>
<details>
<summary>摘要</summary>
“无人航空车（UAV）可以提供无线通信服务，无论地理限制，并将成为未来通信系统的重要组件。在这篇研究中，一个多用户下行双UAV对抗通信系统被研究，其中一个UAV传送安全信息到地面用户，同时友好干扰UAV传送假的干扰信号以抗衡敌人。假设监听者搭配单一天线，DEP（检测错误率）的研究被考虑。然后，根据监听者的位置不确定性，一个具有最坏隐身性约束的强化优化问题被设定，以最大化平均隐身率，通过协同调度和轨道优化。为了解决这个问题，我们提出了基于成功递增法的算法。接下来，我们将结果扩展到所有监听者都搭配多天线情况下。经过分析DEP的情况，我们得到了一个可以计算的下界DEP。然后，我们设定了一个非对称优化问题，并使用相似的算法来解决。 numerics indicate the effectiveness of our proposed algorithm。”Note: Simplified Chinese is used in this translation, which is a more casual and conversational style of Chinese. If you prefer Traditional Chinese or another style, please let me know and I can adjust the translation accordingly.
</details></li>
</ul>
<hr>
<h2 id="Trajectory-and-power-design-for-aerial-CRNs-with-colluding-eavesdroppers"><a href="#Trajectory-and-power-design-for-aerial-CRNs-with-colluding-eavesdroppers" class="headerlink" title="Trajectory and power design for aerial CRNs with colluding eavesdroppers"></a>Trajectory and power design for aerial CRNs with colluding eavesdroppers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13931">http://arxiv.org/abs/2310.13931</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongjiang Lei, Jiacheng Jiang, Haosi Yang, Ki-Hong Park, Imran Shafique Ansari, Gaofeng Pan, Mohamed-Slim Alouini</li>
<li>for: 这篇论文是研究无人机驱动的空中广播网络的安全性的。</li>
<li>methods: 该论文使用了迭代算法基于块协调 descent和逻辑准确 approximation来解决非整数混合变量优化问题。</li>
<li>results: numerical results表明我们的提议的方案可以提高空中广播网络的安全性表现。<details>
<summary>Abstract</summary>
Unmanned aerial vehicles (UAVs) can provide wireless access services to terrestrial users without geographical limitations and will become an essential part of the future communication system. However, the openness of wireless channels and the mobility of UAVs make the security of UAV-based communication systems particularly challenging. This work investigates the security of aerial cognitive radio networks (CRNs) with multiple uncertainties colluding eavesdroppers. A cognitive aerial base station transmits messages to cognitive terrestrial users using the spectrum resource of the primary users. All secondary terrestrial users and illegitimate receivers jointly decode the received message. The average secrecy rate of the aerial CRNs is maximized by jointly optimizing the UAV's trajectory and transmission power. An iterative algorithm based on block coordinate descent and successive convex approximation is proposed to solve the non-convex mixed-variable optimization problem. Numerical results verify the effectiveness of our proposed algorithm and show that our scheme improves the secrecy performance of airborne CRNs.
</details>
<details>
<summary>摘要</summary>
无人飞行器（UAV）可以提供无地点限制的无线访问服务，未来的通信系统中将成为不可或缺的一部分。然而，无线通道的开放性和UAV的移动性使得UAV基站的安全特别挑战。这项工作研究了有多个不确定参与者的邻近广播网络（CRN）的安全性。一个认知空中基站通过主用户频率资源传输消息给认知地面用户。所有次要地面用户和非法接收器共同解码接收到的消息。通过协调UAV的轨迹和传输功率进行最大化平均机密率，解决了混合变量优化问题。我们提出了基于块协调下降和Successive Convex Approximation（SCA）的迭代算法来解决非对称混合变量优化问题。数值结果证明了我们的提议的有效性，并显示了我们的方案在空中CRN中提高了机密性性能。
</details></li>
</ul>
<hr>
<h2 id="Beamforming-Design-for-the-Distributed-RISs-aided-THz-Communications-with-Double-Layer-True-Time-Delays"><a href="#Beamforming-Design-for-the-Distributed-RISs-aided-THz-Communications-with-Double-Layer-True-Time-Delays" class="headerlink" title="Beamforming Design for the Distributed RISs-aided THz Communications with Double-Layer True Time Delays"></a>Beamforming Design for the Distributed RISs-aided THz Communications with Double-Layer True Time Delays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13917">http://arxiv.org/abs/2310.13917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gangcan Sun, Wencai Yan, Wanming Hao, Chongwen Huang, Chau Yuen</li>
<li>for: 本文研究了利用具有稀疏无线电频率链天线结构的基站（BS）的射频信号处理系统，以提高系统性能。</li>
<li>methods: 本文提出了一种double-layer true-time-delay（TTD）方案，以减少BS发射机的扩散辐射损失，并对RIS进行分布式实现。然后，通过协调分析系统性能。</li>
<li>results: 实验结果表明，提出的方案可以有效地提高系统性能，并且可以考虑实际硬件限制。<details>
<summary>Abstract</summary>
In this paper, we investigate the reconfigurable intelligent surface (RIS)-aided terahertz (THz) communication system with the sparse radio frequency chains antenna structure at the base station (BS). To overcome the beam split of the BS, different from the conventional single-layer true-time-delay (TTD) scheme, we propose a double-layer TTD scheme that can effectively reduce the number of large-range delay devices, which involve additional insertion loss and amplification circuitry. Next, we analyze the system performance under the proposed double-layer TTD scheme. To relieve the beam split of the RIS, we consider multiple distributed RISs to replace an ultra-large size RIS. Based on this, we formulate an achievable rate maximization problem for the distributed RISs-aided THz communications via jointly optimizing the hybrid analog/digital beamforming, time delays of the double-layer TTD network and reflection coefficients of RISs. Considering the practical hardware limitation, the finite-resolution phase shift, time delay and reflection phase are constrained. To solve the formulated problem, we first design an analog beamforming scheme including optimizing phase shift and time delay based on the RISs' locations. Then, an alternatively optimization algorithm is proposed to obtain the digital beamforming and reflection coefficients based on the minimum mean square error and coordinate update techniques. Finally, simulation results show the effectiveness of the proposed scheme.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了利用协助器（RIS）的可重新配置的智能表面（RIS）-帮助的 TerraHertz（THz）通信系统，其中基站（BS）使用稀疏的 радио频率链天线结构。为了解决基站的束分，我们提议了一种双层真时延迟（TTD）方案，可以有效减少基站的大范围延迟设备，这些设备包括额外插入损和增益电路。接着，我们分析了系统性能下提议的双层 TTD 方案。为了缓解 RIS 的束分，我们考虑了多个分布式 RIS，以取代巨大Size RIS。基于这，我们提出了一个可实现最大化率的做法，通过同时优化混合式analog/数字扫描、双层 TTD 网络的时延和 RIS 的反射率来实现。受到实际硬件限制，finite-resolution phase shift、时延和反射相位受限。为了解决这个问题，我们首先设计了一种analog扫描方案，包括优化相位和时延基于 RIS 的位置。然后，我们提出了一种alternatively optimization算法，用于取得数字扫描和反射率。最后，我们通过实际结果来证明提议的方案的有效性。
</details></li>
</ul>
<hr>
<h2 id="NMR-Spectra-Denoising-with-Vandermonde-Constraints"><a href="#NMR-Spectra-Denoising-with-Vandermonde-Constraints" class="headerlink" title="NMR Spectra Denoising with Vandermonde Constraints"></a>NMR Spectra Denoising with Vandermonde Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13882">http://arxiv.org/abs/2310.13882</a></li>
<li>repo_url: None</li>
<li>paper_authors: Di Guo, Runmin Xu, Jinyu Wu, Meijin Lin, Xiaofeng Du, Xiaobo Qu</li>
<li>for: 用于分析化学和蛋白质的生物工程中，使用核磁共振（NMR） спектроскопия时，Signal 容易受到数据收集时的噪声污染，这会影响后续的量化分析。因此，去噪NMR Signal 已成为一项长期关注的问题。</li>
<li>methods: 本文提出了一种优化模型基于迭代减噪方法，称为CHORD-V，该方法在时域NMR Signal 上对噪声进行了逼近，并维持了泛函分解。</li>
<li>results: 对于both synthetic和实际的NMR数据，CHORD-V方法表现出了较高的减噪性能，比typical Cadzow和rQRd方法更为出色，同时也比state-of-the-art CHORD方法更为精准。CHORD-V方法能够更好地还原低强度 спектраль峰，特别是当噪声相对较高时。<details>
<summary>Abstract</summary>
Nuclear magnetic resonance (NMR) spectroscopy serves as an important tool to analyze chemicals and proteins in bioengineering. However, NMR signals are easily contaminated by noise during the data acquisition, which can affect subsequent quantitative analysis. Therefore, denoising NMR signals has been a long-time concern. In this work, we propose an optimization model-based iterative denoising method, CHORD-V, by treating the time-domain NMR signal as damped exponentials and maintaining the exponential signal form with a Vandermonde factorization. Results on both synthetic and realistic NMR data show that CHORD-V has a superior denoising performance over typical Cadzow and rQRd methods, and the state-of-the-art CHORD method. CHORD-V restores low-intensity spectral peaks more accurately, especially when the noise is relatively high.
</details>
<details>
<summary>摘要</summary>
核磁共振（NMR）光谱技术在生物工程中扮演着重要的角色，但NMR信号在数据采集过程中容易受到噪声污染，这会影响后续的量化分析。因此，NMR信号的减噪成为了长期的焦点。在这种工作中，我们提出了一种基于优化模型的迭代减噪方法，即CHORD-V，其中将时域NMR信号 treated为抑制的 экспонент，并通过瓦德蒙德分解保持信号形式。对于 synthetic 和实际的 NMR 数据进行了比较，CHORD-V 的减噪性能较 Cadzow 和 rQRd 方法更高，同时也高于现有的 CHORD 方法。CHORD-V 能够更准确地还原低强度的spectral peaks，特别是在噪声较高的情况下。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/21/eess.SP_2023_10_21/" data-id="cloimipjq01a0s488110acqs5" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_10_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/20/cs.SD_2023_10_20/" class="article-date">
  <time datetime="2023-10-20T15:00:00.000Z" itemprop="datePublished">2023-10-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/20/cs.SD_2023_10_20/">cs.SD - 2023-10-20</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Multi-label-Open-set-Audio-Classification"><a href="#Multi-label-Open-set-Audio-Classification" class="headerlink" title="Multi-label Open-set Audio Classification"></a>Multi-label Open-set Audio Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13759">http://arxiv.org/abs/2310.13759</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sripathisridhar/moads">https://github.com/sripathisridhar/moads</a></li>
<li>paper_authors: Sripathi Sridhar, Mark Cartwright</li>
<li>for: 实际世界中的声音事件范围较小，因此现有的标准分类模型可能会遗传重要但未知的声音事件。</li>
<li>methods: 以开集标准方法来探测未知的声音事件，并将其应用到多项分类内容中，如声音景象分类。</li>
<li>results: 这些基本方法在不同未知分布下的评估结果，以及它们如何应对多项标签模型中的声音事件重叠。<details>
<summary>Abstract</summary>
Current audio classification models have small class vocabularies relative to the large number of sound event classes of interest in the real world. Thus, they provide a limited view of the world that may miss important yet unexpected or unknown sound events. To address this issue, open-set audio classification techniques have been developed to detect sound events from unknown classes. Although these methods have been applied to a multi-class context in audio, such as sound scene classification, they have yet to be investigated for polyphonic audio in which sound events overlap, requiring the use of multi-label models. In this study, we establish the problem of multi-label open-set audio classification by creating a dataset with varying unknown class distributions and evaluating baseline approaches built upon existing techniques.
</details>
<details>
<summary>摘要</summary>
当前的听音分类模型有较小的类 vocabulary，相对于实际世界中的听音类型的数量相对较多。因此，它们只能提供有限的视角，可能会错过一些重要却未知的听音事件。为解决这个问题，开放集 audio 分类技术已经开发出来，用于检测未知类别的听音事件。虽然这些方法在音频场景分类中已经应用，但它们尚未在多声音场景中进行研究，需要使用多标签模型。在这项研究中，我们将定义多标签开放集听音分类问题，创建不同未知类分布的数据集，并评估基础方法。
</details></li>
</ul>
<hr>
<h2 id="Intelligibility-prediction-with-a-pretrained-noise-robust-automatic-speech-recognition-model"><a href="#Intelligibility-prediction-with-a-pretrained-noise-robust-automatic-speech-recognition-model" class="headerlink" title="Intelligibility prediction with a pretrained noise-robust automatic speech recognition model"></a>Intelligibility prediction with a pretrained noise-robust automatic speech recognition model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19817">http://arxiv.org/abs/2310.19817</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zehai Tu, Ning Ma, Jon Barker</li>
<li>for: 这两个系统是为了预测声音质量而设计的，具体来说是为了参加第二届清晰预测挑战（CPC2）。</li>
<li>methods: 一个系统是侵入的，利用自动语音识别（ASR）模型的隐藏表示。另一个系统是不侵入的，通过 derivated ASR  uncertainty 进行预测。ASR 模型只是在 simulations 的噪音语音库上进行预训练，没有利用 CPC2 数据。</li>
<li>results: 两个系统在 CPC2 评估中表现出色，具体来说是在不同的噪音环境下预测声音质量的能力。<details>
<summary>Abstract</summary>
This paper describes two intelligibility prediction systems derived from a pretrained noise-robust automatic speech recognition (ASR) model for the second Clarity Prediction Challenge (CPC2). One system is intrusive and leverages the hidden representations of the ASR model. The other system is non-intrusive and makes predictions with derived ASR uncertainty. The ASR model is only pretrained with a simulated noisy speech corpus and does not take advantage of the CPC2 data. For that reason, the intelligibility prediction systems are robust to unseen scenarios given the accurate prediction performance on the CPC2 evaluation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Neural-domain-alignment-for-spoken-language-recognition-based-on-optimal-transport"><a href="#Neural-domain-alignment-for-spoken-language-recognition-based-on-optimal-transport" class="headerlink" title="Neural domain alignment for spoken language recognition based on optimal transport"></a>Neural domain alignment for spoken language recognition based on optimal transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13471">http://arxiv.org/abs/2310.13471</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xugang Lu, Peng Shen, Yu Tsao, Hisashi Kawai</li>
<li>for: 提高cross-domain spoken language recognition（SLR）的效果，addressing domain shift challenge.</li>
<li>methods: 使用Unsupervised domain adaptation（UDA）算法，without relying on class labels in the target domain.</li>
<li>results: 提出了一种基于optimal transport（OT）的UDA算法，significantly improved the performance in a cross-channel SLR task compared to existing UDA algorithms.Here’s the full text in Simplified Chinese:</li>
<li>for: 本研究旨在提高cross-domain spoken language recognition（SLR）的效果， Addressing domain shift challenge.</li>
<li>methods: 我们使用Unsupervised domain adaptation（UDA）算法，不依赖于目标频道上的分类标签。</li>
<li>results: 我们提出了一种基于optimal transport（OT）的UDA算法，在cross-channel SLR任务中与现有UDA算法相比，表现出了显著的改善。<details>
<summary>Abstract</summary>
Domain shift poses a significant challenge in cross-domain spoken language recognition (SLR) by reducing its effectiveness. Unsupervised domain adaptation (UDA) algorithms have been explored to address domain shifts in SLR without relying on class labels in the target domain. One successful UDA approach focuses on learning domain-invariant representations to align feature distributions between domains. However, disregarding the class structure during the learning process of domain-invariant representations can result in over-alignment, negatively impacting the classification task. To overcome this limitation, we propose an optimal transport (OT)-based UDA algorithm for a cross-domain SLR, leveraging the distribution geometry structure-aware property of OT. An OT-based discrepancy measure on a joint distribution over feature and label information is considered during domain alignment in OT-based UDA. Our previous study discovered that completely aligning the distributions between the source and target domains can introduce a negative transfer, where classes or irrelevant classes from the source domain map to a different class in the target domain during distribution alignment. This negative transfer degrades the performance of the adaptive model. To mitigate this issue, we introduce coupling-weighted partial optimal transport (POT) within our UDA framework for SLR, where soft weighting on the OT coupling based on transport cost is adaptively set during domain alignment. A cross-domain SLR task was used in the experiments to evaluate the proposed UDA. The results demonstrated that our proposed UDA algorithm significantly improved the performance over existing UDA algorithms in a cross-channel SLR task.
</details>
<details>
<summary>摘要</summary>
域外迁带来很大的挑战，对cross-domain spoken language recognition（SLR）的效果甚至是降低的。无监督适应（UDA）算法已经被探索以解决域外迁问题，无需在目标域中使用类别标签。一种成功的UDA方法是学习域外适应的域不同表示，以平衡特征分布的分布。但是，在学习过程中忽略目标域的类结构可能会导致过度平衡，从而负面影响分类任务。为了解决这些限制，我们提议一种基于最优运输（OT）的UDA算法，利用OT的分布几何结构特性。在OT中，我们考虑了一个联合分布 над feature和标签信息的误差度量，以便在域对齐过程中进行域外适应。我们的之前研究发现，完全对源和目标域的分布进行对齐可能会导致一种负面传递，其中源域中的类或无关类在目标域中的不同类型。这种负面传递会降低适应模型的性能。为了解决这个问题，我们在UDA框架中引入了coupling-weighted partial optimal transport（POT），其中在对齐过程中采用软约束的OT交互基于运输成本进行调整。我们使用了cross-domain SLR任务来评估我们的UDA算法。实验结果表明，我们的UDA算法在跨频SLR任务中表现得更好，与现有UDA算法相比。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/20/cs.SD_2023_10_20/" data-id="cloimipet00wss488hi0w0qat" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/8/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="page-number" href="/page/8/">8</a><span class="page-number current">9</span><a class="page-number" href="/page/10/">10</a><a class="page-number" href="/page/11/">11</a><span class="space">&hellip;</span><a class="page-number" href="/page/84/">84</a><a class="extend next" rel="next" href="/page/10/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">112</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">62</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
