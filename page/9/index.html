
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/9/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-eess.IV_2023_08_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/09/eess.IV_2023_08_09/" class="article-date">
  <time datetime="2023-08-08T16:00:00.000Z" itemprop="datePublished">2023-08-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/09/eess.IV_2023_08_09/">eess.IV - 2023-08-09 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ACE-HetEM-for-ab-initio-Heterogenous-Cryo-EM-3D-Reconstruction"><a href="#ACE-HetEM-for-ab-initio-Heterogenous-Cryo-EM-3D-Reconstruction" class="headerlink" title="ACE-HetEM for ab initio Heterogenous Cryo-EM 3D Reconstruction"></a>ACE-HetEM for ab initio Heterogenous Cryo-EM 3D Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04956">http://arxiv.org/abs/2308.04956</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijie Chen, Lin Yao, Zeqing Xia, Yuhang Wang</li>
<li>for: 实现高精度的三维结构重建 FROM 二维图像，尤其是在电子显微镜实验中，降低讯号与噪音比和不知之影像转换角度的情况下。</li>
<li>methods: 使用自适应学架构，包括自适应推断和其他相关的构成方法，实现不结合几何预设的实验结构重建。</li>
<li>results: 在实验数据中，ACE-HetEM 可以与非束合方法相比，实现更高的重建分辨率，并且在实验中实现更好的结构重建。<details>
<summary>Abstract</summary>
Due to the extremely low signal-to-noise ratio (SNR) and unknown poses (projection angles and image translation) in cryo-EM experiments, reconstructing 3D structures from 2D images is very challenging. On top of these challenges, heterogeneous cryo-EM reconstruction also has an additional requirement: conformation classification. An emerging solution to this problem is called amortized inference, implemented using the autoencoder architecture or its variants. Instead of searching for the correct image-to-pose/conformation mapping for every image in the dataset as in non-amortized methods, amortized inference only needs to train an encoder that maps images to appropriate latent spaces representing poses or conformations. Unfortunately, standard amortized-inference-based methods with entangled latent spaces have difficulty learning the distribution of conformations and poses from cryo-EM images. In this paper, we propose an unsupervised deep learning architecture called "ACE-HetEM" based on amortized inference. To explicitly enforce the disentanglement of conformation classifications and pose estimations, we designed two alternating training tasks in our method: image-to-image task and pose-to-pose task. Results on simulated datasets show that ACE-HetEM has comparable accuracy in pose estimation and produces even better reconstruction resolution than non-amortized methods. Furthermore, we show that ACE-HetEM is also applicable to real experimental datasets.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:由于低信号噪声比(SNR)和未知投影角和图像翻译等因素，从普遍电子镜像(cryo-EM)实验中提取3D结构信息是非常困难。此外，非标准的普遍精度 reconstruction 还需要进行拓扑分类。一种升级的解决方案是使用束合推理(amortized inference)，通过自适应网络架构或其变体来实现。而标准的束合推理方法在普遍电子镜像图像上学习 pose 和拓扑分类的 distribuition 很困难。在这篇论文中，我们提出了一种无监督深度学习架构，称为 "ACE-HetEM"，基于束合推理。我们为了明确分离 pose 和拓扑分类的束合，设计了两个相互循环训练任务：图像到图像任务和pose到pose任务。实验结果表明，ACE-HetEM 与非束合方法相比，在pose估计方面具有相似的准确性，并且可以生成更高的重建分辨率。此外，我们还证明了 ACE-HetEM 可以应用于实验 datasets。
</details></li>
</ul>
<hr>
<h2 id="HSD-PAM-High-Speed-Super-Resolution-Deep-Penetration-Photoacoustic-Microscopy-Imaging-Boosted-by-Dual-Branch-Fusion-Network"><a href="#HSD-PAM-High-Speed-Super-Resolution-Deep-Penetration-Photoacoustic-Microscopy-Imaging-Boosted-by-Dual-Branch-Fusion-Network" class="headerlink" title="HSD-PAM: High Speed Super Resolution Deep Penetration Photoacoustic Microscopy Imaging Boosted by Dual Branch Fusion Network"></a>HSD-PAM: High Speed Super Resolution Deep Penetration Photoacoustic Microscopy Imaging Boosted by Dual Branch Fusion Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04922">http://arxiv.org/abs/2308.04922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengyuan Zhang, Haoran Jin, Zesheng Zheng, Wenwen Zhang, Wenhao Lu, Feng Qin, Arunima Sharma, Manojit Pramanik, Yuanjin Zheng</li>
<li>for: 这篇论文主要用于提出一种解决photoacoustic microscopy（PAM）中三个关键成像参数之间的紧张关系，即成像速度、横向分辨率和吸收深度之间的紧张关系，以提高PAM系统的总性能。</li>
<li>methods: 该论文提出了硬件和软件共设计的方法，首先使用低横向分辨率、低抽样率AR-PAM成像，然后通过提高横向分辨率和上抽样率来提高成像速度和深度，最终实现高速、超分辨率和深度探测的PAM系统（HSD-PAM）。</li>
<li>results: 该论文通过对大量的模拟和生物体实验来验证提出的算法，实验结果表明，提出的算法可以在PAM系统中提高成像质量，包括提高了成像速度16倍，横向分辨率提高5倍，而且保留了AR-PAM模式的深度探测能力。<details>
<summary>Abstract</summary>
Photoacoustic microscopy (PAM) is a novel implementation of photoacoustic imaging (PAI) for visualizing the 3D bio-structure, which is realized by raster scanning of the tissue. However, as three involved critical imaging parameters, imaging speed, lateral resolution, and penetration depth have mutual effect to one the other. The improvement of one parameter results in the degradation of other two parameters, which constrains the overall performance of the PAM system. Here, we propose to break these limitations by hardware and software co-design. Starting with low lateral resolution, low sampling rate AR-PAM imaging which possesses the deep penetration capability, we aim to enhance the lateral resolution and up sampling the images, so that high speed, super resolution, and deep penetration for the PAM system (HSD-PAM) can be achieved. Data-driven based algorithm is a promising approach to solve this issue, thereby a dedicated novel dual branch fusion network is proposed, which includes a high resolution branch and a high speed branch. Since the availability of switchable AR-OR-PAM imaging system, the corresponding low resolution, undersample AR-PAM and high resolution, full sampled OR-PAM image pairs are utilized for training the network. Extensive simulation and in vivo experiments have been conducted to validate the trained model, enhancement results have proved the proposed algorithm achieved the best perceptual and quantitative image quality. As a result, the imaging speed is increased 16 times and the imaging lateral resolution is improved 5 times, while the deep penetration merit of AR-PAM modality is still reserved.
</details>
<details>
<summary>摘要</summary>
фотоакустическая микроскопия (PAM) 是一种新的实现 фотоакустической成像 (PAI)，用于可见三维生物结构，通过扫描器扫描物质。然而，存在三个关键成像参数之间的互相影响关系：成像速度、水平分辨率和吸收深度。改进一个参数会导致另外两个参数下降，这限制了整体 PAM 系统的性能。我们提出了硬件和软件合作的方法，从低水平分辨率、低抽象率 AR-PAM 成像开始，希望通过提高水平分辨率和增加样本率来提高整体性能。基于数据驱动的算法是一种有前途的方法，因此我们提出了一种专门的双支部合并网络，包括高分辨率支部和高速支部。由于可用的可换 AR-OR-PAM 成像系统，对应的低分辨率、下抽样 AR-PAM 和高分辨率、全样本 OR-PAM 图像对是用于训练网络。广泛的 simulations 和生物实验 validate 了训练模型，提升结果证明了我们提出的算法实现了最佳的感知和量化图像质量。因此，成像速度提高 16 倍，水平分辨率提高 5 倍，而 AR-PAM 模式下的深度探测仍然保留。
</details></li>
</ul>
<hr>
<h2 id="StableVQA-A-Deep-No-Reference-Quality-Assessment-Model-for-Video-Stability"><a href="#StableVQA-A-Deep-No-Reference-Quality-Assessment-Model-for-Video-Stability" class="headerlink" title="StableVQA: A Deep No-Reference Quality Assessment Model for Video Stability"></a>StableVQA: A Deep No-Reference Quality Assessment Model for Video Stability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04904">http://arxiv.org/abs/2308.04904</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qmme/stablevqa">https://github.com/qmme/stablevqa</a></li>
<li>paper_authors: Tengchuan Kou, Xiaohong Liu, Wei Sun, Jun Jia, Xiongkuo Min, Guangtao Zhai, Ning Liu<br>for: 本研究旨在提供一个新的视频稳定评价模型，以便更好地评估用户生成内容（UGC）视频中的不稳定程度。methods: 本研究使用了一种新的特征提取器，包括光流、 semantic 和模糊特征提取器，以及一个回归层来预测视频稳定度。results: 对于1952个多样化的UGC视频，我们的新模型StableVQA可以与34名评分者的主观意见更高度相关，并且与现有的VQA-S模型和通用VQA模型相比，具有更高的协方差。<details>
<summary>Abstract</summary>
Video shakiness is an unpleasant distortion of User Generated Content (UGC) videos, which is usually caused by the unstable hold of cameras. In recent years, many video stabilization algorithms have been proposed, yet no specific and accurate metric enables comprehensively evaluating the stability of videos. Indeed, most existing quality assessment models evaluate video quality as a whole without specifically taking the subjective experience of video stability into consideration. Therefore, these models cannot measure the video stability explicitly and precisely when severe shakes are present. In addition, there is no large-scale video database in public that includes various degrees of shaky videos with the corresponding subjective scores available, which hinders the development of Video Quality Assessment for Stability (VQA-S). To this end, we build a new database named StableDB that contains 1,952 diversely-shaky UGC videos, where each video has a Mean Opinion Score (MOS) on the degree of video stability rated by 34 subjects. Moreover, we elaborately design a novel VQA-S model named StableVQA, which consists of three feature extractors to acquire the optical flow, semantic, and blur features respectively, and a regression layer to predict the final stability score. Extensive experiments demonstrate that the StableVQA achieves a higher correlation with subjective opinions than the existing VQA-S models and generic VQA models. The database and codes are available at https://github.com/QMME/StableVQA.
</details>
<details>
<summary>摘要</summary>
视频抖动是用户生成内容（UGC）视频中不прият的扭曲，通常是因为摄像机不稳定。在过去的几年中，许多视频稳定算法已经被提出，但没有特定和准确的度量可以全面评估视频的稳定性。实际上，大多数现有的视频质量评估模型都是不特别地考虑视频稳定性的，因此无法准确地测量视频中严重的抖动。此外，没有公开的大规模视频数据库，其中包含不同程度的抖动视频和相应的主观评分，这限制了视频质量评估的发展。为此，我们建立了一个新的数据库 named StableDB，其中包含1,952个多样性抖动的UGC视频，每个视频都有34名评分员对视频的稳定度进行了 Mean Opinion Score（MOS）评分。此外，我们还 elaborately 设计了一种新的VQA-S模型 named StableVQA，它包括三个特征提取器来获取光流、semantic和模糊特征，以及一个回归层来预测最终的稳定度分。广泛的实验表明，StableVQA可以与主观意见更高度相关性。数据库和代码可以在 GitHub 上获取。
</details></li>
</ul>
<hr>
<h2 id="An-automated-pipeline-for-quantitative-T2-fetal-body-MRI-and-segmentation-at-low-field"><a href="#An-automated-pipeline-for-quantitative-T2-fetal-body-MRI-and-segmentation-at-low-field" class="headerlink" title="An automated pipeline for quantitative T2* fetal body MRI and segmentation at low field"></a>An automated pipeline for quantitative T2* fetal body MRI and segmentation at low field</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04903">http://arxiv.org/abs/2308.04903</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kelly Payette, Alena Uus, Jordina Aviles Verdera, Carla Avena Zampieri, Megan Hall, Lisa Story, Maria Deprez, Mary A. Rutherford, Joseph V. Hajnal, Sebastien Ourselin, Raphael Tomi-Tricot, Jana Hutter</li>
<li>For: 这项研究旨在开发一种 semi-automatic ipeline，用于在低场强磁共振成像中进行快速和详细的量化 T2* 相关分析。* Methods: 该ipeline 使用了量化 MRI 技术，并利用了可变多重Dynamicsequence和可变压缩 reconstruction 技术，以生成高分辨率的三维Volumetric数据。此外，该ipeline 还使用了一种 semi-supervised  neural network 来自动 segment 胎儿体内的十个不同器官。* Results: 研究发现，使用这种 pipeline，可以成功地进行快速和详细的量化 T2* 相关分析，并且具有高度的 robustness 性，可以抵御运动artefacts。此外，研究还发现了胎儿体内各器官的 T2* 值与 gestational age 之间的强相关关系。<details>
<summary>Abstract</summary>
Fetal Magnetic Resonance Imaging at low field strengths is emerging as an exciting direction in perinatal health. Clinical low field (0.55T) scanners are beneficial for fetal imaging due to their reduced susceptibility-induced artefacts, increased T2* values, and wider bore (widening access for the increasingly obese pregnant population). However, the lack of standard automated image processing tools such as segmentation and reconstruction hampers wider clinical use. In this study, we introduce a semi-automatic pipeline using quantitative MRI for the fetal body at low field strength resulting in fast and detailed quantitative T2* relaxometry analysis of all major fetal body organs. Multi-echo dynamic sequences of the fetal body were acquired and reconstructed into a single high-resolution volume using deformable slice-to-volume reconstruction, generating both structural and quantitative T2* 3D volumes. A neural network trained using a semi-supervised approach was created to automatically segment these fetal body 3D volumes into ten different organs (resulting in dice values > 0.74 for 8 out of 10 organs). The T2* values revealed a strong relationship with GA in the lungs, liver, and kidney parenchyma (R^2>0.5). This pipeline was used successfully for a wide range of GAs (17-40 weeks), and is robust to motion artefacts. Low field fetal MRI can be used to perform advanced MRI analysis, and is a viable option for clinical scanning.
</details>
<details>
<summary>摘要</summary>
低场强矩 Magnetic Resonance Imaging (MRI) 在产科医学中得到了推广应用。低场强矩 (0.55T) 磁共振仪器在胎儿成像中有利，因为它们具有降低感应性引起的误差、提高 T2* 值和更宽的磁共振腔（扩大对增加质量增大的怀孕女性人口的访问）。然而，因为没有标准的自动化图像处理工具，如分 segmentation 和重建，因此对于广泛的临床应用仍存在限制。本研究推出了一种半自动化管道，使用量化 MRI 对胎儿体部进行高精度的 T2* 相relaxometry 分析。多echo 动态序列获取并重建成一个高分辨率的三维卷积体，生成了胎儿体部的结构和量化 T2* 三维卷积体。通过 semi-supervised 方法训练的神经网络可以自动将胎儿体部三维卷积体分割成十个不同的器官（得到了 dice 值 > 0.74 的 eight 个器官）。T2* 值与胎儿龄（GA） exhibit 强相关性（R^2>0.5）。这种管道在 17-40 周的多个 Gestational Age (GA) 上运行，并对运动误差具有抗衡性。低场强矩胎儿 MRI 可以进行高级 MRI 分析，是临床扫描的可能性。
</details></li>
</ul>
<hr>
<h2 id="Transmission-and-Color-guided-Network-for-Underwater-Image-Enhancement"><a href="#Transmission-and-Color-guided-Network-for-Underwater-Image-Enhancement" class="headerlink" title="Transmission and Color-guided Network for Underwater Image Enhancement"></a>Transmission and Color-guided Network for Underwater Image Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04892">http://arxiv.org/abs/2308.04892</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pan Mu, Jing Fang, Haotian Qian, Cong Bai</li>
<li>for: 提高水下图像质量，解决光线干扰和颜色偏差问题</li>
<li>methods: 提出了一种基于自适应传输和动态颜色引导的网络（名为ATDCnet），利用物理知识设计了适应传输指导模块（ATM）和动态颜色引导模块（DCM），并实现了多 Stage feature fusion 和注意力机制来同时进行颜色恢复和对比度增强</li>
<li>results: 对多个标准数据集进行了广泛的实验，并达到了当今最佳性能水平<details>
<summary>Abstract</summary>
In recent years, with the continuous development of the marine industry, underwater image enhancement has attracted plenty of attention. Unfortunately, the propagation of light in water will be absorbed by water bodies and scattered by suspended particles, resulting in color deviation and low contrast. To solve these two problems, we propose an Adaptive Transmission and Dynamic Color guided network (named ATDCnet) for underwater image enhancement. In particular, to exploit the knowledge of physics, we design an Adaptive Transmission-directed Module (ATM) to better guide the network. To deal with the color deviation problem, we design a Dynamic Color-guided Module (DCM) to post-process the enhanced image color. Further, we design an Encoder-Decoder-based Compensation (EDC) structure with attention and a multi-stage feature fusion mechanism to perform color restoration and contrast enhancement simultaneously. Extensive experiments demonstrate the state-of-the-art performance of the ATDCnet on multiple benchmark datasets.
</details>
<details>
<summary>摘要</summary>
Recently, with the continuous development of the marine industry, underwater image enhancement has attracted a lot of attention. Unfortunately, the propagation of light in water will be absorbed by water bodies and scattered by suspended particles, resulting in color deviation and low contrast. To solve these two problems, we propose an Adaptive Transmission and Dynamic Color guided network (named ATDCnet) for underwater image enhancement. In particular, to exploit the knowledge of physics, we design an Adaptive Transmission-directed Module (ATM) to better guide the network. To deal with the color deviation problem, we design a Dynamic Color-guided Module (DCM) to post-process the enhanced image color. Further, we design an Encoder-Decoder-based Compensation (EDC) structure with attention and a multi-stage feature fusion mechanism to perform color restoration and contrast enhancement simultaneously. Extensive experiments demonstrate the state-of-the-art performance of the ATDCnet on multiple benchmark datasets.Here's the word-for-word translation of the text into Simplified Chinese:最近几年，marine工业的不断发展，下水图像提高吸引了很多关注。然而，水中光的传播会被水体吸收和悬浮颗粒扰乱，导致颜色偏移和对比度低。为解决这两个问题，我们提议一种名为ATDCnet的下水图像提高网络。特别是，利用物理知识，我们设计了一个名为ATM的适应传输导向模块，更好地引导网络。另外，我们设计了一个名为DCM的动态颜色导向模块，用于后处理提高图像颜色。此外，我们设计了一个名为EDC的Encoder-Decoder-based Compensation结构，并将注意力和多个阶段特征融合机制用于同时恢复颜色和对比度。广泛的实验证明ATDCnet在多个标准数据集上达到了顶尖性能。
</details></li>
</ul>
<hr>
<h2 id="Deep-Generative-Networks-for-Heterogeneous-Augmentation-of-Cranial-Defects"><a href="#Deep-Generative-Networks-for-Heterogeneous-Augmentation-of-Cranial-Defects" class="headerlink" title="Deep Generative Networks for Heterogeneous Augmentation of Cranial Defects"></a>Deep Generative Networks for Heterogeneous Augmentation of Cranial Defects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04883">http://arxiv.org/abs/2308.04883</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kamil Kwarciak, Marek Wodzinski</li>
<li>for: 这篇论文的目的是提出一种解决个性化颅部嵌入设计中数据缺乏问题的方法，通过利用深度学习技术和生成模型创造Synthetic Skulls。</li>
<li>methods: 这篇论文使用了三种深度生成模型，包括Wasserstein生成对抗网络受条件（WGAN-GP）、WGAN-GP混合模型（VAE&#x2F;WGAN-GP）和Introspective Variational Autoencoder（IntroVAE），将这些模型应用于创建Synthetic Skulls。</li>
<li>results: 这篇论文表明，这些生成模型可以创建大量的具有相同缺陷的Synthetic Skulls，并且可以与实际颅内部嵌入设计中的缺陷匹配。实际应用中，这些生成模型可以帮助提高自动设计个性化颅部嵌入的精度和效率。<details>
<summary>Abstract</summary>
The design of personalized cranial implants is a challenging and tremendous task that has become a hot topic in terms of process automation with the use of deep learning techniques. The main challenge is associated with the high diversity of possible cranial defects. The lack of appropriate data sources negatively influences the data-driven nature of deep learning algorithms. Hence, one of the possible solutions to overcome this problem is to rely on synthetic data. In this work, we propose three volumetric variations of deep generative models to augment the dataset by generating synthetic skulls, i.e. Wasserstein Generative Adversarial Network with Gradient Penalty (WGAN-GP), WGAN-GP hybrid with Variational Autoencoder pretraining (VAE/WGAN-GP) and Introspective Variational Autoencoder (IntroVAE). We show that it is possible to generate dozens of thousands of defective skulls with compatible defects that achieve a trade-off between defect heterogeneity and the realistic shape of the skull. We evaluate obtained synthetic data quantitatively by defect segmentation with the use of V-Net and qualitatively by their latent space exploration. We show that the synthetically generated skulls highly improve the segmentation process compared to using only the original unaugmented data. The generated skulls may improve the automatic design of personalized cranial implants for real medical cases.
</details>
<details>
<summary>摘要</summary>
personalized cranial implant 设计是一项复杂且具有挑战性的任务，这个领域的过程自动化已经成为热点。主要挑战在于可能出现的颅骨缺陷的多样性。因为没有相应的数据源，这对深度学习算法来说是一个缺乏数据的问题。为了解决这个问题，我们提出了三种三维变化的深度生成模型来增强数据集，即 Wasserstein Generative Adversarial Network with Gradient Penalty (WGAN-GP)、WGAN-GP 混合 Variational Autoencoder 预训练 (VAE/WGAN-GP) 以及 Introspective Variational Autoencoder (IntroVAE)。我们证明了可以生成多达万个具有相同缺陷的颅骨，并且这些缺陷具有质量和颅骨的真实形状之间的 compatibles 性。我们通过 V-Net 进行缺陷分割评估和 latent space 探索来评估获得的 sintethic 数据质量。结果表明，使用生成的颅骨可以大幅提高分割过程的精度，比使用原始未修改数据更好。这些生成的颅骨可能会改善实际医疗案例中的自动颅骨Implant 设计。
</details></li>
</ul>
<hr>
<h2 id="HyperCoil-Recon-A-Hypernetwork-based-Adaptive-Coil-Configuration-Task-Switching-Network-for-MRI-Reconstruction"><a href="#HyperCoil-Recon-A-Hypernetwork-based-Adaptive-Coil-Configuration-Task-Switching-Network-for-MRI-Reconstruction" class="headerlink" title="HyperCoil-Recon: A Hypernetwork-based Adaptive Coil Configuration Task Switching Network for MRI Reconstruction"></a>HyperCoil-Recon: A Hypernetwork-based Adaptive Coil Configuration Task Switching Network for MRI Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04821">http://arxiv.org/abs/2308.04821</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sriprabhar/hypercoil-recon">https://github.com/sriprabhar/hypercoil-recon</a></li>
<li>paper_authors: Sriprabha Ramanarayanan, Mohammad Al Fahim, Rahul G. S., Amrit Kumar Jethi, Keerthi Ram, Mohanasankar Sivaprakasam<br>for: HyperCoil-Recon is designed to improve the speed and flexibility of parallel imaging MRI reconstruction by learning to adapt to different coil configurations on the fly.methods: The proposed method uses a hypernetwork-based approach to learn a single set of weights that can be applied to a variety of coil configurations, rather than training separate models for each configuration.results: The proposed method was able to adapt to unseen coil configurations and achieve performance comparable to coil configuration-specific models, and outperform configuration-invariant models in simulations using knee and brain data.<details>
<summary>Abstract</summary>
Parallel imaging, a fast MRI technique, involves dynamic adjustments based on the configuration i.e. number, positioning, and sensitivity of the coils with respect to the anatomy under study. Conventional deep learning-based image reconstruction models have to be trained or fine-tuned for each configuration, posing a barrier to clinical translation, given the lack of computational resources and machine learning expertise for clinicians to train models at deployment. Joint training on diverse datasets learns a single weight set that might underfit to deviated configurations. We propose, HyperCoil-Recon, a hypernetwork-based coil configuration task-switching network for multi-coil MRI reconstruction that encodes varying configurations of the numbers of coils in a multi-tasking perspective, posing each configuration as a task. The hypernetworks infer and embed task-specific weights into the reconstruction network, 1) effectively utilizing the contextual knowledge of common and varying image features among the various fields-of-view of the coils, and 2) enabling generality to unseen configurations at test time. Experiments reveal that our approach 1) adapts on the fly to various unseen configurations up to 32 coils when trained on lower numbers (i.e. 7 to 11) of randomly varying coils, and to 120 deviated unseen configurations when trained on 18 configurations in a single model, 2) matches the performance of coil configuration-specific models, and 3) outperforms configuration-invariant models with improvement margins of around 1 dB / 0.03 and 0.3 dB / 0.02 in PSNR / SSIM for knee and brain data. Our code is available at https://github.com/sriprabhar/HyperCoil-Recon
</details>
<details>
<summary>摘要</summary>
параллельный изображение, a fast MRI technique, involves dynamic adjustments based on the configuration i.e. number, positioning, and sensitivity of the coils with respect to the anatomy under study. Conventional deep learning-based image reconstruction models have to be trained or fine-tuned for each configuration, posing a barrier to clinical translation, given the lack of computational resources and machine learning expertise for clinicians to train models at deployment. Joint training on diverse datasets learns a single weight set that might underfit to deviated configurations. We propose, HyperCoil-Recon, a hypernetwork-based coil configuration task-switching network for multi-coil MRI reconstruction that encodes varying configurations of the numbers of coils in a multi-tasking perspective, posing each configuration as a task. The hypernetworks infer and embed task-specific weights into the reconstruction network, 1) effectively utilizing the contextual knowledge of common and varying image features among the various fields-of-view of the coils, and 2) enabling generality to unseen configurations at test time. Experiments reveal that our approach 1) adapts on the fly to various unseen configurations up to 32 coils when trained on lower numbers (i.e. 7 to 11) of randomly varying coils, and to 120 deviated unseen configurations when trained on 18 configurations in a single model, 2) matches the performance of coil configuration-specific models, and 3) outperforms configuration-invariant models with improvement margins of around 1 dB / 0.03 and 0.3 dB / 0.02 in PSNR / SSIM for knee and brain data. Our code is available at https://github.com/sriprabhar/HyperCoil-Recon.
</details></li>
</ul>
<hr>
<h2 id="An-Integrated-Visual-Analytics-System-for-Studying-Clinical-Carotid-Artery-Plaques"><a href="#An-Integrated-Visual-Analytics-System-for-Studying-Clinical-Carotid-Artery-Plaques" class="headerlink" title="An Integrated Visual Analytics System for Studying Clinical Carotid Artery Plaques"></a>An Integrated Visual Analytics System for Studying Clinical Carotid Artery Plaques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06285">http://arxiv.org/abs/2308.06285</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chaoqing Xu, Zhentao Zheng, Yiting Fu, Baofeng Chang, Legao Chen, Minghui Wu, Mingli Song, Jinsong Jiang</li>
<li>for: 这项研究的目的是为 vascular surgery experts 提供一个完整的血管疾病诊断和治疗指南，以帮助他们更好地理解和分析血管疾病的诱因和组成部分。</li>
<li>methods: 这个系统主要包括两个功能：首先，通过一系列的信息视觉方法显示血管疾病患者的临床生物学指标数据与血管疾病相关性的关系，并将这些数据与患者的医疗图像集成分析。其次，通过机器学习技术提高血管疾病患者的内在关系分析，并在医疗图像上显示血管疾病的空间分布。</li>
<li>results: 我们通过使用实际数据来进行两个案例研究，结果表明，我们设计的血管疾病分析系统可以帮助 vascular surgery experts 更好地诊断和治疗血管疾病。<details>
<summary>Abstract</summary>
Carotid artery plaques can cause arterial vascular diseases such as stroke and myocardial infarction, posing a severe threat to human life. However, the current clinical examination mainly relies on a direct assessment by physicians of patients' clinical indicators and medical images, lacking an integrated visualization tool for analyzing the influencing factors and composition of carotid artery plaques. We have designed an intelligent carotid artery plaque visual analysis system for vascular surgery experts to comprehensively analyze the clinical physiological and imaging indicators of carotid artery diseases. The system mainly includes two functions: First, it displays the correlation between carotid artery plaque and various factors through a series of information visualization methods and integrates the analysis of patient physiological indicator data. Second, it enhances the interface guidance analysis of the inherent correlation between the components of carotid artery plaque through machine learning and displays the spatial distribution of the plaque on medical images. Additionally, we conducted two case studies on carotid artery plaques using real data obtained from a hospital, and the results indicate that our designed carotid analysis system can effectively provide clinical diagnosis and treatment guidance for vascular surgeons.
</details>
<details>
<summary>摘要</summary>
《椎动脉瘤可能导致arterial vascular diseases 如roke和myocardial infarction，对人类生命造成严重威胁。然而，现有诊断方法主要依靠医生直接评估病人的临床指标和医疗影像，缺乏一个统合Visualization工具 для分析椎动脉瘤的影响因素和成分。我们已经设计了一个智能椎动脉瘤分析系统，用于血管医生彻底分析椎动脉瘤疾病的临床生物学和影像指标。系统主要包括两个功能：首先，显示椎动脉瘤和不同因素之间的相互关联，通过一系列的信息可视化方法，并与病人生物学指标数据进行整合分析。其次，通过机器学习增强椎动脉瘤成分的自然相互关联，并在医疗影像上显示椎动脉瘤的空间分布。此外，我们使用了实际数据，进行了两个实验研究，结果显示，我们设计的椎动脉瘤分析系统可以有效地提供临床诊断和治疗指南 для血管医生。》
</details></li>
</ul>
<hr>
<h2 id="Long-Distance-Gesture-Recognition-using-Dynamic-Neural-Networks"><a href="#Long-Distance-Gesture-Recognition-using-Dynamic-Neural-Networks" class="headerlink" title="Long-Distance Gesture Recognition using Dynamic Neural Networks"></a>Long-Distance Gesture Recognition using Dynamic Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04643">http://arxiv.org/abs/2308.04643</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shubhang Bhatnagar, Sharath Gopal, Narendra Ahuja, Liu Ren</li>
<li>for: 本研究旨在提出一种新的、准确和高效的手势识别方法，用于长距离手势识别。</li>
<li>methods: 本方法使用动态神经网络选择手势包含的空间数据特征进行进一步处理，以提高识别精度和计算效率。</li>
<li>results: 对于LD-ConGR长距离数据集，本方法与之前的状态 искусственный智能方法相比，具有更高的识别精度和计算效率。<details>
<summary>Abstract</summary>
Gestures form an important medium of communication between humans and machines. An overwhelming majority of existing gesture recognition methods are tailored to a scenario where humans and machines are located very close to each other. This short-distance assumption does not hold true for several types of interactions, for example gesture-based interactions with a floor cleaning robot or with a drone. Methods made for short-distance recognition are unable to perform well on long-distance recognition due to gestures occupying only a small portion of the input data. Their performance is especially worse in resource constrained settings where they are not able to effectively focus their limited compute on the gesturing subject. We propose a novel, accurate and efficient method for the recognition of gestures from longer distances. It uses a dynamic neural network to select features from gesture-containing spatial regions of the input sensor data for further processing. This helps the network focus on features important for gesture recognition while discarding background features early on, thus making it more compute efficient compared to other techniques. We demonstrate the performance of our method on the LD-ConGR long-distance dataset where it outperforms previous state-of-the-art methods on recognition accuracy and compute efficiency.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language: zh-CN Gestures form an important medium of communication between humans and machines. An overwhelming majority of existing gesture recognition methods are tailored to a scenario where humans and machines are located very close to each other. This short-distance assumption does not hold true for several types of interactions, for example gesture-based interactions with a floor cleaning robot or with a drone. Methods made for short-distance recognition are unable to perform well on long-distance recognition due to gestures occupying only a small portion of the input data. Their performance is especially worse in resource constrained settings where they are not able to effectively focus their limited compute on the gesturing subject. We propose a novel, accurate and efficient method for the recognition of gestures from longer distances. It uses a dynamic neural network to select features from gesture-containing spatial regions of the input sensor data for further processing. This helps the network focus on features important for gesture recognition while discarding background features early on, thus making it more compute efficient compared to other techniques. We demonstrate the performance of our method on the LD-ConGR long-distance dataset where it outperforms previous state-of-the-art methods on recognition accuracy and compute efficiency.Note: The `translate_language` directive is used to specify the language to be translated. In this case, we're using Simplified Chinese (zh-CN).
</details></li>
</ul>
<hr>
<h2 id="1st-Place-Solution-for-CVPR2023-BURST-Long-Tail-and-Open-World-Challenges"><a href="#1st-Place-Solution-for-CVPR2023-BURST-Long-Tail-and-Open-World-Challenges" class="headerlink" title="1st Place Solution for CVPR2023 BURST Long Tail and Open World Challenges"></a>1st Place Solution for CVPR2023 BURST Long Tail and Open World Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04598">http://arxiv.org/abs/2308.04598</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaer Huang</li>
<li>for: 这个研究旨在提高影像对象分割（Video Instance Segmentation，VIS）的精度和可靠性，以应对实际世界中的多样化物品。</li>
<li>methods: 我们使用了一种叫做Repeat Factor Sampling的方法，首先在LVISv0.5和COCO dataset上训练探测器，然后在TAO dataset上训练实例外观相似性头。</li>
<li>results: 我们的方法（LeTracker）在BURST test set上获得14.9 HOTAall的成绩，排名第一名。在开放世界挑战中，我们只使用64个分类（BURST Train subset和COCOdataset的交集类别）的标签资料进行训练，并在BURST test set上进行测试，获得61.4 OWTAall的成绩，排名第一名。<details>
<summary>Abstract</summary>
Currently, Video Instance Segmentation (VIS) aims at segmenting and categorizing objects in videos from a closed set of training categories that contain only a few dozen of categories, lacking the ability to handle diverse objects in real-world videos. As TAO and BURST datasets release, we have the opportunity to research VIS in long-tailed and open-world scenarios. Traditional VIS methods are evaluated on benchmarks limited to a small number of common classes, But practical applications require trackers that go beyond these common classes, detecting and tracking rare and even never-before-seen objects. Inspired by the latest MOT paper for the long tail task (Tracking Every Thing in the Wild, Siyuan Li et), for the BURST long tail challenge, we train our model on a combination of LVISv0.5 and the COCO dataset using repeat factor sampling. First, train the detector with segmentation and CEM on LVISv0.5 + COCO dataset. And then, train the instance appearance similarity head on the TAO dataset. at last, our method (LeTracker) gets 14.9 HOTAall in the BURST test set, ranking 1st in the benchmark. for the open-world challenges, we only use 64 classes (Intersection classes of BURST Train subset and COCO dataset, without LVIS dataset) annotations data training, and testing on BURST test set data and get 61.4 OWTAall, ranking 1st in the benchmark. Our code will be released to facilitate future research.
</details>
<details>
<summary>摘要</summary>
当前，视频实例分割（VIS）目标是将视频中的对象分割和分类，但是它通常只能处理固定的训练类别，lacking the ability to handle diverse objects in real-world videos。随着TAO和BURST数据集的发布，我们有机会进行VIS在长尾和开放世界场景下的研究。传统的VIS方法通常在限定的几个常见类型上进行评估，但是实际应用需要跟踪器能够检测和跟踪不同的、甚至从未seen的对象。 Drawing inspiration from the latest MOT paper on the long tail task (Tracking Every Thing in the Wild, Siyuan Li et al.), for the BURST long tail challenge, we train our model on a combination of LVISv0.5 and the COCO dataset using repeat factor sampling. Specifically, we first train the detector with segmentation and CEM on the LVISv0.5 + COCO dataset. Then, we train the instance appearance similarity head on the TAO dataset. Finally, our method (LeTracker) achieves 14.9 HOTAall in the BURST test set, ranking 1st in the benchmark. For the open-world challenges, we only use 64 classes (Intersection classes of BURST Train subset and COCO dataset, without LVIS dataset) annotations data for training, and test on BURST test set data, achieving 61.4 OWTAall, ranking 1st in the benchmark. Our code will be released to facilitate future research.
</details></li>
</ul>
<hr>
<h2 id="Semi-Supervised-Semantic-Segmentation-of-Cell-Nuclei-via-Diffusion-based-Large-Scale-Pre-Training-and-Collaborative-Learning"><a href="#Semi-Supervised-Semantic-Segmentation-of-Cell-Nuclei-via-Diffusion-based-Large-Scale-Pre-Training-and-Collaborative-Learning" class="headerlink" title="Semi-Supervised Semantic Segmentation of Cell Nuclei via Diffusion-based Large-Scale Pre-Training and Collaborative Learning"></a>Semi-Supervised Semantic Segmentation of Cell Nuclei via Diffusion-based Large-Scale Pre-Training and Collaborative Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04578">http://arxiv.org/abs/2308.04578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuchen Shao, Sourya Sengupta, Hua Li, Mark A. Anastasio<br>for:This paper focuses on the task of automated semantic segmentation of cell nuclei in microscopic images, which is crucial for disease diagnosis and tissue microenvironment analysis.methods:The proposed method is a semi-supervised framework that leverages unsupervised pre-training and collaborative learning to improve segmentation performance. The framework consists of three main components: pre-training a diffusion model on a large-scale unlabeled dataset, aggregating semantic features using a transformer-based decoder, and implementing a collaborative learning framework between the diffusion-based segmentation model and a supervised segmentation model.results:The proposed method achieves significant improvements compared to competitive semi-supervised segmentation methods and supervised baselines on four publicly available datasets. Out-of-distribution tests and thorough ablation experiments further confirm the generality and superiority of the proposed method.<details>
<summary>Abstract</summary>
Automated semantic segmentation of cell nuclei in microscopic images is crucial for disease diagnosis and tissue microenvironment analysis. Nonetheless, this task presents challenges due to the complexity and heterogeneity of cells. While supervised deep learning methods are promising, they necessitate large annotated datasets that are time-consuming and error-prone to acquire. Semi-supervised approaches could provide feasible alternatives to this issue. However, the limited annotated data may lead to subpar performance of semi-supervised methods, regardless of the abundance of unlabeled data. In this paper, we introduce a novel unsupervised pre-training-based semi-supervised framework for cell-nuclei segmentation. Our framework is comprised of three main components. Firstly, we pretrain a diffusion model on a large-scale unlabeled dataset. The diffusion model's explicit modeling capability facilitates the learning of semantic feature representation from the unlabeled data. Secondly, we achieve semantic feature aggregation using a transformer-based decoder, where the pretrained diffusion model acts as the feature extractor, enabling us to fully utilize the small amount of labeled data. Finally, we implement a collaborative learning framework between the diffusion-based segmentation model and a supervised segmentation model to further enhance segmentation performance. Experiments were conducted on four publicly available datasets to demonstrate significant improvements compared to competitive semi-supervised segmentation methods and supervised baselines. A series of out-of-distribution tests further confirmed the generality of our framework. Furthermore, thorough ablation experiments and visual analysis confirmed the superiority of our proposed method.
</details>
<details>
<summary>摘要</summary>
自动化的细胞核体分 segmentation在微scopic图像中是致命的 для疾病诊断和组织微environment分析。然而，这个任务具有复杂性和多样性的细胞问题。虽然深度学习方法有承诺，但它们需要大量的标注数据，这些数据是时间consuming和error-prone的获得的。半supervised方法可能提供可行的解决方案。然而，有限的标注数据可能会导致半supervised方法的性能下降，即使有大量的无标注数据。在这篇文章中，我们介绍了一种新的无supervised预训练基于的半supervised框架 для细胞核体分 segmentation。我们的框架由三个主要组成部分。首先，我们在一个大规模的无标注数据集上预训练了一个扩散模型。扩散模型的Explicit模型化能力使得它可以从无标注数据中学习含义特征表示。其次，我们使用一个transformer-based decoder来实现semantic feature的汇集，其中预训练的扩散模型 acts as the feature extractor，这样我们可以充分利用小量的标注数据。最后，我们实现了一种协同学习框架，其中 diffusion-based segmentation模型和一个supervised segmentation模型进行协同学习，以进一步提高分 segmentation性能。我们在四个公开available的数据集上进行了实验，并证明了与竞争性的半supervised分 segmentation方法和标注基eline相比，我们的方法具有显著的改善。此外，我们还进行了一系列out-of-distribution测试，以确认我们的框架的普适性。此外，我们还进行了严格的层次分析和可见分析，以证明我们的提议的方法的优越性。
</details></li>
</ul>
<hr>
<h2 id="Towards-Automatic-Scoring-of-Spinal-X-ray-for-Ankylosing-Spondylitis"><a href="#Towards-Automatic-Scoring-of-Spinal-X-ray-for-Ankylosing-Spondylitis" class="headerlink" title="Towards Automatic Scoring of Spinal X-ray for Ankylosing Spondylitis"></a>Towards Automatic Scoring of Spinal X-ray for Ankylosing Spondylitis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05123">http://arxiv.org/abs/2308.05123</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanhan Mo, Yao Chen, Aimee Readie, Gregory Ligozio, Thibaud Coroller, Bartłomiej W. Papież</li>
<li>for: 这个研究旨在解决评估X光影像中脊椎疾病的自动化分类问题，以减少评估成本和时间。</li>
<li>methods: 这个研究使用了一个两步自动分类管道，称为VertXGradeNet，以自动预测X光影像中脊椎疾病的 modify Stoke Ankylosing Spondylitis Spinal Score（mSASSS）。VertXGradeNet使用了我们之前开发的VU抽取管道（VertXNet）生成的VUs作为输入，并根据这些VUs预测mSASSS。</li>
<li>results: 我们的结果表明，VertXGradeNet可以在限量和不均衡的数据集上预测每个VU的mSASSS分数。总的来说，它可以在两个测试数据集上 achiev 0.56和0.51的平衡准确率，对于四个不同的mSASSS分数（即分数为0、1、2、3）。这些结果表明了这种方法的可能性，可以减少未来的脊椎X光影像评估成本。<details>
<summary>Abstract</summary>
Manually grading structural changes with the modified Stoke Ankylosing Spondylitis Spinal Score (mSASSS) on spinal X-ray imaging is costly and time-consuming due to bone shape complexity and image quality variations. In this study, we address this challenge by prototyping a 2-step auto-grading pipeline, called VertXGradeNet, to automatically predict mSASSS scores for the cervical and lumbar vertebral units (VUs) in X-ray spinal imaging. The VertXGradeNet utilizes VUs generated by our previously developed VU extraction pipeline (VertXNet) as input and predicts mSASSS based on those VUs. VertXGradeNet was evaluated on an in-house dataset of lateral cervical and lumbar X-ray images for axial spondylarthritis patients. Our results show that VertXGradeNet can predict the mSASSS score for each VU when the data is limited in quantity and imbalanced. Overall, it can achieve a balanced accuracy of 0.56 and 0.51 for 4 different mSASSS scores (i.e., a score of 0, 1, 2, 3) on two test datasets. The accuracy of the presented method shows the potential to streamline the spinal radiograph readings and therefore reduce the cost of future clinical trials.
</details>
<details>
<summary>摘要</summary>
人工评分结构变化的 modificated Stoke Ankylosing Spondylitis Spinal Score (mSASSS) 在脊梁X射线成像中是成本高和时间耗费大的，主要是因为骨形态复杂和图像质量变化。在这项研究中，我们解决这个挑战 by 开发了一个两步自动评分管线，called VertXGradeNet，以自动预测mSASSS 分数 для脊梁骨Unit (VU) 在X射线脊梁成像中。VertXGradeNet 使用我们之前开发的 VU 提取管线 (VertXNet) 生成的 VU 作为输入，并预测 mSASSS 基于这些 VU。我们的结果表明，VertXGradeNet 可以在限量和不均衡的数据集上预测每个 VU 的 mSASSS 分数。总的来说，它可以在两个测试数据集上达到平衡性的准确率为 0.56 和 0.51  для四个不同的 mSASSS 分数（即分数为 0、1、2、3）。表现的准确性表明了这种方法的潜在能力，可以 Streamline 脊梁X射线成像的读取，从而降低未来的临床试验成本。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/09/eess.IV_2023_08_09/" data-id="clly4xtg800exvl88d8bw2ekw" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/08/cs.LG_2023_08_08/" class="article-date">
  <time datetime="2023-08-07T16:00:00.000Z" itemprop="datePublished">2023-08-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/08/cs.LG_2023_08_08/">cs.LG - 2023-08-08 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="TranSTYLer-Multimodal-Behavioral-Style-Transfer-for-Facial-and-Body-Gestures-Generation"><a href="#TranSTYLer-Multimodal-Behavioral-Style-Transfer-for-Facial-and-Body-Gestures-Generation" class="headerlink" title="TranSTYLer: Multimodal Behavioral Style Transfer for Facial and Body Gestures Generation"></a>TranSTYLer: Multimodal Behavioral Style Transfer for Facial and Body Gestures Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10843">http://arxiv.org/abs/2308.10843</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mireille Fares, Catherine Pelachaud, Nicolas Obin</li>
<li>for: 本文解决了让虚拟代表人物的行为特征风格转移到另一个代表人物中，保持行为的形态不变的挑战。</li>
<li>methods: 我们提出了一种基于多模态trasformer模型的TranSTYLer模型，可以将多模态讲话者的行为风格转移到目标讲话者中，同时保持行为的意思表达。</li>
<li>results: 我们的模型在PATS数据集上进行了训练，并在对seen和unseen风格进行了目标和主观评价，结果显示我们的模型在样式转移中超过了现有模型的性能。<details>
<summary>Abstract</summary>
This paper addresses the challenge of transferring the behavior expressivity style of a virtual agent to another one while preserving behaviors shape as they carry communicative meaning. Behavior expressivity style is viewed here as the qualitative properties of behaviors. We propose TranSTYLer, a multimodal transformer based model that synthesizes the multimodal behaviors of a source speaker with the style of a target speaker. We assume that behavior expressivity style is encoded across various modalities of communication, including text, speech, body gestures, and facial expressions. The model employs a style and content disentanglement schema to ensure that the transferred style does not interfere with the meaning conveyed by the source behaviors. Our approach eliminates the need for style labels and allows the generalization to styles that have not been seen during the training phase. We train our model on the PATS corpus, which we extended to include dialog acts and 2D facial landmarks. Objective and subjective evaluations show that our model outperforms state of the art models in style transfer for both seen and unseen styles during training. To tackle the issues of style and content leakage that may arise, we propose a methodology to assess the degree to which behavior and gestures associated with the target style are successfully transferred, while ensuring the preservation of the ones related to the source content.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Accurate-Explainable-and-Private-Models-Providing-Recourse-While-Minimizing-Training-Data-Leakage"><a href="#Accurate-Explainable-and-Private-Models-Providing-Recourse-While-Minimizing-Training-Data-Leakage" class="headerlink" title="Accurate, Explainable, and Private Models: Providing Recourse While Minimizing Training Data Leakage"></a>Accurate, Explainable, and Private Models: Providing Recourse While Minimizing Training Data Leakage</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04341">http://arxiv.org/abs/2308.04341</a></li>
<li>repo_url: None</li>
<li>paper_authors: Catherine Huang, Chelse Swoopes, Christina Xiao, Jiaqi Ma, Himabindu Lakkaraju</li>
<li>for: 防止机器学习模型中的隐私泄露</li>
<li>methods: 使用差异隐私模型（DPM）和拉普拉斯回退（LR）两种方法来生成差异隐私的回退</li>
<li>results: 使用логистиック回归类ifier和实际世界和生成的数据集，发现DPM和LR方法可以减少对敌人的攻击，尤其是在低 False Positive Rate 下。当训练集大小充分时，我们发现LR方法能够很好地防止隐私泄露，同时保持模型和回退的准确率。<details>
<summary>Abstract</summary>
Machine learning models are increasingly utilized across impactful domains to predict individual outcomes. As such, many models provide algorithmic recourse to individuals who receive negative outcomes. However, recourse can be leveraged by adversaries to disclose private information. This work presents the first attempt at mitigating such attacks. We present two novel methods to generate differentially private recourse: Differentially Private Model (DPM) and Laplace Recourse (LR). Using logistic regression classifiers and real world and synthetic datasets, we find that DPM and LR perform well in reducing what an adversary can infer, especially at low FPR. When training dataset size is large enough, we find particular success in preventing privacy leakage while maintaining model and recourse accuracy with our novel LR method.
</details>
<details>
<summary>摘要</summary>
机器学习模型在影响各界预测个人结果的应用越来越普遍。因此，许多模型提供了算法性的纠正机制，以便对不良结果进行纠正。然而，这些纠正机制可能会被敌对者利用，泄露private信息。这项工作首次介绍了纠正攻击的防范方法。我们提出了两种新的敏感度保护纠正方法：敏感度保护模型（DPM）和拉普拉斯纠正（LR）。使用Logistic回归分类器和实际世界和 sintetic数据集，我们发现，DPM和LR在低False Positive Rate（FP）下具有良好的隐私保护性，特别是在训练数据集规模充分时。我们的LR方法在防止隐私泄露的同时保持模型和纠正精度的情况下表现出特别的成功。
</details></li>
</ul>
<hr>
<h2 id="RLHF-Blender-A-Configurable-Interactive-Interface-for-Learning-from-Diverse-Human-Feedback"><a href="#RLHF-Blender-A-Configurable-Interactive-Interface-for-Learning-from-Diverse-Human-Feedback" class="headerlink" title="RLHF-Blender: A Configurable Interactive Interface for Learning from Diverse Human Feedback"></a>RLHF-Blender: A Configurable Interactive Interface for Learning from Diverse Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04332">http://arxiv.org/abs/2308.04332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yannick Metz, David Lindner, Raphaël Baur, Daniel Keim, Mennatallah El-Assady</li>
<li>for: 这篇论文旨在应用人工增强学习（RL）在实际应用中，并且从多种人类反馈中学习奖励模型，同时考虑人类因素的影响。</li>
<li>methods: 这篇论文提出了RLHF-Blender，一个可配置的交互式界面，用于从人类反馈中学习奖励模型。RLHF-Blender提供了一个模块化的实验框架和实现，使研究人员可以系统地研究人类反馈的性质和质量。</li>
<li>results: RLHF-Blender可以帮助研究人员系统地探索不同类型的人类反馈，包括示例、排名、比较和自然语言指令，以及考虑人类因素的影响。RLHF-Blender还提供了一些具体的研究机会，详细信息请参阅<a target="_blank" rel="noopener" href="https://rlhfblender.info/">https://rlhfblender.info/</a>.<details>
<summary>Abstract</summary>
To use reinforcement learning from human feedback (RLHF) in practical applications, it is crucial to learn reward models from diverse sources of human feedback and to consider human factors involved in providing feedback of different types. However, the systematic study of learning from diverse types of feedback is held back by limited standardized tooling available to researchers. To bridge this gap, we propose RLHF-Blender, a configurable, interactive interface for learning from human feedback. RLHF-Blender provides a modular experimentation framework and implementation that enables researchers to systematically investigate the properties and qualities of human feedback for reward learning. The system facilitates the exploration of various feedback types, including demonstrations, rankings, comparisons, and natural language instructions, as well as studies considering the impact of human factors on their effectiveness. We discuss a set of concrete research opportunities enabled by RLHF-Blender. More information is available at https://rlhfblender.info/.
</details>
<details>
<summary>摘要</summary>
为了在实际应用中使用人类反馈学习（RLHF），必须从多种人类反馈来学习奖励模型，并考虑人类提供反馈的因素。然而，实际研究从多种反馈类型中学习的系统atic study 受到了研究人员的限制。为了bridging这个差距，我们提议RLHF-Blender，一个可配置的交互式界面，用于学习人类反馈。RLHF-Blender提供了一个可组合的实验框架和实现，帮助研究人员系统地 investigate人类反馈的性质和质量，以及人类因素对其效果的影响。系统支持 exploration多种反馈类型，包括示例、排名、比较和自然语言指令，以及考虑人类因素对其效果的研究。我们介绍了RLHF-Blender启发的具体研究机会。更多信息请访问https://rlhfblender.info/.
</details></li>
</ul>
<hr>
<h2 id="Cooperative-Multi-agent-Bandits-Distributed-Algorithms-with-Optimal-Individual-Regret-and-Constant-Communication-Costs"><a href="#Cooperative-Multi-agent-Bandits-Distributed-Algorithms-with-Optimal-Individual-Regret-and-Constant-Communication-Costs" class="headerlink" title="Cooperative Multi-agent Bandits: Distributed Algorithms with Optimal Individual Regret and Constant Communication Costs"></a>Cooperative Multi-agent Bandits: Distributed Algorithms with Optimal Individual Regret and Constant Communication Costs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04314">http://arxiv.org/abs/2308.04314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lin Yang, Xuchuang Wang, Mohammad Hajiesmaili, Lijun Zhang, John C. S. Lui, Don Towsley</li>
<li>for: 这个研究旨在开发多代理多臂枪击游戏中的合作多代理算法，以实现最佳的集体和个人后悔，并且将通信成本降到最低。</li>
<li>methods: 这篇论文使用了两种方法：领导者-追随者和完全分布式算法。这些方法都能够实现集体后悔的最佳后悔，但是领导者-追随者算法对个人后悔不够优秀，而完全分布式算法则对通信成本不够优秀。</li>
<li>results: 这篇论文提出了一个简单 yet有效的通信策略，并将其整合到一个学习算法中，以实现最佳的个人后悔和常规通信成本。<details>
<summary>Abstract</summary>
Recently, there has been extensive study of cooperative multi-agent multi-armed bandits where a set of distributed agents cooperatively play the same multi-armed bandit game. The goal is to develop bandit algorithms with the optimal group and individual regrets and low communication between agents. The prior work tackled this problem using two paradigms: leader-follower and fully distributed algorithms. Prior algorithms in both paradigms achieve the optimal group regret. The leader-follower algorithms achieve constant communication costs but fail to achieve optimal individual regrets. The state-of-the-art fully distributed algorithms achieve optimal individual regrets but fail to achieve constant communication costs. This paper presents a simple yet effective communication policy and integrates it into a learning algorithm for cooperative bandits. Our algorithm achieves the best of both paradigms: optimal individual regret and constant communication costs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-Model-Inversion-Eavesdropping-Attack-in-Semantic-Communication-Systems"><a href="#The-Model-Inversion-Eavesdropping-Attack-in-Semantic-Communication-Systems" class="headerlink" title="The Model Inversion Eavesdropping Attack in Semantic Communication Systems"></a>The Model Inversion Eavesdropping Attack in Semantic Communication Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04304">http://arxiv.org/abs/2308.04304</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhao Chen, Qianqian Yang, Zhiguo Shi, Jiming Chen</li>
<li>for: 本研究探讨了 semantic communication 系统中的隐私泄露问题，并提出了一种基于 random permutation 和 substitution 的防御方法来解决这问题。</li>
<li>methods: 本研究使用了 model inversion eavesdropping attack (MIEA) 来攻击 semantic communication 系统，并考虑了 white-box 和 black-box 两种设定。</li>
<li>results: 实验结果表明，提出的防御方法可以有效防止 MIEA，并且在不同的 channel conditions 下可以 obtaint 高质量的重建结果。<details>
<summary>Abstract</summary>
In recent years, semantic communication has been a popular research topic for its superiority in communication efficiency. As semantic communication relies on deep learning to extract meaning from raw messages, it is vulnerable to attacks targeting deep learning models. In this paper, we introduce the model inversion eavesdropping attack (MIEA) to reveal the risk of privacy leaks in the semantic communication system. In MIEA, the attacker first eavesdrops the signal being transmitted by the semantic communication system and then performs model inversion attack to reconstruct the raw message, where both the white-box and black-box settings are considered. Evaluation results show that MIEA can successfully reconstruct the raw message with good quality under different channel conditions. We then propose a defense method based on random permutation and substitution to defend against MIEA in order to achieve secure semantic communication. Our experimental results demonstrate the effectiveness of the proposed defense method in preventing MIEA.
</details>
<details>
<summary>摘要</summary>
现在的几年，semantic communication在通信效率方面的研究非常流行，因为它可以通过深度学习提取消息的意义来提高通信效率。然而，由于semantic communication依赖于深度学习模型，因此它是攻击目标。在这篇论文中，我们介绍了模型反向窃听攻击（MIEA），以探索 semantic communication系统中隐私泄露的风险。在MIEA中，攻击者首先监听 semantic communication系统传输的信号，然后通过模型反向攻击来重construct原始消息，包括白盒和黑盒两种设置。我们的评估结果表明，MIEA可以在不同的通信频率下成功重construct原始消息，并且可以在不同的通信条件下保持高质量。我们then propose了一种基于随机排序和替换的防御方法，以防止MIEA。我们的实验结果表明，我们的防御方法可以有效防止MIEA，以实现安全的semantic communication。
</details></li>
</ul>
<hr>
<h2 id="Comparative-Analysis-of-the-wav2vec-2-0-Feature-Extractor"><a href="#Comparative-Analysis-of-the-wav2vec-2-0-Feature-Extractor" class="headerlink" title="Comparative Analysis of the wav2vec 2.0 Feature Extractor"></a>Comparative Analysis of the wav2vec 2.0 Feature Extractor</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04286">http://arxiv.org/abs/2308.04286</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Vieting, Ralf Schlüter, Hermann Ney</li>
<li>for:  This paper aims to evaluate the capability of neural raw waveform feature extractors (FEs) in replacing standard feature extraction methods in a connectionist temporal classification (CTC) automatic speech recognition (ASR) model, and to compare it with an alternative neural FE.</li>
<li>methods:  The paper uses a convolutional FE, which operates directly on the speech waveform, and a set of bandpass filters to analyze the learned filters and the most important information for the ASR system.</li>
<li>results:  The paper shows that both the neural raw waveform FE and the alternative neural FE are competitive with traditional FEs on the LibriSpeech benchmark, and analyzes the effect of the individual components. The paper also shows that the most important information for the ASR system is obtained by a set of bandpass filters.<details>
<summary>Abstract</summary>
Automatic speech recognition (ASR) systems typically use handcrafted feature extraction pipelines. To avoid their inherent information loss and to achieve more consistent modeling from speech to transcribed text, neural raw waveform feature extractors (FEs) are an appealing approach. Also the wav2vec 2.0 model, which has recently gained large popularity, uses a convolutional FE which operates directly on the speech waveform. However, it is not yet studied extensively in the literature. In this work, we study its capability to replace the standard feature extraction methods in a connectionist temporal classification (CTC) ASR model and compare it to an alternative neural FE. We show that both are competitive with traditional FEs on the LibriSpeech benchmark and analyze the effect of the individual components. Furthermore, we analyze the learned filters and show that the most important information for the ASR system is obtained by a set of bandpass filters.
</details>
<details>
<summary>摘要</summary>
自动语音识别（ASR）系统通常使用手工设计的特征提取管道。以避免它们的内在信息损失并实现更一致的模型化从语音到转录文本，神经原始波形特征提取器（FE）是一种吸引人的方法。另外，最近广受欢迎的wav2vec 2.0模型使用了一种卷积 convolutional FE，该模型直接操作语音波形。然而，它在文献中还没有得到广泛的研究。在这个工作中，我们研究了它是否可以取代标准特征提取方法在一个连接主义时间分类（CTC） ASR 模型中，并与一个 alternativa neural FE 进行比较。我们发现两者都能够与传统的特征提取方法竞争在 LibriSpeech benchmark 上，并分析了各个组件的效果。此外，我们还分析了学习的滤波器，发现最重要的信息对 ASR 系统来说是由一组带通频滤波器提取出来的。
</details></li>
</ul>
<hr>
<h2 id="In-Context-Alignment-Chat-with-Vanilla-Language-Models-Before-Fine-Tuning"><a href="#In-Context-Alignment-Chat-with-Vanilla-Language-Models-Before-Fine-Tuning" class="headerlink" title="In-Context Alignment: Chat with Vanilla Language Models Before Fine-Tuning"></a>In-Context Alignment: Chat with Vanilla Language Models Before Fine-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04275">http://arxiv.org/abs/2308.04275</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xhan77/in-context-alignment">https://github.com/xhan77/in-context-alignment</a></li>
<li>paper_authors: Xiaochuang Han</li>
<li>for: 这个论文是关于在运行时进行匹配的研究，即使没有任何细化调教。</li>
<li>methods: 作者使用了一个名为Llama-2的预训练语言模型，并通过在对话式指令下提取示例来进行匹配。</li>
<li>results: 结果显示，在没有改变模型参数的情况下，通过在上下文中学习进行匹配，可以使得 vanilla 语言模型与 OpenAI 提供的 text-davinci-003 模型相当，并且比直接提示的方法提高了7倍的赢利率。<details>
<summary>Abstract</summary>
In this note, we explore inference-time alignment through in-context learning. We consider a vanilla pretrained language model Llama-2 before any fine-tuning and retrieve an average of 9 demonstration alignment examples when the model is prompted to follow chat-style instructions. Compared to direct prompting, the in-context alignment without changing model weights leads to a 7x increase in win-rate w.r.t. the text-davinci-003 model from OpenAI, making the vanilla language model comparable to strong baselines with alignment fine-tuning.
</details>
<details>
<summary>摘要</summary>
在这份笔记中，我们探讨了在上下文学习中进行推理时的对齐。我们考虑了未经任何精度调整的语言模型Llama-2，并在模型被提示按照带ialog式指令时获取了9个示例对齐例。与直接提示相比，无需更改模型参数的上下文对齐导致了与文本-达梦系统OpenAI的模型003相比的7倍增加赢利率，使得vanilla语言模型与对齐调整相当。
</details></li>
</ul>
<hr>
<h2 id="Teacher-Student-Architecture-for-Knowledge-Distillation-A-Survey"><a href="#Teacher-Student-Architecture-for-Knowledge-Distillation-A-Survey" class="headerlink" title="Teacher-Student Architecture for Knowledge Distillation: A Survey"></a>Teacher-Student Architecture for Knowledge Distillation: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04268">http://arxiv.org/abs/2308.04268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengming Hu, Xuan Li, Dan Liu, Haolun Wu, Xi Chen, Ju Wang, Xue Liu</li>
<li>for: 这篇论文的目的是对于知识传递（Knowledge Distillation，KD）领域的研究和发展，特别是关于多个知识传递目标的教师生成架构。</li>
<li>methods: 这篇论文使用了教师生成架构，包括知识传递、知识增强、知识适应和知识压缩等多种知识传递目标。它还使用了各种学习算法和有效的传递方案。</li>
<li>results: 这篇论文总结了现有的教师生成架构和学习算法，并评估了它们在多个知识传递目标上的性能。它还概述了现有的应用案例，包括分类、识别、生成、排名和回归等多种应用。<details>
<summary>Abstract</summary>
Although Deep neural networks (DNNs) have shown a strong capacity to solve large-scale problems in many areas, such DNNs are hard to be deployed in real-world systems due to their voluminous parameters. To tackle this issue, Teacher-Student architectures were proposed, where simple student networks with a few parameters can achieve comparable performance to deep teacher networks with many parameters. Recently, Teacher-Student architectures have been effectively and widely embraced on various knowledge distillation (KD) objectives, including knowledge compression, knowledge expansion, knowledge adaptation, and knowledge enhancement. With the help of Teacher-Student architectures, current studies are able to achieve multiple distillation objectives through lightweight and generalized student networks. Different from existing KD surveys that primarily focus on knowledge compression, this survey first explores Teacher-Student architectures across multiple distillation objectives. This survey presents an introduction to various knowledge representations and their corresponding optimization objectives. Additionally, we provide a systematic overview of Teacher-Student architectures with representative learning algorithms and effective distillation schemes. This survey also summarizes recent applications of Teacher-Student architectures across multiple purposes, including classification, recognition, generation, ranking, and regression. Lastly, potential research directions in KD are investigated, focusing on architecture design, knowledge quality, and theoretical studies of regression-based learning, respectively. Through this comprehensive survey, industry practitioners and the academic community can gain valuable insights and guidelines for effectively designing, learning, and applying Teacher-Student architectures on various distillation objectives.
</details>
<details>
<summary>摘要</summary>
although deep neural networks (DNNs) have shown strong capacity to solve large-scale problems in many areas, such DNNs are hard to be deployed in real-world systems due to their voluminous parameters. to tackle this issue, teacher-student architectures were proposed, where simple student networks with a few parameters can achieve comparable performance to deep teacher networks with many parameters. recently, teacher-student architectures have been effectively and widely embraced on various knowledge distillation (KD) objectives, including knowledge compression, knowledge expansion, knowledge adaptation, and knowledge enhancement. with the help of teacher-student architectures, current studies are able to achieve multiple distillation objectives through lightweight and generalized student networks. different from existing KD surveys that primarily focus on knowledge compression, this survey first explores teacher-student architectures across multiple distillation objectives. this survey presents an introduction to various knowledge representations and their corresponding optimization objectives. additionally, we provide a systematic overview of teacher-student architectures with representative learning algorithms and effective distillation schemes. this survey also summarizes recent applications of teacher-student architectures across multiple purposes, including classification, recognition, generation, ranking, and regression. lastly, potential research directions in KD are investigated, focusing on architecture design, knowledge quality, and theoretical studies of regression-based learning, respectively. through this comprehensive survey, industry practitioners and the academic community can gain valuable insights and guidelines for effectively designing, learning, and applying teacher-student architectures on various distillation objectives.
</details></li>
</ul>
<hr>
<h2 id="BarlowRL-Barlow-Twins-for-Data-Efficient-Reinforcement-Learning"><a href="#BarlowRL-Barlow-Twins-for-Data-Efficient-Reinforcement-Learning" class="headerlink" title="BarlowRL: Barlow Twins for Data-Efficient Reinforcement Learning"></a>BarlowRL: Barlow Twins for Data-Efficient Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04263">http://arxiv.org/abs/2308.04263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omer Veysel Cagatan</li>
<li>for: 这个论文是为了提出一种数据效果的强化学习代理人BarlowRL，该代理人结合Barlow Twins自动学习框架和DER数据效果雨bow算法。</li>
<li>methods: BarlowRL使用了Barlow Twins自动学习框架和DER算法，以提高数据效果和避免维度塌陷。</li>
<li>results: BarlowRL在Atari 100k测试 benchmark 上表现出色，超过了DER和其对应的对比算法CURL。 BarlowRL能够充分利用均匀分布的状态表示，从而达到了很高的性能。<details>
<summary>Abstract</summary>
This paper introduces BarlowRL, a data-efficient reinforcement learning agent that combines the Barlow Twins self-supervised learning framework with DER (Data-Efficient Rainbow) algorithm. BarlowRL outperforms both DER and its contrastive counterpart CURL on the Atari 100k benchmark. BarlowRL avoids dimensional collapse by enforcing information spread to the whole space. This helps RL algorithms to utilize uniformly spread state representation that eventually results in a remarkable performance. The integration of Barlow Twins with DER enhances data efficiency and achieves superior performance in the RL tasks. BarlowRL demonstrates the potential of incorporating self-supervised learning techniques to improve RL algorithms.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了BarlowRL，一种数据效率的 reinforcement learning代理人，它将Barlow Twins自我超vised学习框架和DER（数据效率雨bow）算法相结合。BarlowRL在Atari 100k测试 benchmark 上表现出色，比DER和其它对比算法 CURL 更高。BarlowRL通过保证信息在全空间传播来避免维度坍缩，从而使RL算法可以利用 uniformly 分布的状态表示，最终导致了非常出色的性能。将Barlow Twins与DER相结合可以提高数据效率，并在RL任务中实现优秀表现。BarlowRL表明了在RL算法中 интеGRating自我超vised学习技术的潜力。
</details></li>
</ul>
<hr>
<h2 id="SDLFormer-A-Sparse-and-Dense-Locality-enhanced-Transformer-for-Accelerated-MR-Image-Reconstruction"><a href="#SDLFormer-A-Sparse-and-Dense-Locality-enhanced-Transformer-for-Accelerated-MR-Image-Reconstruction" class="headerlink" title="SDLFormer: A Sparse and Dense Locality-enhanced Transformer for Accelerated MR Image Reconstruction"></a>SDLFormer: A Sparse and Dense Locality-enhanced Transformer for Accelerated MR Image Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04262">http://arxiv.org/abs/2308.04262</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rahul-gs-16/sdlformer">https://github.com/rahul-gs-16/sdlformer</a></li>
<li>paper_authors: Rahul G. S., Sriprabha Ramnarayanan, Mohammad Al Fahim, Keerthi Ram, Preejith S. P, Mohanasankar Sivaprakasam</li>
<li>for: 本研究旨在提出一种基于窗口变换器的快速MRI图像重建方法，以提高MRI图像重建的计算效率。</li>
<li>methods: 该方法使用了窗口变换器网络，并将它与增强 distant neighborhood pixel relationship 的层 dilated attention 机制和depth-wise convolutions 结合在一起。</li>
<li>results: 对多栅积MRI加速的多栅积PD、PDFS和T2对照图进行了广泛的实验，并与其他重建架构和平行领域自我超vised学习基准进行了比较。结果显示，提出的方法在PSNR和SSIM两个指标上具有1.40dB和0.028的提升差。<details>
<summary>Abstract</summary>
Transformers have emerged as viable alternatives to convolutional neural networks owing to their ability to learn non-local region relationships in the spatial domain. The self-attention mechanism of the transformer enables transformers to capture long-range dependencies in the images, which might be desirable for accelerated MRI image reconstruction as the effect of undersampling is non-local in the image domain. Despite its computational efficiency, the window-based transformers suffer from restricted receptive fields as the dependencies are limited to within the scope of the image windows. We propose a window-based transformer network that integrates dilated attention mechanism and convolution for accelerated MRI image reconstruction. The proposed network consists of dilated and dense neighborhood attention transformers to enhance the distant neighborhood pixel relationship and introduce depth-wise convolutions within the transformer module to learn low-level translation invariant features for accelerated MRI image reconstruction. The proposed model is trained in a self-supervised manner. We perform extensive experiments for multi-coil MRI acceleration for coronal PD, coronal PDFS and axial T2 contrasts with 4x and 5x under-sampling in self-supervised learning based on k-space splitting. We compare our method against other reconstruction architectures and the parallel domain self-supervised learning baseline. Results show that the proposed model exhibits improvement margins of (i) around 1.40 dB in PSNR and around 0.028 in SSIM on average over other architectures (ii) around 1.44 dB in PSNR and around 0.029 in SSIM over parallel domain self-supervised learning. The code is available at https://github.com/rahul-gs-16/sdlformer.git
</details>
<details>
<summary>摘要</summary>
transformers 已经成为了卷积神经网络的可行 альтернатив，因为它们可以学习图像空间中的非本地区关系。 transformers 的归一化机制使得它们可以捕捉图像中的长距离依赖关系，这可能是加速 MRI 图像重建的潜在优点。 despite its 计算效率，窗口基于的 transformers 受限于图像窗口范围内的依赖关系。 we propose a window-based transformer network that integrates dilated attention mechanism and convolution for accelerated MRI image reconstruction. the proposed network consists of dilated and dense neighborhood attention transformers to enhance the distant neighborhood pixel relationship and introduce depth-wise convolutions within the transformer module to learn low-level translation invariant features for accelerated MRI image reconstruction. the proposed model is trained in a self-supervised manner. we perform extensive experiments for multi-coil MRI acceleration for coronal PD, coronal PDFS and axial T2 contrasts with 4x and 5x under-sampling in self-supervised learning based on k-space splitting. we compare our method against other reconstruction architectures and the parallel domain self-supervised learning baseline. results show that the proposed model exhibits improvement margins of (i) around 1.40 dB in PSNR and around 0.028 in SSIM on average over other architectures (ii) around 1.44 dB in PSNR and around 0.029 in SSIM over parallel domain self-supervised learning. the code is available at https://github.com/rahul-gs-16/sdlformer.git.
</details></li>
</ul>
<hr>
<h2 id="Advancing-Natural-Language-Based-Audio-Retrieval-with-PaSST-and-Large-Audio-Caption-Data-Sets"><a href="#Advancing-Natural-Language-Based-Audio-Retrieval-with-PaSST-and-Large-Audio-Caption-Data-Sets" class="headerlink" title="Advancing Natural-Language Based Audio Retrieval with PaSST and Large Audio-Caption Data Sets"></a>Advancing Natural-Language Based Audio Retrieval with PaSST and Large Audio-Caption Data Sets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04258">http://arxiv.org/abs/2308.04258</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/optimusprimus/dcase2023_task6b">https://github.com/optimusprimus/dcase2023_task6b</a></li>
<li>paper_authors: Paul Primus, Khaled Koutini, Gerhard Widmer</li>
<li>for: 这篇论文旨在提出一种基于预训练文本和 спектogram transformer 的文本至声音回 recuperation 系统。</li>
<li>methods: 该方法将录音和文本描述 proyect 到一个共享的声音-caption 空间中，使相关的不同模式的示例在空间上相互靠近。通过系统atic分析，我们查看每个系统组件对回 recuperation性能的影响。</li>
<li>results: 我们发现两个关键组件对回 recuperation性能具有关键作用：基于自我注意力的声音编码器为声音嵌入，以及在预训练阶段使用additional human-generated和 sintetic 数据集。我们进一步尝试了在 ClothoV2 描述中添加可用的关键词，但这只导致了微妙的改进。我们的系统在 2023 年 DCASE 挑战中 Ranking 第一，并在 ClothoV2  bencmark 上比当前状态的艺术高5.6 pp. mAP@10。<details>
<summary>Abstract</summary>
This work presents a text-to-audio-retrieval system based on pre-trained text and spectrogram transformers. Our method projects recordings and textual descriptions into a shared audio-caption space in which related examples from different modalities are close. Through a systematic analysis, we examine how each component of the system influences retrieval performance. As a result, we identify two key components that play a crucial role in driving performance: the self-attention-based audio encoder for audio embedding and the utilization of additional human-generated and synthetic data sets during pre-training. We further experimented with augmenting ClothoV2 captions with available keywords to increase their variety; however, this only led to marginal improvements. Our system ranked first in the 2023's DCASE Challenge, and it outperforms the current state of the art on the ClothoV2 benchmark by 5.6 pp. mAP@10.
</details>
<details>
<summary>摘要</summary>
Note:* "pre-trained text and spectrogram transformers" refers to the use of pre-trained language models and audio processing models to improve the performance of the text-to-audio retrieval system.* "self-attention-based audio encoder" refers to a specific type of audio encoding method that uses self-attention mechanisms to extract relevant features from the audio data.* "additional human-generated and synthetic data sets" refers to the use of additional data sets that are generated by humans or by algorithms to improve the performance of the system.* "ClothoV2 captions" refers to a specific dataset of textual descriptions that are used to evaluate the performance of the text-to-audio retrieval system.* "mAP@10" refers to the mean average precision at the 10th position, which is a common evaluation metric for text-to-audio retrieval systems.
</details></li>
</ul>
<hr>
<h2 id="Federated-Inference-with-Reliable-Uncertainty-Quantification-over-Wireless-Channels-via-Conformal-Prediction"><a href="#Federated-Inference-with-Reliable-Uncertainty-Quantification-over-Wireless-Channels-via-Conformal-Prediction" class="headerlink" title="Federated Inference with Reliable Uncertainty Quantification over Wireless Channels via Conformal Prediction"></a>Federated Inference with Reliable Uncertainty Quantification over Wireless Channels via Conformal Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04237">http://arxiv.org/abs/2308.04237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meiyi Zhu, Matteo Zecchin, Sangwoo Park, Caili Guo, Chunyan Feng, Osvaldo Simeone</li>
<li>for: 这个论文主要探讨了一种名为联邦兼容预测（Federated Conformal Prediction，简称FCP）的技术，该技术利用设备到服务器的通信来提高服务器的决策质量。</li>
<li>methods: 这个论文提出了一种新的协议，名为无线联邦兼容预测（WFCP），它基于类型基本多访问（TBMA）和一种新的量化更正策略。WFCP提供了正式的可靠性保证，包括预测集的覆盖率。</li>
<li>results: 数据分析表明，WFCP在有限通信资源和大量设备情况下具有显著优势，特别是在对比数字实现的现有联邦CP方案时。<details>
<summary>Abstract</summary>
Consider a setting in which devices and a server share a pre-trained model. The server wishes to make an inference on a new input given the model. Devices have access to data, previously not used for training, and can communicate to the server over a common wireless channel. If the devices have no access to the new input, can communication from devices to the server enhance the quality of the inference decision at the server? Recent work has introduced federated conformal prediction (CP), which leverages devices-to-server communication to improve the reliability of the server's decision. With federated CP, devices communicate to the server information about the loss accrued by the shared pre-trained model on the local data, and the server leverages this information to calibrate a decision interval, or set, so that it is guaranteed to contain the correct answer with a pre-defined target reliability level. Previous work assumed noise-free communication, whereby devices can communicate a single real number to the server. In this paper, we study for the first time federated CP in a wireless setting. We introduce a novel protocol, termed wireless federated conformal prediction (WFCP), which builds on type-based multiple access (TBMA) and on a novel quantile correction strategy. WFCP is proved to provide formal reliability guarantees in terms of coverage of the predicted set produced by the server. Using numerical results, we demonstrate the significant advantages of WFCP against digital implementations of existing federated CP schemes, especially in regimes with limited communication resources and/or large number of devices.
</details>
<details>
<summary>摘要</summary>
假设设备和服务器共享预训练模型。服务器想要对新输入进行推断。设备有访问未使用过训练数据的权限，可以与服务器通过共享的无线通信chnnel进行通信。如果设备没有访问新输入，是否可以通过设备到服务器的交流提高服务器的推断决策质量？latest work introduce federated conformal prediction (CP), which leverages devices-to-server communication to improve the reliability of the server's decision. With federated CP, devices communicate to the server information about the loss accrued by the shared pre-trained model on the local data, and the server leverages this information to calibrate a decision interval, or set, so that it is guaranteed to contain the correct answer with a pre-defined target reliability level. Previous work assumed noise-free communication, whereby devices can communicate a single real number to the server. In this paper, we study for the first time federated CP in a wireless setting. We introduce a novel protocol, termed wireless federated conformal prediction (WFCP), which builds on type-based multiple access (TBMA) and on a novel quantile correction strategy. WFCP is proved to provide formal reliability guarantees in terms of coverage of the predicted set produced by the server. Using numerical results, we demonstrate the significant advantages of WFCP against digital implementations of existing federated CP schemes, especially in regimes with limited communication resources and/or large number of devices.
</details></li>
</ul>
<hr>
<h2 id="OpinionConv-Conversational-Product-Search-with-Grounded-Opinions"><a href="#OpinionConv-Conversational-Product-Search-with-Grounded-Opinions" class="headerlink" title="OpinionConv: Conversational Product Search with Grounded Opinions"></a>OpinionConv: Conversational Product Search with Grounded Opinions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04226">http://arxiv.org/abs/2308.04226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vahid Sadiri Javadi, Martin Potthast, Lucie Flek</li>
<li>for: 本研究旨在帮助语言模型在对话中更加真实地表达用户的主观意见，以便更好地帮助用户做出决策。</li>
<li>methods: 本研究使用产品评论作为实际用户的主观意见的来源，并通过一种新的对话模型来 simulate sales conversations。</li>
<li>results: 研究发现，使用产品评论来生成对话可以提供更加真实的用户意见，并且用户认为这些意见具有决策价值。<details>
<summary>Abstract</summary>
When searching for products, the opinions of others play an important role in making informed decisions. Subjective experiences about a product can be a valuable source of information. This is also true in sales conversations, where a customer and a sales assistant exchange facts and opinions about products. However, training an AI for such conversations is complicated by the fact that language models do not possess authentic opinions for their lack of real-world experience. We address this problem by leveraging product reviews as a rich source of product opinions to ground conversational AI in true subjective narratives. With OpinionConv, we develop the first conversational AI for simulating sales conversations. To validate the generated conversations, we conduct several user studies showing that the generated opinions are perceived as realistic. Our assessors also confirm the importance of opinions as an informative basis for decision-making.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:在寻找产品时，其他人的意见具有重要的指导作用，可以提供有价值的信息。这也是销售对话中的事实，客户和销售助手交换产品的信息和意见。然而，用语言模型训练销售对话是复杂的，因为语言模型缺乏真实的世界经验。我们解决这个问题，利用产品评论作为产品意见的丰富源，将对话AI根据真实的主观故事进行定制。通过我们的 OpinionConv，我们开发了首个用于模拟销售对话的对话AI。为验证生成的对话，我们进行了多个用户研究，显示生成的意见被视为真实。我们的评估人也证实了意见作为决策基础的重要性。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Interpretation-and-Validation-of-Graph-Attention-based-Explanations-for-GNN-Models"><a href="#Semantic-Interpretation-and-Validation-of-Graph-Attention-based-Explanations-for-GNN-Models" class="headerlink" title="Semantic Interpretation and Validation of Graph Attention-based Explanations for GNN Models"></a>Semantic Interpretation and Validation of Graph Attention-based Explanations for GNN Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04220">http://arxiv.org/abs/2308.04220</a></li>
<li>repo_url: None</li>
<li>paper_authors: Efimia Panagiotaki, Daniele De Martini, Lars Kunze</li>
<li>for: 这 paper aims to enhance the explainability of Graph Neural Network (GNN)-based models by applying semantic attention and establishing a correlation between predicted feature-importance weights and model accuracy.</li>
<li>methods: 该 paper introduces semantically-informed perturbations and utilizes attention mechanisms to provide feature-based explanations for GNN predictions. It extends existing attention-based graph-explainability methods by analyzing the behavior of predicted attention-weights distribution in correlation with model accuracy.</li>
<li>results: 通过应用这些方法， paper successfully identifies key semantic classes that contribute to enhanced performance and generates reliable post-hoc semantic explanations for the GNN model.<details>
<summary>Abstract</summary>
In this work, we propose a methodology for investigating the application of semantic attention to enhance the explainability of Graph Neural Network (GNN)-based models, introducing semantically-informed perturbations and establishing a correlation between predicted feature-importance weights and model accuracy. Graph Deep Learning (GDL) has emerged as a promising field for tasks like scene interpretation, leveraging flexible graph structures to concisely describe complex features and relationships. As traditional explainability methods used in eXplainable AI (XAI) cannot be directly applied to such structures, graph-specific approaches are introduced. Attention mechanisms have demonstrated their efficacy in estimating the importance of input features in deep learning models and thus have been previously employed to provide feature-based explanations for GNN predictions. Building upon these insights, we extend existing attention-based graph-explainability methods investigating the use of attention weights as importance indicators of semantically sorted feature sets. Through analysing the behaviour of predicted attention-weights distribution in correlation with model accuracy, we gain valuable insights into feature importance with respect to the behaviour of the GNN model. We apply our methodology to a lidar pointcloud estimation model successfully identifying key semantic classes that contribute to enhanced performance effectively generating reliable post-hoc semantic explanations.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们提出了一种方法来使用semantic attention提高graph neural network（GNN）模型的解释性，通过引入semantically-informed的干扰和建立predicted feature-importance权重和模型精度之间的相关性。 Graph Deep Learning（GDL）已经成为一个有前途的领域，用于场景理解等任务，利用flexible graph结构 concisely describe复杂的特征和关系。 Traditional explainability方法在XAI中无法直接应用于这些结构，因此需要graph-specific approach。 Attention机制已经在深度学习模型中证明其能力 estimating输入特征的重要性，因此在GNN预测中也被使用来提供基于特征的解释。 在这些基础上，我们进一步扩展了现有的注意力基本graph-explainability方法， investigate使用注意力权重作为semantic sorted feature set的重要性指标。 通过分析预测注意力权重分布与模型精度之间的相关性，我们获得了对feature importance的重要性见解，并且可以有效地生成post-hoc semantic explanation。 我们应用了我们的方法ology lidar pointcloud estimation模型，成功地标识了改进性的 semantic classes，从而生成了可靠的后期semantic explanation。
</details></li>
</ul>
<hr>
<h2 id="Varying-coefficients-for-regional-quantile-via-KNN-based-LASSO-with-applications-to-health-outcome-study"><a href="#Varying-coefficients-for-regional-quantile-via-KNN-based-LASSO-with-applications-to-health-outcome-study" class="headerlink" title="Varying-coefficients for regional quantile via KNN-based LASSO with applications to health outcome study"></a>Varying-coefficients for regional quantile via KNN-based LASSO with applications to health outcome study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04212">http://arxiv.org/abs/2308.04212</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/younghhk/software">https://github.com/younghhk/software</a></li>
<li>paper_authors: Seyoung Park, Eun Ryung Lee, Hyokyoung G. Hong</li>
<li>for: 本研究旨在建立一种动态模型，描述健康结果和风险因素之间的关系，并考虑年龄的时间变化。</li>
<li>methods: 本研究使用 varying-coefficients (VC) 区域量化回归，结合 K-nearest neighbors (KNN) 融合lasso，以捕捉年龄的时间变化效应。</li>
<li>results: 研究结果表明，提议的方法能够准确捕捉健康结果和风险因素之间的复杂年龄dependent关系。<details>
<summary>Abstract</summary>
Health outcomes, such as body mass index and cholesterol levels, are known to be dependent on age and exhibit varying effects with their associated risk factors. In this paper, we propose a novel framework for dynamic modeling of the associations between health outcomes and risk factors using varying-coefficients (VC) regional quantile regression via K-nearest neighbors (KNN) fused Lasso, which captures the time-varying effects of age. The proposed method has strong theoretical properties, including a tight estimation error bound and the ability to detect exact clustered patterns under certain regularity conditions. To efficiently solve the resulting optimization problem, we develop an alternating direction method of multipliers (ADMM) algorithm. Our empirical results demonstrate the efficacy of the proposed method in capturing the complex age-dependent associations between health outcomes and their risk factors.
</details>
<details>
<summary>摘要</summary>
健康结果，如体重指数和胆固酶水平，与年龄存在相互关系，其影响因素的效果随着年龄的变化而变化。在这篇论文中，我们提出了一种新的动态模型，使用不同系数（VC）地域量规 regression via K-最近邻（KNN）混合lasso，以捕捉年龄的时间变化效应。我们的提议方法具有强制实现的理论属性，包括紧张估计误差 bound和在某些常数条件下检测精确的征性征 Patterns。为解决相应的优化问题，我们开发了一种分解方法 OF multipliers（ADMM）算法。我们的实验结果表明，我们的提议方法能够准确捕捉健康结果和其风险因素之间的复杂年龄相关性。
</details></li>
</ul>
<hr>
<h2 id="Iterative-Sketching-for-Secure-Coded-Regression"><a href="#Iterative-Sketching-for-Secure-Coded-Regression" class="headerlink" title="Iterative Sketching for Secure Coded Regression"></a>Iterative Sketching for Secure Coded Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04185">http://arxiv.org/abs/2308.04185</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neophytos Charalambides, Hessam Mahdavifar, Mert Pilanci, Alfred O. Hero III</li>
<li>for: 提高线性回归的速度，保证安全性。</li>
<li>methods: 利用随机抽取技术，改进异步系统中的延迟性。Specifically, 应用随机正交矩阵，并对块进行抽样，同时保护信息和降低回归问题的维度。</li>
<li>results: 实现了一种分布式iterative sketching方法，用于生成 $\ell_2$-子空间嵌入。此外，还对特殊情况下的随机化哈达姆变换进行推广，并讨论如何保护数据。<details>
<summary>Abstract</summary>
In this work, we propose methods for speeding up linear regression distributively, while ensuring security. We leverage randomized sketching techniques, and improve straggler resilience in asynchronous systems. Specifically, we apply a random orthonormal matrix and then subsample \textit{blocks}, to simultaneously secure the information and reduce the dimension of the regression problem. In our setup, the transformation corresponds to an encoded encryption in an \textit{approximate gradient coding scheme}, and the subsampling corresponds to the responses of the non-straggling workers; in a centralized coded computing network. This results in a distributive \textit{iterative sketching} approach for an $\ell_2$-subspace embedding, \textit{i.e.} a new sketch is considered at each iteration. We also focus on the special case of the \textit{Subsampled Randomized Hadamard Transform}, which we generalize to block sampling; and discuss how it can be modified in order to secure the data.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们提出了加速线性回归的分布式方法，同时保证安全性。我们利用随机抽象技术，并改进异步系统中的延迟鲁棒性。具体来说，我们首先应用随机正交矩阵，然后对块进行抽样，以同时保护信息和减少回归问题的维度。在我们的设置中，这种变换对应于一种编码加密在一个 Approximate Gradient Coding Scheme 中，而抽样对应于非延迟工作者的响应；在中央coded computing网络中。这 führt zu a distributed iterative sketching approach for an $\ell_2$-subspace embedding, i.e., a new sketch is considered at each iteration.我们还专注于特殊情况下的 Subsampled Randomized Hadamard Transform，我们扩展了块抽样，并讨论了如何修改以保护数据。
</details></li>
</ul>
<hr>
<h2 id="Studying-Socially-Unacceptable-Discourse-Classification-SUD-through-different-eyes-“Are-we-on-the-same-page-”"><a href="#Studying-Socially-Unacceptable-Discourse-Classification-SUD-through-different-eyes-“Are-we-on-the-same-page-”" class="headerlink" title="Studying Socially Unacceptable Discourse Classification (SUD) through different eyes: “Are we on the same page ?”"></a>Studying Socially Unacceptable Discourse Classification (SUD) through different eyes: “Are we on the same page ?”</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04180">http://arxiv.org/abs/2308.04180</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mlinardicyu/sud_study_different_eyes">https://github.com/mlinardicyu/sud_study_different_eyes</a></li>
<li>paper_authors: Bruno Machado Carneiro, Michele Linardi, Julien Longhi</li>
<li>for: 本研究探讨了在在线文本中分类和检测社会不CCE接受的语言（SUD）。</li>
<li>methods: 我们首先构建了一个包含多种不同在线来源的手动标注文本的新集合，以便测试现有的机器学习（ML）SUD检测解决方案中的通用性。</li>
<li>results: 我们发现，不同的注解模式可能会影响SUD学习，并提出了一些开放的挑战和研究方向。  Additionally, we provide several data insights that can support domain experts in the annotation task.<details>
<summary>Abstract</summary>
We study Socially Unacceptable Discourse (SUD) characterization and detection in online text. We first build and present a novel corpus that contains a large variety of manually annotated texts from different online sources used so far in state-of-the-art Machine learning (ML) SUD detection solutions. This global context allows us to test the generalization ability of SUD classifiers that acquire knowledge around the same SUD categories, but from different contexts. From this perspective, we can analyze how (possibly) different annotation modalities influence SUD learning by discussing open challenges and open research directions. We also provide several data insights which can support domain experts in the annotation task.
</details>
<details>
<summary>摘要</summary>
我们研究社会不可接受的语言（SUD）Characterization和检测在在线文本中。我们首先建立了一个新的 corpus，包含不同在线来源的手动标注的文本，这些文本在前一个 Machine learning（ML） SUD检测解决方案中使用过。这种全球背景允许我们测试SUD分类器在不同上下文中 acquire 知识的泛化能力。从这个角度来看，我们可以分析不同标注模式对 SUD 学习的影响，并讨论开放的挑战和未来研究方向。我们还提供了一些数据视角，可以支持领域专家在标注任务中。Note: "SUD" stands for "Socially Unacceptable Discourse" in English.
</details></li>
</ul>
<hr>
<h2 id="Dual-input-neural-networks-for-positional-sound-source-localization"><a href="#Dual-input-neural-networks-for-positional-sound-source-localization" class="headerlink" title="Dual input neural networks for positional sound source localization"></a>Dual input neural networks for positional sound source localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04169">http://arxiv.org/abs/2308.04169</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/egrinstein/di_nn">https://github.com/egrinstein/di_nn</a></li>
<li>paper_authors: Eric Grinstein, Vincent W. Neo, Patrick A. Naylor</li>
<li>for: 本研究用于提高音频信号处理中Metadata的使用，以优化输出。</li>
<li>methods: 本研究提出了 dual input neural networks (DI-NNs) 作为一种简单 yet effective的方法，将多个感知器的 audio 信号和场景特性信息结合在一起，以优化音源位置估计。</li>
<li>results: 对于不同的难度和真实性水平的场景，DI-NNs 与基准方法（Least-Squares 方法和 Convolutional Recurrent Neural Network）进行比较，结果显示 DI-NNs 在实际录制数据集中表现出五倍于基准方法的Localization error 下降，并且与 CRNN 相比，有两倍的性能提升。<details>
<summary>Abstract</summary>
In many signal processing applications, metadata may be advantageously used in conjunction with a high dimensional signal to produce a desired output. In the case of classical Sound Source Localization (SSL) algorithms, information from a high dimensional, multichannel audio signals received by many distributed microphones is combined with information describing acoustic properties of the scene, such as the microphones' coordinates in space, to estimate the position of a sound source. We introduce Dual Input Neural Networks (DI-NNs) as a simple and effective way to model these two data types in a neural network. We train and evaluate our proposed DI-NN on scenarios of varying difficulty and realism and compare it against an alternative architecture, a classical Least-Squares (LS) method as well as a classical Convolutional Recurrent Neural Network (CRNN). Our results show that the DI-NN significantly outperforms the baselines, achieving a five times lower localization error than the LS method and two times lower than the CRNN in a test dataset of real recordings.
</details>
<details>
<summary>摘要</summary>
在许多信号处理应用中，元数据可以与高维信号结合使用，以生成所需的输出。在传统的Sound Source Localization（SSL）算法中，来自多个分布式麦克风的高维多通道音频信号和场景的听音性特征，如麦克风的空间坐标，被组合以估算声源的位置。我们介绍了双输入神经网络（DI-NN）作为一种简单而有效的方法，将这两种数据类型模型在神经网络中。我们在不同的难度和真实性水平上训练和评估我们的提议DI-NN，并与基准 Architecture，即经典的最小二乘法（LS）方法和经典的卷积回归神经网络（CRNN）进行比较。我们的结果表明，DI-NN在测试数据集中与LS方法和CRNN相比，显著地下降了5倍的地标错误，达到了2倍的CRNN水平。
</details></li>
</ul>
<hr>
<h2 id="Comprehensive-Assessment-of-the-Performance-of-Deep-Learning-Classifiers-Reveals-a-Surprising-Lack-of-Robustness"><a href="#Comprehensive-Assessment-of-the-Performance-of-Deep-Learning-Classifiers-Reveals-a-Surprising-Lack-of-Robustness" class="headerlink" title="Comprehensive Assessment of the Performance of Deep Learning Classifiers Reveals a Surprising Lack of Robustness"></a>Comprehensive Assessment of the Performance of Deep Learning Classifiers Reveals a Surprising Lack of Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04137">http://arxiv.org/abs/2308.04137</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael W. Spratling</li>
<li>for: The paper aims to evaluate the robustness of machine learning models, particularly deep neural networks, and to develop a more comprehensive benchmarking protocol for assessing their performance.</li>
<li>methods: The paper proposes using a wide range of different types of data to benchmark the performance of machine learning models, and using a single metric that can be applied to all such data types to produce a consistent evaluation of performance.</li>
<li>results: The paper finds that current deep neural networks are extremely vulnerable to making mistakes on certain types of data, and that they are insecure as they can easily be fooled into making the wrong decisions. The results suggest that more comprehensive testing methods are needed to develop more robust machine learning models in the future.<details>
<summary>Abstract</summary>
Reliable and robust evaluation methods are a necessary first step towards developing machine learning models that are themselves robust and reliable. Unfortunately, current evaluation protocols typically used to assess classifiers fail to comprehensively evaluate performance as they tend to rely on limited types of test data, and ignore others. For example, using the standard test data fails to evaluate the predictions made by the classifier to samples from classes it was not trained on. On the other hand, testing with data containing samples from unknown classes fails to evaluate how well the classifier can predict the labels for known classes. This article advocates bench-marking performance using a wide range of different types of data and using a single metric that can be applied to all such data types to produce a consistent evaluation of performance. Using such a benchmark it is found that current deep neural networks, including those trained with methods that are believed to produce state-of-the-art robustness, are extremely vulnerable to making mistakes on certain types of data. This means that such models will be unreliable in real-world scenarios where they may encounter data from many different domains, and that they are insecure as they can easily be fooled into making the wrong decisions. It is hoped that these results will motivate the wider adoption of more comprehensive testing methods that will, in turn, lead to the development of more robust machine learning methods in the future.   Code is available at: \url{https://codeberg.org/mwspratling/RobustnessEvaluation}
</details>
<details>
<summary>摘要</summary>
可靠且可靠的评估方法是机器学习模型的发展的必要首步。然而，当前的评估协议通常只使用有限种的测试数据来评估性能，而忽略其他类型的数据。例如，使用标准测试数据将不能评估机器学习器对未知类型的样本进行预测。相反，使用包含未知类型样本的数据进行测试将不能评估机器学习器对已知类型样本的预测。这篇文章提议使用多种不同类型的数据进行比较，并使用一个可以应用于所有数据类型的单一指标来生成一致的评估性能。使用这种标准可以发现，当前的深度神经网络，包括通过认为会生成状态的最佳Robustness训练方法，在某些类型的数据上表现出极高的敏感性。这意味着这些模型在实际世界中将是不可靠的，因为它们可能会遇到来自多个领域的数据，并且它们容易被骗到作出错误的决策。希望这些结果能够激励更广泛的测试方法的采用，以便在未来发展更加可靠的机器学习方法。Code可以在以下链接中找到：https://codeberg.org/mwspratling/RobustnessEvaluation
</details></li>
</ul>
<hr>
<h2 id="D-Score-A-Synapse-Inspired-Approach-for-Filter-Pruning"><a href="#D-Score-A-Synapse-Inspired-Approach-for-Filter-Pruning" class="headerlink" title="D-Score: A Synapse-Inspired Approach for Filter Pruning"></a>D-Score: A Synapse-Inspired Approach for Filter Pruning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04470">http://arxiv.org/abs/2308.04470</a></li>
<li>repo_url: None</li>
<li>paper_authors: Doyoung Park, Jinsoo Kim, Jina Nam, Jooyoung Chang, Sang Min Park</li>
<li>for: 本研究旨在提出一种基于神经元系统的筛选方法，用于降低卷积神经网络（CNN）中不重要的筛选器的计算量。</li>
<li>methods: 该方法基于神经科学的视角，提出了一种名为动态分数（D-Score）的筛选器分数分析方法，该方法分析了每个筛选器中独立重要的正向和负向权重，并将其分配分数。</li>
<li>results: 实验结果表明，使用该方法可以在CIFAR-10和ImageNet datasets上减少显著的计算量和参数，而无需导致准确率下降。<details>
<summary>Abstract</summary>
This paper introduces a new aspect for determining the rank of the unimportant filters for filter pruning on convolutional neural networks (CNNs). In the human synaptic system, there are two important channels known as excitatory and inhibitory neurotransmitters that transmit a signal from a neuron to a cell. Adopting the neuroscientific perspective, we propose a synapse-inspired filter pruning method, namely Dynamic Score (D-Score). D-Score analyzes the independent importance of positive and negative weights in the filters and ranks the independent importance by assigning scores. Filters having low overall scores, and thus low impact on the accuracy of neural networks are pruned. The experimental results on CIFAR-10 and ImageNet datasets demonstrate the effectiveness of our proposed method by reducing notable amounts of FLOPs and Params without significant Acc. Drop.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="OmniDataComposer-A-Unified-Data-Structure-for-Multimodal-Data-Fusion-and-Infinite-Data-Generation"><a href="#OmniDataComposer-A-Unified-Data-Structure-for-Multimodal-Data-Fusion-and-Infinite-Data-Generation" class="headerlink" title="OmniDataComposer: A Unified Data Structure for Multimodal Data Fusion and Infinite Data Generation"></a>OmniDataComposer: A Unified Data Structure for Multimodal Data Fusion and Infinite Data Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04126">http://arxiv.org/abs/2308.04126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongyang Yu, Shihao Wang, Yuan Fang, Wangpeng An</li>
<li>for: 这篇论文旨在提出一种新的多模态数据融合和无限数据生成方法，以协调和简化不同数据模式之间的交互。</li>
<li>methods: 该方法使用了多种先进算法，包括视频&#x2F;图像描述EXTRACTION、稠密描述EXTRACTION、自动语音识别（ASR）、光学字符识别（OCR）、Recognize Anything Model（RAM）和对象跟踪。</li>
<li>results: 该方法可以识别6400种类型的对象，显著扩大了视觉信息的谱度。它将多种模式融合起来，促进不同模式之间的互助和cross-模式数据校正。最终输出将每个视频输入转化为详细的时间序列文档，使得视频更容易被大语言模型处理。<details>
<summary>Abstract</summary>
This paper presents OmniDataComposer, an innovative approach for multimodal data fusion and unlimited data generation with an intent to refine and uncomplicate interplay among diverse data modalities. Coming to the core breakthrough, it introduces a cohesive data structure proficient in processing and merging multimodal data inputs, which include video, audio, and text.   Our crafted algorithm leverages advancements across multiple operations such as video/image caption extraction, dense caption extraction, Automatic Speech Recognition (ASR), Optical Character Recognition (OCR), Recognize Anything Model(RAM), and object tracking. OmniDataComposer is capable of identifying over 6400 categories of objects, substantially broadening the spectrum of visual information. It amalgamates these diverse modalities, promoting reciprocal enhancement among modalities and facilitating cross-modal data correction. \textbf{The final output metamorphoses each video input into an elaborate sequential document}, virtually transmuting videos into thorough narratives, making them easier to be processed by large language models.   Future prospects include optimizing datasets for each modality to encourage unlimited data generation. This robust base will offer priceless insights to models like ChatGPT, enabling them to create higher quality datasets for video captioning and easing question-answering tasks based on video content. OmniDataComposer inaugurates a new stage in multimodal learning, imparting enormous potential for augmenting AI's understanding and generation of complex, real-world data.
</details>
<details>
<summary>摘要</summary>
The final output transforms each video input into an elaborate sequential document, virtually transmuting videos into thorough narratives that can be easily processed by large language models. Future prospects include optimizing datasets for each modality to encourage unlimited data generation, which will offer valuable insights to models like ChatGPT and ease question-answering tasks based on video content.OmniDataComposer opens a new stage in multimodal learning, possessing enormous potential for augmenting AI's understanding and generation of complex, real-world data. With the ability to identify over 6400 categories of objects, the algorithm significantly broadens the spectrum of visual information. By merging diverse modalities and facilitating cross-modal data correction, OmniDataComposer promotes reciprocal enhancement among modalities, leading to more accurate and comprehensive data analysis.
</details></li>
</ul>
<hr>
<h2 id="Constructing-Custom-Thermodynamics-Using-Deep-Learning"><a href="#Constructing-Custom-Thermodynamics-Using-Deep-Learning" class="headerlink" title="Constructing Custom Thermodynamics Using Deep Learning"></a>Constructing Custom Thermodynamics Using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04119">http://arxiv.org/abs/2308.04119</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoli Chen, Beatrice W. Soh, Zi-En Ooi, Eleonore Vissol-Gaudin, Haijun Yu, Kostya S. Novoselov, Kedar Hippalgaonkar, Qianxiao Li</li>
<li>for: 这项研究旨在开发一种基于总体热动力学原理的自动化科学发现平台，用于研究复杂动态系统，以提高科学家对这些系统的理解和预测能力。</li>
<li>methods: 该平台基于一种总体热动力学原理来学习微观轨迹的 макро观动力学描述，并同时构建了减少的热动力学坐标系和解释动力学。</li>
<li>results: 通过对长聚合物链的延伸行为进行理论和实验研究，该方法学习了三个可解释的热动力学坐标，并构建了聚合物延伸的动力学景观，包括稳定和过渡态的标识以及延伸速率的控制。此外，该方法还应用于了不同领域的另一个问题——构建空间传染疫苗的macroscopic动力学，展示了该方法的广泛科学和技术应用前景。<details>
<summary>Abstract</summary>
One of the most exciting applications of AI is automated scientific discovery based on previously amassed data, coupled with restrictions provided by the known physical principles, including symmetries and conservation laws. Such automated hypothesis creation and verification can assist scientists in studying complex phenomena, where traditional physical intuition may fail. Of particular importance are complex dynamic systems where their time evolution is strongly influenced by varying external parameters. In this paper we develop a platform based on a generalised Onsager principle to learn macroscopic dynamical descriptions of arbitrary stochastic dissipative systems directly from observations of their microscopic trajectories. We focus on systems whose complexity and sheer sizes render complete microscopic description impractical, and constructing theoretical macroscopic models requires extensive domain knowledge or trial-and-error. Our machine learning approach addresses this by simultaneously constructing reduced thermodynamic coordinates and interpreting the dynamics on these coordinates. We demonstrate our method by studying theoretically and validating experimentally, the stretching of long polymer chains in an externally applied field. Specifically, we learn three interpretable thermodynamic coordinates and build a dynamical landscape of polymer stretching, including (1) the identification of stable and transition states and (2) the control of the stretching rate. We further demonstrate the universality of our approach by applying it to an unrelated problem in a different domain: constructing macroscopic dynamics for spatial epidemics, showing that our method addresses wide scientific and technological applications.
</details>
<details>
<summary>摘要</summary>
In this paper, we develop a platform based on a generalised Onsager principle to learn macroscopic dynamical descriptions of arbitrary stochastic dissipative systems directly from observations of their microscopic trajectories. We focus on systems whose complexity and sheer sizes make complete microscopic description impractical, and constructing theoretical macroscopic models requires extensive domain knowledge or trial-and-error. Our machine learning approach addresses this by simultaneously constructing reduced thermodynamic coordinates and interpreting the dynamics on these coordinates.We demonstrate our method by studying theoretically and validating experimentally the stretching of long polymer chains in an externally applied field. Specifically, we learn three interpretable thermodynamic coordinates and build a dynamical landscape of polymer stretching, including the identification of stable and transition states and the control of the stretching rate. We further demonstrate the universality of our approach by applying it to an unrelated problem in a different domain: constructing macroscopic dynamics for spatial epidemics, showing that our method addresses wide scientific and technological applications.
</details></li>
</ul>
<hr>
<h2 id="PTransIPs-Identification-of-phosphorylation-sites-based-on-protein-pretrained-language-model-and-Transformer"><a href="#PTransIPs-Identification-of-phosphorylation-sites-based-on-protein-pretrained-language-model-and-Transformer" class="headerlink" title="PTransIPs: Identification of phosphorylation sites based on protein pretrained language model and Transformer"></a>PTransIPs: Identification of phosphorylation sites based on protein pretrained language model and Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05115">http://arxiv.org/abs/2308.05115</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/statxzy7/ptransips">https://github.com/statxzy7/ptransips</a></li>
<li>paper_authors: Ziyang Xu, Haitian Zhong<br>for:这个研究旨在开发一个新的深度学习模型，用于识别蛋白质中的磷酸化位点。methods:这个模型使用了深度学习的卷积神经网络和传统的Transformer模型，并且使用了大量预训练的蛋白质模型嵌入。results:实验结果显示，这个模型比现有的State-of-the-art方法高效，具有AUROC值为0.9232和0.9660 для识别磷酸化S&#x2F;T和Y位点。<details>
<summary>Abstract</summary>
Phosphorylation is central to numerous fundamental cellular processes, influencing the onset and progression of a variety of diseases. The correct identification of these phosphorylation sites is of great importance to unravel the intricate molecular mechanisms within cells and during viral infections, potentially leading to the discovery of new therapeutic targets. In this study, we introduce PTransIPs, a novel deep learning model for the identification of phosphorylation sites. PTransIPs treat amino acids within protein sequences as words, extracting unique encodings based on their type and sequential position. The model also incorporates embeddings from large pretrained protein models as additional data inputs. PTransIPS is further trained on a combination model of convolutional neural network with residual connections and Transformer model equipped with multi-head attention mechanisms. At last, the model outputs classification results through a fully connected layer. The results of independent testing reveal that PTransIPs outperforms existing state-of-the-art(SOTA) methods, achieving AUROCs of 0.9232 and 0.9660 for identifying phosphorylated S/T and Y sites respectively. In addition, ablation studies prove that pretrained model embeddings contribute to the performance of PTransIPs. Furthermore, PTransIPs has interpretable amino acid preference, visible training process and shows generalizability on other bioactivity classification tasks. To facilitate usage, our code and data are publicly accessible at \url{https://github.com/StatXzy7/PTransIPs}.
</details>
<details>
<summary>摘要</summary>
“磷酸化是细胞内多种基本生物过程的中心，影响疾病的发生和进程。正确识别这些磷酸化位点非常重要，以解释细胞内分子机制，并在病毒感染时发挥作用，可能导致新的药 targets 的发现。在这项研究中，我们介绍 PTransIPs，一种新的深度学习模型，用于磷酸化位点的识别。 PTransIPs 将蛋白质序列中的氨基酸看作为单词，提取唯一的编码，基于其类型和顺序位置。模型还使用大型预训练蛋白质模型的嵌入。 PTransIPS 通过将 convolutional neural network 与 residual connections 和 transformer 模型组合，并在其中添加多头注意机制。最后，模型输出分类结果通过完全连接层。独立测试结果显示，PTransIPs 在识别磷酸化 S/T 和 Y 位点方面的 AUROC 分别达到 0.9232 和 0.9660。此外，归因研究表明，预训练模型嵌入对 PTransIPs 的表现有助。此外，PTransIPs 具有可读性的氨基酸偏好、可见的训练过程和普适性。为便于使用，我们在 GitHub 上公开了代码和数据，请参考 \url{https://github.com/StatXzy7/PTransIPs}.”
</details></li>
</ul>
<hr>
<h2 id="Correlating-Medi-Claim-Service-by-Deep-Learning-Neural-Networks"><a href="#Correlating-Medi-Claim-Service-by-Deep-Learning-Neural-Networks" class="headerlink" title="Correlating Medi-Claim Service by Deep Learning Neural Networks"></a>Correlating Medi-Claim Service by Deep Learning Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04469">http://arxiv.org/abs/2308.04469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jayanthi Vajiram, Negha Senthil, Nean Adhith. P</li>
<li>for: 医疗保险诈骗案件与患者、医生、诊断中心和保险公司之间的关系，需要不断监测。这种诈骗活动会对保险人和医疗保险公司的财务发展产生影响。</li>
<li>methods: 这篇论文使用卷积神经网络架构来检测诈骗申请，通过对不同提供者的申请进行相关性研究，帮助检测洗钱活动。用于检测诈骗和非诈骗申请的有supervised和Unsupervised分类器。</li>
<li>results: 该论文的结果表明，使用卷积神经网络架构和相关性研究可以帮助检测医疗保险诈骗案件，并且可以提高检测精度。<details>
<summary>Abstract</summary>
Medical insurance claims are of organized crimes related to patients, physicians, diagnostic centers, and insurance providers, forming a chain reaction that must be monitored constantly. These kinds of frauds affect the financial growth of both insured people and health insurance companies. The Convolution Neural Network architecture is used to detect fraudulent claims through a correlation study of regression models, which helps to detect money laundering on different claims given by different providers. Supervised and unsupervised classifiers are used to detect fraud and non-fraud claims.
</details>
<details>
<summary>摘要</summary>
医疗保险laims有组织犯罪关系到patients、医生、诊断中心和保险公司，形成一个排序链 reaction，需要不断监测。这种骗局会影响保险人和健康保险公司的财务增长。通过对不同提供者的laims进行相关性研究，涂抹神经网络架构可以检测针对不同提供者的骗局。使用supervised和Unsupervised分类器可以检测骗局和非骗局laims。
</details></li>
</ul>
<hr>
<h2 id="Explainable-machine-learning-to-enable-high-throughput-electrical-conductivity-optimization-of-doped-conjugated-polymers"><a href="#Explainable-machine-learning-to-enable-high-throughput-electrical-conductivity-optimization-of-doped-conjugated-polymers" class="headerlink" title="Explainable machine learning to enable high-throughput electrical conductivity optimization of doped conjugated polymers"></a>Explainable machine learning to enable high-throughput electrical conductivity optimization of doped conjugated polymers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04103">http://arxiv.org/abs/2308.04103</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ji Wei Yoon, Adithya Kumar, Pawan Kumar, Kedar Hippalgaonkar, J Senthilnath, Vijila Chellappan</li>
<li>for: 加速材料发现和优化</li>
<li>methods: 使用机器学习（ML）技术，利用易于测量的吸收спектроскопия数据，加速测量电性征的工作流程</li>
<li>results: 提高测量电性征的效率89%，并且可以解释ML模型的具体影响因素，从而获得了有用的科学见解。<details>
<summary>Abstract</summary>
The combination of high-throughput experimentation techniques and machine learning (ML) has recently ushered in a new era of accelerated material discovery, enabling the identification of materials with cutting-edge properties. However, the measurement of certain physical quantities remains challenging to automate. Specifically, meticulous process control, experimentation and laborious measurements are required to achieve optimal electrical conductivity in doped polymer materials. We propose a ML approach, which relies on readily measured absorbance spectra, to accelerate the workflow associated with measuring electrical conductivity. The first ML model (classification model), accurately classifies samples with a conductivity >~25 to 100 S/cm, achieving a maximum of 100% accuracy rate. For the subset of highly conductive samples, we employed a second ML model (regression model), to predict their conductivities, yielding an impressive test R2 value of 0.984. To validate the approach, we showed that the models, neither trained on the samples with the two highest conductivities of 498 and 506 S/cm, were able to, in an extrapolative manner, correctly classify and predict them at satisfactory levels of errors. The proposed ML workflow results in an improvement in the efficiency of the conductivity measurements by 89% of the maximum achievable using our experimental techniques. Furthermore, our approach addressed the common challenge of the lack of explainability in ML models by exploiting bespoke mathematical properties of the descriptors and ML model, allowing us to gain corroborated insights into the spectral influences on conductivity. Through this study, we offer an accelerated pathway for optimizing the properties of doped polymer materials while showcasing the valuable insights that can be derived from purposeful utilization of ML in experimental science.
</details>
<details>
<summary>摘要</summary>
高通过率实验技术和机器学习（ML）的结合，已经 ushered in一个新的时代，用于加速材料发现，并使得可以通过高级特性来标识材料。然而，一些物理量的测量仍然具有挑战。Specifically, 需要精心控制过程，实验和劳动ioso measurements 以实现高电导率填充 polymer 材料中的优化。我们提议一种 ML 方法，基于Ready-to-measure absorbance spectrum，以加速测量电导率的工作流程。首先，我们使用了一种分类模型（第一种 ML 模型），准确地将样本分类为电导率 > ~25 to 100 S/cm，实现最高的准确率（100%）。对于高电导率样本 subset，我们使用了一种 regression 模型（第二种 ML 模型），来预测他们的电导率，得到了惊人的 test R2 值为 0.984。为了验证方法，我们表明，无论在填充 polymer 材料中的两个最高电导率样本（498和506 S/cm）的情况下，模型都能够，在扩展性的方式下，正确地分类和预测它们，并且得到了满意的误差水平。我们的 ML 工作流程对于测量电导率的效率提高了89%的最大可达水平。此外，我们的方法解决了通用机器学习模型的普遍问题，即无法解释性。我们通过抽象数学属性和 ML 模型的特性，获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠
</details></li>
</ul>
<hr>
<h2 id="Asynchronous-Evolution-of-Deep-Neural-Network-Architectures"><a href="#Asynchronous-Evolution-of-Deep-Neural-Network-Architectures" class="headerlink" title="Asynchronous Evolution of Deep Neural Network Architectures"></a>Asynchronous Evolution of Deep Neural Network Architectures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04102">http://arxiv.org/abs/2308.04102</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jason Liang, Hormoz Shahrzad, Risto Miikkulainen</li>
<li>for: 提高 ENAS 的并发评估速度</li>
<li>methods: 使用异步评估策略 (AES)，维护最多 $K$ 个个体Ready to be sent to the workers for evaluation，并在 $M&lt;&lt;K$ 个个体已经被评估的情况下提交下一代</li>
<li>results: 在 11-bit 多路分配和图像描述任务中观察到多倍性和开放式优化任务中的性能提高，表明 AES 是一种有效的并发评估策略，适用于复杂系统的进化中长度和变化很大的评估时间。<details>
<summary>Abstract</summary>
Many evolutionary algorithms (EAs) take advantage of parallel evaluation of candidates. However, if evaluation times vary significantly, many worker nodes (i.e.,\ compute clients) are idle much of the time, waiting for the next generation to be created. Evolutionary neural architecture search (ENAS), a class of EAs that optimizes the architecture and hyperparameters of deep neural networks, is particularly vulnerable to this issue. This paper proposes a generic asynchronous evaluation strategy (AES) that is then adapted to work with ENAS. AES increases throughput by maintaining a queue of upto $K$ individuals ready to be sent to the workers for evaluation and proceeding to the next generation as soon as $M<<K$ individuals have been evaluated by the workers. A suitable value for $M$ is determined experimentally, balancing diversity and efficiency. To showcase the generality and power of AES, it was first evaluated in 11-bit multiplexer design (a single-population verifiable discovery task) and then scaled up to ENAS for image captioning (a multi-population open-ended-optimization task). In both problems, a multifold performance improvement was observed, suggesting that AES is a promising method for parallelizing the evolution of complex systems with long and variable evaluation times, such as those in ENAS.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)许多进化算法（EA）利用并发评估候选者。然而，如果评估时间异常长， THEN 多个工作节点（即计算客户端）会大部分时间浪费于等待下一代被创建。进化神经网络架构搜索（ENAS），一类进化算法，特别容易受到这种问题的影响。这篇论文提出了一种通用异步评估策略（AES），然后将其应用于 ENAS。AES 通过维护一个最多 $K$ 个个体ready to be sent to the workers for evaluation，并在工作者评估 $M\ll K$ 个个体后进行下一代的创建，提高了 durchput。一个合适的 $M$ 值由实验确定，平衡多样性和效率。为了证明 AES 的普适性和能力，它首先在 11-bit 多路复用器设计（一个单种人口可验证发现任务）中进行了测试，然后扩展到 ENAS  для图像描述（一个多种人口开放结束优化任务）。在这两个问题中，都观察到了多倍性的性能改进，表明 AES 是一种有前途的方法 для并发进化复杂系统的评估，如 ENAS 中的长变化评估时间。
</details></li>
</ul>
<hr>
<h2 id="Why-Data-Science-Projects-Fail"><a href="#Why-Data-Science-Projects-Fail" class="headerlink" title="Why Data Science Projects Fail"></a>Why Data Science Projects Fail</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04896">http://arxiv.org/abs/2308.04896</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xLaszlo/datascience-fails">https://github.com/xLaszlo/datascience-fails</a></li>
<li>paper_authors: Balaram Panda</li>
</ul>
<details>
<summary>Abstract</summary>
Data Science is a modern Data Intelligence practice, which is the core of many businesses and helps businesses build smart strategies around to deal with businesses challenges more efficiently. Data Science practice also helps in automating business processes using the algorithm, and it has several other benefits, which also deliver in a non-profitable framework. In regards to data science, three key components primarily influence the effective outcome of a data science project. Those are 1.Availability of Data 2.Algorithm 3.Processing power or infrastructure
</details>
<details>
<summary>摘要</summary>
现代数据智能实践---数据科学，是许多企业的核心，帮助企业建立更智能的策略，更有效地面对企业挑战。数据科学实践还可以自动化商业过程，并具有许多其他利点，如非营利框架。在关于数据科学的问题上，三个关键组件主要影响项目的效果。那是1.数据可用性2.算法3.处理能力或基础设施。
</details>


<hr>
<h2 id="Application-Oriented-Benchmarking-of-Quantum-Generative-Learning-Using-QUARK"><a href="#Application-Oriented-Benchmarking-of-Quantum-Generative-Learning-Using-QUARK" class="headerlink" title="Application-Oriented Benchmarking of Quantum Generative Learning Using QUARK"></a>Application-Oriented Benchmarking of Quantum Generative Learning Using QUARK</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04082">http://arxiv.org/abs/2308.04082</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florian J. Kiwit, Marwa Marso, Philipp Ross, Carlos A. Riofrío, Johannes Klepsch, Andre Luckow</li>
<li>for: 本研究旨在提供一个标准化和简化量子机器学习（QML）算法评估的框架，以便更好地评估量子 Computing 应用程序的性能。</li>
<li>methods: 本研究使用了更新后的 QUantum computing Application benchmaRK（QUARK）框架，以便评估量子生成模型的训练和部署。</li>
<li>results: 本研究通过训练不同的量子生成模型，使用不同的图体设计、数据集和转换，并评估模型的普遍性和可靠性。<details>
<summary>Abstract</summary>
Benchmarking of quantum machine learning (QML) algorithms is challenging due to the complexity and variability of QML systems, e.g., regarding model ansatzes, data sets, training techniques, and hyper-parameters selection. The QUantum computing Application benchmaRK (QUARK) framework simplifies and standardizes benchmarking studies for quantum computing applications. Here, we propose several extensions of QUARK to include the ability to evaluate the training and deployment of quantum generative models. We describe the updated software architecture and illustrate its flexibility through several example applications: (1) We trained different quantum generative models using several circuit ansatzes, data sets, and data transformations. (2) We evaluated our models on GPU and real quantum hardware. (3) We assessed the generalization capabilities of our generative models using a broad set of metrics that capture, e.g., the novelty and validity of the generated data.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将量子机器学习（QML）算法进行测试和评估是具有复杂性和变化性的，例如模型 ansatzes、数据集、训练技术和超参数选择等。QUantum computing Application benchmaRK（QUARK）框架可以简化和标准化量子计算应用程序的测试和评估研究。我们提议在QUARK框架中添加了评估量子生成模型的训练和部署功能。我们描述了更新后的软件架构，并通过多个示例应用 illustrate its flexibility：(1) 我们使用不同的量子生成模型circuit ansatzes、数据集和数据变换来训练不同的量子生成模型。(2) 我们在GPU和真正量子硬件上评估了我们的模型。(3) 我们使用了一系列的metric来评估我们的生成模型的通用性，例如生成数据的新鲜性和有效性。Note: "quantum generative models" in the text refers to quantum neural networks or quantum machine learning models that can generate new data samples.
</details></li>
</ul>
<hr>
<h2 id="Federated-Zeroth-Order-Optimization-using-Trajectory-Informed-Surrogate-Gradients"><a href="#Federated-Zeroth-Order-Optimization-using-Trajectory-Informed-Surrogate-Gradients" class="headerlink" title="Federated Zeroth-Order Optimization using Trajectory-Informed Surrogate Gradients"></a>Federated Zeroth-Order Optimization using Trajectory-Informed Surrogate Gradients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04077">http://arxiv.org/abs/2308.04077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Shu, Xiaoqiang Lin, Zhongxiang Dai, Bryan Kian Hsiang Low</li>
<li>for: 这个研究是为了提出一种能够在无法读取本地数据的情况下进行联合优化的方法，并且可以在各种实际应用中实现更好的效率和积极性。</li>
<li>methods: 这个研究使用了联合优化的背景下的零次项目优化（ZOO）方法，并且提出了一种基于轨迹信息的渐进幂函数估计（Trajectory-Informed Gradient Surrogates，TIGS）以及一种适应性的渐进幂函数调整（Adaptive Gradient Correction，AGC）技术，以提高查询和通信效率。</li>
<li>results: 这个研究透过实际实验（包括联合黑洞攻击和联合非对称度量优化），证明了FZooS方法的理论上的改进，并且在实际应用中实现了更好的效率和积极性。<details>
<summary>Abstract</summary>
Federated optimization, an emerging paradigm which finds wide real-world applications such as federated learning, enables multiple clients (e.g., edge devices) to collaboratively optimize a global function. The clients do not share their local datasets and typically only share their local gradients. However, the gradient information is not available in many applications of federated optimization, which hence gives rise to the paradigm of federated zeroth-order optimization (ZOO). Existing federated ZOO algorithms suffer from the limitations of query and communication inefficiency, which can be attributed to (a) their reliance on a substantial number of function queries for gradient estimation and (b) the significant disparity between their realized local updates and the intended global updates. To this end, we (a) introduce trajectory-informed gradient surrogates which is able to use the history of function queries during optimization for accurate and query-efficient gradient estimation, and (b) develop the technique of adaptive gradient correction using these gradient surrogates to mitigate the aforementioned disparity. Based on these, we propose the federated zeroth-order optimization using trajectory-informed surrogate gradients (FZooS) algorithm for query- and communication-efficient federated ZOO. Our FZooS achieves theoretical improvements over the existing approaches, which is supported by our real-world experiments such as federated black-box adversarial attack and federated non-differentiable metric optimization.
</details>
<details>
<summary>摘要</summary>
federated optimization, an emerging paradigm which finds wide real-world applications such as federated learning, enables multiple clients (e.g., edge devices) to collaboratively optimize a global function. The clients do not share their local datasets and typically only share their local gradients. However, the gradient information is not available in many applications of federated optimization, which hence gives rise to the paradigm of federated zeroth-order optimization (ZOO). Existing federated ZOO algorithms suffer from the limitations of query and communication inefficiency, which can be attributed to (a) their reliance on a substantial number of function queries for gradient estimation and (b) the significant disparity between their realized local updates and the intended global updates. To this end, we (a) introduce trajectory-informed gradient surrogates which is able to use the history of function queries during optimization for accurate and query-efficient gradient estimation, and (b) develop the technique of adaptive gradient correction using these gradient surrogates to mitigate the aforementioned disparity. Based on these, we propose the federated zeroth-order optimization using trajectory-informed surrogate gradients (FZooS) algorithm for query- and communication-efficient federated ZOO. Our FZooS achieves theoretical improvements over the existing approaches, which is supported by our real-world experiments such as federated black-box adversarial attack and federated non-differentiable metric optimization.Note that Simplified Chinese is a romanization of Chinese, and the actual Chinese characters may be different.
</details></li>
</ul>
<hr>
<h2 id="Learning-Specialized-Activation-Functions-for-Physics-informed-Neural-Networks"><a href="#Learning-Specialized-Activation-Functions-for-Physics-informed-Neural-Networks" class="headerlink" title="Learning Specialized Activation Functions for Physics-informed Neural Networks"></a>Learning Specialized Activation Functions for Physics-informed Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04073">http://arxiv.org/abs/2308.04073</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leaplabthu/adaafforpinns">https://github.com/leaplabthu/adaafforpinns</a></li>
<li>paper_authors: Honghui Wang, Lu Lu, Shiji Song, Gao Huang</li>
<li>for: 解决Physics-informed neural networks (PINNs)中的优化困难问题。</li>
<li>methods: 提出了一种基于活动函数的自适应搜索方法，并对不同的问题进行比较和讨论。</li>
<li>results: 通过对多个 benchmark 进行测试，证明了该方法的效果和可INTERPRETABLE性。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Physics-informed neural networks (PINNs) are known to suffer from optimization difficulty. In this work, we reveal the connection between the optimization difficulty of PINNs and activation functions. Specifically, we show that PINNs exhibit high sensitivity to activation functions when solving PDEs with distinct properties. Existing works usually choose activation functions by inefficient trial-and-error. To avoid the inefficient manual selection and to alleviate the optimization difficulty of PINNs, we introduce adaptive activation functions to search for the optimal function when solving different problems. We compare different adaptive activation functions and discuss their limitations in the context of PINNs. Furthermore, we propose to tailor the idea of learning combinations of candidate activation functions to the PINNs optimization, which has a higher requirement for the smoothness and diversity on learned functions. This is achieved by removing activation functions which cannot provide higher-order derivatives from the candidate set and incorporating elementary functions with different properties according to our prior knowledge about the PDE at hand. We further enhance the search space with adaptive slopes. The proposed adaptive activation function can be used to solve different PDE systems in an interpretable way. Its effectiveness is demonstrated on a series of benchmarks. Code is available at https://github.com/LeapLabTHU/AdaAFforPINNs.
</details>
<details>
<summary>摘要</summary>
物理学信息感知神经网络（PINNs）经常遇到优化困难。在这项工作中，我们揭示了PINNs在解决不同类型的偏微分方程（PDEs）时的优化困难和活动函数之间的关系。特别是，我们发现PINNs在解决不同类型的PDEs时会具有高敏感性，而且现有的活动函数通常是通过不必要的试错来选择的。为了避免不必要的手动选择和提高PINNs的优化效果，我们引入了适应性的活动函数来搜索解决不同问题的最佳函数。我们比较了不同的适应性活动函数，并讨论了它们在PINNs中的限制。此外，我们提议在PINNs优化中应用学习组合候选活动函数的思想，以提高学习的稳定性和多样性。我们通过从候选函数中移除无法提供高阶导数的活动函数，并根据我们对PDE的先验知识添加不同性质的元素函数来实现这一点。最终，我们还使用自适应坡度来增强搜索空间。我们提出的适应性活动函数可以在可解释的方式下解决不同PDE系统。我们在一系列的benchmark中证明了它的效果。代码可以在https://github.com/LeapLabTHU/AdaAFforPINNs上获取。
</details></li>
</ul>
<hr>
<h2 id="Path-Signatures-for-Diversity-in-Probabilistic-Trajectory-Optimisation"><a href="#Path-Signatures-for-Diversity-in-Probabilistic-Trajectory-Optimisation" class="headerlink" title="Path Signatures for Diversity in Probabilistic Trajectory Optimisation"></a>Path Signatures for Diversity in Probabilistic Trajectory Optimisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04071">http://arxiv.org/abs/2308.04071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas Barcelos, Tin Lai, Rafael Oliveira, Paulo Borges, Fabio Ramos</li>
<li>for: 这篇论文主要应用于精确的机器人动作规划，尤其是在具有复杂障碍物和复杂 geometrical 的环境下。</li>
<li>methods: 这篇论文使用了当代 computation 硬件的平行优化方法，通过将多个解 initialize 自不同的起始点，同时解决问题。然而，如果没有一个策略来避免解彼此灰度，则可能会出现模式崩溃，从而降低了方法的效率和寻找全局解的可能性。这篇论文利用了最近的条件paths 理论，设计了一个避免模式崩溃的算法，并且使用了path signatures 和 Hilbert space representation of trajectories。</li>
<li>results: 这篇论文的实验结果显示，该算法可以在许多问题上取得更低的平均成本，比如2D navigation 和 robotic manipulators 在填充环境下的运作。<details>
<summary>Abstract</summary>
Motion planning can be cast as a trajectory optimisation problem where a cost is minimised as a function of the trajectory being generated. In complex environments with several obstacles and complicated geometry, this optimisation problem is usually difficult to solve and prone to local minima. However, recent advancements in computing hardware allow for parallel trajectory optimisation where multiple solutions are obtained simultaneously, each initialised from a different starting point. Unfortunately, without a strategy preventing two solutions to collapse on each other, naive parallel optimisation can suffer from mode collapse diminishing the efficiency of the approach and the likelihood of finding a global solution. In this paper we leverage on recent advances in the theory of rough paths to devise an algorithm for parallel trajectory optimisation that promotes diversity over the range of solutions, therefore avoiding mode collapses and achieving better global properties. Our approach builds on path signatures and Hilbert space representations of trajectories, and connects parallel variational inference for trajectory estimation with diversity promoting kernels. We empirically demonstrate that this strategy achieves lower average costs than competing alternatives on a range of problems, from 2D navigation to robotic manipulators operating in cluttered environments.
</details>
<details>
<summary>摘要</summary>
moviment planning 可以被看作为一个轨迹优化问题，其中要尽可能地降低轨迹的成本。在复杂的环境中，充满障碍物和复杂的几何结构时，这个优化问题通常很难解，容易陷入地方最优点。然而，当前的计算硬件技术允许同时进行多个解的优化，每个解从不同的初始点开始。然而，如果没有避免两个解归并在一起的策略，纯粹的平行优化可能会受到模式归并的影响，从而降低方法的效率和找到全局解的可能性。在这篇论文中，我们利用了最近的粗路理论进行平行轨迹优化算法，以便在轨迹优化过程中促进多元性，因此避免模式归并，并实现更好的全局性质。我们的方法基于路径签名和希尔伯特空间表示轨迹，并将平行变分推理与多样性激活函数连接起来。我们在几个问题上进行了实验，证明了这种策略在相对轨迹成本方面具有更好的性能。
</details></li>
</ul>
<hr>
<h2 id="ConDistFL-Conditional-Distillation-for-Federated-Learning-from-Partially-Annotated-Data"><a href="#ConDistFL-Conditional-Distillation-for-Federated-Learning-from-Partially-Annotated-Data" class="headerlink" title="ConDistFL: Conditional Distillation for Federated Learning from Partially Annotated Data"></a>ConDistFL: Conditional Distillation for Federated Learning from Partially Annotated Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04070">http://arxiv.org/abs/2308.04070</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nvidia/nvflare">https://github.com/nvidia/nvflare</a></li>
<li>paper_authors: Pochuan Wang, Chen Shen, Weichung Wang, Masahiro Oda, Chiou-Shann Fuh, Kensaku Mori, Holger R. Roth</li>
<li>for: 提出了一种总结多器官多疾病的整合分割模型，使用联邦学习（FL）技术实现共同开发模型而无需交换训练数据。</li>
<li>methods: 提出了一种名为ConDistFL的框架，将FL与知识塑造相结合，以便从部分标注数据中提取本地模型中的器官和肿瘤知识。</li>
<li>results: 在四个不同的部分标注腹部CT数据集上进行验证，结果表明提出的框架在FedAvg和FedOpt基elines之上显著提高了性能，并且在外部测试数据集上表现出了更高的普适性。<details>
<summary>Abstract</summary>
Developing a generalized segmentation model capable of simultaneously delineating multiple organs and diseases is highly desirable. Federated learning (FL) is a key technology enabling the collaborative development of a model without exchanging training data. However, the limited access to fully annotated training data poses a major challenge to training generalizable models. We propose "ConDistFL", a framework to solve this problem by combining FL with knowledge distillation. Local models can extract the knowledge of unlabeled organs and tumors from partially annotated data from the global model with an adequately designed conditional probability representation. We validate our framework on four distinct partially annotated abdominal CT datasets from the MSD and KiTS19 challenges. The experimental results show that the proposed framework significantly outperforms FedAvg and FedOpt baselines. Moreover, the performance on an external test dataset demonstrates superior generalizability compared to models trained on each dataset separately. Our ablation study suggests that ConDistFL can perform well without frequent aggregation, reducing the communication cost of FL. Our implementation will be available at https://github.com/NVIDIA/NVFlare/tree/dev/research/condist-fl.
</details>
<details>
<summary>摘要</summary>
开发一个通用分割模型，可同时分割多个器官和疾病，是非常感兴趣的。联邦学习（FL）是一种关键技术，它允许参与者共同开发模型，无需交换训练数据。然而，受到完全注释训练数据的限制，很难训练通用的模型。我们提议一种名为“ConDistFL”的框架，通过结合FL和知识储存来解决这个问题。本地模型可以从全球模型中提取不完全注释的器官和肿瘤的知识，并使用适当的条件概率表示。我们在四个不同的部分注释的腹部CT数据集上验证了我们的框架。实验结果表明，我们的框架在FedAvg和FedOpt基elines之上显著提高了性能。此外，对于外部测试数据集的性能表明，我们的模型具有更高的普适性比于每个数据集 separately 训练的模型。我们的剖析研究表明，ConDistFL可以在不经常的聚合下perform well， thereby reducing the communication cost of FL。我们的实现将在https://github.com/NVIDIA/NVFlare/tree/dev/research/condist-fl中提供。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Adversarial-Robustness-in-Low-Label-Regime-via-Adaptively-Weighted-Regularization-and-Knowledge-Distillation"><a href="#Enhancing-Adversarial-Robustness-in-Low-Label-Regime-via-Adaptively-Weighted-Regularization-and-Knowledge-Distillation" class="headerlink" title="Enhancing Adversarial Robustness in Low-Label Regime via Adaptively Weighted Regularization and Knowledge Distillation"></a>Enhancing Adversarial Robustness in Low-Label Regime via Adaptively Weighted Regularization and Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04061">http://arxiv.org/abs/2308.04061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongyoon Yang, Insung Kong, Yongdai Kim</li>
<li>for: 本研究强调 semi-supervised adversarial training，即在标注数据稀缺的情况下进行鲁棒化训练。</li>
<li>methods: 本文提出了两个Upper bound的robust risk，并提出了一个基于这两个Upper bound的Regularization term。此外，本文还提出了一种基于知识传播的semi-supervised teacher的方法。</li>
<li>results: 实验表明，我们的提议的算法可以 дости得 estado-of-the-art的性能，与现有算法相比，它在标注数据稀缺的情况下表现更优。例如，我们的算法只使用8%的标注数据时，与全量标注数据使用的超参数比较，其性能与标准准确率和鲁棒准确率在CIFAR-10上几乎相同。<details>
<summary>Abstract</summary>
Adversarial robustness is a research area that has recently received a lot of attention in the quest for trustworthy artificial intelligence. However, recent works on adversarial robustness have focused on supervised learning where it is assumed that labeled data is plentiful. In this paper, we investigate semi-supervised adversarial training where labeled data is scarce. We derive two upper bounds for the robust risk and propose a regularization term for unlabeled data motivated by these two upper bounds. Then, we develop a semi-supervised adversarial training algorithm that combines the proposed regularization term with knowledge distillation using a semi-supervised teacher (i.e., a teacher model trained using a semi-supervised learning algorithm). Our experiments show that our proposed algorithm achieves state-of-the-art performance with significant margins compared to existing algorithms. In particular, compared to supervised learning algorithms, performance of our proposed algorithm is not much worse even when the amount of labeled data is very small. For example, our algorithm with only 8\% labeled data is comparable to supervised adversarial training algorithms that use all labeled data, both in terms of standard and robust accuracies on CIFAR-10.
</details>
<details>
<summary>摘要</summary>
“反抗学习”是一个在人工智能领域受到非常多关注的研究领域，最近几年来它已经受到了很多关注。然而，当前的研究主要集中在超级vised学习中，假设有充足的标注数据。在这篇论文中，我们研究了半supervised adversarial training，即在标注数据匮乏的情况下进行反抗训练。我们 derive了两个Upper bound的 robust risk，并提出了一个基于这两个Upper bound的Regularization term。然后，我们开发了一种半supervised adversarial training算法，该算法将该Regularization term与知识塑化（即使用半supervised学习算法训练的教师模型）结合使用。我们的实验表明，我们的提出的算法可以达到现有算法的状态之巅性表现，并且与supervised学习算法相比，它在很小量标注数据下的性能不会太差。例如，我们的算法只使用8%的标注数据时，与supervised adversarial training算法相比，其性能在标准和Robust度上都不会太差，尤其是在CIFAR-10上。
</details></li>
</ul>
<hr>
<h2 id="Backdoor-Federated-Learning-by-Poisoning-Backdoor-Critical-Layers"><a href="#Backdoor-Federated-Learning-by-Poisoning-Backdoor-Critical-Layers" class="headerlink" title="Backdoor Federated Learning by Poisoning Backdoor-Critical Layers"></a>Backdoor Federated Learning by Poisoning Backdoor-Critical Layers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04466">http://arxiv.org/abs/2308.04466</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haomin Zhuang, Mingxian Yu, Hao Wang, Yang Hua, Jian Li, Xu Yuan</li>
<li>for: 本研究旨在探讨 Federated Learning（FL）中的后门攻击和防御策略，特别是关注小部分层次（Backdoor-Critical，BC）的攻击和防御。</li>
<li>methods: 本研究提出了一种基于实际攻击者视角的BC层自适应后门攻击方法，通过细化地攻击BC层来实现较好的攻击效果和隐蔽性。</li>
<li>results: 实验结果显示，对于七种SOTA防御策略，我们的BC层自适应后门攻击可以成功攻击FL模型，仅需10%的坏客户端，并且超过最新的后门攻击方法。<details>
<summary>Abstract</summary>
Federated learning (FL) has been widely deployed to enable machine learning training on sensitive data across distributed devices. However, the decentralized learning paradigm and heterogeneity of FL further extend the attack surface for backdoor attacks. Existing FL attack and defense methodologies typically focus on the whole model. None of them recognizes the existence of backdoor-critical (BC) layers-a small subset of layers that dominate the model vulnerabilities. Attacking the BC layers achieves equivalent effects as attacking the whole model but at a far smaller chance of being detected by state-of-the-art (SOTA) defenses. This paper proposes a general in-situ approach that identifies and verifies BC layers from the perspective of attackers. Based on the identified BC layers, we carefully craft a new backdoor attack methodology that adaptively seeks a fundamental balance between attacking effects and stealthiness under various defense strategies. Extensive experiments show that our BC layer-aware backdoor attacks can successfully backdoor FL under seven SOTA defenses with only 10% malicious clients and outperform the latest backdoor attack methods.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 已经广泛应用于在分布式设备上进行敏感数据的机器学习训练。然而，分布式学习 paradigma 和 FL 中的不同设备之间的差异，使得攻击表面变得更加扩大。现有的 FL 攻击和防御方法ologies 通常集中在整个模型上。其中没有一个认可 BC 层（backdoor-critical layers）——一个小型层的 subsets 控制模型的漏洞。对 BC 层进行攻击可以达到攻击整个模型的同样效果，但是在现有的防御策略下可以让攻击更加隐蔽。本文提出了一种通用的即场方法，可以从攻击者的角度进行 BC 层的识别和验证。根据识别的 BC 层，我们针对这些层进行了一种新的后门攻击方法，可以在不同的防御策略下寻求一种基本的平衡，以确保攻击的效果和隐蔽性。经过广泛的实验，我们发现，我们的 BC 层认可后门攻击可以在七种 SOTA 防御策略下成功后门 FL，且比最新的后门攻击方法更高效。
</details></li>
</ul>
<hr>
<h2 id="Toward-Improving-Predictive-Risk-Modelling-for-New-Zealand’s-Child-Welfare-System-Using-Clustering-Methods"><a href="#Toward-Improving-Predictive-Risk-Modelling-for-New-Zealand’s-Child-Welfare-System-Using-Clustering-Methods" class="headerlink" title="Toward Improving Predictive Risk Modelling for New Zealand’s Child Welfare System Using Clustering Methods"></a>Toward Improving Predictive Risk Modelling for New Zealand’s Child Welfare System Using Clustering Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04060">http://arxiv.org/abs/2308.04060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sahar Barmomanesh, Victor Miranda-Soberanis</li>
<li>For: 这个论文旨在帮助社工更好地识别受到虐待儿童的风险，并决定当局是否应该 intervene。* Methods: 这个论文使用了行政数据和机器学习算法，以及主成分分析和K-Means聚类分析方法，以identify特定的儿童特征，并分析这些特征如何影响现有的风险模型的性能。* Results: 研究发现，使用不同的聚类方法可以分解出不同的儿童群体，并且这些群体之间存在显著的差异。LASSO逻辑回归模型在不同的聚类中都表现了较好的性能，但是对于年龄较小的儿童，模型表现更好。这些结果表明，可能需要为不同的年龄组分开发不同的风险模型，以提高模型的准确性。<details>
<summary>Abstract</summary>
The combination of clinical judgement and predictive risk models crucially assist social workers to segregate children at risk of maltreatment and decide when authorities should intervene. Predictive risk modelling to address this matter has been initiated by several governmental welfare authorities worldwide involving administrative data and machine learning algorithms. While previous studies have investigated risk factors relating to child maltreatment, several gaps remain as to understanding how such risk factors interact and whether predictive risk models perform differently for children with different features. By integrating Principal Component Analysis and K-Means clustering, this paper presents initial findings of our work on the identification of such features as well as their potential effect on current risk modelling frameworks. This approach allows examining existent, unidentified yet, clusters of New Zealand (NZ) children reported with care and protection concerns, as well as to analyse their inner structure, and evaluate the performance of prediction models trained cluster wise. We aim to discover the extent of clustering degree required as an early step in the development of predictive risk models for child maltreatment and so enhance the accuracy of such models intended for use by child protection authorities. The results from testing LASSO logistic regression models trained on identified clusters revealed no significant difference in their performance. The models, however, performed slightly better for two clusters including younger children. our results suggest that separate models might need to be developed for children of certain age to gain additional control over the error rates and to improve model accuracy. While results are promising, more evidence is needed to draw definitive conclusions, and further investigation is necessary.
</details>
<details>
<summary>摘要</summary>
“职业判断和预测风险模型在社工为了识别受到虐待风险的儿童和应否干预时作出决策非常重要。世界各地政府的儿童福利机构已经开始使用行政数据和机器学习算法进行预测风险模型。 although previous studies have investigated child maltreatment risk factors, there are still gaps in understanding how these risk factors interact and whether predictive risk models perform differently for children with different features. 本研究通过统计分析和K-Means聚类技术，初步发现了儿童特有的特征，并评估了这些特征对现有的风险模型的影响。我们的目标是发现这些群集的弹性度合理的程度，以便在开发预测风险模型时提高模型的准确性。我们发现，将模型训练分为各群集而训练，不会有 statistically significant difference in their performance. however, the models performed slightly better for two clusters including younger children. our results suggest that separate models might need to be developed for children of certain age to gain additional control over the error rates and to improve model accuracy. although the results are promising, more evidence is needed to draw definitive conclusions, and further investigation is necessary.”
</details></li>
</ul>
<hr>
<h2 id="The-Five-Dollar-Model-Generating-Game-Maps-and-Sprites-from-Sentence-Embeddings"><a href="#The-Five-Dollar-Model-Generating-Game-Maps-and-Sprites-from-Sentence-Embeddings" class="headerlink" title="The Five-Dollar Model: Generating Game Maps and Sprites from Sentence Embeddings"></a>The Five-Dollar Model: Generating Game Maps and Sprites from Sentence Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04052">http://arxiv.org/abs/2308.04052</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/TimMerino1710/five-dollar-model">https://github.com/TimMerino1710/five-dollar-model</a></li>
<li>paper_authors: Timothy Merino, Roman Negri, Dipika Rajesh, M Charity, Julian Togelius</li>
<li>for: 用于生成低维度图像，维护文本提示中的含义 semantics</li>
<li>methods: 使用五元模型，应用新的扩展策略来提高模型在有限数据集上的性能</li>
<li>results: 生成的图像保持文本提示中的含义，并且在三个小数据集上达到了良好的性能Here’s the same information in Traditional Chinese:</li>
<li>for: 用于生成低维度图像，维护文本提示中的含义 semantics</li>
<li>methods: 使用五元模型，应用新的扩展策略来提高模型在有限数据集上的性能</li>
<li>results: 生成的图像保持文本提示中的含义，并且在三个小数据集上达到了良好的性能<details>
<summary>Abstract</summary>
The five-dollar model is a lightweight text-to-image generative architecture that generates low dimensional images from an encoded text prompt. This model can successfully generate accurate and aesthetically pleasing content in low dimensional domains, with limited amounts of training data. Despite the small size of both the model and datasets, the generated images are still able to maintain the encoded semantic meaning of the textual prompt. We apply this model to three small datasets: pixel art video game maps, video game sprite images, and down-scaled emoji images and apply novel augmentation strategies to improve the performance of our model on these limited datasets. We evaluate our models performance using cosine similarity score between text-image pairs generated by the CLIP VIT-B/32 model.
</details>
<details>
<summary>摘要</summary>
五元模型是一种轻量级的文本到图像生成架构，可以将编码的文本提示转换成低维度的图像。这种模型可以在有限的训练数据量下生成准确和美观的内容，并且保持文本提示中的编码含义。我们对三个小 datasets（像素艺术视频游戏地图、视频游戏 sprite 图像和缩小的表情图像）进行应用，并使用新的扩展策略来提高我们的模型在这些有限的 datasets 上的性能。我们使用 CLIP VIT-B/32 模型计算文本-图像对的cosine相似性分数来评估我们的模型表现。
</details></li>
</ul>
<hr>
<h2 id="Generative-Models-for-Anomaly-Detection-and-Design-Space-Dimensionality-Reduction-in-Shape-Optimization"><a href="#Generative-Models-for-Anomaly-Detection-and-Design-Space-Dimensionality-Reduction-in-Shape-Optimization" class="headerlink" title="Generative Models for Anomaly Detection and Design-Space Dimensionality Reduction in Shape Optimization"></a>Generative Models for Anomaly Detection and Design-Space Dimensionality Reduction in Shape Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04051">http://arxiv.org/abs/2308.04051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Danny D’Agostino</li>
<li>for: 提高全球优化算法的效率，同时促进优化过程中高质量设计的生成。</li>
<li>methods: 减少原始设计变量，定义一个新的减少子空间，在这个子空间中最大化几何差异，并使用概率线性隐藏变量模型，如因子分析和概率主成分分析。</li>
<li>results: 通过测试DTMB 5415模型的壳体优化问题，示出了新框架可以提高全球优化算法的收敛，同时仅生成高质量几何特征的设计。<details>
<summary>Abstract</summary>
Our work presents a novel approach to shape optimization, that has the twofold objective to improve the efficiency of global optimization algorithms while promoting the generation of high-quality designs during the optimization process free of geometrical anomalies. This is accomplished by reducing the number of the original design variables defining a new reduced subspace where the geometrical variance is maximized and modeling the underlying generative process of the data via probabilistic linear latent variable models such as Factor Analysis and Probabilistic Principal Component Analysis. We show that the data follows approximately a Gaussian distribution when the shape modification method is linear and the design variables are sampled uniformly at random, due to the direct application of the central limit theorem. The model uncertainty is measured in terms of Mahalanobis distance, and the paper demonstrates that anomalous designs tend to exhibit a high value of this metric. This enables the definition of a new optimization model where anomalous geometries are penalized and consequently avoided during the optimization loop. The procedure is demonstrated for hull shape optimization of the DTMB 5415 model, extensively used as an international benchmark for shape optimization problems. The global optimization routine is carried out using Bayesian Optimization and the DIRECT algorithm. From the numerical results, the new framework improves the convergence of global optimization algorithms, while only designs with high-quality geometrical features are generated through the optimization routine thereby avoiding the wastage of precious computationally expensive simulations.
</details>
<details>
<summary>摘要</summary>
我们的工作提出了一种新的形优化方法，旨在提高全球优化算法的效率，同时在优化过程中生成高质量的设计，免受几何缺陷。这是通过减少原始设计变量数，定义一个新的减少子空间，在这个子空间中最大化几何差异，并通过泛化线性latent variable模型，如因子分析和概率主成分分析来模型数据。我们证明，当shape modification方法是线性的，并且设计变量是随机选择的时候，数据 approximate Gaussian distribution。模型不确定性是通过Mahalanobis距离测量的。我们发现，异常设计通常具有高值的这个指标。这使得我们可以定义一个新的优化模型，惩罚异常几何，并在优化过程中避免几何缺陷。我们使用 bayesian优化和DIRECT算法进行全球优化。从数值结果来看，新的框架可以提高全球优化算法的收敛速度，同时只有高质量的几何特征被生成，从而避免了 computationally expensive simulations的浪费。
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Study-on-TF-IDF-feature-Weighting-Method-and-its-Analysis-using-Unstructured-Dataset"><a href="#A-Comparative-Study-on-TF-IDF-feature-Weighting-Method-and-its-Analysis-using-Unstructured-Dataset" class="headerlink" title="A Comparative Study on TF-IDF feature Weighting Method and its Analysis using Unstructured Dataset"></a>A Comparative Study on TF-IDF feature Weighting Method and its Analysis using Unstructured Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04037">http://arxiv.org/abs/2308.04037</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mamata Das, Selvakumar K., P. J. A. Alphonse<br>for: 本研究旨在提高文本分类的精度，并 investigate了文本特征权重方法的影响。methods: 本研究使用了两种特征提取方法：N-Grams和TF-IDF，并使用了多种State-of-the-art分类器进行验证，包括支持向量机(SVM)、概率Logistic Regression、多omial Naive Bayes(Multinomial NB)、Random Forest、决策树和k-nearest neighbors(KNN)。results: 结果显示，基于TF-IDF特征而不是基于N-Grams特征时，文本特征权重方法可以获得显著提高的性能，TF-IDF得到了最高的准确率(93.81%),精度(94.20%),回归率(93.81%)和F1-score(91.99%)值，在Random Forest分类器中。<details>
<summary>Abstract</summary>
Text Classification is the process of categorizing text into the relevant categories and its algorithms are at the core of many Natural Language Processing (NLP). Term Frequency-Inverse Document Frequency (TF-IDF) and NLP are the most highly used information retrieval methods in text classification. We have investigated and analyzed the feature weighting method for text classification on unstructured data. The proposed model considered two features N-Grams and TF-IDF on the IMDB movie reviews and Amazon Alexa reviews dataset for sentiment analysis. Then we have used the state-of-the-art classifier to validate the method i.e., Support Vector Machine (SVM), Logistic Regression, Multinomial Naive Bayes (Multinomial NB), Random Forest, Decision Tree, and k-nearest neighbors (KNN). From those two feature extractions, a significant increase in feature extraction with TF-IDF features rather than based on N-Gram. TF-IDF got the maximum accuracy (93.81%), precision (94.20%), recall (93.81%), and F1-score (91.99%) value in Random Forest classifier.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Top-K-Relevant-Passage-Retrieval-for-Biomedical-Question-Answering"><a href="#Top-K-Relevant-Passage-Retrieval-for-Biomedical-Question-Answering" class="headerlink" title="Top K Relevant Passage Retrieval for Biomedical Question Answering"></a>Top K Relevant Passage Retrieval for Biomedical Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04028">http://arxiv.org/abs/2308.04028</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shashank140195/Biomedical_QA_Model">https://github.com/shashank140195/Biomedical_QA_Model</a></li>
<li>paper_authors: Shashank Gupta</li>
<li>for:  answers medical questions in the biomedical domain</li>
<li>methods:  uses the existing Dense Passage Retrieval framework and retrieves answers from Pubmed articles</li>
<li>results:  achieved a 0.81 F1 score on the BioASQ QA dataset<details>
<summary>Abstract</summary>
Question answering is a task that answers factoid questions using a large collection of documents. It aims to provide precise answers in response to the user's questions in natural language. Question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. On the web, there is no single article that could provide all the possible answers available on the internet to the question of the problem asked by the user. The existing Dense Passage Retrieval model has been trained on Wikipedia dump from Dec. 20, 2018, as the source documents for answering questions. Question answering (QA) has made big strides with several open-domain and machine comprehension systems built using large-scale annotated datasets. However, in the clinical domain, this problem remains relatively unexplored. According to multiple surveys, Biomedical Questions cannot be answered correctly from Wikipedia Articles. In this work, we work on the existing DPR framework for the biomedical domain and retrieve answers from the Pubmed articles which is a reliable source to answer medical questions. When evaluated on a BioASQ QA dataset, our fine-tuned dense retriever results in a 0.81 F1 score.
</details>
<details>
<summary>摘要</summary>
问答任务是回答用户问题，使用大量文档库。其目的是在自然语言中提供精确的答案。问答需要高效的段落检索，以选择可能的上下文，传统的稀疏 вектор空间模型，如TF-IDF或BM25，是现行的方法。在互联网上，没有单独的文章可以提供用户问题的所有可能的答案。现有的 dense passage retrieval 模型在2018年12月20日的Wikipedia dump上进行训练，用作答案的来源文档。问答（QA）在开放领域和机器理解领域得到了 significanthuge strides，但在医学领域，这个问题还很未得到了探索。根据多份调查，生物医学问题无法从Wikipedia文章中正确地回答。在这项工作中，我们在现有的 DPR 框架上进行了修改，并从可靠的 Pubmed 文章中检索答案。当 evaluated 在 BioASQ QA 数据集上时，我们的精制 dense retriever 得到了0.81 F1 分数。
</details></li>
</ul>
<hr>
<h2 id="Scope-Loss-for-Imbalanced-Classification-and-RL-Exploration"><a href="#Scope-Loss-for-Imbalanced-Classification-and-RL-Exploration" class="headerlink" title="Scope Loss for Imbalanced Classification and RL Exploration"></a>Scope Loss for Imbalanced Classification and RL Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04024">http://arxiv.org/abs/2308.04024</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hasham Burhani, Xiao Qi Shi, Jonathan Jaegerman, Daniel Balicki</li>
<li>for: This paper is written for researchers and practitioners in the field of reinforcement learning and supervised classification.</li>
<li>methods: The paper uses a novel loss function called Scope Loss, which adjusts gradients to prevent performance losses from over-exploitation and dataset imbalances.</li>
<li>results: The paper shows that Scope Loss outperforms state-of-the-art loss functions over a basket of benchmark reinforcement learning tasks and a skewed classification dataset.Here’s the information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了针对机器学习领域的奖励学习和监督学习而写的。</li>
<li>methods: 这篇论文使用了一种新的损失函数called Scope Loss，它调整 gradients来避免过度利用和数据不均衡导致的性能损失。</li>
<li>results: 这篇论文显示，Scope Loss 在一个笔记式的奖励学习任务和一个偏见的分类 dataset 上比 State-of-the-art 损失函数表现出色。<details>
<summary>Abstract</summary>
We demonstrate equivalence between the reinforcement learning problem and the supervised classification problem. We consequently equate the exploration exploitation trade-off in reinforcement learning to the dataset imbalance problem in supervised classification, and find similarities in how they are addressed. From our analysis of the aforementioned problems we derive a novel loss function for reinforcement learning and supervised classification. Scope Loss, our new loss function, adjusts gradients to prevent performance losses from over-exploitation and dataset imbalances, without the need for any tuning. We test Scope Loss against SOTA loss functions over a basket of benchmark reinforcement learning tasks and a skewed classification dataset, and show that Scope Loss outperforms other loss functions.
</details>
<details>
<summary>摘要</summary>
我们证明了增强学习问题和分类问题之间的等价性。我们遂视探索优化和数据集不均势问题在增强学习和分类中的相似性，并从这些问题的分析中获得了一个新的损失函数。我们称之为Scope Loss。Scope Loss可以调整Gradient以避免过度探索和数据集不均势导致的性能损失，无需任何调整。我们对一签benchmark增强学习任务和一个偏斜的分类 dataset进行测试，与现有的损失函数进行比较，发现Scope Loss在这些任务中表现更好。
</details></li>
</ul>
<hr>
<h2 id="Improving-Performance-of-Semi-Supervised-Learning-by-Adversarial-Attacks"><a href="#Improving-Performance-of-Semi-Supervised-Learning-by-Adversarial-Attacks" class="headerlink" title="Improving Performance of Semi-Supervised Learning by Adversarial Attacks"></a>Improving Performance of Semi-Supervised Learning by Adversarial Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04018">http://arxiv.org/abs/2308.04018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongyoon Yang, Kunwoong Kim, Yongdai Kim</li>
<li>for: 提高现代半监督学习算法的性能</li>
<li>methods: 使用对预训练模型进行 adversarial 攻击，选择高自信度无标样本进行标注</li>
<li>results: 在 CIFAR10 上，与 SCAR 结合的三种现代半监督学习算法显著提高图像分类性能<details>
<summary>Abstract</summary>
Semi-supervised learning (SSL) algorithm is a setup built upon a realistic assumption that access to a large amount of labeled data is tough. In this study, we present a generalized framework, named SCAR, standing for Selecting Clean samples with Adversarial Robustness, for improving the performance of recent SSL algorithms. By adversarially attacking pre-trained models with semi-supervision, our framework shows substantial advances in classifying images. We introduce how adversarial attacks successfully select high-confident unlabeled data to be labeled with current predictions. On CIFAR10, three recent SSL algorithms with SCAR result in significantly improved image classification.
</details>
<details>
<summary>摘要</summary>
半supervised learning（SSL）算法是基于现实的假设，即获得大量标注数据困难。在这项研究中，我们提出了一个通用框架，名为SCAR，意为选择干净样本并具有对抗性强化，以提高最近的SSL算法性能。我们通过对预训练模型进行对抗攻击，成功地选择高度自信的未标注样本进行标注。在CIFAR10上，我们使用SCAR结果与三种最近的SSL算法显著提高图像分类。
</details></li>
</ul>
<hr>
<h2 id="Continual-Pre-Training-of-Large-Language-Models-How-to-re-warm-your-model"><a href="#Continual-Pre-Training-of-Large-Language-Models-How-to-re-warm-your-model" class="headerlink" title="Continual Pre-Training of Large Language Models: How to (re)warm your model?"></a>Continual Pre-Training of Large Language Models: How to (re)warm your model?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04014">http://arxiv.org/abs/2308.04014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kshitij Gupta, Benjamin Thérien, Adam Ibrahim, Mats L. Richter, Quentin Anthony, Eugene Belilovsky, Irina Rish, Timothée Lesort</li>
<li>for: 这个论文的目的是探讨 kontinual pre-training 的可行性，即在新数据上更新先前训练的模型，而不是从scratch重新训练。</li>
<li>methods: 作者们使用了不同的暖身策略来研究模型在新数据上的表现。他们的假设是，在训练新数据时，学习率需要重新增加以提高计算效率。</li>
<li>results: 研究结果显示，虽然在训练新数据时模型的损失 initially increases，但在长期运行时，它们的下游数据表现得更好，even outperforming scratch-trained models。<details>
<summary>Abstract</summary>
Large language models (LLMs) are routinely pre-trained on billions of tokens, only to restart the process over again once new data becomes available. A much cheaper and more efficient solution would be to enable the continual pre-training of these models, i.e. updating pre-trained models with new data instead of re-training them from scratch. However, the distribution shift induced by novel data typically results in degraded performance on past data. Taking a step towards efficient continual pre-training, in this work, we examine the effect of different warm-up strategies. Our hypothesis is that the learning rate must be re-increased to improve compute efficiency when training on a new dataset. We study the warmup phase of models pre-trained on the Pile (upstream data, 300B tokens) as we continue to pre-train on SlimPajama (downstream data, 297B tokens), following a linear warmup and cosine decay schedule. We conduct all experiments on the Pythia 410M language model architecture and evaluate performance through validation perplexity. We experiment with different pre-training checkpoints, various maximum learning rates, and various warmup lengths. Our results show that while rewarming models first increases the loss on upstream and downstream data, in the longer run it improves the downstream performance, outperforming models trained from scratch$\unicode{x2013}$even for a large downstream dataset.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）通常在数十亿个字符上进行预训练，然后重新开始预训练过程。然而，这样的方法可能是比较昂贵和不fficient的。因此，我们提出了一种更有效的方法，即在新数据出现后不断更新已经预训练的模型，而不是从scratch重新训练模型。然而，新数据引入的分布变化通常会导致过去的数据表现下降。为了减少这种问题，在这项工作中，我们研究了不同的暖身策略的效果。我们假设，在训练新数据集时，学习率应该进行适度的增加，以提高计算效率。我们在Pile（上游数据集，300亿个字符）和SlimPajama（下游数据集，297亿个字符）中继续预训练Pythia 410M语言模型 architecture，并采用线性暖身和cosine衰减调度。我们在预训练检查点、最大学习率和暖身长度等方面进行了多种实验。我们的结果表明，虽然在upstream和downstream数据集上，首先使模型暖身会导致损失增加，但在更长时间后，它会提高下游表现，并在大下游数据集上超越从scratch训练的模型，即使使用大型数据集。
</details></li>
</ul>
<hr>
<h2 id="Generalization-bound-for-estimating-causal-effects-from-observational-network-data"><a href="#Generalization-bound-for-estimating-causal-effects-from-observational-network-data" class="headerlink" title="Generalization bound for estimating causal effects from observational network data"></a>Generalization bound for estimating causal effects from observational network data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04011">http://arxiv.org/abs/2308.04011</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruichu Cai, Zeqin Yang, Weilin Chen, Yuguang Yan, Zhifeng Hao</li>
<li>for: 这篇论文旨在适用于 causal effect estimation 问题，具体来说是在 observational network 数据上进行 causal effect 估计。</li>
<li>methods: 该论文使用了 joint propensity score 重新分配 schema 和 Integral Probability Metric (IPM) 表示学习 schema， derive 了一个通用的 generalization bound  для causal effect estimation。</li>
<li>results: 实验研究表明，该算法在两个实际网络上的 semi-synthetic 数据上具有效果。<details>
<summary>Abstract</summary>
Estimating causal effects from observational network data is a significant but challenging problem. Existing works in causal inference for observational network data lack an analysis of the generalization bound, which can theoretically provide support for alleviating the complex confounding bias and practically guide the design of learning objectives in a principled manner. To fill this gap, we derive a generalization bound for causal effect estimation in network scenarios by exploiting 1) the reweighting schema based on joint propensity score and 2) the representation learning schema based on Integral Probability Metric (IPM). We provide two perspectives on the generalization bound in terms of reweighting and representation learning, respectively. Motivated by the analysis of the bound, we propose a weighting regression method based on the joint propensity score augmented with representation learning. Extensive experimental studies on two real-world networks with semi-synthetic data demonstrate the effectiveness of our algorithm.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese: estimating causal effects from observational network data is a significant but challenging problem. existing works in causal inference for observational network data lack an analysis of the generalization bound, which can theoretically provide support for alleviating the complex confounding bias and practically guide the design of learning objectives in a principled manner. to fill this gap, we derive a generalization bound for causal effect estimation in network scenarios by exploiting 1) the reweighting schema based on joint propensity score and 2) the representation learning schema based on integral probability metric (ipm). we provide two perspectives on the generalization bound in terms of reweighting and representation learning, respectively. motivated by the analysis of the bound, we propose a weighting regression method based on the joint propensity score augmented with representation learning. extensive experimental studies on two real-world networks with semi-synthetic data demonstrate the effectiveness of our algorithm.
</details></li>
</ul>
<hr>
<h2 id="Understanding-CNN-Hidden-Neuron-Activations-Using-Structured-Background-Knowledge-and-Deductive-Reasoning"><a href="#Understanding-CNN-Hidden-Neuron-Activations-Using-Structured-Background-Knowledge-and-Deductive-Reasoning" class="headerlink" title="Understanding CNN Hidden Neuron Activations Using Structured Background Knowledge and Deductive Reasoning"></a>Understanding CNN Hidden Neuron Activations Using Structured Background Knowledge and Deductive Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03999">http://arxiv.org/abs/2308.03999</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abhilekha-dalal/xai-using-wikidataAndEcii">https://github.com/abhilekha-dalal/xai-using-wikidataAndEcii</a></li>
<li>paper_authors: Abhilekha Dalal, Md Kamruzzaman Sarker, Adrita Barua, Eugene Vasserman, Pascal Hitzler</li>
<li>for: This paper aims to provide a systematic and automated method for interpreting the activations of hidden neurons in deep learning systems, specifically Convolutional Neural Networks (CNNs).</li>
<li>methods: The proposed method uses large-scale background knowledge and a symbolic reasoning approach called Concept Induction based on description logics to attach meaningful labels from the background knowledge to individual neurons in the dense layer of a CNN.</li>
<li>results: The paper demonstrates that the proposed method provides meaningful interpretations of hidden neuron activations, allowing for a better understanding of what the CNN has internally detected as relevant on the input.<details>
<summary>Abstract</summary>
A major challenge in Explainable AI is in correctly interpreting activations of hidden neurons: accurate interpretations would provide insights into the question of what a deep learning system has internally detected as relevant on the input, demystifying the otherwise black-box character of deep learning systems. The state of the art indicates that hidden node activations can, in some cases, be interpretable in a way that makes sense to humans, but systematic automated methods that would be able to hypothesize and verify interpretations of hidden neuron activations are underexplored. In this paper, we provide such a method and demonstrate that it provides meaningful interpretations. Our approach is based on using large-scale background knowledge approximately 2 million classes curated from the Wikipedia concept hierarchy together with a symbolic reasoning approach called Concept Induction based on description logics, originally developed for applications in the Semantic Web field. Our results show that we can automatically attach meaningful labels from the background knowledge to individual neurons in the dense layer of a Convolutional Neural Network through a hypothesis and verification process.
</details>
<details>
<summary>摘要</summary>
一个主要挑战在可解释AI是正确解释隐藏节点的活动：正确的解释可以提供关于深度学习系统内部检测到的输入相关信息的深入了解，从而使深度学习系统从黑盒模型变为 clearer。现状的最佳实践表明，隐藏节点的活动可以在某些情况下被解释为人类可理解的方式，但是系统化的自动方法，能够对隐藏节点的活动进行假设和验证，尚未得到充分的探索。在这篇论文中，我们提供了一种这样的方法，并证明了它可以提供有意义的解释。我们的方法基于使用大规模的背景知识，约200万个类型从Wikipedia概念层次结构中精心筛选出来，并使用符号逻辑方法called Concept Induction，原本是为Semantic Web领域开发的。我们的结果表明，我们可以通过一种假设和验证过程，自动将 background knowledge 中的意义 labels 附加到 convolutional Neural Network 的稠密层中的各个神经元。
</details></li>
</ul>
<hr>
<h2 id="Cooperative-Multi-Type-Multi-Agent-Deep-Reinforcement-Learning-for-Resource-Management-in-Space-Air-Ground-Integrated-Networks"><a href="#Cooperative-Multi-Type-Multi-Agent-Deep-Reinforcement-Learning-for-Resource-Management-in-Space-Air-Ground-Integrated-Networks" class="headerlink" title="Cooperative Multi-Type Multi-Agent Deep Reinforcement Learning for Resource Management in Space-Air-Ground Integrated Networks"></a>Cooperative Multi-Type Multi-Agent Deep Reinforcement Learning for Resource Management in Space-Air-Ground Integrated Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03995">http://arxiv.org/abs/2308.03995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hengxi Zhang, Huaze Tang, Wenbo Ding, Xiao-Ping Zhang</li>
<li>for: 提高智能城市应用的潜在价值和可行性</li>
<li>methods: 提出了一种涵盖五个不同通信链的完整SAGIN系统，并提出了一种高效的多类多代理深度学习方法来解决资源管理问题</li>
<li>results: 实验结果表明提议的CMT-MARL方法能够提高总传输率和传输成功率，这些结果证明SAGIN的可能性和实现性<details>
<summary>Abstract</summary>
The Space-Air-Ground Integrated Network (SAGIN), integrating heterogeneous devices including low earth orbit (LEO) satellites, unmanned aerial vehicles (UAVs), and ground users (GUs), holds significant promise for advancing smart city applications. However, resource management of the SAGIN is a challenge requiring urgent study in that inappropriate resource management will cause poor data transmission, and hence affect the services in smart cities. In this paper, we develop a comprehensive SAGIN system that encompasses five distinct communication links and propose an efficient cooperative multi-type multi-agent deep reinforcement learning (CMT-MARL) method to address the resource management issue. The experimental results highlight the efficacy of the proposed CMT-MARL, as evidenced by key performance indicators such as the overall transmission rate and transmission success rate. These results underscore the potential value and feasibility of future implementation of the SAGIN.
</details>
<details>
<summary>摘要</summary>
space-air-ground 集成网络（SAGIN），包括低地球轨道卫星（LEO）、无人飞行器（UAV）和地面用户（GU）等多种设备，具有推动智能城市应用的潜在优势。然而，SAGIN资源管理却是一项需要紧迫研究的挑战，因为不当的资源管理可能导致数据传输差，影响智能城市服务的质量。在本文中，我们提出了一个全面的 SAGIN 系统，包括五种不同的通信链接，并提出了一种高效的合作多类多代理深度学习（CMT-MARL）方法来解决资源管理问题。实验结果表明，提议的 CMT-MARL 方法具有优秀的性能指标，如总传输率和传输成功率。这些结果证明了 SAGIN 的可能价值和可行性，并且为未来实施 SAGIN 提供了重要的参考。
</details></li>
</ul>
<hr>
<h2 id="Fourier-neural-operator-for-real-time-simulation-of-3D-dynamic-urban-microclimate"><a href="#Fourier-neural-operator-for-real-time-simulation-of-3D-dynamic-urban-microclimate" class="headerlink" title="Fourier neural operator for real-time simulation of 3D dynamic urban microclimate"></a>Fourier neural operator for real-time simulation of 3D dynamic urban microclimate</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03985">http://arxiv.org/abs/2308.03985</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenhui Peng, Shaoxiang Qin, Senwen Yang, Jianchun Wang, Xue Liu, Liangzhu, Wang</li>
<li>for: 这篇论文主要关注于城市微气候的研究，以提高城市舒适性、健康性和建筑物能效性。</li>
<li>methods: 该论文使用了深度学习技术来加速复杂非线性交互的模型化。特别是使用了快速傅里叶 ней网络（FNO）来实时三维城市风场 simulate。</li>
<li>results: 实验结果表明，FNO模型可以准确重建快速更新的三维城市风场。此外，FNO方法可以在不同风向下进行泛化，并且在图形处理器上进行实时预测，使得城市微气候的实时模拟成为可能。<details>
<summary>Abstract</summary>
Global urbanization has underscored the significance of urban microclimates for human comfort, health, and building/urban energy efficiency. They profoundly influence building design and urban planning as major environmental impacts. Understanding local microclimates is essential for cities to prepare for climate change and effectively implement resilience measures. However, analyzing urban microclimates requires considering a complex array of outdoor parameters within computational domains at the city scale over a longer period than indoors. As a result, numerical methods like Computational Fluid Dynamics (CFD) become computationally expensive when evaluating the impact of urban microclimates. The rise of deep learning techniques has opened new opportunities for accelerating the modeling of complex non-linear interactions and system dynamics. Recently, the Fourier Neural Operator (FNO) has been shown to be very promising in accelerating solving the Partial Differential Equations (PDEs) and modeling fluid dynamic systems. In this work, we apply the FNO network for real-time three-dimensional (3D) urban wind field simulation. The training and testing data are generated from CFD simulation of the urban area, based on the semi-Lagrangian approach and fractional stepping method to simulate urban microclimate features for modeling large-scale urban problems. Numerical experiments show that the FNO model can accurately reconstruct the instantaneous spatial velocity field. We further evaluate the trained FNO model on unseen data with different wind directions, and the results show that the FNO model can generalize well on different wind directions. More importantly, the FNO approach can make predictions within milliseconds on the graphics processing unit, making real-time simulation of 3D dynamic urban microclimate possible.
</details>
<details>
<summary>摘要</summary>
全球城市化强调了城市微气候对人类舒适、健康和建筑/城市能效的重要性。它们对城市规划和建筑设计产生了深远的影响，是主要的环境因素。了解当地微气候非常重要，以便城市在气候变化面前做好准备，有效地实施抗逆性措施。然而，分析城市微气候需要考虑一系列的外部参数，包括城市规模内的计算领域和时间长达于室内。这使得计算流体动力学（CFD）方法成为计算成本较高的方法。随着深度学习技术的发展，新的机会在于加速复杂非线性交互和系统动力学模型化。在这种情况下，我们使用了整流神经网络（FNO）来实现实时三维城市风场 simulate。我们的实验表明，FNO网络可以准确重construct三维风场的快速变化。此外，我们还评估了在不同风向下测试的FNO模型，结果表明FNO模型可以在不同风向下 generale well。更重要的是，FNO方法可以在毫秒级别内进行预测，使实时三维动态城市微气候的模拟成为可能。
</details></li>
</ul>
<hr>
<h2 id="Characterization-of-Human-Balance-through-a-Reinforcement-Learning-based-Muscle-Controller"><a href="#Characterization-of-Human-Balance-through-a-Reinforcement-Learning-based-Muscle-Controller" class="headerlink" title="Characterization of Human Balance through a Reinforcement Learning-based Muscle Controller"></a>Characterization of Human Balance through a Reinforcement Learning-based Muscle Controller</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04462">http://arxiv.org/abs/2308.04462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kübra Akbaş, Carlotta Mummolo, Xianlian Zhou</li>
<li>for: This paper aims to provide a new approach for objectively assessing balance capability in humans by using a musculoskeletal model integrated with a balance controller trained through reinforcement learning.</li>
<li>methods: The paper employs a musculoskeletal model, reinforcement learning, and Proximal Policy Optimization to train a balance controller and investigate balancing capabilities.</li>
<li>results: The study shows that the approach can provide a promising new method for establishing balance recovery limits and objectively assessing balance capability in bipedal systems, particularly in humans, and reveals the effects of muscle weakness and neural excitation delay on balance recovery.Here is the simplified Chinese translation of the three key points:</li>
<li>for: 这篇论文目的是提供一种新的方法来对人类的平衡能力进行 объектив评估，使用一种 integrate 了 musculoskeletal 模型和平衡控制器的 reinforcement learning 方法。</li>
<li>methods: 这篇论文使用了 musculoskeletal 模型、reinforcement learning 和 Proximal Policy Optimization 等方法来训练平衡控制器并调查平衡能力。</li>
<li>results: 这个研究表明这种方法可以提供一种有前途的新方法来确定平衡恢复限制和对人类的平衡能力进行 объектив评估，并揭示了肌肉衰竭和神经延迟对平衡恢复的影响。<details>
<summary>Abstract</summary>
Balance assessment during physical rehabilitation often relies on rubric-oriented battery tests to score a patient's physical capabilities, leading to subjectivity. While some objective balance assessments exist, they are often limited to tracking the center of pressure (COP), which does not fully capture the whole-body postural stability. This study explores the use of the center of mass (COM) state space and presents a promising avenue for monitoring the balance capabilities in humans. We employ a musculoskeletal model integrated with a balance controller, trained through reinforcement learning (RL), to investigate balancing capabilities. The RL framework consists of two interconnected neural networks governing balance recovery and muscle coordination respectively, trained using Proximal Policy Optimization (PPO) with reference state initialization, early termination, and multiple training strategies. By exploring recovery from random initial COM states (position and velocity) space for a trained controller, we obtain the final BR enclosing successful balance recovery trajectories. Comparing the BRs with analytical postural stability limits from a linear inverted pendulum model, we observe a similar trend in successful COM states but more limited ranges in the recoverable areas. We further investigate the effect of muscle weakness and neural excitation delay on the BRs, revealing reduced balancing capability in different regions. Overall, our approach of learning muscular balance controllers presents a promising new method for establishing balance recovery limits and objectively assessing balance capability in bipedal systems, particularly in humans.
</details>
<details>
<summary>摘要</summary>
评估身体重建期间的平衡能力frequently使用测试 battery Rubric-oriented 评估病人的身体能力，导致主观性。 Although some objective balance assessments exist, they are often limited to tracking the center of pressure (COP), which does not fully capture the whole-body postural stability. This study explores the use of the center of mass (COM) state space and presents a promising avenue for monitoring balance capabilities in humans. We employ a musculoskeletal model integrated with a balance controller, trained through reinforcement learning (RL), to investigate balancing capabilities. The RL framework consists of two interconnected neural networks governing balance recovery and muscle coordination, respectively, trained using Proximal Policy Optimization (PPO) with reference state initialization, early termination, and multiple training strategies. By exploring recovery from random initial COM states (position and velocity) space for a trained controller, we obtain the final balance recovery (BR) enclosing successful balance recovery trajectories. Comparing the BRs with analytical postural stability limits from a linear inverted pendulum model, we observe a similar trend in successful COM states but more limited ranges in the recoverable areas. We further investigate the effect of muscle weakness and neural excitation delay on the BRs, revealing reduced balancing capability in different regions. Overall, our approach of learning muscular balance controllers presents a promising new method for establishing balance recovery limits and objectively assessing balance capability in bipedal systems, particularly in humans.
</details></li>
</ul>
<hr>
<h2 id="PUG-Photorealistic-and-Semantically-Controllable-Synthetic-Data-for-Representation-Learning"><a href="#PUG-Photorealistic-and-Semantically-Controllable-Synthetic-Data-for-Representation-Learning" class="headerlink" title="PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning"></a>PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03977">http://arxiv.org/abs/2308.03977</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/pug">https://github.com/facebookresearch/pug</a></li>
<li>paper_authors: Florian Bordes, Shashank Shekhar, Mark Ibrahim, Diane Bouchacourt, Pascal Vincent, Ari S. Morcos</li>
<li>for: 这项研究旨在推广使用真实准确的 sintetic 图像数据，以便更好地设计和评估深度神经网络。</li>
<li>methods: 这项研究使用了 Unreal Engine 游戏引擎，生成了 Photorealistic Unreal Graphics（PUG）环境和数据集，以便进行表示学习研究。</li>
<li>results: 这项研究示出了 PUG 环境和数据集可以帮助进行更加正式的视觉模型评估。<details>
<summary>Abstract</summary>
Synthetic image datasets offer unmatched advantages for designing and evaluating deep neural networks: they make it possible to (i) render as many data samples as needed, (ii) precisely control each scene and yield granular ground truth labels (and captions), (iii) precisely control distribution shifts between training and testing to isolate variables of interest for sound experimentation. Despite such promise, the use of synthetic image data is still limited -- and often played down -- mainly due to their lack of realism. Most works therefore rely on datasets of real images, which have often been scraped from public images on the internet, and may have issues with regards to privacy, bias, and copyright, while offering little control over how objects precisely appear. In this work, we present a path to democratize the use of photorealistic synthetic data: we develop a new generation of interactive environments for representation learning research, that offer both controllability and realism. We use the Unreal Engine, a powerful game engine well known in the entertainment industry, to produce PUG (Photorealistic Unreal Graphics) environments and datasets for representation learning. In this paper, we demonstrate the potential of PUG to enable more rigorous evaluations of vision models.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传统的实验室数据集有以下几个缺点：首先，它们的数据量有限，使得模型的训练和评估受到限制。其次，实验室数据集通常是从互联网上抓取的，可能受到隐私、偏见和版权问题的影响。最后，实验室数据集的分布shift可能会导致模型在测试中表现不佳。在这种情况下，使用合成图像数据集成为一个有利的选择。合成图像数据集可以提供大量的数据样本，并且可以准确地控制每个场景和获得精细的标签和描述。在本研究中，我们提出了一种使用Unreal Engine游戏引擎生成PUG（实际图像）环境和数据集，以便进行表征学学习的研究。我们使用Unreal Engine来生成PUG环境，以提供更加准确和有利的图像数据。在本篇论文中，我们展示了PUG环境的潜在力量，可以帮助更好地评估视觉模型。>>
</details></li>
</ul>
<hr>
<h2 id="Amortized-Global-Search-for-Efficient-Preliminary-Trajectory-Design-with-Deep-Generative-Models"><a href="#Amortized-Global-Search-for-Efficient-Preliminary-Trajectory-Design-with-Deep-Generative-Models" class="headerlink" title="Amortized Global Search for Efficient Preliminary Trajectory Design with Deep Generative Models"></a>Amortized Global Search for Efficient Preliminary Trajectory Design with Deep Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03960">http://arxiv.org/abs/2308.03960</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anjian Li, Amlan Sinha, Ryne Beeson</li>
<li>for: 本 paper 的目的是提出一种基于归一化搜索的抽象轨迹设计方法，以解决高维度和非几何的轨迹优化问题。</li>
<li>methods: 本 paper 使用深度生成模型来预测 trajectory 解的结构，并使用这些结构来加速对未经过seen的参数值进行全球搜索。</li>
<li>results: 本 paper 通过使用 deep generative models 预测 trajectory 解的结构，提高了轨迹优化问题的解决效率。<details>
<summary>Abstract</summary>
Preliminary trajectory design is a global search problem that seeks multiple qualitatively different solutions to a trajectory optimization problem. Due to its high dimensionality and non-convexity, and the frequent adjustment of problem parameters, the global search becomes computationally demanding. In this paper, we exploit the clustering structure in the solutions and propose an amortized global search (AmorGS) framework. We use deep generative models to predict trajectory solutions that share similar structures with previously solved problems, which accelerates the global search for unseen parameter values. Our method is evaluated using De Jong's 5th function and a low-thrust circular restricted three-body problem.
</details>
<details>
<summary>摘要</summary>
先期轨迹设计是一个全球搜索问题，旨在找到多个 качеitatively 不同的轨迹优化问题的解。由于其高维度和非拟合性，以及问题参数的频繁调整，全球搜索变得计算挑战。在这篇论文中，我们利用聚类结构在解决方案中，并提出了一个总体搜索（AmorGS）框架。我们使用深度生成模型预测轨迹解决方案，这些解决方案与之前解决过的问题有相似的结构，从而加速全球搜索未见参数值。我们的方法在De Jong的第五函数和一个低推力圆形三体问题中进行评估。
</details></li>
</ul>
<hr>
<h2 id="Fixed-Inter-Neuron-Covariability-Induces-Adversarial-Robustness"><a href="#Fixed-Inter-Neuron-Covariability-Induces-Adversarial-Robustness" class="headerlink" title="Fixed Inter-Neuron Covariability Induces Adversarial Robustness"></a>Fixed Inter-Neuron Covariability Induces Adversarial Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03956">http://arxiv.org/abs/2308.03956</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Ahmed Shah, Bhiksha Raj</li>
<li>for: 提高深度神经网络（DNN）对 adversarial perturbation 的Robustness，以确保其在实际应用中的可靠性。</li>
<li>methods: 开发了 Self-Consistent Activation（SCA）层，该层包含 neuron 的活动是相互协调的，以满足一定的 covariability 模式。</li>
<li>results: 在图像和声音识别任务中，使用 SCA 层的模型可以达到高精度，并对 state-of-the-art Auto-PGD adversarial attack 显示出更高的Robustness，无需在 adversarially perturbed data 上进行训练。<details>
<summary>Abstract</summary>
The vulnerability to adversarial perturbations is a major flaw of Deep Neural Networks (DNNs) that raises question about their reliability when in real-world scenarios. On the other hand, human perception, which DNNs are supposed to emulate, is highly robust to such perturbations, indicating that there may be certain features of the human perception that make it robust but are not represented in the current class of DNNs. One such feature is that the activity of biological neurons is correlated and the structure of this correlation tends to be rather rigid over long spans of times, even if it hampers performance and learning. We hypothesize that integrating such constraints on the activations of a DNN would improve its adversarial robustness, and, to test this hypothesis, we have developed the Self-Consistent Activation (SCA) layer, which comprises of neurons whose activations are consistent with each other, as they conform to a fixed, but learned, covariability pattern. When evaluated on image and sound recognition tasks, the models with a SCA layer achieved high accuracy, and exhibited significantly greater robustness than multi-layer perceptron models to state-of-the-art Auto-PGD adversarial attacks \textit{without being trained on adversarially perturbed data
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="PMU-measurements-based-short-term-voltage-stability-assessment-of-power-systems-via-deep-transfer-learning"><a href="#PMU-measurements-based-short-term-voltage-stability-assessment-of-power-systems-via-deep-transfer-learning" class="headerlink" title="PMU measurements based short-term voltage stability assessment of power systems via deep transfer learning"></a>PMU measurements based short-term voltage stability assessment of power systems via deep transfer learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03953">http://arxiv.org/abs/2308.03953</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/SuperBruceJia/Power-Systems-Stability-Transfer-Learning">https://github.com/SuperBruceJia/Power-Systems-Stability-Transfer-Learning</a></li>
<li>paper_authors: Yang Li, Shitu Zhang, Yuanzheng Li, Jiting Cao, Shuyue Jia</li>
<li>for: 本研究旨在提出一种基于PMU测量数据的短期电压稳定评估方法，以解决现有深度学习方法对于电网结构变化、标签采集和小样本处理的局限性。</li>
<li>methods: 本方法使用深度传输学习，利用PMU测量数据创建初始数据集，采用时间拼接标注法和LSGAN数据增强技术，实现深度学习在小样本上的有效采用。另外，该方法还具有适应电网结构变化的能力，通过探索不同缺陷之间的连接关系。</li>
<li>results: 对于IEEE 39-bus试验系统，提出的方法可以通过转移学习提高评估精度约20%，并且具有强大的适应电网结构变化能力。与浅学习方法和其他深度学习基于方法相比，该方法具有显著的优势，特别是通过使用Transformer模型中的自注意机制。<details>
<summary>Abstract</summary>
Deep learning has emerged as an effective solution for addressing the challenges of short-term voltage stability assessment (STVSA) in power systems. However, existing deep learning-based STVSA approaches face limitations in adapting to topological changes, sample labeling, and handling small datasets. To overcome these challenges, this paper proposes a novel phasor measurement unit (PMU) measurements-based STVSA method by using deep transfer learning. The method leverages the real-time dynamic information captured by PMUs to create an initial dataset. It employs temporal ensembling for sample labeling and utilizes least squares generative adversarial networks (LSGAN) for data augmentation, enabling effective deep learning on small-scale datasets. Additionally, the method enhances adaptability to topological changes by exploring connections between different faults. Experimental results on the IEEE 39-bus test system demonstrate that the proposed method improves model evaluation accuracy by approximately 20% through transfer learning, exhibiting strong adaptability to topological changes. Leveraging the self-attention mechanism of the Transformer model, this approach offers significant advantages over shallow learning methods and other deep learning-based approaches.
</details>
<details>
<summary>摘要</summary>
深度学习已成为电力系统短期电压稳定评估（STVSA）的有效解决方案。然而，现有的深度学习基于STVSA方法存在适应性改变和小样本标注的限制。为了突破这些挑战，本文提出了一种基于phasor measurement unit（PMU）测量的新型STVSA方法。该方法利用PMU在实时动态信息中捕捉到的实际数据来创建初始数据集。它采用时间ensemble для样本标注，并使用最小二乘生成整形网络（LSGAN）进行数据扩展，以便在小规模数据集上进行深度学习。此外，该方法增强了 topological change 的适应性，通过探索不同的缺陷之间的连接。实验结果表明，该方法在IEEE 39-bus测试系统上提高了模型评估精度约20%，并且具有强大的适应性。基于Transformer模型的自注意机制，这种方法在对比浅学习方法和其他深度学习基于方法上表现出了显著优势。
</details></li>
</ul>
<hr>
<h2 id="The-Prospect-of-Enhancing-Large-Scale-Heterogeneous-Federated-Learning-with-Transformers"><a href="#The-Prospect-of-Enhancing-Large-Scale-Heterogeneous-Federated-Learning-with-Transformers" class="headerlink" title="The Prospect of Enhancing Large-Scale Heterogeneous Federated Learning with Transformers"></a>The Prospect of Enhancing Large-Scale Heterogeneous Federated Learning with Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03945">http://arxiv.org/abs/2308.03945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yulan Gao, Zhaoxiang Hou, Chengyi Yang, Zengxiang Li, Han Yu</li>
<li>for: This paper investigates the use of Transformer-based federated learning (FL) models for achieving generalization and personalization in large-scale, heterogeneous FL tasks.</li>
<li>methods: The paper compares the performance of Transformer-based FL models with other deep neural network-based approaches, including ResNet and personalized ResNet-based FL, under various scenarios with varying numbers of data owners.</li>
<li>results: The paper shows that Transformer-based FL models outperform other approaches in large-scale, heterogeneous FL tasks, and provides insight into the reasons behind their superior performance through analysis of the Centered Kernel Alignment (CKA) representation similarity across different layers and FL models.<details>
<summary>Abstract</summary>
Federated learning (FL) addresses data privacy concerns by enabling collaborative training of AI models across distributed data owners. Wide adoption of FL faces the fundamental challenges of data heterogeneity and the large scale of data owners involved. In this paper, we investigate the prospect of Transformer-based FL models for achieving generalization and personalization in this setting. We conduct extensive comparative experiments involving FL with Transformers, ResNet, and personalized ResNet-based FL approaches under various scenarios. These experiments consider varying numbers of data owners to demonstrate Transformers' advantages over deep neural networks in large-scale heterogeneous FL tasks. In addition, we analyze the superior performance of Transformers by comparing the Centered Kernel Alignment (CKA) representation similarity across different layers and FL models to gain insight into the reasons behind their promising capabilities.
</details>
<details>
<summary>摘要</summary>
合作学习（FL）解决了数据隐私问题，通过在分布式数据所有者之间进行AI模型的共同训练。随着FL的广泛应用，面临着数据不一致和数据所有者的大规模挑战。本文 investigate了使用Transformer-based FL模型来实现泛化和个性化在这种情况下。我们进行了对FL与Transformers、ResNet和个性化ResNet-based FL方法的比较性实验，包括不同数据所有者的场景。这些实验演示了Transformers在大规模不一致FL任务中的优势。此外，我们还分析了CKA表示相似性的中心kernel对FL模型的性能提高的原因。
</details></li>
</ul>
<hr>
<h2 id="GraPhSyM-Graph-Physical-Synthesis-Model"><a href="#GraPhSyM-Graph-Physical-Synthesis-Model" class="headerlink" title="GraPhSyM: Graph Physical Synthesis Model"></a>GraPhSyM: Graph Physical Synthesis Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03944">http://arxiv.org/abs/2308.03944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Agiza, Rajarshi Roy, Teodor Dumitru Ene, Saad Godil, Sherief Reda, Bryan Catanzaro</li>
<li>For: 预计算机化电路延迟和面积指标的快速和准确估算。* Methods: 使用图structure、连接性和电学性特征来预测物理合成变换的影响，并通过图注意力网络模型（GATv2）进行预测。* Results: 在6000个预测器添加器设计中训练GraPhSyM模型，可以快速和准确地预测未看过的添加器延迟和面积指标（98.3%和96.1%），并且可以在不同的延迟目标下精确预测添加器延迟和面积指标。<details>
<summary>Abstract</summary>
In this work, we introduce GraPhSyM, a Graph Attention Network (GATv2) model for fast and accurate estimation of post-physical synthesis circuit delay and area metrics from pre-physical synthesis circuit netlists. Once trained, GraPhSyM provides accurate visibility of final design metrics to early EDA stages, such as logic synthesis, without running the slow physical synthesis flow, enabling global co-optimization across stages. Additionally, the swift and precise feedback provided by GraPhSym is instrumental for machine-learning-based EDA optimization frameworks. Given a gate-level netlist of a circuit represented as a graph, GraPhSyM utilizes graph structure, connectivity, and electrical property features to predict the impact of physical synthesis transformations such as buffer insertion and gate sizing. When trained on a dataset of 6000 prefix adder designs synthesized at an aggressive delay target, GraPhSyM can accurately predict the post-synthesis delay (98.3%) and area (96.1%) metrics of unseen adders with a fast 0.22s inference time. Furthermore, we illustrate the compositionality of GraPhSyM by employing the model trained on a fixed delay target to accurately anticipate post-synthesis metrics at a variety of unseen delay targets. Lastly, we report promising generalization capabilities of the GraPhSyM model when it is evaluated on circuits different from the adders it was exclusively trained on. The results show the potential for GraPhSyM to serve as a powerful tool for advanced optimization techniques and as an oracle for EDA machine learning frameworks.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们引入了GraPhSyM模型，它是基于图注意力网络（GATv2）的一种快速和准确地计算逻辑电路延迟和面积指标的模型。一旦训练完成，GraPhSyM可以在普通逻辑合成阶段提供准确的最终设计指标视图，不需要执行慢速物理合成流程，从而实现全球协调。此外，GraPhSyM提供了快速和准确的反馈，这对机器学习基于EDA优化框架是非常有利的。给定一个逻辑电路的门级网络表示，GraPhSyM利用图结构、连接性和电学性特征来预测物理合成转换（如缓冲插入和ゲート大小调整）后的影响。当训练在6000个逻辑加法器的逻辑合成过程中，GraPhSyM可以准确预测未看过的加法器延迟（98.3%）和面积（96.1%）指标，并且具有0.22秒的快速推理时间。此外，我们证明了GraPhSyM模型的可组合性，可以使用已训练的固定延迟目标来准确预测未看过的延迟目标。最后，我们报告了GraPhSyM模型在不同于它专门训练的逻辑加法器上的良好泛化能力。结果表明，GraPhSyM可能成为高级优化技术和EDA机器学习框架的强大工具。
</details></li>
</ul>
<hr>
<h2 id="The-Compatibility-between-the-Pangu-Weather-Forecasting-Model-and-Meteorological-Operational-Data"><a href="#The-Compatibility-between-the-Pangu-Weather-Forecasting-Model-and-Meteorological-Operational-Data" class="headerlink" title="The Compatibility between the Pangu Weather Forecasting Model and Meteorological Operational Data"></a>The Compatibility between the Pangu Weather Forecasting Model and Meteorological Operational Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04460">http://arxiv.org/abs/2308.04460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wencong Cheng, Yan Yan, Jiangjiang Xia, Qi Liu, Chang Qu, Zhigang Wang</li>
<li>for: 这 paper 是为了评估 Pangu-Weather 模型与各种常用的 NWP 操作分析相容性。</li>
<li>methods: 这 paper 使用了 Pangu-Weather 模型，并通过 caso studies 评估了模型与不同 NWP 系统的操作分析之间的 compatibilty。</li>
<li>results: 结果表明，Pangu-Weather 模型与不同的操作分析相容，并且可以提高初始条件质量以提高预测性能。<details>
<summary>Abstract</summary>
Recently, multiple data-driven models based on machine learning for weather forecasting have emerged. These models are highly competitive in terms of accuracy compared to traditional numerical weather prediction (NWP) systems. In particular, the Pangu-Weather model, which is open source for non-commercial use, has been validated for its forecasting performance by the European Centre for Medium-Range Weather Forecasts (ECMWF) and has recently been published in the journal "Nature". In this paper, we evaluate the compatibility of the Pangu-Weather model with several commonly used NWP operational analyses through case studies. The results indicate that the Pangu-Weather model is compatible with different operational analyses from various NWP systems as the model initial conditions, and it exhibits a relatively stable forecasting capability. Furthermore, we have verified that improving the quality of global or local initial conditions significantly contributes to enhancing the forecasting performance of the Pangu-Weather model.
</details>
<details>
<summary>摘要</summary>
Here's the Simplified Chinese translation:近期，基于机器学习的多种气象预报模型已经出现，与传统的数值天气预测系统（NWP）相比，它们的准确性非常高。特别是Pangu-Weather模型，该模型为非商业用途开源，在ECMWF（欧洲中期天气预测中心）的验证下，最近发表在《自然》杂志上。在这篇论文中，我们通过实例研究了Pangu-Weather模型与各种常用的NWP操作分析相容性。结果表明，Pangu-Weather模型可以与不同的NWP系统的操作分析相容，并且在初始条件质量提高后，预报性能有所提高。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-the-switching-operation-in-monoclonal-antibody-production-Economic-MPC-and-reinforcement-learning"><a href="#Optimizing-the-switching-operation-in-monoclonal-antibody-production-Economic-MPC-and-reinforcement-learning" class="headerlink" title="Optimizing the switching operation in monoclonal antibody production: Economic MPC and reinforcement learning"></a>Optimizing the switching operation in monoclonal antibody production: Economic MPC and reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03928">http://arxiv.org/abs/2308.03928</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sandra A. Obiri, Song Bo, Bernard T. Agyeman, Benjamin Decardi-Nelson, Jinfeng Liu</li>
<li>for: 这paper的目的是提出了一种可靠的、高效的continuous manufacturing process для大规模生产monoclonal antibodies (mAbs)。</li>
<li>methods: 这paper使用了经济模控算法（EMPC）和深度强化学习（DRL）来优化连续生产过程中的换 columns操作。</li>
<li>results: 这paper的实验结果表明，使用sigmoid function approximation approach和ReLU approximation approach可以提高连续生产过程的效率和质量，而传统的 switching approach based on 1% product breakthrough rule则不能达到这样的效果。<details>
<summary>Abstract</summary>
Monoclonal antibodies (mAbs) have emerged as indispensable assets in medicine, and are currently at the forefront of biopharmaceutical product development. However, the growing market demand and the substantial doses required for mAb clinical treatments necessitate significant progress in its large-scale production. Most of the processes for industrial mAb production rely on batch operations, which result in significant downtime. The shift towards a fully continuous and integrated manufacturing process holds the potential to boost product yield and quality, while eliminating the extra expenses associated with storing intermediate products. The integrated continuous mAb production process can be divided into the upstream and downstream processes. One crucial aspect that ensures the continuity of the integrated process is the switching of the capture columns, which are typically chromatography columns operated in a fed-batch manner downstream. Due to the discrete nature of the switching operation, advanced process control algorithms such as economic MPC (EMPC) are computationally difficult to implement. This is because an integer nonlinear program (INLP) needs to be solved online at each sampling time. This paper introduces two computationally-efficient approaches for EMPC implementation, namely, a sigmoid function approximation approach and a rectified linear unit (ReLU) approximation approach. It also explores the application of deep reinforcement learning (DRL). These three methods are compared to the traditional switching approach which is based on a 1% product breakthrough rule and which involves no optimization.
</details>
<details>
<summary>摘要</summary>
单核球抗体（mAb）在医疗中已经成为不可或缺的资产，目前在生物制药产品开发中扮演了前列的角色。然而，增长的市场需求和大量的药品临床使用需要大幅提高大规模生产的效率。现有的大规模生产过程大多数采用批量操作，它们导致了很大的下时间。将生产过程推向完全连续和整合的制程可以提高产品的收益和质量，同时消除储存中间产品的额外成本。生产过程中的连续过程可以分为上游和下游过程。一个重要的确保连续过程的因素是捕捉Column的转换，它们通常是在下游过程中运行的牛顿批量方式。由于这个离散的转换操作，高级的调控算法如经济多项式控制（EMPC）在computationally具有问题。这是因为在每个样本时间点上需要解决一个数学问题。这篇文章介绍了三种 computationally-efficient的EMPC实现方法，namely，σ函数近似方法和Rectified Linear Unit（ReLU）近似方法。它还探讨了深度征兆学习（DRL）的应用。这三种方法与传统的转换方法，基于1%产品破坏规则，并不含优化。
</details></li>
</ul>
<hr>
<h2 id="Spellburst-A-Node-based-Interface-for-Exploratory-Creative-Coding-with-Natural-Language-Prompts"><a href="#Spellburst-A-Node-based-Interface-for-Exploratory-Creative-Coding-with-Natural-Language-Prompts" class="headerlink" title="Spellburst: A Node-based Interface for Exploratory Creative Coding with Natural Language Prompts"></a>Spellburst: A Node-based Interface for Exploratory Creative Coding with Natural Language Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03921">http://arxiv.org/abs/2308.03921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tyler Angert, Miroslav Ivan Suzara, Jenny Han, Christopher Lawrence Pondoc, Hariharan Subramonyam</li>
<li>for: This paper aims to enhance creative coding practices and inform the design of computational creativity tools that bridge semantic and syntactic spaces.</li>
<li>methods: The paper introduces Spellburst, a large language model (LLM) powered creative-coding environment that provides a node-based interface, expressive prompt-based interactions, and dynamic prompt-driven interfaces and direct code editing.</li>
<li>results: The paper evaluates Spellburst with artists and demonstrates its potential to enhance creative coding practices and inform the design of computational creativity tools.<details>
<summary>Abstract</summary>
Creative coding tasks are often exploratory in nature. When producing digital artwork, artists usually begin with a high-level semantic construct such as a "stained glass filter" and programmatically implement it by varying code parameters such as shape, color, lines, and opacity to produce visually appealing results. Based on interviews with artists, it can be effortful to translate semantic constructs to program syntax, and current programming tools don't lend well to rapid creative exploration. To address these challenges, we introduce Spellburst, a large language model (LLM) powered creative-coding environment. Spellburst provides (1) a node-based interface that allows artists to create generative art and explore variations through branching and merging operations, (2) expressive prompt-based interactions to engage in semantic programming, and (3) dynamic prompt-driven interfaces and direct code editing to seamlessly switch between semantic and syntactic exploration. Our evaluation with artists demonstrates Spellburst's potential to enhance creative coding practices and inform the design of computational creativity tools that bridge semantic and syntactic spaces.
</details>
<details>
<summary>摘要</summary>
创造性编程任务经常具有探索性质。当生成数字艺术作品时，艺术家通常从高水平semantic construct开始，如镜面纹理滤波器，然后通过代码参数的变化，如形状、颜色、线条和透明度，来生成visually appealing的结果。根据艺术家的采访，将semantic constructs翻译成编程语法可能会困难，现有的编程工具也不适合快速的创造性探索。为解决这些挑战，我们介绍Spellburst，一个基于大语言模型（LLM）的创造性编程环境。Spellburst提供以下特性：1. 节点基本的界面，允许艺术家通过分支和合并操作来生成生成艺术作品并探索不同的变化。2. 表达式基本的交互方式，让艺术家通过自然语言提示来参与semantic programming。3. dinamic prompt-driven界面和直接代码编辑，允许艺术家轻松地在semantic和syntactic空间之间转换。我们的评估表明，Spellburst可以提高创造性编程实践，并为计算创ativity工具的设计提供指导。
</details></li>
</ul>
<hr>
<h2 id="Predicting-and-explaining-nonlinear-material-response-using-deep-Physically-Guided-Neural-Networks-with-Internal-Variables"><a href="#Predicting-and-explaining-nonlinear-material-response-using-deep-Physically-Guided-Neural-Networks-with-Internal-Variables" class="headerlink" title="Predicting and explaining nonlinear material response using deep Physically Guided Neural Networks with Internal Variables"></a>Predicting and explaining nonlinear material response using deep Physically Guided Neural Networks with Internal Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03915">http://arxiv.org/abs/2308.03915</a></li>
<li>repo_url: None</li>
<li>paper_authors: Javier Orera-Echeverria, Jacobo Ayensa-Jiménez, Manuel Doblare</li>
<li>for: 这篇论文是为了探讨非线性材料模型化的问题，尤其是使用 físicamente guided neural networks with internal variables (PGNNIV) 方法来找出 constitutive laws。</li>
<li>methods: 这篇论文使用了PGNNIV方法，这是一种基于物理定义的神经网络方法，通过对压缩数据进行训练，可以预测 external 和 internal 变量，无需内部变量数据。</li>
<li>results: 研究发现，PGNNIV 方法可以预测不同类型材料（线性、硬化、软化）的 external 和 internal 变量，并且可以解释材料的 constitutive law，属于 Explainable Artificial Intelligence (XAI) 领域。<details>
<summary>Abstract</summary>
Nonlinear materials are often difficult to model with classical state model theory because they have a complex and sometimes inaccurate physical and mathematical description or we simply do not know how to describe such materials in terms of relations between external and internal variables. In many disciplines, Neural Network methods have arisen as powerful tools to identify very complex and non-linear correlations. In this work, we use the very recently developed concept of Physically Guided Neural Networks with Internal Variables (PGNNIV) to discover constitutive laws using a model-free approach and training solely with measured force-displacement data. PGNNIVs make a particular use of the physics of the problem to enforce constraints on specific hidden layers and are able to make predictions without internal variable data. We demonstrate that PGNNIVs are capable of predicting both internal and external variables under unseen load scenarios, regardless of the nature of the material considered (linear, with hardening or softening behavior and hyperelastic), unravelling the constitutive law of the material hence explaining its nature altogether, placing the method in what is known as eXplainable Artificial Intelligence (XAI).
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ViLP-Knowledge-Exploration-using-Vision-Language-and-Pose-Embeddings-for-Video-Action-Recognition"><a href="#ViLP-Knowledge-Exploration-using-Vision-Language-and-Pose-Embeddings-for-Video-Action-Recognition" class="headerlink" title="ViLP: Knowledge Exploration using Vision, Language, and Pose Embeddings for Video Action Recognition"></a>ViLP: Knowledge Exploration using Vision, Language, and Pose Embeddings for Video Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03908">http://arxiv.org/abs/2308.03908</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soumyabrata Chaudhuri, Saumik Bhattacharya</li>
<li>for: 这个论文旨在提出一种基于多modal学习的人体动作识别方法，以提高人体动作识别的准确率。</li>
<li>methods: 该方法使用了视觉信息（RGB模式）和人体姿态信息（2D skeleton或pose模式），并将这两种模式与文本特征相结合，以提高人体动作识别的精度。</li>
<li>results: 该方法在两个人体动作识别 benchmark dataset（UCF-101和HMDB-51）上达到了92.81%和73.02%的准确率，而无需视频数据预训练，并且 после kinetics 预训练，准确率提高至96.11%和75.75%。<details>
<summary>Abstract</summary>
Video Action Recognition (VAR) is a challenging task due to its inherent complexities. Though different approaches have been explored in the literature, designing a unified framework to recognize a large number of human actions is still a challenging problem. Recently, Multi-Modal Learning (MML) has demonstrated promising results in this domain. In literature, 2D skeleton or pose modality has often been used for this task, either independently or in conjunction with the visual information (RGB modality) present in videos. However, the combination of pose, visual information, and text attributes has not been explored yet, though text and pose attributes independently have been proven to be effective in numerous computer vision tasks. In this paper, we present the first pose augmented Vision-language model (VLM) for VAR. Notably, our scheme achieves an accuracy of 92.81% and 73.02% on two popular human video action recognition benchmark datasets, UCF-101 and HMDB-51, respectively, even without any video data pre-training, and an accuracy of 96.11% and 75.75% after kinetics pre-training.
</details>
<details>
<summary>摘要</summary>
视频动作识别（VAR）是一项复杂的任务，具有内在的复杂性。不同的方法在文献中已经被探讨，但设计一个统一的框架来识别大量人类动作仍然是一个挑战。近年来，多模态学习（MML）已经在这个领域展现出了有前途的成绩。在文献中，2D骨骼或姿势特征经常用于这项任务，可以独立或与视觉信息（RGB模式）一起使用。然而，将姿势、视觉信息和文本特征结合使用还没有被探讨，尽管姿势特征和文本特征独立地已经在计算机视觉任务中证明有效。在本文中，我们首次提出了一种姿势增强的视觉语言模型（VLM），并实现了92.81%和73.02%的准确率在两个流行的人类视频动作识别标准 dataset（UCF-101和HMDB-51）中，而无需任何视频数据预训练，并且在预训练后达到96.11%和75.75%的准确率。
</details></li>
</ul>
<hr>
<h2 id="Advancements-In-Crowd-Monitoring-System-A-Comprehensive-Analysis-of-Systematic-Approaches-and-Automation-Algorithms-State-of-The-Art"><a href="#Advancements-In-Crowd-Monitoring-System-A-Comprehensive-Analysis-of-Systematic-Approaches-and-Automation-Algorithms-State-of-The-Art" class="headerlink" title="Advancements In Crowd-Monitoring System: A Comprehensive Analysis of Systematic Approaches and Automation Algorithms: State-of-The-Art"></a>Advancements In Crowd-Monitoring System: A Comprehensive Analysis of Systematic Approaches and Automation Algorithms: State-of-The-Art</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03907">http://arxiv.org/abs/2308.03907</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammed Ameen, Richard Stone</li>
<li>for: 这篇论文主要关注于如何提供可靠和安全的人群监测系统，以满足现在全球各地政府和安全机构对公共安全的担忧。</li>
<li>methods: 这篇论文将研究两种方法：视觉基础的技术和非视觉基础的技术，并进行深入的分析，以了解它们在不同的环境和时间上的效果。</li>
<li>results: 这篇论文将强调现代应用程序和人工智能算法在自动化系统中的应用，以及它们在不同情况下的效果。<details>
<summary>Abstract</summary>
Growing apprehensions surrounding public safety have captured the attention of numerous governments and security agencies across the globe. These entities are increasingly acknowledging the imperative need for reliable and secure crowd-monitoring systems to address these concerns. Effectively managing human gatherings necessitates proactive measures to prevent unforeseen events or complications, ensuring a safe and well-coordinated environment. The scarcity of research focusing on crowd monitoring systems and their security implications has given rise to a burgeoning area of investigation, exploring potential approaches to safeguard human congregations effectively. Crowd monitoring systems depend on a bifurcated approach, encompassing vision-based and non-vision-based technologies. An in-depth analysis of these two methodologies will be conducted in this research. The efficacy of these approaches is contingent upon the specific environment and temporal context in which they are deployed, as they each offer distinct advantages. This paper endeavors to present an in-depth analysis of the recent incorporation of artificial intelligence (AI) algorithms and models into automated systems, emphasizing their contemporary applications and effectiveness in various contexts.
</details>
<details>
<summary>摘要</summary>
全球各地政府和安全机构都有增长的担忧，即使是公众安全的问题。这些机构认为，有效地管理人群聚集需要采取积极措施，以防止意外事件或复杂的情况，确保安全和有效的环境。由于关于人群监测系统的研究不够，这个领域在过去几年中得到了快速发展。人群监测系统通常采用分两部分的方法：视觉基本的和非视觉基本的技术。本研究将进行深入的分析这两种方法，并评估它们在不同的环境和时间上的效果。此外，本文还将评估人群监测系统中的人工智能（AI）算法和模型的应用，包括它们在不同情况下的当代应用和效果。
</details></li>
</ul>
<hr>
<h2 id="Intelligent-Assistant-Language-Understanding-On-Device"><a href="#Intelligent-Assistant-Language-Understanding-On-Device" class="headerlink" title="Intelligent Assistant Language Understanding On Device"></a>Intelligent Assistant Language Understanding On Device</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03905">http://arxiv.org/abs/2308.03905</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Cecilia Aas, Hisham Abdelsalam, Irina Belousova, Shruti Bhargava, Jianpeng Cheng, Robert Daland, Joris Driesen, Federico Flego, Tristan Guigue, Anders Johannsen, Partha Lal, Jiarui Lu, Joel Ruben Antony Moniz, Nathan Perkins, Dhivya Piraviperumal, Stephen Pulman, Diarmuid Ó Séaghdha, David Q. Sun, John Torr, Marco Del Vecchio, Jay Wacker, Jason D. Williams, Hong Yu</li>
<li>for: 这篇论文目标是描述一种运行于个人设备上的自然语言理解系统的设计。</li>
<li>methods: 该系统采用了一些选择的建筑和技术，例如在对话系统文献中一些方法可能在部署环境中困难维护。</li>
<li>results: 该系统比服务器端助手更加私钥、可靠、快速、表达力强、准确。<details>
<summary>Abstract</summary>
It has recently become feasible to run personal digital assistants on phones and other personal devices. In this paper we describe a design for a natural language understanding system that runs on device. In comparison to a server-based assistant, this system is more private, more reliable, faster, more expressive, and more accurate. We describe what led to key choices about architecture and technologies. For example, some approaches in the dialog systems literature are difficult to maintain over time in a deployment setting. We hope that sharing learnings from our practical experiences may help inform future work in the research community.
</details>
<details>
<summary>摘要</summary>
现在可以在手机和其他个人设备上运行个人数字助手。在这篇论文中，我们描述了一种运行于设备上的自然语言理解系统的设计。相比服务器基于的助手，这种系统更加私钥、可靠、快速、表达力 stronger和更准确。我们介绍了一些关键的架构和技术选择的原因。例如，一些对话系统文献中的方法在部署环境中具有困难维护的特点。我们希望通过分享我们的实践经验，可以对未来的研究工作产生影响。
</details></li>
</ul>
<hr>
<h2 id="On-genuine-invariance-learning-without-weight-tying"><a href="#On-genuine-invariance-learning-without-weight-tying" class="headerlink" title="On genuine invariance learning without weight-tying"></a>On genuine invariance learning without weight-tying</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03904">http://arxiv.org/abs/2308.03904</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amoskalev/ginvariance">https://github.com/amoskalev/ginvariance</a></li>
<li>paper_authors: Artem Moskalev, Anna Sepliarskaia, Erik J. Bekkers, Arnold Smeulders</li>
<li>for: 本研究探讨神经网络学习的不变性和其限制，并与固定权重绑定的真正不变性进行比较。</li>
<li>methods: 作者采用群理论的视角分析神经网络的不变性学习，并未受权重绑定的限制。他们发现，即使神经网络能够正确地分类样本，但是下面的决策不具有真正的不变性。</li>
<li>results: 作者提出了几种度量学习不变性的指标，包括预测分布不变性、对数不变性和响应不变性相似性。他们发现，通过在训练过程中加权error regularization来引导学习不变性，可以使得神经网络学习的不变性与固定权重绑定模型的不变性相似。<details>
<summary>Abstract</summary>
In this paper, we investigate properties and limitations of invariance learned by neural networks from the data compared to the genuine invariance achieved through invariant weight-tying. To do so, we adopt a group theoretical perspective and analyze invariance learning in neural networks without weight-tying constraints. We demonstrate that even when a network learns to correctly classify samples on a group orbit, the underlying decision-making in such a model does not attain genuine invariance. Instead, learned invariance is strongly conditioned on the input data, rendering it unreliable if the input distribution shifts. We next demonstrate how to guide invariance learning toward genuine invariance by regularizing the invariance of a model at the training. To this end, we propose several metrics to quantify learned invariance: (i) predictive distribution invariance, (ii) logit invariance, and (iii) saliency invariance similarity. We show that the invariance learned with the invariance error regularization closely reassembles the genuine invariance of weight-tying models and reliably holds even under a severe input distribution shift. Closer analysis of the learned invariance also reveals the spectral decay phenomenon, when a network chooses to achieve the invariance to a specific transformation group by reducing the sensitivity to any input perturbation.
</details>
<details>
<summary>摘要</summary>
本文研究神经网络学习的不变性和其限制，并与真正的不变性相比较。为此，我们采用群理论的视角，分析神经网络没有重量约束时的不变性学习。我们发现，即使神经网络能正确地分类样本，其下面的决策过程并不具有真正的不变性。相反，学习的不变性受到输入数据的强烈条件，因此在输入分布变化时变得不可靠。我们后来提出了一些度量学习的不变性的指标，包括预测分布不变性、Logit不变性和吸引力不变性相似性。我们发现，通过不变性错误规范来帮助学习不变性可以很好地寄存真正的不变性，并且可以在输入分布变化时保持稳定。进一步分析学习的不变性还发现，神经网络会选择通过减少输入变化的敏感度来实现不变性。
</details></li>
</ul>
<hr>
<h2 id="FLIPS-Federated-Learning-using-Intelligent-Participant-Selection"><a href="#FLIPS-Federated-Learning-using-Intelligent-Participant-Selection" class="headerlink" title="FLIPS: Federated Learning using Intelligent Participant Selection"></a>FLIPS: Federated Learning using Intelligent Participant Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03901">http://arxiv.org/abs/2308.03901</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rahul Atul Bhope, K. R. Jayaram, Nalini Venkatasubramanian, Ashish Verma, Gegi Thomas</li>
<li>for: 这篇论文旨在设计和实现一个基于中间件的聚合系统，用于管理数据和参与者多样性在联合学习训练任务中。具体来说，文章研究了在联合学习训练中使用标签分布划分的参与者选择方法的效果。</li>
<li>methods: 文章使用了标签分布划分来 clustering 参与者，并在联合学习训练过程中确保每个群组都具有相对的代表性。文章还 incorporates 一种异常管理机制来处理分布式环境中的资源变化。</li>
<li>results: 文章的实验比较了 FLIPS 与随机选择、Oort 和梯度划分等三种 “聪明” 选择方法，并在两个真实世界数据集和三种常见的联合学习算法（FedYogi、FedProx 和 FedAvg）上进行了广泛的 empirical evaluation。结果表明，FLIPS 可以提高收敛率，并在具有延迟参与者的情况下保持高度的准确率，这些优点在不同的分布式环境和资源条件下也能够持续。<details>
<summary>Abstract</summary>
This paper presents the design and implementation of FLIPS, a middleware system to manage data and participant heterogeneity in federated learning (FL) training workloads. In particular, we examine the benefits of label distribution clustering on participant selection in federated learning. FLIPS clusters parties involved in an FL training job based on the label distribution of their data apriori, and during FL training, ensures that each cluster is equitably represented in the participants selected. FLIPS can support the most common FL algorithms, including FedAvg, FedProx, FedDyn, FedOpt and FedYogi. To manage platform heterogeneity and dynamic resource availability, FLIPS incorporates a straggler management mechanism to handle changing capacities in distributed, smart community applications. Privacy of label distributions, clustering and participant selection is ensured through a trusted execution environment (TEE). Our comprehensive empirical evaluation compares FLIPS with random participant selection, as well as two other "smart" selection mechanisms - Oort and gradient clustering using two real-world datasets, two different non-IID distributions and three common FL algorithms (FedYogi, FedProx and FedAvg). We demonstrate that FLIPS significantly improves convergence, achieving higher accuracy by 17 - 20 % with 20 - 60 % lower communication costs, and these benefits endure in the presence of straggler participants.
</details>
<details>
<summary>摘要</summary>
Empirical evaluation compares FLIPS with random participant selection, as well as two other "smart" selection mechanisms - Oort and gradient clustering. The results show that FLIPS significantly improves convergence, achieving higher accuracy by 17-20% with 20-60% lower communication costs. These benefits persist even in the presence of straggler participants. The paper uses two real-world datasets, two different non-IID distributions, and three common FL algorithms (FedYogi, FedProx, and FedAvg) to demonstrate the effectiveness of FLIPS.
</details></li>
</ul>
<hr>
<h2 id="Scalable-and-Equitable-Math-Problem-Solving-Strategy-Prediction-in-Big-Educational-Data"><a href="#Scalable-and-Equitable-Math-Problem-Solving-Strategy-Prediction-in-Big-Educational-Data" class="headerlink" title="Scalable and Equitable Math Problem Solving Strategy Prediction in Big Educational Data"></a>Scalable and Equitable Math Problem Solving Strategy Prediction in Big Educational Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03892">http://arxiv.org/abs/2308.03892</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anupshakya07/attn-scaling">https://github.com/anupshakya07/attn-scaling</a></li>
<li>paper_authors: Anup Shakya, Vasile Rus, Deepak Venugopal</li>
<li>for: 这项研究的目的是提高学生数学学习的效果，使用智能教育系统（ITS）和适应教学系统（AIS）。</li>
<li>methods: 该研究使用机器学习和人工智能技术，开发了一种名为MVec的嵌入，以学习学生的掌握度表示。然后使用非 Parametric 聚类方法，将这些嵌入分成不同的群集。最后，使用Transformers和Node2Vec进行学习mastery embedding，并使用LSTM进行策略预测。</li>
<li>results: 该研究可以在大规模学生互动数据集上实现高精度的策略预测，并且具有预测平等性，即可以平等地预测学生的策略水平。<details>
<summary>Abstract</summary>
Understanding a student's problem-solving strategy can have a significant impact on effective math learning using Intelligent Tutoring Systems (ITSs) and Adaptive Instructional Systems (AISs). For instance, the ITS/AIS can better personalize itself to correct specific misconceptions that are indicated by incorrect strategies, specific problems can be designed to improve strategies and frustration can be minimized by adapting to a student's natural way of thinking rather than trying to fit a standard strategy for all. While it may be possible for human experts to identify strategies manually in classroom settings with sufficient student interaction, it is not possible to scale this up to big data. Therefore, we leverage advances in Machine Learning and AI methods to perform scalable strategy prediction that is also fair to students at all skill levels. Specifically, we develop an embedding called MVec where we learn a representation based on the mastery of students. We then cluster these embeddings with a non-parametric clustering method where we progressively learn clusters such that we group together instances that have approximately symmetrical strategies. The strategy prediction model is trained on instances sampled from these clusters. This ensures that we train the model over diverse strategies and also that strategies from a particular group do not bias the DNN model, thus allowing it to optimize its parameters over all groups. Using real world large-scale student interaction datasets from MATHia, we implement our approach using transformers and Node2Vec for learning the mastery embeddings and LSTMs for predicting strategies. We show that our approach can scale up to achieve high accuracy by training on a small sample of a large dataset and also has predictive equality, i.e., it can predict strategies equally well for learners at diverse skill levels.
</details>
<details>
<summary>摘要</summary>
理解学生的问题解决策略可以对智能教学系统（ITS）和适应教学系统（AIS）的有效学习产生重要影响。例如，ITS/AIS可以更好地个性化自己，根据学生提交的错误策略来更正特定的误解，设计特定问题以提高策略，并降低学生的沮丧情绪，而不是强制学生遵循标准策略。虽然在教室 SETTINGS 中，人工专家可能可以手动确定策略，但不可能扩大到大数据。因此，我们利用机器学习和人工智能技术进行可扩展的策略预测，并保证该方法对所有技能水平的学生是公平的。我们开发了一个名为 MVec 的嵌入，其中我们学习基于学生的尝试情况的表示。然后，我们使用非Parametric 分 clustering方法，将这些嵌入进行分组，以便将相似策略的实例相互分组。我们的策略预测模型是基于这些分组的实例进行训练的。这种方法可以扩大到达到高准确率，并且有Predictive Equality 性，即它可以平等地预测学生的策略，无论他们的技能水平如何。使用实际世界的大规模学生互动数据集，我们使用 transformers 和 Node2Vec 来学习尝试情况的尝试情况，并使用 LSTM 来预测策略。我们的方法可以扩大到达到高准确率，并且有Predictive Equality 性。
</details></li>
</ul>
<hr>
<h2 id="Generative-Benchmark-Creation-for-Table-Union-Search"><a href="#Generative-Benchmark-Creation-for-Table-Union-Search" class="headerlink" title="Generative Benchmark Creation for Table Union Search"></a>Generative Benchmark Creation for Table Union Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03883">http://arxiv.org/abs/2308.03883</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/northeastern-datalab/alt-gen">https://github.com/northeastern-datalab/alt-gen</a></li>
<li>paper_authors: Koyena Pal, Aamod Khatiwada, Roee Shraga, Renée J. Miller<br>for: tables union search benchmarkmethods: generative AI models large language modelsresults: new benchmark more challenging for existing methods top-performing method achieves lower Mean Average Precision on new benchmark compared to existing manually created benchmarks detailed analysis of methods possible with new benchmark<details>
<summary>Abstract</summary>
Data management has traditionally relied on synthetic data generators to generate structured benchmarks, like the TPC suite, where we can control important parameters like data size and its distribution precisely. These benchmarks were central to the success and adoption of database management systems. But more and more, data management problems are of a semantic nature. An important example is finding tables that can be unioned. While any two tables with the same cardinality can be unioned, table union search is the problem of finding tables whose union is semantically coherent. Semantic problems cannot be benchmarked using synthetic data. Our current methods for creating benchmarks involve the manual curation and labeling of real data. These methods are not robust or scalable and perhaps more importantly, it is not clear how robust the created benchmarks are. We propose to use generative AI models to create structured data benchmarks for table union search. We present a novel method for using generative models to create tables with specified properties. Using this method, we create a new benchmark containing pairs of tables that are both unionable and non-unionable but related. We thoroughly evaluate recent existing table union search methods over existing benchmarks and our new benchmark. We also present and evaluate a new table search methods based on recent large language models over all benchmarks. We show that the new benchmark is more challenging for all methods than hand-curated benchmarks, specifically, the top-performing method achieves a Mean Average Precision of around 60%, over 30% less than its performance on existing manually created benchmarks. We examine why this is the case and show that the new benchmark permits more detailed analysis of methods, including a study of both false positives and false negatives that were not possible with existing benchmarks.
</details>
<details>
<summary>摘要</summary>
传统上，数据管理受到人工生成的数据生成器的限制，如TPC集成，以控制数据大小和分布的重要参数。这些标准 benchmark 对数据库管理系统的采用和普及做出了重要贡献。然而，随着数据管理问题的 semanticization，人工生成的数据不能满足需求。我们的当前方法是通过手动筛选和标注实际数据来创建 benchmark。这些方法不具有可靠性和扩展性，而且无法确定创建的标准 benchmark 的可靠性。我们提议使用生成 AI 模型创建结构化数据 benchmark。我们介绍了一种使用生成模型创建表 avec 指定属性的方法。使用这种方法，我们创建了一个新的标准 benchmark，其中包含可 union 和不可 union 的表对。我们进行了现有benchmark和我们新创建的benchmark上现有方法的严格评估。我们还介绍了基于最新大语言模型的新表搜索方法，并对所有benchmark进行评估。我们发现新benchmark比手动创建的benchmark更加具有挑战性，特别是最高级performing方法在新benchmark上的 Mean Average Precision 约为 60%，相比手动创建 benchmark 上的表现下降了30%。我们分析了这种情况，并证明新benchmark允许更加细致的方法分析，包括对方法的false positives和false negatives进行研究，这些研究不可能通过现有benchmark进行。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Generalization-in-Offline-Reinforcement-Learning-via-Unseen-State-Augmentations"><a href="#Exploiting-Generalization-in-Offline-Reinforcement-Learning-via-Unseen-State-Augmentations" class="headerlink" title="Exploiting Generalization in Offline Reinforcement Learning via Unseen State Augmentations"></a>Exploiting Generalization in Offline Reinforcement Learning via Unseen State Augmentations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03882">http://arxiv.org/abs/2308.03882</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nirbhay Modhe, Qiaozi Gao, Ashwin Kalyan, Dhruv Batra, Govind Thattai, Gaurav Sukhatme</li>
<li>for: 这篇论文是关于线上强化学习（RL）方法的研究，它们可以寻找未经见过的状态和动作。</li>
<li>methods: 这些方法使用保守的价值估计，对未经见过的状态和动作进行惩罚，以保证在探索和利用之间寻找平衡。</li>
<li>results: 研究人员通过提出一种新的未经见过状态扩展策略，使得RL方法能够更好地找到未经见过的状态，并且可以更好地适应不同的任务。此外，他们还发现，使用这种扩展策略可以降低平均数据集Q估计的值，即更保守的估计。<details>
<summary>Abstract</summary>
Offline reinforcement learning (RL) methods strike a balance between exploration and exploitation by conservative value estimation -- penalizing values of unseen states and actions. Model-free methods penalize values at all unseen actions, while model-based methods are able to further exploit unseen states via model rollouts. However, such methods are handicapped in their ability to find unseen states far away from the available offline data due to two factors -- (a) very short rollout horizons in models due to cascading model errors, and (b) model rollouts originating solely from states observed in offline data. We relax the second assumption and present a novel unseen state augmentation strategy to allow exploitation of unseen states where the learned model and value estimates generalize. Our strategy finds unseen states by value-informed perturbations of seen states followed by filtering out states with epistemic uncertainty estimates too high (high error) or too low (too similar to seen data). We observe improved performance in several offline RL tasks and find that our augmentation strategy consistently leads to overall lower average dataset Q-value estimates i.e. more conservative Q-value estimates than a baseline.
</details>
<details>
<summary>摘要</summary>
在线RL方法寻求 между探索和占用的平衡，通过保守的价值估计---对未见的状态和动作进行惩罚。无模型方法对所有未见动作进行惩罚，而模型基于方法可以通过模型执行来进一步利用未见状态。然而，这些方法在找到未见状态的远程处理方面受到两个因素的限制：1. 在模型中的很短执行 horizon，由于链式模型错误而导致。2. 模型执行仅从看到的Offline数据中的状态开始。我们relax这个第二个假设，并提出一种新的未见状态扩充策略，以便在已学习的模型和价值估计中进行扩充。我们的策略通过值指导的扰动seen状态，然后过滤出epistemicuncertainty度量过高（高错误）或过低（太相似于seen数据）的状态。我们发现在多个OfflineRL任务中表现出色，并发现我们的扩充策略通常比基线靠前。我们的augmentation策略通过在seen状态中进行值指导的扰动，以及过滤出高错误或太相似于seen数据的状态来实现。这使得我们的价值估计更加保守，并且在多个OfflineRL任务中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-and-Explaining-Large-Language-Models-for-Code-Using-Syntactic-Structures"><a href="#Evaluating-and-Explaining-Large-Language-Models-for-Code-Using-Syntactic-Structures" class="headerlink" title="Evaluating and Explaining Large Language Models for Code Using Syntactic Structures"></a>Evaluating and Explaining Large Language Models for Code Using Syntactic Structures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03873">http://arxiv.org/abs/2308.03873</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wm-semeru/codesyntaxconcept">https://github.com/wm-semeru/codesyntaxconcept</a></li>
<li>paper_authors: David N Palacio, Alejandro Velasco, Daniel Rodriguez-Cardenas, Kevin Moran, Denys Poshyvanyk</li>
<li>for: This paper aims to provide a new method for explaining the predictions of large language models (LLMs) for code, called ASTxplainer, which can be used to evaluate the effectiveness of these models and help end-users understand their predictions.</li>
<li>methods: The paper proposes a novel approach called ASTxplainer, which aligns token predictions with abstract syntax trees (ASTs) to provide a fine-grained understanding of LLM predictions. The method extracts and aggregates normalized model logits within AST structures to provide insights into model behavior.</li>
<li>results: The paper presents the results of an empirical evaluation on 12 popular LLMs for code using a curated dataset of the most popular GitHub projects. The results show that ASTxplainer can provide valuable insights into LLM effectiveness and aid end-users in understanding predictions. Additionally, a user study found that the visualization of model predictions provided by ASTxplainer was useful for enabling end-users to explain predictions.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) for code are a family of high-parameter, transformer-based neural networks pre-trained on massive datasets of both natural and programming languages. These models are rapidly being employed in commercial AI-based developer tools, such as GitHub CoPilot. However, measuring and explaining their effectiveness on programming tasks is a challenging proposition, given their size and complexity. The methods for evaluating and explaining LLMs for code are inextricably linked. That is, in order to explain a model's predictions, they must be reliably mapped to fine-grained, understandable concepts. Once this mapping is achieved, new methods for detailed model evaluations are possible. However, most current explainability techniques and evaluation benchmarks focus on model robustness or individual task performance, as opposed to interpreting model predictions.   To this end, this paper introduces ASTxplainer, an explainability method specific to LLMs for code that enables both new methods for LLM evaluation and visualizations of LLM predictions that aid end-users in understanding model predictions. At its core, ASTxplainer provides an automated method for aligning token predictions with AST nodes, by extracting and aggregating normalized model logits within AST structures. To demonstrate the practical benefit of ASTxplainer, we illustrate the insights that our framework can provide by performing an empirical evaluation on 12 popular LLMs for code using a curated dataset of the most popular GitHub projects. Additionally, we perform a user study examining the usefulness of an ASTxplainer-derived visualization of model predictions aimed at enabling model users to explain predictions. The results of these studies illustrate the potential for ASTxplainer to provide insights into LLM effectiveness, and aid end-users in understanding predictions.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM） для代码是一家高参数、转换器基于神经网络的家族，预训练在庞大的自然语言和编程语言数据集上。这些模型在商业AI基于开发者工具中使用，如GitHub CoPilot。然而，评估和解释LLMs的效果在编程任务上是一个复杂的问题，因为它们的大小和复杂性。LLMs的评估和解释方法紧密相关，即要可靠地将模型预测映射到细腻可理解的概念上。一旦这种映射实现，新的详细模型评估方法就可能实现。然而，现有的解释技术和评估标准主要关注模型稳定性或个别任务性能，而不是解释模型预测。为此，本文介绍了ASTxplainer，一种特有的解释方法，用于LLMs for code。ASTxplainer提供了一种自动将字符预测映射到AST结构中的方法，通过提取和聚合 норциали化模型强度的技术。为证明ASTxplainer的实用性，我们对12种流行的LLMs for code进行了一系列实验，并使用一个优选的GitHub项目数据集进行了一个详细的评估。此外，我们还进行了一次用户研究，以确定ASTxplainer derivated的可视化是否有用于解释模型预测。研究结果表明，ASTxplainer有可能为LLM效果提供新的视角，并帮助用户理解预测。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Equivalence-of-e-Commerce-Queries"><a href="#Semantic-Equivalence-of-e-Commerce-Queries" class="headerlink" title="Semantic Equivalence of e-Commerce Queries"></a>Semantic Equivalence of e-Commerce Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03869">http://arxiv.org/abs/2308.03869</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aritra Mandal, Daniel Tunkelang, Zhe Wu</li>
<li>for: 本研究旨在提高电商搜索中的搜索结果和商业效果，通过认可和利用查询Equivalence来解决搜索查询的问题。</li>
<li>methods: 本研究提出了一种框架，包括将查询映射到搜索意图的 вектор表示，并使用surface similarity和行为相似性来确定查询Equivalence。</li>
<li>results: 实验结果表明，提出的方法可以高效地认可和利用查询Equivalence，并在 Popular sentence transformer模型之上出performanced。结果还 highlights the potential of  leveraging历史行为数据和训练模型来提高电商搜索的用户体验和商业效果。<details>
<summary>Abstract</summary>
Search query variation poses a challenge in e-commerce search, as equivalent search intents can be expressed through different queries with surface-level differences. This paper introduces a framework to recognize and leverage query equivalence to enhance searcher and business outcomes. The proposed approach addresses three key problems: mapping queries to vector representations of search intent, identifying nearest neighbor queries expressing equivalent or similar intent, and optimizing for user or business objectives. The framework utilizes both surface similarity and behavioral similarity to determine query equivalence. Surface similarity involves canonicalizing queries based on word inflection, word order, compounding, and noise words. Behavioral similarity leverages historical search behavior to generate vector representations of query intent. An offline process is used to train a sentence similarity model, while an online nearest neighbor approach supports processing of unseen queries. Experimental evaluations demonstrate the effectiveness of the proposed approach, outperforming popular sentence transformer models and achieving a Pearson correlation of 0.85 for query similarity. The results highlight the potential of leveraging historical behavior data and training models to recognize and utilize query equivalence in e-commerce search, leading to improved user experiences and business outcomes. Further advancements and benchmark datasets are encouraged to facilitate the development of solutions for this critical problem in the e-commerce domain.
</details>
<details>
<summary>摘要</summary>
<SYS>    输入文本翻译为简化字符串。</SYS>搜索查询的变化 pose 电商搜索中的挑战，因为相同的搜索意图可以通过不同的查询语句表达。本文介绍了一种框架，用于认可和利用查询等价性，以提高搜索者和商业目标的结果。该框架解决了三个关键问题：将查询映射到搜索意图的 вектор表示，标识Equivalent或相似的搜索意图的查询，以及优化用户或商业目标。该框架利用了表面相似性和行为相似性来确定查询等价性。表面相似性包括根据词形变化、词序排序、复合词和噪音词进行 canonicalization。行为相似性利用历史搜索行为生成搜索意图的 вектор表示。在线上进行训练的offline进程用于培训句子相似性模型，而在线上的最近邻近approach支持处理未经看过的查询。实验证明了提案的方法的有效性，比 популяр的句子转换模型更好，并达到了0.85的Pearson相关系数 для查询相似性。结果表明，通过利用历史行为数据和训练模型，可以认可和利用查询等价性，从而提高用户体验和商业结果。进一步的进步和标准 datasets 鼓励开发者们在电商领域开发解决这个关键问题的解决方案。
</details></li>
</ul>
<hr>
<h2 id="AI-Text-to-Behavior-A-Study-In-Steerability"><a href="#AI-Text-to-Behavior-A-Study-In-Steerability" class="headerlink" title="AI Text-to-Behavior: A Study In Steerability"></a>AI Text-to-Behavior: A Study In Steerability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07326">http://arxiv.org/abs/2308.07326</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Noever, Sam Hyams</li>
<li>for: 这项研究探讨了大语言模型（LLM）的可控性，尤其是OpenAI的ChatGPT迭代。</li>
<li>methods: 我们使用了行为心理学框架OCEAN（开放性、聪明性、外向性、合作性、情绪性），量测模型对特定提示的回应。</li>
<li>results: 我们发现，“开放性”存在语言含义模糊，而“聪明性”和“情绪性”在OCEAN框架中明确表现出来，而“外向性”和“合作性”则显示出了明确的分化。这些结果表明GPT的多样性和适应能力，但也指出了LLM的快速进步和一些训练技术的权限性。<details>
<summary>Abstract</summary>
The research explores the steerability of Large Language Models (LLMs), particularly OpenAI's ChatGPT iterations. By employing a behavioral psychology framework called OCEAN (Openness, Conscientiousness, Extroversion, Agreeableness, Neuroticism), we quantitatively gauged the model's responsiveness to tailored prompts. When asked to generate text mimicking an extroverted personality, OCEAN scored the language alignment to that behavioral trait. In our analysis, while "openness" presented linguistic ambiguity, "conscientiousness" and "neuroticism" were distinctly evoked in the OCEAN framework, with "extroversion" and "agreeableness" showcasing a notable overlap yet distinct separation from other traits. Our findings underscore GPT's versatility and ability to discern and adapt to nuanced instructions. Furthermore, historical figure simulations highlighted the LLM's capacity to internalize and project instructible personas, precisely replicating their philosophies and dialogic styles. However, the rapid advancements in LLM capabilities and the opaque nature of some training techniques make metric proposals degrade rapidly. Our research emphasizes a quantitative role to describe steerability in LLMs, presenting both its promise and areas for further refinement in aligning its progress to human intentions.
</details>
<details>
<summary>摘要</summary>
研究探讨大语言模型（LLM）的可控性，特别是OpenAI的ChatGPT迭代。通过使用行为心理学框架 called OCEAN（开放性、亲和力、EXTROVERSION、善于合作性、神经性），我们量化了模型对特定提示的应对性。当请求生成 simulate extroverted personality 的文本时，OCEAN 评分语言对该行为特征的Alignment。在我们的分析中，“开放性”表现出语言的ambiguity，而“谨慎性”和“神经性”在OCEAN框架中得到了明确的识别，而“EXTROVERSION”和“善于合作性”则显示了明显的重叠 yet distinct separation from other traits。我们的发现推动GPT的多样性和能力，并且可以根据人类的INTENTIONS来定制和适应。然而，LLM的快速进步和一些训练技术的不透明性使得度量建议迅速衰退。我们的研究强调了量化描述 LLM 的可控性的作用，并提出了该领域的进一步完善和人类INTENTIONS的Alignment。
</details></li>
</ul>
<hr>
<h2 id="MCTS-guided-Genetic-Algorithm-for-optimization-of-neural-network-weights"><a href="#MCTS-guided-Genetic-Algorithm-for-optimization-of-neural-network-weights" class="headerlink" title="MCTS guided Genetic Algorithm for optimization of neural network weights"></a>MCTS guided Genetic Algorithm for optimization of neural network weights</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04459">http://arxiv.org/abs/2308.04459</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AkshayHebbar/MCTS-GA">https://github.com/AkshayHebbar/MCTS-GA</a></li>
<li>paper_authors: Akshay Hebbar</li>
<li>for: 本研究探讨了如何使用搜寻策略应用于遗传算法，以搜寻整个遗传树结构中的优化解决方案。</li>
<li>methods: 本研究使用了许多搜寻策略，包括宽度优先、深度优先和迭代法，但这些方法通常需要大量计算时间。在搜寻过程中，我们采用了反对抗技术，以快速获得最优解。</li>
<li>results: 本研究结果表明，将遗传算法和蒙特卡洛tree搜寻策略结合使用，可以快速找到遗传算法优化解决方案。<details>
<summary>Abstract</summary>
In this research, we investigate the possibility of applying a search strategy to genetic algorithms to explore the entire genetic tree structure. Several methods aid in performing tree searches; however, simpler algorithms such as breadth-first, depth-first, and iterative techniques are computation-heavy and often result in a long execution time. Adversarial techniques are often the preferred mechanism when performing a probabilistic search, yielding optimal results more quickly. The problem we are trying to tackle in this paper is the optimization of neural networks using genetic algorithms. Genetic algorithms (GA) form a tree of possible states and provide a mechanism for rewards via the fitness function. Monte Carlo Tree Search (MCTS) has proven to be an effective tree search strategy given states and rewards; therefore, we will combine these approaches to optimally search for the best result generated with genetic algorithms.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们研究了将搜索策略应用于生物学算法，以探索整个遗传树结构。许多方法可以进行树搜索，但是简单的算法如广度优先、深度优先和迭代方法通常需要较长的计算时间。对于 probabilistic 搜索，反击技术通常是首选的机制，可以快速获得优化结果。我们在这篇论文中面临的问题是使用生物学算法优化神经网络。生物学算法形成一棵可能状态的树，并提供了回归函数来计算奖励。蒙特卡洛树搜索（MCTS）已经证明是在给定状态和奖励时效果的搜索策略，因此我们将这些方法结合使用，以优化使用生物学算法生成的最佳结果。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Prompt-Engineering-via-Declarative-Crowdsourcing"><a href="#Revisiting-Prompt-Engineering-via-Declarative-Crowdsourcing" class="headerlink" title="Revisiting Prompt Engineering via Declarative Crowdsourcing"></a>Revisiting Prompt Engineering via Declarative Crowdsourcing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03854">http://arxiv.org/abs/2308.03854</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya G. Parameswaran, Shreya Shankar, Parth Asawa, Naman Jain, Yujie Wang</li>
<li>for: 提高 LLM 数据处理工作流程的质量和效率，以及提供一种更原则化的描述引擎方法。</li>
<li>methods: 利用多种描述策略、保证内部一致性，以及混合 LLM 和非 LLM 方法来实现更原则化的描述引擎。</li>
<li>results: 在排序、实体解析和填充等应用中，采用该方法可以提高 LLM 的性能和可靠性，并且可以自动化和机器化描述引擎过程。<details>
<summary>Abstract</summary>
Large language models (LLMs) are incredibly powerful at comprehending and generating data in the form of text, but are brittle and error-prone. There has been an advent of toolkits and recipes centered around so-called prompt engineering-the process of asking an LLM to do something via a series of prompts. However, for LLM-powered data processing workflows, in particular, optimizing for quality, while keeping cost bounded, is a tedious, manual process. We put forth a vision for declarative prompt engineering. We view LLMs like crowd workers and leverage ideas from the declarative crowdsourcing literature-including leveraging multiple prompting strategies, ensuring internal consistency, and exploring hybrid-LLM-non-LLM approaches-to make prompt engineering a more principled process. Preliminary case studies on sorting, entity resolution, and imputation demonstrate the promise of our approach
</details>
<details>
<summary>摘要</summary>
巨型语言模型（LLM）拥有强大的文本理解和生成能力，但是容易受到影响和出错。随着召集工具和热门趋势的出现，有关提问工程（prompt engineering）的研究得到了更多的关注，即通过一系列提问请求LLM进行某种任务。然而，为LLM数据处理工作流程而优化质量，同时保持成本在可控范围内，是一个繁琐、手动的过程。我们提出了声明式提问工程的视野，将LLM视为众生力量，并借鉴了声明式人员召集文献中的一些想法，包括多种提问策略、保证内部一致性，以及混合LLM非LLM方法。这些案例研究显示了排序、实体匹配和填充等应用场景的搅麻潜力。
</details></li>
</ul>
<hr>
<h2 id="Search-Engine-and-Recommendation-System-for-the-Music-Industry-built-with-JinaAI"><a href="#Search-Engine-and-Recommendation-System-for-the-Music-Industry-built-with-JinaAI" class="headerlink" title="Search Engine and Recommendation System for the Music Industry built with JinaAI"></a>Search Engine and Recommendation System for the Music Industry built with JinaAI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03842">http://arxiv.org/abs/2308.03842</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ishita Gopalakrishnan, Sanjjushri Varshini R, Ponshriharini V</li>
<li>for: 这篇论文的目的是为了开发一个基于 Machine Learning 的音乐产业搜索引擎和推荐系统。</li>
<li>methods: 这篇论文使用了 Jina AI 框架，一个基于 MLOps 的搜索引擎架构，以提高搜索引擎的速度、精度和搜寻结果的质量。</li>
<li>results: 这篇论文的结果显示，使用 Jina AI 框架可以实现更好的搜寻结果和推荐系统，帮助用户更加方便地搜寻音乐。<details>
<summary>Abstract</summary>
One of the most intriguing debates regarding a novel task is the development of search engines and recommendation-based systems in the music industry. Studies have shown a drastic depression in the search engine fields, due to concerning factors such as speed, accuracy and the format of data given for querying. Often people face difficulty in searching for a song solely based on the title, hence a solution is proposed to complete a search analysis through a single query input and is matched with the lyrics of the songs present in the database. Hence it is essential to incorporate cutting-edge technology tools for developing a user-friendly search engine. Jina AI is an MLOps framework for building neural search engines that are utilized, in order for the user to obtain accurate results. Jina AI effectively helps to maintain and enhance the quality of performance for the search engine for the query given. An effective search engine and a recommendation system for the music industry, built with JinaAI.
</details>
<details>
<summary>摘要</summary>
一个非常吸引人的问题在音乐产业中是搜索引擎和推荐系统的开发。研究表明，搜索引擎领域受到了严重的萧瑟，主要原因包括速度、准确性和数据格式的问题。人们经常遇到查找歌曲 solely based on the title 是困难的，因此一种解决方案是通过单个查询输入完成搜索分析，并将数据库中的歌曲 lyrics 与输入进行匹配。因此，搜索引擎的开发需要采用 cutting-edge 技术工具，以建立用户友好的搜索引擎。Jina AI 是一个 MLOps 框架，用于构建基于神经网络的搜索引擎，可以帮助用户 obtian 精准的结果。Jina AI 有效地帮助维护和提高搜索引擎的性能质量。一个基于 JinaAI 的高效搜索引擎和推荐系统，可以为音乐产业提供更好的用户体验。
</details></li>
</ul>
<hr>
<h2 id="The-Copycat-Perceptron-Smashing-Barriers-Through-Collective-Learning"><a href="#The-Copycat-Perceptron-Smashing-Barriers-Through-Collective-Learning" class="headerlink" title="The Copycat Perceptron: Smashing Barriers Through Collective Learning"></a>The Copycat Perceptron: Smashing Barriers Through Collective Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03743">http://arxiv.org/abs/2308.03743</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giovanni Catania, Aurélien Decelle, Beatriz Seoane</li>
<li>for: 研究一种 teacher-student enario中的 $y$ coupled binary perceptrons 的平衡性质。</li>
<li>methods: 使用一种适当的学习规则，并对学生模型的权重进行显式杂化，使其与教师模型的权重具有哈明顿距离的关系。</li>
<li>results: 在存在温度噪声的情况下，研究发现，在教师模型的指导下，学生模型的学习表现得到改进，而且随着学生模型的数量增加，学习表现得到进一步改进。<details>
<summary>Abstract</summary>
We characterize the equilibrium properties of a model of $y$ coupled binary perceptrons in the teacher-student scenario, subject to a suitable learning rule, with an explicit ferromagnetic coupling proportional to the Hamming distance between the students' weights. In contrast to recent works, we analyze a more general setting in which a thermal noise is present that affects the generalization performance of each student. Specifically, in the presence of a nonzero temperature, which assigns nonzero probability to configurations that misclassify samples with respect to the teacher's prescription, we find that the coupling of replicas leads to a shift of the phase diagram to smaller values of $\alpha$: This suggests that the free energy landscape gets smoother around the solution with good generalization (i.e., the teacher) at a fixed fraction of reviewed examples, which allows local update algorithms such as Simulated Annealing to reach the solution before the dynamics gets frozen. Finally, from a learning perspective, these results suggest that more students (in this case, with the same amount of data) are able to learn the same rule when coupled together with a smaller amount of data.
</details>
<details>
<summary>摘要</summary>
我们描述一个 teacher-student 模型中的 $y$ coupled binary perceptron 的平衡性能，采用一种合适的学习规则，带有明确的 ferromagnetic 相互作用，其比例与学生们的权重之间的汉明距离相关。与先前的研究不同，我们分析了一个更通用的设置，在其中每个学生都受到一定温度的影响，这使得每个学生的泛化性能受到影响。 Specifically, 在非零温度下，权重的配置可能会错误地分类样本，从而导致学生的泛化性能下降。我们发现，在这种情况下，同学生之间的相互作用会使得解析的相对稳定点变小，这意味着解析的自由能面积变得更加平滑，使得本地更新算法如模拟热化可以更容易地到达解析。最后，从学习角度来看，这些结果表明，当学生们相互couple时，可以通过更小的数据量来学习同样的规则，这意味着更多的学生可以在同样的数据量下学习。
</details></li>
</ul>
<hr>
<h2 id="Randomized-algorithms-for-precise-measurement-of-differentially-private-personalized-recommendations"><a href="#Randomized-algorithms-for-precise-measurement-of-differentially-private-personalized-recommendations" class="headerlink" title="Randomized algorithms for precise measurement of differentially-private, personalized recommendations"></a>Randomized algorithms for precise measurement of differentially-private, personalized recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03735">http://arxiv.org/abs/2308.03735</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/apple/ml-dprecs">https://github.com/apple/ml-dprecs</a></li>
<li>paper_authors: Allegra Laro, Yanqing Chen, Hao He, Babak Aghazadeh</li>
<li>for: 提出一种隐私保护的个性化推荐算法，以便在个性化推荐中保护用户隐私。</li>
<li>methods: 提出的算法使用幂等函数和均值分布来保证隐私，并通过实验证明其可以保持 preciseness 和隐私性。</li>
<li>results: 对于广告应用场景，该算法可以提高用户体验、广告商价值和平台收入，同时保持隐私性。<details>
<summary>Abstract</summary>
Personalized recommendations form an important part of today's internet ecosystem, helping artists and creators to reach interested users, and helping users to discover new and engaging content. However, many users today are skeptical of platforms that personalize recommendations, in part due to historically careless treatment of personal data and data privacy. Now, businesses that rely on personalized recommendations are entering a new paradigm, where many of their systems must be overhauled to be privacy-first. In this article, we propose an algorithm for personalized recommendations that facilitates both precise and differentially-private measurement. We consider advertising as an example application, and conduct offline experiments to quantify how the proposed privacy-preserving algorithm affects key metrics related to user experience, advertiser value, and platform revenue compared to the extremes of both (private) non-personalized and non-private, personalized implementations.
</details>
<details>
<summary>摘要</summary>
个人化推荐作为今天互联网生态系统中的一部分，帮助艺术家和创作者达到有趣用户，并帮助用户发现新的有趣内容。然而，许多用户今天对个人化推荐平台表示怀疑，其中一部分是由于历史上不谨慎处理个人数据和隐私。现在，企业们正在进入一个新的平台，其中许多系统需要重新设计，以保持隐私First。在这篇文章中，我们提出一种用于个人化推荐的算法，以实现精准和隐私保护。我们使用广告作为应用例子，并在线上实验来衡量该提议的隐私保护算法对用户体验、广告商价值和平台收入的影响，与非个人化和非隐私的个人化实现相比。
</details></li>
</ul>
<hr>
<h2 id="SurvBeX-An-explanation-method-of-the-machine-learning-survival-models-based-on-the-Beran-estimator"><a href="#SurvBeX-An-explanation-method-of-the-machine-learning-survival-models-based-on-the-Beran-estimator" class="headerlink" title="SurvBeX: An explanation method of the machine learning survival models based on the Beran estimator"></a>SurvBeX: An explanation method of the machine learning survival models based on the Beran estimator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03730">http://arxiv.org/abs/2308.03730</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/danilaeremenko/survbex">https://github.com/danilaeremenko/survbex</a></li>
<li>paper_authors: Lev V. Utkin, Danila Y. Eremenko, Andrei V. Konstantinov</li>
<li>For: The paper proposes a new method called SurvBeX to explain the predictions of machine learning survival black-box models.* Methods: The method uses a modified Beran estimator as a surrogate explanation model, and generates many points in a local area around an example of interest to compute the survival function of the black-box model and the Beran estimator.* Results: The paper demonstrates the efficiency of SurvBeX through numerical experiments with synthetic and real survival data, and compares the method with the well-known method SurvLIME and SurvSHAP. The code implementing SurvBeX is available online.Here are the three points in Simplified Chinese text:* For: 这篇论文提出了一种新的方法SurvBeX，用于解释机器学习生存黑盒模型的预测结果。* Methods: SurvBeX使用修改后的Beran估计器作为准确解释模型，并在 интерес示例附近生成多个点，以计算黑盒模型和Beran估计器的生存函数。* Results: 论文通过synthetic和实际生存数据的numerical实验，证明SurvBeX的效果，并与SurvLIME和SurvSHAP进行比较。代码实现SurvBeX可以在<a target="_blank" rel="noopener" href="https://github.com/DanilaEremenko/SurvBeX%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/DanilaEremenko/SurvBeX上获取。</a><details>
<summary>Abstract</summary>
An explanation method called SurvBeX is proposed to interpret predictions of the machine learning survival black-box models. The main idea behind the method is to use the modified Beran estimator as the surrogate explanation model. Coefficients, incorporated into Beran estimator, can be regarded as values of the feature impacts on the black-box model prediction. Following the well-known LIME method, many points are generated in a local area around an example of interest. For every generated example, the survival function of the black-box model is computed, and the survival function of the surrogate model (the Beran estimator) is constructed as a function of the explanation coefficients. In order to find the explanation coefficients, it is proposed to minimize the mean distance between the survival functions of the black-box model and the Beran estimator produced by the generated examples. Many numerical experiments with synthetic and real survival data demonstrate the SurvBeX efficiency and compare the method with the well-known method SurvLIME. The method is also compared with the method SurvSHAP. The code implementing SurvBeX is available at: https://github.com/DanilaEremenko/SurvBeX
</details>
<details>
<summary>摘要</summary>
提出一种名为SurvBeX的解释方法，用于解释机器学习预测模型的逝去黑盒模型。该方法的主要想法是使用修改后的Beran估计器作为解释模型。 incorporated into Beran estimator的系数可以 viewed as预测模型中特征影响值。采用LIME方法的思路，在对 interessant example的local区域附近生成多个例子。每个生成的例子中，预测模型的生存函数被计算，并将生存函数转化为解释系数的函数。以 minimize the mean distance between the survival functions of the black-box model and the Beran estimator produced by the generated examples。 numerically experiments with synthetic and real survival data demonstrate SurvBeX efficiency and compare the method with well-known method SurvLIME. The method is also compared with SurvSHAP method. SurvBeX的代码可以在以下链接获取：https://github.com/DanilaEremenko/SurvBeX。
</details></li>
</ul>
<hr>
<h2 id="Dimensionality-Reduction-for-Improving-Out-of-Distribution-Detection-in-Medical-Image-Segmentation"><a href="#Dimensionality-Reduction-for-Improving-Out-of-Distribution-Detection-in-Medical-Image-Segmentation" class="headerlink" title="Dimensionality Reduction for Improving Out-of-Distribution Detection in Medical Image Segmentation"></a>Dimensionality Reduction for Improving Out-of-Distribution Detection in Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03723">http://arxiv.org/abs/2308.03723</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mckellwoodland/dimen_reduce_mahal">https://github.com/mckellwoodland/dimen_reduce_mahal</a></li>
<li>paper_authors: McKell Woodland, Nihil Patel, Mais Al Taie, Joshua P. Yung, Tucker J. Netherton, Ankit B. Patel, Kristy K. Brock</li>
<li>for: 这个研究是为了检测MRI肿瘤影像分类模型的外部分布情况，以避免自动偏见。</li>
<li>methods: 这个研究使用了Mahalanobis距离后置法，将Swin UNITER模型的瓶颈特征使用主成分分析，实现高性能的外部分布检测。</li>
<li>results: 研究发现，这种方法可以实现高度的外部分布检测，并且具有较少的计算负载。<details>
<summary>Abstract</summary>
Clinically deployed segmentation models are known to fail on data outside of their training distribution. As these models perform well on most cases, it is imperative to detect out-of-distribution (OOD) images at inference to protect against automation bias. This work applies the Mahalanobis distance post hoc to the bottleneck features of a Swin UNETR model that segments the liver on T1-weighted magnetic resonance imaging. By reducing the dimensions of the bottleneck features with principal component analysis, OOD images were detected with high performance and minimal computational load.
</details>
<details>
<summary>摘要</summary>
临床部署的分割模型经常会在训练数据外部失败。由于这些模型在大多数情况下表现良好，因此在推断时检测出对外部数据的自动偏见是必要的。这项工作使用Swin UNITER模型的瓶颈特征进行 Mahalanobis 距离后处理，以检测T1核磁共振成像上的肝脏分割图像。通过减少瓶颈特征的维度使用主成分分析，可以高效地检测到对外部数据的图像。
</details></li>
</ul>
<hr>
<h2 id="“Do-Anything-Now”-Characterizing-and-Evaluating-In-The-Wild-Jailbreak-Prompts-on-Large-Language-Models"><a href="#“Do-Anything-Now”-Characterizing-and-Evaluating-In-The-Wild-Jailbreak-Prompts-on-Large-Language-Models" class="headerlink" title="“Do Anything Now”: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models"></a>“Do Anything Now”: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03825">http://arxiv.org/abs/2308.03825</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/verazuo/jailbreak_llms">https://github.com/verazuo/jailbreak_llms</a></li>
<li>paper_authors: Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, Yang Zhang</li>
<li>for: This paper aims to study the emergence and evolution of jailbreak prompts in the wild, and to assess the potential harm caused by these prompts on large language models (LLMs).</li>
<li>methods: The authors use natural language processing technologies and graph-based community detection methods to collect and analyze 6,387 jailbreak prompts from four platforms over six months. They also create a question set comprising 46,800 samples across 13 forbidden scenarios to evaluate the effectiveness of current LLMs and safeguards in defending against jailbreak prompts.</li>
<li>results: The authors find unique characteristics of jailbreak prompts and their major attack strategies, such as prompt injection and privilege escalation. They also observe that jailbreak prompts are increasingly shifting from public platforms to private ones, posing new challenges for LLM vendors in proactive detection. Additionally, they identify two highly effective jailbreak prompts that achieve 0.99 attack success rates on ChatGPT (GPT-3.5) and GPT-4, and have persisted online for over 100 days.<details>
<summary>Abstract</summary>
The misuse of large language models (LLMs) has garnered significant attention from the general public and LLM vendors. In response, efforts have been made to align LLMs with human values and intent use. However, a particular type of adversarial prompts, known as jailbreak prompt, has emerged and continuously evolved to bypass the safeguards and elicit harmful content from LLMs. In this paper, we conduct the first measurement study on jailbreak prompts in the wild, with 6,387 prompts collected from four platforms over six months. Leveraging natural language processing technologies and graph-based community detection methods, we discover unique characteristics of jailbreak prompts and their major attack strategies, such as prompt injection and privilege escalation. We also observe that jailbreak prompts increasingly shift from public platforms to private ones, posing new challenges for LLM vendors in proactive detection. To assess the potential harm caused by jailbreak prompts, we create a question set comprising 46,800 samples across 13 forbidden scenarios. Our experiments show that current LLMs and safeguards cannot adequately defend jailbreak prompts in all scenarios. Particularly, we identify two highly effective jailbreak prompts which achieve 0.99 attack success rates on ChatGPT (GPT-3.5) and GPT-4, and they have persisted online for over 100 days. Our work sheds light on the severe and evolving threat landscape of jailbreak prompts. We hope our study can facilitate the research community and LLM vendors in promoting safer and regulated LLMs.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）的不当使用已经引起了广泛的关注，并且有努力以寻求对human values和合法用途进行Alignment。然而，一种特殊的恶意提示，称为监狱提示，已经出现并不断演化以绕过安全措施并征引出危险内容。在这篇论文中，我们进行了第一次在野外中对监狱提示的量化研究，收集了6,387个提示从四个平台上经过六个月。通过自然语言处理技术和图形基本社区探测方法，我们发现了监狱提示的独特特征和主要攻击策略，如提示注入和特权提升。我们还发现，监狱提示逐渐从公共平台迁移到私有平台，这对LLM供应商提出了新的检测挑战。为了评估监狱提示的可能伤害，我们创建了46,800个问题组合，覆盖13种禁止enario。我们的实验表明，当前的LLM和安全措施无法在所有情况下有效防御监狱提示。特别是，我们标识出了两个非常有效的监狱提示，在ChatGPT（GPT-3.5）和GPT-4上具有0.99攻击成功率，并且它们在线上延续了超过100天。我们的工作照明了监狱提示的严重和演化的威胁风险。我们希望我们的研究可以促进研究社区和LLM供应商在推广安全和有序的LLM方面做出更多的努力。
</details></li>
</ul>
<hr>
<h2 id="Communication-Efficient-Framework-for-Distributed-Image-Semantic-Wireless-Transmission"><a href="#Communication-Efficient-Framework-for-Distributed-Image-Semantic-Wireless-Transmission" class="headerlink" title="Communication-Efficient Framework for Distributed Image Semantic Wireless Transmission"></a>Communication-Efficient Framework for Distributed Image Semantic Wireless Transmission</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03713">http://arxiv.org/abs/2308.03713</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bingyan Xie, Yongpeng Wu, Yuxuan Shi, Derrick Wing Kwan Ng, Wenjun Zhang</li>
<li>for: 这篇论文主要targets the problem of communication-efficient distributed data transmission in Internet-of-Things (IoT) scenarios with multiple devices, and proposes a federated learning-based semantic communication (FLSC) framework for multi-task distributed image transmission.</li>
<li>methods: The proposed FLSC framework uses a hierarchical vision transformer (HVT)-based extractor and a task-adaptive translator for coarse-to-fine semantic extraction and meaning translation, and a channel state information-based multiple-input multiple-output transmission module to combat channel fading and noise.</li>
<li>results: Simulation results show that the coarse semantic information can deal with a range of image-level tasks, and the FLSC framework outperforms traditional schemes in low signal-to-noise ratio and channel bandwidth ratio regimes, with a gain of around 10 peak signal-to-noise ratio in the 3 dB channel condition.<details>
<summary>Abstract</summary>
Multi-node communication, which refers to the interaction among multiple devices, has attracted lots of attention in many Internet-of-Things (IoT) scenarios. However, its huge amounts of data flows and inflexibility for task extension have triggered the urgent requirement of communication-efficient distributed data transmission frameworks. In this paper, inspired by the great superiorities on bandwidth reduction and task adaptation of semantic communications, we propose a federated learning-based semantic communication (FLSC) framework for multi-task distributed image transmission with IoT devices. Federated learning enables the design of independent semantic communication link of each user while further improves the semantic extraction and task performance through global aggregation. Each link in FLSC is composed of a hierarchical vision transformer (HVT)-based extractor and a task-adaptive translator for coarse-to-fine semantic extraction and meaning translation according to specific tasks. In order to extend the FLSC into more realistic conditions, we design a channel state information-based multiple-input multiple-output transmission module to combat channel fading and noise. Simulation results show that the coarse semantic information can deal with a range of image-level tasks. Moreover, especially in low signal-to-noise ratio and channel bandwidth ratio regimes, FLSC evidently outperforms the traditional scheme, e.g. about 10 peak signal-to-noise ratio gain in the 3 dB channel condition.
</details>
<details>
<summary>摘要</summary>
多节点通信，指的是多个设备之间的交互，在互联网关键设备（IoT）场景中吸引了很多关注。然而，它的巨量数据流和任务扩展不灵活性已经触发了高效通信分布数据传输框架的紧迫需求。本文提出了基于联合学习的 semantic communication（FLSC）框架，用于多任务分布式图像传输。联合学习使得每个用户独立的semantic communication链接可以进一步提高semantic抽取和任务性能通过全球汇总。每个FLSC链接都包括层次视transformer（HVT）基于抽取器和任务适应译码器，用于层次semantic抽取和意义翻译。为了扩展FLSC到更加实际的情况，我们设计了基于 kanal 状态信息的多输入多输出传输模块，以对抗通道抖动和噪声。实验结果显示，粗粒度semantic信息可以处理多种图像级任务。此外，特别在低信号噪声比和通道宽bandwidth比例 regime下，FLSC明显超过传统方案，例如在3dB通道条件下约10带宽比例的增强。
</details></li>
</ul>
<hr>
<h2 id="Scaling-may-be-all-you-need-for-achieving-human-level-object-recognition-capacity-with-human-like-visual-experience"><a href="#Scaling-may-be-all-you-need-for-achieving-human-level-object-recognition-capacity-with-human-like-visual-experience" class="headerlink" title="Scaling may be all you need for achieving human-level object recognition capacity with human-like visual experience"></a>Scaling may be all you need for achieving human-level object recognition capacity with human-like visual experience</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03712">http://arxiv.org/abs/2308.03712</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/eminorhan/humanlike-vits">https://github.com/eminorhan/humanlike-vits</a></li>
<li>paper_authors: A. Emin Orhan</li>
<li>for: 这 paper  investigate whether current self-supervised learning methods can reach human-level visual object recognition capabilities with the same type and amount of visual experience as humans.</li>
<li>methods: 这 paper 使用 vision transformers 和 masked autoencoders (MAEs) 进行自然语言处理，并进行了数据大小、模型大小和图像分辨率的同时缩放实验。</li>
<li>results: 结果表明，可以通过同时缩放数据大小、模型大小和图像分辨率来达到人类水平的 объекRecognition 能力，而无需使用专门的 inductive biases。例如，一个 2.5B 参数的 ViT 模型，通过使用 20K 小时的人类样式视频数据和 952x952 像素的空间分辨率，应该可以达到 ImageNet 上的人类水平准确率。<details>
<summary>Abstract</summary>
This paper asks whether current self-supervised learning methods, if sufficiently scaled up, would be able to reach human-level visual object recognition capabilities with the same type and amount of visual experience humans learn from. Previous work on this question only considered the scaling of data size. Here, we consider the simultaneous scaling of data size, model size, and image resolution. We perform a scaling experiment with vision transformers up to 633M parameters in size (ViT-H/14) trained with up to 5K hours of human-like video data (long, continuous, mostly egocentric videos) with image resolutions of up to 476x476 pixels. The efficiency of masked autoencoders (MAEs) as a self-supervised learning algorithm makes it possible to run this scaling experiment on an unassuming academic budget. We find that it is feasible to reach human-level object recognition capacity at sub-human scales of model size, data size, and image size, if these factors are scaled up simultaneously. To give a concrete example, we estimate that a 2.5B parameter ViT model trained with 20K hours (2.3 years) of human-like video data with a spatial resolution of 952x952 pixels should be able to reach roughly human-level accuracy on ImageNet. Human-level competence is thus achievable for a fundamental perceptual capability from human-like perceptual experience (human-like in both amount and type) with extremely generic learning algorithms and architectures and without any substantive inductive biases.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DeRisk-An-Effective-Deep-Learning-Framework-for-Credit-Risk-Prediction-over-Real-World-Financial-Data"><a href="#DeRisk-An-Effective-Deep-Learning-Framework-for-Credit-Risk-Prediction-over-Real-World-Financial-Data" class="headerlink" title="DeRisk: An Effective Deep Learning Framework for Credit Risk Prediction over Real-World Financial Data"></a>DeRisk: An Effective Deep Learning Framework for Credit Risk Prediction over Real-World Financial Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03704">http://arxiv.org/abs/2308.03704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yancheng Liang, Jiajie Zhang, Hui Li, Xiaochen Liu, Yi Hu, Yong Wu, Jinyao Zhang, Yongyan Liu, Yi Wu</li>
<li>for: 预测信用风险（credit risk prediction）在真实世界金融数据上。</li>
<li>methods: 提出了一种深度学习风险预测框架（DeRisk），该框架可以在真实世界金融数据上高效地预测信用风险。</li>
<li>results: DeRisk 已经在实际生产环境中超过了统计学学习方法的表现，并进行了广泛的缺失研究以证明模型的成功因素。<details>
<summary>Abstract</summary>
Despite the tremendous advances achieved over the past years by deep learning techniques, the latest risk prediction models for industrial applications still rely on highly handtuned stage-wised statistical learning tools, such as gradient boosting and random forest methods. Different from images or languages, real-world financial data are high-dimensional, sparse, noisy and extremely imbalanced, which makes deep neural network models particularly challenging to train and fragile in practice. In this work, we propose DeRisk, an effective deep learning risk prediction framework for credit risk prediction on real-world financial data. DeRisk is the first deep risk prediction model that outperforms statistical learning approaches deployed in our company's production system. We also perform extensive ablation studies on our method to present the most critical factors for the empirical success of DeRisk.
</details>
<details>
<summary>摘要</summary>
尽管深度学习技术过去几年取得了很大的进步，现在最新的风险预测模型仍然基于高度手动调整的阶段性统计学学习工具，如梯度批量和随机森林方法。与图像或语言不同，实际世界金融数据具有高维、稀疏、噪音和极其不均衡的特点，这使得深度神经网络模型在实践中特别困难并脆弱。在这项工作中，我们提出了DeRisk，一种高效的深度学习风险预测框架，用于实际世界金融数据上的信用风险预测。DeRisk是我们公司生产系统中部署的统计学学习方法的首个深度风险预测模型，我们还进行了广泛的减少研究，以阐明DeRisk的成功的重要因素。
</details></li>
</ul>
<hr>
<h2 id="AgentBench-Evaluating-LLMs-as-Agents"><a href="#AgentBench-Evaluating-LLMs-as-Agents" class="headerlink" title="AgentBench: Evaluating LLMs as Agents"></a>AgentBench: Evaluating LLMs as Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03688">http://arxiv.org/abs/2308.03688</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thudm/agentbench">https://github.com/thudm/agentbench</a></li>
<li>paper_authors: Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, Jie Tang</li>
<li>for: 评估大型自然语言处理器（LLM）在实际世界中的智能和自主能力。</li>
<li>methods: 使用多维度演化的 benchmark 来评估 LLM 作为代理的判断和决策能力。</li>
<li>results: 测试了 25 个 LLM（包括 API 和开源模型），发现商业 LLM 表现出色，但是开源竞争对手的表现差异较大。Note: “LLM” stands for “Large Language Models” in English.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are becoming increasingly smart and autonomous, targeting real-world pragmatic missions beyond traditional NLP tasks. As a result, there has been an urgent need to evaluate LLMs as agents on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional evolving benchmark that currently consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities in a multi-turn open-ended generation setting. Our extensive test over 25 LLMs (including APIs and open-sourced models) shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and open-sourced competitors. It also serves as a component of an ongoing project with wider coverage and deeper consideration towards systematic LLM evaluation. Datasets, environments, and an integrated evaluation package for AgentBench are released at https://github.com/THUDM/AgentBench
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Almost-sure-convergence-of-iterates-and-multipliers-in-stochastic-sequential-quadratic-optimization"><a href="#Almost-sure-convergence-of-iterates-and-multipliers-in-stochastic-sequential-quadratic-optimization" class="headerlink" title="Almost-sure convergence of iterates and multipliers in stochastic sequential quadratic optimization"></a>Almost-sure convergence of iterates and multipliers in stochastic sequential quadratic optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03687">http://arxiv.org/abs/2308.03687</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frank E. Curtis, Xin Jiang, Qi Wang</li>
<li>for:  solves large-scale data-fitting problems subject to nonconvex constraints</li>
<li>methods:  stochastic-gradient methodology from the unconstrained setting</li>
<li>results:  new almost-sure convergence guarantees for the primal iterates, Lagrange multipliers, and stationarity measures<details>
<summary>Abstract</summary>
Stochastic sequential quadratic optimization (SQP) methods for solving continuous optimization problems with nonlinear equality constraints have attracted attention recently, such as for solving large-scale data-fitting problems subject to nonconvex constraints. However, for a recently proposed subclass of such methods that is built on the popular stochastic-gradient methodology from the unconstrained setting, convergence guarantees have been limited to the asymptotic convergence of the expected value of a stationarity measure to zero. This is in contrast to the unconstrained setting in which almost-sure convergence guarantees (of the gradient of the objective to zero) can be proved for stochastic-gradient-based methods. In this paper, new almost-sure convergence guarantees for the primal iterates, Lagrange multipliers, and stationarity measures generated by a stochastic SQP algorithm in this subclass of methods are proved. It is shown that the error in the Lagrange multipliers can be bounded by the distance of the primal iterate to a primal stationary point plus the error in the latest stochastic gradient estimate. It is further shown that, subject to certain assumptions, this latter error can be made to vanish by employing a running average of the Lagrange multipliers that are computed during the run of the algorithm. The results of numerical experiments are provided to demonstrate the proved theoretical guarantees.
</details>
<details>
<summary>摘要</summary>
This paper presents new almost-sure convergence guarantees for the primal iterates, Lagrange multipliers, and stationarity measures generated by a stochastic SQP algorithm in this subclass of methods. The error in the Lagrange multipliers can be bounded by the distance of the primal iterate to a primal stationary point plus the error in the latest stochastic gradient estimate. Furthermore, it is shown that this latter error can be made to vanish by employing a running average of the Lagrange multipliers computed during the run of the algorithm, subject to certain assumptions.Numerical experiments are provided to demonstrate the proved theoretical guarantees. These results demonstrate the effectiveness of the proposed method in solving continuous optimization problems with nonlinear equality constraints.
</details></li>
</ul>
<hr>
<h2 id="Linear-Convergence-Bounds-for-Diffusion-Models-via-Stochastic-Localization"><a href="#Linear-Convergence-Bounds-for-Diffusion-Models-via-Stochastic-Localization" class="headerlink" title="Linear Convergence Bounds for Diffusion Models via Stochastic Localization"></a>Linear Convergence Bounds for Diffusion Models via Stochastic Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03686">http://arxiv.org/abs/2308.03686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joe Benton, Valentin De Bortoli, Arnaud Doucet, George Deligiannidis</li>
<li>for: 这个论文的目的是提供高维数据分布采样的精度模型，并且提供了对这些模型的证明。</li>
<li>methods: 这个论文使用了扩散模型，并且使用了$L^2$-精度分布估计器。</li>
<li>results: 这个论文提供了高维数据分布采样的 linear  bounds，即在数据维度上具有 $\tilde O(\frac{d \log^2(1&#x2F;\delta)}{\varepsilon^2})$ 步骤可以将数据分布 approximate 到 Within $\varepsilon^2$ 的 Kullback–Leibler 差分。<details>
<summary>Abstract</summary>
Diffusion models are a powerful method for generating approximate samples from high-dimensional data distributions. Several recent results have provided polynomial bounds on the convergence rate of such models, assuming $L^2$-accurate score estimators. However, up until now the best known such bounds were either superlinear in the data dimension or required strong smoothness assumptions. We provide the first convergence bounds which are linear in the data dimension (up to logarithmic factors) assuming only finite second moments of the data distribution. We show that diffusion models require at most $\tilde O(\frac{d \log^2(1/\delta)}{\varepsilon^2})$ steps to approximate an arbitrary data distribution on $\mathbb{R}^d$ corrupted with Gaussian noise of variance $\delta$ to within $\varepsilon^2$ in Kullback--Leibler divergence. Our proof builds on the Girsanov-based methods of previous works. We introduce a refined treatment of the error arising from the discretization of the reverse SDE, which is based on tools from stochastic localization.
</details>
<details>
<summary>摘要</summary>
传播模型是一种强大的方法来生成高维数据分布的近似样本。一些最近的结果提供了多项关于传播模型的数值精度，假设$L^2$-精确的分配 estimator。然而，直到现在，最好的知识是 either 超过线性的数据维度或需要强大的平滑假设。我们提供了首个线性于数据维度（以logarithmic factor）的传播模型数值精度 bound，只需要对数据分布的第二 moment finite。我们证明了传播模型可以在 $\mathbb{R}^d$ 上静态数据分布上静态数据分布 corrupted with Gaussian noise of variance $\delta$ 到 Within $\varepsilon^2$ 库拉克-莱比勒分布 divergence 以内 $\tilde O\left(\frac{d \log^2(1/\delta)}{\varepsilon^2}\right)$ 步骤。我们的证明基于先前的 Girsanov-based 方法，并 introducing 对 reverse SDE 的精度误差的更加精确的处理，基于 Stochastic localization 的工具。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/08/cs.LG_2023_08_08/" data-id="clly4xtdt006hvl889a1p9p79" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/08/cs.SD_2023_08_08/" class="article-date">
  <time datetime="2023-08-07T16:00:00.000Z" itemprop="datePublished">2023-08-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/08/cs.SD_2023_08_08/">cs.SD - 2023-08-08 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Towards-an-AI-to-Win-Ghana’s-National-Science-and-Maths-Quiz"><a href="#Towards-an-AI-to-Win-Ghana’s-National-Science-and-Maths-Quiz" class="headerlink" title="Towards an AI to Win Ghana’s National Science and Maths Quiz"></a>Towards an AI to Win Ghana’s National Science and Maths Quiz</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04333">http://arxiv.org/abs/2308.04333</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nsmq-ai/nsmqai">https://github.com/nsmq-ai/nsmqai</a></li>
<li>paper_authors: George Boateng, Jonathan Abrefah Mensah, Kevin Takyi Yeboah, William Edor, Andrew Kojo Mensah-Onumah, Naafi Dasana Ibrahim, Nana Sam Yeboah</li>
<li>For: The paper aims to build an AI system to compete in Ghana’s National Science and Maths Quiz (NSMQ) and win.* Methods: The project uses open-source technology and involves building AI systems to answer questions across biology, chemistry, physics, and math.* Results: The AI system is being developed and tested, with progress made thus far and the next steps planned for a launch in October 2023.Here is the information in Simplified Chinese text:* For: 这个论文的目的是建立一个AI系统，参加加纳国家科学和数学竞赛（NSMQ），并赢得奖。* Methods: 这个项目使用开源技术，建立AI系统，以回答生物、化学、物理和数学等领域的问题。* Results: AI系统正在开发和测试中，已经完成了一些进度，下一步计划在2023年10月发布。<details>
<summary>Abstract</summary>
Can an AI win Ghana's National Science and Maths Quiz (NSMQ)? That is the question we seek to answer in the NSMQ AI project, an open-source project that is building AI to compete live in the NSMQ and win. The NSMQ is an annual live science and mathematics competition for senior secondary school students in Ghana in which 3 teams of 2 students compete by answering questions across biology, chemistry, physics, and math in 5 rounds over 5 progressive stages until a winning team is crowned for that year. The NSMQ is an exciting live quiz competition with interesting technical challenges across speech-to-text, text-to-speech, question-answering, and human-computer interaction. In this ongoing work that began in January 2023, we give an overview of the project, describe each of the teams, progress made thus far, and the next steps toward our planned launch and debut of the AI in October for NSMQ 2023. An AI that conquers this grand challenge can have real-world impact on education such as enabling millions of students across Africa to have one-on-one learning support from this AI.
</details>
<details>
<summary>摘要</summary>
可以AI赢得加纳的国家科学与数学竞赛（NSMQ）呢？我们正在NSMQ AI项目中努力解决这个问题，这是一个开源项目，用AI参加NSMQ并赢得奖。NSMQ是每年举行的生活科学和数学竞赛，参赛者是加纳高中二年级学生，他们需要在5轮5个阶段中回答生物、化学、物理和数学等领域的问题。这是一项有趣的现场竞赛，涉及到语音识别、文本识别、问题回答和人机交互等技术挑战。在我们自2023年1月开始的工作中，我们将介绍项目的概况，描述各个团队，已经成就的进度以及下一步的计划，以准在10月份的NSMQ 2023上发布和使用AI。如果AI成功完成这项挑战，可以对教育产生实际的影响，例如，使得非洲的数百万学生得到AI一对一的学习支持。
</details></li>
</ul>
<hr>
<h2 id="Auditory-Attention-Decoding-with-Task-Related-Multi-View-Contrastive-Learning"><a href="#Auditory-Attention-Decoding-with-Task-Related-Multi-View-Contrastive-Learning" class="headerlink" title="Auditory Attention Decoding with Task-Related Multi-View Contrastive Learning"></a>Auditory Attention Decoding with Task-Related Multi-View Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04244">http://arxiv.org/abs/2308.04244</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoyu Chen, Changde Du, Qiongyi Zhou, Huiguang He</li>
<li>For: 本研究旨在解决现有深度学习方法困难充分利用不同视角（即注意力和EEG数据）的问题，并提取有利的表示。* Methods: 我们提出了基于多视角VAE的听觉注意力解码方法（AAD），并使用任务相关多视角对比（TMC）学习来融合不同视角的知识。* Results: 我们在两个常用AAD数据集上进行了实验，并证明了我们的方法在比较当前状态方法时表现出了超越。<details>
<summary>Abstract</summary>
The human brain can easily focus on one speaker and suppress others in scenarios such as a cocktail party. Recently, researchers found that auditory attention can be decoded from the electroencephalogram (EEG) data. However, most existing deep learning methods are difficult to use prior knowledge of different views (that is attended speech and EEG are task-related views) and extract an unsatisfactory representation. Inspired by Broadbent's filter model, we decode auditory attention in a multi-view paradigm and extract the most relevant and important information utilizing the missing view. Specifically, we propose an auditory attention decoding (AAD) method based on multi-view VAE with task-related multi-view contrastive (TMC) learning. Employing TMC learning in multi-view VAE can utilize the missing view to accumulate prior knowledge of different views into the fusion of representation, and extract the approximate task-related representation. We examine our method on two popular AAD datasets, and demonstrate the superiority of our method by comparing it to the state-of-the-art method.
</details>
<details>
<summary>摘要</summary>
人脑可以轻松地专注一个说话者，而抑制其他说话者的场景，如cocktail party。最近，研究人员发现了基于电enzephalogram（EEG）数据的听力注意力可以被解码。然而，现有的深度学习方法很难以利用不同视图（即注意力和EEG数据是关联任务的视图）的先前知识，提取不满足的表示。受布洛满特的筛子模型启发，我们在多视图 paradigm 中解码听力注意力，并使用缺失视图来汇集不同视图中的先前知识，提取关键和重要的信息。我们提出了基于多视图VAE的听力注意力解码方法（AAD），并使用任务相关多视图强制学习（TMC）来学习。通过TMC学习在多视图VAE中，可以利用缺失视图来汇集不同视图中的先前知识，提取相似任务的表示。我们在两个流行的AAD数据集上测试了我们的方法，并证明了我们的方法在比较顶尖方法的基础上具有优势。
</details></li>
</ul>
<hr>
<h2 id="Evil-Operation-Breaking-Speaker-Recognition-with-PaddingBack"><a href="#Evil-Operation-Breaking-Speaker-Recognition-with-PaddingBack" class="headerlink" title="Evil Operation: Breaking Speaker Recognition with PaddingBack"></a>Evil Operation: Breaking Speaker Recognition with PaddingBack</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04179">http://arxiv.org/abs/2308.04179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhe Ye, Diqun Yan, Li Dong, Kailai Shen</li>
<li>for: 这篇论文旨在攻击语音识别系统，实现隐蔽的攻击方式。</li>
<li>methods: 论文使用了padding操作来制造恶意样本，并且通过滥聚积极的方式来降低人工干预的可见性。</li>
<li>results: 实验结果表明，提案的方法可以达到高度的攻击成功率，同时保持高度的正常准确率。此外，方法还能够抵抗防御方法并继续保持隐蔽性。<details>
<summary>Abstract</summary>
Machine Learning as a Service (MLaaS) has gained popularity due to advancements in machine learning. However, untrusted third-party platforms have raised concerns about AI security, particularly in backdoor attacks. Recent research has shown that speech backdoors can utilize transformations as triggers, similar to image backdoors. However, human ears easily detect these transformations, leading to suspicion. In this paper, we introduce PaddingBack, an inaudible backdoor attack that utilizes malicious operations to make poisoned samples indistinguishable from clean ones. Instead of using external perturbations as triggers, we exploit the widely used speech signal operation, padding, to break speaker recognition systems. Our experimental results demonstrate the effectiveness of the proposed approach, achieving a significantly high attack success rate while maintaining a high rate of benign accuracy. Furthermore, PaddingBack demonstrates the ability to resist defense methods while maintaining its stealthiness against human perception. The results of the stealthiness experiment have been made available at https://nbufabio25.github.io/paddingback/.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MSAC-Multiple-Speech-Attribute-Control-Method-for-Speech-Emotion-Recognition"><a href="#MSAC-Multiple-Speech-Attribute-Control-Method-for-Speech-Emotion-Recognition" class="headerlink" title="MSAC: Multiple Speech Attribute Control Method for Speech Emotion Recognition"></a>MSAC: Multiple Speech Attribute Control Method for Speech Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04025">http://arxiv.org/abs/2308.04025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Pan</li>
<li>for: 本研究旨在提高speech emotion recognition（SER）的可靠性和泛化能力，探讨如何从数据分布角度模型speech emotion。</li>
<li>methods: 本研究提出了一种基于CNN的SER模型，采用了添加marginsoftmax损失函数以提高类别间特征之间的距离，以提高分类的准确性。此外，还提出了一种多个speech attribute控制方法（MSAC），可以控制speech attribute，使模型更加具有感情相关特征。</li>
<li>results: 实验结果表明，提出的SER工作流程在单个和跨库SER场景中具有优秀的认知、泛化和可靠性性能。单个库SER场景中，提出的SER工作流程在IEMOCAP数据集上达到了72.97%的WR和71.76%的UAR。<details>
<summary>Abstract</summary>
Despite significant progress, speech emotion recognition (SER) remains challenging due to inherent complexity and ambiguity of the emotion attribute, particularly in wild world. Whereas current studies primarily focus on recognition and generalization capabilities, this work pioneers an exploration into the reliability of SER methods and investigates how to model the speech emotion from the aspect of data distribution across various speech attributes. Specifically, we first build a novel CNN-based SER model which adopts additive margin softmax loss to expand the distance between features of different classes, thereby enhancing their discrimination. Second, a novel multiple speech attribute control method MSAC is proposed to explicitly control speech attributes, enabling the model to be less affected by emotion-agnostic attributes and capture more fine-grained emotion-related features. Third, we make a first attempt to test and analyze the reliability of the proposed SER workflow using the out-of-distribution detection method. Extensive experiments on both single and cross-corpus SER scenarios show that our proposed unified SER workflow consistently outperforms the baseline in terms of recognition, generalization, and reliability performance. Besides, in single-corpus SER, the proposed SER workflow achieves superior recognition results with a WAR of 72.97\% and a UAR of 71.76\% on the IEMOCAP corpus.
</details>
<details>
<summary>摘要</summary>
尽管已经做出了 significiant 进步，speech emotion recognition（SER）仍然具有挑战性，尤其在野外环境中。当前的研究主要关注recognition和泛化能力，而这个工作则倡导一种探索SER方法的可靠性，并研究如何从数据分布角度来模型speech emotion。具体来说，我们首先构建了一个基于CNN的SER模型，采用添加式margin softmax损失函数，以增强不同类别之间的距离，从而提高 их分辨率。其次，我们提出了一种多个speech attribute控制方法（MSAC），以控制speech attribute，使模型免受情感无关的 attribute的影响，捕捉更细腻的情感相关特征。最后，我们对提议的SER工作流进行了首次可靠性测试和分析，并通过out-of-distribution检测方法进行评估。广泛的实验表明，我们的提议的SER工作流在recognition、泛化和可靠性性能方面均具有优异表现。此外，在单 Corporpus SER 情况下，我们的提议SER工作流的识别率为72.97%和71.76%，在IEMOCAP corpora上。
</details></li>
</ul>
<hr>
<h2 id="Target-Speech-Extraction-with-Conditional-Diffusion-Model"><a href="#Target-Speech-Extraction-with-Conditional-Diffusion-Model" class="headerlink" title="Target Speech Extraction with Conditional Diffusion Model"></a>Target Speech Extraction with Conditional Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03987">http://arxiv.org/abs/2308.03987</a></li>
<li>repo_url: None</li>
<li>paper_authors: Naoyuki Kamo, Marc Delcroix, Tomohiro Nakatani</li>
<li>for: 这篇论文旨在提出一种基于分散模型的目标语音提取方法，用于在多话者杂音场景中提取目标话者的干净语音信号。</li>
<li>methods: 该方法基于一种基于决定器的分散模型，通过条件式地使用决定器来识别目标话者，然后使用分散模型来提取目标话者的语音信号。此外，该方法还使用ensemble推理来降低可能的提取错误。</li>
<li>results: 在Libri2mix数据集上进行实验，提出的分散模型基于TSE方法，结合ensemble推理，与相对的分类式TSE系统相比，显示更高的性能。<details>
<summary>Abstract</summary>
Diffusion model-based speech enhancement has received increased attention since it can generate very natural enhanced signals and generalizes well to unseen conditions. Diffusion models have been explored for several sub-tasks of speech enhancement, such as speech denoising, dereverberation, and source separation. In this paper, we investigate their use for target speech extraction (TSE), which consists of estimating the clean speech signal of a target speaker in a mixture of multi-talkers. TSE is realized by conditioning the extraction process on a clue identifying the target speaker. We show we can realize TSE using a conditional diffusion model conditioned on the clue. Besides, we introduce ensemble inference to reduce potential extraction errors caused by the diffusion process. In experiments on Libri2mix corpus, we show that the proposed diffusion model-based TSE combined with ensemble inference outperforms a comparable TSE system trained discriminatively.
</details>
<details>
<summary>摘要</summary>
模式：简化中文报告：听话提升技术基于分散模型已经受到更多关注，因为它可以生成非常自然的提升信号，并且适用于未经见过的情况。分散模型在各种听话提升任务中被探索，如听话噪声除除、听话反射消除和源分离。在这篇论文中，我们研究了它们在目标听话提取（TSE）任务中的使用，该任务的目标是估计一个混合多个人的听话信号中的清晰听话信号。TSE通过将提取过程conditioning于特征标识target speaker来实现。我们表明可以使用conditioned diffusion model来实现TSE。此外，我们引入ensemble inference来降低扩散过程可能导致的抽取错误。在对Libri2mix数据集进行实验的结果表明，我们提出的扩散模型基于TSE，并与ensemble inference结合，可以与相似的TSE系统进行比较，并且表现更好。
</details></li>
</ul>
<hr>
<h2 id="Universal-Automatic-Phonetic-Transcription-into-the-International-Phonetic-Alphabet"><a href="#Universal-Automatic-Phonetic-Transcription-into-the-International-Phonetic-Alphabet" class="headerlink" title="Universal Automatic Phonetic Transcription into the International Phonetic Alphabet"></a>Universal Automatic Phonetic Transcription into the International Phonetic Alphabet</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03917">http://arxiv.org/abs/2308.03917</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ctaguchi/multipa">https://github.com/ctaguchi/multipa</a></li>
<li>paper_authors: Chihiro Taguchi, Yusuke Sakai, Parisa Haghani, David Chiang</li>
<li>for: 这个研究旨在开发一种能够将任何语言的语音转换为国际音响字母表（IPA）的模型。</li>
<li>methods: 这个模型基于wav2vec 2.0，并在听音输入上进行了微调，以预测IPA。</li>
<li>results: 该模型可以准确地将语音转换为IPA，并且与人工标注的质量相似。<details>
<summary>Abstract</summary>
This paper presents a state-of-the-art model for transcribing speech in any language into the International Phonetic Alphabet (IPA). Transcription of spoken languages into IPA is an essential yet time-consuming process in language documentation, and even partially automating this process has the potential to drastically speed up the documentation of endangered languages. Like the previous best speech-to-IPA model (Wav2Vec2Phoneme), our model is based on wav2vec 2.0 and is fine-tuned to predict IPA from audio input. We use training data from seven languages from CommonVoice 11.0, transcribed into IPA semi-automatically. Although this training dataset is much smaller than Wav2Vec2Phoneme's, its higher quality lets our model achieve comparable or better results. Furthermore, we show that the quality of our universal speech-to-IPA models is close to that of human annotators.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种最新的语音转写模型，可以将任何语言的语音转写为国际音域字母（IPA）。将口语语言转写为IPA是语言记录的重要步骤，但是这个过程占用了大量时间。即使只是部分自动化这个过程，也可以快速化语言记录的过程，特别是对于受威胁的语言。我们的模型基于wav2vec 2.0，并在音频输入上进行了微调，以预测IPA。我们使用了CommonVoice 11.0中的七种语言的训练数据，这些数据被 semi-automatically 转写为IPA。虽然这个训练集比前一个最佳语音到IPA模型（Wav2Vec2Phoneme）更小，但是它的质量更高，使我们的模型在比较或更好的结果。此外，我们还证明了我们的通用语音到IPA模型的质量与人工标注几乎相同。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/08/cs.SD_2023_08_08/" data-id="clly4xteo009svl8816l7eqan" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_08_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/08/eess.AS_2023_08_08/" class="article-date">
  <time datetime="2023-08-07T16:00:00.000Z" itemprop="datePublished">2023-08-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/08/eess.AS_2023_08_08/">eess.AS - 2023-08-08 22:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Investigating-Speaker-Embedding-Disentanglement-on-Natural-Read-Speech"><a href="#Investigating-Speaker-Embedding-Disentanglement-on-Natural-Read-Speech" class="headerlink" title="Investigating Speaker Embedding Disentanglement on Natural Read Speech"></a>Investigating Speaker Embedding Disentanglement on Natural Read Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04225">http://arxiv.org/abs/2308.04225</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Kuhlmann, Adrian Meise, Fritz Seebauer, Petra Wagner, Reinhold Haeb-Umbach</li>
<li>for:  investigate the degree to which speech representations encoding speaker identity can be disentangled</li>
<li>methods: use standard objectives promoting disentanglement and compare with vanilla representation learning</li>
<li>results: limited disentanglement of the speaker embedding when using standard objectives, but some improvement with vanilla representation learning<details>
<summary>Abstract</summary>
Disentanglement is the task of learning representations that identify and separate factors that explain the variation observed in data. Disentangled representations are useful to increase the generalizability, explainability, and fairness of data-driven models. Only little is known about how well such disentanglement works for speech representations. A major challenge when tackling disentanglement for speech representations are the unknown generative factors underlying the speech signal. In this work, we investigate to what degree speech representations encoding speaker identity can be disentangled. To quantify disentanglement, we identify acoustic features that are highly speaker-variant and can serve as proxies for the factors of variation underlying speech. We find that disentanglement of the speaker embedding is limited when trained with standard objectives promoting disentanglement but can be improved over vanilla representation learning to some extent.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language=zh-CN<</SYS>>文本：分离是学习表示的任务，以分解数据中变化的因素为目的。分离的表示可以提高数据驱动模型的通用性、可读性和公平性。然而，对于语音表示来说，尚未知 много关于如何实现分离。在这项工作中，我们研究了基于说话者标识的语音表示是否可以分离。为了衡量分离程度，我们标识出了具有高度说话者特征的音频特征，这些特征可以作为变化的因素下的 фактор代表。我们发现，通过标准的分离目标训练，说话者嵌入可以有限地分离，但可以通过一些程度上的改进来提高一些。
</details></li>
</ul>
<hr>
<h2 id="EPCFormer-Expression-Prompt-Collaboration-Transformer-for-Universal-Referring-Video-Object-Segmentation"><a href="#EPCFormer-Expression-Prompt-Collaboration-Transformer-for-Universal-Referring-Video-Object-Segmentation" class="headerlink" title="EPCFormer: Expression Prompt Collaboration Transformer for Universal Referring Video Object Segmentation"></a>EPCFormer: Expression Prompt Collaboration Transformer for Universal Referring Video Object Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04162">http://arxiv.org/abs/2308.04162</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lab206/epcformer">https://github.com/lab206/epcformer</a></li>
<li>paper_authors: Jiajun Chen, Jiacheng Lin, Zhiqiang Xiao, Haolong Fu, Ke Nai, Kailun Yang, Zhiyong Li</li>
<li>for: 本研究旨在提出一种能够同时实现高精度地 segmentation 和交互 flexibility 的方法，用于解决现代方法在不同模式之间的表示模型化问题。</li>
<li>methods: 本研究提出了两种方法：一是 Expression Prompt Collaboration Transformer（EPCFormer）架构，二是 Expression Alignment（EA）机制，用于同时利用语音和文本表达的 semantic equivalence 来提高 segmentation 精度。</li>
<li>results: 实验结果表明，通过在语音、文本和视频特征之间进行深度交互，EPCFormer 可以同时实现高精度的 Video Object Segmentation 和 Referring Video Object Segmentation 任务，并且在两个任务上均 achieve state-of-the-art Result。<details>
<summary>Abstract</summary>
Audio-guided Video Object Segmentation (A-VOS) and Referring Video Object Segmentation (R-VOS) are two highly-related tasks, which both aim to segment specific objects from video sequences according to user-provided expression prompts. However, due to the challenges in modeling representations for different modalities, contemporary methods struggle to strike a balance between interaction flexibility and high-precision localization and segmentation. In this paper, we address this problem from two perspectives: the alignment representation of audio and text and the deep interaction among audio, text, and visual features. First, we propose a universal architecture, the Expression Prompt Collaboration Transformer, herein EPCFormer. Next, we propose an Expression Alignment (EA) mechanism for audio and text expressions. By introducing contrastive learning for audio and text expressions, the proposed EPCFormer realizes comprehension of the semantic equivalence between audio and text expressions denoting the same objects. Then, to facilitate deep interactions among audio, text, and video features, we introduce an Expression-Visual Attention (EVA) mechanism. The knowledge of video object segmentation in terms of the expression prompts can seamlessly transfer between the two tasks by deeply exploring complementary cues between text and audio. Experiments on well-recognized benchmarks demonstrate that our universal EPCFormer attains state-of-the-art results on both tasks. The source code of EPCFormer will be made publicly available at https://github.com/lab206/EPCFormer.
</details>
<details>
<summary>摘要</summary>
audio-guided video对象分割(A-VOS)和引用video对象分割(R-VOS)是两个非常相关的任务，它们都是根据用户提供的表达提示分割视频序列中的特定对象。然而，由于不同模式之间的表示模型化困难，当前方法很难平衡用户交互的灵活性和高精度的本地化和分割。在这篇论文中，我们解决这个问题从两个方面：表达提示的对Alignment和深度交互。首先，我们提出一个通用架构，即表达Prompt Collaboration Transformer（EPCFormer）。然后，我们提出一个表达对接（EA）机制，用于对音频和文本表达进行对接。通过对音频和文本表达进行对比学习，我们的EPCFormer实现了对音频和文本表达的semantic相似性的认知。然后，为了促进音频、文本和视频特征之间的深度交互，我们引入表达-视觉注意力（EVA）机制。通过深入探索音频、文本和视频特征之间的相互补做，我们的EPCFormer可以快速 Transfer learning between the two tasks by leveraging complementary cues between text and audio.我们的实验表明，我们的通用EPCFormer在两个任务上都达到了状态艺术的Result。我们将EPCFormer的源代码公开在GitHub上，地址为https://github.com/lab206/EPCFormer。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/08/eess.AS_2023_08_08/" data-id="clly4xtfb00c4vl885yj9eudf" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/08/eess.IV_2023_08_08/" class="article-date">
  <time datetime="2023-08-07T16:00:00.000Z" itemprop="datePublished">2023-08-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/08/eess.IV_2023_08_08/">eess.IV - 2023-08-08 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Blur-aware-metric-depth-estimation-with-multi-focus-plenoptic-cameras"><a href="#Blur-aware-metric-depth-estimation-with-multi-focus-plenoptic-cameras" class="headerlink" title="Blur aware metric depth estimation with multi-focus plenoptic cameras"></a>Blur aware metric depth estimation with multi-focus plenoptic cameras</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04252">http://arxiv.org/abs/2308.04252</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/comsee-research/blade">https://github.com/comsee-research/blade</a></li>
<li>paper_authors: Mathieu Labussière, Céline Teulière, Omar Ait-Aider</li>
<li>for: 这篇论文的目的是提出一种基于raw图像的多Focus plenoptic摄像机的metric depth estimation算法，以提高对不同距离物体的depth estimation。</li>
<li>methods: 该算法使用了 correspondence和defocus规则，并利用了模糊信息来提高depth estimation。具体来说， authors derivied一个 inverse projection模型，包括了defocus模糊，并提出了一种Calibration方法来实现精确的depth scaling。</li>
<li>results: 论文的实验结果表明，通过引入模糊信息，可以提高depth estimation的准确性。 authors还进行了实验，并证明了该算法在实际场景中的效果。<details>
<summary>Abstract</summary>
While a traditional camera only captures one point of view of a scene, a plenoptic or light-field camera, is able to capture spatial and angular information in a single snapshot, enabling depth estimation from a single acquisition. In this paper, we present a new metric depth estimation algorithm using only raw images from a multi-focus plenoptic camera. The proposed approach is especially suited for the multi-focus configuration where several micro-lenses with different focal lengths are used. The main goal of our blur aware depth estimation (BLADE) approach is to improve disparity estimation for defocus stereo images by integrating both correspondence and defocus cues. We thus leverage blur information where it was previously considered a drawback. We explicitly derive an inverse projection model including the defocus blur providing depth estimates up to a scale factor. A method to calibrate the inverse model is then proposed. We thus take into account depth scaling to achieve precise and accurate metric depth estimates. Our results show that introducing defocus cues improves the depth estimation. We demonstrate the effectiveness of our framework and depth scaling calibration on relative depth estimation setups and on real-world 3D complex scenes with ground truth acquired with a 3D lidar scanner.
</details>
<details>
<summary>摘要</summary>
while a traditional camera only captures one point of view of a scene, a plenoptic or light-field camera can capture spatial and angular information in a single snapshot, enabling depth estimation from a single acquisition. in this paper, we present a new metric depth estimation algorithm using only raw images from a multi-focus plenoptic camera. the proposed approach is especially suited for the multi-focus configuration where several micro-lenses with different focal lengths are used. the main goal of our blur-aware depth estimation (blade) approach is to improve disparity estimation for defocus stereo images by integrating both correspondence and defocus cues. we thus leverage blur information where it was previously considered a drawback. we explicitly derive an inverse projection model including the defocus blur providing depth estimates up to a scale factor. a method to calibrate the inverse model is then proposed. we thus take into account depth scaling to achieve precise and accurate metric depth estimates. our results show that introducing defocus cues improves the depth estimation. we demonstrate the effectiveness of our framework and depth scaling calibration on relative depth estimation setups and on real-world 3d complex scenes with ground truth acquired with a 3d lidar scanner.
</details></li>
</ul>
<hr>
<h2 id="Under-Display-Camera-Image-Restoration-with-Scattering-Effect"><a href="#Under-Display-Camera-Image-Restoration-with-Scattering-Effect" class="headerlink" title="Under-Display Camera Image Restoration with Scattering Effect"></a>Under-Display Camera Image Restoration with Scattering Effect</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04163">http://arxiv.org/abs/2308.04163</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/namecantbenull/srudc">https://github.com/namecantbenull/srudc</a></li>
<li>paper_authors: Binbin Song, Xiangyu Chen, Shuning Xu, Jiantao Zhou</li>
<li>for: 提供全屏视频经验而不受缺口或孔洞的干扰，但是半透明显示器引入了严重的图像干扰。</li>
<li>methods: 使用物理干扰模型来模拟显示器的散射效应，并改进图像synthesis的形成管道来构建真实的UDC数据集。</li>
<li>results: 提出了一种基于自注意力的两极分支网络，其中散射分支利用通道级自注意力来估算散射效应的参数，而图像分支利用CNN来恢复清晰的场景图像。对实验和 sinthezied 数据进行了广泛的测试，并证明了该方法在UDC图像恢复方面的优越性。<details>
<summary>Abstract</summary>
The under-display camera (UDC) provides consumers with a full-screen visual experience without any obstruction due to notches or punched holes. However, the semi-transparent nature of the display inevitably introduces the severe degradation into UDC images. In this work, we address the UDC image restoration problem with the specific consideration of the scattering effect caused by the display. We explicitly model the scattering effect by treating the display as a piece of homogeneous scattering medium. With the physical model of the scattering effect, we improve the image formation pipeline for the image synthesis to construct a realistic UDC dataset with ground truths. To suppress the scattering effect for the eventual UDC image recovery, a two-branch restoration network is designed. More specifically, the scattering branch leverages global modeling capabilities of the channel-wise self-attention to estimate parameters of the scattering effect from degraded images. While the image branch exploits the local representation advantage of CNN to recover clear scenes, implicitly guided by the scattering branch. Extensive experiments are conducted on both real-world and synthesized data, demonstrating the superiority of the proposed method over the state-of-the-art UDC restoration techniques. The source code and dataset are available at \url{https://github.com/NamecantbeNULL/SRUDC}.
</details>
<details>
<summary>摘要</summary>
“display under-camera（UDC）允许消耗者欣赏全屏视觉体验，不受萤幕上的不透明或孔径所阻碍。然而，半透明的萤幕将导致UDC图像受到严重的损坏。在这个工作中，我们解决UDC图像修复问题，特别是考虑到类推效应。我们明确地模型类推效应，将类推效应视为一个 homogeneous 散射媒体。使用物理模型，我们改善图像形成管道，以建立一个实际的 UDC 数据集，并提供真实的参考数据。为了抑制类推效应，我们设计了两条分支网络。更 specifically，类推分支利用通道wise self-attention 的全局模型能力，从受损图像中估计类推效应的参数。另一方面，图像分支利用 CNN 的地方表现优势，复原清晰的景象，协力地 guid 由类推分支。我们对真实和 sinthe 的数据进行了广泛的实验，展示了我们的方法与现有的 UDC 修复技术相比，具有superiority。请参考 \url{https://github.com/NamecantbeNULL/SRUDC} 以下载取源代码和数据。”
</details></li>
</ul>
<hr>
<h2 id="Towards-Top-Down-Stereoscopic-Image-Quality-Assessment-via-Stereo-Attention"><a href="#Towards-Top-Down-Stereoscopic-Image-Quality-Assessment-via-Stereo-Attention" class="headerlink" title="Towards Top-Down Stereoscopic Image Quality Assessment via Stereo Attention"></a>Towards Top-Down Stereoscopic Image Quality Assessment via Stereo Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04156">http://arxiv.org/abs/2308.04156</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fanning-zhang/satnet">https://github.com/fanning-zhang/satnet</a></li>
<li>paper_authors: Huilin Zhang, Sumei Li, Yongli Chang</li>
<li>for: This paper proposes a novel network for stereoscopic image quality assessment (SIQA) that utilizes a top-down approach to guide the quality assessment process.</li>
<li>methods: The proposed method employs a stereo attention mechanism that fuses high-level binocular signals with low-level monocular signals, and utilizes an energy coefficient to adaptively tune the magnitude of binocular responses. The method also utilizes a dual-pooling strategy to extract the most discriminative quality information from the two branches of monocular features.</li>
<li>results: Experimental results show that the proposed top-down method outperforms existing bottom-up methods in simulating the property of visual perception and advancing the state-of-the-art in the SIQA field.<details>
<summary>Abstract</summary>
Stereoscopic image quality assessment (SIQA) plays a crucial role in evaluating and improving the visual experience of 3D content. Existing binocular properties and attention-based methods for SIQA have achieved promising performance. However, these bottom-up approaches are inadequate in exploiting the inherent characteristics of the human visual system (HVS). This paper presents a novel network for SIQA via stereo attention, employing a top-down perspective to guide the quality assessment process. Our proposed method realizes the guidance from high-level binocular signals down to low-level monocular signals, while the binocular and monocular information can be calibrated progressively throughout the processing pipeline. We design a generalized Stereo AttenTion (SAT) block to implement the top-down philosophy in stereo perception. This block utilizes the fusion-generated attention map as a high-level binocular modulator, influencing the representation of two low-level monocular features. Additionally, we introduce an Energy Coefficient (EC) to account for recent findings indicating that binocular responses in the primate primary visual cortex are less than the sum of monocular responses. The adaptive EC can tune the magnitude of binocular response flexibly, thus enhancing the formation of robust binocular features within our framework. To extract the most discriminative quality information from the summation and subtraction of the two branches of monocular features, we utilize a dual-pooling strategy that applies min-pooling and max-pooling operations to the respective branches. Experimental results highlight the superiority of our top-down method in simulating the property of visual perception and advancing the state-of-the-art in the SIQA field. The code of this work is available at https://github.com/Fanning-Zhang/SATNet.
</details>
<details>
<summary>摘要</summary>
三维内容的视觉体验评估（SIQA）在评估和改进三维内容的视觉体验方面发挥关键性的作用。现有的异常性和注意力基于的方法已经实现了有前途的表现。然而，这些底层方法无法充分利用人类视觉系统（HVS）的内在特征。本文提出了一种新的网络 для SIQA，通过双目注意力来引导评估过程。我们的提议方法可以从高级双目信号下降到低级单目信号，而双目和单目信息可以在处理管道中进行满足进行进度性规整。我们设计了一种通用的双目注意力块（SAT）来实现上述哲学。这个块使用生成的注意力地图作为高级双目调制器，影响两个低级单目特征表示。此外，我们引入了能量系数（EC），以考虑实验发现，双目响应在人类初级视觉层中小于单目响应的现象。可以根据实际情况灵活调整EC的大小，从而提高在我们框架中形成的可靠双目特征。为了从两个分支中提取最有价值的质量信息，我们采用了双pooling策略，将各个分支的最小和最大池化操作应用于相应的分支。实验结果表明，我们的底层方法在模拟视觉启发和提高SIQA领域的状态而且取得了更好的表现。代码可以在https://github.com/Fanning-Zhang/SATNet上获取。
</details></li>
</ul>
<hr>
<h2 id="Physics-driven-universal-twin-image-removal-network-for-digital-in-line-holographic-microscopy"><a href="#Physics-driven-universal-twin-image-removal-network-for-digital-in-line-holographic-microscopy" class="headerlink" title="Physics-driven universal twin-image removal network for digital in-line holographic microscopy"></a>Physics-driven universal twin-image removal network for digital in-line holographic microscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04471">http://arxiv.org/abs/2308.04471</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikołaj Rogalski, Piotr Arcab, Luiza Stanaszek, Vicente Micó, Chao Zuo, Maciej Trusiak</li>
<li>for: 这项研究旨在开发一种能够高效、成本低的数字探针式干涉 Microscopy（DIHM）中的量子阶段成像技术，以便研究细胞运动、迁徙和生物微流体动力学。</li>
<li>methods: 该研究使用深度学习解决方案UTIRnet，可以快速、稳定、universally applicable地纹理双像干涉，并且通过数字生成的数据进行训练。</li>
<li>results: 实验证明，UTIRnet可以快速、稳定地抑制双像干涉，并且保持输入干涉图像的一致性，从而提高了计算量相对于传统深度学习方法的可靠性。此外，UTIRnet在live neural glial cell culture migration感知中得到了实验证明。<details>
<summary>Abstract</summary>
Digital in-line holographic microscopy (DIHM) enables efficient and cost-effective computational quantitative phase imaging with a large field of view, making it valuable for studying cell motility, migration, and bio-microfluidics. However, the quality of DIHM reconstructions is compromised by twin-image noise, posing a significant challenge. Conventional methods for mitigating this noise involve complex hardware setups or time-consuming algorithms with often limited effectiveness. In this work, we propose UTIRnet, a deep learning solution for fast, robust, and universally applicable twin-image suppression, trained exclusively on numerically generated datasets. The availability of open-source UTIRnet codes facilitates its implementation in various DIHM systems without the need for extensive experimental training data. Notably, our network ensures the consistency of reconstruction results with input holograms, imparting a physics-based foundation and enhancing reliability compared to conventional deep learning approaches. Experimental verification was conducted among others on live neural glial cell culture migration sensing, which is crucial for neurodegenerative disease research.
</details>
<details>
<summary>摘要</summary>
数字内线投射微镜（DIHM）可提供高效且成本下降的计算量相图像，大幅提高了生物学研究中cell motility、迁徙和生物微流体等领域的研究价值。然而，DIHM重建的质量受到双像噪声的限制，这成为一大挑战。传统的方法对这种噪声进行缓解通常包括复杂的硬件设置或时间消耗的算法，效果不一定。在这种工作中，我们提出了UTIRnet，一种深度学习解决方案，能够快速、稳定、universally aplicable的双像抑制。我们的网络具有数字生成的数据集进行培训，不需要大量的实验室训练数据。特别是，我们的网络保证重建结果与输入投射图像之间的一致性，从而增强了physics-based的基础和可靠性，与传统的深度学习方法相比。实验证明包括live neural glial cell culture migration感知等，这些研究对 neuroscience disease 有重要意义。
</details></li>
</ul>
<hr>
<h2 id="Single-shot-experimental-numerical-twin-image-removal-in-lensless-digital-holographic-microscopy"><a href="#Single-shot-experimental-numerical-twin-image-removal-in-lensless-digital-holographic-microscopy" class="headerlink" title="Single-shot experimental-numerical twin-image removal in lensless digital holographic microscopy"></a>Single-shot experimental-numerical twin-image removal in lensless digital holographic microscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04131">http://arxiv.org/abs/2308.04131</a></li>
<li>repo_url: None</li>
<li>paper_authors: Piotr Arcab, Mikolaj Rogalski, Maciej Trusiak</li>
<li>for: 这篇论文旨在提出一种新的单shot实验数字折射镜微scopic技术，用于解决折射镜微scopic图像中的双像问题。</li>
<li>methods: 该技术基于两个源的偏心折射agram记录，使用简单的纤维分 splitting器实现。此外，该技术还提出了一种专门为获取的HOLOGRAMS数据进行 phase retrieval numerical algorithm，以提供无双像的重建。</li>
<li>results: 作者通过对频率测试目标和唾液细胞样本进行质量和量化验证，证明了该技术可以提供低成本、外围实验室的LDHM成像，并且可以提高图像的精度。这些结果开创了新的可靠性和生物医学成像应用领域，特别是在成本效益和可搬运性是关键的场景下。<details>
<summary>Abstract</summary>
Lensless digital holographic microscopy (LDHM) offers very large field-of-view label-free imaging crucial, e.g., in high-throughput particle tracking and biomedical examination of cells and tissues. Compact layouts promote point-of-case and out-of-laboratory applications. The LDHM, based on the Gabor in-line holographic principle, is inherently spoiled by the twin-image effect, which complicates the quantitative analysis of reconstructed phase and amplitude maps. Popular family of solutions consists of numerical methods, which tend to minimize twin-image upon iterative process based on data redundancy. Additional hologram recordings are needed, and final results heavily depend on the algorithmic parameters, however. In this contribution we present a novel single-shot experimental-numerical twin-image removal technique for LDHM. It leverages two-source off-axis hologram recording deploying simple fiber splitter. Additionally, we introduce a novel phase retrieval numerical algorithm specifically tailored to the acquired holograms, that provides twin-image-free reconstruction without compromising the resolution. We quantitatively and qualitatively verify proposed method employing phase test target and cheek cells biosample. The results demonstrate that the proposed technique enables low-cost, out-of-laboratory LDHM imaging with enhanced precision, achieved through the elimination of twin-image errors. This advancement opens new avenues for more accurate technical and biomedical imaging applications using LDHM, particularly in scenarios where cost-effective and portable imaging solutions are desired.
</details>
<details>
<summary>摘要</summary>
凡�жен数字投射icroscopy (LDHM) 提供了非常大的视场label-free 图像，对于高通过率粒子跟踪和生物医学Cells和组织的检查非常重要。 紧凑的设计使得点检测和出 laboratory 应用更加容易。基于Gabor直线投射原理的 LDHM 由于双像效应而受到质量分析重建phasemap和ampliute map中的干扰。通过数字方法来减少双像效应，但需要多个捕获图像，并且算法参数会影响最终结果。在这篇论文中，我们提出了一种新的单shot实验numerical twin-image removal技术，利用两个偏移 angle 的折分纤维Splitter来记录两个源的偏移 angle 折分图像。此外，我们还提出了一种专门为获取的ho lograms而设计的phaserecovery数字算法，可以在无需多个捕获图像的情况下提供无双像效应的重建结果，而且不会减少分辨率。我们使用phasetest target和唾液细胞样本进行量化和质量检测，结果表明，我们的方法可以提供低成本、出 laboratory的LDHM映像，并且提高了精度。这一进展打开了更多的技术和生物医学应用的可能性，特别是在成本效益和可搬式设备的情况下。
</details></li>
</ul>
<hr>
<h2 id="Non-Intrusive-Electric-Load-Monitoring-Approach-Based-on-Current-Feature-Visualization-for-Smart-Energy-Management"><a href="#Non-Intrusive-Electric-Load-Monitoring-Approach-Based-on-Current-Feature-Visualization-for-Smart-Energy-Management" class="headerlink" title="Non-Intrusive Electric Load Monitoring Approach Based on Current Feature Visualization for Smart Energy Management"></a>Non-Intrusive Electric Load Monitoring Approach Based on Current Feature Visualization for Smart Energy Management</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11627">http://arxiv.org/abs/2308.11627</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiwen Xu, Dengfeng Liu, Liangtao Huang, Zhiquan Lin, Tiesong Zhao, Sam Kwong</li>
<li>for: 这篇论文的目的是为智能城市的电力系统进行经济可效的能源管理。特别是监控和分析所有用户的电力负载。</li>
<li>methods: 这篇论文使用了人工智能的受欢迎计算机视觉技术，设计了一种非侵入式负载监控方法。首先，使用信号变换（包括波лет变换和离散傅立叶变换）和gramianangular场（GAF）方法将一维现在信号映射到二维色彩特征图像。其次，提出了使用U型深度神经网络，具有多尺度特征提取和注意机制，从色彩特征图像中识别所有的电力负载。</li>
<li>results: 实验结果显示，该方法在公共和私人数据集上均 achieves superior performance，并支持大规模互联网的非侵入式监控。<details>
<summary>Abstract</summary>
The state-of-the-art smart city has been calling for an economic but efficient energy management over large-scale network, especially for the electric power system. It is a critical issue to monitor, analyze and control electric loads of all users in system. In this paper, we employ the popular computer vision techniques of AI to design a non-invasive load monitoring method for smart electric energy management. First of all, we utilize both signal transforms (including wavelet transform and discrete Fourier transform) and Gramian Angular Field (GAF) methods to map one-dimensional current signals onto two-dimensional color feature images. Second, we propose to recognize all electric loads from color feature images using a U-shape deep neural network with multi-scale feature extraction and attention mechanism. Third, we design our method as a cloud-based, non-invasive monitoring of all users, thereby saving energy cost during electric power system control. Experimental results on both public and our private datasets have demonstrated our method achieves superior performances than its peers, and thus supports efficient energy management over large-scale Internet of Things (IoT).
</details>
<details>
<summary>摘要</summary>
现代智能城市需要一种经济高效的能源管理方法，特别是电力系统。监测、分析和控制所有用户的电压信号是一个关键问题。在这篇论文中，我们使用了人工智能的流行计算机视觉技术，设计了一种不侵入式的负荷监测方法。首先，我们使用了声波变换（包括wavelet transform和Discrete Fourier Transform）和 Gramian Angular Field（GAF）方法将一维电流信号映射到二维颜色特征图像上。其次，我们提议使用U型深度神经网络，并提取多个尺度的特征和注意机制来识别所有的电荷。最后，我们设计了一种云端的、不侵入式的监测方法，从而在电力系统控制中节省能源成本。实验结果表明，我们的方法在公共和私人数据集上达到了比其他方法更高的性能，因此支持了大规模互联网智能（IoT）中的高效能源管理。
</details></li>
</ul>
<hr>
<h2 id="Weakly-Semi-Supervised-Detection-in-Lung-Ultrasound-Videos"><a href="#Weakly-Semi-Supervised-Detection-in-Lung-Ultrasound-Videos" class="headerlink" title="Weakly Semi-Supervised Detection in Lung Ultrasound Videos"></a>Weakly Semi-Supervised Detection in Lung Ultrasound Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04463">http://arxiv.org/abs/2308.04463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiahong Ouyang, Li Chen, Gary Y. Li, Naveen Balaraju, Shubham Patil, Courosh Mehanian, Sourabh Kulhare, Rachel Millin, Kenton W. Gregory, Cynthia R. Gregory, Meihua Zhu, David O. Kessler, Laurie Malia, Almaz Dessie, Joni Rabiner, Di Coneybeare, Bo Shopsin, Andrew Hersh, Cristian Madar, Jeffrey Shupp, Laura S. Johnson, Jacob Avila, Kristin Dwyer, Peter Weimersheimer, Balasundar Raju, Jochen Kruecker, Alvin Chen</li>
<li>for: 医学视频数据上进行全supervised对象检测模型训练中需要临床专家进行 Frame-by-frame 注解 boundling box.</li>
<li>methods: 我们提出了一种基于弱级别 labels 的方法，将个体检测预测结果聚合到视频级别预测中，并通过视频级别损失来提供额外约束。我们还介绍了一些改进 pseudo-labels 的方法，以及适应性调整知识传递Student 和 teacher 网络之间的调整方案。</li>
<li>results: 我们在医学超声视频中检测肺聚集（如 COVID-19 肺炎）的方法中应用了这种方法，实验表明，我们的框架可以提高检测精度和鲁棒性，并提高数据和注解使用效率。<details>
<summary>Abstract</summary>
Frame-by-frame annotation of bounding boxes by clinical experts is often required to train fully supervised object detection models on medical video data. We propose a method for improving object detection in medical videos through weak supervision from video-level labels. More concretely, we aggregate individual detection predictions into video-level predictions and extend a teacher-student training strategy to provide additional supervision via a video-level loss. We also introduce improvements to the underlying teacher-student framework, including methods to improve the quality of pseudo-labels based on weak supervision and adaptive schemes to optimize knowledge transfer between the student and teacher networks. We apply this approach to the clinically important task of detecting lung consolidations (seen in respiratory infections such as COVID-19 pneumonia) in medical ultrasound videos. Experiments reveal that our framework improves detection accuracy and robustness compared to baseline semi-supervised models, and improves efficiency in data and annotation usage.
</details>
<details>
<summary>摘要</summary>
<SYS>医学视频数据上进行完全监督物体检测模型训练常需要 клиниче专家进行框架帧标注。我们提出一种基于弱监督的方法，通过视频级别标签来改进医学视频中的物体检测。更具体来说，我们将个体检测预测结果聚合成视频级别预测，并通过视频级别损失来提供额外监督。我们还介绍了改进 teacher-student 框架的方法，包括基于弱监督的pseudo标签质量改进和 adaptive 优化学习知识传递between teacher和学生网络。我们在识别尘埃散发（常见于呼吸道感染，如 COVID-19 肺炎）的医学超声视频中应用这种方法。实验表明，我们的框架可以提高检测精度和可靠性，并提高数据和标注的使用效率。</SYS>Here's the text with some minor adjustments to make it more natural in Simplified Chinese:<SYS>医学视频数据上训练完全监督物体检测模型时，常常需要 клиниче专家进行框架帧标注。我们提出一种基于弱监督的方法，通过视频级别标签来改进医学视频中的物体检测。更具体来说，我们将个体检测预测结果聚合成视频级别预测，并通过视频级别损失来提供额外监督。我们还介绍了改进 teacher-student 框架的方法，包括基于弱监督的pseudo标签质量改进和 adaptive 优化学习知识传递between teacher和学生网络。我们在识别尘埃散发（常见于呼吸道感染，如 COVID-19 肺炎）的医学超声视频中应用这种方法。实验表明，我们的框架可以提高检测精度和可靠性，并提高数据和标注的使用效率。</SYS>Please note that the translation is based on the original text and may not capture all the nuances and details of the original text.
</details></li>
</ul>
<hr>
<h2 id="DefCor-Net-Physics-Aware-Ultrasound-Deformation-Correction"><a href="#DefCor-Net-Physics-Aware-Ultrasound-Deformation-Correction" class="headerlink" title="DefCor-Net: Physics-Aware Ultrasound Deformation Correction"></a>DefCor-Net: Physics-Aware Ultrasound Deformation Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03865">http://arxiv.org/abs/2308.03865</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/karolinezhy/defcornet">https://github.com/karolinezhy/defcornet</a></li>
<li>paper_authors: Zhongliang Jiang, Yue Zhou, Dongliang Cao, Nassir Navab</li>
<li>for:  correction of deformed anatomical images in ultrasound (US) image acquisition</li>
<li>methods: multi-scale deep neural network (DefCor-Net) with biomedical knowledge and real-time estimation of pixel-wise tissue properties</li>
<li>results: significant improvement in accuracy of deformation correction (Dice Coefficient: from $14.3\pm20.9$ to $82.6\pm12.1$ when the force is $6N$)<details>
<summary>Abstract</summary>
The recovery of morphologically accurate anatomical images from deformed ones is challenging in ultrasound (US) image acquisition, but crucial to accurate and consistent diagnosis, particularly in the emerging field of computer-assisted diagnosis. This article presents a novel anatomy-aware deformation correction approach based on a coarse-to-fine, multi-scale deep neural network (DefCor-Net). To achieve pixel-wise performance, DefCor-Net incorporates biomedical knowledge by estimating pixel-wise stiffness online using a U-shaped feature extractor. The deformation field is then computed using polynomial regression by integrating the measured force applied by the US probe. Based on real-time estimation of pixel-by-pixel tissue properties, the learning-based approach enables the potential for anatomy-aware deformation correction. To demonstrate the effectiveness of the proposed DefCor-Net, images recorded at multiple locations on forearms and upper arms of six volunteers are used to train and validate DefCor-Net. The results demonstrate that DefCor-Net can significantly improve the accuracy of deformation correction to recover the original geometry (Dice Coefficient: from $14.3\pm20.9$ to $82.6\pm12.1$ when the force is $6N$).
</details>
<details>
<summary>摘要</summary>
医学影像中的形态准确度恢复问题在ultrasound（US）图像获取中是一个挑战，但是对医学诊断的准确性和一致性很重要，特别是在计算机助理诊断领域。这篇文章提出了一种基于多尺度深度神经网络（DefCor-Net）的新型形态意识恢复方法。为了实现像素级性能，DefCor-Net在核心网络中加入了生物医学知识，通过在U型特征提取器中线性估计每个像素的刚性来实现。然后，通过积分测量US probes应用的力场来计算扭formation场。基于实时测量像素级刚性特性，这种学习基于的方法具有可能性 для形态意识恢复。为了证明DefCor-Net的效果，文章使用了多个臂部和上臂部的六名志愿者记录的图像进行训练和验证。结果表明，DefCor-Net可以显著提高恢复原geometry的准确度（Dice Coefficient：从14.3±20.9到82.6±12.1，当力场为6N）。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/08/eess.IV_2023_08_08/" data-id="clly4xtg600ervl88a9elf1nj" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/07/cs.LG_2023_08_07/" class="article-date">
  <time datetime="2023-08-06T16:00:00.000Z" itemprop="datePublished">2023-08-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/07/cs.LG_2023_08_07/">cs.LG - 2023-08-07 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Improving-FHB-Screening-in-Wheat-Breeding-Using-an-Efficient-Transformer-Model"><a href="#Improving-FHB-Screening-in-Wheat-Breeding-Using-an-Efficient-Transformer-Model" class="headerlink" title="Improving FHB Screening in Wheat Breeding Using an Efficient Transformer Model"></a>Improving FHB Screening in Wheat Breeding Using an Efficient Transformer Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03670">http://arxiv.org/abs/2308.03670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Babak Azad, Ahmed Abdalla, Kwanghee Won, Ali Mirzakhani Nafchi</li>
<li>for: 这个研究是为了提高小麦和黑麦抗病耐菌creening中的效率、准确率和时间探测病菌病 head blight（FHB）。</li>
<li>methods: 这个研究使用了一种新的Context Bridge，将U-Net网络的本地表现力与transformer模型的全球自注意力机制相结合，以提高病菌探测的精度和效率。此外，原始transformer模型中的标准注意力机制被更改为Efficient Self-attention，以减少其复杂性。</li>
<li>results: 经过广泛的实验和评估，研究发现transformer-based方法在小麦病菌探测中具有高效率和准确率，并且可以处理不同类型的植物形态和病变。<details>
<summary>Abstract</summary>
Fusarium head blight is a devastating disease that causes significant economic losses annually on small grains. Efficiency, accuracy, and timely detection of FHB in the resistance screening are critical for wheat and barley breeding programs. In recent years, various image processing techniques have been developed using supervised machine learning algorithms for the early detection of FHB. The state-of-the-art convolutional neural network-based methods, such as U-Net, employ a series of encoding blocks to create a local representation and a series of decoding blocks to capture the semantic relations. However, these methods are not often capable of long-range modeling dependencies inside the input data, and their ability to model multi-scale objects with significant variations in texture and shape is limited. Vision transformers as alternative architectures with innate global self-attention mechanisms for sequence-to-sequence prediction, due to insufficient low-level details, may also limit localization capabilities. To overcome these limitations, a new Context Bridge is proposed to integrate the local representation capability of the U-Net network in the transformer model. In addition, the standard attention mechanism of the original transformer is replaced with Efficient Self-attention, which is less complicated than other state-of-the-art methods. To train the proposed network, 12,000 wheat images from an FHB-inoculated wheat field at the SDSU research farm in Volga, SD, were captured. In addition to healthy and unhealthy plants, these images encompass various stages of the disease. A team of expert pathologists annotated the images for training and evaluating the developed model. As a result, the effectiveness of the transformer-based method for FHB-disease detection, through extensive experiments across typical tasks for plant image segmentation, is demonstrated.
</details>
<details>
<summary>摘要</summary>
fusarium 头疫是一种致命的病种，每年对小麦和黑麦产生了重大经济损失。在抗性屏测中，效率、准确性和时效检测是关键。在最近几年，各种图像处理技术被开发，使用超级vised机器学习算法进行早期检测。state-of-the-art的卷积神经网络方法，如U-Net，使用一系列的编码块来创建本地表示，并使用一系列的解码块来捕捉 semantic关系。然而，这些方法通常无法长距离模型数据中的相互关系，其能模型多尺度对象的表征和文本特征是有限的。在这种情况下，一种新的 Context Bridge 被提议，以 интеGRATE U-Net 网络的本地表示能力到 transformer 模型中。此外，原始 transformer 的标准注意力机制被 replaced with 高效自注意力，这种机制 simpler than other state-of-the-art methods。为了训练提议的网络，SDSU 研究农场在南达科他州的一个 FHB-感染小麦田中Capture了12,000 个小麦图像。此外，这些图像还包括不同阶段的疾病。一 команoda expert 的病理学家对这些图像进行了训练和评估。结果，通过对plant image segmentation 等常见任务进行广泛的实验， demonstarted  transformer 基于方法的 FHB 疾病检测的效果。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Model-in-Causal-Inference-with-Unmeasured-Confounders"><a href="#Diffusion-Model-in-Causal-Inference-with-Unmeasured-Confounders" class="headerlink" title="Diffusion Model in Causal Inference with Unmeasured Confounders"></a>Diffusion Model in Causal Inference with Unmeasured Confounders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03669">http://arxiv.org/abs/2308.03669</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tatsu432/BDCM">https://github.com/tatsu432/BDCM</a></li>
<li>paper_authors: Tatsuhiro Shimizu</li>
<li>for: 本研究旨在扩展diffusion模型，以便从观察数据中回答 causal问题，即使存在不可观测的假设变量。</li>
<li>methods: 我们使用了Pearl提出的Directed Acyclic Graph (DAG)来捕捉 causal intervention，并提出了一种叫做Diffusion-based Causal Model (DCM)的模型，假设所有的假设变量都可以观测。然而，在实际应用中，不可观测的假设变量存在，这限制了DCM的可用性。为了解决这个限制，我们提出了一种扩展模型，即Backdoor Criterion based DCM (BDCM)。</li>
<li>results: 我们通过synthetic data experiment表明，我们的提议的模型可以更 precisely回答 causal问题，即使存在不可观测的假设变量。<details>
<summary>Abstract</summary>
We study how to extend the use of the diffusion model to answer the causal question from the observational data under the existence of unmeasured confounders. In Pearl's framework of using a Directed Acyclic Graph (DAG) to capture the causal intervention, a Diffusion-based Causal Model (DCM) was proposed incorporating the diffusion model to answer the causal questions more accurately, assuming that all of the confounders are observed. However, unmeasured confounders in practice exist, which hinders DCM from being applicable. To alleviate this limitation of DCM, we propose an extended model called Backdoor Criterion based DCM (BDCM), whose idea is rooted in the Backdoor criterion to find the variables in DAG to be included in the decoding process of the diffusion model so that we can extend DCM to the case with unmeasured confounders. Synthetic data experiment demonstrates that our proposed model captures the counterfactual distribution more precisely than DCM under the unmeasured confounders.
</details>
<details>
<summary>摘要</summary>
我们研究如何将扩散模型应用于从观察数据中回答 causal 问题，即使存在未探测的干扰因素。在珍珠的框架中使用指导的циклиック графи（DAG）捕捉 causal 干扰，一种叫做扩散基于 causal 模型（DCM）的模型被提出，假设所有干扰因素都是观察的。然而，在实践中存在未探测的干扰因素，这限制了 DCM 的应用。为了解决 DCM 的这种限制，我们提出了一种扩展模型，即基于后门准则的 DCM（BDCM）。该模型的思想在于根据后门准则选择 DAG 中的变量，以便在扩散模型的解码过程中包含这些变量，从而扩展 DCM 到具有未探测干扰因素的情况。 synthetic 数据实验表明，我们的提议模型可以更 precisely 捕捉 counterfactual 分布than DCM 在未探测干扰因素的情况下。
</details></li>
</ul>
<hr>
<h2 id="Bridging-Trustworthiness-and-Open-World-Learning-An-Exploratory-Neural-Approach-for-Enhancing-Interpretability-Generalization-and-Robustness"><a href="#Bridging-Trustworthiness-and-Open-World-Learning-An-Exploratory-Neural-Approach-for-Enhancing-Interpretability-Generalization-and-Robustness" class="headerlink" title="Bridging Trustworthiness and Open-World Learning: An Exploratory Neural Approach for Enhancing Interpretability, Generalization, and Robustness"></a>Bridging Trustworthiness and Open-World Learning: An Exploratory Neural Approach for Enhancing Interpretability, Generalization, and Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03666">http://arxiv.org/abs/2308.03666</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shide Du, Zihan Fang, Shiyang Lan, Yanchao Tan, Manuel Günther, Shiping Wang, Wenzhong Guo</li>
<li>for: 提高人工智能系统的可靠性和可解释性，以增强人工智能在开放世界中的应用。</li>
<li>methods: 基于自定义可信网络和灵活学习正则化的方法，以提高学习模型的通用性和适应性。</li>
<li>results: 通过实现设计层解释性、环境健康任务接口和开放世界识别计划，提高了多模态场景下的信任性和可靠性。<details>
<summary>Abstract</summary>
As researchers strive to narrow the gap between machine intelligence and human through the development of artificial intelligence technologies, it is imperative that we recognize the critical importance of trustworthiness in open-world, which has become ubiquitous in all aspects of daily life for everyone. However, several challenges may create a crisis of trust in current artificial intelligence systems that need to be bridged: 1) Insufficient explanation of predictive results; 2) Inadequate generalization for learning models; 3) Poor adaptability to uncertain environments. Consequently, we explore a neural program to bridge trustworthiness and open-world learning, extending from single-modal to multi-modal scenarios for readers. 1) To enhance design-level interpretability, we first customize trustworthy networks with specific physical meanings; 2) We then design environmental well-being task-interfaces via flexible learning regularizers for improving the generalization of trustworthy learning; 3) We propose to increase the robustness of trustworthy learning by integrating open-world recognition losses with agent mechanisms. Eventually, we enhance various trustworthy properties through the establishment of design-level explainability, environmental well-being task-interfaces and open-world recognition programs. These designed open-world protocols are applicable across a wide range of surroundings, under open-world multimedia recognition scenarios with significant performance improvements observed.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Insufficient explanation of predictive results2. Inadequate generalization for learning models3. Poor adaptability to uncertain environmentsTo address these challenges, we explore a neural program that bridges trustworthiness and open-world learning, extending from single-modal to multi-modal scenarios for readers. Our approach includes the following three components:1. Enhancing design-level interpretability: We first customize trustworthy networks with specific physical meanings, making it easier to understand how the AI system works and why it makes certain decisions.2. Improving generalization through flexible learning regularizers: We design environmental well-being task-interfaces to improve the generalization of trustworthy learning, ensuring that the AI system can adapt to different environments and situations.3. Integrating open-world recognition losses with agent mechanisms: We propose to increase the robustness of trustworthy learning by integrating open-world recognition losses with agent mechanisms, enabling the AI system to better handle unexpected events and situations.By combining these three components, we enhance various trustworthy properties through the establishment of design-level explainability, environmental well-being task-interfaces, and open-world recognition programs. These designed open-world protocols are applicable across a wide range of surroundings, under open-world multimedia recognition scenarios with significant performance improvements observed.</details></li>
</ol>
<hr>
<h2 id="Distributionally-Robust-Classification-on-a-Data-Budget"><a href="#Distributionally-Robust-Classification-on-a-Data-Budget" class="headerlink" title="Distributionally Robust Classification on a Data Budget"></a>Distributionally Robust Classification on a Data Budget</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03821">http://arxiv.org/abs/2308.03821</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/penfever/vlhub">https://github.com/penfever/vlhub</a></li>
<li>paper_authors: Benjamin Feuer, Ameya Joshi, Minh Pham, Chinmay Hegde</li>
<li>for: 这篇论文的目的是提高深度学习模型在数据分布变化时的预测可靠性，并且在有限数据情况下进行训练。</li>
<li>methods: 论文使用了一系列控制严格的实验和大规模元分析来研究因素对模型 Robustness 的影响，并使用标准 ResNet-50 和 CLIP ResNet-50 进行比较。</li>
<li>results: 论文显示，使用限制数据量训练标准 ResNet-50 可以达到与 CLIP ResNet-50 训练于400万样本后的相似水平的分布Robustness。这是我们知道的首个在有限数据预算下实现 (near) 状态的艺技 Robustness 的结果。<details>
<summary>Abstract</summary>
Real world uses of deep learning require predictable model behavior under distribution shifts. Models such as CLIP show emergent natural distributional robustness comparable to humans, but may require hundreds of millions of training samples. Can we train robust learners in a domain where data is limited? To rigorously address this question, we introduce JANuS (Joint Annotations and Names Set), a collection of four new training datasets with images, labels, and corresponding captions, and perform a series of carefully controlled investigations of factors contributing to robustness in image classification, then compare those results to findings derived from a large-scale meta-analysis. Using this approach, we show that standard ResNet-50 trained with the cross-entropy loss on 2.4 million image samples can attain comparable robustness to a CLIP ResNet-50 trained on 400 million samples. To our knowledge, this is the first result showing (near) state-of-the-art distributional robustness on limited data budgets. Our dataset is available at \url{https://huggingface.co/datasets/penfever/JANuS_dataset}, and the code used to reproduce our experiments can be found at \url{https://github.com/penfever/vlhub/}.
</details>
<details>
<summary>摘要</summary>
实际应用中的深度学习需要模型在分布变化时保持预测可靠。如CLIP模型，它们可以 Display natural distributional robustness comparable to humans, but may require hundreds of millions of training samples. 我们问道：可以在数据有限的领域中训练强健的学习者吗？为了彻底回答这个问题，我们介绍了 JANuS（共同标注和名称集），一个包含四个新的训练数据集的集合，每个数据集包含图像、标签和相应的描述。我们进行了一系列仔细控制的调查，探讨了影响模型强健性的因素，然后将结果与大规模元分析结果进行比较。我们发现，使用权重损失函数和240万个图像样本训练的标准 ResNet-50 可以达到与CLIP ResNet-50 在4000万个样本上训练后的（近）状态OF-the-art分布强健性。我们认为这是首次在有限数据预算下实现的分布强健性结果。我们的数据集可以在 \url{https://huggingface.co/datasets/penfever/JANuS_dataset} 上下载，并且用于 reproduce我们的实验的代码可以在 \url{https://github.com/penfever/vlhub/} 上找到。
</details></li>
</ul>
<hr>
<h2 id="Two-stage-Early-Prediction-Framework-of-Remaining-Useful-Life-for-Lithium-ion-Batteries"><a href="#Two-stage-Early-Prediction-Framework-of-Remaining-Useful-Life-for-Lithium-ion-Batteries" class="headerlink" title="Two-stage Early Prediction Framework of Remaining Useful Life for Lithium-ion Batteries"></a>Two-stage Early Prediction Framework of Remaining Useful Life for Lithium-ion Batteries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03664">http://arxiv.org/abs/2308.03664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhruv Mittal, Hymalai Bello, Bo Zhou, Mayank Shekhar Jha, Sungho Suh, Paul Lukowicz</li>
<li>for: 预测锂离子电池剩余有用生命（RUL）的早期预测是重要的，以提高电池技术的可靠性和维护性。</li>
<li>methods: 本文提出了一种新的RUL预测方法，包括两个阶段：首先使用神经网络模型确定第一个预测周期（FPC），然后使用预测衰变模式来估算剩余有用生命的百分比。</li>
<li>results: 实验结果表明，提出的方法在RUL预测方面比既有方法更高准确。此外，该方法在实际应用场景中也表现出了优异的应用性和准确性。<details>
<summary>Abstract</summary>
Early prediction of remaining useful life (RUL) is crucial for effective battery management across various industries, ranging from household appliances to large-scale applications. Accurate RUL prediction improves the reliability and maintainability of battery technology. However, existing methods have limitations, including assumptions of data from the same sensors or distribution, foreknowledge of the end of life (EOL), and neglect to determine the first prediction cycle (FPC) to identify the start of the unhealthy stage. This paper proposes a novel method for RUL prediction of Lithium-ion batteries. The proposed framework comprises two stages: determining the FPC using a neural network-based model to divide the degradation data into distinct health states and predicting the degradation pattern after the FPC to estimate the remaining useful life as a percentage. Experimental results demonstrate that the proposed method outperforms conventional approaches in terms of RUL prediction. Furthermore, the proposed method shows promise for real-world scenarios, providing improved accuracy and applicability for battery management.
</details>
<details>
<summary>摘要</summary>
早期预测电池剩余有用寿命（RUL）是多个领域中的关键，从家用电器到大规模应用。准确预测RUL提高电池技术的可靠性和维护性。然而，现有的方法有限制，包括使用同一些感知器的数据或分布，假设结束生命周期（EOL）的知识，以及忽略确定首次预测周期（FPC）以识别不健康的阶段。这篇论文提出了一种新的Li-ion电池RUL预测方法。该框架包括两个阶段：使用神经网络模型来分解衰变数据 into Distinct Health States，并预测衰变模式以估计剩余有用寿命为百分比。实验结果表明，提案的方法在RUL预测方面超过了传统方法的性能。此外，提案的方法在实际应用中具有改善的准确性和可应用性。
</details></li>
</ul>
<hr>
<h2 id="Matrix-Completion-in-Almost-Verification-Time"><a href="#Matrix-Completion-in-Almost-Verification-Time" class="headerlink" title="Matrix Completion in Almost-Verification Time"></a>Matrix Completion in Almost-Verification Time</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03661">http://arxiv.org/abs/2308.03661</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan A. Kelner, Jerry Li, Allen Liu, Aaron Sidford, Kevin Tian</li>
<li>for: 这个论文提出了一种新的低级别矩阵完成问题的解决方案，即从Random观察到rank-$r$矩阵 $\mathbf{M} \in \mathbb{R}^{m \times n}$ (where $m \ge n$)的approximation。</li>
<li>methods: 这个论文提出了一种算法，可以在没有任何假设的情况下，从 $\approx mr$ 个观察数据中完成 $\mathbf{M}$ 的99% 的行和列。这个算法需要 $\approx mr^2$ 时间。在具有特定的行和列范围的 $\mathbf{M}$ 中，通过聚合多个回归问题的解决，这个算法可以完成全矩阵。</li>
<li>results: 论文表明，在具有几乎信息理论最佳的情况下，这个算法可以从 $mr^{2+o(1)}$ 个观察数据中完成 $\mathbf{M}$ 到高精度，并且runtime为 $mr^{3+o(1)}$。在特定的行和列范围下，这个算法可以完成 $\mathbf{M}$ 到信息理论最佳的精度，并且runtime为 $mr^{2+o(1)}$。此外，这个论文还提出了一些Robust的算法，可以在含有噪声的情况下完成 $\mathbf{M}$ 到 Frobenius 范数 distance $\approx r^{1.5}\Delta$，其中 $|\mathbf{N}|_{F} \le \Delta$。在噪声的情况下，这个算法的精度和runtime都比之前的算法更好。<details>
<summary>Abstract</summary>
We give a new framework for solving the fundamental problem of low-rank matrix completion, i.e., approximating a rank-$r$ matrix $\mathbf{M} \in \mathbb{R}^{m \times n}$ (where $m \ge n$) from random observations. First, we provide an algorithm which completes $\mathbf{M}$ on $99\%$ of rows and columns under no further assumptions on $\mathbf{M}$ from $\approx mr$ samples and using $\approx mr^2$ time. Then, assuming the row and column spans of $\mathbf{M}$ satisfy additional regularity properties, we show how to boost this partial completion guarantee to a full matrix completion algorithm by aggregating solutions to regression problems involving the observations.   In the well-studied setting where $\mathbf{M}$ has incoherent row and column spans, our algorithms complete $\mathbf{M}$ to high precision from $mr^{2+o(1)}$ observations in $mr^{3 + o(1)}$ time (omitting logarithmic factors in problem parameters), improving upon the prior state-of-the-art [JN15] which used $\approx mr^5$ samples and $\approx mr^7$ time. Under an assumption on the row and column spans of $\mathbf{M}$ we introduce (which is satisfied by random subspaces with high probability), our sample complexity improves to an almost information-theoretically optimal $mr^{1 + o(1)}$, and our runtime improves to $mr^{2 + o(1)}$. Our runtimes have the appealing property of matching the best known runtime to verify that a rank-$r$ decomposition $\mathbf{U}\mathbf{V}^\top$ agrees with the sampled observations. We also provide robust variants of our algorithms that, given random observations from $\mathbf{M} + \mathbf{N}$ with $\|\mathbf{N}\|_{F} \le \Delta$, complete $\mathbf{M}$ to Frobenius norm distance $\approx r^{1.5}\Delta$ in the same runtimes as the noiseless setting. Prior noisy matrix completion algorithms [CP10] only guaranteed a distance of $\approx \sqrt{n}\Delta$.
</details>
<details>
<summary>摘要</summary>
我们提出了一新的框架来解决低级矩阵完成问题，即 aproximating 一个rank-$r$矩阵 $\mathbf{M} \in \mathbb{R}^{m \times n}$ (where $m \ge n$) 从 random observations。首先，我们提供了一个算法，可以在没有进一步假设的情况下，使 $\mathbf{M}$ 在99% 的行和列上完成，需要 $\approx mr$ 样本和 $\approx mr^2$ 时间。然后，如果行和列范围的范围满足其他正则性质，我们如何通过融合关于 observations 的回归问题的解来提高这个部分完成 garantía。在已有研究的设定中， где $\mathbf{M}$ 的行和列范围是不相关的，我们可以从 $mr^{2+o(1)}$ 样本和 $mr^{3+o(1)}$ 时间中完成 $\mathbf{M}$ 到高精度，超过先前的最佳状态（JN15），其使用 $mr^5$ 样本和 $mr^7$ 时间。如果行和列范围满足我们引入的一个假设（这在Random subspace 中发生的概率很高），我们的样本复杂度可以降低到 almost information-theoretically optimal $mr^{1+o(1)}$，并且时间复杂度可以降低到 $mr^{2+o(1)}$。我们的运行时间具有愉悦的性质，即与verify rank-$r$ 分解 $\mathbf{U}\mathbf{V}^\top$ 与样本观测匹配的最佳known runtime。我们还提供了一些Robust 变体我们的算法，可以在 $\mathbf{M} + \mathbf{N}$ 中的随机观测下，完成 $\mathbf{M}$ 到 Frobenius 范数Distance $\approx r^{1.5}\Delta$ 的同样时间。在先前的噪声矩阵完成算法（CP10）中，只能保证一个距离为 $\approx \sqrt{n}\Delta$。
</details></li>
</ul>
<hr>
<h2 id="Generative-Forests"><a href="#Generative-Forests" class="headerlink" title="Generative Forests"></a>Generative Forests</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03648">http://arxiv.org/abs/2308.03648</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AlCorreia/GeFs">https://github.com/AlCorreia/GeFs</a></li>
<li>paper_authors: Richard Nock, Mathieu Guillame-Bert</li>
<li>for: 本文旨在提出新的树形生成模型，用于Tabular数据生成和概率模型。</li>
<li>methods: 本文使用新的树形生成模型，并提出一种基于supervised学习的训练算法，可以在Tabular数据上实现高质量的数据生成。</li>
<li>results: 实验表明，本文的方法可以在缺失数据填充和生成数据比较实际数据的任务中显示出高质量的结果， especial against state of the art。<details>
<summary>Abstract</summary>
Tabular data represents one of the most prevalent form of data. When it comes to data generation, many approaches would learn a density for the data generation process, but would not necessarily end up with a sampler, even less so being exact with respect to the underlying density. A second issue is on models: while complex modeling based on neural nets thrives in image or text generation (etc.), less is known for powerful generative models on tabular data. A third problem is the visible chasm on tabular data between training algorithms for supervised learning with remarkable properties (e.g. boosting), and a comparative lack of guarantees when it comes to data generation. In this paper, we tackle the three problems, introducing new tree-based generative models convenient for density modeling and tabular data generation that improve on modeling capabilities of recent proposals, and a training algorithm which simplifies the training setting of previous approaches and displays boosting-compliant convergence. This algorithm has the convenient property to rely on a supervised training scheme that can be implemented by a few tweaks to the most popular induction scheme for decision tree induction with two classes. Experiments are provided on missing data imputation and comparing generated data to real data, displaying the quality of the results obtained by our approach, in particular against state of the art.
</details>
<details>
<summary>摘要</summary>
表格数据是现代数据的一种最常见形式。当处理数据时，许多方法都会学习数据的浓度，但它们并不一定会得到一个采样器，更不用说是对于真实的浓度准确。第二个问题是模型：虽然复杂的模型基于神经网络在图像或文本生成等领域得到了广泛的应用，但对于表格数据， menos is known about powerful generative models。第三个问题是表格数据的训练算法和supervised learning之间的可见差异。在这篇论文中，我们解决了这三个问题，提出了新的树形生成模型，可以提高表格数据生成的模型能力，并且提供了一种简化训练过程的算法，可以让之前的方法在训练中更加简单。这种算法可以通过对最流行的决策树引入两类的修改来实现。我们的实验表明，我们的方法可以在缺失数据填充和生成数据与真实数据的比较中 display 出色的结果，特别是与现有技术相比。
</details></li>
</ul>
<hr>
<h2 id="XFlow-Benchmarking-Flow-Behaviors-over-Graphs"><a href="#XFlow-Benchmarking-Flow-Behaviors-over-Graphs" class="headerlink" title="XFlow: Benchmarking Flow Behaviors over Graphs"></a>XFlow: Benchmarking Flow Behaviors over Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03819">http://arxiv.org/abs/2308.03819</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xgraphing/xflow">https://github.com/xgraphing/xflow</a></li>
<li>paper_authors: Zijian Zhang, Zonghan Zhang, Zhiqian Chen</li>
<li>for: 本研究旨在提供一个包容性强的benchmark集合，以便研究在网络场景下的流行行为。</li>
<li>methods: 本研究使用了多种任务、基准模型、图Dataset和评估工具来研究流行行为。</li>
<li>results: 研究发现了现有基础模型的优劣点，并提出了进一步研究的可能性。基于实验结果，提供了一个通用的分析框架，用于研究多种流行任务的多个领域。<details>
<summary>Abstract</summary>
The occurrence of diffusion on a graph is a prevalent and significant phenomenon, as evidenced by the spread of rumors, influenza-like viruses, smart grid failures, and similar events. Comprehending the behaviors of flow is a formidable task, due to the intricate interplay between the distribution of seeds that initiate flow propagation, the propagation model, and the topology of the graph. The study of networks encompasses a diverse range of academic disciplines, including mathematics, physics, social science, and computer science. This interdisciplinary nature of network research is characterized by a high degree of specialization and compartmentalization, and the cooperation facilitated by them is inadequate. From a machine learning standpoint, there is a deficiency in a cohesive platform for assessing algorithms across various domains. One of the primary obstacles to current research in this field is the absence of a comprehensive curated benchmark suite to study the flow behaviors under network scenarios.   To address this disparity, we propose the implementation of a novel benchmark suite that encompasses a variety of tasks, baseline models, graph datasets, and evaluation tools. In addition, we present a comprehensive analytical framework that offers a generalized approach to numerous flow-related tasks across diverse domains, serving as a blueprint and roadmap. Drawing upon the outcomes of our empirical investigation, we analyze the advantages and disadvantages of current foundational models, and we underscore potential avenues for further study. The datasets, code, and baseline models have been made available for the public at: https://github.com/XGraphing/XFlow
</details>
<details>
<summary>摘要</summary>
流行现象在图格中是一种普遍和重要的现象，如传播谣言、流感病毒、智能电网故障等事件。理解流动行为是一项复杂的任务，由于流动 initiation 的分布、传播模型和图格结构之间的复杂交互。网络研究涵盖多个学术领域，包括数学、物理、社会科学和计算机科学。这些学科之间的交流和合作受限。从机器学习的角度来看，流行研究受到了缺乏一个整体的平台来评估算法。目前研究中的主要障碍是缺乏一个完整的精心编辑的benchmark集成来研究流动行为在网络场景下。为了解决这一不足，我们提议实施一个新的benchmark集成，包括多种任务、基线模型、图格数据集和评估工具。此外，我们还提出了一个通用的分析框架，可以在多个流动相关任务上提供一个通用的方法和路线图。通过我们的实验研究的结果，我们分析了当前基础模型的优劣点，并强调了未来研究的可能性。 datasets、代码和基线模型已经公开发布在：https://github.com/XGraphing/XFlow。
</details></li>
</ul>
<hr>
<h2 id="MedMine-Examining-Pre-trained-Language-Models-on-Medication-Mining"><a href="#MedMine-Examining-Pre-trained-Language-Models-on-Medication-Mining" class="headerlink" title="MedMine: Examining Pre-trained Language Models on Medication Mining"></a>MedMine: Examining Pre-trained Language Models on Medication Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03629">http://arxiv.org/abs/2308.03629</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hecta-uom/m3">https://github.com/hecta-uom/m3</a></li>
<li>paper_authors: Haifa Alrdahi, Lifeng Han, Hendrik Šuvalov, Goran Nenadic</li>
<li>For: 本研究旨在探讨现有的预训练语言模型（PLM）在自动药物检索任务上的表现，以及这些模型在不同实体类和医疗事件上的偏见问题。* Methods: 本研究使用了现有的预训练语言模型（PLM），包括Med7和XLM-RoBERTa，进行细化调教。并对这些调教后的模型进行比较，找出它们在不同实体类和医疗事件上的优劣点。* Results: 研究发现，XLM-RoBERTa在自动药物检索任务上表现较好，而Med7在一些实体类上表现较差。此外，研究还发现了这些模型在不同实体类和医疗事件上的偏见问题。<details>
<summary>Abstract</summary>
Automatic medication mining from clinical and biomedical text has become a popular topic due to its real impact on healthcare applications and the recent development of powerful language models (LMs). However, fully-automatic extraction models still face obstacles to be overcome such that they can be deployed directly into clinical practice for better impacts. Such obstacles include their imbalanced performances on different entity types and clinical events. In this work, we examine current state-of-the-art pre-trained language models (PLMs) on such tasks, via fine-tuning including the monolingual model Med7 and multilingual large language model (LLM) XLM-RoBERTa. We compare their advantages and drawbacks using historical medication mining shared task data sets from n2c2-2018 challenges. We report the findings we get from these fine-tuning experiments such that they can facilitate future research on addressing them, for instance, how to combine their outputs, merge such models, or improve their overall accuracy by ensemble learning and data augmentation. MedMine is part of the M3 Initiative \url{https://github.com/HECTA-UoM/M3}
</details>
<details>
<summary>摘要</summary>
自动药物挖掘从临床和生物医学文本中获得了广泛的关注，因为它们在医疗应用中真正有影响。然而，完全自动提取模型仍然需要超越一些障碍物，以便在临床实践中直接部署。这些障碍包括它们在不同实体类型和临床事件上的不均衡性能。在这项工作中，我们评估了当前状态的批处理语言模型（PLM）在这些任务上，包括单语言模型Med7以及多语言大语言模型（LLM）XLM-RoBERTa。我们比较了它们的优点和缺点，使用历史药物挖掘共享任务数据集。我们报告了我们在这些练习中获得的发现，以便未来研究如何解决这些问题，例如如何组合它们的输出、合并这些模型，或者如何提高它们的总准确率通过ensemble学习和数据扩展。MedMine是M3Initiave的一部分，详情请参考<https://github.com/HECTA-UoM/M3>。
</details></li>
</ul>
<hr>
<h2 id="A-sparse-coding-approach-to-inverse-problems-with-application-to-microwave-tomography-imaging"><a href="#A-sparse-coding-approach-to-inverse-problems-with-application-to-microwave-tomography-imaging" class="headerlink" title="A sparse coding approach to inverse problems with application to microwave tomography imaging"></a>A sparse coding approach to inverse problems with application to microwave tomography imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03818">http://arxiv.org/abs/2308.03818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cesar F. Caiafa, Ramiro M. Irastorza</li>
<li>for:  solve ill-posed inverse imaging problems in various domains, such as medical diagnosis and astronomical studies.</li>
<li>methods:  use sparse representation of images, a realistic and effective generative model for natural images inspired by the visual system of mammals, to address ill-posed linear inverse problems.</li>
<li>results:  extend the application of sparse coding to solve non-linear and ill-posed problems in microwave tomography imaging, which could lead to a significant improvement of the state-of-the-arts algorithms.Here are the three points in Simplified Chinese text:</li>
<li>for: 解决不同领域的各种各样的反射图像问题，如医学诊断和天文学研究。</li>
<li>methods: 使用自然图像的稀疏表示，这是一种基于哺乳动物视觉系统的实用和有效的生成模型，来解决线性不定的反射图像问题。</li>
<li>results: 将稀疏码应用到微波tomography图像重建中，以解决非线性和不定的问题，可能会提高现有算法的性能。<details>
<summary>Abstract</summary>
Inverse imaging problems that are ill-posed can be encountered across multiple domains of science and technology, ranging from medical diagnosis to astronomical studies. To reconstruct images from incomplete and distorted data, it is necessary to create algorithms that can take into account both, the physical mechanisms responsible for generating these measurements and the intrinsic characteristics of the images being analyzed. In this work, the sparse representation of images is reviewed, which is a realistic, compact and effective generative model for natural images inspired by the visual system of mammals. It enables us to address ill-posed linear inverse problems by training the model on a vast collection of images. Moreover, we extend the application of sparse coding to solve the non-linear and ill-posed problem in microwave tomography imaging, which could lead to a significant improvement of the state-of-the-arts algorithms.
</details>
<details>
<summary>摘要</summary>
各种科学和技术领域中的反射 imaging 问题可能会出现不定性，从医疗诊断到天文学研究。为重建受损和扭曲数据中的图像，需要开发能够考虑物理机制生成测量数据以及图像本身内在特征的算法。在这项工作中，我们提出了图像稀疏表示，这是一种现实主义、紧凑和有效的自然图像生成模型，启发自哺乳动物视系统。这种模型可以 addresses 不定性线性反射问题，通过训练模型使用大量图像。此外，我们还扩展了稀疏编码的应用，解决微波探测成像中的非线性和不定性问题，这可能会导致现有算法的显著改进。
</details></li>
</ul>
<hr>
<h2 id="A-Meta-learning-based-Stacked-Regression-Approach-for-Customer-Lifetime-Value-Prediction"><a href="#A-Meta-learning-based-Stacked-Regression-Approach-for-Customer-Lifetime-Value-Prediction" class="headerlink" title="A Meta-learning based Stacked Regression Approach for Customer Lifetime Value Prediction"></a>A Meta-learning based Stacked Regression Approach for Customer Lifetime Value Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08502">http://arxiv.org/abs/2308.08502</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karan Gadgil, Sukhpal Singh Gill, Ahmed M. Abdelmoniem</li>
<li>for: 这种研究旨在提供一种能够快速和简单地计算客户生命周期价值（CLV）的方法，以便企业更好地理解客户，扩大收入。</li>
<li>methods: 这种方法使用了元学习和堆叠回归模型，结合了袋包和强化模型的预测结果，以提高计算CLV的精度和效果。</li>
<li>results: 实验结果表明，提议的方法能够快速和简单地计算CLV，并且能够提高计算结果的准确性和稳定性。<details>
<summary>Abstract</summary>
Companies across the globe are keen on targeting potential high-value customers in an attempt to expand revenue and this could be achieved only by understanding the customers more. Customer Lifetime Value (CLV) is the total monetary value of transactions/purchases made by a customer with the business over an intended period of time and is used as means to estimate future customer interactions. CLV finds application in a number of distinct business domains such as Banking, Insurance, Online-entertainment, Gaming, and E-Commerce. The existing distribution-based and basic (recency, frequency & monetary) based models face a limitation in terms of handling a wide variety of input features. Moreover, the more advanced Deep learning approaches could be superfluous and add an undesirable element of complexity in certain application areas. We, therefore, propose a system which is able to qualify both as effective, and comprehensive yet simple and interpretable. With that in mind, we develop a meta-learning-based stacked regression model which combines the predictions from bagging and boosting models that each is found to perform well individually. Empirical tests have been carried out on an openly available Online Retail dataset to evaluate various models and show the efficacy of the proposed approach.
</details>
<details>
<summary>摘要</summary>
To address this, we propose a system that is both effective and simple. We develop a meta-learning-based stacked regression model that combines the predictions of bagging and boosting models, which have been found to perform well individually. We test our approach on an openly available online retail dataset and show that it is effective.
</details></li>
</ul>
<hr>
<h2 id="Stock-Market-Price-Prediction-A-Hybrid-LSTM-and-Sequential-Self-Attention-based-Approach"><a href="#Stock-Market-Price-Prediction-A-Hybrid-LSTM-and-Sequential-Self-Attention-based-Approach" class="headerlink" title="Stock Market Price Prediction: A Hybrid LSTM and Sequential Self-Attention based Approach"></a>Stock Market Price Prediction: A Hybrid LSTM and Sequential Self-Attention based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04419">http://arxiv.org/abs/2308.04419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karan Pardeshi, Sukhpal Singh Gill, Ahmed M. Abdelmoniem</li>
<li>for: 预测股票价格，帮助投资者做出最佳决策</li>
<li>methods: 提出了一种新的Long Short-Term Memory（LSTM）模型，并采用Sequential Self-Attention Mechanism（LSTM-SSAM）来提高预测精度</li>
<li>results: 对三个股票数据集（SBIN、HDFCBANK、BANKBARODA）进行了广泛的实验，结果表明提出的模型比现有模型更有效和可行，RMSE和R2评价指标都达到了最佳效果。<details>
<summary>Abstract</summary>
One of the most enticing research areas is the stock market, and projecting stock prices may help investors profit by making the best decisions at the correct time. Deep learning strategies have emerged as a critical technique in the field of the financial market. The stock market is impacted due to two aspects, one is the geo-political, social and global events on the bases of which the price trends could be affected. Meanwhile, the second aspect purely focuses on historical price trends and seasonality, allowing us to forecast stock prices. In this paper, our aim is to focus on the second aspect and build a model that predicts future prices with minimal errors. In order to provide better prediction results of stock price, we propose a new model named Long Short-Term Memory (LSTM) with Sequential Self-Attention Mechanism (LSTM-SSAM). Finally, we conduct extensive experiments on the three stock datasets: SBIN, HDFCBANK, and BANKBARODA. The experimental results prove the effectiveness and feasibility of the proposed model compared to existing models. The experimental findings demonstrate that the root-mean-squared error (RMSE), and R-square (R2) evaluation indicators are giving the best results.
</details>
<details>
<summary>摘要</summary>
一个非常吸引人的研究领域是股票市场，并且预测股票价格可以帮助投资者获得最佳的决策时机。深度学习策略在金融市场中得到了广泛的应用。股票市场受到两个方面的影响：一是地域政治、社会和全球事件的影响，这些事件可能影响股票价格走势。而第二个方面则专注于历史价格走势和季节性，我们可以通过预测股票价格。在这篇论文中，我们的目标是建立一个可预测股票价格的模型，并且使用新的长期记忆（LSTM）和顺序自我注意机制（LSTM-SSAM）来提高预测结果的准确性。最后，我们对三个股票数据集（SBIN、HDFCBANK和BANKBARODA）进行了广泛的实验，实验结果表明我们提出的模型比现有模型更有效果和可行性。实验结果表明，使用 Root-Mean-Squared Error（RMSE）和R-square（R2）评价指标，我们的模型在预测股票价格方面表现出色。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Semi-Supervised-Segmentation-of-Brain-Vessels-with-Ambiguous-Labels"><a href="#Adaptive-Semi-Supervised-Segmentation-of-Brain-Vessels-with-Ambiguous-Labels" class="headerlink" title="Adaptive Semi-Supervised Segmentation of Brain Vessels with Ambiguous Labels"></a>Adaptive Semi-Supervised Segmentation of Brain Vessels with Ambiguous Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03613">http://arxiv.org/abs/2308.03613</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fengming Lin, Yan Xia, Nishant Ravikumar, Qiongyao Liu, Michael MacRaild, Alejandro F Frangi</li>
<li>for: 这篇论文是为了准确分类脑血管而设计的。</li>
<li>methods: 这篇论文使用了进步式半监督学习、适应性训练策略和边界增强等新技术。</li>
<li>results: 实验结果显示，这篇论文的方法在3DRA数据集上实现了 mesh-based 分类 метри値的超越。它能够充分利用部分和暗箱注意的数据，进而实现了优秀的分类性能。<details>
<summary>Abstract</summary>
Accurate segmentation of brain vessels is crucial for cerebrovascular disease diagnosis and treatment. However, existing methods face challenges in capturing small vessels and handling datasets that are partially or ambiguously annotated. In this paper, we propose an adaptive semi-supervised approach to address these challenges. Our approach incorporates innovative techniques including progressive semi-supervised learning, adaptative training strategy, and boundary enhancement. Experimental results on 3DRA datasets demonstrate the superiority of our method in terms of mesh-based segmentation metrics. By leveraging the partially and ambiguously labeled data, which only annotates the main vessels, our method achieves impressive segmentation performance on mislabeled fine vessels, showcasing its potential for clinical applications.
</details>
<details>
<summary>摘要</summary>
<<SYS>>精准分割脑血管是脑血管疾病诊断和治疗中的关键。然而，现有方法在捕捉小血管和处理部分或杂乱标注的数据集时遇到困难。在这篇论文中，我们提出了一种适应式半supervised方法来解决这些问题。我们的方法包括进步式半supervised学习、适应性训练策略和边界增强等创新技术。在3DRA数据集上进行实验，我们的方法在基于网格的分割指标上表现出色。通过利用部分和杂乱标注的数据，我们的方法在偏移的细血管上实现了优秀的分割性能，这显示了它在临床应用中的潜力。<<SYS>>
</details></li>
</ul>
<hr>
<h2 id="A-machine-learning-sleep-wake-classification-model-using-a-reduced-number-of-features-derived-from-photoplethysmography-and-activity-signals"><a href="#A-machine-learning-sleep-wake-classification-model-using-a-reduced-number-of-features-derived-from-photoplethysmography-and-activity-signals" class="headerlink" title="A machine-learning sleep-wake classification model using a reduced number of features derived from photoplethysmography and activity signals"></a>A machine-learning sleep-wake classification model using a reduced number of features derived from photoplethysmography and activity signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05759">http://arxiv.org/abs/2308.05759</a></li>
<li>repo_url: None</li>
<li>paper_authors: Douglas A. Almeida, Felipe M. Dias, Marcelo A. F. Toledo, Diego A. C. Cardenas, Filipe A. C. Oliveira, Estela Ribeiro, Jose E. Krieger, Marco A. Gutierrez</li>
<li>for: 本研究旨在开发一种基于光谱学振荡分析的睡眠阶段分类模型，以提高睡眠质量和全身健康。</li>
<li>methods: 本研究使用了EXTREME GRADIENT BOOSTING（XGBoost）算法和PPG信号和活动计数特征进行睡眠阶段分类。</li>
<li>results: 本研究的方法与当前状态之册方法相比，敏感性为91.15 $\pm$ 1.16%, 特征选择率为53.66 $\pm$ 1.12%, F1分数为83.88 $\pm$ 0.56%, κ值为48.0 $\pm$ 0.86%。<details>
<summary>Abstract</summary>
Sleep is a crucial aspect of our overall health and well-being. It plays a vital role in regulating our mental and physical health, impacting our mood, memory, and cognitive function to our physical resilience and immune system. The classification of sleep stages is a mandatory step to assess sleep quality, providing the metrics to estimate the quality of sleep and how well our body is functioning during this essential period of rest. Photoplethysmography (PPG) has been demonstrated to be an effective signal for sleep stage inference, meaning it can be used on its own or in a combination with others signals to determine sleep stage. This information is valuable in identifying potential sleep issues and developing strategies to improve sleep quality and overall health. In this work, we present a machine learning sleep-wake classification model based on the eXtreme Gradient Boosting (XGBoost) algorithm and features extracted from PPG signal and activity counts. The performance of our method was comparable to current state-of-the-art methods with a Sensitivity of 91.15 $\pm$ 1.16%, Specificity of 53.66 $\pm$ 1.12%, F1-score of 83.88 $\pm$ 0.56%, and Kappa of 48.0 $\pm$ 0.86%. Our method offers a significant improvement over other approaches as it uses a reduced number of features, making it suitable for implementation in wearable devices that have limited computational power.
</details>
<details>
<summary>摘要</summary>
睡眠是我们身体和心理健康的重要组成部分。它对我们的情绪、记忆和认知功能以及身体的鲁棒性和免疫系统产生重要影响。睡眠阶段的分类是评估睡眠质量的必备步骤，它可以提供评估睡眠质量的度量，以及身体在这一期间的功能如何。聚光折射（PPG）已经被证明可以用于睡眠阶段推断，因此它可以单独使用或与其他信号结合使用来确定睡眠阶段。这些信息非常有价值，可以用于发现可能存在的睡眠问题，并开发改善睡眠质量和整体健康的策略。在这项工作中，我们提出了基于极限梯度提升（XGBoost）算法和PPG信号和活动计数特征的机器学习睡眠-醒目分类模型。我们的方法与当前状态的方法相比，表现出了相似的性能，具体来说，敏感性为91.15 $\pm$ 1.16%，特异性为53.66 $\pm$ 1.12%，F1分数为83.88 $\pm$ 0.56%，κ值为48.0 $\pm$ 0.86%。我们的方法在计算能力有限的穿戴设备中实现更加可行，因此它对现有的方法具有显著改进。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Early-Stopping-in-Evolutionary-Direct-Policy-Search"><a href="#Generalized-Early-Stopping-in-Evolutionary-Direct-Policy-Search" class="headerlink" title="Generalized Early Stopping in Evolutionary Direct Policy Search"></a>Generalized Early Stopping in Evolutionary Direct Policy Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03574">http://arxiv.org/abs/2308.03574</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anonreposit/gesp">https://github.com/anonreposit/gesp</a></li>
<li>paper_authors: Etor Arza, Leni K. Le Goff, Emma Hart</li>
<li>for: 这篇论文主要针对的是优化问题中的长时间评估时间问题，特别是在物理世界中进行评估，例如在机器人应用中。</li>
<li>methods: 本文提出了一个早期停止方法来解决这个问题，这个方法只需要在每个时间步骤中考虑目标值，不需要对问题本身进行具体的知识。</li>
<li>results: 根据五个来自游戏、机器人和 класи控制领域的直接政策搜寻环境中的测试，提出的早期停止条件可以节省大约75%的计算时间，并且与问题特有的停止条件相比，它表现更加稳定且更通用。<details>
<summary>Abstract</summary>
Lengthy evaluation times are common in many optimization problems such as direct policy search tasks, especially when they involve conducting evaluations in the physical world, e.g. in robotics applications. Often, when evaluating a solution over a fixed time period, it becomes clear that the objective value will not increase with additional computation time (for example, when a two-wheeled robot continuously spins on the spot). In such cases, it makes sense to stop the evaluation early to save computation time. However, most approaches to stop the evaluation are problem-specific and need to be specifically designed for the task at hand. Therefore, we propose an early stopping method for direct policy search. The proposed method only looks at the objective value at each time step and requires no problem-specific knowledge.   We test the introduced stopping criterion in five direct policy search environments drawn from games, robotics, and classic control domains, and show that it can save up to 75% of the computation time. We also compare it with problem-specific stopping criteria and demonstrate that it performs comparably while being more generally applicable.
</details>
<details>
<summary>摘要</summary>
长时间的评估时间是许多优化问题的常见现象，如直接策略搜索任务，尤其在物理世界中进行评估，例如在 robotics 应用中。经常情况下，在一定时间间评估解决方案时，会发现目标值不会随着计算时间增加（例如，一辆两轮摩托车连续旋转在一处）。在这些情况下，可以提前结束评估以避免浪费计算时间。然而，大多数止评估方法是任务特定的，需要特定的问题知识。因此，我们提出了一种止评估方法，只需要在每个时间步骤中考虑目标值即可，不需要任务特定的知识。我们在五个直接策略搜索环境中测试了引入的停止标准，这些环境来自游戏、 роботи克和 класси控制领域。我们发现，该方法可以将计算时间减少到75%。我们还与任务特定的停止标准进行比较，并证明它在通用性方面与其相当，而且更加通用。
</details></li>
</ul>
<hr>
<h2 id="When-Federated-Learning-meets-Watermarking-A-Comprehensive-Overview-of-Techniques-for-Intellectual-Property-Protection"><a href="#When-Federated-Learning-meets-Watermarking-A-Comprehensive-Overview-of-Techniques-for-Intellectual-Property-Protection" class="headerlink" title="When Federated Learning meets Watermarking: A Comprehensive Overview of Techniques for Intellectual Property Protection"></a>When Federated Learning meets Watermarking: A Comprehensive Overview of Techniques for Intellectual Property Protection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03573">http://arxiv.org/abs/2308.03573</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammed Lansari, Reda Bellafqira, Katarzyna Kapusta, Vincent Thouvenot, Olivier Bettan, Gouenou Coatrieux</li>
<li>for: 本研究は、 Federated Learning（FL）の领域での标识技术に関するOverviewを提供します。</li>
<li>methods: 本研究では、FLの特有の制约に対応するために、DNN标识法の新しい挑戦と机会に焦点を当てています。</li>
<li>results: 本研究では、过去5年间におけるFL标识法の最新の进歩について详しく述べています。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a technique that allows multiple participants to collaboratively train a Deep Neural Network (DNN) without the need of centralizing their data. Among other advantages, it comes with privacy-preserving properties making it attractive for application in sensitive contexts, such as health care or the military. Although the data are not explicitly exchanged, the training procedure requires sharing information about participants' models. This makes the individual models vulnerable to theft or unauthorized distribution by malicious actors. To address the issue of ownership rights protection in the context of Machine Learning (ML), DNN Watermarking methods have been developed during the last five years. Most existing works have focused on watermarking in a centralized manner, but only a few methods have been designed for FL and its unique constraints. In this paper, we provide an overview of recent advancements in Federated Learning watermarking, shedding light on the new challenges and opportunities that arise in this field.
</details>
<details>
<summary>摘要</summary>
《联合学习（Federated Learning，FL）技术 Allow multiple participants to collaboratively train a deep neural network (DNN) without centralizing their data. Among other advantages, it has privacy-preserving properties that make it attractive for sensitive applications, such as healthcare or the military. However, the training process requires sharing information about participants' models, making individual models vulnerable to theft or unauthorized distribution by malicious actors. To address the issue of ownership rights protection in machine learning (ML), DNN watermarking methods have been developed over the past five years. Most existing works have focused on watermarking in a centralized manner, but only a few methods have been designed for FL and its unique constraints. In this paper, we provide an overview of recent advancements in federated learning watermarking, highlighting the new challenges and opportunities that arise in this field.》Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know and I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Provably-Efficient-Learning-in-Partially-Observable-Contextual-Bandit"><a href="#Provably-Efficient-Learning-in-Partially-Observable-Contextual-Bandit" class="headerlink" title="Provably Efficient Learning in Partially Observable Contextual Bandit"></a>Provably Efficient Learning in Partially Observable Contextual Bandit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03572">http://arxiv.org/abs/2308.03572</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueping Gong, Jiheng Zhang</li>
<li>for: 这个研究探索了对受限知识和部分隐藏因素的偏见 Transfer Learning 在内部 Bandit 中，并研究了在这种情况下的 Casual 影响分析和估计错误。</li>
<li>methods: 本研究首先将问题转换为确定或偏见 Casual 效果的决策问题，并透过阶段性解决 Linear Programming 问题来获得 Casual 上下文范围内的统计误差。然后，我们运用这些 Sampling 算法来获得可靠的数据描述和估计误差。</li>
<li>results: 我们证明了我们的 Casually 增强的 Bandit 算法可以超越传统 Bandit 算法，并在任务中 Handle 通用 Context 分布时提高了训练速度和性能。此外，我们还进行了实验，证明了我们的策略在实际应用中比现有的方法更高效。<details>
<summary>Abstract</summary>
In this paper, we investigate transfer learning in partially observable contextual bandits, where agents have limited knowledge from other agents and partial information about hidden confounders. We first convert the problem to identifying or partially identifying causal effects between actions and rewards through optimization problems. To solve these optimization problems, we discretize the original functional constraints of unknown distributions into linear constraints, and sample compatible causal models via sequentially solving linear programmings to obtain causal bounds with the consideration of estimation error. Our sampling algorithms provide desirable convergence results for suitable sampling distributions. We then show how causal bounds can be applied to improving classical bandit algorithms and affect the regrets with respect to the size of action sets and function spaces. Notably, in the task with function approximation which allows us to handle general context distributions, our method improves the order dependence on function space size compared with previous literatures. We formally prove that our causally enhanced algorithms outperform classical bandit algorithms and achieve orders of magnitude faster convergence rates. Finally, we perform simulations that demonstrate the efficiency of our strategy compared to the current state-of-the-art methods. This research has the potential to enhance the performance of contextual bandit agents in real-world applications where data is scarce and costly to obtain.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了转移学习在部分可见Contextual Bandit中， где代理人具有其他代理人的有限知识以及隐藏的共同因素的局部信息。我们首先将问题转化为标识或部分标识 causal effect  между动作和奖励的优化问题。为解这些优化问题，我们将原始的不确定分布函数约化为线性约化，然后通过顺序解 linear program 来获取 causal bound ，考虑到估计误差。我们的抽样算法提供了desirable的收敛结果，并且我们可以用这些 causal bound 来改进经典bandit算法，从而影响行动集和函数空间的大小。我们正式证明我们的 causally enhanced 算法比经典bandit算法更高效，并且可以在函数近似任务中实现更高的速度比。最后，我们在实验中证明了我们的策略比现有的方法更高效。这些研究有望提高实际应用中的 Contextual Bandit 代理人性能，当数据稀缺和昂贵时。
</details></li>
</ul>
<hr>
<h2 id="Partial-identification-of-kernel-based-two-sample-tests-with-mismeasured-data"><a href="#Partial-identification-of-kernel-based-two-sample-tests-with-mismeasured-data" class="headerlink" title="Partial identification of kernel based two sample tests with mismeasured data"></a>Partial identification of kernel based two sample tests with mismeasured data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03570">http://arxiv.org/abs/2308.03570</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ron Nafshi, Maggie Makar</li>
<li>for: 本研究旨在针对机器学习应用中的两种分布之间的差异探究，并且 relax 了现有文献中假设有错误样本的假设。</li>
<li>methods: 本研究使用了非 Parametric 两种样本测试，包括最大均值差 (MMD)，并且研究了在 $\epsilon$ 污染下的 MMD 估计。</li>
<li>results: 本研究显示了在 $\epsilon$ 污染下，常用的 MMD 估计不可靠，而我们提出了一个方法来估计 MMD 的上下限，并证明这个方法会对 MMD 的估计进行更加精确的 bounds。使用了三个数据集，我们还证明了我们的方法比于其他方法更加稳定和有更好的性能。<details>
<summary>Abstract</summary>
Nonparametric two-sample tests such as the Maximum Mean Discrepancy (MMD) are often used to detect differences between two distributions in machine learning applications. However, the majority of existing literature assumes that error-free samples from the two distributions of interest are available.We relax this assumption and study the estimation of the MMD under $\epsilon$-contamination, where a possibly non-random $\epsilon$ proportion of one distribution is erroneously grouped with the other. We show that under $\epsilon$-contamination, the typical estimate of the MMD is unreliable. Instead, we study partial identification of the MMD, and characterize sharp upper and lower bounds that contain the true, unknown MMD. We propose a method to estimate these bounds, and show that it gives estimates that converge to the sharpest possible bounds on the MMD as sample size increases, with a convergence rate that is faster than alternative approaches. Using three datasets, we empirically validate that our approach is superior to the alternatives: it gives tight bounds with a low false coverage rate.
</details>
<details>
<summary>摘要</summary>
非 Parametric 两个样本测试，如最大均值差 (MMD)，在机器学习应用中 frequently 用于检测两个分布之间的差异。然而，现有的大多数文献假设可以获得无错的样本从两个分布的兴趣中。我们松弛这个假设，研究在 $\epsilon$-杂杂中测试 MMD 的估计，其中 $\epsilon$ 可能是非随机的。我们表明，在 $\epsilon$-杂杂中，通常的估计 MMD 不可靠。而我们研究 MMD 的部分标识，并Characterize 它们是否可以包含真实不知道的 MMD。我们提出了一种方法来估计这些 bound，并证明它们的估计会随样本大小增加，与其他方法相比，具有更快的收敛速率。使用三个数据集，我们实际验证了我们的方法的优越性：它们给出了紧凑的 bound，低于 false coverage 率。
</details></li>
</ul>
<hr>
<h2 id="A-Transfer-Learning-Framework-for-Proactive-Ramp-Metering-Performance-Assessment"><a href="#A-Transfer-Learning-Framework-for-Proactive-Ramp-Metering-Performance-Assessment" class="headerlink" title="A Transfer Learning Framework for Proactive Ramp Metering Performance Assessment"></a>A Transfer Learning Framework for Proactive Ramp Metering Performance Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03542">http://arxiv.org/abs/2308.03542</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaobo Ma, Adrian Cottam, Mohammad Razaur Rahman Shaon, Yao-Jan Wu</li>
<li>for: 本研究旨在评估幕间计量系统的表现，并对交通管理策略的效iveness进行评估。</li>
<li>methods: 本研究采用了机器学习技术，通过学习before和after情况下的交通状况特征，对新的幕间计量控制策略的效iveness进行预测。</li>
<li>results: 实验结果表明，提出的框架可以成功地预测高速公路交通参数（速度、占用率和流速）的after情况，并可以作为评估幕间计量表现的alternative。<details>
<summary>Abstract</summary>
Transportation agencies need to assess ramp metering performance when deploying or expanding a ramp metering system. The evaluation of a ramp metering strategy is primarily centered around examining its impact on freeway traffic mobility. One way these effects can be explored is by comparing traffic states, such as the speed before and after the ramp metering strategy has been altered. Predicting freeway traffic states for the after scenarios following the implementation of a new ramp metering control strategy could offer valuable insights into the potential effectiveness of the target strategy. However, the use of machine learning methods in predicting the freeway traffic state for the after scenarios and evaluating the effectiveness of transportation policies or traffic control strategies such as ramp metering is somewhat limited in the current literature. To bridge the research gap, this study presents a framework for predicting freeway traffic parameters (speed, occupancy, and flow rate) for the after situations when a new ramp metering control strategy is implemented. By learning the association between the spatial-temporal features of traffic states in before and after situations for known freeway segments, the proposed framework can transfer this learning to predict the traffic parameters for new freeway segments. The proposed framework is built upon a transfer learning model. Experimental results show that the proposed framework is feasible for use as an alternative for predicting freeway traffic parameters to proactively evaluate ramp metering performance.
</details>
<details>
<summary>摘要</summary>
Transportation agencies need to assess ramp metering performance when deploying or expanding a ramp metering system. The evaluation of a ramp metering strategy is primarily centered around examining its impact on freeway traffic mobility. One way these effects can be explored is by comparing traffic states, such as the speed before and after the ramp metering strategy has been altered. Predicting freeway traffic states for the after scenarios following the implementation of a new ramp metering control strategy could offer valuable insights into the potential effectiveness of the target strategy. However, the use of machine learning methods in predicting the freeway traffic state for the after scenarios and evaluating the effectiveness of transportation policies or traffic control strategies such as ramp metering is somewhat limited in the current literature. To bridge the research gap, this study presents a framework for predicting freeway traffic parameters (speed, occupancy, and flow rate) for the after situations when a new ramp metering control strategy is implemented. By learning the association between the spatial-temporal features of traffic states in before and after situations for known freeway segments, the proposed framework can transfer this learning to predict the traffic parameters for new freeway segments. The proposed framework is built upon a transfer learning model. Experimental results show that the proposed framework is feasible for use as an alternative for predicting freeway traffic parameters to proactively evaluate ramp metering performance.Here's the text in Simplified Chinese:交通管理机构需要评估干涉表计划的性能，当部署或扩展干涉表计划时。评估干涉表计划的策略的中心在于研究它们对高速公路交通流动性的影响。一种可以探索这些影响的方法是通过比较交通状态的速度之前和之后干涉表计划的变化。预测高速公路交通状态的后 Situations 可以提供有价值的预测干涉表计划的效果。然而，现有文献中使用机器学习方法预测高速公路交通状态的后 Situations 和评估交通政策或交通控制策略的效果是有限的。为了填补这个研究漏洞，本研究提出了一个框架，用于预测高速公路交通参数（速度、占用率和流速）的后 Situations。该框架基于转移学习模型，可以通过学习知道的高速公路段的空间-时间特征，将其传递到预测新的高速公路段的交通参数。实验结果表明，该框架是可行的，可以作为评估干涉表计划性能的代替方法。
</details></li>
</ul>
<hr>
<h2 id="On-ramp-and-Off-ramp-Traffic-Flows-Estimation-Based-on-A-Data-driven-Transfer-Learning-Framework"><a href="#On-ramp-and-Off-ramp-Traffic-Flows-Estimation-Based-on-A-Data-driven-Transfer-Learning-Framework" class="headerlink" title="On-ramp and Off-ramp Traffic Flows Estimation Based on A Data-driven Transfer Learning Framework"></a>On-ramp and Off-ramp Traffic Flows Estimation Based on A Data-driven Transfer Learning Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03538">http://arxiv.org/abs/2308.03538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaobo Ma, Abolfazl Karimpour, Yao-Jan Wu<br>for: 本研究旨在提供一种数据驱动的框架，以便准确估算 freeway 上匝入和出口的流量。methods: 该框架使用了传输学习模型，该模型可以在不同的交通 Pattern、分布和特征下提供高精度的流量估算。results: 实验结果表明，提案的方法可以在不同的 freeway 上匝入和出口处提供高精度的流量估算，其中流量估算的平均绝对误差在 23.90 veh&#x2F;h 到 40.85 veh&#x2F;h 之间，root mean square error 在 34.55 veh&#x2F;h 到 57.77 veh&#x2F;h 之间。此外，相比 conventinal machine learning model，提案的方法显示更高的表现。<details>
<summary>Abstract</summary>
To develop the most appropriate control strategy and monitor, maintain, and evaluate the traffic performance of the freeway weaving areas, state and local Departments of Transportation need to have access to traffic flows at each pair of on-ramp and off-ramp. However, ramp flows are not always readily available to transportation agencies and little effort has been made to estimate these missing flows in locations where no physical sensors are installed. To bridge this research gap, a data-driven framework is proposed that can accurately estimate the missing ramp flows by solely using data collected from loop detectors on freeway mainlines. The proposed framework employs a transfer learning model. The transfer learning model relaxes the assumption that the underlying data distributions of the source and target domains must be the same. Therefore, the proposed framework can guarantee high-accuracy estimation of on-ramp and off-ramp flows on freeways with different traffic patterns, distributions, and characteristics. Based on the experimental results, the flow estimation mean absolute errors range between 23.90 veh/h to 40.85 veh/h for on-ramps, and 31.58 veh/h to 45.31 veh/h for off-ramps; the flow estimation root mean square errors range between 34.55 veh/h to 57.77 veh/h for on-ramps, and 41.75 veh/h to 58.80 veh/h for off-ramps. Further, the comparison analysis shows that the proposed framework outperforms other conventional machine learning models. The estimated ramp flows based on the proposed method can help transportation agencies to enhance the operations of their ramp control strategies for locations where physical sensors are not installed.
</details>
<details>
<summary>摘要</summary>
要开发最佳的控制策略和监测、维护和评估高速公路叉车区的交通性能，国家和地方交通厅需要有访问每对进口和出口的交通流量数据。然而，进口和出口流量并不总是可以提供给交通厅，而且过去几乎没有尝试估算这些缺失的流量。为了填补这一研究漏洞，我们提出了一种数据驱动的框架，可以准确地估算缺失的进口和出口流量，只使用高速公路主线上的循环探测器数据。我们的框架采用了传输学习模型，这种模型不需要源和目标领域数据分布之间的假设相同。因此，我们的框架可以 garantizar高精度地估算进口和出口流量，并且在不同的交通模式、分布和特点下具有广泛的应用可能性。根据实验结果，估算的进口和出口流量平均绝对误差在23.90辆/小时到40.85辆/小时之间，Root mean square error在34.55辆/小时到57.77辆/小时之间。此外，比较分析表明，我们的方法在其他 convential机器学习模型的基础上具有更高的性能。估算的进口和出口流量可以帮助交通厅在没有物理探测器的情况下提高叉车控制策略的运行效果。
</details></li>
</ul>
<hr>
<h2 id="Deep-Feature-Learning-for-Wireless-Spectrum-Data"><a href="#Deep-Feature-Learning-for-Wireless-Spectrum-Data" class="headerlink" title="Deep Feature Learning for Wireless Spectrum Data"></a>Deep Feature Learning for Wireless Spectrum Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03530">http://arxiv.org/abs/2308.03530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ljupcho Milosheski, Gregor Cerar, Blaž Bertalanič, Carolina Fortuna, Mihael Mohorčič</li>
<li>for: 本研究旨在自动学习无supervision的Feature表示，用于无线传输卷积 clustering。</li>
<li>methods: 我们提出一种基于卷积神经网络的模型，可以自动学习输入数据的减少维度表示，比基eline PCA 减少99.3%的维度。</li>
<li>results: 我们的自动表示学习可以提取细腻的含义块，而基eline只能通过背景噪声来分类数据。<details>
<summary>Abstract</summary>
In recent years, the traditional feature engineering process for training machine learning models is being automated by the feature extraction layers integrated in deep learning architectures. In wireless networks, many studies were conducted in automatic learning of feature representations for domain-related challenges. However, most of the existing works assume some supervision along the learning process by using labels to optimize the model. In this paper, we investigate an approach to learning feature representations for wireless transmission clustering in a completely unsupervised manner, i.e. requiring no labels in the process. We propose a model based on convolutional neural networks that automatically learns a reduced dimensionality representation of the input data with 99.3% less components compared to a baseline principal component analysis (PCA). We show that the automatic representation learning is able to extract fine-grained clusters containing the shapes of the wireless transmission bursts, while the baseline enables only general separability of the data based on the background noise.
</details>
<details>
<summary>摘要</summary>
近年来，传统的特征工程过程为训练机器学习模型被深度学习架构中的特征提取层自动化。在无线网络中，许多研究都是自动学习领域相关挑战的特征表示。然而，大多数现有的工作假设了学习过程中有监督，通过标签来优化模型。在这篇论文中，我们调查了一种没有监督的方法，即无标签的自动特征表示学习方法，用于无线传输协调。我们提议一种基于卷积神经网络的模型，可以自动学习输入数据的减少维度表示，与基准PCA相比，减少了99.3%的特征量。我们显示了自动特征表示学习能够提取无线传输强度波形细腻的分布，而基准只能基于背景噪声进行概括分离。
</details></li>
</ul>
<hr>
<h2 id="AlphaStar-Unplugged-Large-Scale-Offline-Reinforcement-Learning"><a href="#AlphaStar-Unplugged-Large-Scale-Offline-Reinforcement-Learning" class="headerlink" title="AlphaStar Unplugged: Large-Scale Offline Reinforcement Learning"></a>AlphaStar Unplugged: Large-Scale Offline Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03526">http://arxiv.org/abs/2308.03526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michaël Mathieu, Sherjil Ozair, Srivatsan Srinivasan, Caglar Gulcehre, Shangtong Zhang, Ray Jiang, Tom Le Paine, Richard Powell, Konrad Żołna, Julian Schrittwieser, David Choi, Petko Georgiev, Daniel Toyama, Aja Huang, Roman Ring, Igor Babuschkin, Timo Ewalds, Mahyar Bordbar, Sarah Henderson, Sergio Gómez Colmenarejo, Aäron van den Oord, Wojciech Marian Czarnecki, Nando de Freitas, Oriol Vinyals</li>
<li>for: This paper is written to advance offline reinforcement learning algorithms by leveraging the challenging and realistic environment of StarCraft II.</li>
<li>methods: The paper introduces a new benchmark called AlphaStar Unplugged, which includes a dataset, tools, and an evaluation protocol for offline reinforcement learning. The authors also present baseline agents, including behavior cloning, offline variants of actor-critic, and MuZero.</li>
<li>results: The authors achieve a 90% win rate against a previously published AlphaStar behavior cloning agent using only offline data, improving the state of the art of agents using offline data.Here is the same information in Simplified Chinese:</li>
<li>for: 这篇论文是为了提高偏置学习算法的进步，利用星际II的挑战性和实际性。</li>
<li>methods: 论文引入了一个新的基准点，叫做AlphaStar Unplugged，包括一个数据集、工具和评估协议。作者还提供了一些基线代理，如行为做 clone、偏置 variant 和 MuZero。</li>
<li>results: 作者使用仅偏置数据达到了90%的胜率，超过了之前发表的AlphaStar行为做 clone 代理。<details>
<summary>Abstract</summary>
StarCraft II is one of the most challenging simulated reinforcement learning environments; it is partially observable, stochastic, multi-agent, and mastering StarCraft II requires strategic planning over long time horizons with real-time low-level execution. It also has an active professional competitive scene. StarCraft II is uniquely suited for advancing offline RL algorithms, both because of its challenging nature and because Blizzard has released a massive dataset of millions of StarCraft II games played by human players. This paper leverages that and establishes a benchmark, called AlphaStar Unplugged, introducing unprecedented challenges for offline reinforcement learning. We define a dataset (a subset of Blizzard's release), tools standardizing an API for machine learning methods, and an evaluation protocol. We also present baseline agents, including behavior cloning, offline variants of actor-critic and MuZero. We improve the state of the art of agents using only offline data, and we achieve 90% win rate against previously published AlphaStar behavior cloning agent.
</details>
<details>
<summary>摘要</summary>
星际II是一个非常具有挑战性的模拟增强学习环境之一，它是部分可见、随机、多代、需要在长时间 horizon 上进行策略规划，并且需要在实时低级别执行。此外，它还拥有活跃的职业竞赛场景。由于星际II的挑战性和Blizzard公司发布了数百万场星际II游戏记录，因此这个纸使用这些数据来建立了一个标准的基准，称为AlphaStar Unplugged，并在这个基准上引入了前所未有的挑战。我们定义了一个数据集（Blizzard发布的一个子集）、工具和标准化API для机器学习方法，以及评估协议。我们还提供了基线代理，包括行为快照、离线actor-critic和MuZero等。我们使用仅基于离线数据的方法提高了代理的状态，并达到了在之前发布的AlphaStar行为快照代理90%的赢利率。
</details></li>
</ul>
<hr>
<h2 id="Worker-Activity-Recognition-in-Manufacturing-Line-Using-Near-body-Electric-Field"><a href="#Worker-Activity-Recognition-in-Manufacturing-Line-Using-Near-body-Electric-Field" class="headerlink" title="Worker Activity Recognition in Manufacturing Line Using Near-body Electric Field"></a>Worker Activity Recognition in Manufacturing Line Using Near-body Electric Field</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03514">http://arxiv.org/abs/2308.03514</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sungho Suh, Vitor Fortes Rey, Sizhen Bian, Yu-Chi Huang, Jože M. Rožanec, Hooman Tavakoli Ghinani, Bo Zhou, Paul Lukowicz</li>
<li>for: 提高生产效率和产品质量</li>
<li>methods:  combining IMU和body capacitance sensing modules，以及多渠道时序卷积神经网络和深度卷积LSTM的早期和晚期整合方法</li>
<li>results: 在生产线上测试和采集感知器数据后，提议的硬件和神经网络模型显示出优于基eline方法的性能，表明该方法在现实世界应用中具有潜在的潜力。此外，通过加入身体电容感测模块和特征融合方法，提议的感知原型在6.35%的提升和9.38%的高于基eline方法的macro F1分数上表现出了提升。<details>
<summary>Abstract</summary>
Manufacturing industries strive to improve production efficiency and product quality by deploying advanced sensing and control systems. Wearable sensors are emerging as a promising solution for achieving this goal, as they can provide continuous and unobtrusive monitoring of workers' activities in the manufacturing line. This paper presents a novel wearable sensing prototype that combines IMU and body capacitance sensing modules to recognize worker activities in the manufacturing line. To handle these multimodal sensor data, we propose and compare early, and late sensor data fusion approaches for multi-channel time-series convolutional neural networks and deep convolutional LSTM. We evaluate the proposed hardware and neural network model by collecting and annotating sensor data using the proposed sensing prototype and Apple Watches in the testbed of the manufacturing line. Experimental results demonstrate that our proposed methods achieve superior performance compared to the baseline methods, indicating the potential of the proposed approach for real-world applications in manufacturing industries. Furthermore, the proposed sensing prototype with a body capacitive sensor and feature fusion method improves by 6.35%, yielding a 9.38% higher macro F1 score than the proposed sensing prototype without a body capacitive sensor and Apple Watch data, respectively.
</details>
<details>
<summary>摘要</summary>
制造业为提高生产效率和产品质量而努力，投入先进的感知和控制系统。舌环感器是制造业实现这一目标的一种有前途的解决方案，因为它们可以提供不间断和不干扰的工作者活动监测。本文提出了一种新的舌环感器原型， combining IMU和体容感测模块，以认知制造线工作者的活动。为处理这些多modal的感知数据，我们提议并比较早期和晚期感知数据融合方法，用于多通道时序卷积神经网络和深度卷积LSTM。我们通过收集和标注感知数据使用我们提出的感知原型和Apple Watches在制造线测试环境中进行评估。实验结果表明，我们的提议方法可以与基准方法相比，表明我们的方法在实际应用中具有潜在的潜力。此外，我们的感知原型与身体电容感测模块和特征融合方法提高了6.35%，即使比起没有身体电容感测模块和Apple Watch数据的情况下提高9.38%的macro F1分数。
</details></li>
</ul>
<hr>
<h2 id="A-data-driven-approach-to-predict-decision-point-choice-during-normal-and-evacuation-wayfinding-in-multi-story-buildings"><a href="#A-data-driven-approach-to-predict-decision-point-choice-during-normal-and-evacuation-wayfinding-in-multi-story-buildings" class="headerlink" title="A data-driven approach to predict decision point choice during normal and evacuation wayfinding in multi-story buildings"></a>A data-driven approach to predict decision point choice during normal and evacuation wayfinding in multi-story buildings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03511">http://arxiv.org/abs/2308.03511</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yan Feng, Panchamy Krishnakumari</li>
<li>For: 本研究旨在理解和预测在复杂建筑物内的人行道径选择行为，以确保行人安全。* Methods: 本研究使用了数据驱动的方法，包括建立内部网络表示和将VR坐标映射到内部表示的技术，以及一种已知的机器学习算法——Random Forest（RF）模型，来预测行人决策点选择行为。* Results: 研究发现，使用RF模型可以具有较高的预测准确率（平均为93%），比对使用logistic regression模型更高。最高的预测准确率达96%，并且测试结果表明，个人特征不会影响决策点选择。<details>
<summary>Abstract</summary>
Understanding pedestrian route choice behavior in complex buildings is important to ensure pedestrian safety. Previous studies have mostly used traditional data collection methods and discrete choice modeling to understand the influence of different factors on pedestrian route and exit choice, particularly in simple indoor environments. However, research on pedestrian route choice in complex buildings is still limited. This paper presents a data-driven approach for understanding and predicting the pedestrian decision point choice during normal and emergency wayfinding in a multi-story building. For this, we first built an indoor network representation and proposed a data mapping technique to map VR coordinates to the indoor representation. We then used a well-established machine learning algorithm, namely the random forest (RF) model to predict pedestrian decision point choice along a route during four wayfinding tasks in a multi-story building. Pedestrian behavioral data in a multi-story building was collected by a Virtual Reality experiment. The results show a much higher prediction accuracy of decision points using the RF model (i.e., 93% on average) compared to the logistic regression model. The highest prediction accuracy was 96% for task 3. Additionally, we tested the model performance combining personal characteristics and we found that personal characteristics did not affect decision point choice. This paper demonstrates the potential of applying a machine learning algorithm to study pedestrian route choice behavior in complex indoor buildings.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用机器学习算法来理解人行道选择行为在复杂的内部建筑中非常重要，以确保行人安全。先前的研究主要采用传统的数据采集方法和精确选择模型来理解不同因素对人行道径和出口选择的影响，特别是在简单的室内环境中。然而，对于复杂的内部建筑中的人行道选择研究仍然有限。这篇文章介绍了一种数据驱动的方法，用于理解和预测行人决策点选择在常规和紧急导航中的多层建筑中。为此，我们首先构建了内部网络表示，并提出了将VR坐标映射到内部表示的技术。然后，我们使用一种已知的机器学习算法，即随机森林（RF）模型来预测行人决策点选择路径中的四个任务在多层建筑中。我们在内部建筑中收集了行人行为数据，并使用VR实验进行数据采集。结果显示，使用RF模型的预测精度远高于逻辑回归模型（即93%的平均精度），最高的预测精度为任务3（即96%）。此外，我们测试了模型性能的组合个人特征，并发现个人特征没有影响决策点选择。这篇文章表明了使用机器学习算法来研究复杂的内部建筑中人行道选择行为的潜力。
</details></li>
</ul>
<hr>
<h2 id="Balanced-Face-Dataset-Guiding-StyleGAN-to-Generate-Labeled-Synthetic-Face-Image-Dataset-for-Underrepresented-Group"><a href="#Balanced-Face-Dataset-Guiding-StyleGAN-to-Generate-Labeled-Synthetic-Face-Image-Dataset-for-Underrepresented-Group" class="headerlink" title="Balanced Face Dataset: Guiding StyleGAN to Generate Labeled Synthetic Face Image Dataset for Underrepresented Group"></a>Balanced Face Dataset: Guiding StyleGAN to Generate Labeled Synthetic Face Image Dataset for Underrepresented Group</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03495">http://arxiv.org/abs/2308.03495</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kidist Amde Mekonnen</li>
<li>for: 这个研究的目的是生成一个可靠的面部图像集，以便实现不同的人种和性别的资料分布。</li>
<li>methods: 这个研究使用了StyleGAN模型来生成面部图像，并通过控制生成过程来确保资料集中的人种和性别分布均衡。</li>
<li>results: 这个研究产生了一个可靠的面部图像集，并通过实验证明了这个资料集可以用于不同的下游任务。<details>
<summary>Abstract</summary>
For a machine learning model to generalize effectively to unseen data within a particular problem domain, it is well-understood that the data needs to be of sufficient size and representative of real-world scenarios. Nonetheless, real-world datasets frequently have overrepresented and underrepresented groups. One solution to mitigate bias in machine learning is to leverage a diverse and representative dataset. Training a model on a dataset that covers all demographics is crucial to reducing bias in machine learning. However, collecting and labeling large-scale datasets has been challenging, prompting the use of synthetic data generation and active labeling to decrease the costs of manual labeling. The focus of this study was to generate a robust face image dataset using the StyleGAN model. In order to achieve a balanced distribution of the dataset among different demographic groups, a synthetic dataset was created by controlling the generation process of StyleGaN and annotated for different downstream tasks.
</details>
<details>
<summary>摘要</summary>
为一个机器学习模型有效泛化到未看到的数据中，已经是非常了解的一点，那么数据需要具有足够的大小和表示现实世界场景。然而，实际世界数据经常会有过度和不足的群体。一种解决机器学习偏见的方法是利用多样化的和表示性的数据集。训练一个模型在覆盖所有民族的数据集是减少偏见的关键，但是收集和手动标注大规模数据集是困难的，因此使用生成 Synthetic 数据和活动标注来降低手动标注的成本。这项研究的目标是使用 StyleGAN 模型生成一个可靠的脸像数据集，以实现数据集的均衡分布。为了实现这一目标，我们控制了 StyleGAN 生成过程，并对不同下游任务进行了注释。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Physical-World-Adversarial-Robustness-of-Vehicle-Detection"><a href="#Exploring-the-Physical-World-Adversarial-Robustness-of-Vehicle-Detection" class="headerlink" title="Exploring the Physical World Adversarial Robustness of Vehicle Detection"></a>Exploring the Physical World Adversarial Robustness of Vehicle Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03476">http://arxiv.org/abs/2308.03476</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Jiang, Tianyuan Zhang, Shuangcheng Liu, Weiyu Ji, Zichao Zhang, Gang Xiao</li>
<li>for: 评估实际场景下的检测模型 Robustness，但实际 эксперимент具有资源占用和复杂性的问题。</li>
<li>methods: 提出了一种创新的快照水平数据生成管道，使用 CARLA 模拟器，并建立了 Discrete and Continuous Instant-level (DCI) 数据集，可以进行三种检测模型和三种物理攻击的全面实验。</li>
<li>results: 发现 Yolo v6 具有强大的抗击性，仅在对抗攻击下出现了小于1%的 average precision (AP) 下降，而 ASA 攻击则导致了 AP 下降约14.51%，远高于其他算法。 static 场景下的识别 AP 值较高，而不同的天气条件下的结果几乎相同。<details>
<summary>Abstract</summary>
Adversarial attacks can compromise the robustness of real-world detection models. However, evaluating these models under real-world conditions poses challenges due to resource-intensive experiments. Virtual simulations offer an alternative, but the absence of standardized benchmarks hampers progress. Addressing this, we propose an innovative instant-level data generation pipeline using the CARLA simulator. Through this pipeline, we establish the Discrete and Continuous Instant-level (DCI) dataset, enabling comprehensive experiments involving three detection models and three physical adversarial attacks. Our findings highlight diverse model performances under adversarial conditions. Yolo v6 demonstrates remarkable resilience, experiencing just a marginal 6.59% average drop in average precision (AP). In contrast, the ASA attack yields a substantial 14.51% average AP reduction, twice the effect of other algorithms. We also note that static scenes yield higher recognition AP values, and outcomes remain relatively consistent across varying weather conditions. Intriguingly, our study suggests that advancements in adversarial attack algorithms may be approaching its ``limitation''.In summary, our work underscores the significance of adversarial attacks in real-world contexts and introduces the DCI dataset as a versatile benchmark. Our findings provide valuable insights for enhancing the robustness of detection models and offer guidance for future research endeavors in the realm of adversarial attacks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="How-to-forecast-power-generation-in-wind-farms-Insights-from-leveraging-hierarchical-structure"><a href="#How-to-forecast-power-generation-in-wind-farms-Insights-from-leveraging-hierarchical-structure" class="headerlink" title="How to forecast power generation in wind farms? Insights from leveraging hierarchical structure"></a>How to forecast power generation in wind farms? Insights from leveraging hierarchical structure</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03472">http://arxiv.org/abs/2308.03472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas English, Mahdi Abolghasemi</li>
<li>for: 预测可再生能源生产，帮助决策全球减排。</li>
<li>methods: 使用树状层次预测和整合预测，包括线性回归和梯度提升机器学习。</li>
<li>results: 跨时间层次预测超过具体时间层次预测，并且机器学习模型在大多数层次上表现较好。<details>
<summary>Abstract</summary>
Forecasting of renewable energy generation provides key insights which may help with decision-making towards global decarbonisation. Renewable energy generation can often be represented through cross-sectional hierarchies, whereby a single farm may have multiple individual generators. Hierarchical forecasting through reconciliation has demonstrated a significant increase in the quality of forecasts both theoretically and empirically. However, it is not evident whether forecasts generated by individual temporal and cross-sectional aggregation can be superior to integrated cross-temporal forecasts and to individual forecasts on more granular data. In this study, we investigate the accuracies of different cross-sectional and cross-temporal reconciliation methods using both linear regression and gradient boosting machine learning for forecasting wind farm power generation. We found that cross-temporal reconciliation is superior to individual cross-sectional reconciliation at multiple temporal aggregations. Cross-temporally reconciled machine learning base forecasts also demonstrated a high accuracy at coarser temporal granularities, which may encourage adoption for short-term wind forecasts. We also show that linear regression can outperform machine learning models across most levels in cross-sectional wind time series.
</details>
<details>
<summary>摘要</summary>
预测可再生能源生产提供关键的洞察，可以帮助决策全球减排。可再生能源生产经常可以用 Hierarchical 模型来表示，一个农场可能有多个个体发电机。通过协调预测，可以显著提高预测质量， both theoretically and empirically。然而，不知道个体时间和横向汇总预测是否高于集成时间汇总预测和更细的数据预测。本研究发现，不同的横向和时间汇总协调方法的准确率，使用线性回归和梯度提升机器学习模型预测风电厂电力生产。我们发现，协调预测在多个时间层次上都高于个体横向协调预测。同时，使用机器学习模型进行协调预测也在大部分水平上达到了高准确率，这可能会促进短期风预测的采用。此外，我们还发现了线性回归在大部分水平上可以超越机器学习模型。
</details></li>
</ul>
<hr>
<h2 id="Wide-Gaps-and-Clustering-Axioms"><a href="#Wide-Gaps-and-Clustering-Axioms" class="headerlink" title="Wide Gaps and Clustering Axioms"></a>Wide Gaps and Clustering Axioms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03464">http://arxiv.org/abs/2308.03464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mieczysław A. Kłopotek</li>
<li>for: 本研究旨在探讨k-means算法是否遵循克林伯格的距离基于减少算法的axiomaatic系统，以及如何修正k-means算法以遵循这些axioma。</li>
<li>methods: 本研究使用了两个新的clusterability性质：variational k-separability和residual k-separability，以确定k-means算法在欧几何或非欧几何空间中是否遵循克林伯格的一致性axioma。此外，本研究还提出了修正k-means算法以遵循克林伯格的贫含性axioma的方法。</li>
<li>results: 本研究发现，k-means算法在欧几何和非欧几何空间中都不遵循克林伯格的一致性axioma，但可以通过修正k-means算法来遵循这些axioma。此外，本研究还提出了一种构建测试数据集的方法，以便测试修正后的k-means算法的性能。<details>
<summary>Abstract</summary>
The widely applied k-means algorithm produces clusterings that violate our expectations with respect to high/low similarity/density and is in conflict with Kleinberg's axiomatic system for distance based clustering algorithms that formalizes those expectations in a natural way. k-means violates in particular the consistency axiom. We hypothesise that this clash is due to the not explicated expectation that the data themselves should have the property of being clusterable in order to expect the algorithm clustering hem to fit a clustering axiomatic system. To demonstrate this, we introduce two new clusterability properties, variational k-separability and residual k-separability and show that then the Kleinberg's consistency axiom holds for k-means operating in the Euclidean or non-Euclidean space. Furthermore, we propose extensions of k-means algorithm that fit approximately the Kleinberg's richness axiom that does not hold for k-means. In this way, we reconcile k-means with Kleinberg's axiomatic framework in Euclidean and non-Euclidean settings. Besides contribution to the theory of axiomatic frameworks of clustering and for clusterability theory, practical contribution is the possibility to construct {datasets for testing purposes of algorithms optimizing k-means cost function. This includes a method of construction of {clusterable data with known in advance global optimum.
</details>
<details>
<summary>摘要</summary>
广泛应用的k-means算法会生成不符我们的预期的分群结果，特别是高低相似度和密度方面的预期不符，这与克莱恩贝格的分群算法axiomaatic系统不符。k-means特别违反了一致性axioma。我们假设这是因为没有明确地预期数据本身应有分群的性质，以期望算法可以遵循分群axiomaatic系统。为了证明这一点，我们引入了两个新的分群性质：variational k-separability和residual k-separability，并证明了在欧几何或非欧几何空间中，k-means算法会遵循克莱恩贝格的一致性axioma。此外，我们提出了对k-means算法进行修改，以便更好地遵循克莱恩贝格的丰富性axioma，这些axioma不会在k-means算法中实现。这种修改可以在欧几何和非欧几何空间中进行。此外，我们还可以根据这些修改建立{测试用数据集，以便确认分群算法的效果。包括一种方法建立高度分群的数据集，并且知道在先的全局最佳解。}
</details></li>
</ul>
<hr>
<h2 id="High-Resolution-Cranial-Defect-Reconstruction-by-Iterative-Low-Resolution-Point-Cloud-Completion-Transformers"><a href="#High-Resolution-Cranial-Defect-Reconstruction-by-Iterative-Low-Resolution-Point-Cloud-Completion-Transformers" class="headerlink" title="High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers"></a>High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03813">http://arxiv.org/abs/2308.03813</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MWod/DeepImplant_MICCAI_2023">https://github.com/MWod/DeepImplant_MICCAI_2023</a></li>
<li>paper_authors: Marek Wodzinski, Mateusz Daniol, Daria Hemmerling, Miroslaw Socha</li>
<li>for:  This paper aims to provide an automatic, dedicated system for personalized cranial reconstruction, addressing the problem of cranial defect reconstruction.</li>
<li>methods:  The proposed method uses an iterative, transformer-based approach to complete point clouds and reconstruct cranial defects at any resolution, while being fast and resource-efficient during training and inference.</li>
<li>results:  The proposed method demonstrates superior performance in terms of GPU memory consumption while maintaining high-quality of the reconstructed defects, compared to state-of-the-art volumetric approaches.<details>
<summary>Abstract</summary>
Each year thousands of people suffer from various types of cranial injuries and require personalized implants whose manual design is expensive and time-consuming. Therefore, an automatic, dedicated system to increase the availability of personalized cranial reconstruction is highly desirable. The problem of the automatic cranial defect reconstruction can be formulated as the shape completion task and solved using dedicated deep networks. Currently, the most common approach is to use the volumetric representation and apply deep networks dedicated to image segmentation. However, this approach has several limitations and does not scale well into high-resolution volumes, nor takes into account the data sparsity. In our work, we reformulate the problem into a point cloud completion task. We propose an iterative, transformer-based method to reconstruct the cranial defect at any resolution while also being fast and resource-efficient during training and inference. We compare the proposed methods to the state-of-the-art volumetric approaches and show superior performance in terms of GPU memory consumption while maintaining high-quality of the reconstructed defects.
</details>
<details>
<summary>摘要</summary>
每年千计人们因各种类型的脑部受伤而需要个性化嵌入，其手动设计成本高昂，时间费时。因此，一个自动化、专门的系统可以提高个性化脑部重建的可用性是非常感兴趣的。脑部异常完成任务可以通过专门的深度网络解决。现在，最常见的方法是使用体积表示法，并将深度网络应用于图像分割。然而，这种方法存在一些限制，不能扩展到高分辨率体积，也不会考虑数据稀缺性。在我们的工作中，我们将问题重新划分为点云完成任务。我们提出一种迭代、基于变换器的方法来重建脑部异常，可以在任何分辨率下进行重建，同时在训练和推理过程中具有快速和资源高效的特点。我们与状态元方法进行比较，并显示我们的方法在GPU内存占用量方面具有明显的优势，而无需牺牲高质量重建异常的性能。
</details></li>
</ul>
<hr>
<h2 id="Redesigning-Out-of-Distribution-Detection-on-3D-Medical-Images"><a href="#Redesigning-Out-of-Distribution-Detection-on-3D-Medical-Images" class="headerlink" title="Redesigning Out-of-Distribution Detection on 3D Medical Images"></a>Redesigning Out-of-Distribution Detection on 3D Medical Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07324">http://arxiv.org/abs/2308.07324</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anton Vasiliuk, Daria Frolova, Mikhail Belyaev, Boris Shirokikh</li>
<li>for: 本文旨在解决验证医疗图像分割中的非常规（OOD）样本检测问题。</li>
<li>methods: 本文提出一种基于下游任务的OOD检测方法，使用下游模型的性能作为图像之间的pseudometric，不需要显式地区分ID和OOD样本。</li>
<li>results: 在11个CT和MRI OOD检测挑战中，EPD metric 能够准确地评估不同方法的临床影响。<details>
<summary>Abstract</summary>
Detecting out-of-distribution (OOD) samples for trusted medical image segmentation remains a significant challenge. The critical issue here is the lack of a strict definition of abnormal data, which often results in artificial problem settings without measurable clinical impact. In this paper, we redesign the OOD detection problem according to the specifics of volumetric medical imaging and related downstream tasks (e.g., segmentation). We propose using the downstream model's performance as a pseudometric between images to define abnormal samples. This approach enables us to weigh different samples based on their performance impact without an explicit ID/OOD distinction. We incorporate this weighting in a new metric called Expected Performance Drop (EPD). EPD is our core contribution to the new problem design, allowing us to rank methods based on their clinical impact. We demonstrate the effectiveness of EPD-based evaluation in 11 CT and MRI OOD detection challenges.
</details>
<details>
<summary>摘要</summary>
检测非常出版（OOD）样本 для可信度医疗影像分割是一个主要挑战。这里的关键问题是缺乏严格的非常定义，这经常导致人工设定的问题 без measurable clinical impact。在这篇论文中，我们重新设计了OOD检测问题，根据医疗影像的特点和相关的下游任务（例如分割）。我们提议使用下游模型的性能作为图像之间的pseudometric。这种方法允许我们根据不同样本的性能影响 assign weights，而不需要显式的ID/OOD分类。我们称之为预期性能下降（EPD）。EPD是我们对新的问题设计的核心贡献，允许我们根据临床影响排名方法。我们在11个CT和MRI OOD检测挑战中证明了EPD基于评价的效果。
</details></li>
</ul>
<hr>
<h2 id="Cross-Silo-Prototypical-Calibration-for-Federated-Learning-with-Non-IID-Data"><a href="#Cross-Silo-Prototypical-Calibration-for-Federated-Learning-with-Non-IID-Data" class="headerlink" title="Cross-Silo Prototypical Calibration for Federated Learning with Non-IID Data"></a>Cross-Silo Prototypical Calibration for Federated Learning with Non-IID Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03457">http://arxiv.org/abs/2308.03457</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qizhuang-qz/FedCSPC">https://github.com/qizhuang-qz/FedCSPC</a></li>
<li>paper_authors: Zhuang Qi, Lei Meng, Zitan Chen, Han Hu, Hui Lin, Xiangxu Meng</li>
<li>for: 这个研究目的是为了在隐私保护下，通过服务器端对各个客户端的本地模型进行联合学习，以获得更好的模型泛化能力。</li>
<li>methods: 这个研究使用了跨批训练（FedCSPC），它首先使用资料prototype模型（DPM）模组来学习资料模式，以帮助标准化。接着，它使用跨批对称学习（CSPC）模组来改善标准化的实现方式，以将不同来源的特征投射到一个共同的空间中，保持清晰的决策界限。</li>
<li>results: 实验结果显示，FedCSPC可以在不同资料来源之间的同类别中学习共同的特征，从而获得更好的性能，比预先的方法更好。<details>
<summary>Abstract</summary>
Federated Learning aims to learn a global model on the server side that generalizes to all clients in a privacy-preserving manner, by leveraging the local models from different clients. Existing solutions focus on either regularizing the objective functions among clients or improving the aggregation mechanism for the improved model generalization capability. However, their performance is typically limited by the dataset biases, such as the heterogeneous data distributions and the missing classes. To address this issue, this paper presents a cross-silo prototypical calibration method (FedCSPC), which takes additional prototype information from the clients to learn a unified feature space on the server side. Specifically, FedCSPC first employs the Data Prototypical Modeling (DPM) module to learn data patterns via clustering to aid calibration. Subsequently, the cross-silo prototypical calibration (CSPC) module develops an augmented contrastive learning method to improve the robustness of the calibration, which can effectively project cross-source features into a consistent space while maintaining clear decision boundaries. Moreover, the CSPC module's ease of implementation and plug-and-play characteristics make it even more remarkable. Experiments were conducted on four datasets in terms of performance comparison, ablation study, in-depth analysis and case study, and the results verified that FedCSPC is capable of learning the consistent features across different data sources of the same class under the guidance of calibrated model, which leads to better performance than the state-of-the-art methods. The source codes have been released at https://github.com/qizhuang-qz/FedCSPC.
</details>
<details>
<summary>摘要</summary>
federated 学习旨在在服务器端学习一个通用模型，以保持所有客户端的隐私，通过客户端的本地模型之间的协同学习。现有的解决方案通常是通过客户端对象函数的规范化或者改进模型融合机制来提高模型通用能力。然而，它们的性能通常受到数据偏见的影响，如不同数据分布和缺失类。为解决这个问题，本文提出了跨批模型准确补偿方法（FedCSPC），它在服务器端使用客户端提供的额外原型信息来学习一个统一的特征空间。具体来说，FedCSPC首先使用数据模型化模块（DPM）来学习数据模式，以帮助准确补偿。接着，跨批模型准确补偿模块（CSPC）发展了一种增强了对比学习方法，可以有效地将各种来源特征投影到一个具有清晰决策边界的共同空间中，而不是只是在不同数据源之间进行准确补偿。此外，CSPC模块的易于实现和插件化特点使得它更加出佩。实验结果表明，FedCSPC能够在不同数据源之间学习一致的特征，从而实现更好的性能，而且超越了当前的方法。代码已经在 GitHub 上发布，请参考 <https://github.com/qizhuang-qz/FedCSPC>。
</details></li>
</ul>
<hr>
<h2 id="Doubly-Robust-Estimator-for-Off-Policy-Evaluation-with-Large-Action-Spaces"><a href="#Doubly-Robust-Estimator-for-Off-Policy-Evaluation-with-Large-Action-Spaces" class="headerlink" title="Doubly Robust Estimator for Off-Policy Evaluation with Large Action Spaces"></a>Doubly Robust Estimator for Off-Policy Evaluation with Large Action Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03443">http://arxiv.org/abs/2308.03443</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tatsu432/DR-estimator-OPE-large-action">https://github.com/tatsu432/DR-estimator-OPE-large-action</a></li>
<li>paper_authors: Tatsuhiro Shimizu, Laura Forastiere</li>
<li>for: 评估无策策略（Off-Policy Evaluation）在Contextual Bandit设置下，即在具有大量动作空间的情况下。</li>
<li>methods: 本研究使用Marginalized Inverse Propensity Scoring（MIPS）和Marginalized Doubly Robust（MDR）等方法来mitigate estimator的偏差和方差问题。</li>
<li>results:  theoretically和empirically验证了MDR estimator的超越性，即在比MIPS更加强的假设下，MDR estimator still maintains variance reduction against IPS。<details>
<summary>Abstract</summary>
We study Off-Policy Evaluation (OPE) in contextual bandit settings with large action spaces. The benchmark estimators suffer from severe bias and variance tradeoffs. Parametric approaches suffer from bias due to difficulty specifying the correct model, whereas ones with importance weight suffer from variance. To overcome these limitations, Marginalized Inverse Propensity Scoring (MIPS) was proposed to mitigate the estimator's variance via embeddings of an action. To make the estimator more accurate, we propose the doubly robust estimator of MIPS called the Marginalized Doubly Robust (MDR) estimator. Theoretical analysis shows that the proposed estimator is unbiased under weaker assumptions than MIPS while maintaining variance reduction against IPS, which was the main advantage of MIPS. The empirical experiment verifies the supremacy of MDR against existing estimators.
</details>
<details>
<summary>摘要</summary>
我们研究偏离策略评估（OPE）在含有大量行动的上下文抽象队列设置下。参考估计器受到严重的偏见和方差负担交易。 Parametric方法受到偏见因为难以正确地特定模型，而重要性Weighted方法受到方差。为了解决这些限制，我们提出了嵌入行动的Marginalized Inverse Propensity Scoring（MIPS）来减少估计器的方差。为了使估计器更加准确，我们提出了MIPS的双重Robust（MDR）估计器。理论分析表明，我们的估计器具有较弱的假设下的不偏性，而且保持与IPS相同的方差减少。实验证明我们的MDR估计器在现有估计器中具有最高的超越性。
</details></li>
</ul>
<hr>
<h2 id="PURL-Safe-and-Effective-Sanitization-of-Link-Decoration"><a href="#PURL-Safe-and-Effective-Sanitization-of-Link-Decoration" class="headerlink" title="PURL: Safe and Effective Sanitization of Link Decoration"></a>PURL: Safe and Effective Sanitization of Link Decoration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03417">http://arxiv.org/abs/2308.03417</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/purl-sanitizer/purl">https://github.com/purl-sanitizer/purl</a></li>
<li>paper_authors: Shaoor Munir, Patrick Lee, Umar Iqbal, Zubair Shafiq, Sandra Siby</li>
<li>for: 防止隐私浏览器中的第三方cookies和浏览器指纹被新的跟踪方法绕过安全措施。</li>
<li>methods: 使用机器学习方法，利用浏览器执行页面的跨层图表来检测和净化链接装饰中的跟踪信息。</li>
<li>results: PURL可以准确地检测和净化链接装饰中的跟踪信息，比已有Countermeasures更高效和更具有抗辐射性。在测试 top-million 网站时，发现了广泛的链接装饰滥用，其中包括著名的广告商和跟踪者，以便从浏览器存储、邮箱地址和指纹扫描中收集用户信息。<details>
<summary>Abstract</summary>
While privacy-focused browsers have taken steps to block third-party cookies and browser fingerprinting, novel tracking methods that bypass existing defenses continue to emerge. Since trackers need to exfiltrate information from the client- to server-side through link decoration regardless of the tracking technique they employ, a promising orthogonal approach is to detect and sanitize tracking information in decorated links. We present PURL, a machine-learning approach that leverages a cross-layer graph representation of webpage execution to safely and effectively sanitize link decoration. Our evaluation shows that PURL significantly outperforms existing countermeasures in terms of accuracy and reducing website breakage while being robust to common evasion techniques. We use PURL to perform a measurement study on top-million websites. We find that link decorations are widely abused by well-known advertisers and trackers to exfiltrate user information collected from browser storage, email addresses, and scripts involved in fingerprinting.
</details>
<details>
<summary>摘要</summary>
While privacy-focused browsers have taken steps to block third-party cookies and browser fingerprinting, novel tracking methods that bypass existing defenses continue to emerge. Since trackers need to exfiltrate information from the client- to server-side through link decoration regardless of the tracking technique they employ, a promising orthogonal approach is to detect and sanitize tracking information in decorated links. We present PURL, a machine-learning approach that leverages a cross-layer graph representation of webpage execution to safely and effectively sanitize link decoration. Our evaluation shows that PURL significantly outperforms existing countermeasures in terms of accuracy and reducing website breakage while being robust to common evasion techniques. We use PURL to perform a measurement study on top-million websites. We find that link decorations are widely abused by well-known advertisers and trackers to exfiltrate user information collected from browser storage, email addresses, and scripts involved in fingerprinting.Here's the translation in Traditional Chinese:随着隐私专注浏览器对第三方Cookie和浏览器指纹进行防护，新的跟踪方法继续出现，这些方法可以绕过现有的防护措施。因为追踪者需要从客户端将资讯传到服务器端，因此一个可能的垂直方法是检测并删除装饰链接中的追踪资讯。我们提出了PURL，一种基于页面执行的机器学习方法，可以安全地和有效地删除链接装饰。我们的评估显示，PURL Significantly Outperform现有的对抗策略，并且具有较高的精度和降低网站损坏的能力，同时具有对常见的逃脱技巧的坚固性。我们使用PURL进行了顶千个网站的量测研究，发现链接装饰被著名的广告商和追踪者广泛运用，以将用户资讯从浏览器存储、电子邮件地址和掌握的指纹资讯泄露出来。
</details></li>
</ul>
<hr>
<h2 id="Noncompact-uniform-universal-approximation"><a href="#Noncompact-uniform-universal-approximation" class="headerlink" title="Noncompact uniform universal approximation"></a>Noncompact uniform universal approximation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03812">http://arxiv.org/abs/2308.03812</a></li>
<li>repo_url: None</li>
<li>paper_authors: Teun D. H. van Nuland</li>
<li>for: 这个论文探讨了一般化的universal approximation theorem的推广，具体来说是在非可ompact的输入空间 $\mathbb R^n$ 上进行的。</li>
<li>methods: 这个论文使用了神经网络来进行uniform approximation，并且研究了不同 activation functions 的影响。</li>
<li>results: 研究发现，对于所有的连续函数，只要它们在 infinity 处消失， Then all continuous functions that vanish at infinity can be uniformly approximated by neural networks with one hidden layer, for all continuous activation functions $\varphi\neq0$ with asymptotically linear behaviour at $\pm\infty$. In addition, the paper also found some unexpected results, such as the algebra of uniformly approximable functions being independent of the activation function and the number of hidden layers.<details>
<summary>Abstract</summary>
The universal approximation theorem is generalised to uniform convergence on the (noncompact) input space $\mathbb R^n$. All continuous functions that vanish at infinity can be uniformly approximated by neural networks with one hidden layer, for all continuous activation functions $\varphi\neq0$ with asymptotically linear behaviour at $\pm\infty$. When $\varphi$ is moreover bounded, we exactly determine which functions can be uniformly approximated by neural networks, with the following unexpected results. Let $\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}$ denote the vector space of functions that are uniformly approximable by neural networks with $l$ hidden layers and $n$ inputs. For all $n$ and all $l\geq2$, $\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}$ turns out to be an algebra under the pointwise product. If the left limit of $\varphi$ differs from its right limit (for instance, when $\varphi$ is sigmoidal) the algebra $\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}$ ($l\geq2$) is independent of $\varphi$ and $l$, and equals the closed span of products of sigmoids composed with one-dimensional projections. If the left limit of $\varphi$ equals its right limit, $\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}$ ($l\geq1$) equals the (real part of the) commutative resolvent algebra, a C*-algebra which is used in mathematical approaches to quantum theory. In the latter case, the algebra is independent of $l\geq1$, whereas in the former case $\overline{\mathcal{N}_\varphi^2(\mathbb R^n)}$ is strictly bigger than $\overline{\mathcal{N}_\varphi^1(\mathbb R^n)}$.
</details>
<details>
<summary>摘要</summary>
“ universal approximation theorem 被推广到非紧集 $\mathbb R^n$ 上的输入空间。所有的连续函数，当 $x$ 趋于 $\pm \infty$ 时，有 asymptotically linear 的行为的函数 $\varphi\neq0$，可以通过含有一个隐藏层的神经网络进行不同化。当 $\varphi$ 又是受限的时，我们可以准确地确定可以通过神经网络进行不同化的函数，并且得到了以下意外的结果。对于所有的 $n$ 和 $l\geq2$，$\mathcal{N}^l_\varphi(\mathbb R^n)$ 是一个点wise乘法的代数。如果 $\varphi$ 的左限与右限不同（例如，当 $\varphi$ 是截股函数），则 $\mathcal{N}^l_\varphi(\mathbb R^n)$ ($l\geq2$) 是 $\varphi$ 和 $l$ 的独立的代数，等于一个由截股函数与一维投影组成的关闭 span。如果 $\varphi$ 的左限与右限相同（例如，当 $\varphi$ 是满足条件的函数），则 $\mathcal{N}^l_\varphi(\mathbb R^n)$ ($l\geq1$) 是一个（实部的） коммуutatvie 分解代数，这种代数在数学方面的量子理论中使用。在后者情况下，该代数是 $l\geq1$ 的独立的，而在前者情况下，$\mathcal{N}^2_\varphi(\mathbb R^n)$ 是 $\mathcal{N}^1_\varphi(\mathbb R^n)$ 的strictly larger。”
</details></li>
</ul>
<hr>
<h2 id="Applied-metamodelling-for-ATM-performance-simulations"><a href="#Applied-metamodelling-for-ATM-performance-simulations" class="headerlink" title="Applied metamodelling for ATM performance simulations"></a>Applied metamodelling for ATM performance simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03404">http://arxiv.org/abs/2308.03404</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christoffer Riis, Francisco N. Antunes, Tatjana Bolić, Gérald Gurtner, Andrew Cook, Carlos Lima Azevedo, Francisco Câmara Pereira</li>
<li>for: 支持空管决策making，提高空管 simulator的解释能力和可repeatability。</li>
<li>methods: 使用活动学习和 SHAP 值 integrate into simulation metamodels，快速浮现空管 simulator中输入和输出变量之间的隐藏关系。</li>
<li>results: 在使用 ‘Mercury’ 空管 simulator的实际场景中，XALM 能够增强simulation解释性和理解变量之间的交互关系，并且与非活动学习 мета模型相比，具有更好的解释能力。<details>
<summary>Abstract</summary>
The use of Air traffic management (ATM) simulators for planing and operations can be challenging due to their modelling complexity. This paper presents XALM (eXplainable Active Learning Metamodel), a three-step framework integrating active learning and SHAP (SHapley Additive exPlanations) values into simulation metamodels for supporting ATM decision-making. XALM efficiently uncovers hidden relationships among input and output variables in ATM simulators, those usually of interest in policy analysis. Our experiments show XALM's predictive performance comparable to the XGBoost metamodel with fewer simulations. Additionally, XALM exhibits superior explanatory capabilities compared to non-active learning metamodels.   Using the `Mercury' (flight and passenger) ATM simulator, XALM is applied to a real-world scenario in Paris Charles de Gaulle airport, extending an arrival manager's range and scope by analysing six variables. This case study illustrates XALM's effectiveness in enhancing simulation interpretability and understanding variable interactions. By addressing computational challenges and improving explainability, XALM complements traditional simulation-based analyses.   Lastly, we discuss two practical approaches for reducing the computational burden of the metamodelling further: we introduce a stopping criterion for active learning based on the inherent uncertainty of the metamodel, and we show how the simulations used for the metamodel can be reused across key performance indicators, thus decreasing the overall number of simulations needed.
</details>
<details>
<summary>摘要</summary>
使用空交通管理（ATM）模拟器进行规划和运营可能会面临模拟复杂性挑战。这篇论文介绍了XALM（可解释性活动学习元模型），一种三步框架，它将活动学习和SHAP（SHapley Additive exPlanations）值 integrate到模拟元模型中，以支持ATM决策。XALM有效地揭示了ATM模拟器中输入和输出变量之间的隐藏关系，通常在政策分析中对于有价值。我们的实验表明，XALM的预测性能与XGBoost元模型相当，但使用 fewer simulations。此外，XALM的解释能力比非活动学习元模型更高。使用“Mercury”（飞机和乘客）ATM模拟器，XALM在法国 CHARLES DE GAULLE机场的一个实际场景中应用，分析了六个变量。这个案例说明了XALM在提高模拟解释性和理解变量间关系方面的效iveness。通过解决计算挑战和提高解释性，XALM补充了传统基于模拟的分析。最后，我们介绍了两种实用的方法来降低元模型计算的压力：我们引入基于元模型的活动学习停止 criterion，以及如何将模拟用于元模型可以重用到关键性能指标上，从而降低总的模拟数量。
</details></li>
</ul>
<hr>
<h2 id="Towards-Machine-Learning-based-Fish-Stock-Assessment"><a href="#Towards-Machine-Learning-based-Fish-Stock-Assessment" class="headerlink" title="Towards Machine Learning-based Fish Stock Assessment"></a>Towards Machine Learning-based Fish Stock Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03403">http://arxiv.org/abs/2308.03403</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Lüdtke, Maria E. Pierce</li>
<li>for: This paper aims to improve the estimation and forecast of fish stock parameters like recruitment and spawning stock biomass, which is crucial for sustainable fisheries management.</li>
<li>methods: The authors propose a hybrid model that combines classical statistical stock assessment models with supervised machine learning, specifically gradient boosted trees. The model leverages the initial estimate provided by the classical model and uses the ML model to make a post-hoc correction to improve accuracy.</li>
<li>results: The authors experiment with five different stocks and find that the forecast accuracy of recruitment and spawning stock biomass improves considerably in most cases.<details>
<summary>Abstract</summary>
The accurate assessment of fish stocks is crucial for sustainable fisheries management. However, existing statistical stock assessment models can have low forecast performance of relevant stock parameters like recruitment or spawning stock biomass, especially in ecosystems that are changing due to global warming and other anthropogenic stressors. In this paper, we investigate the use of machine learning models to improve the estimation and forecast of such stock parameters. We propose a hybrid model that combines classical statistical stock assessment models with supervised ML, specifically gradient boosted trees. Our hybrid model leverages the initial estimate provided by the classical model and uses the ML model to make a post-hoc correction to improve accuracy. We experiment with five different stocks and find that the forecast accuracy of recruitment and spawning stock biomass improves considerably in most cases.
</details>
<details>
<summary>摘要</summary>
Accurate assessment of fish stocks is crucial for sustainable fisheries management. However, existing statistical stock assessment models can have low forecast performance of relevant stock parameters like recruitment or spawning stock biomass, especially in ecosystems that are changing due to global warming and other anthropogenic stressors. In this paper, we investigate the use of machine learning models to improve the estimation and forecast of such stock parameters. We propose a hybrid model that combines classical statistical stock assessment models with supervised machine learning, specifically gradient boosted trees. Our hybrid model leverages the initial estimate provided by the classical model and uses the machine learning model to make a post-hoc correction to improve accuracy. We experiment with five different stocks and find that the forecast accuracy of recruitment and spawning stock biomass improves considerably in most cases.Here's the text with some notes on the translation:* "accurate assessment" is 准确评估 (zhèngjù píngzhèng)* "fish stocks" is 鱼类资源 (yúlèi zīyuán)* "sustainable fisheries management" is 可持续鱼业管理 (kěchéngxù yúyèguǎn lí)* "existing statistical stock assessment models" is 现有的统计鱼类评估模型 (xiàn yǒu de tōngjī yúlèi píngzhèng módel)* "global warming" is 全球变暖 (quánqiú biàndòng)* "anthropogenic stressors" is 人类活动对鱼类资源的影响 (rénxìng huódòng duì yúlèi zīyuán de yìngxiàn)* "machine learning models" is 机器学习模型 (jīshì xuéxí módel)* "hybrid model" is 混合模型 (fù hé módel)* "classical statistical stock assessment models" is 传统的统计鱼类评估模型 (chuán tiān de tōngjī yúlèi píngzhèng módel)* "post-hoc correction" is 后置 corrections (hòu zhì jièduì)* "recruitment" is 增殖 (zēngshòu)* "spawning stock biomass" is 繁殖床质量 (shèngcháng zhīyù)I hope this helps! Let me know if you have any further questions or if you'd like me to translate anything else.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Nucleus-Segmentation-with-HARU-Net-A-Hybrid-Attention-Based-Residual-U-Blocks-Network"><a href="#Enhancing-Nucleus-Segmentation-with-HARU-Net-A-Hybrid-Attention-Based-Residual-U-Blocks-Network" class="headerlink" title="Enhancing Nucleus Segmentation with HARU-Net: A Hybrid Attention Based Residual U-Blocks Network"></a>Enhancing Nucleus Segmentation with HARU-Net: A Hybrid Attention Based Residual U-Blocks Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03382">http://arxiv.org/abs/2308.03382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junzhou Chen, Qian Huang, Yulin Chen, Linyi Qian, Chengyuan Yu</li>
<li>for: 本研究旨在提出一种基于二支分支网络和卷积块的杂合注意力的核实例分割方法，以提高核实例分割的精度和效率。</li>
<li>methods: 本方法使用了hybrid attention based residual U-blocks和context fusion block (CF-block)来同时预测目标信息和目标极值。CF-block可以有效地抽取和融合网络中的Contextual information。</li>
<li>results: 对于BNS、MoNuSeg、CoNSeg和CPM-17等数据集，实验结果表明，提出的方法比state-of-the-art方法具有更高的性能。<details>
<summary>Abstract</summary>
Nucleus image segmentation is a crucial step in the analysis, pathological diagnosis, and classification, which heavily relies on the quality of nucleus segmentation. However, the complexity of issues such as variations in nucleus size, blurred nucleus contours, uneven staining, cell clustering, and overlapping cells poses significant challenges. Current methods for nucleus segmentation primarily rely on nuclear morphology or contour-based approaches. Nuclear morphology-based methods exhibit limited generalization ability and struggle to effectively predict irregular-shaped nuclei, while contour-based extraction methods face challenges in accurately segmenting overlapping nuclei. To address the aforementioned issues, we propose a dual-branch network using hybrid attention based residual U-blocks for nucleus instance segmentation. The network simultaneously predicts target information and target contours. Additionally, we introduce a post-processing method that combines the target information and target contours to distinguish overlapping nuclei and generate an instance segmentation image. Within the network, we propose a context fusion block (CF-block) that effectively extracts and merges contextual information from the network. Extensive quantitative evaluations are conducted to assess the performance of our method. Experimental results demonstrate the superior performance of the proposed method compared to state-of-the-art approaches on the BNS, MoNuSeg, CoNSeg, and CPM-17 datasets.
</details>
<details>
<summary>摘要</summary>
核心图像分割是生物学分析、病理诊断和分类中的关键步骤，其中核心分割质量直接影响分析结果。然而，核心的变化、杂乱的核心边缘、不均匀染色、细胞堆叠和重叠细胞等问题带来了 significiant challenges。现有的核心分割方法主要基于核心形态或边缘提取方法。核心形态基于方法在面对不规则形状的核心时表现有限的泛化能力，而边缘提取方法在处理重叠的核心时遇到困难。为了解决上述问题，我们提出了一种基于 dual-branch 网络和卷积 residual U-块的核心实例分割方法。该网络同时预测目标信息和目标边缘。此外，我们引入了一种将目标信息和目标边缘结合的后处理方法，以便在重叠的核心之间分割。在网络中，我们提出了一种 Context Fusion Block（CF-块），用于有效地提取和融合网络中的Contextual information。我们对方法进行了广泛的量化评估，实验结果表明我们的方法在 BNS、MoNuSeg、CoNSeg 和 CPM-17 数据集上的性能较为出色。
</details></li>
</ul>
<hr>
<h2 id="A-reading-survey-on-adversarial-machine-learning-Adversarial-attacks-and-their-understanding"><a href="#A-reading-survey-on-adversarial-machine-learning-Adversarial-attacks-and-their-understanding" class="headerlink" title="A reading survey on adversarial machine learning: Adversarial attacks and their understanding"></a>A reading survey on adversarial machine learning: Adversarial attacks and their understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03363">http://arxiv.org/abs/2308.03363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shashank Kotyan</li>
<li>for: 本研究旨在系统地探讨针对神经网络的攻击方法和其对神经网络的影响。</li>
<li>methods: 本文使用了现有的攻击方法和对它们的分类，以及对攻击方法的分析和评估。</li>
<li>results: 本文提供了对现有攻击方法的系统性梳理和分析，以及对攻击方法的限制和不足。In English, this means:</li>
<li>for: This research aims to systematically explore the attack methods against neural networks and their impact on them.</li>
<li>methods: This paper uses existing attack methods and classifies them, as well as analyzing and assessing their limitations and drawbacks.</li>
<li>results: This paper provides a systematic overview of existing attack methods, along with their limitations and shortcomings.<details>
<summary>Abstract</summary>
Deep Learning has empowered us to train neural networks for complex data with high performance. However, with the growing research, several vulnerabilities in neural networks have been exposed. A particular branch of research, Adversarial Machine Learning, exploits and understands some of the vulnerabilities that cause the neural networks to misclassify for near original input. A class of algorithms called adversarial attacks is proposed to make the neural networks misclassify for various tasks in different domains. With the extensive and growing research in adversarial attacks, it is crucial to understand the classification of adversarial attacks. This will help us understand the vulnerabilities in a systematic order and help us to mitigate the effects of adversarial attacks. This article provides a survey of existing adversarial attacks and their understanding based on different perspectives. We also provide a brief overview of existing adversarial defences and their limitations in mitigating the effect of adversarial attacks. Further, we conclude with a discussion on the future research directions in the field of adversarial machine learning.
</details>
<details>
<summary>摘要</summary>
深度学习已经赋予我们训练复杂数据的神经网络高性能。然而，随着研究的增长，一些神经网络的漏洞已经被揭露出来。一个特定的研究分支——对抗机器学习——利用和理解神经网络的漏洞，使其在近似原始输入下错分类。一类called adversarial attacks的算法被提议用来使神经网络在不同领域中的多种任务中错分类。随着对抗机器学习的广泛和快速发展，我们需要系统地了解抗击攻击的分类。这将帮助我们系统地了解漏洞，并帮助我们 mitigate the effects of adversarial attacks。本文提供了现有的抗击攻击和其基于不同角度的理解。我们还提供了对抗攻击的简要概述和其限制在 mitigate the effect of adversarial attacks。最后，我们 conclude with a discussion on the future research directions in the field of adversarial machine learning。
</details></li>
</ul>
<hr>
<h2 id="Solving-Falkner-Skan-type-equations-via-Legendre-and-Chebyshev-Neural-Blocks"><a href="#Solving-Falkner-Skan-type-equations-via-Legendre-and-Chebyshev-Neural-Blocks" class="headerlink" title="Solving Falkner-Skan type equations via Legendre and Chebyshev Neural Blocks"></a>Solving Falkner-Skan type equations via Legendre and Chebyshev Neural Blocks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03337">http://arxiv.org/abs/2308.03337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alireza Afzal Aghaei, Kourosh Parand, Ali Nikkhah, Shakila Jaberi</li>
<li>for: 解决非线性法克内尔-斯坦方程</li>
<li>methods: 使用Legendre和Chebyshev神经块，利用orthogonal polynomials在神经网络中提高人工神经网络的近似能力</li>
<li>results: 通过模拟不同的法克内尔-斯坦方程配置，实现提高算法的效率<details>
<summary>Abstract</summary>
In this paper, a new deep-learning architecture for solving the non-linear Falkner-Skan equation is proposed. Using Legendre and Chebyshev neural blocks, this approach shows how orthogonal polynomials can be used in neural networks to increase the approximation capability of artificial neural networks. In addition, utilizing the mathematical properties of these functions, we overcome the computational complexity of the backpropagation algorithm by using the operational matrices of the derivative. The efficiency of the proposed method is carried out by simulating various configurations of the Falkner-Skan equation.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，一种新的深度学习架构用于解决非线性法克纳-斯坦方程被提出。通过使用列朋德和Chebychev神经块，这种方法示出了在人工神经网络中使用正交多项式以提高神经网络的近似能力。此外，利用这些函数的数学性质，我们超越了反向传播算法的计算复杂性，使用操作矩阵的导数。我们对不同的法克纳-斯坦方程配置进行了效率测试。
</details></li>
</ul>
<hr>
<h2 id="Non-Convex-Bilevel-Optimization-with-Time-Varying-Objective-Functions"><a href="#Non-Convex-Bilevel-Optimization-with-Time-Varying-Objective-Functions" class="headerlink" title="Non-Convex Bilevel Optimization with Time-Varying Objective Functions"></a>Non-Convex Bilevel Optimization with Time-Varying Objective Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03811">http://arxiv.org/abs/2308.03811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sen Lin, Daouda Sow, Kaiyi Ji, Yingbin Liang, Ness Shroff</li>
<li>for: 这个研究是针对在线机器学习问题中的两层优化问题（online bilevel optimization，OBO），尤其是在流动数据和时间变化函数下进行优化。</li>
<li>methods: 我们提出了一个单回路线上的网络均值优化器（SOBOW），通过缓存中的窗口均值来更新外层决策。相比于现有的算法，SOBOW更加computationally efficient，并且不需要知道前一Function。</li>
<li>results: 我们显示了SOBOW可以在线机器学习问题中实现低度的两层本地遗憾（bilevel local regret），并且在多个领域进行了广泛的实验，证明了其效果。<details>
<summary>Abstract</summary>
Bilevel optimization has become a powerful tool in a wide variety of machine learning problems. However, the current nonconvex bilevel optimization considers an offline dataset and static functions, which may not work well in emerging online applications with streaming data and time-varying functions. In this work, we study online bilevel optimization (OBO) where the functions can be time-varying and the agent continuously updates the decisions with online streaming data. To deal with the function variations and the unavailability of the true hypergradients in OBO, we propose a single-loop online bilevel optimizer with window averaging (SOBOW), which updates the outer-level decision based on a window average of the most recent hypergradient estimations stored in the memory. Compared to existing algorithms, SOBOW is computationally efficient and does not need to know previous functions. To handle the unique technical difficulties rooted in single-loop update and function variations for OBO, we develop a novel analytical technique that disentangles the complex couplings between decision variables, and carefully controls the hypergradient estimation error. We show that SOBOW can achieve a sublinear bilevel local regret under mild conditions. Extensive experiments across multiple domains corroborate the effectiveness of SOBOW.
</details>
<details>
<summary>摘要</summary>
双层优化已成为许多机器学习问题的重要工具。但是现有的非对称双层优化方法假设数据集和函数是静态的，这可能无法适用于emerging的在线应用程序中，尤其是过去的数据和函数都是时间变化的。在这个研究中，我们研究线上双层优化（OBO），其中函数可以是时间变化的，并且代理人持续更新决策以上线实时数据。对于函数变化和真实对数 gradient 的不可知道问题，我们提出了单轮线上双层优化器（SOBOW），它更新外层决策基于最近的对数gradient 估计中的窗口平均值。相比于现有的算法，SOBOW 具有较低的计算成本和不需要知道前一代函数。对于单轮更新和函数变化带来的困难，我们开发了一种新的分析方法，将决策变量分解为独立的部分，并且精确地控制对数gradient 估计误差。我们显示SOBOW 可以在某些条件下实现双层本地 regret 的下图数学。实验结果显示SOBOW 在多个领域中具有优秀的效能。
</details></li>
</ul>
<hr>
<h2 id="Expediting-Neural-Network-Verification-via-Network-Reduction"><a href="#Expediting-Neural-Network-Verification-via-Network-Reduction" class="headerlink" title="Expediting Neural Network Verification via Network Reduction"></a>Expediting Neural Network Verification via Network Reduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03330">http://arxiv.org/abs/2308.03330</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuyi Zhong, Ruiwei Wang, Siau-Cheng Khoo</li>
<li>for: 验证深度神经网络的安全性 properties</li>
<li>methods: 提议了多种验证方法来验证深度神经网络是否正确工作，但是许多知名的验证工具仍然无法处理复杂的网络架构和大型网络。本文提出了一种网络减少技术作为验证前置处理方法。</li>
<li>results: 我们的实验结果表明，提议的减少技术可以有效地减少神经网络，并使现有的验证工具更快速地处理神经网络。此外，实验结果还显示，网络减少可以提高现有验证工具对许多网络的可用性。<details>
<summary>Abstract</summary>
A wide range of verification methods have been proposed to verify the safety properties of deep neural networks ensuring that the networks function correctly in critical applications. However, many well-known verification tools still struggle with complicated network architectures and large network sizes. In this work, we propose a network reduction technique as a pre-processing method prior to verification. The proposed method reduces neural networks via eliminating stable ReLU neurons, and transforming them into a sequential neural network consisting of ReLU and Affine layers which can be handled by the most verification tools. We instantiate the reduction technique on the state-of-the-art complete and incomplete verification tools, including alpha-beta-crown, VeriNet and PRIMA. Our experiments on a large set of benchmarks indicate that the proposed technique can significantly reduce neural networks and speed up existing verification tools. Furthermore, the experiment results also show that network reduction can improve the availability of existing verification tools on many networks by reducing them into sequential neural networks.
</details>
<details>
<summary>摘要</summary>
各种验证方法已经提议以验证深度神经网络的安全性属性，以确保神经网络在关键应用中正确工作。然而，许多知名的验证工具仍然无法处理复杂的网络架构和大型网络。在这种情况下，我们提议一种网络减少技术作为预处理方法。我们的提议方法通过消除稳定的ReLU神经元并将其转换成一个顺序神经网络，包括ReLU和Affine层，可以由大多数验证工具处理。我们在 alpha-beta-crown、VeriNet 和 PRIMA 等完整和部分验证工具上实现了减少技术，并在一个大量的 benchmark 上进行了实验。实验结果表明，我们的提议方法可以显著减少神经网络，并使现有的验证工具更快速地处理神经网络。此外，实验结果还表明，网络减少可以提高现有验证工具对许多网络的可用性。
</details></li>
</ul>
<hr>
<h2 id="AFN-Adaptive-Fusion-Normalization-via-Encoder-Decoder-Framework"><a href="#AFN-Adaptive-Fusion-Normalization-via-Encoder-Decoder-Framework" class="headerlink" title="AFN: Adaptive Fusion Normalization via Encoder-Decoder Framework"></a>AFN: Adaptive Fusion Normalization via Encoder-Decoder Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03321">http://arxiv.org/abs/2308.03321</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/huanranchen/ASRNorm">https://github.com/huanranchen/ASRNorm</a></li>
<li>paper_authors: Zikai Zhou, Huanran Chen</li>
<li>for: 这篇论文主要是针对深度学习的正常化层进行研究，并提出了一种新的正常化函数 called Adaptive Fusion Normalization (AFN)。</li>
<li>methods: 论文使用了多种正常化函数，包括Batch Normalization (BatchNorm)、Instance Normalization (InstanceNorm) 和 Weight Normalization (WeightNorm)，并评估了这些函数在领域扩大和图像识别 зада务中的表现。</li>
<li>results: 实验结果显示，AFN 在领域扩大和图像识别任务中表现较好，并且超过了先前的正常化技术。<details>
<summary>Abstract</summary>
The success of deep learning is inseparable from normalization layers. Researchers have proposed various normalization functions, and each of them has both advantages and disadvantages. In response, efforts have been made to design a unified normalization function that combines all normalization procedures and mitigates their weaknesses. We also proposed a new normalization function called Adaptive Fusion Normalization. Through experiments, we demonstrate AFN outperforms the previous normalization techniques in domain generalization and image classification tasks.
</details>
<details>
<summary>摘要</summary>
成功的深度学习与归一化层无可分割。研究人员已经提出了多种归一化函数，每种归一化函数各有优点和缺点。为了设计一个综合归一化函数，并且 Mitigate their weaknesses。我们还提出了一种新的归一化函数called Adaptive Fusion Normalization（AFN）。通过实验，我们证明AFN在领域总体化和图像识别任务中超过了之前的归一化技术。
</details></li>
</ul>
<hr>
<h2 id="Binary-Federated-Learning-with-Client-Level-Differential-Privacy"><a href="#Binary-Federated-Learning-with-Client-Level-Differential-Privacy" class="headerlink" title="Binary Federated Learning with Client-Level Differential Privacy"></a>Binary Federated Learning with Client-Level Differential Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03320">http://arxiv.org/abs/2308.03320</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lumin Liu, Jun Zhang, Shenghui Song, Khaled B. Letaief</li>
<li>for: 提高 federated learning（FL）系统的隐私保护和性能。</li>
<li>methods: 采用 binary neural networks（BNNs）和离散噪声来实现客户端级别的隐私保护，并且引入离散噪声以实现隐私保护。</li>
<li>results: 实验结果基于 MNIST 和 Fashion-MNIST 数据集表明，提议的训练算法可以实现客户端级别的隐私保护，同时具有低通信开销和性能提升。<details>
<summary>Abstract</summary>
Federated learning (FL) is a privacy-preserving collaborative learning framework, and differential privacy can be applied to further enhance its privacy protection. Existing FL systems typically adopt Federated Average (FedAvg) as the training algorithm and implement differential privacy with a Gaussian mechanism. However, the inherent privacy-utility trade-off in these systems severely degrades the training performance if a tight privacy budget is enforced. Besides, the Gaussian mechanism requires model weights to be of high-precision. To improve communication efficiency and achieve a better privacy-utility trade-off, we propose a communication-efficient FL training algorithm with differential privacy guarantee. Specifically, we propose to adopt binary neural networks (BNNs) and introduce discrete noise in the FL setting. Binary model parameters are uploaded for higher communication efficiency and discrete noise is added to achieve the client-level differential privacy protection. The achieved performance guarantee is rigorously proved, and it is shown to depend on the level of discrete noise. Experimental results based on MNIST and Fashion-MNIST datasets will demonstrate that the proposed training algorithm achieves client-level privacy protection with performance gain while enjoying the benefits of low communication overhead from binary model updates.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种隐私保护的协同学习框架，可以进一步增强其隐私保护。现有的 FL 系统通常采用 Federated Average (FedAvg) 作为训练算法，并在这之前实现了分布式隐私保护。然而，这些系统中的隐私Utility 质量耗尽会严重降低训练性能，而且 Gaussian 机制需要模型参数的高精度。为了提高通信效率和实现更好的隐私Utility 质量，我们提议一种通信效率高的 FL 训练算法，并提供了隐私保证。 Specifically, we propose to adopt binary neural networks (BNNs) and introduce discrete noise in the FL setting. Binary model parameters are uploaded for higher communication efficiency and discrete noise is added to achieve the client-level differential privacy protection. The achieved performance guarantee is rigorously proved, and it is shown to depend on the level of discrete noise. Experimental results based on MNIST and Fashion-MNIST datasets will demonstrate that the proposed training algorithm achieves client-level privacy protection with performance gain while enjoying the benefits of low communication overhead from binary model updates.Note: The translation is done using Google Translate and may not be perfect.
</details></li>
</ul>
<hr>
<h2 id="HomOpt-A-Homotopy-Based-Hyperparameter-Optimization-Method"><a href="#HomOpt-A-Homotopy-Based-Hyperparameter-Optimization-Method" class="headerlink" title="HomOpt: A Homotopy-Based Hyperparameter Optimization Method"></a>HomOpt: A Homotopy-Based Hyperparameter Optimization Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03317">http://arxiv.org/abs/2308.03317</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jeffkinnison/shadho">https://github.com/jeffkinnison/shadho</a></li>
<li>paper_authors: Sophia J. Abraham, Kehelwala D. G. Maduranga, Jeffery Kinnison, Zachariah Carmichael, Jonathan D. Hauenstein, Walter J. Scheirer</li>
<li>for: 提高机器学习模型的性能和效率，通过优化超参数。</li>
<li>methods: 使用一种基于泛函模型（GAM）的数据驱动方法，与哈洛彩优化（Homotopy Optimization）相结合，以提高优化方法的效率和精度。</li>
<li>results: 在多种优化技术（如随机搜索、TPE、权重平衡和SMAC）中，使用HomOpt方法可以提高目标性能，并在多个标准机器学习 benchmark 和开放集任务中达到更高的性能。<details>
<summary>Abstract</summary>
Machine learning has achieved remarkable success over the past couple of decades, often attributed to a combination of algorithmic innovations and the availability of high-quality data available at scale. However, a third critical component is the fine-tuning of hyperparameters, which plays a pivotal role in achieving optimal model performance. Despite its significance, hyperparameter optimization (HPO) remains a challenging task for several reasons. Many HPO techniques rely on naive search methods or assume that the loss function is smooth and continuous, which may not always be the case. Traditional methods, like grid search and Bayesian optimization, often struggle to quickly adapt and efficiently search the loss landscape. Grid search is computationally expensive, while Bayesian optimization can be slow to prime. Since the search space for HPO is frequently high-dimensional and non-convex, it is often challenging to efficiently find a global minimum. Moreover, optimal hyperparameters can be sensitive to the specific dataset or task, further complicating the search process. To address these issues, we propose a new hyperparameter optimization method, HomOpt, using a data-driven approach based on a generalized additive model (GAM) surrogate combined with homotopy optimization. This strategy augments established optimization methodologies to boost the performance and effectiveness of any given method with faster convergence to the optimum on continuous, discrete, and categorical domain spaces. We compare the effectiveness of HomOpt applied to multiple optimization techniques (e.g., Random Search, TPE, Bayes, and SMAC) showing improved objective performance on many standardized machine learning benchmarks and challenging open-set recognition tasks.
</details>
<details>
<summary>摘要</summary>
为解决这些问题，我们提出了一种新的HPO方法，即HomOpt，使用基于一般添加模型（GAM）函数的数据驱动方法，并与幂等优化结合。这种策略可以增强现有优化方法的性能和有效性，并在连续、离散和分类的域空间上快速 converges to the optimum。我们将HomOpt应用于多种优化技术（如随机搜索、TPE、Bayes和SMAC），并对多个标准机器学习benchmark和复杂的开放任务进行比较，显示HomOpt可以提高优化目标性能。
</details></li>
</ul>
<hr>
<h2 id="Deep-Q-Network-for-Stochastic-Process-Environments"><a href="#Deep-Q-Network-for-Stochastic-Process-Environments" class="headerlink" title="Deep Q-Network for Stochastic Process Environments"></a>Deep Q-Network for Stochastic Process Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03316">http://arxiv.org/abs/2308.03316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kuangheng He</li>
<li>for: 本研究旨在应用强化学习方法在Stochastic Process环境中训练优化策略，包括Flappy Bird和新开发的股票交易环境作为案例研究。</li>
<li>methods: 本研究使用Deep Q-learning网络，评估不同结构的网络，并评估适合Stochastic Process环境的最佳variant。</li>
<li>results: 本研究获得了各种策略的成果，包括Flappy Bird和股票交易环境中的策略。同时，本研究还讨论了环境建立和强化学习技术的现有挑战和可能的改进方案。<details>
<summary>Abstract</summary>
Reinforcement learning is a powerful approach for training an optimal policy to solve complex problems in a given system. This project aims to demonstrate the application of reinforcement learning in stochastic process environments with missing information, using Flappy Bird and a newly developed stock trading environment as case studies. We evaluate various structures of Deep Q-learning networks and identify the most suitable variant for the stochastic process environment. Additionally, we discuss the current challenges and propose potential improvements for further work in environment-building and reinforcement learning techniques.
</details>
<details>
<summary>摘要</summary>
<<SYS>>通过补偿学习训练优化策略来解决复杂系统中的问题，这个项目旨在通过缺失信息的随机过程环境中应用强化学习，使用Flappy Bird和一个新开发的股票交易环境作为案例研究。我们评估了不同结构的深度Q学习网络，并确定适用于随机过程环境的最佳变体。此外，我们还讨论了当前挑战和可能的改进方法，用于环境建构和强化学习技术。Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide that instead.
</details></li>
</ul>
<hr>
<h2 id="Symmetry-Preserving-Program-Representations-for-Learning-Code-Semantics"><a href="#Symmetry-Preserving-Program-Representations-for-Learning-Code-Semantics" class="headerlink" title="Symmetry-Preserving Program Representations for Learning Code Semantics"></a>Symmetry-Preserving Program Representations for Learning Code Semantics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03312">http://arxiv.org/abs/2308.03312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kexin Pei, Weichen Li, Qirui Jin, Shuyang Liu, Scott Geng, Lorenzo Cavallaro, Junfeng Yang, Suman Jana</li>
<li>for: 该论文旨在提高自动程序理解的能力，以便进行安全任务。</li>
<li>methods: 该论文使用了各种自然语言处理技术，以满足不同的代码分析和模型任务。</li>
<li>results: 该论文通过引入代码Symmetry的概念，提高了LLM架构的泛化和稳定性，并在不同的 binary 和源代码分析任务中进行了详细的实验评估。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have shown promise in automated program reasoning, a crucial aspect of many security tasks. However, existing LLM architectures for code are often borrowed from other domains like natural language processing, raising concerns about their generalization and robustness to unseen code. A key generalization challenge is to incorporate the knowledge of code semantics, including control and data flow, into the LLM architectures.   Drawing inspiration from examples of convolution layers exploiting translation symmetry, we explore how code symmetries can enhance LLM architectures for program analysis and modeling. We present a rigorous group-theoretic framework that formally defines code symmetries as semantics-preserving transformations and provides techniques for precisely reasoning about symmetry preservation within LLM architectures. Using this framework, we introduce a novel variant of self-attention that preserves program symmetries, demonstrating its effectiveness in generalization and robustness through detailed experimental evaluations across different binary and source code analysis tasks. Overall, our code symmetry framework offers rigorous and powerful reasoning techniques that can guide the future development of specialized LLMs for code and advance LLM-guided program reasoning tasks.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在自动编程逻辑方面表现出了替代性，这是许多安全任务的关键方面。然而，现有的 LLM 架构 для代码往往从其他领域如自然语言处理中借鉴，这引起了代码泛化和稳定性的问题。一个关键的泛化挑战是如何在 LLM 架构中 интеグ含代码 semantics，包括控制和数据流的知识。 drew inspiration from examples of convolution layers exploiting translation symmetry, we explore how code symmetries can enhance LLM architectures for program analysis and modeling. We present a rigorous group-theoretic framework that formally defines code symmetries as semantics-preserving transformations and provides techniques for precisely reasoning about symmetry preservation within LLM architectures. Using this framework, we introduce a novel variant of self-attention that preserves program symmetries, demonstrating its effectiveness in generalization and robustness through detailed experimental evaluations across different binary and source code analysis tasks. Overall, our code symmetry framework offers rigorous and powerful reasoning techniques that can guide the future development of specialized LLMs for code and advance LLM-guided program reasoning tasks.Translation notes:* "Large Language Models" is translated as "大型语言模型" (dàxí yǔyán módelìng)* "automated program reasoning" is translated as "自动编程逻辑" (zìdōng biānjiāng lógí)* "code" is translated as "代码" (dàikōng)* "semantics" is translated as " semantics" (xìngxìng)* "preserving transformations" is translated as "保持变换" (bǎojì biànbiàn)* "group-theoretic framework" is translated as "群论框架" (qúnlù jiàoxiàng)* "novel variant of self-attention" is translated as "一种新型的自注意力" (yī zhǒng xīn xìng de zìjiāngwù)* "detailed experimental evaluations" is translated as "详细实验评估" (xìngxìng shíyì píngjì)* "specialized LLMs for code" is translated as "专门为代码的 LLM" (zhōngmén wèi dàikōng de LLM)* "LLM-guided program reasoning" is translated as " LLM 导航的程序逻辑" (LLM dàowǎng de chéngjīng lógí)
</details></li>
</ul>
<hr>
<h2 id="Implicit-Graph-Neural-Diffusion-Based-on-Constrained-Dirichlet-Energy-Minimization"><a href="#Implicit-Graph-Neural-Diffusion-Based-on-Constrained-Dirichlet-Energy-Minimization" class="headerlink" title="Implicit Graph Neural Diffusion Based on Constrained Dirichlet Energy Minimization"></a>Implicit Graph Neural Diffusion Based on Constrained Dirichlet Energy Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03306">http://arxiv.org/abs/2308.03306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guoji Fu, Mohammed Haroon Dupty, Yanfei Dong, Lee Wee Sun</li>
<li>for: The paper is written for researchers and practitioners working on graph learning problems, particularly those interested in using implicit graph neural networks (GNNs) to capture long-range dependencies.</li>
<li>methods: The paper introduces a geometric framework for designing implicit GNN layers based on a parameterized graph Laplacian operator. This framework allows for learning the geometry of vertex and edge spaces, as well as the graph gradient operator from data.</li>
<li>results: The paper demonstrates better performance than leading implicit and explicit GNNs on benchmark datasets for node and graph classification tasks, with substantial accuracy improvements observed for some datasets. The proposed method is able to trade off smoothing with the preservation of node feature information, addressing the over-smoothing problem that can occur in some GNN models.<details>
<summary>Abstract</summary>
Implicit graph neural networks (GNNs) have emerged as a potential approach to enable GNNs to capture long-range dependencies effectively. However, poorly designed implicit GNN layers can experience over-smoothing or may have limited adaptability to learn data geometry, potentially hindering their performance in graph learning problems. To address these issues, we introduce a geometric framework to design implicit graph diffusion layers based on a parameterized graph Laplacian operator. Our framework allows learning the geometry of vertex and edge spaces, as well as the graph gradient operator from data. We further show how implicit GNN layers can be viewed as the fixed-point solution of a Dirichlet energy minimization problem and give conditions under which it may suffer from over-smoothing. To overcome the over-smoothing problem, we design our implicit graph diffusion layer as the solution of a Dirichlet energy minimization problem with constraints on vertex features, enabling it to trade off smoothing with the preservation of node feature information. With an appropriate hyperparameter set to be larger than the largest eigenvalue of the parameterized graph Laplacian, our framework guarantees a unique equilibrium and quick convergence. Our models demonstrate better performance than leading implicit and explicit GNNs on benchmark datasets for node and graph classification tasks, with substantial accuracy improvements observed for some datasets.
</details>
<details>
<summary>摘要</summary>
匿名图 neural networks (GNNs) 已成为可能的方法来有效地捕捉长距离依赖关系。然而，不当设计的匿名 GNN 层可能会经受过滤或有限的适应性，从而妨碍其在图学习问题中表现。为解决这些问题，我们提出了一个几何框架，用于设计基于参数化图 Laplacian 算子的匿名图扩散层。我们的框架允许学习图像的几何结构和图形导数算子，以及适应数据的几何特征。我们进一步表明，匿名 GNN 层可以视为 Dirichlet 能量最小化问题的固定点解，并给出了一些条件，以确定它可能会经受过滤。为了缓解过滤问题，我们设计了一种基于 vertex 特征的约束来限制匿名图扩散层的平滑程度，从而让它能够平衡平滑和保留节点特征信息。在适当的超参数设置为大于最大 eigenvalues 的情况下，我们的框架保证唯一的平衡点和快速的收敛。我们的模型在 benchmark 数据集上表现出色，与领先的匿名和显式 GNN 相比，有substantial 的准确率改进，特别是在某些数据集上。
</details></li>
</ul>
<hr>
<h2 id="Do-You-Remember-Overcoming-Catastrophic-Forgetting-for-Fake-Audio-Detection"><a href="#Do-You-Remember-Overcoming-Catastrophic-Forgetting-for-Fake-Audio-Detection" class="headerlink" title="Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection"></a>Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03300">http://arxiv.org/abs/2308.03300</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cecile-hi/regularized-adaptive-weight-modification">https://github.com/cecile-hi/regularized-adaptive-weight-modification</a></li>
<li>paper_authors: Xiaohui Zhang, Jiangyan Yi, Jianhua Tao, Chenglong Wang, Chuyuan Zhang</li>
<li>For: 这个研究旨在开发一个可以适应不同数据集的伪音乐检测算法，以解决对伪音乐检测算法的普遍忘却问题。* Methods: 我们提出了一个叫做Regularized Adaptive Weight Modification（RAWM）的持续学习算法，它可以适应不同数据集的伪音乐检测任务，并且可以保持旧模型的知识。我们还引入了一个规制因子，以强制网络保持旧的特征分布。* Results: 我们在多个数据集上进行了评估，结果显示我们的方法可以对伪音乐检测任务进行有效的适应和忘却避免。<details>
<summary>Abstract</summary>
Current fake audio detection algorithms have achieved promising performances on most datasets. However, their performance may be significantly degraded when dealing with audio of a different dataset. The orthogonal weight modification to overcome catastrophic forgetting does not consider the similarity of genuine audio across different datasets. To overcome this limitation, we propose a continual learning algorithm for fake audio detection to overcome catastrophic forgetting, called Regularized Adaptive Weight Modification (RAWM). When fine-tuning a detection network, our approach adaptively computes the direction of weight modification according to the ratio of genuine utterances and fake utterances. The adaptive modification direction ensures the network can effectively detect fake audio on the new dataset while preserving its knowledge of old model, thus mitigating catastrophic forgetting. In addition, genuine audio collected from quite different acoustic conditions may skew their feature distribution, so we introduce a regularization constraint to force the network to remember the old distribution in this regard. Our method can easily be generalized to related fields, like speech emotion recognition. We also evaluate our approach across multiple datasets and obtain a significant performance improvement on cross-dataset experiments.
</details>
<details>
<summary>摘要</summary>
当前的假音检测算法已经在大多数数据集上实现了有望的性能。然而，它们在处理不同数据集的声音时可能会表现出很大的下降性能。现有的orthogonal weight modification方法不考虑真实声音数据集之间的相似性。为了解决这个限制，我们提出了一种适应学习的假音检测算法，called Regularized Adaptive Weight Modification (RAWM)。当训练检测网络时，我们的方法会动态计算修改方向，根据新数据集中真实的utterances和假的utterances的比率。这种适应修改方向使得网络可以有效地检测新数据集上的假音，同时保持以前的知识，从而避免悬崖式遗忘。此外，来自不同的听音条件下的真实声音收集可能会扭曲其特征分布，所以我们引入了一种正则化约束，使得网络可以保持以前的分布。我们的方法可以轻松扩展到相关的领域，如语音情感识别。我们还在多个数据集上评估了我们的方法，并在交叉数据集测试中获得了显著的性能提升。
</details></li>
</ul>
<hr>
<h2 id="Studying-Large-Language-Model-Generalization-with-Influence-Functions"><a href="#Studying-Large-Language-Model-Generalization-with-Influence-Functions" class="headerlink" title="Studying Large Language Model Generalization with Influence Functions"></a>Studying Large Language Model Generalization with Influence Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03296">http://arxiv.org/abs/2308.03296</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, Evan Hubinger, Kamilė Lukošiūtė, Karina Nguyen, Nicholas Joseph, Sam McCandlish, Jared Kaplan, Samuel R. Bowman</li>
<li>for: 了解大型机器学习模型（LLMs）的特性和风险，以便更好地理解和 mitigate 这些风险。</li>
<li>methods: 使用Influence functions来回答一个Counterfactual：如果一个序列被添加到训练集中， THEN how would the model’s parameters（和其输出）变化？通过使用Eigenvalue-corrected Kronecker-Factored Approximate Curvature（EK-FAC）近似来扩展Influence functions到LLMs中，并使用TF-IDF筛选和查询批处理来降低计算Gradient的成本。</li>
<li>results: 通过Influence functions来研究LLMs的通用特性，包括影响 patrerns的稀畴性、逻辑推理能力、数学能力、跨语言通用性和role-playing行为等。尽管有很多似乎复杂的通用形式，但我们发现一个意外的限制：影响幅 decay 到 Near-zero 当键短语的顺序被反转。总的来说，Influence functions 为研究 LLMs 提供了一种强大的新工具。<details>
<summary>Abstract</summary>
When trying to gain better visibility into a machine learning model in order to understand and mitigate the associated risks, a potentially valuable source of evidence is: which training examples most contribute to a given behavior? Influence functions aim to answer a counterfactual: how would the model's parameters (and hence its outputs) change if a given sequence were added to the training set? While influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP). We use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation to scale influence functions up to LLMs with up to 52 billion parameters. In our experiments, EK-FAC achieves similar accuracy to traditional influence function estimators despite the IHVP computation being orders of magnitude faster. We investigate two algorithmic techniques to reduce the cost of computing gradients of candidate training sequences: TF-IDF filtering and query batching. We use influence functions to investigate the generalization patterns of LLMs, including the sparsity of the influence patterns, increasing abstraction with scale, math and programming abilities, cross-lingual generalization, and role-playing behavior. Despite many apparently sophisticated forms of generalization, we identify a surprising limitation: influences decay to near-zero when the order of key phrases is flipped. Overall, influence functions give us a powerful new tool for studying the generalization properties of LLMs.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)当尝试更好地了解机器学习模型以理解和降低相关风险时，一个有价值的证据来源是：哪些训练示例最大程度地影响模型的行为？影响函数想要回答一个Counterfactual：如果给定的序列添加到训练集中， Then how would the model's parameters (和其输出) change? Although influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP). We use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation to scale influence functions up to LLMs with up to 52 billion parameters. In our experiments, EK-FAC achieves similar accuracy to traditional influence function estimators despite the IHVP computation being orders of magnitude faster. We investigate two algorithmic techniques to reduce the cost of computing gradients of candidate training sequences: TF-IDF filtering and query batching. We use influence functions to investigate the generalization patterns of LLMs, including the sparsity of the influence patterns, increasing abstraction with scale, math and programming abilities, cross-lingual generalization, and role-playing behavior. Despite many apparently sophisticated forms of generalization, we identify a surprising limitation: influences decay to near-zero when the order of key phrases is flipped. Overall, influence functions give us a powerful new tool for studying the generalization properties of LLMs.
</details></li>
</ul>
<hr>
<h2 id="DOMINO-Domain-invariant-Hyperdimensional-Classification-for-Multi-Sensor-Time-Series-Data"><a href="#DOMINO-Domain-invariant-Hyperdimensional-Classification-for-Multi-Sensor-Time-Series-Data" class="headerlink" title="DOMINO: Domain-invariant Hyperdimensional Classification for Multi-Sensor Time Series Data"></a>DOMINO: Domain-invariant Hyperdimensional Classification for Multi-Sensor Time Series Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03295">http://arxiv.org/abs/2308.03295</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyao Wang, Luke Chen, Mohammad Abdullah Al Faruque</li>
<li>for: 这篇论文主要关注于如何 Addressing the distribution shift problem in noisy multi-sensor time-series data using brain-inspired hyperdimensional computing (HDC) learning frameworks.</li>
<li>methods: 这篇论文提出了一个名为 DOMINO 的新的 HDC 学习框架，利用高维度空间的有效并行矩阵运算来动态地识别和范围化领域对称的维度。</li>
<li>results: 在训练多种多感器时间序列标签任务中，DOMINO 比起现有的顶尖深度神经网络（DNN）域对应测试技术，平均提高了2.04%的精度，并在训练和测试过程中提供了16.34倍 faster 和2.89倍 faster 的运算速度。此外，DOMINO 在受到硬件噪声的情况下表现更好，提供了10.93倍更高的韧性。<details>
<summary>Abstract</summary>
With the rapid evolution of the Internet of Things, many real-world applications utilize heterogeneously connected sensors to capture time-series information. Edge-based machine learning (ML) methodologies are often employed to analyze locally collected data. However, a fundamental issue across data-driven ML approaches is distribution shift. It occurs when a model is deployed on a data distribution different from what it was trained on, and can substantially degrade model performance. Additionally, increasingly sophisticated deep neural networks (DNNs) have been proposed to capture spatial and temporal dependencies in multi-sensor time series data, requiring intensive computational resources beyond the capacity of today's edge devices. While brain-inspired hyperdimensional computing (HDC) has been introduced as a lightweight solution for edge-based learning, existing HDCs are also vulnerable to the distribution shift challenge. In this paper, we propose DOMINO, a novel HDC learning framework addressing the distribution shift problem in noisy multi-sensor time-series data. DOMINO leverages efficient and parallel matrix operations on high-dimensional space to dynamically identify and filter out domain-variant dimensions. Our evaluation on a wide range of multi-sensor time series classification tasks shows that DOMINO achieves on average 2.04% higher accuracy than state-of-the-art (SOTA) DNN-based domain generalization techniques, and delivers 16.34x faster training and 2.89x faster inference. More importantly, DOMINO performs notably better when learning from partially labeled and highly imbalanced data, providing 10.93x higher robustness against hardware noises than SOTA DNNs.
</details>
<details>
<summary>摘要</summary>
随着互联网物联网的快速演化，许多实际应用场景使用不同类型的感知器来收集时序信息。Edge-based机器学习（ML）方法ологи是常用来当地分析收集的数据。然而，数据驱动的ML方法面临的基本问题是分布shift问题。这种问题会导致模型在不同于它在训练时的数据分布上部署时表现差。此外，逐渐复杂的深度神经网络（DNN）已经被提出来捕捉多感知器时序数据中的空间和时间相关性，需要今天的边缘设备来进行投入式计算资源。而基于脑神经网络的高维计算（HDC）已经被提出来作为边缘学习的轻量级解决方案。然而，现有的HDC也面临着分布shift挑战。在这篇论文中，我们提出了DOMINO，一种新的HDC学习框架，解决了边缘设备上的分布shift问题。DOMINO利用了高维空间中效率和并行的矩阵运算来动态标识和筛选域variant维度。我们对多种多感知器时序分类任务进行了广泛的评估，结果显示DOMINO相比状态скус（SOTA）神经网络基于领域普适化技术，平均提高了2.04%的准确率，并提供了16.34x快的训练和2.89x快的推理。此外，DOMINO在学习部分 labels和高度不均衡数据时表现更出色，提供了10.93x更高的硬件噪声Robustness。
</details></li>
</ul>
<hr>
<h2 id="SynJax-Structured-Probability-Distributions-for-JAX"><a href="#SynJax-Structured-Probability-Distributions-for-JAX" class="headerlink" title="SynJax: Structured Probability Distributions for JAX"></a>SynJax: Structured Probability Distributions for JAX</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03291">http://arxiv.org/abs/2308.03291</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deepmind/synjax">https://github.com/deepmind/synjax</a></li>
<li>paper_authors: Miloš Stanojević, Laurent Sartran</li>
<li>for: 这篇论文是为了解决深度学习模型中的结构化对象问题而写的，以便建立大规模可导的模型。</li>
<li>methods: 这篇论文使用了SynJax库，提供了高效的向量化实现归并算法，以便处理结构化分布的推理问题。</li>
<li>results: 该论文通过SynJax库实现了大规模可导模型，并且可以处理各种结构化对象，如树和分割。<details>
<summary>Abstract</summary>
The development of deep learning software libraries enabled significant progress in the field by allowing users to focus on modeling, while letting the library to take care of the tedious and time-consuming task of optimizing execution for modern hardware accelerators. However, this has benefited only particular types of deep learning models, such as Transformers, whose primitives map easily to the vectorized computation. The models that explicitly account for structured objects, such as trees and segmentations, did not benefit equally because they require custom algorithms that are difficult to implement in a vectorized form.   SynJax directly addresses this problem by providing an efficient vectorized implementation of inference algorithms for structured distributions covering alignment, tagging, segmentation, constituency trees and spanning trees. With SynJax we can build large-scale differentiable models that explicitly model structure in the data. The code is available at https://github.com/deepmind/synjax.
</details>
<details>
<summary>摘要</summary>
随着深度学习软件库的发展，场景中的进步很大，允许用户专注于模型设计，让库处理现代硬件加速器的繁琐和耗时任务。然而，这些进步主要帮助了某些特定的深度学习模型，如转换器，其元素对vector化计算很容易映射。其他类型的模型，如树和分割，因为它们需要特殊的算法，难以在vector化形式下实现。SynJax直接解决了这个问题，提供了高效的vector化推理算法 для结构化分布，包括对适配、标记、分割、 константы树和扩展树。通过SynJax，我们可以构建大规模可导模型，直接模型数据中的结构。代码可以在https://github.com/deepmind/synjax上获取。
</details></li>
</ul>
<hr>
<h2 id="FLIQS-One-Shot-Mixed-Precision-Floating-Point-and-Integer-Quantization-Search"><a href="#FLIQS-One-Shot-Mixed-Precision-Floating-Point-and-Integer-Quantization-Search" class="headerlink" title="FLIQS: One-Shot Mixed-Precision Floating-Point and Integer Quantization Search"></a>FLIQS: One-Shot Mixed-Precision Floating-Point and Integer Quantization Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03290">http://arxiv.org/abs/2308.03290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jordan Dotzel, Gang Wu, Andrew Li, Muhammad Umar, Yun Ni, Mohamed S. Abdelfattah, Zhiru Zhang, Liqun Cheng, Martin G. Dixon, Norman P. Jouppi, Quoc V. Le, Sheng Li</li>
<li>for: 这个论文主要目的是提出一种一击 mixed-precision quantization search（FLIQS），以找到最佳的混合精度数据模型，并且可以实现高品质和低成本的模型。</li>
<li>methods: 这个方法使用了一种新的一击 mixed-precision quantization search（FLIQS），可以在数据模型中找到最佳的混合精度数据模型，并且不需要重新训练。</li>
<li>results: 这个方法可以对多个 convolutional networks 和 vision transformer 模型进行搜索，并且可以实现高品质和低成本的模型。在 ImageNet 上，这个方法可以将 ResNet-18 的精度提高到 1.31% 点，ResNet-50 的精度提高到 0.90% 点，并且在 MobileNetV2 上可以实现最佳的模型。<details>
<summary>Abstract</summary>
Quantization has become a mainstream compression technique for reducing model size, computational requirements, and energy consumption for modern deep neural networks (DNNs). With the improved numerical support in recent hardware, including multiple variants of integer and floating point, mixed-precision quantization has become necessary to achieve high-quality results with low model cost. Prior mixed-precision quantization methods have performed a post-training quantization search, which compromises on accuracy, or a differentiable quantization search, which leads to high memory usage from branching. Therefore, we propose the first one-shot mixed-precision quantization search that eliminates the need for retraining in both integer and low-precision floating point models. We evaluate our floating-point and integer quantization search (FLIQS) on multiple convolutional networks and vision transformer models to discover Pareto-optimal models. Our approach discovers models that improve upon uniform precision, manual mixed-precision, and recent integer quantization search methods. With the proposed integer quantization search, we increase the accuracy of ResNet-18 on ImageNet by 1.31% points and ResNet-50 by 0.90% points with equivalent model cost over previous methods. Additionally, for the first time, we explore a novel mixed-precision floating-point search and improve MobileNetV2 by up to 0.98% points compared to prior state-of-the-art FP8 models. Finally, we extend FLIQS to simultaneously search a joint quantization and neural architecture space and improve the ImageNet accuracy by 2.69% points with similar model cost on a MobileNetV2 search space.
</details>
<details>
<summary>摘要</summary>
启用量化技术已成为现代深度神经网络（DNN）减少模型大小、计算需求和能耗的主流压缩方法。随着现有硬件的数值支持的改进，包括整数和浮点数多种变体，混合精度量化技术已成为实现高质量结果的低模型成本的必要手段。先前的混合精度量化方法通常是在训练后进行量化搜索，这会妥协准确性，或者使用可导量化搜索，这会导致高内存使用率由分支引起。因此，我们提出了首次一步混合精度量化搜索，从而消除了重训练的需要，并在整数和低精度浮点数模型中实现高度优化。我们对多个卷积神经网络和视Transformer模型进行了评估，并发现了Pareto优质模型。我们的浮点和整数量化搜索（FLIQS）在 uniform精度、手动混合精度和最近整数量化搜索方法上提高了模型的准确性。例如，在 ImageNet 上，我们通过使用整数量化搜索，提高了 ResNet-18 的准确性by 1.31% 点和 ResNet-50 的准确性by 0.90% 点，与之前的方法相当。此外，我们首次探索了一种新的混合精度浮点数搜索，并在 MobileNetV2 上提高了最高达 0.98% 点的准确性。最后，我们将 FLIQS 扩展到同时搜索一个量化和神经网络架构空间，并在 MobileNetV2 搜索空间上提高了 ImageNet 的准确性by 2.69% 点，与相同的模型成本。
</details></li>
</ul>
<hr>
<h2 id="High-rate-discretely-modulated-continuous-variable-quantum-key-distribution-using-quantum-machine-learning"><a href="#High-rate-discretely-modulated-continuous-variable-quantum-key-distribution-using-quantum-machine-learning" class="headerlink" title="High-rate discretely-modulated continuous-variable quantum key distribution using quantum machine learning"></a>High-rate discretely-modulated continuous-variable quantum key distribution using quantum machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03283">http://arxiv.org/abs/2308.03283</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qin Liao, Jieyu Liu, Anqi Huang, Lei Huang, Zhuoying Fei, Xiquan Fu</li>
<li>for: 提出了一种高速率的分割CVQKD系统使用量子机器学习技术，以提高CVQKD系统的安全性和效率。</li>
<li>methods: 使用量子k-最近邻NN分类器来预测Bob方向的失去混合干扰DMCS，并对输出的密钥进行数据处理以生成最终的秘密密钥串。</li>
<li>results: 提出的方案可以在机器学习metric和复杂度方面提高CVQKD系统的性能，并通过使用SDP方法证明其理论安全性。数值仿真结果表明，相比现有的DM CVQKD协议，提出的方案可以获得更高的秘密密钥率，并可以通过增加混合干扰的变化来进一步提高性能。<details>
<summary>Abstract</summary>
We propose a high-rate scheme for discretely-modulated continuous-variable quantum key distribution (DM CVQKD) using quantum machine learning technologies, which divides the whole CVQKD system into three parts, i.e., the initialization part that is used for training and estimating quantum classifier, the prediction part that is used for generating highly correlated raw keys, and the data-postprocessing part that generates the final secret key string shared by Alice and Bob. To this end, a low-complexity quantum k-nearest neighbor (QkNN) classifier is designed for predicting the lossy discretely-modulated coherent states (DMCSs) at Bob's side. The performance of the proposed QkNN-based CVQKD especially in terms of machine learning metrics and complexity is analyzed, and its theoretical security is proved by using semi-definite program (SDP) method. Numerical simulation shows that the secret key rate of our proposed scheme is explicitly superior to the existing DM CVQKD protocols, and it can be further enhanced with the increase of modulation variance.
</details>
<details>
<summary>摘要</summary>
Here is the translation in Simplified Chinese:我们提出了一种高率方案 для抽象变量量kv quantum key distribution（DM CVQKD），使用量子机器学习技术，将整个CVQKD系统分成三部分：初始化部分用于训练和估算量子分类器，预测部分用于生成高相关性的Raw键，以及数据处理部分用于生成最终由阿利斯和布Bob共享的机密键串。为此，我们设计了一种低复杂度量子k最近邻居分类器（QkNN）来预测 бо布的lossy抽象变量量CS。我们分析了我们提议的QkNN-based CVQKD的性能，包括机器学习指标和复杂度，并使用 semi-definite 程序（SDP）方法证明其理论安全性。numerical simulation表明，我们的提议方案的机密键率明显高于现有的DM CVQKD协议，并可以通过增加调制变量的幅度来进一步提高。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Distilled-Ensemble-Model-for-sEMG-based-Silent-Speech-Interface"><a href="#Knowledge-Distilled-Ensemble-Model-for-sEMG-based-Silent-Speech-Interface" class="headerlink" title="Knowledge Distilled Ensemble Model for sEMG-based Silent Speech Interface"></a>Knowledge Distilled Ensemble Model for sEMG-based Silent Speech Interface</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06533">http://arxiv.org/abs/2308.06533</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenqiang Lai, Qihan Yang, Ye Mao, Endong Sun, Jiangnan Ye</li>
<li>for: 这研究旨在提供一种基于表面电Myography（sEMG）的干扰 Speech Interface（SSI），以解决全球数百万人口报告有声音问题。</li>
<li>methods: 我们提出了一种轻量级的深度学习知识压缩混合模型（KDE-SSI），使得可以通过拼写生成任何英语单词。</li>
<li>results: 我们的实验证明了KDE-SSI的效果，实现了26个 NATO fonetic alphabet dataset中的3900个数据样本的测试准确率为85.9%。<details>
<summary>Abstract</summary>
Voice disorders affect millions of people worldwide. Surface electromyography-based Silent Speech Interfaces (sEMG-based SSIs) have been explored as a potential solution for decades. However, previous works were limited by small vocabularies and manually extracted features from raw data. To address these limitations, we propose a lightweight deep learning knowledge-distilled ensemble model for sEMG-based SSI (KDE-SSI). Our model can classify a 26 NATO phonetic alphabets dataset with 3900 data samples, enabling the unambiguous generation of any English word through spelling. Extensive experiments validate the effectiveness of KDE-SSI, achieving a test accuracy of 85.9\%. Our findings also shed light on an end-to-end system for portable, practical equipment.
</details>
<details>
<summary>摘要</summary>
声音疾病影响全球数千万人，表面电动力学基于Silent Speech Interface（sEMG-based SSI）已经在数十年内被探索。然而，先前的工作受限于小词汇和手动提取的特征从原始数据中提取。为了解决这些限制，我们提出了一种轻量级深度学习知识卷积混合模型 для sEMG-based SSI（KDE-SSI）。我们的模型可以分类26个北约字母集数据集，包含3900个数据样本，使得可以不 ambiguously生成任何英文单词的拼写。我们的实验证明了KDE-SSI的效果，测试准确率达85.9%。我们的发现还照明了一个端到端系统，可以在可搬式、实用的设备上实现。
</details></li>
</ul>
<hr>
<h2 id="DSformer-A-Double-Sampling-Transformer-for-Multivariate-Time-Series-Long-term-Prediction"><a href="#DSformer-A-Double-Sampling-Transformer-for-Multivariate-Time-Series-Long-term-Prediction" class="headerlink" title="DSformer: A Double Sampling Transformer for Multivariate Time Series Long-term Prediction"></a>DSformer: A Double Sampling Transformer for Multivariate Time Series Long-term Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03274">http://arxiv.org/abs/2308.03274</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengqing Yu, Fei Wang, Zezhi Shao, Tao Sun, Lin Wu, Yongjun Xu</li>
<li>for: 本研究旨在提出一种基于变换器的多变量时间序列长期预测模型，以提高多变量时间序列预测的精度。</li>
<li>methods: 该模型由双重采样块（DS block）和时间变量注意块（TVA block）组成。DS块使用下采样和分割采样将原始序列转换为专注于全局信息和本地信息的特征 вектор。然后，TVA块使用时间注意力和变量注意力来挖掘这些特征 вектор，并提取关键信息。最后，DSformer 使用多个 TVA 块来挖掘和集成不同维度的特征信息，并将其传递给基于多层感知器的生成解码器，以实现多变量时间序列长期预测。</li>
<li>results: 实验结果表明，DSformer 可以在九个真实世界数据集上超越八个基准模型。<details>
<summary>Abstract</summary>
Multivariate time series long-term prediction, which aims to predict the change of data in a long time, can provide references for decision-making. Although transformer-based models have made progress in this field, they usually do not make full use of three features of multivariate time series: global information, local information, and variables correlation. To effectively mine the above three features and establish a high-precision prediction model, we propose a double sampling transformer (DSformer), which consists of the double sampling (DS) block and the temporal variable attention (TVA) block. Firstly, the DS block employs down sampling and piecewise sampling to transform the original series into feature vectors that focus on global information and local information respectively. Then, TVA block uses temporal attention and variable attention to mine these feature vectors from different dimensions and extract key information. Finally, based on a parallel structure, DSformer uses multiple TVA blocks to mine and integrate different features obtained from DS blocks respectively. The integrated feature information is passed to the generative decoder based on a multi-layer perceptron to realize multivariate time series long-term prediction. Experimental results on nine real-world datasets show that DSformer can outperform eight existing baselines.
</details>
<details>
<summary>摘要</summary>
多变量时间序列长期预测，目的是预测数据在长时间内的变化，可以提供决策参考。虽然变换器基本模型在这个领域中已经作出了进步，但它们通常不充分利用多变量时间序列的三个特点：全球信息、本地信息和变量相关性。为了有效利用这些特点并建立高精度预测模型，我们提出了双重采样变换器（DSformer），它包括双重采样（DS）块和时间变量注意（TVA）块。首先，DS块使用下采样和分割采样将原始系列转换为特征向量，这些向量专注于全球信息和本地信息。然后，TVA块使用时间注意和变量注意来挖掘这些特征向量从不同维度，提取关键信息。最后，基于并行结构，DSformer使用多个TVA块来挖掘和 интеGRATE不同的特征信息，并将其传递给基于多层感知器的生成解码器，实现多变量时间序列长期预测。实验结果表明，DSformer可以在九个真实世界数据集上超越八个基准。
</details></li>
</ul>
<hr>
<h2 id="Local-Structure-aware-Graph-Contrastive-Representation-Learning"><a href="#Local-Structure-aware-Graph-Contrastive-Representation-Learning" class="headerlink" title="Local Structure-aware Graph Contrastive Representation Learning"></a>Local Structure-aware Graph Contrastive Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03271">http://arxiv.org/abs/2308.03271</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Yang, Yuan Liu, Zijuan Zhao, Peijin Ding, Wenqian Zhao</li>
<li>for: 本文提出了一种Local Structure-aware Graph Contrastive representation Learning方法（LS-GCL），用于模型节点的结构信息从多个视图。</li>
<li>methods: 本方法使用了semantic subgraphs，不限于第一邻域，并使用共享GNNEncoder来学习target节点embeddings。在全视图和局部视图之间使用了pooling函数来生成子图级别图像 embeddings。</li>
<li>results: 实验结果表明，LS-GCL方法在五个数据集上的node classification和link prediction任务中都超过了状态方法。<details>
<summary>Abstract</summary>
Traditional Graph Neural Network (GNN), as a graph representation learning method, is constrained by label information. However, Graph Contrastive Learning (GCL) methods, which tackle the label problem effectively, mainly focus on the feature information of the global graph or small subgraph structure (e.g., the first-order neighborhood). In the paper, we propose a Local Structure-aware Graph Contrastive representation Learning method (LS-GCL) to model the structural information of nodes from multiple views. Specifically, we construct the semantic subgraphs that are not limited to the first-order neighbors. For the local view, the semantic subgraph of each target node is input into a shared GNN encoder to obtain the target node embeddings at the subgraph-level. Then, we use a pooling function to generate the subgraph-level graph embeddings. For the global view, considering the original graph preserves indispensable semantic information of nodes, we leverage the shared GNN encoder to learn the target node embeddings at the global graph-level. The proposed LS-GCL model is optimized to maximize the common information among similar instances at three various perspectives through a multi-level contrastive loss function. Experimental results on five datasets illustrate that our method outperforms state-of-the-art graph representation learning approaches for both node classification and link prediction tasks.
</details>
<details>
<summary>摘要</summary>
传统的图 neural network (GNN) 是一种图表示学习方法，它受到标签信息的限制。然而，图相对学习 (GCL) 方法，它们能够有效解决标签问题，主要关注全图或小子图结构（例如，第一颗邻居）的特征信息。在文章中，我们提出了一种本地结构意识 Graph Contrastive representation Learning 方法 (LS-GCL)，用于模型节点的多视图结构信息。具体来说，我们构建了不同于第一颗邻居的semantic subgraph，并将每个目标节点的semantic subgraph输入到共享 GNN Encoder 中，以获得目标节点的subgraph级别嵌入。然后，我们使用一个pooling函数生成subgraph级别图像嵌入。对于全图视图，我们利用共享 GNN Encoder 来学习目标节点的全图级别嵌入。我们的 LS-GCL 模型通过最大化三种不同视角的共同信息来优化一个多级对比损失函数来进行优化。实验结果表明，我们的方法在五个dataset上对节点 classification 和链接预测任务均达到了当前最佳性能。
</details></li>
</ul>
<hr>
<h2 id="Simple-Rule-Injection-for-ComplEx-Embeddings"><a href="#Simple-Rule-Injection-for-ComplEx-Embeddings" class="headerlink" title="Simple Rule Injection for ComplEx Embeddings"></a>Simple Rule Injection for ComplEx Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03269">http://arxiv.org/abs/2308.03269</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haodi Ma, Anthony Colas, Yuejie Wang, Ali Sadeghian, Daisy Zhe Wang</li>
<li>for: 本研究旨在协调逻辑规则与知识图 embeddings，以利用先前知识来提高 neural knowledge graph inference 的性能。</li>
<li>methods: 本研究提出了 InjEx 机制，可以通过简单的约束将多种类型的逻辑规则注入到 embedding 空间中，以捕捉definite Horn 规则。</li>
<li>results: 在知识图完成 (KGC) 和少量shot知识图完成 (FKGC) 设置下，InjEx 比基eline KGC 模型和特殊化的几个shot模型表现出色，同时保持了可扩展性和效率。<details>
<summary>Abstract</summary>
Recent works in neural knowledge graph inference attempt to combine logic rules with knowledge graph embeddings to benefit from prior knowledge. However, they usually cannot avoid rule grounding, and injecting a diverse set of rules has still not been thoroughly explored. In this work, we propose InjEx, a mechanism to inject multiple types of rules through simple constraints, which capture definite Horn rules. To start, we theoretically prove that InjEx can inject such rules. Next, to demonstrate that InjEx infuses interpretable prior knowledge into the embedding space, we evaluate InjEx on both the knowledge graph completion (KGC) and few-shot knowledge graph completion (FKGC) settings. Our experimental results reveal that InjEx outperforms both baseline KGC models as well as specialized few-shot models while maintaining its scalability and efficiency.
</details>
<details>
<summary>摘要</summary>
近期研究在神经知识Graph推理中尝试将逻辑规则与知识Graph嵌入结合以获得优势。然而，他们通常无法避免规则定义，并且尝试插入多种规则的可能性还没有得到了全面的探索。在这种情况下，我们提出了InjEx，一种机制来插入多种规则， capture definite Horn rules。首先，我们 theoretically prove that InjEx可以插入这些规则。然后，我们通过在知识Graph completion（KGC）和少量知识Graph completion（FKGC）设置中评估InjEx，以证明它可以把有意义的先前知识注入到嵌入空间中。我们的实验结果表明，InjEx在KGC和FKGC设置中的性能都高于基eline模型和专门的少量模型，同时保持其可扩展性和高效性。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Different-Time-series-Transformer-TST-Architectures-A-Case-Study-in-Battery-Life-Prediction-for-Electric-Vehicles-EVs"><a href="#Exploring-Different-Time-series-Transformer-TST-Architectures-A-Case-Study-in-Battery-Life-Prediction-for-Electric-Vehicles-EVs" class="headerlink" title="Exploring Different Time-series-Transformer (TST) Architectures: A Case Study in Battery Life Prediction for Electric Vehicles (EVs)"></a>Exploring Different Time-series-Transformer (TST) Architectures: A Case Study in Battery Life Prediction for Electric Vehicles (EVs)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03260">http://arxiv.org/abs/2308.03260</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niranjan Sitapure, Atharva Kulkarni</li>
<li>For: The paper aims to develop accurate battery life prediction models for electric vehicles (EVs) by incorporating environmental, battery, vehicle driving, and heating circuit data.* Methods: The paper uses time-series-transformers (TSTs) and long short-term memory (LSTM) models to predict state-of-charge (SOC) and battery temperature. The authors also explore novel TST architectures, including encoder TST + decoder LSTM and a hybrid TST-LSTM.* Results: The paper uses a dataset of 72 driving trips in a BMW i3 (60 Ah) to evaluate the performance of the proposed models. The results show that the TST models outperform the LSTM models and traditional battery models, with an accuracy of 95.6% for SOC and 93.8% for battery temperature.<details>
<summary>Abstract</summary>
In recent years, battery technology for electric vehicles (EVs) has been a major focus, with a significant emphasis on developing new battery materials and chemistries. However, accurately predicting key battery parameters, such as state-of-charge (SOC) and temperature, remains a challenge for constructing advanced battery management systems (BMS). Existing battery models do not comprehensively cover all parameters affecting battery performance, including non-battery-related factors like ambient temperature, cabin temperature, elevation, and regenerative braking during EV operation. Due to the difficulty of incorporating these auxiliary parameters into traditional models, a data-driven approach is suggested. Time-series-transformers (TSTs), leveraging multiheaded attention and parallelization-friendly architecture, are explored alongside LSTM models. Novel TST architectures, including encoder TST + decoder LSTM and a hybrid TST-LSTM, are also developed and compared against existing models. A dataset comprising 72 driving trips in a BMW i3 (60 Ah) is used to address battery life prediction in EVs, aiming to create accurate TST models that incorporate environmental, battery, vehicle driving, and heating circuit data to predict SOC and battery temperature for future time steps.
</details>
<details>
<summary>摘要</summary>
近年来，电动汽车（EV）的电池技术得到了广泛关注，尤其是开发新的电池材料和化学组合。然而，正确预测电池参数，如充电状态（SOC）和温度，仍然是构建高级电池管理系统（BMS）的挑战。现有的电池模型并不完全覆盖所有影响电池性能的参数，包括非电池相关的因素，如外部温度、车辆内部温度、海拔和恢复摩擦 durante la operación del vehículo eléctrico.由于将这些辅助参数integrated into traditional models是困难的，一种数据驱动的方法被建议。使用时间序列变换（TST）和长短期memory（LSTM）模型，并研发了新的TST架构，包括encoder TST + decoder LSTM和hybrid TST-LSTM。使用一个包含72次开车记录的BMW i3（60 Ah）数据集，以预测将来时间步的SOC和电池温度。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Approximation-and-Learning-Rates-for-Deep-Convolutional-Neural-Networks"><a href="#Optimal-Approximation-and-Learning-Rates-for-Deep-Convolutional-Neural-Networks" class="headerlink" title="Optimal Approximation and Learning Rates for Deep Convolutional Neural Networks"></a>Optimal Approximation and Learning Rates for Deep Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03259">http://arxiv.org/abs/2308.03259</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shao-Bo Lin</li>
<li>for: 本研究探讨了深度卷积神经网络（CNN）的approximation和学习性能分析，特别是采用零填充和最大池化。</li>
<li>methods: 我们提供了一种可以approximate $r$-smooth函数的深度卷积神经网络的分析方法，并证明了这种方法的approximation率为 $(L^2&#x2F;\log L)^{-2r&#x2F;d} $，这是最佳的下界。</li>
<li>results: 我们得到了深度卷积神经网络在实际风险最小化中的几乎最佳学习率。<details>
<summary>Abstract</summary>
This paper focuses on approximation and learning performance analysis for deep convolutional neural networks with zero-padding and max-pooling. We prove that, to approximate $r$-smooth function, the approximation rates of deep convolutional neural networks with depth $L$ are of order $ (L^2/\log L)^{-2r/d} $, which is optimal up to a logarithmic factor. Furthermore, we deduce almost optimal learning rates for implementing empirical risk minimization over deep convolutional neural networks.
</details>
<details>
<summary>摘要</summary>
Here's the Simplified Chinese translation:这篇论文关注深度 convolutional neural networks (CNNs) 的近似和学习性能分析，特别是在零填充和最大 pooling 下。我们证明，要近似 $r $- 平滑函数，深度 $L $ 的 CNNs 的近似率是 $(L^2/\log L)^{-2r/d} $，即最优的logs 因子。此外，我们还得出了对深度 CNNs 的实际风险最小化的几乎最优学习率。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Adversarial-Detection-without-Extra-Model-Training-Loss-Should-Change"><a href="#Unsupervised-Adversarial-Detection-without-Extra-Model-Training-Loss-Should-Change" class="headerlink" title="Unsupervised Adversarial Detection without Extra Model: Training Loss Should Change"></a>Unsupervised Adversarial Detection without Extra Model: Training Loss Should Change</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03243">http://arxiv.org/abs/2308.03243</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cyclebooster/unsupervised-adversarial-detection-without-extra-model">https://github.com/cyclebooster/unsupervised-adversarial-detection-without-extra-model</a></li>
<li>paper_authors: Chien Cheng Chyou, Hung-Ting Su, Winston H. Hsu</li>
<li>for: 防御深度学习模型受到攻击的挑战是现实世界应用中的关键问题。传统的对抗训练和监督检测方法需要先知道攻击类型和训练数据标注，这经常是不实际的。现有的无监督对抗检测方法可以判断目标模型是否正常工作，但它们因使用共同减少极限loss而受到差异攻击的强化，导致准确率差。</li>
<li>methods: 我们提议使用新的训练损失来减少无用的特征，并对无需先知道攻击类型的检测方法。</li>
<li>results: 我们的检测率（真正正确率）对所有给出的白盒攻击是高于93.9%，只有DF($\infty$)攻击没有限制。false positive rate几乎为2.5%。我们的方法在所有攻击类型下都工作良好，并且对某些类型的方法表现更好。<details>
<summary>Abstract</summary>
Adversarial robustness poses a critical challenge in the deployment of deep learning models for real-world applications. Traditional approaches to adversarial training and supervised detection rely on prior knowledge of attack types and access to labeled training data, which is often impractical. Existing unsupervised adversarial detection methods identify whether the target model works properly, but they suffer from bad accuracies owing to the use of common cross-entropy training loss, which relies on unnecessary features and strengthens adversarial attacks. We propose new training losses to reduce useless features and the corresponding detection method without prior knowledge of adversarial attacks. The detection rate (true positive rate) against all given white-box attacks is above 93.9% except for attacks without limits (DF($\infty$)), while the false positive rate is barely 2.5%. The proposed method works well in all tested attack types and the false positive rates are even better than the methods good at certain types.
</details>
<details>
<summary>摘要</summary>
“深度学习模型的应急防范问题是实际应用中的关键挑战。传统的敌对训练和监管检测方法需要先知 adversarial 攻击类型和训练数据的标注，这经常是不现实的。现有的无监管敌对检测方法可以判断目标模型是否正常工作，但它们因使用通用的 cross-entropy 训练损失函数，导致强化敌对攻击。我们提出了新的训练损失函数，以减少不必要的特征，并对这些特征进行检测方法，不需要先知 adversarial 攻击。检测率（真正正确率）对所有给出的白盒攻击是大于 93.9%，只有 attacks without limits（DF（∞））的检测率较低，而假阳性率只有 2.5%。我们的方法在所有攻击类型中都工作良好，假阳性率甚至比其他方法更好。”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. The translation is based on the original text and may not capture all the nuances and details of the original text.
</details></li>
</ul>
<hr>
<h2 id="Asynchronous-Decentralized-Q-Learning-Two-Timescale-Analysis-By-Persistence"><a href="#Asynchronous-Decentralized-Q-Learning-Two-Timescale-Analysis-By-Persistence" class="headerlink" title="Asynchronous Decentralized Q-Learning: Two Timescale Analysis By Persistence"></a>Asynchronous Decentralized Q-Learning: Two Timescale Analysis By Persistence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03239">http://arxiv.org/abs/2308.03239</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bora Yongacoglu, Gürdal Arslan, Serdar Yüksel</li>
<li>for: 本研究旨在探讨非站立性是多智能 reinforcement learning（MARL）挑战的基础问题，并研究如何使用不同的方法解决这个问题。</li>
<li>methods: 本文使用的方法包括各种协调策略更新的方法，包括同步化策略更新的时间。这些同步化方法可以使得多个智能体的策略更新趋于相同，从而使得问题的分析变得更加容易。然而，在许多分散式应用中，同步化是不可能实现的。因此，本文研究了一种异步的Q值学习算法的variant，以解决这个问题。</li>
<li>results: 本文的分析表明，当使用不同的学习率时，异步Q值学习算法可以逐渐靠拢到平衡状态，并且在某些情况下可以达到更高的性能。此外，本文还证明了这种算法在分散式应用中可以有更好的性能，而无需协调智能体的策略更新。<details>
<summary>Abstract</summary>
Non-stationarity is a fundamental challenge in multi-agent reinforcement learning (MARL), where agents update their behaviour as they learn. Many theoretical advances in MARL avoid the challenge of non-stationarity by coordinating the policy updates of agents in various ways, including synchronizing times at which agents are allowed to revise their policies. Synchronization enables analysis of many MARL algorithms via multi-timescale methods, but such synchrony is infeasible in many decentralized applications. In this paper, we study an asynchronous variant of the decentralized Q-learning algorithm, a recent MARL algorithm for stochastic games. We provide sufficient conditions under which the asynchronous algorithm drives play to equilibrium with high probability. Our solution utilizes constant learning rates in the Q-factor update, which we show to be critical for relaxing the synchrony assumptions of earlier work. Our analysis also applies to asynchronous generalizations of a number of other algorithms from the regret testing tradition, whose performance is analyzed by multi-timescale methods that study Markov chains obtained via policy update dynamics. This work extends the applicability of the decentralized Q-learning algorithm and its relatives to settings in which parameters are selected in an independent manner, and tames non-stationarity without imposing the coordination assumptions of prior work.
</details>
<details>
<summary>摘要</summary>
非站点性是多智能游戏学习（MARL）的基本挑战，智能体在学习过程中会更新其行为。许多MARL理论进步避免非站点性挑战，通过协调智能体策略更新的方式，包括同步化智能体更新策略的时间。同步化可以通过多时间标准方法进行分析，但这种同步性在多数分散式应用中是不可能实现的。在这篇论文中，我们研究了一种异步版的分散式Q学习算法，这是一种最近的MARL算法 для随机游戏。我们提供了 sufficient condition，以 guarantee that the asynchronous algorithm drives play to equilibrium with high probability.我们的解决方案使用了常量学习率在Q因子更新中，我们证明这是关键 для放弃先前作品中的同步性假设。我们的分析还适用于异步扩展其他算法，这些算法是由回报测试传统中的表现分析。这项工作扩展了分散式Q学习算法和其相关的算法在不同的参数选择方式下的可用性，并对非站点性进行了控制，而不需要先前作品中的协调假设。
</details></li>
</ul>
<hr>
<h2 id="AdaER-An-Adaptive-Experience-Replay-Approach-for-Continual-Lifelong-Learning"><a href="#AdaER-An-Adaptive-Experience-Replay-Approach-for-Continual-Lifelong-Learning" class="headerlink" title="AdaER: An Adaptive Experience Replay Approach for Continual Lifelong Learning"></a>AdaER: An Adaptive Experience Replay Approach for Continual Lifelong Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03810">http://arxiv.org/abs/2308.03810</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingyu Li, Bo Tang, Haifeng Li</li>
<li>for: 这篇论文是为了解决人工智能学习框架中的持续性学习问题，即在不断学习的过程中，学习器会忘记之前学习的知识。</li>
<li>methods: 这篇论文提出了一种新的算法，叫做自适应经验回忆（AdaER），它包括两个阶段：记忆回忆和记忆更新。在记忆回忆阶段，AdaER使用了一种Contextually-cued Memory Recall（C-CMR）策略，选择最相似的记忆和当前输入数据和任务进行回忆。此外，AdaER还使用了一种Entropy-balanced Reservoir Sampling（E-BRS）策略来增强记忆缓存的性能。</li>
<li>results: 根据实验结果，AdaER在成本累积学习场景下表现出色，比现有的持续性学习基线更好， highlighting its efficacy in mitigating catastrophic forgetting and improving learning performance.<details>
<summary>Abstract</summary>
Continual lifelong learning is an machine learning framework inspired by human learning, where learners are trained to continuously acquire new knowledge in a sequential manner. However, the non-stationary nature of streaming training data poses a significant challenge known as catastrophic forgetting, which refers to the rapid forgetting of previously learned knowledge when new tasks are introduced. While some approaches, such as experience replay (ER), have been proposed to mitigate this issue, their performance remains limited, particularly in the class-incremental scenario which is considered natural and highly challenging. In this paper, we present a novel algorithm, called adaptive-experience replay (AdaER), to address the challenge of continual lifelong learning. AdaER consists of two stages: memory replay and memory update. In the memory replay stage, AdaER introduces a contextually-cued memory recall (C-CMR) strategy, which selectively replays memories that are most conflicting with the current input data in terms of both data and task. Additionally, AdaER incorporates an entropy-balanced reservoir sampling (E-BRS) strategy to enhance the performance of the memory buffer by maximizing information entropy. To evaluate the effectiveness of AdaER, we conduct experiments on established supervised continual lifelong learning benchmarks, specifically focusing on class-incremental learning scenarios. The results demonstrate that AdaER outperforms existing continual lifelong learning baselines, highlighting its efficacy in mitigating catastrophic forgetting and improving learning performance.
</details>
<details>
<summary>摘要</summary>
In this paper, we present a novel algorithm called adaptive-experience replay (AdaER) to address the challenge of continual lifelong learning. AdaER consists of two stages: memory replay and memory update. In the memory replay stage, AdaER introduces a contextually-cued memory recall (C-CMR) strategy, which selectively replays memories that are most conflicting with the current input data in terms of both data and task. Additionally, AdaER incorporates an entropy-balanced reservoir sampling (E-BRS) strategy to enhance the performance of the memory buffer by maximizing information entropy.To evaluate the effectiveness of AdaER, we conduct experiments on established supervised continual lifelong learning benchmarks, specifically focusing on class-incremental learning scenarios. The results demonstrate that AdaER outperforms existing continual lifelong learning baselines, highlighting its efficacy in mitigating catastrophic forgetting and improving learning performance.
</details></li>
</ul>
<hr>
<h2 id="G-Mix-A-Generalized-Mixup-Learning-Framework-Towards-Flat-Minima"><a href="#G-Mix-A-Generalized-Mixup-Learning-Framework-Towards-Flat-Minima" class="headerlink" title="G-Mix: A Generalized Mixup Learning Framework Towards Flat Minima"></a>G-Mix: A Generalized Mixup Learning Framework Towards Flat Minima</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03236">http://arxiv.org/abs/2308.03236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingyu Li, Bo Tang<br>for: 增强深度神经网络（DNN）的泛化能力，特别是在有限的训练数据时。methods:  combinatorial Mixup技术和Sharpness-Aware Minimization（SAM）方法。results: 提出了一种新的学习框架 called Generalized-Mixup（G-Mix），并 introduces two novel algorithms：Binary G-Mix和Decomposed G-Mix。这些算法可以进一步优化DNN性能，并实现多个数据集和模型的状态反映性。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) have demonstrated promising results in various complex tasks. However, current DNNs encounter challenges with over-parameterization, especially when there is limited training data available. To enhance the generalization capability of DNNs, the Mixup technique has gained popularity. Nevertheless, it still produces suboptimal outcomes. Inspired by the successful Sharpness-Aware Minimization (SAM) approach, which establishes a connection between the sharpness of the training loss landscape and model generalization, we propose a new learning framework called Generalized-Mixup, which combines the strengths of Mixup and SAM for training DNN models. The theoretical analysis provided demonstrates how the developed G-Mix framework enhances generalization. Additionally, to further optimize DNN performance with the G-Mix framework, we introduce two novel algorithms: Binary G-Mix and Decomposed G-Mix. These algorithms partition the training data into two subsets based on the sharpness-sensitivity of each example to address the issue of "manifold intrusion" in Mixup. Both theoretical explanations and experimental results reveal that the proposed BG-Mix and DG-Mix algorithms further enhance model generalization across multiple datasets and models, achieving state-of-the-art performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Analysis-of-the-Evolution-of-Advanced-Transformer-Based-Language-Models-Experiments-on-Opinion-Mining"><a href="#Analysis-of-the-Evolution-of-Advanced-Transformer-Based-Language-Models-Experiments-on-Opinion-Mining" class="headerlink" title="Analysis of the Evolution of Advanced Transformer-Based Language Models: Experiments on Opinion Mining"></a>Analysis of the Evolution of Advanced Transformer-Based Language Models: Experiments on Opinion Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03235">http://arxiv.org/abs/2308.03235</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zekaouinoureddine/Opinion-Transformers">https://github.com/zekaouinoureddine/Opinion-Transformers</a></li>
<li>paper_authors: Nour Eddine Zekaoui, Siham Yousfi, Maryem Rhanoui, Mounia Mikram</li>
<li>for: 本研究的目的是研究高性能的Transformer语言模型在意见采集方面的表现，并对它们进行比较，以揭示它们的特点。</li>
<li>methods: 本研究使用了高性能的Transformer语言模型，包括BERT和RoBERTa等，对文本进行意见采集和分析。</li>
<li>results: 研究结果显示，Transformer语言模型在意见采集方面具有优秀的表现，具有较强的表达能力和精度。同时，BERT和RoBERTa等模型在意见采集方面具有显著的差异，可以根据实际应用场景选择合适的模型。<details>
<summary>Abstract</summary>
Opinion mining, also known as sentiment analysis, is a subfield of natural language processing (NLP) that focuses on identifying and extracting subjective information in textual material. This can include determining the overall sentiment of a piece of text (e.g., positive or negative), as well as identifying specific emotions or opinions expressed in the text, that involves the use of advanced machine and deep learning techniques. Recently, transformer-based language models make this task of human emotion analysis intuitive, thanks to the attention mechanism and parallel computation. These advantages make such models very powerful on linguistic tasks, unlike recurrent neural networks that spend a lot of time on sequential processing, making them prone to fail when it comes to processing long text. The scope of our paper aims to study the behaviour of the cutting-edge Transformer-based language models on opinion mining and provide a high-level comparison between them to highlight their key particularities. Additionally, our comparative study shows leads and paves the way for production engineers regarding the approach to focus on and is useful for researchers as it provides guidelines for future research subjects.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Imbalanced-Large-Graph-Learning-Framework-for-FPGA-Logic-Elements-Packing-Prediction"><a href="#Imbalanced-Large-Graph-Learning-Framework-for-FPGA-Logic-Elements-Packing-Prediction" class="headerlink" title="Imbalanced Large Graph Learning Framework for FPGA Logic Elements Packing Prediction"></a>Imbalanced Large Graph Learning Framework for FPGA Logic Elements Packing Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03231">http://arxiv.org/abs/2308.03231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhixiong Di, Runzhe Tao, Lin Chen, Qiang Wu, Yibo Lin</li>
<li>for: This paper is written for the purpose of predicting whether logic elements will be packed after placement in an FPGA CAD flow.</li>
<li>methods: The paper proposes an imbalanced large graph learning framework called ImLG, which uses dedicated feature extraction and feature aggregation methods to enhance the node representation learning of circuit graphs. The framework also employs techniques such as graph oversampling and mini-batch training to handle the imbalanced distribution of packed and unpacked logic elements.</li>
<li>results: The proposed method achieves an F1 score improvement of 42.82% compared to the most recent Gaussian-based prediction method. Additionally, the physical design results show that the proposed method can assist the placer in improving routed wirelength by 0.93% and SLICE occupation by 0.89%.Here is the same information in Simplified Chinese:</li>
<li>for: 这篇论文是为了预测FPGA嵌入式逻辑元素是否会在置换后被压缩而写的。</li>
<li>methods: 论文提出了一种大图学习框架 called ImLG，该框架使用专门的特征提取和特征聚合方法来提高电路图的节点表示学习。它还使用了 Graph oversampling 和 mini-batch 训练来处理嵌入式逻辑元素的不均衡分布。</li>
<li>results: 提议的方法可以提高对最近 Gaussian-based 预测方法的 F1 score 值 by 42.82%。Physical 设计结果表明，提议的方法可以帮助置换器提高路径长度 by 0.93% 和 SLICE 占用率 by 0.89%。<details>
<summary>Abstract</summary>
Packing is a required step in a typical FPGA CAD flow. It has high impacts to the performance of FPGA placement and routing. Early prediction of packing results can guide design optimization and expedite design closure. In this work, we propose an imbalanced large graph learning framework, ImLG, for prediction of whether logic elements will be packed after placement. Specifically, we propose dedicated feature extraction and feature aggregation methods to enhance the node representation learning of circuit graphs. With imbalanced distribution of packed and unpacked logic elements, we further propose techniques such as graph oversampling and mini-batch training for this imbalanced learning task in large circuit graphs. Experimental results demonstrate that our framework can improve the F1 score by 42.82% compared to the most recent Gaussian-based prediction method. Physical design results show that the proposed method can assist the placer in improving routed wirelength by 0.93% and SLICE occupation by 0.89%.
</details>
<details>
<summary>摘要</summary>
packing 是 FPGA CAD 流程中的一个必需步骤，它对 FPGA 的位置和路由产生了重要影响。 early prediction of packing results 可以导引设计优化和加速设计关闭。在这种工作中，我们提出了一个大图学习框架，ImLG，用于预测逻辑元素是否会被压缩 после放置。 Specifically，我们提出了专门的特征提取和特征聚合方法，以增强圈图学习的节点表示学习。 由于逻辑元素的压缩和未压缩的分布是不均衡的，我们还提出了一些技术，如图像扩大和微批训练，来解决这种不均衡的学习任务。 experimental results 表明，我们的框架可以提高 F1 分数比最近 Gaussian-based 预测方法提高42.82%。 physical design results 表明，我们的方法可以帮助置器提高了 routed wirelength 和 SLICE 占用率。
</details></li>
</ul>
<hr>
<h2 id="Tractability-of-approximation-by-general-shallow-networks"><a href="#Tractability-of-approximation-by-general-shallow-networks" class="headerlink" title="Tractability of approximation by general shallow networks"></a>Tractability of approximation by general shallow networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03230">http://arxiv.org/abs/2308.03230</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hrushikesh Mhaskar, Tong Mao</li>
<li>for: 这 paper 描述了一种精炼版的函数approximation算法，用于 approximating 函数的形式为 $x\mapsto\int_{\mathbb{Y}} G(x,y)d\tau(y)$， $x\in\mathbb{X}$， by $G$-networks of the form $x\mapsto\sum_{k&#x3D;1}^n a_kG(x,y_k)$, $y_1,\cdots, y_n\in\mathbb{Y}$, $a_1,\cdots, a_n\in\mathbb{R}$。</li>
<li>methods: 该 paper 使用了covering numbers来定义 $\mathbb{X}$ 和 $\mathbb{Y}$ 的维度，然后通过使用 $G$-networks 获得了独立于维度的approximation bound。</li>
<li>results: 该 paper  obtainted dimension-independent approximation bounds, meaning that the bounds do not depend on the dimensions of the input spaces $\mathbb{X}$ and $\mathbb{Y}$. The paper also provided applications of the results to various types of neural networks, including power rectified linear unit networks, zonal function networks, and radial basis function networks.<details>
<summary>Abstract</summary>
In this paper, we present a sharper version of the results in the paper Dimension independent bounds for general shallow networks; Neural Networks, \textbf{123} (2020), 142-152. Let $\mathbb{X}$ and $\mathbb{Y}$ be compact metric spaces. We consider approximation of functions of the form $ x\mapsto\int_{\mathbb{Y}} G( x, y)d\tau( y)$, $ x\in\mathbb{X}$, by $G$-networks of the form $ x\mapsto \sum_{k=1}^n a_kG( x, y_k)$, $ y_1,\cdots, y_n\in\mathbb{Y}$, $a_1,\cdots, a_n\in\mathbb{R}$. Defining the dimensions of $\mathbb{X}$ and $\mathbb{Y}$ in terms of covering numbers, we obtain dimension independent bounds on the degree of approximation in terms of $n$, where also the constants involved are all dependent at most polynomially on the dimensions. Applications include approximation by power rectified linear unit networks, zonal function networks, certain radial basis function networks as well as the important problem of function extension to higher dimensional spaces.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出了一个更加锐化的结果，即纸张《独立维度下的深度网络 bound》（Neural Networks, 123（2020），142-152页）。我们假设 $\mathbb{X}$ 和 $\mathbb{Y}$ 是两个紧密的 метри空间。我们考虑了 $\mathbb{X}$ 上函数 $x\mapsto\int_{\mathbb{Y}} G(x,y)d\tau(y)$ 的近似问题，其中 $G$ 是一个从 $\mathbb{X}$ 到 $\mathbb{Y}$ 的函数，$x\in\mathbb{X}$，$y_1,\cdots,y_n\in\mathbb{Y}$，$a_1,\cdots,a_n\in\mathbb{R}$。我们使用 $\mathbb{X}$ 和 $\mathbb{Y}$ 的覆盖数来定义这些空间的维度，并得到了独立于维度的近似度 bound，其中包括了 $n$ 的度量，同时 constants 都是仅仅受到维度的 polynomial 依赖。这些结果有广泛的应用，包括矩形函数网络、zonal函数网络、某些径向基函数网络以及高维空间中函数扩展的重要问题。
</details></li>
</ul>
<hr>
<h2 id="Why-Linguistics-Will-Thrive-in-the-21st-Century-A-Reply-to-Piantadosi-2023"><a href="#Why-Linguistics-Will-Thrive-in-the-21st-Century-A-Reply-to-Piantadosi-2023" class="headerlink" title="Why Linguistics Will Thrive in the 21st Century: A Reply to Piantadosi (2023)"></a>Why Linguistics Will Thrive in the 21st Century: A Reply to Piantadosi (2023)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03228">http://arxiv.org/abs/2308.03228</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jordan Kodner, Sarah Payne, Jeffrey Heinz</li>
<li>for: The paper argues against Piantadosi’s (2023) claim that modern language models refute Chomsky’s approach to language, and instead presents a case for the continued relevance of generative linguistics in the 21st century.</li>
<li>methods: The paper focuses on four main points to critique Piantadosi’s claim, including the limited data exposure required for human language acquisition, the inability of LLMs to solve the central mystery of language learning, the lack of interpretable explanations provided by LLMs, and the need for a separate theory of language and cognition.</li>
<li>results: The paper concludes that generative linguistics will remain an indispensable scientific discipline throughout the 21st century and beyond, due to its ability to provide a theory of language and cognition that can explain human linguistic and cognitive capabilities.<details>
<summary>Abstract</summary>
We present a critical assessment of Piantadosi's (2023) claim that "Modern language models refute Chomsky's approach to language," focusing on four main points. First, despite the impressive performance and utility of large language models (LLMs), humans achieve their capacity for language after exposure to several orders of magnitude less data. The fact that young children become competent, fluent speakers of their native languages with relatively little exposure to them is the central mystery of language learning to which Chomsky initially drew attention, and LLMs currently show little promise of solving this mystery. Second, what can the artificial reveal about the natural? Put simply, the implications of LLMs for our understanding of the cognitive structures and mechanisms underlying language and its acquisition are like the implications of airplanes for understanding how birds fly. Third, LLMs cannot constitute scientific theories of language for several reasons, not least of which is that scientific theories must provide interpretable explanations, not just predictions. This leads to our final point: to even determine whether the linguistic and cognitive capabilities of LLMs rival those of humans requires explicating what humans' capacities actually are. In other words, it requires a separate theory of language and cognition; generative linguistics provides precisely such a theory. As such, we conclude that generative linguistics as a scientific discipline will remain indispensable throughout the 21st century and beyond.
</details>
<details>
<summary>摘要</summary>
我们提出了对Piantadosi（2023）的批判，关注四个主要点。首先，虽然大型语言模型（LLMs）表现出色且有用，但人类通过对数量更少的数据进行抵抗而获得语言能力。孩子们在获得native语言的能力上即使只有相对较少的暴露也能够成为流利的语言使用者，这是语言学习的中心谜题，LLMs目前并没有解决这个谜题。第二，人工智能可以抛光自然语言吗？简单地说，LLMs对语言和其学习的各种认知结构和机制的含义是如何飞行机器对鸟类飞行的含义。第三，LLMs不能代表语言科学的理论，主要是因为科学理论需要可解释的解释，而不仅仅是预测。这导致我们的最后一点：以规避解释 humans的语言和认知能力的准确性，我们需要一个分析 humans的语言和认知能力的理论。通过这样的理论，我们可以确定LLMs的语言和认知能力是否与人类相当。因此，我们结论认为生成语言学将在21世纪和以后保持不可或缺的重要性。
</details></li>
</ul>
<hr>
<h2 id="Local-Consensus-Enhanced-Siamese-Network-with-Reciprocal-Loss-for-Two-view-Correspondence-Learning"><a href="#Local-Consensus-Enhanced-Siamese-Network-with-Reciprocal-Loss-for-Two-view-Correspondence-Learning" class="headerlink" title="Local Consensus Enhanced Siamese Network with Reciprocal Loss for Two-view Correspondence Learning"></a>Local Consensus Enhanced Siamese Network with Reciprocal Loss for Two-view Correspondence Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03217">http://arxiv.org/abs/2308.03217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linbo Wang, Jing Wu, Xianyong Fang, Zhengyi Liu, Chenjie Cao, Yanwei Fu</li>
<li>for: 提高两视匹配学习框架的性能，特别是对于缺乏精度的相对pose和匹配可靠性的评估。</li>
<li>methods: 提议了两个方法来提高现有框架：一是Local Feature Consensus（LFC）插件块，可以增强现有模型的特征；二是将现有模型扩展为对称网络，并使用对称损失来利用相互投影的信息。</li>
<li>results: 通过对比 dataset 上的表现，实际 achieved state-of-the-art 性能。<details>
<summary>Abstract</summary>
Recent studies of two-view correspondence learning usually establish an end-to-end network to jointly predict correspondence reliability and relative pose. We improve such a framework from two aspects. First, we propose a Local Feature Consensus (LFC) plugin block to augment the features of existing models. Given a correspondence feature, the block augments its neighboring features with mutual neighborhood consensus and aggregates them to produce an enhanced feature. As inliers obey a uniform cross-view transformation and share more consistent learned features than outliers, feature consensus strengthens inlier correlation and suppresses outlier distraction, which makes output features more discriminative for classifying inliers/outliers. Second, existing approaches supervise network training with the ground truth correspondences and essential matrix projecting one image to the other for an input image pair, without considering the information from the reverse mapping. We extend existing models to a Siamese network with a reciprocal loss that exploits the supervision of mutual projection, which considerably promotes the matching performance without introducing additional model parameters. Building upon MSA-Net, we implement the two proposals and experimentally achieve state-of-the-art performance on benchmark datasets.
</details>
<details>
<summary>摘要</summary>
最近的两视匹配学习研究通常建立一个端到端网络，同时预测匹配可靠性和相对pose。我们从两个方面提高这种框架：首先，我们提议在已有模型中添加本地特征协调（LFC）插件块，用于增强特征。给定一个匹配特征，该块在相邻特征中查找mutual neighborhood consensus，并将其聚合以生成提高后的特征。由于准确的匹配点遵循均匀的双视变换，并且在不同视图中分享更多的学习特征，因此特征协调强化匹配点的相互关联，同时减少了噪声的扰乱，使输出特征更加精细地分类匹配点和噪声。其次，现有方法在网络训练时使用真实的匹配和主对投影，而不考虑反向映射的信息。我们扩展现有模型到siamese网络，并使用相互损失来优化匹配性，不需要添加更多的模型参数。基于MSA-Net，我们实现了两个提议，并在benchmark数据集上实验性实现了状态之前的性能。
</details></li>
</ul>
<hr>
<h2 id="The-Effect-of-SGD-Batch-Size-on-Autoencoder-Learning-Sparsity-Sharpness-and-Feature-Learning"><a href="#The-Effect-of-SGD-Batch-Size-on-Autoencoder-Learning-Sparsity-Sharpness-and-Feature-Learning" class="headerlink" title="The Effect of SGD Batch Size on Autoencoder Learning: Sparsity, Sharpness, and Feature Learning"></a>The Effect of SGD Batch Size on Autoencoder Learning: Sparsity, Sharpness, and Feature Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03215">http://arxiv.org/abs/2308.03215</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikhil Ghosh, Spencer Frei, Wooseok Ha, Bin Yu</li>
<li>for: 这个论文研究了使用梯度下降法（SGD）训练单神经 autoencoder 的动态性，特别是在线性或ReLU激活函数下。</li>
<li>methods: 这篇论文使用了SGD算法，并研究了不同批处理大小的影响。</li>
<li>results: 研究发现，SGD可以在非对称问题上找到全局最优解，但是这个全局最优解与批处理大小有关。在全批处理情况下，解是密集的（即不含杂），并且与初始方向高度一致，这表明在这种情况下， Stochastic gradient descent 并没有进行feature learning。相反，在任何小于样本数的批处理情况下，SGD会找到一个全局最优解，这个解是稀疏的（即与初始方向垂直），并且与feature learning相关。此外，通过评估梯度下降法的锐度，我们发现，使用全批处理 gradient descent 时，解的锐度较低，而使用小于样本数的批处理时，解的锐度较高。<details>
<summary>Abstract</summary>
In this work, we investigate the dynamics of stochastic gradient descent (SGD) when training a single-neuron autoencoder with linear or ReLU activation on orthogonal data. We show that for this non-convex problem, randomly initialized SGD with a constant step size successfully finds a global minimum for any batch size choice. However, the particular global minimum found depends upon the batch size. In the full-batch setting, we show that the solution is dense (i.e., not sparse) and is highly aligned with its initialized direction, showing that relatively little feature learning occurs. On the other hand, for any batch size strictly smaller than the number of samples, SGD finds a global minimum which is sparse and nearly orthogonal to its initialization, showing that the randomness of stochastic gradients induces a qualitatively different type of "feature selection" in this setting. Moreover, if we measure the sharpness of the minimum by the trace of the Hessian, the minima found with full batch gradient descent are flatter than those found with strictly smaller batch sizes, in contrast to previous works which suggest that large batches lead to sharper minima. To prove convergence of SGD with a constant step size, we introduce a powerful tool from the theory of non-homogeneous random walks which may be of independent interest.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们研究束变数学梯度下降（SGD）在训练单神经 autoencoder 时的动态。我们发现，对于非拟合问题，随机初始化的 SGD  WITH constant step size 能够成功找到全局最小值，但特定的全局最小值与批处理大小有关。在完整批处理Setting中，我们显示解是 dense（即不夹），与其初始方向高度对齐，表明相对较少特征学习发生。相反，任何小于样本数量的批处理大小，SGD 会找到一个全局最小值，这个最小值是稀疏的并且几乎与其初始方向垂直，这表明随机梯度的Randomness 导致了一种不同类型的 "特征选择"。此外，如果我们测量 minimum 的锐度通过 Hessian 的跟踪，则使用 full batch 梯度下降的 minimum 是浅的，而不同于先前的研究，大批处理会导致更锐的 minimum。为证明 SGD  WITH constant step size 的收敛，我们引入了非Homogeneous random walks 的一种有力的工具，这可能是独立的 интерес。
</details></li>
</ul>
<hr>
<h2 id="Average-Hard-Attention-Transformers-are-Constant-Depth-Uniform-Threshold-Circuits"><a href="#Average-Hard-Attention-Transformers-are-Constant-Depth-Uniform-Threshold-Circuits" class="headerlink" title="Average-Hard Attention Transformers are Constant-Depth Uniform Threshold Circuits"></a>Average-Hard Attention Transformers are Constant-Depth Uniform Threshold Circuits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03212">http://arxiv.org/abs/2308.03212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lena Strobl</li>
<li>for: 本研究探讨了Transformers模型在自然语言处理任务中的应用，以及它们与常深度阈值电路之间的关系。</li>
<li>methods: 本研究使用了average-hard attention和log-precision transformers两种模型，并证明它们可以recognizeTC0复杂度类型的语言。</li>
<li>results: 研究结果显示，transformers模型可以被视为常深度阈值电路的延展，并且可以通过生成一个固定深度和尺寸的阈值电路来实现。此外，研究还发现log-precision transformers模型比average-hard attention transformers模型更为稳定和可靠。<details>
<summary>Abstract</summary>
Transformers have emerged as a widely used neural network model for various natural language processing tasks. Previous research explored their relationship with constant-depth threshold circuits, making two assumptions: average-hard attention and logarithmic precision for internal computations relative to input length. Merrill et al. (2022) prove that average-hard attention transformers recognize languages that fall within the complexity class TC0, denoting the set of languages that can be recognized by constant-depth polynomial-size threshold circuits. Likewise, Merrill and Sabharwal (2023) show that log-precision transformers recognize languages within the class of uniform TC0. This shows that both transformer models can be simulated by constant-depth threshold circuits, with the latter being more robust due to generating a uniform circuit family. Our paper shows that the first result can be extended to yield uniform circuits as well.
</details>
<details>
<summary>摘要</summary>
启发器（Transformers）已经成为自然语言处理任务中广泛使用的神经网络模型。前一些研究探讨了它们与常深度阈值电路之间的关系，并假设了两点：均值困难注意力和对内部计算的对数精度相对于输入长度。美利尔等（2022）证明了均值困难注意力启发器可以识别TC0复杂度类型中的语言，这种语言可以被表示为常深度多阶阈值电路。 Similarly,美利尔和沙巴华尔（2023）表明了log精度启发器可以识别uniform TC0复杂度类型中的语言。这示出了两种启发器模型都可以被模拟为常深度阈值电路，但后者更加稳定，因为它生成了一个具有 uniform 性的电路家族。我们的论文显示，第一个结果可以扩展到生成uniformCircuits。
</details></li>
</ul>
<hr>
<h2 id="Time-Parameterized-Convolutional-Neural-Networks-for-Irregularly-Sampled-Time-Series"><a href="#Time-Parameterized-Convolutional-Neural-Networks-for-Irregularly-Sampled-Time-Series" class="headerlink" title="Time-Parameterized Convolutional Neural Networks for Irregularly Sampled Time Series"></a>Time-Parameterized Convolutional Neural Networks for Irregularly Sampled Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03210">http://arxiv.org/abs/2308.03210</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chrysoula Kosma, Giannis Nikolentzos, Michalis Vazirgiannis</li>
<li>for: 处理不规则时间序列数据</li>
<li>methods: 使用时间参数化卷积神经网络（TPCNN），其中卷积核心使用时间explicitly初始化</li>
<li>results: TPCNN在 interpolación 和分类任务中表现竞争力强，同时可以具有更高的效率和可读性。<details>
<summary>Abstract</summary>
Irregularly sampled multivariate time series are ubiquitous in several application domains, leading to sparse, not fully-observed and non-aligned observations across different variables. Standard sequential neural network architectures, such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs), consider regular spacing between observation times, posing significant challenges to irregular time series modeling. While most of the proposed architectures incorporate RNN variants to handle irregular time intervals, convolutional neural networks have not been adequately studied in the irregular sampling setting. In this paper, we parameterize convolutional layers by employing time-explicitly initialized kernels. Such general functions of time enhance the learning process of continuous-time hidden dynamics and can be efficiently incorporated into convolutional kernel weights. We, thus, propose the time-parameterized convolutional neural network (TPCNN), which shares similar properties with vanilla convolutions but is carefully designed for irregularly sampled time series. We evaluate TPCNN on both interpolation and classification tasks involving real-world irregularly sampled multivariate time series datasets. Our experimental results indicate the competitive performance of the proposed TPCNN model which is also significantly more efficient than other state-of-the-art methods. At the same time, the proposed architecture allows the interpretability of the input series by leveraging the combination of learnable time functions that improve the network performance in subsequent tasks and expedite the inaugural application of convolutions in this field.
</details>
<details>
<summary>摘要</summary>
非 régulièrement采样多Variable时间序列是各个应用领域中的普遍存在，导致不同变数之间的观测不充分、不一致。标准的时间序列架构，如回传神经网络（RNN）和卷积神经网络（CNN），假设观测时间的规律性，对不规时间序列模型造成严重的挑战。大多数提出的架构包括RNN的变形，但是尚未充分研究在不规时间序列 Setting中的卷积神经网络。在这篇文章中，我们将时间explicitly启动kernel，以提高不规时间序列中隐藏的连续时间动力学学习过程。这些时间敏感的核心函数可以高效地包含到卷积核心中。因此，我们提出了时间参数化卷积神经网络（TPCNN），它和普通的卷积架构相似，但是特别的设计来应对不规时间序列。我们将TPCNN应用到真实世界的不规时间序列多Variable时间序列数据集上的 interpolating 和分类任务中，实验结果显示TPCNN模型的竞争性表现，同时比其他现有的方法更高效。此外，提案的架构可以允许输入序列的解释性，通过搜寻时间函数来提高网络性能，并且将卷积神经网络引入这个领域。
</details></li>
</ul>
<hr>
<h2 id="Communication-Free-Distributed-GNN-Training-with-Vertex-Cut"><a href="#Communication-Free-Distributed-GNN-Training-with-Vertex-Cut" class="headerlink" title="Communication-Free Distributed GNN Training with Vertex Cut"></a>Communication-Free Distributed GNN Training with Vertex Cut</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03209">http://arxiv.org/abs/2308.03209</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaidi Cao, Rui Deng, Shirley Wu, Edward W Huang, Karthik Subbian, Jure Leskovec</li>
<li>for: 快速训练图 neural network（GNN）在实际世界上的图中，即使是图中的亿个节点和边也是很困难的，主要是因为需要很大的内存来存储图和其中的中间节点和边特征。现有的分布式方法需要频繁的跨GPU交互，导致训练过程中的时间开销很大，并且随着训练进程的扩展，缓慢的扩展。这里，我们介绍了CoFree-GNN，一种新的分布式GNN训练框架，可以快速加速GNN训练过程。</li>
<li>methods: CoFree-GNN使用了一种Vertex Cut分区，即在分区过程中，不是截断图中的边，而是将边分区，并将节点信息复制以保持图结构。此外，框架还包括一种重要机制来处理受损的图分布，以保持模型的准确性。此外，我们还提出了一种改进的DropEdge技术来进一步加速训练过程。</li>
<li>results: 使用了一系列实际网络的实验表明，CoFree-GNN可以比现有的GNN训练方法快速到10倍。<details>
<summary>Abstract</summary>
Training Graph Neural Networks (GNNs) on real-world graphs consisting of billions of nodes and edges is quite challenging, primarily due to the substantial memory needed to store the graph and its intermediate node and edge features, and there is a pressing need to speed up the training process. A common approach to achieve speed up is to divide the graph into many smaller subgraphs, which are then distributed across multiple GPUs in one or more machines and processed in parallel. However, existing distributed methods require frequent and substantial cross-GPU communication, leading to significant time overhead and progressively diminishing scalability. Here, we introduce CoFree-GNN, a novel distributed GNN training framework that significantly speeds up the training process by implementing communication-free training. The framework utilizes a Vertex Cut partitioning, i.e., rather than partitioning the graph by cutting the edges between partitions, the Vertex Cut partitions the edges and duplicates the node information to preserve the graph structure. Furthermore, the framework maintains high model accuracy by incorporating a reweighting mechanism to handle a distorted graph distribution that arises from the duplicated nodes. We also propose a modified DropEdge technique to further speed up the training process. Using an extensive set of experiments on real-world networks, we demonstrate that CoFree-GNN speeds up the GNN training process by up to 10 times over the existing state-of-the-art GNN training approaches.
</details>
<details>
<summary>摘要</summary>
训练图 neural network (GNN) 在实际图中包含数百万个节点和边是很困难的，主要因为需要巨量的内存来存储图和其中的中间节点和边特征。随着图的大小不断增长，训练过程的速度变得非常重要。现有的分布式方法需要频繁的跨GPU通信，导致训练过程中的时间开销很大，并且随着图的尺度的增长，缩放性逐渐减退。我们在这里介绍CoFree-GNN，一种新的分布式GNN训练框架，可以快速加速GNN训练过程。CoFree-GNN使用Vertex Cut分区，而不是将图分割为多个分区，然后在每个分区上进行并发训练。此外，框架还包括一种重量调整机制，以处理由复制节点引起的图分布偏见。我们还提出了一种修改后 DropEdge 技术，以进一步加速训练过程。通过对实际网络进行了广泛的实验，我们表明CoFree-GNN可以在 GNN 训练过程中提高速度，达到10倍以上。
</details></li>
</ul>
<hr>
<h2 id="Microvasculature-Segmentation-in-Human-BioMolecular-Atlas-Program-HuBMAP"><a href="#Microvasculature-Segmentation-in-Human-BioMolecular-Atlas-Program-HuBMAP" class="headerlink" title="Microvasculature Segmentation in Human BioMolecular Atlas Program (HuBMAP)"></a>Microvasculature Segmentation in Human BioMolecular Atlas Program (HuBMAP)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03203">http://arxiv.org/abs/2308.03203</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youssef Sultan, Yongqiang Wang, James Scanlon, Lisa D’lima</li>
<li>for: 研究人员对于人体细胞地图的详细分类，以实现更好的生物医学研究。</li>
<li>methods: 使用各种后门架构，包括FastAI U-Net模型、更深的模型和Feature Pyramid Networks，对2D Periodic Acid-Schiff（PAS）染色的人类肾脏 Histology 图像进行分类。</li>
<li>results: 透过对基eline U-Net模型的评估，发现不同的后门架构具有不同的表现，提供了实验领域中未来研究的 valuable insights。<details>
<summary>Abstract</summary>
Image segmentation serves as a critical tool across a range of applications, encompassing autonomous driving's pedestrian detection and pre-operative tumor delineation in the medical sector. Among these applications, we focus on the National Institutes of Health's (NIH) Human BioMolecular Atlas Program (HuBMAP), a significant initiative aimed at creating detailed cellular maps of the human body. In this study, we concentrate on segmenting various microvascular structures in human kidneys, utilizing 2D Periodic Acid-Schiff (PAS)-stained histology images. Our methodology begins with a foundational FastAI U-Net model, upon which we investigate alternative backbone architectures, delve into deeper models, and experiment with Feature Pyramid Networks. We rigorously evaluate these varied approaches by benchmarking their performance against our baseline U-Net model. This study thus offers a comprehensive exploration of cutting-edge segmentation techniques, providing valuable insights for future research in the field.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:图像分割作为应用程序的重要工具，涵盖自动驾驶中的人体检测和医疗领域中的先驱性肿瘤定位。其中我们专注于国家医疗研究所（NIH）的人生物分子地图计划（HuBMAP），这是一项创造细胞图像的重要 iniciative。在这种研究中，我们集中在人体肾脏中的微血管结构分割中，使用2D Periodic Acid-Schiff（PAS）染色 histology 图像。我们的方法开始于基础 FastAI U-Net 模型，然后我们 investigate 其他脊梁架构、深度模型和特征峰网络。我们严格评估这些不同的方法，对比基准 U-Net 模型的性能。这项研究因此提供了当今图像分割技术的全面探索，为未来研究提供了有价值的洞察。
</details></li>
</ul>
<hr>
<h2 id="Source-free-Domain-Adaptive-Human-Pose-Estimation"><a href="#Source-free-Domain-Adaptive-Human-Pose-Estimation" class="headerlink" title="Source-free Domain Adaptive Human Pose Estimation"></a>Source-free Domain Adaptive Human Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03202">http://arxiv.org/abs/2308.03202</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/davidpengucf/sfdahpe">https://github.com/davidpengucf/sfdahpe</a></li>
<li>paper_authors: Qucheng Peng, Ce Zheng, Chen Chen</li>
<li>for: 实现人姿估测（HPE）中的数据隐私和安全性，以及在实际应用中降低数据成本。</li>
<li>methods: 提出了一个新任务 named source-free domain adaptive HPE，并提出了一个新的框架，包括三个模型：源模型、中介模型和目标模型，从源资料保护和目标资料相关的角度来探索任务。</li>
<li>results: 在多个页面适应HPE的评估标准上，提出的方法比现有方法优化了许多margin。<details>
<summary>Abstract</summary>
Human Pose Estimation (HPE) is widely used in various fields, including motion analysis, healthcare, and virtual reality. However, the great expenses of labeled real-world datasets present a significant challenge for HPE. To overcome this, one approach is to train HPE models on synthetic datasets and then perform domain adaptation (DA) on real-world data. Unfortunately, existing DA methods for HPE neglect data privacy and security by using both source and target data in the adaptation process. To this end, we propose a new task, named source-free domain adaptive HPE, which aims to address the challenges of cross-domain learning of HPE without access to source data during the adaptation process. We further propose a novel framework that consists of three models: source model, intermediate model, and target model, which explores the task from both source-protect and target-relevant perspectives. The source-protect module preserves source information more effectively while resisting noise, and the target-relevant module reduces the sparsity of spatial representations by building a novel spatial probability space, and pose-specific contrastive learning and information maximization are proposed on the basis of this space. Comprehensive experiments on several domain adaptive HPE benchmarks show that the proposed method outperforms existing approaches by a considerable margin. The codes are available at https://github.com/davidpengucf/SFDAHPE.
</details>
<details>
<summary>摘要</summary>
人体姿势估计（HPE）在多个领域得到广泛应用，包括运动分析、医疗和虚拟现实。然而，实际世界数据的高成本成为HPE的一大挑战。为了解决这个问题，一种方法是使用生成的 sintetic 数据进行HPE模型的训练，然后进行适应（DA）操作以适应实际世界数据。然而，现有的DA方法 дляHPE忽视了数据隐私和安全性，因为它们使用了源数据和目标数据在适应过程中。为此，我们提出了一个新的任务，即无源领域适应HPE，以解决跨领域学习HPE的挑战。我们还提出了一个新的框架，该框架包括三个模型：源模型、中间模型和目标模型，该框架从源保护和目标相关两个角度出发，以更好地保持源信息和适应目标数据。源保护模块能更好地保持源信息，并抵抗噪声，而目标相关模块通过构建一个新的空间概率空间，减少了空间表示的稀疏性，并通过基于这个空间的姿势特定的对比学习和信息最大化来提高pose的预测精度。经过了多个领域适应HPE的 benchmk 测试，我们发现我们的方法在与现有方法进行比较时表现出了显著的优势。代码可以在 <https://github.com/davidpengucf/SFDAHPE> 中下载。
</details></li>
</ul>
<hr>
<h2 id="Automatically-Correcting-Large-Language-Models-Surveying-the-landscape-of-diverse-self-correction-strategies"><a href="#Automatically-Correcting-Large-Language-Models-Surveying-the-landscape-of-diverse-self-correction-strategies" class="headerlink" title="Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies"></a>Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03188">http://arxiv.org/abs/2308.03188</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/teacherpeterpan/self-correction-llm-papers">https://github.com/teacherpeterpan/self-correction-llm-papers</a></li>
<li>paper_authors: Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, William Yang Wang</li>
<li>for: 本文旨在探讨自动反馈技术的应用在大语言模型（LLM）上，以改进LLM的表现和可deployability。</li>
<li>methods: 本文分析和概括了各种使用自动反馈技术的最新研究，包括训练时间、生成时间和后续修正。</li>
<li>results: 本文总结了这些技术的主要应用场景，并对未来的发展和挑战进行了讨论。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated remarkable performance across a wide array of NLP tasks. However, their efficacy is undermined by undesired and inconsistent behaviors, including hallucination, unfaithful reasoning, and toxic content. A promising approach to rectify these flaws is self-correction, where the LLM itself is prompted or guided to fix problems in its own output. Techniques leveraging automated feedback -- either produced by the LLM itself or some external system -- are of particular interest as they are a promising way to make LLM-based solutions more practical and deployable with minimal human feedback. This paper presents a comprehensive review of this emerging class of techniques. We analyze and taxonomize a wide array of recent work utilizing these strategies, including training-time, generation-time, and post-hoc correction. We also summarize the major applications of this strategy and conclude by discussing future directions and challenges.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "hallucination" 是指 LLM 生成的 conten 中出现的 fictitious or absurd 内容，例如 "the cat is wearing a hat"。* "unfaithful reasoning" 是指 LLM 的输出中出现的不合理或 absurd 逻辑，例如 "the sky is blue because the grass is green"。* "toxic content" 是指 LLM 生成的 conten 中出现的负面或不适合的内容，例如 hate speech 或 vulgar language。* "self-correction" 是指 LLM 本身的输出中自动检测并修正错误或不合理的部分的技术。* "automated feedback" 是指由 LLM 本身或外部系统生成的自动反馈，用于 Rectify  LL M 的输出中的错误或不合理的部分的技术。
</details></li>
</ul>
<hr>
<h2 id="A-Lightweight-Method-for-Modeling-Confidence-in-Recommendations-with-Learned-Beta-Distributions"><a href="#A-Lightweight-Method-for-Modeling-Confidence-in-Recommendations-with-Learned-Beta-Distributions" class="headerlink" title="A Lightweight Method for Modeling Confidence in Recommendations with Learned Beta Distributions"></a>A Lightweight Method for Modeling Confidence in Recommendations with Learned Beta Distributions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03186">http://arxiv.org/abs/2308.03186</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nkny/confidencerecsys2023">https://github.com/nkny/confidencerecsys2023</a></li>
<li>paper_authors: Norman Knyazev, Harrie Oosterhuis</li>
<li>for: 这个论文旨在提供一种简单实用的推荐方法，并提供一个直观的度量confidence。</li>
<li>methods: 该方法使用学习 beta 分布（LBD）来预测用户喜好，并通过 beta 分布的形式来表示 confidence。</li>
<li>results: 该方法可以维持与现有方法相当的准确性，同时具有与准确性直接相关的confidence度量。此外，在高精度目标推荐任务中，LBD表现更优于现有方法。<details>
<summary>Abstract</summary>
Most Recommender Systems (RecSys) do not provide an indication of confidence in their decisions. Therefore, they do not distinguish between recommendations of which they are certain, and those where they are not. Existing confidence methods for RecSys are either inaccurate heuristics, conceptually complex or computationally very expensive. Consequently, real-world RecSys applications rarely adopt these methods, and thus, provide no confidence insights in their behavior. In this work, we propose learned beta distributions (LBD) as a simple and practical recommendation method with an explicit measure of confidence. Our main insight is that beta distributions predict user preferences as probability distributions that naturally model confidence on a closed interval, yet can be implemented with the minimal model-complexity. Our results show that LBD maintains competitive accuracy to existing methods while also having a significantly stronger correlation between its accuracy and confidence. Furthermore, LBD has higher performance when applied to a high-precision targeted recommendation task. Our work thus shows that confidence in RecSys is possible without sacrificing simplicity or accuracy, and without introducing heavy computational complexity. Thereby, we hope it enables better insight into real-world RecSys and opens the door for novel future applications.
</details>
<details>
<summary>摘要</summary>
大多数推荐系统（RecSys）没有提供决策的信任度标示。因此，它们无法 отлича между确定的推荐和不确定的推荐。现有的信任方法对RecSys是不准确的规则、概念上复杂或计算昂贵。因此，现实世界中的RecSys应用rarely采用这些方法，因此无法提供信任情况的视角。在这项工作中，我们提议使用学习beta分布（LBD）作为简单、实用的推荐方法，并提供显式的信任度标示。我们的主要发现是，beta分布预测用户喜好的概率分布，自然模型信任的关闭区间，但可以实现最小的模型复杂度。我们的结果表明，LBD与现有方法的竞争性准确度相当，同时其信任度与准确度之间具有显著的相关性。此外，LBD在高精度目标推荐任务中表现更高。我们的工作因此表明，RecSys中的信任是可能的，不需要牺牲简单性或准确度，也不需要承受重大的计算复杂度。这样，我们希望能够为实际世界中的RecSys提供更好的视角，并开启未来应用的新可能性。
</details></li>
</ul>
<hr>
<h2 id="A-Critical-Review-of-Physics-Informed-Machine-Learning-Applications-in-Subsurface-Energy-Systems"><a href="#A-Critical-Review-of-Physics-Informed-Machine-Learning-Applications-in-Subsurface-Energy-Systems" class="headerlink" title="A Critical Review of Physics-Informed Machine Learning Applications in Subsurface Energy Systems"></a>A Critical Review of Physics-Informed Machine Learning Applications in Subsurface Energy Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04457">http://arxiv.org/abs/2308.04457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdeldjalil Latrach, Mohamed Lamine Malki, Misael Morales, Mohamed Mehana, Minou Rabiei</li>
<li>For: The paper is written for researchers and practitioners in the field of machine learning, particularly in the application of physics-informed machine learning (PIML) to subsurface energy systems, such as the oil and gas industry.* Methods: The paper reviews and discusses the applications of PIML techniques in various tasks related to subsurface energy systems, including seismic applications, reservoir simulation, hydrocarbons production forecasting, and intelligent decision-making in the exploration and production stages.* Results: The paper highlights the successful utilization of PIML for providing more accurate and reliable predictions for resource management and operational efficiency in the oil and gas industry, and demonstrates its potential for revolutionizing the industry and other emerging areas of interest, such as carbon and hydrogen storage, and geothermal systems.<details>
<summary>Abstract</summary>
Machine learning has emerged as a powerful tool in various fields, including computer vision, natural language processing, and speech recognition. It can unravel hidden patterns within large data sets and reveal unparalleled insights, revolutionizing many industries and disciplines. However, machine and deep learning models lack interpretability and limited domain-specific knowledge, especially in applications such as physics and engineering. Alternatively, physics-informed machine learning (PIML) techniques integrate physics principles into data-driven models. By combining deep learning with domain knowledge, PIML improves the generalization of the model, abidance by the governing physical laws, and interpretability. This paper comprehensively reviews PIML applications related to subsurface energy systems, mainly in the oil and gas industry. The review highlights the successful utilization of PIML for tasks such as seismic applications, reservoir simulation, hydrocarbons production forecasting, and intelligent decision-making in the exploration and production stages. Additionally, it demonstrates PIML's capabilities to revolutionize the oil and gas industry and other emerging areas of interest, such as carbon and hydrogen storage; and geothermal systems by providing more accurate and reliable predictions for resource management and operational efficiency.
</details>
<details>
<summary>摘要</summary>
（注意：以下是简化中文版本）机器学习已经成为不同领域的强大工具，包括计算机视觉、自然语言处理和语音识别。它可以找到大量数据中隐藏的模式，并提供无与伦比的发现，革命化许多行业和学科。然而，机器学习和深度学习模型缺乏解释性和具体领域知识，尤其是在应用物理和工程领域。相反，物理知识整合机器学习（PIML）技术将物理原理 integrate 到数据驱动模型中。通过结合深度学习和领域知识，PIML提高模型的总体化能力，遵循物理法律，并提供解释性。本文全面评论了 relate 到地下能源系统的 PIML 应用，主要是石油和天然气领域。文章强调 PIML 在探测和生产阶段的准确预测、资源管理和运营效率等方面的成功应用。此外，它还 demon стри PIML 可以革命化石油和天然气行业，以及其他emerging 领域，如碳和氢存储，以及地热系统，提供更加准确和可靠的预测，为资源管理和运营效率提供更好的基础。
</details></li>
</ul>
<hr>
<h2 id="Adapting-Machine-Learning-Diagnostic-Models-to-New-Populations-Using-a-Small-Amount-of-Data-Results-from-Clinical-Neuroscience"><a href="#Adapting-Machine-Learning-Diagnostic-Models-to-New-Populations-Using-a-Small-Amount-of-Data-Results-from-Clinical-Neuroscience" class="headerlink" title="Adapting Machine Learning Diagnostic Models to New Populations Using a Small Amount of Data: Results from Clinical Neuroscience"></a>Adapting Machine Learning Diagnostic Models to New Populations Using a Small Amount of Data: Results from Clinical Neuroscience</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03175">http://arxiv.org/abs/2308.03175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rongguang Wang, Guray Erus, Pratik Chaudhari, Christos Davatzikos<br>for:This paper aims to address the problem of reproducibility crisis in machine learning (ML) models applied to neuroimaging data, specifically for the diagnosis of Alzheimer’s disease (AD) and schizophrenia (SZ), and estimation of brain age.methods:The authors propose a weighted empirical risk minimization approach that combines data from a source group (e.g., subjects with similar attributes such as sex, age group, race, and clinical cohort) with a small fraction of data from the target group (e.g., other sex, age group, etc.) to make predictions on the target group.results:The approach achieves substantially better accuracy than existing domain adaptation techniques, with area under curve greater than 0.95 for AD classification, area under curve greater than 0.7 for SZ classification, and mean absolute error less than 5 years for brain age prediction on all target groups. The models also demonstrate utility for prognostic tasks such as predicting disease progression in individuals with mild cognitive impairment, and lead to new clinical insights regarding correlations with neurophysiological tests.Here is the result in Simplified Chinese text:for:这篇论文目标是解决机器学习（ML）模型在神经成像数据中的可重复性危机，特别是对于诊断阿尔ц海默病（AD）和偏头病（SZ），以及脑年龄的估计。methods:作者提议一种加权实际风险最小化方法，将来源组（例如，按照性别、年龄组、种族和临床群组划分的subject）与小型数据集（例如，其他性别、年龄组、等等）结合，以便在目标组（例如，其他性别、年龄组、等等）上进行预测。results:该方法在现有的领域适应技术上取得了显著更好的准确性，其中AD分类 tasks 的准确率大于 0.95，SZ分类 tasks 的准确率大于 0.7，脑年龄预测 tasks 的平均绝对误差小于 5 年，并在所有目标组上具有良好的一致性。此外，模型还能够用于诊断患有轻度认知障碍的病人的疾病进程预测，并且提供了新的临床意义，例如脑physiological 测试的相关性。<details>
<summary>Abstract</summary>
Machine learning (ML) has shown great promise for revolutionizing a number of areas, including healthcare. However, it is also facing a reproducibility crisis, especially in medicine. ML models that are carefully constructed from and evaluated on a training set might not generalize well on data from different patient populations or acquisition instrument settings and protocols. We tackle this problem in the context of neuroimaging of Alzheimer's disease (AD), schizophrenia (SZ) and brain aging. We develop a weighted empirical risk minimization approach that optimally combines data from a source group, e.g., subjects are stratified by attributes such as sex, age group, race and clinical cohort to make predictions on a target group, e.g., other sex, age group, etc. using a small fraction (10%) of data from the target group. We apply this method to multi-source data of 15,363 individuals from 20 neuroimaging studies to build ML models for diagnosis of AD and SZ, and estimation of brain age. We found that this approach achieves substantially better accuracy than existing domain adaptation techniques: it obtains area under curve greater than 0.95 for AD classification, area under curve greater than 0.7 for SZ classification and mean absolute error less than 5 years for brain age prediction on all target groups, achieving robustness to variations of scanners, protocols, and demographic or clinical characteristics. In some cases, it is even better than training on all data from the target group, because it leverages the diversity and size of a larger training set. We also demonstrate the utility of our models for prognostic tasks such as predicting disease progression in individuals with mild cognitive impairment. Critically, our brain age prediction models lead to new clinical insights regarding correlations with neurophysiological tests.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Two-Sides-of-Miscalibration-Identifying-Over-and-Under-Confidence-Prediction-for-Network-Calibration"><a href="#Two-Sides-of-Miscalibration-Identifying-Over-and-Under-Confidence-Prediction-for-Network-Calibration" class="headerlink" title="Two Sides of Miscalibration: Identifying Over and Under-Confidence Prediction for Network Calibration"></a>Two Sides of Miscalibration: Identifying Over and Under-Confidence Prediction for Network Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03172">http://arxiv.org/abs/2308.03172</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aoshuang92/miscalibration_ts">https://github.com/aoshuang92/miscalibration_ts</a></li>
<li>paper_authors: Shuang Ao, Stefan Rueger, Advaith Siddharthan</li>
<li>for: 本研究旨在探讨深度神经网络的准确性calibration问题，以提高安全关键任务中模型的预测可靠性。</li>
<li>methods: 本研究提出了一种新的评估指标——miscalibration score，用于评估模型的总体和分类准确性状态，包括过度自信和下降自信。此外，我们还提出了一种基于分类准确性评估的calibration技术，可以处理过度自信和下降自信问题。</li>
<li>results: 我们的实验结果显示，我们的提出的calibration技术在诸多任务上显著超过了现有的calibration技术。此外，我们还验证了我们的方法在自动故障检测任务中的可靠性和信任性，并发现我们的方法可以提高故障检测和模型的信任性。<details>
<summary>Abstract</summary>
Proper confidence calibration of deep neural networks is essential for reliable predictions in safety-critical tasks. Miscalibration can lead to model over-confidence and/or under-confidence; i.e., the model's confidence in its prediction can be greater or less than the model's accuracy. Recent studies have highlighted the over-confidence issue by introducing calibration techniques and demonstrated success on various tasks. However, miscalibration through under-confidence has not yet to receive much attention. In this paper, we address the necessity of paying attention to the under-confidence issue. We first introduce a novel metric, a miscalibration score, to identify the overall and class-wise calibration status, including being over or under-confident. Our proposed metric reveals the pitfalls of existing calibration techniques, where they often overly calibrate the model and worsen under-confident predictions. Then we utilize the class-wise miscalibration score as a proxy to design a calibration technique that can tackle both over and under-confidence. We report extensive experiments that show our proposed methods substantially outperforming existing calibration techniques. We also validate our proposed calibration technique on an automatic failure detection task with a risk-coverage curve, reporting that our methods improve failure detection as well as trustworthiness of the model. The code are available at \url{https://github.com/AoShuang92/miscalibration_TS}.
</details>
<details>
<summary>摘要</summary>
深度神经网络的准确性calibration是安全关键任务中的一个重要因素。不当calibration可能导致模型过于自信或者不足自信，即模型对其预测的自信度高于或低于模型的准确率。 latest studies have highlighted the over-confidence issue by introducing calibration techniques and have demonstrated success on various tasks. However, the under-confidence issue has not yet received much attention. In this paper, we emphasize the need to pay attention to the under-confidence issue. We first introduce a novel metric, a miscalibration score, to identify the overall and class-wise calibration status, including being over or under-confident. Our proposed metric reveals the pitfalls of existing calibration techniques, where they often overly calibrate the model and worsen under-confident predictions. Then we utilize the class-wise miscalibration score as a proxy to design a calibration technique that can tackle both over and under-confidence. We report extensive experiments that show our proposed methods substantially outperforming existing calibration techniques. We also validate our proposed calibration technique on an automatic failure detection task with a risk-coverage curve, reporting that our methods improve failure detection as well as the trustworthiness of the model. The code is available at \url{https://github.com/AoShuang92/miscalibration_TS}.
</details></li>
</ul>
<hr>
<h2 id="Detection-of-Anomalies-in-Multivariate-Time-Series-Using-Ensemble-Techniques"><a href="#Detection-of-Anomalies-in-Multivariate-Time-Series-Using-Ensemble-Techniques" class="headerlink" title="Detection of Anomalies in Multivariate Time Series Using Ensemble Techniques"></a>Detection of Anomalies in Multivariate Time Series Using Ensemble Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03171">http://arxiv.org/abs/2308.03171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anastasios Iliopoulos, John Violos, Christos Diou, Iraklis Varlamis</li>
<li>for: 本研究旨在提出一种基于深度神经网络的多变量时间序列异常检测方法，以解决多变量时间序列中异常事件罕见性的问题。</li>
<li>methods: 本方法基于LSTM、Autoencoder和Convolutional Autoencoder等深度神经网络模型，并采用特征袋包技术和嵌入式PCA变换以提高模型性能。</li>
<li>results: 对于SKAB数据集，提议的ensemble方法在异常检测精度方面比基本模型提高2%，而在半监督模型中，提议的方法在异常检测精度方面比基本模型提高10%以上。<details>
<summary>Abstract</summary>
Anomaly Detection in multivariate time series is a major problem in many fields. Due to their nature, anomalies sparsely occur in real data, thus making the task of anomaly detection a challenging problem for classification algorithms to solve. Methods that are based on Deep Neural Networks such as LSTM, Autoencoders, Convolutional Autoencoders etc., have shown positive results in such imbalanced data. However, the major challenge that algorithms face when applied to multivariate time series is that the anomaly can arise from a small subset of the feature set. To boost the performance of these base models, we propose a feature-bagging technique that considers only a subset of features at a time, and we further apply a transformation that is based on nested rotation computed from Principal Component Analysis (PCA) to improve the effectiveness and generalization of the approach. To further enhance the prediction performance, we propose an ensemble technique that combines multiple base models toward the final decision. In addition, a semi-supervised approach using a Logistic Regressor to combine the base models' outputs is proposed. The proposed methodology is applied to the Skoltech Anomaly Benchmark (SKAB) dataset, which contains time series data related to the flow of water in a closed circuit, and the experimental results show that the proposed ensemble technique outperforms the basic algorithms. More specifically, the performance improvement in terms of anomaly detection accuracy reaches 2% for the unsupervised and at least 10% for the semi-supervised models.
</details>
<details>
<summary>摘要</summary>
异常检测在多变量时间序列中是许多领域的主要问题。由于异常事件罕见发生，因此将异常检测作为分类算法解决的问题是一项挑战。基于深度神经网络的方法，如LSTM、Autoencoder和Convolutional Autoencoder等，在这种不均衡数据中表现了积极的结果。然而，在多变量时间序列中，异常可能来自一个小集合特征。为了提高基本模型的性能，我们提议一种特征袋装技术，该技术仅考虑一部分特征，并应用基于主成分分析（PCA）计算的嵌入式旋转变换来提高效果和泛化性。此外，我们还提议一种 ensemble 技术，该技术将多个基本模型的输出合并到最终决策中。此外，我们还提出了一种半监督方法，该方法使用Logistic Regressor将基本模型的输出合并到最终决策中。我们对 Skoltech Anomaly Benchmark（SKAB）数据集进行实验，该数据集包含关于水流在封闭环流中的时间序列数据，实验结果表明，我们的ensemble技术在异常检测精度方面比基本算法提高了2%（非监督）和至少10%（半监督）。
</details></li>
</ul>
<hr>
<h2 id="FireFly-A-Synthetic-Dataset-for-Ember-Detection-in-Wildfire"><a href="#FireFly-A-Synthetic-Dataset-for-Ember-Detection-in-Wildfire" class="headerlink" title="FireFly A Synthetic Dataset for Ember Detection in Wildfire"></a>FireFly A Synthetic Dataset for Ember Detection in Wildfire</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03164">http://arxiv.org/abs/2308.03164</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ergowho/firefly2.0">https://github.com/ergowho/firefly2.0</a></li>
<li>paper_authors: Yue Hu, Xinan Ye, Yifei Liu, Souvik Kundu, Gourav Datta, Srikar Mutnuri, Namo Asavisanu, Nora Ayanian, Konstantinos Psounis, Peter Beerel</li>
<li>for: 本研究旨在提供一个用于烟火检测的人工数据集，以替代当前烟火特有训练资源的缺乏。</li>
<li>methods: 本研究使用Unreal Engine 4 (UE4)创建了一个名为”FireFly”的人工数据集，并提供了一个自动生成标注数据集的工具，以实现数据多样性和用户需求的定制。</li>
<li>results: 对于四种流行的物体检测模型，使用FireFly数据集进行评估，得到了8.57%的提升 Mean Average Precision (mAP) 在实际野外烟火场景中。<details>
<summary>Abstract</summary>
This paper presents "FireFly", a synthetic dataset for ember detection created using Unreal Engine 4 (UE4), designed to overcome the current lack of ember-specific training resources. To create the dataset, we present a tool that allows the automated generation of the synthetic labeled dataset with adjustable parameters, enabling data diversity from various environmental conditions, making the dataset both diverse and customizable based on user requirements. We generated a total of 19,273 frames that have been used to evaluate FireFly on four popular object detection models. Further to minimize human intervention, we leveraged a trained model to create a semi-automatic labeling process for real-life ember frames. Moreover, we demonstrated an up to 8.57% improvement in mean Average Precision (mAP) in real-world wildfire scenarios compared to models trained exclusively on a small real dataset.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/07/cs.LG_2023_08_07/" data-id="clly4xtds006bvl88albv6srt" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/07/cs.SD_2023_08_07/" class="article-date">
  <time datetime="2023-08-06T16:00:00.000Z" itemprop="datePublished">2023-08-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/07/cs.SD_2023_08_07/">cs.SD - 2023-08-07 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Active-Noise-Control-based-on-the-Momentum-Multichannel-Normalized-Filtered-x-Least-Mean-Square-Algorithm"><a href="#Active-Noise-Control-based-on-the-Momentum-Multichannel-Normalized-Filtered-x-Least-Mean-Square-Algorithm" class="headerlink" title="Active Noise Control based on the Momentum Multichannel Normalized Filtered-x Least Mean Square Algorithm"></a>Active Noise Control based on the Momentum Multichannel Normalized Filtered-x Least Mean Square Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03684">http://arxiv.org/abs/2308.03684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongyuan Shi, Woon-Seng Gan, Bhan Lam, Shulin Wen, Xiaoyi Shen</li>
<li>for: 实现多通道活动噪声控制（MCANC）中的广泛噪声抑制区域。</li>
<li>methods: 使用了Filter-x最小均方（FxLMS）算法，但是它的对应速度较慢，不适合处理快速变化的噪声，如堆积噪声。此外，噪声功率的变化也会对算法的稳定性产生负面影响。</li>
<li>results: 通过与振踪方法结合，实现了对MCANC中的噪声控制的有效控制，并且加速了算法的步进调整。在实际应用中，通过使用多通道噪声控制窗口来控制机器噪声。<details>
<summary>Abstract</summary>
Multichannel active noise control (MCANC) is widely utilized to achieve significant noise cancellation area in the complicated acoustic field. Meanwhile, the filter-x least mean square (FxLMS) algorithm gradually becomes the benchmark solution for the implementation of MCANC due to its low computational complexity. However, its slow convergence speed more or less undermines the performance of dealing with quickly varying disturbances, such as piling noise. Furthermore, the noise power variation also deteriorates the robustness of the algorithm when it adopts the fixed step size. To solve these issues, we integrated the normalized multichannel FxLMS with the momentum method, which hence, effectively avoids the interference of the primary noise power and accelerates the convergence of the algorithm. To validate its effectiveness, we deployed this algorithm in a multichannel noise control window to control the real machine noise.
</details>
<details>
<summary>摘要</summary>
多通道活动噪声控制（MCANC）广泛应用于复杂的噪声场中实现显著的噪声抑制面积。同时，Filter-x最小二乘（FxLMS）算法逐渐成为MCANC的实现标准方案，主要是因为它的计算复杂度较低。然而，它的慢速对应变化的干扰有很大的影响，如堆叠噪声。此外，噪声功率变化也会对算法的稳定性产生负面影响，特别是当采用固定步长时。为解决这些问题，我们将normalized multichannel FxLMS与旋转方法相结合，从而有效避免了主要噪声功率的干扰，并加速了算法的收敛速度。为验证其效果，我们在多通道噪声控制窗口中应用了这种算法，控制了实际机器的噪声。
</details></li>
</ul>
<hr>
<h2 id="AudioVMAF-Audio-Quality-Prediction-with-VMAF"><a href="#AudioVMAF-Audio-Quality-Prediction-with-VMAF" class="headerlink" title="AudioVMAF: Audio Quality Prediction with VMAF"></a>AudioVMAF: Audio Quality Prediction with VMAF</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03437">http://arxiv.org/abs/2308.03437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arijit Biswas, Harald Mundt</li>
<li>for: 这个论文的目的是提出一种基于现有VMAF的听力 inspirited frontend，用于创建参考视频和编码spectrograms，并扩展VMAF以测试编码音质。</li>
<li>methods: 该系统使用了图像复制来进一步提高预测精度，特别是在存在带限 anchors 时。</li>
<li>results: 提议方法在现有视觉质量特征的抽取改进下显著 OUTPERFORMS 所有现有的视觉质量特征重新定义为音频质量特征，并且在一个专门为音频质量metric（ViSQOL-v3 [4]）也 inspirited from the image domain 上显示了7.8%和2.0%的Pearson和Spearman排名相关度系数的显著提高。<details>
<summary>Abstract</summary>
Video Multimethod Assessment Fusion (VMAF) [1], [2], [3] is a popular tool in the industry for measuring coded video quality. In this study, we propose an auditory-inspired frontend in existing VMAF for creating videos of reference and coded spectrograms, and extended VMAF for measuring coded audio quality. We name our system AudioVMAF. We demonstrate that image replication is capable of further enhancing prediction accuracy, especially when band-limited anchors are present. The proposed method significantly outperforms all existing visual quality features repurposed for audio, and even demonstrates a significant overall improvement of 7.8% and 2.0% of Pearson and Spearman rank correlation coefficient, respectively, over a dedicated audio quality metric (ViSQOL-v3 [4]) also inspired from the image domain.
</details>
<details>
<summary>摘要</summary>
видео多方法评估融合（VMAF）[1], [2], [3] 是行业中常用的视频质量测试工具。在这项研究中，我们提议在现有VMAF中添加音频引入的前端，并将扩展VMAF用于测试编码音频质量。我们称之为AudioVMAF。我们发现，图像复制可以进一步提高预测精度，特别是当存在带限 anchors 时。我们的方法在现有视觉质量特征的抽取方面进行了改进，并且显著超过了所有抽取于音频领域的视觉质量特征，以及专门为音频质量指标（ViSQOL-v3 [4]) 的7.8%和2.0%的普森和斯宾塞相关系数。
</details></li>
</ul>
<hr>
<h2 id="Improving-Deep-Attractor-Network-by-BGRU-and-GMM-for-Speech-Separation"><a href="#Improving-Deep-Attractor-Network-by-BGRU-and-GMM-for-Speech-Separation" class="headerlink" title="Improving Deep Attractor Network by BGRU and GMM for Speech Separation"></a>Improving Deep Attractor Network by BGRU and GMM for Speech Separation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03332">http://arxiv.org/abs/2308.03332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rawad Melhem, Assef Jafar, Riad Hamadeh</li>
<li>for: 提高Speech separation技术的简化和效率，使其更适合实际应用。</li>
<li>methods: 使用Bidirectional Gated neural network (BGRU)取代BLSTM，并使用Gaussian Mixture Model (GMM)作为聚类算法。</li>
<li>results: 在TIMIT corpus上评估系统，SDR和PESQ scores分别为12.3 dB和2.94，比原始DANet模型更好，同时减少了20.7%和17.9%的参数和训练时间。<details>
<summary>Abstract</summary>
Deep Attractor Network (DANet) is the state-of-the-art technique in speech separation field, which uses Bidirectional Long Short-Term Memory (BLSTM), but the complexity of the DANet model is very high. In this paper, a simplified and powerful DANet model is proposed using Bidirectional Gated neural network (BGRU) instead of BLSTM. The Gaussian Mixture Model (GMM) other than the k-means was applied in DANet as a clustering algorithm to reduce the complexity and increase the learning speed and accuracy. The metrics used in this paper are Signal to Distortion Ratio (SDR), Signal to Interference Ratio (SIR), Signal to Artifact Ratio (SAR), and Perceptual Evaluation Speech Quality (PESQ) score. Two speaker mixture datasets from TIMIT corpus were prepared to evaluate the proposed model, and the system achieved 12.3 dB and 2.94 for SDR and PESQ scores respectively, which were better than the original DANet model. Other improvements were 20.7% and 17.9% in the number of parameters and time training, respectively. The model was applied on mixed Arabic speech signals and the results were better than that in English.
</details>
<details>
<summary>摘要</summary>
深度吸引网络（DANet）是语音分离领域的Current Technique，使用双向长短期记忆（BLSTM），但模型复杂度很高。本文提出了简化了的DANet模型，使用双向闭合神经网络（BGRU）而不是BLSTM。在DANet中使用 Gaussian Mixture Model（GMM）作为聚类算法，以降低复杂度并提高学习速度和准确性。用于评价模型的度量包括Signal to Distortion Ratio（SDR）、Signal to Interference Ratio（SIR）、Signal to Artifact Ratio（SAR）以及Perceptual Evaluation Speech Quality（PESQ）分数。使用TIMIT corpus中的两个说话者混合数据集评估提出的模型，系统实现了12.3 dB和2.94的SDR和PESQ分数，分别比原始DANet模型更好。此外，模型的参数数量和训练时间都下降了20.7%和17.9%。模型应用于混合阿拉伯语音信号上，结果比英语更好。
</details></li>
</ul>
<hr>
<h2 id="SeACo-Paraformer-A-Non-Autoregressive-ASR-System-with-Flexible-and-Effective-Hotword-Customization-Ability"><a href="#SeACo-Paraformer-A-Non-Autoregressive-ASR-System-with-Flexible-and-Effective-Hotword-Customization-Ability" class="headerlink" title="SeACo-Paraformer: A Non-Autoregressive ASR System with Flexible and Effective Hotword Customization Ability"></a>SeACo-Paraformer: A Non-Autoregressive ASR System with Flexible and Effective Hotword Customization Ability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03266">http://arxiv.org/abs/2308.03266</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/r1ckshi/seaco-paraformer">https://github.com/r1ckshi/seaco-paraformer</a></li>
<li>paper_authors: Xian Shi, Yexin Yang, Zerui Li, Shiliang Zhang</li>
<li>for: 实现ASR系统中的热词自定义，提高ASR系统的准确性和效率。</li>
<li>methods: 提出了Semantic-augmented Contextual-Paraformer（SeACo-Paraformer）模型，结合了AED-based模型的精度、NAR模型的效率，并且具有出色的Contextualization能力。</li>
<li>results: 在50,000小时的工业大数据实验中，提出的模型比强基准模型在自定义和一般ASR任务中表现更好，并且探索了一种高效的大规模来处理热词检查的方法。<details>
<summary>Abstract</summary>
Hotword customization is one of the important issues remained in ASR field - it is of value to enable users of ASR systems to customize names of entities, persons and other phrases. The past few years have seen both implicit and explicit modeling strategies for ASR contextualization developed. While these approaches have performed adequately, they still exhibit certain shortcomings such as instability in effectiveness. In this paper we propose Semantic-augmented Contextual-Paraformer (SeACo-Paraformer) a novel NAR based ASR system with flexible and effective hotword customization ability. It combines the accuracy of the AED-based model, the efficiency of the NAR model, and the excellent performance in contextualization. In 50,000 hours industrial big data experiments, our proposed model outperforms strong baselines in customization and general ASR tasks. Besides, we explore an efficient way to filter large scale incoming hotwords for further improvement. The source codes and industrial models proposed and compared are all opened as well as two hotword test sets.
</details>
<details>
<summary>摘要</summary>
<SYS>    <LANGUAGE_MODEL>        <NAME>Simplified Chinese</NAME>        <PATH>/path/to/model</PATH>    </LANGUAGE_MODEL></SYS>这是一个 ASR 领域中的重要问题 - 允许使用者自定义名词、人名和其他短语。过去几年，有内在和外在模型化策略被开发出来解决这个问题。这些方法优秀地表现，但仍然存在一些缺陷，如效果不稳定。在这篇文章中，我们提出 Semantic-augmented Contextual-Paraformer (SeACo-Paraformer) 一种新的 NAR 基于 ASR 系统，具有轻松实现和有效的自定义热词能力。它结合了 AED-based 模型的精度和 NAR 模型的效率，并且在Contextualization 方面表现出色。在50,000小时的工业大数据中，我们的提案模型比强大的基准模型在自定义和一般 ASR 任务中表现出色。此外，我们还探索了一种高效的方法来筛选大规模的进来热词，以进一步提高效能。我们提供了所有的代码和工业模型，以及两个热词测试集。
</details></li>
</ul>
<hr>
<h2 id="Investigation-of-Self-supervised-Pre-trained-Models-for-Classification-of-Voice-Quality-from-Speech-and-Neck-Surface-Accelerometer-Signals"><a href="#Investigation-of-Self-supervised-Pre-trained-Models-for-Classification-of-Voice-Quality-from-Speech-and-Neck-Surface-Accelerometer-Signals" class="headerlink" title="Investigation of Self-supervised Pre-trained Models for Classification of Voice Quality from Speech and Neck Surface Accelerometer Signals"></a>Investigation of Self-supervised Pre-trained Models for Classification of Voice Quality from Speech and Neck Surface Accelerometer Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03226">http://arxiv.org/abs/2308.03226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sudarsana Reddy Kadiri, Farhad Javanmardi, Paavo Alku</li>
<li>for: 本研究旨在 investigate the effectiveness of simultaneously-recorded speech and neck surface accelerometer (NSA) signals in the classification of voice quality (breathy, modal, and pressed) using deep learning-based features and a support vector machine (SVM) as well as a convolutional neural network (CNN) as classifiers.</li>
<li>methods: 本研究使用了三种自动学习模型（wav2vec2-BASE、wav2vec2-LARGE和HuBERT）的自然语言处理特征，以及SVM和CNN分类器。另外，使用了两种信号处理方法（闭合相位滤波和零频滤波）来估计颤腔源波形从语音和NSA信号中提取的特征。</li>
<li>results: 研究发现NSA输入对分类任务的性能更高于语音输入。另外，使用自动学习模型生成的特征对于语音和NSA输入都显示了更高的分类精度，而使用HuBERT特征则表现更好。<details>
<summary>Abstract</summary>
Prior studies in the automatic classification of voice quality have mainly studied the use of the acoustic speech signal as input. Recently, a few studies have been carried out by jointly using both speech and neck surface accelerometer (NSA) signals as inputs, and by extracting MFCCs and glottal source features. This study examines simultaneously-recorded speech and NSA signals in the classification of voice quality (breathy, modal, and pressed) using features derived from three self-supervised pre-trained models (wav2vec2-BASE, wav2vec2-LARGE, and HuBERT) and using a SVM as well as CNNs as classifiers. Furthermore, the effectiveness of the pre-trained models is compared in feature extraction between glottal source waveforms and raw signal waveforms for both speech and NSA inputs. Using two signal processing methods (quasi-closed phase (QCP) glottal inverse filtering and zero frequency filtering (ZFF)), glottal source waveforms are estimated from both speech and NSA signals. The study has three main goals: (1) to study whether features derived from pre-trained models improve classification accuracy compared to conventional features (spectrogram, mel-spectrogram, MFCCs, i-vector, and x-vector), (2) to investigate which of the two modalities (speech vs. NSA) is more effective in the classification task with pre-trained model-based features, and (3) to evaluate whether the deep learning-based CNN classifier can enhance the classification accuracy in comparison to the SVM classifier. The results revealed that the use of the NSA input showed better classification performance compared to the speech signal. Between the features, the pre-trained model-based features showed better classification accuracies, both for speech and NSA inputs compared to the conventional features. It was also found that the HuBERT features performed better than the wav2vec2-BASE and wav2vec2-LARGE features.
</details>
<details>
<summary>摘要</summary>
前研究主要是使用语音信号作为输入，做自动识别声音质量的研究。近年来，一些研究开始将语音信号和脖子表面加速器（NSA）信号同时录制，并提取MFCCs和喉咙源特征。本研究用三个自我超vised模型（wav2vec2-BASE、wav2vec2-LARGE和HuBERT）提取特征，并使用支持向量机（SVM）和卷积神经网络（CNN）作为分类器。此外，对于语音和NSA输入，对喉咙源波形和原始信号波形进行预处理，并使用 quasi-closed phase（QCP）预测和零频 filtering（ZFF）来估算喉咙源波形。研究的三个主要目标是：（1）研究 Whether features derived from pre-trained models improve classification accuracy compared to conventional features（spectrogram、mel-spectrogram、MFCCs、i-vector、x-vector），（2）investigate which modality（speech vs. NSA）is more effective in the classification task with pre-trained model-based features，（3）evaluate whether the deep learning-based CNN classifier can enhance the classification accuracy in comparison to the SVM classifier。结果表明，使用NSA输入的 classification 性能比语音信号更好。此外，使用 pre-trained model-based features 也比使用传统特征更好，同时 HuBERT 特征也比 wav2vec2-BASE 和 wav2vec2-LARGE 特征更好。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/07/cs.SD_2023_08_07/" data-id="clly4xten009ovl882qobb2ks" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/07/eess.IV_2023_08_07/" class="article-date">
  <time datetime="2023-08-06T16:00:00.000Z" itemprop="datePublished">2023-08-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/07/eess.IV_2023_08_07/">eess.IV - 2023-08-07 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="SoilNet-An-Attention-based-Spatio-temporal-Deep-Learning-Framework-for-Soil-Organic-Carbon-Prediction-with-Digital-Soil-Mapping-in-Europe"><a href="#SoilNet-An-Attention-based-Spatio-temporal-Deep-Learning-Framework-for-Soil-Organic-Carbon-Prediction-with-Digital-Soil-Mapping-in-Europe" class="headerlink" title="SoilNet: An Attention-based Spatio-temporal Deep Learning Framework for Soil Organic Carbon Prediction with Digital Soil Mapping in Europe"></a>SoilNet: An Attention-based Spatio-temporal Deep Learning Framework for Soil Organic Carbon Prediction with Digital Soil Mapping in Europe</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03586">http://arxiv.org/abs/2308.03586</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nafiseh Kakhani, Moien Rangzan, Ali Jamali, Sara Attarchi, Seyed Kazem Alavipanah, Thomas Scholten</li>
<li>for: 这项研究旨在提高数字土壤地图（DSM）技术，尤其是在使用深度学习（DL）方法来预测土壤有机碳（SOC）的空间特征。</li>
<li>methods: 该研究提出了一种新的架构， combining 基于卷积神经网络（CNN）的空间信息和基于长短时间记忆（LSTM）网络的气候时序信息，以预测欧洲各地的SOC。该模型使用了一组全面的环境特征，包括兰达特-8图像、地形、远程感知指数和气候时序序列，作为输入特征。</li>
<li>results: 研究结果表明，提出的方框比常用的多项Random Forest方法（ML）更高效，具有较低的根圆平方误差（RMSE）。这种模型是一种可靠的SOC预测工具，可以应用于其他土壤特征预测，从而为土地管理和决策过程提供更准确的信息。<details>
<summary>Abstract</summary>
Digital soil mapping (DSM) is an advanced approach that integrates statistical modeling and cutting-edge technologies, including machine learning (ML) methods, to accurately depict soil properties and their spatial distribution. Soil organic carbon (SOC) is a crucial soil attribute providing valuable insights into soil health, nutrient cycling, greenhouse gas emissions, and overall ecosystem productivity. This study highlights the significance of spatial-temporal deep learning (DL) techniques within the DSM framework. A novel architecture is proposed, incorporating spatial information using a base convolutional neural network (CNN) model and spatial attention mechanism, along with climate temporal information using a long short-term memory (LSTM) network, for SOC prediction across Europe. The model utilizes a comprehensive set of environmental features, including Landsat-8 images, topography, remote sensing indices, and climate time series, as input features. Results demonstrate that the proposed framework outperforms conventional ML approaches like random forest commonly used in DSM, yielding lower root mean square error (RMSE). This model is a robust tool for predicting SOC and could be applied to other soil properties, thereby contributing to the advancement of DSM techniques and facilitating land management and decision-making processes based on accurate information.
</details>
<details>
<summary>摘要</summary>
数字土壤地图（DSM）是一种先进的方法，它将统计模型和前沿技术，包括机器学习（ML）方法，结合起来准确地描述土壤属性和其空间分布。土壤有机碳（SOC）是一个重要的土壤特征，它提供了valuable insights into soil health, nutrient cycling, greenhouse gas emissions, and overall ecosystem productivity.这项研究强调了在 DSM 框架中使用空间-时间深度学习（DL）技术的重要性。该研究提出了一种新的建议，其包括在基于 convolutional neural network（CNN）模型和空间注意机制的基础上，以及使用 long short-term memory（LSTM）网络来预测欧洲各地的 SOC。该模型使用了包括 Landsat-8 图像、地形、 remote sensing 指标和气候时序序列在内的全面的环境特征作为输入特征。结果表明，提议的框架在与常见的 ML 方法如随机森林相比，具有较低的根圆平均误差（RMSE）。这种模型是一种准确预测 SOC 的工具，可以应用于其他土壤属性，从而为 DSM 技术的发展和土地管理决策提供支持。
</details></li>
</ul>
<hr>
<h2 id="Quantitative-MR-Image-Reconstruction-using-Parameter-Specific-Dictionary-Learning-with-Adaptive-Dictionary-Size-and-Sparsity-Level-Choice"><a href="#Quantitative-MR-Image-Reconstruction-using-Parameter-Specific-Dictionary-Learning-with-Adaptive-Dictionary-Size-and-Sparsity-Level-Choice" class="headerlink" title="Quantitative MR Image Reconstruction using Parameter-Specific Dictionary Learning with Adaptive Dictionary-Size and Sparsity-Level Choice"></a>Quantitative MR Image Reconstruction using Parameter-Specific Dictionary Learning with Adaptive Dictionary-Size and Sparsity-Level Choice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03460">http://arxiv.org/abs/2308.03460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andreas Kofler, Kirsten Miriam Kerkering, Laura Göschel, Ariane Fillmer, Cristoph Kolbitsch</li>
<li>for: 这种方法是用于量子磁共振成像（QMRI）中的参数地图重建的。</li>
<li>methods: 方法使用了字典学习（DL）和稀疏编码（SC）算法自动计算最佳字典大小和稀疏程度，并在不同参数地图之间进行自适应调整。</li>
<li>results: 该方法在RMSE和PSNR方面胜过MAP、TV、Wl和Sh方法，并且与DL+Fit方法具有相似或更好的效果，同时加速了重建过程约7倍。In English, this means:</li>
<li>for: This method is proposed for the reconstruction of parameter maps in Quantitative Magnetic Resonance Imaging (QMRI).</li>
<li>methods: The method uses dictionary learning (DL) and sparse coding (SC) algorithms to automatically estimate the optimal dictionary size and sparsity level for each parameter map, and adaptively adjusts the parameters between different maps.</li>
<li>results: The proposed method outperforms the compared methods (MAP, TV, Wl, and Sh) in terms of RMSE and PSNR, and has similar or better effects as the DL+Fit method, while significantly accelerating the reconstruction process by a factor of approximately seven.<details>
<summary>Abstract</summary>
Objective: We propose a method for the reconstruction of parameter-maps in Quantitative Magnetic Resonance Imaging (QMRI).   Methods: Because different quantitative parameter-maps differ from each other in terms of local features, we propose a method where the employed dictionary learning (DL) and sparse coding (SC) algorithms automatically estimate the optimal dictionary-size and sparsity level separately for each parameter-map. We evaluated the method on a $T_1$-mapping QMRI problem in the brain using the BrainWeb data as well as in-vivo brain images acquired on an ultra-high field 7T scanner. We compared it to a model-based acceleration for parameter mapping (MAP) approach, other sparsity-based methods using total variation (TV), Wavelets (Wl) and Shearlets (Sh), and to a method which uses DL and SC to reconstruct qualitative images, followed by a non-linear (DL+Fit).   Results: Our algorithm surpasses MAP, TV, Wl and Sh in terms of RMSE and PSNR. It yields better or comparable results to DL+Fit by additionally significantly accelerating the reconstruction by a factor of approximately seven.   Conclusion: The proposed method outperforms the reported methods of comparison and yields accurate $T_1$-maps. Although presented for $T_1$-mapping in the brain, our method's structure is general and thus most probably also applicable for the the reconstruction of other quantitative parameters in other organs.   Significance: From a clinical perspective, the obtained $T_1$-maps could be utilized to differentiate between healthy subjects and patients with Alzheimer's disease. From a technical perspective, the proposed unsupervised method could be employed to obtain ground-truth data for the development of data-driven methods based on supervised learning.+
</details>
<details>
<summary>摘要</summary>
Methods: Different quantitative parameter-maps have unique local features, so we use dictionary learning (DL) and sparse coding (SC) algorithms to automatically estimate the optimal dictionary size and sparsity level for each parameter-map. We evaluated our method on a $T_1$-mapping QMRI problem in the brain using the BrainWeb dataset and in-vivo brain images acquired on a 7T scanner. We compared our method to a model-based acceleration for parameter mapping (MAP) approach, as well as other sparsity-based methods using total variation (TV), wavelets (Wl), and shearlets (Sh). We also compared our method to a method that uses DL and SC to reconstruct qualitative images and then uses non-linear registration (DL+Fit).Results: Our method outperformed MAP, TV, Wl, and Sh in terms of root mean squared error (RMSE) and peak signal-to-noise ratio (PSNR). It also yielded better or comparable results to DL+Fit, while significantly accelerating the reconstruction process by a factor of approximately seven.Conclusion: Our proposed method outperforms previous methods and provides accurate $T_1$-maps. Although we focused on $T_1$-mapping in the brain, our method's structure is general and can be applied to the reconstruction of other quantitative parameters in other organs.Significance: From a clinical perspective, the obtained $T_1$-maps could be used to differentiate between healthy subjects and patients with Alzheimer's disease. From a technical perspective, the proposed unsupervised method could be used to obtain ground-truth data for the development of data-driven methods based on supervised learning.
</details></li>
</ul>
<hr>
<h2 id="Lighting-Every-Darkness-in-Two-Pairs-A-Calibration-Free-Pipeline-for-RAW-Denoising"><a href="#Lighting-Every-Darkness-in-Two-Pairs-A-Calibration-Free-Pipeline-for-RAW-Denoising" class="headerlink" title="Lighting Every Darkness in Two Pairs: A Calibration-Free Pipeline for RAW Denoising"></a>Lighting Every Darkness in Two Pairs: A Calibration-Free Pipeline for RAW Denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03448">http://arxiv.org/abs/2308.03448</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/srameo/led">https://github.com/srameo/led</a></li>
<li>paper_authors: Xin Jin, Jia-Wen Xiao, Ling-Hao Han, Chunle Guo, Ruixun Zhang, Xialei Liu, Chongyi Li</li>
<li>For: This paper is written for RAW image denoising under extremely low-light environments, and it aims to overcome the limitations of calibration-based methods.* Methods: The proposed method uses a calibration-free pipeline, which adapts to a target camera with few-shot paired data and fine-tuning. The method also includes well-designed structural modification during both stages to alleviate the domain gap between synthetic and real noise.* Results: The proposed method achieves superior performance over other calibration-based methods with 2 pairs for each additional digital gain (in total 6 pairs) and 0.5% iterations.Here’s the format you requested:* For: 这篇论文是为了对极低光环境下的 RAW 图像推断而写的，并且想要超越传统的测试基于方法。* Methods: 提案的方法使用了没有单位的管道，可以适应目标摄像头只需要几对对照数据和微调。这个方法还包括了妥善的结构修改在两个阶段，以解决实际和 sintetic 噪声之间的领域差。* Results: 提案的方法在对其他传统基于测试方法进行比较时，获得了更好的性能，仅需要2对对照数据和0.5%迭代。<details>
<summary>Abstract</summary>
Calibration-based methods have dominated RAW image denoising under extremely low-light environments. However, these methods suffer from several main deficiencies: 1) the calibration procedure is laborious and time-consuming, 2) denoisers for different cameras are difficult to transfer, and 3) the discrepancy between synthetic noise and real noise is enlarged by high digital gain. To overcome the above shortcomings, we propose a calibration-free pipeline for Lighting Every Drakness (LED), regardless of the digital gain or camera sensor. Instead of calibrating the noise parameters and training repeatedly, our method could adapt to a target camera only with few-shot paired data and fine-tuning. In addition, well-designed structural modification during both stages alleviates the domain gap between synthetic and real noise without any extra computational cost. With 2 pairs for each additional digital gain (in total 6 pairs) and 0.5% iterations, our method achieves superior performance over other calibration-based methods. Our code is available at https://github.com/Srameo/LED .
</details>
<details>
<summary>摘要</summary>
几种基于准确的方法在极低照度环境下进行RAW图像降噪已经占据了主导地位。然而，这些方法受到以下主要缺点的影响：1）准确程度很低，2）适用于不同摄像头的降噪器难以传输，3）高度数字增量使得实际噪声与synthetic噪声之间的差异变大。为了超越这些缺点，我们提出了不需要准确程度的渠道，即Lighting Every Drakness（LED）。相比准确程度的准备和重复训练，我们的方法只需要几次对应的配对数据和微调就能适应目标摄像头。此外，我们在两个阶段中设计了 estructural modification，以alleviate the domain gap between synthetic and real noise without any extra computational cost。通过使用6对每个额外数字增量（共计24对）和0.5%的迭代，我们的方法可以在其他准确基于方法之上达到更高的性能。我们的代码可以在https://github.com/Srameo/LED上找到。
</details></li>
</ul>
<hr>
<h2 id="Energy-Guided-Diffusion-Model-for-CBCT-to-CT-Synthesis"><a href="#Energy-Guided-Diffusion-Model-for-CBCT-to-CT-Synthesis" class="headerlink" title="Energy-Guided Diffusion Model for CBCT-to-CT Synthesis"></a>Energy-Guided Diffusion Model for CBCT-to-CT Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03354">http://arxiv.org/abs/2308.03354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linjie Fu, Xia Li, Xiuding Cai, Dong Miao, Yu Yao, Yali Shen</li>
<li>for: 提高CBCT图像质量和Hounsfield单位精度，以便更好地用于放射治疗。</li>
<li>methods: 基于能量导向扩散模型（EGDiff），从CBCT图像生成Synthetic CT（sCT）。</li>
<li>results: 对胸腺癌数据集进行实验，得到了优秀的性能结果，包括平均绝对错误26.87±6.14HU、结构相似度指标0.850±0.03、峰值信号噪声比19.83±1.39dB和正常化交叉相关指标0.874±0.04。这些结果表明，我们的方法在精度和视觉质量方面都有所提高，生成了superior的sCT图像。<details>
<summary>Abstract</summary>
Cone Beam CT (CBCT) plays a crucial role in Adaptive Radiation Therapy (ART) by accurately providing radiation treatment when organ anatomy changes occur. However, CBCT images suffer from scatter noise and artifacts, making relying solely on CBCT for precise dose calculation and accurate tissue localization challenging. Therefore, there is a need to improve CBCT image quality and Hounsfield Unit (HU) accuracy while preserving anatomical structures. To enhance the role and application value of CBCT in ART, we propose an energy-guided diffusion model (EGDiff) and conduct experiments on a chest tumor dataset to generate synthetic CT (sCT) from CBCT. The experimental results demonstrate impressive performance with an average absolute error of 26.87$\pm$6.14 HU, a structural similarity index measurement of 0.850$\pm$0.03, a peak signal-to-noise ratio of the sCT of 19.83$\pm$1.39 dB, and a normalized cross-correlation of the sCT of 0.874$\pm$0.04. These results indicate that our method outperforms state-of-the-art unsupervised synthesis methods in accuracy and visual quality, producing superior sCT images.
</details>
<details>
<summary>摘要</summary>
cone beam CT (CBCT) 在适应辐射疗法 (ART) 中发挥重要作用，准确地提供辐射治疗当器官解剖结构发生变化时。然而，CBCT图像受到散射噪声和artefacts的影响，使凭借CBCT alone 的精度计算和精确地本结构定位变得困难。因此，需要改进CBCT图像质量和温顺单元 (HU) 精度，保持器官结构的完整性。为了提高 CBCT 在 ART 中的角色和应用价值，我们提议一种能量指导扩散模型 (EGDiff)，并在胸腔肿瘤数据集上进行实验，将 CBCT 转换成 Synthetic CT (sCT)。实验结果表明，我们的方法在精度和视觉质量方面具有卓越表现，与现有的无监督杂合 Synthesis 方法相比，具有更高的 HU 精度、更高的结构相似度、更高的峰信号噪声比和更高的正规化交叉相似度。这些结果表明，我们的方法可以生成高质量的 sCT 图像，超过现有的方法。
</details></li>
</ul>
<hr>
<h2 id="A-Hybrid-CNN-Transformer-Architecture-with-Frequency-Domain-Contrastive-Learning-for-Image-Deraining"><a href="#A-Hybrid-CNN-Transformer-Architecture-with-Frequency-Domain-Contrastive-Learning-for-Image-Deraining" class="headerlink" title="A Hybrid CNN-Transformer Architecture with Frequency Domain Contrastive Learning for Image Deraining"></a>A Hybrid CNN-Transformer Architecture with Frequency Domain Contrastive Learning for Image Deraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03340">http://arxiv.org/abs/2308.03340</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Wang, Wei Li</li>
<li>for: 图像排除雨纹效果的提升</li>
<li>methods: 使用深度学习方法进行图像排除雨纹</li>
<li>results: 实现了高质量的图像排除雨纹效果Here’s a more detailed explanation of each point:</li>
<li>for: The paper aims to improve the effectiveness of image deraining by using deep learning methods.</li>
<li>methods: The paper uses deep learning techniques, specifically a deep neural network, to remove rain streaks from degraded images.</li>
<li>results: The paper achieves high-quality image deraining results by using these methods.<details>
<summary>Abstract</summary>
Image deraining is a challenging task that involves restoring degraded images affected by rain streaks.
</details>
<details>
<summary>摘要</summary>
图像抑雨是一项复杂的任务，涉及到修复受到雨束纹的图像。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/07/eess.IV_2023_08_07/" data-id="clly4xtg600epvl88c02dbcla" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/06/cs.LG_2023_08_06/" class="article-date">
  <time datetime="2023-08-05T16:00:00.000Z" itemprop="datePublished">2023-08-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/06/cs.LG_2023_08_06/">cs.LG - 2023-08-06 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="AI-GOMS-Large-AI-Driven-Global-Ocean-Modeling-System"><a href="#AI-GOMS-Large-AI-Driven-Global-Ocean-Modeling-System" class="headerlink" title="AI-GOMS: Large AI-Driven Global Ocean Modeling System"></a>AI-GOMS: Large AI-Driven Global Ocean Modeling System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03152">http://arxiv.org/abs/2308.03152</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Xiong, Yanfei Xiang, Hao Wu, Shuyi Zhou, Yuze Sun, Muyuan Ma, Xiaomeng Huang</li>
<li>for: 这个论文旨在提出一种基于人工智能的全球海洋模型系统（AI-GOMS），用于准确和高效地预测全球海洋日常变化。</li>
<li>methods: 该模型系统包括基本海洋变量预测的贝叶克自适应网络结构，以及包括地方下降、波解码和生物化交互的轻量级精度模型。</li>
<li>results: 该模型在30天预测全球海洋基本变量（15层深度）的方面达到了最佳性能，并能够模拟kuroshio海域的 mezoscale旋涡和赤道太平洋海洋层分化。<details>
<summary>Abstract</summary>
Ocean modeling is a powerful tool for simulating the physical, chemical, and biological processes of the ocean, which is the foundation for marine science research and operational oceanography. Modern numerical ocean modeling mainly consists of governing equations and numerical algorithms. Nonlinear instability, computational expense, low reusability efficiency and high coupling costs have gradually become the main bottlenecks for the further development of numerical ocean modeling. Recently, artificial intelligence-based modeling in scientific computing has shown revolutionary potential for digital twins and scientific simulations, but the bottlenecks of numerical ocean modeling have not been further solved. Here, we present AI-GOMS, a large AI-driven global ocean modeling system, for accurate and efficient global ocean daily prediction. AI-GOMS consists of a backbone model with the Fourier-based Masked Autoencoder structure for basic ocean variable prediction and lightweight fine-tuning models incorporating regional downscaling, wave decoding, and biochemistry coupling modules. AI-GOMS has achieved the best performance in 30 days of prediction for the global ocean basic variables with 15 depth layers at 1/4{\deg} spatial resolution. Beyond the good performance in statistical metrics, AI-GOMS realizes the simulation of mesoscale eddies in the Kuroshio region at 1/12{\deg} spatial resolution and ocean stratification in the tropical Pacific Ocean. AI-GOMS provides a new backbone-downstream paradigm for Earth system modeling, which makes the system transferable, scalable and reusable.
</details>
<details>
<summary>摘要</summary>
海洋模型是一种强大的工具，用于模拟海洋物理、化学和生物过程，是marine science研究和操作 oceanography的基础。现代数值海洋模型主要由管理方程和数值算法组成。不线性不稳定、计算成本高、再利用率低和对接成本高逐渐成为数值海洋模型的主要瓶颈。在科学计算中，人工智能基于的模型已经展示了革命性的潜力，但数值海洋模型中的瓶颈问题还没有得到解决。在这里，我们介绍AI-GOMS，一个大型基于人工智能的全球海洋模型，用于准确和高效的全球海洋日常预测。AI-GOMS包括一个基本 ocean variable prediction的背bone模型，以及 incorporating regional downscaling、波动解码和生物化学结合模块的轻量级精度增强模型。AI-GOMS在30天预测全球海洋基本变量的15层深度分辨率下达到了最佳性能。除了在统计指标方面的好表现，AI-GOMS还实现了kuroshio区域的 mesoscale eddies 在1/12°的空间分辨率下的模拟，以及在 тропиical Pacific Ocean中的海洋层次分布。AI-GOMS提供了一个新的背部-下游模式，使系统可重用、可扩展和可重复使用。
</details></li>
</ul>
<hr>
<h2 id="Nest-DGIL-Nesterov-optimized-Deep-Geometric-Incremental-Learning-for-CS-Image-Reconstruction"><a href="#Nest-DGIL-Nesterov-optimized-Deep-Geometric-Incremental-Learning-for-CS-Image-Reconstruction" class="headerlink" title="Nest-DGIL: Nesterov-optimized Deep Geometric Incremental Learning for CS Image Reconstruction"></a>Nest-DGIL: Nesterov-optimized Deep Geometric Incremental Learning for CS Image Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03807">http://arxiv.org/abs/2308.03807</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fanxiaohong/Nest-DGIL">https://github.com/fanxiaohong/Nest-DGIL</a></li>
<li>paper_authors: Xiaohong Fan, Yin Yang, Ke Chen, Yujie Feng, Jianping Zhang</li>
<li>for: 这种方法用于解决图像逆问题，包括高&#x2F;低频图像特征的学习能力和保证几何纹理细节的重建。</li>
<li>methods: 基于第二个奈斯特洛夫距离梯度优化的深度幂增量学习框架，包括普通的线性重建、几何增量学习、奈斯特洛夫加速和后处理。</li>
<li>results: 提出的方法可以快速收敛，并且可以避免中间重建结果落入不同几何分解域之外，同时也可以保证高&#x2F;低频图像特征的学习能力和几何纹理细节的重建。<details>
<summary>Abstract</summary>
Proximal gradient-based optimization is one of the most common strategies for solving image inverse problems as well as easy to implement. However, these techniques often generate heavy artifacts in image reconstruction. One of the most popular refinement methods is to fine-tune the regularization parameter to alleviate such artifacts, but it may not always be sufficient or applicable due to increased computational costs. In this work, we propose a deep geometric incremental learning framework based on second Nesterov proximal gradient optimization. The proposed end-to-end network not only has the powerful learning ability for high/low frequency image features,but also can theoretically guarantee that geometric texture details will be reconstructed from preliminary linear reconstruction.Furthermore, it can avoid the risk of intermediate reconstruction results falling outside the geometric decomposition domains and achieve fast convergence. Our reconstruction framework is decomposed into four modules including general linear reconstruction, cascade geometric incremental restoration, Nesterov acceleration and post-processing. In the image restoration step,a cascade geometric incremental learning module is designed to compensate for the missing texture information from different geometric spectral decomposition domains. Inspired by overlap-tile strategy, we also develop a post-processing module to remove the block-effect in patch-wise-based natural image reconstruction. All parameters in the proposed model are learnable,an adaptive initialization technique of physical-parameters is also employed to make model flexibility and ensure converging smoothly. We compare the reconstruction performance of the proposed method with existing state-of-the-art methods to demonstrate its superiority. Our source codes are available at https://github.com/fanxiaohong/Nest-DGIL.
</details>
<details>
<summary>摘要</summary>
近似梯度基于优化是解决图像反问题的一种最常见策略，易于实现，但它们经常产生重要的artefacts。一种常见的改进方法是调整正则化参数，以降低这些artefacts，但这并不总是可行或适用，因为它可能会增加计算成本。在这种工作中，我们提出了深度几何增量学习框架，基于第二个Nesterov proximal梯度优化。我们的提案的端到端网络不仅具有高/低频图像特征的强大学习能力，而且可以理论保证从初步线性重建中恢复几何纹理细节。此外，它可以避免初步重建结果落入不同几何分解域之外，并且可以快速收敛。我们的重建框架分为四个模块：一般线性重建、几何增量学习、Nesterov加速和后处理。在图像恢复阶段，我们设计了几何增量学习模块，以补做不同几何分解域中缺失的纹理信息。受到 overlap-tile 策略的启发，我们还开发了后处理模块，以去除 patch-wise 基于自然图像重建中的块效果。所有模型参数都是可学习的，并且我们采用了 adaptive 初始化技术，以确保模型的灵活性和平滑的收敛。我们与现有的状态 искусственного智能方法进行比较，以证明我们的方法的优越性。我们的源代码可以在 GitHub 上找到：https://github.com/fanxiaohong/Nest-DGIL。
</details></li>
</ul>
<hr>
<h2 id="Self-Directed-Linear-Classification"><a href="#Self-Directed-Linear-Classification" class="headerlink" title="Self-Directed Linear Classification"></a>Self-Directed Linear Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03142">http://arxiv.org/abs/2308.03142</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, Nikos Zarifis</li>
<li>for: 这个论文研究了在线分类中学习者在适应性下预测示例的顺序，以最小化总错误数。</li>
<li>methods: 论文使用了自适应选择示例顺序的方法，并研究了这种方法的力量。</li>
<li>results: 论文表明，在线分类中，适应性下预测示例的顺序可以实现较好的性能，比如最小化错误数。此外，论文还提供了两个主要结果：在random sampling的情况下，可以使用自适应选择示例顺序来预测整个数据集的 labels，而且这种方法的错误数与数据集的大小无关。<details>
<summary>Abstract</summary>
In online classification, a learner is presented with a sequence of examples and aims to predict their labels in an online fashion so as to minimize the total number of mistakes. In the self-directed variant, the learner knows in advance the pool of examples and can adaptively choose the order in which predictions are made. Here we study the power of choosing the prediction order and establish the first strong separation between worst-order and random-order learning for the fundamental task of linear classification. Prior to our work, such a separation was known only for very restricted concept classes, e.g., one-dimensional thresholds or axis-aligned rectangles.   We present two main results. If $X$ is a dataset of $n$ points drawn uniformly at random from the $d$-dimensional unit sphere, we design an efficient self-directed learner that makes $O(d \log \log(n))$ mistakes and classifies the entire dataset. If $X$ is an arbitrary $d$-dimensional dataset of size $n$, we design an efficient self-directed learner that predicts the labels of $99\%$ of the points in $X$ with mistake bound independent of $n$. In contrast, under a worst- or random-ordering, the number of mistakes must be at least $\Omega(d \log n)$, even when the points are drawn uniformly from the unit sphere and the learner only needs to predict the labels for $1\%$ of them.
</details>
<details>
<summary>摘要</summary>
在在线分类中，学习者会看到一串示例，并尝试预测它们的标签，以最小化总错误数量。在自适应变体中，学习者可以适应地选择预测的顺序。我们研究了预测顺序的选择力，并证明了在线分类的基本任务中，自适应学习的最差顺序和随机顺序之间存在首次强分化。在我们的工作前，这种分化只知道在非常限定的概念集合中，例如一维阈值或AXI正方形。我们提出了两个主要结果。如果$X$是一个$d$-维均匀随机分布的点集， then we design an efficient self-directed learner that makes $O(d \log \log n)$ mistakes and classifies the entire dataset.如果$X$是一个任意$d$-维数据集的Size $n$, then we design an efficient self-directed learner that predicts the labels of $99\%$ of the points in $X$ with mistake bound independent of $n$.相比之下，在最差或随机顺序下，错误数量至少为$\Omega(d \log n)$, even when the points are drawn uniformly from the unit sphere and the learner only needs to predict the labels for $1\%$ of them.
</details></li>
</ul>
<hr>
<h2 id="Iterative-Magnitude-Pruning-as-a-Renormalisation-Group-A-Study-in-The-Context-of-The-Lottery-Ticket-Hypothesis"><a href="#Iterative-Magnitude-Pruning-as-a-Renormalisation-Group-A-Study-in-The-Context-of-The-Lottery-Ticket-Hypothesis" class="headerlink" title="Iterative Magnitude Pruning as a Renormalisation Group: A Study in The Context of The Lottery Ticket Hypothesis"></a>Iterative Magnitude Pruning as a Renormalisation Group: A Study in The Context of The Lottery Ticket Hypothesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03128">http://arxiv.org/abs/2308.03128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abu-Al Hassan</li>
<li>for: 这个论文探讨了深度神经网络（DNN）的复杂世界，特别关注了赢家票假设（LTH）。</li>
<li>methods: 这个论文使用了迭代幂额减小（IMP）法则，逐渐消除微型 weights，模拟 DNN 的步进学习。</li>
<li>results: 研究发现，winning ticket 可以在各种相似问题上达到类似性能，并且通过与物理学术 Renormalisation Group（RG）理论的联系，提高了 IMP 的理解。<details>
<summary>Abstract</summary>
This thesis delves into the intricate world of Deep Neural Networks (DNNs), focusing on the exciting concept of the Lottery Ticket Hypothesis (LTH). The LTH posits that within extensive DNNs, smaller, trainable subnetworks termed "winning tickets", can achieve performance comparable to the full model. A key process in LTH, Iterative Magnitude Pruning (IMP), incrementally eliminates minimal weights, emulating stepwise learning in DNNs. Once we identify these winning tickets, we further investigate their "universality". In other words, we check if a winning ticket that works well for one specific problem could also work well for other, similar problems. We also bridge the divide between the IMP and the Renormalisation Group (RG) theory in physics, promoting a more rigorous understanding of IMP.
</details>
<details>
<summary>摘要</summary>
这个论文探讨了深度神经网络（DNN）的复杂世界，专注于吸引人的抽签假设（LTH）。LTH认为，在广泛的DNN中，更小的、可训练的子网络“赢家票”可以达到相同的性能。我们在LTH中使用增量大小减少（IMP）来逐渐消除最小的 weights，模拟了DNN中的步进学习。一旦我们确定了这些赢家票，我们进一步调查它们的“通用性”。即我们检查一个赢家票在一个特定问题上表现良好是否可以在其他相似问题上表现良好。我们还将IMP与物理学中的 renormalization group（RG）理论相连接，以便更好地理解IMP。
</details></li>
</ul>
<hr>
<h2 id="Learning-Rate-Free-Learning-Dissecting-D-Adaptation-and-Probabilistic-Line-Search"><a href="#Learning-Rate-Free-Learning-Dissecting-D-Adaptation-and-Probabilistic-Line-Search" class="headerlink" title="Learning-Rate-Free Learning: Dissecting D-Adaptation and Probabilistic Line Search"></a>Learning-Rate-Free Learning: Dissecting D-Adaptation and Probabilistic Line Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03102">http://arxiv.org/abs/2308.03102</a></li>
<li>repo_url: None</li>
<li>paper_authors: Max McGuinness</li>
<li>for: 这 paper 探讨了两种最近的学习率优化方法在随机梯度下降中：D-Adaptation（arXiv:2301.07733）和 probabilistic line search（arXiv:1502.02846）。这些方法的目的是减轻选择初始学习率的负担，通过包含距离度量和 Gaussian 过程 posterior 估计，respectively。</li>
<li>methods: 这 paper 使用了 D-Adaptation 和 probabilistic line search 两种方法，它们都是为了优化学习率。D-Adaptation 方法使用了距离度量来选择最佳学习率，而 probabilistic line search 方法则使用了 Gaussian 过程 posterior 估计来优化学习率。</li>
<li>results: 这 paper 的结果表明，D-Adaptation 和 probabilistic line search 两种方法都可以减轻选择初始学习率的负担，并且可以提高模型的性能。具体来说，D-Adaptation 方法可以在不同的数据集上实现更好的性能，而 probabilistic line search 方法则可以在不同的学习率下实现更好的性能。<details>
<summary>Abstract</summary>
This paper explores two recent methods for learning rate optimisation in stochastic gradient descent: D-Adaptation (arXiv:2301.07733) and probabilistic line search (arXiv:1502.02846). These approaches aim to alleviate the burden of selecting an initial learning rate by incorporating distance metrics and Gaussian process posterior estimates, respectively. In this report, I provide an intuitive overview of both methods, discuss their shared design goals, and devise scope for merging the two algorithms.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这篇论文探讨了两种最近的学习率优化方法：D-Adaptation（arXiv:2301.07733）和概率线搜索（arXiv:1502.02846）。这两种方法都尝试减轻选择初始学习率的负担，通过 incorporating 距离度量和 Gaussian 过程 posterior 估计，分别。在这份报告中，我提供了这两种方法的直观概述，讨论了它们的共同设计目标，并探讨了将两个算法合并的可能性。
</details></li>
</ul>
<hr>
<h2 id="Gradient-Coding-through-Iterative-Block-Leverage-Score-Sampling"><a href="#Gradient-Coding-through-Iterative-Block-Leverage-Score-Sampling" class="headerlink" title="Gradient Coding through Iterative Block Leverage Score Sampling"></a>Gradient Coding through Iterative Block Leverage Score Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03096">http://arxiv.org/abs/2308.03096</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neophytos Charalambides, Mert Pilanci, Alfred Hero</li>
<li>for: 这篇论文是用于实现分布式计算环境中的线性回传运算加速。</li>
<li>methods: 论文使用了扩展的抽样得分组（Leverage Score Sampling），并将其应用于首项方法（Gradient Coding），以减少分布式计算网络中的延迟。</li>
<li>results: 论文获得了一个可以在分布式计算环境中实现线性回传运算的轻量级化码 computing方案，并且可以在当中获得一个具有抽样范围的 $\ell_2$ 子空间嵌入。<details>
<summary>Abstract</summary>
We generalize the leverage score sampling sketch for $\ell_2$-subspace embeddings, to accommodate sampling subsets of the transformed data, so that the sketching approach is appropriate for distributed settings. This is then used to derive an approximate coded computing approach for first-order methods; known as gradient coding, to accelerate linear regression in the presence of failures in distributed computational networks, \textit{i.e.} stragglers. We replicate the data across the distributed network, to attain the approximation guarantees through the induced sampling distribution. The significance and main contribution of this work, is that it unifies randomized numerical linear algebra with approximate coded computing, while attaining an induced $\ell_2$-subspace embedding through uniform sampling. The transition to uniform sampling is done without applying a random projection, as in the case of the subsampled randomized Hadamard transform. Furthermore, by incorporating this technique to coded computing, our scheme is an iterative sketching approach to approximately solving linear regression. We also propose weighting when sketching takes place through sampling with replacement, for further compression.
</details>
<details>
<summary>摘要</summary>
我们总结了权重评分抽样策略，以适应分布式设置，以便在分布式计算网络中使用抽样subset。这种策略可以用来 derivate一种精确的代码计算方法，称为梯度编码，以加速分布式计算中的线性回归，即在分布式计算网络中的慢速进程（即慢进程）。我们将数据复制到分布式网络中，以实现近似 garantess through the induced sampling distribution。这个研究的重要性和主要贡献在于，它将随机化数字线性代数与近似代码计算相结合，并通过均匀抽样实现$\ell_2$次元空间嵌入。在抽样过程中，我们不会应用随机投影，如Randomized Hadamard Transform中的subsampled抽样。此外，我们还提出了在抽样过程中使用权重，以进一步压缩数据。因此，我们的方案是一种迭代抽样策略，用于约等于解 linear regression。我们的方法可以在分布式计算中实现高效的线性回归解决方案，并且可以扩展到更复杂的机器学习模型。
</details></li>
</ul>
<hr>
<h2 id="Control-aware-echo-state-networks-Ca-ESN-for-the-suppression-of-extreme-events"><a href="#Control-aware-echo-state-networks-Ca-ESN-for-the-suppression-of-extreme-events" class="headerlink" title="Control-aware echo state networks (Ca-ESN) for the suppression of extreme events"></a>Control-aware echo state networks (Ca-ESN) for the suppression of extreme events</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03095">http://arxiv.org/abs/2308.03095</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alberto Racca, Luca Magri</li>
<li>for: 防止非线性系统中的极端事件的发生</li>
<li>methods: 结合ESN和控制策略，如比例- интеграル-导数控制和模型预测控制，实现非线性系统的有效控制</li>
<li>results: 在混沌液体流中，使用Ca-ESN比传统方法减少极端事件的发生，提高控制效果，开启了非线性系统控制的新可能性<details>
<summary>Abstract</summary>
Extreme event are sudden large-amplitude changes in the state or observables of chaotic nonlinear systems, which characterize many scientific phenomena. Because of their violent nature, extreme events typically have adverse consequences, which call for methods to prevent the events from happening. In this work, we introduce the control-aware echo state network (Ca-ESN) to seamlessly combine ESNs and control strategies, such as proportional-integral-derivative and model predictive control, to suppress extreme events. The methodology is showcased on a chaotic-turbulent flow, in which we reduce the occurrence of extreme events with respect to traditional methods by two orders of magnitude. This works opens up new possibilities for the efficient control of nonlinear systems with neural networks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>极端事件是非线性系统中突然大幅度变化的状态或观测量，这些事件frequently occur in many scientific phenomena. 由于其暴力性，极端事件通常会带来不良后果，需要采取方法来预防这些事件的发生。在这种工作中，我们提出了控制意识 echo state network (Ca-ESN)，可以将ESNs和控制策略，如 proporциональ-integral-derivative 和模型预测控制，结合在一起，以降低极端事件的发生频率。我们在混沌-turbulent flow中应用了这种方法，并比传统方法降低了极端事件的发生频率二个数量级。这项工作开 up new possibilities for the efficient control of nonlinear systems with neural networks.Note: "极端事件" in Chinese is usually translated as "extreme events" or "outliers", but in the context of this text, it refers to sudden large-amplitude changes in the state or observables of chaotic nonlinear systems.
</details></li>
</ul>
<hr>
<h2 id="Visualization-of-Extremely-Sparse-Contingency-Table-by-Taxicab-Correspondence-Analysis-A-Case-Study-of-Textual-Data"><a href="#Visualization-of-Extremely-Sparse-Contingency-Table-by-Taxicab-Correspondence-Analysis-A-Case-Study-of-Textual-Data" class="headerlink" title="Visualization of Extremely Sparse Contingency Table by Taxicab Correspondence Analysis: A Case Study of Textual Data"></a>Visualization of Extremely Sparse Contingency Table by Taxicab Correspondence Analysis: A Case Study of Textual Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03079">http://arxiv.org/abs/2308.03079</a></li>
<li>repo_url: None</li>
<li>paper_authors: V. Choulakian, J. Allard</li>
<li>for: Visualization of extremely sparse ontingency tables</li>
<li>methods: Taxicab correspondence analysis, a robust variant of correspondence analysis</li>
<li>results: Visualization of an extremely sparse textual data set of size 590 by 8265 concerning fragments of 8 sacred books<details>
<summary>Abstract</summary>
We present an overview of taxicab correspondence analysis, a robust variant of correspondence analysis, for visualization of extremely sparse ontingency tables. In particular we visualize an extremely sparse textual data set of size 590 by 8265 concerning fragments of 8 sacred books recently introduced by Sah and Fokou\'e (2019) and studied quite in detail by (12 + 1) dimension reduction methods (t-SNE, UMAP, PHATE,...) by Ma, Sun and Zou (2022).
</details>
<details>
<summary>摘要</summary>
我们提供了taxicab对应分析的概述，这是对对应分析的一种稳定版本，用于可见化极稀疏的对应关系表。特别是我们使用了590行x8265列的极稀疏文本数据集，这些数据来自 sah和fokou（2019）所引入的8种圣书的片断，并且通过(12+1)维度减少方法（t-SNE、UMAP、PHATE等）进行了深入研究。这些研究由Ma、sun和Zou（2022）进行了。
</details></li>
</ul>
<hr>
<h2 id="Study-for-Performance-of-MobileNetV1-and-MobileNetV2-Based-on-Breast-Cancer"><a href="#Study-for-Performance-of-MobileNetV1-and-MobileNetV2-Based-on-Breast-Cancer" class="headerlink" title="Study for Performance of MobileNetV1 and MobileNetV2 Based on Breast Cancer"></a>Study for Performance of MobileNetV1 and MobileNetV2 Based on Breast Cancer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03076">http://arxiv.org/abs/2308.03076</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiuqi Yan</li>
<li>for: 这项实验的目的是比较 MobileNetV1 和 MobileNetV2 模型在分类乳腺癌图像方面的性能。</li>
<li>methods: 实验使用了 Kaggle 上下载的数据集，并对其进行Normalization。然后，使用了神经网络模型来学习数据集，找出图像的特征并判断乳腺癌。</li>
<li>results: 实验结果表明，在处理这个数据集时，MobileNetV1 模型表现较好， validation accuracy 和 overfit 也较高。<details>
<summary>Abstract</summary>
Artificial intelligence is constantly evolving and can provide effective help in all aspects of people's lives. The experiment is mainly to study the use of artificial intelligence in the field of medicine. The purpose of this experiment was to compare which of MobileNetV1 and MobileNetV2 models was better at detecting histopathological images of the breast downloaded at Kaggle. When the doctor looks at the pathological image, there may be errors that lead to errors in judgment, and the observation speed is slow. Rational use of artificial intelligence can effectively reduce the error of doctor diagnosis in breast cancer judgment and speed up doctor diagnosis. The dataset was downloaded from Kaggle and then normalized. The basic principle of the experiment is to let the neural network model learn the downloaded data set. Then find the pattern and be able to judge on your own whether breast tissue is cancer. In the dataset, benign tumor pictures and malignant tumor pictures have been classified, of which 198738 are benign tumor pictures and 78, 786 are malignant tumor pictures. After calling MobileNetV1 and MobileNetV2, the dataset is trained separately, the training accuracy and validation accuracy rate are obtained, and the image is drawn. It can be observed that MobileNetV1 has better validation accuracy and overfit during MobileNetV2 training. From the experimental results, it can be seen that in the case of processing this dataset, MobileNetV1 is much better than MobileNetV2.
</details>
<details>
<summary>摘要</summary>
人工智能不断发展，可以提供有效的帮助在人们的生活中。本实验的主要目的是研究人工智能在医学领域的应用。本实验的目的是比较MobileNetV1和MobileNetV2模型在Kaggle上下载的乳腺病理图像上的表现。医生查看病理图像时可能会出现错误，导致诊断错误， observation速度较慢。合理使用人工智能可以有效减少医生诊断乳腺癌判断中的错误，并加快医生诊断速度。数据集来自Kaggle，然后 норциали化。实验的基本原则是让神经网络模型学习下载的数据集。然后找出模式，并能够自己判断乳腺细胞是否为癌细胞。数据集中，恶性肿瘤图像和良性肿瘤图像已经分类，其中198738个是恶性肿瘤图像，78786个是良性肿瘤图像。在 MobileNetV1 和 MobileNetV2 之后，数据集被分别训练，并获得训练精度和验证精度率。图像也被画出来。可以看到，在处理这个数据集时，MobileNetV1 表现得更好。
</details></li>
</ul>
<hr>
<h2 id="Comparative-Analysis-of-Epileptic-Seizure-Prediction-Exploring-Diverse-Pre-Processing-Techniques-and-Machine-Learning-Models"><a href="#Comparative-Analysis-of-Epileptic-Seizure-Prediction-Exploring-Diverse-Pre-Processing-Techniques-and-Machine-Learning-Models" class="headerlink" title="Comparative Analysis of Epileptic Seizure Prediction: Exploring Diverse Pre-Processing Techniques and Machine Learning Models"></a>Comparative Analysis of Epileptic Seizure Prediction: Exploring Diverse Pre-Processing Techniques and Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05176">http://arxiv.org/abs/2308.05176</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md. Simul Hasan Talukder, Rejwan Bin Sulaiman</li>
<li>for: 预测癫痫发作</li>
<li>methods: 使用五种机器学习模型（Random Forest、Decision Tree、Extra Trees、Logistic Regression、Gradient Boosting）对电enzephalogram数据进行预测</li>
<li>results: 结果显示，LR类ifier的准确率为56.95%，GB和DT都达到97.17%的准确率，RT达到98.99%的准确率，ET模型表现最佳，准确率达99.29%。<details>
<summary>Abstract</summary>
Epilepsy is a prevalent neurological disorder characterized by recurrent and unpredictable seizures, necessitating accurate prediction for effective management and patient care. Application of machine learning (ML) on electroencephalogram (EEG) recordings, along with its ability to provide valuable insights into brain activity during seizures, is able to make accurate and robust seizure prediction an indispensable component in relevant studies. In this research, we present a comprehensive comparative analysis of five machine learning models - Random Forest (RF), Decision Tree (DT), Extra Trees (ET), Logistic Regression (LR), and Gradient Boosting (GB) - for the prediction of epileptic seizures using EEG data. The dataset underwent meticulous preprocessing, including cleaning, normalization, outlier handling, and oversampling, ensuring data quality and facilitating accurate model training. These preprocessing techniques played a crucial role in enhancing the models' performance. The results of our analysis demonstrate the performance of each model in terms of accuracy. The LR classifier achieved an accuracy of 56.95%, while GB and DT both attained 97.17% accuracy. RT achieved a higher accuracy of 98.99%, while the ET model exhibited the best performance with an accuracy of 99.29%. Our findings reveal that the ET model outperformed not only the other models in the comparative analysis but also surpassed the state-of-the-art results from previous research. The superior performance of the ET model makes it a compelling choice for accurate and robust epileptic seizure prediction using EEG data.
</details>
<details>
<summary>摘要</summary>
《诊断和治疗精神疾病》中，有一种常见的神经疾病是癫痫症，它表现为不规则和难以预测的癫痫发作，因此需要准确的预测以提供有效的管理和患者护理。在这些研究中，我们使用机器学习（ML）技术对电энцефалограм（EEG）记录进行分析，并通过对大脑活动的描述来提供有价值的预测。本研究中，我们对五种机器学习模型进行了比较分析：Random Forest（RF）、Decision Tree（DT）、Extra Trees（ET）、Logistic Regression（LR）和Gradient Boosting（GB）。我们对EEG数据进行了仔细的处理，包括清洁、 нормализа、异常处理和扩充，以确保数据质量的高度。这些处理技术对模型的表现产生了重要的影响。我们的分析结果显示每个模型的准确率。LR分类器的准确率为56.95%，而GB和DT都达到了97.17%的准确率。RT达到了98.99%的准确率，而ET模型表现出了最佳的性能，准确率达99.29%。我们的发现表明ET模型不仅在 Comparative analysis中表现出色，还超越了过去研究中的状态归化结果。ET模型的优秀表现使其成为精确和可靠的癫痫发作预测的首选方法。
</details></li>
</ul>
<hr>
<h2 id="TARJAMAT-Evaluation-of-Bard-and-ChatGPT-on-Machine-Translation-of-Ten-Arabic-Varieties"><a href="#TARJAMAT-Evaluation-of-Bard-and-ChatGPT-on-Machine-Translation-of-Ten-Arabic-Varieties" class="headerlink" title="TARJAMAT: Evaluation of Bard and ChatGPT on Machine Translation of Ten Arabic Varieties"></a>TARJAMAT: Evaluation of Bard and ChatGPT on Machine Translation of Ten Arabic Varieties</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03051">http://arxiv.org/abs/2308.03051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karima Kadaoui, Samar M. Magdy, Abdul Waheed, Md Tawkat Islam Khondaker, Ahmed Oumar El-Shangiti, El Moatez Billah Nagoudi, Muhammad Abdul-Mageed</li>
<li>for: 这篇论文主要是为了评估大型自然语言模型（LLMs）在不同的阿拉伯语种中的翻译能力。</li>
<li>methods: 这篇论文使用了Google Bard和OpenAI ChatGPT两个模型，并对这两个模型在十种阿拉伯语种中的翻译能力进行了全面的评估。</li>
<li>results: 研究发现，LLMs在一些阿拉伯语种中表现不佳，特别是对于没有充足公共数据的语种，如阿尔及利亚和毛里塔尼亚的方言。然而，它们在更常见的方言中表现较为满意，尽管有时会落后于现有的商业系统 like Google Translate。此外，研究还发现，Bard在翻译任务中遵循人类指示的能力有限。<details>
<summary>Abstract</summary>
Large language models (LLMs) finetuned to follow human instructions have recently emerged as a breakthrough in AI. Models such as Google Bard and OpenAI ChatGPT, for example, are surprisingly powerful tools for question answering, code debugging, and dialogue generation. Despite the purported multilingual proficiency of these models, their linguistic inclusivity remains insufficiently explored. Considering this constraint, we present a thorough assessment of Bard and ChatGPT (encompassing both GPT-3.5 and GPT-4) regarding their machine translation proficiencies across ten varieties of Arabic. Our evaluation covers diverse Arabic varieties such as Classical Arabic, Modern Standard Arabic, and several nuanced dialectal variants. Furthermore, we undertake a human-centric study to scrutinize the efficacy of the most recent model, Bard, in following human instructions during translation tasks. Our exhaustive analysis indicates that LLMs may encounter challenges with certain Arabic dialects, particularly those for which minimal public data exists, such as Algerian and Mauritanian dialects. However, they exhibit satisfactory performance with more prevalent dialects, albeit occasionally trailing behind established commercial systems like Google Translate. Additionally, our analysis reveals a circumscribed capability of Bard in aligning with human instructions in translation contexts. Collectively, our findings underscore that prevailing LLMs remain far from inclusive, with only limited ability to cater for the linguistic and cultural intricacies of diverse communities.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM），如Google Bard和OpenAI ChatGPT，在最近几年内 emerge as a breakthrough in AI。这些模型具有强大的问答、代码调试和对话生成能力。 despite the purported multilingual proficiency of these models, their linguistic inclusivity remains insufficiently explored. Considering this constraint, we present a thorough assessment of Bard and ChatGPT (including both GPT-3.5 and GPT-4) regarding their machine translation proficiencies across ten varieties of Arabic. Our evaluation covers diverse Arabic varieties such as Classical Arabic, Modern Standard Arabic, and several nuanced dialectal variants. Furthermore, we undertake a human-centric study to scrutinize the efficacy of the most recent model, Bard, in following human instructions during translation tasks. Our exhaustive analysis indicates that LLMs may encounter challenges with certain Arabic dialects, particularly those for which minimal public data exists, such as Algerian and Mauritanian dialects. However, they exhibit satisfactory performance with more prevalent dialects, albeit occasionally trailing behind established commercial systems like Google Translate. Additionally, our analysis reveals a circumscribed capability of Bard in aligning with human instructions in translation contexts. Collectively, our findings underscore that prevailing LLMs remain far from inclusive, with only limited ability to cater for the linguistic and cultural intricacies of diverse communities.
</details></li>
</ul>
<hr>
<h2 id="Weakly-Supervised-Multi-Task-Representation-Learning-for-Human-Activity-Analysis-Using-Wearables"><a href="#Weakly-Supervised-Multi-Task-Representation-Learning-for-Human-Activity-Analysis-Using-Wearables" class="headerlink" title="Weakly Supervised Multi-Task Representation Learning for Human Activity Analysis Using Wearables"></a>Weakly Supervised Multi-Task Representation Learning for Human Activity Analysis Using Wearables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03805">http://arxiv.org/abs/2308.03805</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taoran Sheng, Manfred Huber</li>
<li>for: 多元特征掌握和多任务同时处理</li>
<li>methods: 弱监睹多输出SIAMESE网络</li>
<li>results: 可以同时解决多个任务，并且在许多情况下超越单任务监睹方法表现。<details>
<summary>Abstract</summary>
Sensor data streams from wearable devices and smart environments are widely studied in areas like human activity recognition (HAR), person identification, or health monitoring. However, most of the previous works in activity and sensor stream analysis have been focusing on one aspect of the data, e.g. only recognizing the type of the activity or only identifying the person who performed the activity. We instead propose an approach that uses a weakly supervised multi-output siamese network that learns to map the data into multiple representation spaces, where each representation space focuses on one aspect of the data. The representation vectors of the data samples are positioned in the space such that the data with the same semantic meaning in that aspect are closely located to each other. Therefore, as demonstrated with a set of experiments, the trained model can provide metrics for clustering data based on multiple aspects, allowing it to address multiple tasks simultaneously and even to outperform single task supervised methods in many situations. In addition, further experiments are presented that in more detail analyze the effect of the architecture and of using multiple tasks within this framework, that investigate the scalability of the model to include additional tasks, and that demonstrate the ability of the framework to combine data for which only partial relationship information with respect to the target tasks is available.
</details>
<details>
<summary>摘要</summary>
仪器数据流FROM wearable devices和智能环境广泛研究在人体活动识别（HAR）、人员身份识别或健康监测等领域。然而，大多数前一些工作在活动和仪器流数据分析中都是关注一个方面的数据，例如只是识别活动的类型或者只是识别活动的执行者。我们提出了一种方法，使用弱监督多输出siamesenet来映射数据到多个表示空间中，其中每个表示空间都关注一个数据的方面。映射 vectors的数据样本被置于空间中，使得数据具有同一 Semantic meaning在该方面的数据集中均被 closely located。因此，通过一些实验，我们的模型可以为数据提供多个任务的指标，使其可以同时解决多个任务，甚至在许多情况下超越单任务监督方法。此外，我们还进行了更多的实验，分析了这种架构的影响和多个任务的使用情况，以及模型的扩展性。
</details></li>
</ul>
<hr>
<h2 id="Machine-learning-methods-for-the-search-for-L-T-brown-dwarfs-in-the-data-of-modern-sky-surveys"><a href="#Machine-learning-methods-for-the-search-for-L-T-brown-dwarfs-in-the-data-of-modern-sky-surveys" class="headerlink" title="Machine learning methods for the search for L&amp;T brown dwarfs in the data of modern sky surveys"></a>Machine learning methods for the search for L&amp;T brown dwarfs in the data of modern sky surveys</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03045">http://arxiv.org/abs/2308.03045</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iamaleksandra/ml-brown-dwarfs">https://github.com/iamaleksandra/ml-brown-dwarfs</a></li>
<li>paper_authors: Aleksandra Avdeeva</li>
<li>For: 这个论文的目的是创建一个高度可靠的褐矮星样本，以确定褐矮星的特征和分布。* Methods: 这篇论文使用机器学习算法，如Random Forest Classifier、XGBoost、SVM Classifier和TabNet，对PanStarrs DR1、2MASS和WISE数据进行分类，以 distinguishing L和T褐矮星从其他 spectral和照度类型的 объек。* Results: 研究人员使用机器学习算法，成功地分类出L和T褐矮星，并与传统的决策规则进行比较，证明了其效率和相关性。<details>
<summary>Abstract</summary>
According to various estimates, brown dwarfs (BD) should account for up to 25 percent of all objects in the Galaxy. However, few of them are discovered and well-studied, both individually and as a population. Homogeneous and complete samples of brown dwarfs are needed for these kinds of studies. Due to their weakness, spectral studies of brown dwarfs are rather laborious. For this reason, creating a significant reliable sample of brown dwarfs, confirmed by spectroscopic observations, seems unattainable at the moment. Numerous attempts have been made to search for and create a set of brown dwarfs using their colours as a decision rule applied to a vast amount of survey data. In this work, we use machine learning methods such as Random Forest Classifier, XGBoost, SVM Classifier and TabNet on PanStarrs DR1, 2MASS and WISE data to distinguish L and T brown dwarfs from objects of other spectral and luminosity classes. The explanation of the models is discussed. We also compare our models with classical decision rules, proving their efficiency and relevance.
</details>
<details>
<summary>摘要</summary>
To overcome this challenge, numerous attempts have been made to search for and create a set of brown dwarfs using their colors as a decision rule applied to a vast amount of survey data. In this study, we use machine learning methods such as Random Forest Classifier, XGBoost, SVM Classifier, and TabNet on PanStarrs DR1, 2MASS, and WISE data to distinguish L and T brown dwarfs from objects of other spectral and luminosity classes. We discuss the explanation of the models and compare them with classical decision rules, demonstrating their efficiency and relevance.
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-for-Infectious-Disease-Risk-Prediction-A-Survey"><a href="#Machine-Learning-for-Infectious-Disease-Risk-Prediction-A-Survey" class="headerlink" title="Machine Learning for Infectious Disease Risk Prediction: A Survey"></a>Machine Learning for Infectious Disease Risk Prediction: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03037">http://arxiv.org/abs/2308.03037</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mutong Liu, Yang Liu, Jiming Liu</li>
<li>for: 这篇论文的目的是探讨机器学习如何在抑制传染病中发挥作用，以便更有效地预测感染病风险。</li>
<li>methods: 论文中使用的方法包括：介绍背景和动机，介绍不同类型的机器学习模型，讨论模型输入、设计目标和评估性能等挑战，并结尾提出未解决的问题和未来方向。</li>
<li>results: 论文的结果表明，机器学习可以帮助量化疾病传染模式，并准确预测感染病风险。<details>
<summary>Abstract</summary>
Infectious diseases, either emerging or long-lasting, place numerous people at risk and bring heavy public health burdens worldwide. In the process against infectious diseases, predicting the epidemic risk by modeling the disease transmission plays an essential role in assisting with preventing and controlling disease transmission in a more effective way. In this paper, we systematically describe how machine learning can play an essential role in quantitatively characterizing disease transmission patterns and accurately predicting infectious disease risks. First, we introduce the background and motivation of using machine learning for infectious disease risk prediction. Next, we describe the development and components of various machine learning models for infectious disease risk prediction. Specifically, existing models fall into three categories: Statistical prediction, data-driven machine learning, and epidemiology-inspired machine learning. Subsequently, we discuss challenges encountered when dealing with model inputs, designing task-oriented objectives, and conducting performance evaluation. Finally, we conclude with a discussion of open questions and future directions.
</details>
<details>
<summary>摘要</summary>
免疫疾病，无论是新兴的或长期存在的，都会对全球公共卫生带来巨大的压力。在抗击免疫疾病的过程中，预测疾病传播风险的模型化协助了更有效地预防和控制疾病传播。在这篇论文中，我们系统地描述了机器学习如何在免疫疾病风险预测中发挥重要作用。首先，我们介绍了使用机器学习预测免疫疾病风险的背景和动机。然后，我们描述了不同类型的机器学习模型的开发和组成部分。具体来说，现有的模型可以分为三类：统计预测、数据驱动机器学习和医学机器学习。接着，我们讨论了与模型输入、设计任务目标以及性能评价过程中遇到的挑战。最后，我们 conclude with 未来方向的开放问题。
</details></li>
</ul>
<hr>
<h2 id="Serverless-Federated-AUPRC-Optimization-for-Multi-Party-Collaborative-Imbalanced-Data-Mining"><a href="#Serverless-Federated-AUPRC-Optimization-for-Multi-Party-Collaborative-Imbalanced-Data-Mining" class="headerlink" title="Serverless Federated AUPRC Optimization for Multi-Party Collaborative Imbalanced Data Mining"></a>Serverless Federated AUPRC Optimization for Multi-Party Collaborative Imbalanced Data Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03035">http://arxiv.org/abs/2308.03035</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xidongwu/d-auprc">https://github.com/xidongwu/d-auprc</a></li>
<li>paper_authors: Xidong Wu, Zhengmian Hu, Jian Pei, Heng Huang</li>
<li>for: 本文targets the problem of multi-party collaborative training for imbalanced data tasks, and proposes a new algorithm called ServerLess biAsed sTochastic gradiEnt (SLATE) to directly optimize the Area Under Precision-Recall Curve (AUPRC).</li>
<li>methods: 本文使用了服务器less多方合作学习 Setting，并将问题转化为conditional stochastic optimization problem。furthermore, the authors propose a new algorithm called ServerLess biAsed sTochastic gradiEnt with Momentum-based variance reduction (SLATE-M) to improve the convergence rate.</li>
<li>results: 本文的实验结果表明，SLATE-M算法可以减少communication cost，并且与最佳单机Online方法匹配。这是首次解决多方合作AUPRC最大化问题。<details>
<summary>Abstract</summary>
Multi-party collaborative training, such as distributed learning and federated learning, is used to address the big data challenges. However, traditional multi-party collaborative training algorithms were mainly designed for balanced data mining tasks and are intended to optimize accuracy (\emph{e.g.}, cross-entropy). The data distribution in many real-world applications is skewed and classifiers, which are trained to improve accuracy, perform poorly when applied to imbalanced data tasks since models could be significantly biased toward the primary class. Therefore, the Area Under Precision-Recall Curve (AUPRC) was introduced as an effective metric. Although single-machine AUPRC maximization methods have been designed, multi-party collaborative algorithm has never been studied. The change from the single-machine to the multi-party setting poses critical challenges.   To address the above challenge, we study the serverless multi-party collaborative AUPRC maximization problem since serverless multi-party collaborative training can cut down the communications cost by avoiding the server node bottleneck, and reformulate it as a conditional stochastic optimization problem in a serverless multi-party collaborative learning setting and propose a new ServerLess biAsed sTochastic gradiEnt (SLATE) algorithm to directly optimize the AUPRC. After that, we use the variance reduction technique and propose ServerLess biAsed sTochastic gradiEnt with Momentum-based variance reduction (SLATE-M) algorithm to improve the convergence rate, which matches the best theoretical convergence result reached by the single-machine online method. To the best of our knowledge, this is the first work to solve the multi-party collaborative AUPRC maximization problem.
</details>
<details>
<summary>摘要</summary>
多方合作训练，如分布式学习和联邦学习，用于解决大数据问题。然而，传统的多方合作训练算法主要设计用于均衡数据挖掘任务，并且optimize准确率（例如，交叉熵）。在许多实际应用中，数据分布不均，类 clasifier在不均衡数据任务中表现糟糕，因为模型可能受主要类别的偏见。因此，Area Under Precision-Recall Curve（AUPRC）被引入作为有效指标。虽然单机AUPRC最大化方法已经设计过，但多方合作算法尚未研究。在单机到多方 Setting中的变化 pose critical challenges。为了解决以上挑战，我们研究了无服务器多方合作AUPRC最大化问题，因为无服务器多方合作训练可以降低通信成本，并将问题重新定义为conditional stochastic optimization问题在无服务器多方合作学习Setting中。然后，我们提出了一种新的ServerLess biAsed sTochastic gradiEnt（SLATE）算法，以直接优化AUPRC。接着，我们使用了差分reduction技术，并提出了ServerLess biAsed sTochastic gradiEnt with Momentum-based variance reduction（SLATE-M）算法，以提高收敛率，与单机在线方法的最佳理论收敛率匹配。到目前为止，这是首次解决多方合作AUPRC最大化问题的研究。
</details></li>
</ul>
<hr>
<h2 id="Causal-Disentanglement-Hidden-Markov-Model-for-Fault-Diagnosis"><a href="#Causal-Disentanglement-Hidden-Markov-Model-for-Fault-Diagnosis" class="headerlink" title="Causal Disentanglement Hidden Markov Model for Fault Diagnosis"></a>Causal Disentanglement Hidden Markov Model for Fault Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03027">http://arxiv.org/abs/2308.03027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rihao Chang, Yongtao Ma, Weizhi Nie, Jie Nie, An-an Liu</li>
<li>for: 本研究旨在提出一种基于 causal disentanglement hidden markov model (CDHM) 的磨损诊断方法，以便更好地捕捉磨损特征并实现预测磨损类型。</li>
<li>methods: 本方法首先使用时序数据进行磨损特征的捕捉，然后逐渐分离磨损信号中相关和无关的因素。 其中，我们使用 ELBO 来优化学习 causal disentanglement markov model。此外，我们还采用无监督领域适应，将学习的分离表示转移到其他工作环境中。</li>
<li>results: 实验结果表明，提出的方法在 CWRU 数据集和 IMS 数据集上具有优秀的效果，可以准确地预测磨损类型。<details>
<summary>Abstract</summary>
In modern industries, fault diagnosis has been widely applied with the goal of realizing predictive maintenance. The key issue for the fault diagnosis system is to extract representative characteristics of the fault signal and then accurately predict the fault type. In this paper, we propose a Causal Disentanglement Hidden Markov model (CDHM) to learn the causality in the bearing fault mechanism and thus, capture their characteristics to achieve a more robust representation. Specifically, we make full use of the time-series data and progressively disentangle the vibration signal into fault-relevant and fault-irrelevant factors. The ELBO is reformulated to optimize the learning of the causal disentanglement Markov model. Moreover, to expand the scope of the application, we adopt unsupervised domain adaptation to transfer the learned disentangled representations to other working environments. Experiments were conducted on the CWRU dataset and IMS dataset. Relevant results validate the superiority of the proposed method.
</details>
<details>
<summary>摘要</summary>
现代产业中，故障诊断已广泛应用，目的是实现预测维护。故障诊断系统的关键问题是提取表征性的故障信号，并准确预测故障类型。在本文中，我们提议一种因果分解隐藏马尔可夫模型（CDHM），以学习滤波器故障机制中的因果关系，并 capture其特征来实现更加稳定的表征。具体来说，我们利用时间序列数据，逐步分解振荡信号，分解出相关和无关故障因素。我们 reformulate ELBO，以便学习因果分解马尔可夫模型。此外，为扩展应用范围，我们采用无监督领域适应，将学习的分解表征转移到其他工作环境。在CWRU数据集和IMS数据集上进行了实验，实验结果证明了我们提出的方法的优越性。
</details></li>
</ul>
<hr>
<h2 id="Early-Detection-and-Localization-of-Pancreatic-Cancer-by-Label-Free-Tumor-Synthesis"><a href="#Early-Detection-and-Localization-of-Pancreatic-Cancer-by-Label-Free-Tumor-Synthesis" class="headerlink" title="Early Detection and Localization of Pancreatic Cancer by Label-Free Tumor Synthesis"></a>Early Detection and Localization of Pancreatic Cancer by Label-Free Tumor Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03008">http://arxiv.org/abs/2308.03008</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mrgiovanni/synthetictumors">https://github.com/mrgiovanni/synthetictumors</a></li>
<li>paper_authors: Bowen Li, Yu-Cheng Chou, Shuwen Sun, Hualin Qiao, Alan Yuille, Zongwei Zhou<br>for: 这篇论文的目的是提高胰脏癌早期检测和定位，以增加病人5年生存率从8.5%提高到20%。methods: 本研究使用人工智能（AI）模型，将健康胰脏中的小型胰脏肿瘤合成成许多标注的例子，以帮助医生早期检测胰脏癌。results: 我们的实验结果显示，使用合成的胰脏肿瘤训练AI模型，胰脏癌检测率与真实胰脏癌检测率相似，且能够更好地检测小型胰脏肿瘤。此外，我们还证明了使用合成胰脏肿瘤和真实胰脏癌检测结果进行混合训练，可以提高AI模型的普遍性和检测精度。<details>
<summary>Abstract</summary>
Early detection and localization of pancreatic cancer can increase the 5-year survival rate for patients from 8.5% to 20%. Artificial intelligence (AI) can potentially assist radiologists in detecting pancreatic tumors at an early stage. Training AI models require a vast number of annotated examples, but the availability of CT scans obtaining early-stage tumors is constrained. This is because early-stage tumors may not cause any symptoms, which can delay detection, and the tumors are relatively small and may be almost invisible to human eyes on CT scans. To address this issue, we develop a tumor synthesis method that can synthesize enormous examples of small pancreatic tumors in the healthy pancreas without the need for manual annotation. Our experiments demonstrate that the overall detection rate of pancreatic tumors, measured by Sensitivity and Specificity, achieved by AI trained on synthetic tumors is comparable to that of real tumors. More importantly, our method shows a much higher detection rate for small tumors. We further investigate the per-voxel segmentation performance of pancreatic tumors if AI is trained on a combination of CT scans with synthetic tumors and CT scans with annotated large tumors at an advanced stage. Finally, we show that synthetic tumors improve AI generalizability in tumor detection and localization when processing CT scans from different hospitals. Overall, our proposed tumor synthesis method has immense potential to improve the early detection of pancreatic cancer, leading to better patient outcomes.
</details>
<details>
<summary>摘要</summary>
早期检测和肿瘤localization可以提高患者5年生存率从8.5%提高到20%。人工智能（AI）可能能够帮助放射学家在早期发现肿瘤。然而，训练AI模型需要庞大的标注示例，但获得早期肿瘤的CT扫描数据受限。这是因为早期肿瘤可能不会导致任何症状，这可能会延迟检测，并且肿瘤相对较小，可能对人类目视难以看到在CT扫描中。为解决这个问题，我们开发了一种肿瘤合成方法，可以在健康的胰脏中合成巨大的小肿瘤示例，无需人工标注。我们的实验表明，由AI训练在合成肿瘤上的检测率（敏感性和特异性）与真实肿瘤相比较高，并且检测到小肿瘤的率更高。我们进一步调查了使用合成肿瘤和注解大肿瘤的CT扫描结合训练AI的效果，发现这种方法可以提高肿瘤检测和地图localization的普适性。最后，我们证明了合成肿瘤可以提高AI在不同医院CT扫描处理时的普适性。总之，我们提出的肿瘤合成方法有巨大的潜力，可以提高肿瘤检测的早期，导致更好的病例结果。
</details></li>
</ul>
<hr>
<h2 id="Deep-Polar-Codes"><a href="#Deep-Polar-Codes" class="headerlink" title="Deep Polar Codes"></a>Deep Polar Codes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03004">http://arxiv.org/abs/2308.03004</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HzFu/MNet_DeepCDR">https://github.com/HzFu/MNet_DeepCDR</a></li>
<li>paper_authors: Geon Choi, Namyoon Lee</li>
<li>for: 这个论文旨在提出一种新的预变换极码，称为深度极码。</li>
<li>methods: 论文提出了一种深度极编码器，利用多层极化转换来实现低复杂度实现，同时能够改善极码的重量分布。此外，该编码器支持范围广的代码率和块长。</li>
<li>results: 通过 simulations，论文表明深度极码在不同代码率下的块错误率比现有的预变换极码更低，同时保持低的编码和解码复杂度。此外，论文还表明，将深度极码与循环检验码 concatenate 可以达到 finite block length 容量的 meta-converse bound 的下界 within 0.4 dB 以下。<details>
<summary>Abstract</summary>
In this paper, we introduce a novel class of pre-transformed polar codes, termed as deep polar codes. We first present a deep polar encoder that harnesses a series of multi-layered polar transformations with varying sizes. Our approach to encoding enables a low-complexity implementation while significantly enhancing the weight distribution of the code. Moreover, our encoding method offers flexibility in rate-profiling, embracing a wide range of code rates and blocklengths. Next, we put forth a low-complexity decoding algorithm called successive cancellation list with backpropagation parity checks (SCL-BPC). This decoding algorithm leverages the parity check equations in the reverse process of the multi-layered pre-transformed encoding for SCL decoding. Additionally, we present a low-latency decoding algorithm that employs parallel-SCL decoding by treating partially pre-transformed bit patterns as additional frozen bits. Through simulations, we demonstrate that deep polar codes outperform existing pre-transformed polar codes in terms of block error rates across various code rates under short block lengths, while maintaining low encoding and decoding complexity. Furthermore, we show that concatenating deep polar codes with cyclic-redundancy-check codes can achieve the meta-converse bound of the finite block length capacity within 0.4 dB in some instances.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种新的预转换极码，称为深度极码。我们首先提出了一种深度极编码器，利用多层极化转换来实现低复杂性实现，同时提高码的质量分布。此外，我们的编码方法支持范围广的码率和块长度。接着，我们提出了一种低复杂度解码算法，称为顺序取消列表带回传播可能性检查（SCL-BPC）。这种解码算法利用了反向多层预转换的parity check方程来实现SCL解码。此外，我们还提出了一种低延迟解码算法，通过并行SCL解码来处理部分预转换的bit pattern。通过实验，我们证明了深度极码在不同的码率下的块错误率都较低，同时保持了低编码和解码复杂度。此外，我们还显示了 concatenating 深度极码可以实现finite block length容量的meta-converse bound在0.4 dB之间。
</details></li>
</ul>
<hr>
<h2 id="Spanish-Pre-trained-BERT-Model-and-Evaluation-Data"><a href="#Spanish-Pre-trained-BERT-Model-and-Evaluation-Data" class="headerlink" title="Spanish Pre-trained BERT Model and Evaluation Data"></a>Spanish Pre-trained BERT Model and Evaluation Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02976">http://arxiv.org/abs/2308.02976</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dccuchile/beto">https://github.com/dccuchile/beto</a></li>
<li>paper_authors: José Cañete, Gabriel Chaperon, Rodrigo Fuentes, Jou-Hui Ho, Hojin Kang, Jorge Pérez</li>
<li>for: 增强西班牙语模型的训练和评估资源。</li>
<li>methods: BERT基于模型预训练 exclusively on Spanish data。</li>
<li>results: 比其他基于BERT的模型在大多数任务上获得更好的结果，并在一些任务上创造新的状态。<details>
<summary>Abstract</summary>
The Spanish language is one of the top 5 spoken languages in the world. Nevertheless, finding resources to train or evaluate Spanish language models is not an easy task. In this paper we help bridge this gap by presenting a BERT-based language model pre-trained exclusively on Spanish data. As a second contribution, we also compiled several tasks specifically for the Spanish language in a single repository much in the spirit of the GLUE benchmark. By fine-tuning our pre-trained Spanish model, we obtain better results compared to other BERT-based models pre-trained on multilingual corpora for most of the tasks, even achieving a new state-of-the-art on some of them. We have publicly released our model, the pre-training data, and the compilation of the Spanish benchmarks.
</details>
<details>
<summary>摘要</summary>
“西班牙语是全球前五大最受欢迎的语言之一，然而找到用于训练或评估西班牙语模型的资源并不是一个容易的任务。在这篇论文中，我们帮助填补这个差距，提出了基于BERT的西班牙语模型，并将其推广到多个任务中。作为我们的第二次贡献，我们还将西班牙语的多个任务集成了一个单一的存储库，类似于GLUE套件。通过精益地调整我们的预训西班牙语模型，我们在大多数任务上取得了更好的结果，甚至创下了一些新的州态。我们已经公开了我们的模型、预训数据和西班牙语套件。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Generalized-Oversampling-for-Learning-from-Imbalanced-datasets-and-Associated-Theory"><a href="#Generalized-Oversampling-for-Learning-from-Imbalanced-datasets-and-Associated-Theory" class="headerlink" title="Generalized Oversampling for Learning from Imbalanced datasets and Associated Theory"></a>Generalized Oversampling for Learning from Imbalanced datasets and Associated Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02966">http://arxiv.org/abs/2308.02966</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samuel Stocksieker, Denys Pommeret, Arthur Charpentier</li>
<li>for: 本研究旨在解决超参异常学习中的数据不均衡问题，具体来说是用于异常 regression 问题。</li>
<li>methods: 本文提出了一种基于kernel density estimate的数据扩充方法，称为GOLIATH算法，该方法可以应用于分类和回归问题。它包括两大类别的人工扩充：基于扰动的，如加aussian noise，和基于 interpolate的，如 SMOTE。此外，该方法还提供了这些机器学习算法的Explicit表达式和准确性梯度的表达式，特别是对SMOTE算法的表达式。</li>
<li>results: 本文通过应用GOLIATH算法在异常 regression 中进行了实验评估，并与现有状态方法进行了比较。结果表明，GOLIATH算法在异常 regression 中具有显著的改善效果。<details>
<summary>Abstract</summary>
In supervised learning, it is quite frequent to be confronted with real imbalanced datasets. This situation leads to a learning difficulty for standard algorithms. Research and solutions in imbalanced learning have mainly focused on classification tasks. Despite its importance, very few solutions exist for imbalanced regression. In this paper, we propose a data augmentation procedure, the GOLIATH algorithm, based on kernel density estimates which can be used in classification and regression. This general approach encompasses two large families of synthetic oversampling: those based on perturbations, such as Gaussian Noise, and those based on interpolations, such as SMOTE. It also provides an explicit form of these machine learning algorithms and an expression of their conditional densities, in particular for SMOTE. New synthetic data generators are deduced. We apply GOLIATH in imbalanced regression combining such generator procedures with a wild-bootstrap resampling technique for the target values. We evaluate the performance of the GOLIATH algorithm in imbalanced regression situations. We empirically evaluate and compare our approach and demonstrate significant improvement over existing state-of-the-art techniques.
</details>
<details>
<summary>摘要</summary>
在监督学习中，很普遍遇到实际数据集的不均衡问题。这种情况会导致标准算法学习困难。研究和解决不均衡学习问题的研究主要集中在分类任务上。虽然其重要性很大，但是现有的解决方案很少。在这篇论文中，我们提出了一种数据扩充方法，名为GOLIATH算法，基于核密度估计。这种方法可以用于分类和回归任务。这个总体方法包括两大家族的人工扩充：基于扰动，如 Gaussian Noise，和基于 interpolations，如 SMOTE。它还提供了这些机器学习算法的直观表达，特别是对 SMOTE 的表达。我们从这些synthetic数据生成器中推出了新的数据生成器。我们在不均衡回归中使用这些生成器和通用Bootstrap抽取技术来预测值。我们对 GOLIATH 算法在不均衡回归情况下的性能进行了实验性评估和比较，并证明了我们的方法在现有状态的技术上显著提高了性能。
</details></li>
</ul>
<hr>
<h2 id="Data-Fusion-for-Multi-Task-Learning-of-Building-Extraction-and-Height-Estimation"><a href="#Data-Fusion-for-Multi-Task-Learning-of-Building-Extraction-and-Height-Estimation" class="headerlink" title="Data Fusion for Multi-Task Learning of Building Extraction and Height Estimation"></a>Data Fusion for Multi-Task Learning of Building Extraction and Height Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02960">http://arxiv.org/abs/2308.02960</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/SaadAhmedJamal/IEEE_DFC2023">https://github.com/SaadAhmedJamal/IEEE_DFC2023</a></li>
<li>paper_authors: Saad Ahmed Jamal, Arioluwa Aribisala</li>
<li>for: 本研究は都市重建问题上提出的DFC23 Track 2 Contest中的一种多任务学习方法，用于批量抽取和高度估算 optical和雷达卫星图像中。</li>
<li>methods: 本研究使用多任务学习方法，通过重用特征和形成隐式约束 между多个任务来提高解决方案的质量。</li>
<li>results: 本研究的基准结果表明，对于批量抽取和高度估算，在设计了相关实验后，baseline结果显著提高了。<details>
<summary>Abstract</summary>
In accordance with the urban reconstruction problem proposed by the DFC23 Track 2 Contest, this paper attempts a multitask-learning method of building extraction and height estimation using both optical and radar satellite imagery. Contrary to the initial goal of multitask learning which could potentially give a superior solution by reusing features and forming implicit constraints between multiple tasks, this paper reports the individual implementation of the building extraction and height estimation under constraints. The baseline results for the building extraction and the height estimation significantly increased after designed experiments.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "urban reconstruction" was translated as "城市重建" (chéngshì zhòngjiàn)* "multitask-learning" was translated as "多任务学习" (duō zhìxí)* "building extraction" was translated as "建筑物提取" (jiànzhì wù tiēchū)* "height estimation" was translated as "高度估算" (gāodù gèsuan)* "satellite imagery" was translated as "卫星图像" (wèixīng túxiàng)
</details></li>
</ul>
<hr>
<h2 id="K-band-Self-supervised-MRI-Reconstruction-via-Stochastic-Gradient-Descent-over-K-space-Subsets"><a href="#K-band-Self-supervised-MRI-Reconstruction-via-Stochastic-Gradient-Descent-over-K-space-Subsets" class="headerlink" title="K-band: Self-supervised MRI Reconstruction via Stochastic Gradient Descent over K-space Subsets"></a>K-band: Self-supervised MRI Reconstruction via Stochastic Gradient Descent over K-space Subsets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02958">http://arxiv.org/abs/2308.02958</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mikgroup/k-band">https://github.com/mikgroup/k-band</a></li>
<li>paper_authors: Frederic Wang, Han Qi, Alfredo De Goyeneche, Reinhard Heckel, Michael Lustig, Efrat Shimron</li>
<li>for: 这项研究的目的是为了使用只有部分、有限分辨率的k-空间数据进行深度学习模型的训练，以提高MRI图像重建的精度和效率。</li>
<li>methods: 这项研究使用了一种新的数学框架，称为k-band，以便使用只有部分、有限分辨率的k-空间数据进行深度学习模型的训练。具体来说，这种方法使用了在每次训练迭代中使用只有一小部分k-空间数据来计算梯度的束教学法。</li>
<li>results: 实验表明，k-band方法可以与使用高分辨率数据进行训练的状态对照方法（SoTA）的性能相似，而无需使用高分辨率数据进行训练。此外，k-band方法还可以在快速获得的有限分辨率数据上进行自我监督式训练，从而提高了MRI图像重建的精度和效率。<details>
<summary>Abstract</summary>
Although deep learning (DL) methods are powerful for solving inverse problems, their reliance on high-quality training data is a major hurdle. This is significant in high-dimensional (dynamic/volumetric) magnetic resonance imaging (MRI), where acquisition of high-resolution fully sampled k-space data is impractical. We introduce a novel mathematical framework, dubbed k-band, that enables training DL models using only partial, limited-resolution k-space data. Specifically, we introduce training with stochastic gradient descent (SGD) over k-space subsets. In each training iteration, rather than using the fully sampled k-space for computing gradients, we use only a small k-space portion. This concept is compatible with different sampling strategies; here we demonstrate the method for k-space "bands", which have limited resolution in one dimension and can hence be acquired rapidly. We prove analytically that our method stochastically approximates the gradients computed in a fully-supervised setup, when two simple conditions are met: (i) the limited-resolution axis is chosen randomly-uniformly for every new scan, hence k-space is fully covered across the entire training set, and (ii) the loss function is weighed with a mask, derived here analytically, which facilitates accurate reconstruction of high-resolution details. Numerical experiments with raw MRI data indicate that k-band outperforms two other methods trained on limited-resolution data and performs comparably to state-of-the-art (SoTA) methods trained on high-resolution data. k-band hence obtains SoTA performance, with the advantage of training using only limited-resolution data. This work hence introduces a practical, easy-to-implement, self-supervised training framework, which involves fast acquisition and self-supervised reconstruction and offers theoretical guarantees.
</details>
<details>
<summary>摘要</summary>
尽管深度学习（DL）方法有力量解决反向问题，但它们依赖高质量训练数据是一个主要障碍。在高维度（动态/体积）磁共振成像（MRI）中，获取高分辨率完全采样的k空间数据是不现实的。我们介绍了一种新的数学框架，称之为k带，允许使用仅部分、有限分辨率k空间数据进行训练DL模型。具体来说，我们引入了使用批处理的梯度下降（SGD）在k空间子集上训练。在每个训练迭代中，而不是使用完全采样的k空间来计算梯度，我们只使用一小部分k空间。这种概念 compatible with different sampling strategies; here we demonstrate the method for k-space "bands", which have limited resolution in one dimension and can hence be acquired rapidly. We prove analytically that our method stochastically approximates the gradients computed in a fully-supervised setup, when two simple conditions are met: (i) the limited-resolution axis is chosen randomly-uniformly for every new scan, hence k-space is fully covered across the entire training set, and (ii) the loss function is weighed with a mask, derived here analytically, which facilitates accurate reconstruction of high-resolution details. Numerical experiments with raw MRI data indicate that k-band outperforms two other methods trained on limited-resolution data and performs comparably to state-of-the-art (SoTA) methods trained on high-resolution data. k-band hence obtains SoTA performance, with the advantage of training using only limited-resolution data. This work hence introduces a practical, easy-to-implement, self-supervised training framework, which involves fast acquisition and self-supervised reconstruction and offers theoretical guarantees.
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Study-of-AI-based-Smart-Contract-Creation"><a href="#An-Empirical-Study-of-AI-based-Smart-Contract-Creation" class="headerlink" title="An Empirical Study of AI-based Smart Contract Creation"></a>An Empirical Study of AI-based Smart Contract Creation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02955">http://arxiv.org/abs/2308.02955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rabimba Karanjai, Edward Li, Lei Xu, Weidong Shi</li>
<li>for: 本研究的主要目标是评估 LLMS 生成的智能合约代码质量。</li>
<li>methods: 我们使用了一个实验设置来评估生成代码的正确性、安全性和效率。</li>
<li>results: 我们发现生成的智能合约代码存在安全漏洞，同时代码质量和正确性受到输入参数的影响。但我们还发现了一些可以改进的方向。<details>
<summary>Abstract</summary>
The introduction of large language models (LLMs) like ChatGPT and Google Palm2 for smart contract generation seems to be the first well-established instance of an AI pair programmer. LLMs have access to a large number of open-source smart contracts, enabling them to utilize more extensive code in Solidity than other code generation tools. Although the initial and informal assessments of LLMs for smart contract generation are promising, a systematic evaluation is needed to explore the limits and benefits of these models. The main objective of this study is to assess the quality of generated code provided by LLMs for smart contracts. We also aim to evaluate the impact of the quality and variety of input parameters fed to LLMs. To achieve this aim, we created an experimental setup for evaluating the generated code in terms of validity, correctness, and efficiency. Our study finds crucial evidence of security bugs getting introduced in the generated smart contracts as well as the overall quality and correctness of the code getting impacted. However, we also identified the areas where it can be improved. The paper also proposes several potential research directions to improve the process, quality and safety of generated smart contract codes.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）如ChatGPT和Google Palm2的出现似乎是首个成功的AI双程程式。LLMs可以使用大量的开源智能合约，使其在Solidity中运用更大的代码。虽然初步评估结果尚未正式，但是需要系统性的评估来探索这些模型的限制和优点。本研究的主要目标是评估LLMs生成的智能合约代码质量。我们还想评估对LLMs输入参数的影响，以及生成代码的有效性、正确性和安全性。为了完成这个目标，我们设计了一个实验室来评估生成代码的有效性、正确性和安全性。我们发现生成的智能合约中有许多安全漏洞，并且代码的质量和正确性受到影响。但是，我们也发现了一些改善这些过程的研究方向。本文还提出了多个可能的研究方向，以提高生成代码的过程、质量和安全性。
</details></li>
</ul>
<hr>
<h2 id="dPASP-A-Comprehensive-Differentiable-Probabilistic-Answer-Set-Programming-Environment-For-Neurosymbolic-Learning-and-Reasoning"><a href="#dPASP-A-Comprehensive-Differentiable-Probabilistic-Answer-Set-Programming-Environment-For-Neurosymbolic-Learning-and-Reasoning" class="headerlink" title="dPASP: A Comprehensive Differentiable Probabilistic Answer Set Programming Environment For Neurosymbolic Learning and Reasoning"></a>dPASP: A Comprehensive Differentiable Probabilistic Answer Set Programming Environment For Neurosymbolic Learning and Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02944">http://arxiv.org/abs/2308.02944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renato Lui Geh, Jonas Gonçalves, Igor Cataneo Silveira, Denis Deratani Mauá, Fabio Gagliardi Cozman</li>
<li>for: 这篇论文旨在提出一种新的宣言型概率逻辑编程框架，用于可微分的神经符号逻辑推理。</li>
<li>methods: 该框架使得可以指定精确的概率模型，包括神经 predicate、逻辑约束和间隔值概率选择，以支持 combining 低级感知（图像、文本等）、通用的推理和（模糊的）统计知识。</li>
<li>results: 论文提出了多种 semantics  для probabilistic logic programs，以表达不确定、矛盾、不完整和&#x2F;或统计知识。同时，也介绍了如何使用神经 predicate 和概率选择进行 gradient-based 学习。论文还描述了一个实现的 package，用于推理和学习，并提供了一些示例程序。<details>
<summary>Abstract</summary>
We present dPASP, a novel declarative probabilistic logic programming framework for differentiable neuro-symbolic reasoning. The framework allows for the specification of discrete probabilistic models with neural predicates, logic constraints and interval-valued probabilistic choices, thus supporting models that combine low-level perception (images, texts, etc), common-sense reasoning, and (vague) statistical knowledge. To support all such features, we discuss the several semantics for probabilistic logic programs that can express nondeterministic, contradictory, incomplete and/or statistical knowledge. We also discuss how gradient-based learning can be performed with neural predicates and probabilistic choices under selected semantics. We then describe an implemented package that supports inference and learning in the language, along with several example programs. The package requires minimal user knowledge of deep learning system's inner workings, while allowing end-to-end training of rather sophisticated models and loss functions.
</details>
<details>
<summary>摘要</summary>
我们提出了dpASP，一种新的宣告性概率逻辑编程框架，用于可 diferenciable 神经符号逻辑推理。该框架允许用户 specify 权重逻辑模型，神经元 predicate，逻辑约束和间隔值概率选择，因此可以支持模型结合低级感知（图像、文本等）、通用理智和（抽象）统计知识。为支持这些特性，我们讨论了几种 probabilistic logic programs 的 semantics，可以表达不确定、矛盾、不完整和/或统计知识。我们还讨论了如何在 selected semantics 下使用 neural predicates 和 probabilistic choices 进行梯度基于学习。然后，我们描述了一个实现的 package，支持推理和学习语言中的各种例程，并提供了一些示例程序。该 package 需要用户具备最低知识量，同时允许用户通过终端训练较复杂的模型和损失函数。
</details></li>
</ul>
<hr>
<h2 id="Towards-the-Development-of-an-Uncertainty-Quantification-Protocol-for-the-Natural-Gas-Industry"><a href="#Towards-the-Development-of-an-Uncertainty-Quantification-Protocol-for-the-Natural-Gas-Industry" class="headerlink" title="Towards the Development of an Uncertainty Quantification Protocol for the Natural Gas Industry"></a>Towards the Development of an Uncertainty Quantification Protocol for the Natural Gas Industry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02941">http://arxiv.org/abs/2308.02941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Babajide Kolade</li>
<li>for: This paper aims to develop a protocol for assessing uncertainties in predictions of machine learning and mechanistic simulation models, specifically for the gas distribution industry.</li>
<li>methods: The protocol outlines an uncertainty quantification workflow that includes identifying key sources of uncertainties, using applicable methods of uncertainty propagation, and employing statistically rational estimators for output uncertainties.</li>
<li>results: The paper applies the protocol to test cases relevant to the gas distribution industry and presents the learnings from its application. The results demonstrate the effectiveness of the protocol in quantifying uncertainties in simulation results.<details>
<summary>Abstract</summary>
Simulations using machine learning (ML) models and mechanistic models are often run to inform decision-making processes. Uncertainty estimates of simulation results are critical to the decision-making process because simulation results of specific scenarios may have wide, but unspecified, confidence bounds that may impact subsequent analyses and decisions. The objective of this work is to develop a protocol to assess uncertainties in predictions of machine learning and mechanistic simulation models. The protocol will outline an uncertainty quantification workflow that may be used to establish credible bounds of predictability on computed quantities of interest and to assess model sufficiency. The protocol identifies key sources of uncertainties in machine learning and mechanistic modeling, defines applicable methods of uncertainty propagation for these sources, and includes statistically rational estimators for output uncertainties. The work applies the protocol to test cases relevant to the gas distribution industry and presents learnings from its application. The paper concludes with a brief discussion outlining a pathway to the wider adoption of uncertainty quantification within the industry
</details>
<details>
<summary>摘要</summary>
模拟使用机器学习（ML）模型和机制模型经常用来支持决策过程。模拟结果中的不确定性估计对决策过程是关键的，因为特定场景的模拟结果可能具有广泛而不确定的信任范围，这可能影响后续分析和决策。本工作的目标是开发一个协议来评估机器学习和机制模型预测结果中的不确定性。协议将 outline一个不确定性评估工作流程，可以用来确定计算量据点的可靠范围和评估模型的充分性。协议列出了机器学习和机制模型中的主要不确定性来源，采用可靠的方法进行不确定性传播，并提供了统计合理的输出不确定性估计器。本工作应用协议到 relevante test cases，并presented learnings from its application。文章 conclude with a brief discussion outlining a pathway to the wider adoption of uncertainty quantification within the industry。Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Towards-Consistency-Filtering-Free-Unsupervised-Learning-for-Dense-Retrieval"><a href="#Towards-Consistency-Filtering-Free-Unsupervised-Learning-for-Dense-Retrieval" class="headerlink" title="Towards Consistency Filtering-Free Unsupervised Learning for Dense Retrieval"></a>Towards Consistency Filtering-Free Unsupervised Learning for Dense Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02926">http://arxiv.org/abs/2308.02926</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Haoxiang-WasedaU/Towards-Consistency-Filtering-Free-Unsupervised-Learning-for-Dense-Retrieval">https://github.com/Haoxiang-WasedaU/Towards-Consistency-Filtering-Free-Unsupervised-Learning-for-Dense-Retrieval</a></li>
<li>paper_authors: Haoxiang Shi, Sumio Fujita, Tetsuya Sakai</li>
<li>for: 提高现代神经信息检索中的领域传递问题的解决方案</li>
<li>methods: 取消consistency filter，使用直接pseudo-labeling、pseudo-相关反馈或无监督关键词生成方法实现一致性自由粗粒度检索</li>
<li>results: 对多个数据集进行了广泛的实验评估，结果显示，使用TextRank基于pseudo relevance feedback方法可以超过其他方法的表现，并且对训练和推理效率具有显著改进。<details>
<summary>Abstract</summary>
Domain transfer is a prevalent challenge in modern neural Information Retrieval (IR). To overcome this problem, previous research has utilized domain-specific manual annotations and synthetic data produced by consistency filtering to finetune a general ranker and produce a domain-specific ranker. However, training such consistency filters are computationally expensive, which significantly reduces the model efficiency. In addition, consistency filtering often struggles to identify retrieval intentions and recognize query and corpus distributions in a target domain. In this study, we evaluate a more efficient solution: replacing the consistency filter with either direct pseudo-labeling, pseudo-relevance feedback, or unsupervised keyword generation methods for achieving consistent filtering-free unsupervised dense retrieval. Our extensive experimental evaluations demonstrate that, on average, TextRank-based pseudo relevance feedback outperforms other methods. Furthermore, we analyzed the training and inference efficiency of the proposed paradigm. The results indicate that filtering-free unsupervised learning can continuously improve training and inference efficiency while maintaining retrieval performance. In some cases, it can even improve performance based on particular datasets.
</details>
<details>
<summary>摘要</summary>
域名转移是现代神经信息检索（IR）中的一个常见挑战。以前的研究曾利用域名特定的手动标注和由consistency filtering生成的 sintetic数据来训练一个通用排名器并生成域名特定的排名器。然而，训练such consistency filters具有计算成本高的问题，这会significantly reduces the model efficiency。 In addition, consistency filtering often struggles to identify retrieval intentions and recognize query and corpus distributions in a target domain。在本研究中，我们评估了一种更有效的解决方案：取代consistency filter with direct pseudo-labeling、pseudo-relevance feedback或Unsupervised keyword generation方法来实现无 filtering-free无监督的排名。我们的广泛的实验评估表明，TextRank-based pseudo relevance feedback在其他方法中表现更好。此外，我们还分析了提议的训练和推理效率。结果表明，filtering-free无监督学习可以不断提高训练和推理效率，同时保持检索性能。在某些 dataset 上，它可以提高性能。
</details></li>
</ul>
<hr>
<h2 id="An-AI-Enabled-Framework-to-Defend-Ingenious-MDT-based-Attacks-on-the-Emerging-Zero-Touch-Cellular-Networks"><a href="#An-AI-Enabled-Framework-to-Defend-Ingenious-MDT-based-Attacks-on-the-Emerging-Zero-Touch-Cellular-Networks" class="headerlink" title="An AI-Enabled Framework to Defend Ingenious MDT-based Attacks on the Emerging Zero Touch Cellular Networks"></a>An AI-Enabled Framework to Defend Ingenious MDT-based Attacks on the Emerging Zero Touch Cellular Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02923">http://arxiv.org/abs/2308.02923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aneeqa Ijaz, Waseem Raza, Hasan Farooq, Marvin Manalastas, Ali Imran</li>
<li>for: 本研究旨在探讨黑客可以通过劣质MDT报告来攻击深度自动化无线网络的新型攻击方式，并对常见网络自动化功能的性能造成负面影响。</li>
<li>methods: 本研究使用机器学习方法来检测和排除劣质MDT报告，并通过一个实验场景证明其效果。</li>
<li>results: 研究发现，劣质MDT报告可以影响网络自动化功能的性能，而且可以通过Machine Learning来检测和排除这些劣质报告。<details>
<summary>Abstract</summary>
Deep automation provided by self-organizing network (SON) features and their emerging variants such as zero touch automation solutions is a key enabler for increasingly dense wireless networks and pervasive Internet of Things (IoT). To realize their objectives, most automation functionalities rely on the Minimization of Drive Test (MDT) reports. The MDT reports are used to generate inferences about network state and performance, thus dynamically change network parameters accordingly. However, the collection of MDT reports from commodity user devices, particularly low cost IoT devices, make them a vulnerable entry point to launch an adversarial attack on emerging deeply automated wireless networks. This adds a new dimension to the security threats in the IoT and cellular networks. Existing literature on IoT, SON, or zero touch automation does not address this important problem. In this paper, we investigate an impactful, first of its kind adversarial attack that can be launched by exploiting the malicious MDT reports from the compromised user equipment (UE). We highlight the detrimental repercussions of this attack on the performance of common network automation functions. We also propose a novel Malicious MDT Reports Identification framework (MRIF) as a countermeasure to detect and eliminate the malicious MDT reports using Machine Learning and verify it through a use-case. Thus, the defense mechanism can provide the resilience and robustness for zero touch automation SON engines against the adversarial MDT attacks
</details>
<details>
<summary>摘要</summary>
深度自动化提供的无需人工控制网络（SON）功能和其相关的零 touched自动化解决方案是现代无线网络和物联网（IoT）的关键驱动力。为实现他们的目标，大多数自动化功能都依赖于推理测试（MDT）报告。MDT报告用于生成网络状态和性能的推理，并在运行时动态地改变网络参数。然而，从低成本IoT设备收集MDT报告，特别是从受到攻击的用户设备，使得这些报告成为攻击 deeply automated 无线网络的易受攻击点。这添加了一个新的安全隐患，并且现有的相关文献不具备对这一重要问题的讨论。在这篇论文中，我们研究了一种新型的攻击，可以通过恶意MDT报告来发动，从恶意用户设备收集MDT报告。我们强调了这种攻击对常见网络自动化功能的不良影响。此外，我们还提出了一种新的恶意MDT报告识别框架（MRIF），用于检测和消除恶意MDT报告。我们使用机器学习来实现这一目标，并通过用例验证了这种防御机制的有效性。因此，这种防御机制可以为零 touched自动化 SON 引擎提供鲜活性和鲜活性。
</details></li>
</ul>
<hr>
<h2 id="Structured-Low-Rank-Tensors-for-Generalized-Linear-Models"><a href="#Structured-Low-Rank-Tensors-for-Generalized-Linear-Models" class="headerlink" title="Structured Low-Rank Tensors for Generalized Linear Models"></a>Structured Low-Rank Tensors for Generalized Linear Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02922">http://arxiv.org/abs/2308.02922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Batoul Taki, Anand D. Sarwate, Waheed U. Bajwa</li>
<li>for: This paper is written for researchers and practitioners interested in exploring tensor-based methods for generalized linear model (GLM) problems, particularly those dealing with low-rank tensor structures.</li>
<li>methods: The paper proposes a new low-rank tensor model called the Low Separation Rank (LSR) model, which is imposed onto the coefficient tensor in GLM problems. The authors also develop a block coordinate descent algorithm for parameter estimation in LSR-structured tensor GLMs.</li>
<li>results: The paper derives a minimax lower bound on the error threshold for estimating the coefficient tensor in LSR tensor GLM problems, which suggests that the sample complexity of the proposed method may be significantly lower than that of vectorized GLMs. The authors also demonstrate the efficacy of the proposed LSR tensor model on synthetic and real-world datasets.<details>
<summary>Abstract</summary>
Recent works have shown that imposing tensor structures on the coefficient tensor in regression problems can lead to more reliable parameter estimation and lower sample complexity compared to vector-based methods. This work investigates a new low-rank tensor model, called Low Separation Rank (LSR), in Generalized Linear Model (GLM) problems. The LSR model -- which generalizes the well-known Tucker and CANDECOMP/PARAFAC (CP) models, and is a special case of the Block Tensor Decomposition (BTD) model -- is imposed onto the coefficient tensor in the GLM model. This work proposes a block coordinate descent algorithm for parameter estimation in LSR-structured tensor GLMs. Most importantly, it derives a minimax lower bound on the error threshold on estimating the coefficient tensor in LSR tensor GLM problems. The minimax bound is proportional to the intrinsic degrees of freedom in the LSR tensor GLM problem, suggesting that its sample complexity may be significantly lower than that of vectorized GLMs. This result can also be specialised to lower bound the estimation error in CP and Tucker-structured GLMs. The derived bounds are comparable to tight bounds in the literature for Tucker linear regression, and the tightness of the minimax lower bound is further assessed numerically. Finally, numerical experiments on synthetic datasets demonstrate the efficacy of the proposed LSR tensor model for three regression types (linear, logistic and Poisson). Experiments on a collection of medical imaging datasets demonstrate the usefulness of the LSR model over other tensor models (Tucker and CP) on real, imbalanced data with limited available samples.
</details>
<details>
<summary>摘要</summary>
近期研究表明，在回归问题中强制维度结构 onto 参数矩阵可以导致更可靠的参数估计和较低的样本复杂度，相比vector化方法。这个工作 investigate一种新的低级别维度模型（LSR）在泛化线性模型（GLM）问题中。LSR模型是Tucker和CANDECOMP/PARAFAC（CP）模型的推广，并是阻塞矩阵分解（BTD）模型的特殊情况。这个工作提出了一种块坐标极下降算法 для GLM问题中LSR结构的参数估计。最重要的是，它 derivates一个最小最大下界对于GLM问题中LSR结构参数估计的误差阈值。这个下界与LSR结构参数估计的内在度度相关，表明其样本复杂度可能远低于vector化GLM的样本复杂度。这个结果还可以特殊化为对CP和Tucker结构GLM的参数估计误差的下界。 derivates bounds are comparable to tight bounds in the literature for Tucker linear regression, and the tightness of the minimax lower bound is further assessed numerically. Finally, numerical experiments on synthetic datasets demonstrate the efficacy of the proposed LSR tensor model for three regression types (linear, logistic and Poisson). Experiments on a collection of medical imaging datasets demonstrate the usefulness of the LSR model over other tensor models (Tucker and CP) on real, imbalanced data with limited available samples.
</details></li>
</ul>
<hr>
<h2 id="Spectral-Ranking-Inferences-based-on-General-Multiway-Comparisons"><a href="#Spectral-Ranking-Inferences-based-on-General-Multiway-Comparisons" class="headerlink" title="Spectral Ranking Inferences based on General Multiway Comparisons"></a>Spectral Ranking Inferences based on General Multiway Comparisons</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02918">http://arxiv.org/abs/2308.02918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianqing Fan, Zhipeng Lou, Weichen Wang, Mengxin Yu</li>
<li>For:  This paper studies the performance of the spectral method in estimating and quantifying uncertainty of unobserved preference scores in a very general and realistic setup.* Methods: The paper uses the spectral method, which is a more general and flexible approach than the commonly-used Bradley-Terry-Luce (BTL) or Plackett-Luce (PL) models. The paper also introduces a two-step spectral method that can achieve the same asymptotic efficiency as the Maximum Likelihood Estimator (MLE).* Results: The paper provides comprehensive frameworks for one-sample and two-sample ranking inferences, applicable to both fixed and random graph settings. It also introduces effective two-sample rank testing methods that are noteworthy. The paper substantiates its findings via comprehensive numerical simulations and applies its developed methodologies to perform statistical inferences on statistics journals and movie rankings.<details>
<summary>Abstract</summary>
This paper studies the performance of the spectral method in the estimation and uncertainty quantification of the unobserved preference scores of compared entities in a very general and more realistic setup in which the comparison graph consists of hyper-edges of possible heterogeneous sizes and the number of comparisons can be as low as one for a given hyper-edge. Such a setting is pervasive in real applications, circumventing the need to specify the graph randomness and the restrictive homogeneous sampling assumption imposed in the commonly-used Bradley-Terry-Luce (BTL) or Plackett-Luce (PL) models. Furthermore, in the scenarios when the BTL or PL models are appropriate, we unravel the relationship between the spectral estimator and the Maximum Likelihood Estimator (MLE). We discover that a two-step spectral method, where we apply the optimal weighting estimated from the equal weighting vanilla spectral method, can achieve the same asymptotic efficiency as the MLE. Given the asymptotic distributions of the estimated preference scores, we also introduce a comprehensive framework to carry out both one-sample and two-sample ranking inferences, applicable to both fixed and random graph settings. It is noteworthy that it is the first time effective two-sample rank testing methods are proposed. Finally, we substantiate our findings via comprehensive numerical simulations and subsequently apply our developed methodologies to perform statistical inferences on statistics journals and movie rankings.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Adversarial-Erasing-with-Pruned-Elements-Towards-Better-Graph-Lottery-Ticket"><a href="#Adversarial-Erasing-with-Pruned-Elements-Towards-Better-Graph-Lottery-Ticket" class="headerlink" title="Adversarial Erasing with Pruned Elements: Towards Better Graph Lottery Ticket"></a>Adversarial Erasing with Pruned Elements: Towards Better Graph Lottery Ticket</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02916">http://arxiv.org/abs/2308.02916</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangyuwen0627/ace-glt">https://github.com/wangyuwen0627/ace-glt</a></li>
<li>paper_authors: Yuwen Wang, Shunyu Liu, Kaixuan Chen, Tongtian Zhu, Ji Qiao, Mengjie Shi, Yuanyu Wan, Mingli Song</li>
<li>for:  Mitigate the computational cost of deep Graph Neural Networks (GNNs) on large input graphs while preserving original performance.</li>
<li>methods:  Combination of core subgraph and sparse subnetwork, adversarial complementary erasing (ACE) framework to explore valuable information from pruned components.</li>
<li>results:  Outperforms existing methods for searching Graph Lottery Ticket (GLT) in diverse tasks.Here’s the summary in English for reference:</li>
<li>for: Mitigating the computational cost of deep Graph Neural Networks (GNNs) on large input graphs while preserving original performance.</li>
<li>methods: Combining core subgraph and sparse subnetwork, using an adversarial complementary erasing (ACE) framework to explore valuable information from pruned components.</li>
<li>results: Outperforming existing methods for searching Graph Lottery Ticket (GLT) in diverse tasks.<details>
<summary>Abstract</summary>
Graph Lottery Ticket (GLT), a combination of core subgraph and sparse subnetwork, has been proposed to mitigate the computational cost of deep Graph Neural Networks (GNNs) on large input graphs while preserving original performance. However, the winning GLTs in exisiting studies are obtained by applying iterative magnitude-based pruning (IMP) without re-evaluating and re-considering the pruned information, which disregards the dynamic changes in the significance of edges/weights during graph/model structure pruning, and thus limits the appeal of the winning tickets. In this paper, we formulate a conjecture, i.e., existing overlooked valuable information in the pruned graph connections and model parameters which can be re-grouped into GLT to enhance the final performance. Specifically, we propose an adversarial complementary erasing (ACE) framework to explore the valuable information from the pruned components, thereby developing a more powerful GLT, referred to as the ACE-GLT. The main idea is to mine valuable information from pruned edges/weights after each round of IMP, and employ the ACE technique to refine the GLT processing. Finally, experimental results demonstrate that our ACE-GLT outperforms existing methods for searching GLT in diverse tasks. Our code will be made publicly available.
</details>
<details>
<summary>摘要</summary>
《图lottery票（GLT）》，一种组合核心子图和稀疏子网络的方法，用于降低深度图神经网络（GNNs）在大输入图上的计算成本，保持原始性能。然而，现有的研究中的赢家GLT是通过迭代矩阵优化（IMP）而不是重新评估和重新考虑被截割的信息，这会忽略图/模型结构截割中 Edge/权重的动态变化，因此限制了赢家票的吸引力。在这篇论文中，我们提出一个假设，即现有的被遗弃的有价信息在截割的图连接和模型参数中，可以重新组织成GLT，以提高最终性能。 Specifically, we propose an adversarial complementary erasing（ACE）框架，用于探索截割后每个回合的有价信息，并使用ACE技术来细化GLT处理。最后，我们的实验结果表明，我们的ACE-GLT在多种任务中超过现有方法搜索GLT的性能。我们的代码将公开发布。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/06/cs.LG_2023_08_06/" data-id="clly4xtdr0069vl88b1qh6rrf" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/06/cs.SD_2023_08_06/" class="article-date">
  <time datetime="2023-08-05T16:00:00.000Z" itemprop="datePublished">2023-08-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/06/cs.SD_2023_08_06/">cs.SD - 2023-08-06 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="SoK-Acoustic-Side-Channels"><a href="#SoK-Acoustic-Side-Channels" class="headerlink" title="SoK: Acoustic Side Channels"></a>SoK: Acoustic Side Channels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03806">http://arxiv.org/abs/2308.03806</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ping Wang, Shishir Nagaraja, Aurélien Bourquard, Haichang Gao, Jeff Yan</li>
<li>for: 本研究做出了一个 state-of-the-art 的分析，涵盖了全部重要的学术研究领域，讨论了安全性含义和对策，并寻找了未来研究的方向。</li>
<li>methods: 本研究使用了多种方法，包括对渠道的分析、对 inverse problems 的研究以及两者之间的连接。</li>
<li>results: 本研究得到了一些深刻的结论，包括渠道的安全性含义和未来研究的方向。<details>
<summary>Abstract</summary>
We provide a state-of-the-art analysis of acoustic side channels, cover all the significant academic research in the area, discuss their security implications and countermeasures, and identify areas for future research. We also make an attempt to bridge side channels and inverse problems, two fields that appear to be completely isolated from each other but have deep connections.
</details>
<details>
<summary>摘要</summary>
我们提供了当今最先进的声学侧途分析，涵盖了全部主要的学术研究领域，讨论了它们的安全意义和防范措施，并确定了未来研究的方向。我们还尝试将侧途和反问题两个领域联系起来，这两个领域之前被视为完全不相关的，但它们在深层次上有着深刻的联系。
</details></li>
</ul>
<hr>
<h2 id="Characterization-of-cough-sounds-using-statistical-analysis"><a href="#Characterization-of-cough-sounds-using-statistical-analysis" class="headerlink" title="Characterization of cough sounds using statistical analysis"></a>Characterization of cough sounds using statistical analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03019">http://arxiv.org/abs/2308.03019</a></li>
<li>repo_url: None</li>
<li>paper_authors: Naveenkumar Vodnala, Pratap Reddy Lankireddy, Padmasai Yarlagadda</li>
<li>For: 本研究旨在Characterize cough sounds with voiced content and cough sounds without voiced content，以便更好地诊断呼吸疾病。* Methods: 该研究使用spectral roll-off、spectral entropy、spectral flatness、spectral flux、zero crossing rate、spectral centroid和spectral bandwidth属性来描述呼吸 зву频谱特征。这些属性 subsequentially subjected to statistical analysis using measures of minimum, maximum, mean, median, and standard deviation。* Results: 实验结果表明，呼吸音频谱特征的mean和frequency distribution高于speech signals，spectral flatness水平在0.22左右，spectral flux在0.3-0.6之间，Zero Crossing Rate大约在0.05-0.4之间。这些属性具有重要的信息价值，可以帮助更好地Characterize cough sounds。<details>
<summary>Abstract</summary>
Cough is a primary symptom of most respiratory diseases, and changes in cough characteristics provide valuable information for diagnosing respiratory diseases. The characterization of cough sounds still lacks concrete evidence, which makes it difficult to accurately distinguish between different types of coughs and other sounds. The objective of this research work is to characterize cough sounds with voiced content and cough sounds without voiced content. Further, the cough sound characteristics are compared with the characteristics of speech. The proposed method to achieve this goal utilized spectral roll-off, spectral entropy, spectral flatness, spectral flux, zero crossing rate, spectral centroid, and spectral bandwidth attributes which describe the cough sounds related to the respiratory system, glottal information, and voice model. These attributes are then subjected to statistical analysis using the measures of minimum, maximum, mean, median, and standard deviation. The experimental results show that the mean and frequency distribution of spectral roll-off, spectral centroid, and spectral bandwidth are found to be higher for cough sounds than for speech signals. Spectral flatness levels in cough sounds will rise to 0.22, whereas spectral flux varies between 0.3 and 0.6. The Zero Crossing Rate (ZCR) of most frames of cough sounds is between 0.05 and 0.4. These attributes contribute significant information while characterizing cough sounds.
</details>
<details>
<summary>摘要</summary>
<SYS>将文本翻译成简化中文。</SYS>吐は多种呼吸疾病的主要症状之一，而吐的特征变化可以提供诊断呼吸疾病的有价值信息。然而，吐 зву的特征化仍然缺乏具体证据，这使得准确地分辨吐音和其他声音变得困难。本研究的目标是Characterize吐音与有声吐音。此外，吐音特征与语音特征进行比较。提出的方法是利用spectral roll-off、spectral entropy、spectral flatness、spectral flux、zero crossing rate、spectral centroid和spectral bandwidth属性来描述吐音，这些属性是基于呼吸系统、舌叶信息和语音模型。这些属性后来通过统计分析使用 minimum、maximum、mean、median和标准差度量进行评估。实验结果表明，吐音的mean和频谱分布的 spectral roll-off、spectral centroid和spectral bandwidth均高于语音信号。吐音中的spectral flatness水平上升到0.22，而spectral flux在0.3和0.6之间变化。zero crossing rate的大多数帧为0.05和0.4。这些属性对吐音 caracterization提供了重要信息。
</details></li>
</ul>
<hr>
<h2 id="DiffDance-Cascaded-Human-Motion-Diffusion-Model-for-Dance-Generation"><a href="#DiffDance-Cascaded-Human-Motion-Diffusion-Model-for-Dance-Generation" class="headerlink" title="DiffDance: Cascaded Human Motion Diffusion Model for Dance Generation"></a>DiffDance: Cascaded Human Motion Diffusion Model for Dance Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02915">http://arxiv.org/abs/2308.02915</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiaosong Qi, Le Zhuo, Aixi Zhang, Yue Liao, Fei Fang, Si Liu, Shuicheng Yan</li>
<li>for: 本研究旨在生成高分辨率、长形舞蹈序列，以便与音乐进行 conditional generation。</li>
<li>methods: 我们提出了一种新的层次动态扩散模型，即 DiffDance，以解决传统autoregressive方法在采样中引入折衔错误和长期结构捕捉的限制。我们还采用了多种几何损失来限制模型输出的物理可能性，并在扩散过程中采用动态权重来促进样本多样性。</li>
<li>results: 我们通过对 AIST++ 数据集进行 comprehensive 试验，证明了 DiffDance 能够生成与输入音乐高效对齐的实际舞蹈序列，并与状态静态方法相当。<details>
<summary>Abstract</summary>
When hearing music, it is natural for people to dance to its rhythm. Automatic dance generation, however, is a challenging task due to the physical constraints of human motion and rhythmic alignment with target music. Conventional autoregressive methods introduce compounding errors during sampling and struggle to capture the long-term structure of dance sequences. To address these limitations, we present a novel cascaded motion diffusion model, DiffDance, designed for high-resolution, long-form dance generation. This model comprises a music-to-dance diffusion model and a sequence super-resolution diffusion model. To bridge the gap between music and motion for conditional generation, DiffDance employs a pretrained audio representation learning model to extract music embeddings and further align its embedding space to motion via contrastive loss. During training our cascaded diffusion model, we also incorporate multiple geometric losses to constrain the model outputs to be physically plausible and add a dynamic loss weight that adaptively changes over diffusion timesteps to facilitate sample diversity. Through comprehensive experiments performed on the benchmark dataset AIST++, we demonstrate that DiffDance is capable of generating realistic dance sequences that align effectively with the input music. These results are comparable to those achieved by state-of-the-art autoregressive methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Anonymizing-Speech-Evaluating-and-Designing-Speaker-Anonymization-Techniques"><a href="#Anonymizing-Speech-Evaluating-and-Designing-Speaker-Anonymization-Techniques" class="headerlink" title="Anonymizing Speech: Evaluating and Designing Speaker Anonymization Techniques"></a>Anonymizing Speech: Evaluating and Designing Speaker Anonymization Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04455">http://arxiv.org/abs/2308.04455</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deep-privacy/SA-toolkit">https://github.com/deep-privacy/SA-toolkit</a></li>
<li>paper_authors: Pierre Champion<br>for:  This thesis proposes solutions for anonymizing speech and evaluating the degree of the anonymization to address the privacy issues arising from the collection and storage of personal speech data in voice-based digital assistants.methods: The thesis employs a combination of voice conversion-based anonymization systems and quantization-based transformation methods to reduce speaker PPI while maintaining utility.results: The thesis evaluates the degree of privacy protection provided by these methods and proposes a new attack method to invert anonymization, highlighting the limitations of current anonymization systems and identifying areas for improvement.<details>
<summary>Abstract</summary>
The growing use of voice user interfaces has led to a surge in the collection and storage of speech data. While data collection allows for the development of efficient tools powering most speech services, it also poses serious privacy issues for users as centralized storage makes private personal speech data vulnerable to cyber threats. With the increasing use of voice-based digital assistants like Amazon's Alexa, Google's Home, and Apple's Siri, and with the increasing ease with which personal speech data can be collected, the risk of malicious use of voice-cloning and speaker/gender/pathological/etc. recognition has increased.   This thesis proposes solutions for anonymizing speech and evaluating the degree of the anonymization. In this work, anonymization refers to making personal speech data unlinkable to an identity while maintaining the usefulness (utility) of the speech signal (e.g., access to linguistic content). We start by identifying several challenges that evaluation protocols need to consider to evaluate the degree of privacy protection properly. We clarify how anonymization systems must be configured for evaluation purposes and highlight that many practical deployment configurations do not permit privacy evaluation. Furthermore, we study and examine the most common voice conversion-based anonymization system and identify its weak points before suggesting new methods to overcome some limitations. We isolate all components of the anonymization system to evaluate the degree of speaker PPI associated with each of them. Then, we propose several transformation methods for each component to reduce as much as possible speaker PPI while maintaining utility. We promote anonymization algorithms based on quantization-based transformation as an alternative to the most-used and well-known noise-based approach. Finally, we endeavor a new attack method to invert anonymization.
</details>
<details>
<summary>摘要</summary>
声音用户界面的使用量在增长，导致了个人语音数据的收集和存储。而这些数据的收集可以为语音服务的开发提供高效的工具，但也会对用户的隐私造成严重的威胁，因为中央存储的私人语音数据容易受到网络攻击。随着智能语音助手 like Amazon Alexa、Google Home 和 Apple Siri 的使用的加密，以及个人语音数据的收集变得更加容易，黑客利用语音恶作剂和 speaker/性别/疾病等识别的风险也在增加。本论目标是提出一种隐藏个人语音数据的方法，以保护用户的隐私。在这个过程中，我们认为需要考虑以下几个挑战：1. 评估隐私保护程度：我们需要设计一种评估隐私保护程度的方法，以确保个人语音数据不可链接到用户身份。2. 配置评估系统：我们需要配置评估系统，以便在实际应用中进行评估。3. 避免攻击：我们需要研究和探讨常见的语音转换基于隐藏的攻击方法，并提出新的方法来解决一些限制。我们认为，以下几个方法可以用于隐藏个人语音数据：1. 量化变换：我们可以使用量化变换来减少说话人的个人特征，以保护用户的隐私。2. 隐藏语音特征：我们可以使用隐藏语音特征的技术来隐藏个人语音数据，以降低黑客的攻击风险。3. 多模型融合：我们可以使用多个模型来融合语音数据，以提高隐私保护的效果。最后，我们提出了一种新的攻击方法，可以尝试将隐藏后的语音数据恢复到原始形式。这种攻击方法可以帮助我们更好地理解隐私保护的限制，并提高隐私保护的效果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/06/cs.SD_2023_08_06/" data-id="clly4xten009mvl8833oo5ag2" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/8/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="page-number" href="/page/8/">8</a><span class="page-number current">9</span><a class="page-number" href="/page/10/">10</a><a class="page-number" href="/page/11/">11</a><span class="space">&hellip;</span><a class="page-number" href="/page/28/">28</a><a class="extend next" rel="next" href="/page/10/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">59</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">55</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">108</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
