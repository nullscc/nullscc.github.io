
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/38/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-eess.SP_2023_10_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/01/eess.SP_2023_10_01/" class="article-date">
  <time datetime="2023-10-01T08:00:00.000Z" itemprop="datePublished">2023-10-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/01/eess.SP_2023_10_01/">eess.SP - 2023-10-01</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="DISCO-Might-Not-Be-Funky-Random-Intelligent-Reflective-Surface-Configurations-That-Attack"><a href="#DISCO-Might-Not-Be-Funky-Random-Intelligent-Reflective-Surface-Configurations-That-Attack" class="headerlink" title="DISCO Might Not Be Funky: Random Intelligent Reflective Surface Configurations That Attack"></a>DISCO Might Not Be Funky: Random Intelligent Reflective Surface Configurations That Attack</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00687">http://arxiv.org/abs/2310.00687</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huan Huang, Lipeng Dai, Hongliang Zhang, Chongfu Zhang, Zhongxing Tian, Yi Cai, A. Lee Swindlehurst, Zhu Han</li>
<li>for: 这篇论文是关于恶意反射表面（IRS）对物理层安全（PLS）的研究。</li>
<li>methods: 这篇论文提出了一种基于恶意IRS的完全游离式干扰器（FPJ），并介绍了其应用场景和技术原理。</li>
<li>results: 这篇论文提出了一种不需要干扰功率或渠道状态信息（CSI）的恶意干扰器，并提出了一种基于统计的反干扰策略。此外，论文还介绍了一种能够在存在恶意干扰的情况下估计统计CSI的数据帧结构。<details>
<summary>Abstract</summary>
Emerging intelligent reflective surfaces (IRSs) significantly improve system performance, but also pose a signifcant risk for physical layer security (PLS). Unlike the extensive research on legitimate IRS-enhanced communications, in this article we present an adversarial IRS-based fully-passive jammer (FPJ). We describe typical application scenarios for Disco IRS (DIRS)-based FPJ, where an illegitimate IRS with random, time-varying reflection properties acts like a "disco ball" to randomly change the propagation environment. We introduce the principles of DIRS-based FPJ and overview existing investigations of the technology, including a design example employing one-bit phase shifters. The DIRS-based FPJ can be implemented without either jamming power or channel state information (CSI) for the legitimate users (LUs). It does not suffer from the energy constraints of traditional active jammers, nor does it require any knowledge of the LU channels. In addition to the proposed jamming attack, we also propose an anti-jamming strategy that requires only statistical rather than instantaneous CSI. Furthermore, we present a data frame structure that enables the legitimate access point (AP) to estimate the statistical CSI in the presence of the DIRS jamming. Typical cases are discussed to show the impact of the DIRS-based FPJ and the feasibility of the anti-jamming precoder. Moreover, we outline future research directions and challenges for the DIRS-based FPJ and its anti-jamming precoding to stimulate this line of research and pave the way for practical applications.
</details>
<details>
<summary>摘要</summary>
emerging intelligent reflective surfaces (IRSs) significantly improve system performance, but also pose a significant risk for physical layer security (PLS). unlike the extensive research on legitimate IRS-enhanced communications, in this article we present an adversarial IRS-based fully-passive jammer (FPJ). we describe typical application scenarios for Disco IRS (DIRS)-based FPJ, where an illegitimate IRS with random, time-varying reflection properties acts like a "disco ball" to randomly change the propagation environment. we introduce the principles of DIRS-based FPJ and overview existing investigations of the technology, including a design example employing one-bit phase shifters. the DIRS-based FPJ can be implemented without either jamming power or channel state information (CSI) for the legitimate users (LUs). it does not suffer from the energy constraints of traditional active jammers, nor does it require any knowledge of the LU channels. in addition to the proposed jamming attack, we also propose an anti-jamming strategy that requires only statistical rather than instantaneous CSI. furthermore, we present a data frame structure that enables the legitimate access point (AP) to estimate the statistical CSI in the presence of the DIRS jamming. typical cases are discussed to show the impact of the DIRS-based FPJ and the feasibility of the anti-jamming precoder. moreover, we outline future research directions and challenges for the DIRS-based FPJ and its anti-jamming precoding to stimulate this line of research and pave the way for practical applications.
</details></li>
</ul>
<hr>
<h2 id="Sequential-Monte-Carlo-Graph-Convolutional-Network-for-Dynamic-Brain-Connectivity"><a href="#Sequential-Monte-Carlo-Graph-Convolutional-Network-for-Dynamic-Brain-Connectivity" class="headerlink" title="Sequential Monte Carlo Graph Convolutional Network for Dynamic Brain Connectivity"></a>Sequential Monte Carlo Graph Convolutional Network for Dynamic Brain Connectivity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00630">http://arxiv.org/abs/2310.00630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fengfan Zhao, Ercan Engin Kuruoglu</li>
<li>for: 这项研究旨在提出一种基于粒子滤波算法的功能连接分析方法，用于探索脑功能缺陷和脑疾病相关的结构性破坏之间的关系。</li>
<li>methods: 该方法基于粒子滤波算法，可以在只有部分和噪声的观察数据情况下，不假设站立性的连接topology，并通过Sequential Monte Carlo Graph Convolutional Network (SMC-GCN)来限制干扰连接。</li>
<li>results: 实验研究表明，SMC-GCN方法在脑疾病分类任务中表现出色，超过了其他方法的性能。<details>
<summary>Abstract</summary>
An increasingly important brain function analysis modality is functional connectivity analysis which regards connections as statistical codependency between the signals of different brain regions. Graph-based analysis of brain connectivity provides a new way of exploring the association between brain functional deficits and the structural disruption related to brain disorders, but the current implementations have limited capability due to the assumptions of noise-free data and stationary graph topology. We propose a new methodology based on the particle filtering algorithm, with proven success in tracking problems, which estimates the hidden states of a dynamic graph with only partial and noisy observations, without the assumptions of stationarity on connectivity. We enrich the particle filtering state equation with a graph Neural Network called Sequential Monte Carlo Graph Convolutional Network (SMC-GCN), which due to the nonlinear regression capability, can limit spurious connections in the graph. Experiment studies demonstrate that SMC-GCN achieves the superior performance of several methods in brain disorder classification.
</details>
<details>
<summary>摘要</summary>
▼ 请注意，以下文本将被翻译成简化中文。一种日益重要的大脑功能分析方法是函数连接分析，它视连接为脑区域信号的统计 codependency。基于图的Brain Connectivity分析提供了一种探索脑功能缺陷和脑疾病相关的结构性破坏的新方法，但现有实现受限因为假设了噪声自由数据和静止的图表结构。我们提出了一种基于粒子滤波算法的新方法，该算法在跟踪问题中证明了成功，可以在只有部分和噪声的观察数据情况下估计图中隐藏的状态。我们在粒子滤波状态方程中添加了一种图神经网络 called Sequential Monte Carlo Graph Convolutional Network (SMC-GCN)，该网络具有非线性回归能力，可以限制图中的假设连接。实验研究表明，SMC-GCN可以在脑疾病分类方面达到更高的性能。
</details></li>
</ul>
<hr>
<h2 id="Nonlinear-Multi-Carrier-System-with-Signal-Clipping-Measurement-Analysis-and-Optimization"><a href="#Nonlinear-Multi-Carrier-System-with-Signal-Clipping-Measurement-Analysis-and-Optimization" class="headerlink" title="Nonlinear Multi-Carrier System with Signal Clipping: Measurement, Analysis, and Optimization"></a>Nonlinear Multi-Carrier System with Signal Clipping: Measurement, Analysis, and Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00593">http://arxiv.org/abs/2310.00593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuyang Du, Liang Hao, Yiming Lei</li>
<li>for: 降低OFDM系统中的峰峰值至平均值比率 (PAPR)</li>
<li>methods: 使用BFPA模型分析发射器非线性，并通过干扰产品分析简化发射器电压表达</li>
<li>results: 对非线性剪辑OFDM系统进行优化设计，以实现实际系统中的SER下界In English:</li>
<li>for: Reducing the peak-to-average power ratio (PAPR) in OFDM systems</li>
<li>methods: Using the Bessel-Fourier PA (BFPA) model to analyze the nonlinearity of the power amplifier (PA), and simplifying the power expression using inter-modulation product (IMP) analysis</li>
<li>results: Optimizing the system setting for a nonlinear clipped OFDM system to achieve the symbol error rate (SER) lower bound in a practical system that considers both PA nonlinearity and clipping distortion.<details>
<summary>Abstract</summary>
Signal clipping is a classic technique for reducing peak-to-average power ratio (PAPR) in orthogonal frequency division multiplexing (OFDM) systems. It has been widely applied in consumer electronic devices owing to its low complexity and high efficiency. Although clipping reduces the nonlinear distortion caused by power amplifiers (PAs), it induces additional clipping distortion. Optimizing the joint system performance with consideration of both PA nonlinearity and clipping distortion remains an open problem due to the complex PA modeling. In this paper, we analyze the PA nonlinearity through the Bessel-Fourier PA (BFPA) model and simplify its power expression using inter-modulation product (IMP) analysis. We derive expressions of the receiver signal-to-noise ratio (SNR) and system symbol error rate (SER) for the nonlinear clipped OFDM system. With the derivations, we investigate the optimal system setting to achieve the SER lower bound in a practical OFDM system that considers both PA nonlinearity and clipping distortion. The methods and results presented in this paper can serve as a useful reference for the system-level optimization of clipped OFDM systems with nonlinear PA.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXTSignal clipping is a classic technique for reducing peak-to-average power ratio (PAPR) in orthogonal frequency division multiplexing (OFDM) systems. It has been widely applied in consumer electronic devices owing to its low complexity and high efficiency. Although clipping reduces the nonlinear distortion caused by power amplifiers (PAs), it induces additional clipping distortion. Optimizing the joint system performance with consideration of both PA nonlinearity and clipping distortion remains an open problem due to the complex PA modeling. In this paper, we analyze the PA nonlinearity through the Bessel-Fourier PA (BFPA) model and simplify its power expression using inter-modulation product (IMP) analysis. We derive expressions of the receiver signal-to-noise ratio (SNR) and system symbol error rate (SER) for the nonlinear clipped OFDM system. With the derivations, we investigate the optimal system setting to achieve the SER lower bound in a practical OFDM system that considers both PA nonlinearity and clipping distortion. The methods and results presented in this paper can serve as a useful reference for the system-level optimization of clipped OFDM systems with nonlinear PA.TRANSLATE_TEXT
</details></li>
</ul>
<hr>
<h2 id="An-IRS-Assisted-Secure-Dual-Function-Radar-Communication-System"><a href="#An-IRS-Assisted-Secure-Dual-Function-Radar-Communication-System" class="headerlink" title="An IRS-Assisted Secure Dual-Function Radar-Communication System"></a>An IRS-Assisted Secure Dual-Function Radar-Communication System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00555">http://arxiv.org/abs/2310.00555</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi-Kai Li, Athina Petropulu</li>
<li>for: 提高双功能雷达通信系统的物理层安全性（PLS）</li>
<li>methods: 使用智能反射 superficie（IRS）和人工噪声（AN），并optimize the radar waveform, AN jamming noise, and IRS parameters to maximize the communication secrecy rate while meeting radar signal-to-noise ratio（SNR） constraints.</li>
<li>results: 提出一种新的系统设计方案，并使用分数编程技术将分数形目标函数转化为更易处理的非分数多项式。数值结果表明系统设计算法的收敛性，并显示了噪声分配对系统安全性的影响。<details>
<summary>Abstract</summary>
In dual-function radar-communication (DFRC) systems the probing signal contains information intended for the communication users, which makes that information vulnerable to eavesdropping by the targets. We propose a novel design for enhancing the physical layer security (PLS) of DFRC systems, via the help of intelligent reflecting surface (IRS) and artificial noise (AN), transmitted along with the probing waveform. The radar waveform, the AN jamming noise and the IRS parameters are designed to optimize the communication secrecy rate while meeting radar signal-to-noise ratio (SNR) constrains. Key challenges in the resulting optimization problem include the fractional form objective, the SNR being a quartic function of the IRS parameters, and the unit-modulus constraint of the IRS parameters. A fractional programming technique is used to transform the fractional form objective of the optimization problem into more tractable non-fractional polynomials. Numerical results are provided to demonstrate the convergence of the proposed system design algorithm, and also show the impact of the power assigned to the AN on the secrecy performance of the designed system.
</details>
<details>
<summary>摘要</summary>
在双功能雷达通信（DFRC）系统中，探测信号包含向通信用户传递的信息，因此这些信息容易受到目标的窃听。我们提议一种新的设计方案，以增强双功能雷达通信系统的物理层安全性（PLS），通过利用智能反射表面（IRS）和人工噪声（AN），同探测波形一起传输。雷达波形、噪声干扰和IRS参数是根据优化通信秘密率的要求，同时满足雷达信号响应比（SNR）的限制。关键挑战包括分数形目标函数、SNR为IRS参数的四次函数，以及IRS参数的单位模式约束。我们使用分数编程技术将分数形目标函数转换为更易处理的非分数多项式。numerical results show that the proposed system design algorithm converges and demonstrate the impact of the power assigned to the AN on the secrecy performance of the designed system.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. If you prefer Traditional Chinese, please let me know and I will be happy to provide the translation in that form instead.
</details></li>
</ul>
<hr>
<h2 id="An-Experimental-Prototype-for-Multistatic-Asynchronous-ISAC"><a href="#An-Experimental-Prototype-for-Multistatic-Asynchronous-ISAC" class="headerlink" title="An Experimental Prototype for Multistatic Asynchronous ISAC"></a>An Experimental Prototype for Multistatic Asynchronous ISAC</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00548">http://arxiv.org/abs/2310.00548</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Canil, Jacopo Pegoraro, Jesus O. Lacruz, Marco Mezzavilla, Michele Rossi, Joerg Widmer, Sundeep Rangan</li>
<li>for: 该论文旨在实现基于IEEE802.11ay的多Static millimeter wave ISAC系统，并 validate its performance。</li>
<li>methods: 该系统使用了单线对线（LoS）无线信号媒体进行时钟偏差补偿，以实现同时进行目标跟踪和微多普勒估计。</li>
<li>results: 实验结果表明，多Static ISAC系统可以提供更高的感知能力，具有多视角的接收节点空间多样性。<details>
<summary>Abstract</summary>
We prototype and validate a multistatic mmWave ISAC system based on IEEE802.11ay. Compensation of the clock asynchrony between each TX and RX pair is performed using the sole LoS wireless signal propagation. As a result, our system provides concurrent target tracking and micro-Doppler estimation from multiple points of view, paving the way for practical multistatic data fusion. Our results on human movement sensing, complemented with precise, quantitative GT data, demonstrate the enhanced sensing capabilities of multistatic ISAC, due to the spatial diversity of the receiver nodes.
</details>
<details>
<summary>摘要</summary>
我们研究和验证了一个基于IEEE802.11ay的多态 millimeter wave ISAC系统。我们使用唯一的视线无线信号媒体进行时钟偏差补偿，因此我们的系统可以同时进行目标跟踪和微多普勒估算，从多个视点来源获得实用的数据融合。我们对人体运动感知进行了补充，并且通过精确的量化GT数据，示出了多态 ISAC的感知能力的增强，即因为接收节点的空间多样性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/01/eess.SP_2023_10_01/" data-id="clp869u9h01fnk588ej7p11vf" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_09_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/30/cs.SD_2023_09_30/" class="article-date">
  <time datetime="2023-09-30T15:00:00.000Z" itemprop="datePublished">2023-09-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/30/cs.SD_2023_09_30/">cs.SD - 2023-09-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Time-Variant-Overlap-Add-in-Partitions"><a href="#Time-Variant-Overlap-Add-in-Partitions" class="headerlink" title="Time-Variant Overlap-Add in Partitions"></a>Time-Variant Overlap-Add in Partitions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00319">http://arxiv.org/abs/2310.00319</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/TGM-Oldenburg/TVOLAP">https://github.com/TGM-Oldenburg/TVOLAP</a></li>
<li>paper_authors: Hagen Jaeger, Uwe Simmer, Jörg Bitzer, Matthias Blau</li>
<li>for: 这篇论文是关于用于虚拟和增强现实环境中的听音渲染技术的研究。</li>
<li>methods: 该论文提出了一种分解式卷积算法，可以在实时中快速切换各种响应函数，而无需产生明显的切换artifacts，同时保持常见计算成本和内存占用量。</li>
<li>results: 该算法在多种popular编程语言中的实现可以免除听觉switching artifacts，并且可以保持常见计算成本和内存占用量。代码可以在GitHub上免费下载。<details>
<summary>Abstract</summary>
Virtual and augmented realities are increasingly popular tools in many domains such as architecture, production, training and education, (psycho)therapy, gaming, and others. For a convincing rendering of sound in virtual and augmented environments, audio signals must be convolved in real-time with impulse responses that change from one moment in time to another. Key requirements for the implementation of such time-variant real-time convolution algorithms are short latencies, moderate computational cost and memory footprint, and no perceptible switching artifacts. In this engineering report, we introduce a partitioned convolution algorithm that is able to quickly switch between impulse responses without introducing perceptible artifacts, while maintaining a constant computational load and low memory usage. Implementations in several popular programming languages are freely available via GitHub.
</details>
<details>
<summary>摘要</summary>
虚拟和增强现实技术在多个领域得到了广泛应用，如建筑、生产、培训和教育、心理治疗、游戏等。为在虚拟和增强环境中提供真实的声音渲染，音频信号需要在实时中扩散到不同的冲击回应函数，这些函数在时间上变化。实现时变实时扩散算法的关键要求包括：短延迟时间、moderate计算成本和内存占用量，无法识别的切换 artifacts。本工程报告中，我们介绍了一种分解扩散算法，可以快速切换冲击回应函数，而无需引入明显的artefacts，同时保持了常量计算负担和内存占用量。实现在多种popular编程语言上可以免费获取于GitHub。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-U-Net-Architecture-for-Denoising-of-Real-world-Noise-Corrupted-Phonocardiogram-Signal"><a href="#A-Novel-U-Net-Architecture-for-Denoising-of-Real-world-Noise-Corrupted-Phonocardiogram-Signal" class="headerlink" title="A Novel U-Net Architecture for Denoising of Real-world Noise Corrupted Phonocardiogram Signal"></a>A Novel U-Net Architecture for Denoising of Real-world Noise Corrupted Phonocardiogram Signal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00216">http://arxiv.org/abs/2310.00216</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ayan Mukherjee, Rohan Banerjee, Avik Ghose</li>
<li>For: 本研究旨在提出一种基于U-Net深度神经网络架构的心听音信号杂谔除去方法，以解决在医学 auscultation 中心听音信号杂谔问题。* Methods: 为了设计、开发和验证提议的架构，我们提出了一种新的实验方法，利用现实世界噪声污染的PCG信号 DATASET 和一个开放式PCG DATASET。* Results: 对比与现有状态的先进技术，我们的杂谔除去方法在Synthesized noisy PCG DATASET 上的性能评估表明，提出的方法在识别和预测方面具有显著的改进。<details>
<summary>Abstract</summary>
The bio-acoustic information contained within heart sound signals are utilized by physicians world-wide for auscultation purpose. However, the heart sounds are inherently susceptible to noise contamination. Various sources of noises like lung sound, coughing, sneezing, and other background noises are involved in such contamination. Such corruption of the heart sound signal often leads to inconclusive or false diagnosis. To address this issue, we have proposed a novel U-Net based deep neural network architecture for denoising of phonocardiogram (PCG) signal in this paper. For the design, development and validation of the proposed architecture, a novel approach of synthesizing real-world noise corrupted PCG signals have been proposed. For the purpose, an open-access real-world noise sample dataset and an open-access PCG dataset has been utilized. The performance of the proposed denoising methodology has been evaluated on the synthesized noisy PCG dataset. The performance of the proposed algorithm has been compared with existing state-of-the-art (SoA) denoising algorithms qualitatively and quantitatively. The proposed denoising technique has shown improvement in performance as comparison to the SoAs.
</details>
<details>
<summary>摘要</summary>
生物声学信息在心声信号中含有，医生世界各地通过 auscultation 来利用这些信息。然而，心声信号具有自然潜在的噪声污染。这些噪声包括肺 зву、喷嚏、喷嚏、和其他背景噪声等。这种噪声污染可能导致不正确或不 conclution 的诊断。为解决这个问题，我们在本文中提出了一种基于 U-Net 深度神经网络架构的PCG 信号杂音除除法。为了设计、开发和验证该架构，我们提出了一种新的实际噪声污染 PCG 信号生成方法。为此，我们使用了一个开放访问的实际噪声样本数据集和一个开放访问的 PCG 数据集。我们对提出的杂音除法方法进行评估，并与现有状态的最佳方法（SoA）进行比较。我们发现，提出的杂音除法方法在比较 SoA 方法时显示出了改善的性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/30/cs.SD_2023_09_30/" data-id="clp869u3b00zyk5889ori6xmo" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_09_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/30/cs.CV_2023_09_30/" class="article-date">
  <time datetime="2023-09-30T13:00:00.000Z" itemprop="datePublished">2023-09-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/30/cs.CV_2023_09_30/">cs.CV - 2023-09-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Assessing-the-Generalizability-of-Deep-Neural-Networks-Based-Models-for-Black-Skin-Lesions"><a href="#Assessing-the-Generalizability-of-Deep-Neural-Networks-Based-Models-for-Black-Skin-Lesions" class="headerlink" title="Assessing the Generalizability of Deep Neural Networks-Based Models for Black Skin Lesions"></a>Assessing the Generalizability of Deep Neural Networks-Based Models for Black Skin Lesions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00517">http://arxiv.org/abs/2310.00517</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/httplups/black-acral-skin-lesion-detection">https://github.com/httplups/black-acral-skin-lesion-detection</a></li>
<li>paper_authors: Luana Barros, Levy Chaves, Sandra Avila</li>
<li>for: 这个论文主要用于检测皮肤癌症，特别是针对黑人群体中的肤色区域（手掌、足底和指甲）。</li>
<li>methods: 该论文使用深度神经网络进行检测，并分别评估了指导式和自我指导式模型在黑人肤色区域中的表现。</li>
<li>results: 研究发现，现有的深度神经网络模型在黑人肤色区域中的性能不佳，只能在白皮肤区域中表现出色。这显示了这些模型在不同肤色区域中的一致性不足。<details>
<summary>Abstract</summary>
Melanoma is the most severe type of skin cancer due to its ability to cause metastasis. It is more common in black people, often affecting acral regions: palms, soles, and nails. Deep neural networks have shown tremendous potential for improving clinical care and skin cancer diagnosis. Nevertheless, prevailing studies predominantly rely on datasets of white skin tones, neglecting to report diagnostic outcomes for diverse patient skin tones. In this work, we evaluate supervised and self-supervised models in skin lesion images extracted from acral regions commonly observed in black individuals. Also, we carefully curate a dataset containing skin lesions in acral regions and assess the datasets concerning the Fitzpatrick scale to verify performance on black skin. Our results expose the poor generalizability of these models, revealing their favorable performance for lesions on white skin. Neglecting to create diverse datasets, which necessitates the development of specialized models, is unacceptable. Deep neural networks have great potential to improve diagnosis, particularly for populations with limited access to dermatology. However, including black skin lesions is necessary to ensure these populations can access the benefits of inclusive technology.
</details>
<details>
<summary>摘要</summary>
癌症是皮肤癌症中最严重的一种，因为它可以导致肿瘤迁移。它更常见于黑人，通常会影响到手掌、脚底和指甲。深度神经网络在临床护理和皮肤癌诊断方面表现出了巨大的潜力。然而，现有的研究大多涉及白皮肤Dataset，忽略了不同皮肤颜色的患者诊断结果的报告。在这项工作中，我们评估了指导和自动化模型在黑人常见的手掌、脚底和指甲部位上的皮肤癌图像中的表现。此外，我们也仔细筛选了包含黑人皮肤癌图像的Dataset，并评估了该Dataset在Fitzpatrick级别中的表现，以确认模型在黑皮肤上的性能。我们的结果表明，现有的模型在白皮肤上表现良好，但对黑皮肤的患者来说，这些模型的总体性能很差。忽略创建多样化的Dataset是不可接受的。深度神经网络在护理方面具有极大的潜力，特别是对于有限的资源的人群，但是包含黑皮肤癌图像是必要的，以确保这些人群可以通过包容技术获得诊断的优势。
</details></li>
</ul>
<hr>
<h2 id="Exploring-SAM-Ablations-for-Enhancing-Medical-Segmentation-in-Radiology-and-Pathology"><a href="#Exploring-SAM-Ablations-for-Enhancing-Medical-Segmentation-in-Radiology-and-Pathology" class="headerlink" title="Exploring SAM Ablations for Enhancing Medical Segmentation in Radiology and Pathology"></a>Exploring SAM Ablations for Enhancing Medical Segmentation in Radiology and Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00504">http://arxiv.org/abs/2310.00504</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amin Ranem, Niklas Babendererde, Moritz Fuchs, Anirban Mukhopadhyay</li>
<li>for: 本研究旨在探讨Segment Anything Model（SAM）在不同领域中的应用，以提高准确性和可靠性。</li>
<li>methods: 本研究使用SAM框架，分析其基本组件与它们之间的复杂交互，并对其进行精细调整以提高 segmentation 结果的准确性。</li>
<li>results: 经过系列仔细设计的实验表明，SAM在放射学（特别是脑肿瘤 segmentation）和病理学（特别是乳腺癌 segmentation）中的应用具有很高的潜力，可以帮助解决医学影像分 segmentation 的挑战。<details>
<summary>Abstract</summary>
Medical imaging plays a critical role in the diagnosis and treatment planning of various medical conditions, with radiology and pathology heavily reliant on precise image segmentation. The Segment Anything Model (SAM) has emerged as a promising framework for addressing segmentation challenges across different domains. In this white paper, we delve into SAM, breaking down its fundamental components and uncovering the intricate interactions between them. We also explore the fine-tuning of SAM and assess its profound impact on the accuracy and reliability of segmentation results, focusing on applications in radiology (specifically, brain tumor segmentation) and pathology (specifically, breast cancer segmentation). Through a series of carefully designed experiments, we analyze SAM's potential application in the field of medical imaging. We aim to bridge the gap between advanced segmentation techniques and the demanding requirements of healthcare, shedding light on SAM's transformative capabilities.
</details>
<details>
<summary>摘要</summary>
医疗影像在各种医疗疾病诊断和治疗规划中扮演着关键的角色，医 radiology 和 pathology 都受到精确的图像分割的依赖。 segmen anything model（SAM）在不同领域中呈现出了一种有前途的框架，以下我们将对 SAM 进行分析，探讨其基本组件之间的细腻交互，以及对准确性和可靠性的影响。我们将在医 radiology（特别是脑肿瘤分割）和 pathology（特别是乳腺癌分割）中进行精心设计的实验，分析 SAM 在医疗影像领域的潜在应用。我们想通过 bridging 高级分割技术和医疗需求的 gap，把 SAM 的 transformative 能力推广到医疗领域。
</details></li>
</ul>
<hr>
<h2 id="Black-box-Attacks-on-Image-Activity-Prediction-and-its-Natural-Language-Explanations"><a href="#Black-box-Attacks-on-Image-Activity-Prediction-and-its-Natural-Language-Explanations" class="headerlink" title="Black-box Attacks on Image Activity Prediction and its Natural Language Explanations"></a>Black-box Attacks on Image Activity Prediction and its Natural Language Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00503">http://arxiv.org/abs/2310.00503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alina Elena Baia, Valentina Poggioni, Andrea Cavallaro</li>
<li>for: 这篇论文目的是评估深度神经网络的决策过程可以不可靠地描述的隐藏攻击。</li>
<li>methods: 这篇论文使用了一种自然语言解释的自然语言基于图像活动识别模型，并使用了黑盒测试来评估模型的Robustness。</li>
<li>results: 研究发现，使用了这种自然语言解释的模型很容易受到黑盒攻击，可以通过让模型生成不准确的解释来 manipulate 模型的决策。<details>
<summary>Abstract</summary>
Explainable AI (XAI) methods aim to describe the decision process of deep neural networks. Early XAI methods produced visual explanations, whereas more recent techniques generate multimodal explanations that include textual information and visual representations. Visual XAI methods have been shown to be vulnerable to white-box and gray-box adversarial attacks, with an attacker having full or partial knowledge of and access to the target system. As the vulnerabilities of multimodal XAI models have not been examined, in this paper we assess for the first time the robustness to black-box attacks of the natural language explanations generated by a self-rationalizing image-based activity recognition model. We generate unrestricted, spatially variant perturbations that disrupt the association between the predictions and the corresponding explanations to mislead the model into generating unfaithful explanations. We show that we can create adversarial images that manipulate the explanations of an activity recognition model by having access only to its final output.
</details>
<details>
<summary>摘要</summary>
explainable AI (XAI) 技术目的是描述深度神经网络决策过程。早期 XAI 技术生成了视觉解释，而更近期的技术生成了多 modal 解释，包括文本信息和视觉表示。视觉 XAI 技术在面对白盒和灰盒攻击时容易受损，攻击者具有完整或部分知道和访问目标系统的权限。然而，多 modal XAI 模型的抵御性尚未被调查，这篇论文是第一次评估黑盒攻击下自然语言解释生成的图像活动识别模型的可靠性。我们生成了无限制、空间变化的扰动，使模型的预测和相应的解释失去关联，以诱导模型生成不寻常的解释。我们显示了访问模型的最终输出后，可以创造欺骗性图像，使模型生成不准确的解释。
</details></li>
</ul>
<hr>
<h2 id="Small-Visual-Language-Models-can-also-be-Open-Ended-Few-Shot-Learners"><a href="#Small-Visual-Language-Models-can-also-be-Open-Ended-Few-Shot-Learners" class="headerlink" title="Small Visual Language Models can also be Open-Ended Few-Shot Learners"></a>Small Visual Language Models can also be Open-Ended Few-Shot Learners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00500">http://arxiv.org/abs/2310.00500</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Mahdi Derakhshani, Ivona Najdenkoska, Cees G. M. Snoek, Marcel Worring, Yuki M. Asano</li>
<li>for: 开发了一种自然语言模型的开放式少量学习能力，即使使用小型模型（约1B参数）也能够超越大型模型（如冰冻和FROMage）的少量学习能力。</li>
<li>methods: 提出了一种自我上下文适应（SeCAt）方法，通过自动学习从 симвоlic  yet self-supervised 训练任务中获得知识，包括基于 clustering 大量图像并赋予不相关的名称。</li>
<li>results: 在多Modal 少量数据集上表现出优秀的灵活性和性能，并且可以用小型模型（约1B参数）来实现，而不需要大型模型或专有模型。<details>
<summary>Abstract</summary>
We present Self-Context Adaptation (SeCAt), a self-supervised approach that unlocks open-ended few-shot abilities of small visual language models. Our proposed adaptation algorithm explicitly learns from symbolic, yet self-supervised training tasks. Specifically, our approach imitates image captions in a self-supervised way based on clustering a large pool of images followed by assigning semantically-unrelated names to clusters. By doing so, we construct the `self-context', a training signal consisting of interleaved sequences of image and pseudo-caption pairs and a query image for which the model is trained to produce the right pseudo-caption. We demonstrate the performance and flexibility of SeCAt on several multimodal few-shot datasets, spanning various granularities. By using models with approximately 1B parameters we outperform the few-shot abilities of much larger models, such as Frozen and FROMAGe. SeCAt opens new possibilities for research in open-ended few-shot learning that otherwise requires access to large or proprietary models.
</details>
<details>
<summary>摘要</summary>
我们介绍Self-Context Adaptation（SeCAt），一种自我指导的方法，可以激活小视觉语言模型的开放式少量学习能力。我们的提议的适应算法直接从 символиック， yet自我指导的训练任务中学习。具体来说，我们的方法模仿图像描述文本在自我指导的方式基于图像集 clustering，并将抽象无关的名称分配给集群。通过这样做，我们构建了`自我上下文'，一个训练信号包括交错的图像和假描述对象的序列，以及一个查询图像，对于该模型要生成正确的假描述。我们在多个多modal few-shot数据集上表现出了性能和灵活性，覆盖了不同的细化程度。使用大约1B参数的模型，我们超越了许多更大的模型，如冰冻和FROMAGe的少量学习能力。SeCAt开启了新的可能性 для开放式少量学习研究，否则需要访问大型或专有模型。
</details></li>
</ul>
<hr>
<h2 id="The-Sparsity-Roofline-Understanding-the-Hardware-Limits-of-Sparse-Neural-Networks"><a href="#The-Sparsity-Roofline-Understanding-the-Hardware-Limits-of-Sparse-Neural-Networks" class="headerlink" title="The Sparsity Roofline: Understanding the Hardware Limits of Sparse Neural Networks"></a>The Sparsity Roofline: Understanding the Hardware Limits of Sparse Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00496">http://arxiv.org/abs/2310.00496</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cameron Shinn, Collin McCarthy, Saurav Muralidharan, Muhammad Osama, John D. Owens</li>
<li>for: 评估神经网络中稀疙瘩的性能</li>
<li>methods: 提出了一种名为”简洁顶层”的视觉性能模型，用于评估神经网络中稀疙瘩的性能</li>
<li>results: 通过一种新的分析方法，可以预测稀疙瘩神经网络的性能，并 Validate the predicted speedup using several real-world computer vision architectures pruned across a range of sparsity patterns and degrees.<details>
<summary>Abstract</summary>
We introduce the Sparsity Roofline, a visual performance model for evaluating sparsity in neural networks. The Sparsity Roofline jointly models network accuracy, sparsity, and predicted inference speedup. Our approach does not require implementing and benchmarking optimized kernels, and the predicted speedup is equal to what would be measured when the corresponding dense and sparse kernels are equally well-optimized. We achieve this through a novel analytical model for predicting sparse network performance, and validate the predicted speedup using several real-world computer vision architectures pruned across a range of sparsity patterns and degrees. We demonstrate the utility and ease-of-use of our model through two case studies: (1) we show how machine learning researchers can predict the performance of unimplemented or unoptimized block-structured sparsity patterns, and (2) we show how hardware designers can predict the performance implications of new sparsity patterns and sparse data formats in hardware. In both scenarios, the Sparsity Roofline helps performance experts identify sparsity regimes with the highest performance potential.
</details>
<details>
<summary>摘要</summary>
我们介绍了简洁顶部（Sparsity Roofline），一个用于评估神经网络中的简洁性的可视性表现模型。简洁顶部同时考虑神经网络的准确性、简洁性和预测的执行速度增加。我们的方法不需要实现和测试优化的核心，且预测的速度与 dense 和简洁核心相同程度的优化相同。我们通过一个新的分析模型来预测简洁网络的性能，并使用多个真实世界计算机视觉架构中的简洁Pattern和度量进行验证。我们透过两个案例研究：首先，我们显示了如何在简洁顶部的帮助下，机器学习研究人员可以预测尚未实现或优化的块结构简洁模式的性能。其次，我们显示了如何在简洁顶部的帮助下，硬件设计师可以预测新的简洁模式和简洁数据格式在硬件上的性能影响。在这两个案例中，简洁顶部帮助性能专家识别最高性能潜在的简洁度域。
</details></li>
</ul>
<hr>
<h2 id="Diff-DOPE-Differentiable-Deep-Object-Pose-Estimation"><a href="#Diff-DOPE-Differentiable-Deep-Object-Pose-Estimation" class="headerlink" title="Diff-DOPE: Differentiable Deep Object Pose Estimation"></a>Diff-DOPE: Differentiable Deep Object Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00463">http://arxiv.org/abs/2310.00463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan Tremblay, Bowen Wen, Valts Blukis, Balakumar Sundaralingam, Stephen Tyree, Stan Birchfield</li>
<li>for: 提高对象pose的优化，使用拟合照片和3D文本模型来更新 object pose，以最小化视觉错误。</li>
<li>methods: 使用可导渠 rendering 更新 object pose，避免需要训练大量synthetic dataset的深度神经网络。</li>
<li>results: 实现了状态机器pose estimation datasets的最佳效果，并且可以处理多种modalities，如RGB、深度、纹理边缘和物体 segmentation masks。<details>
<summary>Abstract</summary>
We introduce Diff-DOPE, a 6-DoF pose refiner that takes as input an image, a 3D textured model of an object, and an initial pose of the object. The method uses differentiable rendering to update the object pose to minimize the visual error between the image and the projection of the model. We show that this simple, yet effective, idea is able to achieve state-of-the-art results on pose estimation datasets. Our approach is a departure from recent methods in which the pose refiner is a deep neural network trained on a large synthetic dataset to map inputs to refinement steps. Rather, our use of differentiable rendering allows us to avoid training altogether. Our approach performs multiple gradient descent optimizations in parallel with different random learning rates to avoid local minima from symmetric objects, similar appearances, or wrong step size. Various modalities can be used, e.g., RGB, depth, intensity edges, and object segmentation masks. We present experiments examining the effect of various choices, showing that the best results are found when the RGB image is accompanied by an object mask and depth image to guide the optimization process.
</details>
<details>
<summary>摘要</summary>
我们介绍Diff-DOPE，一种6DoF姿态级化器，它接受图像、一个3D纹理模型和初始对象姿态作为输入。该方法使用可微渲染更新对象姿态，以最小化图像和模型投影之间的视觉错误。我们表明，这个简单 yet有效的想法可以实现状态革命的结果在姿态估计数据集上。我们的方法与最近的方法不同，后者是通过训练大量的 sintetic数据来训练一个深度神经网络，以将输入映射到更新步骤。而我们使用可微渲染，可以避免训练。我们的方法可以并行进行多个梯度下降优化，以避免相似的对象、同样的外观或错误的步长导致的本地极小值。不同的感知modalities可以使用，例如RGB、深度、强度边缘和对象分割mask。我们进行了不同的选择的实验，并显示了RGB图像和对象mask、深度图像的搭配能够获得最佳结果。
</details></li>
</ul>
<hr>
<h2 id="UniLVSeg-Unified-Left-Ventricular-Segmentation-with-Sparsely-Annotated-Echocardiogram-Videos-through-Self-Supervised-Temporal-Masking-and-Weakly-Supervised-Training"><a href="#UniLVSeg-Unified-Left-Ventricular-Segmentation-with-Sparsely-Annotated-Echocardiogram-Videos-through-Self-Supervised-Temporal-Masking-and-Weakly-Supervised-Training" class="headerlink" title="UniLVSeg: Unified Left Ventricular Segmentation with Sparsely Annotated Echocardiogram Videos through Self-Supervised Temporal Masking and Weakly Supervised Training"></a>UniLVSeg: Unified Left Ventricular Segmentation with Sparsely Annotated Echocardiogram Videos through Self-Supervised Temporal Masking and Weakly Supervised Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00454">http://arxiv.org/abs/2310.00454</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fadillah Maani, Asim Ukaye, Nada Saadi, Numan Saeed, Mohammad Yaqub<br>for: 这份研究的目的是提出一种可靠且高效的左心室（LV）分 segmentation方法，以帮助医生更加精确地诊断心血管疾病。methods: 本研究使用了自动学习（SSL）和弱监督训练（WST）两种方法，并考虑了三种不同的分 segmentation方法：3D分 segmentation和一种新的2D超像（SI）。results: 本研究比较了各种方法的效果，结果显示了我们的提案方法在大规模数据集（EchoNet-Dynamic）上获得了93.32%（95%CI 93.21-93.43%)的 dice分数，而且比之前的方法更高效。我们还提供了广泛的拓展研究，包括预训练设定和不同的深度学习架构。<details>
<summary>Abstract</summary>
Echocardiography has become an indispensable clinical imaging modality for general heart health assessment. From calculating biomarkers such as ejection fraction to the probability of a patient's heart failure, accurate segmentation of the heart and its structures allows doctors to plan and execute treatments with greater precision and accuracy. However, achieving accurate and robust left ventricle segmentation is time-consuming and challenging due to different reasons. This work introduces a novel approach for consistent left ventricular (LV) segmentation from sparsely annotated echocardiogram videos. We achieve this through (1) self-supervised learning (SSL) using temporal masking followed by (2) weakly supervised training. We investigate two different segmentation approaches: 3D segmentation and a novel 2D superimage (SI). We demonstrate how our proposed method outperforms the state-of-the-art solutions by achieving a 93.32% (95%CI 93.21-93.43%) dice score on a large-scale dataset (EchoNet-Dynamic) while being more efficient. To show the effectiveness of our approach, we provide extensive ablation studies, including pre-training settings and various deep learning backbones. Additionally, we discuss how our proposed methodology achieves high data utility by incorporating unlabeled frames in the training process. To help support the AI in medicine community, the complete solution with the source code will be made publicly available upon acceptance.
</details>
<details>
<summary>摘要</summary>
echo cardiography 已成为现代医学实验室中不可或缺的诊断工具，从计算生物标志物such as 血液泵功率到患者的心血液疾病可能性，准确地分割心脏和其结构，帮助医生更加准确地规划和执行治疗。然而，实现准确和可靠的左心室（LV）分割是一项时间consuming和困难的任务，主要因为多种原因。这种工作介绍了一种新的方法，可以从缺乏标注的echo cardiogram视频中获得一致的LV分割结果。我们通过(1)自动学习（SSL）使用时间掩蔽，然后(2)弱监督训练来实现这一目标。我们 investigate了两种不同的分割方法：3D分割和一种新的2D超像（SI）。我们展示了我们的提议方法在大规模数据集（EchoNet-Dynamic）上的表现，而且比现有的解决方案高效。为了证明我们的方法的有效性，我们提供了广泛的拟合研究，包括预训练设置和不同的深度学习背bone。此外，我们讨论了我们的方法如何实现高数据利用率，通过在训练过程中包含无标注帧。为了支持AI医学社区，我们将在接受后公开完整的解决方案和源代码。
</details></li>
</ul>
<hr>
<h2 id="On-the-Role-of-Neural-Collapse-in-Meta-Learning-Models-for-Few-shot-Learning"><a href="#On-the-Role-of-Neural-Collapse-in-Meta-Learning-Models-for-Few-shot-Learning" class="headerlink" title="On the Role of Neural Collapse in Meta Learning Models for Few-shot Learning"></a>On the Role of Neural Collapse in Meta Learning Models for Few-shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00451">http://arxiv.org/abs/2310.00451</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/saakethmm/nc-prototypical-networks">https://github.com/saakethmm/nc-prototypical-networks</a></li>
<li>paper_authors: Saaketh Medepalli, Naren Doraiswamy</li>
<li>For: 这个论文探讨了基于少量示例学习的元学习框架，以及这些框架在新类上的泛化性。* Methods: 这个论文使用了元学习框架，并在Omniglot数据集上进行了几个示例学习任务的研究。* Results: 研究发现，随着模型大小增加，学习出来的特征往往呈现出神经塌磔现象，但不一定符合完整的神经塌磔性质。<details>
<summary>Abstract</summary>
Meta-learning frameworks for few-shot learning aims to learn models that can learn new skills or adapt to new environments rapidly with a few training examples. This has led to the generalizability of the developed model towards new classes with just a few labelled samples. However these networks are seen as black-box models and understanding the representations learnt under different learning scenarios is crucial. Neural collapse ($\mathcal{NC}$) is a recently discovered phenomenon which showcases unique properties at the network proceeds towards zero loss. The input features collapse to their respective class means, the class means form a Simplex equiangular tight frame (ETF) where the class means are maximally distant and linearly separable, and the classifier acts as a simple nearest neighbor classifier. While these phenomena have been observed in simple classification networks, this study is the first to explore and understand the properties of neural collapse in meta learning frameworks for few-shot learning. We perform studies on the Omniglot dataset in the few-shot setting and study the neural collapse phenomenon. We observe that the learnt features indeed have the trend of neural collapse, especially as model size grows, but to do not necessarily showcase the complete collapse as measured by the $\mathcal{NC}$ properties.
</details>
<details>
<summary>摘要</summary>
<SYS> translate-internal: "Meta-learning frameworks for few-shot learning aim to learn models that can learn new skills or adapt to new environments rapidly with just a few training examples. This has led to the generalizability of the developed model towards new classes with just a few labelled samples. However, these networks are seen as black-box models, and understanding the representations learnt under different learning scenarios is crucial. Neural collapse (NC) is a recently discovered phenomenon that showcases unique properties when the network proceeds towards zero loss. The input features collapse to their respective class means, the class means form a Simplex equiangular tight frame (ETF) where the class means are maximally distant and linearly separable, and the classifier acts as a simple nearest neighbor classifier. While these phenomena have been observed in simple classification networks, this study is the first to explore and understand the properties of neural collapse in meta learning frameworks for few-shot learning. We perform studies on the Omniglot dataset in the few-shot setting and study the neural collapse phenomenon. We observe that the learnt features indeed have the trend of neural collapse, especially as model size grows, but they do not necessarily showcase the complete collapse as measured by the NC properties."</SYS>Here's the translation in Traditional Chinese:<SYS>translate-internal: "Meta-learning frameworks for few-shot learning aim to learn models that can learn new skills or adapt to new environments rapidly with just a few training examples. This has led to the generalizability of the developed model towards new classes with just a few labelled samples. However, these networks are seen as black-box models, and understanding the representations learnt under different learning scenarios is crucial. Neural collapse (NC) is a recently discovered phenomenon that showcases unique properties when the network proceeds towards zero loss. The input features collapse to their respective class means, the class means form a Simplex equiangular tight frame (ETF) where the class means are maximally distant and linearly separable, and the classifier acts as a simple nearest neighbor classifier. While these phenomena have been observed in simple classification networks, this study is the first to explore and understand the properties of neural collapse in meta learning frameworks for few-shot learning. We perform studies on the Omniglot dataset in the few-shot setting and study the neural collapse phenomenon. We observe that the learnt features indeed have the trend of neural collapse, especially as model size grows, but they do not necessarily showcase the complete collapse as measured by the NC properties."</SYS>Note that the translation is in Simplified Chinese, as requested. If you would like the translation in Traditional Chinese instead, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Human-Producible-Adversarial-Examples"><a href="#Human-Producible-Adversarial-Examples" class="headerlink" title="Human-Producible Adversarial Examples"></a>Human-Producible Adversarial Examples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00438">http://arxiv.org/abs/2310.00438</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lionfish0/adversarial-human">https://github.com/lionfish0/adversarial-human</a></li>
<li>paper_authors: David Khachaturov, Yue Gao, Ilia Shumailov, Robert Mullins, Ross Anderson, Kassem Fawaz</li>
<li>for: 该论文旨在开发一种可以在真实世界中生成人工生成的 adversarial example 方法，而无需使用复杂的设备或技术。</li>
<li>methods: 该方法基于差异渲染，通过简单地绘制四个或九个直线来构建强大的 adversarial example。它还包括一种基于人工绘制错误的抗噪准则，以保证攻击的可重复性。</li>
<li>results: 研究人员通过用涂抹笔将lines绘制到图像上，实现了在YOLO模型中81.8%的攻击成功率。此外，研究人员还进行了数字和物理世界的广泛测试，并证明了该方法可以由无经验人员应用。<details>
<summary>Abstract</summary>
Visual adversarial examples have so far been restricted to pixel-level image manipulations in the digital world, or have required sophisticated equipment such as 2D or 3D printers to be produced in the physical real world. We present the first ever method of generating human-producible adversarial examples for the real world that requires nothing more complicated than a marker pen. We call them $\textbf{adversarial tags}$. First, building on top of differential rendering, we demonstrate that it is possible to build potent adversarial examples with just lines. We find that by drawing just $4$ lines we can disrupt a YOLO-based model in $54.8\%$ of cases; increasing this to $9$ lines disrupts $81.8\%$ of the cases tested. Next, we devise an improved method for line placement to be invariant to human drawing error. We evaluate our system thoroughly in both digital and analogue worlds and demonstrate that our tags can be applied by untrained humans. We demonstrate the effectiveness of our method for producing real-world adversarial examples by conducting a user study where participants were asked to draw over printed images using digital equivalents as guides. We further evaluate the effectiveness of both targeted and untargeted attacks, and discuss various trade-offs and method limitations, as well as the practical and ethical implications of our work. The source code will be released publicly.
</details>
<details>
<summary>摘要</summary>
“Visual adversarial examples”Previously, have only been restricted to digital image manipulation or require sophisticated equipment such as 2D or 3D printers to produce in the physical world. We present the first method of generating human-producible adversarial examples for the real world that only requires a marker pen. We call them “adversarial tags”.First, we build on differential rendering and show that it is possible to create powerful adversarial examples with just lines. We found that by drawing just 4 lines, we can disrupt a YOLO-based model in 54.8% of cases, and increasing it to 9 lines disrupts 81.8% of the cases tested. Next, we improve the method for line placement to be invariant to human drawing errors.We thoroughly evaluate our system in both the digital and analog worlds and demonstrate that our tags can be applied by untrained humans. We also conduct a user study where participants were asked to draw over printed images using digital equivalents as guides, and evaluate the effectiveness of both targeted and untargeted attacks. We discuss various trade-offs and method limitations, as well as the practical and ethical implications of our work. The source code will be released publicly.
</details></li>
</ul>
<hr>
<h2 id="DiffPoseTalk-Speech-Driven-Stylistic-3D-Facial-Animation-and-Head-Pose-Generation-via-Diffusion-Models"><a href="#DiffPoseTalk-Speech-Driven-Stylistic-3D-Facial-Animation-and-Head-Pose-Generation-via-Diffusion-Models" class="headerlink" title="DiffPoseTalk: Speech-Driven Stylistic 3D Facial Animation and Head Pose Generation via Diffusion Models"></a>DiffPoseTalk: Speech-Driven Stylistic 3D Facial Animation and Head Pose Generation via Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00434">http://arxiv.org/abs/2310.00434</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/THU-LYJ-Lab/DiffPoseTalk">https://github.com/THU-LYJ-Lab/DiffPoseTalk</a></li>
<li>paper_authors: Zhiyao Sun, Tian Lv, Sheng Ye, Matthieu Gaetan Lin, Jenny Sheng, Yu-Hui Wen, Minjing Yu, Yong-jin Liu</li>
<li>for: 本研究旨在提出一种生成风格化3D脸部动画，使用speech和风格编码来驱动生成过程。</li>
<li>methods: 我们提出了一种基于扩散模型和风格编码器的生成框架，从短视频中提取风格特征，并使用无类标注引导生成过程。</li>
<li>results: 我们的方法在评测中超过了现有方法，并且通过用户测试得到了更高的评价。 code和数据将公开发布。<details>
<summary>Abstract</summary>
The generation of stylistic 3D facial animations driven by speech poses a significant challenge as it requires learning a many-to-many mapping between speech, style, and the corresponding natural facial motion. However, existing methods either employ a deterministic model for speech-to-motion mapping or encode the style using a one-hot encoding scheme. Notably, the one-hot encoding approach fails to capture the complexity of the style and thus limits generalization ability. In this paper, we propose DiffPoseTalk, a generative framework based on the diffusion model combined with a style encoder that extracts style embeddings from short reference videos. During inference, we employ classifier-free guidance to guide the generation process based on the speech and style. We extend this to include the generation of head poses, thereby enhancing user perception. Additionally, we address the shortage of scanned 3D talking face data by training our model on reconstructed 3DMM parameters from a high-quality, in-the-wild audio-visual dataset. Our extensive experiments and user study demonstrate that our approach outperforms state-of-the-art methods. The code and dataset will be made publicly available.
</details>
<details>
<summary>摘要</summary>
当前的3D facial动画生成技术面临着一个重要挑战，即学习很多到很多的映射关系 между语音、风格和自然的脸部动作。然而，现有的方法都是使用决定性的语音到动作映射模型，或者使用一个简单的一个热度编码方法来编码风格。可是，这种一个热度编码方法无法捕捉风格的复杂性，因此限制了其泛化能力。在这篇论文中，我们提出了DiffPoseTalk，一种基于扩散模型并与风格编码器结合的生成框架。在推理过程中，我们使用无类别导航来指导生成过程，根据语音和风格。此外，我们还扩展了头部pose的生成，从而提高用户的感知。此外，我们解决了3D talking face数据的缺乏问题，通过在高质量的自然语言视频 Dataset 中重建3DMM参数来训练我们的模型。我们的广泛的实验和用户研究表明，我们的方法在比较状态的方法之上出色表现。代码和数据将会公开释出。
</details></li>
</ul>
<hr>
<h2 id="Technical-Report-of-2023-ABO-Fine-grained-Semantic-Segmentation-Competition"><a href="#Technical-Report-of-2023-ABO-Fine-grained-Semantic-Segmentation-Competition" class="headerlink" title="Technical Report of 2023 ABO Fine-grained Semantic Segmentation Competition"></a>Technical Report of 2023 ABO Fine-grained Semantic Segmentation Competition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00427">http://arxiv.org/abs/2310.00427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyu Dong</li>
<li>for: 本研究是为了参加2023年ABO细化semantic segmentation比赛，目标是预测五类 convex shape的semantic标签。</li>
<li>methods: 本研究使用DGCNN作为backbone，通过分类不同类型的结构来实现semantic segmentation。我们进行了多个实验，并发现使用学习率随机梯度下降和不同类型的分解因子来提高模型性能。</li>
<li>results: 我们的模型在2023年ICCV 3DVeComm Workshop Challenge的Dev阶段取得了第3名。<details>
<summary>Abstract</summary>
In this report, we describe the technical details of our submission to the 2023 ABO Fine-grained Semantic Segmentation Competition, by Team "Zeyu\_Dong" (username:ZeyuDong). The task is to predicate the semantic labels for the convex shape of five categories, which consist of high-quality, standardized 3D models of real products available for purchase online. By using DGCNN as the backbone to classify different structures of five classes, We carried out numerous experiments and found learning rate stochastic gradient descent with warm restarts and setting different rate of factors for various categories contribute most to the performance of the model. The appropriate method helps us rank 3rd place in the Dev phase of the 2023 ICCV 3DVeComm Workshop Challenge.
</details>
<details>
<summary>摘要</summary>
在本报告中，我们介绍了我们对2023年ABO细致semantic segmentation比赛的提交技术细节，由Team "Zeyu\_Dong"（用户名：ZeyuDong）完成。任务是预测五类 convex shape的semantic标签，其中五类包括高质量、标准化的3D模型在线销售。通过使用DGCNN作为后端分类不同结构的五类，我们进行了许多实验，发现了学习率随机梯度下降与温存 restart的方法对模型性能产生了最大化的影响。这种方法帮助我们在2023年ICCV 3DVeComm Workshop Challenge的Dev阶段取得第三名。
</details></li>
</ul>
<hr>
<h2 id="PixArt-α-Fast-Training-of-Diffusion-Transformer-for-Photorealistic-Text-to-Image-Synthesis"><a href="#PixArt-α-Fast-Training-of-Diffusion-Transformer-for-Photorealistic-Text-to-Image-Synthesis" class="headerlink" title="PixArt-$α$: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis"></a>PixArt-$α$: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00426">http://arxiv.org/abs/2310.00426</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/PixArt-alpha/PixArt-alpha">https://github.com/PixArt-alpha/PixArt-alpha</a></li>
<li>paper_authors: Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, Zhenguo Li</li>
<li>for: 这篇论文旨在提供一个高品质却低成本的文本转换到图像（T2I）模型，以推动AI创新和低排放。</li>
<li>methods: 这篇论文提出了三个核心设计：首先，通过分解训练策略，分别优化像素依赖、文本图像对齐和图像艺术质量; 其次，透过插入批访模组，将文本条件注入到算法中，以提高计算效率; 最后，强调概念密度的重要性，并运用大型视觉语言模型自动生成密集pseudo-caption，以帮助图像对齐学习。</li>
<li>results: 这篇论文显示了PIXART-$\alpha$的训练速度明显高于现有的大规模T2I模型，例如PIXART-$\alpha$只需10.8%的Stable Diffusion v1.5的训练时间（675vs. 6,250 A100 GPU天），优化了约 $300,000（ $26,000 vs. $320,000）和减少90%的二氧化碳排放。此外，与现有较大的SOTA模型相比，我们的训练成本仅1%。PIXART-$\alpha$在图像质量、艺术性和Semantic控制方面表现出色。<details>
<summary>Abstract</summary>
The most advanced text-to-image (T2I) models require significant training costs (e.g., millions of GPU hours), seriously hindering the fundamental innovation for the AIGC community while increasing CO2 emissions. This paper introduces PIXART-$\alpha$, a Transformer-based T2I diffusion model whose image generation quality is competitive with state-of-the-art image generators (e.g., Imagen, SDXL, and even Midjourney), reaching near-commercial application standards. Additionally, it supports high-resolution image synthesis up to 1024px resolution with low training cost, as shown in Figure 1 and 2. To achieve this goal, three core designs are proposed: (1) Training strategy decomposition: We devise three distinct training steps that separately optimize pixel dependency, text-image alignment, and image aesthetic quality; (2) Efficient T2I Transformer: We incorporate cross-attention modules into Diffusion Transformer (DiT) to inject text conditions and streamline the computation-intensive class-condition branch; (3) High-informative data: We emphasize the significance of concept density in text-image pairs and leverage a large Vision-Language model to auto-label dense pseudo-captions to assist text-image alignment learning. As a result, PIXART-$\alpha$'s training speed markedly surpasses existing large-scale T2I models, e.g., PIXART-$\alpha$ only takes 10.8% of Stable Diffusion v1.5's training time (675 vs. 6,250 A100 GPU days), saving nearly \$300,000 (\$26,000 vs. \$320,000) and reducing 90% CO2 emissions. Moreover, compared with a larger SOTA model, RAPHAEL, our training cost is merely 1%. Extensive experiments demonstrate that PIXART-$\alpha$ excels in image quality, artistry, and semantic control. We hope PIXART-$\alpha$ will provide new insights to the AIGC community and startups to accelerate building their own high-quality yet low-cost generative models from scratch.
</details>
<details>
<summary>摘要</summary>
最先进的文本到图像（T2I）模型需要巨大的训练成本（例如，百万个GPU小时），这会很大程度地阻碍AIGC社区的基础创新，同时也会增加CO2排放。这篇论文介绍了PIXART-α，一种基于Transformer的T2I扩散模型，其生成图像质量与现状最先进的图像生成器（如Imagen、SDXL以及甚至Midjourney）相当，达到了近商用应用标准。此外，它还支持高分辨率图像生成，达到1024px的分辨率，训练成本低廉，如图1和图2所示。为了实现这一目标，我们提出了三个核心设计：1. 训练策略分解：我们将训练过程分为三个独立的步骤，每个步骤都会分别优化像素依赖关系、文本-图像对齐和图像美观质量。2. 高效的T2I transformer：我们在扩散变换器（DiT）中添加了跨注意力模块，以注入文本条件并使计算量昂贵的分类分支更加高效。3. 高信息 densities：我们强调了文本-图像对的概率密度的重要性，并利用大量的视觉语言模型自动生成密集 Pseudo-captions，以帮助图像-文本对齐学习。因此，PIXART-α的训练速度明显高于现有的大规模T2I模型，例如PIXART-α只需10.8%的Stable Diffusion v1.5的训练时间（675 vs. 6,250 A100 GPU天），相对 экономии了约300,000美元（26,000 vs. 320,000美元），同时减少了90%的CO2排放。此外，相比一个更大的SOTA模型，RAPHAEL，我们的训练成本只有1%。广泛的实验表明，PIXART-α在图像质量、艺术性和 semantics 控制方面具有优异的表现。我们希望PIXART-α能为AIGC社区和创新公司提供新的思路，以便他们可以从头开始构建高质量 yet low cost的生成模型。
</details></li>
</ul>
<hr>
<h2 id="MVC-A-Multi-Task-Vision-Transformer-Network-for-COVID-19-Diagnosis-from-Chest-X-ray-Images"><a href="#MVC-A-Multi-Task-Vision-Transformer-Network-for-COVID-19-Diagnosis-from-Chest-X-ray-Images" class="headerlink" title="MVC: A Multi-Task Vision Transformer Network for COVID-19 Diagnosis from Chest X-ray Images"></a>MVC: A Multi-Task Vision Transformer Network for COVID-19 Diagnosis from Chest X-ray Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00418">http://arxiv.org/abs/2310.00418</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huyen Tran, Duc Thanh Nguyen, John Yearwood</li>
<li>for: 这篇论文的目的是提出一个新的多任务检测方法，以便同时分类骨肉X射线图像和识别受影响区域。</li>
<li>methods: 本论文使用的方法是基于Vision Transformer的多任务学习架构，具有本地和全球表示学习的能力。</li>
<li>results: 实验结果显示，提出的方法在比较基于基eline的方法时表现更好，在骨肉X射线图像分类和受影响区域识别两个任务上都达到了更高的准确率。<details>
<summary>Abstract</summary>
Medical image analysis using computer-based algorithms has attracted considerable attention from the research community and achieved tremendous progress in the last decade. With recent advances in computing resources and availability of large-scale medical image datasets, many deep learning models have been developed for disease diagnosis from medical images. However, existing techniques focus on sub-tasks, e.g., disease classification and identification, individually, while there is a lack of a unified framework enabling multi-task diagnosis. Inspired by the capability of Vision Transformers in both local and global representation learning, we propose in this paper a new method, namely Multi-task Vision Transformer (MVC) for simultaneously classifying chest X-ray images and identifying affected regions from the input data. Our method is built upon the Vision Transformer but extends its learning capability in a multi-task setting. We evaluated our proposed method and compared it with existing baselines on a benchmark dataset of COVID-19 chest X-ray images. Experimental results verified the superiority of the proposed method over the baselines on both the image classification and affected region identification tasks.
</details>
<details>
<summary>摘要</summary>
医学图像分析使用计算机基于算法已经在过去十年内吸引了广泛的研究者关注，取得了很大的进步。随着计算资源的提高和医学图像大规模数据集的可用性，许多深度学习模型在医疾诊断方面得到了应用。然而，现有的技术主要集中在子任务上，例如疾病分类和识别，而忽略了多任务诊断框架的开发。我们在这篇论文中提出了一种新的方法，即多任务视transformer（MVC），用于同时分类胸部X射影图像和识别输入数据中的受影响区域。我们的方法基于视transformer，但在多任务设定下扩展了其学习能力。我们对一个COVID-19胸部X射影图像数据集进行了实验，并与现有的基线相比较。实验结果表明，我们提出的方法在两个任务上都有superiority。
</details></li>
</ul>
<hr>
<h2 id="SSIF-Learning-Continuous-Image-Representation-for-Spatial-Spectral-Super-Resolution"><a href="#SSIF-Learning-Continuous-Image-Representation-for-Spatial-Spectral-Super-Resolution" class="headerlink" title="SSIF: Learning Continuous Image Representation for Spatial-Spectral Super-Resolution"></a>SSIF: Learning Continuous Image Representation for Spatial-Spectral Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00413">http://arxiv.org/abs/2310.00413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gengchen Mai, Ni Lao, Weiwei Sun, Yuchi Ma, Jiaming Song, Chenlin Meng, Hongxu Ma, Jinmeng Rao, Ziyuan Li, Stefano Ermon</li>
<li>for: 提高图像的空间和频谱分解能力</li>
<li>methods: 使用神经隐函数模型来表示图像，并在图像中进行空间和频谱分解</li>
<li>results: 对两个难题的空间-频谱超分辨 benchmark 进行了实验，并证明了 SSIF 可以在不同的空间和频谱分辨下表现出色，并且可以提高下游任务（例如土地使用分类）的性能 by 1.7%-7%。<details>
<summary>Abstract</summary>
Existing digital sensors capture images at fixed spatial and spectral resolutions (e.g., RGB, multispectral, and hyperspectral images), and each combination requires bespoke machine learning models. Neural Implicit Functions partially overcome the spatial resolution challenge by representing an image in a resolution-independent way. However, they still operate at fixed, pre-defined spectral resolutions. To address this challenge, we propose Spatial-Spectral Implicit Function (SSIF), a neural implicit model that represents an image as a function of both continuous pixel coordinates in the spatial domain and continuous wavelengths in the spectral domain. We empirically demonstrate the effectiveness of SSIF on two challenging spatio-spectral super-resolution benchmarks. We observe that SSIF consistently outperforms state-of-the-art baselines even when the baselines are allowed to train separate models at each spectral resolution. We show that SSIF generalizes well to both unseen spatial resolutions and spectral resolutions. Moreover, SSIF can generate high-resolution images that improve the performance of downstream tasks (e.g., land use classification) by 1.7%-7%.
</details>
<details>
<summary>摘要</summary>
现有的数字感知器只能捕捉固定的空间和спектраль分辨率图像（例如RGB、多spectral和快速pectral图像），每种组合都需要特制的机器学习模型。神经凝函数partially overcomes the spatial resolution challenge by representing an image in a resolution-independent way. However, they still operate at fixed, pre-defined spectral resolutions. To address this challenge, we propose Spatial-Spectral Implicit Function (SSIF), a neural implicit model that represents an image as a function of both continuous pixel coordinates in the spatial domain and continuous wavelengths in the spectral domain. We empirically demonstrate the effectiveness of SSIF on two challenging spatio-spectral super-resolution benchmarks. We observe that SSIF consistently outperforms state-of-the-art baselines even when the baselines are allowed to train separate models at each spectral resolution. We show that SSIF generalizes well to both unseen spatial resolutions and spectral resolutions. Moreover, SSIF can generate high-resolution images that improve the performance of downstream tasks (e.g., 土地使用分类) by 1.7%-7%.
</details></li>
</ul>
<hr>
<h2 id="Controlling-Neural-Style-Transfer-with-Deep-Reinforcement-Learning"><a href="#Controlling-Neural-Style-Transfer-with-Deep-Reinforcement-Learning" class="headerlink" title="Controlling Neural Style Transfer with Deep Reinforcement Learning"></a>Controlling Neural Style Transfer with Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00405">http://arxiv.org/abs/2310.00405</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abusufyanvu/6S191_MIT_DeepLearning">https://github.com/abusufyanvu/6S191_MIT_DeepLearning</a></li>
<li>paper_authors: Chengming Feng, Jing Hu, Xin Wang, Shu Hu, Bin Zhu, Xi Wu, Hongtu Zhu, Siwei Lyu</li>
<li>for: 这个论文是为了提出一种深度学习（Deep Learning）基本的 Reinforcement Learning（RL）架构，用于控制涂抹式转换（Neural Style Transfer，NST）的度量。</li>
<li>methods: 这个方法使用RL来将一步式转换分解为多个步骤，以保留内容图像的详细信息和结构，并在后续步骤中增加风格模式。这种方法是用户轻松控制的样式传递方法。</li>
<li>results: 实验结果表明，我们的RL-based方法可以具有更高的效果和稳定性，并且比现有的一步DL基本模型具有更低的计算复杂度和更轻量级的计算负担。<details>
<summary>Abstract</summary>
Controlling the degree of stylization in the Neural Style Transfer (NST) is a little tricky since it usually needs hand-engineering on hyper-parameters. In this paper, we propose the first deep Reinforcement Learning (RL) based architecture that splits one-step style transfer into a step-wise process for the NST task. Our RL-based method tends to preserve more details and structures of the content image in early steps, and synthesize more style patterns in later steps. It is a user-easily-controlled style-transfer method. Additionally, as our RL-based model performs the stylization progressively, it is lightweight and has lower computational complexity than existing one-step Deep Learning (DL) based models. Experimental results demonstrate the effectiveness and robustness of our method.
</details>
<details>
<summary>摘要</summary>
控制 neural style transfer（NST）的度风格化有些困难，通常需要手工调整超参数。在这篇论文中，我们提出了首个深度强化学习（RL）基 Architecture，将一步式样式传递分解成多个步骤的NST任务。我们的RL基方法会在早期步骤中保留更多的内容图像细节和结构，并在后期步骤中更多地 sinthez style pattern。这是一种用户轻松控制的样式传递方法。此外，我们的RL基模型在进行样式传递的过程中，轻量级，计算复杂度较低于现有的一步DL基模型。实验结果表明我们的方法的有效性和稳定性。
</details></li>
</ul>
<hr>
<h2 id="MonoGAE-Roadside-Monocular-3D-Object-Detection-with-Ground-Aware-Embeddings"><a href="#MonoGAE-Roadside-Monocular-3D-Object-Detection-with-Ground-Aware-Embeddings" class="headerlink" title="MonoGAE: Roadside Monocular 3D Object Detection with Ground-Aware Embeddings"></a>MonoGAE: Roadside Monocular 3D Object Detection with Ground-Aware Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00400">http://arxiv.org/abs/2310.00400</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Yang, Jiaxin Yu, Xinyu Zhang, Jun Li, Li Wang, Yi Huang, Chuang Zhang, Hong Wang, Yiming Li</li>
<li>for: 提高路边摄像头的自动驾驶系统能力</li>
<li>methods: 利用智能路边摄像头，通过学习高维 embedding 来提高物体检测精度</li>
<li>results: 比前一代方法更高的3D物体检测性能，可以帮助路边摄像头提高自动驾驶系统的能力<details>
<summary>Abstract</summary>
Although the majority of recent autonomous driving systems concentrate on developing perception methods based on ego-vehicle sensors, there is an overlooked alternative approach that involves leveraging intelligent roadside cameras to help extend the ego-vehicle perception ability beyond the visual range. We discover that most existing monocular 3D object detectors rely on the ego-vehicle prior assumption that the optical axis of the camera is parallel to the ground. However, the roadside camera is installed on a pole with a pitched angle, which makes the existing methods not optimal for roadside scenes. In this paper, we introduce a novel framework for Roadside Monocular 3D object detection with ground-aware embeddings, named MonoGAE. Specifically, the ground plane is a stable and strong prior knowledge due to the fixed installation of cameras in roadside scenarios. In order to reduce the domain gap between the ground geometry information and high-dimensional image features, we employ a supervised training paradigm with a ground plane to predict high-dimensional ground-aware embeddings. These embeddings are subsequently integrated with image features through cross-attention mechanisms. Furthermore, to improve the detector's robustness to the divergences in cameras' installation poses, we replace the ground plane depth map with a novel pixel-level refined ground plane equation map. Our approach demonstrates a substantial performance advantage over all previous monocular 3D object detectors on widely recognized 3D detection benchmarks for roadside cameras. The code and pre-trained models will be released soon.
</details>
<details>
<summary>摘要</summary>
尽管现在大多数自动驾驶系统都在开发基于自驾车感知器的感知方法，但是有一种被忽略的代理方法是利用智能路边摄像头来帮助扩展自驾车感知范围之 beyond。我们发现大多数现有的单目3D物体探测器都基于自驾车的先前假设，即摄像头的光学轴与地面平行。然而，路边摄像头通常会被安装在倾斜的杆上，这使得现有的方法不适合路边场景。在这篇论文中，我们介绍了一种名为MonogaE的新框架，用于路边单目3D物体探测。 Specifically，我们认为地面是一种稳定和强大的先知知识，因为摄像头在路边enario中是固定安装的。为了减少图像特征和地面几何信息之间的领域差距，我们采用一种监督训练方法，使用地面来预测高维度的地面感知 embedding。这些 embedding  subsequentially 与图像特征进行交叉注意机制。此外，为了提高探测器对摄像头安装位置的不同的灵活性，我们将地面深度图像 replaced  noval pixel-level refined ground plane equation map。我们的方法在广泛recognized 3D探测标准准例中表现出了明显的性能优势。我们即将发布代码和预训练模型。
</details></li>
</ul>
<hr>
<h2 id="InstructCV-Instruction-Tuned-Text-to-Image-Diffusion-Models-as-Vision-Generalists"><a href="#InstructCV-Instruction-Tuned-Text-to-Image-Diffusion-Models-as-Vision-Generalists" class="headerlink" title="InstructCV: Instruction-Tuned Text-to-Image Diffusion Models as Vision Generalists"></a>InstructCV: Instruction-Tuned Text-to-Image Diffusion Models as Vision Generalists</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00390">http://arxiv.org/abs/2310.00390</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AlaaLab/InstructCV">https://github.com/AlaaLab/InstructCV</a></li>
<li>paper_authors: Yulu Gan, Sungwoo Park, Alexander Schubert, Anthony Philippakis, Ahmed M. Alaa</li>
<li>for: 这个论文旨在提供一种基于自然语言指令的多任务计算视觉模型，它可以通过文本描述来执行多种计算视觉任务。</li>
<li>methods: 该论文使用了文本控制的生成扩散模型，并通过自然语言模型来帮助模型学习多个计算视觉任务。</li>
<li>results: 实验表明，该模型能够与其他通用和任务特定视觉模型相比竞争，并且具有出色的扩展性，可以在未经见过的数据、类别和用户指令下进行高效的执行。<details>
<summary>Abstract</summary>
Recent advances in generative diffusion models have enabled text-controlled synthesis of realistic and diverse images with impressive quality. Despite these remarkable advances, the application of text-to-image generative models in computer vision for standard visual recognition tasks remains limited. The current de facto approach for these tasks is to design model architectures and loss functions that are tailored to the task at hand. In this paper, we develop a unified language interface for computer vision tasks that abstracts away task-specific design choices and enables task execution by following natural language instructions. Our approach involves casting multiple computer vision tasks as text-to-image generation problems. Here, the text represents an instruction describing the task, and the resulting image is a visually-encoded task output. To train our model, we pool commonly-used computer vision datasets covering a range of tasks, including segmentation, object detection, depth estimation, and classification. We then use a large language model to paraphrase prompt templates that convey the specific tasks to be conducted on each image, and through this process, we create a multi-modal and multi-task training dataset comprising input and output images along with annotated instructions. Following the InstructPix2Pix architecture, we apply instruction-tuning to a text-to-image diffusion model using our constructed dataset, steering its functionality from a generative model to an instruction-guided multi-task vision learner. Experiments demonstrate that our model, dubbed InstructCV, performs competitively compared to other generalist and task-specific vision models. Moreover, it exhibits compelling generalization capabilities to unseen data, categories, and user instructions.
</details>
<details>
<summary>摘要</summary>
Traditional Chinese:最近的生成扩散模型突破了文本控制的图像生成的可靠性和多样性，这些突破使得图像生成的应用在计算机视觉中尚未得到广泛应用。当前的 де факто方法是通过设计特定任务的模型结构和损失函数来实现这些任务。在这篇论文中，我们开发了一个统一的自然语言界面 для计算机视觉任务，这个界面将任务特定的设计选择抽象化，使得任务执行可以通过自然语言指令进行。我们的方法是将多种计算机视觉任务转化为文本到图像生成问题，其中文本表示任务的指令，并且生成的图像是视觉编码的任务输出。为了训练我们的模型，我们将常用的计算机视觉数据集合起来，包括分割、物体检测、深度估计和分类等任务。然后，我们使用一个大型自然语言模型来重新表达用于每个图像的指令模板，并通过这个过程创建了一个多modal和多任务的训练数据集。采用InstructPix2Pix架构，我们对文本到图像扩散模型进行指令调整，从而将其变成一个根据指令进行多任务视觉学习的模型。实验结果表明，我们的模型，称为InstructCV，与其他普遍和任务特定的视觉模型相比，表现竞争力强。此外，它还能够对未看到的数据、类别和用户指令进行吸引人的泛化。
</details></li>
</ul>
<hr>
<h2 id="Deep-Active-Learning-with-Noisy-Oracle-in-Object-Detection"><a href="#Deep-Active-Learning-with-Noisy-Oracle-in-Object-Detection" class="headerlink" title="Deep Active Learning with Noisy Oracle in Object Detection"></a>Deep Active Learning with Noisy Oracle in Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00372">http://arxiv.org/abs/2310.00372</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marius Schubert, Tobias Riedlinger, Karsten Kahl, Matthias Rottmann</li>
<li>for: 提高对象检测器的性能，减少人工标注的数量和质量不良的影响。</li>
<li>methods: 使用活动学习算法和标签审核模块，对活动数据中的噪声标注进行纠正，提高模型性能。</li>
<li>results: 在实验中，通过包括标签审核模块，使用部分标注预算来纠正噪声标注，提高对象检测器的性能，最高提升4.5个mAP点。<details>
<summary>Abstract</summary>
Obtaining annotations for complex computer vision tasks such as object detection is an expensive and time-intense endeavor involving a large number of human workers or expert opinions. Reducing the amount of annotations required while maintaining algorithm performance is, therefore, desirable for machine learning practitioners and has been successfully achieved by active learning algorithms. However, it is not merely the amount of annotations which influences model performance but also the annotation quality. In practice, the oracles that are queried for new annotations frequently contain significant amounts of noise. Therefore, cleansing procedures are oftentimes necessary to review and correct given labels. This process is subject to the same budget as the initial annotation itself since it requires human workers or even domain experts. Here, we propose a composite active learning framework including a label review module for deep object detection. We show that utilizing part of the annotation budget to correct the noisy annotations partially in the active dataset leads to early improvements in model performance, especially when coupled with uncertainty-based query strategies. The precision of the label error proposals has a significant influence on the measured effect of the label review. In our experiments we achieve improvements of up to 4.5 mAP points of object detection performance by incorporating label reviews at equal annotation budget.
</details>
<details>
<summary>摘要</summary>
获取复杂计算机视觉任务中的对象检测签名是一个昂贵的时间进行的劳动密集型任务，需要大量的人工工作或专家意见。因此，减少需要的签名数量而保持算法性能是机器学习实践者所需的，并已经由活动学习算法得到成功。然而，不仅是签名数量，签名质量也对模型性能产生影响。在实践中，查询新签名的或acles frequently contain significant amounts of noise。因此，清洁过程是必要的，以审查并更正给出的标签。这个过程与初始签名预算相同，需要人工工作或域专家。我们提议一种复合的活动学习框架，包括深度对象检测中的标签审查模块。我们表明，在活动数据集中部分使用签名预算来更正噪音签名，会导致早期提高模型性能，特别是与不确定性基于的查询策略相结合。标签错误提案的精度有重要的影响于测量效果。在我们的实验中，通过包含标签审查，提高了对象检测性能的4.5个MAP点。
</details></li>
</ul>
<hr>
<h2 id="Distilling-Inductive-Bias-Knowledge-Distillation-Beyond-Model-Compression"><a href="#Distilling-Inductive-Bias-Knowledge-Distillation-Beyond-Model-Compression" class="headerlink" title="Distilling Inductive Bias: Knowledge Distillation Beyond Model Compression"></a>Distilling Inductive Bias: Knowledge Distillation Beyond Model Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00369">http://arxiv.org/abs/2310.00369</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gousia Habib, Tausifa Jan Saleem, Brejesh Lall</li>
<li>for: 提高计算效率和实用性，为计算机视觉应用程序提供实用的解决方案。</li>
<li>methods: 采用 ensemble-based distillation 方法，从多种不同架构的轻量级教师模型中继承知识，以提高学生模型的性能。</li>
<li>results: 通过将多种架构的轻量级教师模型 ensemble 教学，学生模型可以从 readily 可识别的存储 dataset 中积累广泛的知识，提高学生模型的性能。<details>
<summary>Abstract</summary>
With the rapid development of computer vision, Vision Transformers (ViTs) offer the tantalizing prospect of unified information processing across visual and textual domains. But due to the lack of inherent inductive biases in ViTs, they require enormous amount of data for training. To make their applications practical, we introduce an innovative ensemble-based distillation approach distilling inductive bias from complementary lightweight teacher models. Prior systems relied solely on convolution-based teaching. However, this method incorporates an ensemble of light teachers with different architectural tendencies, such as convolution and involution, to instruct the student transformer jointly. Because of these unique inductive biases, instructors can accumulate a wide range of knowledge, even from readily identifiable stored datasets, which leads to enhanced student performance. Our proposed framework also involves precomputing and storing logits in advance, essentially the unnormalized predictions of the model. This optimization can accelerate the distillation process by eliminating the need for repeated forward passes during knowledge distillation, significantly reducing the computational burden and enhancing efficiency.
</details>
<details>
<summary>摘要</summary>
随着计算机视觉的快速发展，视觉变换器（ViTs）提供了融合视觉和文本领域的信息处理的吸引人可能性。然而，由于变换器缺乏自然的逻辑假设，因此需要很大量的数据进行训练。为了使其应用实用，我们提出了一种创新的ensemble-based distillation方法，将各种轻量级教学模型中的 inductive bias 逻辑假设传播给学生变换器。先前的系统仅仅依靠卷积而教学，而我们的方法则是结合不同的建筑倾向，如卷积和反卷积，来教学学生变换器。由于这些特有的逻辑假设，教师可以吸收广泛的知识，甚至从Ready readily可识别的存储数据集中，这导致了学生的性能提高。我们的提议的框架还包括预计算和存储logits的步骤，实际上是模型的未正规化预测值。这种优化可以减少了知识传播过程中的计算负担，使得学生的训练变得更加高效，提高了效率。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Posterior-Illumination-for-Ambiguity-aware-Inverse-Rendering"><a href="#Diffusion-Posterior-Illumination-for-Ambiguity-aware-Inverse-Rendering" class="headerlink" title="Diffusion Posterior Illumination for Ambiguity-aware Inverse Rendering"></a>Diffusion Posterior Illumination for Ambiguity-aware Inverse Rendering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00362">http://arxiv.org/abs/2310.00362</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linjie Lyu, Ayush Tewari, Marc Habermann, Shunsuke Saito, Michael Zollhöfer, Thomas Leimkühler, Christian Theobalt</li>
<li>for:  inverse rendering, 即从图像中推断场景属性的问题</li>
<li>methods:  integrate了一种含有自然照明地图的扩散概率模型，并与可微分的跟踪器结合使用优化框架</li>
<li>results: 可以从多种照明和空间分布式表面材料中采样，并且能够生成高质量、多样化的环境地图样本，并准确地反映输入图像的照明情况。<details>
<summary>Abstract</summary>
Inverse rendering, the process of inferring scene properties from images, is a challenging inverse problem. The task is ill-posed, as many different scene configurations can give rise to the same image. Most existing solutions incorporate priors into the inverse-rendering pipeline to encourage plausible solutions, but they do not consider the inherent ambiguities and the multi-modal distribution of possible decompositions. In this work, we propose a novel scheme that integrates a denoising diffusion probabilistic model pre-trained on natural illumination maps into an optimization framework involving a differentiable path tracer. The proposed method allows sampling from combinations of illumination and spatially-varying surface materials that are, both, natural and explain the image observations. We further conduct an extensive comparative study of different priors on illumination used in previous work on inverse rendering. Our method excels in recovering materials and producing highly realistic and diverse environment map samples that faithfully explain the illumination of the input images.
</details>
<details>
<summary>摘要</summary>
<<SYS>> invertible rendering，将场景属性从图像中推算出来的过程，是一个具有很多挑战的反向问题。任务是不定的，因为多种场景配置都可以导致同一张图像。大多数现有的解决方案将约束加入反向渲染管道中，以便推导可能的解决方案，但它们没有考虑内在的抽象和多模分布。在这项工作中，我们提议一种新的方案，即将自然照明地图预训练的杂化扩散概率模型integrated到一个可导的跟踪器框架中。该方法允许采样从组合照明和空间分布的表面材料中，这些材料都是自然的，并且能够解释输入图像的照明。我们进一步进行了对先前工作中不同照明约束的比较研究。我们的方法在恢复材料和生成高真实度、多样化的环境地图样本方面表现出色，能够准确地解释输入图像的照明。
</details></li>
</ul>
<hr>
<h2 id="Improving-Cross-dataset-Deepfake-Detection-with-Deep-Information-Decomposition"><a href="#Improving-Cross-dataset-Deepfake-Detection-with-Deep-Information-Decomposition" class="headerlink" title="Improving Cross-dataset Deepfake Detection with Deep Information Decomposition"></a>Improving Cross-dataset Deepfake Detection with Deep Information Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00359">http://arxiv.org/abs/2310.00359</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shanmin Yang, Shu Hu, Bin Zhu, Ying Fu, Siwei Lyu, Xi Wu, Xin Wang</li>
<li>for: 防止深伪技术威胁安全和社会信任，这篇论文旨在提出一个深信息分解（DID）框架。</li>
<li>methods: 该框架将注重高水平 semantics 特征，而不是视觉特征，以提高深伪检测的稳定性和普遍能力。</li>
<li>results: 实验结果显示，该框架在不同测试数据集中具有更高的检测精度和普遍能力，并且能够对不同的伪造方法进行更好的检测。<details>
<summary>Abstract</summary>
Deepfake technology poses a significant threat to security and social trust. Although existing detection methods have demonstrated high performance in identifying forgeries within datasets using the same techniques for training and testing, they suffer from sharp performance degradation when faced with cross-dataset scenarios where unseen deepfake techniques are tested. To address this challenge, we propose a deep information decomposition (DID) framework in this paper. Unlike most existing deepfake detection methods, our framework prioritizes high-level semantic features over visual artifacts. Specifically, it decomposes facial features into deepfake-related and irrelevant information and optimizes the deepfake information for real/fake discrimination to be independent of other factors. Our approach improves the robustness of deepfake detection against various irrelevant information changes and enhances the generalization ability of the framework to detect unseen forgery methods. Extensive experimental comparisons with existing state-of-the-art detection methods validate the effectiveness and superiority of the DID framework on cross-dataset deepfake detection.
</details>
<details>
<summary>摘要</summary>
深刻的假动作技术对安全和社会信任具有重大威胁。 although existing detection methods have shown high performance in identifying forgeries within datasets using the same techniques for training and testing, they suffer from sharp performance degradation when faced with cross-dataset scenarios where unseen deepfake techniques are tested. To address this challenge, we propose a deep information decomposition (DID) framework in this paper. Unlike most existing deepfake detection methods, our framework prioritizes high-level semantic features over visual artifacts. Specifically, it decomposes facial features into deepfake-related and irrelevant information and optimizes the deepfake information for real/fake discrimination to be independent of other factors. Our approach improves the robustness of deepfake detection against various irrelevant information changes and enhances the generalization ability of the framework to detect unseen forgery methods. Extensive experimental comparisons with existing state-of-the-art detection methods validate the effectiveness and superiority of the DID framework on cross-dataset deepfake detection.Here's the translation breakdown:* 深刻的假动作技术 (shēn kòng de zhèng zhī yì jī jī) - deepfake technology* 对安全和社会信任 (duì ān qì yè shè qì) - pose a significant threat to security and social trust*  existing detection methods (zhèng zhī yì jī) - existing detection methods*  have demonstrated high performance (zhèng zhī yì jī) - have demonstrated high performance*  in identifying forgeries within datasets (shuì zhèng zhī yì jī) - within datasets*  using the same techniques for training and testing (yì jī yuè xíng) - using the same techniques for training and testing*  but they suffer from sharp performance degradation (but they suffer from sharp performance degradation)* when faced with cross-dataset scenarios (zhèng zhī yì jī zhèng zhī yì jī) - when faced with cross-dataset scenarios* where unseen deepfake techniques are tested (where unseen deepfake techniques are tested)* To address this challenge, we propose (To address this challenge, we propose)* a deep information decomposition (DID) framework (a deep information decomposition (DID) framework)* Unlike most existing deepfake detection methods (zhèng zhī yì jī yuè xíng) - Unlike most existing deepfake detection methods* our framework prioritizes high-level semantic features (our framework prioritizes high-level semantic features)* over visual artifacts (over visual artifacts)* Specifically, it decomposes facial features (Specifically, it decomposes facial features)* into deepfake-related and irrelevant information (into deepfake-related and irrelevant information)* and optimizes the deepfake information (and optimizes the deepfake information)* for real/fake discrimination to be independent of other factors (for real/fake discrimination to be independent of other factors)* Our approach improves the robustness (Our approach improves the robustness)* of deepfake detection (of deepfake detection)* against various irrelevant information changes (against various irrelevant information changes)* and enhances the generalization ability (and enhances the generalization ability)* of the framework to detect unseen forgery methods (of the framework to detect unseen forgery methods)* Extensive experimental comparisons (Extensive experimental comparisons)* with existing state-of-the-art detection methods (with existing state-of-the-art detection methods)* validate the effectiveness (validate the effectiveness)* and superiority (and superiority)* of the DID framework (of the DID framework)* on cross-dataset deepfake detection (on cross-dataset deepfake detection)
</details></li>
</ul>
<hr>
<h2 id="Structural-Adversarial-Objectives-for-Self-Supervised-Representation-Learning"><a href="#Structural-Adversarial-Objectives-for-Self-Supervised-Representation-Learning" class="headerlink" title="Structural Adversarial Objectives for Self-Supervised Representation Learning"></a>Structural Adversarial Objectives for Self-Supervised Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00357">http://arxiv.org/abs/2310.00357</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiao7199/structural-adversarial-objectives">https://github.com/xiao7199/structural-adversarial-objectives</a></li>
<li>paper_authors: Xiao Zhang, Michael Maire</li>
<li>for: 本研究使用GANs进行自动化的表示学习，以提高图像识别的性能。</li>
<li>methods: 本文提出了一种基于GANs的自然语言生成方法，通过追加一些结构化模型责任来使混合网络学习表示。</li>
<li>results: 实验表明，通过使用本文提出的自然语言生成方法，GANs可以学习出高质量的表示，与对比学习方法相当。<details>
<summary>Abstract</summary>
Within the framework of generative adversarial networks (GANs), we propose objectives that task the discriminator for self-supervised representation learning via additional structural modeling responsibilities. In combination with an efficient smoothness regularizer imposed on the network, these objectives guide the discriminator to learn to extract informative representations, while maintaining a generator capable of sampling from the domain. Specifically, our objectives encourage the discriminator to structure features at two levels of granularity: aligning distribution characteristics, such as mean and variance, at coarse scales, and grouping features into local clusters at finer scales. Operating as a feature learner within the GAN framework frees our self-supervised system from the reliance on hand-crafted data augmentation schemes that are prevalent across contrastive representation learning methods. Across CIFAR-10/100 and an ImageNet subset, experiments demonstrate that equipping GANs with our self-supervised objectives suffices to produce discriminators which, evaluated in terms of representation learning, compete with networks trained by contrastive learning approaches.
</details>
<details>
<summary>摘要</summary>
在生成对抗网络（GAN）框架内，我们提议一些目标，要让分类器进行自我超vised学习的表示学习。这些目标与网络中的简洁正则化相结合，导引分类器学习提取有用的表示，同时保持生成器可以从领域中随机抽取样本。具体来说，我们的目标让分类器在两级划分粒度上结构化特征：在大规模划分水平上对分布特征进行匹配，并在细规划分水平上将特征分组到本地团集中。作为GAN框架内的特征学习器，我们的自我超vised系统不需要靠手工设计的数据增强方案，这种方法在对比学习方法中广泛存在。在CIFAR-10/100和ImageNet子集上进行了实验，发现当我们将GAN equip with我们的自我超vised目标时，评价在表示学习方面的分类器与对比学习方法训练的网络相比，具有竞争力。
</details></li>
</ul>
<hr>
<h2 id="RBF-Weighted-Hyper-Involution-for-RGB-D-Object-Detection"><a href="#RBF-Weighted-Hyper-Involution-for-RGB-D-Object-Detection" class="headerlink" title="RBF Weighted Hyper-Involution for RGB-D Object Detection"></a>RBF Weighted Hyper-Involution for RGB-D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00342">http://arxiv.org/abs/2310.00342</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mehfuz A Rahman, Jiju Peethambaran, Neil London</li>
<li>for: 实时RGBD物体检测模型</li>
<li>methods: 提议使用深度导航强化和升降样本联合层</li>
<li>results: 在NYU Depth v2和SUN RGB-D datasets上显示出比其他RGB-D基于物体检测模型更高的性能，并在新的室外RGB-D物体检测数据集上取得了最佳性能。<details>
<summary>Abstract</summary>
A vast majority of conventional augmented reality devices are equipped with depth sensors. Depth images produced by such sensors contain complementary information for object detection when used with color images. Despite the benefits, it remains a complex task to simultaneously extract photometric and depth features in real time due to the immanent difference between depth and color images. Moreover, standard convolution operations are not sufficient to properly extract information directly from raw depth images leading to intermediate representations of depth which is inefficient. To address these issues, we propose a real-time and two stream RGBD object detection model. The proposed model consists of two new components: a depth guided hyper-involution that adapts dynamically based on the spatial interaction pattern in the raw depth map and an up-sampling based trainable fusion layer that combines the extracted depth and color image features without blocking the information transfer between them. We show that the proposed model outperforms other RGB-D based object detection models on NYU Depth v2 dataset and achieves comparable (second best) results on SUN RGB-D. Additionally, we introduce a new outdoor RGB-D object detection dataset where our proposed model outperforms other models. The performance evaluation on diverse synthetic data generated from CAD models and images shows the potential of the proposed model to be adapted to augmented reality based applications.
</details>
<details>
<summary>摘要</summary>
大多数传统增强现实设备都配备有深度传感器。深度图像生成于such传感器中包含补偿信息，可以帮助对象检测。despite the benefits, it remains a complex task to simultaneously extract photometric and depth features in real time due to the inherent difference between depth and color images. Moreover, standard convolution operations are not sufficient to properly extract information directly from raw depth images leading to intermediate representations of depth, which is inefficient. To address these issues, we propose a real-time and two-stream RGBD object detection model. The proposed model consists of two new components: a depth-guided hyper-evolution that adapts dynamically based on the spatial interaction pattern in the raw depth map and an up-sampling based trainable fusion layer that combines the extracted depth and color image features without blocking the information transfer between them. We show that the proposed model outperforms other RGB-D based object detection models on NYU Depth v2 dataset and achieves comparable (second best) results on SUN RGB-D. Additionally, we introduce a new outdoor RGB-D object detection dataset where our proposed model outperforms other models. The performance evaluation on diverse synthetic data generated from CAD models and images shows the potential of the proposed model to be adapted to augmented reality based applications.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard Chinese languages. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="MFL-Data-Preprocessing-and-CNN-based-Oil-Pipeline-Defects-Detection"><a href="#MFL-Data-Preprocessing-and-CNN-based-Oil-Pipeline-Defects-Detection" class="headerlink" title="MFL Data Preprocessing and CNN-based Oil Pipeline Defects Detection"></a>MFL Data Preprocessing and CNN-based Oil Pipeline Defects Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00332">http://arxiv.org/abs/2310.00332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iurii Katser, Vyacheslav Kozitsin, Igor Mozolin</li>
<li>for: 这个论文主要用于检测石油管道异常现象，以提高运输系统的可靠性和安全性。</li>
<li>methods: 该论文使用了最新的卷积神经网络结构，并提出了一些有效的预处理技术和检测方法，以解决现有数据的限制。</li>
<li>results: 该论文通过使用实际数据进行验证，并达到了高度的性能水平，以增强石油管道异常检测的精度和效果。<details>
<summary>Abstract</summary>
Recently, the application of computer vision for anomaly detection has been under attention in several industrial fields. An important example is oil pipeline defect detection. Failure of one oil pipeline can interrupt the operation of the entire transportation system or cause a far-reaching failure. The automated defect detection could significantly decrease the inspection time and the related costs. However, there is a gap in the related literature when it comes to dealing with this task. The existing studies do not sufficiently cover the research of the Magnetic Flux Leakage data and the preprocessing techniques that allow overcoming the limitations set by the available data. This work focuses on alleviating these issues. Moreover, in doing so, we exploited the recent convolutional neural network structures and proposed robust approaches, aiming to acquire high performance considering the related metrics. The proposed approaches and their applicability were verified using real-world data.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:近期，计算机视觉在各个 industrielle 领域中的应用异常检测引起了关注。一个重要的例子是油管缺陷检测。油管缺陷的失效可以中断交通系统的全部运行或者引起广泛的故障。自动检测可以显著减少检测时间和相关成本。然而，现有的相关文献不充分考虑了阻碍数据的限制和磁漏泄检测数据的研究。这个工作强调解决这些问题。此外，我们还利用了最新的卷积神经网络结构，并提出了可靠的方法，以达到考虑相关维度的高性能。我们的方法和其可应用性得到了实际数据的验证。
</details></li>
</ul>
<hr>
<h2 id="Decoding-Realistic-Images-from-Brain-Activity-with-Contrastive-Self-supervision-and-Latent-Diffusion"><a href="#Decoding-Realistic-Images-from-Brain-Activity-with-Contrastive-Self-supervision-and-Latent-Diffusion" class="headerlink" title="Decoding Realistic Images from Brain Activity with Contrastive Self-supervision and Latent Diffusion"></a>Decoding Realistic Images from Brain Activity with Contrastive Self-supervision and Latent Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00318">http://arxiv.org/abs/2310.00318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingyuan Sun, Mingxiao Li, Marie-Francine Moens</li>
<li>for: 提高我们对大脑视系统的理解和计算机视觉模型之间的连接，使用人脑活动重建视觉刺激。</li>
<li>methods: 使用深度生成模型进行重建，并采用自我超级vised contrastive learning获取fMRI数据表示。</li>
<li>results: 实验结果显示，CnD可以高效重建复杂图像，并提供了对LDM组件和人脑视系统之间的量化解释。<details>
<summary>Abstract</summary>
Reconstructing visual stimuli from human brain activities provides a promising opportunity to advance our understanding of the brain's visual system and its connection with computer vision models. Although deep generative models have been employed for this task, the challenge of generating high-quality images with accurate semantics persists due to the intricate underlying representations of brain signals and the limited availability of parallel data. In this paper, we propose a two-phase framework named Contrast and Diffuse (CnD) to decode realistic images from functional magnetic resonance imaging (fMRI) recordings. In the first phase, we acquire representations of fMRI data through self-supervised contrastive learning. In the second phase, the encoded fMRI representations condition the diffusion model to reconstruct visual stimulus through our proposed concept-aware conditioning method. Experimental results show that CnD reconstructs highly plausible images on challenging benchmarks. We also provide a quantitative interpretation of the connection between the latent diffusion model (LDM) components and the human brain's visual system. In summary, we present an effective approach for reconstructing visual stimuli based on human brain activity and offer a novel framework to understand the relationship between the diffusion model and the human brain visual system.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将人脑活动转化为可读图像，提供了推进我们对大脑视系统的理解和计算机视觉模型之间的连接的可能性。虽然深入的生成模型已经被应用于这项任务，但是生成高质量图像的挑战仍然存在，因为大脑信号的下面表示和数据并不充分。在这篇论文中，我们提出了一种两阶段框架，名为对比和散射（CnD），用于从功能核磁共振图像记录中重建真实的图像。在第一阶段，我们通过自我超级vised对比学习获得了fMRI数据的表示。在第二阶段，这些编码的fMRI表示条件了我们提出的概念意识对应方法，使得扩散模型重建视觉刺激。实验结果表明，CnD可以在复杂的标准 benchmark 上重建高可能性的图像。我们还提供了人脑视系统中LDM组件和扩散模型之间的量化解释。总之，我们提出了基于人脑活动的可读图像重建方法，并提供了扩散模型和人脑视系统之间的新框架。
</details></li>
</ul>
<hr>
<h2 id="An-easy-zero-shot-learning-combination-Texture-Sensitive-Semantic-Segmentation-IceHrNet-and-Advanced-Style-Transfer-Learning-Strategy"><a href="#An-easy-zero-shot-learning-combination-Texture-Sensitive-Semantic-Segmentation-IceHrNet-and-Advanced-Style-Transfer-Learning-Strategy" class="headerlink" title="An easy zero-shot learning combination: Texture Sensitive Semantic Segmentation IceHrNet and Advanced Style Transfer Learning Strategy"></a>An easy zero-shot learning combination: Texture Sensitive Semantic Segmentation IceHrNet and Advanced Style Transfer Learning Strategy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00310">http://arxiv.org/abs/2310.00310</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pl23k/icehrnet">https://github.com/pl23k/icehrnet</a></li>
<li>paper_authors: Zhiyong Yang, Yuelong Zhu, Xiaoqin Zeng, Jun Zong, Xiuheng Liu, Ran Tao, Xiaofei Cong, Yufeng Yu</li>
<li>for: 本研究旨在提出一种简单的零shot语义 segmentation方法，使用style transfer来实现。</li>
<li>methods: 我们使用了医学影像数据集（血液图像）来训练一个river ice语义 segmentation模型。首先，我们构建了一个river ice语义 segmentation数据集IPC_RI_SEG，使用固定摄像头和涵盖整个河流冰融化过程。其次，我们提出了一种高分辨率Texture Fusion semantic segmentation网络，名为IceHrNet。该网络使用HRNet作为背景，并添加了ASPP和Decoder segmentation头，以保留低级别的Texture特征，进行细致的语义分割。最后，我们提出了一种简单有效的高级 Style transfer学习策略，可以在交叉领域语义分割数据集上进行零shot转移学习，实现了87% mIoU的语义分割效果。</li>
<li>results: 实验显示，IceHrNet在Texture专注数据集IPC_RI_SEG上超过了现状的方法，并在Shape专注river ice数据集上达到了优秀的效果。在零shot转移学习中，IceHrNet比其他方法提高了2个百分点。我们的代码和模型已经发布在<a target="_blank" rel="noopener" href="https://github.com/PL23K/IceHrNet%E3%80%82">https://github.com/PL23K/IceHrNet。</a><details>
<summary>Abstract</summary>
We proposed an easy method of Zero-Shot semantic segmentation by using style transfer. In this case, we successfully used a medical imaging dataset (Blood Cell Imagery) to train a model for river ice semantic segmentation. First, we built a river ice semantic segmentation dataset IPC_RI_SEG using a fixed camera and covering the entire ice melting process of the river. Second, a high-resolution texture fusion semantic segmentation network named IceHrNet is proposed. The network used HRNet as the backbone and added ASPP and Decoder segmentation heads to retain low-level texture features for fine semantic segmentation. Finally, a simple and effective advanced style transfer learning strategy was proposed, which can perform zero-shot transfer learning based on cross-domain semantic segmentation datasets, achieving a practical effect of 87% mIoU for semantic segmentation of river ice without target training dataset (25% mIoU for None Stylized, 65% mIoU for Conventional Stylized, our strategy improved by 22%). Experiments showed that the IceHrNet outperformed the state-of-the-art methods on the texture-focused dataset IPC_RI_SEG, and achieved an excellent result on the shape-focused river ice datasets. In zero-shot transfer learning, IceHrNet achieved an increase of 2 percentage points compared to other methods. Our code and model are published on https://github.com/PL23K/IceHrNet.
</details>
<details>
<summary>摘要</summary>
我们提出了一种简单的零shot semantic segmentation方法，利用样式传输。在这种情况下，我们成功地使用医疗影像 dataset（血球影像）来训练一个河川冰 semantic segmentation 模型。首先，我们建立了一个河川冰 semantic segmentation dataset IPC_RI_SEG，使用固定摄像头和覆盖整个河川冰融化过程。其次，我们提出了一种高分辨率Texture Fusion semantic segmentation网络，名为 IceHrNet。该网络使用 HRNet 作为背景，并添加 ASPP 和 Decoder  segmentation 头，以保留低级别 Texture 特征，以提高精确的semantic segmentation。最后，我们提出了一种简单有效的高级 Style Transfer 学习策略，可以在 cross-domain semantic segmentation 数据集上进行零shot Transfer learning，实现了87% mIoU 的semantic segmentation精度，比无目标训练数据集 (25% mIoU for None Stylized, 65% mIoU for Conventional Stylized) 提高22%。实验显示，IceHrNet 在 texture-focused 数据集 IPC_RI_SEG 上超过了现状级方法，并在 shape-focused 河川冰数据集上达到了出色的结果。在零shot Transfer learning 中，IceHrNet 相比其他方法提高了2个百分点。我们的代码和模型已经在 <https://github.com/PL23K/IceHrNet> 上发布。
</details></li>
</ul>
<hr>
<h2 id="Dual-Augmented-Transformer-Network-for-Weakly-Supervised-Semantic-Segmentation"><a href="#Dual-Augmented-Transformer-Network-for-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="Dual-Augmented Transformer Network for Weakly Supervised Semantic Segmentation"></a>Dual-Augmented Transformer Network for Weakly Supervised Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00307">http://arxiv.org/abs/2310.00307</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingliang Deng, Zonghan Li</li>
<li>for: 这个论文目的是提出一种基于 dual-augmented transformer 网络和自我规则约束的弱监督Semantic Segmentation（WSSS）方法，以提高WSSS的完teness和准确性。</li>
<li>methods: 该方法使用了一个双网络，包括 CNN 和 transformer 网络，并在两个网络之间进行互补学习，以提高 WSSS 的性能。此外，该方法还使用了自我规则约束来避免过拟合。</li>
<li>results: 对于 PASCAL VOC 2012 难度评测 benchmark，该方法达到了最高的效果，超过了之前的州立艺术方法。<details>
<summary>Abstract</summary>
Weakly supervised semantic segmentation (WSSS), a fundamental computer vision task, which aims to segment out the object within only class-level labels. The traditional methods adopt the CNN-based network and utilize the class activation map (CAM) strategy to discover the object regions. However, such methods only focus on the most discriminative region of the object, resulting in incomplete segmentation. An alternative is to explore vision transformers (ViT) to encode the image to acquire the global semantic information. Yet, the lack of transductive bias to objects is a flaw of ViT. In this paper, we explore the dual-augmented transformer network with self-regularization constraints for WSSS. Specifically, we propose a dual network with both CNN-based and transformer networks for mutually complementary learning, where both networks augment the final output for enhancement. Massive systemic evaluations on the challenging PASCAL VOC 2012 benchmark demonstrate the effectiveness of our method, outperforming previous state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
弱类指导 semantic segmentation (WSSS) 是计算机视觉中的基本任务，旨在只使用类别标签来分割对象。传统方法通常采用 CNN 网络和使用 CAM 策略来发现对象区域。然而，这些方法只关注对象中最有特征的区域，导致 segmentation 不够完整。为了解决这问题，我们可以探索使用 transformer 网络来编码图像，以获得全局 semantic 信息。然而，transformer 网络缺乏对物体的推导性偏好。在这篇论文中，我们探索了 dual-augmented transformer 网络，并采用自我regularization 约束来解决 WSSS 问题。具体来说，我们提出了一个 dual 网络，其中包括 CNN 基于网络和 transformer 网络，用于互补学习。在 PASCAL VOC 2012 数据集上进行了大规模系统性评估，我们的方法被证明高效，并超过了之前的状态码法。
</details></li>
</ul>
<hr>
<h2 id="QUIZ-An-Arbitrary-Volumetric-Point-Matching-Method-for-Medical-Image-Registration"><a href="#QUIZ-An-Arbitrary-Volumetric-Point-Matching-Method-for-Medical-Image-Registration" class="headerlink" title="QUIZ: An Arbitrary Volumetric Point Matching Method for Medical Image Registration"></a>QUIZ: An Arbitrary Volumetric Point Matching Method for Medical Image Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00296">http://arxiv.org/abs/2310.00296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lin Liu, Xinxin Fan, Haoyang Liu, Chulong Zhang, Weibin Kong, Jingjing Dai, Yuming Jiang, Yaoqin Xie, Xiaokun Liang</li>
<li>for: 这个研究是为了提出一种基于自适应点对匹配的医疗影像注册方法，以解决现有方法在不同姿势或影像质量差强时的不稳定和错误问题。</li>
<li>methods: 本研究使用了一种新的方法，即查询点问题（QUIZ），它是基于本地-全局匹配点的对应，使用了对应点进行特征提取，并使用Transformer架构进行全球匹配查询，最后是通过实现本地影像对称变换。</li>
<li>results: 实验结果显示，这种方法在一个大尺度变形的阴道癌病人 dataset 上的注册结果比现有方法更为稳定和准确，甚至在跨Modal subjects 上也能够取得更好的结果，超过现有state-of-the-art。<details>
<summary>Abstract</summary>
Rigid pre-registration involving local-global matching or other large deformation scenarios is crucial. Current popular methods rely on unsupervised learning based on grayscale similarity, but under circumstances where different poses lead to varying tissue structures, or where image quality is poor, these methods tend to exhibit instability and inaccuracies. In this study, we propose a novel method for medical image registration based on arbitrary voxel point of interest matching, called query point quizzer (QUIZ). QUIZ focuses on the correspondence between local-global matching points, specifically employing CNN for feature extraction and utilizing the Transformer architecture for global point matching queries, followed by applying average displacement for local image rigid transformation. We have validated this approach on a large deformation dataset of cervical cancer patients, with results indicating substantially smaller deviations compared to state-of-the-art methods. Remarkably, even for cross-modality subjects, it achieves results surpassing the current state-of-the-art.
</details>
<details>
<summary>摘要</summary>
rigid pre-registration involving local-global matching or other large deformation scenarios is crucial. current popular methods rely on unsupervised learning based on grayscale similarity, but under circumstances where different poses lead to varying tissue structures, or where image quality is poor, these methods tend to exhibit instability and inaccuracies. in this study, we propose a novel method for medical image registration based on arbitrary voxel point of interest matching, called query point quizzer (quiz). quiz focuses on the correspondence between local-global matching points, specifically employing cnn for feature extraction and utilizing the transformer architecture for global point matching queries, followed by applying average displacement for local image rigid transformation. we have validated this approach on a large deformation dataset of cervical cancer patients, with results indicating substantially smaller deviations compared to state-of-the-art methods. remarkably, even for cross-modality subjects, it achieves results surpassing the current state-of-the-art.Here's the breakdown of the translation:* rigid pre-registration: 固定预注册* involving local-global matching: 包括本地-全局匹配* or other large deformation scenarios: 或其他大型扭曲场景* is crucial: 是关键的* Current popular methods rely on unsupervised learning: 当前流行的方法依靠无监督学习* based on grayscale similarity: 基于灰度相似性* but under circumstances where different poses lead to varying tissue structures: 但在不同的姿势下导致组织结构变化* or where image quality is poor: 或图像质量不佳* these methods tend to exhibit instability and inaccuracies: 这些方法往往表现出不稳定和不准确* In this study, we propose a novel method: 在这项研究中，我们提出了一种新的方法* for medical image registration: 医疗图像注册* based on arbitrary voxel point of interest matching: 基于任意体素点的关注匹配* called query point quizzer (QUIZ): 称为查询点赛询（QUIZ）* QUIZ focuses on the correspondence between local-global matching points: 赛询集中注重本地-全局匹配点之间的匹配* specifically employing CNN for feature extraction: 特别利用CNN提取特征* and utilizing the Transformer architecture for global point matching queries: 并利用Transformer架构进行全局点匹配查询* followed by applying average displacement for local image rigid transformation: 然后应用平均偏移来实现本地图像固定变换* We have validated this approach on a large deformation dataset of cervical cancer patients: 我们在一个大型扭曲 dataset 上验证了这种方法* with results indicating substantially smaller deviations compared to state-of-the-art methods: 结果表明与当前状态艺的方法相比，这种方法具有较小的偏差* Remarkably, even for cross-modality subjects: 备受惊叹的是，这种方法可以在不同的modalities中进行批处理* it achieves results surpassing the current state-of-the-art: 它在当前状态艺中超越了当前的最佳性能I hope this helps! Let me know if you have any further questions or if there's anything else I can help you with.
</details></li>
</ul>
<hr>
<h2 id="Pubic-Symphysis-Fetal-Head-Segmentation-Using-Pure-Transformer-with-Bi-level-Routing-Attention"><a href="#Pubic-Symphysis-Fetal-Head-Segmentation-Using-Pure-Transformer-with-Bi-level-Routing-Attention" class="headerlink" title="Pubic Symphysis-Fetal Head Segmentation Using Pure Transformer with Bi-level Routing Attention"></a>Pubic Symphysis-Fetal Head Segmentation Using Pure Transformer with Bi-level Routing Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00289">http://arxiv.org/abs/2310.00289</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pengzhou Cai, Jiang Lu, Yanxin Li, Libin Lan</li>
<li>for: 这个论文是为了解决公针缘-胎头分割任务而提出的方法。</li>
<li>methods: 该方法采用了一种类似于U-Net的纯转换器架构，并使用了二级路由注意力和跳过连接，能够有效地学习本地-全球含义。</li>
<li>results: 该方法在 транс体内超声影像数据集上进行评估，并达到了相当于的最终分数。代码将在 GitHub 上公开。<details>
<summary>Abstract</summary>
In this paper, we propose a method, named BRAU-Net, to solve the pubic symphysis-fetal head segmentation task. The method adopts a U-Net-like pure Transformer architecture with bi-level routing attention and skip connections, which effectively learns local-global semantic information. The proposed BRAU-Net was evaluated on transperineal Ultrasound images dataset from the pubic symphysis-fetal head segmentation and angle of progression (FH-PS-AOP) challenge. The results demonstrate that the proposed BRAU-Net achieves comparable a final score. The codes will be available at https://github.com/Caipengzhou/BRAU-Net.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种方法，名为BRAU-Net，用于解决公钵缝-胎头分割任务。该方法采用了一种类似于U-Net的纯Transformer架构，具有 би层路由注意力和跳过连接，能够有效学习本地-全局semantic信息。我们提posed的BRAU-Net在transperineal Ultrasound图像数据集上进行了评估，并实现了相对比较高的最终分数。codes将在https://github.com/Caipengzhou/BRAU-Net中提供。
</details></li>
</ul>
<hr>
<h2 id="InFER-A-Multi-Ethnic-Indian-Facial-Expression-Recognition-Dataset"><a href="#InFER-A-Multi-Ethnic-Indian-Facial-Expression-Recognition-Dataset" class="headerlink" title="InFER: A Multi-Ethnic Indian Facial Expression Recognition Dataset"></a>InFER: A Multi-Ethnic Indian Facial Expression Recognition Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00287">http://arxiv.org/abs/2310.00287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Syed Sameen Ahmad Rizvi, Preyansh Agrawal, Jagat Sesh Challa, Pratik Narang</li>
<li>for: 这个论文是为了开发一个面部表达识别系统，特别是针对印度次大陆的多元人种背景下的人脸表达识别。</li>
<li>methods: 这个论文使用了深度学习技术，并使用了10200张图片和4200段视频，包括7种基本的面部表达和6000张来自互联网的自然表达。</li>
<li>results: 这个论文通过实验表明，使用深度学习技术可以在印度次大陆的多元人种背景下实现高度的人脸表达识别精度。<details>
<summary>Abstract</summary>
The rapid advancement in deep learning over the past decade has transformed Facial Expression Recognition (FER) systems, as newer methods have been proposed that outperform the existing traditional handcrafted techniques. However, such a supervised learning approach requires a sufficiently large training dataset covering all the possible scenarios. And since most people exhibit facial expressions based upon their age group, gender, and ethnicity, a diverse facial expression dataset is needed. This becomes even more crucial while developing a FER system for the Indian subcontinent, which comprises of a diverse multi-ethnic population. In this work, we present InFER, a real-world multi-ethnic Indian Facial Expression Recognition dataset consisting of 10,200 images and 4,200 short videos of seven basic facial expressions. The dataset has posed expressions of 600 human subjects, and spontaneous/acted expressions of 6000 images crowd-sourced from the internet. To the best of our knowledge InFER is the first of its kind consisting of images from 600 subjects from very diverse ethnicity of the Indian Subcontinent. We also present the experimental results of baseline & deep FER methods on our dataset to substantiate its usability in real-world practical applications.
</details>
<details>
<summary>摘要</summary>
随着深度学习技术的快速发展，过去十年，人脸表达识别（FER）系统得到了深刻的改进，新的方法被提出，超越了传统的手工设计方法。然而，这种监督学习方法需要一个具有所有可能情况的充分大的训练数据集。而人们的表达往往与年龄组、性别和民族相关，因此需要一个多样化的人脸表达数据集。这变得更加重要，在开发印度次大陆的FER系统时。在这种情况下，我们提出了InFER，一个包含10,200张图像和4,200个短视频的多元族裔印度人脸表达识别数据集。该数据集包含1000名人类的pose表达和互联网上抓取的6000张自然表达图像。根据我们所知，InFER是世界上第一个包含600名不同民族背景的人脸表达数据集。我们还将展示基线和深度FER方法在我们的数据集上的实验结果，以证明其在实际应用中的可用性。
</details></li>
</ul>
<hr>
<h2 id="Unleash-Data-Generation-for-Efficient-and-Effective-Data-free-Knowledge-Distillation"><a href="#Unleash-Data-Generation-for-Efficient-and-Effective-Data-free-Knowledge-Distillation" class="headerlink" title="Unleash Data Generation for Efficient and Effective Data-free Knowledge Distillation"></a>Unleash Data Generation for Efficient and Effective Data-free Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00258">http://arxiv.org/abs/2310.00258</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fw742211/nayer">https://github.com/fw742211/nayer</a></li>
<li>paper_authors: Minh-Tuan Tran, Trung Le, Xuan-May Le, Mehrtash Harandi, Quan Hung Tran, Dinh Phung</li>
<li>for: 这篇论文的目的是提出一种新的无数据知识传播（DFKD）方法，并且解决了现有方法无法从随机变量中生成高质量数据的问题。</li>
<li>methods: 本文提出的方法为杂凑层生成（NAYER），它将随机性源从输入转移到杂凑层，并使用具有含义的标签文字嵌入（LTE）作为输入。LTE能够含有丰富的意义ful inter-class信息，允许生成高质量数据，只需要几个训练步骤。同时，杂凑层可以解决标签信息的价值过滤问题，使模型不会过度强调标签信息。</li>
<li>results: 实验结果显示， NAYER 不仅超越了现有的方法，而且比前一些方法快5-15倍。<details>
<summary>Abstract</summary>
Data-Free Knowledge Distillation (DFKD) has recently made remarkable advancements with its core principle of transferring knowledge from a teacher neural network to a student neural network without requiring access to the original data. Nonetheless, existing approaches encounter a significant challenge when attempting to generate samples from random noise inputs, which inherently lack meaningful information. Consequently, these models struggle to effectively map this noise to the ground-truth sample distribution, resulting in the production of low-quality data and imposing substantial time requirements for training the generator. In this paper, we propose a novel Noisy Layer Generation method (NAYER) which relocates the randomness source from the input to a noisy layer and utilizes the meaningful label-text embedding (LTE) as the input. The significance of LTE lies in its ability to contain substantial meaningful inter-class information, enabling the generation of high-quality samples with only a few training steps. Simultaneously, the noisy layer plays a key role in addressing the issue of diversity in sample generation by preventing the model from overemphasizing the constrained label information. By reinitializing the noisy layer in each iteration, we aim to facilitate the generation of diverse samples while still retaining the method's efficiency, thanks to the ease of learning provided by LTE. Experiments carried out on multiple datasets demonstrate that our NAYER not only outperforms the state-of-the-art methods but also achieves speeds 5 to 15 times faster than previous approaches.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Data-Free Knowledge Distillation (DFKD) has recently made remarkable advancements with its core principle of transferring knowledge from a teacher neural network to a student neural network without requiring access to the original data. Nonetheless, existing approaches encounter a significant challenge when attempting to generate samples from random noise inputs, which inherently lack meaningful information. Consequently, these models struggle to effectively map this noise to the ground-truth sample distribution, resulting in the production of low-quality data and imposing substantial time requirements for training the generator. In this paper, we propose a novel Noisy Layer Generation method (NAYER) which relocates the randomness source from the input to a noisy layer and utilizes the meaningful label-text embedding (LTE) as the input. The significance of LTE lies in its ability to contain substantial meaningful inter-class information, enabling the generation of high-quality samples with only a few training steps. Simultaneously, the noisy layer plays a key role in addressing the issue of diversity in sample generation by preventing the model from overemphasizing the constrained label information. By reinitializing the noisy layer in each iteration, we aim to facilitate the generation of diverse samples while still retaining the method's efficiency, thanks to the ease of learning provided by LTE. Experiments carried out on multiple datasets demonstrate that our NAYER not only outperforms the state-of-the-art methods but also achieves speeds 5 to 15 times faster than previous approaches."Translation:“数据无法知识传播（DFKD）最近又取得了显著进步，其核心思想是将知识从教师神经网络传播到学生神经网络，无需访问原始数据。然而，现有方法在生成随机噪声输入时遇到了重大挑战，因为这些噪声缺乏有意义信息。这使得这些模型很难准确地将噪声映射到真实样本分布，从而生成低质量数据，并且需要训练生成器的很长时间。在这篇论文中，我们提出了一种新的噪声层生成方法（NAYER），它将噪声源从输入重新定义到噪声层，并使用有意义的标签文本嵌入（LTE）作为输入。LTE的重要性在于它能够包含大量有意义的 между类信息，使得通过只需几个训练步骤就可以生成高质量样本。同时，噪声层对样本生成的多样性做出了重要贡献，避免模型过分强调约束的标签信息。我们在每个迭代中重新初始化噪声层，以便生成多样化的样本，同时仍保持方法的效率，即使是通过LTE的易学习性。我们在多个数据集上进行了实验，结果表明，我们的NAYER不仅超过了现有方法的性能，而且在5-15倍 faster than previous approaches。”
</details></li>
</ul>
<hr>
<h2 id="MMPI-a-Flexible-Radiance-Field-Representation-by-Multiple-Multi-plane-Images-Blending"><a href="#MMPI-a-Flexible-Radiance-Field-Representation-by-Multiple-Multi-plane-Images-Blending" class="headerlink" title="MMPI: a Flexible Radiance Field Representation by Multiple Multi-plane Images Blending"></a>MMPI: a Flexible Radiance Field Representation by Multiple Multi-plane Images Blending</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00249">http://arxiv.org/abs/2310.00249</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuze He, Peng Wang, Yubin Hu, Wang Zhao, Ran Yi, Yong-Jin Liu, Wenping Wang</li>
<li>for: 这篇论文旨在探讨基于多平面图像（MPI）的神经辐射场（NeRF）的高质量视图合成方法，以扩展现有的MPI-based NeRF方法到更复杂的场景中。</li>
<li>methods: 作者采用MPI parameterization的NeRF学习方法，并提出了一种基于多个MPI的适应混合操作，以模拟不同视角和摄像头分布的场景。</li>
<li>results: 实验结果表明，该方法可以高质量地生成不同摄像头分布和视角的新视图图像，并且比前一代快速训练NeRF方法更快速地训练完成。此外，作者还示出了该方法可以处理长轨迹和新视图图像的问题，表明其在自动驾驶等应用中的潜在可能性。<details>
<summary>Abstract</summary>
This paper presents a flexible representation of neural radiance fields based on multi-plane images (MPI), for high-quality view synthesis of complex scenes. MPI with Normalized Device Coordinate (NDC) parameterization is widely used in NeRF learning for its simple definition, easy calculation, and powerful ability to represent unbounded scenes. However, existing NeRF works that adopt MPI representation for novel view synthesis can only handle simple forward-facing unbounded scenes, where the input cameras are all observing in similar directions with small relative translations. Hence, extending these MPI-based methods to more complex scenes like large-range or even 360-degree scenes is very challenging. In this paper, we explore the potential of MPI and show that MPI can synthesize high-quality novel views of complex scenes with diverse camera distributions and view directions, which are not only limited to simple forward-facing scenes. Our key idea is to encode the neural radiance field with multiple MPIs facing different directions and blend them with an adaptive blending operation. For each region of the scene, the blending operation gives larger blending weights to those advantaged MPIs with stronger local representation abilities while giving lower weights to those with weaker representation abilities. Such blending operation automatically modulates the multiple MPIs to appropriately represent the diverse local density and color information. Experiments on the KITTI dataset and ScanNet dataset demonstrate that our proposed MMPI synthesizes high-quality images from diverse camera pose distributions and is fast to train, outperforming the previous fast-training NeRF methods for novel view synthesis. Moreover, we show that MMPI can encode extremely long trajectories and produce novel view renderings, demonstrating its potential in applications like autonomous driving.
</details>
<details>
<summary>摘要</summary>
Our key idea is to encode the neural radiance field with multiple MPIs facing different directions and blend them with an adaptive blending operation. For each region of the scene, the blending operation gives larger blending weights to those MPIs with stronger local representation abilities and lower weights to those with weaker representation abilities. This automatically modulates the multiple MPIs to appropriately represent the diverse local density and color information.Experiments on the KITTI dataset and ScanNet dataset show that our proposed method, called Multi-plane Multi-Image (MMPI), synthesizes high-quality images from diverse camera pose distributions and is fast to train, outperforming previous fast-training NeRF methods for novel view synthesis. Moreover, we demonstrate that MMPI can encode extremely long trajectories and produce novel view renderings, indicating its potential in applications like autonomous driving.
</details></li>
</ul>
<hr>
<h2 id="Walking-Traversable-Traversability-Prediction-via-Multiple-Human-Object-Tracking-under-Occlusion"><a href="#Walking-Traversable-Traversability-Prediction-via-Multiple-Human-Object-Tracking-under-Occlusion" class="headerlink" title="Walking &#x3D; Traversable? : Traversability Prediction via Multiple Human Object Tracking under Occlusion"></a>Walking &#x3D; Traversable? : Traversability Prediction via Multiple Human Object Tracking under Occlusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00242">http://arxiv.org/abs/2310.00242</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan Tay Yu Liang, Kanji Tanaka</li>
<li>for: 这种技术可以提高室内机器人导航，预测受阻的地板。</li>
<li>methods: 该方法使用第三人称视角单目摄像头，使用SLAM和MOT两种跟踪器监测站立物和移动人员的互动。</li>
<li>results: 该方法可以在视觉复杂enario中稳定地预测通行性，包括 occlusion、非线性视角、深度不确定和多个人员的交叠。<details>
<summary>Abstract</summary>
The emerging ``Floor plan from human trails (PfH)" technique has great potential for improving indoor robot navigation by predicting the traversability of occluded floors. This study presents an innovative approach that replaces first-person-view sensors with a third-person-view monocular camera mounted on the observer robot. This approach can gather measurements from multiple humans, expanding its range of applications. The key idea is to use two types of trackers, SLAM and MOT, to monitor stationary objects and moving humans and assess their interactions. This method achieves stable predictions of traversability even in challenging visual scenarios, such as occlusions, nonlinear perspectives, depth uncertainty, and intersections involving multiple humans. Additionally, we extend map quality metrics to apply to traversability maps, facilitating future research. We validate our proposed method through fusion and comparison with established techniques.
</details>
<details>
<summary>摘要</summary>
“人类脚踪映射（PfH）技术在室内机器人导航方面具有潜在的潜力，可以预测受阻的loor的可行性。本研究提出了一种创新的方法，替换了首人视角感知器，使用跟踪器Mounted on the observer robot的第三人视角监测器来收集多个人的数据。这种方法可以监测站ARYObjects和移动人员之间的互动，并对其进行评估。这种方法可以在视觉复杂场景中稳定地预测可行性，包括 occlusions、非线性视角、深度不确定性和多个人的交叉。此外，我们扩展了图像质量指标，以便应用于可行性图。我们验证了我们的提议方法通过融合和与现有技术进行比较。”Note that Simplified Chinese is the official standard for Chinese writing in mainland China, and it is used in this translation. Traditional Chinese is also commonly used in Taiwan and Hong Kong, but it may have slightly different grammar and character forms.
</details></li>
</ul>
<hr>
<h2 id="Learning-Mask-aware-CLIP-Representations-for-Zero-Shot-Segmentation"><a href="#Learning-Mask-aware-CLIP-Representations-for-Zero-Shot-Segmentation" class="headerlink" title="Learning Mask-aware CLIP Representations for Zero-Shot Segmentation"></a>Learning Mask-aware CLIP Representations for Zero-Shot Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00240">http://arxiv.org/abs/2310.00240</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiaosiyu1999/maft">https://github.com/jiaosiyu1999/maft</a></li>
<li>paper_authors: Siyu Jiao, Yunchao Wei, Yaowei Wang, Yao Zhao, Humphrey Shi<br>for:* The paper aims to improve the performance of zero-shot segmentation methods by addressing the insensitivity of CLIP to different mask proposals.methods:* The proposed method, Mask-aware Fine-tuning (MAFT), uses an Image-Proposals CLIP Encoder (IP-CLIP Encoder) to handle arbitrary numbers of image and mask proposals simultaneously.* MAFT introduces mask-aware loss and self-distillation loss to fine-tune IP-CLIP Encoder, ensuring CLIP is responsive to different mask proposals while maintaining transferability.results:* With MAFT, the performance of state-of-the-art methods is promoted by a large margin on popular zero-shot benchmarks, including COCO, Pascal-VOC, and ADE20K. Specifically, the mIoU for unseen classes is improved by 8.2%, 3.2%, and 4.3% respectively.<details>
<summary>Abstract</summary>
Recently, pre-trained vision-language models have been increasingly used to tackle the challenging zero-shot segmentation task. Typical solutions follow the paradigm of first generating mask proposals and then adopting CLIP to classify them. To maintain the CLIP's zero-shot transferability, previous practices favour to freeze CLIP during training. However, in the paper, we reveal that CLIP is insensitive to different mask proposals and tends to produce similar predictions for various mask proposals of the same image. This insensitivity results in numerous false positives when classifying mask proposals. This issue mainly relates to the fact that CLIP is trained with image-level supervision. To alleviate this issue, we propose a simple yet effective method, named Mask-aware Fine-tuning (MAFT). Specifically, Image-Proposals CLIP Encoder (IP-CLIP Encoder) is proposed to handle arbitrary numbers of image and mask proposals simultaneously. Then, mask-aware loss and self-distillation loss are designed to fine-tune IP-CLIP Encoder, ensuring CLIP is responsive to different mask proposals while not sacrificing transferability. In this way, mask-aware representations can be easily learned to make the true positives stand out. Notably, our solution can seamlessly plug into most existing methods without introducing any new parameters during the fine-tuning process. We conduct extensive experiments on the popular zero-shot benchmarks. With MAFT, the performance of the state-of-the-art methods is promoted by a large margin: 50.4% (+ 8.2%) on COCO, 81.8% (+ 3.2%) on Pascal-VOC, and 8.7% (+4.3%) on ADE20K in terms of mIoU for unseen classes. The code is available at https://github.com/jiaosiyu1999/MAFT.git.
</details>
<details>
<summary>摘要</summary>
近期，预训练的视觉语言模型在零样式分割任务中表现越来越出色。一般来说，这些解决方案采用的是首先生成mask提案，然后采用CLIP进行分类的方法。为保持CLIP的零样式传输性，以前的做法是冻结CLIP。然而，我们发现CLIP对不同的mask提案敏感度很低，它会为同一张图片的不同mask提案生成相同的预测结果。这种敏感度问题导致了许多假阳性的分类结果。这主要与CLIP在图像水平上进行训练有关。为解决这个问题，我们提出了一种简单 yet 有效的方法，即Mask-aware Fine-tuning（MAFT）。具体来说，我们提出了Image-Proposals CLIP Encoder（IP-CLIP Encoder），可以同时处理任意数量的图像和mask提案。然后，我们设计了面孔检测和自我顾问损失来细化IP-CLIP Encoder，确保CLIP对不同的mask提案具有响应性。这样，面孔检测可以轻松地学习出mask-aware表示，使真阳性能够出亮。另外，我们的解决方案可以不需要添加任何新参数，直接在已有的训练过程中进行细化。我们在popular zero-shot benchmark上进行了广泛的实验，与MAFT结合，state-of-the-art方法的性能得到了大幅提升：COCO中的50.4% (+ 8.2%)，Pascal-VOC中的81.8% (+ 3.2%)，ADE20K中的8.7% (+4.3%)。相关代码可以在https://github.com/jiaosiyu1999/MAFT.git中找到。
</details></li>
</ul>
<hr>
<h2 id="Domain-Controlled-Prompt-Learning"><a href="#Domain-Controlled-Prompt-Learning" class="headerlink" title="Domain-Controlled Prompt Learning"></a>Domain-Controlled Prompt Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07730">http://arxiv.org/abs/2310.07730</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sfedfcv/redesigned-pancake">https://github.com/Sfedfcv/redesigned-pancake</a></li>
<li>paper_authors: Qinglong Cao, Zhengqin Xu, Yuantian Chen, Chao Ma, Xiaokang Yang</li>
<li>for: 这个研究的目的是为特殊领域的Remote Sensing Image (RSIs) 和医学影像等领域进行适应和扩展。</li>
<li>methods: 我们提出了一种叫做领域控制的提示学习（Domain-Controlled Prompt Learning，DCPL），使用大规模的专业领域基础模型（LSDM）提供特殊领域知识，并使用轻量级神经网络将这些知识转换为领域偏好，以直接控制颜ppo上的提示。</li>
<li>results: 我们的方法在特殊领域影像识别 зада域中得到了最佳性能。<details>
<summary>Abstract</summary>
Large pre-trained vision-language models, such as CLIP, have shown remarkable generalization capabilities across various tasks when appropriate text prompts are provided. However, adapting these models to specialized domains, like remote sensing images (RSIs), medical images, etc, remains unexplored and challenging. Existing prompt learning methods often lack domain-awareness or domain-transfer mechanisms, leading to suboptimal performance due to the misinterpretation of specialized images in natural image patterns. To tackle this dilemma, we proposed a Domain-Controlled Prompt Learning for the specialized domains. Specifically, the large-scale specialized domain foundation model (LSDM) is first introduced to provide essential specialized domain knowledge. Using lightweight neural networks, we transfer this knowledge into domain biases, which control both the visual and language branches to obtain domain-adaptive prompts in a directly incorporating manner. Simultaneously, to overcome the existing overfitting challenge, we propose a novel noisy-adding strategy, without extra trainable parameters, to help the model escape the suboptimal solution in a global domain oscillation manner. Experimental results show our method achieves state-of-the-art performance in specialized domain image recognition datasets. Our code is available at https://anonymous.4open.science/r/DCPL-8588.
</details>
<details>
<summary>摘要</summary>
大型预训 vision-language模型，如CLIP，在不同任务中表现出了惊人的通用能力，但将这些模型适应特殊领域，如遥感图像（RSIs）、医疗图像等，仍然是一个未探索的和挑战性的领域。现有的提问学习方法通常缺乏领域意识或领域传输机制，导致特殊图像在自然图像模式中的误 interpret，从而影响表现。为解决这个困难，我们提出了领域控制的提问学习（DCPL）方法。具体来说，我们首先引入大规模特殊领域基础模型（LSDM），以提供特殊领域知识的基础。使用轻量级神经网络，我们将这些知识转移到领域偏好，以控制视觉和语言 Zweige，从而获得适应特殊领域的提问。同时，为了解决现有的过拟合挑战，我们提出了一种新的噪音添加策略，不需要额外的可训练参数，以帮助模型脱离低效解决方案。实验结果表明，我们的方法在特殊领域图像识别数据集中实现了状态略 луч的表现。我们的代码可以在https://anonymous.4open.science/r/DCPL-8588中找到。
</details></li>
</ul>
<hr>
<h2 id="Pixel-Inconsistency-Modeling-for-Image-Manipulation-Localization"><a href="#Pixel-Inconsistency-Modeling-for-Image-Manipulation-Localization" class="headerlink" title="Pixel-Inconsistency Modeling for Image Manipulation Localization"></a>Pixel-Inconsistency Modeling for Image Manipulation Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00234">http://arxiv.org/abs/2310.00234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenqi Kong, Anwei Luo, Shiqi Wang, Haoliang Li, Anderson Rocha, Alex C. Kot</li>
<li>for: 本研究旨在提高图像修饰检测的通用性和Robustness，以便更好地识别和 lokalisir forgery。</li>
<li>methods: 本文提出了一种基于自注意力的涂抹检测模型，通过分析图像中的像素不一致痕迹来检测修饰。此外，本文还提出了一种新的学习准备模块（LWM），用于将全像和局部像素相关性流合并到一起，从而提高检测性能。</li>
<li>results: 实验结果表明，本文提出的方法可以成功检测图像修饰，并且在不同的数据集和扰动图像上表现出优秀的通用性和Robustness。<details>
<summary>Abstract</summary>
Digital image forensics plays a crucial role in image authentication and manipulation localization. Despite the progress powered by deep neural networks, existing forgery localization methodologies exhibit limitations when deployed to unseen datasets and perturbed images (i.e., lack of generalization and robustness to real-world applications). To circumvent these problems and aid image integrity, this paper presents a generalized and robust manipulation localization model through the analysis of pixel inconsistency artifacts. The rationale is grounded on the observation that most image signal processors (ISP) involve the demosaicing process, which introduces pixel correlations in pristine images. Moreover, manipulating operations, including splicing, copy-move, and inpainting, directly affect such pixel regularity. We, therefore, first split the input image into several blocks and design masked self-attention mechanisms to model the global pixel dependency in input images. Simultaneously, we optimize another local pixel dependency stream to mine local manipulation clues within input forgery images. In addition, we design novel Learning-to-Weight Modules (LWM) to combine features from the two streams, thereby enhancing the final forgery localization performance. To improve the training process, we propose a novel Pixel-Inconsistency Data Augmentation (PIDA) strategy, driving the model to focus on capturing inherent pixel-level artifacts instead of mining semantic forgery traces. This work establishes a comprehensive benchmark integrating 15 representative detection models across 12 datasets. Extensive experiments show that our method successfully extracts inherent pixel-inconsistency forgery fingerprints and achieve state-of-the-art generalization and robustness performances in image manipulation localization.
</details>
<details>
<summary>摘要</summary>
“数字图像科学在图像认证和修改地址中扮演着关键角色。尽管深度神经网络的进步，现有的伪造地址方法在未见数据集和压缩图像上展示了局限性和不可靠性。为了缓解这些问题并帮助图像完整性，本文提出了一种通用和Robust的伪造地址模型，基于像素不一致痕迹的分析。我们认为大多数图像信号处理器（ISP）都包含排除过程，这会在原始图像中引入像素相关性。此外，操作包括拼接、复制、填充等，都会直接影响这种像素规律。因此，我们将输入图像分成多个块，并设计了带有mask的自注意力机制，以模型输入图像的全球像素依赖关系。同时，我们优化了另一个本地像素依赖流，以挖掘输入伪造图像中的本地伪造线索。此外，我们设计了一种新的学习加权模块（LWM），以将两条流合并，从而提高最终伪造地址性能。为了改进训练过程，我们提出了一种新的像素不一致数据增强策略（PIDA），使模型更加专注于捕捉内置像素级别的痕迹，而不是挖掘 semantic伪造迹象。这种工作建立了12个数据集上15种表示性检测模型的通用 benchmark。广泛的实验表明，我们的方法可以成功捕捉内置像素不一致伪造指纹，并在图像伪造地址方面实现了状态 искусственный intelligence的通用和Robust性表现。”
</details></li>
</ul>
<hr>
<h2 id="Scaling-for-Training-Time-and-Post-hoc-Out-of-distribution-Detection-Enhancement"><a href="#Scaling-for-Training-Time-and-Post-hoc-Out-of-distribution-Detection-Enhancement" class="headerlink" title="Scaling for Training Time and Post-hoc Out-of-distribution Detection Enhancement"></a>Scaling for Training Time and Post-hoc Out-of-distribution Detection Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00227">http://arxiv.org/abs/2310.00227</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kai422/scale">https://github.com/kai422/scale</a></li>
<li>paper_authors: Kai Xu, Rongyu Chen, Gianni Franchi, Angela Yao</li>
<li>for: This paper focuses on the task of out-of-distribution (OOD) detection in deep learning systems, specifically on the recent state-of-the-art method of activation shaping (ASH).</li>
<li>methods: The paper proposes two novel methods for OOD detection: 1) SCALE, a post-hoc network enhancement method that achieves state-of-the-art OOD detection performance without compromising in-distribution (ID) accuracy, and 2) Intermediate Tensor SHaping (ISH), a lightweight method for training time OOD detection enhancement.</li>
<li>results: The paper reports AUROC scores of +1.85% for near-OOD and +0.74% for far-OOD datasets on the OpenOOD v1.5 ImageNet-1K benchmark, demonstrating the effectiveness of the proposed methods for OOD detection.Here is the information in Simplified Chinese text:</li>
<li>for: 这篇论文关注深度学习系统中的 OUT-OF-DISTRIBUTION（OOD）检测任务，具体来说是最新的 activation shaping（ASH）方法。</li>
<li>methods: 论文提出了两种新的 OOD 检测方法：1） SCALE，一种后置网络增强方法，可以在保持 ID 准确率的情况下提高 OOD 检测性能，2） Intermediate Tensor SHaping（ISH），一种轻量级的训练时间 OOD 检测增强方法。</li>
<li>results: 论文报告了 OpenOOD v1.5 ImageNet-1K 测试集上的 AUROC 分数，分别为 +1.85% 和 +0.74%，这表明提出的方法对 OOD 检测具有效果。<details>
<summary>Abstract</summary>
The capacity of a modern deep learning system to determine if a sample falls within its realm of knowledge is fundamental and important. In this paper, we offer insights and analyses of recent state-of-the-art out-of-distribution (OOD) detection methods - extremely simple activation shaping (ASH). We demonstrate that activation pruning has a detrimental effect on OOD detection, while activation scaling enhances it. Moreover, we propose SCALE, a simple yet effective post-hoc network enhancement method for OOD detection, which attains state-of-the-art OOD detection performance without compromising in-distribution (ID) accuracy. By integrating scaling concepts into the training process to capture a sample's ID characteristics, we propose Intermediate Tensor SHaping (ISH), a lightweight method for training time OOD detection enhancement. We achieve AUROC scores of +1.85\% for near-OOD and +0.74\% for far-OOD datasets on the OpenOOD v1.5 ImageNet-1K benchmark. Our code and models are available at https://github.com/kai422/SCALE.
</details>
<details>
<summary>摘要</summary>
现代深度学习系统确定样本是否属于其知识范围是基本重要的。在这篇论文中，我们提供了近期状态艺术的out-of-distribution（OOD）检测方法的深入分析和见解，包括极简的活动形状（ASH）。我们表明了活动剪除对OOD检测有负面影响，而活动缩放则有利于其。此外，我们提议SCALE，一种简单又有效的后期网络增强方法，以实现OOD检测性能的状元。通过将扩展概念 integrate到训练过程中，以捕捉样本的ID特征，我们提议Intermediate Tensor SHaping（ISH），一种轻量级的训练时OOD检测增强方法。我们在OpenOOD v1.5 ImageNet-1K测试集上达到了AUROC分数+1.85%的近OOD数据集和+0.74%的远OOD数据集。我们的代码和模型可以在https://github.com/kai422/SCALE上获取。
</details></li>
</ul>
<hr>
<h2 id="LSOR-Longitudinally-Consistent-Self-Organized-Representation-Learning"><a href="#LSOR-Longitudinally-Consistent-Self-Organized-Representation-Learning" class="headerlink" title="LSOR: Longitudinally-Consistent Self-Organized Representation Learning"></a>LSOR: Longitudinally-Consistent Self-Organized Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00213">http://arxiv.org/abs/2310.00213</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ouyangjiahong/longitudinal-som-single-modality">https://github.com/ouyangjiahong/longitudinal-som-single-modality</a></li>
<li>paper_authors: Jiahong Ouyang, Qingyu Zhao, Ehsan Adeli, Wei Peng, Greg Zaharchuk, Kilian M. Pohl</li>
<li>for: 这个论文的目的是提出一种基于单modal MR 的自适应SOM方法，以提高深度学习模型在长itudinal MR 上的可读性。</li>
<li>methods: 该方法使用了自适应SOM，通过将高维的干扰空间分成多个集群，并将每个集群映射到一个离散的（通常是2D）网格上，以保持高维关系 between集群。</li>
<li>results: 该方法可以在长itudinal MR 上生成一个可读的干扰空间，并且可以在不同的诊断任务上达到或超过当前的状态艺 Representatives的性能。code available at <a target="_blank" rel="noopener" href="https://github.com/ouyangjiahong/longitudinal-som-single-modality%E3%80%82">https://github.com/ouyangjiahong/longitudinal-som-single-modality。</a><details>
<summary>Abstract</summary>
Interpretability is a key issue when applying deep learning models to longitudinal brain MRIs. One way to address this issue is by visualizing the high-dimensional latent spaces generated by deep learning via self-organizing maps (SOM). SOM separates the latent space into clusters and then maps the cluster centers to a discrete (typically 2D) grid preserving the high-dimensional relationship between clusters. However, learning SOM in a high-dimensional latent space tends to be unstable, especially in a self-supervision setting. Furthermore, the learned SOM grid does not necessarily capture clinically interesting information, such as brain age. To resolve these issues, we propose the first self-supervised SOM approach that derives a high-dimensional, interpretable representation stratified by brain age solely based on longitudinal brain MRIs (i.e., without demographic or cognitive information). Called Longitudinally-consistent Self-Organized Representation learning (LSOR), the method is stable during training as it relies on soft clustering (vs. the hard cluster assignments used by existing SOM). Furthermore, our approach generates a latent space stratified according to brain age by aligning trajectories inferred from longitudinal MRIs to the reference vector associated with the corresponding SOM cluster. When applied to longitudinal MRIs of the Alzheimer's Disease Neuroimaging Initiative (ADNI, N=632), LSOR generates an interpretable latent space and achieves comparable or higher accuracy than the state-of-the-art representations with respect to the downstream tasks of classification (static vs. progressive mild cognitive impairment) and regression (determining ADAS-Cog score of all subjects). The code is available at https://github.com/ouyangjiahong/longitudinal-som-single-modality.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate into Simplified Chinese��sterreichische Nationalbibliothek, Vienna, Austria� entitled Interpretability in Deep Learning for Longitudinal Brain MRIs, one challenge is the high dimensionality of the latent spaces generated by deep learning models. One approach to address this challenge is to visualize the latent spaces using self-organizing maps (SOM). However, learning SOM in high-dimensional latent spaces can be unstable, especially in self-supervised settings. Moreover, the learned SOM grid may not capture clinically meaningful information such as brain age.� To address these issues, we propose the first self-supervised SOM approach that derives a high-dimensional, interpretable representation stratified by brain age based solely on longitudinal brain MRIs. Called Longitudinally-consistent Self-Organized Representation learning (LSOR), the method is stable during training and relies on soft clustering instead of hard cluster assignments. Furthermore, our approach aligns trajectories inferred from longitudinal MRIs to the reference vector associated with the corresponding SOM cluster, generating a latent space stratified according to brain age.� When applied to longitudinal MRIs of the Alzheimer's Disease Neuroimaging Initiative (ADNI, N=632), LSOR generates an interpretable latent space and achieves comparable or higher accuracy than state-of-the-art representations with respect to downstream tasks such as classification (static vs. progressive mild cognitive impairment) and regression (determining ADAS-Cog score of all subjects). The code is available at https://github.com/ouyangjiahong/longitudinal-som-single-modality.� In summary, LSOR is a stable and interpretable deep learning method for longitudinal brain MRIs that captures brain age information and achieves high accuracy in downstream tasks.
</details></li>
</ul>
<hr>
<h2 id="DeformUX-Net-Exploring-a-3D-Foundation-Backbone-for-Medical-Image-Segmentation-with-Depthwise-Deformable-Convolution"><a href="#DeformUX-Net-Exploring-a-3D-Foundation-Backbone-for-Medical-Image-Segmentation-with-Depthwise-Deformable-Convolution" class="headerlink" title="DeformUX-Net: Exploring a 3D Foundation Backbone for Medical Image Segmentation with Depthwise Deformable Convolution"></a>DeformUX-Net: Exploring a 3D Foundation Backbone for Medical Image Segmentation with Depthwise Deformable Convolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00199">http://arxiv.org/abs/2310.00199</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/masilab/deform-uxnet">https://github.com/masilab/deform-uxnet</a></li>
<li>paper_authors: Ho Hin Lee, Quan Liu, Qi Yang, Xin Yu, Shunxing Bao, Yuankai Huo, Bennett A. Landman<br>for:* The paper is focused on improving medical image segmentation using 3D ViTs and deformable convolution.methods:* The proposed model, 3D DeformUX-Net, combines long-range dependency, adaptive spatial aggregation, and computational efficiency by revisiting volumetric deformable convolution in a depth-wise setting.* The model also includes a parallel branch for generating deformable tri-planar offsets, which provides adaptive spatial aggregation across all channels.results:* The proposed model consistently outperforms existing state-of-the-art ViTs and large kernel convolution models across four challenging public datasets, achieving better segmentation results in terms of mean Dice.<details>
<summary>Abstract</summary>
The application of 3D ViTs to medical image segmentation has seen remarkable strides, somewhat overshadowing the budding advancements in Convolutional Neural Network (CNN)-based models. Large kernel depthwise convolution has emerged as a promising technique, showcasing capabilities akin to hierarchical transformers and facilitating an expansive effective receptive field (ERF) vital for dense predictions. Despite this, existing core operators, ranging from global-local attention to large kernel convolution, exhibit inherent trade-offs and limitations (e.g., global-local range trade-off, aggregating attentional features). We hypothesize that deformable convolution can be an exploratory alternative to combine all advantages from the previous operators, providing long-range dependency, adaptive spatial aggregation and computational efficiency as a foundation backbone. In this work, we introduce 3D DeformUX-Net, a pioneering volumetric CNN model that adeptly navigates the shortcomings traditionally associated with ViTs and large kernel convolution. Specifically, we revisit volumetric deformable convolution in depth-wise setting to adapt long-range dependency with computational efficiency. Inspired by the concepts of structural re-parameterization for convolution kernel weights, we further generate the deformable tri-planar offsets by adapting a parallel branch (starting from $1\times1\times1$ convolution), providing adaptive spatial aggregation across all channels. Our empirical evaluations reveal that the 3D DeformUX-Net consistently outperforms existing state-of-the-art ViTs and large kernel convolution models across four challenging public datasets, spanning various scales from organs (KiTS: 0.680 to 0.720, MSD Pancreas: 0.676 to 0.717, AMOS: 0.871 to 0.902) to vessels (e.g., MSD hepatic vessels: 0.635 to 0.671) in mean Dice.
</details>
<details>
<summary>摘要</summary>
三维ViT的应用在医学图像分割领域已经取得了非常出色的进步，一些超越了增强型神经网络（CNN）模型的发展。大核心深度卷积技术已经出现了可能，与层次转换器类似，并且提供了宽泛的有效接受场（ERF），这些都是为紧密预测而必要的。然而，现有的核心运算符，从全球local注意力到大核心卷积，都存在着内在的负担和限制（例如全球local范围负担）。我们 hypothesize  dass deformable convolution可以是一种探索性的代替方案，结合所有的优点，提供长茨征dependency、适应空间聚合和计算效率作为基础核心。在这种工作中，我们引入了3D DeformUX-Net，一种在深度缩放设置下的三维弹性 convolution 模型，能够灵活地导航传统上与ViTs和大核心卷积相关的缺陷。具体来说，我们在深度缩放设置下对卷积核心进行了修改，以适应长茨征dependency，并且通过缩放后的权重映射来实现适应空间聚合。我们的实验证明了3D DeformUX-Net在四个公共数据集上（包括 KiTS：0.680-0.720、MSD Pancreas：0.676-0.717、AMOS：0.871-0.902）表现出了与现有的状态开头的 ViTs 和大核心卷积模型相对的稳定性和精度。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/30/cs.CV_2023_09_30/" data-id="clp869txz00kjk588gd29a6if" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_09_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/30/cs.AI_2023_09_30/" class="article-date">
  <time datetime="2023-09-30T12:00:00.000Z" itemprop="datePublished">2023-09-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/30/cs.AI_2023_09_30/">cs.AI - 2023-09-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Reinforcement-learning-adaptive-fuzzy-controller-for-lighting-systems-application-to-aircraft-cabin"><a href="#Reinforcement-learning-adaptive-fuzzy-controller-for-lighting-systems-application-to-aircraft-cabin" class="headerlink" title="Reinforcement learning adaptive fuzzy controller for lighting systems: application to aircraft cabin"></a>Reinforcement learning adaptive fuzzy controller for lighting systems: application to aircraft cabin</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00525">http://arxiv.org/abs/2310.00525</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kritika Vashishtha, Anas Saad, Reza Faieghi, Fengfeng Xi</li>
<li>for: 这篇论文旨在开发智能照明算法，以适应用户偏好。</li>
<li>methods: 这篇论文使用了混杂逻辑和强化学习来开发一个可适应用户偏好的照明算法。具体来说，我们使用了领域知识来建立一个基线混杂推理系统（FIS），该系统根据环境条件（例如日均闪光指数）和用户信息（如年龄、活动和生物钟）生成照明设置建议。用户可以通过反馈机制来与算法进行交互，并将自己的偏好反馈给Q学习代理人，以调整FIS参数。</li>
<li>results: 我们在飞机客舱模拟室进行了广泛的用户研究，以评估算法的效果和学习行为。结果表明，开发的算法具有适应用户偏好的能力，并能成功地适应各种环境条件和用户特点。这表明该算法在智能照明领域具有广泛的应用前景。<details>
<summary>Abstract</summary>
The lighting requirements are subjective and one light setting cannot work for all. However, there is little work on developing smart lighting algorithms that can adapt to user preferences. To address this gap, this paper uses fuzzy logic and reinforcement learning to develop an adaptive lighting algorithm. In particular, we develop a baseline fuzzy inference system (FIS) using the domain knowledge. We use the existing literature to create a FIS that generates lighting setting recommendations based on environmental conditions i.e. daily glare index, and user information including age, activity, and chronotype. Through a feedback mechanism, the user interacts with the algorithm, correcting the algorithm output to their preferences. We interpret these corrections as rewards to a Q-learning agent, which tunes the FIS parameters online to match the user preferences. We implement the algorithm in an aircraft cabin mockup and conduct an extensive user study to evaluate the effectiveness of the algorithm and understand its learning behavior. Our implementation results demonstrate that the developed algorithm possesses the capability to learn user preferences while successfully adapting to a wide range of environmental conditions and user characteristics. and can deal with a diverse spectrum of environmental conditions and user characteristics. This underscores its viability as a potent solution for intelligent light management, featuring advanced learning capabilities.
</details>
<details>
<summary>摘要</summary>
“照明需求是主观的，一个照明设定无法适用于所有人。然而，有很少有关于发展智能照明算法的研究，以适应用户喜好。为了填补这个空白，本研究使用混淆逻辑和强化学习开发了一个适应式照明算法。具体来说，我们开发了一个基本的混淆推理系统（FIS），使用领域知识来生成照明设定建议 based on 环境条件（日常闪光指数）和用户信息（年龄、活动和生物体质）。通过反馈机制，用户与算法进行互动，对算法输出进行更正，以对用户喜好进行调整。我们将这些更正视为对Q学习者的奖励，以调整FIS参数在线匹配用户喜好。我们实现了这个算法在飞机客舱模拟中，并进行了广泛的用户研究，以评估算法的效能和学习行为。我们的实现结果显示，开发的算法具有适应用户喜好的能力，并成功地适应了广泛的环境条件和用户特点。这说明了它的可行性，作为智能照管的强大解决方案。”
</details></li>
</ul>
<hr>
<h2 id="Learning-Informative-Latent-Representation-for-Quantum-State-Tomography"><a href="#Learning-Informative-Latent-Representation-for-Quantum-State-Tomography" class="headerlink" title="Learning Informative Latent Representation for Quantum State Tomography"></a>Learning Informative Latent Representation for Quantum State Tomography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00518">http://arxiv.org/abs/2310.00518</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hailan Ma, Zhenhong Sun, Daoyi Dong, Dong Gong</li>
<li>for: 量子状态探测 (QST) 是一种重建完整量子系统（通过数学模型描述为密度矩阵）的过程，通过一系列不同的测量来获得量子状态的完整信息。</li>
<li>methods: 我们提议一种基于 transformer 架构的自动编码器结构，这种结构可以在不具备准确测量数据的情况下，通过提取高维度的干扰 latent representation (ILR) 来重建量子状态。</li>
<li>results: 我们通过对预处理的 encoder 进行训练，使其能够在不具备准确测量数据的情况下，对高维度 ILR 进行重建，并通过 decoder 来预测量子状态。我们的方法在实验中得到了惊人的效果，能够在不具备准确测量数据的情况下重建量子状态。<details>
<summary>Abstract</summary>
Quantum state tomography (QST) is the process of reconstructing the complete state of a quantum system (mathematically described as a density matrix) through a series of different measurements. These measurements are performed on a number of identical copies of the quantum system, with outcomes gathered as frequencies. QST aims to recover the density matrix and the corresponding properties of the quantum state from the measured frequencies. Although an informationally complete set of measurements can specify quantum state accurately in an ideal scenario with a large number of identical copies, both measurements and identical copies are restricted and imperfect in practical scenarios, making QST highly ill-posed. The conventional QST methods usually assume adequate or accurate measured frequencies or rely on manually designed regularizers to handle the ill-posed reconstruction problem, suffering from limited applications in realistic scenarios. Recent advances in deep neural networks (DNNs) led to the emergence of deep learning (DL) in QST. However, existing DL-based QST approaches often employ generic DNN models that are not optimized for imperfect conditions of QST. In this paper, we propose a transformer-based autoencoder architecture tailored for QST with imperfect measurement data. Our method leverages a transformer-based encoder to extract an informative latent representation (ILR) from imperfect measurement data and employs a decoder to predict the quantum states based on the ILR. We anticipate that the high-dimensional ILR will capture more comprehensive information about quantum states. To achieve this, we conduct pre-training of the encoder using a pretext task that involves reconstructing high-quality frequencies from measured frequencies. Extensive simulations and experiments demonstrate the remarkable ability of the ILR in dealing with imperfect measurement data in QST.
</details>
<details>
<summary>摘要</summary>
量子状态拟合（QST）是将量子系统的完整状态（数学描述为密度矩阵）重建的过程，通过一系列不同的测量获得结果。这些测量在多个相同的量子系统上进行，并记录结果为频率。QST的目标是从测量结果中恢复密度矩阵和相应的量子状态属性。然而，在实际情况下，测量和量子系统 копи本都是受限和不完美的，使QST变得高度不定义。传统的QST方法通常假设了充分或准确的测量结果，或者依靠手动设计正则化器来处理不定义重建问题，受到限制的应用场景。现代深度神经网络（DNN）的出现导致深度学习（DL）在QST中出现。然而，现有的DL基于QST方法 oftemploys generic DNN模型，不是为不完美的QST条件优化。在本文中，我们提出一种基于变换器的自动编码器架构，适用于QST中的不完美测量数据。我们的方法利用变换器基本编码器提取不完美测量数据中的有用信息（ILR），并使用解码器预测量子状态基于ILR。我们预计ILR的高维度将捕捉更广泛的量子状态信息。为了实现这一点，我们在encoder中进行预训练，使用一个预TEXT任务，该任务涉及从测量数据中重建高质量的频率。我们的实验和 simulations表明，ILR具有在QST中处理不完美测量数据的出色能力。
</details></li>
</ul>
<hr>
<h2 id="A-Brief-History-of-Prompt-Leveraging-Language-Models"><a href="#A-Brief-History-of-Prompt-Leveraging-Language-Models" class="headerlink" title="A Brief History of Prompt: Leveraging Language Models"></a>A Brief History of Prompt: Leveraging Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04438">http://arxiv.org/abs/2310.04438</a></li>
<li>repo_url: None</li>
<li>paper_authors: Golam Md Muktadir</li>
<li>for: 这篇论文探讨了自然语言处理领域内部件工程的发展历程。</li>
<li>methods: 论文考察了从早期语言模型和信息检索系统开始，随着年月，Prompt工程发展的关键进展。自2015年的注意机制引入以来，语言理解得到了革命性的改进，包括可控性和上下文意识。随后，基于强化学习技术的进展进一步提高了Prompt工程，解决了暴露偏见和生成文本中的偏见。论文还讨论了2018和2019年的重要贡献，包括细化策略、控制码和模板基本生成。</li>
<li>results: 论文详细介绍了2020和2021年的上下文提醒和转移学习的获得，以及2022和2023年的前期无监督训练和新奖励形成的出现。全文引用了具体的研究例子，以示不同发展对Prompt工程的影响。<details>
<summary>Abstract</summary>
This paper presents a comprehensive exploration of the evolution of prompt engineering and generation in the field of natural language processing (NLP). Starting from the early language models and information retrieval systems, we trace the key developments that have shaped prompt engineering over the years. The introduction of attention mechanisms in 2015 revolutionized language understanding, leading to advancements in controllability and context-awareness. Subsequent breakthroughs in reinforcement learning techniques further enhanced prompt engineering, addressing issues like exposure bias and biases in generated text. We examine the significant contributions in 2018 and 2019, focusing on fine-tuning strategies, control codes, and template-based generation. The paper also discusses the growing importance of fairness, human-AI collaboration, and low-resource adaptation. In 2020 and 2021, contextual prompting and transfer learning gained prominence, while 2022 and 2023 witnessed the emergence of advanced techniques like unsupervised pre-training and novel reward shaping. Throughout the paper, we reference specific research studies that exemplify the impact of various developments on prompt engineering. The journey of prompt engineering continues, with ethical considerations being paramount for the responsible and inclusive future of AI systems.
</details>
<details>
<summary>摘要</summary>
In 2015, the introduction of attention mechanisms revolutionized language understanding, leading to advancements in controllability and context-awareness. Subsequent breakthroughs in reinforcement learning techniques further enhanced prompt engineering, addressing issues like exposure bias and biases in generated text.In 2018 and 2019, significant contributions included fine-tuning strategies, control codes, and template-based generation. The paper also discusses the growing importance of fairness, human-AI collaboration, and low-resource adaptation.In 2020 and 2021, contextual prompting and transfer learning gained prominence, while 2022 and 2023 witnessed the emergence of advanced techniques like unsupervised pre-training and novel reward shaping. Throughout the paper, we reference specific research studies that exemplify the impact of various developments on prompt engineering.The journey of prompt engineering continues, with ethical considerations being paramount for the responsible and inclusive future of AI systems.
</details></li>
</ul>
<hr>
<h2 id="Unveiling-the-Unborn-Advancing-Fetal-Health-Classification-through-Machine-Learning"><a href="#Unveiling-the-Unborn-Advancing-Fetal-Health-Classification-through-Machine-Learning" class="headerlink" title="Unveiling the Unborn: Advancing Fetal Health Classification through Machine Learning"></a>Unveiling the Unborn: Advancing Fetal Health Classification through Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00505">http://arxiv.org/abs/2310.00505</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sujith K Mandala</li>
<li>for: 这个研究旨在提高胎儿健康评估的精度，以提供更好的胎儿健康评估方法。</li>
<li>methods: 本研究使用了LightGBM分类器，利用了该模型的强大搜寻和数据分析功能，并结合了多个特征，如胎心率、子宫收缩和 maternal blood pressure，以提供全面的评估。</li>
<li>results: 研究获得了98.31%的准确率，表明了机器学习的潜力在胎儿健康评估中。<details>
<summary>Abstract</summary>
Fetal health classification is a critical task in obstetrics, enabling early identification and management of potential health problems. However, it remains challenging due to data complexity and limited labeled samples. This research paper presents a novel machine-learning approach for fetal health classification, leveraging a LightGBM classifier trained on a comprehensive dataset. The proposed model achieves an impressive accuracy of 98.31% on a test set. Our findings demonstrate the potential of machine learning in enhancing fetal health classification, offering a more objective and accurate assessment. Notably, our approach combines various features, such as fetal heart rate, uterine contractions, and maternal blood pressure, to provide a comprehensive evaluation. This methodology holds promise for improving early detection and treatment of fetal health issues, ensuring better outcomes for both mothers and babies. Beyond the high accuracy achieved, the novelty of our approach lies in its comprehensive feature selection and assessment methodology. By incorporating multiple data points, our model offers a more holistic and reliable evaluation compared to traditional methods. This research has significant implications in the field of obstetrics, paving the way for advancements in early detection and intervention of fetal health concerns. Future work involves validating the model on a larger dataset and developing a clinical application. Ultimately, we anticipate that our research will revolutionize the assessment and management of fetal health, contributing to improved healthcare outcomes for expectant mothers and their babies.
</details>
<details>
<summary>摘要</summary>
婴儿健康分类是妇科领域中一项关键任务，可以早期发现和管理潜在的健康问题。然而，由于数据复杂性和有限的标注样本，这项任务仍然具有挑战性。本研究论文提出了一种新的机器学习方法，利用LightGBM分类器在全面数据集上进行训练。我们的实验结果显示，提案的模型在测试集上达到了98.31%的准确率。我们的发现表明机器学习在婴儿健康分类中具有潜在的潜力，可以提供更加 объек的和准确的评估。我们的方法选择了多种特征，如婴儿心跳、uterine contractions和 maternal blood pressure，以提供全面的评估。这种方法背后的创新在于其全面的特征选择和评估方法。通过结合多个数据点，我们的模型提供了更加全面和可靠的评估，与传统方法相比。这项研究对妇科领域有着深远的影响，开创了早期发现和治疗婴儿健康问题的新途径。未来的工作将包括验证模型在更大的数据集上的可靠性和开发临床应用。最终，我们预计这项研究将对婴儿健康评估和管理产生深远的影响，为预期母亲和婴儿带来更好的医疗结果。
</details></li>
</ul>
<hr>
<h2 id="From-Language-Modeling-to-Instruction-Following-Understanding-the-Behavior-Shift-in-LLMs-after-Instruction-Tuning"><a href="#From-Language-Modeling-to-Instruction-Following-Understanding-the-Behavior-Shift-in-LLMs-after-Instruction-Tuning" class="headerlink" title="From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning"></a>From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00492">http://arxiv.org/abs/2310.00492</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuansheng Wu, Wenlin Yao, Jianshu Chen, Xiaoman Pan, Xiaoyang Wang, Ninghao Liu, Dong Yu</li>
<li>for: 研究如何使用微调 instrucion 来改善预训练模型的指令执行能力。</li>
<li>methods: 使用了多种本地和全局解释方法，包括输入输出偏导的 gradient-based 方法和自注意和循环层中的模式和概念解释技术。</li>
<li>results: 研究发现，微调 instrucion 对预训练模型有三个重要影响：1）帮助模型更好地识别用户提示中的指令部分，从而提高响应生成质量和解决“lost-in-the-middle”问题；2）将知识在循环层中与用户任务相关的知识相互协调，保持语言水平的稳定性；3）通过自注意机制，模型更好地认识指令词。这些发现对于理解预训练模型后 instrucion 微调的行为变化做出了贡献，并为未来针对不同应用场景进行预训练模型的解释和优化做出了基础。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have achieved remarkable success, demonstrating powerful instruction-following capabilities across diverse tasks. Instruction fine-tuning is critical in enabling LLMs to align with user intentions and effectively follow instructions. In this work, we investigate how instruction fine-tuning modifies pre-trained models, focusing on two perspectives: instruction recognition and knowledge evolution. To study the behavior shift of LLMs, we employ a suite of local and global explanation methods, including a gradient-based approach for input-output attribution and techniques for interpreting patterns and concepts in self-attention and feed-forward layers. Our findings reveal three significant impacts of instruction fine-tuning: 1) It empowers LLMs to better recognize the instruction parts from user prompts, thereby facilitating high-quality response generation and addressing the ``lost-in-the-middle'' issue observed in pre-trained models; 2) It aligns the knowledge stored in feed-forward layers with user-oriented tasks, exhibiting minimal shifts across linguistic levels. 3) It facilitates the learning of word-word relations with instruction verbs through the self-attention mechanism, particularly in the lower and middle layers, indicating enhanced recognition of instruction words. These insights contribute to a deeper understanding of the behavior shifts in LLMs after instruction fine-tuning and lay the groundwork for future research aimed at interpreting and optimizing LLMs for various applications. We will release our code and data soon.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>它使得 LLMS 更好地识别用户提示中的指令部分，从而促进高质量的响应生成和解决预训练模型中的“lost-in-the-middle”问题;2. 它将在Feedforward层中存储的知识与用户关注的任务相互协调，表现出较小的语言层次变化;3. 它通过自我注意机制来促进指令词的学习，特别是在下层和中层，表明了指令词的更好的识别。这些发现对于解释 LLMS 后 instruction fine-tuning 的行为变化提供了深入的理解，并为未来关于 LLMS 的多种应用程序进行解释和优化提供了基础。我们即将发布我们的代码和数据。</details></li>
</ol>
<hr>
<h2 id="On-Memorization-and-Privacy-risks-of-Sharpness-Aware-Minimization"><a href="#On-Memorization-and-Privacy-risks-of-Sharpness-Aware-Minimization" class="headerlink" title="On Memorization and Privacy risks of Sharpness Aware Minimization"></a>On Memorization and Privacy risks of Sharpness Aware Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00488">http://arxiv.org/abs/2310.00488</a></li>
<li>repo_url: None</li>
<li>paper_authors: Young In Kim, Pratiksha Agrawal, Johannes O. Royset, Rajiv Khanna</li>
<li>for: 这个论文主要是为了解释在训练神经网络时，为什么使用抛物线优化算法可以获得更好的泛化性能。</li>
<li>methods: 该论文使用了一种新的指标来评估抛物线优化算法对不同数据点的表现。它们还对比了使用抛物线优化算法和标准SGD算法的性能。</li>
<li>results: 研究发现，使用抛物线优化算法可以在一些特殊的数据点上提高泛化性能，但同时也可能增加隐私风险。研究还提出了一些缓解这种隐私风险的策略。<details>
<summary>Abstract</summary>
In many recent works, there is an increased focus on designing algorithms that seek flatter optima for neural network loss optimization as there is empirical evidence that it leads to better generalization performance in many datasets. In this work, we dissect these performance gains through the lens of data memorization in overparameterized models. We define a new metric that helps us identify which data points specifically do algorithms seeking flatter optima do better when compared to vanilla SGD. We find that the generalization gains achieved by Sharpness Aware Minimization (SAM) are particularly pronounced for atypical data points, which necessitate memorization. This insight helps us unearth higher privacy risks associated with SAM, which we verify through exhaustive empirical evaluations. Finally, we propose mitigation strategies to achieve a more desirable accuracy vs privacy tradeoff.
</details>
<details>
<summary>摘要</summary>
很多最近的研究都集中在设计可以寻找更平的最优点的算法，因为有证据表明这会提高多种数据集的泛化性能。在这个工作中，我们通过数据记忆的角度分析这些性能提升的原因。我们定义了一个新的指标，可以帮助我们确定特定的数据点，在比较于普通的SGD时，哪些算法更好地完成。我们发现，使用Sharpness Aware Minimization（SAM）时，特别是在不Typical的数据点上，其性能提升非常明显。这一点帮助我们发现高privacy风险，我们通过详细的实验来验证。最后，我们提出了一些缓解措施，以实现更好的准确率和隐私质量的平衡。
</details></li>
</ul>
<hr>
<h2 id="UPAR-A-Kantian-Inspired-Prompting-Framework-for-Enhancing-Large-Language-Model-Capabilities"><a href="#UPAR-A-Kantian-Inspired-Prompting-Framework-for-Enhancing-Large-Language-Model-Capabilities" class="headerlink" title="UPAR: A Kantian-Inspired Prompting Framework for Enhancing Large Language Model Capabilities"></a>UPAR: A Kantian-Inspired Prompting Framework for Enhancing Large Language Model Capabilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01441">http://arxiv.org/abs/2310.01441</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hejia Geng, Boxun Xu, Peng Li</li>
<li>for: The paper aims to improve the inferential capabilities of large language models (LLMs) by proposing a new prompting framework called UPAR, which is inspired by Kant’s a priori philosophy.</li>
<li>methods: The UPAR framework consists of four phases: “Understand”, “Plan”, “Act”, and “Reflect”. It enables the extraction of structured information from complex contexts, prior planning of solutions, execution according to plan, and self-reflection.</li>
<li>results: The paper demonstrates the effectiveness of the UPAR framework by testing it on two tasks: a challenging subset of GSM8K and the causal judgment task. The results show that the accuracy of LLM inference is significantly improved, with an increase from 22.92% to 58.33% in the GSM8K task and from 67.91% to 75.40% in the causal judgment task.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated impressive inferential capabilities, with numerous research endeavors devoted to enhancing this capacity through prompting. Despite these efforts, a unified epistemological foundation is still conspicuously absent. Drawing inspiration from Kant's a priori philosophy, we propose the UPAR prompting framework, designed to emulate the structure of human cognition within LLMs. The UPAR framework is delineated into four phases: "Understand", "Plan", "Act", and "Reflect", enabling the extraction of structured information from complex contexts, prior planning of solutions, execution according to plan, and self-reflection. This structure significantly augments the explainability and accuracy of LLM inference, producing a human-understandable and inspectable inferential trajectory. Furthermore, our work offers an epistemological foundation for existing prompting techniques, allowing for a possible systematic integration of these methods. With GPT-4, our approach elevates the accuracy from COT baseline of 22.92% to 58.33% in a challenging subset of GSM8K, and from 67.91% to 75.40% in the causal judgment task.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经表现出了吸引人的推理能力，有很多研究努力于提高这种能力通过提示。尽管如此，一个统一的 épistémologique基础仍然缺失。我们 Drawing inspiration from Kant's a priori philosophy, we propose the UPAR prompting framework, designed to emulate the structure of human cognition within LLMs. The UPAR framework is delineated into four phases: "Understand", "Plan", "Act", and "Reflect", enabling the extraction of structured information from complex contexts, prior planning of solutions, execution according to plan, and self-reflection. This structure significantly augments the explainability and accuracy of LLM inference, producing a human-understandable and inspectable inferential trajectory. Furthermore, our work offers an épistémological foundation for existing prompting techniques, allowing for a possible systematic integration of these methods. With GPT-4, our approach elevates the accuracy from COT baseline of 22.92% to 58.33% in a challenging subset of GSM8K, and from 67.91% to 75.40% in the causal judgment task.
</details></li>
</ul>
<hr>
<h2 id="Encouraging-Inferable-Behavior-for-Autonomy-Repeated-Bimatrix-Stackelberg-Games-with-Observations"><a href="#Encouraging-Inferable-Behavior-for-Autonomy-Repeated-Bimatrix-Stackelberg-Games-with-Observations" class="headerlink" title="Encouraging Inferable Behavior for Autonomy: Repeated Bimatrix Stackelberg Games with Observations"></a>Encouraging Inferable Behavior for Autonomy: Repeated Bimatrix Stackelberg Games with Observations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00468">http://arxiv.org/abs/2310.00468</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mustafa O. Karabag, Sophia Smith, David Fridovich-Keil, Ufuk Topcu</li>
<li>For: 这篇论文关注了自主agent在与其他非竞争决策机器人交互时，如何表达其意图和策略。* Methods: 作者使用了一个重复的二元Stackelberg游戏模型，以模拟自主车与其他机器人之间的交互。在这个模型中，领导者采用固定的混合策略，而追随者则根据领导者的前一步动作进行反应。* Results: 作者证明了领导者在有观察情况下可能会受到一定的推断损失，即与领导者的策略完全知情情况下的性能相比。此外，作者还提供了一个游戏，其中需要一定的交互次数来保证推断性。<details>
<summary>Abstract</summary>
When interacting with other non-competitive decision-making agents, it is critical for an autonomous agent to have inferable behavior: Their actions must convey their intention and strategy. For example, an autonomous car's strategy must be inferable by the pedestrians interacting with the car. We model the inferability problem using a repeated bimatrix Stackelberg game with observations where a leader and a follower repeatedly interact. During the interactions, the leader uses a fixed, potentially mixed strategy. The follower, on the other hand, does not know the leader's strategy and dynamically reacts based on observations that are the leader's previous actions. In the setting with observations, the leader may suffer from an inferability loss, i.e., the performance compared to the setting where the follower has perfect information of the leader's strategy. We show that the inferability loss is upper-bounded by a function of the number of interactions and the stochasticity level of the leader's strategy, encouraging the use of inferable strategies with lower stochasticity levels. As a converse result, we also provide a game where the required number of interactions is lower bounded by a function of the desired inferability loss.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Consistent-Aggregation-of-Objectives-with-Diverse-Time-Preferences-Requires-Non-Markovian-Rewards"><a href="#Consistent-Aggregation-of-Objectives-with-Diverse-Time-Preferences-Requires-Non-Markovian-Rewards" class="headerlink" title="Consistent Aggregation of Objectives with Diverse Time Preferences Requires Non-Markovian Rewards"></a>Consistent Aggregation of Objectives with Diverse Time Preferences Requires Non-Markovian Rewards</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00435">http://arxiv.org/abs/2310.00435</a></li>
<li>repo_url: None</li>
<li>paper_authors: Silviu Pitis</li>
<li>for: 这篇论文关注了人工智能系统在服务多个多元目标和利益相关者时，目标组合是否能够有效地进行。</li>
<li>methods: 本论文采用了normative方法，从一组直观上有吸引力的axioms出发，证明了Markov均摊重函数不能够具有不同目标的时间偏好（折扣因子）。</li>
<li>results: 研究发现了一种实用的非Markov均摊集合方案，可以超越这种不可能性，仅需要每个目标添加一个额外参数。这些成果对sequential、多元目标代理和时间选择具有新的理解和实践意义，有助于设计服务多代理的人工智能系统。<details>
<summary>Abstract</summary>
As the capabilities of artificial agents improve, they are being increasingly deployed to service multiple diverse objectives and stakeholders. However, the composition of these objectives is often performed ad hoc, with no clear justification. This paper takes a normative approach to multi-objective agency: from a set of intuitively appealing axioms, it is shown that Markovian aggregation of Markovian reward functions is not possible when the time preference (discount factor) for each objective may vary. It follows that optimal multi-objective agents must admit rewards that are non-Markovian with respect to the individual objectives. To this end, a practical non-Markovian aggregation scheme is proposed, which overcomes the impossibility with only one additional parameter for each objective. This work offers new insights into sequential, multi-objective agency and intertemporal choice, and has practical implications for the design of AI systems deployed to serve multiple generations of principals with varying time preference.
</details>
<details>
<summary>摘要</summary>
随着人工智能技术的进步，人工智能系统被越来越多地用于服务多个多样化的目标和利益相关者。然而，这些目标的组合经常是随意的，没有明确的证明。这篇论文采取了normative方法，从一组直观上有吸引力的axioms开始，Proof that Markovian aggregation of Markovian reward functions is not possible when the time preference (discount factor) for each objective may vary。这意味着优质多目标agent必须承认不同目标之间的非Markovian奖励。为此，一种实用的非Markovian汇集方案被提议，可以在每个目标上增加一个额外参数来解决这个不可能性。这项工作提供了新的思路，对sequential, multi-objective agency和时间偏好选择进行了深入的研究，并对AI系统服务多个代理人的设计产生了实质性的影响。
</details></li>
</ul>
<hr>
<h2 id="Active-Perceptive-Motion-Generation-for-Mobile-Manipulation"><a href="#Active-Perceptive-Motion-Generation-for-Mobile-Manipulation" class="headerlink" title="Active-Perceptive Motion Generation for Mobile Manipulation"></a>Active-Perceptive Motion Generation for Mobile Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00433">http://arxiv.org/abs/2310.00433</a></li>
<li>repo_url: None</li>
<li>paper_authors: Snehal Jauhri, Sophie Lueth, Georgia Chalvatzaki</li>
<li>for: 这个论文的目的是为移动抓取系统提供有用的视觉信息，以便在不知道的环境中完成抓取任务。</li>
<li>methods: 该论文使用了活动感知管道，通过在远程观察器中采样路径并计算路径智能来提高抓取任务的成功率。</li>
<li>results: 实验表明，该方法可以在 simulate 的Scene中提高移动抓取系统的抓取率，并且可以在实际场景中转移。此外，该方法还可以对抓取任务进行优化，以提高抓取率和效率。<details>
<summary>Abstract</summary>
Mobile Manipulation (MoMa) systems incorporate the benefits of mobility and dexterity, thanks to the enlarged space in which they can move and interact with their environment. MoMa robots can also continuously perceive their environment when equipped with onboard sensors, e.g., an embodied camera. However, extracting task-relevant visual information in unstructured and cluttered environments such as households remains a challenge. In this work, we introduce an active perception pipeline for mobile manipulators to generate motions that are informative toward manipulation tasks such as grasping, in initially unknown, cluttered scenes. Our proposed approach ActPerMoMa generates robot trajectories in a receding horizon fashion, sampling trajectories and computing path-wise utilities that trade-off reconstructing the unknown scene by maximizing the visual information gain and the taskoriented objective, e.g., grasp success by maximizing grasp reachability efficiently. We demonstrate the efficacy of our method in simulated experiments with a dual-arm TIAGo++ MoMa robot performing mobile grasping in cluttered scenes and when its path is obstructed by external obstacles. We empirically analyze the contribution of various utilities and hyperparameters, and compare against representative baselines both with and without active perception objectives. Finally, we demonstrate the transfer of our mobile grasping strategy to the real world, showing a promising direction for active-perceptive MoMa.
</details>
<details>
<summary>摘要</summary>
Mobile Manipulation（MoMa）系统具有移动和聪明的优点，感谢装置了 борьбу的感知器，如搭载的相机。但是，在无组织的和混乱的环境中，如家居，提取任务相关的视觉信息仍然是一个挑战。在这个工作中，我们介绍了一个活耀感知管道，用于生产移动掌控器的机动轨迹，以实现实用的掌控任务，如抓取。我们的提案方法ActPerMoMa使用推移视野的方式生成机器人的轨迹，该轨迹将路径实用性和任务目标优先级排序。我们在实验中使用了双臂TIAGo++ MoMa机器人在混乱场景中进行移动抓取，并评估了不同的优点和参数的贡献。我们还与不具有活耀感知目标的基eline进行比较。最后，我们展示了我们的移动抓取策略在实际世界中的实现，显示了活耀感知MoMa的可能性。
</details></li>
</ul>
<hr>
<h2 id="Making-Friends-in-the-Dark-Ad-Hoc-Teamwork-Under-Partial-Observability"><a href="#Making-Friends-in-the-Dark-Ad-Hoc-Teamwork-Under-Partial-Observability" class="headerlink" title="Making Friends in the Dark: Ad Hoc Teamwork Under Partial Observability"></a>Making Friends in the Dark: Ad Hoc Teamwork Under Partial Observability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01439">http://arxiv.org/abs/2310.01439</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jmribeiro/adhoc-teamwork-under-partial-observability">https://github.com/jmribeiro/adhoc-teamwork-under-partial-observability</a></li>
<li>paper_authors: João G. Ribeiroa, Cassandro Martinhoa, Alberto Sardinhaa, Francisco S. Melo</li>
<li>for: 本研究旨在提供一种基于优先知识和偏见环境的适应性团队协作方法，以便在偏见环境下协助未知团队成员解决未知任务。</li>
<li>methods: 本研究提出了三个假设，即环境状态总是半可见，团队成员的行为总是不可见，以及协作代理不可以直接获得奖励信号。基于这些假设，我们提出了一种基于优先知识和偏见环境的适应性协作方法。</li>
<li>results: 我们在70个POMDP问题中进行了11个领域的实验，结果表明，我们的方法不仅能够帮助未知团队成员解决未知任务，而且可以在更加复杂的问题上进行稳定的性能。<details>
<summary>Abstract</summary>
This paper introduces a formal definition of the setting of ad hoc teamwork under partial observability and proposes a first-principled model-based approach which relies only on prior knowledge and partial observations of the environment in order to perform ad hoc teamwork. We make three distinct assumptions that set it apart previous works, namely: i) the state of the environment is always partially observable, ii) the actions of the teammates are always unavailable to the ad hoc agent and iii) the ad hoc agent has no access to a reward signal which could be used to learn the task from scratch. Our results in 70 POMDPs from 11 domains show that our approach is not only effective in assisting unknown teammates in solving unknown tasks but is also robust in scaling to more challenging problems.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了适应性团队工作的正式定义，并提出了基于模型的首则方法，该方法仅基于团队成员的先前知识和环境的部分观察来实现适应性团队工作。我们做出了三个特点分开的假设，即：i) 环境状态总是部分可见，ii) 团队成员的行动总是不可见给适应代理人和 iii) 适应代理人没有直接学习任务的奖励信号。我们在11个领域中的70个POMDP中的结果表明，我们的方法不仅能够帮助未知团队成员解决未知任务，还能够在更加困难的问题上具有稳定性。
</details></li>
</ul>
<hr>
<h2 id="Building-Flexible-Scalable-and-Machine-Learning-ready-Multimodal-Oncology-Datasets"><a href="#Building-Flexible-Scalable-and-Machine-Learning-ready-Multimodal-Oncology-Datasets" class="headerlink" title="Building Flexible, Scalable, and Machine Learning-ready Multimodal Oncology Datasets"></a>Building Flexible, Scalable, and Machine Learning-ready Multimodal Oncology Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01438">http://arxiv.org/abs/2310.01438</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aakash Tripathi, Asim Waqas, Kavya Venkatesan, Yasin Yilmaz, Ghulam Rasool</li>
<li>for: 本研究旨在提出一种可扩展、可扩展、可靠的metadata框架，以便高效地融合不同来源的医疗数据，包括放射学扫描、 histopathology 图像和分子信息，以及临床数据，以实现精准医学和个性化治疗。</li>
<li>methods: 本研究使用了 Multimodal Integration of Oncology Data System (MINDS)，一个可扩展、可扩展、可靠的metadata框架，可以高效地融合不同来源的医疗数据，并提供了一个界面，以便探索不同数据类型之间的关系，并建立大规模多modal机器学习模型。</li>
<li>results: 本研究通过MINDS来融合多modal数据，并提供了一个patient-centric的框架，以便实现精准医学和个性化治疗。MINDS还可以跟踪细致的数据证明，以确保可重现性和透明度。此外，MINDS的云 Native架构可以处理快速增长的数据，并确保安全、可靠、可扩展和高效的数据处理。<details>
<summary>Abstract</summary>
The advancements in data acquisition, storage, and processing techniques have resulted in the rapid growth of heterogeneous medical data. Integrating radiological scans, histopathology images, and molecular information with clinical data is essential for developing a holistic understanding of the disease and optimizing treatment. The need for integrating data from multiple sources is further pronounced in complex diseases such as cancer for enabling precision medicine and personalized treatments. This work proposes Multimodal Integration of Oncology Data System (MINDS) - a flexible, scalable, and cost-effective metadata framework for efficiently fusing disparate data from public sources such as the Cancer Research Data Commons (CRDC) into an interconnected, patient-centric framework. MINDS offers an interface for exploring relationships across data types and building cohorts for developing large-scale multimodal machine learning models. By harmonizing multimodal data, MINDS aims to potentially empower researchers with greater analytical ability to uncover diagnostic and prognostic insights and enable evidence-based personalized care. MINDS tracks granular end-to-end data provenance, ensuring reproducibility and transparency. The cloud-native architecture of MINDS can handle exponential data growth in a secure, cost-optimized manner while ensuring substantial storage optimization, replication avoidance, and dynamic access capabilities. Auto-scaling, access controls, and other mechanisms guarantee pipelines' scalability and security. MINDS overcomes the limitations of existing biomedical data silos via an interoperable metadata-driven approach that represents a pivotal step toward the future of oncology data integration.
</details>
<details>
<summary>摘要</summary>
随着数据获取、存储和处理技术的进步，医疗数据的多样化快速增长。将验理学扫描图像、 Histopathology 图像和分子信息与临床数据集成是为了建立疾病全面理解和优化治疗提供了基础。在复杂的疾病，如癌症，将多种数据集成是为了实现精准医疗和个性化治疗。本文提出了多Modal Integration of Oncology Data System (MINDS) -一个灵活、可扩展、成本效果的元数据框架，可以有效地将不同来源的数据集成成一个连接的、患者中心的框架。MINDS 提供了浏览不同数据类型之间的关系的接口，并可以建立大规模多模态机器学习模型。通过融合多模态数据，MINDS 目标是使研究人员拥有更多的分析能力，探索诊断和预后探索的新知识，并为个性化医疗提供证据基础。MINDS 跟踪精细的数据来源追溯，确保可重现性和透明度。云native 架构可以处理快速增长的数据，并确保安全、成本优化的方式进行存储优化、复制避免和动态访问。自动扩展、访问控制和其他机制确保管道的可扩展性和安全性。MINDS 超越了现有的医学数据困难，通过一种可交互的元数据驱动的方法，代表了未来医学数据集成的重要一步。
</details></li>
</ul>
<hr>
<h2 id="Refutation-of-Shapley-Values-for-XAI-–-Additional-Evidence"><a href="#Refutation-of-Shapley-Values-for-XAI-–-Additional-Evidence" class="headerlink" title="Refutation of Shapley Values for XAI – Additional Evidence"></a>Refutation of Shapley Values for XAI – Additional Evidence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00416">http://arxiv.org/abs/2310.00416</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuanxiang Huang, Joao Marques-Silva</li>
<li>for: 证明Shapley值不适用于可解释人工智能（XAI）。</li>
<li>methods: 使用 families of classifiers，不是 Boolean 分类器，以及 multiple classes 可以选择。</li>
<li>results: 显示 Shapley values 不适用于 XAI，并且features changed in any minimal $l_0$ distance adversarial examples 不包括无关的特征。<details>
<summary>Abstract</summary>
Recent work demonstrated the inadequacy of Shapley values for explainable artificial intelligence (XAI). Although to disprove a theory a single counterexample suffices, a possible criticism of earlier work is that the focus was solely on Boolean classifiers. To address such possible criticism, this paper demonstrates the inadequacy of Shapley values for families of classifiers where features are not boolean, but also for families of classifiers for which multiple classes can be picked. Furthermore, the paper shows that the features changed in any minimal $l_0$ distance adversarial examples do not include irrelevant features, thus offering further arguments regarding the inadequacy of Shapley values for XAI.
</details>
<details>
<summary>摘要</summary>
最近的工作表明了希普利值不适用于可解释人工智能（XAI）。尽管单个反例 suffices to disprove a theory，可能的批评是之前的工作强调了布尔分类器。为解决这种可能的批评，本文示出希普利值对于不是布尔分类器的家族分类器以及可以选择多个类的家族分类器是无效的。此外，本文还证明了在任何最小$l_0$距离抗击例中改变的特征不包括无关的特征，从而提供了更多有关希普利值不适用于 XAI 的论据。
</details></li>
</ul>
<hr>
<h2 id="Order-Preserving-GFlowNets"><a href="#Order-Preserving-GFlowNets" class="headerlink" title="Order-Preserving GFlowNets"></a>Order-Preserving GFlowNets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00386">http://arxiv.org/abs/2310.00386</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihang Chen, Lukas Mauch</li>
<li>for: 这个论文是用来解决多个目标优化任务中的问题，特别是当目标函数不可知或 computationally expensive 时。</li>
<li>methods: 这个论文提出了 Order-Preserving GFlowNets (OP-GFNs)，一种可以根据提供的（部分）排序来评估候选者的可能性，不需要明确表述优化函数。</li>
<li>results: 这个论文的实验结果显示 OP-GFNs 可以在单一目标最大化任务和多个目标 Pareto 前方估算任务中表现出色，包括人工数据集、分子生成和神经架构搜寻。<details>
<summary>Abstract</summary>
Generative Flow Networks (GFlowNets) have been introduced as a method to sample a diverse set of candidates with probabilities proportional to a given reward. However, GFlowNets can only be used with a predefined scalar reward, which can be either computationally expensive or not directly accessible, in the case of multi-objective optimization (MOO) tasks for example. Moreover, to prioritize identifying high-reward candidates, the conventional practice is to raise the reward to a higher exponent, the optimal choice of which may vary across different environments. To address these issues, we propose Order-Preserving GFlowNets (OP-GFNs), which sample with probabilities in proportion to a learned reward function that is consistent with a provided (partial) order on the candidates, thus eliminating the need for an explicit formulation of the reward function. We theoretically prove that the training process of OP-GFNs gradually sparsifies the learned reward landscape in single-objective maximization tasks. The sparsification concentrates on candidates of a higher hierarchy in the ordering, ensuring exploration at the beginning and exploitation towards the end of the training. We demonstrate OP-GFN's state-of-the-art performance in single-objective maximization (totally ordered) and multi-objective Pareto front approximation (partially ordered) tasks, including synthetic datasets, molecule generation, and neural architecture search.
</details>
<details>
<summary>摘要</summary>
生成流网络（GFlowNets）已经被提出，用于采样一个多样化的候选者，概率与给定的奖励相符。然而，GFlowNets只能在预定的整数奖励上使用，这可能是计算成本高或不直接可访问的，例如多目标优化（MOO）任务中。此外，为便于寻找高奖励候选者，通常是通过提高奖励的幂来进行优化，但选择最佳幂值可能会因不同环境而异。为解决这些问题，我们提议Order-Preserving GFlowNets（OP-GFNs），它采样的概率与提供的（部分）顺序中的候选者相符，因此不需要显式表述奖励函数。我们证明OP-GFNs在单目标最大化任务中的训练过程中逐渐简化学习的奖励地形，并且在开始训练时进行探索，到训练结束时则进行利用。我们在单目标最大化（完全排序）和多目标Pareto前缘接近（部分排序）任务中实现OP-GFNs的状态 arts Performances，包括 sintetic dataset、分子生成和神经网络搜索。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Demonstrations-Controller-for-In-Context-Learning"><a href="#Dynamic-Demonstrations-Controller-for-In-Context-Learning" class="headerlink" title="Dynamic Demonstrations Controller for In-Context Learning"></a>Dynamic Demonstrations Controller for In-Context Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00385">http://arxiv.org/abs/2310.00385</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tjtp/d2controller">https://github.com/tjtp/d2controller</a></li>
<li>paper_authors: Fei Zhao, Taotian Pang, Zhen Wu, Zheng Ma, Shujian Huang, Xinyu Dai</li>
<li>for: 本研究旨在探讨启发式学习（ICL）中语言模型（LLM）的示例数量对性能的影响，并提出一种动态示例控制器（D$^2$Controller）以改进ICL性能。</li>
<li>methods: 本研究使用了一些预先设计的示例，并通过对不同大小的LLM进行测试，来检验D$^2$Controller的效果。</li>
<li>results: 实验结果表明，D$^2$Controller可以在八种不同的LLM上提高ICL性能平均5.4%，并且可以与之前的ICL模型进行比较。<details>
<summary>Abstract</summary>
In-Context Learning (ICL) is a new paradigm for natural language processing (NLP), where a large language model (LLM) observes a small number of demonstrations and a test instance as its input, and directly makes predictions without updating model parameters. Previous studies have revealed that ICL is sensitive to the selection and the ordering of demonstrations. However, there are few studies regarding the impact of the demonstration number on the ICL performance within a limited input length of LLM, because it is commonly believed that the number of demonstrations is positively correlated with model performance. In this paper, we found this conclusion does not always hold true. Through pilot experiments, we discover that increasing the number of demonstrations does not necessarily lead to improved performance. Building upon this insight, we propose a Dynamic Demonstrations Controller (D$^2$Controller), which can improve the ICL performance by adjusting the number of demonstrations dynamically. The experimental results show that D$^2$Controller yields a 5.4% relative improvement on eight different sizes of LLMs across ten datasets. Moreover, we also extend our method to previous ICL models and achieve competitive results.
</details>
<details>
<summary>摘要</summary>
新的一代自然语言处理（NLP） paradigma——卷积语言模型（LLM）在观察一小数量示例和测试实例后，直接进行预测而不需要更新模型参数。先前的研究表明，ICL对示例选择和排序具有敏感性。然而，关于 Limited LLM 输入长度内示例数量对 ICL 性能的影响，有少量研究，因为通常认为示例数量与模型性能是正相关的。在这篇论文中，我们发现这种结论并不总是成立。经验测试表明，增加示例数量并不一定能提高性能。基于这一点，我们提出了动态示例控制器（D$^2$Controller），可以在运行时动态调整示例数量，以提高 ICL 性能。实验结果表明，D$^2$Controller 在八种不同大小的 LLM 上对十个数据集进行了5.4%的相对提高。此外，我们还扩展了我们的方法到之前的 ICL 模型，并实现了竞争性的结果。
</details></li>
</ul>
<hr>
<h2 id="Measuring-Value-Understanding-in-Language-Models-through-Discriminator-Critique-Gap"><a href="#Measuring-Value-Understanding-in-Language-Models-through-Discriminator-Critique-Gap" class="headerlink" title="Measuring Value Understanding in Language Models through Discriminator-Critique Gap"></a>Measuring Value Understanding in Language Models through Discriminator-Critique Gap</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00378">http://arxiv.org/abs/2310.00378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaowei Zhang, Fengshuo Bai, Jun Gao, Yaodong Yang</li>
<li>for: 本研究旨在评估大语言模型（LLMs）对人类价值的理解水平，并提出了一个Value Understanding Measurement（VUM）框架来评估LLMs的价值理解。</li>
<li>methods: 本研究使用了 Schwartz Value Survey 来Specify evaluation values，并开发了一个 thousand-level dialogue dataset with GPT-4。对于 LLMs 的评估，分析了其输出与基eline答案之间的差异，以及 LLM 对价值认知的理由与 GPT-4 的注释之间的差异。</li>
<li>results: 研究发现，随着 LLMS 的缩放，”know what” 方面的差异增加，但 “know why” 方面的差异很少变化，这可能指示 LLMS 可能会提供合理的解释，但并不真正理解其内在的价值。这些结果可能表明 LLMS 可能存在风险。<details>
<summary>Abstract</summary>
Recent advancements in Large Language Models (LLMs) have heightened concerns about their potential misalignment with human values. However, evaluating their grasp of these values is complex due to their intricate and adaptable nature. We argue that truly understanding values in LLMs requires considering both "know what" and "know why". To this end, we present the Value Understanding Measurement (VUM) framework that quantitatively assesses both "know what" and "know why" by measuring the discriminator-critique gap related to human values. Using the Schwartz Value Survey, we specify our evaluation values and develop a thousand-level dialogue dataset with GPT-4. Our assessment looks at both the value alignment of LLM's outputs compared to baseline answers and how LLM responses align with reasons for value recognition versus GPT-4's annotations. We evaluate five representative LLMs and provide strong evidence that the scaling law significantly impacts "know what" but not much on "know why", which has consistently maintained a high level. This may further suggest that LLMs might craft plausible explanations based on the provided context without truly understanding their inherent value, indicating potential risks.
</details>
<details>
<summary>摘要</summary>
最近的大语言模型（LLM）技术进步引发了对其可能的偏差问题的担忧。然而，评估这些模型对人类价值的理解复杂，因为它们的结构和适应性很强。我们认为，很难真正理解 LLM 中的价值，不是只是知道“what”，而是知道“why”。为此，我们提出了价值理解度量框架（VUM），用于量化评估 LLM 对人类价值的理解。我们使用了 Schwartz 价值问卷，并开发了一个 thousand-level 对话集，并用 GPT-4 进行评估。我们的评估包括 LLM 输出与基线答案之间的价值Alignment，以及 LLM 对价值认知的原因与 GPT-4 的注释之间的对应性。我们评估了五种代表性 LLM，并发现了扩展法律对“what”有显著影响，但对“why”没有太多影响，这可能表示 LLM 可能会基于提供的 контекст生成可能的解释，而不是真正理解其内在的价值，这可能会带来风险。
</details></li>
</ul>
<hr>
<h2 id="AI-Dentify-Deep-learning-for-proximal-caries-detection-on-bitewing-x-ray-–-HUNT4-Oral-Health-Study"><a href="#AI-Dentify-Deep-learning-for-proximal-caries-detection-on-bitewing-x-ray-–-HUNT4-Oral-Health-Study" class="headerlink" title="AI-Dentify: Deep learning for proximal caries detection on bitewing x-ray – HUNT4 Oral Health Study"></a>AI-Dentify: Deep learning for proximal caries detection on bitewing x-ray – HUNT4 Oral Health Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00354">http://arxiv.org/abs/2310.00354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Javier Pérez de Frutos, Ragnhild Holden Helland, Shreya Desai, Line Cathrine Nymoen, Thomas Langø, Theodor Remman, Abhijit Sen</li>
<li>for: 该研究用于检测牙科疾病（牙肉病）的诊断，以提高牙医诊断的准确率和效率。</li>
<li>methods: 该研究使用了三种深度学习模型：RetinaNet（ResNet50）、YOLOv5（M size）和EfficientDet（D0和D1 size），并使用了13,887张牙科照片的标注数据进行训练。</li>
<li>results: 训练后的模型显示了与牙医专业人员的准确率和F1分数的提高，并且false negative率的减少。其中YOLOv5模型表现最佳，其中的mean average precision为0.647，mean F1分数为0.548，false negative率为0.149。<details>
<summary>Abstract</summary>
Background: Dental caries diagnosis requires the manual inspection of diagnostic bitewing images of the patient, followed by a visual inspection and probing of the identified dental pieces with potential lesions. Yet the use of artificial intelligence, and in particular deep-learning, has the potential to aid in the diagnosis by providing a quick and informative analysis of the bitewing images.   Methods: A dataset of 13,887 bitewings from the HUNT4 Oral Health Study were annotated individually by six different experts, and used to train three different object detection deep-learning architectures: RetinaNet (ResNet50), YOLOv5 (M size), and EfficientDet (D0 and D1 sizes). A consensus dataset of 197 images, annotated jointly by the same six dentist, was used for evaluation. A five-fold cross validation scheme was used to evaluate the performance of the AI models.   Results: the trained models show an increase in average precision and F1-score, and decrease of false negative rate, with respect to the dental clinicians. Out of the three architectures studied, YOLOv5 shows the largest improvement, reporting 0.647 mean average precision, 0.548 mean F1-score, and 0.149 mean false negative rate. Whereas the best annotators on each of these metrics reported 0.299, 0.495, and 0.164 respectively.   Conclusion: Deep-learning models have shown the potential to assist dental professionals in the diagnosis of caries. Yet, the task remains challenging due to the artifacts natural to the bitewings.
</details>
<details>
<summary>摘要</summary>
背景：牙科疾病诊断需要 manually inspect 牙齿图像，然后进行视觉检查和可能的疾病部位的探针。然而，人工智能（AI）和深度学习有助于诊断，可以提供快速和有用的牙齿图像分析。方法：使用了13,887张牙齿图像从HUNT4口腔卫生研究计划，由6名专家分别注释，并用于训练3种深度学习建筑：RetinaNet（ResNet50）、YOLOv5（M大小）和EfficientDet（D0和D1大小）。用于评估AI模型性能的共识数据集包含197张图像，由同6名牙科专家共同注释。采用五fold十字验证法来评估AI模型性能。结果：训练的模型在比较牙科专家时显示了增加的平均精度和F1分数，以及降低的假阳性率。其中YOLOv5显示最大改善，reporting平均精度0.647、平均F1分数0.548和平均假阳性率0.149。而最佳注释员在每个维度上的最佳值分别为0.299、0.495和0.164。结论：深度学习模型在诊断疾病方面表现出了潜力，但牙齿图像中的自然遗传物品使得任务变得更加挑战。
</details></li>
</ul>
<hr>
<h2 id="Neuroadaptation-in-Physical-Human-Robot-Collaboration"><a href="#Neuroadaptation-in-Physical-Human-Robot-Collaboration" class="headerlink" title="Neuroadaptation in Physical Human-Robot Collaboration"></a>Neuroadaptation in Physical Human-Robot Collaboration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00351">http://arxiv.org/abs/2310.00351</a></li>
<li>repo_url: None</li>
<li>paper_authors: Avinash Singh, Dikai Liu, Chin-Teng Lin</li>
<li>for: 这个论文的目的是解决人机合作系统中机器人的行为和操作方式需要根据人工作者的性能和意图以及不同人工作者的碰撞避免和机器人操作的单点性进行调整。</li>
<li>methods: 作者提出了一种基于带有反馈学习的封闭征文化框架，通过利用认知冲突信息来适应机器人策略，并与开 Loop设置进行比较。</li>
<li>results: 实验结果表明，封闭征文化框架在人机合作中降低了认知冲突水平，从而提高了人机合作的平滑度和直观性。这些结果表明了未来人机合作控制系统中使用电энцеfalogram（EEG）信号的可能性。<details>
<summary>Abstract</summary>
Robots for physical Human-Robot Collaboration (pHRC) systems need to change their behavior and how they operate in consideration of several factors, such as the performance and intention of a human co-worker and the capabilities of different human-co-workers in collision avoidance and singularity of the robot operation. As the system's admittance becomes variable throughout the workspace, a potential solution is to tune the interaction forces and control the parameters based on the operator's requirements. To overcome this issue, we have demonstrated a novel closed-loop-neuroadaptive framework for pHRC. We have applied cognitive conflict information in a closed-loop manner, with the help of reinforcement learning, to adapt to robot strategy and compare this with open-loop settings. The experiment results show that the closed-loop-based neuroadaptive framework successfully reduces the level of cognitive conflict during pHRC, consequently increasing the smoothness and intuitiveness of human-robot collaboration. These results suggest the feasibility of a neuroadaptive approach for future pHRC control systems through electroencephalogram (EEG) signals.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Visual-Political-Communication-in-a-Polarized-Society-A-Longitudinal-Study-of-Brazilian-Presidential-Elections-on-Instagram"><a href="#Visual-Political-Communication-in-a-Polarized-Society-A-Longitudinal-Study-of-Brazilian-Presidential-Elections-on-Instagram" class="headerlink" title="Visual Political Communication in a Polarized Society: A Longitudinal Study of Brazilian Presidential Elections on Instagram"></a>Visual Political Communication in a Polarized Society: A Longitudinal Study of Brazilian Presidential Elections on Instagram</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00349">http://arxiv.org/abs/2310.00349</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mathias-Felipe de-Lima-Santos, Isabella Gonçalves, Marcos G. Quiles, Lucia Mesquita, Wilson Ceron<br>for: This study aims to investigate the visual communication strategies employed by Brazilian presidential candidates on Instagram in the 2018 and 2022 national elections.methods: The study employs a combination of computational methods and qualitative approach to analyze a dataset of 11,263 Instagram posts by 19 Brazilian presidential candidates.results: The study finds consistent patterns of celebratory and positively toned images, a strong sense of personalization, and unique contextual nuances specific to the Brazilian political landscape. The study also uncovers the prevalence of screenshots from news websites and other social media platforms, as well as text-edited images with portrayals.<details>
<summary>Abstract</summary>
In today's digital age, images have emerged as powerful tools for politicians to engage with their voters on social media platforms. Visual content possesses a unique emotional appeal that often leads to increased user engagement. However, research on visual communication remains relatively limited, particularly in the Global South. This study aims to bridge this gap by employing a combination of computational methods and qualitative approach to investigate the visual communication strategies employed in a dataset of 11,263 Instagram posts by 19 Brazilian presidential candidates in 2018 and 2022 national elections. Through two studies, we observed consistent patterns across these candidates on their use of visual political communication. Notably, we identify a prevalence of celebratory and positively toned images. They also exhibit a strong sense of personalization, portraying candidates connected with their voters on a more emotional level. Our research also uncovers unique contextual nuances specific to the Brazilian political landscape. We note a substantial presence of screenshots from news websites and other social media platforms. Furthermore, text-edited images with portrayals emerge as a prominent feature. In light of these results, we engage in a discussion regarding the implications for the broader field of visual political communication. This article serves as a testament to the pivotal role that Instagram has played in shaping the narrative of two fiercely polarized Brazilian elections, casting a revealing light on the ever-evolving dynamics of visual political communication in the digital age. Finally, we propose avenues for future research in the realm of visual political communication.
</details>
<details>
<summary>摘要</summary>
今天的数字时代，图像已成为政治家用于社交媒体平台与选民互动的有效工具。图像具有唯一的情感吸引力，导致用户参与度增加。然而，关于视觉通信的研究在全球南方仍然有限，特别是在2018和2022年布razil大选期间。这项研究希望通过计算方法和质量方法相结合，investigate在11,263个Instagram帖子中19名布razil总统候选人的视觉政治通信策略。经两项研究，我们发现了候选人在使用视觉政治通信的一系列办法的一致性。尤其是，我们发现了一种庆祝和积极的图像优势。候选人还具有更深层的人性化，通过更直观的情感连接与选民。我们的研究还发现了特定于布razil政治景观的 Contextual nuances。我们注意到了屏幕截屉和其他社交媒体平台的屏幕截屉的普遍存在。此外，我们发现了文本修改的图像特征。在这些结果的基础之上，我们进行了关于视觉政治通信领域的探讨。这篇文章作为Instagram在两场极其分化的布razil大选中的形象之一，投射出了数字时代的视觉政治通信在不断演化的特点。最后，我们提出了未来在视觉政治通信领域的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Unlocking-Bias-Detection-Leveraging-Transformer-Based-Models-for-Content-Analysis"><a href="#Unlocking-Bias-Detection-Leveraging-Transformer-Based-Models-for-Content-Analysis" class="headerlink" title="Unlocking Bias Detection: Leveraging Transformer-Based Models for Content Analysis"></a>Unlocking Bias Detection: Leveraging Transformer-Based Models for Content Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00347">http://arxiv.org/abs/2310.00347</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaina Raza, Oluwanifemi Bamgbose, Veronica Chatrath, Shardul Ghuge, Yan Sidyakin, Abdullah Y Muaad</li>
<li>for: 检测文本中的偏见，因为它可能会� reinforcing negative stereotypes，diffuse misinformation，和影响决策。</li>
<li>methods: 我们引入了Contextualized Bi-Directional Dual Transformer（CBDT）分类器，该架构使用了两个相互作用的 transformer 网络：Context Transformer 和 Entity Transformer，以提高偏见检测的能力。</li>
<li>results: CBDT 模型在多个数据集上进行了严谨测试，能够准确地 distinguish 偏见与中性声明，并且可以准确地标识具有偏见的单词。与现有方法相比，CBDT 模型表现出了2-4％的提升。<details>
<summary>Abstract</summary>
Bias detection in text is imperative due to its role in reinforcing negative stereotypes, disseminating misinformation, and influencing decisions. Current language models often fall short in generalizing beyond their training sets. In response, we introduce the Contextualized Bi-Directional Dual Transformer (CBDT) Classifier. This novel architecture utilizes two synergistic transformer networks: the Context Transformer and the Entity Transformer, aiming for enhanced bias detection. Our dataset preparation follows the FAIR principles, ensuring ethical data usage. Through rigorous testing on various datasets, CBDT showcases its ability in distinguishing biased from neutral statements, while also pinpointing exact biased lexemes. Our approach outperforms existing methods, achieving a 2-4\% increase over benchmark performances. This opens avenues for adapting the CBDT model across diverse linguistic and cultural landscapes.
</details>
<details>
<summary>摘要</summary>
文本中的偏见检测非常重要，因为它可以扩大负面刻板印象，传播错误信息，并影响决策。现有语言模型经常无法总是泛化到其训练集之外。为此，我们介绍了 Contextualized Bi-Directional Dual Transformer（CBDT）分类器。这种新的架构使用两个相互作用的转换器网络：上下文转换器和实体转换器，以提高偏见检测能力。我们的数据准备遵循了 FAIR 原则，确保数据使用的道德。经过对多个数据集的严格测试，CBDT 能够 отличи出偏见语句和中立语句，同时还能够准确地标识偏见词语。我们的方法在现有方法之上提高了2-4％的性能，这开启了适应不同语言和文化背景的CBDT 模型的可能性。
</details></li>
</ul>
<hr>
<h2 id="Deep-Reinforcement-Learning-for-Autonomous-Vehicle-Intersection-Navigation"><a href="#Deep-Reinforcement-Learning-for-Autonomous-Vehicle-Intersection-Navigation" class="headerlink" title="Deep Reinforcement Learning for Autonomous Vehicle Intersection Navigation"></a>Deep Reinforcement Learning for Autonomous Vehicle Intersection Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08595">http://arxiv.org/abs/2310.08595</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/palatiumAI/deep-intersection-navigation">https://github.com/palatiumAI/deep-intersection-navigation</a></li>
<li>paper_authors: Badr Ben Elallid, Hamza El Alaoui, Nabil Benamar</li>
<li>for: 本研究探讨了自动驾驶车辆（AV）在高密度交通情况下穿梭复杂T字口的挑战。</li>
<li>methods: 本研究使用了杜邦滞后决策函数Gradient（TD3）计算学习算法来解决这些挑战，实现AV在实时下做出安全和高效决策。</li>
<li>results: 我们的TD3基于方法在CARLA simulate平台上训练和测试后显示稳定协调和安全性表现，在不同的交通密度下都有显著改善。结果表明我们的方法可以有效地帮助AV穿梭T字口，比前一些方法减少旅行延迟、减少碰撞和总成本。这篇研究贡献了自动驾驶领域的强化学习应用和单机制方法的前进，并探讨未来强化学习算法的发展。<details>
<summary>Abstract</summary>
In this paper, we explore the challenges associated with navigating complex T-intersections in dense traffic scenarios for autonomous vehicles (AVs). Reinforcement learning algorithms have emerged as a promising approach to address these challenges by enabling AVs to make safe and efficient decisions in real-time. Here, we address the problem of efficiently and safely navigating T-intersections using a lower-cost, single-agent approach based on the Twin Delayed Deep Deterministic Policy Gradient (TD3) reinforcement learning algorithm. We show that our TD3-based method, when trained and tested in the CARLA simulation platform, demonstrates stable convergence and improved safety performance in various traffic densities. Our results reveal that the proposed approach enables the AV to effectively navigate T-intersections, outperforming previous methods in terms of travel delays, collision minimization, and overall cost. This study contributes to the growing body of knowledge on reinforcement learning applications in autonomous driving and highlights the potential of single-agent, cost-effective methods for addressing more complex driving scenarios and advancing reinforcement learning algorithms in the future.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们探讨了自动驾驶车辆（AV）在稠密交通场景中穿梭复杂的T字路口的挑战。 reinforcement learning算法已经出现为解决这些挑战的有力方法，允许AV在实时中作出安全和高效的决策。在这里，我们解决了在低成本、单代理下使用TD3 reinforcement learning算法来有效地和安全地穿梭T字路口的问题。我们在CARLA simulate平台上训练和测试了我们的TD3基本方法，并证明了它在不同的交通密度下具有稳定的凝固和安全性表现。我们的结果表明，我们的方法可以帮助AV有效地穿梭T字路口，在前一些方法的旁路延迟、碰撞降低和总成本方面表现出色。这篇研究贡献了自动驾驶领域的增长体系，并高亮了未来可能的单代理、低成本方法在推进增强学习算法的前景。
</details></li>
</ul>
<hr>
<h2 id="FedLPA-Personalized-One-shot-Federated-Learning-with-Layer-Wise-Posterior-Aggregation"><a href="#FedLPA-Personalized-One-shot-Federated-Learning-with-Layer-Wise-Posterior-Aggregation" class="headerlink" title="FedLPA: Personalized One-shot Federated Learning with Layer-Wise Posterior Aggregation"></a>FedLPA: Personalized One-shot Federated Learning with Layer-Wise Posterior Aggregation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00339">http://arxiv.org/abs/2310.00339</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Liu, Liangxi Liu, Feiyang Ye, Yunheng Shen, Xia Li, Linshan Jiang, Jialin Li</li>
<li>for: 提高 federated learning 中一 shot 聚合性能，减少 client-server 通信 overhead，并保护 client 的隐私。</li>
<li>methods: 提出了一种叫做 FedLPA 的一 shot 聚合方法，通过层 wise  posterior 聚合来快速地从多个不同训练数据的本地模型中获取更准确的全球模型，而无需Extra 附加数据或暴露任何客户端信息。</li>
<li>results: 在几个 metrics 上实现了对 state-of-the-art 方法的显著改进，包括学习性能和通信 overhead 等。<details>
<summary>Abstract</summary>
Efficiently aggregating trained neural networks from local clients into a global model on a server is a widely researched topic in federated learning. Recently, motivated by diminishing privacy concerns, mitigating potential attacks, and reducing the overhead of communication, one-shot federated learning (i.e., limiting client-server communication into a single round) has gained popularity among researchers. However, the one-shot aggregation performances are sensitively affected by the non-identical training data distribution, which exhibits high statistical heterogeneity in some real-world scenarios. To address this issue, we propose a novel one-shot aggregation method with Layer-wise Posterior Aggregation, named FedLPA. FedLPA aggregates local models to obtain a more accurate global model without requiring extra auxiliary datasets or exposing any confidential local information, e.g., label distributions. To effectively capture the statistics maintained in the biased local datasets in the practical non-IID scenario, we efficiently infer the posteriors of each layer in each local model using layer-wise Laplace approximation and aggregate them to train the global parameters. Extensive experimental results demonstrate that FedLPA significantly improves learning performance over state-of-the-art methods across several metrics.
</details>
<details>
<summary>摘要</summary>
通用训练神经网络从本地客户端归一到服务器端是 Federated Learning 领域中广泛研究的主题。在最近，驱动了隐私问题的减少、抵御攻击和通信过程的减少，一shot Federated Learning（即限制客户端服务器通信到一round）在研究人员中得到了广泛的关注。然而，one-shot 集成性表现受到非标准训练数据分布的影响，这些分布在一些实际场景中具有高度的统计差异性。为 Addressing this issue, we propose a novel one-shot aggregation method called FedLPA, which aggregates local models to obtain a more accurate global model without requiring extra auxiliary datasets or exposing any confidential local information, such as label distributions. To effectively capture the statistics maintained in the biased local datasets in the practical non-IID scenario, we efficiently infer the posteriors of each layer in each local model using layer-wise Laplace approximation and aggregate them to train the global parameters. Extensive experimental results demonstrate that FedLPA significantly improves learning performance over state-of-the-art methods across several metrics.
</details></li>
</ul>
<hr>
<h2 id="Quantization-of-Deep-Neural-Networks-to-facilitate-self-correction-of-weights-on-Phase-Change-Memory-based-analog-hardware"><a href="#Quantization-of-Deep-Neural-Networks-to-facilitate-self-correction-of-weights-on-Phase-Change-Memory-based-analog-hardware" class="headerlink" title="Quantization of Deep Neural Networks to facilitate self-correction of weights on Phase Change Memory-based analog hardware"></a>Quantization of Deep Neural Networks to facilitate self-correction of weights on Phase Change Memory-based analog hardware</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00337">http://arxiv.org/abs/2310.00337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arseni Ivanov</li>
<li>for: 这篇研究旨在实现硬件加速的神经网络，以应用于边缘 Computing 应用。</li>
<li>methods: 这篇研究使用了一种特别设计 для硬件架构的量化技术，以及一种自我修复机制。</li>
<li>results: 研究结果显示，当与在芯片上的普勒变化生成器一起使用时，我们的自我修复神经网络可以与使用分析敏感的算法相比。<details>
<summary>Abstract</summary>
In recent years, hardware-accelerated neural networks have gained significant attention for edge computing applications. Among various hardware options, crossbar arrays, offer a promising avenue for efficient storage and manipulation of neural network weights. However, the transition from trained floating-point models to hardware-constrained analog architectures remains a challenge. In this work, we combine a quantization technique specifically designed for such architectures with a novel self-correcting mechanism. By utilizing dual crossbar connections to represent both the positive and negative parts of a single weight, we develop an algorithm to approximate a set of multiplicative weights. These weights, along with their differences, aim to represent the original network's weights with minimal loss in performance. We implement the models using IBM's aihwkit and evaluate their efficacy over time. Our results demonstrate that, when paired with an on-chip pulse generator, our self-correcting neural network performs comparably to those trained with analog-aware algorithms.
</details>
<details>
<summary>摘要</summary>
近年来，固件加速神经网络得到了边缘计算应用的广泛关注。多种硬件选项中，扫描阵列表示出了可能的高效存储和神经网络权重的方式。然而，从训练浮点模型转换到硬件受限的材料建筑仍然是一个挑战。在这种工作中，我们结合特定于这些架构的量化技术和一种新的自我修正机制。通过使用双扫描连接来表示单个权重的正负部分，我们开发了一种近似多个乘数的算法。这些乘数、其差异，目的是表示原始网络的权重，尽可能减少性能损失。我们使用IBM的aihwkit进行实现，并对时间进行评估。我们的结果表明，当与在板内普ulse生成器结合使用时，我们的自我修正神经网络能够与使用材料感知算法训练的神经网络相比走。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Planning-with-Latent-Diffusion"><a href="#Efficient-Planning-with-Latent-Diffusion" class="headerlink" title="Efficient Planning with Latent Diffusion"></a>Efficient Planning with Latent Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00311">http://arxiv.org/abs/2310.00311</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenhao Li</li>
<li>for: 本研究旨在解决离线学习中的时间抽象和有效规划问题，特别是面临长期任务和延迟的奖励问题。现有方法通常在raw action space中规划，可能是不fficient和inflexible。</li>
<li>methods: 本文提出了一种基于latent action space的方法， capturing only possible actions within the behavior policy support and decoupling the temporal structure between planning and modeling。但现有的latent-action-based方法通常是离散的，需要昂贵的规划。</li>
<li>results: 本文提出的$\texttt{LatentDiffuser}$方法在low-dimensional locomotion control任务中表现竞争性，并在高维任务中超越了现有方法。<details>
<summary>Abstract</summary>
Temporal abstraction and efficient planning pose significant challenges in offline reinforcement learning, mainly when dealing with domains that involve temporally extended tasks and delayed sparse rewards. Existing methods typically plan in the raw action space and can be inefficient and inflexible. Latent action spaces offer a more flexible paradigm, capturing only possible actions within the behavior policy support and decoupling the temporal structure between planning and modeling. However, current latent-action-based methods are limited to discrete spaces and require expensive planning. This paper presents a unified framework for continuous latent action space representation learning and planning by leveraging latent, score-based diffusion models. We establish the theoretical equivalence between planning in the latent action space and energy-guided sampling with a pretrained diffusion model and incorporate a novel sequence-level exact sampling method. Our proposed method, $\texttt{LatentDiffuser}$, demonstrates competitive performance on low-dimensional locomotion control tasks and surpasses existing methods in higher-dimensional tasks.
</details>
<details>
<summary>摘要</summary>
temporal abstraction和有效规划在线上游戏学习中具有重要挑战，尤其是在具有长时间任务和延迟的奖励的Domain中。现有方法通常在原生动作空间中规划，可能是不灵活和不高效的。latent action space提供了更加灵活的思想，只 capture行为策略支持下可能的动作和时间结构的分离。然而，当前的latent-action-based方法通常是离散的，需要昂贵的规划。这篇论文提出了一个综合框架，使用latent, score-based diffusion模型来学习和规划连续latent action space。我们证明了在latent action space中规划和energy-guided sampling with pretrained diffusion模型之间的理论等价性，并 интегrios了一种新的序列级别准确 sampling方法。我们的提案方法， $\texttt{LatentDiffuser}$,在low-dimensional locomotion控制任务上显示了竞争性的性能，并在高维任务上超过了现有的方法。
</details></li>
</ul>
<hr>
<h2 id="A-Hierarchical-Approach-to-Environment-Design-with-Generative-Trajectory-Modeling"><a href="#A-Hierarchical-Approach-to-Environment-Design-with-Generative-Trajectory-Modeling" class="headerlink" title="A Hierarchical Approach to Environment Design with Generative Trajectory Modeling"></a>A Hierarchical Approach to Environment Design with Generative Trajectory Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00301">http://arxiv.org/abs/2310.00301</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dexun Li, Pradeep Varakantham</li>
<li>for:  trains generally capable agents to achieve good zero-shot transfer performance</li>
<li>methods:  uses Hierarchical MDP and synthetic experience dataset to reduce resource-intensive interactions</li>
<li>results:  significantly improves efficiency and robustness of agent under limited training resources, with manifold advantages and effectiveness in various domains.<details>
<summary>Abstract</summary>
Unsupervised Environment Design (UED) is a paradigm for training generally capable agents to achieve good zero-shot transfer performance. This paradigm hinges on automatically generating a curriculum of training environments. Leading approaches for UED predominantly use randomly generated environment instances to train the agent. While these methods exhibit good zero-shot transfer performance, they often encounter challenges in effectively exploring large design spaces or leveraging previously discovered underlying structures, To address these challenges, we introduce a novel framework based on Hierarchical MDP (Markov Decision Processes). Our approach includes an upper-level teacher's MDP responsible for training a lower-level MDP student agent, guided by the student's performance. To expedite the learning of the upper leavel MDP, we leverage recent advancements in generative modeling to generate synthetic experience dataset for training the teacher agent. Our algorithm, called Synthetically-enhanced Hierarchical Environment Design (SHED), significantly reduces the resource-intensive interactions between the agent and the environment. To validate the effectiveness of SHED, we conduct empirical experiments across various domains, with the goal of developing an efficient and robust agent under limited training resources. Our results show the manifold advantages of SHED and highlight its effectiveness as a potent instrument for curriculum-based learning within the UED framework. This work contributes to exploring the next generation of RL agents capable of adeptly handling an ever-expanding range of complex tasks.
</details>
<details>
<summary>摘要</summary>
自动化环境设计（UES）是一种训练通用智能代理人达到良好零批转换性的 paradigm。这种 paradigm 基于自动生成训练环境的课程。现有领先的UES方法主要使用随机生成的环境实例来训练代理人。 although these methods have shown good zero-shot transfer performance, they often encounter challenges in effectively exploring large design spaces or leveraging previously discovered underlying structures.To address these challenges, we propose a novel framework based on Hierarchical MDP (Markov Decision Processes). Our approach includes an upper-level teacher's MDP responsible for training a lower-level MDP student agent, guided by the student's performance. To expedite the learning of the upper-level MDP, we leverage recent advancements in generative modeling to generate synthetic experience datasets for training the teacher agent. Our algorithm, called Synthetically-enhanced Hierarchical Environment Design (SHED), significantly reduces the resource-intensive interactions between the agent and the environment.To validate the effectiveness of SHED, we conduct empirical experiments across various domains, with the goal of developing an efficient and robust agent under limited training resources. Our results show the manifold advantages of SHED and highlight its effectiveness as a potent instrument for curriculum-based learning within the UED framework. This work contributes to exploring the next generation of RL agents capable of adeptly handling an ever-expanding range of complex tasks.
</details></li>
</ul>
<hr>
<h2 id="Graph-Neural-Architecture-Search-with-GPT-4"><a href="#Graph-Neural-Architecture-Search-with-GPT-4" class="headerlink" title="Graph Neural Architecture Search with GPT-4"></a>Graph Neural Architecture Search with GPT-4</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01436">http://arxiv.org/abs/2310.01436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haishuai Wang, Yang Gao, Xin Zheng, Peng Zhang, Hongyang Chen, Jiajun Bu</li>
<li>for: 自动设计图像神经网络</li>
<li>methods: 使用 GPT-4 为 GNAS 设计新的提示，将 GPT-4 引导到创造图像神经网络的生成任务中</li>
<li>results: 实验结果显示，将 GPT-4  embed 到 GNAS 中可以超越现有的GNAS方法，并且实现更快的融合速度<details>
<summary>Abstract</summary>
Graph Neural Architecture Search (GNAS) has shown promising results in automatically designing graph neural networks. However, GNAS still requires intensive human labor with rich domain knowledge to design the search space and search strategy. In this paper, we integrate GPT-4 into GNAS and propose a new GPT-4 based Graph Neural Architecture Search method (GPT4GNAS for short). The basic idea of our method is to design a new class of prompts for GPT-4 to guide GPT-4 toward the generative task of graph neural architectures. The prompts consist of descriptions of the search space, search strategy, and search feedback of GNAS. By iteratively running GPT-4 with the prompts, GPT4GNAS generates more accurate graph neural networks with fast convergence. Experimental results show that embedding GPT-4 into GNAS outperforms the state-of-the-art GNAS methods.
</details>
<details>
<summary>摘要</summary>
graph神经架构搜寻（GNAS）已经显示出优异的结果，可以自动设计graph神经网络。然而，GNAS仍然需要专业知识和专业人员的努力来设计搜寻空间和搜寻策略。在这篇论文中，我们将GPT-4 integrate到GNAS中，并提出了一种基于GPT-4的新的Graph Neural Architecture Search方法（GPT4GNAS简称）。我们的方法的基本想法是，通过设计GPT-4的新类型的提示，将GPT-4引导到Graph Neural Architecture Search的生成任务中。这些提示包括搜寻空间、搜寻策略和搜寻反馈。通过轮询GPT-4的提示，GPT4GNAS可以更快地生成更准确的graph神经网络。实验结果显示，将GPT-4 integrate到GNAS中比前者的GNAS方法更高效。
</details></li>
</ul>
<hr>
<h2 id="Active-Learning-Based-Fine-Tuning-Framework-for-Speech-Emotion-Recognition"><a href="#Active-Learning-Based-Fine-Tuning-Framework-for-Speech-Emotion-Recognition" class="headerlink" title="Active Learning Based Fine-Tuning Framework for Speech Emotion Recognition"></a>Active Learning Based Fine-Tuning Framework for Speech Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00283">http://arxiv.org/abs/2310.00283</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongyuan Li, Yusong Wang, Kotaro Funakoshi, Manabu Okumura</li>
<li>for: 这种方法是为了提高人机交互中的语音情感识别（SER）的性能和效率。</li>
<li>methods: 这种方法使用了任务适应预训练（TAPT）和活动学习（AL）方法来增强性能和效率。具体来说，首先使用TAPT来减少预训练和下游任务之间的信息差距。然后，使用AL方法来遍历选择最有信息和多样性的样本进行细化，从而减少时间消耗。</li>
<li>results: 实验表明，只使用20%的样本可以提高8.45%的准确率，同时减少79%的时间消耗。<details>
<summary>Abstract</summary>
Speech emotion recognition (SER) has drawn increasing attention for its applications in human-machine interaction. However, existing SER methods ignore the information gap between the pre-training speech recognition task and the downstream SER task, leading to sub-optimal performance. Moreover, they require much time to fine-tune on each specific speech dataset, restricting their effectiveness in real-world scenes with large-scale noisy data. To address these issues, we propose an active learning (AL) based Fine-Tuning framework for SER that leverages task adaptation pre-training (TAPT) and AL methods to enhance performance and efficiency. Specifically, we first use TAPT to minimize the information gap between the pre-training and the downstream task. Then, AL methods are used to iteratively select a subset of the most informative and diverse samples for fine-tuning, reducing time consumption. Experiments demonstrate that using only 20\%pt. samples improves 8.45\%pt. accuracy and reduces 79\%pt. time consumption.
</details>
<details>
<summary>摘要</summary>
人工智能感情识别（SER）在人机交互中受到越来越多的关注。然而，现有的SER方法忽视了预训练语音识别任务和下游SER任务之间的信息差距，导致性能下降。另外，它们需要大量时间来 Fine-tune 每个特定语音数据集，限制了它们在实际场景中的实用性。为解决这些问题，我们提议一种基于活动学习（AL）的 Fine-Tuning 框架 для SER，利用任务适应预训练（TAPT）和 AL 方法提高性能和效率。具体来说，我们首先使用 TAPT 将预训练任务和下游任务之间的信息差距减少到最小。然后，我们使用 AL 方法选择下游任务中最有用和多样的样本进行 Fine-tuning，从而减少时间消耗。实验结果表明，只使用 20% 的样本可以提高 8.45% 的精度和减少 79% 的时间消耗。
</details></li>
</ul>
<hr>
<h2 id="Corex-Pushing-the-Boundaries-of-Complex-Reasoning-through-Multi-Model-Collaboration"><a href="#Corex-Pushing-the-Boundaries-of-Complex-Reasoning-through-Multi-Model-Collaboration" class="headerlink" title="Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration"></a>Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00280">http://arxiv.org/abs/2310.00280</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qiushisun/corex">https://github.com/qiushisun/corex</a></li>
<li>paper_authors: Qiushi Sun, Zhangyue Yin, Xiang Li, Zhiyong Wu, Xipeng Qiu, Lingpeng Kong</li>
<li>for: 本研究旨在提高大型自然语言处理（NLP）模型在复杂任务解释中的表现，通过设计多模型协作策略，使模型能够’’思考出obox’’，提高事实性、准确性和可靠性。</li>
<li>methods: 本研究提出了一个新的多模型协作策略集合，称为Corex，包括对话、评审和检索模式。这些模式通过启发人类行为，激发模型进行多样化的解释，提高task-agnostic的表现。</li>
<li>results: 经过对四种不同类型的解释任务的广泛实验，我们发现，通过多模型协作，解释性能得到了显著提高，与现有方法相比。此外，我们还提供了更多的结果和深入分析，证明我们的方法的成本效益和注解效率。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are evolving at an unprecedented pace and have exhibited considerable capability in the realm of natural language processing (NLP) with world knowledge. Benefiting from ultra-large-scale training corpora, a single LLM can manage typical NLP tasks competently. However, its performance in executing reasoning tasks is still confined by the limitations of its internal representations. To push this boundary further, we introduce Corex in this paper, a suite of novel general-purpose strategies that transform LLMs into autonomous agents pioneering multi-model collaborations for complex task-solving. Inspired by human behaviors, Corex is constituted by diverse collaboration paradigms including Debate, Review, and Retrieve modes, which collectively work towards enhancing the factuality, faithfulness, and reliability of the reasoning process. These paradigms foster task-agnostic approaches that enable LLMs to ''think outside the box,'' thereby overcoming hallucinations and providing better solutions. Through extensive experiments across four different types of reasoning tasks, we demonstrate that orchestrating multiple LLMs to work in concert yields substantially better performance compared to existing methods. Further results and in-depth analysis demonstrate the cost-effectiveness of our method, facilitating collaboration among different LLMs and promoting annotation efficiency.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Unified-Framework-for-Generative-Data-Augmentation-A-Comprehensive-Survey"><a href="#A-Unified-Framework-for-Generative-Data-Augmentation-A-Comprehensive-Survey" class="headerlink" title="A Unified Framework for Generative Data Augmentation: A Comprehensive Survey"></a>A Unified Framework for Generative Data Augmentation: A Comprehensive Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00277">http://arxiv.org/abs/2310.00277</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunhao Chen, Zihui Yan, Yunjie Zhu</li>
<li>for: 提高机器学习应用中数据缺乏的问题</li>
<li>methods: 使用生成数据生成技术（GDA）来增加数据量</li>
<li>results: 提供了一个总结GDA领域的框架，揭示了一些研究方向，如有效的数据选择、大规模模型的应用、建立GDA标准 bencmark等。In English, that means:</li>
<li>for: To alleviate the problem of data scarcity in machine learning applications</li>
<li>methods: Using generative data augmentation (GDA) techniques to increase data quantity</li>
<li>results: Provide a comprehensive framework for the GDA landscape, reveal some research directions, such as effective data selection, theoretical development for large-scale models, and establishing a benchmark for GDA.<details>
<summary>Abstract</summary>
Generative data augmentation (GDA) has emerged as a promising technique to alleviate data scarcity in machine learning applications. This thesis presents a comprehensive survey and unified framework of the GDA landscape. We first provide an overview of GDA, discussing its motivation, taxonomy, and key distinctions from synthetic data generation. We then systematically analyze the critical aspects of GDA - selection of generative models, techniques to utilize them, data selection methodologies, validation approaches, and diverse applications. Our proposed unified framework categorizes the extensive GDA literature, revealing gaps such as the lack of universal benchmarks. The thesis summarises promising research directions, including , effective data selection, theoretical development for large-scale models' application in GDA and establishing a benchmark for GDA. By laying a structured foundation, this thesis aims to nurture more cohesive development and accelerate progress in the vital arena of generative data augmentation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Investigating-the-Efficacy-of-Large-Language-Models-in-Reflective-Assessment-Methods-through-Chain-of-Thoughts-Prompting"><a href="#Investigating-the-Efficacy-of-Large-Language-Models-in-Reflective-Assessment-Methods-through-Chain-of-Thoughts-Prompting" class="headerlink" title="Investigating the Efficacy of Large Language Models in Reflective Assessment Methods through Chain of Thoughts Prompting"></a>Investigating the Efficacy of Large Language Models in Reflective Assessment Methods through Chain of Thoughts Prompting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00272">http://arxiv.org/abs/2310.00272</a></li>
<li>repo_url: None</li>
<li>paper_authors: Baphumelele Masikisiki, Vukosi Marivate, Yvette Hlope</li>
<li>for: 本研究的目的是评估四种语言模型对第三年医学生的投影文章进行评估，以评估学生的批判思维能力。</li>
<li>methods: 本研究使用了Chain of Thought（CoT）提示法来训练大型语言模型完成特定任务。</li>
<li>results: 结果显示， Among all the models, Llama-7b performs the least effectively, displaying the highest mean squared error。 conversely, ChatGPT emerges as the superior model, boasting a higher Cohen kappa score value of 0.53。<details>
<summary>Abstract</summary>
Large Language Models, such as Generative Pre-trained Transformer 3 (aka. GPT-3), have been developed to understand language through the analysis of extensive text data, allowing them to identify patterns and connections between words. While LLMs have demonstrated impressive performance across various text-related tasks, they encounter challenges in tasks associated with reasoning. To address this challenge, Chain of Thought(CoT) prompting method has been proposed as a means to enhance LLMs' proficiency in complex reasoning tasks like solving math word problems and answering questions based on logical argumentative reasoning. The primary aim of this research is to assess how well four language models can grade reflective essays of third-year medical students. The assessment will specifically target the evaluation of critical thinking skills using CoT prompting.   The research will provide the following contributions; to introduce and educate on the process of instructing models to evaluate reflective essays from a dataset they have not been previously trained on; to illustrate the use of CoT prompting as an instructional approach for training large models to carry out particular tasks. Our results suggest that among all the models, Llama-7b performs the least effectively, displaying the highest mean squared error. Conversely, ChatGPT emerges as the superior model, boasting a higher Cohen kappa score value of 0.53. Lastly, it's important to note that the selected models do prioritise user privacy by allowing users to delete their own conducted conversations.
</details>
<details>
<summary>摘要</summary>
大型语言模型，如生成预训练转换器3（GPT-3），已经开发以便理解语言，通过分析大量文本数据，找到语言中的模式和关系。 Although these models have shown impressive performance in various text-related tasks, they struggle with tasks that require reasoning. To address this challenge, Chain of Thought (CoT) prompting method has been proposed to enhance the models' ability in complex reasoning tasks, such as solving math word problems and answering questions based on logical argumentative reasoning.本研究的主要目标是评估四个语言模型在评估第三年医学生的反思文章时的表现。 研究将特别target evaluation of critical thinking skills using CoT prompting。 我们的结果表明，在所有模型中，Llama-7b表现最差，显示最高的方差平方误差。 相反，ChatGPT emerges as the superior model, with a higher Cohen kappa score value of 0.53. 最后，我们需要注意的是，选择的模型强调用户隐私，允许用户删除自己的进行的对话。
</details></li>
</ul>
<hr>
<h2 id="Unravel-Anomalies-An-End-to-end-Seasonal-Trend-Decomposition-Approach-for-Time-Series-Anomaly-Detection"><a href="#Unravel-Anomalies-An-End-to-end-Seasonal-Trend-Decomposition-Approach-for-Time-Series-Anomaly-Detection" class="headerlink" title="Unravel Anomalies: An End-to-end Seasonal-Trend Decomposition Approach for Time Series Anomaly Detection"></a>Unravel Anomalies: An End-to-end Seasonal-Trend Decomposition Approach for Time Series Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00268">http://arxiv.org/abs/2310.00268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenwei Zhang, Ruiqi Wang, Ran Ding, Yuantao Gu</li>
<li>for: 本研究旨在解决复杂时间序列数据中的各种突变问题，传统时间序列异常检测方法在面临复杂时间序列数据和多种异常时 often struggle.</li>
<li>methods: 我们提出了TADNet，一种结束到端的时间序列异常检测模型，利用季节性-趋势分解将各种异常联系到特定的分解组件，从而简化复杂时间序列分析和异常检测性能。我们的训练方法包括预训练Synthetic dataset followed by fine-tuning, strike a balance between effective decomposition and precise anomaly detection.</li>
<li>results: 实验 validate TADNet在真实世界 dataset 上表现出state-of-the-art的性能，可以准确检测多种异常。<details>
<summary>Abstract</summary>
Traditional Time-series Anomaly Detection (TAD) methods often struggle with the composite nature of complex time-series data and a diverse array of anomalies. We introduce TADNet, an end-to-end TAD model that leverages Seasonal-Trend Decomposition to link various types of anomalies to specific decomposition components, thereby simplifying the analysis of complex time-series and enhancing detection performance. Our training methodology, which includes pre-training on a synthetic dataset followed by fine-tuning, strikes a balance between effective decomposition and precise anomaly detection. Experimental validation on real-world datasets confirms TADNet's state-of-the-art performance across a diverse range of anomalies.
</details>
<details>
<summary>摘要</summary>
传统时序异常检测（TAD）方法经常遇到复杂时序数据的composite性和多样化异常的问题。我们介绍TADNet，一种终端TAD模型，利用季节性趋势分解将各种异常类型关联到特定的分解组件，从而简化复杂时序分析并提高异常检测性能。我们的训练方法，包括先行预训练 followed by fine-tuning，在有效分解和精准异常检测之间寻找平衡点。实验 validate TADNet在实际 dataset上具有现代水平的性能，并在多种异常情况下具有优异的检测性能。
</details></li>
</ul>
<hr>
<h2 id="The-Physics-of-Preference-Unravelling-Imprecision-of-Human-Preferences-through-Magnetisation-Dynamics"><a href="#The-Physics-of-Preference-Unravelling-Imprecision-of-Human-Preferences-through-Magnetisation-Dynamics" class="headerlink" title="The Physics of Preference: Unravelling Imprecision of Human Preferences through Magnetisation Dynamics"></a>The Physics of Preference: Unravelling Imprecision of Human Preferences through Magnetisation Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00267">http://arxiv.org/abs/2310.00267</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ivan S. Maksymov, Ganna Pogrebna</li>
<li>for: 这篇论文是为了研究人类决策过程中的不一致行为和偏好逆转现象而写的。</li>
<li>methods: 这篇论文使用了物理原理，具体来说是磁化逆转现象在磁性奈米结构中的电流驱动，来模拟人类决策过程。</li>
<li>results: 测试结果表明，这种物理和心理学材料的混合可以准确地捕捉人类决策过程中的复杂性。<details>
<summary>Abstract</summary>
Paradoxical decision-making behaviours such as preference reversal often arise from imprecise or noisy human preferences. By harnessing the physical principle of magnetisation reversal in ferromagnetic nanostructures driven by electric current, we developed a model that closely reflects human decision-making dynamics. Tested against a spectrum of psychological data, our model adeptly captures the complexities inherent in individual choices. This blend of physics and psychology paves the way for fresh perspectives on understanding human decision-making processes.
</details>
<details>
<summary>摘要</summary>
人类决策行为经常呈现出悖论的特点，如偏好逆转。我们通过利用束缚性材料中的电流驱动的磁化现象，开发了一种模型，能够准确反映人类决策过程中的复杂性。对各种心理数据进行测试，我们的模型能够成功捕捉人类决策过程中的复杂性。这种物理和心理学的融合，为人类决策过程的理解带来了新的视角。
</details></li>
</ul>
<hr>
<h2 id="A-quantum-system-control-method-based-on-enhanced-reinforcement-learning"><a href="#A-quantum-system-control-method-based-on-enhanced-reinforcement-learning" class="headerlink" title="A quantum system control method based on enhanced reinforcement learning"></a>A quantum system control method based on enhanced reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03036">http://arxiv.org/abs/2310.03036</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjie Liu, Bosi Wang, Jihao Fan, Yebo Ge, Mohammed Zidan</li>
<li>for: 控制量子系统，提高控制精度和效率。</li>
<li>methods: 基于增强再征学习（QSC-ERL）方法，将量子系统控制转化为再征学习任务。使用新的增强神经网络，快速实现最大化长期总奖励，精确地导向量子状态从初始状态到目标状态。</li>
<li>results: 与其他方法比较，QSC-ERL在有限资源条件下达到近似1比例学习控制量子系统，需要更少的集数进行量子状态演化。<details>
<summary>Abstract</summary>
Traditional quantum system control methods often face different constraints, and are easy to cause both leakage and stochastic control errors under the condition of limited resources. Reinforcement learning has been proved as an efficient way to complete the quantum system control task. To learn a satisfactory control strategy under the condition of limited resources, a quantum system control method based on enhanced reinforcement learning (QSC-ERL) is proposed. The states and actions in reinforcement learning are mapped to quantum states and control operations in quantum systems. By using new enhanced neural networks, reinforcement learning can quickly achieve the maximization of long-term cumulative rewards, and a quantum state can be evolved accurately from an initial state to a target state. According to the number of candidate unitary operations, the three-switch control is used for simulation experiments. Compared with other methods, the QSC-ERL achieves close to 1 fidelity learning control of quantum systems, and takes fewer episodes to quantum state evolution under the condition of limited resources.
</details>
<details>
<summary>摘要</summary>
传统量子系统控制方法经常遇到不同的限制，容易导致泄漏和随机控制错误，特别在有限资源的情况下。基于增强的回归学习（QSC-ERL）方法是一种有效的完成量子系统控制任务的方法。在这种方法中， reinforcement learning 中的状态和操作被映射到量子系统中的状态和控制操作。通过使用新的增强神经网络， reinforcement learning 可以快速实现最大化长期累积奖励，并将量子状态从初始状态演化到目标状态。根据候选unitary操作的数量，使用三个转移控制来进行模拟实验。相比其他方法，QSC-ERL 可以在有限资源的情况下准确地控制量子系统，并在 fewer episodes 中实现量子状态演化。
</details></li>
</ul>
<hr>
<h2 id="AdaptNet-Policy-Adaptation-for-Physics-Based-Character-Control"><a href="#AdaptNet-Policy-Adaptation-for-Physics-Based-Character-Control" class="headerlink" title="AdaptNet: Policy Adaptation for Physics-Based Character Control"></a>AdaptNet: Policy Adaptation for Physics-Based Character Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00239">http://arxiv.org/abs/2310.00239</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pei Xu, Kaixiang Xie, Sheldon Andrews, Paul G. Kry, Michael Neff, Morgan McGuire, Ioannis Karamouzas, Victor Zordan</li>
<li>for: 这篇论文旨在提出一种方法，以便从现有策略中快速学习新的行为。</li>
<li>methods: 该方法基于给定的奖励学习控制器，使用两层层次结构，首先增强原始状态嵌入，然后修改策略网络层来实现更大的更改。</li>
<li>results: 该方法可以快速适应现有的物理学习控制器，并在各种新的步态、任务目标、角色 morphology 和环境变化中显示出显著的提高。此外，它也表现出了与其他方法相比明显的增长效率，即训练时间的减少。<details>
<summary>Abstract</summary>
Motivated by humans' ability to adapt skills in the learning of new ones, this paper presents AdaptNet, an approach for modifying the latent space of existing policies to allow new behaviors to be quickly learned from like tasks in comparison to learning from scratch. Building on top of a given reinforcement learning controller, AdaptNet uses a two-tier hierarchy that augments the original state embedding to support modest changes in a behavior and further modifies the policy network layers to make more substantive changes. The technique is shown to be effective for adapting existing physics-based controllers to a wide range of new styles for locomotion, new task targets, changes in character morphology and extensive changes in environment. Furthermore, it exhibits significant increase in learning efficiency, as indicated by greatly reduced training times when compared to training from scratch or using other approaches that modify existing policies. Code is available at https://motion-lab.github.io/AdaptNet.
</details>
<details>
<summary>摘要</summary>
<<TRANSLATION>>受人类学习新技能的能力启发，本文介绍了 AdaptNet，一种修改现有政策的方法，以允许快速从相似任务中学习新行为。基于给定的奖励学习控制器，AdaptNet使用两层层次结构，将原始状态嵌入更改以支持小幅度的行为变化，并对策略网络层进行更加重要的更改。该技术能够有效地适应现有物理学习控制器到各种新的步态、任务目标、角色结构和环境变化中。此外，它还表现出了明显提高学习效率，如训练时间减少了很多，相比于从scratch或使用其他修改现有政策的方法。代码可以在 <https://motion-lab.github.io/AdaptNet> 上获取。<</TRANSLATION>>
</details></li>
</ul>
<hr>
<h2 id="Combining-Spatial-and-Temporal-Abstraction-in-Planning-for-Better-Generalization"><a href="#Combining-Spatial-and-Temporal-Abstraction-in-Planning-for-Better-Generalization" class="headerlink" title="Combining Spatial and Temporal Abstraction in Planning for Better Generalization"></a>Combining Spatial and Temporal Abstraction in Planning for Better Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00229">http://arxiv.org/abs/2310.00229</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pwnerharry/skipper">https://github.com/pwnerharry/skipper</a></li>
<li>paper_authors: Mingde Zhao, Safa Alver, Harm van Seijen, Romain Laroche, Doina Precup, Yoshua Bengio</li>
<li>for: 提高机器人的执行效率和普适性</li>
<li>methods: 基于模型学习的决策减少方法，通过空间和时间抽象来总结学习的技能并应用于新情况</li>
<li>results: 比对现有高级 плани方法， Skipper 在零shot总结中显示出显著优势<details>
<summary>Abstract</summary>
Inspired by human conscious planning, we propose Skipper, a model-based reinforcement learning agent that utilizes spatial and temporal abstractions to generalize learned skills in novel situations. It automatically decomposes the task at hand into smaller-scale, more manageable subtasks and hence enables sparse decision-making and focuses its computation on the relevant parts of the environment. This relies on the definition of a high-level proxy problem represented as a directed graph, in which vertices and edges are learned end-to-end using hindsight. Our theoretical analyses provide performance guarantees under appropriate assumptions and establish where our approach is expected to be helpful. Generalization-focused experiments validate Skipper's significant advantage in zero-shot generalization, compared to existing state-of-the-art hierarchical planning methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Steered-Diffusion-A-Generalized-Framework-for-Plug-and-Play-Conditional-Image-Synthesis"><a href="#Steered-Diffusion-A-Generalized-Framework-for-Plug-and-Play-Conditional-Image-Synthesis" class="headerlink" title="Steered Diffusion: A Generalized Framework for Plug-and-Play Conditional Image Synthesis"></a>Steered Diffusion: A Generalized Framework for Plug-and-Play Conditional Image Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00224">http://arxiv.org/abs/2310.00224</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nithin Gopalakrishnan Nair, Anoop Cherian, Suhas Lohit, Ye Wang, Toshiaki Koike-Akino, Vishal M. Patel, Tim K. Marks</li>
<li>for: 这篇研究是用于实现条件参数化的内部生成模型，以提高内部生成的品质和精确性。</li>
<li>methods: 这篇研究使用了扩散模型，并通过设计一个逆模型来控制扩散过程的推导。</li>
<li>results: 研究所得到的结果显示，这种方法可以在不需要训练的情况下，实现高品质的内部生成，并且可以轻松地整合多个条件。<details>
<summary>Abstract</summary>
Conditional generative models typically demand large annotated training sets to achieve high-quality synthesis. As a result, there has been significant interest in designing models that perform plug-and-play generation, i.e., to use a predefined or pretrained model, which is not explicitly trained on the generative task, to guide the generative process (e.g., using language). However, such guidance is typically useful only towards synthesizing high-level semantics rather than editing fine-grained details as in image-to-image translation tasks. To this end, and capitalizing on the powerful fine-grained generative control offered by the recent diffusion-based generative models, we introduce Steered Diffusion, a generalized framework for photorealistic zero-shot conditional image generation using a diffusion model trained for unconditional generation. The key idea is to steer the image generation of the diffusion model at inference time via designing a loss using a pre-trained inverse model that characterizes the conditional task. This loss modulates the sampling trajectory of the diffusion process. Our framework allows for easy incorporation of multiple conditions during inference. We present experiments using steered diffusion on several tasks including inpainting, colorization, text-guided semantic editing, and image super-resolution. Our results demonstrate clear qualitative and quantitative improvements over state-of-the-art diffusion-based plug-and-play models while adding negligible additional computational cost.
</details>
<details>
<summary>摘要</summary>
通常情况下，条件生成模型需要大量标注训练数据来达到高质量生成。因此，有很大的兴趣在设计可以使用预定义或预训练模型来导引生成过程的模型。然而，这种导引通常只有用于synthesize高级 semantics而不是编辑细节，如图像。为了解决这个问题，我们介绍了Steered Diffusion，一种普适的扩展框架，可以通过定制 diffusion 模型来实现零shot条件图像生成。我们的关键想法是在推理时使用预训练 inverse 模型来定义条件任务的损失函数，以控制 diffusion 过程的抽象样本。我们的框架允许在推理时容易添加多个条件。我们通过对Steered Diffusion进行多种任务的实验，包括填充、色调化、文本干涉性编辑和图像超分辨率重建，并取得了明显的qualitative和quantitative提升。同时，我们的方法添加了negligible的计算成本。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Random-Noise-Insights-on-Anonymization-Strategies-from-a-Latent-Bandit-Study"><a href="#Beyond-Random-Noise-Insights-on-Anonymization-Strategies-from-a-Latent-Bandit-Study" class="headerlink" title="Beyond Random Noise: Insights on Anonymization Strategies from a Latent Bandit Study"></a>Beyond Random Noise: Insights on Anonymization Strategies from a Latent Bandit Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00221">http://arxiv.org/abs/2310.00221</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Galozy, Sadi Alawadi, Victor Kebande, Sławomir Nowaczyk</li>
<li>for: 本研究探讨了学习场景下知识共享的隐私问题。我们的研究贡献到了隐私保护机器学习领域的快速发展，并高亮了特定攻击模式需要适应的隐私技术。</li>
<li>methods: 我们使用隐藏链接设定来评估隐私和推荐性能之间的权衡。我们采用了不同的聚合策略，如平均、最近邻居和聚合加杂。在 simulate 攻击enario中，我们利用了公开的 auxillary 信息，用于攻击者所获取的公共信息。</li>
<li>results: 我们在三个开放的实际数据集上进行了实验，发现将噪声添加到个体用户数据记录中并不是一个好的选择。相比拟标准噪声机制，使用不同的聚合策略和噪声可以提供更多的灵活性。例如，使用不同大小的群组的平均值可以提供不可能由噪声alone 实现的灵活性。总之，没有单一的聚合策略可以在给定的隐私水平下实现最佳的忧郁。<details>
<summary>Abstract</summary>
This paper investigates the issue of privacy in a learning scenario where users share knowledge for a recommendation task. Our study contributes to the growing body of research on privacy-preserving machine learning and underscores the need for tailored privacy techniques that address specific attack patterns rather than relying on one-size-fits-all solutions. We use the latent bandit setting to evaluate the trade-off between privacy and recommender performance by employing various aggregation strategies, such as averaging, nearest neighbor, and clustering combined with noise injection. More specifically, we simulate a linkage attack scenario leveraging publicly available auxiliary information acquired by the adversary. Our results on three open real-world datasets reveal that adding noise using the Laplace mechanism to an individual user's data record is a poor choice. It provides the highest regret for any noise level, relative to de-anonymization probability and the ADS metric. Instead, one should combine noise with appropriate aggregation strategies. For example, using averages from clusters of different sizes provides flexibility not achievable by varying the amount of noise alone. Generally, no single aggregation strategy can consistently achieve the optimum regret for a given desired level of privacy.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Pairwise-Proximal-Policy-Optimization-Harnessing-Relative-Feedback-for-LLM-Alignment"><a href="#Pairwise-Proximal-Policy-Optimization-Harnessing-Relative-Feedback-for-LLM-Alignment" class="headerlink" title="Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment"></a>Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00212">http://arxiv.org/abs/2310.00212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianhao Wu, Banghua Zhu, Ruoyu Zhang, Zhaojin Wen, Kannan Ramchandran, Jiantao Jiao</li>
<li>for: 这篇论文的目的是提出一个新的整合学习框架，以及一种新的政策梯度算法，以帮助大型自然语言模型（LLM）从人类反馈中学习出更好的行为。</li>
<li>methods: 这篇论文使用了一种新的整合学习框架，即“对照式反馈学习”（Pairwise Proximal Policy Optimization，P3O），这种方法可以直接从比较式反馈中学习政策。P3O 使用了一种新的政策梯度算法，它可以跳过 PPO 中的时间对照运算，并且可以更好地适应不同的对照式反馈。</li>
<li>results: 这篇论文的实验结果显示，P3O 可以与 PPO 相比，在 KL-Reward 贡献损失中实现更好的平衡，并且可以更好地与人类偏好相Alignment。此外，P3O 还可以在不同的对照式反馈中进行精确的优化。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) can acquire extensive world knowledge through pre-training on large corpora. However, due to exposure to low-quality data, LLMs may exhibit harmful behavior without aligning with human values. The dominant approach for steering LLMs towards beneficial behavior involves Reinforcement Learning with Human Feedback (RLHF), with Proximal Policy Optimization (PPO) serving as the default RL optimizer. Despite its effectiveness, PPO has limitations when optimizing rewards trained from comparison-based loss. Primarily, PPO is not invariant to equivalent reward functions containing identical preference information due to the need to calibrate the reward scale. Additionally, PPO's necessity for token-wise updates introduces complexity in both function approximation and algorithm design compared to trajectory-wise optimization. This paper proposes a new framework, reinforcement learning with relative feedback, and a novel trajectory-wise policy gradient algorithm, Pairwise Proximal Policy Optimization (P3O) that operates directly on comparative rewards. We show theoretically that P3O is invariant to equivalent rewards and avoids the complexity of PPO. Empirical evaluations demonstrate that P3O outperforms PPO in the KL-Reward trade-off and can align with human preferences as well as or better than prior methods. In summary, this work introduces a simpler yet effective approach for aligning LLMs to human preferences through relative feedback.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）可以通过预训练大量文本获得广泛的世界知识。然而，由于接触低质量数据，LLM可能会表现出不良行为不符合人类价值观。现行的方法是使用人类反馈的强化学习来引导LLM，其中Proximal Policy Optimization（PPO） serve as the default RL optimizer。尽管它有效，但PPO受到比较基于损失函数的限制。一、PPO不惟一地对等抽象的奖励函数进行匹配，需要调整奖励的权重缩放。二、PPO需要在单个字符级别进行更新，这会增加函数近似和算法设计中的复杂度。本文提出了一种新的框架——强化学习与相对反馈，以及一种新的追踪级别政策梯度算法——对比较政策优化（P3O）。我们证明了P3O对等抽象的奖励函数是惟一的，并且不需要单个字符级别的更新。实验证明，P3O在KL-奖励负担中比PPO表现更好，并且可以与人类偏好相似或更好地启合。总之，本文介绍了一种简洁又有效的方法，通过相对反馈来引导LLM符合人类价值观。
</details></li>
</ul>
<hr>
<h2 id="A-Prefrontal-Cortex-inspired-Architecture-for-Planning-in-Large-Language-Models"><a href="#A-Prefrontal-Cortex-inspired-Architecture-for-Planning-in-Large-Language-Models" class="headerlink" title="A Prefrontal Cortex-inspired Architecture for Planning in Large Language Models"></a>A Prefrontal Cortex-inspired Architecture for Planning in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00194">http://arxiv.org/abs/2310.00194</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taylor Webb, Shanka Subhra Mondal, Chi Wang, Brian Krabach, Ida Momennejad</li>
<li>for: 提高大语言模型（LLM）的计划能力，使其更能够完成多步骤的 reasoning 和目标导向的 плани化。</li>
<li>methods: 启取人类大脑的前前叶皮层（PFC）中的特殊化模块，如冲突监测、状态预测、状态评估、任务分解和任务协调等功能，并将其应用于 LLM 中。</li>
<li>results: 使用多个 LLM-based（GPT-4）模块的黑盒体系，可以在解决 graph traversal 和 Tower of Hanoi 等两个困难的计划任务时，取得显著的提高，比如零shot 提示或在Context learning 中。这些结果表明，从 cognitive neuroscience 中获得的知识可以改善 LLM 的计划能力。<details>
<summary>Abstract</summary>
Large language models (LLMs) demonstrate impressive performance on a wide variety of tasks, but they often struggle with tasks that require multi-step reasoning or goal-directed planning. To address this, we take inspiration from the human brain, in which planning is accomplished via the recurrent interaction of specialized modules in the prefrontal cortex (PFC). These modules perform functions such as conflict monitoring, state prediction, state evaluation, task decomposition, and task coordination. We find that LLMs are sometimes capable of carrying out these functions in isolation, but struggle to autonomously coordinate them in the service of a goal. Therefore, we propose a black box architecture with multiple LLM-based (GPT-4) modules. The architecture improves planning through the interaction of specialized PFC-inspired modules that break down a larger problem into multiple brief automated calls to the LLM. We evaluate the combined architecture on two challenging planning tasks -- graph traversal and Tower of Hanoi -- finding that it yields significant improvements over standard LLM methods (e.g., zero-shot prompting or in-context learning). These results demonstrate the benefit of utilizing knowledge from cognitive neuroscience to improve planning in LLMs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/30/cs.AI_2023_09_30/" data-id="clp869tri004nk58820rbey9a" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_09_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/30/cs.CL_2023_09_30/" class="article-date">
  <time datetime="2023-09-30T11:00:00.000Z" itemprop="datePublished">2023-09-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/30/cs.CL_2023_09_30/">cs.CL - 2023-09-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="It-HAS-to-be-Subjective-Human-Annotator-Simulation-via-Zero-shot-Density-Estimation"><a href="#It-HAS-to-be-Subjective-Human-Annotator-Simulation-via-Zero-shot-Density-Estimation" class="headerlink" title="It HAS to be Subjective: Human Annotator Simulation via Zero-shot Density Estimation"></a>It HAS to be Subjective: Human Annotator Simulation via Zero-shot Density Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00486">http://arxiv.org/abs/2310.00486</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/w-wu/has_cnf">https://github.com/w-wu/has_cnf</a></li>
<li>paper_authors: Wen Wu, Wenlin Chen, Chao Zhang, Philip C. Woodland</li>
<li>for: 用于取代人工评估，如数据标注和系统评估。</li>
<li>methods: 使用Zero-shot density estimation问题来模拟人类评估者的认知和行为，并提出了两种新的模型类型： conditional integer flows 和 conditional softmax flows，以处理 ordinal 和 categorical 类型的注解。</li>
<li>results: 在三个实际的人类评估任务上展示了superior的能力和效率，能够预测人类评估者的总行为，匹配人类注解的分布，并模拟人类评估者之间的不一致。<details>
<summary>Abstract</summary>
Human annotator simulation (HAS) serves as a cost-effective substitute for human evaluation such as data annotation and system assessment. Human perception and behaviour during human evaluation exhibit inherent variability due to diverse cognitive processes and subjective interpretations, which should be taken into account in modelling to better mimic the way people perceive and interact with the world. This paper introduces a novel meta-learning framework that treats HAS as a zero-shot density estimation problem, which incorporates human variability and allows for the efficient generation of human-like annotations for unlabelled test inputs. Under this framework, we propose two new model classes, conditional integer flows and conditional softmax flows, to account for ordinal and categorical annotations, respectively. The proposed method is evaluated on three real-world human evaluation tasks and shows superior capability and efficiency to predict the aggregated behaviours of human annotators, match the distribution of human annotations, and simulate the inter-annotator disagreements.
</details>
<details>
<summary>摘要</summary>
人工标注 simulate (HAS)  acted as a cost-effective substitute for human evaluation, such as data annotation and system assessment. Human perception and behavior during human evaluation exhibit inherent variability due to diverse cognitive processes and subjective interpretations, which should be taken into account in modeling to better mimic the way people perceive and interact with the world. This paper introduces a novel meta-learning framework that treats HAS as a zero-shot density estimation problem, which incorporates human variability and allows for the efficient generation of human-like annotations for unlabeled test inputs. Under this framework, we propose two new model classes, conditional integer flows and conditional softmax flows, to account for ordinal and categorical annotations, respectively. The proposed method is evaluated on three real-world human evaluation tasks and shows superior capability and efficiency to predict the aggregated behaviors of human annotators, match the distribution of human annotations, and simulate the inter-annotator disagreements.
</details></li>
</ul>
<hr>
<h2 id="Question-Answering-Model-for-Schizophrenia-Symptoms-and-Their-Impact-on-Daily-Life-using-Mental-Health-Forums-Data"><a href="#Question-Answering-Model-for-Schizophrenia-Symptoms-and-Their-Impact-on-Daily-Life-using-Mental-Health-Forums-Data" class="headerlink" title="Question-Answering Model for Schizophrenia Symptoms and Their Impact on Daily Life using Mental Health Forums Data"></a>Question-Answering Model for Schizophrenia Symptoms and Their Impact on Daily Life using Mental Health Forums Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00448">http://arxiv.org/abs/2310.00448</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ChristianInterno/Question-Answering-Model-for-Schizophrenia-Symptoms">https://github.com/ChristianInterno/Question-Answering-Model-for-Schizophrenia-Symptoms</a></li>
<li>paper_authors: Christian Internò, Eloisa Ambrosini</li>
<li>for: 本研究的目的是提出一种新的方法来建立医疗数据集和实现疾病领域的问答模型，以便对精神疾病的症状和生活影响进行分析。</li>
<li>methods: 本研究使用了一种新的方法，通过抓取“精神疾病”论坛中活跃用户的相关帖子，并对数据集进行预处理，将其转化为问答数据集。</li>
<li>results: 经过实验 validate，提出的方法可以获得一个准确的数据集，并且通过 fine-tuning BioBERT QA模型，实现了精神疾病领域的问答模型。该模型在 F1 分数上达到了 0.885，超过了当前领域的状态码模型。<details>
<summary>Abstract</summary>
In recent years, there is strong emphasis on mining medical data using machine learning techniques. A common problem is to obtain a noiseless set of textual documents, with a relevant content for the research question, and developing a Question Answering (QA) model for a specific medical field. The purpose of this paper is to present a new methodology for building a medical dataset and obtain a QA model for analysis of symptoms and impact on daily life for a specific disease domain. The ``Mental Health'' forum was used, a forum dedicated to people suffering from schizophrenia and different mental disorders. Relevant posts of active users, who regularly participate, were extrapolated providing a new method of obtaining low-bias content and without privacy issues. Furthermore, it is shown how to pre-process the dataset to convert it into a QA dataset. The Bidirectional Encoder Representations from Transformers (BERT), DistilBERT, RoBERTa, and BioBERT models were fine-tuned and evaluated via F1-Score, Exact Match, Precision and Recall. Accurate empirical experiments demonstrated the effectiveness of the proposed method for obtaining an accurate dataset for QA model implementation. By fine-tuning the BioBERT QA model, we achieved an F1 score of 0.885, showing a considerable improvement and outperforming the state-of-the-art model for mental disorders domain.
</details>
<details>
<summary>摘要</summary>
现在，有强烈的强调在医疗数据挖掘中使用机器学习技术。一个常见的问题是获取噪音少的文本文档，具有相关的内容，并开发一个问答（QA）模型 для特定的医疗领域。本文的目的是提出一种新的方法ology for 建立医疗数据集和获得一个QA模型，用于分析疾病和对日常生活的影响的分析。使用“精神健康”论坛，这是一个专门为患有分子难病和不同的精神障碍的人们而设立的论坛。我们从有活跃用户的相关帖子中抽取了有用的帖子，以提供一种新的方法，无需隐私问题。此外，我们还介绍了如何预处理数据，以将其转换为QA数据集。我们使用了BERT、DistilBERT、RoBERTa和BioBERT模型，并对其进行了微调和评估。我们通过F1分数、精确匹配、精确率和受损率进行了实际的实验，并证明了我们提出的方法的有效性。通过微调BioBERT QA模型，我们实现了F1分数0.885，表明我们的方法在精神疾病领域的QA模型实现了显著改进，并超越了现有的状态码模型。
</details></li>
</ul>
<hr>
<h2 id="The-Many-Voices-of-Duying-Revisiting-the-Disputed-Essays-Between-Lu-Xun-and-Zhou-Zuoren"><a href="#The-Many-Voices-of-Duying-Revisiting-the-Disputed-Essays-Between-Lu-Xun-and-Zhou-Zuoren" class="headerlink" title="The Many Voices of Duying: Revisiting the Disputed Essays Between Lu Xun and Zhou Zuoren"></a>The Many Voices of Duying: Revisiting the Disputed Essays Between Lu Xun and Zhou Zuoren</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01440">http://arxiv.org/abs/2310.01440</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://codeberg.org/haining/the_many_voices">https://codeberg.org/haining/the_many_voices</a></li>
<li>paper_authors: Xin Xie, Jiangqiong Li, Haining Wang</li>
<li>for: This research aims to revisit three disputed essays pseudonymously published by Lu Xun and Zhou Zuoren in 1912, using quantitative methods and stylometric analysis to investigate the authors’ respective writing styles and examine the essays’ authorship.</li>
<li>methods: The research employs an interpretable authorship attribution model and visual representations of essay features to facilitate a nuanced understanding of the brothers’ formative intellectual trajectories and their collaboration on these early works.</li>
<li>results: The findings suggest that ‘Looking at the Country of China’ was authored by Lu Xun, while ‘People of Yue, Forget Not Your Ancestors’ Instructions’ seems to be either predominantly authored or extensively revised by Lu Xun, with notable stylistic similarities to ‘Looking at the Land of Yue,’ which Zhou Zuoren recognized as his own but edited by Lu Xun. The third essay, ‘Where Has the Character of the Republic Gone?’, exhibits a ‘diluted’, mixed writing style, suggesting thorough collaboration between the brothers.<details>
<summary>Abstract</summary>
Lu Xun and Zhou Zuoren stand as two of the most influential writers in modern Chinese literature. Beyond their familial ties as brothers, they were also intimate collaborators during the nascent stages of their writing careers. This research employs quantitative methods to revisit three disputed essays pseudonymously published by the brothers in 1912. Our stylometric analysis uses an interpretable authorship attribution model to investigate the essays' authorship and examine the brothers' respective writing styles. Our findings suggest that 'Looking at the Country of China' was authored by Lu Xun. Moreover, 'People of Yue, Forget Not Your Ancestors' Instructions' seems to be either predominantly authored or extensively revised by Lu Xun given its notable stylistic similarities to 'Looking at the Land of Yue,' a piece Zhou Zuoren recognized as his own, but edited by Lu Xun. The third essay, 'Where Has the Character of the Republic Gone?,' exhibits a 'diluted', mixed writing style, suggesting thorough collaboration. We offer visual representations of essay features to facilitate a nuanced and intuitive understanding. We have uncovered evidence suggesting Lu Xun's covert engagement with social issues during his purported 'silent era' and provided insights into the brothers' formative intellectual trajectories.
</details>
<details>
<summary>摘要</summary>
吕迅和周作人是现代中国文学中最有影响力的两位作家。除了他们的 familial ties 外，他们也是在他们写作早期的密切合作者。这项研究使用量化方法回到1912年 pseudonymously 发表的三篇文章中，进行了作者属性分析。我们的样本分析发现，《看中国的国情》是吕迅所作的，而《父母教诲》似乎是吕迅或者大量修改了，因为它的作家风格与吕迅认可的《看粤eland的情》具有显著的相似性。第三篇文章《共和国的人物消失了吗？》显示出了混合的写作风格，表明了两人的合作。我们提供了文章特征的视觉表示，以便更好地理解。我们发现吕迅在报道的“幽默时期”中仍然做出了 covert 的社会问题干预，并为吕迅和周作人的形成 интеллектуаль 轨迹提供了新的视角。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Representation-Generalization-in-Authorship-Identification"><a href="#Enhancing-Representation-Generalization-in-Authorship-Identification" class="headerlink" title="Enhancing Representation Generalization in Authorship Identification"></a>Enhancing Representation Generalization in Authorship Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00436">http://arxiv.org/abs/2310.00436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haining Wang</li>
<li>for: 这个论文的目的是提高作者识别技术的普适性，尤其是在域外场景下。</li>
<li>methods: 该论文使用了词语重复率、函数词和深度学习模型等方法来提高作者识别的普适性。</li>
<li>results: 该论文的结果表明，选择合适的语言特征对于作者识别具有重要性，特别是在域外场景下。同时，使用深度学习模型可以提高作者识别的普适性。<details>
<summary>Abstract</summary>
Authorship identification ascertains the authorship of texts whose origins remain undisclosed. That authorship identification techniques work as reliably as they do has been attributed to the fact that authorial style is properly captured and represented. Although modern authorship identification methods have evolved significantly over the years and have proven effective in distinguishing authorial styles, the generalization of stylistic features across domains has not been systematically reviewed. The presented work addresses the challenge of enhancing the generalization of stylistic representations in authorship identification, particularly when there are discrepancies between training and testing samples. A comprehensive review of empirical studies was conducted, focusing on various stylistic features and their effectiveness in representing an author's style. The influencing factors such as topic, genre, and register on writing style were also explored, along with strategies to mitigate their impact. While some stylistic features, like character n-grams and function words, have proven to be robust and discriminative, others, such as content words, can introduce biases and hinder cross-domain generalization. Representations learned using deep learning models, especially those incorporating character n-grams and syntactic information, show promise in enhancing representation generalization. The findings underscore the importance of selecting appropriate stylistic features for authorship identification, especially in cross-domain scenarios. The recognition of the strengths and weaknesses of various linguistic features paves the way for more accurate authorship identification in diverse contexts.
</details>
<details>
<summary>摘要</summary>
我们对多种实验研究进行了全面的审查，包括不同的风格特征和它们在作者风格表示中的效果。我们还研究了主题、类型和注册等因素对写作风格的影响，以及如何减轻这些因素的影响。结果显示，一些风格特征，如字符n-gram和语法信息，可以提供强大和特征的表示，而其他些风格特征，如内容词，可能会引入偏见并降低跨领域泛化。深度学习模型，特别是包含字符n-gram和语法信息的模型，显示了提高表示泛化的潜力。这些发现强调了选择适当的风格特征对作者识别非常重要，特别是在跨领域场景下。通过了解不同语言特征的优劣点，我们可以更准确地确定文本的作者，并在多种不同的情况下进行更好的作者识别。
</details></li>
</ul>
<hr>
<h2 id="Open-Domain-Dialogue-Quality-Evaluation-Deriving-Nugget-level-Scores-from-Turn-level-Scores"><a href="#Open-Domain-Dialogue-Quality-Evaluation-Deriving-Nugget-level-Scores-from-Turn-level-Scores" class="headerlink" title="Open-Domain Dialogue Quality Evaluation: Deriving Nugget-level Scores from Turn-level Scores"></a>Open-Domain Dialogue Quality Evaluation: Deriving Nugget-level Scores from Turn-level Scores</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00410">http://arxiv.org/abs/2310.00410</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rikiyat/nugget-level-evaluation">https://github.com/rikiyat/nugget-level-evaluation</a></li>
<li>paper_authors: Rikiya Takehi, Akihisa Watanabe, Tetsuya Sakai</li>
<li>for: 本研究旨在提高对话系统的评估，以帮助改进对话系统的表现。</li>
<li>methods: 本研究提议使用套件分解法，将对话转换成多个套件，然后通过现有的转换级评估系统进行评估。</li>
<li>results: 经过实验，该方法可以帮助找到对话转换中的具体问题，从而改进对话系统的表现。<details>
<summary>Abstract</summary>
Existing dialogue quality evaluation systems can return a score for a given system turn from a particular viewpoint, e.g., engagingness. However, to improve dialogue systems by locating exactly where in a system turn potential problems lie, a more fine-grained evaluation may be necessary. We therefore propose an evaluation approach where a turn is decomposed into nuggets (i.e., expressions associated with a dialogue act), and nugget-level evaluation is enabled by leveraging an existing turn-level evaluation system. We demonstrate the potential effectiveness of our evaluation method through a case study.
</details>
<details>
<summary>摘要</summary>
现有的对话质量评估系统可以返回一个给定系统转帧的分数，例如吸引力。然而，要改进对话系统，可能需要更细化的评估方法。我们因此提议一种评估方法，即将转帧 decomposed into 块 (即对话动作相关的表达)，并使用现有的转帧级别评估系统来启用块级别评估。我们通过案例研究示出了我们的评估方法的潜在效果。
</details></li>
</ul>
<hr>
<h2 id="AutomaTikZ-Text-Guided-Synthesis-of-Scientific-Vector-Graphics-with-TikZ"><a href="#AutomaTikZ-Text-Guided-Synthesis-of-Scientific-Vector-Graphics-with-TikZ" class="headerlink" title="AutomaTikZ: Text-Guided Synthesis of Scientific Vector Graphics with TikZ"></a>AutomaTikZ: Text-Guided Synthesis of Scientific Vector Graphics with TikZ</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00367">http://arxiv.org/abs/2310.00367</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/potamides/automatikz">https://github.com/potamides/automatikz</a></li>
<li>paper_authors: Jonas Belouadi, Anne Lauscher, Steffen Eger</li>
<li>for: 该论文旨在使用 TikZ 抽象图形语言生成科学图表，以便使用任何大型语言模型进行条件语言模型化。</li>
<li>methods: 该论文提出使用 TikZ 作为科学图表的中间表示，并使用 LLaMA 和 CLiMA 两种模型进行条件语言模型化。</li>
<li>results: 对于人工和自动评估，CLiMA 和 LLaMA 都能够超越商业 GPT-4 和 Claude 2 模型，在人工创建的图表的相似性方面获得更高的分数。此外，CLiMA 还能够改善文本-图像对齐。<details>
<summary>Abstract</summary>
Generating bitmap graphics from text has gained considerable attention, yet for scientific figures, vector graphics are often preferred. Given that vector graphics are typically encoded using low-level graphics primitives, generating them directly is difficult. To address this, we propose the use of TikZ, a well-known abstract graphics language that can be compiled to vector graphics, as an intermediate representation of scientific figures. TikZ offers human-oriented, high-level commands, thereby facilitating conditional language modeling with any large language model. To this end, we introduce DaTikZ the first large-scale TikZ dataset, consisting of 120k TikZ drawings aligned with captions. We fine-tune LLaMA on DaTikZ, as well as our new model CLiMA, which augments LLaMA with multimodal CLIP embeddings. In both human and automatic evaluation, CLiMA and LLaMA outperform commercial GPT-4 and Claude 2 in terms of similarity to human-created figures, with CLiMA additionally improving text-image alignment. Our detailed analysis shows that all models generalize well and are not susceptible to memorization. GPT-4 and Claude 2, however, tend to generate more simplistic figures compared to both humans and our models. We make our framework, AutomaTikZ, along with model weights and datasets, publicly available.
</details>
<details>
<summary>摘要</summary>
科学图表生成已经受到了广泛关注，但是在科学图表上，vector图形通常被首选。这是因为vector图形通常使用低级图形元素编码，直接生成它们是困难的。为解决这个问题，我们提议使用TikZ，一种广泛使用的抽象图形语言，作为科学图表的中间表示。TikZ提供人类 oriented 高级命令，因此可以使用任何大型语言模型进行条件语言模型化。为此，我们引入了DaTikZ，我们的首个大规模TikZ数据集，包含120k个TikZ绘制和关联的描述。我们在DaTikZ上练习LLaMA，以及我们的新模型CLiMA，它在多Modal CLIP嵌入下进行了增强。在人工和自动评估中，CLiMA和LLaMA都超过了商业GPT-4和Claude 2在人类创建图表的相似性上，并且CLiMA还改善了文本-图像对齐。我们的详细分析表明所有模型都能够通过并不易于记忆。然而，GPT-4和Claude 2倾向于生成更简单的图表，与人类和我们的模型相比。我们在AutomaTikZ框架，以及模型和数据集，公开提供。
</details></li>
</ul>
<hr>
<h2 id="Gaze-Driven-Sentence-Simplification-for-Language-Learners-Enhancing-Comprehension-and-Readability"><a href="#Gaze-Driven-Sentence-Simplification-for-Language-Learners-Enhancing-Comprehension-and-Readability" class="headerlink" title="Gaze-Driven Sentence Simplification for Language Learners: Enhancing Comprehension and Readability"></a>Gaze-Driven Sentence Simplification for Language Learners: Enhancing Comprehension and Readability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00355">http://arxiv.org/abs/2310.00355</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taichi Higasa, Keitaro Tanaka, Qi Feng, Shigeo Morishima</li>
<li>for: 提高英语学习者的阅读理解能力</li>
<li>methods: 使用机器学习模型与个体学习者的眼动特征相结合，对句子进行简化，以提高阅读理解</li>
<li>results: 实验结果表明，该系统可以准确估计句子级别的理解程度，并且通过使用GPT-3.5进行简化，提高了文本的可读性和个体单词难度<details>
<summary>Abstract</summary>
Language learners should regularly engage in reading challenging materials as part of their study routine. Nevertheless, constantly referring to dictionaries is time-consuming and distracting. This paper presents a novel gaze-driven sentence simplification system designed to enhance reading comprehension while maintaining their focus on the content. Our system incorporates machine learning models tailored to individual learners, combining eye gaze features and linguistic features to assess sentence comprehension. When the system identifies comprehension difficulties, it provides simplified versions by replacing complex vocabulary and grammar with simpler alternatives via GPT-3.5. We conducted an experiment with 19 English learners, collecting data on their eye movements while reading English text. The results demonstrated that our system is capable of accurately estimating sentence-level comprehension. Additionally, we found that GPT-3.5 simplification improved readability in terms of traditional readability metrics and individual word difficulty, paraphrasing across different linguistic levels.
</details>
<details>
<summary>摘要</summary>
学习者应 régulièrement 阅读具有挑战性的材料，以提高阅读理解能力。然而，不断地查询词典是时间consuming 和distracting。这篇论文提出了一种基于eye gaze的句子简化系统，用于提高阅读理解而不间断注意力。我们的系统通过对个人学习者的eye gaze特征和语言特征进行机器学习模型，以评估句子理解程度。当系统认为理解有difficulties时，它会提供简化版本，替换复杂词汇和语法 With GPT-3.5。我们进行了19名英语学习者的实验，收集了他们的eye movement数据 while reading English text。结果表明，我们的系统可以准确地估计句子级别的理解程度。此外，我们发现GPT-3.5简化提高了文本的可读性，包括传统的可读性指标和个别词Difficulty，以及各种语言水平的重叠。
</details></li>
</ul>
<hr>
<h2 id="Red-Teaming-Game-A-Game-Theoretic-Framework-for-Red-Teaming-Language-Models"><a href="#Red-Teaming-Game-A-Game-Theoretic-Framework-for-Red-Teaming-Language-Models" class="headerlink" title="Red Teaming Game: A Game-Theoretic Framework for Red Teaming Language Models"></a>Red Teaming Game: A Game-Theoretic Framework for Red Teaming Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00322">http://arxiv.org/abs/2310.00322</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengdong Ma, Ziran Yang, Minquan Gao, Hai Ci, Jun Gao, Xuehai Pan, Yaodong Yang</li>
<li>for: 这个论文的目的是提出一个测试大型自然语言模型（LLM）的测试框架，以确保 LLM 的输出和人类价值之间的一致性。</li>
<li>methods: 这个论文使用了游戏理论的框架，叫做 Red-teaming Game (RTG)，并提出了一个自动化的红队设计技术，叫做 Gamified Red-teaming Solver (GRTS)，以解决 RTG 中的攻击和防御互动问题。</li>
<li>results: 这个论文的实验结果显示，GRTS 能够自动发现多种攻击策略，并对 LLM 的安全性进行改善，比起现有的规律性红队设计方法更好。<details>
<summary>Abstract</summary>
Deployable Large Language Models (LLMs) must conform to the criterion of helpfulness and harmlessness, thereby achieving consistency between LLMs outputs and human values. Red-teaming techniques constitute a critical way towards this criterion. Existing work rely solely on manual red team designs and heuristic adversarial prompts for vulnerability detection and optimization. These approaches lack rigorous mathematical formulation, thus limiting the exploration of diverse attack strategy within quantifiable measure and optimization of LLMs under convergence guarantees. In this paper, we present Red-teaming Game (RTG), a general game-theoretic framework without manual annotation. RTG is designed for analyzing the multi-turn attack and defense interactions between Red-team language Models (RLMs) and Blue-team Language Model (BLM). Within the RTG, we propose Gamified Red-teaming Solver (GRTS) with diversity measure of the semantic space. GRTS is an automated red teaming technique to solve RTG towards Nash equilibrium through meta-game analysis, which corresponds to the theoretically guaranteed optimization direction of both RLMs and BLM. Empirical results in multi-turn attacks with RLMs show that GRTS autonomously discovered diverse attack strategies and effectively improved security of LLMs, outperforming existing heuristic red-team designs. Overall, RTG has established a foundational framework for red teaming tasks and constructed a new scalable oversight technique for alignment.
</details>
<details>
<summary>摘要</summary>
deployable 大语言模型（LLM）必须遵循帮助和无害性的准则，以确保 LLM 的输出与人类价值之间的一致性。红队技术是一种关键的方法，可以帮助实现这一准则。现有的工作仅仅采用手动设计的红队和启发式对抗提示来检测漏洞和优化 LLM。这些方法缺乏准确的数学表述，因此限制了对多种攻击策略的可靠探索和 LLM 的优化 beneath convergence guarantees。在这篇论文中，我们提出了红队游戏（RTG），一种普适的游戏理论基础。RTG 是用于分析红队语言模型（RLM）和蓝队语言模型（BLM）之间的多轮攻击和防御互动的框架。在 RTG 中，我们提出了游戏化红队解决方案（GRTS），具有 semantic space 的多样性度量。GRTS 是一种自动化的红队技术，通过 meta-game 分析解决 RTG，对 RLMs 和 BLMs 都有 theoretically guaranteed optimization direction。实验结果表明，GRTS 可以自动找到多样化的攻击策略，提高 LLMs 的安全性，比传统的启发式红队设计更高效。总的来说，RTG 建立了红队任务的基础框架，并构建了一种新的可扩展的监管技术，以便对适应进行Alignment。
</details></li>
</ul>
<hr>
<h2 id="In-Context-Learning-in-Large-Language-Models-A-Neuroscience-inspired-Analysis-of-Representations"><a href="#In-Context-Learning-in-Large-Language-Models-A-Neuroscience-inspired-Analysis-of-Representations" class="headerlink" title="In-Context Learning in Large Language Models: A Neuroscience-inspired Analysis of Representations"></a>In-Context Learning in Large Language Models: A Neuroscience-inspired Analysis of Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00313">http://arxiv.org/abs/2310.00313</a></li>
<li>repo_url: None</li>
<li>paper_authors: Safoora Yousefi, Leo Betthauser, Hosein Hasanbeig, Akanksha Saran, Raphaël Millière, Ida Momennejad</li>
<li>for:  investigate the mechanisms behind the improvement of large language models (LLMs) through in-context learning (ICL)</li>
<li>methods: employ neuroscience-inspired techniques such as representational similarity analysis (RSA) and propose novel methods for parameterized probing and attention ratio analysis (ARA)</li>
<li>results: found a meaningful correlation between changes in both embeddings and attention representations with improvements in behavioral performance after ICL, offering valuable tools and insights for future research and practical applications.Here’s the full text in Simplified Chinese:</li>
<li>for: 研究大语言模型（LLM）通过受 Context Learning（ICL）改进的机制</li>
<li>methods: 使用 neuroscience-inspired 技术，如表达相似性分析（RSA），并提出了参数化探测和注意力比率分析（ARA）</li>
<li>results: 发现在ICL后， embedding 和注意力表示变化与行为性能改进存在明确的相关性，为未来研究和实际应用提供有价值的工具和洞察。<details>
<summary>Abstract</summary>
Large language models (LLMs) exhibit remarkable performance improvement through in-context learning (ICL) by leveraging task-specific examples in the input. However, the mechanisms behind this improvement remain elusive. In this work, we investigate embeddings and attention representations in Llama-2 70B and Vicuna 13B. Specifically, we study how embeddings and attention change after in-context-learning, and how these changes mediate improvement in behavior. We employ neuroscience-inspired techniques, such as representational similarity analysis (RSA), and propose novel methods for parameterized probing and attention ratio analysis (ARA, measuring the ratio of attention to relevant vs. irrelevant information). We designed three tasks with a priori relationships among their conditions: reading comprehension, linear regression, and adversarial prompt injection. We formed hypotheses about expected similarities in task representations to investigate latent changes in embeddings and attention. Our analyses revealed a meaningful correlation between changes in both embeddings and attention representations with improvements in behavioral performance after ICL. This empirical framework empowers a nuanced understanding of how latent representations affect LLM behavior with and without ICL, offering valuable tools and insights for future research and practical applications.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Large language models" (LLMs) is translated as "大语言模型" (dà yǔ yán módel).* "In-context learning" (ICL) is translated as "在上下文中学习" (zài shàng xiào yì xué xí).* "Embeddings" is translated as "嵌入" (fù rù).* "Attention" is translated as "注意" (zhù yì).* "Representational similarity analysis" (RSA) is translated as "表示相似性分析" (biǎo xiǎng sì xìng bù yì).* "Parameterized probing" is translated as "参数化探测" (cèsuǒ huì yì).* "Attention ratio analysis" (ARA) is translated as "注意率分析" (zhù yì xìng bù yì).* "Reading comprehension" is translated as "阅读理解" (dòng dú lǐ jiě).* "Linear regression" is translated as "直线回归" (zhí xiàn huí qù).* "Adversarial prompt injection" is translated as "敌对提示注入" (dí duì tím shì zhù yì).
</details></li>
</ul>
<hr>
<h2 id="Towards-LLM-based-Fact-Verification-on-News-Claims-with-a-Hierarchical-Step-by-Step-Prompting-Method"><a href="#Towards-LLM-based-Fact-Verification-on-News-Claims-with-a-Hierarchical-Step-by-Step-Prompting-Method" class="headerlink" title="Towards LLM-based Fact Verification on News Claims with a Hierarchical Step-by-Step Prompting Method"></a>Towards LLM-based Fact Verification on News Claims with a Hierarchical Step-by-Step Prompting Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00305">http://arxiv.org/abs/2310.00305</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jadecurl/hiss">https://github.com/jadecurl/hiss</a></li>
<li>paper_authors: Xuan Zhang, Wei Gao</li>
<li>for: 新闻声明验证任务中使用大型预训练语言模型 (LLMs)</li>
<li>methods: 使用协调学习 (ICL) 方法，并引入层次步骤（HiSS）提问方法以分解声明并逐步验证每个子声明</li>
<li>results: 在两个公共谣言数据集上，HiSS提问方法超过了现有的完全监督方法和强几架 ICL-enabled 基eline<details>
<summary>Abstract</summary>
While large pre-trained language models (LLMs) have shown their impressive capabilities in various NLP tasks, they are still under-explored in the misinformation domain. In this paper, we examine LLMs with in-context learning (ICL) for news claim verification, and find that only with 4-shot demonstration examples, the performance of several prompting methods can be comparable with previous supervised models. To further boost performance, we introduce a Hierarchical Step-by-Step (HiSS) prompting method which directs LLMs to separate a claim into several subclaims and then verify each of them via multiple questions-answering steps progressively. Experiment results on two public misinformation datasets show that HiSS prompting outperforms state-of-the-art fully-supervised approach and strong few-shot ICL-enabled baselines.
</details>
<details>
<summary>摘要</summary>
大型预训语言模型（LLM）在不同的自然语言处理任务中已经展示了它们的卓越能力，但它们在假信息领域还尚未得到充分的探索。在这篇论文中，我们对新闻声明验证 зада务使用 LLM 进行培 обу，并发现只需要4个示例示例，可以使得许多提示方法的性能与之前的监督模型相当。为了进一步提高性能，我们介绍了层次步骤进行（HiSS）提示方法，该方法将声明分解成多个子声明，然后通过多个问题回答步骤进行逐步验证。实验结果表明，HiSS 提示方法在两个公共的假信息 datasets 上表现出色，超过了当前的完全监督方法和强几步 ICL-enabled 基elines。
</details></li>
</ul>
<hr>
<h2 id="RelBERT-Embedding-Relations-with-Language-Models"><a href="#RelBERT-Embedding-Relations-with-Language-Models" class="headerlink" title="RelBERT: Embedding Relations with Language Models"></a>RelBERT: Embedding Relations with Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00299">http://arxiv.org/abs/2310.00299</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/asahi417/relbert">https://github.com/asahi417/relbert</a></li>
<li>paper_authors: Asahi Ushio, Jose Camacho-Collados, Steven Schockaert</li>
<li>for: 提供一种使用masked语言模型来提取关系嵌入的方法，以取代知识图和大型自然语言模型。</li>
<li>methods: 使用RoBERTa模型进行微调，只需要一小Amount的训练数据，可以capture关系相似性在非常细化的方式。</li>
<li>results: RelBERT模型可以模型到训练数据之外的关系，例如名称实体之间的关系，并且可以认osciLLMs and GPT-based models。<details>
<summary>Abstract</summary>
Many applications need access to background knowledge about how different concepts and entities are related. Although Knowledge Graphs (KG) and Large Language Models (LLM) can address this need to some extent, KGs are inevitably incomplete and their relational schema is often too coarse-grained, while LLMs are inefficient and difficult to control. As an alternative, we propose to extract relation embeddings from relatively small language models. In particular, we show that masked language models such as RoBERTa can be straightforwardly fine-tuned for this purpose, using only a small amount of training data. The resulting model, which we call RelBERT, captures relational similarity in a surprisingly fine-grained way, allowing us to set a new state-of-the-art in analogy benchmarks. Crucially, RelBERT is capable of modelling relations that go well beyond what the model has seen during training. For instance, we obtained strong results on relations between named entities with a model that was only trained on lexical relations between concepts, and we observed that RelBERT can recognise morphological analogies despite not being trained on such examples. Overall, we find that RelBERT significantly outperforms strategies based on prompting language models that are several orders of magnitude larger, including recent GPT-based models and open source models.
</details>
<details>
<summary>摘要</summary>
Note:* "Knowledge Graph" (知识图)* "Large Language Model" (大语言模型)* "Relation Embeddings" (关系嵌入)* "Masked Language Model" (伪语言模型)* "RoBERTa" (RoBERTa)* "RelBERT" (RelBERT)
</details></li>
</ul>
<hr>
<h2 id="Understanding-In-Context-Learning-from-Repetitions"><a href="#Understanding-In-Context-Learning-from-Repetitions" class="headerlink" title="Understanding In-Context Learning from Repetitions"></a>Understanding In-Context Learning from Repetitions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00297">http://arxiv.org/abs/2310.00297</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/elliottyan/understand-icl-from-repetition">https://github.com/elliottyan/understand-icl-from-repetition</a></li>
<li>paper_authors: Jianhao Yan, Jin Xu, Chiyu Song, Chenming Wu, Yafu Li, Yue Zhang</li>
<li>for: 这篇论文研究了大语言模型中的协同学习机制。</li>
<li>methods: 该研究使用了表面重复来研究文本生成中的协同学习机制，并证明了表面特征强化两个符号之间的关系。</li>
<li>results: 该研究发现了表面特征在文本生成中的双重作用，并解释了协同学习的内在机制和其可能的局限性。<details>
<summary>Abstract</summary>
This paper explores the elusive mechanism underpinning in-context learning in Large Language Models (LLMs). Our work provides a novel perspective by examining in-context learning via the lens of surface repetitions. We quantitatively investigate the role of surface features in text generation, and empirically establish the existence of \emph{token co-occurrence reinforcement}, a principle that strengthens the relationship between two tokens based on their contextual co-occurrences. By investigating the dual impacts of these features, our research illuminates the internal workings of in-context learning and expounds on the reasons for its failures. This paper provides an essential contribution to the understanding of in-context learning and its potential limitations, providing a fresh perspective on this exciting capability.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="AfriSpeech-200-Pan-African-Accented-Speech-Dataset-for-Clinical-and-General-Domain-ASR"><a href="#AfriSpeech-200-Pan-African-Accented-Speech-Dataset-for-Clinical-and-General-Domain-ASR" class="headerlink" title="AfriSpeech-200: Pan-African Accented Speech Dataset for Clinical and General Domain ASR"></a>AfriSpeech-200: Pan-African Accented Speech Dataset for Clinical and General Domain ASR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00274">http://arxiv.org/abs/2310.00274</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tobi Olatunji, Tejumade Afonja, Aditya Yadavalli, Chris Chinenye Emezue, Sahib Singh, Bonaventure F. P. Dossou, Joanne Osuchukwu, Salomey Osei, Atnafu Lambebo Tonja, Naome Etori, Clinton Mbataku</li>
<li>For: The paper aims to address the lack of productivity tools for overworked clinicians in Africa, where the doctor-to-patient ratio is very low.* Methods: The paper uses clinical automatic speech recognition (ASR) systems, which are mature and ubiquitous in developed nations, but have not been widely available for clinicians in Africa. The authors also release a new dataset called AfriSpeech, which includes 200 hours of Pan-African English speech from 2,463 unique speakers across 120 indigenous accents from 13 countries.* Results: The authors release pre-trained models with state-of-the-art (SOTA) performance on the AfriSpeech benchmark, which can be used to improve the accuracy of clinical ASR systems for African accents.Here are the three points in Simplified Chinese:* For: 该论文目的是为非洲的医生缺乏产品力工具，非洲医生比例非常低。* Methods: 论文使用了临床自动语音识别（ASR）系统，这些系统在发达国家是成熔的，但在非洲尚未广泛应用。作者们还发布了新的数据集called AfriSpeech，包括200小时的非洲英语语音，来自2,463名唯一的说话者，来自13个国家的120种本地口音。* Results: 作者们发布了基于AfriSpeechbenchmark的预训练模型，其中的语音识别性能达到了当前最佳状态（SOTA）。<details>
<summary>Abstract</summary>
Africa has a very low doctor-to-patient ratio. At very busy clinics, doctors could see 30+ patients per day -- a heavy patient burden compared with developed countries -- but productivity tools such as clinical automatic speech recognition (ASR) are lacking for these overworked clinicians. However, clinical ASR is mature, even ubiquitous, in developed nations, and clinician-reported performance of commercial clinical ASR systems is generally satisfactory. Furthermore, the recent performance of general domain ASR is approaching human accuracy. However, several gaps exist. Several publications have highlighted racial bias with speech-to-text algorithms and performance on minority accents lags significantly. To our knowledge, there is no publicly available research or benchmark on accented African clinical ASR, and speech data is non-existent for the majority of African accents. We release AfriSpeech, 200hrs of Pan-African English speech, 67,577 clips from 2,463 unique speakers across 120 indigenous accents from 13 countries for clinical and general domain ASR, a benchmark test set, with publicly available pre-trained models with SOTA performance on the AfriSpeech benchmark.
</details>
<details>
<summary>摘要</summary>
非洲医生与病人比例非常低。在非常忙的医院，医生可以每天看到30多名患者，这对于发达国家来说是一个重重的患者负担，但是医疗机器人（ASR）的产业化工具缺乏。然而，在发达国家，临床ASR已经成熟，甚至 ubique，并且医生报告的商业临床ASR系统的性能一般满意。此外，最近的通用领域ASR性能已经接近人类水平。然而，有几个差距。一些发表文章指出了语言算法中的种族偏见，并且少数语言口音的性能明显落后。根据我们所知，没有公开的研究或标准测试数据在非洲语音领域的临床ASR，并且非洲语音数据非常罕见。我们释放了AfriSpeech，200小时的非洲英语语音数据，67,577个clip从2,463名唯一的说话者中，来自120个本地口音，13个国家的医疗和通用领域ASR测试集，以及公共可用的预训练模型，与AfriSpeech测试集的最新表现。
</details></li>
</ul>
<hr>
<h2 id="AutoHall-Automated-Hallucination-Dataset-Generation-for-Large-Language-Models"><a href="#AutoHall-Automated-Hallucination-Dataset-Generation-for-Large-Language-Models" class="headerlink" title="AutoHall: Automated Hallucination Dataset Generation for Large Language Models"></a>AutoHall: Automated Hallucination Dataset Generation for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00259">http://arxiv.org/abs/2310.00259</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zouying Cao, Yifei Yang, Hai Zhao</li>
<li>for: This paper is written for detecting non-factual or hallucinatory content generated by large language models (LLMs).</li>
<li>methods: The paper proposes a method for automatically constructing model-specific hallucination datasets based on existing fact-checking datasets, called AutoHall. Additionally, the paper proposes a zero-resource and black-box hallucination detection method based on self-contradiction.</li>
<li>results: The paper achieves superior hallucination detection performance compared to extant baselines, and reveals variations in hallucination proportions and types among different models.Here’s the information in Simplified Chinese text:</li>
<li>for: 这篇论文是为探测大语言模型（LLMs）生成的非事实或幻想内容。</li>
<li>methods: 论文提出了一种自动构建模型特定幻想集的方法，基于现有的事实检查集，称为AutoHall。此外，论文还提出了一种零资源和黑盒子幻想检测方法，基于自相矛盾。</li>
<li>results: 论文在现有基准下 achieve 了超过基准的幻想检测性能，并发现不同模型中幻想的比例和类型存在差异。<details>
<summary>Abstract</summary>
While Large language models (LLMs) have garnered widespread applications across various domains due to their powerful language understanding and generation capabilities, the detection of non-factual or hallucinatory content generated by LLMs remains scarce. Currently, one significant challenge in hallucination detection is the laborious task of time-consuming and expensive manual annotation of the hallucinatory generation. To address this issue, this paper first introduces a method for automatically constructing model-specific hallucination datasets based on existing fact-checking datasets called AutoHall. Furthermore, we propose a zero-resource and black-box hallucination detection method based on self-contradiction. We conduct experiments towards prevalent open-/closed-source LLMs, achieving superior hallucination detection performance compared to extant baselines. Moreover, our experiments reveal variations in hallucination proportions and types among different models.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在不同领域的应用广泛，主要是因为它们具有强大的语言理解和生成能力。然而，检测 LLM 生成的非事实或幻见内容仍然是一个罕见的问题。现在，一个主要的挑战是手动标注幻见生成的时间和成本很高。为解决这个问题，本文首先提出了一种自动构建基于现有真假检查集的模型特定幻见数据集的方法，称为AutoHall。此外，我们还提出了一种零资源和黑盒子幻见检测方法，基于自相矛盾。我们对广泛存在的开源/关闭源 LLM 进行了实验，并 achievement 超过现有基准。此外，我们的实验还发现了不同模型中幻见的порпор额和类型存在差异。
</details></li>
</ul>
<hr>
<h2 id="SLM-Bridge-the-thin-gap-between-speech-and-text-foundation-models"><a href="#SLM-Bridge-the-thin-gap-between-speech-and-text-foundation-models" class="headerlink" title="SLM: Bridge the thin gap between speech and text foundation models"></a>SLM: Bridge the thin gap between speech and text foundation models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00230">http://arxiv.org/abs/2310.00230</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingqiu Wang, Wei Han, Izhak Shafran, Zelin Wu, Chung-Cheng Chiu, Yuan Cao, Yongqiang Wang, Nanxin Chen, Yu Zhang, Hagen Soltau, Paul Rubenstein, Lukas Zilka, Dian Yu, Zhong Meng, Golan Pundak, Nikhil Siddhartha, Johan Schalkwyk, Yonghui Wu</li>
<li>For: The paper is written for the task of speech and language modeling, with a focus on multitask, multilingual, and dual-modal models.* Methods: The paper uses pretrained foundational speech and language models, and trains a simple adapter with just 1% of the foundation models’ parameters to adapt the model to new tasks.* Results: The paper demonstrates strong performance on conventional tasks such as speech recognition and speech translation, and introduces the novel capability of zero-shot instruction-following for more diverse tasks such as contextual biasing ASR, dialog generation, speech continuation, and question answering.<details>
<summary>Abstract</summary>
We present a joint Speech and Language Model (SLM), a multitask, multilingual, and dual-modal model that takes advantage of pretrained foundational speech and language models. SLM freezes the pretrained foundation models to maximally preserves their capabilities, and only trains a simple adapter with just 1\% (156M) of the foundation models' parameters. This adaptation not only leads SLM to achieve strong performance on conventional tasks such as speech recognition (ASR) and speech translation (AST), but also introduces the novel capability of zero-shot instruction-following for more diverse tasks: given a speech input and a text instruction, SLM is able to perform unseen generation tasks including contextual biasing ASR using real-time context, dialog generation, speech continuation, and question answering, etc. Our approach demonstrates that the representational gap between pretrained speech and language models might be narrower than one would expect, and can be bridged by a simple adaptation mechanism. As a result, SLM is not only efficient to train, but also inherits strong capabilities already acquired in foundation models of different modalities.
</details>
<details>
<summary>摘要</summary>
我们提出了一个共同语音和语言模型（SLM），这是一种多任务、多语言、双modal模型，它利用预训练的基础语音和语言模型。SLM冻结了基础模型的预训练，以便保持其能力的最大化，并只训练一个简单的适应器，占基础模型参数的1%（156M）。这种适应不仅使SLM在传统任务such as语音识别（ASR）和语音翻译（AST）中达到了强大的表现，而且引入了无需训练的零shot指令遵循能力，包括基于实时上下文的语音识别、对话生成、语音续写和问答等多种不同任务。我们的方法表明，预训练的语音和语言模型之间的表示差可能比一 might expect更窄，并且可以通过简单的适应机制来bridged。因此，SLM不仅轻松训练，而且继承了不同模式的基础模型已经获得的强大能力。
</details></li>
</ul>
<hr>
<h2 id="Detecting-Unseen-Multiword-Expressions-in-American-Sign-Language"><a href="#Detecting-Unseen-Multiword-Expressions-in-American-Sign-Language" class="headerlink" title="Detecting Unseen Multiword Expressions in American Sign Language"></a>Detecting Unseen Multiword Expressions in American Sign Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00207">http://arxiv.org/abs/2310.00207</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lee Kezar, Aryan Shukla</li>
<li>for: 测试多个词语的复合性，以应用于美洲手语翻译。</li>
<li>methods: 使用GloVe词嵌入进行预测，以决定lexeme是否组成多个词语。</li>
<li>results: word embeddings可以实现非常高的准确率来探测非结构化词语。<details>
<summary>Abstract</summary>
Multiword expressions present unique challenges in many translation tasks. In an attempt to ultimately apply a multiword expression detection system to the translation of American Sign Language, we built and tested two systems that apply word embeddings from GloVe to determine whether or not the word embeddings of lexemes can be used to predict whether or not those lexemes compose a multiword expression. It became apparent that word embeddings carry data that can detect non-compositionality with decent accuracy.
</details>
<details>
<summary>摘要</summary>
多字表达presentUnique挑战在许多翻译任务中。为了最终应用多字表达检测系统到美国手语翻译，我们建立并测试了两个系统，它们使用GloVeWord embedding来判断lexemes是否组成多字表达。结果表明，word embedding含有数据可以准确地检测非组合性。
</details></li>
</ul>
<hr>
<h2 id="Finding-Pragmatic-Differences-Between-Disciplines"><a href="#Finding-Pragmatic-Differences-Between-Disciplines" class="headerlink" title="Finding Pragmatic Differences Between Disciplines"></a>Finding Pragmatic Differences Between Disciplines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00204">http://arxiv.org/abs/2310.00204</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lee Kezar, Jay Pujara</li>
<li>for: 这篇论文主要是为了研究学术文献的结构和表达方式。</li>
<li>methods: 该论文使用了现有的语言模型技术，对一个多学科论文集进行了学术文献normalization，并分析了文档中的部分和排序结构。</li>
<li>results: 研究发现，不同学科的学术文献具有类似的结构和表达方式，并且在不同学科之间存在一定的相似性和差异性。这些结果可以为未来评估研究质量、域风格传递和进一步的 Pragmatic 分析提供基础。<details>
<summary>Abstract</summary>
Scholarly documents have a great degree of variation, both in terms of content (semantics) and structure (pragmatics). Prior work in scholarly document understanding emphasizes semantics through document summarization and corpus topic modeling but tends to omit pragmatics such as document organization and flow. Using a corpus of scholarly documents across 19 disciplines and state-of-the-art language modeling techniques, we learn a fixed set of domain-agnostic descriptors for document sections and "retrofit" the corpus to these descriptors (also referred to as "normalization"). Then, we analyze the position and ordering of these descriptors across documents to understand the relationship between discipline and structure. We report within-discipline structural archetypes, variability, and between-discipline comparisons, supporting the hypothesis that scholarly communities, despite their size, diversity, and breadth, share similar avenues for expressing their work. Our findings lay the foundation for future work in assessing research quality, domain style transfer, and further pragmatic analysis.
</details>
<details>
<summary>摘要</summary>
学术文献 exhibit 大量变化，包括内容（ semantics）和结构（ pragmatics）两方面。先前的学术文献理解工作强调 semantics 通过文摘和文库主题模型来实现，但它们往往忽略 pragmatics，如文档组织和流程。通过使用 crossed 学术文献资料库（ across 19 学科）和现有的语言模型技术，我们学习了一组适用于所有学科的静态描述符（也称为“正常化”）。然后，我们分析了这些描述符在文档中的位置和顺序，以理解学术领域与结构之间的关系。我们发现了学术社区中文献的内部结构架构，以及不同学科之间的比较。这些发现为未来评估研究质量、领域风格传递和进一步的 Pragmatic 分析提供了基础。
</details></li>
</ul>
<hr>
<h2 id="The-Sem-Lex-Benchmark-Modeling-ASL-Signs-and-Their-Phonemes"><a href="#The-Sem-Lex-Benchmark-Modeling-ASL-Signs-and-Their-Phonemes" class="headerlink" title="The Sem-Lex Benchmark: Modeling ASL Signs and Their Phonemes"></a>The Sem-Lex Benchmark: Modeling ASL Signs and Their Phonemes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00196">http://arxiv.org/abs/2310.00196</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leekezar/semlex">https://github.com/leekezar/semlex</a></li>
<li>paper_authors: Lee Kezar, Elana Pontecorvo, Adele Daniels, Connor Baer, Ruth Ferster, Lauren Berger, Jesse Thomason, Zed Sevcikova Sehyr, Naomi Caselli</li>
<li>for: 这个研究旨在提高耳语社区的访问和包容性，但是研究进展受到 represenative 数据的瓶颈。</li>
<li>methods: 我们介绍了一个新的资源 для美国手语（ASL）模型化，即 Sem-Lex Benchmark。这个资源包括了超过84k个隔离手语制作的视频，这些视频来自于聋哑的 ASL 手语发isher，他们提供了同意和收到了补偿。人工专家将这些视频与其他手语资源，如 ASL-LEX、SignBank 和 ASL Citizen，进行了对应，从而实现了有用的扩展 для手语和音律特征recognition。</li>
<li>results: 我们进行了一系列实验，使用 SL-GCN 模型来证明手语的音律特征可以达到85%的准确率，并且这些特征是 ISR 中有效的辅助目标。学习recognize手语的音律特征并与词义recognition结合，可以提高 few-shot ISR 精度6%，提高 ISR 精度总体2%。有关下载数据的 instrucions 可以在 GitHub 上找到。<details>
<summary>Abstract</summary>
Sign language recognition and translation technologies have the potential to increase access and inclusion of deaf signing communities, but research progress is bottlenecked by a lack of representative data. We introduce a new resource for American Sign Language (ASL) modeling, the Sem-Lex Benchmark. The Benchmark is the current largest of its kind, consisting of over 84k videos of isolated sign productions from deaf ASL signers who gave informed consent and received compensation. Human experts aligned these videos with other sign language resources including ASL-LEX, SignBank, and ASL Citizen, enabling useful expansions for sign and phonological feature recognition. We present a suite of experiments which make use of the linguistic information in ASL-LEX, evaluating the practicality and fairness of the Sem-Lex Benchmark for isolated sign recognition (ISR). We use an SL-GCN model to show that the phonological features are recognizable with 85% accuracy, and that they are effective as an auxiliary target to ISR. Learning to recognize phonological features alongside gloss results in a 6% improvement for few-shot ISR accuracy and a 2% improvement for ISR accuracy overall. Instructions for downloading the data can be found at https://github.com/leekezar/SemLex.
</details>
<details>
<summary>摘要</summary>
sign language recognition和翻译技术有可能提高聋听群体的接入和包容，但研究进展受到数据不充分表征的限制。我们介绍了一个新的美国手语（ASL）模型资源，即Sem-Lex Benchmark。该资源现在是最大的一个，包括了84,000个孤立手语生产视频，这些视频由聋听ASL手语演示者提供，并经过了详细的人工标注和对照。我们进行了一系列实验，利用ASL-LEX等手语资源的语言信息，评估Sem-Lex Benchmark的孤立手语认可率（ISR）的实用性和公平性。我们使用SL-GCN模型显示，手语phonological特征可以达到85%的准确率，并且作为auxiliary target可以提高ISR准确率。通过同时学习手语gloss和phonological特征，可以提高几个shot ISR准确率和总的ISR准确率。下载数据的指导可以在https://github.com/leekezar/SemLex中找到。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Strategies-for-Modeling-Sign-Language-Phonology"><a href="#Exploring-Strategies-for-Modeling-Sign-Language-Phonology" class="headerlink" title="Exploring Strategies for Modeling Sign Language Phonology"></a>Exploring Strategies for Modeling Sign Language Phonology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00195">http://arxiv.org/abs/2310.00195</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leekezar/modeling-asl-phonology">https://github.com/leekezar/modeling-asl-phonology</a></li>
<li>paper_authors: Lee Kezar, Riley Carlin, Tejas Srinivasan, Zed Sehyr, Naomi Caselli, Jesse Thomason</li>
<li>for: 这篇论文的目的是提高手语识别模型的性能，特别是模型能够识别手语phoneme。</li>
<li>methods: 这篇论文使用了图 convolutional neural networks (GCNNs) 来识别 ASL-LEX 2.0 中的十六种 phoneme 类型。研究人员还提出了 multi-task 和 curriculum learning 等学习策略，以利用手语 phoneme 之间的相互有用信息，以提高模型的性能。</li>
<li>results: 研究人员在 Sem-Lex Benchmark 上进行了测试，结果表明，使用 curriculum learning 策略可以在所有 phoneme 类型上实现平均准确率为 87%，超过了 fine-tuning 和 multi-task 策略的性能。<details>
<summary>Abstract</summary>
Like speech, signs are composed of discrete, recombinable features called phonemes. Prior work shows that models which can recognize phonemes are better at sign recognition, motivating deeper exploration into strategies for modeling sign language phonemes. In this work, we learn graph convolution networks to recognize the sixteen phoneme "types" found in ASL-LEX 2.0. Specifically, we explore how learning strategies like multi-task and curriculum learning can leverage mutually useful information between phoneme types to facilitate better modeling of sign language phonemes. Results on the Sem-Lex Benchmark show that curriculum learning yields an average accuracy of 87% across all phoneme types, outperforming fine-tuning and multi-task strategies for most phoneme types.
</details>
<details>
<summary>摘要</summary>
如speech, sign language composed of discrete, recombinable features called phonemes. Prior work shows that models that can recognize phonemes are better at sign recognition, motivating deeper exploration into strategies for modeling sign language phonemes. In this work, we learn graph convolution networks to recognize the sixteen phoneme "types" found in ASL-LEX 2.0. Specifically, we explore how learning strategies like multi-task and curriculum learning can leverage mutually useful information between phoneme types to facilitate better modeling of sign language phonemes. Results on the Sem-Lex Benchmark show that curriculum learning yields an average accuracy of 87% across all phoneme types, outperforming fine-tuning and multi-task strategies for most phoneme types.Note: ASL-LEX 2.0 refers to the American Sign Language Lexicon, which is a dataset of sign language words and their corresponding phonemes. The Sem-Lex Benchmark is a standardized dataset for evaluating the recognition of sign language phonemes.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/30/cs.CL_2023_09_30/" data-id="clp869ttq00cmk58829e01oem" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/30/cs.LG_2023_09_30/" class="article-date">
  <time datetime="2023-09-30T10:00:00.000Z" itemprop="datePublished">2023-09-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/30/cs.LG_2023_09_30/">cs.LG - 2023-09-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Enhancing-Efficiency-and-Privacy-in-Memory-Based-Malware-Classification-through-Feature-Selection"><a href="#Enhancing-Efficiency-and-Privacy-in-Memory-Based-Malware-Classification-through-Feature-Selection" class="headerlink" title="Enhancing Efficiency and Privacy in Memory-Based Malware Classification through Feature Selection"></a>Enhancing Efficiency and Privacy in Memory-Based Malware Classification through Feature Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00516">http://arxiv.org/abs/2310.00516</a></li>
<li>repo_url: None</li>
<li>paper_authors: Salim Sazzed, Sharif Ullah</li>
<li>for: 防范黑客病毒攻击计算机系统和数据，提高计算机安全性。</li>
<li>methods: 利用内存快照来检测和分类黑客病毒，并使用多种分类器来提高分类效果和隐私保护。</li>
<li>results: 研究表明，使用mutual information和其他方法选择特征可以提高分类器性能，只使用25%和50%的输入特征可以得到最佳结果。这些发现有助于提高防范黑客病毒攻击的计算机安全性。<details>
<summary>Abstract</summary>
Malware poses a significant security risk to individuals, organizations, and critical infrastructure by compromising systems and data. Leveraging memory dumps that offer snapshots of computer memory can aid the analysis and detection of malicious content, including malware. To improve the efficacy and address privacy concerns in malware classification systems, feature selection can play a critical role as it is capable of identifying the most relevant features, thus, minimizing the amount of data fed to classifiers. In this study, we employ three feature selection approaches to identify significant features from memory content and use them with a diverse set of classifiers to enhance the performance and privacy of the classification task. Comprehensive experiments are conducted across three levels of malware classification tasks: i) binary-level benign or malware classification, ii) malware type classification (including Trojan horse, ransomware, and spyware), and iii) malware family classification within each family (with varying numbers of classes). Results demonstrate that the feature selection strategy, incorporating mutual information and other methods, enhances classifier performance for all tasks. Notably, selecting only 25\% and 50\% of input features using Mutual Information and then employing the Random Forest classifier yields the best results. Our findings reinforce the importance of feature selection for malware classification and provide valuable insights for identifying appropriate approaches. By advancing the effectiveness and privacy of malware classification systems, this research contributes to safeguarding against security threats posed by malicious software.
</details>
<details>
<summary>摘要</summary>
恶意软件对个人、组织和关键基础设施 pose 安全风险，通过损害系统和数据来潜在地威胁。通过使用内存截图，可以帮助分析和检测恶意内容，包括恶意软件。为了提高分类效果并解决隐私问题，Feature Selection 可以扮演关键的角色，可以将内存中的最相关特征选择出来，从而最小化分类器接受的数据量。在这种研究中，我们采用了三种Feature Selection 方法，并将其与多种分类器结合使用，以提高分类效果和隐私。我们在三个不同的恶意软件分类任务上进行了全面的实验，分别是：1. 二进制级别的坏彩虫或清洁软件分类2. 恶意软件类型分类（包括 Trojan horse、勒索软件和间谍软件）3. 恶意软件家族分类（每个家族有不同的数量的类）结果表明，Feature Selection 策略可以提高所有任务的分类效果。特别是使用 Mutual Information 方法选择输入特征的结果，并使用 Random Forest 分类器，可以获得最佳的结果。我们的发现证明了Feature Selection 对恶意软件分类系统的重要性，并提供了选择合适方法的价值。通过提高恶意软件分类系统的效果和隐私，这项研究对安全威胁 pose 的恶意软件提供了重要的贡献。
</details></li>
</ul>
<hr>
<h2 id="Nonparametric-active-learning-for-cost-sensitive-classification"><a href="#Nonparametric-active-learning-for-cost-sensitive-classification" class="headerlink" title="Nonparametric active learning for cost-sensitive classification"></a>Nonparametric active learning for cost-sensitive classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00511">http://arxiv.org/abs/2310.00511</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boris Ndjia Njike, Xavier Siebert</li>
<li>for: 这个论文是针对成本敏感学习问题的一种常见类型，描述了一种非 Parametric 活动学习算法。</li>
<li>methods: 我们的算法是基于预测成本函数的信息 bounds 的构建，逐步选择最有用的向量点，然后对它们进行互动，仅仅问询预测成本的最小值。</li>
<li>results: 我们证明了我们的算法具有最佳速度对应数目互动次数，并且在一个更一般化的 Tsybakov 噪音假设下，与对应的静态学习方法相比，有一定的优势。<details>
<summary>Abstract</summary>
Cost-sensitive learning is a common type of machine learning problem where different errors of prediction incur different costs. In this paper, we design a generic nonparametric active learning algorithm for cost-sensitive classification. Based on the construction of confidence bounds for the expected prediction cost functions of each label, our algorithm sequentially selects the most informative vector points. Then it interacts with them by only querying the costs of prediction that could be the smallest. We prove that our algorithm attains optimal rate of convergence in terms of the number of interactions with the feature vector space. Furthermore, in terms of a general version of Tsybakov's noise assumption, the gain over the corresponding passive learning is explicitly characterized by the probability-mass of the boundary decision. Additionally, we prove the near-optimality of obtained upper bounds by providing matching (up to logarithmic factor) lower bounds.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传统的机器学习问题中，不同的预测错误有不同的成本。在这篇论文中，我们设计了一种通用非Parametric活动学习算法 для成本敏感分类。基于预测成本函数每个标签的信任范围的建构，我们的算法顺序选择最有用的维度点。然后它仅查询这些预测成本最小的成本。我们证明了我们的算法在相对于特征向量空间的互动数量上具有优化的速度。此外，在一个泛化的 Tsybakov 噪声假设下，我们提供了明确的获益，其与相应的被动学习相比，由边界决策的概率质量来Explicitly characterize。此外，我们证明了我们获得的上限 bounds 的几乎优化性，通过提供相应的下限 bounds (logarithmic factor) 来证明。Note: "cost-sensitive learning" in the text is translated as "成本敏感学习" in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Automated-Gait-Generation-For-Walking-Soft-Robotic-Quadrupeds"><a href="#Automated-Gait-Generation-For-Walking-Soft-Robotic-Quadrupeds" class="headerlink" title="Automated Gait Generation For Walking, Soft Robotic Quadrupeds"></a>Automated Gait Generation For Walking, Soft Robotic Quadrupeds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00498">http://arxiv.org/abs/2310.00498</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jake Ketchum, Sophia Schiffer, Muchen Sun, Pranav Kaarthik, Ryan L. Truby, Todd D. Murphey</li>
<li>for: 这个论文的目的是开发一种能够自动生成软体机器人的步态。</li>
<li>methods: 这种方法使用了非常小的计算量，不需要模拟或用户输入，可以在4分钟内在硬件实验中生成良好的翻译和旋转步态。</li>
<li>results: 实验结果表明，这种方法可以在4分钟内生成出比手工设计的步态更好的翻译和旋转步态，并且可以在不同的软体机器人设计中进行自动化的步态生成。<details>
<summary>Abstract</summary>
Gait generation for soft robots is challenging due to the nonlinear dynamics and high dimensional input spaces of soft actuators. Limitations in soft robotic control and perception force researchers to hand-craft open loop controllers for gait sequences, which is a non-trivial process. Moreover, short soft actuator lifespans and natural variations in actuator behavior limit machine learning techniques to settings that can be learned on the same time scales as robot deployment. Lastly, simulation is not always possible, due to heterogeneity and nonlinearity in soft robotic materials and their dynamics change due to wear. We present a sample-efficient, simulation free, method for self-generating soft robot gaits, using very minimal computation. This technique is demonstrated on a motorized soft robotic quadruped that walks using four legs constructed from 16 "handed shearing auxetic" (HSA) actuators. To manage the dimension of the search space, gaits are composed of two sequential sets of leg motions selected from 7 possible primitives. Pairs of primitives are executed on one leg at a time; we then select the best-performing pair to execute while moving on to subsequent legs. This method -- which uses no simulation, sophisticated computation, or user input -- consistently generates good translation and rotation gaits in as low as 4 minutes of hardware experimentation, outperforming hand-crafted gaits. This is the first demonstration of completely autonomous gait generation in a soft robot.
</details>
<details>
<summary>摘要</summary>
软体机器人步态生成具有较大的挑战，主要是因为软动力器的非线性动态和高维输入空间。控制和感知软体机器人的限制使研究人员需要手工设计开loop控制器，这是一个非常困难的过程。另外，软动力器的寿命和自然变化还限制了机器学习技术的应用，只能在同时间尺度上进行学习。此外，模拟也不可行，因为软体机器人材料的不同和非线性，以及动力学的变化。我们提出了一种样本效率高、无需模拟的软体机器人步态自动生成方法，只需要非常少的计算。这种方法基于两个sequential sets of leg motion primitives，每个leg都选择7种可能的primitives中的一个。这些primitives在一个leg上被执行，然后选择最佳的primitives组合，并在后续的leg上执行。这种方法不需要模拟、复杂的计算或用户输入，可以在4分钟的硬件实验中生成良好的翻译和旋转步态，超出了手工设计的步态。这是软体机器人中的首次完全自动步态生成示例。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-DAG-Discovery-for-Interpretable-Imitation-Learning"><a href="#Dynamic-DAG-Discovery-for-Interpretable-Imitation-Learning" class="headerlink" title="Dynamic DAG Discovery for Interpretable Imitation Learning"></a>Dynamic DAG Discovery for Interpretable Imitation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00489">http://arxiv.org/abs/2310.00489</a></li>
<li>repo_url: None</li>
<li>paper_authors: ianxiang Zhao, Wenchao Yu, Suhang Wang, Lu Wang, Xiang Zhang, Yuncong Chen, Yanchi Liu, Wei Cheng, Haifeng Chen</li>
<li>for: 提高神经网络控制政策的可读性和解释性，使其更容易理解和预测。</li>
<li>methods: 使用动态 causal discovery 模块、 causality encoding 模块和预测模块，通过端到端学习来捕捉神经网络所学的知识，并将其表示为导向的无环图，以便更好地理解神经网络的决策过程。</li>
<li>results: 经验结果表明，提出的方法可以准确地捕捉神经网络所学的知识，并且可以在实际应用中提高神经网络的预测精度和可读性。<details>
<summary>Abstract</summary>
Imitation learning, which learns agent policy by mimicking expert demonstration, has shown promising results in many applications such as medical treatment regimes and self-driving vehicles. However, it remains a difficult task to interpret control policies learned by the agent. Difficulties mainly come from two aspects: 1) agents in imitation learning are usually implemented as deep neural networks, which are black-box models and lack interpretability; 2) the latent causal mechanism behind agents' decisions may vary along the trajectory, rather than staying static throughout time steps. To increase transparency and offer better interpretability of the neural agent, we propose to expose its captured knowledge in the form of a directed acyclic causal graph, with nodes being action and state variables and edges denoting the causal relations behind predictions. Furthermore, we design this causal discovery process to be state-dependent, enabling it to model the dynamics in latent causal graphs. Concretely, we conduct causal discovery from the perspective of Granger causality and propose a self-explainable imitation learning framework, {\method}. The proposed framework is composed of three parts: a dynamic causal discovery module, a causality encoding module, and a prediction module, and is trained in an end-to-end manner. After the model is learned, we can obtain causal relations among states and action variables behind its decisions, exposing policies learned by it. Experimental results on both synthetic and real-world datasets demonstrate the effectiveness of the proposed {\method} in learning the dynamic causal graphs for understanding the decision-making of imitation learning meanwhile maintaining high prediction accuracy.
</details>
<details>
<summary>摘要</summary>
随着各种应用领域的广泛应用，如医疗治疗和自动驾驶车辆等，模仿学习已经显示了有前途的成果。然而，它仍然是一个困难的任务，即解释控制策略学习的agent。主要问题来源于两个方面：1）agent通常是用深度神经网络实现，这些模型是黑盒模型，缺乏可读性；2）代理人做出决策的 latent causal mechanism 可能会随着时间推移而变化，而不是静止的。为了增加透明度并提供更好的可读性，我们提议通过暴露 agent 捕捉的知识来减少这些问题。具体来说，我们通过 causal discovery 进程来暴露 agent 的决策过程中的 causal 关系，并使其能够模型 latent causal graph 的动态。我们的方法包括三部分：动态 causal discovery 模块、 causality encoding 模块和预测模块，并在综合训练的情况下进行学习。 после模型学习完毕，我们可以从 agent 的决策过程中提取 causal 关系，并对它学习的策略进行解释。实验结果表明，我们提出的方法可以准确地捕捉 agent 的决策过程中的 causal 关系，同时保持高的预测精度。
</details></li>
</ul>
<hr>
<h2 id="Prompting-Code-Interpreter-to-Write-Better-Unit-Tests-on-Quixbugs-Functions"><a href="#Prompting-Code-Interpreter-to-Write-Better-Unit-Tests-on-Quixbugs-Functions" class="headerlink" title="Prompting Code Interpreter to Write Better Unit Tests on Quixbugs Functions"></a>Prompting Code Interpreter to Write Better Unit Tests on Quixbugs Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00483">http://arxiv.org/abs/2310.00483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vincent Li, Nick Doiron</li>
<li>for: 本研究旨在探讨 Code Interpreter 如何在 Python 函数上生成单元测试，以及不同提示语言对生成单元测试质量的影响。</li>
<li>methods: 本研究使用 Code Interpreter，一种基于 GPT-4 的 LLM，生成单元测试。</li>
<li>results: 研究发现，对提示语言进行小幅修改不会影响生成单元测试的质量。然而， Code Interpreter 能够有效地检查代码中的错误，因此提供 runnable 代码来检查其输出的正确性是有优势的。我们的发现表明，在提示模型类似于 Code Interpreter 时，只需提供基本信息可以生成单元测试，词句级别的细节不太重要。<details>
<summary>Abstract</summary>
Unit testing is a commonly-used approach in software engineering to test the correctness and robustness of written code. Unit tests are tests designed to test small components of a codebase in isolation, such as an individual function or method. Although unit tests have historically been written by human programmers, recent advancements in AI, particularly LLMs, have shown corresponding advances in automatic unit test generation. In this study, we explore the effect of different prompts on the quality of unit tests generated by Code Interpreter, a GPT-4-based LLM, on Python functions provided by the Quixbugs dataset, and we focus on prompting due to the ease with which users can make use of our findings and observations. We find that the quality of the generated unit tests is not sensitive to changes in minor details in the prompts provided. However, we observe that Code Interpreter is often able to effectively identify and correct mistakes in code that it writes, suggesting that providing it runnable code to check the correctness of its outputs would be beneficial, even though we find that it is already often able to generate correctly-formatted unit tests. Our findings suggest that, when prompting models similar to Code Interpreter, it is important to include the basic information necessary to generate unit tests, but minor details are not as important.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate english text into simplified chineseUnit testing is a commonly-used approach in software engineering to test the correctness and robustness of written code. Unit tests are tests designed to test small components of a codebase in isolation, such as an individual function or method. Although unit tests have historically been written by human programmers, recent advancements in AI, particularly LLMs, have shown corresponding advances in automatic unit test generation. In this study, we explore the effect of different prompts on the quality of unit tests generated by Code Interpreter, a GPT-4-based LLM, on Python functions provided by the Quixbugs dataset, and we focus on prompting due to the ease with which users can make use of our findings and observations. We find that the quality of the generated unit tests is not sensitive to changes in minor details in the prompts provided. However, we observe that Code Interpreter is often able to effectively identify and correct mistakes in code that it writes, suggesting that providing it runnable code to check the correctness of its outputs would be beneficial, even though we find that it is already often able to generate correctly-formatted unit tests. Our findings suggest that, when prompting models similar to Code Interpreter, it is important to include the basic information necessary to generate unit tests, but minor details are not as important.
</details></li>
</ul>
<hr>
<h2 id="Generative-Design-of-inorganic-compounds-using-deep-diffusion-language-models"><a href="#Generative-Design-of-inorganic-compounds-using-deep-diffusion-language-models" class="headerlink" title="Generative Design of inorganic compounds using deep diffusion language models"></a>Generative Design of inorganic compounds using deep diffusion language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00475">http://arxiv.org/abs/2310.00475</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rongzhi Dong, Nihang Fu, dirisuriya M. D. Siriwardane, Jianjun Hu</li>
<li>For: The paper aims to discover new materials with specific functions by leveraging deep learning and chemical knowledge.* Methods: The authors use a deep learning-based generative model for material composition and structure design, which includes deep diffusion language models and a template-based crystal structure prediction algorithm. They also use a universal graph neural network-based potential for structure relaxation and density functional theory (DFT) calculations for validation.* Results: The authors discovered six new materials with formation energy less than zero, among which four materials (Ti2HfO5, TaNbP, YMoN2, and TaReO4) have an e-above-hull energy of less than 0.3 eV, demonstrating the effectiveness of their approach.Here is the simplified Chinese version of the three key points:* 为：本文目标是利用深度学习和化学知识发现具有特定功能的材料。* 方法：作者们使用深度扩散语言模型来生成材料组成和结构设计，并使用模板基于晶体结构预测算法来预测其对应的结构。他们还使用基于图神经网络的晶体结构relaxation算法和能量函数理论计算来验证新结构的有效性。* 结果：作者们发现了六种新的材料，其中四种材料（Ti2HfO5、TaNbP、YMoN2和TaReO4）的形成能量小于零，并且这些材料的e-above-hull能量小于0.3 eV，证明了他们的方法的有效性。<details>
<summary>Abstract</summary>
Due to the vast chemical space, discovering materials with a specific function is challenging. Chemical formulas are obligated to conform to a set of exacting criteria such as charge neutrality, balanced electronegativity, synthesizability, and mechanical stability. In response to this formidable task, we introduce a deep learning-based generative model for material composition and structure design by learning and exploiting explicit and implicit chemical knowledge. Our pipeline first uses deep diffusion language models as the generator of compositions and then applies a template-based crystal structure prediction algorithm to predict their corresponding structures, which is then followed by structure relaxation using a universal graph neural network-based potential. The density functional theory (DFT) calculations of the formation energies and energy-above-the-hull analysis are used to validate new structures generated through our pipeline. Based on the DFT calculation results, six new materials, including Ti2HfO5, TaNbP, YMoN2, TaReO4, HfTiO2, and HfMnO2, with formation energy less than zero have been found. Remarkably, among these, four materials, namely Ti2$HfO5, TaNbP, YMoN2, and TaReO4, exhibit an e-above-hull energy of less than 0.3 eV. These findings have proved the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
Our pipeline first uses deep diffusion language models as the generator of compositions and then applies a template-based crystal structure prediction algorithm to predict their corresponding structures. This is followed by structure relaxation using a universal graph neural network-based potential. The density functional theory (DFT) calculations of the formation energies and energy-above-the-hull analysis are used to validate the new structures generated through our pipeline.Based on the DFT calculation results, six new materials with formation energy less than zero have been found, including Ti2HfO5, TaNbP, YMoN2, TaReO4, HfTiO2, and HfMnO2. Remarkably, among these, four materials (Ti2HfO5, TaNbP, YMoN2, and TaReO4) exhibit an e-above-hull energy of less than 0.3 eV. These findings demonstrate the effectiveness of our approach.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Mortality-Prediction-in-Heart-Failure-Patients-Exploring-Preprocessing-Methods-for-Imbalanced-Clinical-Datasets"><a href="#Enhancing-Mortality-Prediction-in-Heart-Failure-Patients-Exploring-Preprocessing-Methods-for-Imbalanced-Clinical-Datasets" class="headerlink" title="Enhancing Mortality Prediction in Heart Failure Patients: Exploring Preprocessing Methods for Imbalanced Clinical Datasets"></a>Enhancing Mortality Prediction in Heart Failure Patients: Exploring Preprocessing Methods for Imbalanced Clinical Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00457">http://arxiv.org/abs/2310.00457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanif Kia, Mansour Vali, Hadi Sabahi<br>for: 这篇论文是为了提高心血管疾病（HF）患者一个月死亡预测的精度。methods: 这篇论文使用了一个全面的预processing框架，包括尺度调整、异常处理和重样化，以及一个识别Missing值的方法。results: 这篇论文使用了PROVE资料集，并借助适当的预processing技术和机器学习（ML）算法，实现了一个月死亡预测的改善。结果显示，使用这些预processing技术可以提高tree-based模型（例如Random Forest和XGB）的F1分数和MCC分数约3.6%和2.7%。这表明了这种预processing方法在处理不均衡的医疗资料时的效果。<details>
<summary>Abstract</summary>
Heart failure (HF) is a critical condition in which the accurate prediction of mortality plays a vital role in guiding patient management decisions. However, clinical datasets used for mortality prediction in HF often suffer from an imbalanced distribution of classes, posing significant challenges. In this paper, we explore preprocessing methods for enhancing one-month mortality prediction in HF patients. We present a comprehensive preprocessing framework including scaling, outliers processing and resampling as key techniques. We also employed an aware encoding approach to effectively handle missing values in clinical datasets. Our study utilizes a comprehensive dataset from the Persian Registry Of cardio Vascular disease (PROVE) with a significant class imbalance. By leveraging appropriate preprocessing techniques and Machine Learning (ML) algorithms, we aim to improve mortality prediction performance for HF patients. The results reveal an average enhancement of approximately 3.6% in F1 score and 2.7% in MCC for tree-based models, specifically Random Forest (RF) and XGBoost (XGB). This demonstrates the efficiency of our preprocessing approach in effectively handling Imbalanced Clinical Datasets (ICD). Our findings hold promise in guiding healthcare professionals to make informed decisions and improve patient outcomes in HF management.
</details>
<details>
<summary>摘要</summary>
心血液性疾病（HF）是一种严重的疾病状态，其中准确预测死亡率具有重要的指导作用，以帮助医生对患者进行有效的管理决策。然而，在HF疾病中使用的临床数据集经常受到类别的不均衡分布的困扰，这对于预测死亡率具有重要的挑战。在这篇论文中，我们探讨了适用于HF患者一月死亡预测的预处理技术。我们提出了一个完整的预处理框架，包括缩放、异常处理和重采样等关键技巧。此外，我们采用了一种感知编码方法，以有效地处理临床数据集中的缺失数据。我们的研究使用了来自伊朗cardiovascular疾病注册（PROVE）的全面数据集，这个数据集具有显著的类别不均衡。通过适用适当的预处理技术和机器学习（ML）算法，我们希望提高HF患者的一月死亡预测性能。结果表明，使用树状模型（RF和XGB）时，我们的预处理方法可以提高F1分数的平均提升约3.6%和MCC的平均提升约2.7%。这表明我们的预处理方法可以有效地处理临床数据集中的类别不均衡。我们的发现可能会帮助医生做出更有知识的决策，从而改善HF患者的疾病管理。
</details></li>
</ul>
<hr>
<h2 id="Music-and-Lyrics-driven-Dance-Synthesis"><a href="#Music-and-Lyrics-driven-Dance-Synthesis" class="headerlink" title="Music- and Lyrics-driven Dance Synthesis"></a>Music- and Lyrics-driven Dance Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00455">http://arxiv.org/abs/2310.00455</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yy1lab/lmd">https://github.com/yy1lab/lmd</a></li>
<li>paper_authors: Wenjie Yin, Qingyuan Yao, Yi Yu, Hang Yin, Danica Kragic, Mårten Björkman</li>
<li>for: 这个研究是为了创建一个新的多媒体数据集，以便将歌曲和歌词与舞蹈动作结合在一起，增强舞蹈动作的semantic意义。</li>
<li>methods: 这个研究使用了一个跨模式传播网络，将音乐和歌词与舞蹈动作联系在一起，实现了3D舞蹈动作的生成。</li>
<li>results: 这个研究创建了一个名为JustLMD的新的多媒体数据集，包含4.6小时的3D舞蹈动作，以及其 accompaniment的音乐和英文歌词。此外，这个研究还展示了一个跨模式传播网络，可以根据音乐和歌词生成3D舞蹈动作。<details>
<summary>Abstract</summary>
Lyrics often convey information about the songs that are beyond the auditory dimension, enriching the semantic meaning of movements and musical themes. Such insights are important in the dance choreography domain. However, most existing dance synthesis methods mainly focus on music-to-dance generation, without considering the semantic information. To complement it, we introduce JustLMD, a new multimodal dataset of 3D dance motion with music and lyrics. To the best of our knowledge, this is the first dataset with triplet information including dance motion, music, and lyrics. Additionally, we showcase a cross-modal diffusion-based network designed to generate 3D dance motion conditioned on music and lyrics. The proposed JustLMD dataset encompasses 4.6 hours of 3D dance motion in 1867 sequences, accompanied by musical tracks and their corresponding English lyrics.
</details>
<details>
<summary>摘要</summary>
文本经常传递歌曲之外的信息，增强舞蹈主题和乐曲的 semantics 含义。这些信息在舞蹈编排领域是非常重要的。然而，大多数现有的舞蹈生成方法主要集中在音乐到舞蹈生成上，忽略了semantic信息。为了补充它，我们介绍了JustLMD，一个新的多Modal dataset，包括3D舞蹈动作、音乐和歌词。我们知道这是首个包含三元信息的 dataset。此外，我们还展示了一种cross-modal填充网络，用于生成3D舞蹈动作，受音乐和歌词的控制。JustLMD dataset包含4.6小时的3D舞蹈动作，共1867个sequences，每个sequence都附有乐曲和其对应的英文歌词。
</details></li>
</ul>
<hr>
<h2 id="The-objective-function-equality-property-of-infoGAN-for-two-layer-network"><a href="#The-objective-function-equality-property-of-infoGAN-for-two-layer-network" class="headerlink" title="The objective function equality property of infoGAN for two-layer network"></a>The objective function equality property of infoGAN for two-layer network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00443">http://arxiv.org/abs/2310.00443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahmud Hasan</li>
<li>for: 这个论文的目的是提出一种基于信息最大化的生成 adversarial network（infoGAN），以解决生成 adversarial network中的一些问题。</li>
<li>methods: 该论文使用了两种网络：推定网络和生成网络，并使用了共同信息函数。它还包括了隐藏变量、共同信息和目标函数等组成部分。</li>
<li>results: 研究表明，infoGAN中的两个目标函数在推定网络和生成网络样本数趋于无穷大时变得等价。这种等价性得到证明，通过考虑推定网络和生成网络函数类型的Rademacher复杂度。此外，使用两层网络，即推定网络和生成网络，并采用了Lipschitz和不递减 activation函数，也验证了这种等价性。<details>
<summary>Abstract</summary>
Information Maximizing Generative Adversarial Network (infoGAN) can be understood as a minimax problem involving two networks: discriminators and generators with mutual information functions. The infoGAN incorporates various components, including latent variables, mutual information, and objective function. This research demonstrates that the two objective functions in infoGAN become equivalent as the discriminator and generator sample size approaches infinity. This equivalence is established by considering the disparity between the empirical and population versions of the objective function. The bound on this difference is determined by the Rademacher complexity of the discriminator and generator function class. Furthermore, the utilization of a two-layer network for both the discriminator and generator, featuring Lipschitz and non-decreasing activation functions, validates this equality
</details>
<details>
<summary>摘要</summary>
信息最大化生成对抗网络（infoGAN）可以理解为两个网络：分类器和生成器具有互信息函数。infoGAN包含多个组件，包括隐藏变量、互信息和目标函数。本研究表明，infoGAN中的两个目标函数在分类器和生成器抽样数趋于无穷大时变得等价。这种等价性由考虑分类器和生成器抽样数的实际版本和人口版本之间的差异来确定。此外，使用两层网络作为分类器和生成器，其中包括 lipschitz 和非减少 activation 函数，可以证明这种等价性。Note: "Simplified Chinese" is a romanization of the Chinese language that uses a simplified set of characters and pronunciation. It is commonly used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="ResolvNet-A-Graph-Convolutional-Network-with-multi-scale-Consistency"><a href="#ResolvNet-A-Graph-Convolutional-Network-with-multi-scale-Consistency" class="headerlink" title="ResolvNet: A Graph Convolutional Network with multi-scale Consistency"></a>ResolvNet: A Graph Convolutional Network with multi-scale Consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00431">http://arxiv.org/abs/2310.00431</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Koke, Abhishek Saroha, Yuesong Shen, Marvin Eisenberger, Daniel Cremers</li>
<li>for: The paper is written to address the limitations of graph neural networks (GNNs) in propagating information over long distances, particularly in the presence of bottlenecks and strongly connected sub-graphs.</li>
<li>methods: The paper introduces a new graph neural network architecture called ResolvNet, which is based on the mathematical concept of resolvents. The authors claim that ResolvNet is more consistent across multiple scales and outperforms baseline models on many tasks.</li>
<li>results: The authors report extensive experimental results on real-world data that demonstrate the effectiveness of ResolvNet in various tasks, including those with bottlenecks and strongly connected sub-graphs. The results show that ResolvNet outperforms baseline models significantly, and that it is more consistent across multiple scales.<details>
<summary>Abstract</summary>
It is by now a well known fact in the graph learning community that the presence of bottlenecks severely limits the ability of graph neural networks to propagate information over long distances. What so far has not been appreciated is that, counter-intuitively, also the presence of strongly connected sub-graphs may severely restrict information flow in common architectures. Motivated by this observation, we introduce the concept of multi-scale consistency. At the node level this concept refers to the retention of a connected propagation graph even if connectivity varies over a given graph. At the graph-level, multi-scale consistency refers to the fact that distinct graphs describing the same object at different resolutions should be assigned similar feature vectors. As we show, both properties are not satisfied by poular graph neural network architectures. To remedy these shortcomings, we introduce ResolvNet, a flexible graph neural network based on the mathematical concept of resolvents. We rigorously establish its multi-scale consistency theoretically and verify it in extensive experiments on real world data: Here networks based on this ResolvNet architecture prove expressive; out-performing baselines significantly on many tasks; in- and outside the multi-scale setting.
</details>
<details>
<summary>摘要</summary>
现在已经广泛认可的graph学习社区中的一个事实是，瓶颈会严重限制图 neural network的信息传递范围。而且，Counter-intuitively，强Connected sub-graphs 也可能减少信息流动。基于这一观察，我们引入多尺度一致性概念。在节点级别上，这个概念表示连接度变化的图中保持连通的宣传图。在图级别上，多尺度一致性表示同一个对象在不同的分辨率上描述的不同图应该赋予相似的特征 вектор。我们证明，这两个属性都不满足流行的图 neural network 架构。为了缓解这些缺陷，我们引入ResolvNet，基于解析函数的图 neural network 架构。我们严格地证明其多尺度一致性，并在实际数据上进行了广泛的实验，结果表明：ResolvNet 架构基网络表现强大，在许多任务上与基准值相比表现出优异。
</details></li>
</ul>
<hr>
<h2 id="On-the-Stability-of-Iterative-Retraining-of-Generative-Models-on-their-own-Data"><a href="#On-the-Stability-of-Iterative-Retraining-of-Generative-Models-on-their-own-Data" class="headerlink" title="On the Stability of Iterative Retraining of Generative Models on their own Data"></a>On the Stability of Iterative Retraining of Generative Models on their own Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00429">http://arxiv.org/abs/2310.00429</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quentin Bertrand, Avishek Joey Bose, Alexandre Duplessis, Marco Jiralerspong, Gauthier Gidel</li>
<li>for: 这个论文的目的是研究如何在混合数据集上训练深度生成模型，以确保其稳定性。</li>
<li>methods: 这篇论文使用了迭代训练和权重抑制等方法来研究深度生成模型在混合数据集上的稳定性。</li>
<li>results: 经验 validate了这些方法可以确保深度生成模型在混合数据集上的稳定性，并且可以在不同的数据集上进行应用。<details>
<summary>Abstract</summary>
Deep generative models have made tremendous progress in modeling complex data, often exhibiting generation quality that surpasses a typical human's ability to discern the authenticity of samples. Undeniably, a key driver of this success is enabled by the massive amounts of web-scale data consumed by these models. Due to these models' striking performance and ease of availability, the web will inevitably be increasingly populated with synthetic content. Such a fact directly implies that future iterations of generative models must contend with the reality that their training is curated from both clean data and artificially generated data from past models. In this paper, we develop a framework to rigorously study the impact of training generative models on mixed datasets (of real and synthetic data) on their stability. We first prove the stability of iterative training under the condition that the initial generative models approximate the data distribution well enough and the proportion of clean training data (w.r.t. synthetic data) is large enough. We empirically validate our theory on both synthetic and natural images by iteratively training normalizing flows and state-of-the-art diffusion models on CIFAR10 and FFHQ.
</details>
<details>
<summary>摘要</summary>
深度生成模型已经取得了很大的进步，能够模拟复杂的数据，其生成质量经常超过人类能力的识别水平。无疑，这一成就的关键因素是由这些模型所消耗的庞大量数据。由于这些模型的突出表现和易用性，未来网络将被充满假的内容。这一事实直接意味着未来的生成模型需要面对现有的混合数据集（包括真实数据和过去模型生成的假数据）进行训练。在这篇论文中，我们开发了一套框架，用于严谨地研究训练生成模型的稳定性。我们首先证明，如果初始的生成模型足够接近数据分布，并且净训练数据占总训练数据的比重充分大 enough，那么训练过程是稳定的。我们验证了我们的理论通过对Synthetic和自然图像进行iterative训练，使用normalizing flows和state-of-the-art diffusion模型在CIFAR10和FFHQ上进行了实验。
</details></li>
</ul>
<hr>
<h2 id="An-Efficient-Algorithm-for-Clustered-Multi-Task-Compressive-Sensing"><a href="#An-Efficient-Algorithm-for-Clustered-Multi-Task-Compressive-Sensing" class="headerlink" title="An Efficient Algorithm for Clustered Multi-Task Compressive Sensing"></a>An Efficient Algorithm for Clustered Multi-Task Compressive Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00420">http://arxiv.org/abs/2310.00420</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/al5250/multics">https://github.com/al5250/multics</a></li>
<li>paper_authors: Alexander Lin, Demba Ba</li>
<li>for: 这个论文考虑了分区多任务压缩感知，一种层次模型，该模型可以通过找到多个任务之间共享信息的征 cluster 来提高信号重建。</li>
<li>methods: 我们提出了一种新的算法，可以快速地降低模型的推理时间复杂度，而不需要直接计算多个大 covariance 矩阵。我们的方法结合了 Monte Carlo 采样和迭代线性解密。</li>
<li>results: 我们的实验表明，相比现有基eline，我们的算法可以在高维度情况下提高速度，并且可以降低内存占用量。具体来说，我们的算法可以在某些情况下比现有基eline thousands of times  faster和一个数量级更高的内存占用量。<details>
<summary>Abstract</summary>
This paper considers clustered multi-task compressive sensing, a hierarchical model that solves multiple compressive sensing tasks by finding clusters of tasks that leverage shared information to mutually improve signal reconstruction. The existing inference algorithm for this model is computationally expensive and does not scale well in high dimensions. The main bottleneck involves repeated matrix inversion and log-determinant computation for multiple large covariance matrices. We propose a new algorithm that substantially accelerates model inference by avoiding the need to explicitly compute these covariance matrices. Our approach combines Monte Carlo sampling with iterative linear solvers. Our experiments reveal that compared to the existing baseline, our algorithm can be up to thousands of times faster and an order of magnitude more memory-efficient.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Linear-Convergence-of-Pre-Conditioned-PI-Consensus-Algorithm-under-Restricted-Strong-Convexity"><a href="#Linear-Convergence-of-Pre-Conditioned-PI-Consensus-Algorithm-under-Restricted-Strong-Convexity" class="headerlink" title="Linear Convergence of Pre-Conditioned PI Consensus Algorithm under Restricted Strong Convexity"></a>Linear Convergence of Pre-Conditioned PI Consensus Algorithm under Restricted Strong Convexity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00419">http://arxiv.org/abs/2310.00419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kushal Chakrabarti, Mayank Baranwal</li>
<li>for: 本研究考虑了分布式凸优化问题在平行多代理网络中的解决方法。网络假设同步连接。通过使用补偿-积分（PI）控制策略，研究者开发了多种算法，其中最早的是PI妥协算法。使用 Lyapunov 理论，我们保证了PI妥协算法在约度匹配精度步长下展开几何快速收敛，不需要个体本地成本函数的凸性。</li>
<li>methods: 研究者使用了 Lyapunov 理论来保证PI妥协算法的快速收敛。在PI妥协算法中，通过使用常数正定矩阵进行本地预处理，提高了PI妥协算法的效率。</li>
<li>results: 研究者通过数值 validatePI妥协算法的效果，并与其他分布式凸优化算法进行比较。结果显示，采用本地预处理可以减少通信图的影响，提高PI妥协算法的性能。<details>
<summary>Abstract</summary>
This paper considers solving distributed convex optimization problems in peer-to-peer multi-agent networks. The network is assumed to be synchronous and connected. By using the proportional-integral (PI) control strategy, various algorithms with fixed stepsize have been developed. The earliest among them is the PI consensus algorithm. Using Lyapunov theory, we guarantee exponential convergence of the PI consensus algorithm for restricted strongly convex functions with rate-matching discretization, without requiring convexity of individual local cost functions, for the first time. In order to accelerate the PI consensus algorithm, we incorporate local pre-conditioning in the form of constant positive definite matrices and numerically validate its efficiency compared to the prominent distributed convex optimization algorithms. Unlike classical pre-conditioning, where only the gradients are multiplied by a pre-conditioner, the proposed pre-conditioning modifies both the gradients and the consensus terms, thereby controlling the effect of the communication graph between the agents on the PI consensus algorithm.
</details>
<details>
<summary>摘要</summary>
（本文考虑了在多智能机器人网络中解决分布式凸优化问题。网络假设同步连接。通过使用规比积分控制策略，我们开发了多种固定步长的算法。最早的一种是PI妥协算法。使用拉普诺夫理论，我们 garanttees 离散凸函数的凸优化问题的快速收敛，不需要个体本地成本函数的凸性，这是第一次。为了加速PI妥协算法，我们采用了本地预conditioning，通过将常数正定矩阵加到梯度和妥协项中，控制了网络通信图between agents对PI妥协算法的影响。）
</details></li>
</ul>
<hr>
<h2 id="Better-Situational-Graphs-by-Inferring-High-level-Semantic-Relational-Concepts"><a href="#Better-Situational-Graphs-by-Inferring-High-level-Semantic-Relational-Concepts" class="headerlink" title="Better Situational Graphs by Inferring High-level Semantic-Relational Concepts"></a>Better Situational Graphs by Inferring High-level Semantic-Relational Concepts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00401">http://arxiv.org/abs/2310.00401</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jose Andres Millan-Romera, Hriday Bavle, Muhammad Shaheer, Martin R. Oswald, Holger Voos, Jose Luis Sanchez-Lopez</li>
<li>for: 提高 Situational Graphs (S-Graphs) 的准确性和表示力，通过抽象高级semantic关系</li>
<li>methods: 使用 Graph Neural Network (GNN) 学习高级semantic关系，从低级factor graph中推理</li>
<li>results: 在 simulated 和实际数据集上，与基eline算法相比，更高精度和更高效的推理结果，以及新的semantic概念“墙”和其与墙面之间的关系<details>
<summary>Abstract</summary>
Recent works on SLAM extend their pose graphs with higher-level semantic concepts exploiting relationships between them, to provide, not only a richer representation of the situation/environment but also to improve the accuracy of its estimation. Concretely, our previous work, Situational Graphs (S-Graphs), a pioneer in jointly leveraging semantic relationships in the factor optimization process, relies on semantic entities such as wall surfaces and rooms, whose relationship is mathematically defined. Nevertheless, excerpting these high-level concepts relying exclusively on the lower-level factor-graph remains a challenge and it is currently done with ad-hoc algorithms, which limits its capability to include new semantic-relational concepts. To overcome this limitation, in this work, we propose a Graph Neural Network (GNN) for learning high-level semantic-relational concepts that can be inferred from the low-level factor graph. We have demonstrated that we can infer room entities and their relationship to the mapped wall surfaces, more accurately and more computationally efficient than the baseline algorithm. Additionally, to demonstrate the versatility of our method, we provide a new semantic concept, i.e. wall, and its relationship with its wall surfaces. Our proposed method has been integrated into S-Graphs+, and it has been validated in both simulated and real datasets. A docker container with our software will be made available to the scientific community.
</details>
<details>
<summary>摘要</summary>
最近的SLAM研究延伸了它们的姿态图，添加更高一级的semantic概念，利用这些概念之间的关系，以提供更加丰富的情况/环境表示，并提高其估计的准确性。具体来说，我们的前一项工作《 Situational Graphs (S-Graphs)》，是jointly利用semantic关系在因素优化过程中的先驱者， rely on semantic entity如墙面和房间，其关系由数学定义。然而，抽取这些高级概念，即依据低级因素图仅存在的方法，是一个挑战，现在通过ad-hoc算法来实现。为了超越这一限制，在这项工作中，我们提出了一种图神经网络（GNN），用于学习高级semantic-relational概念，可以从低级因素图中被推导出。我们已经证明，我们可以更加准确地、更加计算效率地，从低级因素图中推导出房间实体和它们与映射的墙面之间的关系。此外，为了证明我们的方法的多样性，我们还提出了一新的semantic概念，即墙，以及它与它的墙面之间的关系。我们的提议的方法已经被 integrate到S-Graphs+中，并在 simulations和实际数据中进行了验证。我们将在科学社区中提供一个docker容器，包含我们的软件。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-the-Effect-of-Incidental-Correlations-on-Part-based-Learning"><a href="#Mitigating-the-Effect-of-Incidental-Correlations-on-Part-based-Learning" class="headerlink" title="Mitigating the Effect of Incidental Correlations on Part-based Learning"></a>Mitigating the Effect of Incidental Correlations on Part-based Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00377">http://arxiv.org/abs/2310.00377</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gaurav Bhatt, Deepayan Das, Leonid Sigal, Vineeth N Balasubramanian</li>
<li>for: 该研究旨在提高基于部分的表示，以便更好地处理限制观察的对象，提高泛化和解释性。</li>
<li>methods: 该研究使用了两种创新的正则化方法，包括一种干扰损失和一种弱监督损失，以避免限制观察对学习的影响。此外，研究还添加了稀疏和正交约束，以促进学习高质量的部分表示。</li>
<li>results: 研究表明，通过我们的方法，可以在少量数据下实现领先的表现（State-of-the-art，SoTA），并且在背景变化和常见数据损害下，部分表示仍然能够保持更好的泛化性和解释性。<details>
<summary>Abstract</summary>
Intelligent systems possess a crucial characteristic of breaking complicated problems into smaller reusable components or parts and adjusting to new tasks using these part representations. However, current part-learners encounter difficulties in dealing with incidental correlations resulting from the limited observations of objects that may appear only in specific arrangements or with specific backgrounds. These incidental correlations may have a detrimental impact on the generalization and interpretability of learned part representations. This study asserts that part-based representations could be more interpretable and generalize better with limited data, employing two innovative regularization methods. The first regularization separates foreground and background information's generative process via a unique mixture-of-parts formulation. Structural constraints are imposed on the parts using a weakly-supervised loss, guaranteeing that the mixture-of-parts for foreground and background entails soft, object-agnostic masks. The second regularization assumes the form of a distillation loss, ensuring the invariance of the learned parts to the incidental background correlations. Furthermore, we incorporate sparse and orthogonal constraints to facilitate learning high-quality part representations. By reducing the impact of incidental background correlations on the learned parts, we exhibit state-of-the-art (SoTA) performance on few-shot learning tasks on benchmark datasets, including MiniImagenet, TieredImageNet, and FC100. We also demonstrate that the part-based representations acquired through our approach generalize better than existing techniques, even under domain shifts of the background and common data corruption on the ImageNet-9 dataset. The implementation is available on GitHub: https://github.com/GauravBh1010tt/DPViT.git
</details>
<details>
<summary>摘要</summary>
智能系统具有分解复杂问题为更小可重用组件或部分的重要特点。然而，当前的部件学习方法在面临有限观察对象的限制下遇到了间接相关性问题，这些问题可能会影响学习的泛化和解释性。这种研究表明，使用两种创新的正则化方法可以使部件表示更加可解和泛化。首先，我们使用了一种唯一的混合部分形式来分离前景和背景信息的生成过程。我们使用弱监督损失来强制实施结构约束，确保混合部分对于前景和背景的混合是软的、对象无关的面具。其次，我们使用了一种液体损失来保证学习的部件具有对于干扰背景相关性的抗变异性。此外，我们还添加了稀疏和正交约束，以便学习高质量的部件表示。通过减少学习部件中的干扰背景相关性，我们实现了state-of-the-art（SoTA）性能在少数shot学习任务上，包括MiniImagenet、TieredImageNet和FC100。我们还证明了我们的方法学习的部件表示能够更好地泛化，即使在背景的域变和通用数据腐坏中。实现可以在GitHub上找到：https://github.com/GauravBh1010tt/DPViT.git。
</details></li>
</ul>
<hr>
<h2 id="Harmony-World-Models-Boosting-Sample-Efficiency-for-Model-based-Reinforcement-Learning"><a href="#Harmony-World-Models-Boosting-Sample-Efficiency-for-Model-based-Reinforcement-Learning" class="headerlink" title="Harmony World Models: Boosting Sample Efficiency for Model-based Reinforcement Learning"></a>Harmony World Models: Boosting Sample Efficiency for Model-based Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00344">http://arxiv.org/abs/2310.00344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoyu Ma, Jialong Wu, Ningya Feng, Jianmin Wang, Mingsheng Long</li>
<li>for: 提高模型基于奖励学习（MBRL）的效率，通过对世界模型的专门实践研究来深入理解世界模型中每个任务的角色，并探索它们之间的干扰。</li>
<li>methods: 我们提出了一种简单 yet effective的方法，即和谐世界模型（HarmonyWM），该方法通过维护世界模型学习过程中两个任务之间的动态平衡来提高MBRL的效率。</li>
<li>results: 我们在三个视觉控制领域进行了实验，结果显示，当基于MBRL的方法加以和谐世界模型的改进时，能获得10%-55%的绝对性能提升。<details>
<summary>Abstract</summary>
Model-based reinforcement learning (MBRL) holds the promise of sample-efficient learning by utilizing a world model, which models how the environment works and typically encompasses components for two tasks: observation modeling and reward modeling. In this paper, through a dedicated empirical investigation, we gain a deeper understanding of the role each task plays in world models and uncover the overlooked potential of more efficient MBRL by harmonizing the interference between observation and reward modeling. Our key insight is that while prevalent approaches of explicit MBRL attempt to restore abundant details of the environment through observation models, it is difficult due to the environment's complexity and limited model capacity. On the other hand, reward models, while dominating in implicit MBRL and adept at learning task-centric dynamics, are inadequate for sample-efficient learning without richer learning signals. Capitalizing on these insights and discoveries, we propose a simple yet effective method, Harmony World Models (HarmonyWM), that introduces a lightweight harmonizer to maintain a dynamic equilibrium between the two tasks in world model learning. Our experiments on three visual control domains show that the base MBRL method equipped with HarmonyWM gains 10%-55% absolute performance boosts.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DURENDAL-Graph-deep-learning-framework-for-temporal-heterogeneous-networks"><a href="#DURENDAL-Graph-deep-learning-framework-for-temporal-heterogeneous-networks" class="headerlink" title="DURENDAL: Graph deep learning framework for temporal heterogeneous networks"></a>DURENDAL: Graph deep learning framework for temporal heterogeneous networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00336">http://arxiv.org/abs/2310.00336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manuel Dileo, Matteo Zignani, Sabrina Gaito</li>
<li>for: 这个论文主要针对的是如何应用图神经网络（GNN）来预测动态和多元的网络（THN）中的未来链接。</li>
<li>methods: 该论文提出了一种名为DURENDAL的图深学框架，可以轻松地将异步图学习模型应用到动态网络上。此外，论文还提出了两种更新嵌入表示方法，并对其进行了讨论。</li>
<li>results: 实验表明，DURENDAL在四个动态多元网络 datasets 上的未来链接预测任务中表现出色，比现有的解决方案更具预测力。此外，论文还证明了其模型设计的有效性。<details>
<summary>Abstract</summary>
Temporal heterogeneous networks (THNs) are evolving networks that characterize many real-world applications such as citation and events networks, recommender systems, and knowledge graphs. Although different Graph Neural Networks (GNNs) have been successfully applied to dynamic graphs, most of them only support homogeneous graphs or suffer from model design heavily influenced by specific THNs prediction tasks. Furthermore, there is a lack of temporal heterogeneous networked data in current standard graph benchmark datasets. Hence, in this work, we propose DURENDAL, a graph deep learning framework for THNs. DURENDAL can help to easily repurpose any heterogeneous graph learning model to evolving networks by combining design principles from snapshot-based and multirelational message-passing graph learning models. We introduce two different schemes to update embedding representations for THNs, discussing the strengths and weaknesses of both strategies. We also extend the set of benchmarks for TNHs by introducing two novel high-resolution temporal heterogeneous graph datasets derived from an emerging Web3 platform and a well-established e-commerce website. Overall, we conducted the experimental evaluation of the framework over four temporal heterogeneous network datasets on future link prediction tasks in an evaluation setting that takes into account the evolving nature of the data. Experiments show the prediction power of DURENDAL compared to current solutions for evolving and dynamic graphs, and the effectiveness of its model design.
</details>
<details>
<summary>摘要</summary>
Temporal heterogeneous networks (THNs) 是一种发展中的网络，表现在许多实际应用中，如引用和事件网络、推荐系统和知识图。虽然不同的图神经网络（GNNs）在动态图上得到了成功应用，但大多数其中只支持同质graph或受到特定 THNs 预测任务的设计强烈影响。此外，当前的标准图 benchmark 数据集中缺乏 temporal heterogeneous network 数据。因此，在这项工作中，我们提出了 DURENDAL，一个用于 THNs 的图深度学习框架。DURENDAL 可以帮助将any heterogeneous graph learning model 映射到发展中的网络，通过将 snapshot-based 和多关系消息传递的图学习模型设计原则结合。我们提出了两种不同的 THNs 嵌入表示更新策略，讨论了每个策略的优缺点。此外，我们还扩展了 THNs 的 benchmark 集，通过从一个emerging Web3 平台和一个知名的电商网站 derivated 两个高分辨率的时间含盐多关系图 dataset。总的来说，我们在四个时间含盐多关系图上对 DURENDAL 框架进行了实验评估，以考虑数据的发展性。实验结果表明 DURENDAL 在未来链接预测任务中的预测力与当前的动态和发展图解决方案相比，以及其设计的效果。
</details></li>
</ul>
<hr>
<h2 id="Anomaly-Detection-in-Power-Generation-Plants-with-Generative-Adversarial-Networks"><a href="#Anomaly-Detection-in-Power-Generation-Plants-with-Generative-Adversarial-Networks" class="headerlink" title="Anomaly Detection in Power Generation Plants with Generative Adversarial Networks"></a>Anomaly Detection in Power Generation Plants with Generative Adversarial Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00335">http://arxiv.org/abs/2310.00335</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcellin Atemkeng, Toheeb Aduramomi Jimoh</li>
<li>for: 这个研究是用于探测电力生产设备中的异常点，尤其是用于探测犯罪和相关活动。</li>
<li>methods: 这个研究使用的方法是生成对抗网络（GANs），并在训练和微调过程中使用了数据增强。</li>
<li>results: 研究发现，使用GANs进行异常点探测可以实现高准确率，尤其是在使用大量数据时。在这个研究中，模型的准确率为98.99%，比之前不含数据增强时的准确率（66.45%）高得多。<details>
<summary>Abstract</summary>
Anomaly detection is a critical task that involves the identification of data points that deviate from a predefined pattern, useful for fraud detection and related activities. Various techniques are employed for anomaly detection, but recent research indicates that deep learning methods, with their ability to discern intricate data patterns, are well-suited for this task. This study explores the use of Generative Adversarial Networks (GANs) for anomaly detection in power generation plants. The dataset used in this investigation comprises fuel consumption records obtained from power generation plants operated by a telecommunications company. The data was initially collected in response to observed irregularities in the fuel consumption patterns of the generating sets situated at the company's base stations. The dataset was divided into anomalous and normal data points based on specific variables, with 64.88% classified as normal and 35.12% as anomalous. An analysis of feature importance, employing the random forest classifier, revealed that Running Time Per Day exhibited the highest relative importance. A GANs model was trained and fine-tuned both with and without data augmentation, with the goal of increasing the dataset size to enhance performance. The generator model consisted of five dense layers using the tanh activation function, while the discriminator comprised six dense layers, each integrated with a dropout layer to prevent overfitting. Following data augmentation, the model achieved an accuracy rate of 98.99%, compared to 66.45% before augmentation. This demonstrates that the model nearly perfectly classified data points into normal and anomalous categories, with the augmented data significantly enhancing the GANs' performance in anomaly detection. Consequently, this study recommends the use of GANs, particularly when using large datasets, for effective anomaly detection.
</details>
<details>
<summary>摘要</summary>
异常检测是一项关键任务，涉及到从先定模式中异常出现的数据点的标识，有用于探测诈骗活动等。各种技术被使用于异常检测，但最新的研究表明，深度学习方法，拥有捕捉复杂数据模式的能力，对此任务非常适合。本研究探讨使用生成对抗网络（GANs）进行异常检测在发电厂中。该研究使用的数据集来自一家电信公司运营的发电厂，该数据集包括发电机组的燃料消耗记录。该数据在观察到发电机组的燃料消耗模式异常时被收集。数据集被分为异常和常见数据点，其中64.88%被分类为常见，35.12%被分类为异常。通过特征重要性分析，使用随机森林分类器，显示运行时间每天的相对重要性最高。GANs模型包括五层杂化函数的生成器模型，而批判器则包括六层杂化函数，每个杂化层都有Dropout层来避免过拟合。经过数据增强后，模型达到了98.99%的准确率，比之前增强前的66.45%高得多。这表明模型可以准确地将数据点分类为常见和异常类别，并且增强后的数据对GANs的异常检测性能有很大提升。因此，本研究建议使用GANs，特别是在使用大量数据时， для有效的异常检测。
</details></li>
</ul>
<hr>
<h2 id="Memorization-with-neural-nets-going-beyond-the-worst-case"><a href="#Memorization-with-neural-nets-going-beyond-the-worst-case" class="headerlink" title="Memorization with neural nets: going beyond the worst case"></a>Memorization with neural nets: going beyond the worst case</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00327">http://arxiv.org/abs/2310.00327</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/patrickfinke/memo">https://github.com/patrickfinke/memo</a></li>
<li>paper_authors: Sjoerd Dirksen, Patrick Finke, Martin Genzel</li>
<li>for: 理解深度神经网络易于 interpolate 训练数据的现象，许多研究强调量化神经网络架构的记忆容量。但是，对实际数据来说，一个直观的结构存在，使得 interpolating 已经出现在较小的神经网络大小上。</li>
<li>methods: 我们采用了一种简单的随机算法，给定一个固定的 finite 数据集，高概率地在很短的时间内构建了一个 interpolating 的三层神经网络。该方法的参数数量与数据集中两个类的几何性质和它们之间的排列有关。</li>
<li>results: 我们通过大量的数值实验证明了这种算法的有效性，并将理论结论与实际情况联系起来。<details>
<summary>Abstract</summary>
In practice, deep neural networks are often able to easily interpolate their training data. To understand this phenomenon, many works have aimed to quantify the memorization capacity of a neural network architecture: the largest number of points such that the architecture can interpolate any placement of these points with any assignment of labels. For real-world data, however, one intuitively expects the presence of a benign structure so that interpolation already occurs at a smaller network size than suggested by memorization capacity. In this paper, we investigate interpolation by adopting an instance-specific viewpoint. We introduce a simple randomized algorithm that, given a fixed finite dataset with two classes, with high probability constructs an interpolating three-layer neural network in polynomial time. The required number of parameters is linked to geometric properties of the two classes and their mutual arrangement. As a result, we obtain guarantees that are independent of the number of samples and hence move beyond worst-case memorization capacity bounds. We illustrate the effectiveness of the algorithm in non-pathological situations with extensive numerical experiments and link the insights back to the theoretical results.
</details>
<details>
<summary>摘要</summary>
In this paper, we investigate interpolation by taking an instance-specific viewpoint. We propose a simple randomized algorithm that, given a fixed finite dataset with two classes, can construct an interpolating three-layer neural network in polynomial time. The number of parameters required is linked to the geometric properties of the two classes and their mutual arrangement. As a result, we obtain guarantees that are independent of the number of samples and go beyond worst-case memorization capacity bounds.We demonstrate the effectiveness of the algorithm in non-pathological situations through extensive numerical experiments and link the insights back to the theoretical results.
</details></li>
</ul>
<hr>
<h2 id="Mathematical-structure-of-perfect-predictive-reservoir-computing-for-autoregressive-type-of-time-series-data"><a href="#Mathematical-structure-of-perfect-predictive-reservoir-computing-for-autoregressive-type-of-time-series-data" class="headerlink" title="Mathematical structure of perfect predictive reservoir computing for autoregressive type of time series data"></a>Mathematical structure of perfect predictive reservoir computing for autoregressive type of time series data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00290">http://arxiv.org/abs/2310.00290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tsuyoshi Yoneda</li>
<li>for: 这个论文是用于研究储量计算（RC）神经网络的数学结构的。</li>
<li>methods: 该论文使用了材料学（Wold）分解定理来解释RC神经网络的隐藏结构，并通过对输入和循环权重矩阵的分析来证明RC神经网络可以达到完美预测的水平。</li>
<li>results: 该论文显示了RC神经网络在AR类时间序列数据上的完美预测能力，并证明了其低训练成本、高速度和高计算能力的优势。<details>
<summary>Abstract</summary>
Reservoir Computing (RC) is a type of recursive neural network (RNN), and there can be no doubt that the RC will be more and more widely used for building future prediction models for time-series data, with low training cost, high speed and high computational power. However, research into the mathematical structure of RC neural networks has only recently begun. Bollt (2021) clarified the necessity of the autoregressive (AR) model for gaining the insight into the mathematical structure of RC neural networks, and indicated that the Wold decomposition theorem is the milestone for understanding of these. Keeping this celebrated result in mind, in this paper, we clarify hidden structures of input and recurrent weight matrices in RC neural networks, and show that such structures attain perfect prediction for the AR type of time series data.
</details>
<details>
<summary>摘要</summary>
rezhiyu zhongxin (RC) yisheng yizhi rnn, yige zhisha, RC yisheng yizhi yici yibu zhengxin shi zhengxin yongjian yisheng yizhi yici, gongying yibu zhengxin shi yi yi zhengxin shi yi zhengxin yongjian yisheng yizhi yici. However, RC neural network de yi xiang yu yisheng yizhi yici yongjian yi yi zhengxin shi yi zhengxin yongjian yisheng yizhi yici. Bollt (2021) ying yong zhengxin yisheng yizhi yici yongjian yi yi zhengxin shi yi zhengxin yongjian yisheng yizhi yici, yige zhisha, Wold de zhengxin yisheng yizhi yici yongjian yi yi zhengxin shi yi zhengxin yongjian yisheng yizhi yici. In this paper, we clarify the hidden structures of input and recurrent weight matrices in RC neural networks, and show that such structures achieve perfect prediction for AR type time series data.
</details></li>
</ul>
<hr>
<h2 id="SpatialRank-Urban-Event-Ranking-with-NDCG-Optimization-on-Spatiotemporal-Data"><a href="#SpatialRank-Urban-Event-Ranking-with-NDCG-Optimization-on-Spatiotemporal-Data" class="headerlink" title="SpatialRank: Urban Event Ranking with NDCG Optimization on Spatiotemporal Data"></a>SpatialRank: Urban Event Ranking with NDCG Optimization on Spatiotemporal Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00270">http://arxiv.org/abs/2310.00270</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bang An, Xun Zhou, Yongjian Zhong, Tianbao Yang</li>
<li>for: 预测城市事件的顺位，以优化公共安全和城市管理，特别是在有限资源的情况下。</li>
<li>methods: 提议一种新的空间事件排名方法，名为SpatialRank，它使用自适应图 convolution层学习数据中的空间时间相关性，并且使用混合的NDCG损失函数进行优化。</li>
<li>results: 对三个实际 dataset进行了全面的实验，显示SpatialRank可以有效地预测城市事件的顺位，并且在NDCG指标上与当前状态艺术方法相比，提高了12.7%。<details>
<summary>Abstract</summary>
The problem of urban event ranking aims at predicting the top-k most risky locations of future events such as traffic accidents and crimes. This problem is of fundamental importance to public safety and urban administration especially when limited resources are available. The problem is, however, challenging due to complex and dynamic spatio-temporal correlations between locations, uneven distribution of urban events in space, and the difficulty to correctly rank nearby locations with similar features. Prior works on event forecasting mostly aim at accurately predicting the actual risk score or counts of events for all the locations. Rankings obtained as such usually have low quality due to prediction errors. Learning-to-rank methods directly optimize measures such as Normalized Discounted Cumulative Gain (NDCG), but cannot handle the spatiotemporal autocorrelation existing among locations. In this paper, we bridge the gap by proposing a novel spatial event ranking approach named SpatialRank. SpatialRank features adaptive graph convolution layers that dynamically learn the spatiotemporal dependencies across locations from data. In addition, the model optimizes through surrogates a hybrid NDCG loss with a spatial component to better rank neighboring spatial locations. We design an importance-sampling with a spatial filtering algorithm to effectively evaluate the loss during training. Comprehensive experiments on three real-world datasets demonstrate that SpatialRank can effectively identify the top riskiest locations of crimes and traffic accidents and outperform state-of-art methods in terms of NDCG by up to 12.7%.
</details>
<details>
<summary>摘要</summary>
urbana 事件排名问题目标是预测未来事件 such as traffic accidents和犯罪的 top-k最危险位置。这个问题对公共安全和城市管理非常重要，特别当有限的资源时。然而，这个问题具有复杂的空间时间相关性、不均匀分布的城市事件空间和难以正确排名邻近位置的问题。先前的事件预测方法主要是准确预测所有位置的实际风险分数或事件数量。rankings 得到的质量通常较低，因为预测错误。我们在这篇论文中bridges 这个差距，提出了一种新的城市事件排名方法 named SpatialRank。SpatialRank 特点是动态学习空间时间相关性的适应 граф卷积层。此外，模型还优化了一个混合的NDCG损失函数，以更好地排名邻近空间位置。我们设计了一种importance sampling 的 spatial filtering 算法，以有效评估损失函数在训练中。三个实际数据集的全面实验表明，SpatialRank 可以有效地预测犯罪和交通事故的最危险位置，并在NDCG指标上与状态艺术方法相比提高至12.7%。
</details></li>
</ul>
<hr>
<h2 id="On-Sinkhorn’s-Algorithm-and-Choice-Modeling"><a href="#On-Sinkhorn’s-Algorithm-and-Choice-Modeling" class="headerlink" title="On Sinkhorn’s Algorithm and Choice Modeling"></a>On Sinkhorn’s Algorithm and Choice Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00260">http://arxiv.org/abs/2310.00260</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaonan Qu, Alfred Galichon, Johan Ugander</li>
<li>for: 这 paper 是关于 Luce’s 选择假设下的选择和排名模型的研究，包括 Bradley–Terry–Luce 和 Plackett–Luce 模型。</li>
<li>methods: 这 paper 使用了 Sinkhorn 算法，一种经典的矩阵均衡问题的解决方法，来解决 Luce’s 选择假设下的最大可信度估计问题。</li>
<li>results: 这 paper 显示了 Sinkhorn 算法在非正式矩阵上的全线性归一化速率，并且Characterize 这个全线性归一化速率基于数据中的分布式连接性。此外，paper 还 derive 了关于 Sinkhorn 算法的精确几何归一化速率，这是一个经典结果，但是通过更直观的分析，找到了一个内在的正交结构。<details>
<summary>Abstract</summary>
For a broad class of choice and ranking models based on Luce's choice axiom, including the Bradley--Terry--Luce and Plackett--Luce models, we show that the associated maximum likelihood estimation problems are equivalent to a classic matrix balancing problem with target row and column sums. This perspective opens doors between two seemingly unrelated research areas, and allows us to unify existing algorithms in the choice modeling literature as special instances or analogs of Sinkhorn's celebrated algorithm for matrix balancing. We draw inspirations from these connections and resolve important open problems on the study of Sinkhorn's algorithm. We first prove the global linear convergence of Sinkhorn's algorithm for non-negative matrices whenever finite solutions to the matrix balancing problem exist. We characterize this global rate of convergence in terms of the algebraic connectivity of the bipartite graph constructed from data. Next, we also derive the sharp asymptotic rate of linear convergence, which generalizes a classic result of Knight (2008), but with a more explicit analysis that exploits an intrinsic orthogonality structure. To our knowledge, these are the first quantitative linear convergence results for Sinkhorn's algorithm for general non-negative matrices and positive marginals. The connections we establish in this paper between matrix balancing and choice modeling could help motivate further transmission of ideas and interesting results in both directions.
</details>
<details>
<summary>摘要</summary>
<<SYS>>对一类基于逊的选择axioma的选择和排名模型，包括布拉德利--特里--逊和柏拉克特--逊模型，我们示示出其相关的最大 LIKELIHOOD估计问题与经典的矩阵均衡问题相关。这个视角打开了两个似非相关的研究领域之间的连接，并允许我们将现有的选择模型 литературе中的算法视为矩阵均衡问题的特殊情况或类比。我们从这些连接中继承了想法，并解决了关于Sinkhorn的算法的重要开放问题。我们首先证明了Sinkhorn的算法对非负矩阵的全局线性收敛，当finite solutions存在时。然后，我们 caracterize了这个全局收敛率，以及矩阵均衡问题的解的存在。接下来，我们还 derive了对于一般非负矩阵和正边的情况，Sinkhorn的算法的sharp asymptotic rate of linear convergence，这是一个通过抽象的Orthogonality结构进行更加精细的分析，并扩展了Knight (2008)的经典结果。到我们所知，这些结果是Sinkhorn的算法在总体上的第一个量化线性收敛结果。 connections we establish在这篇论文中 между矩阵均衡和选择模型可能会帮助推动两个方向的想法和结果的传输。
</details></li>
</ul>
<hr>
<h2 id="Learning-State-Augmented-Policies-for-Information-Routing-in-Communication-Networks"><a href="#Learning-State-Augmented-Policies-for-Information-Routing-in-Communication-Networks" class="headerlink" title="Learning State-Augmented Policies for Information Routing in Communication Networks"></a>Learning State-Augmented Policies for Information Routing in Communication Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00248">http://arxiv.org/abs/2310.00248</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sourajitdas/state-augmeted-information-routing">https://github.com/sourajitdas/state-augmeted-information-routing</a></li>
<li>paper_authors: Sourajit Das, Navid NaderiAlizadeh, Alejandro Ribeiro</li>
<li>for: 这 paper 研究了大规模通信网络中信息路由问题，可以视为一个受限制的统计学学习问题，只能使用当地信息。</li>
<li>methods: 该 paper 提出了一种新的状态扩展（SA）策略，通过图神经网络（GNN）架构，在通信网络中启用图 convolution，以便在源节点中最大化总信息。</li>
<li>results: 该 paper 的实验表明，提出的方法可以有效地路由欲要信息到目标节点，并且在实时网络 topology 上进行评估。数值实验表明，该方法比基准算法更好地训练 GNN 参数化。<details>
<summary>Abstract</summary>
This paper examines the problem of information routing in a large-scale communication network, which can be formulated as a constrained statistical learning problem having access to only local information. We delineate a novel State Augmentation (SA) strategy to maximize the aggregate information at source nodes using graph neural network (GNN) architectures, by deploying graph convolutions over the topological links of the communication network. The proposed technique leverages only the local information available at each node and efficiently routes desired information to the destination nodes. We leverage an unsupervised learning procedure to convert the output of the GNN architecture to optimal information routing strategies. In the experiments, we perform the evaluation on real-time network topologies to validate our algorithms. Numerical simulations depict the improved performance of the proposed method in training a GNN parameterization as compared to baseline algorithms.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Bridging-the-Gap-Between-Foundation-Models-and-Heterogeneous-Federated-Learning"><a href="#Bridging-the-Gap-Between-Foundation-Models-and-Heterogeneous-Federated-Learning" class="headerlink" title="Bridging the Gap Between Foundation Models and Heterogeneous Federated Learning"></a>Bridging the Gap Between Foundation Models and Heterogeneous Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00247">http://arxiv.org/abs/2310.00247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sixing Yu, J. Pablo Muñoz, Ali Jannesari</li>
<li>for: 这个论文旨在提出一种适应资源不同的边缘 Federated Learning 系统中实现 Privacy-preserving 的基础模型（Foundation Model），并且能够在不同的资源条件下进行模型优化和部署。</li>
<li>methods: 该论文提出了一种名为 Resource-aware Federated Foundation Models（RaFFM）的框架，该框架使用特殊的模型压缩算法来适应边缘设备的资源限制，例如突出的参数优先级和高性能子网络提取。这些算法允许在Edge FL 系统中动态调整基础模型的大小以适应不同的资源环境。</li>
<li>results: 实验结果表明，RaFFM 可以减少资源使用，同时保持模型性能的水平。具体来说，RaFFM 在自然语言处理和计算机视觉等任务上达到了与传统 Edge FL 方法相同的性能水平，而且占用的资源更少。<details>
<summary>Abstract</summary>
Federated learning (FL) offers privacy-preserving decentralized machine learning, optimizing models at edge clients without sharing private data. Simultaneously, foundation models (FMs) have gained traction in the artificial intelligence (AI) community due to their exceptional performance across various tasks. However, integrating FMs into FL presents challenges, primarily due to their substantial size and intensive resource requirements. This is especially true when considering the resource heterogeneity in edge FL systems. We present an adaptive framework for Resource-aware Federated Foundation Models (RaFFM) to address these challenges. RaFFM introduces specialized model compression algorithms tailored for FL scenarios, such as salient parameter prioritization and high-performance subnetwork extraction. These algorithms enable dynamic scaling of given transformer-based FMs to fit heterogeneous resource constraints at the network edge during both FL's optimization and deployment stages. Experimental results demonstrate that RaFFM shows significant superiority in resource utilization efficiency and uses fewer resources to deploy FMs to FL. Despite the lower resource consumption, target models optimized by RaFFM achieve performance on par with traditional FL methods applied to full-sized FMs. This is evident across tasks in both natural language processing and computer vision domains.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 提供隐私保护的分布式机器学习，在边缘客户端上调参模型而不需要共享私有数据。同时，基础模型 (FM) 在人工智能 (AI) 领域得到了广泛的应用，因为它们在多种任务上表现出色。然而，将 FM integrated into FL 存在挑战，主要是因为它们的较大的大小和资源占用。这pecially true when considering the resource heterogeneity in edge FL systems。我们提出了适应性的 Federated Foundation Models (RaFFM) 框架，以解决这些挑战。RaFFM 引入了特殊的模型压缩算法，适用于 FL 场景，如突出参数优化和高性能子网络提取。这些算法允许在边缘网络中动态scaling给定的 transformer-based FMs，以适应不同资源限制。实验结果表明，RaFFM 在资源利用效率方面表现出显著的优势，并使用 fewer resources 来部署 FMs to FL。尽管资源占用量下降，由 RaFFM 优化的目标模型仍然能够与传统 FL 方法应用于全大小 FMs 的性能相当。这是在自然语言处理和计算机视觉领域中的多个任务上得到证明。
</details></li>
</ul>
<hr>
<h2 id="A-hybrid-quantum-classical-conditional-generative-adversarial-network-algorithm-for-human-centered-paradigm-in-cloud"><a href="#A-hybrid-quantum-classical-conditional-generative-adversarial-network-algorithm-for-human-centered-paradigm-in-cloud" class="headerlink" title="A hybrid quantum-classical conditional generative adversarial network algorithm for human-centered paradigm in cloud"></a>A hybrid quantum-classical conditional generative adversarial network algorithm for human-centered paradigm in cloud</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00246">http://arxiv.org/abs/2310.00246</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjie Liu, Ying Zhang, Zhiliang Deng, Jiaojiao Zhao, Lian Tong</li>
<li>For: The paper aims to improve the quantum generative adversarial network (QGAN) algorithm to conform to the human-centered paradigm, and to solve the problems of random generation and lack of human-computer interaction in QGAN.* Methods: The proposed algorithm, called hybrid quantum-classical conditional generative adversarial network (QCGAN), combines quantum and classical computing to achieve a knowledge-driven human-computer interaction computing mode. The generator uses a parameterized quantum circuit with an all-to-all connected topology, while the discriminator uses a classical neural network.* Results: The QCGAN algorithm can effectively converge to the Nash equilibrium point after training and perform human-centered classification generation tasks, as demonstrated on the quantum cloud computing platform using the BAS training set.<details>
<summary>Abstract</summary>
As an emerging field that aims to bridge the gap between human activities and computing systems, human-centered computing (HCC) in cloud, edge, fog has had a huge impact on the artificial intelligence algorithms. The quantum generative adversarial network (QGAN) is considered to be one of the quantum machine learning algorithms with great application prospects, which also should be improved to conform to the human-centered paradigm. The generation process of QGAN is relatively random and the generated model does not conform to the human-centered concept, so it is not quite suitable for real scenarios. In order to solve these problems, a hybrid quantum-classical conditional generative adversarial network (QCGAN) algorithm is proposed, which is a knowledge-driven human-computer interaction computing mode that can be implemented in cloud. The purposes of stabilizing the generation process and realizing the interaction between human and computing process are achieved by inputting artificial conditional information in the generator and discriminator. The generator uses the parameterized quantum circuit with an all-to-all connected topology, which facilitates the tuning of network parameters during the training process. The discriminator uses the classical neural network, which effectively avoids the "input bottleneck" of quantum machine learning. Finally, the BAS training set is selected to conduct experiment on the quantum cloud computing platform. The result shows that the QCGAN algorithm can effectively converge to the Nash equilibrium point after training and perform human-centered classification generation tasks.
</details>
<details>
<summary>摘要</summary>
traditional Chinese:As an emerging field that aims to bridge the gap between human activities and computing systems, human-centered computing (HCC) in cloud, edge, fog has had a huge impact on artificial intelligence algorithms. The quantum generative adversarial network (QGAN) is considered to be one of the quantum machine learning algorithms with great application prospects, which also should be improved to conform to the human-centered paradigm. The generation process of QGAN is relatively random and the generated model does not conform to the human-centered concept, so it is not quite suitable for real scenarios. In order to solve these problems, a hybrid quantum-classical conditional generative adversarial network (QCGAN) algorithm is proposed, which is a knowledge-driven human-computer interaction computing mode that can be implemented in cloud. The purposes of stabilizing the generation process and realizing the interaction between human and computing process are achieved by inputting artificial conditional information in the generator and discriminator. The generator uses the parameterized quantum circuit with an all-to-all connected topology, which facilitates the tuning of network parameters during the training process. The discriminator uses the classical neural network, which effectively avoids the "input bottleneck" of quantum machine learning. Finally, the BAS training set is selected to conduct experiment on the quantum cloud computing platform. The result shows that the QCGAN algorithm can effectively converge to the Nash equilibrium point after training and perform human-centered classification generation tasks.Simplified Chinese:作为一个崛起的领域，人类活动与计算系统之间的桥梁，人 centered computing（HCC）在云、边缘、fog中已经产生了巨大的影响。量子生成对抗网络（QGAN）是一种具有广泛应用前景的量子机器学习算法，但它的生成过程相对Random，生成的模型不符合人类中心的概念，因此不太适合实际应用。为解决这些问题，我们提出了一种半量子半类 condensed generative adversarial network（QCGAN）算法，这是一种基于知识驱动的人机交互计算模式，可以在云上实现。通过在生成器和识别器中输入人工条件信息，实现了生成过程的稳定化和人机交互的实现。生成器使用具有所有连接的量子Circuit，方便在训练过程中调整网络参数。识别器使用классиical神经网络，有效地避免了量子机器学习的输入瓶颈。最后，选择了BAS训练集，在量子云计算平台上进行实验。结果表明，QCGAN算法可以在训练后快速平衡到纳什平衡点，并完成人类中心的分类生成任务。
</details></li>
</ul>
<hr>
<h2 id="CausalImages-An-R-Package-for-Causal-Inference-with-Earth-Observation-Bio-medical-and-Social-Science-Images"><a href="#CausalImages-An-R-Package-for-Causal-Inference-with-Earth-Observation-Bio-medical-and-Social-Science-Images" class="headerlink" title="CausalImages: An R Package for Causal Inference with Earth Observation, Bio-medical, and Social Science Images"></a>CausalImages: An R Package for Causal Inference with Earth Observation, Bio-medical, and Social Science Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00233">http://arxiv.org/abs/2310.00233</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AIandGlobalDevelopmentLab/causalimages-software">https://github.com/AIandGlobalDevelopmentLab/causalimages-software</a></li>
<li>paper_authors: Connor T. Jerzak, Adel Daoud</li>
<li>for: 用于描述 causal inference 的图像和视频数据的研究</li>
<li>methods: 使用 Bayesian 框架进行图像基 causal inference 分析，并使用图像来控制干扰因素</li>
<li>results: 可以快速和可读地进行大规模图像和视频数据分析，并提供vector评估图像或视频内容的功能<details>
<summary>Abstract</summary>
The causalimages R package enables causal inference with image and image sequence data, providing new tools for integrating novel data sources like satellite and bio-medical imagery into the study of cause and effect. One set of functions enables image-based causal inference analyses. For example, one key function decomposes treatment effect heterogeneity by images using an interpretable Bayesian framework. This allows for determining which types of images or image sequences are most responsive to interventions. A second modeling function allows researchers to control for confounding using images. The package also allows investigators to produce embeddings that serve as vector summaries of the image or video content. Finally, infrastructural functions are also provided, such as tools for writing large-scale image and image sequence data as sequentialized byte strings for more rapid image analysis. causalimages therefore opens new capabilities for causal inference in R, letting researchers use informative imagery in substantive analyses in a fast and accessible manner.
</details>
<details>
<summary>摘要</summary>
causalimages 包可以帮助研究者进行 causal inference 分析，使用图像和视频数据。这个包提供了一些新的函数，可以帮助研究者将新的数据源，如卫星和生物医学图像，integrated 到 causal inference 中。一个关键函数可以用 Bayesian 框架来划分干扰效应，以确定哪些图像或视频序列是干扰干扰最强的。另一个函数可以帮助研究者控制干扰，使用图像。此外，包还提供了一些基础函数，如用于将大规模图像和视频数据作为字节流进行Sequentialized 的存储和分析。因此，causalimages 开启了新的可能性，让研究者通过使用有用的图像，在快速和可 accessible 的方式进行 causal inference 分析。
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Non-IID-Federated-Learning-via-Heterogeneity-Guided-Client-Sampling"><a href="#Accelerating-Non-IID-Federated-Learning-via-Heterogeneity-Guided-Client-Sampling" class="headerlink" title="Accelerating Non-IID Federated Learning via Heterogeneity-Guided Client Sampling"></a>Accelerating Non-IID Federated Learning via Heterogeneity-Guided Client Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00198">http://arxiv.org/abs/2310.00198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huancheng Chen, Haris Vikalo</li>
<li>for: 这个研究目的是提出一种基于层次归一化采样的 federated learning（FL）客户端选择方法，以便在非同一的数据中训练全球模型。</li>
<li>methods: 这个方法使用客户端的更新来估算客户端数据的统计差异，然后使用 Hierarchical Clustered Sampling（层次归一化采样）将客户端分为不同的群组，以选择更有价值的客户端参与训练。</li>
<li>results: 实验结果显示，在非同一的数据设置下，HiCS-FL可以快速到达训练目标，并且比前一代FL客户端选择方法具有更低的训练方差和更高的可攻击性。<details>
<summary>Abstract</summary>
Statistical heterogeneity of data present at client devices in a federated learning (FL) system renders the training of a global model in such systems difficult. Particularly challenging are the settings where due to resource constraints only a small fraction of clients can participate in any given round of FL. Recent approaches to training a global model in FL systems with non-IID data have focused on developing client selection methods that aim to sample clients with more informative updates of the model. However, existing client selection techniques either introduce significant computation overhead or perform well only in the scenarios where clients have data with similar heterogeneity profiles. In this paper, we propose HiCS-FL (Federated Learning via Hierarchical Clustered Sampling), a novel client selection method in which the server estimates statistical heterogeneity of a client's data using the client's update of the network's output layer and relies on this information to cluster and sample the clients. We analyze the ability of the proposed techniques to compare heterogeneity of different datasets, and characterize convergence of the training process that deploys the introduced client selection method. Extensive experimental results demonstrate that in non-IID settings HiCS-FL achieves faster convergence and lower training variance than state-of-the-art FL client selection schemes. Notably, HiCS-FL drastically reduces computation cost compared to existing selection schemes and is adaptable to different heterogeneity scenarios.
</details>
<details>
<summary>摘要</summary>
在 Federated Learning (FL) 系统中，数据在客户端设备上存在统计差异，使得全球模型的训练变得困难。特别是在资源限制下，只有一小部分客户可以参与每个轮次的 FL 训练。现有的 FL 客户选择方法 Either introduce significant computation overhead or perform well only in scenarios where clients have similar heterogeneity profiles. In this paper, we propose HiCS-FL (Federated Learning via Hierarchical Clustered Sampling), a novel client selection method in which the server estimates the statistical heterogeneity of a client's data based on the client's update of the network's output layer and relies on this information to cluster and sample the clients. We analyze the ability of the proposed techniques to compare the heterogeneity of different datasets and characterize the convergence of the training process that deploys the introduced client selection method. Extensive experimental results demonstrate that in non-IID settings, HiCS-FL achieves faster convergence and lower training variance than state-of-the-art FL client selection schemes. Notably, HiCS-FL drastically reduces computation cost compared to existing selection schemes and is adaptable to different heterogeneity scenarios.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/30/cs.LG_2023_09_30/" data-id="clp869u0j00sbk5882rzv39l8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_09_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/30/eess.IV_2023_09_30/" class="article-date">
  <time datetime="2023-09-30T09:00:00.000Z" itemprop="datePublished">2023-09-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/30/eess.IV_2023_09_30/">eess.IV - 2023-09-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Chaos-and-COSMOS-–-Considerations-on-QSM-methods-with-multiple-and-single-orientations-and-effects-from-local-anisotropy"><a href="#Chaos-and-COSMOS-–-Considerations-on-QSM-methods-with-multiple-and-single-orientations-and-effects-from-local-anisotropy" class="headerlink" title="Chaos and COSMOS – Considerations on QSM methods with multiple and single orientations and effects from local anisotropy"></a>Chaos and COSMOS – Considerations on QSM methods with multiple and single orientations and effects from local anisotropy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00392">http://arxiv.org/abs/2310.00392</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitrios G. Gkotsoulias, Carsten Jäger, Roland Müller, Tobias Gräßle, Karin M. Olofsson, Torsten Møller, Steve Unwin, Catherine Crockford, Roman M. Wittig, Berkin Bilgic, Harald E. Möller</li>
<li>For: The paper aims to investigate the effects of non-ideal sampling and susceptibility anisotropy on the accuracy of quantitative susceptibility mapping (QSM) in the primate brain using the COSMOS method.* Methods: The authors used gradient-recalled echo (GRE) data of an entire fixed chimpanzee brain acquired at 7 T, including ideal COSMOS sampling and realistic rotations in vivo. They compared the results with ideal COSMOS, in-vivo feasible acquisitions with 3-8 orientations, and single-orientation iLSQR QSM.* Results: The authors found that in-vivo feasible and optimal COSMOS yielded high-quality susceptibility maps with increased signal-to-noise ratio (SNR) resulting from averaging multiple acquisitions. However, COSMOS reconstructions from non-ideal rotations about a single axis required additional L2-regularization to mitigate residual streaking artifacts.Here is the same information in Simplified Chinese text:* For: 这篇论文目的是 investigate COSMOS方法在非理想样本和磁矢相变的情况下，量子透镜 mapping (QSM) 在 primate 大脑中的准确性。* Methods: 作者使用了 gradient-recalled echo (GRE) 数据，包括理想的 COSMOS 样本和在 vivo 实际旋转。他们比较了理想的 COSMOS，在 vivo 可行的 3-8 个orientation 和单 orientation iLSQR QSM。* Results: 作者发现，在 vivo 可行和优化的 COSMOS 可以生成高质量的抵抗强度图，并通过多个取样提高信号响应比 (SNR)。然而，在非理想的单轴旋转情况下，需要额外的 L2 正则化来mitigate 剩下的扫描 artifacts。<details>
<summary>Abstract</summary>
Purpose: Field-to-susceptibility inversion in quantitative susceptibility mapping (QSM) is ill-posed and needs numerical stabilization through either regularization or oversampling by acquiring data at three or more object orientations. Calculation Of Susceptibility through Multiple Orientations Sampling (COSMOS) is an established oversampling approach and regarded as QSM gold standard. It achieves a well-conditioned inverse problem, requiring rotations by 0{\deg}, 60{\deg} and 120{\deg} in the yz-plane. However, this is impractical in vivo, where head rotations are typically restricted to a range of +-25{\deg}. Non-ideal sampling degrades the conditioning with residual streaking artifacts whose mitigation needs further regularization. Moreover, susceptibility anisotropy in white matter is not considered in the COSMOS model, which may introduce additional bias. The current work presents a thorough investigation of these effects in primate brain.   Methods: Gradient-recalled echo (GRE) data of an entire fixed chimpanzee brain were acquired at 7 T (350 microns resolution, 10 orientations) including ideal COSMOS sampling and realistic rotations in vivo. Comparisons of the results included ideal COSMOS, in-vivo feasible acquisitions with 3-8 orientations and single-orientation iLSQR QSM.   Results: In-vivo feasible and optimal COSMOS yielded high-quality susceptibility maps with increased SNR resulting from averaging multiple acquisitions. COSMOS reconstructions from non-ideal rotations about a single axis required additional L2-regularization to mitigate residual streaking artifacts.   Conclusion: In view of unconsidered anisotropy effects, added complexity of the reconstruction, and the general challenge of multi-orientation acquisitions, advantages of sub-optimal COSMOS schemes over regularized single-orientation QSM appear limited in in-vivo settings.
</details>
<details>
<summary>摘要</summary>
目的：在量化感受mapping（QSM）中，场到感受性的倒置是不稳定的，需要数值稳定化通过Regularization或多样化数据收集。多样化样本收集（COSMOS）是已确立的多样化方法，被视为QSM的金标准。它实现了一个良好的倒置问题，需要在yz平面上进行0°、60°和120°的旋转。然而，这在生物体内不是实际可行的，因为头部旋转通常只能在+/-25°的范围内进行。非理想的抽象会导致倒置的稳定性受损，并需要进一步的Regularization来缓解这些残留的扭曲artefacts。此外，感受性不均匀在白 mater中的影响也没有考虑在COSMOS模型中，可能会引入额外的偏见。本研究对这些效果进行了全面的调查，并使用了7T的扩展echo（GRE）数据，包括理想的COSMOS样本和生物体内实际可行的旋转。方法：使用7T的GRE数据，包括理想的COSMOS样本和生物体内实际可行的旋转，并对比以下方法：理想的COSMOS、生物体内可行的多orientation iLSQR QSM和单orientation iLSQR QSM。结果：生物体内可行的多orientation COSMOS和理想的COSMOS可以提供高质量的感受度图，提高Signal-to-noise ratio（SNR）通过多次抽样。COSMOS重建从非理想旋转中需要额外的L2正则化来缓解剩下的扭曲artefacts。结论：由于不考虑感受性不均匀的影响，加之重建复杂化的问题，以及生物体内多orientation收集的挑战，对于在生物体内设置的情况，优于单orientation QSM的COSMOS方法的优势可能有限。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/30/eess.IV_2023_09_30/" data-id="clp869u7i01auk5881nn7cqsk" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_09_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/30/eess.SP_2023_09_30/" class="article-date">
  <time datetime="2023-09-30T08:00:00.000Z" itemprop="datePublished">2023-09-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/30/eess.SP_2023_09_30/">eess.SP - 2023-09-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Analysis-of-system-capacity-and-spectral-efficiency-of-fixed-grid-network"><a href="#Analysis-of-system-capacity-and-spectral-efficiency-of-fixed-grid-network" class="headerlink" title="Analysis of system capacity and spectral efficiency of fixed-grid network"></a>Analysis of system capacity and spectral efficiency of fixed-grid network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00395">http://arxiv.org/abs/2310.00395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adarsha M, S. Malathi, Santosh Kumar<br>for: 这种研究旨在调查固定格式网络在不同的调制格式下的性能，以估算系统的容量和spectral efficiency。methods: 该研究使用了光学干扰 quadrature modulator结构建立固定格式网络调制，并使用了homodyne探测方法。数据多路复用使用了极化分多路复用技术。results: 在这些情况下，使用不同的调制格式实现了100Gbps、150Gbps和200Gbps的数据速率。使用现代数字信号处理系统，对于PM-QPSK、PM-8QAM和PM-16QAM等不同的调制方式，分别实现了2、3和4位&#x2F;s&#x2F;Hz的spectrum efficiency。不同的调制方式每种系统容量分别为8-9、12-13.5和16-18Tbps，可以在3000、1300和700公里的传输距离上实现接受比特错误率低于等于2*10-3。<details>
<summary>Abstract</summary>
In this article, the performance of a fixed grid network is examined for various modulation formats to estimate the system's capacity and spectral efficiency. The optical In-phase Quadrature Modulator structure is used to build a fixed grid network modulation, and the homodyne detection approach is used for the receiver. Data multiplexing is accomplished using the Polarization Division Multiplexed technology. 100 Gbps, 150 Gbps, and 200 Gbps data rates are transmitted under these circumstances utilizing various modulation formats. Various pre-processing and signal recovery steps are explained by using modern digital signal processing systems. The achieved spectrum efficiencies for PM-QPSK, PM-8 QAM, and PM-16 QAM, respectively, were 2, 3, and 4 bits/s/Hz. Different modulation like PM-QPSK, PM-8-QAM, and PM-16-QAM each has system capacities of 8-9, 12-13.5, and 16-18 Tbps and it reaches transmission distances of 3000, 1300, and 700 kilometers with acceptable Bit Error Rate less than equal to 2*10-3 respectively. Peak optical power for received signal detection and full width at half maximum is noted for the different modulations under a fixed grind network.
</details>
<details>
<summary>摘要</summary>
在这篇文章中，我们研究了固定格网络的性能，用以估算系统的容量和 spectral efficiency。我们使用了光学干扰 quadrature modulator 结构来建立固定格网络的模ulation，并使用了 homodyne 探测方法来接收器。数据多路复用使用了极化分 multiplexing 技术。在这些情况下，我们将100Gbps、150Gbps和200Gbps的数据速率传输，使用不同的模ulation format。我们还介绍了使用现代数字信号处理系统来实现预处理和信号恢复步骤。我们测得了 PM-QPSK、PM-8QAM 和 PM-16QAM 模ulation的spectrum efficiency分别为2、3和4 bits/s/Hz。不同的模ulation各自有系统容量为8-9、12-13.5和16-18 Tbps，可以在3000、1300和700公里的传输距离上实现可接受的 Bit Error Rate 低于等于2*10-3。我们还注意到了不同模ulation的接收信号检测的峰值光学功率和半宽度。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Secrecy-in-UAV-RSMA-Networks-Deep-Unfolding-Meets-Deep-Reinforcement-Learning"><a href="#Enhancing-Secrecy-in-UAV-RSMA-Networks-Deep-Unfolding-Meets-Deep-Reinforcement-Learning" class="headerlink" title="Enhancing Secrecy in UAV RSMA Networks: Deep Unfolding Meets Deep Reinforcement Learning"></a>Enhancing Secrecy in UAV RSMA Networks: Deep Unfolding Meets Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01437">http://arxiv.org/abs/2310.01437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abuzar B. M. Adam, Mohammed A. M. Elhassan</li>
<li>for: 这 paper 是研究多架空无人机（UAV）网络中的秘密率最大化问题。</li>
<li>methods: 这 paper 使用了一个组合的射频、速率分配和无人机轨迹优化问题，这个问题是非核心的。因此，它被转换为一个Markov问题，并使用了一个新的多代理深度学习（DRL）框架，称为DUN-DRL。</li>
<li>results: 该 paper 的结果显示，DUN-DRL 比其他 literatura 中的 DRL 方法表现更好，并且可以实现更高的秘密率。<details>
<summary>Abstract</summary>
In this paper, we consider the maximization of the secrecy rate in multiple unmanned aerial vehicles (UAV) rate-splitting multiple access (RSMA) network. A joint beamforming, rate allocation, and UAV trajectory optimization problem is formulated which is nonconvex. Hence, the problem is transformed into a Markov decision problem and a novel multiagent deep reinforcement learning (DRL) framework is designed. The proposed framework (named DUN-DRL) combines deep unfolding to design beamforming and rate allocation, data-driven to design the UAV trajectory, and deep deterministic policy gradient (DDPG) for the learning procedure. The proposed DUN-DRL have shown great performance and outperformed other DRL-based methods in the literature.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们考虑了多架空航天器（UAV）率分访问（RSMA）网络中的最大秘密率增加问题。我们将这个问题转化为一个非核心的Markov决策问题，并提出了一种基于多代理深度学习（DRL）的新框架（名为DUN-DRL）。该框架结合深度嵌入设计灵敏谱和率分配，基于数据驱动设计UAV轨迹，并使用深度确定策略梯度（DDPG）进行学习过程。我们的DUN-DRL方法在文献中表现出色，并比其他基于DRL的方法表现更好。
</details></li>
</ul>
<hr>
<h2 id="RIS-aided-Near-Field-MIMO-Communications-Codebook-and-Beam-Training-Design"><a href="#RIS-aided-Near-Field-MIMO-Communications-Codebook-and-Beam-Training-Design" class="headerlink" title="RIS-aided Near-Field MIMO Communications: Codebook and Beam Training Design"></a>RIS-aided Near-Field MIMO Communications: Codebook and Beam Training Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00294">http://arxiv.org/abs/2310.00294</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suyu Lv, Yuanwei Liu, Xiaodong Xu, Arumugam Nallanathan, A. Lee Swindlehurst</li>
<li>for: 这个论文旨在研究下降式智能表面（RIS）助持的多输入多输出（MIMO）系统，包括远场、近场和混合远近场通道。</li>
<li>methods: 根据接收信号中包含的角度或距离信息，提出了基于近场MIMO通道的距离基本码字典、混合角度距离码字典和二阶段扫描方案，以减少培训负担。</li>
<li>results: 比较研究表明，提出的扫描方案可以实现近似优化性，同时减少培训负担；相比于只考虑角度信息，将距离信息包含在扫描过程中可以明显提高可得率。<details>
<summary>Abstract</summary>
Downlink reconfigurable intelligent surface (RIS)-assisted multi-input-multi-output (MIMO) systems are considered with far-field, near-field, and hybrid-far-near-field channels. According to the angular or distance information contained in the received signals, 1) a distance-based codebook is designed for near-field MIMO channels, based on which a hierarchical beam training scheme is proposed to reduce the training overhead; 2) a combined angular-distance codebook is designed for mixed-far-near-field MIMO channels, based on which a two-stage beam training scheme is proposed to achieve alignment in the angular and distance domains separately. For maximizing the achievable rate while reducing the complexity, an alternating optimization algorithm is proposed to carry out the joint optimization iteratively. Specifically, the RIS coefficient matrix is optimized through the beam training process, the optimal combining matrix is obtained from the closed-form solution for the mean square error (MSE) minimization problem, and the active beamforming matrix is optimized by exploiting the relationship between the achievable rate and MSE. Numerical results reveal that: 1) the proposed beam training schemes achieve near-optimal performance with a significantly decreased training overhead; 2) compared to the angular-only far-field channel model, taking the additional distance information into consideration will effectively improve the achievable rate when carrying out beam design for near-field communications.
</details>
<details>
<summary>摘要</summary>
下链接智能表面（RIS）助enciphered多输入多输出（MIMO）系统被考虑，包括远场、近场和混合远近场通道。根据接收信号中的角度或距离信息，提出了以下方案：1. 基于近场MIMO通道的距离编码本是基于近场MIMO通道的距离编码本，提出了层次扫描训练方案，以减少训练负担。2. 基于混合远近场MIMO通道的混合角度距离编码本是基于混合远近场MIMO通道的混合角度距离编码本，提出了两stage扫描训练方案，以在angular和距离域分别实现对alignment。3. 为了最大化可达速率而减少复杂性，提出了alternating optimization算法，以iteratively进行共享优化。具体来说，RIS减少系数矩阵通过扫描训练过程进行优化，可能性最小化问题中的closed-form解决方案中得到最佳组合矩阵，并通过利用可达速率和MSE之间的关系来优化活动扩散矩阵。数据示出：1. 提出的扫描训练方案可以实现近似最佳性，同时减少训练负担；2. 在进行 beam设计时，考虑到距离信息的情况下，与angular-only远场通道模型相比，能够更有效地提高可达速率。
</details></li>
</ul>
<hr>
<h2 id="RIS-Aided-Cell-Free-Massive-MIMO-Systems-for-6G-Fundamentals-System-Design-and-Applications"><a href="#RIS-Aided-Cell-Free-Massive-MIMO-Systems-for-6G-Fundamentals-System-Design-and-Applications" class="headerlink" title="RIS-Aided Cell-Free Massive MIMO Systems for 6G: Fundamentals, System Design, and Applications"></a>RIS-Aided Cell-Free Massive MIMO Systems for 6G: Fundamentals, System Design, and Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00263">http://arxiv.org/abs/2310.00263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enyu Shi, Jiayi Zhang, Hongyang Du, Bo Ai, Chau Yuen, Dusit Niyato, Khaled B. Letaief, Xuemin Shen</li>
<li>for: 这篇论文主要针对 sixth-generation (6G) 网络的 intelligent interconnectivity 问题，旨在提高spectral efficiency和energy efficiency、ultra-low latency和ultra-high reliability等性能指标。</li>
<li>methods: 这篇论文主要使用 cell-free (CF) massive multiple-input multiple-output (mMIMO) 和 reconfigurable intelligent surface (RIS) 等技术，以提高无线网络性能。</li>
<li>results: 论文提供了 RIS-aided CF mMIMO 无线通信系统的全面检讨，包括系统体系和应用场景、通信协议和channel模型等方面的概述，以及系统操作和资源分配等方面的深入分析。<details>
<summary>Abstract</summary>
An introduction of intelligent interconnectivity for people and things has posed higher demands and more challenges for sixth-generation (6G) networks, such as high spectral efficiency and energy efficiency, ultra-low latency, and ultra-high reliability. Cell-free (CF) massive multiple-input multiple-output (mMIMO) and reconfigurable intelligent surface (RIS), also called intelligent reflecting surface (IRS), are two promising technologies for coping with these unprecedented demands. Given their distinct capabilities, integrating the two technologies to further enhance wireless network performances has received great research and development attention. In this paper, we provide a comprehensive survey of research on RIS-aided CF mMIMO wireless communication systems. We first introduce system models focusing on system architecture and application scenarios, channel models, and communication protocols. Subsequently, we summarize the relevant studies on system operation and resource allocation, providing in-depth analyses and discussions. Following this, we present practical challenges faced by RIS-aided CF mMIMO systems, particularly those introduced by RIS, such as hardware impairments and electromagnetic interference. We summarize corresponding analyses and solutions to further facilitate the implementation of RIS-aided CF mMIMO systems. Furthermore, we explore an interplay between RIS-aided CF mMIMO and other emerging 6G technologies, such as next-generation multiple-access (NGMA), simultaneous wireless information and power transfer (SWIPT), and millimeter wave (mmWave). Finally, we outline several research directions for future RIS-aided CF mMIMO systems.
</details>
<details>
<summary>摘要</summary>
sixth-generation（6G）网络面临更高的要求和挑战，如高频率效率和能效性、超低延迟和超高可靠性。各种技术，如无绳（CF）大量多输入多输出（mMIMO）和智能反射表面（RIS），也被认为是应对这些前所未有的挑战的两种有前途的技术。 Given their distinct capabilities, integrating the two technologies to further enhance wireless network performances has received great research and development attention. In this paper, we provide a comprehensive survey of research on RIS-aided CF mMIMO wireless communication systems. We first introduce system models focusing on system architecture and application scenarios, channel models, and communication protocols. Subsequently, we summarize the relevant studies on system operation and resource allocation, providing in-depth analyses and discussions. Following this, we present practical challenges faced by RIS-aided CF mMIMO systems, particularly those introduced by RIS, such as hardware impairments and electromagnetic interference. We summarize corresponding analyses and solutions to further facilitate the implementation of RIS-aided CF mMIMO systems. Furthermore, we explore an interplay between RIS-aided CF mMIMO and other emerging 6G technologies, such as next-generation multiple-access (NGMA), simultaneous wireless information and power transfer (SWIPT), and millimeter wave (mmWave). Finally, we outline several research directions for future RIS-aided CF mMIMO systems.
</details></li>
</ul>
<hr>
<h2 id="Identifying-Distribution-Network-Faults-Using-Adaptive-Transition-Probability"><a href="#Identifying-Distribution-Network-Faults-Using-Adaptive-Transition-Probability" class="headerlink" title="Identifying Distribution Network Faults Using Adaptive Transition Probability"></a>Identifying Distribution Network Faults Using Adaptive Transition Probability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00255">http://arxiv.org/abs/2310.00255</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinliang Ma, Weihua Liu, Bingying Jin</li>
<li>for: 提高分布网络中故障检测精度</li>
<li>methods:  combines adaptive probability learning和波形分解，用于优化特征相似性</li>
<li>results: 实验结果表明，该方法在使用适应学习条件时，能够超越常用的分类模型，如卷积神经网络、支持向量机和k-最近邻居分类器，特别是在限制样本大小的情况下。<details>
<summary>Abstract</summary>
A novel approach is suggested for improving the accuracy of fault detection in distribution networks. This technique combines adaptive probability learning and waveform decomposition to optimize the similarity of features. Its objective is to discover the most appropriate linear mapping between simulated and real data to minimize distribution differences. By aligning the data in the same feature space, the proposed method effectively overcomes the challenge posed by limited sample size when identifying faults and classifying real data in distribution networks. Experimental results utilizing simulated system data and real field data demonstrate that this approach outperforms commonly used classification models such as convolutional neural networks, support vector machines, and k-nearest neighbors, especially under adaptive learning conditions. Consequently, this research provides a fresh perspective on fault detection in distribution networks, particularly when adaptive learning conditions are employed.
</details>
<details>
<summary>摘要</summary>
一种新的方法建议用于改进分布网络中故障探测精度。这种技术结合适应概率学习和波形分解，以优化特征相似性。其目标是找到最适合的线性映射，使得实际数据和模拟数据在同一个特征空间内进行对应。通过对数据进行对应，该方法有效地超越了有限样本大小的挑战，使得在分布网络中探测FAULTS和分类实际数据时更加准确。实验结果表明，该方法在使用适应学习条件时，可以超越常用的分类模型，如卷积神经网络、支持向量机和k-最近邻居等，尤其是在适应学习条件下。因此，这种研究提供了一个新的视角，用于分布网络中的故障探测，特别是在使用适应学习条件时。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Approach-for-Adaptive-EMG-Pattern-Classification-Via-Semi-Supervised-Sequential-Learning"><a href="#Bayesian-Approach-for-Adaptive-EMG-Pattern-Classification-Via-Semi-Supervised-Sequential-Learning" class="headerlink" title="Bayesian Approach for Adaptive EMG Pattern Classification Via Semi-Supervised Sequential Learning"></a>Bayesian Approach for Adaptive EMG Pattern Classification Via Semi-Supervised Sequential Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00252">http://arxiv.org/abs/2310.00252</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seitaro Yoneda, Akira Furui</li>
<li>for: 这个研究旨在开发一种基于电omyogram (EMG) 信号的人机界面，并使用模式识别估算执行的人类动作。</li>
<li>methods: 这种方法使用 bayesian 类别模型，基于 Gaussian 分布来预测动作类别并估算其信心程度。</li>
<li>results: 实验结果显示，提案的方法可以降低时间的类别精度下降，并且比传统方法高效。这些结果证明了提案的方法的有效性，并适用于实际应用中的 EMG 基本控制系统。<details>
<summary>Abstract</summary>
Intuitive human-machine interfaces may be developed using pattern classification to estimate executed human motions from electromyogram (EMG) signals generated during muscle contraction. The continual use of EMG-based interfaces gradually alters signal characteristics owing to electrode shift and muscle fatigue, leading to a gradual decline in classification accuracy. This paper proposes a Bayesian approach for adaptive EMG pattern classification using semi-supervised sequential learning. The proposed method uses a Bayesian classification model based on Gaussian distributions to predict the motion class and estimate its confidence. Pseudo-labels are subsequently assigned to data with high-prediction confidence, and the posterior distributions of the model are sequentially updated within the framework of Bayesian updating, thereby achieving adaptive motion recognition to alterations in signal characteristics over time. Experimental results on six healthy adults demonstrated that the proposed method can suppress the degradation of classification accuracy over time and outperforms conventional methods. These findings demonstrate the validity of the proposed approach and its applicability to practical EMG-based control systems.
</details>
<details>
<summary>摘要</summary>
人机界面可能会被开发为使用模式识别来估算执行人类动作的电omyogram (EMG) 信号。这些信号在长期使用EMG-based interface时会逐渐改变特征，这是由于电极移动和肌肉疲劳引起的，导致分类精度逐渐下降。本文提出了一种使用 Bayesian 方法的可靠 EMG 模式分类。这个方法使用 Bayesian 分类模型，根据 Gaussian 分布来预测动作类型和其信度。然后将高预测信度的数据分配为 pseudo-label，并在 Bayesian 更新框架中逐渐更新模型的 posterior 分布，以达到适应性的动作识别。实验结果显示，提案的方法可以抑制分类精度逐渐下降的现象，并且比传统方法表现出色。这些成果证明了提案的方法的有效性，并且适用于实际的 EMG-based 控制系统。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/30/eess.SP_2023_09_30/" data-id="clp869u9601ezk5880v4ndunb" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_09_29" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/29/cs.SD_2023_09_29/" class="article-date">
  <time datetime="2023-09-29T15:00:00.000Z" itemprop="datePublished">2023-09-29</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/29/cs.SD_2023_09_29/">cs.SD - 2023-09-29</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Toward-Universal-Speech-Enhancement-for-Diverse-Input-Conditions"><a href="#Toward-Universal-Speech-Enhancement-for-Diverse-Input-Conditions" class="headerlink" title="Toward Universal Speech Enhancement for Diverse Input Conditions"></a>Toward Universal Speech Enhancement for Diverse Input Conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17384">http://arxiv.org/abs/2309.17384</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Emrys365/Universal-SE-demo">https://github.com/Emrys365/Universal-SE-demo</a></li>
<li>paper_authors: Wangyou Zhang, Kohei Saijo, Zhong-Qiu Wang, Shinji Watanabe, Yanmin Qian</li>
<li>for: 本研究旨在开拓一种能够 universally 处理多种输入条件的数据驱动抖音提升技术。</li>
<li>methods: 我们提出了一种独特的抖音提升模型，该模型独立于麦克风通道、信号长度和采样频率。</li>
<li>results: 我们在各种数据集上进行了广泛的实验，结果表明，提出的单一模型可以成功地处理多种输入条件，并且表现出色。<details>
<summary>Abstract</summary>
The past decade has witnessed substantial growth of data-driven speech enhancement (SE) techniques thanks to deep learning. While existing approaches have shown impressive performance in some common datasets, most of them are designed only for a single condition (e.g., single-channel, multi-channel, or a fixed sampling frequency) or only consider a single task (e.g., denoising or dereverberation). Currently, there is no universal SE approach that can effectively handle diverse input conditions with a single model. In this paper, we make the first attempt to investigate this line of research. First, we devise a single SE model that is independent of microphone channels, signal lengths, and sampling frequencies. Second, we design a universal SE benchmark by combining existing public corpora with multiple conditions. Our experiments on a wide range of datasets show that the proposed single model can successfully handle diverse conditions with strong performance.
</details>
<details>
<summary>摘要</summary>
过去一个十年，数据驱动的语音提升（SE）技术因深度学习而经历了重大的发展。现有的方法在某些常用的数据集上表现出色，但大多数都只适用于单一的条件（例如单通道、多通道或固定采样频率）或只考虑单一任务（例如干声除或抗雷声）。当前没有一种通用的SE方法，可以有效地处理多个输入条件，使用单个模型。在这篇论文中，我们首次进行了这一研究。我们首先设计了独立于 Microphone 通道、信号长度和采样频率的单一 SE 模型。然后，我们设计了一个通用的 SE 测试准则，将现有的公共数据集合并多个条件。我们在各种数据集上进行了广泛的实验，发现我们提议的单一模型可以成功地处理多个条件，表现出色。
</details></li>
</ul>
<hr>
<h2 id="Improving-Audio-Captioning-Models-with-Fine-grained-Audio-Features-Text-Embedding-Supervision-and-LLM-Mix-up-Augmentation"><a href="#Improving-Audio-Captioning-Models-with-Fine-grained-Audio-Features-Text-Embedding-Supervision-and-LLM-Mix-up-Augmentation" class="headerlink" title="Improving Audio Captioning Models with Fine-grained Audio Features, Text Embedding Supervision, and LLM Mix-up Augmentation"></a>Improving Audio Captioning Models with Fine-grained Audio Features, Text Embedding Supervision, and LLM Mix-up Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17352">http://arxiv.org/abs/2309.17352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shih-Lun Wu, Xuankai Chang, Gordon Wichern, Jee-weon Jung, François Germain, Jonathan Le Roux, Shinji Watanabe</li>
<li>for: 提高 seq2seq 自动音频描述系统的性能</li>
<li>methods: 利用预训练模型和大语言模型（LLM），并使用 BEATs 提取细腻的音频特征，以及 Instructor LLM 提取文本描述的语言特征，并通过 auxiliary InfoNCE 损失函数将语言特征注入到 BEATs 音频特征中。此外，我们也提出了一种新的数据增强方法，使用 ChatGPT 生成caption mix-ups，以增加训练数据的多样性和复杂性。</li>
<li>results: 我们的模型在 Clotho 评分分区上取得了新的状态之冠32.6 SPIDEr-FL 分数，并在 2023 年 DCASE AAC 挑战中赢得了比赛。<details>
<summary>Abstract</summary>
Automated audio captioning (AAC) aims to generate informative descriptions for various sounds from nature and/or human activities. In recent years, AAC has quickly attracted research interest, with state-of-the-art systems now relying on a sequence-to-sequence (seq2seq) backbone powered by strong models such as Transformers. Following the macro-trend of applied machine learning research, in this work, we strive to improve the performance of seq2seq AAC models by extensively leveraging pretrained models and large language models (LLMs). Specifically, we utilize BEATs to extract fine-grained audio features. Then, we employ Instructor LLM to fetch text embeddings of captions, and infuse their language-modality knowledge into BEATs audio features via an auxiliary InfoNCE loss function. Moreover, we propose a novel data augmentation method that uses ChatGPT to produce caption mix-ups (i.e., grammatical and compact combinations of two captions) which, together with the corresponding audio mixtures, increase not only the amount but also the complexity and diversity of training data. During inference, we propose to employ nucleus sampling and a hybrid reranking algorithm, which has not been explored in AAC research. Combining our efforts, our model achieves a new state-of-the-art 32.6 SPIDEr-FL score on the Clotho evaluation split, and wins the 2023 DCASE AAC challenge.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="LRPD-Large-Replay-Parallel-Dataset"><a href="#LRPD-Large-Replay-Parallel-Dataset" class="headerlink" title="LRPD: Large Replay Parallel Dataset"></a>LRPD: Large Replay Parallel Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17298">http://arxiv.org/abs/2309.17298</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/idrnd/lrpd-paper-code">https://github.com/idrnd/lrpd-paper-code</a></li>
<li>paper_authors: Ivan Yakovlev, Mikhail Melnikov, Nikita Bukhal, Rostislav Makarov, Alexander Alenin, Nikita Torgashov, Anton Okhotnikov</li>
<li>For: 本研究旨在提高声音反馈攻击检测（VAS）领域的深度神经网络（DNN）性能，并提供大量数据集来促进神经网络系统的进步。* Methods: 本研究使用了大量的批处理并行数据集（LRPD），该数据集包含了19个录音设备在17个不同环境中收集的超过100万个语音示例。* Results: 基于LRPD数据集的模型在完全未知的条件下表现了一致性，其EER在评估subset中为0.28%，并在ASVpoof 2017评估集上达到11.91%的水平。这些结果表明模型在LRPD数据集上训练后在未知条件下具有良好的表现。<details>
<summary>Abstract</summary>
The latest research in the field of voice anti-spoofing (VAS) shows that deep neural networks (DNN) outperform classic approaches like GMM in the task of presentation attack detection. However, DNNs require a lot of data to converge, and still lack generalization ability. In order to foster the progress of neural network systems, we introduce a Large Replay Parallel Dataset (LRPD) aimed for a detection of replay attacks. LRPD contains more than 1M utterances collected by 19 recording devices in 17 various environments. We also provide an example training pipeline in PyTorch [1] and a baseline system, that achieves 0.28% Equal Error Rate (EER) on evaluation subset of LRPD and 11.91% EER on publicly available ASVpoof 2017 [2] eval set. These results show that model trained with LRPD dataset has a consistent performance on the fully unknown conditions. Our dataset is free for research purposes and hosted on GDrive. Baseline code and pre-trained models are available at GitHub.
</details>
<details>
<summary>摘要</summary>
最新的声音反模仿（VAS）研究显示，深度神经网络（DNN）在声音攻击探测任务中表现出色，超过了经典方法如GMM。然而，DNN需要很多数据来融合，并且还缺乏泛化能力。为推动神经网络系统的进步，我们介绍了一个大量重复并行数据集（LRPD），用于检测重复攻击。LRPD包含了超过100万个语音样本，由19个录音设备在17个不同的环境中采集。我们还提供了一个PyTorch中的训练管道示例和一个基线系统，其在LRPD评估子集上达到0.28%的相同错误率（EER），并在公开可用的ASVpoof 2017评估集上达到11.91%的EER。这些结果表明，使用LRPD数据集训练的模型在完全未知的条件下具有一致的表现。我们的数据集是免费用于研究目的，并在GDrive上hosts。基线代码和预训练模型可以在GitHub上获取。
</details></li>
</ul>
<hr>
<h2 id="ReFlow-TTS-A-Rectified-Flow-Model-for-High-fidelity-Text-to-Speech"><a href="#ReFlow-TTS-A-Rectified-Flow-Model-for-High-fidelity-Text-to-Speech" class="headerlink" title="ReFlow-TTS: A Rectified Flow Model for High-fidelity Text-to-Speech"></a>ReFlow-TTS: A Rectified Flow Model for High-fidelity Text-to-Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17056">http://arxiv.org/abs/2309.17056</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenhao Guan, Qi Su, Haodong Zhou, Shiyu Miao, Xingjia Xie, Lin Li, Qingyang Hong</li>
<li>for: 这个研究旨在提出一种基于流函数的高质量语音合成方法，以替代传统的数据生成模型。</li>
<li>methods: 我们的方法使用了一种简单的Ordinary Differential Equation（ODE）模型，将Gaussian分布运输到真实的Mel-spectrogram分布中，使用直线路径以最大化运输程度。</li>
<li>results: 我们的实验结果显示，我们的ReFlow-TTS方法在LJSpeechDataset上实现了最高的性能，并且单步抽样比较高质量的语音合成模型。<details>
<summary>Abstract</summary>
The diffusion models including Denoising Diffusion Probabilistic Models (DDPM) and score-based generative models have demonstrated excellent performance in speech synthesis tasks. However, its effectiveness comes at the cost of numerous sampling steps, resulting in prolonged sampling time required to synthesize high-quality speech. This drawback hinders its practical applicability in real-world scenarios. In this paper, we introduce ReFlow-TTS, a novel rectified flow based method for speech synthesis with high-fidelity. Specifically, our ReFlow-TTS is simply an Ordinary Differential Equation (ODE) model that transports Gaussian distribution to the ground-truth Mel-spectrogram distribution by straight line paths as much as possible. Furthermore, our proposed approach enables high-quality speech synthesis with a single sampling step and eliminates the need for training a teacher model. Our experiments on LJSpeech Dataset show that our ReFlow-TTS method achieves the best performance compared with other diffusion based models. And the ReFlow-TTS with one step sampling achieves competitive performance compared with existing one-step TTS models.
</details>
<details>
<summary>摘要</summary>
Diffusion模型，包括Denosing Diffusion Probabilistic Models (DDPM)和分数基本生成模型，在语音合成任务中表现出色，但是其效果带来许多抽样步骤，导致高质量语音合成需要长时间抽样。这种缺点限制了它在实际场景中的实用性。在这篇论文中，我们介绍ReFlow-TTS，一种基于流函数的新方法，用于高质量语音合成。具体来说，ReFlow-TTS是一个Ordinary Differential Equation (ODE)模型，可以将 Gaussian 分布transport到真实的 Mel-spectrogram 分布，以最大程度地保持直线路径。此外，我们提出的方法可以在一步抽样下实现高质量语音合成，并减少了培训教师模型的需求。我们在LJSpeech Dataset上进行了实验，并证明了ReFlow-TTS方法在 diffusion 基于模型中表现最佳，并且ReFlow-TTS 一步抽样方法与现有的一步 TTS 模型相匹配。
</details></li>
</ul>
<hr>
<h2 id="Low-Resource-Self-Supervised-Learning-with-SSL-Enhanced-TTS"><a href="#Low-Resource-Self-Supervised-Learning-with-SSL-Enhanced-TTS" class="headerlink" title="Low-Resource Self-Supervised Learning with SSL-Enhanced TTS"></a>Low-Resource Self-Supervised Learning with SSL-Enhanced TTS</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17020">http://arxiv.org/abs/2309.17020</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lenguyensonnguyen/CS2309_ChuyenDeThiGiacMayTinh_LeNguyenSonNguyen_CH1702039_GiuaKy">https://github.com/lenguyensonnguyen/CS2309_ChuyenDeThiGiacMayTinh_LeNguyenSonNguyen_CH1702039_GiuaKy</a></li>
<li>paper_authors: Po-chun Hsu, Ali Elkahky, Wei-Ning Hsu, Yossi Adi, Tu Anh Nguyen, Jade Copet, Emmanuel Dupoux, Hung-yi Lee, Abdelrahman Mohamed</li>
<li>for: 提高低资源自动语音识别的性能</li>
<li>methods: 使用合成语音增强低资源预训练 corpus</li>
<li>results: 成功降低了90%的语音数据需求，只有轻微性能下降<details>
<summary>Abstract</summary>
Self-supervised learning (SSL) techniques have achieved remarkable results in various speech processing tasks. Nonetheless, a significant challenge remains in reducing the reliance on vast amounts of speech data for pre-training. This paper proposes to address this challenge by leveraging synthetic speech to augment a low-resource pre-training corpus. We construct a high-quality text-to-speech (TTS) system with limited resources using SSL features and generate a large synthetic corpus for pre-training. Experimental results demonstrate that our proposed approach effectively reduces the demand for speech data by 90\% with only slight performance degradation. To the best of our knowledge, this is the first work aiming to enhance low-resource self-supervised learning in speech processing.
</details>
<details>
<summary>摘要</summary>
自我指导学习（SSL）技术在各种语音处理任务中获得了卓越的结果。然而，尚存在很大的挑战是减少预训练中需要庞大量的语音数据的依赖。本文提议使用生成的 sintetic speech 来增强低资源预训练集。我们使用 SSL 特征构建了高质量的 text-to-speech（TTS）系统，生成了大量的 sintetic 训练集，并实验结果表明，我们的提议方法可以将语音数据需求减少90%，只带来轻微的性能下降。这是我们知道的首次尝试增强低资源自我指导学习在语音处理中。
</details></li>
</ul>
<hr>
<h2 id="Synthetic-Speech-Detection-Based-on-Temporal-Consistency-and-Distribution-of-Speaker-Features"><a href="#Synthetic-Speech-Detection-Based-on-Temporal-Consistency-and-Distribution-of-Speaker-Features" class="headerlink" title="Synthetic Speech Detection Based on Temporal Consistency and Distribution of Speaker Features"></a>Synthetic Speech Detection Based on Temporal Consistency and Distribution of Speaker Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16954">http://arxiv.org/abs/2309.16954</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxiang Zhang, Zhuo Li, Jingze Lu, Wenchao Wang, Pengyuan Zhang</li>
<li>for: 本研究旨在提高现代语音识别（SSD）方法的Robustness和可解性，并解决现有方法在某些数据集上的问题。</li>
<li>methods: 本研究使用了分析文本读音（TTS）过程中的发音特征缺陷，并根据发音特征的时间一致性和间隔特征的分布，提出了一种基于时间一致性和发音特征分布的SSD方法。</li>
<li>results: 该方法在跨数据集和静音修剪场景中都表现出了低计算复杂性和良好的性能，能够提高SSD方法的Robustness和可解性。<details>
<summary>Abstract</summary>
Current synthetic speech detection (SSD) methods perform well on certain datasets but still face issues of robustness and interpretability. A possible reason is that these methods do not analyze the deficiencies of synthetic speech. In this paper, the flaws of the speaker features inherent in the text-to-speech (TTS) process are analyzed. Differences in the temporal consistency of intra-utterance speaker features arise due to the lack of fine-grained control over speaker features in TTS. Since the speaker representations in TTS are based on speaker embeddings extracted by encoders, the distribution of inter-utterance speaker features differs between synthetic and bonafide speech. Based on these analyzes, an SSD method based on temporal consistency and distribution of speaker features is proposed. On one hand, modeling the temporal consistency of intra-utterance speaker features can aid speech anti-spoofing. On the other hand, distribution differences in inter-utterance speaker features can be utilized for SSD. The proposed method offers low computational complexity and performs well in both cross-dataset and silence trimming scenarios.
</details>
<details>
<summary>摘要</summary>
当前的语音合成检测（SSD）方法在某些数据集上表现良好，但仍面临 robustness 和可读性的问题。一个可能的原因是这些方法不进行真实语音的缺陷分析。在这篇论文中，文本到语音（TTS）过程中的 speaker 特征的缺陷被分析了。因为 TTS 中 speaker 特征的时间一致性不够细致，因此 intra-utterance 中的 speaker 特征存在差异。此外，基于 encoder 提取的 speaker 嵌入有所不同，因此 inter-utterance 中的 speaker 特征的分布不同。根据这些分析，一种基于时间一致性和 speaker 特征分布的 SSD 方法被提议。在一个手上，模拟 intra-utterance 中 speaker 特征的时间一致性可以帮助语音防 spoofing。在另一个手上，inter-utterance 中 speaker 特征的分布差异可以被利用于 SSD。提议的方法具有低计算复杂度，在cross-dataset 和沉寂截取场景中表现良好。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Code-switching-Speech-Recognition-with-Interactive-Language-Biases"><a href="#Enhancing-Code-switching-Speech-Recognition-with-Interactive-Language-Biases" class="headerlink" title="Enhancing Code-switching Speech Recognition with Interactive Language Biases"></a>Enhancing Code-switching Speech Recognition with Interactive Language Biases</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16953">http://arxiv.org/abs/2309.16953</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hexin Liu, Leibny Paola Garcia, Xiangyu Zhang, Andy W. H. Khong, Sanjeev Khudanpur</li>
<li>for: 本研究旨在提高多语言交互 circumstance 下的自动话语识别（ASR）性能，通过对混合 CTC&#x2F;注意力 ASR 模型进行多级语言信息偏好。</li>
<li>methods: 本研究使用了 hybrid CTC&#x2F;注意力 ASR 模型，并通过在不同层次语言信息上进行偏好来提高模型性能。</li>
<li>results: 对 ASRU 2019 多语言交互挑战 dataset 进行了实验，并与基eline 进行了比较。结果表明，提出的交互语言偏好（ILB）方法可以提高模型性能，并且对各种语言偏好和它们之间的交互进行了深入的分析。<details>
<summary>Abstract</summary>
Languages usually switch within a multilingual speech signal, especially in a bilingual society. This phenomenon is referred to as code-switching (CS), making automatic speech recognition (ASR) challenging under a multilingual scenario. We propose to improve CS-ASR by biasing the hybrid CTC/attention ASR model with multi-level language information comprising frame- and token-level language posteriors. The interaction between various resolutions of language biases is subsequently explored in this work. We conducted experiments on datasets from the ASRU 2019 code-switching challenge. Compared to the baseline, the proposed interactive language biases (ILB) method achieves higher performance and ablation studies highlight the effects of different language biases and their interactions. In addition, the results presented indicate that language bias implicitly enhances internal language modeling, leading to performance degradation after employing an external language model.
</details>
<details>
<summary>摘要</summary>
语言通常在多语言交流中切换，特别在双语社会中。这种现象被称为代码切换（CS），使自动听说识别（ASR）在多语言场景下变得更加困难。我们提出了改进CS-ASR方法，使用多级语言信息（frame和token）来偏袋混合CTC/注意力ASR模型。我们在这里探索不同分辨率的语言偏袋之间的交互作用。我们在ASRU 2019代码切换挑战 datasets上进行了实验，相比基eline，我们的互动语言偏袋（ILB）方法实现了更高的性能。归根结底，我们的研究表明，语言偏袋隐式地增强了内部语言模型，导致性能下降 después使用外部语言模型。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/29/cs.SD_2023_09_29/" data-id="clp869u3c0100k5884chl9nf2" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_09_29" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/29/cs.CV_2023_09_29/" class="article-date">
  <time datetime="2023-09-29T13:00:00.000Z" itemprop="datePublished">2023-09-29</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/29/cs.CV_2023_09_29/">cs.CV - 2023-09-29</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="MARL-Multi-scale-Archetype-Representation-Learning-for-Urban-Building-Energy-Modeling"><a href="#MARL-Multi-scale-Archetype-Representation-Learning-for-Urban-Building-Energy-Modeling" class="headerlink" title="MARL: Multi-scale Archetype Representation Learning for Urban Building Energy Modeling"></a>MARL: Multi-scale Archetype Representation Learning for Urban Building Energy Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00180">http://arxiv.org/abs/2310.00180</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zixunhuang1997/marl-buildingenergyestimation">https://github.com/zixunhuang1997/marl-buildingenergyestimation</a></li>
<li>paper_authors: Xinwei Zhuang, Zixun Huang, Wentao Zeng, Luisa Caldas</li>
<li>For: 本研究旨在提高城市建筑能源估算的准确性和可靠性，通过提取当地建筑尺度特点，建立代表性的建筑模板。* Methods: 该研究使用了 Representation Learning 技术，具体来说是 VQ-AE 和自适应卷积神经网络，对当地建筑脚印进行了压缩和扩展，提取了建筑尺度特点。* Results: 研究发现，使用 MARL 提取的 geometric feature embeddings 可以提高城市建筑能源估算的准确性和可靠性，并且可以自动生成多scale 区域的建筑模板。 Code、数据集和训练模型都公开可用：<a target="_blank" rel="noopener" href="https://github.com/ZixunHuang1997/MARL-BuildingEnergyEstimation%E3%80%82">https://github.com/ZixunHuang1997/MARL-BuildingEnergyEstimation。</a><details>
<summary>Abstract</summary>
Building archetypes, representative models of building stock, are crucial for precise energy simulations in Urban Building Energy Modeling. The current widely adopted building archetypes are developed on a nationwide scale, potentially neglecting the impact of local buildings' geometric specificities. We present Multi-scale Archetype Representation Learning (MARL), an approach that leverages representation learning to extract geometric features from a specific building stock. Built upon VQ-AE, MARL encodes building footprints and purifies geometric information into latent vectors constrained by multiple architectural downstream tasks. These tailored representations are proven valuable for further clustering and building energy modeling. The advantages of our algorithm are its adaptability with respect to the different building footprint sizes, the ability for automatic generation across multi-scale regions, and the preservation of geometric features across neighborhoods and local ecologies. In our study spanning five regions in LA County, we show MARL surpasses both conventional and VQ-AE extracted archetypes in performance. Results demonstrate that geometric feature embeddings significantly improve the accuracy and reliability of energy consumption estimates. Code, dataset and trained models are publicly available: https://github.com/ZixunHuang1997/MARL-BuildingEnergyEstimation
</details>
<details>
<summary>摘要</summary>
建筑型例，代表性模型的建筑股权，在城市建筑能源模拟中是关键。现有的广泛采用的建筑型例是在国家范围内开发的，可能忽略本地建筑的几何特点。我们提出了多级Archetype Representation Learning（MARL），它利用表示学来提取建筑范围的几何特征。基于VQ-AE，MARL编码建筑范围和约束多个建筑下渠道任务的几何信息，生成受限制的约束 vector。这些适应性的表示值有助于进一步的划分和建筑能源模拟。我们的算法具有适应不同建筑范围大小、自动生成多级区域和保持几何特征的优势。在洛杉矶县五个区域的研究中，我们证明MARL超过了传统和VQ-AE提取的建筑型例，在性能和可靠性方面具有显著优势。结果表明几何特征嵌入有助于提高能源消耗估计的准确性和可靠性。代码、数据集和训练模型在 GitHub 上公开：https://github.com/ZixunHuang1997/MARL-BuildingEnergyEstimation。
</details></li>
</ul>
<hr>
<h2 id="SCoRe-Submodular-Combinatorial-Representation-Learning-for-Real-World-Class-Imbalanced-Settings"><a href="#SCoRe-Submodular-Combinatorial-Representation-Learning-for-Real-World-Class-Imbalanced-Settings" class="headerlink" title="SCoRe: Submodular Combinatorial Representation Learning for Real-World Class-Imbalanced Settings"></a>SCoRe: Submodular Combinatorial Representation Learning for Real-World Class-Imbalanced Settings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00165">http://arxiv.org/abs/2310.00165</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anay Majee, Suraj Kothawade, Krishnateja Killiamsetty, Rishabh Iyer</li>
<li>For: The paper focuses on the challenge of representation learning in real-world class-imbalanced settings, particularly in deep learning.* Methods: The authors propose a new framework called SCoRe (Submodular Combinatorial Representation Learning) that leverages set-based combinatorial functions to model diversity and cooperation among feature clusters. They also introduce a family of Submodular Combinatorial Loss functions to overcome the pitfalls of class-imbalance in contrastive learning.* Results: The authors show that their proposed objectives outperform state-of-the-art metric learners by up to 7.6% for imbalanced classification tasks and up to 19.4% for object detection tasks on several benchmark datasets.<details>
<summary>Abstract</summary>
Representation Learning in real-world class-imbalanced settings has emerged as a challenging task in the evolution of deep learning. Lack of diversity in visual and structural features for rare classes restricts modern neural networks to learn discriminative feature clusters. This manifests in the form of large inter-class bias between rare object classes and elevated intra-class variance among abundant classes in the dataset. Although deep metric learning approaches have shown promise in this domain, significant improvements need to be made to overcome the challenges associated with class-imbalance in mission critical tasks like autonomous navigation and medical diagnostics. Set-based combinatorial functions like Submodular Information Measures exhibit properties that allow them to simultaneously model diversity and cooperation among feature clusters. In this paper, we introduce the SCoRe (Submodular Combinatorial Representation Learning) framework and propose a family of Submodular Combinatorial Loss functions to overcome these pitfalls in contrastive learning. We also show that existing contrastive learning approaches are either submodular or can be re-formulated to create their submodular counterparts. We conduct experiments on the newly introduced family of combinatorial objectives on two image classification benchmarks - pathologically imbalanced CIFAR-10, subsets of MedMNIST and a real-world road object detection benchmark - India Driving Dataset (IDD). Our experiments clearly show that the newly introduced objectives like Facility Location, Graph-Cut and Log Determinant outperform state-of-the-art metric learners by up to 7.6% for the imbalanced classification tasks and up to 19.4% for object detection tasks.
</details>
<details>
<summary>摘要</summary>
“在实际世界中的分布不均的设定下，深度学习中的 Representation Learning 已成为一个挑战。缺乏罕见类别的视觉和结构特征使现代神经网络很难学习标志性的特征群。这会导致资料集中罕见类别和普见类别之间的大型间隔，以及普见类别中的高度内部矩阵。尽管深度度量学习方法有所进步，但还需要进一步改进，以应对实际中的分布不均问题，如自主 Navigation 和医疗诊断。”“Set-based combinatorial functions like Submodular Information Measures exhibit properties that allow them to simultaneously model diversity and cooperation among feature clusters. In this paper, we introduce the SCoRe (Submodular Combinatorial Representation Learning) framework and propose a family of Submodular Combinatorial Loss functions to overcome these pitfalls in contrastive learning. We also show that existing contrastive learning approaches are either submodular or can be re-formulated to create their submodular counterparts.”“我们在新引入的家族 combinatorial 目标上进行了实验，包括 Facility Location, Graph-Cut 和 Log Determinant。我们的实验结果显示，这些新引入的目标可以与现有的 Metric Learners 相比，在不均分的类别任务上进行改进。 Specifically, we observe up to 7.6% improvement for imbalanced classification tasks and up to 19.4% improvement for object detection tasks.”
</details></li>
</ul>
<hr>
<h2 id="PRIME-Prioritizing-Interpretability-in-Failure-Mode-Extraction"><a href="#PRIME-Prioritizing-Interpretability-in-Failure-Mode-Extraction" class="headerlink" title="PRIME: Prioritizing Interpretability in Failure Mode Extraction"></a>PRIME: Prioritizing Interpretability in Failure Mode Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00164">http://arxiv.org/abs/2310.00164</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keivan Rezaei, Mehrdad Saberi, Mazda Moayeri, Soheil Feizi</li>
<li>for: 本研究旨在提供训练过的图像分类模型中出现的失败模式的人类可理解的描述。</li>
<li>methods: 我们提议一种新的方法，强调解释性，来解决这个问题。我们首先从数据集中获取图像的人类可理解的概念（标签），然后根据模型的行为来分析这些标签的组合。我们的方法还确保这些失败模式的描述形成最小的集合，以避免 redundant和噪音的描述。</li>
<li>results: 我们通过在不同的数据集上进行多个实验，成功地 indentified failing modes 并生成了高质量的关于这些失败模式的文本描述。这些结果 highlights 模型失败的解释性的重要性。<details>
<summary>Abstract</summary>
In this work, we study the challenge of providing human-understandable descriptions for failure modes in trained image classification models. Existing works address this problem by first identifying clusters (or directions) of incorrectly classified samples in a latent space and then aiming to provide human-understandable text descriptions for them. We observe that in some cases, describing text does not match well with identified failure modes, partially owing to the fact that shared interpretable attributes of failure modes may not be captured using clustering in the feature space. To improve on these shortcomings, we propose a novel approach that prioritizes interpretability in this problem: we start by obtaining human-understandable concepts (tags) of images in the dataset and then analyze the model's behavior based on the presence or absence of combinations of these tags. Our method also ensures that the tags describing a failure mode form a minimal set, avoiding redundant and noisy descriptions. Through several experiments on different datasets, we show that our method successfully identifies failure modes and generates high-quality text descriptions associated with them. These results highlight the importance of prioritizing interpretability in understanding model failures.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们研究了提供训练过的图像分类模型失败模式的人类可理解描述的挑战。现有的方法通过首先标识特征空间中错误分类样本的集群（或方向）来解决这个问题，然后尝试提供人类可理解的文本描述。我们发现，在某些情况下，描述文本并不好匹配错误模式的共同可解释特征，部分是因为在特征空间中使用 clustering 可能会遗漏共享可解释特征。为了改进这些缺陷，我们提出了一种新的方法，即优先考虑可解释性：我们首先从数据集中获取图像的人类可理解概念（标签），然后分析模型的行为是否符合标签的存在或缺失。我们的方法还确保了失败模式的标签形成最小集，以避免重复和噪音的描述。通过在不同的数据集上进行了多个实验，我们证明了我们的方法可以成功地认出失败模式并生成高质量的相关文本描述。这些结果强调了在理解模型失败时优先考虑可解释性的重要性。
</details></li>
</ul>
<hr>
<h2 id="Prior-Mismatch-and-Adaptation-in-PnP-ADMM-with-a-Nonconvex-Convergence-Analysis"><a href="#Prior-Mismatch-and-Adaptation-in-PnP-ADMM-with-a-Nonconvex-Convergence-Analysis" class="headerlink" title="Prior Mismatch and Adaptation in PnP-ADMM with a Nonconvex Convergence Analysis"></a>Prior Mismatch and Adaptation in PnP-ADMM with a Nonconvex Convergence Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00133">http://arxiv.org/abs/2310.00133</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shirin Shoushtari, Jiaming Liu, Edward P. Chandler, M. Salman Asif, Ulugbek S. Kamilov</li>
<li>for: 这篇论文主要针对解决图像 inverse problems 的问题，通过将物理测量模型与图像约束相结合，使用图像滤波器来指定图像约束。</li>
<li>methods: 这篇论文使用了 Alternating Direction Method of Multipliers (ADMM) 方法，并研究了在不同的数据分布下对 PnP 方法的影响。</li>
<li>results: 研究结果显示，PnP-ADMM 方法对于图像超分辨问题的性能是可以忽略不计的，但是如果使用不同的数据分布来训练和测试，则会导致性能下降。然而，通过一些简单的采样和适应策略，可以减少这种性能差距。<details>
<summary>Abstract</summary>
Plug-and-Play (PnP) priors is a widely-used family of methods for solving imaging inverse problems by integrating physical measurement models with image priors specified using image denoisers. PnP methods have been shown to achieve state-of-the-art performance when the prior is obtained using powerful deep denoisers. Despite extensive work on PnP, the topic of distribution mismatch between the training and testing data has often been overlooked in the PnP literature. This paper presents a set of new theoretical and numerical results on the topic of prior distribution mismatch and domain adaptation for alternating direction method of multipliers (ADMM) variant of PnP. Our theoretical result provides an explicit error bound for PnP-ADMM due to the mismatch between the desired denoiser and the one used for inference. Our analysis contributes to the work in the area by considering the mismatch under nonconvex data-fidelity terms and expansive denoisers. Our first set of numerical results quantifies the impact of the prior distribution mismatch on the performance of PnP-ADMM on the problem of image super-resolution. Our second set of numerical results considers a simple and effective domain adaption strategy that closes the performance gap due to the use of mismatched denoisers. Our results suggest the relative robustness of PnP-ADMM to prior distribution mismatch, while also showing that the performance gap can be significantly reduced with few training samples from the desired distribution.
</details>
<details>
<summary>摘要</summary>
插件和执行（PnP）先验是一种广泛使用的方法来解决图像反问题，通过将物理测量模型与使用图像抑制器指定的图像先验集成。PnP方法已经达到了现状的性能，当使用强大的深度抑制器时。 DESPITE extensive work on PnP, the topic of distribution mismatch between the training and testing data has often been overlooked in the PnP literature. This paper presents a set of new theoretical and numerical results on the topic of prior distribution mismatch and domain adaptation for alternating direction method of multipliers (ADMM) variant of PnP. Our theoretical result provides an explicit error bound for PnP-ADMM due to the mismatch between the desired denoiser and the one used for inference. Our analysis contributes to the work in the area by considering the mismatch under nonconvex data-fidelity terms and expansive denoisers. Our first set of numerical results quantifies the impact of the prior distribution mismatch on the performance of PnP-ADMM on the problem of image super-resolution. Our second set of numerical results considers a simple and effective domain adaption strategy that closes the performance gap due to the use of mismatched denoisers. Our results suggest the relative robustness of PnP-ADMM to prior distribution mismatch, while also showing that the performance gap can be significantly reduced with few training samples from the desired distribution.
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Audiovisual-Segmentation-with-Semantic-Quantization-and-Decomposition"><a href="#Rethinking-Audiovisual-Segmentation-with-Semantic-Quantization-and-Decomposition" class="headerlink" title="Rethinking Audiovisual Segmentation with Semantic Quantization and Decomposition"></a>Rethinking Audiovisual Segmentation with Semantic Quantization and Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00132">http://arxiv.org/abs/2310.00132</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Li, Jinglu Wang, Xiaohao Xu, Xiulian Peng, Rita Singh, Yan Lu, Bhiksha Raj</li>
<li>for: 这篇论文旨在解决视频中的听视对应问题，通过基于听音的Semantic Space分解和量化来提高听视对应性。</li>
<li>methods: 本文提出了一种基于产品量化的Semantic decomposition方法，将多源语音 semantics 分解为几个独立的单源 semantics，并引入全局到本地量化机制来处理听音 semantics的不断变化。</li>
<li>results: 实验结果表明，基于Semantic decomposition和量化的听视对应方法可以显著提高AVS性能，例如在AVS-Semantic标准测试集上提高了21.2%的mIoU。<details>
<summary>Abstract</summary>
Audiovisual segmentation (AVS) is a challenging task that aims to segment visual objects in videos based on their associated acoustic cues. With multiple sound sources involved, establishing robust correspondences between audio and visual contents poses unique challenges due to its (1) intricate entanglement across sound sources and (2) frequent shift among sound events. Assuming sound events occur independently, the multi-source semantic space (which encompasses all possible semantic categories) can be viewed as the Cartesian product of single-source sub-spaces. This motivates us to decompose the multi-source audio semantics into single-source semantics, allowing for more effective interaction with visual content. Specifically, we propose a semantic decomposition method based on product quantization, where the multi-source semantics can be decomposed and represented by several quantized single-source semantics. Furthermore, we introduce a global-to-local quantization mechanism that distills knowledge from stable global (clip-level) features into local (frame-level) ones to handle the constant shift of audio semantics. Extensive experiments demonstrate that semantically quantized and decomposed audio representation significantly improves AVS performance, e.g., +21.2% mIoU on the most challenging AVS-Semantic benchmark.
</details>
<details>
<summary>摘要</summary>
宽泛听视分割（AVS）是一项复杂的任务，旨在根据视频中相关的声音提示，将视觉对象分割成多个类别。由于多个声音源参与，在Audio和视觉内容之间建立坚实的对应关系具有独特挑战，主要是因为声音源之间的复杂互相关系和声音事件频繁变化。假设声音事件独立发生，那么多源语义空间（包括所有可能的语义类别）可以视为Cartesian产品的多个单源语义空间。这使得我们可以将多源语义分解为单源语义，以更好地与视觉内容交互。具体来说，我们提出了基于产品量化的语义分解方法，可以将多源语义分解并表示为多个量化的单源语义。此外，我们还引入了全局到本地量化机制，可以从稳定的全局（clip级）特征中提取知识，并将其转化为本地（帧级）特征，以处理声音语义的不断变化。广泛的实验表明，semantic量化和分解后的声音表示可以显著提高AVS性能，例如+21.2% mIoU在AVS-Semantic标准测试上。
</details></li>
</ul>
<hr>
<h2 id="Fewshot-learning-on-global-multimodal-embeddings-for-earth-observation-tasks"><a href="#Fewshot-learning-on-global-multimodal-embeddings-for-earth-observation-tasks" class="headerlink" title="Fewshot learning on global multimodal embeddings for earth observation tasks"></a>Fewshot learning on global multimodal embeddings for earth observation tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00119">http://arxiv.org/abs/2310.00119</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matt Allen, Francisco Dorr, Joseph A. Gallego-Mejia, Laura Martínez-Ferrer, Anna Jungbluth, Freddie Kalaitzis, Raúl Ramos-Pollán</li>
<li>for: 这个研究使用CLIP&#x2F;ViT模型在五个不同的地区进行了三种不同的卫星图像模式的预训练，涵盖了全球陆地面积的约10%。</li>
<li>methods: 该模型使用了约250M个参数，并使用了经典机器学习方法来进行不同的下游任务，包括覆盖面积、建筑面积、农地和永久水面。</li>
<li>results: 研究表明，只需使用约200-500个随机选择的标注示例（约4K-10K平方公里），可以达到与完整的标注数据集（约150K张图像片或3M平方公里）的性能水平，在所有模式、地区和下游任务中。这表明模型已经捕捉了大地特征，可以在各种enario中应用。<details>
<summary>Abstract</summary>
In this work we pretrain a CLIP/ViT based model using three different modalities of satellite imagery across five AOIs covering over ~10\% of the earth total landmass, namely Sentinel 2 RGB optical imagery, Sentinel 1 SAR amplitude and Sentinel 1 SAR interferometric coherence. This model uses $\sim 250$ M parameters. Then, we use the embeddings produced for each modality with a classical machine learning method to attempt different downstream tasks for earth observation related to vegetation, built up surface, croplands and permanent water. We consistently show how we reduce the need for labeled data by 99\%, so that with ~200-500 randomly selected labeled examples (around 4K-10K km$^2$) we reach performance levels analogous to those achieved with the full labeled datasets (about 150K image chips or 3M km$^2$ in each AOI) on all modalities, AOIs and downstream tasks. This leads us to think that the model has captured significant earth features useful in a wide variety of scenarios. To enhance our model's usability in practice, its architecture allows inference in contexts with missing modalities and even missing channels within each modality. Additionally, we visually show that this embedding space, obtained with no labels, is sensible to the different earth features represented by the labelled datasets we selected.
</details>
<details>
<summary>摘要</summary>
“在这项工作中，我们预训练了基于CLIP/ViT的模型，使用三种不同的卫星图像模式，覆盖地球总面积的约10%的五个AOI，即Sentinel 2 红色光学图像、Sentinel 1 SAR振荡和Sentinel 1 SAR相互干扰干扰。这个模型使用大约250M个参数。然后，我们使用每个模式生成的嵌入，与经典机器学习方法进行不同的下游任务，关于地球观测，如茂密 vegetation、建筑物表面、农地和常年水。我们一致地表明，我们可以降低需要标注数据的量，从99%下降到200-500个随机选择的标注示例（约4K-10K km$^2$），以达到与全部标注数据集（约150K图像块或3M km$^2$在每个AOI）的性能水平。这导致我们认为，这个模型已经捕捉了大地特征，可以在各种enario中应用。为了使这个模型在实际应用中更加可用，它的架构允许在缺失模式和内部模式中进行推理。此外，我们可见地表示，这些无标注空间，通过与我们选择的标注集合进行比较，表明这些空间是有意义的。”
</details></li>
</ul>
<hr>
<h2 id="Practical-Membership-Inference-Attacks-Against-Large-Scale-Multi-Modal-Models-A-Pilot-Study"><a href="#Practical-Membership-Inference-Attacks-Against-Large-Scale-Multi-Modal-Models-A-Pilot-Study" class="headerlink" title="Practical Membership Inference Attacks Against Large-Scale Multi-Modal Models: A Pilot Study"></a>Practical Membership Inference Attacks Against Large-Scale Multi-Modal Models: A Pilot Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00108">http://arxiv.org/abs/2310.00108</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ruoxi-jia-group/clip-mia">https://github.com/ruoxi-jia-group/clip-mia</a></li>
<li>paper_authors: Myeongseob Ko, Ming Jin, Chenguang Wang, Ruoxi Jia</li>
<li>for: This paper aims to develop practical membership inference attacks (MIAs) against large-scale multi-modal models, specifically targeting CLIP models.</li>
<li>methods: The proposed baseline strategy thresholds the cosine similarity between text and image features of a target point, and the enhanced attack method aggregates cosine similarity across transformations of the target. Additionally, a new weakly supervised attack method leverages ground-truth non-members to further enhance the attack.</li>
<li>results: The simple baseline achieves over $75%$ membership identification accuracy, and the enhanced attacks outperform the baseline across multiple models and datasets, with the weakly supervised attack demonstrating an average-case performance improvement of $17%$ and being at least $7$X more effective at low false-positive rates.<details>
<summary>Abstract</summary>
Membership inference attacks (MIAs) aim to infer whether a data point has been used to train a machine learning model. These attacks can be employed to identify potential privacy vulnerabilities and detect unauthorized use of personal data. While MIAs have been traditionally studied for simple classification models, recent advancements in multi-modal pre-training, such as CLIP, have demonstrated remarkable zero-shot performance across a range of computer vision tasks. However, the sheer scale of data and models presents significant computational challenges for performing the attacks.   This paper takes a first step towards developing practical MIAs against large-scale multi-modal models. We introduce a simple baseline strategy by thresholding the cosine similarity between text and image features of a target point and propose further enhancing the baseline by aggregating cosine similarity across transformations of the target. We also present a new weakly supervised attack method that leverages ground-truth non-members (e.g., obtained by using the publication date of a target model and the timestamps of the open data) to further enhance the attack. Our evaluation shows that CLIP models are susceptible to our attack strategies, with our simple baseline achieving over $75\%$ membership identification accuracy. Furthermore, our enhanced attacks outperform the baseline across multiple models and datasets, with the weakly supervised attack demonstrating an average-case performance improvement of $17\%$ and being at least $7$X more effective at low false-positive rates. These findings highlight the importance of protecting the privacy of multi-modal foundational models, which were previously assumed to be less susceptible to MIAs due to less overfitting. Our code is available at https://github.com/ruoxi-jia-group/CLIP-MIA.
</details>
<details>
<summary>摘要</summary>
美国文学攻击（MIA）旨在判断一个数据点是否被用于训练机器学习模型。这些攻击可以用来检测潜在隐私漏洞和未经授权使用个人数据。虽然MIA在简单的分类模型上已经得到了广泛的研究，但是随着多Modal预训练的发展，如CLIP，其在多种计算机视觉任务上表现出了很好的零基础性能。然而，大规模数据和模型的计算挑战仍然存在。这篇文章是开发大规模多Modal模型的实用MIA的第一步。我们提出了一个简单的基线策略，即将文本和图像特征的夹角相似度进行阈值设置，并提出了在 transformations 上聚合 cosine similarity 来增强基线。我们还提出了一种新的弱监督攻击方法，利用非成员（例如，使用目标模型的发布日期和公开数据的时间戳）来进一步提高攻击。我们的评估结果显示，CLIP模型对我们的攻击策略非常易受到影响，我们的简单基线可以达到超过 75%的成员认定精度。此外，我们的增强攻击方法在多个模型和数据集上表现出了超过基eline的性能，弱监督攻击在低 false-positive 率下表现出了平均情况性能提高约 17%，并且在至少 7 倍的 false-positive 率下表现出了最好的性能。这些发现说明了保护多Modal基础模型的隐私的重要性，这些模型曾经被认为是MIA less susceptible due to less overfitting。我们的代码可以在 <https://github.com/ruoxi-jia-group/CLIP-MIA> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Denoising-and-Selecting-Pseudo-Heatmaps-for-Semi-Supervised-Human-Pose-Estimation"><a href="#Denoising-and-Selecting-Pseudo-Heatmaps-for-Semi-Supervised-Human-Pose-Estimation" class="headerlink" title="Denoising and Selecting Pseudo-Heatmaps for Semi-Supervised Human Pose Estimation"></a>Denoising and Selecting Pseudo-Heatmaps for Semi-Supervised Human Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00099">http://arxiv.org/abs/2310.00099</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuoran Yu, Manchen Wang, Yanbei Chen, Paolo Favaro, Davide Modolo</li>
<li>for: 提出一种新的半监督学习设计，用于人姿估计，涵盖了两个方面的优化。</li>
<li>methods: 首先，引入一种去噪方案，生成可靠的 Pseudo-热图作为学习从无标记数据中学习的目标。其次，通过选择学习目标，根据估计的交叉学生uncertainty来选择学习目标。</li>
<li>results: 我们在COCObenchmark上进行了多种评估 setup，并证明了我们的模型在半监督学习下比前一代最佳性能的 semi-supervised pose estimator 更高，特别是在低数据 regime 下。例如，只有0.5K 标注图像，我们的方法可以超越最佳竞争对手by 7.22 mAP (+25% 绝对提升)。此外，我们还证明了我们的模型可以从无标记数据中学习，以进一步提高其泛化和性能。<details>
<summary>Abstract</summary>
We propose a new semi-supervised learning design for human pose estimation that revisits the popular dual-student framework and enhances it two ways. First, we introduce a denoising scheme to generate reliable pseudo-heatmaps as targets for learning from unlabeled data. This uses multi-view augmentations and a threshold-and-refine procedure to produce a pool of pseudo-heatmaps. Second, we select the learning targets from these pseudo-heatmaps guided by the estimated cross-student uncertainty. We evaluate our proposed method on multiple evaluation setups on the COCO benchmark. Our results show that our model outperforms previous state-of-the-art semi-supervised pose estimators, especially in extreme low-data regime. For example with only 0.5K labeled images our method is capable of surpassing the best competitor by 7.22 mAP (+25% absolute improvement). We also demonstrate that our model can learn effectively from unlabeled data in the wild to further boost its generalization and performance.
</details>
<details>
<summary>摘要</summary>
我们提出一种新的半监督学习设计，用于人姿估计，这种设计基于流行的双学生框架，并在两个方面进行了改进。首先，我们引入一种去噪方案，以生成可靠的假热图作为不监督学习的目标。这使用多视图扩展和阈值和精炼过程生成一个热图pool。其次，我们根据这些热图pool中的估计交叉学生不确定性选择学习目标。我们在COCO标准库上多个评估setup中评估了我们的提议方法，结果显示，我们的模型在低数据量情况下超过了之前的半监督pose估计器，具体提高了7.22个mAP (+25%相对提高)。此外，我们还证明了我们的模型可以从无标注数据中学习，并在野外数据中进一步提高其普适性和性能。
</details></li>
</ul>
<hr>
<h2 id="Towards-Few-Call-Model-Stealing-via-Active-Self-Paced-Knowledge-Distillation-and-Diffusion-Based-Image-Generation"><a href="#Towards-Few-Call-Model-Stealing-via-Active-Self-Paced-Knowledge-Distillation-and-Diffusion-Based-Image-Generation" class="headerlink" title="Towards Few-Call Model Stealing via Active Self-Paced Knowledge Distillation and Diffusion-Based Image Generation"></a>Towards Few-Call Model Stealing via Active Self-Paced Knowledge Distillation and Diffusion-Based Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00096">http://arxiv.org/abs/2310.00096</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vlad Hondru, Radu Tudor Ionescu</li>
<li>for: 本研究旨在探讨一种新的应用场景，即使用Diffusion模型来 копироваblack-box类型的图像分类模型，无需访问原始训练数据、模型 architecture和参数。</li>
<li>methods: 我们提出了一种新的框架，使用Diffusion模型生成的假数据集（ proxy data set）作为训练数据，并采用最大化API调用数量的限制。我们通过将一定数量的样本传递 через黑盒模型，收集labels，然后通过沟通知论把黑盒教师（攻击模型）的知识 transferred 到学生模型（复制模型）中。</li>
<li>results: 我们的实验结果表明，我们的框架在几个API调用的情况下比两种现状顶尖方法更高效。<details>
<summary>Abstract</summary>
Diffusion models showcased strong capabilities in image synthesis, being used in many computer vision tasks with great success. To this end, we propose to explore a new use case, namely to copy black-box classification models without having access to the original training data, the architecture, and the weights of the model, \ie~the model is only exposed through an inference API. More specifically, we can only observe the (soft or hard) labels for some image samples passed as input to the model. Furthermore, we consider an additional constraint limiting the number of model calls, mostly focusing our research on few-call model stealing. In order to solve the model extraction task given the applied restrictions, we propose the following framework. As training data, we create a synthetic data set (called proxy data set) by leveraging the ability of diffusion models to generate realistic and diverse images. Given a maximum number of allowed API calls, we pass the respective number of samples through the black-box model to collect labels. Finally, we distill the knowledge of the black-box teacher (attacked model) into a student model (copy of the attacked model), harnessing both labeled and unlabeled data generated by the diffusion model. We employ a novel active self-paced learning framework to make the most of the proxy data during distillation. Our empirical results on two data sets confirm the superiority of our framework over two state-of-the-art methods in the few-call model extraction scenario.
</details>
<details>
<summary>摘要</summary>
Diffusion models have shown great potential in image synthesis and have been successfully applied to various computer vision tasks. In this paper, we aim to explore a new use case for diffusion models: copying black-box classification models without access to the original training data, model architecture, or weights. Specifically, we can only observe the soft or hard labels of some input image samples through the model's inference API. To solve this task under these restrictions, we propose the following framework:1. Create a synthetic data set (proxy data set) using diffusion models to generate realistic and diverse images.2. Pass the maximum number of allowed API calls through the black-box model to collect labels.3. Distill the knowledge of the black-box teacher (attacked model) into a student model (copy of the attacked model) using both labeled and unlabeled data generated by the diffusion model.4. Employ a novel active self-paced learning framework to make the most of the proxy data during distillation.Our experimental results on two data sets demonstrate the superiority of our framework over two state-of-the-art methods in the few-call model extraction scenario.
</details></li>
</ul>
<hr>
<h2 id="DataDAM-Efficient-Dataset-Distillation-with-Attention-Matching"><a href="#DataDAM-Efficient-Dataset-Distillation-with-Attention-Matching" class="headerlink" title="DataDAM: Efficient Dataset Distillation with Attention Matching"></a>DataDAM: Efficient Dataset Distillation with Attention Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00093">http://arxiv.org/abs/2310.00093</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/datadistillation/datadam">https://github.com/datadistillation/datadam</a></li>
<li>paper_authors: Ahmad Sajedi, Samir Khaki, Ehsan Amjadian, Lucy Z. Liu, Yuri A. Lawryshyn, Konstantinos N. Plataniotis</li>
<li>for: 降低深度学习训练成本，保持多个 dataset 上强大的泛化性</li>
<li>methods: 使用高效的 Dataset Distillation with Attention Matching (DataDAM) 方法，通过匹配不同层的神经网络中的空间注意力图来学习高质量的合成图像</li>
<li>results: 在 CIFAR10&#x2F;100、TinyImageNet、ImageNet-1K 和 ImageNet-1K 的一些subset中，与先前方法相比，实现了状态机器的性能，并在 CIFAR100 和 ImageNet-1K 上实现了提高分别为 6.5% 和 4.1%。<details>
<summary>Abstract</summary>
Researchers have long tried to minimize training costs in deep learning while maintaining strong generalization across diverse datasets. Emerging research on dataset distillation aims to reduce training costs by creating a small synthetic set that contains the information of a larger real dataset and ultimately achieves test accuracy equivalent to a model trained on the whole dataset. Unfortunately, the synthetic data generated by previous methods are not guaranteed to distribute and discriminate as well as the original training data, and they incur significant computational costs. Despite promising results, there still exists a significant performance gap between models trained on condensed synthetic sets and those trained on the whole dataset. In this paper, we address these challenges using efficient Dataset Distillation with Attention Matching (DataDAM), achieving state-of-the-art performance while reducing training costs. Specifically, we learn synthetic images by matching the spatial attention maps of real and synthetic data generated by different layers within a family of randomly initialized neural networks. Our method outperforms the prior methods on several datasets, including CIFAR10/100, TinyImageNet, ImageNet-1K, and subsets of ImageNet-1K across most of the settings, and achieves improvements of up to 6.5% and 4.1% on CIFAR100 and ImageNet-1K, respectively. We also show that our high-quality distilled images have practical benefits for downstream applications, such as continual learning and neural architecture search.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Robustness-of-AI-Image-Detectors-Fundamental-Limits-and-Practical-Attacks"><a href="#Robustness-of-AI-Image-Detectors-Fundamental-Limits-and-Practical-Attacks" class="headerlink" title="Robustness of AI-Image Detectors: Fundamental Limits and Practical Attacks"></a>Robustness of AI-Image Detectors: Fundamental Limits and Practical Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00076">http://arxiv.org/abs/2310.00076</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mehrdad Saberi, Vinu Sankar Sadasivan, Keivan Rezaei, Aounon Kumar, Atoosa Chegini, Wenxiao Wang, Soheil Feizi</li>
<li>for: 本研究旨在探讨用于分辨真实图像和人工智能生成的图像的检测技术，以避免使用假图像作为真实图像。</li>
<li>methods: 本研究使用了许多检测技术，包括水印方法和深度模型检测器。</li>
<li>results: 研究发现，对于低扰动量水印方法，存在一种基本的负反向关系，即避免错误率和伪装错误率之间的负反向关系。此外，研究还发现了一种替换模型攻击，可以成功地去除水印。此外，研究还发现了水印方法对于伪装攻击的漏洞，可能导致真实图像被误认为是伪图像。<details>
<summary>Abstract</summary>
In light of recent advancements in generative AI models, it has become essential to distinguish genuine content from AI-generated one to prevent the malicious usage of fake materials as authentic ones and vice versa. Various techniques have been introduced for identifying AI-generated images, with watermarking emerging as a promising approach. In this paper, we analyze the robustness of various AI-image detectors including watermarking and classifier-based deepfake detectors. For watermarking methods that introduce subtle image perturbations (i.e., low perturbation budget methods), we reveal a fundamental trade-off between the evasion error rate (i.e., the fraction of watermarked images detected as non-watermarked ones) and the spoofing error rate (i.e., the fraction of non-watermarked images detected as watermarked ones) upon an application of a diffusion purification attack. In this regime, we also empirically show that diffusion purification effectively removes watermarks with minimal changes to images. For high perturbation watermarking methods where notable changes are applied to images, the diffusion purification attack is not effective. In this case, we develop a model substitution adversarial attack that can successfully remove watermarks. Moreover, we show that watermarking methods are vulnerable to spoofing attacks where the attacker aims to have real images (potentially obscene) identified as watermarked ones, damaging the reputation of the developers. In particular, by just having black-box access to the watermarking method, we show that one can generate a watermarked noise image which can be added to the real images to have them falsely flagged as watermarked ones. Finally, we extend our theory to characterize a fundamental trade-off between the robustness and reliability of classifier-based deep fake detectors and demonstrate it through experiments.
</details>
<details>
<summary>摘要</summary>
随着生成型人工智能模型的发展，识别真实内容和人工生成的内容变得非常重要，以避免使用假材料作为真实内容并 vice versa。各种技术已经被提出来识别人工生成的图像，其中水印技术成为了一种有希望的方法。在这篇论文中，我们分析了不同的人工生成图像检测器，包括水印和深度骗局检测器。对于低干扰量水印方法，我们发现了一个基本的负面负担关系，即逃脱错误率（即水印图像被识别为非水印图像的比率）和骗局错误率（即非水印图像被识别为水印图像的比率）。在这种情况下，我们也实证表明了 diffusion purification 攻击可以有效地去除水印。对于高干扰量水印方法， diffusion purification 攻击不生效。在这种情况下，我们开发了模型替换对抗攻击，可以成功地去除水印。此外，我们还发现了水印方法对骗局攻击（攻击者尝试将真实图像（可能是不良的）识别为水印图像，从而损害开发者的声誉）的漏洞。具体来说，只要对水印方法有黑盒访问，就可以生成一个水印图像，并将其添加到真实图像中，以误导系统将真实图像识别为水印图像。最后，我们扩展了我们的理论，并通过实验来描述一个基本的负面负担关系，即检测器的可靠性和robustness之间的负面负担关系。
</details></li>
</ul>
<hr>
<h2 id="Multi-task-View-Synthesis-with-Neural-Radiance-Fields"><a href="#Multi-task-View-Synthesis-with-Neural-Radiance-Fields" class="headerlink" title="Multi-task View Synthesis with Neural Radiance Fields"></a>Multi-task View Synthesis with Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17450">http://arxiv.org/abs/2309.17450</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zsh2000/muvienerf">https://github.com/zsh2000/muvienerf</a></li>
<li>paper_authors: Shuhong Zheng, Zhipeng Bao, Martial Hebert, Yu-Xiong Wang</li>
<li>for: 本研究旨在探讨多任务视觉学中的多视图同时Synthesize多种场景属性的问题，并提出了一种名为Multi-task View Synthesis（MTVS）的问题设定。</li>
<li>methods: 该研究提出了一种名为MuvieNeRF的框架，该框架包括了多任务和跨视图知识的 integrate，以同时Synthesize多种场景属性。MuvieNeRF包括了两个关键模块：跨任务注意力（CTA）模块和跨视图注意力（CVA）模块，这两个模块使得信息可以有效地在多个视图和任务之间进行交互。</li>
<li>results: 对于 synthetic 和 realistic 的标准准则进行了广泛的评估，表明MuvieNeRF可以同时Synthesize多种场景属性，并且在不同的 NeRF 背景下表现优秀。特别是，我们展示了MuvieNeRF在不同的应用场景中具有通用性。代码可以在<a target="_blank" rel="noopener" href="https://github.com/zsh2000/MuvieNeRF%E4%B8%AD%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/zsh2000/MuvieNeRF中下载。</a><details>
<summary>Abstract</summary>
Multi-task visual learning is a critical aspect of computer vision. Current research, however, predominantly concentrates on the multi-task dense prediction setting, which overlooks the intrinsic 3D world and its multi-view consistent structures, and lacks the capability for versatile imagination. In response to these limitations, we present a novel problem setting -- multi-task view synthesis (MTVS), which reinterprets multi-task prediction as a set of novel-view synthesis tasks for multiple scene properties, including RGB. To tackle the MTVS problem, we propose MuvieNeRF, a framework that incorporates both multi-task and cross-view knowledge to simultaneously synthesize multiple scene properties. MuvieNeRF integrates two key modules, the Cross-Task Attention (CTA) and Cross-View Attention (CVA) modules, enabling the efficient use of information across multiple views and tasks. Extensive evaluation on both synthetic and realistic benchmarks demonstrates that MuvieNeRF is capable of simultaneously synthesizing different scene properties with promising visual quality, even outperforming conventional discriminative models in various settings. Notably, we show that MuvieNeRF exhibits universal applicability across a range of NeRF backbones. Our code is available at https://github.com/zsh2000/MuvieNeRF.
</details>
<details>
<summary>摘要</summary>
多任务视觉学是计算机视觉的关键方面。然而，当前研究主要集中在多任务紧凑预测设置中，忽略了自然3D世界的多视点共同结构，而且缺乏多样化的想象能力。为了解决这些局限性，我们提出了一个新的问题设定---多任务视觉合成（MTVS），将多任务预测重新解释为多个场景属性的新视图合成任务。为了解决MTVS问题，我们提出了MuvieNeRF框架，该框架包含了多任务和交叉视图知识的两个关键模块：交叉任务注意力（CTA）和交叉视图注意力（CVA）模块。这两个模块使得可以有效利用多视点和任务之间的信息。我们对各种 sintetic和实际的标准块进行了广泛的评估，并证明了MuvieNeRF可以同时合成不同场景属性，并且视觉质量得到了广泛的提高。特别是，我们表明了MuvieNeRF在不同的NeRF底层模型上 exhibits 普遍可用性。我们的代码可以在 GitHub 上找到。
</details></li>
</ul>
<hr>
<h2 id="SMPLer-X-Scaling-Up-Expressive-Human-Pose-and-Shape-Estimation"><a href="#SMPLer-X-Scaling-Up-Expressive-Human-Pose-and-Shape-Estimation" class="headerlink" title="SMPLer-X: Scaling Up Expressive Human Pose and Shape Estimation"></a>SMPLer-X: Scaling Up Expressive Human Pose and Shape Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17448">http://arxiv.org/abs/2309.17448</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/caizhongang/SMPLer-X">https://github.com/caizhongang/SMPLer-X</a></li>
<li>paper_authors: Zhongang Cai, Wanqi Yin, Ailing Zeng, Chen Wei, Qingping Sun, Yanjun Wang, Hui En Pang, Haiyi Mei, Mingyuan Zhang, Lei Zhang, Chen Change Loy, Lei Yang, Ziwei Liu</li>
<li>For: 本研究旨在推进人体 pose 和形态估计（EHPS）领域的发展，通过大量数据和大型模型的使用，实现对多种场景的人体动作和形态估计。* Methods: 本研究使用了许多 EHPS 数据集，并对模型训练 scheme 进行了系统atic investigation，以便选择最适合的数据集和训练策略。此外，研究还使用了视力 transformer 来研究模型大小的扩展法律，并实现了模型的特殊化。* Results: 研究表明，通过使用大量数据和大型模型，可以实现对多种场景的人体动作和形态估计的出色表现，并且可以在不同的环境下进行出色的转移。特别是，基于 SMPLer-X 的基础模型可以在七个测试标准中占据国际第一名，包括 AGORA（107.2 mm NMVE）、UBody（57.4 mm PVE）、EgoBody（63.6 mm PVE）和 EHF（62.3 mm PVE 无需 fine-tuning）等。<details>
<summary>Abstract</summary>
Expressive human pose and shape estimation (EHPS) unifies body, hands, and face motion capture with numerous applications. Despite encouraging progress, current state-of-the-art methods still depend largely on a confined set of training datasets. In this work, we investigate scaling up EHPS towards the first generalist foundation model (dubbed SMPLer-X), with up to ViT-Huge as the backbone and training with up to 4.5M instances from diverse data sources. With big data and the large model, SMPLer-X exhibits strong performance across diverse test benchmarks and excellent transferability to even unseen environments. 1) For the data scaling, we perform a systematic investigation on 32 EHPS datasets, including a wide range of scenarios that a model trained on any single dataset cannot handle. More importantly, capitalizing on insights obtained from the extensive benchmarking process, we optimize our training scheme and select datasets that lead to a significant leap in EHPS capabilities. 2) For the model scaling, we take advantage of vision transformers to study the scaling law of model sizes in EHPS. Moreover, our finetuning strategy turn SMPLer-X into specialist models, allowing them to achieve further performance boosts. Notably, our foundation model SMPLer-X consistently delivers state-of-the-art results on seven benchmarks such as AGORA (107.2 mm NMVE), UBody (57.4 mm PVE), EgoBody (63.6 mm PVE), and EHF (62.3 mm PVE without finetuning). Homepage: https://caizhongang.github.io/projects/SMPLer-X/
</details>
<details>
<summary>摘要</summary>
traditional Chinese:这个研究探讨了将表达人体姿态和形状估计（EHPS）与许多应用结合起来，尽管现有的进步已经很鼓舞人，但现在的州OF-the-art方法仍然受到一个仅仅有限的训练数据集的限制。在这个工作中，我们investigated scaling up EHPS towards the first generalist foundation model（dubbed SMPLer-X），使用Up to ViT-Huge as the backbone和Up to 4.5M个实例从多元数据来训练。With big data和大型模型，SMPLer-X exhibits strong performance across diverse test benchmarks and excellent transferability to even unseen environments。1. 我们进行了系统性的调查，探讨在32个EHPS数据集中，包括各种情况，一个训练在任何单一数据集上的模型无法应对的各种情况。更重要的是，我们根据广泛的 benchmarking 过程中所获得的启示，来优化我们的训练方案，并选择最适合的数据集，从而导致EHPS的能力获得了重大的提升。2. 我们利用 Computer Vision Transformers 来研究EHPS中模型的扩展法则。此外，我们还提出了一种finetuning 策略，将SMPLer-X转换成专家模型，以获得更高的性能提升。值得注意的是，我们的基础模型SMPLer-X在七个benchmark上均 delivery state-of-the-art results，包括AGORA（107.2 mm NMVE）、UBody（57.4 mm PVE）、EgoBody（63.6 mm PVE）和EHF（62.3 mm PVE without finetuning）。更多信息可以在以下页面上找到：https://caizhongang.github.io/projects/SMPLer-X/
</details></li>
</ul>
<hr>
<h2 id="FACTS-First-Amplify-Correlations-and-Then-Slice-to-Discover-Bias"><a href="#FACTS-First-Amplify-Correlations-and-Then-Slice-to-Discover-Bias" class="headerlink" title="FACTS: First Amplify Correlations and Then Slice to Discover Bias"></a>FACTS: First Amplify Correlations and Then Slice to Discover Bias</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17430">http://arxiv.org/abs/2309.17430</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yvsriram/facts">https://github.com/yvsriram/facts</a></li>
<li>paper_authors: Sriram Yenamandra, Pratik Ramesh, Viraj Prabhu, Judy Hoffman</li>
<li>for: 本研究旨在identifying spurious correlations in computer vision datasets, in order to improve downstream bias mitigation strategies.</li>
<li>methods: 我们提出了一种方法 called First Amplify Correlations and Then Slice to Discover Bias (FACTS), which involves amplifying correlations and then performing correlation-aware slicing to identify underperforming data slices.</li>
<li>results: 我们的方法可以 considerably improve correlation bias identification compared to prior work, with improvements of up to 35% precision@10 in a range of diverse evaluation settings.<details>
<summary>Abstract</summary>
Computer vision datasets frequently contain spurious correlations between task-relevant labels and (easy to learn) latent task-irrelevant attributes (e.g. context). Models trained on such datasets learn "shortcuts" and underperform on bias-conflicting slices of data where the correlation does not hold. In this work, we study the problem of identifying such slices to inform downstream bias mitigation strategies. We propose First Amplify Correlations and Then Slice to Discover Bias (FACTS), wherein we first amplify correlations to fit a simple bias-aligned hypothesis via strongly regularized empirical risk minimization. Next, we perform correlation-aware slicing via mixture modeling in bias-aligned feature space to discover underperforming data slices that capture distinct correlations. Despite its simplicity, our method considerably improves over prior work (by as much as 35% precision@10) in correlation bias identification across a range of diverse evaluation settings. Our code is available at: https://github.com/yvsriram/FACTS.
</details>
<details>
<summary>摘要</summary>
computer vision datasets  часто包含偶极性关系 между任务相关标签和（容易学习的）隐藏任务不相关属性（例如上下文）。模型在这些 dataset 上训练时会学习 "短cut"，并在数据 slice 中表现不佳，其中 correlation 不存在。在这项工作中，我们研究如何认识这些 slice 以便下游减偏策略。我们提出了 First Amplify Correlations and Then Slice to Discover Bias 方法（FACTS），其中首先使用强regularization 的实际风险最小化来适应偶极性假设。接着，我们通过混合模型在偏aligned feature space 中进行相关探测，以发现表现不佳的数据 slice，这些 slice 捕捉了不同的相关性。尽管其简单，我们的方法在多种多样的评估环境中可以明显提高对偶极性标识的性能，提高了至多 35% 的精度@10。我们的代码可以在：https://github.com/yvsriram/FACTS 中找到。
</details></li>
</ul>
<hr>
<h2 id="Directly-Fine-Tuning-Diffusion-Models-on-Differentiable-Rewards"><a href="#Directly-Fine-Tuning-Diffusion-Models-on-Differentiable-Rewards" class="headerlink" title="Directly Fine-Tuning Diffusion Models on Differentiable Rewards"></a>Directly Fine-Tuning Diffusion Models on Differentiable Rewards</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17400">http://arxiv.org/abs/2309.17400</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kevin Clark, Paul Vicol, Kevin Swersky, David J Fleet</li>
<li>for: 本研究旨在提出一种简单有效的方法，用于微调Diffusion模型，以优化可导 reward函数的极大化。</li>
<li>methods: 我们首先证明了，可以通过整个抽象过程来倒推 reward function 的梯度，并且这种方法可以在不同的奖励函数下达到优秀的性能。然后，我们提出了DRaFT变体：DRaFT-K和DRaFT-LV，它们分别 truncates backpropagation 到最后 K 步和只有一步。我们证明了我们的方法在不同的奖励函数下都能够达到好的性能。</li>
<li>results: 我们的实验结果表明，DRaFT 可以在不同的 reward 函数下提高图像的艺术质量。此外，我们还提出了与先前的工作之间的连接，为 gradient-based 微调算法的设计空间提供了一个统一的视角。<details>
<summary>Abstract</summary>
We present Direct Reward Fine-Tuning (DRaFT), a simple and effective method for fine-tuning diffusion models to maximize differentiable reward functions, such as scores from human preference models. We first show that it is possible to backpropagate the reward function gradient through the full sampling procedure, and that doing so achieves strong performance on a variety of rewards, outperforming reinforcement learning-based approaches. We then propose more efficient variants of DRaFT: DRaFT-K, which truncates backpropagation to only the last K steps of sampling, and DRaFT-LV, which obtains lower-variance gradient estimates for the case when K=1. We show that our methods work well for a variety of reward functions and can be used to substantially improve the aesthetic quality of images generated by Stable Diffusion 1.4. Finally, we draw connections between our approach and prior work, providing a unifying perspective on the design space of gradient-based fine-tuning algorithms.
</details>
<details>
<summary>摘要</summary>
我们介绍 Direct Reward Fine-Tuning (DRaFT)，一种简单有效的方法，可以调整传播模型以最大化可微分的奖励函数，例如人类偏好模型中的分数。我们首先显示了可以将奖励函数梯度传递到整个抽样过程中，并且这会实现强大的表现在多种奖励下，超过循环学习基础的方法。然后我们提出了DRaFT-K和DRaFT-LV，这两种方法分别是将梯度传递到最后K步抽样中和仅将梯度估计为一个低方差的情况下。我们显示了我们的方法可以适用于多种奖励函数，并且可以对Stable Diffusion 1.4中生成的图像进行重大改善。最后，我们将我们的方法与先前的工作进行比较，提供了一个统一的观点，对于梯度基本调整算法的设计空间。
</details></li>
</ul>
<hr>
<h2 id="IFAST-Weakly-Supervised-Interpretable-Face-Anti-spoofing-from-Single-shot-Binocular-NIR-Images"><a href="#IFAST-Weakly-Supervised-Interpretable-Face-Anti-spoofing-from-Single-shot-Binocular-NIR-Images" class="headerlink" title="IFAST: Weakly Supervised Interpretable Face Anti-spoofing from Single-shot Binocular NIR Images"></a>IFAST: Weakly Supervised Interpretable Face Anti-spoofing from Single-shot Binocular NIR Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17399">http://arxiv.org/abs/2309.17399</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiancheng Huang, Donghao Zhou, Shifeng Chen</li>
<li>for: 本研究旨在提高单框面反护身份验证系统的安全性，并且只需要静止图像作为输入。</li>
<li>methods: 本研究使用了一种名为Interpretable FAS Transformer（IFAST）的新方法，该方法只需要弱级指导来生成可解释的预测结果。</li>
<li>results: 经过广泛的实验，我们的IFAST方法可以在一个大量的binocular NIR图像数据集（BNI-FAS）上 дости得状态的前两名的 результаados，证明单框面反护身份验证可以基于双目NIR图像进行。<details>
<summary>Abstract</summary>
Single-shot face anti-spoofing (FAS) is a key technique for securing face recognition systems, and it requires only static images as input. However, single-shot FAS remains a challenging and under-explored problem due to two main reasons: 1) on the data side, learning FAS from RGB images is largely context-dependent, and single-shot images without additional annotations contain limited semantic information. 2) on the model side, existing single-shot FAS models are infeasible to provide proper evidence for their decisions, and FAS methods based on depth estimation require expensive per-pixel annotations. To address these issues, a large binocular NIR image dataset (BNI-FAS) is constructed and published, which contains more than 300,000 real face and plane attack images, and an Interpretable FAS Transformer (IFAST) is proposed that requires only weak supervision to produce interpretable predictions. Our IFAST can produce pixel-wise disparity maps by the proposed disparity estimation Transformer with Dynamic Matching Attention (DMA) block. Besides, a well-designed confidence map generator is adopted to cooperate with the proposed dual-teacher distillation module to obtain the final discriminant results. The comprehensive experiments show that our IFAST can achieve state-of-the-art results on BNI-FAS, proving the effectiveness of the single-shot FAS based on binocular NIR images.
</details>
<details>
<summary>摘要</summary>
单张图像反骗检测（FAS）是保护人脸识别系统的关键技术，它只需要静态图像作为输入。然而，单张图像FAS仍然是一个挑战和未探索的问题，主要由两个原因导致：1）数据方面，从RGB图像学习FAS是高度依赖于上下文，单张图像没有额外标注的情况下具有有限的 semantic information。2）模型方面，现有的单张图像FAS模型无法提供有效的证据，基于深度估计的FAS方法需要每个像素的昂贵标注。为解决这些问题，一个大量的双目near-infrared（NIR）图像集（BNI-FAS）被构建并公布，该集包含了超过300,000个真实的人脸和平面攻击图像，以及一个可解释的FAS转换器（IFAST），该模型只需弱级超vision来生成可解释的预测。我们的IFAST可以生成像素级的差分地图，并采用了一个Well-designed的信任度地图生成器，以及一个 dual-teacher 分离模块，以获得最终的检测结果。广泛的实验表明，我们的IFAST可以在BNI-FAS上达到状态艺术的结果，证明了单张图像FAS的可行性。
</details></li>
</ul>
<hr>
<h2 id="Forward-Flow-for-Novel-View-Synthesis-of-Dynamic-Scenes"><a href="#Forward-Flow-for-Novel-View-Synthesis-of-Dynamic-Scenes" class="headerlink" title="Forward Flow for Novel View Synthesis of Dynamic Scenes"></a>Forward Flow for Novel View Synthesis of Dynamic Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17390">http://arxiv.org/abs/2309.17390</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Guo, Jiadai Sun, Yuchao Dai, Guanying Chen, Xiaoqing Ye, Xiao Tan, Errui Ding, Yumeng Zhang, Jingdong Wang</li>
<li>for: 本文提出了一种基于神经辐射场（NeRF）的方法，用于synthesizing动态场景中的新视图。现有方法frequently采用一个静止的NeRF来表示canonical space，然后通过将采样的3D点映射回canonical space中的学习后向流场景来渲染动态图像。但是，这个后向流场景是不连续的， diffficult to be fitted by commonly used smooth motion models。</li>
<li>methods: 我们提出了一种直接使用forward flow field来截归canonical radiance field到其他时间步骤。这个forward flow field是在物体区域内具有连续和平滑的特点，可以有助于学习动作模型。为实现这一目标，我们使用voxel网格来表示canonical radiance field，并提出了一种可导的截归过程，包括一个平均拼接操作和一个inpaint网络，以解决多对一和一对多映射问题。</li>
<li>results: 我们的方法在对比 existed methods 时表现出优异，在novel view rendering和动作模型学习方面都达到了更高的水平。这demonstrates the effectiveness of our forward flow motion modeling.<details>
<summary>Abstract</summary>
This paper proposes a neural radiance field (NeRF) approach for novel view synthesis of dynamic scenes using forward warping. Existing methods often adopt a static NeRF to represent the canonical space, and render dynamic images at other time steps by mapping the sampled 3D points back to the canonical space with the learned backward flow field. However, this backward flow field is non-smooth and discontinuous, which is difficult to be fitted by commonly used smooth motion models. To address this problem, we propose to estimate the forward flow field and directly warp the canonical radiance field to other time steps. Such forward flow field is smooth and continuous within the object region, which benefits the motion model learning. To achieve this goal, we represent the canonical radiance field with voxel grids to enable efficient forward warping, and propose a differentiable warping process, including an average splatting operation and an inpaint network, to resolve the many-to-one and one-to-many mapping issues. Thorough experiments show that our method outperforms existing methods in both novel view rendering and motion modeling, demonstrating the effectiveness of our forward flow motion modeling. Project page: https://npucvr.github.io/ForwardFlowDNeRF
</details>
<details>
<summary>摘要</summary>
这个论文提出了一种基于神经辐射场（NeRF）的方法，用于新视角合成动态场景中的动态图像Synthesis。现有方法通常采用一个静态NeRF来表示坐标轴空间的底层结构，然后将其他时间步骤中的图像映射回到坐标轴空间中，使用学习的反向流场场所。然而，这个反向流场场是不平滑的，这会使得使用常见的光滑运动模型来拟合困难。为了解决这个问题，我们提出了直接将前进流场场估计到其他时间步骤，以便将坐标轴空间中的Canonical radiance field直接截射到其他时间步骤。这个前进流场场是在物体区域内平滑和连续的，这有助于学习运动模型。为了实现这一目标，我们使用精度的 voxel 网格来表示Canonical radiance field，并提出了一种可导的截射过程，包括一个平均拼接操作和一个填充网络，以解决多对一和一对多的映射问题。经过实验表明，我们的方法在新视角渲染和运动模型学习方面具有优异性，证明了我们的前进流场运动模型的效果。项目页面：https://npucvr.github.io/ForwardFlowDNeRF
</details></li>
</ul>
<hr>
<h2 id="Prompt-based-test-time-real-image-dehazing-a-novel-pipeline"><a href="#Prompt-based-test-time-real-image-dehazing-a-novel-pipeline" class="headerlink" title="Prompt-based test-time real image dehazing: a novel pipeline"></a>Prompt-based test-time real image dehazing: a novel pipeline</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17389">http://arxiv.org/abs/2309.17389</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cecret3350/PTTD-Dehazing">https://github.com/cecret3350/PTTD-Dehazing</a></li>
<li>paper_authors: Zixuan Chen, Zewei He, Ziqian Lu, Zhe-Ming Lu</li>
<li>for: 提高模型对实际雾气图像的泛化能力</li>
<li>methods: 使用提示生成模块（PGM）和特征适应模块（FAM）对已经训练过的某些雾气模型进行微调，以适应实际雾气图像</li>
<li>results: 在实际雾气图像场景下，比对使用现有的雾气模型和PTTD模型，PTTD模型能够更好地提高图像的可见度和细节表示，同时具有更好的泛化能力和灵活性。<details>
<summary>Abstract</summary>
Existing methods attempt to improve models' generalization ability on real-world hazy images by exploring well-designed training schemes (e.g., cycleGAN, prior loss). However, most of them need very complicated training procedures to achieve satisfactory results. In this work, we present a totally novel testing pipeline called Prompt-based Test-Time Dehazing (PTTD) to help generate visually pleasing results of real-captured hazy images during the inference phase. We experimentally find that given a dehazing model trained on synthetic data, by fine-tuning the statistics (i.e., mean and standard deviation) of encoding features, PTTD is able to narrow the domain gap, boosting the performance of real image dehazing. Accordingly, we first apply a prompt generation module (PGM) to generate a visual prompt, which is the source of appropriate statistical perturbations for mean and standard deviation. And then, we employ the feature adaptation module (FAM) into the existing dehazing models for adjusting the original statistics with the guidance of the generated prompt. Note that, PTTD is model-agnostic and can be equipped with various state-of-the-art dehazing models trained on synthetic hazy-clean pairs. Extensive experimental results demonstrate that our PTTD is flexible meanwhile achieves superior performance against state-of-the-art dehazing methods in real-world scenarios. The source code of our PTTD will be made available at https://github.com/cecret3350/PTTD-Dehazing.
</details>
<details>
<summary>摘要</summary>
现有方法尝试提高模型对实际雾图像的泛化能力，通过设计有利的训练方案（如循环GAN、先验损失）。然而，大多数方法需要非常复杂的训练过程来获得满意的结果。在这项工作中，我们提出了一个全新的测试管道，即Prompt-based Test-Time Dehazing（PTTD），以帮助在推理阶段生成高质量的实际雾图像结果。我们实验发现，对于已经训练在 sintetic hazy-clean 对的模型，通过细化编码特征统计信息（即均值和标准差），PTTD可以降低领域差距，提高实际雾图像的表现。我们首先使用Prompt Generation Module（PGM）生成视觉提示，该提示是适合编码特征统计信息的适应性扰动的来源。然后，我们在现有的朔杂雾模型中引入特征适应模块（FAM），以适应原始统计信息的修正。需要注意的是，PTTD是模型无关的，可以与多种现有的 state-of-the-art 朔杂雾模型结合使用。我们的PTTD在实际场景中表现出优于现有的朔杂雾方法，并且具有高灵活性。我们将PTTD的源代码发布在GitHub上，可以在 <https://github.com/cecret3350/PTTD-Dehazing> 中下载。
</details></li>
</ul>
<hr>
<h2 id="Network-Memory-Footprint-Compression-Through-Jointly-Learnable-Codebooks-and-Mappings"><a href="#Network-Memory-Footprint-Compression-Through-Jointly-Learnable-Codebooks-and-Mappings" class="headerlink" title="Network Memory Footprint Compression Through Jointly Learnable Codebooks and Mappings"></a>Network Memory Footprint Compression Through Jointly Learnable Codebooks and Mappings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17361">http://arxiv.org/abs/2309.17361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edouard Yvinec, Arnaud Dapogny, Kevin Bailly</li>
<li>for: 减少深度神经网络（DNNs）的内存占用，以便在常见的移动设备上加载模型。</li>
<li>methods: 提议使用可编程的codebooks，并通过初始化和学习来确定最佳的映射。</li>
<li>results: 实现了高效地压缩Llama 7B模型，可以在5年前的手机上加载。<details>
<summary>Abstract</summary>
The massive interest in deep neural networks (DNNs) for both computer vision and natural language processing has been sparked by the growth in computational power. However, this led to an increase in the memory footprint, to a point where it can be challenging to simply load a model on commodity devices such as mobile phones. To address this limitation, quantization is a favored solution as it maps high precision tensors to a low precision, memory efficient format. In terms of memory footprint reduction, its most effective variants are based on codebooks. These methods, however, suffer from two limitations. First, they either define a single codebook for each tensor, or use a memory-expensive mapping to multiple codebooks. Second, gradient descent optimization of the mapping favors jumps toward extreme values, hence not defining a proximal search. In this work, we propose to address these two limitations. First, we initially group similarly distributed neurons and leverage the re-ordered structure to either apply different scale factors to the different groups, or map weights that fall in these groups to several codebooks, without any mapping overhead. Second, stemming from this initialization, we propose a joint learning of the codebook and weight mappings that bears similarities with recent gradient-based post-training quantization techniques. Third, drawing estimation from straight-through estimation techniques, we introduce a novel gradient update definition to enable a proximal search of the codebooks and their mappings. The proposed jointly learnable codebooks and mappings (JLCM) method allows a very efficient approximation of any DNN: as such, a Llama 7B can be compressed down to 2Go and loaded on 5-year-old smartphones.
</details>
<details>
<summary>摘要</summary>
“深度神经网络（DNN）的巨大兴趣，尤其是计算机视觉和自然语言处理，归功于计算力的增长。然而，这也导致模型的内存占用量增加，到了一定程度，甚至无法将模型加载到常见设备，如智能手机上。为解决这个限制，量化成为一种非常受欢迎的解决方案，它将高精度张量映射到低精度、内存效率高的格式上。在内存占用量减少方面，最有效的变种是基于codebook。然而，这些方法受到两个限制。首先，它们可能会定义每个张量都有自己的codebook，或者使用内存昂贵的映射来多个codebook。其次，对于梯度下降优化，映射偏好跳跃到极值，因此不能定义 proximal search。在这个工作中，我们提出了解决这两个限制的方法。首先，我们将相似分布的神经元Initially group together，并利用这些结构的重新排序来应用不同的准则因子或者映射多个codebook中的映射，无需任何映射开销。其次，基于这种初始化，我们提议一种联合学习codebook和映射的方法，与最近的梯度基于后期quantization技术相似。此外，我们还引入了一种新的梯度更新定义，以实现 proximal search 的搜索。我们提出的联合学习codebooks和映射（JLCM）方法，可以非常有效地近似任何DNN，例如，一个Llama 7B可以压缩到2Go，并在5年前的智能手机上加载。”
</details></li>
</ul>
<hr>
<h2 id="Towards-Free-Data-Selection-with-General-Purpose-Models"><a href="#Towards-Free-Data-Selection-with-General-Purpose-Models" class="headerlink" title="Towards Free Data Selection with General-Purpose Models"></a>Towards Free Data Selection with General-Purpose Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17342">http://arxiv.org/abs/2309.17342</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yichen928/freesel">https://github.com/yichen928/freesel</a></li>
<li>paper_authors: Yichen Xie, Mingyu Ding, Masayoshi Tomizuka, Wei Zhan</li>
<li>for: 提高限制的注释预算下的数据选择效率，以最大化数据选择的用处。</li>
<li>methods: 使用现有的通用模型进行单次推理，不需要额外的训练或监督，直接从多个数据集中选择数据样本。定义基于中间特征的语义模式，以捕捉每幅图像中细腻的地方信息。使用距离基于语义模式的样本选择，不需要批处理选择过程。</li>
<li>results: 在多种计算机视觉任务上实现了显著改进，与现有活跃学习方法相比，速度提高530倍。<details>
<summary>Abstract</summary>
A desirable data selection algorithm can efficiently choose the most informative samples to maximize the utility of limited annotation budgets. However, current approaches, represented by active learning methods, typically follow a cumbersome pipeline that iterates the time-consuming model training and batch data selection repeatedly. In this paper, we challenge this status quo by designing a distinct data selection pipeline that utilizes existing general-purpose models to select data from various datasets with a single-pass inference without the need for additional training or supervision. A novel free data selection (FreeSel) method is proposed following this new pipeline. Specifically, we define semantic patterns extracted from inter-mediate features of the general-purpose model to capture subtle local information in each image. We then enable the selection of all data samples in a single pass through distance-based sampling at the fine-grained semantic pattern level. FreeSel bypasses the heavy batch selection process, achieving a significant improvement in efficiency and being 530x faster than existing active learning methods. Extensive experiments verify the effectiveness of FreeSel on various computer vision tasks. Our code is available at https://github.com/yichen928/FreeSel.
</details>
<details>
<summary>摘要</summary>
一种愿景优选算法可以高效地选择最有用的样本，以最大化limited的注释预算。然而，现有的方法，通常是通过活动学习方法，iterates占用时间consuming的模型训练和批处理数据选择。在这篇论文中，我们挑战这个状况，并提出一个新的数据选择管道。我们使用现有的通用模型来选择不同的数据集中的样本，而不需要额外的训练或监督。我们提出了一种新的自由数据选择（FreeSel）方法。我们定义通过通用模型中的间隔特征EXTRACTINGsemantic pattern，以捕捉每个图像中的细节信息。然后，我们通过基于距离的样本选择，在细化的semantic pattern level上选择所有的数据样本。FreeSel可以 circumvent批处理选择过程，实现了 significantefficiency提升，比现有的活动学习方法快530倍。我们的代码可以在https://github.com/yichen928/FreeSel上下载。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format instead.
</details></li>
</ul>
<hr>
<h2 id="See-Beyond-Seeing-Robust-3D-Object-Detection-from-Point-Clouds-via-Cross-Modal-Hallucination"><a href="#See-Beyond-Seeing-Robust-3D-Object-Detection-from-Point-Clouds-via-Cross-Modal-Hallucination" class="headerlink" title="See Beyond Seeing: Robust 3D Object Detection from Point Clouds via Cross-Modal Hallucination"></a>See Beyond Seeing: Robust 3D Object Detection from Point Clouds via Cross-Modal Hallucination</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17336">http://arxiv.org/abs/2309.17336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianning Deng, Gabriel Chan, Hantao Zhong, Chris Xiaoxuan Lu<br>for:* 这种新的框架是用于鲁棒的3D物体检测从点云数据中，通过跨模态梦境投影来实现。methods:* 我们提出了多种对齐方法，包括空间对齐和特征对齐，以实现同时的后缘修正和梦境生成。results:* 我们的方法可以更好地处理具有各种频谱特征的检测问题，并且在训练和测试阶段都具有优秀的性能和效率。<details>
<summary>Abstract</summary>
This paper presents a novel framework for robust 3D object detection from point clouds via cross-modal hallucination. Our proposed approach is agnostic to either hallucination direction between LiDAR and 4D radar. We introduce multiple alignments on both spatial and feature levels to achieve simultaneous backbone refinement and hallucination generation. Specifically, spatial alignment is proposed to deal with the geometry discrepancy for better instance matching between LiDAR and radar. The feature alignment step further bridges the intrinsic attribute gap between the sensing modalities and stabilizes the training. The trained object detection models can deal with difficult detection cases better, even though only single-modal data is used as the input during the inference stage. Extensive experiments on the View-of-Delft (VoD) dataset show that our proposed method outperforms the state-of-the-art (SOTA) methods for both radar and LiDAR object detection while maintaining competitive efficiency in runtime.
</details>
<details>
<summary>摘要</summary>
To address the geometry discrepancy between LiDAR and radar, we propose spatial alignment for better instance matching. Additionally, we use feature alignment to bridge the intrinsic attribute gap between the sensing modalities and stabilize the training.Our proposed method can handle difficult detection cases with single-modal data as input during the inference stage, and outperforms state-of-the-art methods for both radar and LiDAR object detection while maintaining competitive efficiency in runtime. We demonstrate the effectiveness of our approach through extensive experiments on the View-of-Delft (VoD) dataset.
</details></li>
</ul>
<hr>
<h2 id="Multi-Depth-Branches-Network-for-Efficient-Image-Super-Resolution"><a href="#Multi-Depth-Branches-Network-for-Efficient-Image-Super-Resolution" class="headerlink" title="Multi-Depth Branches Network for Efficient Image Super-Resolution"></a>Multi-Depth Branches Network for Efficient Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17334">http://arxiv.org/abs/2309.17334</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thy960112/mdbn">https://github.com/thy960112/mdbn</a></li>
<li>paper_authors: Huiyuan Tian, Li Zhang, Shijian Li, Min Yao, Gang Pan</li>
<li>for: 这个论文主要针对超解像（SR）领域的问题，旨在提高 convolutional neural networks（CNNs）基于 SR 模型的性能，特别是Restore高频环境信息而忽略低频环境信息。</li>
<li>methods: 该论文提出了一种 Multi-Depth Branches Network (MDBN) 框架，它是基于 ResNet 架构的延伸，通过添加一个额外的分支来捕捉图像中重要的结构特征。我们提出的多深分支模块 (MDBM) 通过在不同的分支中堆叠同样大小的卷积核，使得不同的分支可以捕捉不同的信息，例如，低频环境信息和高频环境信息。</li>
<li>results: 我们的模型在 SR 领域实现了更好的性能，并且在 computation overhead 方面具有更好的性能。我们的代码可以在 <a target="_blank" rel="noopener" href="https://github.com/thy960112/MDBN">https://github.com/thy960112/MDBN</a> 上获取。<details>
<summary>Abstract</summary>
Significant progress has been made in the field of super-resolution (SR), yet many convolutional neural networks (CNNs) based SR models primarily focus on restoring high-frequency details, often overlooking crucial low-frequency contour information. Transformer-based SR methods, while incorporating global structural details, frequently come with an abundance of parameters, leading to high computational overhead. In this paper, we address these challenges by introducing a Multi-Depth Branches Network (MDBN). This framework extends the ResNet architecture by integrating an additional branch that captures vital structural characteristics of images. Our proposed multi-depth branches module (MDBM) involves the stacking of convolutional kernels of identical size at varying depths within distinct branches. By conducting a comprehensive analysis of the feature maps, we observe that branches with differing depths can extract contour and detail information respectively. By integrating these branches, the overall architecture can preserve essential low-frequency semantic structural information during the restoration of high-frequency visual elements, which is more closely with human visual cognition. Compared to GoogLeNet-like models, our basic multi-depth branches structure has fewer parameters, higher computational efficiency, and improved performance. Our model outperforms state-of-the-art (SOTA) lightweight SR methods with less inference time. Our code is available at https://github.com/thy960112/MDBN
</details>
<details>
<summary>摘要</summary>
“在超分解（SR）领域中，有 significiant progress 已经做出，但是许多 convolutional neural networks（CNNs）基于 SR 模型专注于重新实现高频率的详细信息，经常忽略重要的低频率构造信息。Transformer-based SR 方法，尽管包含全球结构的细节，却常常具有很多参数，导致高 computational overhead。在这篇文章中，我们解决这些挑战，通过引入 Multi-Depth Branches Network (MDBN) 架构。这个架构将 ResNet 架构扩展， adds an additional branch 来捕捉图像中重要的构造特征。我们的提案的多层分支模组 (MDBM) 包括对应的嵌入尺寸在不同深度的几个分支中堆叠的卷积核。我们通过对特征图进行全面分析，发现不同的深度分支可以分别提取构造和细节信息。通过融合这些分支，总体架构可以在实现高频率视觉元素的Restoration 过程中保留重要的低频率semantic构造信息，与人类视觉认知更加相似。相比 GoogLeNet-like 模型，我们的基本多层分支结构具有较少参数、更高的计算效率和提高的性能。我们的模型在较低的推导时间内，已经超过了现有的SOTA 轻量级 SR 方法。我们的代码可以在 https://github.com/thy960112/MDBN 中找到。”
</details></li>
</ul>
<hr>
<h2 id="Telling-Stories-for-Common-Sense-Zero-Shot-Action-Recognition"><a href="#Telling-Stories-for-Common-Sense-Zero-Shot-Action-Recognition" class="headerlink" title="Telling Stories for Common Sense Zero-Shot Action Recognition"></a>Telling Stories for Common Sense Zero-Shot Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17327">http://arxiv.org/abs/2309.17327</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kini5gowda/stories">https://github.com/kini5gowda/stories</a></li>
<li>paper_authors: Shreyank N Gowda, Laura Sevilla-Lara</li>
<li>for: 这篇论文旨在提高零例学习视频分类的效果，推动零例视频分析领域的进步。</li>
<li>methods: 该论文使用了一个新的数据集，即Stories数据集，该数据集包含了多种动作类型的文本描述，从 WikiHow 文章中提取出来的。每个类型的动作都有多句叙述，描述了必需的步骤、场景、物品和动词，这些文本数据帮助建立了更加复杂的动作关系，为零例转移提供了基础。此外，该论文还提出了一种使用 Stories 数据集进行零例特征生成的方法，不需要任何目标数据集 fine-tuning，可以达到新的领先性水平，提高 top-1 准确率达到 6.1% 之上。</li>
<li>results: 该论文通过使用 Stories 数据集和提出的方法，在多个 benchmark 上达到了新的领先性水平，提高 top-1 准确率达到 6.1% 之上。<details>
<summary>Abstract</summary>
Video understanding has long suffered from reliance on large labeled datasets, motivating research into zero-shot learning. Recent progress in language modeling presents opportunities to advance zero-shot video analysis, but constructing an effective semantic space relating action classes remains challenging. We address this by introducing a novel dataset, Stories, which contains rich textual descriptions for diverse action classes extracted from WikiHow articles. For each class, we extract multi-sentence narratives detailing the necessary steps, scenes, objects, and verbs that characterize the action. This contextual data enables modeling of nuanced relationships between actions, paving the way for zero-shot transfer. We also propose an approach that harnesses Stories to improve feature generation for training zero-shot classification. Without any target dataset fine-tuning, our method achieves new state-of-the-art on multiple benchmarks, improving top-1 accuracy by up to 6.1%. We believe Stories provides a valuable resource that can catalyze progress in zero-shot action recognition. The textual narratives forge connections between seen and unseen classes, overcoming the bottleneck of labeled data that has long impeded advancements in this exciting domain. The data can be found here: https://github.com/kini5gowda/Stories .
</details>
<details>
<summary>摘要</summary>
视频理解已经长期受到大量标注数据的限制，这促使了研究零shot学习。现在的语言模型研究开创了对零shot视频分析的新机会，但构建有效的Semantic空间关系动作类仍然是挑战。我们解决这个问题，通过引入一个新的数据集，叫做Stories，这个数据集包含了多种动作类的丰富文本描述，从WikiHow文章中提取出来的。对于每个类，我们提取了多句叙述，描述了动作的必要步骤、场景、物品和动词，这些文本数据允许我们模型动作之间的细化关系，为零shot传递开辟新的道路。我们还提出了一种使用Stories来提高零shot分类训练的方法。无需任务集微调，我们的方法在多个benchmark上达到了新的州OF-THE-ART，提高了top-1准确率by up to 6.1%。我们认为Stories提供了一个价值的资源，可以促进零shot动作认知领域的进步。这些文本描述将seen和unseen类之间建立连接，超越了标注数据的瓶颈，长期妨碍了这一领域的发展。数据可以在以下地址找到：https://github.com/kini5gowda/Stories。
</details></li>
</ul>
<hr>
<h2 id="Development-of-a-Deep-Learning-Method-to-Identify-Acute-Ischemic-Stroke-Lesions-on-Brain-CT"><a href="#Development-of-a-Deep-Learning-Method-to-Identify-Acute-Ischemic-Stroke-Lesions-on-Brain-CT" class="headerlink" title="Development of a Deep Learning Method to Identify Acute Ischemic Stroke Lesions on Brain CT"></a>Development of a Deep Learning Method to Identify Acute Ischemic Stroke Lesions on Brain CT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17320">http://arxiv.org/abs/2309.17320</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alessandro Fontanella, Wenwen Li, Grant Mair, Antreas Antoniou, Eleanor Platt, Paul Armitage, Emanuele Trucco, Joanna Wardlaw, Amos Storkey</li>
<li>for: 这个研究的目的是开发一种基于深度学习的Computed Tomography（CT）脑神经凝聚检测方法，以便诊断急性血栓性脑梗（AIS）病例。</li>
<li>methods: 这个研究使用了一种基于 convolutional neural network（CNN）的深度学习算法，使用了来自第三次国际中风试验（IST-3）的 routinely-collected CT脑神经扫描数据，这些数据没有遵循严格的研究协议。研究者还explored了AIS损伤特征、背景脑神经的影响以及时间因素对深度学习性能的影响。</li>
<li>results: 研究发现，使用这种深度学习算法可以准确地检测AIS损伤，并且可以分类脑神经损伤的侧。研究中的最佳方法达到了72%的准确率，并且发现大型损伤（80%的准确率）、多个损伤（87%的准确率）以及follow-up扫描（76%的准确率）的检测率较高。然而，chronic brain conditions会降低深度学习的准确率，特别是非 stroke lesions和老 stroke lesions（32%和31%的错误率）。<details>
<summary>Abstract</summary>
Computed Tomography (CT) is commonly used to image acute ischemic stroke (AIS) patients, but its interpretation by radiologists is time-consuming and subject to inter-observer variability. Deep learning (DL) techniques can provide automated CT brain scan assessment, but usually require annotated images. Aiming to develop a DL method for AIS using labelled but not annotated CT brain scans from patients with AIS, we designed a convolutional neural network-based DL algorithm using routinely-collected CT brain scans from the Third International Stroke Trial (IST-3), which were not acquired using strict research protocols. The DL model aimed to detect AIS lesions and classify the side of the brain affected. We explored the impact of AIS lesion features, background brain appearances, and timing on DL performance. From 5772 unique CT scans of 2347 AIS patients (median age 82), 54% had visible AIS lesions according to expert labelling. Our best-performing DL method achieved 72% accuracy for lesion presence and side. Lesions that were larger (80% accuracy) or multiple (87% accuracy for two lesions, 100% for three or more), were better detected. Follow-up scans had 76% accuracy, while baseline scans 67% accuracy. Chronic brain conditions reduced accuracy, particularly non-stroke lesions and old stroke lesions (32% and 31% error rates respectively). DL methods can be designed for AIS lesion detection on CT using the vast quantities of routinely-collected CT brain scan data. Ultimately, this should lead to more robust and widely-applicable methods.
</details>
<details>
<summary>摘要</summary>
计算Tomography（CT）通常用于图像新生疱疹（AIS）患者，但是它的解释由放射学家是时间消耗和对观察者的不一致的。深度学习（DL）技术可以提供自动化CT脑部扫描评估，但通常需要标注图像。为了开发一种DL方法用于AIS，我们设计了基于 convolutional neural network（CNN）的DL算法，使用了不同研究协议下收集的CT脑部扫描图像。DL模型的目标是检测AIS损害和脑部哪个区域受到影响。我们研究了AIS损害特征、背景脑部 appearances和时间对DL性能的影响。从5772个Unique CT扫描图像中（2347名AIS患者， median age 82），54%有可见的AIS损害。我们最佳的DL方法达到了72%的损害存在和脑部哪个区域受到影响的准确率。大型损害（80%的准确率）、多个损害（87%的准确率）和追踪扫描（76%的准确率）都有更好的检测性能。与此同时，慢性脑部疾病（如非stroke损害和老stroke损害）会降低DL性能，特别是错误率为32%和31%。DL方法可以用于AIS损害检测CT脑部扫描图像，从而提高方法的可靠性和普遍性。最终，这将导致更加稳定和可靠的方法。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Large-Scale-Medical-Image-Dataset-Preparation-for-Machine-Learning-Applications"><a href="#Efficient-Large-Scale-Medical-Image-Dataset-Preparation-for-Machine-Learning-Applications" class="headerlink" title="Efficient Large Scale Medical Image Dataset Preparation for Machine Learning Applications"></a>Efficient Large Scale Medical Image Dataset Preparation for Machine Learning Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17285">http://arxiv.org/abs/2309.17285</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Denner, Jonas Scherer, Klaus Kades, Dimitrios Bounias, Philipp Schader, Lisa Kausch, Markus Bujotzek, Andreas Michael Bucher, Tobias Penzkofer, Klaus Maier-Hein</li>
<li>for: 该论文旨在提高医疗影像识别精度，通过机器学习算法提高诊断精度。</li>
<li>methods: 该论文提出了一种新的数据筛选工具，用于处理大规模医疗影像数据。该工具包括高级搜索、自动注释和高效标注功能，以便改进数据筛选。</li>
<li>results: 该论文通过使用该数据筛选工具，可以提高医疗影像数据的质量和可靠性，并且可以探索数据中的潜在偏见。此外，该工具还可以帮助研究人员 validate 图像和分割质量，以及检测数据中的偏见。<details>
<summary>Abstract</summary>
In the rapidly evolving field of medical imaging, machine learning algorithms have become indispensable for enhancing diagnostic accuracy. However, the effectiveness of these algorithms is contingent upon the availability and organization of high-quality medical imaging datasets. Traditional Digital Imaging and Communications in Medicine (DICOM) data management systems are inadequate for handling the scale and complexity of data required to be facilitated in machine learning algorithms. This paper introduces an innovative data curation tool, developed as part of the Kaapana open-source toolkit, aimed at streamlining the organization, management, and processing of large-scale medical imaging datasets. The tool is specifically tailored to meet the needs of radiologists and machine learning researchers. It incorporates advanced search, auto-annotation and efficient tagging functionalities for improved data curation. Additionally, the tool facilitates quality control and review, enabling researchers to validate image and segmentation quality in large datasets. It also plays a critical role in uncovering potential biases in datasets by aggregating and visualizing metadata, which is essential for developing robust machine learning models. Furthermore, Kaapana is integrated within the Radiological Cooperative Network (RACOON), a pioneering initiative aimed at creating a comprehensive national infrastructure for the aggregation, transmission, and consolidation of radiological data across all university clinics throughout Germany. A supplementary video showcasing the tool's functionalities can be accessed at https://bit.ly/MICCAI-DEMI2023.
</details>
<details>
<summary>摘要</summary>
在医学成像领域的快速发展中，机器学习算法已成为提高诊断精度的不可或缺的工具。然而，这些算法的效iveness取决于医学成像数据的可用性和组织化。传统的Digital Imaging and Communications in Medicine（DICOM）数据管理系统不能满足大规模医学成像数据的需求。这篇论文介绍了一种创新的数据 curación工具，作为Kaapana开源工具包的一部分，用于改善医学成像数据的组织、管理和处理。该工具特地针对医生和机器学习研究人员的需求，并包括高级搜索、自动标注和高效标记功能。此外，工具还实现了质量控制和审核功能，帮助研究人员 validate 图像和分割质量，并推断数据中可能存在的偏见。此外，Kaapana与德国大学医院 radiological Cooperative Network（RACOON）集成，该项目旨在创建一个涵盖所有大学医院的 радиологиical 数据集成、传输和 консоли达到的全国基础设施。补充视频展示工具的功能可以在https://bit.ly/MICCAI-DEMI2023中获取。
</details></li>
</ul>
<hr>
<h2 id="Information-Flow-in-Self-Supervised-Learning"><a href="#Information-Flow-in-Self-Supervised-Learning" class="headerlink" title="Information Flow in Self-Supervised Learning"></a>Information Flow in Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17281">http://arxiv.org/abs/2309.17281</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yifanzhang-pro/m-mae">https://github.com/yifanzhang-pro/m-mae</a></li>
<li>paper_authors: Zhiquan Tan, Jingqin Yang, Weiran Huang, Yang Yuan, Yifan Zhang</li>
<li>for: 本研究提供了一个完整的自助学习（SSL）工具箱，通过矩阵信息理论来理解和提高SSL方法。</li>
<li>methods: 本研究利用矩阵相互信息和共同 entropy的原则，对异构和特征减 corr 方法进行统一分析。此外，我们提议基于矩阵信息理论的矩阵变量掩码自动编码器（M-MAE）方法，用于增强图像模型。</li>
<li>results: 实验表明，与当前方法相比，M-MAE方法具有较高的效果，包括在 ImageNet 上Linear probing ViT-Base 上提高3.9%，和 fine-tuning ViT-Large 上提高1%。<details>
<summary>Abstract</summary>
In this paper, we provide a comprehensive toolbox for understanding and enhancing self-supervised learning (SSL) methods through the lens of matrix information theory. Specifically, by leveraging the principles of matrix mutual information and joint entropy, we offer a unified analysis for both contrastive and feature decorrelation based methods. Furthermore, we propose the matrix variational masked auto-encoder (M-MAE) method, grounded in matrix information theory, as an enhancement to masked image modeling. The empirical evaluations underscore the effectiveness of M-MAE compared with the state-of-the-art methods, including a 3.9% improvement in linear probing ViT-Base, and a 1% improvement in fine-tuning ViT-Large, both on ImageNet.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提供了一个完整的工具箱，用于理解和提高自动编目学习（SSL）方法的表示。具体来说，我们通过利用矩阵共同信息理论和矩阵共同熵的原则，为对比和特征抑制基于方法提供了一个统一的分析。此外，我们还提出了基于矩阵信息理论的矩阵变量隐藏自动编码器（M-MAE）方法，用于提高图像模型。实验证明，M-MAE方法比现有的方法更有效，包括对ViT-Base和ViT-Large图像集进行线性搜索中的3.9%提高，以及对ImageNet图像集进行练习中的1%提高。
</details></li>
</ul>
<hr>
<h2 id="Unpaired-Optical-Coherence-Tomography-Angiography-Image-Super-Resolution-via-Frequency-Aware-Inverse-Consistency-GAN"><a href="#Unpaired-Optical-Coherence-Tomography-Angiography-Image-Super-Resolution-via-Frequency-Aware-Inverse-Consistency-GAN" class="headerlink" title="Unpaired Optical Coherence Tomography Angiography Image Super-Resolution via Frequency-Aware Inverse-Consistency GAN"></a>Unpaired Optical Coherence Tomography Angiography Image Super-Resolution via Frequency-Aware Inverse-Consistency GAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17269">http://arxiv.org/abs/2309.17269</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiwen Zhang, Dawei Yang, Haoxuan Che, An Ran Ran, Carol Y. Cheung, Hao Chen</li>
<li>for: This paper aims to improve the resolution of optical coherence tomography angiography (OCTA) images without paired training data.</li>
<li>methods: The proposed method uses a generator and discriminator based on Generative Adversarial Networks (GANs), with a dual-path generator to emphasize high-frequency fine capillaries and a frequency-aware adversarial loss for the discriminator.</li>
<li>results: The proposed method outperforms other state-of-the-art unpaired methods both quantitatively and visually, with improved preservation of fine capillary details.Here’s the simplified Chinese text:</li>
<li>for: 这个 paper 目的是提高无对应数据的 optical coherence tomography angiography (OCTA) 图像的分辨率。</li>
<li>methods: 提议的方法使用基于 Generative Adversarial Networks (GANs) 的生成器和批准器，并使用双轨生成器来强调高频环境细血管。</li>
<li>results: 提议的方法在对比其他状态体际的无对应方法时，都有较好的表现，并且可以更好地保留细血管的细节。<details>
<summary>Abstract</summary>
For optical coherence tomography angiography (OCTA) images, a limited scanning rate leads to a trade-off between field-of-view (FOV) and imaging resolution. Although larger FOV images may reveal more parafoveal vascular lesions, their application is greatly hampered due to lower resolution. To increase the resolution, previous works only achieved satisfactory performance by using paired data for training, but real-world applications are limited by the challenge of collecting large-scale paired images. Thus, an unpaired approach is highly demanded. Generative Adversarial Network (GAN) has been commonly used in the unpaired setting, but it may struggle to accurately preserve fine-grained capillary details, which are critical biomarkers for OCTA. In this paper, our approach aspires to preserve these details by leveraging the frequency information, which represents details as high-frequencies ($\textbf{hf}$) and coarse-grained backgrounds as low-frequencies ($\textbf{lf}$). In general, we propose a GAN-based unpaired super-resolution method for OCTA images and exceptionally emphasize $\textbf{hf}$ fine capillaries through a dual-path generator. To facilitate a precise spectrum of the reconstructed image, we also propose a frequency-aware adversarial loss for the discriminator and introduce a frequency-aware focal consistency loss for end-to-end optimization. Experiments show that our method outperforms other state-of-the-art unpaired methods both quantitatively and visually.
</details>
<details>
<summary>摘要</summary>
For optical coherence tomography angiography (OCTA) images, a limited scanning rate leads to a trade-off between field-of-view (FOV) and imaging resolution. Although larger FOV images may reveal more parafoveal vascular lesions, their application is greatly hampered due to lower resolution. To increase the resolution, previous works only achieved satisfactory performance by using paired data for training, but real-world applications are limited by the challenge of collecting large-scale paired images. Thus, an unpaired approach is highly demanded. Generative Adversarial Network (GAN) has been commonly used in the unpaired setting, but it may struggle to accurately preserve fine-grained capillary details, which are critical biomarkers for OCTA. In this paper, our approach aspires to preserve these details by leveraging the frequency information, which represents details as high-frequencies ($\textbf{hf}$) and coarse-grained backgrounds as low-frequencies ($\textbf{lf}$). In general, we propose a GAN-based unpaired super-resolution method for OCTA images and exceptionally emphasize $\textbf{hf}$ fine capillaries through a dual-path generator. To facilitate a precise spectrum of the reconstructed image, we also propose a frequency-aware adversarial loss for the discriminator and introduce a frequency-aware focal consistency loss for end-to-end optimization. Experiments show that our method outperforms other state-of-the-art unpaired methods both quantitatively and visually.Here's the translation in Traditional Chinese:For optical coherence tomography angiography (OCTA) images, a limited scanning rate leads to a trade-off between field-of-view (FOV) and imaging resolution. Although larger FOV images may reveal more parafoveal vascular lesions, their application is greatly hampered due to lower resolution. To increase the resolution, previous works only achieved satisfactory performance by using paired data for training, but real-world applications are limited by the challenge of collecting large-scale paired images. Thus, an unpaired approach is highly demanded. Generative Adversarial Network (GAN) has been commonly used in the unpaired setting, but it may struggle to accurately preserve fine-grained capillary details, which are critical biomarkers for OCTA. In this paper, our approach aspires to preserve these details by leveraging the frequency information, which represents details as high-frequencies ($\textbf{hf}$) and coarse-grained backgrounds as low-frequencies ($\textbf{lf}$). In general, we propose a GAN-based unpaired super-resolution method for OCTA images and exceptionally emphasize $\textbf{hf}$ fine capillaries through a dual-path generator. To facilitate a precise spectrum of the reconstructed image, we also propose a frequency-aware adversarial loss for the discriminator and introduce a frequency-aware focal consistency loss for end-to-end optimization. Experiments show that our method outperforms other state-of-the-art unpaired methods both quantitatively and visually.
</details></li>
</ul>
<hr>
<h2 id="Effect-of-structure-based-training-on-3D-localization-precision-and-quality"><a href="#Effect-of-structure-based-training-on-3D-localization-precision-and-quality" class="headerlink" title="Effect of structure-based training on 3D localization precision and quality"></a>Effect of structure-based training on 3D localization precision and quality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17265">http://arxiv.org/abs/2309.17265</a></li>
<li>repo_url: None</li>
<li>paper_authors: Armin Abdehkakha, Craig Snoeyink</li>
<li>for: 这个研究旨在提出基于结构的训练方法，用于在单分子位置显微镜（SMLM）和3D物体重建中使用深度学习算法。</li>
<li>methods: 该方法比传统的随机训练方法更有优势，使用LUENN包作为我们的人工智能管道。</li>
<li>results: 对比训练方法，结构基于训练方法得到了显著提高的检测率和地址精度，特别是在不同的信号噪比（SNR）下。此外，该方法还能有效地消除棱镜质量问题，以确保更准确的3D重建。<details>
<summary>Abstract</summary>
This study introduces a structural-based training approach for CNN-based algorithms in single-molecule localization microscopy (SMLM) and 3D object reconstruction. We compare this approach with the traditional random-based training method, utilizing the LUENN package as our AI pipeline. The quantitative evaluation demonstrates significant improvements in detection rate and localization precision with the structural-based training approach, particularly in varying signal-to-noise ratios (SNRs). Moreover, the method effectively removes checkerboard artifacts, ensuring more accurate 3D reconstructions. Our findings highlight the potential of the structural-based training approach to advance super-resolution microscopy and deepen our understanding of complex biological systems at the nanoscale.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "single-molecule localization microscopy" (SMLM) is translated as "单分子定位微scopie" (SMLM)* "3D object reconstruction" is translated as "3D物体重建"* "deep learning algorithms" is translated as "深度学习算法"* "structural-based training approach" is translated as "结构基于的训练方法"* "random-based training method" is translated as "随机基于的训练方法"* "LUENN package" is translated as "LUENN包"* "quantitative evaluation" is translated as "量化评估"* "detection rate" is translated as "检测率"* "localization precision" is translated as "定位精度"* "checkerboard artifacts" is translated as "棋盘艺术ifacts"
</details></li>
</ul>
<hr>
<h2 id="Consistent123-One-Image-to-Highly-Consistent-3D-Asset-Using-Case-Aware-Diffusion-Priors"><a href="#Consistent123-One-Image-to-Highly-Consistent-3D-Asset-Using-Case-Aware-Diffusion-Priors" class="headerlink" title="Consistent123: One Image to Highly Consistent 3D Asset Using Case-Aware Diffusion Priors"></a>Consistent123: One Image to Highly Consistent 3D Asset Using Case-Aware Diffusion Priors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17261">http://arxiv.org/abs/2309.17261</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yukang Lin, Haonan Han, Chaoqun Gong, Zunnan Xu, Yachao Zhang, Xiu Li</li>
<li>for: 高精度的3D对象重建从单个图像 guidance by pre-trained diffusion models</li>
<li>methods: 提出了一种case-aware two-stage方法，利用2D和3D扩散先验来实现高度一致的3D资产重建</li>
<li>results: 对多种物体进行了详细的测试，并表明了our方法可以具有高度一致的3D重建和强大的泛化能力<details>
<summary>Abstract</summary>
Reconstructing 3D objects from a single image guided by pretrained diffusion models has demonstrated promising outcomes. However, due to utilizing the case-agnostic rigid strategy, their generalization ability to arbitrary cases and the 3D consistency of reconstruction are still poor. In this work, we propose Consistent123, a case-aware two-stage method for highly consistent 3D asset reconstruction from one image with both 2D and 3D diffusion priors. In the first stage, Consistent123 utilizes only 3D structural priors for sufficient geometry exploitation, with a CLIP-based case-aware adaptive detection mechanism embedded within this process. In the second stage, 2D texture priors are introduced and progressively take on a dominant guiding role, delicately sculpting the details of the 3D model. Consistent123 aligns more closely with the evolving trends in guidance requirements, adaptively providing adequate 3D geometric initialization and suitable 2D texture refinement for different objects. Consistent123 can obtain highly 3D-consistent reconstruction and exhibits strong generalization ability across various objects. Qualitative and quantitative experiments show that our method significantly outperforms state-of-the-art image-to-3D methods. See https://Consistent123.github.io for a more comprehensive exploration of our generated 3D assets.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>根据预训练的扩散模型从单张图像中重建3D对象的方法，已经展现出了有前途的结果。然而，由于使用 случаagnostic的静态策略，其对于任意情况和3D重建的一致性仍然不够。在这项工作中，我们提出了Consistent123，一种情况意识的两stage方法，用于从单张图像中高度一致的3D资产重建。在第一阶段，Consistent123仅利用3D结构约束，通过CLIP基于的情况意识适应检测机制来进行充分的geometry利用。在第二阶段，2D文本约束被引入，逐渐取代导向role，细腻地雕琢3D模型的细节。Consistent123与导航需求的演化趋势更加相似，适应不同对象的3D初始化和2D文本细化。Consistent123可以获得高度一致的3D重建，并且在不同对象上展现出强大的泛化能力。Qualitative和量化实验表明，我们的方法在图像到3D重建领域内Significantly outperforms state-of-the-art方法。请参考https://Consistent123.github.io进行更多的3D资产的探索。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Deep-Learning-Techniques-for-Action-Anticipation"><a href="#A-Survey-on-Deep-Learning-Techniques-for-Action-Anticipation" class="headerlink" title="A Survey on Deep Learning Techniques for Action Anticipation"></a>A Survey on Deep Learning Techniques for Action Anticipation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17257">http://arxiv.org/abs/2309.17257</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyun Zhong, Manuel Martin, Michael Voit, Juergen Gall, Jürgen Beyerer</li>
<li>for: 本研究的目的是实时预测人员的动作，以应对日常生活中的各种情况。</li>
<li>methods: 本文专注于使用深度学习方法进行动作预测，并按照它们的主要贡献分为多个字段，供读者快速了解。</li>
<li>results: 本研究总结了各种动作预测算法的最新进展，并评估了不同评估指标和数据集的影响。未来的发展方向亦有系统的讨论。<details>
<summary>Abstract</summary>
The ability to anticipate possible future human actions is essential for a wide range of applications, including autonomous driving and human-robot interaction. Consequently, numerous methods have been introduced for action anticipation in recent years, with deep learning-based approaches being particularly popular. In this work, we review the recent advances of action anticipation algorithms with a particular focus on daily-living scenarios. Additionally, we classify these methods according to their primary contributions and summarize them in tabular form, allowing readers to grasp the details at a glance. Furthermore, we delve into the common evaluation metrics and datasets used for action anticipation and provide future directions with systematical discussions.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。人类未来行为预测能力在各种应用领域都是非常重要，如自动驾驶和人机交互。因此，过去几年内，有很多方法被提出用于行动预测，其中深度学习基本方法尤为流行。在这项工作中，我们对当前的行动预测算法进行了latest advances的综述，并将其分为不同类别，以便读者一眼了解。此外，我们还详细介绍了通用的评价指标和数据集，以及未来的发展方向。>>Here's the translation in Traditional Chinese:<<SYS>>转换文本到简化中文。人类未来行为预测能力在各种应用领域都是非常重要，如自动驾驶和人机交互。因此，过去几年内，有很多方法被提出用于行动预测，其中深度学习基本方法尤为流行。在这个工作中，我们对当前的行动预测算法进行了latest advances的综述，并将其分为不同类别，以便读者一眼了解。此外，我们还详细介绍了通用的评价指标和数据集，以及未来的发展方向。
</details></li>
</ul>
<hr>
<h2 id="EGVD-Event-Guided-Video-Deraining"><a href="#EGVD-Event-Guided-Video-Deraining" class="headerlink" title="EGVD: Event-Guided Video Deraining"></a>EGVD: Event-Guided Video Deraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17239">http://arxiv.org/abs/2309.17239</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/booker-max/egvd">https://github.com/booker-max/egvd</a></li>
<li>paper_authors: Yueyi Zhang, Jin Wang, Wenming Weng, Xiaoyan Sun, Zhiwei Xiong</li>
<li>for: 这篇论文目的是为了解决 Complex Spatio-Temporal Distribution 的雨层推广 Video Deraining 问题。</li>
<li>methods: 本文使用 Event Camera 和 End-to-End Learning-based Network 来解决这个问题。 Specifically, the authors propose an event-aware motion detection module and a pyramidal adaptive selection module to reliably separate the background and rain layers.</li>
<li>results:  compared with existing state-of-the-art methods, the proposed method demonstrates clear superiority on synthetic and self-collected real-world datasets. The code and dataset are available at \url{<a target="_blank" rel="noopener" href="https://github.com/booker-max/EGVD%7D">https://github.com/booker-max/EGVD}</a>.<details>
<summary>Abstract</summary>
With the rapid development of deep learning, video deraining has experienced significant progress. However, existing video deraining pipelines cannot achieve satisfying performance for scenes with rain layers of complex spatio-temporal distribution. In this paper, we approach video deraining by employing an event camera. As a neuromorphic sensor, the event camera suits scenes of non-uniform motion and dynamic light conditions. We propose an end-to-end learning-based network to unlock the potential of the event camera for video deraining. First, we devise an event-aware motion detection module to adaptively aggregate multi-frame motion contexts using event-aware masks. Second, we design a pyramidal adaptive selection module for reliably separating the background and rain layers by incorporating multi-modal contextualized priors. In addition, we build a real-world dataset consisting of rainy videos and temporally synchronized event streams. We compare our method with extensive state-of-the-art methods on synthetic and self-collected real-world datasets, demonstrating the clear superiority of our method. The code and dataset are available at \url{https://github.com/booker-max/EGVD}.
</details>
<details>
<summary>摘要</summary>
随着深度学习的快速发展，视频抖雨得到了显著进步。然而，现有的视频抖雨管道无法实现复杂的雨层分布场景中满意的性能。在这篇论文中，我们使用事件摄像头来解决这个问题。作为一种神经omorphic感知器，事件摄像头适合非均匀的运动场景和动态的照明条件。我们提出了一种基于学习的终端网络，用于解锁事件摄像头的潜在能力。首先，我们设计了一个事件感知模块，用于自适应聚合多帧运动上下文。其次，我们设计了一个层次自适应选择模块，用于可靠地分离背景和雨层。此外，我们构建了一个包含雨天视频和时间同步事件流的实际数据集。我们对与现有的状态艺术方法进行了广泛的比较，并证明了我们的方法的明显优越性。代码和数据集可以在 GitHub 上获取：<https://github.com/booker-max/EGVD>。
</details></li>
</ul>
<hr>
<h2 id="Glioma-subtype-classification-from-histopathological-images-using-in-domain-and-out-of-domain-transfer-learning-An-experimental-study"><a href="#Glioma-subtype-classification-from-histopathological-images-using-in-domain-and-out-of-domain-transfer-learning-An-experimental-study" class="headerlink" title="Glioma subtype classification from histopathological images using in-domain and out-of-domain transfer learning: An experimental study"></a>Glioma subtype classification from histopathological images using in-domain and out-of-domain transfer learning: An experimental study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17223">http://arxiv.org/abs/2309.17223</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vladimir Despotovic, Sang-Yoon Kim, Ann-Christin Hau, Aliaksandra Kakoichankava, Gilbert Georg Klamminger, Felix Bruno Kleine Borgmann, Katrin B. M. Frauenknecht, Michel Mittelbronn, Petr V. Nazarov</li>
<li>for: 这个论文主要目的是对Computer-aided classification of adult-type diffuse gliomas进行了全面的比较和深度学习架构的研究。</li>
<li>methods: 这篇论文使用了多种转移学习策略和深度学习架构，包括自然语言处理ImageNet表示的外部领域适应，以及自我supervised和多任务学习方法。</li>
<li>results: 这篇论文的研究结果表明，使用这些方法可以提高医疗影像分类的性能，并且可以减少病理学家的标注工作。此外，这篇论文还提供了一个可视化工具，可以在整个扫描图像水平上生成热图，以便为病理学家提供有用的信息。<details>
<summary>Abstract</summary>
We provide in this paper a comprehensive comparison of various transfer learning strategies and deep learning architectures for computer-aided classification of adult-type diffuse gliomas. We evaluate the generalizability of out-of-domain ImageNet representations for a target domain of histopathological images, and study the impact of in-domain adaptation using self-supervised and multi-task learning approaches for pretraining the models using the medium-to-large scale datasets of histopathological images. A semi-supervised learning approach is furthermore proposed, where the fine-tuned models are utilized to predict the labels of unannotated regions of the whole slide images (WSI). The models are subsequently retrained using the ground-truth labels and weak labels determined in the previous step, providing superior performance in comparison to standard in-domain transfer learning with balanced accuracy of 96.91% and F1-score 97.07%, and minimizing the pathologist's efforts for annotation. Finally, we provide a visualization tool working at WSI level which generates heatmaps that highlight tumor areas; thus, providing insights to pathologists concerning the most informative parts of the WSI.
</details>
<details>
<summary>摘要</summary>
我们在这篇论文中对各种传输学习策略和深度学习架构进行了广泛的比较，用于计算机辅助Diffuse gliomas的成人类型分类。我们评估了ImageNet表示的 OUT-OF-DOMAIN 表示的泛化性，并研究了采用自我超vised和多任务学习方法进行预训练，以提高模型在历史病理图像数据集上的性能。此外，我们还提出了一种半supervised学习方法，其中使用精度调整的模型来预测整个扫描图像（WSI）中的标签。模型然后被重新训练使用真实标签和弱标签，从而提高性能，并最小化病理医生对标注的努力。最后，我们还提供了一个可视化工具，它可以在WSI级别上生成热图，并高亮显示肿瘤区域，从而为病理医生提供有用的信息。
</details></li>
</ul>
<hr>
<h2 id="When-Epipolar-Constraint-Meets-Non-local-Operators-in-Multi-View-Stereo"><a href="#When-Epipolar-Constraint-Meets-Non-local-Operators-in-Multi-View-Stereo" class="headerlink" title="When Epipolar Constraint Meets Non-local Operators in Multi-View Stereo"></a>When Epipolar Constraint Meets Non-local Operators in Multi-View Stereo</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17218">http://arxiv.org/abs/2309.17218</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tqtqliu/et-mvsnet">https://github.com/tqtqliu/et-mvsnet</a></li>
<li>paper_authors: Tianqi Liu, Xinyi Ye, Weiyue Zhao, Zhiyu Pan, Min Shi, Zhiguo Cao<br>for:This paper proposes a new method for multi-view stereo (MVS) reconstruction, which is designed to improve the efficiency and accuracy of the feature matching process.methods:The proposed method, called ET-MVSNet, uses a novel non-local feature augmentation strategy based on the epipolar geometry. This strategy reduces the 2D search space into the epipolar line in stereo matching, making it more efficient and accurate.results:ET-MVSNet achieves state-of-the-art reconstruction performance on both the DTU and Tanks-and-Temples benchmarks with high efficiency. The proposed method is able to improve the accuracy and speed of MVS reconstruction, making it a valuable contribution to the field.<details>
<summary>Abstract</summary>
Learning-based multi-view stereo (MVS) method heavily relies on feature matching, which requires distinctive and descriptive representations. An effective solution is to apply non-local feature aggregation, e.g., Transformer. Albeit useful, these techniques introduce heavy computation overheads for MVS. Each pixel densely attends to the whole image. In contrast, we propose to constrain non-local feature augmentation within a pair of lines: each point only attends the corresponding pair of epipolar lines. Our idea takes inspiration from the classic epipolar geometry, which shows that one point with different depth hypotheses will be projected to the epipolar line on the other view. This constraint reduces the 2D search space into the epipolar line in stereo matching. Similarly, this suggests that the matching of MVS is to distinguish a series of points lying on the same line. Inspired by this point-to-line search, we devise a line-to-point non-local augmentation strategy. We first devise an optimized searching algorithm to split the 2D feature maps into epipolar line pairs. Then, an Epipolar Transformer (ET) performs non-local feature augmentation among epipolar line pairs. We incorporate the ET into a learning-based MVS baseline, named ET-MVSNet. ET-MVSNet achieves state-of-the-art reconstruction performance on both the DTU and Tanks-and-Temples benchmark with high efficiency. Code is available at https://github.com/TQTQliu/ET-MVSNet.
</details>
<details>
<summary>摘要</summary>
学习基于多视图雷达（MVS）方法仰赖特征匹配，需要突出特征描述。一个有效的解决方案是应用非本地特征聚合，例如Transformer。虽然有用，这些技术增加了MVS中计算负担。每个像素都密集关注整个图像。相比之下，我们提议在对应对线对线上进行非本地特征聚合约束。我们的想法来源于经典的 epipolar  геометрии，它表明一个点在不同深度假设下将被投影到另一个视图中的 epipolar 线上。这种约束将2D 搜索空间缩小到 epipolar 线上。类似地，这表明 MVS 的匹配是将一系列点分类为同一条线上。 inspired by this point-to-line 搜索，我们开发了一种线到点非本地增强策略。我们首先开发了一种优化的搜索算法，将2D 特征图分成 epipolar 线对。然后，我们使用 Epipolar Transformer（ET）进行非本地特征增强。我们将 ET  incorporated 到一个学习基于 MVS 的基础模型中，称为 ET-MVSNet。 ET-MVSNet 在 DTU 和 Tanks-and-Temples 测试 benchmark 上 achieve 状态的重建性能，同时具有高效性。代码可以在 https://github.com/TQTQliu/ET-MVSNet 中找到。
</details></li>
</ul>
<hr>
<h2 id="Instant-Complexity-Reduction-in-CNNs-using-Locality-Sensitive-Hashing"><a href="#Instant-Complexity-Reduction-in-CNNs-using-Locality-Sensitive-Hashing" class="headerlink" title="Instant Complexity Reduction in CNNs using Locality-Sensitive Hashing"></a>Instant Complexity Reduction in CNNs using Locality-Sensitive Hashing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17211">http://arxiv.org/abs/2309.17211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Meiner, Jens Mehnert, Alexandru Paul Condurache</li>
<li>for: 实时对资源有限的设备进行检查和识别，以减少对于调用运算的需求。</li>
<li>methods: 使用构造化排除法，将网络中的浮点运算减少到最小化，而不需要特定的训练或调整程序。</li>
<li>results: 在CIFAR-10和ImageNet等受欢迎的视觉标准库上，实现了对网络的减少，并且仅对网络的调用时间进行快速对应。 Specifically, 在CIFAR-10上，将ResNet34中的条件减少46.72%，仅对网络的精度下降1.25%。<details>
<summary>Abstract</summary>
To reduce the computational cost of convolutional neural networks (CNNs) for usage on resource-constrained devices, structured pruning approaches have shown promising results, drastically reducing floating-point operations (FLOPs) without substantial drops in accuracy. However, most recent methods require fine-tuning or specific training procedures to achieve a reasonable trade-off between retained accuracy and reduction in FLOPs. This introduces additional cost in the form of computational overhead and requires training data to be available. To this end, we propose HASTE (Hashing for Tractable Efficiency), a parameter-free and data-free module that acts as a plug-and-play replacement for any regular convolution module. It instantly reduces the network's test-time inference cost without requiring any training or fine-tuning. We are able to drastically compress latent feature maps without sacrificing much accuracy by using locality-sensitive hashing (LSH) to detect redundancies in the channel dimension. Similar channels are aggregated to reduce the input and filter depth simultaneously, allowing for cheaper convolutions. We demonstrate our approach on the popular vision benchmarks CIFAR-10 and ImageNet. In particular, we are able to instantly drop 46.72% of FLOPs while only losing 1.25% accuracy by just swapping the convolution modules in a ResNet34 on CIFAR-10 for our HASTE module.
</details>
<details>
<summary>摘要</summary>
为了减少深度学习网络（CNN）的计算成本，使其在资源有限的设备上运行，结构化剪辑方法已经显示出了可观的成果，可以减少浮点操作数（FLOPs）而无需减少准确率。然而，大多数最新的方法需要精细调整或特定的训练程序来达到一个合适的平衡点，这会增加计算开销和需要训练数据。为此，我们提出了快速压缩（HASTE）模块，它可以作为任何常见卷积模块的替换部件。它可以立即降低网络的测试时间执行成本，不需要任何训练或精细调整。我们利用了本地性敏感哈希（LSH）检测通道维度中的重复性，从而压缩缓存特征图而无需减少准确率。相似的通道被聚合，以降低输入和滤波器的深度，使得更加便宜的卷积操作。我们在CIFAR-10和ImageNet等流行的视觉benchmark上展示了我们的方法。特别是，我们可以在ResNet34中INSTANTLY降低46.72%的FLOPs，只失去1.25%的准确率，只需替换卷积模块为我们的快速压缩模块。
</details></li>
</ul>
<hr>
<h2 id="Robots-That-Can-See-Leveraging-Human-Pose-for-Trajectory-Prediction"><a href="#Robots-That-Can-See-Leveraging-Human-Pose-for-Trajectory-Prediction" class="headerlink" title="Robots That Can See: Leveraging Human Pose for Trajectory Prediction"></a>Robots That Can See: Leveraging Human Pose for Trajectory Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17209">http://arxiv.org/abs/2309.17209</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research/human-scene-transformer">https://github.com/google-research/human-scene-transformer</a></li>
<li>paper_authors: Tim Salzmann, Lewis Chiang, Markus Ryll, Dorsa Sadigh, Carolina Parada, Alex Bewley</li>
<li>for: 预测人类在动态环境中的运动轨迹，以便安全有效地让Robot进行导航。</li>
<li>methods: 使用Transformer架构，从人类位置、头orientation和3D关节点数据中提取输入特征，预测人类未来轨迹。</li>
<li>results: 实现了预测人类轨迹的未来状态的目标性能，并在常见预测 bencmarks 和一个人类跟踪数据集中达到了状态前的表现。此外，还发现了有限历史数据的新代理人为预测错误的主要贡献者，并证明了3D关节点位置在这些挑战性enario中减少预测错误的作用。<details>
<summary>Abstract</summary>
Anticipating the motion of all humans in dynamic environments such as homes and offices is critical to enable safe and effective robot navigation. Such spaces remain challenging as humans do not follow strict rules of motion and there are often multiple occluded entry points such as corners and doors that create opportunities for sudden encounters. In this work, we present a Transformer based architecture to predict human future trajectories in human-centric environments from input features including human positions, head orientations, and 3D skeletal keypoints from onboard in-the-wild sensory information. The resulting model captures the inherent uncertainty for future human trajectory prediction and achieves state-of-the-art performance on common prediction benchmarks and a human tracking dataset captured from a mobile robot adapted for the prediction task. Furthermore, we identify new agents with limited historical data as a major contributor to error and demonstrate the complementary nature of 3D skeletal poses in reducing prediction error in such challenging scenarios.
</details>
<details>
<summary>摘要</summary>
anticipating the motion of all humans in dynamic environments such as homes and offices is critical to enable safe and effective robot navigation. such spaces remain challenging as humans do not follow strict rules of motion and there are often multiple occluded entry points such as corners and doors that create opportunities for sudden encounters. in this work, we present a transformer-based architecture to predict human future trajectories in human-centric environments from input features including human positions, head orientations, and 3d skeletal keypoints from onboard in-the-wild sensory information. the resulting model captures the inherent uncertainty for future human trajectory prediction and achieves state-of-the-art performance on common prediction benchmarks and a human tracking dataset captured from a mobile robot adapted for the prediction task. furthermore, we identify new agents with limited historical data as a major contributor to error and demonstrate the complementary nature of 3d skeletal poses in reducing prediction error in such challenging scenarios.
</details></li>
</ul>
<hr>
<h2 id="Towards-Complex-query-Referring-Image-Segmentation-A-Novel-Benchmark"><a href="#Towards-Complex-query-Referring-Image-Segmentation-A-Novel-Benchmark" class="headerlink" title="Towards Complex-query Referring Image Segmentation: A Novel Benchmark"></a>Towards Complex-query Referring Image Segmentation: A Novel Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17205">http://arxiv.org/abs/2309.17205</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Ji, Li Li, Hao Fei, Xiangyan Liu, Xun Yang, Juncheng Li, Roger Zimmermann</li>
<li>for: 本研究的目的是提高图像理解（RIS）的性能，特别是在面对复杂语言查询时。</li>
<li>methods: 本研究使用了现有的RefCOCO和Visual Genome datasets，并提出了一个新的RISbenchmark数据集，即RIS-CQ，以挑战现有的RIS方法。此外，提出了一种叫做 dual-modality graph alignment model（\textsc{DuMoGa}）的 nichetargeting方法，以提高RIS-CQ的性能。</li>
<li>results: 实验结果表明，\textsc{DuMoGa}方法在RIS-CQ上表现出色，在不同的数据集和模型下都有显著的提高。<details>
<summary>Abstract</summary>
Referring Image Understanding (RIS) has been extensively studied over the past decade, leading to the development of advanced algorithms. However, there has been a lack of research investigating how existing algorithms should be benchmarked with complex language queries, which include more informative descriptions of surrounding objects and backgrounds (\eg \textit{"the black car."} vs. \textit{"the black car is parking on the road and beside the bus."}). Given the significant improvement in the semantic understanding capability of large pre-trained models, it is crucial to take a step further in RIS by incorporating complex language that resembles real-world applications. To close this gap, building upon the existing RefCOCO and Visual Genome datasets, we propose a new RIS benchmark with complex queries, namely \textbf{RIS-CQ}. The RIS-CQ dataset is of high quality and large scale, which challenges the existing RIS with enriched, specific and informative queries, and enables a more realistic scenario of RIS research. Besides, we present a nichetargeting method to better task the RIS-CQ, called dual-modality graph alignment model (\textbf{\textsc{DuMoGa}), which outperforms a series of RIS methods.
</details>
<details>
<summary>摘要</summary>
总结过去的一 деcade，图像理解（RIS）已经得到了广泛的研究，导致了高级算法的开发。然而，现有的研究很少关注如何将现有算法 benchmarking  WITH complex language queries，这些查询包括更加详细的周围对象和背景描述 (\eg "黑色车" vs. "黑色车在路上停放，和汽车一起"). given the significant improvement in semantic understanding capability of large pre-trained models, it is crucial to take a step further in RIS by incorporating complex language that resembles real-world applications. To close this gap, we propose a new RIS benchmark with complex queries, named \textbf{RIS-CQ}. The RIS-CQ dataset is of high quality and large scale, which challenges the existing RIS with enriched, specific, and informative queries, and enables a more realistic scenario of RIS research. besides, we present a niche-targeting method to better task the RIS-CQ, called dual-modality graph alignment model (\textbf{\textsc{DuMoGa}), which outperforms a series of RIS methods.
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-Incremental-Transfer-Learning-Combining-Peer-to-Peer-Federated-Learning-and-Domain-Incremental-Learning-for-Multicenter-Collaboration"><a href="#A-Survey-of-Incremental-Transfer-Learning-Combining-Peer-to-Peer-Federated-Learning-and-Domain-Incremental-Learning-for-Multicenter-Collaboration" class="headerlink" title="A Survey of Incremental Transfer Learning: Combining Peer-to-Peer Federated Learning and Domain Incremental Learning for Multicenter Collaboration"></a>A Survey of Incremental Transfer Learning: Combining Peer-to-Peer Federated Learning and Domain Incremental Learning for Multicenter Collaboration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17192">http://arxiv.org/abs/2309.17192</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yixinghuang/itlsurvey">https://github.com/yixinghuang/itlsurvey</a></li>
<li>paper_authors: Yixing Huang, Christoph Bert, Ahmed Gomaa, Rainer Fietkau, Andreas Maier, Florian Putz</li>
<li>for: 这篇论文旨在解决因数据隐私限制而妨碍多中心协作深度学习模型的发展。</li>
<li>methods: 本文使用了 peer-to-peer 联邦学习和领域增加学习的结合，以解决数据隐私问题，并使用了不断学习技术来 preserve 模型性能。</li>
<li>results: 本文实现了多中心协作深度学习模型的开发，并评估了不同的常数调整基于循环学习方法的影响。研究发现，对于不同的数据不同性、分类器头设置、网络优化器、模型初始化、中心顺序和权重转移类型，不同的调整方法具有不同的影响。<details>
<summary>Abstract</summary>
Due to data privacy constraints, data sharing among multiple clinical centers is restricted, which impedes the development of high performance deep learning models from multicenter collaboration. Naive weight transfer methods share intermediate model weights without raw data and hence can bypass data privacy restrictions. However, performance drops are typically observed when the model is transferred from one center to the next because of the forgetting problem. Incremental transfer learning, which combines peer-to-peer federated learning and domain incremental learning, can overcome the data privacy issue and meanwhile preserve model performance by using continual learning techniques. In this work, a conventional domain/task incremental learning framework is adapted for incremental transfer learning. A comprehensive survey on the efficacy of different regularization-based continual learning methods for multicenter collaboration is performed. The influences of data heterogeneity, classifier head setting, network optimizer, model initialization, center order, and weight transfer type have been investigated thoroughly. Our framework is publicly accessible to the research community for further development.
</details>
<details>
<summary>摘要</summary>
（注意：以下是简化中文版本，具体翻译结果可能与原文有所不同）
</details></li>
</ul>
<hr>
<h2 id="RTFS-Net-Recurrent-time-frequency-modelling-for-efficient-audio-visual-speech-separation"><a href="#RTFS-Net-Recurrent-time-frequency-modelling-for-efficient-audio-visual-speech-separation" class="headerlink" title="RTFS-Net: Recurrent time-frequency modelling for efficient audio-visual speech separation"></a>RTFS-Net: Recurrent time-frequency modelling for efficient audio-visual speech separation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17189">http://arxiv.org/abs/2309.17189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samuel Pegg, Kai Li, Xiaolin Hu</li>
<li>for: 高品质的单modal和多modal演讲分类</li>
<li>methods: 时频域的Recurrent Time-Frequency Separation Network (RTFS-Net)，组合时间和频率维度的独立模型</li>
<li>results: 比前一代SOTA模型更高的性能，仅使用10%的parameters和18%的MACs<details>
<summary>Abstract</summary>
Audio-visual speech separation methods aim to integrate different modalities to generate high-quality separated speech, thereby enhancing the performance of downstream tasks such as speech recognition. Most existing state-of-the-art (SOTA) models operate in the time domain. However, their overly simplistic approach to modeling acoustic features often necessitates larger and more computationally intensive models in order to achieve SOTA performance. In this paper, we present a novel time-frequency domain audio-visual speech separation method: Recurrent Time-Frequency Separation Network (RTFS-Net), which applies its algorithms on the complex time-frequency bins yielded by the Short-Time Fourier Transform. We model and capture the time and frequency dimensions of the audio independently using a multi-layered RNN along each dimension. Furthermore, we introduce a unique attention-based fusion technique for the efficient integration of audio and visual information, and a new mask separation approach that takes advantage of the intrinsic spectral nature of the acoustic features for a clearer separation. RTFS-Net outperforms the previous SOTA method using only 10% of the parameters and 18% of the MACs. This is the first time-frequency domain audio-visual speech separation method to outperform all contemporary time-domain counterparts.
</details>
<details>
<summary>摘要</summary>
Audio-visual speech separation方法目的是将不同的感知模式融合在一起，以生成高质量的分离后的语音，从而提高下游任务如语音识别的性能。大多数现有的State-of-the-art（SOTA）模型都在时域上运行。然而，它们对音响特征的简单化方法通常会导致模型更大、更计算成本高的模型来达到SOTA性能。在这篇论文中，我们介绍了一种新的时域频域音视频混合方法：Recurrent Time-Frequency Separation Network（RTFS-Net），它在Short-Time Fourier Transform（STFT）生成的复杂时域频域缓冲中应用其算法。我们使用多层RNN来独立地模型和捕捉时间和频率维度上的音频信号，并引入了一种新的注意力机制来有效地混合音频和视频信息。此外，我们还提出了一种基于音频特征自然谱的新的掩码分离方法，以提高分离效果。RTFS-Net比前一个SOTA方法使用的参数数量减少了10%，计算资源减少了18%。这是首次在时域频域上实现了所有当代时域counterparts的音视频混合方法，并超越了它们。
</details></li>
</ul>
<hr>
<h2 id="TBD-Pedestrian-Data-Collection-Towards-Rich-Portable-and-Large-Scale-Natural-Pedestrian-Data"><a href="#TBD-Pedestrian-Data-Collection-Towards-Rich-Portable-and-Large-Scale-Natural-Pedestrian-Data" class="headerlink" title="TBD Pedestrian Data Collection: Towards Rich, Portable, and Large-Scale Natural Pedestrian Data"></a>TBD Pedestrian Data Collection: Towards Rich, Portable, and Large-Scale Natural Pedestrian Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17187">http://arxiv.org/abs/2309.17187</a></li>
<li>repo_url: None</li>
<li>paper_authors: Allan Wang, Daisuke Sato, Yasser Corzo, Sonya Simkin, Aaron Steinfeld</li>
<li>for: 这个论文主要是为了研究人员 navigating 和行人行为，特别是通过机器学习方法来模型人员之间的互动和人员与机器人之间的互动。</li>
<li>methods: 作者提出了一种可搬式的数据收集系统，并与一种半自动化标注管道相结合。在这个管道中，作者设计了一个用于检查自动跟踪结果的人类标注Web应用程序。</li>
<li>results: 作者的系统可以在多种环境中进行大规模数据收集，并且可以快速生成标注结果。与现有的行人数据收集方法相比，作者的系统具有三个特点：结合顶部下看和自我中心视角，在人工智能的社会合适的环境中观察人类行为，以及人类验证的标注。<details>
<summary>Abstract</summary>
Social navigation and pedestrian behavior research has shifted towards machine learning-based methods and converged on the topic of modeling inter-pedestrian interactions and pedestrian-robot interactions. For this, large-scale datasets that contain rich information are needed. We describe a portable data collection system, coupled with a semi-autonomous labeling pipeline. As part of the pipeline, we designed a label correction web app that facilitates human verification of automated pedestrian tracking outcomes. Our system enables large-scale data collection in diverse environments and fast trajectory label production. Compared with existing pedestrian data collection methods, our system contains three components: a combination of top-down and ego-centric views, natural human behavior in the presence of a socially appropriate "robot", and human-verified labels grounded in the metric space. To the best of our knowledge, no prior data collection system has a combination of all three components. We further introduce our ever-expanding dataset from the ongoing data collection effort -- the TBD Pedestrian Dataset and show that our collected data is larger in scale, contains richer information when compared to prior datasets with human-verified labels, and supports new research opportunities.
</details>
<details>
<summary>摘要</summary>
社交导航和行人行为研究已经转移到机器学习基于方法和融合到了对行人间交互和机器人间交互的模型研究。为此，需要大规模的数据集，具有丰富的信息。我们描述了一种可搬式数据收集系统，联合了半自动化标注管道。在管道中，我们设计了一个 labels 修正web应用程序，以便人类确认自动跟踪结果的准确性。我们的系统可以在多种环境下进行大规模数据收集，并快速生成标注结果。相比现有的行人数据收集方法，我们的系统具有三个组成部分：一种组合顶部视角和自我视角，人类在社交合适的“机器人”存在下展现自然的人类行为，以及人类验证的标签，围绕着度量空间定义。根据我们所知，没有任何先前的数据收集系统拥有这三个组成部分。我们进一步介绍了我们的持续扩展的数据集——TBD行人数据集，并证明我们收集到的数据规模更大，包含更多的信息，与先前的人类验证标签相比，支持新的研究机会。
</details></li>
</ul>
<hr>
<h2 id="TextField3D-Towards-Enhancing-Open-Vocabulary-3D-Generation-with-Noisy-Text-Fields"><a href="#TextField3D-Towards-Enhancing-Open-Vocabulary-3D-Generation-with-Noisy-Text-Fields" class="headerlink" title="TextField3D: Towards Enhancing Open-Vocabulary 3D Generation with Noisy Text Fields"></a>TextField3D: Towards Enhancing Open-Vocabulary 3D Generation with Noisy Text Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17175">http://arxiv.org/abs/2309.17175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyu Huang, Yihan Zeng, Bowen Dong, Hang Xu, Songcen Xu, Rynson W. H. Lau, Wangmeng Zuo</li>
<li>for: 本研究旨在开拓文本3D生成领域中的开放词汇能力，提高生成的文本3D模型的多样性和可控性。</li>
<li>methods: 我们提出了一种基于Noisy Text Fields（NTFs）的 Conditional 3D生成模型，即TextField3D。在该模型中，而不是直接使用文本提示作为输入，我们在文本提示的各个字段中引入动态噪声，以扩展文本的各个维度的 latent space。此外，我们还提出了一种 NTFBind 模块，用于将视图不变的图像缓存码与 Noisy Fields 相对 align。</li>
<li>results: 我们的方法在多个方面超越了前期方法，包括大词汇量、文本一致性和响应时间低。我们的实验结果表明，TextField3D 可以实现开放词汇3D生成的可能性。<details>
<summary>Abstract</summary>
Recent works learn 3D representation explicitly under text-3D guidance. However, limited text-3D data restricts the vocabulary scale and text control of generations. Generators may easily fall into a stereotype concept for certain text prompts, thus losing open-vocabulary generation ability. To tackle this issue, we introduce a conditional 3D generative model, namely TextField3D. Specifically, rather than using the text prompts as input directly, we suggest to inject dynamic noise into the latent space of given text prompts, i.e., Noisy Text Fields (NTFs). In this way, limited 3D data can be mapped to the appropriate range of textual latent space that is expanded by NTFs. To this end, an NTFGen module is proposed to model general text latent code in noisy fields. Meanwhile, an NTFBind module is proposed to align view-invariant image latent code to noisy fields, further supporting image-conditional 3D generation. To guide the conditional generation in both geometry and texture, multi-modal discrimination is constructed with a text-3D discriminator and a text-2.5D discriminator. Compared to previous methods, TextField3D includes three merits: 1) large vocabulary, 2) text consistency, and 3) low latency. Extensive experiments demonstrate that our method achieves a potential open-vocabulary 3D generation capability.
</details>
<details>
<summary>摘要</summary>
近期研究强调3D表示方法在文本3D指导下进行学习。然而，有限的文本3D数据限制了生成 vocabulary 范围和文本控制。生成器可能轻易陷入某些文本提示的刻板概念，导致失去开放词汇生成能力。为解决这个问题，我们介绍了一种受控3D生成模型，即 TextField3D。具体来说，而不是直接使用文本提示作为输入，我们建议在给定文本提示的缓存空间中注入动态噪声，即噪声文本场（NTF）。这样，有限的3D数据可以被映射到适当的文本缓存空间，该空间通过 NTF 得到扩展。为此，我们提出了 NTFGen 模块，用于模型文本缓存代码在噪声场中。同时，我们提出了 NTFBind 模块，用于将视图不变的图像缓存代码与噪声场相匹配。这样，我们可以在geometry和texture之间进行条件生成。为了引导条件生成，我们构建了文本-3D分类器和文本-2.5D分类器的多模式抑制。相比之前的方法，TextField3D具有三大优点：1）大词汇，2）文本一致性，3）低延迟。广泛的实验表明，我们的方法实现了可能的开放词汇3D生成能力。
</details></li>
</ul>
<hr>
<h2 id="Domain-Adaptive-Learning-Unsupervised-Adaptation-for-Histology-Images-with-Improved-Loss-Function-Combination"><a href="#Domain-Adaptive-Learning-Unsupervised-Adaptation-for-Histology-Images-with-Improved-Loss-Function-Combination" class="headerlink" title="Domain-Adaptive Learning: Unsupervised Adaptation for Histology Images with Improved Loss Function Combination"></a>Domain-Adaptive Learning: Unsupervised Adaptation for Histology Images with Improved Loss Function Combination</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17172">http://arxiv.org/abs/2309.17172</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ravi Kant Gupta, Shounak Das, Amit Sethi</li>
<li>For: 本研究提出了一种新的频率领域适应（UDA）方法，用于针对 Hematoxylin &amp; Eosin（H&amp;E）染色的病理图像。现有的对抗性频率领域适应方法可能无法有效地将不同频率领域的多modal分布相互对接。* Methods: 我们的方法提出了一个新的损失函数，并与现有的损失函数束合使用，以特化解决病理图像的特殊挑战。我们利用病理图像的特有特征，如组织结构和细胞形态，来增强适应性。* Results: 我们的方法在准确性、稳定性和普适性方面表现出色，超过了当前领域的最佳技术。我们在FHIST数据集上进行了广泛的实验，结果显示，我们的提出的方法——频率领域适应学习（DAL）在病理图像上表现出了1.41%和6.56%的提升，比ViT基于和CNN基于的SoTA方法更高。<details>
<summary>Abstract</summary>
This paper presents a novel approach for unsupervised domain adaptation (UDA) targeting H&E stained histology images. Existing adversarial domain adaptation methods may not effectively align different domains of multimodal distributions associated with classification problems. The objective is to enhance domain alignment and reduce domain shifts between these domains by leveraging their unique characteristics. Our approach proposes a novel loss function along with carefully selected existing loss functions tailored to address the challenges specific to histology images. This loss combination not only makes the model accurate and robust but also faster in terms of training convergence. We specifically focus on leveraging histology-specific features, such as tissue structure and cell morphology, to enhance adaptation performance in the histology domain. The proposed method is extensively evaluated in accuracy, robustness, and generalization, surpassing state-of-the-art techniques for histology images. We conducted extensive experiments on the FHIST dataset and the results show that our proposed method - Domain Adaptive Learning (DAL) significantly surpasses the ViT-based and CNN-based SoTA methods by 1.41% and 6.56% respectively.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Retail-786k-a-Large-Scale-Dataset-for-Visual-Entity-Matching"><a href="#Retail-786k-a-Large-Scale-Dataset-for-Visual-Entity-Matching" class="headerlink" title="Retail-786k: a Large-Scale Dataset for Visual Entity Matching"></a>Retail-786k: a Large-Scale Dataset for Visual Entity Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17164">http://arxiv.org/abs/2309.17164</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bianca Lamm, Janis Keuper</li>
<li>for: 本研究旨在提供一个大规模的视觉实体匹配数据集，用于解决现有的实体匹配问题。</li>
<li>methods: 本研究使用了生成的广告刊物，包括多年的欧洲零售商的广告刊物，共计约786万个手动标注的高分辨率产品图像，其中包含约18千个不同的固定产品，分为约3千个实体。</li>
<li>results: 根据Price comparison任务，每个实体形成一个相似类型的产品集，但是使用标准的图像基于分类和检索算法不能够解决这个问题。因此，需要开发新的方法，可以将示例基于的视觉等同类转移到新数据上。本研究的目的是为这种算法提供 benchmark。<details>
<summary>Abstract</summary>
Entity Matching (EM) defines the task of learning to group objects by transferring semantic concepts from example groups (=entities) to unseen data. Despite the general availability of image data in the context of many EM-problems, most currently available EM-algorithms solely rely on (textual) meta data. In this paper, we introduce the first publicly available large-scale dataset for "visual entity matching", based on a production level use case in the retail domain. Using scanned advertisement leaflets, collected over several years from different European retailers, we provide a total of ~786k manually annotated, high resolution product images containing ~18k different individual retail products which are grouped into ~3k entities. The annotation of these product entities is based on a price comparison task, where each entity forms an equivalence class of comparable products. Following on a first baseline evaluation, we show that the proposed "visual entity matching" constitutes a novel learning problem which can not sufficiently be solved using standard image based classification and retrieval algorithms. Instead, novel approaches which allow to transfer example based visual equivalent classes to new data are needed to address the proposed problem. The aim of this paper is to provide a benchmark for such algorithms.   Information about the dataset, evaluation code and download instructions are provided under https://www.retail-786k.org/.
</details>
<details>
<summary>摘要</summary>
“Entity Matching（EM）定义为将知识传递到未见到的数据上，以将概念汇总到例子中。尽管在许多EM问题上有广泛的图像数据可用，现有大多数EM算法仅仅靠文本元数据。在这篇文章中，我们介绍了首次公开可用的大规模“视觉实体匹配”数据集，基于商业化的使用情况，从欧洲多家商家获取了多年的印刷广告单张，总计约786,000个手动标注、高分辨率产品图像，包含约18,000个不同的单独产品，分为约3,000个实体。这些产品实体的标注基于价格比较任务，每个实体都是一个可比较的产品集。我们透过首创基准评估发现，“视觉实体匹配”是一个新的学习问题，不能由标准图像分类和搜寻算法解决。相反，需要新的方法，以将例子基于的见识汇总到新数据。本文的目的是提供这个问题的参考基准。更多关于数据、评估代码和下载 instruction可以在https://www.retail-786k.org/取得。”
</details></li>
</ul>
<hr>
<h2 id="APNet-Urban-level-Scene-Segmentation-of-Aerial-Images-and-Point-Clouds"><a href="#APNet-Urban-level-Scene-Segmentation-of-Aerial-Images-and-Point-Clouds" class="headerlink" title="APNet: Urban-level Scene Segmentation of Aerial Images and Point Clouds"></a>APNet: Urban-level Scene Segmentation of Aerial Images and Point Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17162">http://arxiv.org/abs/2309.17162</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/codename1995/APNet_ICCVW23">https://github.com/codename1995/APNet_ICCVW23</a></li>
<li>paper_authors: Weijie Wei, Martin R. Oswald, Fatemeh Karimi Nejadasl, Theo Gevers</li>
<li>for: 本研究强调 semantic segmentation 方法 для点云图像urban scenes中。</li>
<li>methods: 我们提出了一种名为 APNet的网络架构，它分为两个支线：一个点云支线和一个飞行图支线，其中点云支线输入是从点云中生成的。为了利用每个支线的不同特性，我们使用一种geometry-aware fusione module来结合每个支线的结果。</li>
<li>results: 我们的实验表明， fusione output consistently outperforms each individual network branch, and APNet achieves state-of-the-art performance of 65.2 mIoU on the SensatUrban dataset。<details>
<summary>Abstract</summary>
In this paper, we focus on semantic segmentation method for point clouds of urban scenes. Our fundamental concept revolves around the collaborative utilization of diverse scene representations to benefit from different context information and network architectures. To this end, the proposed network architecture, called APNet, is split into two branches: a point cloud branch and an aerial image branch which input is generated from a point cloud. To leverage the different properties of each branch, we employ a geometry-aware fusion module that is learned to combine the results of each branch. Additional separate losses for each branch avoid that one branch dominates the results, ensure the best performance for each branch individually and explicitly define the input domain of the fusion network assuring it only performs data fusion. Our experiments demonstrate that the fusion output consistently outperforms the individual network branches and that APNet achieves state-of-the-art performance of 65.2 mIoU on the SensatUrban dataset. Upon acceptance, the source code will be made accessible.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们关注点云Scene的semantic segmentation方法。我们的基本概念是通过不同场景表示的多样化合作来利用不同的上下文信息和网络架构。为此，我们提出了一种名为APNet的网络架构，其分为两个分支：一个点云分支和一个空中图分支，后者是根据点云生成的。为了利用每个分支的不同特性，我们采用了一种geometry-aware合并模块，该模块通过学习将每个分支的结果结合在一起。此外，我们还使用了每个分支的分立损失，以避免一个分支占据结果，保证每个分支的最佳性能，并且明确定义了拟合网络的输入领域，确保它只进行数据融合。我们的实验表明，拟合输出 consistently 超过了每个网络分支的结果，而APNet在SensatUrban数据集上实现了65.2 mIoU的state-of-the-art性能。接受后，源代码将公开。
</details></li>
</ul>
<hr>
<h2 id="Redistributing-the-Precision-and-Content-in-3D-LUT-based-Inverse-Tone-mapping-for-HDR-WCG-Display"><a href="#Redistributing-the-Precision-and-Content-in-3D-LUT-based-Inverse-Tone-mapping-for-HDR-WCG-Display" class="headerlink" title="Redistributing the Precision and Content in 3D-LUT-based Inverse Tone-mapping for HDR&#x2F;WCG Display"></a>Redistributing the Precision and Content in 3D-LUT-based Inverse Tone-mapping for HDR&#x2F;WCG Display</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17160">http://arxiv.org/abs/2309.17160</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/andreguo/itmlut">https://github.com/andreguo/itmlut</a></li>
<li>paper_authors: Cheng Guo, Leidong Fan, Qian Zhang, Hanyuan Liu, Kanglin Liu, Xiuhua Jiang</li>
<li>for: 这个论文是为了提出一种基于AI的增强逻辑（ITM），将标准动态范围（SDR）视频转换为高动态范围（HDR）&#x2F;宽色域（WCG）的方法。</li>
<li>methods: 该方法使用了人工智能（AI）学习，将三个不同精度的Look-up表（LUT）组合在一起，以提高效率和质量。</li>
<li>results: 实验结果表明，该方法可以减少转换错误，并且可以在不同的显示设备上提供更好的视觉效果。<details>
<summary>Abstract</summary>
ITM(inverse tone-mapping) converts SDR (standard dynamic range) footage to HDR/WCG (high dynamic range /wide color gamut) for media production. It happens not only when remastering legacy SDR footage in front-end content provider, but also adapting on-theair SDR service on user-end HDR display. The latter requires more efficiency, thus the pre-calculated LUT (look-up table) has become a popular solution. Yet, conventional fixed LUT lacks adaptability, so we learn from research community and combine it with AI. Meanwhile, higher-bit-depth HDR/WCG requires larger LUT than SDR, so we consult traditional ITM for an efficiency-performance trade-off: We use 3 smaller LUTs, each has a non-uniform packing (precision) respectively denser in dark, middle and bright luma range. In this case, their results will have less error only in their own range, so we use a contribution map to combine their best parts to final result. With the guidance of this map, the elements (content) of 3 LUTs will also be redistributed during training. We conduct ablation studies to verify method's effectiveness, and subjective and objective experiments to show its practicability. Code is available at: https://github.com/AndreGuo/ITMLUT.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="HAvatar-High-fidelity-Head-Avatar-via-Facial-Model-Conditioned-Neural-Radiance-Field"><a href="#HAvatar-High-fidelity-Head-Avatar-via-Facial-Model-Conditioned-Neural-Radiance-Field" class="headerlink" title="HAvatar: High-fidelity Head Avatar via Facial Model Conditioned Neural Radiance Field"></a>HAvatar: High-fidelity Head Avatar via Facial Model Conditioned Neural Radiance Field</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17128">http://arxiv.org/abs/2309.17128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaochen Zhao, Lizhen Wang, Jingxiang Sun, Hongwen Zhang, Jinli Suo, Yebin Liu</li>
<li>for: Addresses the problem of modeling an animatable 3D human head avatar under light-weight setups, which has not been well solved.</li>
<li>methods: Introduces a novel hybrid explicit-implicit 3D representation, Facial Model Conditioned Neural Radiance Field, which integrates the expressiveness of NeRF and the prior information from the parametric template.</li>
<li>results: Achieves state-of-the-art performance for 3D head avatar animation, with high-resolution, realistic, and view-consistent synthesis of dynamic head appearance.<details>
<summary>Abstract</summary>
The problem of modeling an animatable 3D human head avatar under light-weight setups is of significant importance but has not been well solved. Existing 3D representations either perform well in the realism of portrait images synthesis or the accuracy of expression control, but not both. To address the problem, we introduce a novel hybrid explicit-implicit 3D representation, Facial Model Conditioned Neural Radiance Field, which integrates the expressiveness of NeRF and the prior information from the parametric template. At the core of our representation, a synthetic-renderings-based condition method is proposed to fuse the prior information from the parametric model into the implicit field without constraining its topological flexibility. Besides, based on the hybrid representation, we properly overcome the inconsistent shape issue presented in existing methods and improve the animation stability. Moreover, by adopting an overall GAN-based architecture using an image-to-image translation network, we achieve high-resolution, realistic and view-consistent synthesis of dynamic head appearance. Experiments demonstrate that our method can achieve state-of-the-art performance for 3D head avatar animation compared with previous methods.
</details>
<details>
<summary>摘要</summary>
“模型动画3D人头化身的问题具有重要意义，但尚未得到完善的解决。现有的3D表示方式可以在真实性的肖像图像合成方面表现出色，但控制表达的准确性不佳；或者可以在表达控制方面表现出色，但肖像图像的真实性不佳。为解决这个问题，我们提出了一种新的混合式显式隐式3D表示方法，即Facial Model Conditioned Neural Radiance Field。我们的表示方法把NeRF的表达力和参数模板中的先知信息结合在一起，以实现高度的表现真实性和控制灵活性。另外，我们还解决了现有方法中的不一致形状问题，提高动画稳定性。此外，我们采用了一种总体的GAN基 architecture，使用图像到图像翻译网络，实现高分辨率、真实和视角一致的动态头型表 synthesis。实验表明，我们的方法可以与之前的方法相比，达到3D头化身动画的州际性表现。”
</details></li>
</ul>
<hr>
<h2 id="Reconstruction-of-Patient-Specific-Confounders-in-AI-based-Radiologic-Image-Interpretation-using-Generative-Pretraining"><a href="#Reconstruction-of-Patient-Specific-Confounders-in-AI-based-Radiologic-Image-Interpretation-using-Generative-Pretraining" class="headerlink" title="Reconstruction of Patient-Specific Confounders in AI-based Radiologic Image Interpretation using Generative Pretraining"></a>Reconstruction of Patient-Specific Confounders in AI-based Radiologic Image Interpretation using Generative Pretraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17123">http://arxiv.org/abs/2309.17123</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/peterhan91/diffchest">https://github.com/peterhan91/diffchest</a></li>
<li>paper_authors: Tianyu Han, Laura Žigutytė, Luisa Huck, Marc Huppertz, Robert Siepmann, Yossi Gandelsman, Christian Blüthgen, Firas Khader, Christiane Kuhl, Sven Nebelung, Jakob Kather, Daniel Truhn</li>
<li>for: 这个研究旨在检测人工智能支持的自动诊断系统中的欺骗模式，以确保其可靠性，特别是在医疗领域。</li>
<li>methods: 我们提出了一种自我条件Diffusion模型，称为DiffChest，并将其训练在515,704个胸部X-RAY影像和194,956名病人的数据集上。DiffChest可以在每个病人水平解释分类结果，并可以显示出可能欺骗模型的变量因素。</li>
<li>results: 我们发现DiffChest可以实现高度的医疗读者一致性，具体而言，Fleiss的Kappa值在大多数影像找到的情况下都是0.8或更高。DiffChest可以正确地捕捉11.1%至100%的变量因素。此外，我们的预训 проце序可以将模型优化以捕捉输入影像中最重要的信息。DiffChest在11种胸部病情的诊断中表现出色，并在其他情况下至少具有足够的诊断精度。<details>
<summary>Abstract</summary>
Detecting misleading patterns in automated diagnostic assistance systems, such as those powered by Artificial Intelligence, is critical to ensuring their reliability, particularly in healthcare. Current techniques for evaluating deep learning models cannot visualize confounding factors at a diagnostic level. Here, we propose a self-conditioned diffusion model termed DiffChest and train it on a dataset of 515,704 chest radiographs from 194,956 patients from multiple healthcare centers in the United States and Europe. DiffChest explains classifications on a patient-specific level and visualizes the confounding factors that may mislead the model. We found high inter-reader agreement when evaluating DiffChest's capability to identify treatment-related confounders, with Fleiss' Kappa values of 0.8 or higher across most imaging findings. Confounders were accurately captured with 11.1% to 100% prevalence rates. Furthermore, our pretraining process optimized the model to capture the most relevant information from the input radiographs. DiffChest achieved excellent diagnostic accuracy when diagnosing 11 chest conditions, such as pleural effusion and cardiac insufficiency, and at least sufficient diagnostic accuracy for the remaining conditions. Our findings highlight the potential of pretraining based on diffusion models in medical image classification, specifically in providing insights into confounding factors and model robustness.
</details>
<details>
<summary>摘要</summary>
检测自动诊断助手系统中的误导性模式是确保其可靠性的关键，特别在医疗领域。现有的深度学习模型评估技术无法在诊断水平可视化干扰因素。我们提议一种自我条件 diffusion 模型，称为 DiffChest，并在515,704 个胸部X光图像和194,956 名病人从美国和欧洲多个医疗中心获取训练数据。DiffChest 可以在病人特定水平解释分类结果，并可视化可能误导模型的干扰因素。我们发现在评估 DiffChest 是否能够确定治疗相关干扰因子时， Fleiss κ 值在 0.8 或更高，对大多数影像发现都达到了0.8 或更高的一致度。干扰因素被正确地捕捉，存在11.1% 到100% 的发现率。此外，我们的预训练过程将模型优化为从输入胸部X光图像中捕捉最重要的信息。DiffChest 在11 种胸部疾病诊断中达到了出色的诊断精度，并在剩下的疾病诊断中至少达到了足够的诊断精度。我们的发现表明基于扩散模型的预训练在医疗影像分类中具有潜在的优势，特别是在提供干扰因素的透视和模型Robustness。
</details></li>
</ul>
<hr>
<h2 id="Continual-Action-Assessment-via-Task-Consistent-Score-Discriminative-Feature-Distribution-Modeling"><a href="#Continual-Action-Assessment-via-Task-Consistent-Score-Discriminative-Feature-Distribution-Modeling" class="headerlink" title="Continual Action Assessment via Task-Consistent Score-Discriminative Feature Distribution Modeling"></a>Continual Action Assessment via Task-Consistent Score-Discriminative Feature Distribution Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17105">http://arxiv.org/abs/2309.17105</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Lyman-Smoker/ConAQA">https://github.com/Lyman-Smoker/ConAQA</a></li>
<li>paper_authors: Yuan-Ming Li, Ling-An Zeng, Jing-Ke Meng, Wei-Shi Zheng</li>
<li>for: 本文目的是解决 continual learning 问题在 action quality assessment (AQA) 领域，即在不可见的情况下，模型可以 continual learning 地学习 AQA 任务。</li>
<li>methods: 我们提出了一种 Feature-Score Correlation-Aware Rehearsal 技术，以及一种 Action General-Specific Graph 技术，以mitigate 忘记现象。</li>
<li>results: 我们的方法在多个任务和多种动作类型下表现出色，可以减少忘记现象，并且比较有效和灵活。<details>
<summary>Abstract</summary>
Action Quality Assessment (AQA) is a task that tries to answer how well an action is carried out. While remarkable progress has been achieved, existing works on AQA assume that all the training data are visible for training in one time, but do not enable continual learning on assessing new technical actions. In this work, we address such a Continual Learning problem in AQA (Continual-AQA), which urges a unified model to learn AQA tasks sequentially without forgetting. Our idea for modeling Continual-AQA is to sequentially learn a task-consistent score-discriminative feature distribution, in which the latent features express a strong correlation with the score labels regardless of the task or action types. From this perspective, we aim to mitigate the forgetting in Continual-AQA from two aspects. Firstly, to fuse the features of new and previous data into a score-discriminative distribution, a novel Feature-Score Correlation-Aware Rehearsal is proposed to store and reuse data from previous tasks with limited memory size. Secondly, an Action General-Specific Graph is developed to learn and decouple the action-general and action-specific knowledge so that the task-consistent score-discriminative features can be better extracted across various tasks. Extensive experiments are conducted to evaluate the contributions of proposed components. The comparisons with the existing continual learning methods additionally verify the effectiveness and versatility of our approach.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Prototype-guided-Cross-modal-Completion-and-Alignment-for-Incomplete-Text-based-Person-Re-identification"><a href="#Prototype-guided-Cross-modal-Completion-and-Alignment-for-Incomplete-Text-based-Person-Re-identification" class="headerlink" title="Prototype-guided Cross-modal Completion and Alignment for Incomplete Text-based Person Re-identification"></a>Prototype-guided Cross-modal Completion and Alignment for Incomplete Text-based Person Re-identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17104">http://arxiv.org/abs/2309.17104</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiantian Gong, Guodong Du, Junsheng Wang, Yongkang Ding, Liyan Zhang</li>
<li>for:  Addresses the practical issue of incomplete text-based person re-identification (ReID) in real-world applications, where person images and text descriptions are not completely matched and contain partially missing modality data.</li>
<li>methods:  Proposes a novel Prototype-guided Cross-modal Completion and Alignment (PCCA) framework, which includes cross-modal nearest neighbor construction, relation graphs, and prototype-aware cross-modal alignment loss to handle incomplete data.</li>
<li>results:  Consistently outperforms state-of-the-art text-image ReID approaches on several benchmarks with different missing ratios, demonstrating the effectiveness of the proposed method in handling incomplete data.<details>
<summary>Abstract</summary>
Traditional text-based person re-identification (ReID) techniques heavily rely on fully matched multi-modal data, which is an ideal scenario. However, due to inevitable data missing and corruption during the collection and processing of cross-modal data, the incomplete data issue is usually met in real-world applications. Therefore, we consider a more practical task termed the incomplete text-based ReID task, where person images and text descriptions are not completely matched and contain partially missing modality data. To this end, we propose a novel Prototype-guided Cross-modal Completion and Alignment (PCCA) framework to handle the aforementioned issues for incomplete text-based ReID. Specifically, we cannot directly retrieve person images based on a text query on missing modality data. Therefore, we propose the cross-modal nearest neighbor construction strategy for missing data by computing the cross-modal similarity between existing images and texts, which provides key guidance for the completion of missing modal features. Furthermore, to efficiently complete the missing modal features, we construct the relation graphs with the aforementioned cross-modal nearest neighbor sets of missing modal data and the corresponding prototypes, which can further enhance the generated missing modal features. Additionally, for tighter fine-grained alignment between images and texts, we raise a prototype-aware cross-modal alignment loss that can effectively reduce the modality heterogeneity gap for better fine-grained alignment in common space. Extensive experimental results on several benchmarks with different missing ratios amply demonstrate that our method can consistently outperform state-of-the-art text-image ReID approaches.
</details>
<details>
<summary>摘要</summary>
传统的文本基于人识别（ReID）技术强调完全匹配的多模态数据，这是理想的情况。然而，实际应用中经常遇到数据缺失和损坏问题，因此我们考虑了一个更实际的任务：受限文本基于ReID任务，其中人像图像和文本描述不完全匹配，包含部分缺失的modal数据。为此，我们提出了一种 novel Prototype-guided Cross-modal Completion and Alignment（PCCA）框架来解决上述问题。具体来说，我们无法直接根据缺失modal数据来检索人像图像。因此，我们提出了跨模态最近邻构建策略，通过计算跨模态相似性来完成缺失modal特征。此外，为了有效完成缺失modal特征，我们构建了跨模态最近邻集和对应的原型，可以进一步增强生成的缺失modal特征。此外，为了更紧密地对齐图像和文本，我们提出了原型意识的跨模态对齐损失，可以更好地减少模态差异，以便更紧密地对齐在共同空间中。广泛的实验结果表明，我们的方法可以在多个benchmark上与状态ola分别获得优于状态艺。
</details></li>
</ul>
<hr>
<h2 id="Guiding-Instruction-based-Image-Editing-via-Multimodal-Large-Language-Models"><a href="#Guiding-Instruction-based-Image-Editing-via-Multimodal-Large-Language-Models" class="headerlink" title="Guiding Instruction-based Image Editing via Multimodal Large Language Models"></a>Guiding Instruction-based Image Editing via Multimodal Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17102">http://arxiv.org/abs/2309.17102</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tsujuifu/pytorch_mgie">https://github.com/tsujuifu/pytorch_mgie</a></li>
<li>paper_authors: Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, Zhe Gan</li>
<li>for: 这个论文主要用于提高图像修改的可控性和灵活性，通过自然的命令而不需要详细的描述或区域mask。</li>
<li>methods: 这个论文使用多modal大语言模型（MLLM）来提高图像修改的可控性和灵活性，通过LM来实现跨modal的理解和视觉相关的回应生成。</li>
<li>results: 这个论文的实验结果表明，使用MLLM可以大幅提高图像修改的自动度和人工评价，同时保持竞争性的推理效率。<details>
<summary>Abstract</summary>
Instruction-based image editing improves the controllability and flexibility of image manipulation via natural commands without elaborate descriptions or regional masks. However, human instructions are sometimes too brief for current methods to capture and follow. Multimodal large language models (MLLMs) show promising capabilities in cross-modal understanding and visual-aware response generation via LMs. We investigate how MLLMs facilitate edit instructions and present MLLM-Guided Image Editing (MGIE). MGIE learns to derive expressive instructions and provides explicit guidance. The editing model jointly captures this visual imagination and performs manipulation through end-to-end training. We evaluate various aspects of Photoshop-style modification, global photo optimization, and local editing. Extensive experimental results demonstrate that expressive instructions are crucial to instruction-based image editing, and our MGIE can lead to a notable improvement in automatic metrics and human evaluation while maintaining competitive inference efficiency.
</details>
<details>
<summary>摘要</summary>
instruction-based图像编辑提高图像修改的可控性和灵活性，通过自然的命令而无需详细的描述或地域掩码。然而，人类的指令有时候太简短，无法capture和跟踪。多模态大语言模型（MLLM）表现出跨模态理解和视觉意识回快生成的潜力。我们研究如何通过MLLM实现图像修改指令，并提出了MLLM导向图像修改（MGIE）。MGIE学习表达 instrucciones和提供显式导航，修改模型同时捕捉这种视觉想象并进行修改。我们评估了各种修改方面，包括批处理修改、全局图像优化和本地修改。广泛的实验结果表明，表达 instrucciones是指令基于图像修改的关键因素，而我们的MGIE可以在自动指标和人类评估中带来明显的改进，同时保持竞争力强的推理效率。
</details></li>
</ul>
<hr>
<h2 id="Prototype-based-Aleatoric-Uncertainty-Quantification-for-Cross-modal-Retrieval"><a href="#Prototype-based-Aleatoric-Uncertainty-Quantification-for-Cross-modal-Retrieval" class="headerlink" title="Prototype-based Aleatoric Uncertainty Quantification for Cross-modal Retrieval"></a>Prototype-based Aleatoric Uncertainty Quantification for Cross-modal Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17093">http://arxiv.org/abs/2309.17093</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leolee99/pau">https://github.com/leolee99/pau</a></li>
<li>paper_authors: Hao Li, Jingkuan Song, Lianli Gao, Xiaosu Zhu, Heng Tao Shen</li>
<li>For: The paper is written for improving the reliability of cross-modal retrieval methods by quantifying the uncertainty arisen from inherent data ambiguity.* Methods: The paper proposes a novel Prototype-based Aleatoric Uncertainty Quantification (PAU) framework, which constructs learnable prototypes for each modality, uses Dempster-Shafer Theory and Subjective Logic Theory to build an evidential theoretical framework, and induces accurate uncertainty and reliable predictions for cross-modal retrieval.* Results: The paper demonstrates the effectiveness of the PAU model through extensive experiments on four major benchmark datasets, achieving accurate uncertainty and reliable predictions for cross-modal retrieval.Here’s the information in Simplified Chinese text:* For: 本文是为了提高跨模态检索方法的可靠性，通过量化数据本身的不确定性。* Methods: 本文提出了一种基于 проtotypes的 Aleatoric Uncertainty Quantification (PAU) 框架，通过构建每个模式的学习可变的原型，利用 Dempster-Shafer 理论和主观逻辑理论来建立证据理论框架，实现准确的不确定性和可靠的预测。* Results: 本文通过对四个主要的 benchmark 数据集进行了广泛的实验，证明了 PAU 模型的效iveness，实现了准确的不确定性和可靠的预测。<details>
<summary>Abstract</summary>
Cross-modal Retrieval methods build similarity relations between vision and language modalities by jointly learning a common representation space. However, the predictions are often unreliable due to the Aleatoric uncertainty, which is induced by low-quality data, e.g., corrupt images, fast-paced videos, and non-detailed texts. In this paper, we propose a novel Prototype-based Aleatoric Uncertainty Quantification (PAU) framework to provide trustworthy predictions by quantifying the uncertainty arisen from the inherent data ambiguity. Concretely, we first construct a set of various learnable prototypes for each modality to represent the entire semantics subspace. Then Dempster-Shafer Theory and Subjective Logic Theory are utilized to build an evidential theoretical framework by associating evidence with Dirichlet Distribution parameters. The PAU model induces accurate uncertainty and reliable predictions for cross-modal retrieval. Extensive experiments are performed on four major benchmark datasets of MSR-VTT, MSVD, DiDeMo, and MS-COCO, demonstrating the effectiveness of our method. The code is accessible at https://github.com/leolee99/PAU.
</details>
<details>
<summary>摘要</summary>
跨模态检索方法建立视觉语言模态之间的相似关系，通过共同学习一个公共表示空间。然而，预测结果经常不可靠，这是由于 Aleatoric uncertainty 引起的，这种不确定性来自于低质量数据，如损坏图像、快速视频和不细节的文本。在这篇论文中，我们提出了一种新的 Prototype-based Aleatoric Uncertainty Quantification（PAU）框架，以提供可靠的预测。具体来说，我们首先构建了每个模态的多种可学习prototype来表示整个 semantics 子空间。然后，我们使用 Dempster-Shafer 理论和主观逻辑理论来建立证据框架，将证据与 Dirichlet 分布参数相关联。PAU 模型可以准确地量ify Aleatoric 不确定性，并提供可靠的预测 для跨模态检索。我们在四个主要的 benchmark 数据集上进行了广泛的实验，结果表明我们的方法的有效性。代码可以在 https://github.com/leolee99/PAU 中下载。
</details></li>
</ul>
<hr>
<h2 id="SegRCDB-Semantic-Segmentation-via-Formula-Driven-Supervised-Learning"><a href="#SegRCDB-Semantic-Segmentation-via-Formula-Driven-Supervised-Learning" class="headerlink" title="SegRCDB: Semantic Segmentation via Formula-Driven Supervised Learning"></a>SegRCDB: Semantic Segmentation via Formula-Driven Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17083">http://arxiv.org/abs/2309.17083</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dahlian00/segrcdb">https://github.com/dahlian00/segrcdb</a></li>
<li>paper_authors: Risa Shinoda, Ryo Hayamizu, Kodai Nakashima, Nakamasa Inoue, Rio Yokota, Hirokatsu Kataoka<br>for: 这个论文旨在提高视觉模型的训练效果，使其可以使用有限多个标注图像进行训练。methods: 该论文提出了一个新的数据集SegRCDB，该数据集基于一种公式驱动的监督学习方法，可以在无需实际图像或手动Semantic标注的情况下进行预训练。results: 预训练使用SegRCDB得到了高于COCO-Stuff的mIoU，并且在ADE-20k和Cityscapes上进行精度调整也有同样的表现。这些结果表明SegRCDB有很高的潜在价值，可以为semantic segmentation预训练和研究提供帮助。<details>
<summary>Abstract</summary>
Pre-training is a strong strategy for enhancing visual models to efficiently train them with a limited number of labeled images. In semantic segmentation, creating annotation masks requires an intensive amount of labor and time, and therefore, a large-scale pre-training dataset with semantic labels is quite difficult to construct. Moreover, what matters in semantic segmentation pre-training has not been fully investigated. In this paper, we propose the Segmentation Radial Contour DataBase (SegRCDB), which for the first time applies formula-driven supervised learning for semantic segmentation. SegRCDB enables pre-training for semantic segmentation without real images or any manual semantic labels. SegRCDB is based on insights about what is important in pre-training for semantic segmentation and allows efficient pre-training. Pre-training with SegRCDB achieved higher mIoU than the pre-training with COCO-Stuff for fine-tuning on ADE-20k and Cityscapes with the same number of training images. SegRCDB has a high potential to contribute to semantic segmentation pre-training and investigation by enabling the creation of large datasets without manual annotation. The SegRCDB dataset will be released under a license that allows research and commercial use. Code is available at: https://github.com/dahlian00/SegRCDB
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译为简化字符串。<</SYS>>预训练是一种强大的策略，可以帮助图像模型高效地在有限的标注图像上训练。在semantic segmentation中，创建标注mask需要很大的劳动和时间，因此建立大规模的预训练数据集 WITH semantic标签很难。此外，预训练中的什么都没有充分研究。在这篇论文中，我们提出了Segmentation Radial Contour DataBase (SegRCDB)，这是第一次应用式驱动学习来进行semantic segmentation的预训练。SegRCDB可以在无需真实图像或任何手动semantic标签的情况下进行预训练。SegRCDB基于预训练中对semantic segmentation的重要因素的理解，允许高效的预训练。预训练使用SegRCDB达到了与COCO-Stuff的同样数量的训练图像上的mIoU高于fine-tuning。SegRCDB具有推动semantic segmentation预训练和研究的潜力，因为它允许创建大规模的数据集无需手动标注。SegRCDB数据集将在允许研究和商业使用的license下发布。代码可以在以下地址找到：https://github.com/dahlian00/SegRCDB
</details></li>
</ul>
<hr>
<h2 id="Benefits-of-mirror-weight-symmetry-for-3D-mesh-segmentation-in-biomedical-applications"><a href="#Benefits-of-mirror-weight-symmetry-for-3D-mesh-segmentation-in-biomedical-applications" class="headerlink" title="Benefits of mirror weight symmetry for 3D mesh segmentation in biomedical applications"></a>Benefits of mirror weight symmetry for 3D mesh segmentation in biomedical applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17076">http://arxiv.org/abs/2309.17076</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vladislav Dordiuk, Maksim Dzhigil, Konstantin Ushenin</li>
<li>for: 这个研究旨在探讨在3D瓷器分割任务中如何使用重量对称性来提高模型的准确率和参数数量。</li>
<li>methods: 该研究使用了卷积神经网络，并通过对重量进行对称处理来提高模型的泛化能力。</li>
<li>results: 研究发现，通过对重量进行对称处理，可以提高模型的准确率，并且可以降低模型的参数数量，甚至可以使用非常小的训练集来进行模型训练。<details>
<summary>Abstract</summary>
3D mesh segmentation is an important task with many biomedical applications. The human body has bilateral symmetry and some variations in organ positions. It allows us to expect a positive effect of rotation and inversion invariant layers in convolutional neural networks that perform biomedical segmentations. In this study, we show the impact of weight symmetry in neural networks that perform 3D mesh segmentation. We analyze the problem of 3D mesh segmentation for pathological vessel structures (aneurysms) and conventional anatomical structures (endocardium and epicardium of ventricles). Local geometrical features are encoded as sampling from the signed distance function, and the neural network performs prediction for each mesh node. We show that weight symmetry gains from 1 to 3% of additional accuracy and allows decreasing the number of trainable parameters up to 8 times without suffering the performance loss if neural networks have at least three convolutional layers. This also works for very small training sets.
</details>
<details>
<summary>摘要</summary>
三角形网格分割是生物医学应用中非常重要的任务。人体具有左右对称和一些器官位置的变化。这使得我们可以预期对于旋转和反转不变层在卷积神经网络中的积分层。在这个研究中，我们研究了3D网格分割任务中的重量对称的影响。我们分析了血管结构疾病（液体膜瘤）和常见解剖结构（心脏腔和心脏壁）的3D网格分割问题。本地几何特征被编码为signed distance函数的采样，神经网络对每个网格节点进行预测。我们发现，在神经网络具有至少三层卷积层的情况下，Weight Symmetry可以增加1%到3%的准确率，并且可以降低训练集的数量，最多降低8倍，而不会影响性能。这也适用于非常小的训练集。
</details></li>
</ul>
<hr>
<h2 id="DeeDiff-Dynamic-Uncertainty-Aware-Early-Exiting-for-Accelerating-Diffusion-Model-Generation"><a href="#DeeDiff-Dynamic-Uncertainty-Aware-Early-Exiting-for-Accelerating-Diffusion-Model-Generation" class="headerlink" title="DeeDiff: Dynamic Uncertainty-Aware Early Exiting for Accelerating Diffusion Model Generation"></a>DeeDiff: Dynamic Uncertainty-Aware Early Exiting for Accelerating Diffusion Model Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17074">http://arxiv.org/abs/2309.17074</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengkun Tang, Yaqing Wang, Caiwen Ding, Yi Liang, Yao Li, Dongkuan Xu</li>
<li>for: 提高扩散模型的生成效率，适用于实时应用场景。</li>
<li>methods: 提出了一种早离开框架，采用层wise uncertainty estimation module (UEM) 来适应不同层的计算资源分配，以提高生成效率。</li>
<li>results: 对多个数据集进行了广泛的实验，并证明了与全层模型的性能和效率之间的良好平衡。而且，对基eline模型也带来了更好的性能提升。代码和模型已经公开发布 для重现。<details>
<summary>Abstract</summary>
Diffusion models achieve great success in generating diverse and high-fidelity images. The performance improvements come with low generation speed per image, which hinders the application diffusion models in real-time scenarios. While some certain predictions benefit from the full computation of the model in each sample iteration, not every iteration requires the same amount of computation, potentially leading to computation waste. In this work, we propose DeeDiff, an early exiting framework that adaptively allocates computation resources in each sampling step to improve the generation efficiency of diffusion models. Specifically, we introduce a timestep-aware uncertainty estimation module (UEM) for diffusion models which is attached to each intermediate layer to estimate the prediction uncertainty of each layer. The uncertainty is regarded as the signal to decide if the inference terminates. Moreover, we propose uncertainty-aware layer-wise loss to fill the performance gap between full models and early-exited models. With such loss strategy, our model is able to obtain comparable results as full-layer models. Extensive experiments of class-conditional, unconditional, and text-guided generation on several datasets show that our method achieves state-of-the-art performance and efficiency trade-off compared with existing early exiting methods on diffusion models. More importantly, our method even brings extra benefits to baseline models and obtains better performance on CIFAR-10 and Celeb-A datasets. Full code and model are released for reproduction.
</details>
<details>
<summary>摘要</summary>
Diffusion models 取得了高品质和多样化的图像生成成功。然而，每几个图像生成 Speed 较低，对于实时应用而言是一个障碍。一些预测可以从全部模型的计算中获得优化，但不是每个迭代都需要相同的计算量，这可能会导致计算浪费。在这个工作中，我们提出了DeeDiff，一个早期终止框架，可以适当地分配computation资源，以提高对于扩散模型的生成效率。具体来说，我们将时间步长自适应不确定性估计模组（UEM）添加到各个中继层，以估计各个层的预测不确定性。这个不确定性被视为终止决策的信号。此外，我们还提出了层别不确定性感知损失，以填补全层模型和早期终止模型之间的性能差距。这种损失策略使我们的模型能够和全层模型相比较得到相似的表现。我们在多个标准 datasets上进行了广泛的实验，包括类别参数、无条件和文本参数生成。结果显示，我们的方法在效率和表现之间实现了绝佳的调适。此外，我们的方法甚至对基eline模型带来更好的表现，在CIFAR-10和Celeb-A datasets上取得了更高的表现。我们的代码和模型都公开发布，以便重现。
</details></li>
</ul>
<hr>
<h2 id="GSDC-Transformer-An-Efficient-and-Effective-Cue-Fusion-for-Monocular-Multi-Frame-Depth-Estimation"><a href="#GSDC-Transformer-An-Efficient-and-Effective-Cue-Fusion-for-Monocular-Multi-Frame-Depth-Estimation" class="headerlink" title="GSDC Transformer: An Efficient and Effective Cue Fusion for Monocular Multi-Frame Depth Estimation"></a>GSDC Transformer: An Efficient and Effective Cue Fusion for Monocular Multi-Frame Depth Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17059">http://arxiv.org/abs/2309.17059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Naiyu Fang, Lemiao Qiu, Shuyou Zhang, Zili Wang, Zheyuan Zhou, Kerui Hu</li>
<li>for: 本研究旨在提供一种高效和有效的多框架监测驱动 depth estimation，以实现自动驾驶系统中3D信息的感知。</li>
<li>methods: 我们提出了GSDC transformer，一种能够学习缓度关系的灵活注意力机制，以实现精细缓度的cue fusion。此外，我们还使用了空间排序和精细排序来降低计算复杂性。</li>
<li>results: 我们在KITTI dataset上实现了state-of-the-art表现，并且实现了高效的cue fusion速度。<details>
<summary>Abstract</summary>
Depth estimation provides an alternative approach for perceiving 3D information in autonomous driving. Monocular depth estimation, whether with single-frame or multi-frame inputs, has achieved significant success by learning various types of cues and specializing in either static or dynamic scenes. Recently, these cues fusion becomes an attractive topic, aiming to enable the combined cues to perform well in both types of scenes. However, adaptive cue fusion relies on attention mechanisms, where the quadratic complexity limits the granularity of cue representation. Additionally, explicit cue fusion depends on precise segmentation, which imposes a heavy burden on mask prediction. To address these issues, we propose the GSDC Transformer, an efficient and effective component for cue fusion in monocular multi-frame depth estimation. We utilize deformable attention to learn cue relationships at a fine scale, while sparse attention reduces computational requirements when granularity increases. To compensate for the precision drop in dynamic scenes, we represent scene attributes in the form of super tokens without relying on precise shapes. Within each super token attributed to dynamic scenes, we gather its relevant cues and learn local dense relationships to enhance cue fusion. Our method achieves state-of-the-art performance on the KITTI dataset with efficient fusion speed.
</details>
<details>
<summary>摘要</summary>
几何 estimation 提供了一种alternativeapproach  для感知3D信息在自主驾驶中。单框depth estimation 已经取得了 significativesuccess  by learning various types of cues and specializing in either static or dynamic scenes。最近，这些笔记 fusion 成为了一个吸引人的话题， aiming to enable the combined cues to perform well in both types of scenes。然而，adaptive cue fusion  rely on attention mechanisms， where the quadratic complexity limits the granularity of cue representation。 In addition, explicit cue fusion depends on precise segmentation, which imposes a heavy burden on mask prediction。 To address these issues, we propose the GSDC Transformer, an efficient and effective component for cue fusion in monocular multi-frame depth estimation。 We utilize deformable attention to learn cue relationships at a fine scale, while sparse attention reduces computational requirements when granularity increases。 To compensate for the precision drop in dynamic scenes, we represent scene attributes in the form of super tokens without relying on precise shapes。 Within each super token attributed to dynamic scenes, we gather its relevant cues and learn local dense relationships to enhance cue fusion。 Our method achieves state-of-the-art performance on the KITTI dataset with efficient fusion speed。
</details></li>
</ul>
<hr>
<h2 id="Imagery-Dataset-for-Condition-Monitoring-of-Synthetic-Fibre-Ropes"><a href="#Imagery-Dataset-for-Condition-Monitoring-of-Synthetic-Fibre-Ropes" class="headerlink" title="Imagery Dataset for Condition Monitoring of Synthetic Fibre Ropes"></a>Imagery Dataset for Condition Monitoring of Synthetic Fibre Ropes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17058">http://arxiv.org/abs/2309.17058</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anju Rani, Daniel O. Arroyo, Petar Durdevic</li>
<li>for:  automatized visual inspection of synthetic fiber ropes (SFRs) to detect defects and assess remaining useful life (RUL)</li>
<li>methods:  computer vision applications, including object detection, classification, and segmentation</li>
<li>results:  a comprehensive dataset of 6,942 raw images representing both normal and defective SFRs to support the development of robust defect detection algorithms<details>
<summary>Abstract</summary>
Automatic visual inspection of synthetic fibre ropes (SFRs) is a challenging task in the field of offshore, wind turbine industries, etc. The presence of any defect in SFRs can compromise their structural integrity and pose significant safety risks. Due to the large size and weight of these ropes, it is often impractical to detach and inspect them frequently. Therefore, there is a critical need to develop efficient defect detection methods to assess their remaining useful life (RUL). To address this challenge, a comprehensive dataset has been generated, comprising a total of 6,942 raw images representing both normal and defective SFRs. The dataset encompasses a wide array of defect scenarios which may occur throughout their operational lifespan, including but not limited to placking defects, cut strands, chafings, compressions, core outs and normal. This dataset serves as a resource to support computer vision applications, including object detection, classification, and segmentation, aimed at detecting and analyzing defects in SFRs. The availability of this dataset will facilitate the development and evaluation of robust defect detection algorithms. The aim of generating this dataset is to assist in the development of automated defect detection systems that outperform traditional visual inspection methods, thereby paving the way for safer and more efficient utilization of SFRs across a wide range of applications.
</details>
<details>
<summary>摘要</summary>
自动化视觉检测 синтетиче纤维绳 (SFR) 在海上、风力发电等领域是一项复杂的任务。SFR 中任何缺陷都可能会损害其结构完整性，对人员和设备安全造成重要风险。由于这些绳子的大小和重量，常常无法定期检查它们。因此，有一项急需开发高效缺陷检测方法，以评估它们的剩余有用寿（RUL）。为解决这个挑战，我们已经生成了一个全面的数据集，包括总共 6,942 张原始图像，表示正常和缺陷 SFR 的场景。这个数据集包括了各种可能发生在 SFR 的操作寿命中的缺陷enario，包括但不限于斑点缺陷、切断绳、擦伤、压缩、核心缺陷和正常。这个数据集作为计算机视觉应用程序的资源，可以支持对 SFR 中的缺陷进行检测和分类、分割等计算机视觉方法。数据集的可用性将促进了检测和分析 SFR 中的缺陷的自动化系统的开发和评估，从而为 SFR 在各种应用中的更安全和更高效的使用做出了重要贡献。
</details></li>
</ul>
<hr>
<h2 id="A-5-Point-Minimal-Solver-for-Event-Camera-Relative-Motion-Estimation"><a href="#A-5-Point-Minimal-Solver-for-Event-Camera-Relative-Motion-Estimation" class="headerlink" title="A 5-Point Minimal Solver for Event Camera Relative Motion Estimation"></a>A 5-Point Minimal Solver for Event Camera Relative Motion Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17054">http://arxiv.org/abs/2309.17054</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ling Gao, Hang Su, Daniel Gehrig, Marco Cannici, Davide Scaramuzza, Laurent Kneip</li>
<li>for:  Linear motion estimation using event-based cameras</li>
<li>methods:  Derive correct non-linear parametrization of eventails (manifolds generated by lines in the space-time volume of events) and introduce a novel minimal 5-point solver that jointly estimates line parameters and linear camera velocity projections.</li>
<li>results:  Generate more stable relative motion estimates than other methods and consistently achieve a 100% success rate in estimating linear velocity, outperforming existing closed-form solvers.<details>
<summary>Abstract</summary>
Event-based cameras are ideal for line-based motion estimation, since they predominantly respond to edges in the scene. However, accurately determining the camera displacement based on events continues to be an open problem. This is because line feature extraction and dynamics estimation are tightly coupled when using event cameras, and no precise model is currently available for describing the complex structures generated by lines in the space-time volume of events. We solve this problem by deriving the correct non-linear parametrization of such manifolds, which we term eventails, and demonstrate its application to event-based linear motion estimation, with known rotation from an Inertial Measurement Unit. Using this parametrization, we introduce a novel minimal 5-point solver that jointly estimates line parameters and linear camera velocity projections, which can be fused into a single, averaged linear velocity when considering multiple lines. We demonstrate on both synthetic and real data that our solver generates more stable relative motion estimates than other methods while capturing more inliers than clustering based on spatio-temporal planes. In particular, our method consistently achieves a 100% success rate in estimating linear velocity where existing closed-form solvers only achieve between 23% and 70%. The proposed eventails contribute to a better understanding of spatio-temporal event-generated geometries and we thus believe it will become a core building block of future event-based motion estimation algorithms.
</details>
<details>
<summary>摘要</summary>
(Note: The translation is in Simplified Chinese, using the traditional Chinese characters for "event" and "tail".) Event-based 摄像机 идеаль для线性运动估计，因为它们主要响应Scene中的 Edge。  However, accurately determining the camera displacement based on events remains an open problem. This is because line feature extraction and dynamics estimation are closely tied when using event cameras, and no precise model is currently available for describing the complex structures generated by lines in the space-time volume of events. We address this problem by deriving the correct non-linear parametrization of such manifolds, which we term eventails, and demonstrate its application to event-based linear motion estimation, with known rotation from an Inertial Measurement Unit. Using this parametrization, we introduce a novel minimal 5-point solver that jointly estimates line parameters and linear camera velocity projections, which can be fused into a single, averaged linear velocity when considering multiple lines. We demonstrate on both synthetic and real data that our solver generates more stable relative motion estimates than other methods while capturing more inliers than clustering based on spatio-temporal planes. In particular, our method consistently achieves a 100% success rate in estimating linear velocity where existing closed-form solvers only achieve between 23% and 70%. The proposed eventails contribute to a better understanding of spatio-temporal event-generated geometries and we thus believe it will become a core building block of future event-based motion estimation algorithms.
</details></li>
</ul>
<hr>
<h2 id="On-Uniform-Scalar-Quantization-for-Learned-Image-Compression"><a href="#On-Uniform-Scalar-Quantization-for-Learned-Image-Compression" class="headerlink" title="On Uniform Scalar Quantization for Learned Image Compression"></a>On Uniform Scalar Quantization for Learned Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17051">http://arxiv.org/abs/2309.17051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haotian Zhang, Li Li, Dong Liu</li>
<li>for: 本文旨在探讨了在梯度基本训练中使用非 differentiable归一化的图像压缩问题。</li>
<li>methods: 作者提出了一种基于杂化热度控制的方法，并进行了系统的理论分析，包括误差和梯度估计风险的分析。</li>
<li>results: 作者的方法在多种 represntative 图像压缩网络上表现出了更高的性能，并且提供了两个小技巧，一是设置合适的下界参数，二是使用零中心归一化和部分停止梯度。<details>
<summary>Abstract</summary>
Learned image compression possesses a unique challenge when incorporating non-differentiable quantization into the gradient-based training of the networks. Several quantization surrogates have been proposed to fulfill the training, but they were not systematically justified from a theoretical perspective. We fill this gap by contrasting uniform scalar quantization, the most widely used category with rounding being its simplest case, and its training surrogates. In principle, we find two factors crucial: one is the discrepancy between the surrogate and rounding, leading to train-test mismatch; the other is gradient estimation risk due to the surrogate, which consists of bias and variance of the gradient estimation. Our analyses and simulations imply that there is a tradeoff between the train-test mismatch and the gradient estimation risk, and the tradeoff varies across different network structures. Motivated by these analyses, we present a method based on stochastic uniform annealing, which has an adjustable temperature coefficient to control the tradeoff. Moreover, our analyses enlighten us as to two subtle tricks: one is to set an appropriate lower bound for the variance parameter of the estimated quantized latent distribution, which effectively reduces the train-test mismatch; the other is to use zero-center quantization with partial stop-gradient, which reduces the gradient estimation variance and thus stabilize the training. Our method with the tricks is verified to outperform the existing practices of quantization surrogates on a variety of representative image compression networks.
</details>
<details>
<summary>摘要</summary>
学习图像压缩存在独特挑战，因为在梯度基本训练网络时引入不可微化量化。许多量化代理已被提出，但它们没有系统地从理论角度得到正确的正则化。我们填充这一差距，通过对uniform整数量化和其训练代理进行对比。在理论上，我们发现两个因素非常重要：一是量化代理和圆拟的差异，导致训练和测试之间的差异；另一个是因为代理而导致的梯度估计风险，它包括梯度估计的偏差和方差。我们的分析和实验表明，存在训练和测试之间的交易，而这种交易随着不同的网络结构而变化。我们被这些分析激励，并提出一种基于随机均衡的方法，其中包括可调温度系数以控制交易。此外，我们的分析还让我们发现了两个微妙的技巧：一是设置合适的下界参数，以降低训练和测试之间的差异；另一个是使用零中量化，以降低梯度估计方差，从而稳定训练。我们的方法，包括这两个技巧，在一系列代表性的图像压缩网络上被证明为比现有实践更高效。
</details></li>
</ul>
<hr>
<h2 id="UniQuadric-A-SLAM-Backend-for-Unknown-Rigid-Object-3D-Tracking-and-Light-Weight-Modeling"><a href="#UniQuadric-A-SLAM-Backend-for-Unknown-Rigid-Object-3D-Tracking-and-Light-Weight-Modeling" class="headerlink" title="UniQuadric: A SLAM Backend for Unknown Rigid Object 3D Tracking and Light-Weight Modeling"></a>UniQuadric: A SLAM Backend for Unknown Rigid Object 3D Tracking and Light-Weight Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17036">http://arxiv.org/abs/2309.17036</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linghao Yang, Yanmin Wu, Yu Deng, Rui Tian, Xinggang Hu, Tiefeng Ma</li>
<li>for: 这篇论文的目的是实现环境中未知静止物体的追踪和模型化，并且能够同时进行自我运动追踪和物体运动追踪。</li>
<li>methods: 这篇论文使用了一个新的SLAM背端，它结合了自我运动追踪、物体运动追踪和模型化，并且使用了一个新的像素级异步物体追踪器（AOT），让追踪器能够对于不同任务和提示下有效地追踪目标未知物体。</li>
<li>results: 这篇论文的结果显示，在实验和实际应用中，这个系统具有了前所未有的稳定性和精度，并且在追踪和模型化中具有了很高的效率和可靠性。<details>
<summary>Abstract</summary>
Tracking and modeling unknown rigid objects in the environment play a crucial role in autonomous unmanned systems and virtual-real interactive applications. However, many existing Simultaneous Localization, Mapping and Moving Object Tracking (SLAMMOT) methods focus solely on estimating specific object poses and lack estimation of object scales and are unable to effectively track unknown objects. In this paper, we propose a novel SLAM backend that unifies ego-motion tracking, rigid object motion tracking, and modeling within a joint optimization framework. In the perception part, we designed a pixel-level asynchronous object tracker (AOT) based on the Segment Anything Model (SAM) and DeAOT, enabling the tracker to effectively track target unknown objects guided by various predefined tasks and prompts. In the modeling part, we present a novel object-centric quadric parameterization to unify both static and dynamic object initialization and optimization. Subsequently, in the part of object state estimation, we propose a tightly coupled optimization model for object pose and scale estimation, incorporating hybrids constraints into a novel dual sliding window optimization framework for joint estimation. To our knowledge, we are the first to tightly couple object pose tracking with light-weight modeling of dynamic and static objects using quadric. We conduct qualitative and quantitative experiments on simulation datasets and real-world datasets, demonstrating the state-of-the-art robustness and accuracy in motion estimation and modeling. Our system showcases the potential application of object perception in complex dynamic scenes.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translation-environment: zh-CN Tracking和模型未知静止物体在自动驾驶系统和虚拟实际交互应用中扮演着关键性的角色。然而，许多现有的同时地理位、地图和移动物体跟踪（SLAMMOT）方法仅仅关注特定物体姿态的估计，而不能有效地跟踪未知物体。在这篇论文中，我们提出了一种新的SLAM后端，它将ego-动态跟踪、静止物体动态跟踪和模型化集成到一个共同优化框架中。在感知部分，我们设计了基于Segment Anything Model（SAM）和DeAOT的像素级异步物体跟踪器（AOT），使得跟踪器能够根据不同的任务和提示有效地跟踪目标未知物体。在模型部分，我们提出了一种新的物体-центric四元参数化，以统一静止和动态物体的初始化和优化。在物体状态估计部分，我们提出了一种紧密相互关联的优化模型，将物体姿态和Scale的估计集成到一个新的双滑动窗口优化框架中。到我们所知，我们是第一个通过quadric紧密地跟踪动态和静止物体的 pose和Scale。我们在模拟数据集和实际数据集上进行了质量和量化实验，展示了对运动估计和模型化的状态艺术。我们的系统展示了对复杂动态场景的物体感知的潜在应用。Note: The translation is in Simplified Chinese, which is the standard Chinese writing system used in mainland China. The translation is based on the Google Translate API, and may not be perfect or idiomatic in all cases.
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Document-Structures-with-YOLOv5-Layout-Detection"><a href="#Unveiling-Document-Structures-with-YOLOv5-Layout-Detection" class="headerlink" title="Unveiling Document Structures with YOLOv5 Layout Detection"></a>Unveiling Document Structures with YOLOv5 Layout Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17033">http://arxiv.org/abs/2309.17033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Herman Sugiharto, Yorissa Silviana, Yani Siti Nurpazrin<br>for: 本研究旨在快速识别文档布局和抽取无结构数据。methods: 本研究使用 cutting-edge 计算机视觉模型 YOLOv5 进行文档布局识别和无结构数据抽取。results: YOLOv5 模型在文档布局识别任务中表现出众，准确率为 0.91，准确率为 0.971， F1 分数为 0.939，ROC曲线下的面积为 0.975。这个系统可以有效地提高无结构数据抽取的效率。<details>
<summary>Abstract</summary>
The current digital environment is characterized by the widespread presence of data, particularly unstructured data, which poses many issues in sectors including finance, healthcare, and education. Conventional techniques for data extraction encounter difficulties in dealing with the inherent variety and complexity of unstructured data, hence requiring the adoption of more efficient methodologies. This research investigates the utilization of YOLOv5, a cutting-edge computer vision model, for the purpose of rapidly identifying document layouts and extracting unstructured data.   The present study establishes a conceptual framework for delineating the notion of "objects" as they pertain to documents, incorporating various elements such as paragraphs, tables, photos, and other constituent parts. The main objective is to create an autonomous system that can effectively recognize document layouts and extract unstructured data, hence improving the effectiveness of data extraction.   In the conducted examination, the YOLOv5 model exhibits notable effectiveness in the task of document layout identification, attaining a high accuracy rate along with a precision value of 0.91, a recall value of 0.971, an F1-score of 0.939, and an area under the receiver operating characteristic curve (AUC-ROC) of 0.975. The remarkable performance of this system optimizes the process of extracting textual and tabular data from document images. Its prospective applications are not limited to document analysis but can encompass unstructured data from diverse sources, such as audio data.   This study lays the foundation for future investigations into the wider applicability of YOLOv5 in managing various types of unstructured data, offering potential for novel applications across multiple domains.
</details>
<details>
<summary>摘要</summary>
现今数字环境中，尤其是在金融、医疗和教育等领域，数据的普遍存在和不结构化的特点带来了许多问题。传统的数据EXTRACTING技术在处理不结构化数据的自然多样性和复杂性时遇到困难，因此需要采用更高效的方法ologies。本研究利用YOLOv5 cutting-edge计算机视觉模型，为了快速识别文档布局和提取不结构化数据。本研究提出了对文档中的"对象"概念的定义，包括段落、表格、照片等元素。主要目标是创建一个自主的系统，可以快速识别文档布局并提取不结构化数据，从而改善数据EXTRACTING的效率。在实验中，YOLOv5模型在文档布局识别任务中表现出了remarkable的效果，实现了高精度率、精度值0.91、回归值0.971、F1值0.939和ROC曲线下的接收操作特征值0.975。这个系统的出色表现可以优化文档图像中的文本和表格数据EXTRACTING过程。其潜在应用不限于文档分析，还可以涵盖多种不结构化数据的管理，如音频数据。本研究为未来对YOLOv5在不同类型的不结构化数据管理方面的进一步研究提供了基础。
</details></li>
</ul>
<hr>
<h2 id="HoloAssist-an-Egocentric-Human-Interaction-Dataset-for-Interactive-AI-Assistants-in-the-Real-World"><a href="#HoloAssist-an-Egocentric-Human-Interaction-Dataset-for-Interactive-AI-Assistants-in-the-Real-World" class="headerlink" title="HoloAssist: an Egocentric Human Interaction Dataset for Interactive AI Assistants in the Real World"></a>HoloAssist: an Egocentric Human Interaction Dataset for Interactive AI Assistants in the Real World</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17024">http://arxiv.org/abs/2309.17024</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Wang, Taein Kwon, Mahdi Rad, Bowen Pan, Ishani Chakraborty, Sean Andrist, Dan Bohus, Ashley Feniello, Bugra Tekin, Felipe Vieira Frujeri, Neel Joshi, Marc Pollefeys<br>for:这个研究的目的是开发一种可以与人类交互并协助完成物理世界任务的智能助手。methods:这个研究使用了一种大规模的egosistent human interaction数据集，名为HoloAssist，其中两个人共同完成物理搅动任务。任务执行者通过穿着一种混合现实头盔记录了七个同步数据流。任务指导者通过实时观看执行者的 egocentric 视频，给出指导。results:通过对数据进行扩充，并观察参与者的各种行为，我们提供了关键的洞察，包括人工指导者如何 corrrect 错误， intervene 在任务完成过程中，并将指导链接到环境。HoloAssist 涵盖了 166 小时的数据， captured by 350 个唯一的 instructor-performer 对。此外，我们构建了一些 benchmark，包括 mistake detection， intervention type prediction，和 hand forecasting，并进行了详细分析。我们期望 HoloAssist 将成为建立智能助手交互与人类的重要资源。数据可以在 <a target="_blank" rel="noopener" href="https://holoassist.github.io/">https://holoassist.github.io/</a> 下载。<details>
<summary>Abstract</summary>
Building an interactive AI assistant that can perceive, reason, and collaborate with humans in the real world has been a long-standing pursuit in the AI community. This work is part of a broader research effort to develop intelligent agents that can interactively guide humans through performing tasks in the physical world. As a first step in this direction, we introduce HoloAssist, a large-scale egocentric human interaction dataset, where two people collaboratively complete physical manipulation tasks. The task performer executes the task while wearing a mixed-reality headset that captures seven synchronized data streams. The task instructor watches the performer's egocentric video in real time and guides them verbally. By augmenting the data with action and conversational annotations and observing the rich behaviors of various participants, we present key insights into how human assistants correct mistakes, intervene in the task completion procedure, and ground their instructions to the environment. HoloAssist spans 166 hours of data captured by 350 unique instructor-performer pairs. Furthermore, we construct and present benchmarks on mistake detection, intervention type prediction, and hand forecasting, along with detailed analysis. We expect HoloAssist will provide an important resource for building AI assistants that can fluidly collaborate with humans in the real world. Data can be downloaded at https://holoassist.github.io/.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Segment-Anything-Model-is-a-Good-Teacher-for-Local-Feature-Learning"><a href="#Segment-Anything-Model-is-a-Good-Teacher-for-Local-Feature-Learning" class="headerlink" title="Segment Anything Model is a Good Teacher for Local Feature Learning"></a>Segment Anything Model is a Good Teacher for Local Feature Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16992">http://arxiv.org/abs/2309.16992</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vignywang/samfeat">https://github.com/vignywang/samfeat</a></li>
<li>paper_authors: Jingqian Wu, Rongtao Xu, Zach Wood-Doughty, Changwei Wang</li>
<li>for: 提高地方特征描述性能</li>
<li>methods: 使用 SAM 模型作为导师，通过Pixel Semantic Relational Distillation (PSRD) 和 Weakly Supervised Contrastive Learning Based on Semantic Grouping (WSC) 等技术进行地方特征学习和描述</li>
<li>results: 在多个任务上达到了更高的性能，比如图像匹配和长期视觉地址Local feature detection and description are crucial for many computer vision tasks, but data-driven methods rely on pixel-level correspondence, which is challenging to obtain. This paper proposes SAMFeat, which uses a pre-trained SAM model as a teacher to guide local feature learning and improve performance on limited datasets. The proposed method includes Pixel Semantic Relational Distillation (PSRD) and Weakly Supervised Contrastive Learning Based on Semantic Grouping (WSC), and an Edge Attention Guidance (EAG) to improve the accuracy of local feature detection and description. The results show that SAMFeat outperforms previous local features on various tasks, such as image matching on HPatches and long-term visual localization on Aachen Day-Night.<details>
<summary>Abstract</summary>
Local feature detection and description play an important role in many computer vision tasks, which are designed to detect and describe keypoints in "any scene" and "any downstream task". Data-driven local feature learning methods need to rely on pixel-level correspondence for training, which is challenging to acquire at scale, thus hindering further improvements in performance. In this paper, we propose SAMFeat to introduce SAM (segment anything model), a fundamental model trained on 11 million images, as a teacher to guide local feature learning and thus inspire higher performance on limited datasets. To do so, first, we construct an auxiliary task of Pixel Semantic Relational Distillation (PSRD), which distillates feature relations with category-agnostic semantic information learned by the SAM encoder into a local feature learning network, to improve local feature description using semantic discrimination. Second, we develop a technique called Weakly Supervised Contrastive Learning Based on Semantic Grouping (WSC), which utilizes semantic groupings derived from SAM as weakly supervised signals, to optimize the metric space of local descriptors. Third, we design an Edge Attention Guidance (EAG) to further improve the accuracy of local feature detection and description by prompting the network to pay more attention to the edge region guided by SAM. SAMFeat's performance on various tasks such as image matching on HPatches, and long-term visual localization on Aachen Day-Night showcases its superiority over previous local features. The release code is available at https://github.com/vignywang/SAMFeat.
</details>
<details>
<summary>摘要</summary>
本文提出了一种名为SAMFeat的新方法，用于提高计算机视觉任务中的本地特征检测和描述。SAMFeat利用了一个名为SAM（分类任务模型）的基本模型，该模型在1100万张图像上进行训练，并且作为教师来引导本地特征学习。为此，我们首先构建了一个auxiliary任务，即像素semantic relational distillation（PSRD）任务，该任务将SAM模型学习的分类信息转化为本地特征描述网络中的特征关系。其次，我们开发了一种名为弱式监督对比学习基于 semantic grouping（WSC）的技术，该技术利用了SAM模型生成的semantic grouping来提高本地特征描述的度量空间。最后，我们设计了一种Edge Attention Guidance（EAG）技术，以提高本地特征检测和描述的准确率，并且使网络更加注意到引导Edge region。SAMFeat在多个任务上，如HPatches图像匹配和Aachen日夜长期视觉本地化，表现出了与之前的本地特征相比的超越性。 codes可以在https://github.com/vignywang/SAMFeat上下载。
</details></li>
</ul>
<hr>
<h2 id="Text-image-Alignment-for-Diffusion-based-Perception"><a href="#Text-image-Alignment-for-Diffusion-based-Perception" class="headerlink" title="Text-image Alignment for Diffusion-based Perception"></a>Text-image Alignment for Diffusion-based Perception</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00031">http://arxiv.org/abs/2310.00031</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neehar Kondapaneni, Markus Marks, Manuel Knott, Rogério Guimarães, Pietro Perona</li>
<li>For: The paper is written for exploring the use of diffusion models for visual tasks and improving the perceptual performance of diffusion-based models.* Methods: The paper uses automatically generated captions to improve text-image alignment and enhance the cross-attention maps of the model, leading to better perceptual performance.* Results: The paper achieves state-of-the-art (SOTA) results in diffusion-based semantic segmentation on ADE20K and overall SOTA in depth estimation on NYUv2. The method also generalizes to the cross-domain setting and achieves SOTA results in object detection on Watercolor2K and segmentation on Dark Zurich-val and Nighttime Driving.<details>
<summary>Abstract</summary>
Diffusion models are generative models with impressive text-to-image synthesis capabilities and have spurred a new wave of creative methods for classical machine learning tasks. However, the best way to harness the perceptual knowledge of these generative models for visual tasks is still an open question. Specifically, it is unclear how to use the prompting interface when applying diffusion backbones to vision tasks. We find that automatically generated captions can improve text-image alignment and significantly enhance a model's cross-attention maps, leading to better perceptual performance. Our approach improves upon the current SOTA in diffusion-based semantic segmentation on ADE20K and the current overall SOTA in depth estimation on NYUv2. Furthermore, our method generalizes to the cross-domain setting; we use model personalization and caption modifications to align our model to the target domain and find improvements over unaligned baselines. Our object detection model, trained on Pascal VOC, achieves SOTA results on Watercolor2K. Our segmentation method, trained on Cityscapes, achieves SOTA results on Dark Zurich-val and Nighttime Driving. Project page: https://www.vision.caltech.edu/tadp/
</details>
<details>
<summary>摘要</summary>
填挤模型是一类生成模型，具有吸引人的文本到图像合成能力，并促使了传统机器学习任务中的新一波创新方法。然而，如何利用这些生成模型的感知知识来解决视觉任务仍然是一个打开的问题。具体来说，使用推荐界面在扩散幕中应用 diffusion 模型是一个未解决的问题。我们发现，自动生成的标题可以改善文本-图像对齐，并使模型的交叉注意力图显著提高，从而提高模型的感知性能。我们的方法超过当前 SOTA 在扩散基于 semantic segmentation 任务上的 ADE20K，以及当前总体 SOTA 在 depth estimation 任务上的 NYUv2。此外，我们的方法可以跨领域调整，我们使用模型个性化和标题修改来对我们的模型进行对目标领域的调整，并在不对齐基elines上获得改进。我们的对象检测模型，训练在 Pascal VOC，在 Watercolor2K 上达到了 SOTA 结果。我们的 segmentation 方法，训练在 Cityscapes，在 Dark Zurich-val 和 Nighttime Driving 上达到了 SOTA 结果。更多信息请参考我们的项目页面：https://www.vision.caltech.edu/tadp/
</details></li>
</ul>
<hr>
<h2 id="SpikeMOT-Event-based-Multi-Object-Tracking-with-Sparse-Motion-Features"><a href="#SpikeMOT-Event-based-Multi-Object-Tracking-with-Sparse-Motion-Features" class="headerlink" title="SpikeMOT: Event-based Multi-Object Tracking with Sparse Motion Features"></a>SpikeMOT: Event-based Multi-Object Tracking with Sparse Motion Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16987">http://arxiv.org/abs/2309.16987</a></li>
<li>repo_url: None</li>
<li>paper_authors: Song Wang, Zhu Wang, Can Li, Xiaojuan Qi, Hayden Kwok-Hay So</li>
<li>for: Event-based multi-object tracking (MOT) in real-world settings with complex background and camera motion.</li>
<li>methods: SpikeMOT leverages spiking neural networks to extract sparse spatiotemporal features from event streams associated with objects, and a simultaneous object detector provides updated spatial information.</li>
<li>results: SpikeMOT achieves high tracking accuracy amidst challenging real-world scenarios, advancing the state-of-the-art in event-based multi-object tracking.<details>
<summary>Abstract</summary>
In comparison to conventional RGB cameras, the superior temporal resolution of event cameras allows them to capture rich information between frames, making them prime candidates for object tracking. Yet in practice, despite their theoretical advantages, the body of work on event-based multi-object tracking (MOT) remains in its infancy, especially in real-world settings where events from complex background and camera motion can easily obscure the true target motion. In this work, an event-based multi-object tracker, called SpikeMOT, is presented to address these challenges. SpikeMOT leverages spiking neural networks to extract sparse spatiotemporal features from event streams associated with objects. The resulting spike train representations are used to track the object movement at high frequency, while a simultaneous object detector provides updated spatial information of these objects at an equivalent frame rate. To evaluate the effectiveness of SpikeMOT, we introduce DSEC-MOT, the first large-scale event-based MOT benchmark incorporating fine-grained annotations for objects experiencing severe occlusions, frequent trajectory intersections, and long-term re-identification in real-world contexts. Extensive experiments employing DSEC-MOT and another event-based dataset, named FE240hz, demonstrate SpikeMOT's capability to achieve high tracking accuracy amidst challenging real-world scenarios, advancing the state-of-the-art in event-based multi-object tracking.
</details>
<details>
<summary>摘要</summary>
contrast to traditional RGB 摄像头，事件摄像头的高时间分辨率使其能够捕捉到 frames 之间的质量丰富信息，使其成为目标跟踪的优良候选。然而，在实际应用中，尚未有充分的研究对事件基据多对象跟踪（MOT）进行了深入的探索，特别是在实际场景中，背景和摄像头的运动会轻松隐藏真实的目标运动。在这种情况下，一种基于事件的多对象跟踪器，称为SpikeMOT，被提出来解决这些挑战。SpikeMOT 利用脉冲神经网络提取事件流中对象的稀疏 spatial temporal 特征。这些脉冲表示被跟踪对象的运动，同时一个同步的对象探测器提供了这些对象的新的空间信息，并在相同的帧率上进行跟踪。为了评估SpikeMOT 的效果，我们提出了 DSEC-MOT，这是首个包含细化注释的事件基据 MOT benchmark，这些注释包括对象受到严重遮挡、频繁的轨迹交叠以及长期重新识别。通过使用 DSEC-MOT 和另一个事件基据数据集 named FE240hz，我们进行了广泛的实验，demonstrating SpikeMOT 在真实世界场景中实现高跟踪精度，从而提高事件基据 MOT 的国际先进水平。
</details></li>
</ul>
<hr>
<h2 id="Perceptual-Tone-Mapping-Model-for-High-Dynamic-Range-Imaging"><a href="#Perceptual-Tone-Mapping-Model-for-High-Dynamic-Range-Imaging" class="headerlink" title="Perceptual Tone Mapping Model for High Dynamic Range Imaging"></a>Perceptual Tone Mapping Model for High Dynamic Range Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16975">http://arxiv.org/abs/2309.16975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Imran Mehmood, Xinye Shi, M. Usman Khan, Ming Ronnier Luo</li>
<li>for: 该论文旨在解决高Dynamic Range（HDR）图像到标准动态范围（SDR）显示器的显示问题，保持HDR图像的感知质量。</li>
<li>methods: 该论文使用了CIECAMA16感知颜色属性，包括亮度、颜色彩度和色调，来创建一种基于感知属性的滤镜操作（TMO）。</li>
<li>results: 对比和主观评估表明，该模型在对比、颜色彩度和整体图像质量方面表现出色，超过了现有的TMO。<details>
<summary>Abstract</summary>
One of the key challenges in tone mapping is to preserve the perceptual quality of high dynamic range (HDR) images when mapping them to standard dynamic range (SDR) displays. Traditional tone mapping operators (TMOs) compress the luminance of HDR images without considering the surround and display conditions emanating into suboptimal results. Current research addresses this challenge by incorporating perceptual color appearance attributes. In this work, we propose a TMO (TMOz) that leverages CIECAM16 perceptual attributes, i.e., brightness, colorfulness, and hue. TMOz accounts for the effects of both the surround and the display conditions to achieve more optimal colorfulness reproduction. The perceptual brightness is compressed, and the perceptual color scales, i.e., colorfulness and hue are derived from HDR images by employing CIECAM16 color adaptation equations. A psychophysical experiment was conducted to automate the brightness compression parameter. The model employs fully automatic and adaptive approach, obviating the requirement for manual parameter selection. TMOz was evaluated in terms of contrast, colorfulness and overall image quality. The objective and subjective evaluation methods revealed that the proposed model outperformed the state-of-the-art TMOs.
</details>
<details>
<summary>摘要</summary>
一个关键挑战在声音映射是保持高动态范围（HDR）图像的感知质量 cuando mapping 到标准动态范围（SDR）显示器。传统的声音映射运算符（TMO）压缩 HDR 图像的亮度无论考虑围层和显示条件，导致SUBOPTIMAL 结果。当前的研究利用感知色度属性来解决这个挑战。在这种工作中，我们提出了一种基于 CIECAM16 感知属性的 TMO（TMOz），包括亮度、色彩强度和色调。TMOz 考虑了围层和显示条件的影响，以实现更优化的色彩强度复制。HDR 图像中的感知亮度被压缩，并使用 CIECAM16 色色映射方程 derive 感知色彩和色调。我们进行了一次心理学实验，自动化了亮度压缩参数。模型使用了完全自动和适应的方法，不需要手动参数选择。TMOz 被评估基于对比度、色彩强度和整体图像质量。对象和主观评估方法表明，提案的模型在 state-of-the-art TMOs 中表现更好。
</details></li>
</ul>
<hr>
<h2 id="Synthetic-Data-Generation-and-Deep-Learning-for-the-Topological-Analysis-of-3D-Data"><a href="#Synthetic-Data-Generation-and-Deep-Learning-for-the-Topological-Analysis-of-3D-Data" class="headerlink" title="Synthetic Data Generation and Deep Learning for the Topological Analysis of 3D Data"></a>Synthetic Data Generation and Deep Learning for the Topological Analysis of 3D Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16968">http://arxiv.org/abs/2309.16968</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dylan Peek, Matt P. Skerritt, Stephan Chalup</li>
<li>for: 这个研究使用深度学习来估算由稀疏、无序点云场景表示的三维拓扑结构。</li>
<li>methods: 研究使用了新的标注数据集来训练神经网络并评估它们能够估算这些拓扑结构的 genus。这些数据使用了随机的HOMEOMORPHIC DEFORMATIONS来让学习可见的拓扑特征。</li>
<li>results: 研究表明，深度学习模型可以EXTRACT这些特征，并且与基于PERSISTENT HOMOLOGY的现有拓扑数据分析工具相比，它们具有一些优点。此外，研究还使用了semantic segmentation来提供更多的地形信息，并与拓扑标签相结合。<details>
<summary>Abstract</summary>
This research uses deep learning to estimate the topology of manifolds represented by sparse, unordered point cloud scenes in 3D. A new labelled dataset was synthesised to train neural networks and evaluate their ability to estimate the genus of these manifolds. This data used random homeomorphic deformations to provoke the learning of visual topological features. We demonstrate that deep learning models could extract these features and discuss some advantages over existing topological data analysis tools that are based on persistent homology. Semantic segmentation was used to provide additional geometric information in conjunction with topological labels. Common point cloud multi-layer perceptron and transformer networks were both used to compare the viability of these methods. The experimental results of this pilot study support the hypothesis that, with the aid of sophisticated synthetic data generation, neural networks can perform segmentation-based topological data analysis. While our study focused on simulated data, the accuracy achieved suggests a potential for future applications using real data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="nnSAM-Plug-and-play-Segment-Anything-Model-Improves-nnUNet-Performance"><a href="#nnSAM-Plug-and-play-Segment-Anything-Model-Improves-nnUNet-Performance" class="headerlink" title="nnSAM: Plug-and-play Segment Anything Model Improves nnUNet Performance"></a>nnSAM: Plug-and-play Segment Anything Model Improves nnUNet Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16967">http://arxiv.org/abs/2309.16967</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kent0n-li/medical-image-segmentation">https://github.com/kent0n-li/medical-image-segmentation</a></li>
<li>paper_authors: Yunxiang Li, Bowen Jing, Zihan Li, Jing Wang, You Zhang</li>
<li>for: 这个研究的目的是提出一个能够整合底层模型和专业化神经网络的新方法，以提高医疗影像分类的精度和可靠性。</li>
<li>methods: 这个方法使用了Segment Anything Model (SAM)和nnUNet两种神经网络， synergistically 整合它们以实现更高精度和更好的适应能力。</li>
<li>results: 实验结果显示，这个方法可以在不同的训练数据大小下进行几次学习，并且可以在医疗影像分类中实现更高的精度和可靠性。<details>
<summary>Abstract</summary>
The recent developments of foundation models in computer vision, especially the Segment Anything Model (SAM), allow scalable and domain-agnostic image segmentation to serve as a general-purpose segmentation tool. In parallel, the field of medical image segmentation has benefited significantly from specialized neural networks like the nnUNet, which is trained on domain-specific datasets and can automatically configure the network to tailor to specific segmentation challenges. To combine the advantages of foundation models and domain-specific models, we present nnSAM, which synergistically integrates the SAM model with the nnUNet model to achieve more accurate and robust medical image segmentation. The nnSAM model leverages the powerful and robust feature extraction capabilities of SAM, while harnessing the automatic configuration capabilities of nnUNet to promote dataset-tailored learning. Our comprehensive evaluation of nnSAM model on different sizes of training samples shows that it allows few-shot learning, which is highly relevant for medical image segmentation where high-quality, annotated data can be scarce and costly to obtain. By melding the strengths of both its predecessors, nnSAM positions itself as a potential new benchmark in medical image segmentation, offering a tool that combines broad applicability with specialized efficiency. The code is available at https://github.com/Kent0n-Li/Medical-Image-Segmentation.
</details>
<details>
<summary>摘要</summary>
近年来，计算机视觉领域内的基础模型，特别是Segment Anything Model（SAM），允许扫描领域和领域无关的图像分割，成为一种通用的图像分割工具。同时，医疗图像分割领域也受到专门的神经网络，如nnUNet的启用，这种神经网络通过特定数据集进行自动配置，以适应特定的分割挑战。为了融合基础模型和域специфи的模型的优点，我们提出了nnSAM模型，它将SAM模型与nnUNet模型 synergistically 集成，以实现更高精度和更加稳定的医疗图像分割。nnSAM模型利用SAM模型的强大和稳定的特征提取能力，同时利用nnUNet模型的自动配置能力，以适应特定数据集的学习。我们对不同训练样本大小的nnSAM模型进行了全面的评估，发现它具有几shot学习能力，这是医疗图像分割中非常有价值的特点，因为高质量、标注过的数据可能是昂贵和困难的获得。通过融合两者的优点，nnSAM模型在医疗图像分割中positioned itself为一种新的benchmark，提供了一种可以同时拥有通用性和特定性的工具。代码可以在https://github.com/Kent0n-Li/Medical-Image-Segmentation中找到。
</details></li>
</ul>
<hr>
<h2 id="AdaPose-Towards-Cross-Site-Device-Free-Human-Pose-Estimation-with-Commodity-WiFi"><a href="#AdaPose-Towards-Cross-Site-Device-Free-Human-Pose-Estimation-with-Commodity-WiFi" class="headerlink" title="AdaPose: Towards Cross-Site Device-Free Human Pose Estimation with Commodity WiFi"></a>AdaPose: Towards Cross-Site Device-Free Human Pose Estimation with Commodity WiFi</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16964">http://arxiv.org/abs/2309.16964</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunjiao Zhou, Jianfei Yang, He Huang, Lihua Xie</li>
<li>for:  WiFi-based pose estimation 技术的发展，特别是在智能家居和元宇宙人物生成方面。</li>
<li>methods: 提出了一种适应领域的 pose estimation 算法，名为 AdaPose，用于弱监督的 WiFi CSI poses 估计。</li>
<li>results: 实验结果表明，AdaPose 能够有效地消除领域差异，从而推动 WiFi-based pose estimation 技术在智能城市中的普及应用。<details>
<summary>Abstract</summary>
WiFi-based pose estimation is a technology with great potential for the development of smart homes and metaverse avatar generation. However, current WiFi-based pose estimation methods are predominantly evaluated under controlled laboratory conditions with sophisticated vision models to acquire accurately labeled data. Furthermore, WiFi CSI is highly sensitive to environmental variables, and direct application of a pre-trained model to a new environment may yield suboptimal results due to domain shift. In this paper, we proposes a domain adaptation algorithm, AdaPose, designed specifically for weakly-supervised WiFi-based pose estimation. The proposed method aims to identify consistent human poses that are highly resistant to environmental dynamics. To achieve this goal, we introduce a Mapping Consistency Loss that aligns the domain discrepancy of source and target domains based on inner consistency between input and output at the mapping level. We conduct extensive experiments on domain adaptation in two different scenes using our self-collected pose estimation dataset containing WiFi CSI frames. The results demonstrate the effectiveness and robustness of AdaPose in eliminating domain shift, thereby facilitating the widespread application of WiFi-based pose estimation in smart cities.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="COMNet-Co-Occurrent-Matching-for-Weakly-Supervised-Semantic-Segmentation"><a href="#COMNet-Co-Occurrent-Matching-for-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="COMNet: Co-Occurrent Matching for Weakly Supervised Semantic Segmentation"></a>COMNet: Co-Occurrent Matching for Weakly Supervised Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16959">http://arxiv.org/abs/2309.16959</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yukun Su, Jingliang Deng, Zonghan Li</li>
<li>for: 提高图像水平的弱监督semantic segmentation的质量</li>
<li>methods: 提出了一种名为Co-Occurrent Matching Network（COMNet）的新网络，通过在具有共同类别的图像对进行间相匹配，以及在单个图像上进行内部匹配，来提高分类网络的核心区域匹配。</li>
<li>results: 在Pascal VOC 2012和MS-COCO datasets上，我们的网络可以有效地提高基eline模型的性能，并实现新的状态网络。<details>
<summary>Abstract</summary>
Image-level weakly supervised semantic segmentation is a challenging task that has been deeply studied in recent years. Most of the common solutions exploit class activation map (CAM) to locate object regions. However, such response maps generated by the classification network usually focus on discriminative object parts. In this paper, we propose a novel Co-Occurrent Matching Network (COMNet), which can promote the quality of the CAMs and enforce the network to pay attention to the entire parts of objects. Specifically, we perform inter-matching on paired images that contain common classes to enhance the corresponded areas, and construct intra-matching on a single image to propagate the semantic features across the object regions. The experiments on the Pascal VOC 2012 and MS-COCO datasets show that our network can effectively boost the performance of the baseline model and achieve new state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
Image-levelweakly supervised semantic segmentation是一个复杂的任务，近年来得到了广泛研究。大多数常见的解决方案利用类活化图(CAM)来定位对象区域。然而，由分类网络生成的响应图通常会专注于特征性的对象部分。在这篇论文中，我们提出了一种新的协同匹配网络（COMNet），可以提高CAM的质量并使网络对整个对象部分进行注意。具体来说，我们在含共同类的图像对进行交叉匹配以强制相应区域的匹配，并在单个图像上进行内部匹配以传播对象区域中的semantic特征。实验表明，我们的网络可以有效提高基eline模型的性能并实现新的状态对抗性表现。
</details></li>
</ul>
<hr>
<h2 id="Model2Scene-Learning-3D-Scene-Representation-via-Contrastive-Language-CAD-Models-Pre-training"><a href="#Model2Scene-Learning-3D-Scene-Representation-via-Contrastive-Language-CAD-Models-Pre-training" class="headerlink" title="Model2Scene: Learning 3D Scene Representation via Contrastive Language-CAD Models Pre-training"></a>Model2Scene: Learning 3D Scene Representation via Contrastive Language-CAD Models Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16956">http://arxiv.org/abs/2309.16956</a></li>
<li>repo_url: None</li>
<li>paper_authors: Runnan Chen, Xinge Zhu, Nenglun Chen, Dawei Wang, Wei Li, Yuexin Ma, Ruigang Yang, Tongliang Liu, Wenping Wang</li>
<li>for: 本研究目的是学习免需大量标注点云的3D场景识别，通过从计算机支持设计（CAD）模型和语言学习3D场景表示。</li>
<li>methods: 我们提出了Model2Scene模型，它首先在CAD模型中随机混合数据，然后使用深度几何体系减少领域差，最后通过语言编码和点特征强制对齐来预训练3D网络。</li>
<li>results: 我们的Model2Scene模型可以在无标签3D物体突出检测、efficient 3D场景识别和零shot 3Dsemantic segmentation等下游任务中提供优秀表现，特别是在ScanNet和S3DIS数据集上得到了46.08%和55.49%的平均精度。<details>
<summary>Abstract</summary>
Current successful methods of 3D scene perception rely on the large-scale annotated point cloud, which is tedious and expensive to acquire. In this paper, we propose Model2Scene, a novel paradigm that learns free 3D scene representation from Computer-Aided Design (CAD) models and languages. The main challenges are the domain gaps between the CAD models and the real scene's objects, including model-to-scene (from a single model to the scene) and synthetic-to-real (from synthetic model to real scene's object). To handle the above challenges, Model2Scene first simulates a crowded scene by mixing data-augmented CAD models. Next, we propose a novel feature regularization operation, termed Deep Convex-hull Regularization (DCR), to project point features into a unified convex hull space, reducing the domain gap. Ultimately, we impose contrastive loss on language embedding and the point features of CAD models to pre-train the 3D network. Extensive experiments verify the learned 3D scene representation is beneficial for various downstream tasks, including label-free 3D object salient detection, label-efficient 3D scene perception and zero-shot 3D semantic segmentation. Notably, Model2Scene yields impressive label-free 3D object salient detection with an average mAP of 46.08\% and 55.49\% on the ScanNet and S3DIS datasets, respectively. The code will be publicly available.
</details>
<details>
<summary>摘要</summary>
当前成功的3D场景识别方法都基于大规模标注的点云数据，但这些数据是费时和成本高的获得的。在这篇论文中，我们提出了Model2Scene，一种新的思路，它可以自由地学习3D场景表示从计算机支持设计（CAD）模型和语言中。主要挑战是从CAD模型到场景中的对象之间的领域差异，包括模型到场景（从单个模型到场景）和synthetic-to-real（从 sintetic模型到实际场景中的对象）。为了解决以上挑战，Model2Scene首先将CAD模型混合数据进行加工，然后我们提出了一种新的特征规范操作，称为深度凹陷规范（DCR），用于将点特征 проек到一个统一的凹陷空间，从而减少领域差异。最后，我们对语言表示和CAD模型的点特征进行偏置损失，以预训练3D网络。广泛的实验表明，学习的3D场景表示对下游任务具有很好的效果，包括无标签3D对象突出检测、标签有效3D场景识别和零标签3Dsemantic分割。特别是，Model2Scene在无标签3D对象突出检测任务中取得了非常出色的平均精度（mAP）46.08%和55.49%，分别在ScanNet和S3DIS datasets上。代码将公开。
</details></li>
</ul>
<hr>
<h2 id="CrossZoom-Simultaneously-Motion-Deblurring-and-Event-Super-Resolving"><a href="#CrossZoom-Simultaneously-Motion-Deblurring-and-Event-Super-Resolving" class="headerlink" title="CrossZoom: Simultaneously Motion Deblurring and Event Super-Resolving"></a>CrossZoom: Simultaneously Motion Deblurring and Event Super-Resolving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16949">http://arxiv.org/abs/2309.16949</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bestrivenzc/CZ-Net">https://github.com/bestrivenzc/CZ-Net</a></li>
<li>paper_authors: Chi Zhang, Xiang Zhang, Mingyuan Lin, Cheng Li, Chu He, Wen Yang, Gui-Song Xia, Lei Yu<br>for:This paper aims to improve the performance of frame-event based vision applications by bridging the resolution gap between traditional and neuromorphic event cameras.methods:The proposed method, called CrossZoom, uses a novel unified neural network (CZ-Net) to jointly recover sharp latent sequences and high-resolution events from blurry inputs. The method leverages scale-variant properties and effectively fuses cross-modality information to achieve cross-enhancement.results:Extensive experiments on synthetic and real-world datasets demonstrate the effectiveness and robustness of the proposed method. The method can improve the temporal resolution of images and the spatial resolution of events, leading to better performance in frame-event based vision applications.<details>
<summary>Abstract</summary>
Even though the collaboration between traditional and neuromorphic event cameras brings prosperity to frame-event based vision applications, the performance is still confined by the resolution gap crossing two modalities in both spatial and temporal domains. This paper is devoted to bridging the gap by increasing the temporal resolution for images, i.e., motion deblurring, and the spatial resolution for events, i.e., event super-resolving, respectively. To this end, we introduce CrossZoom, a novel unified neural Network (CZ-Net) to jointly recover sharp latent sequences within the exposure period of a blurry input and the corresponding High-Resolution (HR) events. Specifically, we present a multi-scale blur-event fusion architecture that leverages the scale-variant properties and effectively fuses cross-modality information to achieve cross-enhancement. Attention-based adaptive enhancement and cross-interaction prediction modules are devised to alleviate the distortions inherent in Low-Resolution (LR) events and enhance the final results through the prior blur-event complementary information. Furthermore, we propose a new dataset containing HR sharp-blurry images and the corresponding HR-LR event streams to facilitate future research. Extensive qualitative and quantitative experiments on synthetic and real-world datasets demonstrate the effectiveness and robustness of the proposed method. Codes and datasets are released at https://bestrivenzc.github.io/CZ-Net/.
</details>
<details>
<summary>摘要</summary>
即使 tradicional 和 neuromorphic event camera 的合作带来了 frame-event 基于视觉应用程序的繁荣，性能仍然受到两种模式之间的分辨率差距的限制。这篇论文旨在跨过这一差距，通过提高图像的时间分辨率（即运动抖抖）和事件的空间分辨率（即事件超分辨）来bridging this gap。为此，我们介绍了 CrossZoom，一种新的协调 neural Network（CZ-Net），可以同时恢复锐化时间序列中的输入和高分辨度（HR）事件。具体来说，我们提出了一种多尺度抖事件融合架构，利用模式的尺度variant property，以实现跨模态信息的有效融合。我们还设计了注意力机制和交互预测模块，以解决LR事件中的自然偏差，并使得最终结果得到了预先抖事件补偿的信息的帮助。此外，我们提出了一个新的数据集，包括高分辨度锐化图像和对应的高分辨度-低分辨度事件流，以便未来的研究。我们的实验结果表明，我们的方法效果和稳定。codes和数据集可以在https://bestrivenzc.github.io/CZ-Net/ 获取。
</details></li>
</ul>
<hr>
<h2 id="Incremental-Rotation-Averaging-Revisited-and-More-A-New-Rotation-Averaging-Benchmark"><a href="#Incremental-Rotation-Averaging-Revisited-and-More-A-New-Rotation-Averaging-Benchmark" class="headerlink" title="Incremental Rotation Averaging Revisited and More: A New Rotation Averaging Benchmark"></a>Incremental Rotation Averaging Revisited and More: A New Rotation Averaging Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16924">http://arxiv.org/abs/2309.16924</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Gao, Hainan Cui, Shuhan Shen</li>
<li>for: 提高渐进参数估计基于旋转平均方法的精度和稳定性</li>
<li>methods: 引入IRAv4方法，使用任务特定相关联顺序设置为更可靠和准确的参照系</li>
<li>results: 比较IRAv4方法与其他主流旋转平均方法的性能表现， demonstate IRAv4方法的效果<details>
<summary>Abstract</summary>
In order to further advance the accuracy and robustness of the incremental parameter estimation-based rotation averaging methods, in this paper, a new member of the Incremental Rotation Averaging (IRA) family is introduced, which is termed as IRAv4. As the most significant feature of the IRAv4, a task-specific connected dominating set is extracted to serve as a more reliable and accurate reference for rotation global alignment. In addition, to further address the limitations of the existing rotation averaging benchmark of relying on the slightly outdated Bundler camera calibration results as ground truths and focusing solely on rotation estimation accuracy, this paper presents a new COLMAP-based rotation averaging benchmark that incorporates a cross check between COLMAP and Bundler, and employ the accuracy of both rotation and downstream location estimation as evaluation metrics, which is desired to provide a more reliable and comprehensive evaluation tool for the rotation averaging research. Comprehensive comparisons between the proposed IRAv4 and other mainstream rotation averaging methods on this new benchmark demonstrate the effectiveness of our proposed approach.
</details>
<details>
<summary>摘要</summary>
为了进一步提高幂加计数器估计基于旋转均值方法的准确性和可靠性，本文提出了一新的幂加计数器均值方法（IRAv4）。这个方法的最重要特点是从任务特定的连接dominating set中提取一个更可靠和准确的旋转全局参照。此外，为了进一步解决现有的旋转均值标准 benchmark 的局限性，这篇文章提出了一个基于 COLMAP 的新的旋转均值标准 benchmark，该标准具有跨Check между COLMAP 和 Bundler，并使用两者的准确性作为评价指标。这种新的评价工具被期望能够提供更可靠和全面的评价工具 для旋转均值研究。对于提出的 IRAv4 和其他主流的旋转均值方法进行了广泛的比较，实验结果表明了我们提出的方法的效果。
</details></li>
</ul>
<hr>
<h2 id="YOLOR-Based-Multi-Task-Learning"><a href="#YOLOR-Based-Multi-Task-Learning" class="headerlink" title="YOLOR-Based Multi-Task Learning"></a>YOLOR-Based Multi-Task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16921">http://arxiv.org/abs/2309.16921</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/WongKinYiu/yolor">https://github.com/WongKinYiu/yolor</a></li>
<li>paper_authors: Hung-Shuo Chang, Chien-Yao Wang, Richard Robert Wang, Gene Chou, Hong-Yuan Mark Liao</li>
<li>for: 这篇论文目的是学习多个任务，使用单个模型，并且共同提高所有任务的泛化和共同 semantics。</li>
<li>methods: 这篇论文提出了基于 You Only Learn One Representation (YOLOR) 网络架构的方法，该架构特点是结合显式和隐式知识，从数据观察和学习的 latents 来提高共享表示。</li>
<li>results: 该方法可以同时进行对象检测、实例 segmentation、semantic segmentation和图像描述，并且可以保持低参数计数和不需要预训练。<details>
<summary>Abstract</summary>
Multi-task learning (MTL) aims to learn multiple tasks using a single model and jointly improve all of them assuming generalization and shared semantics. Reducing conflicts between tasks during joint learning is difficult and generally requires careful network design and extremely large models. We propose building on You Only Learn One Representation (YOLOR), a network architecture specifically designed for multitasking. YOLOR leverages both explicit and implicit knowledge, from data observations and learned latents, respectively, to improve a shared representation while minimizing the number of training parameters. However, YOLOR and its follow-up, YOLOv7, only trained two tasks at once. In this paper, we jointly train object detection, instance segmentation, semantic segmentation, and image captioning. We analyze tradeoffs and attempt to maximize sharing of semantic information. Through our architecture and training strategies, we find that our method achieves competitive performance on all tasks while maintaining a low parameter count and without any pre-training. We will release code soon.
</details>
<details>
<summary>摘要</summary>
多任务学习（MTL）目标是使用单个模型学习多个任务，并共同提高所有任务的泛化和共享含义。在共同学习过程中避免任务之间冲突是困难的，通常需要非常精心设计网络和庞大的模型。我们提出基于你仅学习一个表示（YOLOR）网络架构，该架构特点是通过数据观察和学习含义来提高共享表示，同时尽量降低训练参数数量。然而，YOLOR和其后续的YOLOv7只是同时训练了两个任务。在这篇论文中，我们同时训练对象检测、实例分割、semantic segmentation和图文描述。我们分析了贸易和共享含义的权衡，并尝试 Maximize shared semantic information。通过我们的架构和训练策略，我们发现我们的方法在所有任务上具有竞争性的性能，同时保持低参数数量，无需预训练。我们即将发布代码。
</details></li>
</ul>
<hr>
<h2 id="Investigating-Shift-Equivalence-of-Convolutional-Neural-Networks-in-Industrial-Defect-Segmentation"><a href="#Investigating-Shift-Equivalence-of-Convolutional-Neural-Networks-in-Industrial-Defect-Segmentation" class="headerlink" title="Investigating Shift Equivalence of Convolutional Neural Networks in Industrial Defect Segmentation"></a>Investigating Shift Equivalence of Convolutional Neural Networks in Industrial Defect Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16902">http://arxiv.org/abs/2309.16902</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiaozhen228/caps">https://github.com/xiaozhen228/caps</a></li>
<li>paper_authors: Zhen Qu, Xian Tao, Fei Shen, Zhengtao Zhang, Tao Li</li>
<li>for: 这个研究是为了解决产业瑕疵分类任务中的一个问题，即模型的出力一致性（也就是等效性）不足。</li>
<li>methods: 这篇研究提出了一个新的分割层组合，即分配注意力多项样本抽取（CAPS），并将其替换到传统的抽取层组合中。此外，还提出了一个适应窗口模组来适应图像边界变化，以及一个 ком成果模组来融合所有下推的特征以改善分类性能。</li>
<li>results: 实验结果显示，提案的方法在微表面瑕疵（MSD）数据集和四个实际的产业瑕疵数据集上具有更高的出力一致性和分类性能，较前state-of-the-art方法更好。<details>
<summary>Abstract</summary>
In industrial defect segmentation tasks, while pixel accuracy and Intersection over Union (IoU) are commonly employed metrics to assess segmentation performance, the output consistency (also referred to equivalence) of the model is often overlooked. Even a small shift in the input image can yield significant fluctuations in the segmentation results. Existing methodologies primarily focus on data augmentation or anti-aliasing to enhance the network's robustness against translational transformations, but their shift equivalence performs poorly on the test set or is susceptible to nonlinear activation functions. Additionally, the variations in boundaries resulting from the translation of input images are consistently disregarded, thus imposing further limitations on the shift equivalence. In response to this particular challenge, a novel pair of down/upsampling layers called component attention polyphase sampling (CAPS) is proposed as a replacement for the conventional sampling layers in CNNs. To mitigate the effect of image boundary variations on the equivalence, an adaptive windowing module is designed in CAPS to adaptively filter out the border pixels of the image. Furthermore, a component attention module is proposed to fuse all downsampled features to improve the segmentation performance. The experimental results on the micro surface defect (MSD) dataset and four real-world industrial defect datasets demonstrate that the proposed method exhibits higher equivalence and segmentation performance compared to other state-of-the-art methods.Our code will be available at https://github.com/xiaozhen228/CAPS.
</details>
<details>
<summary>摘要</summary>
在工业缺陷分割任务中，像素准确率和交集覆盖率（IoU）是常用的评估分割性能的指标，但模型的输出一致性（也称Equivalence）通常被忽略。即使只有小的输入图像shift，分割结果也可能出现显著的变化。现有方法主要通过数据扩展或反馈抑制来提高网络对平移变换的Robustness，但它们的平移Equivalence在测试集上表现不佳或受非线性活化函数的影响。此外，输入图像的边界变化会导致分割结果的变化，从而增加了平移Equivalence的限制。为解决这个问题，一种新的下/上采样层组合called component attention polyphase sampling（CAPS）被提出，以取代传统的采样层在CNN中。为了减少图像边界变化对Equivalence的影响，CAPS中的适应窗口模块可以逐个窗口过滤出图像边界的像素。此外，一种组件注意模块被提出，以将所有下采样的特征进行混合，以提高分割性能。实验结果表明，提出的方法在微表面缺陷（MSD） dataset和四个实际工业缺陷dataset上 exhibits higher Equivalence and segmentation performance compared to other state-of-the-art methods。我们的代码将在https://github.com/xiaozhen228/CAPS上提供。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/29/cs.CV_2023_09_29/" data-id="clp869txy00khk58802oq05rc" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/37/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/36/">36</a><a class="page-number" href="/page/37/">37</a><span class="page-number current">38</span><a class="page-number" href="/page/39/">39</a><a class="page-number" href="/page/40/">40</a><span class="space">&hellip;</span><a class="page-number" href="/page/97/">97</a><a class="extend next" rel="next" href="/page/39/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">66</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">81</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">140</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
