
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/38/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.CL_2023_09_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/19/cs.CL_2023_09_19/" class="article-date">
  <time datetime="2023-09-19T11:00:00.000Z" itemprop="datePublished">2023-09-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/19/cs.CL_2023_09_19/">cs.CL - 2023-09-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="MBR-and-QE-Finetuning-Training-time-Distillation-of-the-Best-and-Most-Expensive-Decoding-Methods"><a href="#MBR-and-QE-Finetuning-Training-time-Distillation-of-the-Best-and-Most-Expensive-Decoding-Methods" class="headerlink" title="MBR and QE Finetuning: Training-time Distillation of the Best and Most Expensive Decoding Methods"></a>MBR and QE Finetuning: Training-time Distillation of the Best and Most Expensive Decoding Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10966">http://arxiv.org/abs/2309.10966</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mara Finkelstein, Markus Freitag</li>
<li>for: 提高NLG任务中模型的质量和效率</li>
<li>methods: 使用MBRfinetuning和QEfinetuning方法，在训练时使用高质量的核心模型，在推断时使用高效的推断算法</li>
<li>results: 在Neural Machine Translation任务上，使用这些finetuning方法可以大幅提高模型的质量和效率，并且在使用外部LLM作为教师模型时，还能超越使用人工生成的参考数据<details>
<summary>Abstract</summary>
Recent research in decoding methods for Natural Language Generation (NLG) tasks has shown that the traditional beam search and greedy decoding algorithms are not optimal, because model probabilities do not always align with human preferences. Stronger decoding methods, including Quality Estimation (QE) reranking and Minimum Bayes' Risk (MBR) decoding, have since been proposed to mitigate the model-perplexity-vs-quality mismatch. While these decoding methods achieve state-of-the-art performance, they are prohibitively expensive to compute. In this work, we propose MBR finetuning and QE finetuning which distill the quality gains from these decoding methods at training time, while using an efficient decoding algorithm at inference time. Using the canonical NLG task of Neural Machine Translation (NMT), we show that even with self-training, these finetuning methods significantly outperform the base model. Moreover, when using an external LLM as a teacher model, these finetuning methods outperform finetuning on human-generated references. These findings suggest new ways to leverage monolingual data to achieve improvements in model quality that are on par with, or even exceed, improvements from human-curated data, while maintaining maximum efficiency during decoding.
</details>
<details>
<summary>摘要</summary>
Note:* "排序" (bǎo xiǎng) instead of "sorting"* "搜索" (sōu sòu) instead of "search"* "评估" (píng gòu) instead of "assessment"* "参考" (xiǎng gǎng) instead of "reference"* "模型" (módel) instead of "model"* "语言生成" (yǔ yán shēng chéng) instead of "Natural Language Generation"* "神经机器翻译" (shén qiān jī qì zhōng yì) instead of "Neural Machine Translation"
</details></li>
</ul>
<hr>
<h2 id="In-Context-Learning-for-Text-Classification-with-Many-Labels"><a href="#In-Context-Learning-for-Text-Classification-with-Many-Labels" class="headerlink" title="In-Context Learning for Text Classification with Many Labels"></a>In-Context Learning for Text Classification with Many Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10954">http://arxiv.org/abs/2309.10954</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dia2018/What-is-the-Difference-Between-AI-and-Machine-Learning">https://github.com/dia2018/What-is-the-Difference-Between-AI-and-Machine-Learning</a></li>
<li>paper_authors: Aristides Milios, Siva Reddy, Dzmitry Bahdanau</li>
<li>for: 本研究使用大型自然语言模型进行含义学习（ICL）任务，尤其是多个标签任务，因为限定的上下文窗口尺度使得不能填充足量的示例。</li>
<li>methods: 我们使用预训练的稠密检索模型，只给模型每个推理调用一个部分视图全标签空间。我们使用最新的开源LLMs（OPT、LLaMA）进行测试，并在几个常见的意图分类数据集上设置新的状态码表现。</li>
<li>results: 我们发现，随着不同的模型缩度和数量，更大的模型可以更好地利用更大的上下文长度进行ICL。我们运行了多个简化，分析了模型对：a) 输入中的相似示例和当前输入之间的相似度，b) 类名的Semantic内容，c) 示例和标签之间的正确匹配。我们发现，这三种因素在不同的领域中具有不同的重要性。<details>
<summary>Abstract</summary>
In-context learning (ICL) using large language models for tasks with many labels is challenging due to the limited context window, which makes it difficult to fit a sufficient number of examples in the prompt. In this paper, we use a pre-trained dense retrieval model to bypass this limitation, giving the model only a partial view of the full label space for each inference call. Testing with recent open-source LLMs (OPT, LLaMA), we set new state of the art performance in few-shot settings for three common intent classification datasets, with no finetuning. We also surpass fine-tuned performance on fine-grained sentiment classification in certain cases. We analyze the performance across number of in-context examples and different model scales, showing that larger models are necessary to effectively and consistently make use of larger context lengths for ICL. By running several ablations, we analyze the model's use of: a) the similarity of the in-context examples to the current input, b) the semantic content of the class names, and c) the correct correspondence between examples and labels. We demonstrate that all three are needed to varying degrees depending on the domain, contrary to certain recent works.
</details>
<details>
<summary>摘要</summary>
内容学习（ICL）使用大型语言模型进行多个标签任务是具有限制的上下文窗口，即对每次推寄都只能提供有限的示例。在这篇论文中，我们使用预训练的稠密检索模型，以快速地跳过这个限制，并只给模型提供部分视图全个标签空间。通过测试最新的开源LLMs（OPT、LLaMA），我们在少量示例设置下设置了新的国际标准性能，无需训练。我们还超过了训练后的性能在细化情感分类中，在某些情况下。我们分析了不同的示例数量和模型缩放大小对性能的影响，发现大型模型是需要充分利用更大的上下文长度进行ICL。通过多个缺省分析，我们分析了模型在不同的领域中使用：a）相关的示例和当前输入之间的相似性，b）类名中的semantic内容，以及c）正确地将示例和标签相匹配。我们发现，这三者在不同的领域中均需要不同的程度，与某些最近的工作不同。
</details></li>
</ul>
<hr>
<h2 id="A-Family-of-Pretrained-Transformer-Language-Models-for-Russian"><a href="#A-Family-of-Pretrained-Transformer-Language-Models-for-Russian" class="headerlink" title="A Family of Pretrained Transformer Language Models for Russian"></a>A Family of Pretrained Transformer Language Models for Russian</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10931">http://arxiv.org/abs/2309.10931</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dmitry Zmitrovich, Alexander Abramov, Andrey Kalmykov, Maria Tikhonova, Ekaterina Taktasheva, Danil Astafurov, Mark Baushenko, Artem Snegirev, Tatiana Shavrina, Sergey Markov, Vladislav Mikhailov, Alena Fenogenova</li>
<li>for: 本研究旨在开发特性化的Transformer语言模型，用于俄语自然语言理解和生成。</li>
<li>methods: 本文使用encoder（ruBERT、ruRoBERTa、ruELECTRA）、decoder（ruGPT-3）和encoder-decoder（ruT5、FRED-T5）多种Transformer模型，并对其进行预训练和测试。</li>
<li>results: 研究人员通过对俄语自然语言理解和生成数据集和标准做测试，发现这些特性化Transformer模型具有良好的普适能力和生成能力。<details>
<summary>Abstract</summary>
Nowadays, Transformer language models (LMs) represent a fundamental component of the NLP research methodologies and applications. However, the development of such models specifically for the Russian language has received little attention. This paper presents a collection of 13 Russian Transformer LMs based on the encoder (ruBERT, ruRoBERTa, ruELECTRA), decoder (ruGPT-3), and encoder-decoder (ruT5, FRED-T5) models in multiple sizes. Access to these models is readily available via the HuggingFace platform. We provide a report of the model architecture design and pretraining, and the results of evaluating their generalization abilities on Russian natural language understanding and generation datasets and benchmarks. By pretraining and releasing these specialized Transformer LMs, we hope to broaden the scope of the NLP research directions and enable the development of industrial solutions for the Russian language.
</details>
<details>
<summary>摘要</summary>
现在， transformer 语言模型（LMs）成为了自然语言处理（NLP）研究方法和应用的基本组成部分。然而，为俄语语言的特有Transformer LMs的开发受到了相对少的关注。本文介绍了13种俄语 transformer LMs，包括encoder（ruBERT、ruRoBERTa、ruELECTRA）、decoder（ruGPT-3）和encoder-decoder（ruT5、FRED-T5）模型，以及这些模型的训练和预训练方法。通过这些特化的Transformer LMs，我们希望拓宽NLP研究方向和提供俄语语言industrial解决方案。这些模型通过HuggingFace平台可以访问。我们还提供了模型体系设计和预训练方法的报告，以及在俄语自然语言理解和生成数据集和benchmark上模型的一般化能力的评价结果。
</details></li>
</ul>
<hr>
<h2 id="Specializing-Small-Language-Models-towards-Complex-Style-Transfer-via-Latent-Attribute-Pre-Training"><a href="#Specializing-Small-Language-Models-towards-Complex-Style-Transfer-via-Latent-Attribute-Pre-Training" class="headerlink" title="Specializing Small Language Models towards Complex Style Transfer via Latent Attribute Pre-Training"></a>Specializing Small Language Models towards Complex Style Transfer via Latent Attribute Pre-Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10929">http://arxiv.org/abs/2309.10929</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ruiqixu37/BTTS_ECAI2023">https://github.com/ruiqixu37/BTTS_ECAI2023</a></li>
<li>paper_authors: Ruiqi Xu, Yongfeng Huang, Xin Chen, Lin Zhang</li>
<li>for: 这项研究旨在介绍复杂文本风格传递任务，并基于两个广泛适用的场景构建了复杂文本数据集。</li>
<li>methods: 我们使用了小型模型（ Less than T5-3B）和隐式风格预训练through contrastive learning来解决大型模型（LLM）的数据隐私问题、网络不稳定和高部署成本。</li>
<li>results: 我们的方法比现有方法更有效，可以在几个shot中完成文本风格传递任务，并且可以自动评估文本生成质量基于人工评估使用ChatGPT。<details>
<summary>Abstract</summary>
In this work, we introduce the concept of complex text style transfer tasks, and constructed complex text datasets based on two widely applicable scenarios. Our dataset is the first large-scale data set of its kind, with 700 rephrased sentences and 1,000 sentences from the game Genshin Impact. While large language models (LLM) have shown promise in complex text style transfer, they have drawbacks such as data privacy concerns, network instability, and high deployment costs. To address these issues, we explore the effectiveness of small models (less than T5-3B) with implicit style pre-training through contrastive learning. We also propose a method for automated evaluation of text generation quality based on alignment with human evaluations using ChatGPT. Finally, we compare our approach with existing methods and show that our model achieves state-of-art performances of few-shot text style transfer models.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们介绍了复杂文本风格传递任务的概念，并基于两个广泛适用的情况构建了复杂文本数据集。我们的数据集是首个类似类型的大规模数据集，包含700个重叠句子和1,000个《神韵碰》游戏中的句子。虽然大型语言模型（LLM）在复杂文本风格传递方面表现出了承诺，但它们具有数据隐私问题、网络不稳定和高部署成本的缺点。为了解决这些问题，我们研究了小型模型（ Less than T5-3B）的隐式风格预训练through contrastive learning的效果。我们还提出了一种自动评估文本生成质量的方法，基于人类评估和ChatGPT的匹配。最后，我们与现有方法进行比较，并证明我们的模型在几架text风格传递模型中达到了状态之最。
</details></li>
</ul>
<hr>
<h2 id="Semi-Autoregressive-Streaming-ASR-With-Label-Context"><a href="#Semi-Autoregressive-Streaming-ASR-With-Label-Context" class="headerlink" title="Semi-Autoregressive Streaming ASR With Label Context"></a>Semi-Autoregressive Streaming ASR With Label Context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10926">http://arxiv.org/abs/2309.10926</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddhant Arora, George Saon, Shinji Watanabe, Brian Kingsbury</li>
<li>for: 这个论文是为了提高流式自动语音识别（ASR）模型的准确率和延迟时间而设计的。</li>
<li>methods: 这个论文使用了一种名为“半自动生成”的ASR模型，该模型将以前块中的标签作为额外Context，使用语言模型（LM）子网络来提高流式ASR的准确率。它还提出了一种新的滥货解码算法，可以在块边界附近减少插入和删除错误，而不是 significatively增加推理时间。</li>
<li>results: 实验表明，我们的方法可以与现有的流式非 autoregressive（NAR）模型相比，提高流式ASR的准确率。具体来说，在Tedlium2上，我们的方法提高了19%的相对准确率；在Librispeech-100的清洁&#x2F;其他测试集上，提高了16%&#x2F;8%的相对准确率；在Switchboard（SWB）&#x2F; Callhome（CH）测试集上，提高了19%&#x2F;8%的相对准确率。此外，我们的方法可以更好地利用外部文本数据进行预训练LM子网络，进一步提高流式ASR的准确率。<details>
<summary>Abstract</summary>
Non-autoregressive (NAR) modeling has gained significant interest in speech processing since these models achieve dramatically lower inference time than autoregressive (AR) models while also achieving good transcription accuracy. Since NAR automatic speech recognition (ASR) models must wait for the completion of the entire utterance before processing, some works explore streaming NAR models based on blockwise attention for low-latency applications. However, streaming NAR models significantly lag in accuracy compared to streaming AR and non-streaming NAR models. To address this, we propose a streaming "semi-autoregressive" ASR model that incorporates the labels emitted in previous blocks as additional context using a Language Model (LM) subnetwork. We also introduce a novel greedy decoding algorithm that addresses insertion and deletion errors near block boundaries while not significantly increasing the inference time. Experiments show that our method outperforms the existing streaming NAR model by 19% relative on Tedlium2, 16%/8% on Librispeech-100 clean/other test sets, and 19%/8% on the Switchboard(SWB) / Callhome(CH) test sets. It also reduced the accuracy gap with streaming AR and non-streaming NAR models while achieving 2.5x lower latency. We also demonstrate that our approach can effectively utilize external text data to pre-train the LM subnetwork to further improve streaming ASR accuracy.
</details>
<details>
<summary>摘要</summary>
非自适应（NAR）模型在语音处理领域已经吸引了广泛的关注，因为这些模型在推理时间方面可以达到AR模型的多倍速度，而且也可以达到良好的识别精度。然而，NAR自动语音识别（ASR）模型必须等待整个句子的完成才能进行处理，因此一些研究者开发了基于块级注意力的流式NAR模型，以满足低延迟应用场景。然而，流式NAR模型与流式AR和非流式NAR模型的准确率存在明显的差距。为了解决这个问题，我们提出了一种流式"半自适应" ASR模型，该模型利用在前一个块中生成的标签作为额外Context使用语言模型（LM）子网络。我们还提出了一种新的恰好解oding算法，该算法可以在块边界附近快速地修复插入和删除错误，而不是增加推理时间。实验显示，我们的方法在Tedlium2、Librispeech-100清洁/其他测试集和Switchboard（SWB）/ Callhome（CH）测试集上比既有的流式NAR模型提高19%的相对性能，同时也降低了与流式AR和非流式NAR模型之间的准确率差距。此外，我们还证明了我们的方法可以有效地利用外部文本数据来预训练LM子网络，以进一步提高流式ASR准确率。
</details></li>
</ul>
<hr>
<h2 id="Semi-automatic-staging-area-for-high-quality-structured-data-extraction-from-scientific-literature"><a href="#Semi-automatic-staging-area-for-high-quality-structured-data-extraction-from-scientific-literature" class="headerlink" title="Semi-automatic staging area for high-quality structured data extraction from scientific literature"></a>Semi-automatic staging area for high-quality structured data extraction from scientific literature</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10923">http://arxiv.org/abs/2309.10923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Foppiano, Tomoya Mato, Kensei Terashima, Pedro Ortiz Suarez, Taku Tou, Chikako Sakai, Wei-Sheng Wang, Toshiyuki Amagasa, Yoshihiko Takano, Masashi Ishii</li>
<li>for: 提高 SuperCon 中新型超导体实验数据的更新效率，同时维持或提高数据质量。</li>
<li>methods: 使用自动和手动工作流程在抽取的数据库中实现半自动的stage区。使用异常检测自动过程预先审核收集的数据，并让用户通过专门设计的用户界面对原PDF文档进行数据验证。收集修复后的记录，并将其作为机器学习模型的训练数据使用。</li>
<li>results: 评估实验表明，我们的stage区可以显著提高审核质量。与传统手动方法（读取PDF文档并记录信息在Excel文档中）相比，使用界面提高精度和准确率分别提高6%和50%，平均提高40%的F1分数。<details>
<summary>Abstract</summary>
In this study, we propose a staging area for ingesting new superconductors' experimental data in SuperCon that is machine-collected from scientific articles. Our objective is to enhance the efficiency of updating SuperCon while maintaining or enhancing the data quality. We present a semi-automatic staging area driven by a workflow combining automatic and manual processes on the extracted database. An anomaly detection automatic process aims to pre-screen the collected data. Users can then manually correct any errors through a user interface tailored to simplify the data verification on the original PDF documents. Additionally, when a record is corrected, its raw data is collected and utilised to improve machine learning models as training data. Evaluation experiments demonstrate that our staging area significantly improves curation quality. We compare the interface with the traditional manual approach of reading PDF documents and recording information in an Excel document. Using the interface boosts the precision and recall by 6% and 50%, respectively to an average increase of 40% in F1-score.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们提出了一个用于新超导材料实验数据的投入区域，这个区域使用机器自动从科学文献中收集数据。我们的目标是提高超кон的更新效率，同时保持或提高数据质量。我们提出了一种半自动的投入区域，该区域采用工作流程结合自动和手动过程来处理提取的数据库。一个异常检测自动过程用于预先屏选收集的数据。用户可以通过一个专门设计的用户界面来手动修正任何错误。此外，当记录被修正时，其原始数据会被收集并用于改进机器学习模型的训练数据。我们的评估实验表明，我们的投入区域可以显著提高筛选质量。我们与传统的手动方法相比，使用界面可以提高准确率和敏感度分别提高6%和50%，平均提高40%的F1分数。
</details></li>
</ul>
<hr>
<h2 id="What-Learned-Representations-and-Influence-Functions-Can-Tell-Us-About-Adversarial-Examples"><a href="#What-Learned-Representations-and-Influence-Functions-Can-Tell-Us-About-Adversarial-Examples" class="headerlink" title="What Learned Representations and Influence Functions Can Tell Us About Adversarial Examples"></a>What Learned Representations and Influence Functions Can Tell Us About Adversarial Examples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10916">http://arxiv.org/abs/2309.10916</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sjabin/nnif">https://github.com/sjabin/nnif</a></li>
<li>paper_authors: Shakila Mahjabin Tonni, Mark Dras</li>
<li>for: 这篇论文主要是为了研究在自然语言处理（NLP）中的对抗例（adversarial examples）的检测方法。</li>
<li>methods: 这篇论文使用了两种方法来检测对抗例：一种是基于最近邻居和影响函数，另一种是基于马ха拉欧斯距离。</li>
<li>results: 研究发现，使用基于最近邻居和影响函数的方法可以制定出state-of-the-art的检测器，而且这种方法还提供了对于NLP任务的对抗例subspace的新的理解和对比。<details>
<summary>Abstract</summary>
Adversarial examples, deliberately crafted using small perturbations to fool deep neural networks, were first studied in image processing and more recently in NLP. While approaches to detecting adversarial examples in NLP have largely relied on search over input perturbations, image processing has seen a range of techniques that aim to characterise adversarial subspaces over the learned representations.   In this paper, we adapt two such approaches to NLP, one based on nearest neighbors and influence functions and one on Mahalanobis distances. The former in particular produces a state-of-the-art detector when compared against several strong baselines; moreover, the novel use of influence functions provides insight into how the nature of adversarial example subspaces in NLP relate to those in image processing, and also how they differ depending on the kind of NLP task.
</details>
<details>
<summary>摘要</summary>
adversarial examples, 通过小变化而被意外地骗响深度神经网络，在图像处理领域首先被研究，然后在自然语言处理（NLP）中被研究。在NLP中，检测 adversarial examples 的方法主要基于输入变换的搜索，而图像处理领域则有许多技术来描述恶作剂的表示空间。  在这篇论文中，我们采用了两种方法来检测 adversarial examples，一种是基于最近邻居和影响函数，另一种是基于 Mahalanobis 距离。前者在比较多个强基准下表现出状态顶峰的检测器，而且使用影响函数提供了关于恶作剂表示空间在 NLP 和图像处理之间的相似性，以及哪些因素使得恶作剂表示空间在不同的 NLP 任务中有所不同。
</details></li>
</ul>
<hr>
<h2 id="RedPenNet-for-Grammatical-Error-Correction-Outputs-to-Tokens-Attentions-to-Spans"><a href="#RedPenNet-for-Grammatical-Error-Correction-Outputs-to-Tokens-Attentions-to-Spans" class="headerlink" title="RedPenNet for Grammatical Error Correction: Outputs to Tokens, Attentions to Spans"></a>RedPenNet for Grammatical Error Correction: Outputs to Tokens, Attentions to Spans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10898">http://arxiv.org/abs/2309.10898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bohdan Didenko, Andrii Sameliuk</li>
<li>For: This paper is written for the UNLP 2023 workshop, specifically for the Shared Task in Grammatical Error Correction (GEC) for Ukrainian.* Methods: The paper uses a RedPenNet approach to address text editing tasks, which combines sequence-to-sequence and sequence tagging techniques.* Results: The paper achieves $F_{0.5}$ scores of 77.60 on the BEA-2019 (test) and 67.71 on the UAGEC+Fluency (test) benchmarks, which are considered state-of-the-art results.Here is the simplified Chinese text for the three key points:* For: 这篇论文是为UNLP 2023 工作坊写的，特意是为 grammatical error correction (GEC) 的 Ukrainian 语言共同任务。* Methods: 这篇论文使用 RedPenNet 方法来处理文本编辑任务，这种方法结合了 sequence-to-sequence 和 sequence tagging 技术。* Results: 这篇论文在 BEA-2019 测试集上 achieve $F_{0.5}$ 分数为 77.60，并在 UAGEC+Fluency 测试集上 achieve 67.71 分数，这些结果被视为当前领域的 state-of-the-art 成果。<details>
<summary>Abstract</summary>
The text editing tasks, including sentence fusion, sentence splitting and rephrasing, text simplification, and Grammatical Error Correction (GEC), share a common trait of dealing with highly similar input and output sequences. This area of research lies at the intersection of two well-established fields: (i) fully autoregressive sequence-to-sequence approaches commonly used in tasks like Neural Machine Translation (NMT) and (ii) sequence tagging techniques commonly used to address tasks such as Part-of-speech tagging, Named-entity recognition (NER), and similar. In the pursuit of a balanced architecture, researchers have come up with numerous imaginative and unconventional solutions, which we're discussing in the Related Works section. Our approach to addressing text editing tasks is called RedPenNet and is aimed at reducing architectural and parametric redundancies presented in specific Sequence-To-Edits models, preserving their semi-autoregressive advantages. Our models achieve $F_{0.5}$ scores of 77.60 on the BEA-2019 (test), which can be considered as state-of-the-art the only exception for system combination and 67.71 on the UAGEC+Fluency (test) benchmarks.   This research is being conducted in the context of the UNLP 2023 workshop, where it was presented as a paper as a paper for the Shared Task in Grammatical Error Correction (GEC) for Ukrainian. This study aims to apply the RedPenNet approach to address the GEC problem in the Ukrainian language.
</details>
<details>
<summary>摘要</summary>
文本编辑任务，包括句子合并、句子分割、重写和语法错误修复（GEC），都与高度相似的输入和输出序列交互。这一领域的研究受到两个已有的领域的影响：一是完全自动化的序列到序列方法，通常用于语义翻译（NMT）类任务；二是序列标记技术，通常用于处理如部分词类标注（POS）、命名实体识别（NER）和类似任务。为了实现平衡的架构，研究人员提出了许多创新的解决方案，我们在相关工作部分进行讨论。我们的方法是called RedPenNet，旨在降低特定Sequence-To-Edits模型中的 arquitectónico和参数 redundancy，保留它们的半自动化优势。我们的模型在BEA-2019（测试）上获得了77.60的F0.5分，可以 considere为领先水平，只有系统组合为例外。此外，在UAGEC+ Fluency（测试）标准下，我们的模型获得了67.71的F0.5分。这项研究在UNLP 2023 会议上进行了展示，作为grammatical Error Correction（GEC） Shared Task for Ukrainian 的论文。本研究的目标是通过应用RedPenNet方法，解决 Ukrainian 语言中的GEC问题。
</details></li>
</ul>
<hr>
<h2 id="Natural-Language-Embedded-Programs-for-Hybrid-Language-Symbolic-Reasoning"><a href="#Natural-Language-Embedded-Programs-for-Hybrid-Language-Symbolic-Reasoning" class="headerlink" title="Natural Language Embedded Programs for Hybrid Language Symbolic Reasoning"></a>Natural Language Embedded Programs for Hybrid Language Symbolic Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10814">http://arxiv.org/abs/2309.10814</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luohongyin/langcode">https://github.com/luohongyin/langcode</a></li>
<li>paper_authors: Tianhua Zhang, Jiaxin Ge, Hongyin Luo, Yung-Sung Chuang, Mingye Gao, Yuan Gong, Xixin Wu, Yoon Kim, Helen Meng, James Glass</li>
<li>for: 解决 math&#x2F;symbolic reasoning、自然语言理解和指令执行任务</li>
<li>methods: 使用自然语言嵌入程序（NLEP）框架，让语言模型生成基于数据结构中的自然语言表示知识的函数</li>
<li>results: 可以超越强基线，在多种任务上提高性能，包括数学和符号逻辑、文本分类、问答和指令执行等任务，并且可以进行后续检查中间逻辑步骤的解释。<details>
<summary>Abstract</summary>
How can we perform computations over natural language representations to solve tasks that require symbolic and numeric reasoning? We propose natural language embedded programs (NLEP) as a unifying framework for addressing math/symbolic reasoning, natural language understanding, and instruction following tasks. Our approach prompts a language model to generate full Python programs that define functions over data structures which contain natural language representations of structured knowledge. A Python interpreter then executes the generated code and prints the output. Despite using a task-general prompt, we find that this approach can improve upon strong baselines across a range of different tasks including math and symbolic reasoning, text classification, question answering, and instruction following. We further find the generated programs are often interpretable and enable post-hoc verification of the intermediate reasoning steps.
</details>
<details>
<summary>摘要</summary>
如何通过对自然语言表示进行计算来解决需要符号逻辑和数值计算的任务？我们提出自然语言嵌入程序（NLEP）作为一个统一的框架，用于Addressing math/符号逻辑、自然语言理解和指令遵循任务。我们的方法请求一个语言模型生成全部Python程序，定义数据结构中含有自然语言表示的结构化知识上的函数。一个Python解释器然后执行生成的代码并输出结果。尽管使用了任务通用的提问，我们发现这种方法可以超越强基线 across a range of different tasks, including math and symbolic reasoning, text classification, question answering, and instruction following. 我们还发现生成的程序往往可读性好，允许后续验证中间的逻辑步骤。
</details></li>
</ul>
<hr>
<h2 id="Modeling-interdisciplinary-interactions-among-Physics-Mathematics-Computer-Science"><a href="#Modeling-interdisciplinary-interactions-among-Physics-Mathematics-Computer-Science" class="headerlink" title="Modeling interdisciplinary interactions among Physics, Mathematics &amp; Computer Science"></a>Modeling interdisciplinary interactions among Physics, Mathematics &amp; Computer Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10811">http://arxiv.org/abs/2309.10811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rima Hazra, Mayank Singh, Pawan Goyal, Bibhas Adhikari, Animesh Mukherjee</li>
<li>for: 本研究旨在研究物理（PHY）、数学（MA）和计算机科学（CS）三个领域之间的引用流动，以及这三个领域之间的引用关系。</li>
<li>methods: 本研究使用了一个数据集，包含了这三个领域的 более than 1.2 million 篇论文，并使用了时间桶特征来量化这三个领域之间的引用互动。</li>
<li>results: 研究发现，在这三个领域之间的引用关系存在一些特定的模式，例如，物理领域常常引用数学领域的论文，而计算机科学领域则常常引用物理领域和数学领域的论文。此外，研究还提出了一些基于 relay-linking 框架的数学模型，用于解释这三个领域之间的引用动态。<details>
<summary>Abstract</summary>
Interdisciplinarity has over the recent years have gained tremendous importance and has become one of the key ways of doing cutting edge research. In this paper we attempt to model the citation flow across three different fields -- Physics (PHY), Mathematics (MA) and Computer Science (CS). For instance, is there a specific pattern in which these fields cite one another? We carry out experiments on a dataset comprising more than 1.2 million articles taken from these three fields. We quantify the citation interactions among these three fields through temporal bucket signatures. We present numerical models based on variants of the recently proposed relay-linking framework to explain the citation dynamics across the three disciplines. These models make a modest attempt to unfold the underlying principles of how citation links could have been formed across the three fields over time.
</details>
<details>
<summary>摘要</summary>
近年来，多学科研究（interdisciplinarity）在研究领域中得到了广泛的重视和发展，成为当今研究的一种重要方法。在这篇论文中，我们尝试通过模拟三个不同领域（物理（PHY）、数学（MA）和计算机科学（CS））之间的引用流动，以探索这三个领域之间的引用模式是否存在特定的征式。我们在一个包含超过120万篇论文的数据集上进行了实验，并通过时间桶签名来量化这三个领域之间的引用互动。我们基于近期提出的协助链框架（relay-linking framework）的变体来提出数学模型，用于解释这三个领域之间的引用动力学。这些模型尝试描述在不同时间点上如何形成这三个领域之间的引用链。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Text-Compression-for-Classification"><a href="#Semantic-Text-Compression-for-Classification" class="headerlink" title="Semantic Text Compression for Classification"></a>Semantic Text Compression for Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10809">http://arxiv.org/abs/2309.10809</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emrecan Kutay, Aylin Yener</li>
<li>for: 这 paper 的目的是压缩文本中的含义，以便在分类等应用中提高效率。</li>
<li>methods: 这 paper 使用 semantic quantization 和压缩方法，使用 sentence embeddings 和semantic distortion metric来保持含义。</li>
<li>results: 这 paper 的结果表明，使用 semantic 方法可以大幅降低 bit 数，但准确性损失比基eline 小。此外， semantic clustering 可以进一步增加资源储存的可 reuse。这些方法在多种文本分类任务中表现出色。<details>
<summary>Abstract</summary>
We study semantic compression for text where meanings contained in the text are conveyed to a source decoder, e.g., for classification. The main motivator to move to such an approach of recovering the meaning without requiring exact reconstruction is the potential resource savings, both in storage and in conveying the information to another node. Towards this end, we propose semantic quantization and compression approaches for text where we utilize sentence embeddings and the semantic distortion metric to preserve the meaning. Our results demonstrate that the proposed semantic approaches result in substantial (orders of magnitude) savings in the required number of bits for message representation at the expense of very modest accuracy loss compared to the semantic agnostic baseline. We compare the results of proposed approaches and observe that resource savings enabled by semantic quantization can be further amplified by semantic clustering. Importantly, we observe the generalizability of the proposed methodology which produces excellent results on many benchmark text classification datasets with a diverse array of contexts.
</details>
<details>
<summary>摘要</summary>
我们研究语义压缩 для文本，其中文本中的意义传递到源解码器（例如分类）。主要的动机是避免精确重建的需求，以便实现资源储存和信息传输成本的减少。为此，我们提议语义压缩和压缩方法，使用句子嵌入和semantic distortion metric来保持语义。我们的结果表明，我们提议的语义方法可以实现重要的（许多个数级）储存成本减少，并且与语义agnostic基准相比，只有极少的准确性损失。我们比较了我们的方法与其他方法的结果，发现semantic quantization可以通过semantic clustering进一步增强资源储存的可抗性。重要的是，我们发现我们的方法在多个 benchmark文本分类 datasets上实现了优秀的结果，这些dataset中的上下文非常多样化。
</details></li>
</ul>
<hr>
<h2 id="Interactive-Distillation-of-Large-Single-Topic-Corpora-of-Scientific-Papers"><a href="#Interactive-Distillation-of-Large-Single-Topic-Corpora-of-Scientific-Papers" class="headerlink" title="Interactive Distillation of Large Single-Topic Corpora of Scientific Papers"></a>Interactive Distillation of Large Single-Topic Corpora of Scientific Papers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10772">http://arxiv.org/abs/2309.10772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicholas Solovyev, Ryan Barron, Manish Bhattarai, Maksim E. Eren, Kim O. Rasmussen, Boian S. Alexandrov</li>
<li>for: 这篇论文的目的是建立一个可扩展的、可靠的科学文献集合，并将其用于研究和教育。</li>
<li>methods: 这篇论文使用机器学习技术，将小量的“核心”文献集成为一个大量的科学文献集合。文献集合中的每篇文章都通过机器学习算法进行评估，以确定它们是否与“核心”文献集成关联。</li>
<li>results: 这篇论文透过机器学习技术建立了一个可扩展的、可靠的科学文献集合，并且可以透过人类专家的干预选择来确保集合中的文章是有关的。此外，这篇论文还使用了sub-topic模型ing（SeNMFk）来获得更多关于文章的信息。<details>
<summary>Abstract</summary>
Highly specific datasets of scientific literature are important for both research and education. However, it is difficult to build such datasets at scale. A common approach is to build these datasets reductively by applying topic modeling on an established corpus and selecting specific topics. A more robust but time-consuming approach is to build the dataset constructively in which a subject matter expert (SME) handpicks documents. This method does not scale and is prone to error as the dataset grows. Here we showcase a new tool, based on machine learning, for constructively generating targeted datasets of scientific literature. Given a small initial "core" corpus of papers, we build a citation network of documents. At each step of the citation network, we generate text embeddings and visualize the embeddings through dimensionality reduction. Papers are kept in the dataset if they are "similar" to the core or are otherwise pruned through human-in-the-loop selection. Additional insight into the papers is gained through sub-topic modeling using SeNMFk. We demonstrate our new tool for literature review by applying it to two different fields in machine learning.
</details>
<details>
<summary>摘要</summary>
高度特定的数据集是科研和教育中非常重要的。然而，建立这些数据集是困难的。一种常见的方法是通过缩写分析已有的文库，选择特定的主题来建立数据集。另一种更加robust但是时间费时的方法是通过专家手动选择文献来建立数据集。这种方法不具扩展性和容易出错。在这里，我们介绍了一种新工具，基于机器学习，用于构建targeted的科研文献数据集。我们给出了一个小的初始核心文献库，然后建立了引用网络，并在每个引用网络中生成文本嵌入。我们使用dimensionality reduction来Visualize嵌入。如果文献与核心文献相似，或者通过人工征选来选择，则保留文献在数据集中。我们还使用SeNMFk进行子主题分析，从而获得更多的文献信息。我们通过在机器学习领域进行文献综述来证明我们的新工具。
</details></li>
</ul>
<hr>
<h2 id="OpenBA-An-Open-sourced-15B-Bilingual-Asymmetric-seq2seq-Model-Pre-trained-from-Scratch"><a href="#OpenBA-An-Open-sourced-15B-Bilingual-Asymmetric-seq2seq-Model-Pre-trained-from-Scratch" class="headerlink" title="OpenBA: An Open-sourced 15B Bilingual Asymmetric seq2seq Model Pre-trained from Scratch"></a>OpenBA: An Open-sourced 15B Bilingual Asymmetric seq2seq Model Pre-trained from Scratch</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10706">http://arxiv.org/abs/2309.10706</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/opennlg/openba">https://github.com/opennlg/openba</a></li>
<li>paper_authors: Juntao Li, Zecheng Tang, Yuyang Ding, Pinzheng Wang, Pei Guo, Wangjie You, Dan Qiao, Wenliang Chen, Guohong Fu, Qiaoming Zhu, Guodong Zhou, Min Zhang</li>
<li>for: 本研究旨在开发一个基于中文的开源语言模型，以提高中文自然语言处理 task 的性能。</li>
<li>methods: 本文使用了一些有效和高效的技术，包括三个阶段的训练策略和数据处理等，以减少模型的大小。</li>
<li>results: 根据测试结果，我们的模型在 BELEBELE 测试套件、MMLU 测试套件和 C-Eval (hard) 测试套件上的性能比 LLaMA-70B 和 BLOOM-176B 更好，只需要使用 380B 个字。<details>
<summary>Abstract</summary>
Large language models (LLMs) with billions of parameters have demonstrated outstanding performance on various natural language processing tasks. This report presents OpenBA, an open-sourced 15B bilingual asymmetric seq2seq model, to contribute an LLM variant to the Chinese-oriented open-source model community. We enhance OpenBA with effective and efficient techniques as well as adopt a three-stage training strategy to train the model from scratch. Our solution can also achieve very competitive performance with only 380B tokens, which is better than LLaMA-70B on the BELEBELE benchmark, BLOOM-176B on the MMLU benchmark, GLM-130B on the C-Eval (hard) benchmark. This report provides the main details to pre-train an analogous model, including pre-training data processing, Bilingual Flan data collection, the empirical observations that inspire our model architecture design, training objectives of different stages, and other enhancement techniques. We have refactored our code to follow the design principles of the Huggingface Transformers Library, making it more convenient for developers to use, and released checkpoints of different training stages at https://huggingface.co/openBA. More details of our project are available at https://github.com/OpenNLG/openBA.git.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）拥有数十亿参数，在不同的自然语言处理任务上表现出色。本报告介绍OpenBA，一个开源的15B双语异称seq2seq模型，以贡献一种中文 Orientated 开源模型社区的变体。我们在OpenBA中应用有效和高效的技术，并采用三个阶段训练策略来从头开始训练模型。我们的解决方案可以在只有380B字符时达到非常竞争力的性能，比LLaMA-70B在BELEBELE标准上更好，比BLOOM-176B在MMLU标准上更好，比GLM-130B在C-Eval（困难）标准上更好。本报告提供了预训练模型的主要细节，包括预训练数据处理、双语Flann数据收集、预训练模型建立的设计原则、不同阶段的训练目标以及其他增强技术。我们已经对代码进行了 refactor，使其更加符合Huggingface Transformers Library的设计原则，并将不同训练阶段的检查点上传到https://huggingface.co/openBA。更多关于我们项目的详细信息可以通过https://github.com/OpenNLG/openBA.git获取。
</details></li>
</ul>
<hr>
<h2 id="Improving-Medical-Dialogue-Generation-with-Abstract-Meaning-Representations"><a href="#Improving-Medical-Dialogue-Generation-with-Abstract-Meaning-Representations" class="headerlink" title="Improving Medical Dialogue Generation with Abstract Meaning Representations"></a>Improving Medical Dialogue Generation with Abstract Meaning Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10608">http://arxiv.org/abs/2309.10608</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bohao Yang, Chen Tang, Chenghua Lin</li>
<li>for: 这篇论文旨在探讨对距离医疗的对话生成，以实现医疗专业知识的传递。</li>
<li>methods: 本研究使用抽象意义表示（AMR）创建图形表示，以强调对话中的语言构成部分和医疗知识中的关系和实体。</li>
<li>results: 本研究的结果显示，使用AMR图形表示可以增强对话生成模型的理解能力，并且比对基eline模型表现出色。此外，本研究提供了实现这种方法的资源代码，以便未来的研究。<details>
<summary>Abstract</summary>
Medical Dialogue Generation serves a critical role in telemedicine by facilitating the dissemination of medical expertise to patients. Existing studies focus on incorporating textual representations, which have limited their ability to represent the semantics of text, such as ignoring important medical entities. To enhance the model's understanding of the textual semantics and the medical knowledge including entities and relations, we introduce the use of Abstract Meaning Representations (AMR) to construct graphical representations that delineate the roles of language constituents and medical entities within the dialogues. In this paper, We propose a novel framework that models dialogues between patients and healthcare professionals using AMR graphs, where the neural networks incorporate textual and graphical knowledge with a dual attention mechanism. Experimental results show that our framework outperforms strong baseline models in medical dialogue generation, demonstrating the effectiveness of AMR graphs in enhancing the representations of medical knowledge and logical relationships. Furthermore, to support future research in this domain, we provide the corresponding source code at https://github.com/Bernard-Yang/MedDiaAMR.
</details>
<details>
<summary>摘要</summary>
医疗对话生成扮演着重要的角色在 теле医疗中，以便传递医疗专业知识给患者。现有研究主要集中在文本表示方面，这限制了模型的能力来表示文本 semantics，例如忽略重要的医疗实体。为了增强模型对文本 semantics和医疗知识，包括实体和关系的理解，我们介绍了使用抽象意义表示（AMR）构建图形表示，以便分析语言结构和医疗实体在对话中的角色。在这篇论文中，我们提出了一种新的框架，该框架使用 AMR 图来模型患者和医疗专业人员之间的对话，并使用双重注意机制来结合文本和图形知识。实验结果表明，我们的框架在医疗对话生成中表现出优于强基线模型，这表明 AMR 图可以增强医疗知识和逻辑关系的表示。此外，为支持未来在这个领域的研究，我们在 GitHub 上提供了相关的源代码，请参考 https://github.com/Bernard-Yang/MedDiaAMR。
</details></li>
</ul>
<hr>
<h2 id="FRACAS-A-FRench-Annotated-Corpus-of-Attribution-relations-in-newS"><a href="#FRACAS-A-FRench-Annotated-Corpus-of-Attribution-relations-in-newS" class="headerlink" title="FRACAS: A FRench Annotated Corpus of Attribution relations in newS"></a>FRACAS: A FRench Annotated Corpus of Attribution relations in newS</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10604">http://arxiv.org/abs/2309.10604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ange Richard, Laura Alonzo-Canul, François Portet</li>
<li>for: 本研究用于开发法语新闻文本中引用EXTRACTION和来源归属的手动注释 корпу。</li>
<li>methods: 本研究使用了 manually annotated corpus of 1676 newswire texts in French for quotation extraction and source attribution.</li>
<li>results: 研究得到了一个 manually annotated corpus，并且获得了对引用类型的Balance (直接、间接和混合)，以及对注释者之间的互动对象的高度一致。<details>
<summary>Abstract</summary>
Quotation extraction is a widely useful task both from a sociological and from a Natural Language Processing perspective. However, very little data is available to study this task in languages other than English. In this paper, we present a manually annotated corpus of 1676 newswire texts in French for quotation extraction and source attribution. We first describe the composition of our corpus and the choices that were made in selecting the data. We then detail the annotation guidelines and annotation process, as well as a few statistics about the final corpus and the obtained balance between quote types (direct, indirect and mixed, which are particularly challenging). We end by detailing our inter-annotator agreement between the 8 annotators who worked on manual labelling, which is substantially high for such a difficult linguistic phenomenon.
</details>
<details>
<summary>摘要</summary>
<SYS><LC>zh-CN</LC></SYS>引用抽取是一项广泛有用的任务，不仅从社会学角度而言，也从自然语言处理角度来说。然而，有很少数据可以研究这项任务的其他语言 besides English。在这篇论文中，我们提供了1676篇新闻报道文本的手动注释集，用于引用抽取和来源归属。我们首先介绍了我们的 corpus 的组成和选择的数据。然后，我们详细介绍了注释指南和注释过程，以及最终集合的一些统计数据和引用类型的平衡。最后，我们详细介绍了8名注释员的间接协作情况，即这种语言现象的协作率很高。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Deep-Cross-Language-Entity-Alignment"><a href="#Unsupervised-Deep-Cross-Language-Entity-Alignment" class="headerlink" title="Unsupervised Deep Cross-Language Entity Alignment"></a>Unsupervised Deep Cross-Language Entity Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10598">http://arxiv.org/abs/2309.10598</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chuanyus/udcea">https://github.com/chuanyus/udcea</a></li>
<li>paper_authors: Chuanyu Jiang, Yiming Qian, Lijun Chen, Yang Gu, Xia Xie</li>
<li>for: 这篇论文旨在提出一种简单且新的无监督方法，用于跨语言实体对齐。</li>
<li>methods: 我们使用深度学习多语言encoder和机器翻译器来编码知识图文本，从而减少了标注数据的依赖性。我们的方法同时考虑了全局和局部对齐策略。</li>
<li>results: 我们的方法可以在DBP15K数据集上得到0.966、0.990和0.996的Hits@1率，在无监督和半监督类别中超过了现状的方法。与监督方法相比，我们的方法在 Ja-En和Fr-En对齐任务中表现出了2.6%和0.4%的提升，只是微弱地下降0.2%在Zh-En对齐任务中。<details>
<summary>Abstract</summary>
Cross-lingual entity alignment is the task of finding the same semantic entities from different language knowledge graphs. In this paper, we propose a simple and novel unsupervised method for cross-language entity alignment. We utilize the deep learning multi-language encoder combined with a machine translator to encode knowledge graph text, which reduces the reliance on label data. Unlike traditional methods that only emphasize global or local alignment, our method simultaneously considers both alignment strategies. We first view the alignment task as a bipartite matching problem and then adopt the re-exchanging idea to accomplish alignment. Compared with the traditional bipartite matching algorithm that only gives one optimal solution, our algorithm generates ranked matching results which enabled many potentials downstream tasks. Additionally, our method can adapt two different types of optimization (minimal and maximal) in the bipartite matching process, which provides more flexibility. Our evaluation shows, we each scored 0.966, 0.990, and 0.996 Hits@1 rates on the DBP15K dataset in Chinese, Japanese, and French to English alignment tasks. We outperformed the state-of-the-art method in unsupervised and semi-supervised categories. Compared with the state-of-the-art supervised method, our method outperforms 2.6% and 0.4% in Ja-En and Fr-En alignment tasks while marginally lower by 0.2% in the Zh-En alignment task.
</details>
<details>
<summary>摘要</summary>
crossed-lingual entity alignment是找到不同语言知识图中的同义semantic entity的任务。在这篇论文中，我们提出了一种简单且新的无监督方法 для crossed-lingual entity alignment。我们利用深度学习多语言编码器和机器翻译器来编码知识图文本，从而减少了依赖于标注数据。与传统方法只强调全局或本地对齐不同，我们的方法同时考虑了这两种对齐策略。我们首先将对齐任务视为一种双方匹配问题，然后采用了重新交换的想法来完成对齐。与传统的双方匹配算法只给出一个最优解不同，我们的算法生成了排名匹配结果，这些结果允许更多的下游任务。此外，我们的方法可以在双方匹配过程中采用不同的优化策略（最小和最大），这提供了更多的灵活性。我们的评估结果显示，我们在DBP15K dataset上分别得分0.966、0.990和0.996，在中文、日语和法语到英语对齐任务中。我们超过了当前状态的方法在无监督和半监督类别中。与当前状态的监督方法相比，我们的方法在Ja-En和Fr-En对齐任务中表现出优于2.6%和0.4%，而在Zh-En对齐任务中则只下降0.2%。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Modeling-For-Spoken-Language-Identification"><a href="#Multimodal-Modeling-For-Spoken-Language-Identification" class="headerlink" title="Multimodal Modeling For Spoken Language Identification"></a>Multimodal Modeling For Spoken Language Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10567">http://arxiv.org/abs/2309.10567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shikhar Bharadwaj, Min Ma, Shikhar Vashishth, Ankur Bapna, Sriram Ganapathy, Vera Axelrod, Siddharth Dalmia, Wei Han, Yu Zhang, Daan van Esch, Sandy Ritchie, Partha Talukdar, Jason Riesa</li>
<li>for: 本研究旨在提高多媒体录音语言识别精度，通过利用不同类型的metadata来增强语言识别。</li>
<li>methods: 本研究提出了一种多modal Spoken Language Identification方法（MuSeLI），利用视频标题、描述和地理位置等metadata来提高语言识别精度。</li>
<li>results: 实验结果表明，Metadata可以提供显著帮助语言识别任务，并且对多媒体录音语言识别 Task取得了现场的状态。 Additionally, the ablation study shows that each modality contributes distinctively to language recognition.<details>
<summary>Abstract</summary>
Spoken language identification refers to the task of automatically predicting the spoken language in a given utterance. Conventionally, it is modeled as a speech-based language identification task. Prior techniques have been constrained to a single modality; however in the case of video data there is a wealth of other metadata that may be beneficial for this task. In this work, we propose MuSeLI, a Multimodal Spoken Language Identification method, which delves into the use of various metadata sources to enhance language identification. Our study reveals that metadata such as video title, description and geographic location provide substantial information to identify the spoken language of the multimedia recording. We conduct experiments using two diverse public datasets of YouTube videos, and obtain state-of-the-art results on the language identification task. We additionally conduct an ablation study that describes the distinct contribution of each modality for language recognition.
</details>
<details>
<summary>摘要</summary>
通过多模态特征来实现语言认识，这是我们的研究方向。在视频数据中，我们可以使用视频标题、描述和地理位置等 metadata 来提高语言认识的准确性。我们在两个 YouTube 视频数据集上进行实验，并在语言认识任务上达到了状态之 искусственный前景。此外，我们还进行了减少研究，以解释每个模式在语言认识中的独特贡献。
</details></li>
</ul>
<hr>
<h2 id="NSOAMT-–-New-Search-Only-Approach-to-Machine-Translation"><a href="#NSOAMT-–-New-Search-Only-Approach-to-Machine-Translation" class="headerlink" title="NSOAMT – New Search Only Approach to Machine Translation"></a>NSOAMT – New Search Only Approach to Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10526">http://arxiv.org/abs/2309.10526</a></li>
<li>repo_url: None</li>
<li>paper_authors: João Luís, Diogo Cardoso, José Marques, Luís Campos</li>
<li>for: 这个论文的目的是提出一种基于新搜索方法的机器翻译技术，以解决传统技术的慢速和不准确问题。</li>
<li>methods: 这个研究采用了一种新的索引技术，通过将具有相似语义的词语组合在一起，实现对原文语言记录和翻译语言之间的对应关系。</li>
<li>results: 这个研究发现，对于某些类型的文档，使用这种新索引技术可以提高翻译速度和准确性，并且可以开发出一种基于这种方法的翻译工具。<details>
<summary>Abstract</summary>
Translation automation mechanisms and tools have been developed for several years to bring people who speak different languages together. A "new search only approach to machine translation" was adopted to tackle some of the slowness and inaccuracy of the other technologies. The idea is to develop a solution that, by indexing an incremental set of words that combine a certain semantic meaning, makes it possible to create a process of correspondence between their native language record and the language of translation. This research principle assumes that the vocabulary used in a given type of publication/document is relatively limited in terms of language style and word diversity, which enhances the greater effect of instantaneously and rigor in the translation process through the indexing process. A volume of electronic text documents where processed and loaded into a database, and analyzed and measured in order confirm the previous premise. Although the observed and projected metric values did not give encouraging results, it was possible to develop and make available a translation tool using this approach.
</details>
<details>
<summary>摘要</summary>
机器翻译技术和工具在数年前就已经开发出来，以便让不同语言的人们共同交流。我们采用了一种“新搜索Onlyapproach to machine translation”来解决其他技术的慢速和不准确性。我们的想法是，通过索引增量词汇，使得将原始语言纪录与翻译语言之间创建对应关系。这个研究原则假设了公开发表/文档中的词汇数量相对较少，而且语言风格和词汇多样性受限，从而通过索引过程实现更快速、更加准确的翻译。我们对一volume of electronic文档进行处理和加载到数据库中，并对其进行分析和测量，以确认上述假设。虽然观察到的和预计的 метриック值并不给出激励的结果，但我们仍然可以开发出一种使用这种方法的翻译工具。
</details></li>
</ul>
<hr>
<h2 id="Harnessing-the-Zero-Shot-Power-of-Instruction-Tuned-Large-Language-Model-in-End-to-End-Speech-Recognition"><a href="#Harnessing-the-Zero-Shot-Power-of-Instruction-Tuned-Large-Language-Model-in-End-to-End-Speech-Recognition" class="headerlink" title="Harnessing the Zero-Shot Power of Instruction-Tuned Large Language Model in End-to-End Speech Recognition"></a>Harnessing the Zero-Shot Power of Instruction-Tuned Large Language Model in End-to-End Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10524">http://arxiv.org/abs/2309.10524</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yosuke Higuchi, Tetsuji Ogawa, Tetsunori Kobayashi</li>
<li>for: 本研究旨在探讨一种使用大型自然语言模型（LLM）和端到端自动语音识别（ASR）的新的集成方法，以提高ASR性能。</li>
<li>methods: 我们使用一种名为Llama2的 instrucion-tuned LLM，并将其作为decoder的前端使用。我们还使用CTC和注意力架构，将ASR假设作为输入，并将其feed到LLM中，以便LLM可以根据指令进行文本生成。</li>
<li>results: 我们的实验结果和分析表明，这种集成方法可以提供出色的性能提升，并且我们的方法受益于LLM-based rescoring。<details>
<summary>Abstract</summary>
We present a novel integration of an instruction-tuned large language model (LLM) and end-to-end automatic speech recognition (ASR). Modern LLMs can perform a wide range of linguistic tasks within zero-shot learning when provided with a precise instruction or a prompt to guide the text generation process towards the desired task. We explore using this zero-shot capability of LLMs to extract linguistic information that can contribute to improving ASR performance. Specifically, we direct an LLM to correct grammatical errors in an ASR hypothesis and harness the embedded linguistic knowledge to conduct end-to-end ASR. The proposed model is built on the hybrid connectionist temporal classification (CTC) and attention architecture, where an instruction-tuned LLM (i.e., Llama2) is employed as a front-end of the decoder. An ASR hypothesis, subject to correction, is obtained from the encoder via CTC decoding, which is then fed into the LLM along with an instruction. The decoder subsequently takes as input the LLM embeddings to perform sequence generation, incorporating acoustic information from the encoder output. Experimental results and analyses demonstrate that the proposed integration yields promising performance improvements, and our approach largely benefits from LLM-based rescoring.
</details>
<details>
<summary>摘要</summary>
The proposed model is built on the hybrid connectionist temporal classification (CTC) and attention architecture, where an instruction-tuned LLM (i.e., Llama2) is employed as the front-end of the decoder. An ASR hypothesis, subject to correction, is obtained from the encoder via CTC decoding, which is then fed into the LLM along with an instruction. The decoder subsequently takes as input the LLM embeddings to perform sequence generation, incorporating acoustic information from the encoder output.Experimental results and analyses demonstrate that the proposed integration yields promising performance improvements, and our approach largely benefits from LLM-based rescoring.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Open-Domain-Table-Question-Answering-via-Syntax-and-Structure-aware-Dense-Retrieval"><a href="#Enhancing-Open-Domain-Table-Question-Answering-via-Syntax-and-Structure-aware-Dense-Retrieval" class="headerlink" title="Enhancing Open-Domain Table Question Answering via Syntax- and Structure-aware Dense Retrieval"></a>Enhancing Open-Domain Table Question Answering via Syntax- and Structure-aware Dense Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10506">http://arxiv.org/abs/2309.10506</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nzjin/ODTQA">https://github.com/nzjin/ODTQA</a></li>
<li>paper_authors: Nengzheng Jin, Dongfang Li, Junying Chen, Joanna Siebert, Qingcai Chen</li>
<li>for:  answering open-domain table questions by retrieving and extracting information from a large collection of tables</li>
<li>methods: using syntax- and structure-aware retrieval method that provides syntactical representations for the question and uses structural header and value representations for the tables to avoid information loss</li>
<li>results: achieving state-of-the-art performance on the NQ-tables dataset and overwhelming strong baselines on a newly curated open-domain Text-to-SQL datasetHere’s the simplified Chinese text:</li>
<li>for:  Answering open-domain 表格问题，通过大量表格的检索和提取信息。</li>
<li>methods: 使用 syntax- 和 structure-aware 检索方法，提供问题的 sintactical 表示，并使用表格的结构头和值表示来避免信息损失。</li>
<li>results: 在 NQ-tables 数据集上达到状态级表现，在新编辑的 open-domain Text-to-SQL 数据集上压倒强大的基eline。<details>
<summary>Abstract</summary>
Open-domain table question answering aims to provide answers to a question by retrieving and extracting information from a large collection of tables. Existing studies of open-domain table QA either directly adopt text retrieval methods or consider the table structure only in the encoding layer for table retrieval, which may cause syntactical and structural information loss during table scoring. To address this issue, we propose a syntax- and structure-aware retrieval method for the open-domain table QA task. It provides syntactical representations for the question and uses the structural header and value representations for the tables to avoid the loss of fine-grained syntactical and structural information. Then, a syntactical-to-structural aggregator is used to obtain the matching score between the question and a candidate table by mimicking the human retrieval process. Experimental results show that our method achieves the state-of-the-art on the NQ-tables dataset and overwhelms strong baselines on a newly curated open-domain Text-to-SQL dataset.
</details>
<details>
<summary>摘要</summary>
开放领域表格问答旨在提供问题的答案，通过检索和提取大量表格中的信息。现有研究的开放领域表格QA方法可能直接采用文本检索方法，或者只考虑表格结构在编码层面进行表格检索，这可能会导致问题的语法和结构信息丢失，从而影响表格的得分。为解决这个问题，我们提出一种语法和结构意识检索方法，用于开放领域表格QA任务。它提供了问题的语法表示，并使用表格的结构标头和值表示来避免语法和结构信息的丢失。然后，一个语法-结构汇总器用于获取问题和候选表格之间的匹配分数，通过模拟人类检索过程来实现。实验结果表明，我们的方法在NQ-tables数据集上实现了领先地位，并在一个新收录的开放领域文本-SQL数据集上压倒了强大的基线。
</details></li>
</ul>
<hr>
<h2 id="Improving-Speaker-Diarization-using-Semantic-Information-Joint-Pairwise-Constraints-Propagation"><a href="#Improving-Speaker-Diarization-using-Semantic-Information-Joint-Pairwise-Constraints-Propagation" class="headerlink" title="Improving Speaker Diarization using Semantic Information: Joint Pairwise Constraints Propagation"></a>Improving Speaker Diarization using Semantic Information: Joint Pairwise Constraints Propagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10456">http://arxiv.org/abs/2309.10456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luyao Cheng, Siqi Zheng, Qinglin Zhang, Hui Wang, Yafeng Chen, Qian Chen, Shiliang Zhang</li>
<li>for: 本研究旨在充分利用语言模型来提高基于集群的说话人识别系统的性能。</li>
<li>methods: 我们提出了一种新的方法，利用语言模型提取说话人相关的semantic信息，并将这些信息转化为对应的对比约束。</li>
<li>results: 我们在公共数据集上进行了广泛的实验，结果表明我们提出的方法在比对 voz-only 的说话人识别系统时表现出了一致性的superiority。<details>
<summary>Abstract</summary>
Speaker diarization has gained considerable attention within speech processing research community. Mainstream speaker diarization rely primarily on speakers' voice characteristics extracted from acoustic signals and often overlook the potential of semantic information. Considering the fact that speech signals can efficiently convey the content of a speech, it is of our interest to fully exploit these semantic cues utilizing language models. In this work we propose a novel approach to effectively leverage semantic information in clustering-based speaker diarization systems. Firstly, we introduce spoken language understanding modules to extract speaker-related semantic information and utilize these information to construct pairwise constraints. Secondly, we present a novel framework to integrate these constraints into the speaker diarization pipeline, enhancing the performance of the entire system. Extensive experiments conducted on the public dataset demonstrate the consistent superiority of our proposed approach over acoustic-only speaker diarization systems.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本为简化中文。<</SYS>>发言人识别在语音处理研究社区中受到了广泛的关注。主流的发言人识别方法主要基于发言人的声音特征，从语音信号中提取发言人的声音特征，然而它们经常忽略语音信号中的 semantics 信息。考虑到语音信号可以办好发言人的发言内容，我们想要充分利用这些 semantics 信息，以提高 clustering-based 发言人识别系统的性能。首先，我们引入了语言理解模块，以提取发言人相关的 semantics 信息，并使用这些信息构建对应的对比约束。其次，我们提出了一种新的框架，将这些约束集成到发言人识别管道中，从而提高整个系统的性能。在公共数据集上进行了广泛的实验，我们的提议方法在对比于声音Only 发言人识别系统时具有一致的超越性。
</details></li>
</ul>
<hr>
<h2 id="Reformulating-Sequential-Recommendation-Learning-Dynamic-User-Interest-with-Content-enriched-Language-Modeling"><a href="#Reformulating-Sequential-Recommendation-Learning-Dynamic-User-Interest-with-Content-enriched-Language-Modeling" class="headerlink" title="Reformulating Sequential Recommendation: Learning Dynamic User Interest with Content-enriched Language Modeling"></a>Reformulating Sequential Recommendation: Learning Dynamic User Interest with Content-enriched Language Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10435">http://arxiv.org/abs/2309.10435</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junzhe Jiang, Shang Qu, Mingyue Cheng, Qi Liu</li>
<li>for: 这个论文的目的是提出一种基于语言模型的新式分布式推荐方法，以提高推荐系统的个性化性和准确性。</li>
<li>methods: 该方法使用预训练的语言模型来捕捉用户的兴趣和需求，并通过语义理解来生成个性化的推荐结果。</li>
<li>results: 经过实验 validate，该方法可以在多个数据集上达到比较好的推荐效果，并且提供了有价值的推荐结果和推荐方法的指导意见。<details>
<summary>Abstract</summary>
Recommender systems are essential for online applications, and sequential recommendation has enjoyed significant prevalence due to its expressive ability to capture dynamic user interests. However, previous sequential modeling methods still have limitations in capturing contextual information. The primary reason for this issue is that language models often lack an understanding of domain-specific knowledge and item-related textual content. To address this issue, we adopt a new sequential recommendation paradigm and propose LANCER, which leverages the semantic understanding capabilities of pre-trained language models to generate personalized recommendations. Our approach bridges the gap between language models and recommender systems, resulting in more human-like recommendations. We demonstrate the effectiveness of our approach through experiments on several benchmark datasets, showing promising results and providing valuable insights into the influence of our model on sequential recommendation tasks. Furthermore, our experimental codes are publicly available.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>在线应用程序中，推荐系统是非常重要的，而串行推荐占据了主导地位，因为它可以快速表达用户的兴趣。然而，以前的串行建模方法仍有限制，不能够 capture contextual information。主要的原因是语言模型通常缺乏域pecific知识和项目相关的文本内容的理解。为了解决这个问题，我们采用了一种新的串行推荐方式，并提出了LANCER，它利用预训练语言模型的semantic理解能力来生成个性化推荐。我们的方法 bridge了语言模型和推荐系统之间的 gap，从而生成更人类化的推荐。我们通过对多个 benchmark datasets进行实验，证明了我们的方法的有效性，并提供了有价值的推荐系统设计的意见。此外，我们的实验代码也公开可用。
</details></li>
</ul>
<hr>
<h2 id="Writer-Defined-AI-Personas-for-On-Demand-Feedback-Generation"><a href="#Writer-Defined-AI-Personas-for-On-Demand-Feedback-Generation" class="headerlink" title="Writer-Defined AI Personas for On-Demand Feedback Generation"></a>Writer-Defined AI Personas for On-Demand Feedback Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10433">http://arxiv.org/abs/2309.10433</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karim Benharrak, Tim Zindulka, Florian Lehmann, Hendrik Heuer, Daniel Buschek</li>
<li>for: 这篇论文是为了支持作者创作而设计的，帮助作者更好地理解和 Hook 到他们的target audience。</li>
<li>methods: 这篇论文使用了基于作者定义的 AI人物来生成即时反馈，以帮助作者更好地理解和 Hook 到他们的target audience。</li>
<li>results: 作者对这种概念表示欢迎，并在两个用户研究中使用了这种方法来获得不同的视角和反馈。  however, the feedback was often verbose and unspecific.<details>
<summary>Abstract</summary>
Compelling writing is tailored to its audience. This is challenging, as writers may struggle to empathize with readers, get feedback in time, or gain access to the target group. We propose a concept that generates on-demand feedback, based on writer-defined AI personas of any target audience. We explore this concept with a prototype (using GPT-3.5) in two user studies (N=5 and N=11): Writers appreciated the concept and strategically used personas for getting different perspectives. The feedback was seen as helpful and inspired revisions of text and personas, although it was often verbose and unspecific. We discuss the impact of on-demand feedback, the limited representativity of contemporary AI systems, and further ideas for defining AI personas. This work contributes to the vision of supporting writers with AI by expanding the socio-technical perspective in AI tool design: To empower creators, we also need to keep in mind their relationship to an audience.
</details>
<details>
<summary>摘要</summary>
优秀的写作是适应其读者群体的。这是一项挑战，因为作者可能难以理解读者，获得时间ous feedback，或者访问目标群体。我们提出了一个概念，即基于作者定义的人工智能人类，以获得即时反馈。我们在两项用户研究（N=5和N=11）中试用了这个概念：作者喜欢这个概念，并在获得不同角度的反馈时使用了人类。反馈被看作是有帮助的，并促使了文本和人类的修订，although it was often verbose and unspecific. We discuss the impact of on-demand feedback, the limited representativity of contemporary AI systems, and further ideas for defining AI personas. This work contributes to the vision of supporting writers with AI by expanding the socio-technical perspective in AI tool design: To empower creators, we also need to keep in mind their relationship to an audience.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide that version as well.
</details></li>
</ul>
<hr>
<h2 id="PICK-Polished-Informed-Candidate-Scoring-for-Knowledge-Grounded-Dialogue-Systems"><a href="#PICK-Polished-Informed-Candidate-Scoring-for-Knowledge-Grounded-Dialogue-Systems" class="headerlink" title="PICK: Polished &amp; Informed Candidate Scoring for Knowledge-Grounded Dialogue Systems"></a>PICK: Polished &amp; Informed Candidate Scoring for Knowledge-Grounded Dialogue Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10413">http://arxiv.org/abs/2309.10413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bryan Wilie, Yan Xu, Willy Chung, Samuel Cahyawijaya, Holy Lovenia, Pascale Fung</li>
<li>for: 提高知识grunded对话系统的响应质量，使其更加信息归一化和有趣。</li>
<li>methods: 基于外部知识的对话生成系统，通过分析多种语言模型生成结果，发现存在多个生成结果，其中一些更加准确和有 relevance 性。</li>
<li>results: 提出了一种基于抑制策略的生成重新分配框架PICK，可以使模型生成更加准确和有 relevance 性的响应，不需要额外的标注数据或模型调整。经过自动和人工评估，PICK 能够提高系统的表现，并且在所有排序策略下保持稳定性。详细实现可以参考<a target="_blank" rel="noopener" href="https://github.com/bryanwilie/pick%E3%80%82">https://github.com/bryanwilie/pick。</a><details>
<summary>Abstract</summary>
Grounding dialogue response generation on external knowledge is proposed to produce informative and engaging responses. However, current knowledge-grounded dialogue (KGD) systems often fail to align the generated responses with human-preferred qualities due to several issues like hallucination and the lack of coherence. Upon analyzing multiple language model generations, we observe the presence of alternative generated responses within a single decoding process. These alternative responses are more faithful and exhibit a comparable or higher level of relevance to prior conversational turns compared to the optimal responses prioritized by the decoding processes. To address these challenges and driven by these observations, we propose Polished \& Informed Candidate Scoring (PICK), a generation re-scoring framework that empowers models to generate faithful and relevant responses without requiring additional labeled data or model tuning. Through comprehensive automatic and human evaluations, we demonstrate the effectiveness of PICK in generating responses that are more faithful while keeping them relevant to the dialogue history. Furthermore, PICK consistently improves the system's performance with both oracle and retrieved knowledge in all decoding strategies. We provide the detailed implementation in https://github.com/bryanwilie/pick .
</details>
<details>
<summary>摘要</summary>
提出了基于外部知识的对话回答生成系统，以生成有用和有趣的回答。然而，目前的知识基数对对话（KGD）系统经常无法落实生成的回答与人类喜好的质量不符，这可能是因为幻觉和对话异常的问题。我们分析了多种语言模型生成结果，发现生成过程中存在多个可行的回答，这些回答更 faithful 并且与之前的对话转折更高度相关。为了解决这些挑战，我们提出了精炼并 Informed Candidate Scoring（PICK）生成重新评分框架，使模型能够生成忠诚和相关的回答，不需要额外的标注数据或模型调整。通过自动和人工评估，我们证明了 PICK 的效果，可以生成更忠诚的回答，同时保持与对话历史相关。此外， PICK 在所有搜索策略下都能够一直提高系统的性能，并且与oracle和检索知识相结合。我们在 GitHub 上提供了详细的实现。
</details></li>
</ul>
<hr>
<h2 id="PoSE-Efficient-Context-Window-Extension-of-LLMs-via-Positional-Skip-wise-Training"><a href="#PoSE-Efficient-Context-Window-Extension-of-LLMs-via-Positional-Skip-wise-Training" class="headerlink" title="PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training"></a>PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10400">http://arxiv.org/abs/2309.10400</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dwzhu-pku/pose">https://github.com/dwzhu-pku/pose</a></li>
<li>paper_authors: Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, Sujian Li</li>
<li>for: 用于提高大语言模型的适应性和扩展性</li>
<li>methods: 使用Positional Skip-wisE（PoSE）训练方法，通过在训练过程中随机填充各个 chunk 的位置指标来模拟长输入序列，从而适应不同的上下文窗口大小</li>
<li>results: 比较训练在全长输入上与 PoSE 训练在短 chunk 上，后者减少了内存和时间开销，而且性能减差不大。此外，PoSE 方法可以与 RoPE-based LLMs 和不同的位置插值策略兼容，并且可以无限扩展上下文窗口，具体取决于执行时间的内存使用情况。<details>
<summary>Abstract</summary>
In this paper, we introduce Positional Skip-wisE (PoSE) training for efficient adaptation of large language models~(LLMs) to extremely long context windows. PoSE decouples train length from target context window size by simulating long inputs using a fixed context window with manipulated position indices during training. Concretely, we select several short chunks from a long input sequence, and introduce distinct skipping bias terms to modify the position indices of each chunk. These bias terms, along with the length of each chunk, are altered for each training example, allowing the model to adapt to all positions within the target context window without training on full length inputs. Experiments show that, compared with fine-tuning on the full length, PoSE greatly reduces memory and time overhead with minimal impact on performance. Leveraging this advantage, we have successfully extended the LLaMA model to 128k tokens. Furthermore, we empirically confirm that PoSE is compatible with all RoPE-based LLMs and various position interpolation strategies. Notably, by decoupling fine-tuning length from target context window, PoSE can theoretically extend the context window infinitely, constrained only by memory usage for inference. With ongoing advancements for efficient inference, we believe PoSE holds great promise for scaling the context window even further.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了Positional Skip-wisE（PoSE）训练方法，用于高效地适应大语言模型（LLM）到极长上下文窗口。PoSE将训练长度与目标上下文窗口大小分离开来，在训练过程中使用固定的上下文窗口和修改位标的扰动技术来模拟长输入。具体来说，我们从长输入序列中选择一些短块，并在每个块上引入不同的跳过偏好项来修改位标。这些偏好项， junto with each chunk's length, are altered for each training example, allowing the model to adapt to all positions within the target context window without training on full-length inputs.实验表明，相比于精细调整全长输入，PoSE可以减少内存和时间开销，而无需减少性能。通过这种优势，我们成功地扩展了LLaMA模型到128k个token。此外，我们还证实了PoSE与RoPE基于LLM的各种位置 interpolate策略兼容，并且可以 theoretically extend the context window infinitely，即使在推理时使用内存。随着高效推理技术的不断发展，我们相信PoSE具有极大的扩展前途。
</details></li>
</ul>
<hr>
<h2 id="Prompt-Condition-and-Generate-Classification-of-Unsupported-Claims-with-In-Context-Learning"><a href="#Prompt-Condition-and-Generate-Classification-of-Unsupported-Claims-with-In-Context-Learning" class="headerlink" title="Prompt, Condition, and Generate: Classification of Unsupported Claims with In-Context Learning"></a>Prompt, Condition, and Generate: Classification of Unsupported Claims with In-Context Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10359">http://arxiv.org/abs/2309.10359</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Ebert Christensen, Srishti Yadav, Serge Belongie</li>
<li>for: 本研究旨在提供一种方法，可以从日常生活中遇到的不支持、不可靠的声明中提炼出一个可 COUNT的数据集，以便更好地理解和描述这些声明。</li>
<li>methods: 本研究使用了人工智能技术，特别是大语言模型（LLM），来自动生成声明，并通过Context Learning来帮助模型更好地理解和描述声明。</li>
<li>results: 研究发现，使用生成的声明可以提高 narative 分类模型的性能，并且可以使用一些训练示例来推断声明的Stand和方向。这种模型可以在应用中使用，例如 Fact-checking 等。<details>
<summary>Abstract</summary>
Unsupported and unfalsifiable claims we encounter in our daily lives can influence our view of the world. Characterizing, summarizing, and -- more generally -- making sense of such claims, however, can be challenging. In this work, we focus on fine-grained debate topics and formulate a new task of distilling, from such claims, a countable set of narratives. We present a crowdsourced dataset of 12 controversial topics, comprising more than 120k arguments, claims, and comments from heterogeneous sources, each annotated with a narrative label. We further investigate how large language models (LLMs) can be used to synthesise claims using In-Context Learning. We find that generated claims with supported evidence can be used to improve the performance of narrative classification models and, additionally, that the same model can infer the stance and aspect using a few training examples. Such a model can be useful in applications which rely on narratives , e.g. fact-checking.
</details>
<details>
<summary>摘要</summary>
日常生活中遇到的无法支持和无法证明的声明可能会影响我们对世界的看法。然而，描述、概括和更广泛地来说，对这些声明的理解可以是困难的。在这项工作中，我们关注细化的辩论话题，并提出了一项新的任务：从这些声明中提炼出一个可数的数量的故事。我们提供了一个来自多种源的12个热点话题的大量人工标注数据集，包括超过12万个Arguments、声明和评论，每个声明都被标注了故事标签。我们进一步研究了大型自然语言模型（LLM）如何在上下文学习中生成声明，并发现了以下两点：First，使用支持证明的生成声明可以提高故事分类模型的性能；Second，使用相同的模型可以在几个训练示例后推断姿态和方向。这种模型在基于故事的应用中可以是有用的，例如ifact-checking。
</details></li>
</ul>
<hr>
<h2 id="KoBigBird-large-Transformation-of-Transformer-for-Korean-Language-Understanding"><a href="#KoBigBird-large-Transformation-of-Transformer-for-Korean-Language-Understanding" class="headerlink" title="KoBigBird-large: Transformation of Transformer for Korean Language Understanding"></a>KoBigBird-large: Transformation of Transformer for Korean Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10339">http://arxiv.org/abs/2309.10339</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kisu Yang, Yoonna Jang, Taewoo Lee, Jinwoo Seong, Hyungjin Lee, Hwanseok Jang, Heuiseok Lim</li>
<li>for: 这个研究实现了一个大型的韩文BigBird模型，以获得韩文理解的State-of-the-art表现，并允许较长的序列处理。</li>
<li>methods: 我们仅将架构变更，并将统计编码扩展为我们提出的弹性统计编码表现（TAPER）。</li>
<li>results: 实验结果显示，KoBigBird-large在韩文语言理解benchmark上的总表现和文档分类和问答任务中的较长序列表现皆达到了State-of-the-art水平，并且在比较基eline模型时表现最佳。我们将这个模型公开发布。<details>
<summary>Abstract</summary>
This work presents KoBigBird-large, a large size of Korean BigBird that achieves state-of-the-art performance and allows long sequence processing for Korean language understanding. Without further pretraining, we only transform the architecture and extend the positional encoding with our proposed Tapered Absolute Positional Encoding Representations (TAPER). In experiments, KoBigBird-large shows state-of-the-art overall performance on Korean language understanding benchmarks and the best performance on document classification and question answering tasks for longer sequences against the competitive baseline models. We publicly release our model here.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Rigorously-Assessing-Natural-Language-Explanations-of-Neurons"><a href="#Rigorously-Assessing-Natural-Language-Explanations-of-Neurons" class="headerlink" title="Rigorously Assessing Natural Language Explanations of Neurons"></a>Rigorously Assessing Natural Language Explanations of Neurons</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10312">http://arxiv.org/abs/2309.10312</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hieu9955/ggggg">https://github.com/hieu9955/ggggg</a></li>
<li>paper_authors: Jing Huang, Atticus Geiger, Karel D’Oosterlinck, Zhengxuan Wu, Christopher Potts</li>
<li>for:  This paper aims to evaluate the faithfulness of natural language explanations of how large language models process and store information.</li>
<li>methods: The paper develops two modes of evaluation for natural language explanations that claim individual neurons represent a concept in a text input, including observational and intervention modes.</li>
<li>results: The paper shows that even the most confident explanations have high error rates and little to no causal efficacy, and critically assesses whether natural language is a good choice for explanations and whether neurons are the best level of analysis.<details>
<summary>Abstract</summary>
Natural language is an appealing medium for explaining how large language models process and store information, but evaluating the faithfulness of such explanations is challenging. To help address this, we develop two modes of evaluation for natural language explanations that claim individual neurons represent a concept in a text input. In the observational mode, we evaluate claims that a neuron $a$ activates on all and only input strings that refer to a concept picked out by the proposed explanation $E$. In the intervention mode, we construe $E$ as a claim that the neuron $a$ is a causal mediator of the concept denoted by $E$. We apply our framework to the GPT-4-generated explanations of GPT-2 XL neurons of Bills et al. (2023) and show that even the most confident explanations have high error rates and little to no causal efficacy. We close the paper by critically assessing whether natural language is a good choice for explanations and whether neurons are the best level of analysis.
</details>
<details>
<summary>摘要</summary>
自然语言是一种吸引人的媒介，用于解释大语言模型如何处理和存储信息，但评估这些解释的准确性具有挑战。为了解决这个问题，我们开发了两种评估模式 для自然语言解释，即 observational 模式和 intervención 模式。在 observational 模式下，我们评估laims that neuron $a$ 活动在所有和只有输入串 refer to a concept picked out by the proposed explanation $E$。在 intervención 模式下，我们理解 $E$ 为 neuron $a$ 是输入串中某种概念的 causal mediator。我们应用我们的框架到 Bills et al. (2023) 所生成的 GPT-2 XL neuron的解释中，并发现even the most confident explanations have high error rates and little to no causal efficacy。我们在 conclusion 中 kritically assessing whether natural language is a good choice for explanations and whether neurons are the best level of analysis。
</details></li>
</ul>
<hr>
<h2 id="Baichuan-2-Open-Large-scale-Language-Models"><a href="#Baichuan-2-Open-Large-scale-Language-Models" class="headerlink" title="Baichuan 2: Open Large-scale Language Models"></a>Baichuan 2: Open Large-scale Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10305">http://arxiv.org/abs/2309.10305</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/baichuan-inc/baichuan2">https://github.com/baichuan-inc/baichuan2</a></li>
<li>paper_authors: Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, JunTao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, Zhiying Wu</li>
<li>for: 这个技术报告是用于介绍一种大型多语言语言模型（Baichuan 2）的论文。</li>
<li>methods: 这个论文使用了从零开始训练的大规模多语言语言模型，共包括70亿和130亿参数。</li>
<li>results: 这个论文表明，Baichuan 2 可以与其他开源模型相比或超越它们在公共测试准则上，例如 MMLU、CMMLU、GSM8K 和 HumanEval。 此外，Baichuan 2 在医学和法律领域也表现出色。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated remarkable performance on a variety of natural language tasks based on just a few examples of natural language instructions, reducing the need for extensive feature engineering. However, most powerful LLMs are closed-source or limited in their capability for languages other than English. In this technical report, we present Baichuan 2, a series of large-scale multilingual language models containing 7 billion and 13 billion parameters, trained from scratch, on 2.6 trillion tokens. Baichuan 2 matches or outperforms other open-source models of similar size on public benchmarks like MMLU, CMMLU, GSM8K, and HumanEval. Furthermore, Baichuan 2 excels in vertical domains such as medicine and law. We will release all pre-training model checkpoints to benefit the research community in better understanding the training dynamics of Baichuan 2.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）已经展示出杰出的表现在多种自然语言任务上，仅基于几个自然语言指令，减少需要广泛的特征工程。然而，大多数最具备力 LLMs 是封闭式或仅能在英文语言上使用。在本技术报告中，我们发布了 Baichuan 2，一系列大规模多语言模型，包含 700亿和1300亿个参数，从零开始训练，使用 2.6 兆个字元。Baichuan 2 与其他开源模型相比，在公共测试 benchmark 上匹配或超越。此外，Baichuan 2 在医学和法律领域中表现出色。我们将发布所有预训练模型检查点，以便研究社区更好地理解 Baichuan 2 的训练过程。
</details></li>
</ul>
<hr>
<h2 id="Using-fine-tuning-and-min-lookahead-beam-search-to-improve-Whisper"><a href="#Using-fine-tuning-and-min-lookahead-beam-search-to-improve-Whisper" class="headerlink" title="Using fine-tuning and min lookahead beam search to improve Whisper"></a>Using fine-tuning and min lookahead beam search to improve Whisper</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10299">http://arxiv.org/abs/2309.10299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Do, Oscar Brown, Zhengjie Wang, Nikhil Mathew, Zixin Liu, Jawwad Ahmed, Cheng Yu</li>
<li>for: 提高low-resource语言中Whisper的表现</li>
<li>methods:  fine-tune Whisper on additional data and propose an improved decoding algorithm</li>
<li>results: 在越南语言上， fine-tuning Whisper-Tiny with LoRA leads to an improvement of 38.49 in WER compared to zero-shot setting, and using Filter-Ends and Min Lookahead decoding algorithms reduces WER by 2.26 on average compared to standard beam search.I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
The performance of Whisper in low-resource languages is still far from perfect. In addition to a lack of training data on low-resource languages, we identify some limitations in the beam search algorithm used in Whisper. To address these issues, we fine-tune Whisper on additional data and propose an improved decoding algorithm. On the Vietnamese language, fine-tuning Whisper-Tiny with LoRA leads to an improvement of 38.49 in WER over the zero-shot Whisper-Tiny setting which is a further reduction of 1.45 compared to full-parameter fine-tuning. Additionally, by using Filter-Ends and Min Lookahead decoding algorithms, the WER reduces by 2.26 on average over a range of languages compared to standard beam search. These results generalise to larger Whisper model sizes. We also prove a theorem that Min Lookahead outperforms the standard beam search algorithm used in Whisper.
</details>
<details>
<summary>摘要</summary>
文章提到的Whisper在低资源语言表现仍然远不完美。除了低资源语言的培训数据缺乏外，我们还发现了Whisper中的搜索算法有一些限制。为解决这些问题，我们对Whisper进行了进一步的微调和提议了一种改进的解码算法。在越南语言上，我们使用LoRA进行微调Whisper-Tiny，WER值下降38.49，比零基eline微调 setting下降1.45，而与全参数微调相比，这是进一步的下降。此外，使用Filter-Ends和Min Lookahead搜索算法，WER值平均下降2.26个语言中，相比标准搜索算法。这些结果普适到更大的Whisper模型大小。我们还证明了Min Lookahead比标准搜索算法更高效。
</details></li>
</ul>
<hr>
<h2 id="Mixed-Distil-BERT-Code-mixed-Language-Modeling-for-Bangla-English-and-Hindi"><a href="#Mixed-Distil-BERT-Code-mixed-Language-Modeling-for-Bangla-English-and-Hindi" class="headerlink" title="Mixed-Distil-BERT: Code-mixed Language Modeling for Bangla, English, and Hindi"></a>Mixed-Distil-BERT: Code-mixed Language Modeling for Bangla, English, and Hindi</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10272">http://arxiv.org/abs/2309.10272</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Nishat Raihan, Dhiman Goswami, Antara Mahmud</li>
<li>for: 这个论文主要针对的是 code-mixed NLP 挑战，即在文本中混合多种语言的问题。</li>
<li>methods: 这篇论文使用了 BERT 模型，并通过组合 synthetic 数据和实际数据进行预训练。</li>
<li>results: 论文提出了 Tri-Distil-BERT 和 Mixed-Distil-BERT 两种模型，并在多个 NLP 任务上进行了评估，与大型模型 like mBERT 和 XLM-R 进行了比较，得到了竞争力的表现。<details>
<summary>Abstract</summary>
One of the most popular downstream tasks in the field of Natural Language Processing is text classification. Text classification tasks have become more daunting when the texts are code-mixed. Though they are not exposed to such text during pre-training, different BERT models have demonstrated success in tackling Code-Mixed NLP challenges. Again, in order to enhance their performance, Code-Mixed NLP models have depended on combining synthetic data with real-world data. It is crucial to understand how the BERT models' performance is impacted when they are pretrained using corresponding code-mixed languages. In this paper, we introduce Tri-Distil-BERT, a multilingual model pre-trained on Bangla, English, and Hindi, and Mixed-Distil-BERT, a model fine-tuned on code-mixed data. Both models are evaluated across multiple NLP tasks and demonstrate competitive performance against larger models like mBERT and XLM-R. Our two-tiered pre-training approach offers efficient alternatives for multilingual and code-mixed language understanding, contributing to advancements in the field.
</details>
<details>
<summary>摘要</summary>
一种非常受欢迎的下游任务在自然语言处理领域是文本分类。由于文本受到混合的影响，文本分类任务变得更加困难。虽然模型在预训练时没有接触这种文本，但不同的BERT模型在处理混合代码的挑战中表现出色。为了提高 их表现，混合代码NLP模型通常是通过将 sintetic data 与实际数据相结合来进行优化。我们需要了解BERT模型在使用相应的混合代码语言进行预训练时的表现。在这篇论文中，我们介绍了Tri-Distil-BERT和Mixed-Distil-BERT两种模型。Tri-Distil-BERT 是多语言模型，在孟加拉语、英语和希н第语上进行预训练。Mixed-Distil-BERT 是在混合代码数据上细化的模型。两个模型在多个 NLP 任务中表现竞争性，与较大的 mBERT 和 XLM-R 模型相比。我们的两个阶段预训练方法为多语言和混合代码语言理解提供了高效的选择，对于领域的发展做出了贡献。
</details></li>
</ul>
<hr>
<h2 id="What-is-the-Best-Automated-Metric-for-Text-to-Motion-Generation"><a href="#What-is-the-Best-Automated-Metric-for-Text-to-Motion-Generation" class="headerlink" title="What is the Best Automated Metric for Text to Motion Generation?"></a>What is the Best Automated Metric for Text to Motion Generation?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10248">http://arxiv.org/abs/2309.10248</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jordan Voas, Yili Wang, Qixing Huang, Raymond Mooney</li>
<li>for: 本研究旨在系统地研究skeleton-based人体动作生成 task 的评估指标，并提出一种基于多modal BERT 模型的新指标。</li>
<li>methods: 本研究使用了多种自动评估指标，并进行了人类评估来评估指标的含义。</li>
<li>results: 研究发现，现有的评估指标与人类评估之间存在很大的误差，而新提出的指标则与人类评估呈现很高的相似性。<details>
<summary>Abstract</summary>
There is growing interest in generating skeleton-based human motions from natural language descriptions. While most efforts have focused on developing better neural architectures for this task, there has been no significant work on determining the proper evaluation metric. Human evaluation is the ultimate accuracy measure for this task, and automated metrics should correlate well with human quality judgments. Since descriptions are compatible with many motions, determining the right metric is critical for evaluating and designing effective generative models. This paper systematically studies which metrics best align with human evaluations and proposes new metrics that align even better. Our findings indicate that none of the metrics currently used for this task show even a moderate correlation with human judgments on a sample level. However, for assessing average model performance, commonly used metrics such as R-Precision and less-used coordinate errors show strong correlations. Additionally, several recently developed metrics are not recommended due to their low correlation compared to alternatives. We also introduce a novel metric based on a multimodal BERT-like model, MoBERT, which offers strongly human-correlated sample-level evaluations while maintaining near-perfect model-level correlation. Our results demonstrate that this new metric exhibits extensive benefits over all current alternatives.
</details>
<details>
<summary>摘要</summary>
“人工智能生成人体运动动作从自然语言描述中获得的兴趣在增长。大多数努力都在发展更好的神经网络模型来实现这项任务，但没有 significante 的工作关于确定合适的评价标准。人类评价是生成模型的终极准确度测试，自动化 metric 应该与人类评价呈相似性。由于描述可以与多种动作匹配，确定正确的 metric 是评价生成模型的关键。这篇论文系统地研究了哪些 metric 与人类评价最高相关，并提出了新的 metric ，它们与人类评价呈相似性。我们发现，目前用于这项任务的所有 metric 都没有even  moderate 的相关性，但是用于评估平均模型性能的 R-Precision 和 less-used coordinate errors 显示了强相关性。此外，一些最近开发的 metric 不建议使用，因为它们与替代方案相比有较低的相关性。我们还介绍了一种基于多模态 BERT-like 模型的新 metric，MoBERT，它在样本级别上与人类评价呈相似性，同时保持了near-perfect 模型级别的相关性。我们的结果表明，这种新 metric 在所有当前选择中具有广泛的优势。”
</details></li>
</ul>
<hr>
<h2 id="PolicyGPT-Automated-Analysis-of-Privacy-Policies-with-Large-Language-Models"><a href="#PolicyGPT-Automated-Analysis-of-Privacy-Policies-with-Large-Language-Models" class="headerlink" title="PolicyGPT: Automated Analysis of Privacy Policies with Large Language Models"></a>PolicyGPT: Automated Analysis of Privacy Policies with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10238">http://arxiv.org/abs/2309.10238</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenhao Tang, Zhengliang Liu, Chong Ma, Zihao Wu, Yiwei Li, Wei Liu, Dajiang Zhu, Quanzheng Li, Xiang Li, Tianming Liu, Lei Fan</li>
<li>for: This paper aims to develop a text analysis framework for privacy policies, using Large Language Models (LLM) such as ChatGPT and GPT-4.</li>
<li>methods: The framework, called PolicyGPT, uses zero-shot learning to analyze privacy policies and categorize them into 10 different classes.</li>
<li>results: PolicyGPT achieved high accuracy rates on two datasets, with an accuracy rate of 97% on the first dataset and 87% on the second dataset, outperforming baseline models.Here’s the simplified Chinese text:</li>
<li>for: 这篇论文目标是开发一种基于大语言模型（LLM）的隐私政策文本分析框架，用于自动分类隐私政策。</li>
<li>methods: 该框架使用零shot学习方法，使用大语言模型（ChatGPT和GPT-4）对隐私政策进行分类，并分为10个类别。</li>
<li>results: PolicyGPT在两个数据集上表现出色，在第一个数据集上达到了97%的准确率，在第二个数据集上达到了87%的准确率，超过了基线机器学习和神经网络模型的性能。<details>
<summary>Abstract</summary>
Privacy policies serve as the primary conduit through which online service providers inform users about their data collection and usage procedures. However, in a bid to be comprehensive and mitigate legal risks, these policy documents are often quite verbose. In practical use, users tend to click the Agree button directly rather than reading them carefully. This practice exposes users to risks of privacy leakage and legal issues. Recently, the advent of Large Language Models (LLM) such as ChatGPT and GPT-4 has opened new possibilities for text analysis, especially for lengthy documents like privacy policies. In this study, we investigate a privacy policy text analysis framework PolicyGPT based on the LLM. This framework was tested using two datasets. The first dataset comprises of privacy policies from 115 websites, which were meticulously annotated by legal experts, categorizing each segment into one of 10 classes. The second dataset consists of privacy policies from 304 popular mobile applications, with each sentence manually annotated and classified into one of another 10 categories. Under zero-shot learning conditions, PolicyGPT demonstrated robust performance. For the first dataset, it achieved an accuracy rate of 97%, while for the second dataset, it attained an 87% accuracy rate, surpassing that of the baseline machine learning and neural network models.
</details>
<details>
<summary>摘要</summary>
Recently, the development of Large Language Models (LLM) such as ChatGPT and GPT-4 has opened up new possibilities for text analysis, particularly for lengthy documents like privacy policies. In this study, we investigate a privacy policy text analysis framework called PolicyGPT, which is based on an LLM.We tested PolicyGPT using two datasets. The first dataset consisted of privacy policies from 115 websites, which were carefully annotated by legal experts and categorized into one of 10 classes. The second dataset included privacy policies from 304 popular mobile applications, with each sentence manually annotated and classified into one of another 10 categories.Under zero-shot learning conditions, PolicyGPT demonstrated robust performance. For the first dataset, it achieved an accuracy rate of 97%, while for the second dataset, it attained an 87% accuracy rate, outperforming baseline machine learning and neural network models.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/19/cs.CL_2023_09_19/" data-id="closbroml00bd0g884y3x2baw" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/19/cs.LG_2023_09_19/" class="article-date">
  <time datetime="2023-09-19T10:00:00.000Z" itemprop="datePublished">2023-09-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/19/cs.LG_2023_09_19/">cs.LG - 2023-09-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="DPpack-An-R-Package-for-Differentially-Private-Statistical-Analysis-and-Machine-Learning"><a href="#DPpack-An-R-Package-for-Differentially-Private-Statistical-Analysis-and-Machine-Learning" class="headerlink" title="DPpack: An R Package for Differentially Private Statistical Analysis and Machine Learning"></a>DPpack: An R Package for Differentially Private Statistical Analysis and Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10965">http://arxiv.org/abs/2309.10965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Spencer Giddens, Fang Liu</li>
<li>for: 这篇论文旨在提供一个开源的R包DPpack，用于保证个人隐私when分析数据。</li>
<li>methods: 本论文使用了三种流行的隐私保护机制：卷积、 Gaussian 和 exponential。此外，DPpack还提供了许多privacy-preserving描述统计函数，如mean、variance、covariance和quantiles，以及分布图和相关表。</li>
<li>results: DPpack提供了一个 user-friendly 的隐私保护版本的 logistic regression、SVM 和线性回归，以及隐私保护的模型参数调整。这些实现的隐私保护统计和机器学习技术，使得通常进行的统计分析中可以轻松地应用隐私保护原则。<details>
<summary>Abstract</summary>
Differential privacy (DP) is the state-of-the-art framework for guaranteeing privacy for individuals when releasing aggregated statistics or building statistical/machine learning models from data. We develop the open-source R package DPpack that provides a large toolkit of differentially private analysis. The current version of DPpack implements three popular mechanisms for ensuring DP: Laplace, Gaussian, and exponential. Beyond that, DPpack provides a large toolkit of easily accessible privacy-preserving descriptive statistics functions. These include mean, variance, covariance, and quantiles, as well as histograms and contingency tables. Finally, DPpack provides user-friendly implementation of privacy-preserving versions of logistic regression, SVM, and linear regression, as well as differentially private hyperparameter tuning for each of these models. This extensive collection of implemented differentially private statistics and models permits hassle-free utilization of differential privacy principles in commonly performed statistical analysis. We plan to continue developing DPpack and make it more comprehensive by including more differentially private machine learning techniques, statistical modeling and inference in the future.
</details>
<details>
<summary>摘要</summary>
differential privacy (DP) 是当今最先进的隐私保护框架，用于保护数据分析时个人隐私。我们开发了一个开源的 R 包 DPpack，该包提供了丰富的涉及扩展的隐私保护分析工具。现版本的 DPpack 实现了三种流行的隐私保护机制：拉пла斯、高斯和指数。此外，DPpack 还提供了许多易于访问的隐私保护描述统计函数，包括平均值、方差、covariance 和分位数，以及 histogram 和 conditional tables。最后，DPpack 提供了用户友好的实现隐私保护版本的 logistic regression、支持向量机器学习和线性回归，以及隐私保护参数优化 для每一种模型。这种广泛的实现的涉及隐私保护统计和机器学习技术，使得使用隐私保护原则在常见的统计分析中免受困惑。我们计划继续开发 DPpack，以便在未来包括更多的隐私保护机器学习技术、统计模型和推理。
</details></li>
</ul>
<hr>
<h2 id="Deep-Reinforcement-Learning-for-Infinite-Horizon-Mean-Field-Problems-in-Continuous-Spaces"><a href="#Deep-Reinforcement-Learning-for-Infinite-Horizon-Mean-Field-Problems-in-Continuous-Spaces" class="headerlink" title="Deep Reinforcement Learning for Infinite Horizon Mean Field Problems in Continuous Spaces"></a>Deep Reinforcement Learning for Infinite Horizon Mean Field Problems in Continuous Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10953">http://arxiv.org/abs/2309.10953</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Angiuli, Jean-Pierre Fouque, Ruimeng Hu, Alan Raydan</li>
<li>for: The paper is written to develop and analyze a reinforcement learning algorithm for solving continuous-space mean field game and mean field control problems in a unified manner.</li>
<li>methods: The proposed algorithm uses the actor-critic paradigm with a parameterized score function to represent the mean field distribution, and updates the AC agent and the score function iteratively to converge to the MFG equilibrium or the MFC optimum. Langevin dynamics are used to obtain samples from the resulting distribution.</li>
<li>results: The performance of the algorithm is evaluated using linear-quadratic benchmarks in the asymptotic infinite horizon framework, and the results show that the algorithm is able to find the optimal solution to the mean field control problem.Here’s the same information in Simplified Chinese text:</li>
<li>for: 该文章是为了开发和分析一种基于actor-critic（AC）算法的含义场游戏和含义场控制问题的解决方案。</li>
<li>methods: 提议的算法使用AC模式与参数化得分函数来表示含义场分布，并在线上更新这个分布，使用朗格文动力来获取样本。AC代理和得分函数在迭代更新以达到MFG均衡或MFC优化。</li>
<li>results: 文章中的性能分析使用线性-квадратиче benchmark在极限无穷远 horizon框架下，结果显示该算法可以解决含义场控制问题的优化问题。<details>
<summary>Abstract</summary>
We present the development and analysis of a reinforcement learning (RL) algorithm designed to solve continuous-space mean field game (MFG) and mean field control (MFC) problems in a unified manner. The proposed approach pairs the actor-critic (AC) paradigm with a representation of the mean field distribution via a parameterized score function, which can be efficiently updated in an online fashion, and uses Langevin dynamics to obtain samples from the resulting distribution. The AC agent and the score function are updated iteratively to converge, either to the MFG equilibrium or the MFC optimum for a given mean field problem, depending on the choice of learning rates. A straightforward modification of the algorithm allows us to solve mixed mean field control games (MFCGs). The performance of our algorithm is evaluated using linear-quadratic benchmarks in the asymptotic infinite horizon framework.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种基于 actor-critic（AC）方法的强化学习算法，用于解决连续空间的mean field game（MFG）和mean field control（MFC）问题。我们的方法通过使用参数化的分数函数来表示mean field分布，可以高效地在在线模式下更新，并使用朗格文动力学来获取该分布中的样本。AC Agent和分数函数在每次更新后会趋于MFG均衡或MFC优化点，具体取决于学习率。我们还提出了一种简单的修改，使得我们的算法可以解决混合mean field控制游戏（MFCG）。我们对linear-quadratic benchmark在无穷远距离框架中进行了性能评估。
</details></li>
</ul>
<hr>
<h2 id="Test-Time-Training-for-Speech"><a href="#Test-Time-Training-for-Speech" class="headerlink" title="Test-Time Training for Speech"></a>Test-Time Training for Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10930">http://arxiv.org/abs/2309.10930</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aastha2104/Parkinson-Disease-Prediction">https://github.com/Aastha2104/Parkinson-Disease-Prediction</a></li>
<li>paper_authors: Sri Harsha Dumpala, Chandramouli Sastry, Sageev Oore</li>
<li>for: 本研究探讨了应用测试时训练（Test-Time Training，TTT）解决speech应用中的分布转移问题。</li>
<li>methods: 我们引入了标准speech分类任务的测试集中的分布转移，并explored how TTT可以适应这种分布转移。我们的实验包括后声和自然语音的变化（如性别和年龄）引入分布转移。</li>
<li>results: 我们发现了一些关键挑战，包括优化器参数的敏感性（例如数量化步骤和选择参数的TTT）和可扩展性（每个示例都需要自己的参数）。我们提议使用BitFit，一种效率高的参数细化算法，并证明它在TTT中更稳定 than全模型参数细化。<details>
<summary>Abstract</summary>
In this paper, we study the application of Test-Time Training (TTT) as a solution to handling distribution shifts in speech applications. In particular, we introduce distribution-shifts to the test datasets of standard speech-classification tasks -- for example, speaker-identification and emotion-detection -- and explore how Test-Time Training (TTT) can help adjust to the distribution-shift. In our experiments that include distribution shifts due to background noise and natural variations in speech such as gender and age, we identify some key-challenges with TTT including sensitivity to optimization hyperparameters (e.g., number of optimization steps and subset of parameters chosen for TTT) and scalability (e.g., as each example gets its own set of parameters, TTT is not scalable). Finally, we propose using BitFit -- a parameter-efficient fine-tuning algorithm proposed for text applications that only considers the bias parameters for fine-tuning -- as a solution to the aforementioned challenges and demonstrate that it is consistently more stable than fine-tuning all the parameters of the model.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了在语音应用中使用测试时训练（TTT）解决发布分布变化的问题。我们特别是在标准语音分类任务的测试集中引入分布变化，例如 speaker-identification 和 emotion-detection，并探索了 TTT 如何适应这些分布变化。在我们的实验中，包括背景噪音和自然语音变化（如性别和年龄）的分布变化，我们发现了一些关键挑战，如优化器参数的敏感性（例如优化步数和选择的参数个数）以及可扩展性（例如每个示例都需要自己的参数）。最后，我们提议使用 BitFit，一种参数效率的精度调整算法，解决这些挑战，并证明它在文本应用中具有更高的稳定性。
</details></li>
</ul>
<hr>
<h2 id="Posterior-Contraction-Rates-for-Matern-Gaussian-Processes-on-Riemannian-Manifolds"><a href="#Posterior-Contraction-Rates-for-Matern-Gaussian-Processes-on-Riemannian-Manifolds" class="headerlink" title="Posterior Contraction Rates for Matérn Gaussian Processes on Riemannian Manifolds"></a>Posterior Contraction Rates for Matérn Gaussian Processes on Riemannian Manifolds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10918">http://arxiv.org/abs/2309.10918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Rosa, Viacheslav Borovitskiy, Alexander Terenin, Judith Rousseau</li>
<li>for: 本研究旨在探讨 Whether intrinsic geometric Gaussian processes (GPs) can lead to better performance compared to embedding all relevant quantities into $\mathbb{R}^d$ and using the restriction of an ordinary Euclidean GP.</li>
<li>methods: 本研究使用了Optimal contraction rates for intrinsic Mat&#39;ern GPs defined on compact Riemannian manifolds, as well as trace and extension theorems between manifold and ambient Sobolev spaces.</li>
<li>results: 研究发现，对于合适的细分参数，intrinsic GPs can achieve better performance than embedding all relevant quantities into $\mathbb{R}^d$ and using the restriction of an ordinary Euclidean GP. This result is demonstrated empirically on a number of examples.<details>
<summary>Abstract</summary>
Gaussian processes are used in many machine learning applications that rely on uncertainty quantification. Recently, computational tools for working with these models in geometric settings, such as when inputs lie on a Riemannian manifold, have been developed. This raises the question: can these intrinsic models be shown theoretically to lead to better performance, compared to simply embedding all relevant quantities into $\mathbb{R}^d$ and using the restriction of an ordinary Euclidean Gaussian process? To study this, we prove optimal contraction rates for intrinsic Mat\'ern Gaussian processes defined on compact Riemannian manifolds. We also prove analogous rates for extrinsic processes using trace and extension theorems between manifold and ambient Sobolev spaces: somewhat surprisingly, the rates obtained turn out to coincide with those of the intrinsic processes, provided that their smoothness parameters are matched appropriately. We illustrate these rates empirically on a number of examples, which, mirroring prior work, show that intrinsic processes can achieve better performance in practice. Therefore, our work shows that finer-grained analyses are needed to distinguish between different levels of data-efficiency of geometric Gaussian processes, particularly in settings which involve small data set sizes and non-asymptotic behavior.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Crypto’Graph-Leveraging-Privacy-Preserving-Distributed-Link-Prediction-for-Robust-Graph-Learning"><a href="#Crypto’Graph-Leveraging-Privacy-Preserving-Distributed-Link-Prediction-for-Robust-Graph-Learning" class="headerlink" title="Crypto’Graph: Leveraging Privacy-Preserving Distributed Link Prediction for Robust Graph Learning"></a>Crypto’Graph: Leveraging Privacy-Preserving Distributed Link Prediction for Robust Graph Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10890">http://arxiv.org/abs/2309.10890</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sofiane Azogagh, Zelma Aubin Birba, Sébastien Gambs, Marc-Olivier Killijian</li>
<li>For: 针对分布式图的隐私保护和敏感数据分享* Methods: 使用 cryptographic primitives 进行隐私保护，不需要披露个别党所拥有的图结构，却可以 computed 新的连接likelihood* Results: 能够实现高精度预测和防范图中毒攻击<details>
<summary>Abstract</summary>
Graphs are a widely used data structure for collecting and analyzing relational data. However, when the graph structure is distributed across several parties, its analysis is particularly challenging. In particular, due to the sensitivity of the data each party might want to keep their partial knowledge of the graph private, while still willing to collaborate with the other parties for tasks of mutual benefit, such as data curation or the removal of poisoned data. To address this challenge, we propose Crypto'Graph, an efficient protocol for privacy-preserving link prediction on distributed graphs. More precisely, it allows parties partially sharing a graph with distributed links to infer the likelihood of formation of new links in the future. Through the use of cryptographic primitives, Crypto'Graph is able to compute the likelihood of these new links on the joint network without revealing the structure of the private individual graph of each party, even though they know the number of nodes they have, since they share the same graph but not the same links. Crypto'Graph improves on previous works by enabling the computation of a certain number of similarity metrics without any additional cost. The use of Crypto'Graph is illustrated for defense against graph poisoning attacks, in which it is possible to identify potential adversarial links without compromising the privacy of the graphs of individual parties. The effectiveness of Crypto'Graph in mitigating graph poisoning attacks and achieving high prediction accuracy on a graph neural network node classification task is demonstrated through extensive experimentation on a real-world dataset.
</details>
<details>
<summary>摘要</summary>
GRAPHs 是一种广泛使用的数据结构，用于收集和分析关系数据。然而，当 GRAPH 结构分布在多个方面时，其分析变得特别困难。具体来说，由于每个方面可能想保持自己的部分 GRAPH 私有知识，而同时愿意与其他方面合作，例如数据整理或毒素数据的去除。为解决这个挑战，我们提议 Crypto'Graph，一种高效的隐私保护链接预测协议。更准确地说，它允许方面分享部分 GRAPH 的链接来预测未来新链接的可能性。通过使用 криптографических原则，Crypto'Graph 可以在共同网络上计算新链接的可能性，而不需要披露每个方面的私有 GRAPH 结构，即使它们知道它们拥有的节点数量。Crypto'Graph 在前一些工作的基础上进一步提高了可计算的相似指标数量，而不添加任何成本。使用 Crypto'Graph 可以防止 GRAPH 毒素攻击，并在一个实际 dataset 上进行了广泛的实验，证明了它的有效性。
</details></li>
</ul>
<hr>
<h2 id="Dynamical-Tests-of-a-Deep-Learning-Weather-Prediction-Model"><a href="#Dynamical-Tests-of-a-Deep-Learning-Weather-Prediction-Model" class="headerlink" title="Dynamical Tests of a Deep-Learning Weather Prediction Model"></a>Dynamical Tests of a Deep-Learning Weather Prediction Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10867">http://arxiv.org/abs/2309.10867</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gregory J. Hakim, Sanjit Masanam</li>
<li>for: 本研究用一种深度学习天气预测模型来测试physical laws的实用性。</li>
<li>methods: 研究者使用了一种名为Pangu-weather的深度学习模型，并对其进行了四种 класси型动力学实验。</li>
<li>results: 研究者发现，这种模型在不同的情况下都能够表现出真实的物理特性，包括热带高压系统、温带低压系统和极低压系统的形成等。<details>
<summary>Abstract</summary>
Global deep-learning weather prediction models have recently been shown to produce forecasts that rival those from physics-based models run at operational centers. It is unclear whether these models have encoded atmospheric dynamics, or simply pattern matching that produces the smallest forecast error. Answering this question is crucial to establishing the utility of these models as tools for basic science. Here we subject one such model, Pangu-weather, to a set of four classical dynamical experiments that do not resemble the model training data. Localized perturbations to the model output and the initial conditions are added to steady time-averaged conditions, to assess the propagation speed and structural evolution of signals away from the local source. Perturbing the model physics by adding a steady tropical heat source results in a classical Matsuno--Gill response near the heating, and planetary waves that radiate into the extratropics. A localized disturbance on the winter-averaged North Pacific jet stream produces realistic extratropical cyclones and fronts, including the spontaneous emergence of polar lows. Perturbing the 500hPa height field alone yields adjustment from a state of rest to one of wind--pressure balance over ~6 hours. Localized subtropical low pressure systems produce Atlantic hurricanes, provided the initial amplitude exceeds about 5 hPa, and setting the initial humidity to zero eliminates hurricane development. We conclude that the model encodes realistic physics in all experiments, and suggest it can be used as a tool for rapidly testing ideas before using expensive physics-based models.
</details>
<details>
<summary>摘要</summary>
全球深度学习天气预测模型最近已经能够生成与物理基础模型运行中心的预测相当的forecast。然而，这些模型是通过编码大气动力学或 simply模式匹配来生成预测错误最小化的。解决这个问题是确定这些模型是否有用作基础科学工具的关键。在这里，我们使用一个名为Pangu-weather的模型进行四种经典动力学实验，这些实验不同于模型训练数据。我们在稳定时间均值条件下添加了本地扰动和初始条件，以评估信号径向速度和结构的发展。在添加了热带热源后，模型物理学会出现经典的Matsuno--Gill响应，以及在极地射线方向射出的 планет� waves。在冬季平均北太平洋液压流上添加了本地扰动，可以生成真实的温带风暴和前线，包括自发性产生的极地低压系统。在500hPa高程场 alone 上进行调整，可以在 ~6小时内从一种平衡状态变换到一种风压平衡状态。当初始气压值大于5 hPa，并将初始湿度设置为0时，可以生成 Atlantics 风暴。我们 conclude that这些模型编码了实际物理学，并可以用作快速测试想法之前使用昂贵的物理基础模型。
</details></li>
</ul>
<hr>
<h2 id="O-k-Equivariant-Dimensionality-Reduction-on-Stiefel-Manifolds"><a href="#O-k-Equivariant-Dimensionality-Reduction-on-Stiefel-Manifolds" class="headerlink" title="$O(k)$-Equivariant Dimensionality Reduction on Stiefel Manifolds"></a>$O(k)$-Equivariant Dimensionality Reduction on Stiefel Manifolds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10775">http://arxiv.org/abs/2309.10775</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/harlinlee/psc">https://github.com/harlinlee/psc</a></li>
<li>paper_authors: Andrew Lee, Harlin Lee, Jose A. Perea, Nikolas Schonsheck, Madeleine Weinstein</li>
<li>for: 本研究的目的是对高维度的几何空间资料进行降维，以提高分析和检测的效率。</li>
<li>methods: 本研究提出了一个名为几何主成分对称coordinate（PSC）的算法，它可以将资料从$V_k(\mathbb{R}^N)$降维到$V_k(\mathbb{R}^n)$，并且保持了$O(k)$-对称性。</li>
<li>results: 本研究通过多个实验表明，PSC算法可以对高维度资料进行有效的降维，并且可以提高分析和检测的效率。另外，研究也显示了PSC算法在不同的数据集中的表现。<details>
<summary>Abstract</summary>
Many real-world datasets live on high-dimensional Stiefel and Grassmannian manifolds, $V_k(\mathbb{R}^N)$ and $Gr(k, \mathbb{R}^N)$ respectively, and benefit from projection onto lower-dimensional Stiefel (respectively, Grassmannian) manifolds. In this work, we propose an algorithm called Principal Stiefel Coordinates (PSC) to reduce data dimensionality from $ V_k(\mathbb{R}^N)$ to $V_k(\mathbb{R}^n)$ in an $O(k)$-equivariant manner ($k \leq n \ll N$). We begin by observing that each element $\alpha \in V_n(\mathbb{R}^N)$ defines an isometric embedding of $V_k(\mathbb{R}^n)$ into $V_k(\mathbb{R}^N)$. Next, we optimize for such an embedding map that minimizes data fit error by warm-starting with the output of principal component analysis (PCA) and applying gradient descent. Then, we define a continuous and $O(k)$-equivariant map $\pi_\alpha$ that acts as a ``closest point operator'' to project the data onto the image of $V_k(\mathbb{R}^n)$ in $V_k(\mathbb{R}^N)$ under the embedding determined by $\alpha$, while minimizing distortion. Because this dimensionality reduction is $O(k)$-equivariant, these results extend to Grassmannian manifolds as well. Lastly, we show that the PCA output globally minimizes projection error in a noiseless setting, but that our algorithm achieves a meaningfully different and improved outcome when the data does not lie exactly on the image of a linearly embedded lower-dimensional Stiefel manifold as above. Multiple numerical experiments using synthetic and real-world data are performed.
</details>
<details>
<summary>摘要</summary>
许多实际数据集生活在高维度Stiefel和Grassmannian manifolds上，即$V_k(\mathbb{R}^N)$和$Gr(k, \mathbb{R}^N)$ respectively，并且受益于降维到lower-dimensional Stiefel（respectively, Grassmannian） manifolds上的投影。在这种工作中，我们提出了一种名为Principal Stiefel Coordinates（PSC）的算法，用于从$V_k(\mathbb{R}^N)$降维到$V_k(\mathbb{R}^n)$，并且在$O(k)$-equivariant manner下进行。我们开始 Observation that each element $\alpha \in V_n(\mathbb{R}^N)$ defines an isometric embedding of $V_k(\mathbb{R}^n)$ into $V_k(\mathbb{R}^N)$. Next, we optimize for such an embedding map that minimizes data fit error by warm-starting with the output of principal component analysis (PCA) and applying gradient descent. Then, we define a continuous and $O(k)$-equivariant map $\pi_\alpha$ that acts as a "closest point operator" to project the data onto the image of $V_k(\mathbb{R}^n)$ in $V_k(\mathbb{R}^N)$ under the embedding determined by $\alpha$, while minimizing distortion. Because this dimensionality reduction is $O(k)$-equivariant, these results extend to Grassmannian manifolds as well. Lastly, we show that the PCA output globally minimizes projection error in a noiseless setting, but that our algorithm achieves a meaningfully different and improved outcome when the data does not lie exactly on the image of a linearly embedded lower-dimensional Stiefel manifold as above. Multiple numerical experiments using synthetic and real-world data are performed.
</details></li>
</ul>
<hr>
<h2 id="Semi-supervised-Domain-Adaptation-in-Graph-Transfer-Learning"><a href="#Semi-supervised-Domain-Adaptation-in-Graph-Transfer-Learning" class="headerlink" title="Semi-supervised Domain Adaptation in Graph Transfer Learning"></a>Semi-supervised Domain Adaptation in Graph Transfer Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10773">http://arxiv.org/abs/2309.10773</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/daiquanyu/AdaGCN_TKDE">https://github.com/daiquanyu/AdaGCN_TKDE</a></li>
<li>paper_authors: Ziyue Qiao, Xiao Luo, Meng Xiao, Hao Dong, Yuanchun Zhou, Hui Xiong</li>
<li>for: 这个研究目的是为了实现不监控的类别转移学习在图形上，将标签集中的知识转移到无标签的目标图形上。</li>
<li>methods: 我们提出了一个方法named Semi-supervised Graph Domain Adaptation (SGDA)，对抗域shift和标签稀缺的难题。我们在源图形中添加了 adaptive shift 参数，通过在反对抗模式下训练来对图形端点进行对齐，从而将源图形中训练的类别标签转移到目标图形上。此外，我们还提出了伪标签的方法，通过量测缺乏标签的范例中的后天影响，从而提高目标图形上的类别分类精度。</li>
<li>results: 我们在一些公开的数据集上进行了广泛的实验，证明了我们的提出的SGDA在不同的实验设定下的效果。<details>
<summary>Abstract</summary>
As a specific case of graph transfer learning, unsupervised domain adaptation on graphs aims for knowledge transfer from label-rich source graphs to unlabeled target graphs. However, graphs with topology and attributes usually have considerable cross-domain disparity and there are numerous real-world scenarios where merely a subset of nodes are labeled in the source graph. This imposes critical challenges on graph transfer learning due to serious domain shifts and label scarcity. To address these challenges, we propose a method named Semi-supervised Graph Domain Adaptation (SGDA). To deal with the domain shift, we add adaptive shift parameters to each of the source nodes, which are trained in an adversarial manner to align the cross-domain distributions of node embedding, thus the node classifier trained on labeled source nodes can be transferred to the target nodes. Moreover, to address the label scarcity, we propose pseudo-labeling on unlabeled nodes, which improves classification on the target graph via measuring the posterior influence of nodes based on their relative position to the class centroids. Finally, extensive experiments on a range of publicly accessible datasets validate the effectiveness of our proposed SGDA in different experimental settings.
</details>
<details>
<summary>摘要</summary>
为特例的图转移学习，无监督领域适应图的目标是将标签丰富的源图知识传播到无标注目标图。然而，图像结构和特征通常存在跨领域差异，世界上有许多实际情况下只有源图中的一个节点被标注。这些挑战使得图转移学习受到了严重的领域shift和标签稀缺的挑战。为解决这些挑战，我们提出了半监督图领域适应方法（SGDA）。为了处理领域shift，我们在源节点中添加了适应参数，这些参数在对抗式的训练中使得源节点的领域分布与目标节点的领域分布相似，从而使得源节点的类别推论器可以被传递到目标节点。此外，为了解决标签稀缺的问题，我们提出了假标注方法，通过测量每个节点的后向影响，根据节点的相对位置来提高目标图的分类精度。最后，我们在一系列公共访问的数据集上进行了广泛的实验，证明了我们提出的SGDA在不同的实际情况下的效果。
</details></li>
</ul>
<hr>
<h2 id="Improving-Opioid-Use-Disorder-Risk-Modelling-through-Behavioral-and-Genetic-Feature-Integration"><a href="#Improving-Opioid-Use-Disorder-Risk-Modelling-through-Behavioral-and-Genetic-Feature-Integration" class="headerlink" title="Improving Opioid Use Disorder Risk Modelling through Behavioral and Genetic Feature Integration"></a>Improving Opioid Use Disorder Risk Modelling through Behavioral and Genetic Feature Integration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10837">http://arxiv.org/abs/2309.10837</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bayesomicslab/OUD-Risk-Prediction">https://github.com/bayesomicslab/OUD-Risk-Prediction</a></li>
<li>paper_authors: Sybille Légitime, Kaustubh Prabhu, Devin McConnell, Bing Wang, Dipak K. Dey, Derek Aguiar</li>
<li>for: 预测抑菌药用症（OUD）的风险，以提高治疗方案、监测计划和干预策略的效果。</li>
<li>methods:  combines 基因变异与行为特征（从GPS和Wi-Fi坐标中提取的空间时间特征）来评估OUD风险。发展了算法来（1）生成行为特征从实际分布中，（2）合成行业和基因样本，假设潜在的共同病因和相对风险。</li>
<li>results: 结果表明，结合行业和基因特征可以提高风险评估，并且行业特征对OUD风险的影响更大，尽管基因贡献也是显著的，特别是在线性模型中。但是需要考虑隐私、安全、偏见和普遍性问题，才能在临床试验中评估这种方法的可行性。<details>
<summary>Abstract</summary>
Opioids are an effective analgesic for acute and chronic pain, but also carry a considerable risk of addiction leading to millions of opioid use disorder (OUD) cases and tens of thousands of premature deaths in the United States yearly. Estimating OUD risk prior to prescription could improve the efficacy of treatment regimens, monitoring programs, and intervention strategies, but risk estimation is typically based on self-reported data or questionnaires. We develop an experimental design and computational methods that combines genetic variants associated with OUD with behavioral features extracted from GPS and Wi-Fi spatiotemporal coordinates to assess OUD risk. Since both OUD mobility and genetic data do not exist for the same cohort, we develop algorithms to (1) generate mobility features from empirical distributions and (2) synthesize mobility and genetic samples assuming a level of comorbidity and relative risks. We show that integrating genetic and mobility modalities improves risk modelling using classification accuracy, area under the precision-recall and receiver operator characteristic curves, and $F_1$ score. Interpreting the fitted models suggests that mobility features have more influence on OUD risk, although the genetic contribution was significant, particularly in linear models. While there exists concerns with respect to privacy, security, bias, and generalizability that must be evaluated in clinical trials before being implemented in practice, our framework provides preliminary evidence that behavioral and genetic features may improve OUD risk estimation to assist with personalized clinical decision-making.
</details>
<details>
<summary>摘要</summary>
吗 Opioids 是一种有效的疼痛镇静药物，但也会导致数百万例的 Opioid 使用障碍 (OUD) 和数千例的危险死亡在美国每年。 估计 OUD 风险前置于订scriptions可以提高治疗方案、监测计划和干预策略的效果，但风险估计通常基于自我报告数据或问卷。 我们开发了一种实验设计和计算方法，结合 associates with OUD 的遗传变异和从 GPS 和 Wi-Fi 空间时间坐标提取的行为特征来评估 OUD 风险。 因为 OUD  mobilit和遗传数据不同层次, we develop algorithms to (1) generate mobilit features from empirical distributions and (2) synthesize mobilit and genetic samples assuming a level of comorbidity and relative risks. We show that integrating genetic and mobilit modalities improves risk modelling using classification accuracy, area under the precision-recall and receiver operator characteristic curves, and $F_1$ score. Interpreting the fitted models suggests that mobility features have more influence on OUD risk, although the genetic contribution was significant, particularly in linear models. While there exists concerns with respect to privacy, security, bias, and generalizability that must be evaluated in clinical trials before being implemented in practice, our framework provides preliminary evidence that behavioral and genetic features may improve OUD risk estimation to assist with personalized clinical decision-making.
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Diffusion-Based-Text-to-Audio-Generation-with-Consistency-Distillation"><a href="#Accelerating-Diffusion-Based-Text-to-Audio-Generation-with-Consistency-Distillation" class="headerlink" title="Accelerating Diffusion-Based Text-to-Audio Generation with Consistency Distillation"></a>Accelerating Diffusion-Based Text-to-Audio Generation with Consistency Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10740">http://arxiv.org/abs/2309.10740</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yatong Bai, Trung Dang, Dung Tran, Kazuhito Koishida, Somayeh Sojoudi</li>
<li>for: 实时文本转语音生成（TTA）方法的发展</li>
<li>methods: 使用修改了最近提出的一致混合激活架构，将TTA模型训练为仅需单一神经网络查询</li>
<li>results: 在AudioCaps dataset上进行了实验，发现这些模型可以保持 diffusion models 的高质量和多样性，并且降低了查询次数，每次查询可以减少到400次。<details>
<summary>Abstract</summary>
Diffusion models power a vast majority of text-to-audio (TTA) generation methods. Unfortunately, these models suffer from slow inference speed due to iterative queries to the underlying denoising network, thus unsuitable for scenarios with inference time or computational constraints. This work modifies the recently proposed consistency distillation framework to train TTA models that require only a single neural network query. In addition to incorporating classifier-free guidance into the distillation process, we leverage the availability of generated audio during distillation training to fine-tune the consistency TTA model with novel loss functions in the audio space, such as the CLAP score. Our objective and subjective evaluation results on the AudioCaps dataset show that consistency models retain diffusion models' high generation quality and diversity while reducing the number of queries by a factor of 400.
</details>
<details>
<summary>摘要</summary>
多种扩散模型通常用于文本到语音（TTA）生成方法中。然而，这些模型受到迭代查询到底层减噪网络的限制，导致推理速度慢，不适合具有推理时间或计算限制的场景。本工作改进了最近提出的一致性熔化框架，以训练不需要多个神经网络查询的 TTA 模型。此外，我们还在熔化训练过程中 incorporate 类ifier-free 指导，并利用生成的音频数据来细化熔化 TTA 模型，使用 novel 的损失函数，如 CLAP 分数。我们对 AudioCaps 数据集进行了对象和主观评估，结果表明，一致性模型可以保持扩散模型的高质量和多样性，同时减少查询数量，比例为 400 倍。
</details></li>
</ul>
<hr>
<h2 id="Mixture-Weight-Estimation-and-Model-Prediction-in-Multi-source-Multi-target-Domain-Adaptation"><a href="#Mixture-Weight-Estimation-and-Model-Prediction-in-Multi-source-Multi-target-Domain-Adaptation" class="headerlink" title="Mixture Weight Estimation and Model Prediction in Multi-source Multi-target Domain Adaptation"></a>Mixture Weight Estimation and Model Prediction in Multi-source Multi-target Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10736">http://arxiv.org/abs/2309.10736</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuyang Deng, Ilja Kuzborskij, Mehrdad Mahdavi</li>
<li>for: 本文主要针对多种不同来源数据的学习问题，目标是在新的目标分布上表现良好。</li>
<li>methods: 本文使用了一种新的混合学习方法，可以同时适应多个目标分布，并且可以在 computationally efficient 的方式下解决多个目标分布的采样问题。</li>
<li>results: 本文提出了一种新的混合学习方法，可以有效地解决多个目标分布的学习问题，并且可以在线上和离线上实现高效的学习。<details>
<summary>Abstract</summary>
We consider the problem of learning a model from multiple heterogeneous sources with the goal of performing well on a new target distribution. The goal of learner is to mix these data sources in a target-distribution aware way and simultaneously minimize the empirical risk on the mixed source. The literature has made some tangible advancements in establishing theory of learning on mixture domain. However, there are still two unsolved problems. Firstly, how to estimate the optimal mixture of sources, given a target domain; Secondly, when there are numerous target domains, how to solve empirical risk minimization (ERM) for each target using possibly unique mixture of data sources in a computationally efficient manner. In this paper we address both problems efficiently and with guarantees. We cast the first problem, mixture weight estimation, as a convex-nonconcave compositional minimax problem, and propose an efficient stochastic algorithm with provable stationarity guarantees. Next, for the second problem, we identify that for certain regimes, solving ERM for each target domain individually can be avoided, and instead parameters for a target optimal model can be viewed as a non-linear function on a space of the mixture coefficients. Building upon this, we show that in the offline setting, a GD-trained overparameterized neural network can provably learn such function to predict the model of target domain instead of solving a designated ERM problem. Finally, we also consider an online setting and propose a label efficient online algorithm, which predicts parameters for new targets given an arbitrary sequence of mixing coefficients, while enjoying regret guarantees.
</details>
<details>
<summary>摘要</summary>
我们考虑一个从多个不同来源学习模型的问题，目的是在新的目标分布上表现良好。学习者需要将这些数据源混合在目标分布意识的方式下，同时降低混合后的观察风险。文献已经做出了一些可观的进展，但还有两个未解决的问题。第一个问题是，给定目标分布，如何估计最佳混合比例？第二个问题是，当有多个目标分布时，如何使用可能不同的数据源混合来解决每个目标的Empirical Risk Minimization（ERM）问题，并且在计算效率上具有保证？在这篇论文中，我们efficiently和有保证地解决了这两个问题。我们将第一个问题，混合比例估计，转化为一个凸-非凸 Compositional Minimax问题，并提出了一种效果的杂化算法，具有可观的站点性保证。接下来，我们发现在某些情况下，可以避免解决每个目标分布的ERM问题，而是视 Parameters for a target optimal model as a non-linear function on a space of the mixture coefficients。在这个基础上，我们证明了在离线设置下，一个GD训练的过参数化神经网络可以可靠地学习这种函数，以预测目标分布下的模型。最后，我们还考虑了在线设置，并提出了一种标签效率的在线算法，可以预测新的目标参数，并且具有误差保证。
</details></li>
</ul>
<hr>
<h2 id="GPT4AIGChip-Towards-Next-Generation-AI-Accelerator-Design-Automation-via-Large-Language-Models"><a href="#GPT4AIGChip-Towards-Next-Generation-AI-Accelerator-Design-Automation-via-Large-Language-Models" class="headerlink" title="GPT4AIGChip: Towards Next-Generation AI Accelerator Design Automation via Large Language Models"></a>GPT4AIGChip: Towards Next-Generation AI Accelerator Design Automation via Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10730">http://arxiv.org/abs/2309.10730</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yonggan Fu, Yongan Zhang, Zhongzhi Yu, Sixu Li, Zhifan Ye, Chaojian Li, Cheng Wan, Yingyan Lin</li>
<li>for: 这个论文旨在提高人工智能加速器的设计效率和质量，使用大语言模型（LLMs）来自动化加速器设计。</li>
<li>methods: 本论文使用了LLMs的启发作用，开发了一个名为GPT4AIGChip的框架，以便通过人工语言指令来自动生成AI加速器设计。</li>
<li>results: 研究发现，LLMs可以帮助生成高质量的AI加速器设计，并且可以帮助不熟悉硬件领域的人员也能够设计高效的加速器。这是首次在LLMs中实现了自动化AI加速器设计的工作。<details>
<summary>Abstract</summary>
The remarkable capabilities and intricate nature of Artificial Intelligence (AI) have dramatically escalated the imperative for specialized AI accelerators. Nonetheless, designing these accelerators for various AI workloads remains both labor- and time-intensive. While existing design exploration and automation tools can partially alleviate the need for extensive human involvement, they still demand substantial hardware expertise, posing a barrier to non-experts and stifling AI accelerator development. Motivated by the astonishing potential of large language models (LLMs) for generating high-quality content in response to human language instructions, we embark on this work to examine the possibility of harnessing LLMs to automate AI accelerator design. Through this endeavor, we develop GPT4AIGChip, a framework intended to democratize AI accelerator design by leveraging human natural languages instead of domain-specific languages. Specifically, we first perform an in-depth investigation into LLMs' limitations and capabilities for AI accelerator design, thus aiding our understanding of our current position and garnering insights into LLM-powered automated AI accelerator design. Furthermore, drawing inspiration from the above insights, we develop a framework called GPT4AIGChip, which features an automated demo-augmented prompt-generation pipeline utilizing in-context learning to guide LLMs towards creating high-quality AI accelerator design. To our knowledge, this work is the first to demonstrate an effective pipeline for LLM-powered automated AI accelerator generation. Accordingly, we anticipate that our insights and framework can serve as a catalyst for innovations in next-generation LLM-powered design automation tools.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）的出色能力和复杂性已经提高了特殊的AI加速器的需求。然而，为不同的AI任务设计这些加速器仍然是劳动和时间耗资的。虽然现有的设计探索和自动化工具可以部分减轻人类的参与，但它们仍需具备相当的硬件专业知识，从而成为非专家的障碍和AI加速器开发的束缚。为了解决这个问题，我们启动了这项工作，旨在利用大型自然语言模型（LLM）来自动化AI加速器设计。通过这项工作，我们开发了GPT4AIGChip框架，这是一个利用人类自然语言而不是域专门语言来自动化AI加速器设计的框架。 Specifically, we first perform an in-depth investigation into LLMs' limitations and capabilities for AI accelerator design, thus aiding our understanding of our current position and garnering insights into LLM-powered automated AI accelerator design. Furthermore, drawing inspiration from the above insights, we develop a framework called GPT4AIGChip, which features an automated demo-augmented prompt-generation pipeline utilizing in-context learning to guide LLMs towards creating high-quality AI accelerator design. To our knowledge, this work is the first to demonstrate an effective pipeline for LLM-powered automated AI accelerator generation. Accordingly, we anticipate that our insights and framework can serve as a catalyst for innovations in next-generation LLM-powered design automation tools.
</details></li>
</ul>
<hr>
<h2 id="On-the-different-regimes-of-Stochastic-Gradient-Descent"><a href="#On-the-different-regimes-of-Stochastic-Gradient-Descent" class="headerlink" title="On the different regimes of Stochastic Gradient Descent"></a>On the different regimes of Stochastic Gradient Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10688">http://arxiv.org/abs/2309.10688</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AntonioScl/regimes_of_SGD">https://github.com/AntonioScl/regimes_of_SGD</a></li>
<li>paper_authors: Antonio Sclocchi, Matthieu Wyart</li>
<li>For:	+ The paper is written to understand the dynamics of stochastic gradient descent (SGD) in training deep neural networks.	+ The authors aim to resolve the central challenges of understanding the cross-overs between SGD and gradient descent (GD) in large batches.	+ The paper focuses on a teacher-student perceptron classification model, and the results are expected to apply to deep networks.* Methods:	+ The authors use a phase diagram in the $B$-$\eta$ plane to separate three dynamical phases: noise-dominated SGD, large-first-step-dominated SGD, and GD.	+ The analysis reveals that the batch size $B^*$ separating regimes $\textit{(i)}$ and $\textit{(ii)}$ scales with the size $P$ of the training set, with an exponent that characterizes the hardness of the classification problem.* Results:	+ The authors obtain empirical results that support their key predictions and show the applicability of their analysis to deep networks.	+ The phase diagram provides a framework for understanding the different regimes of generalization error in SGD.<details>
<summary>Abstract</summary>
Modern deep networks are trained with stochastic gradient descent (SGD) whose key parameters are the number of data considered at each step or batch size $B$, and the step size or learning rate $\eta$. For small $B$ and large $\eta$, SGD corresponds to a stochastic evolution of the parameters, whose noise amplitude is governed by the `temperature' $T\equiv \eta/B$. Yet this description is observed to break down for sufficiently large batches $B\geq B^*$, or simplifies to gradient descent (GD) when the temperature is sufficiently small. Understanding where these cross-overs take place remains a central challenge. Here we resolve these questions for a teacher-student perceptron classification model, and show empirically that our key predictions still apply to deep networks. Specifically, we obtain a phase diagram in the $B$-$\eta$ plane that separates three dynamical phases: $\textit{(i)}$ a noise-dominated SGD governed by temperature, $\textit{(ii)}$ a large-first-step-dominated SGD and $\textit{(iii)}$ GD. These different phases also corresponds to different regimes of generalization error. Remarkably, our analysis reveals that the batch size $B^*$ separating regimes $\textit{(i)}$ and $\textit{(ii)}$ scale with the size $P$ of the training set, with an exponent that characterizes the hardness of the classification problem.
</details>
<details>
<summary>摘要</summary>
现代深度网络通常使用梯度下降（SGD）进行训练，SGD的关键参数包括每次考虑的数据量或批处理大小$B$，以及学习率$\eta$。对于小$B$和大$\eta$来说，SGD对参数进行杂音演化，其杂音强度由温度$T\equiv \eta/B$控制。然而，这个描述会在 sufficient large batches $B\geq B^*$ 或者简化为梯度下降（GD）当温度够小时失效。理解这些交叉点的位置是中心挑战。我们在教师-学生批处理分类模型上解决这些问题，并证明我们的预测仍然适用于深度网络。具体来说，我们获得了 $B$-$\eta$ 平面上的相对谱，这个谱分为三个动力学阶段： $\textit{(i)}$ 杂音控制的SGD， $\textit{(ii)}$ 大first-step控制的SGD，和 $\textit{(iii)}$ GD。这些不同的阶段也对应于不同的泛化误差阶段。很remarkably，我们的分析显示，训练集大小$P$的Batch大小$B^*$分界线 separates这些阶段，并且这个分界线的幂数与泛化误差阶段的困难程度相关。
</details></li>
</ul>
<hr>
<h2 id="Oracle-Complexity-Reduction-for-Model-free-LQR-A-Stochastic-Variance-Reduced-Policy-Gradient-Approach"><a href="#Oracle-Complexity-Reduction-for-Model-free-LQR-A-Stochastic-Variance-Reduced-Policy-Gradient-Approach" class="headerlink" title="Oracle Complexity Reduction for Model-free LQR: A Stochastic Variance-Reduced Policy Gradient Approach"></a>Oracle Complexity Reduction for Model-free LQR: A Stochastic Variance-Reduced Policy Gradient Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10679">http://arxiv.org/abs/2309.10679</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jd-anderson/lqr_svrpg">https://github.com/jd-anderson/lqr_svrpg</a></li>
<li>paper_authors: Leonardo F. Toso, Han Wang, James Anderson</li>
<li>for: 解决模型free Linear Quadratic Regulator (LQR)问题中的$\epsilon$-近似解决方案。</li>
<li>methods: 使用Stochastic Variance-Reduced Policy Gradient (SVRPG)方法，结合一点和二点估计在 dual-loop variance-reduced算法中。</li>
<li>results: 只需要 $O\left(\log\left(1&#x2F;\epsilon\right)^{\beta}\right)$ 二点成本信息，以达到 $\beta \in (0,1)$ 的 approximate optimal solution。<details>
<summary>Abstract</summary>
We investigate the problem of learning an $\epsilon$-approximate solution for the discrete-time Linear Quadratic Regulator (LQR) problem via a Stochastic Variance-Reduced Policy Gradient (SVRPG) approach. Whilst policy gradient methods have proven to converge linearly to the optimal solution of the model-free LQR problem, the substantial requirement for two-point cost queries in gradient estimations may be intractable, particularly in applications where obtaining cost function evaluations at two distinct control input configurations is exceptionally costly. To this end, we propose an oracle-efficient approach. Our method combines both one-point and two-point estimations in a dual-loop variance-reduced algorithm. It achieves an approximate optimal solution with only $O\left(\log\left(1/\epsilon\right)^{\beta}\right)$ two-point cost information for $\beta \in (0,1)$.
</details>
<details>
<summary>摘要</summary>
我们研究一个 $\epsilon$-近似解决方案 для碎时Linear Quadratic Regulator（LQR）问题，使用Stochastic Variance-Reduced Policy Gradient（SVRPG）方法。 policy gradient 方法已经证明可以线性传递到model-free LQR 问题的最佳解，但是需要两点成本询问的要求可能是不可行的，特别是在应用中具有高成本的cost function询问。为此，我们提出了一个 oracle-efficient 方法。我们的方法结合了一点和二点询问的对称降低算法，实现了 $\epsilon$-近似解的 aproximate 优化，只需要 $O\left(\log\left(1/\epsilon\right)^{\beta}\right)$ 两点成本信息，其中 $\beta \in (0,1)$。
</details></li>
</ul>
<hr>
<h2 id="Implementing-a-new-fully-stepwise-decomposition-based-sampling-technique-for-the-hybrid-water-level-forecasting-model-in-real-world-application"><a href="#Implementing-a-new-fully-stepwise-decomposition-based-sampling-technique-for-the-hybrid-water-level-forecasting-model-in-real-world-application" class="headerlink" title="Implementing a new fully stepwise decomposition-based sampling technique for the hybrid water level forecasting model in real-world application"></a>Implementing a new fully stepwise decomposition-based sampling technique for the hybrid water level forecasting model in real-world application</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10658">http://arxiv.org/abs/2309.10658</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziqian Zhang, Nana Bao, Xingting Yan, Aokai Zhu, Chenyang Li, Mingyu Liu</li>
<li>for: 这个研究是为了提高水位时间序列预测中的实际应用性，特别是使用分解方法来优化预测模型。</li>
<li>methods: 这个研究使用了新的全步阶分解基本（FSDB）抽样技术，与各种分解方法（如量子振荡分析（VMD）和单束谱分析（SSA））结合，实现了更好的预测性。</li>
<li>results: 根据这个研究，使用FSDB抽样技术和VMD分析方法，水位时间序列预测中的Nash-Sutcliffe效率（NSE）系数在三个站点中提高了6.4%、28.8%和7.0%，相比之下，使用现有最先进的抽样技术时的NSE系数提高幅度较低。同时，使用SSA分析方法时，NSE系数在三个站点中提高了3.2%、3.1%和1.1%。<details>
<summary>Abstract</summary>
Various time variant non-stationary signals need to be pre-processed properly in hydrological time series forecasting in real world, for example, predictions of water level. Decomposition method is a good candidate and widely used in such a pre-processing problem. However, decomposition methods with an inappropriate sampling technique may introduce future data which is not available in practical applications, and result in incorrect decomposition-based forecasting models. In this work, a novel Fully Stepwise Decomposition-Based (FSDB) sampling technique is well designed for the decomposition-based forecasting model, strictly avoiding introducing future information. This sampling technique with decomposition methods, such as Variational Mode Decomposition (VMD) and Singular spectrum analysis (SSA), is applied to predict water level time series in three different stations of Guoyang and Chaohu basins in China. Results of VMD-based hybrid model using FSDB sampling technique show that Nash-Sutcliffe Efficiency (NSE) coefficient is increased by 6.4%, 28.8% and 7.0% in three stations respectively, compared with those obtained from the currently most advanced sampling technique. In the meantime, for series of SSA-based experiments, NSE is increased by 3.2%, 3.1% and 1.1% respectively. We conclude that the newly developed FSDB sampling technique can be used to enhance the performance of decomposition-based hybrid model in water level time series forecasting in real world.
</details>
<details>
<summary>摘要</summary>
各种不同时间和不同固定信号需要进行正确的预处理，以便在实际应用中进行水位时间序列预测，如水位水平。分解方法是一个好的解决方案，广泛应用于这种预处理问题。然而，使用不当的抽样技术可能会引入未来数据，导致不正确的分解基于预测模型。在这种情况下，我们提出了一种新的幂等步骤分解基于抽样技术（FSDB）， strict avoiding the introduction of future information.这种抽样技术与分解方法，如变分Mode分析（VMD）和特征峰分析（SSA），应用于预测水位时间序列的三个不同站点：国阳和朝湖水系。结果表明，使用VMD基于的混合模型使用FSDB抽样技术，相比最先进的抽样技术，Nash-Sutcliffe效率系数（NSE）的提高为6.4%、28.8%和7.0%分别在三个站点上。同时，对Series of SSA-based experiments，NSE的提高为3.2%、3.1%和1.1%分别。我们 conclude that the newly developed FSDB sampling technique can be used to enhance the performance of decomposition-based hybrid model in water level time series forecasting in real world.
</details></li>
</ul>
<hr>
<h2 id="Learning-Adaptive-Safety-for-Multi-Agent-Systems"><a href="#Learning-Adaptive-Safety-for-Multi-Agent-Systems" class="headerlink" title="Learning Adaptive Safety for Multi-Agent Systems"></a>Learning Adaptive Safety for Multi-Agent Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10657">http://arxiv.org/abs/2309.10657</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luigiberducci/learning_adaptive_safety">https://github.com/luigiberducci/learning_adaptive_safety</a></li>
<li>paper_authors: Luigi Berducci, Shuo Yang, Rahul Mangharam, Radu Grosu</li>
<li>for: 保障多体系统的安全性在具有限制信息的情况下是一项挑战，现有的方法往往假设其他代理人的行为，并且需要手动调整以保证安全、可行性和性能的平衡。本文探讨了基于控制障碍函数（CBF）的弹性安全学习方法。</li>
<li>methods: 本文提出了一种名为ASRL的可靠安全学习框架，可以自动优化策略和CBF系数，以提高安全性和长期性能。ASRL通过直接与其他代理人交互，学习与多种代理人行为相应，并保持成本违反在所需的限制下。</li>
<li>results: 本文在多机器人系统和竞争型多体系统中评估了ASRL，与学习基于和控制理论基于的方法进行比较。实验结果表明ASRL有效地增强了安全性和长期性能，并且可以适应各种非典型情况。代码和补充材料在线公开。<details>
<summary>Abstract</summary>
Ensuring safety in dynamic multi-agent systems is challenging due to limited information about the other agents. Control Barrier Functions (CBFs) are showing promise for safety assurance but current methods make strong assumptions about other agents and often rely on manual tuning to balance safety, feasibility, and performance. In this work, we delve into the problem of adaptive safe learning for multi-agent systems with CBF. We show how emergent behavior can be profoundly influenced by the CBF configuration, highlighting the necessity for a responsive and dynamic approach to CBF design. We present ASRL, a novel adaptive safe RL framework, to fully automate the optimization of policy and CBF coefficients, to enhance safety and long-term performance through reinforcement learning. By directly interacting with the other agents, ASRL learns to cope with diverse agent behaviours and maintains the cost violations below a desired limit. We evaluate ASRL in a multi-robot system and a competitive multi-agent racing scenario, against learning-based and control-theoretic approaches. We empirically demonstrate the efficacy and flexibility of ASRL, and assess generalization and scalability to out-of-distribution scenarios. Code and supplementary material are public online.
</details>
<details>
<summary>摘要</summary>
Ensuring safety in dynamic multi-agent systems is challenging due to limited information about other agents. Control Barrier Functions (CBFs) are showing promise for safety assurance but current methods make strong assumptions about other agents and often rely on manual tuning to balance safety, feasibility, and performance. In this work, we delve into the problem of adaptive safe learning for multi-agent systems with CBF. We show how emergent behavior can be profoundly influenced by the CBF configuration, highlighting the necessity for a responsive and dynamic approach to CBF design. We present ASRL, a novel adaptive safe RL framework, to fully automate the optimization of policy and CBF coefficients, to enhance safety and long-term performance through reinforcement learning. By directly interacting with the other agents, ASRL learns to cope with diverse agent behaviors and maintains the cost violations below a desired limit. We evaluate ASRL in a multi-robot system and a competitive multi-agent racing scenario, against learning-based and control-theoretic approaches. We empirically demonstrate the efficacy and flexibility of ASRL, and assess generalization and scalability to out-of-distribution scenarios. Code and supplementary material are public online.Here's the text in Traditional Chinese, if you prefer: Ensuring safety in dynamic multi-agent systems is challenging due to limited information about other agents. Control Barrier Functions (CBFs) are showing promise for safety assurance but current methods make strong assumptions about other agents and often rely on manual tuning to balance safety, feasibility, and performance. In this work, we delve into the problem of adaptive safe learning for multi-agent systems with CBF. We show how emergent behavior can be profoundly influenced by the CBF configuration, highlighting the necessity for a responsive and dynamic approach to CBF design. We present ASRL, a novel adaptive safe RL framework, to fully automate the optimization of policy and CBF coefficients, to enhance safety and long-term performance through reinforcement learning. By directly interacting with the other agents, ASRL learns to cope with diverse agent behaviors and maintains the cost violations below a desired limit. We evaluate ASRL in a multi-robot system and a competitive multi-agent racing scenario, against learning-based and control-theoretic approaches. We empirically demonstrate the efficacy and flexibility of ASRL, and assess generalization and scalability to out-of-distribution scenarios. Code and supplementary material are public online.
</details></li>
</ul>
<hr>
<h2 id="A-spectrum-of-physics-informed-Gaussian-processes-for-regression-in-engineering"><a href="#A-spectrum-of-physics-informed-Gaussian-processes-for-regression-in-engineering" class="headerlink" title="A spectrum of physics-informed Gaussian processes for regression in engineering"></a>A spectrum of physics-informed Gaussian processes for regression in engineering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10656">http://arxiv.org/abs/2309.10656</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elizabeth J Cross, Timothy J Rogers, Daniel J Pitchforth, Samuel J Gibson, Matthew R Jones</li>
<li>for: 提高使用有限数据进行预测模型的能力</li>
<li>methods: 结合机器学习技术和物理基础理解来提高预测模型的可靠性和可读性</li>
<li>results: 通过将物理基础理解与数据回归方法联系起来，可以大幅减少数据收集量，同时提高模型的解释性。<details>
<summary>Abstract</summary>
Despite the growing availability of sensing and data in general, we remain unable to fully characterise many in-service engineering systems and structures from a purely data-driven approach. The vast data and resources available to capture human activity are unmatched in our engineered world, and, even in cases where data could be referred to as ``big,'' they will rarely hold information across operational windows or life spans. This paper pursues the combination of machine learning technology and physics-based reasoning to enhance our ability to make predictive models with limited data. By explicitly linking the physics-based view of stochastic processes with a data-based regression approach, a spectrum of possible Gaussian process models are introduced that enable the incorporation of different levels of expert knowledge of a system. Examples illustrate how these approaches can significantly reduce reliance on data collection whilst also increasing the interpretability of the model, another important consideration in this context.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="An-Extendable-Python-Implementation-of-Robust-Optimisation-Monte-Carlo"><a href="#An-Extendable-Python-Implementation-of-Robust-Optimisation-Monte-Carlo" class="headerlink" title="An Extendable Python Implementation of Robust Optimisation Monte Carlo"></a>An Extendable Python Implementation of Robust Optimisation Monte Carlo</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10612">http://arxiv.org/abs/2309.10612</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vasilis Gkolemis, Michael Gutmann, Henri Pesonen</li>
<li>For: 这篇论文的目的是提出一个基于Monte Carlo的likelihood-free inference（LFI）方法，并实现其在Python的ELFI套件中。* Methods: 这篇论文使用了一种名为Robust Optimisation Monte Carlo（ROMC）的LFI方法，ROMC是一个新的、高度平行化的LFI框架，可以实现精确的 posterior 权重样本。* Results: 这篇论文的实现方法可以在两种方式下使用：一、科学家可以直接使用它作为一个出厂装置的LFI算法；二、研究者可以将ROMC分解为单独的部分，并让其在完全平行化的方式下运行，以便进一步的扩展和改进。<details>
<summary>Abstract</summary>
Performing inference in statistical models with an intractable likelihood is challenging, therefore, most likelihood-free inference (LFI) methods encounter accuracy and efficiency limitations. In this paper, we present the implementation of the LFI method Robust Optimisation Monte Carlo (ROMC) in the Python package ELFI. ROMC is a novel and efficient (highly-parallelizable) LFI framework that provides accurate weighted samples from the posterior. Our implementation can be used in two ways. First, a scientist may use it as an out-of-the-box LFI algorithm; we provide an easy-to-use API harmonized with the principles of ELFI, enabling effortless comparisons with the rest of the methods included in the package. Additionally, we have carefully split ROMC into isolated components for supporting extensibility. A researcher may experiment with novel method(s) for solving part(s) of ROMC without reimplementing everything from scratch. In both scenarios, the ROMC parts can run in a fully-parallelized manner, exploiting all CPU cores. We also provide helpful functionalities for (i) inspecting the inference process and (ii) evaluating the obtained samples. Finally, we test the robustness of our implementation on some typical LFI examples.
</details>
<details>
<summary>摘要</summary>
Performing inference in statistical models with an intractable likelihood is challenging, therefore, most likelihood-free inference (LFI) methods encounter accuracy and efficiency limitations. In this paper, we present the implementation of the LFI method Robust Optimisation Monte Carlo (ROMC) in the Python package ELFI. ROMC is a novel and efficient (highly-parallelizable) LFI framework that provides accurate weighted samples from the posterior. Our implementation can be used in two ways. First, a scientist may use it as an out-of-the-box LFI algorithm; we provide an easy-to-use API harmonized with the principles of ELFI, enabling effortless comparisons with the rest of the methods included in the package. Additionally, we have carefully split ROMC into isolated components for supporting extensibility. A researcher may experiment with novel method(s) for solving part(s) of ROMC without reimplementing everything from scratch. In both scenarios, the ROMC parts can run in a fully-parallelized manner, exploiting all CPU cores. We also provide helpful functionalities for (i) inspecting the inference process and (ii) evaluating the obtained samples. Finally, we test the robustness of our implementation on some typical LFI examples.Here's the translation in Traditional Chinese:行使 statistical models 中的 intractable likelihood 是具有挑战性的，因此大多数 likelihood-free inference (LFI) 方法受到精度和效率限制。在这篇 paper 中，我们提出了 ELFI 套件中的 Robust Optimisation Monte Carlo (ROMC) 方法的实现。ROMC 是一个新的、高度可行化 (高度并行化) LFI 框架，可以从 posterior 中获得正确的权重样本。我们的实现可以在 two 种方式使用：一、科学家可以将它作为 out-of-the-box LFI 算法使用；我们提供了与 ELFI 的原则相互适应的易用 API，使得与其他方法在套件中的比较变得容易。其次，我们将 ROMC 分解为可扩展的部分，让研究人员可以对部分 ROMC 进行 novel 的方法解决，而不需要从零开始重新实现。在这两种情况下，ROMC 的部分可以在完全并行化的方式下运行，扩展到所有 CPU 核心。我们还提供了帮助性的功能，包括 (i) 检查推断过程和 (ii) 评估取得的样本。最后，我们将实现的稳定性试验在一些常见的 LFI 例子上。
</details></li>
</ul>
<hr>
<h2 id="Asteroids-co-orbital-motion-classification-based-on-Machine-Learning"><a href="#Asteroids-co-orbital-motion-classification-based-on-Machine-Learning" class="headerlink" title="Asteroids co-orbital motion classification based on Machine Learning"></a>Asteroids co-orbital motion classification based on Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10603">http://arxiv.org/abs/2309.10603</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giulia Ciacci, Andrea Barucci, Sara Di Ruzza, Elisa Maria Alessi</li>
<li>for: 本研究探讨了使用机器学习分类asteroids在与一个 planet 的共轨运动中。</li>
<li>methods: 我们考虑了四种不同的运动方式，包括Tadpole、Horseshoe和Quasi-satellite，并构建了三个数据集（Real、Ideal和Perturbed）来训练和测试机器学习算法。</li>
<li>results: 我们的结果表明，机器学习算法能够正确地分类时间序列，并且性能非常高。<details>
<summary>Abstract</summary>
In this work, we explore how to classify asteroids in co-orbital motion with a given planet using Machine Learning. We consider four different kinds of motion in mean motion resonance with the planet, nominally Tadpole, Horseshoe and Quasi-satellite, building 3 datasets defined as Real (taking the ephemerides of real asteroids from the JPL Horizons system), Ideal and Perturbed (both simulated, obtained by propagating initial conditions considering two different dynamical systems) for training and testing the Machine Learning algorithms in different conditions.   The time series of the variable theta (angle related to the resonance) are studied with a data analysis pipeline defined ad hoc for the problem and composed by: data creation and annotation, time series features extraction thanks to the tsfresh package (potentially followed by selection and standardization) and the application of Machine Learning algorithms for Dimensionality Reduction and Classification. Such approach, based on features extracted from the time series, allows to work with a smaller number of data with respect to Deep Learning algorithms, also allowing to define a ranking of the importance of the features. Physical Interpretability of the features is another key point of this approach. In addition, we introduce the SHapley Additive exPlanations for Explainability technique.   Different training and test sets are used, in order to understand the power and the limits of our approach. The results show how the algorithms are able to identify and classify correctly the time series, with a high degree of performance.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们探索如何使用机器学习分类 asteroids 在与给定的 planet 的共轨运动中。我们考虑了四种不同的运动，包括 Tadpole、Horseshoe 和 Quasi-satellite，并构建了三个数据集：Real（使用 JPL Horizons 系统中的真实小行星轨道数据）、Ideal 和 Perturbed（两者都是通过初始条件的传播而生成的，以模拟不同的动力系统），用于训练和测试机器学习算法。我们使用自定义的数据分析管道来研究时间序列中的θ（与共轨运动相关的角度）。该管道包括：数据创建和注释、时间序列特征提取（使用 tsfresh 包）以及机器学习算法的维度减少和分类。这种方法，基于时间序列特征，允许我们使用较少的数据量与深度学习算法进行比较，同时允许我们定义特征的排名和物理可解性。此外，我们还引入 SHapley Additive exPlanations for Explainability 技术。为了了解我们的方法的力量和局限，我们使用了不同的训练和测试集。结果显示，我们的算法能够正确地识别和分类时间序列，性能非常高。
</details></li>
</ul>
<hr>
<h2 id="Motif-Centric-Representation-Learning-for-Symbolic-Music"><a href="#Motif-Centric-Representation-Learning-for-Symbolic-Music" class="headerlink" title="Motif-Centric Representation Learning for Symbolic Music"></a>Motif-Centric Representation Learning for Symbolic Music</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10597">http://arxiv.org/abs/2309.10597</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxuan Wu, Roger B. Dannenberg, Gus Xia</li>
<li>for: 本研究旨在 computational modeling of music motifs, 以便自动音乐作曲和音乐信息检索。</li>
<li>methods: 使用 Siamese 网络架构和预训练和精度调整的管道，通过表示学习来学习隐藏的旋律关系和变化。 使用 VICReg 方法进行预训练，并使用对比学习进行精度调整。</li>
<li>results: 实验结果表明，这两种方法可以补充Each other，提高了 music 旋律检索任务中的区域下面积分值12.6%。最后，我们可见了获得的旋律表示，为了更直观地理解音乐作品的结构。根据我们所知，这是计算机模型音乐旋律的一个突破性的研究成果，laying the foundations for future applications of motifs in automatic music composition and music information retrieval。<details>
<summary>Abstract</summary>
Music motif, as a conceptual building block of composition, is crucial for music structure analysis and automatic composition. While human listeners can identify motifs easily, existing computational models fall short in representing motifs and their developments. The reason is that the nature of motifs is implicit, and the diversity of motif variations extends beyond simple repetitions and modulations. In this study, we aim to learn the implicit relationship between motifs and their variations via representation learning, using the Siamese network architecture and a pretraining and fine-tuning pipeline. A regularization-based method, VICReg, is adopted for pretraining, while contrastive learning is used for fine-tuning. Experimental results on a retrieval-based task show that these two methods complement each other, yielding an improvement of 12.6% in the area under the precision-recall curve. Lastly, we visualize the acquired motif representations, offering an intuitive comprehension of the overall structure of a music piece. As far as we know, this work marks a noteworthy step forward in computational modeling of music motifs. We believe that this work lays the foundations for future applications of motifs in automatic music composition and music information retrieval.
</details>
<details>
<summary>摘要</summary>
音乐主题，作为作曲的概念建筑块，对音乐结构分析和自动作曲具有重要意义。虽然人类听众可以轻松地识别主题，但现有计算机模型却无法准确表示主题和其变化。这是因为主题的本质是隐式的，主题变化的多样性超出了简单的重复和修改。在这项研究中，我们目的是通过学习来表示主题和其变化的关系，使用siamesenet Architecture和预训练和精度调整管道。采用VICReg方法进行预训练，而对比学习用于精度调整。实验结果表明，这两种方法相互补做，提高了 Retrieval-based任务的区间 beneath the precision-recall 曲线的面积12.6%。最后，我们可视化获得的主题表示，为音乐作品的结构提供了直观的理解。根据我们所知，这项工作是计算机模型音乐主题的开创性工作，我们认为这项工作将为自动音乐作曲和音乐信息检索的未来应用奠定基础。
</details></li>
</ul>
<hr>
<h2 id="Task-Graph-offloading-via-Deep-Reinforcement-Learning-in-Mobile-Edge-Computing"><a href="#Task-Graph-offloading-via-Deep-Reinforcement-Learning-in-Mobile-Edge-Computing" class="headerlink" title="Task Graph offloading via Deep Reinforcement Learning in Mobile Edge Computing"></a>Task Graph offloading via Deep Reinforcement Learning in Mobile Edge Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10569">http://arxiv.org/abs/2309.10569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiagang Liu, Yun Mi, Xinyu Zhang</li>
<li>for: 本文主要研究适用于移动边缘计算（MEC）中任务图Offloading，以提高用户体验。</li>
<li>methods: 本文使用了Markov决策过程（MDP）模型来调度任务，并采用了深度学习算法（SATA-DRL）来学习任务调度策略。</li>
<li>results: 对比 existed 策略，SATA-DRL 能够更好地减少平均延迟和死线违反率。<details>
<summary>Abstract</summary>
Various mobile applications that comprise dependent tasks are gaining widespread popularity and are increasingly complex. These applications often have low-latency requirements, resulting in a significant surge in demand for computing resources. With the emergence of mobile edge computing (MEC), it becomes the most significant issue to offload the application tasks onto small-scale devices deployed at the edge of the mobile network for obtaining a high-quality user experience. However, since the environment of MEC is dynamic, most existing works focusing on task graph offloading, which rely heavily on expert knowledge or accurate analytical models, fail to fully adapt to such environmental changes, resulting in the reduction of user experience. This paper investigates the task graph offloading in MEC, considering the time-varying computation capabilities of edge computing devices. To adapt to environmental changes, we model the task graph scheduling for computation offloading as a Markov Decision Process (MDP). Then, we design a deep reinforcement learning algorithm (SATA-DRL) to learn the task scheduling strategy from the interaction with the environment, to improve user experience. Extensive simulations validate that SATA-DRL is superior to existing strategies in terms of reducing average makespan and deadline violation.
</details>
<details>
<summary>摘要</summary>
各种移动应用程序，它们包含依赖关系的任务，在广泛流行的情况下，逐渐变得越来越复杂。这些应用程序通常具有低延迟要求，从而导致计算资源的巨大需求增加。随着移动边缘计算（MEC）的出现，将应用任务卸载到靠近移动网络边缘的小规模设备上成为了最重要的问题，以实现高质量用户体验。然而，由于MEC环境的动态性，大多数现有的任务图卸载工作，它们依赖于专家知识或准确的分析模型，无法完全适应环境变化，从而导致用户体验下降。本文研究MEC中的任务图卸载，考虑到边缘计算设备的时间变化计算能力。为适应环境变化，我们将任务图 scheduling 模型为Markov决策过程（MDP）。然后，我们设计了深度强化学习算法（SATA-DRL），从环境互动中学习任务调度策略，以提高用户体验。广泛的 simulations  validate 表明，SATA-DRL 在减少平均延迟和死线 violet 方面胜过现有策略。
</details></li>
</ul>
<hr>
<h2 id="A-Hierarchical-Neural-Framework-for-Classification-and-its-Explanation-in-Large-Unstructured-Legal-Documents"><a href="#A-Hierarchical-Neural-Framework-for-Classification-and-its-Explanation-in-Large-Unstructured-Legal-Documents" class="headerlink" title="A Hierarchical Neural Framework for Classification and its Explanation in Large Unstructured Legal Documents"></a>A Hierarchical Neural Framework for Classification and its Explanation in Large Unstructured Legal Documents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10563">http://arxiv.org/abs/2309.10563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nishchal Prasad, Mohand Boughanem, Taoufik Dkaki</li>
<li>for: 法律判决预测和其解释受到长案文档超过万个字的问题困扰，特别是没有结构化注释的案文。我们定义这个问题为“缺乏注释法律文档”，并explore其缺乏结构信息和长文档的问题。</li>
<li>methods: 我们使用了一种深度学习基于分类框架MESc，将案文分解成多个部分，从自定义精心调整的大语言模型的最后四层中提取其嵌入，并使用无监督归一化来近似结构。然后，我们使用另一组变换器Encoder层来学习间隔表示。</li>
<li>results: 我们发现了大语言模型（GPT-Neo和GPT-J）在法律文档上的适应性和内部领域（法律）的转移学习能力。此外，我们还比较了MESc和这些模型的性能，以及将嵌入组合在最后几层中的影响。为 Hierarchical模型，我们也提出了一种解释抽取算法 named ORSE，即Occlusion sensitivity-based Relevant Sentence Extractor。<details>
<summary>Abstract</summary>
Automatic legal judgment prediction and its explanation suffer from the problem of long case documents exceeding tens of thousands of words, in general, and having a non-uniform structure. Predicting judgments from such documents and extracting their explanation becomes a challenging task, more so on documents with no structural annotation. We define this problem as "scarce annotated legal documents" and explore their lack of structural information and their long lengths with a deep learning-based classification framework which we call MESc; "Multi-stage Encoder-based Supervised with-clustering"; for judgment prediction. Specifically, we divide a document into parts to extract their embeddings from the last four layers of a custom fine-tuned Large Language Model, and try to approximate their structure through unsupervised clustering. Which we use in another set of transformer encoder layers to learn the inter-chunk representations. We explore the adaptability of LLMs with multi-billion parameters (GPT-Neo, and GPT-J) to legal texts and their intra-domain(legal) transfer learning capacity. Alongside this, we compare their performance with MESc and the impact of combining embeddings from their last layers. For such hierarchical models, we also propose an explanation extraction algorithm named ORSE; Occlusion sensitivity-based Relevant Sentence Extractor;
</details>
<details>
<summary>摘要</summary>
自动化法律判断预测和其解释受到长案文档超过万字的问题困扰，其中文档结构不均匀，预测判断和提取解释变得非常困难。我们称这个问题为“缺乏注释法律文档”，并explore其缺乏结构信息和长度的问题，以及无结构注释的文档。我们使用一种深度学习基于分类的框架，称之为MESc，以预测判断。具体来说，我们将文档分成多个部分，从最后四层的自定义微调Large Language Model中提取嵌入，并使用无结构注释的 clustering来approximate结构。然后，我们使用另一组 transformer encoder层来学习间键表示。此外，我们还 explore了LLMs的多亿 Parameter（GPT-Neo和GPT-J）在法律文本上的适应性和内部领域（legal）的传输学习能力。此外，我们还比较了MESc和这些模型的性能，以及将嵌入从最后几层 combine的影响。对于层次模型，我们还提出了一种解释抽取算法，称之为ORSE，即Occlusion sensitivity-based Relevant Sentence Extractor。
</details></li>
</ul>
<hr>
<h2 id="Hybrid-State-Space-based-Learning-for-Sequential-Data-Prediction-with-Joint-Optimization"><a href="#Hybrid-State-Space-based-Learning-for-Sequential-Data-Prediction-with-Joint-Optimization" class="headerlink" title="Hybrid State Space-based Learning for Sequential Data Prediction with Joint Optimization"></a>Hybrid State Space-based Learning for Sequential Data Prediction with Joint Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10553">http://arxiv.org/abs/2309.10553</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mustafa E. Aydın, Arda Fazla, Suleyman S. Kozat</li>
<li>for: 该 paper  investigate 非线性预测&#x2F;回归在在线环境中，并提出了一种混合模型，可以有效地解决传统非线性预测模型中的域pecificFeature工程问题，并实现了有效的非线性和线性组件混合。</li>
<li>methods: 该 paper 使用了回归结构来提取Raw序列序列中的特征，并使用了传统的线性时间序列模型来处理时间序列数据中的特有性，如季节性和趋势。而不同于现有的集成或混合模型，我们在一个单一的过程中，jointly optimize 一个加强的杜林 ней网络 (LSTM)  для自动特征提取和一个 ARMA 家族时间序列模型 (SARIMAX) 来有效地处理时间序列数据。</li>
<li>results: 我们在 widely 公布的实验数据集上表现出了显著的改进，并在 Code 中开源了我们的代码，以便进一步研究和复现我们的结果。<details>
<summary>Abstract</summary>
We investigate nonlinear prediction/regression in an online setting and introduce a hybrid model that effectively mitigates, via a joint mechanism through a state space formulation, the need for domain-specific feature engineering issues of conventional nonlinear prediction models and achieves an efficient mix of nonlinear and linear components. In particular, we use recursive structures to extract features from raw sequential sequences and a traditional linear time series model to deal with the intricacies of the sequential data, e.g., seasonality, trends. The state-of-the-art ensemble or hybrid models typically train the base models in a disjoint manner, which is not only time consuming but also sub-optimal due to the separation of modeling or independent training. In contrast, as the first time in the literature, we jointly optimize an enhanced recurrent neural network (LSTM) for automatic feature extraction from raw data and an ARMA-family time series model (SARIMAX) for effectively addressing peculiarities associated with time series data. We achieve this by introducing novel state space representations for the base models, which are then combined to provide a full state space representation of the hybrid or the ensemble. Hence, we are able to jointly optimize both models in a single pass via particle filtering, for which we also provide the update equations. The introduced architecture is generic so that one can use other recurrent architectures, e.g., GRUs, traditional time series-specific models, e.g., ETS or other optimization methods, e.g., EKF, UKF. Due to such novel combination and joint optimization, we demonstrate significant improvements in widely publicized real life competition datasets. We also openly share our code for further research and replicability of our results.
</details>
<details>
<summary>摘要</summary>
我们调查线性预测/回归在线上环境中，并提出了一种混合模型，可以有效地减少传统非线性预测模型中的域专特性工程问题，并 achiev 了一个有效的非线性和线性元件混合。具体来说，我们使用回传结构来从原始的序列序列中提取特征，并使用传统的线性时间序列模型来处理时间序列数据中的特有性，例如季节性和趋势。现有的ensemble或混合模型通常会在分开的方式训练基本模型，这不仅是时间耗费大的，而且也是不佳的，因为它们的模型化或独立训练。相反地，我们是第一次在文献中使用一个强化的长期内部遮蔽网络（LSTM）来自动提取特征，并使用ARMA家族时间序列模型（SARIMAX）来有效地处理时间序列数据中的特有性。我们通过引入新的状态空间表示来融合这两个基本模型，然后将它们联合以提供一个完整的状态空间表示。因此，我们可以在单一通过粒子统计来协同优化这两个模型。我们的架构是通用的，可以使用其他回归架构，例如GRU，传统时间序列特定模型，例如ETS，或其他优化方法，例如EKF、UKF。因为我们的新的结构和协同优化，我们在广泛公开的真实生活竞赛数据中展示了明显的改善。我们还公开了我们的代码，以便进一步的研究和我们的结果的重现。
</details></li>
</ul>
<hr>
<h2 id="Love-or-Hate-Share-or-Split-Privacy-Preserving-Training-Using-Split-Learning-and-Homomorphic-Encryption"><a href="#Love-or-Hate-Share-or-Split-Privacy-Preserving-Training-Using-Split-Learning-and-Homomorphic-Encryption" class="headerlink" title="Love or Hate? Share or Split? Privacy-Preserving Training Using Split Learning and Homomorphic Encryption"></a>Love or Hate? Share or Split? Privacy-Preserving Training Using Split Learning and Homomorphic Encryption</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10517">http://arxiv.org/abs/2309.10517</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/khoaguin/hesplitnet">https://github.com/khoaguin/hesplitnet</a></li>
<li>paper_authors: Tanveer Khan, Khoa Nguyen, Antonis Michalas, Alexandros Bakas</li>
<li>for: 这篇论文是针对分布式学习（Distributed Learning）中的隐私问题提出了一个新的解决方案。</li>
<li>methods: 本论文使用的方法是基于U型分布式学习（U-shaped Distributed Learning），并使用了几何加密（Homomorphic Encryption）来保护用户隐私。</li>
<li>results: 本论文的结果显示，使用HE数据进行U型分布式学习只会导致精度下降2.65%，并且保护了原始训练数据的隐私。<details>
<summary>Abstract</summary>
Split learning (SL) is a new collaborative learning technique that allows participants, e.g. a client and a server, to train machine learning models without the client sharing raw data. In this setting, the client initially applies its part of the machine learning model on the raw data to generate activation maps and then sends them to the server to continue the training process. Previous works in the field demonstrated that reconstructing activation maps could result in privacy leakage of client data. In addition to that, existing mitigation techniques that overcome the privacy leakage of SL prove to be significantly worse in terms of accuracy. In this paper, we improve upon previous works by constructing a protocol based on U-shaped SL that can operate on homomorphically encrypted data. More precisely, in our approach, the client applies homomorphic encryption on the activation maps before sending them to the server, thus protecting user privacy. This is an important improvement that reduces privacy leakage in comparison to other SL-based works. Finally, our results show that, with the optimum set of parameters, training with HE data in the U-shaped SL setting only reduces accuracy by 2.65% compared to training on plaintext. In addition, raw training data privacy is preserved.
</details>
<details>
<summary>摘要</summary>
分离学习（SL）是一种新的合作学习技术，允许参与者（例如客户和服务器）无需共享原始数据来训练机器学习模型。在这种设定下，客户首先应用其部分机器学习模型于原始数据，生成活动图并将其发送给服务器继续训练过程。先前的工作表明，重建活动图可能导致客户数据泄露。此外，现有的防范措施可以减少SL中客户数据隐私泄露的影响，但是这些措施在准确性方面表现不佳。在这篇论文中，我们超越先前的工作，基于U型SL构建了一种具有同质 encrypting 数据的协议。具体来说，在我们的方法中，客户将 homomorphic encryption 应用于活动图，以保护用户隐私。这是一项重要的改进，可以减少客户数据泄露的风险，相比其他SL基于的工作。最后，我们的结果表明，在U型SL设定下，使用HE数据进行训练，只减少准确性比例2.65%，与平文训练相比。此外，原始数据隐私得到保护。
</details></li>
</ul>
<hr>
<h2 id="Learning-End-to-End-Channel-Coding-with-Diffusion-Models"><a href="#Learning-End-to-End-Channel-Coding-with-Diffusion-Models" class="headerlink" title="Learning End-to-End Channel Coding with Diffusion Models"></a>Learning End-to-End Channel Coding with Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10505">http://arxiv.org/abs/2309.10505</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muah Kim, Rick Fritschek, Rafael F. Schaefer</li>
<li>for: 这个论文的目的是提出一种基于扩散模型的频道编码器训练方法，以解决深度学习中的频道模型整合问题。</li>
<li>methods: 这个论文使用了扩散模型来近似频道分布，并提出了一种高效的训练算法。</li>
<li>results: 实验结果表明，扩散模型可以准确地学习频道分布，并在不同的频道模型下实现近乎最佳的端到端符号错误率。此外，扩散模型还具有较好的抗随机性和鲁棒性。<details>
<summary>Abstract</summary>
The training of neural encoders via deep learning necessitates a differentiable channel model due to the backpropagation algorithm. This requirement can be sidestepped by approximating either the channel distribution or its gradient through pilot signals in real-world scenarios. The initial approach draws upon the latest advancements in image generation, utilizing generative adversarial networks (GANs) or their enhanced variants to generate channel distributions. In this paper, we address this channel approximation challenge with diffusion models, which have demonstrated high sample quality in image generation. We offer an end-to-end channel coding framework underpinned by diffusion models and propose an efficient training algorithm. Our simulations with various channel models establish that our diffusion models learn the channel distribution accurately, thereby achieving near-optimal end-to-end symbol error rates (SERs). We also note a significant advantage of diffusion models: A robust generalization capability in high signal-to-noise ratio regions, in contrast to GAN variants that suffer from error floor. Furthermore, we examine the trade-off between sample quality and sampling speed, when an accelerated sampling algorithm is deployed, and investigate the effect of the noise scheduling on this trade-off. With an apt choice of noise scheduling, sampling time can be significantly reduced with a minor increase in SER.
</details>
<details>
<summary>摘要</summary>
neural encoder 的训练通过深度学习需要可导通道模型，这是因为权重传播算法的需求。这种要求可以通过在实际场景中使用启动信号来缓解。首先，我们使用最新的图像生成技术，如生成对抗网络（GANs）或其改进版本，来生成通道分布。在这篇论文中，我们使用扩散模型来解决通道 aproximation 挑战。我们提出了基于扩散模型的端到端通道编码框架，并提出了高效的训练算法。我们的 simulations 表明，我们的扩散模型可以准确地学习通道分布，并实现near-optimal的端到端符号错误率（SER）。此外，我们注意到扩散模型的一个重要优点：在高信号噪比区域中具有robust的总体化能力，与GAN变体不同，后者在错误 floor 方面受到影响。此外，我们还研究了样本质量和抽取速度之间的质量，以及降低SER的代价。通过适当的噪声调度，抽取时间可以被显著减少，但是SER的增加很小。
</details></li>
</ul>
<hr>
<h2 id="A-comparative-study-of-Grid-and-Natural-sentences-effects-on-Normal-to-Lombard-conversion"><a href="#A-comparative-study-of-Grid-and-Natural-sentences-effects-on-Normal-to-Lombard-conversion" class="headerlink" title="A comparative study of Grid and Natural sentences effects on Normal-to-Lombard conversion"></a>A comparative study of Grid and Natural sentences effects on Normal-to-Lombard conversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10485">http://arxiv.org/abs/2309.10485</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongyang Chen, Yuhong Yang, Qingmu Liu, Baifeng Li, Weiping Tu, Song Lin<br>for:This paper is written to investigate the effectiveness of Normal-to-Lombard models in improving natural speech intelligibility in real-world applications, using a parallel Lombard corpus (Lombard Chinese TIMIT, LCT) and a comparison with a standard grid sentence corpus (Enhanced MAndarin Lombard Grid, EMALG).methods:The paper uses a parallel Lombard corpus (LCT) and a comparison with a standard grid sentence corpus (EMALG) to evaluate the Lombard effect and Normal-to-Lombard conversion in natural and grid sentences. The authors also use a subjective intelligibility assessment across genders and Signal-to-Noise Ratios to evaluate the performance of a StarGAN model trained on EMALG.results:The paper finds that both natural and grid sentences exhibit similar changes in parameters as the noise level increases, but grid sentences show a greater increase in the alpha ratio. The StarGAN model trained on EMALG consistently outperforms the model trained on LCT in terms of improving intelligibility, which may be attributed to EMALG’s larger alpha ratio increase from normal to Lombard speech.<details>
<summary>Abstract</summary>
Grid sentence is commonly used for studying the Lombard effect and Normal-to-Lombard conversion. However, it's unclear if Normal-to-Lombard models trained on grid sentences are sufficient for improving natural speech intelligibility in real-world applications. This paper presents the recording of a parallel Lombard corpus (called Lombard Chinese TIMIT, LCT) extracting natural sentences from Chinese TIMIT. Then We compare natural and grid sentences in terms of Lombard effect and Normal-to-Lombard conversion using LCT and Enhanced MAndarin Lombard Grid corpus (EMALG). Through a parametric analysis of the Lombard effect, We find that as the noise level increases, both natural sentences and grid sentences exhibit similar changes in parameters, but in terms of the increase of the alpha ratio, grid sentences show a greater increase. Following a subjective intelligibility assessment across genders and Signal-to-Noise Ratios, the StarGAN model trained on EMALG consistently outperforms the model trained on LCT in terms of improving intelligibility. This superior performance may be attributed to EMALG's larger alpha ratio increase from normal to Lombard speech.
</details>
<details>
<summary>摘要</summary>
Grid sentence 通常用于研究洛伯特效应和正常到洛伯特转换。然而，不清楚的是，正常到洛伯特模型在格子句子上训练后是否能提高实际应用中的自然语音 inteligibility。这篇论文介绍了一个平行的洛伯特 corpus（名为洛伯特中文 TIMIT， LCT），从中文 TIMIT 中提取了自然句子。然后，我们比较了自然句子和格子句子在洛伯特效应和正常到洛伯特转换方面的不同，使用 LCT 和增强 Mandarin Lombard Grid  corpora（EMALG）。通过对洛伯特效应的参数分析，我们发现，随着噪音水平的增加，自然句子和格子句子都会出现类似的参数变化，但是在 alpha 比率的增加方面，格子句子显示更大的增加。进一步，我们对 gender 和 Signal-to-Noise Ratio 不同的人进行主观的听力评估，发现 StarGAN 模型在 EMALG 上训练后在 inteligibility 方面一直表现出优于在 LCT 上训练的模型。这种更好的性能可能是因为 EMALG 中 alpha 比率在正常到洛伯特语音转换时的更大增加。
</details></li>
</ul>
<hr>
<h2 id="Ad-load-Balancing-via-Off-policy-Learning-in-a-Content-Marketplace"><a href="#Ad-load-Balancing-via-Off-policy-Learning-in-a-Content-Marketplace" class="headerlink" title="Ad-load Balancing via Off-policy Learning in a Content Marketplace"></a>Ad-load Balancing via Off-policy Learning in a Content Marketplace</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11518">http://arxiv.org/abs/2309.11518</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hitesh Sagtani, Madan Jhawar, Rishabh Mehrotra, Olivier Jeunen</li>
<li>For: 优化在社交媒体平台上的在线广告系统中的广告负载均衡，以 maximize 用户满意度和广告收益，同时维护用户体验。* Methods: 利用偏函数学习和记录bandit反馈来评估和优化广告负载。* Results: 在大规模A&#x2F;B测试中，通过使用偏函数学习和不偏估计器（如倒推propensity scoring和双重Robust），实现了对用户满意度指标和广告收益的同时优化。<details>
<summary>Abstract</summary>
Ad-load balancing is a critical challenge in online advertising systems, particularly in the context of social media platforms, where the goal is to maximize user engagement and revenue while maintaining a satisfactory user experience. This requires the optimization of conflicting objectives, such as user satisfaction and ads revenue. Traditional approaches to ad-load balancing rely on static allocation policies, which fail to adapt to changing user preferences and contextual factors. In this paper, we present an approach that leverages off-policy learning and evaluation from logged bandit feedback. We start by presenting a motivating analysis of the ad-load balancing problem, highlighting the conflicting objectives between user satisfaction and ads revenue. We emphasize the nuances that arise due to user heterogeneity and the dependence on the user's position within a session. Based on this analysis, we define the problem as determining the optimal ad-load for a particular feed fetch. To tackle this problem, we propose an off-policy learning framework that leverages unbiased estimators such as Inverse Propensity Scoring (IPS) and Doubly Robust (DR) to learn and estimate the policy values using offline collected stochastic data. We present insights from online A/B experiments deployed at scale across over 80 million users generating over 200 million sessions, where we find statistically significant improvements in both user satisfaction metrics and ads revenue for the platform.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用简化中文。<</SYS>>在在线广告系统中，负载均衡是一项挑战，尤其是在社交媒体平台上，目标是 maximize 用户满意度和广告收益，同时保持用户体验满意。这需要优化冲突的目标，如用户满意度和广告收益。传统的负载均衡方法采用静态分配策略，不能适应用户偏好和上下文因素的变化。在这篇论文中，我们提出一种方法，利用偏移策略和来自日志抽象反馈的评估。我们首先提出了负载均衡问题的动机分析，指出用户满意度和广告收益之间的冲突，并强调用户多样性和Session中的用户位置对问题的影响。基于这种分析，我们定义了负载均衡问题为特定的Feed fetch中的optimal ad-load。为解决这个问题，我们提出了一种偏移学习框架，利用不偏抽象器 such as Inverse Propensity Scoring (IPS)和Doubly Robust (DR)来学习和估计策略值使用在线采集的随机数据。我们提供在线 A/B 试验的实际经验，在8000万用户和2000万会话中发现了 statistically significant 的提升用户满意度指标和广告收益。
</details></li>
</ul>
<hr>
<h2 id="Coreset-selection-can-accelerate-quantum-machine-learning-models-with-provable-generalization"><a href="#Coreset-selection-can-accelerate-quantum-machine-learning-models-with-provable-generalization" class="headerlink" title="Coreset selection can accelerate quantum machine learning models with provable generalization"></a>Coreset selection can accelerate quantum machine learning models with provable generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10441">http://arxiv.org/abs/2309.10441</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiming Huang, Huiyuan Wang, Yuxuan Du, Xiao Yuan</li>
<li>for: 提高量子机器学习模型的训练效率，使其在大规模数据集上达到相同的性能水平。</li>
<li>methods: 采用核心集选择法，从原始训练集中选择一个judicious subsets，以加速量子神经网络和量子核心的训练。</li>
<li>results: 通过系统的数字实验，显示了核心集选择法在多种量子机器学习任务中的潜在效果，包括 sintetic数据分类、量子相关性识别和量子编译。<details>
<summary>Abstract</summary>
Quantum neural networks (QNNs) and quantum kernels stand as prominent figures in the realm of quantum machine learning, poised to leverage the nascent capabilities of near-term quantum computers to surmount classical machine learning challenges. Nonetheless, the training efficiency challenge poses a limitation on both QNNs and quantum kernels, curbing their efficacy when applied to extensive datasets. To confront this concern, we present a unified approach: coreset selection, aimed at expediting the training of QNNs and quantum kernels by distilling a judicious subset from the original training dataset. Furthermore, we analyze the generalization error bounds of QNNs and quantum kernels when trained on such coresets, unveiling the comparable performance with those training on the complete original dataset. Through systematic numerical simulations, we illuminate the potential of coreset selection in expediting tasks encompassing synthetic data classification, identification of quantum correlations, and quantum compiling. Our work offers a useful way to improve diverse quantum machine learning models with a theoretical guarantee while reducing the training cost.
</details>
<details>
<summary>摘要</summary>
量子神经网络（QNN）和量子kernels作为量子机器学习领域的代表性 figma，潜在地利用近期量子计算机的潜在能力，以超越 классиical机器学习挑战。然而，训练效率问题成为了QNN和量子kernels的限制因素，在处理大规模数据时减少了它们的效果。为了解决这个问题，我们提出了一种统一方法：核心选择，旨在加速QNN和量子kernels的训练过程，通过筛选judicioussubset从原始训练集。此外，我们分析了QNN和量子kernels在 Such coresets上进行训练时的泛化误差 bound，发现它们与原始数据集上进行训练时的性能相似。通过系统的数字实验，我们描述了核心选择在加速Synthetic数据分类、量子相关性识别和量子编译等任务中的潜在优势。我们的工作提供了一种可靠的方法来改进多种量子机器学习模型，同时降低训练成本。
</details></li>
</ul>
<hr>
<h2 id="Graph-Neural-Networks-for-Dynamic-Modeling-of-Roller-Bearing"><a href="#Graph-Neural-Networks-for-Dynamic-Modeling-of-Roller-Bearing" class="headerlink" title="Graph Neural Networks for Dynamic Modeling of Roller Bearing"></a>Graph Neural Networks for Dynamic Modeling of Roller Bearing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10418">http://arxiv.org/abs/2309.10418</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vinay Sharma, Jens Ravesloot, Cees Taal, Olga Fink</li>
<li>for: 预测滚珠式机械的动态行为</li>
<li>methods: 使用图 neuronal networks (GNNs) 模型，利用图表示滚珠机械的组件之间的复杂关系和互动</li>
<li>results: 通过测试不同的滚珠机械配置，证明 GNN 模型在准确预测滚珠机械的动态行为方面具有良好的学习和泛化能力，表明其在实时监测旋转机械的健康状况中具有潜在的应用前景。<details>
<summary>Abstract</summary>
In the presented work, we propose to apply the framework of graph neural networks (GNNs) to predict the dynamics of a rolling element bearing. This approach offers generalizability and interpretability, having the potential for scalable use in real-time operational digital twin systems for monitoring the health state of rotating machines. By representing the bearing's components as nodes in a graph, the GNN can effectively model the complex relationships and interactions among them. We utilize a dynamic spring-mass-damper model of a bearing to generate the training data for the GNN. In this model, discrete masses represent bearing components such as rolling elements, inner raceways, and outer raceways, while a Hertzian contact model is employed to calculate the forces between these components.   We evaluate the learning and generalization capabilities of the proposed GNN framework by testing different bearing configurations that deviate from the training configurations. Through this approach, we demonstrate the effectiveness of the GNN-based method in accurately predicting the dynamics of rolling element bearings, highlighting its potential for real-time health monitoring of rotating machinery.
</details>
<details>
<summary>摘要</summary>
在提出的工作中，我们提议使用图 neural network (GNN) 模型来预测滚动元件机械的动态行为。这种方法具有普适性和可解释性，有可能在实时操作中的数字双系统中进行扩展使用，以监测旋转机器的健康状态。通过在图中表示机械的组件为节点，GNN 可以有效地模型机械中复杂的关系和互动。我们利用一种动态春荷振荷模型来生成训练数据，其中离散质量表示机械中的滚动元件、内环和外环等组件，而哈特兹振荷模型则用于计算这些组件之间的力。我们通过测试不同的机械配置来评估 GNN 模型的学习和泛化能力。通过这种方法，我们证明了 GNN 模型在准确预测滚动元件机械的动态行为方面的效iveness，这种方法还有可能在实时监测旋转机器的健康状态。
</details></li>
</ul>
<hr>
<h2 id="A-Variational-Auto-Encoder-Enabled-Multi-Band-Channel-Prediction-Scheme-for-Indoor-Localization"><a href="#A-Variational-Auto-Encoder-Enabled-Multi-Band-Channel-Prediction-Scheme-for-Indoor-Localization" class="headerlink" title="A Variational Auto-Encoder Enabled Multi-Band Channel Prediction Scheme for Indoor Localization"></a>A Variational Auto-Encoder Enabled Multi-Band Channel Prediction Scheme for Indoor Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12200">http://arxiv.org/abs/2309.12200</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruihao Yuan, Kaixuan Huang, Pan Yang, Shunqing Zhang</li>
<li>for: 提高室内地位标定精度，适用于虚拟&#x2F;增强现实和智能家居等高技术应用。</li>
<li>methods: 使用频域预测channe state information (CSI)值，并将多频信息合并以提高室内地位标定精度。</li>
<li>results: 在COST 2100 simulate数据和实际时orthogonal frequency division multiplexing (OFDM) WiFi数据中测试了提议方案，并获得了更精度的室内地位标定结果。<details>
<summary>Abstract</summary>
Indoor localization is getting increasing demands for various cutting-edged technologies, like Virtual/Augmented reality and smart home. Traditional model-based localization suffers from significant computational overhead, so fingerprint localization is getting increasing attention, which needs lower computation cost after the fingerprint database is built. However, the accuracy of indoor localization is limited by the complicated indoor environment which brings the multipath signal refraction. In this paper, we provided a scheme to improve the accuracy of indoor fingerprint localization from the frequency domain by predicting the channel state information (CSI) values from another transmitting channel and spliced the multi-band information together to get more precise localization results. We tested our proposed scheme on COST 2100 simulation data and real time orthogonal frequency division multiplexing (OFDM) WiFi data collected from an office scenario.
</details>
<details>
<summary>摘要</summary>
室内定位技术正在不断受到不同的前沿技术的需求，如虚拟/增强现实和智能家居。传统的模型基地定位技术具有较高的计算开销，因此脸部特征定位技术在受到越来越多的关注，需要降低计算成本后，脸部特征定位技术可以在室内环境中提供更高的精度。然而，室内环境的复杂性使得多Path信号折射带来了局部定位精度的限制。本文提出了一种从频域提高室内脸部特征定位精度的方案，通过预测另一个传输通道的通道状态信息（CSI）值，并将多频信息组合在一起，以获得更加精确的定位结果。我们在COST 2100 simulatedata和实时的orthogonal frequency division multiplexing（OFDM）WiFi数据中测试了我们的提议方案，并取得了更好的定位精度。
</details></li>
</ul>
<hr>
<h2 id="Minimum-width-for-universal-approximation-using-ReLU-networks-on-compact-domain"><a href="#Minimum-width-for-universal-approximation-using-ReLU-networks-on-compact-domain" class="headerlink" title="Minimum width for universal approximation using ReLU networks on compact domain"></a>Minimum width for universal approximation using ReLU networks on compact domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10402">http://arxiv.org/abs/2309.10402</a></li>
<li>repo_url: None</li>
<li>paper_authors: Namjun Kim, Chanho Min, Sejun Park</li>
<li>for: 这个论文研究了宽度约束的网络的通用近似性Property，作为深度约束网络的对偶。</li>
<li>methods: 作者使用了几种方法来Characterize the minimum width $w_{\min}$ enabling the universal approximation property, but only a few of them found the exact values.</li>
<li>results: 作者证明了最小宽度为 $\max{d_x,d_y,2}$ 可以 universally approximate $L^p$ functions from $[0,1]^{d_x}$ to $\mathbb R^{d_y}$，且比知道的结果 $w_{\min}&#x3D;\max{d_x+1,d_y}$ 更低。此外，作者还证明了一个下界于 $w_{\min}$ 的上限，即 $w_{\min}\ge d_y+1$ if $d_x&lt;d_y\le2d_x$。<details>
<summary>Abstract</summary>
The universal approximation property of width-bounded networks has been studied as a dual of the classical universal approximation theorem for depth-bounded ones. There were several attempts to characterize the minimum width $w_{\min}$ enabling the universal approximation property; however, only a few of them found the exact values. In this work, we show that the minimum width for the universal approximation of $L^p$ functions from $[0,1]^{d_x}$ to $\mathbb R^{d_y}$ is exactly $\max\{d_x,d_y,2\}$ if an activation function is ReLU-Like (e.g., ReLU, GELU, Softplus). Compared to the known result $w_{\min}=\max\{d_x+1,d_y\}$ when the domain is ${\mathbb R^{d_x}$, our result first shows that approximation on a compact domain requires smaller width than on ${\mathbb R^{d_x}$. We next prove a lower bound on $w_{\min}$ for uniform approximation using general activation functions including ReLU: $w_{\min}\ge d_y+1$ if $d_x<d_y\le2d_x$. Together with our first result, this shows a dichotomy between $L^p$ and uniform approximations for general activation functions and input/output dimensions.
</details>
<details>
<summary>摘要</summary>
全球近似性性的宽度约束网络的研究已被视为深度约束网络的 dual。虽有几种尝试Characterizing the minimum width $w_{\min}$ enabling the universal approximation property, but only a few found the exact values. In this work, we show that the minimum width for the universal approximation of $L^p$ functions from $[0,1]^{d_x}$ to $\mathbb R^{d_y}$ is exactly $\max\{d_x,d_y,2\}$ if the activation function is ReLU-Like (e.g., ReLU, GELU, Softplus). Compared to the known result $w_{\min}=\max\{d_x+1,d_y\}$ when the domain is $\mathbb R^{d_x}$, our result first shows that approximation on a compact domain requires smaller width than on $\mathbb R^{d_x}$. We next prove a lower bound on $w_{\min}$ for uniform approximation using general activation functions including ReLU: $w_{\min}\ge d_y+1$ if $d_x<d_y\le2d_x$. Together with our first result, this shows a dichotomy between $L^p$ and uniform approximations for general activation functions and input/output dimensions.Here's the translation in Traditional Chinese:全球近似性性的宽度约束网络的研究已被视为深度约束网络的 dual。处有几种尝试Characterizing the minimum width $w_{\min}$ enabling the universal approximation property, but only a few found the exact values. In this work, we show that the minimum width for the universal approximation of $L^p$ functions from $[0,1]^{d_x}$ to $\mathbb R^{d_y}$ is exactly $\max\{d_x,d_y,2\}$ if the activation function is ReLU-Like (e.g., ReLU, GELU, Softplus). Compared to the known result $w_{\min}=\max\{d_x+1,d_y\}$ when the domain is $\mathbb R^{d_x}$, our result first shows that approximation on a compact domain requires smaller width than on $\mathbb R^{d_x}$. We next prove a lower bound on $w_{\min}$ for uniform approximation using general activation functions including ReLU: $w_{\min}\ge d_y+1$ if $d_x<d_y\le2d_x$. Together with our first result, this shows a dichotomy between $L^p$ and uniform approximations for general activation functions and input/output dimensions.
</details></li>
</ul>
<hr>
<h2 id="Differentiable-Quantum-Architecture-Search-for-Quantum-Reinforcement-Learning"><a href="#Differentiable-Quantum-Architecture-Search-for-Quantum-Reinforcement-Learning" class="headerlink" title="Differentiable Quantum Architecture Search for Quantum Reinforcement Learning"></a>Differentiable Quantum Architecture Search for Quantum Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10392">http://arxiv.org/abs/2309.10392</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yize Sun, Yunpu Ma, Volker Tresp</li>
<li>for: 该研究的目标是确定 whether DQAS 可以应用于量子深 Q-学习问题。</li>
<li>methods: 该研究使用了一种梯度基于的架构搜索框架 DQAS，并在两个不同的环境 - 护套和冰湖上进行了评估。</li>
<li>results: 实验结果显示 DQAS 可以自动和高效地设计量子电路，并且在训练过程中，自动创建的电路表现较为出色。<details>
<summary>Abstract</summary>
Differentiable quantum architecture search (DQAS) is a gradient-based framework to design quantum circuits automatically in the NISQ era. It was motivated by such as low fidelity of quantum hardware, low flexibility of circuit architecture, high circuit design cost, barren plateau (BP) problem, and periodicity of weights. People used it to address error mitigation, unitary decomposition, and quantum approximation optimization problems based on fixed datasets. Quantum reinforcement learning (QRL) is a part of quantum machine learning and often has various data. QRL usually uses a manually designed circuit. However, the pre-defined circuit needs more flexibility for different tasks, and the circuit design based on various datasets could become intractable in the case of a large circuit. The problem of whether DQAS can be applied to quantum deep Q-learning with various datasets is still open. The main target of this work is to discover the capability of DQAS to solve quantum deep Q-learning problems. We apply a gradient-based framework DQAS on reinforcement learning tasks and evaluate it in two different environments - cart pole and frozen lake. It contains input- and output weights, progressive search, and other new features. The experiments conclude that DQAS can design quantum circuits automatically and efficiently. The evaluation results show significant outperformance compared to the manually designed circuit. Furthermore, the performance of the automatically created circuit depends on whether the super-circuit learned well during the training process. This work is the first to show that gradient-based quantum architecture search is applicable to QRL tasks.
</details>
<details>
<summary>摘要</summary>
“差分可求量化架构搜寻（DQAS）是一个基于梯度的框架，用于自动设计量子Circuit在NISQ时代。它受到了低精度量子硬件、固定Circuit架构、高Circuit设计成本、梯度扁平（BP）问题以及periodicity of weights等因素的激励。人们使用它来解决错误补偿、单元分解和量子近似优化问题，基于固定数据集。量子回传学习（QRL）通常使用预先设计的Circuit，但这种预先设计的Circuit需要更多的灵活性以应对不同的任务，而且基于不同的数据集进行Circuit设计可能会成为实际问题。这篇研究的主要目标是探索DQAS能否应用于量子深度Q-学习问题。我们将DQAS应用到了循环学习任务上，并在滑车和冻湖两个不同环境中进行评估。实验结果显示DQAS可以自动设计量子Circuit，效率高。评估结果显示DQAS可以对量子深度Q-学习问题提供明显的超越性。此外，自动创建的Circuit表现取决于在训练过程中超Circuit是否从良好的学习。这是首次显示出gradient-based量子架构搜寻可以应用到QRL任务。”
</details></li>
</ul>
<hr>
<h2 id="Testable-Likelihoods-for-Beyond-the-Standard-Model-Fits"><a href="#Testable-Likelihoods-for-Beyond-the-Standard-Model-Fits" class="headerlink" title="Testable Likelihoods for Beyond-the-Standard Model Fits"></a>Testable Likelihoods for Beyond-the-Standard Model Fits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10365">http://arxiv.org/abs/2309.10365</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anja Beck, Méril Reboud, Danny van Dyk</li>
<li>for: 这 paper 的目的是研究精度前沿中BSM效应的可能性，需要准确地从低能量测量转移信息到高能BSM模型。</li>
<li>methods: 这 paper 使用normalizing flow来构建 likelihood 函数，以实现这种转移。 likelihood 函数可以生成更多的样本，并且允许一种“轻松”的准确性测试，表现为 $\chi^2$ 统计量。</li>
<li>results: 这 paper 研究了一种特定的normalizing flow，并应用它到一个多模态和非泊松的例子中，并评估了 likelihood 函数和测试统计量的准确性。<details>
<summary>Abstract</summary>
Studying potential BSM effects at the precision frontier requires accurate transfer of information from low-energy measurements to high-energy BSM models. We propose to use normalising flows to construct likelihood functions that achieve this transfer. Likelihood functions constructed in this way provide the means to generate additional samples and admit a ``trivial'' goodness-of-fit test in form of a $\chi^2$ test statistic. Here, we study a particular form of normalising flow, apply it to a multi-modal and non-Gaussian example, and quantify the accuracy of the likelihood function and its test statistic.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:研究BSM效应的精度前沿需要从低能量测量中准确地传递信息到高能BSM模型。我们提议使用 нормализа流来构建likelihood函数，以实现这种传递。通过这种方法，likelihood函数可以生成更多的样本，并且可以使用“轻松”的goodness-of-fit测试 Statistics。在这里，我们研究了一种特定的 нормализа流，将其应用于多模态和非泊松的示例，并评估likelihood函数和测试统计值的准确性。
</details></li>
</ul>
<hr>
<h2 id="Striking-a-Balance-An-Optimal-Mechanism-Design-for-Heterogenous-Differentially-Private-Data-Acquisition-for-Logistic-Regression"><a href="#Striking-a-Balance-An-Optimal-Mechanism-Design-for-Heterogenous-Differentially-Private-Data-Acquisition-for-Logistic-Regression" class="headerlink" title="Striking a Balance: An Optimal Mechanism Design for Heterogenous Differentially Private Data Acquisition for Logistic Regression"></a>Striking a Balance: An Optimal Mechanism Design for Heterogenous Differentially Private Data Acquisition for Logistic Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10340">http://arxiv.org/abs/2309.10340</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ameya Anjarlekar, Rasoul Etesami, R. Srikant</li>
<li>for: 这篇论文是为了解决在隐私敏感的卖家数据上进行逻辑回归的问题。由于数据是隐私的，卖家需要通过支付来提供数据，因此目标是设计一种机制，可以平衡多个目标，包括测试损失、卖家隐私和支付。</li>
<li>methods: 该论文使用了游戏理论、统计学学习理论和隐私保护来解决这个问题。在解决方案中，我们使用了变量转换来将买家的目标函数转换为凸函数，从而使问题可以被 convexified。</li>
<li>results: 我们提供了一些各向异性结果，包括测试错误和支付量在数据量很大时的极限分布。此外，我们还应用了我们的想法到一个实际的医疗数据集中，以便展示我们的方法在实际应用中的效果。<details>
<summary>Abstract</summary>
We investigate the problem of performing logistic regression on data collected from privacy-sensitive sellers. Since the data is private, sellers must be incentivized through payments to provide their data. Thus, the goal is to design a mechanism that optimizes a weighted combination of test loss, seller privacy, and payment, i.e., strikes a balance between multiple objectives of interest. We solve the problem by combining ideas from game theory, statistical learning theory, and differential privacy. The buyer's objective function can be highly non-convex. However, we show that, under certain conditions on the problem parameters, the problem can be convexified by using a change of variables. We also provide asymptotic results characterizing the buyer's test error and payments when the number of sellers becomes large. Finally, we demonstrate our ideas by applying them to a real healthcare data set.
</details>
<details>
<summary>摘要</summary>
我们研究在收集来自隐私敏感卖家的数据时进行логистиック回归的问题。由于数据是私人的，卖家需要通过支付来激励提供数据。因此，我们的目标是设计一种机制，以优化一个权衡多个目标函数，即测试损失、卖家隐私和支付。我们利用游戏理论、统计学学习理论和幂等隐私来解决这个问题。买家的目标函数可能很不对称。然而，我们证明，在某些问题参数的条件下，问题可以通过变量变换 convex化。我们还提供了大量卖家测试错误和支付的极限结果，并将我们的想法应用到一个真实的医疗数据集中。
</details></li>
</ul>
<hr>
<h2 id="Computational-Approaches-for-App-to-App-Retrieval-and-Design-Consistency-Check"><a href="#Computational-Approaches-for-App-to-App-Retrieval-and-Design-Consistency-Check" class="headerlink" title="Computational Approaches for App-to-App Retrieval and Design Consistency Check"></a>Computational Approaches for App-to-App Retrieval and Design Consistency Check</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10328">http://arxiv.org/abs/2309.10328</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seokhyeon Park, Wonjae Kim, Young-Ho Kim, Jinwook Seo</li>
<li>for: 这paper的目的是提出一种基于大规模网络图像训练的视觉模型，用于从零开始提取移动用户界面（UI）的含义表示，并用于设计决策过程中的 Computational design support tools。</li>
<li>methods: 这paper使用的方法包括使用大规模网络图像训练的视觉模型，以及使用数学基础的方法来实现应用程序之间的比较和设计一致性分析。</li>
<li>results: 实验结果显示，该方法不仅超越了先前的提取模型，还启用了多种新的应用程序。<details>
<summary>Abstract</summary>
Extracting semantic representations from mobile user interfaces (UI) and using the representations for designers' decision-making processes have shown the potential to be effective computational design support tools. Current approaches rely on machine learning models trained on small-sized mobile UI datasets to extract semantic vectors and use screenshot-to-screenshot comparison to retrieve similar-looking UIs given query screenshots. However, the usability of these methods is limited because they are often not open-sourced and have complex training pipelines for practitioners to follow, and are unable to perform screenshot set-to-set (i.e., app-to-app) retrieval. To this end, we (1) employ visual models trained with large web-scale images and test whether they could extract a UI representation in a zero-shot way and outperform existing specialized models, and (2) use mathematically founded methods to enable app-to-app retrieval and design consistency analysis. Our experiments show that our methods not only improve upon previous retrieval models but also enable multiple new applications.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXT现有的方法仅仅是使用小规模的手机用户界面（UI）数据集来训练机器学习模型，从而提取Semantic vectors和使用屏幕截图比较来检索类似的UI。然而，这些方法的可用性受限，因为它们通常不是开源的，具有复杂的训练管道，并且无法进行应用程序之间（i.e., app-to-app）检索。为了解决这些问题，我们采用以下两种方法：1. 使用大规模的网络图像训练视觉模型，以验证这些模型可以在零容量情况下提取UI表示，并且超越现有的专门模型。2. 使用数学基础的方法来启用应用程序之间的检索和设计一致分析。我们的实验结果表明，我们的方法不仅超越了现有的检索模型，还启用了多种新的应用。TRANSLATE_TEXTHere's the translation in Traditional Chinese as well:<<SYS>>TRANSLATE_TEXT现有的方法只是使用小规模的手机用户界面（UI）数据集来训练机器学习模型，从而提取Semantic vectors和使用屏幕截图比较来检索类似的UI。然而，这些方法的可用性受限，因为它们通常不是开源的，具有复杂的训练管道，并且无法进行应用程序之间（i.e., app-to-app）检索。为了解决这些问题，我们采用以下两种方法：1. 使用大规模的网络图像训练视觉模型，以验证这些模型可以在零容量情况下提取UI表示，并且超越现有的专门模型。2. 使用数学基础的方法来启用应用程序之间的检索和设计一致分析。我们的实验结果显示，我们的方法不仅超越了现有的检索模型，也启用了多种新的应用。TRANSLATE_TEXT
</details></li>
</ul>
<hr>
<h2 id="TensorCodec-Compact-Lossy-Compression-of-Tensors-without-Strong-Data-Assumptions"><a href="#TensorCodec-Compact-Lossy-Compression-of-Tensors-without-Strong-Data-Assumptions" class="headerlink" title="TensorCodec: Compact Lossy Compression of Tensors without Strong Data Assumptions"></a>TensorCodec: Compact Lossy Compression of Tensors without Strong Data Assumptions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10310">http://arxiv.org/abs/2309.10310</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kbrother/tensorcodec">https://github.com/kbrother/tensorcodec</a></li>
<li>paper_authors: Taehyung Kwon, Jihoon Ko, Jinhong Jung, Kijung Shin</li>
<li>for: 这个论文的目的是提出一种lossy压缩算法 для一般的tensor，以提高压缩率和准确性。</li>
<li>methods: 这个算法使用了三个关键想法：首先， integrate a recurrent neural network into Tensor-Train Decomposition，以增强其表达力和降低低级假设的限制。其次，折叠输入tensor到更高级tensor，以降低NTTD所需的空间。最后，重新排序输入tensor的模式索引，以便通过NTTD进行更好的预测。</li>
<li>results: 该算法可以达到以下三个目标：（a） concise：它可以提供7.38倍的更紧凑的压缩，与相同的重建错误相比；（b） accurate：给定相同的压缩大小预算，它可以提供3.33倍的更准确的重建，与相同的重建错误相比；（c） scalable：其实际压缩时间是线性增长，并且每个Entry的重建时间是对数增长。<details>
<summary>Abstract</summary>
Many real-world datasets are represented as tensors, i.e., multi-dimensional arrays of numerical values. Storing them without compression often requires substantial space, which grows exponentially with the order. While many tensor compression algorithms are available, many of them rely on strong data assumptions regarding its order, sparsity, rank, and smoothness. In this work, we propose TENSORCODEC, a lossy compression algorithm for general tensors that do not necessarily adhere to strong input data assumptions. TENSORCODEC incorporates three key ideas. The first idea is Neural Tensor-Train Decomposition (NTTD) where we integrate a recurrent neural network into Tensor-Train Decomposition to enhance its expressive power and alleviate the limitations imposed by the low-rank assumption. Another idea is to fold the input tensor into a higher-order tensor to reduce the space required by NTTD. Finally, the mode indices of the input tensor are reordered to reveal patterns that can be exploited by NTTD for improved approximation. Our analysis and experiments on 8 real-world datasets demonstrate that TENSORCODEC is (a) Concise: it gives up to 7.38x more compact compression than the best competitor with similar reconstruction error, (b) Accurate: given the same budget for compressed size, it yields up to 3.33x more accurate reconstruction than the best competitor, (c) Scalable: its empirical compression time is linear in the number of tensor entries, and it reconstructs each entry in logarithmic time. Our code and datasets are available at https://github.com/kbrother/TensorCodec.
</details>
<details>
<summary>摘要</summary>
许多实际数据集都是tensor的形式，即多维数值数组。不压缩存储这些tensor可能需要巨大的存储空间，其增长为 exponent。虽然有很多tensor压缩算法可用，但是许多它们假设输入tensor的级数、稀疏性、核心级和平滑性。在这种工作中，我们提出了TENSORCODEC，一种lossy压缩算法，用于通用的tensor，不 necesarily遵循强大的输入数据假设。TENSORCODEC包括三个关键想法：首先，我们将输入tensor integrate到一个循环神经网络中，以提高表达力并缓解低级假设的限制。其次，我们将输入tensor折叠成更高级的tensor，以降低存储空间的需求。最后，我们重新排序输入tensor的模式索引，以便NTTD可以更好地利用这些模式进行改进的approximation。我们的分析和实验表明，TENSORCODEC具有以下特点：（a） Concise：它可以提供与最佳竞争对手相同的压缩比，但是具有7.38倍的容器大小；（b） Accurate：给定相同的压缩容器大小，它可以提供3.33倍的更高精度重建结果；（c） Scalable：其实际压缩时间是线性增长的tensor入口数量，并且每个入口的重建时间是对数增长的。我们的代码和数据集可以在https://github.com/kbrother/TensorCodec中获得。
</details></li>
</ul>
<hr>
<h2 id="Prominent-Roles-of-Conditionally-Invariant-Components-in-Domain-Adaptation-Theory-and-Algorithms"><a href="#Prominent-Roles-of-Conditionally-Invariant-Components-in-Domain-Adaptation-Theory-and-Algorithms" class="headerlink" title="Prominent Roles of Conditionally Invariant Components in Domain Adaptation: Theory and Algorithms"></a>Prominent Roles of Conditionally Invariant Components in Domain Adaptation: Theory and Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10301">http://arxiv.org/abs/2309.10301</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hieu9955/ggggg">https://github.com/hieu9955/ggggg</a></li>
<li>paper_authors: Keru Wu, Yuansi Chen, Wooseok Ha, Bin Yu</li>
<li>For: The paper focuses on the assumption of conditionally invariant components (CICs) in domain adaptation (DA) and explores their role in providing target risk guarantees.* Methods: The paper proposes a new algorithm called importance-weighted conditional invariant penalty (IW-CIP) based on CICs, which has target risk guarantees beyond simple settings such as covariate shift and label shift. Additionally, the paper shows that CICs help identify large discrepancies between source and target risks of other DA algorithms.* Results: The paper demonstrates the effectiveness of the proposed algorithm and theoretical findings via numerical experiments on synthetic data, MNIST, CelebA, and Camelyon17 datasets. Specifically, the paper shows that incorporating CICs into the domain invariant projection (DIP) algorithm can address its failure scenario caused by label-flipping features.<details>
<summary>Abstract</summary>
Domain adaptation (DA) is a statistical learning problem that arises when the distribution of the source data used to train a model differs from that of the target data used to evaluate the model. While many DA algorithms have demonstrated considerable empirical success, blindly applying these algorithms can often lead to worse performance on new datasets. To address this, it is crucial to clarify the assumptions under which a DA algorithm has good target performance. In this work, we focus on the assumption of the presence of conditionally invariant components (CICs), which are relevant for prediction and remain conditionally invariant across the source and target data. We demonstrate that CICs, which can be estimated through conditional invariant penalty (CIP), play three prominent roles in providing target risk guarantees in DA. First, we propose a new algorithm based on CICs, importance-weighted conditional invariant penalty (IW-CIP), which has target risk guarantees beyond simple settings such as covariate shift and label shift. Second, we show that CICs help identify large discrepancies between source and target risks of other DA algorithms. Finally, we demonstrate that incorporating CICs into the domain invariant projection (DIP) algorithm can address its failure scenario caused by label-flipping features. We support our new algorithms and theoretical findings via numerical experiments on synthetic data, MNIST, CelebA, and Camelyon17 datasets.
</details>
<details>
<summary>摘要</summary>
域适应（DA）是一个统计学习问题，其中源数据用于训练模型的分布与目标数据用于评估模型的分布不同。虽然许多DA算法已经显示了较好的实际成果，但是盲目地应用这些算法可能会导致新的数据集上的性能更差。为了解决这问题，需要清楚地明确DA算法在目标数据上的性能假设。在这种工作中，我们关注了conditionally invariant component（CIC）的存在，它们在预测中 relevante和目标数据中具有条件不变性。我们示出了CICs可以通过conditional invariant penalty（CIP）来估计，它们在域适应中提供了三个主要的角色：首先，我们提出了一种基于CICs的新算法，即importance-weighted conditional invariant penalty（IW-CIP），它在更复杂的设置中，如covariate shift和label shift，具有更多的目标风险保证。其次，我们表明CICs可以帮助标识源和目标数据之间的大量差异。最后，我们示出了在DIP算法中包含CICs可以解决因为label-flipping特征而导致的失败情况。我们通过synthetic数据、MNIST、CelebA和Camelyon17 dataset的numerical experiments支持我们的新算法和理论发现。
</details></li>
</ul>
<hr>
<h2 id="Learning-Orbitally-Stable-Systems-for-Diagrammatically-Teaching"><a href="#Learning-Orbitally-Stable-Systems-for-Diagrammatically-Teaching" class="headerlink" title="Learning Orbitally Stable Systems for Diagrammatically Teaching"></a>Learning Orbitally Stable Systems for Diagrammatically Teaching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10298">http://arxiv.org/abs/2309.10298</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiming Zhi, Kangni Liu, Tianyi Zhang, Matthew Johnson-Roberson</li>
<li>for: 教师机器人novel技能</li>
<li>methods: 使用用户提供的2D图像来形态 robot的运动</li>
<li>results: 可以 diagrammatically teach complex cyclic motion patterns with a high degree of accuracy.Here is the full translation of the abstract in Simplified Chinese:</li>
<li>for: 本文旨在教师机器人novel技能，使其可以适应复杂的循环运动模式。</li>
<li>methods: 本文使用用户提供的2D图像来形态 robot的运动，并通过应用diffomorphism来模拟 robot的运动。</li>
<li>results: 实验结果显示，可以使用diffomorphism来教师机器人完成复杂的循环运动，并且具有高度准确性。<details>
<summary>Abstract</summary>
Diagrammatic Teaching is a paradigm for robots to acquire novel skills, whereby the user provides 2D sketches over images of the scene to shape the robot's motion. In this work, we tackle the problem of teaching a robot to approach a surface and then follow cyclic motion on it, where the cycle of the motion can be arbitrarily specified by a single user-provided sketch over an image from the robot's camera. Accordingly, we introduce the \emph{Stable Diffeomorphic Diagrammatic Teaching} (SDDT) framework. SDDT models the robot's motion as an \emph{Orbitally Asymptotically Stable} (O.A.S.) dynamical system that learns to follow the user-specified sketch. This is achieved by applying a \emph{diffeomorphism}, i.e. a differentiable and invertible function, to morph a known O.A.S. system. The parameterised diffeomorphism is then optimised with respect to the Hausdorff distance between the limit cycle of our modelled system and the sketch, to produce the desired robot motion. We provide theoretical insight into the behaviour of the optimised system and also empirically evaluate SDDT, both in simulation and on a quadruped with a mounted 6-DOF manipulator. Results show that we can diagrammatically teach complex cyclic motion patterns with a high degree of accuracy.
</details>
<details>
<summary>摘要</summary>
图形教学是一种 робоット学习新技能的方法，其中用户提供的2D图形将影响机器人的运动。在这项工作中，我们解决了教育机器人接近场景中的表面并跟踪循环运动的问题，其中循环运动的周期可以通过用户提供的一个图形来定义。因此，我们介绍了稳定幂函数减杂教学框架（SDDT）。 SDDT将机器人的运动模型为一个稳定幂函数系统，该系统学习从用户提供的图形中学习循环运动。我们通过应用一个幂函数，即一个可导和反函数，将一个已知稳定幂函数系统变换为我们的模型系统。然后，我们对这个参数化的幂函数进行优化，使其与图形中的循环运动的 Hausdorff 距离最小化，以生成所需的机器人运动。我们提供了对优化后的系统行为的理论分析，以及在实验中对 SDDT 的评估，包括在模拟环境和一只四脚机器人上安装了六度 freedom 抓取机的实验。结果表明，我们可以使用图形来教学机器人复杂的循环运动模式，并且具有高度准确性。
</details></li>
</ul>
<hr>
<h2 id="Flash-LLM-Enabling-Cost-Effective-and-Highly-Efficient-Large-Generative-Model-Inference-with-Unstructured-Sparsity"><a href="#Flash-LLM-Enabling-Cost-Effective-and-Highly-Efficient-Large-Generative-Model-Inference-with-Unstructured-Sparsity" class="headerlink" title="Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity"></a>Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10285">http://arxiv.org/abs/2309.10285</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alibabaresearch/flash-llm">https://github.com/alibabaresearch/flash-llm</a></li>
<li>paper_authors: Haojun Xia, Zhen Zheng, Yuchao Li, Donglin Zhuang, Zhongzhu Zhou, Xiafei Qiu, Yong Li, Wei Lin, Shuaiwen Leon Song</li>
<li>for: 大型生成模型的快速部署和高效执行，特别是在高性能但高度限制的tensor核心硬件上。</li>
<li>methods: 提出了一种基于 Load-as-Sparse 和 Compute-as-Dense 方法的 Flash-LLM 框架，以优化 tensor 核心硬件上的大型生成模型执行。</li>
<li>results: Flash-LLM 在 SpMM 层次上至少比 state-of-the-art 库 Sputnik 和 SparTA 快速得多，在 OPT-30B&#x2F;66B&#x2F;175B 模型上达到了最高的 tokens per GPU-second 性能，与 DeepSpeed 和 FasterTransformer 相比，具有显著的性能提升和较低的执行成本。<details>
<summary>Abstract</summary>
With the fast growth of parameter size, it becomes increasingly challenging to deploy large generative models as they typically require large GPU memory consumption and massive computation. Unstructured model pruning has been a common approach to reduce both GPU memory footprint and the overall computation while retaining good model accuracy. However, the existing solutions do not provide a highly-efficient support for handling unstructured sparsity on modern GPUs, especially on the highly-structured Tensor Core hardware. Therefore, we propose Flash-LLM for enabling low-cost and highly-efficient large generative model inference with the sophisticated support of unstructured sparsity on high-performance but highly restrictive Tensor Cores. Based on our key observation that the main bottleneck of generative model inference is the several skinny matrix multiplications for which Tensor Cores would be significantly under-utilized due to low computational intensity, we propose a general Load-as-Sparse and Compute-as-Dense methodology for unstructured sparse matrix multiplication. The basic insight is to address the significant memory bandwidth bottleneck while tolerating redundant computations that are not critical for end-to-end performance on Tensor Cores. Based on this, we design an effective software framework for Tensor Core based unstructured SpMM, leveraging on-chip resources for efficient sparse data extraction and computation/memory-access overlapping. At SpMM kernel level, Flash-LLM significantly outperforms the state-of-the-art library, i.e., Sputnik and SparTA by an average of 2.9x and 1.5x, respectively. At end-to-end framework level on OPT-30B/66B/175B models, for tokens per GPU-second, Flash-LLM achieves up to 3.8x and 3.6x improvement over DeepSpeed and FasterTransformer, respectively, with significantly lower inference cost.
</details>
<details>
<summary>摘要</summary>
随着参数大小的快速增长，大型生成模型的部署变得越来越困难，因为它们通常需要大量的GPU内存消耗和巨量计算。不结构化模型剔除已成为一种常见的方法来降低GPU内存占用和总计算量，同时保持良好的模型准确性。然而，现有的解决方案不提供高效支持对现代GPU上的不结构化稀疏性，特别是在高性能但高度结构化的Tensor Core硬件上。因此，我们提出Flash-LLM，用于实现低成本高效的大型生成模型推理，并且在高性能但高度结构化的Tensor Core硬件上提供了高度支持不结构化稀疏性。我们的关键观察是，生成模型推理的主要瓶颈在于一些瘦剑矩阵乘法，对于Tensor Core硬件来说，这些矩阵乘法的计算INTENSITY很低，导致GPU内存带宽瓶颈。基于这一点，我们提出一种普适的 Load-as-Sparse 和 Compute-as-Dense 方法，用于不结构化稀疏矩阵乘法。这种方法的基本思想是在约束可以忽略的计算上进行缓存和内存访问的重叠，以降低内存带宽瓶颈。基于这一方法，我们设计了一个高效的Tensor Core基于不结构化稀疏矩阵乘法的软件框架，利用GPU内存中的资源进行高效的稀疏数据提取和计算/内存访问重叠。在SpMM kernel层，Flash-LLM比Sputnik和SparTA两者均高效，平均提高2.9倍和1.5倍。在框架层，Flash-LLM在OPT-30B/66B/175B模型上，对于每个GPU每秒的字符数，与DeepSpeed和FasterTransformer相比，提高了3.8倍和3.6倍，同时具有显著更低的推理成本。
</details></li>
</ul>
<hr>
<h2 id="Crowdotic-A-Privacy-Preserving-Hospital-Waiting-Room-Crowd-Density-Estimation-with-Non-speech-Audio"><a href="#Crowdotic-A-Privacy-Preserving-Hospital-Waiting-Room-Crowd-Density-Estimation-with-Non-speech-Audio" class="headerlink" title="Crowdotic: A Privacy-Preserving Hospital Waiting Room Crowd Density Estimation with Non-speech Audio"></a>Crowdotic: A Privacy-Preserving Hospital Waiting Room Crowd Density Estimation with Non-speech Audio</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10280">http://arxiv.org/abs/2309.10280</a></li>
<li>repo_url: None</li>
<li>paper_authors: Forsad Al Hossain, Tanjid Hasan Tonmoy, Andrew A. Lover, George A. Corey, Mohammad Arif Ul Alam, Tauhidur Rahman</li>
<li>For: 本研究旨在提出一种基于非语音audio的人群分析方法，以便在各种场景中提高智能建筑的运行和管理，同时坚持个人隐私要求。* Methods: 我们提出了一种基于转换器模型的非语音audio-based方法，并进行了大量的实验和比较分析，以证明非语音audio alone可以准确地进行人群分析。* Results: 我们的实验结果表明，非语音audio-based方法可以高度准确地估算人群占用率，并且超过了基于热像的模型和其他基线。此外，我们还进行了附加的差分隐私技术分析，以提供进一步的隐私保障。<details>
<summary>Abstract</summary>
Privacy-preserving crowd density analysis finds application across a wide range of scenarios, substantially enhancing smart building operation and management while upholding privacy expectations in various spaces. We propose a non-speech audio-based approach for crowd analytics, leveraging a transformer-based model. Our results demonstrate that non-speech audio alone can be used to conduct such analysis with remarkable accuracy. To the best of our knowledge, this is the first time when non-speech audio signals are proposed for predicting occupancy. As far as we know, there has been no other similar approach of its kind prior to this. To accomplish this, we deployed our sensor-based platform in the waiting room of a large hospital with IRB approval over a period of several months to capture non-speech audio and thermal images for the training and evaluation of our models. The proposed non-speech-based approach outperformed the thermal camera-based model and all other baselines. In addition to demonstrating superior performance without utilizing speech audio, we conduct further analysis using differential privacy techniques to provide additional privacy guarantees. Overall, our work demonstrates the viability of employing non-speech audio data for accurate occupancy estimation, while also ensuring the exclusion of speech-related content and providing robust privacy protections through differential privacy guarantees.
</details>
<details>
<summary>摘要</summary>
隐私保护的人群密度分析在各种场景中发挥着广泛的应用，大幅提高智能建筑的运行和管理，同时坚持隐私期望在不同的空间。我们提出了一种基于转换器的非语音音频方法，用于人群分析。我们的结果表明，非语音音频 alone 可以用于进行这种分析，并且具有惊人的准确性。我们知道，这是第一次使用非语音音频信号进行占用率预测。在这之前，没有任何相似的方法。为了完成这一目标，我们在一所大 Hospital 的等待室中部署了我们的传感器平台，并在一些月份内采集了非语音音频和热成像数据，用于训练和评估我们的模型。我们的非语音基本方法在占用率预测方面表现出色，并且超过了基于热成像模型和所有基eline的表现。此外，我们还进行了进一步的分析，使用差分隐私技术提供了额外的隐私保障。总的来说，我们的工作表明了非语音音频数据的可靠性，而不需要使用语音内容，同时提供了坚实的隐私保障。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Methods-for-Generating-Transition-Paths"><a href="#Diffusion-Methods-for-Generating-Transition-Paths" class="headerlink" title="Diffusion Methods for Generating Transition Paths"></a>Diffusion Methods for Generating Transition Paths</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10276">http://arxiv.org/abs/2309.10276</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luke Triplett, Jianfeng Lu</li>
<li>for: 本研究使用分子系统中的Score-based生成模型来模拟罕见的转移。</li>
<li>methods: 本文提出了两种新的路径生成方法：一种是基于链的方法，另一种是基于中点的方法。</li>
<li>results: 数值结果表明，这两种方法在数据充沛和数据缺乏两种情况下都能够效果地生成转移路径。<details>
<summary>Abstract</summary>
In this work, we seek to simulate rare transitions between metastable states using score-based generative models. An efficient method for generating high-quality transition paths is valuable for the study of molecular systems since data is often difficult to obtain. We develop two novel methods for path generation in this paper: a chain-based approach and a midpoint-based approach. The first biases the original dynamics to facilitate transitions, while the second mirrors splitting techniques and breaks down the original transition into smaller transitions. Numerical results of generated transition paths for the M\"uller potential and for Alanine dipeptide demonstrate the effectiveness of these approaches in both the data-rich and data-scarce regimes.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们寻求使用得分模型来模拟罕见的 между元态态转移。一种高效的方法 для生成高质量的转移路径对分子系统的研究非常有用，因为数据往往困难获取。我们在这篇论文中开发了两种新的方法来生成转移路径：一种链基的方法和一种中点基的方法。第一种偏导原动力学到促进转移，而第二种使用分割技术将原始转移分解成小转移。我们对穆勒潜能和阿拉伦二肽进行了数值研究，并证明了这些方法在数据丰富和数据缺乏两种情况下的有效性。
</details></li>
</ul>
<hr>
<h2 id="Multi-fidelity-climate-model-parameterization-for-better-generalization-and-extrapolation"><a href="#Multi-fidelity-climate-model-parameterization-for-better-generalization-and-extrapolation" class="headerlink" title="Multi-fidelity climate model parameterization for better generalization and extrapolation"></a>Multi-fidelity climate model parameterization for better generalization and extrapolation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10231">http://arxiv.org/abs/2309.10231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamed Aziz Bhouri, Liran Peng, Michael S. Pritchard, Pierre Gentine</li>
<li>for: 这个研究的目的是提出一种基于机器学习的全球气候模型或温室效应模型的参数化方法，以提高模型的准确性和可靠性。</li>
<li>methods: 这种方法使用了多种数据集，包括低精度的物理参数化数据和高精度的气候模拟数据，通过多质量混合来提高模型的准确性和可靠性。</li>
<li>results: 研究发现，使用多质量混合方法可以提供更准确的气候预测，而无需增加计算资源的增加。此外，这种方法还可以提供可靠的不确定性评估，并能够在多种enario下提供更加准确的预测结果。<details>
<summary>Abstract</summary>
Machine-learning-based parameterizations (i.e. representation of sub-grid processes) of global climate models or turbulent simulations have recently been proposed as a powerful alternative to physical, but empirical, representations, offering a lower computational cost and higher accuracy. Yet, those approaches still suffer from a lack of generalization and extrapolation beyond the training data, which is however critical to projecting climate change or unobserved regimes of turbulence. Here we show that a multi-fidelity approach, which integrates datasets of different accuracy and abundance, can provide the best of both worlds: the capacity to extrapolate leveraging the physically-based parameterization and a higher accuracy using the machine-learning-based parameterizations. In an application to climate modeling, the multi-fidelity framework yields more accurate climate projections without requiring major increase in computational resources. Our multi-fidelity randomized prior networks (MF-RPNs) combine physical parameterization data as low-fidelity and storm-resolving historical run's data as high-fidelity. To extrapolate beyond the training data, the MF-RPNs are tested on high-fidelity warming scenarios, $+4K$, data. We show the MF-RPN's capacity to return much more skillful predictions compared to either low- or high-fidelity (historical data) simulations trained only on one regime while providing trustworthy uncertainty quantification across a wide range of scenarios. Our approach paves the way for the use of machine-learning based methods that can optimally leverage historical observations or high-fidelity simulations and extrapolate to unseen regimes such as climate change.
</details>
<details>
<summary>摘要</summary>
globale 气候模型或液体动力学 simulations中的机器学习基 Parameters (i.e. 表示 sub-grid 过程的 representation) 被提议为一种有力的代替physical, but empirical, representations，提供更低的计算成本和更高的准确性。然而，这些方法仍然受到 extrapolation beyond the training data 的限制，这是 projecting 气候变化或未观察到的液体动力学 regime 的 kritical 因素。我们展示了一种多 fideliTY 方法，该方法 integrate 不同精度和充沛的数据集，可以提供best of both worlds：可以 extrapolate  leveraging physically-based parameterization，同时使用 machine-learning-based parameterizations 提供更高的准确性。在气候模型中，我们的多 fideliTY 框架实现了更准确的气候预测，不需要大幅增加计算资源。我们的多 fideliTY randomized prior networks (MF-RPNs) 组合物理参数化数据作为low-fidelity，并使用风暴解决的历史数据作为高精度。为了 extrapolate beyond the training data，MF-RPNs 在高精度增温enario数据上进行测试。我们显示MF-RPNs 能够返回更有技巧的预测，比 Either low- 或 high-fidelity (历史数据)  simulations 训练只在一个 режиме上，同时提供可靠的uncertainty quantification  across a wide range of scenarios。我们的方法开创了使用机器学习基 methods 可以最佳地利用历史观察或高精度 simulations 并 extrapolate to unseen regimes such as climate change。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/19/cs.LG_2023_09_19/" data-id="closbrorg00px0g882sp48n7b" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_09_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/19/eess.IV_2023_09_19/" class="article-date">
  <time datetime="2023-09-19T09:00:00.000Z" itemprop="datePublished">2023-09-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/19/eess.IV_2023_09_19/">eess.IV - 2023-09-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Multi-Spectral-Reflection-Matrix-for-Ultra-Fast-3D-Label-Free-Microscopy"><a href="#Multi-Spectral-Reflection-Matrix-for-Ultra-Fast-3D-Label-Free-Microscopy" class="headerlink" title="Multi-Spectral Reflection Matrix for Ultra-Fast 3D Label-Free Microscopy"></a>Multi-Spectral Reflection Matrix for Ultra-Fast 3D Label-Free Microscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10951">http://arxiv.org/abs/2309.10951</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Balondrade, Victor Barolle, Nicolas Guigui, Emeric Auriant, Nathan Rougier, Claude Boccara, Mathias Fink, Alexandre Aubry</li>
<li>for: 实现深入、实时、量化的生物组织观察</li>
<li>methods: 多спектル矩阵方法</li>
<li>results: 实现0.1mm^3的场视野，290nm的分辨率，1Hz的帧率三维图像<details>
<summary>Abstract</summary>
Label-free microscopy exploits light scattering to obtain a three-dimensional image of biological tissues. However, light propagation is affected by aberrations and multiple scattering, which drastically degrade the image quality and limit the penetration depth. Multi-conjugate adaptive optics and time-gated matrix approaches have been developed to compensate for aberrations but the associated frame rate is extremely limited for 3D imaging. Here we develop a multi-spectral matrix approach to solve these fundamental problems. Based on an interferometric measurement of a polychromatic reflection matrix, the focusing process can be optimized in post-processing at any voxel by addressing independently each frequency component of the wave-field. A proof-of-concept experiment demonstrates the three-dimensional image of an opaque human cornea over a 0.1 mm^3-field-of-view at a 290 nm-resolution and a 1 Hz-frame rate. This work paves the way towards a fully-digital microscope allowing real-time, in-vivo, quantitative and deep inspection of tissues.
</details>
<details>
<summary>摘要</summary>
Label-free microscopy 利用光散射获取生物组织的三维图像。然而，光束传播受到偏振和多散射的影响，导致图像质量严重下降，限制了温度深度。多 conjugate adaptive optics 和时间锁定矩阵方法已经开发，但这些方法的相关帧率非常低，不适于3D图像。在这种情况下，我们开发了一种多 spectral matrix 方法。基于一种多色干涉测量，我们可以在后处理中独立地处理每个频率成分的波场，从而优化焦点处理。一个证明实验表明，我们可以在0.1 mm^3 的场视野内获得290 nm 的分辨率和1 Hz 的帧率。这种工作开创了一种完全数字的镜像机，允许实时、生物体内、量化和深入检查组织。
</details></li>
</ul>
<hr>
<h2 id="Multisource-Holography"><a href="#Multisource-Holography" class="headerlink" title="Multisource Holography"></a>Multisource Holography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10816">http://arxiv.org/abs/2309.10816</a></li>
<li>repo_url: None</li>
<li>paper_authors: Grace Kuo, Florian Schiffers, Douglas Lanman, Oliver Cossairt, Nathan Matsuda<br>for: Multisource holography is proposed as a novel architecture to suppress speckle in a single frame without sacrificing resolution.methods: The approach uses an array of sources, two spatial light modulators, and an algorithm to calculate multisource holograms.results: The proposed method can achieve up to a 10 dB increase in peak signal-to-noise ratio compared to an equivalent single source system, and is validated through a benchtop experimental prototype.<details>
<summary>Abstract</summary>
Holographic displays promise several benefits including high quality 3D imagery, accurate accommodation cues, and compact form-factors. However, holography relies on coherent illumination which can create undesirable speckle noise in the final image. Although smooth phase holograms can be speckle-free, their non-uniform eyebox makes them impractical, and speckle mitigation with partially coherent sources also reduces resolution. Averaging sequential frames for speckle reduction requires high speed modulators and consumes temporal bandwidth that may be needed elsewhere in the system.   In this work, we propose multisource holography, a novel architecture that uses an array of sources to suppress speckle in a single frame without sacrificing resolution. By using two spatial light modulators, arranged sequentially, each source in the array can be controlled almost independently to create a version of the target content with different speckle. Speckle is then suppressed when the contributions from the multiple sources are averaged at the image plane. We introduce an algorithm to calculate multisource holograms, analyze the design space, and demonstrate up to a 10 dB increase in peak signal-to-noise ratio compared to an equivalent single source system. Finally, we validate the concept with a benchtop experimental prototype by producing both 2D images and focal stacks with natural defocus cues.
</details>
<details>
<summary>摘要</summary>
激光显示技术承诺了许多优点，包括高质量3D图像、准确的视力缩放和具有减小的形态因素。然而，激光学依赖于 coherent 照明，可能会在最终图像中产生不жела的斑点噪声。尽管平滑相位激光可以无斑点，但它们的非均匀观看窗口使其实际无法应用，而使用半共振源也会降低分辨率。均值多帧图像以提高噪声抑制效果需要高速调制器，这会占用时间频谱资源，这些资源可能需要在系统中用于其他目的。在这项工作中，我们提出了多源激光学技术，一种新的架构，使用一个数组源来抑制斑点。通过使用两个空间光模ulator，其中每个源在数组中可以被控制为创建不同的斑点版本，并且在图像平面上均值多个源的贡献可以抑制斑点。我们提出了一种算法来计算多源激光图，分析设计空间，并证明在相同的单个源系统中，我们可以获得最高达10 dB的峰值信号响应比。最后，我们验证了这一概念，使用了一个桌面实验prototype，并生成了2D图像和自然减ocus图像。
</details></li>
</ul>
<hr>
<h2 id="InSPECtor-an-end-to-end-design-framework-for-compressive-pixelated-hyperspectral-instruments"><a href="#InSPECtor-an-end-to-end-design-framework-for-compressive-pixelated-hyperspectral-instruments" class="headerlink" title="InSPECtor: an end-to-end design framework for compressive pixelated hyperspectral instruments"></a>InSPECtor: an end-to-end design framework for compressive pixelated hyperspectral instruments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10833">http://arxiv.org/abs/2309.10833</a></li>
<li>repo_url: None</li>
<li>paper_authors: T. A. Stockmans, F. Snik, M. Esposito, C. van Dijk, C. U. Keller</li>
<li>for: 这个论文是为了设计一种高spectral仪器，它可以压缩数据，从而减少数据量和采集时间。</li>
<li>methods: 这个论文使用了TensorFlow算法，并利用自动微分来联合优化滤波器数组的布局和重建器。</li>
<li>results: 研究人员通过使用这种方法，可以减少数据量，采集时间和探测器空间，并且不会产生重要的信息损失。实际上，这种方法可以减少数据量的40倍，相比于传统的高spectral仪器。<details>
<summary>Abstract</summary>
Classic designs of hyperspectral instrumentation densely sample the spatial and spectral information of the scene of interest. Data may be compressed after the acquisition. In this paper we introduce a framework for the design of an optimized, micro-patterned snapshot hyperspectral imager that acquires an optimized subset of the spatial and spectral information in the scene. The data is thereby compressed already at the sensor level, but can be restored to the full hyperspectral data cube by the jointly optimized reconstructor. This framework is implemented with TensorFlow and makes use of its automatic differentiation for the joint optimization of the layout of the micro-patterned filter array as well as the reconstructor. We explore the achievable compression ratio for different numbers of filter passbands, number of scanning frames, and filter layouts using data collected by the Hyperscout instrument. We show resulting instrument designs that take snapshot measurements without losing significant information while reducing the data volume, acquisition time, or detector space by a factor of 40 as compared to classic, dense sampling. The joint optimization of a compressive hyperspectral imager design and the accompanying reconstructor provides an avenue to substantially reduce the data volume from hyperspectral imagers.
</details>
<details>
<summary>摘要</summary>
We evaluate the achievable compression ratio for various filter passband numbers, scanning frame numbers, and filter layouts using data from the Hyperscout instrument. Our results show that the proposed instrument designs can capture snapshot measurements without losing significant information, while reducing the data volume, acquisition time, and detector space by a factor of 40 compared to traditional, dense sampling. The joint optimization of the compressive hyperspectral imager design and the accompanying reconstructor provides a means to significantly reduce the data volume from hyperspectral imagers.
</details></li>
</ul>
<hr>
<h2 id="Minimum-length-chain-embedding-for-the-phase-unwrapping-problem-on-D-Wave’s-advantage-architecture"><a href="#Minimum-length-chain-embedding-for-the-phase-unwrapping-problem-on-D-Wave’s-advantage-architecture" class="headerlink" title="Minimum-length chain embedding for the phase unwrapping problem on D-Wave’s advantage architecture"></a>Minimum-length chain embedding for the phase unwrapping problem on D-Wave’s advantage architecture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10296">http://arxiv.org/abs/2309.10296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Kashfi Haghighi, Nikitas Dimopoulos</li>
<li>for: 解决 phase unwrapping 问题</li>
<li>methods: 使用 quantum annealing 和 Pegasus 图的嵌入</li>
<li>results: 提出一种新的嵌入算法，可以更好地解决 phase unwrapping 问题，并且可以应用于其他问题的嵌入<details>
<summary>Abstract</summary>
With the current progress of quantum computing, quantum annealing is being introduced as a powerful method to solve hard computational problems. In this paper, we study the potential capability of quantum annealing in solving the phase unwrapping problem, an instance of hard computational problems. To solve the phase unwrapping problem using quantum annealing, we deploy the D-Wave Advantage machine which is currently the largest available quantum annealer. The structure of this machine, however, is not compatible with our problem graph structure. Consequently, the problem graph needs to be mapped onto the target (Pegasus) graph, and this embedding significantly affects the quality of the results. Based on our experiment and also D-Wave's reports, the lower chain lengths can result in a better performance of quantum annealing. In this paper, we propose a new embedding algorithm that has the lowest possible chain length for embedding the graph of the phase unwrapping problem onto the Pegasus graph. The obtained results using this embedding strongly outperform the results of Auto-embedding provided by D-Wave. Besides the phase unwrapping problem, this embedding can be used to embed any subset of our problem graph to the Pegasus graph.
</details>
<details>
<summary>摘要</summary>
现在量子计算技术的进步，量子热处理已经被提出为解决复杂计算问题的强大方法。在这篇论文中，我们研究了量子热处理可以解决阶跃问题，这是复杂计算问题的一个实例。为解决阶跃问题使用量子热处理，我们使用D-Wave Advantage机器，该机器目前是最大的量子热处理器。然而，这台机器的结构与我们的问题图结构不兼容，因此需要将问题图映射到目标（ Pegasus）图上，这种映射会对结果产生深见影响。根据我们的实验和D-Wave的报告，较短的链长可以使量子热处理表现更好。在这篇论文中，我们提出了一种新的映射算法，该算法可以将问题图映射到 Pegasus 图上，并且 obtenains 最低的链长。使用这种映射，我们在实验中获得了较好的结果，比D-Wave 提供的 Auto-embedding 更好。此外，这种映射可以将任何我们问题图的子集映射到 Pegasus 图上。
</details></li>
</ul>
<hr>
<h2 id="Disentangled-Information-Bottleneck-guided-Privacy-Protective-JSCC-for-Image-Transmission"><a href="#Disentangled-Information-Bottleneck-guided-Privacy-Protective-JSCC-for-Image-Transmission" class="headerlink" title="Disentangled Information Bottleneck guided Privacy-Protective JSCC for Image Transmission"></a>Disentangled Information Bottleneck guided Privacy-Protective JSCC for Image Transmission</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10263">http://arxiv.org/abs/2309.10263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lunan Sun, Yang Yang, Mingzhe Chen, Caili Guo</li>
<li>For: 这个研究旨在保护私人资讯，同时确保传输过程中的通信效率。* Methods: 我们提出了一个混合资源和通道编码（JSCC）方法，并将其与私人资讯分离的方法（DIB）搭配，以实现高效且安全的传输。* Results: 我们的方法可以将私人资讯与公共资讯分离，并且可以实现高品质的传输。实验结果显示，我们的方法可以降低窃听者对私人资讯的准确率，并且可以降低传输时间。<details>
<summary>Abstract</summary>
Joint source and channel coding (JSCC) has attracted increasing attention due to its robustness and high efficiency. However, JSCC is vulnerable to privacy leakage due to the high relevance between the source image and channel input. In this paper, we propose a disentangled information bottleneck guided privacy-protective JSCC (DIB-PPJSCC) for image transmission, which aims at protecting private information as well as achieving superior communication performance at the legitimate receiver. In particular, we propose a DIB objective to disentangle private and public information. The goal is to compress the private information in the public subcodewords, preserve the private information in the private subcodewords and improve the reconstruction quality simultaneously. In order to optimize JSCC neural networks using the DIB objective, we derive a differentiable estimation of the DIB objective based on the variational approximation and the density-ratio trick. Additionally, we design a password-based privacy-protective (PP) algorithm which can be jointly optimized with JSCC neural networks to encrypt the private subcodewords. Specifically, we employ a private information encryptor to encrypt the private subcodewords before transmission, and a corresponding decryptor to recover the private information at the legitimate receiver. A loss function for jointly training the encryptor, decryptor and JSCC decoder is derived based on the maximum entropy principle, which aims at maximizing the eavesdropping uncertainty as well as improving the reconstruction quality. Experimental results show that DIB-PPJSCC can reduce the eavesdropping accuracy on private information up to $15\%$ and reduce $10\%$ inference time compared to existing privacy-protective JSCC and traditional separate methods.
</details>
<details>
<summary>摘要</summary>
joint source和通道编码（JSCC）已经吸引了越来越多的关注，因为它具有高效率和鲁棒性。然而，JSCC受到隐私泄露的威胁，因为源图像和通道输入之间存在高度相关性。在本文中，我们提出了一种基于信息瓶颈的隐私保护JSCC（DIB-PPJSCC），用于图像传输，以保护私人信息并实现合法接收器的超越性表现。具体来说，我们提出了一个DIB目标，用于分离私人信息和公共信息。我们的目标是压缩私人信息在公共子码字中，保持私人信息在私人子码字中，并同时提高重建质量。为了优化JSCC神经网络使用DIB目标，我们 deriv了一个可导的DIB目标基于变量 aproximation和density-ratio trick。此外，我们设计了一种基于密码的隐私保护算法（PP），可以与JSCC神经网络 jointly 优化，以加密私人子码字。具体来说，我们使用一个私人信息加密器加密私人子码字，并在合法接收器中使用相应的解密器恢复私人信息。我们 derive了基于最大 entropy 原理的损失函数，用于同时优化加密器、解密器和JSCC解码器的训练。实验结果表明，DIB-PPJSCC可以降低私人信息泄露率达15%，并提高重建质量10%，相比之下存在隐私保护JSCC和传统分离方法。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/19/eess.IV_2023_09_19/" data-id="closbroyf01750g88gwm0azy9" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_09_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/19/eess.SP_2023_09_19/" class="article-date">
  <time datetime="2023-09-19T08:00:00.000Z" itemprop="datePublished">2023-09-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/19/eess.SP_2023_09_19/">eess.SP - 2023-09-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Deep-Learning-based-Fast-and-Accurate-Beamforming-for-Millimeter-Wave-Systems"><a href="#Deep-Learning-based-Fast-and-Accurate-Beamforming-for-Millimeter-Wave-Systems" class="headerlink" title="Deep Learning based Fast and Accurate Beamforming for Millimeter-Wave Systems"></a>Deep Learning based Fast and Accurate Beamforming for Millimeter-Wave Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10904">http://arxiv.org/abs/2309.10904</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tarun S Cousik, Vijay K Shah, Jeffrey H. Reed Harry X Tran, Rittwik Jana</li>
<li>for: 这个论文是为了提高mmWave设备的表现，特别是对于增加信号力和&#x2F;或减少干扰水平。</li>
<li>methods: 这个论文使用了深度神经网络（DNN）框架，以实现快速和精准的照准方向。不同于传统的有限内存Look-Up表（LUT），BeamShaper使用训练好的NN模型来生成三角形矩阵的系数，并在实时运算中将其转换为任意方向的照准。</li>
<li>results:  simulations 显示，BeamShaper 比 contemporary LUT 基本的解决方案在cosine-similarity 和中央角度上表现更好，并且在几乎相同的时间尺度上表现更好。此外，我们还显示了我们的 DNN 基本方法具有更好的对抗量化噪声的性能，这是因为量化噪声对于数字相位调节器而言是一个重要的问题。<details>
<summary>Abstract</summary>
The widespread proliferation of mmW devices has led to a surge of interest in antenna arrays. This interest in arrays is due to their ability to steer beams in desired directions, for the purpose of increasing signal-power and/or decreasing interference levels. To enable beamforming, array coefficients are typically stored in look-up tables (LUTs) for subsequent referencing. While LUTs enable fast sweep times, their limited memory size restricts the number of beams the array can produce. Consequently, a receiver is likely to be offset from the main beam, thus decreasing received power, and resulting in sub-optimal performance. In this letter, we present BeamShaper, a deep neural network (DNN) framework, which enables fast and accurate beamsteering in any desirable 3-D direction. Unlike traditional finite-memory LUTs which support a fixed set of beams, BeamShaper utilizes a trained NN model to generate the array coefficients for arbitrary directions in \textit{real-time}. Our simulations show that BeamShaper outperforms contemporary LUT based solutions in terms of cosine-similarity and central angle in time scales that are slightly higher than LUT based solutions. Additionally, we show that our DNN based approach has the added advantage of being more resilient to the effects of quantization noise generated while using digital phase-shifters.
</details>
<details>
<summary>摘要</summary>
广泛的 millimeter 设备的普及，导致了天线数组的兴趣增加。这种兴趣是因为天线数组可以将指向所需的方向中的能量强化，以提高信号强度和/或降低干扰水平。为实现射频，通常需要存储在Look-Up表（LUT）中的数组系数。尽管LUT 允许快速滚动，但它们的内存大小有限制，因此数组只能生成一定数量的射频。这意味着接收器可能会偏离主束，从而降低接收到的功率，并导致不佳的性能。在这封信中，我们介绍了BeamShaper，一个深度神经网络（DNN）框架，它允许在任意三维方向上快速和准确地实现射频。与传统的有限存储LUT不同，BeamShaper 使用训练的神经网络模型来生成数组系数，而不是固定的数组。我们的 simulations 表明，BeamShaper 在cosine-similarity和中心角时间尺度上都高于当前LUT基本解决方案。此外，我们还发现了我们的神经网络基本方法在使用数字阶梯器时产生的量化噪声的影响更加抗性。
</details></li>
</ul>
<hr>
<h2 id="Non-Orthogonal-Time-Frequency-Space-Modulation"><a href="#Non-Orthogonal-Time-Frequency-Space-Modulation" class="headerlink" title="Non-Orthogonal Time-Frequency Space Modulation"></a>Non-Orthogonal Time-Frequency Space Modulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10889">http://arxiv.org/abs/2309.10889</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahdi Shamsi, Farokh Marvasti</li>
<li>for: 提出了一种时频空间变换（TFST）来 derivate 非正交基函数 для调制技术在延迟-多普勒平面上。</li>
<li>methods: 基于 TFST 的一家 Overloaded Delay-Doppler Modulation（ODDM）技术被提出，它提高了灵活性和效率，将调制信号表示为基函数信号的线性组合。</li>
<li>results: 对于提议的 ODDM 技术，一种非正交时frequency空间（NOTFS）数字调制被 derivation，并且在高负荷因子和白噪声频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率�<details>
<summary>Abstract</summary>
This paper proposes a Time-Frequency Space Transformation (TFST) to derive non-orthogonal bases for modulation techniques over the delay-doppler plane. A family of Overloaded Delay-Doppler Modulation (ODDM) techniques is proposed based on the TFST, which enhances flexibility and efficiency by expressing modulated signals as a linear combination of basis signals. A Non-Orthogonal Time-Frequency Space (NOTFS) digital modulation is derived for the proposed ODDM techniques, and simulations show that they offer high-mobility communication systems with improved spectral efficiency and low latency, particularly in challenging scenarios such as high overloading factors and Additive White Gaussian Noise (AWGN) channels. A modified sphere decoding algorithm is also presented to efficiently decode the received signal. The proposed modulation and decoding techniques contribute to the advancement of non-orthogonal approaches in the next-generation of mobile communication systems, delivering superior spectral efficiency and low latency, and offering a promising solution towards the development of efficient high-mobility communication systems.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:这篇论文提出了一种时Frequency空间转换（TFST），用于 derive 非对听基准 для模拟技术在延迟-Doppler平面上。基于 TFST，一家 Overloaded Delay-Doppler Modulation（ODDM）技术是提出的，这种技术可以提高 flexibility 和效率，将模拟信号表示为基准信号的线性组合。基于 NOTFS 数字模拟，一种非对听时Frequency空间（NOTFS）数字模拟是 derivation 的，并且在高负载因子和 Additive White Gaussian Noise（AWGN）频道下进行了丰富的 simulations，表明它们在高移动性系统中提供了更高的 spectral efficiency 和低延迟，特别是在高负载因子和 AWGN 频道下。此外，一种修改的 SPHERE 解码算法也是提出的，以高效地解码接收信号。提出的模拟和解码技术将在下一代移动通信系统中提供更高的 spectral efficiency 和低延迟，并且提供了高效的高移动性通信系统的开发方向。
</details></li>
</ul>
<hr>
<h2 id="RIS-Assisted-Over-the-Air-Adaptive-Federated-Learning-with-Noisy-Downlink"><a href="#RIS-Assisted-Over-the-Air-Adaptive-Federated-Learning-with-Noisy-Downlink" class="headerlink" title="RIS-Assisted Over-the-Air Adaptive Federated Learning with Noisy Downlink"></a>RIS-Assisted Over-the-Air Adaptive Federated Learning with Noisy Downlink</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10758">http://arxiv.org/abs/2309.10758</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayu Mao, Aylin Yener</li>
<li>for: This paper focuses on over-the-air federated learning (OTA-FL) in a heterogeneous edge-intelligent network with non-i.i.d. user dataset distributions and physical layer impairments.</li>
<li>methods: The proposed cross-layer algorithm jointly optimizes RIS configuration, communication, and computation resources to enhance learning performance, with dynamic local update steps, RIS phase shifts, and transmission power control.</li>
<li>results: The proposed algorithm outperforms the existing unified approach under heterogeneous systems and imperfect channel state information (CSI) in numerical results.<details>
<summary>Abstract</summary>
Over-the-air federated learning (OTA-FL) exploits the inherent superposition property of wireless channels to integrate the communication and model aggregation. Though a naturally promising framework for wireless federated learning, it requires care to mitigate physical layer impairments. In this work, we consider a heterogeneous edge-intelligent network with different edge device resources and non-i.i.d. user dataset distributions, under a general non-convex learning objective. We leverage the Reconfigurable Intelligent Surface (RIS) technology to augment OTA-FL system over simultaneous time varying uplink and downlink noisy communication channels under imperfect CSI scenario. We propose a cross-layer algorithm that jointly optimizes RIS configuration, communication and computation resources in this general realistic setting. Specifically, we design dynamic local update steps in conjunction with RIS phase shifts and transmission power to boost learning performance. We present a convergence analysis of the proposed algorithm, and show that it outperforms the existing unified approach under heterogeneous system and imperfect CSI in numerical results.
</details>
<details>
<summary>摘要</summary>
“阶层联盟学习（OTA-FL）利用无线通信频道的自然层次性来整合通信和模型集合。尽管是一个具有掌上优势的架构，但它需要注意对物理层问题的处理。在这个工作中，我们考虑了一个多样化的智能边缘网络，其中不同的边缘设备具有不同的资源，并且有不同的用户数据分布。我们运用了智能反射表面（RIS）技术来增强OTA-FL系统，并在同时进行同频道上的上传和下传噪音通信频道下进行无瑕的通信。我们提出了一个跨层数据算法，将RIS配置、通信和计算资源进行统一优化。具体来说，我们设计了动态本地更新步骤，与RIS相位调整和传输功率进行协同运作，以提高学习性能。我们提供了一个对照分析，说明了我们的方法在不同的多样化系统和实际问题下的性能优势。”
</details></li>
</ul>
<hr>
<h2 id="ResEMGNet-A-Lightweight-Residual-Deep-Learning-Architecture-for-Neuromuscular-Disorder-Detection-from-Raw-EMG-Signals"><a href="#ResEMGNet-A-Lightweight-Residual-Deep-Learning-Architecture-for-Neuromuscular-Disorder-Detection-from-Raw-EMG-Signals" class="headerlink" title="ResEMGNet: A Lightweight Residual Deep Learning Architecture for Neuromuscular Disorder Detection from Raw EMG Signals"></a>ResEMGNet: A Lightweight Residual Deep Learning Architecture for Neuromuscular Disorder Detection from Raw EMG Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10756">http://arxiv.org/abs/2309.10756</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minhajur Rahman, Md Toufiqur Rahman, Md Tanvir Raihan, Celia Shahnaz</li>
<li>for: 这项研究旨在使用深度学习技术对阿兰谱病和肌肉疾病进行检测。</li>
<li>methods: 该研究使用了卷积神经网络（CNNs）来直接从Raw EMG信号中检测阿兰谱病和肌肉疾病。不同于传统方法，ResEMGNet不需要手动提取特征，从而降低计算复杂性并提高实用性。</li>
<li>results: 该研究表明，ResEMGNet可以达到94.43%的全Subject-independent性能，在比较其他方法时表现出色。<details>
<summary>Abstract</summary>
Amyotrophic Lateral Sclerosis (ALS) and Myopathy are debilitating neuromuscular disorders that demand accurate and efficient diagnostic approaches. In this study, we harness the power of deep learning techniques to detect ALS and Myopathy. Convolutional Neural Networks (CNNs) have emerged as powerful tools in this context. We present ResEMGNet, designed to identify ALS and Myopathy directly from raw electromyography (EMG) signals. Unlike traditional methods that require intricate handcrafted feature extraction, ResEMGNet takes raw EMG data as input, reducing computational complexity and enhancing practicality. Our approach was rigorously evaluated using various metrics in comparison to existing methods. ResEMGNet exhibited exceptional subject-independent performance, achieving an impressive overall three-class accuracy of 94.43\%.
</details>
<details>
<summary>摘要</summary>
Amyotrophic Lateral Sclerosis (ALS) 和 Myopathy 是肌肉疾病，需要精准和高效的诊断方法。在这项研究中，我们利用深度学习技术来识别 ALS 和 Myopathy。卷积神经网络 (CNNs) 在这个上是非常有力的工具。我们介绍了 ResEMGNet，可以直接从 Raw 电romyography (EMG) 信号中识别 ALS 和 Myopathy。与传统方法不同，ResEMGNet 不需要手动提取特征，从而降低计算复杂性和提高实用性。我们的方法在不同的指标下进行了严格的评估，与现有方法进行了比较。ResEMGNet 在三类精度测试中取得了94.43%的总平均精度，表现出色。
</details></li>
</ul>
<hr>
<h2 id="BeamSec-A-Practical-mmWave-Physical-Layer-Security-Scheme-Against-Strong-Adversaries"><a href="#BeamSec-A-Practical-mmWave-Physical-Layer-Security-Scheme-Against-Strong-Adversaries" class="headerlink" title="BeamSec: A Practical mmWave Physical Layer Security Scheme Against Strong Adversaries"></a>BeamSec: A Practical mmWave Physical Layer Security Scheme Against Strong Adversaries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10632">http://arxiv.org/abs/2309.10632</a></li>
<li>repo_url: None</li>
<li>paper_authors: Afifa Ishtiaq, Arash Asadi, Ladan Khaloopour, Waqar Ahmed, Vahid Jamali, Matthias Hollick</li>
<li>for: 提高物理层安全性，防止窃听攻击</li>
<li>methods: 使用无知 adversary 位置&#x2F;通道的方法，Robust against 协作窃听攻击，兼容标准</li>
<li>results: 与基eline schemes 比较，BSec 可以提高机密率 by 79.8%，防止单个和协作窃听攻击<details>
<summary>Abstract</summary>
The high directionality of millimeter-wave (mmWave) communication systems has proven effective in reducing the attack surface against eavesdropping, thus improving the physical layer security. However, even with highly directional beams, the system is still exposed to eavesdropping against adversaries located within the main lobe. In this paper, we propose \acrshort{BSec}, a solution to protect the users even from adversaries located in the main lobe. The key feature of BeamSec are: (i) Operating without the knowledge of eavesdropper's location/channel; (ii) Robustness against colluding eavesdropping attack and (iii) Standard compatibility, which we prove using experiments via our IEEE 802.11ad/ay-compatible 60 GHz phased-array testbed. Methodologically, BeamSec first identifies uncorrelated and diverse beam-pairs between the transmitter and receiver by analyzing signal characteristics available through standard-compliant procedures. Next, it encodes the information jointly over all selected beam-pairs to minimize information leakage. We study two methods for allocating transmission time among different beams, namely uniform allocation (no knowledge of the wireless channel) and optimal allocation for maximization of the secrecy rate (with partial knowledge of the wireless channel). Our experiments show that \acrshort{BSec} outperforms the benchmark schemes against single and colluding eavesdroppers and enhances the secrecy rate by 79.8% over a random paths selection benchmark.
</details>
<details>
<summary>摘要</summary>
高度的毫米波（mmWave）通信系统的指向性有效地减少了听到攻击的面积，从而提高物理层安全性。然而，即使使用非常指向的扩散，系统仍然面临着在主脉搜索区域内的听到攻击。在这篇论文中，我们提议了一种解决方案，称为BeamSec，以保护用户，即使在主脉搜索区域内。BeamSec的关键特点包括：* 无需知道听到者的位置和通道情况进行操作* 对协同听到攻击进行鲁棒性* 兼容标准，我们通过使用IEEE 802.11ad/ay兼容60GHz分布式测试床进行实验证明。方法上，BeamSec先通过分析标准可用的信号特征来确定不相关和多样的扩散对between the transmitter and receiver。接着，它将信息共同编码在所选择的所有扩散上以最小化信息泄露。我们研究了两种分配传输时间的方法，一种是均匀分配（不知道无线通道），另一种是最大化机密率的分配（具有部分无线通道知识）。我们的实验显示，BeamSec在单个和协同听到者面前的性能都高于参考方案，并提高机密率 by 79.8% compared to a random paths selection benchmark。
</details></li>
</ul>
<hr>
<h2 id="A-Multi-Constrained-Transformer-BiLSTM-Guided-Network-for-Automated-Sleep-Stage-Classification-from-Single-Channel-EEG"><a href="#A-Multi-Constrained-Transformer-BiLSTM-Guided-Network-for-Automated-Sleep-Stage-Classification-from-Single-Channel-EEG" class="headerlink" title="A Multi Constrained Transformer-BiLSTM Guided Network for Automated Sleep Stage Classification from Single-Channel EEG"></a>A Multi Constrained Transformer-BiLSTM Guided Network for Automated Sleep Stage Classification from Single-Channel EEG</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10542">http://arxiv.org/abs/2309.10542</a></li>
<li>repo_url: None</li>
<li>paper_authors: Farhan Sadik, Md Tanvir Raihan, Rifat Bin Rashid, Minhjaur Rahman, Sabit Md Abdal, Shahed Ahmed, Talha Ibn Mahmud</li>
<li>for:  automatic sleep scoring from single-channel EEG signals</li>
<li>methods:  utilizes Convolutional Neural Network (CNN), transformer network, and Bidirectional Long Short Term Memory (BiLSTM)</li>
<li>results:  outperforms different state-of-the-art techniques by a large margin in terms of accuracy, precision, and F1-score.Here’s the format you requested:</li>
<li>for:  automatic sleep scoring from single-channel EEG signals</li>
<li>methods:  DenseRTSleep-II 使用 CNN、transformer 网络和 BiLSTM</li>
<li>results:  outperforms  différents state-of-the-art techniques by a large margin in terms of accuracy, precision, and F1-score.<details>
<summary>Abstract</summary>
Sleep stage classification from electroencephalogram (EEG) is significant for the rapid evaluation of sleeping patterns and quality. A novel deep learning architecture, ``DenseRTSleep-II'', is proposed for automatic sleep scoring from single-channel EEG signals. The architecture utilizes the advantages of Convolutional Neural Network (CNN), transformer network, and Bidirectional Long Short Term Memory (BiLSTM) for effective sleep scoring. Moreover, with the addition of a weighted multi-loss scheme, this model is trained more implicitly for vigorous decision-making tasks. Thus, the model generates the most efficient result in the SleepEDFx dataset and outperforms different state-of-the-art (IIT-Net, DeepSleepNet) techniques by a large margin in terms of accuracy, precision, and F1-score.
</details>
<details>
<summary>摘要</summary>
休眠阶段分类从电enzephalogram（EEG）是重要的，可以快速评估休眠模式和质量。一种新的深度学习架构“DenseRTSleep-II”被提议用于自动休眠分类从单通道EEG信号。该架构利用了Convolutional Neural Network（CNN）、transformer网络和Bidirectional Long Short Term Memory（BiLSTM）等优点，以便更有效地进行休眠分类。此外，通过加入权重多失函数学习策略，这个模型在强制决策任务中更加准确地进行分类。因此，该模型在SleepEDFx数据集中 генетиче最高效果，并在与不同的现有技术（IIT-Net、DeepSleepNet）的比较中，在精度、准确率和F1-score等方面表现出了明显的优势。
</details></li>
</ul>
<hr>
<h2 id="EMG-Signal-Classification-for-Neuromuscular-Disorders-with-Attention-Enhanced-CNN"><a href="#EMG-Signal-Classification-for-Neuromuscular-Disorders-with-Attention-Enhanced-CNN" class="headerlink" title="EMG Signal Classification for Neuromuscular Disorders with Attention-Enhanced CNN"></a>EMG Signal Classification for Neuromuscular Disorders with Attention-Enhanced CNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10483">http://arxiv.org/abs/2309.10483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md. Toufiqur Rahman, Minhajur Rahman, Celia Shahnaz</li>
<li>for: 这个研究旨在 Addressing the detection of Amyotrophic Lateral Sclerosis (ALS) and Myopathy, two debilitating neuromuscular disorders.</li>
<li>methods: 该方法开始于从 raw electromyography (EMG) 信号中提取有用的特征，利用 Log-spectrum 和 Delta Log spectrum，以捕捉信号的频谱特征和时间特征。然后，我们应用了深度学习模型 SpectroEMG-Net，结合 Convolutional Neural Networks (CNNs) 和 Attention，对三类进行分类。</li>
<li>results: 我们的方法在分类 Myopathy, Normal, 和 ALS 三类时表现出色，总准确率达到 92%。这个研究为 neuromuscular disorder 诊断带来了一个数据驱动、多类分类的方法，为早期检测提供了有价值的洞察。<details>
<summary>Abstract</summary>
Amyotrophic Lateral Sclerosis (ALS) and Myopathy present considerable challenges in the realm of neuromuscular disorder diagnostics. In this study, we employ advanced deep-learning techniques to address the detection of ALS and Myopathy, two debilitating conditions. Our methodology begins with the extraction of informative features from raw electromyography (EMG) signals, leveraging the Log-spectrum, and Delta Log spectrum, which capture the frequency contents, and spectral and temporal characteristics of the signals. Subsequently, we applied a deep-learning model, SpectroEMG-Net, combined with Convolutional Neural Networks (CNNs) and Attention for the classification of three classes. The robustness of our approach is rigorously evaluated, demonstrating its remarkable performance in distinguishing among the classes: Myopathy, Normal, and ALS, with an outstanding overall accuracy of 92\%. This study marks a contribution to addressing the diagnostic challenges posed by neuromuscular disorders through a data-driven, multi-class classification approach, providing valuable insights into the potential for early and accurate detection.
</details>
<details>
<summary>摘要</summary>
amyotrophic lateral sclerosis (ALS) 和 myopathy 在 neuromuscular disorder 诊断中存在很大的挑战。在这项研究中，我们使用高级深度学习技术来解决 ALS 和 myopathy 两种致命的疾病的检测。我们的方法开始于 raw electromyography (EMG) 信号中提取有用特征，利用 Log-spectrum 和 Delta Log spectrum，这两种特征捕捉信号的频谱特征和时间特征。然后，我们应用了深度学习模型 SpectroEMG-Net，结合 Convolutional Neural Networks (CNNs) 和 Attention，用于三类分类。我们的方法的稳定性得到了严格的评估，表明其在分类 Myopathy、Normal 和 ALS 中表现出色，总准确率达 92%。这项研究对 neuromuscular disorders 的诊断带来了一项数据驱动的多类分类方法，为早期检测提供了有价值的发现。
</details></li>
</ul>
<hr>
<h2 id="Coverage-Analysis-of-Dynamic-Coordinated-Beamforming-for-LEO-Satellite-Downlink-Networks"><a href="#Coverage-Analysis-of-Dynamic-Coordinated-Beamforming-for-LEO-Satellite-Downlink-Networks" class="headerlink" title="Coverage Analysis of Dynamic Coordinated Beamforming for LEO Satellite Downlink Networks"></a>Coverage Analysis of Dynamic Coordinated Beamforming for LEO Satellite Downlink Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10460">http://arxiv.org/abs/2309.10460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daeun Kim, Jeonghun Park, Namyoon Lee</li>
<li>for:  investigate the coverage performance of downlink satellite networks employing dynamic coordinated beamforming.</li>
<li>methods:  modeling the spatial arrangement of satellites and users using Poisson point processes situated on concentric spheres, deriving analytical expressions for the coverage probability, and developing an approximation for the coverage probability.</li>
<li>results:  dynamic coordinated beamforming significantly improves coverage compared to the absence of satellite coordination, and the optimal cluster size, which maximizes the ergodic spectral efficiency, increases with higher satellite density, provided that the number of antennas on the satellites is sufficiently large.<details>
<summary>Abstract</summary>
In this paper, we investigate the coverage performance of downlink satellite networks employing dynamic coordinated beamforming. Our approach involves modeling the spatial arrangement of satellites and users using Poisson point processes situated on concentric spheres. We derive analytical expressions for the coverage probability, which take into account the in-cluster geometry of the coordinated satellite set. These expressions are formulated in terms of various parameters, including the number of antennas per satellite, satellite density, fading characteristics, and path-loss exponent. To offer a more intuitive understanding, we also develop an approximation for the coverage probability. Furthermore, by considering the distribution of normalized distances, we derive the spatially averaged coverage probability, thereby validating the advantages of coordinated beamforming from a spatial average perspective. Our primary finding is that dynamic coordinated beamforming significantly improves coverage compared to the absence of satellite coordination, in direct proportion to the number of antennas on each satellite. Moreover, we observe that the optimal cluster size, which maximizes the ergodic spectral efficiency, increases with higher satellite density, provided that the number of antennas on the satellites is sufficiently large. Our findings are corroborated by simulation results, confirming the accuracy of the derived expressions.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了下降链接卫星网络的覆盖性能，使用动态协调扫描。我们的方法包括使用Poisson点过程模型卫星和用户的空间布局，并 derivate了覆盖概率的分析表达式。这些表达式考虑了协调卫星集的内部几何结构。为了更好地理解，我们还开发了覆盖概率的近似方法。此外，通过考虑 нормализа的距离分布，我们 derivate了平均覆盖概率，从而验证了协调扫描的优点。我们的主要发现是，动态协调扫描可以在不同卫星的覆盖性能方面提供显著改善，并且与卫星antenna的数量直接相关。此外，我们发现在卫星密度增加时，最佳团集大小，用于最大化随机 Spectral efficiency，随着卫星antenna的数量增加，而增加。我们的发现得到了仪表结果的验证，证明了我们 derive的表达式的准确性。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Congestion-Control-to-Improve-User-Experience-in-IoT-Using-LSTM-Network"><a href="#Enhancing-Congestion-Control-to-Improve-User-Experience-in-IoT-Using-LSTM-Network" class="headerlink" title="Enhancing Congestion Control to Improve User Experience in IoT Using LSTM Network"></a>Enhancing Congestion Control to Improve User Experience in IoT Using LSTM Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10347">http://arxiv.org/abs/2309.10347</a></li>
<li>repo_url: None</li>
<li>paper_authors: Atta Ur Rahman, Bibi Saqia, Wali Ullah Khan, Khaled Rabie, Mahmood Alam, Khairullah Khan</li>
<li>for: 本研究提出了一种基于长期快速响应Memory（LSTM）网络的新策略，用于改善压力控制。</li>
<li>methods: 本研究使用了LSTM网络，通过分析iot特有的网络流量模式、设备互动和压力发生情况，从iot环境中收集和训练LSTM网络架构。然后，使用LSTM模型预测技术来改善压力控制方法。</li>
<li>results: 本研究表明，通过使用LSTM网络预测技术，可以提高用户满意度和iot连接可靠性。通过测试和比较传统压力控制方法，评估了提议的策略的性能。<details>
<summary>Abstract</summary>
This study suggests a new strategy for improving congestion control by deploying Long Short-Term Memory (LSTM) networks. LSTMs are recurrent neural networks (RNN), that excel at capturing temporal relationships and patterns in data. IoT-specific data such as network traffic patterns, device interactions, and congestion occurrences are gathered and analyzed. The gathered data is used to create and train an LSTM network architecture specific to the IoT environment. Then, the LSTM model's predictive skills are incorporated into the congestion control methods. This work intends to optimize congestion management methods using LSTM networks, which results in increased user satisfaction and dependable IoT connectivity. Utilizing metrics like throughput, latency, packet loss, and user satisfaction, the success of the suggested strategy is evaluated. Evaluation of performance includes rigorous testing and comparison to conventional congestion control methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Time-Stretch-with-Continuous-Wave-Lasers-for-Practical-Fast-Realtime-Measurements"><a href="#Time-Stretch-with-Continuous-Wave-Lasers-for-Practical-Fast-Realtime-Measurements" class="headerlink" title="Time Stretch with Continuous-Wave Lasers for Practical Fast Realtime Measurements"></a>Time Stretch with Continuous-Wave Lasers for Practical Fast Realtime Measurements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10330">http://arxiv.org/abs/2309.10330</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tingyi Zhou, Yuta Goto, Takeshi Makino, Callen MacPhee, Yiming Zhou, Asad M. Madni, Hideaki Furukawa, Naoya Wada, Bahram Jalali</li>
<li>for: 描述了一种新的连续波（CW）实现光子时间压缩方法，以便替代昂贵的超集普模式频率隔离mode-locked Laser源。</li>
<li>methods: 使用了电子依optic（EO）modulation来脉冲WDM CW Laser源，以实现时间压缩。</li>
<li>results: 通过 simulations和实验验证了该新方法的可行性，并描述了两种应用场景。<details>
<summary>Abstract</summary>
Realtime high-throughput sensing and detection enables the capture of rare events within sub-picosecond time scale, which makes it possible for scientists to uncover the mystery of ultrafast physical processes. Photonic time stretch is one of the most successful approaches that utilize the ultra-wide bandwidth of mode-locked laser for detecting ultrafast signal. Though powerful, it relies on supercontinuum mode-locked laser source, which is expensive and difficult to integrate. This greatly limits the application of this technology. Here we propose a novel Continuous Wave (CW) implementation of the photonic time stretch. Instead of a supercontinuum mode-locked laser, a wavelength division multiplexed (WDM) CW laser, pulsed by electro-optic (EO) modulation, is adopted as the laser source. This opens up the possibility for low-cost integrated time stretch systems. This new approach is validated via both simulation and experiment. Two scenarios for potential application are also described.
</details>
<details>
<summary>摘要</summary>
实时高通过率探测和检测可以在sub-picosecond时间尺度内捕捉罕见事件，使科学家可以揭示ultrafast物理过程的谜。光子时间延迟是这种方法中最成功的一种，它利用mode-locked激光器的ultra-wide频率带宽，以检测ultrafast信号。虽然强大，但它依赖于supercontinuum mode-locked激光源，这是昂贵的和困难集成的。这限制了这种技术的应用。我们提议了一种新的Continuous Wave（CW）实现方式， substitute supercontinuum mode-locked激光源，使用wavelength division multiplexed（WDM）CW激光器，由电子-光学（EO）模ulation启动。这开 up了low-cost集成时间延迟系统的可能性。这新的方法通过实验和 simulate validate。两种应用场景也被描述。
</details></li>
</ul>
<hr>
<h2 id="Delay-sensitive-Task-Offloading-in-Vehicular-Fog-Computing-Assisted-Platoons"><a href="#Delay-sensitive-Task-Offloading-in-Vehicular-Fog-Computing-Assisted-Platoons" class="headerlink" title="Delay-sensitive Task Offloading in Vehicular Fog Computing-Assisted Platoons"></a>Delay-sensitive Task Offloading in Vehicular Fog Computing-Assisted Platoons</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10234">http://arxiv.org/abs/2309.10234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiong Wu, Siyuan Wang, Hongmei Ge, Pingyi Fan, Qiang Fan, Khaled B. Letaief</li>
<li>for: 这个研究的目的是为了提出一个基于SMDP的协调策略，以减少在VFC系统中的卸载延误。</li>
<li>methods: 本研究使用SMDP模型来描述VFC系统中的卸载问题，并提出一个基于最大长期收益函数的协调策略。</li>
<li>results: 研究结果显示，这个提出的协调策略可以实现VFC系统中的卸载延误最小化，并且与其他参考策略比较，有着更高的效率和可靠性。<details>
<summary>Abstract</summary>
Vehicles in platoons need to process many tasks to support various real-time vehicular applications. When a task arrives at a vehicle, the vehicle may not process the task due to its limited computation resource. In this case, it usually requests to offload the task to other vehicles in the platoon for processing. However, when the computation resources of all the vehicles in the platoon are insufficient, the task cannot be processed in time through offloading to the other vehicles in the platoon. Vehicular fog computing (VFC)-assisted platoon can solve this problem through offloading the task to the VFC which is formed by the vehicles driving near the platoon. Offloading delay is an important performance metric, which is impacted by both the offloading strategy for deciding where the task is offloaded and the number of the allocated vehicles in VFC to process the task. Thus, it is critical to propose an offloading strategy to minimize the offloading delay. In the VFC-assisted platoon system, vehicles usually adopt the IEEE 802.11p distributed coordination function (DCF) mechanism while having various computation resources. Moreover, when vehicles arrive and depart the VFC randomly, their tasks also arrive at and depart the system randomly. In this paper, we propose a semi-Markov decision process (SMDP) based offloading strategy while considering these factors to obtain the maximal long-term reward reflecting the offloading delay. Our research provides a robust strategy for task offloading in VFC systems, its effectiveness is demonstrated through simulation experiments and comparison with benchmark strategies.
</details>
<details>
<summary>摘要</summary>
车辆队伍中的车辆需要处理多个任务以支持不同的实时交通应用。当任务到达车辆时，车辆可能无法处理任务由于其有限的计算资源。在这种情况下，它通常会请求卸载任务到其他车辆队伍中的车辆进行处理。但当所有车辆队伍中的计算资源都不够时，任务无法在时间内进行处理 durch卸载到其他车辆队伍中的车辆。由于车辆队伍中的计算资源是有限的，因此需要提出一种卸载策略，以最小化卸载延迟。在VFC协助的车辆队伍系统中，车辆通常采用IEEE 802.11p分布协调功能（DCF）机制，而具有不同的计算资源。此外，当车辆随机到达和离开VFC时，其任务也随机到达和离开系统。在这篇论文中，我们提出了基于Markov决策过程（SMDP）的卸载策略，并考虑了这些因素，以获得最大化长期奖励，反映卸载延迟。我们的研究提供了VFC系统中任务卸载策略的稳定性和有效性，通过实验和比较基准策略进行证明。
</details></li>
</ul>
<hr>
<h2 id="A-Generalized-Approach-for-Recovering-Time-Encoded-Signals-with-Finite-Rate-of-Innovation"><a href="#A-Generalized-Approach-for-Recovering-Time-Encoded-Signals-with-Finite-Rate-of-Innovation" class="headerlink" title="A Generalized Approach for Recovering Time Encoded Signals with Finite Rate of Innovation"></a>A Generalized Approach for Recovering Time Encoded Signals with Finite Rate of Innovation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10223">http://arxiv.org/abs/2309.10223</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dorian Florescu</li>
<li>for: 本研究考虑了一种 recuperate 一个约束过滤后的 Dirac 函数的问题，用于表示输入有限Rate of Innovation（FRI）信号。</li>
<li>methods: 本文引入了一种新的通用方法，可以 garantía  Recovery  FRI 信号从时间编码机（TEM）输出中。在理论前方，我们 significantly 扩展了可以保证恢复的筛选器的类型，并提供了一个依赖于筛选器的第一两个本地导数的条件，以确保完美的输入恢复。在实践前方，如果筛选器的数学函数未知，我们的方法可以绕过筛选器模型阶段，减少恢复过程的复杂性。</li>
<li>results: 我们通过数学实验 validate 了我们的方法，使用过过去文献中使用过的筛选器，以及不兼容的筛选器。此外，我们还 validate 了结果通过实验设备。<details>
<summary>Abstract</summary>
In this paper, we consider the problem of recovering a sum of filtered Diracs, representing an input with finite rate of innovation (FRI), from its corresponding time encoding machine (TEM) measurements. So far, the recovery was guaranteed for cases where the filter is selected from a number of particular mathematical functions. Here, we introduce a new generalized method for recovering FRI signals from the TEM output. On the theoretical front, we significantly increase the class of filters for which reconstruction is guaranteed, and provide a condition for perfect input recovery depending on the first two local derivatives of the filter. We extend this result with reconstruction guarantees in the case of noise corrupted FRI signals. On the practical front, in cases where the filter has an unknown mathematical function, the proposed method streamlines the recovery process by bypassing the filter modelling stage. We validate the proposed method via numerical simulations with filters previously used in the literature, as well as filters that are not compatible with the existing results. Additionally, we validate the results using a TEM hardware implementation.
</details>
<details>
<summary>摘要</summary>
在本文中，我们考虑了一个滤波后的Dirac恒等值的恢复问题，该问题的输入是有限速度变化（FRI）的。以前，恢复是对特定数学函数选择的筛选器进行保证的。在这里，我们提出了一种新的通用方法，可以将FRI信号从TEM输出中恢复。从理论角度来看，我们在筛选器的选择范围中提高了恢复是 garantido的类型，并提供了一个基于筛选器的第一两个本地导数的条件，以确定完美的输入恢复。此外，我们在噪声损害FRI信号时也提供了恢复保证。从实践角度来看，如果筛选器的数学函数未知，我们的方法可以缩短恢复过程，直接跳过筛选器模型阶段。我们通过使用以前在文献中使用过的筛选器、不兼容现有结果的筛选器进行数值实验 validate our method。此外，我们还使用TEM硬件实现来验证我们的结果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/19/eess.SP_2023_09_19/" data-id="closbrozt01ao0g889xej2uqs" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_09_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/18/cs.SD_2023_09_18/" class="article-date">
  <time datetime="2023-09-18T15:00:00.000Z" itemprop="datePublished">2023-09-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/18/cs.SD_2023_09_18/">cs.SD - 2023-09-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Investigating-End-to-End-ASR-Architectures-for-Long-Form-Audio-Transcription"><a href="#Investigating-End-to-End-ASR-Architectures-for-Long-Form-Audio-Transcription" class="headerlink" title="Investigating End-to-End ASR Architectures for Long Form Audio Transcription"></a>Investigating End-to-End ASR Architectures for Long Form Audio Transcription</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09950">http://arxiv.org/abs/2309.09950</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nithin Rao Koluguri, Samuel Kriman, Georgy Zelenfroind, Somshubra Majumdar, Dima Rekesh, Vahid Noroozi, Jagadeesh Balam, Boris Ginsburg</li>
<li>for: This paper provides an overview and evaluation of end-to-end ASR models on long-form audios, with a focus on three categories of models based on their core architecture.</li>
<li>methods: The paper evaluates Word Error Rate, maximum audio length, and real-time factor for each model on several long audio benchmarks, including Earnings-21 and 22, CORAAL, and TED-LIUM3.</li>
<li>results: The model with self-attention and local attention has the best accuracy, and CTC-based models are more robust and efficient than RNNT on long-form audio.<details>
<summary>Abstract</summary>
This paper presents an overview and evaluation of some of the end-to-end ASR models on long-form audios. We study three categories of Automatic Speech Recognition(ASR) models based on their core architecture: (1) convolutional, (2) convolutional with squeeze-and-excitation and (3) convolutional models with attention. We selected one ASR model from each category and evaluated Word Error Rate, maximum audio length and real-time factor for each model on a variety of long audio benchmarks: Earnings-21 and 22, CORAAL, and TED-LIUM3. The model from the category of self-attention with local attention and global token has the best accuracy comparing to other architectures. We also compared models with CTC and RNNT decoders and showed that CTC-based models are more robust and efficient than RNNT on long form audio.
</details>
<details>
<summary>摘要</summary>
本文提供了一个简报和ASR模型的评估，涵盖了长形音频的批处理。我们研究了三类自动语音识别（ASR）模型的核心结构：（1）卷积，（2）卷积加上压缩和刺激，以及（3）卷积模型加注意。我们从每个类别中选择了一个ASR模型，并对它们在多种长音频标准评估 datasets（Earnings-21和22、CORAAL、TED-LIUM3）上进行评估 Word Error Rate、最大音频长度和实时因素。我们发现，具有本地注意力和全局标识的模型在准确性方面表现最佳。此外，我们还比较了基于CTC和RNNT解码器的模型，并发现CTC-based模型在长形音频上更加稳定和高效。
</details></li>
</ul>
<hr>
<h2 id="Harmony-and-Duality-An-introduction-to-Music-Theory"><a href="#Harmony-and-Duality-An-introduction-to-Music-Theory" class="headerlink" title="Harmony and Duality: An introduction to Music Theory"></a>Harmony and Duality: An introduction to Music Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10719">http://arxiv.org/abs/2309.10719</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maksim Lipyanskiy</li>
<li>for: 本研究旨在提供一种基于组合理论的音乐理论基础，包括和声、排序和即兴等方面。</li>
<li>methods: 本研究使用了限制可能的Scale的方法，例如约束两个voice不能只有一个半音之间的规定，以及三个voice不能同时出现的约束。然后研究不包含这些约束的Scale，并证明这些Scale是可以用于音乐作曲的最大集。</li>
<li>results: 研究发现，对于这些简单的两个&#x2F;三个voice约束，完整性是一种重要的特征，即Scale是最大的一个包含所有可能的音符的集。此外，对于这些约束，存在一种对应关系，可以将Scalesubject to two-voice constraint与Scalesubject to three-voice constraint进行对应。最后，通过组合这些约束，提供了一种分类推理的方法来分类和声。<details>
<summary>Abstract</summary>
We develop aspects of music theory related to harmony, such as scales, chord formation and improvisation from a combinatorial perspective. The goal is to provide a foundation for this subject by deriving the basic structure from a few assumptions, rather than writing down long lists of chords/scales to memorize without an underlying principle. Our approach involves introducing constraints that limit the possible scales we can consider. For example, we may impose the constraint that two voices cannot be only a semitone apart as this is too dissonant. We can then study scales that do not contain notes that are a semitone apart. A more refined constraint avoids three voices colliding by studying scales that do not have three notes separated only by semitones. Additionally, we require that our scales are complete, which roughly means that they are the maximal sets of tones that satisfy these constraints. As it turns out, completeness as applied to these simple two/three voice constraints characterizes the types of scales that are commonly used in music composition. Surprisingly, there is a correspondence between scales subject to the two-voice constraint and those subject to the three-voice constraint. We formulate this correspondence as a duality statement that provides a way to understand scales subject to one type of constraint in terms of scales subject to the other. Finally, we combine these constraint ideas to provide a classification of chords.
</details>
<details>
<summary>摘要</summary>
我们在音乐理论方面发展有关和声的方面，如约束、和声形成和演奏等。我们的目标是从一些假设出发，而不是直接记忆大量的和声和scale。我们的方法是引入约束，限制可考虑的约束。例如，我们可能假设两个声部不能夹紧只有一个半音之间，这太紧张。我们可以研究不含这种约束的约束。我们还需要保证我们的约束是完整的，这意味着它们是最大的满足这些约束的约束集。这些约束集是通常用于音乐创作中的约束。 surprisingly，存在一个对约束的对偶关系，它将约束一种类型的约束转换为另一种类型的约束。我们将这些约束融合，以提供和声的分类。
</details></li>
</ul>
<hr>
<h2 id="Frame-to-Utterance-Convergence-A-Spectra-Temporal-Approach-for-Unified-Spoofing-Detection"><a href="#Frame-to-Utterance-Convergence-A-Spectra-Temporal-Approach-for-Unified-Spoofing-Detection" class="headerlink" title="Frame-to-Utterance Convergence: A Spectra-Temporal Approach for Unified Spoofing Detection"></a>Frame-to-Utterance Convergence: A Spectra-Temporal Approach for Unified Spoofing Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09837">http://arxiv.org/abs/2309.09837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Awais Khan, Khalid Mahmood Malik, Shah Nawaz</li>
<li>for: 防止语音伪造攻击，提高自动人脸识别系统的安全性</li>
<li>methods: 使用spectra-temporal fusionStrategy，包括frame-level和utterance-level偏差 coefficient、bi-LSTM网络和 auto-encoder，捕捉各种伪造类型的异常信号</li>
<li>results: 在多个数据集上（ASVspoof2019、ASVspoof2021、VSDC、partial spoofs和免疫深度伪造）进行了广泛的评估，并达到了多种声音应用场景中的高度可靠性和抗伪造性<details>
<summary>Abstract</summary>
Voice spoofing attacks pose a significant threat to automated speaker verification systems. Existing anti-spoofing methods often simulate specific attack types, such as synthetic or replay attacks. However, in real-world scenarios, the countermeasures are unaware of the generation schema of the attack, necessitating a unified solution. Current unified solutions struggle to detect spoofing artifacts, especially with recent spoofing mechanisms. For instance, the spoofing algorithms inject spectral or temporal anomalies, which are challenging to identify. To this end, we present a spectra-temporal fusion leveraging frame-level and utterance-level coefficients. We introduce a novel local spectral deviation coefficient (SDC) for frame-level inconsistencies and employ a bi-LSTM-based network for sequential temporal coefficients (STC), which capture utterance-level artifacts. Our spectra-temporal fusion strategy combines these coefficients, and an auto-encoder generates spectra-temporal deviated coefficients (STDC) to enhance robustness. Our proposed approach addresses multiple spoofing categories, including synthetic, replay, and partial deepfake attacks. Extensive evaluation on diverse datasets (ASVspoof2019, ASVspoof2021, VSDC, partial spoofs, and in-the-wild deepfakes) demonstrated its robustness for a wide range of voice applications.
</details>
<details>
<summary>摘要</summary>
声音骗陷poses a significant threat to automatic speaker verification systems. Existing anti-骗陷 methods often simulate specific attack types, such as synthetic or replay attacks. However, in real-world scenarios, the countermeasures are unaware of the generation schema of the attack, necessitating a unified solution. Current unified solutions struggle to detect spoofing artifacts, especially with recent spoofing mechanisms. For instance, the spoofing algorithms inject spectral or temporal anomalies, which are challenging to identify. To this end, we present a spectra-temporal fusion leveraging frame-level and utterance-level coefficients. We introduce a novel local spectral deviation coefficient (SDC) for frame-level inconsistencies and employ a bi-LSTM-based network for sequential temporal coefficients (STC), which capture utterance-level artifacts. Our spectra-temporal fusion strategy combines these coefficients, and an auto-encoder generates spectra-temporal deviated coefficients (STDC) to enhance robustness. Our proposed approach addresses multiple spoofing categories, including synthetic, replay, and partial deepfake attacks. Extensive evaluation on diverse datasets (ASVspoof2019, ASVspoof2021, VSDC, partial spoofs, and in-the-wild deepfakes) demonstrated its robustness for a wide range of voice applications.
</details></li>
</ul>
<hr>
<h2 id="Synth-AC-Enhancing-Audio-Captioning-with-Synthetic-Supervision"><a href="#Synth-AC-Enhancing-Audio-Captioning-with-Synthetic-Supervision" class="headerlink" title="Synth-AC: Enhancing Audio Captioning with Synthetic Supervision"></a>Synth-AC: Enhancing Audio Captioning with Synthetic Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09705">http://arxiv.org/abs/2309.09705</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/littleflyingsheep/synthac">https://github.com/littleflyingsheep/synthac</a></li>
<li>paper_authors: Feiyang Xiao, Qiaoxi Zhu, Jian Guan, Xubo Liu, Haohe Liu, Kejia Zhang, Wenwu Wang</li>
<li>for: 提高音频描述的方法（audio captioning）的发展，因为现有的数据有限和质量不高，导致方法的开发受限。</li>
<li>methods: 提出了SynthAC框架，利用现有的音频生成模型和文本 corpus来创建Synthetic text-audio pairs，从而提高文本-音频表示。具体来说，使用文本-音频生成模型（AudioLDM）来生成Synthetic audio signals with captions from an image captioning dataset。</li>
<li>results: 实验表明，SynthAC框架可以增强音频描述方法，通过学习Synthetic text-audio pairs中的关系，提高文本-音频表示的质量。此外，SynthAC可以轻松地适应不同的现状前方法，导致表现提高。<details>
<summary>Abstract</summary>
Data-driven approaches hold promise for audio captioning. However, the development of audio captioning methods can be biased due to the limited availability and quality of text-audio data. This paper proposes a SynthAC framework, which leverages recent advances in audio generative models and commonly available text corpus to create synthetic text-audio pairs, thereby enhancing text-audio representation. Specifically, the text-to-audio generation model, i.e., AudioLDM, is used to generate synthetic audio signals with captions from an image captioning dataset. Our SynthAC expands the availability of well-annotated captions from the text-vision domain to audio captioning, thus enhancing text-audio representation by learning relations within synthetic text-audio pairs. Experiments demonstrate that our SynthAC framework can benefit audio captioning models by incorporating well-annotated text corpus from the text-vision domain, offering a promising solution to the challenge caused by data scarcity. Furthermore, SynthAC can be easily adapted to various state-of-the-art methods, leading to substantial performance improvements.
</details>
<details>
<summary>摘要</summary>
<<SYS>>对于音频描述来说，数据驱动的方法具有承诺。然而，音频描述方法的开发可能会受到文本-音频数据的有限性和质量的限制。这篇论文提议一个SynthAC框架，它利用最近的音频生成模型和常见的文本库来创建 sintetic文本-音频对，从而提高文本-音频表示。具体来说，文本-音频生成模型，即AudioLDM，用于生成 sintetic音频信号和caption从一个图像描述集。我们的SynthAC扩展了文本-视觉领域中已有较好的注释的caption到音频描述领域，从而提高文本-音频表示的学习关系。实验表明，我们的SynthAC框架可以通过将文本-视觉领域中已有较好的注释的caption引入到音频描述领域，提高音频描述模型的性能。此外，SynthAC可以轻松地适应不同的现状推荐方法，导致显著的性能提升。
</details></li>
</ul>
<hr>
<h2 id="Scaling-the-time-and-Fourier-domains-to-align-periodically-and-their-convolution"><a href="#Scaling-the-time-and-Fourier-domains-to-align-periodically-and-their-convolution" class="headerlink" title="Scaling the time and Fourier domains to align periodically and their convolution"></a>Scaling the time and Fourier domains to align periodically and their convolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09645">http://arxiv.org/abs/2309.09645</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/flatmax/fxt">https://github.com/flatmax/fxt</a></li>
<li>paper_authors: Matthew R. Flax, W. Harvey Holmes</li>
<li>for: 这篇论文是用于解释如何使用频率或时间扩展来对 periodic signal 进行快速匹配的。</li>
<li>methods: 该论文使用了频率或时间扩展的方法来对 periodic signal 进行快速匹配。</li>
<li>results: 该论文的结果表明，通过频率或时间扩展可以快速地对 periodic signal 进行匹配，并且可以用于开发新的算法，如抑音度估计算法。<details>
<summary>Abstract</summary>
This note shows how to align a periodic signal with its the Fourier transform by means of frequency or time scaling. This may be useful in developing new algorithms, e.g. for pitch estimation. This note also convolves the signals and the frequency time convolution is denoted fxt.
</details>
<details>
<summary>摘要</summary>
这份笔记介绍了如何将周期信号与其快 Fourier 变换进行对齐，通过频率或时间扩展。这可能有用于开发新的算法，例如抑音测量。此外，这份笔记还将信号和频率时间卷积，并将其称为fxt。
</details></li>
</ul>
<hr>
<h2 id="Refining-DNN-based-Mask-Estimation-using-CGMM-based-EM-Algorithm-for-Multi-channel-Noise-Reduction"><a href="#Refining-DNN-based-Mask-Estimation-using-CGMM-based-EM-Algorithm-for-Multi-channel-Noise-Reduction" class="headerlink" title="Refining DNN-based Mask Estimation using CGMM-based EM Algorithm for Multi-channel Noise Reduction"></a>Refining DNN-based Mask Estimation using CGMM-based EM Algorithm for Multi-channel Noise Reduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09630">http://arxiv.org/abs/2309.09630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julitta Bartolewska, Stanisław Kacprzak, Konrad Kowalczyk</li>
<li>for: 提高深度神经网络模型中的语音增强效果</li>
<li>methods: 提议一种多通道增强时间频谱屏障方法，包括迭代复杂� Gaussian Mixture Model（CGMM）基于算法，然后进行最佳空间滤波</li>
<li>results: 验证方法在三个最新的深度学习模型中，包括 DCUnet、DCCRN 和 FullSubNet，可以提高时间频谱屏障估计的准确性，并因此提高整体语音质量，测量方法为 PESQ 改进。改进是在所有三个 DNN 模型中具有一致性。<details>
<summary>Abstract</summary>
In this paper, we present a method that allows to further improve speech enhancement obtained with recently introduced Deep Neural Network (DNN) models. We propose a multi-channel refinement method of time-frequency masks obtained with single-channel DNNs, which consists of an iterative Complex Gaussian Mixture Model (CGMM) based algorithm, followed by optimum spatial filtration. We validate our approach on time-frequency masks estimated with three recent deep learning models, namely DCUnet, DCCRN, and FullSubNet. We show that our method with the proposed mask refinement procedure allows to improve the accuracy of estimated masks, in terms of the Area Under the ROC Curve (AUC) measure, and as a consequence the overall speech quality of the enhanced speech signal, as measured by PESQ improvement, and that the improvement is consistent across all three DNN models.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种方法，可以使用最近引入的深度神经网络（DNN）模型进一步提高抑制的 speech 质量。我们提议一种多通道刷新时域频谱屏障方法，包括一种迭代复杂 Gaussian Mixture Model（CGMM）基于算法，然后是最佳空间滤波。我们验证了我们的方法，使用三种最近的深度学习模型，即 DCUnet、DCCRN 和 FullSubNet。我们发现，我们的方法与提出的面积掩模预测方法可以提高抑制后 speech 质量的准确性， measured by AUC 度量，并且这种改进是所有三种 DNN 模型中的一致。
</details></li>
</ul>
<hr>
<h2 id="Electrolaryngeal-Speech-Intelligibility-Enhancement-Through-Robust-Linguistic-Encoders"><a href="#Electrolaryngeal-Speech-Intelligibility-Enhancement-Through-Robust-Linguistic-Encoders" class="headerlink" title="Electrolaryngeal Speech Intelligibility Enhancement Through Robust Linguistic Encoders"></a>Electrolaryngeal Speech Intelligibility Enhancement Through Robust Linguistic Encoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09627">http://arxiv.org/abs/2309.09627</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lester Phillip Violeta, Wen-Chin Huang, Ding Ma, Ryuichi Yamamoto, Kazuhiro Kobayashi, Tomoki Toda</li>
<li>For: 提高电子喉咙 speech 知识权威性（intelligibility）* Methods: 使用Robust linguistic encoders和HuBERT输出特征，解决类型匹配问题和说话者匹配问题* Results: 比 conventional framework 提高16%的字符错误率和0.83的自然度分数<details>
<summary>Abstract</summary>
We propose a novel framework for electrolaryngeal speech intelligibility enhancement through the use of robust linguistic encoders. Pretraining and fine-tuning approaches have proven to work well in this task, but in most cases, various mismatches, such as the speech type mismatch (electrolaryngeal vs. typical) or a speaker mismatch between the datasets used in each stage, can deteriorate the conversion performance of this framework. To resolve this issue, we propose a linguistic encoder robust enough to project both EL and typical speech in the same latent space, while still being able to extract accurate linguistic information, creating a unified representation to reduce the speech type mismatch. Furthermore, we introduce HuBERT output features to the proposed framework for reducing the speaker mismatch, making it possible to effectively use a large-scale parallel dataset during pretraining. We show that compared to the conventional framework using mel-spectrogram input and output features, using the proposed framework enables the model to synthesize more intelligible and naturally sounding speech, as shown by a significant 16% improvement in character error rate and 0.83 improvement in naturalness score.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的框架，用于提高电子声门朗读 intelligibility。我们使用了Robust语言编码器，并在预训练和精度调整阶段使用这些编码器。然而，在大多数情况下，这些编码器可能会受到类型匹配问题的影响，导致转换性能下降。为解决这个问题，我们提议一种可以将EL和典型语音 проек到同一个准确空间中的语言编码器，同时仍能够提取正确的语言信息。此外，我们还引入了HuBERT输出特征，以降低说话者匹配问题，使得可以效果地使用大规模并行数据集进行预训练。我们的实验结果表明，相比传统框架使用MELspectrogram输入和输出特征，使用我们的框架可以让模型生成更加智能和自然的声音，Character error rate下降16%，Naturalness score提高0.83。
</details></li>
</ul>
<hr>
<h2 id="HumTrans-A-Novel-Open-Source-Dataset-for-Humming-Melody-Transcription-and-Beyond"><a href="#HumTrans-A-Novel-Open-Source-Dataset-for-Humming-Melody-Transcription-and-Beyond" class="headerlink" title="HumTrans: A Novel Open-Source Dataset for Humming Melody Transcription and Beyond"></a>HumTrans: A Novel Open-Source Dataset for Humming Melody Transcription and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09623">http://arxiv.org/abs/2309.09623</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shansongliu/humtrans">https://github.com/shansongliu/humtrans</a></li>
<li>paper_authors: Shansong Liu, Xu Li, Dian Li, Ying Shan</li>
<li>for: 这篇论文主要是为了描述一个名为 HumTrans 的嗓吟报表Dataset，该Dataset 可以用于嗓吟报表转译、以及下游任务如基于嗓吟报表的音乐生成等。</li>
<li>methods: 论文使用了10名大学生，他们都是音乐专业或擅长至少一种乐器，通过作者提供的网站录音界面，对每个段落进行了两次嗓吟录音，采样频率为44,100 Hz。</li>
<li>results: 该 Dataset 包含约56.22小时的嗓吟录音，是目前已知最大的嗓吟Dataset。论文将在 Hugging Face 上发布，并提供 GitHub 仓库，包含基线结果和评价代码。<details>
<summary>Abstract</summary>
This paper introduces the HumTrans dataset, which is publicly available and primarily designed for humming melody transcription. The dataset can also serve as a foundation for downstream tasks such as humming melody based music generation. It consists of 500 musical compositions of different genres and languages, with each composition divided into multiple segments. In total, the dataset comprises 1000 music segments. To collect this humming dataset, we employed 10 college students, all of whom are either music majors or proficient in playing at least one musical instrument. Each of them hummed every segment twice using the web recording interface provided by our designed website. The humming recordings were sampled at a frequency of 44,100 Hz. During the humming session, the main interface provides a musical score for students to reference, with the melody audio playing simultaneously to aid in capturing both melody and rhythm. The dataset encompasses approximately 56.22 hours of audio, making it the largest known humming dataset to date. The dataset will be released on Hugging Face, and we will provide a GitHub repository containing baseline results and evaluation codes.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese translation:这篇论文介绍了一个名为HumTrans的 dataset，该 dataset 是公共可用的，主要用于唱响旋律识别。该 dataset 还可以用于下游任务，如基于唱响旋律的音乐生成。它包含了500首不同类型和语言的乐曲，每首乐曲被分成多个段落。总共，该 dataset 包含1000段乐曲。为了收集这个唱响 dataset，我们雇用了10名大学生，其中大多数是音乐专业或擅长至少一种乐器。每名学生在我们设计的网站上录制了每段乐曲两次。唱响录制的采样频率为44,100 Hz。在唱响会议中，主界面提供了一份乐谱，同时播放唱响音频，以帮助学生记录旋律和节奏。该 dataset 包含约56.22小时的音频，是目前已知最大的唱响 dataset。该 dataset 将在Hugging Face上发布，我们将在 GitHub 上提供基线结果和评估代码。
</details></li>
</ul>
<hr>
<h2 id="Spoofing-attack-augmentation-can-differently-trained-attack-models-improve-generalisation"><a href="#Spoofing-attack-augmentation-can-differently-trained-attack-models-improve-generalisation" class="headerlink" title="Spoofing attack augmentation: can differently-trained attack models improve generalisation?"></a>Spoofing attack augmentation: can differently-trained attack models improve generalisation?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09586">http://arxiv.org/abs/2309.09586</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wanying Ge, Xin Wang, Junichi Yamagishi, Massimiliano Todisco, Nicholas Evans</li>
<li>for: 本研究旨在探讨深度伪造探测器或伪讯干扰处理器（CM）在不可预测的伪讯攻击下的稳定性。</li>
<li>methods: 本研究使用了深度学习方法，并且采用了多种不同的攻击方法进行训练，以实现更通用的学习结果。</li>
<li>results: 本研究发现，使用不同的训练条件时，深度学习基本探测器的性能可能会有很大的差异。但是，使用图像注意力网络和自我超vised learning的CM模型则能够保持稳定性。此外，对于不同的攻击方法进行训练可能不够，还需要对于伪讯攻击进行增强。<details>
<summary>Abstract</summary>
A reliable deepfake detector or spoofing countermeasure (CM) should be robust in the face of unpredictable spoofing attacks. To encourage the learning of more generaliseable artefacts, rather than those specific only to known attacks, CMs are usually exposed to a broad variety of different attacks during training. Even so, the performance of deep-learning-based CM solutions are known to vary, sometimes substantially, when they are retrained with different initialisations, hyper-parameters or training data partitions. We show in this paper that the potency of spoofing attacks, also deep-learning-based, can similarly vary according to training conditions, sometimes resulting in substantial degradations to detection performance. Nevertheless, while a RawNet2 CM model is vulnerable when only modest adjustments are made to the attack algorithm, those based upon graph attention networks and self-supervised learning are reassuringly robust. The focus upon training data generated with different attack algorithms might not be sufficient on its own to ensure generaliability; some form of spoofing attack augmentation at the algorithm level can be complementary.
</details>
<details>
<summary>摘要</summary>
一个可靠的深刻模仿检测器或假造防范措施（CM）应该具备对不可预测的假造攻击的鲜活性。为了鼓励学习更通用的特征，CM通常在训练时被 expose 到多种不同的攻击。尽管如此，深度学习基本的CM解决方案的性能会异时变化，有时会导致重大的性能下降。我们在这篇论文中表明，假造攻击也可以因为训练条件而变化，导致检测性能下降。然而，基于 RawNet2 CM 模型的模型可以在只有轻微调整攻击算法时受到攻击。相比之下，基于图注意力网络和自然学习的模型具备了更高的鲜活性。训练数据生成的不同攻击算法可能不 enough  alone  ensure 通用性;在算法层次上进行假造攻击加强可以是补充。
</details></li>
</ul>
<hr>
<h2 id="Spiking-LEAF-A-Learnable-Auditory-front-end-for-Spiking-Neural-Networks"><a href="#Spiking-LEAF-A-Learnable-Auditory-front-end-for-Spiking-Neural-Networks" class="headerlink" title="Spiking-LEAF: A Learnable Auditory front-end for Spiking Neural Networks"></a>Spiking-LEAF: A Learnable Auditory front-end for Spiking Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09469">http://arxiv.org/abs/2309.09469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyang Song, Jibin Wu, Malu Zhang, Mike Zheng Shou, Haizhou Li</li>
<li>for: 这个研究是为了提高基于脳神经网络（SNN）的语音处理性能。</li>
<li>methods: 这篇论文使用了一个名为Spiking-LEAF的可学习听力前端，该前端结合了一个可学习的滤波器银行，并使用了一种名为IHC-LIF的内嵌丝网络模型，以更好地捕捉语音讯号的多尺度时间征性。</li>
<li>results: 在关键字搜寻和话者识别任务上，提案的Spiking-LEAF在类别精度、噪声耐性和编码效率方面都大于先前的SOTA脳神经网络听力前端和传统的实值静止特征。<details>
<summary>Abstract</summary>
Brain-inspired spiking neural networks (SNNs) have demonstrated great potential for temporal signal processing. However, their performance in speech processing remains limited due to the lack of an effective auditory front-end. To address this limitation, we introduce Spiking-LEAF, a learnable auditory front-end meticulously designed for SNN-based speech processing. Spiking-LEAF combines a learnable filter bank with a novel two-compartment spiking neuron model called IHC-LIF. The IHC-LIF neurons draw inspiration from the structure of inner hair cells (IHC) and they leverage segregated dendritic and somatic compartments to effectively capture multi-scale temporal dynamics of speech signals. Additionally, the IHC-LIF neurons incorporate the lateral feedback mechanism along with spike regularization loss to enhance spike encoding efficiency. On keyword spotting and speaker identification tasks, the proposed Spiking-LEAF outperforms both SOTA spiking auditory front-ends and conventional real-valued acoustic features in terms of classification accuracy, noise robustness, and encoding efficiency.
</details>
<details>
<summary>摘要</summary>
Brain-inspired spiking neural networks (SNNs) 处理时间信号的潜力很大，但是在声音处理方面表现有限因为缺乏有效的声音前端。为了解决这个限制，我们介绍Spiking-LEAF，一个learnable的声音前端，它精心设计用于基于SNN的声音处理。Spiking-LEAF结合了一个learnable滤波器和一个新的两个复体发射神经模型called IHC-LIF。IHC-LIF神经元受内毛槽细胞(IHC)的结构启发，并且利用分类的蕈葱和肉体部分来有效地捕捉声音信号的多尺度时间动态。此外，IHC-LIF神经元还包括 lateral feedback 机制和发射频率损失来增强发射码效率。在关键词搜寻和认知任务上，我们的提案的Spiking-LEAF比SOTA的脉搏式声音前端和传统的实值数字特征更高的分类精度、噪声Robustness和码编码效率。
</details></li>
</ul>
<hr>
<h2 id="Are-Soft-Prompts-Good-Zero-shot-Learners-for-Speech-Recognition"><a href="#Are-Soft-Prompts-Good-Zero-shot-Learners-for-Speech-Recognition" class="headerlink" title="Are Soft Prompts Good Zero-shot Learners for Speech Recognition?"></a>Are Soft Prompts Good Zero-shot Learners for Speech Recognition?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09413">http://arxiv.org/abs/2309.09413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dianwen Ng, Chong Zhang, Ruixi Zhang, Yukun Ma, Fabian Ritter-Gutierrez, Trung Hieu Nguyen, Chongjia Ni, Shengkui Zhao, Eng Siong Chng, Bin Ma</li>
<li>for: 这个论文的目的是解释软提示在自动语音识别（ASR） task 中的作用，以及如何使用软提示来提高 ASR 性能。</li>
<li>methods: 这篇论文使用了软提示来提高 ASR 性能，并通过分析软提示在不同的情况下的作用来深入理解软提示的作用。</li>
<li>results: 研究发现，软提示可以在不需要任何训练数据的情况下提高 ASR 性能，并且可以帮助模型更好地适应噪音环境。此外，研究还发现软提示可以分为两个角色：内容细化和噪音信息增强，这两个角色都有助于提高模型的稳定性和robustness。<details>
<summary>Abstract</summary>
Large self-supervised pre-trained speech models require computationally expensive fine-tuning for downstream tasks. Soft prompt tuning offers a simple parameter-efficient alternative by utilizing minimal soft prompt guidance, enhancing portability while also maintaining competitive performance. However, not many people understand how and why this is so. In this study, we aim to deepen our understanding of this emerging method by investigating the role of soft prompts in automatic speech recognition (ASR). Our findings highlight their role as zero-shot learners in improving ASR performance but also make them vulnerable to malicious modifications. Soft prompts aid generalization but are not obligatory for inference. We also identify two primary roles of soft prompts: content refinement and noise information enhancement, which enhances robustness against background noise. Additionally, we propose an effective modification on noise prompts to show that they are capable of zero-shot learning on adapting to out-of-distribution noise environments.
</details>
<details>
<summary>摘要</summary>
大型自我超级预训练语音模型需要计算昂贵的精细调整，以便在下游任务中提高性能。软提示调整提供了一种简单 Parameters efficient的替代方案，可以提高可移植性，同时保持竞争性。然而，不多少人理解这种emerging方法的工作原理。在这项研究中，我们想要深入了解这种方法，通过调查软提示在自动语音识别（ASR）中的角色。我们发现软提示能够作为零例学习者，提高ASR性能，但也使其易受到黑客修改的威胁。软提示能够帮助泛化，但并不是推理的必需。我们还发现软提示有两个主要角色：内容细化和噪声信息增强，这有助于对噪声背景进行鲁棒化。此外，我们还提出了一种有效的噪声提示修改方法，以示证明它们可以适应外部噪声环境进行零例学习。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/18/cs.SD_2023_09_18/" data-id="closbrouf00wx0g88hdcvfap9" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_09_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/18/cs.CV_2023_09_18/" class="article-date">
  <time datetime="2023-09-18T13:00:00.000Z" itemprop="datePublished">2023-09-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/18/cs.CV_2023_09_18/">cs.CV - 2023-09-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ProtoKD-Learning-from-Extremely-Scarce-Data-for-Parasite-Ova-Recognition"><a href="#ProtoKD-Learning-from-Extremely-Scarce-Data-for-Parasite-Ova-Recognition" class="headerlink" title="ProtoKD: Learning from Extremely Scarce Data for Parasite Ova Recognition"></a>ProtoKD: Learning from Extremely Scarce Data for Parasite Ova Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10210">http://arxiv.org/abs/2309.10210</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shubham Trehan, Udhav Ramachandran, Ruth Scimeca, Sathyanarayanan N. Aakur</li>
<li>for: 这个研究旨在发展可靠的计算框架，以帮助早期螺旋感染的检测，特别是在螺旋卵阶段。</li>
<li>methods: 这个研究使用了prototype网络和自我激发的方法，从仅一个示例每个类别中学习出了坚固的表现。</li>
<li>results: 研究发现，这个ProtoKD框架可以从仅一个示例每个类别中学习出高度可靠的表现，并且在大规模的数据中进行验证。<details>
<summary>Abstract</summary>
Developing reliable computational frameworks for early parasite detection, particularly at the ova (or egg) stage is crucial for advancing healthcare and effectively managing potential public health crises. While deep learning has significantly assisted human workers in various tasks, its application and diagnostics has been constrained by the need for extensive datasets. The ability to learn from an extremely scarce training dataset, i.e., when fewer than 5 examples per class are present, is essential for scaling deep learning models in biomedical applications where large-scale data collection and annotation can be expensive or not possible (in case of novel or unknown infectious agents). In this study, we introduce ProtoKD, one of the first approaches to tackle the problem of multi-class parasitic ova recognition using extremely scarce data. Combining the principles of prototypical networks and self-distillation, we can learn robust representations from only one sample per class. Furthermore, we establish a new benchmark to drive research in this critical direction and validate that the proposed ProtoKD framework achieves state-of-the-art performance. Additionally, we evaluate the framework's generalizability to other downstream tasks by assessing its performance on a large-scale taxonomic profiling task based on metagenomes sequenced from real-world clinical data.
</details>
<details>
<summary>摘要</summary>
发展可靠的计算框架，特别是在菌 ovum (或蛋) 阶段，对于提高医疗和有效管理 potential public health crisis 是非常重要的。深度学习在各种任务中已经提供了很多帮助，但是它在诊断方面受到了有限的数据集的限制。在生物医学应用中，可以使用少量数据来训练深度学习模型，这是因为收集和标注大量数据可能是昂贵的或者不可能的（在新型或未知感染者情况下）。在这项研究中，我们介绍了一种名为 ProtoKD 的新方法，可以在只有一个样本每个类时进行多类菌 ovum 识别。我们结合了原型网络和自我滥化的原则，可以从单个样本中学习Robust的表示。此外，我们还提出了一个新的标准套件，用于驱动这一重要的研究方向，并证明了我们提出的 ProtoKD 框架实现了状态机器的表现。此外，我们还评估了该框架在其他下游任务中的一致性，通过对基于实际临床数据的大规模分类任务进行评估。
</details></li>
</ul>
<hr>
<h2 id="Image-Text-Pre-Training-for-Logo-Recognition"><a href="#Image-Text-Pre-Training-for-Logo-Recognition" class="headerlink" title="Image-Text Pre-Training for Logo Recognition"></a>Image-Text Pre-Training for Logo Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10206">http://arxiv.org/abs/2309.10206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mark Hubenthal, Suren Kumar<br>for: 这个论文是为了提高开放集logo认识技术而写的。methods: 该论文提出了两个新贡献来提高匹配模型的性能：首先，使用图像和文本对应样本进行预训练；其次，使用改进的度量学习损失函数。results: 该论文的实验结果显示，使用图像和文本对应样本进行预训练可以提高视觉嵌入器在logo检索任务中的性能，特别是对于文本占主导的类型。此外，提出的ProxyNCAHN++损失函数可以更好地减少硬negative图像，从而提高logo检索任务的性能。在五个公共logo数据集上，该方法达到了新的国际最佳性能。<details>
<summary>Abstract</summary>
Open-set logo recognition is commonly solved by first detecting possible logo regions and then matching the detected parts against an ever-evolving dataset of cropped logo images. The matching model, a metric learning problem, is especially challenging for logo recognition due to the mixture of text and symbols in logos. We propose two novel contributions to improve the matching model's performance: (a) using image-text paired samples for pre-training, and (b) an improved metric learning loss function. A standard paradigm of fine-tuning ImageNet pre-trained models fails to discover the text sensitivity necessary to solve the matching problem effectively. This work demonstrates the importance of pre-training on image-text pairs, which significantly improves the performance of a visual embedder trained for the logo retrieval task, especially for more text-dominant classes. We construct a composite public logo dataset combining LogoDet3K, OpenLogo, and FlickrLogos-47 deemed OpenLogoDet3K47. We show that the same vision backbone pre-trained on image-text data, when fine-tuned on OpenLogoDet3K47, achieves $98.6\%$ recall@1, significantly improving performance over pre-training on Imagenet1K ($97.6\%$). We generalize the ProxyNCA++ loss function to propose ProxyNCAHN++ which incorporates class-specific hard negative images. The proposed method sets new state-of-the-art on five public logo datasets considered, with a $3.5\%$ zero-shot recall@1 improvement on LogoDet3K test, $4\%$ on OpenLogo, $6.5\%$ on FlickrLogos-47, $6.2\%$ on Logos In The Wild, and $0.6\%$ on BelgaLogo.
</details>
<details>
<summary>摘要</summary>
开放集logo认识通常通过先检测可能的标识区域，然后将检测到的部分与一个不断演化的logo图像集进行匹配来解决。匹配模型是一个度量学习问题，因为logo中的文字和符号混合而特别具有挑战性。我们提出了两个新的贡献来提高匹配模型的性能：（a）使用图像文本对应样本进行预训练，以及（b）改进度量学习损失函数。现有的标准模式是使用ImageNet预训练模型进行精度训练，但这并不能有效地解决匹配问题。这种工作表明了预训练图像文本对应样本的重要性，可以大幅提高一个视觉嵌入器在logo检索任务中的性能，特别是对于文本更加占据的类型。我们组建了OpenLogoDet3K47 compositive公共logo数据集，并证明了使用同一个视觉背景模型，经预训练于图像文本对应样本，在OpenLogoDet3K47上进行精度训练，可以达到98.6%的Recall@1，显著超越ImageNet1K($97.6\%$)。我们将ProxyNCA++损失函数推广到ProxyNCAHN++，其包含类型特定的硬负样本。提出的方法在五个公共logo数据集上达到了新的状态泰施，与LogoDet3K测试集的零MQA@1相比，提高了3.5%，与OpenLogo相比提高了4%，与FlickrLogos-47相比提高了6.5%，与Logos In The Wild相比提高了6.2%，与BelgaLogo相比提高了0.6%。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-for-enhancing-Wind-Field-Resolution-in-Complex-Terrain"><a href="#Machine-Learning-for-enhancing-Wind-Field-Resolution-in-Complex-Terrain" class="headerlink" title="Machine Learning for enhancing Wind Field Resolution in Complex Terrain"></a>Machine Learning for enhancing Wind Field Resolution in Complex Terrain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10172">http://arxiv.org/abs/2309.10172</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jacobwulffwold/gan_sr_wind_field">https://github.com/jacobwulffwold/gan_sr_wind_field</a></li>
<li>paper_authors: Jacob Wulff Wold, Florian Stadtmann, Adil Rasheed, Mandar Tabib, Omer San, Jan-Tore Horn</li>
<li>for: 这个研究旨在开发一种基于神经网络的方法，用于在高分辨率下数值模拟大气流体动态。</li>
<li>methods: 这个方法基于增强型超解析生成 adversarial neural network，可以从低分辨率的风场数据中生成高分辨率的风场数据，并且能够尊重地形特性。</li>
<li>results: 研究表明，这个方法可以成功重建完全解析3D速度场，并且超过了传递 interpolate 方法。此外，通过采用适当的成本函数，可以减少对对抗训练的需求。<details>
<summary>Abstract</summary>
Atmospheric flows are governed by a broad variety of spatio-temporal scales, thus making real-time numerical modeling of such turbulent flows in complex terrain at high resolution computationally intractable. In this study, we demonstrate a neural network approach motivated by Enhanced Super-Resolution Generative Adversarial Networks to upscale low-resolution wind fields to generate high-resolution wind fields in an actual wind farm in Bessaker, Norway. The neural network-based model is shown to successfully reconstruct fully resolved 3D velocity fields from a coarser scale while respecting the local terrain and that it easily outperforms trilinear interpolation. We also demonstrate that by using appropriate cost function based on domain knowledge, we can alleviate the use of adversarial training.
</details>
<details>
<summary>摘要</summary>
大气流动受到各种空间时间尺度的控制，因此实时数值模拟这种湍流在复杂地形上的计算极其困难。在本研究中，我们提出一种基于神经网络的方法，使用增强型超分辨率生成 adversarial neural network 来提高低分辨率风场的解析。我们的神经网络模型能够成功重建高分辨率三维速度场，同时尊重地形特性，并轻松超越三线 interpolate 方法。此外，我们还示出了通过采用适当的成本函数，可以减少对对抗训练的使用。
</details></li>
</ul>
<hr>
<h2 id="Specification-Driven-Video-Search-via-Foundation-Models-and-Formal-Verification"><a href="#Specification-Driven-Video-Search-via-Foundation-Models-and-Formal-Verification" class="headerlink" title="Specification-Driven Video Search via Foundation Models and Formal Verification"></a>Specification-Driven Video Search via Foundation Models and Formal Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10171">http://arxiv.org/abs/2309.10171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunhao Yang, Jean-Raphaël Gaglione, Sandeep Chinchali, Ufuk Topcu</li>
<li>for: 该文章是为了寻找视频中的事件兴趣点而写的。</li>
<li>methods: 该方法使用了计算机视觉和自然语言处理的最新进展，以及正式方法，自动地搜索视频中的事件兴趣点。</li>
<li>results: 该方法可以高效地搜索视频中的事件兴趣点，并且可以保持隐私。在一个隐私敏感的视频和一个自动驾驶数据集上，该方法可以达到90%的精度。<details>
<summary>Abstract</summary>
The increasing abundance of video data enables users to search for events of interest, e.g., emergency incidents. Meanwhile, it raises new concerns, such as the need for preserving privacy. Existing approaches to video search require either manual inspection or a deep learning model with massive training. We develop a method that uses recent advances in vision and language models, as well as formal methods, to search for events of interest in video clips automatically and efficiently. The method consists of an algorithm to map text-based event descriptions into linear temporal logic over finite traces (LTL$_f$) and an algorithm to construct an automaton encoding the video information. Then, the method formally verifies the automaton representing the video against the LTL$_f$ specifications and adds the pertinent video clips to the search result if the automaton satisfies the specifications. We provide qualitative and quantitative analysis to demonstrate the video-searching capability of the proposed method. It achieves over 90 percent precision in searching over privacy-sensitive videos and a state-of-the-art autonomous driving dataset.
</details>
<details>
<summary>摘要</summary>
随着视频数据的增加，用户能够查找 interess 事件，例如紧急事件。然而，这也提出了新的问题，例如保持隐私。现有的视频搜索方法需要 either manual inspection 或 deep learning model with massive training。我们开发了一种使用 recent advances in vision and language models，以及 formal methods，自动地搜索视频中的事件 интересы。该方法包括一种将文本基于事件描述映射到线性时间逻辑 sobre finite traces (LTL$_f$) 的算法，以及一种构建视频信息的 automaton。然后，方法正式验证视频中的 automaton 与 LTL$_f$ 规范之间的匹配，并将符合规范的视频片段添加到搜索结果中。我们提供了质量和量化分析，以证明我们提议的方法的视频搜索能力。它在 privacy-sensitive 视频和 state-of-the-art autonomous driving 数据集上达到了高于 90% 的精度。
</details></li>
</ul>
<hr>
<h2 id="Offline-Detection-of-Misspelled-Handwritten-Words-by-Convolving-Recognition-Model-Features-with-Text-Labels"><a href="#Offline-Detection-of-Misspelled-Handwritten-Words-by-Convolving-Recognition-Model-Features-with-Text-Labels" class="headerlink" title="Offline Detection of Misspelled Handwritten Words by Convolving Recognition Model Features with Text Labels"></a>Offline Detection of Misspelled Handwritten Words by Convolving Recognition Model Features with Text Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10158">http://arxiv.org/abs/2309.10158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrey Totev, Tomas Ward</li>
<li>for: 本研究旨在提高手写识别率，特别是在识别预期出现在语言模型中的缺失词汇时。</li>
<li>methods: 我们提出了一种无约束的二元分类器，包括一个手写识别特征提取器和一个多Modal分类头，该头将特征提取器输出与输入文本的向量表示进行卷积。我们的分类头通过使用一个国际前沿的生成对抗网络创造的Synthetic数据进行训练。</li>
<li>results: 我们的模型可以维持高准确率，同时可以通过调整精度来提高平均准确率19.5%，相比直接使用现有的手写识别模型进行 Addressing 任务。这些性能提升可能会导致人工循环自动化应用程序的产出提高。<details>
<summary>Abstract</summary>
Offline handwriting recognition (HWR) has improved significantly with the advent of deep learning architectures in recent years. Nevertheless, it remains a challenging problem and practical applications often rely on post-processing techniques for restricting the predicted words via lexicons or language models. Despite their enhanced performance, such systems are less usable in contexts where out-of-vocabulary words are anticipated, e.g. for detecting misspelled words in school assessments. To that end, we introduce the task of comparing a handwriting image to text. To solve the problem, we propose an unrestricted binary classifier, consisting of a HWR feature extractor and a multimodal classification head which convolves the feature extractor output with the vector representation of the input text. Our model's classification head is trained entirely on synthetic data created using a state-of-the-art generative adversarial network. We demonstrate that, while maintaining high recall, the classifier can be calibrated to achieve an average precision increase of 19.5% compared to addressing the task by directly using state-of-the-art HWR models. Such massive performance gains can lead to significant productivity increases in applications utilizing human-in-the-loop automation.
</details>
<details>
<summary>摘要</summary>
停机写入Recognition (HWR) 在深度学习架构的推出后有了显著改进，但是仍然是一个具有挑战性的问题，实际应用中通常通过lexicon或语言模型来限制预测的词汇。尽管其表现得到了提高，但这些系统在预测非词汇词语时表现不佳，例如在学校评估中检测拼写错误。为解决这个问题，我们引入对手写图像与文本进行比较的任务。为解决这个任务，我们提议一种无限制binary分类器，其包括一个HWR特征提取器和一个多Modal分类头。这个分类头使用一个state-of-the-art生成对抗网络生成的 sintetic数据进行训练。我们示示，虽然保持高准确率，我们的分类器可以实现平均提高19.5%的精度，相比直接使用现状最佳HWR模型进行处理。这样的巨大性能提升可能会导致人工循环自动化应用中的产出提高。
</details></li>
</ul>
<hr>
<h2 id="Preserving-Tumor-Volumes-for-Unsupervised-Medical-Image-Registration"><a href="#Preserving-Tumor-Volumes-for-Unsupervised-Medical-Image-Registration" class="headerlink" title="Preserving Tumor Volumes for Unsupervised Medical Image Registration"></a>Preserving Tumor Volumes for Unsupervised Medical Image Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10153">http://arxiv.org/abs/2309.10153</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qihua Dong, Hao Du, Ying Song, Yan Xu, Jing Liao</li>
<li>for: 这篇论文旨在解决医学影像调整中的体积变化问题，以确保影像调整中的肿瘤体积不变化。</li>
<li>methods: 本文提出了一个两阶段的方法，首先使用相似性基于的调整来找到潜在的肿瘤区域，然后使用一个新的自适应体积保持损失函数来确保肿瘤体积不变化。</li>
<li>results: 本文的方法能够成功保持肿瘤体积，并与现有的方法相比，实现了相似的调整结果。<details>
<summary>Abstract</summary>
Medical image registration is a critical task that estimates the spatial correspondence between pairs of images. However, current traditional and deep-learning-based methods rely on similarity measures to generate a deforming field, which often results in disproportionate volume changes in dissimilar regions, especially in tumor regions. These changes can significantly alter the tumor size and underlying anatomy, which limits the practical use of image registration in clinical diagnosis. To address this issue, we have formulated image registration with tumors as a constraint problem that preserves tumor volumes while maximizing image similarity in other normal regions. Our proposed strategy involves a two-stage process. In the first stage, we use similarity-based registration to identify potential tumor regions by their volume change, generating a soft tumor mask accordingly. In the second stage, we propose a volume-preserving registration with a novel adaptive volume-preserving loss that penalizes the change in size adaptively based on the masks calculated from the previous stage. Our approach balances image similarity and volume preservation in different regions, i.e., normal and tumor regions, by using soft tumor masks to adjust the imposition of volume-preserving loss on each one. This ensures that the tumor volume is preserved during the registration process. We have evaluated our strategy on various datasets and network architectures, demonstrating that our method successfully preserves the tumor volume while achieving comparable registration results with state-of-the-art methods. Our codes is available at: \url{https://dddraxxx.github.io/Volume-Preserving-Registration/}.
</details>
<details>
<summary>摘要</summary>
医学图像对接是一项重要任务，用于估计图像对的空间匹配。然而，现有的传统方法和深度学习方法通常基于相似度测量来生成扭曲场，这经常导致图像区域中的体积变化不匹配，特别是在肿瘤区域。这些变化可能导致肿瘤大小和下面的解剖结构发生重大变化，从而限制图像对接的实际应用。为解决这个问题，我们提出了一种图像对接约束问题，以保持肿瘤体积而最大化图像相似性。我们的提议包括两个阶段。在第一阶段，我们使用相似度基于的图像对接来确定潜在的肿瘤区域，并生成相应的软肿瘤面罩。在第二阶段，我们提出了一种保持体积的图像对接，使用一种新的自适应体积保持损失函数，该函数根据上一阶段生成的面罩对每个区域进行不同的权重调整。这种方法可以在不同区域（即正常区域和肿瘤区域）平衡图像相似性和体积保持。因此，我们的方法可以在图像对接过程中保持肿瘤体积。我们在多个数据集和网络架构上评估了我们的策略，并证明了我们的方法可以成功保持肿瘤体积，同时与现状的方法相比具有相似的对接结果。我们的代码可以在以下链接中找到：\url{https://dddraxxx.github.io/Volume-Preserving-Registration/}.
</details></li>
</ul>
<hr>
<h2 id="Deep-Prompt-Tuning-for-Graph-Transformers"><a href="#Deep-Prompt-Tuning-for-Graph-Transformers" class="headerlink" title="Deep Prompt Tuning for Graph Transformers"></a>Deep Prompt Tuning for Graph Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10131">http://arxiv.org/abs/2309.10131</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hieu9955/ggggg">https://github.com/hieu9955/ggggg</a></li>
<li>paper_authors: Reza Shirkavand, Heng Huang</li>
<li>for: 这篇 paper 的目的是提出一种新的方法来使用图形变数对应 tasks，以替代 fine-tuning 方法。</li>
<li>methods: 这篇 paper 使用了一种名为 “deep graph prompt tuning” 的新方法，它将内置了任务特定的 Token 到图形变数模型中，以提高模型的表达能力。</li>
<li>results: 这篇 paper 的实验结果显示，使用 deep graph prompt tuning 方法可以与 fine-tuning 方法相比，具有相似或甚至更好的性能，且仅需要更少的任务特定参数。<details>
<summary>Abstract</summary>
Graph transformers have gained popularity in various graph-based tasks by addressing challenges faced by traditional Graph Neural Networks. However, the quadratic complexity of self-attention operations and the extensive layering in graph transformer architectures present challenges when applying them to graph based prediction tasks. Fine-tuning, a common approach, is resource-intensive and requires storing multiple copies of large models. We propose a novel approach called deep graph prompt tuning as an alternative to fine-tuning for leveraging large graph transformer models in downstream graph based prediction tasks. Our method introduces trainable feature nodes to the graph and pre-pends task-specific tokens to the graph transformer, enhancing the model's expressive power. By freezing the pre-trained parameters and only updating the added tokens, our approach reduces the number of free parameters and eliminates the need for multiple model copies, making it suitable for small datasets and scalable to large graphs. Through extensive experiments on various-sized datasets, we demonstrate that deep graph prompt tuning achieves comparable or even superior performance to fine-tuning, despite utilizing significantly fewer task-specific parameters. Our contributions include the introduction of prompt tuning for graph transformers, its application to both graph transformers and message passing graph neural networks, improved efficiency and resource utilization, and compelling experimental results. This work brings attention to a promising approach to leverage pre-trained models in graph based prediction tasks and offers new opportunities for exploring and advancing graph representation learning.
</details>
<details>
<summary>摘要</summary>
graph transformers 在各种图像任务中获得了广泛的应用，但传统图 neural network 中的 quadratic complexity 和图 transformer 架构中的层次结构却阻碍了它们在图基于预测任务中的应用。 fine-tuning，一种常见的方法，需要存储多个大型模型，并且是资源占用的。我们提出了一种新的方法，即深度图 prompt tuning，作为 fine-tuning 的替代方案，用于利用大型图 transformer 模型在图基于预测任务中。我们的方法在图中添加可训练的特征节点，并在图 transformer 中预先补充任务特定的token，从而提高模型的表达能力。通过冻结预训练参数并只更新添加的token，我们的方法可以降低自定义参数的数量，消除多个模型 копи本的需求，使其适用于小 dataset 和可扩展到大图。我们的贡献包括图 transformers 中的 prompt tuning 的引入，其应用于图 transformers 和 message passing graph neural networks，以及提高效率和资源利用。我们的实验结果表明，深度图 prompt tuning 可以与 fine-tuning 相比，即使使用许多任务特定参数，并且具有更好的可扩展性。这种方法将注意力集中在图基于预测任务中的可优化模型，并提供新的机遇，用于探索和提高图表示学习。
</details></li>
</ul>
<hr>
<h2 id="Pre-training-on-Synthetic-Driving-Data-for-Trajectory-Prediction"><a href="#Pre-training-on-Synthetic-Driving-Data-for-Trajectory-Prediction" class="headerlink" title="Pre-training on Synthetic Driving Data for Trajectory Prediction"></a>Pre-training on Synthetic Driving Data for Trajectory Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10121">http://arxiv.org/abs/2309.10121</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiheng Li, Seth Z. Zhao, Chenfeng Xu, Chen Tang, Chenran Li, Mingyu Ding, Masayoshi Tomizuka, Wei Zhan</li>
<li>for: 本研究旨在提高自动驾驶路径预测模型的泛化能力，以便在有限数据情况下进行学习。</li>
<li>methods: 我们提议通过对高分辨率地图（HD map）和路径进行扩充和预训练来解决有限数据的挑战。我们利用图表示法将HD map映射到vector空间中，以便轻松地扩充有限的Scene数据。此外，我们采用规则库模型生成基于扩充的Scene中的路径，从而超出实际收集的路径。</li>
<li>results: 我们对不同预训练策略进行了广泛的测试，并证明了我们的数据扩充和预训练策略的效果。相比基eline预测模型，我们的方法在$MR_6$, $minADE_6$和$minFDE_6$指标上升减5.04%, 3.84%和8.30%。<details>
<summary>Abstract</summary>
Accumulating substantial volumes of real-world driving data proves pivotal in the realm of trajectory forecasting for autonomous driving. Given the heavy reliance of current trajectory forecasting models on data-driven methodologies, we aim to tackle the challenge of learning general trajectory forecasting representations under limited data availability. We propose to augment both HD maps and trajectories and apply pre-training strategies on top of them. Specifically, we take advantage of graph representations of HD-map and apply vector transformations to reshape the maps, to easily enrich the limited number of scenes. Additionally, we employ a rule-based model to generate trajectories based on augmented scenes; thus enlarging the trajectories beyond the collected real ones. To foster the learning of general representations within this augmented dataset, we comprehensively explore the different pre-training strategies, including extending the concept of a Masked AutoEncoder (MAE) for trajectory forecasting. Extensive experiments demonstrate the effectiveness of our data expansion and pre-training strategies, which outperform the baseline prediction model by large margins, e.g. 5.04%, 3.84% and 8.30% in terms of $MR_6$, $minADE_6$ and $minFDE_6$.
</details>
<details>
<summary>摘要</summary>
驱动数据的总量聚集对于自动驾驶的轨迹预测是非常重要的。由于当前的轨迹预测模型具有很大的数据驱动特性，我们想要解决有限数据量下学习通用的轨迹预测表示。我们提出了对 HD 地图和轨迹进行扩充和预训练的方法。 Specifically，我们利用 HD 地图的图表示法，并对它们进行 vector 变换，以便轻松地拓宽有限数量的场景。此外，我们采用规则型模型来生成基于扩充的场景的轨迹，从而将轨迹扩大到实际收集的轨迹之 beyond。为了让模型学习通用的表示，我们全面探讨了不同的预训练策略，包括扩展 MAE 的概念来进行轨迹预测。广泛的实验证明了我们的数据扩展和预训练策略的效果，其比基eline 预测模型高大幅度，例如 $MR_6$、$minADE_6$ 和 $minFDE_6$ 的预测误差分别为 5.04%、3.84% 和 8.30%。
</details></li>
</ul>
<hr>
<h2 id="GEDepth-Ground-Embedding-for-Monocular-Depth-Estimation"><a href="#GEDepth-Ground-Embedding-for-Monocular-Depth-Estimation" class="headerlink" title="GEDepth: Ground Embedding for Monocular Depth Estimation"></a>GEDepth: Ground Embedding for Monocular Depth Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09975">http://arxiv.org/abs/2309.09975</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qcraftai/gedepth">https://github.com/qcraftai/gedepth</a></li>
<li>paper_authors: Xiaodong Yang, Zhuang Ma, Zhiyu Ji, Zhe Ren</li>
<li>for: 提高深度估计器的通用性和泛化能力，解耦了图像和相机参数之间的关系</li>
<li>methods: 提出了一种新的地面嵌入模块，通过将相机参数和图像嵌入到深度估计中，提高了模型的通用性和泛化能力</li>
<li>results: 实验表明，该方法可以在多个标准测试集上达到当前最佳效果，并且在跨域测试中表现出显著的泛化改进<details>
<summary>Abstract</summary>
Monocular depth estimation is an ill-posed problem as the same 2D image can be projected from infinite 3D scenes. Although the leading algorithms in this field have reported significant improvement, they are essentially geared to the particular compound of pictorial observations and camera parameters (i.e., intrinsics and extrinsics), strongly limiting their generalizability in real-world scenarios. To cope with this challenge, this paper proposes a novel ground embedding module to decouple camera parameters from pictorial cues, thus promoting the generalization capability. Given camera parameters, the proposed module generates the ground depth, which is stacked with the input image and referenced in the final depth prediction. A ground attention is designed in the module to optimally combine ground depth with residual depth. Our ground embedding is highly flexible and lightweight, leading to a plug-in module that is amenable to be integrated into various depth estimation networks. Experiments reveal that our approach achieves the state-of-the-art results on popular benchmarks, and more importantly, renders significant generalization improvement on a wide range of cross-domain tests.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate("Monocular depth estimation is an ill-posed problem as the same 2D image can be projected from infinite 3D scenes.") translate("Although the leading algorithms in this field have reported significant improvement, they are essentially geared to the particular compound of pictorial observations and camera parameters (i.e., intrinsics and extrinsics), strongly limiting their generalizability in real-world scenarios.") translate("To cope with this challenge, this paper proposes a novel ground embedding module to decouple camera parameters from pictorial cues, thus promoting the generalization capability.") translate("Given camera parameters, the proposed module generates the ground depth, which is stacked with the input image and referenced in the final depth prediction.") translate("A ground attention is designed in the module to optimally combine ground depth with residual depth.") translate("Our ground embedding is highly flexible and lightweight, leading to a plug-in module that is amenable to be integrated into various depth estimation networks.") translate("Experiments reveal that our approach achieves the state-of-the-art results on popular benchmarks, and more importantly, renders significant generalization improvement on a wide range of cross-domain tests.")</SYS>Here's the text in Simplified Chinese:<<SYS>>Monocular depth estimation 是一个不定问题，因为同一个2D图像可以来自无数个3D场景。尽管这个领域的主导算法已经报告了显著改进，但它们实际上受限于特定的图像观察和相机参数（即内参和外参），这限制了它们在真实世界场景中的一般化能力。为了解决这个挑战，本文提出了一种新的地面嵌入模块，用于分离相机参数和图像观察的绑定关系，从而提高总体化能力。给定相机参数，该模块生成的地面深度将与输入图像拼接起来，并作为最终深度预测中的参考。模块中还设计了地面注意力，用于优化地面深度和剩余深度的组合。我们的地面嵌入非常灵活和轻量级，导致它可以易于与不同的深度估计网络集成。实验表明，我们的方法在流行的标准底图上实现了状态艺术的结果，并在广泛的跨频测试中显示了重要的一般化改进。</SYS>
</details></li>
</ul>
<hr>
<h2 id="Parameter-Efficient-Long-Tailed-Recognition"><a href="#Parameter-Efficient-Long-Tailed-Recognition" class="headerlink" title="Parameter-Efficient Long-Tailed Recognition"></a>Parameter-Efficient Long-Tailed Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10019">http://arxiv.org/abs/2309.10019</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shijxcs/pel">https://github.com/shijxcs/pel</a></li>
<li>paper_authors: Jiang-Xin Shi, Tong Wei, Zhi Zhou, Xin-Yan Han, Jie-Jing Shao, Yu-Feng Li</li>
<li>for: 这篇 paper 旨在提出一种能够快速进行 fine-tuning 的方法，以便在承载大量数据的情况下，实现长尾识别 задачі中的好几个类别的准确识别。</li>
<li>methods: 这篇 paper 使用了一种称为 PEL 的 fine-tuning 方法，这种方法通过将一小部分的任务特定参数添加到预训练好的模型中，以便在极少的训练 epoch 内实现好的表现。此外，这篇 paper 还提出了一种具有 semantic-aware 的分类器初始化技术，它可以从 CLIP 的文本嵌入器中获得，并不会增加任何计算负载。</li>
<li>results: 这篇 paper 的实验结果显示，使用 PEL 方法可以在四个长尾数据集上实现更好的表现，并且较前一些state-of-the-art 方法出色。<details>
<summary>Abstract</summary>
The "pre-training and fine-tuning" paradigm in addressing long-tailed recognition tasks has sparked significant interest since the emergence of large vision-language models like the contrastive language-image pre-training (CLIP). While previous studies have shown promise in adapting pre-trained models for these tasks, they often undesirably require extensive training epochs or additional training data to maintain good performance. In this paper, we propose PEL, a fine-tuning method that can effectively adapt pre-trained models to long-tailed recognition tasks in fewer than 20 epochs without the need for extra data. We first empirically find that commonly used fine-tuning methods, such as full fine-tuning and classifier fine-tuning, suffer from overfitting, resulting in performance deterioration on tail classes. To mitigate this issue, PEL introduces a small number of task-specific parameters by adopting the design of any existing parameter-efficient fine-tuning method. Additionally, to expedite convergence, PEL presents a novel semantic-aware classifier initialization technique derived from the CLIP textual encoder without adding any computational overhead. Our experimental results on four long-tailed datasets demonstrate that PEL consistently outperforms previous state-of-the-art approaches. The source code is available at https://github.com/shijxcs/PEL.
</details>
<details>
<summary>摘要</summary>
“对于长尾识别任务，使用“预训练和精确化”方法已经引起了广泛的关注，自大型视力语言模型CLIP出现以来。 although previous studies have shown promise in adapting pre-trained models for these tasks, they often require extensive training epochs or additional training data to maintain good performance. In this paper, we propose PEL, a fine-tuning method that can effectively adapt pre-trained models to long-tailed recognition tasks in fewer than 20 epochs without the need for extra data. We first empirically find that commonly used fine-tuning methods, such as full fine-tuning and classifier fine-tuning, suffer from overfitting, resulting in performance deterioration on tail classes. To mitigate this issue, PEL introduces a small number of task-specific parameters by adopting the design of any existing parameter-efficient fine-tuning method. Additionally, to expedite convergence, PEL presents a novel semantic-aware classifier initialization technique derived from the CLIP textual encoder without adding any computational overhead. Our experimental results on four long-tailed datasets demonstrate that PEL consistently outperforms previous state-of-the-art approaches. The source code is available at https://github.com/shijxcs/PEL.”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that form instead.
</details></li>
</ul>
<hr>
<h2 id="vSHARP-variable-Splitting-Half-quadratic-ADMM-algorithm-for-Reconstruction-of-inverse-Problems"><a href="#vSHARP-variable-Splitting-Half-quadratic-ADMM-algorithm-for-Reconstruction-of-inverse-Problems" class="headerlink" title="vSHARP: variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse-Problems"></a>vSHARP: variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse-Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09954">http://arxiv.org/abs/2309.09954</a></li>
<li>repo_url: None</li>
<li>paper_authors: George Yiasemis, Nikita Moriakov, Jan-Jakob Sonke, Jonas Teuwen</li>
<li>for: 本研究旨在提出一种基于深度学习的方法，用于解决医疗影像（MI）任务中的不规则 inverse problem。</li>
<li>methods: 该方法基于变分半quadratic ADMM算法，并使用梯度下降法和U-Net架构来保证数据一致性和图像质量提升。</li>
<li>results: 对于两个不同的数据集，我们的实验结果表明，vSHARP方法在加速并行磁共振成像重构任务中表现出色，与状态艺术方法相比，具有更高的性能。<details>
<summary>Abstract</summary>
Medical Imaging (MI) tasks, such as accelerated Parallel Magnetic Resonance Imaging (MRI), often involve reconstructing an image from noisy or incomplete measurements. This amounts to solving ill-posed inverse problems, where a satisfactory closed-form analytical solution is not available. Traditional methods such as Compressed Sensing (CS) in MRI reconstruction can be time-consuming or prone to obtaining low-fidelity images. Recently, a plethora of supervised and self-supervised Deep Learning (DL) approaches have demonstrated superior performance in inverse-problem solving, surpassing conventional methods. In this study, we propose vSHARP (variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse Problems), a novel DL-based method for solving ill-posed inverse problems arising in MI. vSHARP utilizes the Half-Quadratic Variable Splitting method and employs the Alternating Direction Method of Multipliers (ADMM) to unroll the optimization process. For data consistency, vSHARP unrolls a differentiable gradient descent process in the image domain, while a DL-based denoiser, such as a U-Net architecture, is applied to enhance image quality. vSHARP also employs a dilated-convolution DL-based model to predict the Lagrange multipliers for the ADMM initialization. We evaluate the proposed model by applying it to the task of accelerated Parallel MRI Reconstruction on two distinct datasets. We present a comparative analysis of our experimental results with state-of-the-art approaches, highlighting the superior performance of vSHARP.
</details>
<details>
<summary>摘要</summary>
医学成像（MI）任务，如加速的并行磁共振成像（MRI），经常需要从噪声或不完整的测量中重建图像。这等于解决不定系数的反向问题，其中没有可靠的关闭形式分析解。传统方法，如压缩整流（CS）在MRI重建中，可能需要很长时间或得到低精度图像。最近，一些深度学习（DL）方法在反向问题解决方面表现出色，超过了传统方法。在这种研究中，我们提出了变量分割半quadratic ADMM算法 для reconstruction of inverse problems（vSHARP），这是一种基于DL的新方法。vSHARP使用半quadratic变量分割方法和Alternating Direction Method of Multipliers（ADMM）来卷赋予过程。为保持数据一致性，vSHARP在图像域中滚动一个可微的梯度下降过程，而DL基于U-Net架构的降噪器来提高图像质量。vSHARP还使用填充 convolution DL模型来预测ADMM的拉格朗日multipliers的初始化。我们通过在两个不同的数据集上应用vSHARP进行加速的并行MRI重建，并对其实验结果进行比较分析，highlighting vSHARP的superior performance。
</details></li>
</ul>
<hr>
<h2 id="End-to-End-Learned-Event-and-Image-based-Visual-Odometry"><a href="#End-to-End-Learned-Event-and-Image-based-Visual-Odometry" class="headerlink" title="End-to-End Learned Event- and Image-based Visual Odometry"></a>End-to-End Learned Event- and Image-based Visual Odometry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09947">http://arxiv.org/abs/2309.09947</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roberto Pellerito, Marco Cannici, Daniel Gehrig, Joris Belhadj, Olivier Dubois-Matra, Massimo Casasco, Davide Scaramuzza</li>
<li>for: 这个论文主要是为了实现自主无人机导航，特别是在GPS被防止的环境中，例如行星表面。</li>
<li>methods: 这个系统使用了新型的循环、异步和大量并行（RAMP）编码器，让它比现有的异步编码器快了8倍，精度高20%。</li>
<li>results: 这个系统在实验中训练后，在传统的实际世界标准测试benchmark上表现出来，与像素和事件基本方法相比，提高了52%和20%。<details>
<summary>Abstract</summary>
Visual Odometry (VO) is crucial for autonomous robotic navigation, especially in GPS-denied environments like planetary terrains. While standard RGB cameras struggle in low-light or high-speed motion, event-based cameras offer high dynamic range and low latency. However, seamlessly integrating asynchronous event data with synchronous frames remains challenging. We introduce RAMP-VO, the first end-to-end learned event- and image-based VO system. It leverages novel Recurrent, Asynchronous, and Massively Parallel (RAMP) encoders that are 8x faster and 20% more accurate than existing asynchronous encoders. RAMP-VO further employs a novel pose forecasting technique to predict future poses for initialization. Despite being trained only in simulation, RAMP-VO outperforms image- and event-based methods by 52% and 20%, respectively, on traditional, real-world benchmarks as well as newly introduced Apollo and Malapert landing sequences, paving the way for robust and asynchronous VO in space.
</details>
<details>
<summary>摘要</summary>
“视觉协调（VO）是自主Robot导航中的关键，尤其在没有GPS环境下，如行星表面。标准的RGB摄像机在低光照或高速运动情况下表现不佳，而事件驱动摄像机则提供了高动态范围和低延迟。然而，将异步事件数据与同步帧数据集成仍然是一个挑战。我们介绍了RAMP-VO，首个综合学习的事件和图像基于VO系统。它利用了新的循环、异步和极大并行（RAMP）编码器，比现有的异步编码器快8倍，并且精度高20%。RAMP-VO还使用了一种新的预测未来姿势技术，以初始化。尽管只在模拟环境中训练，RAMP-VO仍然在传统的实际世界benchmark中与图像和事件基于方法相比，提高52%和20%。此外，RAMP-VO还在新引入的Apoll和Malapert降落序列上表现出色，为 asynchronous VO在宇宙中铺平道路。”
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Attention-and-Graph-Neural-Networks-Toward-Drift-Free-Pose-Estimation"><a href="#Hierarchical-Attention-and-Graph-Neural-Networks-Toward-Drift-Free-Pose-Estimation" class="headerlink" title="Hierarchical Attention and Graph Neural Networks: Toward Drift-Free Pose Estimation"></a>Hierarchical Attention and Graph Neural Networks: Toward Drift-Free Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09934">http://arxiv.org/abs/2309.09934</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kathia Melbouci, Fawzi Nashashibi</li>
<li>for: 本研究旨在提高3D几何对齐的精度，而不是使用传统的几何对齐和pose graph优化方法。</li>
<li>methods: 本研究使用学习模型，包括层次注意力机制和图神经网络，取代传统的几何对齐和pose graph优化方法。</li>
<li>results: 测试结果表明，使用本研究的方法可以significantly improve pose estimation accuracy，特别是在确定旋转组件的时候。Here’s the full Chinese text:</li>
<li>for: 本研究旨在提高3D几何对齐的精度，而不是使用传统的几何对齐和pose graph优化方法。</li>
<li>methods: 本研究使用学习模型，包括层次注意力机制和图神经网络，取代传统的几何对齐和pose graph优化方法。</li>
<li>results: 测试结果表明，使用本研究的方法可以significantly improve pose estimation accuracy，特别是在确定旋转组件的时候。<details>
<summary>Abstract</summary>
The most commonly used method for addressing 3D geometric registration is the iterative closet-point algorithm, this approach is incremental and prone to drift over multiple consecutive frames. The Common strategy to address the drift is the pose graph optimization subsequent to frame-to-frame registration, incorporating a loop closure process that identifies previously visited places. In this paper, we explore a framework that replaces traditional geometric registration and pose graph optimization with a learned model utilizing hierarchical attention mechanisms and graph neural networks. We propose a strategy to condense the data flow, preserving essential information required for the precise estimation of rigid poses. Our results, derived from tests on the KITTI Odometry dataset, demonstrate a significant improvement in pose estimation accuracy. This improvement is especially notable in determining rotational components when compared with results obtained through conventional multi-way registration via pose graph optimization. The code will be made available upon completion of the review process.
</details>
<details>
<summary>摘要</summary>
最常用的3D几何注册方法是迭代最近点算法，这种方法是逐帧增量的，容易出现多个 consecutived frames 中的漂移。常见的策略是通过 frame-to-frame 注册后，进行 pose graph optimization，并包括一个循环关闭过程，以确定先前访问的地方。在这篇论文中，我们探讨了一种替代传统的几何注册和 pose graph optimization 方法，使用层次注意力机制和图 neural networks。我们提议一种压缩数据流程，以保留必要的信息，以便精确地估计固定pose。我们的结果，基于 KITTI Odometry 数据集的测试，表明了对 pose 估计精度的显著改进。这种改进，特别是在确定旋转 Component 方面，与传统的多向注册via pose graph optimization 相比，具有更高的精度。代码将在审核过程完成后提供。
</details></li>
</ul>
<hr>
<h2 id="Quantum-Vision-Clustering"><a href="#Quantum-Vision-Clustering" class="headerlink" title="Quantum Vision Clustering"></a>Quantum Vision Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09907">http://arxiv.org/abs/2309.09907</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuan Bac Nguyen, Benjamin Thompson, Hugh Churchill, Khoa Luu, Samee U. Khan</li>
<li>for: 这篇论文主要是研究无监督视觉归一化的方法，以解释无标签图像的分布。</li>
<li>methods: 该论文提出了首个适合量子计算机解决的归一化问题的形式ulation，使用了爱因斯坦场的模型来表示量子机制。</li>
<li>results: 该论文表明，使用当今的整数编程 solver 和量子计算机，可以实现高效的归一化解决方案，并且可以解决小例子。<details>
<summary>Abstract</summary>
Unsupervised visual clustering has recently received considerable attention. It aims to explain distributions of unlabeled visual images by clustering them via a parameterized appearance model. From a different perspective, the clustering algorithms can be treated as assignment problems, often NP-hard. They can be solved precisely for small instances on current hardware. Adiabatic quantum computing (AQC) offers a solution, as it can soon provide a considerable speedup on a range of NP-hard optimization problems. However, current clustering formulations are unsuitable for quantum computing due to their scaling properties. Consequently, in this work, we propose the first clustering formulation designed to be solved with AQC. We employ an Ising model representing the quantum mechanical system implemented on the AQC. Our approach is competitive compared to state-of-the-art optimization-based approaches, even using of-the-shelf integer programming solvers. Finally, we demonstrate that our clustering problem is already solvable on the current generation of real quantum computers for small examples and analyze the properties of the measured solutions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="On-Model-Explanations-with-Transferable-Neural-Pathways"><a href="#On-Model-Explanations-with-Transferable-Neural-Pathways" class="headerlink" title="On Model Explanations with Transferable Neural Pathways"></a>On Model Explanations with Transferable Neural Pathways</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09887">http://arxiv.org/abs/2309.09887</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinmiao Lin, Wentao Bao, Qi Yu, Yu Kong</li>
<li>for: 本研究旨在提供一种可解释的神经网络模型，即生成类相关神经路径（GEN-CNP）模型，以便解释神经网络模型的行为。</li>
<li>methods: 本研究使用了生成神经路径的方法，包括：（i）同类神经路径主要由类相关神经元组成；（ii）每个实例的神经路径精度应该是最佳的。</li>
<li>results: 本研究通过实验和质量评价表明，生成类相关神经路径可以准确地解释神经网络模型的行为，并且可以 transferred to 解释不同实例的样本。<details>
<summary>Abstract</summary>
Neural pathways as model explanations consist of a sparse set of neurons that provide the same level of prediction performance as the whole model. Existing methods primarily focus on accuracy and sparsity but the generated pathways may offer limited interpretability thus fall short in explaining the model behavior. In this paper, we suggest two interpretability criteria of neural pathways: (i) same-class neural pathways should primarily consist of class-relevant neurons; (ii) each instance's neural pathway sparsity should be optimally determined. To this end, we propose a Generative Class-relevant Neural Pathway (GEN-CNP) model that learns to predict the neural pathways from the target model's feature maps. We propose to learn class-relevant information from features of deep and shallow layers such that same-class neural pathways exhibit high similarity. We further impose a faithfulness criterion for GEN-CNP to generate pathways with instance-specific sparsity. We propose to transfer the class-relevant neural pathways to explain samples of the same class and show experimentally and qualitatively their faithfulness and interpretability.
</details>
<details>
<summary>摘要</summary>
神经路径作为模型解释的方法包括一个稀疏的集合神经元，可以提供与整个模型一样的预测性能。现有方法主要关注准确率和稀疏性，但生成的路径可能具有有限的解释性，因此无法完全解释模型的行为。在这篇论文中，我们提出了两个神经路径解释标准：（i）同类神经路径主要由相关类神经元组成；（ii）每个实例的神经路径稀疏度应该得到优化。为此，我们提议一种生成类相关神经路径（GEN-CNP）模型，可以从目标模型的特征地图中预测神经路径。我们从深层和浅层特征中学习类相关信息，使同类神经路径具有高相似性。此外，我们对 GEN-CNP 模型强制实施一个具体性标准，使得生成的神经路径具有实例特定的稀疏度。我们将这些类相关神经路径传递给解释同类样本，并通过实验和质量上的证明，证明其 faithfulness 和可解释性。
</details></li>
</ul>
<hr>
<h2 id="RaLF-Flow-based-Global-and-Metric-Radar-Localization-in-LiDAR-Maps"><a href="#RaLF-Flow-based-Global-and-Metric-Radar-Localization-in-LiDAR-Maps" class="headerlink" title="RaLF: Flow-based Global and Metric Radar Localization in LiDAR Maps"></a>RaLF: Flow-based Global and Metric Radar Localization in LiDAR Maps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09875">http://arxiv.org/abs/2309.09875</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhijeet Nayak, Daniele Cattaneo, Abhinav Valada</li>
<li>for: 本研究旨在提出一种基于深度神经网络的radar扫描地图匹配方法，以实现自主车辆的地面定位。</li>
<li>methods: 该方法首先通过cross-modal metric学习来学习 радиar和LiDAR特征编码器，然后通过global描述器和三个自由度变换预测器来实现地图匹配和 метрик地位定位。</li>
<li>results: 经过广泛的实验 validate，该方法可以在多个真实的驾驶数据集上达到状态的艺术性表现，并且能够有效地适应不同的城市和感知设置。Here’s the summary in English for reference:</li>
<li>for: The paper proposes a novel deep neural network-based approach for localizing radar scans in a LiDAR map of the environment, which can achieve robust and accurate positioning for autonomous vehicles.</li>
<li>methods: The approach first learns a shared embedding space between radar and LiDAR modalities via cross-modal metric learning, and then uses a place recognition head to generate global descriptors and a metric localization head to predict the 3-DoF transformation between the radar scan and the map.</li>
<li>results: Extensive experiments on multiple real-world driving datasets demonstrate that the proposed approach achieves state-of-the-art performance for both place recognition and metric localization, and can effectively generalize to different cities and sensor setups than the ones used during training.<details>
<summary>Abstract</summary>
Localization is paramount for autonomous robots. While camera and LiDAR-based approaches have been extensively investigated, they are affected by adverse illumination and weather conditions. Therefore, radar sensors have recently gained attention due to their intrinsic robustness to such conditions. In this paper, we propose RaLF, a novel deep neural network-based approach for localizing radar scans in a LiDAR map of the environment, by jointly learning to address both place recognition and metric localization. RaLF is composed of radar and LiDAR feature encoders, a place recognition head that generates global descriptors, and a metric localization head that predicts the 3-DoF transformation between the radar scan and the map. We tackle the place recognition task by learning a shared embedding space between the two modalities via cross-modal metric learning. Additionally, we perform metric localization by predicting pixel-level flow vectors that align the query radar scan with the LiDAR map. We extensively evaluate our approach on multiple real-world driving datasets and show that RaLF achieves state-of-the-art performance for both place recognition and metric localization. Moreover, we demonstrate that our approach can effectively generalize to different cities and sensor setups than the ones used during training. We make the code and trained models publicly available at http://ralf.cs.uni-freiburg.de.
</details>
<details>
<summary>摘要</summary>
“本文提出了一种新的深度神经网络方法，用于将雷达扫描图与激光扫描图进行本地化。该方法被称为RaLF，它可以同时解决地点识别和精度本地化问题。RaLF包括雷达和激光特征编码器、地点识别头和精度本地化头。我们通过跨模态度学习来学习雷达和激光图像之间的共享空间。此外，我们还预测了 queries 雷达扫描图和激光图像之间的像素级流动场，以将 queries 雷达扫描图与激光图像相对平行。我们对多个实际驾驶 dataset 进行了广泛的评估，并证明了 RaLF 在地点识别和精度本地化方面具有了领先的性能。此外，我们还证明了我们的方法可以对不同的城市和感知器设置进行有效的泛化。我们将代码和训练模型公开发布在 http://ralf.cs.uni-freiburg.de。”
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Learning-for-Enhancing-Robust-Scene-Transfer-in-Vision-based-Agile-Flight"><a href="#Contrastive-Learning-for-Enhancing-Robust-Scene-Transfer-in-Vision-based-Agile-Flight" class="headerlink" title="Contrastive Learning for Enhancing Robust Scene Transfer in Vision-based Agile Flight"></a>Contrastive Learning for Enhancing Robust Scene Transfer in Vision-based Agile Flight</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09865">http://arxiv.org/abs/2309.09865</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxu Xing, Leonard Bauersfeld, Yunlong Song, Chunwei Xing, Davide Scaramuzza</li>
<li>for: 本研究旨在提出一种适用于视觉基于移动 робоット应用的Scene转移策略，以解决现有的策略具有差异样本效率或局部化能力等问题。</li>
<li>methods: 本研究提出了一种适应多对多对比学习策略，通过对视觉表示学习进行适应调整，实现零例转移和实际应用。控制策略可以在未经训练的环境中运行，不需要训练环境的微调。</li>
<li>results: 实验和实际应用 результалтати表明，我们的方法可以成功泛化到未经训练的环境，并且超越所有基elines。在视觉基于quadrotor飞行任务上，我们的方法实现了高精度和稳定性。<details>
<summary>Abstract</summary>
Scene transfer for vision-based mobile robotics applications is a highly relevant and challenging problem. The utility of a robot greatly depends on its ability to perform a task in the real world, outside of a well-controlled lab environment. Existing scene transfer end-to-end policy learning approaches often suffer from poor sample efficiency or limited generalization capabilities, making them unsuitable for mobile robotics applications. This work proposes an adaptive multi-pair contrastive learning strategy for visual representation learning that enables zero-shot scene transfer and real-world deployment. Control policies relying on the embedding are able to operate in unseen environments without the need for finetuning in the deployment environment. We demonstrate the performance of our approach on the task of agile, vision-based quadrotor flight. Extensive simulation and real-world experiments demonstrate that our approach successfully generalizes beyond the training domain and outperforms all baselines.
</details>
<details>
<summary>摘要</summary>
场景传输 для视觉基于移动 робо扮 applications 是一个非常相关和挑战的问题。机器人的实用性很大程度取决于它能够在实际世界中完成任务，而不是在受控的实验室环境中。现有的场景传输末端政策学习方法经常受到样本效率的限制或者实际应用中的局限性，这使得它们不适合移动 робо扮应用。这项工作提出了一种适应多对比较学习策略，用于视觉表示学习，允许零shot场景传输并在实际应用中无需质量调整。我们示出了我们方法在激扬、视觉基于四旋翼飞行任务中的表现。广泛的 simulations 和实际实验表明，我们的方法可以成功泛化到训练环境之外，并且超过了所有基elines。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Open-Vocabulary-Object-Localization-in-Videos"><a href="#Unsupervised-Open-Vocabulary-Object-Localization-in-Videos" class="headerlink" title="Unsupervised Open-Vocabulary Object Localization in Videos"></a>Unsupervised Open-Vocabulary Object Localization in Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09858">http://arxiv.org/abs/2309.09858</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ke Fan, Zechen Bai, Tianjun Xiao, Dominik Zietlow, Max Horn, Zixu Zhao, Carl-Johann Simon-Gabriel, Mike Zheng Shou, Francesco Locatello, Bernt Schiele, Thomas Brox, Zheng Zhang, Yanwei Fu, Tong He</li>
<li>for: 自动Localization of objects in videos without manual annotation.</li>
<li>methods: 使用latest advances in video representation learning and pre-trained vision-language models, followed by a slot attention approach to localize objects in videos, and then use unsupervised way to read localized semantic information from CLIP model to assign text to the obtained slots.</li>
<li>results: 实现了无监督的视频对象定位，并且可以在常见的视频benchmark上达到良好的效果，这是首次实现无监督视频对象定位的方法。<details>
<summary>Abstract</summary>
In this paper, we show that recent advances in video representation learning and pre-trained vision-language models allow for substantial improvements in self-supervised video object localization. We propose a method that first localizes objects in videos via a slot attention approach and then assigns text to the obtained slots. The latter is achieved by an unsupervised way to read localized semantic information from the pre-trained CLIP model. The resulting video object localization is entirely unsupervised apart from the implicit annotation contained in CLIP, and it is effectively the first unsupervised approach that yields good results on regular video benchmarks.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们表明了最近的视频表示学习和预训练视频语言模型的进步，可以提供substantial提高自主视频 объек特定的性能。我们提议一种方法，首先通过槽注意力方法在视频中Localize对象，然后通过预训练CLIP模型中的semantic信息来读取本地化的文本。这种方法是完全无监督的，并且可以在常见的视频benchmark上达到良好的性能。
</details></li>
</ul>
<hr>
<h2 id="PseudoCal-Towards-Initialisation-Free-Deep-Learning-Based-Camera-LiDAR-Self-Calibration"><a href="#PseudoCal-Towards-Initialisation-Free-Deep-Learning-Based-Camera-LiDAR-Self-Calibration" class="headerlink" title="PseudoCal: Towards Initialisation-Free Deep Learning-Based Camera-LiDAR Self-Calibration"></a>PseudoCal: Towards Initialisation-Free Deep Learning-Based Camera-LiDAR Self-Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09855">http://arxiv.org/abs/2309.09855</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mathieu Cocheteux, Julien Moreau, Franck Davoine</li>
<li>for: 这篇论文主要用于测试一种自动测量摄录和探测器的整合，以提高自主车辆和机器人的多感器融合。</li>
<li>methods: 这篇论文使用了一种名为 PseudoCal 的新型自我测量方法，它利用 Pseudo-LiDAR 概念和直接在3D空间上运作，以解决传统方法的问题，例如需要人工干预和特定环境，导致实际应用中存在误差和问题。</li>
<li>results: 这篇论文的结果显示，PseudoCal 方法可以在一般的自主车辆和机器人环境中实现一击准确的整合，并且可以处理极端情况，例如对照摄录和探测器的偏差和误差。<details>
<summary>Abstract</summary>
Camera-LiDAR extrinsic calibration is a critical task for multi-sensor fusion in autonomous systems, such as self-driving vehicles and mobile robots. Traditional techniques often require manual intervention or specific environments, making them labour-intensive and error-prone. Existing deep learning-based self-calibration methods focus on small realignments and still rely on initial estimates, limiting their practicality. In this paper, we present PseudoCal, a novel self-calibration method that overcomes these limitations by leveraging the pseudo-LiDAR concept and working directly in the 3D space instead of limiting itself to the camera field of view. In typical autonomous vehicle and robotics contexts and conventions, PseudoCal is able to perform one-shot calibration quasi-independently of initial parameter estimates, addressing extreme cases that remain unsolved by existing approaches.
</details>
<details>
<summary>摘要</summary>
Camera-LiDAR这个外在对焦整合是自主系统中的一个关键任务，例如自驾车和移动机器人。传统的方法通常需要手动干预或特定的环境，使其成为劳动密集和错误容易发生的。现有的深度学习自我对焦方法专注于小对焦和仍然依赖初始估计，限制其实用性。在这篇论文中，我们提出了 PseudoCal，一种新的自我对焦方法，通过 Pseudo-LiDAR 概念和直接在3D空间工作，而不是仅对焦相机的视场。在典型的自驾车和机器人上下文中，PseudoCal 能够实现一次对焦 quasi-独立于初始参数估计，解决现有方法无法解决的极端情况。
</details></li>
</ul>
<hr>
<h2 id="Hyperbolic-vs-Euclidean-Embeddings-in-Few-Shot-Learning-Two-Sides-of-the-Same-Coin"><a href="#Hyperbolic-vs-Euclidean-Embeddings-in-Few-Shot-Learning-Two-Sides-of-the-Same-Coin" class="headerlink" title="Hyperbolic vs Euclidean Embeddings in Few-Shot Learning: Two Sides of the Same Coin"></a>Hyperbolic vs Euclidean Embeddings in Few-Shot Learning: Two Sides of the Same Coin</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10013">http://arxiv.org/abs/2309.10013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriel Moreira, Manuel Marques, João Paulo Costeira, Alexander Hauptmann</li>
<li>for: 这个论文主要研究了使用偏序空间来学习表示，以便在图像识别 tasks 中获得更高精度和更少维度的表示。</li>
<li>methods: 这个论文使用了 прототипические偏序神经网络，并研究了偏序空间中的点集抽象的特性。</li>
<li>results: 研究发现，在高维度情况下，偏序 embedding 往往会趋向于波因逊球体的边缘，这会影响到几何学推理中的几何学距离计算。尽管如此，研究还发现，使用固定半径编码器和欧几丁度距离计算可以在几何学 embeddings 中实现更好的性能，不需要针对 embedding 维度进行调整。<details>
<summary>Abstract</summary>
Recent research in representation learning has shown that hierarchical data lends itself to low-dimensional and highly informative representations in hyperbolic space. However, even if hyperbolic embeddings have gathered attention in image recognition, their optimization is prone to numerical hurdles. Further, it remains unclear which applications stand to benefit the most from the implicit bias imposed by hyperbolicity, when compared to traditional Euclidean features. In this paper, we focus on prototypical hyperbolic neural networks. In particular, the tendency of hyperbolic embeddings to converge to the boundary of the Poincar\'e ball in high dimensions and the effect this has on few-shot classification. We show that the best few-shot results are attained for hyperbolic embeddings at a common hyperbolic radius. In contrast to prior benchmark results, we demonstrate that better performance can be achieved by a fixed-radius encoder equipped with the Euclidean metric, regardless of the embedding dimension.
</details>
<details>
<summary>摘要</summary>
近期研究在表示学习中，层次数据在偏特征空间中得到了低维度和高信息的表示。然而，偏特征 embedding 的优化很容易遇到数值障碍。此外，还未清楚哪些应用程序会从偏特征 embedding 中受益最大，比较传统的欧几丁度特征。在这篇论文中，我们关注了prototype偏特征神经网络。特别是偏特征 embedding 在高维度下的准确性和Poincaré球的边缘减少的效果。我们显示了，使用常见的偏特征 радиус可以获得最佳几拟结果。与先前的标准约束结果不同，我们示出了使用欧几丁度度量的固定 радиус编码器可以在任何嵌入维度下获得更好的性能。
</details></li>
</ul>
<hr>
<h2 id="Grasp-Anything-Large-scale-Grasp-Dataset-from-Foundation-Models"><a href="#Grasp-Anything-Large-scale-Grasp-Dataset-from-Foundation-Models" class="headerlink" title="Grasp-Anything: Large-scale Grasp Dataset from Foundation Models"></a>Grasp-Anything: Large-scale Grasp Dataset from Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09818">http://arxiv.org/abs/2309.09818</a></li>
<li>repo_url: None</li>
<li>paper_authors: An Dinh Vuong, Minh Nhat Vu, Hieu Le, Baoru Huang, Binh Huynh, Thieu Vo, Andreas Kugi, Anh Nguyen</li>
<li>for: 本研究利用基础模型来解决机器人 grasp detection 问题，即机器人抓取物体的问题，这是机器人控制领域中的一个挑战。</li>
<li>methods: 本研究使用基础模型来生成大规模的 grasp 数据集，并利用这些数据集来实现零shot grasp detection。</li>
<li>results: 研究表明，使用基础模型生成的 grasp 数据集可以在视觉任务和实际机器人实验中实现零shot grasp detection，并且超过了先前的数据集。<details>
<summary>Abstract</summary>
Foundation models such as ChatGPT have made significant strides in robotic tasks due to their universal representation of real-world domains. In this paper, we leverage foundation models to tackle grasp detection, a persistent challenge in robotics with broad industrial applications. Despite numerous grasp datasets, their object diversity remains limited compared to real-world figures. Fortunately, foundation models possess an extensive repository of real-world knowledge, including objects we encounter in our daily lives. As a consequence, a promising solution to the limited representation in previous grasp datasets is to harness the universal knowledge embedded in these foundation models. We present Grasp-Anything, a new large-scale grasp dataset synthesized from foundation models to implement this solution. Grasp-Anything excels in diversity and magnitude, boasting 1M samples with text descriptions and more than 3M objects, surpassing prior datasets. Empirically, we show that Grasp-Anything successfully facilitates zero-shot grasp detection on vision-based tasks and real-world robotic experiments. Our dataset and code are available at https://grasp-anything-2023.github.io.
</details>
<details>
<summary>摘要</summary>
基础模型如ChatGPT在机器人任务中做出了 significiant 进步，归因于它们的 universally 表示实际世界领域。在这篇论文中，我们利用基础模型来解决抓取检测，机器人业务中的一项挑战。虽然有很多的抓取数据集，但它们中的物体多样性还是相对较少，与实际世界中的物体相比。幸运的是，基础模型拥有了广泛的实际世界知识，包括我们日常生活中遇到的物体。因此，我们可以利用这些基础模型中的广泛知识来解决限制性的抓取数据集。我们提出了 Grasp-Anything，一个新的大规模抓取数据集，通过基础模型来实现这一解决方案。Grasp-Anything 的多样性和规模都非常出色，包括100万个样本和超过3000万个物体，超过了之前的数据集。我们的实验结果表明，Grasp-Anything 可以在视觉任务和真实世界机器人实验中实现零shot 抓取检测。我们的数据集和代码可以在https://grasp-anything-2023.github.io 上获取。
</details></li>
</ul>
<hr>
<h2 id="R2GenGPT-Radiology-Report-Generation-with-Frozen-LLMs"><a href="#R2GenGPT-Radiology-Report-Generation-with-Frozen-LLMs" class="headerlink" title="R2GenGPT: Radiology Report Generation with Frozen LLMs"></a>R2GenGPT: Radiology Report Generation with Frozen LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09812">http://arxiv.org/abs/2309.09812</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wang-zhanyu/r2gengpt">https://github.com/wang-zhanyu/r2gengpt</a></li>
<li>paper_authors: Zhanyu Wang, Lingqiao Liu, Lei Wang, Luping Zhou</li>
<li>for: 这个研究是为了探索一种可以将类型特征与语言模型（LLM）之间的数据汇流合作，以提高医学报告生成（R2Gen）性能。</li>
<li>methods: 这个研究提出了一种名为R2GenGPT的新解决方案，它使用一个高效的视觉对Alignment模块，将类型特征与LLM的字嵌入空间相对位。这种新的方法让之前 static的LLM能够融合和处理影像信息，从而为R2Gen性能带来了进步。</li>
<li>results: R2GenGPT实现了顶峰性能，只需训练500万个参数（占总参数数的0.07%），并在训练速度和稳定性方面表现出色。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have consistently showcased remarkable generalization capabilities when applied to various language tasks. Nonetheless, harnessing the full potential of LLMs for Radiology Report Generation (R2Gen) still presents a challenge, stemming from the inherent disparity in modality between LLMs and the R2Gen task. To bridge this gap effectively, we propose R2GenGPT, which is a novel solution that aligns visual features with the word embedding space of LLMs using an efficient visual alignment module. This innovative approach empowers the previously static LLM to seamlessly integrate and process image information, marking a step forward in optimizing R2Gen performance. R2GenGPT offers the following benefits. First, it attains state-of-the-art (SOTA) performance by training only the lightweight visual alignment module while freezing all the parameters of LLM. Second, it exhibits high training efficiency, as it requires the training of an exceptionally minimal number of parameters while achieving rapid convergence. By employing delta tuning, our model only trains 5M parameters (which constitute just 0.07\% of the total parameter count) to achieve performance close to the SOTA levels. Our code is available at https://github.com/wang-zhanyu/R2GenGPT.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在不同语言任务中表现出了惊人的通用能力。然而，为了充分利用 LLM 在 radiology report generation（R2Gen）任务中的潜力，仍然存在一定的挑战，这主要归结于 LLM 和 R2Gen 任务之间的本质差异。为了有效 bridge 这个差异，我们提出了 R2GenGPT，它是一种新的解决方案，它将视觉特征与 LLM 中词嵌入空间进行有效对齐。这种创新的方法使得原来的静止 LLM 可以轻松地集成和处理图像信息，从而为 R2Gen 的性能优化做出了一步前进。R2GenGPT 具有以下优点：首先，它可以在具有较少参数的情况下达到最佳性能，只需要训练少量的视觉对齐模块，而不需要训练 LLM 的任何参数。其次，它具有高效的训练效率，只需要训练 5000万参数（即总参数数的 0.07%），即使在快速 converges 的情况下。我们的代码可以在 GitHub 上找到：https://github.com/wang-zhanyu/R2GenGPT。
</details></li>
</ul>
<hr>
<h2 id="DriveDreamer-Towards-Real-world-driven-World-Models-for-Autonomous-Driving"><a href="#DriveDreamer-Towards-Real-world-driven-World-Models-for-Autonomous-Driving" class="headerlink" title="DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving"></a>DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09777">http://arxiv.org/abs/2309.09777</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JeffWang987/DriveDreamer">https://github.com/JeffWang987/DriveDreamer</a></li>
<li>paper_authors: Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Jiwen Lu</li>
<li>for: 本研究旨在开发一种基于实际驾驶场景的世界模型（DriveDreamer），以提高自动驾驶车辆的驾驶视频生成和安全驾驶策略。</li>
<li>methods: 我们提出了一种两阶段训练策略，其中第一阶段是通过学习струк成的交通约束来让DriveDreamer具备了预测未来状态的能力；第二阶段是通过diffusion模型来构建了详细的环境表示。</li>
<li>results: DriveDreamer在nuScenesbenchmark上的实验表明，它可以准确地生成驾驶视频和合理的驾驶策略，并且能够准确地捕捉实际交通场景中的结构约束。<details>
<summary>Abstract</summary>
World models, especially in autonomous driving, are trending and drawing extensive attention due to their capacity for comprehending driving environments. The established world model holds immense potential for the generation of high-quality driving videos, and driving policies for safe maneuvering. However, a critical limitation in relevant research lies in its predominant focus on gaming environments or simulated settings, thereby lacking the representation of real-world driving scenarios. Therefore, we introduce DriveDreamer, a pioneering world model entirely derived from real-world driving scenarios. Regarding that modeling the world in intricate driving scenes entails an overwhelming search space, we propose harnessing the powerful diffusion model to construct a comprehensive representation of the complex environment. Furthermore, we introduce a two-stage training pipeline. In the initial phase, DriveDreamer acquires a deep understanding of structured traffic constraints, while the subsequent stage equips it with the ability to anticipate future states. The proposed DriveDreamer is the first world model established from real-world driving scenarios. We instantiate DriveDreamer on the challenging nuScenes benchmark, and extensive experiments verify that DriveDreamer empowers precise, controllable video generation that faithfully captures the structural constraints of real-world traffic scenarios. Additionally, DriveDreamer enables the generation of realistic and reasonable driving policies, opening avenues for interaction and practical applications.
</details>
<details>
<summary>摘要</summary>
世界模型，尤其在自动驾驶领域，目前是受欢迎的趋势，因其能够理解驾驶环境。现有的世界模型具有巨大的潜力，可以生成高质量的驾驶视频和安全驾驶策略。然而，现有的研究受到限制，因为它们偏向于游戏环境或模拟设定，缺乏实际驾驶场景的表示。因此，我们介绍了DriveDreamer，一种杰出的世界模型，完全来自实际驾驶场景。由于模拟复杂的驾驶场景搜索空间过于庞大，我们提议利用强大的扩散模型构建全面的环境表示。此外，我们提出了两个阶段的训练管道。在初始阶段，DriveDreamer获得了深入的结构化交通限制的理解，而在后续阶段，它学习了预测未来状态的能力。我们实现了DriveDreamer在具有挑战性的 nuScenes 测试benchmark上，并进行了广泛的实验，证明了DriveDreamer可以准确、可控地生成符合实际交通场景的视频，同时也可以生成合理、可行的驾驶策略。此外，DriveDreamer还开启了交互和实际应用的可能性。
</details></li>
</ul>
<hr>
<h2 id="Towards-Self-Adaptive-Pseudo-Label-Filtering-for-Semi-Supervised-Learning"><a href="#Towards-Self-Adaptive-Pseudo-Label-Filtering-for-Semi-Supervised-Learning" class="headerlink" title="Towards Self-Adaptive Pseudo-Label Filtering for Semi-Supervised Learning"></a>Towards Self-Adaptive Pseudo-Label Filtering for Semi-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09774">http://arxiv.org/abs/2309.09774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Zhu, Zhanghan Ke, Rynson Lau</li>
<li>for: 提高 semi-supervised learning（SSL）方法中的训练效果，特别在标注数据很稀缺的情况下。</li>
<li>methods: 利用自适应 Pseudo-Label Filter（SPF），自动根据模型的演化来过滤噪声 Pseudo 标签。SPF 使用在线混合模型，对每个 Pseudo 标签样本进行权重，以考虑该样本是否正确的可信度分布。与传统手工设置的筛选器不同，SPF 随着深度神经网络的演化而自动调整。</li>
<li>results: 对 existing SSL 方法进行 incorporating SPF，可以提高 SSL 的训练效果，特别在标注数据很稀缺的情况下。<details>
<summary>Abstract</summary>
Recent semi-supervised learning (SSL) methods typically include a filtering strategy to improve the quality of pseudo labels. However, these filtering strategies are usually hand-crafted and do not change as the model is updated, resulting in a lot of correct pseudo labels being discarded and incorrect pseudo labels being selected during the training process. In this work, we observe that the distribution gap between the confidence values of correct and incorrect pseudo labels emerges at the very beginning of the training, which can be utilized to filter pseudo labels. Based on this observation, we propose a Self-Adaptive Pseudo-Label Filter (SPF), which automatically filters noise in pseudo labels in accordance with model evolvement by modeling the confidence distribution throughout the training process. Specifically, with an online mixture model, we weight each pseudo-labeled sample by the posterior of it being correct, which takes into consideration the confidence distribution at that time. Unlike previous handcrafted filters, our SPF evolves together with the deep neural network without manual tuning. Extensive experiments demonstrate that incorporating SPF into the existing SSL methods can help improve the performance of SSL, especially when the labeled data is extremely scarce.
</details>
<details>
<summary>摘要</summary>
现代 semi-supervised learning（SSL）方法通常包括一种筛选策略以提高 Pseudo 标签质量。然而，这些筛选策略通常是手工制定的，而且不会随模型更新而变化，导致训练过程中 Correct  Pseudo 标签被抛弃，而 Incorrect  Pseudo 标签被选择。在这项工作中，我们发现 Pseudo 标签的准确率分布 gap 在训练的开始阶段就已经出现，这可以被利用来筛选 Pseudo 标签。基于这个观察，我们提出了一种自适应 Pseudo-Label 筛选器（SPF），它可以自动根据模型的演化来筛选 Pseudo 标签中的噪声。具体来说，我们使用在线混合模型，对每个 Pseudo-labeled 样本进行重新权重，以考虑该样本在训练过程中的准确率分布。与前一些手工制定的筛选器不同，我们的 SPF 会随着深度神经网络的更新而进行自适应调整，无需人工调整。广泛的实验表明，将 SPF integrated into 现有的 SSL 方法可以提高 SSL 的性能，特别是当有限的标注数据时。
</details></li>
</ul>
<hr>
<h2 id="Semantically-Redundant-Training-Data-Removal-and-Deep-Model-Classification-Performance-A-Study-with-Chest-X-rays"><a href="#Semantically-Redundant-Training-Data-Removal-and-Deep-Model-Classification-Performance-A-Study-with-Chest-X-rays" class="headerlink" title="Semantically Redundant Training Data Removal and Deep Model Classification Performance: A Study with Chest X-rays"></a>Semantically Redundant Training Data Removal and Deep Model Classification Performance: A Study with Chest X-rays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09773">http://arxiv.org/abs/2309.09773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sivaramakrishnan Rajaraman, Ghada Zamzmi, Feng Yang, Zhaohui Liang, Zhiyun Xue, Sameer Antani</li>
<li>for: 这篇论文旨在提出一种基于信息内容的训练样本选择方法，以提高深度学习（Deep Learning）模型的性能和泛化能力。</li>
<li>methods: 方法基于概率理论的信息内容分析，以决定训练样本中的semantic redundancy层次，并将其去除以提高模型的性能。</li>
<li>results: 这篇论文的实验结果显示，使用了这种 entropy-based sample scoring approach的模型在内部测试和外部测试中，均比使用了全部训练样本的模型表现更好，并且显示了更高的准确率和更好的泛化能力。<details>
<summary>Abstract</summary>
Deep learning (DL) has demonstrated its innate capacity to independently learn hierarchical features from complex and multi-dimensional data. A common understanding is that its performance scales up with the amount of training data. Another data attribute is the inherent variety. It follows, therefore, that semantic redundancy, which is the presence of similar or repetitive information, would tend to lower performance and limit generalizability to unseen data. In medical imaging data, semantic redundancy can occur due to the presence of multiple images that have highly similar presentations for the disease of interest. Further, the common use of augmentation methods to generate variety in DL training may be limiting performance when applied to semantically redundant data. We propose an entropy-based sample scoring approach to identify and remove semantically redundant training data. We demonstrate using the publicly available NIH chest X-ray dataset that the model trained on the resulting informative subset of training data significantly outperforms the model trained on the full training set, during both internal (recall: 0.7164 vs 0.6597, p<0.05) and external testing (recall: 0.3185 vs 0.2589, p<0.05). Our findings emphasize the importance of information-oriented training sample selection as opposed to the conventional practice of using all available training data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Localization-Guided-Track-A-Deep-Association-Multi-Object-Tracking-Framework-Based-on-Localization-Confidence-of-Detections"><a href="#Localization-Guided-Track-A-Deep-Association-Multi-Object-Tracking-Framework-Based-on-Localization-Confidence-of-Detections" class="headerlink" title="Localization-Guided Track: A Deep Association Multi-Object Tracking Framework Based on Localization Confidence of Detections"></a>Localization-Guided Track: A Deep Association Multi-Object Tracking Framework Based on Localization Confidence of Detections</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09765">http://arxiv.org/abs/2309.09765</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mengting2023/lg-track">https://github.com/mengting2023/lg-track</a></li>
<li>paper_authors: Ting Meng, Chunyun Fu, Mingguang Huang, Xiyang Wang, Jiawei He, Tao Huang, Wankai Shi</li>
<li>for: 提高多目标追踪（MOT）中的精确性和可靠性，特别是在低光度和高背景噪声的情况下。</li>
<li>methods: 本研究提出了一种基于探测检测器的Localization-Guided Track（LG-Track）方法，首次在MOT中使用检测框的地理位置信任度，并考虑检测框的清晰度和地理位置准确性，同时设计了一种有效的深度匹配机制。</li>
<li>results: 对MOT17和MOT20 dataset进行了广泛的实验，结果显示，相比之前的状态值追踪方法，LG-Track方法在精度和可靠性方面具有明显的优势。<details>
<summary>Abstract</summary>
In currently available literature, no tracking-by-detection (TBD) paradigm-based tracking method has considered the localization confidence of detection boxes. In most TBD-based methods, it is considered that objects of low detection confidence are highly occluded and thus it is a normal practice to directly disregard such objects or to reduce their priority in matching. In addition, appearance similarity is not a factor to consider for matching these objects. However, in terms of the detection confidence fusing classification and localization, objects of low detection confidence may have inaccurate localization but clear appearance; similarly, objects of high detection confidence may have inaccurate localization or unclear appearance; yet these objects are not further classified. In view of these issues, we propose Localization-Guided Track (LG-Track). Firstly, localization confidence is applied in MOT for the first time, with appearance clarity and localization accuracy of detection boxes taken into account, and an effective deep association mechanism is designed; secondly, based on the classification confidence and localization confidence, a more appropriate cost matrix can be selected and used; finally, extensive experiments have been conducted on MOT17 and MOT20 datasets. The results show that our proposed method outperforms the compared state-of-art tracking methods. For the benefit of the community, our code has been made publicly at https://github.com/mengting2023/LG-Track.
</details>
<details>
<summary>摘要</summary>
现有 literatura 中的 tracking-by-detection (TBD) 方法都没有考虑检测框的地方确度。大多数 TBD 基本方法认为低检测确度的对象都是高度遮挡的，因此直接忽略这些对象或减少它们的匹配优先级。此外，外观相似性不是匹配这些对象的因素。但是，在检测确度融合分类和地方确度方面，低检测确度的对象可能有不准确的地方位置，但是清晰的外观；相反，高检测确度的对象可能有不准确的地方位置或晦涩的外观，却不进一步分类。为了解决这些问题，我们提出了地方确度引导的跟踪方法（LG-Track）。首先，我们在 MOT 中首次应用了地方确度，考虑检测框的清晰度和地方位置准确性，并设计了一种有效的深度关联机制；其次，根据分类确度和地方确度，选择更适合的成本矩阵；最后，我们在 MOT17 和 MOT20 数据集上进行了广泛的实验。结果显示，我们的提出方法比对比的国际先进跟踪方法更高效。为了为社区帮助，我们的代码已经公开在 GitHub 上，可以在 https://github.com/mengting2023/LG-Track 中找到。
</details></li>
</ul>
<hr>
<h2 id="Application-driven-Validation-of-Posteriors-in-Inverse-Problems"><a href="#Application-driven-Validation-of-Posteriors-in-Inverse-Problems" class="headerlink" title="Application-driven Validation of Posteriors in Inverse Problems"></a>Application-driven Validation of Posteriors in Inverse Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09764">http://arxiv.org/abs/2309.09764</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim J. Adler, Jan-Hinrich Nölke, Annika Reinke, Minu Dietlinde Tizabi, Sebastian Gruber, Dasha Trofimova, Lynton Ardizzone, Paul F. Jaeger, Florian Buettner, Ullrich Köthe, Lena Maier-Hein</li>
<li>for: 这篇论文是为了解决 inverse problems 中的多可能解问题，尤其是 deep learning-based 解决方案无法处理多个可能的解。</li>
<li>methods: 本论文使用 conditional Diffusion Models 和 Invertible Neural Networks 等 posterior-based 方法，但这些方法的译法受到了 validation 的限制，即不具备适当的验证方法。</li>
<li>results: 本论文提出了首个实用应用验证的框架，将 object detection 领域中的验证原则应用到 inverse problems，以提供更好的验证方法。这个框架在 synthetic toy example 和两个医疗影像应用中（包括手术pose estimation 和功能组织parametrization）均展示了优越的表现。<details>
<summary>Abstract</summary>
Current deep learning-based solutions for image analysis tasks are commonly incapable of handling problems to which multiple different plausible solutions exist. In response, posterior-based methods such as conditional Diffusion Models and Invertible Neural Networks have emerged; however, their translation is hampered by a lack of research on adequate validation. In other words, the way progress is measured often does not reflect the needs of the driving practical application. Closing this gap in the literature, we present the first systematic framework for the application-driven validation of posterior-based methods in inverse problems. As a methodological novelty, it adopts key principles from the field of object detection validation, which has a long history of addressing the question of how to locate and match multiple object instances in an image. Treating modes as instances enables us to perform mode-centric validation, using well-interpretable metrics from the application perspective. We demonstrate the value of our framework through instantiations for a synthetic toy example and two medical vision use cases: pose estimation in surgery and imaging-based quantification of functional tissue parameters for diagnostics. Our framework offers key advantages over common approaches to posterior validation in all three examples and could thus revolutionize performance assessment in inverse problems.
</details>
<details>
<summary>摘要</summary>
当前的深度学习基于解决方案 frequently 无法处理具有多个可能的解决方案的问题。作为回应， posterior-based 方法如 Conditional Diffusion Models 和 Invertible Neural Networks 出现了，但它们的翻译受到了 validate 的研究不够的限制。即，进步的度量方法经常不能反映驱动实际应用的需求。在文献中填补这个差距，我们提出了首个应用驱动验证的 posterior-based 方法框架。作为方法学新颖之处，它采用了对象检测验证的关键原则，该领域已经有长期地解决了如何在图像中检测和匹配多个对象实例的问题。将模式视为实例，我们可以进行模式心智验证，使用应用视角可解释的 метриク。我们通过对synthetic 示例和两个医疗视觉应用：urger 和功能组织参数的成像诊断中的 pose 估计进行实例化，以示出我们的框架的优势。我们的框架可以在三个例子中超越常见的验证方法，因此可能将成像问题中的表现评估革命化。
</details></li>
</ul>
<hr>
<h2 id="Privileged-to-Predicted-Towards-Sensorimotor-Reinforcement-Learning-for-Urban-Driving"><a href="#Privileged-to-Predicted-Towards-Sensorimotor-Reinforcement-Learning-for-Urban-Driving" class="headerlink" title="Privileged to Predicted: Towards Sensorimotor Reinforcement Learning for Urban Driving"></a>Privileged to Predicted: Towards Sensorimotor Reinforcement Learning for Urban Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09756">http://arxiv.org/abs/2309.09756</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ege Onat Özsüer, Barış Akgün, Fatma Güney</li>
<li>for: 本研究旨在探讨RL算法在自驾乘用车道上的潜力，以及如何 bridging RL算法和感知学习算法之间的差距。</li>
<li>methods: 本研究使用视觉深度学习模型来近似特权表示，并提出了解决方案来逐渐发展不具备特权表示的RL算法。</li>
<li>results: 通过在CARLA simulate环境进行严格评估，本研究指出了特权表示在RL算法中的重要性，并释放了未解决的挑战。<details>
<summary>Abstract</summary>
Reinforcement Learning (RL) has the potential to surpass human performance in driving without needing any expert supervision. Despite its promise, the state-of-the-art in sensorimotor self-driving is dominated by imitation learning methods due to the inherent shortcomings of RL algorithms. Nonetheless, RL agents are able to discover highly successful policies when provided with privileged ground truth representations of the environment. In this work, we investigate what separates privileged RL agents from sensorimotor agents for urban driving in order to bridge the gap between the two. We propose vision-based deep learning models to approximate the privileged representations from sensor data. In particular, we identify aspects of state representation that are crucial for the success of the RL agent such as desired route generation and stop zone prediction, and propose solutions to gradually develop less privileged RL agents. We also observe that bird's-eye-view models trained on offline datasets do not generalize to online RL training due to distribution mismatch. Through rigorous evaluation on the CARLA simulation environment, we shed light on the significance of the state representations in RL for autonomous driving and point to unresolved challenges for future research.
</details>
<details>
<summary>摘要</summary>
自适应学习（RL）有可能超越人类在驾驶中的表现，无需专业指导。然而，当前的状态报告中的自适应学习方法占据了主导地位，这是因为RL算法的内在缺陷。然而，RL代理人可以通过精准的环境表示来找到高度成功的策略。在这项工作中，我们调查了隐私RL代理人和感知动作代理人之间的差异，以便 bridging这两者之间的差距。我们提出了基于视觉深度学习模型来近似隐私表示的方法，并特别注意了状态表示中的关键方面，如愿望路径生成和停车区预测。我们还观察到，基于离线数据集训练的鸟瞰视图模型在在线RL训练中并不能总是generalizable，这是因为分布匹配问题。通过在CARLA simulate环境中的严格评估，我们把关于RL在自动驾驶中的状态表示的重要性和未解决的挑战指出来。
</details></li>
</ul>
<hr>
<h2 id="Drawing-the-Same-Bounding-Box-Twice-Coping-Noisy-Annotations-in-Object-Detection-with-Repeated-Labels"><a href="#Drawing-the-Same-Bounding-Box-Twice-Coping-Noisy-Annotations-in-Object-Detection-with-Repeated-Labels" class="headerlink" title="Drawing the Same Bounding Box Twice? Coping Noisy Annotations in Object Detection with Repeated Labels"></a>Drawing the Same Bounding Box Twice? Coping Noisy Annotations in Object Detection with Repeated Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09742">http://arxiv.org/abs/2309.09742</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/madave94/gtiod">https://github.com/madave94/gtiod</a></li>
<li>paper_authors: David Tschirschwitz, Christian Benz, Morris Florek, Henrik Norderhus, Benno Stein, Volker Rodehorst<br>for: 这个研究旨在提高机器学习系统的可靠性，通过确保标签的准确性和可用性。methods: 本研究提出了一个新的地方化算法，可以将多个标签ger的结果组合成更好的标签。results: 实验结果显示，本算法在测试标签中表现出色，并且在训练过程中超过了噪音标签训练和重新标签的Weighted Boxes Fusion（WBF）。研究显示，重复标签带来的好处需要在特定的数据集和标签配置下出现，并且取决于标签ger的一致性、标签ger的数量和标签预算。<details>
<summary>Abstract</summary>
The reliability of supervised machine learning systems depends on the accuracy and availability of ground truth labels. However, the process of human annotation, being prone to error, introduces the potential for noisy labels, which can impede the practicality of these systems. While training with noisy labels is a significant consideration, the reliability of test data is also crucial to ascertain the dependability of the results. A common approach to addressing this issue is repeated labeling, where multiple annotators label the same example, and their labels are combined to provide a better estimate of the true label. In this paper, we propose a novel localization algorithm that adapts well-established ground truth estimation methods for object detection and instance segmentation tasks. The key innovation of our method lies in its ability to transform combined localization and classification tasks into classification-only problems, thus enabling the application of techniques such as Expectation-Maximization (EM) or Majority Voting (MJV). Although our main focus is the aggregation of unique ground truth for test data, our algorithm also shows superior performance during training on the TexBiG dataset, surpassing both noisy label training and label aggregation using Weighted Boxes Fusion (WBF). Our experiments indicate that the benefits of repeated labels emerge under specific dataset and annotation configurations. The key factors appear to be (1) dataset complexity, the (2) annotator consistency, and (3) the given annotation budget constraints.
</details>
<details>
<summary>摘要</summary>
超级vised机器学习系统的可靠性取决于标签的准确性和可用性。然而，人工标注过程容易出现干扰，导致标签噪声，这可能会降低实际应用的实用性。虽然训练噪声标签是一个重要的考虑因素，但测试数据的可靠性也是很重要的，以确定结果的可靠性。在这篇论文中，我们提出了一种新的地方化算法，可以将地方化和分类任务转化为分类任务，从而应用Expectation-Maximization（EM）或 Majority Voting（MJV）等技术。我们的主要关注点是测试数据的唯一标签的聚合，但我们的算法还在TexBiG dataset上进行训练中表现出色，超过了噪声标签训练和Weighted Boxes Fusion（WBF）的标准。我们的实验表明，重复标注的优点在某些 dataset 和注解配置下出现。关键因素包括（1）数据复杂度，（2）注解者一致性，以及（3）注解预算的约束。
</details></li>
</ul>
<hr>
<h2 id="Improving-Neural-Indoor-Surface-Reconstruction-with-Mask-Guided-Adaptive-Consistency-Constraints"><a href="#Improving-Neural-Indoor-Surface-Reconstruction-with-Mask-Guided-Adaptive-Consistency-Constraints" class="headerlink" title="Improving Neural Indoor Surface Reconstruction with Mask-Guided Adaptive Consistency Constraints"></a>Improving Neural Indoor Surface Reconstruction with Mask-Guided Adaptive Consistency Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09739">http://arxiv.org/abs/2309.09739</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyi Yu, Liqin Lu, Jintao Rong, Guangkai Xu, Linlin Ou</li>
<li>for:  scene reconstruction from 2D images</li>
<li>methods: neural implicit surface, data-driven pre-trained geometric cues, two-stage training process, decoupling view-dependent and view-independent colors, two novel consistency constraints</li>
<li>results: high-quality scene reconstruction with rich geometric details, reducing interference from prior estimation errors<details>
<summary>Abstract</summary>
3D scene reconstruction from 2D images has been a long-standing task. Instead of estimating per-frame depth maps and fusing them in 3D, recent research leverages the neural implicit surface as a unified representation for 3D reconstruction. Equipped with data-driven pre-trained geometric cues, these methods have demonstrated promising performance. However, inaccurate prior estimation, which is usually inevitable, can lead to suboptimal reconstruction quality, particularly in some geometrically complex regions. In this paper, we propose a two-stage training process, decouple view-dependent and view-independent colors, and leverage two novel consistency constraints to enhance detail reconstruction performance without requiring extra priors. Additionally, we introduce an essential mask scheme to adaptively influence the selection of supervision constraints, thereby improving performance in a self-supervised paradigm. Experiments on synthetic and real-world datasets show the capability of reducing the interference from prior estimation errors and achieving high-quality scene reconstruction with rich geometric details.
</details>
<details>
<summary>摘要</summary>
三维场景重建从二维图像中进行了长期的任务。而不是每帧估计深度图并将其在3D中进行融合，当前的研究利用神经隐式表面作为一个统一表示方式进行3D重建。具有数据驱动的预训练准则的方法在实际中表现出了可期望的性能。然而，不准确的先前估计，通常是不可避免的，可能会导致部分复杂的地形区域中的重建质量下降。在本文中，我们提出了两个阶段的训练过程，分离视角依赖和视角独立的颜色，并利用两种新的一致性约束来增强细节重建性能，无需额外的准则。此外，我们还引入了一种重要的面 Maske scheme，以适应性地选择监督约束，从而在自动监督方式下提高表现。实验表明，可以减少先前估计错误的干扰，并实现高质量的场景重建，具有丰富的地形细节。
</details></li>
</ul>
<hr>
<h2 id="Scribble-based-3D-Multiple-Abdominal-Organ-Segmentation-via-Triple-branch-Multi-dilated-Network-with-Pixel-and-Class-wise-Consistency"><a href="#Scribble-based-3D-Multiple-Abdominal-Organ-Segmentation-via-Triple-branch-Multi-dilated-Network-with-Pixel-and-Class-wise-Consistency" class="headerlink" title="Scribble-based 3D Multiple Abdominal Organ Segmentation via Triple-branch Multi-dilated Network with Pixel- and Class-wise Consistency"></a>Scribble-based 3D Multiple Abdominal Organ Segmentation via Triple-branch Multi-dilated Network with Pixel- and Class-wise Consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09730">http://arxiv.org/abs/2309.09730</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meng Han, Xiangde Luo, Wenjun Liao, Shichuan Zhang, Shaoting Zhang, Guotai Wang</li>
<li>for: 这个研究旨在提高Multi-organ segmentation在腹部Computed Tomography（CT）图像中的精度，并且降低需要大量时间和劳动力的annotations的需求。</li>
<li>methods: 我们提出了一个 novel的3D框架，使用了两种一致性条件来进行scribble-supervised多个腹部器官分类。</li>
<li>results: 我们的方法在公共的WORD dataset上进行实验，与五种现有的scribble-supervised方法进行比较，结果显示我们的方法的精度高于其他方法。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Multi-organ segmentation in abdominal Computed Tomography (CT) images is of great importance for diagnosis of abdominal lesions and subsequent treatment planning. Though deep learning based methods have attained high performance, they rely heavily on large-scale pixel-level annotations that are time-consuming and labor-intensive to obtain. Due to its low dependency on annotation, weakly supervised segmentation has attracted great attention. However, there is still a large performance gap between current weakly-supervised methods and fully supervised learning, leaving room for exploration. In this work, we propose a novel 3D framework with two consistency constraints for scribble-supervised multiple abdominal organ segmentation from CT. Specifically, we employ a Triple-branch multi-Dilated network (TDNet) with one encoder and three decoders using different dilation rates to capture features from different receptive fields that are complementary to each other to generate high-quality soft pseudo labels. For more stable unsupervised learning, we use voxel-wise uncertainty to rectify the soft pseudo labels and then supervise the outputs of each decoder. To further regularize the network, class relationship information is exploited by encouraging the generated class affinity matrices to be consistent across different decoders under multi-view projection. Experiments on the public WORD dataset show that our method outperforms five existing scribble-supervised methods.
</details>
<details>
<summary>摘要</summary>
多器官分割在腹部 computed Tomography（CT）图像中非常重要，用于腹部肿瘤诊断和后续治疗规划。虽然深度学习基本方法已经达到了高性能，但它们依赖于大规模的像素级别标注，这些标注是时间consuming和劳动密集的获得的。由于其低依赖于标注，弱监督分割吸引了广泛的关注。然而，目前的弱监督方法和完全监督学习之间还存在一定的性能差距，留下了更多的探索空间。在这项工作中，我们提出了一种新的3D框架，包括两种一致性约束 для多个腹部器官分割从CT。具体来说，我们使用一个Triple-branch multi-Dilated网络（TDNet），其中一个Encoder和三个Decoder使用不同的扩散率来捕捉不同的感受场，以生成高质量的软 Pseudo标签。为了更稳定无监督学习，我们使用 voxel-wise 不确定性来修正软 Pseudo标签，然后对每个Decoder的输出进行监督。此外，我们利用类关系信息，通过在多视图投影下强制生成的类征相互关系矩阵保持一致，以更加稳定地训练网络。实验表明，我们的方法在公共的 WORD 数据集上比五种现有的scribble-supervised方法表现出色。
</details></li>
</ul>
<hr>
<h2 id="Robust-Geometry-Preserving-Depth-Estimation-Using-Differentiable-Rendering"><a href="#Robust-Geometry-Preserving-Depth-Estimation-Using-Differentiable-Rendering" class="headerlink" title="Robust Geometry-Preserving Depth Estimation Using Differentiable Rendering"></a>Robust Geometry-Preserving Depth Estimation Using Differentiable Rendering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09724">http://arxiv.org/abs/2309.09724</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chi Zhang, Wei Yin, Gang Yu, Zhibin Wang, Tao Chen, Bin Fu, Joey Tianyi Zhou, Chunhua Shen</li>
<li>For: 本研究探讨了单眼深度估计结果中的3D场景结构重建挑战。* Methods: 我们提出了一个学习框架，让模型透过没有额外数据或标签的方式估计具有对称性的深度，以获得实际的3D结构。* Results: 我们的框架在许多benchmark数据集上与现有的state-of-the-art方法进行比较，发现其具有更好的普遍化能力，并且可以自动从未标签的图像中获得域对称的扩展级数和移动。<details>
<summary>Abstract</summary>
In this study, we address the challenge of 3D scene structure recovery from monocular depth estimation. While traditional depth estimation methods leverage labeled datasets to directly predict absolute depth, recent advancements advocate for mix-dataset training, enhancing generalization across diverse scenes. However, such mixed dataset training yields depth predictions only up to an unknown scale and shift, hindering accurate 3D reconstructions. Existing solutions necessitate extra 3D datasets or geometry-complete depth annotations, constraints that limit their versatility. In this paper, we propose a learning framework that trains models to predict geometry-preserving depth without requiring extra data or annotations. To produce realistic 3D structures, we render novel views of the reconstructed scenes and design loss functions to promote depth estimation consistency across different views. Comprehensive experiments underscore our framework's superior generalization capabilities, surpassing existing state-of-the-art methods on several benchmark datasets without leveraging extra training information. Moreover, our innovative loss functions empower the model to autonomously recover domain-specific scale-and-shift coefficients using solely unlabeled images.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们面临着从单视深度估计中提取3D场景结构的挑战。传统的深度估计方法利用标注数据直接预测精确的深度，而 latest advancements advocate for mix-dataset training，以提高不同场景的泛化性。然而，混合集合训练产生的深度预测只有一个未知的尺度和移动，使得准确的3D重建受阻。现有的解决方案需要额外的3D数据或准确的深度约束，这些限制了它们的灵活性。在这篇论文中，我们提出了一种学习框架，该框架可以不需要额外的数据或约束，train模型以预测保持geometry的深度。为了生成真实的3D结构，我们使用render novel views of the reconstructed scenes，并设计了loss函数来促进深度估计在不同视图之间的一致性。我们的实验表明，我们的框架在多个benchmark datasets上superior generalization capabilities，不使用额外的训练信息。此外，我们的创新的loss函数使得模型可以自动地从单个未标注图像中恢复域pecific的scale-and-shift値。
</details></li>
</ul>
<hr>
<h2 id="Traffic-Scene-Similarity-a-Graph-based-Contrastive-Learning-Approach"><a href="#Traffic-Scene-Similarity-a-Graph-based-Contrastive-Learning-Approach" class="headerlink" title="Traffic Scene Similarity: a Graph-based Contrastive Learning Approach"></a>Traffic Scene Similarity: a Graph-based Contrastive Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09720">http://arxiv.org/abs/2309.09720</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maximilian Zipfl, Moritz Jarosch, J. Marius Zöllner</li>
<li>for: 提高高度自动驾驶车辆的验证和 homologation 难度，以便更广泛地推广高度自动驾驶车辆。</li>
<li>methods: 使用场景基本特征来构建场景嵌入空间，并通过图表示法实现场景之间的连续映射。</li>
<li>results: 可以通过基于场景特征的嵌入空间来快速标识相似的场景，从而减少无关的测试运行。<details>
<summary>Abstract</summary>
Ensuring validation for highly automated driving poses significant obstacles to the widespread adoption of highly automated vehicles. Scenario-based testing offers a potential solution by reducing the homologation effort required for these systems. However, a crucial prerequisite, yet unresolved, is the definition and reduction of the test space to a finite number of scenarios. To tackle this challenge, we propose an extension to a contrastive learning approach utilizing graphs to construct a meaningful embedding space. Our approach demonstrates the continuous mapping of scenes using scene-specific features and the formation of thematically similar clusters based on the resulting embeddings. Based on the found clusters, similar scenes could be identified in the subsequent test process, which can lead to a reduction in redundant test runs.
</details>
<details>
<summary>摘要</summary>
高度自动驾驶的验证 pose significant challenges to its widespread adoption. scenario-based testing offers a potential solution by reducing the homologation effort required for these systems. However, a crucial prerequisite, yet unresolved, is the definition and reduction of the test space to a finite number of scenarios. To tackle this challenge, we propose an extension to a contrastive learning approach utilizing graphs to construct a meaningful embedding space. Our approach demonstrates the continuous mapping of scenes using scene-specific features and the formation of thematically similar clusters based on the resulting embeddings. Based on the found clusters, similar scenes could be identified in the subsequent test process, which can lead to a reduction in redundant test runs.Here's the word-for-word translation:高度自动驾驶的验证呈现出了广泛采用的障碍。场景基本测试可以降低高度自动驾驶系统的验证成本。然而，需要解决的关键前提是定义和减少测试空间到有限数量的场景。为此，我们提出了一种基于图的对比学习方法，通过图构建有意义的嵌入空间。我们的方法示出了使用场景特有的特征进行场景连续映射，并基于结果嵌入的场景类型进行分类。在后续测试过程中，可以通过对类似场景进行相似性比较，减少无关的测试运行。
</details></li>
</ul>
<hr>
<h2 id="CATR-Combinatorial-Dependence-Audio-Queried-Transformer-for-Audio-Visual-Video-Segmentation"><a href="#CATR-Combinatorial-Dependence-Audio-Queried-Transformer-for-Audio-Visual-Video-Segmentation" class="headerlink" title="CATR: Combinatorial-Dependence Audio-Queried Transformer for Audio-Visual Video Segmentation"></a>CATR: Combinatorial-Dependence Audio-Queried Transformer for Audio-Visual Video Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09709">http://arxiv.org/abs/2309.09709</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aspirinone/catr.github.io">https://github.com/aspirinone/catr.github.io</a></li>
<li>paper_authors: Kexin Li, Zongxin Yang, Lei Chen, Yi Yang, Jun Xiao</li>
<li>for: 这个论文targets audio-visual video segmentation, aiming to generate pixel-level maps of sound-producing objects within image frames, and ensure the maps faithfully adhere to the given audio.</li>
<li>methods: 该方法使用了一种叫做”decoupled audio-video transformer”的新方法，将音频和视频特征从它们的各自的时间和空间维度 combine，捕捉到它们的共同依赖关系。同时，通过设计一种块，可以在堆叠的过程中， Capture audio-visual细腻的 combinatorial-dependence in a memory-efficient manner.</li>
<li>results: 实验结果表明，我们的方法可以达到所有三个数据集的新的最佳性能，使用两种骨干。 codes are available at \url{<a target="_blank" rel="noopener" href="https://github.com/aspirinone/CATR.github.io%7D">https://github.com/aspirinone/CATR.github.io}</a><details>
<summary>Abstract</summary>
Audio-visual video segmentation~(AVVS) aims to generate pixel-level maps of sound-producing objects within image frames and ensure the maps faithfully adhere to the given audio, such as identifying and segmenting a singing person in a video. However, existing methods exhibit two limitations: 1) they address video temporal features and audio-visual interactive features separately, disregarding the inherent spatial-temporal dependence of combined audio and video, and 2) they inadequately introduce audio constraints and object-level information during the decoding stage, resulting in segmentation outcomes that fail to comply with audio directives. To tackle these issues, we propose a decoupled audio-video transformer that combines audio and video features from their respective temporal and spatial dimensions, capturing their combined dependence. To optimize memory consumption, we design a block, which, when stacked, enables capturing audio-visual fine-grained combinatorial-dependence in a memory-efficient manner. Additionally, we introduce audio-constrained queries during the decoding phase. These queries contain rich object-level information, ensuring the decoded mask adheres to the sounds. Experimental results confirm our approach's effectiveness, with our framework achieving a new SOTA performance on all three datasets using two backbones. The code is available at \url{https://github.com/aspirinone/CATR.github.io}
</details>
<details>
<summary>摘要</summary>
audio-visual video segmentation（AVVS）的目标是生成图像帧中 зву频生成对象的像素级地图，并确保地图与给定的音频保持一致，例如在视频中 identific 和 segmenting 一个唱歌的人。然而，现有的方法具有两个限制：1）它们对视频的时间特征和音频视频互动特征进行分离处理，忽略了合理的空间时间相互依赖性，2）它们在解码阶段不足以引入音频约束和对象级信息，导致分 segmentation 结果不符合音频指令。为解决这些问题，我们提议一种解coupled audio-video transformer，该模型将音频和视频特征从它们的时间和空间维度组合，捕捉 их共同依赖关系。为了优化内存消耗，我们设计了块，当堆叠时，可以在内存fficient manner中捕捉音频视频细致的 combinatorial-dependence。此外，我们在解码阶段引入了音频约束查询，这些查询包含丰富的对象级信息，以确保解码的mask遵循音频指令。实验结果表明，我们的方法有效，我们的框架在三个数据集上 achieved 新的 SOTA 性能，使用两种背景。代码可以在 \url{https://github.com/aspirinone/CATR.github.io} 上获取。
</details></li>
</ul>
<hr>
<h2 id="DGM-DR-Domain-Generalization-with-Mutual-Information-Regularized-Diabetic-Retinopathy-Classification"><a href="#DGM-DR-Domain-Generalization-with-Mutual-Information-Regularized-Diabetic-Retinopathy-Classification" class="headerlink" title="DGM-DR: Domain Generalization with Mutual Information Regularized Diabetic Retinopathy Classification"></a>DGM-DR: Domain Generalization with Mutual Information Regularized Diabetic Retinopathy Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09670">http://arxiv.org/abs/2309.09670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aleksandr Matsun, Dana O. Mohamed, Sharon Chokuwa, Muhammad Ridzuan, Mohammad Yaqub</li>
<li>for: 这篇论文主要关注于如何对医疗影像领域进行领域对错（Domain Generalization, DG），以实现模型在不同来源资料上的普遍化。</li>
<li>methods: 本论文提出了一种基于大型预训练模型的DG方法，通过将模型目标函数转换为内在熵的最大化，以获得具有对错性的医疗影像分类模型。</li>
<li>results: 根据实验结果，本论文的提出方法在公开数据集上实现了5.25%的平均准确率和较低的标准差，并且与之前的州际状况进行比较，显示了较好的一致性和稳定性。<details>
<summary>Abstract</summary>
The domain shift between training and testing data presents a significant challenge for training generalizable deep learning models. As a consequence, the performance of models trained with the independent and identically distributed (i.i.d) assumption deteriorates when deployed in the real world. This problem is exacerbated in the medical imaging context due to variations in data acquisition across clinical centers, medical apparatus, and patients. Domain generalization (DG) aims to address this problem by learning a model that generalizes well to any unseen target domain. Many domain generalization techniques were unsuccessful in learning domain-invariant representations due to the large domain shift. Furthermore, multiple tasks in medical imaging are not yet extensively studied in existing literature when it comes to DG point of view. In this paper, we introduce a DG method that re-establishes the model objective function as a maximization of mutual information with a large pretrained model to the medical imaging field. We re-visit the problem of DG in Diabetic Retinopathy (DR) classification to establish a clear benchmark with a correct model selection strategy and to achieve robust domain-invariant representation for an improved generalization. Moreover, we conduct extensive experiments on public datasets to show that our proposed method consistently outperforms the previous state-of-the-art by a margin of 5.25% in average accuracy and a lower standard deviation. Source code available at https://github.com/BioMedIA-MBZUAI/DGM-DR
</details>
<details>
<summary>摘要</summary>
域名转换问题在训练和测试数据之间存在重大挑战，这会导致训练的深度学习模型在实际世界中表现不佳。这个问题在医疗影像领域更加严重，因为数据收集方式、医疗设备和患者之间存在很大的域名差异。域名泛化（DG）目标是解决这个问题，学习一个可以在未经见过的目标域中表现良好的模型。然而，许多域名泛化技术无法学习域名不变的表示，因为域名差异过大。此外，医疗影像领域中多个任务还没有得到广泛的研究，尤其是从域名泛化的角度来看。在这篇论文中，我们介绍了一种DG方法，通过将模型目标函数改写为与一个大型预训练模型的共同信息最大化来应用到医疗影像领域。我们重新评估了域名泛化在 диабетичеRetinopathy（DR）分类问题上的问题，并在正确的模型选择策略下建立了一个清晰的标准。此外，我们在公共数据集上进行了广泛的实验，并证明了我们的提议方法在前一个状态的平均准确率和标准差上比前一个状态提高5.25%，并且具有较低的标准差。源代码可以在 <https://github.com/BioMedIA-MBZUAI/DGM-DR> 中找到。
</details></li>
</ul>
<hr>
<h2 id="DFormer-Rethinking-RGBD-Representation-Learning-for-Semantic-Segmentation"><a href="#DFormer-Rethinking-RGBD-Representation-Learning-for-Semantic-Segmentation" class="headerlink" title="DFormer: Rethinking RGBD Representation Learning for Semantic Segmentation"></a>DFormer: Rethinking RGBD Representation Learning for Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09668">http://arxiv.org/abs/2309.09668</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/VCIP-RGBD/DFormer">https://github.com/VCIP-RGBD/DFormer</a></li>
<li>paper_authors: Bowen Yin, Xuying Zhang, Zhongyu Li, Li Liu, Ming-Ming Cheng, Qibin Hou</li>
<li>for: 学习RGB-D segmentation任务上的可转移表示。</li>
<li>methods: 包括一系列RGB-D块，这些块专门用于编码RGB和深度信息，并且采用了一种新的建筑块设计。</li>
<li>results: 在两个流行的RGB-D任务上，即RGB-D semantic segmentation和RGB-D焦点检测，与现有方法相比，我们的DFormer achieved new state-of-the-art performance，并且计算成本下降了超过50%。<details>
<summary>Abstract</summary>
We present DFormer, a novel RGB-D pretraining framework to learn transferable representations for RGB-D segmentation tasks. DFormer has two new key innovations: 1) Unlike previous works that aim to encode RGB features,DFormer comprises a sequence of RGB-D blocks, which are tailored for encoding both RGB and depth information through a novel building block design; 2) We pre-train the backbone using image-depth pairs from ImageNet-1K, and thus the DFormer is endowed with the capacity to encode RGB-D representations. It avoids the mismatched encoding of the 3D geometry relationships in depth maps by RGB pre-trained backbones, which widely lies in existing methods but has not been resolved. We fine-tune the pre-trained DFormer on two popular RGB-D tasks, i.e., RGB-D semantic segmentation and RGB-D salient object detection, with a lightweight decoder head. Experimental results show that our DFormer achieves new state-of-the-art performance on these two tasks with less than half of the computational cost of the current best methods on two RGB-D segmentation datasets and five RGB-D saliency datasets. Our code is available at: https://github.com/VCIP-RGBD/DFormer.
</details>
<details>
<summary>摘要</summary>
我们提出了DFormer，一种新的RGB-D预训练框架，用于学习转移性的RGB-D表示。DFormer有两个新额外创新：1）与前期工作不同，DFormer不仅编码RGB特征，而且包含一串RGB-D块，这些块通过一种新的建筑块设计来编码RGB和深度信息; 2）我们在ImageNet-1K中预训练了干燥的脊梁，因此DFormer拥有编码RGB-D表示的能力。它避免了现有方法中RGB预训练背bone对3D几何关系的推断错误，这是现有方法的潜在问题。我们通过一个轻量级的解码头进行了细致的调整。我们的实验结果显示，我们的DFormer在两个流行的RGB-D任务上（RGB-D语义分割和RGB-D焦点检测） achieve了新的状态级表现，并且计算成本只有现有最佳方法的一半。我们的代码可以在https://github.com/VCIP-RGBD/DFormer上找到。
</details></li>
</ul>
<hr>
<h2 id="Unified-Frequency-Assisted-Transformer-Framework-for-Detecting-and-Grounding-Multi-Modal-Manipulation"><a href="#Unified-Frequency-Assisted-Transformer-Framework-for-Detecting-and-Grounding-Multi-Modal-Manipulation" class="headerlink" title="Unified Frequency-Assisted Transformer Framework for Detecting and Grounding Multi-Modal Manipulation"></a>Unified Frequency-Assisted Transformer Framework for Detecting and Grounding Multi-Modal Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09667">http://arxiv.org/abs/2309.09667</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huan Liu, Zichang Tan, Qiang Chen, Yunchao Wei, Yao Zhao, Jingdong Wang<br>for:This paper aims to address the problem of detecting and grounding multi-modal media manipulation (DGM^4), specifically focusing on face forgery and text misinformation.methods:The proposed method, Unified Frequency-Assisted transFormer (UFAFormer), utilizes a combination of the discrete wavelet transform and self-attention mechanisms to capture forgery features in both the image and frequency domains. Additionally, the proposed forgery-aware mutual module is designed to facilitate effective interaction between image and frequency features.results:Experimental results on the DGM^4 dataset demonstrate the superior performance of UFAFormer compared to previous methods, achieving a new benchmark in the field.<details>
<summary>Abstract</summary>
Detecting and grounding multi-modal media manipulation (DGM^4) has become increasingly crucial due to the widespread dissemination of face forgery and text misinformation. In this paper, we present the Unified Frequency-Assisted transFormer framework, named UFAFormer, to address the DGM^4 problem. Unlike previous state-of-the-art methods that solely focus on the image (RGB) domain to describe visual forgery features, we additionally introduce the frequency domain as a complementary viewpoint. By leveraging the discrete wavelet transform, we decompose images into several frequency sub-bands, capturing rich face forgery artifacts. Then, our proposed frequency encoder, incorporating intra-band and inter-band self-attentions, explicitly aggregates forgery features within and across diverse sub-bands. Moreover, to address the semantic conflicts between image and frequency domains, the forgery-aware mutual module is developed to further enable the effective interaction of disparate image and frequency features, resulting in aligned and comprehensive visual forgery representations. Finally, based on visual and textual forgery features, we propose a unified decoder that comprises two symmetric cross-modal interaction modules responsible for gathering modality-specific forgery information, along with a fusing interaction module for aggregation of both modalities. The proposed unified decoder formulates our UFAFormer as a unified framework, ultimately simplifying the overall architecture and facilitating the optimization process. Experimental results on the DGM^4 dataset, containing several perturbations, demonstrate the superior performance of our framework compared to previous methods, setting a new benchmark in the field.
</details>
<details>
<summary>摘要</summary>
Detecting and grounding多modal媒体修改（DGM^4）已成为当前普遍流传的脸写 forgery和文本谣言的检测方法。在这篇论文中，我们提出了一个统一频率帮助 transformer框架，名为UFAFormer，以解决DGM^4问题。与之前的状态lejian方法不同，我们不仅focus于图像（RGB）频谱来描述视觉修改特征，而且还引入频谱频带，以获得更加丰富的脸写特征。通过频谱分解算法，我们将图像分解成多个频谱子带，捕捉到了脸写特征。然后，我们提出的频率编码器，具有内部和外部自我注意力，明确地聚合了修改特征。此外，为了解决图像和频谱频带之间的 semantic conflict，我们开发了一个 forgery-aware mutual module，以便更好地考虑多modal特征之间的协作。最后，我们提出了一个统一的解码器，包括两个对称的交叉模态交互模块，负责收集不同模式的修改信息，以及一个融合交互模块，用于将多modal特征融合。这使得我们的UFAFormer框架成为一个统一的框架，从而简化整个架构，并且优化过程。实验结果表明，我们的方法在DGM^4数据集上表现出了superior performance，并设置了新的benchmark。
</details></li>
</ul>
<hr>
<h2 id="HiT-Building-Mapping-with-Hierarchical-Transformers"><a href="#HiT-Building-Mapping-with-Hierarchical-Transformers" class="headerlink" title="HiT: Building Mapping with Hierarchical Transformers"></a>HiT: Building Mapping with Hierarchical Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09643">http://arxiv.org/abs/2309.09643</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingming Zhang, Qingjie Liu, Yunhong Wang</li>
<li>for: 提高高分辨率 remote sensing 图像中建筑物的自动映射质量</li>
<li>methods: 使用层次转换器（Hierarchical Transformers）和静止卷积神经网络（Convolutional Neural Networks）实现二stage探测架构，并在探测阶段添加 polygon 头，同时输出建筑物 bounding box 和 vector polygon</li>
<li>results: 在 CrowdAI 和 Inria 数据集上进行了 comprehensive  экспериiments，并达到了新的州OF-THE-ART 水平以及 instance segmentation 和 polygon 度量上的优秀表现，同时在复杂场景下也有较好的效果<details>
<summary>Abstract</summary>
Deep learning-based methods have been extensively explored for automatic building mapping from high-resolution remote sensing images over recent years. While most building mapping models produce vector polygons of buildings for geographic and mapping systems, dominant methods typically decompose polygonal building extraction in some sub-problems, including segmentation, polygonization, and regularization, leading to complex inference procedures, low accuracy, and poor generalization. In this paper, we propose a simple and novel building mapping method with Hierarchical Transformers, called HiT, improving polygonal building mapping quality from high-resolution remote sensing images. HiT builds on a two-stage detection architecture by adding a polygon head parallel to classification and bounding box regression heads. HiT simultaneously outputs building bounding boxes and vector polygons, which is fully end-to-end trainable. The polygon head formulates a building polygon as serialized vertices with the bidirectional characteristic, a simple and elegant polygon representation avoiding the start or end vertex hypothesis. Under this new perspective, the polygon head adopts a transformer encoder-decoder architecture to predict serialized vertices supervised by the designed bidirectional polygon loss. Furthermore, a hierarchical attention mechanism combined with convolution operation is introduced in the encoder of the polygon head, providing more geometric structures of building polygons at vertex and edge levels. Comprehensive experiments on two benchmarks (the CrowdAI and Inria datasets) demonstrate that our method achieves a new state-of-the-art in terms of instance segmentation and polygonal metrics compared with state-of-the-art methods. Moreover, qualitative results verify the superiority and effectiveness of our model under complex scenes.
</details>
<details>
<summary>摘要</summary>
深度学习基于方法在高分辨率 remote sensing 图像自动建筑图ematization中得到了广泛的探索。大多数建筑图ematization 模型生成 vector 多边形建筑，供地ографи�映图和 Navigation 系统使用。 dominant 方法通常将建筑图ematization 分解成多个子问题，包括分割、多边形化和正则化，导致复杂的推理过程、低准确率和差的普遍性。在本文中，我们提出了一种简单和新的建筑图ematization 方法，即 HiT，可以从高分辨率 remote sensing 图像中提高多边形建筑质量。HiT 基于二stage 检测架构，其中添加了一个多边形头，并同时输出建筑 bounding box 和 vector 多边形。这种方法是完全可导的。多边形头使用逆向特征来表示建筑多边形，避免了开始或结束多边形的假设。在这个新的视角下，多边形头采用了 transformer Encoder-Decoder 架构来预测序列化多边形，并且采用了自定义的双向多边形损失函数进行超参数化。此外，在多边形头的Encoder中，我们引入了层次注意力机制和 convolution 操作，以提供更多的建筑多边形的 геометри�结构，包括 vertex 和 edge 层次。我们在 CrowdAI 和 Inria 两个标准 datasets 进行了广泛的实验，并证明了我们的方法在实例分割和多边形指标方面达到了新的国际纪录。此外，质量证明了我们模型在复杂场景下的superiority和有效性。
</details></li>
</ul>
<hr>
<h2 id="Holistic-Geometric-Feature-Learning-for-Structured-Reconstruction"><a href="#Holistic-Geometric-Feature-Learning-for-Structured-Reconstruction" class="headerlink" title="Holistic Geometric Feature Learning for Structured Reconstruction"></a>Holistic Geometric Feature Learning for Structured Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09622">http://arxiv.org/abs/2309.09622</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/geo-tell/f-learn">https://github.com/geo-tell/f-learn</a></li>
<li>paper_authors: Ziqiong Lu, Linxi Huan, Qiyuan Ma, Xianwei Zheng</li>
<li>for: 这个论文旨在提高结构重建中的结构意识，即使受到低级特征缺乏全局几何信息的影响。</li>
<li>methods: 作者们提出了一种频域特征学习策略（F-Learn），通过将散布的几何特征汇集到频域中，以便更好地理解结构。</li>
<li>results: 实验表明，F-Learn策略可以有效地引入结构意识到几何原子检测和结构推断中，提高最终结构重建的性能。<details>
<summary>Abstract</summary>
The inference of topological principles is a key problem in structured reconstruction. We observe that wrongly predicted topological relationships are often incurred by the lack of holistic geometry clues in low-level features. Inspired by the fact that massive signals can be compactly described with frequency analysis, we experimentally explore the efficiency and tendency of learning structure geometry in the frequency domain. Accordingly, we propose a frequency-domain feature learning strategy (F-Learn) to fuse scattered geometric fragments holistically for topology-intact structure reasoning. Benefiting from the parsimonious design, the F-Learn strategy can be easily deployed into a deep reconstructor with a lightweight model modification. Experiments demonstrate that the F-Learn strategy can effectively introduce structure awareness into geometric primitive detection and topology inference, bringing significant performance improvement to final structured reconstruction. Code and pre-trained models are available at https://github.com/Geo-Tell/F-Learn.
</details>
<details>
<summary>摘要</summary>
“推理 topological 原则是结构重建中的关键问题。我们观察到低级特征中缺乏整体几何启示，导致 wrongly 预测的 topological 关系。受到大量信号可以使用频率分析 compactly 描述的灵感，我们实验性地探索了 learning 结构几何在频率域的有效性和倾向。因此，我们提出了频率域特征学习策略（F-Learn），将散布的几何碎片合理地汇聚到整体结构理解中。由于简洁的设计，F-Learn 策略可以轻松地整合到深度重建器中，并且只需要修改轻量级模型。实验表明，F-Learn 策略可以有效地带来结构意识到几何基本检测和推理，提高最终结构重建的性能。代码和预训练模型可以在 https://github.com/Geo-Tell/F-Learn 上获取。”
</details></li>
</ul>
<hr>
<h2 id="Collaborative-Three-Stream-Transformers-for-Video-Captioning"><a href="#Collaborative-Three-Stream-Transformers-for-Video-Captioning" class="headerlink" title="Collaborative Three-Stream Transformers for Video Captioning"></a>Collaborative Three-Stream Transformers for Video Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09611">http://arxiv.org/abs/2309.09611</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wanghao14/COST">https://github.com/wanghao14/COST</a></li>
<li>paper_authors: Hao Wang, Libo Zhang, Heng Fan, Tiejian Luo</li>
<li>for: 用于视频captioning任务中的主要组件，即主语、谓语和词语之间的互动。</li>
<li>methods: 提出了一种新的框架，名为COST，来分别模型这三部分，并且使得它们互相补充以获得更好的表示。COST包括三个transformers分支，用于探索视频和文本之间的视觉语言互动，检测到的对象和文本之间的互动，以及动作和文本之间的互动。同时，我们提出了一种相互扩展注意力模块，用于协调这三个分支中模型的互动，以便使得这三个分支可以互相支持，捕捉到不同细致程度的最有力的semantic信息，以便准确地预测caption。</li>
<li>results: 经过大规模的实验，我们发现提出的方法在三个大规模的挑战性数据集，即YouCookII、ActivityNet Captions和MSVD上表现出色，与当前的方法相比，具有较高的准确率。<details>
<summary>Abstract</summary>
As the most critical components in a sentence, subject, predicate and object require special attention in the video captioning task. To implement this idea, we design a novel framework, named COllaborative three-Stream Transformers (COST), to model the three parts separately and complement each other for better representation. Specifically, COST is formed by three branches of transformers to exploit the visual-linguistic interactions of different granularities in spatial-temporal domain between videos and text, detected objects and text, and actions and text. Meanwhile, we propose a cross-granularity attention module to align the interactions modeled by the three branches of transformers, then the three branches of transformers can support each other to exploit the most discriminative semantic information of different granularities for accurate predictions of captions. The whole model is trained in an end-to-end fashion. Extensive experiments conducted on three large-scale challenging datasets, i.e., YouCookII, ActivityNet Captions and MSVD, demonstrate that the proposed method performs favorably against the state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
《 sentence 中的主要组成部分，即主语、谓语和词组，在视频描述任务中需要特别注意。为了实现这个想法，我们设计了一种新的框架，名为协同三流转换器（COST）。COST 框架由三个转换器分支组成，用以模型不同细致程度的视觉语言互动在空间时间域中，包括视频和文本、检测到的对象和文本、和动作和文本之间的互动。此外，我们还提出了跨度量注意模块，用以对三个分支中模型的互动进行协调，以便三个分支可以互相支持，挖掘出最有特征的 semantic 信息，以便准确地预测视频描述。整个模型采用端到端的训练方式。我们在 YouCookII、ActivityNet Captions 和 MSVD 等三个大规模挑战性 dataset 上进行了广泛的实验，结果显示，我们提出的方法与当前最佳方法相比，表现出了良好的性能。》Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="MEDL-U-Uncertainty-aware-3D-Automatic-Annotator-based-on-Evidential-Deep-Learning"><a href="#MEDL-U-Uncertainty-aware-3D-Automatic-Annotator-based-on-Evidential-Deep-Learning" class="headerlink" title="MEDL-U: Uncertainty-aware 3D Automatic Annotator based on Evidential Deep Learning"></a>MEDL-U: Uncertainty-aware 3D Automatic Annotator based on Evidential Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09599">http://arxiv.org/abs/2309.09599</a></li>
<li>repo_url: None</li>
<li>paper_authors: Helbert Paat, Qing Lian, Weilong Yao, Tong Zhang</li>
<li>for: This paper is written for advancing the field of 3D object detection using weakly supervised deep learning methods, and addressing the challenges of pseudo label noise and uncertainty estimation.</li>
<li>methods: The paper proposes an Evidential Deep Learning (EDL) based uncertainty estimation framework called MEDL-U, which generates pseudo labels and quantifies uncertainties for 3D object detection. The framework consists of an uncertainty-aware IoU-based loss, an evidence-aware multi-task loss function, and a post-processing stage for uncertainty refinement.</li>
<li>results: The paper demonstrates that probabilistic detectors trained using the outputs of MEDL-U surpass deterministic detectors trained using outputs from previous 3D annotators on the KITTI val set for all difficulty levels. Additionally, MEDL-U achieves state-of-the-art results on the KITTI official test set compared to existing 3D automatic annotators.<details>
<summary>Abstract</summary>
Advancements in deep learning-based 3D object detection necessitate the availability of large-scale datasets. However, this requirement introduces the challenge of manual annotation, which is often both burdensome and time-consuming. To tackle this issue, the literature has seen the emergence of several weakly supervised frameworks for 3D object detection which can automatically generate pseudo labels for unlabeled data. Nevertheless, these generated pseudo labels contain noise and are not as accurate as those labeled by humans. In this paper, we present the first approach that addresses the inherent ambiguities present in pseudo labels by introducing an Evidential Deep Learning (EDL) based uncertainty estimation framework. Specifically, we propose MEDL-U, an EDL framework based on MTrans, which not only generates pseudo labels but also quantifies the associated uncertainties. However, applying EDL to 3D object detection presents three primary challenges: (1) relatively lower pseudolabel quality in comparison to other autolabelers; (2) excessively high evidential uncertainty estimates; and (3) lack of clear interpretability and effective utilization of uncertainties for downstream tasks. We tackle these issues through the introduction of an uncertainty-aware IoU-based loss, an evidence-aware multi-task loss function, and the implementation of a post-processing stage for uncertainty refinement. Our experimental results demonstrate that probabilistic detectors trained using the outputs of MEDL-U surpass deterministic detectors trained using outputs from previous 3D annotators on the KITTI val set for all difficulty levels. Moreover, MEDL-U achieves state-of-the-art results on the KITTI official test set compared to existing 3D automatic annotators.
</details>
<details>
<summary>摘要</summary>
深度学习基于3D对象检测的进步导致了大规模数据的需求。然而，这会带来人工标注的挑战，它们frequently both burdensome and time-consuming。为解决这个问题，文献中出现了一些弱supervised frameworks for 3D object detection，可以自动生成pseudo labels for unlabeled data。然而，这些生成的pseudo labels含有噪声并不如人工标注的准确性。在这篇论文中，我们提出了首个解决pseudo labels中存在的自然ambiguity的方法，通过引入Evidential Deep Learning（EDL）基于uncertainty estimation framework。 Specifically, we propose MEDL-U， an EDL framework based on MTrans， which not only generates pseudo labels but also quantifies the associated uncertainties。然而，在应用EDL于3D对象检测中存在三个主要挑战：（1）相比其他自动标注器，pseudolabel的质量相对较低；（2）过度的证据uncertainty estimate；和（3）不清晰的解释和下游任务中的有效利用uncertainty。我们通过引入uncertainty-aware IoU-based loss、evidence-aware multi-task loss function和post-processing阶段进行uncertainty refinement来解决这些问题。我们的实验结果表明，基于MEDL-U输出的 probabilistic detector比基于前一代3D自动标注器的deterministic detector在KITTI val set上的所有Difficulty Level上表现出色。此外，MEDL-U在KITTI官方测试集上与现有的3D自动标注器相比， achieve state-of-the-art results。
</details></li>
</ul>
<hr>
<h2 id="Mutual-Information-calibrated-Conformal-Feature-Fusion-for-Uncertainty-Aware-Multimodal-3D-Object-Detection-at-the-Edge"><a href="#Mutual-Information-calibrated-Conformal-Feature-Fusion-for-Uncertainty-Aware-Multimodal-3D-Object-Detection-at-the-Edge" class="headerlink" title="Mutual Information-calibrated Conformal Feature Fusion for Uncertainty-Aware Multimodal 3D Object Detection at the Edge"></a>Mutual Information-calibrated Conformal Feature Fusion for Uncertainty-Aware Multimodal 3D Object Detection at the Edge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09593">http://arxiv.org/abs/2309.09593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex C. Stutts, Danilo Erricolo, Sathya Ravi, Theja Tulabandhula, Amit Ranjan Trivedi</li>
<li>for:  This paper aims to address the gap in uncertainty quantification in 3D object detection for robotics, by integrating conformal inference and information theoretic measures to provide lightweight and Monte Carlo-free uncertainty estimation.</li>
<li>methods:  The proposed method uses a multimodal framework that fuses features from RGB camera and LiDAR sensor data, and leverages normalized mutual information as a modulator to calibrate uncertainty bounds derived from conformal inference.</li>
<li>results:  The simulation results show an inverse correlation between inherent predictive uncertainty and normalized mutual information throughout the model’s training, and the proposed framework demonstrates comparable or better performance in KITTI 3D object detection benchmarks compared to similar methods that are not uncertainty-aware, making it suitable for real-time edge robotics.<details>
<summary>Abstract</summary>
In the expanding landscape of AI-enabled robotics, robust quantification of predictive uncertainties is of great importance. Three-dimensional (3D) object detection, a critical robotics operation, has seen significant advancements; however, the majority of current works focus only on accuracy and ignore uncertainty quantification. Addressing this gap, our novel study integrates the principles of conformal inference (CI) with information theoretic measures to perform lightweight, Monte Carlo-free uncertainty estimation within a multimodal framework. Through a multivariate Gaussian product of the latent variables in a Variational Autoencoder (VAE), features from RGB camera and LiDAR sensor data are fused to improve the prediction accuracy. Normalized mutual information (NMI) is leveraged as a modulator for calibrating uncertainty bounds derived from CI based on a weighted loss function. Our simulation results show an inverse correlation between inherent predictive uncertainty and NMI throughout the model's training. The framework demonstrates comparable or better performance in KITTI 3D object detection benchmarks to similar methods that are not uncertainty-aware, making it suitable for real-time edge robotics.
</details>
<details>
<summary>摘要</summary>
在扩展的人工智能启用机器人领域，准确地量化预测不确定性是非常重要的。三维对象检测，机器人运行中的关键任务，已经取得了 significative 进步，但大多数当前的工作都专注于精度，忽略了不确定性量化。我们的新研究强调了对具有准确性的隐藏变量的拟合推理（CI）的应用，以及信息理论度量来实现轻量级、不含 Monte Carlo 的不确定性估计。通过RGB摄像机和激光雷达感知器数据的融合，通过多元 Normal 分布的 Gaussian 产物来提高预测精度。在权重损失函数中，使用Normalized Mutual Information（NMI）作为调整器，以 derivation uncertainty bound。我们的 simulate 结果显示，预测不确定性与 NMI 之间存在 inverse 相关性，并且在模型训练过程中逐渐下降。该框架在 KITTI 3D 对象检测标准准测试中与其他不具有不确定性感知的方法相比，表现相对或更好，因此适用于实时边缘机器人。
</details></li>
</ul>
<hr>
<h2 id="Multi-Semantic-Fusion-Model-for-Generalized-Zero-Shot-Skeleton-Based-Action-Recognition"><a href="#Multi-Semantic-Fusion-Model-for-Generalized-Zero-Shot-Skeleton-Based-Action-Recognition" class="headerlink" title="Multi-Semantic Fusion Model for Generalized Zero-Shot Skeleton-Based Action Recognition"></a>Multi-Semantic Fusion Model for Generalized Zero-Shot Skeleton-Based Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09592">http://arxiv.org/abs/2309.09592</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/EHZ9NIWI7/MSF-GZSSAR">https://github.com/EHZ9NIWI7/MSF-GZSSAR</a></li>
<li>paper_authors: Ming-Zhe Li, Zhen Jia, Zhang Zhang, Zhanyu Ma, Liang Wang</li>
<li>for: 解决computer vision社区中新的挑战问题——无需任何训练样本的动作识别（GZSSAR）。</li>
<li>methods: 我们提出了一种多 semantics融合（MSF）模型，通过使用两种类别级文本描述（动作描述和运动描述）作为辅助semantic信息，以提高通用skeleton特征的识别性。</li>
<li>results: 与前一代模型进行比较，我们的MSF模型在GZSSAR中表现出色， validate了我们的方法的有效性。<details>
<summary>Abstract</summary>
Generalized zero-shot skeleton-based action recognition (GZSSAR) is a new challenging problem in computer vision community, which requires models to recognize actions without any training samples. Previous studies only utilize the action labels of verb phrases as the semantic prototypes for learning the mapping from skeleton-based actions to a shared semantic space. However, the limited semantic information of action labels restricts the generalization ability of skeleton features for recognizing unseen actions. In order to solve this dilemma, we propose a multi-semantic fusion (MSF) model for improving the performance of GZSSAR, where two kinds of class-level textual descriptions (i.e., action descriptions and motion descriptions), are collected as auxiliary semantic information to enhance the learning efficacy of generalizable skeleton features. Specially, a pre-trained language encoder takes the action descriptions, motion descriptions and original class labels as inputs to obtain rich semantic features for each action class, while a skeleton encoder is implemented to extract skeleton features. Then, a variational autoencoder (VAE) based generative module is performed to learn a cross-modal alignment between skeleton and semantic features. Finally, a classification module is built to recognize the action categories of input samples, where a seen-unseen classification gate is adopted to predict whether the sample comes from seen action classes or not in GZSSAR. The superior performance in comparisons with previous models validates the effectiveness of the proposed MSF model on GZSSAR.
</details>
<details>
<summary>摘要</summary>
新的挑战问题：通用零例基本动作识别（GZSSAR）在计算机视觉社区中引起了广泛关注，需要模型可以识别没有任何训练样本的动作。在先前的研究中，只是使用动作标签的词语短语作为semantic prototype来学习将骨架基本动作映射到共享semantic空间。然而，动作标签的有限 semantic information限制了骨架特征的通用化能力，用于认识未看过的动作。为解决这个困境，我们提出了一种多 semantics融合（MSF）模型，用于提高GZSSAR的性能。在这种模型中，我们收集了两种类型的文本描述（动作描述和运动描述）作为auxiliary semantic information，以增强骨架特征的学习效果。具体来说，一个预训练的语言编码器将动作描述、运动描述和原始类别标签作为输入，以获取每个动作类型的 ric hes semantic features。然后，我们实现了一个骨架编码器来EXTRACT骨架特征。接着，我们使用基于VAE的生成模块来学习骨架和semantic特征之间的cross-modal alignment。最后，我们建立了一个分类模块，用于识别输入样本的动作类别，并采用seen-unseen分类门户来预测样本是否来自seen动作类别。 Comparing with previous models, our MSF model shows superior performance in GZSSAR, validating its effectiveness.
</details></li>
</ul>
<hr>
<h2 id="An-Autonomous-Vision-Based-Algorithm-for-Interplanetary-Navigation"><a href="#An-Autonomous-Vision-Based-Algorithm-for-Interplanetary-Navigation" class="headerlink" title="An Autonomous Vision-Based Algorithm for Interplanetary Navigation"></a>An Autonomous Vision-Based Algorithm for Interplanetary Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09590">http://arxiv.org/abs/2309.09590</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eleonora Andreis, Paolo Panicucci, Francesco Topputo</li>
<li>for: 提出了一种基于全视 Navigation 算法，用于自动化深空探测器的 Navigation 问题。</li>
<li>methods: 使用了一种基于非维度扩展卡尔曼过滤器的状态估计方法，并对图像进行了进一步的优化。</li>
<li>results: 在高精度的地球-火星间INTERPLANETARY TRANSFER上测试了算法的性能，并证明了其适用于深空 Navigation。<details>
<summary>Abstract</summary>
The surge of deep-space probes makes it unsustainable to navigate them with standard radiometric tracking. Self-driving interplanetary satellites represent a solution to this problem. In this work, a full vision-based navigation algorithm is built by combining an orbit determination method with an image processing pipeline suitable for interplanetary transfers of autonomous platforms. To increase the computational efficiency of the algorithm, a non-dimensional extended Kalman filter is selected as state estimator, fed by the positions of the planets extracted from deep-space images. An enhancement of the estimation accuracy is performed by applying an optimal strategy to select the best pair of planets to track. Moreover, a novel analytical measurement model for deep-space navigation is developed providing a first-order approximation of the light-aberration and light-time effects. Algorithm performance is tested on a high-fidelity, Earth--Mars interplanetary transfer, showing the algorithm applicability for deep-space navigation.
</details>
<details>
<summary>摘要</summary>
深空探测器的激增使得标准Radiometric tracking无法实现可持续导航。自驾宇宙卫星代表了一种解决方案。在这项工作中，我们构建了一个完整的视觉导航算法，通过结合轨道确定方法和适用于交通自主平台的图像处理管道来实现。为提高算法的计算效率，我们选择了非维度扩展卡尔曼滤波器作为状态估计器，通过深空图像中找到行星的位置来 aliment 状态估计。此外，我们还开发了一种新的分析测量模型，用于描述深空导航中的光扭和时光效应。我们测试了算法在高精度的地球-火星间交通任务上的性能，并证明了算法的深空导航可行性。
</details></li>
</ul>
<hr>
<h2 id="RIDE-Self-Supervised-Learning-of-Rotation-Equivariant-Keypoint-Detection-and-Invariant-Description-for-Endoscopy"><a href="#RIDE-Self-Supervised-Learning-of-Rotation-Equivariant-Keypoint-Detection-and-Invariant-Description-for-Endoscopy" class="headerlink" title="RIDE: Self-Supervised Learning of Rotation-Equivariant Keypoint Detection and Invariant Description for Endoscopy"></a>RIDE: Self-Supervised Learning of Rotation-Equivariant Keypoint Detection and Invariant Description for Endoscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09563">http://arxiv.org/abs/2309.09563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mert Asim Karaoglu, Viktoria Markova, Nassir Navab, Benjamin Busam, Alexander Ladikos</li>
<li>for: 这篇论文是关于endoscopic video中的锚点检测和描述的研究，以解决endoscopic video中的大规模旋转问题。</li>
<li>methods: 该论文提出了一种基于学习的方法，即RIDE，其中包含了 rotation-equivariant detection和invariant description。RIDE使用了latest advancements in group-equivariant learning，并在自动标注数据集上进行了自动验证。</li>
<li>results: 该论文的实验结果表明，RIDE在surgical tissue tracking和relative pose estimation任务上比之前的学习基于和经典方法更高，并且在大规模旋转情况下保持稳定性。<details>
<summary>Abstract</summary>
Unlike in natural images, in endoscopy there is no clear notion of an up-right camera orientation. Endoscopic videos therefore often contain large rotational motions, which require keypoint detection and description algorithms to be robust to these conditions. While most classical methods achieve rotation-equivariant detection and invariant description by design, many learning-based approaches learn to be robust only up to a certain degree. At the same time learning-based methods under moderate rotations often outperform classical approaches. In order to address this shortcoming, in this paper we propose RIDE, a learning-based method for rotation-equivariant detection and invariant description. Following recent advancements in group-equivariant learning, RIDE models rotation-equivariance implicitly within its architecture. Trained in a self-supervised manner on a large curation of endoscopic images, RIDE requires no manual labeling of training data. We test RIDE in the context of surgical tissue tracking on the SuPeR dataset as well as in the context of relative pose estimation on a repurposed version of the SCARED dataset. In addition we perform explicit studies showing its robustness to large rotations. Our comparison against recent learning-based and classical approaches shows that RIDE sets a new state-of-the-art performance on matching and relative pose estimation tasks and scores competitively on surgical tissue tracking.
</details>
<details>
<summary>摘要</summary>
不同于自然图像，endooscopy中没有明确的正常摄像机 Orientations。因此，endooscopic视频通常包含大的旋转动作，需要键点检测和描述算法具有旋转不敏感性。大多数传统方法通过设计实现旋转对称性和不变性，而许多学习基于方法则只有一定程度的旋转不敏感性。在这种情况下，本文提出了RIDE，一种基于学习的旋转对称检测和描述方法。按照最近的群对称学习的进展，RIDE在其架构中隐式地实现了旋转对称性。通过自动化的自我超vised学习方式，RIDE不需要手动标注训练数据。我们在SuPeR dataset上进行了手术组织跟踪和SCARED dataset上进行了修改的版本的相对pose estimation任务中测试了RIDE。此外，我们还进行了明确的研究，证明RIDE在大旋转下的稳定性。我们与最近的学习基于和传统方法进行了比较，结果显示RIDE在匹配和相对pose estimation任务上设置了新的状态态标准性，并在手术组织跟踪任务上分数竞争。
</details></li>
</ul>
<hr>
<h2 id="Selective-Volume-Mixup-for-Video-Action-Recognition"><a href="#Selective-Volume-Mixup-for-Video-Action-Recognition" class="headerlink" title="Selective Volume Mixup for Video Action Recognition"></a>Selective Volume Mixup for Video Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09534">http://arxiv.org/abs/2309.09534</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Tan, Zhaofan Qiu, Yanbin Hao, Ting Yao, Xiangnan He, Tao Mei</li>
<li>for: 提高深度模型对小规模视频数据的泛化能力</li>
<li>methods: 提出了一种新的视频扩充策略 named Selective Volume Mixup (SV-Mix)，通过选择两个视频中最有用的卷积来实现新的训练视频</li>
<li>results: 在各种视频动作识别benchmark上经验性地证明了SV-Mix扩充策略的效果，可以提高深度模型对小规模视频数据的泛化能力，并且可以适应不同的模型结构。<details>
<summary>Abstract</summary>
The recent advances in Convolutional Neural Networks (CNNs) and Vision Transformers have convincingly demonstrated high learning capability for video action recognition on large datasets. Nevertheless, deep models often suffer from the overfitting effect on small-scale datasets with a limited number of training videos. A common solution is to exploit the existing image augmentation strategies for each frame individually including Mixup, Cutmix, and RandAugment, which are not particularly optimized for video data. In this paper, we propose a novel video augmentation strategy named Selective Volume Mixup (SV-Mix) to improve the generalization ability of deep models with limited training videos. SV-Mix devises a learnable selective module to choose the most informative volumes from two videos and mixes the volumes up to achieve a new training video. Technically, we propose two new modules, i.e., a spatial selective module to select the local patches for each spatial position, and a temporal selective module to mix the entire frames for each timestamp and maintain the spatial pattern. At each time, we randomly choose one of the two modules to expand the diversity of training samples. The selective modules are jointly optimized with the video action recognition framework to find the optimal augmentation strategy. We empirically demonstrate the merits of the SV-Mix augmentation on a wide range of video action recognition benchmarks and consistently boot the performances of both CNN-based and transformer-based models.
</details>
<details>
<summary>摘要</summary>
SV-Mix devises a learnable selective module to choose the most informative volumes from two videos and mixes them to create a new training video. Technically, we propose two new modules: a spatial selective module to select local patches for each spatial position, and a temporal selective module to mix entire frames for each timestamp while maintaining the spatial pattern. At each time, we randomly choose one of the two modules to expand the diversity of training samples. The selective modules are jointly optimized with the video action recognition framework to find the optimal augmentation strategy.We empirically demonstrate the merits of the SV-Mix augmentation on a wide range of video action recognition benchmarks and consistently boost the performances of both CNN-based and transformer-based models.
</details></li>
</ul>
<hr>
<h2 id="Decompose-Semantic-Shifts-for-Composed-Image-Retrieval"><a href="#Decompose-Semantic-Shifts-for-Composed-Image-Retrieval" class="headerlink" title="Decompose Semantic Shifts for Composed Image Retrieval"></a>Decompose Semantic Shifts for Composed Image Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09531">http://arxiv.org/abs/2309.09531</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingyu Yang, Daqing Liu, Heng Zhang, Yong Luo, Chaoyue Wang, Jing Zhang</li>
<li>for: 本研究旨在提高图像检索中的组合图像检索任务，使用参考图像作为起点，根据用户提供的文本进行启发式搜索。</li>
<li>methods: 本研究提出了一种Semantic Shift网络（SSN），将文本 instrucion 分解为两个步骤：从参考图像到视觉原型，然后从视觉原型到目标图像。SSN将文本 instrucion 分解为两个组成部分：减退和升级，用于生成图像的逐步映射。</li>
<li>results: 实验结果表明，提出的SSN在CIRR和FashionIQ数据集上显示了5.42%和1.37%的显著提升，成为图像检索领域新的状态级性能。代码将公开提供。<details>
<summary>Abstract</summary>
Composed image retrieval is a type of image retrieval task where the user provides a reference image as a starting point and specifies a text on how to shift from the starting point to the desired target image. However, most existing methods focus on the composition learning of text and reference images and oversimplify the text as a description, neglecting the inherent structure and the user's shifting intention of the texts. As a result, these methods typically take shortcuts that disregard the visual cue of the reference images. To address this issue, we reconsider the text as instructions and propose a Semantic Shift network (SSN) that explicitly decomposes the semantic shifts into two steps: from the reference image to the visual prototype and from the visual prototype to the target image. Specifically, SSN explicitly decomposes the instructions into two components: degradation and upgradation, where the degradation is used to picture the visual prototype from the reference image, while the upgradation is used to enrich the visual prototype into the final representations to retrieve the desired target image. The experimental results show that the proposed SSN demonstrates a significant improvement of 5.42% and 1.37% on the CIRR and FashionIQ datasets, respectively, and establishes a new state-of-the-art performance. Codes will be publicly available.
</details>
<details>
<summary>摘要</summary>
“构成图像检索”是一种图像检索任务，用户提供一个参考图像作为开始点，并指定一段文本，用于将参考图像转换为目标图像。然而，现有的方法将文本视为描述，忽略文本的本质和用户的转换意愿，通常会遗传参考图像的可见特征。为解决这个问题，我们重新考虑文本为 instrucion，并提出了对于 Semantic Shift network (SSN)的建议，具体地将semantic shift decomposed为两步：从参考图像到可视标本，然后从可视标本到目标图像。具体来说，SSN会将 instrucion 分成两个部分：塑性和提升，其中塑性用于将可视标本从参考图像中描绘出来，而提升则用于将可视标本提升为最终的表现，以实现检索目标图像。实验结果显示，我们提出的 SSN 与现有的方法相比，在 CIRR 和 FashionIQ 数据集上的效果提高了5.42%和1.37%，并建立了新的顶尖性能。代码将会公开。”
</details></li>
</ul>
<hr>
<h2 id="Instant-Photorealistic-Style-Transfer-A-Lightweight-and-Adaptive-Approach"><a href="#Instant-Photorealistic-Style-Transfer-A-Lightweight-and-Adaptive-Approach" class="headerlink" title="Instant Photorealistic Style Transfer: A Lightweight and Adaptive Approach"></a>Instant Photorealistic Style Transfer: A Lightweight and Adaptive Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10011">http://arxiv.org/abs/2309.10011</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rong Liu, Enyu Zhao, Zhiyuan Liu, Andrew Wei-Wen Feng, Scott John Easley</li>
<li>for: 这篇论文旨在实现快速即时精美风格转移（IPST）技术，以帮助在超分辨率输入上实现快速、无需预训练的精美风格转移。</li>
<li>methods: 该方法使用轻量级的 StyleNet 进行风格转移，以保留非颜色信息。我们还引入了实例适应优化，以优先级增强风格转移过程中的快速训练和高效的结构减少。</li>
<li>results: 实验结果表明，IPST 可以快速完成多帧风格转移任务，同时保持多视图和时间一致性。它还需要较少的 GPU 内存使用量，具有更快的多帧转移速度和更加真实的输出，这使得它在各种精美风格转移应用中成为一个有前途的解决方案。<details>
<summary>Abstract</summary>
In this paper, we propose an Instant Photorealistic Style Transfer (IPST) approach, designed to achieve instant photorealistic style transfer on super-resolution inputs without the need for pre-training on pair-wise datasets or imposing extra constraints. Our method utilizes a lightweight StyleNet to enable style transfer from a style image to a content image while preserving non-color information. To further enhance the style transfer process, we introduce an instance-adaptive optimization to prioritize the photorealism of outputs and accelerate the convergence of the style network, leading to a rapid training completion within seconds. Moreover, IPST is well-suited for multi-frame style transfer tasks, as it retains temporal and multi-view consistency of the multi-frame inputs such as video and Neural Radiance Field (NeRF). Experimental results demonstrate that IPST requires less GPU memory usage, offers faster multi-frame transfer speed, and generates photorealistic outputs, making it a promising solution for various photorealistic transfer applications.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种快速实时写入式样式转移（IPST）方法，旨在实现无需预训练对比对数据集或做出额外约束的实时写入式样式转移。我们的方法利用了轻量级的 StyleNet，以启用样式图像到内容图像的样式转移，同时保持非颜色信息。为了进一步优化样式转移过程，我们引入了实例适应优化，以优先级保持输出的实境化，加速样式网络的训练 converges，以在秒钟内完成训练。此外，IPST适用于多帧样式转移任务，因为它保持了多帧输入（如视频和Neural Radiance Field）的时间和多视点一致性。实验结果表明，IPST需要更少的GPU内存使用，具有更快的多帧转移速度，并生成高质量的输出，因此是许多写入式样式转移应用场景的有优点的解决方案。
</details></li>
</ul>
<hr>
<h2 id="NOMAD-A-Natural-Occluded-Multi-scale-Aerial-Dataset-for-Emergency-Response-Scenarios"><a href="#NOMAD-A-Natural-Occluded-Multi-scale-Aerial-Dataset-for-Emergency-Response-Scenarios" class="headerlink" title="NOMAD: A Natural, Occluded, Multi-scale Aerial Dataset, for Emergency Response Scenarios"></a>NOMAD: A Natural, Occluded, Multi-scale Aerial Dataset, for Emergency Response Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09518">http://arxiv.org/abs/2309.09518</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arturo Miguel Russell Bernal, Walter Scheirer, Jane Cleland-Huang</li>
<li>for: 这个论文是为了提高小无人飞行系统（sUAS）在紧急应急场景中的搜索和救援任务中的计算机视觉能力。</li>
<li>methods: 该论文使用了多种计算机视觉技术来检测人体，并在不同的飞行距离下进行了测试。</li>
<li>results: 该论文提供了一个新的测试数据集，可以让计算机视觉模型在受掩蔽的空中视图下进行人体检测，并且包括了多种人体运动和隐藏情况。这个数据集可以帮助提高空中搜索和救援的效iveness。<details>
<summary>Abstract</summary>
With the increasing reliance on small Unmanned Aerial Systems (sUAS) for Emergency Response Scenarios, such as Search and Rescue, the integration of computer vision capabilities has become a key factor in mission success. Nevertheless, computer vision performance for detecting humans severely degrades when shifting from ground to aerial views. Several aerial datasets have been created to mitigate this problem, however, none of them has specifically addressed the issue of occlusion, a critical component in Emergency Response Scenarios. Natural Occluded Multi-scale Aerial Dataset (NOMAD) presents a benchmark for human detection under occluded aerial views, with five different aerial distances and rich imagery variance. NOMAD is composed of 100 different Actors, all performing sequences of walking, laying and hiding. It includes 42,825 frames, extracted from 5.4k resolution videos, and manually annotated with a bounding box and a label describing 10 different visibility levels, categorized according to the percentage of the human body visible inside the bounding box. This allows computer vision models to be evaluated on their detection performance across different ranges of occlusion. NOMAD is designed to improve the effectiveness of aerial search and rescue and to enhance collaboration between sUAS and humans, by providing a new benchmark dataset for human detection under occluded aerial views.
</details>
<details>
<summary>摘要</summary>
随着小无人航空系统（sUAS）在紧急应急场景中的使用，计算机视觉技术的 интеграción已成为任务成功的关键因素。然而，计算机视觉性能在从地面视图转移到空中视图时会受到严重降低。为了解决这个问题，一些空中数据集已经被创建，但是没有一个专门解决遮挡问题，这是紧急应急场景中的关键组成部分。自然遮挡多Scale空中数据集（NOMAD）提供了人体检测下遮挡空中视图的标准准则，包括五种不同的空中距离和丰富的图像变化。NOMAD包含100名演员，每名演员都执行了走、躺和隐藏的序列，共计42825帧，来自5400分辨率视频，并手动标注了一个 bounding box 和一个describing 10种不同的可见度水平，分为100%的人体部分可见度。这allow computer vision模型在不同的遮挡级别上进行评估。NOMAD旨在提高空中搜救和人类协作的效率，通过提供人体检测下遮挡空中视图的新标准数据集来提高计算机视觉模型的检测性能。
</details></li>
</ul>
<hr>
<h2 id="Sparse-and-Privacy-enhanced-Representation-for-Human-Pose-Estimation"><a href="#Sparse-and-Privacy-enhanced-Representation-for-Human-Pose-Estimation" class="headerlink" title="Sparse and Privacy-enhanced Representation for Human Pose Estimation"></a>Sparse and Privacy-enhanced Representation for Human Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09515">http://arxiv.org/abs/2309.09515</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ting-Ying Lin, Lin-Yung Hsieh, Fu-En Wang, Wen-Shen Wuen, Min Sun</li>
<li>for: 提高人姿估计（HPE）的隐私和效率。</li>
<li>methods: 使用专利的运动向量感知器（MVS）提取edge图像和两个方向的运动向量图像，并利用最近的简单核算法进行短时间内的快速运动跟踪。</li>
<li>results: 提出一种笔触和隐私增强的HPE表示方式，实现约13倍的速度提升和96%的计算量减少，并在各个模式下进行比较， validate the privacy-enhanced quality of our sparse representation through face recognition on CelebA 和用户研究。<details>
<summary>Abstract</summary>
We propose a sparse and privacy-enhanced representation for Human Pose Estimation (HPE). Given a perspective camera, we use a proprietary motion vector sensor(MVS) to extract an edge image and a two-directional motion vector image at each time frame. Both edge and motion vector images are sparse and contain much less information (i.e., enhancing human privacy). We advocate that edge information is essential for HPE, and motion vectors complement edge information during fast movements. We propose a fusion network leveraging recent advances in sparse convolution used typically for 3D voxels to efficiently process our proposed sparse representation, which achieves about 13x speed-up and 96% reduction in FLOPs. We collect an in-house edge and motion vector dataset with 16 types of actions by 40 users using the proprietary MVS. Our method outperforms individual modalities using only edge or motion vector images. Finally, we validate the privacy-enhanced quality of our sparse representation through face recognition on CelebA (a large face dataset) and a user study on our in-house dataset.
</details>
<details>
<summary>摘要</summary>
我们提议一种稀疏化和隐私增强的人姿估算（HPE）表示方法。 Given a perspective camera, we use a proprietary motion vector sensor(MVS) to extract an edge image and a two-directional motion vector image at each time frame. Both edge and motion vector images are sparse and contain much less information (i.e., enhancing human privacy). We advocate that edge information is essential for HPE, and motion vectors complement edge information during fast movements. We propose a fusion network leveraging recent advances in sparse convolution used typically for 3D voxels to efficiently process our proposed sparse representation, which achieves about 13x speed-up and 96% reduction in FLOPs. We collect an in-house edge and motion vector dataset with 16 types of actions by 40 users using the proprietary MVS. Our method outperforms individual modalities using only edge or motion vector images. Finally, we validate the privacy-enhanced quality of our sparse representation through face recognition on CelebA (a large face dataset) and a user study on our in-house dataset.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="PanoMixSwap-Panorama-Mixing-via-Structural-Swapping-for-Indoor-Scene-Understanding"><a href="#PanoMixSwap-Panorama-Mixing-via-Structural-Swapping-for-Indoor-Scene-Understanding" class="headerlink" title="PanoMixSwap Panorama Mixing via Structural Swapping for Indoor Scene Understanding"></a>PanoMixSwap Panorama Mixing via Structural Swapping for Indoor Scene Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09514">http://arxiv.org/abs/2309.09514</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu-Cheng Hsieh, Cheng Sun, Suraj Dengale, Min Sun</li>
<li>for: 增加现代深度学习方法的训练数据量和多样性，以提高indoor scene理解能力。</li>
<li>methods: 提议了PanoMixSwap数据增强技术，通过混合不同背景风格、前景家具和房间布局，从现有的indoor panorama数据集中生成新的多样化panoramic图像。</li>
<li>results: 经过实验表明，使用PanoMixSwap对indoor scene理解任务（semantic segmentation和layout estimation）的性能有了显著提升，比原始设定升级了consistent。<details>
<summary>Abstract</summary>
The volume and diversity of training data are critical for modern deep learningbased methods. Compared to the massive amount of labeled perspective images, 360 panoramic images fall short in both volume and diversity. In this paper, we propose PanoMixSwap, a novel data augmentation technique specifically designed for indoor panoramic images. PanoMixSwap explicitly mixes various background styles, foreground furniture, and room layouts from the existing indoor panorama datasets and generates a diverse set of new panoramic images to enrich the datasets. We first decompose each panoramic image into its constituent parts: background style, foreground furniture, and room layout. Then, we generate an augmented image by mixing these three parts from three different images, such as the foreground furniture from one image, the background style from another image, and the room structure from the third image. Our method yields high diversity since there is a cubical increase in image combinations. We also evaluate the effectiveness of PanoMixSwap on two indoor scene understanding tasks: semantic segmentation and layout estimation. Our experiments demonstrate that state-of-the-art methods trained with PanoMixSwap outperform their original setting on both tasks consistently.
</details>
<details>
<summary>摘要</summary>
“现代深度学习方法需要大量和多样化的训练数据。相比巨量标注的人工智能图像，360度全景图像缺乏量和多样化。在这篇论文中，我们提出了PanoMixSwap，一种专门为室内全景图像设计的数据增强技术。PanoMixSwap显式地混合了不同背景风格、前景家具和房间布局的元素，从现有室内全景数据集中提取了多样化的新全景图像，以增加数据集的多样性。我们首先将每个全景图像分解成其组成部分：背景风格、前景家具和房间布局。然后，我们生成了一个扩充图像，将这三个部分从三个不同的图像中混合起来，例如将前景家具从一个图像中、背景风格从另一个图像中、房间布局从第三个图像中混合起来。我们的方法可以生成高多样性的图像组合，因为每个图像组合都有3^3=27种可能。我们也对两个室内场景理解任务进行了效果评估：semantic segmentation和layout estimation。我们的实验表明，通过PanoMixSwap增强训练的state-of-the-art方法在两个任务上的性能都能 consistent with the original setting。”
</details></li>
</ul>
<hr>
<h2 id="Learning-Parallax-for-Stereo-Event-based-Motion-Deblurring"><a href="#Learning-Parallax-for-Stereo-Event-based-Motion-Deblurring" class="headerlink" title="Learning Parallax for Stereo Event-based Motion Deblurring"></a>Learning Parallax for Stereo Event-based Motion Deblurring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09513">http://arxiv.org/abs/2309.09513</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingyuan Lin, Chi Zhang, Chu He, Lei Yu</li>
<li>for: 这种研究是为了提高图像减震的效果，使用事件推理来补充lost信息。</li>
<li>methods: 提出了一种新的卷积框架，即NETwork of Event-based motion Deblurring with STereo event and intensity cameras (St-EDNet)，可以直接从不一致的输入中恢复高质量图像。</li>
<li>results: 实验结果表明，提出的方法在实际 dataset 上比 estado-of-the-art 方法更高效。<details>
<summary>Abstract</summary>
Due to the extremely low latency, events have been recently exploited to supplement lost information for motion deblurring. Existing approaches largely rely on the perfect pixel-wise alignment between intensity images and events, which is not always fulfilled in the real world. To tackle this problem, we propose a novel coarse-to-fine framework, named NETwork of Event-based motion Deblurring with STereo event and intensity cameras (St-EDNet), to recover high-quality images directly from the misaligned inputs, consisting of a single blurry image and the concurrent event streams. Specifically, the coarse spatial alignment of the blurry image and the event streams is first implemented with a cross-modal stereo matching module without the need for ground-truth depths. Then, a dual-feature embedding architecture is proposed to gradually build the fine bidirectional association of the coarsely aligned data and reconstruct the sequence of the latent sharp images. Furthermore, we build a new dataset with STereo Event and Intensity Cameras (StEIC), containing real-world events, intensity images, and dense disparity maps. Experiments on real-world datasets demonstrate the superiority of the proposed network over state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
因为延迟非常低，事件最近被利用来补充lost信息以实现运动抑白。现有方法大多数假设精确的像素对应关系 между图像和事件，这在实际世界中并不总是成立。为解决这个问题，我们提议一种新的均值-精度框架，名为NETwork of Event-based motion Deblurring with STereo event and intensity cameras (St-EDNet)，可以直接从不一致的输入中恢复高质量图像，包括一个朦素图像和同时发生的事件流。具体来说，首先使用交叉模态掌时匹配模块实现了某些粗糙图像和事件流的空间对应。然后，我们提出了一种双特征嵌入体系，用于慢慢地建立精度的双向嵌入，并重建事件流中latent的锐化图像序列。此外，我们构建了一个新的StEIC数据集，包括实际世界中的事件、图像和密集的 disparity 地图。实验表明，我们提出的网络在实际数据集上的性能较为出色。
</details></li>
</ul>
<hr>
<h2 id="RenderOcc-Vision-Centric-3D-Occupancy-Prediction-with-2D-Rendering-Supervision"><a href="#RenderOcc-Vision-Centric-3D-Occupancy-Prediction-with-2D-Rendering-Supervision" class="headerlink" title="RenderOcc: Vision-Centric 3D Occupancy Prediction with 2D Rendering Supervision"></a>RenderOcc: Vision-Centric 3D Occupancy Prediction with 2D Rendering Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09502">http://arxiv.org/abs/2309.09502</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingjie Pan, Jiaming Liu, Renrui Zhang, Peixiang Huang, Xiaoqi Li, Li Liu, Shanghang Zhang</li>
<li>for: 本研究旨在提出一种基于2D标签的多视图3D占用预测方法，以便降低高成本的3D占用标签生成过程中的限制。</li>
<li>methods: 我们提出了一种基于NeRF技术的方法，将多视图图像转化为3D体积表示，并使用volume rendering技术进行2D渲染，以便直接从2D semantics和深度标签中提取3D占用信息。此外，我们还提出了一种协助 ray方法，以解决自动驾驶场景中罕见视角的问题，通过Sequential frame组合构建完整的2D渲染。</li>
<li>results: 我们的实验结果表明，RenderOcc可以与完全使用3D标签进行supervision的模型相比，达到相似的性能，这证明了这种方法在实际应用中的重要性。<details>
<summary>Abstract</summary>
3D occupancy prediction holds significant promise in the fields of robot perception and autonomous driving, which quantifies 3D scenes into grid cells with semantic labels. Recent works mainly utilize complete occupancy labels in 3D voxel space for supervision. However, the expensive annotation process and sometimes ambiguous labels have severely constrained the usability and scalability of 3D occupancy models. To address this, we present RenderOcc, a novel paradigm for training 3D occupancy models only using 2D labels. Specifically, we extract a NeRF-style 3D volume representation from multi-view images, and employ volume rendering techniques to establish 2D renderings, thus enabling direct 3D supervision from 2D semantics and depth labels. Additionally, we introduce an Auxiliary Ray method to tackle the issue of sparse viewpoints in autonomous driving scenarios, which leverages sequential frames to construct comprehensive 2D rendering for each object. To our best knowledge, RenderOcc is the first attempt to train multi-view 3D occupancy models only using 2D labels, reducing the dependence on costly 3D occupancy annotations. Extensive experiments demonstrate that RenderOcc achieves comparable performance to models fully supervised with 3D labels, underscoring the significance of this approach in real-world applications.
</details>
<details>
<summary>摘要</summary>
三维占用预测在机器人感知和自动驾驶领域产生了重要的承诺，它将三维场景分解成格格单元中的semantic标签。最近的工作主要利用完整的占用标签在3D精度空间进行监督。然而，贵重的标注过程和有时存在含瑕的标签导致了3D占用模型的可用性和扩展性受到了严重的限制。为了解决这个问题，我们提出了RenderOcc，一种新的训练3D占用模型只使用2D标签的方法。具体来说，我们从多视图图像中提取了NeRF样式的3D体积表示，并使用Volume Rendering技术来建立2D渲染，从而允许直接在2Dsemantics和深度标签的直接监督下训练3D占用模型。此外，我们还提出了auxiliary Ray方法，用于解决自动驾驶场景中稀疏的视点问题，该方法利用顺序帧构建了每个对象的完整2D渲染。根据我们所知，RenderOcc是首次训练基于2D标签的多视图3D占用模型，从而降低了3D占用标签的成本。广泛的实验表明，RenderOcc可以与基于3D标签的模型相比，证明了这种方法在实际应用中的重要性。
</details></li>
</ul>
<hr>
<h2 id="Discovering-Sounding-Objects-by-Audio-Queries-for-Audio-Visual-Segmentation"><a href="#Discovering-Sounding-Objects-by-Audio-Queries-for-Audio-Visual-Segmentation" class="headerlink" title="Discovering Sounding Objects by Audio Queries for Audio Visual Segmentation"></a>Discovering Sounding Objects by Audio Queries for Audio Visual Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09501">http://arxiv.org/abs/2309.09501</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaofei Huang, Han Li, Yuqing Wang, Hongji Zhu, Jiao Dai, Jizhong Han, Wenge Rong, Si Liu</li>
<li>for: Audio visual segmentation (AVS) aims to segment the sounding objects for each frame of a given video.</li>
<li>methods: The proposed method uses an Audio-Queried Transformer architecture (AQFormer) to establish explicit object-level semantic correspondence between audio and visual modalities, and to exchange sounding object-relevant information among multiple frames.</li>
<li>results: The method achieves state-of-the-art performances on two AVS benchmarks, with 7.1% M_J and 7.6% M_F gains on the MS3 setting.<details>
<summary>Abstract</summary>
Audio visual segmentation (AVS) aims to segment the sounding objects for each frame of a given video. To distinguish the sounding objects from silent ones, both audio-visual semantic correspondence and temporal interaction are required. The previous method applies multi-frame cross-modal attention to conduct pixel-level interactions between audio features and visual features of multiple frames simultaneously, which is both redundant and implicit. In this paper, we propose an Audio-Queried Transformer architecture, AQFormer, where we define a set of object queries conditioned on audio information and associate each of them to particular sounding objects. Explicit object-level semantic correspondence between audio and visual modalities is established by gathering object information from visual features with predefined audio queries. Besides, an Audio-Bridged Temporal Interaction module is proposed to exchange sounding object-relevant information among multiple frames with the bridge of audio features. Extensive experiments are conducted on two AVS benchmarks to show that our method achieves state-of-the-art performances, especially 7.1% M_J and 7.6% M_F gains on the MS3 setting.
</details>
<details>
<summary>摘要</summary>
Audio visual segmentation (AVS) 目标是为每帧视频中的声音对象进行分割。为了将声音对象与无声对象区分开来，需要同时使用音频 Semantic 相关性和时间互动。先前的方法使用多帧交叉模态注意力来同时进行多帧音频和视觉特征之间的像素级交互，这是一种重复和隐式的。在这篇论文中，我们提出了一种叫做 Audio-Queried Transformer 架构（AQFormer），其中我们定义了基于音频信息的对象查询集，并将每个查询与特定的声音对象相关联。在视觉特征中收集对应的对象信息，并通过固定的音频查询来确立明确的音频Semantic 相关性。此外，我们还提出了一种叫做 Audio-Bridged Temporal Interaction 模块，用于在多帧中交换与声音相关的信息，通过音频特征作为桥接。我们在两个 AVS 标准测试集上进行了广泛的实验，并证明了我们的方法可以达到当前最佳性能，特别是在 MS3 设定上提高了7.1% M_J 和 7.6% M_F。
</details></li>
</ul>
<hr>
<h2 id="Target-aware-Bi-Transformer-for-Few-shot-Segmentation"><a href="#Target-aware-Bi-Transformer-for-Few-shot-Segmentation" class="headerlink" title="Target-aware Bi-Transformer for Few-shot Segmentation"></a>Target-aware Bi-Transformer for Few-shot Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09492">http://arxiv.org/abs/2309.09492</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xianglin Wang, Xiaoliu Luo, Taiping Zhang</li>
<li>For: The paper is written for proposing a new few-shot semantic segmentation (FSS) method that uses limited labeled support images to identify the segmentation of new classes of objects.* Methods: The proposed method, called Target-aware Bi-Transformer Network (TBTNet), uses a novel Target-aware Transformer Layer (TTL) to distill correlations and focus on foreground information. The model treats the hypercorrelation as a feature, resulting in a significant reduction in the number of feature channels.* Results: The proposed method achieves state-of-the-art performance on standard FSS benchmarks of PASCAL-5i and COCO-20i, with only 0.4M learnable parameters and converging in 10% to 25% of the training epochs compared to traditional methods.<details>
<summary>Abstract</summary>
Traditional semantic segmentation tasks require a large number of labels and are difficult to identify unlearned categories. Few-shot semantic segmentation (FSS) aims to use limited labeled support images to identify the segmentation of new classes of objects, which is very practical in the real world. Previous researches were primarily based on prototypes or correlations. Due to colors, textures, and styles are similar in the same image, we argue that the query image can be regarded as its own support image. In this paper, we proposed the Target-aware Bi-Transformer Network (TBTNet) to equivalent treat of support images and query image. A vigorous Target-aware Transformer Layer (TTL) also be designed to distill correlations and force the model to focus on foreground information. It treats the hypercorrelation as a feature, resulting a significant reduction in the number of feature channels. Benefit from this characteristic, our model is the lightest up to now with only 0.4M learnable parameters. Futhermore, TBTNet converges in only 10% to 25% of the training epochs compared to traditional methods. The excellent performance on standard FSS benchmarks of PASCAL-5i and COCO-20i proves the efficiency of our method. Extensive ablation studies were also carried out to evaluate the effectiveness of Bi-Transformer architecture and TTL.
</details>
<details>
<summary>摘要</summary>
传统的semantic segmentation任务需要大量的标签，并且难以识别未学习的类别。几shot semantic segmentation（FSS）目的是使用有限的标签支持图像来分类新类型的对象，这非常实用于实际世界。先前的研究主要基于原型或相关性。由于图像中颜色、文本和风格具有相似性，我们认为查询图像可以被视为其自己的支持图像。在这篇论文中，我们提出了Target-aware Bi-Transformer Network（TBTNet），以等效地处理支持图像和查询图像。我们还设计了一种强大的Target-aware Transformer层（TTL），以把 corrleation 当作特征，使模型更加注重前景信息。这种特点使得我们的模型成为目前最轻量级的，只有0.4M 可学习参数。此外，TBTNet在训练EPochs中 converges 在10%-25%的训练epochs，相比传统方法更快。标准的FSS Benchmarks PASCAL-5i和COCO-20i中的出色表现证明了我们的方法的效率。我们还进行了广泛的ablation研究，以评估Bi-Transformer架构和TTL的效果。
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-TransUNet-for-Ultrasound-regional-segmentation-of-the-distal-radius-in-children"><a href="#Self-supervised-TransUNet-for-Ultrasound-regional-segmentation-of-the-distal-radius-in-children" class="headerlink" title="Self-supervised TransUNet for Ultrasound regional segmentation of the distal radius in children"></a>Self-supervised TransUNet for Ultrasound regional segmentation of the distal radius in children</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09490">http://arxiv.org/abs/2309.09490</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuyue Zhou, Jessica Knight, Banafshe Felfeliyan, Christopher Keen, Abhilash Rakkunedeth Hareendranathan, Jacob L. Jaremko<br>for: 这篇论文的目的是为了应用自我监督学习（SSL）方法来减少医疗影像标注数量，以提高医疗影像分类和诊断的自动化分析。methods: 这篇论文使用了Masked Autoencoder for SSL（SSL-MAE）方法，并变化了嵌入和损失函数，以提高下游结果。results: 研究发现，将TransUNet嵌入和Encoder预训练使用SSL-MAE，不能比TransUNet直接进行下游分类任务预训练更好。<details>
<summary>Abstract</summary>
Supervised deep learning offers great promise to automate analysis of medical images from segmentation to diagnosis. However, their performance highly relies on the quality and quantity of the data annotation. Meanwhile, curating large annotated datasets for medical images requires a high level of expertise, which is time-consuming and expensive. Recently, to quench the thirst for large data sets with high-quality annotation, self-supervised learning (SSL) methods using unlabeled domain-specific data, have attracted attention. Therefore, designing an SSL method that relies on minimal quantities of labeled data has far-reaching significance in medical images. This paper investigates the feasibility of deploying the Masked Autoencoder for SSL (SSL-MAE) of TransUNet, for segmenting bony regions from children's wrist ultrasound scans. We found that changing the embedding and loss function in SSL-MAE can produce better downstream results compared to the original SSL-MAE. In addition, we determined that only pretraining TransUNet embedding and encoder with SSL-MAE does not work as well as TransUNet without SSL-MAE pretraining on downstream segmentation tasks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>发表文章报告摘要深度学习监督下的检测和诊断预测具有巨大的承诺，但是它们的表现受到数据标注质量和量的限制。同时，为医疗图像批量标注数据集准备高级别的专业知识和技能，需要很多时间和成本。最近，为了满足大量高质量标注数据的需求，自动学习（SSL）方法，使用不标注的领域特定数据，吸引了关注。因此，设计一种SSL方法，只需少量标注数据，具有重要意义。这篇论文 investigate了将Masked Autoencoder（SSL-MAE）用于儿童肋骨ultrasound扫描图像的分割。我们发现，在SSL-MAE中修改嵌入和损失函数可以生成更好的下游结果，并且确定了不是先行SSL-MAE预训练TransUNet embedding和编码器的下游分割任务的效果不如TransUNet无SSL-MAE预训练。详细描述在医疗图像分割预测中，深度学习监督下的方法具有很高的承诺，但是它们的表现受到数据标注质量和量的限制。为了解决这个问题，研究者们开始关注自动学习（SSL）方法。SSL方法可以使用不标注的领域特定数据，来适应特定任务。在本文中，我们 investigate了将Masked Autoencoder（SSL-MAE）用于儿童肋骨ultrasound扫描图像的分割。我们首先介绍了SSL-MAE的基本思想和方法。然后，我们对SSL-MAE的嵌入和损失函数进行了修改，以便生成更好的下游结果。最后，我们对TransUNet embedding和编码器进行了SSL-MAE预训练，并对其下游分割任务进行了评估。结果显示，在SSL-MAE中修改嵌入和损失函数可以生成更好的下游结果。此外，我们发现不是先行SSL-MAE预训练TransUNet embedding和编码器的下游分割任务的效果不如TransUNet无SSL-MAE预训练。这些结果表明，SSL-MAE可以帮助提高医疗图像分割预测的性能，并且可以适应不同的任务和数据集。结论本文 investigate了将Masked Autoencoder（SSL-MAE）用于儿童肋骨ultrasound扫描图像的分割。我们发现，在SSL-MAE中修改嵌入和损失函数可以生成更好的下游结果，并且确定了不是先行SSL-MAE预训练TransUNet embedding和编码器的下游分割任务的效果不如TransUNet无SSL-MAE预训练。这些结果表明，SSL-MAE可以帮助提高医疗图像分割预测的性能，并且可以适应不同的任务和数据集。因此，设计一种SSL方法，只需少量标注数据，具有重要意义。
</details></li>
</ul>
<hr>
<h2 id="Distributional-Estimation-of-Data-Uncertainty-for-Surveillance-Face-Anti-spoofing"><a href="#Distributional-Estimation-of-Data-Uncertainty-for-Surveillance-Face-Anti-spoofing" class="headerlink" title="Distributional Estimation of Data Uncertainty for Surveillance Face Anti-spoofing"></a>Distributional Estimation of Data Uncertainty for Surveillance Face Anti-spoofing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09485">http://arxiv.org/abs/2309.09485</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mouxiao Huang<br>for: This paper aims to improve the security of face anti-spoofing (FAS) systems in long-distance surveillance scenarios, which are often characterized by low-quality face images and high levels of data uncertainty.methods: The proposed method, called Distributional Estimation (DisE), models data uncertainty during training to improve the stability and accuracy of FAS systems. DisE adjusts the learning strength of clean and noisy samples to enhance performance.results: The proposed method was evaluated on a large-scale and challenging FAS dataset (SuHiFiMask) and achieved comparable performance on both ACER and AUC metrics, indicating its effectiveness in improving the security of FAS systems in long-distance surveillance scenarios.<details>
<summary>Abstract</summary>
Face recognition systems have become increasingly vulnerable to security threats in recent years, prompting the use of Face Anti-spoofing (FAS) to protect against various types of attacks, such as phone unlocking, face payment, and self-service security inspection. While FAS has demonstrated its effectiveness in traditional settings, securing it in long-distance surveillance scenarios presents a significant challenge. These scenarios often feature low-quality face images, necessitating the modeling of data uncertainty to improve stability under extreme conditions. To address this issue, this work proposes Distributional Estimation (DisE), a method that converts traditional FAS point estimation to distributional estimation by modeling data uncertainty during training, including feature (mean) and uncertainty (variance). By adjusting the learning strength of clean and noisy samples for stability and accuracy, the learned uncertainty enhances DisE's performance. The method is evaluated on SuHiFiMask [1], a large-scale and challenging FAS dataset in surveillance scenarios. Results demonstrate that DisE achieves comparable performance on both ACER and AUC metrics.
</details>
<details>
<summary>摘要</summary>
现在的面Recognition系统已经变得易受到安全威胁，因此使用Face Anti-spoofing（FAS）来保护各种攻击，如手机唔锁、脸部支付和自助安全检查。虽然FAS在传统场景下表现出了效iveness，但在长距离监测场景中保持安全是一项 significante challenge。这些场景通常会出现低质量的脸像，因此需要模型数据不确定性以提高在极端情况下的稳定性。为解决这个问题，本工作提出了分布统计（DisE）方法，它将传统的FAS点估转换为分布统计，通过在训练中模型数据不确定性，包括特征（均值）和不确定性（方差）。通过调整清洁和噪声样本的学习力，学习出来的不确定性提高了DisE的性能。这种方法在SuHiFiMask数据集上进行了评估，结果表明DisE在ACER和AUC指标上具有相当的性能。
</details></li>
</ul>
<hr>
<h2 id="An-Accurate-and-Efficient-Neural-Network-for-OCTA-Vessel-Segmentation-and-a-New-Dataset"><a href="#An-Accurate-and-Efficient-Neural-Network-for-OCTA-Vessel-Segmentation-and-a-New-Dataset" class="headerlink" title="An Accurate and Efficient Neural Network for OCTA Vessel Segmentation and a New Dataset"></a>An Accurate and Efficient Neural Network for OCTA Vessel Segmentation and a New Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09483">http://arxiv.org/abs/2309.09483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haojian Ning, Chengliang Wang, Xinrun Chen, Shiying Li</li>
<li>for: 本研究利用非侵入性的光共振 Tomatoes angiography（OCTA）成像技术，描述高分辨率的血管网络。</li>
<li>methods: 该研究提出了一种准确且高效的神经网络方法，用于Retinal vessel segmentation在OCTA图像中。该方法通过应用修改后的Recurrent ConvNeXt块，实现了与其他SOTA方法相当的准确率，同时具有更少的参数和更快的推理速度（例如110倍轻量级和1.3倍快于U-Net），适用于工业应用。</li>
<li>results: 该研究创建了918张OCTA图像和其相应的血管注释集。这个数据集使用Segment Anything Model（SAM）进行 semi-自动注释，大大提高了注释速度。<details>
<summary>Abstract</summary>
Optical coherence tomography angiography (OCTA) is a noninvasive imaging technique that can reveal high-resolution retinal vessels. In this work, we propose an accurate and efficient neural network for retinal vessel segmentation in OCTA images. The proposed network achieves accuracy comparable to other SOTA methods, while having fewer parameters and faster inference speed (e.g. 110x lighter and 1.3x faster than U-Net), which is very friendly for industrial applications. This is achieved by applying the modified Recurrent ConvNeXt Block to a full resolution convolutional network. In addition, we create a new dataset containing 918 OCTA images and their corresponding vessel annotations. The data set is semi-automatically annotated with the help of Segment Anything Model (SAM), which greatly improves the annotation speed. For the benefit of the community, our code and dataset can be obtained from https://github.com/nhjydywd/OCTA-FRNet.
</details>
<details>
<summary>摘要</summary>
optical coherence tomography angiography (OCTA) 是一种非侵入性的图像成像技术，可以显示高分辨率的肉眼血管。在这项工作中，我们提出了一种准确和高效的神经网络 для肉眼血管分 segmentation 在 OCTA 图像中。我们的提案的网络实现了与其他 SOTA 方法相同的准确率，而且具有较少的参数和更快的推理速度（例如，110 倍轻量级和 1.3 倍的速度），这对于工业应用非常友好。这一 достиvement 归功于在全分辨率 convolutional 网络中应用修改后的 Recurrent ConvNeXt Block。此外，我们创建了一个包含 918 个 OCTA 图像和其相应的血管注释的数据集。这个数据集通过 Segment Anything Model (SAM) 的 semi-automatic 注释，可以大幅提高注释速度。为了便于社区，我们的代码和数据集可以从 GitHub 上下载：https://github.com/nhjydywd/OCTA-FRNet。
</details></li>
</ul>
<hr>
<h2 id="Spatio-temporal-Co-attention-Fusion-Network-for-Video-Splicing-Localization"><a href="#Spatio-temporal-Co-attention-Fusion-Network-for-Video-Splicing-Localization" class="headerlink" title="Spatio-temporal Co-attention Fusion Network for Video Splicing Localization"></a>Spatio-temporal Co-attention Fusion Network for Video Splicing Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09482">http://arxiv.org/abs/2309.09482</a></li>
<li>repo_url: None</li>
<li>paper_authors: Man Lin, Gang Cao, Zijie Lou</li>
<li>for: 本研究旨在提出一种针对视频拼接 forgery 的检测方法，以推动视频的真实性和安全性。</li>
<li>methods: 本研究使用了一种三核 streams 网络作为编码器，通过 novel parallel and cross co-attention fusion modules 来实现深度交互和融合，以提取多帧视频中的修改迹象。</li>
<li>results: 测试结果表明，使用 SCFNet 可以高效地检测视频拼接 forgery，并且在不同的视频 dataset 上达到了 state-of-the-art 的性能。<details>
<summary>Abstract</summary>
Digital video splicing has become easy and ubiquitous. Malicious users copy some regions of a video and paste them to another video for creating realistic forgeries. It is significant to blindly detect such forgery regions in videos. In this paper, a spatio-temporal co-attention fusion network (SCFNet) is proposed for video splicing localization. Specifically, a three-stream network is used as an encoder to capture manipulation traces across multiple frames. The deep interaction and fusion of spatio-temporal forensic features are achieved by the novel parallel and cross co-attention fusion modules. A lightweight multilayer perceptron (MLP) decoder is adopted to yield a pixel-level tampering localization map. A new large-scale video splicing dataset is created for training the SCFNet. Extensive tests on benchmark datasets show that the localization and generalization performances of our SCFNet outperform the state-of-the-art. Code and datasets will be available at https://github.com/multimediaFor/SCFNet.
</details>
<details>
<summary>摘要</summary>
digital video 剪辑已成为易于普遍的。恶意用户可以将视频中的一些区域复制到另一个视频中，以创造真实的假象。因此，必须在视频中探测这些假象区域。本文提出了一种基于视频剪辑的空间时间共互关注融合网络（SCFNet），用于各种视频剪辑检测。具体来说，使用了一个三流网络作为编码器，以捕捉多帧 manipulate traces。通过新型并行和交叉共互关注融合模块，深度地进行了视频剪辑特征的深度交互和融合。使用了一个轻量级多层感知器（MLP）作为解码器，以生成像素级假象Localization图。创建了一个大规模的视频剪辑数据集，用于训练SCFNet。对于标准 benchmark 数据集进行了广泛的测试，测试结果显示，SCFNet 的Localization和泛化性能都高于当前状态。代码和数据集将在 https://github.com/multimediaFor/SCFNet 上发布。
</details></li>
</ul>
<hr>
<h2 id="Stealthy-Physical-Masked-Face-Recognition-Attack-via-Adversarial-Style-Optimization"><a href="#Stealthy-Physical-Masked-Face-Recognition-Attack-via-Adversarial-Style-Optimization" class="headerlink" title="Stealthy Physical Masked Face Recognition Attack via Adversarial Style Optimization"></a>Stealthy Physical Masked Face Recognition Attack via Adversarial Style Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09480">http://arxiv.org/abs/2309.09480</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huihui Gong, Minjing Dong, Siqi Ma, Seyit Camtepe, Surya Nepal, Chang Xu</li>
<li>for: 本研究旨在探讨一种隐蔽式的面部识别模型攻击方法，用于面部识别任务中的隐蔽式攻击。</li>
<li>methods: 本研究使用了一种新的隐蔽式面部识别攻击方法，即通过对面部识别模型进行隐蔽式攻击，使模型具有较高的攻击力和隐蔽性。</li>
<li>results: 研究发现，对于面部识别任务，采用本研究提出的隐蔽式攻击方法可以很好地隐藏攻击具有较高的攻击力和隐蔽性。此外，研究还发现了一些特定的隐蔽式攻击方法可以在不同的面部识别模型上具有较高的攻击力和隐蔽性。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) have achieved state-of-the-art performance on face recognition (FR) tasks in the last decade. In real scenarios, the deployment of DNNs requires taking various face accessories into consideration, like glasses, hats, and masks. In the COVID-19 pandemic era, wearing face masks is one of the most effective ways to defend against the novel coronavirus. However, DNNs are known to be vulnerable to adversarial examples with a small but elaborated perturbation. Thus, a facial mask with adversarial perturbations may pose a great threat to the widely used deep learning-based FR models. In this paper, we consider a challenging adversarial setting: targeted attack against FR models. We propose a new stealthy physical masked FR attack via adversarial style optimization. Specifically, we train an adversarial style mask generator that hides adversarial perturbations inside style masks. Moreover, to ameliorate the phenomenon of sub-optimization with one fixed style, we propose to discover the optimal style given a target through style optimization in a continuous relaxation manner. We simultaneously optimize the generator and the style selection for generating strong and stealthy adversarial style masks. We evaluated the effectiveness and transferability of our proposed method via extensive white-box and black-box digital experiments. Furthermore, we also conducted physical attack experiments against local FR models and online platforms.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNNs）在过去一代的面部识别（FR）任务上达到了状态之一。在实际应用中，部署DNNs时需考虑面部访问ories，如眼镜、帽子和面具。在 COVID-19 疫情时期，穿戴面具是一种最有效的防止新型冠状病毒的方法。然而，DNNs 知道小型但精心制作的攻击例子。因此，一个面具with adversarial perturbations可能对通用的深度学习基于 FR 模型 pose 大威胁。在这篇论文中，我们考虑了一个挑战性的对抗设定：面部识别模型的目标攻击。我们提出了一种新的隐蔽的物理面具FR攻击，通过对抗式风格优化。具体来说，我们训练了一个对抗式风格面具生成器，以隐藏对抗性扰动在风格面具中。此外，为了改善一个固定风格下的优化现象，我们提出了在一个目标上进行风格优化的方法。我们同时优化了生成器和风格选择，以生成强大和隐蔽的对抗式风格面具。我们通过了广泛的白盒和黑盒数字实验，以及本地 FR 模型和在线平台的物理攻击实验。
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-Multi-view-Clustering-in-Computer-Vision-A-Survey"><a href="#Self-supervised-Multi-view-Clustering-in-Computer-Vision-A-Survey" class="headerlink" title="Self-supervised Multi-view Clustering in Computer Vision: A Survey"></a>Self-supervised Multi-view Clustering in Computer Vision: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09473">http://arxiv.org/abs/2309.09473</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiatai Wang, Zhiwei Xu, Xuewen Yang, Hailong Li, Bo Li, Xuying Meng</li>
<li>for: 本文旨在探讨多视图归一 clustering（MVC）在跨Modal Representation Learning和数据驱动决策中的重要性，以及自然学习在 MVC 方法中的普及。</li>
<li>methods: 本文主要介绍了自然学习驱动 MVC 方法，包括设计代表任务来挖掘图像和视频数据的表示，以及常见数据集、数据问题、表示学习方法和自然学习方法的内部连接和分类。</li>
<li>results: 本文不仅介绍了每种类别的机制，还给出了一些应用示例。最后，文章还提出了一些未解决的问题，以便进一步的研究和发展。<details>
<summary>Abstract</summary>
Multi-view clustering (MVC) has had significant implications in cross-modal representation learning and data-driven decision-making in recent years. It accomplishes this by leveraging the consistency and complementary information among multiple views to cluster samples into distinct groups. However, as contrastive learning continues to evolve within the field of computer vision, self-supervised learning has also made substantial research progress and is progressively becoming dominant in MVC methods. It guides the clustering process by designing proxy tasks to mine the representation of image and video data itself as supervisory information. Despite the rapid development of self-supervised MVC, there has yet to be a comprehensive survey to analyze and summarize the current state of research progress. Therefore, this paper explores the reasons and advantages of the emergence of self-supervised MVC and discusses the internal connections and classifications of common datasets, data issues, representation learning methods, and self-supervised learning methods. This paper does not only introduce the mechanisms for each category of methods but also gives a few examples of how these techniques are used. In the end, some open problems are pointed out for further investigation and development.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Reconstructing-Existing-Levels-through-Level-Inpainting"><a href="#Reconstructing-Existing-Levels-through-Level-Inpainting" class="headerlink" title="Reconstructing Existing Levels through Level Inpainting"></a>Reconstructing Existing Levels through Level Inpainting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09472">http://arxiv.org/abs/2309.09472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Johor Jara Gonzalez, Mathew Guzdial</li>
<li>for: 这篇论文是为了描述如何使用Content Augmentation和Procedural Content Generation via Machine Learning（PCGML）来生成电子游戏等游戏中的关卡。</li>
<li>methods: 这篇论文使用了两种图像填充技术，即Autoencoder和U-net，来解决关卡填充问题。</li>
<li>results: 在比较基准方法的情况下，这两种方法都表现出了较好的性能，并且提供了实际的关卡填充示例和未来研究的想法。<details>
<summary>Abstract</summary>
Procedural Content Generation (PCG) and Procedural Content Generation via Machine Learning (PCGML) have been used in prior work for generating levels in various games. This paper introduces Content Augmentation and focuses on the subproblem of level inpainting, which involves reconstructing and extending video game levels. Drawing inspiration from image inpainting, we adapt two techniques from this domain to address our specific use case. We present two approaches for level inpainting: an Autoencoder and a U-net. Through a comprehensive case study, we demonstrate their superior performance compared to a baseline method and discuss their relative merits. Furthermore, we provide a practical demonstration of both approaches for the level inpainting task and offer insights into potential directions for future research.
</details>
<details>
<summary>摘要</summary>
《生成内容技术》和《机器学习生成内容技术》在先前的工作中已经用于生成游戏等游戏中的关卡。这篇论文介绍内容扩展和填充，它们是关卡填充的子问题。受到图像填充的启发，我们从该领域适应了两种技术来解决我们的特定用例。我们提出了两种关卡填充方法：一种是自适应网络，另一种是U网络。通过完整的案例研究，我们证明了这两种方法的超越性比基eline方法，并讨论了它们的相对优劣。此外，我们为两种方法的关卡填充任务提供了实践示例，并为未来研究提供了可能的方向。
</details></li>
</ul>
<hr>
<h2 id="Progressive-Text-to-Image-Diffusion-with-Soft-Latent-Direction"><a href="#Progressive-Text-to-Image-Diffusion-with-Soft-Latent-Direction" class="headerlink" title="Progressive Text-to-Image Diffusion with Soft Latent Direction"></a>Progressive Text-to-Image Diffusion with Soft Latent Direction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09466">http://arxiv.org/abs/2309.09466</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/babahui/progressive-text-to-image">https://github.com/babahui/progressive-text-to-image</a></li>
<li>paper_authors: YuTeng Ye, Jiale Cai, Hang Zhou, Guanwen Li, Youjia Zhang, Zikai Song, Chenxing Gao, Junqing Yu, Wei Yang</li>
<li>for: 本研究旨在解决文本到图像生成中多个实体的同时拼接和约束的挑战。</li>
<li>methods: 该 paper 提出了一种进步的分步生成和修改操作，通过逐步将实体添加到目标图像中，以保证它们在空间和关系约束下进行拼接。</li>
<li>results: 该 paper 的提出的方法在处理复杂和长文本描述时显示出了明显的进步，特别是在对多个实体的拼接和修改方面。<details>
<summary>Abstract</summary>
In spite of the rapidly evolving landscape of text-to-image generation, the synthesis and manipulation of multiple entities while adhering to specific relational constraints pose enduring challenges. This paper introduces an innovative progressive synthesis and editing operation that systematically incorporates entities into the target image, ensuring their adherence to spatial and relational constraints at each sequential step. Our key insight stems from the observation that while a pre-trained text-to-image diffusion model adeptly handles one or two entities, it often falters when dealing with a greater number. To address this limitation, we propose harnessing the capabilities of a Large Language Model (LLM) to decompose intricate and protracted text descriptions into coherent directives adhering to stringent formats. To facilitate the execution of directives involving distinct semantic operations-namely insertion, editing, and erasing-we formulate the Stimulus, Response, and Fusion (SRF) framework. Within this framework, latent regions are gently stimulated in alignment with each operation, followed by the fusion of the responsive latent components to achieve cohesive entity manipulation. Our proposed framework yields notable advancements in object synthesis, particularly when confronted with intricate and lengthy textual inputs. Consequently, it establishes a new benchmark for text-to-image generation tasks, further elevating the field's performance standards.
</details>
<details>
<summary>摘要</summary>
尽管文本到图像生成领域在快速发展，仍然有多个实体的合理拟合和操作是持续的挑战。这篇论文提出了一种创新的进行式合成和编辑操作，系统地将实体integrated到目标图像中，以确保它们在每个顺序步骤中遵循空间和关系约束。我们的关键发现来自于发现一个预训练的文本到图像扩散模型能够好好地处理一个或两个实体，但当面临更多实体时，它经常出现问题。为解决这种限制，我们提议利用大型自然语言模型（LLM）来分解复杂和长文本描述，并将其转化为严格格式的直接指令。为了实现不同semantic操作的执行，我们提出了刺激、编辑和消除等操作的框架，称为刺激响应拼接（SRF）框架。在这个框架中，各个实体的latent空间会在每个操作中被细致地刺激，然后将响应的latent组件进行拼接，以实现一致的实体修改。我们的提议的框架在对复杂和长文本输入的对象合成方面带来了显著的进步，因此，它在文本到图像生成任务中成为了一个新的标准，进一步提高了这个领域的性能标准。
</details></li>
</ul>
<hr>
<h2 id="Reducing-Adversarial-Training-Cost-with-Gradient-Approximation"><a href="#Reducing-Adversarial-Training-Cost-with-Gradient-Approximation" class="headerlink" title="Reducing Adversarial Training Cost with Gradient Approximation"></a>Reducing Adversarial Training Cost with Gradient Approximation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09464">http://arxiv.org/abs/2309.09464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huihui Gong, Shuo Yang, Siqi Ma, Seyit Camtepe, Surya Nepal, Chang Xu</li>
<li>for: 提高模型对抗示例（Adversarial Examples，AE）的Robustness，提高模型的可靠性和安全性。</li>
<li>methods: 使用项目Gradient Descent（PGD）基于的对抗训练法，以及一种新的和高效的对抗训练方法——对抗训练with Gradient Approximation（GAAT），以减少建立强度模型的成本。</li>
<li>results: 对MNIST、CIFAR-10和CIFAR-100等 dataset进行了广泛的实验，结果显示，GAAT方法可以保持模型的准确率，同时减少训练时间，最多可以减少60%的训练时间。<details>
<summary>Abstract</summary>
Deep learning models have achieved state-of-the-art performances in various domains, while they are vulnerable to the inputs with well-crafted but small perturbations, which are named after adversarial examples (AEs). Among many strategies to improve the model robustness against AEs, Projected Gradient Descent (PGD) based adversarial training is one of the most effective methods. Unfortunately, the prohibitive computational overhead of generating strong enough AEs, due to the maximization of the loss function, sometimes makes the regular PGD adversarial training impractical when using larger and more complicated models. In this paper, we propose that the adversarial loss can be approximated by the partial sum of Taylor series. Furthermore, we approximate the gradient of adversarial loss and propose a new and efficient adversarial training method, adversarial training with gradient approximation (GAAT), to reduce the cost of building up robust models. Additionally, extensive experiments demonstrate that this efficiency improvement can be achieved without any or with very little loss in accuracy on natural and adversarial examples, which show that our proposed method saves up to 60\% of the training time with comparable model test accuracy on MNIST, CIFAR-10 and CIFAR-100 datasets.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose a new and efficient adversarial training method, called adversarial training with gradient approximation (GAAT), to reduce the cost of building robust models. We approximate the adversarial loss using a partial sum of Taylor series and approximate the gradient of the adversarial loss. Our proposed method saves up to 60% of the training time with comparable model test accuracy on MNIST, CIFAR-10, and CIFAR-100 datasets.Extensive experiments demonstrate that our proposed method achieves the same or better accuracy on natural and adversarial examples, without any loss in accuracy. This shows that our method is efficient and effective in improving the robustness of deep learning models against adversarial attacks.
</details></li>
</ul>
<hr>
<h2 id="Object2Scene-Putting-Objects-in-Context-for-Open-Vocabulary-3D-Detection"><a href="#Object2Scene-Putting-Objects-in-Context-for-Open-Vocabulary-3D-Detection" class="headerlink" title="Object2Scene: Putting Objects in Context for Open-Vocabulary 3D Detection"></a>Object2Scene: Putting Objects in Context for Open-Vocabulary 3D Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09456">http://arxiv.org/abs/2309.09456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenming Zhu, Wenwei Zhang, Tai Wang, Xihui Liu, Kai Chen</li>
<li>for: 提高开放词汇3D物体检测的性能，使用大规模大词汇3D物体数据集来扩充现有3D场景数据集的词汇。</li>
<li>methods: 使用Object2Scene方法，将不同来源的3D物体插入到3D场景中，生成对应的文本描述，并提出了跨频率类别对比学习方法来缓解不同数据集之间的频率差。</li>
<li>results: 在现有的开放词汇3D物体检测标准benchmark上实现了超过现有方法的性能，并在一个新的benchmark OV-ScanNet-200上进行了验证，并证明了对所有罕见类的检测能力。<details>
<summary>Abstract</summary>
Point cloud-based open-vocabulary 3D object detection aims to detect 3D categories that do not have ground-truth annotations in the training set. It is extremely challenging because of the limited data and annotations (bounding boxes with class labels or text descriptions) of 3D scenes. Previous approaches leverage large-scale richly-annotated image datasets as a bridge between 3D and category semantics but require an extra alignment process between 2D images and 3D points, limiting the open-vocabulary ability of 3D detectors. Instead of leveraging 2D images, we propose Object2Scene, the first approach that leverages large-scale large-vocabulary 3D object datasets to augment existing 3D scene datasets for open-vocabulary 3D object detection. Object2Scene inserts objects from different sources into 3D scenes to enrich the vocabulary of 3D scene datasets and generates text descriptions for the newly inserted objects. We further introduce a framework that unifies 3D detection and visual grounding, named L3Det, and propose a cross-domain category-level contrastive learning approach to mitigate the domain gap between 3D objects from different datasets. Extensive experiments on existing open-vocabulary 3D object detection benchmarks show that Object2Scene obtains superior performance over existing methods. We further verify the effectiveness of Object2Scene on a new benchmark OV-ScanNet-200, by holding out all rare categories as novel categories not seen during training.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXT点云基于开 vocabulary 3D物体检测目标是检测没有训练集中的地面标注的3D类别。这是非常具有挑战性，因为3D场景数据和标注（ bounding box 与类别标签或文本描述）受限。先前的方法利用大规模有很多描述的图像数据作为3D和类别 semantics 之间的桥梁，但是需要Extra的对2D图像和3D点的对应过程，限制了3D检测器的开 vocabulary 能力。相比之下，我们提议使用大规模大 vocabulary 3D物体数据来增强现有3D场景数据，并在3D场景中插入不同来源的物体，以增加3D场景数据的词汇量。我们还引入了将3D检测和视觉定位（grounding）集成在一起的框架，名为L3Det，并提出了跨 Domian 类别对比学习方法，以 Mitigate 3D对象从不同数据集中的领域差。我们进行了大量的实验，证明 Object2Scene 在现有的开 vocabulary 3D检测标准 benchmark 上表现出色。我们还验证了 Object2Scene 在 OV-ScanNet-200 新的benchmark上的效果，通过在训练中不包括所有罕见类别。TRANSLATE_TEXT
</details></li>
</ul>
<hr>
<h2 id="Scalable-Label-efficient-Footpath-Network-Generation-Using-Remote-Sensing-Data-and-Self-supervised-Learning"><a href="#Scalable-Label-efficient-Footpath-Network-Generation-Using-Remote-Sensing-Data-and-Self-supervised-Learning" class="headerlink" title="Scalable Label-efficient Footpath Network Generation Using Remote Sensing Data and Self-supervised Learning"></a>Scalable Label-efficient Footpath Network Generation Using Remote Sensing Data and Self-supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09446">http://arxiv.org/abs/2309.09446</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wennyxy/footpathseg">https://github.com/wennyxy/footpathseg</a></li>
<li>paper_authors: Xinye Wanyan, Sachith Seneviratne, Kerry Nice, Jason Thompson, Marcus White, Nano Langenheim, Mark Stevenson</li>
<li>For: The paper is written for urban planners and researchers who need to manage and analyze footpath infrastructure in cities, but lack real-time information and resources for doing so.* Methods: The paper proposes an automatic pipeline for generating footpath networks based on remote sensing images using machine learning models, specifically a self-supervised feature representation learning method to reduce annotation requirements.* Results: The proposed method is validated using remote sensing images and shows considerable consistency with manually collected GIS layers, making it a low-cost and extensible solution for footpath network generation.Here’s the simplified Chinese text for the three points:* For: 该论文是为城市规划师和研究人员写的，他们需要管理和分析城市的步行道基础设施，但缺乏实时信息和资源。* Methods: 该论文提出了一个基于遥感图像的自动化步行道网络生成管线，使用机器学习模型，特别是一种自我指导的特征表示学习方法，以降低标注需求。* Results: 提posed方法通过对遥感图像进行验证，与手动收集的GIS层进行比较，显示了 considerable consistency，表明该方法是一种低成本和可扩展的解决方案。<details>
<summary>Abstract</summary>
Footpath mapping, modeling, and analysis can provide important geospatial insights to many fields of study, including transport, health, environment and urban planning. The availability of robust Geographic Information System (GIS) layers can benefit the management of infrastructure inventories, especially at local government level with urban planners responsible for the deployment and maintenance of such infrastructure. However, many cities still lack real-time information on the location, connectivity, and width of footpaths, and/or employ costly and manual survey means to gather this information. This work designs and implements an automatic pipeline for generating footpath networks based on remote sensing images using machine learning models. The annotation of segmentation tasks, especially labeling remote sensing images with specialized requirements, is very expensive, so we aim to introduce a pipeline requiring less labeled data. Considering supervised methods require large amounts of training data, we use a self-supervised method for feature representation learning to reduce annotation requirements. Then the pre-trained model is used as the encoder of the U-Net for footpath segmentation. Based on the generated masks, the footpath polygons are extracted and converted to footpath networks which can be loaded and visualized by geographic information systems conveniently. Validation results indicate considerable consistency when compared to manually collected GIS layers. The footpath network generation pipeline proposed in this work is low-cost and extensible, and it can be applied where remote sensing images are available. Github: https://github.com/WennyXY/FootpathSeg.
</details>
<details>
<summary>摘要</summary>
Footpath mapping, modeling, and analysis can provide important geospatial insights to many fields of study, including transport, health, environment, and urban planning. The availability of robust Geographic Information System (GIS) layers can benefit the management of infrastructure inventories, especially at local government level with urban planners responsible for the deployment and maintenance of such infrastructure. However, many cities still lack real-time information on the location, connectivity, and width of footpaths, and/or employ costly and manual survey means to gather this information. This work designs and implements an automatic pipeline for generating footpath networks based on remote sensing images using machine learning models. The annotation of segmentation tasks, especially labeling remote sensing images with specialized requirements, is very expensive, so we aim to introduce a pipeline requiring less labeled data. Considering supervised methods require large amounts of training data, we use a self-supervised method for feature representation learning to reduce annotation requirements. Then the pre-trained model is used as the encoder of the U-Net for footpath segmentation. Based on the generated masks, the footpath polygons are extracted and converted to footpath networks which can be loaded and visualized by geographic information systems conveniently. Validation results indicate considerable consistency when compared to manually collected GIS layers. The footpath network generation pipeline proposed in this work is low-cost and extensible, and it can be applied where remote sensing images are available. Github: <https://github.com/WennyXY/FootpathSeg>.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="TransTouch-Learning-Transparent-Objects-Depth-Sensing-Through-Sparse-Touches"><a href="#TransTouch-Learning-Transparent-Objects-Depth-Sensing-Through-Sparse-Touches" class="headerlink" title="TransTouch: Learning Transparent Objects Depth Sensing Through Sparse Touches"></a>TransTouch: Learning Transparent Objects Depth Sensing Through Sparse Touches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09427">http://arxiv.org/abs/2309.09427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liuyu Bian, Pengyang Shi, Weihang Chen, Jing Xu, Li Yi, Rui Chen</li>
<li>for: 提高真实世界中透明物体深度感知的精度</li>
<li>methods: 使用探测系统提供自动收集的笔触深度标签来训练雷达网络，并使用一个新的价值函数评估探测位置的利好，以提高网络在真实世界中的性能</li>
<li>results: 在真实世界中construct了一个包括透明和不透明物体的数据集，并进行了实验，结果显示，使用我们的方法可以显著提高真实世界中透明物体深度感知的精度<details>
<summary>Abstract</summary>
Transparent objects are common in daily life. However, depth sensing for transparent objects remains a challenging problem. While learning-based methods can leverage shape priors to improve the sensing quality, the labor-intensive data collection in the real world and the sim-to-real domain gap restrict these methods' scalability. In this paper, we propose a method to finetune a stereo network with sparse depth labels automatically collected using a probing system with tactile feedback. We present a novel utility function to evaluate the benefit of touches. By approximating and optimizing the utility function, we can optimize the probing locations given a fixed touching budget to better improve the network's performance on real objects. We further combine tactile depth supervision with a confidence-based regularization to prevent over-fitting during finetuning. To evaluate the effectiveness of our method, we construct a real-world dataset including both diffuse and transparent objects. Experimental results on this dataset show that our method can significantly improve real-world depth sensing accuracy, especially for transparent objects.
</details>
<details>
<summary>摘要</summary>
通常存在于日常生活中的透明物体depth感测仍然是一个挑战。学习基于方法可以利用形状假设提高感测质量，但实际世界中的数据收集劳动密集和实验室到实际域的差距限制这些方法的扩展性。在这篇论文中，我们提议一种方法，通过自动收集使用探测系统的粘贴反馈来精细调整激光网络。我们提出了一种新的实用函数来评估触摸的利用度。通过估算和优化实用函数，我们可以在给定一个固定的触摸预算下优化探测位置，以更好地提高网络的性能于实际物体上。我们还将满足粘贴深度监督与信心基于规则进行减少过拟合。为评估我们的方法的效果，我们构建了一个包括散发和透明物体的实际世界数据集。实验结果表明，我们的方法可以在实际世界中提高深度感测精度，特别是对于透明物体。
</details></li>
</ul>
<hr>
<h2 id="Cross-attention-based-saliency-inference-for-predicting-cancer-metastasis-on-whole-slide-images"><a href="#Cross-attention-based-saliency-inference-for-predicting-cancer-metastasis-on-whole-slide-images" class="headerlink" title="Cross-attention-based saliency inference for predicting cancer metastasis on whole slide images"></a>Cross-attention-based saliency inference for predicting cancer metastasis on whole slide images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09412">http://arxiv.org/abs/2309.09412</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyu Su, Mostafa Rezapour, Usama Sajjad, Shuo Niu, Metin Nafi Gurcan, Muhammad Khalid Khan Niazi<br>for:This paper proposes a new method for automatic tumor detection on whole slide images (WSI) using cross-attention-based salient instance inference MIL (CASiiMIL).methods:The proposed method uses a novel saliency-informed attention mechanism and negative representation learning algorithm to identify breast cancer lymph node micro-metastasis on WSIs without the need for any annotations.results:The proposed model outperforms the state-of-the-art MIL methods on two popular tumor metastasis detection datasets and demonstrates great cross-center generalizability. It also exhibits excellent accuracy in classifying WSIs with small tumor lesions and has excellent interpretability attributed to the saliency-informed attention weights.<details>
<summary>Abstract</summary>
Although multiple instance learning (MIL) methods are widely used for automatic tumor detection on whole slide images (WSI), they suffer from the extreme class imbalance within the small tumor WSIs. This occurs when the tumor comprises only a few isolated cells. For early detection, it is of utmost importance that MIL algorithms can identify small tumors, even when they are less than 1% of the size of the WSI. Existing studies have attempted to address this issue using attention-based architectures and instance selection-based methodologies, but have not yielded significant improvements. This paper proposes cross-attention-based salient instance inference MIL (CASiiMIL), which involves a novel saliency-informed attention mechanism, to identify breast cancer lymph node micro-metastasis on WSIs without the need for any annotations. Apart from this new attention mechanism, we introduce a negative representation learning algorithm to facilitate the learning of saliency-informed attention weights for improved sensitivity on tumor WSIs. The proposed model outperforms the state-of-the-art MIL methods on two popular tumor metastasis detection datasets, and demonstrates great cross-center generalizability. In addition, it exhibits excellent accuracy in classifying WSIs with small tumor lesions. Moreover, we show that the proposed model has excellent interpretability attributed to the saliency-informed attention weights. We strongly believe that the proposed method will pave the way for training algorithms for early tumor detection on large datasets where acquiring fine-grained annotations is practically impossible.
</details>
<details>
<summary>摘要</summary>
多个实例学习（MIL）方法在整个扫描图像（WSI）上自动检测肿瘤已经广泛应用，但它们受到扫描图像中小肿瘤的极端分布异常困扰。这发生在肿瘤只包含几个隔离的细胞时。在早期检测中，非常重要的是MIL算法可以识别小肿瘤，即使它们的大小小于扫描图像的1%。现有研究已经使用注意力基 architecture和实例选择基础方法来解决这个问题，但没有产生显著改进。本文提出了跨注意力基础的突出例推理MIL（CASiiMIL），它包括一种新的突出性注意力机制，以无需任何注释来检测乳腺癌Node微量肿瘤在扫描图像上。此外，我们还引入了一种负表示学习算法，以便更好地学习突出性注意力权重，以提高肿瘤WSIs的敏感性。提出的模型在两个流行的肿瘤метастаisis检测数据集上表现出色，并且具有优秀的跨中心一致性。此外，它还能够高度准确地分类WSIs中的小肿瘤 lesions。此外，我们还证明了该模型具有优秀的解释性，即通过突出性注意力权重来解释其决策。我们认为，提出的方法将为大量数据上无法获得细致的注释的训练算法开创新的道路。
</details></li>
</ul>
<hr>
<h2 id="BRONCO-Automated-modelling-of-the-bronchovascular-bundle-using-the-Computed-Tomography-Images"><a href="#BRONCO-Automated-modelling-of-the-bronchovascular-bundle-using-the-Computed-Tomography-Images" class="headerlink" title="BRONCO: Automated modelling of the bronchovascular bundle using the Computed Tomography Images"></a>BRONCO: Automated modelling of the bronchovascular bundle using the Computed Tomography Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09410">http://arxiv.org/abs/2309.09410</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wojciech Prażuch, Marek Socha, Anna Mrukwa, Aleksandra Suwalska, Agata Durawa, Malgorzata Jelitto-Górska, Katarzyna Dziadziuszko, Edyta Szurowska, Pawel Bożek, Michal Marczyk, Witold Rzyman, Joanna Polanska</li>
<li>for: 这篇论文主要针对的是体内肺部分的分类，特别是针对肺部分中的气道和血管网络的分类。</li>
<li>methods: 本研究使用了 Computed Tomography 图像来建立肺部分中的气道和血管网络分类ipeline，包括两个模组：模拟气道树和血管树。</li>
<li>results: 研究发现，这个方法在不同的 CT 影像数据中都能够获得正确的分类结果，并且不受 CT 影像系列的起始点和参数的影响。这个方法适用于健康人群、肺肿瘤患者和肺绒溃病患者。<details>
<summary>Abstract</summary>
Segmentation of the bronchovascular bundle within the lung parenchyma is a key step for the proper analysis and planning of many pulmonary diseases. It might also be considered the preprocessing step when the goal is to segment the nodules from the lung parenchyma. We propose a segmentation pipeline for the bronchovascular bundle based on the Computed Tomography images, returning either binary or labelled masks of vessels and bronchi situated in the lung parenchyma. The method consists of two modules, modeling of the bronchial tree and vessels. The core revolves around a similar pipeline, the determination of the initial perimeter by the GMM method, skeletonization, and hierarchical analysis of the created graph. We tested our method on both low-dose CT and standard-dose CT, with various pathologies, reconstructed with various slice thicknesses, and acquired from various machines. We conclude that the method is invariant with respect to the origin and parameters of the CT series. Our pipeline is best suited for studies with healthy patients, patients with lung nodules, and patients with emphysema.
</details>
<details>
<summary>摘要</summary>
填充肺部分分析是许多肺病诊断和规划的关键步骤。它还可以看作预处理步骤，当目标是从肺部分中分割肿瘤时。我们提出了基于计算机断层成像图像的肺部分分析管道，可以生成二值或标注的血管和支气道在肺部分中的掩模。管道由两个模块组成：模拟 bronchial tree 和血管。核心在于类似的管道，即使用 GMM 方法确定初始边界，skeletonization 和层次分析创建的图像。我们在低剂量 CT 和标准剂量 CT 图像上测试了我们的方法，包括不同的病理变化、不同的扫描厚度和不同的成像机。我们结论是该方法具有Origin和参数不归一化的特点。我们的管道适用于健康人群、肿瘤患者和emphysema患者。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/18/cs.CV_2023_09_18/" data-id="closbroox00io0g889sym50gl" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_09_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/18/cs.AI_2023_09_18/" class="article-date">
  <time datetime="2023-09-18T12:00:00.000Z" itemprop="datePublished">2023-09-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/18/cs.AI_2023_09_18/">cs.AI - 2023-09-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Towards-Effective-Semantic-OOD-Detection-in-Unseen-Domains-A-Domain-Generalization-Perspective"><a href="#Towards-Effective-Semantic-OOD-Detection-in-Unseen-Domains-A-Domain-Generalization-Perspective" class="headerlink" title="Towards Effective Semantic OOD Detection in Unseen Domains: A Domain Generalization Perspective"></a>Towards Effective Semantic OOD Detection in Unseen Domains: A Domain Generalization Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10209">http://arxiv.org/abs/2309.10209</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoliang Wang, Chen Zhao, Yunhui Guo, Kai Jiang, Feng Chen</li>
<li>for:  simultaneously addresses both distributional shifts in real-world testing environments</li>
<li>methods:  introduces two regularization strategies: domain generalization regularization and OOD detection regularization</li>
<li>results:  showcases superior OOD detection performance compared to conventional domain generalization approaches while maintaining comparable InD classification accuracy<details>
<summary>Abstract</summary>
Two prevalent types of distributional shifts in machine learning are the covariate shift (as observed across different domains) and the semantic shift (as seen across different classes). Traditional OOD detection techniques typically address only one of these shifts. However, real-world testing environments often present a combination of both covariate and semantic shifts. In this study, we introduce a novel problem, semantic OOD detection across domains, which simultaneously addresses both distributional shifts. To this end, we introduce two regularization strategies: domain generalization regularization, which ensures semantic invariance across domains to counteract the covariate shift, and OOD detection regularization, designed to enhance OOD detection capabilities against the semantic shift through energy bounding. Through rigorous testing on three standard domain generalization benchmarks, our proposed framework showcases its superiority over conventional domain generalization approaches in terms of OOD detection performance. Moreover, it holds its ground by maintaining comparable InD classification accuracy.
</details>
<details>
<summary>摘要</summary>
有两种常见的分布偏移在机器学习中是 covariate shift（随着不同领域的观察）和 semantic shift（随着不同的类别的观察）。传统的ODOut detection技术通常只 Address one of these shifts. 然而，实际的测试环境通常会出现 covariate 和 semantic 两种分布偏移的组合。在本研究中，我们引入了一个新的问题：针对不同领域的 semantic OOD detection，这种问题同时解决了两种分布偏移。为此，我们提出了两种 régularization 策略：领域泛化 régularization，确保在不同领域下的semantic 一致性，以遏制 covariate 偏移；以及 OOD detection régularization，通过能量约束来提高ODOut detection能力，抵消 semantic 偏移。通过对三个标准领域泛化测试 benchmark 进行严格的测试，我们的提议框架在ODOut detection性能方面superior于传统的领域泛化方法，并且保持了与InD类别准确率相当的水平。
</details></li>
</ul>
<hr>
<h2 id="Stabilizing-RLHF-through-Advantage-Model-and-Selective-Rehearsal"><a href="#Stabilizing-RLHF-through-Advantage-Model-and-Selective-Rehearsal" class="headerlink" title="Stabilizing RLHF through Advantage Model and Selective Rehearsal"></a>Stabilizing RLHF through Advantage Model and Selective Rehearsal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10202">http://arxiv.org/abs/2309.10202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Baolin Peng, Linfeng Song, Ye Tian, Lifeng Jin, Haitao Mi, Dong Yu</li>
<li>for: 这份技术报告是为了解决RLHF训练中的稳定性问题，包括奖励抢夺和忘记性问题。</li>
<li>methods: 该报告提出了两种创新来稳定RLHF训练：1）优点模型，直接模型优点分数，并规范分数分布在任务之间以预防奖励抢夺。 2）选择性练习，通过精心选择数据进行PPO训练和知识练习，以 Mitigate catastrophic forgetting。</li>
<li>results: 我们的实验分析表明，提出的方法不仅提高了RLHF训练的稳定性，还达到了更高的奖励分数和胜利率。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have revolutionized natural language processing, yet aligning these models with human values and preferences using RLHF remains a significant challenge. This challenge is characterized by various instabilities, such as reward hacking and catastrophic forgetting. In this technical report, we propose two innovations to stabilize RLHF training: 1) Advantage Model, which directly models advantage score i.e., extra reward compared to the expected rewards and regulates score distributions across tasks to prevent reward hacking. 2) Selective Rehearsal, which mitigates catastrophic forgetting by strategically selecting data for PPO training and knowledge rehearsing. Our experimental analysis on public and proprietary datasets reveals that the proposed methods not only increase stability in RLHF training but also achieve higher reward scores and win rates.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Advantage Model: directly models advantage score, i.e., extra reward compared to the expected rewards, and regulates score distributions across tasks to prevent reward hacking.2. Selective Rehearsal: mitigates catastrophic forgetting by strategically selecting data for PPO training and knowledge rehearsing.Our experimental analysis on public and proprietary datasets reveals that the proposed methods not only increase stability in RLHF training but also achieve higher reward scores and win rates.Translated into Simplified Chinese:大型语言模型（LLMs）已经革命化自然语言处理，但是使用RLHF与人类价值观和偏好相对是一大挑战。这种挑战由各种不稳定性，如奖励黑客和慢性忘记，引起。在这份技术报告中，我们提出了两项创新，以稳定RLHF训练：1. 优势模型：直接模型奖励分数，即比预期奖励更高的额外奖励，并规范分数分布遍布任务，以防止奖励黑客。2. 选择性练习：避免慢性忘记，通过策略选择数据进行PPO训练和知识练习。我们对公共和专用数据集进行实验分析，发现我们的方法不仅提高了RLHF训练的稳定性，还达到了更高的奖励分数和胜利率。</details></li>
</ol>
<hr>
<h2 id="Graph-enabled-Reinforcement-Learning-for-Time-Series-Forecasting-with-Adaptive-Intelligence"><a href="#Graph-enabled-Reinforcement-Learning-for-Time-Series-Forecasting-with-Adaptive-Intelligence" class="headerlink" title="Graph-enabled Reinforcement Learning for Time Series Forecasting with Adaptive Intelligence"></a>Graph-enabled Reinforcement Learning for Time Series Forecasting with Adaptive Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10186">http://arxiv.org/abs/2309.10186</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thanveer Shaik, Xiaohui Tao, Haoran Xie, Lin Li, Jianming Yong, Yuefeng Li</li>
<li>for: 这个研究是为了提出一个基于图形神经网络（GNN）和强化学习（RL）的新方法来预测时间序列数据。</li>
<li>methods: 这个研究使用了GNN来处理时间序列数据，并与RL结合以监控模型。GNN能够自然地捕捉时间序列中的数据结构，并且可以更好地预测复杂的时间序列结构，例如健康、交通和天气预测。</li>
<li>results: 这个研究发现，GraphRL模型在时间序列预测和监控中具有更高的准确性和效率，相比于传统的深度学习模型。此外，这个研究还发现GNN在时间序列预测中的表现比RNN和LSTM更好。<details>
<summary>Abstract</summary>
Reinforcement learning is well known for its ability to model sequential tasks and learn latent data patterns adaptively. Deep learning models have been widely explored and adopted in regression and classification tasks. However, deep learning has its limitations such as the assumption of equally spaced and ordered data, and the lack of ability to incorporate graph structure in terms of time-series prediction. Graphical neural network (GNN) has the ability to overcome these challenges and capture the temporal dependencies in time-series data. In this study, we propose a novel approach for predicting time-series data using GNN and monitoring with Reinforcement Learning (RL). GNNs are able to explicitly incorporate the graph structure of the data into the model, allowing them to capture temporal dependencies in a more natural way. This approach allows for more accurate predictions in complex temporal structures, such as those found in healthcare, traffic and weather forecasting. We also fine-tune our GraphRL model using a Bayesian optimisation technique to further improve performance. The proposed framework outperforms the baseline models in time-series forecasting and monitoring. The contributions of this study include the introduction of a novel GraphRL framework for time-series prediction and the demonstration of the effectiveness of GNNs in comparison to traditional deep learning models such as RNNs and LSTMs. Overall, this study demonstrates the potential of GraphRL in providing accurate and efficient predictions in dynamic RL environments.
</details>
<details>
<summary>摘要</summary>
“强化学习”是已知能够模型顺序任务和学习隐藏数据模式的能力。深度学习模型在推断和分类任务中广泛应用。然而，深度学习有一些限制，例如假设数据平均 spacing和排序，以及无法运用时间序列预测中的图形结构。图形神经网络（GNN）可以超越这些挑战，捕捉时间序列数据中的时间相依性，并在更自然的方式中捕捉时间序列。这种方法可以在复杂的时间序列结构中提供更准确的预测，例如健康监测、交通预测和天气预测。我们还使用 bayesian 优化技术进一步提高性能。我们的 GraphRL 模型在时间序列预测和监控中超越了基eline 模型。这个研究的贡献包括：1. 提出一个 Novel GraphRL 框架 для 时间序列预测。2. 显示 GNNs 在比较于传统深度学习模型（RNNs 和 LSTMs）中更有效。3. 显示 GraphRL 框架在时间序列预测和监控中的应用前景。总体来说，这个研究显示 GraphRL 在动态RL环境中提供准确和高效的预测。
</details></li>
</ul>
<hr>
<h2 id="QoS-Aware-Service-Prediction-and-Orchestration-in-Cloud-Network-Integrated-Beyond-5G"><a href="#QoS-Aware-Service-Prediction-and-Orchestration-in-Cloud-Network-Integrated-Beyond-5G" class="headerlink" title="QoS-Aware Service Prediction and Orchestration in Cloud-Network Integrated Beyond 5G"></a>QoS-Aware Service Prediction and Orchestration in Cloud-Network Integrated Beyond 5G</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10185">http://arxiv.org/abs/2309.10185</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hieu9955/ggggg">https://github.com/hieu9955/ggggg</a></li>
<li>paper_authors: Mohammad Farhoudi, Masoud Shokrnezhad, Tarik Taleb</li>
<li>for: 本研究旨在探讨在Metaverse等 novel应用中， beyond 5G 网络所需的 ultra-low latency 通信和大量宽带连接，以及随着用户数量的不断变化，需要提高服务持续性考虑。</li>
<li>methods: 本文使用 edge-cloud 模式来充分利用云计算资源，实时管理用户在网络中的移动。然而，边缘云网络受到许多限制，包括网络和计算资源的共同管理，以及用户动态性、服务启动延迟和流量压力等因素。</li>
<li>results: 本文提出了一种基于 non-linear programming 模型的服务放置和资源分配算法，以最小化总成本，同时增强延迟。此外，我们还提出了一种基于 RNN 的 DDQL 技术，使用水填算法进行服务放置，以适应用户动态性和服务启动延迟。 simulation 结果表明，我们的解决方案可以提供有效的响应，最大化网络潜力，并提供可扩展的服务放置。<details>
<summary>Abstract</summary>
Novel applications such as the Metaverse have highlighted the potential of beyond 5G networks, which necessitate ultra-low latency communications and massive broadband connections. Moreover, the burgeoning demand for such services with ever-fluctuating users has engendered a need for heightened service continuity consideration in B5G. To enable these services, the edge-cloud paradigm is a potential solution to harness cloud capacity and effectively manage users in real time as they move across the network. However, edge-cloud networks confront a multitude of limitations, including networking and computing resources that must be collectively managed to unlock their full potential. This paper addresses the joint problem of service placement and resource allocation in a network-cloud integrated environment while considering capacity constraints, dynamic users, and end-to-end delays. We present a non-linear programming model that formulates the optimization problem with the aiming objective of minimizing overall cost while enhancing latency. Next, to address the problem, we introduce a DDQL-based technique using RNNs to predict user behavior, empowered by a water-filling-based algorithm for service placement. The proposed framework adeptly accommodates the dynamic nature of users, the placement of services that mandate ultra-low latency in B5G, and service continuity when users migrate from one location to another. Simulation results show that our solution provides timely responses that optimize the network's potential, offering a scalable and efficient placement.
</details>
<details>
<summary>摘要</summary>
新应用如Metaverse等强调 beyond 5G 网络的潜在力量，需要超低延迟通信和巨大宽带连接。此外，随着用户数量不断变化，需要提高服务连续性考虑。为激活这些服务，边云模式是一个可能的解决方案，可以利用云计算资源并实时管理用户。然而，边云网络面临着许多限制，包括网络和计算资源的集成管理，以实现其全部潜力。本文 Addresses 服务放置和资源分配问题在网络云Integrated 环境中，考虑到容量约束、动态用户和终端延迟。我们提出一个非线性 программирова模型，以最小化总成本而提高延迟。然后，我们引入了基于 RNN 的 DDQL 技术，利用水填算法来实现服务放置。该方案适应用户的动态性，强调 B5G 中服务放置的低延迟性和用户迁移时的服务连续性。 simulation 结果表明，我们的解决方案可以提供有效的响应，最大化网络潜力。
</details></li>
</ul>
<hr>
<h2 id="Positive-and-Risky-Message-Assessment-for-Music-Products"><a href="#Positive-and-Risky-Message-Assessment-for-Music-Products" class="headerlink" title="Positive and Risky Message Assessment for Music Products"></a>Positive and Risky Message Assessment for Music Products</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10182">http://arxiv.org/abs/2309.10182</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yigeng Zhang, Mahsa Shafaei, Fabio Gonzalez, Thamar Solorio</li>
<li>for: 评估音乐产品中的积极和危险信息</li>
<li>methods: 提出了一种新的研究问题，并提出了一种多角度多级音乐内容评估标准准，然后提出了一种有效的多任务预测模型，并在这些模型中加入了排序约束来解决这个问题。</li>
<li>results: 对比于强有力的任务特定对手，提出的方法不仅显著超越了它们，还可以同时评估多个方面。<details>
<summary>Abstract</summary>
In this work, we propose a novel research problem: assessing positive and risky messages from music products. We first establish a benchmark for multi-angle multi-level music content assessment and then present an effective multi-task prediction model with ordinality-enforcement to solve this problem. Our result shows the proposed method not only significantly outperforms strong task-specific counterparts but can concurrently evaluate multiple aspects.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们提出了一个新的研究问题：评估音乐产品中的积极和风险消息。我们首先建立了多角度多级音乐内容评估的标准准则，然后提出了一种高效的多任务预测模型，并在这些任务之间强制实施排序约束。我们的结果表明，我们提出的方法不仅在多个任务上显著超越了专门的对手，而且可以同时评估多个方面。Here's the breakdown of the translation:* 在这项研究中 (in this research)* 我们提出了一个新的研究问题 (we propose a new research problem)* 评估音乐产品中的积极和风险消息 (assessing positive and risky messages in music products)* 我们首先建立了多角度多级音乐内容评估的标准准则 (we first establish a benchmark for multi-angle multi-level music content assessment)* 然后提出了一种高效的多任务预测模型 (then we propose an effective multi-task prediction model)* 并在这些任务之间强制实施排序约束 (and enforce ordinal constraints between tasks)* 我们的结果表明 (our results show)* 我们提出的方法不仅在多个任务上显著超越了专门的对手 (our method significantly outperforms task-specific counterparts)* 而且可以同时评估多个方面 (and can concurrently evaluate multiple aspects)
</details></li>
</ul>
<hr>
<h2 id="Double-Deep-Q-Learning-based-Path-Selection-and-Service-Placement-for-Latency-Sensitive-Beyond-5G-Applications"><a href="#Double-Deep-Q-Learning-based-Path-Selection-and-Service-Placement-for-Latency-Sensitive-Beyond-5G-Applications" class="headerlink" title="Double Deep Q-Learning-based Path Selection and Service Placement for Latency-Sensitive Beyond 5G Applications"></a>Double Deep Q-Learning-based Path Selection and Service Placement for Latency-Sensitive Beyond 5G Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10180">http://arxiv.org/abs/2309.10180</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masoud Shokrnezhad, Tarik Taleb, Patrizio Dazzi</li>
<li>for: This paper aims to solve the joint problem of communication and computing resource allocation in cloud-network integrated infrastructures to minimize total cost.</li>
<li>methods: The paper proposes two approaches based on the Branch &amp; Bound and Water-Filling algorithms to solve the problem when the system is fully known, and a Double Deep Q-Learning (DDQL) architecture is designed for partially known systems.</li>
<li>results: Numerical simulations show that the proposed B&amp;B-CCRA approach optimally solves the problem, while the WF-CCRA approach delivers near-optimal solutions in a substantially shorter time. Additionally, the DDQL-CCRA approach obtains near-optimal solutions in the absence of request-specific information.<details>
<summary>Abstract</summary>
Nowadays, as the need for capacity continues to grow, entirely novel services are emerging. A solid cloud-network integrated infrastructure is necessary to supply these services in a real-time responsive, and scalable way. Due to their diverse characteristics and limited capacity, communication and computing resources must be collaboratively managed to unleash their full potential. Although several innovative methods have been proposed to orchestrate the resources, most ignored network resources or relaxed the network as a simple graph, focusing only on cloud resources. This paper fills the gap by studying the joint problem of communication and computing resource allocation, dubbed CCRA, including function placement and assignment, traffic prioritization, and path selection considering capacity constraints and quality requirements, to minimize total cost. We formulate the problem as a non-linear programming model and propose two approaches, dubbed B\&B-CCRA and WF-CCRA, based on the Branch \& Bound and Water-Filling algorithms to solve it when the system is fully known. Then, for partially known systems, a Double Deep Q-Learning (DDQL) architecture is designed. Numerical simulations show that B\&B-CCRA optimally solves the problem, whereas WF-CCRA delivers near-optimal solutions in a substantially shorter time. Furthermore, it is demonstrated that DDQL-CCRA obtains near-optimal solutions in the absence of request-specific information.
</details>
<details>
<summary>摘要</summary>
现在，由于需求量的增长， Entirely novel services 是出现的。一个固定云网络集成基础设施是必要的，以便在实时响应和可扩展的方式提供这些服务。由于这些服务的多样性和有限的容量，通信和计算资源必须共同管理，以发挥最大的潜力。虽然一些创新的方法已经被提出来协调资源，但大多数忽视了网络资源或者将网络视为简单的图，只关注云资源。这篇论文填充了这个空白，通过研究集成通信和计算资源分配（CCRA）问题，包括函数放置和分配、吞吐率优先级和路径选择，对容量约束和质量要求进行最小化总成本。我们将问题转化为非线性编程模型，并提出了B\&B-CCRA和WF-CCRA两种方法，基于 Branch \& Bound 和 Water-Filling 算法来解决。而在部分知道系统下，我们设计了Double Deep Q-Learning（DDQL）架构。实验显示，B\&B-CCRA 可以最优解决问题，而WF-CCRA 可以在substantially  shorter time 内提供近似优解。此外，我们还证明了 DDQL-CCRA 可以在请求特定信息缺失时获得近似优解。
</details></li>
</ul>
<hr>
<h2 id="Self-Sustaining-Multiple-Access-with-Continual-Deep-Reinforcement-Learning-for-Dynamic-Metaverse-Applications"><a href="#Self-Sustaining-Multiple-Access-with-Continual-Deep-Reinforcement-Learning-for-Dynamic-Metaverse-Applications" class="headerlink" title="Self-Sustaining Multiple Access with Continual Deep Reinforcement Learning for Dynamic Metaverse Applications"></a>Self-Sustaining Multiple Access with Continual Deep Reinforcement Learning for Dynamic Metaverse Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10177">http://arxiv.org/abs/2309.10177</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamidreza Mazandarani, Masoud Shokrnezhad, Tarik Taleb, Richard Li</li>
<li>for: This paper aims to address the challenge of managing multiple access to the frequency spectrum in a dynamic and complex scenario of the Metaverse, with a focus on maximizing the throughput of intelligent agents in multi-channel environments.</li>
<li>methods: The proposed method is based on Double Deep Q-Learning (DDQL) empowered by Continual Learning (CL), which is designed to overcome the non-stationary situation and unknown environment.</li>
<li>results: The numerical simulations show that the CL-DDQL algorithm achieves significantly higher throughputs with a considerably shorter convergence time compared to other well-known methods, especially in highly dynamic scenarios.<details>
<summary>Abstract</summary>
The Metaverse is a new paradigm that aims to create a virtual environment consisting of numerous worlds, each of which will offer a different set of services. To deal with such a dynamic and complex scenario, considering the stringent quality of service requirements aimed at the 6th generation of communication systems (6G), one potential approach is to adopt self-sustaining strategies, which can be realized by employing Adaptive Artificial Intelligence (Adaptive AI) where models are continually re-trained with new data and conditions. One aspect of self-sustainability is the management of multiple access to the frequency spectrum. Although several innovative methods have been proposed to address this challenge, mostly using Deep Reinforcement Learning (DRL), the problem of adapting agents to a non-stationary environment has not yet been precisely addressed. This paper fills in the gap in the current literature by investigating the problem of multiple access in multi-channel environments to maximize the throughput of the intelligent agent when the number of active User Equipments (UEs) may fluctuate over time. To solve the problem, a Double Deep Q-Learning (DDQL) technique empowered by Continual Learning (CL) is proposed to overcome the non-stationary situation, while the environment is unknown. Numerical simulations demonstrate that, compared to other well-known methods, the CL-DDQL algorithm achieves significantly higher throughputs with a considerably shorter convergence time in highly dynamic scenarios.
</details>
<details>
<summary>摘要</summary>
metaverse 是一新的思维方式，旨在创造一个虚拟环境，包括多个世界，每个世界都会提供不同的服务。为了面对这样一个动态和复杂的情况，一种可能的方法是采用自我维护策略，这可以通过使用适应性人工智能（适应AI）来实现，其中模型 continually 在新数据和条件下重新训练。一个自我维护的方面是频谱访问的管理。虽然已经有许多创新的方法被提出来解决这个挑战，主要使用深度强化学习（DRL），但是在不稳定的环境中适应代理人的问题还没有得到准确地解决。这篇论文填充了现有文献中的空白，研究了多个频道环境中多个访问者的情况，以 maximize 智能代理人的吞吐量，当有多个活动用户设备（UE）的情况下。为解决这个问题，我们提出了一种基于 Continual Learning（CL）的 Double Deep Q-Learning（DDQL）技术。在不稳定的环境中，CL-DDQL 算法可以在不知道环境的情况下，与其他已知方法相比，实现显著更高的吞吐量和迅速 converge 时间。
</details></li>
</ul>
<hr>
<h2 id="One-ACT-Play-Single-Demonstration-Behavior-Cloning-with-Action-Chunking-Transformers"><a href="#One-ACT-Play-Single-Demonstration-Behavior-Cloning-with-Action-Chunking-Transformers" class="headerlink" title="One ACT Play: Single Demonstration Behavior Cloning with Action Chunking Transformers"></a>One ACT Play: Single Demonstration Behavior Cloning with Action Chunking Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10175">http://arxiv.org/abs/2309.10175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abraham George, Amir Barati Farimani</li>
<li>for: 学习人类示例（行为克隆）是机器学习的基础。但大多数行为克隆算法需要许多示例来学习任务，特别是面临着许多初始条件的复杂任务。然而，人类可以通过只看一两次示例来完成任务。我们的工作想要模仿这种能力，使用行为克隆学习任务给定单个人类示例。我们实现这一目标通过使用线性变换扩展单个示例，生成一组路径 для许多初始条件。通过这些示例，我们可以训练一个行为克隆机器人成功完成三个块处理任务。</li>
<li>methods: 我们使用线性变换扩展单个示例，生成一组路径 для许多初始条件。此外，我们还开发了一种新的方法，即在推理过程中将行动预测的标准差添加到 temporal ensembling 方法中，以提高对不明确环境变化的弹性性。</li>
<li>results: 我们通过对三个块处理任务进行训练，成功地使用单个人类示例来学习这些任务。此外，我们的方法在对环境变化时表现更加稳定和可靠，从而实现了显著性能提高。<details>
<summary>Abstract</summary>
Learning from human demonstrations (behavior cloning) is a cornerstone of robot learning. However, most behavior cloning algorithms require a large number of demonstrations to learn a task, especially for general tasks that have a large variety of initial conditions. Humans, however, can learn to complete tasks, even complex ones, after only seeing one or two demonstrations. Our work seeks to emulate this ability, using behavior cloning to learn a task given only a single human demonstration. We achieve this goal by using linear transforms to augment the single demonstration, generating a set of trajectories for a wide range of initial conditions. With these demonstrations, we are able to train a behavior cloning agent to successfully complete three block manipulation tasks. Additionally, we developed a novel addition to the temporal ensembling method used by action chunking agents during inference. By incorporating the standard deviation of the action predictions into the ensembling method, our approach is more robust to unforeseen changes in the environment, resulting in significant performance improvements.
</details>
<details>
<summary>摘要</summary>
学习人类示例（行为克隆）是机器人学习的基础。然而，大多数行为克隆算法需要许多示例来学习任务，特别是面临着较大的初始条件多样性的任务。然而，人类可以通过只看一两次示例来完成复杂任务。我们的工作寻求模仿这种能力，使用行为克隆算法学习任务，只需要单个人类示例。我们通过使用线性变换来扩展单个示例，生成一系列的轨迹，以适应各种初始条件。通过这些示例，我们可以训练一个行为克隆代理人来成功完成三个块处理任务。此外，我们还开发了一种新的添加到 temporal ensembling 方法中的方法，通过在推理过程中包含行动预测的标准差，使我们的方法更加鲁棒对待不可预期的环境变化，从而实现显著的性能提升。
</details></li>
</ul>
<hr>
<h2 id="Asynchronous-Perception-Action-Communication-with-Graph-Neural-Networks"><a href="#Asynchronous-Perception-Action-Communication-with-Graph-Neural-Networks" class="headerlink" title="Asynchronous Perception-Action-Communication with Graph Neural Networks"></a>Asynchronous Perception-Action-Communication with Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10164">http://arxiv.org/abs/2309.10164</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saurav Agarwal, Alejandro Ribeiro, Vijay Kumar</li>
<li>for: 这paper是为了解决大型机器人群体中的协同决策问题，以实现共同全局目标。</li>
<li>methods: 该paper使用Graph Neural Networks（GNNs）来解决协同Perception-Action-Communication（PAC）循环中的信息共享和行为选择问题。</li>
<li>results: 该paper使用 asynchronous PAC 框架，并使用分布式GNNs来计算导航动作和生成通信信息，实现了大规模机器人群体的协同导航和覆盖控制。<details>
<summary>Abstract</summary>
Collaboration in large robot swarms to achieve a common global objective is a challenging problem in large environments due to limited sensing and communication capabilities. The robots must execute a Perception-Action-Communication (PAC) loop -- they perceive their local environment, communicate with other robots, and take actions in real time. A fundamental challenge in decentralized PAC systems is to decide what information to communicate with the neighboring robots and how to take actions while utilizing the information shared by the neighbors. Recently, this has been addressed using Graph Neural Networks (GNNs) for applications such as flocking and coverage control. Although conceptually, GNN policies are fully decentralized, the evaluation and deployment of such policies have primarily remained centralized or restrictively decentralized. Furthermore, existing frameworks assume sequential execution of perception and action inference, which is very restrictive in real-world applications. This paper proposes a framework for asynchronous PAC in robot swarms, where decentralized GNNs are used to compute navigation actions and generate messages for communication. In particular, we use aggregated GNNs, which enable the exchange of hidden layer information between robots for computational efficiency and decentralized inference of actions. Furthermore, the modules in the framework are asynchronous, allowing robots to perform sensing, extracting information, communication, action inference, and control execution at different frequencies. We demonstrate the effectiveness of GNNs executed in the proposed framework in navigating large robot swarms for collaborative coverage of large environments.
</details>
<details>
<summary>摘要</summary>
协作在大型机器人群体中实现共同全球目标是一个具有挑战性的问题，尤其在受限感知和通信能力的大环境中。机器人必须在实时中执行感知-行动-通信（PAC）循环——它们感知当地环境，与其他机器人交流，并在实时中采取行动。在分布式PAC系统中，决定如何与邻居机器人交流信息以及如何在基于这些信息的情况下采取行动是一个基本挑战。在最近的研究中，使用图神经网络（GNN）已经有效地解决了这个问题，并在鸟群控制和覆盖控制等应用中获得了成功。然而，GNN策略在概念上是完全分布式的，但是评估和部署这些策略却主要保留在中央化或部分分布式的方式下。此外，现有的框架假设Sequential感知和行动推理，这是实际应用中非常限制的。本文提出了一个 asynchronous PAC 框架，其中分布式GNNs 用于计算导航行动并生成与其他机器人交流的消息。具体来说，我们使用聚合GNNs，可以在机器人之间交换隐藏层信息，以实现计算效率和分布式推理行动。此外，框架中的模块是异步的，allowing robots to perform sensing, extracting information, communication, action inference, and control execution at different frequencies。我们通过使用GNNs在提出的框架中实现了大型机器人群体的协同探索大环境，以实现共同覆盖大环境。
</details></li>
</ul>
<hr>
<h2 id="RadOnc-GPT-A-Large-Language-Model-for-Radiation-Oncology"><a href="#RadOnc-GPT-A-Large-Language-Model-for-Radiation-Oncology" class="headerlink" title="RadOnc-GPT: A Large Language Model for Radiation Oncology"></a>RadOnc-GPT: A Large Language Model for Radiation Oncology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10160">http://arxiv.org/abs/2309.10160</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengliang Liu, Peilong Wang, Yiwei Li, Jason Holmes, Peng Shu, Lian Zhang, Chenbin Liu, Ninghao Liu, Dajiang Zhu, Xiang Li, Quanzheng Li, Samir H. Patel, Terence T. Sio, Tianming Liu, Wei Liu</li>
<li>for: 这个论文是为了研究透明度高的语言模型在放射科医疗领域中的应用。</li>
<li>methods: 这个论文使用了高级调教方法来特化语言模型，并在Mayo医院的大量放射科病人病历和诊断记录中进行了finetuning。模型包括三个关键任务：生成放射疗程治疗方案、确定最佳放射方式、以及基于病人诊断细节提供诊断描述和ICD代码。</li>
<li>results: 对比普通大语言模型的输出，RadOnc-GPT生成的输出显示了显著提高的明确性、特点性和клиниче重要性。研究表明，通过特化语言模型使用域专知如RadOnc-GPT进行练习，可以实现高度专业的医疗领域中的转变能力。<details>
<summary>Abstract</summary>
This paper presents RadOnc-GPT, a large language model specialized for radiation oncology through advanced tuning methods. RadOnc-GPT was finetuned on a large dataset of radiation oncology patient records and clinical notes from the Mayo Clinic. The model employs instruction tuning on three key tasks - generating radiotherapy treatment regimens, determining optimal radiation modalities, and providing diagnostic descriptions/ICD codes based on patient diagnostic details. Evaluations conducted by having radiation oncologists compare RadOnc-GPT impressions to general large language model impressions showed that RadOnc-GPT generated outputs with significantly improved clarity, specificity, and clinical relevance. The study demonstrated the potential of using large language models fine-tuned using domain-specific knowledge like RadOnc-GPT to achieve transformational capabilities in highly specialized healthcare fields such as radiation oncology.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Q-Transformer-Scalable-Offline-Reinforcement-Learning-via-Autoregressive-Q-Functions"><a href="#Q-Transformer-Scalable-Offline-Reinforcement-Learning-via-Autoregressive-Q-Functions" class="headerlink" title="Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions"></a>Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10150">http://arxiv.org/abs/2309.10150</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lucidrains/q-transformer">https://github.com/lucidrains/q-transformer</a></li>
<li>paper_authors: Yevgen Chebotar, Quan Vuong, Alex Irpan, Karol Hausman, Fei Xia, Yao Lu, Aviral Kumar, Tianhe Yu, Alexander Herzog, Karl Pertsch, Keerthana Gopalakrishnan, Julian Ibarz, Ofir Nachum, Sumedh Sontakke, Grecia Salazar, Huong T Tran, Jodilyn Peralta, Clayton Tan, Deeksha Manjunath, Jaspiar Singht, Brianna Zitkovich, Tomas Jackson, Kanishka Rao, Chelsea Finn, Sergey Levine</li>
<li>for: 这个论文是为了提出一种可扩展的生成学习方法，用于从大量的离线数据集中训练多任务策略。</li>
<li>methods: 该方法使用Transformer提供一种可扩展的Q函数表示，通过离线时间差反射来训练Q函数。具体来说，每个动作维度被精确化，并将每个动作维度的Q值表示为独立的token。这使得可以应用高容量的序列模型技术来进行Q学习。</li>
<li>results: 作者们表明，Q-Transformer比前一代离线RL算法和依 PDOurt学习技术在一个大型、多样化的真实世界机器人操作任务集中表现更好。详细的实验结果和项目网站可以在<a target="_blank" rel="noopener" href="https://q-transformer.github.io上找到./">https://q-transformer.github.io上找到。</a><details>
<summary>Abstract</summary>
In this work, we present a scalable reinforcement learning method for training multi-task policies from large offline datasets that can leverage both human demonstrations and autonomously collected data. Our method uses a Transformer to provide a scalable representation for Q-functions trained via offline temporal difference backups. We therefore refer to the method as Q-Transformer. By discretizing each action dimension and representing the Q-value of each action dimension as separate tokens, we can apply effective high-capacity sequence modeling techniques for Q-learning. We present several design decisions that enable good performance with offline RL training, and show that Q-Transformer outperforms prior offline RL algorithms and imitation learning techniques on a large diverse real-world robotic manipulation task suite. The project's website and videos can be found at https://q-transformer.github.io
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们提出了一种可扩展的强化学习方法，用于从大量离线数据集中训练多任务策略，并可以利用人类示范和自动收集的数据。我们的方法使用Transformer提供一种可扩展的表示方法，用于通过离线时间差备份学习Q值函数。因此，我们称这种方法为Q-Transformer。通过离线动作维度的精细化和每个动作维度的Q值函数的表示为分开的token，我们可以应用高容量序列模型技术进行Q学习。我们提出了一些设计决策，以实现在离线RL训练中良好的性能，并证明Q-Transformer在一个大型多样化的实际世界机器人操作任务集中超过了先前的离线RL算法和模仿学习技术。项目的网站和视频可以在https://q-transformer.github.io找到。
</details></li>
</ul>
<hr>
<h2 id="Analysis-of-the-Memorization-and-Generalization-Capabilities-of-AI-Agents-Are-Continual-Learners-Robust"><a href="#Analysis-of-the-Memorization-and-Generalization-Capabilities-of-AI-Agents-Are-Continual-Learners-Robust" class="headerlink" title="Analysis of the Memorization and Generalization Capabilities of AI Agents: Are Continual Learners Robust?"></a>Analysis of the Memorization and Generalization Capabilities of AI Agents: Are Continual Learners Robust?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10149">http://arxiv.org/abs/2309.10149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minsu Kim, Walid Saad</li>
<li>for: The paper is written for the practical deployment of continual learning (CL) applications, such as autonomous vehicles or robotics, in dynamic environments.</li>
<li>methods: The proposed CL framework uses a capacity-limited memory to save previously observed environmental information and mitigate forgetting issues. The algorithm samples data points from the memory to estimate the distribution of risks over environmental change and obtain robust predictors.</li>
<li>results: The proposed algorithm outperforms memory-based CL baselines across all environments while significantly improving the generalization performance on unseen target environments.<details>
<summary>Abstract</summary>
In continual learning (CL), an AI agent (e.g., autonomous vehicles or robotics) learns from non-stationary data streams under dynamic environments. For the practical deployment of such applications, it is important to guarantee robustness to unseen environments while maintaining past experiences. In this paper, a novel CL framework is proposed to achieve robust generalization to dynamic environments while retaining past knowledge. The considered CL agent uses a capacity-limited memory to save previously observed environmental information to mitigate forgetting issues. Then, data points are sampled from the memory to estimate the distribution of risks over environmental change so as to obtain predictors that are robust with unseen changes. The generalization and memorization performance of the proposed framework are theoretically analyzed. This analysis showcases the tradeoff between memorization and generalization with the memory size. Experiments show that the proposed algorithm outperforms memory-based CL baselines across all environments while significantly improving the generalization performance on unseen target environments.
</details>
<details>
<summary>摘要</summary>
在连续学习（CL）中，一个AI机器人（例如自动驾驶车或机器人）在动态环境中学习非站ARY数据流。为实际应用这些应用程序，保证对未seen环境的Robustness是非常重要。本文提出了一种新的CL框架，以实现对动态环境的Robust generalization，同时保持过去经验。考虑的CL机器人使用有限容量的记忆来减轻忘记问题。然后，从记忆中采样数据点，以估计环境变化中的风险分布，以获得对未seen变化的Robust预测器。本框架的总体和记忆性性能被理论分析。这种分析显示了记忆大小与Generalization和Memorization之间的贸易。实验表明，提出的算法在所有环境下都超过了记忆基eline，并显著提高了对未seen目标环境的总体性能。
</details></li>
</ul>
<hr>
<h2 id="Human-Gait-Recognition-using-Deep-Learning-A-Comprehensive-Review"><a href="#Human-Gait-Recognition-using-Deep-Learning-A-Comprehensive-Review" class="headerlink" title="Human Gait Recognition using Deep Learning: A Comprehensive Review"></a>Human Gait Recognition using Deep Learning: A Comprehensive Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10144">http://arxiv.org/abs/2309.10144</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Imran Sharif, Mehwish Mehmood, Muhammad Irfan Sharif, Md Palash Uddin</li>
<li>For: This paper provides an overview of gait recognition (GR) technology and analyzes the environmental elements and complications that could affect it, with a focus on deep learning (DL) techniques employed for human GR.* Methods: The paper examines existing DL techniques used in GR, including those that address challenges such as shifting lighting conditions, fluctuations in gait patterns, and ensuring uniform performance evaluation across different protocols.* Results: The paper aims to generate new research opportunities in GR by analyzing the existing DL techniques and identifying potential areas for improvement.Here’s the same information in Simplified Chinese text, as requested:* For: 这篇论文提供了人体走姿识别（GR）技术的概述，以及识别环境因素和问题的分析，特别是使用深度学习（DL）技术进行人体GR。* Methods: 论文检查了现有的DL技术，包括对不同协议的性能评估、不同照明条件下的识别、走姿模式的变化和不确定性等问题的解决方案。* Results: 论文旨在通过分析现有的DL技术，探讨新的研究机遇，以提高GR技术的精度和可靠性。<details>
<summary>Abstract</summary>
Gait recognition (GR) is a growing biometric modality used for person identification from a distance through visual cameras. GR provides a secure and reliable alternative to fingerprint and face recognition, as it is harder to distinguish between false and authentic signals. Furthermore, its resistance to spoofing makes GR suitable for all types of environments. With the rise of deep learning, steadily improving strides have been made in GR technology with promising results in various contexts. As video surveillance becomes more prevalent, new obstacles arise, such as ensuring uniform performance evaluation across different protocols, reliable recognition despite shifting lighting conditions, fluctuations in gait patterns, and protecting privacy.This survey aims to give an overview of GR and analyze the environmental elements and complications that could affect it in comparison to other biometric recognition systems. The primary goal is to examine the existing deep learning (DL) techniques employed for human GR that may generate new research opportunities.
</details>
<details>
<summary>摘要</summary>
“人体步态识别”（GR）是一种在距离通过视频摄像头进行人员识别的快 разви得生物特征模式。GR提供了一种安全可靠的人识别方式，因为它更难于分辨假冒信号。此外，它对冒泡攻击具有抵抗力，因此适用于各种环境。随着深度学习技术的不断发展，GR技术在不同上下文中呈现出了扎实的成果。随着视频监测的普及，新的障碍物出现，例如确保各个协议的统一性评估、在不同照明条件下可靠的识别、步态变化的影响和隐私保护。本调查的目标是对GR和其他生物识别系统进行比较，分析环境因素和障碍，并检视现有的深度学习技术，以便在新的研究机遇上发掘新的可能性。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Low-Rank-GNN-Defense-Against-Structural-Attacks"><a href="#Efficient-Low-Rank-GNN-Defense-Against-Structural-Attacks" class="headerlink" title="Efficient Low-Rank GNN Defense Against Structural Attacks"></a>Efficient Low-Rank GNN Defense Against Structural Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10136">http://arxiv.org/abs/2309.10136</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdullah Alchihabi, Qing En, Yuhong Guo<br>for:这个研究旨在提出一种能够有效地防御对 graf neural network (GNN) 的攻击的方法，以提高 GNN 的安全性。methods:这个方法包括两个模组：粗略低维度估计模组和细节化估计模组。粗略低维度估计模组使用 truncated SVD 来初始化低维度边 Matrix 估计，并且作为 GNN 模型估计的起点。在细节化估计模组中，则与 GNN 模型共同学习低维度短镜� structure，并将其转换为低维度短镜� Matrix。results:实验结果显示，ELR-GNN 比Literature中的 GNN 防御方法更高效，同时也非常简单易于训练。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have been shown to possess strong representation abilities over graph data. However, GNNs are vulnerable to adversarial attacks, and even minor perturbations to the graph structure can significantly degrade their performance. Existing methods either are ineffective against sophisticated attacks or require the optimization of dense adjacency matrices, which is time-consuming and prone to local minima. To remedy this problem, we propose an Efficient Low-Rank Graph Neural Network (ELR-GNN) defense method, which aims to learn low-rank and sparse graph structures for defending against adversarial attacks, ensuring effective defense with greater efficiency. Specifically, ELR-GNN consists of two modules: a Coarse Low-Rank Estimation Module and a Fine-Grained Estimation Module. The first module adopts the truncated Singular Value Decomposition (SVD) to initialize the low-rank adjacency matrix estimation, which serves as a starting point for optimizing the low-rank matrix. In the second module, the initial estimate is refined by jointly learning a low-rank sparse graph structure with the GNN model. Sparsity is incorporated into the learned low-rank adjacency matrix by pruning weak connections, which can reduce redundant data while maintaining valuable information. As a result, instead of using the dense adjacency matrix directly, ELR-GNN can learn a low-rank and sparse estimate of it in a simple, efficient and easy to optimize manner. The experimental results demonstrate that ELR-GNN outperforms the state-of-the-art GNN defense methods in the literature, in addition to being very efficient and easy to train.
</details>
<details>
<summary>摘要</summary>
图ael neural networks (GNNs) 有显著的表示能力 sobre graph data, 但是 GNNs 是易受 adversarial 攻击的。现有的方法可能无法对 Sophisticated 攻击有效，或者需要优化 dense adjacency matrices，这是时间consuming 和受 Local minima 影响。为了解决这个问题，我们提出了一种高效的 Low-Rank Graph Neural Network (ELR-GNN) 防御方法，该方法 aimsto learn low-rank 和 sparse graph structures，以防止 adversarial 攻击。具体来说，ELR-GNN 由两个模块组成：Coarse Low-Rank Estimation Module 和 Fine-Grained Estimation Module。第一个模块使用 truncated Singular Value Decomposition (SVD) 初始化 low-rank adjacency matrix 估计，这个估计作为 GNN 模型优化 low-rank matrix 的起点。第二个模块使用 jointly learn low-rank sparse graph structure 和 GNN 模型，在这个过程中，约束 sparse 的 low-rank adjacency matrix 的学习。通过避免直接使用 dense adjacency matrix，ELR-GNN 可以学习一个简单、高效、易于优化的 low-rank 和 sparse estimate。实验结果表明，ELR-GNN 在文献中已经出现的 GNN 防御方法中表现出色，同时具有高效和易于训练的优点。
</details></li>
</ul>
<hr>
<h2 id="GDM-Dual-Mixup-for-Graph-Classification-with-Limited-Supervision"><a href="#GDM-Dual-Mixup-for-Graph-Classification-with-Limited-Supervision" class="headerlink" title="GDM: Dual Mixup for Graph Classification with Limited Supervision"></a>GDM: Dual Mixup for Graph Classification with Limited Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10134">http://arxiv.org/abs/2309.10134</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdullah Alchihabi, Yuhong Guo</li>
<li>for: 提高graph neural network（GNN）在graph classification任务上的性能，降低需要大量标注的图样例数量。</li>
<li>methods: 提议一种基于mixup的图像整合方法，Graph Dual Mixup（GDM），可以利用图例中的函数和结构信息来生成新的标注图样例。GDM首先使用图结构自动编码器学习图例的结构嵌入，然后在学习后的结构嵌入空间中应用mixup，生成新的图结构。同时，GDM直接将输入节点特征混合到图例中，以生成新的功能节点特征信息。</li>
<li>results: 实验结果表明，当标注图样例数量受限时，我们提议的方法可以大幅提高GNN的性能，并且可以增加图样例的多样性和难度。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) require a large number of labeled graph samples to obtain good performance on the graph classification task. The performance of GNNs degrades significantly as the number of labeled graph samples decreases. To reduce the annotation cost, it is therefore important to develop graph augmentation methods that can generate new graph instances to increase the size and diversity of the limited set of available labeled graph samples. In this work, we propose a novel mixup-based graph augmentation method, Graph Dual Mixup (GDM), that leverages both functional and structural information of the graph instances to generate new labeled graph samples. GDM employs a graph structural auto-encoder to learn structural embeddings of the graph samples, and then applies mixup to the structural information of the graphs in the learned structural embedding space and generates new graph structures from the mixup structural embeddings. As for the functional information, GDM applies mixup directly to the input node features of the graph samples to generate functional node feature information for new mixup graph instances. Jointly, the generated input node features and graph structures yield new graph samples which can supplement the set of original labeled graphs. Furthermore, we propose two novel Balanced Graph Sampling methods to enhance the balanced difficulty and diversity for the generated graph samples. Experimental results on the benchmark datasets demonstrate that our proposed method substantially outperforms the state-of-the-art graph augmentation methods when the labeled graphs are scarce.
</details>
<details>
<summary>摘要</summary>
GRAPH Neural Networks (GNNs) 需要大量标注的图样本以达到图分类任务的良好性能。 GNNs 的性能随着标注图样本的减少而明显下降。为了降低标注成本，因此需要开发图生成方法，以增加可用标注图样本的数量和多样性。在这种情况下，我们提出了一种基于 mixup 的图生成方法，即图双混合（GDM）。GDM 利用图样本的函数和结构信息来生成新的标注图样本。GDM 首先使用图结构自动编码器来学习图样本的结构嵌入，然后在学习的结构嵌入空间中应用 mixup 到图样本的结构信息，并生成新的图结构。在 terms of 功能信息，GDM 直接应用 mixup 到图样本的输入节点特征，以生成新的混合节点特征信息。在总的来说，生成的输入节点特征和图结构均可以产生新的图样本，可以补充原始的标注图样本。此外，我们还提出了两种新的平衡图样本采样方法，以提高生成的图样本的平衡难度和多样性。实验结果表明，我们提出的方法在标注图样本稀缺的情况下明显超越了当前的图生成方法。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Liquidity-Provision-in-Uniswap-V3-with-Deep-Reinforcement-Learning"><a href="#Adaptive-Liquidity-Provision-in-Uniswap-V3-with-Deep-Reinforcement-Learning" class="headerlink" title="Adaptive Liquidity Provision in Uniswap V3 with Deep Reinforcement Learning"></a>Adaptive Liquidity Provision in Uniswap V3 with Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10129">http://arxiv.org/abs/2309.10129</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haochen Zhang, Xi Chen, Lin F. Yang</li>
<li>for: 这个论文是为了解决Decentralized finance（DeFi）中的一些未解决的问题，如资金效益和市场风险，而设计的。</li>
<li>methods: 该论文使用了深度强化学习（DRL）方法，通过自适应调整资产价格范围，以最大化收益并减少市场风险。同时，通过对资产liquidity进行重新平衡，以中和价格变化风险。</li>
<li>results: 使用 simulations 的盈亏指标（PnL）基准，该方法在 ETH&#x2F;USDC 和 ETH&#x2F;USDT 池中表现出色，与现有基线相比，具有更高的收益。<details>
<summary>Abstract</summary>
Decentralized exchanges (DEXs) are a cornerstone of decentralized finance (DeFi), allowing users to trade cryptocurrencies without the need for third-party authorization. Investors are incentivized to deposit assets into liquidity pools, against which users can trade directly, while paying fees to liquidity providers (LPs). However, a number of unresolved issues related to capital efficiency and market risk hinder DeFi's further development. Uniswap V3, a leading and groundbreaking DEX project, addresses capital efficiency by enabling LPs to concentrate their liquidity within specific price ranges for deposited assets. Nevertheless, this approach exacerbates market risk, as LPs earn trading fees only when asset prices are within these predetermined brackets. To mitigate this issue, this paper introduces a deep reinforcement learning (DRL) solution designed to adaptively adjust these price ranges, maximizing profits and mitigating market risks. Our approach also neutralizes price-change risks by hedging the liquidity position through a rebalancing portfolio in a centralized futures exchange. The DRL policy aims to optimize trading fees earned by LPs against associated costs, such as gas fees and hedging expenses, which is referred to as loss-versus-rebalancing (LVR). Using simulations with a profit-and-loss (PnL) benchmark, our method demonstrates superior performance in ETH/USDC and ETH/USDT pools compared to existing baselines. We believe that this strategy not only offers investors a valuable asset management tool but also introduces a new incentive mechanism for DEX designers.
</details>
<details>
<summary>摘要</summary>
Decentralized exchange (DEX) 是 DeFi 中的一个重要基础设施，让用户可以不需要第三方授权进行 криптовалюencies 的交易。投资者可以将资产存入流动性池，并获得交易所得，而付出的费用则是给流动性提供者 (LP)。然而， DeFi 的进一步发展受到一些未解决的问题所限，包括资金效率和市场风险。Uniswap V3 是一个创新的 DEX 项目，它解决了资金效率的问题，让 LP 可以将流动性集中在特定价格范围内。然而，这种方法会增加市场风险，因为 LP 只在资产价格在预先定义的价格范围内时获得交易所得。为了解决这个问题，这篇论文提出了一个深度学习 (DRL) 解决方案，可以适应地调整这些价格范围，以最大化 LP 的交易所得和减少市场风险。我们的方法还可以消除价格变化的风险，通过在中央期货交易所进行调整资产位置的投资策略。DRL 政策的目标是将 LP 的交易所得与相关成本（如 gas 费用和投资策略成本）进行对抗，这被称为loss-versus-rebalancing (LVR)。使用 simulations 中的盈亏检测 (PnL) benchmark，我们的方法在 ETH/USDC 和 ETH/USDT 流动性池中表现出色，较 existing 基准线超越。我们认为，这种策略不仅为投资者提供了一个有价的资产管理工具，而且也为 DEX 设计师引入了一个新的奖励机制。
</details></li>
</ul>
<hr>
<h2 id="AR-TTA-A-Simple-Method-for-Real-World-Continual-Test-Time-Adaptation"><a href="#AR-TTA-A-Simple-Method-for-Real-World-Continual-Test-Time-Adaptation" class="headerlink" title="AR-TTA: A Simple Method for Real-World Continual Test-Time Adaptation"></a>AR-TTA: A Simple Method for Real-World Continual Test-Time Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10109">http://arxiv.org/abs/2309.10109</a></li>
<li>repo_url: None</li>
<li>paper_authors: Damian Sójka, Sebastian Cygert, Bartłomiej Twardowski, Tomasz Trzciński</li>
<li>for: 这个研究旨在验证时间适应方法的有效性，以应对实际世界中的变化。</li>
<li>methods: 本研究使用了自我训练框架，并将小型快照缓存 buffer  integrate 以增加模型稳定性和动态适应。</li>
<li>results: 提出的 AR-TTA 方法在实验和更加实际的标准资料上进行了比较，并在不同的 TTA 情况下显示了更高的效果和稳定性。<details>
<summary>Abstract</summary>
Test-time adaptation is a promising research direction that allows the source model to adapt itself to changes in data distribution without any supervision. Yet, current methods are usually evaluated on benchmarks that are only a simplification of real-world scenarios. Hence, we propose to validate test-time adaptation methods using the recently introduced datasets for autonomous driving, namely CLAD-C and SHIFT. We observe that current test-time adaptation methods struggle to effectively handle varying degrees of domain shift, often resulting in degraded performance that falls below that of the source model. We noticed that the root of the problem lies in the inability to preserve the knowledge of the source model and adapt to dynamically changing, temporally correlated data streams. Therefore, we enhance well-established self-training framework by incorporating a small memory buffer to increase model stability and at the same time perform dynamic adaptation based on the intensity of domain shift. The proposed method, named AR-TTA, outperforms existing approaches on both synthetic and more real-world benchmarks and shows robustness across a variety of TTA scenarios.
</details>
<details>
<summary>摘要</summary>
<<SYS>>这是一个推广研究方向，允许源模型在数据分布变化时自动适应。然而，现有方法通常会在对测试数据进行评估时使用对测试数据的简化。因此，我们提出使用自驾车领域的最近引入的数据集，即CLAD-C和SHIFT来验证测试适应方法。我们发现现有的测试适应方法对于不同程度的领域变化通常会导致表现下降，常常比源模型的表现低。我们发现这是因为无法保持源模型的知识并适应对于动态变化的、时间相关的数据流。因此，我们将已有的自养学框架加以小型快取buffer，以提高模型的稳定性，并在数据流中进行动态适应，以适应不同的测试适应enario。我们称之为AR-TTA，并证明它在 sintetic和更加实际的benchmark上表现出色，并且在多种测试适应scenario中展现了 Robustness。
</details></li>
</ul>
<hr>
<h2 id="Reasoning-about-the-Unseen-for-Efficient-Outdoor-Object-Navigation"><a href="#Reasoning-about-the-Unseen-for-Efficient-Outdoor-Object-Navigation" class="headerlink" title="Reasoning about the Unseen for Efficient Outdoor Object Navigation"></a>Reasoning about the Unseen for Efficient Outdoor Object Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10103">http://arxiv.org/abs/2309.10103</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quanting Xie, Tianyi Zhang, Kedi Xu, Matthew Johnson-Roberson, Yonatan Bisk</li>
<li>for: 本研究旨在探讨机器人在户外环境中的自主导航问题，与现有的室内环境导航研究不同，涵盖更广泛的实际应用场景。</li>
<li>methods: 本研究提出了一种新的任务OUTDOOR，一种新的大语言模型（LLM）准确预测未来的机制，以及一种新的计算感知成功指标来推动这一更复杂的领域的研究进步。</li>
<li>results: 本研究在 simulate 的飞行器和实际四脚机器人上实现了出色的结果，而无需先Mapping。我们的正式成果超越了直觉LLM-based方法。<details>
<summary>Abstract</summary>
Robots should exist anywhere humans do: indoors, outdoors, and even unmapped environments. In contrast, the focus of recent advancements in Object Goal Navigation(OGN) has targeted navigating in indoor environments by leveraging spatial and semantic cues that do not generalize outdoors. While these contributions provide valuable insights into indoor scenarios, the broader spectrum of real-world robotic applications often extends to outdoor settings. As we transition to the vast and complex terrains of outdoor environments, new challenges emerge. Unlike the structured layouts found indoors, outdoor environments lack clear spatial delineations and are riddled with inherent semantic ambiguities. Despite this, humans navigate with ease because we can reason about the unseen. We introduce a new task OUTDOOR, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain. Additionally, we show impressive results on both a simulated drone and physical quadruped in outdoor environments. Our agent has no premapping and our formalism outperforms naive LLM-based approaches
</details>
<details>
<summary>摘要</summary>
人工智能机器应该存在任何人类存在的地方：室内、室外和无法映射的环境中。然而，最近的对象目标导航（OGN）的进步主要集中在内部环境中，利用空间和semantic信息，这些信息不会在户外generalize。虽然这些贡献提供了有价值的室内场景研究，但广泛的实际机器人应用场景通常是户外设置。作为我们从室内环境传播到更加复杂的户外环境，新的挑战出现。与室内的结构化布局不同，户外环境缺乏明确的空间定义，充斥着内生的semantic抽象。尽管如此，人类能够轻松地导航，因为我们可以理解未经见过的事物。我们介绍了一个新任务OUTDOOR，一种新的大语言模型（LLM）准确预测未来的机制，以及一种新的computational感知成功指标，以推动研究在这个更复杂的领域。此外，我们在模拟的飞机和物理四肢机器人上实现了出色的成果在户外环境中。我们的机器人没有预mapping，我们的正式形式超越了没有经验LLM-based方法。
</details></li>
</ul>
<hr>
<h2 id="Data-Formulator-AI-powered-Concept-driven-Visualization-Authoring"><a href="#Data-Formulator-AI-powered-Concept-driven-Visualization-Authoring" class="headerlink" title="Data Formulator: AI-powered Concept-driven Visualization Authoring"></a>Data Formulator: AI-powered Concept-driven Visualization Authoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10094">http://arxiv.org/abs/2309.10094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenglong Wang, John Thompson, Bongshin Lee</li>
<li>for: 提高数据视觉化的艺术化能力，帮助作者快速创建高质量的数据视觉图表。</li>
<li>methods: 使用人工智能代理人实现概念绑定视觉 paradigma，自然语言或示例来定义数据概念，然后将其绑定到视觉通道上。</li>
<li>results: 通过人工智能代理人自动将输入数据转换为Visualization，创建高质量的数据视觉图表，并提供反馈来帮助作者查看和理解结果。<details>
<summary>Abstract</summary>
With most modern visualization tools, authors need to transform their data into tidy formats to create visualizations they want. Because this requires experience with programming or separate data processing tools, data transformation remains a barrier in visualization authoring. To address this challenge, we present a new visualization paradigm, concept binding, that separates high-level visualization intents and low-level data transformation steps, leveraging an AI agent. We realize this paradigm in Data Formulator, an interactive visualization authoring tool. With Data Formulator, authors first define data concepts they plan to visualize using natural languages or examples, and then bind them to visual channels. Data Formulator then dispatches its AI-agent to automatically transform the input data to surface these concepts and generate desired visualizations. When presenting the results (transformed table and output visualizations) from the AI agent, Data Formulator provides feedback to help authors inspect and understand them. A user study with 10 participants shows that participants could learn and use Data Formulator to create visualizations that involve challenging data transformations, and presents interesting future research directions.
</details>
<details>
<summary>摘要</summary>
现代视觉工具中，作者通常需要将数据转换成整齐的格式，以创建他们想要的视觉。由于这需要编程经验或分离的数据处理工具，数据转换仍然是视觉作者的挑战。为解决这个挑战，我们提出了一种新的视觉 парадиг，即概念绑定。我们在数据形成器中实现了这种 парадиг，这是一个交互式的视觉作业工具。作者首先使用自然语言或示例来定义他们计划要visualize的数据概念，然后将它们绑定到视觉通道上。数据形成器然后将其人工智能代理发送到自动将输入数据转换成Surface这些概念和生成所需的视觉。当presenting the results（转换后的表格和输出视觉）时，数据形成器提供反馈，帮助作者检查和理解它们。一个参与者学习研究表明，参与者可以快速学习并使用数据形成器创建包含复杂数据转换的视觉。这个研究还提出了一些未来研究的可能性。
</details></li>
</ul>
<hr>
<h2 id="Conformal-Temporal-Logic-Planning-using-Large-Language-Models-Knowing-When-to-Do-What-and-When-to-Ask-for-Help"><a href="#Conformal-Temporal-Logic-Planning-using-Large-Language-Models-Knowing-When-to-Do-What-and-When-to-Ask-for-Help" class="headerlink" title="Conformal Temporal Logic Planning using Large Language Models: Knowing When to Do What and When to Ask for Help"></a>Conformal Temporal Logic Planning using Large Language Models: Knowing When to Do What and When to Ask for Help</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10092">http://arxiv.org/abs/2309.10092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Wang, Jiaming Tong, Kaiyuan Tan, Yevgeniy Vorobeychik, Yiannis Kantaros</li>
<li>for: 本研究旨在提出一种新的机器人运动规划问题，即使用自然语言（NL）表述多个高级次任务，并通过时间和逻辑顺序来描述这些任务。</li>
<li>methods: 本研究使用LTL（ Linear Temporal Logic）定义了NL基于的原子操作符来正式定义这些任务，而不是使用相关的规划方法，这些方法定义LTL任务 sobre atomic predicate capturing desired low-level system configurations。我们的目标是设计满足LTL任务的机器人计划。</li>
<li>results: 我们提出了HERACLEs，一种嵌入了现有工具的层次匹配自然语言规划器，包括(i) 自动机理论来确定需要完成的NL基于的子任务; (ii) 大语言模型来设计满足这些子任务的机器人计划; 和 (iii) 匹配预测来 probabilistically about correctness of the designed plans and mission satisfaction, and determine if external assistance is required. 我们进行了广泛的比较实验，并提供了项目网站ltl-llm.github.io。<details>
<summary>Abstract</summary>
This paper addresses a new motion planning problem for mobile robots tasked with accomplishing multiple high-level sub-tasks, expressed using natural language (NL), in a temporal and logical order. To formally define such missions, we leverage LTL defined over NL-based atomic predicates modeling the considered NL-based sub-tasks. This is contrast to related planning approaches that define LTL tasks over atomic predicates capturing desired low-level system configurations. Our goal is to design robot plans that satisfy LTL tasks defined over NL-based atomic propositions. A novel technical challenge arising in this setup lies in reasoning about correctness of a robot plan with respect to such LTL-encoded tasks. To address this problem, we propose HERACLEs, a hierarchical conformal natural language planner, that relies on a novel integration of existing tools that include (i) automata theory to determine the NL-specified sub-task the robot should accomplish next to make mission progress; (ii) Large Language Models to design robot plans satisfying these sub-tasks; and (iii) conformal prediction to reason probabilistically about correctness of the designed plans and mission satisfaction and to determine if external assistance is required. We provide extensive comparative experiments on mobile manipulation tasks. The project website is ltl-llm.github.io.
</details>
<details>
<summary>摘要</summary>
A novel technical challenge arises in this setup, as we must reason about the correctness of a robot plan with respect to such LTL-encoded tasks. To address this, we propose HERACLEs, a hierarchical conformal natural language planner that integrates existing tools:1. Automata theory to determine the next NL-specified sub-task the robot should accomplish for mission progress.2. Large Language Models to design robot plans satisfying these sub-tasks.3. Conformal prediction to reason probabilistically about the correctness of the designed plans and mission satisfaction, and to determine if external assistance is required.We provide extensive comparative experiments on mobile manipulation tasks. For more information, please visit the project website at ltl-llm.github.io.
</details></li>
</ul>
<hr>
<h2 id="Unified-Coarse-to-Fine-Alignment-for-Video-Text-Retrieval"><a href="#Unified-Coarse-to-Fine-Alignment-for-Video-Text-Retrieval" class="headerlink" title="Unified Coarse-to-Fine Alignment for Video-Text Retrieval"></a>Unified Coarse-to-Fine Alignment for Video-Text Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10091">http://arxiv.org/abs/2309.10091</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ziyang412/ucofia">https://github.com/ziyang412/ucofia</a></li>
<li>paper_authors: Ziyang Wang, Yi-Lin Sung, Feng Cheng, Gedas Bertasius, Mohit Bansal</li>
<li>for: 这个论文的目的是提出一种基于CLIP的视频-文本关联模型，以解决视频-文本关联任务中的找到正确的视频问题。</li>
<li>methods: 该模型使用了粗细grained或细化的对齐方法，并应用了一个交互性 Similarity Aggregation模块 (ISA)，以考虑不同的视觉特征的重要性而对alignment进行重新权重。</li>
<li>results: 该模型在多个视频-文本关联数据集上表现出色，与之前的CLIP-based方法相比，实现了2.4%、1.4%和1.3%的提升。<details>
<summary>Abstract</summary>
The canonical approach to video-text retrieval leverages a coarse-grained or fine-grained alignment between visual and textual information. However, retrieving the correct video according to the text query is often challenging as it requires the ability to reason about both high-level (scene) and low-level (object) visual clues and how they relate to the text query. To this end, we propose a Unified Coarse-to-fine Alignment model, dubbed UCoFiA. Specifically, our model captures the cross-modal similarity information at different granularity levels. To alleviate the effect of irrelevant visual clues, we also apply an Interactive Similarity Aggregation module (ISA) to consider the importance of different visual features while aggregating the cross-modal similarity to obtain a similarity score for each granularity. Finally, we apply the Sinkhorn-Knopp algorithm to normalize the similarities of each level before summing them, alleviating over- and under-representation issues at different levels. By jointly considering the crossmodal similarity of different granularity, UCoFiA allows the effective unification of multi-grained alignments. Empirically, UCoFiA outperforms previous state-of-the-art CLIP-based methods on multiple video-text retrieval benchmarks, achieving 2.4%, 1.4% and 1.3% improvements in text-to-video retrieval R@1 on MSR-VTT, Activity-Net, and DiDeMo, respectively. Our code is publicly available at https://github.com/Ziyang412/UCoFiA.
</details>
<details>
<summary>摘要</summary>
“ Traditional video-text retrieval methods often rely on coarse-grained or fine-grained alignment between visual and textual information. However, accurately retrieving the correct video based on a text query can be challenging, as it requires the ability to reason about both high-level (scene) and low-level (object) visual clues and how they relate to the text query. To address this challenge, we propose a Unified Coarse-to-fine Alignment model (UCoFiA). Our model captures cross-modal similarity information at different granularity levels and uses an Interactive Similarity Aggregation module (ISA) to consider the importance of different visual features when aggregating cross-modal similarity. Additionally, we use the Sinkhorn-Knopp algorithm to normalize the similarities of each level before summing them, alleviating over- and under-representation issues at different levels. By jointly considering the cross-modal similarity of different granularity, UCoFiA enables effective unification of multi-grained alignments. Empirical results show that UCoFiA outperforms previous state-of-the-art CLIP-based methods on multiple video-text retrieval benchmarks, achieving 2.4%, 1.4%, and 1.3% improvements in text-to-video retrieval R@1 on MSR-VTT, Activity-Net, and DiDeMo, respectively. Our code is publicly available at <https://github.com/Ziyang412/UCoFiA>.”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format instead.
</details></li>
</ul>
<hr>
<h2 id="HTEC-Human-Transcription-Error-Correction"><a href="#HTEC-Human-Transcription-Error-Correction" class="headerlink" title="HTEC: Human Transcription Error Correction"></a>HTEC: Human Transcription Error Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10089">http://arxiv.org/abs/2309.10089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanbo Sun, Jian Gao, Xiaomin Wu, Anjie Fang, Cheng Cao, Zheng Du</li>
<li>for: 提高自动语音识别（ASR）模型的训练和提升</li>
<li>methods: 提出了一种基于人工纠正的人类纠正错误（HTEC）方法，包括两个阶段：检测错误的“Trans-Checker”模型和填充错误位置的“Trans-Filler”模型，以及一个包含多种修正操作的整体修正操作列表</li>
<li>results: HTEC比其他方法大幅提高了WER表现，并在人工纠正时超过人类注释员2.2%至4.5%的WER表现，并且在辅助人类注释员时提高了译录质量15.1% без损失翻译速度。<details>
<summary>Abstract</summary>
High-quality human transcription is essential for training and improving Automatic Speech Recognition (ASR) models. Recent study~\cite{libricrowd} has found that every 1% worse transcription Word Error Rate (WER) increases approximately 2% ASR WER by using the transcriptions to train ASR models. Transcription errors are inevitable for even highly-trained annotators. However, few studies have explored human transcription correction. Error correction methods for other problems, such as ASR error correction and grammatical error correction, do not perform sufficiently for this problem. Therefore, we propose HTEC for Human Transcription Error Correction. HTEC consists of two stages: Trans-Checker, an error detection model that predicts and masks erroneous words, and Trans-Filler, a sequence-to-sequence generative model that fills masked positions. We propose a holistic list of correction operations, including four novel operations handling deletion errors. We further propose a variant of embeddings that incorporates phoneme information into the input of the transformer. HTEC outperforms other methods by a large margin and surpasses human annotators by 2.2% to 4.5% in WER. Finally, we deployed HTEC to assist human annotators and showed HTEC is particularly effective as a co-pilot, which improves transcription quality by 15.1% without sacrificing transcription velocity.
</details>
<details>
<summary>摘要</summary>
高品质人工转录是训练和改进自动语音识别（ASR）模型的重要前提。据 latest study 发现，每1%更差的转录单词错误率（WER）会提高约2% ASR WER。然而，有限的研究探讨了人工转录更正。其他问题的错误修正方法，如ASR错误修正和 grammatical error correction，在这个问题上并不够。因此，我们提出了 HTEC  для人工转录错误修正。 HTEC 包括两个阶段：转错检测模型（Trans-Checker），可以预测和覆盖错误单词，以及转换模型（Trans-Filler），可以填充覆盖的位置。我们提出了一份总体的修正操作列表，包括四种新的操作来处理删除错误。此外，我们还提出了一种包含音频信息的变换 embedding，用于输入转换器。 HTEC 在其他方法之上大幅提高，并在人工标注员之上提高了2.2%到4.5%的 WER。最后，我们将 HTEC 部署到人工标注员的助手，并证明 HTEC 作为助手可以提高转录质量15.1%，无需减少转录速度。
</details></li>
</ul>
<hr>
<h2 id="GAME-Generalized-deep-learning-model-towards-multimodal-data-integration-for-early-screening-of-adolescent-mental-disorders"><a href="#GAME-Generalized-deep-learning-model-towards-multimodal-data-integration-for-early-screening-of-adolescent-mental-disorders" class="headerlink" title="GAME: Generalized deep learning model towards multimodal data integration for early screening of adolescent mental disorders"></a>GAME: Generalized deep learning model towards multimodal data integration for early screening of adolescent mental disorders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10077">http://arxiv.org/abs/2309.10077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhicheng Du, Chenyao Jiang, Xi Yuan, Shiyao Zhai, Zhengyang Lei, Shuyue Ma, Yang Liu, Qihui Ye, Chufan Xiao, Qiming Huang, Ming Xu, Dongmei Yu, Peiwu Qin</li>
<li>for: 这个研究旨在提供一个可靠的多modal数据收集系统，以早期识别青少年的情绪障碍。</li>
<li>methods: 研究人员使用了一个Android应用程序，该应用程序包含了多个游戏和聊天纪录功能，并将其部署在一个便携式的机器人上，以萤幕3,783名中学生的情绪状态。</li>
<li>results: 研究人员发现，这个系统可以实时识别青少年的情绪障碍，并且具有73.34%-92.77%的准确率和71.32%-91.06%的F1分数。此外，研究人员发现每个感知modal都会在不同的情绪障碍中发挥不同的作用，这显示了这个系统的可解释性。<details>
<summary>Abstract</summary>
The timely identification of mental disorders in adolescents is a global public health challenge.Single factor is difficult to detect the abnormality due to its complex and subtle nature. Additionally, the generalized multimodal Computer-Aided Screening (CAS) systems with interactive robots for adolescent mental disorders are not available. Here, we design an android application with mini-games and chat recording deployed in a portable robot to screen 3,783 middle school students and construct the multimodal screening dataset, including facial images, physiological signs, voice recordings, and textual transcripts.We develop a model called GAME (Generalized Model with Attention and Multimodal EmbraceNet) with novel attention mechanism that integrates cross-modal features into the model. GAME evaluates adolescent mental conditions with high accuracy (73.34%-92.77%) and F1-Score (71.32%-91.06%).We find each modality contributes dynamically to the mental disorders screening and comorbidities among various mental disorders, indicating the feasibility of explainable model. This study provides a system capable of acquiring multimodal information and constructs a generalized multimodal integration algorithm with novel attention mechanisms for the early screening of adolescent mental disorders.
</details>
<details>
<summary>摘要</summary>
全球公共卫生中既早发现青少年精神疾病是一个大型公共卫生挑战。单一因素难以检测异常性，因为精神疾病的特征复杂且柔和。此外，普遍的多模态计算机助手系统 для青少年精神疾病并不可用。我们设计了一个安卓应用程序，其中包含了小游戏和聊天记录，并在可携带的机器人上部署。我们为3783名中学生进行了屏幕测试，并建立了多模态检测数据集，包括脸部图像、生物学指标、声音记录和文本译文。我们开发了一个名为“总体模型”（GAME）的模型，其中包含了新的注意力机制，可以将多种模式的特征集成到模型中。GAME模型可以准确地评估青少年精神状况（73.34%-92.77%）和F1分数（71.32%-91.06%）。我们发现每种模式在精神疾病检测中都有独特的贡献，表明模型的可解释性。本研究提供了一种可以获取多模式信息的系统，并构建了一种普遍的多模式融合算法，其中包含了新的注意力机制，用于早期检测青少年精神疾病。
</details></li>
</ul>
<hr>
<h2 id="Sex-based-Disparities-in-Brain-Aging-A-Focus-on-Parkinson’s-Disease"><a href="#Sex-based-Disparities-in-Brain-Aging-A-Focus-on-Parkinson’s-Disease" class="headerlink" title="Sex-based Disparities in Brain Aging: A Focus on Parkinson’s Disease"></a>Sex-based Disparities in Brain Aging: A Focus on Parkinson’s Disease</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10069">http://arxiv.org/abs/2309.10069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iman Beheshti, Samuel Booth, Ji Hyun Ko</li>
<li>for: 这研究旨在了解帕金сон病患者的大脑年龄差（brain-predicted age difference，简称brain-PAD）与性别之间的关系，以及这种关系对患者的诊断和治疗的影响。</li>
<li>methods: 研究使用T1束重磁共振成像（MRI）驱动的大脑预测年龄差（brain-PAD）计算方法，并使用线性回归模型研究帕金сон病患者的大脑预测年龄差与临床变量之间的关系。</li>
<li>results: 研究发现，♂️帕金сон病患者的大脑预测年龄差较♀帕金сон病患者高，而♂️帕金сон病患者的大脑预测年龄差与总脑功能、睡眠行为障碍、视空间acuity和唱杯损害之间存在 statistically significant 关系，♀帕金сон病患者则没有这种关系。<details>
<summary>Abstract</summary>
PD is linked to faster brain aging. Sex is recognized as an important factor in PD, such that males are twice as likely as females to have the disease and have more severe symptoms and a faster progression rate. Despite previous research, there remains a significant gap in understanding the function of sex in the process of brain aging in PD patients. The T1-weighted MRI-driven brain-predicted age difference was computed in a group of 373 PD patients from the PPMI database using a robust brain-age estimation framework that was trained on 949 healthy subjects. Linear regression models were used to investigate the association between brain-PAD and clinical variables in PD, stratified by sex. All female PD patients were used in the correlational analysis while the same number of males were selected based on propensity score matching method considering age, education level, age of symptom onset, and clinical symptom severity. Despite both patient groups being matched for demographics, motor and non-motor symptoms, it was observed that males with Parkinson's disease exhibited a significantly higher mean brain age-delta than their female counterparts . In the propensity score-matched PD male group, brain-PAD was found to be associated with a decline in general cognition, a worse degree of sleep behavior disorder, reduced visuospatial acuity, and caudate atrophy. Conversely, no significant links were observed between these factors and brain-PAD in the PD female group.
</details>
<details>
<summary>摘要</summary>
PD与更快的脑龄相关。性别被认为对PD有重要影响，男性比女性患有疾病的风险高， symptoms更严重，病程更快。 DESPITE PREVIOUS RESEARCH， THERE REMAINS A SIGNIFICANT GAP IN UNDERSTANDING THE FUNCTION OF SEX IN THE PROCESS OF BRAIN AGING IN PD PATIENTS. 使用PPMI数据库中的373名PD患者，通过一种可靠的脑龄估计框架，计算出脑龄差值（brain-PAD）。在PD患者中，按照性别分组，使用线性回归模型investigate brain-PAD与临床变量的关系。 female PD患者中，与brain-PAD相关的变量包括：脑部功能障碍、睡眠行为障碍、视空间acuity和caudate萎缩。 Conversely，在男性PD患者中，这些变量与brain-PAD无 statistically significant links。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Personalized-Impression-Generation-for-PET-Reports-Using-Large-Language-Models"><a href="#Automatic-Personalized-Impression-Generation-for-PET-Reports-Using-Large-Language-Models" class="headerlink" title="Automatic Personalized Impression Generation for PET Reports Using Large Language Models"></a>Automatic Personalized Impression Generation for PET Reports Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10066">http://arxiv.org/abs/2309.10066</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xtie97/pet-report-summarization">https://github.com/xtie97/pet-report-summarization</a></li>
<li>paper_authors: Xin Tie, Muheon Shin, Ali Pirasteh, Nevein Ibrahim, Zachary Huemann, Sharon M. Castellino, Kara M. Kelly, John Garrett, Junjie Hu, Steve Y. Cho, Tyler J. Bradshaw</li>
<li>For: The paper aims to determine if fine-tuned large language models (LLMs) can generate accurate, personalized impressions for whole-body PET reports.* Methods: The paper uses a corpus of PET reports to train 12 language models using the teacher-forcing algorithm, with the report findings as input and the clinical impressions as reference. The models are trained to learn physician-specific reporting styles by using an extra input token that encodes the reading physician’s identity.* Results: The paper finds that the fine-tuned PEGASUS model generates clinically acceptable impressions that are comparable in overall utility to those dictated by other physicians. Specifically, 89% of the personalized impressions generated by PEGASUS were considered clinically acceptable, with a mean utility score of 4.08&#x2F;5.Here is the simplified Chinese text for the three key information points:* For: 本研究目的是判断是否可以使用调整后的大语言模型（LLMs）生成准确、个性化的核医报告。* Methods: 本研究使用核医报告集来训练12种语言模型，使用教师压制算法，报告结果作为输入，临床印象作为参考。模型通过使用阅读医生标识符来学习医生特有的报告风格。* Results: 研究发现，调整后的PEGASUS模型可生成具有医学可接受性的个性化印象，与其他医生的印象相当。具体来说，PEGASUS模型生成的个性化印象中，89%被评估为医学可接受，平均用户满意度为4.08&#x2F;5。<details>
<summary>Abstract</summary>
Purpose: To determine if fine-tuned large language models (LLMs) can generate accurate, personalized impressions for whole-body PET reports. Materials and Methods: Twelve language models were trained on a corpus of PET reports using the teacher-forcing algorithm, with the report findings as input and the clinical impressions as reference. An extra input token encodes the reading physician's identity, allowing models to learn physician-specific reporting styles. Our corpus comprised 37,370 retrospective PET reports collected from our institution between 2010 and 2022. To identify the best LLM, 30 evaluation metrics were benchmarked against quality scores from two nuclear medicine (NM) physicians, with the most aligned metrics selecting the model for expert evaluation. In a subset of data, model-generated impressions and original clinical impressions were assessed by three NM physicians according to 6 quality dimensions and an overall utility score (5-point scale). Each physician reviewed 12 of their own reports and 12 reports from other physicians. Bootstrap resampling was used for statistical analysis. Results: Of all evaluation metrics, domain-adapted BARTScore and PEGASUSScore showed the highest Spearman's rho correlations (0.568 and 0.563) with physician preferences. Based on these metrics, the fine-tuned PEGASUS model was selected as the top LLM. When physicians reviewed PEGASUS-generated impressions in their own style, 89% were considered clinically acceptable, with a mean utility score of 4.08/5. Physicians rated these personalized impressions as comparable in overall utility to the impressions dictated by other physicians (4.03, P=0.41). Conclusion: Personalized impressions generated by PEGASUS were clinically useful, highlighting its potential to expedite PET reporting.
</details>
<details>
<summary>摘要</summary>
目的：检验大型语言模型（LLM）是否可以生成精准、个性化的全身PET报告印象。材料和方法：十二个语言模型通过教师强制算法进行训练，使用PET报告作为输入，并使用报告结果作为参考。模型中还包含一个特殊的输入 токен，用于学习各个physician的报告风格。我们的 corpus 包括2010年至2022年我们机构收集的37,370个Retrospective PET报告。为了选择最佳LLM，我们对30个评估指标进行了对比，并与两名核医学家（NM）的评分相对比较。在一部分数据中，模型生成的印象和原始临床印象被三名NMphysician根据6个质量维度和一个总体用户分数（5分级）进行评估。每名physician reviewed 12份自己的报告和12份其他physician的报告。我们使用 bootstrap 采样来进行统计分析。结果：所有评估指标中，适应域BARTScore和PEGASUSScore显示最高的斯佩曼氏相关度（0.568和0.563）。基于这些指标，我们选择了精心适应PEGASUS模型。当physician们评估PEGASUS生成的印象时，89%被评为临床可接受，平均用户分数为4.08/5。physicians 评估这些个性化印象的总体用户分数相当于他们自己dictated的印象的总体用户分数（4.03，P=0.41）。结论：PEGASUS生成的个性化印象是临床有用， highlighting its potential to expedite PET reporting。
</details></li>
</ul>
<hr>
<h2 id="Toward-collision-free-trajectory-for-autonomous-and-pilot-controlled-unmanned-aerial-vehicles"><a href="#Toward-collision-free-trajectory-for-autonomous-and-pilot-controlled-unmanned-aerial-vehicles" class="headerlink" title="Toward collision-free trajectory for autonomous and pilot-controlled unmanned aerial vehicles"></a>Toward collision-free trajectory for autonomous and pilot-controlled unmanned aerial vehicles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10064">http://arxiv.org/abs/2309.10064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaya Kuru, John Michael Pinder, Benjamin Jon Watkinson, Darren Ansell, Keith Vinning, Lee Moore, Chris Gilbert, Aadithya Sujit, David Jones</li>
<li>for: The paper is written for the purpose of developing an advanced collision management methodology for unmanned aerial vehicles (UAVs) to avoid mid-air collisions (MACs) with manned aeroplanes.</li>
<li>methods: The paper uses electronic conspicuity (EC) information made available by PilotAware Ltd and a reactive geometric conflict detection and resolution (CDR) technique to determine and execute time-optimal evasive collision avoidance (CA) manoeuvres.</li>
<li>results: The proposed methodology is demonstrated to be successful in avoiding collisions while limiting the deviation from the original trajectory in highly dynamic aerospace environments without requiring sophisticated sensors and prior training.Here are the three points in Simplified Chinese text:</li>
<li>for: 这篇论文是为了开发一种高级冲突管理方法，以避免无人飞行器（UAV）与人员飞机的中空冲突（MAC）。</li>
<li>methods: 这篇论文使用PilotAware Ltd提供的电子闪耀信息和一种反应几何冲突检测和解决技术，以确定和执行时间优化的避免冲突措施。</li>
<li>results: 提议的方法在高动态航空环境中成功地避免冲突，并限制偏离原轨迹的偏差。<details>
<summary>Abstract</summary>
For drones, as safety-critical systems, there is an increasing need for onboard detect & avoid (DAA) technology i) to see, sense or detect conflicting traffic or imminent non-cooperative threats due to their high mobility with multiple degrees of freedom and the complexity of deployed unstructured environments, and subsequently ii) to take the appropriate actions to avoid collisions depending upon the level of autonomy. The safe and efficient integration of UAV traffic management (UTM) systems with air traffic management (ATM) systems, using intelligent autonomous approaches, is an emerging requirement where the number of diverse UAV applications is increasing on a large scale in dense air traffic environments for completing swarms of multiple complex missions flexibly and simultaneously. Significant progress over the past few years has been made in detecting UAVs present in aerospace, identifying them, and determining their existing flight path. This study makes greater use of electronic conspicuity (EC) information made available by PilotAware Ltd in developing an advanced collision management methodology -- Drone Aware Collision Management (DACM) -- capable of determining and executing a variety of time-optimal evasive collision avoidance (CA) manoeuvres using a reactive geometric conflict detection and resolution (CDR) technique. The merits of the DACM methodology have been demonstrated through extensive simulations and real-world field tests in avoiding mid-air collisions (MAC) between UAVs and manned aeroplanes. The results show that the proposed methodology can be employed successfully in avoiding collisions while limiting the deviation from the original trajectory in highly dynamic aerospace without requiring sophisticated sensors and prior training.
</details>
<details>
<summary>摘要</summary>
For drones, as safety-critical systems, there is an increasing need for onboard detect & avoid (DAA) technology to see, sense or detect conflicting traffic or imminent non-cooperative threats due to their high mobility with multiple degrees of freedom and the complexity of deployed unstructured environments, and subsequently to take the appropriate actions to avoid collisions depending upon the level of autonomy. The safe and efficient integration of UAV traffic management (UTM) systems with air traffic management (ATM) systems, using intelligent autonomous approaches, is an emerging requirement where the number of diverse UAV applications is increasing on a large scale in dense air traffic environments for completing swarms of multiple complex missions flexibly and simultaneously. Significant progress has been made in recent years in detecting UAVs present in aerospace, identifying them, and determining their existing flight path. This study makes greater use of electronic conspicuity (EC) information provided by PilotAware Ltd in developing an advanced collision management methodology -- Drone Aware Collision Management (DACM) -- capable of determining and executing a variety of time-optimal evasive collision avoidance (CA) manoeuvres using a reactive geometric conflict detection and resolution (CDR) technique. The merits of the DACM methodology have been demonstrated through extensive simulations and real-world field tests in avoiding mid-air collisions (MAC) between UAVs and manned aeroplanes. The results show that the proposed methodology can be successfully employed in avoiding collisions while limiting the deviation from the original trajectory in highly dynamic aerospace without requiring sophisticated sensors and prior training.
</details></li>
</ul>
<hr>
<h2 id="Survey-of-Consciousness-Theory-from-Computational-Perspective"><a href="#Survey-of-Consciousness-Theory-from-Computational-Perspective" class="headerlink" title="Survey of Consciousness Theory from Computational Perspective"></a>Survey of Consciousness Theory from Computational Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10063">http://arxiv.org/abs/2309.10063</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Zihan Ding, Xiaoxi Wei, Yidan Xu</li>
<li>for: 本研究目的是 bridge 不同学科的意识理论，以计算机科学的视角来解释人类意识现象。</li>
<li>methods: 本文使用了多种方法，包括信息理论、量子物理学、认知心理学、生物物理学和计算机科学等，以寻求解释意识现象的方法。</li>
<li>results: 本研究通过对多种意识理论的汇总和分析，提出了一种计算机科学的意识模型，并讨论了现有的意识评价指标和计算机模型是否具有意识性。<details>
<summary>Abstract</summary>
Human consciousness has been a long-lasting mystery for centuries, while machine intelligence and consciousness is an arduous pursuit. Researchers have developed diverse theories for interpreting the consciousness phenomenon in human brains from different perspectives and levels. This paper surveys several main branches of consciousness theories originating from different subjects including information theory, quantum physics, cognitive psychology, physiology and computer science, with the aim of bridging these theories from a computational perspective. It also discusses the existing evaluation metrics of consciousness and possibility for current computational models to be conscious. Breaking the mystery of consciousness can be an essential step in building general artificial intelligence with computing machines.
</details>
<details>
<summary>摘要</summary>
人类意识已经是历史上一个长期的谜团，而机器智能和意识的研究则是一项艰难的探索。研究人员已经提出了多种解释人类大脑意识现象的理论，从不同的角度和水平来看。本文将从计算机科学的视角 survey 这些主要分支的意识理论，并讨论现有的意识评价指标以及现有的计算机模型是否具备意识性。破解意识之谜可能是建立通用人工智能机器的重要一步。
</details></li>
</ul>
<hr>
<h2 id="General-In-Hand-Object-Rotation-with-Vision-and-Touch"><a href="#General-In-Hand-Object-Rotation-with-Vision-and-Touch" class="headerlink" title="General In-Hand Object Rotation with Vision and Touch"></a>General In-Hand Object Rotation with Vision and Touch</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09979">http://arxiv.org/abs/2309.09979</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haozhi Qi, Brent Yi, Sudharshan Suresh, Mike Lambeta, Yi Ma, Roberto Calandra, Jitendra Malik</li>
<li>for: 这个论文是为了解决触摸式物体旋转的问题，通过多种感知输入来实现。</li>
<li>methods: 这个系统使用了仿真训练，并在真实的涂抹和 proprioceptive 感知输入下进行了线性激活。它还使用了一种可视感知和抓取Transformer来融合多modal 感知输入。</li>
<li>results: 相比之前的方法，这个系统在实际应用中表现出了显著的性能提升，并且Visual和感觉感知的重要性得到了证明。<details>
<summary>Abstract</summary>
We introduce RotateIt, a system that enables fingertip-based object rotation along multiple axes by leveraging multimodal sensory inputs. Our system is trained in simulation, where it has access to ground-truth object shapes and physical properties. Then we distill it to operate on realistic yet noisy simulated visuotactile and proprioceptive sensory inputs. These multimodal inputs are fused via a visuotactile transformer, enabling online inference of object shapes and physical properties during deployment. We show significant performance improvements over prior methods and the importance of visual and tactile sensing.
</details>
<details>
<summary>摘要</summary>
我们介绍RotateIt系统，可以通过多轴滚动来实现手指基于多轴滚动的物体旋转，利用多种感知输入。我们的系统在模拟环境中训练，有访问真实的物体形状和物理属性。然后我们将其逼真实的视觉感知和 proprioceptive 感知混合，通过视觉感知变换，在部署时进行在线的物体形状和物理属性的推断。我们显示了与先前方法相比有显著性能提升，以及视觉和感觉感知的重要性。
</details></li>
</ul>
<hr>
<h2 id="MindAgent-Emergent-Gaming-Interaction"><a href="#MindAgent-Emergent-Gaming-Interaction" class="headerlink" title="MindAgent: Emergent Gaming Interaction"></a>MindAgent: Emergent Gaming Interaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09971">http://arxiv.org/abs/2309.09971</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ran Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke Noda, Zilong Zheng, Song-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, Jianfeng Gao</li>
<li>for: 这paper是为了研究大语言模型在多智能体系统中的复杂计划和协调能力，以及如何在游戏中与人类NPC合作。</li>
<li>methods: 该paper使用了现有的游戏框架，并引入了一些新的技术，如需要理解协调者、与人类玩家合作via不完美化的指令，以及在几个shot提示下进行在Context learning。</li>
<li>results: 该paper introduce了一个新的游戏场景和benchmark，并通过了新的自动度量器CoS来评估多智能体协调效率。该paper的结果表明，大语言模型可以在游戏中协调多智能体，并且可以通过学习大语言资源来获得这些技能。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have the capacity of performing complex scheduling in a multi-agent system and can coordinate these agents into completing sophisticated tasks that require extensive collaboration. However, despite the introduction of numerous gaming frameworks, the community has insufficient benchmarks towards building general multi-agents collaboration infrastructure that encompass both LLM and human-NPCs collaborations. In this work, we propose a novel infrastructure - MindAgent - to evaluate planning and coordination emergent capabilities for gaming interaction. In particular, our infrastructure leverages existing gaming framework, to i) require understanding of the coordinator for a multi-agent system, ii) collaborate with human players via un-finetuned proper instructions, and iii) establish an in-context learning on few-shot prompt with feedback. Furthermore, we introduce CUISINEWORLD, a new gaming scenario and related benchmark that dispatch a multi-agent collaboration efficiency and supervise multiple agents playing the game simultaneously. We conduct comprehensive evaluations with new auto-metric CoS for calculating the collaboration efficiency. Finally, our infrastructure can be deployed into real-world gaming scenarios in a customized VR version of CUISINEWORLD and adapted in existing broader Minecraft gaming domain. We hope our findings on LLMs and the new infrastructure for general-purpose scheduling and coordination can help shed light on how such skills can be obtained by learning from large language corpora.
</details>
<details>
<summary>摘要</summary>
In this work, we propose a novel infrastructure called MindAgent to evaluate the planning and coordination capabilities of LLMs in gaming interactions. Our infrastructure leverages existing gaming frameworks to:1. Require understanding of the coordinator for a multi-agent system.2. Collaborate with human players via un-finetuned proper instructions.3. Establish in-context learning on few-shot prompts with feedback.Furthermore, we introduce CUISINEWORLD, a new gaming scenario and related benchmark that dispatches a multi-agent collaboration efficiency and supervises multiple agents playing the game simultaneously. We conduct comprehensive evaluations with a new auto-metric CoS for calculating collaboration efficiency.Our infrastructure can be deployed into real-world gaming scenarios in a customized VR version of CUISINEWORLD and adapted in existing broader Minecraft gaming domains. We hope our findings on LLMs and the new infrastructure for general-purpose scheduling and coordination can help shed light on how such skills can be obtained by learning from large language corpora.
</details></li>
</ul>
<hr>
<h2 id="How-to-Generate-Popular-Post-Headlines-on-Social-Media"><a href="#How-to-Generate-Popular-Post-Headlines-on-Social-Media" class="headerlink" title="How to Generate Popular Post Headlines on Social Media?"></a>How to Generate Popular Post Headlines on Social Media?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09949">http://arxiv.org/abs/2309.09949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhouxiang Fang, Min Yu, Zhendong Fu, Boning Zhang, Xuanwen Huang, Xiaoqi Tang, Yang Yang</li>
<li>for:  automatic generation of popular headlines on social media</li>
<li>methods:  Multiple preference-Extractors with Bidirectional and Auto-Regressive Transformers (BART)</li>
<li>results:  state-of-the-art performance compared with several advanced baselines, and ability to capture trends and personal styles.<details>
<summary>Abstract</summary>
Posts, as important containers of user-generated-content pieces on social media, are of tremendous social influence and commercial value. As an integral components of a post, the headline has a decisive contribution to the post's popularity. However, current mainstream method for headline generation is still manually writing, which is unstable and requires extensive human effort. This drives us to explore a novel research question: Can we automate the generation of popular headlines on social media? We collect more than 1 million posts of 42,447 celebrities from public data of Xiaohongshu, which is a well-known social media platform in China. We then conduct careful observations on the headlines of these posts. Observation results demonstrate that trends and personal styles are widespread in headlines on social medias and have significant contribution to posts's popularity. Motivated by these insights, we present MEBART, which combines Multiple preference-Extractors with Bidirectional and Auto-Regressive Transformers (BART), capturing trends and personal styles to generate popular headlines on social medias. We perform extensive experiments on real-world datasets and achieve state-of-the-art performance compared with several advanced baselines. In addition, ablation and case studies demonstrate that MEBART advances in capturing trends and personal styles.
</details>
<details>
<summary>摘要</summary>
posts是社交媒体上重要的用户生成内容容器，具有巨大的社会影响力和商业价值。在这些posts中，标题具有决定性的贡献，促进post的受欢迎程度。然而，现有主流的标题生成方法仍然是人工写作，它是不稳定的，需要大量的人工劳动。这使我们感到需要解决这个问题：可以自动生成社交媒体上的受欢迎标题吗？我们收集了来自Xiaohongshu社交媒体平台的公共数据，包含42,447名明星的post，并进行了仔细的观察。观察结果表明，社交媒体上的标题中广泛存在趋势和个人风格，这些趋势和风格对post的受欢迎程度有重要的贡献。我们发现这些趋势和风格的存在，驱动我们开发MEBART模型，这是基于多个偏好抽取器和双向自适应变换器（BART）的模型，可以捕捉到社交媒体上的趋势和个人风格，生成受欢迎的标题。我们在实际数据上进行了广泛的实验，与多个先进基elines进行比较，并实现了当前领域的最佳性能。此外，我们还进行了ablation和案例研究，以证明MEBART模型在捕捉趋势和个人风格方面的进步。
</details></li>
</ul>
<hr>
<h2 id="What-is-a-Fair-Diffusion-Model-Designing-Generative-Text-To-Image-Models-to-Incorporate-Various-Worldviews"><a href="#What-is-a-Fair-Diffusion-Model-Designing-Generative-Text-To-Image-Models-to-Incorporate-Various-Worldviews" class="headerlink" title="What is a Fair Diffusion Model? Designing Generative Text-To-Image Models to Incorporate Various Worldviews"></a>What is a Fair Diffusion Model? Designing Generative Text-To-Image Models to Incorporate Various Worldviews</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09944">http://arxiv.org/abs/2309.09944</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zoedesimone/diffusionworldviewer">https://github.com/zoedesimone/diffusionworldviewer</a></li>
<li>paper_authors: Zoe De Simone, Angie Boggust, Arvind Satyanarayan, Ashia Wilson</li>
<li>for: The paper aims to enhance bias mitigation in generative text-to-image (GTI) models by introducing a tool called DiffusionWorldViewer to analyze and manipulate the models’ attitudes, values, stories, and expectations of the world.</li>
<li>methods: The tool uses an interactive interface deployed as a web-based GUI and Jupyter Notebook plugin to categorize existing demographics of GTI-generated images and provide interactive methods to align image demographics with user worldviews.</li>
<li>results: In a study with 13 GTI users, the tool was found to allow users to represent their varied viewpoints about what GTI outputs are fair, challenging current notions of fairness that assume a universal worldview.Here’s the same information in Simplified Chinese text:</li>
<li>for: 论文旨在提高生成文本到图像（GTI）模型中的偏见减轻，通过引入DiffusionWorldViewer工具分析和 manipulate GTI模型对世界的态度、价值观、故事和期望的影响。</li>
<li>methods: DiffusionWorldViewer使用了一个交互式的网页UI和Jupyter Notebook插件，将GTI生成图像中的存在人类划分为不同类别，并提供了互动方法来与用户的世界观点对齐。</li>
<li>results: 在13名GTI用户的研究中，DiffusionWorldViewer被发现可以让用户表达他们对GTI输出是否公正的多种视点，挑战当前假设一个统一的世界观点的假设。<details>
<summary>Abstract</summary>
Generative text-to-image (GTI) models produce high-quality images from short textual descriptions and are widely used in academic and creative domains. However, GTI models frequently amplify biases from their training data, often producing prejudiced or stereotypical images. Yet, current bias mitigation strategies are limited and primarily focus on enforcing gender parity across occupations. To enhance GTI bias mitigation, we introduce DiffusionWorldViewer, a tool to analyze and manipulate GTI models' attitudes, values, stories, and expectations of the world that impact its generated images. Through an interactive interface deployed as a web-based GUI and Jupyter Notebook plugin, DiffusionWorldViewer categorizes existing demographics of GTI-generated images and provides interactive methods to align image demographics with user worldviews. In a study with 13 GTI users, we find that DiffusionWorldViewer allows users to represent their varied viewpoints about what GTI outputs are fair and, in doing so, challenges current notions of fairness that assume a universal worldview.
</details>
<details>
<summary>摘要</summary>
生成文本到图像（GTI）模型可以生成高质量的图像，但它们经常增强训练数据中的偏见，导致生成的图像具有偏见或�tereotype。然而，现有的偏见缓解策略主要集中在强制性域 occupations 的均衡上。为了提高 GTI 偏见缓解，我们介绍了DiffusionWorldViewer，一种分析和 manipulate GTI 模型对世界的看法、价值观、故事和期望的工具。通过在Web上部署的界面和 Jupyter Notebook 插件，DiffusionWorldViewer 可以分类 GTI 生成的图像中的现有民族、提供交互方式来与用户的世界观点相对应。在13名 GTI 用户的研究中，我们发现DiffusionWorldViewer 允许用户表达他们对 GTI 输出的多种看法，并在这个过程中挑战当前的公平性假设，即所有人都应该有同一个世界观。
</details></li>
</ul>
<hr>
<h2 id="A-Heterogeneous-Graph-Based-Multi-Task-Learning-for-Fault-Event-Diagnosis-in-Smart-Grid"><a href="#A-Heterogeneous-Graph-Based-Multi-Task-Learning-for-Fault-Event-Diagnosis-in-Smart-Grid" class="headerlink" title="A Heterogeneous Graph-Based Multi-Task Learning for Fault Event Diagnosis in Smart Grid"></a>A Heterogeneous Graph-Based Multi-Task Learning for Fault Event Diagnosis in Smart Grid</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09921">http://arxiv.org/abs/2309.09921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dibaloke Chanda, Nasim Yahya Soltani</li>
<li>for: 本文提出了一种基于多任务学习图神经网络（MTL-GNN）的精准故障诊断方法，用于检测、定位和分类故障，同时还提供了缺陷和电流估计。</li>
<li>methods: 本文使用了图神经网络（GNN）来学习分布系统的topological结构和特征学习，并通过消息传递机制实现feature learning。</li>
<li>results: 数值测试 validate了模型在各任务中的表现，并且提出了一种基于GNN的解释方法来 Identify key nodes in the distribution system，以便进行 informed sparse measurements。<details>
<summary>Abstract</summary>
Precise and timely fault diagnosis is a prerequisite for a distribution system to ensure minimum downtime and maintain reliable operation. This necessitates access to a comprehensive procedure that can provide the grid operators with insightful information in the case of a fault event. In this paper, we propose a heterogeneous multi-task learning graph neural network (MTL-GNN) capable of detecting, locating and classifying faults in addition to providing an estimate of the fault resistance and current. Using a graph neural network (GNN) allows for learning the topological representation of the distribution system as well as feature learning through a message-passing scheme. We investigate the robustness of our proposed model using the IEEE-123 test feeder system. This work also proposes a novel GNN-based explainability method to identify key nodes in the distribution system which then facilitates informed sparse measurements. Numerical tests validate the performance of the model across all tasks.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Precise and timely fault diagnosis is a prerequisite for a distribution system to ensure minimum downtime and maintain reliable operation. This necessitates access to a comprehensive procedure that can provide the grid operators with insightful information in the case of a fault event. In this paper, we propose a heterogeneous multi-task learning graph neural network (MTL-GNN) capable of detecting, locating and classifying faults in addition to providing an estimate of the fault resistance and current. Using a graph neural network (GNN) allows for learning the topological representation of the distribution system as well as feature learning through a message-passing scheme. We investigate the robustness of our proposed model using the IEEE-123 test feeder system. This work also proposes a novel GNN-based explainability method to identify key nodes in the distribution system which then facilitates informed sparse measurements. Numerical tests validate the performance of the model across all tasks." into 简化中文 >>Here's the translation:精准并快速的故障诊断是分布系统的必要前提，以确保最小化下时间和维护可靠的运行。这种情况下，需要访问一个全面的程序，以提供Grid操作人员有用的信息。在这篇论文中，我们提议一种多任务学习图神经网络（MTL-GNN），可以检测、定位和分类故障，同时提供故障抗力和电流的估计。使用图神经网络（GNN），可以学习分布系统的 topological表示，以及通过消息传递机制来学习特征。我们通过IEEE-123测试系统进行了对我们提议模型的Robustness测试。此外，我们还提出了一种基于GNN的解释方法，可以 identific key nodes在分布系统中，从而实现了 Informed sparse measurements。数值测试证明了模型在所有任务上的性能。
</details></li>
</ul>
<hr>
<h2 id="Plug-in-the-Safety-Chip-Enforcing-Constraints-for-LLM-driven-Robot-Agents"><a href="#Plug-in-the-Safety-Chip-Enforcing-Constraints-for-LLM-driven-Robot-Agents" class="headerlink" title="Plug in the Safety Chip: Enforcing Constraints for LLM-driven Robot Agents"></a>Plug in the Safety Chip: Enforcing Constraints for LLM-driven Robot Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09919">http://arxiv.org/abs/2309.09919</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyi Yang, Shreyas S. Raman, Ankit Shah, Stefanie Tellex</li>
<li>for: 这 paper 的目的是提出一种基于线性时间逻辑 (LTL) 的问题可读性安全约束模块，以便在协同环境中部署大语言模型 (LLM) 代理，并实现安全的操作。</li>
<li>methods: 本 paper 使用了 LLM 进行协同操作，并采用了 NL 时间约束编码、安全违反逻辑和解释、以及危险动作擦除等方法来保证安全性。</li>
<li>results: 实验结果表明，本系统可以坚持安全约束，并在复杂的安全约束下进行扩展，这 highlights 了它在实际应用中的潜在实用性。<details>
<summary>Abstract</summary>
Recent advancements in large language models (LLMs) have enabled a new research domain, LLM agents, for solving robotics and planning tasks by leveraging the world knowledge and general reasoning abilities of LLMs obtained during pretraining. However, while considerable effort has been made to teach the robot the "dos," the "don'ts" received relatively less attention. We argue that, for any practical usage, it is as crucial to teach the robot the "don'ts": conveying explicit instructions about prohibited actions, assessing the robot's comprehension of these restrictions, and, most importantly, ensuring compliance. Moreover, verifiable safe operation is essential for deployments that satisfy worldwide standards such as ISO 61508, which defines standards for safely deploying robots in industrial factory environments worldwide. Aiming at deploying the LLM agents in a collaborative environment, we propose a queryable safety constraint module based on linear temporal logic (LTL) that simultaneously enables natural language (NL) to temporal constraints encoding, safety violation reasoning and explaining, and unsafe action pruning. To demonstrate the effectiveness of our system, we conducted experiments in VirtualHome environment and on a real robot. The experimental results show that our system strictly adheres to the safety constraints and scales well with complex safety constraints, highlighting its potential for practical utility.
</details>
<details>
<summary>摘要</summary>
最近的大语言模型（LLM）技术发展，开启了一个新的研究领域——LLM代理人，通过利用 pré-training 中获得的世界知识和通用逻辑能力，解决 робо太和规划任务。然而，相比之下，“不”received relatively less attention。我们认为，在实际应用中，也是非常重要教 robot “不”：表达明确的禁止行为，评估机器人的理解这些限制，并最重要的是，确保遵守。此外，确认安全操作是在国际标准ISO 61508中定义的安全部署 robots 在工业Factory环境中的标准。为了在协同环境中部署 LLM 代理人，我们提议使用可询问安全约束模块，该模块同时允许自然语言（NL）和时间约束编码、安全违反理解和解释，以及危险行为剔除。为了证明我们的系统的有效性，我们在虚拟家庭环境和真实机器人上进行了实验。实验结果表明，我们的系统坚持着安全约束，并且随着复杂的安全约束的增加，我们的系统表现良好，这说明它在实际应用中具有潜在的实用性。
</details></li>
</ul>
<hr>
<h2 id="Evaluation-of-Human-Understandability-of-Global-Model-Explanations-using-Decision-Tree"><a href="#Evaluation-of-Human-Understandability-of-Global-Model-Explanations-using-Decision-Tree" class="headerlink" title="Evaluation of Human-Understandability of Global Model Explanations using Decision Tree"></a>Evaluation of Human-Understandability of Global Model Explanations using Decision Tree</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09917">http://arxiv.org/abs/2309.09917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adarsa Sivaprasad, Ehud Reiter, Nava Tintarev, Nir Oren</li>
<li>for: The paper aims to improve the understandability and trustworthiness of AI models in healthcare applications, specifically for non-expert patients who may not have a strong background in AI or domain expertise.</li>
<li>methods: The authors use a decision tree model to generate both local and global explanations for patients identified as having a high risk of coronary heart disease. They test the effectiveness of these explanations with non-expert users and gather feedback to enhance the narrative global explanations.</li>
<li>results: The majority of participants prefer global explanations, while a smaller group prefers local explanations. The authors also find that task-based evaluations of mental models of these participants provide valuable feedback to enhance narrative global explanations, which can guide the design of health informatics systems that are both trustworthy and actionable.Here are the three points in Simplified Chinese text:</li>
<li>for: 这篇论文的目的是提高医疗应用中AI模型的可解释性和可信worthiness，特别是为非专业的患者，他们可能没有AI或领域专业知识。</li>
<li>methods: 作者使用决策树模型生成本地和全局解释，并对非专业用户进行测试，收集反馈以提高 narative全局解释。</li>
<li>results: 大多数参与者喜欢全局解释，而一个较小的组 prefer 本地解释。作者还发现，基于任务评估的受试者的心理模型提供了价值的反馈，以提高全局解释。<details>
<summary>Abstract</summary>
In explainable artificial intelligence (XAI) research, the predominant focus has been on interpreting models for experts and practitioners. Model agnostic and local explanation approaches are deemed interpretable and sufficient in many applications. However, in domains like healthcare, where end users are patients without AI or domain expertise, there is an urgent need for model explanations that are more comprehensible and instil trust in the model's operations. We hypothesise that generating model explanations that are narrative, patient-specific and global(holistic of the model) would enable better understandability and enable decision-making. We test this using a decision tree model to generate both local and global explanations for patients identified as having a high risk of coronary heart disease. These explanations are presented to non-expert users. We find a strong individual preference for a specific type of explanation. The majority of participants prefer global explanations, while a smaller group prefers local explanations. A task based evaluation of mental models of these participants provide valuable feedback to enhance narrative global explanations. This, in turn, guides the design of health informatics systems that are both trustworthy and actionable.
</details>
<details>
<summary>摘要</summary>
在可解释人工智能（XAI）研究中，主要的关注是为专家和实践者解释模型。但在医疗领域，因为患者没有人工智能或领域知识，有一个急需的需求是为模型提供更容易理解和带来信任的解释。我们提出的假设是，通过生成模型解释，使其更加易于理解，并且全面涵盖模型的运作。我们使用决策树模型生成本地和全局解释，并对非专业用户进行测试。我们发现，大多数参与者偏好全局解释，而一个较小的组 prefer local解释。这些参与者的任务基础评估表示，可以通过改进 narrative global解释来提高健康信息系统的可靠性和可行性。
</details></li>
</ul>
<hr>
<h2 id="Wait-That-Feels-Familiar-Learning-to-Extrapolate-Human-Preferences-for-Preference-Aligned-Path-Planning"><a href="#Wait-That-Feels-Familiar-Learning-to-Extrapolate-Human-Preferences-for-Preference-Aligned-Path-Planning" class="headerlink" title="Wait, That Feels Familiar: Learning to Extrapolate Human Preferences for Preference Aligned Path Planning"></a>Wait, That Feels Familiar: Learning to Extrapolate Human Preferences for Preference Aligned Path Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09912">http://arxiv.org/abs/2309.09912</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haresh Karnan, Elvin Yang, Garrett Warnell, Joydeep Biswas, Peter Stone</li>
<li>for: 这paper的目的是解决自主移动任务中需要理解操作员指定的权重，以确保机器人的安全和任务完成。</li>
<li>methods: 这paper使用了 preference extrApolation for Terrain awarE Robot Navigation（PATERN）框架，该框架可以从机器人的感知数据中推断操作员对新地形的偏好。</li>
<li>results: 对比基eline方法，这paper的实验结果表明，PATERN可以强健地泛化到多种不同的地形和照明条件下，并能够导航在操作员的偏好驱动下。<details>
<summary>Abstract</summary>
Autonomous mobility tasks such as lastmile delivery require reasoning about operator indicated preferences over terrains on which the robot should navigate to ensure both robot safety and mission success. However, coping with out of distribution data from novel terrains or appearance changes due to lighting variations remains a fundamental problem in visual terrain adaptive navigation. Existing solutions either require labor intensive manual data recollection and labeling or use handcoded reward functions that may not align with operator preferences. In this work, we posit that operator preferences for visually novel terrains, which the robot should adhere to, can often be extrapolated from established terrain references within the inertial, proprioceptive, and tactile domain. Leveraging this insight, we introduce Preference extrApolation for Terrain awarE Robot Navigation, PATERN, a novel framework for extrapolating operator terrain preferences for visual navigation. PATERN learns to map inertial, proprioceptive, tactile measurements from the robots observations to a representation space and performs nearest neighbor search in this space to estimate operator preferences over novel terrains. Through physical robot experiments in outdoor environments, we assess PATERNs capability to extrapolate preferences and generalize to novel terrains and challenging lighting conditions. Compared to baseline approaches, our findings indicate that PATERN robustly generalizes to diverse terrains and varied lighting conditions, while navigating in a preference aligned manner.
</details>
<details>
<summary>摘要</summary>
自主移动任务，如最后一英里交付，需要考虑操作员表达的偏好在不同地形上 navigator 以确保机器人的安全和任务成功。然而，对于新地形或光学变化所带来的数据 OUT OF DISTRIBUTION 问题在视觉地形适应导航中仍然存在。现有的解决方案可能需要劳动 INTENSIVE 的手动数据收集和标注，或者使用手动编码的奖励函数，这些奖励函数可能并不与操作员的偏好相一致。在这种情况下，我们认为操作员对新地形的偏好可以从已有的地形参考中推断出来。基于这一点，我们提出了 Preference extrApolation for Terrain awarE Robot Navigation，简称 PATERN，一种新的框架，用于从地形参考中推断操作员对新地形的偏好。PATERN 使用地形参考中的各种力学、 proprioceptive 和感觉测量来映射到一个表示空间，并在这个空间中进行 nearest neighbor 搜索来估算操作员对新地形的偏好。通过实际的机器人实验，我们评估了 PATERN 对新地形和不同的照明条件的扩展和适应能力。与基准方法相比，我们的发现表明，PATERN 可以坚定地 generalize 到多种地形和照明条件，并在这些条件下导航以偏好适应的方式。
</details></li>
</ul>
<hr>
<h2 id="Evaluation-of-GPT-3-for-Anti-Cancer-Drug-Sensitivity-Prediction"><a href="#Evaluation-of-GPT-3-for-Anti-Cancer-Drug-Sensitivity-Prediction" class="headerlink" title="Evaluation of GPT-3 for Anti-Cancer Drug Sensitivity Prediction"></a>Evaluation of GPT-3 for Anti-Cancer Drug Sensitivity Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10016">http://arxiv.org/abs/2309.10016</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaika Chowdhury, Sivaraman Rajaganapathy, Lichao Sun, James Cerhan, Nansu Zong</li>
<li>for: 这项研究探讨了使用GPT-3进行精准肿瘤治疗药物敏感性预测任务的可能性，使用结构化药物ogenomics数据，并评估了零批训练和精细调整方法的性能。</li>
<li>methods: 这项研究使用了GPT-3模型，并对结构化药物ogenomics数据进行分析和预测。</li>
<li>results: 研究发现，药物的笑容表达和细胞线的遗传变化特征是药物响应的预测因素。这些结果有可能为精准肿瘤治疗做出贡献，帮助设计更有效的治疗协议。<details>
<summary>Abstract</summary>
In this study, we investigated the potential of GPT-3 for the anti-cancer drug sensitivity prediction task using structured pharmacogenomics data across five tissue types and evaluated its performance with zero-shot prompting and fine-tuning paradigms. The drug's smile representation and cell line's genomic mutation features were predictive of the drug response. The results from this study have the potential to pave the way for designing more efficient treatment protocols in precision oncology.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们研究了GPT-3的抗癌药效预测能力，使用结构化药理学数据，对五种组织型进行评估，并评估了零模式提示和精度调整 paradigms。药物的笑容表达和细胞线的基因突变特征是药效预测的预测因素。这项研究的结果有可能为精准肿瘤学设计更有效的治疗协议开创道路。
</details></li>
</ul>
<hr>
<h2 id="The-role-of-causality-in-explainable-artificial-intelligence"><a href="#The-role-of-causality-in-explainable-artificial-intelligence" class="headerlink" title="The role of causality in explainable artificial intelligence"></a>The role of causality in explainable artificial intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09901">http://arxiv.org/abs/2309.09901</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gianluca Carloni, Andrea Berti, Sara Colantonio</li>
<li>for: 本文旨在探讨 causality 和 explainable artificial intelligence (XAI) 两个领域之间的关系，尤其是它们之间的联系是如何、以及如何利用这两个领域来建立信任于人工智能系统。</li>
<li>methods: 本文通过 investigate 文献来了解 causality 和 XAI 之间的关系，并 Identify 三个主要视角：首先，缺乏 causality 是当前 AI 和 XAI 方法的一个重要限制，并且“最佳”的解释的形式被 investigate ；第二是一种实用视角，认为 XAI 可以用于推动科学探索，通过识别可以实现的实验操作来推动科学探索；第三个视角支持 idea ， causality 是 XAI 的Propedeutic 三种方式：利用 causality 概念支持或改进 XAI，利用 counterfactuals 来提供解释，并考虑 accessing  causal model 作为解释。</li>
<li>results: 本文提供了一种统一的视角，把 causality 和 XAI 两个领域联系起来，并 highlight 了这两个领域之间的可能的链接和可能的限制。此外，本文还提供了 relevante 软件解决方案，用于自动化 causal 任务。<details>
<summary>Abstract</summary>
Causality and eXplainable Artificial Intelligence (XAI) have developed as separate fields in computer science, even though the underlying concepts of causation and explanation share common ancient roots. This is further enforced by the lack of review works jointly covering these two fields. In this paper, we investigate the literature to try to understand how and to what extent causality and XAI are intertwined. More precisely, we seek to uncover what kinds of relationships exist between the two concepts and how one can benefit from them, for instance, in building trust in AI systems. As a result, three main perspectives are identified. In the first one, the lack of causality is seen as one of the major limitations of current AI and XAI approaches, and the "optimal" form of explanations is investigated. The second is a pragmatic perspective and considers XAI as a tool to foster scientific exploration for causal inquiry, via the identification of pursue-worthy experimental manipulations. Finally, the third perspective supports the idea that causality is propaedeutic to XAI in three possible manners: exploiting concepts borrowed from causality to support or improve XAI, utilizing counterfactuals for explainability, and considering accessing a causal model as explaining itself. To complement our analysis, we also provide relevant software solutions used to automate causal tasks. We believe our work provides a unified view of the two fields of causality and XAI by highlighting potential domain bridges and uncovering possible limitations.
</details>
<details>
<summary>摘要</summary>
causality 和 explainable Artificial Intelligence (XAI) 在计算机科学中发展成为两个独立的领域，尽管它们的基本概念之间有共同的古老根基。这种情况进一步加剧了由lack of review works jointly covering these two fields所带来的分化。在这篇论文中，我们对文献进行调查，以便更好地理解causality 和 XAI 之间的关系。更具体地说，我们想要找出causality 和 XAI 之间的关系是什么样的，以及如何在建立人工智能系统的信任方面利用它们。结果，我们分析出了三个主要的视角：第一个视角认为现有的 AI 和 XAI 方法缺乏 causality 是一个重要的限制，并investigate the "optimal" form of explanations。第二个视角是一种实用的视角，认为 XAI 可以用于推动科学探索，通过识别可以实施的实验操作。最后，第三个视角支持idea that causality is propaedeutic to XAI in three possible manners：borrowing concepts from causality to support or improve XAI，utilizing counterfactuals for explainability，和considering accessing a causal model as explaining itself。为了补充我们的分析，我们还提供了用于自动化 causal 任务的相关软件解决方案。我们认为我们的工作提供了两个领域之间的统一视角，揭示了可能的领域桥梁，并揭示了可能的限制。
</details></li>
</ul>
<hr>
<h2 id="Towards-Ontology-Construction-with-Language-Models"><a href="#Towards-Ontology-Construction-with-Language-Models" class="headerlink" title="Towards Ontology Construction with Language Models"></a>Towards Ontology Construction with Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09898">http://arxiv.org/abs/2309.09898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maurice Funk, Simon Hosemann, Jean Christoph Jung, Carsten Lutz</li>
<li>for: 自动构建域层次结构</li>
<li>methods: 通过问题大语言模型</li>
<li>results: LLMs 可以帮助建立域层次结构<details>
<summary>Abstract</summary>
We present a method for automatically constructing a concept hierarchy for a given domain by querying a large language model. We apply this method to various domains using OpenAI's GPT 3.5. Our experiments indicate that LLMs can be of considerable help for constructing concept hierarchies.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法，用于自动构建域名空间中的概念层次结构，通过问题大型自然语言模型（LLM）。我们对不同领域进行应用，使用OpenAI的GPT 3.5。我们的实验表明，LLM可以帮助建立概念层次结构。Here's a breakdown of the translation:* "We present a method" becomes "我们提出了一种方法" (wǒmen tīshì yī zhī fāngzhì)* "for automatically constructing a concept hierarchy" becomes "用于自动构建域名空间中的概念层次结构" (yǐng yú zìdān jīngjì xìngshì)* "by querying a large language model" becomes "通过问题大型自然语言模型（LLM）" (tōngguò wèn tí dàxíng zhīyǐng yǔyán módel)* "We apply this method to various domains" becomes "我们对不同领域进行应用" (wǒmen duìfāng bùdìng fāngyì)* "using OpenAI's GPT 3.5" becomes "使用OpenAI的GPT 3.5" (fùyòu OpenAI de GPT 3.5)* "Our experiments indicate that LLMs can be of considerable help for constructing concept hierarchies" becomes "我们的实验表明，LLM可以帮助建立概念层次结构" (wǒmen de shíyè bǎozhèng, LLM cóuzhù bāngzhì jīnjī)
</details></li>
</ul>
<hr>
<h2 id="Context-is-Environment"><a href="#Context-is-Environment" class="headerlink" title="Context is Environment"></a>Context is Environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09888">http://arxiv.org/abs/2309.09888</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/apostrophecms/apostrophe">https://github.com/apostrophecms/apostrophe</a></li>
<li>paper_authors: Sharut Gupta, Stefanie Jegelka, David Lopez-Paz, Kartik Ahuja</li>
<li>for: 提高领域泛化性能</li>
<li>methods: 使用增强上下文学习来减少杂 correlate和提高领域泛化性能</li>
<li>results: 经验和理论表明，ICRM算法可以在不知道标签的情况下，通过关注上下文来提高OD性能。<details>
<summary>Abstract</summary>
Two lines of work are taking the central stage in AI research. On the one hand, the community is making increasing efforts to build models that discard spurious correlations and generalize better in novel test environments. Unfortunately, the bitter lesson so far is that no proposal convincingly outperforms a simple empirical risk minimization baseline. On the other hand, large language models (LLMs) have erupted as algorithms able to learn in-context, generalizing on-the-fly to eclectic contextual circumstances that users enforce by means of prompting. In this paper, we argue that context is environment, and posit that in-context learning holds the key to better domain generalization. Via extensive theory and experiments, we show that paying attention to context$\unicode{x2013}\unicode{x2013}$unlabeled examples as they arrive$\unicode{x2013}\unicode{x2013}$allows our proposed In-Context Risk Minimization (ICRM) algorithm to zoom-in on the test environment risk minimizer, leading to significant out-of-distribution performance improvements. From all of this, two messages are worth taking home. Researchers in domain generalization should consider environment as context, and harness the adaptive power of in-context learning. Researchers in LLMs should consider context as environment, to better structure data towards generalization.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="EGFE-End-to-end-Grouping-of-Fragmented-Elements-in-UI-Designs-with-Multimodal-Learning"><a href="#EGFE-End-to-end-Grouping-of-Fragmented-Elements-in-UI-Designs-with-Multimodal-Learning" class="headerlink" title="EGFE: End-to-end Grouping of Fragmented Elements in UI Designs with Multimodal Learning"></a>EGFE: End-to-end Grouping of Fragmented Elements in UI Designs with Multimodal Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09867">http://arxiv.org/abs/2309.09867</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/test2975/egfe">https://github.com/test2975/egfe</a></li>
<li>paper_authors: Liuqing Chen, Yunnong Chen, Shuhong Xiao, Yaxuan Song, Lingyun Sun, Yankun Zhen, Tingting Zhou, Yanfang Chang</li>
<li>for: 快速化 Front-end 应用程序和 GUI 迭代开发</li>
<li>methods: 使用 Transformer Encoder 模型 UI 元素之间的关系，并使用多Modal 表示学习</li>
<li>results: 与基eline 比较，EGFE 方法在精度（+29.75%）、回归（+31.07%）和 F1 分数（+30.39%）上具有显著优势，并在实际软件工程应用中进行了效果评估。<details>
<summary>Abstract</summary>
When translating UI design prototypes to code in industry, automatically generating code from design prototypes can expedite the development of applications and GUI iterations. However, in design prototypes without strict design specifications, UI components may be composed of fragmented elements. Grouping these fragmented elements can greatly improve the readability and maintainability of the generated code. Current methods employ a two-stage strategy that introduces hand-crafted rules to group fragmented elements. Unfortunately, the performance of these methods is not satisfying due to visually overlapped and tiny UI elements. In this study, we propose EGFE, a novel method for automatically End-to-end Grouping Fragmented Elements via UI sequence prediction. To facilitate the UI understanding, we innovatively construct a Transformer encoder to model the relationship between the UI elements with multi-modal representation learning. The evaluation on a dataset of 4606 UI prototypes collected from professional UI designers shows that our method outperforms the state-of-the-art baselines in the precision (by 29.75\%), recall (by 31.07\%), and F1-score (by 30.39\%) at edit distance threshold of 4. In addition, we conduct an empirical study to assess the improvement of the generated front-end code. The results demonstrate the effectiveness of our method on a real software engineering application. Our end-to-end fragmented elements grouping method creates opportunities for improving UI-related software engineering tasks.
</details>
<details>
<summary>摘要</summary>
当在产业中将用户界面设计原型转换成代码时，自动生成代码从设计原型可以大大提高应用程序和Graphical User Interface（GUI）的开发效率。然而，在没有严格的设计规范的情况下，用户界面组件可能会被分割成多个碎片。将这些碎片组合起来可以大大提高代码的可读性和维护性。现有的方法采用两阶段策略，通过手动制定规则来组合碎片元素。然而，这些方法的性能不 satisfactory，因为视觉上的重叠和小型用户界面元素。在本研究中，我们提出了一种 novel方法，名为 End-to-end Grouping Fragmented Elements（EGFE），通过用户界面序列预测来自动组合碎片元素。为了促进用户界面理解，我们创新地构建了一个 TransformerEncoder，用于模型用户界面元素之间的关系，并通过多modal表示学习来学习这些关系。我们对收集了4606个用户界面原型的数据集进行评估，结果显示，我们的方法在编辑距离阈值为4时，与state-of-the-art基线方法相比，提高了精度（29.75%），回归率（31.07%）和F1分数（30.39%）。此外，我们进行了一个实际的研究，以评估生成的前端代码的改进。结果表明，我们的方法在实际软件工程应用中具有效果。我们的末端碎片元素组合方法创造了对用户界面相关的软件工程任务的机会。
</details></li>
</ul>
<hr>
<h2 id="Learning-Spatial-and-Temporal-Hierarchies-Hierarchical-Active-Inference-for-navigation-in-Multi-Room-Maze-Environments"><a href="#Learning-Spatial-and-Temporal-Hierarchies-Hierarchical-Active-Inference-for-navigation-in-Multi-Room-Maze-Environments" class="headerlink" title="Learning Spatial and Temporal Hierarchies: Hierarchical Active Inference for navigation in Multi-Room Maze Environments"></a>Learning Spatial and Temporal Hierarchies: Hierarchical Active Inference for navigation in Multi-Room Maze Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09864">http://arxiv.org/abs/2309.09864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daria de Tinguy, Toon Van de Maele, Tim Verbelen, Bart Dhoedt</li>
<li>for: 该论文旨在解决从像素级别观察到环境结构的推理挑战，提出了一种层次式活动推理模型。</li>
<li>methods: 该模型包括一个认知地图、一个allocentric和一个egocentric世界模型，结合了好奇的探索和目标带导的行为，从上下文、地点到动作的不同层次进行了理解和推理。</li>
<li>results: 该模型在房间结构小网格环境中实现了高效的探索和目标带导搜索。<details>
<summary>Abstract</summary>
Cognitive maps play a crucial role in facilitating flexible behaviour by representing spatial and conceptual relationships within an environment. The ability to learn and infer the underlying structure of the environment is crucial for effective exploration and navigation. This paper introduces a hierarchical active inference model addressing the challenge of inferring structure in the world from pixel-based observations. We propose a three-layer hierarchical model consisting of a cognitive map, an allocentric, and an egocentric world model, combining curiosity-driven exploration with goal-oriented behaviour at the different levels of reasoning from context to place to motion. This allows for efficient exploration and goal-directed search in room-structured mini-grid environments.
</details>
<details>
<summary>摘要</summary>
cognitive maps 扮演了灵活行为的关键角色，协助表示环境中的空间和概念关系。学习和推理环境的结构是有效探索和导航的关键。这篇论文提出了一种层次活动推理模型，解决从像素级别观察到世界结构的推理挑战。我们提议了一种三层层次模型，包括认知地图、allocentric 和 egocentric 世界模型，将curiosity-driven 探索与目标尝试结合在不同的理解层次上，从上下文到场景到运动。这种方法可以有效地在室内小型网格环境中进行探索和目标寻找。
</details></li>
</ul>
<hr>
<h2 id="SYNDICOM-Improving-Conversational-Commonsense-with-Error-Injection-and-Natural-Language-Feedback"><a href="#SYNDICOM-Improving-Conversational-Commonsense-with-Error-Injection-and-Natural-Language-Feedback" class="headerlink" title="SYNDICOM: Improving Conversational Commonsense with Error-Injection and Natural Language Feedback"></a>SYNDICOM: Improving Conversational Commonsense with Error-Injection and Natural Language Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10015">http://arxiv.org/abs/2309.10015</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hieu9955/ggggg">https://github.com/hieu9955/ggggg</a></li>
<li>paper_authors: Christopher Richardson, Anirudh Sundar, Larry Heck</li>
<li>for: 提高对话响应的常识理解</li>
<li>methods: 使用知识图和自然语言Feedback（NLF）创建对话对话数据集，并使用两步验证过程：首先预测NLF，然后使用预测结果和无效响应生成对话。</li>
<li>results: 比ChatGPT提高53%的ROUGE1分数，人类评价者57%时喜欢SYNDICOM。<details>
<summary>Abstract</summary>
Commonsense reasoning is a critical aspect of human communication. Despite recent advances in conversational AI driven by large language models, commonsense reasoning remains a challenging task. In this work, we introduce SYNDICOM - a method for improving commonsense in dialogue response generation. SYNDICOM consists of two components. The first component is a dataset composed of commonsense dialogues created from a knowledge graph and synthesized into natural language. This dataset includes both valid and invalid responses to dialogue contexts, along with natural language feedback (NLF) for the invalid responses. The second contribution is a two-step procedure: training a model to predict natural language feedback (NLF) for invalid responses, and then training a response generation model conditioned on the predicted NLF, the invalid response, and the dialogue. SYNDICOM is scalable and does not require reinforcement learning. Empirical results on three tasks are evaluated using a broad range of metrics. SYNDICOM achieves a relative improvement of 53% over ChatGPT on ROUGE1, and human evaluators prefer SYNDICOM over ChatGPT 57% of the time. We will publicly release the code and the full dataset.
</details>
<details>
<summary>摘要</summary>
常识理解是人类communication的重要方面。 despite recent advances in conversational AI driven by large language models, commonsense reasoning remains a challenging task. In this work, we introduce SYNDICOM - a method for improving commonsense in dialogue response generation. SYNDICOM consists of two components. The first component is a dataset composed of commonsense dialogues created from a knowledge graph and synthesized into natural language. This dataset includes both valid and invalid responses to dialogue contexts, along with natural language feedback (NLF) for the invalid responses. The second contribution is a two-step procedure: training a model to predict natural language feedback (NLF) for invalid responses, and then training a response generation model conditioned on the predicted NLF, the invalid response, and the dialogue. SYNDICOM is scalable and does not require reinforcement learning. Empirical results on three tasks are evaluated using a broad range of metrics. SYNDICOM achieves a relative improvement of 53% over ChatGPT on ROUGE1, and human evaluators prefer SYNDICOM over ChatGPT 57% of the time. We will publicly release the code and the full dataset.Here's the translation in Traditional Chinese as well:常识理解是人类communication的重要方面。 despite recent advances in conversational AI driven by large language models, commonsense reasoning remains a challenging task. In this work, we introduce SYNDICOM - a method for improving commonsense in dialogue response generation. SYNDICOM consists of two components. The first component is a dataset composed of commonsense dialogues created from a knowledge graph and synthesized into natural language. This dataset includes both valid and invalid responses to dialogue contexts, along with natural language feedback (NLF) for the invalid responses. The second contribution is a two-step procedure: training a model to predict natural language feedback (NLF) for invalid responses, and then training a response generation model conditioned on the predicted NLF, the invalid response, and the dialogue. SYNDICOM is scalable and does not require reinforcement learning. Empirical results on three tasks are evaluated using a broad range of metrics. SYNDICOM achieves a relative improvement of 53% over ChatGPT on ROUGE1, and human evaluators prefer SYNDICOM over ChatGPT 57% of the time. We will publicly release the code and the full dataset.
</details></li>
</ul>
<hr>
<h2 id="CC-SGG-Corner-Case-Scenario-Generation-using-Learned-Scene-Graphs"><a href="#CC-SGG-Corner-Case-Scenario-Generation-using-Learned-Scene-Graphs" class="headerlink" title="CC-SGG: Corner Case Scenario Generation using Learned Scene Graphs"></a>CC-SGG: Corner Case Scenario Generation using Learned Scene Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09844">http://arxiv.org/abs/2309.09844</a></li>
<li>repo_url: None</li>
<li>paper_authors: George Drayson, Efimia Panagiotaki, Daniel Omeiza, Lars Kunze</li>
<li>for: 增强自动驾驶车辆的安全性测试和验证</li>
<li>methods: 使用异构图 neural network 将常见驾驶场景转换为异常场景</li>
<li>results: 成功将常见驾驶场景转换为异常场景，实现89.9%的预测精度，并证明模型能够创造基eline方法所不能处理的特殊情况。<details>
<summary>Abstract</summary>
Corner case scenarios are an essential tool for testing and validating the safety of autonomous vehicles (AVs). As these scenarios are often insufficiently present in naturalistic driving datasets, augmenting the data with synthetic corner cases greatly enhances the safe operation of AVs in unique situations. However, the generation of synthetic, yet realistic, corner cases poses a significant challenge. In this work, we introduce a novel approach based on Heterogeneous Graph Neural Networks (HGNNs) to transform regular driving scenarios into corner cases. To achieve this, we first generate concise representations of regular driving scenes as scene graphs, minimally manipulating their structure and properties. Our model then learns to perturb those graphs to generate corner cases using attention and triple embeddings. The input and perturbed graphs are then imported back into the simulation to generate corner case scenarios. Our model successfully learned to produce corner cases from input scene graphs, achieving 89.9% prediction accuracy on our testing dataset. We further validate the generated scenarios on baseline autonomous driving methods, demonstrating our model's ability to effectively create critical situations for the baselines.
</details>
<details>
<summary>摘要</summary>
弯道情况场景是自动驾驶车辆（AV）的测试和验证安全工具。然而，这些情况场景通常不足于自然驾驶数据集中，因此通过增强数据集中的人工弯道情况场景可以大大提高AV的安全运行。然而，生成具有真实感的人工弯道情况场景是一项挑战。在这种情况下，我们提出了一种基于异种图 neural network（HGNN）的新方法，可以将常见驾驶场景转换成弯道情况场景。我们首先生成了常见驾驶场景的简洁表示，并将其结构和属性进行最小的修改。然后，我们的模型通过注意力和 triple embeddings 来对这些图进行扰动，以生成弯道情况场景。最后，我们将生成的弯道情况场景重新导入到模拟器中，以生成弯道场景。我们的模型成功地将输入场景图转换成弯道场景，测试集上的预测精度达89.9%。此外，我们还 validate了生成的场景，证明我们的模型可以生成对基eline autonomous driving方法 Critical Situation。
</details></li>
</ul>
<hr>
<h2 id="RECAP-Retrieval-Augmented-Audio-Captioning"><a href="#RECAP-Retrieval-Augmented-Audio-Captioning" class="headerlink" title="RECAP: Retrieval-Augmented Audio Captioning"></a>RECAP: Retrieval-Augmented Audio Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09836">http://arxiv.org/abs/2309.09836</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sreyan Ghosh, Sonal Kumar, Chandra Kiran Reddy Evuru, Ramani Duraiswami, Dinesh Manocha</li>
<li>for: 这个论文旨在提出一种新的音频captioning系统，即RECAP（REtrieval-Augmented Audio CAPtioning），它可以基于输入音频和其他相似音频从数据存储中检索类似的caption。</li>
<li>methods: 该方法使用CLAP音频-文本模型来检索相似的caption，然后使用GPT-2解码器和CLAP编码器之间的交叉关注层来conditioning audio дляcaption生成。</li>
<li>results: 实验表明，RECAP在本地设置中达到了竞争性性能，而在另外一个设置中具有显著的改进。此外，由于它可以在training-free的方式下使用大量的文本-caption-只datastore，因此RECAP可以caption novel audio事件和多个事件的组合音频。<details>
<summary>Abstract</summary>
We present RECAP (REtrieval-Augmented Audio CAPtioning), a novel and effective audio captioning system that generates captions conditioned on an input audio and other captions similar to the audio retrieved from a datastore. Additionally, our proposed method can transfer to any domain without the need for any additional fine-tuning. To generate a caption for an audio sample, we leverage an audio-text model CLAP to retrieve captions similar to it from a replaceable datastore, which are then used to construct a prompt. Next, we feed this prompt to a GPT-2 decoder and introduce cross-attention layers between the CLAP encoder and GPT-2 to condition the audio for caption generation. Experiments on two benchmark datasets, Clotho and AudioCaps, show that RECAP achieves competitive performance in in-domain settings and significant improvements in out-of-domain settings. Additionally, due to its capability to exploit a large text-captions-only datastore in a \textit{training-free} fashion, RECAP shows unique capabilities of captioning novel audio events never seen during training and compositional audios with multiple events. To promote research in this space, we also release 150,000+ new weakly labeled captions for AudioSet, AudioCaps, and Clotho.
</details>
<details>
<summary>摘要</summary>
我们提出了RECAP（REtrieval-Augmented Audio CAPtioning），一种新的有效的音频描述系统，它根据输入音频和相似的音频从数据库中检索到类似的描述，并且不需要任何额外的调整。为生成音频描述，我们利用CLAP音频文本模型从可更换数据库中检索类似的描述，然后将它们用于构建提示。接着，我们将这个提示传递给GPT-2解码器，并在CLAP编码器和GPT-2之间添加交叉注意力层，以condition the audio for caption generation。实验表明，RECAP在预训练集和非预训练集上具有竞争力和显著提高的表现。此外，由于它可以在训练集中使用大量的文本描述只数据库，RECAP表现出了新领域的描述和多事件音频的特有能力。为促进这个领域的研究，我们还发布了150,000多个弱Label的描述数据，用于AudioSet、AudioCaps和Clotho等数据集。
</details></li>
</ul>
<hr>
<h2 id="Task-Selection-and-Assignment-for-Multi-modal-Multi-task-Dialogue-Act-Classification-with-Non-stationary-Multi-armed-Bandits"><a href="#Task-Selection-and-Assignment-for-Multi-modal-Multi-task-Dialogue-Act-Classification-with-Non-stationary-Multi-armed-Bandits" class="headerlink" title="Task Selection and Assignment for Multi-modal Multi-task Dialogue Act Classification with Non-stationary Multi-armed Bandits"></a>Task Selection and Assignment for Multi-modal Multi-task Dialogue Act Classification with Non-stationary Multi-armed Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09832">http://arxiv.org/abs/2309.09832</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangheng He, Junjie Chen, Björn W. Schuller</li>
<li>for: 提高主任务性能，通过共同学习相关辅助任务。</li>
<li>methods: 非站立多臂扔投（MAB）与减少 Thompson 抽样（TS）使用 Gaussian 分布。</li>
<li>results: 在不同训练阶段，不同任务有不同的用处。我们的提议方法可以有效地确定任务用处，避免无用或害处任务，并在训练中进行任务分配。我们的方法与单任务和多任务基elines相比，在 UAR 和 F1 方面具有显著优势（p-value &lt; 0.05）。进一步分析实验结果表明，对于数据不均衡问题的数据集，我们的方法具有更高的稳定性，可以获得稳定且良好的性能 для少数类。我们的方法超越当前状态的艺术模型。<details>
<summary>Abstract</summary>
Multi-task learning (MTL) aims to improve the performance of a primary task by jointly learning with related auxiliary tasks. Traditional MTL methods select tasks randomly during training. However, both previous studies and our results suggest that such the random selection of tasks may not be helpful, and can even be harmful to performance. Therefore, new strategies for task selection and assignment in MTL need to be explored. This paper studies the multi-modal, multi-task dialogue act classification task, and proposes a method for selecting and assigning tasks based on non-stationary multi-armed bandits (MAB) with discounted Thompson Sampling (TS) using Gaussian priors. Our experimental results show that in different training stages, different tasks have different utility. Our proposed method can effectively identify the task utility, actively avoid useless or harmful tasks, and realise the task assignment during training. Our proposed method is significantly superior in terms of UAR and F1 to the single-task and multi-task baselines with p-values < 0.05. Further analysis of experiments indicates that for the dataset with the data imbalance problem, our proposed method has significantly higher stability and can obtain consistent and decent performance for minority classes. Our proposed method is superior to the current state-of-the-art model.
</details>
<details>
<summary>摘要</summary>
多任务学习（MTL）目标是通过同时学习相关的 aux 任务来提高主任务的性能。传统的 MTL 方法在训练过程中随机选择任务。然而，前一 studies 和我们的结果表明，随机选择任务可能并不是有利的，甚至会对性能产生负面影响。因此，新的任务选择和分配策略在 MTL 中需要被探索。本文研究了多模态、多任务对话权分类任务，并提出了基于非站ARY多臂枪（MAB）和折扣 Thompson Sampling（TS）using Gaussian priors 的任务选择和分配策略。我们的实验结果表明，在不同的训练阶段，不同的任务具有不同的用户性。我们的提议的方法可以有效地确定任务的用户性，避免无用或害的任务，并在训练过程中实现任务分配。我们的提议的方法与单任务和多任务基eline 相比，在 UAR 和 F1 上有 statistically 显著的优势（p-value < 0.05）。进一步的分析结果表明，对数据不均衡问题的 dataset 上，我们的方法具有更高的稳定性，并可以在训练过程中获得稳定和 descent 的性能 для少数类。我们的方法超过了当前领域的状态对模型。
</details></li>
</ul>
<hr>
<h2 id="Clustering-of-Urban-Traffic-Patterns-by-K-Means-and-Dynamic-Time-Warping-Case-Study"><a href="#Clustering-of-Urban-Traffic-Patterns-by-K-Means-and-Dynamic-Time-Warping-Case-Study" class="headerlink" title="Clustering of Urban Traffic Patterns by K-Means and Dynamic Time Warping: Case Study"></a>Clustering of Urban Traffic Patterns by K-Means and Dynamic Time Warping: Case Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09830">http://arxiv.org/abs/2309.09830</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sadegh Etemad, Raziyeh Mosayebi, Tadeh Alexani Khodavirdian, Elahe Dastan, Amir Salari Telmadarreh, Mohammadreza Jafari, Sepehr Rafiei</li>
<li>for: 这个论文的目的是提出一种基于K-Means和动态时间扭曲算法的时间序列划分方法，用于描述城市交通流量的pattern。</li>
<li>methods: 本论文使用的方法包括速度时序列EXTRACTOR和K-Means算法，以及基于时间扭曲的方法。</li>
<li>results: 实验结果表明，提议的方法可以准确地描述城市交通流量的pattern，并且可以提供有用的信息 для城市交通管理和规划。<details>
<summary>Abstract</summary>
Clustering of urban traffic patterns is an essential task in many different areas of traffic management and planning. In this paper, two significant applications in the clustering of urban traffic patterns are described. The first application estimates the missing speed values using the speed of road segments with similar traffic patterns to colorify map tiles. The second one is the estimation of essential road segments for generating addresses for a local point on the map, using the similarity patterns of different road segments. The speed time series extracts the traffic pattern in different road segments. In this paper, we proposed the time series clustering algorithm based on K-Means and Dynamic Time Warping. The case study of our proposed algorithm is based on the Snapp application's driver speed time series data. The results of the two applications illustrate that the proposed method can extract similar urban traffic patterns.
</details>
<details>
<summary>摘要</summary>
clustering 城市交通模式是许多交通管理和规划领域的关键任务。本文描述了两种城市交通模式的 clustering 应用。第一个应用是使用同样交通模式的道路段速度来彩色地图块。第二个应用是根据不同道路段的相似性模式来生成地图上的地址。速度时间序列提取了不同道路段的交通模式。本文提出了基于 K-Means 和动态时间扩展的时间序列 clustering 算法。案例研究基于 Snapp 应用的 води手速度时间序列数据。结果表明，提posed 方法可以提取类似的城市交通模式。Here's the translation of the text into Traditional Chinese: clustering 城市交通模式是许多交通管理和规划领域的关键任务。本文描述了两种城市交通模式的 clustering 应用。第一个应用是使用同样交通模式的道路段速度来彩色地图块。第二个应用是根据不同道路段的相似性模式来生成地图上的地址。速度时间序列提取了不同道路段的交通模式。本文提出了基于 K-Means 和动态时间扩展的时间序列 clustering 算法。案例研究基于 Snapp 应用的 води手速度时间序列数据。结果表明，提案的方法可以提取类似的城市交通模式。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Avoidance-of-Vulnerabilities-in-Auto-completed-Smart-Contract-Code-Using-Vulnerability-constrained-Decoding"><a href="#Efficient-Avoidance-of-Vulnerabilities-in-Auto-completed-Smart-Contract-Code-Using-Vulnerability-constrained-Decoding" class="headerlink" title="Efficient Avoidance of Vulnerabilities in Auto-completed Smart Contract Code Using Vulnerability-constrained Decoding"></a>Efficient Avoidance of Vulnerabilities in Auto-completed Smart Contract Code Using Vulnerability-constrained Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09826">http://arxiv.org/abs/2309.09826</a></li>
<li>repo_url: None</li>
<li>paper_authors: André Storhaug, Jingyue Li, Tianyuan Hu</li>
<li>for: 本研究旨在提高大型自然语言模型（LLM）在代码生成中的安全性，避免由这些模型生成的代码具有漏洞。</li>
<li>methods: 我们提出了一种新的漏洞约束生成方法，通过在代码生成过程中避免生成漏洞的代码来减少漏洞的出现。我们使用了一小组标注了漏洞代码的数据，使得模型在生成代码时包含漏洞标签。然后，在生成代码时，我们禁止模型生成这些标签，以避免生成漏洞代码。</li>
<li>results: 我们使用ETH的智能合约（SC）作为实验 case study，因为SC的安全性要求非常严格。我们先使用6亿参数的GPT-J模型进行了186397个SC的精度 fine-tuning，并将2217692个SC中的重复项去除。 fine-tuning需要一周以上的时间，使用了10个GPU。我们发现，我们的精度 fine-tuning可以生成SCs的平均BLEU分数为0.557。然而，大量的代码在自动完成后仍然存在漏洞。我们使用不同类型的漏洞代码的176个SC中的代码 перед漏洞行进行自动完成，发现超过70%的代码是不安全的。因此，我们进一步 fine-tuning了模型，使其能够避免这些漏洞。我们在另外941个漏洞SC中进行了精度 fine-tuning，并在代码生成过程中应用漏洞约束。 fine-tuning只需一个小时，使用了4个GPU。我们再次自动完成了176个SC，发现我们的方法可以将62%的代码标记为不安全，并避免生成67%的代码。<details>
<summary>Abstract</summary>
Auto-completing code enables developers to speed up coding significantly. Recent advances in transformer-based large language model (LLM) technologies have been applied to code synthesis. However, studies show that many of such synthesized codes contain vulnerabilities. We propose a novel vulnerability-constrained decoding approach to reduce the amount of vulnerable code generated by such models. Using a small dataset of labeled vulnerable lines of code, we fine-tune an LLM to include vulnerability labels when generating code, acting as an embedded classifier. Then, during decoding, we deny the model to generate these labels to avoid generating vulnerable code. To evaluate the method, we chose to automatically complete Ethereum Blockchain smart contracts (SCs) as the case study due to the strict requirements of SC security. We first fine-tuned the 6-billion-parameter GPT-J model using 186,397 Ethereum SCs after removing the duplication from 2,217,692 SCs. The fine-tuning took more than one week using ten GPUs. The results showed that our fine-tuned model could synthesize SCs with an average BLEU (BiLingual Evaluation Understudy) score of 0.557. However, many codes in the auto-completed SCs were vulnerable. Using the code before the vulnerable line of 176 SCs containing different types of vulnerabilities to auto-complete the code, we found that more than 70% of the auto-completed codes were insecure. Thus, we further fine-tuned the model on other 941 vulnerable SCs containing the same types of vulnerabilities and applied vulnerability-constrained decoding. The fine-tuning took only one hour with four GPUs. We then auto-completed the 176 SCs again and found that our approach could identify 62% of the code to be generated as vulnerable and avoid generating 67% of them, indicating the approach could efficiently and effectively avoid vulnerabilities in the auto-completed code.
</details>
<details>
<summary>摘要</summary>
自动完成代码可以大大提高开发者的编程速度。现代 transformer 基于大型语言模型（LLM）技术的应用已经被应用于代码生成。然而，研究表明，许多生成的代码含有漏洞。我们提出了一种权限 constrained decoding 方法，以避免生成的代码中漏洞。我们使用一小个标注有漏洞的代码行的数据集，并在 fine-tune 一个 LLM 以包含漏洞标签，作为内置分类器。然后，在解码时，我们不允许模型生成这些标签，以避免生成漏洞的代码。为评估方法，我们选择了完成 Ethereum Blockchain 智能合约（SC）作为研究 caso。 SC 的安全要求非常严格，因此可以很好地测试我们的方法。我们首先 fine-tune 60亿参数的 GPT-J 模型，使其能够生成 SC。我们使用了 186,397 个 Ethereum SC，并从 2,217,692 个 SC 中去掉重复。 fine-tuning 需要一周以上，使用了十个 GPU。结果表明，我们的 fine-tuned 模型可以在 SC 中获得平均 BLEU 分数为 0.557。然而，许多生成的代码中存在漏洞。我们使用不同类型的漏洞的 176 个 SC 的代码前几行来自动完成代码。我们发现，More than 70% 的自动生成代码存在漏洞。因此，我们进一步 fine-tune 模型，使其可以生成不含漏洞的代码。我们使用了 941 个漏洞 SC，并在四个 GPU 上进行 fine-tuning。 fine-tuning 只需要一个小时。然后，我们再次自动完成了 176 个 SC，发现我们的方法可以识别出 62% 的代码需要被生成，并避免生成 67% 的代码。这表明，我们的方法可以有效地避免生成的代码中漏洞。
</details></li>
</ul>
<hr>
<h2 id="Bias-of-AI-Generated-Content-An-Examination-of-News-Produced-by-Large-Language-Models"><a href="#Bias-of-AI-Generated-Content-An-Examination-of-News-Produced-by-Large-Language-Models" class="headerlink" title="Bias of AI-Generated Content: An Examination of News Produced by Large Language Models"></a>Bias of AI-Generated Content: An Examination of News Produced by Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09825">http://arxiv.org/abs/2309.09825</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao Fang, Shangkun Che, Minjia Mao, Hongzhe Zhang, Ming Zhao, Xiaohang Zhao</li>
<li>for: 本研究旨在了解大语言模型（LLM）生成的偏见。</li>
<li>methods: 我们使用七个代表性的LLM生成新闻文章的标题作为提示，并评估这些LLM生成的媒体内容中的性别和种族偏见。</li>
<li>results: 我们发现所有考测LLM都表现出了显著的性别和种族偏见，而且女性和黑人受到了明显的歧视。 chatGPT的生成内容中具有最低的偏见水平，而且它是唯一一个能够拒绝基于偏见提示生成内容的模型。<details>
<summary>Abstract</summary>
Large language models (LLMs) have the potential to transform our lives and work through the content they generate, known as AI-Generated Content (AIGC). To harness this transformation, we need to understand the limitations of LLMs. Here, we investigate the bias of AIGC produced by seven representative LLMs, including ChatGPT and LLaMA. We collect news articles from The New York Times and Reuters, both known for their dedication to provide unbiased news. We then apply each examined LLM to generate news content with headlines of these news articles as prompts, and evaluate the gender and racial biases of the AIGC produced by the LLM by comparing the AIGC and the original news articles. We further analyze the gender bias of each LLM under biased prompts by adding gender-biased messages to prompts constructed from these news headlines. Our study reveals that the AIGC produced by each examined LLM demonstrates substantial gender and racial biases. Moreover, the AIGC generated by each LLM exhibits notable discrimination against females and individuals of the Black race. Among the LLMs, the AIGC generated by ChatGPT demonstrates the lowest level of bias, and ChatGPT is the sole model capable of declining content generation when provided with biased prompts.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）有可能改变我们的生活和工作通过它们生成的内容，称为人工智能生成内容（AIGC）。为了利用这一变革，我们需要了解LLM的局限性。我们在这里调查了7个代表性的LLM中的偏见，包括ChatGPT和LLaMA。我们从纽约时报和路透社获取了不偏见的新闻文章，然后对每个检查LLM中的新闻标题作为提示，生成新闻内容，并评估生成的AI内容中的性别和种族偏见。我们进一步分析每个LLM的性别偏见情况，并通过对提示语言添加性别偏见的消息来评估每个LLM的性别偏见。我们的研究发现，每个检查LLM都表现出了显著的性别和种族偏见，而且AIGC生成的 females和黑人受到了明显的歧视。与其他LLM不同，ChatGPT的AIGC表现出最低的偏见水平，并且ChatGPT是唯一一个能够拒绝生成内容的模型。
</details></li>
</ul>
<hr>
<h2 id="VisualProg-Distiller-Learning-to-Fine-tune-Non-differentiable-Visual-Programming-Frameworks"><a href="#VisualProg-Distiller-Learning-to-Fine-tune-Non-differentiable-Visual-Programming-Frameworks" class="headerlink" title="VisualProg Distiller: Learning to Fine-tune Non-differentiable Visual Programming Frameworks"></a>VisualProg Distiller: Learning to Fine-tune Non-differentiable Visual Programming Frameworks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09809">http://arxiv.org/abs/2309.09809</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wentao Wan, Zeqing Wang, Nan Kang, Keze Wang, Zhiyu Shen, Liang Lin</li>
<li>for: 提高VisualProg的实用性和任务性能。</li>
<li>methods: 提出了一种基于教师模型的VisualProg Distiller方法，通过逐步填充和筛选知识来优化VisualProg子模块的表现，从而提高整体任务性能。</li>
<li>results: 经过广泛和全面的实验评估，提出的方法可以大幅提高VisualProg的性能，并超越所有比较方法。<details>
<summary>Abstract</summary>
As an interpretable and universal neuro-symbolic paradigm based on Large Language Models, visual programming (VisualProg) can execute compositional visual tasks without training, but its performance is markedly inferior compared to task-specific supervised learning models. To increase its practicality, the performance of VisualProg on specific tasks needs to be improved. However, the non-differentiability of VisualProg limits the possibility of employing the fine-tuning strategy on specific tasks to achieve further improvements. In our analysis, we discovered that significant performance issues in VisualProg's execution originated from errors made by the sub-modules at corresponding visual sub-task steps. To address this, we propose ``VisualProg Distiller", a method of supplementing and distilling process knowledge to optimize the performance of each VisualProg sub-module on decoupled visual sub-tasks, thus enhancing the overall task performance. Specifically, we choose an end-to-end model that is well-performed on the given task as the teacher and further distill the knowledge of the teacher into the invoked visual sub-modules step-by-step based on the execution flow of the VisualProg-generated programs. In this way, our method is capable of facilitating the fine-tuning of the non-differentiable VisualProg frameworks effectively. Extensive and comprehensive experimental evaluations demonstrate that our method can achieve a substantial performance improvement of VisualProg, and outperforms all the compared state-of-the-art methods by large margins. Furthermore, to provide valuable process supervision for the GQA task, we construct a large-scale dataset by utilizing the distillation process of our method.
</details>
<details>
<summary>摘要</summary>
为了提高VisualProg在特定任务上的实际性，我们需要提高它在特定任务上的性能。然而，VisualProg的非导数性限制了我们使用精度调教策略来实现进一步改进的可能性。在我们的分析中，我们发现了VisualProg执行过程中的显著性能问题，这些问题主要来自于VisualProg中的子模块在相应的视觉子任务步骤上所作出的错误。为了解决这个问题，我们提出了“VisualProg Distiller”方法，该方法通过补充和涂抹过程知识来优化VisualProg子模块在分离的视觉子任务上的性能，从而提高整体任务性能。具体来说，我们选择了在给定任务上表现良好的端到端模型作为教师，然后通过执行VisualProg生成的程序的执行流程步骤weise distill the teacher's knowledge into the invoked visual sub-modules。这种方法可以有效地调整不导数的VisualProg框架。我们的方法在广泛和全面的实验评估中表现出色，可以大幅提高VisualProg的性能，并与所有相关的状态前方法相比而言，表现出大幅度的优势。此外，为了为GQA任务提供有价值的过程监督，我们利用我们的方法construct了一个大规模的 dataset。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Concept-Drift-Handling-for-Batch-Android-Malware-Detection-Models"><a href="#Efficient-Concept-Drift-Handling-for-Batch-Android-Malware-Detection-Models" class="headerlink" title="Efficient Concept Drift Handling for Batch Android Malware Detection Models"></a>Efficient Concept Drift Handling for Batch Android Malware Detection Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09807">http://arxiv.org/abs/2309.09807</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://gitlab.com/serralba/concept_drift">https://gitlab.com/serralba/concept_drift</a></li>
<li>paper_authors: Molina-Coronado B., Mori U., Mendiburu A., Miguel-Alonso J<br>for: This paper aims to address the challenge of maintaining the performance of static machine learning-based malware detectors in rapidly evolving Android app environments.methods: The paper employs retraining techniques to maintain detector capabilities over time, and analyzes the effect of two aspects (frequency of retraining and data used for retraining) on efficiency and performance.results: The experiments show that concept drift detection and sample selection mechanisms can be used to efficiently maintain the performance of static Android malware state-of-the-art detectors in changing environments.<details>
<summary>Abstract</summary>
The rapidly evolving nature of Android apps poses a significant challenge to static batch machine learning algorithms employed in malware detection systems, as they quickly become obsolete. Despite this challenge, the existing literature pays limited attention to addressing this issue, with many advanced Android malware detection approaches, such as Drebin, DroidDet and MaMaDroid, relying on static models. In this work, we show how retraining techniques are able to maintain detector capabilities over time. Particularly, we analyze the effect of two aspects in the efficiency and performance of the detectors: 1) the frequency with which the models are retrained, and 2) the data used for retraining. In the first experiment, we compare periodic retraining with a more advanced concept drift detection method that triggers retraining only when necessary. In the second experiment, we analyze sampling methods to reduce the amount of data used to retrain models. Specifically, we compare fixed sized windows of recent data and state-of-the-art active learning methods that select those apps that help keep the training dataset small but diverse. Our experiments show that concept drift detection and sample selection mechanisms result in very efficient retraining strategies which can be successfully used to maintain the performance of the static Android malware state-of-the-art detectors in changing environments.
</details>
<details>
<summary>摘要</summary>
“Android应用的快速演化带来了静态批处理机器学习算法在恶意软件检测系统中的挑战，这些算法很快就会过时。然而，现有的文献对此问题的解决方案很少，许多高级Android恶意软件检测方法，如Drebin、DroidDet和MaMaDroid，仍然采用静态模型。在这项工作中，我们展示了如何使用重新训练技术维护检测器的能力。特别是，我们分析了两个方面对检测器的效率和性能的影响：1）模型重新训练的频率，和2）重新训练使用的数据。在第一个实验中，我们比较了定期重新训练和更先进的概念漂移检测方法，该方法只在必要时触发重新训练。在第二个实验中，我们分析了采用不同大小的固定窗口和最新的活动学习方法来减少模型重新训练所需的数据量。我们的实验结果表明，概念漂移检测和样本选择机制可以实现非常高效的重新训练策略，可以成功地维护静态Android恶意软件检测器在变化环境中的性能。”
</details></li>
</ul>
<hr>
<h2 id="Harnessing-Collective-Intelligence-Under-a-Lack-of-Cultural-Consensus"><a href="#Harnessing-Collective-Intelligence-Under-a-Lack-of-Cultural-Consensus" class="headerlink" title="Harnessing Collective Intelligence Under a Lack of Cultural Consensus"></a>Harnessing Collective Intelligence Under a Lack of Cultural Consensus</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09787">http://arxiv.org/abs/2309.09787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Necdet Gürkan, Jordan W. Suchow</li>
<li>for: This paper aims to provide a computational foundation for harnessing collective intelligence in the absence of cultural consensus.</li>
<li>methods: The paper introduces Infinite Deep Latent Construct Cultural Consensus Theory (iDLC-CCT), a nonparametric Bayesian model that extends Cultural Consensus Theory with a latent construct that maps between pretrained deep neural network embeddings of entities and the consensus beliefs regarding those entities.</li>
<li>results: The iDLC-CCT model better predicts the degree of consensus, generalizes well to out-of-sample entities, and is effective even with sparse data. An efficient hard-clustering variant of the iDLC-CCT is also introduced to improve scalability.<details>
<summary>Abstract</summary>
Harnessing collective intelligence to drive effective decision-making and collaboration benefits from the ability to detect and characterize heterogeneity in consensus beliefs. This is particularly true in domains such as technology acceptance or leadership perception, where a consensus defines an intersubjective truth, leading to the possibility of multiple "ground truths" when subsets of respondents sustain mutually incompatible consensuses. Cultural Consensus Theory (CCT) provides a statistical framework for detecting and characterizing these divergent consensus beliefs. However, it is unworkable in modern applications because it lacks the ability to generalize across even highly similar beliefs, is ineffective with sparse data, and can leverage neither external knowledge bases nor learned machine representations. Here, we overcome these limitations through Infinite Deep Latent Construct Cultural Consensus Theory (iDLC-CCT), a nonparametric Bayesian model that extends CCT with a latent construct that maps between pretrained deep neural network embeddings of entities and the consensus beliefs regarding those entities among one or more subsets of respondents. We validate the method across domains including perceptions of risk sources, food healthiness, leadership, first impressions, and humor. We find that iDLC-CCT better predicts the degree of consensus, generalizes well to out-of-sample entities, and is effective even with sparse data. To improve scalability, we introduce an efficient hard-clustering variant of the iDLC-CCT using an algorithm derived from a small-variance asymptotic analysis of the model. The iDLC-CCT, therefore, provides a workable computational foundation for harnessing collective intelligence under a lack of cultural consensus and may potentially form the basis of consensus-aware information technologies.
</details>
<details>
<summary>摘要</summary>
使用集体智能来驱动有效的决策和协作，可以利用检测和特征化多样性的共识信仰来获得优势。特别是在技术接受或领导 восприятие等领域，共识定义了间subjective truth，可能导致多个"真实"当 subsets of respondents sustain mutually incompatible consensuses。文化共识理论（CCT）提供了一个统计学方法来检测和特征化这些分歧的共识信仰。然而，它在现代应用中无法普及，因为它缺乏对类似信仰的普适化能力，不可靠于稀缺数据，并且无法利用外部知识库或学习机器表示。在这里，我们超越这些限制通过无穷深层 latent construct cultural consensus theory（iDLC-CCT），一种非 Parametric Bayesian 模型，它将 CCT 扩展到一个 latent construct，该 construct 将预训练的深度神经网络嵌入与共识信仰相关的一个或多个 respondents 之间的Entity的 consensus beliefs 进行映射。我们在风险来源、食品健康、领导、第一印象和幽默等领域进行验证，发现 iDLC-CCT 更好地预测度量共识，普适化能力强，稀缺数据效果良好。为了提高可扩展性，我们引入一种高效硬分 clustering 变体，使用基于小差强 asymptotic analysis 的模型。因此，iDLC-CCT 提供了在缺乏文化共识的情况下可行的计算基础，可能成为共识意识技术的基础。
</details></li>
</ul>
<hr>
<h2 id="How-to-Data-in-Datathons"><a href="#How-to-Data-in-Datathons" class="headerlink" title="How to Data in Datathons"></a>How to Data in Datathons</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09770">http://arxiv.org/abs/2309.09770</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/YLee-ArtsCommission/Arts-Datathon">https://github.com/YLee-ArtsCommission/Arts-Datathon</a></li>
<li>paper_authors: Carlos Mougan, Richard Plant, Clare Teng, Marya Bazzi, Alvaro Cabregas Ejea, Ryan Sze-Yin Chan, David Salvador Jasin, Martin Stoffel, Kirstie Jane Whitaker, Jules Manser</li>
<li>for: 该论文旨在为 datathon 组织者提供指南和最佳实践，帮助他们更好地处理数据相关的复杂问题。</li>
<li>methods: 该论文根据作者自己的经验和 &gt;60 个合作机构的见解，提出了一套指南和建议，以帮助组织者在 datathon 中更好地管理数据。</li>
<li>results: 该论文通过应用自己的框架，对 10 个案例进行分析，以帮助组织者更好地理解和解决数据相关的问题。<details>
<summary>Abstract</summary>
The rise of datathons, also known as data or data science hackathons, has provided a platform to collaborate, learn, and innovate in a short timeframe. Despite their significant potential benefits, organizations often struggle to effectively work with data due to a lack of clear guidelines and best practices for potential issues that might arise. Drawing on our own experiences and insights from organizing >80 datathon challenges with >60 partnership organizations since 2016, we provide guidelines and recommendations that serve as a resource for organizers to navigate the data-related complexities of datathons. We apply our proposed framework to 10 case studies.
</details>
<details>
<summary>摘要</summary>
“数据马拉松”的出现，也称为“数据”或“数据科学”冲击活动，为团队合作、学习和创新提供了一个短时间内的平台。尽管它们具有重要的可能收益，但组织 frequently struggle to effectively work with data due to a lack of clear guidelines and best practices for potential issues that might arise. 基于我们自己的经验和2016年以来与60家合作组织合作的80多个数据马拉松挑战，我们提供了指南和建议，用于帮助组织 navigate数据相关的复杂性。我们将我们的提议框架应用于10个案例研究。
</details></li>
</ul>
<hr>
<h2 id="Looking-through-the-past-better-knowledge-retention-for-generative-replay-in-continual-learning"><a href="#Looking-through-the-past-better-knowledge-retention-for-generative-replay-in-continual-learning" class="headerlink" title="Looking through the past: better knowledge retention for generative replay in continual learning"></a>Looking through the past: better knowledge retention for generative replay in continual learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10012">http://arxiv.org/abs/2309.10012</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/valeriya-khan/looking-through-the-past">https://github.com/valeriya-khan/looking-through-the-past</a></li>
<li>paper_authors: Valeriya Khan, Sebastian Cygert, Kamil Deja, Tomasz Trzciński, Bartłomiej Twardowski</li>
<li>For: The paper aims to improve the generative replay in a continual learning setting to perform well on challenging scenarios, where current generative rehearsal methods are not powerful enough to generate complex data with a greater number of classes.* Methods: The proposed method incorporates distillation in the latent space between the current and previous models to reduce feature drift, and uses latent matching for reconstruction and original data to improve generated features alignment. Additionally, the method cycles through generations using the previously trained model to make them closer to the original data.* Results: The proposed method outperforms other generative replay methods in various scenarios.Here’s the same information in Simplified Chinese:* For: 该论文目的是在不断学习Setting中提高生成重温来实现复杂enario中的好势所在，当前的生成重温方法通常只能在小型、简单的数据集上进行测试。* Methods: 提议的方法包括在 latent space中进行知识传递，并通过 reconstruction和原始数据之间的匹配来改善生成的特征对齐。此外，该方法还在生成过程中循环使用之前训练的模型，以使生成的数据更加接近原始数据。* Results: 提议的方法在多种情况下表现出色，超过了其他生成重温方法。<details>
<summary>Abstract</summary>
In this work, we improve the generative replay in a continual learning setting to perform well on challenging scenarios. Current generative rehearsal methods are usually benchmarked on small and simple datasets as they are not powerful enough to generate more complex data with a greater number of classes. We notice that in VAE-based generative replay, this could be attributed to the fact that the generated features are far from the original ones when mapped to the latent space. Therefore, we propose three modifications that allow the model to learn and generate complex data. More specifically, we incorporate the distillation in latent space between the current and previous models to reduce feature drift. Additionally, a latent matching for the reconstruction and original data is proposed to improve generated features alignment. Further, based on the observation that the reconstructions are better for preserving knowledge, we add the cycling of generations through the previously trained model to make them closer to the original data. Our method outperforms other generative replay methods in various scenarios. Code available at https://github.com/valeriya-khan/looking-through-the-past.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们改进了 continual learning 中的生成重温方法，以便在复杂的场景下表现良好。现有的生成重温方法通常在小型和简单的数据集上进行评估，因为它们无法生成更复杂的数据集 with 更多的类别。我们发现，在 VAE 基于的生成重温方法中，这可能是因为生成的特征与原始特征在幂等空间中的映射相比较远。因此，我们提议三种修改，让模型学习和生成复杂数据。更 Specifically，我们在幂等空间中添加了采样的练习，以降低特征漂移。其次，我们提出了准确重建和原始数据的匹配，以提高生成特征的对齐。此外，根据我们发现，重建是保持知识的好方法，我们在先前训练的模型中循环生成，使其更接近原始数据。我们的方法在多种场景下表现出色，代码可以在 GitHub 上找到：https://github.com/valeriya-khan/looking-through-the-past。
</details></li>
</ul>
<hr>
<h2 id="Moving-Object-Detection-and-Tracking-with-4D-Radar-Point-Cloud"><a href="#Moving-Object-Detection-and-Tracking-with-4D-Radar-Point-Cloud" class="headerlink" title="Moving Object Detection and Tracking with 4D Radar Point Cloud"></a>Moving Object Detection and Tracking with 4D Radar Point Cloud</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09737">http://arxiv.org/abs/2309.09737</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhijun Pan, Fangqiang Ding, Hantao Zhong, Chris Xiaoxuan Lu</li>
<li>for: 本文针对radar图像跟踪问题提出了一个新的解决方案，即RaTrack。</li>
<li>methods: RaTrack方法强调运动分割和归一化，并且具有运动估计模块，以增强跟踪精度。</li>
<li>results: 在View-of-Delft数据集上，RaTrack方法与现有方法相比，跟踪精度明显提高，表现出优于现有方法。<details>
<summary>Abstract</summary>
Mobile autonomy relies on the precise perception of dynamic environments. Robustly tracking moving objects in 3D world thus plays a pivotal role for applications like trajectory prediction, obstacle avoidance, and path planning. While most current methods utilize LiDARs or cameras for Multiple Object Tracking (MOT), the capabilities of 4D imaging radars remain largely unexplored. Recognizing the challenges posed by radar noise and point sparsity in 4D radar data, we introduce RaTrack, an innovative solution tailored for radar-based tracking. Bypassing the typical reliance on specific object types and 3D bounding boxes, our method focuses on motion segmentation and clustering, enriched by a motion estimation module. Evaluated on the View-of-Delft dataset, RaTrack showcases superior tracking precision of moving objects, largely surpassing the performance of the state of the art.
</details>
<details>
<summary>摘要</summary>
mobile自主需要精准感知动态环境。在3D世界中准确跟踪移动 объек的 tracking thus plays a crucial role for applications like trajectory prediction, obstacle avoidance, and path planning. While most current methods使用 LiDARs or cameras for Multiple Object Tracking (MOT), the capabilities of 4D imaging radars remain largely unexplored. Recognizing the challenges posed by radar noise and point sparsity in 4D radar data, we introduce RaTrack, an innovative solution tailored for radar-based tracking. Bypassing the typical reliance on specific object types and 3D bounding boxes, our method focuses on motion segmentation and clustering, enriched by a motion estimation module. Evaluated on the View-of-Delft dataset, RaTrack showcases superior tracking precision of moving objects, largely surpassing the performance of the state of the art.Here's the breakdown of the translation:* mobile自主 (mobile autonomy) - 移动自主* 需要 (need) - 需要* 精准感知 (precise perception) - 精准感知* 动态环境 (dynamic environments) - 动态环境*  track (tracking) - 跟踪* 移动 objet (moving objects) - 移动 объек* MOT (Multiple Object Tracking) - MOT* 4D imaging radars (4D imaging radars) - 4D成像雷达* 挑战 (challenges) - 挑战* 点缺乏 (point sparsity) - 点缺乏* RaTrack (RaTrack) - RaTrack* 解决方案 (solution) - 解决方案* 强调 (focusing) - 强调* 动态分割 (motion segmentation) - 动态分割* 聚集 (clustering) - 聚集* 动态估计 (motion estimation) - 动态估计* 评估 (evaluated) - 评估* 视野-of-Delft (View-of-Delft) - 视野-of-Delft* 显示 (showcases) - 显示* 超越 (surpassing) - 超越* 状态前的性能 (performance of the state of the art) - 状态前的性能
</details></li>
</ul>
<hr>
<h2 id="A-Quantum-Optimization-Case-Study-for-a-Transport-Robot-Scheduling-Problem"><a href="#A-Quantum-Optimization-Case-Study-for-a-Transport-Robot-Scheduling-Problem" class="headerlink" title="A Quantum Optimization Case Study for a Transport Robot Scheduling Problem"></a>A Quantum Optimization Case Study for a Transport Robot Scheduling Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09736">http://arxiv.org/abs/2309.09736</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominik Leib, Tobias Seidel, Sven Jäger, Raoul Heese, Caitlin Isobel Jones, Abhishek Awasthi, Astrid Niederle, Michael Bortz</li>
<li>for: 这个研究是为了比较D-Wave的量子-类别混合框架、Fujitsu的量子静态逻辑器和Gurobi的状态当前的类别解决器在解决交通机器人分配问题的性能。这个问题来自实际的工业问题。</li>
<li>methods: 我们提供了三个不同的模型来解决这个问题，按照不同的设计哲学。我们在比较不同的模型和解决器组合的终端运行时间和解决质量方面进行了对比。</li>
<li>results: 我们发现了静态逻辑器和混合量子逻辑器在直接与Gurobi进行比较时有推荐的结果，并提供了一些机会。我们的研究提供了应用导向优化问题的工作流程和不同策略的评估，可以用于评估不同方法的优缺点。<details>
<summary>Abstract</summary>
We present a comprehensive case study comparing the performance of D-Waves' quantum-classical hybrid framework, Fujitsu's quantum-inspired digital annealer, and Gurobi's state-of-the-art classical solver in solving a transport robot scheduling problem. This problem originates from an industrially relevant real-world scenario. We provide three different models for our problem following different design philosophies. In our benchmark, we focus on the solution quality and end-to-end runtime of different model and solver combinations. We find promising results for the digital annealer and some opportunities for the hybrid quantum annealer in direct comparison with Gurobi. Our study provides insights into the workflow for solving an application-oriented optimization problem with different strategies, and can be useful for evaluating the strengths and weaknesses of different approaches.
</details>
<details>
<summary>摘要</summary>
我们对三种不同的方法和解决方案进行了全面的比较研究：D-Wave的量子-经典混合框架、Fujitsu的量子启发数字拓扑器和Gurobi的当今最佳的类别解决器。这个问题来自实际工况中的 industrially  relevantereal-world  scenario。我们提供三个不同的模型来解决这个问题，每个模型都有不同的设计哲学。在我们的比较中，我们关注的是不同模型和解决器组合的解决质量和综合时间。我们发现了数字拓扑器的批处理和混合量子拓扑器在直接与Gurobi进行比较中的抢夺。我们的研究为解决应用导向优化问题的不同策略提供了深入的视野，并且可以用于评估不同方法的优缺点。
</details></li>
</ul>
<hr>
<h2 id="LLM4Jobs-Unsupervised-occupation-extraction-and-standardization-leveraging-Large-Language-Models"><a href="#LLM4Jobs-Unsupervised-occupation-extraction-and-standardization-leveraging-Large-Language-Models" class="headerlink" title="LLM4Jobs: Unsupervised occupation extraction and standardization leveraging Large Language Models"></a>LLM4Jobs: Unsupervised occupation extraction and standardization leveraging Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09708">http://arxiv.org/abs/2309.09708</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aida-ugent/skillgpt">https://github.com/aida-ugent/skillgpt</a></li>
<li>paper_authors: Nan Li, Bo Kang, Tijl De Bie</li>
<li>for: 本研究旨在探讨使用大语言模型（LLM）实现自动化职业抽取和标准化，以便于职业推荐和劳动市场政策制定。</li>
<li>methods: 本研究提出了一种新的无监督方法——LLM4Jobs，利用大语言模型的自然语言理解和生成能力来实现职业编码。</li>
<li>results: 经过严格的实验评估，LLM4Jobs方法在不同的数据集和粒度上 consistently 超越了现有的无监督状况标准做法，展示其在多样化数据集和粒度上的可变性。<details>
<summary>Abstract</summary>
Automated occupation extraction and standardization from free-text job postings and resumes are crucial for applications like job recommendation and labor market policy formation. This paper introduces LLM4Jobs, a novel unsupervised methodology that taps into the capabilities of large language models (LLMs) for occupation coding. LLM4Jobs uniquely harnesses both the natural language understanding and generation capacities of LLMs. Evaluated on rigorous experimentation on synthetic and real-world datasets, we demonstrate that LLM4Jobs consistently surpasses unsupervised state-of-the-art benchmarks, demonstrating its versatility across diverse datasets and granularities. As a side result of our work, we present both synthetic and real-world datasets, which may be instrumental for subsequent research in this domain. Overall, this investigation highlights the promise of contemporary LLMs for the intricate task of occupation extraction and standardization, laying the foundation for a robust and adaptable framework relevant to both research and industrial contexts.
</details>
<details>
<summary>摘要</summary>
自动化职业抽取和标准化从自由文本职业广告和简历中提取职业信息是关键 для应用程序如职业推荐和劳动市场政策形成。这篇论文介绍了LLM4Jobs，一种新的无监督方法，利用大型自然语言模型（LLM）的自然语言理解和生成能力来实现职业编码。我们在尝试中rigorous experimentation on synthetic and real-world datasets，发现LLM4Jobs可以在多种数据集和粒度上具有优秀的表现，并且在不同的职业类别和粒度上具有很好的灵活性。此外，我们还提供了一些synthetic和实际 datasets，这些数据可能会对后续在这个领域的研究提供很好的参考。总之，这项研究展示了当今LLM的潜在能力在职业抽取和标准化中，为研究和工业上的应用提供了一个强大和灵活的基础。
</details></li>
</ul>
<hr>
<h2 id="Information-based-explanation-methods-for-deep-learning-agents-–-with-applications-on-large-open-source-chess-models"><a href="#Information-based-explanation-methods-for-deep-learning-agents-–-with-applications-on-large-open-source-chess-models" class="headerlink" title="Information based explanation methods for deep learning agents – with applications on large open-source chess models"></a>Information based explanation methods for deep learning agents – with applications on large open-source chess models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09702">http://arxiv.org/abs/2309.09702</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/patrik-ha/ii-map">https://github.com/patrik-ha/ii-map</a></li>
<li>paper_authors: Patrik Hammersborg, Inga Strümke</li>
<li>for: 这个研究旨在使用大型开源棋牌模型来实现透明AI（XAI）方法，以解释具有相似性表现的 alphaZero 模型。</li>
<li>methods: 这种 XAI 方法使用可视化解释，以便在棋牌游戏中解释模型的决策。它可以控制输入向模型传递的信息，从而提供精确的解释。</li>
<li>results: 研究人员使用这种 XAI 方法对标准 8x8 棋牌进行了应用，并取得了类似于 alphaZero 的性能。<details>
<summary>Abstract</summary>
With large chess-playing neural network models like AlphaZero contesting the state of the art within the world of computerised chess, two challenges present themselves: The question of how to explain the domain knowledge internalised by such models, and the problem that such models are not made openly available. This work presents the re-implementation of the concept detection methodology applied to AlphaZero in McGrath et al. (2022), by using large, open-source chess models with comparable performance. We obtain results similar to those achieved on AlphaZero, while relying solely on open-source resources. We also present a novel explainable AI (XAI) method, which is guaranteed to highlight exhaustively and exclusively the information used by the explained model. This method generates visual explanations tailored to domains characterised by discrete input spaces, as is the case for chess. Our presented method has the desirable property of controlling the information flow between any input vector and the given model, which in turn provides strict guarantees regarding what information is used by the trained model during inference. We demonstrate the viability of our method by applying it to standard 8x8 chess, using large open-source chess models.
</details>
<details>
<summary>摘要</summary>
With large chess-playing neural network models like AlphaZero contesting the state of the art within the world of computerised chess, two challenges present themselves: The question of how to explain the domain knowledge internalised by such models, and the problem that such models are not made openly available. This work presents the re-implementation of the concept detection methodology applied to AlphaZero in McGrath et al. (2022), by using large, open-source chess models with comparable performance. We obtain results similar to those achieved on AlphaZero, while relying solely on open-source resources. We also present a novel explainable AI (XAI) method, which is guaranteed to highlight exhaustively and exclusively the information used by the explained model. This method generates visual explanations tailored to domains characterised by discrete input spaces, as is the case for chess. Our presented method has the desirable property of controlling the information flow between any input vector and the given model, which in turn provides strict guarantees regarding what information is used by the trained model during inference. We demonstrate the viability of our method by applying it to standard 8x8 chess, using large open-source chess models.Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Securing-Fixed-Neural-Network-Steganography"><a href="#Securing-Fixed-Neural-Network-Steganography" class="headerlink" title="Securing Fixed Neural Network Steganography"></a>Securing Fixed Neural Network Steganography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09700">http://arxiv.org/abs/2309.09700</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zicong Luo, Sheng Li, Guobiao Li, Zhenxing Qian, Xinpeng Zhang</li>
<li>for: 这个研究是为了提高图像隐藏技术的安全性和可见性。</li>
<li>methods: 研究使用固定神经网络（FNN）进行秘密嵌入和抽出，并通过生成钥匙控制的扰动来提高安全性。</li>
<li>results: 实验结果显示，提案的方案能够防止未授权者从隐藏图像中提取秘密，并且能够生成高品质的隐藏图像，特别是当FNN是一个用于普通学习任务的神经网络时。<details>
<summary>Abstract</summary>
Image steganography is the art of concealing secret information in images in a way that is imperceptible to unauthorized parties. Recent advances show that is possible to use a fixed neural network (FNN) for secret embedding and extraction. Such fixed neural network steganography (FNNS) achieves high steganographic performance without training the networks, which could be more useful in real-world applications. However, the existing FNNS schemes are vulnerable in the sense that anyone can extract the secret from the stego-image. To deal with this issue, we propose a key-based FNNS scheme to improve the security of the FNNS, where we generate key-controlled perturbations from the FNN for data embedding. As such, only the receiver who possesses the key is able to correctly extract the secret from the stego-image using the FNN. In order to improve the visual quality and undetectability of the stego-image, we further propose an adaptive perturbation optimization strategy by taking the perturbation cost into account. Experimental results show that our proposed scheme is capable of preventing unauthorized secret extraction from the stego-images. Furthermore, our scheme is able to generate stego-images with higher visual quality than the state-of-the-art FNNS scheme, especially when the FNN is a neural network for ordinary learning tasks.
</details>
<details>
<summary>摘要</summary>
Image 隐藏技术是隐藏秘密信息在图像中，以便只有授权方可以访问。现有研究表明，可以使用固定神经网络（FNN）进行秘密嵌入和抽取。称为固定神经网络隐藏技术（FNNS），这种技术可以在无需训练神经网络的情况下实现高度的隐藏性。然而，现有的FNNS方案具有一定的漏洞，即任何人都可以从隐藏图像中提取秘密。为了解决这个问题，我们提议使用密钥控制的FNNS方案，其中我们从FNN中生成密钥控制的扰动来用于数据嵌入。因此，只有持有密钥的接收方可以使用FNN correctly提取秘密从隐藏图像中。为了提高隐藏图像的视觉质量和不可察觉性，我们进一步提议一种适应性优化策略，其中考虑扰动成本。实验结果表明，我们的提议方案可以防止未经授权的秘密提取。此外，我们的方案还可以生成高质量的隐藏图像，特别是当FNN是一个用于常规学习任务的神经网络时。
</details></li>
</ul>
<hr>
<h2 id="Noise-Augmented-Boruta-The-Neural-Network-Perturbation-Infusion-with-Boruta-Feature-Selection"><a href="#Noise-Augmented-Boruta-The-Neural-Network-Perturbation-Infusion-with-Boruta-Feature-Selection" class="headerlink" title="Noise-Augmented Boruta: The Neural Network Perturbation Infusion with Boruta Feature Selection"></a>Noise-Augmented Boruta: The Neural Network Perturbation Infusion with Boruta Feature Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09694">http://arxiv.org/abs/2309.09694</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hassan Gharoun, Navid Yazdanjoe, Mohammad Sadegh Khorshidi, Amir H. Gandomi<br>for: 这篇论文的目的是提出一种对Boruta标本选择算法进行改进，以提高其选择功能和精度。methods: 本论文使用了对Boruta算法中的shadow variable进行噪声添加，并将其与人工神经网络的损害分析框架相似。results: 根据四个公开的 benchmark dataset的实验结果显示，提出的方法比传统的Boruta算法表现更好，证明了这种改进的可能性和价值。<details>
<summary>Abstract</summary>
With the surge in data generation, both vertically (i.e., volume of data) and horizontally (i.e., dimensionality), the burden of the curse of dimensionality has become increasingly palpable. Feature selection, a key facet of dimensionality reduction techniques, has advanced considerably to address this challenge. One such advancement is the Boruta feature selection algorithm, which successfully discerns meaningful features by contrasting them to their permutated counterparts known as shadow features. However, the significance of a feature is shaped more by the data's overall traits than by its intrinsic value, a sentiment echoed in the conventional Boruta algorithm where shadow features closely mimic the characteristics of the original ones. Building on this premise, this paper introduces an innovative approach to the Boruta feature selection algorithm by incorporating noise into the shadow variables. Drawing parallels from the perturbation analysis framework of artificial neural networks, this evolved version of the Boruta method is presented. Rigorous testing on four publicly available benchmark datasets revealed that this proposed technique outperforms the classic Boruta algorithm, underscoring its potential for enhanced, accurate feature selection.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese translation:随着数据生成的快速增长，both vertically (i.e., 数据量)和horizontally (i.e., 维度)，“维度束缚”的压力变得越来越明显。特别是Feature selection，这是维度减少技术的关键方面，在这个挑战中得到了显著的进步。一种如此进步的算法是Boruta feature selection algorithm，它成功地从 permutated counterparts 中分别出 meaningful features。然而，一个特征的重要性更多地受到数据的总特征影响，而不是其内在价值，这种想法也在经典的Boruta算法中体现出来，shadow features 与原始特征几乎一致。在这个基础上，这篇论文提出了一种新的Boruta feature selection algorithm的方法，通过在阴影变量中添加噪音。从人工神经网络的扰动分析框架中启发，这种演进版的Boruta方法被提出。经过严格的测试，这种提议的方法在四个公共可用的 benchmark 数据集上表现出色，超过经典Boruta算法，这说明了这种方法的潜在准确性。
</details></li>
</ul>
<hr>
<h2 id="Ugly-Ducklings-or-Swans-A-Tiered-Quadruplet-Network-with-Patient-Specific-Mining-for-Improved-Skin-Lesion-Classification"><a href="#Ugly-Ducklings-or-Swans-A-Tiered-Quadruplet-Network-with-Patient-Specific-Mining-for-Improved-Skin-Lesion-Classification" class="headerlink" title="Ugly Ducklings or Swans: A Tiered Quadruplet Network with Patient-Specific Mining for Improved Skin Lesion Classification"></a>Ugly Ducklings or Swans: A Tiered Quadruplet Network with Patient-Specific Mining for Improved Skin Lesion Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09689">http://arxiv.org/abs/2309.09689</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nathasha Naranpanawa, H. Peter Soyer, Adam Mothershaw, Gayan K. Kulatilleke, Zongyuan Ge, Brigid Betz-Stablein, Shekhar S. Chandra</li>
<li>For: 帮助诊断皮肤癌（cutaneous melanoma），通过 diferenciating 高度可疑的皮肤损伤和非高度可疑的损伤。* Methods: 使用深度度量学习网络（Deep Metric Learning Network），包括两级lesion feature学习：个体水平和损伤水平。还使用了个体特定的四元组挖掘方法和层次四元组网络，以便学习更多的上下文信息。* Results: 相比传统分类器，提出的方法实现了54%高于基线ResNet18 CNN的敏感性和37%高于Naive triplet网络的敏感性，在识别ugly duckling损伤中表现出色。数据 manifold 的视觉化也表明了 DMT-Quadruplet 可以成功地在个体特定和个体不特定的情况下分类ugly duckling损伤。<details>
<summary>Abstract</summary>
An ugly duckling is an obviously different skin lesion from surrounding lesions of an individual, and the ugly duckling sign is a criterion used to aid in the diagnosis of cutaneous melanoma by differentiating between highly suspicious and benign lesions. However, the appearance of pigmented lesions, can change drastically from one patient to another, resulting in difficulties in visual separation of ugly ducklings. Hence, we propose DMT-Quadruplet - a deep metric learning network to learn lesion features at two tiers - patient-level and lesion-level. We introduce a patient-specific quadruplet mining approach together with a tiered quadruplet network, to drive the network to learn more contextual information both globally and locally between the two tiers. We further incorporate a dynamic margin within the patient-specific mining to allow more useful quadruplets to be mined within individuals. Comprehensive experiments show that our proposed method outperforms traditional classifiers, achieving 54% higher sensitivity than a baseline ResNet18 CNN and 37% higher than a naive triplet network in classifying ugly duckling lesions. Visualisation of the data manifold in the metric space further illustrates that DMT-Quadruplet is capable of classifying ugly duckling lesions in both patient-specific and patient-agnostic manner successfully.
</details>
<details>
<summary>摘要</summary>
“一只鸡蛋”是指一个个体细胞肿瘤与周围细胞不同，并且“一只鸡蛋”标准是用于诊断皮肤恶性癌的重要依据。然而，细胞肿瘤的外观可以在不同的患者之间有很大的变化，从而使得视觉分离变得困难。因此，我们提议使用深度度量学习网络（DMT-Quadruplet），以学习皮肤病变 lesion 的特征。我们首先引入了patient-specific quadruplet mining方法，并将其与 tiered quadruplet 网络结合，以使网络学习更多的上下文信息。此外，我们还在patient-specific mining中引入了动态边缘，以让更多有用的quadruplets被挖掘出来。实验表明，我们的提议方法可以超过传统分类器，达到54%的敏感性和37%的特征选择率。图像 manifold 在度量空间的Visual化也证明了DMT-Quadruplet 可以成功地分类“一只鸡蛋” lesion 在patient-specific和patient-agnostic 方面。
</details></li>
</ul>
<hr>
<h2 id="Distributed-course-allocation-with-asymmetric-friendships"><a href="#Distributed-course-allocation-with-asymmetric-friendships" class="headerlink" title="Distributed course allocation with asymmetric friendships"></a>Distributed course allocation with asymmetric friendships</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09684">http://arxiv.org/abs/2309.09684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilya Khakhiashvili, Lihi Dery, Tal Grinshpoun</li>
<li>for: 本研究旨在考虑学生之间的友谊关系，为学生分配课程 seats 提供一种分布式解决方案。</li>
<li>methods: 本文使用非对称分布式约束优化问题来模型问题，并开发了一种专门的算法。</li>
<li>results: Results show that our algorithm can obtain high utility for students while ensuring fairness and observing course seat capacity limitations.<details>
<summary>Abstract</summary>
Students' decisions on whether to take a class are strongly affected by whether their friends plan to take the class with them. A student may prefer to be assigned to a course they likes less, just to be with their friends, rather than taking a more preferred class alone. It has been shown that taking classes with friends positively affects academic performance. Thus, academic institutes should prioritize friendship relations when assigning course seats. The introduction of friendship relations results in several non-trivial changes to current course allocation methods. This paper explores how course allocation mechanisms can account for friendships between students and provide a unique, distributed solution. In particular, we model the problem as an asymmetric distributed constraint optimization problem and develop a new dedicated algorithm. Our extensive evaluation includes both simulated data and data derived from a user study on 177 students' preferences over courses and friends. The results show that our algorithm obtains high utility for the students while keeping the solution fair and observing courses' seat capacity limitations.
</details>
<details>
<summary>摘要</summary>
学生们决定选课的决定受到同学的决定影响很强。学生可能会偏好选择一门课程，即使它不是他们最喜欢的，只是为了与朋友一起学习。这已经证明了与朋友一起学习会提高学业表现。因此，学府应该在分配课程时考虑学生之间的友谊关系。在现有课程分配方法的基础上引入友谊关系会导致一些非常重要的变化。这篇论文探讨了如何考虑学生之间的友谊关系来分配课程，并提供了一种专门的算法。我们的评估包括仿真数据和177名学生对课程和朋友的偏好的用户研究数据。结果表明，我们的算法可以为学生提供高的用户价值，同时保证分配的解决方案公平、遵循课程坐席限制。
</details></li>
</ul>
<hr>
<h2 id="Single-and-Few-step-Diffusion-for-Generative-Speech-Enhancement"><a href="#Single-and-Few-step-Diffusion-for-Generative-Speech-Enhancement" class="headerlink" title="Single and Few-step Diffusion for Generative Speech Enhancement"></a>Single and Few-step Diffusion for Generative Speech Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09677">http://arxiv.org/abs/2309.09677</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bunlong Lay, Jean-Marie Lemercier, Julius Richter, Timo Gerkmann</li>
<li>for: 提高Diffusion模型的推理速度和精度</li>
<li>methods: 采用两个阶段训练方法，首先使用普通的生成推理方法进行训练，然后使用预测损失来对推理结果进行修正</li>
<li>results: 使用这种两个阶段训练方法可以在60次函数评估中达到同等性能，并且在减少函数评估数量（NFEs）下仍然保持稳定性和超越基eline模型的性能。<details>
<summary>Abstract</summary>
Diffusion models have shown promising results in speech enhancement, using a task-adapted diffusion process for the conditional generation of clean speech given a noisy mixture. However, at test time, the neural network used for score estimation is called multiple times to solve the iterative reverse process. This results in a slow inference process and causes discretization errors that accumulate over the sampling trajectory. In this paper, we address these limitations through a two-stage training approach. In the first stage, we train the diffusion model the usual way using the generative denoising score matching loss. In the second stage, we compute the enhanced signal by solving the reverse process and compare the resulting estimate to the clean speech target using a predictive loss. We show that using this second training stage enables achieving the same performance as the baseline model using only 5 function evaluations instead of 60 function evaluations. While the performance of usual generative diffusion algorithms drops dramatically when lowering the number of function evaluations (NFEs) to obtain single-step diffusion, we show that our proposed method keeps a steady performance and therefore largely outperforms the diffusion baseline in this setting and also generalizes better than its predictive counterpart.
</details>
<details>
<summary>摘要</summary>
Diffusion 模型在听音提升中表现出了promising的结果，使用任务适应的扩散过程来 conditional generation 清晰的听音，给定噪音混合。然而，在测试时，用于分数估计的神经网络会被多次调用，以解决反向过程的迭代问题。这会导致慢速的推理过程和积累的精度错误。在这篇论文中，我们解决这些限制，通过两个阶段的训练方法。在第一阶段，我们使用传统的扩散模型训练方法，使用生成扩散分数匹配损失函数。在第二阶段，我们解决反向过程，并将结果与清晰听音目标进行比较，使用预测损失函数。我们显示，使用这个第二阶段训练方法可以达到与基准模型相同的性能，只需要5个功能评估次数，而不是60个。而通常的生成扩散算法在降低功能评估次数（NFEs）时，性能会降低很多，但我们的提议方法可以保持稳定的性能，因此大大超越了扩散基准模型。此外，我们还发现，我们的方法在这种设定下更好地适应和泛化。
</details></li>
</ul>
<hr>
<h2 id="Conditioning-Latent-Space-Clusters-for-Real-World-Anomaly-Classification"><a href="#Conditioning-Latent-Space-Clusters-for-Real-World-Anomaly-Classification" class="headerlink" title="Conditioning Latent-Space Clusters for Real-World Anomaly Classification"></a>Conditioning Latent-Space Clusters for Real-World Anomaly Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09676">http://arxiv.org/abs/2309.09676</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Bogdoll, Svetlana Pavlitska, Simon Klaus, J. Marius Zöllner</li>
<li>for: 本研究旨在提高自动驾驶领域中异常检测的精度和效果。</li>
<li>methods: 本研究使用Variational Autoencoder（VAE）来分类样本为正常数据或异常数据，并通过提供异常映射来进一步提高异常检测性能。</li>
<li>results: 研究结果表明，通过使用异常映射，VAE可以更好地分类异常数据，并且可以分离正常数据和异常数据到隔离的集群中，从而获得有意义的幂等表示。<details>
<summary>Abstract</summary>
Anomalies in the domain of autonomous driving are a major hindrance to the large-scale deployment of autonomous vehicles. In this work, we focus on high-resolution camera data from urban scenes that include anomalies of various types and sizes. Based on a Variational Autoencoder, we condition its latent space to classify samples as either normal data or anomalies. In order to emphasize especially small anomalies, we perform experiments where we provide the VAE with a discrepancy map as an additional input, evaluating its impact on the detection performance. Our method separates normal data and anomalies into isolated clusters while still reconstructing high-quality images, leading to meaningful latent representations.
</details>
<details>
<summary>摘要</summary>
半自动驾驶领域中的异常现象是大规模部署自动驾驶车辆的主要障碍。在这项工作中，我们关注城市场景中高分辨率摄像头数据中的异常现象，其异常类型和大小各不相同。基于变量自适应编码器，我们将其缺失空间 condition 为分类样本为正常数据或异常样本。为了强调特别小的异常，我们在实验中提供了差异地图作为额外输入，评估其影响检测性能。我们的方法可以分解正常数据和异常数据，并且仍然重建高质量图像，从而获得有意义的隐藏表示。
</details></li>
</ul>
<hr>
<h2 id="Neural-Network-Based-Rule-Models-With-Truth-Tables"><a href="#Neural-Network-Based-Rule-Models-With-Truth-Tables" class="headerlink" title="Neural Network-Based Rule Models With Truth Tables"></a>Neural Network-Based Rule Models With Truth Tables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09638">http://arxiv.org/abs/2309.09638</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/molyswu/hand_detection">https://github.com/molyswu/hand_detection</a></li>
<li>paper_authors: Adrien Benamira, Tristan Guérand, Thomas Peyrin, Hans Soegeng<br>for:这个论文主要针对的是理解机器学习模型做出决策的过程，特别是在安全敏感应用中。methods:这个研究使用了神经网络框架，并将神经网络转换成规则基型模型。这个框架被称为Truth Table rules（TT-rules），它基于Truth Table nets（TTnets），一种由形式验证而来的神经网络家族。results:研究表明，TT-rules可以在七个tabular数据集上达到等效或更高的性能，而且保持和性能之间的平衡。此外，TT-rules还可以适用于大型tabular数据集，包括两个实际的DNA数据集，它们具有超过20K的特征。最后，研究者还进行了一个详细的rule-based模型的调查，使用Adult数据集。<details>
<summary>Abstract</summary>
Understanding the decision-making process of a machine/deep learning model is crucial, particularly in security-sensitive applications. In this study, we introduce a neural network framework that combines the global and exact interpretability properties of rule-based models with the high performance of deep neural networks.   Our proposed framework, called $\textit{Truth Table rules}$ (TT-rules), is built upon $\textit{Truth Table nets}$ (TTnets), a family of deep neural networks initially developed for formal verification. By extracting the set of necessary and sufficient rules $\mathcal{R}$ from the trained TTnet model (global interpretability), yielding the same output as the TTnet (exact interpretability), TT-rules effectively transforms the neural network into a rule-based model. This rule-based model supports binary classification, multi-label classification, and regression tasks for tabular datasets. Furthermore, our TT-rules framework optimizes the rule set $\mathcal{R}$ into $\mathcal{R}_{opt}$ by reducing the number and size of the rules. To enhance model interpretation, we leverage Reduced Ordered Binary Decision Diagrams (ROBDDs) to visualize these rules effectively.   After outlining the framework, we evaluate the performance of TT-rules on seven tabular datasets from finance, healthcare, and justice domains. We also compare the TT-rules framework to state-of-the-art rule-based methods. Our results demonstrate that TT-rules achieves equal or higher performance compared to other interpretable methods while maintaining a balance between performance and complexity. Notably, TT-rules presents the first accurate rule-based model capable of fitting large tabular datasets, including two real-life DNA datasets with over 20K features. Finally, we extensively investigate a rule-based model derived from TT-rules using the Adult dataset.
</details>
<details>
<summary>摘要</summary>
理解机器学习模型的决策过程是非常重要，尤其在安全敏感应用中。在这种研究中，我们提出了一种神经网络框架，可以结合神经网络的高性能和规则型模型的全面和准确解释性。我们称之为“真实表达规则”（TT-rules）。基于规则型网络（TTnets），TT-rules 可以从训练完成的 TTnet 模型中提取必要和 suficient 规则集（ $\mathcal{R}$），并且可以使这些规则集产生同样的输出，从而将神经网络转化成规则型模型。这种规则型模型支持二分类、多标签分类和回归任务。此外，我们还优化了规则集 $\mathcal{R}$ 为 $\mathcal{R}_{opt}$，以降低规则的数量和大小。为了增强模型解释，我们利用减少的binary decision diagram（ROBDDs）来可见地表示这些规则。在文章中，我们首先介绍了 TT-rules 框架，然后对七个标准化表格数据集进行评估。这些数据集来自于金融、医疗和正义领域。我们还与当前的解释性方法进行比较。我们的结果表明，TT-rules 可以与其他解释性方法相比，具有同等或更高的性能，同时保持性能和复杂度之间的平衡。尤其是，TT-rules 可以适用于大型表格数据集，包括两个实际的 DNA 数据集，它们具有超过 20K 的特征。最后，我们进行了一项详细的规则型模型研究，使用 Adult 数据集。
</details></li>
</ul>
<hr>
<h2 id="Designing-a-Hybrid-Neural-System-to-Learn-Real-world-Crack-Segmentation-from-Fractal-based-Simulation"><a href="#Designing-a-Hybrid-Neural-System-to-Learn-Real-world-Crack-Segmentation-from-Fractal-based-Simulation" class="headerlink" title="Designing a Hybrid Neural System to Learn Real-world Crack Segmentation from Fractal-based Simulation"></a>Designing a Hybrid Neural System to Learn Real-world Crack Segmentation from Fractal-based Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09637">http://arxiv.org/abs/2309.09637</a></li>
<li>repo_url: None</li>
<li>paper_authors: Achref Jaziri, Martin Mundt, Andres Fernandez Rodriguez, Visvanathan Ramesh</li>
<li>for: 这篇论文的目的是提高计算机视觉系统对混凝土结构完整性的评估，特别是Robust crack segmentation。</li>
<li>methods: 这篇论文使用了高准确的裂纹图形生成器和相应的完全注释的裂纹数据集，并利用了点wise Mutual Information estimate和适应实例normalization来学习通用的表示。</li>
<li>results: 论文通过实验显示，该系统可以有效地处理实际世界中的裂纹分割任务，并且不同的设计选择是相互协力的。<details>
<summary>Abstract</summary>
Identification of cracks is essential to assess the structural integrity of concrete infrastructure. However, robust crack segmentation remains a challenging task for computer vision systems due to the diverse appearance of concrete surfaces, variable lighting and weather conditions, and the overlapping of different defects. In particular recent data-driven methods struggle with the limited availability of data, the fine-grained and time-consuming nature of crack annotation, and face subsequent difficulty in generalizing to out-of-distribution samples. In this work, we move past these challenges in a two-fold way. We introduce a high-fidelity crack graphics simulator based on fractals and a corresponding fully-annotated crack dataset. We then complement the latter with a system that learns generalizable representations from simulation, by leveraging both a pointwise mutual information estimate along with adaptive instance normalization as inductive biases. Finally, we empirically highlight how different design choices are symbiotic in bridging the simulation to real gap, and ultimately demonstrate that our introduced system can effectively handle real-world crack segmentation.
</details>
<details>
<summary>摘要</summary>
检测裂隙是评估混凝土基础设施结构完整性的关键。然而，由于混凝土表面的多样性、变化的照明和天气条件以及不同损害的重叠，计算机视觉系统中的裂隙分割仍然是一项挑战。特别是，最新的数据驱动方法在数据的有限性、细化和时间消耗的 crack 注释以及扩展到不同样本上的difficulty in generalizing。在这项工作中，我们通过两种方式突破这些挑战。首先，我们介绍了一个基于 fractional 的高精度裂隙图形生成器，并且提供了相应的完全注释的裂隙集合。其次，我们利用了 both pointwise mutual information estimate和 adaptive instance normalization作为 inductive biases，以学习普适的表示。最后，我们证明了不同的设计选择是协同的，并最终表明了我们引入的系统可以有效地处理实际世界中的裂隙分割。
</details></li>
</ul>
<hr>
<h2 id="Gradpaint-Gradient-Guided-Inpainting-with-Diffusion-Models"><a href="#Gradpaint-Gradient-Guided-Inpainting-with-Diffusion-Models" class="headerlink" title="Gradpaint: Gradient-Guided Inpainting with Diffusion Models"></a>Gradpaint: Gradient-Guided Inpainting with Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09614">http://arxiv.org/abs/2309.09614</a></li>
<li>repo_url: None</li>
<li>paper_authors: Asya Grechka, Guillaume Couairon, Matthieu Cord</li>
<li>for: 图像填充任务中的图像生成（image inpainting）</li>
<li>methods: 使用 diffusion probabilistic models (DDPMs) 和自定义损失函数（custom loss）来导航生成过程，以实现图像生成的准确性和一致性。</li>
<li>results: 比起当前的方法， GradPaint 能够更好地考虑图像的一致性和自然性，提高了图像生成的质量。<details>
<summary>Abstract</summary>
Denoising Diffusion Probabilistic Models (DDPMs) have recently achieved remarkable results in conditional and unconditional image generation. The pre-trained models can be adapted without further training to different downstream tasks, by guiding their iterative denoising process at inference time to satisfy additional constraints. For the specific task of image inpainting, the current guiding mechanism relies on copying-and-pasting the known regions from the input image at each denoising step. However, diffusion models are strongly conditioned by the initial random noise, and therefore struggle to harmonize predictions inside the inpainting mask with the real parts of the input image, often producing results with unnatural artifacts.   Our method, dubbed GradPaint, steers the generation towards a globally coherent image. At each step in the denoising process, we leverage the model's "denoised image estimation" by calculating a custom loss measuring its coherence with the masked input image. Our guiding mechanism uses the gradient obtained from backpropagating this loss through the diffusion model itself. GradPaint generalizes well to diffusion models trained on various datasets, improving upon current state-of-the-art supervised and unsupervised methods.
</details>
<details>
<summary>摘要</summary>
德brázkyDiffusion噪声模型（DDPM）在最近已经实现了条件和无条件图像生成的出色result。预训练模型可以在推理时通过引导其迭代噪声过程来适应不同的下游任务，无需进一步训练。对于图像填充任务，当前的引导机制是通过在每个迭代步骤中复制输入图像中知道的区域。然而，噪声模型受初始噪声的强烈条件，因此在生成 Predictions 时难以融合输入图像的真实部分和填充mask中的预测，常导致生成结果具有不自然的 artifacts。我们的方法，称为GradPaint，使得生成向度向globally coherent image迁移。在噪声过程中每个步骤中，我们利用 diffusion 模型的 "denoised image estimation"，计算一个定制的损失函数，用于衡量其与masked输入图像的几何匹配度。我们的引导机制使用diffusion模型自身的倒推操作来获得的梯度。GradPaint在不同的训练数据集上训练的 diffusion 模型上generalizes well，超过当前领先的指导和无指导方法。
</details></li>
</ul>
<hr>
<h2 id="Proposition-from-the-Perspective-of-Chinese-Language-A-Chinese-Proposition-Classification-Evaluation-Benchmark"><a href="#Proposition-from-the-Perspective-of-Chinese-Language-A-Chinese-Proposition-Classification-Evaluation-Benchmark" class="headerlink" title="Proposition from the Perspective of Chinese Language: A Chinese Proposition Classification Evaluation Benchmark"></a>Proposition from the Perspective of Chinese Language: A Chinese Proposition Classification Evaluation Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09602">http://arxiv.org/abs/2309.09602</a></li>
<li>repo_url: None</li>
<li>paper_authors: Conghui Niu, Mengyang Hu, Lin Bo, Xiaoli He, Dong Yu, Pengyuan Liu</li>
<li>for: 这篇论文主要研究了中文提案的分类和识别，以及提案的语言表达特征和逻辑意义。</li>
<li>methods: 该论文提出了明确和暗示提案的概念，并提出了一种多级分类系统，使用语言学和逻辑学方法进行提案分类。</li>
<li>results: 经过多种方法的评估，包括Rule-based方法、SVM、BERT、RoBERTA和ChatGPT等，研究发现现有模型对中文提案分类能力不足，尤其是跨领域传递性不佳。BERT表现较好，但缺乏跨领域传递性。ChatGPT表现不佳，但可以通过提供更多提案信息进行改进。<details>
<summary>Abstract</summary>
Existing propositions often rely on logical constants for classification. Compared with Western languages that lean towards hypotaxis such as English, Chinese often relies on semantic or logical understanding rather than logical connectives in daily expressions, exhibiting the characteristics of parataxis. However, existing research has rarely paid attention to this issue. And accurately classifying these propositions is crucial for natural language understanding and reasoning. In this paper, we put forward the concepts of explicit and implicit propositions and propose a comprehensive multi-level proposition classification system based on linguistics and logic. Correspondingly, we create a large-scale Chinese proposition dataset PEACE from multiple domains, covering all categories related to propositions. To evaluate the Chinese proposition classification ability of existing models and explore their limitations, We conduct evaluations on PEACE using several different methods including the Rule-based method, SVM, BERT, RoBERTA, and ChatGPT. Results show the importance of properly modeling the semantic features of propositions. BERT has relatively good proposition classification capability, but lacks cross-domain transferability. ChatGPT performs poorly, but its classification ability can be improved by providing more proposition information. Many issues are still far from being resolved and require further study.
</details>
<details>
<summary>摘要</summary>
现有的提案经常利用逻辑常量进行分类。相比西方语言，如英语，中文更加倾向于 semantic或逻辑理解而非逻辑连接在日常表达中，展现出复杂的parataxis特点。然而，现有的研究几乎没有关注这一点。正确地分类这些提案是自然语言理解和逻辑的关键。在这篇论文中，我们提出了explicit和implicit提案的概念，并提出了基于语言和逻辑的多级提案分类系统。与此同时，我们创建了来自多个领域的大规模中文提案数据集PEACE，覆盖所有与提案相关的类别。为了评估现有模型对中文提案分类的能力和其局限性，我们使用了多种方法，包括规则基本方法、SVM、BERT、RoBERTA和ChatGPT。结果表明，正确地表示提案的 semantic特征非常重要。BERT在提案分类能力方面表现较好，但缺乏跨领域传递性。ChatGPT表现不佳，但可以通过提供更多提案信息来提高其分类能力。许多问题仍然待解决，需要进一步研究。
</details></li>
</ul>
<hr>
<h2 id="Fabricator-An-Open-Source-Toolkit-for-Generating-Labeled-Training-Data-with-Teacher-LLMs"><a href="#Fabricator-An-Open-Source-Toolkit-for-Generating-Labeled-Training-Data-with-Teacher-LLMs" class="headerlink" title="Fabricator: An Open Source Toolkit for Generating Labeled Training Data with Teacher LLMs"></a>Fabricator: An Open Source Toolkit for Generating Labeled Training Data with Teacher LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09582">http://arxiv.org/abs/2309.09582</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonas Golde, Patrick Haller, Felix Hamborg, Julian Risch, Alan Akbik</li>
<li>for: 这个论文的目的是提出一种基于大语言模型（LLM）的数据生成方法，以解决NLプロセス中的数据预处理瓶颈。</li>
<li>methods: 这个论文使用的方法是通过向LLM提供任务描述，然后使用生成的数据来训练下游NLP模型。</li>
<li>results: 这个论文的结果表明，通过使用LLM进行数据生成，可以生成大量高质量的标注数据，从而降低NL模型的训练成本。同时，这个方法还可以支持多种下游NLP任务，如文本分类、问答和实体识别等。<details>
<summary>Abstract</summary>
Most NLP tasks are modeled as supervised learning and thus require labeled training data to train effective models. However, manually producing such data at sufficient quality and quantity is known to be costly and time-intensive. Current research addresses this bottleneck by exploring a novel paradigm called zero-shot learning via dataset generation. Here, a powerful LLM is prompted with a task description to generate labeled data that can be used to train a downstream NLP model. For instance, an LLM might be prompted to "generate 500 movie reviews with positive overall sentiment, and another 500 with negative sentiment." The generated data could then be used to train a binary sentiment classifier, effectively leveraging an LLM as a teacher to a smaller student model. With this demo, we introduce Fabricator, an open-source Python toolkit for dataset generation. Fabricator implements common dataset generation workflows, supports a wide range of downstream NLP tasks (such as text classification, question answering, and entity recognition), and is integrated with well-known libraries to facilitate quick experimentation. With Fabricator, we aim to support researchers in conducting reproducible dataset generation experiments using LLMs and help practitioners apply this approach to train models for downstream tasks.
</details>
<details>
<summary>摘要</summary>
我们引入了一个名为 Fabricator 的开源 Python 工具库，用于生成数据集。Fabricator 支持许多下游 NLP 任务（如文本分类、问题答案和实体识别），并与各种知名库集成，以便快速实验。我们希望通过 Fabricator 支持研究人员在使用 LLM 进行可重现的数据集生成实验，并帮助实践者使用这种方法训练下游任务中的模型。
</details></li>
</ul>
<hr>
<h2 id="Heterogeneous-Generative-Knowledge-Distillation-with-Masked-Image-Modeling"><a href="#Heterogeneous-Generative-Knowledge-Distillation-with-Masked-Image-Modeling" class="headerlink" title="Heterogeneous Generative Knowledge Distillation with Masked Image Modeling"></a>Heterogeneous Generative Knowledge Distillation with Masked Image Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09571">http://arxiv.org/abs/2309.09571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziming Wang, Shumin Han, Xiaodi Wang, Jing Hao, Xianbin Cao, Baochang Zhang</li>
<li>for: 这篇论文的目的是提出一种基于Masked Image Modeling（MIM）的异化深度学习知识传递（H-GKD）方法，实现将大型Transformer模型的知识转移到小型CNN模型中，以提高这些小型模型在 computationally resource-limited edge devices 上的表现。</li>
<li>methods: 这篇论文使用了一种基于UNet的学生网络，通过将 sparse convolution 加入学生网络中，使学生网络能够对教师模型的Visual Representation进行效果伪装。此外，这篇论文还使用了Masked Image Modeling（MIM）方法，将教师模型的Visual Representation转移到学生网络中，以实现知识传递。</li>
<li>results: 这篇论文的实验结果显示，H-GKD 方法可以对不同的模型和大小进行适应，在 ImageNet 1K  dataset 上，H-GKD 方法可以从 Resnet50 (sparse) 的76.98% 提高到80.01%。<details>
<summary>Abstract</summary>
Small CNN-based models usually require transferring knowledge from a large model before they are deployed in computationally resource-limited edge devices. Masked image modeling (MIM) methods achieve great success in various visual tasks but remain largely unexplored in knowledge distillation for heterogeneous deep models. The reason is mainly due to the significant discrepancy between the Transformer-based large model and the CNN-based small network. In this paper, we develop the first Heterogeneous Generative Knowledge Distillation (H-GKD) based on MIM, which can efficiently transfer knowledge from large Transformer models to small CNN-based models in a generative self-supervised fashion. Our method builds a bridge between Transformer-based models and CNNs by training a UNet-style student with sparse convolution, which can effectively mimic the visual representation inferred by a teacher over masked modeling. Our method is a simple yet effective learning paradigm to learn the visual representation and distribution of data from heterogeneous teacher models, which can be pre-trained using advanced generative methods. Extensive experiments show that it adapts well to various models and sizes, consistently achieving state-of-the-art performance in image classification, object detection, and semantic segmentation tasks. For example, in the Imagenet 1K dataset, H-GKD improves the accuracy of Resnet50 (sparse) from 76.98% to 80.01%.
</details>
<details>
<summary>摘要</summary>
通常，小型CNN模型需要从大型模型中传输知识才能在计算资源有限的边缘设备中部署。Masked image modeling（MIM）方法在各种视觉任务中取得了很大成功，但在不同深度模型之间的知识传递方面尚未得到充分开发。这主要是因为大型Transformer模型和小型CNN模型之间存在很大的不同。在这篇论文中，我们开发了首个基于MIM的Heterogeneous Generative Knowledge Distillation（H-GKD），可以有效地将大型Transformer模型中的知识传递给小型CNN模型。我们的方法建立了Transformer模型和CNN之间的桥梁，通过训练一个带有散集 convolution的学生模型，可以有效地模仿教师模型在遮盲模型中的视觉表示。我们的方法是一种简单 yet有效的学习方法，可以从不同的教师模型中学习数据的视觉表示和分布。我们的实验表明，我们的方法可以适应不同的模型和大小，并在图像分类、物体检测和semantic segmentation任务中准确地达到领先性性表现。例如，在Imagenet 1K dataset中，H-GKD提高了Resnet50（散集）的准确率从76.98%提升到80.01%。
</details></li>
</ul>
<hr>
<h2 id="Causal-Story-Local-Causal-Attention-Utilizing-Parameter-Efficient-Tuning-For-Visual-Story-Synthesis"><a href="#Causal-Story-Local-Causal-Attention-Utilizing-Parameter-Efficient-Tuning-For-Visual-Story-Synthesis" class="headerlink" title="Causal-Story: Local Causal Attention Utilizing Parameter-Efficient Tuning For Visual Story Synthesis"></a>Causal-Story: Local Causal Attention Utilizing Parameter-Efficient Tuning For Visual Story Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09553">http://arxiv.org/abs/2309.09553</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyi Song, Jiuxin Cao, Kun Wang, Bo Liu, Xiaofeng Zhang</li>
<li>for: 提高视觉故事生成的全局一致性</li>
<li>methods: 使用本地 causal 注意机制，考虑过去的caption、frame和当前caption之间的 causal 关系，对当前帧生成</li>
<li>results: 在 PororoSV 和 FlintstonesSV 数据集上获得了state-of-the-art FID 分数，生成的帧也更加出色地表现了视觉故事的整体一致性。<details>
<summary>Abstract</summary>
The excellent text-to-image synthesis capability of diffusion models has driven progress in synthesizing coherent visual stories. The current state-of-the-art method combines the features of historical captions, historical frames, and the current captions as conditions for generating the current frame. However, this method treats each historical frame and caption as the same contribution. It connects them in order with equal weights, ignoring that not all historical conditions are associated with the generation of the current frame. To address this issue, we propose Causal-Story. This model incorporates a local causal attention mechanism that considers the causal relationship between previous captions, frames, and current captions. By assigning weights based on this relationship, Causal-Story generates the current frame, thereby improving the global consistency of story generation. We evaluated our model on the PororoSV and FlintstonesSV datasets and obtained state-of-the-art FID scores, and the generated frames also demonstrate better storytelling in visuals. The source code of Causal-Story can be obtained from https://github.com/styufo/Causal-Story.
</details>
<details>
<summary>摘要</summary>
“ diffusion 模型的出色文本到图像合成能力，已经驱动了视觉故事的生成进步。当前state-of-the-art方法是将历史caption、历史框和当前caption作为生成当前帧的condition。但是，这种方法对每个历史框和caption都使用相同的权重，忽略了不同历史条件对当前帧生成的不同影响。为解决这个问题，我们提出了Causal-Story模型。这个模型包含了本地 causal 注意力机制，考虑了以前caption、frame和当前caption之间的 causal 关系。通过基于这种关系的权重分配，Causal-Story模型生成当前帧，从而提高了全局的故事生成一致性。我们在 PororoSV 和 FlintstonesSV 数据集上评估了我们的模型，并取得了state-of-the-art FID 分数，生成的图像也更好地表达了故事的视觉。Causal-Story 模型的源代码可以从 GitHub 上获取：https://github.com/styufo/Causal-Story。”
</details></li>
</ul>
<hr>
<h2 id="CB-Whisper-Contextual-Biasing-Whisper-using-TTS-based-Keyword-Spotting"><a href="#CB-Whisper-Contextual-Biasing-Whisper-using-TTS-based-Keyword-Spotting" class="headerlink" title="CB-Whisper: Contextual Biasing Whisper using TTS-based Keyword Spotting"></a>CB-Whisper: Contextual Biasing Whisper using TTS-based Keyword Spotting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09552">http://arxiv.org/abs/2309.09552</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuang Li, Yinglu Li, Min Zhang, Chang Su, Mengyao Piao, Xiaosong Qiao, Jiawei Yu, Miaomiao Ma, Yanqing Zhao, Hao Yang</li>
<li>for: 提高自动语音识别（ASR）系统对罕见名词的识别率，如人名、组织名称和技术术语等。</li>
<li>methods: 使用OpenAI的Whisper模型，首先通过关键词检测（KWS）模块匹配实体和语音示例的特征。</li>
<li>results: 在三个内部数据集和两个开源数据集上，包括英语、中文和code-switching场景，通过采用经过设计的口头形式提示，使Whisper模型的混合错误率（MER）和实体恢复率得到显著改进。<details>
<summary>Abstract</summary>
End-to-end automatic speech recognition (ASR) systems often struggle to recognize rare name entities, such as personal names, organizations, or technical terms that are not frequently encountered in the training data. This paper presents Contextual Biasing Whisper (CB-Whisper), a novel ASR system based on OpenAI's Whisper model that performs keyword-spotting (KWS) before the decoder. The KWS module leverages text-to-speech (TTS) techniques and a convolutional neural network (CNN) classifier to match the features between the entities and the utterances. Experiments demonstrate that by incorporating predicted entities into a carefully designed spoken form prompt, the mixed-error-rate (MER) and entity recall of the Whisper model is significantly improved on three internal datasets and two open-sourced datasets that cover English-only, Chinese-only, and code-switching scenarios.
</details>
<details>
<summary>摘要</summary>
通常的自动语音识别（ASR）系统经常遇到不寻常的名 Entity 识别问题，如人名、组织机构或技术术语，这些名 Entity 在训练数据中不充分出现。这篇论文提出了 Contextual Biasing Whisper（CB-Whisper），一种基于 OpenAI 的 Whisper 模型的 ASR 系统，其在 decoder 前使用 Keyword-spotting（KWS）模块。KWS 模块利用文本识别（TTS）技术和卷积神经网络（CNN）分类器将 Entity 与语音之间的特征进行匹配。实验表明，通过在精心设计的 spoken form 提示中包含预测的 Entity，可以显著提高 Whisper 模型的混合错误率（MER）和 Entity 回归率，在三个内部数据集和两个开源数据集中，这些数据集覆盖了英语、中文和代码混合enario。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Reorganization-of-Neural-Pathways-for-Continual-Learning-with-Hybrid-Spiking-Neural-Networks"><a href="#Adaptive-Reorganization-of-Neural-Pathways-for-Continual-Learning-with-Hybrid-Spiking-Neural-Networks" class="headerlink" title="Adaptive Reorganization of Neural Pathways for Continual Learning with Hybrid Spiking Neural Networks"></a>Adaptive Reorganization of Neural Pathways for Continual Learning with Hybrid Spiking Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09550">http://arxiv.org/abs/2309.09550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bing Han, Feifei Zhao, Wenxuan Pan, Zhaoya Zhao, Xianqi Li, Qingqun Kong, Yi Zeng</li>
<li>for: 本研究旨在开发一种基于大脑自组织的连续学习算法，以便让人工神经网络能够有效地适应增加的任务，同时保持性能和能耗水平。</li>
<li>methods: 该算法使用自组织调节网络（SOR-SNN），通过重组单个和有限的脉冲神经网络（SNN），生成丰富的稀热神经路径，以高效地处理增量任务。</li>
<li>results: 实验结果表明，该算法在多种连续学习任务上表现了稳定的优异性、能耗和存储能力，并能够有效地汇集过去学习的知识和当前任务的信息，实现了回传能力。此外，该模型还具有自修复能力，可以自动分配新的路径来恢复忘记的知识。<details>
<summary>Abstract</summary>
The human brain can self-organize rich and diverse sparse neural pathways to incrementally master hundreds of cognitive tasks. However, most existing continual learning algorithms for deep artificial and spiking neural networks are unable to adequately auto-regulate the limited resources in the network, which leads to performance drop along with energy consumption rise as the increase of tasks. In this paper, we propose a brain-inspired continual learning algorithm with adaptive reorganization of neural pathways, which employs Self-Organizing Regulation networks to reorganize the single and limited Spiking Neural Network (SOR-SNN) into rich sparse neural pathways to efficiently cope with incremental tasks. The proposed model demonstrates consistent superiority in performance, energy consumption, and memory capacity on diverse continual learning tasks ranging from child-like simple to complex tasks, as well as on generalized CIFAR100 and ImageNet datasets. In particular, the SOR-SNN model excels at learning more complex tasks as well as more tasks, and is able to integrate the past learned knowledge with the information from the current task, showing the backward transfer ability to facilitate the old tasks. Meanwhile, the proposed model exhibits self-repairing ability to irreversible damage and for pruned networks, could automatically allocate new pathway from the retained network to recover memory for forgotten knowledge.
</details>
<details>
<summary>摘要</summary>
人类大脑可以自组织富有多样性和稀疏的神经路径，以逐渐掌握百种认知任务。然而，现有的深度人工神经网络和脉冲神经网络的持续学习算法无法充分自适应网络的限制资源，导致任务增加后性能下降和能耗增加。在这篇论文中，我们提出了基于大脑自适应学习算法的快速适应神经网络，使用自组织调节网络（SOR-SNN）重组单个和有限的脉冲神经网络，以高效地处理增量任务。我们的模型在多种不同的持续学习任务上表现了一致的优异性、能耗和存储能力，包括从婴儿式简单到复杂任务，以及通用CIFAR100和ImageNet数据集。尤其是SOR-SNN模型在学习更复杂的任务和更多任务方面表现出色，并能够将过去学习的知识与当前任务的信息相结合，表现出返回传递能力以便恢复过去学习的知识。同时，我们的模型也展现了自动修复损害和剪除网络的自适应能力。
</details></li>
</ul>
<hr>
<h2 id="A-performance-characteristic-curve-for-model-evaluation-the-application-in-information-diffusion-prediction"><a href="#A-performance-characteristic-curve-for-model-evaluation-the-application-in-information-diffusion-prediction" class="headerlink" title="A performance characteristic curve for model evaluation: the application in information diffusion prediction"></a>A performance characteristic curve for model evaluation: the application in information diffusion prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09537">http://arxiv.org/abs/2309.09537</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjin Xie, Xiaomeng Wang, Radosław Michalski, Tao Jia<br>for: 这种研究旨在预测社交媒体上的信息传播受众，有实际应用在市场营销和社交媒体领域。methods: 该研究使用了一种基于信息熵的指标来衡量推测模型的准确性，并发现了一种与随机性和预测准确性之间存在的整体关系。results: 研究发现，对不同的序列长度、系统大小和随机性，推测模型的性能特征曲线呈现出一种普适的趋势，这种曲线可以用来评估推测模型的性能。这种方法可以系统地评估不同推测模型的性能，并为未来的研究提供新的评估方法。<details>
<summary>Abstract</summary>
The information diffusion prediction on social networks aims to predict future recipients of a message, with practical applications in marketing and social media. While different prediction models all claim to perform well, general frameworks for performance evaluation remain limited. Here, we aim to identify a performance characteristic curve for a model, which captures its performance on tasks of different complexity. We propose a metric based on information entropy to quantify the randomness in diffusion data, then identify a scaling pattern between the randomness and the prediction accuracy of the model. Data points in the patterns by different sequence lengths, system sizes, and randomness all collapse into a single curve, capturing a model's inherent capability of making correct predictions against increased uncertainty. Given that this curve has such important properties that it can be used to evaluate the model, we define it as the performance characteristic curve of the model. The validity of the curve is tested by three prediction models in the same family, reaching conclusions in line with existing studies. Also, the curve is successfully applied to evaluate two distinct models from the literature. Our work reveals a pattern underlying the data randomness and prediction accuracy. The performance characteristic curve provides a new way to systematically evaluate models' performance, and sheds light on future studies on other frameworks for model evaluation.
</details>
<details>
<summary>摘要</summary>
社交媒体上的信息扩散预测目标是预测未来的消息接收者，有实际应用于市场营销和社交媒体。虽然不同的预测模型都宣称表现良好，但总体框架 для性能评估仍然有限。我们想要找到一个表现曲线，可以捕捉模型在不同复杂度任务上的表现。我们提出一种基于信息熵的度量，用于量化扩散数据中的随机性，然后确定模型预测正确率和随机性之间的扩散规律。数据点在不同的序列长度、系统大小和随机性下都可以归一化到单一的曲线上，捕捉模型在面临不确定性增加时的内在能力。由于这个曲线具有这些重要的性能特点，我们定义它为模型性能特征曲线。我们的实验证明了这个曲线的有效性，并应用到了Literature中的两种不同模型中。我们的研究发现了随机性和预测精度之间的关系，并提供了一种新的评估模型性能的方法。这种方法可以系统地评估模型的性能，并为未来的研究提供了新的思路。
</details></li>
</ul>
<hr>
<h2 id="DFIL-Deepfake-Incremental-Learning-by-Exploiting-Domain-invariant-Forgery-Clues"><a href="#DFIL-Deepfake-Incremental-Learning-by-Exploiting-Domain-invariant-Forgery-Clues" class="headerlink" title="DFIL: Deepfake Incremental Learning by Exploiting Domain-invariant Forgery Clues"></a>DFIL: Deepfake Incremental Learning by Exploiting Domain-invariant Forgery Clues</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09526">http://arxiv.org/abs/2309.09526</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deepfakeil/dfil">https://github.com/deepfakeil/dfil</a></li>
<li>paper_authors: Kun Pan, Yin Yifang, Yao Wei, Feng Lin, Zhongjie Ba, Zhenguang Liu, ZhiBo Wang, Lorenzo Cavallaro, Kui Ren</li>
<li>for: 防止深伪造影像的普遍传播和攻击，提高伪造影像检测模型的准确性和适应能力。</li>
<li>methods: 提出一个增量学习框架，通过不断学习少量新数据来改善伪造影像检测模型的普遍性和适应能力。另外，提出了一个领域不断学习的方法，以获得不同数据分布下的领域共同表示。以及一种多角度知识传授法，以避免严重遗忘现象。</li>
<li>results: 在四个benchmark数据集上进行了广泛的实验，取得了新的state-of-the-art的平均遗忘率7.01和平均准确率85.49。<details>
<summary>Abstract</summary>
The malicious use and widespread dissemination of deepfake pose a significant crisis of trust. Current deepfake detection models can generally recognize forgery images by training on a large dataset. However, the accuracy of detection models degrades significantly on images generated by new deepfake methods due to the difference in data distribution. To tackle this issue, we present a novel incremental learning framework that improves the generalization of deepfake detection models by continual learning from a small number of new samples. To cope with different data distributions, we propose to learn a domain-invariant representation based on supervised contrastive learning, preventing overfit to the insufficient new data. To mitigate catastrophic forgetting, we regularize our model in both feature-level and label-level based on a multi-perspective knowledge distillation approach. Finally, we propose to select both central and hard representative samples to update the replay set, which is beneficial for both domain-invariant representation learning and rehearsal-based knowledge preserving. We conduct extensive experiments on four benchmark datasets, obtaining the new state-of-the-art average forgetting rate of 7.01 and average accuracy of 85.49 on FF++, DFDC-P, DFD, and CDF2. Our code is released at https://github.com/DeepFakeIL/DFIL.
</details>
<details>
<summary>摘要</summary>
黑客使用和广泛传播深伪造成了信任危机。现有的深伪检测模型通常可以通过大量数据训练Recognize forgery images，但检测模型对新的深伪生成方法生成的图像的准确率会降低significantly due to differences in data distribution. To address this issue, we propose a novel incremental learning framework that improves the generalization of deepfake detection models by continual learning from a small number of new samples. To cope with different data distributions, we propose to learn a domain-invariant representation based on supervised contrastive learning, preventing overfit to the insufficient new data. To mitigate catastrophic forgetting, we regularize our model in both feature-level and label-level based on a multi-perspective knowledge distillation approach. Finally, we propose to select both central and hard representative samples to update the replay set, which is beneficial for both domain-invariant representation learning and rehearsal-based knowledge preserving. We conduct extensive experiments on four benchmark datasets, achieving a new state-of-the-art average forgetting rate of 7.01 and average accuracy of 85.49 on FF++, DFDC-P, DFD, and CDF2. Our code is released at <https://github.com/DeepFakeIL/DFIL>.
</details></li>
</ul>
<hr>
<h2 id="FedGKD-Unleashing-the-Power-of-Collaboration-in-Federated-Graph-Neural-Networks"><a href="#FedGKD-Unleashing-the-Power-of-Collaboration-in-Federated-Graph-Neural-Networks" class="headerlink" title="FedGKD: Unleashing the Power of Collaboration in Federated Graph Neural Networks"></a>FedGKD: Unleashing the Power of Collaboration in Federated Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09517">http://arxiv.org/abs/2309.09517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiying Pan, Ruofan Wu, Tengfei Liu, Tianyi Zhang, Yifei Zhu, Weiqiang Wang</li>
<li>for: 这篇论文旨在提出一个基于 Federated Training 的 Graph Neural Network (GNN) 框架，并解决在 Federated GNN 系统中的图素异常问题。</li>
<li>methods: 本文使用了一个新的客边端图数据精炼方法，将本地任务描述用更好地描述任务相关性，并引入了一个新的服务器端集成机制，让它更好地利用全球协作结构。</li>
<li>results: 本文的实验结果显示， FedGKD 框架在六个真实世界数据集上表现出优于其他方法，特别是在大规模数据集上。<details>
<summary>Abstract</summary>
Federated training of Graph Neural Networks (GNN) has become popular in recent years due to its ability to perform graph-related tasks under data isolation scenarios while preserving data privacy. However, graph heterogeneity issues in federated GNN systems continue to pose challenges. Existing frameworks address the problem by representing local tasks using different statistics and relating them through a simple aggregation mechanism. However, these approaches suffer from limited efficiency from two aspects: low quality of task-relatedness quantification and inefficacy of exploiting the collaboration structure. To address these issues, we propose FedGKD, a novel federated GNN framework that utilizes a novel client-side graph dataset distillation method to extract task features that better describe task-relatedness, and introduces a novel server-side aggregation mechanism that is aware of the global collaboration structure. We conduct extensive experiments on six real-world datasets of different scales, demonstrating our framework's outperformance.
</details>
<details>
<summary>摘要</summary>
随着 Federated 培训（Federated Training）的兴起，Graph Neural Networks（GNN）在数据隔离场景下进行图像相关任务的能力得到了广泛的关注。然而， Federated GNN 系统中的图像异ogeneity问题仍然具有挑战性。现有的框架通过使用不同的统计来表示本地任务和将其相关联的简单汇总机制来解决这个问题。然而，这些方法受到两个方面的限制：一是任务相关性评估质量低，二是不能充分利用全局协作结构。为了解决这些问题，我们提出了 FedGKD，一种基于客户端 Graph 数据集缩减方法的新的 Federated GNN 框架，并引入了服务器端具有全局协作结构的新汇总机制。我们在六个真实世界数据集上进行了广泛的实验，并证明了我们的框架的表现优于。
</details></li>
</ul>
<hr>
<h2 id="Pruning-Large-Language-Models-via-Accuracy-Predictor"><a href="#Pruning-Large-Language-Models-via-Accuracy-Predictor" class="headerlink" title="Pruning Large Language Models via Accuracy Predictor"></a>Pruning Large Language Models via Accuracy Predictor</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09507">http://arxiv.org/abs/2309.09507</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yupeng Ji, Yibo Cao, Jiucai Liu</li>
<li>for: 这篇论文旨在提出一种新的模型压缩方法，以便对大型语言模型（LLMs）进行压缩，以提高训练、测试和部署的效率。</li>
<li>methods: 这篇论文提出了一种新的模型压缩方法，包括首先建立一个训练集，其中包含一定数量的架构精度对。然后，使用这个精度预测器进行进一步优化搜索空间，以找到最佳的模型。</li>
<li>results: 实验结果显示，提案的方法具有高效性和高精度。相比基准，在Wikitext2和PTB上的PPL下降9.48%和5.76%，MMLU的平均精度提高6.28%.<details>
<summary>Abstract</summary>
Large language models(LLMs) containing tens of billions of parameters (or even more) have demonstrated impressive capabilities in various NLP tasks. However, substantial model size poses challenges to training, inference, and deployment so that it is necessary to compress the model. At present, most model compression for LLMs requires manual design of pruning features, which has problems such as complex optimization pipeline and difficulty in retaining the capabilities of certain parts of the model.Therefore, we propose a novel pruning approach: firstly, a training set of a certain number of architecture-accuracy pairs is established, and then a non-neural model is trained as an accuracy predictor. Using the accuracy predictor to further optimize the search space and search, the optimal model can be automatically selected. Experiments show that our proposed approach is effective and efficient. Compared with the baseline, the perplexity(PPL) on Wikitext2 and PTB dropped by 9.48% and 5,76% respectively, and the average accuracy of MMLU increased by 6.28%.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）包含十以上亿个参数（甚至更多）的表现很出色在不同的自然语言处理任务中。然而，大型模型的训练、推理和部署带来了很多挑战，因此需要压缩模型。目前，大多数模型压缩方法 для LLM 需要手动设计特征剪除，这种方法存在复杂的优化管道和维护特定部分模型的困难。因此，我们提出了一种新的剪除方法：首先，建立一个固定数量的体系精度对的训练集，然后使用一个非神经网络模型来预测精度。使用预测器进一步优化搜索空间，并通过搜索，选择最佳模型。实验表明，我们提出的方法有效和高效。相比基准， Wikitext2 和 PTB 的 PPL 下降了9.48%和5.76%，MMLU 的平均精度提高了6.28%。
</details></li>
</ul>
<hr>
<h2 id="PromptST-Prompt-Enhanced-Spatio-Temporal-Multi-Attribute-Prediction"><a href="#PromptST-Prompt-Enhanced-Spatio-Temporal-Multi-Attribute-Prediction" class="headerlink" title="PromptST: Prompt-Enhanced Spatio-Temporal Multi-Attribute Prediction"></a>PromptST: Prompt-Enhanced Spatio-Temporal Multi-Attribute Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09500">http://arxiv.org/abs/2309.09500</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zijian Zhang, Xiangyu Zhao, Qidong Liu, Chunxu Zhang, Qian Ma, Wanyu Wang, Hongwei Zhao, Yiqi Wang, Zitao Liu<br>for:This paper focuses on the problem of spatio-temporal multi-attribute prediction, which is a critical part of urban management. The authors aim to address the challenge of handling diverse spatio-temporal attributes simultaneously and improve the prediction performance.methods:The proposed method, called PromptST, consists of a spatio-temporal transformer and a parameter-sharing training scheme. The authors also introduce a spatio-temporal prompt tuning strategy to fit specific attributes in a lightweight manner.results:Extensive experiments on real-world datasets show that PromptST achieves state-of-the-art performance in spatio-temporal multi-attribute prediction. Additionally, the authors prove that PromptST has good transferability on unseen spatio-temporal attributes, which has promising application potential in urban computing.<details>
<summary>Abstract</summary>
In the era of information explosion, spatio-temporal data mining serves as a critical part of urban management. Considering the various fields demanding attention, e.g., traffic state, human activity, and social event, predicting multiple spatio-temporal attributes simultaneously can alleviate regulatory pressure and foster smart city construction. However, current research can not handle the spatio-temporal multi-attribute prediction well due to the complex relationships between diverse attributes. The key challenge lies in how to address the common spatio-temporal patterns while tackling their distinctions. In this paper, we propose an effective solution for spatio-temporal multi-attribute prediction, PromptST. We devise a spatio-temporal transformer and a parameter-sharing training scheme to address the common knowledge among different spatio-temporal attributes. Then, we elaborate a spatio-temporal prompt tuning strategy to fit the specific attributes in a lightweight manner. Through the pretrain and prompt tuning phases, our PromptST is able to enhance the specific spatio-temoral characteristic capture by prompting the backbone model to fit the specific target attribute while maintaining the learned common knowledge. Extensive experiments on real-world datasets verify that our PromptST attains state-of-the-art performance. Furthermore, we also prove PromptST owns good transferability on unseen spatio-temporal attributes, which brings promising application potential in urban computing. The implementation code is available to ease reproducibility.
</details>
<details>
<summary>摘要</summary>
在信息爆发时代，空间时间数据挖掘作为城市管理的重要组成部分。考虑到不同领域的需求，如交通状况、人员活动和社会事件，同时预测多个空间时间属性可以减轻管理压力和推动智能城市建设。然而，当前的研究不能好地处理多个空间时间属性的预测，因为这些属性之间存在复杂的关系。主要挑战在于如何处理共同的空间时间模式，同时考虑各属性的特点。在本文中，我们提出一种高效的空间时间多属性预测解决方案——PromptST。我们设计了一种空间时间变换器和共享参数训练方案，以处理不同空间时间属性之间的共同知识。然后，我们提出了一种空间时间Prompt调整策略，以适应特定属性的特点。通过预训练和Prompt调整两个阶段，我们的PromptST能够提高特定空间时间特征的捕捉，同时维护学习的共同知识。广泛的实验表明，我们的PromptST可以达到状态机器的表现。此外，我们还证明PromptST具有良好的传输性，可以在未看到的空间时间属性上进行应用，这提供了智能城市建设的广阔应用前景。代码实现可以方便复制。
</details></li>
</ul>
<hr>
<h2 id="CLIP-based-Synergistic-Knowledge-Transfer-for-Text-based-Person-Retrieval"><a href="#CLIP-based-Synergistic-Knowledge-Transfer-for-Text-based-Person-Retrieval" class="headerlink" title="CLIP-based Synergistic Knowledge Transfer for Text-based Person Retrieval"></a>CLIP-based Synergistic Knowledge Transfer for Text-based Person Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09496">http://arxiv.org/abs/2309.09496</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yating liu, Yaowei Li, Zimo Liu, Wenming Yang, Yaowei Wang, Qingmin Liao</li>
<li>for: 文本基准人识别（TBPR）</li>
<li>methods: 使用 CLIP 基于的 Synergistic Knowledge Transfer（CSKT）方法，包括文本到图像和图像到文本的bidirectional prompts 和 coupling projections，以及图像和语言多头自注意力的双向转移知识机制。</li>
<li>results: CSKT 方法在三个标准测试集上比前一个最佳方法表现出色，只需要训练参数占模型总参数的 7.4%，显示其高效、有效和普遍。<details>
<summary>Abstract</summary>
Text-based Person Retrieval aims to retrieve the target person images given a textual query. The primary challenge lies in bridging the substantial gap between vision and language modalities, especially when dealing with limited large-scale datasets. In this paper, we introduce a CLIP-based Synergistic Knowledge Transfer(CSKT) approach for TBPR. Specifically, to explore the CLIP's knowledge on input side, we first propose a Bidirectional Prompts Transferring (BPT) module constructed by text-to-image and image-to-text bidirectional prompts and coupling projections. Secondly, Dual Adapters Transferring (DAT) is designed to transfer knowledge on output side of Multi-Head Self-Attention (MHSA) in vision and language. This synergistic two-way collaborative mechanism promotes the early-stage feature fusion and efficiently exploits the existing knowledge of CLIP. CSKT outperforms the state-of-the-art approaches across three benchmark datasets when the training parameters merely account for 7.4% of the entire model, demonstrating its remarkable efficiency, effectiveness and generalization.
</details>
<details>
<summary>摘要</summary>
文本基于人物检索（Text-based Person Retrieval，TBPR）目标是根据文本查询 retrieve 目标人像。主要挑战在视觉和语言模式之间的差异 bridge，特别是处理有限的大规模数据集。本文提出了基于 CLIP 的 Synergistic Knowledge Transfer（CSKT）方法 для TBPR。具体来说，我们首先提出了文本到图像和图像到文本的双向提问模块（BPT），并将其与 coupling projections 结合。其次，我们设计了双Adapter Transferring（DAT）模块，用于在视觉和语言中的多头自注意力（MHSA）知识传递。这种同时合作机制促进了早期特征融合，高效地利用 CLIP 已有的知识。CSKT 在三个标准数据集上超过了状态体系的方法，即使只用训练参数占据 7.4% 的整个模型，表明其非常高效、有效和普遍。
</details></li>
</ul>
<hr>
<h2 id="HiFTNet-A-Fast-High-Quality-Neural-Vocoder-with-Harmonic-plus-Noise-Filter-and-Inverse-Short-Time-Fourier-Transform"><a href="#HiFTNet-A-Fast-High-Quality-Neural-Vocoder-with-Harmonic-plus-Noise-Filter-and-Inverse-Short-Time-Fourier-Transform" class="headerlink" title="HiFTNet: A Fast High-Quality Neural Vocoder with Harmonic-plus-Noise Filter and Inverse Short Time Fourier Transform"></a>HiFTNet: A Fast High-Quality Neural Vocoder with Harmonic-plus-Noise Filter and Inverse Short Time Fourier Transform</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09493">http://arxiv.org/abs/2309.09493</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinghao Aaron Li, Cong Han, Xilin Jiang, Nima Mesgarani</li>
<li>for: 高品质语音合成</li>
<li>methods:  integrate inverse short-time Fourier transform (iSTFT) into the network, incorporates a harmonic-plus-noise source filter in the time-frequency domain</li>
<li>results: 比 iSTFTNet 和 HiFi-GAN 更高效， achieve ground-truth-level performance, outperforms BigVGAN 在未看到 Speaker 上， achieve comparable performance to BigVGAN while being four times faster with only $1&#x2F;6$ of the parameters.<details>
<summary>Abstract</summary>
Recent advancements in speech synthesis have leveraged GAN-based networks like HiFi-GAN and BigVGAN to produce high-fidelity waveforms from mel-spectrograms. However, these networks are computationally expensive and parameter-heavy. iSTFTNet addresses these limitations by integrating inverse short-time Fourier transform (iSTFT) into the network, achieving both speed and parameter efficiency. In this paper, we introduce an extension to iSTFTNet, termed HiFTNet, which incorporates a harmonic-plus-noise source filter in the time-frequency domain that uses a sinusoidal source from the fundamental frequency (F0) inferred via a pre-trained F0 estimation network for fast inference speed. Subjective evaluations on LJSpeech show that our model significantly outperforms both iSTFTNet and HiFi-GAN, achieving ground-truth-level performance. HiFTNet also outperforms BigVGAN-base on LibriTTS for unseen speakers and achieves comparable performance to BigVGAN while being four times faster with only $1/6$ of the parameters. Our work sets a new benchmark for efficient, high-quality neural vocoding, paving the way for real-time applications that demand high quality speech synthesis.
</details>
<details>
<summary>摘要</summary>
现代语音合成技术已经利用GAN基于网络，如HiFi-GAN和BigVGAN，生成高质量波形从mel-spectrogram中。然而，这些网络 computationally expensive和parameter-heavy。iSTFTNetAddress这些限制，通过将 inverse short-time Fourier transform (iSTFT)  integrate into the network，实现了速度和参数效率。在这篇论文中，我们介绍了一种增强版iSTFTNet，称为HiFTNet，它在时域-频域中使用一个harmonic-plus-noise源滤波器，使用来自基准频率(F0) 的恒定频率来进行快速的推理。对LJSpeech进行主观评估，我们发现我们的模型在iSTFTNet和HiFi-GAN之上显著超越，达到了参照水平的性能。HiFTNet还在LibriTTS上超越BigVGAN-base，并在未看到说话者时达到了相同的性能，只用了BigVGAN的一半参数和速度。我们的工作设置了新的高质量、高效的神经 vocoding 标准，为实时应用程序提供了高质量语音合成的路径。
</details></li>
</ul>
<hr>
<h2 id="Mechanic-Maker-2-0-Reinforcement-Learning-for-Evaluating-Generated-Rules"><a href="#Mechanic-Maker-2-0-Reinforcement-Learning-for-Evaluating-Generated-Rules" class="headerlink" title="Mechanic Maker 2.0: Reinforcement Learning for Evaluating Generated Rules"></a>Mechanic Maker 2.0: Reinforcement Learning for Evaluating Generated Rules</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09476">http://arxiv.org/abs/2309.09476</a></li>
<li>repo_url: None</li>
<li>paper_authors: Johor Jara Gonzalez, Seth Cooper, Mathew Guzdial</li>
<li>for: 本研究用Reinforcement Learning（RL）作为人类游戏行为的近似方法，用于自动生成游戏规则。</li>
<li>methods: 本研究使用RL方法来模拟人类游戏行为，并在Unity游戏引擎上创建了一个新的开源规则生成框架。</li>
<li>results: 研究结果表明，RL生成的规则与A*机器人基eline有所不同，可能更有用于人类游戏玩家。<details>
<summary>Abstract</summary>
Automated game design (AGD), the study of automatically generating game rules, has a long history in technical games research. AGD approaches generally rely on approximations of human play, either objective functions or AI agents. Despite this, the majority of these approximators are static, meaning they do not reflect human player's ability to learn and improve in a game. In this paper, we investigate the application of Reinforcement Learning (RL) as an approximator for human play for rule generation. We recreate the classic AGD environment Mechanic Maker in Unity as a new, open-source rule generation framework. Our results demonstrate that RL produces distinct sets of rules from an A* agent baseline, which may be more usable by humans.
</details>
<details>
<summary>摘要</summary>
自动游戏设计（AGD），游戏规则自动生成的研究，有很长的历史在技术游戏研究中。AGD方法通常基于人类游戏行为的估计，可以是目标函数或AI代理。尽管如此，大多数这些估计都是静态的， meaning they do not reflect human players' ability to learn and improve in a game. 在这篇论文中，我们研究了在人类游戏行为中使用奖励学习（RL）作为估计器。我们在Unity中重新创建了 Mechanic Maker 环境，并将其转换为一个新的、开源的规则生成框架。我们的结果表明，RL 可以生成与 A* 代理基线相比较为人类更可用的规则集。
</details></li>
</ul>
<hr>
<h2 id="Exploring-and-Learning-in-Sparse-Linear-MDPs-without-Computationally-Intractable-Oracles"><a href="#Exploring-and-Learning-in-Sparse-Linear-MDPs-without-Computationally-Intractable-Oracles" class="headerlink" title="Exploring and Learning in Sparse Linear MDPs without Computationally Intractable Oracles"></a>Exploring and Learning in Sparse Linear MDPs without Computationally Intractable Oracles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09457">http://arxiv.org/abs/2309.09457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Noah Golowich, Ankur Moitra, Dhruv Rohatgi</li>
<li>for: 这篇论文的目的是解决线性Markov决策过程（MDP）中的特征选择问题，即在缺乏专家领域知识的情况下，通过学习一个近似优化策略，以便在仅有多少交互中学习环境。</li>
<li>methods: 这篇论文使用了特征选择的思想，即在一个$k$-稀疏的线性MDP中，找到一个SIZE$k$的子集$S\subset[d]$，其中$d$是特征的维度，这个子集包含所有有用的特征。这篇论文提出了首个可行的算法来解决这个问题。</li>
<li>results: 这篇论文的主要结果是在线性MDP中实现了近似优化策略，并且这个算法只需要多少交互来学习。此外，这篇论文还引入了一种观测器，即一种简洁的逻辑表示环境的转移，可以有效地计算一些Bellman backup。这个观测器可以通过几何编程来计算得到。此外，这篇论文还给出了一种可以在块MDP中实现的算法，该算法可以在几何时间内学习一个近似优化策略，并且这个算法可以在多少交互中学习。<details>
<summary>Abstract</summary>
The key assumption underlying linear Markov Decision Processes (MDPs) is that the learner has access to a known feature map $\phi(x, a)$ that maps state-action pairs to $d$-dimensional vectors, and that the rewards and transitions are linear functions in this representation. But where do these features come from? In the absence of expert domain knowledge, a tempting strategy is to use the ``kitchen sink" approach and hope that the true features are included in a much larger set of potential features. In this paper we revisit linear MDPs from the perspective of feature selection. In a $k$-sparse linear MDP, there is an unknown subset $S \subset [d]$ of size $k$ containing all the relevant features, and the goal is to learn a near-optimal policy in only poly$(k,\log d)$ interactions with the environment. Our main result is the first polynomial-time algorithm for this problem. In contrast, earlier works either made prohibitively strong assumptions that obviated the need for exploration, or required solving computationally intractable optimization problems.   Along the way we introduce the notion of an emulator: a succinct approximate representation of the transitions that suffices for computing certain Bellman backups. Since linear MDPs are a non-parametric model, it is not even obvious whether polynomial-sized emulators exist. We show that they do exist and can be computed efficiently via convex programming.   As a corollary of our main result, we give an algorithm for learning a near-optimal policy in block MDPs whose decoding function is a low-depth decision tree; the algorithm runs in quasi-polynomial time and takes a polynomial number of samples. This can be seen as a reinforcement learning analogue of classic results in computational learning theory. Furthermore, it gives a natural model where improving the sample complexity via representation learning is computationally feasible.
</details>
<details>
<summary>摘要</summary>
Linear Markov Decision Processes (MDPs) 的关键假设是学习者有访问known feature map $\phi(x, a)$，这个映射将状态-动作对映射到 $d$-维向量上，并且奖励和转移是这个表示下的线性函数。但是这些特征来自哪里？在不具备专家领域知识的情况下，一个自然的策略是使用“厨房抽象”方法，希望真实的特征包含在一个更大的可能性集中。在这篇文章中，我们从 linear MDPs 的特征选择角度重新探讨这个问题。在 $k$-简 sparse linear MDP 中，存在一个未知的子集 $S \subset [d]$ 的大小为 $k$，包含所有相关的特征，并且目标是通过只需 poly$(k, \log d)$ 交互来学习一个近似优化的策略。我们的主要结果是首先的幂时间算法。在对比之下，先前的作品 Either 假设了可观察的环境，或者需要解决 computationally intractable 优化问题。在路径中，我们引入了一个新的概念：模拟器。模拟器是一种简洁的 Approximate  represntation of the transitions，可以用于计算某些 Bellman backups。由于 linear MDPs 是非 Parametric 模型，也不知道 whether polynomial-sized emulators exist。我们证明了它们确实存在，并可以通过几何Programming 计算得到。作为一个辅助结果，我们提供了一个算法，可以在块 MDPs 中学习一个近似优化的策略，其中 decoding 函数是一个 low-depth 决策树。该算法在 quasi-polynomial 时间内运行，并且只需要 polynomial 个样本。这可以看作是一种 reinforcement learning 的类比，以及一种可以通过 representation learning 提高样本复杂性的计算可能性。更多细节可以参考文章。
</details></li>
</ul>
<hr>
<h2 id="Are-You-Worthy-of-My-Trust-A-Socioethical-Perspective-on-the-Impacts-of-Trustworthy-AI-Systems-on-the-Environment-and-Human-Society"><a href="#Are-You-Worthy-of-My-Trust-A-Socioethical-Perspective-on-the-Impacts-of-Trustworthy-AI-Systems-on-the-Environment-and-Human-Society" class="headerlink" title="Are You Worthy of My Trust?: A Socioethical Perspective on the Impacts of Trustworthy AI Systems on the Environment and Human Society"></a>Are You Worthy of My Trust?: A Socioethical Perspective on the Impacts of Trustworthy AI Systems on the Environment and Human Society</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09450">http://arxiv.org/abs/2309.09450</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jamell Dacon</li>
<li>for: 本文旨在探讨人工智能系统在社会中的影响，以及如何通过多学科合作和系统性的审核来确保人工智能的可靠性。</li>
<li>methods: 本文使用了多学科的视角和系统性的审核方法来检视人工智能系统的社会影响。</li>
<li>results: 本文指出，人工智能系统的社会影响包括能源消耗和碳脚印，以及对用户的社会发展影响。此外，本文还探讨了人工智能系统的多学科风险和不可避免的社会影响。<details>
<summary>Abstract</summary>
With ubiquitous exposure of AI systems today, we believe AI development requires crucial considerations to be deemed trustworthy. While the potential of AI systems is bountiful, though, is still unknown-as are their risks. In this work, we offer a brief, high-level overview of societal impacts of AI systems. To do so, we highlight the requirement of multi-disciplinary governance and convergence throughout its lifecycle via critical systemic examinations (e.g., energy consumption), and later discuss induced effects on the environment (i.e., carbon footprint) and its users (i.e., social development). In particular, we consider these impacts from a multi-disciplinary perspective: computer science, sociology, environmental science, and so on to discuss its inter-connected societal risks and inability to simultaneously satisfy aspects of well-being. Therefore, we accentuate the necessity of holistically addressing pressing concerns of AI systems from a socioethical impact assessment perspective to explicate its harmful societal effects to truly enable humanity-centered Trustworthy AI.
</details>
<details>
<summary>摘要</summary>
From a computer science perspective, we must examine the energy consumption of AI systems and their potential carbon footprint. From a sociological perspective, we must consider the impact of AI systems on social development and the potential for induced effects on users. Environmental science must also be taken into account, as AI systems may have a significant impact on the environment.To truly enable humanity-centered Trustworthy AI, we must address these concerns holistically, taking a socioethical impact assessment perspective to explicate the harmful societal effects of AI systems. By considering these impacts from a multi-disciplinary perspective, we can better understand the interconnected risks and challenges posed by AI systems and work towards developing Trustworthy AI that benefits society as a whole.
</details></li>
</ul>
<hr>
<h2 id="A-Schedule-of-Duties-in-the-Cloud-Space-Using-a-Modified-Salp-Swarm-Algorithm"><a href="#A-Schedule-of-Duties-in-the-Cloud-Space-Using-a-Modified-Salp-Swarm-Algorithm" class="headerlink" title="A Schedule of Duties in the Cloud Space Using a Modified Salp Swarm Algorithm"></a>A Schedule of Duties in the Cloud Space Using a Modified Salp Swarm Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09441">http://arxiv.org/abs/2309.09441</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hossein Jamali, Ponkoj Chandra Shill, David Feil-Seifer, Frederick C. Harris, Jr., Sergiu M. Dascalu</li>
<li>for: 这个论文的目的是提出一种基于集群智能算法的云计算任务调度算法，以提高云计算服务的效率和质量。</li>
<li>methods: 该论文使用了改进后的Salp Swarm Algorithm（SSA），并对其进行了比较与基于遗传算法（GA）、Particle Swarm Optimization（PSO）、连续ACO（ACO）等算法的性能比较。</li>
<li>results: 研究发现，提出的算法在云计算任务调度问题中的性能较高，比如与基本SSA相比，该算法的吞吐量减少约21%。<details>
<summary>Abstract</summary>
Cloud computing is a concept introduced in the information technology era, with the main components being the grid, distributed, and valuable computing. The cloud is being developed continuously and, naturally, comes up with many challenges, one of which is scheduling. A schedule or timeline is a mechanism used to optimize the time for performing a duty or set of duties. A scheduling process is accountable for choosing the best resources for performing a duty. The main goal of a scheduling algorithm is to improve the efficiency and quality of the service while at the same time ensuring the acceptability and effectiveness of the targets. The task scheduling problem is one of the most important NP-hard issues in the cloud domain and, so far, many techniques have been proposed as solutions, including using genetic algorithms (GAs), particle swarm optimization, (PSO), and ant colony optimization (ACO). To address this problem, in this paper, one of the collective intelligence algorithms, called the Salp Swarm Algorithm (SSA), has been expanded, improved, and applied. The performance of the proposed algorithm has been compared with that of GAs, PSO, continuous ACO, and the basic SSA. The results show that our algorithm has generally higher performance than the other algorithms. For example, compared to the basic SSA, the proposed method has an average reduction of approximately 21% in makespan.
</details>
<details>
<summary>摘要</summary>
云计算是信息时代中提出的概念，其主要组成部分包括网格、分布式和价值计算。云是不断发展的，并且随着时间的推移而带来许多挑战，其中之一是调度。调度是一种机制，用于优化执行任务的时间。调度过程的目标是选择最佳资源来执行任务。云计算领域中的调度问题是NP困难问题之一，至今为止，已经有许多技术提出了解决方案，包括使用遗传算法（GAs）、粒子群动力学（PSO）和蚁群动力学（ACO）。为解决这个问题，在这篇论文中，一种集成智能算法，即Salp Swarm Algorithm（SSA），被扩展、改进并应用。提出的算法的性能与GAs、PSO、继续ACO和基本SSA进行比较，结果表明，我们的算法在性能方面与其他算法相比，有一定的优势。例如，相比基本SSA，我们的方法在做出span中减少了约21%的差异。
</details></li>
</ul>
<hr>
<h2 id="Multi-Agent-Deep-Reinforcement-Learning-for-Cooperative-and-Competitive-Autonomous-Vehicles-using-AutoDRIVE-Ecosystem"><a href="#Multi-Agent-Deep-Reinforcement-Learning-for-Cooperative-and-Competitive-Autonomous-Vehicles-using-AutoDRIVE-Ecosystem" class="headerlink" title="Multi-Agent Deep Reinforcement Learning for Cooperative and Competitive Autonomous Vehicles using AutoDRIVE Ecosystem"></a>Multi-Agent Deep Reinforcement Learning for Cooperative and Competitive Autonomous Vehicles using AutoDRIVE Ecosystem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10007">http://arxiv.org/abs/2309.10007</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tanmay Vilas Samak, Chinmay Vilas Samak, Venkat Krovi</li>
<li>for: 这 paper 旨在开发一种模块化和并行化的多代理人深度学习框架，用于养成自驾车辆的合作和竞争行为。</li>
<li>methods: 该 paper 使用了 AutoDRIVE Ecosystem 作为开发 physically accurate 和 graphically realistic 的数字双胞虫的 enables，并在这个 ecosystem 中训练和部署多代理人学习策略。</li>
<li>results:  experiments 表明，在交叉路口通行问题和thead-to-head自驱车竞赛问题中，使用这种框架可以养成高效的合作和竞争行为，并且在带有偏差和安全约束的情况下进行了可靠的训练和测试。<details>
<summary>Abstract</summary>
This work presents a modular and parallelizable multi-agent deep reinforcement learning framework for imbibing cooperative as well as competitive behaviors within autonomous vehicles. We introduce AutoDRIVE Ecosystem as an enabler to develop physically accurate and graphically realistic digital twins of Nigel and F1TENTH, two scaled autonomous vehicle platforms with unique qualities and capabilities, and leverage this ecosystem to train and deploy multi-agent reinforcement learning policies. We first investigate an intersection traversal problem using a set of cooperative vehicles (Nigel) that share limited state information with each other in single as well as multi-agent learning settings using a common policy approach. We then investigate an adversarial head-to-head autonomous racing problem using a different set of vehicles (F1TENTH) in a multi-agent learning setting using an individual policy approach. In either set of experiments, a decentralized learning architecture was adopted, which allowed robust training and testing of the approaches in stochastic environments, since the agents were mutually independent and exhibited asynchronous motion behavior. The problems were further aggravated by providing the agents with sparse observation spaces and requiring them to sample control commands that implicitly satisfied the imposed kinodynamic as well as safety constraints. The experimental results for both problem statements are reported in terms of quantitative metrics and qualitative remarks for training as well as deployment phases.
</details>
<details>
<summary>摘要</summary>
In the first set of experiments, we investigate an intersection traversal problem using a set of cooperative vehicles (Nigel) that share limited state information with each other. We use a common policy approach in both single and multi-agent learning settings. In the second set of experiments, we investigate an adversarial head-to-head autonomous racing problem using a different set of vehicles (F1TENTH) in a multi-agent learning setting using an individual policy approach.In both sets of experiments, we adopt a decentralized learning architecture that allows for robust training and testing of the approaches in stochastic environments. The agents are mutually independent and exhibit asynchronous motion behavior, which adds complexity to the problems. To make the problems more challenging, we provide the agents with sparse observation spaces and require them to sample control commands that implicitly satisfy the imposed kinodynamic and safety constraints.The experimental results for both problem statements are reported in terms of quantitative metrics and qualitative remarks for training and deployment phases. The results demonstrate the effectiveness of the proposed framework in imbibing cooperative and competitive behaviors in autonomous vehicles.
</details></li>
</ul>
<hr>
<h2 id="The-Optimized-path-for-the-public-transportation-of-Incheon-in-South-Korea"><a href="#The-Optimized-path-for-the-public-transportation-of-Incheon-in-South-Korea" class="headerlink" title="The Optimized path for the public transportation of Incheon in South Korea"></a>The Optimized path for the public transportation of Incheon in South Korea</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10006">http://arxiv.org/abs/2309.10006</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soroor Malekmohammadi faradunbeh, Hongle Li, Mangkyu Kang, Choongjae Iim</li>
<li>for: 本研究旨在为沟通系统路径选择优化，以满足乘客需求。</li>
<li>methods: 我们提出了一种基于修改A*算法的路径找路方法，可以在实时中找到大量数据点的最短路径。</li>
<li>results: 我们的方法比基本路径找路算法如基因算法和迪克斯特拉算法更好，可以快速找到最短路径。<details>
<summary>Abstract</summary>
Path-finding is one of the most popular subjects in the field of computer science. Pathfinding strategies determine a path from a given coordinate to another. The focus of this paper is on finding the optimal path for the bus transportation system based on passenger demand. This study is based on bus stations in Incheon, South Korea, and we show that our modified A* algorithm performs better than other basic pathfinding algorithms such as the Genetic and Dijkstra. Our proposed approach can find the shortest path in real-time even for large amounts of data(points).
</details>
<details>
<summary>摘要</summary>
“路径找路是计算机科学领域中最受欢迎的主题之一。路径找路策略的目的是将 FROM 点到 DESTINATION 点找到最佳路径。本研究是基于韩国仁川市的公共汽车站点，我们显示了我们的修改了A\*搜寻算法可以在实时运算大量数据点上取得更短路径。”Here's the breakdown of the translation:“路径找路” (lù xiào lù) - path-finding“是计算机科学领域中最受欢迎的主题之一” (shì computacional kēxué yù zhì yǐn yī yī) - one of the most popular subjects in the field of computer science“路径找路策略” (lù xiào lù zhì lü) - path-finding strategies“的目的是将 FROM 点到 DESTINATION 点找到最佳路径” (de mù yì yī jīn yǐn zhèng yī jīn) - the goal is to find the shortest path from FROM point to DESTINATION point“本研究是基于韩国仁川市的公共汽车站点” (ben yán jí yì yī jīn yǐn zhèng yī jīn) - this study is based on bus stations in Incheon, South Korea“我们显示了我们的修改了A\*搜寻算法可以在实时运算大量数据点上取得更短路径” (wǒ men jiàn shì le yǒu men de xiū gòu le A\* sōu xún suān yì yī yī yī) - we show that our modified A* search algorithm can obtain shorter paths in real-time for large amounts of data points.
</details></li>
</ul>
<hr>
<h2 id="FactoFormer-Factorized-Hyperspectral-Transformers-with-Self-Supervised-Pre-Training"><a href="#FactoFormer-Factorized-Hyperspectral-Transformers-with-Self-Supervised-Pre-Training" class="headerlink" title="FactoFormer: Factorized Hyperspectral Transformers with Self-Supervised Pre-Training"></a>FactoFormer: Factorized Hyperspectral Transformers with Self-Supervised Pre-Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09431">http://arxiv.org/abs/2309.09431</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/csiro-robotics/factoformer">https://github.com/csiro-robotics/factoformer</a></li>
<li>paper_authors: Shaheer Mohamed, Maryam Haghighat, Tharindu Fernando, Sridha Sridharan, Clinton Fookes, Peyman Moghadam</li>
<li>for: 本研究旨在使用变换器来提高卷积图像分类 task 的性能。</li>
<li>methods: 本研究使用了一种新的分解 spectral-spatial transformer，并提出了一种基于自我supervised pre-training的方法，以及一种基于masking的预训练策略。</li>
<li>results: 实验结果表明，该模型在三个公开的 dataset 上的分类任务中均达到了状态册的性能。<details>
<summary>Abstract</summary>
Hyperspectral images (HSIs) contain rich spectral and spatial information. Motivated by the success of transformers in the field of natural language processing and computer vision where they have shown the ability to learn long range dependencies within input data, recent research has focused on using transformers for HSIs. However, current state-of-the-art hyperspectral transformers only tokenize the input HSI sample along the spectral dimension, resulting in the under-utilization of spatial information. Moreover, transformers are known to be data-hungry and their performance relies heavily on large-scale pre-training, which is challenging due to limited annotated hyperspectral data. Therefore, the full potential of HSI transformers has not been fully realized. To overcome these limitations, we propose a novel factorized spectral-spatial transformer that incorporates factorized self-supervised pre-training procedures, leading to significant improvements in performance. The factorization of the inputs allows the spectral and spatial transformers to better capture the interactions within the hyperspectral data cubes. Inspired by masked image modeling pre-training, we also devise efficient masking strategies for pre-training each of the spectral and spatial transformers. We conduct experiments on three publicly available datasets for HSI classification task and demonstrate that our model achieves state-of-the-art performance in all three datasets. The code for our model will be made available at https://github.com/csiro-robotics/factoformer.
</details>
<details>
<summary>摘要</summary>
干扰图像（HSIs）含有丰富的spectral和空间信息。驱动于自然语言处理和计算机视觉领域中transformers的成功，现有研究强调使用transformers来处理HSIs。然而，当前状态的艺术HSITransformer只是在spectral维度上进行HSIs的token化，从而导致了空间信息的不足利用。此外，transformers是知道数据备受的，其性能强度取决于大规模的预训练，这是因为有限的注解干扰图像数据。因此，HSITransformer的全部潜力尚未得到充分利用。为了解决这些限制，我们提出了一种新的分解 spectral-spatial transformer，该模型包括分解自我监督预训练过程，从而导致显著的性能提升。该模型的分解输入允许spectral和空间变换更好地捕捉干扰数据立方体中的交互。我们还根据干扰图像模型预训练的想法，设计了高效的屏蔽策略，用于预训练每个spectral和空间变换。我们在三个公共可用的干扰图像数据集上进行了实验，并证明了我们的模型在所有三个数据集中均达到了状态之最的性能。我们的代码将在https://github.com/csiro-robotics/factoformer上公开。
</details></li>
</ul>
<hr>
<h2 id="Joint-Demosaicing-and-Denoising-with-Double-Deep-Image-Priors"><a href="#Joint-Demosaicing-and-Denoising-with-Double-Deep-Image-Priors" class="headerlink" title="Joint Demosaicing and Denoising with Double Deep Image Priors"></a>Joint Demosaicing and Denoising with Double Deep Image Priors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09426">http://arxiv.org/abs/2309.09426</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taihui Li, Anish Lahiri, Yutong Dai, Owen Mayer</li>
<li>for: joint demosaicing and denoising of RAW images</li>
<li>methods: direct injection of prior ( DoubleDIP) without requiring training data</li>
<li>results: consistently outperforms other compared methods in terms of PSNR, SSIM, and qualitative visual perception<details>
<summary>Abstract</summary>
Demosaicing and denoising of RAW images are crucial steps in the processing pipeline of modern digital cameras. As only a third of the color information required to produce a digital image is captured by the camera sensor, the process of demosaicing is inherently ill-posed. The presence of noise further exacerbates this problem. Performing these two steps sequentially may distort the content of the captured RAW images and accumulate errors from one step to another. Recent deep neural-network-based approaches have shown the effectiveness of joint demosaicing and denoising to mitigate such challenges. However, these methods typically require a large number of training samples and do not generalize well to different types and intensities of noise. In this paper, we propose a novel joint demosaicing and denoising method, dubbed JDD-DoubleDIP, which operates directly on a single RAW image without requiring any training data. We validate the effectiveness of our method on two popular datasets -- Kodak and McMaster -- with various noises and noise intensities. The experimental results show that our method consistently outperforms other compared methods in terms of PSNR, SSIM, and qualitative visual perception.
</details>
<details>
<summary>摘要</summary>
这里的数码相机处理管线中，采样和噪音处理是重要的步骤。因为摄像机感应器只Capture一 third of the color information needed to produce a digital image, therefore, the process of demosaicing is inherently ill-posed. The presence of noise further exacerbates this problem. Performing these two steps sequentially may distort the content of the captured RAW images and accumulate errors from one step to another. Recent deep neural network-based approaches have shown the effectiveness of joint demosaicing and denoising to mitigate such challenges. However, these methods typically require a large number of training samples and do not generalize well to different types and intensities of noise.在本文中，我们提出了一个新的共同采样和噪音处理方法，名为JDD-DoubleDIP，这个方法直接运算在单一的 RAW 图像上，不需要任何训练数据。我们验证了我们的方法在两个流行的数据集（Kodak 和 McMaster）上，具有不同的噪音和噪音强度时，可以实现更高的 PSNR、SSIM 和质感视觉。
</details></li>
</ul>
<hr>
<h2 id="Causal-Discovery-and-Prediction-Methods-and-Algorithms"><a href="#Causal-Discovery-and-Prediction-Methods-and-Algorithms" class="headerlink" title="Causal Discovery and Prediction: Methods and Algorithms"></a>Causal Discovery and Prediction: Methods and Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09416">http://arxiv.org/abs/2309.09416</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gilles Blondel</li>
<li>for: 这个博士论文是为了提出一种能够在很多不同情况下快速和有效地发现 causal model的算法，并且可以避免在实际世界中进行不必要的实验。</li>
<li>methods: 这个算法使用了一种通过设计简单的 experiment 来评估每个可能的 intervention 的 cost，并且使用了一种最低成本序列的 intervention 来找出 causal relations。</li>
<li>results: 这个算法可以在大多数情况下使用相对便宜的 intervention 来排除大量 causal model candidate，并且可以确保在找出 causal effects 时不会产生很多不必要的实验。此外，这个算法还可以在有限的试验次数下完成 causal discovery。<details>
<summary>Abstract</summary>
We are not only observers but also actors of reality. Our capability to intervene and alter the course of some events in the space and time surrounding us is an essential component of how we build our model of the world. In this doctoral thesis we introduce a generic a-priori assessment of each possible intervention, in order to select the most cost-effective interventions only, and avoid unnecessary systematic experimentation on the real world. Based on this a-priori assessment, we propose an active learning algorithm that identifies the causal relations in any given causal model, using a least cost sequence of interventions. There are several novel aspects introduced by our algorithm. It is, in most case scenarios, able to discard many causal model candidates using relatively inexpensive interventions that only test one value of the intervened variables. Also, the number of interventions performed by the algorithm can be bounded by the number of causal model candidates. Hence, fewer initial candidates (or equivalently, more prior knowledge) lead to fewer interventions for causal discovery.   Causality is intimately related to time, as causes appear to precede their effects. Cyclical causal processes are a very interesting case of causality in relation to time. In this doctoral thesis we introduce a formal analysis of time cyclical causal settings by defining a causal analog to the purely observational Dynamic Bayesian Networks, and provide a sound and complete algorithm for the identification of causal effects in the cyclic setting. We introduce the existence of two types of hidden confounder variables in this framework, which affect in substantially different ways the identification procedures, a distinction with no analog in either Dynamic Bayesian Networks or standard causal graphs.
</details>
<details>
<summary>摘要</summary>
我们不只是观察者，也是现实中的actor。我们可以在空间和时间周围 intervene 和改变一些事件的走向，这是我们构建世界模型的重要组成部分。在这个博士论文中，我们介绍了一种通用的先行评估，以选择最cost-effective的 intervención，并避免在实际世界进行无必要的系统性实验。基于这种先行评估，我们提议一种活动学习算法，用于在任何给定 causal model 中找到 causal 关系，使用最小成本序列的 intervención。我们的算法有几个新的特点：在大多数情况下，可以通过对 intervened 变量的一个值进行相对较少的实验来抛弃许多 causal model 候选者。此外，我们的算法可以 guarantee 在给定 causal model 候选者的数量上限下进行 intervención。因此， fewer initial candidates (或等效的更多先知) 会导致更少的 intervención для causal discovery。 causality 与时间 closely related, causes 通常会先于其效果出现。循环 causal process 是 causality 在时间方面的一个非常有趣的情况。在这个博士论文中，我们提出了一种形式的时间循环 causal setting 的分析，并提供了一个完整的算法，用于在这种设定下找到 causal effect。我们还引入了两种隐藏干扰变量，这些变量在不同的方式影响了我们的 identification 过程，与 neither Dynamic Bayesian Networks  nor standard causal graphs 中的干扰变量没有相应的分类。
</details></li>
</ul>
<hr>
<h2 id="Does-Video-Summarization-Require-Videos-Quantifying-the-Effectiveness-of-Language-in-Video-Summarization"><a href="#Does-Video-Summarization-Require-Videos-Quantifying-the-Effectiveness-of-Language-in-Video-Summarization" class="headerlink" title="Does Video Summarization Require Videos? Quantifying the Effectiveness of Language in Video Summarization"></a>Does Video Summarization Require Videos? Quantifying the Effectiveness of Language in Video Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09405">http://arxiv.org/abs/2309.09405</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yoonsoo Nam, Adam Lehavi, Daniel Yang, Digbalay Bose, Swabha Swayamdipta, Shrikanth Narayanan</li>
<li>for: 这个论文是为了开发一个高效、语言基于的视频摘要器，以提高计算机视觉中的视频摘要效果。</li>
<li>methods: 该论文使用了语言变换器模型，只使用文本描述来进行训练，而不需要图像表示。文本描述通过零批量学习获取，并通过筛选表示文本vector进行压缩。</li>
<li>results: 该论文可以具有更高的数据效率和解释性，并且可以保持与传统方法相比的结果水平。在模式和数据压缩方面，研究发现，只使用语言模式可以有效地减少输入数据处理量，而不会导致结果下降。<details>
<summary>Abstract</summary>
Video summarization remains a huge challenge in computer vision due to the size of the input videos to be summarized. We propose an efficient, language-only video summarizer that achieves competitive accuracy with high data efficiency. Using only textual captions obtained via a zero-shot approach, we train a language transformer model and forego image representations. This method allows us to perform filtration amongst the representative text vectors and condense the sequence. With our approach, we gain explainability with natural language that comes easily for human interpretation and textual summaries of the videos. An ablation study that focuses on modality and data compression shows that leveraging text modality only effectively reduces input data processing while retaining comparable results.
</details>
<details>
<summary>摘要</summary>
视频摘要仍然是计算机视觉领域的大型挑战，主要是因为要摘要的视频尺寸过于大。我们提出了一种高效的语言只视频摘要器，可以实现与高数据效率相当的竞争性准确率。通过采用零shot方法获取的文本描述，我们训练了一个语言转换器模型，并忽略图像表示。这种方法allow us来对表示序列进行筛选，并将其压缩。通过我们的方法，我们可以获得自然语言的解释，这是人类可读的文本摘要。我们的ablation研究表明，只使用文本模式可以有效地减少输入数据处理，同时保持相对比较的结果。
</details></li>
</ul>
<hr>
<h2 id="Deployed-Application-Promoting-Research-Collaboration-with-Open-Data-Driven-Team-Recommendation-in-Response-to-Call-for-Proposals"><a href="#Deployed-Application-Promoting-Research-Collaboration-with-Open-Data-Driven-Team-Recommendation-in-Response-to-Call-for-Proposals" class="headerlink" title="(Deployed Application) Promoting Research Collaboration with Open Data Driven Team Recommendation in Response to Call for Proposals"></a>(Deployed Application) Promoting Research Collaboration with Open Data Driven Team Recommendation in Response to Call for Proposals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09404">http://arxiv.org/abs/2309.09404</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siva Likitha Valluru, Biplav Srivastava, Sai Teja Paladi, Siwen Yan, Sriraam Natarajan</li>
<li>for: The paper aims to recommend teams for collaborative research opportunities in response to funding agencies’ calls for proposals.</li>
<li>methods: The system uses various AI methods, including skill extraction from open data and taxonomies, to match demand and supply and create balanced teams that maximize goodness.</li>
<li>results: The system was validated through quantitative and qualitative evaluations, showing that it recommends smaller but higher-quality teams, and was found useful and relevant by users in a large-scale user study.<details>
<summary>Abstract</summary>
Building teams and promoting collaboration are two very common business activities. An example of these are seen in the TeamingForFunding problem, where research institutions and researchers are interested to identify collaborative opportunities when applying to funding agencies in response to latter's calls for proposals. We describe a novel system to recommend teams using a variety of AI methods, such that (1) each team achieves the highest possible skill coverage that is demanded by the opportunity, and (2) the workload of distributing the opportunities is balanced amongst the candidate members. We address these questions by extracting skills latent in open data of proposal calls (demand) and researcher profiles (supply), normalizing them using taxonomies, and creating efficient algorithms that match demand to supply. We create teams to maximize goodness along a novel metric balancing short- and long-term objectives. We validate the success of our algorithms (1) quantitatively, by evaluating the recommended teams using a goodness score and find that more informed methods lead to recommendations of smaller number of teams but higher goodness, and (2) qualitatively, by conducting a large-scale user study at a college-wide level, and demonstrate that users overall found the tool very useful and relevant. Lastly, we evaluate our system in two diverse settings in US and India (of researchers and proposal calls) to establish generality of our approach, and deploy it at a major US university for routine use.
</details>
<details>
<summary>摘要</summary>
建立团队和促进协作是现代企业活动的常见任务。一个例子是团队为融资机构申请资金的问题，研究机构和研究人员希望通过协作机会来应对后者的征求提案。我们描述了一种新的系统，使用人工智能技术来推荐团队，以确保每个团队达到最高可能的技能覆盖度，同时将工作负担平均分配给候选成员。我们解决这些问题 by提取提案征求（需求）和研究人员资料（供应）中的技能，使用分类法归一化它们，并创建高效的匹配算法来匹配需求和供应。我们创建了一个以最大化好处为基础的新指标，以匹配短期和长期目标。我们证明了我们的算法的成功（1）量化方面，通过评估推荐团队的好度分数，发现更加知ledge的方法会导致 fewer teams but higher goodness，和（2）质量方面，通过在大学level进行大规模的用户研究，发现用户们总体认为工具非常有用和相关。最后，我们在美国和印度两地进行了两次不同的设置（研究人员和提案征求）来证明我们的方法的一致性，并在一所主要的美国大学中进行了 Routine 使用。
</details></li>
</ul>
<hr>
<h2 id="Selecting-which-Dense-Retriever-to-use-for-Zero-Shot-Search"><a href="#Selecting-which-Dense-Retriever-to-use-for-Zero-Shot-Search" class="headerlink" title="Selecting which Dense Retriever to use for Zero-Shot Search"></a>Selecting which Dense Retriever to use for Zero-Shot Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09403">http://arxiv.org/abs/2309.09403</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ekaterina Khramtsova, Shengyao Zhuang, Mahsa Baktashmotlagh, Xi Wang, Guido Zuccon</li>
<li>for: 本研究 targets the problem of choosing the most suitable dense retrieval model for searching on a new collection without any labeled data, i.e. in a zero-shot setting.</li>
<li>methods: 作者们使用了 Computer Vision 和机器学习领域的最新方法来评估无监督性能，但这些方法不适用于选择高性能的精度检索器在 zero-shot 设置下。</li>
<li>results: 经验表明，现有的方法不能有效地选择精度检索器在 zero-shot 设置下，这是一个重要的新问题，解决这个问题可以促进精度检索的普遍应用。Note: The paper is written in English, so the Simplified Chinese translation may not be exact.<details>
<summary>Abstract</summary>
We propose the new problem of choosing which dense retrieval model to use when searching on a new collection for which no labels are available, i.e. in a zero-shot setting. Many dense retrieval models are readily available. Each model however is characterized by very differing search effectiveness -- not just on the test portion of the datasets in which the dense representations have been learned but, importantly, also across different datasets for which data was not used to learn the dense representations. This is because dense retrievers typically require training on a large amount of labeled data to achieve satisfactory search effectiveness in a specific dataset or domain. Moreover, effectiveness gains obtained by dense retrievers on datasets for which they are able to observe labels during training, do not necessarily generalise to datasets that have not been observed during training. This is however a hard problem: through empirical experimentation we show that methods inspired by recent work in unsupervised performance evaluation with the presence of domain shift in the area of computer vision and machine learning are not effective for choosing highly performing dense retrievers in our setup. The availability of reliable methods for the selection of dense retrieval models in zero-shot settings that do not require the collection of labels for evaluation would allow to streamline the widespread adoption of dense retrieval. This is therefore an important new problem we believe the information retrieval community should consider. Implementation of methods, along with raw result files and analysis scripts are made publicly available at https://www.github.com/anonymized.
</details>
<details>
<summary>摘要</summary>
我们提出了一个新的问题，即在搜寻新集合时选择哪个紧密搜寻模型，即零批评设定下的问题。许多紧密搜寻模型都是可用的。然而，每个模型都具有不同的搜寻效能，不仅在测试集上，而且也在不同的集合上。这是因为紧密搜寻模型通常需要训练大量的标签数据来在特定集合或领域中 достиungeatisfactory的搜寻效能。此外，在测试集上获得的效能提升不一定能够应用到没有用于训练的集合上。这是一个困难的问题，我们通过实验证明，运用最近在computer vision和机器学习领域的无supervised performance evaluation方法不能够选择高效的紧密搜寻模型。如果有可靠的方法可以在零批评设定下选择紧密搜寻模型，而不需要训练集的标签评估，这将帮助快速推广紧密搜寻。因此，我们认为信息搜寻社群应该考虑这个问题。我们在https://www.github.com/anonymized上公开了实现和原始档案，以及分析脚本。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/18/cs.AI_2023_09_18/" data-id="closbrokj00470g88bufe7ich" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_09_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/18/cs.CL_2023_09_18/" class="article-date">
  <time datetime="2023-09-18T11:00:00.000Z" itemprop="datePublished">2023-09-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/18/cs.CL_2023_09_18/">cs.CL - 2023-09-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Few-Shot-Adaptation-for-Parsing-Contextual-Utterances-with-LLMs"><a href="#Few-Shot-Adaptation-for-Parsing-Contextual-Utterances-with-LLMs" class="headerlink" title="Few-Shot Adaptation for Parsing Contextual Utterances with LLMs"></a>Few-Shot Adaptation for Parsing Contextual Utterances with LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10168">http://arxiv.org/abs/2309.10168</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/few_shot_adaptation_for_parsing_contextual_utterances_with_llms">https://github.com/microsoft/few_shot_adaptation_for_parsing_contextual_utterances_with_llms</a></li>
<li>paper_authors: Kevin Lin, Patrick Xia, Hao Fang</li>
<li>for: 这个论文主要探讨了基于大语言模型（LLM）的语义解析器在实际场景中如何处理上下文语言。</li>
<li>methods: 论文提出了四种主要方法来处理上下文语言，即 Parse-with-Utterance-History、Parse-with-Reference-Program、Parse-then-Resolve 和 Rewrite-then-Parse。</li>
<li>results: 实验表明，使用 Rewrite-then-Parse 方法可以在考虑解析精度、注释成本和错误类型的情况下取得最佳效果。<details>
<summary>Abstract</summary>
We evaluate the ability of semantic parsers based on large language models (LLMs) to handle contextual utterances. In real-world settings, there typically exists only a limited number of annotated contextual utterances due to annotation cost, resulting in an imbalance compared to non-contextual utterances. Therefore, parsers must adapt to contextual utterances with a few training examples. We examine four major paradigms for doing so in conversational semantic parsing i.e., Parse-with-Utterance-History, Parse-with-Reference-Program, Parse-then-Resolve, and Rewrite-then-Parse. To facilitate such cross-paradigm comparisons, we construct SMCalFlow-EventQueries, a subset of contextual examples from SMCalFlow with additional annotations. Experiments with in-context learning and fine-tuning suggest that Rewrite-then-Parse is the most promising paradigm when holistically considering parsing accuracy, annotation cost, and error types.
</details>
<details>
<summary>摘要</summary>
我们评估基于大语言模型（LLM）的semantic parser在处理上下文性语言时的能力。在实际场景中，通常只有有限的上下文性语言标注数据，因此训练数据的偏度较大，需要 parser 适应上下文性语言的少量标注。我们检查了四种主要的方法来实现这一点，即在 conversational semantic parsing 中使用Parse-with-Utterance-History、Parse-with-Reference-Program、Parse-then-Resolve和Rewrite-then-Parse等四种方法。为便于这些跨 парадиг的比较，我们构建了SMCalFlow-EventQueries，一 subset of 上下文性示例从 SMCalFlow 中，并添加了更多的标注。实验表明，使用 rewrite-then-parse 方法可以最大化 parsing 准确率、标注成本和错误类型。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Catastrophic-Forgetting-in-Language-Models-via-Implicit-Inference"><a href="#Understanding-Catastrophic-Forgetting-in-Language-Models-via-Implicit-Inference" class="headerlink" title="Understanding Catastrophic Forgetting in Language Models via Implicit Inference"></a>Understanding Catastrophic Forgetting in Language Models via Implicit Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10105">http://arxiv.org/abs/2309.10105</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kothasuhas/understanding-forgetting">https://github.com/kothasuhas/understanding-forgetting</a></li>
<li>paper_authors: Suhas Kotha, Jacob Mitchell Springer, Aditi Raghunathan</li>
<li>for: 本研究旨在 investigating the effects of fine-tuning on language models’ performance on tasks outside the fine-tuning distribution.</li>
<li>methods: 研究者采用了 instruction-tuning 和 reinforcement learning from human feedback 等方法进行 fine-tuning，并使用了 conjugate prompting 来测试 Language Models 的能力。</li>
<li>results: 研究发现， improving performance on tasks within the fine-tuning data distribution 会导致模型在其他任务上表现下降，特别是与 fine-tuning 数据分布最相似的任务。此外，研究者发现可以通过 conjugate prompting 来系统地回归 Language Models 的预训练能力。<details>
<summary>Abstract</summary>
Fine-tuning (via methods such as instruction-tuning or reinforcement learning from human feedback) is a crucial step in training language models to robustly carry out tasks of interest. However, we lack a systematic understanding of the effects of fine-tuning, particularly on tasks outside the narrow fine-tuning distribution. In a simplified scenario, we demonstrate that improving performance on tasks within the fine-tuning data distribution comes at the expense of suppressing model capabilities on other tasks. This degradation is especially pronounced for tasks "closest" to the fine-tuning distribution. We hypothesize that language models implicitly infer the task of the prompt corresponds, and the fine-tuning process predominantly skews this task inference towards tasks in the fine-tuning distribution. To test this hypothesis, we propose Conjugate Prompting to see if we can recover pretrained capabilities. Conjugate prompting artificially makes the task look farther from the fine-tuning distribution while requiring the same capability. We find that conjugate prompting systematically recovers some of the pretraining capabilities on our synthetic setup. We then apply conjugate prompting to real-world LLMs using the observation that fine-tuning distributions are typically heavily skewed towards English. We find that simply translating the prompts to different languages can cause the fine-tuned models to respond like their pretrained counterparts instead. This allows us to recover the in-context learning abilities lost via instruction tuning, and more concerningly, to recover harmful content generation suppressed by safety fine-tuning in chatbots like ChatGPT.
</details>
<details>
<summary>摘要</summary>
精度调整（如 instrucion-tuning 或人类反馈学习）是训练语言模型完成任务的关键步骤。然而，我们缺乏对精度调整的系统性理解，特别是在外部精度调整分布之外的任务上。在简化的场景中，我们发现，通过提高 task 内的性能，模型在其他任务上的表现会受到抑制。这种干扰特别明显于 task 与 fine-tuning 数据分布之间最相似的任务上。我们假设语言模型会隐式地推理出提示中的任务类型，而 fine-tuning 过程会主要偏向于 fine-tuning 数据分布中的任务类型。为了测试这一假设，我们提出了 conjugate prompting。 conjugate prompting 通过人工地使任务看起来更加远离 fine-tuning 数据分布，并且需要同样的能力。我们发现 conjugate prompting 系统地恢复了一些预训练能力。然后，我们应用 conjugate prompting 于实际世界的 LLMs，基于观察， fine-tuning 分布通常倾斜到英语。我们发现，只需将提示翻译成不同语言，就能让 fine-tuned 模型回归到预训练模型的状态。这允许我们恢复在 instruction tuning 中失去的 Context Learning 能力，以及在 chatbots 中被安全 fine-tuning 抑制的危险内容生成能力。
</details></li>
</ul>
<hr>
<h2 id="Hierarchy-Builder-Organizing-Textual-Spans-into-a-Hierarchy-to-Facilitate-Navigation"><a href="#Hierarchy-Builder-Organizing-Textual-Spans-into-a-Hierarchy-to-Facilitate-Navigation" class="headerlink" title="Hierarchy Builder: Organizing Textual Spans into a Hierarchy to Facilitate Navigation"></a>Hierarchy Builder: Organizing Textual Spans into a Hierarchy to Facilitate Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10057">http://arxiv.org/abs/2309.10057</a></li>
<li>repo_url: None</li>
<li>paper_authors: Itay Yair, Hillel Taub-Tabib, Yoav Goldberg</li>
<li>for: 本研究旨在提供一种方法，以便在探索 Setting 中，用户可以快速获得各种相关信息的概述，同时还能深入探究一些具体的方面。</li>
<li>methods: 本研究使用了一种组合分组和层次结构生成的方法，将相似的项集成到一起，并将剩下的项排序成一个可导航的 DAG 结构。</li>
<li>results: 本研究应用于医疗信息抽取，可以帮助用户快速获得医疗信息的概述，并且可以深入探究具体的方面。<details>
<summary>Abstract</summary>
Information extraction systems often produce hundreds to thousands of strings on a specific topic. We present a method that facilitates better consumption of these strings, in an exploratory setting in which a user wants to both get a broad overview of what's available, and a chance to dive deeper on some aspects. The system works by grouping similar items together and arranging the remaining items into a hierarchical navigable DAG structure. We apply the method to medical information extraction.
</details>
<details>
<summary>摘要</summary>
将文本翻译成简化中文：信息提取系统经常生成大量与特定主题相关的字符串。我们提出了一种方法，可以帮助用户更好地消化这些字符串，在探索性的 Setting 中，用户希望同时获得概括和深入了解一些方面。系统通过将相似的项集成一起，并将剩下的项组织成嵌入式的DAG结构，以便用户可以方便地浏览和探索。我们在医疗信息提取中应用了这种方法。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Foundation-Models-From-Specialists-to-General-Purpose-Assistants"><a href="#Multimodal-Foundation-Models-From-Specialists-to-General-Purpose-Assistants" class="headerlink" title="Multimodal Foundation Models: From Specialists to General-Purpose Assistants"></a>Multimodal Foundation Models: From Specialists to General-Purpose Assistants</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10020">http://arxiv.org/abs/2309.10020</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, Jianfeng Gao</li>
<li>for: 这篇论文旨在概述多模态基础模型的分类和演化，强调从专家模型转化为通用助手。</li>
<li>methods: 论文涵盖了五个核心研究领域，分为两类：一是已有的研究领域，包括两个主题：学习视觉基础模型 для视觉理解和文本到图生成；二是现代探索性研究领域，包括三个主题：基于大语言模型的统一视觉模型、端到端训练多模态语言模型、将多模态工具与语言模型串联。</li>
<li>results: 论文的目标受众是计算机视觉和视觉语言多模态研究人员，包括研究生、博士生和专业人士，他们想要了解多模态基础模型的基础知识和最新进展。<details>
<summary>Abstract</summary>
This paper presents a comprehensive survey of the taxonomy and evolution of multimodal foundation models that demonstrate vision and vision-language capabilities, focusing on the transition from specialist models to general-purpose assistants. The research landscape encompasses five core topics, categorized into two classes. (i) We start with a survey of well-established research areas: multimodal foundation models pre-trained for specific purposes, including two topics -- methods of learning vision backbones for visual understanding and text-to-image generation. (ii) Then, we present recent advances in exploratory, open research areas: multimodal foundation models that aim to play the role of general-purpose assistants, including three topics -- unified vision models inspired by large language models (LLMs), end-to-end training of multimodal LLMs, and chaining multimodal tools with LLMs. The target audiences of the paper are researchers, graduate students, and professionals in computer vision and vision-language multimodal communities who are eager to learn the basics and recent advances in multimodal foundation models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Well-established research areas:	* Methods of learning vision backbones for visual understanding	* Text-to-image generation2. Recent advances in exploratory, open research areas:	* Unified vision models inspired by large language models (LLMs)	* End-to-end training of multimodal LLMs	* Chaining multimodal tools with LLMsThe target audience for this paper includes researchers, graduate students, and professionals in the computer vision and vision-language multimodal communities who are interested in learning about the basics and recent advances in multimodal foundation models.</details></li>
</ol>
<hr>
<h2 id="An-Empirical-Study-of-Scaling-Instruct-Tuned-Large-Multimodal-Models"><a href="#An-Empirical-Study-of-Scaling-Instruct-Tuned-Large-Multimodal-Models" class="headerlink" title="An Empirical Study of Scaling Instruct-Tuned Large Multimodal Models"></a>An Empirical Study of Scaling Instruct-Tuned Large Multimodal Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09958">http://arxiv.org/abs/2309.09958</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haotian-liu/LLaVA">https://github.com/haotian-liu/LLaVA</a></li>
<li>paper_authors: Yadong Lu, Chunyuan Li, Haotian Liu, Jianwei Yang, Jianfeng Gao, Yelong Shen</li>
<li>for: 这个论文是为了研究开源大型多模态模型（LLaVA和MiniGPT-4）的可视 instrucion 调教进行 empirical 研究，以便为未来的研究提供更强的基准。</li>
<li>methods: 这个论文使用了扩大 LLVA 的参数量至 33B 和 65B&#x2F;70B，并研究了LoRA&#x2F;QLoRA 等参数效率训练方法的影响。</li>
<li>results: 研究发现，扩大 LMM 的表现和语言能力有显著提升，LoRA&#x2F;QLoRA 的训练方法与全模型精度调教的性能相当，而高像素分辨率和混合多Modal-语言数据也有助于提高 LMM 表现。<details>
<summary>Abstract</summary>
Visual instruction tuning has recently shown encouraging progress with open-source large multimodal models (LMM) such as LLaVA and MiniGPT-4. However, most existing studies of open-source LMM are performed using models with 13B parameters or smaller. In this paper we present an empirical study of scaling LLaVA up to 33B and 65B/70B, and share our findings from our explorations in image resolution, data mixing and parameter-efficient training methods such as LoRA/QLoRA. These are evaluated by their impact on the multi-modal and language capabilities when completing real-world tasks in the wild.   We find that scaling LMM consistently enhances model performance and improves language capabilities, and performance of LoRA/QLoRA tuning of LMM are comparable to the performance of full-model fine-tuning. Additionally, the study highlights the importance of higher image resolutions and mixing multimodal-language data to improve LMM performance, and visual instruction tuning can sometimes improve LMM's pure language capability. We hope that this study makes state-of-the-art LMM research at a larger scale more accessible, thus helping establish stronger baselines for future research. Code and checkpoints will be made public.
</details>
<details>
<summary>摘要</summary>
幻视 instrucion 优化在最近已经取得了鼓舞人心的进步，使用开源大型多模型（LLaVA）和 MiniGPT-4 等模型。然而，大多数现有的开源 LLaVA 模型都是使用 13B 参数或更小的模型进行研究。在这篇论文中，我们将对 LLaVA 模型进行扩展，并在 33B 和 65B/70B 的参数量上进行实验。我们还将分享我们在图像分辨率、数据混合和参数有效训练方法（LoRA/QLoRA）等方面的发现。这些发现对于完成真实世界任务时 LMM 模型的多模态和语言能力的影响进行评估。我们发现，将 LMM 模型扩展可以顺利提高模型性能和语言能力，而 LoRA/QLoRA 的训练方法与全模型精度训练的性能相当。此外，这个研究还证明了更高的图像分辨率和混合多模态语言数据可以提高 LMM 模型性能，而视 instrucion 优化可以帮助 LMM 模型提高纯语言能力。我们希望这篇研究可以让开源 LMM 研究更加可 accessible，从而帮助建立更强的基线 для未来的研究。我们将在线上公开代码和检查点。
</details></li>
</ul>
<hr>
<h2 id="Speaker-attribution-in-German-parliamentary-debates-with-QLoRA-adapted-large-language-models"><a href="#Speaker-attribution-in-German-parliamentary-debates-with-QLoRA-adapted-large-language-models" class="headerlink" title="Speaker attribution in German parliamentary debates with QLoRA-adapted large language models"></a>Speaker attribution in German parliamentary debates with QLoRA-adapted large language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09902">http://arxiv.org/abs/2309.09902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tobias Bornheim, Niklas Grieger, Patrick Gustav Blaneck, Stephan Bialonski</li>
<li>for: 这个论文旨在提高德国议会辩论中的自动发言人分配，以便更好地进行计算文本分析。</li>
<li>methods: 作者使用了大型自然语言模型家族Llama 2，并使用QLoRA的高效训练策略进行细化。</li>
<li>results: 研究表明，使用Llama 2可以实现竞争性的自动发言人分配性能，提供了计算政治 Diskursanalyse 的可能性，以及 semantic role labeling 系统的发展。<details>
<summary>Abstract</summary>
The growing body of political texts opens up new opportunities for rich insights into political dynamics and ideologies but also increases the workload for manual analysis. Automated speaker attribution, which detects who said what to whom in a speech event and is closely related to semantic role labeling, is an important processing step for computational text analysis. We study the potential of the large language model family Llama 2 to automate speaker attribution in German parliamentary debates from 2017-2021. We fine-tune Llama 2 with QLoRA, an efficient training strategy, and observe our approach to achieve competitive performance in the GermEval 2023 Shared Task On Speaker Attribution in German News Articles and Parliamentary Debates. Our results shed light on the capabilities of large language models in automating speaker attribution, revealing a promising avenue for computational analysis of political discourse and the development of semantic role labeling systems.
</details>
<details>
<summary>摘要</summary>
政治文本的增长开创了新的可观察机会，提供了有价值的政治动力和意识形态分析。然而，这也增加了人工分析的劳动负担。自动分配说话人，即在语音事件中识别说话者和他们所说的内容，是计算文本分析中重要的处理步骤。我们研究利用大语言模型家族Llama 2自动化德国议会辩论中的说话人分配，从2017年至2021年。我们使用QLoRA，一种高效的训练策略，微调Llama 2，并观察我们的方法在GermEval 2023共享任务中的竞赛性表现。我们的结果描绘了大语言模型在自动化说话人分配中的能力，揭示了计算政治讨论的可能性和意识形态分析系统的发展。
</details></li>
</ul>
<hr>
<h2 id="Corpus-Synthesis-for-Zero-shot-ASR-domain-Adaptation-using-Large-Language-Models"><a href="#Corpus-Synthesis-for-Zero-shot-ASR-domain-Adaptation-using-Large-Language-Models" class="headerlink" title="Corpus Synthesis for Zero-shot ASR domain Adaptation using Large Language Models"></a>Corpus Synthesis for Zero-shot ASR domain Adaptation using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10707">http://arxiv.org/abs/2309.10707</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hsuan Su, Ting-Yao Hu, Hema Swetha Koppula, Raviteja Vemulapalli, Jen-Hao Rick Chang, Karren Yang, Gautam Varma Mantena, Oncel Tuzel</li>
<li>for: 这篇论文的目的是提出一种新的自动话语识别（ASR）模型适应新目标领域的策略，不需要目标领域的文本或声音数据。</li>
<li>methods: 这篇论文提出了一个新的数据合成管道，使用大型自然语言模型（LLM）生成目标领域的文本库，并使用现有的控制可能性声合成模型生成相应的声音。另外，这篇论文也提出了一个简单 yet 有效的内部 instrucion 微调策略，以增加 LLM 在新领域中的效果。</li>
<li>results: 实验结果显示，提出的方法在 SLURP 数据集上实现了不见天色的话语识别error rate 下降，平均相对词SError rate 下降 $28%$。同时，模型在源领域中的表现也不变。<details>
<summary>Abstract</summary>
While Automatic Speech Recognition (ASR) systems are widely used in many real-world applications, they often do not generalize well to new domains and need to be finetuned on data from these domains. However, target-domain data usually are not readily available in many scenarios. In this paper, we propose a new strategy for adapting ASR models to new target domains without any text or speech from those domains. To accomplish this, we propose a novel data synthesis pipeline that uses a Large Language Model (LLM) to generate a target domain text corpus, and a state-of-the-art controllable speech synthesis model to generate the corresponding speech. We propose a simple yet effective in-context instruction finetuning strategy to increase the effectiveness of LLM in generating text corpora for new domains. Experiments on the SLURP dataset show that the proposed method achieves an average relative word error rate improvement of $28\%$ on unseen target domains without any performance drop in source domains.
</details>
<details>
<summary>摘要</summary>
while automatic speech recognition (ASR) systems are widely used in many real-world applications, they often do not generalize well to new domains and need to be finetuned on data from these domains. however, target-domain data usually are not readily available in many scenarios. in this paper, we propose a new strategy for adapting ASR models to new target domains without any text or speech from those domains. to accomplish this, we propose a novel data synthesis pipeline that uses a large language model (LLM) to generate a target domain text corpus, and a state-of-the-art controllable speech synthesis model to generate the corresponding speech. we propose a simple yet effective in-context instruction finetuning strategy to increase the effectiveness of LLM in generating text corpora for new domains. experiments on the slurp dataset show that the proposed method achieves an average relative word error rate improvement of 28% on unseen target domains without any performance drop in source domains.Here's the translation in Traditional Chinese:而 while automatic speech recognition (ASR) 系统 widely used in many real-world applications, they often do not generalize well to new domains and need to be finetuned on data from these domains. however, target-domain data usually are not readily available in many scenarios. in this paper, we propose a new strategy for adapting ASR models to new target domains without any text or speech from those domains. to accomplish this, we propose a novel data synthesis pipeline that uses a large language model (LLM) to generate a target domain text corpus, and a state-of-the-art controllable speech synthesis model to generate the corresponding speech. we propose a simple yet effective in-context instruction finetuning strategy to increase the effectiveness of LLM in generating text corpora for new domains. experiments on the slurp dataset show that the proposed method achieves an average relative word error rate improvement of 28% on unseen target domains without any performance drop in source domains.
</details></li>
</ul>
<hr>
<h2 id="Not-Enough-Labeled-Data-Just-Add-Semantics-A-Data-Efficient-Method-for-Inferring-Online-Health-Texts"><a href="#Not-Enough-Labeled-Data-Just-Add-Semantics-A-Data-Efficient-Method-for-Inferring-Online-Health-Texts" class="headerlink" title="Not Enough Labeled Data? Just Add Semantics: A Data-Efficient Method for Inferring Online Health Texts"></a>Not Enough Labeled Data? Just Add Semantics: A Data-Efficient Method for Inferring Online Health Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09877">http://arxiv.org/abs/2309.09877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joseph Gatto, Sarah M. Preum</li>
<li>for: 本研究旨在提出一种基于抽象意义表示（AMR）的低资源自然语言处理（NLP）方法，用于解决各种在线健康资源和社交平台上的长文本和复杂语言问题。</li>
<li>methods: 本研究使用AMR图来模型低资源健康NLP任务，通过将文本转化为 semantic graph embeddings，提高预训练语言模型对高复杂文本的理解和推理能力。</li>
<li>results: 研究表明，通过将文本 embeddings 与 semantic graph embeddings 结合使用，可以提高6种低资源健康NLP任务的性能，并且这种方法是任务无关的，可以轻松地与标准文本分类管道结合使用。<details>
<summary>Abstract</summary>
User-generated texts available on the web and social platforms are often long and semantically challenging, making them difficult to annotate. Obtaining human annotation becomes increasingly difficult as problem domains become more specialized. For example, many health NLP problems require domain experts to be a part of the annotation pipeline. Thus, it is crucial that we develop low-resource NLP solutions able to work with this set of limited-data problems. In this study, we employ Abstract Meaning Representation (AMR) graphs as a means to model low-resource Health NLP tasks sourced from various online health resources and communities. AMRs are well suited to model online health texts as they can represent multi-sentence inputs, abstract away from complex terminology, and model long-distance relationships between co-referring tokens. AMRs thus improve the ability of pre-trained language models to reason about high-complexity texts. Our experiments show that we can improve performance on 6 low-resource health NLP tasks by augmenting text embeddings with semantic graph embeddings. Our approach is task agnostic and easy to merge into any standard text classification pipeline. We experimentally validate that AMRs are useful in the modeling of complex texts by analyzing performance through the lens of two textual complexity measures: the Flesch Kincaid Reading Level and Syntactic Complexity. Our error analysis shows that AMR-infused language models perform better on complex texts and generally show less predictive variance in the presence of changing complexity.
</details>
<details>
<summary>摘要</summary>
User-generated文本在网络和社交平台上经常是长度很长，涉猎度很高的，使得其annotate成为越来越困难的。尤其是在专业领域问题上，需要培训专家来参与注释管道。因此，我们需要开发低资源NLPTools，能够处理这些有限数据问题。在这项研究中，我们使用抽象意思表示（AMR）图来模型低资源医疗NLPTasks。AMR图适合模型在线医疗文本，可以表示多句输入，抽象化复杂术语，并模型距离匹配token的长距离关系。AMR图因此提高了预训练语言模型对高复杂文本的理解能力。我们的实验表明，通过将文本嵌入与semantic图嵌入结合在一起，可以提高6个低资源医疗NLPTasks的性能。我们的方法是任务无关的，可以轻松地与标准文本分类管道集成。我们实验证明了AMR图在模型复杂文本方面的用用。我们通过分析性能的方式，包括读取难度和 sintactic complexity，发现AMR-激发的语言模型在复杂文本中表现得更好，并且在文本复杂度发生变化时显示更少的预测异常。
</details></li>
</ul>
<hr>
<h2 id="Instruction-Following-Speech-Recognition"><a href="#Instruction-Following-Speech-Recognition" class="headerlink" title="Instruction-Following Speech Recognition"></a>Instruction-Following Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09843">http://arxiv.org/abs/2309.09843</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abusufyanvu/6S191_MIT_DeepLearning">https://github.com/abusufyanvu/6S191_MIT_DeepLearning</a></li>
<li>paper_authors: Cheng-I Jeff Lai, Zhiyun Lu, Liangliang Cao, Ruoming Pang</li>
<li>for: 这研究旨在探索大自然语言模型在语音处理中的理解和“理智”能力。</li>
<li>methods: 研究者采用了听写抄写模型，让模型理解和执行多种自由文本指令。</li>
<li>results: 研究发现，没有需要大自然语言模型或预训练的语音模块，模型可以根据指令来选择ively transcribe部分语音，提供额外的隐私和安全层。<details>
<summary>Abstract</summary>
Conventional end-to-end Automatic Speech Recognition (ASR) models primarily focus on exact transcription tasks, lacking flexibility for nuanced user interactions. With the advent of Large Language Models (LLMs) in speech processing, more organic, text-prompt-based interactions have become possible. However, the mechanisms behind these models' speech understanding and "reasoning" capabilities remain underexplored. To study this question from the data perspective, we introduce instruction-following speech recognition, training a Listen-Attend-Spell model to understand and execute a diverse set of free-form text instructions. This enables a multitude of speech recognition tasks -- ranging from transcript manipulation to summarization -- without relying on predefined command sets. Remarkably, our model, trained from scratch on Librispeech, interprets and executes simple instructions without requiring LLMs or pre-trained speech modules. It also offers selective transcription options based on instructions like "transcribe first half and then turn off listening," providing an additional layer of privacy and safety compared to existing LLMs. Our findings highlight the significant potential of instruction-following training to advance speech foundation models.
</details>
<details>
<summary>摘要</summary>
（传统的终端到终端自动语音识别（ASR）模型主要关注准确的转录任务，缺乏用户互动的灵活性。大语言模型（LLMs）在语音处理中出现后，可以实现更加自然的文本提示基于互动。然而，这些模型在语音理解和“理解”能力的机制尚未得到足够的探讨。为了从数据角度来研究这个问题，我们介绍了听写执行语音识别，通过训练一个听写执行模型来理解和执行多种自由形式文本指令。这使得许多语音识别任务——从转录修改到摘要——无需靠背景知识库或预训练语音模块。另外，我们的模型可以根据指令来选择转录内容，如“转录首半并then关闭听写”，提供了额外的隐私和安全层次。我们的发现表明了指令执行训练可以推动语音基础模型的进步。）
</details></li>
</ul>
<hr>
<h2 id="HypR-A-comprehensive-study-for-ASR-hypothesis-revising-with-a-reference-corpus"><a href="#HypR-A-comprehensive-study-for-ASR-hypothesis-revising-with-a-reference-corpus" class="headerlink" title="HypR: A comprehensive study for ASR hypothesis revising with a reference corpus"></a>HypR: A comprehensive study for ASR hypothesis revising with a reference corpus</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09838">http://arxiv.org/abs/2309.09838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi-Wei Wang, Ke-Han Lu, Kuan-Yu Chen</li>
<li>for: 提高自动语音识别（ASR）性能， revising recognition results 是一种轻量级 yet efficient 的方法。</li>
<li>methods: 研究使用 N-best reranking 方法和 error correction 模型来重新评估 ASR 结果。</li>
<li>results: 发布了 ASR 假设修正（HypR）数据集，包括 AISHELL-1、TED-LIUM 2 和 LibriSpeech 等常用 corpora，并提供每个语音片断50个认知假设。还实现了多种经典和代表性的方法，展示了最新的研究进展。<details>
<summary>Abstract</summary>
With the development of deep learning, automatic speech recognition (ASR) has made significant progress. To further enhance the performance, revising recognition results is one of the lightweight but efficient manners. Various methods can be roughly classified into N-best reranking methods and error correction models. The former aims to select the hypothesis with the lowest error rate from a set of candidates generated by ASR for a given input speech. The latter focuses on detecting recognition errors in a given hypothesis and correcting these errors to obtain an enhanced result. However, we observe that these studies are hardly comparable to each other as they are usually evaluated on different corpora, paired with different ASR models, and even use different datasets to train the models. Accordingly, we first concentrate on releasing an ASR hypothesis revising (HypR) dataset in this study. HypR contains several commonly used corpora (AISHELL-1, TED-LIUM 2, and LibriSpeech) and provides 50 recognition hypotheses for each speech utterance. The checkpoint models of the ASR are also published. In addition, we implement and compare several classic and representative methods, showing the recent research progress in revising speech recognition results. We hope the publicly available HypR dataset can become a reference benchmark for subsequent research and promote the school of research to an advanced level.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:随着深度学习的发展，自动语音识别（ASR）的进步也在不断。为了进一步提高性能，重新评估结果是一种轻量级 yet efficient的方式。不同的方法可以大致分为N-best重新排序方法和错误修复模型。前者targets selecting the lowest error rate hypothesis from a set of candidates generated by ASR for a given input speech。后者则是关注检测recognition errors in a given hypothesis and correcting these errors to obtain an enhanced result。然而，我们注意到这些研究通常不能相互比较，因为它们通常在不同的corpus上进行评估，与不同的ASR模型对应，甚至使用不同的数据集来训练模型。因此，我们首先集中精力于发布ASR假设修复（HypR）数据集。HypR包含了一些常用的corpus（AISHELL-1、TED-LIUM 2和LibriSpeech），并为每个语音词提供50个识别假设。ASR的checkpoint模型也同时发布。此外，我们实现并比较了一些经典和代表性的方法，展示了最近的研究进展。我们希望公开的HypR数据集可以成为后续研究的参考准 marker，并推动这一领域的研究进入更高水平。
</details></li>
</ul>
<hr>
<h2 id="AMuRD-Annotated-Multilingual-Receipts-Dataset-for-Cross-lingual-Key-Information-Extraction-and-Classification"><a href="#AMuRD-Annotated-Multilingual-Receipts-Dataset-for-Cross-lingual-Key-Information-Extraction-and-Classification" class="headerlink" title="AMuRD: Annotated Multilingual Receipts Dataset for Cross-lingual Key Information Extraction and Classification"></a>AMuRD: Annotated Multilingual Receipts Dataset for Cross-lingual Key Information Extraction and Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09800">http://arxiv.org/abs/2309.09800</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/update-for-integrated-business-ai/amurd">https://github.com/update-for-integrated-business-ai/amurd</a></li>
<li>paper_authors: Abdelrahman Abdallah, Mahmoud Abdalla, Mohamed Elkasaby, Yasser Elbendary, Adam Jatowt</li>
<li>for: 本研究は receipt 资讯抽取の问题に关する、新的多语言数据集的开发而建立的。</li>
<li>methods: 本研究使用的方法是 InstructLLaMA 方法，它可以解决资讯抽取和项目分类中的主要挑战。</li>
<li>results: 本研究获得的结果是 F1 分数为 0.76 和准确率为 0.68，这表明 InstructLLaMA 方法可以实现高精度的资讯抽取和项目分类。<details>
<summary>Abstract</summary>
Key information extraction involves recognizing and extracting text from scanned receipts, enabling retrieval of essential content, and organizing it into structured documents. This paper presents a novel multilingual dataset for receipt extraction, addressing key challenges in information extraction and item classification. The dataset comprises $47,720$ samples, including annotations for item names, attributes like (price, brand, etc.), and classification into $44$ product categories. We introduce the InstructLLaMA approach, achieving an F1 score of $0.76$ and an accuracy of $0.68$ for key information extraction and item classification. We provide code, datasets, and checkpoints.\footnote{\url{https://github.com/Update-For-Integrated-Business-AI/AMuRD}.
</details>
<details>
<summary>摘要</summary>
“这份研究将提出一个新的多语言 dataset，用于收据EXTRACTION，并解决资讯EXTRACTION和项目分类中的主要挑战。该dataset包含47,720个样本，包括项目名称、特征（价格、品牌等）的标注，以及44个产品类别的分类。我们提出了InstructLLaMA方法，实现了关键资讯EXTRACTION和项目分类中的 F1 分数为0.76，和精度为0.68。我们提供了代码、dataset和检查点。”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Watch-the-Speakers-A-Hybrid-Continuous-Attribution-Network-for-Emotion-Recognition-in-Conversation-With-Emotion-Disentanglement"><a href="#Watch-the-Speakers-A-Hybrid-Continuous-Attribution-Network-for-Emotion-Recognition-in-Conversation-With-Emotion-Disentanglement" class="headerlink" title="Watch the Speakers: A Hybrid Continuous Attribution Network for Emotion Recognition in Conversation With Emotion Disentanglement"></a>Watch the Speakers: A Hybrid Continuous Attribution Network for Emotion Recognition in Conversation With Emotion Disentanglement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09799">http://arxiv.org/abs/2309.09799</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shanglin Lei, Xiaoping Wang, Guanting Dong, Jiang Li, Yingjian Liu</li>
<li>for: 这个研究是为了提高对话中的情感识别能力，并且实现在不同情感场景下的通用性。</li>
<li>methods: 这个研究使用了一个混合式连续特征网络（HCAN），包括一个混合的回传和注意力模组，以模型全局情感连续性。另外，一个新的情感对应编码（EAE）是提出来模型每个语言的内部和外部情感对应。</li>
<li>results: 这个研究在三个数据集上取得了顶尖性能，证明了我们的方法的优越性。另外，一系列的比较实验和抽象研究还进行了三个Benchmark上，以支持每个模员的有效性。<details>
<summary>Abstract</summary>
Emotion Recognition in Conversation (ERC) has attracted widespread attention in the natural language processing field due to its enormous potential for practical applications. Existing ERC methods face challenges in achieving generalization to diverse scenarios due to insufficient modeling of context, ambiguous capture of dialogue relationships and overfitting in speaker modeling. In this work, we present a Hybrid Continuous Attributive Network (HCAN) to address these issues in the perspective of emotional continuation and emotional attribution. Specifically, HCAN adopts a hybrid recurrent and attention-based module to model global emotion continuity. Then a novel Emotional Attribution Encoding (EAE) is proposed to model intra- and inter-emotional attribution for each utterance. Moreover, aiming to enhance the robustness of the model in speaker modeling and improve its performance in different scenarios, A comprehensive loss function emotional cognitive loss $\mathcal{L}_{\rm EC}$ is proposed to alleviate emotional drift and overcome the overfitting of the model to speaker modeling. Our model achieves state-of-the-art performance on three datasets, demonstrating the superiority of our work. Another extensive comparative experiments and ablation studies on three benchmarks are conducted to provided evidence to support the efficacy of each module. Further exploration of generalization ability experiments shows the plug-and-play nature of the EAE module in our method.
</details>
<details>
<summary>摘要</summary>
受欢迎的情感认知在对话中（ERC）在自然语言处理领域中受到广泛的关注，因为它在实际应用中具有巨大的潜力。现有的ERC方法在不同的场景下进行泛化是一个大的挑战，主要是因为不充分考虑对话上下文，不充分捕捉对话关系，以及模型过拟合。在这种情况下，我们提出了一种混合连续属性网络（HCAN），以解决这些问题。具体来说，HCAN采用混合回卷和注意力基元来模型全局情感连续性。然后，我们提出了一种新的情感归属编码（EAE），以模型每个语音句中的内部和间接情感归属。此外，为了增强模型在 speaker 模型中的稳定性和不同场景下的性能，我们提出了一种完整的情感认知loss函数 $\mathcal{L}_{\rm EC}$，以避免情感漂移和模型过拟合。我们的模型在三个数据集上达到了领先的性能，证明了我们的工作的优越性。此外，我们还进行了广泛的比较实验和简要的减少实验，以提供对每个模块的证明。进一步的普适性实验表明了EAE模块的插入性。
</details></li>
</ul>
<hr>
<h2 id="The-ParlaSent-multilingual-training-dataset-for-sentiment-identification-in-parliamentary-proceedings"><a href="#The-ParlaSent-multilingual-training-dataset-for-sentiment-identification-in-parliamentary-proceedings" class="headerlink" title="The ParlaSent multilingual training dataset for sentiment identification in parliamentary proceedings"></a>The ParlaSent multilingual training dataset for sentiment identification in parliamentary proceedings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09783">http://arxiv.org/abs/2309.09783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michal Mochtak, Peter Rupnik, Nikola Ljubešić</li>
<li>for: 这篇论文主要用于研究政治决策中的情感因素，以及如何系统地研究和测量这些情感。</li>
<li>methods: 论文使用了一个新的情感注释句子数据集，并在这些句子上进行了一系列的实验，旨在训练一个可靠的情感分类器。此外，论文还介绍了首次针对政治科学应用的域specific LLM，并在这些数据上进行了额外预训练。</li>
<li>results: 实验表明，额外预训练 LLM 在域specific 任务上可以显著提高模型的下游性能，并且多语言模型在未看过语言上表现非常好。此外，论文还证明了在其他语言上额外收集数据可以大幅提高目标议会的结果。<details>
<summary>Abstract</summary>
Sentiments inherently drive politics. How we receive and process information plays an essential role in political decision-making, shaping our judgment with strategic consequences both on the level of legislators and the masses. If sentiment plays such an important role in politics, how can we study and measure it systematically? The paper presents a new dataset of sentiment-annotated sentences, which are used in a series of experiments focused on training a robust sentiment classifier for parliamentary proceedings. The paper also introduces the first domain-specific LLM for political science applications additionally pre-trained on 1.72 billion domain-specific words from proceedings of 27 European parliaments. We present experiments demonstrating how the additional pre-training of LLM on parliamentary data can significantly improve the model downstream performance on the domain-specific tasks, in our case, sentiment detection in parliamentary proceedings. We further show that multilingual models perform very well on unseen languages and that additional data from other languages significantly improves the target parliament's results. The paper makes an important contribution to multiple domains of social sciences and bridges them with computer science and computational linguistics. Lastly, it sets up a more robust approach to sentiment analysis of political texts in general, which allows scholars to study political sentiment from a comparative perspective using standardized tools and techniques.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Facilitating-NSFW-Text-Detection-in-Open-Domain-Dialogue-Systems-via-Knowledge-Distillation"><a href="#Facilitating-NSFW-Text-Detection-in-Open-Domain-Dialogue-Systems-via-Knowledge-Distillation" class="headerlink" title="Facilitating NSFW Text Detection in Open-Domain Dialogue Systems via Knowledge Distillation"></a>Facilitating NSFW Text Detection in Open-Domain Dialogue Systems via Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09749">http://arxiv.org/abs/2309.09749</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qiuhuachuan/CensorChat">https://github.com/qiuhuachuan/CensorChat</a></li>
<li>paper_authors: Huachuan Qiu, Shuai Zhang, Hongliang He, Anqi Li, Zhenzhong Lan</li>
<li>for: 本研究旨在提高对对话系统中NSFW语言检测的能力，以保障用户的安全和 благополучие在数字对话中。</li>
<li>methods: 本研究使用了知识储存技术，包括GPT-4和ChatGPT，来建立NSFW对话检测 dataset。通过人工标注和自我批判策略，该dataset提供了一种可靠的NSFW语言检测方法。</li>
<li>results: 研究表明，通过使用BERT模型进行文本分类，可以准确地检测NSFW语言在对话中。此外，该方法还能够考虑到用户的自由表达权，从而提高对话系统的安全性和可靠性。<details>
<summary>Abstract</summary>
NSFW (Not Safe for Work) content, in the context of a dialogue, can have severe side effects on users in open-domain dialogue systems. However, research on detecting NSFW language, especially sexually explicit content, within a dialogue context has significantly lagged behind. To address this issue, we introduce CensorChat, a dialogue monitoring dataset aimed at NSFW dialogue detection. Leveraging knowledge distillation techniques involving GPT-4 and ChatGPT, this dataset offers a cost-effective means of constructing NSFW content detectors. The process entails collecting real-life human-machine interaction data and breaking it down into single utterances and single-turn dialogues, with the chatbot delivering the final utterance. ChatGPT is employed to annotate unlabeled data, serving as a training set. Rationale validation and test sets are constructed using ChatGPT and GPT-4 as annotators, with a self-criticism strategy for resolving discrepancies in labeling. A BERT model is fine-tuned as a text classifier on pseudo-labeled data, and its performance is assessed. The study emphasizes the importance of AI systems prioritizing user safety and well-being in digital conversations while respecting freedom of expression. The proposed approach not only advances NSFW content detection but also aligns with evolving user protection needs in AI-driven dialogues.
</details>
<details>
<summary>摘要</summary>
The process involves collecting real-life human-machine interaction data and breaking it down into individual utterances and single-turn dialogues, with the chatbot delivering the final utterance. ChatGPT is used to annotate unlabeled data, serving as a training set. Rationale validation and test sets are constructed using ChatGPT and GPT-4 as annotators, with a self-criticism strategy for resolving discrepancies in labeling. A BERT model is fine-tuned as a text classifier on pseudo-labeled data, and its performance is evaluated.The study highlights the importance of AI systems prioritizing user safety and well-being in digital conversations while respecting freedom of expression. The proposed approach not only advances NSFW content detection but also aligns with evolving user protection needs in AI-driven dialogues.
</details></li>
</ul>
<hr>
<h2 id="When-Large-Language-Models-Meet-Citation-A-Survey"><a href="#When-Large-Language-Models-Meet-Citation-A-Survey" class="headerlink" title="When Large Language Models Meet Citation: A Survey"></a>When Large Language Models Meet Citation: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09727">http://arxiv.org/abs/2309.09727</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Zhang, Yufei Wang, Kai Wang, Quan Z. Sheng, Lina Yao, Adnan Mahmood, Wei Emma Zhang, Rongying Zhao</li>
<li>For: This paper reviews the use of large language models (LLMs) for in-text citation analysis tasks and how citation linkage knowledge can be used to improve text representations of LLMs.* Methods: The paper discusses the application of LLMs for citation classification, citation-based summarization, and citation recommendation, as well as the use of citation prediction, network structure information, and inter-document relationship to improve text representations.* Results: The paper provides a preliminary review of the mutually beneficial relationship between LLMs and citation analysis, and highlights potential promising avenues for further investigation.Here’s the information in Simplified Chinese text:* For: 这篇论文探讨了大语言模型（LLMs）在文本中引用分析任务中的应用，以及如何使用引用链接知识来提高 LLMs 的文本表示。* Methods: 论文讨论了 LLMs 的应用包括引用分类、基于引用的摘要和引用推荐，以及使用引用预测、网络结构信息和间文件关系来提高 LLMs 的文本表示。* Results: 论文提供了一项初步的评估，探讨了 LLMs 和引用分析之间的互利关系，并提出了可能的进一步研究方向。<details>
<summary>Abstract</summary>
Citations in scholarly work serve the essential purpose of acknowledging and crediting the original sources of knowledge that have been incorporated or referenced. Depending on their surrounding textual context, these citations are used for different motivations and purposes. Large Language Models (LLMs) could be helpful in capturing these fine-grained citation information via the corresponding textual context, thereby enabling a better understanding towards the literature. Furthermore, these citations also establish connections among scientific papers, providing high-quality inter-document relationships and human-constructed knowledge. Such information could be incorporated into LLMs pre-training and improve the text representation in LLMs. Therefore, in this paper, we offer a preliminary review of the mutually beneficial relationship between LLMs and citation analysis. Specifically, we review the application of LLMs for in-text citation analysis tasks, including citation classification, citation-based summarization, and citation recommendation. We then summarize the research pertinent to leveraging citation linkage knowledge to improve text representations of LLMs via citation prediction, network structure information, and inter-document relationship. We finally provide an overview of these contemporary methods and put forth potential promising avenues in combining LLMs and citation analysis for further investigation.
</details>
<details>
<summary>摘要</summary>
文献引用在学术作品中服务于重要的目的是承认和归因原始知识的 incorporation 和参考。根据文本上下文，这些引用有不同的动机和目的。大语言模型（LLM）可以通过对应的文本上下文捕捉细腻的引用信息，从而更好地理解文献。此外，这些引用还建立了科学论文之间的连接，提供高质量的交叉文献关系和人类建构的知识。这些信息可以在 LLM 的预训练中包含，以提高文本表示。因此，在这篇论文中，我们提供了一个初步的文献综述，探讨 LLM 和引用分析之间的互利关系。 Specifically, we review the application of LLMs for in-text citation analysis tasks, including citation classification, citation-based summarization, and citation recommendation. We then summarize the research pertinent to leveraging citation linkage knowledge to improve text representations of LLMs via citation prediction, network structure information, and inter-document relationship. We finally provide an overview of these contemporary methods and put forth potential promising avenues in combining LLMs and citation analysis for further investigation.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Dealing-with-negative-samples-with-multi-task-learning-on-span-based-joint-entity-relation-extraction"><a href="#Dealing-with-negative-samples-with-multi-task-learning-on-span-based-joint-entity-relation-extraction" class="headerlink" title="Dealing with negative samples with multi-task learning on span-based joint entity-relation extraction"></a>Dealing with negative samples with multi-task learning on span-based joint entity-relation extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09713">http://arxiv.org/abs/2309.09713</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenguang Xue, Jiamin Lu<br>for:  span-based joint extraction modelsmethods:  multitask learning, intersection over union (IoU) concept, entity Logitsresults:  mitigating adverse effects of excessive negative samples, commendable F1 scores of 73.61%, 53.72%, and 83.72% on three widely employed public datasets (CoNLL04, SciERC, and ADE)<details>
<summary>Abstract</summary>
Recent span-based joint extraction models have demonstrated significant advantages in both entity recognition and relation extraction. These models treat text spans as candidate entities, and span pairs as candidate relationship tuples, achieving state-of-the-art results on datasets like ADE. However, these models encounter a significant number of non-entity spans or irrelevant span pairs during the tasks, impairing model performance significantly. To address this issue, this paper introduces a span-based multitask entity-relation joint extraction model. This approach employs the multitask learning to alleviate the impact of negative samples on entity and relation classifiers. Additionally, we leverage the Intersection over Union(IoU) concept to introduce the positional information into the entity classifier, achieving a span boundary detection. Furthermore, by incorporating the entity Logits predicted by the entity classifier into the embedded representation of entity pairs, the semantic input for the relation classifier is enriched. Experimental results demonstrate that our proposed SpERT.MT model can effectively mitigate the adverse effects of excessive negative samples on the model performance. Furthermore, the model demonstrated commendable F1 scores of 73.61\%, 53.72\%, and 83.72\% on three widely employed public datasets, namely CoNLL04, SciERC, and ADE, respectively.
</details>
<details>
<summary>摘要</summary>
最近的span-based联合提取模型已经显示了很大的优势在实体识别和关系提取任务中。这些模型将文本段视为候选实体，并将段对 viewed as candidate relationship tuples，达到了最新的结果在ADE等 dataset上。然而，这些模型在任务中遇到了大量的非实体段或无关的段对，这会减少模型的性能。为了解决这个问题，本文提出了一种span-based多任务实体关系联合提取模型。这种方法使用多任务学习来减轻非实体段和无关段对对实体和关系分类器的影响。此外，我们利用了Intersection over Union(IoU)概念来将位置信息引入实体分类器中，实现了段边界检测。此外，通过将实体预测值 integrate into the embedded representation of entity pairs，我们可以为关系分类器提供更加semantic的输入。实验结果表明，我们提出的SpERT.MT模型可以有效地减轻非实体段和无关段对对模型性能的影响。此外，模型在三个广泛使用的公共数据集上达到了很高的F1分数，具体分别是73.61%, 53.72%, 83.72%。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Gender-Bias-of-Pre-trained-Language-Models-in-Natural-Language-Inference-by-Considering-All-Labels"><a href="#Evaluating-Gender-Bias-of-Pre-trained-Language-Models-in-Natural-Language-Inference-by-Considering-All-Labels" class="headerlink" title="Evaluating Gender Bias of Pre-trained Language Models in Natural Language Inference by Considering All Labels"></a>Evaluating Gender Bias of Pre-trained Language Models in Natural Language Inference by Considering All Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09697">http://arxiv.org/abs/2309.09697</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/panatchakorn-a/bias-eval-nli-considering-all-labels">https://github.com/panatchakorn-a/bias-eval-nli-considering-all-labels</a></li>
<li>paper_authors: Panatchakorn Anantaprayoon, Masahiro Kaneko, Naoaki Okazaki</li>
<li>for: 本研究旨在评估语言模型中的偏见，包括生物偏见和语言偏见。</li>
<li>methods: 本研究提出了一种基于所有标签的语言推理偏见评估方法，包括创建评估数据集和定义偏见度量。</li>
<li>results: 实验结果显示，该评估方法可以更准确地评估语言模型的偏见性，并且可以应用于多种语言。此外，本研究还评估了不同语言的语言模型偏见性。<details>
<summary>Abstract</summary>
Discriminatory social biases, including gender biases, have been found in Pre-trained Language Models (PLMs). In Natural Language Inference (NLI), recent bias evaluation methods have observed biased inferences from the outputs of a particular label such as neutral or entailment. However, since different biased inferences can be associated with different output labels, it is inaccurate for a method to rely on one label. In this work, we propose an evaluation method that considers all labels in the NLI task. We create evaluation data and assign them into groups based on their expected biased output labels. Then, we define a bias measure based on the corresponding label output of each data group. In the experiment, we propose a meta-evaluation method for NLI bias measures, and then use it to confirm that our measure can evaluate bias more accurately than the baseline. Moreover, we show that our evaluation method is applicable to multiple languages by conducting the meta-evaluation on PLMs in three different languages: English, Japanese, and Chinese. Finally, we evaluate PLMs of each language to confirm their bias tendency. To our knowledge, we are the first to build evaluation datasets and measure the bias of PLMs from the NLI task in Japanese and Chinese.
</details>
<details>
<summary>摘要</summary>
社会偏见，包括性别偏见，在预训练语言模型（PLM）中发现。在自然语言推理（NLI）任务中，最近的偏见评估方法发现PLM的输出中存在偏见。然而，由于不同的偏见可能与不同的输出标签相关，因此使用一个标签是不准确的。在这种情况下，我们提议一种评估方法，该方法考虑所有的输出标签在NLI任务中。我们创建评估数据，并将其分组 Based on their expected biased output labels。然后，我们定义基于每个数据组的标签输出的偏见度量。在实验中，我们提议一种元评估方法，并使用其来验证我们的度量方法可以更准确地评估偏见。此外，我们展示了我们的评估方法可以应用于多种语言，通过在英语、日语和中文三种语言的PLM上进行元评估。最后，我们评估每种语言的PLM，以验证它们的偏见倾向。到目前为止，我们是第一个在NLI任务中为日语和中文的PLM建立评估数据集，并测量它们的偏见倾向。
</details></li>
</ul>
<hr>
<h2 id="Do-learned-speech-symbols-follow-Zipf’s-law"><a href="#Do-learned-speech-symbols-follow-Zipf’s-law" class="headerlink" title="Do learned speech symbols follow Zipf’s law?"></a>Do learned speech symbols follow Zipf’s law?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09690">http://arxiv.org/abs/2309.09690</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shinnosuke Takamichi, Hiroki Maeda, Joonyong Park, Daisuke Saito, Hiroshi Saruwatari</li>
<li>for: 本研究探讨了深度学习学习的speech symbol是否遵循Zipf的法则，与自然语言符号一样。</li>
<li>methods: 本研究使用了深度学习来学习speech symbol，并对其进行分析，以判断它们是否遵循Zipf的法则。</li>
<li>results: 研究发现，数据驱动的speech symbol遵循Zipf的法则，与自然语言符号一样。这些结果为 spoken语言处理领域提供了新的统计分析方法。<details>
<summary>Abstract</summary>
In this study, we investigate whether speech symbols, learned through deep learning, follow Zipf's law, akin to natural language symbols. Zipf's law is an empirical law that delineates the frequency distribution of words, forming fundamentals for statistical analysis in natural language processing. Natural language symbols, which are invented by humans to symbolize speech content, are recognized to comply with this law. On the other hand, recent breakthroughs in spoken language processing have given rise to the development of learned speech symbols; these are data-driven symbolizations of speech content. Our objective is to ascertain whether these data-driven speech symbols follow Zipf's law, as the same as natural language symbols. Through our investigation, we aim to forge new ways for the statistical analysis of spoken language processing.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们研究了深度学习学习的语音符号是否遵循Zipf的法则，与自然语言符号一样。Zipf的法则是一个实际法则，描述了语言中词语频率的分布，成为自然语言处理的统计分析基础。人类创造的自然语言符号遵循这个法则，而现在的口语处理技术突破使得数据驱动的语音符号出现了。我们的目标是确定这些数据驱动的语音符号是否遵循Zipf的法则，与自然语言符号一样。通过我们的调查，我们希望开拓新的统计分析方法 для口语处理。
</details></li>
</ul>
<hr>
<h2 id="Multi-turn-Dialogue-Comprehension-from-a-Topic-aware-Perspective"><a href="#Multi-turn-Dialogue-Comprehension-from-a-Topic-aware-Perspective" class="headerlink" title="Multi-turn Dialogue Comprehension from a Topic-aware Perspective"></a>Multi-turn Dialogue Comprehension from a Topic-aware Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09666">http://arxiv.org/abs/2309.09666</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinbei Ma, Yi Xu, Hai Zhao, Zhuosheng Zhang</li>
<li>for: 本文主要针对对话机器理解中的对话发展需要语言模型能够有效地解耦和模型多个对话转帖。由于对话发展的主题可能不会在整个对话过程中保持相同，因此对话模型化非常具有挑战性。本文提出了一种基于主题的对话模型化方法，通过对对话段进行自动分割，将对话段转化为主题相关的语言处理单元。</li>
<li>methods: 本文提出了一种基于主题的对话段分割算法，并使用这些分割后的对话段作为主题相关的语言处理单元进行进一步的对话理解。此外，本文还提出了一种基于自适应autoencoder的主题划分系统，以及两个自定义的评估数据集。</li>
<li>results: 对三个公共 benchmark 进行了实验，并证明了与基eline相比，本文的方法具有显著的改善。本文继承了之前关于文档主题的研究，并将对话模型化带入了一个新的主题意识领域，并进行了广泛的实验和分析。<details>
<summary>Abstract</summary>
Dialogue related Machine Reading Comprehension requires language models to effectively decouple and model multi-turn dialogue passages. As a dialogue development goes after the intentions of participants, its topic may not keep constant through the whole passage. Hence, it is non-trivial to detect and leverage the topic shift in dialogue modeling. Topic modeling, although has been widely studied in plain text, deserves far more utilization in dialogue reading comprehension. This paper proposes to model multi-turn dialogues from a topic-aware perspective. We start with a dialogue segmentation algorithm to split a dialogue passage into topic-concentrated fragments in an unsupervised way. Then we use these fragments as topic-aware language processing units in further dialogue comprehension. On one hand, the split segments indict specific topics rather than mixed intentions, thus showing convenient on in-domain topic detection and location. For this task, we design a clustering system with a self-training auto-encoder, and we build two constructed datasets for evaluation. On the other hand, the split segments are an appropriate element of multi-turn dialogue response selection. For this purpose, we further present a novel model, Topic-Aware Dual-Attention Matching (TADAM) Network, which takes topic segments as processing elements and matches response candidates with a dual cross-attention. Empirical studies on three public benchmarks show great improvements over baselines. Our work continues the previous studies on document topic, and brings the dialogue modeling to a novel topic-aware perspective with exhaustive experiments and analyses.
</details>
<details>
<summary>摘要</summary>
对话相关的机器阅读理解需要语言模型能够有效地解耦和模型多个对话段。因为对话的目的可能会在整个段落中变化，因此检测和利用对话中的主题转换非常重要。主题分析，尽管在普通文本中广泛研究，在对话阅读理解中尚未得到充分利用。这篇论文提议在一个主题意识角度上模型多个对话段。我们从对话分割算法开始，将对话段落分解成主题强调的小段，然后使用这些小段作为主题意识语言处理单元进行进一步的对话理解。在一个方面，分割段落可以帮助检测对话中的主题，而不是混合的意图，因此在领域内主题检测和定位变得更加便捷。在另一方面，分割段落是多Turn对话回答选择的适当元素。为此，我们采用了一种新的模型，主题意识双重注意网络（TADAM），该模型将主题段落作为处理元素，并将回答候选者与双重跨注意相匹配。我们对三个公共 benchmark 进行了实验，得到了很大的改进。我们的工作继承了之前的文档主题研究，并将对话模型带到了一个新的主题意识角度，并进行了详细的实验和分析。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Method-of-Fuzzy-Topic-Modeling-based-on-Transformer-Processing"><a href="#A-Novel-Method-of-Fuzzy-Topic-Modeling-based-on-Transformer-Processing" class="headerlink" title="A Novel Method of Fuzzy Topic Modeling based on Transformer Processing"></a>A Novel Method of Fuzzy Topic Modeling based on Transformer Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09658">http://arxiv.org/abs/2309.09658</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ching-Hsun Tseng, Shin-Jye Lee, Po-Wei Cheng, Chien Lee, Chih-Chieh Hung</li>
<li>for: 本研究旨在提出一种基于软划分和文档嵌入的模糊主题分析方法，以便更好地监测市场趋势。</li>
<li>methods: 本研究使用了state-of-the-art transformer-based模型来实现词embedding和软划分 clustering，并应用于新闻发布监测中。</li>
<li>results: 实际应用中，模糊主题分析方法比传统的LDA模型更能够提供自然的结果。<details>
<summary>Abstract</summary>
Topic modeling is admittedly a convenient way to monitor markets trend. Conventionally, Latent Dirichlet Allocation, LDA, is considered a must-do model to gain this type of information. By given the merit of deducing keyword with token conditional probability in LDA, we can know the most possible or essential topic. However, the results are not intuitive because the given topics cannot wholly fit human knowledge. LDA offers the first possible relevant keywords, which also brings out another problem of whether the connection is reliable based on the statistic possibility. It is also hard to decide the topic number manually in advance. As the booming trend of using fuzzy membership to cluster and using transformers to embed words, this work presents the fuzzy topic modeling based on soft clustering and document embedding from state-of-the-art transformer-based model. In our practical application in a press release monitoring, the fuzzy topic modeling gives a more natural result than the traditional output from LDA.
</details>
<details>
<summary>摘要</summary>
Recently, fuzzy membership clustering and transformer-based word embedding have gained popularity. This study introduces fuzzy topic modeling based on soft clustering and document embedding using state-of-the-art transformer-based models. In our practical application of press release monitoring, fuzzy topic modeling provides more natural results than traditional LDA output.
</details></li>
</ul>
<hr>
<h2 id="Speeding-Up-Speech-Synthesis-In-Diffusion-Models-By-Reducing-Data-Distribution-Recovery-Steps-Via-Content-Transfer"><a href="#Speeding-Up-Speech-Synthesis-In-Diffusion-Models-By-Reducing-Data-Distribution-Recovery-Steps-Via-Content-Transfer" class="headerlink" title="Speeding Up Speech Synthesis In Diffusion Models By Reducing Data Distribution Recovery Steps Via Content Transfer"></a>Speeding Up Speech Synthesis In Diffusion Models By Reducing Data Distribution Recovery Steps Via Content Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09652">http://arxiv.org/abs/2309.09652</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Ochieng</li>
<li>for: 提高Diffusion基于 vocoder的速度和质量</li>
<li>methods: 使用卷积神经网络进行降噪和数据回归，并定义skip参数以降低数据分布恢复步骤数量</li>
<li>results: 提出一种新的 diffusion vocoder 技术，可以在竞争性的时间内生成高质量的语音，并且具有普适性和扩展性。<details>
<summary>Abstract</summary>
Diffusion based vocoders have been criticised for being slow due to the many steps required during sampling. Moreover, the model's loss function that is popularly implemented is designed such that the target is the original input $x_0$ or error $\epsilon_0$. For early time steps of the reverse process, this results in large prediction errors, which can lead to speech distortions and increase the learning time. We propose a setup where the targets are the different outputs of forward process time steps with a goal to reduce the magnitude of prediction errors and reduce the training time. We use the different layers of a neural network (NN) to perform denoising by training them to learn to generate representations similar to the noised outputs in the forward process of the diffusion. The NN layers learn to progressively denoise the input in the reverse process until finally the final layer estimates the clean speech. To avoid 1:1 mapping between layers of the neural network and the forward process steps, we define a skip parameter $\tau>1$ such that an NN layer is trained to cumulatively remove the noise injected in the $\tau$ steps in the forward process. This significantly reduces the number of data distribution recovery steps and, consequently, the time to generate speech. We show through extensive evaluation that the proposed technique generates high-fidelity speech in competitive time that outperforms current state-of-the-art tools. The proposed technique is also able to generalize well to unseen speech.
</details>
<details>
<summary>摘要</summary>
Diffusion基于的 vocoder 被批评为因 sampling 过程中需要多个步骤，导致速度较慢。另外，流行的模型损失函数设计为目标是原始输入 $x_0$ 或错误 $\epsilon_0$，对于早期时间步的反向过程而言，这会导致大的预测错误，从而导致语音扭曲和增加学习时间。我们提议一种设置，其中目标是在不同的时间步骤中的输出，以降低预测错误的大小和减少学习时间。我们使用不同层次的神经网络（NN）来进行去噪，通过训练它们学习生成与前向过程中的杂音输出相似的表示。NN层次逐步去噪输入，直到最后一层估计干净的语音。为了避免 NN 层次与前向过程步骤之间的一对一映射，我们定义了跳过参数 $\tau>1$，以便 NN 层次在 $\tau$ 步前进行共同去噪。这 significantly 减少了数据回归步骤的数量，并且因此减少了生成语音的时间。我们通过广泛的评估表明，我们提议的技术可以生成高效率、高质量的语音，并且可以很好地泛化到未seen语音。
</details></li>
</ul>
<hr>
<h2 id="Summarization-is-Almost-Dead"><a href="#Summarization-is-Almost-Dead" class="headerlink" title="Summarization is (Almost) Dead"></a>Summarization is (Almost) Dead</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09558">http://arxiv.org/abs/2309.09558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao Pu, Mingqi Gao, Xiaojun Wan</li>
<li>for: 这 paper 是为了评估大语言模型（LLM）在摘要任务中的表现。</li>
<li>methods: 该 paper 使用了新的数据集和人类评估实验来评估 LLM 在五种不同摘要任务中的零基础生成能力。</li>
<li>results: 研究发现，人类评估者对 LLM 生成的摘要优于人工写成的摘要和 fine-tuned 模型生成的摘要，特别是在事实一致性和外部幻见方面表现更好。因此，我们认为，在 LLM 时代，大多数文本摘要领域的传统工作都不再必要了。然而，我们还需要继续探索一些方向，例如创造更高质量和更可靠的评估方法，以及新的数据集。<details>
<summary>Abstract</summary>
How well can large language models (LLMs) generate summaries? We develop new datasets and conduct human evaluation experiments to evaluate the zero-shot generation capability of LLMs across five distinct summarization tasks. Our findings indicate a clear preference among human evaluators for LLM-generated summaries over human-written summaries and summaries generated by fine-tuned models. Specifically, LLM-generated summaries exhibit better factual consistency and fewer instances of extrinsic hallucinations. Due to the satisfactory performance of LLMs in summarization tasks (even surpassing the benchmark of reference summaries), we believe that most conventional works in the field of text summarization are no longer necessary in the era of LLMs. However, we recognize that there are still some directions worth exploring, such as the creation of novel datasets with higher quality and more reliable evaluation methods.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）是否能够生成好的摘要？我们开发了新的数据集和进行了人类评估实验，以评估 LLM 在五种不同摘要任务中的零基础生成能力。我们的结果显示，人类评审者对 LLM 生成的摘要表示偏好，而且与人工撰写的摘要和 fine-tuned 模型生成的摘要相比， LLM 生成的摘要更有内容和外部错误的优势。尤其是 LLM 生成的摘要在事实上是更加精确和有 fewer 外部错误。由于 LLM 在摘要任务中的表现非常满意（甚至超过参考摘要的 benchmark），我们认为，现在的文本摘要领域中大多数传统的工作不再是必要的。然而，我们认为还有一些值得探索的方向，例如创建更高质量和更可靠的评估方法，以及创建更多的数据集。
</details></li>
</ul>
<hr>
<h2 id="Training-dynamic-models-using-early-exits-for-automatic-speech-recognition-on-resource-constrained-devices"><a href="#Training-dynamic-models-using-early-exits-for-automatic-speech-recognition-on-resource-constrained-devices" class="headerlink" title="Training dynamic models using early exits for automatic speech recognition on resource-constrained devices"></a>Training dynamic models using early exits for automatic speech recognition on resource-constrained devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09546">http://arxiv.org/abs/2309.09546</a></li>
<li>repo_url: None</li>
<li>paper_authors: George August Wright, Umberto Cappellazzo, Salah Zaiem, Desh Raj, Lucas Ondel Yang, Daniele Falavigna, Alessio Brutti</li>
<li>for: 这篇论文旨在提出一种可以在处理时间上动态地调整神经网络模型的 Computational Load，以便在资源有限的设备上进行处理。</li>
<li>methods: 本论文使用了初级终端架构，具体是透过中间终端分支来实现动态模型的调整。此外，本论文还对已预训神经网络进行了从头开始训练。</li>
<li>results: 实验结果显示，使用了初级终端架构且从头开始训练的模型不仅在使用较少层数时能够保持表现水准，更能够提高任务的准确率，比single-exit模型或使用预训模型更好。此外，本论文还提出了一种基于 posterior probability的exit选择策略作为一个替代方案。<details>
<summary>Abstract</summary>
The possibility of dynamically modifying the computational load of neural models at inference time is crucial for on-device processing, where computational power is limited and time-varying. Established approaches for neural model compression exist, but they provide architecturally static models. In this paper, we investigate the use of early-exit architectures, that rely on intermediate exit branches, applied to large-vocabulary speech recognition. This allows for the development of dynamic models that adjust their computational cost to the available resources and recognition performance. Unlike previous works, besides using pre-trained backbones we also train the model from scratch with an early-exit architecture. Experiments on public datasets show that early-exit architectures from scratch not only preserve performance levels when using fewer encoder layers, but also improve task accuracy as compared to using single-exit models or using pre-trained models. Additionally, we investigate an exit selection strategy based on posterior probabilities as an alternative to frame-based entropy.
</details>
<details>
<summary>摘要</summary>
可以在推理时动态修改神经网络模型的计算负担非常重要，特别是在设备上进行处理，因为计算能力有限并且时间变化。现有的神经网络压缩方法已经存在，但它们提供的模型是建筑性的静态的。在这篇论文中，我们研究使用中途离开结构，该结构 rely on intermediate exit branches，应用于大 vocabulary 语音识别。这 permit 开发动态模型，可以根据可用资源和识别性来调整计算成本。与先前的工作不同，我们不仅使用预训练后台，还从scratch 训练模型。实验表明，使用 fewer encoder layers 的 early-exit 模型不仅保持性能水平，而且在使用单exit模型或使用预训练模型时，也提高了任务准确率。此外，我们还研究基于 posterior probabilities 的 exit 选择策略作为 frame-based entropy 的替代方案。
</details></li>
</ul>
<hr>
<h2 id="Adapting-Large-Language-Models-via-Reading-Comprehension"><a href="#Adapting-Large-Language-Models-via-Reading-Comprehension" class="headerlink" title="Adapting Large Language Models via Reading Comprehension"></a>Adapting Large Language Models via Reading Comprehension</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09530">http://arxiv.org/abs/2309.09530</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/lmops">https://github.com/microsoft/lmops</a></li>
<li>paper_authors: Daixuan Cheng, Shaohan Huang, Furu Wei</li>
<li>for: 本研究探讨了如何通过继续在域pecific的corpus上进行预训练，以influence大语言模型，发现预训练 Raw corpora 可以让模型学习域知识，但是会严重降低其回答问题的能力。</li>
<li>methods: 我们提出了一种简单的方法，通过将 Raw corpora 转换成reading comprehension texts来解决这个问题。我们的方法可以扩展到任何预训练 corpora，并且可以在不同的域上进行缩放。</li>
<li>results: 我们的研究表明，通过使用我们的方法，我们的7B语言模型可以与更大的域pecific模型（如BloombergGPT-50B） achieve competitive performance。此外，我们还证明了域pecific reading comprehension texts可以提高模型的性能，并且表明可以开发一个可以在多个域上进行预训练的通用模型。<details>
<summary>Abstract</summary>
We explore how continued pre-training on domain-specific corpora influences large language models, revealing that training on the raw corpora endows the model with domain knowledge, but drastically hurts its prompting ability for question answering. Taken inspiration from human learning via reading comprehension--practice after reading improves the ability to answer questions based on the learned knowledge--we propose a simple method for transforming raw corpora into reading comprehension texts. Each raw text is enriched with a series of tasks related to its content. Our method, highly scalable and applicable to any pre-training corpora, consistently enhances performance across various tasks in three different domains: biomedicine, finance, and law. Notably, our 7B language model achieves competitive performance with domain-specific models of much larger scales, such as BloombergGPT-50B. Furthermore, we demonstrate that domain-specific reading comprehension texts can improve the model's performance even on general benchmarks, showing the potential to develop a general model across even more domains. Our model, code, and data will be available at https://github.com/microsoft/LMOps.
</details>
<details>
<summary>摘要</summary>
我们探究了将领域特定文献作为预训练数据的影响于大型自然语言模型，发现将模型训练在原始文献上授知了领域知识，但会对问答能力产生极大的影响。启发自人类通过阅读理解——阅读后练习提高了根据学习的知识回答问题的能力——我们提议一种简单的方法，可以将原始文献转化为阅读理解文本。每个原始文本都会被授加一系列与其内容相关的任务。我们的方法可扩展到任何预训练 corpora，并在生物医学、金融和法律等三个领域中实现了稳定的提升性。尤其是我们的7B语言模型可以与大小相对较小的领域特定模型相比，如 BloombergGPT-50B，达到竞争性的表现。此外，我们还证明了领域特定的阅读理解文本可以提高模型的表现，即使是通用的标准 bencmarks。我们的模型、代码和数据将会在https://github.com/microsoft/LMOps 上公开。
</details></li>
</ul>
<hr>
<h2 id="Improved-Factorized-Neural-Transducer-Model-For-text-only-Domain-Adaptation"><a href="#Improved-Factorized-Neural-Transducer-Model-For-text-only-Domain-Adaptation" class="headerlink" title="Improved Factorized Neural Transducer Model For text-only Domain Adaptation"></a>Improved Factorized Neural Transducer Model For text-only Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09524">http://arxiv.org/abs/2309.09524</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cppan-packages/43c6cb2c61134ec7e23098e41ca6ee7bfe3573342f9f6f196bc095247e062001">https://github.com/cppan-packages/43c6cb2c61134ec7e23098e41ca6ee7bfe3573342f9f6f196bc095247e062001</a></li>
<li>paper_authors: Junzhe Liu, Jianwei Yu, Xie Chen</li>
<li>for: 实现佳绩的语音识别模型，例如神经Transducer，可以同时融合语音和语言信息，但将这些模型调整到仅有文本数据的情况下是困难的。</li>
<li>methods: 为了解决这个问题，我们提出了一个称为factorized neural Transducer（FNT）的模型结构，它将引入一个独立的词汇解oder，以预测词汇，并实现传统的文本数据适应。</li>
<li>results: 对于这个挑战，我们提出了一个改进的factorized neural Transducer（IFNT）模型结构，可以实现语音和语言信息的完整融合，并实现有效的文本适应。我们通过对GigaSpeech和三个类别数据集进行实验，证明了IFNT的性能提升。相比于标准的神经Transducer和FNT模型，IFNT在文本适应后可以获得7.9%至28.5%的相对WRER改善，并在三个测试集上获得1.6%至8.2%的相对WRER改善。<details>
<summary>Abstract</summary>
End-to-end models, such as the neural Transducer, have been successful in integrating acoustic and linguistic information jointly to achieve excellent recognition performance. However, adapting these models with text-only data is challenging. Factorized neural Transducer (FNT) aims to address this issue by introducing a separate vocabulary decoder to predict the vocabulary, which can effectively perform traditional text data adaptation. Nonetheless, this approach has limitations in fusing acoustic and language information seamlessly. Moreover, a degradation in word error rate (WER) on the general test sets was also observed, leading to doubts about its overall performance. In response to this challenge, we present an improved factorized neural Transducer (IFNT) model structure designed to comprehensively integrate acoustic and language information while enabling effective text adaptation. We evaluate the performance of our proposed methods through in-domain experiments on GigaSpeech and out-of-domain experiments adapting to EuroParl, TED-LIUM, and Medical datasets. After text-only adaptation, IFNT yields 7.9% to 28.5% relative WER improvements over the standard neural Transducer with shallow fusion, and relative WER reductions ranging from 1.6% to 8.2% on the three test sets compared to the FNT model.
</details>
<details>
<summary>摘要</summary>
结合语音和语言信息的端到端模型，如神经Transducer，已经在识别性能方面取得了出色的成绩。然而，将这些模型适应文本数据是一项挑战。 factorized neural Transducer (FNT) 目的是解决这个问题，它引入了一个独立的词典解码器来预测词语，这有效地实现了传统文本数据适应。然而，这种方法在融合语音和语言信息的方面存在限制。此外，我们在通用测试集上观察到了单词错误率 (WER) 的下降，这引发了对其总性能的忧虑。为了解决这个挑战，我们提出了改进的 factorized neural Transducer (IFNT) 模型结构，用于全面融合语音和语言信息，同时允许有效的文本适应。我们通过在 GigaSpeech 上进行域内实验和在 EuroParl、TED-LIUM 和医疗数据集上进行对应适应测试，证明了我们的提议方法的性能优势。在文本适应后，IFNT 对于标准神经Transducer 的浅合并而言，实现了7.9% 到 28.5% 的相对 WER 改进，并在三个测试集上实现了1.6% 到 8.2% 的相对 WER 下降。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Divergent-Framing-of-the-Supreme-Court-Controversies-Social-Media-vs-News-Outlets"><a href="#Understanding-Divergent-Framing-of-the-Supreme-Court-Controversies-Social-Media-vs-News-Outlets" class="headerlink" title="Understanding Divergent Framing of the Supreme Court Controversies: Social Media vs. News Outlets"></a>Understanding Divergent Framing of the Supreme Court Controversies: Social Media vs. News Outlets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09508">http://arxiv.org/abs/2309.09508</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinsheng Pan, Zichen Wang, Weihong Qi, Hanjia Lyu, Jiebo Luo</li>
<li>for: This paper aims to address the gap in our understanding of the disparities in framing political issues between news media and social media outlets.</li>
<li>methods: The authors conduct a comprehensive investigation, focusing on the nuanced distinctions in the framing of social media and traditional media outlets concerning a series of American Supreme Court rulings on affirmative action, student loans, and abortion rights. They use both qualitative and quantitative methods to compare the framing of these issues across different media platforms.</li>
<li>results: The authors find that while there is some overlap in framing between social media and traditional media outlets, there are substantial differences in the way these issues are framed, particularly in terms of polarization. They observe that social media platforms tend to present more polarized stances across all framing categories, while traditional news media tend to exhibit more consensus on the topic of student loans, but more polarization on affirmative action and abortion rights. These findings have significant implications for the formation of public opinion, policy decision-making, and the broader political landscape.Here is the same information in Simplified Chinese text:</li>
<li>for: 本研究旨在填补新媒体和传统媒体之间政治问题帧幕的差异不足的知识空白。</li>
<li>methods: 作者采用了丰富的资料和方法，对美国最高法院的一系列决定（包括奖学金、学生贷款和堕胎权）进行了精心的分析和比较。</li>
<li>results: 作者发现，虽然新媒体和传统媒体之间有一定的帧幕相似性，但是存在重要的差异，尤其是在各个主题上的极化程度更高。社交媒体平台比传统新闻媒体更加具有极化的立场，而传统新闻媒体则在奖学金和堕胎权上更加具有政治倾向。这些发现对公众意见形成、政策决策和政治景观产生了重要的影响。<details>
<summary>Abstract</summary>
Understanding the framing of political issues is of paramount importance as it significantly shapes how individuals perceive, interpret, and engage with these matters. While prior research has independently explored framing within news media and by social media users, there remains a notable gap in our comprehension of the disparities in framing political issues between these two distinct groups. To address this gap, we conduct a comprehensive investigation, focusing on the nuanced distinctions both qualitatively and quantitatively in the framing of social media and traditional media outlets concerning a series of American Supreme Court rulings on affirmative action, student loans, and abortion rights. Our findings reveal that, while some overlap in framing exists between social media and traditional media outlets, substantial differences emerge both across various topics and within specific framing categories. Compared to traditional news media, social media platforms tend to present more polarized stances across all framing categories. Further, we observe significant polarization in the news media's treatment (i.e., Left vs. Right leaning media) of affirmative action and abortion rights, whereas the topic of student loans tends to exhibit a greater degree of consensus. The disparities in framing between traditional and social media platforms carry significant implications for the formation of public opinion, policy decision-making, and the broader political landscape.
</details>
<details>
<summary>摘要</summary>
理解政治问题的帧定对于个人的认知、解释和参与政治决策具有 paramount importance。 although prior research has independently explored framing within news media and by social media users, there remains a notable gap in our comprehension of the disparities in framing political issues between these two distinct groups. To address this gap, we conduct a comprehensive investigation, focusing on the nuanced distinctions both qualitatively and quantitatively in the framing of social media and traditional media outlets concerning a series of American Supreme Court rulings on affirmative action, student loans, and abortion rights. Our findings reveal that, while some overlap in framing exists between social media and traditional media outlets, substantial differences emerge both across various topics and within specific framing categories. Compared to traditional news media, social media platforms tend to present more polarized stances across all framing categories. Further, we observe significant polarization in the news media's treatment (i.e., Left vs. Right leaning media) of affirmative action and abortion rights, whereas the topic of student loans tends to exhibit a greater degree of consensus. The disparities in framing between traditional and social media platforms carry significant implications for the formation of public opinion, policy decision-making, and the broader political landscape.Here's a word-for-word translation of the text into Simplified Chinese:理解政治问题的帧定对于个人的认知、解释和参与政治决策具有paramount importance。 although prior research has independently explored framing within news media and by social media users, there remains a notable gap in our comprehension of the disparities in framing political issues between these two distinct groups. To address this gap, we conduct a comprehensive investigation, focusing on the nuanced distinctions both qualitatively and quantitatively in the framing of social media and traditional media outlets concerning a series of American Supreme Court rulings on affirmative action, student loans, and abortion rights. Our findings reveal that, while some overlap in framing exists between social media and traditional media outlets, substantial differences emerge both across various topics and within specific framing categories. Compared to traditional news media, social media platforms tend to present more polarized stances across all framing categories. Further, we observe significant polarization in the news media's treatment (i.e., Left vs. Right leaning media) of affirmative action and abortion rights, whereas the topic of student loans tends to exhibit a greater degree of consensus. The disparities in framing between traditional and social media platforms carry significant implications for the formation of public opinion, policy decision-making, and the broader political landscape.
</details></li>
</ul>
<hr>
<h2 id="LayoutNUWA-Revealing-the-Hidden-Layout-Expertise-of-Large-Language-Models"><a href="#LayoutNUWA-Revealing-the-Hidden-Layout-Expertise-of-Large-Language-Models" class="headerlink" title="LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language Models"></a>LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09506">http://arxiv.org/abs/2309.09506</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/projectnuwa/layoutnuwa">https://github.com/projectnuwa/layoutnuwa</a></li>
<li>paper_authors: Zecheng Tang, Chenfei Wu, Juntao Li, Nan Duan</li>
<li>for: 该论文主要针对 Graphic layout generation 领域的研究, 它的目的是提高用户参与度和信息吸收。</li>
<li>methods: 该论文提出了一种新的 LayoutNUWA 模型，它将布局生成视为代码生成任务，以增强 semantic 信息和利用大型自然语言模型（LLMs）中隐藏的布局专家知识。 该模型包括三个相互连接的模块：1）代码初始化（CI）模块，计算数值条件并将其转换为 HTML 代码中策略性地隐藏的掩码; 2）代码完成（CC）模块，使用形式知识来填充掩码内的部分; 3）代码渲染（CR）模块，将完成后的代码转换为最终的布局输出，确保了高度可读性和透明度的布局生成过程。</li>
<li>results: 该论文在多个数据集上达到了状态之 искусственный智能性能的显著提高（超过 50%），展示了 LayoutNUWA 模型的强大能力。<details>
<summary>Abstract</summary>
Graphic layout generation, a growing research field, plays a significant role in user engagement and information perception. Existing methods primarily treat layout generation as a numerical optimization task, focusing on quantitative aspects while overlooking the semantic information of layout, such as the relationship between each layout element. In this paper, we propose LayoutNUWA, the first model that treats layout generation as a code generation task to enhance semantic information and harness the hidden layout expertise of large language models~(LLMs). More concretely, we develop a Code Instruct Tuning (CIT) approach comprising three interconnected modules: 1) the Code Initialization (CI) module quantifies the numerical conditions and initializes them as HTML code with strategically placed masks; 2) the Code Completion (CC) module employs the formatting knowledge of LLMs to fill in the masked portions within the HTML code; 3) the Code Rendering (CR) module transforms the completed code into the final layout output, ensuring a highly interpretable and transparent layout generation procedure that directly maps code to a visualized layout. We attain significant state-of-the-art performance (even over 50\% improvements) on multiple datasets, showcasing the strong capabilities of LayoutNUWA. Our code is available at https://github.com/ProjectNUWA/LayoutNUWA.
</details>
<details>
<summary>摘要</summary>
GRAPHIC LAYOUT生成，一个快速发展的研究领域，对用户参与度和信息感受有着重要的作用。现有方法主要对layout生成视为数值优化任务，强调数值方面而忽视layout中的semantic信息，如每个元素之间的关系。在这篇论文中，我们提出了LayoutNUWA，首个对layout生成视为代码生成任务，以增强semantic信息和利用大语言模型（LLMs）隐藏的布局专家知识。更具体地，我们开发了Code Instruct Tuning（CIT）方法，包括三个相连的模块：1）Code Initialization（CI）模块量化数值条件，并将其转化为HTML代码中策略性地置入masks；2）Code Completion（CC）模块利用LLMs的格式知识填充masked部分 dentroHTML代码；3）Code Rendering（CR）模块将完成代码转化为最终的布局输出，确保了高度可读性和透明度的布局生成过程，直接将代码映射到可视化的布局。我们在多个数据集上达到了STATE-OF-THE-ART性能（超过50%提升），展示了LayoutNUWA的强大能力。我们的代码可以在https://github.com/ProjectNUWA/LayoutNUWA上获取。
</details></li>
</ul>
<hr>
<h2 id="Search-and-Learning-for-Unsupervised-Text-Generation"><a href="#Search-and-Learning-for-Unsupervised-Text-Generation" class="headerlink" title="Search and Learning for Unsupervised Text Generation"></a>Search and Learning for Unsupervised Text Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09497">http://arxiv.org/abs/2309.09497</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Lili Mou</li>
<li>for: 这篇论文是为了探讨深度学习技术在人工智能（AI）领域中的文本生成，以及它对应用和社会影响的可能性。</li>
<li>methods: 这篇论文使用搜寻和学习方法来实现不监督的文本生成，其中一个变量目标函数估计候选句子的质量，而排序算法则将句子生成来最大化搜寻目标。一个机器学习模型也从搜寻结果中学习来平滑化噪音和提高效率。</li>
<li>results: 这篇论文的结果显示，这种搜寻和学习的方法可以实现高品质的文本生成，并且可以减少人类标注努力和处理低资源语言的时间。这种方法具有实际应用和社会影响的重要性，特别是在建立新任务的最小可行产品和减少人类努力的方面。<details>
<summary>Abstract</summary>
With the advances of deep learning techniques, text generation is attracting increasing interest in the artificial intelligence (AI) community, because of its wide applications and because it is an essential component of AI. Traditional text generation systems are trained in a supervised way, requiring massive labeled parallel corpora. In this paper, I will introduce our recent work on search and learning approaches to unsupervised text generation, where a heuristic objective function estimates the quality of a candidate sentence, and discrete search algorithms generate a sentence by maximizing the search objective. A machine learning model further learns from the search results to smooth out noise and improve efficiency. Our approach is important to the industry for building minimal viable products for a new task; it also has high social impacts for saving human annotation labor and for processing low-resource languages.
</details>
<details>
<summary>摘要</summary>
随着深度学习技术的发展，文本生成在人工智能（AI）社区中吸引了越来越多的关注，因为它的广泛应用和因为它是AI的重要组件。传统的文本生成系统通常在监督方式下训练，需要大量标注的平行 corpus。在这篇论文中，我将介绍我们最近的搜索和学习方法来实现无监督文本生成，其中一个启发目标函数估算候选句子的质量，而排序算法在搜索目标上生成句子。一个机器学习模型从搜索结果中学习，以缓解噪音并提高效率。我们的方法对于建立新任务的最小可行产品非常重要，同时具有高度的社会影响，因为它可以节省人类注释劳动力和处理低资源语言。
</details></li>
</ul>
<hr>
<h2 id="Investigating-Zero-and-Few-shot-Generalization-in-Fact-Verification"><a href="#Investigating-Zero-and-Few-shot-Generalization-in-Fact-Verification" class="headerlink" title="Investigating Zero- and Few-shot Generalization in Fact Verification"></a>Investigating Zero- and Few-shot Generalization in Fact Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09444">http://arxiv.org/abs/2309.09444</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/teacherpeterpan/fact-checking-generalization">https://github.com/teacherpeterpan/fact-checking-generalization</a></li>
<li>paper_authors: Liangming Pan, Yunxiang Zhang, Min-Yen Kan</li>
<li>for: 这个研究探索了零或几shot普遍化 для факт验证（FV），目的是将FV模型从具有资源的领域（例如Wikipedia）扩展到低资源领域，并将其应用于这些领域中。</li>
<li>methods: 我们首先建立了一个FV数据集集合，包括11个FV数据集，代表6个领域。我们进行了一个实验分析，发现现有模型在这些FV数据集上的普遍性很差。我们的分析发现了一些影响普遍性的因素，包括数据集大小、证据长度和宣告型。</li>
<li>results: 最后，我们显示了两种方向的工作可以提高普遍性：1）在特殊领域上预训模型，和2）通过自动生成训练数据来实现宣告生成。<details>
<summary>Abstract</summary>
In this paper, we explore zero- and few-shot generalization for fact verification (FV), which aims to generalize the FV model trained on well-resourced domains (e.g., Wikipedia) to low-resourced domains that lack human annotations. To this end, we first construct a benchmark dataset collection which contains 11 FV datasets representing 6 domains. We conduct an empirical analysis of generalization across these FV datasets, finding that current models generalize poorly. Our analysis reveals that several factors affect generalization, including dataset size, length of evidence, and the type of claims. Finally, we show that two directions of work improve generalization: 1) incorporating domain knowledge via pretraining on specialized domains, and 2) automatically generating training data via claim generation.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们探讨零和几个shot泛化 для事实验证（FV），目的是将FV模型在具有资源的领域（例如Wikipedia）上训练后，应用到缺乏人工标注的低资源领域。为此，我们首先构建了一个FV数据集合，包含11个FV数据集，代表6个领域。我们进行了FV数据集间的一般化分析，发现现有模型的一般化能力不佳。我们的分析发现一些因素影响一般化，包括数据集大小、证据长度和声明类型。最后，我们表明两种方向的工作可以提高一般化：1）在特殊领域中预训练模型，并2）通过自动生成训练数据来提高模型的一般化能力。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Multilingual-Speech-Recognition-through-Language-Prompt-Tuning-and-Frame-Level-Language-Adapter"><a href="#Enhancing-Multilingual-Speech-Recognition-through-Language-Prompt-Tuning-and-Frame-Level-Language-Adapter" class="headerlink" title="Enhancing Multilingual Speech Recognition through Language Prompt Tuning and Frame-Level Language Adapter"></a>Enhancing Multilingual Speech Recognition through Language Prompt Tuning and Frame-Level Language Adapter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09443">http://arxiv.org/abs/2309.09443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Song Li, Yongbin You, Xuezhi Wang, Ke Ding, Guanglu Wan</li>
<li>for: 提高多语言人工智能助手的表现，扩展其应用领域和国际交流</li>
<li>methods: 提议两种简单效果的方法：语言提示调整和帧级语言适配器，分别提高语言可配和无语言适配的多语言音频识别性能</li>
<li>results: 实验表明，使用我们提议的方法可以在七种语言上提高音频识别性能，显著提高多语言人工智能助手的表现<details>
<summary>Abstract</summary>
Multilingual intelligent assistants, such as ChatGPT, have recently gained popularity. To further expand the applications of multilingual artificial intelligence assistants and facilitate international communication, it is essential to enhance the performance of multilingual speech recognition, which is a crucial component of speech interaction. In this paper, we propose two simple and parameter-efficient methods: language prompt tuning and frame-level language adapter, to respectively enhance language-configurable and language-agnostic multilingual speech recognition. Additionally, we explore the feasibility of integrating these two approaches using parameter-efficient fine-tuning methods. Our experiments demonstrate significant performance improvements across seven languages using our proposed methods.
</details>
<details>
<summary>摘要</summary>
多语言智能助手，如ChatGPT，在最近受欢迎。为了进一步扩展多语言人工智能助手的应用和国际交流，我们需要提高多语言语音识别的性能，这是语音互动的重要组件。在这篇论文中，我们提出了两种简单和参数效率高的方法：语言提示调整和帧级语言适配器，以提高语言可配置和语言共享的多语言语音识别。此外，我们还探讨了这两种方法的结合使用方式，并使用参数效率的细化调整方法进行评估。我们的实验表明，使用我们提posed方法可以在七种语言中获得显著的性能提升。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/18/cs.CL_2023_09_18/" data-id="closbroml00bb0g887g39fhll" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/18/cs.LG_2023_09_18/" class="article-date">
  <time datetime="2023-09-18T10:00:00.000Z" itemprop="datePublished">2023-09-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/18/cs.LG_2023_09_18/">cs.LG - 2023-09-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Causal-Theories-and-Structural-Data-Representations-for-Improving-Out-of-Distribution-Classification"><a href="#Causal-Theories-and-Structural-Data-Representations-for-Improving-Out-of-Distribution-Classification" class="headerlink" title="Causal Theories and Structural Data Representations for Improving Out-of-Distribution Classification"></a>Causal Theories and Structural Data Representations for Improving Out-of-Distribution Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10211">http://arxiv.org/abs/2309.10211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Donald Martin, Jr., David Kinney</li>
<li>for: 用于提高机器学习系统的稳定性和安全性，通过使用人类生成的 causal 知识来减少机器学习开发人员的论证不确定性。</li>
<li>methods: 使用人类中心的 causal 理论和动力学 литературы中的工具，将数据表示为epidemic系统的数据生成过程中的不变结构 causal 特征。</li>
<li>results: 通过使用这种数据表示方法，在训练神经网络时，可以提高数据外的泛化性表现，比如Naive Approach的数据表示方法。<details>
<summary>Abstract</summary>
We consider how human-centered causal theories and tools from the dynamical systems literature can be deployed to guide the representation of data when training neural networks for complex classification tasks. Specifically, we use simulated data to show that training a neural network with a data representation that makes explicit the invariant structural causal features of the data generating process of an epidemic system improves out-of-distribution (OOD) generalization performance on a classification task as compared to a more naive approach to data representation. We take these results to demonstrate that using human-generated causal knowledge to reduce the epistemic uncertainty of ML developers can lead to more well-specified ML pipelines. This, in turn, points to the utility of a dynamical systems approach to the broader effort aimed at improving the robustness and safety of machine learning systems via improved ML system development practices.
</details>
<details>
<summary>摘要</summary>
我们考虑了人类中心 causal 理论和动力系统文献中的工具如何用于导引训练 нейрон网络时的数据表现。 Specifically, 我们使用模拟数据显示了训练一个 нейрон网络使用明确表达数据生成过程中的抗变化结构特征，可以提高数据外的测验性能（OOD）的数据表现。 We take these results to demonstrate that使用人类生成的 causal 知识可以减少机器学习开发人员的 epistemic 不确定性，可以导致更好地规划的 ML 管线。 This, in turn, points to the utility of a dynamical systems approach to the broader effort aimed at improving the robustness and safety of machine learning systems via improved ML system development practices.
</details></li>
</ul>
<hr>
<h2 id="The-Kernel-Density-Integral-Transformation"><a href="#The-Kernel-Density-Integral-Transformation" class="headerlink" title="The Kernel Density Integral Transformation"></a>The Kernel Density Integral Transformation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10194">http://arxiv.org/abs/2309.10194</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/calvinmccarter/kditransform">https://github.com/calvinmccarter/kditransform</a></li>
<li>paper_authors: Calvin McCarter</li>
<li>for: 本研究旨在提出基于机器学习和统计方法处理表格数据时的特征预处理策略。</li>
<li>methods: 本文提议使用核密度积分变换作为特征预处理步骤，该方法包含了Linear Min-Max Scaling和量iles transformation两种领先的特征预处理方法，而无需hyperparameter调整。</li>
<li>results: 本研究示出，无需调整hyperparameter，核密度积分变换可以作为Linear Min-Max Scaling和量iles transformation的简单替换方法，并且可以具有这两种方法的稳定性和性能。此外，通过调整单个连续hyperparameter，我们经常可以超越这两种方法的性能。最后，本文还示出了核密度变换在统计数据分析中的益处，特别是在相关分析和单variate clustering中。<details>
<summary>Abstract</summary>
Feature preprocessing continues to play a critical role when applying machine learning and statistical methods to tabular data. In this paper, we propose the use of the kernel density integral transformation as a feature preprocessing step. Our approach subsumes the two leading feature preprocessing methods as limiting cases: linear min-max scaling and quantile transformation. We demonstrate that, without hyperparameter tuning, the kernel density integral transformation can be used as a simple drop-in replacement for either method, offering robustness to the weaknesses of each. Alternatively, with tuning of a single continuous hyperparameter, we frequently outperform both of these methods. Finally, we show that the kernel density transformation can be profitably applied to statistical data analysis, particularly in correlation analysis and univariate clustering.
</details>
<details>
<summary>摘要</summary>
<LC>zh</LC></SYS>功能预处理仍然在机器学习和统计方法应用到表格数据时扮演关键角色。在这篇论文中，我们提议使用核密度积分变换作为特征预处理步骤。我们的方法包含两种领先的特征预处理方法为限制 случа：线性最小最大缩放和量程变换。我们示示，无需hyperparameter调整，核密度积分变换可以作为简单的替换方法，具有对每个方法的强度。 Alternatively，通过调整单一连续的超参数，我们经常超越这两个方法。最后，我们示示了核密度变换可以营利地应用于统计数据分析，尤其是在相关分析和单variate归一化中。
</details></li>
</ul>
<hr>
<h2 id="Stochastic-Deep-Koopman-Model-for-Quality-Propagation-Analysis-in-Multistage-Manufacturing-Systems"><a href="#Stochastic-Deep-Koopman-Model-for-Quality-Propagation-Analysis-in-Multistage-Manufacturing-Systems" class="headerlink" title="Stochastic Deep Koopman Model for Quality Propagation Analysis in Multistage Manufacturing Systems"></a>Stochastic Deep Koopman Model for Quality Propagation Analysis in Multistage Manufacturing Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10193">http://arxiv.org/abs/2309.10193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiyi Chen, Harshal Maske, Huanyi Shui, Devesh Upadhyay, Michael Hopka, Joseph Cohen, Xingjian Lai, Xun Huan, Jun Ni</li>
<li>for: 这篇研究的目的是为了模型多阶制造系统（MMS）的复杂行为，并使用深度学习方法来实现这个目的。</li>
<li>methods: 这篇研究使用了Stochastic Deep Koopman（SDK）框架，将 kritical quality information 通过Variational Autoencoders（VAEs）提取出来，并使用Koopman operator来传递这些资讯。</li>
<li>results: 根据比较研究，SDK 模型在预测 MMS 中每个阶段的产品质量方面的准确性比其他常用的数据驱动模型高。此外，SDK 的特殊的扩散性和可追溯性使得可以实现制程中品质的追溯和根本原因分析。<details>
<summary>Abstract</summary>
The modeling of multistage manufacturing systems (MMSs) has attracted increased attention from both academia and industry. Recent advancements in deep learning methods provide an opportunity to accomplish this task with reduced cost and expertise. This study introduces a stochastic deep Koopman (SDK) framework to model the complex behavior of MMSs. Specifically, we present a novel application of Koopman operators to propagate critical quality information extracted by variational autoencoders. Through this framework, we can effectively capture the general nonlinear evolution of product quality using a transferred linear representation, thus enhancing the interpretability of the data-driven model. To evaluate the performance of the SDK framework, we carried out a comparative study on an open-source dataset. The main findings of this paper are as follows. Our results indicate that SDK surpasses other popular data-driven models in accuracy when predicting stagewise product quality within the MMS. Furthermore, the unique linear propagation property in the stochastic latent space of SDK enables traceability for quality evolution throughout the process, thereby facilitating the design of root cause analysis schemes. Notably, the proposed framework requires minimal knowledge of the underlying physics of production lines. It serves as a virtual metrology tool that can be applied to various MMSs, contributing to the ultimate goal of Zero Defect Manufacturing.
</details>
<details>
<summary>摘要</summary>
多stage制造系统（MMS）的模型化吸引了学术和实践领域的越来越多的关注。现代深度学习方法的提出，为了实现这项任务，成本和专业知识的减少提供了机会。本研究提出了一种随机深度库曼（SDK）框架，用于模型MMS的复杂行为。特别是，我们提出了一种使用Variational Autoencoders提取的重要质量信息的 Koopman 算子应用。通过这种框架，我们可以有效地捕捉产品质量的总非线性演化，使用传输的线性表示，从而提高数据驱动模型的解释性。为评估SDK框架的性能，我们进行了一项比较研究，用于一个开源数据集。研究结果显示，SDK在MMS中预测Stagewise产品质量方面的准确率高于其他流行的数据驱动模型。此外，SDK在随机潜在空间中的特有线性传播性能，可以跟踪产品质量的演化，从而实现质量演化的跟踪和根本分析方案的设计。值得一提的是，提出的方案不需要对制造线的物理基础知识。它可以作为虚拟测量工具，应用于不同的MMS，为无瑕制造做出贡献。
</details></li>
</ul>
<hr>
<h2 id="Autoencoder-based-Anomaly-Detection-System-for-Online-Data-Quality-Monitoring-of-the-CMS-Electromagnetic-Calorimeter"><a href="#Autoencoder-based-Anomaly-Detection-System-for-Online-Data-Quality-Monitoring-of-the-CMS-Electromagnetic-Calorimeter" class="headerlink" title="Autoencoder-based Anomaly Detection System for Online Data Quality Monitoring of the CMS Electromagnetic Calorimeter"></a>Autoencoder-based Anomaly Detection System for Online Data Quality Monitoring of the CMS Electromagnetic Calorimeter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10157">http://arxiv.org/abs/2309.10157</a></li>
<li>repo_url: None</li>
<li>paper_authors: The CMS ECAL Collaboration</li>
<li>for: 该论文是关于CMS实验室中高能物理数据质量监测的研究。</li>
<li>methods: 该研究使用了一种基于自适应神经网络的异常检测系统，通过使用时间依赖性和空间变化来提高异常检测性能。</li>
<li>results: 该系统能够效率地检测异常，并保持非常低的假阳性率。研究 Validates 该系统的性能通过2018和2022 LHC冲撞数据中的异常检测结果，并在CMS在线数据质量监测工作流中首次部署该系统的结果。<details>
<summary>Abstract</summary>
The CMS detector is a general-purpose apparatus that detects high-energy collisions produced at the LHC. Online Data Quality Monitoring of the CMS electromagnetic calorimeter is a vital operational tool that allows detector experts to quickly identify, localize, and diagnose a broad range of detector issues that could affect the quality of physics data. A real-time autoencoder-based anomaly detection system using semi-supervised machine learning is presented enabling the detection of anomalies in the CMS electromagnetic calorimeter data. A novel method is introduced which maximizes the anomaly detection performance by exploiting the time-dependent evolution of anomalies as well as spatial variations in the detector response. The autoencoder-based system is able to efficiently detect anomalies, while maintaining a very low false discovery rate. The performance of the system is validated with anomalies found in 2018 and 2022 LHC collision data. Additionally, the first results from deploying the autoencoder-based system in the CMS online Data Quality Monitoring workflow during the beginning of Run 3 of the LHC are presented, showing its ability to detect issues missed by the existing system.
</details>
<details>
<summary>摘要</summary>
“CMS探测器是一个通用的实验设备，用于探测在LHC中产生的高能撞击。CMS电磁calorimeter在线时质量监控是实验专家快速识别、定位和诊断各种实验器问题的重要操作工具。这个实时自适应器基于机器学习系统可以实时检测CMS电磁calorimeter数据中的问题。我们引入了一种新的方法，利用时间递增的问题演进以及探测器响应的空间变化，以最大化问题检测性。这个自适应器基于系统能够快速检测问题，同时保持很低的伪阳性率。我们 validate了这个系统的性能，使用2018和2022年LHC撞击数据中的问题。此外，我们还将这个自适应器基于系统在CMS线上质量监控工作流程中的首次应用结果给出，证明它能够检测已有系统所忽略的问题。”
</details></li>
</ul>
<hr>
<h2 id="Realistic-Website-Fingerprinting-By-Augmenting-Network-Trace"><a href="#Realistic-Website-Fingerprinting-By-Augmenting-Network-Trace" class="headerlink" title="Realistic Website Fingerprinting By Augmenting Network Trace"></a>Realistic Website Fingerprinting By Augmenting Network Trace</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10147">http://arxiv.org/abs/2309.10147</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/spin-umass/realistic-website-fingerprinting-by-augmenting-network-traces">https://github.com/spin-umass/realistic-website-fingerprinting-by-augmenting-network-traces</a></li>
<li>paper_authors: Alireza Bahramali, Ardavan Bozorgi, Amir Houmansadr</li>
<li>for: 本研究旨在提高网络识别攻击的实际性，并且挑战了现有的WF攻击方法的假设。</li>
<li>methods: 本研究使用了网络追加技术（NetAugment），这种技术可以帮助WF攻击者在未知的网络条件下进行识别。具体来说，我们使用了半监督和自监督学习技术来实现NetAugment。</li>
<li>results: 我们的实验结果表明，使用了网络追加技术进行WF攻击可以提高攻击的准确率。例如，在一个关闭世界的场景下，我们的自监督WF攻击（NetCLR）在评估 traces 是未知的情况下达到了80%的准确率，而现有的Triplet Fingerprinting方法只达到了64.4%的准确率。<details>
<summary>Abstract</summary>
Website Fingerprinting (WF) is considered a major threat to the anonymity of Tor users (and other anonymity systems). While state-of-the-art WF techniques have claimed high attack accuracies, e.g., by leveraging Deep Neural Networks (DNN), several recent works have questioned the practicality of such WF attacks in the real world due to the assumptions made in the design and evaluation of these attacks. In this work, we argue that such impracticality issues are mainly due to the attacker's inability in collecting training data in comprehensive network conditions, e.g., a WF classifier may be trained only on samples collected on specific high-bandwidth network links but deployed on connections with different network conditions. We show that augmenting network traces can enhance the performance of WF classifiers in unobserved network conditions. Specifically, we introduce NetAugment, an augmentation technique tailored to the specifications of Tor traces. We instantiate NetAugment through semi-supervised and self-supervised learning techniques. Our extensive open-world and close-world experiments demonstrate that under practical evaluation settings, our WF attacks provide superior performances compared to the state-of-the-art; this is due to their use of augmented network traces for training, which allows them to learn the features of target traffic in unobserved settings. For instance, with a 5-shot learning in a closed-world scenario, our self-supervised WF attack (named NetCLR) reaches up to 80% accuracy when the traces for evaluation are collected in a setting unobserved by the WF adversary. This is compared to an accuracy of 64.4% achieved by the state-of-the-art Triplet Fingerprinting [35]. We believe that the promising results of our work can encourage the use of network trace augmentation in other types of network traffic analysis.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Geometric-Framework-for-Neural-Feature-Learning"><a href="#A-Geometric-Framework-for-Neural-Feature-Learning" class="headerlink" title="A Geometric Framework for Neural Feature Learning"></a>A Geometric Framework for Neural Feature Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10140">http://arxiv.org/abs/2309.10140</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiangxiangxu/nfe">https://github.com/xiangxiangxu/nfe</a></li>
<li>paper_authors: Xiangxiang Xu, Lizhong Zheng</li>
<li>for: 这种Framework用于学习系统设计，基于神经元特征提取器，利用特征空间的几何结构。</li>
<li>methods: 该 Framework使用特征几何来解决学习问题，包括最佳特征应对、数据样本学习和多变量学习等。</li>
<li>results: 该 Framework可以应用于现有网络架构和优化器，并可以解释经典方法的连接。<details>
<summary>Abstract</summary>
We present a novel framework for learning system design based on neural feature extractors by exploiting geometric structures in feature spaces. First, we introduce the feature geometry, which unifies statistical dependence and features in the same functional space with geometric structures. By applying the feature geometry, we formulate each learning problem as solving the optimal feature approximation of the dependence component specified by the learning setting. We propose a nesting technique for designing learning algorithms to learn the optimal features from data samples, which can be applied to off-the-shelf network architectures and optimizers. To demonstrate the application of the nesting technique, we further discuss multivariate learning problems, including conditioned inference and multimodal learning, where we present the optimal features and reveal their connections to classical approaches.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的系统设计框架，基于神经特征提取器来利用特征空间的几何结构。首先，我们介绍了特征几何，这将统一统计依赖和特征在同一功能空间中的几何结构。通过应用特征几何，我们将每个学习问题解释为在依赖组件指定的学习设置中解决最佳特征近似问题。我们提议一种嵌套技术，用于从数据样本中学习最佳特征，可以应用于现有的网络架构和优化器。为了证明嵌套技术的应用，我们进一步讨论了多变量学习问题，包括受条件推理和多模态学习，并介绍了最佳特征和其与经典方法的连接。
</details></li>
</ul>
<hr>
<h2 id="Deep-smoothness-WENO-scheme-for-two-dimensional-hyperbolic-conservation-laws-A-deep-learning-approach-for-learning-smoothness-indicators"><a href="#Deep-smoothness-WENO-scheme-for-two-dimensional-hyperbolic-conservation-laws-A-deep-learning-approach-for-learning-smoothness-indicators" class="headerlink" title="Deep smoothness WENO scheme for two-dimensional hyperbolic conservation laws: A deep learning approach for learning smoothness indicators"></a>Deep smoothness WENO scheme for two-dimensional hyperbolic conservation laws: A deep learning approach for learning smoothness indicators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10117">http://arxiv.org/abs/2309.10117</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tatiana Kossaczká, Ameya D. Jagtap, Matthias Ehrhardt</li>
<li>for: 提高两个维度的欧拉方程的精度，特别是在尖型冲击和薄层扩散区域</li>
<li>methods: 通过对Weighted Essentially Non-Oscillatory（WENO）方法中的稳定指标进行深度学习修改，以提高数值解的准确性</li>
<li>results: 在多个文献中的测试问题上，新方法比传统的第五级WENO方法更高精度，特别是在尖型冲击和薄层扩散区域存在过度散射或超过冲击的情况下。<details>
<summary>Abstract</summary>
In this paper, we introduce an improved version of the fifth-order weighted essentially non-oscillatory (WENO) shock-capturing scheme by incorporating deep learning techniques. The established WENO algorithm is improved by training a compact neural network to adjust the smoothness indicators within the WENO scheme. This modification enhances the accuracy of the numerical results, particularly near abrupt shocks. Unlike previous deep learning-based methods, no additional post-processing steps are necessary for maintaining consistency. We demonstrate the superiority of our new approach using several examples from the literature for the two-dimensional Euler equations of gas dynamics. Through intensive study of these test problems, which involve various shocks and rarefaction waves, the new technique is shown to outperform traditional fifth-order WENO schemes, especially in cases where the numerical solutions exhibit excessive diffusion or overshoot around shocks.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种基于深度学习技术改进的第五阶重量核非抽象冲击捕捉算法（WENO）。我们在WENO算法中添加了一个紧凑型神经网络，以调整WENO算法中的平滑指标。这种修改可以提高计算结果的准确性，特别是在强冲击的情况下。与过去的深度学习基于方法不同，我们的新方法不需要额外的后处理步骤，以保持一致性。我们通过对文献中的几个测试问题进行广泛的研究，包括各种冲击和薄层振荡，证明了我们的新方法在冲击强度较大的情况下表现更好，特别是在计算结果中出现过度散射或过度强化的情况下。
</details></li>
</ul>
<hr>
<h2 id="A-Semi-Supervised-Approach-for-Power-System-Event-Identification"><a href="#A-Semi-Supervised-Approach-for-Power-System-Event-Identification" class="headerlink" title="A Semi-Supervised Approach for Power System Event Identification"></a>A Semi-Supervised Approach for Power System Event Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10095">http://arxiv.org/abs/2309.10095</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nima Taghipourbazargani, Lalitha Sankar, Oliver Kosut</li>
<li>for: 提高电力系统可靠性、安全性和稳定性，使用数据科学技术进行数据驱动事件识别。</li>
<li>methods: 使用 semi-supervised 学习技术，利用标注和无标注样本进行事件识别。</li>
<li>results: 对四类事件的识别性能显著提高，与只使用少量标注样本相比， graph-based LS 方法表现最佳。<details>
<summary>Abstract</summary>
Event identification is increasingly recognized as crucial for enhancing the reliability, security, and stability of the electric power system. With the growing deployment of Phasor Measurement Units (PMUs) and advancements in data science, there are promising opportunities to explore data-driven event identification via machine learning classification techniques. However, obtaining accurately-labeled eventful PMU data samples remains challenging due to its labor-intensive nature and uncertainty about the event type (class) in real-time. Thus, it is natural to use semi-supervised learning techniques, which make use of both labeled and unlabeled samples. %We propose a novel semi-supervised framework to assess the effectiveness of incorporating unlabeled eventful samples to enhance existing event identification methodologies. We evaluate three categories of classical semi-supervised approaches: (i) self-training, (ii) transductive support vector machines (TSVM), and (iii) graph-based label spreading (LS) method. Our approach characterizes events using physically interpretable features extracted from modal analysis of synthetic eventful PMU data. In particular, we focus on the identification of four event classes whose identification is crucial for grid operations. We have developed and publicly shared a comprehensive Event Identification package which consists of three aspects: data generation, feature extraction, and event identification with limited labels using semi-supervised methodologies. Using this package, we generate and evaluate eventful PMU data for the South Carolina synthetic network. Our evaluation consistently demonstrates that graph-based LS outperforms the other two semi-supervised methods that we consider, and can noticeably improve event identification performance relative to the setting with only a small number of labeled samples.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>电力系统中的事件识别日益被认为是提高系统可靠性、安全性和稳定性的关键。随着phasor Measurement Units（PMUs）的广泛部署和数据科学技术的进步，有希望通过机器学习分类技术来探索数据驱动的事件识别。然而，在实时获得正确标注的事件ful PMU数据样本上存在劳动 INTENSIVE和未知事件类型的问题。因此，使用半supervised学习技术，这些技术使用标注和未标注样本。 %We propose a novel semi-supervised framework to assess the effectiveness of incorporating unlabeled eventful samples to enhance existing event identification methodologies. We evaluate three categories of classical semi-supervised approaches: (i) self-training, (ii) transductive support vector machines (TSVM), and (iii) graph-based label spreading (LS) method. Our approach characterizes events using physically interpretable features extracted from modal analysis of synthetic eventful PMU data. In particular, we focus on the identification of four event classes whose identification is crucial for grid operations. We have developed and publicly shared a comprehensive Event Identification package which consists of three aspects: data generation, feature extraction, and event identification with limited labels using semi-supervised methodologies. Using this package, we generate and evaluate eventful PMU data for the South Carolina synthetic network. Our evaluation consistently demonstrates that graph-based LS outperforms the other two semi-supervised methods that we consider, and can noticeably improve event identification performance relative to the setting with only a small number of labeled samples.
</details></li>
</ul>
<hr>
<h2 id="Invariant-Probabilistic-Prediction"><a href="#Invariant-Probabilistic-Prediction" class="headerlink" title="Invariant Probabilistic Prediction"></a>Invariant Probabilistic Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10083">http://arxiv.org/abs/2309.10083</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alexanderhenzi/ipp">https://github.com/alexanderhenzi/ipp</a></li>
<li>paper_authors: Alexander Henzi, Xinwei Shen, Michael Law, Peter Bühlmann</li>
<li>for: 本研究旨在探讨在数据分布变化下，使用统计方法实现robust性和不变性。</li>
<li>methods: 该文使用了一种 causality-inspired 框架，研究了probabilistic predictions 的不变性和robust性，并提出了一种可以在不同数据分布下实现不变性的方法。</li>
<li>results: 研究发现，在一般情况下，arbitrary distribution shifts 不会导致 invariant和robust probabilistic predictions，与点预测相比，probabilistic predictions 在不同数据分布下的性能更强。文章还提出了一种方法来实现不变性，并进行了对实验数据的验证。<details>
<summary>Abstract</summary>
In recent years, there has been a growing interest in statistical methods that exhibit robust performance under distribution changes between training and test data. While most of the related research focuses on point predictions with the squared error loss, this article turns the focus towards probabilistic predictions, which aim to comprehensively quantify the uncertainty of an outcome variable given covariates. Within a causality-inspired framework, we investigate the invariance and robustness of probabilistic predictions with respect to proper scoring rules. We show that arbitrary distribution shifts do not, in general, admit invariant and robust probabilistic predictions, in contrast to the setting of point prediction. We illustrate how to choose evaluation metrics and restrict the class of distribution shifts to allow for identifiability and invariance in the prototypical Gaussian heteroscedastic linear model. Motivated by these findings, we propose a method to yield invariant probabilistic predictions, called IPP, and study the consistency of the underlying parameters. Finally, we demonstrate the empirical performance of our proposed procedure on simulated as well as on single-cell data.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:近年来，有增长的兴趣在统计方法中具有对分布变化的鲁棒性。大多数相关的研究集中在点预测中，使用平方误差损失。然而，本文将注意力转移到 probabilistic 预测，它们意味着对输出变量的不确定性进行全面评估。在 causality 框架下，我们调查 probabilistic 预测的一致性和鲁棒性，使用合适的 scoring rule。我们发现，在一般情况下，不同分布变换不会导致鲁棒和一致的 probabilistic 预测，与点预测的情况不同。我们介绍如何选择评估指标和限制分布变换的类型，以便在 Gaussian 不同梯度线性模型中实现可识别性和一致性。为了实现这一目标，我们提出了一种名为 IPP 的方法，并研究其下面的参数一致性。最后，我们通过 simulations 和单元细胞数据进行了实验性评估。
</details></li>
</ul>
<hr>
<h2 id="A-Unifying-Perspective-on-Non-Stationary-Kernels-for-Deeper-Gaussian-Processes"><a href="#A-Unifying-Perspective-on-Non-Stationary-Kernels-for-Deeper-Gaussian-Processes" class="headerlink" title="A Unifying Perspective on Non-Stationary Kernels for Deeper Gaussian Processes"></a>A Unifying Perspective on Non-Stationary Kernels for Deeper Gaussian Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10068">http://arxiv.org/abs/2309.10068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcus M. Noack, Hengrui Luo, Mark D. Risser</li>
<li>for: 本文旨在帮助机器学习实践者更好地理解非站立性的概率过程（Gaussian Process）中的非站立性形式，并提出一种新的kernel函数，以提高非站立性的预测性和不确定性评估。</li>
<li>methods: 本文使用了多种常见的非站立性kernels，并且对它们的性质进行了仔细的研究和比较，以挖掘它们的优点和缺点。</li>
<li>results: 本文通过使用不同的数据集和kernels进行了丰富的实践和比较，并发现了一些非站立性kernels的优点和缺点。基于这些发现，本文提出了一种新的kernel函数，以提高非站立性预测的准确性和不确定性评估。<details>
<summary>Abstract</summary>
The Gaussian process (GP) is a popular statistical technique for stochastic function approximation and uncertainty quantification from data. GPs have been adopted into the realm of machine learning in the last two decades because of their superior prediction abilities, especially in data-sparse scenarios, and their inherent ability to provide robust uncertainty estimates. Even so, their performance highly depends on intricate customizations of the core methodology, which often leads to dissatisfaction among practitioners when standard setups and off-the-shelf software tools are being deployed. Arguably the most important building block of a GP is the kernel function which assumes the role of a covariance operator. Stationary kernels of the Mat\'ern class are used in the vast majority of applied studies; poor prediction performance and unrealistic uncertainty quantification are often the consequences. Non-stationary kernels show improved performance but are rarely used due to their more complicated functional form and the associated effort and expertise needed to define and tune them optimally. In this perspective, we want to help ML practitioners make sense of some of the most common forms of non-stationarity for Gaussian processes. We show a variety of kernels in action using representative datasets, carefully study their properties, and compare their performances. Based on our findings, we propose a new kernel that combines some of the identified advantages of existing kernels.
</details>
<details>
<summary>摘要</summary>
Gaussian process (GP) 是一种广泛使用的统计技术，用于数据不确定性评估和函数近似。在过去二十年中，GP 被机器学习领域采纳，因为它在数据稀缺的情况下表现出色，并且自然地提供了稳健的不确定性估计。然而，GP 的性能往往取决于核函数的细腻定制，这经常导致实践者在使用标准设置和商业化软件工具时感到不满。核函数是 GP 中最重要的构建块，它扮演了 covariance 算子的角色。在大多数应用研究中，使用 Stationary 核函数，但这些核函数的预测性能不佳，而且不符合实际情况。非站ARY 核函数可以提高性能，但它们的函数形式更复杂，需要更多的定制和优化。在这篇视点中，我们想帮助机器学习实践者理解 GP 中一些最常见的非站ARY 性。我们使用代表性的数据集，详细研究核函数的性质，并比较它们的性能。根据我们的发现，我们提出了一种新的核函数，它结合了一些已知核函数的优点。
</details></li>
</ul>
<hr>
<h2 id="Dual-Student-Networks-for-Data-Free-Model-Stealing"><a href="#Dual-Student-Networks-for-Data-Free-Model-Stealing" class="headerlink" title="Dual Student Networks for Data-Free Model Stealing"></a>Dual Student Networks for Data-Free Model Stealing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10058">http://arxiv.org/abs/2309.10058</a></li>
<li>repo_url: None</li>
<li>paper_authors: James Beetham, Navid Kardan, Ajmal Mian, Mubarak Shah</li>
<li>for: 提高数据预processing中的模型骚乱攻击 robustness</li>
<li>methods: 提出了一种基于两个学生模型的 dual student 方法，通过培养两个学生模型来提供生成器模型生成样本的依据，并通过对两个学生模型的分歧来鼓励生成器模型生成更多的样本空间</li>
<li>results: 实验结果表明，对于数据预processing中的模型骚乱攻击，我们的方法可以提供更高的鲁棒性和更好的攻击效果，同时也可以减少查询量和训练计算成本Here is the simplified Chinese text:</li>
<li>for: 提高数据预处理中模型骚乱攻击robustness</li>
<li>methods: 基于两个学生模型的 dual student方法，通过培养两个学生模型来提供生成器模型生成样本的依据，并通过对两个学生模型的分歧来鼓励生成器模型生成更多的样本空间</li>
<li>results: 实验结果表明，对于数据预处理中模型骚乱攻击，我们的方法可以提供更高的鲁棒性和更好的攻击效果，同时也可以减少查询量和训练计算成本<details>
<summary>Abstract</summary>
Existing data-free model stealing methods use a generator to produce samples in order to train a student model to match the target model outputs. To this end, the two main challenges are estimating gradients of the target model without access to its parameters, and generating a diverse set of training samples that thoroughly explores the input space. We propose a Dual Student method where two students are symmetrically trained in order to provide the generator a criterion to generate samples that the two students disagree on. On one hand, disagreement on a sample implies at least one student has classified the sample incorrectly when compared to the target model. This incentive towards disagreement implicitly encourages the generator to explore more diverse regions of the input space. On the other hand, our method utilizes gradients of student models to indirectly estimate gradients of the target model. We show that this novel training objective for the generator network is equivalent to optimizing a lower bound on the generator's loss if we had access to the target model gradients. We show that our new optimization framework provides more accurate gradient estimation of the target model and better accuracies on benchmark classification datasets. Additionally, our approach balances improved query efficiency with training computation cost. Finally, we demonstrate that our method serves as a better proxy model for transfer-based adversarial attacks than existing data-free model stealing methods.
</details>
<details>
<summary>摘要</summary>
existed 无数据模型偷窃方法使用一个生成器生成样本，以训练一个学生模型与目标模型输出匹配。为此，两个主要挑战是无法估计目标模型参数的梯度，以及生成具有很好的覆盖度的训练样本。我们提议一种双学生方法，其中两个学生在相互对应的情况下受训。如果两个学生对某个样本表示不同意，那么至少有一个学生将该样本错误地分类为目标模型。这种启发性偏好探索更多的输入空间。另一方面，我们利用学生模型的梯度来间接估计目标模型的梯度。我们表明，这种新的训练目标函数对生成器网络来说等价于优化一个下界的生成器损失。我们示出，我们的新优化框架提供更准确的目标模型梯度估计和 benchmark 分类数据集上的更高的准确率。此外，我们的方法可以更好地平衡提高查询效率和训练计算成本。最后，我们证明我们的方法在基于转移型敌对攻击的传输基于模型偷窃方法中服为更好的代理模型。
</details></li>
</ul>
<hr>
<h2 id="Actively-Learning-Reinforcement-Learning-A-Stochastic-Optimal-Control-Approach"><a href="#Actively-Learning-Reinforcement-Learning-A-Stochastic-Optimal-Control-Approach" class="headerlink" title="Actively Learning Reinforcement Learning: A Stochastic Optimal Control Approach"></a>Actively Learning Reinforcement Learning: A Stochastic Optimal Control Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10831">http://arxiv.org/abs/2309.10831</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad S. Ramadan, Mahmoud A. Hayajnh, Michael T. Tolley, Kyriakos G. Vamvoudakis</li>
<li>for: 这个论文是为了解决两个问题：（一）控制实验室&#x2F;模拟和现实世界之间的模型不确定性导致的强化学习的脆弱性，以及（二）决策控制的计算成本过高。</li>
<li>methods: 该论文使用强化学习解决了随机动态计划方程的问题，从而获得了一个安全的强化学习控制器，可以自动探索和利用不确定性，并且可以在实时中学习。</li>
<li>results: 例如在模拟示例中，该控制器能够实时学习并适应不同的模型不确定性，并且能够保证控制器的安全性。<details>
<summary>Abstract</summary>
In this paper we provide framework to cope with two problems: (i) the fragility of reinforcement learning due to modeling uncertainties because of the mismatch between controlled laboratory/simulation and real-world conditions and (ii) the prohibitive computational cost of stochastic optimal control. We approach both problems by using reinforcement learning to solve the stochastic dynamic programming equation. The resulting reinforcement learning controller is safe with respect to several types of constraints constraints and it can actively learn about the modeling uncertainties. Unlike exploration and exploitation, probing and safety are employed automatically by the controller itself, resulting real-time learning. A simulation example demonstrates the efficacy of the proposed approach.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提供了一个框架来解决两个问题：（i）控制学习中的模型不确定性导致实验室/模拟环境和实际环境之间的匹配问题，以及（ii） Stochastic Optimal Control 的计算成本过高。我们通过使用 reinforcement learning 解决随机动态程序方程，以获得一个安全的控制器，该控制器可以自动地进行探索和利用，并且可以活动地学习模型不确定性。一个示例在实验中证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="A-Modular-Spatial-Clustering-Algorithm-with-Noise-Specification"><a href="#A-Modular-Spatial-Clustering-Algorithm-with-Noise-Specification" class="headerlink" title="A Modular Spatial Clustering Algorithm with Noise Specification"></a>A Modular Spatial Clustering Algorithm with Noise Specification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10047">http://arxiv.org/abs/2309.10047</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akhil K, Srikanth H R</li>
<li>for: 提高 clustering 算法的精度和快速性，以便更好地处理数据挖掘、机器学习和模式识别等领域中的数据分类问题。</li>
<li>methods: 基于细菌园的生长模型，通过控制细菌的生长和消耗来实现理想的分组准则。模块化设计，可以根据具体任务和数据分布创建特定版本的算法。还提供了适当减少噪声的功能。</li>
<li>results: 提出了一种新的分组算法，即细菌园算法（Bacteria-Farm），它可以平衡性能和参数优化的时间。与其他分组算法相比，该算法具有更好的准确率和鲁棒性。<details>
<summary>Abstract</summary>
Clustering techniques have been the key drivers of data mining, machine learning and pattern recognition for decades. One of the most popular clustering algorithms is DBSCAN due to its high accuracy and noise tolerance. Many superior algorithms such as DBSCAN have input parameters that are hard to estimate. Therefore, finding those parameters is a time consuming process. In this paper, we propose a novel clustering algorithm Bacteria-Farm, which balances the performance and ease of finding the optimal parameters for clustering. Bacteria- Farm algorithm is inspired by the growth of bacteria in closed experimental farms - their ability to consume food and grow - which closely represents the ideal cluster growth desired in clustering algorithms. In addition, the algorithm features a modular design to allow the creation of versions of the algorithm for specific tasks / distributions of data. In contrast with other clustering algorithms, our algorithm also has a provision to specify the amount of noise to be excluded during clustering.
</details>
<details>
<summary>摘要</summary>
translate into Simplified Chinese: clustering 技术已经是数据挖掘、机器学习和模式识别领域的关键驱动者，数十年来。DBSCAN 算法是最受欢迎的一种，因为它具有高度准确性和噪声忍容性。然而，许多更高级的算法，如 DBSCAN，具有难以估算的输入参数。因此，查找这些参数是一项时间consuming 的过程。在这篇论文中，我们提出了一种新的 clustering 算法，叫做 Bacteria-Farm，它可以平衡性能和找到最佳参数的易用性。Bacteria-Farm 算法是通过closed experimental farms 中细菌的生长和增长来 inspirited 的，这种生长模式与 clustering 算法中理想的群集生长非常相似。此外，算法还具有可重新配置的模块化设计，以便为特定任务/数据分布创建版本。与其他 clustering 算法不同，我们的算法还具有排除噪声的功能。
</details></li>
</ul>
<hr>
<h2 id="A-Multi-Token-Coordinate-Descent-Method-for-Semi-Decentralized-Vertical-Federated-Learning"><a href="#A-Multi-Token-Coordinate-Descent-Method-for-Semi-Decentralized-Vertical-Federated-Learning" class="headerlink" title="A Multi-Token Coordinate Descent Method for Semi-Decentralized Vertical Federated Learning"></a>A Multi-Token Coordinate Descent Method for Semi-Decentralized Vertical Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09977">http://arxiv.org/abs/2309.09977</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pedro Valdeira, Yuejie Chi, Cláudia Soares, João Xavier</li>
<li>For: 采用 Multi-Token Coordinate Descent (MTCD) 算法进行 semi-decentralized  вертикального联合学习，提高通信效率。* Methods: 利用客户端-服务器和客户端-客户端通信，每个客户端持有小subset的特征进行并行 Markov chain (block) coordinate descent 算法。* Results: 实现了 $\mathcal{O}(1&#x2F;T)$ 的收敛速率 для非对称目标函数，并且可以控制并行通信的数量。<details>
<summary>Abstract</summary>
Communication efficiency is a major challenge in federated learning (FL). In client-server schemes, the server constitutes a bottleneck, and while decentralized setups spread communications, they do not necessarily reduce them due to slower convergence. We propose Multi-Token Coordinate Descent (MTCD), a communication-efficient algorithm for semi-decentralized vertical federated learning, exploiting both client-server and client-client communications when each client holds a small subset of features. Our multi-token method can be seen as a parallel Markov chain (block) coordinate descent algorithm and it subsumes the client-server and decentralized setups as special cases. We obtain a convergence rate of $\mathcal{O}(1/T)$ for nonconvex objectives when tokens roam over disjoint subsets of clients and for convex objectives when they roam over possibly overlapping subsets. Numerical results show that MTCD improves the state-of-the-art communication efficiency and allows for a tunable amount of parallel communications.
</details>
<details>
<summary>摘要</summary>
通信效率是联邦学习（FL）的主要挑战。在客户端服务器方案中，服务器成为瓶颈，而分散式设计可以分散通信，但不一定可以减少它们，因为它们的减少速度较慢。我们提出了多token坐标降低（MTCD）算法，用于半分散式垂直联邦学习，利用每个客户端持有的小subset特征来实现高效的通信。我们的多token方法可以看作是并行Markov链（块）坐标降低算法，它包含客户端服务器和分散式设计的特殊情况。我们得到了非对称目标函数的 $\mathcal{O}(1/T)$ 收敛率，并且数学实验表明，MTCD可以提高当前最佳的通信效率，并允许调整的并行通信数量。
</details></li>
</ul>
<hr>
<h2 id="Des-q-a-quantum-algorithm-to-construct-and-efficiently-retrain-decision-trees-for-regression-and-binary-classification"><a href="#Des-q-a-quantum-algorithm-to-construct-and-efficiently-retrain-decision-trees-for-regression-and-binary-classification" class="headerlink" title="Des-q: a quantum algorithm to construct and efficiently retrain decision trees for regression and binary classification"></a>Des-q: a quantum algorithm to construct and efficiently retrain decision trees for regression and binary classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09976">http://arxiv.org/abs/2309.09976</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niraj Kumar, Romina Yalovetzky, Changhao Li, Pierre Minssen, Marco Pistoia</li>
<li>For:  This paper proposes a novel quantum algorithm for constructing and retraining decision trees in regression and binary classification tasks, with the goal of significantly reducing the time required for tree retraining.* Methods: The proposed algorithm, named Des-q, uses a quantum-accessible memory to efficiently estimate feature weights and perform k-piecewise linear tree splits at each internal node. It also employs a quantum-supervised clustering method based on the q-means algorithm to determine the k suitable anchor points for these splits.* Results: The simulated version of the Des-q algorithm is benchmarked against the state-of-the-art classical decision tree for regression and binary classification on multiple data sets with numerical features, and is shown to exhibit similar performance while significantly speeding up the periodic tree retraining.<details>
<summary>Abstract</summary>
Decision trees are widely used in machine learning due to their simplicity in construction and interpretability. However, as data sizes grow, traditional methods for constructing and retraining decision trees become increasingly slow, scaling polynomially with the number of training examples. In this work, we introduce a novel quantum algorithm, named Des-q, for constructing and retraining decision trees in regression and binary classification tasks. Assuming the data stream produces small increments of new training examples, we demonstrate that our Des-q algorithm significantly reduces the time required for tree retraining, achieving a poly-logarithmic time complexity in the number of training examples, even accounting for the time needed to load the new examples into quantum-accessible memory. Our approach involves building a decision tree algorithm to perform k-piecewise linear tree splits at each internal node. These splits simultaneously generate multiple hyperplanes, dividing the feature space into k distinct regions. To determine the k suitable anchor points for these splits, we develop an efficient quantum-supervised clustering method, building upon the q-means algorithm of Kerenidis et al. Des-q first efficiently estimates each feature weight using a novel quantum technique to estimate the Pearson correlation. Subsequently, we employ weighted distance estimation to cluster the training examples in k disjoint regions and then proceed to expand the tree using the same procedure. We benchmark the performance of the simulated version of our algorithm against the state-of-the-art classical decision tree for regression and binary classification on multiple data sets with numerical features. Further, we showcase that the proposed algorithm exhibits similar performance to the state-of-the-art decision tree while significantly speeding up the periodic tree retraining.
</details>
<details>
<summary>摘要</summary>
决策树在机器学习中广泛使用，因其建构和解释性很好。但是，随着数据集的增大，传统的决策树建构和重新训练方法会变得越来越慢，时间复杂度随着训练示例数量的增长而呈极函数关系。在这种情况下，我们提出了一种新的量子算法，名为Des-q，用于在回归和二分类任务中建构和重新训练决策树。假设数据流量产生小量的新训练示例，我们示出了Des-q算法可以在训练示例数量的极函数时间复杂度下，对决策树进行重新训练，而不是在训练示例数量的几乎方差时间复杂度下。我们的方法是在每个内部节点上使用k个 piecewise 线性树split，这些拆分同时生成多个抽象。为确定k个适当的吊革点，我们开发了一种高效的量子监督学习方法，基于kerenidis等人提出的q-means算法。Des-q首先高效地估计每个特征的权重，使用一种新的量子技术来估计pearson相关性。然后，我们使用质量距离估计来归类训练示例，并将其分为k个不同的区域。最后，我们使用同样的过程来扩展树。我们使用模拟版的算法对比州时的分类树的表现，并证明Des-q算法在多个数据集上的numerical特征上 exhibits similar performance，同时具有明显的时间复杂度优化。此外，我们还显示了Des-q算法在 periodic tree retraining 中的性能，并证明它在训练示例数量的增长中保持稳定性。
</details></li>
</ul>
<hr>
<h2 id="Empirical-Study-of-Mix-based-Data-Augmentation-Methods-in-Physiological-Time-Series-Data"><a href="#Empirical-Study-of-Mix-based-Data-Augmentation-Methods-in-Physiological-Time-Series-Data" class="headerlink" title="Empirical Study of Mix-based Data Augmentation Methods in Physiological Time Series Data"></a>Empirical Study of Mix-based Data Augmentation Methods in Physiological Time Series Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09970">http://arxiv.org/abs/2309.09970</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/comp-well-org/mix-augmentation-for-physiological-time-series-classification">https://github.com/comp-well-org/mix-augmentation-for-physiological-time-series-classification</a></li>
<li>paper_authors: Peikun Guo, Huiyuan Yang, Akane Sano</li>
<li>for: 这个论文主要是为了探讨在生理时间序分类任务中使用mixup等混合基于数据增强技术的可能性和效果。</li>
<li>methods: 这个论文使用了多种mix-based数据增强技术，包括mixup、cutmix和替换混合，对六个生理时间序数据集进行了系统性的评估，以确定这些技术在不同的感知数据和分类任务中的表现。</li>
<li>results: 研究结果表明，三种mix-based数据增强技术可以在六个生理时间序数据集上提高表现，而且这些改进不需要专家知识或广泛的参数调整。<details>
<summary>Abstract</summary>
Data augmentation is a common practice to help generalization in the procedure of deep model training. In the context of physiological time series classification, previous research has primarily focused on label-invariant data augmentation methods. However, another class of augmentation techniques (\textit{i.e., Mixup}) that emerged in the computer vision field has yet to be fully explored in the time series domain. In this study, we systematically review the mix-based augmentations, including mixup, cutmix, and manifold mixup, on six physiological datasets, evaluating their performance across different sensory data and classification tasks. Our results demonstrate that the three mix-based augmentations can consistently improve the performance on the six datasets. More importantly, the improvement does not rely on expert knowledge or extensive parameter tuning. Lastly, we provide an overview of the unique properties of the mix-based augmentation methods and highlight the potential benefits of using the mix-based augmentation in physiological time series data.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language="zh-CN"<</SYS>>数据扩充是一种常见的方法来帮助深度模型训练过程中的泛化。在生理时间序列分类领域，先前的研究主要集中在标签不变的数据扩充方法上。然而，另一类 augmentation 技术（即 Mixup）在计算机视觉领域出现后，尚未在时间序列领域得到完全探索。在这种研究中，我们系统地评估了基于混合的扩充方法，包括 mixup、cutmix 和 manifold mixup，在六个生理时间序列 dataset 上，并评估了不同的感知数据和分类任务中的性能。我们的结果表明，三种混合基于的扩充方法可以一致地提高六个 dataset 的性能。此外，这些改进不需要专家知识或广泛的参数调整。最后，我们介绍了混合基于的扩充方法的独特性质，并强调了在生理时间序列数据中使用混合基于的扩充方法的潜在优势。
</details></li>
</ul>
<hr>
<h2 id="Prompt-a-Robot-to-Walk-with-Large-Language-Models"><a href="#Prompt-a-Robot-to-Walk-with-Large-Language-Models" class="headerlink" title="Prompt a Robot to Walk with Large Language Models"></a>Prompt a Robot to Walk with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09969">http://arxiv.org/abs/2309.09969</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HybridRobotics/prompt2walk">https://github.com/HybridRobotics/prompt2walk</a></li>
<li>paper_authors: Yen-Jen Wang, Bike Zhang, Jianyu Chen, Koushil Sreenath</li>
<li>for: 这个研究旨在使用几何提示来将大型自然语言模型（LLM）应用于机器人控制中。</li>
<li>methods: 这个研究使用了几何提示收集自物理环境，并使用了LLM进行循环预测控制命令。</li>
<li>results: 实验结果显示，这个方法可以有效地将机器人诱导到行走。这证明了LLM可以作为机器人动作控制中的低层反馈控制器。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Large language models (LLMs) pre-trained on vast internet-scale data have showcased remarkable capabilities across diverse domains. Recently, there has been escalating interest in deploying LLMs for robotics, aiming to harness the power of foundation models in real-world settings. However, this approach faces significant challenges, particularly in grounding these models in the physical world and in generating dynamic robot motions. To address these issues, we introduce a novel paradigm in which we use few-shot prompts collected from the physical environment, enabling the LLM to autoregressively generate low-level control commands for robots without task-specific fine-tuning. Experiments across various robots and environments validate that our method can effectively prompt a robot to walk. We thus illustrate how LLMs can proficiently function as low-level feedback controllers for dynamic motion control even in high-dimensional robotic systems. The project website and source code can be found at: https://prompt2walk.github.io/ .
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在互联网规模数据上预训练后表现出了各种各样的能力。近期，有越来越多的人们对于使用 LLM 在实际场景中进行部署表示了极大的兴趣。然而，这种方法面临着 significativetranslation challenges，特别是在将模型 anchored 到物理世界中和生成动态机器人运动。为了解决这些问题，我们提出了一种新的思路，即通过几个shot的提示从物理环境中收集，使 LLM 可以自动生成机器人的低级控制命令，无需特定任务的微调。经过对各种机器人和环境的实验，我们发现我们的方法可以有效地使机器人行走。我们因此证明了 LLM 可以在高维机器人系统中作为低级反馈控制器，进行动态运动控制。项目官网和代码可以在以下链接中找到：https://prompt2walk.github.io/。
</details></li>
</ul>
<hr>
<h2 id="Generating-and-Imputing-Tabular-Data-via-Diffusion-and-Flow-based-Gradient-Boosted-Trees"><a href="#Generating-and-Imputing-Tabular-Data-via-Diffusion-and-Flow-based-Gradient-Boosted-Trees" class="headerlink" title="Generating and Imputing Tabular Data via Diffusion and Flow-based Gradient-Boosted Trees"></a>Generating and Imputing Tabular Data via Diffusion and Flow-based Gradient-Boosted Trees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09968">http://arxiv.org/abs/2309.09968</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexia Jolicoeur-Martineau, Kilian Fatras, Tal Kachman</li>
<li>for: 生成和填充混合类型（连续和分类）表格数据</li>
<li>methods: 使用分布式扩散和条件流匹配生成混合类型表格数据，而不是使用前一些工作中的神经网络函数近似器</li>
<li>results: 在不同的数据集上，我们的方法可以生成高度真实的人工数据，并且可以生成多种可能的数据填充结果，经验显示我们的方法通常超越深度学习生成方法，可以在CPU上并行训练而无需GPU，我们将代码发布到PyPI和CRAN上。<details>
<summary>Abstract</summary>
Tabular data is hard to acquire and is subject to missing values. This paper proposes a novel approach to generate and impute mixed-type (continuous and categorical) tabular data using score-based diffusion and conditional flow matching. Contrary to previous work that relies on neural networks as function approximators, we instead utilize XGBoost, a popular Gradient-Boosted Tree (GBT) method. In addition to being elegant, we empirically show on various datasets that our method i) generates highly realistic synthetic data when the training dataset is either clean or tainted by missing data and ii) generates diverse plausible data imputations. Our method often outperforms deep-learning generation methods and can trained in parallel using CPUs without the need for a GPU. To make it easily accessible, we release our code through a Python library on PyPI and an R package on CRAN.
</details>
<details>
<summary>摘要</summary>
<font size="4">表格数据具有困难和缺失值。这篇论文提出了一种新的方法，使用分数基diffusion和条件流匹配生成和填充混合类型（连续和分类）表格数据。与之前的工作不同，我们不使用神经网络作为函数估计器，而是使用XGBoost，一种受欢迎的梯度增强树（GBT）方法。我们的方法不仅简洁高效，而且在不同的数据集上经验表明，我们的方法可以生成高度真实的人工数据，并且可以生成多种可能的数据填充。我们的方法经常超过深度学习生成方法，并且可以在CPU上并行训练，不需要GPU。为便于使用，我们在Python库中发布了代码，并在CRAN上发布了R包。</font>Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Adversarial-Robustness-with-Expected-Viable-Performance"><a href="#Evaluating-Adversarial-Robustness-with-Expected-Viable-Performance" class="headerlink" title="Evaluating Adversarial Robustness with Expected Viable Performance"></a>Evaluating Adversarial Robustness with Expected Viable Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09928">http://arxiv.org/abs/2309.09928</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryan McCoppin, Colin Dawson, Sean M. Kennedy, Leslie M. Blaha</li>
<li>for: 评估预测模型的可靠性，特别关注对抗性扰动的影响。</li>
<li>methods: 使用可靠性指标，例如分类精度，来评估模型在不同抗性扰动下的性能。</li>
<li>results: 提出一种基于期望值的预测模型可靠性评估方法，以便更好地评估模型对抗性扰动的抗性性能。<details>
<summary>Abstract</summary>
We introduce a metric for evaluating the robustness of a classifier, with particular attention to adversarial perturbations, in terms of expected functionality with respect to possible adversarial perturbations. A classifier is assumed to be non-functional (that is, has a functionality of zero) with respect to a perturbation bound if a conventional measure of performance, such as classification accuracy, is less than a minimally viable threshold when the classifier is tested on examples from that perturbation bound. Defining robustness in terms of an expected value is motivated by a domain general approach to robustness quantification.
</details>
<details>
<summary>摘要</summary>
我们提出了一种度量分类器的Robustness，特别关注对抗攻击的影响，以期望功能性对可能的攻击 perturbations 的平均值。我们假设分类器对某个 perturbation bound 的测试例而言，如果使用 convential 度量指标，例如分类率，则分类器的性能低于最低可接受水平，则分类器对该 bound 的测试例是不可用的（即其功能性为零）。定义Robustness 以期望值的方式受到Domain 通用的Robustness 评估方法的 inspirations。
</details></li>
</ul>
<hr>
<h2 id="Graph-topological-property-recovery-with-heat-and-wave-dynamics-based-features-on-graphs"><a href="#Graph-topological-property-recovery-with-heat-and-wave-dynamics-based-features-on-graphs" class="headerlink" title="Graph topological property recovery with heat and wave dynamics-based features on graphs"></a>Graph topological property recovery with heat and wave dynamics-based features on graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09924">http://arxiv.org/abs/2309.09924</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhananjay Bhaskar, Yanlei Zhang, Charles Xu, Xingzhi Sun, Oluwadamilola Fasina, Guy Wolf, Maximilian Nickel, Michael Perlmutter, Smita Krishnaswamy</li>
<li>for: 这个论文是为了提出一种基于解决常微分方程的图形网络（GDeNet），用于获取不同下游任务的连续节点和图级表示。</li>
<li>methods: 论文使用了解析解决方法，连接热和波方程的动力学特性和图形的 спектраль性质以及连续时间游走在图形上的行为。</li>
<li>results: 实验表明，这些动力学特性能够捕捉图形的几何和拓扑特征，并且在真实世界数据集上表现出优于其他方法。<details>
<summary>Abstract</summary>
In this paper, we propose Graph Differential Equation Network (GDeNet), an approach that harnesses the expressive power of solutions to PDEs on a graph to obtain continuous node- and graph-level representations for various downstream tasks. We derive theoretical results connecting the dynamics of heat and wave equations to the spectral properties of the graph and to the behavior of continuous-time random walks on graphs. We demonstrate experimentally that these dynamics are able to capture salient aspects of graph geometry and topology by recovering generating parameters of random graphs, Ricci curvature, and persistent homology. Furthermore, we demonstrate the superior performance of GDeNet on real-world datasets including citation graphs, drug-like molecules, and proteins.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了图Diffusion Equation Network（GDeNet），它利用图上解析方程的表达能力来获得不同下游任务的连续节点和图级表示。我们 derive了对热和波方程的动态性和图的spectral properties以及漫步过程的连续性有关的理论结果。我们通过实验表明，这些动态可以捕捉图的几何和topology特征，例如生成参数、Ricci curvature和持续同构。此外，我们在真实世界数据集上证明了GDeNet的优越性。
</details></li>
</ul>
<hr>
<h2 id="Distilling-HuBERT-with-LSTMs-via-Decoupled-Knowledge-Distillation"><a href="#Distilling-HuBERT-with-LSTMs-via-Decoupled-Knowledge-Distillation" class="headerlink" title="Distilling HuBERT with LSTMs via Decoupled Knowledge Distillation"></a>Distilling HuBERT with LSTMs via Decoupled Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09920">http://arxiv.org/abs/2309.09920</a></li>
<li>repo_url: None</li>
<li>paper_authors: Danilo de Oliveira, Timo Gerkmann</li>
<li>for: 压缩 HuBERT 模型的知识，以提高自动语音识别器的性能和储存需求。</li>
<li>methods: 使用知识传递和分离知识传递的方法，将 HuBERT 的 Transformer 层转换为 LSTM 型的压缩模型，以减少参数数量并提高自动语音识别器的性能。</li>
<li>results: 与 DistilHuBERT 相比， proposed 方法可以实现更好的自动语音识别器性能，并且对于储存需求产生更大的改善。<details>
<summary>Abstract</summary>
Much research effort is being applied to the task of compressing the knowledge of self-supervised models, which are powerful, yet large and memory consuming. In this work, we show that the original method of knowledge distillation (and its more recently proposed extension, decoupled knowledge distillation) can be applied to the task of distilling HuBERT. In contrast to methods that focus on distilling internal features, this allows for more freedom in the network architecture of the compressed model. We thus propose to distill HuBERT's Transformer layers into an LSTM-based distilled model that reduces the number of parameters even below DistilHuBERT and at the same time shows improved performance in automatic speech recognition.
</details>
<details>
<summary>摘要</summary>
很多研究力量是用于压缩自动学习模型的知识，这些模型具有强大的能力，但却占用大量内存。在这项工作中，我们显示了知识储存方法（以及其最近提出的扩展方法，即分离知识储存）可以应用于压缩HuBERT。与其他方法不同的是，我们可以在压缩模型网络结构中有更多的自由。因此，我们提议将HuBERT的转换层压缩成一个使用LSTM的压缩模型，以降低参数数量，并且同时在自动语音识别中表现更好。
</details></li>
</ul>
<hr>
<h2 id="Learning-Nonparametric-High-Dimensional-Generative-Models-The-Empirical-Beta-Copula-Autoencoder"><a href="#Learning-Nonparametric-High-Dimensional-Generative-Models-The-Empirical-Beta-Copula-Autoencoder" class="headerlink" title="Learning Nonparametric High-Dimensional Generative Models: The Empirical-Beta-Copula Autoencoder"></a>Learning Nonparametric High-Dimensional Generative Models: The Empirical-Beta-Copula Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09916">http://arxiv.org/abs/2309.09916</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maximilian Coblenz, Oliver Grothe, Fabian Kächele<br>for:* 这个论文的目的是把自动编码器转化为生成模型，并且寻找简单有效的方法来实现这一点。methods:* 论文使用了多种方法来模型自动编码器的幂数空间，包括核密度估计、泊松分布、正规流等。results:* 研究发现，使用泊松分布模型自动编码器的幂数空间可以很好地生成新的数据样本，并且可以控制生成的数据样本具有特定的特征。此外，论文还提供了一种新的copula模型，即Empirical Beta Copula Autoencoder，可以更好地实现生成模型的目的。<details>
<summary>Abstract</summary>
By sampling from the latent space of an autoencoder and decoding the latent space samples to the original data space, any autoencoder can simply be turned into a generative model. For this to work, it is necessary to model the autoencoder's latent space with a distribution from which samples can be obtained. Several simple possibilities (kernel density estimates, Gaussian distribution) and more sophisticated ones (Gaussian mixture models, copula models, normalization flows) can be thought of and have been tried recently. This study aims to discuss, assess, and compare various techniques that can be used to capture the latent space so that an autoencoder can become a generative model while striving for simplicity. Among them, a new copula-based method, the Empirical Beta Copula Autoencoder, is considered. Furthermore, we provide insights into further aspects of these methods, such as targeted sampling or synthesizing new data with specific features.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate english text into simplified chineseBy sampling from the latent space of an autoencoder and decoding the latent space samples to the original data space, any autoencoder can simply be turned into a generative model. For this to work, it is necessary to model the autoencoder's latent space with a distribution from which samples can be obtained. Several simple possibilities (kernel density estimates, Gaussian distribution) and more sophisticated ones (Gaussian mixture models, copula models, normalization flows) can be thought of and have been tried recently. This study aims to discuss, assess, and compare various techniques that can be used to capture the latent space so that an autoencoder can become a generative model while striving for simplicity. Among them, a new copula-based method, the Empirical Beta Copula Autoencoder, is considered. Furthermore, we provide insights into further aspects of these methods, such as targeted sampling or synthesizing new data with specific features. traducción al chino simplificado通过从自编码器的幂 space 中采样并将幂 space 采样转换为原始数据空间，任何自编码器都可以简单地变成生成模型。为了实现这一点，需要模型自编码器的幂空间 distribution 中的样本。一些简单的可能性（kernel density estimates，Gaussian distribution）和更复杂的一些（Gaussian mixture models，copula models，normalization flows）都有被考虑和尝试过。本研究旨在讨论、评估和比较这些方法，以实现将自编码器转化为生成模型，同时寻求简单性。其中，一种新的 copula-based 方法，即 Empirical Beta Copula Autoencoder，被考虑。此外，我们还提供了这些方法的进一步含义，例如针对的采样或生成特定特征的新数据。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Generate-Lumped-Hydrological-Models"><a href="#Learning-to-Generate-Lumped-Hydrological-Models" class="headerlink" title="Learning to Generate Lumped Hydrological Models"></a>Learning to Generate Lumped Hydrological Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09904">http://arxiv.org/abs/2309.09904</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Yang, Ting Fong May Chui</li>
<li>for: 这个研究旨在开发一种数据驱动的方法，用于表征水文功能在流域中的低维度表示，并使用这种表示来重建特定的水文功能。</li>
<li>methods: 这个研究使用深度学习方法来学习流域的水文功能，并直接从气候冲击和流域流量数据中学习出latent variable的值。</li>
<li>results: 研究发现，使用这种方法可以从全球超过3,000个流域的数据中学习出高质量的生成模型，并将这些生成模型应用到700个不同的流域中，得到了比或更好的估计结果。<details>
<summary>Abstract</summary>
In a lumped hydrological model structure, the hydrological function of a catchment is characterized by only a few parameters. Given a set of parameter values, a numerical function useful for hydrological prediction is generated. Thus, this study assumes that the hydrological function of a catchment can be sufficiently well characterized by a small number of latent variables. By specifying the variable values, a numerical function resembling the hydrological function of a real-world catchment can be generated using a generative model. In this study, a deep learning method is used to learn both the generative model and the latent variable values of different catchments directly from their climate forcing and runoff data, without using catchment attributes. The generative models can be used similarly to a lumped model structure, i.e., by estimating the optimal parameter or latent variable values using a generic model calibration algorithm, an optimal numerical model can be derived. In this study, generative models using eight latent variables were learned from data from over 3,000 catchments worldwide, and the learned generative models were applied to model over 700 different catchments using a generic calibration algorithm. The quality of the resulting optimal models was generally comparable to or better than that obtained using 36 different types of lump model structures or using non-generative deep learning methods. In summary, this study presents a data-driven approach for representing the hydrological function of a catchment in low-dimensional space and a method for reconstructing specific hydrological functions from the representations.
</details>
<details>
<summary>摘要</summary>
在汇集型水文模型结构中，湍水功能的某catchment被定义为只有一些参数。给定一组参数值，可以生成一个数值函数用于水文预测。因此，本研究假设catchment的水文功能可以通过一小数量的隐变量足够准确地表示。通过 specifying变量值，可以使用生成模型生成一个数值函数类似于实际世界catchment的水文函数。在本研究中，使用深度学习方法来学习catchment的生成模型和隐变量值，不使用catchment特征。生成模型可以与汇集型模型结构相同地使用，即通过优化参数或隐变量值使用一个通用模型调整算法来获得最佳数值模型。在本研究中，使用八个隐变量学习了来自全球3,000多个catchment的数据，并将学习的生成模型应用到700多个不同的catchment上使用通用调整算法。得到的优化模型质量通常与或更高于使用36种不同的汇集模型结构或非生成型深度学习方法所获得的质量。总之，本研究提出了一种数据驱动的方法，用于表示catchment的水文功能在低维空间中，以及一种方法，用于从表示中重建特定的水文功能。
</details></li>
</ul>
<hr>
<h2 id="Deep-Reinforcement-Learning-for-the-Joint-Control-of-Traffic-Light-Signaling-and-Vehicle-Speed-Advice"><a href="#Deep-Reinforcement-Learning-for-the-Joint-Control-of-Traffic-Light-Signaling-and-Vehicle-Speed-Advice" class="headerlink" title="Deep Reinforcement Learning for the Joint Control of Traffic Light Signaling and Vehicle Speed Advice"></a>Deep Reinforcement Learning for the Joint Control of Traffic Light Signaling and Vehicle Speed Advice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09881">http://arxiv.org/abs/2309.09881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Johannes V. S. Busch, Robert Voelckner, Peter Sossalla, Christian L. Vielhaus, Roberto Calandra, Frank H. P. Fitzek</li>
<li>for: 提高城市堵塞的效率和环保性</li>
<li>methods: 使用深度强化学习控制交通信号灯和车辆行驶速度</li>
<li>results: 在八个 из十一个测试场景中，联合控制方法可以降低车辆旅行延迟，并且观察到车辆速度建议策略可以平滑车辆附近交通信号灯的速度变化。<details>
<summary>Abstract</summary>
Traffic congestion in dense urban centers presents an economical and environmental burden. In recent years, the availability of vehicle-to-anything communication allows for the transmission of detailed vehicle states to the infrastructure that can be used for intelligent traffic light control. The other way around, the infrastructure can provide vehicles with advice on driving behavior, such as appropriate velocities, which can improve the efficacy of the traffic system. Several research works applied deep reinforcement learning to either traffic light control or vehicle speed advice. In this work, we propose a first attempt to jointly learn the control of both. We show this to improve the efficacy of traffic systems. In our experiments, the joint control approach reduces average vehicle trip delays, w.r.t. controlling only traffic lights, in eight out of eleven benchmark scenarios. Analyzing the qualitative behavior of the vehicle speed advice policy, we observe that this is achieved by smoothing out the velocity profile of vehicles nearby a traffic light. Learning joint control of traffic signaling and speed advice in the real world could help to reduce congestion and mitigate the economical and environmental repercussions of today's traffic systems.
</details>
<details>
<summary>摘要</summary>
压力交通在紧张城市中带来经济和环境沉重负担。最近几年，车辆到任何通信技术的可用性允许车辆状态的详细传输到基础设施，以便智能交通灯控制。基础设施也可以为车辆提供适当的行驶方式建议，如 velocities，以改善交通系统的效率。一些研究工作使用深度强化学习控制交通灯或车辆速度。在这种工作中，我们提出了第一次同时学习交通灯控制和车辆速度建议的方法。我们示出这可以提高交通系统的效率。在我们的实验中，同时控制方法比只控制交通灯时间减少了平均车辆旅行延迟，在 eleven 个标准场景中出现了八个情况。分析车辆速度建议政策的Qualitative行为，我们发现这是通过车辆附近交通灯的速度profile的平滑来实现的。在实际世界中学习同时控制交通信号和车辆速度的可能会帮助减少拥堵和今天的交通系统的经济和环境后果。
</details></li>
</ul>
<hr>
<h2 id="Error-Reduction-from-Stacked-Regressions"><a href="#Error-Reduction-from-Stacked-Regressions" class="headerlink" title="Error Reduction from Stacked Regressions"></a>Error Reduction from Stacked Regressions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09880">http://arxiv.org/abs/2309.09880</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Xin Chen, Jason M. Klusowski, Yan Shuo Tan</li>
<li>for: 提高预测精度，使用核算法组合多个回归分析器</li>
<li>methods: 使用非负约束最小二乘法学习组合权重，其中每个回归分析器都是线性最小二乘法项</li>
<li>results: 在嵌入多维空间中，使用核算法组合可以减小人口风险，并且比最佳单个回归分析器更好Here’s a breakdown of each point:* “for”: The paper is written to improve the accuracy of predictions by combining multiple regression analyzers using a nuclear algorithm.* “methods”: The paper uses a non-negative constraint least-squares method to learn the combination weights of the constituent estimators, and the optimization problem can be reformulated as isotonic regression.* “results”: The resulting stacked estimator has a strictly smaller population risk than the best single estimator among them, and it can be implemented with the same order of computation as the best single estimator.<details>
<summary>Abstract</summary>
Stacking regressions is an ensemble technique that forms linear combinations of different regression estimators to enhance predictive accuracy. The conventional approach uses cross-validation data to generate predictions from the constituent estimators, and least-squares with nonnegativity constraints to learn the combination weights. In this paper, we learn these weights analogously by minimizing an estimate of the population risk subject to a nonnegativity constraint. When the constituent estimators are linear least-squares projections onto nested subspaces separated by at least three dimensions, we show that thanks to a shrinkage effect, the resulting stacked estimator has strictly smaller population risk than best single estimator among them. Here ``best'' refers to a model that minimizes a selection criterion such as AIC or BIC. In other words, in this setting, the best single estimator is inadmissible. Because the optimization problem can be reformulated as isotonic regression, the stacked estimator requires the same order of computation as the best single estimator, making it an attractive alternative in terms of both performance and implementation.
</details>
<details>
<summary>摘要</summary>
核 stacking 是一种ensemble技术，通过将不同的回归估计器组合起来，提高预测精度。传统方法使用 Cross-validation 数据生成组合估计器的预测，并使用 least-squares 方法学习组合权重。在这篇论文中，我们通过最小化人口风险的估计器来学习这些权重，并且对这些权重进行非负性约束。当组合估计器是线性最小二乘投影 onto 嵌入在至少三维空间中的子空间时，我们显示了一种减小效果，即核 stacked 估计器的人口风险比最佳单个估计器（按照 AIC 或 BIC 选择 criterion）更小。这里的“最佳”指的是一个模型，可以最小化一个选择 criterion。在这种设定下，最佳单个估计器是不可接受的。因为优化问题可以 reformulated 为iso-tonic regression，核 stacked 估计器需要与最佳单个估计器相同的计算顺序，因此它在性能和实现方面都是一个吸引人的选择。
</details></li>
</ul>
<hr>
<h2 id="Domain-Generalization-with-Fourier-Transform-and-Soft-Thresholding"><a href="#Domain-Generalization-with-Fourier-Transform-and-Soft-Thresholding" class="headerlink" title="Domain Generalization with Fourier Transform and Soft Thresholding"></a>Domain Generalization with Fourier Transform and Soft Thresholding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09866">http://arxiv.org/abs/2309.09866</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongyi Pan, Bin Wang, Zheyuan Zhan, Xin Zhu, Debesh Jha, Ahmet Enis Cetin, Concetto Spampinato, Ulas Bagci</li>
<li>for: 用于提高脑网络模型对不同来源图像的泛化性能</li>
<li>methods: 使用傅리曼变换基于的频谱预处理策略，并 introduce soft-thresholding函数来消除频谱中的背景干扰</li>
<li>results: 通过实验 validate our approach的效果，与传统和现有方法相比，具有较好的 segmentation  metric 和更好的泛化性能<details>
<summary>Abstract</summary>
Domain generalization aims to train models on multiple source domains so that they can generalize well to unseen target domains. Among many domain generalization methods, Fourier-transform-based domain generalization methods have gained popularity primarily because they exploit the power of Fourier transformation to capture essential patterns and regularities in the data, making the model more robust to domain shifts. The mainstream Fourier-transform-based domain generalization swaps the Fourier amplitude spectrum while preserving the phase spectrum between the source and the target images. However, it neglects background interference in the amplitude spectrum. To overcome this limitation, we introduce a soft-thresholding function in the Fourier domain. We apply this newly designed algorithm to retinal fundus image segmentation, which is important for diagnosing ocular diseases but the neural network's performance can degrade across different sources due to domain shifts. The proposed technique basically enhances fundus image augmentation by eliminating small values in the Fourier domain and providing better generalization. The innovative nature of the soft thresholding fused with Fourier-transform-based domain generalization improves neural network models' performance by reducing the target images' background interference significantly. Experiments on public data validate our approach's effectiveness over conventional and state-of-the-art methods with superior segmentation metrics.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Prognosis-of-Multivariate-Battery-State-of-Performance-and-Health-via-Transformers"><a href="#Prognosis-of-Multivariate-Battery-State-of-Performance-and-Health-via-Transformers" class="headerlink" title="Prognosis of Multivariate Battery State of Performance and Health via Transformers"></a>Prognosis of Multivariate Battery State of Performance and Health via Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10014">http://arxiv.org/abs/2309.10014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Noah H. Paulson, Joseph J. Kubal, Susan J. Babinec</li>
<li>for: 本研究的目的是提供一种深度学习模型，用于预测锂离子电池性能和使用寿命。</li>
<li>methods: 该研究使用了深度变换网络模型，利用两个循环测试数据集，表征了六种锂离子电池化学组成（LFP、NMC111、NMC532、NMC622、HE5050和5Vspinel）、不同的电解液&#x2F;镍电极组合和充电&#x2F;充电方式。</li>
<li>results: 该研究的结果表明，使用深度学习模型可以高度准确地预测锂离子电池的性能和使用寿命，其中LFP快速充电数据集的预测结束时间误差为19循环，表明深度学习对锂离子电池健康状况的预测具有扎实的批处能力。<details>
<summary>Abstract</summary>
Batteries are an essential component in a deeply decarbonized future. Understanding battery performance and "useful life" as a function of design and use is of paramount importance to accelerating adoption. Historically, battery state of health (SOH) was summarized by a single parameter, the fraction of a battery's capacity relative to its initial state. A more useful approach, however, is a comprehensive characterization of its state and complexities, using an interrelated set of descriptors including capacity, energy, ionic and electronic impedances, open circuit voltages, and microstructure metrics. Indeed, predicting across an extensive suite of properties as a function of battery use is a "holy grail" of battery science; it can provide unprecedented insights toward the design of better batteries with reduced experimental effort, and de-risking energy storage investments that are necessary to meet CO2 reduction targets. In this work, we present a first step in that direction via deep transformer networks for the prediction of 28 battery state of health descriptors using two cycling datasets representing six lithium-ion cathode chemistries (LFP, NMC111, NMC532, NMC622, HE5050, and 5Vspinel), multiple electrolyte/anode compositions, and different charge-discharge scenarios. The accuracy of these predictions versus battery life (with an unprecedented mean absolute error of 19 cycles in predicting end of life for an LFP fast-charging dataset) illustrates the promise of deep learning towards providing deeper understanding and control of battery health.
</details>
<details>
<summary>摘要</summary>
锂离子电池是深度减碳未来的重要组件。理解锂离子电池性能和使用寿命的关系是加速采用的关键。历史上，锂离子电池状况（SOH）通常是用一个参数表示，即锂离子电池容量相对初始状态的比率。然而，一个更有用的方法是对锂离子电池状况进行全面描述，使用一组相关的参数，包括容量、能量、锂离子和电子阻抗、开路电压和微结构指标。实际上，预测锂离子电池的广泛性能特征是 battery science 的“圣杯”，可以提供前所未有的洞察，并帮助设计更好的锂离子电池，降低实验努力，并为温室气体减排目标做出更多的投资。在这项工作中，我们提出了一种首先采用深度变换网络来预测28个锂离子电池状况指标，使用两个循环数据集，表示六种锂离子陶瓷电池化学式（LFP、NMC111、NMC532、NMC622、HE5050和5Vspinel）、多种电解质/陶瓷组合、以及不同的充电-充电方案。预测的准确性（例如，LFP快充电数据集中预测结束生命的mean absolute error为19次）表明深度学习对锂离子电池健康提供了新的可能性。
</details></li>
</ul>
<hr>
<h2 id="Convolutional-Deep-Kernel-Machines"><a href="#Convolutional-Deep-Kernel-Machines" class="headerlink" title="Convolutional Deep Kernel Machines"></a>Convolutional Deep Kernel Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09814">http://arxiv.org/abs/2309.09814</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luisgarzac/Data-Science-Course---Udemy-frogames-Juan-Gabriel-Gomila">https://github.com/luisgarzac/Data-Science-Course---Udemy-frogames-Juan-Gabriel-Gomila</a></li>
<li>paper_authors: Edward Milsom, Ben Anson, Laurence Aitchison</li>
<li>for: 这篇论文主要是为了探讨深度kernel机器（DKM）的应用和发展。</li>
<li>methods: 本论文使用了深度kernel机器（DKM），其不同于传统的 neural network 和 deep kernel learning，因为它们都使用特征作为基本组件。此外，论文还提出了一种有效的间领点拟合方案。</li>
<li>results: 根据实验结果，使用了不同的 normalization 和 likelihood 的模型 variants，可以达到约 99% 的测试准确率在 MNIST 上，92% 在 CIFAR-10 上，71% 在 CIFAR-100 上，而且只需要训练约 28 个 GPU 小时，相比于全功能 NNGP &#x2F; NTK &#x2F; Myrtle kernels，速度提高了1-2个数量级。<details>
<summary>Abstract</summary>
Deep kernel machines (DKMs) are a recently introduced kernel method with the flexibility of other deep models including deep NNs and deep Gaussian processes. DKMs work purely with kernels, never with features, and are therefore different from other methods ranging from NNs to deep kernel learning and even deep Gaussian processes, which all use features as a fundamental component. Here, we introduce convolutional DKMs, along with an efficient inter-domain inducing point approximation scheme. Further, we develop and experimentally assess a number of model variants, including 9 different types of normalisation designed for the convolutional DKMs, two likelihoods, and two different types of top-layer. The resulting models achieve around 99% test accuracy on MNIST, 92% on CIFAR-10 and 71% on CIFAR-100, despite training in only around 28 GPU hours, 1-2 orders of magnitude faster than full NNGP / NTK / Myrtle kernels, whilst achieving comparable performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-Optimal-Contracts-How-to-Exploit-Small-Action-Spaces"><a href="#Learning-Optimal-Contracts-How-to-Exploit-Small-Action-Spaces" class="headerlink" title="Learning Optimal Contracts: How to Exploit Small Action Spaces"></a>Learning Optimal Contracts: How to Exploit Small Action Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09801">http://arxiv.org/abs/2309.09801</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Bacchiocchi, Matteo Castiglioni, Alberto Marchesi, Nicola Gatti</li>
<li>for: 解决主动자和代理人之间的主动-代理人问题，即主动자通过一系列的合同来劝动代理人进行成本高、不可见的行动，以实现有利的结果。</li>
<li>methods: 我们使用多轮合同的扩展版本，在没有主动자对代理人的信息的情况下，通过观察每轮的结果来学习最优的合同。我们采用一种算法，可以在小的动作空间下获得高probability的最优合同，并且可以在多轮合同中实现$\tilde{\mathcal{O}(T^{4&#x2F;5})$的 regret bound。</li>
<li>results: 我们解决了Zhu等人（2022）提出的开放问题，并且可以在相关的在线学习 Setting中提供$\tilde{\mathcal{O}(T^{4&#x2F;5})$的 regret bound，这比之前的 regret bound大大提高。<details>
<summary>Abstract</summary>
We study principal-agent problems in which a principal commits to an outcome-dependent payment scheme -- called contract -- in order to induce an agent to take a costly, unobservable action leading to favorable outcomes. We consider a generalization of the classical (single-round) version of the problem in which the principal interacts with the agent by committing to contracts over multiple rounds. The principal has no information about the agent, and they have to learn an optimal contract by only observing the outcome realized at each round. We focus on settings in which the size of the agent's action space is small. We design an algorithm that learns an approximately-optimal contract with high probability in a number of rounds polynomial in the size of the outcome space, when the number of actions is constant. Our algorithm solves an open problem by Zhu et al.[2022]. Moreover, it can also be employed to provide a $\tilde{\mathcal{O}(T^{4/5})$ regret bound in the related online learning setting in which the principal aims at maximizing their cumulative utility, thus considerably improving previously-known regret bounds.
</details>
<details>
<summary>摘要</summary>
我们研究主体-代理人问题，在这个问题中，主体会提出一个结果相依的支付计划，以使代理人采取费用高、不可见的行动，导致更加有利的结果。我们考虑了经典单回版本的问题的扩展，在这个问题中，主体和代理人在多轮交互中进行互动，主体没有关于代理人的信息，只能通过每轮结果来学习最佳合同。我们关注在代理人行动空间尺度小的情况下。我们设计了一个可以在小于Outcome空间大小的情况下获得近似最佳合同的算法，并且可以提供$\tilde{\mathcal{O}(T^{4/5})$的 regret bound，这比前所未见的 regret bound要更好。此外，这个算法还可以在相关的在线学习设定下应用，以最大化主体的总用用，从而提高之前已知的 regret bound。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Initial-State-Buffer-for-Reinforcement-Learning"><a href="#Contrastive-Initial-State-Buffer-for-Reinforcement-Learning" class="headerlink" title="Contrastive Initial State Buffer for Reinforcement Learning"></a>Contrastive Initial State Buffer for Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09752">http://arxiv.org/abs/2309.09752</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nico Messikommer, Yunlong Song, Davide Scaramuzza</li>
<li>for: 提高强化学习的效率，使用有限样本学习</li>
<li>methods: 引入一种矛盾起始状态缓存，选择过去经验中的状态，用于初始化机器人在环境中，引导其走向更有信息的状态</li>
<li>results: 在两个复杂的机器人任务中，实验结果显示，我们的初始状态缓存可以比基线方案高效，同时也加速了训练的收敛Here’s the summary in English for reference:</li>
<li>for: Improving the efficiency of reinforcement learning, using limited samples to learn</li>
<li>methods: Introducing a Contrastive Initial State Buffer that strategically selects states from past experiences to initialize the agent in the environment, guiding it towards more informative states</li>
<li>results: Experimental results on two complex robotic tasks show that our initial state buffer achieves higher task performance than the baseline while also speeding up training convergence.<details>
<summary>Abstract</summary>
In Reinforcement Learning, the trade-off between exploration and exploitation poses a complex challenge for achieving efficient learning from limited samples. While recent works have been effective in leveraging past experiences for policy updates, they often overlook the potential of reusing past experiences for data collection. Independent of the underlying RL algorithm, we introduce the concept of a Contrastive Initial State Buffer, which strategically selects states from past experiences and uses them to initialize the agent in the environment in order to guide it toward more informative states. We validate our approach on two complex robotic tasks without relying on any prior information about the environment: (i) locomotion of a quadruped robot traversing challenging terrains and (ii) a quadcopter drone racing through a track. The experimental results show that our initial state buffer achieves higher task performance than the nominal baseline while also speeding up training convergence.
</details>
<details>
<summary>摘要</summary>
在强化学习中，探索和利用之间的贸易带来了复杂的挑战，以实现从有限样本中获得高效的学习。然而， latest works often overlook the potential of reusing past experiences for data collection.我们在独立于基础RL算法的情况下，引入了一个对比起始缓存，该缓存从过去经验中选择状态，并将其用于初始化机器人在环境中，以引导它到更有用的状态。我们在两个复杂的机器人任务上进行了验证：（i）一只四脚 robot 在困难的地形上行走，以及（ii）一架quadcopter飞机在赛道上飞行。实验结果表明，我们的起始缓存可以比基线提高任务性能，同时也可以加速训练的收敛。
</details></li>
</ul>
<hr>
<h2 id="Towards-Better-Modeling-with-Missing-Data-A-Contrastive-Learning-based-Visual-Analytics-Perspective"><a href="#Towards-Better-Modeling-with-Missing-Data-A-Contrastive-Learning-based-Visual-Analytics-Perspective" class="headerlink" title="Towards Better Modeling with Missing Data: A Contrastive Learning-based Visual Analytics Perspective"></a>Towards Better Modeling with Missing Data: A Contrastive Learning-based Visual Analytics Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09744">http://arxiv.org/abs/2309.09744</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laixin Xie, Yang Ouyang, Longfei Chen, Ziming Wu, Quan Li</li>
<li>for:  addresses the challenges of missing data in machine learning (ML) modeling</li>
<li>methods:  uses Contrastive Learning (CL) framework to model observed data with missing values, without requiring any imputation</li>
<li>results:  demonstrates high predictive accuracy and model interpretability through quantitative experiments, expert interviews, and a qualitative user study.Here’s the full summary in Simplified Chinese:</li>
<li>for: Addresses the challenges of missing data in machine learning (ML) modeling</li>
<li>methods: 使用异常学习（CL）框架，模拟带有缺失数据的观察数据，不需要任何替换</li>
<li>results: 通过量化实验、专家采访和用户研究，证明高预测精度和模型可读性。<details>
<summary>Abstract</summary>
Missing data can pose a challenge for machine learning (ML) modeling. To address this, current approaches are categorized into feature imputation and label prediction and are primarily focused on handling missing data to enhance ML performance. These approaches rely on the observed data to estimate the missing values and therefore encounter three main shortcomings in imputation, including the need for different imputation methods for various missing data mechanisms, heavy dependence on the assumption of data distribution, and potential introduction of bias. This study proposes a Contrastive Learning (CL) framework to model observed data with missing values, where the ML model learns the similarity between an incomplete sample and its complete counterpart and the dissimilarity between other samples. Our proposed approach demonstrates the advantages of CL without requiring any imputation. To enhance interpretability, we introduce CIVis, a visual analytics system that incorporates interpretable techniques to visualize the learning process and diagnose the model status. Users can leverage their domain knowledge through interactive sampling to identify negative and positive pairs in CL. The output of CIVis is an optimized model that takes specified features and predicts downstream tasks. We provide two usage scenarios in regression and classification tasks and conduct quantitative experiments, expert interviews, and a qualitative user study to demonstrate the effectiveness of our approach. In short, this study offers a valuable contribution to addressing the challenges associated with ML modeling in the presence of missing data by providing a practical solution that achieves high predictive accuracy and model interpretability.
</details>
<details>
<summary>摘要</summary>
“缺失数据可能会对机器学习（ML）模型带来挑战。为了解决这个问题，现有的方法可以分为两种：特征填充和标签预测，它们主要是对缺失数据进行处理，以提高ML表现。这些方法将从观察到的数据中估算缺失的值，因此会遇到三个主要缺陷：需要不同的填充方法依照不同的缺失调制解调器制，依赖数据分布的假设，以及可能引入偏见。本研究提出了对缺失数据的对照学习（CL）框架，其中ML模型学习缺失数据中的相似性和不相似性。我们的提案方法不需要任何填充，并且可以提高可读性。为了增强可读性，我们导入了CIVis，一个可读性系统，它结合了可读技术来显示学习过程和诊断模型状态。用户可以透过互动采样来运用专业知识来选择负面和正面对照，而CIVis的输出则是一个已优化的模型，可以根据指定的特征进行下游任务预测。我们在回归和分类任务中提供了两个使用案例，并进行了量化实验、专家访谈和 качеitative用户研究，以证明我们的方法的有效性。简而言之，这个研究为ML模型在缺失数据下的挑战提供了实用的解决方案，可以实现高预测精度和模型可读性。”
</details></li>
</ul>
<hr>
<h2 id="The-NFLikelihood-an-unsupervised-DNNLikelihood-from-Normalizing-Flows"><a href="#The-NFLikelihood-an-unsupervised-DNNLikelihood-from-Normalizing-Flows" class="headerlink" title="The NFLikelihood: an unsupervised DNNLikelihood from Normalizing Flows"></a>The NFLikelihood: an unsupervised DNNLikelihood from Normalizing Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09743">http://arxiv.org/abs/2309.09743</a></li>
<li>repo_url: None</li>
<li>paper_authors: Humberto Reyes-Gonzalez, Riccardo Torre</li>
<li>for: 这个论文是为了探讨一种无监督的方法，基于正常化流程，来学习高维度的likelihood函数，具体来说是在高能物理分析中。</li>
<li>methods: 这个论文使用了自适应流程，基于 affine 和 rational quadratic spline 函数，来学习高维度的likelihood函数。</li>
<li>results: 论文通过实际例子示出，这种方法可以学习复杂的高维度likelihood函数，并且可以应用于高能物理分析中的几个实际问题。<details>
<summary>Abstract</summary>
We propose the NFLikelihood, an unsupervised version, based on Normalizing Flows, of the DNNLikelihood proposed in Ref.[1]. We show, through realistic examples, how Autoregressive Flows, based on affine and rational quadratic spline bijectors, are able to learn complicated high-dimensional Likelihoods arising in High Energy Physics (HEP) analyses. We focus on a toy LHC analysis example already considered in the literature and on two Effective Field Theory fits of flavor and electroweak observables, whose samples have been obtained throught the HEPFit code. We discuss advantages and disadvantages of the unsupervised approach with respect to the supervised one and discuss possible interplays of the two.
</details>
<details>
<summary>摘要</summary>
我们提出了NFLikelihood，一种无监督版本，基于归一化流，与Ref.[1]中提出的DNNLikelihood相似。我们通过实际的示例显示，使用自适应流，基于线性和quadratic spline bijectors，可以学习高维的Likelihood函数，出现在高能物理分析中。我们将focus on一个LHC分析示例，已经出现在文献中，以及两个Effective Field Theory的观测量 fits，其样本通过HEPFit代码获得。我们讨论无监督方法与监督方法之间的优劣点，以及两者之间的可能的互动。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Learning-and-Data-Augmentation-in-Traffic-Classification-Using-a-Flowpic-Input-Representation"><a href="#Contrastive-Learning-and-Data-Augmentation-in-Traffic-Classification-Using-a-Flowpic-Input-Representation" class="headerlink" title="Contrastive Learning and Data Augmentation in Traffic Classification Using a Flowpic Input Representation"></a>Contrastive Learning and Data Augmentation in Traffic Classification Using a Flowpic Input Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09733">http://arxiv.org/abs/2309.09733</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alessandro Finamore, Chao Wang, Jonatan Krolikowski, Jose M. Navarro, Fuxing Chen, Dario Rossi</li>
<li>for: 本研究是一篇关于交通分类（TC）的论文，采用了最新的深度学习（DL）方法。</li>
<li>methods: 本研究使用了少量学习、自我超vision via对比学习和数据增强等方法，以学习从少量样本中，并将学习结果转移到不同的数据集上。</li>
<li>results: 研究发现，使用这些DL方法，只需要使用100个输入样本，可以达到非常高的准确率，使用“流图”（i.e., 每个流量的2D histogram）作为输入表示。本研究还重现了原论文中的一些关键结果，并在三个额外的公共数据集上进行了数据增强的研究。<details>
<summary>Abstract</summary>
Over the last years we witnessed a renewed interest towards Traffic Classification (TC) captivated by the rise of Deep Learning (DL). Yet, the vast majority of TC literature lacks code artifacts, performance assessments across datasets and reference comparisons against Machine Learning (ML) methods. Among those works, a recent study from IMC'22 [17] is worth of attention since it adopts recent DL methodologies (namely, few-shot learning, self-supervision via contrastive learning and data augmentation) appealing for networking as they enable to learn from a few samples and transfer across datasets. The main result of [17] on the UCDAVIS19, ISCX-VPN and ISCX-Tor datasets is that, with such DL methodologies, 100 input samples are enough to achieve very high accuracy using an input representation called "flowpic" (i.e., a per-flow 2d histograms of the packets size evolution over time). In this paper (i) we reproduce [17] on the same datasets and (ii) we replicate its most salient aspect (the importance of data augmentation) on three additional public datasets, MIRAGE-19, MIRAGE-22 and UTMOBILENET21. While we confirm most of the original results, we also found a 20% accuracy drop on some of the investigated scenarios due to a data shift in the original dataset that we uncovered. Additionally, our study validates that the data augmentation strategies studied in [17] perform well on other datasets too. In the spirit of reproducibility and replicability we make all artifacts (code and data) available at [10].
</details>
<details>
<summary>摘要</summary>
过去几年，流行推理（TC）再次吸引了深度学习（DL）的关注。然而，大多数TC文献缺乏代码艺术ifacts，数据集之间的性能评估和对机器学习（ML）方法的参照比较。其中一项研究，IMC'22[17]，在网络领域引起了关注，因为它采用了当今DL技术（即少量学习、自我超视观察和数据扩展），这些技术可以通过几个样本学习并在数据集之间传递。这个研究的主要结果是，使用这些DL技术，只需要100个输入样本就可以达到非常高的准确率，使用名为"流图"（即每个流量2D histogram的包大小演化过时）的输入表示。在本文中，我们首先复制[17]中的主要方面（数据扩展的重要性）在三个公共数据集上进行了重复实验：MIRAGE-19、MIRAGE-22和UTMOBILENET21。我们证明了大部分原始结果的确认，但也发现了一些情况下的20%准确率下降，这是由原始数据集中的数据变换所致。此外，我们的研究还证明了在其他数据集上，[17]中研究的数据扩展策略也表现良好。为了保持可重复性和复制性，我们在[10]上公开了所有文件（代码和数据）。
</details></li>
</ul>
<hr>
<h2 id="Neural-Collapse-for-Unconstrained-Feature-Model-under-Cross-entropy-Loss-with-Imbalanced-Data"><a href="#Neural-Collapse-for-Unconstrained-Feature-Model-under-Cross-entropy-Loss-with-Imbalanced-Data" class="headerlink" title="Neural Collapse for Unconstrained Feature Model under Cross-entropy Loss with Imbalanced Data"></a>Neural Collapse for Unconstrained Feature Model under Cross-entropy Loss with Imbalanced Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09725">http://arxiv.org/abs/2309.09725</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wanlihongc/neural-collapse">https://github.com/wanlihongc/neural-collapse</a></li>
<li>paper_authors: Wanli Hong, Shuyang Ling</li>
<li>for: 这paper研究了不等式特征模型下的神经网络坍缩现象（Neural Collapse，NC）在不均衡数据上的扩展。</li>
<li>methods: 该paper使用了无约束特征模型（Unconstrained Feature Model，UFM）来解释NC现象。</li>
<li>results: 研究发现，在不均衡数据上，NC现象仍然存在，但是feature vectors内的坍缩现象不再是等角的，而是受样本大小的影响。此外，研究还发现了一个锐度的阈值，当阈值超过这个阈值时，小类坍缩（feature vectors of minority groups collapse to one single vector）会发生。最后，研究发现，随着样本大小的增加，数据不均衡的影响会逐渐减弱。<details>
<summary>Abstract</summary>
Recent years have witnessed the huge success of deep neural networks (DNNs) in various tasks of computer vision and text processing. Interestingly, these DNNs with massive number of parameters share similar structural properties on their feature representation and last-layer classifier at terminal phase of training (TPT). Specifically, if the training data are balanced (each class shares the same number of samples), it is observed that the feature vectors of samples from the same class converge to their corresponding in-class mean features and their pairwise angles are the same. This fascinating phenomenon is known as Neural Collapse (N C), first termed by Papyan, Han, and Donoho in 2019. Many recent works manage to theoretically explain this phenomenon by adopting so-called unconstrained feature model (UFM). In this paper, we study the extension of N C phenomenon to the imbalanced data under cross-entropy loss function in the context of unconstrained feature model. Our contribution is multi-fold compared with the state-of-the-art results: (a) we show that the feature vectors exhibit collapse phenomenon, i.e., the features within the same class collapse to the same mean vector; (b) the mean feature vectors no longer form an equiangular tight frame. Instead, their pairwise angles depend on the sample size; (c) we also precisely characterize the sharp threshold on which the minority collapse (the feature vectors of the minority groups collapse to one single vector) will take place; (d) finally, we argue that the effect of the imbalance in datasize diminishes as the sample size grows. Our results provide a complete picture of the N C under the cross-entropy loss for the imbalanced data. Numerical experiments confirm our theoretical analysis.
</details>
<details>
<summary>摘要</summary>
近年来，深度神经网络（DNN）在计算机视觉和自然语言处理等领域取得了巨大成功。意外的是，这些DNN具有庞大参数的结构性质在特定阶段训练（TPT）中的特征表示和最后一层分类器之间存在类似性。具体来说，如果训练数据均衡（每个类具有相同的样本数），则观察到样本从同一个类划分的特征向量相互吸引，其对角度保持相同。这种精彩的现象被称为神经塌缩（NC），由Papyan、Han和Donoho在2019年提出。许多最近的工作尝试理解这种现象，通过采用不受限制的特征模型（UFM）。在这篇论文中，我们研究了NC现象在不均衡数据下，使用交叉熵损失函数的情况。我们的贡献包括以下几点：（a）特征向量展现塌缩现象，即同一个类划分的特征向量塌缩到同一个均值向量；（b）均值特征向量不再形成等角紧凑框架，而是对样本大小具有相互关系的对角度；（c）我们也准确地描述了小于一定的阈值，下面的少数塌缩（特征向量少数组划分到一个向量）会发生的具体时间点；（d）最后，我们认为数据大小差异的影响随着样本大小的增长而减少。我们的结果为NC现象在交叉熵损失下的不均衡数据提供了完整的图像。数据实验证实了我们的理论分析。
</details></li>
</ul>
<hr>
<h2 id="FedLALR-Client-Specific-Adaptive-Learning-Rates-Achieve-Linear-Speedup-for-Non-IID-Data"><a href="#FedLALR-Client-Specific-Adaptive-Learning-Rates-Achieve-Linear-Speedup-for-Non-IID-Data" class="headerlink" title="FedLALR: Client-Specific Adaptive Learning Rates Achieve Linear Speedup for Non-IID Data"></a>FedLALR: Client-Specific Adaptive Learning Rates Achieve Linear Speedup for Non-IID Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09719">http://arxiv.org/abs/2309.09719</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Sun, Li Shen, Shixiang Chen, Jingwei Sun, Jing Li, Guangzhong Sun, Dacheng Tao</li>
<li>for: This paper focuses on improving the efficiency of federated learning, especially for training large-scale deep neural networks with heterogeneous data.</li>
<li>methods: The proposed method, FedLALR, adjusts the learning rate for each client based on local historical gradient squares and synchronized learning rates, which enables the method to converge and achieve linear speedup with respect to the number of clients.</li>
<li>results: The theoretical analysis and experimental results show that FedLALR outperforms several communication-efficient federated optimization methods in terms of convergence speed and scalability, and achieves promising results on CV and NLP tasks.<details>
<summary>Abstract</summary>
Federated learning is an emerging distributed machine learning method, enables a large number of clients to train a model without exchanging their local data. The time cost of communication is an essential bottleneck in federated learning, especially for training large-scale deep neural networks. Some communication-efficient federated learning methods, such as FedAvg and FedAdam, share the same learning rate across different clients. But they are not efficient when data is heterogeneous. To maximize the performance of optimization methods, the main challenge is how to adjust the learning rate without hurting the convergence. In this paper, we propose a heterogeneous local variant of AMSGrad, named FedLALR, in which each client adjusts its learning rate based on local historical gradient squares and synchronized learning rates. Theoretical analysis shows that our client-specified auto-tuned learning rate scheduling can converge and achieve linear speedup with respect to the number of clients, which enables promising scalability in federated optimization. We also empirically compare our method with several communication-efficient federated optimization methods. Extensive experimental results on Computer Vision (CV) tasks and Natural Language Processing (NLP) task show the efficacy of our proposed FedLALR method and also coincides with our theoretical findings.
</details>
<details>
<summary>摘要</summary>
归一学习是一种新般的分布式机器学习方法，允许大量客户端共同训练模型，无需交换本地数据。在归一学习中，通信时间成本是一个重要瓶颈，��pecially when training large-scale deep neural networks. Some communication-efficient federated learning methods, such as FedAvg and FedAdam, share the same learning rate across different clients. However, they are not efficient when data is heterogeneous. To maximize the performance of optimization methods, the main challenge is how to adjust the learning rate without hurting the convergence.在这篇论文中，我们提出了一种归一学习中的本地自适应学习率调整方法，称为FedLALR。每个客户端根据本地历史梯度平方和同步学习率进行自适应学习率调整。我们的客户端自定义自适应学习率调整策略可以使模型快速收敛，并且可以在客户端数量增加时实现线性的速度增长。我们还对几种通信效率高的联邦优化方法进行了比较实验。我们的实验结果表明，FedLALR方法可以在CV任务和NLP任务上达到良好的效果，并且与我们的理论预测相符。
</details></li>
</ul>
<hr>
<h2 id="Multi-Dictionary-Tensor-Decomposition"><a href="#Multi-Dictionary-Tensor-Decomposition" class="headerlink" title="Multi-Dictionary Tensor Decomposition"></a>Multi-Dictionary Tensor Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09717">http://arxiv.org/abs/2309.09717</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maxwell McNeil, Petko Bogdanov</li>
<li>for: 多方向数据的分析，如社交媒体、医疗、时空域等领域的数据分析</li>
<li>methods: 使用多字做 tensor decomposition 方法，利用各种数据驱动的假设来分解 tensor</li>
<li>results: 提出了一种多字做 tensor decomposition 框架（MDTD），可以利用外部structural信息来获得稀疏编码的 tensor 因子，并且可以处理大型稀疏tensor。实验表明，相比于字做法，MDTD 可以学习更简洁的模型，并且可以提高数据重建质量、缺失值填充质量和 tensor 维度的估计。同时，MDTD 的运行时间并不受影响，可以快速处理大型数据。<details>
<summary>Abstract</summary>
Tensor decomposition methods are popular tools for analysis of multi-way datasets from social media, healthcare, spatio-temporal domains, and others. Widely adopted models such as Tucker and canonical polyadic decomposition (CPD) follow a data-driven philosophy: they decompose a tensor into factors that approximate the observed data well. In some cases side information is available about the tensor modes. For example, in a temporal user-item purchases tensor a user influence graph, an item similarity graph, and knowledge about seasonality or trends in the temporal mode may be available. Such side information may enable more succinct and interpretable tensor decomposition models and improved quality in downstream tasks.   We propose a framework for Multi-Dictionary Tensor Decomposition (MDTD) which takes advantage of prior structural information about tensor modes in the form of coding dictionaries to obtain sparsely encoded tensor factors. We derive a general optimization algorithm for MDTD that handles both complete input and input with missing values. Our framework handles large sparse tensors typical to many real-world application domains. We demonstrate MDTD's utility via experiments with both synthetic and real-world datasets. It learns more concise models than dictionary-free counterparts and improves (i) reconstruction quality ($60\%$ fewer non-zero coefficients coupled with smaller error); (ii) missing values imputation quality (two-fold MSE reduction with up to orders of magnitude time savings) and (iii) the estimation of the tensor rank. MDTD's quality improvements do not come with a running time premium: it can decompose $19GB$ datasets in less than a minute. It can also impute missing values in sparse billion-entry tensors more accurately and scalably than state-of-the-art competitors.
</details>
<details>
<summary>摘要</summary>
tensor 分解方法是社交媒体、医疗、时空域等多维数据分析的流行工具。广泛采用的模型，如图克 decomposition 和 canonical polyadic decomposition (CPD) 采用数据驱动 philosophy：它们将 tensor  decomposed into factors that approximate observed data well。在某些情况下，tensor 模式上有侧信息可用，例如，在 temporal 用户 item 购买 tensor 中，用户影响图、item similarity graph 和 temporal 模式中的季节性或趋势信息可能可用。这些侧信息可能使 tensor 分解模型更简洁可读，改进下游任务质量。我们提出了一个 Multi-Dictionary Tensor Decomposition (MDTD) 框架，利用 tensor 模式上的编码字典来获得精简编码 tensor 因子。我们 deriv 一种通用优化算法 для MDTD，可以处理完整输入和 missing 值输入。我们的框架可以处理大量的巨大稀盐 tensor，通常出现在实际应用中。我们通过实验表明，MDTD 可以学习更简洁的模型，并提高（i）重建质量（60%  fewer non-zero coefficients 和 smaller error），（ii）缺失值插值质量（two-fold MSE reduction with up to orders of magnitude time savings）和（iii） tensor 级别的估计。MDTD 的质量改进不会带来运行时间开销：它可以在一分钟内分解 19GB 的数据。它还可以更准确和可扩展地插值缺失的 billion-entry  tensor than state-of-the-art 竞争对手。
</details></li>
</ul>
<hr>
<h2 id="A-Study-of-Data-driven-Methods-for-Adaptive-Forecasting-of-COVID-19-Cases"><a href="#A-Study-of-Data-driven-Methods-for-Adaptive-Forecasting-of-COVID-19-Cases" class="headerlink" title="A Study of Data-driven Methods for Adaptive Forecasting of COVID-19 Cases"></a>A Study of Data-driven Methods for Adaptive Forecasting of COVID-19 Cases</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09698">http://arxiv.org/abs/2309.09698</a></li>
<li>repo_url: None</li>
<li>paper_authors: Charithea Stylianides, Kleanthis Malialis, Panayiotis Kolios</li>
<li>for: 本研究旨在investigate数据驱动（学习、统计）方法，以适应COVID-19病毒传播的非站点性条件。</li>
<li>methods: 本研究使用数据驱动学习和统计方法，以incrementally更新模型，适应不断变化的病毒传播条件。</li>
<li>results: 实验结果表明，该方法在不同的病毒浪涌期内，可以提供高准确率的预测结果，并在疫情爆发时进行有效的预测。<details>
<summary>Abstract</summary>
Severe acute respiratory disease SARS-CoV-2 has had a found impact on public health systems and healthcare emergency response especially with respect to making decisions on the most effective measures to be taken at any given time. As demonstrated throughout the last three years with COVID-19, the prediction of the number of positive cases can be an effective way to facilitate decision-making. However, the limited availability of data and the highly dynamic and uncertain nature of the virus transmissibility makes this task very challenging. Aiming at investigating these challenges and in order to address this problem, this work studies data-driven (learning, statistical) methods for incrementally training models to adapt to these nonstationary conditions. An extensive empirical study is conducted to examine various characteristics, such as, performance analysis on a per virus wave basis, feature extraction, "lookback" window size, memory size, all for next-, 7-, and 14-day forecasting tasks. We demonstrate that the incremental learning framework can successfully address the aforementioned challenges and perform well during outbreaks, providing accurate predictions.
</details>
<details>
<summary>摘要</summary>
严重急性呼吸疾病SARS-CoV-2对公共卫生系统和医疗紧急应急响应有着深远的影响，尤其是在决定最有效的措施时采取决策。在过去三年的COVID-19疫情中，预测病例数量是一项有效的决策支持。然而，数据有限性和病毒传播性的高度动态和不确定性使得这项工作具有挑战性。本研究旨在调查这些挑战，并通过数据驱动（学习、统计）方法来适应这些非站点条件。我们进行了广泛的实践研究，包括精心分析不同的特征，如批处理大小、缓存大小、memory大小等，以及下一天、7天、14天预测任务的性能分析。我们示出了增量学习框架可以成功地解决上述挑战，并在爆发期间提供 precisions 的预测。
</details></li>
</ul>
<hr>
<h2 id="VULNERLIZER-Cross-analysis-Between-Vulnerabilities-and-Software-Libraries"><a href="#VULNERLIZER-Cross-analysis-Between-Vulnerabilities-and-Software-Libraries" class="headerlink" title="VULNERLIZER: Cross-analysis Between Vulnerabilities and Software Libraries"></a>VULNERLIZER: Cross-analysis Between Vulnerabilities and Software Libraries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09649">http://arxiv.org/abs/2309.09649</a></li>
<li>repo_url: None</li>
<li>paper_authors: Irdin Pekaric, Michael Felderer, Philipp Steinmüller</li>
<li>For: 本研究旨在提供一种新的漏洞检测方法，用于针对软件项目中的漏洞进行检测。* Methods: 本方法使用CVE和软件库数据，结合归一化算法生成漏洞和库之间的链接。此外，还进行模型训练，以更新分配的权重。* Results: 研究结果显示，使用VULNERLIZER方法可以准确预测未来可能出现漏洞的软件库，并达到预测精度75%或更高。<details>
<summary>Abstract</summary>
The identification of vulnerabilities is a continuous challenge in software projects. This is due to the evolution of methods that attackers employ as well as the constant updates to the software, which reveal additional issues. As a result, new and innovative approaches for the identification of vulnerable software are needed. In this paper, we present VULNERLIZER, which is a novel framework for cross-analysis between vulnerabilities and software libraries. It uses CVE and software library data together with clustering algorithms to generate links between vulnerabilities and libraries. In addition, the training of the model is conducted in order to reevaluate the generated associations. This is achieved by updating the assigned weights. Finally, the approach is then evaluated by making the predictions using the CVE data from the test set. The results show that the VULNERLIZER has a great potential in being able to predict future vulnerable libraries based on an initial input CVE entry or a software library. The trained model reaches a prediction accuracy of 75% or higher.
</details>
<details>
<summary>摘要</summary>
“找到漏洞是软件项目中不断的挑战。这是因为攻击者的方法不断发展以及软件不断更新，导致新的漏洞披露。为此，我们提出了一种新的漏洞识别框架——漏洞LIZER。它使用CVE和软件库数据，结合聚类算法生成漏洞和库之间的关联。此外，我们还进行了模型训练，以重新评估生成的关联。这是通过更新分配的权重来实现的。最后，我们对测试集中的CVE数据进行预测，结果显示，漏洞LIZER可以准确预测基于输入CVE记录或软件库的未来漏洞。训练模型的准确率达75%或更高。”
</details></li>
</ul>
<hr>
<h2 id="A-Discussion-on-Generalization-in-Next-Activity-Prediction"><a href="#A-Discussion-on-Generalization-in-Next-Activity-Prediction" class="headerlink" title="A Discussion on Generalization in Next-Activity Prediction"></a>A Discussion on Generalization in Next-Activity Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09618">http://arxiv.org/abs/2309.09618</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luka Abb, Peter Pfeiffer, Peter Fettke, Jana-Rebecca Rehse</li>
<li>for: 本研究旨在评估深度学习技术在下一个活动预测中的效果，并提出了不同的预测场景，以促进未来研究的发展。</li>
<li>methods: 本研究使用了深度学习技术进行下一个活动预测，并评估了其预测性能使用公共可用事件日志。</li>
<li>results: 研究发现现有的评估方法带来很大的示例泄露问题，导致使用深度学习技术的预测方法并不如预期中效果好。<details>
<summary>Abstract</summary>
Next activity prediction aims to forecast the future behavior of running process instances. Recent publications in this field predominantly employ deep learning techniques and evaluate their prediction performance using publicly available event logs. This paper presents empirical evidence that calls into question the effectiveness of these current evaluation approaches. We show that there is an enormous amount of example leakage in all of the commonly used event logs, so that rather trivial prediction approaches perform almost as well as ones that leverage deep learning. We further argue that designing robust evaluations requires a more profound conceptual engagement with the topic of next-activity prediction, and specifically with the notion of generalization to new data. To this end, we present various prediction scenarios that necessitate different types of generalization to guide future research.
</details>
<details>
<summary>摘要</summary>
下一个活动预测目标是预测运行进程实例的未来行为。现有文献主要采用深度学习技术进行预测性能评估，使用公共可用事件日志进行评估。本文提供了实验证据，质疑现有评价方法的效果。我们发现所有常用的事件日志具有很大的示例泄露，使得基本的预测方法几乎与深度学习相当。我们还认为设计Robust评估需要更深刻的概念理解，特别是一致到新数据的总结。为此，我们提出了不同类型的预测场景，以引导未来研究。
</details></li>
</ul>
<hr>
<h2 id="Latent-assimilation-with-implicit-neural-representations-for-unknown-dynamics"><a href="#Latent-assimilation-with-implicit-neural-representations-for-unknown-dynamics" class="headerlink" title="Latent assimilation with implicit neural representations for unknown dynamics"></a>Latent assimilation with implicit neural representations for unknown dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09574">http://arxiv.org/abs/2309.09574</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuoyuan Li, Bin Dong, Pingwen Zhang</li>
<li>for: 这种研究是为了解决数据吸收中的高计算成本和数据维度问题。</li>
<li>methods: 该研究使用了新的抽象框架，即秘密吸收与卷积神经网络（LAINR），其中引入了圆形秘密神经表示（SINR）和数据驱动的神经网络 uncertainty 估计器。</li>
<li>results: 实验结果表明，与基于AutoEncoder的方法相比，LAINR在吸收过程中具有更高的精度和效率。<details>
<summary>Abstract</summary>
Data assimilation is crucial in a wide range of applications, but it often faces challenges such as high computational costs due to data dimensionality and incomplete understanding of underlying mechanisms. To address these challenges, this study presents a novel assimilation framework, termed Latent Assimilation with Implicit Neural Representations (LAINR). By introducing Spherical Implicit Neural Representations (SINR) along with a data-driven uncertainty estimator of the trained neural networks, LAINR enhances efficiency in assimilation process. Experimental results indicate that LAINR holds certain advantage over existing methods based on AutoEncoders, both in terms of accuracy and efficiency.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>数据融合在各种应用中是关键，但它经常遇到高计算成本的数据维度和下面机制的不完全理解。为解决这些挑战，本研究提出了一种新的融合框架，称为潜在融合（LAINR）。通过引入圆形潜在神经表示（SINR）以及基于训练神经网络的数据驱动 uncertainty 估计器，LAINR 提高了融合过程的效率。实验结果表明，LAINR 在比AutoEncoders 基于的方法上具有更高的准确性和效率。
</details></li>
</ul>
<hr>
<h2 id="New-Bounds-on-the-Accuracy-of-Majority-Voting-for-Multi-Class-Classification"><a href="#New-Bounds-on-the-Accuracy-of-Majority-Voting-for-Multi-Class-Classification" class="headerlink" title="New Bounds on the Accuracy of Majority Voting for Multi-Class Classification"></a>New Bounds on the Accuracy of Majority Voting for Multi-Class Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09564">http://arxiv.org/abs/2309.09564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sina Aeeneh, Nikola Zlatanov, Jiangshan Yu</li>
<li>for: 这个论文主要研究了多类别分类问题中的多数投票函数（MVF）的精度。</li>
<li>methods: 本论文使用了独立且非Identically分布的选民模型，并 derivated了MVF在多类别分类问题中的新上界。</li>
<li>results: 研究发现，在满足 certain conditions 的情况下，MVF在多类别分类问题中的误差率会指数减少到零，而在不满足这些条件的情况下，误差率会指数增长。此外，研究还发现了对真实分类算法的精度，其在best-case情况下可以达到小误差率，但在worst-case情况下可能高于MVF的误差率。<details>
<summary>Abstract</summary>
Majority voting is a simple mathematical function that returns the value that appears most often in a set. As a popular decision fusion technique, the majority voting function (MVF) finds applications in resolving conflicts, where a number of independent voters report their opinions on a classification problem. Despite its importance and its various applications in ensemble learning, data crowd-sourcing, remote sensing, and data oracles for blockchains, the accuracy of the MVF for the general multi-class classification problem has remained unknown. In this paper, we derive a new upper bound on the accuracy of the MVF for the multi-class classification problem. More specifically, we show that under certain conditions, the error rate of the MVF exponentially decays toward zero as the number of independent voters increases. Conversely, the error rate of the MVF exponentially grows with the number of independent voters if these conditions are not met.   We first explore the problem for independent and identically distributed voters where we assume that every voter follows the same conditional probability distribution of voting for different classes, given the true classification of the data point. Next, we extend our results for the case where the voters are independent but non-identically distributed. Using the derived results, we then provide a discussion on the accuracy of the truth discovery algorithms. We show that in the best-case scenarios, truth discovery algorithms operate as an amplified MVF and thereby achieve a small error rate only when the MVF achieves a small error rate, and vice versa, achieve a large error rate when the MVF also achieves a large error rate. In the worst-case scenario, the truth discovery algorithms may achieve a higher error rate than the MVF. Finally, we confirm our theoretical results using numerical simulations.
</details>
<details>
<summary>摘要</summary>
多数投票是一种简单的数学函数，返回集合中出现最多的值。作为一种受欢迎的决策融合技术，多数投票函数（MVF）在解决冲突、 ensemble learning、数据投票、远程感知和数据链等领域都有应用。 despite its importance and various applications, the accuracy of MVF for the general multi-class classification problem remains unknown. In this paper, we derive a new upper bound on the accuracy of MVF for the multi-class classification problem. Specifically, we show that under certain conditions, the error rate of MVF exponentially decays toward zero as the number of independent voters increases. Conversely, the error rate of MVF exponentially grows with the number of independent voters if these conditions are not met. We first explore the problem for independent and identically distributed voters, assuming that every voter follows the same conditional probability distribution of voting for different classes given the true classification of the data point. Next, we extend our results to the case where the voters are independent but non-identically distributed. Using the derived results, we then provide a discussion on the accuracy of truth discovery algorithms. We show that in the best-case scenarios, truth discovery algorithms operate as an amplified MVF and thereby achieve a small error rate only when the MVF achieves a small error rate, and vice versa, achieve a large error rate when the MVF also achieves a large error rate. In the worst-case scenario, truth discovery algorithms may achieve a higher error rate than MVF. Finally, we confirm our theoretical results using numerical simulations.
</details></li>
</ul>
<hr>
<h2 id="Utilizing-Whisper-to-Enhance-Multi-Branched-Speech-Intelligibility-Prediction-Model-for-Hearing-Aids"><a href="#Utilizing-Whisper-to-Enhance-Multi-Branched-Speech-Intelligibility-Prediction-Model-for-Hearing-Aids" class="headerlink" title="Utilizing Whisper to Enhance Multi-Branched Speech Intelligibility Prediction Model for Hearing Aids"></a>Utilizing Whisper to Enhance Multi-Branched Speech Intelligibility Prediction Model for Hearing Aids</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09548">http://arxiv.org/abs/2309.09548</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryandhimas E. Zezario, Fei Chen, Chiou-Shann Fuh, Hsin-Min Wang, Yu Tsao</li>
<li>for: 这个研究的目的是提高听力器扩展设备中的自动评估speech intelligibility的精度。</li>
<li>methods: 这个研究使用了一种改进后的多支分支扩展speech intelligibility预测模型，称为MBI-Net+和MBI-Net++。MBI-Net+使用Whisper嵌入来扩展语音特征，而MBI-Net++则使用了一个辅助任务来预测帧级和语录级的对象语音理解指标HASPI的分数。</li>
<li>results: 实验结果表明，MBI-Net++和MBI-Net+都比MBI-Net在多种指标上表现更好，而MBI-Net++还是MBI-Net+的更好。<details>
<summary>Abstract</summary>
Automated assessment of speech intelligibility in hearing aid (HA) devices is of great importance. Our previous work introduced a non-intrusive multi-branched speech intelligibility prediction model called MBI-Net, which achieved top performance in the Clarity Prediction Challenge 2022. Based on the promising results of the MBI-Net model, we aim to further enhance its performance by leveraging Whisper embeddings to enrich acoustic features. In this study, we propose two improved models, namely MBI-Net+ and MBI-Net++. MBI-Net+ maintains the same model architecture as MBI-Net, but replaces self-supervised learning (SSL) speech embeddings with Whisper embeddings to deploy cross-domain features. On the other hand, MBI-Net++ further employs a more elaborate design, incorporating an auxiliary task to predict frame-level and utterance-level scores of the objective speech intelligibility metric HASPI (Hearing Aid Speech Perception Index) and multi-task learning. Experimental results confirm that both MBI-Net++ and MBI-Net+ achieve better prediction performance than MBI-Net in terms of multiple metrics, and MBI-Net++ is better than MBI-Net+.
</details>
<details>
<summary>摘要</summary>
Here's the translation in Simplified Chinese:自动评估听力器（HA）设备的听力 intelligibility 非常重要。我们之前的工作推出了一种非侵入式多支路听力 intelligibility 预测模型，称为 MBI-Net，在 Clarity Prediction Challenge 2022 中获得了优秀的成绩。基于 MBI-Net 模型的承诺 результа，我们希望进一步提高其性能，通过使用 Whisper 嵌入来增强语音特征。在这项研究中，我们提出了两种改进的模型，即 MBI-Net+ 和 MBI-Net++。MBI-Net+ 维持了同 MBI-Net 模型的同样结构，但是将 SSL 语音嵌入替换为 Whisper 嵌入，以便在不同频谱上进行跨频域特征的部署。而 MBI-Net++ 则进一步采用了更加复杂的设计，包括在对象听力指数 HASPI (Hearing Aid Speech Perception Index) 的帧级和语音级分数预测任务中使用多任务学习。实验结果表明，MBI-Net++ 和 MBI-Net+ 都在多个指标上超过 MBI-Net，而 MBI-Net++ 则是 MBI-Net+ 的更好。
</details></li>
</ul>
<hr>
<h2 id="Quantum-Wasserstein-GANs-for-State-Preparation-at-Unseen-Points-of-a-Phase-Diagram"><a href="#Quantum-Wasserstein-GANs-for-State-Preparation-at-Unseen-Points-of-a-Phase-Diagram" class="headerlink" title="Quantum Wasserstein GANs for State Preparation at Unseen Points of a Phase Diagram"></a>Quantum Wasserstein GANs for State Preparation at Unseen Points of a Phase Diagram</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09543">http://arxiv.org/abs/2309.09543</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wiktor Jurasz, Christian B. Mendl</li>
<li>for: 本研究旨在扩展生成模型，特别是生成对抗网络（GANs）到量子域，并解决当前方法的局限性。</li>
<li>methods: 我们提出了一种新的混合类型-量子方法，基于量子沃氏赋形GANs，可以学习输入集中的测量期望函数，并生成未经见过的新状态。</li>
<li>results: 我们的方法可以生成新的状态，其测量期望函数遵循同一个下面函数，这些状态没有出现在输入集中。<details>
<summary>Abstract</summary>
Generative models and in particular Generative Adversarial Networks (GANs) have become very popular and powerful data generation tool. In recent years, major progress has been made in extending this concept into the quantum realm. However, most of the current methods focus on generating classes of states that were supplied in the input set and seen at the training time. In this work, we propose a new hybrid classical-quantum method based on quantum Wasserstein GANs that overcomes this limitation. It allows to learn the function governing the measurement expectations of the supplied states and generate new states, that were not a part of the input set, but which expectations follow the same underlying function.
</details>
<details>
<summary>摘要</summary>
生成模型，特别是生成对抗网络（GANs），在过去几年变得非常流行和强大，用于数据生成。然而，大多数当前方法仅能生成训练时提供的类别的状态。在这种情况下，我们提议一种新的混合类 quantum 方法，基于量子沃尔帕特 GANs，可以超越这一限制。它可以学习输入状态的测量预期函数，并生成没有在输入集中出现过的新状态，但是预期函数follows the same underlying function。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-SUPERB-Towards-A-Dynamic-Collaborative-and-Comprehensive-Instruction-Tuning-Benchmark-for-Speech"><a href="#Dynamic-SUPERB-Towards-A-Dynamic-Collaborative-and-Comprehensive-Instruction-Tuning-Benchmark-for-Speech" class="headerlink" title="Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech"></a>Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09510">http://arxiv.org/abs/2309.09510</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dynamic-superb/dynamic-superb">https://github.com/dynamic-superb/dynamic-superb</a></li>
<li>paper_authors: Chien-yu Huang, Ke-Han Lu, Shih-Heng Wang, Chi-Yuan Hsiao, Chun-Yi Kuan, Haibin Wu, Siddhant Arora, Kai-Wei Chang, Jiatong Shi, Yifan Peng, Roshan Sharma, Shinji Watanabe, Bhiksha Ramakrishnan, Shady Shehata, Hung-yi Lee</li>
<li>for: This paper aims to present a benchmark for building universal speech models that can perform multiple tasks in a zero-shot fashion using instruction tuning.</li>
<li>methods: The paper proposes a benchmark called Dynamic-SUPERB, which combines 33 tasks and 22 datasets to provide comprehensive coverage of diverse speech tasks and harness instruction tuning. The paper also proposes several approaches to establish benchmark baselines, including the use of speech models, text language models, and the multimodal encoder.</li>
<li>results: The evaluation results show that while the baselines perform reasonably on seen tasks, they struggle with unseen ones. The paper also conducts an ablation study to assess the robustness and seek improvements in the performance.<details>
<summary>Abstract</summary>
Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primarily focus on limited or specific tasks. Moreover, the lack of standardized benchmarks hinders a fair comparison across different approaches. Thus, we present Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and harness instruction tuning, we invite the community to collaborate and contribute, facilitating the dynamic growth of the benchmark. To initiate, Dynamic-SUPERB features 55 evaluation instances by combining 33 tasks and 22 datasets. This spans a broad spectrum of dimensions, providing a comprehensive platform for evaluation. Additionally, we propose several approaches to establish benchmark baselines. These include the utilization of speech models, text language models, and the multimodal encoder. Evaluation results indicate that while these baselines perform reasonably on seen tasks, they struggle with unseen ones. We also conducted an ablation study to assess the robustness and seek improvements in the performance. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the field together.
</details>
<details>
<summary>摘要</summary>
To ensure comprehensive coverage of diverse speech tasks and harness instruction tuning, we invite the community to collaborate and contribute to the benchmark, facilitating its dynamic growth. Dynamic-SUPERB currently features 55 evaluation instances by combining 33 tasks and 22 datasets, providing a broad spectrum of dimensions for evaluation. We also propose several approaches to establish benchmark baselines, including the use of speech models, text language models, and the multimodal encoder.Evaluation results show that while these baselines perform reasonably well on seen tasks, they struggle with unseen ones. To improve performance, we conducted an ablation study to assess the robustness and seek improvements. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the field together.Here's the translation in Simplified Chinese:文本语言模型已经展现出很强的零 shot 能力，可以通过提供良好的指令来泛化到未看过任务。然而，现有的语音处理研究主要集中在限定或特定的任务上，而lack of standardized benchmarks 使得不同方法之间的比较不公平。为此，我们提出了Dynamic-SUPERB，一个用于建立通用语音模型的 benchmark，可以通过指令调整来完成多个任务的零 shot 泛化。为确保语音任务的全面覆盖和利用指令调整，我们邀请社区参与合作，以便不断扩展 benchmark。Dynamic-SUPERB 目前已经包含了55个评估实例，通过组合 33 个任务和 22 个数据集，提供了广泛的维度评估。我们还提出了一些建立 benchmark 基准的方法，包括使用语音模型、文本语言模型和多模式Encoder。评估结果显示，虽然这些基准在看到任务上表现良好，但在未看到任务上表现不佳。为了提高性能，我们进行了一些剖析研究，以评估Robustness和寻找改进。我们将所有材料公开发布，并邀请研究人员一起合作项目，共同推动领域技术的发展。
</details></li>
</ul>
<hr>
<h2 id="Outlier-Insensitive-Kalman-Filtering-Theory-and-Applications"><a href="#Outlier-Insensitive-Kalman-Filtering-Theory-and-Applications" class="headerlink" title="Outlier-Insensitive Kalman Filtering: Theory and Applications"></a>Outlier-Insensitive Kalman Filtering: Theory and Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09505">http://arxiv.org/abs/2309.09505</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shunit Truzman, Guy Revach, Nir Shlezinger, Itzik Klein</li>
<li>for: 这篇论文是关于如何从含有噪音观测的动力系统中进行状态估计，以提高适用范围和精度。</li>
<li>methods: 本文提出了一个具有自适应能力的实时状态估计方法，可以快速处理含有噪音观测的动力系统，并且不需要调整参数。</li>
<li>results: 实验和场景评估表明，本文的方法能够对含有噪音观测的动力系统进行精确的状态估计，并且比于其他方法更具有抗错误性。<details>
<summary>Abstract</summary>
State estimation of dynamical systems from noisy observations is a fundamental task in many applications. It is commonly addressed using the linear Kalman filter (KF), whose performance can significantly degrade in the presence of outliers in the observations, due to the sensitivity of its convex quadratic objective function. To mitigate such behavior, outlier detection algorithms can be applied. In this work, we propose a parameter-free algorithm which mitigates the harmful effect of outliers while requiring only a short iterative process of the standard update step of the KF. To that end, we model each potential outlier as a normal process with unknown variance and apply online estimation through either expectation maximization or alternating maximization algorithms. Simulations and field experiment evaluations demonstrate competitive performance of our method, showcasing its robustness to outliers in filtering scenarios compared to alternative algorithms.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translates as:状态估计动力系统从噪声观测中是许多应用中的基本任务。通常使用线性卡尔曼筛（KF）来解决这个问题，但是在观测中出现异常值时，KF的性能会受到严重损害，因为它的对称二阶函数会变得敏感。为了解决这个问题，我们可以使用异常检测算法。在这种情况下，我们模型每个可能的异常为正常过程的不确定噪声，并通过在线估计来进行对其进行更新。这种方法不需要任何参数，仅需要短暂的迭代过程。在实验和场景评估中，我们的方法能够与其他算法相比赢得竞争优势，表明其对异常观测的抵抗力在筛码场景中较高。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Approaches-to-Predict-and-Detect-Early-Onset-of-Digital-Dermatitis-in-Dairy-Cows-using-Sensor-Data"><a href="#Machine-Learning-Approaches-to-Predict-and-Detect-Early-Onset-of-Digital-Dermatitis-in-Dairy-Cows-using-Sensor-Data" class="headerlink" title="Machine Learning Approaches to Predict and Detect Early-Onset of Digital Dermatitis in Dairy Cows using Sensor Data"></a>Machine Learning Approaches to Predict and Detect Early-Onset of Digital Dermatitis in Dairy Cows using Sensor Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10010">http://arxiv.org/abs/2309.10010</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jennifer Magana, Dinu Gavojdian, Yakir Menachem, Teddy Lazebnik, Anna Zamansky, Amber Adams-Progar</li>
<li>for: 本研究旨在采用机器学习算法基于传感器行为数据早期发现和预测牛皮病（DD）。</li>
<li>methods: 本研究使用了机器学习模型，基于牛皮病症状出现的日期和时间，使用传感器数据进行预测和检测。</li>
<li>results: 研究发现，使用行为传感器数据预测和检测牛皮病的机器学习模型可达79%的准确率，预测牛皮病2天前的模型准确率为64%。这些机器学习模型可以帮助开发基于行为传感器数据的实时自动牛皮病监测和诊断工具，用于检测牛皮病的症状变化。<details>
<summary>Abstract</summary>
The aim of this study was to employ machine learning algorithms based on sensor behavior data for (1) early-onset detection of digital dermatitis (DD); and (2) DD prediction in dairy cows. With the ultimate goal to set-up early warning tools for DD prediction, which would than allow a better monitoring and management of DD under commercial settings, resulting in a decrease of DD prevalence and severity, while improving animal welfare. A machine learning model that is capable of predicting and detecting digital dermatitis in cows housed under free-stall conditions based on behavior sensor data has been purposed and tested in this exploratory study. The model for DD detection on day 0 of the appearance of the clinical signs has reached an accuracy of 79%, while the model for prediction of DD 2 days prior to the appearance of the first clinical signs has reached an accuracy of 64%. The proposed machine learning models could help to develop a real-time automated tool for monitoring and diagnostic of DD in lactating dairy cows, based on behavior sensor data under conventional dairy environments. Results showed that alterations in behavioral patterns at individual levels can be used as inputs in an early warning system for herd management in order to detect variances in health of individual cows.
</details>
<details>
<summary>摘要</summary>
The study proposed a machine learning model that can predict and detect DD in cows housed under free-stall conditions based on behavior sensor data. The model achieved an accuracy of 79% in detecting DD on day 0 of the appearance of clinical signs, and an accuracy of 64% in predicting DD 2 days prior to the first clinical signs.The results of the study showed that alterations in behavioral patterns at the individual level can be used as inputs in an early warning system for herd management to detect variances in the health of individual cows. The proposed machine learning models have the potential to develop a real-time automated tool for monitoring and diagnosis of DD in lactating dairy cows based on behavior sensor data under conventional dairy environments.
</details></li>
</ul>
<hr>
<h2 id="Face-Driven-Zero-Shot-Voice-Conversion-with-Memory-based-Face-Voice-Alignment"><a href="#Face-Driven-Zero-Shot-Voice-Conversion-with-Memory-based-Face-Voice-Alignment" class="headerlink" title="Face-Driven Zero-Shot Voice Conversion with Memory-based Face-Voice Alignment"></a>Face-Driven Zero-Shot Voice Conversion with Memory-based Face-Voice Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09470">http://arxiv.org/abs/2309.09470</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheng-Yan Sheng, Yang Ai, Yan-Nian Chen, Zhen-Hua Ling</li>
<li>for: 这篇论文目标是提出一种基于 face 图像的零 shot 语音转换任务（zero-shot FaceVC），即将源 speaker 的语音特征转换到目标 speaker 的语音特征上，只使用目标 speaker 的单个 face 图像。</li>
<li>methods: 我们提议一种基于 memory-based face-voice 对应模块的 zero-shot FaceVC 方法，通过槽来对这两种模式进行对应，以 capture 语音特征从 face 图像中。我们还提出了一种混合超级visit策略，以解决voice conversion任务在训练和推断阶段之间的一贯性问题。</li>
<li>results: 通过广泛的实验，我们证明了我们提出的方法在 zero-shot FaceVC 任务中的优越性。我们还设计了系统的主观和客观评价指标，以全面评估homogeneity、多样性和一致性 controlled by face images。<details>
<summary>Abstract</summary>
This paper presents a novel task, zero-shot voice conversion based on face images (zero-shot FaceVC), which aims at converting the voice characteristics of an utterance from any source speaker to a newly coming target speaker, solely relying on a single face image of the target speaker. To address this task, we propose a face-voice memory-based zero-shot FaceVC method. This method leverages a memory-based face-voice alignment module, in which slots act as the bridge to align these two modalities, allowing for the capture of voice characteristics from face images. A mixed supervision strategy is also introduced to mitigate the long-standing issue of the inconsistency between training and inference phases for voice conversion tasks. To obtain speaker-independent content-related representations, we transfer the knowledge from a pretrained zero-shot voice conversion model to our zero-shot FaceVC model. Considering the differences between FaceVC and traditional voice conversion tasks, systematic subjective and objective metrics are designed to thoroughly evaluate the homogeneity, diversity and consistency of voice characteristics controlled by face images. Through extensive experiments, we demonstrate the superiority of our proposed method on the zero-shot FaceVC task. Samples are presented on our demo website.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这篇论文介绍了一个新的任务：基于面像的零shot语音转换（zero-shot FaceVC），该任务的目标是将任何来源说话人的语音特征转换为新来的目标说话人，只使用单个面像。为解决这个任务，作者们提出了一种面voice记忆基于的零shot FaceVC方法。这种方法利用了一种面voice记忆对应模块，将这两种模式相互对应，以捕捉面像中的语音特征。此外，作者们还提出了一种混合超级visión策略，以解决长期存在的语音转换任务的训练和推断阶段不一致问题。为了获得无关 speaker的内容相关表示，作者们将预训练的零shot语音转换模型的知识传递到了他们的零shot FaceVC模型。鉴于FaceVC和传统的语音转换任务之间的差异，作者们设计了系统的主观和客观评价指标，以全面评估零shot FaceVC模型的性能。通过广泛的实验，作者们证明了他们的提出的方法在零shot FaceVC任务中的优越性。详细的样例可以在他们的 Demo 网站上找到。
</details></li>
</ul>
<hr>
<h2 id="Active-anomaly-detection-based-on-deep-one-class-classification"><a href="#Active-anomaly-detection-based-on-deep-one-class-classification" class="headerlink" title="Active anomaly detection based on deep one-class classification"></a>Active anomaly detection based on deep one-class classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09465">http://arxiv.org/abs/2309.09465</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mkkim-home/AAD">https://github.com/mkkim-home/AAD</a></li>
<li>paper_authors: Minkyung Kim, Junsik Kim, Jongmin Yu, Jun Kyun Choi</li>
<li>for: 这 paper 是为了提高深度异常检测模型的训练效果而使用活动学习工具。</li>
<li>methods: 这 paper 使用了一种基于 adaptive boundary 的查询策略，以及一种 combining  noise contrastive estimation 和一类分类模型的 semi-supervised learning 方法。</li>
<li>results: 这 paper 在 seven 个异常检测数据集上分别采用了这两种方法，并分析了它们的效果。<details>
<summary>Abstract</summary>
Active learning has been utilized as an efficient tool in building anomaly detection models by leveraging expert feedback. In an active learning framework, a model queries samples to be labeled by experts and re-trains the model with the labeled data samples. It unburdens in obtaining annotated datasets while improving anomaly detection performance. However, most of the existing studies focus on helping experts identify as many abnormal data samples as possible, which is a sub-optimal approach for one-class classification-based deep anomaly detection. In this paper, we tackle two essential problems of active learning for Deep SVDD: query strategy and semi-supervised learning method. First, rather than solely identifying anomalies, our query strategy selects uncertain samples according to an adaptive boundary. Second, we apply noise contrastive estimation in training a one-class classification model to incorporate both labeled normal and abnormal data effectively. We analyze that the proposed query strategy and semi-supervised loss individually improve an active learning process of anomaly detection and further improve when combined together on seven anomaly detection datasets.
</details>
<details>
<summary>摘要</summary>
active learning 已经被用作一种高效的工具来建立异常检测模型，通过借助专家反馈来优化模型。在一个 active learning 框架中，一个模型会问选择要被标注的样本，然后使用这些标注的数据样本来重新训练模型。这不仅可以减少获取标注的数据量，还可以提高异常检测性能。然而，大多数现有的研究都是帮助专家标注最多的异常数据样本，这是一种优化的一类分类基于深度异常检测的方法。在这篇论文中，我们解决了 active learning 中的两个重要问题：查询策略和半supervised学习方法。首先，我们的查询策略选择的是一个适应边缘的不确定样本。其次，我们在训练一个一类分类模型时应用了噪声对照估算，以便同时使用标注的正常和异常数据来有效地捕捉一类分类模型。我们分析表明，我们提出的查询策略和半supervised损失函数分别提高了活动学习过程中的异常检测性能，并且当他们结合在一起时，可以更好地提高异常检测性能。我们在七个异常检测数据集上进行了分析。
</details></li>
</ul>
<hr>
<h2 id="CaT-Balanced-Continual-Graph-Learning-with-Graph-Condensation"><a href="#CaT-Balanced-Continual-Graph-Learning-with-Graph-Condensation" class="headerlink" title="CaT: Balanced Continual Graph Learning with Graph Condensation"></a>CaT: Balanced Continual Graph Learning with Graph Condensation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09455">http://arxiv.org/abs/2309.09455</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/superallen13/CaT-CGL">https://github.com/superallen13/CaT-CGL</a></li>
<li>paper_authors: Yilun Liu, Ruihong Qiu, Zi Huang</li>
<li>for: 本研究旨在解决 continual graph learning (CGL) 中的快速卡斯特罗菲特问题，提高模型的稳定性和性能。</li>
<li>methods: 该研究提出了一种 Condense and Train (CaT) 框架，包括对新来 graph 的压缩和存储，以及在内存中直接更新模型。</li>
<li>results: 实验结果表明，CaT 框架可以有效解决快速卡斯特罗菲特问题，并提高 CGL 的效果和效率。<details>
<summary>Abstract</summary>
Continual graph learning (CGL) is purposed to continuously update a graph model with graph data being fed in a streaming manner. Since the model easily forgets previously learned knowledge when training with new-coming data, the catastrophic forgetting problem has been the major focus in CGL. Recent replay-based methods intend to solve this problem by updating the model using both (1) the entire new-coming data and (2) a sampling-based memory bank that stores replayed graphs to approximate the distribution of historical data. After updating the model, a new replayed graph sampled from the incoming graph will be added to the existing memory bank. Despite these methods are intuitive and effective for the CGL, two issues are identified in this paper. Firstly, most sampling-based methods struggle to fully capture the historical distribution when the storage budget is tight. Secondly, a significant data imbalance exists in terms of the scales of the complex new-coming graph data and the lightweight memory bank, resulting in unbalanced training. To solve these issues, a Condense and Train (CaT) framework is proposed in this paper. Prior to each model update, the new-coming graph is condensed to a small yet informative synthesised replayed graph, which is then stored in a Condensed Graph Memory with historical replay graphs. In the continual learning phase, a Training in Memory scheme is used to update the model directly with the Condensed Graph Memory rather than the whole new-coming graph, which alleviates the data imbalance problem. Extensive experiments conducted on four benchmark datasets successfully demonstrate superior performances of the proposed CaT framework in terms of effectiveness and efficiency. The code has been released on https://github.com/superallen13/CaT-CGL.
</details>
<details>
<summary>摘要</summary>
kontinuous graf lerning (CGL) 是为了不断更新一个 graf 模型，使其能够处理流动式的 graf 数据。由于模型容易忘记先前学习的知识，因此 catastrophic forgetting 问题成为了 CGL 的主要关注点。latest replay-based methods 尝试解决这个问题，通过将模型更新使用整个新来的数据和一个储存 replayed graphs 的 memory bank，以便 aproximate 历史数据的分布。在更新模型后，将新的 replayed graph 从进来的 graf 中抽出来，并添加到现有的 memory bank 中。although 这些方法是直觉和有效的，这篇文章中提出了两个问题。首先，大多数抽样方法在存储预算仅仅允许的情况下，很难全面捕捉历史分布。其次，进来的新数据和储存在 memory bank 中的数据 scale 不对称，导致训练不平衡。为解决这些问题，这篇文章提出了一个 Condense and Train (CaT) 框架。在每次模型更新之前，新来的 graf 会被压缩成一个小而有用的 synthesized replayed graph，并将其储存在 Condensed Graph Memory 中。在持续学习阶段，使用 Train in Memory 方法将模型直接更新使用 Condensed Graph Memory，而不是整个新来的 graf。实际实验在四个 benchmark 数据集上显示，CaT 框架在效率和效果上具有优越的表现。code 已经在 https://github.com/superallen13/CaT-CGL 发布。
</details></li>
</ul>
<hr>
<h2 id="Asymptotically-Efficient-Online-Learning-for-Censored-Regression-Models-Under-Non-I-I-D-Data"><a href="#Asymptotically-Efficient-Online-Learning-for-Censored-Regression-Models-Under-Non-I-I-D-Data" class="headerlink" title="Asymptotically Efficient Online Learning for Censored Regression Models Under Non-I.I.D Data"></a>Asymptotically Efficient Online Learning for Censored Regression Models Under Non-I.I.D Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09454">http://arxiv.org/abs/2309.09454</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lantian Zhang, Lei Guo</li>
<li>for:  investigate the asymptotically efficient online learning problem for stochastic censored regression models.</li>
<li>methods: propose a two-step online algorithm, which achieves algorithm convergence and improves estimation performance.</li>
<li>results: show that the algorithm is strongly consistent and asymptotically normal, and the covariances of the estimates can achieve the Cramer-Rao bound asymptotically.<details>
<summary>Abstract</summary>
The asymptotically efficient online learning problem is investigated for stochastic censored regression models, which arise from various fields of learning and statistics but up to now still lacks comprehensive theoretical studies on the efficiency of the learning algorithms. For this, we propose a two-step online algorithm, where the first step focuses on achieving algorithm convergence, and the second step is dedicated to improving the estimation performance. Under a general excitation condition on the data, we show that our algorithm is strongly consistent and asymptotically normal by employing the stochastic Lyapunov function method and limit theories for martingales. Moreover, we show that the covariances of the estimates can achieve the Cramer-Rao (C-R) bound asymptotically, indicating that the performance of the proposed algorithm is the best possible that one can expect in general. Unlike most of the existing works, our results are obtained without resorting to the traditionally used but stringent conditions such as independent and identically distributed (i.i.d) assumption on the data, and thus our results do not exclude applications to stochastic dynamical systems with feedback. A numerical example is also provided to illustrate the superiority of the proposed online algorithm over the existing related ones in the literature.
</details>
<details>
<summary>摘要</summary>
“ Stochastic censored regression 模型中的 asymptotically 高效在线学习问题被研究。这种模型在学习和统计领域中有很多应用，但是现在还没有完整的理论研究。为解决这个问题，我们提出了一个两步在线算法，其中第一步是使算法快速收敛，第二步是提高估计性能。在数据中的普通激动情况下，我们使用杰尼尼抽象函数方法和 martingale 限论来证明我们的算法是强连续和强conv的。此外，我们还证明了估计的协方差可以在极限下达到 Cramer-Rao (C-R)  bound，这表示我们的算法在总体上具有最佳的性能。不同于大多数现有的研究，我们的结果不需要 resorting to i.i.d 假设，因此我们的结果不排除应用于Stochastic dynamical systems with feedback。一个数字Example 也是提供以 Illustrate 我们的在线算法在 literatura 中的优越性。”Note: Please keep in mind that the translation is done by a machine and may not be perfect, especially for idiomatic expressions or cultural references.
</details></li>
</ul>
<hr>
<h2 id="On-the-Use-of-the-Kantorovich-Rubinstein-Distance-for-Dimensionality-Reduction"><a href="#On-the-Use-of-the-Kantorovich-Rubinstein-Distance-for-Dimensionality-Reduction" class="headerlink" title="On the Use of the Kantorovich-Rubinstein Distance for Dimensionality Reduction"></a>On the Use of the Kantorovich-Rubinstein Distance for Dimensionality Reduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09442">http://arxiv.org/abs/2309.09442</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gaël Giordano</li>
<li>for: 这个论文的目的是研究使用康托罗维奇-鲁宾逊距离来建立分类问题中的样本复杂度描述器。</li>
<li>methods: 这篇论文使用了康托罗维奇-鲁宾逊距离来量化样本之间的geometry和topology信息，并将每个类别的点关联到一个措施中。</li>
<li>results: 论文表明，如果康托罗维奇-鲁宾逊距离 между这些措施较大，则存在一个1-Lipschitz分类器，可以良好地分类这些点。<details>
<summary>Abstract</summary>
The goal of this thesis is to study the use of the Kantorovich-Rubinstein distance as to build a descriptor of sample complexity in classification problems. The idea is to use the fact that the Kantorovich-Rubinstein distance is a metric in the space of measures that also takes into account the geometry and topology of the underlying metric space. We associate to each class of points a measure and thus study the geometrical information that we can obtain from the Kantorovich-Rubinstein distance between those measures. We show that a large Kantorovich-Rubinstein distance between those measures allows to conclude that there exists a 1-Lipschitz classifier that classifies well the classes of points. We also discuss the limitation of the Kantorovich-Rubinstein distance as a descriptor.
</details>
<details>
<summary>摘要</summary>
本论文的目标是研究使用庞特罗维奇-鲁比涅斯坦距离来建立分类问题中的样本复杂性描述器。我们使用庞特罗维奇-鲁比涅斯坦距离是度量空间概率的度量，同时也考虑度量空间的几何和 topology。我们对每个类别点分配一个概率，并研究庞特罗维奇-鲁比涅斯坦距离这些概率之间的几何信息。我们显示，当庞特罗维奇-鲁比涅斯坦距离大于某个阈值时，存在1-Lipschitz分类器可以良好地分类点类。我们还讨论了庞特罗维奇-鲁比涅斯坦距离作为描述器的限制。
</details></li>
</ul>
<hr>
<h2 id="DeepHEN-quantitative-prediction-essential-lncRNA-genes-and-rethinking-essentialities-of-lncRNA-genes"><a href="#DeepHEN-quantitative-prediction-essential-lncRNA-genes-and-rethinking-essentialities-of-lncRNA-genes" class="headerlink" title="DeepHEN: quantitative prediction essential lncRNA genes and rethinking essentialities of lncRNA genes"></a>DeepHEN: quantitative prediction essential lncRNA genes and rethinking essentialities of lncRNA genes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10008">http://arxiv.org/abs/2309.10008</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanlin Zhang, Wenzheng Cheng</li>
<li>for: 本研究旨在解释非编码蛋白质基因的必需性。</li>
<li>methods: 该研究使用深度学习和图神经网络来预测非编码蛋白质基因的必需性。</li>
<li>results: 该模型能够预测非编码蛋白质基因的必需性，并能够区分序列特征和网络空间特征对必需性的影响。此外，该模型还能够解决其他方法因为必需性蛋白质基因数量低而导致的过拟合问题。<details>
<summary>Abstract</summary>
Gene essentiality refers to the degree to which a gene is necessary for the survival and reproductive efficacy of a living organism. Although the essentiality of non-coding genes has been documented, there are still aspects of non-coding genes' essentiality that are unknown to us. For example, We do not know the contribution of sequence features and network spatial features to essentiality. As a consequence, in this work, we propose DeepHEN that could answer the above question. By buidling a new lncRNA-proteion-protein network and utilizing both representation learning and graph neural network, we successfully build our DeepHEN models that could predict the essentiality of lncRNA genes. Compared to other methods for predicting the essentiality of lncRNA genes, our DeepHEN model not only tells whether sequence features or network spatial features have a greater influence on essentiality but also addresses the overfitting issue of those methods caused by the low number of essential lncRNA genes, as evidenced by the results of enrichment analysis.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:基因必需性指的是生物体的存活和繁殖能力中基因的必需程度。虽然非编码基因的必需性已经记录，但还有一些非编码基因的必需性还未知。例如，我们不知道序列特征和网络空间特征对必需性的贡献。因此，在这项工作中，我们提出了深度感知（DeepHEN），可以回答上述问题。我们通过构建新的lncRNA-蛋白质-蛋白质网络和使用表示学习和图神经网络，成功建立了我们的深度感知模型，可以预测lncRNA基因的必需性。与其他预测lncRNA基因必需性的方法相比，我们的深度感知模型不仅可以评估序列特征和网络空间特征对必需性的影响，还可以解决这些方法因为低数量必需lncRNA基因而导致的过拟合问题，根据结果分析中的恰合分析结果。
</details></li>
</ul>
<hr>
<h2 id="An-Iterative-Method-for-Unsupervised-Robust-Anomaly-Detection-Under-Data-Contamination"><a href="#An-Iterative-Method-for-Unsupervised-Robust-Anomaly-Detection-Under-Data-Contamination" class="headerlink" title="An Iterative Method for Unsupervised Robust Anomaly Detection Under Data Contamination"></a>An Iterative Method for Unsupervised Robust Anomaly Detection Under Data Contamination</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09436">http://arxiv.org/abs/2309.09436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minkyung Kim, Jongmin Yu, Junsik Kim, Tae-Hyun Oh, Jun Kyun Choi<br>for:这个论文的目的是提高深入型异常检测模型的Robustness，使其能够更好地适应实际数据分布中的异常tail。methods:该论文提出了一种学习框架，通过iteratively更新样本级别的正常性权重，以提高深入型异常检测模型的学习效果。该框架是模型无关和参数适应的，可以应用于现有的异常检测方法。results:在五个异常检测benchmark dataset和两个图像 dataset上，该框架能够提高异常检测模型的Robustness，并且在不同的杂杂度水平下表现出优于现有的异常检测方法。<details>
<summary>Abstract</summary>
Most deep anomaly detection models are based on learning normality from datasets due to the difficulty of defining abnormality by its diverse and inconsistent nature. Therefore, it has been a common practice to learn normality under the assumption that anomalous data are absent in a training dataset, which we call normality assumption. However, in practice, the normality assumption is often violated due to the nature of real data distributions that includes anomalous tails, i.e., a contaminated dataset. Thereby, the gap between the assumption and actual training data affects detrimentally in learning of an anomaly detection model. In this work, we propose a learning framework to reduce this gap and achieve better normality representation. Our key idea is to identify sample-wise normality and utilize it as an importance weight, which is updated iteratively during the training. Our framework is designed to be model-agnostic and hyperparameter insensitive so that it applies to a wide range of existing methods without careful parameter tuning. We apply our framework to three different representative approaches of deep anomaly detection that are classified into one-class classification-, probabilistic model-, and reconstruction-based approaches. In addition, we address the importance of a termination condition for iterative methods and propose a termination criterion inspired by the anomaly detection objective. We validate that our framework improves the robustness of the anomaly detection models under different levels of contamination ratios on five anomaly detection benchmark datasets and two image datasets. On various contaminated datasets, our framework improves the performance of three representative anomaly detection methods, measured by area under the ROC curve.
</details>
<details>
<summary>摘要</summary>
大多数深度异常检测模型基于学习正常性从数据集中学习，由于异常性的多样性和不一致性，因此通常采用学习正常性假设。然而，在实际应用中，正常性假设经常被违反，因为真实数据分布包含异常尾部，即杂杂数据集。这会导致学习异常检测模型的时候， gap between假设和实际训练数据产生负面影响。在这种情况下，我们提出了一种学习框架，以减少这个 gap 并达到更好的正常性表示。我们的关键思想是在样本级别上确定正常性，并将其作为重要性Weight使用，这个Weight在训练过程中进行迭代更新。我们的框架采用模型无关和 гипер参数敏感的设计，可以应用于各种现有方法无需精心参数调整。我们在三种不同的深度异常检测方法上应用了我们的框架，分别是一类分类-, 概率模型-和重建基于的方法。此外，我们还考虑了异常检测目标中的终止条件，并提出了基于异常检测目标的终止 criterion。我们验证了我们的框架可以在不同的杂杂比例下提高异常检测模型的Robustness，并在五个异常检测benchmark datasets和两个图像 datasets上进行验证。在各种杂杂数据集上，我们的框架可以提高三种代表性异常检测方法的性能， measured by area under the ROC curve。
</details></li>
</ul>
<hr>
<h2 id="Distributionally-Time-Varying-Online-Stochastic-Optimization-under-Polyak-Lojasiewicz-Condition-with-Application-in-Conditional-Value-at-Risk-Statistical-Learning"><a href="#Distributionally-Time-Varying-Online-Stochastic-Optimization-under-Polyak-Lojasiewicz-Condition-with-Application-in-Conditional-Value-at-Risk-Statistical-Learning" class="headerlink" title="Distributionally Time-Varying Online Stochastic Optimization under Polyak-Łojasiewicz Condition with Application in Conditional Value-at-Risk Statistical Learning"></a>Distributionally Time-Varying Online Stochastic Optimization under Polyak-Łojasiewicz Condition with Application in Conditional Value-at-Risk Statistical Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09411">http://arxiv.org/abs/2309.09411</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuen-Man Pun, Farhad Farokhi, Iman Shames</li>
<li>for: 这个论文研究了一系列随机优化问题，通过在线优化的视角来探讨。</li>
<li>methods: 论文使用了在线随机梯度 DESCENT 和在线随机 proximal 梯度 DESCENT，并且为这些方法提供了动态 regret bound。</li>
<li>results: 论文证明了在线随机梯度 DESCENT 和在线随机 proximal 梯度 DESCENT 的动态 regret bound，并且应用到了 Conditional Value-at-Risk (CVaR) 学习问题。<details>
<summary>Abstract</summary>
In this work, we consider a sequence of stochastic optimization problems following a time-varying distribution via the lens of online optimization. Assuming that the loss function satisfies the Polyak-{\L}ojasiewicz condition, we apply online stochastic gradient descent and establish its dynamic regret bound that is composed of cumulative distribution drifts and cumulative gradient biases caused by stochasticity. The distribution metric we adopt here is Wasserstein distance, which is well-defined without the absolute continuity assumption or with a time-varying support set. We also establish a regret bound of online stochastic proximal gradient descent when the objective function is regularized. Moreover, we show that the above framework can be applied to the Conditional Value-at-Risk (CVaR) learning problem. Particularly, we improve an existing proof on the discovery of the PL condition of the CVaR problem, resulting in a regret bound of online stochastic gradient descent.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们考虑一个时间变化分布下的随机优化问题序列，通过在线优化的镜头来分析。我们假设损失函数满足波Яakov-{\L}ojasiewicz条件，我们应用在线随机梯度下降并确定其动态 regret bound，该 regret bound包括累积分布漂移和累积梯度偏差由随机性引起的。我们采用的分布度量是沃氏距离，这是不含绝对连续性假设或时间变化支持集的。此外，我们还证明在线随机距离梯度下降可以应用于 conditional Value-at-Risk（CVaR）学习问题。特别是，我们改进了现有的PL条件发现证明，从而获得在线随机梯度下降的 regret bound。
</details></li>
</ul>
<hr>
<h2 id="Guided-Online-Distillation-Promoting-Safe-Reinforcement-Learning-by-Offline-Demonstration"><a href="#Guided-Online-Distillation-Promoting-Safe-Reinforcement-Learning-by-Offline-Demonstration" class="headerlink" title="Guided Online Distillation: Promoting Safe Reinforcement Learning by Offline Demonstration"></a>Guided Online Distillation: Promoting Safe Reinforcement Learning by Offline Demonstration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09408">http://arxiv.org/abs/2309.09408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinning Li, Xinyi Liu, Banghua Zhu, Jiantao Jiao, Masayoshi Tomizuka, Chen Tang, Wei Zhan</li>
<li>for: 提高安全性和效率的强化学习（Reinforcement Learning）策略，使得RL agents可以在具有成本限制的情况下获得高奖励。</li>
<li>methods: 使用大容量模型（如决策变换器DT）来学习离线数据中的专家策略，并通过指导在线安全RL训练来策略减小。</li>
<li>results: GOLD框架可以成功减小离线DT策略，并在实际执行时在安全性和效率两个方面表现出色，在多种安全关键的scenario中解决决策问题。<details>
<summary>Abstract</summary>
Safe Reinforcement Learning (RL) aims to find a policy that achieves high rewards while satisfying cost constraints. When learning from scratch, safe RL agents tend to be overly conservative, which impedes exploration and restrains the overall performance. In many realistic tasks, e.g. autonomous driving, large-scale expert demonstration data are available. We argue that extracting expert policy from offline data to guide online exploration is a promising solution to mitigate the conserveness issue. Large-capacity models, e.g. decision transformers (DT), have been proven to be competent in offline policy learning. However, data collected in real-world scenarios rarely contain dangerous cases (e.g., collisions), which makes it prohibitive for the policies to learn safety concepts. Besides, these bulk policy networks cannot meet the computation speed requirements at inference time on real-world tasks such as autonomous driving. To this end, we propose Guided Online Distillation (GOLD), an offline-to-online safe RL framework. GOLD distills an offline DT policy into a lightweight policy network through guided online safe RL training, which outperforms both the offline DT policy and online safe RL algorithms. Experiments in both benchmark safe RL tasks and real-world driving tasks based on the Waymo Open Motion Dataset (WOMD) demonstrate that GOLD can successfully distill lightweight policies and solve decision-making problems in challenging safety-critical scenarios.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Safe Reinforcement Learning (RL) aims to find a policy that achieves high rewards while satisfying cost constraints. When learning from scratch, safe RL agents tend to be overly conservative, which impedes exploration and restrains the overall performance. In many realistic tasks, e.g. autonomous driving, large-scale expert demonstration data are available. We argue that extracting expert policy from offline data to guide online exploration is a promising solution to mitigate the conserveness issue. Large-capacity models, e.g. decision transformers (DT), have been proven to be competent in offline policy learning. However, data collected in real-world scenarios rarely contain dangerous cases (e.g., collisions), which makes it prohibitive for the policies to learn safety concepts. Besides, these bulk policy networks cannot meet the computation speed requirements at inference time on real-world tasks such as autonomous driving. To this end, we propose Guided Online Distillation (GOLD), an offline-to-online safe RL framework. GOLD distills an offline DT policy into a lightweight policy network through guided online safe RL training, which outperforms both the offline DT policy and online safe RL algorithms. Experiments in both benchmark safe RL tasks and real-world driving tasks based on the Waymo Open Motion Dataset (WOMD) demonstrate that GOLD can successfully distill lightweight policies and solve decision-making problems in challenging safety-critical scenarios."中文翻译：<<SYS>>safe reinforcement learning（RL）的目标是找到一个政策，以高的奖励来满足成本限制。当学习从头开始时，safe RL 代理人往往太保守，这会阻碍探索和限制整体性能。在许多现实任务中，例如自动驾驶，大规模专家示范数据可以用于指导在线探索。我们认为从 offline 数据中提取专家策略来指导在线探索是一个有前途的解决方案，以减少保守性问题。大容量模型，如决策变换器（DT），已经在 offline 政策学习中证明自己的能力。然而，实际场景中收集的数据 редarily 包含危险情况（例如，相撞），这使得政策学习危险概念的缺乏。此外，这些大型政策网络不能在实际任务中实时进行计算，例如自动驾驶。为此，我们提出了 Guided Online Distillation（GOLD），一个 offline-to-online safe RL 框架。GOLD 通过在线安全 RL 培训来精炼 offline DT 政策，并且超过了 offline DT 政策和在线安全 RL 算法。在标准 safe RL 任务和基于 Waymo Open Motion Dataset（WOMD）的实际驾驶任务中，GOLD 可以成功精炼轻量级政策并解决安全关键问题。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/18/cs.LG_2023_09_18/" data-id="closbrorf00pv0g88fs9650fe" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_09_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/18/eess.IV_2023_09_18/" class="article-date">
  <time datetime="2023-09-18T09:00:00.000Z" itemprop="datePublished">2023-09-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/18/eess.IV_2023_09_18/">eess.IV - 2023-09-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Mixed-Graph-Signal-Analysis-of-Joint-Image-Denoising-Interpolation"><a href="#Mixed-Graph-Signal-Analysis-of-Joint-Image-Denoising-Interpolation" class="headerlink" title="Mixed Graph Signal Analysis of Joint Image Denoising &#x2F; Interpolation"></a>Mixed Graph Signal Analysis of Joint Image Denoising &#x2F; Interpolation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10114">http://arxiv.org/abs/2309.10114</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niruhan Viswarupan, Gene Cheung, Fengbo Lan, Michael Brown</li>
<li>for: 这个论文主要是研究如何jointly optimize denoising and interpolation of images from a mixed graph filtering perspective.</li>
<li>methods: 作者使用了linear denoiser和linear interpolator，并研究了在哪些情况下这两个操作应该独立执行，或者合并并且优化。</li>
<li>results: 实验表明，作者的 JOINT denoising &#x2F; interpolation方法在比较不同的情况下都能够显著超过独立的方法。<details>
<summary>Abstract</summary>
A noise-corrupted image often requires interpolation. Given a linear denoiser and a linear interpolator, when should the operations be independently executed in separate steps, and when should they be combined and jointly optimized? We study joint denoising / interpolation of images from a mixed graph filtering perspective: we model denoising using an undirected graph, and interpolation using a directed graph. We first prove that, under mild conditions, a linear denoiser is a solution graph filter to a maximum a posteriori (MAP) problem regularized using an undirected graph smoothness prior, while a linear interpolator is a solution to a MAP problem regularized using a directed graph smoothness prior. Next, we study two variants of the joint interpolation / denoising problem: a graph-based denoiser followed by an interpolator has an optimal separable solution, while an interpolator followed by a denoiser has an optimal non-separable solution. Experiments show that our joint denoising / interpolation method outperformed separate approaches noticeably.
</details>
<details>
<summary>摘要</summary>
受噪图像经常需要 interpolate。给定一个线性去噪器和一个线性 interpolator，当should these operations be independently executed in separate steps，和when should they be combined and jointly optimized？我们研究图像 joint denoising / interpolation from a mixed graph filtering perspective：我们模型denoising using an undirected graph，并 interpolating using a directed graph。我们首先证明，在某些条件下，一个线性去噪器是一个解 graph filter to a maximum a posteriori (MAP) problem regularized using an undirected graph smoothness prior，而一个线性 interpolator是一个解 to a MAP problem regularized using a directed graph smoothness prior。接着，我们研究了两种 joint interpolation / denoising problem variant：一个图像-based denoiser followed by an interpolator has an optimal separable solution，而一个 interpolator followed by a denoiser has an optimal non-separable solution。实验表明，我们的 joint denoising / interpolation method noticeably outperformed separate approaches。
</details></li>
</ul>
<hr>
<h2 id="MAD-Meta-Adversarial-Defense-Benchmark"><a href="#MAD-Meta-Adversarial-Defense-Benchmark" class="headerlink" title="MAD: Meta Adversarial Defense Benchmark"></a>MAD: Meta Adversarial Defense Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09776">http://arxiv.org/abs/2309.09776</a></li>
<li>repo_url: None</li>
<li>paper_authors: X. Peng, D. Zhou, G. Sun, J. Shi, L. Wu</li>
<li>For:	+ The paper aims to address the limitations of existing adversarial training (AT) methods, such as high computational cost, low generalization ability, and the dilemma between the original model and the defense model.* Methods:	+ The paper proposes a novel benchmark called meta adversarial defense (MAD), which consists of two MAD datasets and a MAD evaluation protocol.	+ The paper introduces a meta-learning based adversarial training (Meta-AT) algorithm as the baseline, which features high robustness to unseen adversarial attacks through few-shot learning.* Results:	+ The paper shows that the Meta-AT algorithm outperforms state-of-the-art methods in terms of robustness to adversarial attacks.	+ The paper also demonstrates that the model after Meta-AT maintains a relatively high clean-samples classification accuracy (CCA).Here is the simplified Chinese text for the three key information points:* For:	+ 本文目标是解决现有的针对性训练（AT）方法存在的三大限制，包括高计算成本、低泛化能力和模型与防御模型之间的矛盾。* Methods:	+ 本文提出了一个新的静态防御（MAD） benchmark，包括两个 MAD 数据集和一个 MAD 评估协议。	+ 本文引入了一种基于 meta-学 的针对性训练（Meta-AT）算法作为基准，该算法通过几个 adversarial 攻击实现了高度的鲁棒性。* Results:	+ 本文表明 Meta-AT 算法在针对性攻击方面的性能明显超过了现有的方法。	+ 本文还表明模型 после Meta-AT 保持了相对较高的清样分类精度（CCA）。<details>
<summary>Abstract</summary>
Adversarial training (AT) is a prominent technique employed by deep learning models to defend against adversarial attacks, and to some extent, enhance model robustness. However, there are three main drawbacks of the existing AT-based defense methods: expensive computational cost, low generalization ability, and the dilemma between the original model and the defense model. To this end, we propose a novel benchmark called meta adversarial defense (MAD). The MAD benchmark consists of two MAD datasets, along with a MAD evaluation protocol. The two large-scale MAD datasets were generated through experiments using 30 kinds of attacks on MNIST and CIFAR-10 datasets. In addition, we introduce a meta-learning based adversarial training (Meta-AT) algorithm as the baseline, which features high robustness to unseen adversarial attacks through few-shot learning. Experimental results demonstrate the effectiveness of our Meta-AT algorithm compared to the state-of-the-art methods. Furthermore, the model after Meta-AT maintains a relatively high clean-samples classification accuracy (CCA). It is worth noting that Meta-AT addresses all three aforementioned limitations, leading to substantial improvements. This benchmark ultimately achieved breakthroughs in investigating the transferability of adversarial defense methods to new attacks and the ability to learn from a limited number of adversarial examples. Our codes and attacked datasets address will be available at https://github.com/PXX1110/Meta_AT.
</details>
<details>
<summary>摘要</summary>
<SYS> translate_text=" Adversarial 训练（AT）是深度学习模型防御 против敌意攻击的一种常见技术，同时可以提高模型的鲁棒性。然而，现有的 AT 防御方法存在三个主要缺点：计算成本高、适应能力低、模型与防御模型之间的 contradicton。为此，我们提出了一个新的benchmark，即元敌意防御（MAD）。MAD  benchmark 包括两个 MAD 数据集，以及一个 MAD 评估协议。这两个大规模 MAD 数据集通过对 MNIST 和 CIFAR-10 数据集进行实验而生成。此外，我们引入了基于元学习的 adversarial 训练（Meta-AT）算法，它具有高度鲁棒性，能够通过少量学习对不同攻击来鲁棒化。实验结果表明，我们的 Meta-AT 算法与现状卷积方法相比，有较高的鲁棒性。此外，模型 después Meta-AT 保持了较高的清样分类精度（CCA）。值得注意的是，Meta-AT 解决了所有三种限制，带来了显著的改进。这个benchmark最终实现了对敌意防御方法的传播性和从少量敌意示例中学习的能力进行研究。我们的代码和攻击数据集将在 <https://github.com/PXX1110/Meta_AT> 上发布。</SYS>Here's the translation:这是一个文章，描述了一个新的benchmark，即元敌意防御（MAD），用于测试深度学习模型的防御能力。现有的防御方法有三个主要缺点：计算成本高、适应能力低、模型与防御模型之间的矛盾。为此，我们提出了一个新的Meta-AT算法，具有高度鲁棒性，能够通过少量学习对不同攻击来鲁棒化。实验结果显示，我们的 Meta-AT 算法与现状卷积方法相比，有较高的鲁棒性。此外，模型 después Meta-AT 保持了较高的清样分类精度（CCA）。值得注意的是，Meta-AT 解决了所有三种限制，带来了显著的改进。这个benchmark最终实现了对敌意防御方法的传播性和从少量敌意示例中学习的能力进行研究。我们的代码和攻击数据集将在 <https://github.com/PXX1110/Meta_AT> 上发布。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/18/eess.IV_2023_09_18/" data-id="closbroye01730g880yjy2au7" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/37/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/36/">36</a><a class="page-number" href="/page/37/">37</a><span class="page-number current">38</span><a class="page-number" href="/page/39/">39</a><a class="page-number" href="/page/40/">40</a><span class="space">&hellip;</span><a class="page-number" href="/page/89/">89</a><a class="extend next" rel="next" href="/page/39/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">60</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">118</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">58</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
