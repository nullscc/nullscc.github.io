
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/38/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.SD_2023_10_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/01/cs.SD_2023_10_01/" class="article-date">
  <time datetime="2023-10-01T15:00:00.000Z" itemprop="datePublished">2023-10-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/01/cs.SD_2023_10_01/">cs.SD - 2023-10-01</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="UniAudio-An-Audio-Foundation-Model-Toward-Universal-Audio-Generation"><a href="#UniAudio-An-Audio-Foundation-Model-Toward-Universal-Audio-Generation" class="headerlink" title="UniAudio: An Audio Foundation Model Toward Universal Audio Generation"></a>UniAudio: An Audio Foundation Model Toward Universal Audio Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00704">http://arxiv.org/abs/2310.00704</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yangdongchao/UniAudio_demo">https://github.com/yangdongchao/UniAudio_demo</a></li>
<li>paper_authors: Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang, Songxiang Liu, Xuankai Chang, Jiatong Shi, Sheng Zhao, Jiang Bian, Xixin Wu, Zhou Zhao, Shinji Watanabe, Helen Meng</li>
<li>for: 这个论文的目标是开发一个可以处理多种生成任务的语言模型（LLM），并使其能够生成具有给定输入条件的多种音频类型（包括语音、声音、音乐和歌唱）。</li>
<li>methods: 这个论文使用了一种新的 Tokenization 技术，即 residual vector quantization based neural codec，来处理各种目标音频的tokenization。它还使用了一种多尺度 transformer 模型来处理长度过长的序列问题。</li>
<li>results: 论文在11个任务上实现了州际级或至少竞争性的成绩，并且发现UniAudio模型在所有训练任务中表现出了强大的能力。<details>
<summary>Abstract</summary>
Large Language models (LLM) have demonstrated the capability to handle a variety of generative tasks. This paper presents the UniAudio system, which, unlike prior task-specific approaches, leverages LLM techniques to generate multiple types of audio (including speech, sounds, music, and singing) with given input conditions. UniAudio 1) first tokenizes all types of target audio along with other condition modalities, 2) concatenates source-target pair as a single sequence, and 3) performs next-token prediction using LLM. Also, a multi-scale Transformer model is proposed to handle the overly long sequences caused by the residual vector quantization based neural codec in tokenization. Training of UniAudio is scaled up to 165K hours of audio and 1B parameters, based on all generative tasks, aiming to obtain sufficient prior knowledge not only in the intrinsic properties of audio but also the inter-relationship between audio and other modalities. Therefore, the trained UniAudio model has the potential to become a foundation model for universal audio generation: it shows strong capability in all trained tasks and can seamlessly support new audio generation tasks after simple fine-tuning. Experiments demonstrate that UniAudio achieves state-of-the-art or at least competitive results on most of the 11 tasks. Demo and code are released at https://github.com/yangdongchao/UniAudio
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经证明了处理多种生成任务的能力。这篇论文介绍了UniAudio系统，与前一些任务特定的方法不同，通过LLM技术来生成多种音频（包括语音、声音、音乐和歌唱），并且可以根据输入条件进行生成。UniAudio的实现方式包括以下三个步骤：1. 对所有类型的目标音频进行token化，并将其与其他条件模式一起 concatenate 成一个序列。2. 使用 LLM 进行下一个token预测。3. 使用多级 transformer 模型来处理由 residual vector quantization 基于的 neural codec 生成的过长序列。在训练UniAudio时，使用了165K小时的音频和1B参数，基于所有生成任务，以获得充足的先验知识不仅在音频的内在性能，还在音频和其他模式之间的关系。因此，训练UniAudio模型后，可以作为普适的音频生成基模型，它在所有训练任务中表现出了强大的能力，并且可以通过简单的微调来支持新的音频生成任务。实验结果表明，UniAudio在大多数11个任务中具有国际级或至少竞争力的成绩。示例和代码可以在https://github.com/yangdongchao/UniAudio 中下载。
</details></li>
</ul>
<hr>
<h2 id="Pianist-Identification-Using-Convolutional-Neural-Networks"><a href="#Pianist-Identification-Using-Convolutional-Neural-Networks" class="headerlink" title="Pianist Identification Using Convolutional Neural Networks"></a>Pianist Identification Using Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00699">http://arxiv.org/abs/2310.00699</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/betsytang/pid-cnn">https://github.com/betsytang/pid-cnn</a></li>
<li>paper_authors: Jingjing Tang, Geraint Wiggins, Gyorgy Fazekas</li>
<li>For: 本研究旨在用深度学习技术自动识别表演型钢琴演奏者，解决了建立智能音乐 инструмент和智能音乐系统的挑战。* Methods: 我们使用卷积神经网络和表达特征来实现自动识别，并对大规模的表演型钢琴演奏数据进行了深度学习技术的应用和改进。* Results: 我们的模型在6类识别任务中达到85.3%的准确率，比基eline模型高出了20.8%。我们的改进的数据集也提供了更好的训练数据，为自动演奏者识别做出了重要贡献。<details>
<summary>Abstract</summary>
This paper presents a comprehensive study of automatic performer identification in expressive piano performances using convolutional neural networks (CNNs) and expressive features. Our work addresses the challenging multi-class classification task of identifying virtuoso pianists, which has substantial implications for building dynamic musical instruments with intelligence and smart musical systems. Incorporating recent advancements, we leveraged large-scale expressive piano performance datasets and deep learning techniques. We refined the scores by expanding repetitions and ornaments for more accurate feature extraction. We demonstrated the capability of one-dimensional CNNs for identifying pianists based on expressive features and analyzed the impact of the input sequence lengths and different features. The proposed model outperforms the baseline, achieving 85.3% accuracy in a 6-way identification task. Our refined dataset proved more apt for training a robust pianist identifier, making a substantial contribution to the field of automatic performer identification. Our codes have been released at https://github.com/BetsyTang/PID-CNN.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/01/cs.SD_2023_10_01/" data-id="clpxp6c77010iee88f0ugabux" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_10_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/01/eess.AS_2023_10_01/" class="article-date">
  <time datetime="2023-10-01T14:00:00.000Z" itemprop="datePublished">2023-10-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/01/eess.AS_2023_10_01/">eess.AS - 2023-10-01</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Mechatronic-Generation-of-Datasets-for-Acoustics-Research"><a href="#Mechatronic-Generation-of-Datasets-for-Acoustics-Research" class="headerlink" title="Mechatronic Generation of Datasets for Acoustics Research"></a>Mechatronic Generation of Datasets for Acoustics Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00587">http://arxiv.org/abs/2310.00587</a></li>
<li>repo_url: None</li>
<li>paper_authors: Austin Lu, Ethaniel Moore, Arya Nallanthighall, Kanad Sarkar, Manan Mittal, Ryan M. Corey, Paris Smaragdis, Andrew Singer</li>
<li>for: 这篇论文是为了描述一种机器人共享测试空间，用于实现自动化听音实验。</li>
<li>methods: 该系统使用无线多机器人协调技术，实现同步机器人运动，以适应动态场景中的移动发声器和收音器。用户可以通过虚拟控制界面来设计自动化实验，收集大规模的听音数据。</li>
<li>results: 实验结果表明，MARS系统可以生成高可靠性的听音数据，并且可以帮助研究人员无需特有听音研究空间来收集听音数据。<details>
<summary>Abstract</summary>
We address the challenge of making spatial audio datasets by proposing a shared mechanized recording space that can run custom acoustic experiments: a Mechatronic Acoustic Research System (MARS). To accommodate a wide variety of experiments, we implement an extensible architecture for wireless multi-robot coordination which enables synchronized robot motion for dynamic scenes with moving speakers and microphones. Using a virtual control interface, we can remotely design automated experiments to collect large-scale audio data. This data is shown to be similar across repeated runs, demonstrating the reliability of MARS. We discuss the potential for MARS to make audio data collection accessible for researchers without dedicated acoustic research spaces.
</details>
<details>
<summary>摘要</summary>
我们面临的挑战是创建空间听音数据集，我们提议一种共享机械化录音空间，可以进行自定义听音实验：一个名为 MARS 的机械听音研究系统。为了满足广泛的实验需求，我们实施了可扩展的无线多机器人协调架构，可以实现同步的机器人运动，以便在动态场景中进行移动speaker和 microphone的记录。通过虚拟控制界面，我们可以远程设计自动化实验，收集大规模的听音数据。这些数据显示与重复运行中的相似性，证明 MARS 的可靠性。我们讨论了 MARS 的潜在可能性，使听音数据采集变得对研究人员而言可 accessible。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/01/eess.AS_2023_10_01/" data-id="clpxp6c8x014tee88f8xrg3er" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_10_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/01/cs.CV_2023_10_01/" class="article-date">
  <time datetime="2023-10-01T13:00:00.000Z" itemprop="datePublished">2023-10-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/01/cs.CV_2023_10_01/">cs.CV - 2023-10-01</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Sharingan-A-Transformer-based-Architecture-for-Gaze-Following"><a href="#Sharingan-A-Transformer-based-Architecture-for-Gaze-Following" class="headerlink" title="Sharingan: A Transformer-based Architecture for Gaze Following"></a>Sharingan: A Transformer-based Architecture for Gaze Following</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00816">http://arxiv.org/abs/2310.00816</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samy Tafasca, Anshul Gupta, Jean-Marc Odobez</li>
<li>for: 这 paper 是为了研究人类视线跟踪的模型，以便在各种应用领域中使用。</li>
<li>methods: 这 paper 使用了一种新的 transformer-based 架构来实现 2D 视线预测。</li>
<li>results: 这 paper 在 GazeFollow 和 VideoAttentionTarget 数据集上 achieved state-of-the-art 结果。Here’s the full translation in Simplified Chinese:</li>
<li>for: 这 paper 是为了研究人类视线跟踪的模型，以便在各种应用领域中使用。</li>
<li>methods: 这 paper 使用了一种新的 transformer-based 架构来实现 2D 视线预测。</li>
<li>results: 这 paper 在 GazeFollow 和 VideoAttentionTarget 数据集上 achieved state-of-the-art 结果。I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
Gaze is a powerful form of non-verbal communication and social interaction that humans develop from an early age. As such, modeling this behavior is an important task that can benefit a broad set of application domains ranging from robotics to sociology. In particular, Gaze Following is defined as the prediction of the pixel-wise 2D location where a person in the image is looking. Prior efforts in this direction have focused primarily on CNN-based architectures to perform the task. In this paper, we introduce a novel transformer-based architecture for 2D gaze prediction. We experiment with 2 variants: the first one retains the same task formulation of predicting a gaze heatmap for one person at a time, while the second one casts the problem as a 2D point regression and allows us to perform multi-person gaze prediction with a single forward pass. This new architecture achieves state-of-the-art results on the GazeFollow and VideoAttentionTarget datasets. The code for this paper will be made publicly available.
</details>
<details>
<summary>摘要</summary>
gaze 是一种强大的非语言通信和社交互动方式，人类从 early age 开始发展。因此，模拟这种行为是一项重要的任务，可以 benefiting  Broad 应用领域，从机器人学到社会学。特别是，瞥向预测（Gaze Following）定义为图像中人员的 pixel-wise 2D 位置预测。先前的尝试都是通过 CNN  arquitectures 来完成这项任务。在这篇论文中，我们提出了一种新的 transformer 结构来实现 2D 瞥向预测。我们实验了两个变体：第一个保持了同样的任务表述，即预测一个人的瞥向热图 ; 第二个将问题定义为2D 点 regression，允许我们通过单一的前进 pass 进行多人瞥向预测。这种新的结构实现了 GazeFollow 和 VideoAttentionTarget 数据集的状态图。代码将公开发布。
</details></li>
</ul>
<hr>
<h2 id="Completing-Visual-Objects-via-Bridging-Generation-and-Segmentation"><a href="#Completing-Visual-Objects-via-Bridging-Generation-and-Segmentation" class="headerlink" title="Completing Visual Objects via Bridging Generation and Segmentation"></a>Completing Visual Objects via Bridging Generation and Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00808">http://arxiv.org/abs/2310.00808</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Li, Yinpeng Chen, Chung-Ching Lin, Rita Singh, Bhiksha Raj, Zicheng Liu</li>
<li>for:  reconstruction of a complete object from its partially visible components</li>
<li>methods: iterative stages of generation and segmentation, with the object mask provided as an additional condition</li>
<li>results: superior object completion results compared to existing approaches such as ControlNet and Stable Diffusion<details>
<summary>Abstract</summary>
This paper presents a novel approach to object completion, with the primary goal of reconstructing a complete object from its partially visible components. Our method, named MaskComp, delineates the completion process through iterative stages of generation and segmentation. In each iteration, the object mask is provided as an additional condition to boost image generation, and, in return, the generated images can lead to a more accurate mask by fusing the segmentation of images. We demonstrate that the combination of one generation and one segmentation stage effectively functions as a mask denoiser. Through alternation between the generation and segmentation stages, the partial object mask is progressively refined, providing precise shape guidance and yielding superior object completion results. Our experiments demonstrate the superiority of MaskComp over existing approaches, e.g., ControlNet and Stable Diffusion, establishing it as an effective solution for object completion.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种新的物体完成方法，主要目标是从部分可见的组件中重建完整的物体。我们的方法，名为MaskComp，通过 iterate 的生成和分割阶段来进行分割。在每个迭代阶段，提供对象Mask作为附加条件，以提高图像生成，并在返回的图像中提取更加精确的Mask。我们发现，通过生成和分割阶段的交互，可以有效地减少Mask的噪声。通过 alternate 生成和分割阶段，部分物体Mask可以逐渐进行精细化，提供精确的形状指导，并且实现了更好的物体完成效果。我们的实验表明，MaskComp 比 existed 方法（如 ControlNet 和 Stable Diffusion）更加有效， establishing 它为物体完成的有效解决方案。
</details></li>
</ul>
<hr>
<h2 id="Propagating-Semantic-Labels-in-Video-Data"><a href="#Propagating-Semantic-Labels-in-Video-Data" class="headerlink" title="Propagating Semantic Labels in Video Data"></a>Propagating Semantic Labels in Video Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00783">http://arxiv.org/abs/2310.00783</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Balaban, Justin Medich, Pranay Gosar, Justin Hart</li>
<li>for: 这个论文的目的是提出一种基于Foundation Models的视频 segmentation方法，以减少人工标注成本。</li>
<li>methods: 该方法使用Segment Anything Model (SAM)和Structure from Motion (SfM)两种技术来实现视频 segmentation。首先，视频输入被重构为3D几何结构使用SfM，然后使用SAM进行每帧的分割。最后，对于每帧的分割结果，进行3D几何投影，以便在新的视角下进行跟踪。</li>
<li>results: 该方法可以大幅减少人工标注成本，但是与人工标注的性能相比，其性能有所下降。三个主要纪录器都用于评估系统性能：计算时间、面积 overlap with manual labels和跟踪损失数量。结果表明，该系统在跟踪对象在视频帧上的计算时间方面具有显著的提高，但是在性能方面却存在一定的下降。<details>
<summary>Abstract</summary>
Semantic Segmentation combines two sub-tasks: the identification of pixel-level image masks and the application of semantic labels to those masks. Recently, so-called Foundation Models have been introduced; general models trained on very large datasets which can be specialized and applied to more specific tasks. One such model, the Segment Anything Model (SAM), performs image segmentation. Semantic segmentation systems such as CLIPSeg and MaskRCNN are trained on datasets of paired segments and semantic labels. Manual labeling of custom data, however, is time-consuming. This work presents a method for performing segmentation for objects in video. Once an object has been found in a frame of video, the segment can then be propagated to future frames; thus reducing manual annotation effort. The method works by combining SAM with Structure from Motion (SfM). The video input to the system is first reconstructed into 3D geometry using SfM. A frame of video is then segmented using SAM. Segments identified by SAM are then projected onto the the reconstructed 3D geometry. In subsequent video frames, the labeled 3D geometry is reprojected into the new perspective, allowing SAM to be invoked fewer times. System performance is evaluated, including the contributions of the SAM and SfM components. Performance is evaluated over three main metrics: computation time, mask IOU with manual labels, and the number of tracking losses. Results demonstrate that the system has substantial computation time improvements over human performance for tracking objects over video frames, but suffers in performance.
</details>
<details>
<summary>摘要</summary>
Semantic Segmentation 将两个子任务结合在一起：Pixel-level图像mask的标识和图像mask的semantic标签应用。最近，称之为基础模型的模型被引入，这些模型可以在很大的数据集上训练，然后应用到更特定的任务上。一个such model是Segment Anything Model（SAM），它实现了图像 segmentation。图像 segmentation系统such as CLIPSeg和MaskRCNN通常是在paired segments和semantic labels的数据集上训练的。然而，手动标注自定义数据是时间consuming。这个工作提出了一种方法，通过结合SAM和Structure from Motion（SfM）来实现对视频帧中对象的分割。首先，视频输入被重建为3D几何结构使用SfM。然后，在SAM中Segment一帧视频。由SAM标识的分割被 проекted onto the reconstructed 3D几何结构。在后续的视频帧中，标注的3D几何结构被重新投影到新的视角，以便在新的视频帧中invoked SAM fewer times。系统性能被评估，包括SAM和SfM组件的贡献。性能被评估以三个主要指标：计算时间、mask IOU with manual labels和跟踪损失数。结果表明，系统在跟踪对象在视频帧之间的计算时间上有substantial的提高，但是性能不如人工标注。
</details></li>
</ul>
<hr>
<h2 id="SMOOT-Saliency-Guided-Mask-Optimized-Online-Training"><a href="#SMOOT-Saliency-Guided-Mask-Optimized-Online-Training" class="headerlink" title="SMOOT: Saliency Guided Mask Optimized Online Training"></a>SMOOT: Saliency Guided Mask Optimized Online Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00772">http://arxiv.org/abs/2310.00772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Karkehabadi, Houman Homayoun, Avesta Sasan</li>
<li>for: 这种论文的目的是提出一种新的隐藏导航法（Saliency-Guided Training，SGT），以提高深度神经网络的解释性。</li>
<li>methods: 这种方法使用反射和修改Gradient来引导模型强调最重要的特征，以提高模型的解释性。</li>
<li>results: 实验结果表明，我们的提案可以有效地提高模型的准确率和隐藏特征的明确度。<details>
<summary>Abstract</summary>
Deep Neural Networks are powerful tools for understanding complex patterns and making decisions. However, their black-box nature impedes a complete understanding of their inner workings. Saliency-Guided Training (SGT) methods try to highlight the prominent features in the model's training based on the output to alleviate this problem. These methods use back-propagation and modified gradients to guide the model toward the most relevant features while keeping the impact on the prediction accuracy negligible. SGT makes the model's final result more interpretable by masking input partially. In this way, considering the model's output, we can infer how each segment of the input affects the output. In the particular case of image as the input, masking is applied to the input pixels. However, the masking strategy and number of pixels which we mask, are considered as a hyperparameter. Appropriate setting of masking strategy can directly affect the model's training. In this paper, we focus on this issue and present our contribution. We propose a novel method to determine the optimal number of masked images based on input, accuracy, and model loss during the training. The strategy prevents information loss which leads to better accuracy values. Also, by integrating the model's performance in the strategy formula, we show that our model represents the salient features more meaningful. Our experimental results demonstrate a substantial improvement in both model accuracy and the prominence of saliency, thereby affirming the effectiveness of our proposed solution.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Counterfactual-Image-Generation-for-adversarially-robust-and-interpretable-Classifiers"><a href="#Counterfactual-Image-Generation-for-adversarially-robust-and-interpretable-Classifiers" class="headerlink" title="Counterfactual Image Generation for adversarially robust and interpretable Classifiers"></a>Counterfactual Image Generation for adversarially robust and interpretable Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00761">http://arxiv.org/abs/2310.00761</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rafael Bischof, Florian Scheidegger, Michael A. Kraus, A. Cristiano I. Malossi</li>
<li>for: 这种方法的目的是提高神经网络图像分类器的解释性和robustness。</li>
<li>methods: 该方法使用图像到图像翻译生成器（GANs）来生成对应的替换样本，以提高解释性和对抗性。</li>
<li>results: 该方法可以生成高度描述性的解释图像，并且可以提高模型对抗性。此外，该方法还可以用来评估模型的不确定性。<details>
<summary>Abstract</summary>
Neural Image Classifiers are effective but inherently hard to interpret and susceptible to adversarial attacks. Solutions to both problems exist, among others, in the form of counterfactual examples generation to enhance explainability or adversarially augment training datasets for improved robustness. However, existing methods exclusively address only one of the issues. We propose a unified framework leveraging image-to-image translation Generative Adversarial Networks (GANs) to produce counterfactual samples that highlight salient regions for interpretability and act as adversarial samples to augment the dataset for more robustness. This is achieved by combining the classifier and discriminator into a single model that attributes real images to their respective classes and flags generated images as "fake". We assess the method's effectiveness by evaluating (i) the produced explainability masks on a semantic segmentation task for concrete cracks and (ii) the model's resilience against the Projected Gradient Descent (PGD) attack on a fruit defects detection problem. Our produced saliency maps are highly descriptive, achieving competitive IoU values compared to classical segmentation models despite being trained exclusively on classification labels. Furthermore, the model exhibits improved robustness to adversarial attacks, and we show how the discriminator's "fakeness" value serves as an uncertainty measure of the predictions.
</details>
<details>
<summary>摘要</summary>
Our framework combines the classifier and discriminator into a single model, which attributes real images to their respective classes and flags generated images as "fake". We evaluate the effectiveness of our method by assessing the produced explainability masks on a semantic segmentation task for concrete cracks and the model's resilience against the Projected Gradient Descent (PGD) attack on a fruit defects detection problem.Our produced saliency maps are highly descriptive and achieve competitive IoU values compared to classical segmentation models, despite being trained exclusively on classification labels. Additionally, the model exhibits improved robustness to adversarial attacks, and we show how the discriminator's "fakeness" value serves as an uncertainty measure of the predictions.
</details></li>
</ul>
<hr>
<h2 id="Top-down-Green-ups-Satellite-Sensing-and-Deep-Models-to-Predict-Buffelgrass-Phenology"><a href="#Top-down-Green-ups-Satellite-Sensing-and-Deep-Models-to-Predict-Buffelgrass-Phenology" class="headerlink" title="Top-down Green-ups: Satellite Sensing and Deep Models to Predict Buffelgrass Phenology"></a>Top-down Green-ups: Satellite Sensing and Deep Models to Predict Buffelgrass Phenology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00740">http://arxiv.org/abs/2310.00740</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lurosenb/phenology_projects">https://github.com/lurosenb/phenology_projects</a></li>
<li>paper_authors: Lucas Rosenblatt, Bin Han, Erin Posthumus, Theresa Crimmins, Bill Howe</li>
<li>For: 预测buffelgrass的”绿化”（即Ready for herbicidal treatment），以预防南部美国的严重野火和生物多样性损失。* Methods: 使用卫星感知和深度学习模型，包括时间、视觉和多模态模型，以提高预测 buffelgrass 绿化的精度。* Results: 所有神经网络基于的方法都超越了传统 buffelgrass 绿化模型，并讨论了如何实现神经网络模型的部署，以实现 significiant resource savings。<details>
<summary>Abstract</summary>
An invasive species of grass known as "buffelgrass" contributes to severe wildfires and biodiversity loss in the Southwest United States. We tackle the problem of predicting buffelgrass "green-ups" (i.e. readiness for herbicidal treatment). To make our predictions, we explore temporal, visual and multi-modal models that combine satellite sensing and deep learning. We find that all of our neural-based approaches improve over conventional buffelgrass green-up models, and discuss how neural model deployment promises significant resource savings.
</details>
<details>
<summary>摘要</summary>
“一种入侵性的草本植物──牛肚草”在南部美国引起了严重的野火和生物多样性损失。我们面临着预测牛肚草“绿化”（即Ready for 药物处理）的问题。为了实现这一目标，我们探讨了时间、视觉和多模态模型， combining satellite sensing和深度学习。我们发现所有的神经网络方法都超过了传统的牛肚草绿化模型，并讨论了如何部署神经网络模型以实现显著的资源节约。”Note that "牛肚草" (bù dù cǎo) is the Simplified Chinese term for "buffelgrass".
</details></li>
</ul>
<hr>
<h2 id="HOH-Markerless-Multimodal-Human-Object-Human-Handover-Dataset-with-Large-Object-Count"><a href="#HOH-Markerless-Multimodal-Human-Object-Human-Handover-Dataset-with-Large-Object-Count" class="headerlink" title="HOH: Markerless Multimodal Human-Object-Human Handover Dataset with Large Object Count"></a>HOH: Markerless Multimodal Human-Object-Human Handover Dataset with Large Object Count</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00723">http://arxiv.org/abs/2310.00723</a></li>
<li>repo_url: None</li>
<li>paper_authors: Noah Wiederhold, Ava Megyeri, DiMaggio Paris, Sean Banerjee, Natasha Kholgade Banerjee</li>
<li>for: 论文主要用于促进数据驱动的手夹研究、人机手夹实现以及人工智能手夹参数估计的数据集。</li>
<li>methods: 本论文使用了多视图RGB和深度数据、skeleton、笔oint clouds、抓取类型和手夹性labels、物体、接受手和发送手2D和3D分割、接受手和发送手舒适评分、对象元数据和对应的3D模型等数据来描述136种物品的人类互动。</li>
<li>results: 本论文通过使用HOH数据集进行神经网络训练，实现了抓取、orientation和轨迹预测等任务。相比标注数据集，HOH数据集不需要特定的装备，可以更自然地捕捉人类之间的手夹互动，并且包含了人类手夹互动的高分辨率手夹跟踪数据。至今为止，HOH数据集是手夹数据集中最大的物品数、参与者数、对应的对话对数和总交互记录的数据集。<details>
<summary>Abstract</summary>
We present the HOH (Human-Object-Human) Handover Dataset, a large object count dataset with 136 objects, to accelerate data-driven research on handover studies, human-robot handover implementation, and artificial intelligence (AI) on handover parameter estimation from 2D and 3D data of person interactions. HOH contains multi-view RGB and depth data, skeletons, fused point clouds, grasp type and handedness labels, object, giver hand, and receiver hand 2D and 3D segmentations, giver and receiver comfort ratings, and paired object metadata and aligned 3D models for 2,720 handover interactions spanning 136 objects and 20 giver-receiver pairs-40 with role-reversal-organized from 40 participants. We also show experimental results of neural networks trained using HOH to perform grasp, orientation, and trajectory prediction. As the only fully markerless handover capture dataset, HOH represents natural human-human handover interactions, overcoming challenges with markered datasets that require specific suiting for body tracking, and lack high-resolution hand tracking. To date, HOH is the largest handover dataset in number of objects, participants, pairs with role reversal accounted for, and total interactions captured.
</details>
<details>
<summary>摘要</summary>
我们提出了人机物交换数据集（HOH），包含136种物品，以加速基于数据驱动的手柄研究、人机手柄实现和人工智能（AI）在手柄参数估计方面。HOH包含多视角RGB和深度数据、skeleton、粘合点云、抓取类型和手征标注、物品、付给人手和接受人手2D和3D分割、付给人和接受人的舒适评分、对应的对象元数据和对应的3D模型。我们还显示了使用HOH训练神经网络进行抓取、方向和轨迹预测的实验结果。作为唯一的无标记手柄捕捉数据集，HOH表现了自然的人人手柄交换互动，超越了标记数据集需要特定的服装以满足身体跟踪的问题，以及缺乏高分辨率手征跟踪。到目前为止，HOH是手柄数据集中最大的物品数、参与者数、对话对数和总交换次数。
</details></li>
</ul>
<hr>
<h2 id="Logical-Bias-Learning-for-Object-Relation-Prediction"><a href="#Logical-Bias-Learning-for-Object-Relation-Prediction" class="headerlink" title="Logical Bias Learning for Object Relation Prediction"></a>Logical Bias Learning for Object Relation Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00712">http://arxiv.org/abs/2310.00712</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyu Zhou, Zihan Ji, Anna Zhu</li>
<li>for: 提高Scene Graph生成（SGG）的精度和可靠性，以提高图像理解和下游任务的能力。</li>
<li>methods: 基于 causal inference 的对象关系预测策略，并提出一个对象提升模块来进行缺省研究。</li>
<li>results: 在Visual Gnome 150（VG-150）dataset上实验证明了我们提议的方法的有效性。<details>
<summary>Abstract</summary>
Scene graph generation (SGG) aims to automatically map an image into a semantic structural graph for better scene understanding. It has attracted significant attention for its ability to provide object and relation information, enabling graph reasoning for downstream tasks. However, it faces severe limitations in practice due to the biased data and training method. In this paper, we present a more rational and effective strategy based on causal inference for object relation prediction. To further evaluate the superiority of our strategy, we propose an object enhancement module to conduct ablation studies. Experimental results on the Visual Gnome 150 (VG-150) dataset demonstrate the effectiveness of our proposed method. These contributions can provide great potential for foundation models for decision-making.
</details>
<details>
<summary>摘要</summary>
Scene graph generation (SGG) 目标是自动将图像映射到 semantic 结构图，以提高场景理解。它吸引了大量注意力，因为它可以提供对象和关系信息，使得图reasoning 可能。然而，在实践中，它面临严重的限制，主要是因为数据和训练方法偏向。在这篇论文中，我们提出了基于 causal inference 的更合理和有效的策略，用于对象关系预测。为了进一步证明我们的策略的超越性，我们提出了对象增强模块进行缺失研究。实验结果表明，我们提posed 方法在 Visual Gnome 150 (VG-150) 数据集上得到了较好的效果。这些贡献可以为基础模型提供巨大的潜力。
</details></li>
</ul>
<hr>
<h2 id="You-Do-Not-Need-Additional-Priors-in-Camouflage-Object-Detection"><a href="#You-Do-Not-Need-Additional-Priors-in-Camouflage-Object-Detection" class="headerlink" title="You Do Not Need Additional Priors in Camouflage Object Detection"></a>You Do Not Need Additional Priors in Camouflage Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00702">http://arxiv.org/abs/2310.00702</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuchen Dong, Heng Zhou, Chengyang Li, Junjie Xie, Yongqiang Xie, Zhongbo Li</li>
<li>for: 本研究旨在开发一种不需要额外知识的掩蔽物检测网络，以解决现有方法强调额外知识的问题。</li>
<li>methods: 我们提出了一种新的自适应特征综合方法，通过多层特征信息的组合生成导航信息，不同于之前的方法，我们直接从图像特征中提取信息来导航模型训练。</li>
<li>results: 我们通过广泛的实验结果表明，我们的提议方法可以与现有的方法相比或超越其性能。<details>
<summary>Abstract</summary>
Camouflage object detection (COD) poses a significant challenge due to the high resemblance between camouflaged objects and their surroundings. Although current deep learning methods have made significant progress in detecting camouflaged objects, many of them heavily rely on additional prior information. However, acquiring such additional prior information is both expensive and impractical in real-world scenarios. Therefore, there is a need to develop a network for camouflage object detection that does not depend on additional priors. In this paper, we propose a novel adaptive feature aggregation method that effectively combines multi-layer feature information to generate guidance information. In contrast to previous approaches that rely on edge or ranking priors, our method directly leverages information extracted from image features to guide model training. Through extensive experimental results, we demonstrate that our proposed method achieves comparable or superior performance when compared to state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
高度掩蔽物检测（COD）具有 significannot challenges，因为掩蔽物和周围环境的高度相似性。当前深度学习方法已经在检测掩蔽物方面做出了 significannot进步，但大多数其中依赖于额外的先验信息。然而，在实际场景中获取这种额外先验信息是both expensive和不实际的。因此，有必要开发一种不依赖于额外先验信息的掩蔽物检测网络。在这篇论文中，我们提出了一种新的 adaptive feature aggregation 方法，可以有效地将多层特征信息集成成导航信息。与先前的方法相比，我们的方法直接利用图像特征中提取的信息来导航模型训练。通过广泛的实验结果，我们证明了我们提出的方法可以与当前状态的方法相比或更高的性能。
</details></li>
</ul>
<hr>
<h2 id="A-quantum-moving-target-segmentation-algorithm-for-grayscale-video"><a href="#A-quantum-moving-target-segmentation-algorithm-for-grayscale-video" class="headerlink" title="A quantum moving target segmentation algorithm for grayscale video"></a>A quantum moving target segmentation algorithm for grayscale video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03038">http://arxiv.org/abs/2310.03038</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjie Liu, Lu Wang, Qingshan Wu</li>
<li>for: 用于实时分割视频中移动目标。</li>
<li>methods: 使用量子机制同时计算所有邻帧图像差异，然后快速分割移动目标。设计了可行的量子比较器，用于判断灰度值与阈值的差异。</li>
<li>results: 对 IBM Q 进行实验，确认了我们的算法在不纯量子时代（NISQ）中的可行性。对于一个量子视频包含 $2^m$ 帧 ($每帧是 $2^n\times 2^n$ 图像，每个像素有 $q$ 灰度水平），我们的算法的复杂度可以降至 O $(n^2 + q) $。与 классический对比，它具有对数快速速度增长，同时也高于现有的量子算法。<details>
<summary>Abstract</summary>
The moving target segmentation (MTS) aims to segment out moving targets in the video, however, the classical algorithm faces the huge challenge of real-time processing in the current video era. Some scholars have successfully demonstrated the quantum advantages in some video processing tasks, but not concerning moving target segmentation. In this paper, a quantum moving target segmentation algorithm for grayscale video is proposed, which can use quantum mechanism to simultaneously calculate the difference of all pixels in all adjacent frames and then quickly segment out the moving target. In addition, a feasible quantum comparator is designed to distinguish the grayscale values with the threshold. Then several quantum circuit units, including three-frame difference, binarization and AND operation, are designed in detail, and then are combined together to construct the complete quantum circuits for segmenting the moving target. For a quantum video with $2^m$ frames (every frame is a $2^n\times 2^n$ image with $q$ grayscale levels), the complexity of our algorithm can be reduced to O$(n^2 + q)$. Compared with the classic counterpart, it is an exponential speedup, while its complexity is also superior to the existing quantum algorithms. Finally, the experiment is conducted on IBM Q to show the feasibility of our algorithm in the noisy intermediate-scale quantum (NISQ) era.
</details>
<details>
<summary>摘要</summary>
traditional Chinese version:运动目标分割（MTS）的目标是将影像中的运动目标分割出来，但 класиical algorithm在现今的影像时代中面临巨大的实时处理挑战。一些学者已经成功地显示了量子优势在一些影像处理任务中，但不包括运动目标分割。本文提出了一个量子运动目标分割算法 для灰度影像，可以使用量子机制同时计算所有帧的差值，快速地分割出运动目标。此外，一个可行的量子比较器也被设计出来，用于区分灰度值与阈值。然后，一些量子Circuit单元，包括三帧差值、binarization 和 AND 操作，在细节中被设计出来，然后被组合起来建立完整的量子Circuits для分割运动目标。对于一个具有 $2^m$ 帧影像（每帧是 $2^n\times 2^n$ 图像，每个像素有 $q$ 灰度水平）的量子影像，我们的算法的复杂度可以降至 O $(n^2 + q)$。相比 классиical counterpart，这是一个指数快速的优化，而且其复杂度也高于现有的量子算法。最后，我们在 IBM Q 上进行实验，以显示我们的算法在不确定中等量子（NISQ）时代的可行性。Here's the translation in Simplified Chinese:运动目标分割（MTS）的目标是将影像中的运动目标分割出来，但 classical algorithm在现今的影像时代中面临巨大的实时处理挑战。一些学者已经成功地显示了量子优势在一些影像处理任务中，但不包括运动目标分割。本文提出了一个量子运动目标分割算法 для灰度影像，可以使用量子机制同时计算所有帧的差值，快速地分割出运动目标。此外，一个可行的量子比较器也被设计出来，用于区分灰度值与阈值。然后，一些量子Circuit单元，包括三帧差值、binarization 和 AND 操作，在细节中被设计出来，然后被组合起来建立完整的量子Circuits для分割运动目标。对于一个具有 $2^m$ 帧影像（每帧是 $2^n\times 2^n$ 图像，每个像素有 $q$ 灰度水平）的量子影像，我们的算法的复杂度可以降至 O $(n^2 + q)$。相比 classical counterpart，这是一个指数快速的优化，而且其复杂度也高于现有的量子算法。最后，我们在 IBM Q 上进行实验，以显示我们的算法在不确定中等量子（NISQ）时代的可行性。
</details></li>
</ul>
<hr>
<h2 id="Comics-for-Everyone-Generating-Accessible-Text-Descriptions-for-Comic-Strips"><a href="#Comics-for-Everyone-Generating-Accessible-Text-Descriptions-for-Comic-Strips" class="headerlink" title="Comics for Everyone: Generating Accessible Text Descriptions for Comic Strips"></a>Comics for Everyone: Generating Accessible Text Descriptions for Comic Strips</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00698">http://arxiv.org/abs/2310.00698</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reshma Ramaprasad</li>
<li>for: 为了让漫画可以对视障群体开放，提供可读的自然语言描述。</li>
<li>methods: 使用计算机视觉技术提取漫画图片中的信息，包括panel、角色和文本信息，然后使用这些信息作为多Modal大语言模型的提示，生成描述。</li>
<li>results: 对一组已经得到人工注释的漫画进行测试，测试结果具有较好的量化和质量指标。<details>
<summary>Abstract</summary>
Comic strips are a popular and expressive form of visual storytelling that can convey humor, emotion, and information. However, they are inaccessible to the BLV (Blind or Low Vision) community, who cannot perceive the images, layouts, and text of comics. Our goal in this paper is to create natural language descriptions of comic strips that are accessible to the visually impaired community. Our method consists of two steps: first, we use computer vision techniques to extract information about the panels, characters, and text of the comic images; second, we use this information as additional context to prompt a multimodal large language model (MLLM) to produce the descriptions. We test our method on a collection of comics that have been annotated by human experts and measure its performance using both quantitative and qualitative metrics. The outcomes of our experiments are encouraging and promising.
</details>
<details>
<summary>摘要</summary>
漫画是一种受欢迎且表达力强的视觉故事形式，可以传达幽默、情感和信息。然而，它们对视障（Blind or Low Vision）社区不可见，无法感受到漫画的图片、布局和文本。我们的目标是创建可访问的漫画描述，以便让视障社区可以享受漫画的乐趣。我们的方法包括两步：第一步，我们使用计算机视觉技术提取漫画图片中的信息，包括画格、人物和文本信息；第二步，我们使用这些信息作为多模态大语言模型（MLLM）的提示，以生成描述。我们对一个收录了人工 эксперTS的漫画集进行测试，并使用量化和质量指标评估我们的方法的性能。实验结果很Encouraging和Promising。
</details></li>
</ul>
<hr>
<h2 id="A-Hierarchical-Graph-based-Approach-for-Recognition-and-Description-Generation-of-Bimanual-Actions-in-Videos"><a href="#A-Hierarchical-Graph-based-Approach-for-Recognition-and-Description-Generation-of-Bimanual-Actions-in-Videos" class="headerlink" title="A Hierarchical Graph-based Approach for Recognition and Description Generation of Bimanual Actions in Videos"></a>A Hierarchical Graph-based Approach for Recognition and Description Generation of Bimanual Actions in Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00670">http://arxiv.org/abs/2310.00670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fatemeh Ziaeetabar, Reza Safabakhsh, Saeedeh Momtazi, Minija Tamosiunaite, Florentin Wörgötter</li>
<li>for: 这项研究旨在提高视频中人体动作的描述精度和全面性，以满足机器人学、人机交互和视频分析等领域的需求。</li>
<li>methods: 该研究提出了一种新的方法，结合图形模型和层次嵌入式注意机制，以提高视频描述的精度和全面性。该方法首先编码视频中对象和动作之间的空间时间相互关系，然后使用三级建构的层次注意机制，以recognize本地和全局上下文元素。</li>
<li>results: 对多个2D和3D数据集进行了实验，并与状态对比，该方法 consistently 获得了更高的准确率、精度和上下文相关性。在大量的减少实验中，我们也评估了不同组件的作用。该方法可以生成不同 semantic 深度的描述，类似于不同人的描述。此外，更深入的二手手Object交互的理解可能会降低人工智能领域中的机器人模拟动作的准确性。<details>
<summary>Abstract</summary>
Nuanced understanding and the generation of detailed descriptive content for (bimanual) manipulation actions in videos is important for disciplines such as robotics, human-computer interaction, and video content analysis. This study describes a novel method, integrating graph based modeling with layered hierarchical attention mechanisms, resulting in higher precision and better comprehensiveness of video descriptions. To achieve this, we encode, first, the spatio-temporal inter dependencies between objects and actions with scene graphs and we combine this, in a second step, with a novel 3-level architecture creating a hierarchical attention mechanism using Graph Attention Networks (GATs). The 3-level GAT architecture allows recognizing local, but also global contextual elements. This way several descriptions with different semantic complexity can be generated in parallel for the same video clip, enhancing the discriminative accuracy of action recognition and action description. The performance of our approach is empirically tested using several 2D and 3D datasets. By comparing our method to the state of the art we consistently obtain better performance concerning accuracy, precision, and contextual relevance when evaluating action recognition as well as description generation. In a large set of ablation experiments we also assess the role of the different components of our model. With our multi-level approach the system obtains different semantic description depths, often observed in descriptions made by different people, too. Furthermore, better insight into bimanual hand-object interactions as achieved by our model may portend advancements in the field of robotics, enabling the emulation of intricate human actions with heightened precision.
</details>
<details>
<summary>摘要</summary>
importance of nuanced understanding and detailed descriptive content for (bimanual) manipulation actions in videos is crucial for fields such as robotics, human-computer interaction, and video content analysis. This study introduces a novel method that combines graph-based modeling with layered hierarchical attention mechanisms, resulting in more precise and comprehensive video descriptions. To achieve this, we first encode the spatio-temporal interdependencies between objects and actions using scene graphs, and then combine this with a novel 3-level architecture that creates a hierarchical attention mechanism using Graph Attention Networks (GATs). The 3-level GAT architecture allows for the recognition of both local and global contextual elements, enabling the generation of multiple descriptions with different semantic complexity for the same video clip. This approach improves the discriminative accuracy of action recognition and description generation. Our method is empirically tested on several 2D and 3D datasets, and we consistently obtain better performance compared to the state of the art in terms of accuracy, precision, and contextual relevance. In a series of ablation experiments, we also assess the role of the different components of our model. Our multi-level approach enables the system to obtain different semantic description depths, often observed in descriptions made by different people, and may also contribute to advancements in the field of robotics by enabling the emulation of intricate human actions with heightened precision.
</details></li>
</ul>
<hr>
<h2 id="Liveness-Detection-Competition-–-Noncontact-based-Fingerprint-Algorithms-and-Systems-LivDet-2023-Noncontact-Fingerprint"><a href="#Liveness-Detection-Competition-–-Noncontact-based-Fingerprint-Algorithms-and-Systems-LivDet-2023-Noncontact-Fingerprint" class="headerlink" title="Liveness Detection Competition – Noncontact-based Fingerprint Algorithms and Systems (LivDet-2023 Noncontact Fingerprint)"></a>Liveness Detection Competition – Noncontact-based Fingerprint Algorithms and Systems (LivDet-2023 Noncontact Fingerprint)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00659">http://arxiv.org/abs/2310.00659</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sandip Purnapatra, Humaira Rezaie, Bhavin Jawade, Yu Liu, Yue Pan, Luke Brosell, Mst Rumana Sumi, Lambert Igene, Alden Dimarco, Srirangaraj Setlur, Soumyabrata Dey, Stephanie Schuckers, Marco Huber, Jan Niklas Kolf, Meiling Fang, Naser Damer, Banafsheh Adami, Raul Chitic, Karsten Seelert, Vishesh Mistry, Rahul Parthe, Umit Kacar</li>
<li>For: The paper is written for the assessment and reporting of state-of-the-art in Presentation Attack Detection (PAD) using noncontact fingerprint-based methods.* Methods: The paper uses a noncontact fingerprint-based PAD competition for algorithms and systems, with a common evaluation protocol that includes finger photos of various Presentation Attack Instruments (PAIs) and live fingers.* Results: The winning algorithm achieved an APCER of 11.35% and a BPCER of 0.62%, while the winning system achieved an APCER of 13.04% and a BPCER of 1.68%. Additionally, four-finger systems that make individual finger-based PAD decisions were also tested.Here are the three key points in Simplified Chinese text:* For: 这篇论文是用于评估和报告非接触指纹基于方法的攻击检测（PAD）的国际竞赛系列。* Methods: 这篇论文使用了一种非接触指纹基于的PAD竞赛，使用共同评估协议，包括指纹 фотографирования多种攻击工具（PAIs）和真实的手指。* Results: 赢家算法实现了APCER的11.35%和BPCER的0.62%，而赢家系统实现了APCER的13.04%和BPCER的1.68%。此外，四根手指系统也进行了个体指纹基于的PAD决策。<details>
<summary>Abstract</summary>
Liveness Detection (LivDet) is an international competition series open to academia and industry with the objec-tive to assess and report state-of-the-art in Presentation Attack Detection (PAD). LivDet-2023 Noncontact Fingerprint is the first edition of the noncontact fingerprint-based PAD competition for algorithms and systems. The competition serves as an important benchmark in noncontact-based fingerprint PAD, offering (a) independent assessment of the state-of-the-art in noncontact-based fingerprint PAD for algorithms and systems, and (b) common evaluation protocol, which includes finger photos of a variety of Presentation Attack Instruments (PAIs) and live fingers to the biometric research community (c) provides standard algorithm and system evaluation protocols, along with the comparative analysis of state-of-the-art algorithms from academia and industry with both old and new android smartphones. The winning algorithm achieved an APCER of 11.35% averaged overall PAIs and a BPCER of 0.62%. The winning system achieved an APCER of 13.0.4%, averaged over all PAIs tested over all the smartphones, and a BPCER of 1.68% over all smartphones tested. Four-finger systems that make individual finger-based PAD decisions were also tested. The dataset used for competition will be available 1 to all researchers as per data share protocol
</details>
<details>
<summary>摘要</summary>
生命检测（LivDet）是一个国际竞赛系列，开放于学术和产业领域，旨在评估和报告当前最佳的演示攻击检测（PAD）技术。LivDet-2023非接触指纹是第一届非接触指纹基于PAD竞赛，用于评估和比较不同算法和系统的性能。这个竞赛作为非接触指纹PAD领域的重要标准，提供了独立的评估标准，以及一套共同的评估协议。该竞赛包括了多种演示攻击工具（PAIs）和真实的手指图像，以及一套标准的评估协议。winning algorithm achieved an APCER of 11.35% and a BPCER of 0.62% over all PAIs, and the winning system achieved an APCER of 13.04% and a BPCER of 1.68% over all smartphones tested. In addition, four-finger systems that make individual finger-based PAD decisions were also tested. The dataset used for the competition will be made available to all researchers according to the data sharing protocol.
</details></li>
</ul>
<hr>
<h2 id="Beyond-Task-Performance-Evaluating-and-Reducing-the-Flaws-of-Large-Multimodal-Models-with-In-Context-Learning"><a href="#Beyond-Task-Performance-Evaluating-and-Reducing-the-Flaws-of-Large-Multimodal-Models-with-In-Context-Learning" class="headerlink" title="Beyond Task Performance: Evaluating and Reducing the Flaws of Large Multimodal Models with In-Context Learning"></a>Beyond Task Performance: Evaluating and Reducing the Flaws of Large Multimodal Models with In-Context Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00647">http://arxiv.org/abs/2310.00647</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mshukor/EvALign-ICL">https://github.com/mshukor/EvALign-ICL</a></li>
<li>paper_authors: Mustafa Shukor, Alexandre Rame, Corentin Dancette, Matthieu Cord</li>
<li>for: 这个论文旨在探讨大型多模型（LMMs）的问题和局限性，以及如何通过增强ICL（增强内容学习）来解决这些问题。</li>
<li>methods: 这个论文使用了8种不同的开源LMM（基于FLAMINGO架构），并对这些模型进行了5个轴的评估：幻觉、抑郁、 композиitional、解释性和遵循指令。此外，论文还研究了ICL的效果于LMMs的问题。</li>
<li>results: 论文发现，尽管LMMs在任务性能方面表现出色，但它们仍然存在许多问题，例如幻觉、抑郁、不compositional和解释性不足。ICL可以有效解决一些问题，但并不能解决所有问题。此外，论文还提出了一些新的多模态ICL方法，如多任务ICL、链式回忆ICL和自我修正ICL，以解决LMMs的问题。<details>
<summary>Abstract</summary>
Following the success of Large Language Models (LLMs), Large Multimodal Models (LMMs), such as the Flamingo model and its subsequent competitors, have started to emerge as natural steps towards generalist agents. However, interacting with recent LMMs reveals major limitations that are hardly captured by the current evaluation benchmarks. Indeed, task performances (e.g., VQA accuracy) alone do not provide enough clues to understand their real capabilities, limitations, and to which extent such models are aligned to human expectations. To refine our understanding of those flaws, we deviate from the current evaluation paradigm and propose the EvALign-ICL framework, in which we (1) evaluate 8 recent open-source LMMs (based on the Flamingo architecture such as OpenFlamingo and IDEFICS) on 5 different axes; hallucinations, abstention, compositionality, explainability and instruction following. Our evaluation on these axes reveals major flaws in LMMs. To efficiently address these problems, and inspired by the success of in-context learning (ICL) in LLMs, (2) we explore ICL as a solution and study how it affects these limitations. Based on our ICL study, (3) we push ICL further and propose new multimodal ICL approaches such as; Multitask-ICL, Chain-of-Hindsight-ICL, and Self-Correcting-ICL. Our findings are as follows; (1) Despite their success, LMMs have flaws that remain unsolved with scaling alone. (2) The effect of ICL on LMMs flaws is nuanced; despite its effectiveness for improved explainability, abstention, and instruction following, ICL does not improve compositional abilities, and actually even amplifies hallucinations. (3) The proposed ICL variants are promising as post-hoc approaches to efficiently tackle some of those flaws. The code is available here: https://evalign-icl.github.io/
</details>
<details>
<summary>摘要</summary>
以 Large Language Models (LLMs) 的成功为契机，Large Multimodal Models (LMMs) 也在出现，如FLAMINGO模型和其竞争对手。然而，与最近的 LMMs 交互后，我们发现它们存在重要的局限性，这些局限性并不被当前的评价标准完全捕捉。实际上，任务性能（如 VQA 准确率） alone 不能够反映它们的真正能力和局限性，以及与人类期望的对应度。为了更好地理解这些问题，我们在评价标准之外尝试了 EvALign-ICL 框架，其中我们（1）评价了 8 个最近开源 LMMs（基于 FLAMINGO 架构，如 OpenFlamingo 和 IDEFICS）在 5 个轴上，即幻觉、抑制、复合性、解释性和遵从性。我们的评价表明，LMMs 存在重要的问题。为了有效地解决这些问题，我们（2）探索了 ICL 的潜在作用，并研究了 ICL 如何影响这些局限性。基于我们的 ICL 研究，我们（3）将 ICL 推广到多Modal ICL，并提出了新的多模态 ICL 方法，如 Multitask-ICL、Chain-of-Hindsight-ICL 和 Self-Correcting-ICL。我们的发现是，（1）虽然 LMMs 成功，但它们仍存在不解决的问题，不能通过缩放 alone 解决。（2）ICL 对 LMMs 的缺陷有复杂的影响，虽有效提高了解释性、抑制和遵从性，但是不会改善复合性，并且实际上会加剧幻觉。（3）我们提出的 ICL 变体是可以有效地解决一些问题的后续方法。代码可以在以下链接获取：https://evalign-icl.github.io/
</details></li>
</ul>
<hr>
<h2 id="RegBN-Batch-Normalization-of-Multimodal-Data-with-Regularization"><a href="#RegBN-Batch-Normalization-of-Multimodal-Data-with-Regularization" class="headerlink" title="RegBN: Batch Normalization of Multimodal Data with Regularization"></a>RegBN: Batch Normalization of Multimodal Data with Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00641">http://arxiv.org/abs/2310.00641</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mogvision/regbn">https://github.com/mogvision/regbn</a></li>
<li>paper_authors: Morteza Ghahremani, Christian Wachinger</li>
<li>for: 这篇论文的目的是提出一种新的多modal资料Normalization方法，以便将多种不同的数据模式融合在一起，提高模组的表现。</li>
<li>methods: 这篇论文使用了RegBN方法，具有调整Regularization的功能，可以干预干扰因素和背景噪音的影响，并且可以跨多个数据模式进行normalization。</li>
<li>results: 这篇论文在八个数据库上进行验证，包括语言、音频、图像、视频、深度、表格和3D MRI等多种数据模式，以及不同的架构（如多层感知神经网络、卷积神经网络和视觉转移神经网络），展示了RegBN方法的通用性和效iveness。<details>
<summary>Abstract</summary>
Recent years have witnessed a surge of interest in integrating high-dimensional data captured by multisource sensors, driven by the impressive success of neural networks in the integration of multimodal data. However, the integration of heterogeneous multimodal data poses a significant challenge, as confounding effects and dependencies among such heterogeneous data sources introduce unwanted variability and bias, leading to suboptimal performance of multimodal models. Therefore, it becomes crucial to normalize the low- or high-level features extracted from data modalities before their fusion takes place. This paper introduces a novel approach for the normalization of multimodal data, called RegBN, that incorporates regularization. RegBN uses the Frobenius norm as a regularizer term to address the side effects of confounders and underlying dependencies among different data sources. The proposed method generalizes well across multiple modalities and eliminates the need for learnable parameters, simplifying training and inference. We validate the effectiveness of RegBN on eight databases from five research areas, encompassing diverse modalities such as language, audio, image, video, depth, tabular, and 3D MRI. The proposed method demonstrates broad applicability across different architectures such as multilayer perceptrons, convolutional neural networks, and vision transformers, enabling effective normalization of both low- and high-level features in multimodal neural networks. RegBN is available at \url{https://github.com/mogvision/regbn}.
</details>
<details>
<summary>摘要</summary>
近年来，有一个强大的兴趣在将多维数据集成到多源感知器中，这主要归功于神经网络在多模态数据的集成中的出色成绩。然而，多模态数据的集成带来一些挑战，因为不同的数据来源之间存在干扰效应和依赖关系，这会导致模型的性能下降。因此，在模型融合之前，需要对数据模式中的低级或高级特征进行Normalization。这篇文章提出了一种新的多模态数据Normalization方法，称为RegBN，它包含了正则化项。RegBN使用 Frobenius  нор为正则化项，以解决不同数据来源之间的干扰效应和依赖关系。提出的方法可以通过多种模式进行扩展，无需学习参数，因此训练和推理变得更加简单。我们在八个数据库中进行了验证，包括语言、音频、图像、视频、深度、表格和3D MRI 等多种模式，RegBN 的效果广泛，可以effective地Normalization多模式的低级和高级特征。RegBN 可以在多层感知器、卷积神经网络和视transformer 等不同的架构上进行应用，为多模态神经网络的Normalization提供了一个简单的解决方案。RegBN 的代码可以在 <https://github.com/mogvision/regbn> 上下载。
</details></li>
</ul>
<hr>
<h2 id="Segmentation-based-Assessment-of-Tumor-Vessel-Involvement-for-Surgical-Resectability-Prediction-of-Pancreatic-Ductal-Adenocarcinoma"><a href="#Segmentation-based-Assessment-of-Tumor-Vessel-Involvement-for-Surgical-Resectability-Prediction-of-Pancreatic-Ductal-Adenocarcinoma" class="headerlink" title="Segmentation-based Assessment of Tumor-Vessel Involvement for Surgical Resectability Prediction of Pancreatic Ductal Adenocarcinoma"></a>Segmentation-based Assessment of Tumor-Vessel Involvement for Surgical Resectability Prediction of Pancreatic Ductal Adenocarcinoma</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00639">http://arxiv.org/abs/2310.00639</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christiaan Viviers, Mark Ramaekers, Amaan Valiuddin, Terese Hellström, Nick Tasios, John van der Ven, Igor Jacobs, Lotte Ewals, Joost Nederend, Peter de With, Misha Luyer, Fons van der Sommen</li>
<li>for:  This research aims to provide a workflow and deep learning-based segmentation models to automatically assess tumor-vessel involvement in Pancreatic ductal adenocarcinoma (PDAC) patients, which is crucial for determining treatment options and improving patient outcomes.</li>
<li>methods:  The proposed workflow involves processing CT scans to segment the tumor and vascular structures, analyzing spatial relationships and the extent of vascular involvement, using three different deep learning-based segmentation architectures (nnU-Net, 3D U-Net, and Probabilistic 3D U-Net).</li>
<li>results:  The segmentations achieved a high accuracy in segmenting veins, arteries, and the tumor, and enabled automated detection of tumor involvement with high accuracy (0.88 sensitivity and 0.86 specificity). Additionally, the models captured uncertainty in the predicted involvement, providing clinicians with a clear indication of tumor-vessel involvement and facilitating more informed decision-making for surgical interventions.<details>
<summary>Abstract</summary>
Pancreatic ductal adenocarcinoma (PDAC) is a highly aggressive cancer with limited treatment options. This research proposes a workflow and deep learning-based segmentation models to automatically assess tumor-vessel involvement, a key factor in determining tumor resectability. Correct assessment of resectability is vital to determine treatment options. The proposed workflow involves processing CT scans to segment the tumor and vascular structures, analyzing spatial relationships and the extent of vascular involvement, which follows a similar way of working as expert radiologists in PDAC assessment. Three segmentation architectures (nnU-Net, 3D U-Net, and Probabilistic 3D U-Net) achieve a high accuracy in segmenting veins, arteries, and the tumor. The segmentations enable automated detection of tumor involvement with high accuracy (0.88 sensitivity and 0.86 specificity) and automated computation of the degree of tumor-vessel contact. Additionally, due to significant inter-observer variability in these important structures, we present the uncertainty captured by each of the models to further increase insights into the predicted involvement. This result provides clinicians with a clear indication of tumor-vessel involvement and may be used to facilitate more informed decision-making for surgical interventions. The proposed method offers a valuable tool for improving patient outcomes, personalized treatment strategies and survival rates in pancreatic cancer.
</details>
<details>
<summary>摘要</summary>
《胰腺ductal adenocarcinoma（PDAC）是一种高度侵略性的Cancer，具有有限的治疗选择。本研究提出了一种工作流程和深度学习基于的分割模型，以自动评估肿瘤-血管涉及度，这是确定肿瘤可否切除的关键因素。正确评估可以决定疗程选择。本工作流程包括对CT扫描图进行肿瘤和血管结构分割，分析肿瘤和血管之间的空间关系和血管涉及度，与专业放射科医生在PDAC评估中采用相似的方法。三种分割建筑（nnU-Net、3D U-Net和概率3D U-Net）实现了高精度分割血管、肿瘤和血管。这些分割可以自动检测肿瘤涉及度，并计算肿瘤与血管之间的接触度，并且由于肿瘤-血管结构之间存在显著的Observer variability，我们还提供了每个模型对应的不确定性，以增加预测涉及度的信息。这些结果为临床医生提供了诊断肿瘤涉及度的清晰指导，可能用于改进患者的疗效、个性化治疗策略和存活率。》
</details></li>
</ul>
<hr>
<h2 id="Win-Win-Training-High-Resolution-Vision-Transformers-from-Two-Windows"><a href="#Win-Win-Training-High-Resolution-Vision-Transformers-from-Two-Windows" class="headerlink" title="Win-Win: Training High-Resolution Vision Transformers from Two Windows"></a>Win-Win: Training High-Resolution Vision Transformers from Two Windows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00632">http://arxiv.org/abs/2310.00632</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vincent Leroy, Jerome Revaud, Thomas Lucas, Philippe Weinzaepfel</li>
<li>For: 提高高分辨率视觉转换器的训练和执行效率。* Methods: 随机窗口Masking技术，使模型只需学习每个窗口内的本地交互，以及不同窗口间的全局交互。* Results: 在推理时直接处理高分辨率输入，不需特殊处理，并且在 semantic segmentation 和 optical flow 任务上达到了最佳性能。<details>
<summary>Abstract</summary>
Transformers have become the standard in state-of-the-art vision architectures, achieving impressive performance on both image-level and dense pixelwise tasks. However, training vision transformers for high-resolution pixelwise tasks has a prohibitive cost. Typical solutions boil down to hierarchical architectures, fast and approximate attention, or training on low-resolution crops. This latter solution does not constrain architectural choices, but it leads to a clear performance drop when testing at resolutions significantly higher than that used for training, thus requiring ad-hoc and slow post-processing schemes. In this paper, we propose a novel strategy for efficient training and inference of high-resolution vision transformers: the key principle is to mask out most of the high-resolution inputs during training, keeping only N random windows. This allows the model to learn local interactions between tokens inside each window, and global interactions between tokens from different windows. As a result, the model can directly process the high-resolution input at test time without any special trick. We show that this strategy is effective when using relative positional embedding such as rotary embeddings. It is 4 times faster to train than a full-resolution network, and it is straightforward to use at test time compared to existing approaches. We apply this strategy to the dense monocular task of semantic segmentation, and find that a simple setting with 2 windows performs best, hence the name of our method: Win-Win. To demonstrate the generality of our contribution, we further extend it to the binocular task of optical flow, reaching state-of-the-art performance on the Spring benchmark that contains Full-HD images with an inference time an order of magnitude faster than the best competitor.
</details>
<details>
<summary>摘要</summary>
启示器变得是现代视觉建筑标准，在图像级和密集像素级任务上达到了印象性的表现。然而，在高分辨率像素级任务上训练视觉启示器有束缚的成本。常见的解决方案包括层次结构、快速和 aproximate 注意力以及在训练低分辨率裁剪上进行训练。这个后者不会限制建筑选择，但会导致在测试分辨率远高于训练分辨率时的表现下降，需要特殊的预处理方案。在这篇论文中，我们提出了一种新的高分辨率视觉启示器训练和执行策略：关键原则是在训练时随机隐藏大多数高分辨率输入，只保留N个随机窗口。这 позвоits 模型学习每个窗口内Token之间的本地互动，以及不同窗口内Token之间的全局互动。因此，模型可以直接在测试时处理高分辨率输入，不需要特殊的技巧。我们发现，使用相对位置嵌入，如旋转嵌入，这种策略是最效的。训练时间比普通网络快四倍，并且在测试时使用非常简单。我们在激素分割任务中应用了这种策略，并发现使用2个窗口得到最佳性能，因此我们称之为Win-Win。为了证明我们的贡献的通用性，我们进一步扩展了它到双目任务中，达到了SpringBenchmark上的最新纪录，该纪录包含高清晰度图像，并且在执行时间上比最佳竞争者快一个数量级。
</details></li>
</ul>
<hr>
<h2 id="Finger-UNet-A-U-Net-based-Multi-Task-Architecture-for-Deep-Fingerprint-Enhancement"><a href="#Finger-UNet-A-U-Net-based-Multi-Task-Architecture-for-Deep-Fingerprint-Enhancement" class="headerlink" title="Finger-UNet: A U-Net based Multi-Task Architecture for Deep Fingerprint Enhancement"></a>Finger-UNet: A U-Net based Multi-Task Architecture for Deep Fingerprint Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00629">http://arxiv.org/abs/2310.00629</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ekta Gavas, Anoop Namboodiri</li>
<li>for: 提高低质量指纹识别率</li>
<li>methods: 使用Discrete Wavelet Transform（DWT）进行指纹提高，并使用波峰注意模块代替最大池化，同时使用多任务学习和方向估计任务进行指纹重建。</li>
<li>results: 在FVC 2002和NIST SD302数据库上进行实验，证明我们的方法可以提高低质量指纹识别率，并且比前一些方法更高效。<details>
<summary>Abstract</summary>
For decades, fingerprint recognition has been prevalent for security, forensics, and other biometric applications. However, the availability of good-quality fingerprints is challenging, making recognition difficult. Fingerprint images might be degraded with a poor ridge structure and noisy or less contrasting backgrounds. Hence, fingerprint enhancement plays a vital role in the early stages of the fingerprint recognition/verification pipeline. In this paper, we investigate and improvise the encoder-decoder style architecture and suggest intuitive modifications to U-Net to enhance low-quality fingerprints effectively. We investigate the use of Discrete Wavelet Transform (DWT) for fingerprint enhancement and use a wavelet attention module instead of max pooling which proves advantageous for our task. Moreover, we replace regular convolutions with depthwise separable convolutions, which significantly reduces the memory footprint of the model without degrading the performance. We also demonstrate that incorporating domain knowledge with fingerprint minutiae prediction task can improve fingerprint reconstruction through multi-task learning. Furthermore, we also integrate the orientation estimation task to propagate the knowledge of ridge orientations to enhance the performance further. We present the experimental results and evaluate our model on FVC 2002 and NIST SD302 databases to show the effectiveness of our approach compared to previous works.
</details>
<details>
<summary>摘要</summary>
In this paper, we improve the encoder-decoder style architecture and suggest intuitive modifications to U-Net to enhance low-quality fingerprints effectively. We use Discrete Wavelet Transform (DWT) for fingerprint enhancement and replace regular convolutions with depthwise separable convolutions, which significantly reduces the memory footprint of the model without compromising performance.Moreover, we incorporate domain knowledge with fingerprint minutiae prediction tasks to improve fingerprint reconstruction through multi-task learning. We also integrate orientation estimation tasks to propagate the knowledge of ridge orientations and enhance performance further.We present experimental results and evaluate our model on FVC 2002 and NIST SD302 databases to demonstrate the effectiveness of our approach compared to previous works.
</details></li>
</ul>
<hr>
<h2 id="GhostEncoder-Stealthy-Backdoor-Attacks-with-Dynamic-Triggers-to-Pre-trained-Encoders-in-Self-supervised-Learning"><a href="#GhostEncoder-Stealthy-Backdoor-Attacks-with-Dynamic-Triggers-to-Pre-trained-Encoders-in-Self-supervised-Learning" class="headerlink" title="GhostEncoder: Stealthy Backdoor Attacks with Dynamic Triggers to Pre-trained Encoders in Self-supervised Learning"></a>GhostEncoder: Stealthy Backdoor Attacks with Dynamic Triggers to Pre-trained Encoders in Self-supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00626">http://arxiv.org/abs/2310.00626</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiannan Wang, Changchun Yin, Zhe Liu, Liming Fang, Run Wang, Chenhao Lin</li>
<li>for: 本研究旨在提出一种隐藏式、动态Backdoor攻击方法，用于自动学习预训练的图像编码器。</li>
<li>methods: 该攻击方法利用图像隐写技术，将隐藏信息编码到无害图像中，生成后门样本。然后，通过精制预训练图像编码器，植入后门。</li>
<li>results: 试验结果表明，GhostEncoder可以在图像上实现高度的隐藏性，让目标模型具有高度的攻击成功率，而不会丢失其实用性。此外，GhostEncoder也可以抵御现有的防御技术。<details>
<summary>Abstract</summary>
Within the realm of computer vision, self-supervised learning (SSL) pertains to training pre-trained image encoders utilizing a substantial quantity of unlabeled images. Pre-trained image encoders can serve as feature extractors, facilitating the construction of downstream classifiers for various tasks. However, the use of SSL has led to an increase in security research related to various backdoor attacks. Currently, the trigger patterns used in backdoor attacks on SSL are mostly visible or static (sample-agnostic), making backdoors less covert and significantly affecting the attack performance. In this work, we propose GhostEncoder, the first dynamic invisible backdoor attack on SSL. Unlike existing backdoor attacks on SSL, which use visible or static trigger patterns, GhostEncoder utilizes image steganography techniques to encode hidden information into benign images and generate backdoor samples. We then fine-tune the pre-trained image encoder on a manipulation dataset to inject the backdoor, enabling downstream classifiers built upon the backdoored encoder to inherit the backdoor behavior for target downstream tasks. We evaluate GhostEncoder on three downstream tasks and results demonstrate that GhostEncoder provides practical stealthiness on images and deceives the victim model with a high attack success rate without compromising its utility. Furthermore, GhostEncoder withstands state-of-the-art defenses, including STRIP, STRIP-Cl, and SSL-Cleanse.
</details>
<details>
<summary>摘要</summary>
在计算机视觉领域，自主学习（SSL）指的是使用大量未标注图像进行训练已经预训练的图像编码器。这些预训练图像编码器可以作为特征提取器，帮助建立下游分类器 для多种任务。然而，使用SSL带来了安全研究中的各种后门攻击。现在，许多后门攻击使用SSL的触发模式都是可见或静止的（样本不具特定），这使得后门变得更加明显，对攻击性能产生负面影响。在这种情况下，我们提出了 GhostEncoder，首个在SSL中的动态隐藏后门攻击。与现有的SSL后门攻击不同，GhostEncoder使用图像隐写技术来编码隐藏信息到正常图像中，并生成后门样本。然后，我们精细调整预训练图像编码器，使其在扭曲数据集上进行后门插入，使得基于后门编码器的下游分类器继承后门行为，并且不会增加负面影响。我们对 GhostEncoder 进行了三个下游任务的评估，结果表明，GhostEncoder 在图像上具有实际的隐藏性，诱导了受试模型，并且不会降低其实用性。此外，GhostEncoder 可以抵御当前的防御技术，包括 STRIP、STRIP-Cl 和 SSL-Cleanse。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Adversarial-Transferability-in-Federated-Learning"><a href="#Understanding-Adversarial-Transferability-in-Federated-Learning" class="headerlink" title="Understanding Adversarial Transferability in Federated Learning"></a>Understanding Adversarial Transferability in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00616">http://arxiv.org/abs/2310.00616</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yijiang Li, Ying Gao, Haohan Wang<br>for:  This paper investigates the robustness and security issues of federated learning (FL) systems in a practical setting where malicious clients disguise their identities and launch transferable adversarial attacks.methods: The paper uses empirical experiments and theoretical analysis to study the robustness of FL systems against such attacks, and hypothesizes that the decentralized training on distributed data and the averaging operation contribute to the system’s robustness.results: The paper finds that the federated model is more robust compared to its centralized counterpart when the accuracy on clean images is comparable, and provides evidence from both empirical experiments and theoretical analysis to support this conclusion.<details>
<summary>Abstract</summary>
We investigate the robustness and security issues from a novel and practical setting: a group of malicious clients has impacted the model during training by disguising their identities and acting as benign clients, and only revealing their adversary position after the training to conduct transferable adversarial attacks with their data, which is usually a subset of the data that FL system is trained with. Our aim is to offer a full understanding of the challenges the FL system faces in this practical setting across a spectrum of configurations. We notice that such an attack is possible, but the federated model is more robust compared with its centralized counterpart when the accuracy on clean images is comparable. Through our study, we hypothesized the robustness is from two factors: the decentralized training on distributed data and the averaging operation. We provide evidence from both the perspective of empirical experiments and theoretical analysis. Our work has implications for understanding the robustness of federated learning systems and poses a practical question for federated learning applications.
</details>
<details>
<summary>摘要</summary>
我们研究了一种新和实际的场景中的安全和稳定性问题：一群恶意客户端在训练过程中对模型产生了影响，通过掩饰自己的身份和行为如善意客户端，并只在训练后 revelation 自己为敌对位置，以进行可转移性攻击。我们的目标是对 Federated Learning 系统在这种实际场景中所面临的挑战进行全面的理解，并通过不同的配置进行spectrum 的研究。我们发现这种攻击是可能的，但在模型级别的清洁图像准确率相似时， federated model 比其中央化模型更加稳定。我们认为这种稳定性来自两个因素：分布式训练在分布式数据上和平均操作。我们通过实验和理论分析提供证据。我们的工作对 Federated Learning 系统的稳定性有重要的意义，并提出了实际问题 для Federated Learning 应用。
</details></li>
</ul>
<hr>
<h2 id="Scene-aware-Human-Motion-Forecasting-via-Mutual-Distance-Prediction"><a href="#Scene-aware-Human-Motion-Forecasting-via-Mutual-Distance-Prediction" class="headerlink" title="Scene-aware Human Motion Forecasting via Mutual Distance Prediction"></a>Scene-aware Human Motion Forecasting via Mutual Distance Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00615">http://arxiv.org/abs/2310.00615</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chaoyue Xing, Wei Mao, Miaomiao Liu</li>
<li>for: 本研究强调解决人体动作预测中的场景相关性问题，通过模elling人体-场景交互来预测未来人体动作。</li>
<li>methods: 我们提出了基于人体-场景距离的人体动作预测方法，其中距离包括人体Vertex与场景表面之间的积分距离和基准场景点与人体网格之间的距离。我们还开发了一个预测步骤两步管道，先预测未来距离，然后根据预测距离预测未来人体动作。在训练过程中，我们显式地促进了预测pose与距离之间的一致性。</li>
<li>results: 我们的方法在 sintetic和实际数据集上比前学者的方法表现更好，提高了人体动作预测的精度和可靠性。<details>
<summary>Abstract</summary>
In this paper, we tackle the problem of scene-aware 3D human motion forecasting. A key challenge of this task is to predict future human motions that are consistent with the scene, by modelling the human-scene interactions. While recent works have demonstrated that explicit constraints on human-scene interactions can prevent the occurrence of ghost motion, they only provide constraints on partial human motion e.g., the global motion of the human or a few joints contacting the scene, leaving the rest motion unconstrained. To address this limitation, we propose to model the human-scene interaction with the mutual distance between the human body and the scene. Such mutual distances constrain both the local and global human motion, resulting in a whole-body motion constrained prediction. In particular, mutual distance constraints consist of two components, the signed distance of each vertex on the human mesh to the scene surface, and the distance of basis scene points to the human mesh. We develop a pipeline with two prediction steps that first predicts the future mutual distances from the past human motion sequence and the scene, and then forecasts the future human motion conditioning on the predicted mutual distances. During training, we explicitly encourage consistency between the predicted poses and the mutual distances. Our approach outperforms the state-of-the-art methods on both synthetic and real datasets.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Skip-Plan-Procedure-Planning-in-Instructional-Videos-via-Condensed-Action-Space-Learning"><a href="#Skip-Plan-Procedure-Planning-in-Instructional-Videos-via-Condensed-Action-Space-Learning" class="headerlink" title="Skip-Plan: Procedure Planning in Instructional Videos via Condensed Action Space Learning"></a>Skip-Plan: Procedure Planning in Instructional Videos via Condensed Action Space Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00608">http://arxiv.org/abs/2310.00608</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiheng Li, Wenjia Geng, Muheng Li, Lei Chen, Yansong Tang, Jiwen Lu, Jie Zhou</li>
<li>for: 提出了一种基于减少行动空间学习的过程规划方法，以解决现有方法在高维状态监测和动作序列错误积累问题上遇到困难。</li>
<li>methods: 将过程规划问题抽象为数学链模型，通过跳过不确定节点和边，将长和复杂的序列函数转化为短而可靠的两种方式。</li>
<li>results: 对 CrossTask 和 COIN 测试集进行了广泛的实验，并达到了当前状态的最佳性能。<details>
<summary>Abstract</summary>
In this paper, we propose Skip-Plan, a condensed action space learning method for procedure planning in instructional videos. Current procedure planning methods all stick to the state-action pair prediction at every timestep and generate actions adjacently. Although it coincides with human intuition, such a methodology consistently struggles with high-dimensional state supervision and error accumulation on action sequences. In this work, we abstract the procedure planning problem as a mathematical chain model. By skipping uncertain nodes and edges in action chains, we transfer long and complex sequence functions into short but reliable ones in two ways. First, we skip all the intermediate state supervision and only focus on action predictions. Second, we decompose relatively long chains into multiple short sub-chains by skipping unreliable intermediate actions. By this means, our model explores all sorts of reliable sub-relations within an action sequence in the condensed action space. Extensive experiments show Skip-Plan achieves state-of-the-art performance on the CrossTask and COIN benchmarks for procedure planning.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了Skip-Plan方法，这是一种简化动作空间学习方法 для过程规划在教程视频中。现有的过程规划方法都是在每个时间步骤上预测状态-动作对，这与人类直觉相吻合，但这种方法ология在高维状态监督和动作序列错误积累方面一直遇到困难。在这种工作中，我们抽象了过程规划问题为数学链模型。通过在动作链中跳过不确定的节点和边，我们将长而复杂的序列函数转化为短而可靠的两种方式。第一种方法是跳过所有间接状态监督，只Focus on 动作预测。第二种方法是将相对较长的链分解成多个短的子链，通过跳过不可靠的间接动作来实现。通过这种方式，我们的模型可以在简化动作空间中探索所有可靠的子关系。我们的实验表明Skip-Plan在CrossTask和COIN测试准则上达到了过程规划领域的状态天空。
</details></li>
</ul>
<hr>
<h2 id="Quantum-image-edge-detection-based-on-eight-direction-Sobel-operator-for-NEQR"><a href="#Quantum-image-edge-detection-based-on-eight-direction-Sobel-operator-for-NEQR" class="headerlink" title="Quantum image edge detection based on eight-direction Sobel operator for NEQR"></a>Quantum image edge detection based on eight-direction Sobel operator for NEQR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03037">http://arxiv.org/abs/2310.03037</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjie Liu, Lu Wang</li>
<li>for: 这个论文是为了提出一种基于量子机制的图像边缘检测算法（QSED），以解决经典算法遇到的实时问题。</li>
<li>methods: 该算法基于八个方向的 Sobel 算子，不仅可以减少部分图像的边缘信息损失，还同时计算所有像素的八个方向的梯度值。</li>
<li>results: 对于 2^n x 2^n 图像，该算法的复杂度可以降至 O(n^2 + q^2)，比其他经典或量子算法低。实验表明，该算法可以更好地检测高清像中的对角边缘。<details>
<summary>Abstract</summary>
Quantum Sobel edge detection (QSED) is a kind of algorithm for image edge detection using quantum mechanism, which can solve the real-time problem encountered by classical algorithms. However, the existing QSED algorithms only consider two- or four-direction Sobel operator, which leads to a certain loss of edge detail information in some high-definition images. In this paper, a novel QSED algorithm based on eight-direction Sobel operator is proposed, which not only reduces the loss of edge information, but also simultaneously calculates eight directions' gradient values of all pixel in a quantum image. In addition, the concrete quantum circuits, which consist of gradient calculation, non-maximum suppression, double threshold detection and edge tracking units, are designed in details. For a 2^n x 2^n image with q gray scale, the complexity of our algorithm can be reduced to O(n^2 + q^2), which is lower than other existing classical or quantum algorithms. And the simulation experiment demonstrates that our algorithm can detect more edge information, especially diagonal edges, than the two- and four-direction QSED algorithms.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Image-Data-Hiding-in-Neural-Compressed-Latent-Representations"><a href="#Image-Data-Hiding-in-Neural-Compressed-Latent-Representations" class="headerlink" title="Image Data Hiding in Neural Compressed Latent Representations"></a>Image Data Hiding in Neural Compressed Latent Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00568">http://arxiv.org/abs/2310.00568</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen-Hsiu Huang, Ja-Ling Wu</li>
<li>for: 这个论文是为了开发一个朴素的图像数据隐藏框架，用于嵌入和提取秘密信息。</li>
<li>methods: 该方法使用了一种朴素的神经压缩器，并且使用了一种我们提出的消息编码器和解码器，同时使用了一种感知损失函数来实现高品质图像和高比特率。</li>
<li>results: 该方法可以在压缩领域中实现高水平的图像秘密性和竞争力强的水印鲁棒性，同时提高嵌入速度，比传统方法快上百倍。这些结果表明了将数据隐藏技术与神经压缩相结合的潜在优势和应用前景。<details>
<summary>Abstract</summary>
We propose an end-to-end learned image data hiding framework that embeds and extracts secrets in the latent representations of a generic neural compressor. By leveraging a perceptual loss function in conjunction with our proposed message encoder and decoder, our approach simultaneously achieves high image quality and high bit accuracy. Compared to existing techniques, our framework offers superior image secrecy and competitive watermarking robustness in the compressed domain while accelerating the embedding speed by over 50 times. These results demonstrate the potential of combining data hiding techniques and neural compression and offer new insights into developing neural compression techniques and their applications.
</details>
<details>
<summary>摘要</summary>
我们提出了一个末端学习的图像数据隐藏框架，该框架在一个通用的神经压缩器中嵌入和提取秘密。通过我们提出的消息编码器和解码器以及一种感知损失函数，我们的方法同时实现高质量图像和高比特率。与现有技术相比，我们的框架在压缩领域中提供了更高的图像机密性和竞争力强的水印鲁棒性，同时加速嵌入速度，提高了50倍以上。这些结果表明将数据隐藏技术与神经压缩结合可以实现新的应用和技术突破，并为神经压缩技术的发展提供新的视角。
</details></li>
</ul>
<hr>
<h2 id="CPIPS-Learning-to-Preserve-Perceptual-Distances-in-End-to-End-Image-Compression"><a href="#CPIPS-Learning-to-Preserve-Perceptual-Distances-in-End-to-End-Image-Compression" class="headerlink" title="CPIPS: Learning to Preserve Perceptual Distances in End-to-End Image Compression"></a>CPIPS: Learning to Preserve Perceptual Distances in End-to-End Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00559">http://arxiv.org/abs/2310.00559</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen-Hsiu Huang, Ja-Ling Wu</li>
<li>for: 这篇论文目的是提出一种基于神经科学和生物系统的压缩图像 Similarity Metric，以提高机器视觉任务中的图像压缩和比较效率。</li>
<li>methods: 该方法基于一种已经学习的神经网络编码器，通过修改压缩缓存来优先级化 semantics relevance，同时保持 perceived distance。</li>
<li>results: 对比 traditional DNN-based perceptual metrics，CPIPS 可以在计算速度和复杂度上具有明显的优势，而且可以在机器视觉任务中提高图像压缩和比较效率。<details>
<summary>Abstract</summary>
Lossy image coding standards such as JPEG and MPEG have successfully achieved high compression rates for human consumption of multimedia data. However, with the increasing prevalence of IoT devices, drones, and self-driving cars, machines rather than humans are processing a greater portion of captured visual content. Consequently, it is crucial to pursue an efficient compressed representation that caters not only to human vision but also to image processing and machine vision tasks. Drawing inspiration from the efficient coding hypothesis in biological systems and the modeling of the sensory cortex in neural science, we repurpose the compressed latent representation to prioritize semantic relevance while preserving perceptual distance. Our proposed method, Compressed Perceptual Image Patch Similarity (CPIPS), can be derived at a minimal cost from a learned neural codec and computed significantly faster than DNN-based perceptual metrics such as LPIPS and DISTS.
</details>
<details>
<summary>摘要</summary>
产生损失的图像编码标准如JPEG和MPEG已经成功实现了多媒体数据的高压缩率 для人类消耗。然而，随着互联网物联网设备、无人机和自动驾驶车的普及，机器正在处理更多的捕捉视觉内容。因此，我们需要追求一种高效的压缩表示，不仅适合人类视觉，还适合图像处理和机器视觉任务。 drawing inspiration from生物系统中的高效编码假设和神经科学中的感觉脑层模型，我们重新利用压缩潜在表示，优先级 semantic relevance 而保持perceptual distance。我们提议的方法，压缩感知图像patch similarity（CPIPS），可以在学习神经编码器的基础上得到，并且可以在DNN基于的感知度量方法，如LPIPS和DISTS，中计算得到更快。
</details></li>
</ul>
<hr>
<h2 id="Diving-into-the-Depths-of-Spotting-Text-in-Multi-Domain-Noisy-Scenes"><a href="#Diving-into-the-Depths-of-Spotting-Text-in-Multi-Domain-Noisy-Scenes" class="headerlink" title="Diving into the Depths of Spotting Text in Multi-Domain Noisy Scenes"></a>Diving into the Depths of Spotting Text in Multi-Domain Noisy Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00558">http://arxiv.org/abs/2310.00558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alloy Das, Sanket Biswas, Umapada Pal, Josep Lladós</li>
<li>for: 本研究旨在开发一种能够通用多个频道的自动Scene文本检测系统，以便在实际世界中针对不同频道进行文本检测。</li>
<li>methods: 我们采用了一种培训模型使用多个频道源数据，以便将其直接应用于目标频道中进行文本检测，而不是特定频道或enario中的精化。</li>
<li>results: 我们提出了一种基于超解析的终端转换器基线模型，称为DA-TextSpotter，可以在常见和arbitrary-shapedScene文本检测benchmark上达到或超越现有的文本检测建筑，同时具有较高的模型效率。<details>
<summary>Abstract</summary>
When used in a real-world noisy environment, the capacity to generalize to multiple domains is essential for any autonomous scene text spotting system. However, existing state-of-the-art methods employ pretraining and fine-tuning strategies on natural scene datasets, which do not exploit the feature interaction across other complex domains. In this work, we explore and investigate the problem of domain-agnostic scene text spotting, i.e., training a model on multi-domain source data such that it can directly generalize to target domains rather than being specialized for a specific domain or scenario. In this regard, we present the community a text spotting validation benchmark called Under-Water Text (UWT) for noisy underwater scenes to establish an important case study. Moreover, we also design an efficient super-resolution based end-to-end transformer baseline called DA-TextSpotter which achieves comparable or superior performance over existing text spotting architectures for both regular and arbitrary-shaped scene text spotting benchmarks in terms of both accuracy and model efficiency. The dataset, code and pre-trained models will be released upon acceptance.
</details>
<details>
<summary>摘要</summary>
当用于实际噪声环境中的自动Scene文本检测系统时，能够泛化到多个领域是非常重要的。然而，现有的状态艺术方法通常采用预训练和细化策略在自然场景数据集上，这并不利用场景文本之间的特征互动。在这种情况下，我们探索和探讨域性文本检测问题，即在多个领域源数据上训练一个模型，使其直接泛化到目标领域而不是特定领域或enario。为此，我们向社区提供了文本检测验证 benchmark called Under-Water Text (UWT)，以便在水下场景中进行噪声检测。此外，我们还设计了一种高效的超解像基于 transformer 结构的终端模型called DA-TextSpotter，它在常见和任意形状场景文本检测benchmark上实现了和现有文本检测建筑物之间的比较或更好的性能，同时具有更高的模型效率。数据集、代码和预训练模型将在接受后发布。
</details></li>
</ul>
<hr>
<h2 id="Seal2Real-Prompt-Prior-Learning-on-Diffusion-Model-for-Unsupervised-Document-Seal-Data-Generation-and-Realisation"><a href="#Seal2Real-Prompt-Prior-Learning-on-Diffusion-Model-for-Unsupervised-Document-Seal-Data-Generation-and-Realisation" class="headerlink" title="Seal2Real: Prompt Prior Learning on Diffusion Model for Unsupervised Document Seal Data Generation and Realisation"></a>Seal2Real: Prompt Prior Learning on Diffusion Model for Unsupervised Document Seal Data Generation and Realisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00546">http://arxiv.org/abs/2310.00546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiancheng Huang, Yifan Liu, Yi Huang, Shifeng Chen</li>
<li>For: 提供了一种生成大量标注文档印章数据的方法，以便提高Docuement Processing中的印章相关任务的性能。* Methods: 使用了一种基于静止扩散模型的提问先学架构，通过无监督训练将生成器的优先生成能力迁移到印章生成任务中。* Results: 在Seal-DB dataset上进行实验，表明Seal2Real方法可以生成高度真实的印章图像，对后续的印章相关任务进行实际数据上的提升。<details>
<summary>Abstract</summary>
In document processing, seal-related tasks have very large commercial applications, such as seal segmentation, seal authenticity discrimination, seal removal, and text recognition under seals. However, these seal-related tasks are highly dependent on labelled document seal datasets, resulting in very little work on these tasks. To address the lack of labelled datasets for these seal-related tasks, we propose Seal2Real, a generative method that generates a large amount of labelled document seal data, and construct a Seal-DB dataset containing 20K images with labels. In Seal2Real, we propose a prompt prior learning architecture based on a pre-trained Stable Diffusion Model that migrates the prior generative power of to our seal generation task with unsupervised training. The realistic seal generation capability greatly facilitates the performance of downstream seal-related tasks on real data. Experimental results on the Seal-DB dataset demonstrate the effectiveness of Seal2Real.
</details>
<details>
<summary>摘要</summary>
在文档处理领域中，有很多商业应用，如印章分割、印章真实性识别、印章去除和文本识别下印章。然而，这些印章相关任务都受到了标注文档印章数据的限制，导致了这些任务的研究得到了非常少的积极性。为了解决标注文档印章数据的缺乏，我们提出了Seal2Real方法，该方法可以生成大量标注文档印章数据，并构建了一个名为Seal-DB的数据集，包含20K个图像和标签。在Seal2Real中，我们提出了一种提前学习的推荐模型，基于静止扩散模型，将先前的生成能力传递到我们的印章生成任务中，并在无监督下训练。这种印章生成能力可以帮助下游印章相关任务在真实数据上表现出色。实验结果表明，Seal2Real是有效的。
</details></li>
</ul>
<hr>
<h2 id="Implicit-Neural-Representations-and-the-Algebra-of-Complex-Wavelets"><a href="#Implicit-Neural-Representations-and-the-Algebra-of-Complex-Wavelets" class="headerlink" title="Implicit Neural Representations and the Algebra of Complex Wavelets"></a>Implicit Neural Representations and the Algebra of Complex Wavelets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00545">http://arxiv.org/abs/2310.00545</a></li>
<li>repo_url: None</li>
<li>paper_authors: T. Mitchell Roddenberry, Vishwanath Saragadam, Maarten V. de Hoop, Richard G. Baraniuk</li>
<li>for: 这个论文旨在探讨隐形神经表示（INR）如何用于欧几丁素空间上的信号处理和机器学习。</li>
<li>methods: 该论文使用多层感知器（MLP）来Parameterize图像，并使用浮动函数或浮动滤波器来实现INR。</li>
<li>results: 研究发现，使用浮动滤波器作为激活函数可以同时具有频率和空间特征的地方化特征，从而提高信号处理和机器学习的性能。此外，该论文还提出了多种INR架构设计方法，包括复杂滤波器、分离低频和高频拟合、以及基于所求信号的初始化方案。<details>
<summary>Abstract</summary>
Implicit neural representations (INRs) have arisen as useful methods for representing signals on Euclidean domains. By parameterizing an image as a multilayer perceptron (MLP) on Euclidean space, INRs effectively represent signals in a way that couples spatial and spectral features of the signal that is not obvious in the usual discrete representation, paving the way for continuous signal processing and machine learning approaches that were not previously possible. Although INRs using sinusoidal activation functions have been studied in terms of Fourier theory, recent works have shown the advantage of using wavelets instead of sinusoids as activation functions, due to their ability to simultaneously localize in both frequency and space. In this work, we approach such INRs and demonstrate how they resolve high-frequency features of signals from coarse approximations done in the first layer of the MLP. This leads to multiple prescriptions for the design of INR architectures, including the use of complex wavelets, decoupling of low and band-pass approximations, and initialization schemes based on the singularities of the desired signal.
</details>
<details>
<summary>摘要</summary>
启发神经表示（INR）在欧几何空间上表示信号已成为有用的方法。通过将图像 Parametric 为多层感知器（MLP）在欧几何空间中，INR 可以将信号表示为不可分离的空间和频谱特征，使得不可分离的信号处理和机器学习方法变得可能。虽然使用惯性函数的 INR 已经被研究，但是最近的工作表明使用浪谱函数作为激活函数的优势，因为它可以同时在频谱和空间中进行本地化。在这个工作中，我们研究了这些 INR 和它们如何在 MLP 的第一层中解决高频特征。这导致了多种 INR 架构的设计方法，包括复杂浪谱、分离低频和高频拟合、以及基于感知信号的初始化方案。
</details></li>
</ul>
<hr>
<h2 id="Enabling-Neural-Radiance-Fields-NeRF-for-Large-scale-Aerial-Images-–-A-Multi-tiling-Approach-and-the-Geometry-Assessment-of-NeRF"><a href="#Enabling-Neural-Radiance-Fields-NeRF-for-Large-scale-Aerial-Images-–-A-Multi-tiling-Approach-and-the-Geometry-Assessment-of-NeRF" class="headerlink" title="Enabling Neural Radiance Fields (NeRF) for Large-scale Aerial Images – A Multi-tiling Approach and the Geometry Assessment of NeRF"></a>Enabling Neural Radiance Fields (NeRF) for Large-scale Aerial Images – A Multi-tiling Approach and the Geometry Assessment of NeRF</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00530">http://arxiv.org/abs/2310.00530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ningli Xu, Rongjun Qin, Debao Huang, Fabio Remondino</li>
<li>for: 这个论文旨在提高大规模飞行图像数据上的NeRF纹理场的渐进性和准确性。</li>
<li>methods: 作者提出了一种位置特定采样技术和多摄像头分割策略来降低图像加载、表示训练和缓存内存占用，并提高内部缓存的速度。</li>
<li>results: 作者对两个典型的飞行图像数据集进行了比较，结果表明提出的NeRF方法在完整性和物体细节方面表现更好，但还有一定的准确性不足。<details>
<summary>Abstract</summary>
Neural Radiance Fields (NeRF) offer the potential to benefit 3D reconstruction tasks, including aerial photogrammetry. However, the scalability and accuracy of the inferred geometry are not well-documented for large-scale aerial assets,since such datasets usually result in very high memory consumption and slow convergence.. In this paper, we aim to scale the NeRF on large-scael aerial datasets and provide a thorough geometry assessment of NeRF. Specifically, we introduce a location-specific sampling technique as well as a multi-camera tiling (MCT) strategy to reduce memory consumption during image loading for RAM, representation training for GPU memory, and increase the convergence rate within tiles. MCT decomposes a large-frame image into multiple tiled images with different camera models, allowing these small-frame images to be fed into the training process as needed for specific locations without a loss of accuracy. We implement our method on a representative approach, Mip-NeRF, and compare its geometry performance with threephotgrammetric MVS pipelines on two typical aerial datasets against LiDAR reference data. Both qualitative and quantitative results suggest that the proposed NeRF approach produces better completeness and object details than traditional approaches, although as of now, it still falls short in terms of accuracy.
</details>
<details>
<summary>摘要</summary>
neural radiance fields (NeRF) 可能帮助3D重建任务，包括航空相机摄影。然而，对大规模航空资产的推广和准确性不够 document。在这篇论文中，我们希望通过缩放NeRF来适应大规模航空资产，并对NeRF的geometry进行全面评估。我们引入了位置特定的采样技术以及多camera tilting（MCT）策略，以降低内存占用量，提高内存中的表示训练，并提高分割区域内的快速转换。MCT将大幅度图像分解成多个不同摄像机模型的小幅度图像，以便在特定位置上无损loss的方式进行训练。我们实现了这种方法，并与三种光学多视角摄影管道进行比较，以评估NeRF的geometry性能。结果表明，我们的方法可以在两个典型的航空数据集上提供更好的完整性和物体细节，although it still lags behind in terms of accuracy。
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-Learning-of-Contextualized-Local-Visual-Embeddings"><a href="#Self-supervised-Learning-of-Contextualized-Local-Visual-Embeddings" class="headerlink" title="Self-supervised Learning of Contextualized Local Visual Embeddings"></a>Self-supervised Learning of Contextualized Local Visual Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00527">http://arxiv.org/abs/2310.00527</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sthalles/clove">https://github.com/sthalles/clove</a></li>
<li>paper_authors: Thalles Santos Silva, Helio Pedrini, Adín Ramírez Rivera</li>
<li>for: 这篇论文是为了提出一种基于自我supervised convolutional neural network（CNN）的方法，以学习适合紧密预测任务的表示。</li>
<li>methods: 这篇论文使用了一种新的多头自我注意层，通过对不同部分的图像特征进行相似性combine来学习 contextualized embedding。</li>
<li>results: 该论文在多个数据集上进行了广泛的 benchmarking，并达到了基于CNN架构的 dense prediction downstream tasks中的国际级表现，包括物体检测、实例分割、关键点检测和紧密pose estimation。<details>
<summary>Abstract</summary>
We present Contextualized Local Visual Embeddings (CLoVE), a self-supervised convolutional-based method that learns representations suited for dense prediction tasks. CLoVE deviates from current methods and optimizes a single loss function that operates at the level of contextualized local embeddings learned from output feature maps of convolution neural network (CNN) encoders. To learn contextualized embeddings, CLoVE proposes a normalized mult-head self-attention layer that combines local features from different parts of an image based on similarity. We extensively benchmark CLoVE's pre-trained representations on multiple datasets. CLoVE reaches state-of-the-art performance for CNN-based architectures in 4 dense prediction downstream tasks, including object detection, instance segmentation, keypoint detection, and dense pose estimation.
</details>
<details>
<summary>摘要</summary>
我团队现在发布 Contextualized Local Visual Embeddings (CLoVE)，这是一种自动学习的卷积神经网络方法，用于学习适用于紧凑预测任务的表示。CLoVE与现有方法不同，它优化了基于输出特征图卷积神经网络Encoder学习的上下文化 embedding 的单个损失函数。为了学习上下文化 embedding，CLoVE提议了一种归一化多头自注意层，通过相似性将不同部分的图像特征相结合。我们对 CLoVE 的预训练表示进行了广泛的比较，并达到了基于 CNN 架构的 dense prediction 下游任务中的国际级表现。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/01/cs.CV_2023_10_01/" data-id="clpxp6c1s00kmee881h3h2y2n" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_10_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/01/cs.AI_2023_10_01/" class="article-date">
  <time datetime="2023-10-01T12:00:00.000Z" itemprop="datePublished">2023-10-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/01/cs.AI_2023_10_01/">cs.AI - 2023-10-01</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="OceanNet-A-principled-neural-operator-based-digital-twin-for-regional-oceans"><a href="#OceanNet-A-principled-neural-operator-based-digital-twin-for-regional-oceans" class="headerlink" title="OceanNet: A principled neural operator-based digital twin for regional oceans"></a>OceanNet: A principled neural operator-based digital twin for regional oceans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00813">http://arxiv.org/abs/2310.00813</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashesh Chattopadhyay, Michael Gray, Tianning Wu, Anna B. Lowe, Ruoying He</li>
<li>for: 这个研究旨在开发一种基于神经网络的数字孪生模型，用于海洋径流预测。</li>
<li>methods: 该模型使用FOURNIER神经网络算法和评估误差修正方法，以提高预测稳定性和抑制自回卷积误差增长。此外，使用 спектраль regularizer 减少小规模谱偏误。</li>
<li>results: 在北大西洋西部边域流（卡里布湾流）中测试了这种模型，并成功地预测了径流聚合体和弯曲流的季节预报。与传统的不连接、状态环境模型预测相比，这种模型显示出竞争的预测能力，同时减少了计算量500,000倍。这些成果表明物理启发的深度神经算法可能成为高分辨率数字海洋模型的成本效果的替代方案。<details>
<summary>Abstract</summary>
While data-driven approaches demonstrate great potential in atmospheric modeling and weather forecasting, ocean modeling poses distinct challenges due to complex bathymetry, land, vertical structure, and flow non-linearity. This study introduces OceanNet, a principled neural operator-based digital twin for ocean circulation. OceanNet uses a Fourier neural operator and predictor-evaluate-corrector integration scheme to mitigate autoregressive error growth and enhance stability over extended time scales. A spectral regularizer counteracts spectral bias at smaller scales. OceanNet is applied to the northwest Atlantic Ocean western boundary current (the Gulf Stream), focusing on the task of seasonal prediction for Loop Current eddies and the Gulf Stream meander. Trained using historical sea surface height (SSH) data, OceanNet demonstrates competitive forecast skill by outperforming SSH predictions by an uncoupled, state-of-the-art dynamical ocean model forecast, reducing computation by 500,000 times. These accomplishments demonstrate the potential of physics-inspired deep neural operators as cost-effective alternatives to high-resolution numerical ocean models.
</details>
<details>
<summary>摘要</summary>
While data-driven approaches have shown great potential in atmospheric modeling and weather forecasting, ocean modeling poses distinct challenges due to complex bathymetry, land, vertical structure, and flow non-linearity. This study introduces OceanNet, a principled neural operator-based digital twin for ocean circulation. OceanNet uses a Fourier neural operator and predictor-evaluate-corrector integration scheme to mitigate autoregressive error growth and enhance stability over extended time scales. A spectral regularizer counteracts spectral bias at smaller scales. OceanNet is applied to the northwest Atlantic Ocean western boundary current (the Gulf Stream), focusing on the task of seasonal prediction for Loop Current eddies and the Gulf Stream meander. Trained using historical sea surface height (SSH) data, OceanNet demonstrates competitive forecast skill by outperforming SSH predictions by an uncoupled, state-of-the-art dynamical ocean model forecast, reducing computation by 500,000 times. These accomplishments demonstrate the potential of physics-inspired deep neural operators as cost-effective alternatives to high-resolution numerical ocean models.Here's the translation in Traditional Chinese:而data-driven方法在大气模拟和天气预测中表现出了很大的潜力，但是海洋模拟却存在复杂的海底地形、陆地、垂直结构和流体非线性等挑战。本研究提出了OceanNet，一种基于神经算子的数字双胞虫 для海洋流动。OceanNet使用了福洛神经算子和预测评估修正 integrate scheme来减少自回归错误增长和提高时间尺度上的稳定性。另外，一种 Spectral regularizer 来抵消小尺度的 spectral bias。OceanNet 应用于北大西洋西部边Current（ GolStream），专注于季节预测Loop Current eddies 和 GolStream meander。使用历史海面高度数据进行训练，OceanNet 表现出了与不可分离的、现有的动力海洋模型预测 SSH 数据的竞争力，并且减少了计算量500,000倍。这些成就表明 physics-inspired deep neural operators 可以成为高分解能数字海洋模型的成本效果的替代方案。
</details></li>
</ul>
<hr>
<h2 id="Sparse-Backpropagation-for-MoE-Training"><a href="#Sparse-Backpropagation-for-MoE-Training" class="headerlink" title="Sparse Backpropagation for MoE Training"></a>Sparse Backpropagation for MoE Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00811">http://arxiv.org/abs/2310.00811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liyuan Liu, Jianfeng Gao, Weizhu Chen</li>
<li>for: 这篇论文主要旨在解决深度学习中的权值计算问题，特别是在混合专家（Mixture-of-Expert，MoE）模型中，通过专家路由实现稀疏计算，从而实现很好的扩展性。</li>
<li>methods: 该论文提出了一种名为SparseMixer的扩展性 gradient estimator，它可以在混合专家模型中实现可靠的梯度估计，并且不需要忽略某些梯度项，从而实现更加准确的梯度估计。SparseMixer基于数字差分方法，利用中点法来提供精确的梯度估计，计算 overhead 很低。</li>
<li>results: 应用SparseMixer于 Switch Transformer 上，在预训练和机器翻译任务中，可以见到较大的性能提升，快速加速训练过程，最多提高训练速度2倍。<details>
<summary>Abstract</summary>
One defining characteristic of Mixture-of-Expert (MoE) models is their capacity for conducting sparse computation via expert routing, leading to remarkable scalability. However, backpropagation, the cornerstone of deep learning, requires dense computation, thereby posting challenges in MoE gradient computations. Here, we introduce SparseMixer, a scalable gradient estimator that bridges the gap between backpropagation and sparse expert routing. Unlike typical MoE training which strategically neglects certain gradient terms for the sake of sparse computation and scalability, SparseMixer provides scalable gradient approximations for these terms, enabling reliable gradient estimation in MoE training. Grounded in a numerical ODE framework, SparseMixer harnesses the mid-point method, a second-order ODE solver, to deliver precise gradient approximations with negligible computational overhead. Applying SparseMixer to Switch Transformer on both pre-training and machine translation tasks, SparseMixer showcases considerable performance gain, accelerating training convergence up to 2 times.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. The translation may not be perfect, and some nuances or idiomatic expressions may be lost in translation.
</details></li>
</ul>
<hr>
<h2 id="Towards-Causal-Foundation-Model-on-Duality-between-Causal-Inference-and-Attention"><a href="#Towards-Causal-Foundation-Model-on-Duality-between-Causal-Inference-and-Attention" class="headerlink" title="Towards Causal Foundation Model: on Duality between Causal Inference and Attention"></a>Towards Causal Foundation Model: on Duality between Causal Inference and Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00809">http://arxiv.org/abs/2310.00809</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaqi Zhang, Joel Jennings, Cheng Zhang, Chao Ma</li>
<li>for: 这篇论文旨在建立复杂任务中的 causal inference 模型，以提高机器学习的效果。</li>
<li>methods: 该论文提出了一种新的、理论上正确的方法 called Causal Inference with Attention (CInA)，该方法通过多个无标注数据进行自主学习 causal learning，并在新数据上进行零shot causal inference。</li>
<li>results: 实验结果表明，CInA方法能够通过最终层的 transformer-type 架构实现零shot causal inference，并能够在不同的数据集上进行效果的泛化。<details>
<summary>Abstract</summary>
Foundation models have brought changes to the landscape of machine learning, demonstrating sparks of human-level intelligence across a diverse array of tasks. However, a gap persists in complex tasks such as causal inference, primarily due to challenges associated with intricate reasoning steps and high numerical precision requirements. In this work, we take a first step towards building causally-aware foundation models for complex tasks. We propose a novel, theoretically sound method called Causal Inference with Attention (CInA), which utilizes multiple unlabeled datasets to perform self-supervised causal learning, and subsequently enables zero-shot causal inference on unseen tasks with new data. This is based on our theoretical results that demonstrate the primal-dual connection between optimal covariate balancing and self-attention, facilitating zero-shot causal inference through the final layer of a trained transformer-type architecture. We demonstrate empirically that our approach CInA effectively generalizes to out-of-distribution datasets and various real-world datasets, matching or even surpassing traditional per-dataset causal inference methodologies.
</details>
<details>
<summary>摘要</summary>
基础模型已经带来了机器学习领域的变革，展示出人类水平的智能特性在多种任务上。然而，在复杂任务中，如 causal inference，仍存在一个差距，主要归结于复杂的逻辑步骤和高精度数字需求。在这项工作中，我们首次实现了基于自我超vised causal learning的可 causal-aware基础模型。我们提出了一种新的、理论上正确的方法called Causal Inference with Attention（CInA），通过多个无标签数据集进行自我超vised causal learning，并在新数据上进行零实际参数的 causal inference。这基于我们的理论结果，证明了优化 covariate balancing 和 self-attention 的 primal-dual 连接，从而实现零实际参数的 causal inference through 训练过的 transformer-type 架构的最后一层。我们通过实验证明，我们的方法 CInA 可以对不同的数据集和实际世界任务进行有效的泛化，与传统每个数据集的 causal inference 方法相当或者even surpass。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Engineering-for-Wind-Energy"><a href="#Knowledge-Engineering-for-Wind-Energy" class="headerlink" title="Knowledge Engineering for Wind Energy"></a>Knowledge Engineering for Wind Energy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00804">http://arxiv.org/abs/2310.00804</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Planet21century/TECHALDO">https://github.com/Planet21century/TECHALDO</a>.</li>
<li>paper_authors: Yuriy Marykovskiy, Thomas Clark, Justin Day, Marcus Wiens, Charles Henderson, Julian Quick, Imad Abdallah, Anna Maria Sempreviva, Jean-Paul Calbimonte, Eleni Chatzi, Sarah Barber</li>
<li>for: 本研究旨在帮助风能领域专家将数据转化为域知识，与其他知识源集成，并为下一代人工智能系统提供可用的数据。</li>
<li>methods: 本文使用知识工程来支持风能领域的数字变革，并提出了域知识表示的主要概念。 previous work 在风能领域知识工程和知识表示方面进行了系统性的分析，并提供了适用于域专家的指南。</li>
<li>results: 本文通过系统分析当前风能领域知识工程的状况，并将主要域算法和工具置于风能领域专家需求和问题点上下文中，以帮助读者更好地理解和应用知识工程技术。<details>
<summary>Abstract</summary>
With the rapid evolution of the wind energy sector, there is an ever-increasing need to create value from the vast amounts of data made available both from within the domain, as well as from other sectors. This article addresses the challenges faced by wind energy domain experts in converting data into domain knowledge, connecting and integrating it with other sources of knowledge, and making it available for use in next generation artificially intelligent systems. To this end, this article highlights the role that knowledge engineering can play in the process of digital transformation of the wind energy sector. It presents the main concepts underpinning Knowledge-Based Systems and summarises previous work in the areas of knowledge engineering and knowledge representation in a manner that is relevant and accessible to domain experts. A systematic analysis of the current state-of-the-art on knowledge engineering in the wind energy domain is performed, with available tools put into perspective by establishing the main domain actors and their needs and identifying key problematic areas. Finally, guidelines for further development and improvement are provided.
</details>
<details>
<summary>摘要</summary>
随着风能行业的快速发展，需要从各个领域中提取丰富的数据，并将其与其他领域的知识相连接和融合。这篇文章挑战风能领域专家将数据转化为域知识，并将其与其他知识源融合，以便在下一代人工智能系统中使用。为此，本文强调了知识工程在风能领域的数字转型过程中的重要作用。文章介绍了知识工程的主要概念，并总结了过去在风能领域的知识工程和知识表示方面的工作，以便对风能领域专家有所帮助。本文进行了风能领域知识工程的系统性分析，并将可用工具放在风能领域主要拥有者和他们的需求之前提下进行了比较。文章还标识了主要问题点，以便进一步的发展和改进。最后，文章提供了进一步发展和改进的指南。
</details></li>
</ul>
<hr>
<h2 id="GraphPatcher-Mitigating-Degree-Bias-for-Graph-Neural-Networks-via-Test-time-Augmentation"><a href="#GraphPatcher-Mitigating-Degree-Bias-for-Graph-Neural-Networks-via-Test-time-Augmentation" class="headerlink" title="GraphPatcher: Mitigating Degree Bias for Graph Neural Networks via Test-time Augmentation"></a>GraphPatcher: Mitigating Degree Bias for Graph Neural Networks via Test-time Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00800">http://arxiv.org/abs/2310.00800</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jumxglhf/graphpatcher">https://github.com/jumxglhf/graphpatcher</a></li>
<li>paper_authors: Mingxuan Ju, Tong Zhao, Wenhao Yu, Neil Shah, Yanfang Ye</li>
<li>for: 提高 graph neural network (GNN) 的测试时通用性和低度节点表现。</li>
<li>methods: 提出了一种名为 GraphPatcher 的测试时扩充框架，通过在训练时生成虚拟节点来强化 GNN 的测试时性能。</li>
<li>results: 对七个基准数据集进行了广泛的实验，并 consistently 提高了常见 GNN 的总性能和低度节点表现，相比之前的状态态标准基eline。<details>
<summary>Abstract</summary>
Recent studies have shown that graph neural networks (GNNs) exhibit strong biases towards the node degree: they usually perform satisfactorily on high-degree nodes with rich neighbor information but struggle with low-degree nodes. Existing works tackle this problem by deriving either designated GNN architectures or training strategies specifically for low-degree nodes. Though effective, these approaches unintentionally create an artificial out-of-distribution scenario, where models mainly or even only observe low-degree nodes during the training, leading to a downgraded performance for high-degree nodes that GNNs originally perform well at. In light of this, we propose a test-time augmentation framework, namely GraphPatcher, to enhance test-time generalization of any GNNs on low-degree nodes. Specifically, GraphPatcher iteratively generates virtual nodes to patch artificially created low-degree nodes via corruptions, aiming at progressively reconstructing target GNN's predictions over a sequence of increasingly corrupted nodes. Through this scheme, GraphPatcher not only learns how to enhance low-degree nodes (when the neighborhoods are heavily corrupted) but also preserves the original superior performance of GNNs on high-degree nodes (when lightly corrupted). Additionally, GraphPatcher is model-agnostic and can also mitigate the degree bias for either self-supervised or supervised GNNs. Comprehensive experiments are conducted over seven benchmark datasets and GraphPatcher consistently enhances common GNNs' overall performance by up to 3.6% and low-degree performance by up to 6.5%, significantly outperforming state-of-the-art baselines. The source code is publicly available at https://github.com/jumxglhf/GraphPatcher.
</details>
<details>
<summary>摘要</summary>
近期研究发现，图 neural network (GNN) 具有节点度偏好：它们通常在高度节点上表现良好，但是在低度节点上遇到困难。现有的方法包括设计专门的 GNN 架构或训练策略，以解决这个问题。虽然有效，这些方法会意外创造一种人工的异常情况，导致模型在训练中主要或仅仅观察低度节点，从而导致高度节点的性能下降。为了解决这个问题，我们提出了一个测试时扩展框架，即 GraphPatcher，以提高任何 GNN 的测试时通用性。Specifically, GraphPatcher iteratively generates virtual nodes to patch artificially created low-degree nodes via corruptions, aiming at progressively reconstructing target GNN's predictions over a sequence of increasingly corrupted nodes. Through this scheme, GraphPatcher not only learns how to enhance low-degree nodes (when the neighborhoods are heavily corrupted) but also preserves the original superior performance of GNNs on high-degree nodes (when lightly corrupted). Additionally, GraphPatcher is model-agnostic and can also mitigate the degree bias for either self-supervised or supervised GNNs.我们在七个 benchmark 数据集上进行了广泛的实验，并证明 GraphPatcher 可以一直提高常见 GNN 的总性能和低度节点性能，最高提高3.6%和6.5%。与现有的基elines相比，GraphPatcher 显示出了显著的优势。源代码可以在 GitHub 上下载，请参阅 <https://github.com/jumxglhf/GraphPatcher>。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Review-of-Generative-AI-in-Healthcare"><a href="#A-Comprehensive-Review-of-Generative-AI-in-Healthcare" class="headerlink" title="A Comprehensive Review of Generative AI in Healthcare"></a>A Comprehensive Review of Generative AI in Healthcare</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00795">http://arxiv.org/abs/2310.00795</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yasin Shokrollahi, Sahar Yarmohammadtoosky, Matthew M. Nikahd, Pengfei Dong, Xianqi Li, Linxia Gu</li>
<li>for: 本文主要探讨了生成式人工智能（AI）在医疗领域的应用，尤其是转换器和扩散模型。</li>
<li>methods: 本文使用的方法包括医疗影像分析、预测蛋白结构、临床文档、诊断协助、放射学解读、临床决策支持、医疗代码和财务处理等。</li>
<li>results: 本文总结了各种生成式AI应用在医疗领域的进展，包括医疗影像重建、图像至图像翻译、图像生成和分类、蛋白结构预测、临床诊断和决策支持等，并提出了未来研究的可能性以满足医疗领域的发展需求。<details>
<summary>Abstract</summary>
The advancement of Artificial Intelligence (AI) has catalyzed revolutionary changes across various sectors, notably in healthcare. Among the significant developments in this field are the applications of generative AI models, specifically transformers and diffusion models. These models have played a crucial role in analyzing diverse forms of data, including medical imaging (encompassing image reconstruction, image-to-image translation, image generation, and image classification), protein structure prediction, clinical documentation, diagnostic assistance, radiology interpretation, clinical decision support, medical coding, and billing, as well as drug design and molecular representation. Such applications have enhanced clinical diagnosis, data reconstruction, and drug synthesis. This review paper aims to offer a thorough overview of the generative AI applications in healthcare, focusing on transformers and diffusion models. Additionally, we propose potential directions for future research to tackle the existing limitations and meet the evolving demands of the healthcare sector. Intended to serve as a comprehensive guide for researchers and practitioners interested in the healthcare applications of generative AI, this review provides valuable insights into the current state of the art, challenges faced, and prospective future directions.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）的发展对各个领域产生了革命性的变革，医疗领域是其中之一。在这个领域中，生成式AI模型，特别是转换器和扩散模型，对医疗数据进行分析发挥了重要作用。这些模型可以处理各种不同的数据类型，包括医疗影像重建、图像到图像翻译、图像生成和图像分类、蛋白质结构预测、临床记录、诊断助手、医学影像理解、诊断支持、医疗代码和财务处理等。这些应用程序提高了临床诊断、数据重建和药物合成。本文旨在为医疗领域的研究人员和实践者提供一份全面的综述，探讨生成式AI在医疗领域的应用，特别是转换器和扩散模型。此外，我们还提出了未来研究的可能性，以满足医疗领域的发展需求。这篇文章旨在为医疗领域的研究人员和实践者提供一份价值的指南，帮助他们更好地理解现有技术的状况、挑战和未来发展趋势。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Link-Prediction-A-Data-Perspective"><a href="#Revisiting-Link-Prediction-A-Data-Perspective" class="headerlink" title="Revisiting Link Prediction: A Data Perspective"></a>Revisiting Link Prediction: A Data Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00793">http://arxiv.org/abs/2310.00793</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uisim2020/uisim2020">https://github.com/uisim2020/uisim2020</a></li>
<li>paper_authors: Haitao Mao, Juanhui Li, Harry Shomer, Bingheng Li, Wenqi Fan, Yao Ma, Tong Zhao, Neil Shah, Jiliang Tang</li>
<li>for: 本研究旨在探讨链接预测task在不同领域 dataset 之间的共通原理，以提高链接预测模型的普适性。</li>
<li>methods: 本研究使用了三种关键因素：本地结构靠近性、全局结构靠近性和特征靠近性，以探索链接预测task 的数据中心视角。</li>
<li>results: 研究发现，全局结构靠近性只有在本地结构靠近性不足时才有效。此外，特征靠近性和结构靠近性之间存在冲突，导致 GNN4LP 模型在一些链接上表现不佳。<details>
<summary>Abstract</summary>
Link prediction, a fundamental task on graphs, has proven indispensable in various applications, e.g., friend recommendation, protein analysis, and drug interaction prediction. However, since datasets span a multitude of domains, they could have distinct underlying mechanisms of link formation. Evidence in existing literature underscores the absence of a universally best algorithm suitable for all datasets. In this paper, we endeavor to explore principles of link prediction across diverse datasets from a data-centric perspective. We recognize three fundamental factors critical to link prediction: local structural proximity, global structural proximity, and feature proximity. We then unearth relationships among those factors where (i) global structural proximity only shows effectiveness when local structural proximity is deficient. (ii) The incompatibility can be found between feature and structural proximity. Such incompatibility leads to GNNs for Link Prediction (GNN4LP) consistently underperforming on edges where the feature proximity factor dominates. Inspired by these new insights from a data perspective, we offer practical instruction for GNN4LP model design and guidelines for selecting appropriate benchmark datasets for more comprehensive evaluations.
</details>
<details>
<summary>摘要</summary>
链接预测，一项基本任务在图上，已经在各种应用中证明无可或，例如朋友推荐、蛋白分析和药物交互预测。然而， datasets  span 多个领域，它们可能具有不同的下面机制。文献证明了无一个通用的算法适用于所有 datasets。在这篇文章中，我们尝试通过数据中心的视角来探索链接预测的原则。我们认为链接预测中有三个基本因素是关键的：本地结构靠近性、全局结构靠近性和特征靠近性。然后，我们发现这些因素之间存在关系，包括（i）全局结构靠近性只有当本地结构靠近性不足时才能够有效。（ii）特征和结构靠近性之间存在不兼容性，这导致 GNNs for Link Prediction (GNN4LP) 在特征靠近性因素占主导地位的边上表现不佳。被这些新的数据视角所 inspirited，我们提供了实用的 GNN4LP 模型设计指南和选择合适的 benchmark 数据集的指南，以便更全面的评估。
</details></li>
</ul>
<hr>
<h2 id="Towards-a-Universal-Understanding-of-Color-Harmony-Fuzzy-Approach"><a href="#Towards-a-Universal-Understanding-of-Color-Harmony-Fuzzy-Approach" class="headerlink" title="Towards a Universal Understanding of Color Harmony: Fuzzy Approach"></a>Towards a Universal Understanding of Color Harmony: Fuzzy Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00791">http://arxiv.org/abs/2310.00791</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pakizar Shamoi, Muragul Muratbekova, Assylzhan Izbassar, Atsushi Inoue, Hiroharu Kawanaka</li>
<li>for:  explore color harmony using a fuzzy-based color model and evaluate its universality</li>
<li>methods:  use a dataset of attractive images from five different domains, apply a fuzzy approach to identify harmony patterns and dominant color palettes</li>
<li>results:  color harmony is largely universal, influenced by hue relationships, saturation, and intensity of colors, with prevalent adherence to color wheel principles in palettes with high harmony levels.<details>
<summary>Abstract</summary>
Harmony level prediction is receiving increasing attention nowadays. Color plays a crucial role in affecting human aesthetic responses. In this paper, we explore color harmony using a fuzzy-based color model and address the question of its universality. For our experiments, we utilize a dataset containing attractive images from five different domains: fashion, art, nature, interior design, and brand logos. We aim to identify harmony patterns and dominant color palettes within these images using a fuzzy approach. It is well-suited for this task because it can handle the inherent subjectivity and contextual variability associated with aesthetics and color harmony evaluation. Our experimental results suggest that color harmony is largely universal. Additionally, our findings reveal that color harmony is not solely influenced by hue relationships on the color wheel but also by the saturation and intensity of colors. In palettes with high harmony levels, we observed a prevalent adherence to color wheel principles while maintaining moderate levels of saturation and intensity. These findings contribute to ongoing research on color harmony and its underlying principles, offering valuable insights for designers, artists, and researchers in the field of aesthetics.
</details>
<details>
<summary>摘要</summary>
现在，谐契度预测已经得到了越来越多的关注。颜色在人类美学反应中发挥了关键性的作用。在这篇论文中，我们使用基于朴素集的颜色模型来探讨颜色谐契，并评估其universality。我们使用包含有吸引人的图像的五个领域：时尚、艺术、自然、家居设计和品牌LOGO的 dataset进行实验。我们希望通过朴素方法来确定图像中的谐契模式和主导的颜色alette。这种方法适合这种任务，因为它可以处理美学和颜色谐契评估中的内在主观性和上下文变化。我们的实验结果表明，颜色谐契是大体上的universal。此外，我们发现颜色谐契不仅受到颜色轮的颜色关系的影响，还受到颜色的浓淡和强度的影响。在高谐契水平的颜色alette中，我们发现了较高的颜色轮原则遵循性，同时保持了中等的浓淡和强度。这些发现对美学颜色谐契的研究提供了有价值的见解，对设计师、艺术家和研究人员在美学颜色谐契方面的工作都是有益的。
</details></li>
</ul>
<hr>
<h2 id="BooookScore-A-systematic-exploration-of-book-length-summarization-in-the-era-of-LLMs"><a href="#BooookScore-A-systematic-exploration-of-book-length-summarization-in-the-era-of-LLMs" class="headerlink" title="BooookScore: A systematic exploration of book-length summarization in the era of LLMs"></a>BooookScore: A systematic exploration of book-length summarization in the era of LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00785">http://arxiv.org/abs/2310.00785</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lilakk/booookscore">https://github.com/lilakk/booookscore</a></li>
<li>paper_authors: Yapei Chang, Kyle Lo, Tanya Goyal, Mohit Iyyer</li>
<li>For: This paper focuses on developing a method to evaluate the coherence of book-length summaries generated by large language models (LLMs). The authors aim to address the challenges of evaluating summarization of long documents, which are not well-studied due to the lack of datasets and evaluation methods.* Methods: The authors use two prompting workflows to generate book-length summaries: (1) hierarchically merging chunk-level summaries, and (2) incrementally updating a running summary. They also develop an automatic metric, BooookScore, to measure the coherence of the summaries.* Results: The authors obtain human annotations on GPT-4 generated summaries of 100 recently-published books and identify eight common types of coherence errors made by LLMs. They find that closed-source LLMs such as GPT-4 and Claude 2 produce summaries with higher BooookScore than the oft-repetitive ones generated by LLaMA 2. Incremental updating yields lower BooookScore but higher level of detail than hierarchical merging, a trade-off sometimes preferred by human annotators.Here is the Chinese translation of the three points:* For: 本文旨在开发一种方法来评估大语言模型（LLM）生成的长文摘要的准确性。作者们面临长文摘要评估的挑战，因为现有的数据集和评估方法尚未得到了深入研究。* Methods: 作者们使用两种提示工作流程来生成长文摘要：（1）层次合并 chunk-level 摘要，和（2）逐步更新 Running 摘要。他们还开发了一个自动度量器，叫做 BooookScore，用于衡量摘要的准确性。* Results: 作者们 obt 100 篇最近发表的书籍的 GPT-4 生成的摘要，并将其分为八种常见的准确性错误。他们发现，关闭源 LLM such as GPT-4 和 Claude 2 生成的摘要具有更高的 BooookScore，与 oft-repetitive 的 LLaMA 2 生成的摘要不同。增量更新 yields 较低的 BooookScore，但是具有更高的细节水平。<details>
<summary>Abstract</summary>
Summarizing book-length documents (>100K tokens) that exceed the context window size of large language models (LLMs) requires first breaking the input document into smaller chunks and then prompting an LLM to merge, update, and compress chunk-level summaries. Despite the complexity and importance of this task, it has yet to be meaningfully studied due to the challenges of evaluation: existing book-length summarization datasets (e.g., BookSum) are in the pretraining data of most public LLMs, and existing evaluation methods struggle to capture errors made by modern LLM summarizers. In this paper, we present the first study of the coherence of LLM-based book-length summarizers implemented via two prompting workflows: (1) hierarchically merging chunk-level summaries, and (2) incrementally updating a running summary. We obtain 1193 fine-grained human annotations on GPT-4 generated summaries of 100 recently-published books and identify eight common types of coherence errors made by LLMs. Because human evaluation is expensive and time-consuming, we develop an automatic metric, BooookScore, that measures the proportion of sentences in a summary that do not contain any of the identified error types. BooookScore has high agreement with human annotations and allows us to systematically evaluate the impact of many other critical parameters (e.g., chunk size, base LLM) while saving $15K and 500 hours in human evaluation costs. We find that closed-source LLMs such as GPT-4 and Claude 2 produce summaries with higher BooookScore than the oft-repetitive ones generated by LLaMA 2. Incremental updating yields lower BooookScore but higher level of detail than hierarchical merging, a trade-off sometimes preferred by human annotators. We release code and annotations after blind review to spur more principled research on book-length summarization.
</details>
<details>
<summary>摘要</summary>
大量文档摘要（>100K tokens）需要首先将输入文档分成更小的块，然后使用大型自然语言模型（LLM）来合并、更新和压缩块级摘要。尽管这项任务的复杂性和重要性尚未得到系统的研究，但是现有的书籍摘要 dataset（例如 BookSum）都包含在大多数公共 LLM 的预训练数据中，而现有的评估方法很难 Capture LLM 摘要器中的错误。在这篇论文中，我们提出了首次对 LLM 基于的书籍摘要器的准确性进行了研究。我们使用两种提示工作流程：（1）层次合并块级摘要，和（2）逐步更新RunningSummary。我们获得了1193个精细的人类标注，对 GPT-4 生成的 100 部最新出版的书籍摘要进行了评估，并发现了 eight 种常见的准确性错误。由于人类评估是昂贵和时间consuming的，我们开发了一个自动度量器，BooookScore，可以衡量摘要中含有准确性错误的句子的比例。BooookScore 与人类标注具有高一致性，我们可以通过 sistematic 地评估多个关键参数（例如块大小、基础 LLN）而节省 $15K 和 500 小时的人类评估成本。我们发现closed-source LLMs 如 GPT-4 和 Claude 2 生成的摘要具有更高的 BooookScore，而 LLLaMA 2 的摘要则具有较高的重复性。逐次更新具有较低的 BooookScore，但是具有更高的细节水平，这些trade-off 有时被人类 annotators 首选。我们在审查后发布代码和标注，以促进更理性的研究在书籍摘要领域。
</details></li>
</ul>
<hr>
<h2 id="Mining-Java-Memory-Errors-using-Subjective-Interesting-Subgroups-with-Hierarchical-Targets"><a href="#Mining-Java-Memory-Errors-using-Subjective-Interesting-Subgroups-with-Hierarchical-Targets" class="headerlink" title="Mining Java Memory Errors using Subjective Interesting Subgroups with Hierarchical Targets"></a>Mining Java Memory Errors using Subjective Interesting Subgroups with Hierarchical Targets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00781">http://arxiv.org/abs/2310.00781</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/remilyoucef/sca-miner">https://github.com/remilyoucef/sca-miner</a></li>
<li>paper_authors: Youcef Remil, Anes Bendimerad, Mathieu Chambard, Romain Mathonat, Marc Plantevit, Mehdi Kaytoue</li>
<li>for: 本文主要针对软件应用程序，尤其是企业资源计划（ERP）系统的维护问题。</li>
<li>methods: 本文提出了一种新的子组发现（SD）技术，可以自动 mines incident数据并提取独特的模式，以识别问题的根本原因。</li>
<li>results: 本文通过一个Empirical Study validate了该方法的有效性和Pattern的质量。<details>
<summary>Abstract</summary>
Software applications, especially Enterprise Resource Planning (ERP) systems, are crucial to the day-to-day operations of many industries. Therefore, it is essential to maintain these systems effectively using tools that can identify, diagnose, and mitigate their incidents. One promising data-driven approach is the Subgroup Discovery (SD) technique, a data mining method that can automatically mine incident datasets and extract discriminant patterns to identify the root causes of issues. However, current SD solutions have limitations in handling complex target concepts with multiple attributes organized hierarchically. To illustrate this scenario, we examine the case of Java out-of-memory incidents among several possible applications. We have a dataset that describes these incidents, including their context and the types of Java objects occupying memory when it reaches saturation, with these types arranged hierarchically. This scenario inspires us to propose a novel Subgroup Discovery approach that can handle complex target concepts with hierarchies. To achieve this, we design a pattern syntax and a quality measure that ensure the identified subgroups are relevant, non-redundant, and resilient to noise. To achieve the desired quality measure, we use the Subjective Interestingness model that incorporates prior knowledge about the data and promotes patterns that are both informative and surprising relative to that knowledge. We apply this framework to investigate out-of-memory errors and demonstrate its usefulness in incident diagnosis. To validate the effectiveness of our approach and the quality of the identified patterns, we present an empirical study. The source code and data used in the evaluation are publicly accessible, ensuring transparency and reproducibility.
</details>
<details>
<summary>摘要</summary>
To illustrate this scenario, we examine the case of Java out-of-memory incidents among several possible applications. We have a dataset that describes these incidents, including their context and the types of Java objects occupying memory when it reaches saturation, with these types arranged hierarchically. This scenario inspires us to propose a novel Subgroup Discovery approach that can handle complex target concepts with hierarchies.To achieve this, we design a pattern syntax and a quality measure that ensure the identified subgroups are relevant, non-redundant, and resilient to noise. To achieve the desired quality measure, we use the Subjective Interestingness model that incorporates prior knowledge about the data and promotes patterns that are both informative and surprising relative to that knowledge. We apply this framework to investigate out-of-memory errors and demonstrate its usefulness in incident diagnosis.To validate the effectiveness of our approach and the quality of the identified patterns, we present an empirical study. The source code and data used in the evaluation are publicly accessible, ensuring transparency and reproducibility.硬件应用程序，特别是企业资源规划（ERP）系统，对许多行业的日常运营是关键。因此，保持这些系统的效果是非常重要，使用可以识别、诊断和缓解incident的工具。一种有前途的数据驱动方法是Subgroup Discovery（SD）技术，可以自动挖掘incident数据集并提取描述性模式，以识别问题的根本原因。然而，现有的SD解决方案在处理复杂目标概念中存在限制，这些概念通常具有多个属性，并且归类在层次结构中。为了解释这种情况，我们选择了Java垃圾回收incident作为例子，我们有一个描述这些incident的数据集，包括incident的 контекст和占用内存资源的Java对象类型，这些类型以层次结构组织。这种情况提醒我们提出一种处理复杂目标概念的Subgroup Discovery方法。为了实现这一目标，我们设计了一种模式语法和质量度量，确保提取的子组是有用、非重复、抗噪的。为了实现所需的质量度量，我们使用Subjective Interestingness模型，该模型将数据中的知识纳入考虑，并且提高模式的有用性和surprise性，以便更好地描述数据。我们在调查垃圾回收incident中应用这种框架，并示出其在事件诊断中的有用性。为了证明我们的方法的有效性和模式的质量，我们进行了一个实验研究。研究中使用的源代码和数据都公开 accessible，以确保透明度和可重现性。
</details></li>
</ul>
<hr>
<h2 id="Pre-training-with-Synthetic-Data-Helps-Offline-Reinforcement-Learning"><a href="#Pre-training-with-Synthetic-Data-Helps-Offline-Reinforcement-Learning" class="headerlink" title="Pre-training with Synthetic Data Helps Offline Reinforcement Learning"></a>Pre-training with Synthetic Data Helps Offline Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00771">http://arxiv.org/abs/2310.00771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zecheng Wang, Che Wang, Zixuan Dong, Keith Ross</li>
<li>for: 这个论文主要研究了深度强化学习（DRL）的离线预训练方法，特别是使用大量语言资料来提高下游性能（Reid et al., 2022）。</li>
<li>methods: 本论文使用了几种简单的预训练方案，包括使用生成的IID数据和一步随机链生成的数据，以及使用Q学习算法和多层感知器（MLP）作为后续。</li>
<li>results: 实验结果表明，使用这些简单的预训练方案可以提高DRL的性能，并且可以与使用大量语言资料预训练的性能相比肩。此外，采用这些预训练方案可以提高CQL算法的性能，并且在D4RL Gym游戏数据集上获得了一致的性能提升。<details>
<summary>Abstract</summary>
Recently, it has been shown that for offline deep reinforcement learning (DRL), pre-training Decision Transformer with a large language corpus can improve downstream performance (Reid et al., 2022). A natural question to ask is whether this performance gain can only be achieved with language pre-training, or can be achieved with simpler pre-training schemes which do not involve language. In this paper, we first show that language is not essential for improved performance, and indeed pre-training with synthetic IID data for a small number of updates can match the performance gains from pre-training with a large language corpus; moreover, pre-training with data generated by a one-step Markov chain can further improve the performance. Inspired by these experimental results, we then consider pre-training Conservative Q-Learning (CQL), a popular offline DRL algorithm, which is Q-learning-based and typically employs a Multi-Layer Perceptron (MLP) backbone. Surprisingly, pre-training with simple synthetic data for a small number of updates can also improve CQL, providing consistent performance improvement on D4RL Gym locomotion datasets. The results of this paper not only illustrate the importance of pre-training for offline DRL but also show that the pre-training data can be synthetic and generated with remarkably simple mechanisms.
</details>
<details>
<summary>摘要</summary>
Inspired by these results, we then explore pre-training Conservative Q-Learning (CQL), a popular offline DRL algorithm that uses Q-learning and typically employs a Multi-Layer Perceptron (MLP) backbone. Surprisingly, we find that pre-training with simple synthetic data for a small number of updates can also improve CQL, providing consistent performance improvement on D4RL Gym locomotion datasets.The findings of this paper demonstrate the importance of pre-training for offline DRL and show that the pre-training data can be synthetic and generated with remarkably simple mechanisms. This has significant implications for the development of offline DRL algorithms and highlights the potential for using simple pre-training schemes to improve performance.
</details></li>
</ul>
<hr>
<h2 id="Facilitating-Battery-Swapping-Services-for-Freight-Trucks-with-Spatial-Temporal-Demand-Prediction"><a href="#Facilitating-Battery-Swapping-Services-for-Freight-Trucks-with-Spatial-Temporal-Demand-Prediction" class="headerlink" title="Facilitating Battery Swapping Services for Freight Trucks with Spatial-Temporal Demand Prediction"></a>Facilitating Battery Swapping Services for Freight Trucks with Spatial-Temporal Demand Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04440">http://arxiv.org/abs/2310.04440</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linyu Liu, Zhen Dai, Shiji Song, Xiaocheng Li, Guanting Chen</li>
<li>for: 这篇论文旨在探讨重型卡车电池更换服务的潜力和效率，以实现碳neutral未来。</li>
<li>methods: 论文运用了双重方法，首先预测了运输网络上未来几个小时的交通模式，然后将预测结果引入优化模组，实现电池的有效分配和部署。</li>
<li>results: 分析了2,500英里长的高速公路重型卡车数据，我们发现预测&#x2F;机器学习可以帮助未来的决策。具体来说，我们发现在设置早期的移动电池更换站更有利，但是随着系统的成熟，固定位置的电池更换站更受欢迎。<details>
<summary>Abstract</summary>
Electrifying heavy-duty trucks offers a substantial opportunity to curtail carbon emissions, advancing toward a carbon-neutral future. However, the inherent challenges of limited battery energy and the sheer weight of heavy-duty trucks lead to reduced mileage and prolonged charging durations. Consequently, battery-swapping services emerge as an attractive solution for these trucks. This paper employs a two-fold approach to investigate the potential and enhance the efficacy of such services. Firstly, spatial-temporal demand prediction models are adopted to predict the traffic patterns for the upcoming hours. Subsequently, the prediction guides an optimization module for efficient battery allocation and deployment. Analyzing the heavy-duty truck data on a highway network spanning over 2,500 miles, our model and analysis underscore the value of prediction/machine learning in facilitating future decision-makings. In particular, we find that the initial phase of implementing battery-swapping services favors mobile battery-swapping stations, but as the system matures, fixed-location stations are preferred.
</details>
<details>
<summary>摘要</summary>
电动重型卡车的应用提供了巨大的减少碳排放的机会，推进向碳中和未来。然而，重型卡车的自然限制，如电池能量有限和车辆总重，导致很长的充电时间和减少的行驶距离。因此，电池换卡服务出现了一个有吸引力的解决方案。本文采用两重方法来探讨这种服务的潜在和提高效率。首先，采用空间-时间需求预测模型预测下一个几个小时的交通趋势。然后，预测导引一个优化模块，以便有效地分配和部署电池。分析了2,500英里长的高速公路上的重型卡车数据，我们的模型和分析表明，预测/机器学习在未来决策中发挥了重要作用。尤其是在实施电池换卡服务的初期阶段，移动电池换卡站更有优势；而在系统成熟后，固定位置的电池换卡站变得更加受欢迎。
</details></li>
</ul>
<hr>
<h2 id="Mind-the-Gap-Federated-Learning-Broadens-Domain-Generalization-in-Diagnostic-AI-Models"><a href="#Mind-the-Gap-Federated-Learning-Broadens-Domain-Generalization-in-Diagnostic-AI-Models" class="headerlink" title="Mind the Gap: Federated Learning Broadens Domain Generalization in Diagnostic AI Models"></a>Mind the Gap: Federated Learning Broadens Domain Generalization in Diagnostic AI Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00757">http://arxiv.org/abs/2310.00757</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tayebiarasteh/fldomain">https://github.com/tayebiarasteh/fldomain</a></li>
<li>paper_authors: Soroosh Tayebi Arasteh, Christiane Kuhl, Marwin-Jonathan Saehn, Peter Isfort, Daniel Truhn, Sven Nebelung<br>for: 这项研究旨在评估 Federated Learning（FL）在骨肢X射线图像分类 task 中的影响，特别是训练策略、网络架构和数据多样性等因素对模型的预测性能的影响。methods: 研究使用了610,000个骨肢X射线图像数据集，来评估不同训练策略、网络架构和数据多样性对模型的预测性能。results: 研究发现，虽然大型数据集可能会增加FL的性能，但是在某些情况下，甚至会导致性能下降。相反，小型数据集表现出了明显的改善。因此，本地训练和FL的性能主要受到训练数据大小的影响，而不同数据集之间的多样性则对于Off-domain任务的性能产生了更大的影响。通过合作训练在多个外部机构的数据上，FL可以提高隐私、可重现性和 Off-domain 可靠性，并且可能提高医疗结果。<details>
<summary>Abstract</summary>
Developing robust artificial intelligence (AI) models that generalize well to unseen datasets is challenging and usually requires large and variable datasets, preferably from multiple institutions. In federated learning (FL), a model is trained collaboratively at numerous sites that hold local datasets without exchanging them. So far, the impact of training strategy, i.e., local versus collaborative, on the diagnostic on-domain and off-domain performance of AI models interpreting chest radiographs has not been assessed. Consequently, using 610,000 chest radiographs from five institutions across the globe, we assessed diagnostic performance as a function of training strategy (i.e., local vs. collaborative), network architecture (i.e., convolutional vs. transformer-based), generalization performance (i.e., on-domain vs. off-domain), imaging finding (i.e., cardiomegaly, pleural effusion, pneumonia, atelectasis, consolidation, pneumothorax, and no abnormality), dataset size (i.e., from n=18,000 to 213,921 radiographs), and dataset diversity. Large datasets not only showed minimal performance gains with FL but, in some instances, even exhibited decreases. In contrast, smaller datasets revealed marked improvements. Thus, on-domain performance was mainly driven by training data size. However, off-domain performance leaned more on training diversity. When trained collaboratively across diverse external institutions, AI models consistently surpassed models trained locally for off-domain tasks, emphasizing FL's potential in leveraging data diversity. In conclusion, FL can bolster diagnostic privacy, reproducibility, and off-domain reliability of AI models and, potentially, optimize healthcare outcomes.
</details>
<details>
<summary>摘要</summary>
<<SYS>>发展强健的人工智能（AI）模型，使其在未见数据集上具有良好的泛化性是一项挑战。通常需要大量和多样的数据集，从多个机构获取。在联合学习（FL）中，模型在多个地点进行协同学习，而不需要交换本地数据。 jusqu'à présent，训练策略（本地 versus 协同）对AI模型解剖学影像鉴定性的影响尚未得到评估。为了解决这个问题，我们使用了全球五个机构的610,000张胸部X射影像，评估AI模型的鉴定性以训练策略、网络架构、泛化性、影像发现（cardiomegaly, pleural effusion, pneumonia, atelectasis, consolidation, pneumothorax, 和无异常）、数据集大小（从n=18,000到213,921 radiographs）和数据多样性之间的关系。结果表明，大型数据集不仅在FL中没有获得明显的性能提升，有些情况下甚至显示下降。相反，较小的数据集表现出了明显的改善。因此，本地训练的性能主要受到训练数据集大小的影响，而在不同机构的外部数据集上进行协同训练的性能则更多受到训练多样性的影响。当AI模型在多个外部机构上进行协同训练时，其在外部任务上的性能 consistently 高于本地训练的模型，这将FL的潜在作用在拓展数据多样性方面强调。总之，FL可以提高鉴定隐私、重现性和外部可靠性的AI模型，并且可能会优化医疗结果。
</details></li>
</ul>
<hr>
<h2 id="TIGERScore-Towards-Building-Explainable-Metric-for-All-Text-Generation-Tasks"><a href="#TIGERScore-Towards-Building-Explainable-Metric-for-All-Text-Generation-Tasks" class="headerlink" title="TIGERScore: Towards Building Explainable Metric for All Text Generation Tasks"></a>TIGERScore: Towards Building Explainable Metric for All Text Generation Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00752">http://arxiv.org/abs/2310.00752</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongfu Jiang, Yishan Li, Ge Zhang, Wenhao Huang, Bill Yuchen Lin, Wenhu Chen</li>
<li>for: 本研究开发了一个名为TIGERScore的自动评估指标，用于评估文本生成任务的效果。</li>
<li>methods: TIGERScore使用了专门训练的LLaMA模型，并基于自己调整的MetricInstruct dataset，以提供可读的错误分析，并不需要参考。</li>
<li>results: TIGERScore在5个对接评分数据集上 Achieves the highest overall Spearman’s correlation with human ratings，并且与其他指标相比表现更好，甚至可以超越参考基于的指标。<details>
<summary>Abstract</summary>
We present TIGERScore, a \textbf{T}rained metric that follows \textbf{I}nstruction \textbf{G}uidance to perform \textbf{E}xplainable, and \textbf{R}eference-free evaluation over a wide spectrum of text generation tasks. Different from other automatic evaluation methods that only provide arcane scores, TIGERScore is guided by the natural language instruction to provide error analysis to pinpoint the mistakes in the generated text. Our metric is based on LLaMA, trained on our meticulously curated instruction-tuning dataset MetricInstruct which covers 6 text generation tasks and 23 text generation datasets. The dataset consists of 48K quadruple in the form of (instruction, input, system output $\rightarrow$ error analysis). We collected the `system outputs' through diverse channels to cover different types of errors. To quantitatively assess our metric, we evaluate its correlation with human ratings on 5 held-in datasets, 2 held-out datasets and show that TIGERScore can achieve the highest overall Spearman's correlation with human ratings across these datasets and outperforms other metrics significantly. As a reference-free metric, its correlation can even surpass the best existing reference-based metrics. To further qualitatively assess the rationale generated by our metric, we conduct human evaluation on the generated explanations and found that the explanations are 70.8\% accurate. Through these experimental results, we believe TIGERScore demonstrates the possibility of building universal explainable metrics to evaluate any text generation task.
</details>
<details>
<summary>摘要</summary>
我们介绍TIGERScore，一个已经训练的度量，可以根据自然语言指南进行可解释的、无参考度的文本生成任务评价。与其他自动评价方法不同，TIGERScore不仅提供神秘的分数，还可以通过错误分析来 pinpoint生成文本中的错误。我们的度量基于LLaMA，并在我们精心抽样的指南调度集MetricInstruct上训练。这个集合包括6种文本生成任务和23种文本生成数据集，共48000个四元组（指南、输入、系统输出 → 错误分析）。我们通过多种途径收集了“系统输出”，以覆盖不同类型的错误。为了评估我们的度量，我们对5个保留数据集、2个保 OUT数据集进行了量化评估，并发现TIGERScore可以在这些数据集中 achiev the highest Spearman correlation coefficient with human ratings，并且与其他度量相比显著出perform better。作为一个无参考度量，TIGERScore的相关性可以甚至超过最佳参考基础度量。为了进一步评估我们的度量生成的理由，我们对生成的解释进行了人工评估，并发现解释的准确率为70.8%。通过这些实验结果，我们认为TIGERScore表明了可以建立 universal explainable metrics，用于评价任何文本生成任务。
</details></li>
</ul>
<hr>
<h2 id="NoxTrader-LSTM-Based-Stock-Return-Momentum-Prediction-for-Quantitative-Trading"><a href="#NoxTrader-LSTM-Based-Stock-Return-Momentum-Prediction-for-Quantitative-Trading" class="headerlink" title="NoxTrader: LSTM-Based Stock Return Momentum Prediction for Quantitative Trading"></a>NoxTrader: LSTM-Based Stock Return Momentum Prediction for Quantitative Trading</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00747">http://arxiv.org/abs/2310.00747</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hsiang-Hui Liu, Han-Jay Shu, Wei-Ning Chiu</li>
<li>for: 这个研究主要目的是在股票市场中获得资金收益，尤其是在中期至长期的时间预测。</li>
<li>methods: 这个研究使用时间序列分析来学习股票市场的趋势，并使用价格和股票量数据进行特征工程。他们还使用Long Short-Term Memory（LSTM）模型来捕捉价格趋势，并在交易过程中进行动态模型更新。</li>
<li>results: 这个研究获得了一些优秀的预测数据，其中预测和实际市场资料之间的距离在0.65至0.75之间。他们还使用筛选技术来改善初始投资回报，从-60%提升到325%.<details>
<summary>Abstract</summary>
We introduce NoxTrader, a sophisticated system designed for portfolio construction and trading execution with the primary objective of achieving profitable outcomes in the stock market, specifically aiming to generate moderate to long-term profits. The underlying learning process of NoxTrader is rooted in the assimilation of valuable insights derived from historical trading data, particularly focusing on time-series analysis due to the nature of the dataset employed. In our approach, we utilize price and volume data of US stock market for feature engineering to generate effective features, including Return Momentum, Week Price Momentum, and Month Price Momentum. We choose the Long Short-Term Memory (LSTM)model to capture continuous price trends and implement dynamic model updates during the trading execution process, enabling the model to continuously adapt to the current market trends. Notably, we have developed a comprehensive trading backtesting system - NoxTrader, which allows us to manage portfolios based on predictive scores and utilize custom evaluation metrics to conduct a thorough assessment of our trading performance. Our rigorous feature engineering and careful selection of prediction targets enable us to generate prediction data with an impressive correlation range between 0.65 and 0.75. Finally, we monitor the dispersion of our prediction data and perform a comparative analysis against actual market data. Through the use of filtering techniques, we improved the initial -60% investment return to 325%.
</details>
<details>
<summary>摘要</summary>
我们介绍NoxTrader，一个复杂的系统，用于股票投资组合建立和交易执行，主要目标是在股市中实现可观的收益。我们的学习过程借鉴了历史交易数据中的宝贵经验，特别是时间序列分析，因为我们使用的数据集是时间序列型的。在我们的方法中，我们利用美国股市价格和量数据进行特征工程，生成有效特征，包括回报势力、周期势力和月度势力。我们选择Long Short-Term Memory（LSTM）模型，以捕捉连续价格趋势，并在交易执行过程中进行动态模型更新，使模型能够不断适应当前市场趋势。值得一提的是，我们开发了一套完整的交易回测系统——NoxTrader，它允许我们基于预测得分来管理投资组合，并使用自定义评估 metric来进行严格的评估我们的交易性能。我们的严格的特征工程和预测目标的精心选择，使我们能够生成预测数据的各种相关度范围在0.65-0.75之间。最后，我们监测预测数据的分散情况，并对实际市场数据进行比较分析。通过筛选技术，我们从初始投资回报下降至60%的位置提高至325%。
</details></li>
</ul>
<hr>
<h2 id="RoleLLM-Benchmarking-Eliciting-and-Enhancing-Role-Playing-Abilities-of-Large-Language-Models"><a href="#RoleLLM-Benchmarking-Eliciting-and-Enhancing-Role-Playing-Abilities-of-Large-Language-Models" class="headerlink" title="RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models"></a>RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00746">http://arxiv.org/abs/2310.00746</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/interactivenlp-team/rolellm-public">https://github.com/interactivenlp-team/rolellm-public</a></li>
<li>paper_authors: Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, Zhaoxiang Zhang, Wanli Ouyang, Ke Xu, Wenhu Chen, Jie Fu, Junran Peng</li>
<li>for: 本文旨在提高语言模型（LLM）的角色扮演能力，以增强用户交互。</li>
<li>methods: 本文提出了一个框架，名为RoleLLM，用于评估、引出和提高 LLM 的角色扮演能力。RoleLLM 包括四个阶段：（1）角色资料构建（Role Profile Construction），（2）基于上下文的指令生成（Context-Based Instruction Generation），（3）角色提示（Role Prompting），以及（4）角色定制化指令调整（Role-Conditioned Instruction Tuning）。</li>
<li>results: 通过 Context-Instruct 和 RoleGPT，我们创建了 RoleBench，这是首个系统性的、细致的字级 benchmark 数据集，用于测试角色扮演能力。此外，通过 RoCIT 在 RoleBench 上进行调整，我们获得了 RoleLLaMA（英文）和 RoleGLM（中文），这些模型显著提高了角色扮演能力，甚至与 RoleGPT（使用 GPT-4）具有相同的Results。<details>
<summary>Abstract</summary>
The advent of Large Language Models (LLMs) has paved the way for complex tasks such as role-playing, which enhances user interactions by enabling models to imitate various characters. However, the closed-source nature of state-of-the-art LLMs and their general-purpose training limit role-playing optimization. In this paper, we introduce RoleLLM, a framework to benchmark, elicit, and enhance role-playing abilities in LLMs. RoleLLM comprises four stages: (1) Role Profile Construction for 100 roles; (2) Context-Based Instruction Generation (Context-Instruct) for role-specific knowledge extraction; (3) Role Prompting using GPT (RoleGPT) for speaking style imitation; and (4) Role-Conditioned Instruction Tuning (RoCIT) for fine-tuning open-source models along with role customization. By Context-Instruct and RoleGPT, we create RoleBench, the first systematic and fine-grained character-level benchmark dataset for role-playing with 168,093 samples. Moreover, RoCIT on RoleBench yields RoleLLaMA (English) and RoleGLM (Chinese), significantly enhancing role-playing abilities and even achieving comparable results with RoleGPT (using GPT-4).
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的出现已经为复杂的任务如角色扮演提供了方便，这些任务可以使模型模拟多种角色，从而提高用户交互的体验。然而，现有的State-of-the-art LLMs的关闭源代码和通用训练限制了角色扮演优化。在这篇论文中，我们介绍了RoleLLM框架，用于评价、引导和提高LLMs中的角色扮演能力。RoleLLM包括四个阶段：（1）角色Profile构建100个角色；（2）基于上下文的指令生成（Context-Instruct）用于角色特定知识提取；（3）基于GPT的角色提示（RoleGPT）用于模仿说话风格；以及（4）基于角色的Conditioned Instruction Tuning（RoCIT）用于 fine-tuning开源模型以及角色定制。通过Context-Instruct和RoleGPT，我们创建了RoleBench，第一个系统和细化的字符级 benchmark dataset для角色扮演，包含168,093个样本。此外，在RoleBench上进行RoCIT后，我们获得了RoleLLaMA（英语）和RoleGLM（中文），两个能够明显提高角色扮演能力的模型，甚至与RoleGPT（使用GPT-4）相当。
</details></li>
</ul>
<hr>
<h2 id="My-Machine-and-I-ChatGPT-and-the-Future-of-Human-Machine-Collaboration-in-Africa"><a href="#My-Machine-and-I-ChatGPT-and-the-Future-of-Human-Machine-Collaboration-in-Africa" class="headerlink" title="My Machine and I: ChatGPT and the Future of Human-Machine Collaboration in Africa"></a>My Machine and I: ChatGPT and the Future of Human-Machine Collaboration in Africa</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13704">http://arxiv.org/abs/2310.13704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Munachimso Blessing Oguine, Chidera Godsfavor Oguine, Kanyifeechukwu Jane Oguine</li>
<li>for: 本研究旨在探讨聊天GPT在人机合作中的效果。</li>
<li>methods: 本研究使用反透明主题分析方法对51篇2019-2023年的文章进行分析。</li>
<li>results: 研究发现聊天GPT在学术领域 such as 教育和研究中的人机交互非常普遍，而且聊天GPT在改善人机合作方面的效果较高。<details>
<summary>Abstract</summary>
Recent advancements in technology have necessitated a paradigm shift in the people use technology necessitating a new research field called Human-Machine collaboration. ChatGPT, an Artificial intelligence (AI) assistive technology, has gained mainstream adoption and implementation in academia and industry; however, a lot is left unknown about how this new technology holds for Human-Machine Collaboration in Africa. Our survey paper highlights to answer some of these questions. To understand the effectiveness of ChatGPT on human-machine collaboration we utilized reflexive thematic analysis to analyze (N= 51) articles between 2019 and 2023 obtained from our literature search. Our findings indicate the prevalence of ChatGPT for human-computer interaction within academic sectors such as education, and research; trends also revealed the relatively high effectiveness of ChatGPT in improving human-machine collaboration.
</details>
<details>
<summary>摘要</summary>
最近的技术发展使得人机合作的研究领域得到了推动，这种新的研究领域被称为人机合作。智能人工智能（AI）协助技术ChatGPT在学术和产业界得到了广泛的批处和实施，但是关于这种新技术在非洲的人机合作方面还有很多未知之处。我们的调查论文旨在回答这些问题。为了评估ChatGPT在人机合作效果，我们使用了反思主题分析法分析（N=51）于2019年至2023年之间的文章。我们的发现表明了ChatGPT在教育和研究领域的人机交互非常普遍，并且发现ChatGPT在改善人机合作效果方面的趋势相对较高。
</details></li>
</ul>
<hr>
<h2 id="GenAI-Against-Humanity-Nefarious-Applications-of-Generative-Artificial-Intelligence-and-Large-Language-Models"><a href="#GenAI-Against-Humanity-Nefarious-Applications-of-Generative-Artificial-Intelligence-and-Large-Language-Models" class="headerlink" title="GenAI Against Humanity: Nefarious Applications of Generative Artificial Intelligence and Large Language Models"></a>GenAI Against Humanity: Nefarious Applications of Generative Artificial Intelligence and Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00737">http://arxiv.org/abs/2310.00737</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emilio Ferrara</li>
<li>for: This paper is written to raise awareness about the potential risks and challenges of Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) being misused for nefarious purposes.</li>
<li>methods: The paper uses a combination of research and analysis to identify the potential risks of GenAI and LLMs, including their use in deepfakes, malicious content generation, and the creation of synthetic identities.</li>
<li>results: The paper highlights the potential consequences of GenAI and LLMs being misused, including the blurring of the lines between the virtual and real worlds, the potential for targeted misinformation and scams, and the creation of sophisticated malware. The paper also serves as a call to action to prepare for these potential risks and challenges.In Simplified Chinese text, the three key points would be:</li>
<li>for: 这篇论文是为了提醒大家关于生成人工智能（GenAI）和大语言模型（LLMs）的可能的风险和挑战。</li>
<li>methods: 论文使用了组合的研究和分析来识别GenAI和LLMs的可能的风险，包括它们在深圳中的使用、邪恶内容生成和假造标识等。</li>
<li>results: 论文 highlights GenAI和LLMs的可能的后果，包括虚拟和现实世界之间的边界模糊、targeted的谣言和骗局、以及高级的黑客软件。论文也 serves as a call to action，准备这些可能的风险和挑战。<details>
<summary>Abstract</summary>
Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) are marvels of technology; celebrated for their prowess in natural language processing and multimodal content generation, they promise a transformative future. But as with all powerful tools, they come with their shadows. Picture living in a world where deepfakes are indistinguishable from reality, where synthetic identities orchestrate malicious campaigns, and where targeted misinformation or scams are crafted with unparalleled precision. Welcome to the darker side of GenAI applications. This article is not just a journey through the meanders of potential misuse of GenAI and LLMs, but also a call to recognize the urgency of the challenges ahead. As we navigate the seas of misinformation campaigns, malicious content generation, and the eerie creation of sophisticated malware, we'll uncover the societal implications that ripple through the GenAI revolution we are witnessing. From AI-powered botnets on social media platforms to the unnerving potential of AI to generate fabricated identities, or alibis made of synthetic realities, the stakes have never been higher. The lines between the virtual and the real worlds are blurring, and the consequences of potential GenAI's nefarious applications impact us all. This article serves both as a synthesis of rigorous research presented on the risks of GenAI and misuse of LLMs and as a thought-provoking vision of the different types of harmful GenAI applications we might encounter in the near future, and some ways we can prepare for them.
</details>
<details>
<summary>摘要</summary>
生成人工智能（GenAI）和大型语言模型（LLMs）是技术的宠儿，被庆贤以其在自然语言处理和多模式内容生成的能力。它们承诺一个转型的未来。但就像所有的强大工具一样，它们也有阴影。 imagine living in a world where deepfakes are indistinguishable from reality, where synthetic identities orchestrate malicious campaigns, and where targeted misinformation or scams are crafted with unparalleled precision. Welcome to the darker side of GenAI applications. This article is not just a journey through the meanders of potential misuse of GenAI and LLMs, but also a call to recognize the urgency of the challenges ahead. As we navigate the seas of misinformation campaigns, malicious content generation, and the eerie creation of sophisticated malware, we'll uncover the societal implications that ripple through the GenAI revolution we are witnessing. From AI-powered botnets on social media platforms to the unnerving potential of AI to generate fabricated identities, or alibis made of synthetic realities, the stakes have never been higher. The lines between the virtual and the real worlds are blurring, and the consequences of potential GenAI's nefarious applications impact us all. This article serves both as a synthesis of rigorous research presented on the risks of GenAI and misuse of LLMs and as a thought-provoking vision of the different types of harmful GenAI applications we might encounter in the near future, and some ways we can prepare for them.
</details></li>
</ul>
<hr>
<h2 id="Review-of-deep-learning-in-healthcare"><a href="#Review-of-deep-learning-in-healthcare" class="headerlink" title="Review of deep learning in healthcare"></a>Review of deep learning in healthcare</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00727">http://arxiv.org/abs/2310.00727</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/avadhutsonavane/Diagnosis-of-Coronavirus-using-chest-X-RAY">https://github.com/avadhutsonavane/Diagnosis-of-Coronavirus-using-chest-X-RAY</a></li>
<li>paper_authors: Hasan Hejbari Zargar, Saha Hejbari Zargar, Raziye Mehri</li>
<li>for: 本研究旨在探讨医疗系统中使用深度学习方法，包括最新的网络设计、应用和市场趋势。</li>
<li>methods: 本研究使用深度学习方法，包括深度神经网络模型，以提取医疗数据中隐藏的模式和有价值信息。</li>
<li>results: 研究发现，深度学习方法在医疗系统中可以提取到有价值的信息，但是需要更好地结合人类医疗解释才能实现更高效的应用。<details>
<summary>Abstract</summary>
Given the growing complexity of healthcare data over the last several years, using machine learning techniques like Deep Neural Network (DNN) models has gained increased appeal. In order to extract hidden patterns and other valuable information from the huge quantity of health data, which traditional analytics are unable to do in a reasonable length of time, machine learning (ML) techniques are used. Deep Learning (DL) algorithms in particular have been shown as potential approaches to pattern identification in healthcare systems. This thought has led to the contribution of this research, which examines deep learning methods used in healthcare systems via an examination of cutting-edge network designs, applications, and market trends. To connect deep learning methodologies and human healthcare interpretability, the initial objective is to provide in-depth insight into the deployment of deep learning models in healthcare solutions. And last, to outline the current unresolved issues and potential directions.
</details>
<details>
<summary>摘要</summary>
随着医疗数据的增长复杂性，使用机器学习技术如深度神经网络（DNN）模型已经得到了加大的appeal。为了从庞大量的医疗数据中提取隐藏的模式和其他有价值的信息，传统分析无法在合理的时间内完成，因此机器学习（ML）技术被使用。深度学习（DL）算法在医疗系统中特别有潜力，这也导致了本研究的出发，即通过对当前最新的网络设计、应用和市场趋势进行检验，探讨深度学习在医疗解决方案中的应用。为了将深度学习方法与人类医疗解释相连接，初始的目标是提供深度学习模型在医疗解决方案中的深入分析。最后，总结当前未解决的问题和可能的发展方向。
</details></li>
</ul>
<hr>
<h2 id="Improving-Length-Generalization-in-Transformers-via-Task-Hinting"><a href="#Improving-Length-Generalization-in-Transformers-via-Task-Hinting" class="headerlink" title="Improving Length-Generalization in Transformers via Task Hinting"></a>Improving Length-Generalization in Transformers via Task Hinting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00726">http://arxiv.org/abs/2310.00726</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranjal Awasthi, Anupam Gupta</li>
<li>for: 本研究旨在解决 transformer 模型在某些逻辑和数学任务上长度泛化问题。特别是，一个基于添加的 transformer 模型在应用于更长的实例时表现会下降很快。本研究提出了一种基于任务提示的方法，以解决长度泛化问题。</li>
<li>methods: 本研究使用了多任务训练框架，并在训练过程中同时训练模型解决一个简单且相关的 auxillary 任务。</li>
<li>results: 对于排序问题，我们发现可以使用 sequences 的 length 不超过 20 来训练模型，并在 test 数据上提高了模型的测试准确率从 less than 1% (标准训练) 提高到更多于 92% (via 任务提示)。此外，我们还发现了一些有趣的长度泛化问题的方面，包括不同的 auxillary 任务的效iveness 在提高长度泛化方面有很大差异。<details>
<summary>Abstract</summary>
It has been observed in recent years that transformers have problems with length generalization for certain types of reasoning and arithmetic tasks. In particular, the performance of a transformer model trained on tasks (say addition) up to a certain length (e.g., 5 digit numbers) drops sharply when applied to longer instances of the same problem. This work proposes an approach based on task hinting towards addressing length generalization. Our key idea is that while training the model on task-specific data, it is helpful to simultaneously train the model to solve a simpler but related auxiliary task as well.   We study the classical sorting problem as a canonical example to evaluate our approach. We design a multitask training framework and show that task hinting significantly improve length generalization. For sorting we show that it is possible to train models on data consisting of sequences having length at most $20$, and improve the test accuracy on sequences of length $100$ from less than 1% (for standard training) to more than 92% (via task hinting).   Our study uncovers several interesting aspects of length generalization. We observe that while several auxiliary tasks may seem natural a priori, their effectiveness in improving length generalization differs dramatically. We further use probing and visualization-based techniques to understand the internal mechanisms via which the model performs the task, and propose a theoretical construction consistent with the observed learning behaviors of the model. Based on our construction, we show that introducing a small number of length dependent parameters into the training procedure can further boost the performance on unseen lengths. Finally, we also show the efficacy of our task hinting based approach beyond sorting, giving hope that these techniques will be applicable in broader contexts.
</details>
<details>
<summary>摘要</summary>
近年来，transformer模型在某些逻辑和数学任务中表现出长度泛化问题。具体来说，一个基于添加任务的transformer模型在应用于更长的问题时表现下降。这项工作提出一种基于任务提示的方法来解决长度泛化问题。我们的关键想法是在训练模型时，同时训练模型解决一个相关的简单任务。我们选择排序问题作为一个典型的例子来评估我们的方法。我们设计了一个多任务训练框架，并证明了任务提示可以显著提高长度泛化。对于排序问题，我们可以在数据中包含长度不超过20的序列，并在测试时提高测试 accuracy 从 less than 1% (标准训练) 到更多于92% (via任务提示)。我们的研究揭示了长度泛化的几个有趣方面。我们发现，虽然一些 auxillary task 可能看起来很自然，但它们在提高长度泛化效果上差异很大。我们还使用探测和视觉化技术来理解模型如何完成任务，并提出了一种理论建构，该建构与模型学习行为相符。基于该建构，我们表明在训练过程中引入一小数量的长度参数可以进一步提高对未经见长度的表现。最后，我们还证明了我们的任务提示基本方法在更广泛的上下文中有效。
</details></li>
</ul>
<hr>
<h2 id="Subtractive-Mixture-Models-via-Squaring-Representation-and-Learning"><a href="#Subtractive-Mixture-Models-via-Squaring-Representation-and-Learning" class="headerlink" title="Subtractive Mixture Models via Squaring: Representation and Learning"></a>Subtractive Mixture Models via Squaring: Representation and Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00724">http://arxiv.org/abs/2310.00724</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anon-npc/squared-npcs">https://github.com/anon-npc/squared-npcs</a></li>
<li>paper_authors: Lorenzo Loconte, Aleksanteri M. Sladek, Stefan Mengel, Martin Trapp, Arno Solin, Nicolas Gillis, Antonio Vergari</li>
<li>for: 用于模型复杂的分布</li>
<li>methods: 使用深度减法 mixture 模型</li>
<li>results: 可以提高表达能力，并且在实际分布估计任务中得到良好效果<details>
<summary>Abstract</summary>
Mixture models are traditionally represented and learned by adding several distributions as components. Allowing mixtures to subtract probability mass or density can drastically reduce the number of components needed to model complex distributions. However, learning such subtractive mixtures while ensuring they still encode a non-negative function is challenging. We investigate how to learn and perform inference on deep subtractive mixtures by squaring them. We do this in the framework of probabilistic circuits, which enable us to represent tensorized mixtures and generalize several other subtractive models. We theoretically prove that the class of squared circuits allowing subtractions can be exponentially more expressive than traditional additive mixtures; and, we empirically show this increased expressiveness on a series of real-world distribution estimation tasks.
</details>
<details>
<summary>摘要</summary>
混合模型通常通过添加多个分布来表示和学习。然而，允许混合 subtract 概率质量或密度可以很快减少需要odel复杂分布的组件数量。然而，学习这种 subtractive 混合并确保它们仍然表示非负函数是困难的。我们研究如何在概率Circuits框架下学习和进行推理深 subtractive 混合，并证明在这种框架下，allowing squaring 可以在exponentially more expressive的基础上表示。此外，我们还employs empirical evidence demonstrate this increased expressiveness on a series of real-world distribution estimation tasks。Note: Simplified Chinese is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="A-Simple-Yet-Effective-Strategy-to-Robustify-the-Meta-Learning-Paradigm"><a href="#A-Simple-Yet-Effective-Strategy-to-Robustify-the-Meta-Learning-Paradigm" class="headerlink" title="A Simple Yet Effective Strategy to Robustify the Meta Learning Paradigm"></a>A Simple Yet Effective Strategy to Robustify the Meta Learning Paradigm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00708">http://arxiv.org/abs/2310.00708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qi Wang, Yiqin Lv, Yanghe Feng, Zheng Xie, Jincai Huang</li>
<li>for: 提高 meta 学习的可靠性和鲁棒性，尤其是在风险敏感的情况下。</li>
<li>methods: 基于分布 robust 思想来优化 meta 学习管道，并使用预期尾风险度量进行优化。</li>
<li>results: 实验结果显示，我们的简单方法可以提高 meta 学习对任务分布的Robustness，降低 conditional 预期最坏快速风险的平均值。<details>
<summary>Abstract</summary>
Meta learning is a promising paradigm to enable skill transfer across tasks. Most previous methods employ the empirical risk minimization principle in optimization. However, the resulting worst fast adaptation to a subset of tasks can be catastrophic in risk-sensitive scenarios. To robustify fast adaptation, this paper optimizes meta learning pipelines from a distributionally robust perspective and meta trains models with the measure of expected tail risk. We take the two-stage strategy as heuristics to solve the robust meta learning problem, controlling the worst fast adaptation cases at a certain probabilistic level. Experimental results show that our simple method can improve the robustness of meta learning to task distributions and reduce the conditional expectation of the worst fast adaptation risk.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>基于实际风险最小化原则的现有方法通常采用Meta学习。然而，这可能导致在任务分布下的最坏快adaptation情况，在风险敏感场景下可能是灾难性的。为了强化快adaptation的稳定性，这篇论文从分布 robust perspective来优化Meta学习管道，并使用度量预期的尾风险来训练Meta模型。我们采用两阶段策略来解决Robust Meta学习问题，在某些 probabilistic水平上控制最坏快adaptation的情况。实验结果表明，我们的简单方法可以提高Meta学习的任务分布Robustness和降低最坏快adaptation风险的 conditional expectation。
</details></li>
</ul>
<hr>
<h2 id="Meta-Semantic-Template-for-Evaluation-of-Large-Language-Models"><a href="#Meta-Semantic-Template-for-Evaluation-of-Large-Language-Models" class="headerlink" title="Meta Semantic Template for Evaluation of Large Language Models"></a>Meta Semantic Template for Evaluation of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01448">http://arxiv.org/abs/2310.01448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yachuan Liu, Liang Chen, Jindong Wang, Qiaozhu Mei, Xing Xie</li>
<li>for: 评估大语言模型（LLMs）的 semantics 理解能力，不是仅仅是 memorize 训练数据。</li>
<li>methods: 提出了 MSTemp 方法，通过创建meta semantic templates来评估 LLMs 的 semantics 理解能力。</li>
<li>results: MSTemp 可以生成高度 OUT-OF-DISTRIBUTION（OOD）评估样本，并且可以显著降低 LLMS 使用现有数据集作为种子时的性能。<details>
<summary>Abstract</summary>
Do large language models (LLMs) genuinely understand the semantics of the language, or just memorize the training data? The recent concern on potential data contamination of LLMs has raised awareness of the community to conduct research on LLMs evaluation. In this paper, we propose MSTemp, an approach that creates meta semantic templates to evaluate the semantic understanding ability of LLMs. The core of MSTemp is not to perform evaluation directly on existing benchmark datasets, but to generate new out-of-distribution (OOD) evaluation sets using existing datasets as seeds. Specifically, for a given sentence, MSTemp leverages another language model to generate new samples while preserving its semantics. The new samples are called semantic templates to the original sentence. Then, MSTemp generates evaluation samples via sentence parsing and random word replacement on the semantic templates. MSTemp is highly flexible, dynamic, and cost-effective. Our initial experiments show that MSTemp-generated samples can significantly reduce the performance of LLMs using existing datasets as seeds. We hope this initial work can shed light on future research of LLMs evaluation.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>> LLMs 真的理解语言 semantics 吗，或者只是Memorize 训练数据？在社区关注 LLMS 可能存在数据束缚的问题后，我们提出了一种方法来评估 LLMS。在这篇论文中，我们提出了 MSTemp，它使用现有的语言模型生成新的 OUT-OF-DISTRIBUTION（OOD）评估集。具体来说，对于一个句子，MSTemp 利用另一个语言模型生成新的样本，保持句子的 semantics。然后，MSTemp 使用句子分析和随机词替换来生成评估样本。MSTemp 具有高度的灵活性、动态性和成本效益。我们的初步实验表明，MSTemp 生成的样本可以使 LLMS 使用现有数据集作为种子时表现出显著的下降性能。我们希望这些初步研究可以鼓励未来 LLMS 评估的研究。
</details></li>
</ul>
<hr>
<h2 id="Exchange-means-change-an-unsupervised-single-temporal-change-detection-framework-based-on-intra-and-inter-image-patch-exchange"><a href="#Exchange-means-change-an-unsupervised-single-temporal-change-detection-framework-based-on-intra-and-inter-image-patch-exchange" class="headerlink" title="Exchange means change: an unsupervised single-temporal change detection framework based on intra- and inter-image patch exchange"></a>Exchange means change: an unsupervised single-temporal change detection framework based on intra- and inter-image patch exchange</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00689">http://arxiv.org/abs/2310.00689</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chenhongruixuan/i3pe">https://github.com/chenhongruixuan/i3pe</a></li>
<li>paper_authors: Hongruixuan Chen, Jian Song, Chen Wu, Bo Du, Naoto Yokoya</li>
<li>for: 这个研究旨在提出一个无监控、无标注的单时间变化检测 Framework，以实现单时间 remote sensing 图像上的变化检测。</li>
<li>methods: 这个 Framework 使用了内部和外部图像块交换 (I3PE) 方法，通过交换内部图像块和外部图像块，从单时间图像中生成 pseudo-bi-temporal 图像组和变化标签。</li>
<li>results: 实验结果显示，I3PE 可以超过表现最佳方法的代表无监控方法，实现 F1 值提升约 10.65% 和 6.99%。此外，I3PE 可以在单监控和半监控情况下提高变化检测器的性能。<details>
<summary>Abstract</summary>
Change detection (CD) is a critical task in studying the dynamics of ecosystems and human activities using multi-temporal remote sensing images. While deep learning has shown promising results in CD tasks, it requires a large number of labeled and paired multi-temporal images to achieve high performance. Pairing and annotating large-scale multi-temporal remote sensing images is both expensive and time-consuming. To make deep learning-based CD techniques more practical and cost-effective, we propose an unsupervised single-temporal CD framework based on intra- and inter-image patch exchange (I3PE). The I3PE framework allows for training deep change detectors on unpaired and unlabeled single-temporal remote sensing images that are readily available in real-world applications. The I3PE framework comprises four steps: 1) intra-image patch exchange method is based on an object-based image analysis method and adaptive clustering algorithm, which generates pseudo-bi-temporal image pairs and corresponding change labels from single-temporal images by exchanging patches within the image; 2) inter-image patch exchange method can generate more types of land-cover changes by exchanging patches between images; 3) a simulation pipeline consisting of several image enhancement methods is proposed to simulate the radiometric difference between pre- and post-event images caused by different imaging conditions in real situations; 4) self-supervised learning based on pseudo-labels is applied to further improve the performance of the change detectors in both unsupervised and semi-supervised cases. Extensive experiments on two large-scale datasets demonstrate that I3PE outperforms representative unsupervised approaches and achieves F1 value improvements of 10.65% and 6.99% to the SOTA method. Moreover, I3PE can improve the performance of the ... (see the original article for full abstract)
</details>
<details>
<summary>摘要</summary>
Change detection (CD) 是生态系统和人类活动研究中的关键任务，使用多时间 remote sensing 图像进行研究。深度学习 已经在 CD 任务中表现出色，但它需要大量标注和对应的多时间图像来达到高性能。对于大规模多时间 remote sensing 图像的对应和标注是非常昂贵和时间消耗的。为了使深度学习 基于 CD 技术更实用和成本效果，我们提出了一个无监督单时 CD 框架，基于内部和外部图像块交换（I3PE）。I3PE 框架包括四个步骤：1. 内部图像块交换方法基于 объек 基于 image 分析方法和自适应聚类算法，通过在图像中交换块来生成 pseudo-bi-temporal 图像对和相应的变化标签。2. 外部图像块交换方法可以生成更多的土地覆盖变化类型，通过在图像之间交换块。3. 我们提出了一个模拟管道，包括多种图像提升方法，以模拟在实际情况下的 радиометрические差异。4. 我们采用了自动标注的自我超vised 学习方法，以进一步提高 CD 检测器的性能。我们在两个大规模数据集上进行了广泛的实验，并证明 I3PE 可以在无监督和半监督情况下超越代表性的无监督方法，并在 SOTA 方法上实现 F1 值提升率为 10.65% 和 6.99%。此外，I3PE 还可以提高 CD 检测器的性能。
</details></li>
</ul>
<hr>
<h2 id="The-Robots-are-Here-Navigating-the-Generative-AI-Revolution-in-Computing-Education"><a href="#The-Robots-are-Here-Navigating-the-Generative-AI-Revolution-in-Computing-Education" class="headerlink" title="The Robots are Here: Navigating the Generative AI Revolution in Computing Education"></a>The Robots are Here: Navigating the Generative AI Revolution in Computing Education</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00658">http://arxiv.org/abs/2310.00658</a></li>
<li>repo_url: None</li>
<li>paper_authors: James Prather, Paul Denny, Juho Leinonen, Brett A. Becker, Ibrahim Albluwi, Michelle Craig, Hieke Keuning, Natalie Kiesler, Tobias Kohn, Andrew Luxton-Reilly, Stephen MacNeil, Andrew Peterson, Raymond Pettit, Brent N. Reeves, Jaromir Savelka</li>
<li>for: 这份工作组报告旨在探讨大语言模型（LLMs）在计算教育中的应用和挑战，以及如何适应和利用这些新技术。</li>
<li>methods: 本报告使用Literature Review和论坛调查来探讨LLMs在计算教育中的应用，并从22名计算教育专家的深入采访中收集了实践经验。</li>
<li>results: 本报告的主要结论是：LLMs在计算教育中的应用可以提高学生的学习效果和创新能力，但也存在一些伦理和教学方法的挑战。同时，现有的LLMs在计算教育领域的性能水平在不断提高。<details>
<summary>Abstract</summary>
Recent advancements in artificial intelligence (AI) are fundamentally reshaping computing, with large language models (LLMs) now effectively being able to generate and interpret source code and natural language instructions. These emergent capabilities have sparked urgent questions in the computing education community around how educators should adapt their pedagogy to address the challenges and to leverage the opportunities presented by this new technology. In this working group report, we undertake a comprehensive exploration of LLMs in the context of computing education and make five significant contributions. First, we provide a detailed review of the literature on LLMs in computing education and synthesise findings from 71 primary articles. Second, we report the findings of a survey of computing students and instructors from across 20 countries, capturing prevailing attitudes towards LLMs and their use in computing education contexts. Third, to understand how pedagogy is already changing, we offer insights collected from in-depth interviews with 22 computing educators from five continents who have already adapted their curricula and assessments. Fourth, we use the ACM Code of Ethics to frame a discussion of ethical issues raised by the use of large language models in computing education, and we provide concrete advice for policy makers, educators, and students. Finally, we benchmark the performance of LLMs on various computing education datasets, and highlight the extent to which the capabilities of current models are rapidly improving. Our aim is that this report will serve as a focal point for both researchers and practitioners who are exploring, adapting, using, and evaluating LLMs and LLM-based tools in computing classrooms.
</details>
<details>
<summary>摘要</summary>
Recent advancements in artificial intelligence (AI) are fundamentally reshaping computing, with large language models (LLMs) now effectively being able to generate and interpret source code and natural language instructions. These emergent capabilities have sparked urgent questions in the computing education community around how educators should adapt their pedagogy to address the challenges and to leverage the opportunities presented by this new technology. In this working group report, we undertake a comprehensive exploration of LLMs in the context of computing education and make five significant contributions. First, we provide a detailed review of the literature on LLMs in computing education and synthesize findings from 71 primary articles. Second, we report the findings of a survey of computing students and instructors from across 20 countries, capturing prevailing attitudes towards LLMs and their use in computing education contexts. Third, to understand how pedagogy is already changing, we offer insights collected from in-depth interviews with 22 computing educators from five continents who have already adapted their curricula and assessments. Fourth, we use the ACM Code of Ethics to frame a discussion of ethical issues raised by the use of large language models in computing education, and we provide concrete advice for policy makers, educators, and students. Finally, we benchmark the performance of LLMs on various computing education datasets, and highlight the extent to which the capabilities of current models are rapidly improving. Our aim is that this report will serve as a focal point for both researchers and practitioners who are exploring, adapting, using, and evaluating LLMs and LLM-based tools in computing classrooms.
</details></li>
</ul>
<hr>
<h2 id="LEGO-Prover-Neural-Theorem-Proving-with-Growing-Libraries"><a href="#LEGO-Prover-Neural-Theorem-Proving-with-Growing-Libraries" class="headerlink" title="LEGO-Prover: Neural Theorem Proving with Growing Libraries"></a>LEGO-Prover: Neural Theorem Proving with Growing Libraries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00656">http://arxiv.org/abs/2310.00656</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wiio12/LEGO-Prover">https://github.com/wiio12/LEGO-Prover</a></li>
<li>paper_authors: Haiming Wang, Huajian Xin, Chuanyang Zheng, Lin Li, Zhengying Liu, Qingxing Cao, Yinya Huang, Jing Xiong, Han Shi, Enze Xie, Jian Yin, Zhenguo Li, Heng Liao, Xiaodan Liang</li>
<li>for: This paper aims to improve the ability of large language models (LLMs) to prove mathematical theorems by employing a growing skill library containing verified lemmas as skills.</li>
<li>methods: The proposed method, called LEGO-Prover, constructs the proof modularly and uses existing skills retrieved from the library to augment the capability of LLMs. The skills are further evolved by prompting an LLM to enrich the library on another scale.</li>
<li>results: The proposed method advances the state-of-the-art pass rate on miniF2F-valid and miniF2F-test, and generates over 20,000 skills (theorems&#x2F;lemmas) that are added to the growing library. The ablation study shows that these newly added skills are indeed helpful for proving theorems, resulting in an improvement from a success rate of 47.1% to 50.4%.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文的目的是使大型自然语言模型（LLM）能够更好地证明数学定理，通过使用增长的技能库，这个库包含已验证的证明。</li>
<li>methods: 提议的方法是LEGO-Prover，它将证明构造为模块化的方式，使用已存在的技能库中的技能来增强LLM的能力。这些技能还会在证明过程中进行进一步的演化，以便在另一个尺度上增强库。</li>
<li>results: 提议的方法提高了miniF2F-valid和miniF2F-test的状态前的通过率，并生成了超过20,000个技能（定理&#x2F;证明），这些技能被加入了增长的库中。我们的减少研究表明，这些新增的技能确实对证明定理有帮助，从47.1%提高到50.4%。我们还发布了我们的代码和所有生成的技能。<details>
<summary>Abstract</summary>
Despite the success of large language models (LLMs), the task of theorem proving still remains one of the hardest reasoning tasks that is far from being fully solved. Prior methods using language models have demonstrated promising results, but they still struggle to prove even middle school level theorems. One common limitation of these methods is that they assume a fixed theorem library during the whole theorem proving process. However, as we all know, creating new useful theorems or even new theories is not only helpful but crucial and necessary for advancing mathematics and proving harder and deeper results. In this work, we present LEGO-Prover, which employs a growing skill library containing verified lemmas as skills to augment the capability of LLMs used in theorem proving. By constructing the proof modularly, LEGO-Prover enables LLMs to utilize existing skills retrieved from the library and to create new skills during the proving process. These skills are further evolved (by prompting an LLM) to enrich the library on another scale. Modular and reusable skills are constantly added to the library to enable tackling increasingly intricate mathematical problems. Moreover, the learned library further bridges the gap between human proofs and formal proofs by making it easier to impute missing steps. LEGO-Prover advances the state-of-the-art pass rate on miniF2F-valid (48.0% to 57.0%) and miniF2F-test (45.5% to 47.1%). During the proving process, LEGO-Prover also manages to generate over 20,000 skills (theorems/lemmas) and adds them to the growing library. Our ablation study indicates that these newly added skills are indeed helpful for proving theorems, resulting in an improvement from a success rate of 47.1% to 50.4%. We also release our code and all the generated skills.
</details>
<details>
<summary>摘要</summary>
尽管大型语言模型（LLM）取得了成功，但 theorem proving 仍然是一项非常困难的推理任务，尚未被完全解决。先前的方法使用语言模型已经取得了有望的结果，但它们仍然无法证明中学水平的定理。一个常见的限制是这些方法假设定量定理库在整个定理证明过程中保持不变。然而，我们所知道，创造新有用的定理或新的理论是不仅有帮助作用，而且是必要和必要的，以前进 mathematics 和证明更深入的结果。在这项工作中，我们提出了 LEGO-Prover，它使用增长的技能库，其中包含验证的证明为技能来增强 LLMS 在定理证明中的能力。通过构建证明为模块，LEGO-Prover 让 LLMS 可以在证明过程中使用现有的技能库中的技能，以及在证明过程中创建新的技能。这些技能被进一步演化（通过提示 LLMS），以拓展库的规模。我们不断增加可重用的技能，以便解决越来越复杂的数学问题。此外，学习的库还使得人类证明和正式证明之间的差距变得更小，使得补充缺失的步骤更加容易。LEGO-Prover 提高了 miniF2F-valid 和 miniF2F-test 的通过率（48.0% 到 57.0%）和 miniF2F-test 的成功率（45.5% 到 47.1%）。在证明过程中，LEGO-Prover 还生成了超过 20,000 个技能（定理/证明），并将它们添加到增长的库中。我们的剥离研究表明，这些新增的技能确实对于证明定理有帮助，导致成功率从 47.1% 提高到 50.4%。我们还发布了我们的代码和所有生成的技能。
</details></li>
</ul>
<hr>
<h2 id="Reformulating-Vision-Language-Foundation-Models-and-Datasets-Towards-Universal-Multimodal-Assistants"><a href="#Reformulating-Vision-Language-Foundation-Models-and-Datasets-Towards-Universal-Multimodal-Assistants" class="headerlink" title="Reformulating Vision-Language Foundation Models and Datasets Towards Universal Multimodal Assistants"></a>Reformulating Vision-Language Foundation Models and Datasets Towards Universal Multimodal Assistants</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00653">http://arxiv.org/abs/2310.00653</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thunlp/muffin">https://github.com/thunlp/muffin</a></li>
<li>paper_authors: Tianyu Yu, Jinyi Hu, Yuan Yao, Haoye Zhang, Yue Zhao, Chongyi Wang, Shan Wang, Yinxv Pan, Jiao Xue, Dahai Li, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun</li>
<li>for: 这个论文主要是为了提出一种新的视觉语言模型框架和 Multimodal 指令训练数据集，以提高现有的 Multimodal 语言模型的性能。</li>
<li>methods: 这个论文使用了一种称为 Muffin 的新框架，该框架直接使用预训练的视觉语言模型来连接视觉模块和语言模型，而不需要额外的特征Alignment预训练。此外，该论文还提出了一个名为 UniMM-Chat 的新数据集，该数据集通过将不同任务的数据集融合而成，以生成高质量和多样化的 Multimodal 指令。</li>
<li>results: 实验结果表明，Muffin 框架和 UniMM-Chat 数据集可以提高 Multimodal 语言模型的性能，并且超越了现有的状态机器人模型 like LLaVA 和 InstructBLIP。<details>
<summary>Abstract</summary>
Recent Multimodal Large Language Models (MLLMs) exhibit impressive abilities to perceive images and follow open-ended instructions. The capabilities of MLLMs depend on two crucial factors: the model architecture to facilitate the feature alignment of visual modules and large language models; the multimodal instruction tuning datasets for human instruction following. (i) For the model architecture, most existing models introduce an external bridge module to connect vision encoders with language models, which needs an additional feature-alignment pre-training. In this work, we discover that compact pre-trained vision language models can inherently serve as ``out-of-the-box'' bridges between vision and language. Based on this, we propose Muffin framework, which directly employs pre-trained vision-language models to act as providers of visual signals. (ii) For the multimodal instruction tuning datasets, existing methods omit the complementary relationship between different datasets and simply mix datasets from different tasks. Instead, we propose UniMM-Chat dataset which explores the complementarities of datasets to generate 1.1M high-quality and diverse multimodal instructions. We merge information describing the same image from diverse datasets and transforms it into more knowledge-intensive conversation data. Experimental results demonstrate the effectiveness of the Muffin framework and UniMM-Chat dataset. Muffin achieves state-of-the-art performance on a wide range of vision-language tasks, significantly surpassing state-of-the-art models like LLaVA and InstructBLIP. Our model and dataset are all accessible at https://github.com/thunlp/muffin.
</details>
<details>
<summary>摘要</summary>
最近的多模态大语言模型（MLLM）展现出了惊人的图像识别和开放式指令遵从能力。MLLM的能力取决于两个关键因素：模型架构来实现视觉模块的特征对应，以及多模态指令调整数据集来训练人类指令遵从。在这种情况下，我们发现了一种``出团''的解决方案：使用预训练的视觉语言模型作为视觉信号的提供者。基于这一点，我们提出了甜甜干涯（Muffin）框架，直接employs预训练的视觉语言模型来处理视觉信号。其次，我们发现现有的多模态指令调整数据集忽略了不同任务之间的补偿关系，而是将不同任务的数据集混合在一起。相反，我们提出了UniMM-Chat数据集，它探索不同任务之间的补偿关系，并将这些数据集转化为更加知识充沛的对话数据。实验结果表明甜甜干涯框架和UniMM-Chat数据集的效果。甜甜干涯在各种视觉语言任务上达到了状态之arte的表现，significantly超越了状态之arte的模型 like LLaVA和InstructBLIP。我们的模型和数据集都可以在https://github.com/thunlp/muffin中下载。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Solver-Framework-for-Dynamic-Strategy-Selection-in-Large-Language-Model-Reasoning"><a href="#Adaptive-Solver-Framework-for-Dynamic-Strategy-Selection-in-Large-Language-Model-Reasoning" class="headerlink" title="Adaptive-Solver Framework for Dynamic Strategy Selection in Large Language Model Reasoning"></a>Adaptive-Solver Framework for Dynamic Strategy Selection in Large Language Model Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01446">http://arxiv.org/abs/2310.01446</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianpeng Zhou, Wanjun Zhong, Yanlin Wang, Jiahai Wang</li>
<li>for: 本研究旨在提高大语言模型（LLM）在复杂理解任务中的表现，并适应实际问题的多样性。</li>
<li>methods: 本研究提出了一种适应性解决框架，该框架可以根据问题的复杂性进行灵活的调整。具体来说，该框架包括两个主要模块：初始评估模块和后续适应模块。在后续适应模块中，研究者采用了三种适应策略：（1）模型适应策略：根据问题的复杂性，选择合适的大语言模型；（2）提示方法适应策略：根据问题的特点，选择合适的提示方法；（3）归纳粒度适应策略：根据问题的复杂性，进行细化的问题分解。</li>
<li>results: 实验结果显示，提示方法适应策略和归纳粒度适应策略在所有任务中均提高了表现，而模型适应策略可以减少API成本（最多50%），同时保持高水平的表现。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are showcasing impressive ability in handling complex reasoning tasks. In real-world situations, problems often span a spectrum of complexities. Humans inherently adjust their problem-solving approaches based on task complexity. However, most methodologies that leverage LLMs tend to adopt a uniform approach: utilizing consistent models, prompting methods, and degrees of problem decomposition, regardless of the problem complexity. Inflexibility of them can bring unnecessary computational overhead or sub-optimal performance. To address this problem, we introduce an Adaptive-Solver framework. It strategically modulates solving strategies based on the difficulties of the problems. Given an initial solution, the framework functions with two primary modules. The initial evaluation module assesses the adequacy of the current solution. If improvements are needed, the subsequent adaptation module comes into play. Within this module, three key adaptation strategies are employed: (1) Model Adaptation: Switching to a stronger LLM when a weaker variant is inadequate. (2) Prompting Method Adaptation: Alternating between different prompting techniques to suit the problem's nuances. (3) Decomposition Granularity Adaptation: Breaking down a complex problem into more fine-grained sub-questions to enhance solvability. Through such dynamic adaptations, our framework not only enhances computational efficiency but also elevates the overall performance. This dual-benefit ensures both the efficiency of the system for simpler tasks and the precision required for more complex questions. Experimental results from complex reasoning tasks reveal that the prompting method adaptation and decomposition granularity adaptation enhance performance across all tasks. Furthermore, the model adaptation approach significantly reduces API costs (up to 50%) while maintaining superior performance.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）在复杂逻辑任务中表现出色，但在实际情况中，问题 часто处于复杂性spectrum中。人类自然地根据任务复杂性调整问题解决方法，而多数利用LLMs的方法ologies却采用一致的方法：使用一致的模型、提示方法和问题剖析级别，无论问题复杂性如何。这种不灵活性可能会带来不必要的计算开销或低效性。为解决这个问题，我们介绍了一个适应解决器框架。它在给定的解决方案基础上，策略地调整解决方法，以适应问题的复杂度。解决器框架包括两个主要模块：初始评估模块和后续适应模块。初始评估模块评估当前解决方案的妥当性。如果需要改进，后续适应模块就会起到作用。在这个模块中，我们采用了三种适应策略：1. 模型适应：在弱模型无法解决问题时，切换到更强的LLM。2. 提示方法适应：根据问题的特点，采用不同的提示方法。3.  decompositions Granularity适应：将复杂问题 decompositions into更细grained的子问题，以提高可解性。通过这些动态适应策略，我们的框架不 только提高计算效率，还能够保持高度的表现。这种双重优点确保系统在简单任务上的效率，以及复杂任务上的准确性。实验结果表明，提示方法适应和 decompositions Granularity适应在所有任务上提高表现，而模型适应策略可以减少API成本（最多50%），同时保持高度表现。
</details></li>
</ul>
<hr>
<h2 id="WASA-WAtermark-based-Source-Attribution-for-Large-Language-Model-Generated-Data"><a href="#WASA-WAtermark-based-Source-Attribution-for-Large-Language-Model-Generated-Data" class="headerlink" title="WASA: WAtermark-based Source Attribution for Large Language Model-Generated Data"></a>WASA: WAtermark-based Source Attribution for Large Language Model-Generated Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00646">http://arxiv.org/abs/2310.00646</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingtan Wang, Xinyang Lu, Zitong Zhao, Zhongxiang Dai, Chuan-Sheng Foo, See-Kiong Ng, Bryan Kian Hsiang Low</li>
<li>for: 这个论文是为了解决大语言模型（LLM）训练数据的知识产权问题而写的。</li>
<li>methods: 这个论文使用了水印技术（watermarking）来解决知识产权问题。 specifically, it proposes a WAtermarking for Source Attribution (WASA) framework that enables an LLM to generate synthetic texts with embedded watermarks that contain information about their source(s).</li>
<li>results: 该论文通过实验证明，使用WASA框架可以实现有效的源归属和数据来源验证。<details>
<summary>Abstract</summary>
The impressive performances of large language models (LLMs) and their immense potential for commercialization have given rise to serious concerns over the intellectual property (IP) of their training data. In particular, the synthetic texts generated by LLMs may infringe the IP of the data being used to train the LLMs. To this end, it is imperative to be able to (a) identify the data provider who contributed to the generation of a synthetic text by an LLM (source attribution) and (b) verify whether the text data from a data provider has been used to train an LLM (data provenance). In this paper, we show that both problems can be solved by watermarking, i.e., by enabling an LLM to generate synthetic texts with embedded watermarks that contain information about their source(s). We identify the key properties of such watermarking frameworks (e.g., source attribution accuracy, robustness against adversaries), and propose a WAtermarking for Source Attribution (WASA) framework that satisfies these key properties due to our algorithmic designs. Our WASA framework enables an LLM to learn an accurate mapping from the texts of different data providers to their corresponding unique watermarks, which sets the foundation for effective source attribution (and hence data provenance). Extensive empirical evaluations show that our WASA framework achieves effective source attribution and data provenance.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的吸引人表现和其商业化潜力已经引起了训练数据知识产权（IP）的严重担忧。具体来说， LLM 生成的 sintetic 文本可能会侵犯训练数据的 IP。因此，必须能够（a）确定 LLM 生成 sintetic 文本中的数据提供者（源归属），以及（b）验证数据提供者的文本数据是否被用来训练 LLM。在这篇论文中，我们表明了这两个问题可以通过水印来解决，即使 LLM 生成 sintetic 文本时包含水印，其中包含了文本的来源信息。我们标识了水印框架的关键属性（如源归属精度和对抗攻击者的Robustness），并提出了一个基于 WASA 框架的水印方法，该方法满足这些关键属性。我们的 WASA 框架使得 LLM 可以学习不同数据提供者的文本和它们对应的唯一水印之间的准确映射，这为有效的源归属（以及数据来源）提供了基础。我们的 empirical 评估表明，我们的 WASA 框架可以实现有效的源归属和数据来源识别。
</details></li>
</ul>
<hr>
<h2 id="From-Bandits-Model-to-Deep-Deterministic-Policy-Gradient-Reinforcement-Learning-with-Contextual-Information"><a href="#From-Bandits-Model-to-Deep-Deterministic-Policy-Gradient-Reinforcement-Learning-with-Contextual-Information" class="headerlink" title="From Bandits Model to Deep Deterministic Policy Gradient, Reinforcement Learning with Contextual Information"></a>From Bandits Model to Deep Deterministic Policy Gradient, Reinforcement Learning with Contextual Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00642">http://arxiv.org/abs/2310.00642</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhendong Shi, Xiaoli Wei, Ercan E. Kuruoglu</li>
<li>for: 本研究旨在解决Sequential process中的各种复杂环境下的投资决策问题，使用了两种方法来增强遗传学习的性能：contextual Thompson sampling和 reinforcement learning under supervision。</li>
<li>methods: 本研究使用了遗传学习和CPPI（常数比例资产保险），并将其与DDPG（深度决定策函数优化）相结合，以加速遗传学习的迭代过程，寻找最佳策略。</li>
<li>results: 实验结果显示，使用了上述两种方法可以加速遗传学习的迭代过程，并且可以快速获得最佳策略。<details>
<summary>Abstract</summary>
The problem of how to take the right actions to make profits in sequential process continues to be difficult due to the quick dynamics and a significant amount of uncertainty in many application scenarios. In such complicated environments, reinforcement learning (RL), a reward-oriented strategy for optimum control, has emerged as a potential technique to address this strategic decision-making issue. However, reinforcement learning also has some shortcomings that make it unsuitable for solving many financial problems, excessive resource consumption, and inability to quickly obtain optimal solutions, making it unsuitable for quantitative trading markets. In this study, we use two methods to overcome the issue with contextual information: contextual Thompson sampling and reinforcement learning under supervision which can accelerate the iterations in search of the best answer. In order to investigate strategic trading in quantitative markets, we merged the earlier financial trading strategy known as constant proportion portfolio insurance (CPPI) into deep deterministic policy gradient (DDPG). The experimental results show that both methods can accelerate the progress of reinforcement learning to obtain the optimal solution.
</details>
<details>
<summary>摘要</summary>
“对于续行过程中获利的选择问题，由于动态变化快速且在许多应用场景中存在许多不确定性，这个问题仍然具有困难。在这些复杂的环境中，奖励学习（RL），一种奖励控制的整合策略，已经被视为一种解决此策略决策问题的技术。然而，奖励学习也有一些缺陷，使其不适合解决许多金融问题，包括资源耗用量过大和寻找最佳解答的速度太慢，这使其不适合量化交易市场。在这篇研究中，我们使用了两种方法来突破问题，即在上下文信息中进行奖励探索和奖励学习的监督。为了研究量化交易的战略问题，我们将以前的金融交易策略known as constant proportion portfolio insurance (CPPI)与深度决定性策略gradient (DDPG) 混合。实验结果显示，这两种方法可以加速奖励学习的进程，以取得最佳解答。”Note: The translation is done using Google Translate and may not be perfect. Please note that the translation is done in a simplified Chinese, if you need a traditional Chinese translation, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Engineering-using-Large-Language-Models"><a href="#Knowledge-Engineering-using-Large-Language-Models" class="headerlink" title="Knowledge Engineering using Large Language Models"></a>Knowledge Engineering using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00637">http://arxiv.org/abs/2310.00637</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Bradley P. Allen, Lise Stork, Paul Groth</li>
<li>for: 这篇论文旨在探讨大语言模型在知识工程中的潜在作用，以及如何将其与传统的符号知识系统融合。</li>
<li>methods: 该论文提出了两个中心方向：1）创建混合神经符号知识系统；2）在自然语言中进行知识工程。</li>
<li>results: 该论文提出了一些关键的未解决问题，以便进一步探讨这两个方向。<details>
<summary>Abstract</summary>
Knowledge engineering is a discipline that focuses on the creation and maintenance of processes that generate and apply knowledge. Traditionally, knowledge engineering approaches have focused on knowledge expressed in formal languages. The emergence of large language models and their capabilities to effectively work with natural language, in its broadest sense, raises questions about the foundations and practice of knowledge engineering. Here, we outline the potential role of LLMs in knowledge engineering, identifying two central directions: 1) creating hybrid neuro-symbolic knowledge systems; and 2) enabling knowledge engineering in natural language. Additionally, we formulate key open research questions to tackle these directions.
</details>
<details>
<summary>摘要</summary>
知识工程是一个领域，它关注创建和维护生成和应用知识的过程。传统上，知识工程方法都是关注正式语言表达的知识。然而，大型自然语言模型的出现和它们可以有效地与自然语言进行交互，使得知识工程的基础和实践面临到了新的问题。以下是我们对大型自然语言模型在知识工程中的潜在作用的描述，以及两个中心方向：1. 创建混合神经符号知识系统；2. 实现自然语言知识工程。此外，我们还提出了关键的开放研究问题，以便解决这两个方向。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-Robustness-and-Safety-of-2D-and-3D-Deep-Learning-Models-Against-Adversarial-Attacks"><a href="#A-Survey-of-Robustness-and-Safety-of-2D-and-3D-Deep-Learning-Models-Against-Adversarial-Attacks" class="headerlink" title="A Survey of Robustness and Safety of 2D and 3D Deep Learning Models Against Adversarial Attacks"></a>A Survey of Robustness and Safety of 2D and 3D Deep Learning Models Against Adversarial Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00633">http://arxiv.org/abs/2310.00633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanjie Li, Bin Xie, Songtao Guo, Yuanyuan Yang, Bin Xiao</li>
<li>for: 本研究旨在提高深度学习模型的可靠性和安全性，对抗训练时的敏感攻击和应用场景中的物理攻击。</li>
<li>methods: 本文首先构建了不同角度的威胁模型，然后对最新的2D和3D敏感攻击进行了全面的文献综述。同时，本文还扩展了敏感示例的概念，涵盖了不同类型的攻击方法。</li>
<li>results: 本文系统性地Investigated 3D模型对各种敏感攻击的 robustness，并发现了许多现有的攻击方法。此外，本文还发现了物理攻击可能导致安全风险的问题。最后，本文Summarize 现有的主流话题，预测未来研究的挑战和方向，以帮助建立可靠的AI系统。<details>
<summary>Abstract</summary>
Benefiting from the rapid development of deep learning, 2D and 3D computer vision applications are deployed in many safe-critical systems, such as autopilot and identity authentication. However, deep learning models are not trustworthy enough because of their limited robustness against adversarial attacks. The physically realizable adversarial attacks further pose fatal threats to the application and human safety. Lots of papers have emerged to investigate the robustness and safety of deep learning models against adversarial attacks. To lead to trustworthy AI, we first construct a general threat model from different perspectives and then comprehensively review the latest progress of both 2D and 3D adversarial attacks. We extend the concept of adversarial examples beyond imperceptive perturbations and collate over 170 papers to give an overview of deep learning model robustness against various adversarial attacks. To the best of our knowledge, we are the first to systematically investigate adversarial attacks for 3D models, a flourishing field applied to many real-world applications. In addition, we examine physical adversarial attacks that lead to safety violations. Last but not least, we summarize present popular topics, give insights on challenges, and shed light on future research on trustworthy AI.
</details>
<details>
<summary>摘要</summary>
利用深度学习快速发展，2D和3D计算机视觉应用在许多安全关键系统中部署，如自动驾驶和身份验证。然而，深度学习模型没有够的可靠性，因为它们对骚动攻击有限制的Robustness。物理可行的骚动攻击更加 pose 致命的威胁，对应用和人类安全构成了 fatal 威胁。许多论文已经出现，以 investigate 深度学习模型对骚动攻击的Robustness和安全性。为了带来可靠的 AI，我们首先从不同的角度构建一个通用威胁模型，然后对最新的2D和3D骚动攻击进行全面的回顾。我们将 adversarial 例外扩展到不可见的扰动，并将超过 170 篇论文综述深度学习模型对不同骚动攻击的Robustness。我们认为是首次系统地调查3D模型对骚动攻击的Robustness，这是应用于许多实际应用的蓬勃领域。此外，我们还检查了物理骚动攻击导致的安全违反。最后，我们 summarize 当前流行的话题，提供挑战的视角，并照明未来可靠 AI 的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Intelligent-Client-Selection-for-Federated-Learning-using-Cellular-Automata"><a href="#Intelligent-Client-Selection-for-Federated-Learning-using-Cellular-Automata" class="headerlink" title="Intelligent Client Selection for Federated Learning using Cellular Automata"></a>Intelligent Client Selection for Federated Learning using Cellular Automata</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00627">http://arxiv.org/abs/2310.00627</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nikopavl4/ca_client_selection">https://github.com/nikopavl4/ca_client_selection</a></li>
<li>paper_authors: Nikolaos Pavlidis, Vasileios Perifanis, Theodoros Panagiotis Chatzinikolaou, Georgios Ch. Sirakoulis, Pavlos S. Efraimidis</li>
<li>for: 这个研究旨在提出一个基于自动化机器学习的联盟学习（Federated Learning）客户端选择算法，以提高隐私保护和减少延迟，并且能够适应实际应用中的快速变化环境。</li>
<li>methods: 本研究提出了一个基于细胞自动机（Cellular Automata）的客户端选择算法（CA-CS），它考虑了参与客户端的 Computational Resources 和通信能力，并且考虑了客户端之间的互动，以选择最适合的客户端进行联盟学习过程。</li>
<li>results: 根据实验结果显示，CA-CS 可以与随机选择方法相比，具有与随机选择方法相似的准确性，而且可以快速避免高延迟的客户端。<details>
<summary>Abstract</summary>
Federated Learning (FL) has emerged as a promising solution for privacy-enhancement and latency minimization in various real-world applications, such as transportation, communications, and healthcare. FL endeavors to bring Machine Learning (ML) down to the edge by harnessing data from million of devices and IoT sensors, thus enabling rapid responses to dynamic environments and yielding highly personalized results. However, the increased amount of sensors across diverse applications poses challenges in terms of communication and resource allocation, hindering the participation of all devices in the federated process and prompting the need for effective FL client selection. To address this issue, we propose Cellular Automaton-based Client Selection (CA-CS), a novel client selection algorithm, which leverages Cellular Automata (CA) as models to effectively capture spatio-temporal changes in a fast-evolving environment. CA-CS considers the computational resources and communication capacity of each participating client, while also accounting for inter-client interactions between neighbors during the client selection process, enabling intelligent client selection for online FL processes on data streams that closely resemble real-world scenarios. In this paper, we present a thorough evaluation of the proposed CA-CS algorithm using MNIST and CIFAR-10 datasets, while making a direct comparison against a uniformly random client selection scheme. Our results demonstrate that CA-CS achieves comparable accuracy to the random selection approach, while effectively avoiding high-latency clients.
</details>
<details>
<summary>摘要</summary>
通用学习（FL）已经出现为保护隐私和减少延迟的有力解决方案，在交通、通信和医疗等实际应用中得到广泛应用。FL目的是将机器学习（ML）带到边缘，通过收集数百万个设备和物联网感知器的数据，以实现快速应对动态环境和提供高度个性化结果。然而，在多个应用中的多种感知器上增加了通信和资源分配的挑战，这会阻碍所有设备参与联邦过程，并提高效果的联邦学习客户端选择的需求。为解决这个问题，我们提出了基于Cellular Automata（CA）的客户端选择算法（CA-CS），利用CA模型来有效地捕捉快速发展环境中的空间-时间变化。CA-CS考虑每个参与联邦学习的客户端的计算资源和通信能力，同时也考虑客户端之间的互动，以实现在线联邦学习过程中智能客户端选择。在这篇论文中，我们对提出的CA-CS算法进行了住ehour评估，使用MNIST和CIFAR-10数据集，并对Random client selection scheme进行了直接比较。我们的结果表明，CA-CS可以与随机选择方案具有相同的准确率，同时有效地避免高延迟客户端。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Adaptation-with-Hypernetworks-for-Few-shot-Molecular-Property-Prediction"><a href="#Hierarchical-Adaptation-with-Hypernetworks-for-Few-shot-Molecular-Property-Prediction" class="headerlink" title="Hierarchical Adaptation with Hypernetworks for Few-shot Molecular Property Prediction"></a>Hierarchical Adaptation with Hypernetworks for Few-shot Molecular Property Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00614">http://arxiv.org/abs/2310.00614</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiguang Wu, Yaqing Wang, Quanming Yao</li>
<li>for: 这篇论文的目的是提出一种基于卷积神经网络的层次适应机制，以解决生物医学应用中的分类问题。</li>
<li>methods: 该论文提出了一种基于卷积神经网络的层次适应机制，包括在编码器中选择性地适应参数，以及在预测器中对分子的适应进行层次适应。</li>
<li>results: 该论文的实验结果显示，基于层次适应机制的方法可以在几拟shot学习问题中取得state-of-the-art的性能。<details>
<summary>Abstract</summary>
Molecular property prediction (MPP) is important in biomedical applications, which naturally suffers from a lack of labels, thus forming a few-shot learning problem. State-of-the-art approaches are usually based on gradient-based meta learning strategy, which ignore difference in model parameter and molecule's learning difficulty. To address above problems, we propose a novel hierarchical adaptation mechanism for few-shot MPP (HiMPP). The model follows a encoder-predictor framework. First, to make molecular representation property-adaptive, we selectively adapt encoder's parameter by designing a hypernetwork to modulate node embeddings during message propagation. Next, we make molecule-level adaptation by design another hypernetwork, which assigns larger propagating steps for harder molecules in predictor. In this way, molecular representation is transformed by HiMPP hierarchically from property-level to molecular level. Extensive results show that HiMPP obtains the state-of-the-art performance in few-shot MPP problems, and our proposed hierarchical adaptation mechanism is rational and effective.
</details>
<details>
<summary>摘要</summary>
молекулярная свойство предсказание（MPP）在生物医学应用中具有重要意义，但受到标签缺乏的限制，形成了几个shot学习问题。现状的方法通常基于梯度based meta学习策略，忽略了模型参数和分子学习难度之间的差异。为解决上述问题，我们提出了一种新的层次适应机制 для几个shot MPP（HiMPP）。模型采用encoder-predictor框架，首先使分子表示性能adaptive，通过设计一个权重网络来修饰节点嵌入的消息传播过程中的模型参数。然后，我们又使用另一个权重网络，将更难的分子 assign 更大的传播步长，从而使分子表示被HiMPP层次适应。这种方法使得分子表示被HiMPP层次适应，从属性层次适应到分子层次适应。我们的实验结果表明，HiMPP在几个shot MPP问题中获得了状态计算机科学中的最佳性能，而我们提出的层次适应机制是合理和有效的。
</details></li>
</ul>
<hr>
<h2 id="Understanding-AI-Cognition-A-Neural-Module-for-Inference-Inspired-by-Human-Memory-Mechanisms"><a href="#Understanding-AI-Cognition-A-Neural-Module-for-Inference-Inspired-by-Human-Memory-Mechanisms" class="headerlink" title="Understanding AI Cognition: A Neural Module for Inference Inspired by Human Memory Mechanisms"></a>Understanding AI Cognition: A Neural Module for Inference Inspired by Human Memory Mechanisms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09297">http://arxiv.org/abs/2310.09297</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zengxyyu/A-neural-module-for-inference-inspired-by-human-memory-mechanisms">https://github.com/zengxyyu/A-neural-module-for-inference-inspired-by-human-memory-mechanisms</a></li>
<li>paper_authors: Xiangyu Zeng, Jie Lin, Piao Hu, Ruizheng Huang, Zhicheng Zhang</li>
<li>for: The paper aims to improve the ability of machines to make sense of current inputs and retain information for relation reasoning and question-answering by proposing a PMI framework inspired by human brain’s memory system and cognitive architectures.</li>
<li>methods: The PMI framework consists of perception, memory, and inference components, with a differentiable competitive write access, working memory, and long-term memory with a higher-order structure. The framework also uses outer product associations to merge working memory with long-term memory and retrieve relevant information from two separate memory origins for associative integration.</li>
<li>results: The paper exploratively applies the PMI framework to improve prevailing Transformers and CNN models on question-answering tasks like bAbI-20k and Sort-of-CLEVR datasets, as well as relation calculation and image classification tasks, and in each case, the PMI enhancements consistently outshine their original counterparts significantly. Visualization analyses reveal that memory consolidation and the interaction and integration of information from diverse memory sources substantially contribute to the model effectiveness on inference tasks.Here’s the format you requested:</li>
<li>for: 论文目标是提高机器对当前输入的理解和保留信息以便关系逻辑和问答。</li>
<li>methods: PMI框架包括感知、记忆和推理组件，具有可 differentiable 竞争写访问，工作记忆和长期记忆，其中长期记忆具有更高级结构以保留更多的积累知识和经验。outer product associations 将工作记忆与长期记忆 merge，并在两个不同的记忆来源之间进行相关的集成。</li>
<li>results: 论文应用 PMI 框架进行 prevailing Transformers 和 CNN 模型的改进，包括 bAbI-20k 和 Sort-of-CLEVR 数据集上的问答任务，以及关系计算和图像分类任务，并在每一个任务上，PMI 改进均以显著的程度超越原始模型。视觉分析表明，记忆整合和多种记忆来源之间的交互和集成对推理任务的效果具有重要作用。<details>
<summary>Abstract</summary>
How humans and machines make sense of current inputs for relation reasoning and question-answering while putting the perceived information into context of our past memories, has been a challenging conundrum in cognitive science and artificial intelligence. Inspired by human brain's memory system and cognitive architectures, we propose a PMI framework that consists of perception, memory and inference components. Notably, the memory module comprises working and long-term memory, with the latter endowed with a higher-order structure to retain more accumulated knowledge and experiences. Through a differentiable competitive write access, current perceptions update working memory, which is later merged with long-term memory via outer product associations, averting memory overflow and minimizing information conflicts. In the inference module, relevant information is retrieved from two separate memory origins and associatively integrated to attain a more comprehensive and precise interpretation of current perceptions. We exploratively apply our PMI to improve prevailing Transformers and CNN models on question-answering tasks like bAbI-20k and Sort-of-CLEVR datasets, as well as relation calculation and image classification tasks, and in each case, our PMI enhancements consistently outshine their original counterparts significantly. Visualization analyses reveal that memory consolidation, along with the interaction and integration of information from diverse memory sources, substantially contributes to the model effectiveness on inference tasks.
</details>
<details>
<summary>摘要</summary>
人们和机器如何对当前输入进行关系理解和问答，将感知信息置入我们过去经验的 контекст，是认知科学和人工智能领域的挑战。我们提出了一个PMI框架，包括感知、记忆和推理组件。特别是记忆模块包括工作记忆和长期记忆，其中后者具有更高级别结构，以保留更多的总知识和经验。通过可 diferenciable 竞争写访问，当前感知更新工作记忆，并 eventually 与长期记忆通过外产品关联相结合，避免记忆溢出和信息冲突。在推理模块中，来自不同记忆来源的相关信息被asso ciatively 集成，以实现更全面和准确的当前感知解释。我们考虑应用PMI来改进现有的Transformers和CNN模型，在问答任务和关系计算任务上，以及图像分类任务上，并在每个任务上，我们的PMI改进都能够显著超越原始模型。视觉分析表明，记忆凝固以及不同记忆来源之间的交互和集成，对推理任务的效果具有重要作用。
</details></li>
</ul>
<hr>
<h2 id="Adapting-LLM-Agents-Through-Communication"><a href="#Adapting-LLM-Agents-Through-Communication" class="headerlink" title="Adapting LLM Agents Through Communication"></a>Adapting LLM Agents Through Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01444">http://arxiv.org/abs/2310.01444</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kuan Wang, Yadong Lu, Michael Santacroce, Yeyun Gong, Chao Zhang, Yelong Shen</li>
<li>for: 这个论文旨在提出一种名为“学习通信”（LTC）的训练方法，帮助大型自然语言模型（LLM） agents 在不需要广泛人类指导下，适应新任务。</li>
<li>methods: 该方法基于 iterative exploration 和 PPO 训练，使得 LLM agents 可以通过与环境和其他代理交互，不断提高自己的能力。</li>
<li>results: 在 ALFWorld、HotpotQA 和 GSM8k 三个数据集上，LTC 方法比基eline 高出 12%、5.1% 和 3.6%  respectively，这些结果表明 LTC 方法在多种领域中具有广泛的应用前景。<details>
<summary>Abstract</summary>
Recent advancements in large language models (LLMs) have shown potential for human-like agents. To help these agents adapt to new tasks without extensive human supervision, we propose the Learning through Communication (LTC) paradigm, a novel training approach enabling LLM agents to improve continuously through interactions with their environments and other agents. Recent advancements in large language models (LLMs) have shown potential for human-like agents. To help these agents adapt to new tasks without extensive human supervision, we propose the Learning through Communication (LTC) paradigm, a novel training approach enabling LLM agents to improve continuously through interactions with their environments and other agents. Through iterative exploration and PPO training, LTC empowers the agent to assimilate short-term experiences into long-term memory. To optimize agent interactions for task-specific learning, we introduce three structured communication patterns: Monologue, Dialogue, and Analogue-tailored for common tasks such as decision-making, knowledge-intensive reasoning, and numerical reasoning. We evaluated LTC on three datasets: ALFWorld (decision-making), HotpotQA (knowledge-intensive reasoning), and GSM8k (numerical reasoning). On ALFWorld, it exceeds the instruction tuning baseline by 12% in success rate. On HotpotQA, LTC surpasses the instruction-tuned LLaMA-7B agent by 5.1% in EM score, and it outperforms the instruction-tuned 9x larger PaLM-62B agent by 0.6%. On GSM8k, LTC outperforms the CoT-Tuning baseline by 3.6% in accuracy. The results showcase the versatility and efficiency of the LTC approach across diverse domains. We will open-source our code to promote further development of the community.
</details>
<details>
<summary>摘要</summary>
最近的大语言模型（LLM）的进步已经表现出人类样式的代理。为了帮助这些代理适应新任务而不需极大的人类指导，我们提议了学习通信（LTC）方法，这是一种新的训练方法，可以让 LLM 代理通过与环境和其他代理的交互来不断改进。通过迭代探索和 PPO 训练，LTC 让代理可以将短期经验转化为长期记忆。为了优化代理之间的交互以掌握任务特定的学习，我们引入了三种结构化的通信模式：假言、对话和数据辅助，这些模式特化于常见的决策、知识激发和数学计算等任务。我们在 ALFWorld、HotpotQA 和 GSM8k 三个 dataset 上评估了 LTC，结果显示 LTC 在Success rate、EM 分数和准确率等方面都表现出优异。这些结果表明 LTC 方法在多种领域中具有广泛的应用前景和高效性。我们将在未来开源代码，以便更多的社区成员参与发展。
</details></li>
</ul>
<hr>
<h2 id="Faithful-Explanations-of-Black-box-NLP-Models-Using-LLM-generated-Counterfactuals"><a href="#Faithful-Explanations-of-Black-box-NLP-Models-Using-LLM-generated-Counterfactuals" class="headerlink" title="Faithful Explanations of Black-box NLP Models Using LLM-generated Counterfactuals"></a>Faithful Explanations of Black-box NLP Models Using LLM-generated Counterfactuals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00603">http://arxiv.org/abs/2310.00603</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yair Gat, Nitay Calderon, Amir Feder, Alexander Chapanin, Amit Sharma, Roi Reichart</li>
<li>for: 提高NLプロセッサの安全性和信任性を确保するための说明の强化</li>
<li>methods: 2种のアプローチを提案します：1つはCF生成アプローチで、具体的なテキスト概念を変更することでCFを生成する方法です。2つ目はマッチングアプローチで、トレーニング时にLLMを使用して特别な拟似的空间を学习する方法です。</li>
<li>results: 実験结果では、CF生成アプローチが非常に效果的ですが、検证时间が高くなるDrawbackがあります。一方、マッチングアプローチは、テスト时间の资源を削减した上で效果的な说明を提供することができます。また、Top-K技术を适用することで、すべてのテストされた方法を超える说明を提供することができます。<details>
<summary>Abstract</summary>
Causal explanations of the predictions of NLP systems are essential to ensure safety and establish trust. Yet, existing methods often fall short of explaining model predictions effectively or efficiently and are often model-specific. In this paper, we address model-agnostic explanations, proposing two approaches for counterfactual (CF) approximation. The first approach is CF generation, where a large language model (LLM) is prompted to change a specific text concept while keeping confounding concepts unchanged. While this approach is demonstrated to be very effective, applying LLM at inference-time is costly. We hence present a second approach based on matching, and propose a method that is guided by an LLM at training-time and learns a dedicated embedding space. This space is faithful to a given causal graph and effectively serves to identify matches that approximate CFs. After showing theoretically that approximating CFs is required in order to construct faithful explanations, we benchmark our approaches and explain several models, including LLMs with billions of parameters. Our empirical results demonstrate the excellent performance of CF generation models as model-agnostic explainers. Moreover, our matching approach, which requires far less test-time resources, also provides effective explanations, surpassing many baselines. We also find that Top-K techniques universally improve every tested method. Finally, we showcase the potential of LLMs in constructing new benchmarks for model explanation and subsequently validate our conclusions. Our work illuminates new pathways for efficient and accurate approaches to interpreting NLP systems.
</details>
<details>
<summary>摘要</summary>
natura  causa 解释 预测 是非常重要的，以确保安全性和建立信任。然而，现有的方法经常无法有效地解释模型预测或有效地适用于不同模型。在这篇论文中，我们提出了两种方法来实现模型无关的解释。首先，我们提出了一种基于生成的方法，使用大型自然语言模型（LLM）在预测时对特定文本概念进行修改，保持干扰因素不变。虽然这种方法具有很高的效果，但是在执行时需要费力。我们因此提出了第二种方法，基于匹配的方法，并提出了一种受 LLM 培训时引导的方法，学习专门的嵌入空间。这个空间忠实于给定的 causal 图，并能够准确地标识符合 CF 的匹配。我们理论上证明，以 Approximating CF 为前提，才能建立 faithful 的解释。我们对我们的方法进行了比较，并将其应用于多个模型，包括具有数百亿参数的 LLM。我们的实验结果表明，CF 生成模型在无关模型中具有非常高的表现，并且我们的匹配方法，需要较少的测试资源，也提供了有效的解释。此外，我们发现 Top-K 技术在所有测试方法中都有优化效果。最后，我们展示了 LLB 的潜在在建立新的解释指标和验证我们的结论。我们的工作揭示了新的高效和准确的方法，用于解释 NLP 系统。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Computational-and-Modeling-Foundation-for-Automatic-Coherence-Assessment"><a href="#A-Novel-Computational-and-Modeling-Foundation-for-Automatic-Coherence-Assessment" class="headerlink" title="A Novel Computational and Modeling Foundation for Automatic Coherence Assessment"></a>A Novel Computational and Modeling Foundation for Automatic Coherence Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00598">http://arxiv.org/abs/2310.00598</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aviya Maimon, Reut Tsarfaty</li>
<li>for: 这篇论文主要针对了自然语言处理（NLP）中的 coherence 评估问题，即文本听起来有意义和连贯的问题。</li>
<li>methods: 该论文提出了一种基于 формаль语言定义的 coherence 评估方法，包括三个条件：cohesion、consistency 和 relevance。这些条件被формализова为不同的计算任务，并假设一个涵盖所有任务的模型会学习出 coherence 评估所需的特征。</li>
<li>results: 在两个人类评分的 benchmark 上进行了实验，结果表明，对于每个任务和总的 coherence 评估来说，使用 joint 模型比使用单个任务模型更好。这些结果表明，该方法可以提供一个强大的基础 для大规模自动 coherence 评估。<details>
<summary>Abstract</summary>
Coherence is an essential property of well-written texts, that refers to the way textual units relate to one another. In the era of generative AI, coherence assessment is essential for many NLP tasks; summarization, generation, long-form question-answering, and more. However, in NLP {coherence} is an ill-defined notion, not having a formal definition or evaluation metrics, that would allow for large-scale automatic and systematic coherence assessment. To bridge this gap, in this work we employ the formal linguistic definition of \citet{Reinhart:1980} of what makes a discourse coherent, consisting of three conditions -- {\em cohesion, consistency} and {\em relevance} -- and formalize these conditions as respective computational tasks. We hypothesize that (i) a model trained on all of these tasks will learn the features required for coherence detection, and that (ii) a joint model for all tasks will exceed the performance of models trained on each task individually. On two benchmarks for coherence scoring rated by humans, one containing 500 automatically-generated short stories and another containing 4k real-world texts, our experiments confirm that jointly training on the proposed tasks leads to better performance on each task compared with task-specific models, and to better performance on assessing coherence overall, compared with strong baselines. We conclude that the formal and computational setup of coherence as proposed here provides a solid foundation for advanced methods of large-scale automatic assessment of coherence.
</details>
<details>
<summary>摘要</summary>
“一致性”是文本写作中非常重要的特性，指的是文本单位之间的关联方式。在生成AI时代，一致性评估成为许多自然语言处理（NLP）任务的重要组成部分，包括概要、生成、长文问答等。但在NLP中，“一致性”是一个不具体定义或评估指标的概念，无法进行大规模自动化和系统化的评估。为了bridging这个差距，在这个工作中，我们运用了实际语言学定义（Reinhart, 1980）所定义的一致性条件，包括“结合”、“一致”和“相关”三个条件，并将这些条件ormal化为各自的计算任务。我们假设（i）一个对所有这些任务进行训练的模型将学习出一致性检测所需的特征，并且（ii）将所有这些任务联合训练的模型会比单独训练的模型表现更好。在人类评分的两个库中，一个包含500个自动生成的短篇故事，另一个包含4000个真实世界文本，我们的实验显示，将所有这些任务联合训练的模型比单独训练的模型表现更好，并且在评估一致性方面表现更好，比单独使用强大的基准模型。我们 conclude that这种以形式和计算为基础的一致性设置提供了一个坚实的基础 для进一步的大规模自动一致性评估。
</details></li>
</ul>
<hr>
<h2 id="Quantum-generative-adversarial-learning-in-photonics"><a href="#Quantum-generative-adversarial-learning-in-photonics" class="headerlink" title="Quantum generative adversarial learning in photonics"></a>Quantum generative adversarial learning in photonics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00585">http://arxiv.org/abs/2310.00585</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yizhi Wang, Shichuan Xue, Yaxuan Wang, Yong Liu, Jiangfang Ding, Weixu Shi, Dongyang Wang, Yingwen Liu, Xiang Fu, Guangyao Huang, Anqi Huang, Mingtang Deng, Junjie Wu</li>
<li>for: 本研究旨在调查 Whether Quantum Generative Adversarial Networks (QGANs) can perform learning tasks on near-term quantum devices usually affected by noise and even defects.</li>
<li>methods: 我们使用了一个可编程的硅量子光学芯片，实验了 QGAN 模型在光学领域中，并研究了噪声和缺陷对其性能的影响。</li>
<li>results: 我们的结果表明，即使Generator的相位调制器中有一半被损坏，或Generator和Discriminator的相位调制器都受到相位噪声达0.04π，QGANs仍然可以生成高质量量子数据，其准确率高于90%。<details>
<summary>Abstract</summary>
Quantum Generative Adversarial Networks (QGANs), an intersection of quantum computing and machine learning, have attracted widespread attention due to their potential advantages over classical analogs. However, in the current era of Noisy Intermediate-Scale Quantum (NISQ) computing, it is essential to investigate whether QGANs can perform learning tasks on near-term quantum devices usually affected by noise and even defects. In this Letter, using a programmable silicon quantum photonic chip, we experimentally demonstrate the QGAN model in photonics for the first time, and investigate the effects of noise and defects on its performance. Our results show that QGANs can generate high-quality quantum data with a fidelity higher than 90\%, even under conditions where up to half of the generator's phase shifters are damaged, or all of the generator and discriminator's phase shifters are subjected to phase noise up to 0.04$\pi$. Our work sheds light on the feasibility of implementing QGANs on NISQ-era quantum hardware.
</details>
<details>
<summary>摘要</summary>
量子生成对抗网络（QGAN），量子计算和机器学习的交叉点，在当今中等规模量子计算（NISQ）时代受到广泛关注，因为它们可能比类比的古典模型具有优势。然而，在NISQ时代的近期量子设备上进行学习任务，受到噪声和瑕疵的影响是必须考虑的。在这封信中，我们使用可编程的硅量子光学芯片实验ally QGAN模型在光学中，并研究噪声和瑕疵对其性能的影响。我们的结果表明，QGAN可以生成高质量量子数据，其准确率高于90%， même under conditions where up to half of the generator's phase shifters are damaged, or all of the generator and discriminator's phase shifters are subjected to phase noise up to 0.04π。我们的工作照明了在NISQ时代量子硬件上实现QGAN的可能性。
</details></li>
</ul>
<hr>
<h2 id="CityFM-City-Foundation-Models-to-Solve-Urban-Challenges"><a href="#CityFM-City-Foundation-Models-to-Solve-Urban-Challenges" class="headerlink" title="CityFM: City Foundation Models to Solve Urban Challenges"></a>CityFM: City Foundation Models to Solve Urban Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00583">http://arxiv.org/abs/2310.00583</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pasquale Balsebre, Weiming Huang, Gao Cong, Yi Li</li>
<li>for: 本研究旨在开发一种基于自适应学习的城市基础模型（CityFM），以便在选定的地理区域内（如城市）进行自动化学习。</li>
<li>methods: CityFM 基于开源地理数据（如 OpenStreetMap）进行自我超vision，通过对不同类型实体（如路径、建筑物、区域）的多模式信息进行拟合，生成高质量的基础表示。</li>
<li>results: 对于路、建筑物和区域等下游任务，CityFM 的表示能够超过或与特定应用程序的基elines匹配。<details>
<summary>Abstract</summary>
Pre-trained Foundation Models (PFMs) have ushered in a paradigm-shift in Artificial Intelligence, due to their ability to learn general-purpose representations that can be readily employed in a wide range of downstream tasks. While PFMs have been successfully adopted in various fields such as Natural Language Processing and Computer Vision, their capacity in handling geospatial data and answering urban questions remains limited. This can be attributed to the intrinsic heterogeneity of geospatial data, which encompasses different data types, including points, segments and regions, as well as multiple information modalities, such as a spatial position, visual characteristics and textual annotations. The proliferation of Volunteered Geographic Information initiatives, and the ever-increasing availability of open geospatial data sources, like OpenStreetMap, which is freely accessible globally, unveil a promising opportunity to bridge this gap. In this paper, we present CityFM, a self-supervised framework to train a foundation model within a selected geographical area of interest, such as a city. CityFM relies solely on open data from OSM, and produces multimodal representations of entities of different types, incorporating spatial, visual, and textual information. We analyse the entity representations generated using our foundation models from a qualitative perspective, and conduct quantitative experiments on road, building, and region-level downstream tasks. We compare its results to algorithms tailored specifically for the respective applications. In all the experiments, CityFM achieves performance superior to, or on par with, the baselines.
</details>
<details>
<summary>摘要</summary>
干支基模型（PFM）已经引入了人工智能中的一个新模式，因为它们可以学习通用表示，可以在多种下游任务中使用。 although PFMs have been successfully applied in various fields such as natural language processing and computer vision, their ability to handle geospatial data and answer urban questions is still limited. This is because geospatial data is inherently heterogeneous, including different data types such as points, segments, and regions, as well as multiple information modalities such as spatial position, visual characteristics, and textual annotations. With the proliferation of Volunteered Geographic Information initiatives and the increasing availability of open geospatial data sources like OpenStreetMap, which is freely accessible globally, there is a promising opportunity to bridge this gap.在本文中，我们提出了CityFM，一种自我超vised框架，用于在选择的地理区域内（如城市）训练基本模型。 CityFM仅使用OpenStreetMap开源数据，生成多模式表示实体不同类型，包括空间、视觉和文本信息。我们从质量角度分析基本模型生成的实体表示，并对路、建筑物和区域级下游任务进行量测试。我们与专门为这些应用程序开发的算法进行比较。在所有实验中，CityFM的性能都高于或与基eline相当。
</details></li>
</ul>
<hr>
<h2 id="Pink-Unveiling-the-Power-of-Referential-Comprehension-for-Multi-modal-LLMs"><a href="#Pink-Unveiling-the-Power-of-Referential-Comprehension-for-Multi-modal-LLMs" class="headerlink" title="Pink: Unveiling the Power of Referential Comprehension for Multi-modal LLMs"></a>Pink: Unveiling the Power of Referential Comprehension for Multi-modal LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00582">http://arxiv.org/abs/2310.00582</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sy-xuan/pink">https://github.com/sy-xuan/pink</a></li>
<li>paper_authors: Shiyu Xuan, Qingpei Guo, Ming Yang, Shiliang Zhang<br>For:This paper aims to enhance the Referential Comprehension (RC) ability of Multi-modal Large Language Models (MLLMs) for fine-grained perception tasks.Methods:The proposed method represents the referring object in the image using the coordinates of its bounding box and converts the coordinates into texts in a specific format, allowing the model to treat the coordinates as natural language. The model is trained end-to-end with a parameter-efficient tuning framework that allows both modalities to benefit from multi-modal instruction tuning.Results:The proposed method demonstrates superior performance on conventional vision-language and RC tasks, achieving a 12.0% absolute accuracy improvement over Instruct-BLIP on VSR and surpassing Kosmos-2 by 24.7% on RefCOCO_val under zero-shot settings. The model also attains the top position on the leaderboard of MMBench.<details>
<summary>Abstract</summary>
Multi-modal Large Language Models (MLLMs) have shown remarkable capabilities in many vision-language tasks. Nevertheless, most MLLMs still lack the Referential Comprehension (RC) ability to identify a specific object or area in images, limiting their application in fine-grained perception tasks. This paper proposes a novel method to enhance the RC capability for MLLMs. Our model represents the referring object in the image using the coordinates of its bounding box and converts the coordinates into texts in a specific format. This allows the model to treat the coordinates as natural language. Moreover, we construct the instruction tuning dataset with various designed RC tasks at a low cost by unleashing the potential of annotations in existing datasets. To further boost the RC ability of the model, we propose a self-consistent bootstrapping method that extends dense object annotations of a dataset into high-quality referring-expression-bounding-box pairs. The model is trained end-to-end with a parameter-efficient tuning framework that allows both modalities to benefit from multi-modal instruction tuning. This framework requires fewer trainable parameters and less training data. Experimental results on conventional vision-language and RC tasks demonstrate the superior performance of our method. For instance, our model exhibits a 12.0% absolute accuracy improvement over Instruct-BLIP on VSR and surpasses Kosmos-2 by 24.7% on RefCOCO_val under zero-shot settings. We also attain the top position on the leaderboard of MMBench. The models, datasets, and codes are publicly available at https://github.com/SY-Xuan/Pink
</details>
<details>
<summary>摘要</summary>
多modal大语言模型（MLLM）已经表现出了很好的能力在视觉语言任务中。然而，大多数MLLM仍然缺乏指向某个特定 объек或区域在图像中的能力，限制了它们在细化感知任务中的应用。这篇论文提出了一种新的方法来增强MLLM的指向能力。我们的模型使用图像中引用对象的矩形框坐标来表示引用对象，并将坐标转换成特定格式的文本。这 позвоils 模型对坐标视为自然语言。此外，我们构建了一个指令调整数据集，包括了多种设计的指令调整任务，并且可以在低成本下实现。为了进一步提高模型的指向能力，我们提出了一种自适应增强方法，该方法可以将 dense object 注解 extend 到高质量的引用表示矩形框对。模型通过一个简单的参数效率的调参框架进行全局调参，这使得两种模式都可以从多模态指令调整中受益。实验结果表明，我们的方法可以在 convential 视觉语言任务和指向任务中表现出较好的性能。例如，我们的模型在 VSR 任务上比 Instruct-BLIP 提高 12.0% 绝对准确率，并在 RefCOCO_val 任务上比 Kosmos-2 提高 24.7% 绝对准确率，这些结果均在零shot设置下获得。此外，我们的模型在 MMBench 领导板块上位居榜首。模型、数据集和代码都可以在 https://github.com/SY-Xuan/Pink 上获取。
</details></li>
</ul>
<hr>
<h2 id="Consistency-Trajectory-Models-Learning-Probability-Flow-ODE-Trajectory-of-Diffusion"><a href="#Consistency-Trajectory-Models-Learning-Probability-Flow-ODE-Trajectory-of-Diffusion" class="headerlink" title="Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion"></a>Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02279">http://arxiv.org/abs/2310.02279</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sony/ctm">https://github.com/sony/ctm</a></li>
<li>paper_authors: Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, Stefano Ermon</li>
<li>for: 加速扩散模型采样，提高扩散模型的性能。</li>
<li>methods: 提议一种新的兼容性轨迹模型（CTM），可以在单个前进 pass中输出分数（即极化流动方程中的导数），并允许在扩散过程中任意时刻进行交互。</li>
<li>results: CTM在CIFAR-10和ImageNet的64x64分辨率上达到了新的州际级FID值（FID 1.73和FID 2.06），并可以在计算预算增加时，不断提高样本质量，避免了CM中的降低。<details>
<summary>Abstract</summary>
Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion model sampling at the cost of sample quality but lack a natural way to trade-off quality for speed. To address this limitation, we propose Consistency Trajectory Model (CTM), a generalization encompassing CM and score-based models as special cases. CTM trains a single neural network that can -- in a single forward pass -- output scores (i.e., gradients of log-density) and enables unrestricted traversal between any initial and final time along the Probability Flow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables the efficient combination of adversarial training and denoising score matching loss to enhance performance and achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at 64X64 resolution (FID 2.06). CTM also enables a new family of sampling schemes, both deterministic and stochastic, involving long jumps along the ODE solution trajectories. It consistently improves sample quality as computational budgets increase, avoiding the degradation seen in CM. Furthermore, CTM's access to the score accommodates all diffusion model inference techniques, including exact likelihood computation.
</details>
<details>
<summary>摘要</summary>
协调模型（CM）（Song et al., 2023）可以加速基于分数的扩散模型抽象，但是会增加样本质量的成本。为了解决这个限制，我们提出了一种新的模型——一致轨迹模型（CTM）。CTM可以在单个前进 pass中输出分数（即极化流速度的导数），并且允许在扩散过程中任意时刻之间进行不受限制的游走。此外，CTM还可以通过 combining adversarial training和杂噪分数匹配损失来提高性能，并实现了单步扩散模型抽象中的新的州态-of-the-art FID 值（FID 1.73）和 ImageNet 的 64x64 分辨率上的 FID 值（FID 2.06）。此外，CTM还可以实现一种新的抽象方式，包括 deterministic 和 stochastic 的长距离跳跃。在计算预算增加时，CTM可以逐步提高样本质量，而不是如CM所见的协调模型。此外，CTM可以访问分数，因此可以应用于所有扩散模型的推理技术，包括准确的概率计算。
</details></li>
</ul>
<hr>
<h2 id="LaPLACE-Probabilistic-Local-Model-Agnostic-Causal-Explanations"><a href="#LaPLACE-Probabilistic-Local-Model-Agnostic-Causal-Explanations" class="headerlink" title="LaPLACE: Probabilistic Local Model-Agnostic Causal Explanations"></a>LaPLACE: Probabilistic Local Model-Agnostic Causal Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00570">http://arxiv.org/abs/2310.00570</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/simon-tan/laplace">https://github.com/simon-tan/laplace</a></li>
<li>paper_authors: Sein Minn</li>
<li>for: The paper aims to provide probabilistic cause-and-effect explanations for any classifier operating on tabular data, in a human-understandable manner.</li>
<li>methods: The LaPLACE-Explainer component leverages the concept of a Markov blanket to establish statistical boundaries between relevant and non-relevant features automatically, and incorporates conditional probabilities to offer probabilistic causal explanations.</li>
<li>results: The approach outperforms LIME and SHAP in terms of local accuracy and consistency of explained features, and is validated across various classification models through experiments with both simulated and real-world datasets. The explanations provided by LaPLACE can address trust-related issues such as evaluating prediction reliability, facilitating model selection, enhancing trustworthiness, and identifying fairness-related concerns within classifiers.Here is the information in Simplified Chinese text:</li>
<li>for: 本文目的是提供任何类别器操作于表格数据上的可能性 causa causal 解释，以人类可理解的方式。</li>
<li>methods: LaPLACE-Explainer 组件利用 Markov 围栏的概念，自动地建立表格数据上相关和非相关特征的统计边界，并通过 conditional probabilities 提供可能性解释。</li>
<li>results: LaPLACE 的方法比 LIME 和 SHAP 在本地准确率和解释特征的一致性方面表现出色，并通过多种分类模型的实验，在 simulate 和实际数据集上进行了验证。 LaPLACE 的解释可以解决一些信任问题，如评估预测可靠性、促进模型选择、增强可靠性和检测 fairness 相关问题在类别器中。<details>
<summary>Abstract</summary>
Machine learning models have undeniably achieved impressive performance across a range of applications. However, their often perceived black-box nature, and lack of transparency in decision-making, have raised concerns about understanding their predictions. To tackle this challenge, researchers have developed methods to provide explanations for machine learning models. In this paper, we introduce LaPLACE-explainer, designed to provide probabilistic cause-and-effect explanations for any classifier operating on tabular data, in a human-understandable manner. The LaPLACE-Explainer component leverages the concept of a Markov blanket to establish statistical boundaries between relevant and non-relevant features automatically. This approach results in the automatic generation of optimal feature subsets, serving as explanations for predictions. Importantly, this eliminates the need to predetermine a fixed number N of top features as explanations, enhancing the flexibility and adaptability of our methodology. Through the incorporation of conditional probabilities, our approach offers probabilistic causal explanations and outperforms LIME and SHAP (well-known model-agnostic explainers) in terms of local accuracy and consistency of explained features. LaPLACE's soundness, consistency, local accuracy, and adaptability are rigorously validated across various classification models. Furthermore, we demonstrate the practical utility of these explanations via experiments with both simulated and real-world datasets. This encompasses addressing trust-related issues, such as evaluating prediction reliability, facilitating model selection, enhancing trustworthiness, and identifying fairness-related concerns within classifiers.
</details>
<details>
<summary>摘要</summary>
机器学习模型在多种应用场景中表现出色，但它们的很多时候被视为黑盒模型，无法准确地描述它们的预测结果。为解决这个问题，研究人员开发了一些方法来提供机器学习模型的解释。本文介绍了LaPLACE-explainer，可以为任何基于表格数据的分类器提供 probabilistic cause-and-effect 的解释，并且在人类可以理解的方式下进行解释。LaPLACE-Explainer 组件利用 Markov blanket 的概念，自动地确定相关和无关的特征。这种方法可以自动生成最佳的特征子集，作为预测的解释。这种方法不需要手动决定固定的特征数 N 作为解释，从而提高了方法的灵活性和适应性。通过 incorporating  conditional probabilities，我们的方法可以提供 probabilistic causal 的解释，并且在本地准确性和解释特征的一致性方面超过 LIME 和 SHAP（已知的模型无关解释器）。LaPLACE 的准确性、一致性、本地准确性和适应性被严格验证了多种分类模型。此外，我们通过对 simulated 和实际数据进行实验，证明了这些解释的实际用途。这包括评估预测可靠性、促进模型选择、增强可靠性和识别分类器中的公平问题。
</details></li>
</ul>
<hr>
<h2 id="Quantum-Based-Feature-Selection-for-Multi-classification-Problem-in-Complex-Systems-with-Edge-Computing"><a href="#Quantum-Based-Feature-Selection-for-Multi-classification-Problem-in-Complex-Systems-with-Edge-Computing" class="headerlink" title="Quantum-Based Feature Selection for Multi-classification Problem in Complex Systems with Edge Computing"></a>Quantum-Based Feature Selection for Multi-classification Problem in Complex Systems with Edge Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01443">http://arxiv.org/abs/2310.01443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjie Liu, Junxiu Chen, Yuxiang Wang, Peipei Gao, Zhibin Lei, Xu Ma</li>
<li>for: 本研究提出了一种基于量子算法的特征选择方法，以提高计算效率和降低资源消耗。</li>
<li>methods: 本方法使用量子态编码法将每个样本的特征编码为量子状态，然后应用振荡器算法计算任务之间的相似性。接着，根据相似性，使用格罗韦-隆方法找到最近的k个邻居样本，并更新权重矩阵。</li>
<li>results: 与传统的类ReliefF算法相比，本方法可以降低相似性计算的复杂度从O(MN)降至O(M)，找到最近的邻居的复杂度从O(M)降至O(sqrt(M))，并降低资源消耗从O(MN)降至O(MlogN)。同时，与量子Relief算法相比，本方法在找到最近的邻居方面更为精准，从O(M)降至O(sqrt(M))。最后，通过基于Rigetti的一个简单示例的实验来验证方法的可行性。<details>
<summary>Abstract</summary>
The complex systems with edge computing require a huge amount of multi-feature data to extract appropriate insights for their decision making, so it is important to find a feasible feature selection method to improve the computational efficiency and save the resource consumption. In this paper, a quantum-based feature selection algorithm for the multi-classification problem, namely, QReliefF, is proposed, which can effectively reduce the complexity of algorithm and improve its computational efficiency. First, all features of each sample are encoded into a quantum state by performing operations CMP and R_y, and then the amplitude estimation is applied to calculate the similarity between any two quantum states (i.e., two samples). According to the similarities, the Grover-Long method is utilized to find the nearest k neighbor samples, and then the weight vector is updated. After a certain number of iterations through the above process, the desired features can be selected with regards to the final weight vector and the threshold {\tau}. Compared with the classical ReliefF algorithm, our algorithm reduces the complexity of similarity calculation from O(MN) to O(M), the complexity of finding the nearest neighbor from O(M) to O(sqrt(M)), and resource consumption from O(MN) to O(MlogN). Meanwhile, compared with the quantum Relief algorithm, our algorithm is superior in finding the nearest neighbor, reducing the complexity from O(M) to O(sqrt(M)). Finally, in order to verify the feasibility of our algorithm, a simulation experiment based on Rigetti with a simple example is performed.
</details>
<details>
<summary>摘要</summary>
复杂系统与边计算需要巨量多元特征数据提取适当的洞察，因此需要一种可行的特征选择方法来提高计算效率和节省资源消耗。本文提出了一种基于量子算法的多类划分问题特征选择算法，即QReliefF，可以有效减少算法的复杂性和提高计算效率。首先，每个样本的所有特征都被编码成量子状态，并通过操作CMP和R_y进行实现。然后，对任意两个量子状态（即两个样本）进行振荡检测，并根据相似性，使用格罗弗-隆方法查找最近的k个邻居样本。然后更新权重 вектор。经过一定的迭代过程，可以选择符合最终权重 вектор和阈值{\tau}的特征。与 классическойReliefF算法相比，我们的算法减少了相似性计算的复杂性从O(MN)降低到O(M)，寻找最近邻居的复杂性从O(M)降低到O(sqrt(M))，资源消耗从O(MN)降低到O(MlogN)。同时，与量子Relief算法相比，我们的算法在寻找最近邻居方面更加突出，从O(M)降低到O(sqrt(M))。 finally，为证明我们的算法的可行性，我们在Rigetti上进行了一个简单的实验。
</details></li>
</ul>
<hr>
<h2 id="TDCGL-Two-Level-Debiased-Contrastive-Graph-Learning-for-Recommendation"><a href="#TDCGL-Two-Level-Debiased-Contrastive-Graph-Learning-for-Recommendation" class="headerlink" title="TDCGL: Two-Level Debiased Contrastive Graph Learning for Recommendation"></a>TDCGL: Two-Level Debiased Contrastive Graph Learning for Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00569">http://arxiv.org/abs/2310.00569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yubo Gao, Haotian Wu<br>for:The paper aims to address the problems of over-reliance on high-quality knowledge graphs and noise issues in real-world data, which can negatively impact the performance of knowledge graph-based recommendation methods.methods:The proposed method, Two-Level Debiased Contrastive Graph Learning (TDCGL), combines contrastive learning with debiasing techniques to improve the performance of knowledge graph-based recommendation methods. The method is designed to work on both User-Item and User-User pairs to model higher-order relations.results:The proposed method significantly outperforms state-of-the-art baselines in terms of anti-noise capability and recommendation performance. Ablation studies demonstrate the necessity of each level of the TDCGL method.<details>
<summary>Abstract</summary>
knowledge graph-based recommendation methods have achieved great success in the field of recommender systems. However, over-reliance on high-quality knowledge graphs is a bottleneck for such methods. Specifically, the long-tailed distribution of entities of KG and noise issues in the real world will make item-entity dependent relations deviate from reflecting true characteristics and significantly harm the performance of modeling user preference. Contrastive learning, as a novel method that is employed for data augmentation and denoising, provides inspiration to fill this research gap. However, the mainstream work only focuses on the long-tail properties of the number of items clicked, while ignoring that the long-tail properties of total number of clicks per user may also affect the performance of the recommendation model. Therefore, to tackle these problems, motivated by the Debiased Contrastive Learning of Unsupervised Sentence Representations (DCLR), we propose Two-Level Debiased Contrastive Graph Learning (TDCGL) model. Specifically, we design the Two-Level Debiased Contrastive Learning (TDCL) and deploy it in the KG, which is conducted not only on User-Item pairs but also on User-User pairs for modeling higher-order relations. Also, to reduce the bias caused by random sampling in contrastive learning, with the exception of the negative samples obtained by random sampling, we add a noise-based generation of negation to ensure spatial uniformity. Considerable experiments on open-source datasets demonstrate that our method has excellent anti-noise capability and significantly outperforms state-of-the-art baselines. In addition, ablation studies about the necessity for each level of TDCL are conducted.
</details>
<details>
<summary>摘要</summary>
知识图库（KG）基于推荐方法在推荐系统中取得了很大的成功。然而，高质量知识图的过亢使得这些方法受到了阻碍。具体来说，知识图中实体的长尾分布和实际世界中的噪声问题会使Item-Entity相关性偏离真实特性，从而对模型用户喜好的表达有很大的负面影响。对此，对数据增强和降噪的contrastive学习提供了灵感，但主流工作只关注长尾数量的点击项，而忽略了每个用户的总点击量长尾属性的影响。因此，为了解决这些问题，我们提出了Two-Level Debiased Contrastive Graph Learning（TDCGL）模型。具体来说，我们设计了Two-Level Debiased Contrastive Learning（TDCL），并在KG中进行了实现，不仅在用户-项对上进行了实现，还在用户-用户对上进行了实现，以模型高级别关系。此外，为了减少Random sampling导致的偏见，除了随机抽取的负样本外，我们还添加了随机生成的负样本，以确保空间均匀性。经过了一系列的实验，我们发现我们的方法在开源数据集上具有极高的反噪能力，并在比较之下显著超越了状态精算标准。此外，我们还进行了剖析研究，以确定每级TDCL的必要性。
</details></li>
</ul>
<hr>
<h2 id="Understanding-the-Robustness-of-Randomized-Feature-Defense-Against-Query-Based-Adversarial-Attacks"><a href="#Understanding-the-Robustness-of-Randomized-Feature-Defense-Against-Query-Based-Adversarial-Attacks" class="headerlink" title="Understanding the Robustness of Randomized Feature Defense Against Query-Based Adversarial Attacks"></a>Understanding the Robustness of Randomized Feature Defense Against Query-Based Adversarial Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00567">http://arxiv.org/abs/2310.00567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quang H. Nguyen, Yingjie Lao, Tung Pham, Kok-Seng Wong, Khoa D. Doan</li>
<li>for: 防止深度神经网络受到黑盒攻击，即使攻击者只有模型的输出信息。</li>
<li>methods: 提出了一种简单、轻量级的防御策略，在推理时将隐藏层的特征加上随机噪音，以提高模型免受黑盒攻击。</li>
<li>results: 经过 teorical 分析和实验 validate，该方法可以有效地增强模型对黑盒攻击的抵抗力，并不需要对模型进行 adversarial 训练，对模型的准确率也没有明显的影响。<details>
<summary>Abstract</summary>
Recent works have shown that deep neural networks are vulnerable to adversarial examples that find samples close to the original image but can make the model misclassify. Even with access only to the model's output, an attacker can employ black-box attacks to generate such adversarial examples. In this work, we propose a simple and lightweight defense against black-box attacks by adding random noise to hidden features at intermediate layers of the model at inference time. Our theoretical analysis confirms that this method effectively enhances the model's resilience against both score-based and decision-based black-box attacks. Importantly, our defense does not necessitate adversarial training and has minimal impact on accuracy, rendering it applicable to any pre-trained model. Our analysis also reveals the significance of selectively adding noise to different parts of the model based on the gradient of the adversarial objective function, which can be varied during the attack. We demonstrate the robustness of our defense against multiple black-box attacks through extensive empirical experiments involving diverse models with various architectures.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Empowering-Many-Biasing-a-Few-Generalist-Credit-Scoring-through-Large-Language-Models"><a href="#Empowering-Many-Biasing-a-Few-Generalist-Credit-Scoring-through-Large-Language-Models" class="headerlink" title="Empowering Many, Biasing a Few: Generalist Credit Scoring through Large Language Models"></a>Empowering Many, Biasing a Few: Generalist Credit Scoring through Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00566">http://arxiv.org/abs/2310.00566</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/colfeng/calm">https://github.com/colfeng/calm</a></li>
<li>paper_authors: Duanyu Feng, Yongfu Dai, Jimin Huang, Yifang Zhang, Qianqian Xie, Weiguang Han, Alejandro Lopez-Lira, Hao Wang</li>
<li>for: 这篇论文旨在检验大语言模型（LLM）是否可以用于信用评估。</li>
<li>methods: 作者使用了三个假设和一个大量的实验研究LLM在信用评估中的可行性。他们首先制定了一个特有的信用评估大语言模型（CALM），然后对LLM的偏见进行了严格的检查。</li>
<li>results: 研究发现LLM可以超越传统模型的局限性，并且在不同的金融评估中表现出优异的适应能力。同时，研究也发现LLM可能存在一些偏见，因此提出了一些改进方案。<details>
<summary>Abstract</summary>
Credit and risk assessments are cornerstones of the financial landscape, impacting both individual futures and broader societal constructs. Existing credit scoring models often exhibit limitations stemming from knowledge myopia and task isolation. In response, we formulate three hypotheses and undertake an extensive case study to investigate LLMs' viability in credit assessment. Our empirical investigations unveil LLMs' ability to overcome the limitations inherent in conventional models. We introduce a novel benchmark curated for credit assessment purposes, fine-tune a specialized Credit and Risk Assessment Large Language Model (CALM), and rigorously examine the biases that LLMs may harbor. Our findings underscore LLMs' potential in revolutionizing credit assessment, showcasing their adaptability across diverse financial evaluations, and emphasizing the critical importance of impartial decision-making in the financial sector. Our datasets, models, and benchmarks are open-sourced for other researchers.
</details>
<details>
<summary>摘要</summary>
信用和风险评估是金融景观中的两个重要基础，对个人未来和社会构建都产生了深远的影响。现有的信用评估模型经常受到知识偏见和任务隔离的限制。为了应对这些限制，我们提出了三个假设，并进行了广泛的案例研究，以评估LLMs在信用评估中的可行性。我们的实际调查发现，LLMs可以超越传统模型中的限制。我们开发了一个专门为信用评估目的制定的benchmark，细化一个特殊的信用和风险评估大语言模型（CALM），并且严格地检查LLMs可能披露的偏见。我们的发现表明，LLMs在改变信用评估的方式方面具有启示性，并且在多种金融评估中展现出了适应性。我们的数据集、模型和benchmark都公开发布，以便其他研究人员进行进一步的研究。
</details></li>
</ul>
<hr>
<h2 id="DYNAP-SE2-a-scalable-multi-core-dynamic-neuromorphic-asynchronous-spiking-neural-network-processor"><a href="#DYNAP-SE2-a-scalable-multi-core-dynamic-neuromorphic-asynchronous-spiking-neural-network-processor" class="headerlink" title="DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous spiking neural network processor"></a>DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous spiking neural network processor</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00564">http://arxiv.org/abs/2310.00564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ole Richter, Chenxi Wu, Adrian M. Whatley, German Köstinger, Carsten Nielsen, Ning Qiao, Giacomo Indiveri</li>
<li>for: 这个论文旨在提出一种基于生物神经系统的概念的卷积神经网络平台，用于实时处理感知信号。</li>
<li>methods: 该平台使用了卷积神经网络，并实现了各种生物学上的神经处理现象，如短期抑制、NMDA阻链、AMPA扩散、家OSTAT、脉冲频率调整、抗阻填充和脉冲传输延迟。</li>
<li>results: 该平台可以实现实时处理感知信号，并且可以模拟不同的生物学上的神经网络，包括单个神经元和脑细胞信号的监测。<details>
<summary>Abstract</summary>
With the remarkable progress that technology has made, the need for processing data near the sensors at the edge has increased dramatically. The electronic systems used in these applications must process data continuously, in real-time, and extract relevant information using the smallest possible energy budgets. A promising approach for implementing always-on processing of sensory signals that supports on-demand, sparse, and edge-computing is to take inspiration from biological nervous system. Following this approach, we present a brain-inspired platform for prototyping real-time event-based Spiking Neural Networks (SNNs). The system proposed supports the direct emulation of dynamic and realistic neural processing phenomena such as short-term plasticity, NMDA gating, AMPA diffusion, homeostasis, spike frequency adaptation, conductance-based dendritic compartments and spike transmission delays. The analog circuits that implement such primitives are paired with a low latency asynchronous digital circuits for routing and mapping events. This asynchronous infrastructure enables the definition of different network architectures, and provides direct event-based interfaces to convert and encode data from event-based and continuous-signal sensors. Here we describe the overall system architecture, we characterize the mixed signal analog-digital circuits that emulate neural dynamics, demonstrate their features with experimental measurements, and present a low- and high-level software ecosystem that can be used for configuring the system. The flexibility to emulate different biologically plausible neural networks, and the chip's ability to monitor both population and single neuron signals in real-time, allow to develop and validate complex models of neural processing for both basic research and edge-computing applications.
</details>
<details>
<summary>摘要</summary>
随着技术的快速发展，处理数据在边缘的需求减少了很多。电子系统在这些应用程序中必须在实时中处理数据，并在最小的能量预算下提取相关信息。一种有前途的方法是根据生物神经系统来实现持续时间的触发神经网络（SNN）。我们在这篇文章中提出了一种基于脑神经系统的平台，用于实时驱动SNN。该系统支持直接模拟生物化的神经处理现象，如短期抑制、NMDA闭合、AMPA扩散、家OSTASIS、脉冲频率调整、抗场基于脑干细胞和脉冲传输延迟。这些分析电路与低延迟的异步数字电路结合，以实现不同网络架构和直接将事件转换为数据。这个异步基础设施允许定义不同的网络架构，并提供直接基于事件的数据编码和转换接口。我们在这篇文章中描述了整体系统架构，Characterize mixed signal analog-digital circuits that emulate neural dynamics, demonstrate their features with experimental measurements, and present a low- and high-level software ecosystem that can be used for configuring the system。系统的灵活性可以模拟不同的生物学可能的神经网络，系统的检测功能可以在实时中监测单个神经元和群体神经元的信号。这些功能使得可以开发和验证复杂的神经处理模型，以满足边缘计算应用和基础研究的需求。
</details></li>
</ul>
<hr>
<h2 id="Siamese-Representation-Learning-for-Unsupervised-Relation-Extraction"><a href="#Siamese-Representation-Learning-for-Unsupervised-Relation-Extraction" class="headerlink" title="Siamese Representation Learning for Unsupervised Relation Extraction"></a>Siamese Representation Learning for Unsupervised Relation Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00552">http://arxiv.org/abs/2310.00552</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gxxxzhang/siamese-ure">https://github.com/gxxxzhang/siamese-ure</a></li>
<li>paper_authors: Guangxin Zhang, Shu Chen</li>
<li>for: 掌握开放平台文本中Named Entity对的下一级关系，无需先知 relacional distribution。</li>
<li>methods: 使用对比学习，吸引正样本，排斥负样本，以提高分类的分化。</li>
<li>results: 我们提出的Siamese Representation Learning for Unsupervised Relation Extraction模型，可以有效优化关系表示例子，保持关系特征空间的层次结构，并在无监督的情况下提高关系EXTRACTION的性能。<details>
<summary>Abstract</summary>
Unsupervised relation extraction (URE) aims at discovering underlying relations between named entity pairs from open-domain plain text without prior information on relational distribution. Existing URE models utilizing contrastive learning, which attract positive samples and repulse negative samples to promote better separation, have got decent effect. However, fine-grained relational semantic in relationship makes spurious negative samples, damaging the inherent hierarchical structure and hindering performances. To tackle this problem, we propose Siamese Representation Learning for Unsupervised Relation Extraction -- a novel framework to simply leverage positive pairs to representation learning, possessing the capability to effectively optimize relation representation of instances and retain hierarchical information in relational feature space. Experimental results show that our model significantly advances the state-of-the-art results on two benchmark datasets and detailed analyses demonstrate the effectiveness and robustness of our proposed model on unsupervised relation extraction.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>无监督关系抽取（URE）目标是从开放领域平滑文本中发现下面的关系，无需先知relational分布。现有URE模型使用对照学习，吸引正样本并排斥负样本，以促进更好的分离。然而，细腻的关系semantic在关系中导致假性负样本的生成，损害内在的层次结构，降低性能。为解决这个问题，我们提出了对称表示学习 для无监督关系抽取——一种新的框架，可以简单地利用正样本来 representation学习，具有可以有效优化关系表示实例的能力，并保留关系特征空间中的层次信息。实验结果显示，我们的模型在两个 benchmark 数据集上显著提高了状态的报告结果，并在详细分析中证明了我们提出的模型在无监督关系抽取中的效果和稳定性。
</details></li>
</ul>
<hr>
<h2 id="JoMA-Demystifying-Multilayer-Transformers-via-JOint-Dynamics-of-MLP-and-Attention"><a href="#JoMA-Demystifying-Multilayer-Transformers-via-JOint-Dynamics-of-MLP-and-Attention" class="headerlink" title="JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention"></a>JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00535">http://arxiv.org/abs/2310.00535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, Simon Du</li>
<li>for: 这篇论文旨在理解多层Transformer架构在训练过程中的行为。</li>
<li>methods: 论文提出了一种新的数学框架，称为Join MLP&#x2F;Attention（JoMA）动力学，它将Transformer架构中的自注意层替换为多层MLP层，从而更好地理解训练过程。</li>
<li>results: 实验表明，在使用真实世界数据集（Wikitext2&#x2F;Wikitext103）和不同的预训练模型（OPT、Pythia）训练的情况下，JoMA能够准确预测多层Transformer中Token的组合方式，并且能够解释在不同的 activations 下，注意力在不同阶段变得稀疏或密集。<details>
<summary>Abstract</summary>
We propose Joint MLP/Attention (JoMA) dynamics, a novel mathematical framework to understand the training procedure of multilayer Transformer architectures. This is achieved by integrating out the self-attention layer in Transformers, producing a modified dynamics of MLP layers only. JoMA removes unrealistic assumptions in previous analysis (e.g., lack of residual connection) and predicts that the attention first becomes sparse (to learn salient tokens), then dense (to learn less salient tokens) in the presence of nonlinear activations, while in the linear case, it is consistent with existing works that show attention becomes sparse over time. We leverage JoMA to qualitatively explains how tokens are combined to form hierarchies in multilayer Transformers, when the input tokens are generated by a latent hierarchical generative model. Experiments on models trained from real-world dataset (Wikitext2/Wikitext103) and various pre-trained models (OPT, Pythia) verify our theoretical findings.
</details>
<details>
<summary>摘要</summary>
我们提议的 JOINT MLP/ATTENTION（JoMA）动力学框架，用于理解多层Transformer结构的训练过程。我们在Transformer中抽取了自注意层，生成了修改后的MLP层 dynamics。JoMA eliminates unrealistic assumptions in previous analysis（例如缺乏径向连接），并预测在非线性活化下，注意力首先变得稀疏（以学习重要的token），然后变得密集（以学习较不重要的token）。在线性情况下，它与先前的研究一致，注意力随时间变得稀疏。我们利用JoMA来质量地解释在多层Transformer中如何将输入token组合成层次结构，当输入token由隐藏的层次生成模型生成。我们通过在真实世界数据集（Wikitext2/Wikitext103）和多种预训练模型（OPT、Pythia）进行实验，证明我们的理论发现。
</details></li>
</ul>
<hr>
<h2 id="SELF-Language-Driven-Self-Evolution-for-Large-Language-Model"><a href="#SELF-Language-Driven-Self-Evolution-for-Large-Language-Model" class="headerlink" title="SELF: Language-Driven Self-Evolution for Large Language Model"></a>SELF: Language-Driven Self-Evolution for Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00533">http://arxiv.org/abs/2310.00533</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianqiao Lu, Wanjun Zhong, Wenyong Huang, Yufei Wang, Fei Mi, Baojun Wang, Weichao Wang, Lifeng Shang, Qun Liu</li>
<li>for: The paper aims to introduce an innovative approach for autonomous model development in large language models (LLMs), enabling them to undergo continual self-evolution and improve their intrinsic abilities without human intervention.</li>
<li>methods: The proposed approach, called “SELF” (Self-Evolution with Language Feedback), employs language-based feedback as a versatile and comprehensive evaluative tool to guide the model’s self-evolutionary training. SELF acquires foundational meta-skills through meta-skill learning, and uses self-curated data for perpetual training and iterative fine-tuning to enhance its capabilities.</li>
<li>results: The experimental results on representative benchmarks demonstrate that SELF can progressively advance its inherent abilities without human intervention, producing responses of superior quality. The SELF framework signifies a viable pathway for autonomous LLM development, transforming the LLM from a passive recipient of information into an active participant in its own evolution.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have showcased remarkable versatility across diverse domains. However, the pathway toward autonomous model development, a cornerstone for achieving human-level learning and advancing autonomous AI, remains largely uncharted. We introduce an innovative approach, termed "SELF" (Self-Evolution with Language Feedback). This methodology empowers LLMs to undergo continual self-evolution. Furthermore, SELF employs language-based feedback as a versatile and comprehensive evaluative tool, pinpointing areas for response refinement and bolstering the stability of self-evolutionary training. Initiating with meta-skill learning, SELF acquires foundational meta-skills with a focus on self-feedback and self-refinement. These meta-skills are critical, guiding the model's subsequent self-evolution through a cycle of perpetual training with self-curated data, thereby enhancing its intrinsic abilities. Given unlabeled instructions, SELF equips the model with the capability to autonomously generate and interactively refine responses. This synthesized training data is subsequently filtered and utilized for iterative fine-tuning, enhancing the model's capabilities. Experimental results on representative benchmarks substantiate that SELF can progressively advance its inherent abilities without the requirement of human intervention, thereby indicating a viable pathway for autonomous model evolution. Additionally, SELF can employ online self-refinement strategy to produce responses of superior quality. In essence, the SELF framework signifies a progressive step towards autonomous LLM development, transforming the LLM from a mere passive recipient of information into an active participant in its own evolution.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在多种领域表现出了惊人的多面性。然而，把模型发展成为自主的核心目标，以实现人类水平的学习和自主AI的进步，仍然是一个未探索的路径。我们提出了一种创新的方法，称为“自我演进”（Self-Evolution with Language Feedback，SELF）。这种方法使得LLM可以不断自我演进。此外，SELF使用语言反馈作为多方面的评价工具，帮助模型自我评估和改进。通过初级技能学习，SELF取得了基本的初级技能，重点是自我反馈和自我改进。这些初级技能是关键的，帮助模型在后续的自我演进过程中自动生成和互动地反复修改答案。在没有 Label 的情况下，SELF 使得模型可以自动生成和修改答案。这些合成的训练数据被筛选并用于迭代练化，从而提高模型的能力。实验结果表明，SELF 可以不断提高其内在能力，无需人类干预，这表明了一个可行的自主模型演进路径。此外，SELF 还可以使用在线自我反finement策略生成高质量答案。总之，SELF 框架表示了一个自主 LLM 发展的进步，将 LLM 转化为一个活跃参与自己演进的参与者。
</details></li>
</ul>
<hr>
<h2 id="Are-Graph-Neural-Networks-Optimal-Approximation-Algorithms"><a href="#Are-Graph-Neural-Networks-Optimal-Approximation-Algorithms" class="headerlink" title="Are Graph Neural Networks Optimal Approximation Algorithms?"></a>Are Graph Neural Networks Optimal Approximation Algorithms?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00526">http://arxiv.org/abs/2310.00526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Morris Yau, Eric Lu, Nikolaos Karalias, Jessica Xu, Stefanie Jegelka</li>
<li>for: 这个论文目的是设计用于获得优化算法的图 neural network 架构，用于解决一类 combinatorial optimization problems。</li>
<li>methods: 论文使用了强大的算法工具 from semidefinite programming (SDP)，并证明了可以使用 polynomial-sized message passing algorithms 来表示最强 polynomial time algorithms for Max Constraint Satisfaction Problems，假设Unique Games Conjecture 成立。</li>
<li>results: 论文实现了高质量的近似解决方案，在多种实际和 sintetic 数据集上对比 both neural baselines 和 classical algorithms 表现出色。此外，论文还利用 OptGNN 的 convex relaxation 能力设计了一种生成 dual certificates of optimality 的算法。<details>
<summary>Abstract</summary>
In this work we design graph neural network architectures that can be used to obtain optimal approximation algorithms for a large class of combinatorial optimization problems using powerful algorithmic tools from semidefinite programming (SDP). Concretely, we prove that polynomial-sized message passing algorithms can represent the most powerful polynomial time algorithms for Max Constraint Satisfaction Problems assuming the Unique Games Conjecture. We leverage this result to construct efficient graph neural network architectures, OptGNN, that obtain high-quality approximate solutions on landmark combinatorial optimization problems such as Max Cut and maximum independent set. Our approach achieves strong empirical results across a wide range of real-world and synthetic datasets against both neural baselines and classical algorithms. Finally, we take advantage of OptGNN's ability to capture convex relaxations to design an algorithm for producing dual certificates of optimality (bounds on the optimal solution) from the learned embeddings of OptGNN.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们设计了图神经网络架构，可以用来获取大类 combinatorial optimization 问题的优化算法。我们证明了，使用半definite 程序（SDP）的强大算法工具，可以通过极限下的讯息传递算法来获取最优解。我们利用这个结果，构建了高效的图神经网络架构 OptGNN，可以在 landmark  combinatorial optimization 问题中获得高质量的近似解。我们的方法在各种实际和 sintetic 数据集上实现了强有力的实际结果，比较 neural 基elines 和经典算法。最后，我们利用 OptGNN 捕捉到的 convex relaxation，设计了一种生成优化解的 dual certificate 算法。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/01/cs.AI_2023_10_01/" data-id="clpxp6bwv004tee88d5t57h4e" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/01/cs.CL_2023_10_01/" class="article-date">
  <time datetime="2023-10-01T11:00:00.000Z" itemprop="datePublished">2023-10-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/01/cs.CL_2023_10_01/">cs.CL - 2023-10-01</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Parameter-Efficient-Tuning-Helps-Language-Model-Alignment"><a href="#Parameter-Efficient-Tuning-Helps-Language-Model-Alignment" class="headerlink" title="Parameter-Efficient Tuning Helps Language Model Alignment"></a>Parameter-Efficient Tuning Helps Language Model Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00819">http://arxiv.org/abs/2310.00819</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianci Xue, Ziqi Wang, Heng Ji</li>
<li>for: 这个研究的目的是将大型自然语言模型（LLMs）调整为人类的喜好，以确保LLMs在使用时能够安全且有用。</li>
<li>methods: 这个研究使用了强化学习（RLHF）和直接喜好优化（DPO）以进行调整，但这些方法有一些限制，例如只能在训练时间对一个喜好进行调整（例如不能将模型训练为生成简润的回答时对应多个喜好），或者有特定的数据格式限制（例如DPO只支持双向喜好数据）。为了解决这个问题，先前的研究将控制生成集成到调整过程中，让模型在测试时根据不同的喜好生成不同的回答。控制生成还提供了更多的数据格式可能性（例如支持点对数据）。</li>
<li>results: 这个研究的结果显示，使用实体效率的调整（例如提示调整和低维度适应）来优化控制token，然后进行控制生成，可以将控制token的质量提高，并在两个公认的数据集上显著提高控制生成质量，与先前的研究相比。<details>
<summary>Abstract</summary>
Aligning large language models (LLMs) with human preferences is essential for safe and useful LLMs. Previous works mainly adopt reinforcement learning (RLHF) and direct preference optimization (DPO) with human feedback for alignment. Nevertheless, they have certain drawbacks. One such limitation is that they can only align models with one preference at the training time (e.g., they cannot learn to generate concise responses when the preference data prefers detailed responses), or have certain constraints for the data format (e.g., DPO only supports pairwise preference data). To this end, prior works incorporate controllable generations for alignment to make language models learn multiple preferences and provide outputs with different preferences during inference if asked. Controllable generation also offers more flexibility with regard to data format (e.g., it supports pointwise preference data). Specifically, it uses different control tokens for different preferences during training and inference, making LLMs behave differently when required. Current controllable generation methods either use a special token or hand-crafted prompts as control tokens, and optimize them together with LLMs. As control tokens are typically much lighter than LLMs, this optimization strategy may not effectively optimize control tokens. To this end, we first use parameter-efficient tuning (e.g., prompting tuning and low-rank adaptation) to optimize control tokens and then fine-tune models for controllable generations, similar to prior works. Our approach, alignMEnt with parameter-Efficient Tuning (MEET), improves the quality of control tokens, thus improving controllable generation quality consistently by an apparent margin on two well-recognized datasets compared with prior works.
</details>
<details>
<summary>摘要</summary>
对大型语言模型（LLM）的调整是非常重要，以确保其安全和有用。以前的工作主要采用了强化学习（RLHF）和直接喜好优化（DPO），并通过人类反馈来进行调整。然而，这些方法有一些缺点。例如，它们只能在训练时间内对一个喜好进行调整（例如，它们无法学习生成简洁响应，当喜好数据偏好详细响应时），或者有一些数据格式的限制（例如，DPO只支持对数据进行对比优化）。为了解决这个问题，先前的工作会 incorporate 可控生成，以使语言模型学习多个喜好，并在推理时根据需要生成不同的响应。可控生成还提供了更多的数据格式灵活性（例如，它支持点对数据）。具体来说，它在训练和推理时使用不同的控制符，使模型在不同的喜好下行为不同。现有的可控生成方法通常使用特殊符号或手工制定的提示作为控制符，并与模型一起优化。然而，这种优化策略可能不能有效地优化控制符。为了解决这个问题，我们首先使用参数高效调整（例如，提示调整和低级变换）来优化控制符，然后继续调整模型以实现可控生成。我们的方法，名为 alignMEnt with parameter-Efficient Tuning（MEET），可以不断提高控制符的质量，从而提高可控生成质量，并在两个常见的数据集上显著超越先前的工作。
</details></li>
</ul>
<hr>
<h2 id="Injecting-a-Structural-Inductive-Bias-into-a-Seq2Seq-Model-by-Simulation"><a href="#Injecting-a-Structural-Inductive-Bias-into-a-Seq2Seq-Model-by-Simulation" class="headerlink" title="Injecting a Structural Inductive Bias into a Seq2Seq Model by Simulation"></a>Injecting a Structural Inductive Bias into a Seq2Seq Model by Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00796">http://arxiv.org/abs/2310.00796</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthias Lindemann, Alexander Koller, Ivan Titov</li>
<li>for: 提高seq2seq NLP任务的系统泛化和少量数据学习能力</li>
<li>methods: 通过预训练模型对 synthetic 数据进行结构变换的模拟来注入强制性 inductive bias</li>
<li>results: 实验结果表明，我们的方法可以带给Transformer模型强制性 inductive bias，从而提高系统泛化和少量数据学习的能力，特别是 для FST-like 任务。<details>
<summary>Abstract</summary>
Strong inductive biases enable learning from little data and help generalization outside of the training distribution. Popular neural architectures such as Transformers lack strong structural inductive biases for seq2seq NLP tasks on their own. Consequently, they struggle with systematic generalization beyond the training distribution, e.g. with extrapolating to longer inputs, even when pre-trained on large amounts of text. We show how a structural inductive bias can be injected into a seq2seq model by pre-training it to simulate structural transformations on synthetic data. Specifically, we inject an inductive bias towards Finite State Transducers (FSTs) into a Transformer by pre-training it to simulate FSTs given their descriptions. Our experiments show that our method imparts the desired inductive bias, resulting in improved systematic generalization and better few-shot learning for FST-like tasks.
</details>
<details>
<summary>摘要</summary>
强大的推导偏好可以帮助学习从少量数据中学习和泛化到训练分布之外。流行的神经网络架构如Transformer在seq2seq NLP任务中缺乏强制性的推导偏好，因此在训练分布之外的泛化方面会遇到困难，如 extrapolating 到更长的输入。我们示示了如何通过在模型中注入结构偏好来增强 seq2seq 模型的泛化能力。特别是，我们将 Transformer 模型预训练以模拟 Finite State Transducers (FSTs) 的结构变换。我们的实验结果表明，我们的方法可以增强模型的泛化能力和几拘学习能力，特别是在 FST-like 任务中。
</details></li>
</ul>
<hr>
<h2 id="Testing-the-Limits-of-Unified-Sequence-to-Sequence-LLM-Pretraining-on-Diverse-Table-Data-Tasks"><a href="#Testing-the-Limits-of-Unified-Sequence-to-Sequence-LLM-Pretraining-on-Diverse-Table-Data-Tasks" class="headerlink" title="Testing the Limits of Unified Sequence to Sequence LLM Pretraining on Diverse Table Data Tasks"></a>Testing the Limits of Unified Sequence to Sequence LLM Pretraining on Diverse Table Data Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00789">http://arxiv.org/abs/2310.00789</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soumajyoti Sarkar, Leonard Lausen</li>
<li>for: 本研究旨在开发一个可以解决多种表格任务的模型方法，包括Semantic parsing、问题回答和分类问题。</li>
<li>methods: 我们使用了encoder-decoder式的大型自然语言模型（LLMs）来解决这些任务。我们在预训阶段共同将不同任务的模型整合到一个模型中，以提高模型的通用性和效率。</li>
<li>results: 我们通过多个减少研究发现，预训自我的目标可以大幅提高模型的表格特定任务表现。例如，我们发现在表格内容中训练的文本问题回答（QA）模型，虽然已经特化了，但仍然有很大的改善空间。我们的研究是首次尝试将表格特定预训扩展到770M至11B字串处理器模型，并与对表格数据进行特化的模型进行比较。<details>
<summary>Abstract</summary>
Tables stored in databases and tables which are present in web pages and articles account for a large part of semi-structured data that is available on the internet. It then becomes pertinent to develop a modeling approach with large language models (LLMs) that can be used to solve diverse table tasks such as semantic parsing, question answering as well as classification problems. Traditionally, there existed separate models specialized for each task individually. It raises the question of how far can we go to build a unified model that works well on some table tasks without significant degradation on others. To that end, we attempt at creating a shared modeling approach in the pretraining stage with encoder-decoder style LLMs that can cater to diverse tasks. We evaluate our approach that continually pretrains and finetunes different model families of T5 with data from tables and surrounding context, on these downstream tasks at different model scales. Through multiple ablation studies, we observe that our pretraining with self-supervised objectives can significantly boost the performance of the models on these tasks. As an example of one improvement, we observe that the instruction finetuned public models which come specialized on text question answering (QA) and have been trained on table data still have room for improvement when it comes to table specific QA. Our work is the first attempt at studying the advantages of a unified approach to table specific pretraining when scaled from 770M to 11B sequence to sequence models while also comparing the instruction finetuned variants of the models.
</details>
<details>
<summary>摘要</summary>
《文档存储在数据库和网页上的表格占据互联网上很大一部分半结构化数据。随后，我们需要开发一种模型方法，使用大型自然语言模型（LLM）来解决多种表格任务，如semantic parsing、问答以及分类问题。过去，我们有着专门为每个任务设计的单独模型。这引发了我们是否可以建立一个统一的模型，可以在不同任务之间无需重大下降性的情况下工作。为此，我们尝试了在预训练阶段使用encoder-decoder式LLM来建立共享模型方法，可以满足多种任务。我们通过多个缺省研究发现，我们的预训练自然语言对象可以显著提高模型在这些任务上的性能。例如，我们发现，通过训练文本问答（QA）模型，并将其特化为表格数据，仍然可以进一步提高表格特定的QA表现。我们的工作是首次研究表格特定预训练的优点，在扩展自然语言模型规模从770M到11B时进行比较，同时对特定 instrucion 的训练过程进行比较。》Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Analyzing-and-Mitigating-Object-Hallucination-in-Large-Vision-Language-Models"><a href="#Analyzing-and-Mitigating-Object-Hallucination-in-Large-Vision-Language-Models" class="headerlink" title="Analyzing and Mitigating Object Hallucination in Large Vision-Language Models"></a>Analyzing and Mitigating Object Hallucination in Large Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00754">http://arxiv.org/abs/2310.00754</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yiyangzhou/lure">https://github.com/yiyangzhou/lure</a></li>
<li>paper_authors: Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, Huaxiu Yao</li>
<li>for: 该研究旨在解决大规模视语言模型（LVLM）中的对象幻觉问题，以提高视语言任务的精度和可靠性。</li>
<li>methods: 该研究提出了一种简单 yet powerful的算法——LVLM Hallucination Revisor（LURE），通过重建较少幻觉的描述来修正LVLM中的对象幻觉。LURE基于对对象幻觉的主要因素进行了严格的统计分析，包括相伴（图像中certain object的频繁出现）、uncertainty（LVLM解码过程中对象的不确定性）和object position（幻觉通常出现在生成文本的后半部分）。</li>
<li>results: 该研究在六个开源LVLM中测试了LURE，并取得了23%的全面对象幻觉评价指标提升，比前一个最佳方法更高。在GPT和人类评估中，LURE一直 ranks at the top。数据和代码可以在<a target="_blank" rel="noopener" href="https://github.com/YiyangZhou/LURE%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/YiyangZhou/LURE上获取。</a><details>
<summary>Abstract</summary>
Large vision-language models (LVLMs) have shown remarkable abilities in understanding visual information with human languages. However, LVLMs still suffer from object hallucination, which is the problem of generating descriptions that include objects that do not actually exist in the images. This can negatively impact many vision-language tasks, such as visual summarization and reasoning. To address this issue, we propose a simple yet powerful algorithm, LVLM Hallucination Revisor (LURE), to post-hoc rectify object hallucination in LVLMs by reconstructing less hallucinatory descriptions. LURE is grounded in a rigorous statistical analysis of the key factors underlying object hallucination, including co-occurrence (the frequent appearance of certain objects alongside others in images), uncertainty (objects with higher uncertainty during LVLM decoding), and object position (hallucination often appears in the later part of the generated text). LURE can also be seamlessly integrated with any LVLMs. We evaluate LURE on six open-source LVLMs, achieving a 23% improvement in general object hallucination evaluation metrics over the previous best approach. In both GPT and human evaluations, LURE consistently ranks at the top. Our data and code are available at https://github.com/YiyangZhou/LURE.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)大量视力语言模型（LVLM）已经表现出了对人类语言的Visual Information的强大理解能力。然而，LVLM仍然受到对象幻觉的困扰，即生成包含不存在于图像中的对象的描述。这可能会对视力语言任务产生负面影响，如视觉概要和理解。为解决这个问题，我们提议一种简单 yet powerful的算法，即LVLM幻觉修正器（LURE），以后期修正LVLM中的对象幻觉。LURE基于对对象幻觉的关键因素进行了严格的统计分析，包括共occurrence（图像中certain对象的频繁出现）、uncertainty（LVLM解码过程中对象的高度不确定性）和object position（幻觉通常在生成文本的后半部分出现）。此外，LURE还可以与任何LVLM集成。我们对六个开源LVLM进行评估，实现了以往最佳方法的23%提升。在GPT和人类评估中，LURE也一直 ranked at the top。我们的数据和代码可以在https://github.com/YiyangZhou/LURE中获得。
</details></li>
</ul>
<hr>
<h2 id="FELM-Benchmarking-Factuality-Evaluation-of-Large-Language-Models"><a href="#FELM-Benchmarking-Factuality-Evaluation-of-Large-Language-Models" class="headerlink" title="FELM: Benchmarking Factuality Evaluation of Large Language Models"></a>FELM: Benchmarking Factuality Evaluation of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00741">http://arxiv.org/abs/2310.00741</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hkust-nlp/felm">https://github.com/hkust-nlp/felm</a></li>
<li>paper_authors: Shiqi Chen, Yiran Zhao, Jinghan Zhang, I-Chun Chern, Siyang Gao, Pengfei Liu, Junxian He</li>
<li>for: 这个研究的目的是评估大型自然语言模型（LLM）生成的文本是否准确，以便警示用户可能存在错误并促进更可靠的LLM发展。</li>
<li>methods: 这个研究使用了一个新的benchmark，称为felm，来评估LLM的准确性。这个benchmark包括从世界知识到数学和逻辑等多个领域的准确性标签，并且使用文本段来帮助特定错误的发现。</li>
<li>results: 研究发现，虽然 Retrieval 可以帮助factuality evaluation，但目前的LLM仍然远远不够，无法准确检测factual errors。<details>
<summary>Abstract</summary>
Assessing factuality of text generated by large language models (LLMs) is an emerging yet crucial research area, aimed at alerting users to potential errors and guiding the development of more reliable LLMs. Nonetheless, the evaluators assessing factuality necessitate suitable evaluation themselves to gauge progress and foster advancements. This direction remains under-explored, resulting in substantial impediments to the progress of factuality evaluators. To mitigate this issue, we introduce a benchmark for Factuality Evaluation of large Language Models, referred to as felm. In this benchmark, we collect responses generated from LLMs and annotate factuality labels in a fine-grained manner. Contrary to previous studies that primarily concentrate on the factuality of world knowledge (e.g.~information from Wikipedia), felm focuses on factuality across diverse domains, spanning from world knowledge to math and reasoning. Our annotation is based on text segments, which can help pinpoint specific factual errors. The factuality annotations are further supplemented by predefined error types and reference links that either support or contradict the statement. In our experiments, we investigate the performance of several LLM-based factuality evaluators on felm, including both vanilla LLMs and those augmented with retrieval mechanisms and chain-of-thought processes. Our findings reveal that while retrieval aids factuality evaluation, current LLMs are far from satisfactory to faithfully detect factual errors.
</details>
<details>
<summary>摘要</summary>
正在评估大语言模型（LLM）生成的文本真实性是一个emerging yet crucial的研究领域，旨在警示用户 potential errors和导向更可靠的 LLM 发展。然而，评估真实性的评估人员自己也需要适当的评估，以便衡量进步和促进进步。这个方向还未得到充分的探索，导致 LLM 的发展受到了重大的阻碍。为了解决这个问题，我们提出了一个大语言模型真实性评估标准（Felm）。在这个标准中，我们收集了由 LLM 生成的回答，并对其进行细化的标签分类。与前一些研究主要集中于世界知识（例如Wikipedia）的真实性，felm 强调在多个领域中的真实性，包括世界知识、数学和逻辑。我们的标注基于文本段，可以帮助特定的错误找到。真实性标注还得到了预定义的错误类型和参考链接，这些链接可以支持或反对声明。在我们的实验中，我们调查了一些基于 LLM 的真实性评估器在 felm 上的表现，包括基于 vanilla LLM 和增强了检索机制和链式思维的 LLM。我们的发现表明，虽然检索可以帮助真实性评估，但目前的 LLM 还远不够可靠地检测错误。
</details></li>
</ul>
<hr>
<h2 id="Robust-Sentiment-Analysis-for-Low-Resource-languages-Using-Data-Augmentation-Approaches-A-Case-Study-in-Marathi"><a href="#Robust-Sentiment-Analysis-for-Low-Resource-languages-Using-Data-Augmentation-Approaches-A-Case-Study-in-Marathi" class="headerlink" title="Robust Sentiment Analysis for Low Resource languages Using Data Augmentation Approaches: A Case Study in Marathi"></a>Robust Sentiment Analysis for Low Resource languages Using Data Augmentation Approaches: A Case Study in Marathi</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00734">http://arxiv.org/abs/2310.00734</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aabha Pingle, Aditya Vyawahare, Isha Joshi, Rahul Tangsali, Geetanjali Kale, Raviraj Joshi</li>
<li>for: 本研究旨在提高低资源语言 sentiment 分析的表现，特别是对印度语言 Marathi 进行了一项全面的数据扩充研究。</li>
<li>methods: 本文提出了四种数据扩充技术，包括 paraphrasing、back-translation、BERT 基于随机Token 替换和 named entity 替换，以及 GPT 基于文本和标签生成。</li>
<li>results: 研究结果显示，这些数据扩充方法可以提高 Marathi 语言的 sentiment 分析模型在跨频道情况下的表现，并且这些技术可以扩展到其他低资源语言和普通文本分类任务。<details>
<summary>Abstract</summary>
Sentiment analysis plays a crucial role in understanding the sentiment expressed in text data. While sentiment analysis research has been extensively conducted in English and other Western languages, there exists a significant gap in research efforts for sentiment analysis in low-resource languages. Limited resources, including datasets and NLP research, hinder the progress in this area. In this work, we present an exhaustive study of data augmentation approaches for the low-resource Indic language Marathi. Although domain-specific datasets for sentiment analysis in Marathi exist, they often fall short when applied to generalized and variable-length inputs. To address this challenge, this research paper proposes four data augmentation techniques for sentiment analysis in Marathi. The paper focuses on augmenting existing datasets to compensate for the lack of sufficient resources. The primary objective is to enhance sentiment analysis model performance in both in-domain and cross-domain scenarios by leveraging data augmentation strategies. The data augmentation approaches proposed showed a significant performance improvement for cross-domain accuracies. The augmentation methods include paraphrasing, back-translation; BERT-based random token replacement, named entity replacement, and pseudo-label generation; GPT-based text and label generation. Furthermore, these techniques can be extended to other low-resource languages and for general text classification tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Evaluating-Speech-Synthesis-by-Training-Recognizers-on-Synthetic-Speech"><a href="#Evaluating-Speech-Synthesis-by-Training-Recognizers-on-Synthetic-Speech" class="headerlink" title="Evaluating Speech Synthesis by Training Recognizers on Synthetic Speech"></a>Evaluating Speech Synthesis by Training Recognizers on Synthetic Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00706">http://arxiv.org/abs/2310.00706</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dareen Alharthi, Roshan Sharma, Hira Dhamyal, Soumi Maiti, Bhiksha Raj, Rita Singh</li>
<li>for: 这篇论文的目的是提出一种用于评估文本到语音转化系统的自动评价方法，以取代人工评估方法。</li>
<li>methods: 这篇论文使用的方法是使用一个自然语言处理模型来训练一个语音识别模型，然后使用这个语音识别模型来评估文本到语音转化系统的质量。</li>
<li>results: 这篇论文的结果表明，使用这种方法可以对文本到语音转化系统的质量进行更广泛的评估，而不是仅仅是测试语音智能的准确率。此外，这种方法还可以与人工评估方法相比肩，并且可以减少人工评估的成本。<details>
<summary>Abstract</summary>
Modern speech synthesis systems have improved significantly, with synthetic speech being indistinguishable from real speech. However, efficient and holistic evaluation of synthetic speech still remains a significant challenge. Human evaluation using Mean Opinion Score (MOS) is ideal, but inefficient due to high costs. Therefore, researchers have developed auxiliary automatic metrics like Word Error Rate (WER) to measure intelligibility. Prior works focus on evaluating synthetic speech based on pre-trained speech recognition models, however, this can be limiting since this approach primarily measures speech intelligibility. In this paper, we propose an evaluation technique involving the training of an ASR model on synthetic speech and assessing its performance on real speech. Our main assumption is that by training the ASR model on the synthetic speech, the WER on real speech reflects the similarity between distributions, a broader assessment of synthetic speech quality beyond intelligibility. Our proposed metric demonstrates a strong correlation with both MOS naturalness and MOS intelligibility when compared to SpeechLMScore and MOSNet on three recent Text-to-Speech (TTS) systems: MQTTS, StyleTTS, and YourTTS.
</details>
<details>
<summary>摘要</summary>
现代语音合成系统已经进步很 significatively，Synthetic speech 和 real speech 之间的差别已经变得极其微scopic。然而，efficiently and holistically evaluate synthetic speech 仍然是一个主要挑战。人工评分使用 Mean Opinion Score (MOS) 是理想的，但是它的成本很高。因此，研究人员已经开发了auxiliary automatic metrics like Word Error Rate (WER) 来度量语音明亮度。先前的研究主要基于使用预训练的speech recognition 模型来评估合成语音的质量，但这种方法只能测量语音的elligibility。在这篇论文中，我们提出了一种评估技术，即使用 ASR 模型来训练 synthetic speech，并用其在真实语音上的性能来度量合成语音的质量。我们的主要假设是，通过训练 ASR 模型使 synthetic speech 与 real speech 之间的分布相似，那么 WER 在真实语音上的性能将反映合成语音的质量，不仅是语音可理解性。我们的提出的度量与 MOS naturalness 和 MOS intelligibility 具有强相关性，并且在三个 latest Text-to-Speech (TTS) 系统（MQTTS、StyleTTS 和 YourTTS）上进行了比较。
</details></li>
</ul>
<hr>
<h2 id="Do-the-Benefits-of-Joint-Models-for-Relation-Extraction-Extend-to-Document-level-Tasks"><a href="#Do-the-Benefits-of-Joint-Models-for-Relation-Extraction-Extend-to-Document-level-Tasks" class="headerlink" title="Do the Benefits of Joint Models for Relation Extraction Extend to Document-level Tasks?"></a>Do the Benefits of Joint Models for Relation Extraction Extend to Document-level Tasks?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00696">http://arxiv.org/abs/2310.00696</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pratik Saini, Tapas Nayak, Indrajit Bhattacharya</li>
<li>for: 这篇论文主要针对的是关系 triple 提取的 sentence-level 和 document-level 任务。</li>
<li>methods: 论文提出了两种不同的方法：pipeline 和 joint。joint 模型可以捕捉到关系之间的交互，在 sentence-level 任务上显示出了更高的性能。</li>
<li>results: 实验结果表明，joint 模型在 sentence-level 任务上比 pipeline 模型显示出了更高的性能，但是在 document-level 任务上，joint 模型的性能下降了，与 pipeline 模型的性能相比。<details>
<summary>Abstract</summary>
Two distinct approaches have been proposed for relational triple extraction - pipeline and joint. Joint models, which capture interactions across triples, are the more recent development, and have been shown to outperform pipeline models for sentence-level extraction tasks. Document-level extraction is a more challenging setting where interactions across triples can be long-range, and individual triples can also span across sentences. Joint models have not been applied for document-level tasks so far. In this paper, we benchmark state-of-the-art pipeline and joint extraction models on sentence-level as well as document-level datasets. Our experiments show that while joint models outperform pipeline models significantly for sentence-level extraction, their performance drops sharply below that of pipeline models for the document-level dataset.
</details>
<details>
<summary>摘要</summary>
两种不同的方法有被提议用于关系三元EXTRACT - 管道和共同。共同模型， capture关系三元之间的互动，是更新的发展，并在句子级EXTRACT任务中显示出perform得到更好的结果。文档级EXTRACT是一个更加复杂的设定， где交互关系可以是长距离的，并且每个三元也可以跨 sentence。共同模型没有被应用于文档级任务上。在这篇文章中，我们对 sentence级和文档级的EXTRACT模型进行了比较。我们的实验结果表明，虽然共同模型在句子级EXTRACT任务上表现明显 луч于管道模型，但是对文档级数据集的性能下降了很多。
</details></li>
</ul>
<hr>
<h2 id="CebuaNER-A-New-Baseline-Cebuano-Named-Entity-Recognition-Model"><a href="#CebuaNER-A-New-Baseline-Cebuano-Named-Entity-Recognition-Model" class="headerlink" title="CebuaNER: A New Baseline Cebuano Named Entity Recognition Model"></a>CebuaNER: A New Baseline Cebuano Named Entity Recognition Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00679">http://arxiv.org/abs/2310.00679</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mebzmoren/cebuaner">https://github.com/mebzmoren/cebuaner</a></li>
<li>paper_authors: Ma. Beatrice Emanuela Pilar, Ellyza Mari Papas, Mary Loise Buenaventura, Dane Dedoroy, Myron Darrel Montefalcon, Jay Rhald Padilla, Lany Maceda, Mideth Abisado, Joseph Marvin Imperial</li>
<li>for: 这个研究的目的是为了提供一个基线模型 для缅甸语名实体识别（NER）任务。</li>
<li>methods: 这个研究使用了Conditional Random Field和Bidirectional LSTM算法来适应缅甸语文本，并对4000份当地新闻文章进行了标注和训练。</li>
<li>results: 研究发现这个基线模型在精度、准确率和F1指标上达到了70%以上，并且在跨语言设置下与标准模型进行比较表现良好。<details>
<summary>Abstract</summary>
Despite being one of the most linguistically diverse groups of countries, computational linguistics and language processing research in Southeast Asia has struggled to match the level of countries from the Global North. Thus, initiatives such as open-sourcing corpora and the development of baseline models for basic language processing tasks are important stepping stones to encourage the growth of research efforts in the field. To answer this call, we introduce CebuaNER, a new baseline model for named entity recognition (NER) in the Cebuano language. Cebuano is the second most-used native language in the Philippines, with over 20 million speakers. To build the model, we collected and annotated over 4,000 news articles, the largest of any work in the language, retrieved from online local Cebuano platforms to train algorithms such as Conditional Random Field and Bidirectional LSTM. Our findings show promising results as a new baseline model, achieving over 70% performance on precision, recall, and F1 across all entity tags, as well as potential efficacy in a crosslingual setup with Tagalog.
</details>
<details>
<summary>摘要</summary>
Despite being one of the most linguistically diverse regions in the world, computational linguistics and language processing research in Southeast Asia has struggled to keep up with the level of countries from the Global North. To address this challenge, initiatives such as open-sourcing corpora and developing baseline models for basic language processing tasks are crucial stepping stones to encourage the growth of research efforts in the field. In response to this call, we introduce CebuaNER, a new baseline model for named entity recognition (NER) in the Cebuano language. Cebuano is the second most widely spoken native language in the Philippines, with over 20 million speakers. To build the model, we collected and annotated over 4,000 news articles, the largest dataset of any work in the language, retrieved from online local Cebuano platforms and trained algorithms such as Conditional Random Field and Bidirectional LSTM. Our findings show promising results as a new baseline model, achieving over 70% performance on precision, recall, and F1 across all entity tags, as well as potential efficacy in a crosslingual setup with Tagalog.
</details></li>
</ul>
<hr>
<h2 id="GeRA-Label-Efficient-Geometrically-Regularized-Alignment"><a href="#GeRA-Label-Efficient-Geometrically-Regularized-Alignment" class="headerlink" title="GeRA: Label-Efficient Geometrically Regularized Alignment"></a>GeRA: Label-Efficient Geometrically Regularized Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00672">http://arxiv.org/abs/2310.00672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dustin Klebe, Tal Shnitzer, Mikhail Yurochkin, Leonid Karlinsky, Justin Solomon</li>
<li>for: 这 paper 的目的是为了减少对彩色数据的需求，并在label-efficient的情况下进行多modal embedding空间的Alignment。</li>
<li>methods: 这 paper 使用了一种名为 Geometrically Regularized Alignment (GeRA) 的 semi-supervised方法，该方法利用了无关数据的演化geometry来改进对 embedding 空间的Alignment。</li>
<li>results:  experiments 表明，GeRA 方法在 speech-text 和 image-text 领域中表现出了明显的改进，特别是使用小量对数据的 paired data。<details>
<summary>Abstract</summary>
Pretrained unimodal encoders incorporate rich semantic information into embedding space structures. To be similarly informative, multi-modal encoders typically require massive amounts of paired data for alignment and training. We introduce a semi-supervised Geometrically Regularized Alignment (GeRA) method to align the embedding spaces of pretrained unimodal encoders in a label-efficient way. Our method leverages the manifold geometry of unpaired (unlabeled) data to improve alignment performance. To prevent distortions to local geometry during the alignment process, potentially disrupting semantic neighborhood structures and causing misalignment of unobserved pairs, we introduce a geometric loss term. This term is built upon a diffusion operator that captures the local manifold geometry of the unimodal pretrained encoders. GeRA is modality-agnostic and thus can be used to align pretrained encoders from any data modalities. We provide empirical evidence to the effectiveness of our method in the domains of speech-text and image-text alignment. Our experiments demonstrate significant improvement in alignment quality compared to a variaty of leading baselines, especially with a small amount of paired data, using our proposed geometric regularization.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate the following text into Simplified Chinese:Pretrained unimodal encoders incorporate rich semantic information into embedding space structures. To be similarly informative, multi-modal encoders typically require massive amounts of paired data for alignment and training. We introduce a semi-supervised Geometrically Regularized Alignment (GeRA) method to align the embedding spaces of pretrained unimodal encoders in a label-efficient way. Our method leverages the manifold geometry of unpaired (unlabeled) data to improve alignment performance. To prevent distortions to local geometry during the alignment process, potentially disrupting semantic neighborhood structures and causing misalignment of unobserved pairs, we introduce a geometric loss term. This term is built upon a diffusion operator that captures the local manifold geometry of the unimodal pretrained encoders. GeRA is modality-agnostic and thus can be used to align pretrained encoders from any data modalities. We provide empirical evidence to the effectiveness of our method in the domains of speech-text and image-text alignment. Our experiments demonstrate significant improvement in alignment quality compared to a variety of leading baselines, especially with a small amount of paired data, using our proposed geometric regularization.Translate the text into Simplified Chinese: preprained 单modal encoders 含有丰富的 semantic 信息，将 embedding 空间结构中的信息升级为多modal  encoders 需要巨量的对应数据对Alignment和training。我们介绍了一种 semi-supervised 的 Geometrically Regularized Alignment (GeRA) 方法，用于对 preprained 单modal encoders 的 embedding 空间进行标签效率的对Alignment。我们的方法利用了无对应数据的 manifold geometry，以提高对Alignment的性能。为避免对Local geometry的扭曲，可能导致 semantic 邻居结构的扰乱和未观察对的歪曲，我们引入了一个 geometric 损失项。这个项目基于一个 diffusion 算子，捕捉了单modal 预训练 encoders 的 Local manifold geometry。GeRA 是modal-agnostic，因此可以用于对任何数据模式的预训练 encoders 进行对Alignment。我们提供了实验证明我们的方法在speech-text 和 image-text 对Alignment中的效果。我们的实验表明，使用我们提posed的 geometric 正则化可以在小量对数据情况下达到显著提高对Alignment质量的效果，特别是与多种主流基准值进行比较。
</details></li>
</ul>
<hr>
<h2 id="Fewer-is-More-Trojan-Attacks-on-Parameter-Efficient-Fine-Tuning"><a href="#Fewer-is-More-Trojan-Attacks-on-Parameter-Efficient-Fine-Tuning" class="headerlink" title="Fewer is More: Trojan Attacks on Parameter-Efficient Fine-Tuning"></a>Fewer is More: Trojan Attacks on Parameter-Efficient Fine-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00648">http://arxiv.org/abs/2310.00648</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lauren Hong, Ting Wang</li>
<li>for: This paper explores the security implications of parameter-efficient fine-tuning (PEFT) for pre-trained language models (PLMs), and reveals a novel attack called PETA that can successfully inject a backdoor into a PLM using PEFT.</li>
<li>methods: The attack uses bilevel optimization to embed a backdoor into a PLM while retaining the PLM’s task-specific performance, and the defense omits PEFT in selected layers of the backdoored PLM and unfreezes a subset of these layers’ parameters to neutralize the attack.</li>
<li>results: The attack is effective in terms of both attack success rate and unaffected clean accuracy, even after the victim user performs PEFT over the backdoored PLM using untainted data. The defense is effective in neutralizing the attack.Here is the summary in Traditional Chinese:</li>
<li>for: 本研究探讨parameter-efficient fine-tuning (PEFT)所带来的安全问题，并发现了一种称为PETA的攻击，可以成功地将backdoor注入到pre-trained language models (PLMs)中。</li>
<li>methods: 这个攻击使用了 bilateral optimization来嵌入backdoor到PLM中，并保持PLM的任务特定性能。防御方法是将PEFT快照在选择的层中，并将这些层的parameters解冻。</li>
<li>results: 这个攻击具有成功率和不受污染的清洁率，甚至在受害者使用不混合的数据进行PEFT后仍然有效。防御方法能够有效地中和攻击。<details>
<summary>Abstract</summary>
Parameter-efficient fine-tuning (PEFT) enables efficient adaptation of pre-trained language models (PLMs) to specific tasks. By tuning only a minimal set of (extra) parameters, PEFT achieves performance comparable to full fine-tuning. However, despite its prevalent use, the security implications of PEFT remain largely unexplored. In this paper, we conduct a pilot study revealing that PEFT exhibits unique vulnerability to trojan attacks. Specifically, we present PETA, a novel attack that accounts for downstream adaptation through bilevel optimization: the upper-level objective embeds the backdoor into a PLM while the lower-level objective simulates PEFT to retain the PLM's task-specific performance. With extensive evaluation across a variety of downstream tasks and trigger designs, we demonstrate PETA's effectiveness in terms of both attack success rate and unaffected clean accuracy, even after the victim user performs PEFT over the backdoored PLM using untainted data. Moreover, we empirically provide possible explanations for PETA's efficacy: the bilevel optimization inherently 'orthogonalizes' the backdoor and PEFT modules, thereby retaining the backdoor throughout PEFT. Based on this insight, we explore a simple defense that omits PEFT in selected layers of the backdoored PLM and unfreezes a subset of these layers' parameters, which is shown to effectively neutralize PETA.
</details>
<details>
<summary>摘要</summary>
parameter-efficient fine-tuning (PEFT) 可以快速地适应预训练语言模型 (PLM) 到特定任务。通过只调整一小部分 (Extra) 的参数，PEFT 可以达到与全面 fine-tuning 相同的性能。然而，尽管它在广泛使用，PEFT 的安全性问题仍然未得到足够的探讨。在这篇论文中，我们进行了一个小型研究，揭示 PEFT 存在独特的潜在攻击点。 Specifically, we present PETA, a novel attack that accounts for downstream adaptation through bilevel optimization: the upper-level objective embeds the backdoor into a PLM while the lower-level objective simulates PEFT to retain the PLM's task-specific performance. With extensive evaluation across a variety of downstream tasks and trigger designs, we demonstrate PETA's effectiveness in terms of both attack success rate and unaffected clean accuracy, even after the victim user performs PEFT over the backdoored PLM using untainted data. Moreover, we empirically provide possible explanations for PETA's efficacy: the bilevel optimization inherently 'orthogonalizes' the backdoor and PEFT modules, thereby retaining the backdoor throughout PEFT. Based on this insight, we explore a simple defense that omits PEFT in selected layers of the backdoored PLM and unfreezes a subset of these layers' parameters, which is shown to effectively neutralize PETA.
</details></li>
</ul>
<hr>
<h2 id="Wavelet-Scattering-Transform-for-Improving-Generalization-in-Low-Resourced-Spoken-Language-Identification"><a href="#Wavelet-Scattering-Transform-for-Improving-Generalization-in-Low-Resourced-Spoken-Language-Identification" class="headerlink" title="Wavelet Scattering Transform for Improving Generalization in Low-Resourced Spoken Language Identification"></a>Wavelet Scattering Transform for Improving Generalization in Low-Resourced Spoken Language Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00602">http://arxiv.org/abs/2310.00602</a></li>
<li>repo_url: None</li>
<li>paper_authors: Spandan Dey, Premjeet Singh, Goutam Saha</li>
<li>for: 提高低资源语音认识系统的泛化性</li>
<li>methods: 使用浪涌扩散变换（WST）来代替通常使用的MEL-spectrogram或MFCC特征，以弥补高频信息损失问题</li>
<li>results: 与MFCC相比，使用WST特征可以降低识别错误率，最多降低14.05%和6.40% для同一 corpus和隐藏 VoxLingua107评估 respectively<details>
<summary>Abstract</summary>
Commonly used features in spoken language identification (LID), such as mel-spectrogram or MFCC, lose high-frequency information due to windowing. The loss further increases for longer temporal contexts. To improve generalization of the low-resourced LID systems, we investigate an alternate feature representation, wavelet scattering transform (WST), that compensates for the shortcomings. To our knowledge, WST is not explored earlier in LID tasks. We first optimize WST features for multiple South Asian LID corpora. We show that LID requires low octave resolution and frequency-scattering is not useful. Further, cross-corpora evaluations show that the optimal WST hyper-parameters depend on both train and test corpora. Hence, we develop fused ECAPA-TDNN based LID systems with different sets of WST hyper-parameters to improve generalization for unknown data. Compared to MFCC, EER is reduced upto 14.05% and 6.40% for same-corpora and blind VoxLingua107 evaluations, respectively.
</details>
<details>
<summary>摘要</summary>
通常使用的语音识别（LID）任务中的特征，如MEL-spectrogram或MFCC，因窗口效应而产生高频信息损失，这种损失随着时间上下文的增加而加大。为了改善低资源的LID系统的通用性，我们 investigate了一种 alternate 特征表示，wavelet scattering transform（WST），该表示可以补偿这些缺点。据我们所知，WST在LID任务中没有被探索过。我们首先优化WST特征 для多个南亚语言LID corpus。我们发现，LID需要低 octave 分辨率，而频率散射并不是有用。此外，跨 corpus 评估表明，优化 WST 超参数取决于训练和测试 corpus。因此，我们开发了 fusion ECAPA-TDNN 基于 WST 的 LID 系统，以提高对不知数据的泛化性。相比 MFCC，我们在同一 corpora 和 blind VoxLingua107 评估中分别减少了 EER 14.05% 和 6.40%。
</details></li>
</ul>
<hr>
<h2 id="A-Task-oriented-Dialog-Model-with-Task-progressive-and-Policy-aware-Pre-training"><a href="#A-Task-oriented-Dialog-Model-with-Task-progressive-and-Policy-aware-Pre-training" class="headerlink" title="A Task-oriented Dialog Model with Task-progressive and Policy-aware Pre-training"></a>A Task-oriented Dialog Model with Task-progressive and Policy-aware Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00597">http://arxiv.org/abs/2310.00597</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lucenzhong/tpld">https://github.com/lucenzhong/tpld</a></li>
<li>paper_authors: Lucen Zhong, Hengtong Lu, Caixia Yuan, Xiaojie Wang, Jiashen Sun, Ke Zeng, Guanglu Wan</li>
<li>for: 提高任务对话（TOD）相关任务的顺序性和对话策略学习</li>
<li>methods: 使用两种策略相关预训练任务进行预训练，包括全球策略一致性任务和行为相似学习任务</li>
<li>results: 在多个WOZ和车辆内端对话模型评价标准中表现更好，只使用18%的参数和25%的预训练数据，与之前的状态当前PCMGALAXY相比<details>
<summary>Abstract</summary>
Pre-trained conversation models (PCMs) have achieved promising progress in recent years. However, existing PCMs for Task-oriented dialog (TOD) are insufficient for capturing the sequential nature of the TOD-related tasks, as well as for learning dialog policy information. To alleviate these problems, this paper proposes a task-progressive PCM with two policy-aware pre-training tasks. The model is pre-trained through three stages where TOD-related tasks are progressively employed according to the task logic of the TOD system. A global policy consistency task is designed to capture the multi-turn dialog policy sequential relation, and an act-based contrastive learning task is designed to capture similarities among samples with the same dialog policy. Our model achieves better results on both MultiWOZ and In-Car end-to-end dialog modeling benchmarks with only 18\% parameters and 25\% pre-training data compared to the previous state-of-the-art PCM, GALAXY.
</details>
<details>
<summary>摘要</summary>
各种前置模型（PCM）在过去几年内已经取得了令人满意的进步。然而，现有的PCM对任务导向对话（TOD）不足以捕捉TOD相关任务的顺序性，以及对话策略信息的学习。为了解决这些问题，这篇论文提出了一种任务逐步进行的PCM，其中包括两个策略意识的预训练任务。模型在三个阶段中预训练，其中TOD相关任务逐步应用于TOD系统的任务逻辑。为了捕捉多Turn对话策略的顺序关系，我们设计了全球策略一致任务。同时，为了捕捉同一策略下的对话样本的相似性，我们设计了基于行为的对比学习任务。我们的模型在MultiWOZ和In-Car终端对话模型 benchmark上达到了之前的state-of-the-art PCMGALAXY的性能，但它只有18%的参数和25%的预训练数据。
</details></li>
</ul>
<hr>
<h2 id="Nine-year-old-children-outperformed-ChatGPT-in-emotion-Evidence-from-Chinese-writing"><a href="#Nine-year-old-children-outperformed-ChatGPT-in-emotion-Evidence-from-Chinese-writing" class="headerlink" title="Nine-year-old children outperformed ChatGPT in emotion: Evidence from Chinese writing"></a>Nine-year-old children outperformed ChatGPT in emotion: Evidence from Chinese writing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00578">http://arxiv.org/abs/2310.00578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyi Cao, Tongquan Zhou, Siruo Zhou</li>
<li>for: 本研究旨在探讨 chatGPT 和九岁儿童在中文写作方面的Relative优劣。</li>
<li>methods: 研究采用了对 narrative 和 scientific 主题的中文写作进行比较分析，并通过五种语言 dimensión 的评价指标来评估 chatGPT 和儿童的写作水平。</li>
<li>results: 结果显示 nine-year-old 儿童在 fluency 和 cohesion 方面的写作水平胜过 chatGPT，但 chatGPT 在 accuracy 方面表现出色。 children 在 science-themed 写作中表现出更高的 complexity，而 chatGPT 在 nature-themed 写作中表现出更高的 accuracy。 最重要的是，这项研究发现 nine-year-old 儿童在中文作文中表达的情感更强于 chatGPT。<details>
<summary>Abstract</summary>
ChatGPT has been demonstrated to possess significant capabilities in generating intricate, human-like text, and recent studies have established that its performance in theory of mind tasks is comparable to that of a nine-year-old child. However, it remains uncertain whether ChatGPT surpasses nine-year-old children in Chinese writing proficiency. To explore this, our study juxtaposed the Chinese writing performance of ChatGPT and nine-year-old children on both narrative and scientific topics, aiming to uncover the relative strengths and weaknesses of ChatGPT in writing.   The collected data were analyzed across five linguistic dimensions: fluency, accuracy, complexity, cohesion, and emotion. Each dimension underwent assessment through precise indices. The findings revealed that nine-year-old children excelled beyond ChatGPT in terms of fluency and cohesion within their writing. In contrast, ChatGPT manifested a superior performance in accuracy compared to the children. Concerning complexity, children exhibited superior skills in science-themed writing, while ChatGPT prevailed in nature-themed writing. Significantly, this research is pioneering in revealing that nine-year-old children convey stronger emotions than ChatGPT in their Chinese compositions.
</details>
<details>
<summary>摘要</summary>
chatGPT possess了较强的文本生成能力，并且研究表明其在理解人类思维方面的表现与9岁孩子相当。然而，是否chatGPT在中文写作方面超过9岁孩子仍然存在uncertainty。为了解答这个问题，我们的研究将chatGPT和9岁孩子的中文写作比较在 narative和科学话题上。我们通过分析5种语言特征，包括流畅、准确、复杂度、连贯和情感，来评估这两个组合的写作能力。我们发现，9岁孩子在流畅和连贯方面的写作能力比chatGPT更强，而chatGPT在准确性方面表现更优。在复杂度方面，孩子在科学话题上表现出了更高的技巧水平，而chatGPT在自然话题上表现更优。最重要的是，这项研究发现，9岁孩子在中文作文中表达的情感更强于chatGPT。
</details></li>
</ul>
<hr>
<h2 id="GrowLength-Accelerating-LLMs-Pretraining-by-Progressively-Growing-Training-Length"><a href="#GrowLength-Accelerating-LLMs-Pretraining-by-Progressively-Growing-Training-Length" class="headerlink" title="GrowLength: Accelerating LLMs Pretraining by Progressively Growing Training Length"></a>GrowLength: Accelerating LLMs Pretraining by Progressively Growing Training Length</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00576">http://arxiv.org/abs/2310.00576</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Chia-Yuan Chang, Xia Hu</li>
<li>for: 这篇论文是为了提高大型语言模型（LLMs）的预训程序 accelerate the pretraining process 的目的。</li>
<li>methods: 这篇论文提出了一个新的、简单、有效的方法 named“\growlength”，可以加速 LLMs 的预训过程。这个方法在预训过程中逐步增加训练序列长度，从而减少 computional costs 和提高效率。例如，它从128字串开始，逐步增加到4096字串。这种方法可以让模型在有限时间内处理更多的字串，并可能提高其性能。</li>
<li>results: 我们的实验结果显示，使用我们的方法训练 LLMs 可以更快地趋向于极值，并且比使用现有方法训练的模型表现更好。此外，我们的方法不需要任何额外的工程实践，因此是实际的解决方案在 LLMs 领域。<details>
<summary>Abstract</summary>
The evolving sophistication and intricacies of Large Language Models (LLMs) yield unprecedented advancements, yet they simultaneously demand considerable computational resources and incur significant costs. To alleviate these challenges, this paper introduces a novel, simple, and effective method named ``\growlength'' to accelerate the pretraining process of LLMs. Our method progressively increases the training length throughout the pretraining phase, thereby mitigating computational costs and enhancing efficiency. For instance, it begins with a sequence length of 128 and progressively extends to 4096. This approach enables models to process a larger number of tokens within limited time frames, potentially boosting their performance. In other words, the efficiency gain is derived from training with shorter sequences optimizing the utilization of resources. Our extensive experiments with various state-of-the-art LLMs have revealed that models trained using our method not only converge more swiftly but also exhibit superior performance metrics compared to those trained with existing methods. Furthermore, our method for LLMs pretraining acceleration does not require any additional engineering efforts, making it a practical solution in the realm of LLMs.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）的发展和复杂性带来了前所未有的进步，但它们同时需要很大的计算资源和成本。为了解决这些挑战，本文提出了一种新的、简单的和有效的方法名为“\growlength”，用于加速 LLMS 的预训练过程。我们的方法在预训练阶段逐步增长训练长度，从而减少计算成本并提高效率。例如，它从序列长度为 128 开始，逐步增长到 4096。这种方法使得模型在限时内处理更多的字符，可能提高其性能。换句话说，效率提升来自于在限时内训练使用资源的优化。我们对各种现代 LLMS 进行了广泛的实验，发现使用我们的方法训练的模型不仅更快 converges，而且也表现出了较高的性能指标，比于使用现有方法训练的模型。此外，我们的方法不需要任何额外的工程努力，因此是 LLMS 预训练加速方法中的实用解决方案。
</details></li>
</ul>
<hr>
<h2 id="Colloquial-Persian-POS-CPPOS-Corpus-A-Novel-Corpus-for-Colloquial-Persian-Part-of-Speech-Tagging"><a href="#Colloquial-Persian-POS-CPPOS-Corpus-A-Novel-Corpus-for-Colloquial-Persian-Part-of-Speech-Tagging" class="headerlink" title="Colloquial Persian POS (CPPOS) Corpus: A Novel Corpus for Colloquial Persian Part of Speech Tagging"></a>Colloquial Persian POS (CPPOS) Corpus: A Novel Corpus for Colloquial Persian Part of Speech Tagging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00572">http://arxiv.org/abs/2310.00572</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leyla Rabiei, Farzaneh Rahmani, Mohammad Khansari, Zeinab Rajabi, Moein Salimi</li>
<li>for: This paper is written for those interested in natural language processing and POS tagging in Persian, specifically for colloquial text in social network analysis.</li>
<li>methods: The paper introduces a novel corpus called “Colloquial Persian POS” (CPPOS), which includes formal and informal text collected from various social media platforms such as Telegram, Twitter, and Instagram. The corpus was manually annotated and verified by a team of linguistic experts, and a POS tagging guideline was defined for annotating the data.</li>
<li>results: The paper evaluates the quality of CPPOS by training various deep learning models, such as the RNN family, on the constructed corpus. The results show that the model trained on CPPOS outperforms other existing Persian POS corpora and tools, achieving a 14% improvement over the previous dataset.<details>
<summary>Abstract</summary>
Introduction: Part-of-Speech (POS) Tagging, the process of classifying words into their respective parts of speech (e.g., verb or noun), is essential in various natural language processing applications. POS tagging is a crucial preprocessing task for applications like machine translation, question answering, sentiment analysis, etc. However, existing corpora for POS tagging in Persian mainly consist of formal texts, such as daily news and newspapers. As a result, smart POS tools, machine learning models, and deep learning models trained on these corpora may not perform optimally for processing colloquial text in social network analysis. Method: This paper introduces a novel corpus, "Colloquial Persian POS" (CPPOS), specifically designed to support colloquial Persian text. The corpus includes formal and informal text collected from various domains such as political, social, and commercial on Telegram, Twitter, and Instagram more than 520K labeled tokens. After collecting posts from these social platforms for one year, special preprocessing steps were conducted, including normalization, sentence tokenizing, and word tokenizing for social text. The tokens and sentences were then manually annotated and verified by a team of linguistic experts. This study also defines a POS tagging guideline for annotating the data and conducting the annotation process. Results: To evaluate the quality of CPPOS, various deep learning models, such as the RNN family, were trained using the constructed corpus. A comparison with another well-known Persian POS corpus named "Bijankhan" and the Persian Hazm POS tool trained on Bijankhan revealed that our model trained on CPPOS outperforms them. With the new corpus and the BiLSTM deep neural model, we achieved a 14% improvement over the previous dataset.
</details>
<details>
<summary>摘要</summary>
Introduction: 部件之分标记（POS）标注，将词语分类为它们的各种部件（如动词或名词），是自然语言处理应用中的重要预处理任务。POS标注是机器翻译、问答、情感分析等应用中的关键预处理任务。然而，现有的波斯语POS标注 corpora主要由正式文本组成，如日报和报纸。这导致了聪明POS工具、机器学习模型和深度学习模型在处理社交网络分析中的混乱文本时可能不具备最佳性能。方法：本文介绍了一个新的 corpora，名为“通用波斯语POS”（CPPOS），用于支持通用波斯语文本。该 corpora 包括了正式和非正式文本，从各种领域，如政治、社会和商业，收集自 Telegram、Twitter 和 Instagram 等社交平台上的大于520K个标注的字符。在收集一年的社交媒体文本后，我们进行了特殊的预处理步骤，包括Normalization、句子分割和词语分割。这些字符和句子 THEN 被一群语言专家 manually annotate 和验证。本研究还定义了POS标注指南，用于标注数据并进行标注过程。结果：为评估 CPPOS 的质量，我们使用constructed corpora  trains 了多种深度学习模型，如 RNN 家族。与另一个已知的波斯语POS corpus名为“ Bijankhan” 和 Persian Hazm POS 工具在 Bijankhan 上训练而成的模型相比，我们的模型在 CPPOS 上训练的结果表明，我们的模型在 CPPOS 上训练的结果表明，我们的模型在 CPPOS 上训练的结果比之前的数据集提高了14%。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/01/cs.CL_2023_10_01/" data-id="clpxp6bz700cpee880mwr8wh1" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_10_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/01/cs.LG_2023_10_01/" class="article-date">
  <time datetime="2023-10-01T10:00:00.000Z" itemprop="datePublished">2023-10-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/01/cs.LG_2023_10_01/">cs.LG - 2023-10-01</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Determining-the-Optimal-Number-of-Clusters-for-Time-Series-Datasets-with-Symbolic-Pattern-Forest"><a href="#Determining-the-Optimal-Number-of-Clusters-for-Time-Series-Datasets-with-Symbolic-Pattern-Forest" class="headerlink" title="Determining the Optimal Number of Clusters for Time Series Datasets with Symbolic Pattern Forest"></a>Determining the Optimal Number of Clusters for Time Series Datasets with Symbolic Pattern Forest</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00820">http://arxiv.org/abs/2310.00820</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Nishat Raihan</li>
<li>for: 该论文的目的是提出一种基于Symbolic Pattern Forest（SPF）算法的时间序列嵌入分类方法，以确定时间序列数据集中的优化数量分支。</li>
<li>methods: 该方法使用了SPF算法生成时间序列数据集中的嵌入分类结果，并根据Silhouette系数选择优化数量分支。Silhouette系数在bag of word vector和tf-idf vector两个方面进行计算。</li>
<li>results: 对于UCRLibrary数据集，该方法实验结果表明与基准相比有显著改善。<details>
<summary>Abstract</summary>
Clustering algorithms are among the most widely used data mining methods due to their exploratory power and being an initial preprocessing step that paves the way for other techniques. But the problem of calculating the optimal number of clusters (say k) is one of the significant challenges for such methods. The most widely used clustering algorithms like k-means and k-shape in time series data mining also need the ground truth for the number of clusters that need to be generated. In this work, we extended the Symbolic Pattern Forest algorithm, another time series clustering algorithm, to determine the optimal number of clusters for the time series datasets. We used SPF to generate the clusters from the datasets and chose the optimal number of clusters based on the Silhouette Coefficient, a metric used to calculate the goodness of a clustering technique. Silhouette was calculated on both the bag of word vectors and the tf-idf vectors generated from the SAX words of each time series. We tested our approach on the UCR archive datasets, and our experimental results so far showed significant improvement over the baseline.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ECG-SL-Electrocardiogram-ECG-Segment-Learning-a-deep-learning-method-for-ECG-signal"><a href="#ECG-SL-Electrocardiogram-ECG-Segment-Learning-a-deep-learning-method-for-ECG-signal" class="headerlink" title="ECG-SL: Electrocardiogram(ECG) Segment Learning, a deep learning method for ECG signal"></a>ECG-SL: Electrocardiogram(ECG) Segment Learning, a deep learning method for ECG signal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00818">http://arxiv.org/abs/2310.00818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Yu, Huiyuan Yang, Akane Sano</li>
<li>for: 本研究旨在使用深度学习模型以优化心跳信号的分析，提高心跳信号的诊断精度。</li>
<li>methods: 本研究提出了一种基于心跳段分的ECG-Segment based Learning（ECG-SL）框架，从心跳段中提取了结构特征，并使用时间模型学习时间信息。此外，还explored一种自动标注的自我超vised学习策略以预训练模型，从而提高下游任务的性能。</li>
<li>results: 对于三种临床应用（心脏病诊断、呼吸暂停检测和cardiac arrhythmia分类），ECG-SL方法显示了与基eline模型和任务特定方法相比的竞争性表现。此外，通过分心跳 segments的Visualization Map可以看到ECG-SL方法更强调每个心跳的峰值和ST范围。<details>
<summary>Abstract</summary>
Electrocardiogram (ECG) is an essential signal in monitoring human heart activities. Researchers have achieved promising results in leveraging ECGs in clinical applications with deep learning models. However, the mainstream deep learning approaches usually neglect the periodic and formative attribute of the ECG heartbeat waveform. In this work, we propose a novel ECG-Segment based Learning (ECG-SL) framework to explicitly model the periodic nature of ECG signals. More specifically, ECG signals are first split into heartbeat segments, and then structural features are extracted from each of the segments. Based on the structural features, a temporal model is designed to learn the temporal information for various clinical tasks. Further, due to the fact that massive ECG signals are available but the labeled data are very limited, we also explore self-supervised learning strategy to pre-train the models, resulting significant improvement for downstream tasks. The proposed method outperforms the baseline model and shows competitive performances compared with task-specific methods in three clinical applications: cardiac condition diagnosis, sleep apnea detection, and arrhythmia classification. Further, we find that the ECG-SL tends to focus more on each heartbeat's peak and ST range than ResNet by visualizing the saliency maps.
</details>
<details>
<summary>摘要</summary>
电心图（ECG）是人类心脏活动监测中的关键信号。研究人员在临床应用中已经取得了深受欢迎的结果，使用深度学习模型。然而，主流深度学习方法通常忽略ECG心跳波形的周期性和结构特征。在这项工作中，我们提出了一种基于ECG心跳分割的学习框架（ECG-SL），以明确ECG信号的周期性。具体来说，ECG信号首先被分割成心跳分割，然后从每个分割中提取结构特征。基于这些结构特征，我们设计了一个时间模型，以学习不同临床任务中的时间信息。由于大量的ECG信号 disponible，但标注数据却很有限，因此我们还探索了自动学习策略，以预训练模型，从而实现了显著的提升。我们的方法超过基线模型，并与特定任务方法相比，在三种临床应用中（心脏病诊断、呼吸暂停检测和心动过速分类）显示了竞争力。此外，我们发现ECG-SL在每个心跳的峰值和ST范围方面更加强调，相比ResNet。我们可以通过Visualize saliency maps来见到这一点。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Make-Adherence-Aware-Advice"><a href="#Learning-to-Make-Adherence-Aware-Advice" class="headerlink" title="Learning to Make Adherence-Aware Advice"></a>Learning to Make Adherence-Aware Advice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00817">http://arxiv.org/abs/2310.00817</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guanting Chen, Xiaocheng Li, Chunlin Sun, Hanzhao Wang</li>
<li>for: 这篇论文目的是提出一个序推问题解决模型，以满足人类与人工智能（AI）之间的互动挑战。</li>
<li>methods: 这个模型考虑了人类的遵循度（机器建议被接受或拒绝的可能性），并提供了一个折补选项，以便机器在重要时刻提供建议。这篇论文还提供了专门的学习算法，以学习最佳建议策略，并只在重要时刻提供建议。</li>
<li>results: 与问题独立的算法相比，这篇论文的专门学习算法不仅具有更好的理论均衡性，还在实验中显示出强大的表现。<details>
<summary>Abstract</summary>
As artificial intelligence (AI) systems play an increasingly prominent role in human decision-making, challenges surface in the realm of human-AI interactions. One challenge arises from the suboptimal AI policies due to the inadequate consideration of humans disregarding AI recommendations, as well as the need for AI to provide advice selectively when it is most pertinent. This paper presents a sequential decision-making model that (i) takes into account the human's adherence level (the probability that the human follows/rejects machine advice) and (ii) incorporates a defer option so that the machine can temporarily refrain from making advice. We provide learning algorithms that learn the optimal advice policy and make advice only at critical time stamps. Compared to problem-agnostic reinforcement learning algorithms, our specialized learning algorithms not only enjoy better theoretical convergence properties but also show strong empirical performance.
</details>
<details>
<summary>摘要</summary>
artificial intelligence (AI) 系统在人类做出决策中扮演越来越重要的角色，但是人类与 AI 之间的互动问题开始浮现。一个挑战是由于 AI 策略不够佳，人类可能不会遵循 AI 的建议，同时 AI 需要提供建议时机选择性地为人类提供建议。这篇论文提出了一个顺序决策模型，该模型（i）考虑人类遵循度（机器建议被接受或拒绝的概率），（ii）将机器给出建议的时间点选择性地推荐。我们提供了特殊的学习算法，这些算法不仅具有更好的理论均衡性，并且在实验中表现出色。相比问题agnostic 征求学习算法，我们的特殊学习算法不仅具有更好的理论均衡性，而且在实验中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Design-Principles-for-Frequentist-Sequential-Learning"><a href="#Bayesian-Design-Principles-for-Frequentist-Sequential-Learning" class="headerlink" title="Bayesian Design Principles for Frequentist Sequential Learning"></a>Bayesian Design Principles for Frequentist Sequential Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00806">http://arxiv.org/abs/2310.00806</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xuyunbei/mab-code">https://github.com/xuyunbei/mab-code</a></li>
<li>paper_authors: Yunbei Xu, Assaf Zeevi<br>for:这篇论文的目的是优化频繁ister regret for sequential learning problems，并提供了一种总结 Bayesian principles的通用理论。methods:论文使用了一种新的优化方法，即“algorithmic beliefs”的生成，以及基于 Bayesian posteriors 的决策。results:论文提出了一种新的算法，可以在随机、对抗和不同环境下实现“best-of-all-worlds”的 empirical performance。此外，这些原理还可以应用于线性 bandits、bandit convex optimization 和 reinforcement learning。<details>
<summary>Abstract</summary>
We develop a general theory to optimize the frequentist regret for sequential learning problems, where efficient bandit and reinforcement learning algorithms can be derived from unified Bayesian principles. We propose a novel optimization approach to generate "algorithmic beliefs" at each round, and use Bayesian posteriors to make decisions. The optimization objective to create "algorithmic beliefs," which we term "Algorithmic Information Ratio," represents an intrinsic complexity measure that effectively characterizes the frequentist regret of any algorithm. To the best of our knowledge, this is the first systematical approach to make Bayesian-type algorithms prior-free and applicable to adversarial settings, in a generic and optimal manner. Moreover, the algorithms are simple and often efficient to implement. As a major application, we present a novel algorithm for multi-armed bandits that achieves the "best-of-all-worlds" empirical performance in the stochastic, adversarial, and non-stationary environments. And we illustrate how these principles can be used in linear bandits, bandit convex optimization, and reinforcement learning.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation note:* "frequentist regret" 改为 "频率 regret"* "sequential learning problems" 改为 "顺序学习问题"* "efficient bandit and reinforcement learning algorithms" 改为 "高效的随机抽象和奖励学习算法"* "Algorithmic Information Ratio" 改为 "算法信息比率"* "prior-free" 改为 "无先验"* "adversarial settings" 改为 "对抗设定"* "linear bandits" 改为 "线性随机抽象"* "bandit convex optimization" 改为 "随机抽象优化"* "reinforcement learning" 改为 "奖励学习")
</details></li>
</ul>
<hr>
<h2 id="Going-Beyond-Familiar-Features-for-Deep-Anomaly-Detection"><a href="#Going-Beyond-Familiar-Features-for-Deep-Anomaly-Detection" class="headerlink" title="Going Beyond Familiar Features for Deep Anomaly Detection"></a>Going Beyond Familiar Features for Deep Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00797">http://arxiv.org/abs/2310.00797</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarath Sivaprasad, Mario Fritz</li>
<li>For: The paper is written for detecting anomalies in deep learning models, specifically addressing the problem of false negatives caused by uncaptured novel features.* Methods: The paper proposes a novel approach to anomaly detection using explainability, which captures novel features as unexplained observations in the input space. The approach combines similarity and novelty in a hybrid approach, eliminating the need for expensive background models and dense matching.* Results: The paper achieves strong performance across a wide range of anomaly benchmarks, reducing false negative anomalies by up to 40% compared to the state-of-the-art. The method also provides visually inspectable explanations for pixel-level anomalies.Here are the three points in Simplified Chinese text:* For: 本文是为检测深度学习模型中的异常点而写的，特别是解决由未捕捉的新特征引起的假阳性问题。* Methods: 本文提出了一种新的异常检测方法，使用可解释性来捕捉新特征，并将相似性和新鲜度结合在一起。这种方法可以无需昂贵的背景模型和紧密匹配。* Results: 本文在多种异常标准benchmark上实现了优秀的表现，相比之前的状态态-of-the-art，减少了假阳性异常的比例达40%。此外，方法还提供了可视化的解释，用于检测像素级异常。<details>
<summary>Abstract</summary>
Anomaly Detection (AD) is a critical task that involves identifying observations that do not conform to a learned model of normality. Prior work in deep AD is predominantly based on a familiarity hypothesis, where familiar features serve as the reference in a pre-trained embedding space. While this strategy has proven highly successful, it turns out that it causes consistent false negatives when anomalies consist of truly novel features that are not well captured by the pre-trained encoding. We propose a novel approach to AD using explainability to capture novel features as unexplained observations in the input space. We achieve strong performance across a wide range of anomaly benchmarks by combining similarity and novelty in a hybrid approach. Our approach establishes a new state-of-the-art across multiple benchmarks, handling diverse anomaly types while eliminating the need for expensive background models and dense matching. In particular, we show that by taking account of novel features, we reduce false negative anomalies by up to 40% on challenging benchmarks compared to the state-of-the-art. Our method gives visually inspectable explanations for pixel-level anomalies.
</details>
<details>
<summary>摘要</summary>
异常检测（AD）是一项关键任务，它的目标是找到不符合学习的模型正常性的观察值。现有的深度AD研究大多基于 Familiarity 假设，即使用已经训练过的特征空间中的熟悉特征作为参考。然而，这种策略会导致常见的假阳性结果，即在异常值中包含未 capture 的新特征。我们提出一种基于解释力的新方法，可以捕捉输入空间中的新特征作为未解释的观察值。我们通过将相似性和新鲜度结合在一起来实现了一种混合方法，并在多个异常 benchmark 上达到了新的状态对领导地位。我们的方法可以处理多种异常类型，而无需购买贵重的背景模型和紧密匹配。特别是，我们发现通过考虑新特征，可以降低 false negative 异常值达到 40% 以上，相比之前的状态对领导地位。我们的方法还可以为像素级异常值提供可见的解释。
</details></li>
</ul>
<hr>
<h2 id="Categorizing-Flight-Paths-using-Data-Visualization-and-Clustering-Methodologies"><a href="#Categorizing-Flight-Paths-using-Data-Visualization-and-Clustering-Methodologies" class="headerlink" title="Categorizing Flight Paths using Data Visualization and Clustering Methodologies"></a>Categorizing Flight Paths using Data Visualization and Clustering Methodologies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00773">http://arxiv.org/abs/2310.00773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Song, Keyang Yu, Seth Young</li>
<li>for: 本研究使用美国联邦航空管理局的航空流管理系统数据和DV8工具来开发了飞行路径分 clustering算法，以分类不同的飞行路径。</li>
<li>methods: 研究使用了两种分 clustering方法：一种是基于空间地理准备的距离模型，另一种是基于向量cosine相似性模型。两种方法的比较和应用示例演示了自动 clustering结果决定和人工循环过程的成功应用。</li>
<li>results: 研究发现，基于地理距离模型在航道部分的 clustering效果较好，而基于cosine相似性模型在近端操作部分，如到达路径，的 clustering效果较好。此外，使用点抽象技术可以提高计算效率。<details>
<summary>Abstract</summary>
This work leverages the U.S. Federal Aviation Administration's Traffic Flow Management System dataset and DV8, a recently developed tool for highly interactive visualization of air traffic data, to develop clustering algorithms for categorizing air traffic by their varying flight paths. Two clustering methodologies, a spatial-based geographic distance model, and a vector-based cosine similarity model, are demonstrated and compared for their clustering effectiveness. Examples of their applications reveal successful, realistic clustering based on automated clustering result determination and human-in-the-loop processes, with geographic distance algorithms performing better for enroute portions of flight paths and cosine similarity algorithms performing better for near-terminal operations, such as arrival paths. A point extraction technique is applied to improve computation efficiency.
</details>
<details>
<summary>摘要</summary>
这项工作利用美国联邦航空管理局的交通流管理系统数据集和DV8工具，一种最近开发的高度互动式航空交通数据可视化工具，开发出 clustering 算法来分类不同的航空交通路径。我们示出了两种 clustering 方法，一种基于空间准备的地理距离模型，另一种基于向量的 косину similarity 模型，并对它们的划分效果进行比较。我们还提供了自动划分结果决定和人工循环过程的应用示例，其中地理距离算法在航道部分表现较好，而 косину similarity 算法在近机场操作，如进近路径，表现较好。此外，我们还应用了点提取技术来提高计算效率。
</details></li>
</ul>
<hr>
<h2 id="Data-Efficient-Power-Flow-Learning-for-Network-Contingencies"><a href="#Data-Efficient-Power-Flow-Learning-for-Network-Contingencies" class="headerlink" title="Data-Efficient Power Flow Learning for Network Contingencies"></a>Data-Efficient Power Flow Learning for Network Contingencies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00763">http://arxiv.org/abs/2310.00763</a></li>
<li>repo_url: None</li>
<li>paper_authors: Parikshit Pareek, Deepjyoti Deka, Sidhant Misra</li>
<li>for: 学习电网中网络异常情况下的电流流动和相应的可能性电压范围（PVE）。</li>
<li>methods: 使用一种网络感知的 Gaussian Process（GP）称为顶点度 kernel（VDK-GP）来估算电压-功率函数，并提出一种新的多任务顶点度 kernel（MT-VDK）来确定未经见过的电网中的电流流动。</li>
<li>results: 在IEEE 30-Bus 电网上进行了 simulations，发现MT-VDK-GP方法可以在低训练数据范围（50-250样本）下减少了平均预测错误的50%以上，并在75%以上的 N-2 停机网络结构中超过了基于超参数的传输学习方法。此外，MT-VDK-GP方法还可以使用64倍少的电流解题方法来实现PVE。<details>
<summary>Abstract</summary>
This work presents an efficient data-driven method to learn power flows in grids with network contingencies and to estimate corresponding probabilistic voltage envelopes (PVE). First, a network-aware Gaussian process (GP) termed Vertex-Degree Kernel (VDK-GP), developed in prior work, is used to estimate voltage-power functions for a few network configurations. The paper introduces a novel multi-task vertex degree kernel (MT-VDK) that amalgamates the learned VDK-GPs to determine power flows for unseen networks, with a significant reduction in the computational complexity and hyperparameter requirements compared to alternate approaches. Simulations on the IEEE 30-Bus network demonstrate the retention and transfer of power flow knowledge in both N-1 and N-2 contingency scenarios. The MT-VDK-GP approach achieves over 50% reduction in mean prediction error for novel N-1 contingency network configurations in low training data regimes (50-250 samples) over VDK-GP. Additionally, MT-VDK-GP outperforms a hyper-parameter based transfer learning approach in over 75% of N-2 contingency network structures, even without historical N-2 outage data. The proposed method demonstrates the ability to achieve PVEs using sixteen times fewer power flow solutions compared to Monte-Carlo sampling-based methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Data-driven-adaptive-building-thermal-controller-tuning-with-constraints-A-primal-dual-contextual-Bayesian-optimization-approach"><a href="#Data-driven-adaptive-building-thermal-controller-tuning-with-constraints-A-primal-dual-contextual-Bayesian-optimization-approach" class="headerlink" title="Data-driven adaptive building thermal controller tuning with constraints: A primal-dual contextual Bayesian optimization approach"></a>Data-driven adaptive building thermal controller tuning with constraints: A primal-dual contextual Bayesian optimization approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00758">http://arxiv.org/abs/2310.00758</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjie Xu, Bratislav Svetozarevic, Loris Di Natale, Philipp Heer, Colin N Jones</li>
<li>for: 本文 targets the problem of minimizing the energy consumption of a room temperature controller while ensuring the daily cumulative thermal discomfort of occupants is below a given threshold.</li>
<li>methods: 本文提出了一种数据驱动的逻辑-对抗搜索（PDCBO）方法来解决这个问题。</li>
<li>results: 在一个单个房间的 simulate case study中，我们运用了我们的算法来调整PI饱和预热时间的参数，并获得了至多4.7%的能源减少，同时保证每天的温室不超过给定的快速阈值。此外，PDCBO还可以自动跟踪时间变化的快速阈值，而其他方法无法完成这一任务。<details>
<summary>Abstract</summary>
We study the problem of tuning the parameters of a room temperature controller to minimize its energy consumption, subject to the constraint that the daily cumulative thermal discomfort of the occupants is below a given threshold. We formulate it as an online constrained black-box optimization problem where, on each day, we observe some relevant environmental context and adaptively select the controller parameters. In this paper, we propose to use a data-driven Primal-Dual Contextual Bayesian Optimization (PDCBO) approach to solve this problem. In a simulation case study on a single room, we apply our algorithm to tune the parameters of a Proportional Integral (PI) heating controller and the pre-heating time. Our results show that PDCBO can save up to 4.7% energy consumption compared to other state-of-the-art Bayesian optimization-based methods while keeping the daily thermal discomfort below the given tolerable threshold on average. Additionally, PDCBO can automatically track time-varying tolerable thresholds while existing methods fail to do so. We then study an alternative constrained tuning problem where we aim to minimize the thermal discomfort with a given energy budget. With this formulation, PDCBO reduces the average discomfort by up to 63% compared to state-of-the-art safe optimization methods while keeping the average daily energy consumption below the required threshold.
</details>
<details>
<summary>摘要</summary>
我们研究控制室内温度的参数来减少能源消耗，并且保持每天累累感觉下限。我们将这个问题转化为线上受限制的黑盒优化问题，每天我们可以观察环境上的一些相关数据，然后选择参数。在这篇论文中，我们提出使用基于Primal-Dual Contextual Bayesian Optimization（PDCBO）的数据驱动方法来解决这个问题。在单一房间的实验案例中，我们使用我们的算法来调整PI适应器和预热时间的参数。我们的结果显示，PDCBO可以与其他现有的Bayesian优化基于方法相比，在平均每天的能源消耗下降4.7%，同时保持每天累累感觉下限。此外，PDCBO可以自动跟踪时间变化的受限制耐受阈值，而现有的方法则无法实现这一点。然后，我们研究一个受限制的问题，即将累累感觉降到最低，并且保持每天能源消耗在所需的预算下。这个问题中，PDCBO可以在平均每天的累累感觉下降63%，同时保持每天能源消耗在所需的预算下。
</details></li>
</ul>
<hr>
<h2 id="Identifying-Copeland-Winners-in-Dueling-Bandits-with-Indifferences"><a href="#Identifying-Copeland-Winners-in-Dueling-Bandits-with-Indifferences" class="headerlink" title="Identifying Copeland Winners in Dueling Bandits with Indifferences"></a>Identifying Copeland Winners in Dueling Bandits with Indifferences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00750">http://arxiv.org/abs/2310.00750</a></li>
<li>repo_url: None</li>
<li>paper_authors: Viktor Bengs, Björn Haddenhorst, Eyke Hüllermeier</li>
<li>for: 本文研究了一种叫做“对抗炮手问题”的概念，其中一个玩家可以通过选择一个或多个“炮手”来获得奖励。本文是关于这种问题的一种特殊情况，即在玩家提供反馈时，可能会出现“无偏好”的情况。</li>
<li>methods: 本文提出了一种名为POCOWISTA的算法，该算法可以寻找玩家的 Copeland 赢家（即最佳炮手）。此外，本文还提供了一个lower bound的下界，表明任何学习算法都需要至少这么多样本来找到 Copeland 赢家。</li>
<li>results: 本文的实验结果表明，POCOWISTA 算法在实际中表现出色，它可以快速寻找玩家的 Copeland 赢家，并且在普通的对抗炮手问题中也有优秀的表现。此外，如果 preference probabilities 满足一种特殊的随机对称性条件，则可以提供一个改进的 worst-case 下界。<details>
<summary>Abstract</summary>
We consider the task of identifying the Copeland winner(s) in a dueling bandits problem with ternary feedback. This is an underexplored but practically relevant variant of the conventional dueling bandits problem, in which, in addition to strict preference between two arms, one may observe feedback in the form of an indifference. We provide a lower bound on the sample complexity for any learning algorithm finding the Copeland winner(s) with a fixed error probability. Moreover, we propose POCOWISTA, an algorithm with a sample complexity that almost matches this lower bound, and which shows excellent empirical performance, even for the conventional dueling bandits problem. For the case where the preference probabilities satisfy a specific type of stochastic transitivity, we provide a refined version with an improved worst case sample complexity.
</details>
<details>
<summary>摘要</summary>
我们考虑了在战斗炮手问题中确定科普兰赢家的任务，这是一种未得到充分研究但实际上很有实际意义的战斗炮手问题变种，在这种变种中，除了简单的首选之外，还可能观察到反馈形式的半同意。我们提供了确定科普兰赢家的样本复杂度下界，以及一种名为POCOWISTA的算法，该算法的样本复杂度几乎与下界匹配，并在实际中表现出色，包括传统的战斗炮手问题。在首选概率满足特定的随机transitivity性时，我们提供了一种改进的启发版本，其 worst case 样本复杂度得到改进。
</details></li>
</ul>
<hr>
<h2 id="SEED-Simple-Efficient-and-Effective-Data-Management-via-Large-Language-Models"><a href="#SEED-Simple-Efficient-and-Effective-Data-Management-via-Large-Language-Models" class="headerlink" title="SEED: Simple, Efficient, and Effective Data Management via Large Language Models"></a>SEED: Simple, Efficient, and Effective Data Management via Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00749">http://arxiv.org/abs/2310.00749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zui CHen, Lei Cao, Sam Madden, Ju Fan, Nan Tang, Zihui Gu, Zeyuan Shang, Chunwei Liu, Michael Cafarella, Tim Kraska</li>
<li>for: This paper aims to provide an efficient and effective data management system for large language models (LLMs) by addressing the challenges of computational and economic expense.</li>
<li>methods: The paper proposes a system called SEED, which consists of three main components: code generation, model generation, and augmented LLM query. SEED localizes LLM computation as much as possible, uses optimization techniques to enhance the localized solution and LLM queries, and allows users to easily construct a customized data management solution.</li>
<li>results: The paper achieves state-of-the-art few-shot performance while significantly reducing the number of required LLM calls for diverse data management tasks such as data imputation and NL2SQL translation.<details>
<summary>Abstract</summary>
We introduce SEED, an LLM-centric system that allows users to easily create efficient, and effective data management applications. SEED comprises three main components: code generation, model generation, and augmented LLM query to address the challenges that LLM services are computationally and economically expensive and do not always work well on all cases for a given data management task. SEED addresses the expense challenge by localizing LLM computation as much as possible. This includes replacing most of LLM calls with local code, local models, and augmenting LLM queries with batching and data access tools, etc. To ensure effectiveness, SEED features a bunch of optimization techniques to enhance the localized solution and the LLM queries, including automatic code validation, code ensemble, model representatives selection, selective tool usages, etc. Moreover, with SEED users are able to easily construct a data management solution customized to their applications. It allows the users to configure each component and compose an execution pipeline in natural language. SEED then automatically compiles it into an executable program. We showcase the efficiency and effectiveness of SEED using diverse data management tasks such as data imputation, NL2SQL translation, etc., achieving state-of-the-art few-shot performance while significantly reducing the number of required LLM calls.
</details>
<details>
<summary>摘要</summary>
我们介绍SEED系统，它是基于LLM的系统，让用户可以轻松地创建高效、高效的数据管理应用程序。SEED包括三个主要 ком成分：代码生成、模型生成和增强LLM查询。这些 ком成分是为了解决LLM服务 computationally和经济成本高，并且不一定在所有情况下能够实现数据管理任务。SEED通过地方化LLM计算来解决这个挑战，包括将大多数LLM请求替换为本地代码、本地模型和增强LLM查询批处理等。为了保证效果，SEED具有许多优化技术，包括自动验证代码、代码合并、模型选择、选择工具使用等。此外，SEED还允许用户轻松地建立自定义的数据管理解决方案，并且可以自然语言中 configurations 和构成执行管线。SEED 将自动将其转换为可执行程式。我们透过使用多种数据管理任务，例如数据补充、NL2SQL翻译等，得到了状况之中的几个shot性能，同时对LLM请求数量进行了重要削减。
</details></li>
</ul>
<hr>
<h2 id="Deterministic-Langevin-Unconstrained-Optimization-with-Normalizing-Flows"><a href="#Deterministic-Langevin-Unconstrained-Optimization-with-Normalizing-Flows" class="headerlink" title="Deterministic Langevin Unconstrained Optimization with Normalizing Flows"></a>Deterministic Langevin Unconstrained Optimization with Normalizing Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00745">http://arxiv.org/abs/2310.00745</a></li>
<li>repo_url: None</li>
<li>paper_authors: James M. Sullivan, Uros Seljak</li>
<li>for: 该论文旨在开发一种全球、不使用梯度的优化策略，用于解决costly黑桶函数问题。</li>
<li>methods: 该方法基于Fokker-Planck和Langevin方程，并且利用Normalizing Flow来进行活动学习和选择提案点。</li>
<li>results: 该方法在标准的synthetic测试函数上实现了superior或竞争性的进步，并在实际的科学和神经网络优化问题上达到了竞争性的result。<details>
<summary>Abstract</summary>
We introduce a global, gradient-free surrogate optimization strategy for expensive black-box functions inspired by the Fokker-Planck and Langevin equations. These can be written as an optimization problem where the objective is the target function to maximize minus the logarithm of the current density of evaluated samples. This objective balances exploitation of the target objective with exploration of low-density regions. The method, Deterministic Langevin Optimization (DLO), relies on a Normalizing Flow density estimate to perform active learning and select proposal points for evaluation. This strategy differs qualitatively from the widely-used acquisition functions employed by Bayesian Optimization methods, and can accommodate a range of surrogate choices. We demonstrate superior or competitive progress toward objective optima on standard synthetic test functions, as well as on non-convex and multi-modal posteriors of moderate dimension. On real-world objectives, such as scientific and neural network hyperparameter optimization, DLO is competitive with state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种全球、gradient-free的优化策略，用于优化costly黑obox函数。这种策略 Draw inspiration from the Fokker-Planck and Langevin equations, and can be formulated as an optimization problem where the objective is to maximize the target function minus the logarithm of the current density of evaluated samples. This objective balances the exploitation of the target objective with the exploration of low-density regions. Our method, Deterministic Langevin Optimization (DLO), uses a Normalizing Flow density estimate to perform active learning and select proposal points for evaluation. This strategy differs qualitatively from the widely-used acquisition functions employed by Bayesian Optimization methods, and can accommodate a range of surrogate choices. We demonstrate superior or competitive progress toward objective optima on standard synthetic test functions, as well as on non-convex and multi-modal posteriors of moderate dimension. On real-world objectives, such as scientific and neural network hyperparameter optimization, DLO is competitive with state-of-the-art baselines.
</details></li>
</ul>
<hr>
<h2 id="Spectral-Neural-Networks-Approximation-Theory-and-Optimization-Landscape"><a href="#Spectral-Neural-Networks-Approximation-Theory-and-Optimization-Landscape" class="headerlink" title="Spectral Neural Networks: Approximation Theory and Optimization Landscape"></a>Spectral Neural Networks: Approximation Theory and Optimization Landscape</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00729">http://arxiv.org/abs/2310.00729</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenghui Li, Rishi Sonthalia, Nicolas Garcia Trillos</li>
<li>for: This paper investigates the theoretical aspects of Spectral Neural Networks (SNN) and their tradeoffs with respect to the number of neurons and the amount of spectral geometric information learned.</li>
<li>methods: The paper uses a theoretical approach to explore the optimization landscape of SNN’s objective function, shedding light on the training dynamics of SNN and its non-convex ambient loss function.</li>
<li>results: The paper presents quantitative insights into the tradeoff between the number of neurons and the amount of spectral geometric information a neural network learns, and initiates a theoretical exploration of the training dynamics of SNN.<details>
<summary>Abstract</summary>
There is a large variety of machine learning methodologies that are based on the extraction of spectral geometric information from data. However, the implementations of many of these methods often depend on traditional eigensolvers, which present limitations when applied in practical online big data scenarios. To address some of these challenges, researchers have proposed different strategies for training neural networks as alternatives to traditional eigensolvers, with one such approach known as Spectral Neural Network (SNN). In this paper, we investigate key theoretical aspects of SNN. First, we present quantitative insights into the tradeoff between the number of neurons and the amount of spectral geometric information a neural network learns. Second, we initiate a theoretical exploration of the optimization landscape of SNN's objective to shed light on the training dynamics of SNN. Unlike typical studies of convergence to global solutions of NN training dynamics, SNN presents an additional complexity due to its non-convex ambient loss function.
</details>
<details>
<summary>摘要</summary>
有很多机器学习方法基于数据中特征几何信息的提取，但是许多实现方法常常依赖于传统的特征值解决方案，这些解决方案在实际上线大数据场景中存在限制。为了解决这些挑战，研究人员已经提议了不同的替代方案，其中一种是叫做特征神经网络（SNN）。在这篇论文中，我们调查了SNN的关键理论方面。首先，我们提供了量化的视角，描述了神经网络学习过程中特征几何信息和神经元数之间的负反关系。其次，我们开始了SNN目标函数优化境地的理论探索，以便更好地理解SNN训练过程的动态。不同于传统的NN训练动态研究，SNN增加了非对称的抽象损失函数，使其训练动态更加复杂。
</details></li>
</ul>
<hr>
<h2 id="Physics-Informed-Graph-Neural-Network-for-Dynamic-Reconfiguration-of-Power-Systems"><a href="#Physics-Informed-Graph-Neural-Network-for-Dynamic-Reconfiguration-of-Power-Systems" class="headerlink" title="Physics-Informed Graph Neural Network for Dynamic Reconfiguration of Power Systems"></a>Physics-Informed Graph Neural Network for Dynamic Reconfiguration of Power Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00728">http://arxiv.org/abs/2310.00728</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jules Authier, Rabab Haider, Anuradha Annaswamy, Florian Dorfler</li>
<li>for: 这个论文是为了解决动态重配置（DyR）问题的快速决策算法。DyR 是一个扩展到大规模网格和快速时间步骤的混合整数问题，可能是计算 tractable 的问题。</li>
<li>methods: 该论文提出了一种基于物理学习树（GNNs）框架的 GraPhyR，用于解决 DyR 问题。该框架直接包含了操作和连接约束，并通过练习策略来训练。</li>
<li>results: 论文的结果表明，GraPhyR 能够学习解决 DyR 问题，并且比传统的方法更快和更有效。<details>
<summary>Abstract</summary>
To maintain a reliable grid we need fast decision-making algorithms for complex problems like Dynamic Reconfiguration (DyR). DyR optimizes distribution grid switch settings in real-time to minimize grid losses and dispatches resources to supply loads with available generation. DyR is a mixed-integer problem and can be computationally intractable to solve for large grids and at fast timescales. We propose GraPhyR, a Physics-Informed Graph Neural Network (GNNs) framework tailored for DyR. We incorporate essential operational and connectivity constraints directly within the GNN framework and train it end-to-end. Our results show that GraPhyR is able to learn to optimize the DyR task.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:维护可靠的电网需要快速的决策算法来解决复杂的问题，如动态重组（DyR）。DyR在实时中 ottimize 分配网络设置，以最小化电网损失和将资源派发给可用的生产。DyR是一个混合整数问题，可能需要大量的计算时间和复杂的数据分析。我们提出了 GraPhyR，一个基于物理网络学习（GNNs）框架，特别适合DyR。我们直接将运作和连接约束 integrate 到 GNN 框架中，并将其训练成一个终端解决方案。我们的结果显示，GraPhyR 能够学习来优化 DyR 任务。
</details></li>
</ul>
<hr>
<h2 id="Learning-How-to-Propagate-Messages-in-Graph-Neural-Networks"><a href="#Learning-How-to-Propagate-Messages-in-Graph-Neural-Networks" class="headerlink" title="Learning How to Propagate Messages in Graph Neural Networks"></a>Learning How to Propagate Messages in Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00697">http://arxiv.org/abs/2310.00697</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tengxiao1/l2p">https://github.com/tengxiao1/l2p</a></li>
<li>paper_authors: Teng Xiao, Zhengyu Chen, Donglin Wang, Suhang Wang</li>
<li>for: 这个论文研究了图神经网络（GNNs）中的信息传播策略学习问题。</li>
<li>methods: 该论文提出了一种通用学习框架，可以不仅学习GNN参数进行预测，而且可以显式地学习不同节点和不同图类型的可解释性和个性化的传播策略。</li>
<li>results: 经验表明，该提议的框架可以在不同类型的图benchmark上显著提高性能，并可以有效地学习GNN中的可解释性和个性化的传播策略。<details>
<summary>Abstract</summary>
This paper studies the problem of learning message propagation strategies for graph neural networks (GNNs). One of the challenges for graph neural networks is that of defining the propagation strategy. For instance, the choices of propagation steps are often specialized to a single graph and are not personalized to different nodes. To compensate for this, in this paper, we present learning to propagate, a general learning framework that not only learns the GNN parameters for prediction but more importantly, can explicitly learn the interpretable and personalized propagate strategies for different nodes and various types of graphs. We introduce the optimal propagation steps as latent variables to help find the maximum-likelihood estimation of the GNN parameters in a variational Expectation-Maximization (VEM) framework. Extensive experiments on various types of graph benchmarks demonstrate that our proposed framework can significantly achieve better performance compared with the state-of-the-art methods, and can effectively learn personalized and interpretable propagate strategies of messages in GNNs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-Noise-Geometry-of-Stochastic-Gradient-Descent-A-Quantitative-and-Analytical-Characterization"><a href="#The-Noise-Geometry-of-Stochastic-Gradient-Descent-A-Quantitative-and-Analytical-Characterization" class="headerlink" title="The Noise Geometry of Stochastic Gradient Descent: A Quantitative and Analytical Characterization"></a>The Noise Geometry of Stochastic Gradient Descent: A Quantitative and Analytical Characterization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00692">http://arxiv.org/abs/2310.00692</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingze Wang, Lei Wu</li>
<li>for: 本研究旨在理解梯度下降法（SGD）中噪声的地理学性质，并提供了对于过参数化线性模型（OLMs）和层次神经网络的全面理论分析。</li>
<li>methods: 本研究使用了平均和方向含义的对比，特别是考虑样本大小和输入数据缺乏对对焊的影响。</li>
<li>results: 研究发现，SGD在梯度下降过程中噪声会与损失函数的本地几何相似，并且SGD在避免锐 minimum 的过程中会选择平行于损失函数的平坦方向进行跃点。这与梯度下降法不同，后者只能逃脱锐 minimum 方向。实验 validate 了理论发现。<details>
<summary>Abstract</summary>
Empirical studies have demonstrated that the noise in stochastic gradient descent (SGD) aligns favorably with the local geometry of loss landscape. However, theoretical and quantitative explanations for this phenomenon remain sparse. In this paper, we offer a comprehensive theoretical investigation into the aforementioned {\em noise geometry} for over-parameterized linear (OLMs) models and two-layer neural networks. We scrutinize both average and directional alignments, paying special attention to how factors like sample size and input data degeneracy affect the alignment strength. As a specific application, we leverage our noise geometry characterizations to study how SGD escapes from sharp minima, revealing that the escape direction has significant components along flat directions. This is in stark contrast to GD, which escapes only along the sharpest directions. To substantiate our theoretical findings, both synthetic and real-world experiments are provided.
</details>
<details>
<summary>摘要</summary>
empirical studies have shown that the noise in stochastic gradient descent (SGD) is aligned with the local geometry of the loss landscape. however, there is a lack of theoretical and quantitative explanations for this phenomenon. in this paper, we provide a comprehensive theoretical investigation into the "noise geometry" of over-parameterized linear (OLMs) models and two-layer neural networks. we examine both average and directional alignments, paying special attention to how factors such as sample size and input data degeneracy affect the alignment strength. as a specific application, we use our noise geometry characterizations to study how SGD escapes from sharp minima, revealing that the escape direction has significant components along flat directions. this is in stark contrast to GD, which escapes only along the sharpest directions. to substantiate our theoretical findings, we provide both synthetic and real-world experiments.
</details></li>
</ul>
<hr>
<h2 id="PharmacoNet-Accelerating-Large-Scale-Virtual-Screening-by-Deep-Pharmacophore-Modeling"><a href="#PharmacoNet-Accelerating-Large-Scale-Virtual-Screening-by-Deep-Pharmacophore-Modeling" class="headerlink" title="PharmacoNet: Accelerating Large-Scale Virtual Screening by Deep Pharmacophore Modeling"></a>PharmacoNet: Accelerating Large-Scale Virtual Screening by Deep Pharmacophore Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00681">http://arxiv.org/abs/2310.00681</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/seonghwanseo/pharmaconet">https://github.com/seonghwanseo/pharmaconet</a></li>
<li>paper_authors: Seonghwan Seo, Woo Youn Kim</li>
<li>for: 该研究旨在开发一种高效的结构基Virtual Screening方法，以应对越来越大的访问ible compound库。</li>
<li>methods: 该方法使用深度学习框架，通过识别稳定结合的3D药物配置，从binding site中预测药物和蛋白质的绑定pose。通过粗粒度图匹配，一步解决了现有方法中的昂贵绑定pose采样和评分过程。</li>
<li>results: 对比现有方法，PharmacoNet显示了更高的速度和更好的准确性，同时能够保留高过滤率下的hit候选者。研究发现，深度学习基于药物搜寻的方法可以激活未探索的药物搜寻潜力。<details>
<summary>Abstract</summary>
As the size of accessible compound libraries expands to over 10 billion, the need for more efficient structure-based virtual screening methods is emerging. Different pre-screening methods have been developed to rapidly screen the library, but the structure-based methods applicable to general proteins are still lacking: the challenge is to predict the binding pose between proteins and ligands and perform scoring in an extremely short time. We introduce PharmacoNet, a deep learning framework that identifies the optimal 3D pharmacophore arrangement which a ligand should have for stable binding from the binding site. By coarse-grained graph matching between ligands and the generated pharmacophore arrangement, we solve the expensive binding pose sampling and scoring procedures of existing methods in a single step. PharmacoNet is significantly faster than state-of-the-art structure-based approaches, yet reasonably accurate with a simple scoring function. Furthermore, we show the promising result that PharmacoNet effectively retains hit candidates even under the high pre-screening filtration rates. Overall, our study uncovers the hitherto untapped potential of a pharmacophore modeling approach in deep learning-based drug discovery.
</details>
<details>
<summary>摘要</summary>
We introduce PharmacoNet, a deep learning framework that identifies the optimal 3D pharmacophore arrangement for stable binding from the binding site. By coarse-grained graph matching between ligands and the generated pharmacophore arrangement, we eliminate the expensive binding pose sampling and scoring procedures of existing methods in a single step. PharmacoNet is significantly faster than state-of-the-art structure-based approaches, yet reasonably accurate with a simple scoring function.Moreover, we show that PharmacoNet effectively retains hit candidates even under high pre-screening filtration rates. Our study uncovers the hitherto untapped potential of a pharmacophore modeling approach in deep learning-based drug discovery.
</details></li>
</ul>
<hr>
<h2 id="A-General-Offline-Reinforcement-Learning-Framework-for-Interactive-Recommendation"><a href="#A-General-Offline-Reinforcement-Learning-Framework-for-Interactive-Recommendation" class="headerlink" title="A General Offline Reinforcement Learning Framework for Interactive Recommendation"></a>A General Offline Reinforcement Learning Framework for Interactive Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00678">http://arxiv.org/abs/2310.00678</a></li>
<li>repo_url: None</li>
<li>paper_authors: Teng Xiao, Donglin Wang</li>
<li>for: 这个论文研究了在在线环境中学习互动推荐系统的问题，无需在线探索。</li>
<li>methods: 论文提出了一种通用的Offline reinforcement learning框架，可以在不进行线上探索的情况下，最大化用户奖励。特别是，论文首先引入了一种 probabilistic generative model for interactive recommendation，然后提出了一种有效的推理算法基于历史反馈。</li>
<li>results: 论文通过五种方法来减少分布匹配问题，包括支持约束、监督辅助、政策约束、对偶约束和奖励推断。实验表明，提出的方法可以在两个公共的实验数据集上达到比现有的监督学习和强化学习方法更高的性能。<details>
<summary>Abstract</summary>
This paper studies the problem of learning interactive recommender systems from logged feedbacks without any exploration in online environments. We address the problem by proposing a general offline reinforcement learning framework for recommendation, which enables maximizing cumulative user rewards without online exploration. Specifically, we first introduce a probabilistic generative model for interactive recommendation, and then propose an effective inference algorithm for discrete and stochastic policy learning based on logged feedbacks. In order to perform offline learning more effectively, we propose five approaches to minimize the distribution mismatch between the logging policy and recommendation policy: support constraints, supervised regularization, policy constraints, dual constraints and reward extrapolation. We conduct extensive experiments on two public real-world datasets, demonstrating that the proposed methods can achieve superior performance over existing supervised learning and reinforcement learning methods for recommendation.
</details>
<details>
<summary>摘要</summary>
这个论文研究在在线环境中学习互动推荐系统的问题，不需要在线探索。我们解决这个问题，提出了一种通用的离线强化学习推荐框架，可以在离线环境中最大化用户奖励。 Specifically, we first introduce a probabilistic生成模型 for interactive recommendation, and then propose an effective inference algorithm for discrete and stochastic policy learning based on logged feedbacks. In order to perform offline learning more effectively, we propose five approaches to minimize the distribution mismatch between the logging policy and recommendation policy: support constraints, supervised regularization, policy constraints, dual constraints and reward extrapolation. We conduct extensive experiments on two public real-world datasets, demonstrating that the proposed methods can achieve superior performance over existing supervised learning and reinforcement learning methods for recommendation.Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Optimization-or-Architecture-How-to-Hack-Kalman-Filtering"><a href="#Optimization-or-Architecture-How-to-Hack-Kalman-Filtering" class="headerlink" title="Optimization or Architecture: How to Hack Kalman Filtering"></a>Optimization or Architecture: How to Hack Kalman Filtering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00675">http://arxiv.org/abs/2310.00675</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ido90/UsingKalmanFilterTheRightWay">https://github.com/ido90/UsingKalmanFilterTheRightWay</a></li>
<li>paper_authors: Ido Greenberg, Netanel Yannay, Shie Mannor</li>
<li>for: 这个论文是为了探讨非线性滤波器的问题。</li>
<li>methods: 该论文使用了一种名为Optimized Kalman Filter（OKF）的方法，该方法可以对非线性模型进行优化，使其与标准的线性加权滤波器（KF）相比赢得竞争力。</li>
<li>results: 该论文表明，通过使用OKF来优化非线性模型，可以使KF在某些问题上与神经网络模型相比赢得竞争力。此外，OKF还有较好的理论基础和实际表现。<details>
<summary>Abstract</summary>
In non-linear filtering, it is traditional to compare non-linear architectures such as neural networks to the standard linear Kalman Filter (KF). We observe that this mixes the evaluation of two separate components: the non-linear architecture, and the parameters optimization method. In particular, the non-linear model is often optimized, whereas the reference KF model is not. We argue that both should be optimized similarly, and to that end present the Optimized KF (OKF). We demonstrate that the KF may become competitive to neural models - if optimized using OKF. This implies that experimental conclusions of certain previous studies were derived from a flawed process. The advantage of OKF over the standard KF is further studied theoretically and empirically, in a variety of problems. Conveniently, OKF can replace the KF in real-world systems by merely updating the parameters.
</details>
<details>
<summary>摘要</summary>
“在非线性滤波中，传统上对非线性架构如神经网络进行比较，与标准的线性卡尔曼筛（KF）进行对比。我们注意到这两者是不同的两个组件：非线性架构和参数优化方法。具体来说，非线性模型经常被优化，而参考KF模型则不是。我们 argueThat both should be optimized similarly，并为此提出了优化后KF（OKF）。我们示出了KF可能与神经网络竞争，如果使用OKF进行优化。这意味着一些先前的研究结论可能基于不正确的过程得到。OKF比标准KF具有更大的优势，并在各种问题上进行了理论和实验研究。可以说，OKF可以在实际应用中取代KF，只需要更新参数即可。”
</details></li>
</ul>
<hr>
<h2 id="Learning-Type-Inference-for-Enhanced-Dataflow-Analysis"><a href="#Learning-Type-Inference-for-Enhanced-Dataflow-Analysis" class="headerlink" title="Learning Type Inference for Enhanced Dataflow Analysis"></a>Learning Type Inference for Enhanced Dataflow Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00673">http://arxiv.org/abs/2310.00673</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/joernio/joernti-codetidal5">https://github.com/joernio/joernti-codetidal5</a></li>
<li>paper_authors: Lukas Seidel, Sedick David Baker Effendi, Xavier Pinho, Konrad Rieck, Brink van der Merwe, Fabian Yamaguchi</li>
<li>for: This paper aims to improve the accuracy and efficiency of type inference for dynamically-typed languages, specifically TypeScript, by using machine learning techniques.</li>
<li>methods: The paper proposes a Transformer-based model called CodeTIDAL5, which is trained to predict type annotations and integrates with an open-source static analysis tool called Joern.</li>
<li>results: The paper reports that CodeTIDAL5 outperforms the current state-of-the-art by 7.85% on the ManyTypes4TypeScript benchmark, achieving 71.27% accuracy overall, and demonstrates the benefits of using the additional type information for security research.Here’s the same information in Simplified Chinese:</li>
<li>for: 这篇论文目标是提高动态类型语言中的类型推理精度和效率，具体是用机器学习技术来进行类型推理。</li>
<li>methods: 论文提出了一种基于转换器的模型，称为CodeTIDAL5，它可以预测类型注释，并与开源的静态分析工具Joern集成。</li>
<li>results: 论文表明，CodeTIDAL5比当前状态体系的最佳实现提高了7.85%的精度，达到了71.27%的总精度，并通过示出额外类型信息对安全研究带来了优势。<details>
<summary>Abstract</summary>
Statically analyzing dynamically-typed code is a challenging endeavor, as even seemingly trivial tasks such as determining the targets of procedure calls are non-trivial without knowing the types of objects at compile time. Addressing this challenge, gradual typing is increasingly added to dynamically-typed languages, a prominent example being TypeScript that introduces static typing to JavaScript. Gradual typing improves the developer's ability to verify program behavior, contributing to robust, secure and debuggable programs. In practice, however, users only sparsely annotate types directly. At the same time, conventional type inference faces performance-related challenges as program size grows. Statistical techniques based on machine learning offer faster inference, but although recent approaches demonstrate overall improved accuracy, they still perform significantly worse on user-defined types than on the most common built-in types. Limiting their real-world usefulness even more, they rarely integrate with user-facing applications. We propose CodeTIDAL5, a Transformer-based model trained to reliably predict type annotations. For effective result retrieval and re-integration, we extract usage slices from a program's code property graph. Comparing our approach against recent neural type inference systems, our model outperforms the current state-of-the-art by 7.85% on the ManyTypes4TypeScript benchmark, achieving 71.27% accuracy overall. Furthermore, we present JoernTI, an integration of our approach into Joern, an open source static analysis tool, and demonstrate that the analysis benefits from the additional type information. As our model allows for fast inference times even on commodity CPUs, making our system available through Joern leads to high accessibility and facilitates security research.
</details>
<details>
<summary>摘要</summary>
这是一个挑战性的任务，因为在类型是在编译时才能知道的情况下，确定程式中的目标类型是非常困难的。为了解决这个问题，有越来越多的 dynamically-typed 语言加入了渐进类型系统，例如 TypeScript，它将类型系统引入 JavaScript 中。渐进类型可以帮助开发者更好地验证程式的行为，从而实现更加稳定、安全和可靠的程式。然而，在实践中，用户几乎不直接将类型资讯输入。另一方面，传统的类型推论面临着程序大小增长时的性能问题，而且机器学习技术的使用可以提供更快的推论，但是这些方法通常在用户自定义的类型上表现较差。为了解决这个问题，我们提出了 CodeTIDAL5，一个基于 Transformer 模型的类型预测方法。为了有效地从程式码中提取类型资讯，我们将程式码转换为 code property graph，并从中提取使用类型的片段。与最新的神经网络类型推论系统相比，我们的模型在 ManyTypes4TypeScript 测试 benchmark 上的表现比前者高7.85%，总的来说是71.27%的准确率。此外，我们还将我们的方法与 open source 静态分析工具 Joern 集成，并证明了这种结合带来的分析优化。由于我们的模型可以在廉价的实体CPU上进行快速的类型推论，因此通过 Joern 进行分析可以实现高可用性和促进安全研究。
</details></li>
</ul>
<hr>
<h2 id="Balancing-Efficiency-vs-Effectiveness-and-Providing-Missing-Label-Robustness-in-Multi-Label-Stream-Classification"><a href="#Balancing-Efficiency-vs-Effectiveness-and-Providing-Missing-Label-Robustness-in-Multi-Label-Stream-Classification" class="headerlink" title="Balancing Efficiency vs. Effectiveness and Providing Missing Label Robustness in Multi-Label Stream Classification"></a>Balancing Efficiency vs. Effectiveness and Providing Missing Label Robustness in Multi-Label Stream Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00665">http://arxiv.org/abs/2310.00665</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sepehrbakhshi/ml-bels">https://github.com/sepehrbakhshi/ml-bels</a></li>
<li>paper_authors: Sepehr Bakhshi, Fazli Can</li>
<li>for: 这个论文的目的是提出一种适用于高维多标签分类的神经网络模型，以解决现有模型的不稳定性和效率问题。</li>
<li>methods: 我们的模型使用了选择性概念演化适应机制，使其适用于非站台环境。此外，我们还采用了一种简单 yet effective的抽象策略来处理缺失标签问题，并证明其在大多数状态前的监测模型中表现出色。</li>
<li>results: 我们的模型ML-BELS在11个基准模型、5个Synthetics和13个实际数据集上进行了广泛的评估，结果显示其能够平衡效率和有效性，同时对缺失标签和概念演化具有良好的Robustness。<details>
<summary>Abstract</summary>
Available works addressing multi-label classification in a data stream environment focus on proposing accurate models; however, these models often exhibit inefficiency and cannot balance effectiveness and efficiency. In this work, we propose a neural network-based approach that tackles this issue and is suitable for high-dimensional multi-label classification. Our model uses a selective concept drift adaptation mechanism that makes it suitable for a non-stationary environment. Additionally, we adapt our model to an environment with missing labels using a simple yet effective imputation strategy and demonstrate that it outperforms a vast majority of the state-of-the-art supervised models. To achieve our purposes, we introduce a weighted binary relevance-based approach named ML-BELS using the Broad Ensemble Learning System (BELS) as its base classifier. Instead of a chain of stacked classifiers, our model employs independent weighted ensembles, with the weights generated by the predictions of a BELS classifier. We show that using the weighting strategy on datasets with low label cardinality negatively impacts the accuracy of the model; with this in mind, we use the label cardinality as a trigger for applying the weights. We present an extensive assessment of our model using 11 state-of-the-art baselines, five synthetics, and 13 real-world datasets, all with different characteristics. Our results demonstrate that the proposed approach ML-BELS is successful in balancing effectiveness and efficiency, and is robust to missing labels and concept drift.
</details>
<details>
<summary>摘要</summary>
可用的工作，关于多标签分类在数据流环境中，主要关注提出准确的模型，但这些模型经常表现不具有效率。在这种情况下，我们提出一种基于神经网络的方法，可以解决这个问题，并适用于高维多标签分类。我们的模型使用一种选择性概念漂移适应机制，使其适用于不站ARY环境。此外，我们采用一种简单 yet effective的损失函数填充策略，以适应缺失标签的环境。为了实现我们的目标，我们引入了一种权重Binary relevance-based approach named ML-BELS，使用Broad Ensemble Learning System（BELS）作为基类фика器。而不是一串堆叠的类фика器，我们的模型使用独立的权重ensemble，其权重由BELS类фика器的预测值生成。我们发现，在 datasets with low label cardinality 上，使用权重策略会下降模型的准确率。因此，我们使用标签 cardinality 作为触发器，只在标签 cardinality 高于某个阈值时应用权重。我们对 11 种基线模型、5 种 sintetics 和 13 个实际 datasets进行了广泛的评估。我们的结果表明，我们的方法 ML-BELS 能够均衡效果和效率，并对缺失标签和概念漂移 exhibit  robustness。
</details></li>
</ul>
<hr>
<h2 id="Twin-Neural-Network-Improved-k-Nearest-Neighbor-Regression"><a href="#Twin-Neural-Network-Improved-k-Nearest-Neighbor-Regression" class="headerlink" title="Twin Neural Network Improved k-Nearest Neighbor Regression"></a>Twin Neural Network Improved k-Nearest Neighbor Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00664">http://arxiv.org/abs/2310.00664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sebastian J. Wetzel</li>
<li>for: 用于预测差异而不是目标值本身的双神经网络回归。</li>
<li>methods: 使用预测差异的方法，选择最近的 anchor 数据点作为预测差异的 Referent。</li>
<li>results: 在小到中等大的数据集上，该算法可以超越神经网络和 k-最近邻 regression。<details>
<summary>Abstract</summary>
Twin neural network regression is trained to predict differences between regression targets rather than the targets themselves. A solution to the original regression problem can be obtained by ensembling predicted differences between the targets of an unknown data point and multiple known anchor data points. Choosing the anchors to be the nearest neighbors of the unknown data point leads to a neural network-based improvement of k-nearest neighbor regression. This algorithm is shown to outperform both neural networks and k-nearest neighbor regression on small to medium-sized data sets.
</details>
<details>
<summary>摘要</summary>
双 neural network  regression 是用来预测目标之间的差异而不是目标本身。一种解决原始回归问题的解决方案是通过 ensemble 预测未知数据点的目标之间的差异和多个已知的 anchor 数据点之间的差异。选择 anchor 为未知数据点最近的邻居，可以实现基于 neural network 的 k-nearest neighbor 回归的改进。这种算法在小到中型数据集上表现出了超过 neural networks 和 k-nearest neighbor 回归的性能。
</details></li>
</ul>
<hr>
<h2 id="PatchMixer-A-Patch-Mixing-Architecture-for-Long-Term-Time-Series-Forecasting"><a href="#PatchMixer-A-Patch-Mixing-Architecture-for-Long-Term-Time-Series-Forecasting" class="headerlink" title="PatchMixer: A Patch-Mixing Architecture for Long-Term Time Series Forecasting"></a>PatchMixer: A Patch-Mixing Architecture for Long-Term Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00655">http://arxiv.org/abs/2310.00655</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Zeying-Gong/PatchMixer">https://github.com/Zeying-Gong/PatchMixer</a></li>
<li>paper_authors: Zeying Gong, Yujin Tang, Junwei Liang</li>
<li>for: 这篇论文主要针对时间序列预测任务进行了研究，尤其是解决Transformer Architecture中的Permutation-Invariant Self-Attention机制导致的一般挑战。</li>
<li>methods: 该论文提出了一种新的CNN基于模型，即PatchMixer，它使用了可变 convolutional structure来保留时间信息。与传统的CNN在这个领域中使用多尺度或多支流的方法不同，我们的方法仅使用了深度分割 convolutions，以EXTRACT both local features和全局相关性。</li>
<li>results: 我们的实验结果表明，相比最佳的方法和CNN，PatchMixer在七个时间序列预测 bencmarks上提供了3.9%和21.2%的相对改进，同时比最高级别的方法快2-3倍。<details>
<summary>Abstract</summary>
Although the Transformer has been the dominant architecture for time series forecasting tasks in recent years, a fundamental challenge remains: the permutation-invariant self-attention mechanism within Transformers leads to a loss of temporal information. To tackle these challenges, we propose PatchMixer, a novel CNN-based model. It introduces a permutation-variant convolutional structure to preserve temporal information. Diverging from conventional CNNs in this field, which often employ multiple scales or numerous branches, our method relies exclusively on depthwise separable convolutions. This allows us to extract both local features and global correlations using a single-scale architecture. Furthermore, we employ dual forecasting heads that encompass both linear and nonlinear components to better model future curve trends and details. Our experimental results on seven time-series forecasting benchmarks indicate that compared with the state-of-the-art method and the best-performing CNN, PatchMixer yields $3.9\%$ and $21.2\%$ relative improvements, respectively, while being 2-3x faster than the most advanced method. We will release our code and model.
</details>
<details>
<summary>摘要</summary>
尽管Transformer在最近几年内时序预测任务中占据主导地位，但是一个基本挑战仍然存在：Transformer中的卷积层的自注意力机制会导致时间信息的丢失。为解决这些挑战，我们提出了PatchMixer，一种新的CNN基于模型。它引入了一种可变卷积结构，以保留时间信息。与传统的CNN在这个领域中，通常采用多个缩放或多个分支，我们的方法仅仅采用深度分解卷积。这使得我们可以提取本地特征和全局相关性，使用单一的大小结构。此外，我们使用双重预测头，包括线性和非线性组件，以更好地模型未来曲线趋势和细节。我们的实验结果表明，与状态之前的方法和最佳CNN相比，PatchMixer在七个时序预测标准 benchmark上提供了3.9%和21.2%的相对提升，同时比最先进的方法快2-3倍。我们将发布我们的代码和模型。
</details></li>
</ul>
<hr>
<h2 id="A-primal-dual-perspective-for-distributed-TD-learning"><a href="#A-primal-dual-perspective-for-distributed-TD-learning" class="headerlink" title="A primal-dual perspective for distributed TD-learning"></a>A primal-dual perspective for distributed TD-learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00638">http://arxiv.org/abs/2310.00638</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han-Dong Lim, Donghwan Lee</li>
<li>for:  investigate distributed temporal difference (TD) learning for a networked multi-agent Markov decision process</li>
<li>methods:  based on distributed optimization algorithms, which can be interpreted as primal-dual Ordinary differential equation (ODE) dynamics subject to null-space constraints</li>
<li>results:  examined the behavior of the final iterate in various distributed TD-learning scenarios, considering both constant and diminishing step-sizes and incorporating both i.i.d. and Markovian observation models, without assuming a doubly stochastic matrix for the communication network structure.<details>
<summary>Abstract</summary>
The goal of this paper is to investigate distributed temporal difference (TD) learning for a networked multi-agent Markov decision process. The proposed approach is based on distributed optimization algorithms, which can be interpreted as primal-dual Ordinary differential equation (ODE) dynamics subject to null-space constraints. Based on the exponential convergence behavior of the primal-dual ODE dynamics subject to null-space constraints, we examine the behavior of the final iterate in various distributed TD-learning scenarios, considering both constant and diminishing step-sizes and incorporating both i.i.d. and Markovian observation models. Unlike existing methods, the proposed algorithm does not require the assumption that the underlying communication network structure is characterized by a doubly stochastic matrix.
</details>
<details>
<summary>摘要</summary>
本文的目标是研究分布式时间差（TD）学习 для网络化多 Agent Markov决策过程。我们提出的方法基于分布式优化算法，可以被看作为 primal-dual ordinary differential equation（ODE）动力学Subject to null-space constraints。基于 primal-dual ODE 动力学Subject to null-space constraints 的快速抽象行为，我们研究了不同分布式 TD-学习场景中的最终迭代器行为，包括常数和减少步长和 Markovian 观测模型。不同于现有方法，我们的算法不需要假设基础通信网络结构是 doubly stochastic matrix。
</details></li>
</ul>
<hr>
<h2 id="GNRK-Graph-Neural-Runge-Kutta-method-for-solving-partial-differential-equations"><a href="#GNRK-Graph-Neural-Runge-Kutta-method-for-solving-partial-differential-equations" class="headerlink" title="GNRK: Graph Neural Runge-Kutta method for solving partial differential equations"></a>GNRK: Graph Neural Runge-Kutta method for solving partial differential equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00618">http://arxiv.org/abs/2310.00618</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hoyunchoi/GNRK">https://github.com/hoyunchoi/GNRK</a></li>
<li>paper_authors: Hoyun Choi, Sungyeop Lee, B. Kahng, Junghyo Jo<br>for:* 这种新的方法可以用来解决广泛的偏微分方程（PDEs），而不需要特定的初始条件或偏微分方程的系数。methods:* 该方法基于图structures，使其对域分辨率和时间分辨率的变化具有抗锋性。* 该方法结合了图神经网络模块和回归结构，以提高其效率和通用性。results:* 对于2维布尔氏方程，GNRK表现出了较高的准确率和模型体积。* 该方法可以 straightforwardly拓展到解决相互关联的偏微分方程。<details>
<summary>Abstract</summary>
Neural networks have proven to be efficient surrogate models for tackling partial differential equations (PDEs). However, their applicability is often confined to specific PDEs under certain constraints, in contrast to classical PDE solvers that rely on numerical differentiation. Striking a balance between efficiency and versatility, this study introduces a novel approach called Graph Neural Runge-Kutta (GNRK), which integrates graph neural network modules with a recurrent structure inspired by the classical solvers. The GNRK operates on graph structures, ensuring its resilience to changes in spatial and temporal resolutions during domain discretization. Moreover, it demonstrates the capability to address general PDEs, irrespective of initial conditions or PDE coefficients. To assess its performance, we benchmark the GNRK against existing neural network based PDE solvers using the 2-dimensional Burgers' equation, revealing the GNRK's superiority in terms of model size and accuracy. Additionally, this graph-based methodology offers a straightforward extension for solving coupled differential equations, typically necessitating more intricate models.
</details>
<details>
<summary>摘要</summary>
The GNRK operates on graph structures, ensuring its ability to adapt to changes in spatial and temporal resolutions during domain discretization. Moreover, it can handle general PDEs regardless of initial conditions or PDE coefficients. To evaluate its performance, we benchmark the GNRK against existing neural network-based PDE solvers using the 2-dimensional Burgers' equation, demonstrating its superiority in terms of model size and accuracy.Furthermore, the graph-based methodology used in the GNRK provides a straightforward extension for solving coupled differential equations, which are typically more challenging to model. This study opens up new possibilities for using neural networks to solve a wide range of PDEs, with potential applications in fields such as fluid dynamics, heat transfer, and wave propagation.
</details></li>
</ul>
<hr>
<h2 id="On-the-Onset-of-Robust-Overfitting-in-Adversarial-Training"><a href="#On-the-Onset-of-Robust-Overfitting-in-Adversarial-Training" class="headerlink" title="On the Onset of Robust Overfitting in Adversarial Training"></a>On the Onset of Robust Overfitting in Adversarial Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00607">http://arxiv.org/abs/2310.00607</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chaojian Yu, Xiaolong Shi, Jun Yu, Bo Han, Tongliang Liu</li>
<li>for: 本研究旨在解释robust overfitting的基本机制，以及提出两种方法来缓解这种现象。</li>
<li>methods: 研究者通过分析normal data和敌方攻击的影响，并提出了一种基于factor ablation的方法来解释robust overfitting的起源。</li>
<li>results: 实验结果表明，提出的两种方法能够有效地缓解robust overfitting，并提高模型的 adversarial robustness。<details>
<summary>Abstract</summary>
Adversarial Training (AT) is a widely-used algorithm for building robust neural networks, but it suffers from the issue of robust overfitting, the fundamental mechanism of which remains unclear. In this work, we consider normal data and adversarial perturbation as separate factors, and identify that the underlying causes of robust overfitting stem from the normal data through factor ablation in AT. Furthermore, we explain the onset of robust overfitting as a result of the model learning features that lack robust generalization, which we refer to as non-effective features. Specifically, we provide a detailed analysis of the generation of non-effective features and how they lead to robust overfitting. Additionally, we explain various empirical behaviors observed in robust overfitting and revisit different techniques to mitigate robust overfitting from the perspective of non-effective features, providing a comprehensive understanding of the robust overfitting phenomenon. This understanding inspires us to propose two measures, attack strength and data augmentation, to hinder the learning of non-effective features by the neural network, thereby alleviating robust overfitting. Extensive experiments conducted on benchmark datasets demonstrate the effectiveness of the proposed methods in mitigating robust overfitting and enhancing adversarial robustness.
</details>
<details>
<summary>摘要</summary>
针对抗性训练（AT） Algorithm，我们尝试分离 normal data 和抗击干扰作用，并发现了 robust overfitting 的根本机制。我们发现，AT 中的 robust overfitting 问题来自 normal data 的因素缺失，而这些因素缺失导致模型学习无效的特征，我们称之为 non-effective features。我们进行了详细的非效果特征生成分析和如何导致 robust overfitting 的分析。此外，我们还解释了 robust overfitting 的不同实验现象，并重新评估了不同的技术来mitigate robust overfitting，从非效果特征的角度出发。基于这种理解，我们提出了两种方法，攻击强度和数据增强，来阻止神经网络学习非效果特征，从而缓解 robust overfitting。我们在标准数据集上进行了广泛的实验，并证明了我们的方法可以有效地缓解 robust overfitting 并提高对抗性。
</details></li>
</ul>
<hr>
<h2 id="Path-Structured-Multimarginal-Schrodinger-Bridge-for-Probabilistic-Learning-of-Hardware-Resource-Usage-by-Control-Software"><a href="#Path-Structured-Multimarginal-Schrodinger-Bridge-for-Probabilistic-Learning-of-Hardware-Resource-Usage-by-Control-Software" class="headerlink" title="Path Structured Multimarginal Schrödinger Bridge for Probabilistic Learning of Hardware Resource Usage by Control Software"></a>Path Structured Multimarginal Schrödinger Bridge for Probabilistic Learning of Hardware Resource Usage by Control Software</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00604">http://arxiv.org/abs/2310.00604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Georgiy A. Bondar, Robert Gifford, Linh Thi Xuan Phan, Abhishek Halder</li>
<li>for: 解决路径结构多重 Marginal Schrödinger Bridge 问题 (MSBP)，获得最可能的测量对象 trajectory，以便预测软件控制下硬件资源的时变分布。</li>
<li>methods: 利用最近的算法技术解决这类结构化 MSBP，以学习软件控制下硬件资源的使用情况。</li>
<li>results: 方法可以快速减少至精度预测硬件资源使用情况，并且可以应用到任何软件预测Cyber-physical上下文相依性性能。<details>
<summary>Abstract</summary>
The solution of the path structured multimarginal Schr\"{o}dinger bridge problem (MSBP) is the most-likely measure-valued trajectory consistent with a sequence of observed probability measures or distributional snapshots. We leverage recent algorithmic advances in solving such structured MSBPs for learning stochastic hardware resource usage by control software. The solution enables predicting the time-varying distribution of hardware resource availability at a desired time with guaranteed linear convergence. We demonstrate the efficacy of our probabilistic learning approach in a model predictive control software execution case study. The method exhibits rapid convergence to an accurate prediction of hardware resource utilization of the controller. The method can be broadly applied to any software to predict cyber-physical context-dependent performance at arbitrary time.
</details>
<details>
<summary>摘要</summary>
解决方案是多 Structured 多 marginal Schrödinger 桥Problem (MSBP) 的解决方案，它是一系列观测概率分布或分布快照的最有可能的测试值轨迹。我们利用了最新的算法技术来解决这种结构化MSBP，用于学习干扰控制软件的硬件资源使用情况。解决方案可以预测时间变化的硬件资源可用性，并且保证线性快速收敛。我们在一个模型预测控制软件执行 caso study 中证明了我们的概率学方法的有效性。方法在控制器中的硬件资源使用情况中显示了快速收敛到准确的预测。该方法可以广泛应用于任何软件，以预测任意时间点的Cyber-Physical context-dependent性能。
</details></li>
</ul>
<hr>
<h2 id="SIMD-Dataflow-Co-optimization-for-Efficient-Neural-Networks-Inferences-on-CPUs"><a href="#SIMD-Dataflow-Co-optimization-for-Efficient-Neural-Networks-Inferences-on-CPUs" class="headerlink" title="SIMD Dataflow Co-optimization for Efficient Neural Networks Inferences on CPUs"></a>SIMD Dataflow Co-optimization for Efficient Neural Networks Inferences on CPUs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00574">http://arxiv.org/abs/2310.00574</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cyrus Zhou, Zack Hassman, Ruize Xu, Dhirpal Shah, Vaugnn Richard, Yanjing Li</li>
<li>for: 这篇论文主要关注在CPU上执行神经网络时所遇到的挑战，特别是对于实时运算时间的最佳化，同时保持精度。</li>
<li>methods: 本研究使用了资料流（即神经网络计算顺序）来探索数据重复机会，运用了一个基于规律的分析和代码生成框架，以探索不同的单指令多数据（SIMD）实现方法，以获得优化神经网络执行。</li>
<li>results: 研究结果显示，以保持输出在SIMD注册中，同时将输入和权重重复 maximize 可以实现最好的性能，实现了对各种推理任务的广泛性和可靠性。具体来说，对于8位数字神经网络，可以 achieve 3x 速度提升；对于二进制神经网络，可以 achieve 4.8x 速度提升，对于现有的神经网络实现最佳化。<details>
<summary>Abstract</summary>
We address the challenges associated with deploying neural networks on CPUs, with a particular focus on minimizing inference time while maintaining accuracy. Our novel approach is to use the dataflow (i.e., computation order) of a neural network to explore data reuse opportunities using heuristic-guided analysis and a code generation framework, which enables exploration of various Single Instruction, Multiple Data (SIMD) implementations to achieve optimized neural network execution. Our results demonstrate that the dataflow that keeps outputs in SIMD registers while also maximizing both input and weight reuse consistently yields the best performance for a wide variety of inference workloads, achieving up to 3x speedup for 8-bit neural networks, and up to 4.8x speedup for binary neural networks, respectively, over the optimized implementations of neural networks today.
</details>
<details>
<summary>摘要</summary>
我们面临将神经网络部署到CPU上的挑战，尤其是对于降低推导时间而保持精度的最佳化。我们的新方法是根据神经网络的资料流（即computation order）进行回传分析和代码生成框架，以寻找可以使用单指令多数据（SIMD）实现最佳化的神经网络执行。我们的结果显示，可以将输出保持在SIMD寄存器中，同时将输入和权重重用到最大化的情况下，协助实现广泛的推导工作负载上的最高性能，相比今天的最佳化神经网络实现，8位数字神经网络可以达到3倍的速度提升，而二进制神经网络可以达到4.8倍的速度提升。
</details></li>
</ul>
<hr>
<h2 id="Discrete-Choice-Multi-Armed-Bandits"><a href="#Discrete-Choice-Multi-Armed-Bandits" class="headerlink" title="Discrete Choice Multi-Armed Bandits"></a>Discrete Choice Multi-Armed Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00562">http://arxiv.org/abs/2310.00562</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emerson Melo, David Müller</li>
<li>for: 这篇论文将绘制连接分类选择模型和在线学习多重枪支算法的关系。我们的贡献可以概括为两个关键方面：	1. 我们提供了下线 regret bound，覆盖广泛的算法家族，其中包括Exp3算法为特例。	2. 我们引入了一种新的对抗多重枪支算法家族， Drawing inspiration from generalized nested logit models introduced by \citet{wen:2001}.</li>
<li>methods: 我们的算法使用了分类选择模型，并且提供了closed-form sampling distribution probabilities，使其可以实现高效。</li>
<li>results: 我们通过数值实验，将我们的算法应用于随机枪支问题，并取得了实质性的结果。<details>
<summary>Abstract</summary>
This paper establishes a connection between a category of discrete choice models and the realms of online learning and multiarmed bandit algorithms. Our contributions can be summarized in two key aspects. Firstly, we furnish sublinear regret bounds for a comprehensive family of algorithms, encompassing the Exp3 algorithm as a particular case. Secondly, we introduce a novel family of adversarial multiarmed bandit algorithms, drawing inspiration from the generalized nested logit models initially introduced by \citet{wen:2001}. These algorithms offer users the flexibility to fine-tune the model extensively, as they can be implemented efficiently due to their closed-form sampling distribution probabilities. To demonstrate the practical implementation of our algorithms, we present numerical experiments, focusing on the stochastic bandit case.
</details>
<details>
<summary>摘要</summary>
Firstly, we provide sublinear regret bounds for a wide range of algorithms, including the Exp3 algorithm as a special case.Secondly, we introduce a new family of adversarial multiarmed bandit algorithms, inspired by the generalized nested logit models first introduced by \citet{wen:2001}. These algorithms allow for extensive fine-tuning of the model and can be efficiently implemented due to their closed-form sampling distribution probabilities.To demonstrate the practical application of our algorithms, we present numerical experiments focusing on the stochastic bandit case.
</details></li>
</ul>
<hr>
<h2 id="Horizontal-Class-Backdoor-to-Deep-Learning"><a href="#Horizontal-Class-Backdoor-to-Deep-Learning" class="headerlink" title="Horizontal Class Backdoor to Deep Learning"></a>Horizontal Class Backdoor to Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00542">http://arxiv.org/abs/2310.00542</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hua Ma, Shang Wang, Yansong Gao</li>
<li>for: 这篇论文旨在描述一种新型的水平类后门攻击（HCB），它可以通过含有无关的自然特征来让模型在某些情况下具有负面影响。</li>
<li>methods: 这篇论文使用了一种新的水平类后门攻击方法，它可以在模型中嵌入后门，并通过含有无关的自然特征来让后门在某些情况下被触发。</li>
<li>results: 实验结果表明，这种水平类后门攻击方法可以具有高效率和高攻击成功率，并且可以轻松地绕过多种已知的防御方法。<details>
<summary>Abstract</summary>
All existing backdoor attacks to deep learning (DL) models belong to the vertical class backdoor (VCB). That is, any sample from a class will activate the implanted backdoor in the presence of the secret trigger, regardless of source-class-agnostic or source-class-specific backdoor. Current trends of existing defenses are overwhelmingly devised for VCB attacks especially the source-class-agnostic backdoor, which essentially neglects other potential simple but general backdoor types, thus giving false security implications. It is thus urgent to discover unknown backdoor types.   This work reveals a new, simple, and general horizontal class backdoor (HCB) attack. We show that the backdoor can be naturally bounded with innocuous natural features that are common and pervasive in the real world. Note that an innocuous feature (e.g., expression) is irrelevant to the main task of the model (e.g., recognizing a person from one to another). The innocuous feature spans across classes horizontally but is exhibited by partial samples per class -- satisfying the horizontal class (HC) property. Only when the trigger is concurrently presented with the HC innocuous feature, can the backdoor be effectively activated. Extensive experiments on attacking performance in terms of high attack success rates with tasks of 1) MNIST, 2) facial recognition, 3) traffic sign recognition, and 4) object detection demonstrate that the HCB is highly efficient and effective. We extensively evaluate the HCB evasiveness against a (chronologically) series of 9 influential countermeasures of Fine-Pruning (RAID 18'), STRIP (ACSAC 19'), Neural Cleanse (Oakland 19'), ABS (CCS 19'), Februus (ACSAC 20'), MNTD (Oakland 21'), SCAn (USENIX SEC 21'), MOTH (Oakland 22'), and Beatrix (NDSS 23'), where none of them can succeed even when a simplest trigger is used.
</details>
<details>
<summary>摘要</summary>
所有现有的深度学习（DL）模型攻击都属于垂直类后门（VCB）。即任何一个类的样本在存在秘密触发符时，无论来源是否特定或不特定，都会触发嵌入的后门。现有的防御策略几乎都是为VCB攻击而设计的，特别是源特定后门，这实际上忽略了其他可能的简单但普遍的后门类型，从而给予false安全性。因此，发现未知后门类型是 Urgent。本工作发现了一种新的、简单的、普遍的水平类后门（HCB）攻击方法。我们表明，这种后门可以自然地与无关于主任务的 innocuous feature（例如表情）相结合，这种 innocuous feature 在实际世界中很普遍。注意，无关主任务的 innocuous feature 可以水平地跨类，但只有在触发符同时出现时，HC innocuous feature 才能够有效地触发后门。我们通过对不同任务的 MNIST、人脸识别、交通标识和物体检测进行了广泛的实验，并证明了 HCB 的高效率和可靠性。我们也进行了广泛的验证HCB的逃避性，并证明了HCB不受9种influential countermeasures的侵害，其中包括Fine-Pruning (RAID 18')、STRIP (ACSAC 19')、Neural Cleanse (Oakland 19')、ABS (CCS 19')、Februus (ACSAC 20')、MNTD (Oakland 21')、SCAn (USENIX SEC 21')、MOTH (Oakland 22')和Beatrix (NDSS 23')。 None of them can succeed even when a simplest trigger is used.
</details></li>
</ul>
<hr>
<h2 id="Robust-Nonparametric-Hypothesis-Testing-to-Understand-Variability-in-Training-Neural-Networks"><a href="#Robust-Nonparametric-Hypothesis-Testing-to-Understand-Variability-in-Training-Neural-Networks" class="headerlink" title="Robust Nonparametric Hypothesis Testing to Understand Variability in Training Neural Networks"></a>Robust Nonparametric Hypothesis Testing to Understand Variability in Training Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00541">http://arxiv.org/abs/2310.00541</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sinjini Banerjee, Reilly Cannon, Tim Marrinan, Tony Chiang, Anand D. Sarwate</li>
<li>for: 这个论文是为了描述一种新的模型相似度计算方法，用于评估深度神经网络（DNN）的训练过程中的杂态优化问题。</li>
<li>methods: 这篇论文使用了一种基于Robust Hypothesis Testing框架的新方法，用于评估DNN模型之间的相似度。这种方法不仅可以评估模型的测试准确率，还可以捕捉到模型计算的不同。</li>
<li>results: 这篇论文的结果表明，使用这种新方法可以更好地评估DNN模型之间的相似度，并且可以捕捉到模型计算的不同。这种方法可以用于评估其他从训练过程中得到的模型属性。<details>
<summary>Abstract</summary>
Training a deep neural network (DNN) often involves stochastic optimization, which means each run will produce a different model. Several works suggest this variability is negligible when models have the same performance, which in the case of classification is test accuracy. However, models with similar test accuracy may not be computing the same function. We propose a new measure of closeness between classification models based on the output of the network before thresholding. Our measure is based on a robust hypothesis-testing framework and can be adapted to other quantities derived from trained models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Thompson-Exploration-with-Best-Challenger-Rule-in-Best-Arm-Identification"><a href="#Thompson-Exploration-with-Best-Challenger-Rule-in-Best-Arm-Identification" class="headerlink" title="Thompson Exploration with Best Challenger Rule in Best Arm Identification"></a>Thompson Exploration with Best Challenger Rule in Best Arm Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00539">http://arxiv.org/abs/2310.00539</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jongyeong Lee, Junya Honda, Masashi Sugiyama</li>
<li>for: 本研究证明了一种在单参数几何模型下的fixed-confidence最佳臂标识问题（BAI）的解决方案。</li>
<li>methods: 我们提出了一种 combining Thompson sampling 和 computationally efficient approach 的策略，即最佳挑战者规则。</li>
<li>results: 我们证明了该策略是 any two-armed bandit problem 的 asymptotically optimal 策略，并且对于 general $K$-armed bandit problem  ($K\geq 3$) 也有 near optimality。在numerical experiments中，我们的策略与 asymptotically optimal 策略相比，具有更好的sample complexity 性能，同时具有更低的计算成本。<details>
<summary>Abstract</summary>
This paper studies the fixed-confidence best arm identification (BAI) problem in the bandit framework in the canonical single-parameter exponential models. For this problem, many policies have been proposed, but most of them require solving an optimization problem at every round and/or are forced to explore an arm at least a certain number of times except those restricted to the Gaussian model. To address these limitations, we propose a novel policy that combines Thompson sampling with a computationally efficient approach known as the best challenger rule. While Thompson sampling was originally considered for maximizing the cumulative reward, we demonstrate that it can be used to naturally explore arms in BAI without forcing it. We show that our policy is asymptotically optimal for any two-armed bandit problems and achieves near optimality for general $K$-armed bandit problems for $K\geq 3$. Nevertheless, in numerical experiments, our policy shows competitive performance compared to asymptotically optimal policies in terms of sample complexity while requiring less computation cost. In addition, we highlight the advantages of our policy by comparing it to the concept of $\beta$-optimality, a relaxed notion of asymptotic optimality commonly considered in the analysis of a class of policies including the proposed one.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Statistical-Limits-of-Adaptive-Linear-Models-Low-Dimensional-Estimation-and-Inference"><a href="#Statistical-Limits-of-Adaptive-Linear-Models-Low-Dimensional-Estimation-and-Inference" class="headerlink" title="Statistical Limits of Adaptive Linear Models: Low-Dimensional Estimation and Inference"></a>Statistical Limits of Adaptive Linear Models: Low-Dimensional Estimation and Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00532">http://arxiv.org/abs/2310.00532</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/licong-lin/low-dim-debias">https://github.com/licong-lin/low-dim-debias</a></li>
<li>paper_authors: Licong Lin, Mufang Ying, Suvrojit Ghosh, Koulik Khamaru, Cun-Hui Zhang</li>
<li>for: 本文研究了采集数据是否会对统计学 estimation和推断带来影响，具体来说是 linear models 中的 Ordinary Least Squares (OLS) 估计器是否能够在不同的数据采集方式下保持 asymptotic normality。</li>
<li>methods: 本文使用了 minimax lower bound 来描述在不同数据采集方式下 estimation 的性能差异，并 investigate 了高维 linear models 中 parameter 组件的估计性能如何受到数据采集方式的影响。</li>
<li>results: 本文发现，当数据采集方式是 adaptive 时，OLS 估计器可能会具有较大的 estimation error，而且这个 error 与数据采集方式的度数相关。然而，当数据采集方式是 i.i.d. 时，OLS 估计器的 estimation error 可以达到最佳性。此外，本文还提出了一种新的估计器，可以在 adaptive 数据采集方式下实现 single coordinate inference。这个估计器的 asymptotic normality 性也得到了证明。<details>
<summary>Abstract</summary>
Estimation and inference in statistics pose significant challenges when data are collected adaptively. Even in linear models, the Ordinary Least Squares (OLS) estimator may fail to exhibit asymptotic normality for single coordinate estimation and have inflated error. This issue is highlighted by a recent minimax lower bound, which shows that the error of estimating a single coordinate can be enlarged by a multiple of $\sqrt{d}$ when data are allowed to be arbitrarily adaptive, compared with the case when they are i.i.d. Our work explores this striking difference in estimation performance between utilizing i.i.d. and adaptive data. We investigate how the degree of adaptivity in data collection impacts the performance of estimating a low-dimensional parameter component in high-dimensional linear models. We identify conditions on the data collection mechanism under which the estimation error for a low-dimensional parameter component matches its counterpart in the i.i.d. setting, up to a factor that depends on the degree of adaptivity. We show that OLS or OLS on centered data can achieve this matching error. In addition, we propose a novel estimator for single coordinate inference via solving a Two-stage Adaptive Linear Estimating equation (TALE). Under a weaker form of adaptivity in data collection, we establish an asymptotic normality property of the proposed estimator.
</details>
<details>
<summary>摘要</summary>
“统计中的估计和推断在收集数据时存在重要的挑战。甚至在线性模型中，常数最小二乘（OLS）估计器可能无法在单坐标估计中展现 asymptotic normality，并且有较大的误差。这个问题得到了最近的最小下界 bound，显示了数据收集机制允许自由变化时，估计单坐标误差可以被增加为 $\sqrt{d}$ 倍，与独立Identically distributed（i.i.d）数据相比。我们的工作探讨了这种估计性能之间的差异，并研究了高维线性模型中低维参数组件的估计性能如何受到数据收集机制的影响。我们确定了数据收集机制下的condition under which the estimation error for a low-dimensional parameter component matches its counterpart in the i.i.d. setting, up to a factor that depends on the degree of adaptivity。我们还提出了一种新的估计器，可以在高维线性模型中实现单坐标推断。在一种更弱的数据收集机制下，我们证明了该估计器的 asymptotic normality 性。”Note: Please note that the translation is in Simplified Chinese, and the word order and grammar may be different from the original text.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/01/cs.LG_2023_10_01/" data-id="clpxp6c4f00suee88gj5kfbws" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_10_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/01/eess.IV_2023_10_01/" class="article-date">
  <time datetime="2023-10-01T09:00:00.000Z" itemprop="datePublished">2023-10-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/01/eess.IV_2023_10_01/">eess.IV - 2023-10-01</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="EventLFM-Event-Camera-integrated-Fourier-Light-Field-Microscopy-for-Ultrafast-3D-imaging"><a href="#EventLFM-Event-Camera-integrated-Fourier-Light-Field-Microscopy-for-Ultrafast-3D-imaging" class="headerlink" title="EventLFM: Event Camera integrated Fourier Light Field Microscopy for Ultrafast 3D imaging"></a>EventLFM: Event Camera integrated Fourier Light Field Microscopy for Ultrafast 3D imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00730">http://arxiv.org/abs/2310.00730</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruipeng Guo, Qianwan Yang, Andrew S. Chang, Guorong Hu, Joseph Greene, Christopher V. Gabel, Sixian You, Lei Tian</li>
<li>for:  This paper aims to develop a new imaging technique for visualizing complex and dynamic biological processes with high speed and large 3D space-bandwidth product (SBP).</li>
<li>methods:  The proposed technique, called EventLFM, combines an event camera with Fourier light field microscopy (LFM) to achieve single-shot 3D wide-field imaging with asynchronous readout and high data throughput.</li>
<li>results:  The authors demonstrate the ability of EventLFM to image fast-moving and rapidly blinking 3D samples at KHz frame rates and track GFP-labeled neurons in freely moving C. elegans with high accuracy.Here’s the Chinese translation of the three points:</li>
<li>for: 这篇论文的目的是开发一种能够高速、大的3D空间带宽产品（SBP）来可见复杂和动态生物过程的成像技术。</li>
<li>methods: 该提案的技术是将事件相机与富ouriet light field microscopy（LFM）相结合，实现单次广角成像，并且使用异步读取，以提高数据传输速率。</li>
<li>results: 作者们证明了事件LFM可以在KHz帧率下成像高速和快速闪烁的3D样品，并且可以准确地跟踪在自由移动C. elegans中的GFP标记neuron。<details>
<summary>Abstract</summary>
Ultrafast 3D imaging is indispensable for visualizing complex and dynamic biological processes. Conventional scanning-based techniques necessitate an inherent tradeoff between the acquisition speed and space-bandwidth product (SBP). While single-shot 3D wide-field techniques have emerged as an attractive solution, they are still bottlenecked by the synchronous readout constraints of conventional CMOS architectures, thereby limiting the data throughput by frame rate to maintain a high SBP. Here, we present EventLFM, a straightforward and cost-effective system that circumnavigates these challenges by integrating an event camera with Fourier light field microscopy (LFM), a single-shot 3D wide-field imaging technique. The event camera operates on a novel asynchronous readout architecture, thereby bypassing the frame rate limitations intrinsic to conventional CMOS systems. We further develop a simple and robust event-driven LFM reconstruction algorithm that can reliably reconstruct 3D dynamics from the unique spatiotemporal measurements from EventLFM. We experimentally demonstrate that EventLFM can robustly image fast-moving and rapidly blinking 3D samples at KHz frame rates and furthermore, showcase EventLFM's ability to achieve 3D tracking of GFP-labeled neurons in freely moving C. elegans. We believe that the combined ultrafast speed and large 3D SBP offered by EventLFM may open up new possibilities across many biomedical applications.
</details>
<details>
<summary>摘要</summary>
超速3D成像是生物过程视化中不可或缺的。传统的扫描方式存在固有的质量比速度产品（SBP）交换限制，而单发3D广阔技术受到传统CMOS架构的同步读取限制，因此对数据传输率做出了限制，即保持高SBP的情况下，帧率限制。在这里，我们介绍了EventLFM，一种简单且cost-effective的系统，通过将事件摄像头与快速光场镜微scopio（LFM）结合，绕过了传统CMOS系统中的帧率限制。事件摄像头使用了新的异步读取架构，因此可以快速响应快速变化的3D动态过程。我们还开发了一种简单可靠的事件驱动LFM重构算法，可以可靠地从EventLFM中获取3D动力学。我们实验表明，EventLFM可以Robustly图像高速运动和快速灯泡3D样本，并且可以实现C. elegans中GFP标记的 neuron 3D跟踪。我们认为EventLFM的总体快速速度和大3DSBP可能会开拓新的生物医学应用领域。
</details></li>
</ul>
<hr>
<h2 id="Spatiotemporal-Image-Reconstruction-to-Enable-High-Frame-Rate-Dynamic-Photoacoustic-Tomography-with-Rotating-Gantry-Volumetric-Imagers"><a href="#Spatiotemporal-Image-Reconstruction-to-Enable-High-Frame-Rate-Dynamic-Photoacoustic-Tomography-with-Rotating-Gantry-Volumetric-Imagers" class="headerlink" title="Spatiotemporal Image Reconstruction to Enable High-Frame Rate Dynamic Photoacoustic Tomography with Rotating-Gantry Volumetric Imagers"></a>Spatiotemporal Image Reconstruction to Enable High-Frame Rate Dynamic Photoacoustic Tomography with Rotating-Gantry Volumetric Imagers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00529">http://arxiv.org/abs/2310.00529</a></li>
<li>repo_url: None</li>
<li>paper_authors: Refik M. Cam, Chao Wang, Weylan Thompson, Sergey A. Ermilov, Mark A. Anastasio, Umberto Villa</li>
<li>For: 这种研究旨在开发一种能够应用于现有的扫描仪器上的快速扫描 PACT 图像重建方法，以解决现有系统中数据缺失的问题，并提高图像重建的精度和速度。* Methods: 该方法基于低级别矩阵估计（LRME），利用空间时间重复性来准确重建4D 空间时间图像。* Results: 数值研究表明，该方法可以准确地重建4D 动态图像，而实验研究则证明了该方法在实际应用中的可靠性和效果。<details>
<summary>Abstract</summary>
Significance: Dynamic photoacoustic computed tomography (PACT) is a valuable technique for monitoring physiological processes. However, current dynamic PACT techniques are often limited to 2D spatial imaging. While volumetric PACT imagers are commercially available, these systems typically employ a rotating gantry in which the tomographic data are sequentially acquired. Because the object varies during the data-acquisition process, the sequential data-acquisition poses challenges to image reconstruction associated with data incompleteness. The proposed method is highly significant in that it will address these challenges and enable volumetric dynamic PACT imaging with existing imagers. Aim: The aim of this study is to develop a spatiotemporal image reconstruction (STIR) method for dynamic PACT that can be applied to commercially available volumetric PACT imagers that employ a sequential scanning strategy. The proposed method aims to overcome the challenges caused by the limited number of tomographic measurements acquired per frame. Approach: A low-rank matrix estimation-based STIR method (LRME-STIR) is proposed to enable dynamic volumetric PACT. The LRME-STIR method leverages the spatiotemporal redundancies to accurately reconstruct a 4D spatiotemporal image. Results: The numerical studies substantiate the LRME-STIR method's efficacy in reconstructing 4D dynamic images from measurements acquired with a rotating gantry. The experimental study demonstrates the method's ability to faithfully recover the flow of a contrast agent at a frame rate of 0.1 s even when only a single tomographic measurement per frame is available. Conclusions: The LRME-STIR method offers a promising solution to the challenges faced by enabling 4D dynamic imaging using commercially available volumetric imagers. By enabling accurate 4D reconstruction, this method has the potential to advance preclinical research.
</details>
<details>
<summary>摘要</summary>
significación: La tomografía por sonido fotográfico dinámico (PACT) es una técnica valiosa para monitorear procesos fisiológicos. Sin embargo, las técnicas de PACT dinámicas actuales suelen limitarse a la imagen espacial bidimensional. Los escáneres de PACT volumétricos comerciales suelen emplear una gánglia rotatoria en la que los datos tomográficos se adquieren secuencialmente. Como el objeto varía durante el proceso de adquisición de datos, la adquisición de datos secuenciales plantea desafíos en la reconstrucción de imágenes asociada con la incompletitud de los datos. El método propuesto es altamente significativo ya que abordará estos desafíos y permitirá la imagen de volumetría dinámica PACT con imagers existentes. objetivo: El objetivo de este estudio es desarrollar un método de reconstrucción de imágenes espacio-temporal (STIR) para la PACT dinámica que pueda aplicarse a los imagers volumétricos PACT comerciales que utilizan una estrategia de escaneo secuencial. El método propuesto busca superar los desafíos causados por el número limitado de medidas tomográficas adquiridas por frame. enfoque: Se propone un método de estimación de matrices de baja riqueza (LRME-STIR) para la reconstrucción de imágenes espacio-temporales. El método de LRME-STIR aprovecha las redundancias espacio-temporales para reconstruir precisamente una imagen espacio-temporal de 4D. resultados: Los estudios numéricos respaldan la eficacia del método LRME-STIR en la reconstrucción de imágenes dinámicas de 4D a partir de medidas adquiridas con una gánglia rotatoria. El estudio experimental demuestra la capacidad del método para recuperar fielmente el flujo de un agente de contraste a una tasa de cuadros de 0,1 s, incluso cuando solo se adquieren medidas tomográficas por frame. conclusiones: El método LRME-STIR ofrece una solución prometedora para los desafíos que enfrenta la imagen de volumetría dinámica con imagers existentes. Al permitir la reconstrucción precisa de imágenes de 4D, este método tiene el potencial de avanzar en la investigación preclínica.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/01/eess.IV_2023_10_01/" data-id="clpxp6cbk01bfee880nn8gdfi" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_10_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/01/eess.SP_2023_10_01/" class="article-date">
  <time datetime="2023-10-01T08:00:00.000Z" itemprop="datePublished">2023-10-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/01/eess.SP_2023_10_01/">eess.SP - 2023-10-01</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="DISCO-Might-Not-Be-Funky-Random-Intelligent-Reflective-Surface-Configurations-That-Attack"><a href="#DISCO-Might-Not-Be-Funky-Random-Intelligent-Reflective-Surface-Configurations-That-Attack" class="headerlink" title="DISCO Might Not Be Funky: Random Intelligent Reflective Surface Configurations That Attack"></a>DISCO Might Not Be Funky: Random Intelligent Reflective Surface Configurations That Attack</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00687">http://arxiv.org/abs/2310.00687</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huan Huang, Lipeng Dai, Hongliang Zhang, Chongfu Zhang, Zhongxing Tian, Yi Cai, A. Lee Swindlehurst, Zhu Han</li>
<li>for: 这篇论文是关于恶意反射表面（IRS）对物理层安全（PLS）的研究。</li>
<li>methods: 这篇论文提出了一种基于恶意IRS的完全游离式干扰器（FPJ），并介绍了其应用场景和技术原理。</li>
<li>results: 这篇论文提出了一种不需要干扰功率或渠道状态信息（CSI）的恶意干扰器，并提出了一种基于统计的反干扰策略。此外，论文还介绍了一种能够在存在恶意干扰的情况下估计统计CSI的数据帧结构。<details>
<summary>Abstract</summary>
Emerging intelligent reflective surfaces (IRSs) significantly improve system performance, but also pose a signifcant risk for physical layer security (PLS). Unlike the extensive research on legitimate IRS-enhanced communications, in this article we present an adversarial IRS-based fully-passive jammer (FPJ). We describe typical application scenarios for Disco IRS (DIRS)-based FPJ, where an illegitimate IRS with random, time-varying reflection properties acts like a "disco ball" to randomly change the propagation environment. We introduce the principles of DIRS-based FPJ and overview existing investigations of the technology, including a design example employing one-bit phase shifters. The DIRS-based FPJ can be implemented without either jamming power or channel state information (CSI) for the legitimate users (LUs). It does not suffer from the energy constraints of traditional active jammers, nor does it require any knowledge of the LU channels. In addition to the proposed jamming attack, we also propose an anti-jamming strategy that requires only statistical rather than instantaneous CSI. Furthermore, we present a data frame structure that enables the legitimate access point (AP) to estimate the statistical CSI in the presence of the DIRS jamming. Typical cases are discussed to show the impact of the DIRS-based FPJ and the feasibility of the anti-jamming precoder. Moreover, we outline future research directions and challenges for the DIRS-based FPJ and its anti-jamming precoding to stimulate this line of research and pave the way for practical applications.
</details>
<details>
<summary>摘要</summary>
emerging intelligent reflective surfaces (IRSs) significantly improve system performance, but also pose a significant risk for physical layer security (PLS). unlike the extensive research on legitimate IRS-enhanced communications, in this article we present an adversarial IRS-based fully-passive jammer (FPJ). we describe typical application scenarios for Disco IRS (DIRS)-based FPJ, where an illegitimate IRS with random, time-varying reflection properties acts like a "disco ball" to randomly change the propagation environment. we introduce the principles of DIRS-based FPJ and overview existing investigations of the technology, including a design example employing one-bit phase shifters. the DIRS-based FPJ can be implemented without either jamming power or channel state information (CSI) for the legitimate users (LUs). it does not suffer from the energy constraints of traditional active jammers, nor does it require any knowledge of the LU channels. in addition to the proposed jamming attack, we also propose an anti-jamming strategy that requires only statistical rather than instantaneous CSI. furthermore, we present a data frame structure that enables the legitimate access point (AP) to estimate the statistical CSI in the presence of the DIRS jamming. typical cases are discussed to show the impact of the DIRS-based FPJ and the feasibility of the anti-jamming precoder. moreover, we outline future research directions and challenges for the DIRS-based FPJ and its anti-jamming precoding to stimulate this line of research and pave the way for practical applications.
</details></li>
</ul>
<hr>
<h2 id="Sequential-Monte-Carlo-Graph-Convolutional-Network-for-Dynamic-Brain-Connectivity"><a href="#Sequential-Monte-Carlo-Graph-Convolutional-Network-for-Dynamic-Brain-Connectivity" class="headerlink" title="Sequential Monte Carlo Graph Convolutional Network for Dynamic Brain Connectivity"></a>Sequential Monte Carlo Graph Convolutional Network for Dynamic Brain Connectivity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00630">http://arxiv.org/abs/2310.00630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fengfan Zhao, Ercan Engin Kuruoglu</li>
<li>for: 这项研究旨在提出一种基于粒子滤波算法的功能连接分析方法，用于探索脑功能缺陷和脑疾病相关的结构性破坏之间的关系。</li>
<li>methods: 该方法基于粒子滤波算法，可以在只有部分和噪声的观察数据情况下，不假设站立性的连接topology，并通过Sequential Monte Carlo Graph Convolutional Network (SMC-GCN)来限制干扰连接。</li>
<li>results: 实验研究表明，SMC-GCN方法在脑疾病分类任务中表现出色，超过了其他方法的性能。<details>
<summary>Abstract</summary>
An increasingly important brain function analysis modality is functional connectivity analysis which regards connections as statistical codependency between the signals of different brain regions. Graph-based analysis of brain connectivity provides a new way of exploring the association between brain functional deficits and the structural disruption related to brain disorders, but the current implementations have limited capability due to the assumptions of noise-free data and stationary graph topology. We propose a new methodology based on the particle filtering algorithm, with proven success in tracking problems, which estimates the hidden states of a dynamic graph with only partial and noisy observations, without the assumptions of stationarity on connectivity. We enrich the particle filtering state equation with a graph Neural Network called Sequential Monte Carlo Graph Convolutional Network (SMC-GCN), which due to the nonlinear regression capability, can limit spurious connections in the graph. Experiment studies demonstrate that SMC-GCN achieves the superior performance of several methods in brain disorder classification.
</details>
<details>
<summary>摘要</summary>
▼ 请注意，以下文本将被翻译成简化中文。一种日益重要的大脑功能分析方法是函数连接分析，它视连接为脑区域信号的统计 codependency。基于图的Brain Connectivity分析提供了一种探索脑功能缺陷和脑疾病相关的结构性破坏的新方法，但现有实现受限因为假设了噪声自由数据和静止的图表结构。我们提出了一种基于粒子滤波算法的新方法，该算法在跟踪问题中证明了成功，可以在只有部分和噪声的观察数据情况下估计图中隐藏的状态。我们在粒子滤波状态方程中添加了一种图神经网络 called Sequential Monte Carlo Graph Convolutional Network (SMC-GCN)，该网络具有非线性回归能力，可以限制图中的假设连接。实验研究表明，SMC-GCN可以在脑疾病分类方面达到更高的性能。
</details></li>
</ul>
<hr>
<h2 id="Nonlinear-Multi-Carrier-System-with-Signal-Clipping-Measurement-Analysis-and-Optimization"><a href="#Nonlinear-Multi-Carrier-System-with-Signal-Clipping-Measurement-Analysis-and-Optimization" class="headerlink" title="Nonlinear Multi-Carrier System with Signal Clipping: Measurement, Analysis, and Optimization"></a>Nonlinear Multi-Carrier System with Signal Clipping: Measurement, Analysis, and Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00593">http://arxiv.org/abs/2310.00593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuyang Du, Liang Hao, Yiming Lei</li>
<li>for: 降低OFDM系统中的峰峰值至平均值比率 (PAPR)</li>
<li>methods: 使用BFPA模型分析发射器非线性，并通过干扰产品分析简化发射器电压表达</li>
<li>results: 对非线性剪辑OFDM系统进行优化设计，以实现实际系统中的SER下界In English:</li>
<li>for: Reducing the peak-to-average power ratio (PAPR) in OFDM systems</li>
<li>methods: Using the Bessel-Fourier PA (BFPA) model to analyze the nonlinearity of the power amplifier (PA), and simplifying the power expression using inter-modulation product (IMP) analysis</li>
<li>results: Optimizing the system setting for a nonlinear clipped OFDM system to achieve the symbol error rate (SER) lower bound in a practical system that considers both PA nonlinearity and clipping distortion.<details>
<summary>Abstract</summary>
Signal clipping is a classic technique for reducing peak-to-average power ratio (PAPR) in orthogonal frequency division multiplexing (OFDM) systems. It has been widely applied in consumer electronic devices owing to its low complexity and high efficiency. Although clipping reduces the nonlinear distortion caused by power amplifiers (PAs), it induces additional clipping distortion. Optimizing the joint system performance with consideration of both PA nonlinearity and clipping distortion remains an open problem due to the complex PA modeling. In this paper, we analyze the PA nonlinearity through the Bessel-Fourier PA (BFPA) model and simplify its power expression using inter-modulation product (IMP) analysis. We derive expressions of the receiver signal-to-noise ratio (SNR) and system symbol error rate (SER) for the nonlinear clipped OFDM system. With the derivations, we investigate the optimal system setting to achieve the SER lower bound in a practical OFDM system that considers both PA nonlinearity and clipping distortion. The methods and results presented in this paper can serve as a useful reference for the system-level optimization of clipped OFDM systems with nonlinear PA.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXTSignal clipping is a classic technique for reducing peak-to-average power ratio (PAPR) in orthogonal frequency division multiplexing (OFDM) systems. It has been widely applied in consumer electronic devices owing to its low complexity and high efficiency. Although clipping reduces the nonlinear distortion caused by power amplifiers (PAs), it induces additional clipping distortion. Optimizing the joint system performance with consideration of both PA nonlinearity and clipping distortion remains an open problem due to the complex PA modeling. In this paper, we analyze the PA nonlinearity through the Bessel-Fourier PA (BFPA) model and simplify its power expression using inter-modulation product (IMP) analysis. We derive expressions of the receiver signal-to-noise ratio (SNR) and system symbol error rate (SER) for the nonlinear clipped OFDM system. With the derivations, we investigate the optimal system setting to achieve the SER lower bound in a practical OFDM system that considers both PA nonlinearity and clipping distortion. The methods and results presented in this paper can serve as a useful reference for the system-level optimization of clipped OFDM systems with nonlinear PA.TRANSLATE_TEXT
</details></li>
</ul>
<hr>
<h2 id="An-IRS-Assisted-Secure-Dual-Function-Radar-Communication-System"><a href="#An-IRS-Assisted-Secure-Dual-Function-Radar-Communication-System" class="headerlink" title="An IRS-Assisted Secure Dual-Function Radar-Communication System"></a>An IRS-Assisted Secure Dual-Function Radar-Communication System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00555">http://arxiv.org/abs/2310.00555</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi-Kai Li, Athina Petropulu</li>
<li>for: 提高双功能雷达通信系统的物理层安全性（PLS）</li>
<li>methods: 使用智能反射 superficie（IRS）和人工噪声（AN），并optimize the radar waveform, AN jamming noise, and IRS parameters to maximize the communication secrecy rate while meeting radar signal-to-noise ratio（SNR） constraints.</li>
<li>results: 提出一种新的系统设计方案，并使用分数编程技术将分数形目标函数转化为更易处理的非分数多项式。数值结果表明系统设计算法的收敛性，并显示了噪声分配对系统安全性的影响。<details>
<summary>Abstract</summary>
In dual-function radar-communication (DFRC) systems the probing signal contains information intended for the communication users, which makes that information vulnerable to eavesdropping by the targets. We propose a novel design for enhancing the physical layer security (PLS) of DFRC systems, via the help of intelligent reflecting surface (IRS) and artificial noise (AN), transmitted along with the probing waveform. The radar waveform, the AN jamming noise and the IRS parameters are designed to optimize the communication secrecy rate while meeting radar signal-to-noise ratio (SNR) constrains. Key challenges in the resulting optimization problem include the fractional form objective, the SNR being a quartic function of the IRS parameters, and the unit-modulus constraint of the IRS parameters. A fractional programming technique is used to transform the fractional form objective of the optimization problem into more tractable non-fractional polynomials. Numerical results are provided to demonstrate the convergence of the proposed system design algorithm, and also show the impact of the power assigned to the AN on the secrecy performance of the designed system.
</details>
<details>
<summary>摘要</summary>
在双功能雷达通信（DFRC）系统中，探测信号包含向通信用户传递的信息，因此这些信息容易受到目标的窃听。我们提议一种新的设计方案，以增强双功能雷达通信系统的物理层安全性（PLS），通过利用智能反射表面（IRS）和人工噪声（AN），同探测波形一起传输。雷达波形、噪声干扰和IRS参数是根据优化通信秘密率的要求，同时满足雷达信号响应比（SNR）的限制。关键挑战包括分数形目标函数、SNR为IRS参数的四次函数，以及IRS参数的单位模式约束。我们使用分数编程技术将分数形目标函数转换为更易处理的非分数多项式。numerical results show that the proposed system design algorithm converges and demonstrate the impact of the power assigned to the AN on the secrecy performance of the designed system.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. If you prefer Traditional Chinese, please let me know and I will be happy to provide the translation in that form instead.
</details></li>
</ul>
<hr>
<h2 id="An-Experimental-Prototype-for-Multistatic-Asynchronous-ISAC"><a href="#An-Experimental-Prototype-for-Multistatic-Asynchronous-ISAC" class="headerlink" title="An Experimental Prototype for Multistatic Asynchronous ISAC"></a>An Experimental Prototype for Multistatic Asynchronous ISAC</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00548">http://arxiv.org/abs/2310.00548</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Canil, Jacopo Pegoraro, Jesus O. Lacruz, Marco Mezzavilla, Michele Rossi, Joerg Widmer, Sundeep Rangan</li>
<li>for: 该论文旨在实现基于IEEE802.11ay的多Static millimeter wave ISAC系统，并 validate its performance。</li>
<li>methods: 该系统使用了单线对线（LoS）无线信号媒体进行时钟偏差补偿，以实现同时进行目标跟踪和微多普勒估计。</li>
<li>results: 实验结果表明，多Static ISAC系统可以提供更高的感知能力，具有多视角的接收节点空间多样性。<details>
<summary>Abstract</summary>
We prototype and validate a multistatic mmWave ISAC system based on IEEE802.11ay. Compensation of the clock asynchrony between each TX and RX pair is performed using the sole LoS wireless signal propagation. As a result, our system provides concurrent target tracking and micro-Doppler estimation from multiple points of view, paving the way for practical multistatic data fusion. Our results on human movement sensing, complemented with precise, quantitative GT data, demonstrate the enhanced sensing capabilities of multistatic ISAC, due to the spatial diversity of the receiver nodes.
</details>
<details>
<summary>摘要</summary>
我们研究和验证了一个基于IEEE802.11ay的多态 millimeter wave ISAC系统。我们使用唯一的视线无线信号媒体进行时钟偏差补偿，因此我们的系统可以同时进行目标跟踪和微多普勒估算，从多个视点来源获得实用的数据融合。我们对人体运动感知进行了补充，并且通过精确的量化GT数据，示出了多态 ISAC的感知能力的增强，即因为接收节点的空间多样性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/01/eess.SP_2023_10_01/" data-id="clpxp6cde01fjee88893of3zp" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_09_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/30/cs.SD_2023_09_30/" class="article-date">
  <time datetime="2023-09-30T15:00:00.000Z" itemprop="datePublished">2023-09-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/30/cs.SD_2023_09_30/">cs.SD - 2023-09-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Time-Variant-Overlap-Add-in-Partitions"><a href="#Time-Variant-Overlap-Add-in-Partitions" class="headerlink" title="Time-Variant Overlap-Add in Partitions"></a>Time-Variant Overlap-Add in Partitions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00319">http://arxiv.org/abs/2310.00319</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/TGM-Oldenburg/TVOLAP">https://github.com/TGM-Oldenburg/TVOLAP</a></li>
<li>paper_authors: Hagen Jaeger, Uwe Simmer, Jörg Bitzer, Matthias Blau</li>
<li>for: 这篇论文是关于用于虚拟和增强现实环境中的听音渲染技术的研究。</li>
<li>methods: 该论文提出了一种分解式卷积算法，可以在实时中快速切换各种响应函数，而无需产生明显的切换artifacts，同时保持常见计算成本和内存占用量。</li>
<li>results: 该算法在多种popular编程语言中的实现可以免除听觉switching artifacts，并且可以保持常见计算成本和内存占用量。代码可以在GitHub上免费下载。<details>
<summary>Abstract</summary>
Virtual and augmented realities are increasingly popular tools in many domains such as architecture, production, training and education, (psycho)therapy, gaming, and others. For a convincing rendering of sound in virtual and augmented environments, audio signals must be convolved in real-time with impulse responses that change from one moment in time to another. Key requirements for the implementation of such time-variant real-time convolution algorithms are short latencies, moderate computational cost and memory footprint, and no perceptible switching artifacts. In this engineering report, we introduce a partitioned convolution algorithm that is able to quickly switch between impulse responses without introducing perceptible artifacts, while maintaining a constant computational load and low memory usage. Implementations in several popular programming languages are freely available via GitHub.
</details>
<details>
<summary>摘要</summary>
虚拟和增强现实技术在多个领域得到了广泛应用，如建筑、生产、培训和教育、心理治疗、游戏等。为在虚拟和增强环境中提供真实的声音渲染，音频信号需要在实时中扩散到不同的冲击回应函数，这些函数在时间上变化。实现时变实时扩散算法的关键要求包括：短延迟时间、moderate计算成本和内存占用量，无法识别的切换 artifacts。本工程报告中，我们介绍了一种分解扩散算法，可以快速切换冲击回应函数，而无需引入明显的artefacts，同时保持了常量计算负担和内存占用量。实现在多种popular编程语言上可以免费获取于GitHub。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-U-Net-Architecture-for-Denoising-of-Real-world-Noise-Corrupted-Phonocardiogram-Signal"><a href="#A-Novel-U-Net-Architecture-for-Denoising-of-Real-world-Noise-Corrupted-Phonocardiogram-Signal" class="headerlink" title="A Novel U-Net Architecture for Denoising of Real-world Noise Corrupted Phonocardiogram Signal"></a>A Novel U-Net Architecture for Denoising of Real-world Noise Corrupted Phonocardiogram Signal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00216">http://arxiv.org/abs/2310.00216</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ayan Mukherjee, Rohan Banerjee, Avik Ghose</li>
<li>For: 本研究旨在提出一种基于U-Net深度神经网络架构的心听音信号杂谔除去方法，以解决在医学 auscultation 中心听音信号杂谔问题。* Methods: 为了设计、开发和验证提议的架构，我们提出了一种新的实验方法，利用现实世界噪声污染的PCG信号 DATASET 和一个开放式PCG DATASET。* Results: 对比与现有状态的先进技术，我们的杂谔除去方法在Synthesized noisy PCG DATASET 上的性能评估表明，提出的方法在识别和预测方面具有显著的改进。<details>
<summary>Abstract</summary>
The bio-acoustic information contained within heart sound signals are utilized by physicians world-wide for auscultation purpose. However, the heart sounds are inherently susceptible to noise contamination. Various sources of noises like lung sound, coughing, sneezing, and other background noises are involved in such contamination. Such corruption of the heart sound signal often leads to inconclusive or false diagnosis. To address this issue, we have proposed a novel U-Net based deep neural network architecture for denoising of phonocardiogram (PCG) signal in this paper. For the design, development and validation of the proposed architecture, a novel approach of synthesizing real-world noise corrupted PCG signals have been proposed. For the purpose, an open-access real-world noise sample dataset and an open-access PCG dataset has been utilized. The performance of the proposed denoising methodology has been evaluated on the synthesized noisy PCG dataset. The performance of the proposed algorithm has been compared with existing state-of-the-art (SoA) denoising algorithms qualitatively and quantitatively. The proposed denoising technique has shown improvement in performance as comparison to the SoAs.
</details>
<details>
<summary>摘要</summary>
生物声学信息在心声信号中含有，医生世界各地通过 auscultation 来利用这些信息。然而，心声信号具有自然潜在的噪声污染。这些噪声包括肺 зву、喷嚏、喷嚏、和其他背景噪声等。这种噪声污染可能导致不正确或不 conclution 的诊断。为解决这个问题，我们在本文中提出了一种基于 U-Net 深度神经网络架构的PCG 信号杂音除除法。为了设计、开发和验证该架构，我们提出了一种新的实际噪声污染 PCG 信号生成方法。为此，我们使用了一个开放访问的实际噪声样本数据集和一个开放访问的 PCG 数据集。我们对提出的杂音除法方法进行评估，并与现有状态的最佳方法（SoA）进行比较。我们发现，提出的杂音除法方法在比较 SoA 方法时显示出了改善的性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/30/cs.SD_2023_09_30/" data-id="clpxp6c74010cee886o20dg95" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_09_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/30/cs.CV_2023_09_30/" class="article-date">
  <time datetime="2023-09-30T13:00:00.000Z" itemprop="datePublished">2023-09-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/30/cs.CV_2023_09_30/">cs.CV - 2023-09-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Assessing-the-Generalizability-of-Deep-Neural-Networks-Based-Models-for-Black-Skin-Lesions"><a href="#Assessing-the-Generalizability-of-Deep-Neural-Networks-Based-Models-for-Black-Skin-Lesions" class="headerlink" title="Assessing the Generalizability of Deep Neural Networks-Based Models for Black Skin Lesions"></a>Assessing the Generalizability of Deep Neural Networks-Based Models for Black Skin Lesions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00517">http://arxiv.org/abs/2310.00517</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/httplups/black-acral-skin-lesion-detection">https://github.com/httplups/black-acral-skin-lesion-detection</a></li>
<li>paper_authors: Luana Barros, Levy Chaves, Sandra Avila</li>
<li>for: 这个论文主要用于检测皮肤癌症，特别是针对黑人群体中的肤色区域（手掌、足底和指甲）。</li>
<li>methods: 该论文使用深度神经网络进行检测，并分别评估了指导式和自我指导式模型在黑人肤色区域中的表现。</li>
<li>results: 研究发现，现有的深度神经网络模型在黑人肤色区域中的性能不佳，只能在白皮肤区域中表现出色。这显示了这些模型在不同肤色区域中的一致性不足。<details>
<summary>Abstract</summary>
Melanoma is the most severe type of skin cancer due to its ability to cause metastasis. It is more common in black people, often affecting acral regions: palms, soles, and nails. Deep neural networks have shown tremendous potential for improving clinical care and skin cancer diagnosis. Nevertheless, prevailing studies predominantly rely on datasets of white skin tones, neglecting to report diagnostic outcomes for diverse patient skin tones. In this work, we evaluate supervised and self-supervised models in skin lesion images extracted from acral regions commonly observed in black individuals. Also, we carefully curate a dataset containing skin lesions in acral regions and assess the datasets concerning the Fitzpatrick scale to verify performance on black skin. Our results expose the poor generalizability of these models, revealing their favorable performance for lesions on white skin. Neglecting to create diverse datasets, which necessitates the development of specialized models, is unacceptable. Deep neural networks have great potential to improve diagnosis, particularly for populations with limited access to dermatology. However, including black skin lesions is necessary to ensure these populations can access the benefits of inclusive technology.
</details>
<details>
<summary>摘要</summary>
癌症是皮肤癌症中最严重的一种，因为它可以导致肿瘤迁移。它更常见于黑人，通常会影响到手掌、脚底和指甲。深度神经网络在临床护理和皮肤癌诊断方面表现出了巨大的潜力。然而，现有的研究大多涉及白皮肤Dataset，忽略了不同皮肤颜色的患者诊断结果的报告。在这项工作中，我们评估了指导和自动化模型在黑人常见的手掌、脚底和指甲部位上的皮肤癌图像中的表现。此外，我们也仔细筛选了包含黑人皮肤癌图像的Dataset，并评估了该Dataset在Fitzpatrick级别中的表现，以确认模型在黑皮肤上的性能。我们的结果表明，现有的模型在白皮肤上表现良好，但对黑皮肤的患者来说，这些模型的总体性能很差。忽略创建多样化的Dataset是不可接受的。深度神经网络在护理方面具有极大的潜力，特别是对于有限的资源的人群，但是包含黑皮肤癌图像是必要的，以确保这些人群可以通过包容技术获得诊断的优势。
</details></li>
</ul>
<hr>
<h2 id="Exploring-SAM-Ablations-for-Enhancing-Medical-Segmentation-in-Radiology-and-Pathology"><a href="#Exploring-SAM-Ablations-for-Enhancing-Medical-Segmentation-in-Radiology-and-Pathology" class="headerlink" title="Exploring SAM Ablations for Enhancing Medical Segmentation in Radiology and Pathology"></a>Exploring SAM Ablations for Enhancing Medical Segmentation in Radiology and Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00504">http://arxiv.org/abs/2310.00504</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amin Ranem, Niklas Babendererde, Moritz Fuchs, Anirban Mukhopadhyay</li>
<li>for: 本研究旨在探讨Segment Anything Model（SAM）在不同领域中的应用，以提高准确性和可靠性。</li>
<li>methods: 本研究使用SAM框架，分析其基本组件与它们之间的复杂交互，并对其进行精细调整以提高 segmentation 结果的准确性。</li>
<li>results: 经过系列仔细设计的实验表明，SAM在放射学（特别是脑肿瘤 segmentation）和病理学（特别是乳腺癌 segmentation）中的应用具有很高的潜力，可以帮助解决医学影像分 segmentation 的挑战。<details>
<summary>Abstract</summary>
Medical imaging plays a critical role in the diagnosis and treatment planning of various medical conditions, with radiology and pathology heavily reliant on precise image segmentation. The Segment Anything Model (SAM) has emerged as a promising framework for addressing segmentation challenges across different domains. In this white paper, we delve into SAM, breaking down its fundamental components and uncovering the intricate interactions between them. We also explore the fine-tuning of SAM and assess its profound impact on the accuracy and reliability of segmentation results, focusing on applications in radiology (specifically, brain tumor segmentation) and pathology (specifically, breast cancer segmentation). Through a series of carefully designed experiments, we analyze SAM's potential application in the field of medical imaging. We aim to bridge the gap between advanced segmentation techniques and the demanding requirements of healthcare, shedding light on SAM's transformative capabilities.
</details>
<details>
<summary>摘要</summary>
医疗影像在各种医疗疾病诊断和治疗规划中扮演着关键的角色，医 radiology 和 pathology 都受到精确的图像分割的依赖。 segmen anything model（SAM）在不同领域中呈现出了一种有前途的框架，以下我们将对 SAM 进行分析，探讨其基本组件之间的细腻交互，以及对准确性和可靠性的影响。我们将在医 radiology（特别是脑肿瘤分割）和 pathology（特别是乳腺癌分割）中进行精心设计的实验，分析 SAM 在医疗影像领域的潜在应用。我们想通过 bridging 高级分割技术和医疗需求的 gap，把 SAM 的 transformative 能力推广到医疗领域。
</details></li>
</ul>
<hr>
<h2 id="Black-box-Attacks-on-Image-Activity-Prediction-and-its-Natural-Language-Explanations"><a href="#Black-box-Attacks-on-Image-Activity-Prediction-and-its-Natural-Language-Explanations" class="headerlink" title="Black-box Attacks on Image Activity Prediction and its Natural Language Explanations"></a>Black-box Attacks on Image Activity Prediction and its Natural Language Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00503">http://arxiv.org/abs/2310.00503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alina Elena Baia, Valentina Poggioni, Andrea Cavallaro</li>
<li>for: 这篇论文目的是评估深度神经网络的决策过程可以不可靠地描述的隐藏攻击。</li>
<li>methods: 这篇论文使用了一种自然语言解释的自然语言基于图像活动识别模型，并使用了黑盒测试来评估模型的Robustness。</li>
<li>results: 研究发现，使用了这种自然语言解释的模型很容易受到黑盒攻击，可以通过让模型生成不准确的解释来 manipulate 模型的决策。<details>
<summary>Abstract</summary>
Explainable AI (XAI) methods aim to describe the decision process of deep neural networks. Early XAI methods produced visual explanations, whereas more recent techniques generate multimodal explanations that include textual information and visual representations. Visual XAI methods have been shown to be vulnerable to white-box and gray-box adversarial attacks, with an attacker having full or partial knowledge of and access to the target system. As the vulnerabilities of multimodal XAI models have not been examined, in this paper we assess for the first time the robustness to black-box attacks of the natural language explanations generated by a self-rationalizing image-based activity recognition model. We generate unrestricted, spatially variant perturbations that disrupt the association between the predictions and the corresponding explanations to mislead the model into generating unfaithful explanations. We show that we can create adversarial images that manipulate the explanations of an activity recognition model by having access only to its final output.
</details>
<details>
<summary>摘要</summary>
explainable AI (XAI) 技术目的是描述深度神经网络决策过程。早期 XAI 技术生成了视觉解释，而更近期的技术生成了多 modal 解释，包括文本信息和视觉表示。视觉 XAI 技术在面对白盒和灰盒攻击时容易受损，攻击者具有完整或部分知道和访问目标系统的权限。然而，多 modal XAI 模型的抵御性尚未被调查，这篇论文是第一次评估黑盒攻击下自然语言解释生成的图像活动识别模型的可靠性。我们生成了无限制、空间变化的扰动，使模型的预测和相应的解释失去关联，以诱导模型生成不寻常的解释。我们显示了访问模型的最终输出后，可以创造欺骗性图像，使模型生成不准确的解释。
</details></li>
</ul>
<hr>
<h2 id="Small-Visual-Language-Models-can-also-be-Open-Ended-Few-Shot-Learners"><a href="#Small-Visual-Language-Models-can-also-be-Open-Ended-Few-Shot-Learners" class="headerlink" title="Small Visual Language Models can also be Open-Ended Few-Shot Learners"></a>Small Visual Language Models can also be Open-Ended Few-Shot Learners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00500">http://arxiv.org/abs/2310.00500</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Mahdi Derakhshani, Ivona Najdenkoska, Cees G. M. Snoek, Marcel Worring, Yuki M. Asano</li>
<li>for: 开发了一种自然语言模型的开放式少量学习能力，即使使用小型模型（约1B参数）也能够超越大型模型（如冰冻和FROMage）的少量学习能力。</li>
<li>methods: 提出了一种自我上下文适应（SeCAt）方法，通过自动学习从 симвоlic  yet self-supervised 训练任务中获得知识，包括基于 clustering 大量图像并赋予不相关的名称。</li>
<li>results: 在多Modal 少量数据集上表现出优秀的灵活性和性能，并且可以用小型模型（约1B参数）来实现，而不需要大型模型或专有模型。<details>
<summary>Abstract</summary>
We present Self-Context Adaptation (SeCAt), a self-supervised approach that unlocks open-ended few-shot abilities of small visual language models. Our proposed adaptation algorithm explicitly learns from symbolic, yet self-supervised training tasks. Specifically, our approach imitates image captions in a self-supervised way based on clustering a large pool of images followed by assigning semantically-unrelated names to clusters. By doing so, we construct the `self-context', a training signal consisting of interleaved sequences of image and pseudo-caption pairs and a query image for which the model is trained to produce the right pseudo-caption. We demonstrate the performance and flexibility of SeCAt on several multimodal few-shot datasets, spanning various granularities. By using models with approximately 1B parameters we outperform the few-shot abilities of much larger models, such as Frozen and FROMAGe. SeCAt opens new possibilities for research in open-ended few-shot learning that otherwise requires access to large or proprietary models.
</details>
<details>
<summary>摘要</summary>
我们介绍Self-Context Adaptation（SeCAt），一种自我指导的方法，可以激活小视觉语言模型的开放式少量学习能力。我们的提议的适应算法直接从 символиック， yet自我指导的训练任务中学习。具体来说，我们的方法模仿图像描述文本在自我指导的方式基于图像集 clustering，并将抽象无关的名称分配给集群。通过这样做，我们构建了`自我上下文'，一个训练信号包括交错的图像和假描述对象的序列，以及一个查询图像，对于该模型要生成正确的假描述。我们在多个多modal few-shot数据集上表现出了性能和灵活性，覆盖了不同的细化程度。使用大约1B参数的模型，我们超越了许多更大的模型，如冰冻和FROMAGe的少量学习能力。SeCAt开启了新的可能性 для开放式少量学习研究，否则需要访问大型或专有模型。
</details></li>
</ul>
<hr>
<h2 id="The-Sparsity-Roofline-Understanding-the-Hardware-Limits-of-Sparse-Neural-Networks"><a href="#The-Sparsity-Roofline-Understanding-the-Hardware-Limits-of-Sparse-Neural-Networks" class="headerlink" title="The Sparsity Roofline: Understanding the Hardware Limits of Sparse Neural Networks"></a>The Sparsity Roofline: Understanding the Hardware Limits of Sparse Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00496">http://arxiv.org/abs/2310.00496</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cameron Shinn, Collin McCarthy, Saurav Muralidharan, Muhammad Osama, John D. Owens</li>
<li>for: 评估神经网络中稀疙瘩的性能</li>
<li>methods: 提出了一种名为”简洁顶层”的视觉性能模型，用于评估神经网络中稀疙瘩的性能</li>
<li>results: 通过一种新的分析方法，可以预测稀疙瘩神经网络的性能，并 Validate the predicted speedup using several real-world computer vision architectures pruned across a range of sparsity patterns and degrees.<details>
<summary>Abstract</summary>
We introduce the Sparsity Roofline, a visual performance model for evaluating sparsity in neural networks. The Sparsity Roofline jointly models network accuracy, sparsity, and predicted inference speedup. Our approach does not require implementing and benchmarking optimized kernels, and the predicted speedup is equal to what would be measured when the corresponding dense and sparse kernels are equally well-optimized. We achieve this through a novel analytical model for predicting sparse network performance, and validate the predicted speedup using several real-world computer vision architectures pruned across a range of sparsity patterns and degrees. We demonstrate the utility and ease-of-use of our model through two case studies: (1) we show how machine learning researchers can predict the performance of unimplemented or unoptimized block-structured sparsity patterns, and (2) we show how hardware designers can predict the performance implications of new sparsity patterns and sparse data formats in hardware. In both scenarios, the Sparsity Roofline helps performance experts identify sparsity regimes with the highest performance potential.
</details>
<details>
<summary>摘要</summary>
我们介绍了简洁顶部（Sparsity Roofline），一个用于评估神经网络中的简洁性的可视性表现模型。简洁顶部同时考虑神经网络的准确性、简洁性和预测的执行速度增加。我们的方法不需要实现和测试优化的核心，且预测的速度与 dense 和简洁核心相同程度的优化相同。我们通过一个新的分析模型来预测简洁网络的性能，并使用多个真实世界计算机视觉架构中的简洁Pattern和度量进行验证。我们透过两个案例研究：首先，我们显示了如何在简洁顶部的帮助下，机器学习研究人员可以预测尚未实现或优化的块结构简洁模式的性能。其次，我们显示了如何在简洁顶部的帮助下，硬件设计师可以预测新的简洁模式和简洁数据格式在硬件上的性能影响。在这两个案例中，简洁顶部帮助性能专家识别最高性能潜在的简洁度域。
</details></li>
</ul>
<hr>
<h2 id="Diff-DOPE-Differentiable-Deep-Object-Pose-Estimation"><a href="#Diff-DOPE-Differentiable-Deep-Object-Pose-Estimation" class="headerlink" title="Diff-DOPE: Differentiable Deep Object Pose Estimation"></a>Diff-DOPE: Differentiable Deep Object Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00463">http://arxiv.org/abs/2310.00463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan Tremblay, Bowen Wen, Valts Blukis, Balakumar Sundaralingam, Stephen Tyree, Stan Birchfield</li>
<li>for: 提高对象pose的优化，使用拟合照片和3D文本模型来更新 object pose，以最小化视觉错误。</li>
<li>methods: 使用可导渠 rendering 更新 object pose，避免需要训练大量synthetic dataset的深度神经网络。</li>
<li>results: 实现了状态机器pose estimation datasets的最佳效果，并且可以处理多种modalities，如RGB、深度、纹理边缘和物体 segmentation masks。<details>
<summary>Abstract</summary>
We introduce Diff-DOPE, a 6-DoF pose refiner that takes as input an image, a 3D textured model of an object, and an initial pose of the object. The method uses differentiable rendering to update the object pose to minimize the visual error between the image and the projection of the model. We show that this simple, yet effective, idea is able to achieve state-of-the-art results on pose estimation datasets. Our approach is a departure from recent methods in which the pose refiner is a deep neural network trained on a large synthetic dataset to map inputs to refinement steps. Rather, our use of differentiable rendering allows us to avoid training altogether. Our approach performs multiple gradient descent optimizations in parallel with different random learning rates to avoid local minima from symmetric objects, similar appearances, or wrong step size. Various modalities can be used, e.g., RGB, depth, intensity edges, and object segmentation masks. We present experiments examining the effect of various choices, showing that the best results are found when the RGB image is accompanied by an object mask and depth image to guide the optimization process.
</details>
<details>
<summary>摘要</summary>
我们介绍Diff-DOPE，一种6DoF姿态级化器，它接受图像、一个3D纹理模型和初始对象姿态作为输入。该方法使用可微渲染更新对象姿态，以最小化图像和模型投影之间的视觉错误。我们表明，这个简单 yet有效的想法可以实现状态革命的结果在姿态估计数据集上。我们的方法与最近的方法不同，后者是通过训练大量的 sintetic数据来训练一个深度神经网络，以将输入映射到更新步骤。而我们使用可微渲染，可以避免训练。我们的方法可以并行进行多个梯度下降优化，以避免相似的对象、同样的外观或错误的步长导致的本地极小值。不同的感知modalities可以使用，例如RGB、深度、强度边缘和对象分割mask。我们进行了不同的选择的实验，并显示了RGB图像和对象mask、深度图像的搭配能够获得最佳结果。
</details></li>
</ul>
<hr>
<h2 id="UniLVSeg-Unified-Left-Ventricular-Segmentation-with-Sparsely-Annotated-Echocardiogram-Videos-through-Self-Supervised-Temporal-Masking-and-Weakly-Supervised-Training"><a href="#UniLVSeg-Unified-Left-Ventricular-Segmentation-with-Sparsely-Annotated-Echocardiogram-Videos-through-Self-Supervised-Temporal-Masking-and-Weakly-Supervised-Training" class="headerlink" title="UniLVSeg: Unified Left Ventricular Segmentation with Sparsely Annotated Echocardiogram Videos through Self-Supervised Temporal Masking and Weakly Supervised Training"></a>UniLVSeg: Unified Left Ventricular Segmentation with Sparsely Annotated Echocardiogram Videos through Self-Supervised Temporal Masking and Weakly Supervised Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00454">http://arxiv.org/abs/2310.00454</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fadillah Maani, Asim Ukaye, Nada Saadi, Numan Saeed, Mohammad Yaqub<br>for: 这份研究的目的是提出一种可靠且高效的左心室（LV）分 segmentation方法，以帮助医生更加精确地诊断心血管疾病。methods: 本研究使用了自动学习（SSL）和弱监督训练（WST）两种方法，并考虑了三种不同的分 segmentation方法：3D分 segmentation和一种新的2D超像（SI）。results: 本研究比较了各种方法的效果，结果显示了我们的提案方法在大规模数据集（EchoNet-Dynamic）上获得了93.32%（95%CI 93.21-93.43%)的 dice分数，而且比之前的方法更高效。我们还提供了广泛的拓展研究，包括预训练设定和不同的深度学习架构。<details>
<summary>Abstract</summary>
Echocardiography has become an indispensable clinical imaging modality for general heart health assessment. From calculating biomarkers such as ejection fraction to the probability of a patient's heart failure, accurate segmentation of the heart and its structures allows doctors to plan and execute treatments with greater precision and accuracy. However, achieving accurate and robust left ventricle segmentation is time-consuming and challenging due to different reasons. This work introduces a novel approach for consistent left ventricular (LV) segmentation from sparsely annotated echocardiogram videos. We achieve this through (1) self-supervised learning (SSL) using temporal masking followed by (2) weakly supervised training. We investigate two different segmentation approaches: 3D segmentation and a novel 2D superimage (SI). We demonstrate how our proposed method outperforms the state-of-the-art solutions by achieving a 93.32% (95%CI 93.21-93.43%) dice score on a large-scale dataset (EchoNet-Dynamic) while being more efficient. To show the effectiveness of our approach, we provide extensive ablation studies, including pre-training settings and various deep learning backbones. Additionally, we discuss how our proposed methodology achieves high data utility by incorporating unlabeled frames in the training process. To help support the AI in medicine community, the complete solution with the source code will be made publicly available upon acceptance.
</details>
<details>
<summary>摘要</summary>
echo cardiography 已成为现代医学实验室中不可或缺的诊断工具，从计算生物标志物such as 血液泵功率到患者的心血液疾病可能性，准确地分割心脏和其结构，帮助医生更加准确地规划和执行治疗。然而，实现准确和可靠的左心室（LV）分割是一项时间consuming和困难的任务，主要因为多种原因。这种工作介绍了一种新的方法，可以从缺乏标注的echo cardiogram视频中获得一致的LV分割结果。我们通过(1)自动学习（SSL）使用时间掩蔽，然后(2)弱监督训练来实现这一目标。我们 investigate了两种不同的分割方法：3D分割和一种新的2D超像（SI）。我们展示了我们的提议方法在大规模数据集（EchoNet-Dynamic）上的表现，而且比现有的解决方案高效。为了证明我们的方法的有效性，我们提供了广泛的拟合研究，包括预训练设置和不同的深度学习背bone。此外，我们讨论了我们的方法如何实现高数据利用率，通过在训练过程中包含无标注帧。为了支持AI医学社区，我们将在接受后公开完整的解决方案和源代码。
</details></li>
</ul>
<hr>
<h2 id="On-the-Role-of-Neural-Collapse-in-Meta-Learning-Models-for-Few-shot-Learning"><a href="#On-the-Role-of-Neural-Collapse-in-Meta-Learning-Models-for-Few-shot-Learning" class="headerlink" title="On the Role of Neural Collapse in Meta Learning Models for Few-shot Learning"></a>On the Role of Neural Collapse in Meta Learning Models for Few-shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00451">http://arxiv.org/abs/2310.00451</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/saakethmm/nc-prototypical-networks">https://github.com/saakethmm/nc-prototypical-networks</a></li>
<li>paper_authors: Saaketh Medepalli, Naren Doraiswamy</li>
<li>For: 这个论文探讨了基于少量示例学习的元学习框架，以及这些框架在新类上的泛化性。* Methods: 这个论文使用了元学习框架，并在Omniglot数据集上进行了几个示例学习任务的研究。* Results: 研究发现，随着模型大小增加，学习出来的特征往往呈现出神经塌磔现象，但不一定符合完整的神经塌磔性质。<details>
<summary>Abstract</summary>
Meta-learning frameworks for few-shot learning aims to learn models that can learn new skills or adapt to new environments rapidly with a few training examples. This has led to the generalizability of the developed model towards new classes with just a few labelled samples. However these networks are seen as black-box models and understanding the representations learnt under different learning scenarios is crucial. Neural collapse ($\mathcal{NC}$) is a recently discovered phenomenon which showcases unique properties at the network proceeds towards zero loss. The input features collapse to their respective class means, the class means form a Simplex equiangular tight frame (ETF) where the class means are maximally distant and linearly separable, and the classifier acts as a simple nearest neighbor classifier. While these phenomena have been observed in simple classification networks, this study is the first to explore and understand the properties of neural collapse in meta learning frameworks for few-shot learning. We perform studies on the Omniglot dataset in the few-shot setting and study the neural collapse phenomenon. We observe that the learnt features indeed have the trend of neural collapse, especially as model size grows, but to do not necessarily showcase the complete collapse as measured by the $\mathcal{NC}$ properties.
</details>
<details>
<summary>摘要</summary>
<SYS> translate-internal: "Meta-learning frameworks for few-shot learning aim to learn models that can learn new skills or adapt to new environments rapidly with just a few training examples. This has led to the generalizability of the developed model towards new classes with just a few labelled samples. However, these networks are seen as black-box models, and understanding the representations learnt under different learning scenarios is crucial. Neural collapse (NC) is a recently discovered phenomenon that showcases unique properties when the network proceeds towards zero loss. The input features collapse to their respective class means, the class means form a Simplex equiangular tight frame (ETF) where the class means are maximally distant and linearly separable, and the classifier acts as a simple nearest neighbor classifier. While these phenomena have been observed in simple classification networks, this study is the first to explore and understand the properties of neural collapse in meta learning frameworks for few-shot learning. We perform studies on the Omniglot dataset in the few-shot setting and study the neural collapse phenomenon. We observe that the learnt features indeed have the trend of neural collapse, especially as model size grows, but they do not necessarily showcase the complete collapse as measured by the NC properties."</SYS>Here's the translation in Traditional Chinese:<SYS>translate-internal: "Meta-learning frameworks for few-shot learning aim to learn models that can learn new skills or adapt to new environments rapidly with just a few training examples. This has led to the generalizability of the developed model towards new classes with just a few labelled samples. However, these networks are seen as black-box models, and understanding the representations learnt under different learning scenarios is crucial. Neural collapse (NC) is a recently discovered phenomenon that showcases unique properties when the network proceeds towards zero loss. The input features collapse to their respective class means, the class means form a Simplex equiangular tight frame (ETF) where the class means are maximally distant and linearly separable, and the classifier acts as a simple nearest neighbor classifier. While these phenomena have been observed in simple classification networks, this study is the first to explore and understand the properties of neural collapse in meta learning frameworks for few-shot learning. We perform studies on the Omniglot dataset in the few-shot setting and study the neural collapse phenomenon. We observe that the learnt features indeed have the trend of neural collapse, especially as model size grows, but they do not necessarily showcase the complete collapse as measured by the NC properties."</SYS>Note that the translation is in Simplified Chinese, as requested. If you would like the translation in Traditional Chinese instead, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Human-Producible-Adversarial-Examples"><a href="#Human-Producible-Adversarial-Examples" class="headerlink" title="Human-Producible Adversarial Examples"></a>Human-Producible Adversarial Examples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00438">http://arxiv.org/abs/2310.00438</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lionfish0/adversarial-human">https://github.com/lionfish0/adversarial-human</a></li>
<li>paper_authors: David Khachaturov, Yue Gao, Ilia Shumailov, Robert Mullins, Ross Anderson, Kassem Fawaz</li>
<li>for: 该论文旨在开发一种可以在真实世界中生成人工生成的 adversarial example 方法，而无需使用复杂的设备或技术。</li>
<li>methods: 该方法基于差异渲染，通过简单地绘制四个或九个直线来构建强大的 adversarial example。它还包括一种基于人工绘制错误的抗噪准则，以保证攻击的可重复性。</li>
<li>results: 研究人员通过用涂抹笔将lines绘制到图像上，实现了在YOLO模型中81.8%的攻击成功率。此外，研究人员还进行了数字和物理世界的广泛测试，并证明了该方法可以由无经验人员应用。<details>
<summary>Abstract</summary>
Visual adversarial examples have so far been restricted to pixel-level image manipulations in the digital world, or have required sophisticated equipment such as 2D or 3D printers to be produced in the physical real world. We present the first ever method of generating human-producible adversarial examples for the real world that requires nothing more complicated than a marker pen. We call them $\textbf{adversarial tags}$. First, building on top of differential rendering, we demonstrate that it is possible to build potent adversarial examples with just lines. We find that by drawing just $4$ lines we can disrupt a YOLO-based model in $54.8\%$ of cases; increasing this to $9$ lines disrupts $81.8\%$ of the cases tested. Next, we devise an improved method for line placement to be invariant to human drawing error. We evaluate our system thoroughly in both digital and analogue worlds and demonstrate that our tags can be applied by untrained humans. We demonstrate the effectiveness of our method for producing real-world adversarial examples by conducting a user study where participants were asked to draw over printed images using digital equivalents as guides. We further evaluate the effectiveness of both targeted and untargeted attacks, and discuss various trade-offs and method limitations, as well as the practical and ethical implications of our work. The source code will be released publicly.
</details>
<details>
<summary>摘要</summary>
“Visual adversarial examples”Previously, have only been restricted to digital image manipulation or require sophisticated equipment such as 2D or 3D printers to produce in the physical world. We present the first method of generating human-producible adversarial examples for the real world that only requires a marker pen. We call them “adversarial tags”.First, we build on differential rendering and show that it is possible to create powerful adversarial examples with just lines. We found that by drawing just 4 lines, we can disrupt a YOLO-based model in 54.8% of cases, and increasing it to 9 lines disrupts 81.8% of the cases tested. Next, we improve the method for line placement to be invariant to human drawing errors.We thoroughly evaluate our system in both the digital and analog worlds and demonstrate that our tags can be applied by untrained humans. We also conduct a user study where participants were asked to draw over printed images using digital equivalents as guides, and evaluate the effectiveness of both targeted and untargeted attacks. We discuss various trade-offs and method limitations, as well as the practical and ethical implications of our work. The source code will be released publicly.
</details></li>
</ul>
<hr>
<h2 id="DiffPoseTalk-Speech-Driven-Stylistic-3D-Facial-Animation-and-Head-Pose-Generation-via-Diffusion-Models"><a href="#DiffPoseTalk-Speech-Driven-Stylistic-3D-Facial-Animation-and-Head-Pose-Generation-via-Diffusion-Models" class="headerlink" title="DiffPoseTalk: Speech-Driven Stylistic 3D Facial Animation and Head Pose Generation via Diffusion Models"></a>DiffPoseTalk: Speech-Driven Stylistic 3D Facial Animation and Head Pose Generation via Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00434">http://arxiv.org/abs/2310.00434</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/THU-LYJ-Lab/DiffPoseTalk">https://github.com/THU-LYJ-Lab/DiffPoseTalk</a></li>
<li>paper_authors: Zhiyao Sun, Tian Lv, Sheng Ye, Matthieu Gaetan Lin, Jenny Sheng, Yu-Hui Wen, Minjing Yu, Yong-jin Liu</li>
<li>for: 本研究旨在提出一种生成风格化3D脸部动画，使用speech和风格编码来驱动生成过程。</li>
<li>methods: 我们提出了一种基于扩散模型和风格编码器的生成框架，从短视频中提取风格特征，并使用无类标注引导生成过程。</li>
<li>results: 我们的方法在评测中超过了现有方法，并且通过用户测试得到了更高的评价。 code和数据将公开发布。<details>
<summary>Abstract</summary>
The generation of stylistic 3D facial animations driven by speech poses a significant challenge as it requires learning a many-to-many mapping between speech, style, and the corresponding natural facial motion. However, existing methods either employ a deterministic model for speech-to-motion mapping or encode the style using a one-hot encoding scheme. Notably, the one-hot encoding approach fails to capture the complexity of the style and thus limits generalization ability. In this paper, we propose DiffPoseTalk, a generative framework based on the diffusion model combined with a style encoder that extracts style embeddings from short reference videos. During inference, we employ classifier-free guidance to guide the generation process based on the speech and style. We extend this to include the generation of head poses, thereby enhancing user perception. Additionally, we address the shortage of scanned 3D talking face data by training our model on reconstructed 3DMM parameters from a high-quality, in-the-wild audio-visual dataset. Our extensive experiments and user study demonstrate that our approach outperforms state-of-the-art methods. The code and dataset will be made publicly available.
</details>
<details>
<summary>摘要</summary>
当前的3D facial动画生成技术面临着一个重要挑战，即学习很多到很多的映射关系 между语音、风格和自然的脸部动作。然而，现有的方法都是使用决定性的语音到动作映射模型，或者使用一个简单的一个热度编码方法来编码风格。可是，这种一个热度编码方法无法捕捉风格的复杂性，因此限制了其泛化能力。在这篇论文中，我们提出了DiffPoseTalk，一种基于扩散模型并与风格编码器结合的生成框架。在推理过程中，我们使用无类别导航来指导生成过程，根据语音和风格。此外，我们还扩展了头部pose的生成，从而提高用户的感知。此外，我们解决了3D talking face数据的缺乏问题，通过在高质量的自然语言视频 Dataset 中重建3DMM参数来训练我们的模型。我们的广泛的实验和用户研究表明，我们的方法在比较状态的方法之上出色表现。代码和数据将会公开释出。
</details></li>
</ul>
<hr>
<h2 id="Technical-Report-of-2023-ABO-Fine-grained-Semantic-Segmentation-Competition"><a href="#Technical-Report-of-2023-ABO-Fine-grained-Semantic-Segmentation-Competition" class="headerlink" title="Technical Report of 2023 ABO Fine-grained Semantic Segmentation Competition"></a>Technical Report of 2023 ABO Fine-grained Semantic Segmentation Competition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00427">http://arxiv.org/abs/2310.00427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyu Dong</li>
<li>for: 本研究是为了参加2023年ABO细化semantic segmentation比赛，目标是预测五类 convex shape的semantic标签。</li>
<li>methods: 本研究使用DGCNN作为backbone，通过分类不同类型的结构来实现semantic segmentation。我们进行了多个实验，并发现使用学习率随机梯度下降和不同类型的分解因子来提高模型性能。</li>
<li>results: 我们的模型在2023年ICCV 3DVeComm Workshop Challenge的Dev阶段取得了第3名。<details>
<summary>Abstract</summary>
In this report, we describe the technical details of our submission to the 2023 ABO Fine-grained Semantic Segmentation Competition, by Team "Zeyu\_Dong" (username:ZeyuDong). The task is to predicate the semantic labels for the convex shape of five categories, which consist of high-quality, standardized 3D models of real products available for purchase online. By using DGCNN as the backbone to classify different structures of five classes, We carried out numerous experiments and found learning rate stochastic gradient descent with warm restarts and setting different rate of factors for various categories contribute most to the performance of the model. The appropriate method helps us rank 3rd place in the Dev phase of the 2023 ICCV 3DVeComm Workshop Challenge.
</details>
<details>
<summary>摘要</summary>
在本报告中，我们介绍了我们对2023年ABO细致semantic segmentation比赛的提交技术细节，由Team "Zeyu\_Dong"（用户名：ZeyuDong）完成。任务是预测五类 convex shape的semantic标签，其中五类包括高质量、标准化的3D模型在线销售。通过使用DGCNN作为后端分类不同结构的五类，我们进行了许多实验，发现了学习率随机梯度下降与温存 restart的方法对模型性能产生了最大化的影响。这种方法帮助我们在2023年ICCV 3DVeComm Workshop Challenge的Dev阶段取得第三名。
</details></li>
</ul>
<hr>
<h2 id="PixArt-α-Fast-Training-of-Diffusion-Transformer-for-Photorealistic-Text-to-Image-Synthesis"><a href="#PixArt-α-Fast-Training-of-Diffusion-Transformer-for-Photorealistic-Text-to-Image-Synthesis" class="headerlink" title="PixArt-$α$: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis"></a>PixArt-$α$: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00426">http://arxiv.org/abs/2310.00426</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/PixArt-alpha/PixArt-alpha">https://github.com/PixArt-alpha/PixArt-alpha</a></li>
<li>paper_authors: Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, Zhenguo Li</li>
<li>for: 这篇论文旨在提供一个高品质却低成本的文本转换到图像（T2I）模型，以推动AI创新和低排放。</li>
<li>methods: 这篇论文提出了三个核心设计：首先，通过分解训练策略，分别优化像素依赖、文本图像对齐和图像艺术质量; 其次，透过插入批访模组，将文本条件注入到算法中，以提高计算效率; 最后，强调概念密度的重要性，并运用大型视觉语言模型自动生成密集pseudo-caption，以帮助图像对齐学习。</li>
<li>results: 这篇论文显示了PIXART-$\alpha$的训练速度明显高于现有的大规模T2I模型，例如PIXART-$\alpha$只需10.8%的Stable Diffusion v1.5的训练时间（675vs. 6,250 A100 GPU天），优化了约 $300,000（ $26,000 vs. $320,000）和减少90%的二氧化碳排放。此外，与现有较大的SOTA模型相比，我们的训练成本仅1%。PIXART-$\alpha$在图像质量、艺术性和Semantic控制方面表现出色。<details>
<summary>Abstract</summary>
The most advanced text-to-image (T2I) models require significant training costs (e.g., millions of GPU hours), seriously hindering the fundamental innovation for the AIGC community while increasing CO2 emissions. This paper introduces PIXART-$\alpha$, a Transformer-based T2I diffusion model whose image generation quality is competitive with state-of-the-art image generators (e.g., Imagen, SDXL, and even Midjourney), reaching near-commercial application standards. Additionally, it supports high-resolution image synthesis up to 1024px resolution with low training cost, as shown in Figure 1 and 2. To achieve this goal, three core designs are proposed: (1) Training strategy decomposition: We devise three distinct training steps that separately optimize pixel dependency, text-image alignment, and image aesthetic quality; (2) Efficient T2I Transformer: We incorporate cross-attention modules into Diffusion Transformer (DiT) to inject text conditions and streamline the computation-intensive class-condition branch; (3) High-informative data: We emphasize the significance of concept density in text-image pairs and leverage a large Vision-Language model to auto-label dense pseudo-captions to assist text-image alignment learning. As a result, PIXART-$\alpha$'s training speed markedly surpasses existing large-scale T2I models, e.g., PIXART-$\alpha$ only takes 10.8% of Stable Diffusion v1.5's training time (675 vs. 6,250 A100 GPU days), saving nearly \$300,000 (\$26,000 vs. \$320,000) and reducing 90% CO2 emissions. Moreover, compared with a larger SOTA model, RAPHAEL, our training cost is merely 1%. Extensive experiments demonstrate that PIXART-$\alpha$ excels in image quality, artistry, and semantic control. We hope PIXART-$\alpha$ will provide new insights to the AIGC community and startups to accelerate building their own high-quality yet low-cost generative models from scratch.
</details>
<details>
<summary>摘要</summary>
最先进的文本到图像（T2I）模型需要巨大的训练成本（例如，百万个GPU小时），这会很大程度地阻碍AIGC社区的基础创新，同时也会增加CO2排放。这篇论文介绍了PIXART-α，一种基于Transformer的T2I扩散模型，其生成图像质量与现状最先进的图像生成器（如Imagen、SDXL以及甚至Midjourney）相当，达到了近商用应用标准。此外，它还支持高分辨率图像生成，达到1024px的分辨率，训练成本低廉，如图1和图2所示。为了实现这一目标，我们提出了三个核心设计：1. 训练策略分解：我们将训练过程分为三个独立的步骤，每个步骤都会分别优化像素依赖关系、文本-图像对齐和图像美观质量。2. 高效的T2I transformer：我们在扩散变换器（DiT）中添加了跨注意力模块，以注入文本条件并使计算量昂贵的分类分支更加高效。3. 高信息 densities：我们强调了文本-图像对的概率密度的重要性，并利用大量的视觉语言模型自动生成密集 Pseudo-captions，以帮助图像-文本对齐学习。因此，PIXART-α的训练速度明显高于现有的大规模T2I模型，例如PIXART-α只需10.8%的Stable Diffusion v1.5的训练时间（675 vs. 6,250 A100 GPU天），相对 экономии了约300,000美元（26,000 vs. 320,000美元），同时减少了90%的CO2排放。此外，相比一个更大的SOTA模型，RAPHAEL，我们的训练成本只有1%。广泛的实验表明，PIXART-α在图像质量、艺术性和 semantics 控制方面具有优异的表现。我们希望PIXART-α能为AIGC社区和创新公司提供新的思路，以便他们可以从头开始构建高质量 yet low cost的生成模型。
</details></li>
</ul>
<hr>
<h2 id="MVC-A-Multi-Task-Vision-Transformer-Network-for-COVID-19-Diagnosis-from-Chest-X-ray-Images"><a href="#MVC-A-Multi-Task-Vision-Transformer-Network-for-COVID-19-Diagnosis-from-Chest-X-ray-Images" class="headerlink" title="MVC: A Multi-Task Vision Transformer Network for COVID-19 Diagnosis from Chest X-ray Images"></a>MVC: A Multi-Task Vision Transformer Network for COVID-19 Diagnosis from Chest X-ray Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00418">http://arxiv.org/abs/2310.00418</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huyen Tran, Duc Thanh Nguyen, John Yearwood</li>
<li>for: 这篇论文的目的是提出一个新的多任务检测方法，以便同时分类骨肉X射线图像和识别受影响区域。</li>
<li>methods: 本论文使用的方法是基于Vision Transformer的多任务学习架构，具有本地和全球表示学习的能力。</li>
<li>results: 实验结果显示，提出的方法在比较基于基eline的方法时表现更好，在骨肉X射线图像分类和受影响区域识别两个任务上都达到了更高的准确率。<details>
<summary>Abstract</summary>
Medical image analysis using computer-based algorithms has attracted considerable attention from the research community and achieved tremendous progress in the last decade. With recent advances in computing resources and availability of large-scale medical image datasets, many deep learning models have been developed for disease diagnosis from medical images. However, existing techniques focus on sub-tasks, e.g., disease classification and identification, individually, while there is a lack of a unified framework enabling multi-task diagnosis. Inspired by the capability of Vision Transformers in both local and global representation learning, we propose in this paper a new method, namely Multi-task Vision Transformer (MVC) for simultaneously classifying chest X-ray images and identifying affected regions from the input data. Our method is built upon the Vision Transformer but extends its learning capability in a multi-task setting. We evaluated our proposed method and compared it with existing baselines on a benchmark dataset of COVID-19 chest X-ray images. Experimental results verified the superiority of the proposed method over the baselines on both the image classification and affected region identification tasks.
</details>
<details>
<summary>摘要</summary>
医学图像分析使用计算机基于算法已经在过去十年内吸引了广泛的研究者关注，取得了很大的进步。随着计算资源的提高和医学图像大规模数据集的可用性，许多深度学习模型在医疾诊断方面得到了应用。然而，现有的技术主要集中在子任务上，例如疾病分类和识别，而忽略了多任务诊断框架的开发。我们在这篇论文中提出了一种新的方法，即多任务视transformer（MVC），用于同时分类胸部X射影图像和识别输入数据中的受影响区域。我们的方法基于视transformer，但在多任务设定下扩展了其学习能力。我们对一个COVID-19胸部X射影图像数据集进行了实验，并与现有的基线相比较。实验结果表明，我们提出的方法在两个任务上都有superiority。
</details></li>
</ul>
<hr>
<h2 id="SSIF-Learning-Continuous-Image-Representation-for-Spatial-Spectral-Super-Resolution"><a href="#SSIF-Learning-Continuous-Image-Representation-for-Spatial-Spectral-Super-Resolution" class="headerlink" title="SSIF: Learning Continuous Image Representation for Spatial-Spectral Super-Resolution"></a>SSIF: Learning Continuous Image Representation for Spatial-Spectral Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00413">http://arxiv.org/abs/2310.00413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gengchen Mai, Ni Lao, Weiwei Sun, Yuchi Ma, Jiaming Song, Chenlin Meng, Hongxu Ma, Jinmeng Rao, Ziyuan Li, Stefano Ermon</li>
<li>for: 提高图像的空间和频谱分解能力</li>
<li>methods: 使用神经隐函数模型来表示图像，并在图像中进行空间和频谱分解</li>
<li>results: 对两个难题的空间-频谱超分辨 benchmark 进行了实验，并证明了 SSIF 可以在不同的空间和频谱分辨下表现出色，并且可以提高下游任务（例如土地使用分类）的性能 by 1.7%-7%。<details>
<summary>Abstract</summary>
Existing digital sensors capture images at fixed spatial and spectral resolutions (e.g., RGB, multispectral, and hyperspectral images), and each combination requires bespoke machine learning models. Neural Implicit Functions partially overcome the spatial resolution challenge by representing an image in a resolution-independent way. However, they still operate at fixed, pre-defined spectral resolutions. To address this challenge, we propose Spatial-Spectral Implicit Function (SSIF), a neural implicit model that represents an image as a function of both continuous pixel coordinates in the spatial domain and continuous wavelengths in the spectral domain. We empirically demonstrate the effectiveness of SSIF on two challenging spatio-spectral super-resolution benchmarks. We observe that SSIF consistently outperforms state-of-the-art baselines even when the baselines are allowed to train separate models at each spectral resolution. We show that SSIF generalizes well to both unseen spatial resolutions and spectral resolutions. Moreover, SSIF can generate high-resolution images that improve the performance of downstream tasks (e.g., land use classification) by 1.7%-7%.
</details>
<details>
<summary>摘要</summary>
现有的数字感知器只能捕捉固定的空间和спектраль分辨率图像（例如RGB、多spectral和快速pectral图像），每种组合都需要特制的机器学习模型。神经凝函数partially overcomes the spatial resolution challenge by representing an image in a resolution-independent way. However, they still operate at fixed, pre-defined spectral resolutions. To address this challenge, we propose Spatial-Spectral Implicit Function (SSIF), a neural implicit model that represents an image as a function of both continuous pixel coordinates in the spatial domain and continuous wavelengths in the spectral domain. We empirically demonstrate the effectiveness of SSIF on two challenging spatio-spectral super-resolution benchmarks. We observe that SSIF consistently outperforms state-of-the-art baselines even when the baselines are allowed to train separate models at each spectral resolution. We show that SSIF generalizes well to both unseen spatial resolutions and spectral resolutions. Moreover, SSIF can generate high-resolution images that improve the performance of downstream tasks (e.g., 土地使用分类) by 1.7%-7%.
</details></li>
</ul>
<hr>
<h2 id="Controlling-Neural-Style-Transfer-with-Deep-Reinforcement-Learning"><a href="#Controlling-Neural-Style-Transfer-with-Deep-Reinforcement-Learning" class="headerlink" title="Controlling Neural Style Transfer with Deep Reinforcement Learning"></a>Controlling Neural Style Transfer with Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00405">http://arxiv.org/abs/2310.00405</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abusufyanvu/6S191_MIT_DeepLearning">https://github.com/abusufyanvu/6S191_MIT_DeepLearning</a></li>
<li>paper_authors: Chengming Feng, Jing Hu, Xin Wang, Shu Hu, Bin Zhu, Xi Wu, Hongtu Zhu, Siwei Lyu</li>
<li>for: 这个论文是为了提出一种深度学习（Deep Learning）基本的 Reinforcement Learning（RL）架构，用于控制涂抹式转换（Neural Style Transfer，NST）的度量。</li>
<li>methods: 这个方法使用RL来将一步式转换分解为多个步骤，以保留内容图像的详细信息和结构，并在后续步骤中增加风格模式。这种方法是用户轻松控制的样式传递方法。</li>
<li>results: 实验结果表明，我们的RL-based方法可以具有更高的效果和稳定性，并且比现有的一步DL基本模型具有更低的计算复杂度和更轻量级的计算负担。<details>
<summary>Abstract</summary>
Controlling the degree of stylization in the Neural Style Transfer (NST) is a little tricky since it usually needs hand-engineering on hyper-parameters. In this paper, we propose the first deep Reinforcement Learning (RL) based architecture that splits one-step style transfer into a step-wise process for the NST task. Our RL-based method tends to preserve more details and structures of the content image in early steps, and synthesize more style patterns in later steps. It is a user-easily-controlled style-transfer method. Additionally, as our RL-based model performs the stylization progressively, it is lightweight and has lower computational complexity than existing one-step Deep Learning (DL) based models. Experimental results demonstrate the effectiveness and robustness of our method.
</details>
<details>
<summary>摘要</summary>
控制 neural style transfer（NST）的度风格化有些困难，通常需要手工调整超参数。在这篇论文中，我们提出了首个深度强化学习（RL）基 Architecture，将一步式样式传递分解成多个步骤的NST任务。我们的RL基方法会在早期步骤中保留更多的内容图像细节和结构，并在后期步骤中更多地 sinthez style pattern。这是一种用户轻松控制的样式传递方法。此外，我们的RL基模型在进行样式传递的过程中，轻量级，计算复杂度较低于现有的一步DL基模型。实验结果表明我们的方法的有效性和稳定性。
</details></li>
</ul>
<hr>
<h2 id="MonoGAE-Roadside-Monocular-3D-Object-Detection-with-Ground-Aware-Embeddings"><a href="#MonoGAE-Roadside-Monocular-3D-Object-Detection-with-Ground-Aware-Embeddings" class="headerlink" title="MonoGAE: Roadside Monocular 3D Object Detection with Ground-Aware Embeddings"></a>MonoGAE: Roadside Monocular 3D Object Detection with Ground-Aware Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00400">http://arxiv.org/abs/2310.00400</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Yang, Jiaxin Yu, Xinyu Zhang, Jun Li, Li Wang, Yi Huang, Chuang Zhang, Hong Wang, Yiming Li</li>
<li>for: 提高路边摄像头的自动驾驶系统能力</li>
<li>methods: 利用智能路边摄像头，通过学习高维 embedding 来提高物体检测精度</li>
<li>results: 比前一代方法更高的3D物体检测性能，可以帮助路边摄像头提高自动驾驶系统的能力<details>
<summary>Abstract</summary>
Although the majority of recent autonomous driving systems concentrate on developing perception methods based on ego-vehicle sensors, there is an overlooked alternative approach that involves leveraging intelligent roadside cameras to help extend the ego-vehicle perception ability beyond the visual range. We discover that most existing monocular 3D object detectors rely on the ego-vehicle prior assumption that the optical axis of the camera is parallel to the ground. However, the roadside camera is installed on a pole with a pitched angle, which makes the existing methods not optimal for roadside scenes. In this paper, we introduce a novel framework for Roadside Monocular 3D object detection with ground-aware embeddings, named MonoGAE. Specifically, the ground plane is a stable and strong prior knowledge due to the fixed installation of cameras in roadside scenarios. In order to reduce the domain gap between the ground geometry information and high-dimensional image features, we employ a supervised training paradigm with a ground plane to predict high-dimensional ground-aware embeddings. These embeddings are subsequently integrated with image features through cross-attention mechanisms. Furthermore, to improve the detector's robustness to the divergences in cameras' installation poses, we replace the ground plane depth map with a novel pixel-level refined ground plane equation map. Our approach demonstrates a substantial performance advantage over all previous monocular 3D object detectors on widely recognized 3D detection benchmarks for roadside cameras. The code and pre-trained models will be released soon.
</details>
<details>
<summary>摘要</summary>
尽管现在大多数自动驾驶系统都在开发基于自驾车感知器的感知方法，但是有一种被忽略的代理方法是利用智能路边摄像头来帮助扩展自驾车感知范围之 beyond。我们发现大多数现有的单目3D物体探测器都基于自驾车的先前假设，即摄像头的光学轴与地面平行。然而，路边摄像头通常会被安装在倾斜的杆上，这使得现有的方法不适合路边场景。在这篇论文中，我们介绍了一种名为MonogaE的新框架，用于路边单目3D物体探测。 Specifically，我们认为地面是一种稳定和强大的先知知识，因为摄像头在路边enario中是固定安装的。为了减少图像特征和地面几何信息之间的领域差距，我们采用一种监督训练方法，使用地面来预测高维度的地面感知 embedding。这些 embedding  subsequentially 与图像特征进行交叉注意机制。此外，为了提高探测器对摄像头安装位置的不同的灵活性，我们将地面深度图像 replaced  noval pixel-level refined ground plane equation map。我们的方法在广泛recognized 3D探测标准准例中表现出了明显的性能优势。我们即将发布代码和预训练模型。
</details></li>
</ul>
<hr>
<h2 id="InstructCV-Instruction-Tuned-Text-to-Image-Diffusion-Models-as-Vision-Generalists"><a href="#InstructCV-Instruction-Tuned-Text-to-Image-Diffusion-Models-as-Vision-Generalists" class="headerlink" title="InstructCV: Instruction-Tuned Text-to-Image Diffusion Models as Vision Generalists"></a>InstructCV: Instruction-Tuned Text-to-Image Diffusion Models as Vision Generalists</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00390">http://arxiv.org/abs/2310.00390</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AlaaLab/InstructCV">https://github.com/AlaaLab/InstructCV</a></li>
<li>paper_authors: Yulu Gan, Sungwoo Park, Alexander Schubert, Anthony Philippakis, Ahmed M. Alaa</li>
<li>for: 这个论文旨在提供一种基于自然语言指令的多任务计算视觉模型，它可以通过文本描述来执行多种计算视觉任务。</li>
<li>methods: 该论文使用了文本控制的生成扩散模型，并通过自然语言模型来帮助模型学习多个计算视觉任务。</li>
<li>results: 实验表明，该模型能够与其他通用和任务特定视觉模型相比竞争，并且具有出色的扩展性，可以在未经见过的数据、类别和用户指令下进行高效的执行。<details>
<summary>Abstract</summary>
Recent advances in generative diffusion models have enabled text-controlled synthesis of realistic and diverse images with impressive quality. Despite these remarkable advances, the application of text-to-image generative models in computer vision for standard visual recognition tasks remains limited. The current de facto approach for these tasks is to design model architectures and loss functions that are tailored to the task at hand. In this paper, we develop a unified language interface for computer vision tasks that abstracts away task-specific design choices and enables task execution by following natural language instructions. Our approach involves casting multiple computer vision tasks as text-to-image generation problems. Here, the text represents an instruction describing the task, and the resulting image is a visually-encoded task output. To train our model, we pool commonly-used computer vision datasets covering a range of tasks, including segmentation, object detection, depth estimation, and classification. We then use a large language model to paraphrase prompt templates that convey the specific tasks to be conducted on each image, and through this process, we create a multi-modal and multi-task training dataset comprising input and output images along with annotated instructions. Following the InstructPix2Pix architecture, we apply instruction-tuning to a text-to-image diffusion model using our constructed dataset, steering its functionality from a generative model to an instruction-guided multi-task vision learner. Experiments demonstrate that our model, dubbed InstructCV, performs competitively compared to other generalist and task-specific vision models. Moreover, it exhibits compelling generalization capabilities to unseen data, categories, and user instructions.
</details>
<details>
<summary>摘要</summary>
Traditional Chinese:最近的生成扩散模型突破了文本控制的图像生成的可靠性和多样性，这些突破使得图像生成的应用在计算机视觉中尚未得到广泛应用。当前的 де факто方法是通过设计特定任务的模型结构和损失函数来实现这些任务。在这篇论文中，我们开发了一个统一的自然语言界面 для计算机视觉任务，这个界面将任务特定的设计选择抽象化，使得任务执行可以通过自然语言指令进行。我们的方法是将多种计算机视觉任务转化为文本到图像生成问题，其中文本表示任务的指令，并且生成的图像是视觉编码的任务输出。为了训练我们的模型，我们将常用的计算机视觉数据集合起来，包括分割、物体检测、深度估计和分类等任务。然后，我们使用一个大型自然语言模型来重新表达用于每个图像的指令模板，并通过这个过程创建了一个多modal和多任务的训练数据集。采用InstructPix2Pix架构，我们对文本到图像扩散模型进行指令调整，从而将其变成一个根据指令进行多任务视觉学习的模型。实验结果表明，我们的模型，称为InstructCV，与其他普遍和任务特定的视觉模型相比，表现竞争力强。此外，它还能够对未看到的数据、类别和用户指令进行吸引人的泛化。
</details></li>
</ul>
<hr>
<h2 id="Deep-Active-Learning-with-Noisy-Oracle-in-Object-Detection"><a href="#Deep-Active-Learning-with-Noisy-Oracle-in-Object-Detection" class="headerlink" title="Deep Active Learning with Noisy Oracle in Object Detection"></a>Deep Active Learning with Noisy Oracle in Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00372">http://arxiv.org/abs/2310.00372</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marius Schubert, Tobias Riedlinger, Karsten Kahl, Matthias Rottmann</li>
<li>for: 提高对象检测器的性能，减少人工标注的数量和质量不良的影响。</li>
<li>methods: 使用活动学习算法和标签审核模块，对活动数据中的噪声标注进行纠正，提高模型性能。</li>
<li>results: 在实验中，通过包括标签审核模块，使用部分标注预算来纠正噪声标注，提高对象检测器的性能，最高提升4.5个mAP点。<details>
<summary>Abstract</summary>
Obtaining annotations for complex computer vision tasks such as object detection is an expensive and time-intense endeavor involving a large number of human workers or expert opinions. Reducing the amount of annotations required while maintaining algorithm performance is, therefore, desirable for machine learning practitioners and has been successfully achieved by active learning algorithms. However, it is not merely the amount of annotations which influences model performance but also the annotation quality. In practice, the oracles that are queried for new annotations frequently contain significant amounts of noise. Therefore, cleansing procedures are oftentimes necessary to review and correct given labels. This process is subject to the same budget as the initial annotation itself since it requires human workers or even domain experts. Here, we propose a composite active learning framework including a label review module for deep object detection. We show that utilizing part of the annotation budget to correct the noisy annotations partially in the active dataset leads to early improvements in model performance, especially when coupled with uncertainty-based query strategies. The precision of the label error proposals has a significant influence on the measured effect of the label review. In our experiments we achieve improvements of up to 4.5 mAP points of object detection performance by incorporating label reviews at equal annotation budget.
</details>
<details>
<summary>摘要</summary>
获取复杂计算机视觉任务中的对象检测签名是一个昂贵的时间进行的劳动密集型任务，需要大量的人工工作或专家意见。因此，减少需要的签名数量而保持算法性能是机器学习实践者所需的，并已经由活动学习算法得到成功。然而，不仅是签名数量，签名质量也对模型性能产生影响。在实践中，查询新签名的或acles frequently contain significant amounts of noise。因此，清洁过程是必要的，以审查并更正给出的标签。这个过程与初始签名预算相同，需要人工工作或域专家。我们提议一种复合的活动学习框架，包括深度对象检测中的标签审查模块。我们表明，在活动数据集中部分使用签名预算来更正噪音签名，会导致早期提高模型性能，特别是与不确定性基于的查询策略相结合。标签错误提案的精度有重要的影响于测量效果。在我们的实验中，通过包含标签审查，提高了对象检测性能的4.5个MAP点。
</details></li>
</ul>
<hr>
<h2 id="Distilling-Inductive-Bias-Knowledge-Distillation-Beyond-Model-Compression"><a href="#Distilling-Inductive-Bias-Knowledge-Distillation-Beyond-Model-Compression" class="headerlink" title="Distilling Inductive Bias: Knowledge Distillation Beyond Model Compression"></a>Distilling Inductive Bias: Knowledge Distillation Beyond Model Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00369">http://arxiv.org/abs/2310.00369</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gousia Habib, Tausifa Jan Saleem, Brejesh Lall</li>
<li>for: 提高计算效率和实用性，为计算机视觉应用程序提供实用的解决方案。</li>
<li>methods: 采用 ensemble-based distillation 方法，从多种不同架构的轻量级教师模型中继承知识，以提高学生模型的性能。</li>
<li>results: 通过将多种架构的轻量级教师模型 ensemble 教学，学生模型可以从 readily 可识别的存储 dataset 中积累广泛的知识，提高学生模型的性能。<details>
<summary>Abstract</summary>
With the rapid development of computer vision, Vision Transformers (ViTs) offer the tantalizing prospect of unified information processing across visual and textual domains. But due to the lack of inherent inductive biases in ViTs, they require enormous amount of data for training. To make their applications practical, we introduce an innovative ensemble-based distillation approach distilling inductive bias from complementary lightweight teacher models. Prior systems relied solely on convolution-based teaching. However, this method incorporates an ensemble of light teachers with different architectural tendencies, such as convolution and involution, to instruct the student transformer jointly. Because of these unique inductive biases, instructors can accumulate a wide range of knowledge, even from readily identifiable stored datasets, which leads to enhanced student performance. Our proposed framework also involves precomputing and storing logits in advance, essentially the unnormalized predictions of the model. This optimization can accelerate the distillation process by eliminating the need for repeated forward passes during knowledge distillation, significantly reducing the computational burden and enhancing efficiency.
</details>
<details>
<summary>摘要</summary>
随着计算机视觉的快速发展，视觉变换器（ViTs）提供了融合视觉和文本领域的信息处理的吸引人可能性。然而，由于变换器缺乏自然的逻辑假设，因此需要很大量的数据进行训练。为了使其应用实用，我们提出了一种创新的ensemble-based distillation方法，将各种轻量级教学模型中的 inductive bias 逻辑假设传播给学生变换器。先前的系统仅仅依靠卷积而教学，而我们的方法则是结合不同的建筑倾向，如卷积和反卷积，来教学学生变换器。由于这些特有的逻辑假设，教师可以吸收广泛的知识，甚至从Ready readily可识别的存储数据集中，这导致了学生的性能提高。我们的提议的框架还包括预计算和存储logits的步骤，实际上是模型的未正规化预测值。这种优化可以减少了知识传播过程中的计算负担，使得学生的训练变得更加高效，提高了效率。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Posterior-Illumination-for-Ambiguity-aware-Inverse-Rendering"><a href="#Diffusion-Posterior-Illumination-for-Ambiguity-aware-Inverse-Rendering" class="headerlink" title="Diffusion Posterior Illumination for Ambiguity-aware Inverse Rendering"></a>Diffusion Posterior Illumination for Ambiguity-aware Inverse Rendering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00362">http://arxiv.org/abs/2310.00362</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linjie Lyu, Ayush Tewari, Marc Habermann, Shunsuke Saito, Michael Zollhöfer, Thomas Leimkühler, Christian Theobalt</li>
<li>for:  inverse rendering, 即从图像中推断场景属性的问题</li>
<li>methods:  integrate了一种含有自然照明地图的扩散概率模型，并与可微分的跟踪器结合使用优化框架</li>
<li>results: 可以从多种照明和空间分布式表面材料中采样，并且能够生成高质量、多样化的环境地图样本，并准确地反映输入图像的照明情况。<details>
<summary>Abstract</summary>
Inverse rendering, the process of inferring scene properties from images, is a challenging inverse problem. The task is ill-posed, as many different scene configurations can give rise to the same image. Most existing solutions incorporate priors into the inverse-rendering pipeline to encourage plausible solutions, but they do not consider the inherent ambiguities and the multi-modal distribution of possible decompositions. In this work, we propose a novel scheme that integrates a denoising diffusion probabilistic model pre-trained on natural illumination maps into an optimization framework involving a differentiable path tracer. The proposed method allows sampling from combinations of illumination and spatially-varying surface materials that are, both, natural and explain the image observations. We further conduct an extensive comparative study of different priors on illumination used in previous work on inverse rendering. Our method excels in recovering materials and producing highly realistic and diverse environment map samples that faithfully explain the illumination of the input images.
</details>
<details>
<summary>摘要</summary>
<<SYS>> invertible rendering，将场景属性从图像中推算出来的过程，是一个具有很多挑战的反向问题。任务是不定的，因为多种场景配置都可以导致同一张图像。大多数现有的解决方案将约束加入反向渲染管道中，以便推导可能的解决方案，但它们没有考虑内在的抽象和多模分布。在这项工作中，我们提议一种新的方案，即将自然照明地图预训练的杂化扩散概率模型integrated到一个可导的跟踪器框架中。该方法允许采样从组合照明和空间分布的表面材料中，这些材料都是自然的，并且能够解释输入图像的照明。我们进一步进行了对先前工作中不同照明约束的比较研究。我们的方法在恢复材料和生成高真实度、多样化的环境地图样本方面表现出色，能够准确地解释输入图像的照明。
</details></li>
</ul>
<hr>
<h2 id="Improving-Cross-dataset-Deepfake-Detection-with-Deep-Information-Decomposition"><a href="#Improving-Cross-dataset-Deepfake-Detection-with-Deep-Information-Decomposition" class="headerlink" title="Improving Cross-dataset Deepfake Detection with Deep Information Decomposition"></a>Improving Cross-dataset Deepfake Detection with Deep Information Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00359">http://arxiv.org/abs/2310.00359</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shanmin Yang, Shu Hu, Bin Zhu, Ying Fu, Siwei Lyu, Xi Wu, Xin Wang</li>
<li>for: 防止深伪技术威胁安全和社会信任，这篇论文旨在提出一个深信息分解（DID）框架。</li>
<li>methods: 该框架将注重高水平 semantics 特征，而不是视觉特征，以提高深伪检测的稳定性和普遍能力。</li>
<li>results: 实验结果显示，该框架在不同测试数据集中具有更高的检测精度和普遍能力，并且能够对不同的伪造方法进行更好的检测。<details>
<summary>Abstract</summary>
Deepfake technology poses a significant threat to security and social trust. Although existing detection methods have demonstrated high performance in identifying forgeries within datasets using the same techniques for training and testing, they suffer from sharp performance degradation when faced with cross-dataset scenarios where unseen deepfake techniques are tested. To address this challenge, we propose a deep information decomposition (DID) framework in this paper. Unlike most existing deepfake detection methods, our framework prioritizes high-level semantic features over visual artifacts. Specifically, it decomposes facial features into deepfake-related and irrelevant information and optimizes the deepfake information for real/fake discrimination to be independent of other factors. Our approach improves the robustness of deepfake detection against various irrelevant information changes and enhances the generalization ability of the framework to detect unseen forgery methods. Extensive experimental comparisons with existing state-of-the-art detection methods validate the effectiveness and superiority of the DID framework on cross-dataset deepfake detection.
</details>
<details>
<summary>摘要</summary>
深刻的假动作技术对安全和社会信任具有重大威胁。 although existing detection methods have shown high performance in identifying forgeries within datasets using the same techniques for training and testing, they suffer from sharp performance degradation when faced with cross-dataset scenarios where unseen deepfake techniques are tested. To address this challenge, we propose a deep information decomposition (DID) framework in this paper. Unlike most existing deepfake detection methods, our framework prioritizes high-level semantic features over visual artifacts. Specifically, it decomposes facial features into deepfake-related and irrelevant information and optimizes the deepfake information for real/fake discrimination to be independent of other factors. Our approach improves the robustness of deepfake detection against various irrelevant information changes and enhances the generalization ability of the framework to detect unseen forgery methods. Extensive experimental comparisons with existing state-of-the-art detection methods validate the effectiveness and superiority of the DID framework on cross-dataset deepfake detection.Here's the translation breakdown:* 深刻的假动作技术 (shēn kòng de zhèng zhī yì jī jī) - deepfake technology* 对安全和社会信任 (duì ān qì yè shè qì) - pose a significant threat to security and social trust*  existing detection methods (zhèng zhī yì jī) - existing detection methods*  have demonstrated high performance (zhèng zhī yì jī) - have demonstrated high performance*  in identifying forgeries within datasets (shuì zhèng zhī yì jī) - within datasets*  using the same techniques for training and testing (yì jī yuè xíng) - using the same techniques for training and testing*  but they suffer from sharp performance degradation (but they suffer from sharp performance degradation)* when faced with cross-dataset scenarios (zhèng zhī yì jī zhèng zhī yì jī) - when faced with cross-dataset scenarios* where unseen deepfake techniques are tested (where unseen deepfake techniques are tested)* To address this challenge, we propose (To address this challenge, we propose)* a deep information decomposition (DID) framework (a deep information decomposition (DID) framework)* Unlike most existing deepfake detection methods (zhèng zhī yì jī yuè xíng) - Unlike most existing deepfake detection methods* our framework prioritizes high-level semantic features (our framework prioritizes high-level semantic features)* over visual artifacts (over visual artifacts)* Specifically, it decomposes facial features (Specifically, it decomposes facial features)* into deepfake-related and irrelevant information (into deepfake-related and irrelevant information)* and optimizes the deepfake information (and optimizes the deepfake information)* for real/fake discrimination to be independent of other factors (for real/fake discrimination to be independent of other factors)* Our approach improves the robustness (Our approach improves the robustness)* of deepfake detection (of deepfake detection)* against various irrelevant information changes (against various irrelevant information changes)* and enhances the generalization ability (and enhances the generalization ability)* of the framework to detect unseen forgery methods (of the framework to detect unseen forgery methods)* Extensive experimental comparisons (Extensive experimental comparisons)* with existing state-of-the-art detection methods (with existing state-of-the-art detection methods)* validate the effectiveness (validate the effectiveness)* and superiority (and superiority)* of the DID framework (of the DID framework)* on cross-dataset deepfake detection (on cross-dataset deepfake detection)
</details></li>
</ul>
<hr>
<h2 id="Structural-Adversarial-Objectives-for-Self-Supervised-Representation-Learning"><a href="#Structural-Adversarial-Objectives-for-Self-Supervised-Representation-Learning" class="headerlink" title="Structural Adversarial Objectives for Self-Supervised Representation Learning"></a>Structural Adversarial Objectives for Self-Supervised Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00357">http://arxiv.org/abs/2310.00357</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiao7199/structural-adversarial-objectives">https://github.com/xiao7199/structural-adversarial-objectives</a></li>
<li>paper_authors: Xiao Zhang, Michael Maire</li>
<li>for: 本研究使用GANs进行自动化的表示学习，以提高图像识别的性能。</li>
<li>methods: 本文提出了一种基于GANs的自然语言生成方法，通过追加一些结构化模型责任来使混合网络学习表示。</li>
<li>results: 实验表明，通过使用本文提出的自然语言生成方法，GANs可以学习出高质量的表示，与对比学习方法相当。<details>
<summary>Abstract</summary>
Within the framework of generative adversarial networks (GANs), we propose objectives that task the discriminator for self-supervised representation learning via additional structural modeling responsibilities. In combination with an efficient smoothness regularizer imposed on the network, these objectives guide the discriminator to learn to extract informative representations, while maintaining a generator capable of sampling from the domain. Specifically, our objectives encourage the discriminator to structure features at two levels of granularity: aligning distribution characteristics, such as mean and variance, at coarse scales, and grouping features into local clusters at finer scales. Operating as a feature learner within the GAN framework frees our self-supervised system from the reliance on hand-crafted data augmentation schemes that are prevalent across contrastive representation learning methods. Across CIFAR-10/100 and an ImageNet subset, experiments demonstrate that equipping GANs with our self-supervised objectives suffices to produce discriminators which, evaluated in terms of representation learning, compete with networks trained by contrastive learning approaches.
</details>
<details>
<summary>摘要</summary>
在生成对抗网络（GAN）框架内，我们提议一些目标，要让分类器进行自我超vised学习的表示学习。这些目标与网络中的简洁正则化相结合，导引分类器学习提取有用的表示，同时保持生成器可以从领域中随机抽取样本。具体来说，我们的目标让分类器在两级划分粒度上结构化特征：在大规模划分水平上对分布特征进行匹配，并在细规划分水平上将特征分组到本地团集中。作为GAN框架内的特征学习器，我们的自我超vised系统不需要靠手工设计的数据增强方案，这种方法在对比学习方法中广泛存在。在CIFAR-10/100和ImageNet子集上进行了实验，发现当我们将GAN equip with我们的自我超vised目标时，评价在表示学习方面的分类器与对比学习方法训练的网络相比，具有竞争力。
</details></li>
</ul>
<hr>
<h2 id="RBF-Weighted-Hyper-Involution-for-RGB-D-Object-Detection"><a href="#RBF-Weighted-Hyper-Involution-for-RGB-D-Object-Detection" class="headerlink" title="RBF Weighted Hyper-Involution for RGB-D Object Detection"></a>RBF Weighted Hyper-Involution for RGB-D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00342">http://arxiv.org/abs/2310.00342</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mehfuz A Rahman, Jiju Peethambaran, Neil London</li>
<li>for: 实时RGBD物体检测模型</li>
<li>methods: 提议使用深度导航强化和升降样本联合层</li>
<li>results: 在NYU Depth v2和SUN RGB-D datasets上显示出比其他RGB-D基于物体检测模型更高的性能，并在新的室外RGB-D物体检测数据集上取得了最佳性能。<details>
<summary>Abstract</summary>
A vast majority of conventional augmented reality devices are equipped with depth sensors. Depth images produced by such sensors contain complementary information for object detection when used with color images. Despite the benefits, it remains a complex task to simultaneously extract photometric and depth features in real time due to the immanent difference between depth and color images. Moreover, standard convolution operations are not sufficient to properly extract information directly from raw depth images leading to intermediate representations of depth which is inefficient. To address these issues, we propose a real-time and two stream RGBD object detection model. The proposed model consists of two new components: a depth guided hyper-involution that adapts dynamically based on the spatial interaction pattern in the raw depth map and an up-sampling based trainable fusion layer that combines the extracted depth and color image features without blocking the information transfer between them. We show that the proposed model outperforms other RGB-D based object detection models on NYU Depth v2 dataset and achieves comparable (second best) results on SUN RGB-D. Additionally, we introduce a new outdoor RGB-D object detection dataset where our proposed model outperforms other models. The performance evaluation on diverse synthetic data generated from CAD models and images shows the potential of the proposed model to be adapted to augmented reality based applications.
</details>
<details>
<summary>摘要</summary>
大多数传统增强现实设备都配备有深度传感器。深度图像生成于such传感器中包含补偿信息，可以帮助对象检测。despite the benefits, it remains a complex task to simultaneously extract photometric and depth features in real time due to the inherent difference between depth and color images. Moreover, standard convolution operations are not sufficient to properly extract information directly from raw depth images leading to intermediate representations of depth, which is inefficient. To address these issues, we propose a real-time and two-stream RGBD object detection model. The proposed model consists of two new components: a depth-guided hyper-evolution that adapts dynamically based on the spatial interaction pattern in the raw depth map and an up-sampling based trainable fusion layer that combines the extracted depth and color image features without blocking the information transfer between them. We show that the proposed model outperforms other RGB-D based object detection models on NYU Depth v2 dataset and achieves comparable (second best) results on SUN RGB-D. Additionally, we introduce a new outdoor RGB-D object detection dataset where our proposed model outperforms other models. The performance evaluation on diverse synthetic data generated from CAD models and images shows the potential of the proposed model to be adapted to augmented reality based applications.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard Chinese languages. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="MFL-Data-Preprocessing-and-CNN-based-Oil-Pipeline-Defects-Detection"><a href="#MFL-Data-Preprocessing-and-CNN-based-Oil-Pipeline-Defects-Detection" class="headerlink" title="MFL Data Preprocessing and CNN-based Oil Pipeline Defects Detection"></a>MFL Data Preprocessing and CNN-based Oil Pipeline Defects Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00332">http://arxiv.org/abs/2310.00332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iurii Katser, Vyacheslav Kozitsin, Igor Mozolin</li>
<li>for: 这个论文主要用于检测石油管道异常现象，以提高运输系统的可靠性和安全性。</li>
<li>methods: 该论文使用了最新的卷积神经网络结构，并提出了一些有效的预处理技术和检测方法，以解决现有数据的限制。</li>
<li>results: 该论文通过使用实际数据进行验证，并达到了高度的性能水平，以增强石油管道异常检测的精度和效果。<details>
<summary>Abstract</summary>
Recently, the application of computer vision for anomaly detection has been under attention in several industrial fields. An important example is oil pipeline defect detection. Failure of one oil pipeline can interrupt the operation of the entire transportation system or cause a far-reaching failure. The automated defect detection could significantly decrease the inspection time and the related costs. However, there is a gap in the related literature when it comes to dealing with this task. The existing studies do not sufficiently cover the research of the Magnetic Flux Leakage data and the preprocessing techniques that allow overcoming the limitations set by the available data. This work focuses on alleviating these issues. Moreover, in doing so, we exploited the recent convolutional neural network structures and proposed robust approaches, aiming to acquire high performance considering the related metrics. The proposed approaches and their applicability were verified using real-world data.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:近期，计算机视觉在各个 industrielle 领域中的应用异常检测引起了关注。一个重要的例子是油管缺陷检测。油管缺陷的失效可以中断交通系统的全部运行或者引起广泛的故障。自动检测可以显著减少检测时间和相关成本。然而，现有的相关文献不充分考虑了阻碍数据的限制和磁漏泄检测数据的研究。这个工作强调解决这些问题。此外，我们还利用了最新的卷积神经网络结构，并提出了可靠的方法，以达到考虑相关维度的高性能。我们的方法和其可应用性得到了实际数据的验证。
</details></li>
</ul>
<hr>
<h2 id="Decoding-Realistic-Images-from-Brain-Activity-with-Contrastive-Self-supervision-and-Latent-Diffusion"><a href="#Decoding-Realistic-Images-from-Brain-Activity-with-Contrastive-Self-supervision-and-Latent-Diffusion" class="headerlink" title="Decoding Realistic Images from Brain Activity with Contrastive Self-supervision and Latent Diffusion"></a>Decoding Realistic Images from Brain Activity with Contrastive Self-supervision and Latent Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00318">http://arxiv.org/abs/2310.00318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingyuan Sun, Mingxiao Li, Marie-Francine Moens</li>
<li>for: 提高我们对大脑视系统的理解和计算机视觉模型之间的连接，使用人脑活动重建视觉刺激。</li>
<li>methods: 使用深度生成模型进行重建，并采用自我超级vised contrastive learning获取fMRI数据表示。</li>
<li>results: 实验结果显示，CnD可以高效重建复杂图像，并提供了对LDM组件和人脑视系统之间的量化解释。<details>
<summary>Abstract</summary>
Reconstructing visual stimuli from human brain activities provides a promising opportunity to advance our understanding of the brain's visual system and its connection with computer vision models. Although deep generative models have been employed for this task, the challenge of generating high-quality images with accurate semantics persists due to the intricate underlying representations of brain signals and the limited availability of parallel data. In this paper, we propose a two-phase framework named Contrast and Diffuse (CnD) to decode realistic images from functional magnetic resonance imaging (fMRI) recordings. In the first phase, we acquire representations of fMRI data through self-supervised contrastive learning. In the second phase, the encoded fMRI representations condition the diffusion model to reconstruct visual stimulus through our proposed concept-aware conditioning method. Experimental results show that CnD reconstructs highly plausible images on challenging benchmarks. We also provide a quantitative interpretation of the connection between the latent diffusion model (LDM) components and the human brain's visual system. In summary, we present an effective approach for reconstructing visual stimuli based on human brain activity and offer a novel framework to understand the relationship between the diffusion model and the human brain visual system.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将人脑活动转化为可读图像，提供了推进我们对大脑视系统的理解和计算机视觉模型之间的连接的可能性。虽然深入的生成模型已经被应用于这项任务，但是生成高质量图像的挑战仍然存在，因为大脑信号的下面表示和数据并不充分。在这篇论文中，我们提出了一种两阶段框架，名为对比和散射（CnD），用于从功能核磁共振图像记录中重建真实的图像。在第一阶段，我们通过自我超级vised对比学习获得了fMRI数据的表示。在第二阶段，这些编码的fMRI表示条件了我们提出的概念意识对应方法，使得扩散模型重建视觉刺激。实验结果表明，CnD可以在复杂的标准 benchmark 上重建高可能性的图像。我们还提供了人脑视系统中LDM组件和扩散模型之间的量化解释。总之，我们提出了基于人脑活动的可读图像重建方法，并提供了扩散模型和人脑视系统之间的新框架。
</details></li>
</ul>
<hr>
<h2 id="An-easy-zero-shot-learning-combination-Texture-Sensitive-Semantic-Segmentation-IceHrNet-and-Advanced-Style-Transfer-Learning-Strategy"><a href="#An-easy-zero-shot-learning-combination-Texture-Sensitive-Semantic-Segmentation-IceHrNet-and-Advanced-Style-Transfer-Learning-Strategy" class="headerlink" title="An easy zero-shot learning combination: Texture Sensitive Semantic Segmentation IceHrNet and Advanced Style Transfer Learning Strategy"></a>An easy zero-shot learning combination: Texture Sensitive Semantic Segmentation IceHrNet and Advanced Style Transfer Learning Strategy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00310">http://arxiv.org/abs/2310.00310</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pl23k/icehrnet">https://github.com/pl23k/icehrnet</a></li>
<li>paper_authors: Zhiyong Yang, Yuelong Zhu, Xiaoqin Zeng, Jun Zong, Xiuheng Liu, Ran Tao, Xiaofei Cong, Yufeng Yu</li>
<li>for: 本研究旨在提出一种简单的零shot语义 segmentation方法，使用style transfer来实现。</li>
<li>methods: 我们使用了医学影像数据集（血液图像）来训练一个river ice语义 segmentation模型。首先，我们构建了一个river ice语义 segmentation数据集IPC_RI_SEG，使用固定摄像头和涵盖整个河流冰融化过程。其次，我们提出了一种高分辨率Texture Fusion semantic segmentation网络，名为IceHrNet。该网络使用HRNet作为背景，并添加了ASPP和Decoder segmentation头，以保留低级别的Texture特征，进行细致的语义分割。最后，我们提出了一种简单有效的高级 Style transfer学习策略，可以在交叉领域语义分割数据集上进行零shot转移学习，实现了87% mIoU的语义分割效果。</li>
<li>results: 实验显示，IceHrNet在Texture专注数据集IPC_RI_SEG上超过了现状的方法，并在Shape专注river ice数据集上达到了优秀的效果。在零shot转移学习中，IceHrNet比其他方法提高了2个百分点。我们的代码和模型已经发布在<a target="_blank" rel="noopener" href="https://github.com/PL23K/IceHrNet%E3%80%82">https://github.com/PL23K/IceHrNet。</a><details>
<summary>Abstract</summary>
We proposed an easy method of Zero-Shot semantic segmentation by using style transfer. In this case, we successfully used a medical imaging dataset (Blood Cell Imagery) to train a model for river ice semantic segmentation. First, we built a river ice semantic segmentation dataset IPC_RI_SEG using a fixed camera and covering the entire ice melting process of the river. Second, a high-resolution texture fusion semantic segmentation network named IceHrNet is proposed. The network used HRNet as the backbone and added ASPP and Decoder segmentation heads to retain low-level texture features for fine semantic segmentation. Finally, a simple and effective advanced style transfer learning strategy was proposed, which can perform zero-shot transfer learning based on cross-domain semantic segmentation datasets, achieving a practical effect of 87% mIoU for semantic segmentation of river ice without target training dataset (25% mIoU for None Stylized, 65% mIoU for Conventional Stylized, our strategy improved by 22%). Experiments showed that the IceHrNet outperformed the state-of-the-art methods on the texture-focused dataset IPC_RI_SEG, and achieved an excellent result on the shape-focused river ice datasets. In zero-shot transfer learning, IceHrNet achieved an increase of 2 percentage points compared to other methods. Our code and model are published on https://github.com/PL23K/IceHrNet.
</details>
<details>
<summary>摘要</summary>
我们提出了一种简单的零shot semantic segmentation方法，利用样式传输。在这种情况下，我们成功地使用医疗影像 dataset（血球影像）来训练一个河川冰 semantic segmentation 模型。首先，我们建立了一个河川冰 semantic segmentation dataset IPC_RI_SEG，使用固定摄像头和覆盖整个河川冰融化过程。其次，我们提出了一种高分辨率Texture Fusion semantic segmentation网络，名为 IceHrNet。该网络使用 HRNet 作为背景，并添加 ASPP 和 Decoder  segmentation 头，以保留低级别 Texture 特征，以提高精确的semantic segmentation。最后，我们提出了一种简单有效的高级 Style Transfer 学习策略，可以在 cross-domain semantic segmentation 数据集上进行零shot Transfer learning，实现了87% mIoU 的semantic segmentation精度，比无目标训练数据集 (25% mIoU for None Stylized, 65% mIoU for Conventional Stylized) 提高22%。实验显示，IceHrNet 在 texture-focused 数据集 IPC_RI_SEG 上超过了现状级方法，并在 shape-focused 河川冰数据集上达到了出色的结果。在零shot Transfer learning 中，IceHrNet 相比其他方法提高了2个百分点。我们的代码和模型已经在 <https://github.com/PL23K/IceHrNet> 上发布。
</details></li>
</ul>
<hr>
<h2 id="Dual-Augmented-Transformer-Network-for-Weakly-Supervised-Semantic-Segmentation"><a href="#Dual-Augmented-Transformer-Network-for-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="Dual-Augmented Transformer Network for Weakly Supervised Semantic Segmentation"></a>Dual-Augmented Transformer Network for Weakly Supervised Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00307">http://arxiv.org/abs/2310.00307</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingliang Deng, Zonghan Li</li>
<li>for: 这个论文目的是提出一种基于 dual-augmented transformer 网络和自我规则约束的弱监督Semantic Segmentation（WSSS）方法，以提高WSSS的完teness和准确性。</li>
<li>methods: 该方法使用了一个双网络，包括 CNN 和 transformer 网络，并在两个网络之间进行互补学习，以提高 WSSS 的性能。此外，该方法还使用了自我规则约束来避免过拟合。</li>
<li>results: 对于 PASCAL VOC 2012 难度评测 benchmark，该方法达到了最高的效果，超过了之前的州立艺术方法。<details>
<summary>Abstract</summary>
Weakly supervised semantic segmentation (WSSS), a fundamental computer vision task, which aims to segment out the object within only class-level labels. The traditional methods adopt the CNN-based network and utilize the class activation map (CAM) strategy to discover the object regions. However, such methods only focus on the most discriminative region of the object, resulting in incomplete segmentation. An alternative is to explore vision transformers (ViT) to encode the image to acquire the global semantic information. Yet, the lack of transductive bias to objects is a flaw of ViT. In this paper, we explore the dual-augmented transformer network with self-regularization constraints for WSSS. Specifically, we propose a dual network with both CNN-based and transformer networks for mutually complementary learning, where both networks augment the final output for enhancement. Massive systemic evaluations on the challenging PASCAL VOC 2012 benchmark demonstrate the effectiveness of our method, outperforming previous state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
弱类指导 semantic segmentation (WSSS) 是计算机视觉中的基本任务，旨在只使用类别标签来分割对象。传统方法通常采用 CNN 网络和使用 CAM 策略来发现对象区域。然而，这些方法只关注对象中最有特征的区域，导致 segmentation 不够完整。为了解决这问题，我们可以探索使用 transformer 网络来编码图像，以获得全局 semantic 信息。然而，transformer 网络缺乏对物体的推导性偏好。在这篇论文中，我们探索了 dual-augmented transformer 网络，并采用自我regularization 约束来解决 WSSS 问题。具体来说，我们提出了一个 dual 网络，其中包括 CNN 基于网络和 transformer 网络，用于互补学习。在 PASCAL VOC 2012 数据集上进行了大规模系统性评估，我们的方法被证明高效，并超过了之前的状态码法。
</details></li>
</ul>
<hr>
<h2 id="QUIZ-An-Arbitrary-Volumetric-Point-Matching-Method-for-Medical-Image-Registration"><a href="#QUIZ-An-Arbitrary-Volumetric-Point-Matching-Method-for-Medical-Image-Registration" class="headerlink" title="QUIZ: An Arbitrary Volumetric Point Matching Method for Medical Image Registration"></a>QUIZ: An Arbitrary Volumetric Point Matching Method for Medical Image Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00296">http://arxiv.org/abs/2310.00296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lin Liu, Xinxin Fan, Haoyang Liu, Chulong Zhang, Weibin Kong, Jingjing Dai, Yuming Jiang, Yaoqin Xie, Xiaokun Liang</li>
<li>for: 这个研究是为了提出一种基于自适应点对匹配的医疗影像注册方法，以解决现有方法在不同姿势或影像质量差强时的不稳定和错误问题。</li>
<li>methods: 本研究使用了一种新的方法，即查询点问题（QUIZ），它是基于本地-全局匹配点的对应，使用了对应点进行特征提取，并使用Transformer架构进行全球匹配查询，最后是通过实现本地影像对称变换。</li>
<li>results: 实验结果显示，这种方法在一个大尺度变形的阴道癌病人 dataset 上的注册结果比现有方法更为稳定和准确，甚至在跨Modal subjects 上也能够取得更好的结果，超过现有state-of-the-art。<details>
<summary>Abstract</summary>
Rigid pre-registration involving local-global matching or other large deformation scenarios is crucial. Current popular methods rely on unsupervised learning based on grayscale similarity, but under circumstances where different poses lead to varying tissue structures, or where image quality is poor, these methods tend to exhibit instability and inaccuracies. In this study, we propose a novel method for medical image registration based on arbitrary voxel point of interest matching, called query point quizzer (QUIZ). QUIZ focuses on the correspondence between local-global matching points, specifically employing CNN for feature extraction and utilizing the Transformer architecture for global point matching queries, followed by applying average displacement for local image rigid transformation. We have validated this approach on a large deformation dataset of cervical cancer patients, with results indicating substantially smaller deviations compared to state-of-the-art methods. Remarkably, even for cross-modality subjects, it achieves results surpassing the current state-of-the-art.
</details>
<details>
<summary>摘要</summary>
rigid pre-registration involving local-global matching or other large deformation scenarios is crucial. current popular methods rely on unsupervised learning based on grayscale similarity, but under circumstances where different poses lead to varying tissue structures, or where image quality is poor, these methods tend to exhibit instability and inaccuracies. in this study, we propose a novel method for medical image registration based on arbitrary voxel point of interest matching, called query point quizzer (quiz). quiz focuses on the correspondence between local-global matching points, specifically employing cnn for feature extraction and utilizing the transformer architecture for global point matching queries, followed by applying average displacement for local image rigid transformation. we have validated this approach on a large deformation dataset of cervical cancer patients, with results indicating substantially smaller deviations compared to state-of-the-art methods. remarkably, even for cross-modality subjects, it achieves results surpassing the current state-of-the-art.Here's the breakdown of the translation:* rigid pre-registration: 固定预注册* involving local-global matching: 包括本地-全局匹配* or other large deformation scenarios: 或其他大型扭曲场景* is crucial: 是关键的* Current popular methods rely on unsupervised learning: 当前流行的方法依靠无监督学习* based on grayscale similarity: 基于灰度相似性* but under circumstances where different poses lead to varying tissue structures: 但在不同的姿势下导致组织结构变化* or where image quality is poor: 或图像质量不佳* these methods tend to exhibit instability and inaccuracies: 这些方法往往表现出不稳定和不准确* In this study, we propose a novel method: 在这项研究中，我们提出了一种新的方法* for medical image registration: 医疗图像注册* based on arbitrary voxel point of interest matching: 基于任意体素点的关注匹配* called query point quizzer (QUIZ): 称为查询点赛询（QUIZ）* QUIZ focuses on the correspondence between local-global matching points: 赛询集中注重本地-全局匹配点之间的匹配* specifically employing CNN for feature extraction: 特别利用CNN提取特征* and utilizing the Transformer architecture for global point matching queries: 并利用Transformer架构进行全局点匹配查询* followed by applying average displacement for local image rigid transformation: 然后应用平均偏移来实现本地图像固定变换* We have validated this approach on a large deformation dataset of cervical cancer patients: 我们在一个大型扭曲 dataset 上验证了这种方法* with results indicating substantially smaller deviations compared to state-of-the-art methods: 结果表明与当前状态艺的方法相比，这种方法具有较小的偏差* Remarkably, even for cross-modality subjects: 备受惊叹的是，这种方法可以在不同的modalities中进行批处理* it achieves results surpassing the current state-of-the-art: 它在当前状态艺中超越了当前的最佳性能I hope this helps! Let me know if you have any further questions or if there's anything else I can help you with.
</details></li>
</ul>
<hr>
<h2 id="Pubic-Symphysis-Fetal-Head-Segmentation-Using-Pure-Transformer-with-Bi-level-Routing-Attention"><a href="#Pubic-Symphysis-Fetal-Head-Segmentation-Using-Pure-Transformer-with-Bi-level-Routing-Attention" class="headerlink" title="Pubic Symphysis-Fetal Head Segmentation Using Pure Transformer with Bi-level Routing Attention"></a>Pubic Symphysis-Fetal Head Segmentation Using Pure Transformer with Bi-level Routing Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00289">http://arxiv.org/abs/2310.00289</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pengzhou Cai, Jiang Lu, Yanxin Li, Libin Lan</li>
<li>for: 这个论文是为了解决公针缘-胎头分割任务而提出的方法。</li>
<li>methods: 该方法采用了一种类似于U-Net的纯转换器架构，并使用了二级路由注意力和跳过连接，能够有效地学习本地-全球含义。</li>
<li>results: 该方法在 транс体内超声影像数据集上进行评估，并达到了相当于的最终分数。代码将在 GitHub 上公开。<details>
<summary>Abstract</summary>
In this paper, we propose a method, named BRAU-Net, to solve the pubic symphysis-fetal head segmentation task. The method adopts a U-Net-like pure Transformer architecture with bi-level routing attention and skip connections, which effectively learns local-global semantic information. The proposed BRAU-Net was evaluated on transperineal Ultrasound images dataset from the pubic symphysis-fetal head segmentation and angle of progression (FH-PS-AOP) challenge. The results demonstrate that the proposed BRAU-Net achieves comparable a final score. The codes will be available at https://github.com/Caipengzhou/BRAU-Net.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种方法，名为BRAU-Net，用于解决公钵缝-胎头分割任务。该方法采用了一种类似于U-Net的纯Transformer架构，具有 би层路由注意力和跳过连接，能够有效学习本地-全局semantic信息。我们提posed的BRAU-Net在transperineal Ultrasound图像数据集上进行了评估，并实现了相对比较高的最终分数。codes将在https://github.com/Caipengzhou/BRAU-Net中提供。
</details></li>
</ul>
<hr>
<h2 id="InFER-A-Multi-Ethnic-Indian-Facial-Expression-Recognition-Dataset"><a href="#InFER-A-Multi-Ethnic-Indian-Facial-Expression-Recognition-Dataset" class="headerlink" title="InFER: A Multi-Ethnic Indian Facial Expression Recognition Dataset"></a>InFER: A Multi-Ethnic Indian Facial Expression Recognition Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00287">http://arxiv.org/abs/2310.00287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Syed Sameen Ahmad Rizvi, Preyansh Agrawal, Jagat Sesh Challa, Pratik Narang</li>
<li>for: 这个论文是为了开发一个面部表达识别系统，特别是针对印度次大陆的多元人种背景下的人脸表达识别。</li>
<li>methods: 这个论文使用了深度学习技术，并使用了10200张图片和4200段视频，包括7种基本的面部表达和6000张来自互联网的自然表达。</li>
<li>results: 这个论文通过实验表明，使用深度学习技术可以在印度次大陆的多元人种背景下实现高度的人脸表达识别精度。<details>
<summary>Abstract</summary>
The rapid advancement in deep learning over the past decade has transformed Facial Expression Recognition (FER) systems, as newer methods have been proposed that outperform the existing traditional handcrafted techniques. However, such a supervised learning approach requires a sufficiently large training dataset covering all the possible scenarios. And since most people exhibit facial expressions based upon their age group, gender, and ethnicity, a diverse facial expression dataset is needed. This becomes even more crucial while developing a FER system for the Indian subcontinent, which comprises of a diverse multi-ethnic population. In this work, we present InFER, a real-world multi-ethnic Indian Facial Expression Recognition dataset consisting of 10,200 images and 4,200 short videos of seven basic facial expressions. The dataset has posed expressions of 600 human subjects, and spontaneous/acted expressions of 6000 images crowd-sourced from the internet. To the best of our knowledge InFER is the first of its kind consisting of images from 600 subjects from very diverse ethnicity of the Indian Subcontinent. We also present the experimental results of baseline & deep FER methods on our dataset to substantiate its usability in real-world practical applications.
</details>
<details>
<summary>摘要</summary>
随着深度学习技术的快速发展，过去十年，人脸表达识别（FER）系统得到了深刻的改进，新的方法被提出，超越了传统的手工设计方法。然而，这种监督学习方法需要一个具有所有可能情况的充分大的训练数据集。而人们的表达往往与年龄组、性别和民族相关，因此需要一个多样化的人脸表达数据集。这变得更加重要，在开发印度次大陆的FER系统时。在这种情况下，我们提出了InFER，一个包含10,200张图像和4,200个短视频的多元族裔印度人脸表达识别数据集。该数据集包含1000名人类的pose表达和互联网上抓取的6000张自然表达图像。根据我们所知，InFER是世界上第一个包含600名不同民族背景的人脸表达数据集。我们还将展示基线和深度FER方法在我们的数据集上的实验结果，以证明其在实际应用中的可用性。
</details></li>
</ul>
<hr>
<h2 id="Unleash-Data-Generation-for-Efficient-and-Effective-Data-free-Knowledge-Distillation"><a href="#Unleash-Data-Generation-for-Efficient-and-Effective-Data-free-Knowledge-Distillation" class="headerlink" title="Unleash Data Generation for Efficient and Effective Data-free Knowledge Distillation"></a>Unleash Data Generation for Efficient and Effective Data-free Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00258">http://arxiv.org/abs/2310.00258</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fw742211/nayer">https://github.com/fw742211/nayer</a></li>
<li>paper_authors: Minh-Tuan Tran, Trung Le, Xuan-May Le, Mehrtash Harandi, Quan Hung Tran, Dinh Phung</li>
<li>for: 这篇论文的目的是提出一种新的无数据知识传播（DFKD）方法，并且解决了现有方法无法从随机变量中生成高质量数据的问题。</li>
<li>methods: 本文提出的方法为杂凑层生成（NAYER），它将随机性源从输入转移到杂凑层，并使用具有含义的标签文字嵌入（LTE）作为输入。LTE能够含有丰富的意义ful inter-class信息，允许生成高质量数据，只需要几个训练步骤。同时，杂凑层可以解决标签信息的价值过滤问题，使模型不会过度强调标签信息。</li>
<li>results: 实验结果显示， NAYER 不仅超越了现有的方法，而且比前一些方法快5-15倍。<details>
<summary>Abstract</summary>
Data-Free Knowledge Distillation (DFKD) has recently made remarkable advancements with its core principle of transferring knowledge from a teacher neural network to a student neural network without requiring access to the original data. Nonetheless, existing approaches encounter a significant challenge when attempting to generate samples from random noise inputs, which inherently lack meaningful information. Consequently, these models struggle to effectively map this noise to the ground-truth sample distribution, resulting in the production of low-quality data and imposing substantial time requirements for training the generator. In this paper, we propose a novel Noisy Layer Generation method (NAYER) which relocates the randomness source from the input to a noisy layer and utilizes the meaningful label-text embedding (LTE) as the input. The significance of LTE lies in its ability to contain substantial meaningful inter-class information, enabling the generation of high-quality samples with only a few training steps. Simultaneously, the noisy layer plays a key role in addressing the issue of diversity in sample generation by preventing the model from overemphasizing the constrained label information. By reinitializing the noisy layer in each iteration, we aim to facilitate the generation of diverse samples while still retaining the method's efficiency, thanks to the ease of learning provided by LTE. Experiments carried out on multiple datasets demonstrate that our NAYER not only outperforms the state-of-the-art methods but also achieves speeds 5 to 15 times faster than previous approaches.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Data-Free Knowledge Distillation (DFKD) has recently made remarkable advancements with its core principle of transferring knowledge from a teacher neural network to a student neural network without requiring access to the original data. Nonetheless, existing approaches encounter a significant challenge when attempting to generate samples from random noise inputs, which inherently lack meaningful information. Consequently, these models struggle to effectively map this noise to the ground-truth sample distribution, resulting in the production of low-quality data and imposing substantial time requirements for training the generator. In this paper, we propose a novel Noisy Layer Generation method (NAYER) which relocates the randomness source from the input to a noisy layer and utilizes the meaningful label-text embedding (LTE) as the input. The significance of LTE lies in its ability to contain substantial meaningful inter-class information, enabling the generation of high-quality samples with only a few training steps. Simultaneously, the noisy layer plays a key role in addressing the issue of diversity in sample generation by preventing the model from overemphasizing the constrained label information. By reinitializing the noisy layer in each iteration, we aim to facilitate the generation of diverse samples while still retaining the method's efficiency, thanks to the ease of learning provided by LTE. Experiments carried out on multiple datasets demonstrate that our NAYER not only outperforms the state-of-the-art methods but also achieves speeds 5 to 15 times faster than previous approaches."Translation:“数据无法知识传播（DFKD）最近又取得了显著进步，其核心思想是将知识从教师神经网络传播到学生神经网络，无需访问原始数据。然而，现有方法在生成随机噪声输入时遇到了重大挑战，因为这些噪声缺乏有意义信息。这使得这些模型很难准确地将噪声映射到真实样本分布，从而生成低质量数据，并且需要训练生成器的很长时间。在这篇论文中，我们提出了一种新的噪声层生成方法（NAYER），它将噪声源从输入重新定义到噪声层，并使用有意义的标签文本嵌入（LTE）作为输入。LTE的重要性在于它能够包含大量有意义的 между类信息，使得通过只需几个训练步骤就可以生成高质量样本。同时，噪声层对样本生成的多样性做出了重要贡献，避免模型过分强调约束的标签信息。我们在每个迭代中重新初始化噪声层，以便生成多样化的样本，同时仍保持方法的效率，即使是通过LTE的易学习性。我们在多个数据集上进行了实验，结果表明，我们的NAYER不仅超过了现有方法的性能，而且在5-15倍 faster than previous approaches。”
</details></li>
</ul>
<hr>
<h2 id="MMPI-a-Flexible-Radiance-Field-Representation-by-Multiple-Multi-plane-Images-Blending"><a href="#MMPI-a-Flexible-Radiance-Field-Representation-by-Multiple-Multi-plane-Images-Blending" class="headerlink" title="MMPI: a Flexible Radiance Field Representation by Multiple Multi-plane Images Blending"></a>MMPI: a Flexible Radiance Field Representation by Multiple Multi-plane Images Blending</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00249">http://arxiv.org/abs/2310.00249</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuze He, Peng Wang, Yubin Hu, Wang Zhao, Ran Yi, Yong-Jin Liu, Wenping Wang</li>
<li>for: 这篇论文旨在探讨基于多平面图像（MPI）的神经辐射场（NeRF）的高质量视图合成方法，以扩展现有的MPI-based NeRF方法到更复杂的场景中。</li>
<li>methods: 作者采用MPI parameterization的NeRF学习方法，并提出了一种基于多个MPI的适应混合操作，以模拟不同视角和摄像头分布的场景。</li>
<li>results: 实验结果表明，该方法可以高质量地生成不同摄像头分布和视角的新视图图像，并且比前一代快速训练NeRF方法更快速地训练完成。此外，作者还示出了该方法可以处理长轨迹和新视图图像的问题，表明其在自动驾驶等应用中的潜在可能性。<details>
<summary>Abstract</summary>
This paper presents a flexible representation of neural radiance fields based on multi-plane images (MPI), for high-quality view synthesis of complex scenes. MPI with Normalized Device Coordinate (NDC) parameterization is widely used in NeRF learning for its simple definition, easy calculation, and powerful ability to represent unbounded scenes. However, existing NeRF works that adopt MPI representation for novel view synthesis can only handle simple forward-facing unbounded scenes, where the input cameras are all observing in similar directions with small relative translations. Hence, extending these MPI-based methods to more complex scenes like large-range or even 360-degree scenes is very challenging. In this paper, we explore the potential of MPI and show that MPI can synthesize high-quality novel views of complex scenes with diverse camera distributions and view directions, which are not only limited to simple forward-facing scenes. Our key idea is to encode the neural radiance field with multiple MPIs facing different directions and blend them with an adaptive blending operation. For each region of the scene, the blending operation gives larger blending weights to those advantaged MPIs with stronger local representation abilities while giving lower weights to those with weaker representation abilities. Such blending operation automatically modulates the multiple MPIs to appropriately represent the diverse local density and color information. Experiments on the KITTI dataset and ScanNet dataset demonstrate that our proposed MMPI synthesizes high-quality images from diverse camera pose distributions and is fast to train, outperforming the previous fast-training NeRF methods for novel view synthesis. Moreover, we show that MMPI can encode extremely long trajectories and produce novel view renderings, demonstrating its potential in applications like autonomous driving.
</details>
<details>
<summary>摘要</summary>
Our key idea is to encode the neural radiance field with multiple MPIs facing different directions and blend them with an adaptive blending operation. For each region of the scene, the blending operation gives larger blending weights to those MPIs with stronger local representation abilities and lower weights to those with weaker representation abilities. This automatically modulates the multiple MPIs to appropriately represent the diverse local density and color information.Experiments on the KITTI dataset and ScanNet dataset show that our proposed method, called Multi-plane Multi-Image (MMPI), synthesizes high-quality images from diverse camera pose distributions and is fast to train, outperforming previous fast-training NeRF methods for novel view synthesis. Moreover, we demonstrate that MMPI can encode extremely long trajectories and produce novel view renderings, indicating its potential in applications like autonomous driving.
</details></li>
</ul>
<hr>
<h2 id="Walking-Traversable-Traversability-Prediction-via-Multiple-Human-Object-Tracking-under-Occlusion"><a href="#Walking-Traversable-Traversability-Prediction-via-Multiple-Human-Object-Tracking-under-Occlusion" class="headerlink" title="Walking &#x3D; Traversable? : Traversability Prediction via Multiple Human Object Tracking under Occlusion"></a>Walking &#x3D; Traversable? : Traversability Prediction via Multiple Human Object Tracking under Occlusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00242">http://arxiv.org/abs/2310.00242</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan Tay Yu Liang, Kanji Tanaka</li>
<li>for: 这种技术可以提高室内机器人导航，预测受阻的地板。</li>
<li>methods: 该方法使用第三人称视角单目摄像头，使用SLAM和MOT两种跟踪器监测站立物和移动人员的互动。</li>
<li>results: 该方法可以在视觉复杂enario中稳定地预测通行性，包括 occlusion、非线性视角、深度不确定和多个人员的交叠。<details>
<summary>Abstract</summary>
The emerging ``Floor plan from human trails (PfH)" technique has great potential for improving indoor robot navigation by predicting the traversability of occluded floors. This study presents an innovative approach that replaces first-person-view sensors with a third-person-view monocular camera mounted on the observer robot. This approach can gather measurements from multiple humans, expanding its range of applications. The key idea is to use two types of trackers, SLAM and MOT, to monitor stationary objects and moving humans and assess their interactions. This method achieves stable predictions of traversability even in challenging visual scenarios, such as occlusions, nonlinear perspectives, depth uncertainty, and intersections involving multiple humans. Additionally, we extend map quality metrics to apply to traversability maps, facilitating future research. We validate our proposed method through fusion and comparison with established techniques.
</details>
<details>
<summary>摘要</summary>
“人类脚踪映射（PfH）技术在室内机器人导航方面具有潜在的潜力，可以预测受阻的loor的可行性。本研究提出了一种创新的方法，替换了首人视角感知器，使用跟踪器Mounted on the observer robot的第三人视角监测器来收集多个人的数据。这种方法可以监测站ARYObjects和移动人员之间的互动，并对其进行评估。这种方法可以在视觉复杂场景中稳定地预测可行性，包括 occlusions、非线性视角、深度不确定性和多个人的交叉。此外，我们扩展了图像质量指标，以便应用于可行性图。我们验证了我们的提议方法通过融合和与现有技术进行比较。”Note that Simplified Chinese is the official standard for Chinese writing in mainland China, and it is used in this translation. Traditional Chinese is also commonly used in Taiwan and Hong Kong, but it may have slightly different grammar and character forms.
</details></li>
</ul>
<hr>
<h2 id="Learning-Mask-aware-CLIP-Representations-for-Zero-Shot-Segmentation"><a href="#Learning-Mask-aware-CLIP-Representations-for-Zero-Shot-Segmentation" class="headerlink" title="Learning Mask-aware CLIP Representations for Zero-Shot Segmentation"></a>Learning Mask-aware CLIP Representations for Zero-Shot Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00240">http://arxiv.org/abs/2310.00240</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiaosiyu1999/maft">https://github.com/jiaosiyu1999/maft</a></li>
<li>paper_authors: Siyu Jiao, Yunchao Wei, Yaowei Wang, Yao Zhao, Humphrey Shi<br>for:* The paper aims to improve the performance of zero-shot segmentation methods by addressing the insensitivity of CLIP to different mask proposals.methods:* The proposed method, Mask-aware Fine-tuning (MAFT), uses an Image-Proposals CLIP Encoder (IP-CLIP Encoder) to handle arbitrary numbers of image and mask proposals simultaneously.* MAFT introduces mask-aware loss and self-distillation loss to fine-tune IP-CLIP Encoder, ensuring CLIP is responsive to different mask proposals while maintaining transferability.results:* With MAFT, the performance of state-of-the-art methods is promoted by a large margin on popular zero-shot benchmarks, including COCO, Pascal-VOC, and ADE20K. Specifically, the mIoU for unseen classes is improved by 8.2%, 3.2%, and 4.3% respectively.<details>
<summary>Abstract</summary>
Recently, pre-trained vision-language models have been increasingly used to tackle the challenging zero-shot segmentation task. Typical solutions follow the paradigm of first generating mask proposals and then adopting CLIP to classify them. To maintain the CLIP's zero-shot transferability, previous practices favour to freeze CLIP during training. However, in the paper, we reveal that CLIP is insensitive to different mask proposals and tends to produce similar predictions for various mask proposals of the same image. This insensitivity results in numerous false positives when classifying mask proposals. This issue mainly relates to the fact that CLIP is trained with image-level supervision. To alleviate this issue, we propose a simple yet effective method, named Mask-aware Fine-tuning (MAFT). Specifically, Image-Proposals CLIP Encoder (IP-CLIP Encoder) is proposed to handle arbitrary numbers of image and mask proposals simultaneously. Then, mask-aware loss and self-distillation loss are designed to fine-tune IP-CLIP Encoder, ensuring CLIP is responsive to different mask proposals while not sacrificing transferability. In this way, mask-aware representations can be easily learned to make the true positives stand out. Notably, our solution can seamlessly plug into most existing methods without introducing any new parameters during the fine-tuning process. We conduct extensive experiments on the popular zero-shot benchmarks. With MAFT, the performance of the state-of-the-art methods is promoted by a large margin: 50.4% (+ 8.2%) on COCO, 81.8% (+ 3.2%) on Pascal-VOC, and 8.7% (+4.3%) on ADE20K in terms of mIoU for unseen classes. The code is available at https://github.com/jiaosiyu1999/MAFT.git.
</details>
<details>
<summary>摘要</summary>
近期，预训练的视觉语言模型在零样式分割任务中表现越来越出色。一般来说，这些解决方案采用的是首先生成mask提案，然后采用CLIP进行分类的方法。为保持CLIP的零样式传输性，以前的做法是冻结CLIP。然而，我们发现CLIP对不同的mask提案敏感度很低，它会为同一张图片的不同mask提案生成相同的预测结果。这种敏感度问题导致了许多假阳性的分类结果。这主要与CLIP在图像水平上进行训练有关。为解决这个问题，我们提出了一种简单 yet 有效的方法，即Mask-aware Fine-tuning（MAFT）。具体来说，我们提出了Image-Proposals CLIP Encoder（IP-CLIP Encoder），可以同时处理任意数量的图像和mask提案。然后，我们设计了面孔检测和自我顾问损失来细化IP-CLIP Encoder，确保CLIP对不同的mask提案具有响应性。这样，面孔检测可以轻松地学习出mask-aware表示，使真阳性能够出亮。另外，我们的解决方案可以不需要添加任何新参数，直接在已有的训练过程中进行细化。我们在popular zero-shot benchmark上进行了广泛的实验，与MAFT结合，state-of-the-art方法的性能得到了大幅提升：COCO中的50.4% (+ 8.2%)，Pascal-VOC中的81.8% (+ 3.2%)，ADE20K中的8.7% (+4.3%)。相关代码可以在https://github.com/jiaosiyu1999/MAFT.git中找到。
</details></li>
</ul>
<hr>
<h2 id="Domain-Controlled-Prompt-Learning"><a href="#Domain-Controlled-Prompt-Learning" class="headerlink" title="Domain-Controlled Prompt Learning"></a>Domain-Controlled Prompt Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07730">http://arxiv.org/abs/2310.07730</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sfedfcv/redesigned-pancake">https://github.com/Sfedfcv/redesigned-pancake</a></li>
<li>paper_authors: Qinglong Cao, Zhengqin Xu, Yuantian Chen, Chao Ma, Xiaokang Yang</li>
<li>for: 这个研究的目的是为特殊领域的Remote Sensing Image (RSIs) 和医学影像等领域进行适应和扩展。</li>
<li>methods: 我们提出了一种叫做领域控制的提示学习（Domain-Controlled Prompt Learning，DCPL），使用大规模的专业领域基础模型（LSDM）提供特殊领域知识，并使用轻量级神经网络将这些知识转换为领域偏好，以直接控制颜ppo上的提示。</li>
<li>results: 我们的方法在特殊领域影像识别 зада域中得到了最佳性能。<details>
<summary>Abstract</summary>
Large pre-trained vision-language models, such as CLIP, have shown remarkable generalization capabilities across various tasks when appropriate text prompts are provided. However, adapting these models to specialized domains, like remote sensing images (RSIs), medical images, etc, remains unexplored and challenging. Existing prompt learning methods often lack domain-awareness or domain-transfer mechanisms, leading to suboptimal performance due to the misinterpretation of specialized images in natural image patterns. To tackle this dilemma, we proposed a Domain-Controlled Prompt Learning for the specialized domains. Specifically, the large-scale specialized domain foundation model (LSDM) is first introduced to provide essential specialized domain knowledge. Using lightweight neural networks, we transfer this knowledge into domain biases, which control both the visual and language branches to obtain domain-adaptive prompts in a directly incorporating manner. Simultaneously, to overcome the existing overfitting challenge, we propose a novel noisy-adding strategy, without extra trainable parameters, to help the model escape the suboptimal solution in a global domain oscillation manner. Experimental results show our method achieves state-of-the-art performance in specialized domain image recognition datasets. Our code is available at https://anonymous.4open.science/r/DCPL-8588.
</details>
<details>
<summary>摘要</summary>
大型预训 vision-language模型，如CLIP，在不同任务中表现出了惊人的通用能力，但将这些模型适应特殊领域，如遥感图像（RSIs）、医疗图像等，仍然是一个未探索的和挑战性的领域。现有的提问学习方法通常缺乏领域意识或领域传输机制，导致特殊图像在自然图像模式中的误 interpret，从而影响表现。为解决这个困难，我们提出了领域控制的提问学习（DCPL）方法。具体来说，我们首先引入大规模特殊领域基础模型（LSDM），以提供特殊领域知识的基础。使用轻量级神经网络，我们将这些知识转移到领域偏好，以控制视觉和语言 Zweige，从而获得适应特殊领域的提问。同时，为了解决现有的过拟合挑战，我们提出了一种新的噪音添加策略，不需要额外的可训练参数，以帮助模型脱离低效解决方案。实验结果表明，我们的方法在特殊领域图像识别数据集中实现了状态略 луч的表现。我们的代码可以在https://anonymous.4open.science/r/DCPL-8588中找到。
</details></li>
</ul>
<hr>
<h2 id="Pixel-Inconsistency-Modeling-for-Image-Manipulation-Localization"><a href="#Pixel-Inconsistency-Modeling-for-Image-Manipulation-Localization" class="headerlink" title="Pixel-Inconsistency Modeling for Image Manipulation Localization"></a>Pixel-Inconsistency Modeling for Image Manipulation Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00234">http://arxiv.org/abs/2310.00234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenqi Kong, Anwei Luo, Shiqi Wang, Haoliang Li, Anderson Rocha, Alex C. Kot</li>
<li>for: 本研究旨在提高图像修饰检测的通用性和Robustness，以便更好地识别和 lokalisir forgery。</li>
<li>methods: 本文提出了一种基于自注意力的涂抹检测模型，通过分析图像中的像素不一致痕迹来检测修饰。此外，本文还提出了一种新的学习准备模块（LWM），用于将全像和局部像素相关性流合并到一起，从而提高检测性能。</li>
<li>results: 实验结果表明，本文提出的方法可以成功检测图像修饰，并且在不同的数据集和扰动图像上表现出优秀的通用性和Robustness。<details>
<summary>Abstract</summary>
Digital image forensics plays a crucial role in image authentication and manipulation localization. Despite the progress powered by deep neural networks, existing forgery localization methodologies exhibit limitations when deployed to unseen datasets and perturbed images (i.e., lack of generalization and robustness to real-world applications). To circumvent these problems and aid image integrity, this paper presents a generalized and robust manipulation localization model through the analysis of pixel inconsistency artifacts. The rationale is grounded on the observation that most image signal processors (ISP) involve the demosaicing process, which introduces pixel correlations in pristine images. Moreover, manipulating operations, including splicing, copy-move, and inpainting, directly affect such pixel regularity. We, therefore, first split the input image into several blocks and design masked self-attention mechanisms to model the global pixel dependency in input images. Simultaneously, we optimize another local pixel dependency stream to mine local manipulation clues within input forgery images. In addition, we design novel Learning-to-Weight Modules (LWM) to combine features from the two streams, thereby enhancing the final forgery localization performance. To improve the training process, we propose a novel Pixel-Inconsistency Data Augmentation (PIDA) strategy, driving the model to focus on capturing inherent pixel-level artifacts instead of mining semantic forgery traces. This work establishes a comprehensive benchmark integrating 15 representative detection models across 12 datasets. Extensive experiments show that our method successfully extracts inherent pixel-inconsistency forgery fingerprints and achieve state-of-the-art generalization and robustness performances in image manipulation localization.
</details>
<details>
<summary>摘要</summary>
“数字图像科学在图像认证和修改地址中扮演着关键角色。尽管深度神经网络的进步，现有的伪造地址方法在未见数据集和压缩图像上展示了局限性和不可靠性。为了缓解这些问题并帮助图像完整性，本文提出了一种通用和Robust的伪造地址模型，基于像素不一致痕迹的分析。我们认为大多数图像信号处理器（ISP）都包含排除过程，这会在原始图像中引入像素相关性。此外，操作包括拼接、复制、填充等，都会直接影响这种像素规律。因此，我们将输入图像分成多个块，并设计了带有mask的自注意力机制，以模型输入图像的全球像素依赖关系。同时，我们优化了另一个本地像素依赖流，以挖掘输入伪造图像中的本地伪造线索。此外，我们设计了一种新的学习加权模块（LWM），以将两条流合并，从而提高最终伪造地址性能。为了改进训练过程，我们提出了一种新的像素不一致数据增强策略（PIDA），使模型更加专注于捕捉内置像素级别的痕迹，而不是挖掘 semantic伪造迹象。这种工作建立了12个数据集上15种表示性检测模型的通用 benchmark。广泛的实验表明，我们的方法可以成功捕捉内置像素不一致伪造指纹，并在图像伪造地址方面实现了状态 искусственный intelligence的通用和Robust性表现。”
</details></li>
</ul>
<hr>
<h2 id="Scaling-for-Training-Time-and-Post-hoc-Out-of-distribution-Detection-Enhancement"><a href="#Scaling-for-Training-Time-and-Post-hoc-Out-of-distribution-Detection-Enhancement" class="headerlink" title="Scaling for Training Time and Post-hoc Out-of-distribution Detection Enhancement"></a>Scaling for Training Time and Post-hoc Out-of-distribution Detection Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00227">http://arxiv.org/abs/2310.00227</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kai422/scale">https://github.com/kai422/scale</a></li>
<li>paper_authors: Kai Xu, Rongyu Chen, Gianni Franchi, Angela Yao</li>
<li>for: This paper focuses on the task of out-of-distribution (OOD) detection in deep learning systems, specifically on the recent state-of-the-art method of activation shaping (ASH).</li>
<li>methods: The paper proposes two novel methods for OOD detection: 1) SCALE, a post-hoc network enhancement method that achieves state-of-the-art OOD detection performance without compromising in-distribution (ID) accuracy, and 2) Intermediate Tensor SHaping (ISH), a lightweight method for training time OOD detection enhancement.</li>
<li>results: The paper reports AUROC scores of +1.85% for near-OOD and +0.74% for far-OOD datasets on the OpenOOD v1.5 ImageNet-1K benchmark, demonstrating the effectiveness of the proposed methods for OOD detection.Here is the information in Simplified Chinese text:</li>
<li>for: 这篇论文关注深度学习系统中的 OUT-OF-DISTRIBUTION（OOD）检测任务，具体来说是最新的 activation shaping（ASH）方法。</li>
<li>methods: 论文提出了两种新的 OOD 检测方法：1） SCALE，一种后置网络增强方法，可以在保持 ID 准确率的情况下提高 OOD 检测性能，2） Intermediate Tensor SHaping（ISH），一种轻量级的训练时间 OOD 检测增强方法。</li>
<li>results: 论文报告了 OpenOOD v1.5 ImageNet-1K 测试集上的 AUROC 分数，分别为 +1.85% 和 +0.74%，这表明提出的方法对 OOD 检测具有效果。<details>
<summary>Abstract</summary>
The capacity of a modern deep learning system to determine if a sample falls within its realm of knowledge is fundamental and important. In this paper, we offer insights and analyses of recent state-of-the-art out-of-distribution (OOD) detection methods - extremely simple activation shaping (ASH). We demonstrate that activation pruning has a detrimental effect on OOD detection, while activation scaling enhances it. Moreover, we propose SCALE, a simple yet effective post-hoc network enhancement method for OOD detection, which attains state-of-the-art OOD detection performance without compromising in-distribution (ID) accuracy. By integrating scaling concepts into the training process to capture a sample's ID characteristics, we propose Intermediate Tensor SHaping (ISH), a lightweight method for training time OOD detection enhancement. We achieve AUROC scores of +1.85\% for near-OOD and +0.74\% for far-OOD datasets on the OpenOOD v1.5 ImageNet-1K benchmark. Our code and models are available at https://github.com/kai422/SCALE.
</details>
<details>
<summary>摘要</summary>
现代深度学习系统确定样本是否属于其知识范围是基本重要的。在这篇论文中，我们提供了近期状态艺术的out-of-distribution（OOD）检测方法的深入分析和见解，包括极简的活动形状（ASH）。我们表明了活动剪除对OOD检测有负面影响，而活动缩放则有利于其。此外，我们提议SCALE，一种简单又有效的后期网络增强方法，以实现OOD检测性能的状元。通过将扩展概念 integrate到训练过程中，以捕捉样本的ID特征，我们提议Intermediate Tensor SHaping（ISH），一种轻量级的训练时OOD检测增强方法。我们在OpenOOD v1.5 ImageNet-1K测试集上达到了AUROC分数+1.85%的近OOD数据集和+0.74%的远OOD数据集。我们的代码和模型可以在https://github.com/kai422/SCALE上获取。
</details></li>
</ul>
<hr>
<h2 id="LSOR-Longitudinally-Consistent-Self-Organized-Representation-Learning"><a href="#LSOR-Longitudinally-Consistent-Self-Organized-Representation-Learning" class="headerlink" title="LSOR: Longitudinally-Consistent Self-Organized Representation Learning"></a>LSOR: Longitudinally-Consistent Self-Organized Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00213">http://arxiv.org/abs/2310.00213</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ouyangjiahong/longitudinal-som-single-modality">https://github.com/ouyangjiahong/longitudinal-som-single-modality</a></li>
<li>paper_authors: Jiahong Ouyang, Qingyu Zhao, Ehsan Adeli, Wei Peng, Greg Zaharchuk, Kilian M. Pohl</li>
<li>for: 这个论文的目的是提出一种基于单modal MR 的自适应SOM方法，以提高深度学习模型在长itudinal MR 上的可读性。</li>
<li>methods: 该方法使用了自适应SOM，通过将高维的干扰空间分成多个集群，并将每个集群映射到一个离散的（通常是2D）网格上，以保持高维关系 between集群。</li>
<li>results: 该方法可以在长itudinal MR 上生成一个可读的干扰空间，并且可以在不同的诊断任务上达到或超过当前的状态艺 Representatives的性能。code available at <a target="_blank" rel="noopener" href="https://github.com/ouyangjiahong/longitudinal-som-single-modality%E3%80%82">https://github.com/ouyangjiahong/longitudinal-som-single-modality。</a><details>
<summary>Abstract</summary>
Interpretability is a key issue when applying deep learning models to longitudinal brain MRIs. One way to address this issue is by visualizing the high-dimensional latent spaces generated by deep learning via self-organizing maps (SOM). SOM separates the latent space into clusters and then maps the cluster centers to a discrete (typically 2D) grid preserving the high-dimensional relationship between clusters. However, learning SOM in a high-dimensional latent space tends to be unstable, especially in a self-supervision setting. Furthermore, the learned SOM grid does not necessarily capture clinically interesting information, such as brain age. To resolve these issues, we propose the first self-supervised SOM approach that derives a high-dimensional, interpretable representation stratified by brain age solely based on longitudinal brain MRIs (i.e., without demographic or cognitive information). Called Longitudinally-consistent Self-Organized Representation learning (LSOR), the method is stable during training as it relies on soft clustering (vs. the hard cluster assignments used by existing SOM). Furthermore, our approach generates a latent space stratified according to brain age by aligning trajectories inferred from longitudinal MRIs to the reference vector associated with the corresponding SOM cluster. When applied to longitudinal MRIs of the Alzheimer's Disease Neuroimaging Initiative (ADNI, N=632), LSOR generates an interpretable latent space and achieves comparable or higher accuracy than the state-of-the-art representations with respect to the downstream tasks of classification (static vs. progressive mild cognitive impairment) and regression (determining ADAS-Cog score of all subjects). The code is available at https://github.com/ouyangjiahong/longitudinal-som-single-modality.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate into Simplified Chinese��sterreichische Nationalbibliothek, Vienna, Austria� entitled Interpretability in Deep Learning for Longitudinal Brain MRIs, one challenge is the high dimensionality of the latent spaces generated by deep learning models. One approach to address this challenge is to visualize the latent spaces using self-organizing maps (SOM). However, learning SOM in high-dimensional latent spaces can be unstable, especially in self-supervised settings. Moreover, the learned SOM grid may not capture clinically meaningful information such as brain age.� To address these issues, we propose the first self-supervised SOM approach that derives a high-dimensional, interpretable representation stratified by brain age based solely on longitudinal brain MRIs. Called Longitudinally-consistent Self-Organized Representation learning (LSOR), the method is stable during training and relies on soft clustering instead of hard cluster assignments. Furthermore, our approach aligns trajectories inferred from longitudinal MRIs to the reference vector associated with the corresponding SOM cluster, generating a latent space stratified according to brain age.� When applied to longitudinal MRIs of the Alzheimer's Disease Neuroimaging Initiative (ADNI, N=632), LSOR generates an interpretable latent space and achieves comparable or higher accuracy than state-of-the-art representations with respect to downstream tasks such as classification (static vs. progressive mild cognitive impairment) and regression (determining ADAS-Cog score of all subjects). The code is available at https://github.com/ouyangjiahong/longitudinal-som-single-modality.� In summary, LSOR is a stable and interpretable deep learning method for longitudinal brain MRIs that captures brain age information and achieves high accuracy in downstream tasks.
</details></li>
</ul>
<hr>
<h2 id="DeformUX-Net-Exploring-a-3D-Foundation-Backbone-for-Medical-Image-Segmentation-with-Depthwise-Deformable-Convolution"><a href="#DeformUX-Net-Exploring-a-3D-Foundation-Backbone-for-Medical-Image-Segmentation-with-Depthwise-Deformable-Convolution" class="headerlink" title="DeformUX-Net: Exploring a 3D Foundation Backbone for Medical Image Segmentation with Depthwise Deformable Convolution"></a>DeformUX-Net: Exploring a 3D Foundation Backbone for Medical Image Segmentation with Depthwise Deformable Convolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00199">http://arxiv.org/abs/2310.00199</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/masilab/deform-uxnet">https://github.com/masilab/deform-uxnet</a></li>
<li>paper_authors: Ho Hin Lee, Quan Liu, Qi Yang, Xin Yu, Shunxing Bao, Yuankai Huo, Bennett A. Landman<br>for:* The paper is focused on improving medical image segmentation using 3D ViTs and deformable convolution.methods:* The proposed model, 3D DeformUX-Net, combines long-range dependency, adaptive spatial aggregation, and computational efficiency by revisiting volumetric deformable convolution in a depth-wise setting.* The model also includes a parallel branch for generating deformable tri-planar offsets, which provides adaptive spatial aggregation across all channels.results:* The proposed model consistently outperforms existing state-of-the-art ViTs and large kernel convolution models across four challenging public datasets, achieving better segmentation results in terms of mean Dice.<details>
<summary>Abstract</summary>
The application of 3D ViTs to medical image segmentation has seen remarkable strides, somewhat overshadowing the budding advancements in Convolutional Neural Network (CNN)-based models. Large kernel depthwise convolution has emerged as a promising technique, showcasing capabilities akin to hierarchical transformers and facilitating an expansive effective receptive field (ERF) vital for dense predictions. Despite this, existing core operators, ranging from global-local attention to large kernel convolution, exhibit inherent trade-offs and limitations (e.g., global-local range trade-off, aggregating attentional features). We hypothesize that deformable convolution can be an exploratory alternative to combine all advantages from the previous operators, providing long-range dependency, adaptive spatial aggregation and computational efficiency as a foundation backbone. In this work, we introduce 3D DeformUX-Net, a pioneering volumetric CNN model that adeptly navigates the shortcomings traditionally associated with ViTs and large kernel convolution. Specifically, we revisit volumetric deformable convolution in depth-wise setting to adapt long-range dependency with computational efficiency. Inspired by the concepts of structural re-parameterization for convolution kernel weights, we further generate the deformable tri-planar offsets by adapting a parallel branch (starting from $1\times1\times1$ convolution), providing adaptive spatial aggregation across all channels. Our empirical evaluations reveal that the 3D DeformUX-Net consistently outperforms existing state-of-the-art ViTs and large kernel convolution models across four challenging public datasets, spanning various scales from organs (KiTS: 0.680 to 0.720, MSD Pancreas: 0.676 to 0.717, AMOS: 0.871 to 0.902) to vessels (e.g., MSD hepatic vessels: 0.635 to 0.671) in mean Dice.
</details>
<details>
<summary>摘要</summary>
三维ViT的应用在医学图像分割领域已经取得了非常出色的进步，一些超越了增强型神经网络（CNN）模型的发展。大核心深度卷积技术已经出现了可能，与层次转换器类似，并且提供了宽泛的有效接受场（ERF），这些都是为紧密预测而必要的。然而，现有的核心运算符，从全球local注意力到大核心卷积，都存在着内在的负担和限制（例如全球local范围负担）。我们 hypothesize  dass deformable convolution可以是一种探索性的代替方案，结合所有的优点，提供长茨征dependency、适应空间聚合和计算效率作为基础核心。在这种工作中，我们引入了3D DeformUX-Net，一种在深度缩放设置下的三维弹性 convolution 模型，能够灵活地导航传统上与ViTs和大核心卷积相关的缺陷。具体来说，我们在深度缩放设置下对卷积核心进行了修改，以适应长茨征dependency，并且通过缩放后的权重映射来实现适应空间聚合。我们的实验证明了3D DeformUX-Net在四个公共数据集上（包括 KiTS：0.680-0.720、MSD Pancreas：0.676-0.717、AMOS：0.871-0.902）表现出了与现有的状态开头的 ViTs 和大核心卷积模型相对的稳定性和精度。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/30/cs.CV_2023_09_30/" data-id="clpxp6c1u00ksee88dehd0oms" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/37/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/36/">36</a><a class="page-number" href="/page/37/">37</a><span class="page-number current">38</span><a class="page-number" href="/page/39/">39</a><a class="page-number" href="/page/40/">40</a><span class="space">&hellip;</span><a class="page-number" href="/page/98/">98</a><a class="extend next" rel="next" href="/page/39/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">67</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">82</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">147</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
