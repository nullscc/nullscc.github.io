
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/37/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.LG_2023_10_02" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/02/cs.LG_2023_10_02/" class="article-date">
  <time datetime="2023-10-02T10:00:00.000Z" itemprop="datePublished">2023-10-02</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/02/cs.LG_2023_10_02/">cs.LG - 2023-10-02</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Transformers-are-efficient-hierarchical-chemical-graph-learners"><a href="#Transformers-are-efficient-hierarchical-chemical-graph-learners" class="headerlink" title="Transformers are efficient hierarchical chemical graph learners"></a>Transformers are efficient hierarchical chemical graph learners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01704">http://arxiv.org/abs/2310.01704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihan Pengmei, Zimu Li, Chih-chan Tien, Risi Kondor, Aaron R. Dinner</li>
<li>for: 这篇论文是为了提出一种基于自然语言处理的图表示学习方法，即 SubFormer，以解决现代图 transformer 中节点或边视为分立的做法所导致的计算挑战。</li>
<li>methods: SubFormer 使用了一种消息传递机制来聚合信息，从而减少了 tokens 的数量并提高了长距离交互的学习。</li>
<li>results: 作者在使用 SubFormer 进行化学结构预测任务上达到了与当前状态OF-the-art 图 transformer 的竞争水平，并且在消耗了许多 fewer 计算资源的情况下。 具体来说，训练时间在consumer-grade graphics card 上只需要几分钟。此外，作者还进行了对 attention weights 的解释，并证明 SubFormer 具有限制过拟合和过压缩的特点。<details>
<summary>Abstract</summary>
Transformers, adapted from natural language processing, are emerging as a leading approach for graph representation learning. Contemporary graph transformers often treat nodes or edges as separate tokens. This approach leads to computational challenges for even moderately-sized graphs due to the quadratic scaling of self-attention complexity with token count. In this paper, we introduce SubFormer, a graph transformer that operates on subgraphs that aggregate information by a message-passing mechanism. This approach reduces the number of tokens and enhances learning long-range interactions. We demonstrate SubFormer on benchmarks for predicting molecular properties from chemical structures and show that it is competitive with state-of-the-art graph transformers at a fraction of the computational cost, with training times on the order of minutes on a consumer-grade graphics card. We interpret the attention weights in terms of chemical structures. We show that SubFormer exhibits limited over-smoothing and avoids over-squashing, which is prevalent in traditional graph neural networks.
</details>
<details>
<summary>摘要</summary>
transformers，起源于自然语言处理，在图表示学习中emerging为领先方法。当前的图transformers通常将节点或边视为分立的token。这种方法会导致对几乎任何大小的图进行计算而带来挑战，因为自我注意复杂性与token数平方成正比。在这篇论文中，我们介绍SubFormer，一种基于消息传递机制的图transformer，可以在subgraph上进行图表示学习。这种方法可以减少token数量，提高了长距离交互的学习。我们在化学结构预测 tasks上使用SubFormer，并证明它与当前的图transformers在计算成本上一样竞争，但是训练时间只需几分钟，可以在consumer级别的图形处理卡上完成。我们还对SubFormer的注意力权重进行了解释，并证明它在化学结构中具有有限的过滤和压缩现象，这些现象在传统的图神经网络中很普遍。
</details></li>
</ul>
<hr>
<h2 id="Robustifying-State-space-Models-for-Long-Sequences-via-Approximate-Diagonalization"><a href="#Robustifying-State-space-Models-for-Long-Sequences-via-Approximate-Diagonalization" class="headerlink" title="Robustifying State-space Models for Long Sequences via Approximate Diagonalization"></a>Robustifying State-space Models for Long Sequences via Approximate Diagonalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01698">http://arxiv.org/abs/2310.01698</a></li>
<li>repo_url: None</li>
<li>paper_authors: Annan Yu, Arnur Nigmetov, Dmitriy Morozov, Michael W. Mahoney, N. Benjamin Erichson</li>
<li>for: 本研究旨在提出一种泛化的”强制后向稳定”（PTD）方法，用于解决机器学习中的非正常矩阵对称化问题。</li>
<li>methods: 我们提出了一种基于pseudospectral理论的非正常矩阵对称化方法，并在S4-PTD和S5-PTD模型中应用了这种方法。</li>
<li>results: 我们通过对不同初始化方案的传输函数的分析，证明了S4-PTD&#x2F;S5-PTD初始化对HiPPO框架强有吸引力，而S4D&#x2F;S5初始化只能获得弱连续性。此外，我们的S5-PTD模型在Long-Range Arenabenchmark上得到了87.6%的准确率，表明PTD方法可以提高深度学习模型的准确率。<details>
<summary>Abstract</summary>
State-space models (SSMs) have recently emerged as a framework for learning long-range sequence tasks. An example is the structured state-space sequence (S4) layer, which uses the diagonal-plus-low-rank structure of the HiPPO initialization framework. However, the complicated structure of the S4 layer poses challenges; and, in an effort to address these challenges, models such as S4D and S5 have considered a purely diagonal structure. This choice simplifies the implementation, improves computational efficiency, and allows channel communication. However, diagonalizing the HiPPO framework is itself an ill-posed problem. In this paper, we propose a general solution for this and related ill-posed diagonalization problems in machine learning. We introduce a generic, backward-stable "perturb-then-diagonalize" (PTD) methodology, which is based on the pseudospectral theory of non-normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs. Based on this, we introduce the S4-PTD and S5-PTD models. Through theoretical analysis of the transfer functions of different initialization schemes, we demonstrate that the S4-PTD/S5-PTD initialization strongly converges to the HiPPO framework, while the S4D/S5 initialization only achieves weak convergences. As a result, our new models show resilience to Fourier-mode noise-perturbed inputs, a crucial property not achieved by the S4D/S5 models. In addition to improved robustness, our S5-PTD model averages 87.6% accuracy on the Long-Range Arena benchmark, demonstrating that the PTD methodology helps to improve the accuracy of deep learning models.
</details>
<details>
<summary>摘要</summary>
状态空间模型（SSM）最近在长距离序列任务中得到应用。例如，结构化状态空间序列（S4）层使用了HiPPO初始化框架的对角线加低级结构。然而，S4层的复杂结构带来挑战，以至于模型如S4D和S5在解决这些挑战时考虑了纯对角结构。这种选择简化实现，提高计算效率，并允许通道通信。然而，对HiPPO框架的对角化本身是一个不定 пробле。在这篇文章中，我们提出了一种通用的解决方案，基于非正常算子的pseudospectral理论，并可以看作是SSM中非正常矩阵的 Approximate diagonalization。基于这，我们引入了S4-PTD和S5-PTD模型。通过对不同初始化方案的传输函数的分析，我们证明了S4-PTD/S5-PTD初始化强 converges to HiPPO框架，而S4D/S5初始化只有weak converges。因此，我们新的模型具有耐 Fourier-mode 噪声扰动输入的性能，而S4D/S5模型没有达到这种性能。此外，我们的S5-PTD模型在Long-Range Arena benchmark上的准确率为87.6%，表明PTD方法可以提高深度学习模型的准确率。
</details></li>
</ul>
<hr>
<h2 id="DANI-Fast-Diffusion-Aware-Network-Inference-with-Preserving-Topological-Structure-Property"><a href="#DANI-Fast-Diffusion-Aware-Network-Inference-with-Preserving-Topological-Structure-Property" class="headerlink" title="DANI: Fast Diffusion Aware Network Inference with Preserving Topological Structure Property"></a>DANI: Fast Diffusion Aware Network Inference with Preserving Topological Structure Property</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01696">http://arxiv.org/abs/2310.01696</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aryanahadinia/dani">https://github.com/aryanahadinia/dani</a></li>
<li>paper_authors: Maryam Ramezani, Aryan Ahadinia, Erfan Farhadi, Hamid R. Rabiee</li>
<li>for: 推断社交网络的底层结构</li>
<li>methods: 基于时序链reactivity Matrix和节点之间相似性的方法</li>
<li>results: 高精度和低运行时间，保持结构性特征，包括模块结构、度分布、连接分量、浸泡度和嵌入度Here’s the same information in English:</li>
<li>for: Inferring the underlying structure of social networks</li>
<li>methods: Based on the Markov transition matrix derived from time series cascades and node-node similarity from a structural perspective</li>
<li>results: High accuracy and low running time, preserving structural properties, including modular structure, degree distribution, connected components, density, and clustering coefficients.<details>
<summary>Abstract</summary>
The fast growth of social networks and their data access limitations in recent years has led to increasing difficulty in obtaining the complete topology of these networks. However, diffusion information over these networks is available, and many algorithms have been proposed to infer the underlying networks using this information. The previously proposed algorithms only focus on inferring more links and ignore preserving the critical topological characteristics of the underlying social networks. In this paper, we propose a novel method called DANI to infer the underlying network while preserving its structural properties. It is based on the Markov transition matrix derived from time series cascades, as well as the node-node similarity that can be observed in the cascade behavior from a structural point of view. In addition, the presented method has linear time complexity (increases linearly with the number of nodes, number of cascades, and square of the average length of cascades), and its distributed version in the MapReduce framework is also scalable. We applied the proposed approach to both real and synthetic networks. The experimental results showed that DANI has higher accuracy and lower run time while maintaining structural properties, including modular structure, degree distribution, connected components, density, and clustering coefficients, than well-known network inference methods.
</details>
<details>
<summary>摘要</summary>
“社交网络的快速增长和数据访问限制在最近几年中，导致了获取社交网络的完整拓扑结构的增加困难。然而，社交网络上的协议信息可以获取，许多算法已经被提出来使用这些信息推断社交网络的下面结构。但这些算法只注重推断更多的链接，忽略了保持社交网络的结构性特征。在这篇论文中，我们提出了一种新的方法 called DANI，可以在保持社交网络的结构性特征的情况下推断社交网络的下面结构。它基于时间序列冲击矩阵，以及从结构角度观察到的节点对节点相似性。此外，提出的方法的时间复杂度为线性时间复杂度（与节点数、冲击数、冲击链的平均长度平方成正比增长），其分布式版本在MapReduce框架中也可扩展。我们对真实网络和synthetic网络进行了实验，结果表明，DANI比较知名的网络推断方法更高精度且更快速，同时保持了社交网络的结构特征，包括模块结构、度分布、连接分布、密度和嵌入系数。”
</details></li>
</ul>
<hr>
<h2 id="Forecasting-Tropical-Cyclones-with-Cascaded-Diffusion-Models"><a href="#Forecasting-Tropical-Cyclones-with-Cascaded-Diffusion-Models" class="headerlink" title="Forecasting Tropical Cyclones with Cascaded Diffusion Models"></a>Forecasting Tropical Cyclones with Cascaded Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01690">http://arxiv.org/abs/2310.01690</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nathzi1505/forecast-diffmodels">https://github.com/nathzi1505/forecast-diffmodels</a></li>
<li>paper_authors: Pritthijit Nath, Pancham Shukla, César Quilodrán-Casas</li>
<li>for: 预测飓风轨迹和降水强度</li>
<li>methods: 利用扩散模型 Integrate 卫星成像、远程感知和大气数据，采用级联方法，包括预测、超分辨和降水模型，对51个飓风基区进行训练</li>
<li>results: 实验表明，级联模型的最终预测可以准确预测飓风轨迹和降水强度，SSIM和PSNR值分别高于0.5和20 dB，适用于高性能需求和财力有限的地区。Here’s the English version for reference:</li>
<li>for: Forecasting cyclone trajectories and precipitation patterns</li>
<li>methods: Leveraging diffusion models to integrate satellite imaging, remote sensing, and atmospheric data, using a cascaded approach that includes forecasting, super-resolution, and precipitation modeling, with training on a dataset of 51 cyclones from six major basins.</li>
<li>results: Experiments show that the final forecasts from the cascaded models can accurately predict cyclone trajectories and precipitation patterns up to a 36-hour rollout, with SSIM and PSNR values exceeding 0.5 and 20 dB, respectively, for all three tasks.<details>
<summary>Abstract</summary>
As cyclones become more intense due to climate change, the rise of AI-based modelling provides a more affordable and accessible approach compared to traditional methods based on mathematical models. This work leverages diffusion models to forecast cyclone trajectories and precipitation patterns by integrating satellite imaging, remote sensing, and atmospheric data, employing a cascaded approach that incorporates forecasting, super-resolution, and precipitation modelling, with training on a dataset of 51 cyclones from six major basins. Experiments demonstrate that the final forecasts from the cascaded models show accurate predictions up to a 36-hour rollout, with SSIM and PSNR values exceeding 0.5 and 20 dB, respectively, for all three tasks. This work also highlights the promising efficiency of AI methods such as diffusion models for high-performance needs, such as cyclone forecasting, while remaining computationally affordable, making them ideal for highly vulnerable regions with critical forecasting needs and financial limitations. Code accessible at \url{https://github.com/nathzi1505/forecast-diffmodels}.
</details>
<details>
<summary>摘要</summary>
随着气候变化，风暴的强度变得越来越高，AI模型提供了一种更加可靠和可 accessible的方法，相比传统基于数学模型的方法。这项工作利用扩散模型预测风暴轨迹和降水模式，并将卫星影像、远程感知和大气数据集成起来，采用层次结构的方法，包括预测、超分解和降水模型，并对51个风暴数据进行训练。实验表明，最终预测结果从扩散模型中得到了准确的预测结果，SSIM和PSNR值分别高于0.5和20 dB，对所有三个任务都有良好的预测性。此外，这项工作也指出了AI方法如扩散模型在高性能需求下的可行性，同时保持计算可持，使其成为有严重预测需求和财务限制的地区的理想选择。代码可以在 GitHub上获取：\url{https://github.com/nathzi1505/forecast-diffmodels}。
</details></li>
</ul>
<hr>
<h2 id="From-Stability-to-Chaos-Analyzing-Gradient-Descent-Dynamics-in-Quadratic-Regression"><a href="#From-Stability-to-Chaos-Analyzing-Gradient-Descent-Dynamics-in-Quadratic-Regression" class="headerlink" title="From Stability to Chaos: Analyzing Gradient Descent Dynamics in Quadratic Regression"></a>From Stability to Chaos: Analyzing Gradient Descent Dynamics in Quadratic Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01687">http://arxiv.org/abs/2310.01687</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuxing Chen, Krishnakumar Balasubramanian, Promit Ghosal, Bhavya Agrawalla</li>
<li>for:  investigate the dynamics of gradient descent using large-order constant step-sizes in quadratic regression models.</li>
<li>methods:  use a specific cubic map to encapsulate the dynamics, and conduct a fine-grained bifurcation analysis concerning the step-size parameter.</li>
<li>results:  identify five distinct training phases, including monotonic, catapult, periodic, chaotic, and divergent phases, and observe that performing an ergodic trajectory averaging stabilizes the test error in non-monotonic (and non-divergent) phases.<details>
<summary>Abstract</summary>
We conduct a comprehensive investigation into the dynamics of gradient descent using large-order constant step-sizes in the context of quadratic regression models. Within this framework, we reveal that the dynamics can be encapsulated by a specific cubic map, naturally parameterized by the step-size. Through a fine-grained bifurcation analysis concerning the step-size parameter, we delineate five distinct training phases: (1) monotonic, (2) catapult, (3) periodic, (4) chaotic, and (5) divergent, precisely demarcating the boundaries of each phase. As illustrations, we provide examples involving phase retrieval and two-layer neural networks employing quadratic activation functions and constant outer-layers, utilizing orthogonal training data. Our simulations indicate that these five phases also manifest with generic non-orthogonal data. We also empirically investigate the generalization performance when training in the various non-monotonic (and non-divergent) phases. In particular, we observe that performing an ergodic trajectory averaging stabilizes the test error in non-monotonic (and non-divergent) phases.
</details>
<details>
<summary>摘要</summary>
我们进行了对梯度下降的全面调查，使用大顺序常数步长在 quadratic 回归模型中。在这个框架下，我们发现这些动态可以通过特定的立方图表示，自然地归一化到步长参数。通过细腻的分岔分析，我们划分出五种不同的训练阶段：（1）升序、（2）炸彩、（3）周期、（4）危机和（5）分散，准确地界定每个阶段的边界。例如，我们提供了phaserecovery和两层神经网络，使用quadratic activation functions和常数外层，使用正交训练数据。我们的实验表明，这五个阶段也会在非正交数据上出现。此外，我们还employs empirical investigation of the generalization performance during training in the various non-monotonic（和非分散）阶段，并发现在非升序（和非分散）阶段执行随机轨迹平均可以稳定测试错误。
</details></li>
</ul>
<hr>
<h2 id="A-Framework-for-Interpretability-in-Machine-Learning-for-Medical-Imaging"><a href="#A-Framework-for-Interpretability-in-Machine-Learning-for-Medical-Imaging" class="headerlink" title="A Framework for Interpretability in Machine Learning for Medical Imaging"></a>A Framework for Interpretability in Machine Learning for Medical Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01685">http://arxiv.org/abs/2310.01685</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alan Q. Wang, Batuhan K. Karaman, Heejong Kim, Jacob Rosenthal, Rachit Saluja, Sean I. Young, Mert R. Sabuncu</li>
<li>for: 本研究的目的是提高医学影像分析中的机器学习模型可解性。</li>
<li>methods: 本研究使用了 formalize 可解性需求，通过理解医学影像分析和机器学习的实际任务和目标，identify 四个核心可解性元素：localization、视觉可识别、物理归因和透明度。</li>
<li>results: 本研究为医学影像分析领域的机器学习模型设计提供了实用和教学信息， inspiritedevelopers可以更深入理解可解性的目的和方法，并提出了未来可解性研究的方向。<details>
<summary>Abstract</summary>
Interpretability for machine learning models in medical imaging (MLMI) is an important direction of research. However, there is a general sense of murkiness in what interpretability means. Why does the need for interpretability in MLMI arise? What goals does one actually seek to address when interpretability is needed? To answer these questions, we identify a need to formalize the goals and elements of interpretability in MLMI. By reasoning about real-world tasks and goals common in both medical image analysis and its intersection with machine learning, we identify four core elements of interpretability: localization, visual recognizability, physical attribution, and transparency. Overall, this paper formalizes interpretability needs in the context of medical imaging, and our applied perspective clarifies concrete MLMI-specific goals and considerations in order to guide method design and improve real-world usage. Our goal is to provide practical and didactic information for model designers and practitioners, inspire developers of models in the medical imaging field to reason more deeply about what interpretability is achieving, and suggest future directions of interpretability research.
</details>
<details>
<summary>摘要</summary>
machine learning models in medical imaging (MLMI) 的可解释性是一个重要的研究方向。然而，有一个通用的感觉是，可解释性的含义不够明确。为什么需要在 MLMI 中的可解释性？我们需要解决什么问题时需要可解释性？为了回答这些问题，我们需要正式化 MLMI 中的目标和元素。通过考虑医学影像分析和机器学习的实际任务和目标，我们确定了 MLMI 中的四个核心元素：局部化、视觉可识别性、物理归因和透明度。总之，这篇论文将 MLMI 中的可解释性需求进行了形式化，并通过应用实际的视角，为模型设计者和实践者提供了实用和教学的信息，激励医学影像领域中的模型开发者更深入思考可解释性的目的，并建议将来的可解释性研究的未来方向。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Commutative-Width-and-Depth-Scaling-in-Deep-Neural-Networks"><a href="#Commutative-Width-and-Depth-Scaling-in-Deep-Neural-Networks" class="headerlink" title="Commutative Width and Depth Scaling in Deep Neural Networks"></a>Commutative Width and Depth Scaling in Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01683">http://arxiv.org/abs/2310.01683</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soufiane Hayou</li>
<li>for: 本研究是深度神经网络中 commutativity 的第二篇文章，旨在理解深度神经网络中宽度和深度在无穷大时的行为，并 eventually 确定 commutativity 是否成立。</li>
<li>methods: 本文使用新的证明技术，基于更加容易理解的杂event calculus，证明深度神经网络中 skip connections 的使用可以使 covariance 结构保持不变，无论宽度和深度在无穷大时如何取得。</li>
<li>results: 本文的结果表明，在深度神经网络中，采用 skip connections 的方法，可以使 covariance 结构保持不变，无论宽度和深度在无穷大时如何取得。这些结果扩展了先前的研究（参考 [55]），并有很多理论和实践上的意义，我们在文章中进行了详细的介绍和讨论。<details>
<summary>Abstract</summary>
This paper is the second in the series Commutative Scaling of Width and Depth (WD) about commutativity of infinite width and depth limits in deep neural networks. Our aim is to understand the behaviour of neural functions (functions that depend on a neural network model) as width and depth go to infinity (in some sense), and eventually identify settings under which commutativity holds, i.e. the neural function tends to the same limit no matter how width and depth limits are taken. In this paper, we formally introduce and define the commutativity framework, and discuss its implications on neural network design and scaling. We study commutativity for the neural covariance kernel which reflects how network layers separate data. Our findings extend previous results established in [55] by showing that taking the width and depth to infinity in a deep neural network with skip connections, when branches are suitably scaled to avoid exploding behaviour, result in the same covariance structure no matter how that limit is taken. This has a number of theoretical and practical implications that we discuss in the paper. The proof techniques in this paper are novel and rely on tools that are more accessible to readers who are not familiar with stochastic calculus (used in the proofs of WD(I))).
</details>
<details>
<summary>摘要</summary>
这份论文是WD系列的第二篇，探讨深度神经网络中宽度和深度范围的交换性。我们的目标是在宽度和深度范围趋向于无穷大时，理解神经函数（取决于神经网络模型的函数）的行为，并 eventually 确定在某些设置下，交换性存在，即神经函数往往趋向同一个边界，无论宽度和深度范围如何选择。在这篇论文中，我们正式引入和定义交换性框架，并讨论其对神经网络设计和缩放的影响。我们研究交换性在神经卷积核中，这个核心反映了神经网络层次如何分离数据。我们的发现超越了之前的结果（参考 [55]），显示在深度神经网络中具有跳跃连接的情况下，当分支适当缩放以避免暴跌行为时，宽度和深度范围趋向同一个covariance结构，无论如何选择这个边界。这有许多理论和实践意义，我们在论文中详细介绍。这份论文的证明技巧是新的，并且基于更加可 accessible 的概率Calculus（WD(I) 证明中使用的概率Calculus）。
</details></li>
</ul>
<hr>
<h2 id="Estimating-and-Implementing-Conventional-Fairness-Metrics-With-Probabilistic-Protected-Features"><a href="#Estimating-and-Implementing-Conventional-Fairness-Metrics-With-Probabilistic-Protected-Features" class="headerlink" title="Estimating and Implementing Conventional Fairness Metrics With Probabilistic Protected Features"></a>Estimating and Implementing Conventional Fairness Metrics With Probabilistic Protected Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01679">http://arxiv.org/abs/2310.01679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hadi Elzayn, Emily Black, Patrick Vossler, Nathanael Jo, Jacob Goldin, Daniel E. Ho</li>
<li>For: 本研究的目的是开发一种能够在有限protected attribute标签的情况下训练公平模型的方法。* Methods: 我们提出了一种使用可信度提升 surname geocoding 来获取保护属性标签的 probabilistic 估计，并使用这些估计来计算公平指标的上下限。另外，我们还提出了一种基于上下文信息的具体化方法，该方法利用模型预测结果和保护属性的 probabilistic 预测结果之间的关系来提供更紧的上限。* Results: 我们的实验表明，我们的测量方法可以与previous方法相比，在这些应用程序中紧跟true disparity的上限。此外，我们的训练方法可以减少disparity，同时与其他具有有限保护属性标签的公平优化方法相比，具有较小的公平精度质量trade-off。<details>
<summary>Abstract</summary>
The vast majority of techniques to train fair models require access to the protected attribute (e.g., race, gender), either at train time or in production. However, in many important applications this protected attribute is largely unavailable. In this paper, we develop methods for measuring and reducing fairness violations in a setting with limited access to protected attribute labels. Specifically, we assume access to protected attribute labels on a small subset of the dataset of interest, but only probabilistic estimates of protected attribute labels (e.g., via Bayesian Improved Surname Geocoding) for the rest of the dataset. With this setting in mind, we propose a method to estimate bounds on common fairness metrics for an existing model, as well as a method for training a model to limit fairness violations by solving a constrained non-convex optimization problem. Unlike similar existing approaches, our methods take advantage of contextual information -- specifically, the relationships between a model's predictions and the probabilistic prediction of protected attributes, given the true protected attribute, and vice versa -- to provide tighter bounds on the true disparity. We provide an empirical illustration of our methods using voting data. First, we show our measurement method can bound the true disparity up to 5.5x tighter than previous methods in these applications. Then, we demonstrate that our training technique effectively reduces disparity while incurring lesser fairness-accuracy trade-offs than other fair optimization methods with limited access to protected attributes.
</details>
<details>
<summary>摘要</summary>
大多数尝试培训公平模型都需要访问保护属性（例如种族、性别），ether during training or in production. However, in many important applications, this protected attribute is not readily available. In this paper, we develop methods for measuring and reducing fairness violations in a setting with limited access to protected attribute labels. Specifically, we assume access to protected attribute labels for a small subset of the dataset of interest, but only probabilistic estimates of protected attribute labels (e.g., via Bayesian Improved Surname Geocoding) for the rest of the dataset. With this setting in mind, we propose a method to estimate bounds on common fairness metrics for an existing model, as well as a method for training a model to limit fairness violations by solving a constrained non-convex optimization problem. Unlike similar existing approaches, our methods take advantage of contextual information -- specifically, the relationships between a model's predictions and the probabilistic prediction of protected attributes, given the true protected attribute, and vice versa -- to provide tighter bounds on the true disparity. We provide an empirical illustration of our methods using voting data. First, we show our measurement method can bound the true disparity up to 5.5 times tighter than previous methods in these applications. Then, we demonstrate that our training technique effectively reduces disparity while incurring lesser fairness-accuracy trade-offs than other fair optimization methods with limited access to protected attributes.
</details></li>
</ul>
<hr>
<h2 id="Score-dynamics-scaling-molecular-dynamics-with-picosecond-timesteps-via-conditional-diffusion-model"><a href="#Score-dynamics-scaling-molecular-dynamics-with-picosecond-timesteps-via-conditional-diffusion-model" class="headerlink" title="Score dynamics: scaling molecular dynamics with picosecond timesteps via conditional diffusion model"></a>Score dynamics: scaling molecular dynamics with picosecond timesteps via conditional diffusion model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01678">http://arxiv.org/abs/2310.01678</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim Hsu, Babak Sadigh, Vasily Bulatov, Fei Zhou</li>
<li>for: 这个论文是为了学习有效的演化运算符，从分子动力学实验中获得的分子动力学模型。</li>
<li>methods: 这个论文使用了分子动力学实验中的分子动力学模型，并使用了图神经网络来构建分子动力学系统的分数动力学模型。</li>
<li>results: 这个论文的实验结果表明，使用分数动力学模型可以在1~ps时间步长下进行高速的分子动力学模拟，并且可以与分子动力学实验的结果相符。<details>
<summary>Abstract</summary>
We propose score dynamics (SD), a general framework for learning effective evolution operators for atomistic as well as coarse-grained dynamics from molecular-dynamics (MD) simulations. SD is centered around scores, or derivatives of the transition log-probability with respect to the dynamical degrees of freedom. The latter play the same role as force fields in MD but are used in denoising diffusion probability models to generate discrete transitions of the dynamical variables in an SD timestep, which can be orders of magnitude larger than a typical MD timestep. In this work, we construct graph neural network based score dynamics models of realistic molecular systems that are evolved with 1~ps timesteps. We demonstrate the efficacy of score dynamics with case studies of alanine dipeptide and short alkanes in aqueous solution. Both equilibrium predictions derived from the stationary distributions of the conditional probability and kinetic predictions for the transition rates and transition paths are in good agreement with MD at about 8-18 fold wall-clock speedup. Open challenges and possible future remedies to improve score dynamics are also discussed.
</details>
<details>
<summary>摘要</summary>
我们提出了得分动力学（SD），一种泛化框架，可以从分子动力学（MD）仿真中学习有效的演化运算符。SD中心在于得分，即动力学变量的转移极 probabilistic 的导数。这些导数与力场在MD中扮演相同的角色，但是在排除噪声扩散概率模型中使用，以生成动态变量的精炼过程中的精炼步骤，这些步骤可以是MD步骤的数个数量级。在这种工作中，我们使用图 neural network 构建了真实分子系统的Score Dynamics 模型，这些模型在1~ps步骤中进行了演化。我们通过对 Alanine dipeptide 和尘埃烷在液态中的情况进行了 caso study，并证明了得分动力学的有效性。我们的方法可以与MD相比，提高了8-18倍的计时速度。我们还讨论了现有的挑战和可能的未来改进。
</details></li>
</ul>
<hr>
<h2 id="Locality-Aware-Graph-Rewiring-in-GNNs"><a href="#Locality-Aware-Graph-Rewiring-in-GNNs" class="headerlink" title="Locality-Aware Graph-Rewiring in GNNs"></a>Locality-Aware Graph-Rewiring in GNNs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01668">http://arxiv.org/abs/2310.01668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Federico Barbero, Ameya Velingker, Amin Saberi, Michael Bronstein, Francesco Di Giovanni</li>
<li>for: 本文旨在提高图像学习中的图结构学习模型（Graph Neural Networks，GNNs）的表现，通过修改图的连接方式来改善信息流动。</li>
<li>methods: 本文提出了三种重要的条件 для图重编组：减少过载、尊重图的本地特性和保持图的稀疏性。同时，本文还提出了一种新的重编组框架，通过地域性执行重编组操作来满足这三个条件。</li>
<li>results: 本文通过多个实验 validate 了新的重编组框架的有效性，并证明它可以与或大幅超过现有的重编组方法。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) are popular models for machine learning on graphs that typically follow the message-passing paradigm, whereby the feature of a node is updated recursively upon aggregating information over its neighbors. While exchanging messages over the input graph endows GNNs with a strong inductive bias, it can also make GNNs susceptible to over-squashing, thereby preventing them from capturing long-range interactions in the given graph. To rectify this issue, graph rewiring techniques have been proposed as a means of improving information flow by altering the graph connectivity. In this work, we identify three desiderata for graph-rewiring: (i) reduce over-squashing, (ii) respect the locality of the graph, and (iii) preserve the sparsity of the graph. We highlight fundamental trade-offs that occur between spatial and spectral rewiring techniques; while the former often satisfy (i) and (ii) but not (iii), the latter generally satisfy (i) and (iii) at the expense of (ii). We propose a novel rewiring framework that satisfies all of (i)--(iii) through a locality-aware sequence of rewiring operations. We then discuss a specific instance of such rewiring framework and validate its effectiveness on several real-world benchmarks, showing that it either matches or significantly outperforms existing rewiring approaches.
</details>
<details>
<summary>摘要</summary>
图神网络（GNN）是常用的机器学习模型，它通常遵循消息传递假设，其中节点的特征通过与邻居 nodes 的信息聚合来进行更新。在传递消息过程中，GNN 具有强 inductive bias，但是这也可能导致 GNN 不能捕捉图中远距离的交互。为了解决这个问题，人们提出了图重排技术，以改善信息流动。在这个工作中，我们提出了三个愿景 для图重排：1. 减少过度压缩：图重排应该减少 GNN 中节点之间信息的重复，以便更好地捕捉图中的交互。2. 尊重图的本地性：图重排应该尊重图的本地结构，避免对图的整体结构进行大规模的改变。3. 保持图的稀疏性：图重排应该保持图的稀疏性，避免对图的稀疏性进行大规模的破坏。我们指出了图重排技术之间的基本质量贝各，其中一般来说，空间重排技术可以减少过度压缩，但是通常不能尊重图的本地性和稀疏性。相反，spectral重排技术通常能够尊重图的本地性和稀疏性，但是通常不能减少过度压缩。我们提出了一种新的重排框架，该框架通过一系列本地化的重排操作来满足所有的愿景。我们then 讨论了这种重排框架的一个具体实现，并在多个实际 benchmark 上验证了其效果，显示它可以与或大于现有的重排方法相比。
</details></li>
</ul>
<hr>
<h2 id="Home-Electricity-Data-Generator-HEDGE-An-open-access-tool-for-the-generation-of-electric-vehicle-residential-demand-and-PV-generation-profiles"><a href="#Home-Electricity-Data-Generator-HEDGE-An-open-access-tool-for-the-generation-of-electric-vehicle-residential-demand-and-PV-generation-profiles" class="headerlink" title="Home Electricity Data Generator (HEDGE): An open-access tool for the generation of electric vehicle, residential demand, and PV generation profiles"></a>Home Electricity Data Generator (HEDGE): An open-access tool for the generation of electric vehicle, residential demand, and PV generation profiles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01661">http://arxiv.org/abs/2310.01661</a></li>
<li>repo_url: None</li>
<li>paper_authors: Flora Charbonnier, Thomas Morstyn, Malcolm McCulloch</li>
<li>for: 本研究开发了一个名为Home Electricity Data Generator（HEDGE）的开源工具，用于随机生成真实的住宅电力数据。</li>
<li>methods: 本研究使用了生成对抗网络（GANs）来训练生成真实的人工数据，并将其分为不同的行为群。</li>
<li>results: HEDGE可以填补现有数据库中的数据损失，并生成一些真实的住宅电力数据，包括太阳能发电、家用电力负载和电动车的消耗。这些数据可以用于研究住宅分布式能源资源的特性和协调。<details>
<summary>Abstract</summary>
In this paper, we present the Home Electricity Data Generator (HEDGE), an open-access tool for the random generation of realistic residential energy data. HEDGE generates realistic daily profiles of residential PV generation, household electric loads, and electric vehicle consumption and at-home availability, based on real-life UK datasets. The lack of usable data is a major hurdle for research on residential distributed energy resources characterisation and coordination, especially when using data-driven methods such as machine learning-based forecasting and reinforcement learning-based control. A key issue is that while large data banks are available, they are not in a usable format, and numerous subsequent days of data for a given single home are unavailable. We fill these gaps with the open-access HEDGE tool which generates data sequences of energy data for several days in a way that is consistent for single homes, both in terms of profile magnitude and behavioural clusters. From raw datasets, pre-processing steps are conducted, including filling in incomplete data sequences and clustering profiles into behaviour clusters. Generative adversarial networks (GANs) are then trained to generate realistic synthetic data representative of each behaviour groups consistent with real-life behavioural and physical patterns.
</details>
<details>
<summary>摘要</summary>
本文介绍了家庭电力数据生成器（HEDGE），一种开源工具，用于随机生成真实的家庭可再生能源数据。HEDGE生成了真实的每天家庭太阳能生成、家庭电力负荷和电动车消耗的日程表，基于英国实际数据。由于家庭分布式能源资源特征化和协调研究的数据缺乏问题，特别是使用数据驱动方法such as机器学习预测和强化学习控制时，HEDGE工具填补了这些缺失。HEDGE工具可以生成一系列的能源数据序列，包括家庭特有的能源资源特征和行为带。从原始数据开始，进行了预处理步骤，包括填充不完整的数据序列和对 Profile clustering。然后，使用生成敌方网络（GANs）训练生成真实的同一个行为群的合理的 sintetic数据，与实际行为和物理特征相符。
</details></li>
</ul>
<hr>
<h2 id="REMEDI-REinforcement-learning-driven-adaptive-MEtabolism-modeling-of-primary-sclerosing-cholangitis-DIsease-progression"><a href="#REMEDI-REinforcement-learning-driven-adaptive-MEtabolism-modeling-of-primary-sclerosing-cholangitis-DIsease-progression" class="headerlink" title="REMEDI: REinforcement learning-driven adaptive MEtabolism modeling of primary sclerosing cholangitis DIsease progression"></a>REMEDI: REinforcement learning-driven adaptive MEtabolism modeling of primary sclerosing cholangitis DIsease progression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01426">http://arxiv.org/abs/2310.01426</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chang Hu, Krishnakant V. Saboo, Ahmad H. Ali, Brian D. Juran, Konstantinos N. Lazaridis, Ravishankar K. Iyer</li>
<li>For: This paper aims to introduce a framework called REMEDI, which can assist in exploring treatments for Primary Sclerosing Cholangitis (PSC) by capturing bile acid dynamics and the body’s adaptive response during PSC progression.* Methods: REMEDI combines a differential equation (DE)-based mechanistic model of bile acid metabolism with reinforcement learning (RL) to emulate the body’s adaptations to PSC continuously. The framework leverages RL to approximate adaptations in PSC, treating homeostasis as a reward signal and adjusting the DE parameters as the corresponding actions.* Results: On real-world data, REMEDI generated bile acid dynamics and parameter adjustments consistent with published findings, and supported discussions in the literature that early administration of drugs that suppress bile acid synthesis may be effective in PSC treatment.Here’s the simplified Chinese text for the three key points:* For: 这篇论文目的是介绍一种名为REMEDI的框架，该框架可以帮助研究Primary Sclerosing Cholangitis (PSC) 的治疗方法，通过捕捉胆囊酸的动态和身体的适应反应来模拟PSC的进程。* Methods: REMEDI 结合了差分方程 (DE) 基本的机制模型和奖励学习 (RL) 来模拟身体在PSC 进程中的适应。框架通过RL来估算PSC 的适应，将身体的适应视为奖励信号，并将DE 参数的调整视为相应的行动。* Results: 在实际数据上，REMEDI 生成的胆囊酸动态和参数调整与已发表文献相符，并支持 literatura 中关于PSC 治疗的讨论，提出了抑制胆囊酸 synthesis 的药物可能在PSC 治疗中的早期行使有效性。<details>
<summary>Abstract</summary>
Primary sclerosing cholangitis (PSC) is a rare disease wherein altered bile acid metabolism contributes to sustained liver injury. This paper introduces REMEDI, a framework that captures bile acid dynamics and the body's adaptive response during PSC progression that can assist in exploring treatments. REMEDI merges a differential equation (DE)-based mechanistic model that describes bile acid metabolism with reinforcement learning (RL) to emulate the body's adaptations to PSC continuously. An objective of adaptation is to maintain homeostasis by regulating enzymes involved in bile acid metabolism. These enzymes correspond to the parameters of the DEs. REMEDI leverages RL to approximate adaptations in PSC, treating homeostasis as a reward signal and the adjustment of the DE parameters as the corresponding actions. On real-world data, REMEDI generated bile acid dynamics and parameter adjustments consistent with published findings. Also, our results support discussions in the literature that early administration of drugs that suppress bile acid synthesis may be effective in PSC treatment.
</details>
<details>
<summary>摘要</summary>
主要硬化性胆汁炎（PSC）是一种罕见的疾病，其中改变的胆汁酸代谢过程对持续liver injury做出了贡献。本文介绍了REMEDI框架，该框架旨在捕捉胆汁酸动力学和身体的适应应对PSC进程中的变化。REMEDI通过结合极限值方程（DE）基本机制模型和强化学习（RL）来模拟身体适应PSC的过程，并且通过RL来让身体在PSC进程中实现homeostasis。在这个过程中，RL通过调整DE参数来实现这一目标。在实际数据上，REMEDI生成的胆汁酸动力学和参数调整均与出版物中的发现一致。此外，我们的结果支持文献中的讨论， early administration of drugs that suppress bile acid synthesis may be effective in PSC treatment。
</details></li>
</ul>
<hr>
<h2 id="PolySketchFormer-Fast-Transformers-via-Sketches-for-Polynomial-Kernels"><a href="#PolySketchFormer-Fast-Transformers-via-Sketches-for-Polynomial-Kernels" class="headerlink" title="PolySketchFormer: Fast Transformers via Sketches for Polynomial Kernels"></a>PolySketchFormer: Fast Transformers via Sketches for Polynomial Kernels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01655">http://arxiv.org/abs/2310.01655</a></li>
<li>repo_url: None</li>
<li>paper_authors: Praneeth Kacham, Vahab Mirrokni, Peilin Zhong</li>
<li>for:  This paper aims to improve the efficiency of transformer architectures for language modeling by replacing softmax attention with a polynomial function and using polynomial sketching.</li>
<li>methods: The paper proposes a new attention mechanism called polynomial attention, which uses sketches for Polynomial Kernel from the randomized numerical linear algebra literature to approximate the attention output. The paper also introduces an efficient block-based algorithm to apply the causal mask to the attention matrix without explicitly realizing the $n \times n$ attention matrix.</li>
<li>results: The paper shows that the proposed polynomial attention mechanism leads to a significantly faster attention mechanism without assuming any sparse structure for the attention matrix, and the block-based algorithm gives significant speedups over the cumulative sum algorithm used by Performer. The paper also validates the design empirically by training language models with long context lengths and shows that the eval perplexities of the models are comparable to those of models trained with softmax attention, and the training times are significantly faster than FlashAttention.<details>
<summary>Abstract</summary>
The quadratic complexity of attention in transformer architectures remains a big bottleneck in scaling up large foundation models for long context. In fact, recent theoretical results show the hardness of approximating the output of softmax attention mechanism in sub-quadratic time assuming Strong Exponential Time Hypothesis. In this paper, we show how to break this theoretical barrier by replacing softmax with a polynomial function and polynomial sketching. In particular we show that sketches for Polynomial Kernel from the randomized numerical linear algebra literature can be used to approximate the polynomial attention which leads to a significantly faster attention mechanism without assuming any sparse structure for the attention matrix that has been done in many previous works.   In addition, we propose an efficient block-based algorithm that lets us apply the causal mask to the attention matrix without explicitly realizing the $n \times n$ attention matrix and compute the output of the polynomial attention mechanism in time linear in the context length. The block-based algorithm gives significant speedups over the \emph{cumulative sum} algorithm used by Performer to apply the causal mask to the attention matrix. These observations help us design \emph{PolySketchFormer}, a practical linear-time transformer architecture for language modeling with provable guarantees.   We validate our design empirically by training language models with long context lengths. We first show that the eval perplexities of our models are comparable to that of models trained with softmax attention. We then show that for large context lengths our training times are significantly faster than FlashAttention.
</details>
<details>
<summary>摘要</summary>
“对于对称架构中的注意力运算，这是一个很大的瓶颈，尤其是在扩展大型基础模型时。事实上，最近的理论成果显示，对于softmax注意力机制的输出应用权值矩阵在下ynomial时间内难以近似。在本文中，我们显示了如何突破这个理论障碍，通过取代softmax WITH polynomial函数和概率图 sketching。具体来说，我们显示了图 sketches for Polynomial Kernel from the randomized numerical linear algebra literature可以用来近似 polynomial attention，从而实现了较快的注意力运算，不需要假设注意力矩阵的罕见结构。此外，我们提出了一个高效的封页基于算法，可以将 causal mask 应用到注意力矩阵中，而不需要直接建立 $n \times n$ 的注意力矩阵。这个封页基于算法可以实现linear时间内 compute 出 polynomial attention 的输出。与Performer的 cumulative sum 算法相比，这个封页基于算法可以提供重要的几何速度增加。这些观察帮助我们设计了PolySketchFormer，一个实际的linear-time transformer架构，具有证明的保证。我们透过训练语言模型来验证我们的设计。我们首先显示了我们的模型在不同的文本长度下的eval perplexity是相似的，与使用 softmax attention 训练的模型相似。然后，我们显示了在大文本长度下，我们的训练时间是与FlashAttention相比的significantly faster。”
</details></li>
</ul>
<hr>
<h2 id="Fool-Your-Vision-and-Language-Model-With-Embarrassingly-Simple-Permutations"><a href="#Fool-Your-Vision-and-Language-Model-With-Embarrassingly-Simple-Permutations" class="headerlink" title="Fool Your (Vision and) Language Model With Embarrassingly Simple Permutations"></a>Fool Your (Vision and) Language Model With Embarrassingly Simple Permutations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01651">http://arxiv.org/abs/2310.01651</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ys-zong/foolyourvllms">https://github.com/ys-zong/foolyourvllms</a></li>
<li>paper_authors: Yongshuo Zong, Tingyang Yu, Bingchen Zhao, Ruchika Chavhan, Timothy Hospedales</li>
<li>for: 这篇论文旨在检测流行语言和视觉语言模型中的敏感性问题，即在多选问题回答中对答案集的排序影响。</li>
<li>methods: 作者使用了多种方法来检测模型的敏感性，包括对模型的输入和输出进行 permutation 操作，并对模型的性能进行分析。</li>
<li>results: 研究发现，流行的语言和视觉语言模型具有 permutation 敏感性问题，即对答案集的排序会导致模型的性能下降。这种敏感性存在于不同的模型大小和最新的语言和视觉语言模型中。<details>
<summary>Abstract</summary>
Large language and vision-language models are rapidly being deployed in practice thanks to their impressive capabilities in instruction following, in-context learning, and so on. This raises an urgent need to carefully analyse their robustness so that stakeholders can understand if and when such models are trustworthy enough to be relied upon in any given application. In this paper, we highlight a specific vulnerability in popular models, namely permutation sensitivity in multiple-choice question answering (MCQA). Specifically, we show empirically that popular models are vulnerable to adversarial permutation in answer sets for multiple-choice prompting, which is surprising as models should ideally be as invariant to prompt permutation as humans are. These vulnerabilities persist across various model sizes, and exist in very recent language and vision-language models. Code is available at \url{https://github.com/ys-zong/FoolyourVLLMs}.
</details>
<details>
<summary>摘要</summary>
大型语言和视觉语言模型在实践中迅速投入使用，其吸引力在 instrucion following、上下文学习等方面表现出色。然而，这也提高了对这些模型的robustness进行仔细分析的需求，以便各方可以了解这些模型在具体应用中是否可靠。在这篇论文中，我们强调了流行模型中的一个特点，即多项选择问题回答中的排序敏感性。我们通过实验证明，流行的模型对答案集的排序很敏感，这是人类应该是不敏感的。这些敏感性存在不同模型大小和最新语言和视觉语言模型中。可以在 \url{https://github.com/ys-zong/FoolyourVLLMs} 上获取代码。
</details></li>
</ul>
<hr>
<h2 id="Equivariant-Adaptation-of-Large-Pretrained-Models"><a href="#Equivariant-Adaptation-of-Large-Pretrained-Models" class="headerlink" title="Equivariant Adaptation of Large Pretrained Models"></a>Equivariant Adaptation of Large Pretrained Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01647">http://arxiv.org/abs/2310.01647</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arnab Kumar Mondal, Siba Smarak Panigrahi, Sékou-Oumar Kaba, Sai Rajeswar, Siamak Ravanbakhsh</li>
<li>for: 使得大型预训练模型具有更高的采样效率和更准确的预测结果，并且能够快速地在训练和推理过程中进行变换</li>
<li>methods: 使用简单的均值化网络将输入转换到均值形式，然后将其传递给未Constrained预测网络</li>
<li>results: 使用 dataset-dependent priors 来指导均值化函数，使得大型预训练模型能够具有更高的采样效率和更准确的预测结果，并且能够在某些情况下提高对于旋转等概率变换的Robustness。<details>
<summary>Abstract</summary>
Equivariant networks are specifically designed to ensure consistent behavior with respect to a set of input transformations, leading to higher sample efficiency and more accurate and robust predictions. However, redesigning each component of prevalent deep neural network architectures to achieve chosen equivariance is a difficult problem and can result in a computationally expensive network during both training and inference. A recently proposed alternative towards equivariance that removes the architectural constraints is to use a simple canonicalization network that transforms the input to a canonical form before feeding it to an unconstrained prediction network. We show here that this approach can effectively be used to make a large pretrained network equivariant. However, we observe that the produced canonical orientations can be misaligned with those of the training distribution, hindering performance. Using dataset-dependent priors to inform the canonicalization function, we are able to make large pretrained models equivariant while maintaining their performance. This significantly improves the robustness of these models to deterministic transformations of the data, such as rotations. We believe this equivariant adaptation of large pretrained models can help their domain-specific applications with known symmetry priors.
</details>
<details>
<summary>摘要</summary>
Equivariant 网络是专门为了保证输入变换的一致性，以提高样本效率和更准确的预测。然而，为了实现这种选择的一致性，现有的深度神经网络架构中的每个组件都需要重新设计，这会导致训练和推断过程中的计算成本增加。一种最近提出的代替方案是使用一个简单的标准化网络，将输入转换为一个标准形式，然后将其传递给一个未定型预测网络。我们在这里表明，这种方法可以有效地使大型预训练模型变换成一致的。然而，我们发现生产的标准方向可能与训练分布的方向不一致，这会降低性能。使用数据集依赖的先验来 inform 标准化函数，我们能够使大型预训练模型变换成一致，同时保持其性能。这会大幅提高这些模型对 deterministic 变换数据（如旋转）的Robustness。我们认为这种一致适应的大型预训练模型可以帮助它们在知道Symmetry先验的领域应用中提高性能。
</details></li>
</ul>
<hr>
<h2 id="Deep-Insights-into-Noisy-Pseudo-Labeling-on-Graph-Data"><a href="#Deep-Insights-into-Noisy-Pseudo-Labeling-on-Graph-Data" class="headerlink" title="Deep Insights into Noisy Pseudo Labeling on Graph Data"></a>Deep Insights into Noisy Pseudo Labeling on Graph Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01634">http://arxiv.org/abs/2310.01634</a></li>
<li>repo_url: None</li>
<li>paper_authors: Botao Wang, Jia Li, Yang Liu, Jiashun Cheng, Yu Rong, Wenjia Wang, Fugee Tsung</li>
<li>for: 本文旨在对 Pseudo Labeling (PL) 策略在图学习模型中的应用进行深入分析，并提出一种谨慎的PL方法来改进图学习过程。</li>
<li>methods: 本文使用错误分析方法对 PL 策略进行了深入分析，并提出了一种基于 confidence 和多视图一致性的PL方法。</li>
<li>results: 实验结果显示，提出的方法可以改善图学习过程，并在链接预测和节点分类任务上超过了其他 PL 策略。<details>
<summary>Abstract</summary>
Pseudo labeling (PL) is a wide-applied strategy to enlarge the labeled dataset by self-annotating the potential samples during the training process. Several works have shown that it can improve the graph learning model performance in general. However, we notice that the incorrect labels can be fatal to the graph training process. Inappropriate PL may result in the performance degrading, especially on graph data where the noise can propagate. Surprisingly, the corresponding error is seldom theoretically analyzed in the literature. In this paper, we aim to give deep insights of PL on graph learning models. We first present the error analysis of PL strategy by showing that the error is bounded by the confidence of PL threshold and consistency of multi-view prediction. Then, we theoretically illustrate the effect of PL on convergence property. Based on the analysis, we propose a cautious pseudo labeling methodology in which we pseudo label the samples with highest confidence and multi-view consistency. Finally, extensive experiments demonstrate that the proposed strategy improves graph learning process and outperforms other PL strategies on link prediction and node classification tasks.
</details>
<details>
<summary>摘要</summary>
假标签（PL）是一种广泛应用的策略，用于扩大标注数据集的训练过程中。许多研究表明，PL可以提高图学习模型的性能。然而，我们发现 incorrect labels 可能对图学习过程产生致命的影响。不当的 PL 可能导致性能下降，尤其是在图数据中， где 噪声可能进行卷积。 surprisingly，相关的错误分析在文献中 rarely 被 theoretically 探讨。在这篇论文中，我们希望给 PL 在图学习模型中提供深入的理解。我们首先给出 PL 策略的错误分析，并证明 error 是 PL 置信度和多视图预测一致性的 bound。然后，我们 theoretically 描述了 PL 对于融合性的影响。基于分析，我们提出了一种谨慎的假标签方法，其中我们假标签的样本是 confidence 最高和多视图一致的。最后，我们进行了广泛的实验，并证明了我们提出的策略可以改善图学习过程，并在链接预测和节点分类任务上超越其他 PL 策略。
</details></li>
</ul>
<hr>
<h2 id="Operator-Learning-Meets-Numerical-Analysis-Improving-Neural-Networks-through-Iterative-Methods"><a href="#Operator-Learning-Meets-Numerical-Analysis-Improving-Neural-Networks-through-Iterative-Methods" class="headerlink" title="Operator Learning Meets Numerical Analysis: Improving Neural Networks through Iterative Methods"></a>Operator Learning Meets Numerical Analysis: Improving Neural Networks through Iterative Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01618">http://arxiv.org/abs/2310.01618</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emanuele Zappala, Daniel Levine, Sizhuang He, Syed Rizvi, Sacha Levy, David van Dijk</li>
<li>for: 这篇论文目的是帮助深度学习模型更好地理解和设计，通过与数值分析的联系来提供理论基础。</li>
<li>methods: 这篇论文使用了迭代方法来描述神经网络，并提出了一种基于迭代法的神经网络架构。</li>
<li>results: 实验表明，迭代神经网络可以提高性能，而 alphaFold 和扩散模型等流行的架构也是基于迭代法。<details>
<summary>Abstract</summary>
Deep neural networks, despite their success in numerous applications, often function without established theoretical foundations. In this paper, we bridge this gap by drawing parallels between deep learning and classical numerical analysis. By framing neural networks as operators with fixed points representing desired solutions, we develop a theoretical framework grounded in iterative methods for operator equations. Under defined conditions, we present convergence proofs based on fixed point theory. We demonstrate that popular architectures, such as diffusion models and AlphaFold, inherently employ iterative operator learning. Empirical assessments highlight that performing iterations through network operators improves performance. We also introduce an iterative graph neural network, PIGN, that further demonstrates benefits of iterations. Our work aims to enhance the understanding of deep learning by merging insights from numerical analysis, potentially guiding the design of future networks with clearer theoretical underpinnings and improved performance.
</details>
<details>
<summary>摘要</summary>
深度神经网络，尽管在许多应用中取得了成功，但它们往往没有明确的理论基础。在这篇论文中，我们尝试填补这一漏洞，通过将神经网络视为有定点表示希望的解的运算器，开发了一个基于迭代方法的理论框架。在定义的条件下，我们提供了收敛证明基于定点理论。我们发现，流行的架构，如扩散模型和AlphaFold，实际上是使用迭代运算学习。Empirical assessments表明，通过网络运算器进行迭代可以提高性能。我们还介绍了一种迭代图 neural network，PIGN，它进一步证明了迭代的好处。我们的工作的目的是增强深度学习的理解，通过与数学分析的交互，可能导向未来的网络设计有 clearer theoretical underpinnings和提高性能。
</details></li>
</ul>
<hr>
<h2 id="Intractability-of-Learning-the-Discrete-Logarithm-with-Gradient-Based-Methods"><a href="#Intractability-of-Learning-the-Discrete-Logarithm-with-Gradient-Based-Methods" class="headerlink" title="Intractability of Learning the Discrete Logarithm with Gradient-Based Methods"></a>Intractability of Learning the Discrete Logarithm with Gradient-Based Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01611">http://arxiv.org/abs/2310.01611</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/armanbolatov/hardness_of_learning">https://github.com/armanbolatov/hardness_of_learning</a></li>
<li>paper_authors: Rustem Takhanov, Maxat Tezekbayev, Artur Pak, Arman Bolatov, Zhibek Kadyrsizova, Zhenisbek Assylbekov</li>
<li>for: 本研究探讨了使用梯度下降法学习质数logarithm的缺点。</li>
<li>methods: 本研究使用了梯度下降法和内存梯度下降法，并通过对特定的矩阵的spectral norm进行分析，证明了梯度下降法在学习质数logarithm的缺点上具有局限性。</li>
<li>results: 研究发现，使用梯度下降法学习质数logarithm的缺点时，梯度的强度会受到基数的影响，而不是logarithm的基数。此外，随着群体的规模增加，预测缺点的成功率会下降。<details>
<summary>Abstract</summary>
The discrete logarithm problem is a fundamental challenge in number theory with significant implications for cryptographic protocols. In this paper, we investigate the limitations of gradient-based methods for learning the parity bit of the discrete logarithm in finite cyclic groups of prime order. Our main result, supported by theoretical analysis and empirical verification, reveals the concentration of the gradient of the loss function around a fixed point, independent of the logarithm's base used. This concentration property leads to a restricted ability to learn the parity bit efficiently using gradient-based methods, irrespective of the complexity of the network architecture being trained.   Our proof relies on Boas-Bellman inequality in inner product spaces and it involves establishing approximate orthogonality of discrete logarithm's parity bit functions through the spectral norm of certain matrices. Empirical experiments using a neural network-based approach further verify the limitations of gradient-based learning, demonstrating the decreasing success rate in predicting the parity bit as the group order increases.
</details>
<details>
<summary>摘要</summary>
“离散logsarithm问题是数理学中的基本挑战，具有临� Notices significant implications for cryptographic protocols. In this paper, we investigate the limitations of gradient-based methods for learning the parity bit of the discrete logarithm in finite cyclic groups of prime order. Our main result, supported by theoretical analysis and empirical verification, reveals the concentration of the gradient of the loss function around a fixed point, independent of the logarithm's base used. This concentration property leads to a restricted ability to learn the parity bit efficiently using gradient-based methods, irrespective of the complexity of the network architecture being trained.  Our proof relies on Boas-Bellman inequality in inner product spaces and it involves establishing approximate orthogonality of discrete logarithm's parity bit functions through the spectral norm of certain matrices. Empirical experiments using a neural network-based approach further verify the limitations of gradient-based learning, demonstrating the decreasing success rate in predicting the parity bit as the group order increases.”
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Contextual-Bandits-Go-Kernelized"><a href="#Adversarial-Contextual-Bandits-Go-Kernelized" class="headerlink" title="Adversarial Contextual Bandits Go Kernelized"></a>Adversarial Contextual Bandits Go Kernelized</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01609">http://arxiv.org/abs/2310.01609</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gergely Neu, Julia Olkhovskaya, Sattar Vakili</li>
<li>for: 本研究探讨了在线学习中的反对抗敌性线性上下文随机带动问题的一种普适化问题，通过使用可重构kernel空间中的损失函数，以更加灵活地模型复杂的决策场景。</li>
<li>methods: 我们提出了一种计算效率高的算法，使用了一种新的乐观偏置估计器来估计损失函数，并实现了近似optimal的恐慌保证下界，对于多种 eigenvalue decay 假设。</li>
<li>results: 我们的算法在多种情况下都可以实现near-optimal的恐慌保证下界，包括对于polynomial eigendecay的情况下， regret 为 $\widetilde{O}(KT^{(\frac{1}{2}(1+\frac{1}{c})}$，其中 $T$ 是 Round 的数量， $K$ 是行动的数量。当 eigendecay 遵循 exponential 模式时，我们可以实现even tighter的 regret bound，即 $\widetilde{O}(\sqrt{T})$。这些率与所有已知的下界匹配，并且与已知的最佳上界匹配。<details>
<summary>Abstract</summary>
We study a generalization of the problem of online learning in adversarial linear contextual bandits by incorporating loss functions that belong to a reproducing kernel Hilbert space, which allows for a more flexible modeling of complex decision-making scenarios. We propose a computationally efficient algorithm that makes use of a new optimistically biased estimator for the loss functions and achieves near-optimal regret guarantees under a variety of eigenvalue decay assumptions made on the underlying kernel. Specifically, under the assumption of polynomial eigendecay with exponent $c>1$, the regret is $\widetilde{O}(KT^{\frac{1}{2}(1+\frac{1}{c})})$, where $T$ denotes the number of rounds and $K$ the number of actions. Furthermore, when the eigendecay follows an exponential pattern, we achieve an even tighter regret bound of $\widetilde{O}(\sqrt{T})$. These rates match the lower bounds in all special cases where lower bounds are known at all, and match the best known upper bounds available for the more well-studied stochastic counterpart of our problem.
</details>
<details>
<summary>摘要</summary>
我们研究一种扩展线性上下文ual bandit问题的在线学习泛化问题，该问题允许更加灵活地模型复杂的决策场景。我们提出了一种 computationally efficient 的算法，该算法使用了一种新的乐观偏向估计器来估计损失函数，并实现了近似optimal的 regret guarantee。 Specifically, under the assumption of 多项幂减少($c>1$)，我们的 regret是 $\widetilde{O}(KT^{\frac{1}{2}(1+\frac{1}{c})})$, where $T$ denotes the number of rounds and $K$ the number of actions. 另外，当欧几何减少 follows an exponential pattern 时，我们可以达到更紧的 regret bound of $\widetilde{O}(\sqrt{T})$. These rates match the lower bounds in all special cases where lower bounds are known at all, and match the best known upper bounds available for the more well-studied stochastic counterpart of our problem.
</details></li>
</ul>
<hr>
<h2 id="Pool-Based-Active-Learning-with-Proper-Topological-Regions"><a href="#Pool-Based-Active-Learning-with-Proper-Topological-Regions" class="headerlink" title="Pool-Based Active Learning with Proper Topological Regions"></a>Pool-Based Active Learning with Proper Topological Regions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01597">http://arxiv.org/abs/2310.01597</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Lies0zeta/PALPTR-">https://github.com/Lies0zeta/PALPTR-</a></li>
<li>paper_authors: Lies Hadjadj, Emilie Devijver, Remi Molinier, Massih-Reza Amini</li>
<li>for: 本研究提出了一种基于多类分类任务的池型活动学习策略，用于增强机器学习模型的性能。</li>
<li>methods: 本文提出了一种基于 topological data analysis（TDA）的Proper Topological Regions（PTR）方法，用于在池型活动学习中选择最有价值的无标注数据。</li>
<li>results: 实验表明，提出的方法在多种 benchmark 数据集上具有竞争力，并且与传统方法相比，可以更好地增强机器学习模型的性能。<details>
<summary>Abstract</summary>
Machine learning methods usually rely on large sample size to have good performance, while it is difficult to provide labeled set in many applications. Pool-based active learning methods are there to detect, among a set of unlabeled data, the ones that are the most relevant for the training. We propose in this paper a meta-approach for pool-based active learning strategies in the context of multi-class classification tasks based on Proper Topological Regions. PTR, based on topological data analysis (TDA), are relevant regions used to sample cold-start points or within the active learning scheme. The proposed method is illustrated empirically on various benchmark datasets, being competitive to the classical methods from the literature.
</details>
<details>
<summary>摘要</summary>
文本翻译成简化中文：机器学习方法通常需要大量数据来达到良好的性能，而在许多应用场景中提供标注数据却是困难的。基于池的活动学习方法可以探测一个未标注数据集中最相关的数据，以便在活动学习中训练。本文提出了一种基于多 класс分类任务的池基活动学习策略的meta方法，使用Proper Topological Regions（PTR）来检测 relevance。PTR基于数据 topological分析（TDA），可以在活动学习中作为冷开始点或在激活学习中选择数据。Empirical experiment表明，提议的方法与文献中的传统方法竞争。Note:* "Pool-based active learning" refers to the approach of using a pool of unlabeled data to select the most relevant instances for labeling.* "Proper Topological Regions" (PTR) are regions in the data space that are defined based on topological data analysis (TDA) and are used to detect relevance in the unlabeled data.* "Multi-class classification" refers to the task of classifying instances into one of multiple classes.
</details></li>
</ul>
<hr>
<h2 id="An-Investigation-of-Representation-and-Allocation-Harms-in-Contrastive-Learning"><a href="#An-Investigation-of-Representation-and-Allocation-Harms-in-Contrastive-Learning" class="headerlink" title="An Investigation of Representation and Allocation Harms in Contrastive Learning"></a>An Investigation of Representation and Allocation Harms in Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01583">http://arxiv.org/abs/2310.01583</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/smaityumich/cl-representation-harm">https://github.com/smaityumich/cl-representation-harm</a></li>
<li>paper_authors: Subha Maity, Mayank Agarwal, Mikhail Yurochkin, Yuekai Sun</li>
<li>for: 本研究探讨了自动学习中少数群体表现下降的原因，具体来说是对于自适应学习（SSL）中的对比学习（CL）方法的影响。</li>
<li>methods: 本研究使用了图像和文本数据集，以及相关的流行CL方法，来描述对少数群体的 represeting潜在危害。</li>
<li>results: 研究发现，CL方法在处理少数群体时容易导致对少数群体的表示潜在危害，并且这种危害对于下游分类任务有一定的影响。此外，研究还提供了一种理论解释，即在CLSetting中的概率链模型导致了表示潜在危害。<details>
<summary>Abstract</summary>
The effect of underrepresentation on the performance of minority groups is known to be a serious problem in supervised learning settings; however, it has been underexplored so far in the context of self-supervised learning (SSL). In this paper, we demonstrate that contrastive learning (CL), a popular variant of SSL, tends to collapse representations of minority groups with certain majority groups. We refer to this phenomenon as representation harm and demonstrate it on image and text datasets using the corresponding popular CL methods. Furthermore, our causal mediation analysis of allocation harm on a downstream classification task reveals that representation harm is partly responsible for it, thus emphasizing the importance of studying and mitigating representation harm. Finally, we provide a theoretical explanation for representation harm using a stochastic block model that leads to a representational neural collapse in a contrastive learning setting.
</details>
<details>
<summary>摘要</summary>
supervised learning 中少数群体的表现问题已经得到了广泛关注，但在自动学习（SSL）上还未得到足够的探讨。本文显示，在对比学习（CL）中，少数群体的表现会与主要群体的表现相归缩合。我们称此现象为表现害，并在图像和文本 dataset 上使用相关的流行 CL 方法进行证明。此外，我们通过对下游分类任务的干扰分析表明，表现害对 representation harm 具有一定的贡献，因此就是要研究和缓解表现害的重要性。最后，我们提供了一种 theoretically explain representation harm 的Stochastic block model，导致了对比学习设置中的表现害。
</details></li>
</ul>
<hr>
<h2 id="Contraction-Properties-of-the-Global-Workspace-Primitive"><a href="#Contraction-Properties-of-the-Global-Workspace-Primitive" class="headerlink" title="Contraction Properties of the Global Workspace Primitive"></a>Contraction Properties of the Global Workspace Primitive</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01571">http://arxiv.org/abs/2310.01571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michaela Ennis, Leo Kozachkov, Jean-Jacques Slotine</li>
<li>for: 这 paper 探讨了多个领域的循环神经网络（RNN）的重要研究领域，特别是 Kozachkov et al. 提出的可证实的RNN（RNNs）。</li>
<li>methods: 该 paper 通过理论和实验方式扩展了 RNNs 的稳定性条件，特别是对全球工作空间模块结构的研究。</li>
<li>results: 该 paper 通过实验成功地示出了 Global Workspace Sparse Combo Nets 具有少量可训练参数，并且在缺少个体子网络时具有更好的抗耗性。这些实验结果表明了我们的理论研究对于实现模块 RNN 的稳定性具有重要意义。<details>
<summary>Abstract</summary>
To push forward the important emerging research field surrounding multi-area recurrent neural networks (RNNs), we expand theoretically and empirically on the provably stable RNNs of RNNs introduced by Kozachkov et al. in "RNNs of RNNs: Recursive Construction of Stable Assemblies of Recurrent Neural Networks". We prove relaxed stability conditions for salient special cases of this architecture, most notably for a global workspace modular structure. We then demonstrate empirical success for Global Workspace Sparse Combo Nets with a small number of trainable parameters, not only through strong overall test performance but also greater resilience to removal of individual subnetworks. These empirical results for the global workspace inter-area topology are contingent on stability preservation, highlighting the relevance of our theoretical work for enabling modular RNN success. Further, by exploring sparsity in the connectivity structure between different subnetwork modules more broadly, we improve the state of the art performance for stable RNNs on benchmark sequence processing tasks, thus underscoring the general utility of specialized graph structures for multi-area RNNs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Causality-informed-Rapid-Post-hurricane-Building-Damage-Detection-in-Large-Scale-from-InSAR-Imagery"><a href="#Causality-informed-Rapid-Post-hurricane-Building-Damage-Detection-in-Large-Scale-from-InSAR-Imagery" class="headerlink" title="Causality-informed Rapid Post-hurricane Building Damage Detection in Large Scale from InSAR Imagery"></a>Causality-informed Rapid Post-hurricane Building Damage Detection in Large Scale from InSAR Imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01565">http://arxiv.org/abs/2310.01565</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenguang Wang, Yepeng Liu, Xiaojian Zhang, Xuechun Li, Vladimir Paramygin, Arthriya Subgranon, Peter Sheng, Xilei Zhao, Susu Xu<br>for:* 这 paper 是为了快速评估飓风引起的建筑物损害而写的。methods:* 这 paper 使用了 remote sensing 技术获取大规模的光学或 Interferometric Synthetic Aperture Radar（InSAR）图像数据，并使用了 causal Bayesian network 编码了风、洪水、建筑物损害、InSAR 图像之间的复杂 causal 关系。results:* 这 paper 的结果表明，使用这种方法可以快速 и准确地检测飓风引起的建筑物损害，并且可以避免了传统的手动检查方法所需的较长的处理时间。<details>
<summary>Abstract</summary>
Timely and accurate assessment of hurricane-induced building damage is crucial for effective post-hurricane response and recovery efforts. Recently, remote sensing technologies provide large-scale optical or Interferometric Synthetic Aperture Radar (InSAR) imagery data immediately after a disastrous event, which can be readily used to conduct rapid building damage assessment. Compared to optical satellite imageries, the Synthetic Aperture Radar can penetrate cloud cover and provide more complete spatial coverage of damaged zones in various weather conditions. However, these InSAR imageries often contain highly noisy and mixed signals induced by co-occurring or co-located building damage, flood, flood/wind-induced vegetation changes, as well as anthropogenic activities, making it challenging to extract accurate building damage information. In this paper, we introduced an approach for rapid post-hurricane building damage detection from InSAR imagery. This approach encoded complex causal dependencies among wind, flood, building damage, and InSAR imagery using a holistic causal Bayesian network. Based on the causal Bayesian network, we further jointly inferred the large-scale unobserved building damage by fusing the information from InSAR imagery with prior physical models of flood and wind, without the need for ground truth labels. Furthermore, we validated our estimation results in a real-world devastating hurricane -- the 2022 Hurricane Ian. We gathered and annotated building damage ground truth data in Lee County, Florida, and compared the introduced method's estimation results with the ground truth and benchmarked it against state-of-the-art models to assess the effectiveness of our proposed method. Results show that our method achieves rapid and accurate detection of building damage, with significantly reduced processing time compared to traditional manual inspection methods.
</details>
<details>
<summary>摘要</summary>
时刻和精准的飓风导致建筑物损坏评估是应急回应和恢复努力的关键。现在，远程感知技术提供大规模的光学或折射 Synthetic Aperture Radar（InSAR）图像数据，可以快速进行飓风后建筑物损坏评估。相比光学卫星图像，Synthetic Aperture Radar可以穿过云层和提供更完整的损坏区域各种天气情况下的损坏评估。然而，这些InSAR图像经常含有高度杂音和混合信号，由于同时发生或位于损坏区域的建筑物损坏、洪水、洪水/风吹落 vegetation 变化以及人类活动，使其� Extracting accurate building damage information challenging。在本文中，我们介绍了一种快速飓风后建筑物损坏检测方法，基于数学关系网络（Bayesian network）。这个方法利用这些建筑物损坏、洪水、风吹的复杂 causal 关系，通过组合 InSAR 图像资讯和预先建立的洪水和风吹 Physical 模型，无需地面实验标签。此外，我们还 validate 了我们的估计结果，在2022年飓风 Ian 中进行了真实世界的应用。我们在李县、佛罗里达聚集和标注建筑物损坏的实际数据，并与地面实验标签相比较，以评估我们提出的方法的有效性。结果显示，我们的方法可以快速和精准地检测建筑物损坏，并且与传统手动检查方法相比，具有明显的处理时间缩短。
</details></li>
</ul>
<hr>
<h2 id="On-the-near-optimality-of-betting-confidence-sets-for-bounded-means"><a href="#On-the-near-optimality-of-betting-confidence-sets-for-bounded-means" class="headerlink" title="On the near-optimality of betting confidence sets for bounded means"></a>On the near-optimality of betting confidence sets for bounded means</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01547">http://arxiv.org/abs/2310.01547</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shubhanshu Shekhar, Aaditya Ramdas</li>
<li>for: 这个论文的目的是提供一种非对称信息Interval的建立方法，以及其时间平衡变体 confidence sequence。</li>
<li>methods: 这个论文使用了一种基于赌博的方法，即Waudby-Smith和Ramdas（2023）的赌博信息Interval。</li>
<li>results: 这个论文提供了一些 teorethical guarantees for this improved empirical performance of betting CIs and CSs，包括limiting width comparison, lower bounds characterization, and matching fundamental limits.<details>
<summary>Abstract</summary>
Constructing nonasymptotic confidence intervals (CIs) for the mean of a univariate distribution from independent and identically distributed (i.i.d.) observations is a fundamental task in statistics. For bounded observations, a classical nonparametric approach proceeds by inverting standard concentration bounds, such as Hoeffding's or Bernstein's inequalities. Recently, an alternative betting-based approach for defining CIs and their time-uniform variants called confidence sequences (CSs), has been shown to be empirically superior to the classical methods. In this paper, we provide theoretical justification for this improved empirical performance of betting CIs and CSs.   Our main contributions are as follows: (i) We first compare CIs using the values of their first-order asymptotic widths (scaled by $\sqrt{n}$), and show that the betting CI of Waudby-Smith and Ramdas (2023) has a smaller limiting width than existing empirical Bernstein (EB)-CIs. (ii) Next, we establish two lower bounds that characterize the minimum width achievable by any method for constructing CIs/CSs in terms of certain inverse information projections. (iii) Finally, we show that the betting CI and CS match the fundamental limits, modulo an additive logarithmic term and a multiplicative constant. Overall these results imply that the betting CI~(and CS) admit stronger theoretical guarantees than the existing state-of-the-art EB-CI~(and CS); both in the asymptotic and finite-sample regimes.
</details>
<details>
<summary>摘要</summary>
constructing nonasymptotic confidence intervals (CIs) for the mean of a univariate distribution from independent and identically distributed (i.i.d.) observations is a fundamental task in statistics. For bounded observations, a classical nonparametric approach proceeds by inverting standard concentration bounds, such as Hoeffding's or Bernstein's inequalities. Recently, an alternative betting-based approach for defining CIs and their time-uniform variants called confidence sequences (CSs), has been shown to be empirically superior to the classical methods. In this paper, we provide theoretical justification for this improved empirical performance of betting CIs and CSs.  我们的主要贡献如下：（i）我们首先比较CIs的第一个 asymptotic width（按照n的平方根 scaling），并显示WAudby-Smith和Ramdas（2023）的赌博CI的限制宽度小于现有的empirical Bernstein（EB）-CI。（ii）然后，我们设定了两个下界，用于描述任何方法构造CIs/CSs的最小宽度，并表示这些下界与certain inverse information projections有关。（iii）最后，我们表明了赌博CI和CS与基本限制相匹配，即，对于任何方法，其宽度至少要比基本限制宽度加上一个对数函数和一个常数多少。总的来说，这些结果表明赌博CI（和CS）在 both the asymptotic and finite-sample regimes具有更强的理论保证，比现有的状态 искусственный智能EB-CI（和CS）更强。
</details></li>
</ul>
<hr>
<h2 id="Fusing-Models-with-Complementary-Expertise"><a href="#Fusing-Models-with-Complementary-Expertise" class="headerlink" title="Fusing Models with Complementary Expertise"></a>Fusing Models with Complementary Expertise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01542">http://arxiv.org/abs/2310.01542</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongyi Wang, Felipe Maia Polo, Yuekai Sun, Souvik Kundu, Eric Xing, Mikhail Yurochkin</li>
<li>for: 这个论文的目的是解决训练AI模型通用多任务多领域的问题，以便在测试时能够更好地掌握数据分布的各种多样性。</li>
<li>methods: 这篇论文使用了专家模型的融合（Fusion of Experts， FoE）方法，将专家模型的输出融合到一起，以提高任务的性能。这种方法适用于推理和生成任务，并且在图像和文本分类、文本摘要、多选问答以及自动评估生成文本等任务中得到了显著的性能提升。</li>
<li>results: 这篇论文的实验结果表明，使用 FoE 方法可以在图像和文本分类、文本摘要、多选问答以及自动评估生成文本等任务中提高性能，并且在“倔强”（frugal）设定下，可以减少专家模型评估的次数。<details>
<summary>Abstract</summary>
Training AI models that generalize across tasks and domains has long been among the open problems driving AI research. The emergence of Foundation Models made it easier to obtain expert models for a given task, but the heterogeneity of data that may be encountered at test time often means that any single expert is insufficient. We consider the Fusion of Experts (FoE) problem of fusing outputs of expert models with complementary knowledge of the data distribution and formulate it as an instance of supervised learning. Our method is applicable to both discriminative and generative tasks and leads to significant performance improvements in image and text classification, text summarization, multiple-choice QA, and automatic evaluation of generated text. We also extend our method to the "frugal" setting where it is desired to reduce the number of expert model evaluations at test time.
</details>
<details>
<summary>摘要</summary>
<<SYS>>对于训练通用的人工智能模型，长期以来是开放问题驱动着人工智能研究的一个重要问题。基础模型的出现使得获得特定任务的专家模型更加容易，但是在测试时可能遇到的数据多样性通常意味着任何一个专家都不够。我们将专家融合（FoE）问题定义为将专家模型输出的 complementary 知识与数据分布相结合，并将其视为一种supervised learning实例。我们的方法适用于推论和生成任务，并在图像和文本分类、文本摘要、多选问答和自动评估生成文本中导致显著性能提升。我们还将方法推广到"倔强"设定，即在测试时尽可能减少专家模型评估数量。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Client-Detection-via-Non-parametric-Subspace-Monitoring-in-the-Internet-of-Federated-Things"><a href="#Adversarial-Client-Detection-via-Non-parametric-Subspace-Monitoring-in-the-Internet-of-Federated-Things" class="headerlink" title="Adversarial Client Detection via Non-parametric Subspace Monitoring in the Internet of Federated Things"></a>Adversarial Client Detection via Non-parametric Subspace Monitoring in the Internet of Federated Things</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01537">http://arxiv.org/abs/2310.01537</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xianjian Xie, Xiaochen Xian, Dan Li, Andi Wang</li>
<li>for: 该论文旨在提出一种有效的非参数方法FedRR，用于解决 federated learning 网络中的恶意攻击问题。</li>
<li>methods: 该方法基于 transmitted 参数更新的低级特征，并可以准确地检测恶意客户端和控制假阳性率。</li>
<li>results: 实验基于 MNIST 数据集的 digit 识别 validate 了我们的方法的优势。<details>
<summary>Abstract</summary>
The Internet of Federated Things (IoFT) represents a network of interconnected systems with federated learning as the backbone, facilitating collaborative knowledge acquisition while ensuring data privacy for individual systems. The wide adoption of IoFT, however, is hindered by security concerns, particularly the susceptibility of federated learning networks to adversarial attacks. In this paper, we propose an effective non-parametric approach FedRR, which leverages the low-rank features of the transmitted parameter updates generated by federated learning to address the adversarial attack problem. Besides, our proposed method is capable of accurately detecting adversarial clients and controlling the false alarm rate under the scenario with no attack occurring. Experiments based on digit recognition using the MNIST datasets validated the advantages of our approach.
</details>
<details>
<summary>摘要</summary>
互联网联邦智能（IoFT）表示一个联网了多个系统，带有联邦学习作为核心，实现共同知识获取的网络，同时保障个体系统的数据隐私。然而，IoFT的广泛应用受到了安全问题的限制，尤其是联邦学习网络对 adversarial 攻击的抵触。在这篇论文中，我们提出了一种有效的非参数方法 FedRR，它利用联邦学习传输的参数更新低级特征来解决 adversarial 攻击问题。此外，我们的提议方法可以准确地检测出恶意客户端，并在没有攻击情况下控制假阳性率。基于 digit 识别 using MNIST 数据集，我们的方法在实验中证明了其优势。
</details></li>
</ul>
<hr>
<h2 id="Nowcasting-day-ahead-marginal-emissions-using-multi-headed-CNNs-and-deep-generative-models"><a href="#Nowcasting-day-ahead-marginal-emissions-using-multi-headed-CNNs-and-deep-generative-models" class="headerlink" title="Nowcasting day-ahead marginal emissions using multi-headed CNNs and deep generative models"></a>Nowcasting day-ahead marginal emissions using multi-headed CNNs and deep generative models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01524">http://arxiv.org/abs/2310.01524</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhruv Suri, Anela Arifi, Ines Azevedo</li>
<li>for: 预测当天纳入系统的碳排放因素，以便在高灵活性和分布式能源资源的能源系统中更好地管理能源。</li>
<li>methods: 使用多头 convolutional neural networks（CNN）生成当天碳排放预测，以便更好地理解一个独立系统运营商的投入决策对碳排放的影响。</li>
<li>results: 通过使用多头 CNN 生成当天碳排放预测，可以更好地理解一个独立系统运营商的投入决策对碳排放的影响，从而更好地管理能源系统。<details>
<summary>Abstract</summary>
Nowcasting day-ahead marginal emissions factors is increasingly important for power systems with high flexibility and penetration of distributed energy resources. With a significant share of firm generation from natural gas and coal power plants, forecasting day-ahead emissions in the current energy system has been widely studied. In contrast, as we shift to an energy system characterized by flexible power markets, dispatchable sources, and competing low-cost generation such as large-scale battery or hydrogen storage, system operators will be able to choose from a mix of different generation as well as emission pathways. To fully develop the emissions implications of a given dispatch schedule, we need a near real-time workflow with two layers. The first layer is a market model that continuously solves a security-constrained economic dispatch model. The second layer determines the marginal emissions based on the output of the market model, which is the subject of this paper. We propose using multi-headed convolutional neural networks to generate day-ahead forecasts of marginal and average emissions for a given independent system operator.
</details>
<details>
<summary>摘要</summary>
现在casting日前边额排放因子是现代化能源系统中增加的重要问题，特别是在高灵活性和分布式能源资源的普及下。现有的研究主要关注天然气和煤矿发电厂的固定产量，预测当前能源系统的日前排放。然而，随着我们转向一个具有灵活电力市场、投放可靠发电源和低成本生产如大规模电池或氢存储的能源系统，系统运营商将有多种不同的发电和排放路径可供选择。为了充分发挥排放的影响，我们需要一个实时工作流程，包括两层。第一层是一个安全保证的经济调度模型，第二层确定基于第一层模型的输出的边额排放。我们提议使用多头 convolutional neural networks（CNN）生成日前预测边额和平均排放的方法，这是本文的研究对象。
</details></li>
</ul>
<hr>
<h2 id="The-Benefit-of-Noise-Injection-for-Dynamic-Gray-Box-Model-Creation"><a href="#The-Benefit-of-Noise-Injection-for-Dynamic-Gray-Box-Model-Creation" class="headerlink" title="The Benefit of Noise-Injection for Dynamic Gray-Box Model Creation"></a>The Benefit of Noise-Injection for Dynamic Gray-Box Model Creation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01517">http://arxiv.org/abs/2310.01517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamed Kandil, J. J. McArthur<br>for: This paper aims to improve the performance of gray-box models for equipment emulator development by addressing uncertainties in the model creation process.methods: The paper proposes injecting noise into the training dataset to enrich the data and provide a measure of robustness against uncertainties.results: The approach was tested on a water-to-water heat exchanger using real devices with live data streaming, resulting in a significant reduction in modeling error (root mean square error) compared to the unprocessed signal data. The improvement amounted to 60% on the training set, and 50% and 45% on the test and validation sets, respectively.<details>
<summary>Abstract</summary>
Gray-box models offer significant benefit over black-box approaches for equipment emulator development for equipment since their integration of physics provides more confidence in the model outside of the training domain. However, challenges such as model nonlinearity, unmodeled dynamics, and local minima introduce uncertainties into grey-box creation that contemporary approaches have failed to overcome, leading to their under-performance compared with black-box models. This paper seeks to address these uncertainties by injecting noise into the training dataset. This noise injection enriches the dataset and provides a measure of robustness against such uncertainties. A dynamic model for a water-to-water heat exchanger has been used as a demonstration case for this approach and tested using a pair of real devices with live data streaming. Compared to the unprocessed signal data, the application of noise injection resulted in a significant reduction in modeling error (root mean square error), decreasing from 0.68 to 0.27{\deg}C. This improvement amounts to a 60% enhancement when assessed on the training set, and improvements of 50% and 45% when validated against the test and validation sets, respectively.
</details>
<details>
<summary>摘要</summary>
灰色模型对设备模拟器开发具有显著的优势，因为它们 integrates 物理学提供了更多的信任度在训练领域之外。然而，模型不线性、不确定性和地方极值引入了不确定性，使得现代方法无法超越这些不确定性，导致其表现相对落后于黑色模型。本文提出了将噪声掺入训练集的方法，以增强数据集的质量和模型对不确定性的Robustness。我们使用了一个水到水热交换器的动态模型作为示例，并使用了两个真实的设备进行实际测试。与未处理的信号数据相比，噪声掺入导致模型错误减少了从0.68到0.27℃，即60%的提高。在训练集上评估时，改进了50%，在验证集和验证集上分别提高了45%。
</details></li>
</ul>
<hr>
<h2 id="Tensor-Ring-Optimized-Quantum-Enhanced-Tensor-Neural-Networks"><a href="#Tensor-Ring-Optimized-Quantum-Enhanced-Tensor-Neural-Networks" class="headerlink" title="Tensor Ring Optimized Quantum-Enhanced Tensor Neural Networks"></a>Tensor Ring Optimized Quantum-Enhanced Tensor Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01515">http://arxiv.org/abs/2310.01515</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/konar1987/tr-qnet">https://github.com/konar1987/tr-qnet</a></li>
<li>paper_authors: Debanjan Konar, Dheeraj Peddireddy, Vaneet Aggarwal, Bijaya K. Panigrahi<br>For:The paper is written for researchers in the field of quantum machine learning, specifically those interested in incorporating tensor networks into deep neural networks and variational optimization.Methods:The paper proposes a multi-layer design of a Tensor Ring optimized variational Quantum learning classifier (Quan-TR), which consists of cascading entangling gates replacing the fully connected layers of a tensor network. The parameters of the TR-QNet are optimized through stochastic gradient descent algorithm on qubit measurements.Results:The proposed TR-QNet achieves promising accuracy on three distinct datasets, namely Iris, MNIST, and CIFAR-10, with accuracy of 94.5%, 86.16%, and 83.54%, respectively, on quantum simulations. The paper also conducts benchmark studies on state-of-the-art quantum and classical implementations of tensor network models to demonstrate the efficacy of the proposed TR-QNet. Additionally, the scalability of TR-QNet highlights its potential for deep learning applications on a large scale.<details>
<summary>Abstract</summary>
Quantum machine learning researchers often rely on incorporating Tensor Networks (TN) into Deep Neural Networks (DNN) and variational optimization. However, the standard optimization techniques used for training the contracted trainable weights of each model layer suffer from the correlations and entanglement structure between the model parameters on classical implementations. To address this issue, a multi-layer design of a Tensor Ring optimized variational Quantum learning classifier (Quan-TR) comprising cascading entangling gates replacing the fully connected (dense) layers of a TN is proposed, and it is referred to as Tensor Ring optimized Quantum-enhanced tensor neural Networks (TR-QNet). TR-QNet parameters are optimized through the stochastic gradient descent algorithm on qubit measurements. The proposed TR-QNet is assessed on three distinct datasets, namely Iris, MNIST, and CIFAR-10, to demonstrate the enhanced precision achieved for binary classification. On quantum simulations, the proposed TR-QNet achieves promising accuracy of $94.5\%$, $86.16\%$, and $83.54\%$ on the Iris, MNIST, and CIFAR-10 datasets, respectively. Benchmark studies have been conducted on state-of-the-art quantum and classical implementations of TN models to show the efficacy of the proposed TR-QNet. Moreover, the scalability of TR-QNet highlights its potential for exhibiting in deep learning applications on a large scale. The PyTorch implementation of TR-QNet is available on Github:https://github.com/konar1987/TR-QNet/
</details>
<details>
<summary>摘要</summary>
研究员们常常将量子机器学习与含tensor网络（TN）和变量优化结合起来。然而，在классифика翻译器中使用标准优化技术来训练每层模型参数的问题受到 correlate 和束缚结构的影响。为了解决这个问题，我们提出了一种多层设计的tensor环优化量子学习分类器（Quan-TR），其中每层的含tensor网络（TN）中的完全连接（dense）层被替换为束缚门。这种模型被称为tensor环优化量子含tensor神经网络（TR-QNet）。TR-QNet的参数通过随机梯度下降算法在量子测量中进行优化。我们在三个不同的数据集上（namely Iris、MNIST和CIFAR-10）进行了评估，并达到了高精度的分类结果。在量子仿真中，我们的TR-QNet实现了可观的准确率，即$94.5\%$, $86.16\%$和$83.54\%$。我们还对现有的量子和类型实现的TN模型进行了比较，以显示TR-QNet的效果。此外，TR-QNet的可扩展性表明它在深度学习应用中具有潜在的潜力。TR-QNet的PyTorch实现可以在GitHub上找到：https://github.com/konar1987/TR-QNet/。
</details></li>
</ul>
<hr>
<h2 id="CODA-Temporal-Domain-Generalization-via-Concept-Drift-Simulator"><a href="#CODA-Temporal-Domain-Generalization-via-Concept-Drift-Simulator" class="headerlink" title="CODA: Temporal Domain Generalization via Concept Drift Simulator"></a>CODA: Temporal Domain Generalization via Concept Drift Simulator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01508">http://arxiv.org/abs/2310.01508</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chia-Yuan Chang, Yu-Neng Chuang, Zhimeng Jiang, Kwei-Herng Lai, Anxiao Jiang, Na Zou</li>
<li>for: 这个研究旨在解决机器学习模型在概念漂移（concept drift）中的问题，以提高模型在不同时间点的通用性。</li>
<li>methods: 研究使用了一个名为CODA（Concept Drift simulAtor）的框架，它利用预测的特征相互 correlations来生成未来数据，以便训练模型。</li>
<li>results: 实验结果显示，使用CODA-生成的数据作为训练输入可以有效地实现时间领域通用性，并且可以适用于不同的模型架构。<details>
<summary>Abstract</summary>
In real-world applications, machine learning models often become obsolete due to shifts in the joint distribution arising from underlying temporal trends, a phenomenon known as the "concept drift". Existing works propose model-specific strategies to achieve temporal generalization in the near-future domain. However, the diverse characteristics of real-world datasets necessitate customized prediction model architectures. To this end, there is an urgent demand for a model-agnostic temporal domain generalization approach that maintains generality across diverse data modalities and architectures. In this work, we aim to address the concept drift problem from a data-centric perspective to bypass considering the interaction between data and model. Developing such a framework presents non-trivial challenges: (i) existing generative models struggle to generate out-of-distribution future data, and (ii) precisely capturing the temporal trends of joint distribution along chronological source domains is computationally infeasible. To tackle the challenges, we propose the COncept Drift simulAtor (CODA) framework incorporating a predicted feature correlation matrix to simulate future data for model training. Specifically, CODA leverages feature correlations to represent data characteristics at specific time points, thereby circumventing the daunting computational costs. Experimental results demonstrate that using CODA-generated data as training input effectively achieves temporal domain generalization across different model architectures.
</details>
<details>
<summary>摘要</summary>
在实际应用中，机器学习模型经常因为 JOINT 分布的变化而变得过时，这种现象被称为 "概念漂移"。现有的工作提出了特定于模型的策略来实现时间总结。然而，实际数据的多样性需要特定的预测模型建 architecture。因此，有一项非常需要的是一种模型无关的时间域总结方法，可以在不同的数据模式和建 architecture 下保持一致性。在这项工作中，我们尝试通过数据中心的方式解决概念漂移问题，而不是考虑数据和模型之间的交互。开发这样的框架具有非常大的挑战：（i）现有的生成模型很难生成未经验数据，（ii）准确地捕捉 JOINT 分布中的时间趋势是计算不可能的。为了解决这些挑战，我们提出了 COncept Drift simulAtor（CODA）框架，该框架利用预测的特征相关矩阵来模拟未来数据，以便对模型进行训练。具体来说，CODA 利用特征相关来表示特定时间点的数据特征，从而绕过了计算不可能的问题。实验结果表明，使用 CODA-生成的数据作为训练输入可以实现时间域总结，并且可以在不同的模型建 architecture 下实现。
</details></li>
</ul>
<hr>
<h2 id="A-Learning-Based-Scheme-for-Fair-Timeliness-in-Sparse-Gossip-Networks"><a href="#A-Learning-Based-Scheme-for-Fair-Timeliness-in-Sparse-Gossip-Networks" class="headerlink" title="A Learning Based Scheme for Fair Timeliness in Sparse Gossip Networks"></a>A Learning Based Scheme for Fair Timeliness in Sparse Gossip Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01396">http://arxiv.org/abs/2310.01396</a></li>
<li>repo_url: None</li>
<li>paper_authors: Purbesh Mitra, Sennur Ulukus</li>
<li>for: 本研究旨在研究一个带有各种连接性的谣言网络，source更新信息采用波动过程，并且将信息传递给网络中的节点。由于网络结构不均衡，不同节点的实时性不同，因此需要研究如何对网络进行公平的时间分配，以最小化总体最差性能。</li>
<li>methods: 本研究使用连续搜索空间的枪戈投掷问题形式化了问题，并采用 Gaussian process基于 Bayesian 优化来实现探索和利用的权衡。</li>
<li>results: 研究发现，采用 Gaussian process基于 Bayesian 优化的方法可以在不同的网络结构下实现公平的时间分配，并且可以最小化总体最差性能。<details>
<summary>Abstract</summary>
We consider a gossip network, consisting of $n$ nodes, which tracks the information at a source. The source updates its information with a Poisson arrival process and also sends updates to the nodes in the network. The nodes themselves can exchange information among themselves to become as timely as possible. However, the network structure is sparse and irregular, i.e., not every node is connected to every other node in the network, rather, the order of connectivity is low, and varies across different nodes. This asymmetry of the network implies that the nodes in the network do not perform equally in terms of timelines. Due to the gossiping nature of the network, some nodes are able to track the source very timely, whereas, some nodes fall behind versions quite often. In this work, we investigate how the rate-constrained source should distribute its update rate across the network to maintain fairness regarding timeliness, i.e., the overall worst case performance of the network can be minimized. Due to the continuous search space for optimum rate allocation, we formulate this problem as a continuum-armed bandit problem and employ Gaussian process based Bayesian optimization to meet a trade-off between exploration and exploitation sequentially.
</details>
<details>
<summary>摘要</summary>
我们考虑一个嗅探网络，包含 $n$ 个节点，跟踪源信息的变化。源节点通过波动过程更新自己的信息，并将更新传递给网络中的其他节点。节点之间可以互相交换信息，以使自己的时间线最为整拢。然而，网络结构稀疏和不规则，即不是所有节点与所有其他节点相连，而是每个节点与其他节点之间的连接关系较弱，因此不同节点在网络中的性能不同。由于嗅探网络的自我感知特性，一些节点可以很快地跟踪源信息，而其他节点则经常落后版本。在这个工作中，我们研究如何Constrained source应该在网络中分配更新率，以保持公平性，即最大化网络总体最差性能。由于搜索空间是连续的，我们将这个问题转化为连续武器问题，并使用 Gaussian process 基于 Bayesian 优化来实现搜索和利用的权衡。
</details></li>
</ul>
<hr>
<h2 id="Pessimistic-Nonlinear-Least-Squares-Value-Iteration-for-Offline-Reinforcement-Learning"><a href="#Pessimistic-Nonlinear-Least-Squares-Value-Iteration-for-Offline-Reinforcement-Learning" class="headerlink" title="Pessimistic Nonlinear Least-Squares Value Iteration for Offline Reinforcement Learning"></a>Pessimistic Nonlinear Least-Squares Value Iteration for Offline Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01380">http://arxiv.org/abs/2310.01380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiwei Di, Heyang Zhao, Jiafan He, Quanquan Gu</li>
<li>for: 本研究旨在提出一种 oracle-efficient 算法，用于 offline 强化学习（RL）中的非线性函数approximation。</li>
<li>methods: 我们的算法采用了三个创新的Component：(1) 一种基于差异的重 regression scheme，可以应用于各种函数类型; (2) 一种用于幂度估计的 subroutine; (3) 一种计划阶段使用的 pessimistic value iteration 方法。</li>
<li>results: 我们的算法可以 garantuetotal  achieve minimax 优化的实例特性 regret，并且在特定的函数类型下，其 regret bound 具有紧张的函数类型复杂度的关系。<details>
<summary>Abstract</summary>
Offline reinforcement learning (RL), where the agent aims to learn the optimal policy based on the data collected by a behavior policy, has attracted increasing attention in recent years. While offline RL with linear function approximation has been extensively studied with optimal results achieved under certain assumptions, many works shift their interest to offline RL with non-linear function approximation. However, limited works on offline RL with non-linear function approximation have instance-dependent regret guarantees. In this paper, we propose an oracle-efficient algorithm, dubbed Pessimistic Nonlinear Least-Square Value Iteration (PNLSVI), for offline RL with non-linear function approximation. Our algorithmic design comprises three innovative components: (1) a variance-based weighted regression scheme that can be applied to a wide range of function classes, (2) a subroutine for variance estimation, and (3) a planning phase that utilizes a pessimistic value iteration approach. Our algorithm enjoys a regret bound that has a tight dependency on the function class complexity and achieves minimax optimal instance-dependent regret when specialized to linear function approximation. Our work extends the previous instance-dependent results within simpler function classes, such as linear and differentiable function to a more general framework.
</details>
<details>
<summary>摘要</summary>
养成机器人学习（RL）在线上进行学习，以便学习最佳策略基于行为策略收集的数据。Recent years have seen increasing attention paid to offline RL with linear function approximation. However, many works have shifted their focus to offline RL with non-linear function approximation. Although there have been limited works on offline RL with non-linear function approximation that provide instance-dependent regret guarantees. In this paper, we propose an oracle-efficient algorithm, called Pessimistic Nonlinear Least-Square Value Iteration (PNLSVI), for offline RL with non-linear function approximation. Our algorithm design includes three innovative components:1. 一种基于方差的重量回归方案，可以应用于各种函数类型2. 一种归一化误差估计的子routine3. 一个使用悲观值迭代方法的规划阶段我们的算法拥有一个具有函数类型复杂度的 regret bound，并在特殊化为线性函数approximation时实现最佳最小化例外 regret。我们的工作扩展了之前只适用于更简单的函数类型，如线性和导数函数的结果，到一个更通用的框架。
</details></li>
</ul>
<hr>
<h2 id="Window-based-Model-Averaging-Improves-Generalization-in-Heterogeneous-Federated-Learning"><a href="#Window-based-Model-Averaging-Improves-Generalization-in-Heterogeneous-Federated-Learning" class="headerlink" title="Window-based Model Averaging Improves Generalization in Heterogeneous Federated Learning"></a>Window-based Model Averaging Improves Generalization in Heterogeneous Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01366">http://arxiv.org/abs/2310.01366</a></li>
<li>repo_url: None</li>
<li>paper_authors: Debora Caldarola, Barbara Caputo, Marco Ciccone</li>
<li>for: 提高 Federated Learning（FL）中数据分布不均的问题，保护用户隐私。</li>
<li>methods: 提出了窗口基于的模型均值（WIMA）方法，通过融合不同回合的全球模型，有效地捕捉多个用户的知识，降低最后见 Client 数据偏见。</li>
<li>results: 在不同的分布Shift和坏 Client 采样情况下，WIMA 能够提供更平滑、稳定的学习趋势，同时不增加客户端计算或通信开销。<details>
<summary>Abstract</summary>
Federated Learning (FL) aims to learn a global model from distributed users while protecting their privacy. However, when data are distributed heterogeneously the learning process becomes noisy, unstable, and biased towards the last seen clients' data, slowing down convergence. To address these issues and improve the robustness and generalization capabilities of the global model, we propose WIMA (Window-based Model Averaging). WIMA aggregates global models from different rounds using a window-based approach, effectively capturing knowledge from multiple users and reducing the bias from the last ones. By adopting a windowed view on the rounds, WIMA can be applied from the initial stages of training. Importantly, our method introduces no additional communication or client-side computation overhead. Our experiments demonstrate the robustness of WIMA against distribution shifts and bad client sampling, resulting in smoother and more stable learning trends. Additionally, WIMA can be easily integrated with state-of-the-art algorithms. We extensively evaluate our approach on standard FL benchmarks, demonstrating its effectiveness.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Fleet-Policy-Learning-via-Weight-Merging-and-An-Application-to-Robotic-Tool-Use"><a href="#Fleet-Policy-Learning-via-Weight-Merging-and-An-Application-to-Robotic-Tool-Use" class="headerlink" title="Fleet Policy Learning via Weight Merging and An Application to Robotic Tool-Use"></a>Fleet Policy Learning via Weight Merging and An Application to Robotic Tool-Use</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01362">http://arxiv.org/abs/2310.01362</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liruiw/fleet-tools">https://github.com/liruiw/fleet-tools</a></li>
<li>paper_authors: Lirui Wang, Kaiqing Zhang, Allan Zhou, Max Simchowitz, Russ Tedrake</li>
<li>for: 这篇论文的目的是探讨分布式学习可以如何实现群体级别的机器人学习，而不需要传输或中央化群体级别数据。</li>
<li>methods: 该论文提出了一种分布式学习策略，称为“队伍合并”（fleet-merge），可以有效地将多个策略 Parameterized by recurrent neural networks (RNNs) 集成在分布式环境中。</li>
<li>results: 研究人员在Meta-World环境中训练了50个任务，并通过队伍合并策略将其们的策略集成起来，得到了良好的性能。此外，他们还提出了一个新的机器人工具使用标准，称为“队伍工具”（fleet-tools），可以用于评估群体级别的机器人学习在复杂和有接触的机器人手 manipulate 任务中的性能。<details>
<summary>Abstract</summary>
Fleets of robots ingest massive amounts of streaming data generated by interacting with their environments, far more than those that can be stored or transmitted with ease. At the same time, we hope that teams of robots can co-acquire diverse skills through their experiences in varied settings. How can we enable such fleet-level learning without having to transmit or centralize fleet-scale data? In this paper, we investigate distributed learning of policies as a potential solution. To efficiently merge policies in the distributed setting, we propose fleet-merge, an instantiation of distributed learning that accounts for the symmetries that can arise in learning policies that are parameterized by recurrent neural networks. We show that fleet-merge consolidates the behavior of policies trained on 50 tasks in the Meta-World environment, with the merged policy achieving good performance on nearly all training tasks at test time. Moreover, we introduce a novel robotic tool-use benchmark, fleet-tools, for fleet policy learning in compositional and contact-rich robot manipulation tasks, which might be of broader interest, and validate the efficacy of fleet-merge on the benchmark.
</details>
<details>
<summary>摘要</summary>
大量的机器人队伍通过与环境互动生成大量流动数据，远远超出了可以存储或传输的范围。同时，我们希望机器人队伍可以通过不同的场景经验共同获得多样化的技能。如何实现这种队伍级学习而无需传输或中央化队伍级数据？在这篇论文中，我们调查分布式学习策略为可能的解决方案。为了有效地融合分布式环境中的策略，我们提出了“队伍融合”（fleet-merge），这是基于循环神经网络参数化策略的分布式学习实现，考虑到分布式环境中策略学习时可能出现的对称性。我们显示，队伍融合可以有效地将50个任务的策略在Meta-World环境中的行为协调，并在测试时对大多数训练任务表现良好。此外，我们还介绍了一个新的机器人工具使用指标，称为“队伍工具”（fleet-tools），用于评估机器人队伍在复杂的机器人拼接和接触rich任务中的策略学习能力，这可能对更广泛的领域有所启发。我们 Validate the effectiveness of fleet-merge on the benchmark.
</details></li>
</ul>
<hr>
<h2 id="A-peridynamic-informed-deep-learning-model-for-brittle-damage-prediction"><a href="#A-peridynamic-informed-deep-learning-model-for-brittle-damage-prediction" class="headerlink" title="A peridynamic-informed deep learning model for brittle damage prediction"></a>A peridynamic-informed deep learning model for brittle damage prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01350">http://arxiv.org/abs/2310.01350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roozbeh Eghbalpoor, Azadeh Sheidaei</li>
<li>for: 预测质量材料中的 quasi-static 损害和裂化</li>
<li>methods: combines  périodic 理论与Physics-Informed Neural Network (PINN) 方法</li>
<li>results: 能准确预测质量材料中的损害和裂化，并且高效率Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written to predict the quasi-static damage and crack propagation in brittle materials using a novel approach that combines the principles of peridynamic theory with PINN.</li>
<li>methods: The proposed approach uses the linearized PD governing equation to enforce the PD principles in the PINN’s residual-based loss function, allowing the model to learn and capture intricate displacement patterns associated with different geometrical parameters. The paper also proposes several enhancements, such as cyclical annealing schedule and deformation gradient aware optimization technique, to ensure the model’s convergence and accuracy.</li>
<li>results: The paper’s results show that the proposed PD-INN approach can accurately predict damage and crack propagation in brittle materials, and it is more efficient than traditional methods such as PD direct numerical method and Extended-Finite Element Method. The paper provides several benchmark cases to validate the accuracy of the proposed approach.<details>
<summary>Abstract</summary>
In this study, a novel approach that combines the principles of peridynamic (PD) theory with PINN is presented to predict quasi-static damage and crack propagation in brittle materials. To achieve high prediction accuracy and convergence rate, the linearized PD governing equation is enforced in the PINN's residual-based loss function. The proposed PD-INN is able to learn and capture intricate displacement patterns associated with different geometrical parameters, such as pre-crack position and length. Several enhancements like cyclical annealing schedule and deformation gradient aware optimization technique are proposed to ensure the model would not get stuck in its trivial solution. The model's performance assessment is conducted by monitoring the behavior of loss function throughout the training process. The PD-INN predictions are also validated through several benchmark cases with the results obtained from high-fidelity techniques such as PD direct numerical method and Extended-Finite Element Method. Our results show the ability of the nonlocal PD-INN to predict damage and crack propagation accurately and efficiently.
</details>
<details>
<summary>摘要</summary>
在这一研究中，我们提出了一种新的方法，即将普适动学（PD）原理与人工神经网络（PINN）结合以预测质量静的损害和裂缝升温。为了保证预测精度和收敛率高，我们在PINN的剩余基于损失函数中 enforces 了线性化的PD公式。我们的PD-INN可以学习和捕捉不同的几何参数（如预先裂位和长度）对应的复杂的位移模式。我们还提出了循环退火 schedule 和减弱材料响应优化技术来确保模型不会陷入到极少的解。我们通过监测损失函数的行为进行模型评估。我们的PD-INN预测也与高精度技术such as PD直接数值方法和扩展Finite Element方法的结果进行了验证。我们的结果表明PD-INN能够高效地和准确地预测损害和裂缝升温。
</details></li>
</ul>
<hr>
<h2 id="The-Optimal-use-of-Segmentation-for-Sampling-Calorimeters"><a href="#The-Optimal-use-of-Segmentation-for-Sampling-Calorimeters" class="headerlink" title="The Optimal use of Segmentation for Sampling Calorimeters"></a>The Optimal use of Segmentation for Sampling Calorimeters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04442">http://arxiv.org/abs/2310.04442</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/eiccodesign/regressiononly">https://github.com/eiccodesign/regressiononly</a></li>
<li>paper_authors: Fernando Torales Acosta, Bishnu Karki, Piyush Karande, Aaron Angerami, Miguel Arratia, Kenneth Barish, Ryan Milton, Sebastián Morán, Benjamin Nachman, Anshuman Sinha</li>
<li>for: 这个论文是为了研究探测器的能量重建方法。</li>
<li>methods: 该论文使用深度神经网络来表示探测器，并利用所有可用信息来进行能量重建。</li>
<li>results: 研究发现，在隔离带电离袋中， relativelly细的长itudinal分割是重建能量的关键。这些结果可以作为未来EIC探测器优化的标准，以及其他实验室中高分辨率探测器的研究。<details>
<summary>Abstract</summary>
One of the key design choices of any sampling calorimeter is how fine to make the longitudinal and transverse segmentation. To inform this choice, we study the impact of calorimeter segmentation on energy reconstruction. To ensure that the trends are due entirely to hardware and not to a sub-optimal use of segmentation, we deploy deep neural networks to perform the reconstruction. These networks make use of all available information by representing the calorimeter as a point cloud. To demonstrate our approach, we simulate a detector similar to the forward calorimeter system intended for use in the ePIC detector, which will operate at the upcoming Electron Ion Collider. We find that for the energy estimation of isolated charged pion showers, relatively fine longitudinal segmentation is key to achieving an energy resolution that is better than 10% across the full phase space. These results provide a valuable benchmark for ongoing EIC detector optimizations and may also inform future studies involving high-granularity calorimeters in other experiments at various facilities.
</details>
<details>
<summary>摘要</summary>
一个重要的设计选择 для任何采样加热计是如何细化 longitudinal 和 transverse 分 segmentation。为了决定这个选择，我们研究采用加热计分 segmentation 对能量重建的影响。为确保这些趋势是固有的硬件效应而不是不当使用分 segmentation，我们使用深度神经网络进行重建。这些网络利用所有可用信息，将加热计表示为点云。为了证明我们的方法，我们模拟了类似于前向加热计系统，这将在未来的 Electron Ion Collider 中使用。我们发现，对孤立 charged pion 散射的能量估计，相对细化 longitudinal 分 segmentation 是达到更好于 10% 的全频范围能量分辨率的关键。这些结果提供了价值的参考点 для进行中的 EIC 仪器优化，也可能会影响未来在其他实验室中的高精度加热计研究。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Estimator-for-Linear-Regression-with-Shuffled-Labels"><a href="#Optimal-Estimator-for-Linear-Regression-with-Shuffled-Labels" class="headerlink" title="Optimal Estimator for Linear Regression with Shuffled Labels"></a>Optimal Estimator for Linear Regression with Shuffled Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01326">http://arxiv.org/abs/2310.01326</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hang Zhang, Ping Li</li>
<li>for: Linear regression with shuffled labels, specifically reconstructing the permutation matrix and signal of interest from the sensing results.</li>
<li>methods: One-step estimator with a computational complexity of $O(n^3 + np^2m)$, which is comparable to the maximum complexity of linear assignment and least square algorithms.</li>
<li>results: Sufficient conditions for correct permutation recovery under different regimes of signal-to-noise ratio (SNR), including an easy regime, a medium regime, and a hard regime. Numerical experiments confirm the theoretical claims.<details>
<summary>Abstract</summary>
This paper considers the task of linear regression with shuffled labels, i.e., $\mathbf Y = \mathbf \Pi \mathbf X \mathbf B + \mathbf W$, where $\mathbf Y \in \mathbb R^{n\times m}, \mathbf Pi \in \mathbb R^{n\times n}, \mathbf X\in \mathbb R^{n\times p}, \mathbf B \in \mathbb R^{p\times m}$, and $\mathbf W\in \mathbb R^{n\times m}$, respectively, represent the sensing results, (unknown or missing) corresponding information, sensing matrix, signal of interest, and additive sensing noise. Given the observation $\mathbf Y$ and sensing matrix $\mathbf X$, we propose a one-step estimator to reconstruct $(\mathbf \Pi, \mathbf B)$. From the computational perspective, our estimator's complexity is $O(n^3 + np^2m)$, which is no greater than the maximum complexity of a linear assignment algorithm (e.g., $O(n^3)$) and a least square algorithm (e.g., $O(np^2 m)$). From the statistical perspective, we divide the minimum $snr$ requirement into four regimes, e.g., unknown, hard, medium, and easy regimes; and present sufficient conditions for the correct permutation recovery under each regime: $(i)$ $snr \geq \Omega(1)$ in the easy regime; $(ii)$ $snr \geq \Omega(\log n)$ in the medium regime; and $(iii)$ $snr \geq \Omega((\log n)^{c_0}\cdot n^{c_1}/{srank(\mathbf B)})$ in the hard regime ($c_0, c_1$ are some positive constants and $srank(\mathbf B)$ denotes the stable rank of $\mathbf B$). In the end, we also provide numerical experiments to confirm the above claims.
</details>
<details>
<summary>摘要</summary>
这篇论文考虑了线性回归问题，即 $\mathbf{Y = \Pi XB + W}$, 其中 $\mathbf{Y} \in \mathbb{R}^{n \times m}, \mathbf{\Pi} \in \mathbb{R}^{n \times n}, \mathbf{X} \in \mathbb{R}^{n \times p}, \mathbf{B} \in \mathbb{R}^{p \times m}$, 和 $\mathbf{W} \in \mathbb{R}^{n \times m}$ 分别表示探测结果、对应信息、探测矩阵、信号 OF interest 和随机探测噪音。给定观测值 $\mathbf{Y}$ 和探测矩阵 $\mathbf{X}$，我们提议一步估计器来重建 $(\mathbf{\Pi}, \mathbf{B})$。从计算角度来看，我们的估计器的复杂度为 $O(n^3 + np^2m)$，不超过最大的线性分配算法的复杂度（例如 $O(n^3)$）和最小二乘算法的复杂度（例如 $O(np^2m)$）。从统计角度来看，我们将最小 $snr$ 要求分为四个 режиmes，即未知 режиme、困难 режиme、中等 режиme 和容易 режиme，并给出了各 режиme 下correct permutation recovery的 suficient conditions： $(i)$ $snr \geq \Omega(1)$ 在容易 режиme; $(ii)$ $snr \geq \Omega(\log n)$ 在中等 режиme; 和 $(iii)$ $snr \geq \Omega((log n)^{c_0} \cdot n^{c_1}/{srank(\mathbf{B})})$ 在困难 режиme（$c_0, c_1$ 是一些正数， $srank(\mathbf{B})$ 表示 $\mathbf{B}$ 的稳定秩）。 finally, we also provide numerical experiments to confirm the above claims.
</details></li>
</ul>
<hr>
<h2 id="Coupling-public-and-private-gradient-provably-helps-optimization"><a href="#Coupling-public-and-private-gradient-provably-helps-optimization" class="headerlink" title="Coupling public and private gradient provably helps optimization"></a>Coupling public and private gradient provably helps optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01304">http://arxiv.org/abs/2310.01304</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruixuan Liu, Zhiqi Bu, Yu-xiang Wang, Sheng Zha, George Karypis</li>
<li>for: 提高大神经网络的成功率，通过结合私人和公共数据进行优化。</li>
<li>methods: 使用权重Linear Combination将私人和公共数据的梯度相互 Coupling，并在 convex 设定下解析出 оптималь solution。</li>
<li>results: 通过实验证明，在语言和视觉benchmark上， gradient Coupling可以加速非对称损失的收敛，并且Hyperparameter如隐私预算、迭代次数、批处理大小和模型大小对 choosing 优化的权重有影响。<details>
<summary>Abstract</summary>
The success of large neural networks is crucially determined by the availability of data. It has been observed that training only on a small amount of public data, or privately on the abundant private data can lead to undesirable degradation of accuracy. In this work, we leverage both private and public data to improve the optimization, by coupling their gradients via a weighted linear combination. We formulate an optimal solution for the optimal weight in the convex setting to indicate that the weighting coefficient should be hyperparameter-dependent. Then, we prove the acceleration in the convergence of non-convex loss and the effects of hyper-parameters such as privacy budget, number of iterations, batch size, and model size on the choice of the weighting coefficient. We support our analysis with empirical experiments across language and vision benchmarks, and provide a guideline for choosing the optimal weight of the gradient coupling.
</details>
<details>
<summary>摘要</summary>
“大型神经网络的成功几率受到数据的可用性的重要限制。已经观察到只在少量公共数据或私有数据上训练时，可能会导致准确度下降。在这种工作中，我们利用了私有和公共数据来改善优化，通过将其权重组合。我们在凸Setting中提出了最佳解决方案，其中权重系数应该是Hyperparameter-dependent。然后，我们证明了加速非 convex损失的收敛速度和 гиперparameters的影响，如隐私预算、迭代次数、批处理大小和模型大小。我们支持我们的分析通过语言和视觉 benchmarks 的实验，并提供了选择最佳权重的指南。”Note that Simplified Chinese is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and other parts of the world.
</details></li>
</ul>
<hr>
<h2 id="Automated-regime-detection-in-multidimensional-time-series-data-using-sliced-Wasserstein-k-means-clustering"><a href="#Automated-regime-detection-in-multidimensional-time-series-data-using-sliced-Wasserstein-k-means-clustering" class="headerlink" title="Automated regime detection in multidimensional time series data using sliced Wasserstein k-means clustering"></a>Automated regime detection in multidimensional time series data using sliced Wasserstein k-means clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01285">http://arxiv.org/abs/2310.01285</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinmeng Luan, James Hamp</li>
<li>for: 本研究使用 Wasserstein k-means  clustering 方法来标识时间序列数据中的不同频率模式。</li>
<li>methods: 本研究首先对一维时间序列数据应用 Wasserstein k-means  clustering 算法，并研究了不同初始化的影响。然后，对多维时间序列数据，我们使用 slice  Wasserstein k-means  clustering 方法（sWk-means），并用合成数据示出了该方法的有效性。</li>
<li>results: 本研究使用实际的外汇spot价数据进行了一个案例研究，并证明了 sWk-means 方法的有效性。研究还发现了一些限制，并提出了可能的补充或替代方法。<details>
<summary>Abstract</summary>
Recent work has proposed Wasserstein k-means (Wk-means) clustering as a powerful method to identify regimes in time series data, and one-dimensional asset returns in particular. In this paper, we begin by studying in detail the behaviour of the Wasserstein k-means clustering algorithm applied to synthetic one-dimensional time series data. We study the dynamics of the algorithm and investigate how varying different hyperparameters impacts the performance of the clustering algorithm for different random initialisations. We compute simple metrics that we find are useful in identifying high-quality clusterings. Then, we extend the technique of Wasserstein k-means clustering to multidimensional time series data by approximating the multidimensional Wasserstein distance as a sliced Wasserstein distance, resulting in a method we call `sliced Wasserstein k-means (sWk-means) clustering'. We apply the sWk-means clustering method to the problem of automated regime detection in multidimensional time series data, using synthetic data to demonstrate the validity of the approach. Finally, we show that the sWk-means method is effective in identifying distinct market regimes in real multidimensional financial time series, using publicly available foreign exchange spot rate data as a case study. We conclude with remarks about some limitations of our approach and potential complementary or alternative approaches.
</details>
<details>
<summary>摘要</summary>
最近的工作提出了 Wasserstein k-means（Wk-means）归一 clustering 方法，用于时间序列数据中的分区。本文首先对 synthetic 一维时间序列数据进行了详细的研究，探讨了 Wasserstein k-means  clustering 算法的行为和不同权重参数对不同初始化的影响。我们计算了一些简单的指标，用于评价高质量的归一结果。然后，我们将多维时间序列数据中的 Wasserstein k-means clustering 方法扩展为 sliced Wasserstein k-means（sWk-means）归一方法，通过 aproximating 多维 Wasserstein 距离为 slice Wasserstein 距离。我们使用 synthetic 数据 demonstrate 了这种方法的有效性。最后，我们使用公开available foreign exchange spot rate 数据作为案例研究，证明了 sWk-means 方法在实际多维金融时间序列中可以有效地 Identify 市场 режимы。我们结束时提出了一些限制和可能的补充或替代方法。
</details></li>
</ul>
<hr>
<h2 id="Non-Exchangeable-Conformal-Risk-Control"><a href="#Non-Exchangeable-Conformal-Risk-Control" class="headerlink" title="Non-Exchangeable Conformal Risk Control"></a>Non-Exchangeable Conformal Risk Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01262">http://arxiv.org/abs/2310.01262</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deep-spin/non-exchangeable-crc">https://github.com/deep-spin/non-exchangeable-crc</a></li>
<li>paper_authors: António Farinhas, Chrysoula Zerva, Dennis Ulmer, André F. T. Martins</li>
<li>for: 提供形式保证的uncertainty集或间隔 для黑盒神经网络预测，确保先定的概率包含实际的地面真值。</li>
<li>methods: 基于非交换性数据的扩展，以及提供更广泛的目标的统计保证，如确界最好的F1分数或预期false negative rate。</li>
<li>results: 在 synthetic 和实际数据上实现了非交换性扩展的 conformal risk control，可控制任意升序损失函数的期望值，无需假设，可以根据测试示例的统计相似性进行数据权重。<details>
<summary>Abstract</summary>
Split conformal prediction has recently sparked great interest due to its ability to provide formally guaranteed uncertainty sets or intervals for predictions made by black-box neural models, ensuring a predefined probability of containing the actual ground truth. While the original formulation assumes data exchangeability, some extensions handle non-exchangeable data, which is often the case in many real-world scenarios. In parallel, some progress has been made in conformal methods that provide statistical guarantees for a broader range of objectives, such as bounding the best F1-score or minimizing the false negative rate in expectation. In this paper, we leverage and extend these two lines of work by proposing non-exchangeable conformal risk control, which allows controlling the expected value of any monotone loss function when the data is not exchangeable. Our framework is flexible, makes very few assumptions, and allows weighting the data based on its statistical similarity with the test examples; a careful choice of weights may result on tighter bounds, making our framework useful in the presence of change points, time series, or other forms of distribution drift. Experiments with both synthetic and real world data show the usefulness of our method.
</details>
<details>
<summary>摘要</summary>
划分预测（Conformal Prediction）在最近几年内产生了广泛的兴趣，它可以提供对黑盒神经网络模型的预测结果的形式保证的不确定集或间隔，保证预测结果符合定义的概率。然而，原始 формулировщин assumes 数据均匀性，一些扩展可以处理非均匀数据，这些数据在许多实际场景中很常见。另外，一些进展在划分方法上，可以为更广泛的目标提供统计保证，例如缩小最佳 F1 分数或预测FALSE Negative 率的预期。在这篇文章中，我们利用和扩展这两个线索，提出非均匀划分风险控制，可以在不均匀数据上控制预测结果的预期值。我们的框架具有很少假设，可以根据测试例子的统计相似性来赋重数据，选择合适的赋重可能会使我们的框架在变化点、时间序列或其他形式的分布漂移中更加有用。实验表明，我们的方法在实际数据上具有很好的用处。
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-Learning-for-Anomaly-Detection-in-Computational-Workflows"><a href="#Self-supervised-Learning-for-Anomaly-Detection-in-Computational-Workflows" class="headerlink" title="Self-supervised Learning for Anomaly Detection in Computational Workflows"></a>Self-supervised Learning for Anomaly Detection in Computational Workflows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01247">http://arxiv.org/abs/2310.01247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongwei Jin, Krishnan Raghavan, George Papadimitriou, Cong Wang, Anirban Mandal, Ewa Deelman, Prasanna Balaprakash</li>
<li>for: 这个研究旨在探讨计算工作流程中的异常检测问题，以涵盖各领域如防火墙、金融和社交网络等。</li>
<li>methods: 这篇研究使用自动encoder驱动的自我超vised learning（SSL）方法，从无标注的工作流程数据中学习一个总体统计，以评估计算工作流程的正常行为。</li>
<li>results: 研究结果显示，通过估计正常行为的分布在隐藏空间，可以超越现有的异常检测方法在我们的参考数据集上。<details>
<summary>Abstract</summary>
Anomaly detection is the task of identifying abnormal behavior of a system. Anomaly detection in computational workflows is of special interest because of its wide implications in various domains such as cybersecurity, finance, and social networks. However, anomaly detection in computational workflows~(often modeled as graphs) is a relatively unexplored problem and poses distinct challenges. For instance, when anomaly detection is performed on graph data, the complex interdependency of nodes and edges, the heterogeneity of node attributes, and edge types must be accounted for. Although the use of graph neural networks can help capture complex inter-dependencies, the scarcity of labeled anomalous examples from workflow executions is still a significant challenge. To address this problem, we introduce an autoencoder-driven self-supervised learning~(SSL) approach that learns a summary statistic from unlabeled workflow data and estimates the normal behavior of the computational workflow in the latent space. In this approach, we combine generative and contrastive learning objectives to detect outliers in the summary statistics. We demonstrate that by estimating the distribution of normal behavior in the latent space, we can outperform state-of-the-art anomaly detection methods on our benchmark datasets.
</details>
<details>
<summary>摘要</summary>
《异常检测在计算工作流中是一项特殊的任务，因为它在各个领域，如网络安全、金融和社交媒体中具有广泛的应用。然而，在计算工作流中进行异常检测（通常模型为图）是一个相对未经探索的问题，它具有许多独特的挑战。例如，在图数据上进行异常检测时，需要考虑图中节点和边之间的复杂依赖关系，节点属性和边类型的异常性。虽然使用图神经网络可以帮助捕捉图中的复杂依赖关系，但是从计算工作流中获得标注的异常示例还是一个主要的挑战。为解决这个问题，我们提出了一种自动编码器驱动的自我超级vised学习（SSL）方法，该方法通过不supervised learning来学习计算工作流的正常行为的摘要统计。在这种方法中，我们将生成和对比学习目标结合起来，以检测摘要统计中的异常点。我们示示了，通过估计计算工作流的正常行为的分布在隐藏空间，我们可以超越现有的异常检测方法在我们的标准散点集上表现。》
</details></li>
</ul>
<hr>
<h2 id="Modality-aware-Transformer-for-Time-series-Forecasting"><a href="#Modality-aware-Transformer-for-Time-series-Forecasting" class="headerlink" title="Modality-aware Transformer for Time series Forecasting"></a>Modality-aware Transformer for Time series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01232">http://arxiv.org/abs/2310.01232</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hajar Emami, Xuan-Hong Dang, Yousaf Shah, Petros Zerfos</li>
<li>for: 这篇论文主要针对多modal时间序列预测问题，特别是在金融领域，时间序列的未来行为frequently linked to information derived from various textual reports和多个经济指标。</li>
<li>methods: 我们提出了一个名为Modality-aware Transformer的新型多modal transformer-based模型，利用这个模型可以充分利用不同modal的信息，同时实现时间序列预测和多modal跨模式理解。我们在这个模型中开发了一个内置特性级别注意力层，让模型在每个数据模式中对最重要的特性进行注意。</li>
<li>results: 我们的实验结果显示，Modality-aware Transformer在金融数据上比较 existed方法更好，提供了一个新和实际的解决方案 для多modal时间序列预测问题。<details>
<summary>Abstract</summary>
Time series forecasting presents a significant challenge, particularly when its accuracy relies on external data sources rather than solely on historical values. This issue is prevalent in the financial sector, where the future behavior of time series is often intricately linked to information derived from various textual reports and a multitude of economic indicators. In practice, the key challenge lies in constructing a reliable time series forecasting model capable of harnessing data from diverse sources and extracting valuable insights to predict the target time series accurately. In this work, we tackle this challenging problem and introduce a novel multimodal transformer-based model named the Modality-aware Transformer. Our model excels in exploring the power of both categorical text and numerical timeseries to forecast the target time series effectively while providing insights through its neural attention mechanism. To achieve this, we develop feature-level attention layers that encourage the model to focus on the most relevant features within each data modality. By incorporating the proposed feature-level attention, we develop a novel Intra-modal multi-head attention (MHA), Inter-modal MHA and Modality-target MHA in a way that both feature and temporal attentions are incorporated in MHAs. This enables the MHAs to generate temporal attentions with consideration of modality and feature importance which leads to more informative embeddings. The proposed modality-aware structure enables the model to effectively exploit information within each modality as well as foster cross-modal understanding. Our extensive experiments on financial datasets demonstrate that Modality-aware Transformer outperforms existing methods, offering a novel and practical solution to the complex challenges of multi-modality time series forecasting.
</details>
<details>
<summary>摘要</summary>
时间序列预测存在 significativ Challenge，特别是当它的准确性取决于外部数据源而不仅仅是历史值。这个问题在金融领域非常普遍，因为未来时间序列的行为frequently linked to information derived from various textual reports and a multitude of economic indicators。在实践中，关键挑战在于构建可靠的时间序列预测模型，能够从多种数据源中提取有价值的信息，并准确预测目标时间序列。在这种工作中，我们解决这个挑战，并提出了一种新的多modal transformer-based模型，名为Modality-aware Transformer。我们的模型能够effectively explore the power of both categorical text and numerical time series to forecast the target time series accurately while providing insights through its neural attention mechanism。为了实现这一点，我们开发了一种特有的Feature-level attention层，该层鼓励模型对每个数据模式中最相关的特征进行注意力。通过在MHA中 integrate feature-level attention，我们开发了一种新的Intra-modal multi-head attention (MHA)、Inter-modal MHA和Modality-target MHA，其中both feature和 temporal attentions are incorporated in MHAs。这使得MHAs可以生成基于模式和特征重要性的temporal attention，从而生成更有信息的嵌入。我们的模型结构能够effectively exploit information within each modality as well as foster cross-modal understanding。我们对金融数据集进行了广泛的实验，并证明Modality-aware Transformer可以超过现有方法，提供一种新和实用的解决方案 для复杂的多模态时间序列预测问题。
</details></li>
</ul>
<hr>
<h2 id="Reconstructing-Atmospheric-Parameters-of-Exoplanets-Using-Deep-Learning"><a href="#Reconstructing-Atmospheric-Parameters-of-Exoplanets-Using-Deep-Learning" class="headerlink" title="Reconstructing Atmospheric Parameters of Exoplanets Using Deep Learning"></a>Reconstructing Atmospheric Parameters of Exoplanets Using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01227">http://arxiv.org/abs/2310.01227</a></li>
<li>repo_url: None</li>
<li>paper_authors: Flavio Giobergia, Alkis Koudounas, Elena Baralis</li>
<li>for: 这篇论文是为了研究外层行星大气的Properties和特性，提出了一种基于深度学习和反向模型的多目标概率回归方法。</li>
<li>methods: 该方法结合了深度学习和反向模型技术，在多模式架构中实现了大气参数的估算。</li>
<li>results: 该方法可以更好地处理多目标问题，提高了计算效率和准确率，为外层行星研究提供了有价值的新思路和方法。<details>
<summary>Abstract</summary>
Exploring exoplanets has transformed our understanding of the universe by revealing many planetary systems that defy our current understanding. To study their atmospheres, spectroscopic observations are used to infer essential atmospheric properties that are not directly measurable. Estimating atmospheric parameters that best fit the observed spectrum within a specified atmospheric model is a complex problem that is difficult to model. In this paper, we present a multi-target probabilistic regression approach that combines deep learning and inverse modeling techniques within a multimodal architecture to extract atmospheric parameters from exoplanets. Our methodology overcomes computational limitations and outperforms previous approaches, enabling efficient analysis of exoplanetary atmospheres. This research contributes to advancements in the field of exoplanet research and offers valuable insights for future studies.
</details>
<details>
<summary>摘要</summary>
translate_text: 探索外行星已经重新定义了我们对宇宙的理解，揭示了许多不同于我们当前理解的行星系统。为了研究它们的大气，我们使用光谱观测获取不直接测量的大气属性。估算大气参数，使得光谱特征最佳匹配指定的大气模型是一个复杂的问题，具有计算限制。在这篇论文中，我们提出了一种多目标概率回归方法，结合深度学习和反向模型技术，在多Modal 架构中提取大气参数。我们的方法超越计算限制，并超越先前的方法，使得对外行星大气的分析变得效率。这项研究对外行星研究领域的发展做出了贡献，并为未来的研究提供了价值的意见。</SYS>Note: The translation is in Simplified Chinese, which is the standard version of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="A-path-norm-toolkit-for-modern-networks-consequences-promises-and-challenges"><a href="#A-path-norm-toolkit-for-modern-networks-consequences-promises-and-challenges" class="headerlink" title="A path-norm toolkit for modern networks: consequences, promises and challenges"></a>A path-norm toolkit for modern networks: consequences, promises and challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01225">http://arxiv.org/abs/2310.01225</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antoine Gonon, Nicolas Brisebarre, Elisa Riccietti, Rémi Gribonval</li>
<li>for: This paper introduces a toolkit for generalization bounds of modern neural networks, specifically for DAG ReLU networks with biases, skip connections, and any operation based on the extraction of order statistics.</li>
<li>methods: The toolkit uses path-norms, which are a type of complexity measure that is easy to compute, invariant under network symmetries, and improves sharpness on feedforward networks.</li>
<li>results: The paper establishes generalization bounds for modern neural networks that are the most widely applicable and recover or beat the sharpest known bounds of this type. The toolkit is also used to numerically evaluate the sharpest known bounds for ResNets on ImageNet.<details>
<summary>Abstract</summary>
This work introduces the first toolkit around path-norms that is fully able to encompass general DAG ReLU networks with biases, skip connections and any operation based on the extraction of order statistics: max pooling, GroupSort etc. This toolkit notably allows us to establish generalization bounds for modern neural networks that are not only the most widely applicable path-norm based ones, but also recover or beat the sharpest known bounds of this type. These extended path-norms further enjoy the usual benefits of path-norms: ease of computation, invariance under the symmetries of the network, and improved sharpness on feedforward networks compared to the product of operators' norms, another complexity measure most commonly used.   The versatility of the toolkit and its ease of implementation allow us to challenge the concrete promises of path-norm-based generalization bounds, by numerically evaluating the sharpest known bounds for ResNets on ImageNet.
</details>
<details>
<summary>摘要</summary>
这个工具包 introduces the first toolkit around path-norms that can fully encompass general DAG ReLU networks with biases, skip connections, and any operation based on the extraction of order statistics: max pooling, GroupSort, etc. This toolkit allows us to establish generalization bounds for modern neural networks that are not only the most widely applicable path-norm based ones, but also recover or beat the sharpest known bounds of this type. These extended path-norms also enjoy the usual benefits of path-norms: ease of computation, invariance under the symmetries of the network, and improved sharpness on feedforward networks compared to the product of operators' norms, another complexity measure most commonly used. 该工具的多样性和易用性，使我们能够 numerically evaluate the sharpest known bounds for ResNets on ImageNet, challenging the concrete promises of path-norm-based generalization bounds.
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Mobility-Modeling-with-Graph-A-Graph-Transformer-Model-for-Next-Point-of-Interest-Recommendation"><a href="#Revisiting-Mobility-Modeling-with-Graph-A-Graph-Transformer-Model-for-Next-Point-of-Interest-Recommendation" class="headerlink" title="Revisiting Mobility Modeling with Graph: A Graph Transformer Model for Next Point-of-Interest Recommendation"></a>Revisiting Mobility Modeling with Graph: A Graph Transformer Model for Next Point-of-Interest Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01224">http://arxiv.org/abs/2310.01224</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yukayo/mobgt">https://github.com/yukayo/mobgt</a></li>
<li>paper_authors: Xiaohang Xu, Toyotaro Suzumura, Jiawei Yong, Masatoshi Hanai, Chuang Yang, Hiroki Kanezashi, Renhe Jiang, Shintaro Fukushima</li>
<li>for: 本研究旨在提出一种能够充分利用图模型来捕捉用户流动数据中的空间和时间特征的POI推荐模型。</li>
<li>methods: 该模型基于图神经网络（GNN），并将个体空间和时间图嵌入器与全球用户位置关系嵌入器结合，以捕捉唯一的特征。此外，模型还包括基于图变换器的流动嵌入器，以提取更高级别的POI之间关系。</li>
<li>results: 实验结果表明，MobGT模型在多个数据集和指标上都有显著提高，相比之前的模型，平均提高24%。 codes 可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/Yukayo/MobGT%7D">https://github.com/Yukayo/MobGT}</a> 上获取。<details>
<summary>Abstract</summary>
Next Point-of-Interest (POI) recommendation plays a crucial role in urban mobility applications. Recently, POI recommendation models based on Graph Neural Networks (GNN) have been extensively studied and achieved, however, the effective incorporation of both spatial and temporal information into such GNN-based models remains challenging. Extracting distinct fine-grained features unique to each piece of information is difficult since temporal information often includes spatial information, as users tend to visit nearby POIs. To address the challenge, we propose \textbf{\underline{Mob}ility \textbf{\underline{G}raph \textbf{\underline{T}ransformer (MobGT) that enables us to fully leverage graphs to capture both the spatial and temporal features in users' mobility patterns. MobGT combines individual spatial and temporal graph encoders to capture unique features and global user-location relations. Additionally, it incorporates a mobility encoder based on Graph Transformer to extract higher-order information between POIs. To address the long-tailed problem in spatial-temporal data, MobGT introduces a novel loss function, Tail Loss. Experimental results demonstrate that MobGT outperforms state-of-the-art models on various datasets and metrics, achieving 24\% improvement on average. Our codes are available at \url{https://github.com/Yukayo/MobGT}.
</details>
<details>
<summary>摘要</summary>
Next Point-of-Interest (POI) recommendation plays a crucial role in urban mobility applications. Recently, POI recommendation models based on Graph Neural Networks (GNN) have been extensively studied and achieved, but the effective incorporation of both spatial and temporal information into such GNN-based models remains challenging. Extracting distinct fine-grained features unique to each piece of information is difficult since temporal information often includes spatial information, as users tend to visit nearby POIs. To address the challenge, we propose \textbf{\underline{Mobile} \textbf{\underline{Graph} \textbf{\underline{Transformer} (MobGT) that enables us to fully leverage graphs to capture both the spatial and temporal features in users' mobility patterns. MobGT combines individual spatial and temporal graph encoders to capture unique features and global user-location relations. Additionally, it incorporates a mobility encoder based on Graph Transformer to extract higher-order information between POIs. To address the long-tailed problem in spatial-temporal data, MobGT introduces a novel loss function, Tail Loss. Experimental results demonstrate that MobGT outperforms state-of-the-art models on various datasets and metrics, achieving 24\% improvement on average. Our codes are available at \url{https://github.com/Yukayo/MobGT}.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="From-Bricks-to-Bridges-Product-of-Invariances-to-Enhance-Latent-Space-Communication"><a href="#From-Bricks-to-Bridges-Product-of-Invariances-to-Enhance-Latent-Space-Communication" class="headerlink" title="From Bricks to Bridges: Product of Invariances to Enhance Latent Space Communication"></a>From Bricks to Bridges: Product of Invariances to Enhance Latent Space Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01211">http://arxiv.org/abs/2310.01211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Irene Cannistraci, Luca Moschella, Marco Fumero, Valentino Maiorca, Emanuele Rodolà</li>
<li>for: 提高 neural network 模块的重用和合并性能</li>
<li>methods: 直接 incorporate 一组抽象到 latent representation 中的几何变换，无需先知道优化的抽象</li>
<li>results: 在 classification 和重建任务中，观察了一致的潜在相似性和下游性能提升在零shot合并设定下<details>
<summary>Abstract</summary>
It has been observed that representations learned by distinct neural networks conceal structural similarities when the models are trained under similar inductive biases. From a geometric perspective, identifying the classes of transformations and the related invariances that connect these representations is fundamental to unlocking applications, such as merging, stitching, and reusing different neural modules. However, estimating task-specific transformations a priori can be challenging and expensive due to several factors (e.g., weights initialization, training hyperparameters, or data modality). To this end, we introduce a versatile method to directly incorporate a set of invariances into the representations, constructing a product space of invariant components on top of the latent representations without requiring prior knowledge about the optimal invariance to infuse. We validate our solution on classification and reconstruction tasks, observing consistent latent similarity and downstream performance improvements in a zero-shot stitching setting. The experimental analysis comprises three modalities (vision, text, and graphs), twelve pretrained foundational models, eight benchmarks, and several architectures trained from scratch.
</details>
<details>
<summary>摘要</summary>
各种神经网络学习的表示器可能会隐藏同类结构的相似性，当模型在类似启发假设下训练时。从几何角度来看，找出这些表示器中的类别转换和相关的免疫性是解锁应用的关键，如合并、缝合和重用不同神经模块。然而，在不同任务上预先估算任务特定的转换可能是困难和昂贵的，因为多种因素（例如权重初始化、训练超参数和数据类型）。为此，我们提出了一种通用的方法，直接将一组免疫性 incorporated 到表示器中，在幂 space 上构建免疫性的产品空间，无需先知道最佳免疫性。我们在分类和重建任务上验证了我们的解决方案，观察到了静态相似性和下游性能提升。实验分析包括三种模式（视觉、文本和图）、十二个预训练基础模型、八个标准核心和多个从零开始训练的建筑。
</details></li>
</ul>
<hr>
<h2 id="Unified-Uncertainty-Calibration"><a href="#Unified-Uncertainty-Calibration" class="headerlink" title="Unified Uncertainty Calibration"></a>Unified Uncertainty Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01202">http://arxiv.org/abs/2310.01202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kamalika Chaudhuri, David Lopez-Paz</li>
<li>for: 提高AI系统的稳定性、公平性和安全性，让分类器在测试示例上采取“我不知道”的决策。</li>
<li>methods: 提出了一种名为“统一不确定性均衡（U2C）”的框架，将 aleatoric 和 epistemic 不确定性集成到一起，以便对不确定性进行有效的学习理论分析，并在 ImageNet benchmark 上超越了 reject-or-classify 策略。</li>
<li>results: U2C 在 ImageNet 上实现了比 reject-or-classify 更高的性能，并且可以更好地捕捉不同来源的不确定性，进一步提高了 AI 系统的稳定性、公平性和安全性。<details>
<summary>Abstract</summary>
To build robust, fair, and safe AI systems, we would like our classifiers to say ``I don't know'' when facing test examples that are difficult or fall outside of the training classes.The ubiquitous strategy to predict under uncertainty is the simplistic \emph{reject-or-classify} rule: abstain from prediction if epistemic uncertainty is high, classify otherwise.Unfortunately, this recipe does not allow different sources of uncertainty to communicate with each other, produces miscalibrated predictions, and it does not allow to correct for misspecifications in our uncertainty estimates. To address these three issues, we introduce \emph{unified uncertainty calibration (U2C)}, a holistic framework to combine aleatoric and epistemic uncertainties. U2C enables a clean learning-theoretical analysis of uncertainty estimation, and outperforms reject-or-classify across a variety of ImageNet benchmarks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SWoTTeD-An-Extension-of-Tensor-Decomposition-to-Temporal-Phenotyping"><a href="#SWoTTeD-An-Extension-of-Tensor-Decomposition-to-Temporal-Phenotyping" class="headerlink" title="SWoTTeD: An Extension of Tensor Decomposition to Temporal Phenotyping"></a>SWoTTeD: An Extension of Tensor Decomposition to Temporal Phenotyping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01201">http://arxiv.org/abs/2310.01201</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hana Sebia, Thomas Guyet, Etienne Audureau</li>
<li>for: 本研究旨在探讨electronic health records（EHR）数据中的个体轨迹分析，并提出了一种基于时间fenotype的新方法SWoTTeD（Sliding Window for Temporal Tensor Decomposition）来挖掘隐藏的时间模式。</li>
<li>methods: 本研究提出了一种 integrate several constraints and regularizations的方法SWoTTeD，以增强 extracted phenotypes的解释性。</li>
<li>results: 通过synthetic和实际数据 validate，SWoTTeD可以与最新的tensor decomposition模型匹配或超越它们，并提取了 meaningful for clinicians的时间 fenotypes。<details>
<summary>Abstract</summary>
Tensor decomposition has recently been gaining attention in the machine learning community for the analysis of individual traces, such as Electronic Health Records (EHR). However, this task becomes significantly more difficult when the data follows complex temporal patterns. This paper introduces the notion of a temporal phenotype as an arrangement of features over time and it proposes SWoTTeD (Sliding Window for Temporal Tensor Decomposition), a novel method to discover hidden temporal patterns. SWoTTeD integrates several constraints and regularizations to enhance the interpretability of the extracted phenotypes. We validate our proposal using both synthetic and real-world datasets, and we present an original usecase using data from the Greater Paris University Hospital. The results show that SWoTTeD achieves at least as accurate reconstruction as recent state-of-the-art tensor decomposition models, and extracts temporal phenotypes that are meaningful for clinicians.
</details>
<details>
<summary>摘要</summary>
Recently, tensor decomposition has been gaining attention in the machine learning community for analyzing individual traces, such as Electronic Health Records (EHR). However, this task becomes significantly more difficult when the data follows complex temporal patterns. This paper introduces the concept of a temporal phenotype as an arrangement of features over time and proposes SWoTTeD (Sliding Window for Temporal Tensor Decomposition), a novel method to discover hidden temporal patterns. SWoTTeD incorporates several constraints and regularizations to enhance the interpretability of the extracted phenotypes. We validate our proposal using both synthetic and real-world datasets and present an original use case using data from the Greater Paris University Hospital. The results show that SWoTTeD achieves at least as accurate reconstruction as recent state-of-the-art tensor decomposition models and extracts temporal phenotypes that are meaningful for clinicians.Here's the Chinese text with traditional characters:近期，tensor decomposition在机器学习社区内已引起关注，用于分析个体轨迹，如电子医疗记录（EHR）。然而，当数据表现出复杂的时间模式时，这种任务变得非常困难。这篇论文提出了时间型现象（temporal phenotype）的概念，即时间上的特征排列，并提出了SWoTTeD（Sliding Window for Temporal Tensor Decomposition），一种新的方法，用于发现隐藏的时间模式。SWoTTeDintegrates several constraints and regularizations to enhance the interpretability of the extracted phenotypes。我们验证了我们的提议使用了 both synthetic和实际数据集，并提供了一个原创的用例，使用了法国巴黎大学医院的数据。结果表明，SWoTTeD在最近的状态艺术tensor decomposition模型中至少具有相同的准确重建能力，并提取了有意义的临床型现象。
</details></li>
</ul>
<hr>
<h2 id="Federated-K-means-Clustering"><a href="#Federated-K-means-Clustering" class="headerlink" title="Federated K-means Clustering"></a>Federated K-means Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01195">http://arxiv.org/abs/2310.01195</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ourownstory/federated_kmeans">https://github.com/ourownstory/federated_kmeans</a></li>
<li>paper_authors: Swier Garst, Marcel Reinders</li>
<li>for: 本研究旨在提出一种基于 federated learning 的 K-means 嵌入 clustering 算法，以保持数据隐私和拥有权。</li>
<li>methods: 该算法使用 federated averaging 方法，并采用一种新的聚合策略来Address the challenges of varying number of clusters between centers 和 less separable datasets。</li>
<li>results: 实验结果表明，该算法能够在不同数据中心之间的不同数据分布下准确地进行嵌入 clustering，并且在 less separable datasets 上具有更高的鲁棒性和稳定性。<details>
<summary>Abstract</summary>
Federated learning is a technique that enables the use of distributed datasets for machine learning purposes without requiring data to be pooled, thereby better preserving privacy and ownership of the data. While supervised FL research has grown substantially over the last years, unsupervised FL methods remain scarce. This work introduces an algorithm which implements K-means clustering in a federated manner, addressing the challenges of varying number of clusters between centers, as well as convergence on less separable datasets.
</details>
<details>
<summary>摘要</summary>
设置语言为简化中文。 Federated learning 是一种技术，允许在分布式数据集上进行机器学习，而不需要数据集集中化，从而更好地保护数据隐私和所有权。  although supervised FL research has grown substantially in recent years, unsupervised FL methods are still scarce. This work introduces an algorithm that implements K-means clustering in a federated manner, addressing the challenges of varying number of clusters between centers, as well as convergence on less separable datasets.Note: "简化中文" (Simplified Chinese) is a romanization of Chinese characters, which is used in mainland China and Singapore. It is different from "traditional Chinese" (Traditional Chinese) which is used in Hong Kong, Taiwan, and other countries.
</details></li>
</ul>
<hr>
<h2 id="If-there-is-no-underfitting-there-is-no-Cold-Posterior-Effect"><a href="#If-there-is-no-underfitting-there-is-no-Cold-Posterior-Effect" class="headerlink" title="If there is no underfitting, there is no Cold Posterior Effect"></a>If there is no underfitting, there is no Cold Posterior Effect</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01189">http://arxiv.org/abs/2310.01189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yijie Zhang, Yi-Shan Wu, Luis A. Ortega, Andrés R. Masegosa</li>
<li>for: 这篇论文研究了温 posterior effect（CPE）在 bayesian deep learning 中的存在，并发现在温度 $T&lt;1$ 下， posterior predictive 可能会比 bayesian posterior ($T&#x3D;1$) 更好。</li>
<li>methods: 这篇论文使用了 bayesian deep learning 方法，并研究了 CPE 是否为模型误差问题。</li>
<li>results: 这篇论文发现，如果存在过度适应（underfitting），那么 CPE 会出现；如果没有过度适应，那么 CPE 不会出现。<details>
<summary>Abstract</summary>
The cold posterior effect (CPE) (Wenzel et al., 2020) in Bayesian deep learning shows that, for posteriors with a temperature $T<1$, the resulting posterior predictive could have better performances than the Bayesian posterior ($T=1$). As the Bayesian posterior is known to be optimal under perfect model specification, many recent works have studied the presence of CPE as a model misspecification problem, arising from the prior and/or from the likelihood function. In this work, we provide a more nuanced understanding of the CPE as we show that misspecification leads to CPE only when the resulting Bayesian posterior underfits. In fact, we theoretically show that if there is no underfitting, there is no CPE.
</details>
<details>
<summary>摘要</summary>
冷后效应（CPE）（文泽等，2020）在 bayesian深度学习中显示，当 posterior 的温度 $T<1$ 时，结果的 posterior predictive 可能比 bayesian posterior ($T=1）更好。由于 bayesian posterior 在完美模型假设下是最优的，因此许多最近的工作都在研究 CPE 是模型假设错误的问题，来自 prior 和/或 likelihood 函数。在这个工作中，我们提供了更加细腻的理解 CPE，显示了 misspecification 导致 CPE 只有当 bayesian posterior 下降时。事实上，我们理论上表明，如果没有下降，就没有 CPE。Note: "冷后效应" (CPE) is the Chinese translation of "cold posterior effect".
</details></li>
</ul>
<hr>
<h2 id="Light-Schrodinger-Bridge"><a href="#Light-Schrodinger-Bridge" class="headerlink" title="Light Schrödinger Bridge"></a>Light Schrödinger Bridge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01174">http://arxiv.org/abs/2310.01174</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ngushchin/lightsb">https://github.com/ngushchin/lightsb</a></li>
<li>paper_authors: Alexander Korotin, Nikita Gushchin, Evgeny Burnaev</li>
<li>for: This paper aims to address the issue of heavy-weighted and complex optimization of existing Schrodinger Bridges (SB) solvers, and proposes a novel fast and simple SB solver.</li>
<li>methods: The proposed LightSB solver combines two ideas from the field: parameterizing the Schrodinger potentials with sum-exp quadratic functions, and viewing the log-Schrodinger potentials as energy functions. The optimization objective is simple and straightforward, and the solver is lightweight, simulation-free, and theoretically justified.</li>
<li>results: The LightSB solver is able to solve SB in moderate dimensions in a matter of minutes on CPU without painful hyperparameter selection, and is proven to be a universal approximator of SBs. The code for the LightSB solver is available at <a target="_blank" rel="noopener" href="https://github.com/ngushchin/LightSB">https://github.com/ngushchin/LightSB</a>.<details>
<summary>Abstract</summary>
Despite the recent advances in the field of computational Schrodinger Bridges (SB), most existing SB solvers are still heavy-weighted and require complex optimization of several neural networks. It turns out that there is no principal solver which plays the role of simple-yet-effective baseline for SB just like, e.g., $k$-means method in clustering, logistic regression in classification or Sinkhorn algorithm in discrete optimal transport. We address this issue and propose a novel fast and simple SB solver. Our development is a smart combination of two ideas which recently appeared in the field: (a) parameterization of the Schrodinger potentials with sum-exp quadratic functions and (b) viewing the log-Schrodinger potentials as the energy functions. We show that combined together these ideas yield a lightweight, simulation-free and theoretically justified SB solver with a simple straightforward optimization objective. As a result, it allows solving SB in moderate dimensions in a matter of minutes on CPU without a painful hyperparameter selection. Our light solver resembles the Gaussian mixture model which is widely used for density estimation. Inspired by this similarity, we also prove an important theoretical result showing that our light solver is a universal approximator of SBs. The code for the LightSB solver can be found at https://github.com/ngushchin/LightSB
</details>
<details>
<summary>摘要</summary>
Despite recent advances in computational Schrödinger bridges (SB), most existing solvers are still computationally expensive and require complex optimization of multiple neural networks. There is no simple yet effective baseline solver for SB, similar to methods like $k$-means in clustering, logistic regression in classification, or the Sinkhorn algorithm in discrete optimal transport. We address this issue and propose a novel fast and simple SB solver. Our approach combines two recent ideas in the field: (a) parameterizing the Schrödinger potentials with sum-exp quadratic functions, and (b) viewing the log-Schrödinger potentials as energy functions. By combining these ideas, we obtain a lightweight, simulation-free, and theoretically justified SB solver with a simple and straightforward optimization objective. This allows for solving SB in moderate dimensions in just a few minutes on CPU without painful hyperparameter selection. Our light solver is similar to the Gaussian mixture model, which is widely used for density estimation. Inspired by this similarity, we also prove an important theoretical result showing that our light solver is a universal approximator of SBs. The code for the LightSB solver can be found at https://github.com/ngushchin/LightSB.
</details></li>
</ul>
<hr>
<h2 id="RRR-Net-Reusing-Reducing-and-Recycling-a-Deep-Backbone-Network"><a href="#RRR-Net-Reusing-Reducing-and-Recycling-a-Deep-Backbone-Network" class="headerlink" title="RRR-Net: Reusing, Reducing, and Recycling a Deep Backbone Network"></a>RRR-Net: Reusing, Reducing, and Recycling a Deep Backbone Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01157">http://arxiv.org/abs/2310.01157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haozhe Sun, Isabelle Guyon, Felix Mohr, Hedi Tabia</li>
<li>for: 这篇论文的目的是创建一个较小的、较快的模型，并且保持与大型背景网络相似的性能。</li>
<li>methods: 这篇论文使用了将预训练的大型背景网络（ResNet152）缩减为5个块，并分割模型为多个分支来提高性能。</li>
<li>results: 这篇论文的实验结果显示，使用这些技术可以创建一个较小的、较快的模型，并且与传统的背景网络组合相似的性能。<details>
<summary>Abstract</summary>
It has become mainstream in computer vision and other machine learning domains to reuse backbone networks pre-trained on large datasets as preprocessors. Typically, the last layer is replaced by a shallow learning machine of sorts; the newly-added classification head and (optionally) deeper layers are fine-tuned on a new task. Due to its strong performance and simplicity, a common pre-trained backbone network is ResNet152.However, ResNet152 is relatively large and induces inference latency. In many cases, a compact and efficient backbone with similar performance would be preferable over a larger, slower one. This paper investigates techniques to reuse a pre-trained backbone with the objective of creating a smaller and faster model. Starting from a large ResNet152 backbone pre-trained on ImageNet, we first reduce it from 51 blocks to 5 blocks, reducing its number of parameters and FLOPs by more than 6 times, without significant performance degradation. Then, we split the model after 3 blocks into several branches, while preserving the same number of parameters and FLOPs, to create an ensemble of sub-networks to improve performance. Our experiments on a large benchmark of $40$ image classification datasets from various domains suggest that our techniques match the performance (if not better) of ``classical backbone fine-tuning'' while achieving a smaller model size and faster inference speed.
</details>
<details>
<summary>摘要</summary>
现在计算机视觉和其他机器学习领域中已成为主流的做法是 reuse 已经预训练的基准网络。通常是将最后一层替换为一个浅学习机制，新增的分类头和（选项别）更深的层进行 fine-tuning 新任务。由于其强大的表现和简单性，一个常见的预训练基准网络是 ResNet152。然而，ResNet152 相对较大，导致推理延迟。在许多情况下，一个更加 компакт和高效的基准网络会更有优势于一个更大和更慢的网络。这篇论文研究了如何重用预训练的基准网络，以创建一个更小更快的模型。从 ImageNet 预训练的大 ResNet152 基准网络开始，我们首先将其减少为 5 层，从而减少参数和 FLOPs 的数量比 exceeds 6 times，无需重要性下降。然后，我们在 3 层处将模型分割成多个分支，保持同样的参数和 FLOPs 数量，以创建一个 ensemble 的子网络，以提高性能。我们对 $40$ 个图像分类数据集进行了大规模的实验，结果表明，我们的技术与“经典基准网络精度”匹配或更好，同时实现了更小的模型大小和更快的推理速度。
</details></li>
</ul>
<hr>
<h2 id="Modularity-in-Deep-Learning-A-Survey"><a href="#Modularity-in-Deep-Learning-A-Survey" class="headerlink" title="Modularity in Deep Learning: A Survey"></a>Modularity in Deep Learning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01154">http://arxiv.org/abs/2310.01154</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ckg-DeepLearning/CoverTypeClassification">https://github.com/ckg-DeepLearning/CoverTypeClassification</a></li>
<li>paper_authors: Haozhe Sun, Isabelle Guyon</li>
<li>For: This paper reviews the concept of modularity in deep learning, focusing on three axes: data, task, and model.* Methods: The paper discusses different instantiations of the modularity principle in deep learning, including data modularity, task modularity, and model modularity.* Results: The paper provides a comprehensive overview of the advantages of modularity in deep learning, including ease of conceptualization, interpretability, scalability, module combinability, and module reusability.Here is the same information in Simplified Chinese text:* For: 这篇论文探讨了深度学习中的模块化原则，围绕着数据、任务和模型三个轴线进行了详细的介绍。* Methods: 论文讲述了不同的模块化实现方式，包括数据模块化、任务模块化和模型模块化。* Results: 论文提供了深度学习中模块化的优点，包括易于理解、可读性、可扩展性、模块可组合性和模块可重用性等。<details>
<summary>Abstract</summary>
Modularity is a general principle present in many fields. It offers attractive advantages, including, among others, ease of conceptualization, interpretability, scalability, module combinability, and module reusability. The deep learning community has long sought to take inspiration from the modularity principle, either implicitly or explicitly. This interest has been increasing over recent years. We review the notion of modularity in deep learning around three axes: data, task, and model, which characterize the life cycle of deep learning. Data modularity refers to the observation or creation of data groups for various purposes. Task modularity refers to the decomposition of tasks into sub-tasks. Model modularity means that the architecture of a neural network system can be decomposed into identifiable modules. We describe different instantiations of the modularity principle, and we contextualize their advantages in different deep learning sub-fields. Finally, we conclude the paper with a discussion of the definition of modularity and directions for future research.
</details>
<details>
<summary>摘要</summary>
modularity是一种通用的原则，存在很多领域中。它带来了一些优点，包括易于概念、可解释性、可扩展性、模块可组合性和模块可重用性。深度学习社区长期寻求从模块原则中得到灵感，或者直接或间接地。这些 интерест在最近几年内不断增长。本文将对深度学习中的模块原理进行评论，分为三个轴：数据、任务和模型，这些轴描述了深度学习生命周期。数据模块性指的是将数据分组为不同目的。任务模块性指的是将任务拆分成子任务。模型模块性是指神经网络系统的架构可以被分解成可识别的模块。我们将介绍不同的模块原理的实现方式，并在不同的深度学习子领域中评估其优点。最后，我们将结束这篇论文，并对模块原理的定义和未来研究的方向进行讨论。
</details></li>
</ul>
<hr>
<h2 id="Cryptocurrency-Portfolio-Optimization-by-Neural-Networks"><a href="#Cryptocurrency-Portfolio-Optimization-by-Neural-Networks" class="headerlink" title="Cryptocurrency Portfolio Optimization by Neural Networks"></a>Cryptocurrency Portfolio Optimization by Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01148">http://arxiv.org/abs/2310.01148</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quoc Minh Nguyen, Dat Thanh Tran, Juho Kanniainen, Alexandros Iosifidis, Moncef Gabbouj</li>
<li>for: 提出了一种基于神经网络的算法，用于利用现代 cryptocurrency 投资产品进行减少风险或投资。</li>
<li>methods: 使用深度神经网络，输出每个时间间隔的资产分配重量，并通过最大化尖峰比率来减少网络偏袋性。</li>
<li>results: 经过19个月的Backtest测试，提出的算法可以生成能够在不同市场情况下实现盈利的神经网络。<details>
<summary>Abstract</summary>
Many cryptocurrency brokers nowadays offer a variety of derivative assets that allow traders to perform hedging or speculation. This paper proposes an effective algorithm based on neural networks to take advantage of these investment products. The proposed algorithm constructs a portfolio that contains a pair of negatively correlated assets. A deep neural network, which outputs the allocation weight of each asset at a time interval, is trained to maximize the Sharpe ratio. A novel loss term is proposed to regulate the network's bias towards a specific asset, thus enforcing the network to learn an allocation strategy that is close to a minimum variance strategy. Extensive experiments were conducted using data collected from Binance spanning 19 months to evaluate the effectiveness of our approach. The backtest results show that the proposed algorithm can produce neural networks that are able to make profits in different market situations.
</details>
<details>
<summary>摘要</summary>
很多现代加密货币经纪人现在提供一种多种Derivative资产，让投资者进行减风险或投机投资。这篇论文提出了一种基于神经网络的有效算法，用于利用这些投资产品。提议的算法构建了一个包含一对相互负相关资产的资产组合。一个深度神经网络，输出每个时间间隔的每个资产的投资Weight，通过最大化Sharpe比来寻找最佳投资策略。提出了一个新的损失函数，以规避神经网络偏好某个资产，从而使神经网络学习一种减风险的投资策略。对 Binance 收集的19个月的数据进行了广泛的实验测试，以评估我们的方法的有效性。backtest结果表明，我们的算法可以生成能够在不同市场情况下产生利润的神经网络。
</details></li>
</ul>
<hr>
<h2 id="Parallel-in-Time-Probabilistic-Numerical-ODE-Solvers"><a href="#Parallel-in-Time-Probabilistic-Numerical-ODE-Solvers" class="headerlink" title="Parallel-in-Time Probabilistic Numerical ODE Solvers"></a>Parallel-in-Time Probabilistic Numerical ODE Solvers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01145">http://arxiv.org/abs/2310.01145</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nathanaelbosch/parallel-in-time-ode-filters">https://github.com/nathanaelbosch/parallel-in-time-ode-filters</a></li>
<li>paper_authors: Nathanael Bosch, Adrien Corenflos, Fatemeh Yaghoobi, Filip Tronarp, Philipp Hennig, Simo Särkkä</li>
<li>for:  numerical simulation of dynamical systems as problems of Bayesian state estimation</li>
<li>methods:  time-parallel formulation of iterated extended Kalman smoothers</li>
<li>results:  reduced span cost from linear to logarithmic in the number of time steps<details>
<summary>Abstract</summary>
Probabilistic numerical solvers for ordinary differential equations (ODEs) treat the numerical simulation of dynamical systems as problems of Bayesian state estimation. Aside from producing posterior distributions over ODE solutions and thereby quantifying the numerical approximation error of the method itself, one less-often noted advantage of this formalism is the algorithmic flexibility gained by formulating numerical simulation in the framework of Bayesian filtering and smoothing. In this paper, we leverage this flexibility and build on the time-parallel formulation of iterated extended Kalman smoothers to formulate a parallel-in-time probabilistic numerical ODE solver. Instead of simulating the dynamical system sequentially in time, as done by current probabilistic solvers, the proposed method processes all time steps in parallel and thereby reduces the span cost from linear to logarithmic in the number of time steps. We demonstrate the effectiveness of our approach on a variety of ODEs and compare it to a range of both classic and probabilistic numerical ODE solvers.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传送给定文本到简化中文。</SYS>>概率数学方法 для常微分方程（ODE）视数学动力系统的数字 simulate 为某种抽象 Bayesian 状态估计问题。除了生成 posterior 分布于 ODE 解和数字方法自身的误差外，这种形式主义还具有通过 Bayesian 滤波和平滑来获得的算法 Fleibility。在这篇文章中，我们利用这种灵活性，并基于时间平行的迭代扩展 Kalman 平滑器来构建一种并发在时间上的概率数学 ODE 解决方案。而不是在时间序列中顺序地 simulate 动力系统，这种方法在所有时间步长进行并行处理，从而将 span 成本由线性降低到对数型。我们在各种 ODE 上测试了我们的方法，并与经典和概率数学 ODE 解决方案进行了比较。
</details></li>
</ul>
<hr>
<h2 id="The-Map-Equation-Goes-Neural"><a href="#The-Map-Equation-Goes-Neural" class="headerlink" title="The Map Equation Goes Neural"></a>The Map Equation Goes Neural</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01144">http://arxiv.org/abs/2310.01144</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher Blöcker, Chester Tan, Ingo Scholtes</li>
<li>for: 本研究旨在bridging深度学习和网络科学两个领域，提出一种基于深度学习的社区检测方法，以便自动找出高级组织结构。</li>
<li>methods: 我们使用了map方程，一种信息论函数来实现社区检测。将其表示为完全可导的矩阵形式，然后通过梯度下降优化。这种方法可以兼容任何图神经网络架构，从而实现灵活的归一化和图 pooling，自动找出最佳数量的群集，并且自然地检测 overlap 社区。</li>
<li>results: 我们通过实验表明，我们的方法可以与基eline比肩，自动找出最佳数量的群集，避免了稀疏图的过分 partitioning。我们的方法还能够自然地检测 overlap 社区，并且不需要手动添加辅助特征。<details>
<summary>Abstract</summary>
Community detection and graph clustering are essential for unsupervised data exploration and understanding the high-level organisation of networked systems. Recently, graph clustering has been highlighted as an under-explored primary task for graph neural networks. While hierarchical graph pooling has been shown to improve performance in graph and node classification tasks, it performs poorly in identifying meaningful clusters. Community detection has a long history in network science, but typically relies on optimising objective functions with custom-tailored search algorithms, not leveraging recent advances in deep learning, particularly from graph neural networks. In this paper, we narrow this gap between the deep learning and network science communities. We consider the map equation, an information-theoretic objective function for community detection. Expressing it in a fully differentiable tensor form that produces soft cluster assignments, we optimise the map equation with deep learning through gradient descent. More specifically, the reformulated map equation is a loss function compatible with any graph neural network architecture, enabling flexible clustering and graph pooling that clusters both graph structure and data features in an end-to-end way, automatically finding an optimum number of clusters without explicit regularisation. We evaluate our approach experimentally using different neural network architectures for unsupervised clustering in synthetic and real data. Our results show that our approach achieves competitive performance against baselines, naturally detects overlapping communities, and avoids over-partitioning sparse graphs.
</details>
<details>
<summary>摘要</summary>
In this paper, we bridge the gap between the deep learning and network science communities by considering the map equation, an information-theoretic objective function for community detection. We express it in a fully differentiable tensor form that produces soft cluster assignments, and we optimize it with deep learning through gradient descent. Our approach is compatible with any graph neural network architecture, allowing for flexible clustering and graph pooling that simultaneously clusters both graph structure and data features in an end-to-end manner. This enables the automatic discovery of an optimal number of clusters without explicit regularization.We evaluate our approach experimentally using different neural network architectures for unsupervised clustering in synthetic and real data. Our results show that our approach achieves competitive performance against baselines, naturally detects overlapping communities, and avoids over-partitioning sparse graphs.
</details></li>
</ul>
<hr>
<h2 id="CommIN-Semantic-Image-Communications-as-an-Inverse-Problem-with-INN-Guided-Diffusion-Models"><a href="#CommIN-Semantic-Image-Communications-as-an-Inverse-Problem-with-INN-Guided-Diffusion-Models" class="headerlink" title="CommIN: Semantic Image Communications as an Inverse Problem with INN-Guided Diffusion Models"></a>CommIN: Semantic Image Communications as an Inverse Problem with INN-Guided Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01130">http://arxiv.org/abs/2310.01130</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiakang Chen, Di You, Deniz Gündüz, Pier Luigi Dragotti</li>
<li>for: 提高无线图像传输中的品质</li>
<li>methods: 使用傅立叶网络（INN）和扩散模型</li>
<li>results: 在极端条件下（如低带宽和低信号噪比）显著改善图像的品质<details>
<summary>Abstract</summary>
Joint source-channel coding schemes based on deep neural networks (DeepJSCC) have recently achieved remarkable performance for wireless image transmission. However, these methods usually focus only on the distortion of the reconstructed signal at the receiver side with respect to the source at the transmitter side, rather than the perceptual quality of the reconstruction which carries more semantic information. As a result, severe perceptual distortion can be introduced under extreme conditions such as low bandwidth and low signal-to-noise ratio. In this work, we propose CommIN, which views the recovery of high-quality source images from degraded reconstructions as an inverse problem. To address this, CommIN combines Invertible Neural Networks (INN) with diffusion models, aiming for superior perceptual quality. Through experiments, we show that our CommIN significantly improves the perceptual quality compared to DeepJSCC under extreme conditions and outperforms other inverse problem approaches used in DeepJSCC.
</details>
<details>
<summary>摘要</summary>
joint source-channel coding schemes based on deep neural networks (DeepJSCC) 已经取得了对无线影像传输中的出色表现。然而，这些方法通常仅专注于从接收端传输端到源端的变数的干扰，而不是传输端对源端的semantic信息传输的质量。因此，在极端情况下，如低带宽和低信号至杂音比，可能导致严重的semantic扭曲。在这个工作中，我们提出了CommIN，它视为从损坏重建中恢复高质量源影像的问题为一个逆问题。为解决这个问题，CommIN结合了反射神经网络（INN）和扩散模型，实现了更好的semantic质量。经过实验，我们发现CommIN在极端情况下比DeepJSCC更有优秀的semantic质量表现，并且在其他 inverse problem 方法上进行了比较。
</details></li>
</ul>
<hr>
<h2 id="Predicting-emergence-of-crystals-from-amorphous-matter-with-deep-learning"><a href="#Predicting-emergence-of-crystals-from-amorphous-matter-with-deep-learning" class="headerlink" title="Predicting emergence of crystals from amorphous matter with deep learning"></a>Predicting emergence of crystals from amorphous matter with deep learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01117">http://arxiv.org/abs/2310.01117</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muratahan Aykol, Amil Merchant, Simon Batzner, Jennifer N. Wei, Ekin Dogus Cubuk</li>
<li>for: 这个论文的目的是predicting the outcome of phase transitions in inorganic materials, enabling new research directions in material synthesis and development.</li>
<li>methods: 该论文使用了universal deep learning potentials to sample the crystallization pathways of local structural motifs at the atomistic level, allowing for the prediction of crystal structures of polymorphs from amorphous precursors with high accuracy.</li>
<li>results: 该论文的结果表明，通过利用 Ostwald’s rule of stages mechanistically at the molecular level, it is possible to predictably access new metastable crystals from the amorphous phase in material synthesis, across a diverse set of material systems including polymorphic oxides, nitrides, carbides, fluorides, chlorides, chalcogenides, and metal alloys.<details>
<summary>Abstract</summary>
Crystallization of the amorphous phases into metastable crystals plays a fundamental role in the formation of new matter, from geological to biological processes in nature to synthesis and development of new materials in the laboratory. Predicting the outcome of such phase transitions reliably would enable new research directions in these areas, but has remained beyond reach with molecular modeling or ab-initio methods. Here, we show that crystallization products of amorphous phases can be predicted in any inorganic chemistry by sampling the crystallization pathways of their local structural motifs at the atomistic level using universal deep learning potentials. We show that this approach identifies the crystal structures of polymorphs that initially nucleate from amorphous precursors with high accuracy across a diverse set of material systems, including polymorphic oxides, nitrides, carbides, fluorides, chlorides, chalcogenides, and metal alloys. Our results demonstrate that Ostwald's rule of stages can be exploited mechanistically at the molecular level to predictably access new metastable crystals from the amorphous phase in material synthesis.
</details>
<details>
<summary>摘要</summary>
晶体化的杂形阶段到稳定的晶体发散着重要作用于自然界的形成和人工材料的 synthesis 中。可预测晶体化结果的方法会开拓新的研究方向，但这一目标尚未被分子模型或初始方法实现。本文显示，在无机化学中，通过 sampling 杂形阶段的晶体化路径的本地结构模式，使用 universal deep learning potentials 可预测晶体化产物的晶体结构。我们的结果表明，这种方法可以高精度地预测各种材料系统中的多形体，包括氧化物、硼化物、碳化物、氟化物、氯化物、硫化物和金属合金。我们的结果还示出，在材料合成中，奥斯特瓦尔的规则可以在分子水平上机制地抓住，以预测从杂形阶段到新的稳定晶体的晶体化过程。
</details></li>
</ul>
<hr>
<h2 id="R-divergence-for-Estimating-Model-oriented-Distribution-Discrepancy"><a href="#R-divergence-for-Estimating-Model-oriented-Distribution-Discrepancy" class="headerlink" title="R-divergence for Estimating Model-oriented Distribution Discrepancy"></a>R-divergence for Estimating Model-oriented Distribution Discrepancy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01109">http://arxiv.org/abs/2310.01109</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lawliet-zzl/r-div">https://github.com/lawliet-zzl/r-div</a></li>
<li>paper_authors: Zhilin Zhao, Longbing Cao</li>
<li>for: 本文旨在检测两个数据集的概率分布是否相同，以便在不同的概率分布下进行模型训练。</li>
<li>methods: 本文提出了R- divergence方法，该方法通过估计最优假设来评估两个数据集的概率分布差异。</li>
<li>results: 实验表明，R-divergence方法可以准确地检测不同概率分布下的数据集差异，并且在不同任务上达到了状态 искусственный的性能。此外，本文还应用R-divergence方法在受损标签数据集上训练Robust神经网络。<details>
<summary>Abstract</summary>
Real-life data are often non-IID due to complex distributions and interactions, and the sensitivity to the distribution of samples can differ among learning models. Accordingly, a key question for any supervised or unsupervised model is whether the probability distributions of two given datasets can be considered identical. To address this question, we introduce R-divergence, designed to assess model-oriented distribution discrepancies. The core insight is that two distributions are likely identical if their optimal hypothesis yields the same expected risk for each distribution. To estimate the distribution discrepancy between two datasets, R-divergence learns a minimum hypothesis on the mixed data and then gauges the empirical risk difference between them. We evaluate the test power across various unsupervised and supervised tasks and find that R-divergence achieves state-of-the-art performance. To demonstrate the practicality of R-divergence, we employ R-divergence to train robust neural networks on samples with noisy labels.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified ChineseReal-life data are often non-IID due to complex distributions and interactions, and the sensitivity to the distribution of samples can differ among learning models. Accordingly, a key question for any supervised or unsupervised model is whether the probability distributions of two given datasets can be considered identical. To address this question, we introduce R-divergence, designed to assess model-oriented distribution discrepancies. The core insight is that two distributions are likely identical if their optimal hypothesis yields the same expected risk for each distribution. To estimate the distribution discrepancy between two datasets, R-divergence learns a minimum hypothesis on the mixed data and then gauges the empirical risk difference between them. We evaluate the test power across various unsupervised and supervised tasks and find that R-divergence achieves state-of-the-art performance. To demonstrate the practicality of R-divergence, we employ R-divergence to train robust neural networks on samples with noisy labels.中文简体版：实际数据往往非相关的，因为样本分布复杂和互动，而不同学习模型对样本分布的敏感度也不同。因此，任何supervised或unsupersvised模型的关键问题是否可以视两个给定的数据集为同一个分布。为解决这个问题，我们介绍了R-divergence，用于评估模型层次分布差异。R-divergence的核心思想是，如果两个分布的优化假设都能够对它们的预期风险做出同样的预测，那么这两个分布可能是同一个。为估计两个数据集之间的分布差异，R-divergence学习了混合数据上的最小假设，然后计算这两个数据集之间的实际风险差异。我们在不同的Unsupervised和Supervised任务中评估了R-divergence的测试能力，并发现它达到了当前最佳性能。为证明R-divergence的实用性，我们使用R-divergence来训练对样本标签有误的神经网络。
</details></li>
</ul>
<hr>
<h2 id="Energy-Guided-Continuous-Entropic-Barycenter-Estimation-for-General-Costs"><a href="#Energy-Guided-Continuous-Entropic-Barycenter-Estimation-for-General-Costs" class="headerlink" title="Energy-Guided Continuous Entropic Barycenter Estimation for General Costs"></a>Energy-Guided Continuous Entropic Barycenter Estimation for General Costs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01105">http://arxiv.org/abs/2310.01105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Kolesov, Petr Mokrov, Igor Udovichenko, Milena Gazdieva, Gudmund Pammer, Evgeny Burnaev, Alexander Korotin</li>
<li>for: averaging probability distributions while capturing their geometric properties</li>
<li>methods: novel algorithm based on weak OT and dual reformulation, with quality bounds and interconnectivity with Energy-Based Models</li>
<li>results: validated on low-dimensional scenarios and image-space setups, with practical applications in learning barycenter on image manifolds generated by pretrained generative models<details>
<summary>Abstract</summary>
Optimal transport (OT) barycenters are a mathematically grounded way of averaging probability distributions while capturing their geometric properties. In short, the barycenter task is to take the average of a collection of probability distributions w.r.t. given OT discrepancies. We propose a novel algorithm for approximating the continuous Entropic OT (EOT) barycenter for arbitrary OT cost functions. Our approach is built upon the dual reformulation of the EOT problem based on weak OT, which has recently gained the attention of the ML community. Beyond its novelty, our method enjoys several advantageous properties: (i) we establish quality bounds for the recovered solution; (ii) this approach seemlessly interconnects with the Energy-Based Models (EBMs) learning procedure enabling the use of well-tuned algorithms for the problem of interest; (iii) it provides an intuitive optimization scheme avoiding min-max, reinforce and other intricate technical tricks. For validation, we consider several low-dimensional scenarios and image-space setups, including non-Euclidean cost functions. Furthermore, we investigate the practical task of learning the barycenter on an image manifold generated by a pretrained generative model, opening up new directions for real-world applications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>我们设定了解决方案的质量上限，从而确保方案的可靠性。2. 我们的方法可以继续使用已经调整好的EBMs学习过程，从而使用已有的算法解决问题。3. 我们的方法具有直观的优化方案，不需要使用复杂的技术手段，如min-max、强化等。为了验证我们的方法，我们在低维度的场景和图像空间中进行了多个实验，包括非欧几何成本函数。此外，我们还研究了在一个由预训练的生成模型生成的图像概率空间中学习矩形成的实际任务，开启了新的应用场景。</details></li>
</ol>
<hr>
<h2 id="Seismogram-Transformer-A-generic-deep-learning-backbone-network-for-multiple-earthquake-monitoring-tasks"><a href="#Seismogram-Transformer-A-generic-deep-learning-backbone-network-for-multiple-earthquake-monitoring-tasks" class="headerlink" title="Seismogram Transformer: A generic deep learning backbone network for multiple earthquake monitoring tasks"></a>Seismogram Transformer: A generic deep learning backbone network for multiple earthquake monitoring tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01037">http://arxiv.org/abs/2310.01037</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/senli1073/seist">https://github.com/senli1073/seist</a></li>
<li>paper_authors: Sen Li, Xu Yang, Anye Cao, Changbin Wang, Yaoqi Liu, Yapeng Liu, Qiang Niu</li>
<li>for: 这个论文主要探讨了地震记录（seismogram）处理的深度学习方法，以提高地震研究和监测的精度和效率。</li>
<li>methods: 这个论文提出了一种新的含有多种基础块的径向神经网络模型，称为Seismogram Transformer（SeisT），用于地震检测、地震阶段选择、初动方向分类、强度估计和反射方向估计等多种地震监测任务。</li>
<li>results: 这个论文的实验结果表明，SeisT模型在不同的任务上能够匹配或者甚至超越现有的状态对照模型，特别是在对于不同数据集的扩展性表现上。SeisT模型通过不同基础块的组合，能够从低级到高级复杂特征之间提取多种特征，如频率、阶段和时间-频率关系。<details>
<summary>Abstract</summary>
Seismic records, known as seismograms, are crucial records of ground motion resulting from seismic events, constituting the backbone of earthquake research and monitoring. The latest advancements in deep learning have significantly facilitated various seismic signal processing tasks. This paper introduces a novel backbone neural network model designed for various seismic monitoring tasks, named Seismogram Transformer (SeisT). Thanks to its efficient network architecture, SeisT matches or even outperforms the state-of-the-art models in earthquake detection, seismic phase picking, first-motion polarity classification, magnitude estimation, and back-azimuth estimation tasks, particularly in terms of out-of-distribution generalization performance. SeisT consists of multiple network layers composed of different foundational blocks, which help the model understand multi-level feature representations of seismograms from low-level to high-level complex features, effectively extracting features such as frequency, phase, and time-frequency relationships from input seismograms. Three different-sized models were customized based on these diverse foundational modules. Through extensive experiments and performance evaluations, this study showcases the capabilities and potential of SeisT in advancing seismic signal processing and earthquake research.
</details>
<details>
<summary>摘要</summary>
震动记录，即震ogram，是地震研究和监测中非常重要的记录，表现地面上的运动。最新的深度学习技术在不同的震动信号处理任务中帮助了我们更好地完成工作。这篇论文介绍了一种新的震动监测模型，名为震ogram Transformer（SeisT），用于不同的震动监测任务。这种模型具有高效的网络架构，可以匹配或者超越现有的状态 искусственный智能模型在地震检测、震动相位选择、首动方向分类、 magnitude 估计和反射角估计等任务中的性能。SeisT 模型由多层网络组成，每层由不同的基础块组成，这些基础块帮助模型理解震ogram 中的多级特征表示，从低级特征到高级复杂特征，有效地提取震ogram 中的频率、相位和时间频率关系等特征。为了适应不同的应用场景，我们还制定了三个不同的模型大小。经过了广泛的实验和性能评估，这篇论文展示了 SeisT 模型在震动信号处理和地震研究中的可能性和应用前景。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Approach-for-Machine-Learning-based-Load-Balancing-in-High-speed-Train-System-using-Nested-Cross-Validation"><a href="#A-Novel-Approach-for-Machine-Learning-based-Load-Balancing-in-High-speed-Train-System-using-Nested-Cross-Validation" class="headerlink" title="A Novel Approach for Machine Learning-based Load Balancing in High-speed Train System using Nested Cross Validation"></a>A Novel Approach for Machine Learning-based Load Balancing in High-speed Train System using Nested Cross Validation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01034">http://arxiv.org/abs/2310.01034</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ibrahim Yazici, Emre Gures</li>
<li>for: 高速铁路系统中的5G无线通信网络，以提高 mobil 用户的服务质量。</li>
<li>methods: 使用机器学习方法，包括嵌入式检查运算（Nested Cross Validation），以避免模型评估中的信息泄露，并且避免适材料过滤（Overfitting），以获得更好的一致错误（Generalization Error）。</li>
<li>results: 使用不同的机器学习方法，包括渐进增强回传（GBR）、适材料增强（AdaBoost）、猫缩回传（CBR）、人工神经网络（ANN）、核心ridge回传（KRR）、支持向量回传（SVR）和k-最近邻回传（KNNR），可以对高速铁路系统的问题进行优化。而使用嵌入式检查运算的结果，比较了不同的跨 VALIDATION 方案，发现boosting方法、ABR、CBR、GBR在嵌入式检查运算下表现最佳，而SVR、KNNR、KRR、ANN在嵌入式检查运算下的预测结果则有 promise。<details>
<summary>Abstract</summary>
Fifth-generation (5G) mobile communication networks have recently emerged in various fields, including highspeed trains. However, the dense deployment of 5G millimeter wave (mmWave) base stations (BSs) and the high speed of moving trains lead to frequent handovers (HOs), which can adversely affect the Quality-of-Service (QoS) of mobile users. As a result, HO optimization and resource allocation are essential considerations for managing mobility in high-speed train systems. In this paper, we model system performance of a high-speed train system with a novel machine learning (ML) approach that is nested cross validation scheme that prevents information leakage from model evaluation into the model parameter tuning, thereby avoiding overfitting and resulting in better generalization error. To this end, we employ ML methods for the high-speed train system scenario. Handover Margin (HOM) and Time-to-Trigger (TTT) values are used as features, and several KPIs are used as outputs, and several ML methods including Gradient Boosting Regression (GBR), Adaptive Boosting (AdaBoost), CatBoost Regression (CBR), Artificial Neural Network (ANN), Kernel Ridge Regression (KRR), Support Vector Regression (SVR), and k-Nearest Neighbor Regression (KNNR) are employed for the problem. Finally, performance comparisons of the cross validation schemes with the methods are made in terms of mean absolute error (MAE) and mean square error (MSE) metrics are made. As per obtained results, boosting methods, ABR, CBR, GBR, with nested cross validation scheme superiorly outperforms conventional cross validation scheme results with the same methods. On the other hand, SVR, KNRR, KRR, ANN with the nested scheme produce promising results for prediction of some KPIs with respect to their conventional scheme employment.
</details>
<details>
<summary>摘要</summary>
fifth-generation (5G) 移动通信网络在不同领域出现，包括高速列车。然而，5G毫米波基站的密集部署和高速列车的运动速度导致频繁的手动更新（HO），可能会影响移动用户的服务质量（QoS）。因此，HO优化和资源分配是管理移动高速列车系统的关键考虑因素。在这篇论文中，我们使用一种新的机器学习（ML）方法，即嵌入式十字验证（Nested CV），来模型高速列车系统的性能。这种方法可以避免信息泄露，从而避免过拟合和更好地适应泛化误差。为此，我们在高速列车系统场景中采用ML方法。手动更新边缘（HOM）和时间触发（TTT）值被用作特征，并使用多个key performance indicators（KPIs）作为输出。此外，我们采用了多种ML方法，包括梯度提升回归（GBR）、适应提升（AdaBoost）、CatBoost回归（CBR）、人工神经网络（ANN）、核心ridge回归（KRR）、支持向量回归（SVR）和k-最近邻回归（KNNR）。最后，我们对嵌入式十字验证方案与这些方法进行了性能比较，并通过MAE和MSE指标进行评估。根据所获结果，梯度提升方法、ABR、CBR、GBR等方法，在嵌入式十字验证方案下表现出色，而SVR、KNRR、KRR、ANN等方法在嵌入式十字验证方案下表现出色。
</details></li>
</ul>
<hr>
<h2 id="The-Fisher-Rao-geometry-of-CES-distributions"><a href="#The-Fisher-Rao-geometry-of-CES-distributions" class="headerlink" title="The Fisher-Rao geometry of CES distributions"></a>The Fisher-Rao geometry of CES distributions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01032">http://arxiv.org/abs/2310.01032</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florent Bouchard, Arnaud Breloy, Antoine Collas, Alexandre Renaux, Guillaume Ginolhac</li>
<li>for: 这篇论文是关于 Parametric Statistical Model 的扩展，通过在参数空间上尝试 Fisher 信息度量来自然地具有 Riemannian 拓扑结构，并利用这种结构来应用 differential geometry 的多种工具。</li>
<li>methods: 这篇论文使用了 Fisher 信息度量来 induce Riemannian 拓扑结构在参数空间上，并利用这种结构来解决 Covariance 矩阵估计、内在 Cramér-Rao 上限、以及使用 Riemannian 距离进行分类等问题。</li>
<li>results: 这篇论文的结果表明，通过使用 Riemannian 拓扑结构和 differential geometry 的工具，可以解决 Parametric Statistical Model 中的一些实际问题，如 Covariance 矩阵估计、内在 Cramér-Rao 上限、以及分类等问题。<details>
<summary>Abstract</summary>
When dealing with a parametric statistical model, a Riemannian manifold can naturally appear by endowing the parameter space with the Fisher information metric. The geometry induced on the parameters by this metric is then referred to as the Fisher-Rao information geometry. Interestingly, this yields a point of view that allows for leveragingmany tools from differential geometry. After a brief introduction about these concepts, we will present some practical uses of these geometric tools in the framework of elliptical distributions. This second part of the exposition is divided into three main axes: Riemannian optimization for covariance matrix estimation, Intrinsic Cram\'er-Rao bounds, and classification using Riemannian distances.
</details>
<details>
<summary>摘要</summary>
当处理parametric统计模型时，Riemannian manifold自然而然地出现，通过将参数空间授予Fisher信息度量。这个度量在参数上引入的几何是Fisher-Rao信息几何。这种角度可以利用多种杂分几何工具。在这篇文章中，我们将首先介绍这些概念，然后在elliptical分布框架中展示一些实际应用。这一部分分为三个主要轴：Riemannian优化 дляcovariance矩阵估计、内在Cramér-Rao bound和Riemannian距离分类。
</details></li>
</ul>
<hr>
<h2 id="A-Robust-Machine-Learning-Approach-for-Path-Loss-Prediction-in-5G-Networks-with-Nested-Cross-Validation"><a href="#A-Robust-Machine-Learning-Approach-for-Path-Loss-Prediction-in-5G-Networks-with-Nested-Cross-Validation" class="headerlink" title="A Robust Machine Learning Approach for Path Loss Prediction in 5G Networks with Nested Cross Validation"></a>A Robust Machine Learning Approach for Path Loss Prediction in 5G Networks with Nested Cross Validation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01030">http://arxiv.org/abs/2310.01030</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ibrahim Yazıcı, Emre Gures</li>
<li>for: This paper aims to improve the accuracy of path loss prediction in 5G wireless networks using machine learning (ML) methods, which can facilitate more accurate network planning, resource optimization, and performance improvement.</li>
<li>methods: The paper utilizes a nested cross validation scheme and six different ML methods (Support Vector Regression, CatBoost Regression, eXtreme Gradient Boosting Regression, Artificial Neural Network, and Random Forest) to predict path loss in a 5G network system, and compares the prediction results in terms of Mean Absolute Error and Mean Square Error.</li>
<li>results: The results show that XGBR outperforms the other methods, with a slight performance difference of 0.4% and 1% in terms of MAE and MSE, respectively, compared to CBR. The rest of the methods are outperformed by XGBR with clear performance differences.<details>
<summary>Abstract</summary>
The design and deployment of fifth-generation (5G) wireless networks pose significant challenges due to the increasing number of wireless devices. Path loss has a landmark importance in network performance optimization, and accurate prediction of the path loss, which characterizes the attenuation of signal power during transmission, is critical for effective network planning, coverage estimation, and optimization. In this sense, we utilize machine learning (ML) methods, which overcome conventional path loss prediction models drawbacks, for path loss prediction in a 5G network system to facilitate more accurate network planning, resource optimization, and performance improvement in wireless communication systems. To this end, we utilize a novel approach, nested cross validation scheme, with ML to prevent overfitting, thereby getting better generalization error and stable results for ML deployment. First, we acquire a publicly available dataset obtained through a comprehensive measurement campaign conducted in an urban macro-cell scenario located in Beijing, China. The dataset includes crucial information such as longitude, latitude, elevation, altitude, clutter height, and distance, which are utilized as essential features to predict the path loss in the 5G network system. We deploy Support Vector Regression (SVR), CatBoost Regression (CBR), eXtreme Gradient Boosting Regression (XGBR), Artificial Neural Network (ANN), and Random Forest (RF) methods to predict the path loss, and compare the prediction results in terms of Mean Absolute Error (MAE) and Mean Square Error (MSE). As per obtained results, XGBR outperforms the rest of the methods. It outperforms CBR with a slight performance differences by 0.4 % and 1 % in terms of MAE and MSE metrics, respectively. On the other hand, it outperforms the rest of the methods with clear performance differences.
</details>
<details>
<summary>摘要</summary>
fifth-generation (5G) 无线网络的设计和部署具有显著的挑战，主要是因为无线设备的增加数量。path loss的预测对网络性能优化具有重要的意义，因此我们使用机器学习（ML）方法来预测path loss，以便更好地规划网络、资源优化和性能提高。为此，我们采用了一种新的方法——嵌套交叉验证算法，以避免过拟合，从而获得更好的总体误差和稳定的结果。首先，我们获得了一个公共可用的数据集，通过在北京市区 macro-cell enario中进行了全面的测量活动来获得。该数据集包括了重要的信息，如 longitude、latitude、高度、高度、垃圾高度和距离，这些信息被用作5G网络系统中path loss预测的关键特征。我们使用支持向量回归（SVR）、CatBoost回归（CBR）、极限Gradient Boosting回归（XGBR）、人工神经网络（ANN）和Random Forest（RF）方法来预测path loss，并将结果比较以 Mean Absolute Error（MAE）和Mean Square Error（MSE）指标。根据结果显示，XGBR方法在所有方法中表现最佳，其与CBR方法的性能差距只有0.4%和1%，在MAE和MSE指标上分别下降。此外，XGBR方法在其他方法之间表现出清晰的性能差异。
</details></li>
</ul>
<hr>
<h2 id="Conflict-Aware-Active-Automata-Learning"><a href="#Conflict-Aware-Active-Automata-Learning" class="headerlink" title="Conflict-Aware Active Automata Learning"></a>Conflict-Aware Active Automata Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01003">http://arxiv.org/abs/2310.01003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiago Ferreira, Léo Henry, Raquel Fernandes da Silva, Alexandra Silva</li>
<li>for: 这篇论文是用于解决活动自动化学习算法对于观察数据中的矛盾问题的。</li>
<li>methods: 本文提出了一个具有观察树的对话式活动自动化学习框架（C3AL），以便在学习过程中处理矛盾资料。</li>
<li>results: 根据大量的实验结果显示，C3AL可以更好地处理错误和系统变化，并且可以与现有的学习算法一起使用。<details>
<summary>Abstract</summary>
Active automata learning algorithms cannot easily handle conflict in the observation data (different outputs observed for the same inputs). This inherent inability to recover after a conflict impairs their effective applicability in scenarios where noise is present or the system under learning is mutating. We propose the Conflict-Aware Active Automata Learning (C3AL) framework to enable handling conflicting information during the learning process. The core idea is to consider the so-called observation tree as a first-class citizen in the learning process. Though this idea is explored in recent work, we take it to its full effect by enabling its use with any existing learner and minimizing the number of tests performed on the system under learning, specially in the face of conflicts. We evaluate C3AL in a large set of benchmarks, covering over 30 different realistic targets, and over 18,000 different scenarios. The results of the evaluation show that C3AL is a suitable alternative framework for closed-box learning that can better handle noise and mutations.
</details>
<details>
<summary>摘要</summary>
aktive automata learning algoritmen kan ikke lette håndtere konflikt i observasjonsdata (forskjellige utgaver observert for de samme inputtene). Dette innbygge impiderer deres effektive anvendelighet i scenarioer der støy er tilstede eller systemet under læring er muterende. Vi foreslår Conflict-Aware Active Automata Learning (C3AL) rammeverk for å håndtere konflikt information under læringprosessen. Hovedidéen er å betrakte den kaldte observasjon tre som en førsteklasses borger i læringprosessen. Selv om denne ideen er utforsket i recent arbeid, tas vi den til sitt fulle uttrykk ved å tillate bruk av den med enhver eksisterende lærer og å minimere antall tester på systemet under læring, særlig i møte med konflikter. Vi evaluerer C3AL i et stort sett med benchmarks, dekkende over 30 forskjellige reelle mål og over 18 000 forskjellige scenarioer. Resultatene av evaluasjonen viser at C3AL er en passende alternativ rammeverk for lukket bok learning som kan bedre håndtere støy og mutasjoner.
</details></li>
</ul>
<hr>
<h2 id="A-Theoretical-Analysis-of-the-Test-Error-of-Finite-Rank-Kernel-Ridge-Regression"><a href="#A-Theoretical-Analysis-of-the-Test-Error-of-Finite-Rank-Kernel-Ridge-Regression" class="headerlink" title="A Theoretical Analysis of the Test Error of Finite-Rank Kernel Ridge Regression"></a>A Theoretical Analysis of the Test Error of Finite-Rank Kernel Ridge Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00987">http://arxiv.org/abs/2310.00987</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tin Sum Cheng, Aurelien Lucchi, Ivan Dokmanić, Anastasis Kratsios, David Belius</li>
<li>for: 这个论文旨在提供对任意Finite-rank kernel ridge regression（KRR）的精细学习保证。</li>
<li>methods: 这篇论文使用了非难式的方法，包括 derive sharp non-asymptotic upper and lower bounds for KRR test error。</li>
<li>results: 这篇论文提供了较为紧凑的 bounds，与之前的结果相比，它们在任何正则化参数下都 remained valid。<details>
<summary>Abstract</summary>
Existing statistical learning guarantees for general kernel regressors often yield loose bounds when used with finite-rank kernels. Yet, finite-rank kernels naturally appear in several machine learning problems, e.g.\ when fine-tuning a pre-trained deep neural network's last layer to adapt it to a novel task when performing transfer learning. We address this gap for finite-rank kernel ridge regression (KRR) by deriving sharp non-asymptotic upper and lower bounds for the KRR test error of any finite-rank KRR. Our bounds are tighter than previously derived bounds on finite-rank KRR, and unlike comparable results, they also remain valid for any regularization parameters.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将给定文本翻译成简化中文。<</SYS>>现有的统计学学习保证对通用kernel regression器通常会提供松弛的上限。然而，finite-rank kernel naturally appears in several machine learning problems,例如在转移学习中使用pre-trained deep neural network的最后一层进行微调以适应novel task。我们解决了这个差异 для finite-rank kernel ridge regression (KRR) by deriving sharp non-asymptotic upper and lower bounds for the KRR test error of any finite-rank KRR.我们的 boundstighter than previously derived bounds on finite-rank KRR, and unlike comparable results, they also remain valid for any regularization parameters.
</details></li>
</ul>
<hr>
<h2 id="Variance-Aware-Regret-Bounds-for-Stochastic-Contextual-Dueling-Bandits"><a href="#Variance-Aware-Regret-Bounds-for-Stochastic-Contextual-Dueling-Bandits" class="headerlink" title="Variance-Aware Regret Bounds for Stochastic Contextual Dueling Bandits"></a>Variance-Aware Regret Bounds for Stochastic Contextual Dueling Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00968">http://arxiv.org/abs/2310.00968</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiwei Di, Tao Jin, Yue Wu, Heyang Zhao, Farzad Farnoud, Quanquan Gu</li>
<li>for: 这篇论文旨在提出一种Contextual Dueling Bandits算法，用于处理带有对比性不确定性的决策问题。</li>
<li>methods: 该算法使用一种新的SupLinUCB型算法，具有计算效率和对偏差的 regret bound。</li>
<li>results: 实验结果表明，该算法在synthetic数据上比前一代不考虑偏差的算法表现更好，特别是在对比性较高的情况下。<details>
<summary>Abstract</summary>
Dueling bandits is a prominent framework for decision-making involving preferential feedback, a valuable feature that fits various applications involving human interaction, such as ranking, information retrieval, and recommendation systems. While substantial efforts have been made to minimize the cumulative regret in dueling bandits, a notable gap in the current research is the absence of regret bounds that account for the inherent uncertainty in pairwise comparisons between the dueling arms. Intuitively, greater uncertainty suggests a higher level of difficulty in the problem. To bridge this gap, this paper studies the problem of contextual dueling bandits, where the binary comparison of dueling arms is generated from a generalized linear model (GLM). We propose a new SupLinUCB-type algorithm that enjoys computational efficiency and a variance-aware regret bound $\tilde O\big(d\sqrt{\sum_{t=1}^T\sigma_t^2} + d\big)$, where $\sigma_t$ is the variance of the pairwise comparison in round $t$, $d$ is the dimension of the context vectors, and $T$ is the time horizon. Our regret bound naturally aligns with the intuitive expectation in scenarios where the comparison is deterministic, the algorithm only suffers from an $\tilde O(d)$ regret. We perform empirical experiments on synthetic data to confirm the advantage of our method over previous variance-agnostic algorithms.
</details>
<details>
<summary>摘要</summary>
“对抗匪徒”是一个具有偏好反馈的决策框架，这种特性适合人际互动的应用，如排名、资料搜寻和推荐系统。 despite significant efforts to minimize the cumulative regret in dueling bandits, there is a notable gap in the current research: the absence of regret bounds that account for the inherent uncertainty in pairwise comparisons between the dueling arms. Intuitively, greater uncertainty suggests a higher level of difficulty in the problem. To bridge this gap, this paper studies the problem of contextual dueling bandits, where the binary comparison of dueling arms is generated from a generalized linear model (GLM). We propose a new SupLinUCB-type algorithm that enjoys computational efficiency and a variance-aware regret bound $\tilde O\big(d\sqrt{\sum_{t=1}^T\sigma_t^2} + d\big)$, where $\sigma_t$ is the variance of the pairwise comparison in round $t$, $d$ is the dimension of the context vectors, and $T$ is the time horizon. Our regret bound naturally aligns with the intuitive expectation in scenarios where the comparison is deterministic, the algorithm only suffers from an $\tilde O(d)$ regret. We perform empirical experiments on synthetic data to confirm the advantage of our method over previous variance-agnostic algorithms.
</details></li>
</ul>
<hr>
<h2 id="MiCRO-Near-Zero-Cost-Gradient-Sparsification-for-Scaling-and-Accelerating-Distributed-DNN-Training"><a href="#MiCRO-Near-Zero-Cost-Gradient-Sparsification-for-Scaling-and-Accelerating-Distributed-DNN-Training" class="headerlink" title="MiCRO: Near-Zero Cost Gradient Sparsification for Scaling and Accelerating Distributed DNN Training"></a>MiCRO: Near-Zero Cost Gradient Sparsification for Scaling and Accelerating Distributed DNN Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00967">http://arxiv.org/abs/2310.00967</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kljp/micro">https://github.com/kljp/micro</a></li>
<li>paper_authors: Daegun Yoon, Sangyoon Oh</li>
<li>for: 加速分布式深度神经网络（DNN）训练，提高训练效率和可扩展性。</li>
<li>methods: 提出了一种新的梯度减少方法，即MiCRO，通过将梯度向量分割，并对每个分割分配到相应的工作者进行梯度选择，以避免梯度积累和不当的阈值选择，并且可以根据用户需求设定最佳阈值，以实现近于零成本的梯度减少。</li>
<li>results: 在广泛的实验中，MiCRO比状态空间的减少器更具有极高的快速收敛率。<details>
<summary>Abstract</summary>
Gradient sparsification is a communication optimisation technique for scaling and accelerating distributed deep neural network (DNN) training. It reduces the increasing communication traffic for gradient aggregation. However, existing sparsifiers have poor scalability because of the high computational cost of gradient selection and/or increase in communication traffic. In particular, an increase in communication traffic is caused by gradient build-up and inappropriate threshold for gradient selection.   To address these challenges, we propose a novel gradient sparsification method called MiCRO. In MiCRO, the gradient vector is partitioned, and each partition is assigned to the corresponding worker. Each worker then selects gradients from its partition, and the aggregated gradients are free from gradient build-up. Moreover, MiCRO estimates the accurate threshold to maintain the communication traffic as per user requirement by minimising the compression ratio error. MiCRO enables near-zero cost gradient sparsification by solving existing problems that hinder the scalability and acceleration of distributed DNN training. In our extensive experiments, MiCRO outperformed state-of-the-art sparsifiers with an outstanding convergence rate.
</details>
<details>
<summary>摘要</summary>
Gradient sparsification 是一种分布式深度神经网络（DNN）训练中的通信优化技术，它降低了梯度聚合所导致的通信峰值。然而，现有的简化器具有较差的扩展性，因为梯度选择所需的计算成本高，以及或者通信峰值的增加。特别是，通信峰值的增加是由梯度建立和不当的阈值导致的。为解决这些挑战，我们提出了一种新的梯度简化方法，称为 MiCRO。在 MiCRO 中，梯度 вектор 被分区，每个分区被分配到对应的工作者。每个工作者然后从其分区中选择梯度，并将所有分区的积和的梯度进行聚合。此外，MiCRO 估算了精准的阈值，以保持通信峰值与用户需求一致。MiCRO 允许 near-zero 成本的梯度简化，解决了分布式 DNN 训练中的扩展性和加速性的问题。在我们的广泛实验中，MiCRO 与状态之前的简化器相比，具有出色的收敛率。
</details></li>
</ul>
<hr>
<h2 id="Effective-Learning-with-Node-Perturbation-in-Deep-Neural-Networks"><a href="#Effective-Learning-with-Node-Perturbation-in-Deep-Neural-Networks" class="headerlink" title="Effective Learning with Node Perturbation in Deep Neural Networks"></a>Effective Learning with Node Perturbation in Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00965">http://arxiv.org/abs/2310.00965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sander Dalm, Marcel van Gerven, Nasir Ahmad</li>
<li>for: 提高深度神经网络模型的训练参数，替代传统的反射推传方法。</li>
<li>methods: 使用节点干扰（NP）方法，通过在网络活动中扔入噪声，并测量引起的损失变化来学习。</li>
<li>results: 通过与方向导Derivatives的关联和层weise输入decorrelating机制，NP学习得到显著改进，与传统的反射推传方法竞争。<details>
<summary>Abstract</summary>
Backpropagation (BP) is the dominant and most successful method for training parameters of deep neural network models. However, BP relies on two computationally distinct phases, does not provide a satisfactory explanation of biological learning, and can be challenging to apply for training of networks with discontinuities or noisy node dynamics. By comparison, node perturbation (NP) proposes learning by the injection of noise into the network activations, and subsequent measurement of the induced loss change. NP relies on two forward (inference) passes, does not make use of network derivatives, and has been proposed as a model for learning in biological systems. However, standard NP is highly data inefficient and unstable due to its unguided, noise-based, activity search. In this work, we investigate different formulations of NP and relate it to the concept of directional derivatives as well as combining it with a decorrelating mechanism for layer-wise inputs. We find that a closer alignment with directional derivatives, and induction of decorrelation of inputs at every layer significantly enhances performance of NP learning making it competitive with BP.
</details>
<details>
<summary>摘要</summary>
“背测传播”（Backpropagation，BP）是深度神经网络模型的训练方法之主流和最成功的方法。然而，BP 依赖了两个计算上不同的阶段，无法提供满意的生物学学习解释，且可能对网络中的随机性或不确定性进行训练时具有挑战。相比之下，“节点干扰”（Node Perturbation，NP）提出了通过对网络活动的噪声注入，并且 mesure 对� induced loss change 的方法。NP 依赖了两个前（推论）通过，没有使用网络 Derivative，并且被视为生物学系统中的学习模型。然而，标准的NP 对于资料效率和稳定性而言相当不利，因为它的不导向、噪声基的活动搜寻。在这个研究中，我们调查了不同的NP  формулювання，并与方向 derivative 的概念和层别输入的decorrelating Mechanism 结合。我们发现，与方向 derivative 更加接近的NP 学习，并且在每个层级 inducing decorrelation of inputs 可以很好地提高 NP 的性能，使其与 BP 竞争。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-IoT-Trust-Model-Leveraging-Fully-Distributed-Behavioral-Fingerprinting-and-Secure-Delegation"><a href="#A-Novel-IoT-Trust-Model-Leveraging-Fully-Distributed-Behavioral-Fingerprinting-and-Secure-Delegation" class="headerlink" title="A Novel IoT Trust Model Leveraging Fully Distributed Behavioral Fingerprinting and Secure Delegation"></a>A Novel IoT Trust Model Leveraging Fully Distributed Behavioral Fingerprinting and Secure Delegation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00953">http://arxiv.org/abs/2310.00953</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Arazzi, Serena Nicolazzo, Antonino Nocera</li>
<li>for: 提供一种在互联网物联网设备之间evaluate对象的信任性的方法，以帮助解决数据黑客和丢失问题。</li>
<li>methods: 我们提出了一种基于行为指纹、分布式一致算法和区块链技术的全分布式信任模型，以及一种安全模型和测试方法来评估模型的正确性和性能。</li>
<li>results: 我们的研究表明，我们的方法可以帮助iot设备在网络中评估对象的信任性，从而减少数据黑客和丢失的风险。我们的方法可以在不同类型的设备上实现，并且在不同的网络环境下进行有效地评估对象的信任性。<details>
<summary>Abstract</summary>
With the number of connected smart devices expected to constantly grow in the next years, Internet of Things (IoT) solutions are experimenting a booming demand to make data collection and processing easier. The ability of IoT appliances to provide pervasive and better support to everyday tasks, in most cases transparently to humans, is also achieved through the high degree of autonomy of such devices. However, the higher the number of new capabilities and services provided in an autonomous way, the wider the attack surface that exposes users to data hacking and lost. In this scenario, many critical challenges arise also because IoT devices have heterogeneous computational capabilities (i.e., in the same network there might be simple sensors/actuators as well as more complex and smart nodes). In this paper, we try to provide a contribution in this setting, tackling the non-trivial issues of equipping smart things with a strategy to evaluate, also through their neighbors, the trustworthiness of an object in the network before interacting with it. To do so, we design a novel and fully distributed trust model exploiting devices' behavioral fingerprints, a distributed consensus mechanism and the Blockchain technology. Beyond the detailed description of our framework, we also illustrate the security model associated with it and the tests carried out to evaluate its correctness and performance.
</details>
<details>
<summary>摘要</summary>
随着智能设备的连接数量逐渐增加，互联网智能（IoT）解决方案的需求也在不断增长，以便更方便地收集和处理数据。智能设备的高度自主性使得它们能够在大多数情况下透明地为人类提供每天任务的支持。然而，随着新功能和服务的增加，用户面临的数据入侵和丢失的风险也在增加。在这种情况下，许多重要的挑战也在出现，其中一个是因为 IoT 设备的计算能力具有多种不同的水平（即在同一个网络中可能有简单的感知器/动作器以及更复杂的智能节点）。在这篇论文中，我们尝试提供一种在这种设定下的贡献，即为智能东西 equip 一种评估网络中对象的可信worthiness 的策略，并通过其邻居进行评估。为此，我们设计了一种全新的、分布式的信任模型，利用设备的行为指纹、分布式共识机制和区块链技术。我们还详细描述了我们的框架的安全模型和对其正确性和性能的测试。
</details></li>
</ul>
<hr>
<h2 id="Improved-Variational-Bayesian-Phylogenetic-Inference-using-Mixtures"><a href="#Improved-Variational-Bayesian-Phylogenetic-Inference-using-Mixtures" class="headerlink" title="Improved Variational Bayesian Phylogenetic Inference using Mixtures"></a>Improved Variational Bayesian Phylogenetic Inference using Mixtures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00941">http://arxiv.org/abs/2310.00941</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lagergren-lab/vbpi-mixtures">https://github.com/lagergren-lab/vbpi-mixtures</a></li>
<li>paper_authors: Oskar Kviman, Ricky Molén, Jens Lagergren</li>
<li>for: 增强生物进化树的准确性，特别是树结构和分支长度的近似。</li>
<li>methods: 使用Variational Bayesian Phylogenetic Inference（VBPI）框架，加上现代深度学习技术如正常化流和图神经网络，进行树 topology 和分支长度的近似。</li>
<li>results: 在多个实际生物演化数据集上达到了状态体现性能。<details>
<summary>Abstract</summary>
We present VBPI-Mixtures, an algorithm designed to enhance the accuracy of phylogenetic posterior distributions, particularly for tree-topology and branch-length approximations. Despite the Variational Bayesian Phylogenetic Inference (VBPI), a leading-edge black-box variational inference (BBVI) framework, achieving remarkable approximations of these distributions, the multimodality of the tree-topology posterior presents a formidable challenge to sampling-based learning techniques such as BBVI. Advanced deep learning methodologies such as normalizing flows and graph neural networks have been explored to refine the branch-length posterior approximation, yet efforts to ameliorate the posterior approximation over tree topologies have been lacking. Our novel VBPI-Mixtures algorithm bridges this gap by harnessing the latest breakthroughs in mixture learning within the BBVI domain. As a result, VBPI-Mixtures is capable of capturing distributions over tree-topologies that VBPI fails to model. We deliver state-of-the-art performance on difficult density estimation tasks across numerous real phylogenetic datasets.
</details>
<details>
<summary>摘要</summary>
我团队 todavía present VBPI-Mixtures，一种算法用于提高生物进化 posterior 分布的准确性，特别是树体态和分支长度的估计。 despite Variational Bayesian Phylogenetic Inference (VBPI) 是一个 cutting-edge black-box 变量推理 (BBVI) 框架，它已经实现了这些分布的很好的估计，但是树体态 posterior 的多模性是一个困难的挑战，而 sampling-based learning 技术如 BBVI 无法有效地处理这种多模性。 在这种情况下，我们提出了一种新的 VBPI-Mixtures 算法，它利用 BBVI 领域中最新的混合学习突破口，以解决 VBPI 无法模型的树体态 posterior 问题。我们的 VBPI-Mixtures 算法能够捕捉 VBPI 无法模型的分布，并在许多实际生物进化数据集上达到了state-of-the-art 性能。
</details></li>
</ul>
<hr>
<h2 id="Integration-of-Graph-Neural-Network-and-Neural-ODEs-for-Tumor-Dynamic-Prediction"><a href="#Integration-of-Graph-Neural-Network-and-Neural-ODEs-for-Tumor-Dynamic-Prediction" class="headerlink" title="Integration of Graph Neural Network and Neural-ODEs for Tumor Dynamic Prediction"></a>Integration of Graph Neural Network and Neural-ODEs for Tumor Dynamic Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00926">http://arxiv.org/abs/2310.00926</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omid Bazgir, Zichen Wang, Marc Hafner, James Lu</li>
<li>for: 这个研究旨在帮助抗癌药物开发中解决高维度 genomics 数据、肿瘤来源、治疗目标和治疗反应之间的复杂关系。</li>
<li>methods: 本研究提出了一种异源graph Encoder，该方法利用了双方向图connolly（GCN）和神经普通对数方程（Neural-ODEs），以实现个性化肿瘤动态预测。</li>
<li>results: 研究发现，该方法能够提高个性化肿瘤动态预测，并且能够充分利用多modal数据来增强肿瘤预测。<details>
<summary>Abstract</summary>
In anti-cancer drug development, a major scientific challenge is disentangling the complex relationships between high-dimensional genomics data from patient tumor samples, the corresponding tumor's organ of origin, the drug targets associated with given treatments and the resulting treatment response. Furthermore, to realize the aspirations of precision medicine in identifying and adjusting treatments for patients depending on the therapeutic response, there is a need for building tumor dynamic models that can integrate both longitudinal tumor size as well as multimodal, high-content data. In this work, we take a step towards enhancing personalized tumor dynamic predictions by proposing a heterogeneous graph encoder that utilizes a bipartite Graph Convolutional Neural network (GCN) combined with Neural Ordinary Differential Equations (Neural-ODEs). We applied the methodology to a large collection of patient-derived xenograft (PDX) data, spanning a wide variety of treatments (as well as their combinations) on tumors that originated from a number of different organs. We first show that the methodology is able to discover a tumor dynamic model that significantly improves upon an empirical model which is in current use. Additionally, we show that the graph encoder is able to effectively utilize multimodal data to enhance tumor predictions. Our findings indicate that the methodology holds significant promise and offers potential applications in pre-clinical settings.
</details>
<details>
<summary>摘要</summary>
在抗癌药物开发中，一个主要的科学挑战是分离高维 genomics 数据，来自病人肿瘤样本，与相应的肿瘤所属的器官、与给定治疗相关的药Target以及治疗效果的关系。此外，为实现精准医学的目标，需要建立肿瘤动态模型，可以结合长期肿瘤大小和多Modal、高Content数据。在这项工作中，我们提出一种异构图像编码器，使用BiPartite Graph Convolutional Neural network (GCN)和Neural Ordinary Differential Equations (Neural-ODEs)。我们应用这种方法ологи到了一个大量的病人 derivated xenograft (PDX) 数据集，覆盖了多种治疗（以及其组合），对来自多种器官的肿瘤进行预测。我们首先显示该方法可以发现一个肿瘤动态模型，Significantly improves upon an empirical model ，现在使用。此外，我们还表明异构图像编码器可以有效地利用多Modal数据来增强肿瘤预测。我们的发现表明该方法具有普遍应用的潜在应用，特别是在预 клиниче设置中。
</details></li>
</ul>
<hr>
<h2 id="DataInf-Efficiently-Estimating-Data-Influence-in-LoRA-tuned-LLMs-and-Diffusion-Models"><a href="#DataInf-Efficiently-Estimating-Data-Influence-in-LoRA-tuned-LLMs-and-Diffusion-Models" class="headerlink" title="DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models"></a>DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00902">http://arxiv.org/abs/2310.00902</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ykwon0407/datainf">https://github.com/ykwon0407/datainf</a></li>
<li>paper_authors: Yongchan Kwon, Eric Wu, Kevin Wu, James Zou</li>
<li>for: 这paper是为了提高机器学习模型的透明度和理解输出，并且可以帮助标注数据点的批注。</li>
<li>methods: 这paper使用了一种名为DataInf的有效影响近似方法，该方法可以实现大规模的生成AI模型中的影响计算。</li>
<li>results: 经过系统的实验评估，DataInf可以准确地计算影响得分，并且比现有的方法更快速和有更少的内存占用。在应用于RoBERTa-large、Llama-2-13B-chat和stable-diffusion-v1.5模型中，DataInf能够更好地 identificet最重要的 fine-tuning 示例，并且可以帮助标注数据点的批注。<details>
<summary>Abstract</summary>
Quantifying the impact of training data points is crucial for understanding the outputs of machine learning models and for improving the transparency of the AI pipeline. The influence function is a principled and popular data attribution method, but its computational cost often makes it challenging to use. This issue becomes more pronounced in the setting of large language models and text-to-image models. In this work, we propose DataInf, an efficient influence approximation method that is practical for large-scale generative AI models. Leveraging an easy-to-compute closed-form expression, DataInf outperforms existing influence computation algorithms in terms of computational and memory efficiency. Our theoretical analysis shows that DataInf is particularly well-suited for parameter-efficient fine-tuning techniques such as LoRA. Through systematic empirical evaluations, we show that DataInf accurately approximates influence scores and is orders of magnitude faster than existing methods. In applications to RoBERTa-large, Llama-2-13B-chat, and stable-diffusion-v1.5 models, DataInf effectively identifies the most influential fine-tuning examples better than other approximate influence scores. Moreover, it can help to identify which data points are mislabeled.
</details>
<details>
<summary>摘要</summary>
量化训练数据点的影响是机器学习模型输出的理解和AI管道的透明度的关键。影响函数是一种有 principios y popular data attribution方法，但其计算成本经常使其成为实现困难的。在大语言模型和文本到图像模型的设置中，这个问题更加突出。在这种情况下，我们提出了DataInf，一种高效的影响估计方法，适用于大规模生成AI模型。通过一个容易计算的关闭式表达，DataInf在计算和内存效率方面比既有的影响计算算法高效得多。我们的理论分析表明，DataInf在精细调整技术such as LoRA中特别有效。通过系统的实验评估，我们发现DataInf可以准确地估计影响得分，并且比现有方法快得多。在应用于RoBERTa-large、Llama-2-13B-chat和stable-diffusion-v1.5模型时，DataInf能够更好地确定最有影响的练习示例，并且能够帮助标识数据点是否被标注错误。
</details></li>
</ul>
<hr>
<h2 id="Organized-Event-Participant-Prediction-Enhanced-by-Social-Media-Retweeting-Data"><a href="#Organized-Event-Participant-Prediction-Enhanced-by-Social-Media-Retweeting-Data" class="headerlink" title="Organized Event Participant Prediction Enhanced by Social Media Retweeting Data"></a>Organized Event Participant Prediction Enhanced by Social Media Retweeting Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00896">http://arxiv.org/abs/2310.00896</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihong Zhang, Takahiro Hara<br>for: 这个论文旨在预测活动参与者。methods: 该论文使用社交媒体转发活动数据来增强活动参与者预测模型。它创建了一个共同知识图，将社交媒体和目标领域的信息相互关联。此外，它提出了一种利用转发信息更好地预测目标领域的学习模型。results: 作者在两个场景中进行了广泛的实验，使用实际数据。在每个场景中，他们设置了不同的训练数据大小和热和冷测试 caso。结果显示，他们的方法在热测试 caso 和数据有限情况下一直表现出优异性，特别是在热测试 caso 上。<details>
<summary>Abstract</summary>
Nowadays, many platforms on the Web offer organized events, allowing users to be organizers or participants. For such platforms, it is beneficial to predict potential event participants. Existing work on this problem tends to borrow recommendation techniques. However, compared to e-commerce items and purchases, events and participation are usually of a much smaller frequency, and the data may be insufficient to learn an accurate model. In this paper, we propose to utilize social media retweeting activity data to enhance the learning of event participant prediction models. We create a joint knowledge graph to bridge the social media and the target domain, assuming that event descriptions and tweets are written in the same language. Furthermore, we propose a learning model that utilizes retweeting information for the target domain prediction more effectively. We conduct comprehensive experiments in two scenarios with real-world data. In each scenario, we set up training data of different sizes, as well as warm and cold test cases. The evaluation results show that our approach consistently outperforms several baseline models, especially with the warm test cases, and when target domain data is limited.
</details>
<details>
<summary>摘要</summary>
现在，许多网络平台上提供了有组织的活动，让用户成为组织者或参与者。为这些平台，预测可能参加活动的人员是有利的。现有的工作通常是借鉴推荐技术。然而，与电商Item和购买相比，活动和参与的频率通常较少，数据可能不够学习准确的模型。在这篇论文中，我们提议使用社交媒体转发活动数据来增强参与者预测模型的学习。我们创建了一个共同知识图，将社交媒体和目标领域的信息相互连接，假设活动描述和微博都是同一种语言。此外，我们提出了一种利用转发信息更好地预测目标领域的学习模型。我们对实际数据进行了广泛的实验，在两个场景中，每个场景都设置了不同的训练数据大小和热和冷测试 caso。评估结果显示，我们的方法在热测试 caso中具有显著的优势，特别是当目标领域数据有限时。
</details></li>
</ul>
<hr>
<h2 id="Engineering-the-Neural-Collapse-Geometry-of-Supervised-Contrastive-Loss"><a href="#Engineering-the-Neural-Collapse-Geometry-of-Supervised-Contrastive-Loss" class="headerlink" title="Engineering the Neural Collapse Geometry of Supervised-Contrastive Loss"></a>Engineering the Neural Collapse Geometry of Supervised-Contrastive Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00893">http://arxiv.org/abs/2310.00893</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaidev Gill, Vala Vakilian, Christos Thrampoulidis</li>
<li>for: 这篇论文的目的是提出一种代替cross-entropy（CE）的分类任务方法，使用相似性在嵌入空间来允许更加丰富的表示。</li>
<li>methods: 这篇论文提出了 modifying the contrastive loss来引导学习的嵌入空间的geometry的方法，并通过实验发现，在每个batch中包含prototypes可以使得learnt embedding的geometry与prototypes的geometry相似。</li>
<li>results: 通过对深度神经网络进行多个实验， authors validate their findings and show that this method can improve the performance of the classifier.Here’s the full text in Simplified Chinese:</li>
<li>for: 这篇论文的目的是提出一种代替cross-entropy（CE）的分类任务方法，使用相似性在嵌入空间来允许更加丰富的表示。</li>
<li>methods: 这篇论文提出了 modifying the contrastive loss来引导学习的嵌入空间的geometry的方法，并通过实验发现，在每个batch中包含prototypes可以使得learnt embedding的geometry与prototypes的geometry相似。</li>
<li>results: 通过对深度神经网络进行多个实验， authors validate their findings and show that this method can improve the performance of the classifier.<details>
<summary>Abstract</summary>
Supervised-contrastive loss (SCL) is an alternative to cross-entropy (CE) for classification tasks that makes use of similarities in the embedding space to allow for richer representations. In this work, we propose methods to engineer the geometry of these learnt feature embeddings by modifying the contrastive loss. In pursuit of adjusting the geometry we explore the impact of prototypes, fixed embeddings included during training to alter the final feature geometry. Specifically, through empirical findings, we demonstrate that the inclusion of prototypes in every batch induces the geometry of the learnt embeddings to align with that of the prototypes. We gain further insights by considering a limiting scenario where the number of prototypes far outnumber the original batch size. Through this, we establish a connection to cross-entropy (CE) loss with a fixed classifier and normalized embeddings. We validate our findings by conducting a series of experiments with deep neural networks on benchmark vision datasets.
</details>
<details>
<summary>摘要</summary>
超级vised-contrastive loss (SCL) 是一种用于分类任务的替代方法，它利用特征空间中的相似性来允许更加丰富的表示。在这项工作中，我们提议修改对冲损失来控制learnt的特征嵌入的geometry。为了调整geometry，我们探索包括在训练中添加prototype的方法。具体来说，我们通过实验发现，在每个batch中包含prototype的 inclusioninduceslearnt的特征嵌入的geometry与prototype的geometry相对符合。我们还通过考虑一种情况，即原始batch size与prototype的数量之间的比例较大，来获得更多的内容。通过这种方式，我们建立了与cross-entropy (CE)损失和固定分类器的连接，并且使用normalized的嵌入。我们验证了我们的发现通过对深度神经网络进行一系列实验，并在图像识别 benchmark datasets 上进行了验证。
</details></li>
</ul>
<hr>
<h2 id="Deep-Neural-Networks-Tend-To-Extrapolate-Predictably"><a href="#Deep-Neural-Networks-Tend-To-Extrapolate-Predictably" class="headerlink" title="Deep Neural Networks Tend To Extrapolate Predictably"></a>Deep Neural Networks Tend To Extrapolate Predictably</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00873">http://arxiv.org/abs/2310.00873</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/katiekang1998/cautious_extrapolation">https://github.com/katiekang1998/cautious_extrapolation</a></li>
<li>paper_authors: Katie Kang, Amrith Setlur, Claire Tomlin, Sergey Levine</li>
<li>for: 该研究检验了神经网络对不同类型的输入数据的预测性能，以及如何在面对不同类型的输入数据时使用神经网络进行风险感知。</li>
<li>methods: 该研究使用了多个 datasets，不同的损失函数和网络架构，并通过观察神经网络预测值的变化情况来描述神经网络在面对不同类型的输入数据时的行为。</li>
<li>results: 研究发现，对于高维输入的神经网络预测结果往往受到输入数据的类型的影响，而且在输入数据变得越来越不同于训练数据时，神经网络预测结果往往会变得更加稳定，并且与最优常数解（OCS）之间的差异逐渐减少。<details>
<summary>Abstract</summary>
Conventional wisdom suggests that neural network predictions tend to be unpredictable and overconfident when faced with out-of-distribution (OOD) inputs. Our work reassesses this assumption for neural networks with high-dimensional inputs. Rather than extrapolating in arbitrary ways, we observe that neural network predictions often tend towards a constant value as input data becomes increasingly OOD. Moreover, we find that this value often closely approximates the optimal constant solution (OCS), i.e., the prediction that minimizes the average loss over the training data without observing the input. We present results showing this phenomenon across 8 datasets with different distributional shifts (including CIFAR10-C and ImageNet-R, S), different loss functions (cross entropy, MSE, and Gaussian NLL), and different architectures (CNNs and transformers). Furthermore, we present an explanation for this behavior, which we first validate empirically and then study theoretically in a simplified setting involving deep homogeneous networks with ReLU activations. Finally, we show how one can leverage our insights in practice to enable risk-sensitive decision-making in the presence of OOD inputs.
</details>
<details>
<summary>摘要</summary>
We present results demonstrating this phenomenon across 8 datasets with different distributional shifts, loss functions, and architectures. We also provide an explanation for this behavior, which we first validate empirically and then study theoretically in a simplified setting involving deep homogeneous networks with ReLU activations. Finally, we show how one can leverage our insights in practice to enable risk-sensitive decision-making in the presence of OOD inputs.
</details></li>
</ul>
<hr>
<h2 id="COMPOSER-Scalable-and-Robust-Modular-Policies-for-Snake-Robots"><a href="#COMPOSER-Scalable-and-Robust-Modular-Policies-for-Snake-Robots" class="headerlink" title="COMPOSER: Scalable and Robust Modular Policies for Snake Robots"></a>COMPOSER: Scalable and Robust Modular Policies for Snake Robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00871">http://arxiv.org/abs/2310.00871</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuyou Zhang, Yaru Niu, Xingyu Liu, Ding Zhao</li>
<li>For:	+ The paper aims to develop a control policy for snake robots that leverages their hyper-redundancy and flexibility to enhance robustness and generalizability.* Methods:	+ The paper formulates the control of the snake robot as a cooperative Multi-Agent Reinforcement Learning (MARL) problem, with each segment of the snake robot functioning as an individual agent.	+ The paper incorporates a self-attention mechanism to enhance cooperative behavior between agents, and proposes a high-level imagination policy to provide additional rewards to guide the low-level control policy.* Results:	+ The proposed method COMPOSER achieves the highest success rate across five snake robot tasks, including goal reaching, wall climbing, shape formation, tube crossing, and block pushing, compared to a centralized baseline and four modular policy baselines.	+ The method demonstrates enhanced robustness against module corruption and significantly superior zero-shot generalizability.Here is the information in Simplified Chinese text:* For:	+ 论文目标是开发一种利用蛇机器人的超低维度和灵活性来增强Robustness和普遍性的控制策略。* Methods:	+ 论文将蛇机器人控制问题设置为一个协同多智能体学习（MARL）问题，每个蛇机器人段都作为一个个体 Agent。	+ 论文含有自注意机制来增强协同行为，并提出一种高级幻想策略来为低级控制策略提供额外奖励。* Results:	+ 提案的方法COMPOSER在五个蛇机器人任务中取得了最高成功率，包括目标达成、墙 climbing、形态形成、管道跨越和块推动等，比中央基线和四个模块策略基线高。	+ 方法还表现出了增强的模块腐坏鲁棒性和零基础学习可重复性。<details>
<summary>Abstract</summary>
Snake robots have showcased remarkable compliance and adaptability in their interaction with environments, mirroring the traits of their natural counterparts. While their hyper-redundant and high-dimensional characteristics add to this adaptability, they also pose great challenges to robot control. Instead of perceiving the hyper-redundancy and flexibility of snake robots as mere challenges, there lies an unexplored potential in leveraging these traits to enhance robustness and generalizability at the control policy level. We seek to develop a control policy that effectively breaks down the high dimensionality of snake robots while harnessing their redundancy. In this work, we consider the snake robot as a modular robot and formulate the control of the snake robot as a cooperative Multi-Agent Reinforcement Learning (MARL) problem. Each segment of the snake robot functions as an individual agent. Specifically, we incorporate a self-attention mechanism to enhance the cooperative behavior between agents. A high-level imagination policy is proposed to provide additional rewards to guide the low-level control policy. We validate the proposed method COMPOSER with five snake robot tasks, including goal reaching, wall climbing, shape formation, tube crossing, and block pushing. COMPOSER achieves the highest success rate across all tasks when compared to a centralized baseline and four modular policy baselines. Additionally, we show enhanced robustness against module corruption and significantly superior zero-shot generalizability in our proposed method. The videos of this work are available on our project page: https://sites.google.com/view/composer-snake/.
</details>
<details>
<summary>摘要</summary>
神经骨蟹机器人在与环境互动中表现出了惊人的适应性和灵活性，与其自然对应者一样。尽管神经骨蟹机器人的高级别和多维度特征增加了控制难度，但是这些特征也隐藏了控制策略的不利影响。我们寻求开发一种控制策略，可以有效地将神经骨蟹机器人的高维度特征纳入控制范畴，同时利用其灵活性。在这项工作中，我们将神经骨蟹机器人视为模块化机器人，并将其控制问题形式为合作多智能体学习（MARL）问题。每个神经骨蟹机器人段功能为个体代理。我们采用自注意机制来增强代理之间的合作行为。我们还提出了高级别想象策略，以提供低级别控制策略的引导。我们验证了我们的方法COMPOSER，并在五个神经骨蟹机器人任务中取得了最高成功率，比中央基线和四个模块策略基线更高。此外，我们还证明了我们的方法具有更高的机器人模块损害robustness和零基础学习能力。视频 demo 可以在我们项目页面上找到：https://sites.google.com/view/composer-snake/.
</details></li>
</ul>
<hr>
<h2 id="Drug-Discovery-with-Dynamic-Goal-aware-Fragments"><a href="#Drug-Discovery-with-Dynamic-Goal-aware-Fragments" class="headerlink" title="Drug Discovery with Dynamic Goal-aware Fragments"></a>Drug Discovery with Dynamic Goal-aware Fragments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00841">http://arxiv.org/abs/2310.00841</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seul Lee, Seanie Lee, Sung Ju Hwang</li>
<li>for: 用于药物探索和发现新药候选体</li>
<li>methods: 使用目标化学性质信息瓶颈原理提取目标化学性质的重要片段，并将其组装成一个有目标性的片段词典。然后通过增强的碎片修改模块，继续探索和更新碎片词典。</li>
<li>results: 通过三个模块的生成循环，GEAM有效地探索和发现了许多有优点的药物候选体。<details>
<summary>Abstract</summary>
Fragment-based drug discovery is an effective strategy for discovering drug candidates in the vast chemical space, and has been widely employed in molecular generative models. However, many existing fragment extraction methods in such models do not take the target chemical properties into account or rely on heuristic rules. Additionally, the existing fragment-based generative models cannot update the fragment vocabulary with goal-aware fragments newly discovered during the generation. To this end, we propose a molecular generative framework for drug discovery, named Goal-aware fragment Extraction, Assembly, and Modification (GEAM). GEAM consists of three modules, each responsible for goal-aware fragment extraction, fragment assembly, and fragment modification. The fragment extraction module identifies important fragments that contribute to the desired target properties with the information bottleneck principle, thereby constructing an effective goal-aware fragment vocabulary. Moreover, GEAM can explore beyond the initial vocabulary with the fragment modification module, and the exploration is further enhanced through the dynamic goal-aware vocabulary update. We experimentally demonstrate that GEAM effectively discovers drug candidates through the generative cycle of the three modules in various drug discovery tasks.
</details>
<details>
<summary>摘要</summary>
Fragment-based drug discovery 是一种有效的探索药物候选者的策略，广泛应用于分子生成模型中。然而，许多现有的 Fragment 提取方法在这些模型中并不考虑目标化学性质或者采用规则性的方法。此外，现有的 Fragment-based 生成模型无法在生成过程中更新 Fragment 词汇库，以满足目标化学性质。为此，我们提出了一种用于药物探索的分子生成框架，名为 Goal-aware Fragment Extraction、Assembly、and Modification（GEAM）。 GEAM 包括三个模块，每个模块负责goal-aware Fragment 提取、Fragment 组装和Fragment 修改。 Fragment 提取模块通过信息瓶颈原理来确定重要的 Fragment，以构建有效的目标化学性质相关的 Fragment 词汇库。此外，GEAM 可以在生成过程中超越初始词汇库，并通过动态更新目标化学性质相关的 Fragment 词汇库来进一步增强探索。我们在多个药物探索任务中实验表明，GEAM 能够有效地通过生成模型的三个模块来找到药物候选者。
</details></li>
</ul>
<hr>
<h2 id="Subsurface-Characterization-using-Ensemble-based-Approaches-with-Deep-Generative-Models"><a href="#Subsurface-Characterization-using-Ensemble-based-Approaches-with-Deep-Generative-Models" class="headerlink" title="Subsurface Characterization using Ensemble-based Approaches with Deep Generative Models"></a>Subsurface Characterization using Ensemble-based Approaches with Deep Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00839">http://arxiv.org/abs/2310.00839</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jichao1/wgan-gp">https://github.com/jichao1/wgan-gp</a></li>
<li>paper_authors: Jichao Bao, Hongkyu Yoon, Jonghyun Lee<br>for:This paper aims to accurately and efficiently estimate spatially distributed properties like hydraulic conductivity (K) from sparse measurements using a deep generative model and ensemble-based inversion method.methods:The proposed method combines Wasserstein Generative Adversarial Network with Gradient Penalty (WGAN-GP) and Ensemble Smoother with Multiple Data Assimilation (ES-MDA) to generate high-dimensional K fields from a low-dimensional latent space and update the latent variables by assimilating available measurements.results:The proposed method accurately characterizes the main features of the unknown K fields with reliable uncertainty quantification, outperforming a widely-used variational inversion approach, especially for channelized and fractured field examples. The ensemble-based approach smooths out the complex objective function surface during minimization, leading to improved performance.<details>
<summary>Abstract</summary>
Estimating spatially distributed properties such as hydraulic conductivity (K) from available sparse measurements is a great challenge in subsurface characterization. However, the use of inverse modeling is limited for ill-posed, high-dimensional applications due to computational costs and poor prediction accuracy with sparse datasets. In this paper, we combine Wasserstein Generative Adversarial Network with Gradient Penalty (WGAN-GP), a deep generative model that can accurately capture complex subsurface structure, and Ensemble Smoother with Multiple Data Assimilation (ES-MDA), an ensemble-based inversion method, for accurate and accelerated subsurface characterization. WGAN-GP is trained to generate high-dimensional K fields from a low-dimensional latent space and ES-MDA then updates the latent variables by assimilating available measurements. Several subsurface examples are used to evaluate the accuracy and efficiency of the proposed method and the main features of the unknown K fields are characterized accurately with reliable uncertainty quantification. Furthermore, the estimation performance is compared with a widely-used variational, i.e., optimization-based, inversion approach, and the proposed approach outperforms the variational inversion method, especially for the channelized and fractured field examples. We explain such superior performance by visualizing the objective function in the latent space: because of nonlinear and aggressive dimension reduction via generative modeling, the objective function surface becomes extremely complex while the ensemble approximation can smooth out the multi-modal surface during the minimization. This suggests that the ensemble-based approach works well over the variational approach when combined with deep generative models at the cost of forward model runs unless convergence-ensuring modifications are implemented in the variational inversion.
</details>
<details>
<summary>摘要</summary>
估算沿体分布的特性，如水利导能（K），从可用的稀疏测量数据中估算是一项大allenge。然而，因为这类应用的维度太多，使用反向模型受限于计算成本和精度不高。在这篇论文中，我们结合 Wasserstein Generative Adversarial Network with Gradient Penalty（WGAN-GP）和 Ensemble Smoother with Multiple Data Assimilation（ES-MDA），一种深度生成模型和一种ensemble-based倒推方法，以实现高精度和加速的地下特性估算。WGAN-GP是用于生成高维K场的深度生成模型，ES-MDA则将可用测量数据 assimilate到latent变量中。我们使用多个地下示例来评估提案的准确性和效率，并发现提案可以准确地 caracterize unknown K场的主要特征，并提供可靠的不确定量评估。此外，我们与一种广泛使用的变量，即优化基于推理的倒推方法进行比较，并发现提案的方法在渠化和裂隙场示例中表现更优异。我们通过Visualizing the objective function in the latent space来解释这种更好的性能，因为通过非线性和攻击性的维度减少，生成模型可以生成非常复杂的目标函数表面，而ensemble approximation可以在最小化过程中平滑出多模态表面。这表明， ensemble-based方法在Variational inversion方法中表现更好，尤其是在渠化和裂隙场示例中。
</details></li>
</ul>
<hr>
<h2 id="Online-Sensitivity-Optimization-in-Differentially-Private-Learning"><a href="#Online-Sensitivity-Optimization-in-Differentially-Private-Learning" class="headerlink" title="Online Sensitivity Optimization in Differentially Private Learning"></a>Online Sensitivity Optimization in Differentially Private Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00829">http://arxiv.org/abs/2310.00829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Filippo Galli, Catuscia Palamidessi, Tommaso Cucinotta</li>
<li>for: 这篇研究的目的是为了开发具有隐私保证的机器学习模型，并且需要对个人贡献的限制。</li>
<li>methods: 这篇研究使用的方法包括将个人的梯度转换为$2$-norm，并且在批制程中进行数据隐藏。</li>
<li>results: 这篇研究的结果显示，这种动态化 clipping 阈值的方法可以与固定的阈值比较，具有相同或更好的性能，并且可以在不同的数据集、模型结构和隐私水平下进行最佳化。<details>
<summary>Abstract</summary>
Training differentially private machine learning models requires constraining an individual's contribution to the optimization process. This is achieved by clipping the $2$-norm of their gradient at a predetermined threshold prior to averaging and batch sanitization. This selection adversely influences optimization in two opposing ways: it either exacerbates the bias due to excessive clipping at lower values, or augments sanitization noise at higher values. The choice significantly hinges on factors such as the dataset, model architecture, and even varies within the same optimization, demanding meticulous tuning usually accomplished through a grid search. In order to circumvent the privacy expenses incurred in hyperparameter tuning, we present a novel approach to dynamically optimize the clipping threshold. We treat this threshold as an additional learnable parameter, establishing a clean relationship between the threshold and the cost function. This allows us to optimize the former with gradient descent, with minimal repercussions on the overall privacy analysis. Our method is thoroughly assessed against alternative fixed and adaptive strategies across diverse datasets, tasks, model dimensions, and privacy levels. Our results demonstrate its comparable or superior performance in all evaluated scenarios, given the same privacy requirements.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/02/cs.LG_2023_10_02/" data-id="clp869u0m00sjk5881cikdue3" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_10_02" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/02/eess.IV_2023_10_02/" class="article-date">
  <time datetime="2023-10-02T09:00:00.000Z" itemprop="datePublished">2023-10-02</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/02/eess.IV_2023_10_02/">eess.IV - 2023-10-02</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Restoration-Network-as-an-Implicit-Prior"><a href="#A-Restoration-Network-as-an-Implicit-Prior" class="headerlink" title="A Restoration Network as an Implicit Prior"></a>A Restoration Network as an Implicit Prior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01391">http://arxiv.org/abs/2310.01391</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuyang Hu, Mauricio Delbracio, Peyman Milanfar, Ulugbek S. Kamilov</li>
<li>for: 这个论文是为了解决图像恢复问题而写的。</li>
<li>methods: 这个论文使用的方法是使用先行训练的深度神经网络作为恢复运算的假设。</li>
<li>results:  numerical results show that the method using a super-resolution prior achieves state-of-the-art performance both quantitatively and qualitatively。Here’s the full translation of the abstract in Simplified Chinese:</li>
<li>for: 这个论文是为了解决图像恢复问题而写的。</li>
<li>methods: 这个论文使用的方法是使用先行训练的深度神经网络作为恢复运算的假设。</li>
<li>results: 数值结果表明，使用超解像前提可以达到状态体系数量和质量上的最佳性能。I hope that helps!<details>
<summary>Abstract</summary>
Image denoisers have been shown to be powerful priors for solving inverse problems in imaging. In this work, we introduce a generalization of these methods that allows any image restoration network to be used as an implicit prior. The proposed method uses priors specified by deep neural networks pre-trained as general restoration operators. The method provides a principled approach for adapting state-of-the-art restoration models for other inverse problems. Our theoretical result analyzes its convergence to a stationary point of a global functional associated with the restoration operator. Numerical results show that the method using a super-resolution prior achieves state-of-the-art performance both quantitatively and qualitatively. Overall, this work offers a step forward for solving inverse problems by enabling the use of powerful pre-trained restoration models as priors.
</details>
<details>
<summary>摘要</summary>
图像去噪器已经被证明是解析问题的强大先验。在这项工作中，我们介绍了一种扩展这些方法，允许任何图像恢复网络被用作隐藏先验。我们的方法使用由深度神经网络预训练为普通恢复操作器的先验。我们的理论结果分析其趋向于一个全局函数相关的恢复运算的站点点。数值结果表明，使用超解像先验可以达到现代水平的性能 both quantitatively and qualitatively。总之，这项工作为解析问题提供了一个前进，允许使用强大的预训练恢复模型作为先验。
</details></li>
</ul>
<hr>
<h2 id="Predicting-Lung-Cancer’s-Metastats’-Locations-Using-Bioclinical-Model"><a href="#Predicting-Lung-Cancer’s-Metastats’-Locations-Using-Bioclinical-Model" class="headerlink" title="Predicting Lung Cancer’s Metastats’ Locations Using Bioclinical Model"></a>Predicting Lung Cancer’s Metastats’ Locations Using Bioclinical Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.08596">http://arxiv.org/abs/2310.08596</a></li>
<li>repo_url: None</li>
<li>paper_authors: Teddy Lazebnik, Svetlana Bunimovich-Mendrazitsky</li>
<li>for: 预测肺癌 метастаisis的空间扩散</li>
<li>methods: 使用三维计算机断层成像(CT)扫描和生物物理学模型</li>
<li>results: 实验 validate the bioclinical model on 10 patient data, with 74% accuracy in metastasis location prediction.<details>
<summary>Abstract</summary>
Lung cancer is a leading cause of cancer-related deaths worldwide. The spread of the disease from its primary site to other parts of the lungs, known as metastasis, significantly impacts the course of treatment. Early identification of metastatic lesions is crucial for prompt and effective treatment, but conventional imaging techniques have limitations in detecting small metastases. In this study, we develop a bioclinical model for predicting the spatial spread of lung cancer's metastasis using a three-dimensional computed tomography (CT) scan. We used a three-layer biological model of cancer spread to predict locations with a high probability of metastasis colonization. We validated the bioclinical model on real-world data from 10 patients, showing promising 74% accuracy in the metastasis location prediction. Our study highlights the potential of the combination of biophysical and ML models to advance the way that lung cancer is diagnosed and treated, by providing a more comprehensive understanding of the spread of the disease and informing treatment decisions.
</details>
<details>
<summary>摘要</summary>
肺癌是全球最主要的肿瘤相关死亡原因之一。肿瘤从主要位点至其他肺部的传播，即肿瘤转移，对治疗诊断产生重要影响。早期识别转移 lesions 非常重要，但传统的成像技术有限制可以探测小转移。在这项研究中，我们开发了一种生物клиниче模型，用于预测肺癌转移的空间扩散。我们使用了三层生物模型来预测可能受感染的位置。我们验证了生物клиниче模型使用实际数据，从10名患者中提取了数据，并达到了74%的准确率。我们的研究表明，将生物物理和机器学习模型相结合，可以推动肺癌的诊断和治疗方法的进步，提供更全面的疾病扩散的理解，以及更加准确的诊断和治疗决策。
</details></li>
</ul>
<hr>
<h2 id="Fourier-PD-and-PDUNet-Complex-valued-networks-to-speed-up-MR-Thermometry-during-Hypterthermia"><a href="#Fourier-PD-and-PDUNet-Complex-valued-networks-to-speed-up-MR-Thermometry-during-Hypterthermia" class="headerlink" title="Fourier PD and PDUNet: Complex-valued networks to speed-up MR Thermometry during Hypterthermia"></a>Fourier PD and PDUNet: Complex-valued networks to speed-up MR Thermometry during Hypterthermia</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01073">http://arxiv.org/abs/2310.01073</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rupali Khatun, Soumick Chatterjee, Christoph Bert, Martin Wadepohl, Manfred Schmidt, Oliver J. Ott, Rainer Fietkau, Andreas Nürnberger, Udo S. Gaipl, Benjamin Frey<br>for: 这个研究的目的是提高下采样的MR温度测量数据重建的解决方案，以提高解决速度和减少artefacts。methods: 这个研究使用了深度学习技术来重建高度下采样的MR温度测量数据，并使用了两种不同的深度学习模型：Fourier Primal-Dual网络和Fourier Primal-Dual UNet。results: 研究发现，使用深度学习模型可以减少下采样MR温度测量数据与完全采样MR温度测量数据之间的温度差距，从1.5 $\degree$C降至0.5 $\degree$C。<details>
<summary>Abstract</summary>
Hyperthermia (HT) in combination with radio- and/or chemotherapy has become an accepted cancer treatment for distinct solid tumour entities. In HT, tumour tissue is exogenously heated to temperatures of 39 to 43 $\degree$C for 60 minutes. Temperature monitoring can be performed noninvasively using dynamic magnetic resonance imaging (MRI). However, the slow nature of MRI leads to motion artefacts in the images due to the movements of patients during image acquisition time. By discarding parts of the data, the speed of the acquisition can be increased - known as Undersampling. However, due to the invalidation of the Nyquist criterion, the acquired images have lower resolution and can also produce artefacts. The aim of this work was, therefore, to reconstruct highly undersampled MR thermometry acquisitions with better resolution and with less artefacts compared to conventional techniques like compressed sensing. The use of deep learning in the medical field has emerged in recent times, and various studies have shown that deep learning has the potential to solve inverse problems such as MR image reconstruction. However, most of the published work only focusses on the magnitude images, while the phase images are ignored, which are fundamental requirements for MR thermometry. This work, for the first time ever, presents deep learning based solutions for reconstructing undersampled MR thermometry data. Two different deep learning models have been employed here, the Fourier Primal-Dual network and Fourier Primal-Dual UNet, to reconstruct highly undersampled complex images of MR thermometry. It was observed that the method was able to reduce the temperature difference between the undersampled MRIs and the fully sampled MRIs from 1.5 $\degree$C to 0.5 $\degree$C.
</details>
<details>
<summary>摘要</summary>
高级热辐射（HT）在结合放射线和/或化学疗法的抗癌治疗中得到了承认。在HT中，肿瘤组织被外源性加热到39-43℃的温度，持续60分钟。肿瘤组织的温度可以非侵入性地监测使用动力磁共振成像（MRI）。然而，由于患者在获取图像时的运动，MRI图像会受到运动artefacts的影响。通过抛弃一部分数据，可以快速化图像获取过程 -  bekannt als Undersampling。然而，由于遵循 Nyquist  критериion的无效化，获取的图像具有更低的分辨率，并且可能会产生artefacts。因此，本研究的目标是使用深度学习来重建高度受抽象的 MR 热图像数据，以提高分辨率和减少artefacts，并且不同于传统的压缩感知技术。深度学习在医学领域的应用已经在最近几年得到了广泛的关注，而且许多研究表明，深度学习有可能解决 inverse 问题，如 MR 图像重建。然而，大多数已发表的研究仅关注了 magnitude 图像，而忽略了阶跃图像，这些图像是 MR 热测量的基本需求。本研究是首次使用深度学习来重建高度受抽象的 MR 热图像数据。我们在这里采用了两种深度学习模型：Fourier Primal-Dual 网络和 Fourier Primal-Dual UNet，来重建高度受抽象的 MR 热图像。我们发现，该方法可以将高级受抽象 MR 热图像和完全样本 MR 热图像之间的温度差降低至0.5℃。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/02/eess.IV_2023_10_02/" data-id="clp869u7k01b0k5889fh0ckjq" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_10_02" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/02/eess.SP_2023_10_02/" class="article-date">
  <time datetime="2023-10-02T08:00:00.000Z" itemprop="datePublished">2023-10-02</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/02/eess.SP_2023_10_02/">eess.SP - 2023-10-02</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="OFDM-RSMA-Robust-Transmission-under-Inter-Carrier-Interference"><a href="#OFDM-RSMA-Robust-Transmission-under-Inter-Carrier-Interference" class="headerlink" title="OFDM-RSMA: Robust Transmission under Inter-Carrier Interference"></a>OFDM-RSMA: Robust Transmission under Inter-Carrier Interference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01686">http://arxiv.org/abs/2310.01686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mehmet Mert Sahin, Onur Dizdar, Bruno Clerckx, Huseyin Arslan</li>
<li>for: 这篇论文目的是提出一种基于rate-splitting多访问(RSMA)和orthogonal frequency division multiplexing(OFDM)的下行传输方案，以提高多用户多antenna系统中的性能。</li>
<li>methods: 该论文使用了weighted minimum mean-square error(WMMSE)算法来解决非对称问题，并且利用了RSMA的干扰管理能力来处理多个干扰源的干扰。</li>
<li>results: 论文的计算结果表明，在不同的媒体通道条件下，提案的OFDM-RSMA方案可以与OFDMA和NOMA方案相比，具有更高的总速率性能。<details>
<summary>Abstract</summary>
Rate-splitting multiple access (RSMA) is a multiple access scheme to mitigate the effects of the multi-user interference (MUI) in multi-antenna systems. In this study, we leverage the interference management capabilities of RSMA to tackle the issue of inter-carrier interference (ICI) in orthogonal frequency division multiplexing (OFDM) waveform. We formulate a sum-rate maximization problem to find the optimal subcarrier and power allocation for downlink transmission in a two-user system using RSMA and OFDM. A weighted minimum mean-square error (WMMSE)-based algorithm is proposed to obtain a solution for the formulated non-convex problem. We show that the marriage of rate-splitting (RS) with OFDM provides complementary strengths to cope with peculiar characteristic of wireless medium and its performance-limiting challenges including inter-symbol interference (ISI), MUI, ICI, and inter-numerology interference (INI). The sum-rate performance of the proposed OFDM-RSMA scheme is numerically compared with that of conventional orthogonal frequency division multiple access (OFDMA) and OFDM-non-orthogonal multiple access (NOMA). It is shown that the proposed OFDM-RSMA outperforms OFDM-NOMA and OFDMA in diverse propagation channel conditions owing to its flexible structure and robust interference management capabilities.
</details>
<details>
<summary>摘要</summary>
rate-splitting多Access（RSMA）是一种多Access方案，用于 Mitigate the effects of multi-user interference（MUI）in multi-antenna systems. In this study, we leverage the interference management capabilities of RSMA to tackle the issue of inter-carrier interference（ICI）in orthogonal frequency division multiplexing（OFDM）waveform. We formulate a sum-rate maximization problem to find the optimal subcarrier and power allocation for downlink transmission in a two-user system using RSMA and OFDM. A weighted minimum mean-square error（WMMSE）-based algorithm is proposed to obtain a solution for the formulated non-convex problem. We show that the marriage of rate-splitting（RS）with OFDM provides complementary strengths to cope with the peculiar characteristic of wireless medium and its performance-limiting challenges including inter-symbol interference（ISI）, MUI, ICI, and inter-numerology interference（INI）. The sum-rate performance of the proposed OFDM-RSMA scheme is numerically compared with that of conventional orthogonal frequency division multiple access（OFDMA）and OFDM-non-orthogonal multiple access（NOMA）. It is shown that the proposed OFDM-RSMA outperforms OFDM-NOMA and OFDMA in diverse propagation channel conditions owing to its flexible structure and robust interference management capabilities.
</details></li>
</ul>
<hr>
<h2 id="Data-driven-Forced-Oscillation-Localization-using-Inferred-Impulse-Responses"><a href="#Data-driven-Forced-Oscillation-Localization-using-Inferred-Impulse-Responses" class="headerlink" title="Data-driven Forced Oscillation Localization using Inferred Impulse Responses"></a>Data-driven Forced Oscillation Localization using Inferred Impulse Responses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01656">http://arxiv.org/abs/2310.01656</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ShaohuiLiu/fo_local">https://github.com/ShaohuiLiu/fo_local</a></li>
<li>paper_authors: Shaohui Liu, Hao Zhu, Vassilis Kekatos</li>
<li>for: 本文旨在推断强制振荡（FO）的来源，只使用同步phasor测量数据。</li>
<li>methods: 提议的数据驱动框架使用快速环境数据进行小信号响应的恢复，不需要系统模型。在FO事件发生时，使用预先恢复的冲击响应进行频域分析，并使用LS误差函数进行适应。</li>
<li>results: 数字验证表明该方法可以应用于实际的电力系统，包括非线性、高阶动力学和控制效应。总的来说，该方法可以准确地推断FO的来源，并且可以扩展到不同的测量类型和部分感知覆盖条件。<details>
<summary>Abstract</summary>
Poorly damped oscillations pose threats to the stability and reliability of interconnected power systems. In this work, we propose a comprehensive data-driven framework for inferring the sources of forced oscillation (FO) using only synchrophasor measurements. During normal grid operations, fast-rate ambient data are collected to recover the impulse responses in the small-signal regime, without requiring the system models. When FO events occur, the source is estimated based on the frequency domain analysis by fitting the least-squares (LS) error for the FO data using the impulse responses recovered previously. Although the proposed framework is purely data-driven, the result has been established theoretically via model-based analysis of linearized dynamics under a few realistic assumptions. Numerical validations demonstrate its applicability to realistic power systems including nonlinear, higher-order dynamics with control effects using the IEEE 68-bus system. The generalizability of the proposed methodology has been validated using different types of measurements and partial sensor coverage conditions.
</details>
<details>
<summary>摘要</summary>
低刚性振荡会对电力系统稳定性和可靠性提出威胁。在这项工作中，我们提议了一个全面的数据驱动方法，使用同步phasor测量来推测强制振荡（FO）的来源。在正常网络运行时， быстро速的 ambient数据被收集来恢复小信号域中的冲击响应，无需系统模型。当FO事件发生时，源的估计基于频域分析，通过LS误差适应FO数据使用先前恢复的冲击响应进行适应。 although the proposed framework is purely data-driven, the result has been established theoretically via model-based analysis of linearized dynamics under a few realistic assumptions. numerical validations demonstrate its applicability to realistic power systems including nonlinear, higher-order dynamics with control effects using the IEEE 68-bus system. The generalizability of the proposed methodology has been validated using different types of measurements and partial sensor coverage conditions.
</details></li>
</ul>
<hr>
<h2 id="Near-field-Integrated-Sensing-and-Communication-Opportunities-and-Challenges"><a href="#Near-field-Integrated-Sensing-and-Communication-Opportunities-and-Challenges" class="headerlink" title="Near-field Integrated Sensing and Communication: Opportunities and Challenges"></a>Near-field Integrated Sensing and Communication: Opportunities and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01342">http://arxiv.org/abs/2310.01342</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayi Cong, Changsheng You, Jiapeng Li, Li Chen, Beixiong Zheng, Yuanwei Liu, Wen Wu, Yi Gong, Shi Jin, Rui Zhang</li>
<li>for:  investigate the near-field ISAC, which integrates sensing and communication in the near-field region</li>
<li>methods:  joint near-field communication and sensing, sensing-assisted near-field communication, and communication-assisted near-field sensing</li>
<li>results:  new research opportunities, new design issues, and promising solutions for near-field ISAC<details>
<summary>Abstract</summary>
With the extremely large-scale array XL-array deployed in future wireless systems, wireless communication and sensing are expected to operate in the radiative near-field region, which needs to be characterized by the spherical rather than planar wavefronts. Unlike most existing works that considered far-field integrated sensing and communication (ISAC), we study in this article the new near-field ISAC, which integrates both functions of sensing and communication in the near-field region. To this end, we first discuss the appealing advantages of near-field communication and sensing over their far-field counterparts, respectively. Then, we introduce three approaches for near-field ISAC, including joint near-field communication and sensing, sensing-assisted near-field communication, and communication-assisted near-field sensing. We discuss their individual research opportunities, new design issues, as well as propose promising solutions. Finally, several important directions in near-field ISAC are also highlighted to motivate future work.
</details>
<details>
<summary>摘要</summary>
With the extremely large-scale array XL-array deployed in future wireless systems, wireless communication and sensing are expected to operate in the radiative near-field region, which needs to be characterized by the spherical rather than planar wavefronts. Unlike most existing works that considered far-field integrated sensing and communication (ISAC), we study in this article the new near-field ISAC, which integrates both functions of sensing and communication in the near-field region. To this end, we first discuss the appealing advantages of near-field communication and sensing over their far-field counterparts, respectively. Then, we introduce three approaches for near-field ISAC, including joint near-field communication and sensing, sensing-assisted near-field communication, and communication-assisted near-field sensing. We discuss their individual research opportunities, new design issues, as well as propose promising solutions. Finally, several important directions in near-field ISAC are also highlighted to motivate future work.Here's the translation in Traditional Chinese:With the extremely large-scale array XL-array deployed in future wireless systems, wireless communication and sensing are expected to operate in the radiative near-field region, which needs to be characterized by the spherical rather than planar wavefronts. Unlike most existing works that considered far-field integrated sensing and communication (ISAC), we study in this article the new near-field ISAC, which integrates both functions of sensing and communication in the near-field region. To this end, we first discuss the appealing advantages of near-field communication and sensing over their far-field counterparts, respectively. Then, we introduce three approaches for near-field ISAC, including joint near-field communication and sensing, sensing-assisted near-field communication, and communication-assisted near-field sensing. We discuss their individual research opportunities, new design issues, as well as propose promising solutions. Finally, several important directions in near-field ISAC are also highlighted to motivate future work.
</details></li>
</ul>
<hr>
<h2 id="Cell-Free-Bistatic-Backscatter-Communication-Channel-Estimation-Optimization-and-Performance-Analysis"><a href="#Cell-Free-Bistatic-Backscatter-Communication-Channel-Estimation-Optimization-and-Performance-Analysis" class="headerlink" title="Cell-Free Bistatic Backscatter Communication: Channel Estimation, Optimization, and Performance Analysis"></a>Cell-Free Bistatic Backscatter Communication: Channel Estimation, Optimization, and Performance Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01264">http://arxiv.org/abs/2310.01264</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diluka Galappaththige, Fatemeh Rezaei, Chintha Tellambura, Amine Maaref</li>
<li>for: 这项研究旨在探讨和实现基于终端无尘核心架构和反射式回播通信（BiBC）的未来（EH）基于互联网物联网（IoT）网络中的潜在应用。</li>
<li>methods: 我们首先提出了一种频道估计方案，用于估计提案系统集成的直接、叠加、前向通道。然后，我们使用频道估计来设计最佳的AP束重、标签反射率和接收器滤波器，以最大化标签总速率，并满足标签的最小能量需求。由于提案的最大化问题是非凸的，我们提出了一种基于备选优化、分配Programming和几何Quotient技术的解决方案。</li>
<li>results: 我们 presente extensa numerical results，用于验证我们的频道估计方案和优化框架，以及提案的集成性能。与Random Beamforming&#x2F;Combining benchmark相比，我们的算法实现了非常出色的提升。例如，它在10dBm的输入电平下，使用36个AP和3个标签时，实现了约64.8%和约253.5%的能量收集和标签总速率提升。<details>
<summary>Abstract</summary>
This study introduces and investigates the integration of a cell-free architecture with bistatic backscatter communication (BiBC), referred to as cell-free BiBC or distributed access point (AP)-assisted BiBC, which can enable potential applications in future (EH)-based Internet-of-Things (IoT) networks. To that purpose, we first present a pilot-based channel estimation scheme for estimating the direct, cascaded, forward channels of the proposed system setup. We next utilize the channel estimates for designing the optimal beamforming weights at the APs, reflection coefficients at the tags, and reception filters at the reader to maximize the tag sum rate while meeting the tags' minimum energy requirements. Because the proposed maximization problem is non-convex, we propose a solution based on alternative optimization, fractional programming, and Rayleigh quotient techniques. We also quantify the computational complexity of the developed algorithms. Finally, we present extensive numerical results to validate the proposed channel estimation scheme and optimization framework, as well as the performance of the integration of these two technologies. Compared to the random beamforming/combining benchmark, our algorithm yields impressive gains. For example, it achieves $\sim$ 64.8\% and $\sim$ 253.5\% gains in harvested power and tag sum rate, respectively, for 10 dBm with 36 APs and 3 tags.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Wireless-strain-and-temperature-monitoring-in-reinforced-concrete-using-Surface-Acoustic-Wave-SAW-sensors"><a href="#Wireless-strain-and-temperature-monitoring-in-reinforced-concrete-using-Surface-Acoustic-Wave-SAW-sensors" class="headerlink" title="Wireless strain and temperature monitoring in reinforced concrete using Surface Acoustic Wave (SAW) sensors"></a>Wireless strain and temperature monitoring in reinforced concrete using Surface Acoustic Wave (SAW) sensors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03765">http://arxiv.org/abs/2310.03765</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Jeltiri, Firas Al-Mahmoud, Rémi Boissière, Baptiste Paulmier, Tony Makdissy, Omar Elmazria, Pascal Nicolay, Sami Hage-Ali</li>
<li>for: 这个论文的目的是为了监测土木工程结构的健康状况，并通过植入减形、温度和腐蚀传感器来提高维护和延长服务寿命。</li>
<li>methods: 这个论文使用了商业SAW设备，将其附加到钢矱上，以测量混凝土梁的减形和温度。不需要电缆或内置电子设备。</li>
<li>results: 研究发现，SAW传感器可以有效地测量混凝土梁的减形和温度，并且可以持续三周不间断地测量温度。<details>
<summary>Abstract</summary>
Monitoring the health of civil engineering structures using implanted deformation, temperature and corrosion sensors would further improve maintenance and extend the service life of those structures. However, sensor integration poses a number of problems, due to the presence of cables and on-board electronics. Passive, wireless SAW sensors offer a very promising solution, here. We used commercial SAW devices mounted on steel rebars to carry out an initial feasibility study. Without cables or embedded electronics, we were able to measure the deformation of a concrete beam subjected to bending load. We were also able to measure the temperature continuously over a three-week period.
</details>
<details>
<summary>摘要</summary>
监测ivil工程结构的健康状况使用植入的弯形、温度和腐蚀感知器，可以进一步提高维护和延长结构的服务寿命。但是感知器集成带来了一些问题，即有电缆和固定电子设备的存在。无缆无嵌入电子设备的感知器所提供的可能性非常吸引人。我们使用商业SAW设备 mounted on steel rebars进行了一项初步可行性研究。无需电缆或嵌入电子设备，我们成功地测量了一根受轴向荷重负荷的混凝土箍的弯形。我们还成功地测量了三个星期内的温度连续变化。
</details></li>
</ul>
<hr>
<h2 id="Generative-AI-for-Integrated-Sensing-and-Communication-Insights-from-the-Physical-Layer-Perspective"><a href="#Generative-AI-for-Integrated-Sensing-and-Communication-Insights-from-the-Physical-Layer-Perspective" class="headerlink" title="Generative AI for Integrated Sensing and Communication: Insights from the Physical Layer Perspective"></a>Generative AI for Integrated Sensing and Communication: Insights from the Physical Layer Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01036">http://arxiv.org/abs/2310.01036</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiacheng Wang, Hongyang Du, Dusit Niyato, Jiawen Kang, Shuguang Cui, Xuemin Shen, Ping Zhang</li>
<li>for: 本研究探讨了基于生成人工智能（GAI）的物理层应用，特别是整合感知通信（ISAC）系统的支持。</li>
<li>methods: 本文首先提供了GAI和ISAC的概述，并详细介绍了GAI在物理层上的应用，包括通道估计等方面。</li>
<li>results: 在实验中，提出的扩散模型基本方法能够在近场条件下高精度地估计信号发射方向，具体达到了1.03度的平均方差。这confirm GAI在物理层的支持。<details>
<summary>Abstract</summary>
As generative artificial intelligence (GAI) models continue to evolve, their generative capabilities are increasingly enhanced and being used extensively in content generation. Beyond this, GAI also excels in data modeling and analysis, benefitting wireless communication systems. In this article, we investigate applications of GAI in the physical layer and analyze its support for integrated sensing and communications (ISAC) systems. Specifically, we first provide an overview of GAI and ISAC, touching on GAI's potential support across multiple layers of ISAC. We then concentrate on the physical layer, investigating GAI's applications from various perspectives thoroughly, such as channel estimation, and demonstrate the value of these GAI-enhanced physical layer technologies for ISAC systems. In the case study, the proposed diffusion model-based method effectively estimates the signal direction of arrival under the near-field condition based on the uniform linear array, when antenna spacing surpassing half the wavelength. With a mean square error of 1.03 degrees, it confirms GAI's support for the physical layer in near-field sensing and communications.
</details>
<details>
<summary>摘要</summary>
如果生成人工智能（GAI）模型继续演化，它们的生成能力将会不断增强，并在内容生成中得到广泛的应用。此外，GAI还在数据模型化和分析方面表现出色，对无线通信系统产生了很大的影响。在这篇文章中，我们 investigate GAI在物理层中的应用，并分析它在集成感知通信（ISAC）系统中的支持。我们首先提供GAI和ISAC的概述，然后专门关注物理层，从多种角度进行深入的研究，例如通道估计，并证明GAI在物理层技术方面对ISAC系统提供了价值。在 caso study中，我们提出了基于扩散模型的方法，用于估计信号到来方向，当antenna spacing超过半波长时。该方法的均方差为1.03度，确认了GAI在近场感知通信中的支持。
</details></li>
</ul>
<hr>
<h2 id="Joint-Source-Channel-Coding-System-for-6G-Communication-Design-Prototype-and-Future-Directions"><a href="#Joint-Source-Channel-Coding-System-for-6G-Communication-Design-Prototype-and-Future-Directions" class="headerlink" title="Joint Source-Channel Coding System for 6G Communication: Design, Prototype and Future Directions"></a>Joint Source-Channel Coding System for 6G Communication: Design, Prototype and Future Directions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01024">http://arxiv.org/abs/2310.01024</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinchao Zhong, Sean Longyu Ma, Hong-fu Chou, Arsham Mostaani, Thang X. Vu, Symeon Chatzinotas</li>
<li>for: 本研究旨在超越逻辑 communicate 的优化问题，即将发送源和通道协同设计 source coding 和 channel coding 的整合，以实现未来通信系统的高效性和可靠性。</li>
<li>methods: 本研究使用了 flexible structural hardware design 和 joint source-channel coding (JSCC) 技术，以实现源字符串的多样性和 unity code rate。另外，本研究还使用了 Unequal Error Protection (UEP) 技术，以保持 semantic importance 的恢复。</li>
<li>results: 本研究表明，使用 quasi-cyclic (QC) 特点和 UEP 技术可以实现高效的 semantic communication，并且可以在各种通信频率和信道条件下实现优秀的表达效果。<details>
<summary>Abstract</summary>
The goal of semantic communication is to surpass optimal Shannon's criterion regarding a notable problem for future communication which lies in the integration of collaborative efforts between the intelligence of the transmission source and the joint design of source coding and channel coding. The convergence of scholarly investigation and applicable products in the field of semantic communication is facilitated by the utilization of flexible structural hardware design, which is constrained by the computational capabilities of edge devices. This characteristic represents a significant benefit of joint source-channel coding (JSCC), as it enables the generation of source alphabets with diverse lengths and achieves a code rate of unity. Moreover, JSCC exhibits near-capacity performance while maintaining low complexity. Therefore, we leverage not only quasi-cyclic (QC) characteristics to propose a QC-LDPC code-based JSCC scheme but also Unequal Error Protection (UEP) to ensure the recovery of semantic importance. In this study, the feasibility for using a semantic encoder/decoder that is aware of UEP can be explored based on the existing JSCC system. This approach is aimed at protecting the significance of semantic task-oriented information. Additionally, the deployment of a JSCC system can be facilitated by employing Low-Density Parity-Check (LDPC) codes on a reconfigurable device. This is achieved by reconstructing the LDPC codes as QC-LDPC codes. The QC-LDPC layered decoding technique, which has been specifically optimized for hardware parallelism and tailored for channel decoding applications, can be suitably adapted to accommodate the JSCC system. The performance of the proposed system is evaluated by conducting BER measurements using both floating-point and 6-bit quantization.
</details>
<details>
<summary>摘要</summary>
《semantic communication的目标是超越希耶纳的最佳吞吐量标准，解决未来通信中的一个重要问题，即源传输和通道编码的共同设计。学术研究和实用产品在semantic communication领域的结合，得益于灵活的结构硬件设计，这种设计受到边缘设备的计算能力的限制。这种特点是JSCC的一大优点，它可以生成源字母的不同长度，实现码率unity，同时具有低复杂性和高效率。因此，我们不仅利用逻辑（QC）特征，还提出了LDPC码基于JSCC方案，并通过不等错误保护（UEP）确保 semantic importance的恢复。在这项研究中，我们可以通过现有JSCC系统的semantic编码/解码器来探索使用UEP的可能性。这种方法旨在保护 semantic任务关键信息的重要性。此外，通过在可重新配置的设备上使用LDPC码，我们可以方便地实现JSCC系统的部署。通过重新构造LDPC码为QC-LDPC码，我们可以采用QC-LDPC层次解码技术，这种技术已经特Optimized for hardware parallelism和tailored for channel decoding应用。我们通过使用浮点数和6比特量化进行BER测量来评估提议的系统性能。
</details></li>
</ul>
<hr>
<h2 id="Magnetic-SAW-RFID-Sensor-Based-on-Love-Wave-for-Detection-of-Magnetic-Field-and-Temperature"><a href="#Magnetic-SAW-RFID-Sensor-Based-on-Love-Wave-for-Detection-of-Magnetic-Field-and-Temperature" class="headerlink" title="Magnetic SAW RFID Sensor Based on Love Wave for Detection of Magnetic Field and Temperature"></a>Magnetic SAW RFID Sensor Based on Love Wave for Detection of Magnetic Field and Temperature</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03764">http://arxiv.org/abs/2310.03764</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prince Mengue, Laurine Meistersheim, Sami Hage-Ali, Cécile Floer, Sébastien Petit-Watelot, Daniel Lacour, Michel Hehn, Omar Elmazria</li>
<li>for: 这个研究旨在开发一种抗频率干扰的磁场测量系统，包括磁场的温度补做。</li>
<li>methods: 该研究使用了一种抗频率干扰的磁场探测器，基于ZnO&#x2F;LiNbO$_3$ Ycut (X轴) 层结构，并使用了Co-Fe-B敏感层来探测磁场变化。</li>
<li>results: 研究表明，该探测器在各种温度下展现出了高磁场和温度敏感度，分别为-63 ppm&#x2F;$^\circ$C和-781 ppm&#x2F;mT。此外，该探测器还实现了温度补做，并且可以实现多感器功能和RFID功能。<details>
<summary>Abstract</summary>
Magnetic field measurement including a temperature compensation is essential for a magnetic field sensor. This study investigates a magnetic surface acoustic wave (MSAW) sensor in a reflective delay line configuration with two acoustic propagation paths with and without magnetic field sensitive layer. The delay in path with sensitive layer leads to magnetic field detection and the one without enable temperature measurement and thus compensation for the first path. The developed sensor is based on a ZnO/LiNbO$_3$ Ycut (X-direction) layered structure as Love wave platform. Love wave as a shear wave being more favorable for magnetic detection. Co-Fe-B is considered as sensitive layer to detect magnetic field changes and is deposited on the top of ZnO, but only on one of the two paths. We combined an original configuration of connected IDTs with a high electromechanical coupling coefficient (K$^2$) mode to improve the signal amplitude. The achieved sensor exhibits a high temperature and magnetic field sensitivity of -63 ppm/$^\circ$C and -781 ppm/mT, respectively. The temperature compensation method for magnetic field measurement is demonstrated using a differential measurement by subtracting the delay times obtained for the two paths with and without the sensitive layer. Finally, The sensor exhibited good repeatability at various temperatures. Moreover, the device developed allows in addition to the multisensor functionality, the radio frequency identification (RFID) which is necessary for the deployment of sensor networks.
</details>
<details>
<summary>摘要</summary>
магнитное поле измерение, включая температурную компенсацию, является важным для магнитного поля сенсора. Это исследование изучает магнитный поверхностный акустический волны (MSAW) сенсор в конфигурации отрактивной задержки с двумя путями пропаганции с и без магнитнечувствительного слоя. Задержка на пути с чувствительным слоем приводит к измерению магнитного поля, а путь без него позволяет измерять температуру и thus компенсировать ее в первом пути. Разработанный сенсор основан на структуре ZnO/LiNbO$_3$ Ycut (по kierungi X) с платформой Love волны, которая более удобна для магнитного детектирования. Сенсор использует слой Co-Fe-B для детектирования изменений магнитного поля и нанесен на верхнюю часть ZnO, но только на одном из двух путей. Мы комбинировали оригинальную конфигурацию соединенных IDTs с высоким коэффициентом электромеханической взаимодействия (K^2) для улучшения амплитуды сигнала. Разработанный сенсор демонстрирует высокую температурную и магнитную чувствительность -63 ppm/$^\circ$C и -781 ppm/mT, соответственно. Метод температурной компенсации для измерения магнитного поля демонстрируется с помощью дифференциального измерения, subtracting the delay times obtained for the two paths with and without the sensitive layer. Кроме того, сенсор показал хорошую повторяемость при различных температурах. Кроме того, разработанный сенсор позволяет в дополнение к многосенсорной функции, радиоидентификацию (RFID), которая необходима для деплоирования сетей сенсоров.
</details></li>
</ul>
<hr>
<h2 id="Managing-the-Impact-of-Sensor’s-Thermal-Noise-in-Machine-Learning-for-Nuclear-Applications"><a href="#Managing-the-Impact-of-Sensor’s-Thermal-Noise-in-Machine-Learning-for-Nuclear-Applications" class="headerlink" title="Managing the Impact of Sensor’s Thermal Noise in Machine Learning for Nuclear Applications"></a>Managing the Impact of Sensor’s Thermal Noise in Machine Learning for Nuclear Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01014">http://arxiv.org/abs/2310.01014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Issam Hammad</li>
<li>for: 这个论文是为了探讨加拧度、磁力和自转仪在核电厂中进行测量时的噪声问题，以及这些噪声对于基于感知器的机器学习模型的影响。</li>
<li>methods: 这篇论文使用了机器学习技术，以analyze the impact of thermal noise on sensor-fusion-based machine learning models in nuclear applications.</li>
<li>results: 该论文发现，在生产环境中部署机器学习模型时，温度噪声会导致感知器的精度下降，从而影响模型的准确性。此外，论文还发现了不同机器学习算法对于温度噪声的影响不同，选择更加鲜硬的模型可以减轻这种影响。<details>
<summary>Abstract</summary>
Sensors such as accelerometers, magnetometers, and gyroscopes are frequently utilized to perform measurements in nuclear power plants. For example, accelerometers are used for vibration monitoring of critical systems. With the recent rise of machine learning, data captured from such sensors can be used to build machine learning models for predictive maintenance and automation. However, these sensors are known to have thermal noise that can affect the sensor's accuracy. Thermal noise differs between sensors in terms of signal-to-noise ratio (SNR). This thermal noise will cause an accuracy drop in sensor-fusion-based machine learning models when deployed in production. This paper lists some applications for Canada Deuterium Uranium (CANDU) reactors where such sensors are used and therefore can be impacted by the thermal noise issue if machine learning is utilized. A list of recommendations to help mitigate the issue when building future machine learning models for nuclear applications based on sensor fusion is provided. Additionally, this paper demonstrates that machine learning algorithms can be impacted differently by the issue, therefore selecting a more resilient model can help in mitigating it.
</details>
<details>
<summary>摘要</summary>
感知器如加速计、磁计和陀螺仪在核电厂中广泛应用于测量。例如，加速计用于关键系统的振荡监测。随着机器学习的兴起，从感知器获取的数据可以用于建立机器学习模型，以实现预测维护和自动化。然而，感知器受到热噪声的影响，可能导致感知器的精度下降。热噪声 между感知器不同，这会导致机器学习模型在生产环境中的精度下降。这篇论文介绍了加拿大氘气氘化燃料（CANDU）堆反应器中的应用，因此可能受到热噪声问题的影响。此外，文章还提供了一些建议来减轻这一问题，以及选择更鲁容的机器学习模型，以减轻其影响。此外，文章还示出了不同的机器学习算法受到热噪声问题的影响，因此选择更鲁容的机器学习模型可以减轻这一问题。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Secrecy-Capacity-in-PLS-Communication-with-NORAN-based-on-Pilot-Information-Codebooks"><a href="#Enhancing-Secrecy-Capacity-in-PLS-Communication-with-NORAN-based-on-Pilot-Information-Codebooks" class="headerlink" title="Enhancing Secrecy Capacity in PLS Communication with NORAN based on Pilot Information Codebooks"></a>Enhancing Secrecy Capacity in PLS Communication with NORAN based on Pilot Information Codebooks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01453">http://arxiv.org/abs/2310.01453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yebo Gu, Tao Shen, Jian Song, Qingbo Wang</li>
<li>for: 这篇论文是关于非正交人造障碍（NORAN）的研究，旨在提高通信系统的安全性和隐私性。</li>
<li>methods: 这篇论文提出了一种基于飞行信息编码表（PIC）的新的NORAN方案，该方案可以在不增加正式障碍（LC）的情况下提高安全性。</li>
<li>results: numerical simulations和分析表明，使用PIC基于NORAN方案可以显著提高通信系统的安全性和隐私性。<details>
<summary>Abstract</summary>
In recent research, non-orthogonal artificial noise (NORAN) has been proposed as an alternative to orthogonal artificial noise (AN). However, NORAN introduces additional noise into the channel, which reduces the capacity of the legitimate channel (LC). At the same time, selecting a NORAN design with ideal security performance from a large number of design options is also a challenging problem. To address these two issues, a novel NORAN based on a pilot information codebook is proposed in this letter. The codebook associates different suboptimal NORANs with pilot information as the key under different channel state information (CSI). The receiver interrogates the codebook using the pilot information to obtain the NORAN that the transmitter will transmit in the next moment, in order to eliminate the NORAN when receiving information. Therefore, NORAN based on pilot information codebooks can improve the secrecy capacity (SC) of the communication system by directly using suboptimal NORAN design schemes without increasing the noise in the LC. Numerical simulations and analyses show that the introduction of NORAN with a novel design using pilot information codebooks significantly enhances the security and improves the SC of the communication system.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Resource-efficient-FIR-Filter-Design-Based-on-an-RAG-Improved-Algorithm"><a href="#A-Resource-efficient-FIR-Filter-Design-Based-on-an-RAG-Improved-Algorithm" class="headerlink" title="A Resource-efficient FIR Filter Design Based on an RAG Improved Algorithm"></a>A Resource-efficient FIR Filter Design Based on an RAG Improved Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00912">http://arxiv.org/abs/2310.00912</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengwei Hu, Zhengxiong Li, Xianyang Jiang</li>
<li>for: 这个论文主要用于提出一种高效的数字滤波器芯片设计方法，以优化资源利用率。</li>
<li>methods: 该论文使用了一种改进的RAG算法，以减少 multiplication 函数的硬件资源消耗。</li>
<li>results: 对比各种算法和矩阵大小，实验结果显示，提出的算法在逻辑资源利用率、资源分配策略、运行速度和功耗消耗等方面具有优势。<details>
<summary>Abstract</summary>
In modern digital filter chip design, efficient resource utilization is a hot topic. Due to the linear phase characteristics of FIR filters, a pulsed fully parallel structure can be applied to address the problem. To further reduce hardware resource consumption, especially related to multiplication functions, an improved RAG algorithm has been proposed. Filters with different orders and for different algorithms have been compared, and the experimental results show that the improved RAG algorithm excels in terms of logic resource utilization, resource allocation, running speed, and power consumption under various application scenarios. The proposed algorithm introduces a better circuit structure for FIR filters, fully leveraging resource allocation strategies to reduce logic resource consumption. The proposed circuit is faster and more stable, making it suitable for a variety of complex application scenarios.
</details>
<details>
<summary>摘要</summary>
现代数字筛选器设计中，高效资源利用是一个热门话题。由于FIR筛选器的线性阶段特性，可以应用精心设计的全параллеLRagstruktur来解决这个问题。为了进一步减少硬件资源占用，特别是相multiplication功能，一种改进的RAG算法已经被提议。对于不同顺序和不同算法的筛选器进行了比较，实验结果表明，改进的RAG算法在逻辑资源利用、资源分配、运行速度和电力占用等多种应用场景中具有优异性。提出的Circuit结构可以充分利用资源分配策略，以减少逻辑资源占用。这种快速稳定的Circuit适用于许多复杂的应用场景。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/02/eess.SP_2023_10_02/" data-id="clp869u9801f3k588c7ye381r" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_10_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/01/cs.SD_2023_10_01/" class="article-date">
  <time datetime="2023-10-01T15:00:00.000Z" itemprop="datePublished">2023-10-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/01/cs.SD_2023_10_01/">cs.SD - 2023-10-01</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="UniAudio-An-Audio-Foundation-Model-Toward-Universal-Audio-Generation"><a href="#UniAudio-An-Audio-Foundation-Model-Toward-Universal-Audio-Generation" class="headerlink" title="UniAudio: An Audio Foundation Model Toward Universal Audio Generation"></a>UniAudio: An Audio Foundation Model Toward Universal Audio Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00704">http://arxiv.org/abs/2310.00704</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yangdongchao/UniAudio_demo">https://github.com/yangdongchao/UniAudio_demo</a></li>
<li>paper_authors: Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang, Songxiang Liu, Xuankai Chang, Jiatong Shi, Sheng Zhao, Jiang Bian, Xixin Wu, Zhou Zhao, Shinji Watanabe, Helen Meng</li>
<li>for: 这个论文的目标是开发一个可以处理多种生成任务的语言模型（LLM），并使其能够生成具有给定输入条件的多种音频类型（包括语音、声音、音乐和歌唱）。</li>
<li>methods: 这个论文使用了一种新的 Tokenization 技术，即 residual vector quantization based neural codec，来处理各种目标音频的tokenization。它还使用了一种多尺度 transformer 模型来处理长度过长的序列问题。</li>
<li>results: 论文在11个任务上实现了州际级或至少竞争性的成绩，并且发现UniAudio模型在所有训练任务中表现出了强大的能力。<details>
<summary>Abstract</summary>
Large Language models (LLM) have demonstrated the capability to handle a variety of generative tasks. This paper presents the UniAudio system, which, unlike prior task-specific approaches, leverages LLM techniques to generate multiple types of audio (including speech, sounds, music, and singing) with given input conditions. UniAudio 1) first tokenizes all types of target audio along with other condition modalities, 2) concatenates source-target pair as a single sequence, and 3) performs next-token prediction using LLM. Also, a multi-scale Transformer model is proposed to handle the overly long sequences caused by the residual vector quantization based neural codec in tokenization. Training of UniAudio is scaled up to 165K hours of audio and 1B parameters, based on all generative tasks, aiming to obtain sufficient prior knowledge not only in the intrinsic properties of audio but also the inter-relationship between audio and other modalities. Therefore, the trained UniAudio model has the potential to become a foundation model for universal audio generation: it shows strong capability in all trained tasks and can seamlessly support new audio generation tasks after simple fine-tuning. Experiments demonstrate that UniAudio achieves state-of-the-art or at least competitive results on most of the 11 tasks. Demo and code are released at https://github.com/yangdongchao/UniAudio
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经证明了处理多种生成任务的能力。这篇论文介绍了UniAudio系统，与前一些任务特定的方法不同，通过LLM技术来生成多种音频（包括语音、声音、音乐和歌唱），并且可以根据输入条件进行生成。UniAudio的实现方式包括以下三个步骤：1. 对所有类型的目标音频进行token化，并将其与其他条件模式一起 concatenate 成一个序列。2. 使用 LLM 进行下一个token预测。3. 使用多级 transformer 模型来处理由 residual vector quantization 基于的 neural codec 生成的过长序列。在训练UniAudio时，使用了165K小时的音频和1B参数，基于所有生成任务，以获得充足的先验知识不仅在音频的内在性能，还在音频和其他模式之间的关系。因此，训练UniAudio模型后，可以作为普适的音频生成基模型，它在所有训练任务中表现出了强大的能力，并且可以通过简单的微调来支持新的音频生成任务。实验结果表明，UniAudio在大多数11个任务中具有国际级或至少竞争力的成绩。示例和代码可以在https://github.com/yangdongchao/UniAudio 中下载。
</details></li>
</ul>
<hr>
<h2 id="Pianist-Identification-Using-Convolutional-Neural-Networks"><a href="#Pianist-Identification-Using-Convolutional-Neural-Networks" class="headerlink" title="Pianist Identification Using Convolutional Neural Networks"></a>Pianist Identification Using Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00699">http://arxiv.org/abs/2310.00699</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/betsytang/pid-cnn">https://github.com/betsytang/pid-cnn</a></li>
<li>paper_authors: Jingjing Tang, Geraint Wiggins, Gyorgy Fazekas</li>
<li>For: 本研究旨在用深度学习技术自动识别表演型钢琴演奏者，解决了建立智能音乐 инструмент和智能音乐系统的挑战。* Methods: 我们使用卷积神经网络和表达特征来实现自动识别，并对大规模的表演型钢琴演奏数据进行了深度学习技术的应用和改进。* Results: 我们的模型在6类识别任务中达到85.3%的准确率，比基eline模型高出了20.8%。我们的改进的数据集也提供了更好的训练数据，为自动演奏者识别做出了重要贡献。<details>
<summary>Abstract</summary>
This paper presents a comprehensive study of automatic performer identification in expressive piano performances using convolutional neural networks (CNNs) and expressive features. Our work addresses the challenging multi-class classification task of identifying virtuoso pianists, which has substantial implications for building dynamic musical instruments with intelligence and smart musical systems. Incorporating recent advancements, we leveraged large-scale expressive piano performance datasets and deep learning techniques. We refined the scores by expanding repetitions and ornaments for more accurate feature extraction. We demonstrated the capability of one-dimensional CNNs for identifying pianists based on expressive features and analyzed the impact of the input sequence lengths and different features. The proposed model outperforms the baseline, achieving 85.3% accuracy in a 6-way identification task. Our refined dataset proved more apt for training a robust pianist identifier, making a substantial contribution to the field of automatic performer identification. Our codes have been released at https://github.com/BetsyTang/PID-CNN.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/01/cs.SD_2023_10_01/" data-id="clp869u3e0106k5883s18fe28" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_10_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/01/eess.AS_2023_10_01/" class="article-date">
  <time datetime="2023-10-01T14:00:00.000Z" itemprop="datePublished">2023-10-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/01/eess.AS_2023_10_01/">eess.AS - 2023-10-01</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Mechatronic-Generation-of-Datasets-for-Acoustics-Research"><a href="#Mechatronic-Generation-of-Datasets-for-Acoustics-Research" class="headerlink" title="Mechatronic Generation of Datasets for Acoustics Research"></a>Mechatronic Generation of Datasets for Acoustics Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00587">http://arxiv.org/abs/2310.00587</a></li>
<li>repo_url: None</li>
<li>paper_authors: Austin Lu, Ethaniel Moore, Arya Nallanthighall, Kanad Sarkar, Manan Mittal, Ryan M. Corey, Paris Smaragdis, Andrew Singer</li>
<li>for: 这篇论文是为了描述一种机器人共享测试空间，用于实现自动化听音实验。</li>
<li>methods: 该系统使用无线多机器人协调技术，实现同步机器人运动，以适应动态场景中的移动发声器和收音器。用户可以通过虚拟控制界面来设计自动化实验，收集大规模的听音数据。</li>
<li>results: 实验结果表明，MARS系统可以生成高可靠性的听音数据，并且可以帮助研究人员无需特有听音研究空间来收集听音数据。<details>
<summary>Abstract</summary>
We address the challenge of making spatial audio datasets by proposing a shared mechanized recording space that can run custom acoustic experiments: a Mechatronic Acoustic Research System (MARS). To accommodate a wide variety of experiments, we implement an extensible architecture for wireless multi-robot coordination which enables synchronized robot motion for dynamic scenes with moving speakers and microphones. Using a virtual control interface, we can remotely design automated experiments to collect large-scale audio data. This data is shown to be similar across repeated runs, demonstrating the reliability of MARS. We discuss the potential for MARS to make audio data collection accessible for researchers without dedicated acoustic research spaces.
</details>
<details>
<summary>摘要</summary>
我们面临的挑战是创建空间听音数据集，我们提议一种共享机械化录音空间，可以进行自定义听音实验：一个名为 MARS 的机械听音研究系统。为了满足广泛的实验需求，我们实施了可扩展的无线多机器人协调架构，可以实现同步的机器人运动，以便在动态场景中进行移动speaker和 microphone的记录。通过虚拟控制界面，我们可以远程设计自动化实验，收集大规模的听音数据。这些数据显示与重复运行中的相似性，证明 MARS 的可靠性。我们讨论了 MARS 的潜在可能性，使听音数据采集变得对研究人员而言可 accessible。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/01/eess.AS_2023_10_01/" data-id="clp869u4y014jk5889y4n7ssd" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_10_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/01/cs.CV_2023_10_01/" class="article-date">
  <time datetime="2023-10-01T13:00:00.000Z" itemprop="datePublished">2023-10-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/01/cs.CV_2023_10_01/">cs.CV - 2023-10-01</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Sharingan-A-Transformer-based-Architecture-for-Gaze-Following"><a href="#Sharingan-A-Transformer-based-Architecture-for-Gaze-Following" class="headerlink" title="Sharingan: A Transformer-based Architecture for Gaze Following"></a>Sharingan: A Transformer-based Architecture for Gaze Following</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00816">http://arxiv.org/abs/2310.00816</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samy Tafasca, Anshul Gupta, Jean-Marc Odobez</li>
<li>for: 这 paper 是为了研究人类视线跟踪的模型，以便在各种应用领域中使用。</li>
<li>methods: 这 paper 使用了一种新的 transformer-based 架构来实现 2D 视线预测。</li>
<li>results: 这 paper 在 GazeFollow 和 VideoAttentionTarget 数据集上 achieved state-of-the-art 结果。Here’s the full translation in Simplified Chinese:</li>
<li>for: 这 paper 是为了研究人类视线跟踪的模型，以便在各种应用领域中使用。</li>
<li>methods: 这 paper 使用了一种新的 transformer-based 架构来实现 2D 视线预测。</li>
<li>results: 这 paper 在 GazeFollow 和 VideoAttentionTarget 数据集上 achieved state-of-the-art 结果。I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
Gaze is a powerful form of non-verbal communication and social interaction that humans develop from an early age. As such, modeling this behavior is an important task that can benefit a broad set of application domains ranging from robotics to sociology. In particular, Gaze Following is defined as the prediction of the pixel-wise 2D location where a person in the image is looking. Prior efforts in this direction have focused primarily on CNN-based architectures to perform the task. In this paper, we introduce a novel transformer-based architecture for 2D gaze prediction. We experiment with 2 variants: the first one retains the same task formulation of predicting a gaze heatmap for one person at a time, while the second one casts the problem as a 2D point regression and allows us to perform multi-person gaze prediction with a single forward pass. This new architecture achieves state-of-the-art results on the GazeFollow and VideoAttentionTarget datasets. The code for this paper will be made publicly available.
</details>
<details>
<summary>摘要</summary>
gaze 是一种强大的非语言通信和社交互动方式，人类从 early age 开始发展。因此，模拟这种行为是一项重要的任务，可以 benefiting  Broad 应用领域，从机器人学到社会学。特别是，瞥向预测（Gaze Following）定义为图像中人员的 pixel-wise 2D 位置预测。先前的尝试都是通过 CNN  arquitectures 来完成这项任务。在这篇论文中，我们提出了一种新的 transformer 结构来实现 2D 瞥向预测。我们实验了两个变体：第一个保持了同样的任务表述，即预测一个人的瞥向热图 ; 第二个将问题定义为2D 点 regression，允许我们通过单一的前进 pass 进行多人瞥向预测。这种新的结构实现了 GazeFollow 和 VideoAttentionTarget 数据集的状态图。代码将公开发布。
</details></li>
</ul>
<hr>
<h2 id="Completing-Visual-Objects-via-Bridging-Generation-and-Segmentation"><a href="#Completing-Visual-Objects-via-Bridging-Generation-and-Segmentation" class="headerlink" title="Completing Visual Objects via Bridging Generation and Segmentation"></a>Completing Visual Objects via Bridging Generation and Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00808">http://arxiv.org/abs/2310.00808</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Li, Yinpeng Chen, Chung-Ching Lin, Rita Singh, Bhiksha Raj, Zicheng Liu</li>
<li>for:  reconstruction of a complete object from its partially visible components</li>
<li>methods: iterative stages of generation and segmentation, with the object mask provided as an additional condition</li>
<li>results: superior object completion results compared to existing approaches such as ControlNet and Stable Diffusion<details>
<summary>Abstract</summary>
This paper presents a novel approach to object completion, with the primary goal of reconstructing a complete object from its partially visible components. Our method, named MaskComp, delineates the completion process through iterative stages of generation and segmentation. In each iteration, the object mask is provided as an additional condition to boost image generation, and, in return, the generated images can lead to a more accurate mask by fusing the segmentation of images. We demonstrate that the combination of one generation and one segmentation stage effectively functions as a mask denoiser. Through alternation between the generation and segmentation stages, the partial object mask is progressively refined, providing precise shape guidance and yielding superior object completion results. Our experiments demonstrate the superiority of MaskComp over existing approaches, e.g., ControlNet and Stable Diffusion, establishing it as an effective solution for object completion.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种新的物体完成方法，主要目标是从部分可见的组件中重建完整的物体。我们的方法，名为MaskComp，通过 iterate 的生成和分割阶段来进行分割。在每个迭代阶段，提供对象Mask作为附加条件，以提高图像生成，并在返回的图像中提取更加精确的Mask。我们发现，通过生成和分割阶段的交互，可以有效地减少Mask的噪声。通过 alternate 生成和分割阶段，部分物体Mask可以逐渐进行精细化，提供精确的形状指导，并且实现了更好的物体完成效果。我们的实验表明，MaskComp 比 existed 方法（如 ControlNet 和 Stable Diffusion）更加有效， establishing 它为物体完成的有效解决方案。
</details></li>
</ul>
<hr>
<h2 id="Propagating-Semantic-Labels-in-Video-Data"><a href="#Propagating-Semantic-Labels-in-Video-Data" class="headerlink" title="Propagating Semantic Labels in Video Data"></a>Propagating Semantic Labels in Video Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00783">http://arxiv.org/abs/2310.00783</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Balaban, Justin Medich, Pranay Gosar, Justin Hart</li>
<li>for: 这个论文的目的是提出一种基于Foundation Models的视频 segmentation方法，以减少人工标注成本。</li>
<li>methods: 该方法使用Segment Anything Model (SAM)和Structure from Motion (SfM)两种技术来实现视频 segmentation。首先，视频输入被重构为3D几何结构使用SfM，然后使用SAM进行每帧的分割。最后，对于每帧的分割结果，进行3D几何投影，以便在新的视角下进行跟踪。</li>
<li>results: 该方法可以大幅减少人工标注成本，但是与人工标注的性能相比，其性能有所下降。三个主要纪录器都用于评估系统性能：计算时间、面积 overlap with manual labels和跟踪损失数量。结果表明，该系统在跟踪对象在视频帧上的计算时间方面具有显著的提高，但是在性能方面却存在一定的下降。<details>
<summary>Abstract</summary>
Semantic Segmentation combines two sub-tasks: the identification of pixel-level image masks and the application of semantic labels to those masks. Recently, so-called Foundation Models have been introduced; general models trained on very large datasets which can be specialized and applied to more specific tasks. One such model, the Segment Anything Model (SAM), performs image segmentation. Semantic segmentation systems such as CLIPSeg and MaskRCNN are trained on datasets of paired segments and semantic labels. Manual labeling of custom data, however, is time-consuming. This work presents a method for performing segmentation for objects in video. Once an object has been found in a frame of video, the segment can then be propagated to future frames; thus reducing manual annotation effort. The method works by combining SAM with Structure from Motion (SfM). The video input to the system is first reconstructed into 3D geometry using SfM. A frame of video is then segmented using SAM. Segments identified by SAM are then projected onto the the reconstructed 3D geometry. In subsequent video frames, the labeled 3D geometry is reprojected into the new perspective, allowing SAM to be invoked fewer times. System performance is evaluated, including the contributions of the SAM and SfM components. Performance is evaluated over three main metrics: computation time, mask IOU with manual labels, and the number of tracking losses. Results demonstrate that the system has substantial computation time improvements over human performance for tracking objects over video frames, but suffers in performance.
</details>
<details>
<summary>摘要</summary>
Semantic Segmentation 将两个子任务结合在一起：Pixel-level图像mask的标识和图像mask的semantic标签应用。最近，称之为基础模型的模型被引入，这些模型可以在很大的数据集上训练，然后应用到更特定的任务上。一个such model是Segment Anything Model（SAM），它实现了图像 segmentation。图像 segmentation系统such as CLIPSeg和MaskRCNN通常是在paired segments和semantic labels的数据集上训练的。然而，手动标注自定义数据是时间consuming。这个工作提出了一种方法，通过结合SAM和Structure from Motion（SfM）来实现对视频帧中对象的分割。首先，视频输入被重建为3D几何结构使用SfM。然后，在SAM中Segment一帧视频。由SAM标识的分割被 проекted onto the reconstructed 3D几何结构。在后续的视频帧中，标注的3D几何结构被重新投影到新的视角，以便在新的视频帧中invoked SAM fewer times。系统性能被评估，包括SAM和SfM组件的贡献。性能被评估以三个主要指标：计算时间、mask IOU with manual labels和跟踪损失数。结果表明，系统在跟踪对象在视频帧之间的计算时间上有substantial的提高，但是性能不如人工标注。
</details></li>
</ul>
<hr>
<h2 id="SMOOT-Saliency-Guided-Mask-Optimized-Online-Training"><a href="#SMOOT-Saliency-Guided-Mask-Optimized-Online-Training" class="headerlink" title="SMOOT: Saliency Guided Mask Optimized Online Training"></a>SMOOT: Saliency Guided Mask Optimized Online Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00772">http://arxiv.org/abs/2310.00772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Karkehabadi, Houman Homayoun, Avesta Sasan</li>
<li>for: 这种论文的目的是提出一种新的隐藏导航法（Saliency-Guided Training，SGT），以提高深度神经网络的解释性。</li>
<li>methods: 这种方法使用反射和修改Gradient来引导模型强调最重要的特征，以提高模型的解释性。</li>
<li>results: 实验结果表明，我们的提案可以有效地提高模型的准确率和隐藏特征的明确度。<details>
<summary>Abstract</summary>
Deep Neural Networks are powerful tools for understanding complex patterns and making decisions. However, their black-box nature impedes a complete understanding of their inner workings. Saliency-Guided Training (SGT) methods try to highlight the prominent features in the model's training based on the output to alleviate this problem. These methods use back-propagation and modified gradients to guide the model toward the most relevant features while keeping the impact on the prediction accuracy negligible. SGT makes the model's final result more interpretable by masking input partially. In this way, considering the model's output, we can infer how each segment of the input affects the output. In the particular case of image as the input, masking is applied to the input pixels. However, the masking strategy and number of pixels which we mask, are considered as a hyperparameter. Appropriate setting of masking strategy can directly affect the model's training. In this paper, we focus on this issue and present our contribution. We propose a novel method to determine the optimal number of masked images based on input, accuracy, and model loss during the training. The strategy prevents information loss which leads to better accuracy values. Also, by integrating the model's performance in the strategy formula, we show that our model represents the salient features more meaningful. Our experimental results demonstrate a substantial improvement in both model accuracy and the prominence of saliency, thereby affirming the effectiveness of our proposed solution.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Counterfactual-Image-Generation-for-adversarially-robust-and-interpretable-Classifiers"><a href="#Counterfactual-Image-Generation-for-adversarially-robust-and-interpretable-Classifiers" class="headerlink" title="Counterfactual Image Generation for adversarially robust and interpretable Classifiers"></a>Counterfactual Image Generation for adversarially robust and interpretable Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00761">http://arxiv.org/abs/2310.00761</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rafael Bischof, Florian Scheidegger, Michael A. Kraus, A. Cristiano I. Malossi</li>
<li>for: 这种方法的目的是提高神经网络图像分类器的解释性和robustness。</li>
<li>methods: 该方法使用图像到图像翻译生成器（GANs）来生成对应的替换样本，以提高解释性和对抗性。</li>
<li>results: 该方法可以生成高度描述性的解释图像，并且可以提高模型对抗性。此外，该方法还可以用来评估模型的不确定性。<details>
<summary>Abstract</summary>
Neural Image Classifiers are effective but inherently hard to interpret and susceptible to adversarial attacks. Solutions to both problems exist, among others, in the form of counterfactual examples generation to enhance explainability or adversarially augment training datasets for improved robustness. However, existing methods exclusively address only one of the issues. We propose a unified framework leveraging image-to-image translation Generative Adversarial Networks (GANs) to produce counterfactual samples that highlight salient regions for interpretability and act as adversarial samples to augment the dataset for more robustness. This is achieved by combining the classifier and discriminator into a single model that attributes real images to their respective classes and flags generated images as "fake". We assess the method's effectiveness by evaluating (i) the produced explainability masks on a semantic segmentation task for concrete cracks and (ii) the model's resilience against the Projected Gradient Descent (PGD) attack on a fruit defects detection problem. Our produced saliency maps are highly descriptive, achieving competitive IoU values compared to classical segmentation models despite being trained exclusively on classification labels. Furthermore, the model exhibits improved robustness to adversarial attacks, and we show how the discriminator's "fakeness" value serves as an uncertainty measure of the predictions.
</details>
<details>
<summary>摘要</summary>
Our framework combines the classifier and discriminator into a single model, which attributes real images to their respective classes and flags generated images as "fake". We evaluate the effectiveness of our method by assessing the produced explainability masks on a semantic segmentation task for concrete cracks and the model's resilience against the Projected Gradient Descent (PGD) attack on a fruit defects detection problem.Our produced saliency maps are highly descriptive and achieve competitive IoU values compared to classical segmentation models, despite being trained exclusively on classification labels. Additionally, the model exhibits improved robustness to adversarial attacks, and we show how the discriminator's "fakeness" value serves as an uncertainty measure of the predictions.
</details></li>
</ul>
<hr>
<h2 id="Top-down-Green-ups-Satellite-Sensing-and-Deep-Models-to-Predict-Buffelgrass-Phenology"><a href="#Top-down-Green-ups-Satellite-Sensing-and-Deep-Models-to-Predict-Buffelgrass-Phenology" class="headerlink" title="Top-down Green-ups: Satellite Sensing and Deep Models to Predict Buffelgrass Phenology"></a>Top-down Green-ups: Satellite Sensing and Deep Models to Predict Buffelgrass Phenology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00740">http://arxiv.org/abs/2310.00740</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lurosenb/phenology_projects">https://github.com/lurosenb/phenology_projects</a></li>
<li>paper_authors: Lucas Rosenblatt, Bin Han, Erin Posthumus, Theresa Crimmins, Bill Howe</li>
<li>For: 预测buffelgrass的”绿化”（即Ready for herbicidal treatment），以预防南部美国的严重野火和生物多样性损失。* Methods: 使用卫星感知和深度学习模型，包括时间、视觉和多模态模型，以提高预测 buffelgrass 绿化的精度。* Results: 所有神经网络基于的方法都超越了传统 buffelgrass 绿化模型，并讨论了如何实现神经网络模型的部署，以实现 significiant resource savings。<details>
<summary>Abstract</summary>
An invasive species of grass known as "buffelgrass" contributes to severe wildfires and biodiversity loss in the Southwest United States. We tackle the problem of predicting buffelgrass "green-ups" (i.e. readiness for herbicidal treatment). To make our predictions, we explore temporal, visual and multi-modal models that combine satellite sensing and deep learning. We find that all of our neural-based approaches improve over conventional buffelgrass green-up models, and discuss how neural model deployment promises significant resource savings.
</details>
<details>
<summary>摘要</summary>
“一种入侵性的草本植物──牛肚草”在南部美国引起了严重的野火和生物多样性损失。我们面临着预测牛肚草“绿化”（即Ready for 药物处理）的问题。为了实现这一目标，我们探讨了时间、视觉和多模态模型， combining satellite sensing和深度学习。我们发现所有的神经网络方法都超过了传统的牛肚草绿化模型，并讨论了如何部署神经网络模型以实现显著的资源节约。”Note that "牛肚草" (bù dù cǎo) is the Simplified Chinese term for "buffelgrass".
</details></li>
</ul>
<hr>
<h2 id="HOH-Markerless-Multimodal-Human-Object-Human-Handover-Dataset-with-Large-Object-Count"><a href="#HOH-Markerless-Multimodal-Human-Object-Human-Handover-Dataset-with-Large-Object-Count" class="headerlink" title="HOH: Markerless Multimodal Human-Object-Human Handover Dataset with Large Object Count"></a>HOH: Markerless Multimodal Human-Object-Human Handover Dataset with Large Object Count</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00723">http://arxiv.org/abs/2310.00723</a></li>
<li>repo_url: None</li>
<li>paper_authors: Noah Wiederhold, Ava Megyeri, DiMaggio Paris, Sean Banerjee, Natasha Kholgade Banerjee</li>
<li>for: 论文主要用于促进数据驱动的手夹研究、人机手夹实现以及人工智能手夹参数估计的数据集。</li>
<li>methods: 本论文使用了多视图RGB和深度数据、skeleton、笔oint clouds、抓取类型和手夹性labels、物体、接受手和发送手2D和3D分割、接受手和发送手舒适评分、对象元数据和对应的3D模型等数据来描述136种物品的人类互动。</li>
<li>results: 本论文通过使用HOH数据集进行神经网络训练，实现了抓取、orientation和轨迹预测等任务。相比标注数据集，HOH数据集不需要特定的装备，可以更自然地捕捉人类之间的手夹互动，并且包含了人类手夹互动的高分辨率手夹跟踪数据。至今为止，HOH数据集是手夹数据集中最大的物品数、参与者数、对应的对话对数和总交互记录的数据集。<details>
<summary>Abstract</summary>
We present the HOH (Human-Object-Human) Handover Dataset, a large object count dataset with 136 objects, to accelerate data-driven research on handover studies, human-robot handover implementation, and artificial intelligence (AI) on handover parameter estimation from 2D and 3D data of person interactions. HOH contains multi-view RGB and depth data, skeletons, fused point clouds, grasp type and handedness labels, object, giver hand, and receiver hand 2D and 3D segmentations, giver and receiver comfort ratings, and paired object metadata and aligned 3D models for 2,720 handover interactions spanning 136 objects and 20 giver-receiver pairs-40 with role-reversal-organized from 40 participants. We also show experimental results of neural networks trained using HOH to perform grasp, orientation, and trajectory prediction. As the only fully markerless handover capture dataset, HOH represents natural human-human handover interactions, overcoming challenges with markered datasets that require specific suiting for body tracking, and lack high-resolution hand tracking. To date, HOH is the largest handover dataset in number of objects, participants, pairs with role reversal accounted for, and total interactions captured.
</details>
<details>
<summary>摘要</summary>
我们提出了人机物交换数据集（HOH），包含136种物品，以加速基于数据驱动的手柄研究、人机手柄实现和人工智能（AI）在手柄参数估计方面。HOH包含多视角RGB和深度数据、skeleton、粘合点云、抓取类型和手征标注、物品、付给人手和接受人手2D和3D分割、付给人和接受人的舒适评分、对应的对象元数据和对应的3D模型。我们还显示了使用HOH训练神经网络进行抓取、方向和轨迹预测的实验结果。作为唯一的无标记手柄捕捉数据集，HOH表现了自然的人人手柄交换互动，超越了标记数据集需要特定的服装以满足身体跟踪的问题，以及缺乏高分辨率手征跟踪。到目前为止，HOH是手柄数据集中最大的物品数、参与者数、对话对数和总交换次数。
</details></li>
</ul>
<hr>
<h2 id="Logical-Bias-Learning-for-Object-Relation-Prediction"><a href="#Logical-Bias-Learning-for-Object-Relation-Prediction" class="headerlink" title="Logical Bias Learning for Object Relation Prediction"></a>Logical Bias Learning for Object Relation Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00712">http://arxiv.org/abs/2310.00712</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyu Zhou, Zihan Ji, Anna Zhu</li>
<li>for: 提高Scene Graph生成（SGG）的精度和可靠性，以提高图像理解和下游任务的能力。</li>
<li>methods: 基于 causal inference 的对象关系预测策略，并提出一个对象提升模块来进行缺省研究。</li>
<li>results: 在Visual Gnome 150（VG-150）dataset上实验证明了我们提议的方法的有效性。<details>
<summary>Abstract</summary>
Scene graph generation (SGG) aims to automatically map an image into a semantic structural graph for better scene understanding. It has attracted significant attention for its ability to provide object and relation information, enabling graph reasoning for downstream tasks. However, it faces severe limitations in practice due to the biased data and training method. In this paper, we present a more rational and effective strategy based on causal inference for object relation prediction. To further evaluate the superiority of our strategy, we propose an object enhancement module to conduct ablation studies. Experimental results on the Visual Gnome 150 (VG-150) dataset demonstrate the effectiveness of our proposed method. These contributions can provide great potential for foundation models for decision-making.
</details>
<details>
<summary>摘要</summary>
Scene graph generation (SGG) 目标是自动将图像映射到 semantic 结构图，以提高场景理解。它吸引了大量注意力，因为它可以提供对象和关系信息，使得图reasoning 可能。然而，在实践中，它面临严重的限制，主要是因为数据和训练方法偏向。在这篇论文中，我们提出了基于 causal inference 的更合理和有效的策略，用于对象关系预测。为了进一步证明我们的策略的超越性，我们提出了对象增强模块进行缺失研究。实验结果表明，我们提posed 方法在 Visual Gnome 150 (VG-150) 数据集上得到了较好的效果。这些贡献可以为基础模型提供巨大的潜力。
</details></li>
</ul>
<hr>
<h2 id="You-Do-Not-Need-Additional-Priors-in-Camouflage-Object-Detection"><a href="#You-Do-Not-Need-Additional-Priors-in-Camouflage-Object-Detection" class="headerlink" title="You Do Not Need Additional Priors in Camouflage Object Detection"></a>You Do Not Need Additional Priors in Camouflage Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00702">http://arxiv.org/abs/2310.00702</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuchen Dong, Heng Zhou, Chengyang Li, Junjie Xie, Yongqiang Xie, Zhongbo Li</li>
<li>for: 本研究旨在开发一种不需要额外知识的掩蔽物检测网络，以解决现有方法强调额外知识的问题。</li>
<li>methods: 我们提出了一种新的自适应特征综合方法，通过多层特征信息的组合生成导航信息，不同于之前的方法，我们直接从图像特征中提取信息来导航模型训练。</li>
<li>results: 我们通过广泛的实验结果表明，我们的提议方法可以与现有的方法相比或超越其性能。<details>
<summary>Abstract</summary>
Camouflage object detection (COD) poses a significant challenge due to the high resemblance between camouflaged objects and their surroundings. Although current deep learning methods have made significant progress in detecting camouflaged objects, many of them heavily rely on additional prior information. However, acquiring such additional prior information is both expensive and impractical in real-world scenarios. Therefore, there is a need to develop a network for camouflage object detection that does not depend on additional priors. In this paper, we propose a novel adaptive feature aggregation method that effectively combines multi-layer feature information to generate guidance information. In contrast to previous approaches that rely on edge or ranking priors, our method directly leverages information extracted from image features to guide model training. Through extensive experimental results, we demonstrate that our proposed method achieves comparable or superior performance when compared to state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
高度掩蔽物检测（COD）具有 significannot challenges，因为掩蔽物和周围环境的高度相似性。当前深度学习方法已经在检测掩蔽物方面做出了 significannot进步，但大多数其中依赖于额外的先验信息。然而，在实际场景中获取这种额外先验信息是both expensive和不实际的。因此，有必要开发一种不依赖于额外先验信息的掩蔽物检测网络。在这篇论文中，我们提出了一种新的 adaptive feature aggregation 方法，可以有效地将多层特征信息集成成导航信息。与先前的方法相比，我们的方法直接利用图像特征中提取的信息来导航模型训练。通过广泛的实验结果，我们证明了我们提出的方法可以与当前状态的方法相比或更高的性能。
</details></li>
</ul>
<hr>
<h2 id="A-quantum-moving-target-segmentation-algorithm-for-grayscale-video"><a href="#A-quantum-moving-target-segmentation-algorithm-for-grayscale-video" class="headerlink" title="A quantum moving target segmentation algorithm for grayscale video"></a>A quantum moving target segmentation algorithm for grayscale video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03038">http://arxiv.org/abs/2310.03038</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjie Liu, Lu Wang, Qingshan Wu</li>
<li>for: 用于实时分割视频中移动目标。</li>
<li>methods: 使用量子机制同时计算所有邻帧图像差异，然后快速分割移动目标。设计了可行的量子比较器，用于判断灰度值与阈值的差异。</li>
<li>results: 对 IBM Q 进行实验，确认了我们的算法在不纯量子时代（NISQ）中的可行性。对于一个量子视频包含 $2^m$ 帧 ($每帧是 $2^n\times 2^n$ 图像，每个像素有 $q$ 灰度水平），我们的算法的复杂度可以降至 O $(n^2 + q) $。与 классический对比，它具有对数快速速度增长，同时也高于现有的量子算法。<details>
<summary>Abstract</summary>
The moving target segmentation (MTS) aims to segment out moving targets in the video, however, the classical algorithm faces the huge challenge of real-time processing in the current video era. Some scholars have successfully demonstrated the quantum advantages in some video processing tasks, but not concerning moving target segmentation. In this paper, a quantum moving target segmentation algorithm for grayscale video is proposed, which can use quantum mechanism to simultaneously calculate the difference of all pixels in all adjacent frames and then quickly segment out the moving target. In addition, a feasible quantum comparator is designed to distinguish the grayscale values with the threshold. Then several quantum circuit units, including three-frame difference, binarization and AND operation, are designed in detail, and then are combined together to construct the complete quantum circuits for segmenting the moving target. For a quantum video with $2^m$ frames (every frame is a $2^n\times 2^n$ image with $q$ grayscale levels), the complexity of our algorithm can be reduced to O$(n^2 + q)$. Compared with the classic counterpart, it is an exponential speedup, while its complexity is also superior to the existing quantum algorithms. Finally, the experiment is conducted on IBM Q to show the feasibility of our algorithm in the noisy intermediate-scale quantum (NISQ) era.
</details>
<details>
<summary>摘要</summary>
traditional Chinese version:运动目标分割（MTS）的目标是将影像中的运动目标分割出来，但 класиical algorithm在现今的影像时代中面临巨大的实时处理挑战。一些学者已经成功地显示了量子优势在一些影像处理任务中，但不包括运动目标分割。本文提出了一个量子运动目标分割算法 для灰度影像，可以使用量子机制同时计算所有帧的差值，快速地分割出运动目标。此外，一个可行的量子比较器也被设计出来，用于区分灰度值与阈值。然后，一些量子Circuit单元，包括三帧差值、binarization 和 AND 操作，在细节中被设计出来，然后被组合起来建立完整的量子Circuits для分割运动目标。对于一个具有 $2^m$ 帧影像（每帧是 $2^n\times 2^n$ 图像，每个像素有 $q$ 灰度水平）的量子影像，我们的算法的复杂度可以降至 O $(n^2 + q)$。相比 классиical counterpart，这是一个指数快速的优化，而且其复杂度也高于现有的量子算法。最后，我们在 IBM Q 上进行实验，以显示我们的算法在不确定中等量子（NISQ）时代的可行性。Here's the translation in Simplified Chinese:运动目标分割（MTS）的目标是将影像中的运动目标分割出来，但 classical algorithm在现今的影像时代中面临巨大的实时处理挑战。一些学者已经成功地显示了量子优势在一些影像处理任务中，但不包括运动目标分割。本文提出了一个量子运动目标分割算法 для灰度影像，可以使用量子机制同时计算所有帧的差值，快速地分割出运动目标。此外，一个可行的量子比较器也被设计出来，用于区分灰度值与阈值。然后，一些量子Circuit单元，包括三帧差值、binarization 和 AND 操作，在细节中被设计出来，然后被组合起来建立完整的量子Circuits для分割运动目标。对于一个具有 $2^m$ 帧影像（每帧是 $2^n\times 2^n$ 图像，每个像素有 $q$ 灰度水平）的量子影像，我们的算法的复杂度可以降至 O $(n^2 + q)$。相比 classical counterpart，这是一个指数快速的优化，而且其复杂度也高于现有的量子算法。最后，我们在 IBM Q 上进行实验，以显示我们的算法在不确定中等量子（NISQ）时代的可行性。
</details></li>
</ul>
<hr>
<h2 id="Comics-for-Everyone-Generating-Accessible-Text-Descriptions-for-Comic-Strips"><a href="#Comics-for-Everyone-Generating-Accessible-Text-Descriptions-for-Comic-Strips" class="headerlink" title="Comics for Everyone: Generating Accessible Text Descriptions for Comic Strips"></a>Comics for Everyone: Generating Accessible Text Descriptions for Comic Strips</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00698">http://arxiv.org/abs/2310.00698</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reshma Ramaprasad</li>
<li>for: 为了让漫画可以对视障群体开放，提供可读的自然语言描述。</li>
<li>methods: 使用计算机视觉技术提取漫画图片中的信息，包括panel、角色和文本信息，然后使用这些信息作为多Modal大语言模型的提示，生成描述。</li>
<li>results: 对一组已经得到人工注释的漫画进行测试，测试结果具有较好的量化和质量指标。<details>
<summary>Abstract</summary>
Comic strips are a popular and expressive form of visual storytelling that can convey humor, emotion, and information. However, they are inaccessible to the BLV (Blind or Low Vision) community, who cannot perceive the images, layouts, and text of comics. Our goal in this paper is to create natural language descriptions of comic strips that are accessible to the visually impaired community. Our method consists of two steps: first, we use computer vision techniques to extract information about the panels, characters, and text of the comic images; second, we use this information as additional context to prompt a multimodal large language model (MLLM) to produce the descriptions. We test our method on a collection of comics that have been annotated by human experts and measure its performance using both quantitative and qualitative metrics. The outcomes of our experiments are encouraging and promising.
</details>
<details>
<summary>摘要</summary>
漫画是一种受欢迎且表达力强的视觉故事形式，可以传达幽默、情感和信息。然而，它们对视障（Blind or Low Vision）社区不可见，无法感受到漫画的图片、布局和文本。我们的目标是创建可访问的漫画描述，以便让视障社区可以享受漫画的乐趣。我们的方法包括两步：第一步，我们使用计算机视觉技术提取漫画图片中的信息，包括画格、人物和文本信息；第二步，我们使用这些信息作为多模态大语言模型（MLLM）的提示，以生成描述。我们对一个收录了人工 эксперTS的漫画集进行测试，并使用量化和质量指标评估我们的方法的性能。实验结果很Encouraging和Promising。
</details></li>
</ul>
<hr>
<h2 id="A-Hierarchical-Graph-based-Approach-for-Recognition-and-Description-Generation-of-Bimanual-Actions-in-Videos"><a href="#A-Hierarchical-Graph-based-Approach-for-Recognition-and-Description-Generation-of-Bimanual-Actions-in-Videos" class="headerlink" title="A Hierarchical Graph-based Approach for Recognition and Description Generation of Bimanual Actions in Videos"></a>A Hierarchical Graph-based Approach for Recognition and Description Generation of Bimanual Actions in Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00670">http://arxiv.org/abs/2310.00670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fatemeh Ziaeetabar, Reza Safabakhsh, Saeedeh Momtazi, Minija Tamosiunaite, Florentin Wörgötter</li>
<li>for: 这项研究旨在提高视频中人体动作的描述精度和全面性，以满足机器人学、人机交互和视频分析等领域的需求。</li>
<li>methods: 该研究提出了一种新的方法，结合图形模型和层次嵌入式注意机制，以提高视频描述的精度和全面性。该方法首先编码视频中对象和动作之间的空间时间相互关系，然后使用三级建构的层次注意机制，以recognize本地和全局上下文元素。</li>
<li>results: 对多个2D和3D数据集进行了实验，并与状态对比，该方法 consistently 获得了更高的准确率、精度和上下文相关性。在大量的减少实验中，我们也评估了不同组件的作用。该方法可以生成不同 semantic 深度的描述，类似于不同人的描述。此外，更深入的二手手Object交互的理解可能会降低人工智能领域中的机器人模拟动作的准确性。<details>
<summary>Abstract</summary>
Nuanced understanding and the generation of detailed descriptive content for (bimanual) manipulation actions in videos is important for disciplines such as robotics, human-computer interaction, and video content analysis. This study describes a novel method, integrating graph based modeling with layered hierarchical attention mechanisms, resulting in higher precision and better comprehensiveness of video descriptions. To achieve this, we encode, first, the spatio-temporal inter dependencies between objects and actions with scene graphs and we combine this, in a second step, with a novel 3-level architecture creating a hierarchical attention mechanism using Graph Attention Networks (GATs). The 3-level GAT architecture allows recognizing local, but also global contextual elements. This way several descriptions with different semantic complexity can be generated in parallel for the same video clip, enhancing the discriminative accuracy of action recognition and action description. The performance of our approach is empirically tested using several 2D and 3D datasets. By comparing our method to the state of the art we consistently obtain better performance concerning accuracy, precision, and contextual relevance when evaluating action recognition as well as description generation. In a large set of ablation experiments we also assess the role of the different components of our model. With our multi-level approach the system obtains different semantic description depths, often observed in descriptions made by different people, too. Furthermore, better insight into bimanual hand-object interactions as achieved by our model may portend advancements in the field of robotics, enabling the emulation of intricate human actions with heightened precision.
</details>
<details>
<summary>摘要</summary>
importance of nuanced understanding and detailed descriptive content for (bimanual) manipulation actions in videos is crucial for fields such as robotics, human-computer interaction, and video content analysis. This study introduces a novel method that combines graph-based modeling with layered hierarchical attention mechanisms, resulting in more precise and comprehensive video descriptions. To achieve this, we first encode the spatio-temporal interdependencies between objects and actions using scene graphs, and then combine this with a novel 3-level architecture that creates a hierarchical attention mechanism using Graph Attention Networks (GATs). The 3-level GAT architecture allows for the recognition of both local and global contextual elements, enabling the generation of multiple descriptions with different semantic complexity for the same video clip. This approach improves the discriminative accuracy of action recognition and description generation. Our method is empirically tested on several 2D and 3D datasets, and we consistently obtain better performance compared to the state of the art in terms of accuracy, precision, and contextual relevance. In a series of ablation experiments, we also assess the role of the different components of our model. Our multi-level approach enables the system to obtain different semantic description depths, often observed in descriptions made by different people, and may also contribute to advancements in the field of robotics by enabling the emulation of intricate human actions with heightened precision.
</details></li>
</ul>
<hr>
<h2 id="Liveness-Detection-Competition-–-Noncontact-based-Fingerprint-Algorithms-and-Systems-LivDet-2023-Noncontact-Fingerprint"><a href="#Liveness-Detection-Competition-–-Noncontact-based-Fingerprint-Algorithms-and-Systems-LivDet-2023-Noncontact-Fingerprint" class="headerlink" title="Liveness Detection Competition – Noncontact-based Fingerprint Algorithms and Systems (LivDet-2023 Noncontact Fingerprint)"></a>Liveness Detection Competition – Noncontact-based Fingerprint Algorithms and Systems (LivDet-2023 Noncontact Fingerprint)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00659">http://arxiv.org/abs/2310.00659</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sandip Purnapatra, Humaira Rezaie, Bhavin Jawade, Yu Liu, Yue Pan, Luke Brosell, Mst Rumana Sumi, Lambert Igene, Alden Dimarco, Srirangaraj Setlur, Soumyabrata Dey, Stephanie Schuckers, Marco Huber, Jan Niklas Kolf, Meiling Fang, Naser Damer, Banafsheh Adami, Raul Chitic, Karsten Seelert, Vishesh Mistry, Rahul Parthe, Umit Kacar</li>
<li>For: The paper is written for the assessment and reporting of state-of-the-art in Presentation Attack Detection (PAD) using noncontact fingerprint-based methods.* Methods: The paper uses a noncontact fingerprint-based PAD competition for algorithms and systems, with a common evaluation protocol that includes finger photos of various Presentation Attack Instruments (PAIs) and live fingers.* Results: The winning algorithm achieved an APCER of 11.35% and a BPCER of 0.62%, while the winning system achieved an APCER of 13.04% and a BPCER of 1.68%. Additionally, four-finger systems that make individual finger-based PAD decisions were also tested.Here are the three key points in Simplified Chinese text:* For: 这篇论文是用于评估和报告非接触指纹基于方法的攻击检测（PAD）的国际竞赛系列。* Methods: 这篇论文使用了一种非接触指纹基于的PAD竞赛，使用共同评估协议，包括指纹 фотографирования多种攻击工具（PAIs）和真实的手指。* Results: 赢家算法实现了APCER的11.35%和BPCER的0.62%，而赢家系统实现了APCER的13.04%和BPCER的1.68%。此外，四根手指系统也进行了个体指纹基于的PAD决策。<details>
<summary>Abstract</summary>
Liveness Detection (LivDet) is an international competition series open to academia and industry with the objec-tive to assess and report state-of-the-art in Presentation Attack Detection (PAD). LivDet-2023 Noncontact Fingerprint is the first edition of the noncontact fingerprint-based PAD competition for algorithms and systems. The competition serves as an important benchmark in noncontact-based fingerprint PAD, offering (a) independent assessment of the state-of-the-art in noncontact-based fingerprint PAD for algorithms and systems, and (b) common evaluation protocol, which includes finger photos of a variety of Presentation Attack Instruments (PAIs) and live fingers to the biometric research community (c) provides standard algorithm and system evaluation protocols, along with the comparative analysis of state-of-the-art algorithms from academia and industry with both old and new android smartphones. The winning algorithm achieved an APCER of 11.35% averaged overall PAIs and a BPCER of 0.62%. The winning system achieved an APCER of 13.0.4%, averaged over all PAIs tested over all the smartphones, and a BPCER of 1.68% over all smartphones tested. Four-finger systems that make individual finger-based PAD decisions were also tested. The dataset used for competition will be available 1 to all researchers as per data share protocol
</details>
<details>
<summary>摘要</summary>
生命检测（LivDet）是一个国际竞赛系列，开放于学术和产业领域，旨在评估和报告当前最佳的演示攻击检测（PAD）技术。LivDet-2023非接触指纹是第一届非接触指纹基于PAD竞赛，用于评估和比较不同算法和系统的性能。这个竞赛作为非接触指纹PAD领域的重要标准，提供了独立的评估标准，以及一套共同的评估协议。该竞赛包括了多种演示攻击工具（PAIs）和真实的手指图像，以及一套标准的评估协议。winning algorithm achieved an APCER of 11.35% and a BPCER of 0.62% over all PAIs, and the winning system achieved an APCER of 13.04% and a BPCER of 1.68% over all smartphones tested. In addition, four-finger systems that make individual finger-based PAD decisions were also tested. The dataset used for the competition will be made available to all researchers according to the data sharing protocol.
</details></li>
</ul>
<hr>
<h2 id="Beyond-Task-Performance-Evaluating-and-Reducing-the-Flaws-of-Large-Multimodal-Models-with-In-Context-Learning"><a href="#Beyond-Task-Performance-Evaluating-and-Reducing-the-Flaws-of-Large-Multimodal-Models-with-In-Context-Learning" class="headerlink" title="Beyond Task Performance: Evaluating and Reducing the Flaws of Large Multimodal Models with In-Context Learning"></a>Beyond Task Performance: Evaluating and Reducing the Flaws of Large Multimodal Models with In-Context Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00647">http://arxiv.org/abs/2310.00647</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mshukor/EvALign-ICL">https://github.com/mshukor/EvALign-ICL</a></li>
<li>paper_authors: Mustafa Shukor, Alexandre Rame, Corentin Dancette, Matthieu Cord</li>
<li>for: 这个论文旨在探讨大型多模型（LMMs）的问题和局限性，以及如何通过增强ICL（增强内容学习）来解决这些问题。</li>
<li>methods: 这个论文使用了8种不同的开源LMM（基于FLAMINGO架构），并对这些模型进行了5个轴的评估：幻觉、抑郁、 композиitional、解释性和遵循指令。此外，论文还研究了ICL的效果于LMMs的问题。</li>
<li>results: 论文发现，尽管LMMs在任务性能方面表现出色，但它们仍然存在许多问题，例如幻觉、抑郁、不compositional和解释性不足。ICL可以有效解决一些问题，但并不能解决所有问题。此外，论文还提出了一些新的多模态ICL方法，如多任务ICL、链式回忆ICL和自我修正ICL，以解决LMMs的问题。<details>
<summary>Abstract</summary>
Following the success of Large Language Models (LLMs), Large Multimodal Models (LMMs), such as the Flamingo model and its subsequent competitors, have started to emerge as natural steps towards generalist agents. However, interacting with recent LMMs reveals major limitations that are hardly captured by the current evaluation benchmarks. Indeed, task performances (e.g., VQA accuracy) alone do not provide enough clues to understand their real capabilities, limitations, and to which extent such models are aligned to human expectations. To refine our understanding of those flaws, we deviate from the current evaluation paradigm and propose the EvALign-ICL framework, in which we (1) evaluate 8 recent open-source LMMs (based on the Flamingo architecture such as OpenFlamingo and IDEFICS) on 5 different axes; hallucinations, abstention, compositionality, explainability and instruction following. Our evaluation on these axes reveals major flaws in LMMs. To efficiently address these problems, and inspired by the success of in-context learning (ICL) in LLMs, (2) we explore ICL as a solution and study how it affects these limitations. Based on our ICL study, (3) we push ICL further and propose new multimodal ICL approaches such as; Multitask-ICL, Chain-of-Hindsight-ICL, and Self-Correcting-ICL. Our findings are as follows; (1) Despite their success, LMMs have flaws that remain unsolved with scaling alone. (2) The effect of ICL on LMMs flaws is nuanced; despite its effectiveness for improved explainability, abstention, and instruction following, ICL does not improve compositional abilities, and actually even amplifies hallucinations. (3) The proposed ICL variants are promising as post-hoc approaches to efficiently tackle some of those flaws. The code is available here: https://evalign-icl.github.io/
</details>
<details>
<summary>摘要</summary>
以 Large Language Models (LLMs) 的成功为契机，Large Multimodal Models (LMMs) 也在出现，如FLAMINGO模型和其竞争对手。然而，与最近的 LMMs 交互后，我们发现它们存在重要的局限性，这些局限性并不被当前的评价标准完全捕捉。实际上，任务性能（如 VQA 准确率） alone 不能够反映它们的真正能力和局限性，以及与人类期望的对应度。为了更好地理解这些问题，我们在评价标准之外尝试了 EvALign-ICL 框架，其中我们（1）评价了 8 个最近开源 LMMs（基于 FLAMINGO 架构，如 OpenFlamingo 和 IDEFICS）在 5 个轴上，即幻觉、抑制、复合性、解释性和遵从性。我们的评价表明，LMMs 存在重要的问题。为了有效地解决这些问题，我们（2）探索了 ICL 的潜在作用，并研究了 ICL 如何影响这些局限性。基于我们的 ICL 研究，我们（3）将 ICL 推广到多Modal ICL，并提出了新的多模态 ICL 方法，如 Multitask-ICL、Chain-of-Hindsight-ICL 和 Self-Correcting-ICL。我们的发现是，（1）虽然 LMMs 成功，但它们仍存在不解决的问题，不能通过缩放 alone 解决。（2）ICL 对 LMMs 的缺陷有复杂的影响，虽有效提高了解释性、抑制和遵从性，但是不会改善复合性，并且实际上会加剧幻觉。（3）我们提出的 ICL 变体是可以有效地解决一些问题的后续方法。代码可以在以下链接获取：https://evalign-icl.github.io/
</details></li>
</ul>
<hr>
<h2 id="RegBN-Batch-Normalization-of-Multimodal-Data-with-Regularization"><a href="#RegBN-Batch-Normalization-of-Multimodal-Data-with-Regularization" class="headerlink" title="RegBN: Batch Normalization of Multimodal Data with Regularization"></a>RegBN: Batch Normalization of Multimodal Data with Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00641">http://arxiv.org/abs/2310.00641</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mogvision/regbn">https://github.com/mogvision/regbn</a></li>
<li>paper_authors: Morteza Ghahremani, Christian Wachinger</li>
<li>for: 这篇论文的目的是提出一种新的多modal资料Normalization方法，以便将多种不同的数据模式融合在一起，提高模组的表现。</li>
<li>methods: 这篇论文使用了RegBN方法，具有调整Regularization的功能，可以干预干扰因素和背景噪音的影响，并且可以跨多个数据模式进行normalization。</li>
<li>results: 这篇论文在八个数据库上进行验证，包括语言、音频、图像、视频、深度、表格和3D MRI等多种数据模式，以及不同的架构（如多层感知神经网络、卷积神经网络和视觉转移神经网络），展示了RegBN方法的通用性和效iveness。<details>
<summary>Abstract</summary>
Recent years have witnessed a surge of interest in integrating high-dimensional data captured by multisource sensors, driven by the impressive success of neural networks in the integration of multimodal data. However, the integration of heterogeneous multimodal data poses a significant challenge, as confounding effects and dependencies among such heterogeneous data sources introduce unwanted variability and bias, leading to suboptimal performance of multimodal models. Therefore, it becomes crucial to normalize the low- or high-level features extracted from data modalities before their fusion takes place. This paper introduces a novel approach for the normalization of multimodal data, called RegBN, that incorporates regularization. RegBN uses the Frobenius norm as a regularizer term to address the side effects of confounders and underlying dependencies among different data sources. The proposed method generalizes well across multiple modalities and eliminates the need for learnable parameters, simplifying training and inference. We validate the effectiveness of RegBN on eight databases from five research areas, encompassing diverse modalities such as language, audio, image, video, depth, tabular, and 3D MRI. The proposed method demonstrates broad applicability across different architectures such as multilayer perceptrons, convolutional neural networks, and vision transformers, enabling effective normalization of both low- and high-level features in multimodal neural networks. RegBN is available at \url{https://github.com/mogvision/regbn}.
</details>
<details>
<summary>摘要</summary>
近年来，有一个强大的兴趣在将多维数据集成到多源感知器中，这主要归功于神经网络在多模态数据的集成中的出色成绩。然而，多模态数据的集成带来一些挑战，因为不同的数据来源之间存在干扰效应和依赖关系，这会导致模型的性能下降。因此，在模型融合之前，需要对数据模式中的低级或高级特征进行Normalization。这篇文章提出了一种新的多模态数据Normalization方法，称为RegBN，它包含了正则化项。RegBN使用 Frobenius  нор为正则化项，以解决不同数据来源之间的干扰效应和依赖关系。提出的方法可以通过多种模式进行扩展，无需学习参数，因此训练和推理变得更加简单。我们在八个数据库中进行了验证，包括语言、音频、图像、视频、深度、表格和3D MRI 等多种模式，RegBN 的效果广泛，可以effective地Normalization多模式的低级和高级特征。RegBN 可以在多层感知器、卷积神经网络和视transformer 等不同的架构上进行应用，为多模态神经网络的Normalization提供了一个简单的解决方案。RegBN 的代码可以在 <https://github.com/mogvision/regbn> 上下载。
</details></li>
</ul>
<hr>
<h2 id="Segmentation-based-Assessment-of-Tumor-Vessel-Involvement-for-Surgical-Resectability-Prediction-of-Pancreatic-Ductal-Adenocarcinoma"><a href="#Segmentation-based-Assessment-of-Tumor-Vessel-Involvement-for-Surgical-Resectability-Prediction-of-Pancreatic-Ductal-Adenocarcinoma" class="headerlink" title="Segmentation-based Assessment of Tumor-Vessel Involvement for Surgical Resectability Prediction of Pancreatic Ductal Adenocarcinoma"></a>Segmentation-based Assessment of Tumor-Vessel Involvement for Surgical Resectability Prediction of Pancreatic Ductal Adenocarcinoma</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00639">http://arxiv.org/abs/2310.00639</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christiaan Viviers, Mark Ramaekers, Amaan Valiuddin, Terese Hellström, Nick Tasios, John van der Ven, Igor Jacobs, Lotte Ewals, Joost Nederend, Peter de With, Misha Luyer, Fons van der Sommen</li>
<li>for:  This research aims to provide a workflow and deep learning-based segmentation models to automatically assess tumor-vessel involvement in Pancreatic ductal adenocarcinoma (PDAC) patients, which is crucial for determining treatment options and improving patient outcomes.</li>
<li>methods:  The proposed workflow involves processing CT scans to segment the tumor and vascular structures, analyzing spatial relationships and the extent of vascular involvement, using three different deep learning-based segmentation architectures (nnU-Net, 3D U-Net, and Probabilistic 3D U-Net).</li>
<li>results:  The segmentations achieved a high accuracy in segmenting veins, arteries, and the tumor, and enabled automated detection of tumor involvement with high accuracy (0.88 sensitivity and 0.86 specificity). Additionally, the models captured uncertainty in the predicted involvement, providing clinicians with a clear indication of tumor-vessel involvement and facilitating more informed decision-making for surgical interventions.<details>
<summary>Abstract</summary>
Pancreatic ductal adenocarcinoma (PDAC) is a highly aggressive cancer with limited treatment options. This research proposes a workflow and deep learning-based segmentation models to automatically assess tumor-vessel involvement, a key factor in determining tumor resectability. Correct assessment of resectability is vital to determine treatment options. The proposed workflow involves processing CT scans to segment the tumor and vascular structures, analyzing spatial relationships and the extent of vascular involvement, which follows a similar way of working as expert radiologists in PDAC assessment. Three segmentation architectures (nnU-Net, 3D U-Net, and Probabilistic 3D U-Net) achieve a high accuracy in segmenting veins, arteries, and the tumor. The segmentations enable automated detection of tumor involvement with high accuracy (0.88 sensitivity and 0.86 specificity) and automated computation of the degree of tumor-vessel contact. Additionally, due to significant inter-observer variability in these important structures, we present the uncertainty captured by each of the models to further increase insights into the predicted involvement. This result provides clinicians with a clear indication of tumor-vessel involvement and may be used to facilitate more informed decision-making for surgical interventions. The proposed method offers a valuable tool for improving patient outcomes, personalized treatment strategies and survival rates in pancreatic cancer.
</details>
<details>
<summary>摘要</summary>
《胰腺ductal adenocarcinoma（PDAC）是一种高度侵略性的Cancer，具有有限的治疗选择。本研究提出了一种工作流程和深度学习基于的分割模型，以自动评估肿瘤-血管涉及度，这是确定肿瘤可否切除的关键因素。正确评估可以决定疗程选择。本工作流程包括对CT扫描图进行肿瘤和血管结构分割，分析肿瘤和血管之间的空间关系和血管涉及度，与专业放射科医生在PDAC评估中采用相似的方法。三种分割建筑（nnU-Net、3D U-Net和概率3D U-Net）实现了高精度分割血管、肿瘤和血管。这些分割可以自动检测肿瘤涉及度，并计算肿瘤与血管之间的接触度，并且由于肿瘤-血管结构之间存在显著的Observer variability，我们还提供了每个模型对应的不确定性，以增加预测涉及度的信息。这些结果为临床医生提供了诊断肿瘤涉及度的清晰指导，可能用于改进患者的疗效、个性化治疗策略和存活率。》
</details></li>
</ul>
<hr>
<h2 id="Win-Win-Training-High-Resolution-Vision-Transformers-from-Two-Windows"><a href="#Win-Win-Training-High-Resolution-Vision-Transformers-from-Two-Windows" class="headerlink" title="Win-Win: Training High-Resolution Vision Transformers from Two Windows"></a>Win-Win: Training High-Resolution Vision Transformers from Two Windows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00632">http://arxiv.org/abs/2310.00632</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vincent Leroy, Jerome Revaud, Thomas Lucas, Philippe Weinzaepfel</li>
<li>For: 提高高分辨率视觉转换器的训练和执行效率。* Methods: 随机窗口Masking技术，使模型只需学习每个窗口内的本地交互，以及不同窗口间的全局交互。* Results: 在推理时直接处理高分辨率输入，不需特殊处理，并且在 semantic segmentation 和 optical flow 任务上达到了最佳性能。<details>
<summary>Abstract</summary>
Transformers have become the standard in state-of-the-art vision architectures, achieving impressive performance on both image-level and dense pixelwise tasks. However, training vision transformers for high-resolution pixelwise tasks has a prohibitive cost. Typical solutions boil down to hierarchical architectures, fast and approximate attention, or training on low-resolution crops. This latter solution does not constrain architectural choices, but it leads to a clear performance drop when testing at resolutions significantly higher than that used for training, thus requiring ad-hoc and slow post-processing schemes. In this paper, we propose a novel strategy for efficient training and inference of high-resolution vision transformers: the key principle is to mask out most of the high-resolution inputs during training, keeping only N random windows. This allows the model to learn local interactions between tokens inside each window, and global interactions between tokens from different windows. As a result, the model can directly process the high-resolution input at test time without any special trick. We show that this strategy is effective when using relative positional embedding such as rotary embeddings. It is 4 times faster to train than a full-resolution network, and it is straightforward to use at test time compared to existing approaches. We apply this strategy to the dense monocular task of semantic segmentation, and find that a simple setting with 2 windows performs best, hence the name of our method: Win-Win. To demonstrate the generality of our contribution, we further extend it to the binocular task of optical flow, reaching state-of-the-art performance on the Spring benchmark that contains Full-HD images with an inference time an order of magnitude faster than the best competitor.
</details>
<details>
<summary>摘要</summary>
启示器变得是现代视觉建筑标准，在图像级和密集像素级任务上达到了印象性的表现。然而，在高分辨率像素级任务上训练视觉启示器有束缚的成本。常见的解决方案包括层次结构、快速和 aproximate 注意力以及在训练低分辨率裁剪上进行训练。这个后者不会限制建筑选择，但会导致在测试分辨率远高于训练分辨率时的表现下降，需要特殊的预处理方案。在这篇论文中，我们提出了一种新的高分辨率视觉启示器训练和执行策略：关键原则是在训练时随机隐藏大多数高分辨率输入，只保留N个随机窗口。这 позвоits 模型学习每个窗口内Token之间的本地互动，以及不同窗口内Token之间的全局互动。因此，模型可以直接在测试时处理高分辨率输入，不需要特殊的技巧。我们发现，使用相对位置嵌入，如旋转嵌入，这种策略是最效的。训练时间比普通网络快四倍，并且在测试时使用非常简单。我们在激素分割任务中应用了这种策略，并发现使用2个窗口得到最佳性能，因此我们称之为Win-Win。为了证明我们的贡献的通用性，我们进一步扩展了它到双目任务中，达到了SpringBenchmark上的最新纪录，该纪录包含高清晰度图像，并且在执行时间上比最佳竞争者快一个数量级。
</details></li>
</ul>
<hr>
<h2 id="Finger-UNet-A-U-Net-based-Multi-Task-Architecture-for-Deep-Fingerprint-Enhancement"><a href="#Finger-UNet-A-U-Net-based-Multi-Task-Architecture-for-Deep-Fingerprint-Enhancement" class="headerlink" title="Finger-UNet: A U-Net based Multi-Task Architecture for Deep Fingerprint Enhancement"></a>Finger-UNet: A U-Net based Multi-Task Architecture for Deep Fingerprint Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00629">http://arxiv.org/abs/2310.00629</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ekta Gavas, Anoop Namboodiri</li>
<li>for: 提高低质量指纹识别率</li>
<li>methods: 使用Discrete Wavelet Transform（DWT）进行指纹提高，并使用波峰注意模块代替最大池化，同时使用多任务学习和方向估计任务进行指纹重建。</li>
<li>results: 在FVC 2002和NIST SD302数据库上进行实验，证明我们的方法可以提高低质量指纹识别率，并且比前一些方法更高效。<details>
<summary>Abstract</summary>
For decades, fingerprint recognition has been prevalent for security, forensics, and other biometric applications. However, the availability of good-quality fingerprints is challenging, making recognition difficult. Fingerprint images might be degraded with a poor ridge structure and noisy or less contrasting backgrounds. Hence, fingerprint enhancement plays a vital role in the early stages of the fingerprint recognition/verification pipeline. In this paper, we investigate and improvise the encoder-decoder style architecture and suggest intuitive modifications to U-Net to enhance low-quality fingerprints effectively. We investigate the use of Discrete Wavelet Transform (DWT) for fingerprint enhancement and use a wavelet attention module instead of max pooling which proves advantageous for our task. Moreover, we replace regular convolutions with depthwise separable convolutions, which significantly reduces the memory footprint of the model without degrading the performance. We also demonstrate that incorporating domain knowledge with fingerprint minutiae prediction task can improve fingerprint reconstruction through multi-task learning. Furthermore, we also integrate the orientation estimation task to propagate the knowledge of ridge orientations to enhance the performance further. We present the experimental results and evaluate our model on FVC 2002 and NIST SD302 databases to show the effectiveness of our approach compared to previous works.
</details>
<details>
<summary>摘要</summary>
In this paper, we improve the encoder-decoder style architecture and suggest intuitive modifications to U-Net to enhance low-quality fingerprints effectively. We use Discrete Wavelet Transform (DWT) for fingerprint enhancement and replace regular convolutions with depthwise separable convolutions, which significantly reduces the memory footprint of the model without compromising performance.Moreover, we incorporate domain knowledge with fingerprint minutiae prediction tasks to improve fingerprint reconstruction through multi-task learning. We also integrate orientation estimation tasks to propagate the knowledge of ridge orientations and enhance performance further.We present experimental results and evaluate our model on FVC 2002 and NIST SD302 databases to demonstrate the effectiveness of our approach compared to previous works.
</details></li>
</ul>
<hr>
<h2 id="GhostEncoder-Stealthy-Backdoor-Attacks-with-Dynamic-Triggers-to-Pre-trained-Encoders-in-Self-supervised-Learning"><a href="#GhostEncoder-Stealthy-Backdoor-Attacks-with-Dynamic-Triggers-to-Pre-trained-Encoders-in-Self-supervised-Learning" class="headerlink" title="GhostEncoder: Stealthy Backdoor Attacks with Dynamic Triggers to Pre-trained Encoders in Self-supervised Learning"></a>GhostEncoder: Stealthy Backdoor Attacks with Dynamic Triggers to Pre-trained Encoders in Self-supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00626">http://arxiv.org/abs/2310.00626</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiannan Wang, Changchun Yin, Zhe Liu, Liming Fang, Run Wang, Chenhao Lin</li>
<li>for: 本研究旨在提出一种隐藏式、动态Backdoor攻击方法，用于自动学习预训练的图像编码器。</li>
<li>methods: 该攻击方法利用图像隐写技术，将隐藏信息编码到无害图像中，生成后门样本。然后，通过精制预训练图像编码器，植入后门。</li>
<li>results: 试验结果表明，GhostEncoder可以在图像上实现高度的隐藏性，让目标模型具有高度的攻击成功率，而不会丢失其实用性。此外，GhostEncoder也可以抵御现有的防御技术。<details>
<summary>Abstract</summary>
Within the realm of computer vision, self-supervised learning (SSL) pertains to training pre-trained image encoders utilizing a substantial quantity of unlabeled images. Pre-trained image encoders can serve as feature extractors, facilitating the construction of downstream classifiers for various tasks. However, the use of SSL has led to an increase in security research related to various backdoor attacks. Currently, the trigger patterns used in backdoor attacks on SSL are mostly visible or static (sample-agnostic), making backdoors less covert and significantly affecting the attack performance. In this work, we propose GhostEncoder, the first dynamic invisible backdoor attack on SSL. Unlike existing backdoor attacks on SSL, which use visible or static trigger patterns, GhostEncoder utilizes image steganography techniques to encode hidden information into benign images and generate backdoor samples. We then fine-tune the pre-trained image encoder on a manipulation dataset to inject the backdoor, enabling downstream classifiers built upon the backdoored encoder to inherit the backdoor behavior for target downstream tasks. We evaluate GhostEncoder on three downstream tasks and results demonstrate that GhostEncoder provides practical stealthiness on images and deceives the victim model with a high attack success rate without compromising its utility. Furthermore, GhostEncoder withstands state-of-the-art defenses, including STRIP, STRIP-Cl, and SSL-Cleanse.
</details>
<details>
<summary>摘要</summary>
在计算机视觉领域，自主学习（SSL）指的是使用大量未标注图像进行训练已经预训练的图像编码器。这些预训练图像编码器可以作为特征提取器，帮助建立下游分类器 для多种任务。然而，使用SSL带来了安全研究中的各种后门攻击。现在，许多后门攻击使用SSL的触发模式都是可见或静止的（样本不具特定），这使得后门变得更加明显，对攻击性能产生负面影响。在这种情况下，我们提出了 GhostEncoder，首个在SSL中的动态隐藏后门攻击。与现有的SSL后门攻击不同，GhostEncoder使用图像隐写技术来编码隐藏信息到正常图像中，并生成后门样本。然后，我们精细调整预训练图像编码器，使其在扭曲数据集上进行后门插入，使得基于后门编码器的下游分类器继承后门行为，并且不会增加负面影响。我们对 GhostEncoder 进行了三个下游任务的评估，结果表明，GhostEncoder 在图像上具有实际的隐藏性，诱导了受试模型，并且不会降低其实用性。此外，GhostEncoder 可以抵御当前的防御技术，包括 STRIP、STRIP-Cl 和 SSL-Cleanse。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Adversarial-Transferability-in-Federated-Learning"><a href="#Understanding-Adversarial-Transferability-in-Federated-Learning" class="headerlink" title="Understanding Adversarial Transferability in Federated Learning"></a>Understanding Adversarial Transferability in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00616">http://arxiv.org/abs/2310.00616</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yijiang Li, Ying Gao, Haohan Wang<br>for:  This paper investigates the robustness and security issues of federated learning (FL) systems in a practical setting where malicious clients disguise their identities and launch transferable adversarial attacks.methods: The paper uses empirical experiments and theoretical analysis to study the robustness of FL systems against such attacks, and hypothesizes that the decentralized training on distributed data and the averaging operation contribute to the system’s robustness.results: The paper finds that the federated model is more robust compared to its centralized counterpart when the accuracy on clean images is comparable, and provides evidence from both empirical experiments and theoretical analysis to support this conclusion.<details>
<summary>Abstract</summary>
We investigate the robustness and security issues from a novel and practical setting: a group of malicious clients has impacted the model during training by disguising their identities and acting as benign clients, and only revealing their adversary position after the training to conduct transferable adversarial attacks with their data, which is usually a subset of the data that FL system is trained with. Our aim is to offer a full understanding of the challenges the FL system faces in this practical setting across a spectrum of configurations. We notice that such an attack is possible, but the federated model is more robust compared with its centralized counterpart when the accuracy on clean images is comparable. Through our study, we hypothesized the robustness is from two factors: the decentralized training on distributed data and the averaging operation. We provide evidence from both the perspective of empirical experiments and theoretical analysis. Our work has implications for understanding the robustness of federated learning systems and poses a practical question for federated learning applications.
</details>
<details>
<summary>摘要</summary>
我们研究了一种新和实际的场景中的安全和稳定性问题：一群恶意客户端在训练过程中对模型产生了影响，通过掩饰自己的身份和行为如善意客户端，并只在训练后 revelation 自己为敌对位置，以进行可转移性攻击。我们的目标是对 Federated Learning 系统在这种实际场景中所面临的挑战进行全面的理解，并通过不同的配置进行spectrum 的研究。我们发现这种攻击是可能的，但在模型级别的清洁图像准确率相似时， federated model 比其中央化模型更加稳定。我们认为这种稳定性来自两个因素：分布式训练在分布式数据上和平均操作。我们通过实验和理论分析提供证据。我们的工作对 Federated Learning 系统的稳定性有重要的意义，并提出了实际问题 для Federated Learning 应用。
</details></li>
</ul>
<hr>
<h2 id="Scene-aware-Human-Motion-Forecasting-via-Mutual-Distance-Prediction"><a href="#Scene-aware-Human-Motion-Forecasting-via-Mutual-Distance-Prediction" class="headerlink" title="Scene-aware Human Motion Forecasting via Mutual Distance Prediction"></a>Scene-aware Human Motion Forecasting via Mutual Distance Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00615">http://arxiv.org/abs/2310.00615</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chaoyue Xing, Wei Mao, Miaomiao Liu</li>
<li>for: 本研究强调解决人体动作预测中的场景相关性问题，通过模elling人体-场景交互来预测未来人体动作。</li>
<li>methods: 我们提出了基于人体-场景距离的人体动作预测方法，其中距离包括人体Vertex与场景表面之间的积分距离和基准场景点与人体网格之间的距离。我们还开发了一个预测步骤两步管道，先预测未来距离，然后根据预测距离预测未来人体动作。在训练过程中，我们显式地促进了预测pose与距离之间的一致性。</li>
<li>results: 我们的方法在 sintetic和实际数据集上比前学者的方法表现更好，提高了人体动作预测的精度和可靠性。<details>
<summary>Abstract</summary>
In this paper, we tackle the problem of scene-aware 3D human motion forecasting. A key challenge of this task is to predict future human motions that are consistent with the scene, by modelling the human-scene interactions. While recent works have demonstrated that explicit constraints on human-scene interactions can prevent the occurrence of ghost motion, they only provide constraints on partial human motion e.g., the global motion of the human or a few joints contacting the scene, leaving the rest motion unconstrained. To address this limitation, we propose to model the human-scene interaction with the mutual distance between the human body and the scene. Such mutual distances constrain both the local and global human motion, resulting in a whole-body motion constrained prediction. In particular, mutual distance constraints consist of two components, the signed distance of each vertex on the human mesh to the scene surface, and the distance of basis scene points to the human mesh. We develop a pipeline with two prediction steps that first predicts the future mutual distances from the past human motion sequence and the scene, and then forecasts the future human motion conditioning on the predicted mutual distances. During training, we explicitly encourage consistency between the predicted poses and the mutual distances. Our approach outperforms the state-of-the-art methods on both synthetic and real datasets.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Skip-Plan-Procedure-Planning-in-Instructional-Videos-via-Condensed-Action-Space-Learning"><a href="#Skip-Plan-Procedure-Planning-in-Instructional-Videos-via-Condensed-Action-Space-Learning" class="headerlink" title="Skip-Plan: Procedure Planning in Instructional Videos via Condensed Action Space Learning"></a>Skip-Plan: Procedure Planning in Instructional Videos via Condensed Action Space Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00608">http://arxiv.org/abs/2310.00608</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiheng Li, Wenjia Geng, Muheng Li, Lei Chen, Yansong Tang, Jiwen Lu, Jie Zhou</li>
<li>for: 提出了一种基于减少行动空间学习的过程规划方法，以解决现有方法在高维状态监测和动作序列错误积累问题上遇到困难。</li>
<li>methods: 将过程规划问题抽象为数学链模型，通过跳过不确定节点和边，将长和复杂的序列函数转化为短而可靠的两种方式。</li>
<li>results: 对 CrossTask 和 COIN 测试集进行了广泛的实验，并达到了当前状态的最佳性能。<details>
<summary>Abstract</summary>
In this paper, we propose Skip-Plan, a condensed action space learning method for procedure planning in instructional videos. Current procedure planning methods all stick to the state-action pair prediction at every timestep and generate actions adjacently. Although it coincides with human intuition, such a methodology consistently struggles with high-dimensional state supervision and error accumulation on action sequences. In this work, we abstract the procedure planning problem as a mathematical chain model. By skipping uncertain nodes and edges in action chains, we transfer long and complex sequence functions into short but reliable ones in two ways. First, we skip all the intermediate state supervision and only focus on action predictions. Second, we decompose relatively long chains into multiple short sub-chains by skipping unreliable intermediate actions. By this means, our model explores all sorts of reliable sub-relations within an action sequence in the condensed action space. Extensive experiments show Skip-Plan achieves state-of-the-art performance on the CrossTask and COIN benchmarks for procedure planning.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了Skip-Plan方法，这是一种简化动作空间学习方法 для过程规划在教程视频中。现有的过程规划方法都是在每个时间步骤上预测状态-动作对，这与人类直觉相吻合，但这种方法ология在高维状态监督和动作序列错误积累方面一直遇到困难。在这种工作中，我们抽象了过程规划问题为数学链模型。通过在动作链中跳过不确定的节点和边，我们将长而复杂的序列函数转化为短而可靠的两种方式。第一种方法是跳过所有间接状态监督，只Focus on 动作预测。第二种方法是将相对较长的链分解成多个短的子链，通过跳过不可靠的间接动作来实现。通过这种方式，我们的模型可以在简化动作空间中探索所有可靠的子关系。我们的实验表明Skip-Plan在CrossTask和COIN测试准则上达到了过程规划领域的状态天空。
</details></li>
</ul>
<hr>
<h2 id="Quantum-image-edge-detection-based-on-eight-direction-Sobel-operator-for-NEQR"><a href="#Quantum-image-edge-detection-based-on-eight-direction-Sobel-operator-for-NEQR" class="headerlink" title="Quantum image edge detection based on eight-direction Sobel operator for NEQR"></a>Quantum image edge detection based on eight-direction Sobel operator for NEQR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03037">http://arxiv.org/abs/2310.03037</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjie Liu, Lu Wang</li>
<li>for: 这个论文是为了提出一种基于量子机制的图像边缘检测算法（QSED），以解决经典算法遇到的实时问题。</li>
<li>methods: 该算法基于八个方向的 Sobel 算子，不仅可以减少部分图像的边缘信息损失，还同时计算所有像素的八个方向的梯度值。</li>
<li>results: 对于 2^n x 2^n 图像，该算法的复杂度可以降至 O(n^2 + q^2)，比其他经典或量子算法低。实验表明，该算法可以更好地检测高清像中的对角边缘。<details>
<summary>Abstract</summary>
Quantum Sobel edge detection (QSED) is a kind of algorithm for image edge detection using quantum mechanism, which can solve the real-time problem encountered by classical algorithms. However, the existing QSED algorithms only consider two- or four-direction Sobel operator, which leads to a certain loss of edge detail information in some high-definition images. In this paper, a novel QSED algorithm based on eight-direction Sobel operator is proposed, which not only reduces the loss of edge information, but also simultaneously calculates eight directions' gradient values of all pixel in a quantum image. In addition, the concrete quantum circuits, which consist of gradient calculation, non-maximum suppression, double threshold detection and edge tracking units, are designed in details. For a 2^n x 2^n image with q gray scale, the complexity of our algorithm can be reduced to O(n^2 + q^2), which is lower than other existing classical or quantum algorithms. And the simulation experiment demonstrates that our algorithm can detect more edge information, especially diagonal edges, than the two- and four-direction QSED algorithms.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Image-Data-Hiding-in-Neural-Compressed-Latent-Representations"><a href="#Image-Data-Hiding-in-Neural-Compressed-Latent-Representations" class="headerlink" title="Image Data Hiding in Neural Compressed Latent Representations"></a>Image Data Hiding in Neural Compressed Latent Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00568">http://arxiv.org/abs/2310.00568</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen-Hsiu Huang, Ja-Ling Wu</li>
<li>for: 这个论文是为了开发一个朴素的图像数据隐藏框架，用于嵌入和提取秘密信息。</li>
<li>methods: 该方法使用了一种朴素的神经压缩器，并且使用了一种我们提出的消息编码器和解码器，同时使用了一种感知损失函数来实现高品质图像和高比特率。</li>
<li>results: 该方法可以在压缩领域中实现高水平的图像秘密性和竞争力强的水印鲁棒性，同时提高嵌入速度，比传统方法快上百倍。这些结果表明了将数据隐藏技术与神经压缩相结合的潜在优势和应用前景。<details>
<summary>Abstract</summary>
We propose an end-to-end learned image data hiding framework that embeds and extracts secrets in the latent representations of a generic neural compressor. By leveraging a perceptual loss function in conjunction with our proposed message encoder and decoder, our approach simultaneously achieves high image quality and high bit accuracy. Compared to existing techniques, our framework offers superior image secrecy and competitive watermarking robustness in the compressed domain while accelerating the embedding speed by over 50 times. These results demonstrate the potential of combining data hiding techniques and neural compression and offer new insights into developing neural compression techniques and their applications.
</details>
<details>
<summary>摘要</summary>
我们提出了一个末端学习的图像数据隐藏框架，该框架在一个通用的神经压缩器中嵌入和提取秘密。通过我们提出的消息编码器和解码器以及一种感知损失函数，我们的方法同时实现高质量图像和高比特率。与现有技术相比，我们的框架在压缩领域中提供了更高的图像机密性和竞争力强的水印鲁棒性，同时加速嵌入速度，提高了50倍以上。这些结果表明将数据隐藏技术与神经压缩结合可以实现新的应用和技术突破，并为神经压缩技术的发展提供新的视角。
</details></li>
</ul>
<hr>
<h2 id="CPIPS-Learning-to-Preserve-Perceptual-Distances-in-End-to-End-Image-Compression"><a href="#CPIPS-Learning-to-Preserve-Perceptual-Distances-in-End-to-End-Image-Compression" class="headerlink" title="CPIPS: Learning to Preserve Perceptual Distances in End-to-End Image Compression"></a>CPIPS: Learning to Preserve Perceptual Distances in End-to-End Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00559">http://arxiv.org/abs/2310.00559</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen-Hsiu Huang, Ja-Ling Wu</li>
<li>for: 这篇论文目的是提出一种基于神经科学和生物系统的压缩图像 Similarity Metric，以提高机器视觉任务中的图像压缩和比较效率。</li>
<li>methods: 该方法基于一种已经学习的神经网络编码器，通过修改压缩缓存来优先级化 semantics relevance，同时保持 perceived distance。</li>
<li>results: 对比 traditional DNN-based perceptual metrics，CPIPS 可以在计算速度和复杂度上具有明显的优势，而且可以在机器视觉任务中提高图像压缩和比较效率。<details>
<summary>Abstract</summary>
Lossy image coding standards such as JPEG and MPEG have successfully achieved high compression rates for human consumption of multimedia data. However, with the increasing prevalence of IoT devices, drones, and self-driving cars, machines rather than humans are processing a greater portion of captured visual content. Consequently, it is crucial to pursue an efficient compressed representation that caters not only to human vision but also to image processing and machine vision tasks. Drawing inspiration from the efficient coding hypothesis in biological systems and the modeling of the sensory cortex in neural science, we repurpose the compressed latent representation to prioritize semantic relevance while preserving perceptual distance. Our proposed method, Compressed Perceptual Image Patch Similarity (CPIPS), can be derived at a minimal cost from a learned neural codec and computed significantly faster than DNN-based perceptual metrics such as LPIPS and DISTS.
</details>
<details>
<summary>摘要</summary>
产生损失的图像编码标准如JPEG和MPEG已经成功实现了多媒体数据的高压缩率 для人类消耗。然而，随着互联网物联网设备、无人机和自动驾驶车的普及，机器正在处理更多的捕捉视觉内容。因此，我们需要追求一种高效的压缩表示，不仅适合人类视觉，还适合图像处理和机器视觉任务。 drawing inspiration from生物系统中的高效编码假设和神经科学中的感觉脑层模型，我们重新利用压缩潜在表示，优先级 semantic relevance 而保持perceptual distance。我们提议的方法，压缩感知图像patch similarity（CPIPS），可以在学习神经编码器的基础上得到，并且可以在DNN基于的感知度量方法，如LPIPS和DISTS，中计算得到更快。
</details></li>
</ul>
<hr>
<h2 id="Diving-into-the-Depths-of-Spotting-Text-in-Multi-Domain-Noisy-Scenes"><a href="#Diving-into-the-Depths-of-Spotting-Text-in-Multi-Domain-Noisy-Scenes" class="headerlink" title="Diving into the Depths of Spotting Text in Multi-Domain Noisy Scenes"></a>Diving into the Depths of Spotting Text in Multi-Domain Noisy Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00558">http://arxiv.org/abs/2310.00558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alloy Das, Sanket Biswas, Umapada Pal, Josep Lladós</li>
<li>for: 本研究旨在开发一种能够通用多个频道的自动Scene文本检测系统，以便在实际世界中针对不同频道进行文本检测。</li>
<li>methods: 我们采用了一种培训模型使用多个频道源数据，以便将其直接应用于目标频道中进行文本检测，而不是特定频道或enario中的精化。</li>
<li>results: 我们提出了一种基于超解析的终端转换器基线模型，称为DA-TextSpotter，可以在常见和arbitrary-shapedScene文本检测benchmark上达到或超越现有的文本检测建筑，同时具有较高的模型效率。<details>
<summary>Abstract</summary>
When used in a real-world noisy environment, the capacity to generalize to multiple domains is essential for any autonomous scene text spotting system. However, existing state-of-the-art methods employ pretraining and fine-tuning strategies on natural scene datasets, which do not exploit the feature interaction across other complex domains. In this work, we explore and investigate the problem of domain-agnostic scene text spotting, i.e., training a model on multi-domain source data such that it can directly generalize to target domains rather than being specialized for a specific domain or scenario. In this regard, we present the community a text spotting validation benchmark called Under-Water Text (UWT) for noisy underwater scenes to establish an important case study. Moreover, we also design an efficient super-resolution based end-to-end transformer baseline called DA-TextSpotter which achieves comparable or superior performance over existing text spotting architectures for both regular and arbitrary-shaped scene text spotting benchmarks in terms of both accuracy and model efficiency. The dataset, code and pre-trained models will be released upon acceptance.
</details>
<details>
<summary>摘要</summary>
当用于实际噪声环境中的自动Scene文本检测系统时，能够泛化到多个领域是非常重要的。然而，现有的状态艺术方法通常采用预训练和细化策略在自然场景数据集上，这并不利用场景文本之间的特征互动。在这种情况下，我们探索和探讨域性文本检测问题，即在多个领域源数据上训练一个模型，使其直接泛化到目标领域而不是特定领域或enario。为此，我们向社区提供了文本检测验证 benchmark called Under-Water Text (UWT)，以便在水下场景中进行噪声检测。此外，我们还设计了一种高效的超解像基于 transformer 结构的终端模型called DA-TextSpotter，它在常见和任意形状场景文本检测benchmark上实现了和现有文本检测建筑物之间的比较或更好的性能，同时具有更高的模型效率。数据集、代码和预训练模型将在接受后发布。
</details></li>
</ul>
<hr>
<h2 id="Seal2Real-Prompt-Prior-Learning-on-Diffusion-Model-for-Unsupervised-Document-Seal-Data-Generation-and-Realisation"><a href="#Seal2Real-Prompt-Prior-Learning-on-Diffusion-Model-for-Unsupervised-Document-Seal-Data-Generation-and-Realisation" class="headerlink" title="Seal2Real: Prompt Prior Learning on Diffusion Model for Unsupervised Document Seal Data Generation and Realisation"></a>Seal2Real: Prompt Prior Learning on Diffusion Model for Unsupervised Document Seal Data Generation and Realisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00546">http://arxiv.org/abs/2310.00546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiancheng Huang, Yifan Liu, Yi Huang, Shifeng Chen</li>
<li>For: 提供了一种生成大量标注文档印章数据的方法，以便提高Docuement Processing中的印章相关任务的性能。* Methods: 使用了一种基于静止扩散模型的提问先学架构，通过无监督训练将生成器的优先生成能力迁移到印章生成任务中。* Results: 在Seal-DB dataset上进行实验，表明Seal2Real方法可以生成高度真实的印章图像，对后续的印章相关任务进行实际数据上的提升。<details>
<summary>Abstract</summary>
In document processing, seal-related tasks have very large commercial applications, such as seal segmentation, seal authenticity discrimination, seal removal, and text recognition under seals. However, these seal-related tasks are highly dependent on labelled document seal datasets, resulting in very little work on these tasks. To address the lack of labelled datasets for these seal-related tasks, we propose Seal2Real, a generative method that generates a large amount of labelled document seal data, and construct a Seal-DB dataset containing 20K images with labels. In Seal2Real, we propose a prompt prior learning architecture based on a pre-trained Stable Diffusion Model that migrates the prior generative power of to our seal generation task with unsupervised training. The realistic seal generation capability greatly facilitates the performance of downstream seal-related tasks on real data. Experimental results on the Seal-DB dataset demonstrate the effectiveness of Seal2Real.
</details>
<details>
<summary>摘要</summary>
在文档处理领域中，有很多商业应用，如印章分割、印章真实性识别、印章去除和文本识别下印章。然而，这些印章相关任务都受到了标注文档印章数据的限制，导致了这些任务的研究得到了非常少的积极性。为了解决标注文档印章数据的缺乏，我们提出了Seal2Real方法，该方法可以生成大量标注文档印章数据，并构建了一个名为Seal-DB的数据集，包含20K个图像和标签。在Seal2Real中，我们提出了一种提前学习的推荐模型，基于静止扩散模型，将先前的生成能力传递到我们的印章生成任务中，并在无监督下训练。这种印章生成能力可以帮助下游印章相关任务在真实数据上表现出色。实验结果表明，Seal2Real是有效的。
</details></li>
</ul>
<hr>
<h2 id="Implicit-Neural-Representations-and-the-Algebra-of-Complex-Wavelets"><a href="#Implicit-Neural-Representations-and-the-Algebra-of-Complex-Wavelets" class="headerlink" title="Implicit Neural Representations and the Algebra of Complex Wavelets"></a>Implicit Neural Representations and the Algebra of Complex Wavelets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00545">http://arxiv.org/abs/2310.00545</a></li>
<li>repo_url: None</li>
<li>paper_authors: T. Mitchell Roddenberry, Vishwanath Saragadam, Maarten V. de Hoop, Richard G. Baraniuk</li>
<li>for: 这个论文旨在探讨隐形神经表示（INR）如何用于欧几丁素空间上的信号处理和机器学习。</li>
<li>methods: 该论文使用多层感知器（MLP）来Parameterize图像，并使用浮动函数或浮动滤波器来实现INR。</li>
<li>results: 研究发现，使用浮动滤波器作为激活函数可以同时具有频率和空间特征的地方化特征，从而提高信号处理和机器学习的性能。此外，该论文还提出了多种INR架构设计方法，包括复杂滤波器、分离低频和高频拟合、以及基于所求信号的初始化方案。<details>
<summary>Abstract</summary>
Implicit neural representations (INRs) have arisen as useful methods for representing signals on Euclidean domains. By parameterizing an image as a multilayer perceptron (MLP) on Euclidean space, INRs effectively represent signals in a way that couples spatial and spectral features of the signal that is not obvious in the usual discrete representation, paving the way for continuous signal processing and machine learning approaches that were not previously possible. Although INRs using sinusoidal activation functions have been studied in terms of Fourier theory, recent works have shown the advantage of using wavelets instead of sinusoids as activation functions, due to their ability to simultaneously localize in both frequency and space. In this work, we approach such INRs and demonstrate how they resolve high-frequency features of signals from coarse approximations done in the first layer of the MLP. This leads to multiple prescriptions for the design of INR architectures, including the use of complex wavelets, decoupling of low and band-pass approximations, and initialization schemes based on the singularities of the desired signal.
</details>
<details>
<summary>摘要</summary>
启发神经表示（INR）在欧几何空间上表示信号已成为有用的方法。通过将图像 Parametric 为多层感知器（MLP）在欧几何空间中，INR 可以将信号表示为不可分离的空间和频谱特征，使得不可分离的信号处理和机器学习方法变得可能。虽然使用惯性函数的 INR 已经被研究，但是最近的工作表明使用浪谱函数作为激活函数的优势，因为它可以同时在频谱和空间中进行本地化。在这个工作中，我们研究了这些 INR 和它们如何在 MLP 的第一层中解决高频特征。这导致了多种 INR 架构的设计方法，包括复杂浪谱、分离低频和高频拟合、以及基于感知信号的初始化方案。
</details></li>
</ul>
<hr>
<h2 id="Enabling-Neural-Radiance-Fields-NeRF-for-Large-scale-Aerial-Images-–-A-Multi-tiling-Approach-and-the-Geometry-Assessment-of-NeRF"><a href="#Enabling-Neural-Radiance-Fields-NeRF-for-Large-scale-Aerial-Images-–-A-Multi-tiling-Approach-and-the-Geometry-Assessment-of-NeRF" class="headerlink" title="Enabling Neural Radiance Fields (NeRF) for Large-scale Aerial Images – A Multi-tiling Approach and the Geometry Assessment of NeRF"></a>Enabling Neural Radiance Fields (NeRF) for Large-scale Aerial Images – A Multi-tiling Approach and the Geometry Assessment of NeRF</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00530">http://arxiv.org/abs/2310.00530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ningli Xu, Rongjun Qin, Debao Huang, Fabio Remondino</li>
<li>for: 这个论文旨在提高大规模飞行图像数据上的NeRF纹理场的渐进性和准确性。</li>
<li>methods: 作者提出了一种位置特定采样技术和多摄像头分割策略来降低图像加载、表示训练和缓存内存占用，并提高内部缓存的速度。</li>
<li>results: 作者对两个典型的飞行图像数据集进行了比较，结果表明提出的NeRF方法在完整性和物体细节方面表现更好，但还有一定的准确性不足。<details>
<summary>Abstract</summary>
Neural Radiance Fields (NeRF) offer the potential to benefit 3D reconstruction tasks, including aerial photogrammetry. However, the scalability and accuracy of the inferred geometry are not well-documented for large-scale aerial assets,since such datasets usually result in very high memory consumption and slow convergence.. In this paper, we aim to scale the NeRF on large-scael aerial datasets and provide a thorough geometry assessment of NeRF. Specifically, we introduce a location-specific sampling technique as well as a multi-camera tiling (MCT) strategy to reduce memory consumption during image loading for RAM, representation training for GPU memory, and increase the convergence rate within tiles. MCT decomposes a large-frame image into multiple tiled images with different camera models, allowing these small-frame images to be fed into the training process as needed for specific locations without a loss of accuracy. We implement our method on a representative approach, Mip-NeRF, and compare its geometry performance with threephotgrammetric MVS pipelines on two typical aerial datasets against LiDAR reference data. Both qualitative and quantitative results suggest that the proposed NeRF approach produces better completeness and object details than traditional approaches, although as of now, it still falls short in terms of accuracy.
</details>
<details>
<summary>摘要</summary>
neural radiance fields (NeRF) 可能帮助3D重建任务，包括航空相机摄影。然而，对大规模航空资产的推广和准确性不够 document。在这篇论文中，我们希望通过缩放NeRF来适应大规模航空资产，并对NeRF的geometry进行全面评估。我们引入了位置特定的采样技术以及多camera tilting（MCT）策略，以降低内存占用量，提高内存中的表示训练，并提高分割区域内的快速转换。MCT将大幅度图像分解成多个不同摄像机模型的小幅度图像，以便在特定位置上无损loss的方式进行训练。我们实现了这种方法，并与三种光学多视角摄影管道进行比较，以评估NeRF的geometry性能。结果表明，我们的方法可以在两个典型的航空数据集上提供更好的完整性和物体细节，although it still lags behind in terms of accuracy。
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-Learning-of-Contextualized-Local-Visual-Embeddings"><a href="#Self-supervised-Learning-of-Contextualized-Local-Visual-Embeddings" class="headerlink" title="Self-supervised Learning of Contextualized Local Visual Embeddings"></a>Self-supervised Learning of Contextualized Local Visual Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00527">http://arxiv.org/abs/2310.00527</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sthalles/clove">https://github.com/sthalles/clove</a></li>
<li>paper_authors: Thalles Santos Silva, Helio Pedrini, Adín Ramírez Rivera</li>
<li>for: 这篇论文是为了提出一种基于自我supervised convolutional neural network（CNN）的方法，以学习适合紧密预测任务的表示。</li>
<li>methods: 这篇论文使用了一种新的多头自我注意层，通过对不同部分的图像特征进行相似性combine来学习 contextualized embedding。</li>
<li>results: 该论文在多个数据集上进行了广泛的 benchmarking，并达到了基于CNN架构的 dense prediction downstream tasks中的国际级表现，包括物体检测、实例分割、关键点检测和紧密pose estimation。<details>
<summary>Abstract</summary>
We present Contextualized Local Visual Embeddings (CLoVE), a self-supervised convolutional-based method that learns representations suited for dense prediction tasks. CLoVE deviates from current methods and optimizes a single loss function that operates at the level of contextualized local embeddings learned from output feature maps of convolution neural network (CNN) encoders. To learn contextualized embeddings, CLoVE proposes a normalized mult-head self-attention layer that combines local features from different parts of an image based on similarity. We extensively benchmark CLoVE's pre-trained representations on multiple datasets. CLoVE reaches state-of-the-art performance for CNN-based architectures in 4 dense prediction downstream tasks, including object detection, instance segmentation, keypoint detection, and dense pose estimation.
</details>
<details>
<summary>摘要</summary>
我团队现在发布 Contextualized Local Visual Embeddings (CLoVE)，这是一种自动学习的卷积神经网络方法，用于学习适用于紧凑预测任务的表示。CLoVE与现有方法不同，它优化了基于输出特征图卷积神经网络Encoder学习的上下文化 embedding 的单个损失函数。为了学习上下文化 embedding，CLoVE提议了一种归一化多头自注意层，通过相似性将不同部分的图像特征相结合。我们对 CLoVE 的预训练表示进行了广泛的比较，并达到了基于 CNN 架构的 dense prediction 下游任务中的国际级表现。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/01/cs.CV_2023_10_01/" data-id="clp869txz00klk588gvby27cq" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_10_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/01/cs.AI_2023_10_01/" class="article-date">
  <time datetime="2023-10-01T12:00:00.000Z" itemprop="datePublished">2023-10-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/01/cs.AI_2023_10_01/">cs.AI - 2023-10-01</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="OceanNet-A-principled-neural-operator-based-digital-twin-for-regional-oceans"><a href="#OceanNet-A-principled-neural-operator-based-digital-twin-for-regional-oceans" class="headerlink" title="OceanNet: A principled neural operator-based digital twin for regional oceans"></a>OceanNet: A principled neural operator-based digital twin for regional oceans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00813">http://arxiv.org/abs/2310.00813</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashesh Chattopadhyay, Michael Gray, Tianning Wu, Anna B. Lowe, Ruoying He</li>
<li>for: 这个研究旨在开发一种基于神经网络的数字孪生模型，用于海洋径流预测。</li>
<li>methods: 该模型使用FOURNIER神经网络算法和评估误差修正方法，以提高预测稳定性和抑制自回卷积误差增长。此外，使用 спектраль regularizer 减少小规模谱偏误。</li>
<li>results: 在北大西洋西部边域流（卡里布湾流）中测试了这种模型，并成功地预测了径流聚合体和弯曲流的季节预报。与传统的不连接、状态环境模型预测相比，这种模型显示出竞争的预测能力，同时减少了计算量500,000倍。这些成果表明物理启发的深度神经算法可能成为高分辨率数字海洋模型的成本效果的替代方案。<details>
<summary>Abstract</summary>
While data-driven approaches demonstrate great potential in atmospheric modeling and weather forecasting, ocean modeling poses distinct challenges due to complex bathymetry, land, vertical structure, and flow non-linearity. This study introduces OceanNet, a principled neural operator-based digital twin for ocean circulation. OceanNet uses a Fourier neural operator and predictor-evaluate-corrector integration scheme to mitigate autoregressive error growth and enhance stability over extended time scales. A spectral regularizer counteracts spectral bias at smaller scales. OceanNet is applied to the northwest Atlantic Ocean western boundary current (the Gulf Stream), focusing on the task of seasonal prediction for Loop Current eddies and the Gulf Stream meander. Trained using historical sea surface height (SSH) data, OceanNet demonstrates competitive forecast skill by outperforming SSH predictions by an uncoupled, state-of-the-art dynamical ocean model forecast, reducing computation by 500,000 times. These accomplishments demonstrate the potential of physics-inspired deep neural operators as cost-effective alternatives to high-resolution numerical ocean models.
</details>
<details>
<summary>摘要</summary>
While data-driven approaches have shown great potential in atmospheric modeling and weather forecasting, ocean modeling poses distinct challenges due to complex bathymetry, land, vertical structure, and flow non-linearity. This study introduces OceanNet, a principled neural operator-based digital twin for ocean circulation. OceanNet uses a Fourier neural operator and predictor-evaluate-corrector integration scheme to mitigate autoregressive error growth and enhance stability over extended time scales. A spectral regularizer counteracts spectral bias at smaller scales. OceanNet is applied to the northwest Atlantic Ocean western boundary current (the Gulf Stream), focusing on the task of seasonal prediction for Loop Current eddies and the Gulf Stream meander. Trained using historical sea surface height (SSH) data, OceanNet demonstrates competitive forecast skill by outperforming SSH predictions by an uncoupled, state-of-the-art dynamical ocean model forecast, reducing computation by 500,000 times. These accomplishments demonstrate the potential of physics-inspired deep neural operators as cost-effective alternatives to high-resolution numerical ocean models.Here's the translation in Traditional Chinese:而data-driven方法在大气模拟和天气预测中表现出了很大的潜力，但是海洋模拟却存在复杂的海底地形、陆地、垂直结构和流体非线性等挑战。本研究提出了OceanNet，一种基于神经算子的数字双胞虫 для海洋流动。OceanNet使用了福洛神经算子和预测评估修正 integrate scheme来减少自回归错误增长和提高时间尺度上的稳定性。另外，一种 Spectral regularizer 来抵消小尺度的 spectral bias。OceanNet 应用于北大西洋西部边Current（ GolStream），专注于季节预测Loop Current eddies 和 GolStream meander。使用历史海面高度数据进行训练，OceanNet 表现出了与不可分离的、现有的动力海洋模型预测 SSH 数据的竞争力，并且减少了计算量500,000倍。这些成就表明 physics-inspired deep neural operators 可以成为高分解能数字海洋模型的成本效果的替代方案。
</details></li>
</ul>
<hr>
<h2 id="Sparse-Backpropagation-for-MoE-Training"><a href="#Sparse-Backpropagation-for-MoE-Training" class="headerlink" title="Sparse Backpropagation for MoE Training"></a>Sparse Backpropagation for MoE Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00811">http://arxiv.org/abs/2310.00811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liyuan Liu, Jianfeng Gao, Weizhu Chen</li>
<li>for: 这篇论文主要旨在解决深度学习中的权值计算问题，特别是在混合专家（Mixture-of-Expert，MoE）模型中，通过专家路由实现稀疏计算，从而实现很好的扩展性。</li>
<li>methods: 该论文提出了一种名为SparseMixer的扩展性 gradient estimator，它可以在混合专家模型中实现可靠的梯度估计，并且不需要忽略某些梯度项，从而实现更加准确的梯度估计。SparseMixer基于数字差分方法，利用中点法来提供精确的梯度估计，计算 overhead 很低。</li>
<li>results: 应用SparseMixer于 Switch Transformer 上，在预训练和机器翻译任务中，可以见到较大的性能提升，快速加速训练过程，最多提高训练速度2倍。<details>
<summary>Abstract</summary>
One defining characteristic of Mixture-of-Expert (MoE) models is their capacity for conducting sparse computation via expert routing, leading to remarkable scalability. However, backpropagation, the cornerstone of deep learning, requires dense computation, thereby posting challenges in MoE gradient computations. Here, we introduce SparseMixer, a scalable gradient estimator that bridges the gap between backpropagation and sparse expert routing. Unlike typical MoE training which strategically neglects certain gradient terms for the sake of sparse computation and scalability, SparseMixer provides scalable gradient approximations for these terms, enabling reliable gradient estimation in MoE training. Grounded in a numerical ODE framework, SparseMixer harnesses the mid-point method, a second-order ODE solver, to deliver precise gradient approximations with negligible computational overhead. Applying SparseMixer to Switch Transformer on both pre-training and machine translation tasks, SparseMixer showcases considerable performance gain, accelerating training convergence up to 2 times.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. The translation may not be perfect, and some nuances or idiomatic expressions may be lost in translation.
</details></li>
</ul>
<hr>
<h2 id="Towards-Causal-Foundation-Model-on-Duality-between-Causal-Inference-and-Attention"><a href="#Towards-Causal-Foundation-Model-on-Duality-between-Causal-Inference-and-Attention" class="headerlink" title="Towards Causal Foundation Model: on Duality between Causal Inference and Attention"></a>Towards Causal Foundation Model: on Duality between Causal Inference and Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00809">http://arxiv.org/abs/2310.00809</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaqi Zhang, Joel Jennings, Cheng Zhang, Chao Ma</li>
<li>for: 这篇论文旨在建立复杂任务中的 causal inference 模型，以提高机器学习的效果。</li>
<li>methods: 该论文提出了一种新的、理论上正确的方法 called Causal Inference with Attention (CInA)，该方法通过多个无标注数据进行自主学习 causal learning，并在新数据上进行零shot causal inference。</li>
<li>results: 实验结果表明，CInA方法能够通过最终层的 transformer-type 架构实现零shot causal inference，并能够在不同的数据集上进行效果的泛化。<details>
<summary>Abstract</summary>
Foundation models have brought changes to the landscape of machine learning, demonstrating sparks of human-level intelligence across a diverse array of tasks. However, a gap persists in complex tasks such as causal inference, primarily due to challenges associated with intricate reasoning steps and high numerical precision requirements. In this work, we take a first step towards building causally-aware foundation models for complex tasks. We propose a novel, theoretically sound method called Causal Inference with Attention (CInA), which utilizes multiple unlabeled datasets to perform self-supervised causal learning, and subsequently enables zero-shot causal inference on unseen tasks with new data. This is based on our theoretical results that demonstrate the primal-dual connection between optimal covariate balancing and self-attention, facilitating zero-shot causal inference through the final layer of a trained transformer-type architecture. We demonstrate empirically that our approach CInA effectively generalizes to out-of-distribution datasets and various real-world datasets, matching or even surpassing traditional per-dataset causal inference methodologies.
</details>
<details>
<summary>摘要</summary>
基础模型已经带来了机器学习领域的变革，展示出人类水平的智能特性在多种任务上。然而，在复杂任务中，如 causal inference，仍存在一个差距，主要归结于复杂的逻辑步骤和高精度数字需求。在这项工作中，我们首次实现了基于自我超vised causal learning的可 causal-aware基础模型。我们提出了一种新的、理论上正确的方法called Causal Inference with Attention（CInA），通过多个无标签数据集进行自我超vised causal learning，并在新数据上进行零实际参数的 causal inference。这基于我们的理论结果，证明了优化 covariate balancing 和 self-attention 的 primal-dual 连接，从而实现零实际参数的 causal inference through 训练过的 transformer-type 架构的最后一层。我们通过实验证明，我们的方法 CInA 可以对不同的数据集和实际世界任务进行有效的泛化，与传统每个数据集的 causal inference 方法相当或者even surpass。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Engineering-for-Wind-Energy"><a href="#Knowledge-Engineering-for-Wind-Energy" class="headerlink" title="Knowledge Engineering for Wind Energy"></a>Knowledge Engineering for Wind Energy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00804">http://arxiv.org/abs/2310.00804</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Planet21century/TECHALDO">https://github.com/Planet21century/TECHALDO</a>.</li>
<li>paper_authors: Yuriy Marykovskiy, Thomas Clark, Justin Day, Marcus Wiens, Charles Henderson, Julian Quick, Imad Abdallah, Anna Maria Sempreviva, Jean-Paul Calbimonte, Eleni Chatzi, Sarah Barber</li>
<li>for: 本研究旨在帮助风能领域专家将数据转化为域知识，与其他知识源集成，并为下一代人工智能系统提供可用的数据。</li>
<li>methods: 本文使用知识工程来支持风能领域的数字变革，并提出了域知识表示的主要概念。 previous work 在风能领域知识工程和知识表示方面进行了系统性的分析，并提供了适用于域专家的指南。</li>
<li>results: 本文通过系统分析当前风能领域知识工程的状况，并将主要域算法和工具置于风能领域专家需求和问题点上下文中，以帮助读者更好地理解和应用知识工程技术。<details>
<summary>Abstract</summary>
With the rapid evolution of the wind energy sector, there is an ever-increasing need to create value from the vast amounts of data made available both from within the domain, as well as from other sectors. This article addresses the challenges faced by wind energy domain experts in converting data into domain knowledge, connecting and integrating it with other sources of knowledge, and making it available for use in next generation artificially intelligent systems. To this end, this article highlights the role that knowledge engineering can play in the process of digital transformation of the wind energy sector. It presents the main concepts underpinning Knowledge-Based Systems and summarises previous work in the areas of knowledge engineering and knowledge representation in a manner that is relevant and accessible to domain experts. A systematic analysis of the current state-of-the-art on knowledge engineering in the wind energy domain is performed, with available tools put into perspective by establishing the main domain actors and their needs and identifying key problematic areas. Finally, guidelines for further development and improvement are provided.
</details>
<details>
<summary>摘要</summary>
随着风能行业的快速发展，需要从各个领域中提取丰富的数据，并将其与其他领域的知识相连接和融合。这篇文章挑战风能领域专家将数据转化为域知识，并将其与其他知识源融合，以便在下一代人工智能系统中使用。为此，本文强调了知识工程在风能领域的数字转型过程中的重要作用。文章介绍了知识工程的主要概念，并总结了过去在风能领域的知识工程和知识表示方面的工作，以便对风能领域专家有所帮助。本文进行了风能领域知识工程的系统性分析，并将可用工具放在风能领域主要拥有者和他们的需求之前提下进行了比较。文章还标识了主要问题点，以便进一步的发展和改进。最后，文章提供了进一步发展和改进的指南。
</details></li>
</ul>
<hr>
<h2 id="GraphPatcher-Mitigating-Degree-Bias-for-Graph-Neural-Networks-via-Test-time-Augmentation"><a href="#GraphPatcher-Mitigating-Degree-Bias-for-Graph-Neural-Networks-via-Test-time-Augmentation" class="headerlink" title="GraphPatcher: Mitigating Degree Bias for Graph Neural Networks via Test-time Augmentation"></a>GraphPatcher: Mitigating Degree Bias for Graph Neural Networks via Test-time Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00800">http://arxiv.org/abs/2310.00800</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jumxglhf/graphpatcher">https://github.com/jumxglhf/graphpatcher</a></li>
<li>paper_authors: Mingxuan Ju, Tong Zhao, Wenhao Yu, Neil Shah, Yanfang Ye</li>
<li>for: 提高 graph neural network (GNN) 的测试时通用性和低度节点表现。</li>
<li>methods: 提出了一种名为 GraphPatcher 的测试时扩充框架，通过在训练时生成虚拟节点来强化 GNN 的测试时性能。</li>
<li>results: 对七个基准数据集进行了广泛的实验，并 consistently 提高了常见 GNN 的总性能和低度节点表现，相比之前的状态态标准基eline。<details>
<summary>Abstract</summary>
Recent studies have shown that graph neural networks (GNNs) exhibit strong biases towards the node degree: they usually perform satisfactorily on high-degree nodes with rich neighbor information but struggle with low-degree nodes. Existing works tackle this problem by deriving either designated GNN architectures or training strategies specifically for low-degree nodes. Though effective, these approaches unintentionally create an artificial out-of-distribution scenario, where models mainly or even only observe low-degree nodes during the training, leading to a downgraded performance for high-degree nodes that GNNs originally perform well at. In light of this, we propose a test-time augmentation framework, namely GraphPatcher, to enhance test-time generalization of any GNNs on low-degree nodes. Specifically, GraphPatcher iteratively generates virtual nodes to patch artificially created low-degree nodes via corruptions, aiming at progressively reconstructing target GNN's predictions over a sequence of increasingly corrupted nodes. Through this scheme, GraphPatcher not only learns how to enhance low-degree nodes (when the neighborhoods are heavily corrupted) but also preserves the original superior performance of GNNs on high-degree nodes (when lightly corrupted). Additionally, GraphPatcher is model-agnostic and can also mitigate the degree bias for either self-supervised or supervised GNNs. Comprehensive experiments are conducted over seven benchmark datasets and GraphPatcher consistently enhances common GNNs' overall performance by up to 3.6% and low-degree performance by up to 6.5%, significantly outperforming state-of-the-art baselines. The source code is publicly available at https://github.com/jumxglhf/GraphPatcher.
</details>
<details>
<summary>摘要</summary>
近期研究发现，图 neural network (GNN) 具有节点度偏好：它们通常在高度节点上表现良好，但是在低度节点上遇到困难。现有的方法包括设计专门的 GNN 架构或训练策略，以解决这个问题。虽然有效，这些方法会意外创造一种人工的异常情况，导致模型在训练中主要或仅仅观察低度节点，从而导致高度节点的性能下降。为了解决这个问题，我们提出了一个测试时扩展框架，即 GraphPatcher，以提高任何 GNN 的测试时通用性。Specifically, GraphPatcher iteratively generates virtual nodes to patch artificially created low-degree nodes via corruptions, aiming at progressively reconstructing target GNN's predictions over a sequence of increasingly corrupted nodes. Through this scheme, GraphPatcher not only learns how to enhance low-degree nodes (when the neighborhoods are heavily corrupted) but also preserves the original superior performance of GNNs on high-degree nodes (when lightly corrupted). Additionally, GraphPatcher is model-agnostic and can also mitigate the degree bias for either self-supervised or supervised GNNs.我们在七个 benchmark 数据集上进行了广泛的实验，并证明 GraphPatcher 可以一直提高常见 GNN 的总性能和低度节点性能，最高提高3.6%和6.5%。与现有的基elines相比，GraphPatcher 显示出了显著的优势。源代码可以在 GitHub 上下载，请参阅 <https://github.com/jumxglhf/GraphPatcher>。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Review-of-Generative-AI-in-Healthcare"><a href="#A-Comprehensive-Review-of-Generative-AI-in-Healthcare" class="headerlink" title="A Comprehensive Review of Generative AI in Healthcare"></a>A Comprehensive Review of Generative AI in Healthcare</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00795">http://arxiv.org/abs/2310.00795</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yasin Shokrollahi, Sahar Yarmohammadtoosky, Matthew M. Nikahd, Pengfei Dong, Xianqi Li, Linxia Gu</li>
<li>for: 本文主要探讨了生成式人工智能（AI）在医疗领域的应用，尤其是转换器和扩散模型。</li>
<li>methods: 本文使用的方法包括医疗影像分析、预测蛋白结构、临床文档、诊断协助、放射学解读、临床决策支持、医疗代码和财务处理等。</li>
<li>results: 本文总结了各种生成式AI应用在医疗领域的进展，包括医疗影像重建、图像至图像翻译、图像生成和分类、蛋白结构预测、临床诊断和决策支持等，并提出了未来研究的可能性以满足医疗领域的发展需求。<details>
<summary>Abstract</summary>
The advancement of Artificial Intelligence (AI) has catalyzed revolutionary changes across various sectors, notably in healthcare. Among the significant developments in this field are the applications of generative AI models, specifically transformers and diffusion models. These models have played a crucial role in analyzing diverse forms of data, including medical imaging (encompassing image reconstruction, image-to-image translation, image generation, and image classification), protein structure prediction, clinical documentation, diagnostic assistance, radiology interpretation, clinical decision support, medical coding, and billing, as well as drug design and molecular representation. Such applications have enhanced clinical diagnosis, data reconstruction, and drug synthesis. This review paper aims to offer a thorough overview of the generative AI applications in healthcare, focusing on transformers and diffusion models. Additionally, we propose potential directions for future research to tackle the existing limitations and meet the evolving demands of the healthcare sector. Intended to serve as a comprehensive guide for researchers and practitioners interested in the healthcare applications of generative AI, this review provides valuable insights into the current state of the art, challenges faced, and prospective future directions.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）的发展对各个领域产生了革命性的变革，医疗领域是其中之一。在这个领域中，生成式AI模型，特别是转换器和扩散模型，对医疗数据进行分析发挥了重要作用。这些模型可以处理各种不同的数据类型，包括医疗影像重建、图像到图像翻译、图像生成和图像分类、蛋白质结构预测、临床记录、诊断助手、医学影像理解、诊断支持、医疗代码和财务处理等。这些应用程序提高了临床诊断、数据重建和药物合成。本文旨在为医疗领域的研究人员和实践者提供一份全面的综述，探讨生成式AI在医疗领域的应用，特别是转换器和扩散模型。此外，我们还提出了未来研究的可能性，以满足医疗领域的发展需求。这篇文章旨在为医疗领域的研究人员和实践者提供一份价值的指南，帮助他们更好地理解现有技术的状况、挑战和未来发展趋势。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Link-Prediction-A-Data-Perspective"><a href="#Revisiting-Link-Prediction-A-Data-Perspective" class="headerlink" title="Revisiting Link Prediction: A Data Perspective"></a>Revisiting Link Prediction: A Data Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00793">http://arxiv.org/abs/2310.00793</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uisim2020/uisim2020">https://github.com/uisim2020/uisim2020</a></li>
<li>paper_authors: Haitao Mao, Juanhui Li, Harry Shomer, Bingheng Li, Wenqi Fan, Yao Ma, Tong Zhao, Neil Shah, Jiliang Tang</li>
<li>for: 本研究旨在探讨链接预测task在不同领域 dataset 之间的共通原理，以提高链接预测模型的普适性。</li>
<li>methods: 本研究使用了三种关键因素：本地结构靠近性、全局结构靠近性和特征靠近性，以探索链接预测task 的数据中心视角。</li>
<li>results: 研究发现，全局结构靠近性只有在本地结构靠近性不足时才有效。此外，特征靠近性和结构靠近性之间存在冲突，导致 GNN4LP 模型在一些链接上表现不佳。<details>
<summary>Abstract</summary>
Link prediction, a fundamental task on graphs, has proven indispensable in various applications, e.g., friend recommendation, protein analysis, and drug interaction prediction. However, since datasets span a multitude of domains, they could have distinct underlying mechanisms of link formation. Evidence in existing literature underscores the absence of a universally best algorithm suitable for all datasets. In this paper, we endeavor to explore principles of link prediction across diverse datasets from a data-centric perspective. We recognize three fundamental factors critical to link prediction: local structural proximity, global structural proximity, and feature proximity. We then unearth relationships among those factors where (i) global structural proximity only shows effectiveness when local structural proximity is deficient. (ii) The incompatibility can be found between feature and structural proximity. Such incompatibility leads to GNNs for Link Prediction (GNN4LP) consistently underperforming on edges where the feature proximity factor dominates. Inspired by these new insights from a data perspective, we offer practical instruction for GNN4LP model design and guidelines for selecting appropriate benchmark datasets for more comprehensive evaluations.
</details>
<details>
<summary>摘要</summary>
链接预测，一项基本任务在图上，已经在各种应用中证明无可或，例如朋友推荐、蛋白分析和药物交互预测。然而， datasets  span 多个领域，它们可能具有不同的下面机制。文献证明了无一个通用的算法适用于所有 datasets。在这篇文章中，我们尝试通过数据中心的视角来探索链接预测的原则。我们认为链接预测中有三个基本因素是关键的：本地结构靠近性、全局结构靠近性和特征靠近性。然后，我们发现这些因素之间存在关系，包括（i）全局结构靠近性只有当本地结构靠近性不足时才能够有效。（ii）特征和结构靠近性之间存在不兼容性，这导致 GNNs for Link Prediction (GNN4LP) 在特征靠近性因素占主导地位的边上表现不佳。被这些新的数据视角所 inspirited，我们提供了实用的 GNN4LP 模型设计指南和选择合适的 benchmark 数据集的指南，以便更全面的评估。
</details></li>
</ul>
<hr>
<h2 id="Towards-a-Universal-Understanding-of-Color-Harmony-Fuzzy-Approach"><a href="#Towards-a-Universal-Understanding-of-Color-Harmony-Fuzzy-Approach" class="headerlink" title="Towards a Universal Understanding of Color Harmony: Fuzzy Approach"></a>Towards a Universal Understanding of Color Harmony: Fuzzy Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00791">http://arxiv.org/abs/2310.00791</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pakizar Shamoi, Muragul Muratbekova, Assylzhan Izbassar, Atsushi Inoue, Hiroharu Kawanaka</li>
<li>for:  explore color harmony using a fuzzy-based color model and evaluate its universality</li>
<li>methods:  use a dataset of attractive images from five different domains, apply a fuzzy approach to identify harmony patterns and dominant color palettes</li>
<li>results:  color harmony is largely universal, influenced by hue relationships, saturation, and intensity of colors, with prevalent adherence to color wheel principles in palettes with high harmony levels.<details>
<summary>Abstract</summary>
Harmony level prediction is receiving increasing attention nowadays. Color plays a crucial role in affecting human aesthetic responses. In this paper, we explore color harmony using a fuzzy-based color model and address the question of its universality. For our experiments, we utilize a dataset containing attractive images from five different domains: fashion, art, nature, interior design, and brand logos. We aim to identify harmony patterns and dominant color palettes within these images using a fuzzy approach. It is well-suited for this task because it can handle the inherent subjectivity and contextual variability associated with aesthetics and color harmony evaluation. Our experimental results suggest that color harmony is largely universal. Additionally, our findings reveal that color harmony is not solely influenced by hue relationships on the color wheel but also by the saturation and intensity of colors. In palettes with high harmony levels, we observed a prevalent adherence to color wheel principles while maintaining moderate levels of saturation and intensity. These findings contribute to ongoing research on color harmony and its underlying principles, offering valuable insights for designers, artists, and researchers in the field of aesthetics.
</details>
<details>
<summary>摘要</summary>
现在，谐契度预测已经得到了越来越多的关注。颜色在人类美学反应中发挥了关键性的作用。在这篇论文中，我们使用基于朴素集的颜色模型来探讨颜色谐契，并评估其universality。我们使用包含有吸引人的图像的五个领域：时尚、艺术、自然、家居设计和品牌LOGO的 dataset进行实验。我们希望通过朴素方法来确定图像中的谐契模式和主导的颜色alette。这种方法适合这种任务，因为它可以处理美学和颜色谐契评估中的内在主观性和上下文变化。我们的实验结果表明，颜色谐契是大体上的universal。此外，我们发现颜色谐契不仅受到颜色轮的颜色关系的影响，还受到颜色的浓淡和强度的影响。在高谐契水平的颜色alette中，我们发现了较高的颜色轮原则遵循性，同时保持了中等的浓淡和强度。这些发现对美学颜色谐契的研究提供了有价值的见解，对设计师、艺术家和研究人员在美学颜色谐契方面的工作都是有益的。
</details></li>
</ul>
<hr>
<h2 id="BooookScore-A-systematic-exploration-of-book-length-summarization-in-the-era-of-LLMs"><a href="#BooookScore-A-systematic-exploration-of-book-length-summarization-in-the-era-of-LLMs" class="headerlink" title="BooookScore: A systematic exploration of book-length summarization in the era of LLMs"></a>BooookScore: A systematic exploration of book-length summarization in the era of LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00785">http://arxiv.org/abs/2310.00785</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lilakk/booookscore">https://github.com/lilakk/booookscore</a></li>
<li>paper_authors: Yapei Chang, Kyle Lo, Tanya Goyal, Mohit Iyyer</li>
<li>For: This paper focuses on developing a method to evaluate the coherence of book-length summaries generated by large language models (LLMs). The authors aim to address the challenges of evaluating summarization of long documents, which are not well-studied due to the lack of datasets and evaluation methods.* Methods: The authors use two prompting workflows to generate book-length summaries: (1) hierarchically merging chunk-level summaries, and (2) incrementally updating a running summary. They also develop an automatic metric, BooookScore, to measure the coherence of the summaries.* Results: The authors obtain human annotations on GPT-4 generated summaries of 100 recently-published books and identify eight common types of coherence errors made by LLMs. They find that closed-source LLMs such as GPT-4 and Claude 2 produce summaries with higher BooookScore than the oft-repetitive ones generated by LLaMA 2. Incremental updating yields lower BooookScore but higher level of detail than hierarchical merging, a trade-off sometimes preferred by human annotators.Here is the Chinese translation of the three points:* For: 本文旨在开发一种方法来评估大语言模型（LLM）生成的长文摘要的准确性。作者们面临长文摘要评估的挑战，因为现有的数据集和评估方法尚未得到了深入研究。* Methods: 作者们使用两种提示工作流程来生成长文摘要：（1）层次合并 chunk-level 摘要，和（2）逐步更新 Running 摘要。他们还开发了一个自动度量器，叫做 BooookScore，用于衡量摘要的准确性。* Results: 作者们 obt 100 篇最近发表的书籍的 GPT-4 生成的摘要，并将其分为八种常见的准确性错误。他们发现，关闭源 LLM such as GPT-4 和 Claude 2 生成的摘要具有更高的 BooookScore，与 oft-repetitive 的 LLaMA 2 生成的摘要不同。增量更新 yields 较低的 BooookScore，但是具有更高的细节水平。<details>
<summary>Abstract</summary>
Summarizing book-length documents (>100K tokens) that exceed the context window size of large language models (LLMs) requires first breaking the input document into smaller chunks and then prompting an LLM to merge, update, and compress chunk-level summaries. Despite the complexity and importance of this task, it has yet to be meaningfully studied due to the challenges of evaluation: existing book-length summarization datasets (e.g., BookSum) are in the pretraining data of most public LLMs, and existing evaluation methods struggle to capture errors made by modern LLM summarizers. In this paper, we present the first study of the coherence of LLM-based book-length summarizers implemented via two prompting workflows: (1) hierarchically merging chunk-level summaries, and (2) incrementally updating a running summary. We obtain 1193 fine-grained human annotations on GPT-4 generated summaries of 100 recently-published books and identify eight common types of coherence errors made by LLMs. Because human evaluation is expensive and time-consuming, we develop an automatic metric, BooookScore, that measures the proportion of sentences in a summary that do not contain any of the identified error types. BooookScore has high agreement with human annotations and allows us to systematically evaluate the impact of many other critical parameters (e.g., chunk size, base LLM) while saving $15K and 500 hours in human evaluation costs. We find that closed-source LLMs such as GPT-4 and Claude 2 produce summaries with higher BooookScore than the oft-repetitive ones generated by LLaMA 2. Incremental updating yields lower BooookScore but higher level of detail than hierarchical merging, a trade-off sometimes preferred by human annotators. We release code and annotations after blind review to spur more principled research on book-length summarization.
</details>
<details>
<summary>摘要</summary>
大量文档摘要（>100K tokens）需要首先将输入文档分成更小的块，然后使用大型自然语言模型（LLM）来合并、更新和压缩块级摘要。尽管这项任务的复杂性和重要性尚未得到系统的研究，但是现有的书籍摘要 dataset（例如 BookSum）都包含在大多数公共 LLM 的预训练数据中，而现有的评估方法很难 Capture LLM 摘要器中的错误。在这篇论文中，我们提出了首次对 LLM 基于的书籍摘要器的准确性进行了研究。我们使用两种提示工作流程：（1）层次合并块级摘要，和（2）逐步更新RunningSummary。我们获得了1193个精细的人类标注，对 GPT-4 生成的 100 部最新出版的书籍摘要进行了评估，并发现了 eight 种常见的准确性错误。由于人类评估是昂贵和时间consuming的，我们开发了一个自动度量器，BooookScore，可以衡量摘要中含有准确性错误的句子的比例。BooookScore 与人类标注具有高一致性，我们可以通过 sistematic 地评估多个关键参数（例如块大小、基础 LLN）而节省 $15K 和 500 小时的人类评估成本。我们发现closed-source LLMs 如 GPT-4 和 Claude 2 生成的摘要具有更高的 BooookScore，而 LLLaMA 2 的摘要则具有较高的重复性。逐次更新具有较低的 BooookScore，但是具有更高的细节水平，这些trade-off 有时被人类 annotators 首选。我们在审查后发布代码和标注，以促进更理性的研究在书籍摘要领域。
</details></li>
</ul>
<hr>
<h2 id="Mining-Java-Memory-Errors-using-Subjective-Interesting-Subgroups-with-Hierarchical-Targets"><a href="#Mining-Java-Memory-Errors-using-Subjective-Interesting-Subgroups-with-Hierarchical-Targets" class="headerlink" title="Mining Java Memory Errors using Subjective Interesting Subgroups with Hierarchical Targets"></a>Mining Java Memory Errors using Subjective Interesting Subgroups with Hierarchical Targets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00781">http://arxiv.org/abs/2310.00781</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/remilyoucef/sca-miner">https://github.com/remilyoucef/sca-miner</a></li>
<li>paper_authors: Youcef Remil, Anes Bendimerad, Mathieu Chambard, Romain Mathonat, Marc Plantevit, Mehdi Kaytoue</li>
<li>for: 本文主要针对软件应用程序，尤其是企业资源计划（ERP）系统的维护问题。</li>
<li>methods: 本文提出了一种新的子组发现（SD）技术，可以自动 mines incident数据并提取独特的模式，以识别问题的根本原因。</li>
<li>results: 本文通过一个Empirical Study validate了该方法的有效性和Pattern的质量。<details>
<summary>Abstract</summary>
Software applications, especially Enterprise Resource Planning (ERP) systems, are crucial to the day-to-day operations of many industries. Therefore, it is essential to maintain these systems effectively using tools that can identify, diagnose, and mitigate their incidents. One promising data-driven approach is the Subgroup Discovery (SD) technique, a data mining method that can automatically mine incident datasets and extract discriminant patterns to identify the root causes of issues. However, current SD solutions have limitations in handling complex target concepts with multiple attributes organized hierarchically. To illustrate this scenario, we examine the case of Java out-of-memory incidents among several possible applications. We have a dataset that describes these incidents, including their context and the types of Java objects occupying memory when it reaches saturation, with these types arranged hierarchically. This scenario inspires us to propose a novel Subgroup Discovery approach that can handle complex target concepts with hierarchies. To achieve this, we design a pattern syntax and a quality measure that ensure the identified subgroups are relevant, non-redundant, and resilient to noise. To achieve the desired quality measure, we use the Subjective Interestingness model that incorporates prior knowledge about the data and promotes patterns that are both informative and surprising relative to that knowledge. We apply this framework to investigate out-of-memory errors and demonstrate its usefulness in incident diagnosis. To validate the effectiveness of our approach and the quality of the identified patterns, we present an empirical study. The source code and data used in the evaluation are publicly accessible, ensuring transparency and reproducibility.
</details>
<details>
<summary>摘要</summary>
To illustrate this scenario, we examine the case of Java out-of-memory incidents among several possible applications. We have a dataset that describes these incidents, including their context and the types of Java objects occupying memory when it reaches saturation, with these types arranged hierarchically. This scenario inspires us to propose a novel Subgroup Discovery approach that can handle complex target concepts with hierarchies.To achieve this, we design a pattern syntax and a quality measure that ensure the identified subgroups are relevant, non-redundant, and resilient to noise. To achieve the desired quality measure, we use the Subjective Interestingness model that incorporates prior knowledge about the data and promotes patterns that are both informative and surprising relative to that knowledge. We apply this framework to investigate out-of-memory errors and demonstrate its usefulness in incident diagnosis.To validate the effectiveness of our approach and the quality of the identified patterns, we present an empirical study. The source code and data used in the evaluation are publicly accessible, ensuring transparency and reproducibility.硬件应用程序，特别是企业资源规划（ERP）系统，对许多行业的日常运营是关键。因此，保持这些系统的效果是非常重要，使用可以识别、诊断和缓解incident的工具。一种有前途的数据驱动方法是Subgroup Discovery（SD）技术，可以自动挖掘incident数据集并提取描述性模式，以识别问题的根本原因。然而，现有的SD解决方案在处理复杂目标概念中存在限制，这些概念通常具有多个属性，并且归类在层次结构中。为了解释这种情况，我们选择了Java垃圾回收incident作为例子，我们有一个描述这些incident的数据集，包括incident的 контекст和占用内存资源的Java对象类型，这些类型以层次结构组织。这种情况提醒我们提出一种处理复杂目标概念的Subgroup Discovery方法。为了实现这一目标，我们设计了一种模式语法和质量度量，确保提取的子组是有用、非重复、抗噪的。为了实现所需的质量度量，我们使用Subjective Interestingness模型，该模型将数据中的知识纳入考虑，并且提高模式的有用性和surprise性，以便更好地描述数据。我们在调查垃圾回收incident中应用这种框架，并示出其在事件诊断中的有用性。为了证明我们的方法的有效性和模式的质量，我们进行了一个实验研究。研究中使用的源代码和数据都公开 accessible，以确保透明度和可重现性。
</details></li>
</ul>
<hr>
<h2 id="Pre-training-with-Synthetic-Data-Helps-Offline-Reinforcement-Learning"><a href="#Pre-training-with-Synthetic-Data-Helps-Offline-Reinforcement-Learning" class="headerlink" title="Pre-training with Synthetic Data Helps Offline Reinforcement Learning"></a>Pre-training with Synthetic Data Helps Offline Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00771">http://arxiv.org/abs/2310.00771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zecheng Wang, Che Wang, Zixuan Dong, Keith Ross</li>
<li>for: 这个论文主要研究了深度强化学习（DRL）的离线预训练方法，特别是使用大量语言资料来提高下游性能（Reid et al., 2022）。</li>
<li>methods: 本论文使用了几种简单的预训练方案，包括使用生成的IID数据和一步随机链生成的数据，以及使用Q学习算法和多层感知器（MLP）作为后续。</li>
<li>results: 实验结果表明，使用这些简单的预训练方案可以提高DRL的性能，并且可以与使用大量语言资料预训练的性能相比肩。此外，采用这些预训练方案可以提高CQL算法的性能，并且在D4RL Gym游戏数据集上获得了一致的性能提升。<details>
<summary>Abstract</summary>
Recently, it has been shown that for offline deep reinforcement learning (DRL), pre-training Decision Transformer with a large language corpus can improve downstream performance (Reid et al., 2022). A natural question to ask is whether this performance gain can only be achieved with language pre-training, or can be achieved with simpler pre-training schemes which do not involve language. In this paper, we first show that language is not essential for improved performance, and indeed pre-training with synthetic IID data for a small number of updates can match the performance gains from pre-training with a large language corpus; moreover, pre-training with data generated by a one-step Markov chain can further improve the performance. Inspired by these experimental results, we then consider pre-training Conservative Q-Learning (CQL), a popular offline DRL algorithm, which is Q-learning-based and typically employs a Multi-Layer Perceptron (MLP) backbone. Surprisingly, pre-training with simple synthetic data for a small number of updates can also improve CQL, providing consistent performance improvement on D4RL Gym locomotion datasets. The results of this paper not only illustrate the importance of pre-training for offline DRL but also show that the pre-training data can be synthetic and generated with remarkably simple mechanisms.
</details>
<details>
<summary>摘要</summary>
Inspired by these results, we then explore pre-training Conservative Q-Learning (CQL), a popular offline DRL algorithm that uses Q-learning and typically employs a Multi-Layer Perceptron (MLP) backbone. Surprisingly, we find that pre-training with simple synthetic data for a small number of updates can also improve CQL, providing consistent performance improvement on D4RL Gym locomotion datasets.The findings of this paper demonstrate the importance of pre-training for offline DRL and show that the pre-training data can be synthetic and generated with remarkably simple mechanisms. This has significant implications for the development of offline DRL algorithms and highlights the potential for using simple pre-training schemes to improve performance.
</details></li>
</ul>
<hr>
<h2 id="Facilitating-Battery-Swapping-Services-for-Freight-Trucks-with-Spatial-Temporal-Demand-Prediction"><a href="#Facilitating-Battery-Swapping-Services-for-Freight-Trucks-with-Spatial-Temporal-Demand-Prediction" class="headerlink" title="Facilitating Battery Swapping Services for Freight Trucks with Spatial-Temporal Demand Prediction"></a>Facilitating Battery Swapping Services for Freight Trucks with Spatial-Temporal Demand Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04440">http://arxiv.org/abs/2310.04440</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linyu Liu, Zhen Dai, Shiji Song, Xiaocheng Li, Guanting Chen</li>
<li>for: 这篇论文旨在探讨重型卡车电池更换服务的潜力和效率，以实现碳neutral未来。</li>
<li>methods: 论文运用了双重方法，首先预测了运输网络上未来几个小时的交通模式，然后将预测结果引入优化模组，实现电池的有效分配和部署。</li>
<li>results: 分析了2,500英里长的高速公路重型卡车数据，我们发现预测&#x2F;机器学习可以帮助未来的决策。具体来说，我们发现在设置早期的移动电池更换站更有利，但是随着系统的成熟，固定位置的电池更换站更受欢迎。<details>
<summary>Abstract</summary>
Electrifying heavy-duty trucks offers a substantial opportunity to curtail carbon emissions, advancing toward a carbon-neutral future. However, the inherent challenges of limited battery energy and the sheer weight of heavy-duty trucks lead to reduced mileage and prolonged charging durations. Consequently, battery-swapping services emerge as an attractive solution for these trucks. This paper employs a two-fold approach to investigate the potential and enhance the efficacy of such services. Firstly, spatial-temporal demand prediction models are adopted to predict the traffic patterns for the upcoming hours. Subsequently, the prediction guides an optimization module for efficient battery allocation and deployment. Analyzing the heavy-duty truck data on a highway network spanning over 2,500 miles, our model and analysis underscore the value of prediction/machine learning in facilitating future decision-makings. In particular, we find that the initial phase of implementing battery-swapping services favors mobile battery-swapping stations, but as the system matures, fixed-location stations are preferred.
</details>
<details>
<summary>摘要</summary>
电动重型卡车的应用提供了巨大的减少碳排放的机会，推进向碳中和未来。然而，重型卡车的自然限制，如电池能量有限和车辆总重，导致很长的充电时间和减少的行驶距离。因此，电池换卡服务出现了一个有吸引力的解决方案。本文采用两重方法来探讨这种服务的潜在和提高效率。首先，采用空间-时间需求预测模型预测下一个几个小时的交通趋势。然后，预测导引一个优化模块，以便有效地分配和部署电池。分析了2,500英里长的高速公路上的重型卡车数据，我们的模型和分析表明，预测/机器学习在未来决策中发挥了重要作用。尤其是在实施电池换卡服务的初期阶段，移动电池换卡站更有优势；而在系统成熟后，固定位置的电池换卡站变得更加受欢迎。
</details></li>
</ul>
<hr>
<h2 id="Mind-the-Gap-Federated-Learning-Broadens-Domain-Generalization-in-Diagnostic-AI-Models"><a href="#Mind-the-Gap-Federated-Learning-Broadens-Domain-Generalization-in-Diagnostic-AI-Models" class="headerlink" title="Mind the Gap: Federated Learning Broadens Domain Generalization in Diagnostic AI Models"></a>Mind the Gap: Federated Learning Broadens Domain Generalization in Diagnostic AI Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00757">http://arxiv.org/abs/2310.00757</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tayebiarasteh/fldomain">https://github.com/tayebiarasteh/fldomain</a></li>
<li>paper_authors: Soroosh Tayebi Arasteh, Christiane Kuhl, Marwin-Jonathan Saehn, Peter Isfort, Daniel Truhn, Sven Nebelung<br>for: 这项研究旨在评估 Federated Learning（FL）在骨肢X射线图像分类 task 中的影响，特别是训练策略、网络架构和数据多样性等因素对模型的预测性能的影响。methods: 研究使用了610,000个骨肢X射线图像数据集，来评估不同训练策略、网络架构和数据多样性对模型的预测性能。results: 研究发现，虽然大型数据集可能会增加FL的性能，但是在某些情况下，甚至会导致性能下降。相反，小型数据集表现出了明显的改善。因此，本地训练和FL的性能主要受到训练数据大小的影响，而不同数据集之间的多样性则对于Off-domain任务的性能产生了更大的影响。通过合作训练在多个外部机构的数据上，FL可以提高隐私、可重现性和 Off-domain 可靠性，并且可能提高医疗结果。<details>
<summary>Abstract</summary>
Developing robust artificial intelligence (AI) models that generalize well to unseen datasets is challenging and usually requires large and variable datasets, preferably from multiple institutions. In federated learning (FL), a model is trained collaboratively at numerous sites that hold local datasets without exchanging them. So far, the impact of training strategy, i.e., local versus collaborative, on the diagnostic on-domain and off-domain performance of AI models interpreting chest radiographs has not been assessed. Consequently, using 610,000 chest radiographs from five institutions across the globe, we assessed diagnostic performance as a function of training strategy (i.e., local vs. collaborative), network architecture (i.e., convolutional vs. transformer-based), generalization performance (i.e., on-domain vs. off-domain), imaging finding (i.e., cardiomegaly, pleural effusion, pneumonia, atelectasis, consolidation, pneumothorax, and no abnormality), dataset size (i.e., from n=18,000 to 213,921 radiographs), and dataset diversity. Large datasets not only showed minimal performance gains with FL but, in some instances, even exhibited decreases. In contrast, smaller datasets revealed marked improvements. Thus, on-domain performance was mainly driven by training data size. However, off-domain performance leaned more on training diversity. When trained collaboratively across diverse external institutions, AI models consistently surpassed models trained locally for off-domain tasks, emphasizing FL's potential in leveraging data diversity. In conclusion, FL can bolster diagnostic privacy, reproducibility, and off-domain reliability of AI models and, potentially, optimize healthcare outcomes.
</details>
<details>
<summary>摘要</summary>
<<SYS>>发展强健的人工智能（AI）模型，使其在未见数据集上具有良好的泛化性是一项挑战。通常需要大量和多样的数据集，从多个机构获取。在联合学习（FL）中，模型在多个地点进行协同学习，而不需要交换本地数据。 jusqu'à présent，训练策略（本地 versus 协同）对AI模型解剖学影像鉴定性的影响尚未得到评估。为了解决这个问题，我们使用了全球五个机构的610,000张胸部X射影像，评估AI模型的鉴定性以训练策略、网络架构、泛化性、影像发现（cardiomegaly, pleural effusion, pneumonia, atelectasis, consolidation, pneumothorax, 和无异常）、数据集大小（从n=18,000到213,921 radiographs）和数据多样性之间的关系。结果表明，大型数据集不仅在FL中没有获得明显的性能提升，有些情况下甚至显示下降。相反，较小的数据集表现出了明显的改善。因此，本地训练的性能主要受到训练数据集大小的影响，而在不同机构的外部数据集上进行协同训练的性能则更多受到训练多样性的影响。当AI模型在多个外部机构上进行协同训练时，其在外部任务上的性能 consistently 高于本地训练的模型，这将FL的潜在作用在拓展数据多样性方面强调。总之，FL可以提高鉴定隐私、重现性和外部可靠性的AI模型，并且可能会优化医疗结果。
</details></li>
</ul>
<hr>
<h2 id="TIGERScore-Towards-Building-Explainable-Metric-for-All-Text-Generation-Tasks"><a href="#TIGERScore-Towards-Building-Explainable-Metric-for-All-Text-Generation-Tasks" class="headerlink" title="TIGERScore: Towards Building Explainable Metric for All Text Generation Tasks"></a>TIGERScore: Towards Building Explainable Metric for All Text Generation Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00752">http://arxiv.org/abs/2310.00752</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongfu Jiang, Yishan Li, Ge Zhang, Wenhao Huang, Bill Yuchen Lin, Wenhu Chen</li>
<li>for: 本研究开发了一个名为TIGERScore的自动评估指标，用于评估文本生成任务的效果。</li>
<li>methods: TIGERScore使用了专门训练的LLaMA模型，并基于自己调整的MetricInstruct dataset，以提供可读的错误分析，并不需要参考。</li>
<li>results: TIGERScore在5个对接评分数据集上 Achieves the highest overall Spearman’s correlation with human ratings，并且与其他指标相比表现更好，甚至可以超越参考基于的指标。<details>
<summary>Abstract</summary>
We present TIGERScore, a \textbf{T}rained metric that follows \textbf{I}nstruction \textbf{G}uidance to perform \textbf{E}xplainable, and \textbf{R}eference-free evaluation over a wide spectrum of text generation tasks. Different from other automatic evaluation methods that only provide arcane scores, TIGERScore is guided by the natural language instruction to provide error analysis to pinpoint the mistakes in the generated text. Our metric is based on LLaMA, trained on our meticulously curated instruction-tuning dataset MetricInstruct which covers 6 text generation tasks and 23 text generation datasets. The dataset consists of 48K quadruple in the form of (instruction, input, system output $\rightarrow$ error analysis). We collected the `system outputs' through diverse channels to cover different types of errors. To quantitatively assess our metric, we evaluate its correlation with human ratings on 5 held-in datasets, 2 held-out datasets and show that TIGERScore can achieve the highest overall Spearman's correlation with human ratings across these datasets and outperforms other metrics significantly. As a reference-free metric, its correlation can even surpass the best existing reference-based metrics. To further qualitatively assess the rationale generated by our metric, we conduct human evaluation on the generated explanations and found that the explanations are 70.8\% accurate. Through these experimental results, we believe TIGERScore demonstrates the possibility of building universal explainable metrics to evaluate any text generation task.
</details>
<details>
<summary>摘要</summary>
我们介绍TIGERScore，一个已经训练的度量，可以根据自然语言指南进行可解释的、无参考度的文本生成任务评价。与其他自动评价方法不同，TIGERScore不仅提供神秘的分数，还可以通过错误分析来 pinpoint生成文本中的错误。我们的度量基于LLaMA，并在我们精心抽样的指南调度集MetricInstruct上训练。这个集合包括6种文本生成任务和23种文本生成数据集，共48000个四元组（指南、输入、系统输出 → 错误分析）。我们通过多种途径收集了“系统输出”，以覆盖不同类型的错误。为了评估我们的度量，我们对5个保留数据集、2个保 OUT数据集进行了量化评估，并发现TIGERScore可以在这些数据集中 achiev the highest Spearman correlation coefficient with human ratings，并且与其他度量相比显著出perform better。作为一个无参考度量，TIGERScore的相关性可以甚至超过最佳参考基础度量。为了进一步评估我们的度量生成的理由，我们对生成的解释进行了人工评估，并发现解释的准确率为70.8%。通过这些实验结果，我们认为TIGERScore表明了可以建立 universal explainable metrics，用于评价任何文本生成任务。
</details></li>
</ul>
<hr>
<h2 id="NoxTrader-LSTM-Based-Stock-Return-Momentum-Prediction-for-Quantitative-Trading"><a href="#NoxTrader-LSTM-Based-Stock-Return-Momentum-Prediction-for-Quantitative-Trading" class="headerlink" title="NoxTrader: LSTM-Based Stock Return Momentum Prediction for Quantitative Trading"></a>NoxTrader: LSTM-Based Stock Return Momentum Prediction for Quantitative Trading</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00747">http://arxiv.org/abs/2310.00747</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hsiang-Hui Liu, Han-Jay Shu, Wei-Ning Chiu</li>
<li>for: 这个研究主要目的是在股票市场中获得资金收益，尤其是在中期至长期的时间预测。</li>
<li>methods: 这个研究使用时间序列分析来学习股票市场的趋势，并使用价格和股票量数据进行特征工程。他们还使用Long Short-Term Memory（LSTM）模型来捕捉价格趋势，并在交易过程中进行动态模型更新。</li>
<li>results: 这个研究获得了一些优秀的预测数据，其中预测和实际市场资料之间的距离在0.65至0.75之间。他们还使用筛选技术来改善初始投资回报，从-60%提升到325%.<details>
<summary>Abstract</summary>
We introduce NoxTrader, a sophisticated system designed for portfolio construction and trading execution with the primary objective of achieving profitable outcomes in the stock market, specifically aiming to generate moderate to long-term profits. The underlying learning process of NoxTrader is rooted in the assimilation of valuable insights derived from historical trading data, particularly focusing on time-series analysis due to the nature of the dataset employed. In our approach, we utilize price and volume data of US stock market for feature engineering to generate effective features, including Return Momentum, Week Price Momentum, and Month Price Momentum. We choose the Long Short-Term Memory (LSTM)model to capture continuous price trends and implement dynamic model updates during the trading execution process, enabling the model to continuously adapt to the current market trends. Notably, we have developed a comprehensive trading backtesting system - NoxTrader, which allows us to manage portfolios based on predictive scores and utilize custom evaluation metrics to conduct a thorough assessment of our trading performance. Our rigorous feature engineering and careful selection of prediction targets enable us to generate prediction data with an impressive correlation range between 0.65 and 0.75. Finally, we monitor the dispersion of our prediction data and perform a comparative analysis against actual market data. Through the use of filtering techniques, we improved the initial -60% investment return to 325%.
</details>
<details>
<summary>摘要</summary>
我们介绍NoxTrader，一个复杂的系统，用于股票投资组合建立和交易执行，主要目标是在股市中实现可观的收益。我们的学习过程借鉴了历史交易数据中的宝贵经验，特别是时间序列分析，因为我们使用的数据集是时间序列型的。在我们的方法中，我们利用美国股市价格和量数据进行特征工程，生成有效特征，包括回报势力、周期势力和月度势力。我们选择Long Short-Term Memory（LSTM）模型，以捕捉连续价格趋势，并在交易执行过程中进行动态模型更新，使模型能够不断适应当前市场趋势。值得一提的是，我们开发了一套完整的交易回测系统——NoxTrader，它允许我们基于预测得分来管理投资组合，并使用自定义评估 metric来进行严格的评估我们的交易性能。我们的严格的特征工程和预测目标的精心选择，使我们能够生成预测数据的各种相关度范围在0.65-0.75之间。最后，我们监测预测数据的分散情况，并对实际市场数据进行比较分析。通过筛选技术，我们从初始投资回报下降至60%的位置提高至325%。
</details></li>
</ul>
<hr>
<h2 id="RoleLLM-Benchmarking-Eliciting-and-Enhancing-Role-Playing-Abilities-of-Large-Language-Models"><a href="#RoleLLM-Benchmarking-Eliciting-and-Enhancing-Role-Playing-Abilities-of-Large-Language-Models" class="headerlink" title="RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models"></a>RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00746">http://arxiv.org/abs/2310.00746</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/interactivenlp-team/rolellm-public">https://github.com/interactivenlp-team/rolellm-public</a></li>
<li>paper_authors: Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, Zhaoxiang Zhang, Wanli Ouyang, Ke Xu, Wenhu Chen, Jie Fu, Junran Peng</li>
<li>for: 本文旨在提高语言模型（LLM）的角色扮演能力，以增强用户交互。</li>
<li>methods: 本文提出了一个框架，名为RoleLLM，用于评估、引出和提高 LLM 的角色扮演能力。RoleLLM 包括四个阶段：（1）角色资料构建（Role Profile Construction），（2）基于上下文的指令生成（Context-Based Instruction Generation），（3）角色提示（Role Prompting），以及（4）角色定制化指令调整（Role-Conditioned Instruction Tuning）。</li>
<li>results: 通过 Context-Instruct 和 RoleGPT，我们创建了 RoleBench，这是首个系统性的、细致的字级 benchmark 数据集，用于测试角色扮演能力。此外，通过 RoCIT 在 RoleBench 上进行调整，我们获得了 RoleLLaMA（英文）和 RoleGLM（中文），这些模型显著提高了角色扮演能力，甚至与 RoleGPT（使用 GPT-4）具有相同的Results。<details>
<summary>Abstract</summary>
The advent of Large Language Models (LLMs) has paved the way for complex tasks such as role-playing, which enhances user interactions by enabling models to imitate various characters. However, the closed-source nature of state-of-the-art LLMs and their general-purpose training limit role-playing optimization. In this paper, we introduce RoleLLM, a framework to benchmark, elicit, and enhance role-playing abilities in LLMs. RoleLLM comprises four stages: (1) Role Profile Construction for 100 roles; (2) Context-Based Instruction Generation (Context-Instruct) for role-specific knowledge extraction; (3) Role Prompting using GPT (RoleGPT) for speaking style imitation; and (4) Role-Conditioned Instruction Tuning (RoCIT) for fine-tuning open-source models along with role customization. By Context-Instruct and RoleGPT, we create RoleBench, the first systematic and fine-grained character-level benchmark dataset for role-playing with 168,093 samples. Moreover, RoCIT on RoleBench yields RoleLLaMA (English) and RoleGLM (Chinese), significantly enhancing role-playing abilities and even achieving comparable results with RoleGPT (using GPT-4).
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的出现已经为复杂的任务如角色扮演提供了方便，这些任务可以使模型模拟多种角色，从而提高用户交互的体验。然而，现有的State-of-the-art LLMs的关闭源代码和通用训练限制了角色扮演优化。在这篇论文中，我们介绍了RoleLLM框架，用于评价、引导和提高LLMs中的角色扮演能力。RoleLLM包括四个阶段：（1）角色Profile构建100个角色；（2）基于上下文的指令生成（Context-Instruct）用于角色特定知识提取；（3）基于GPT的角色提示（RoleGPT）用于模仿说话风格；以及（4）基于角色的Conditioned Instruction Tuning（RoCIT）用于 fine-tuning开源模型以及角色定制。通过Context-Instruct和RoleGPT，我们创建了RoleBench，第一个系统和细化的字符级 benchmark dataset для角色扮演，包含168,093个样本。此外，在RoleBench上进行RoCIT后，我们获得了RoleLLaMA（英语）和RoleGLM（中文），两个能够明显提高角色扮演能力的模型，甚至与RoleGPT（使用GPT-4）相当。
</details></li>
</ul>
<hr>
<h2 id="My-Machine-and-I-ChatGPT-and-the-Future-of-Human-Machine-Collaboration-in-Africa"><a href="#My-Machine-and-I-ChatGPT-and-the-Future-of-Human-Machine-Collaboration-in-Africa" class="headerlink" title="My Machine and I: ChatGPT and the Future of Human-Machine Collaboration in Africa"></a>My Machine and I: ChatGPT and the Future of Human-Machine Collaboration in Africa</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13704">http://arxiv.org/abs/2310.13704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Munachimso Blessing Oguine, Chidera Godsfavor Oguine, Kanyifeechukwu Jane Oguine</li>
<li>for: 本研究旨在探讨聊天GPT在人机合作中的效果。</li>
<li>methods: 本研究使用反透明主题分析方法对51篇2019-2023年的文章进行分析。</li>
<li>results: 研究发现聊天GPT在学术领域 such as 教育和研究中的人机交互非常普遍，而且聊天GPT在改善人机合作方面的效果较高。<details>
<summary>Abstract</summary>
Recent advancements in technology have necessitated a paradigm shift in the people use technology necessitating a new research field called Human-Machine collaboration. ChatGPT, an Artificial intelligence (AI) assistive technology, has gained mainstream adoption and implementation in academia and industry; however, a lot is left unknown about how this new technology holds for Human-Machine Collaboration in Africa. Our survey paper highlights to answer some of these questions. To understand the effectiveness of ChatGPT on human-machine collaboration we utilized reflexive thematic analysis to analyze (N= 51) articles between 2019 and 2023 obtained from our literature search. Our findings indicate the prevalence of ChatGPT for human-computer interaction within academic sectors such as education, and research; trends also revealed the relatively high effectiveness of ChatGPT in improving human-machine collaboration.
</details>
<details>
<summary>摘要</summary>
最近的技术发展使得人机合作的研究领域得到了推动，这种新的研究领域被称为人机合作。智能人工智能（AI）协助技术ChatGPT在学术和产业界得到了广泛的批处和实施，但是关于这种新技术在非洲的人机合作方面还有很多未知之处。我们的调查论文旨在回答这些问题。为了评估ChatGPT在人机合作效果，我们使用了反思主题分析法分析（N=51）于2019年至2023年之间的文章。我们的发现表明了ChatGPT在教育和研究领域的人机交互非常普遍，并且发现ChatGPT在改善人机合作效果方面的趋势相对较高。
</details></li>
</ul>
<hr>
<h2 id="GenAI-Against-Humanity-Nefarious-Applications-of-Generative-Artificial-Intelligence-and-Large-Language-Models"><a href="#GenAI-Against-Humanity-Nefarious-Applications-of-Generative-Artificial-Intelligence-and-Large-Language-Models" class="headerlink" title="GenAI Against Humanity: Nefarious Applications of Generative Artificial Intelligence and Large Language Models"></a>GenAI Against Humanity: Nefarious Applications of Generative Artificial Intelligence and Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00737">http://arxiv.org/abs/2310.00737</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emilio Ferrara</li>
<li>for: This paper is written to raise awareness about the potential risks and challenges of Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) being misused for nefarious purposes.</li>
<li>methods: The paper uses a combination of research and analysis to identify the potential risks of GenAI and LLMs, including their use in deepfakes, malicious content generation, and the creation of synthetic identities.</li>
<li>results: The paper highlights the potential consequences of GenAI and LLMs being misused, including the blurring of the lines between the virtual and real worlds, the potential for targeted misinformation and scams, and the creation of sophisticated malware. The paper also serves as a call to action to prepare for these potential risks and challenges.In Simplified Chinese text, the three key points would be:</li>
<li>for: 这篇论文是为了提醒大家关于生成人工智能（GenAI）和大语言模型（LLMs）的可能的风险和挑战。</li>
<li>methods: 论文使用了组合的研究和分析来识别GenAI和LLMs的可能的风险，包括它们在深圳中的使用、邪恶内容生成和假造标识等。</li>
<li>results: 论文 highlights GenAI和LLMs的可能的后果，包括虚拟和现实世界之间的边界模糊、targeted的谣言和骗局、以及高级的黑客软件。论文也 serves as a call to action，准备这些可能的风险和挑战。<details>
<summary>Abstract</summary>
Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) are marvels of technology; celebrated for their prowess in natural language processing and multimodal content generation, they promise a transformative future. But as with all powerful tools, they come with their shadows. Picture living in a world where deepfakes are indistinguishable from reality, where synthetic identities orchestrate malicious campaigns, and where targeted misinformation or scams are crafted with unparalleled precision. Welcome to the darker side of GenAI applications. This article is not just a journey through the meanders of potential misuse of GenAI and LLMs, but also a call to recognize the urgency of the challenges ahead. As we navigate the seas of misinformation campaigns, malicious content generation, and the eerie creation of sophisticated malware, we'll uncover the societal implications that ripple through the GenAI revolution we are witnessing. From AI-powered botnets on social media platforms to the unnerving potential of AI to generate fabricated identities, or alibis made of synthetic realities, the stakes have never been higher. The lines between the virtual and the real worlds are blurring, and the consequences of potential GenAI's nefarious applications impact us all. This article serves both as a synthesis of rigorous research presented on the risks of GenAI and misuse of LLMs and as a thought-provoking vision of the different types of harmful GenAI applications we might encounter in the near future, and some ways we can prepare for them.
</details>
<details>
<summary>摘要</summary>
生成人工智能（GenAI）和大型语言模型（LLMs）是技术的宠儿，被庆贤以其在自然语言处理和多模式内容生成的能力。它们承诺一个转型的未来。但就像所有的强大工具一样，它们也有阴影。 imagine living in a world where deepfakes are indistinguishable from reality, where synthetic identities orchestrate malicious campaigns, and where targeted misinformation or scams are crafted with unparalleled precision. Welcome to the darker side of GenAI applications. This article is not just a journey through the meanders of potential misuse of GenAI and LLMs, but also a call to recognize the urgency of the challenges ahead. As we navigate the seas of misinformation campaigns, malicious content generation, and the eerie creation of sophisticated malware, we'll uncover the societal implications that ripple through the GenAI revolution we are witnessing. From AI-powered botnets on social media platforms to the unnerving potential of AI to generate fabricated identities, or alibis made of synthetic realities, the stakes have never been higher. The lines between the virtual and the real worlds are blurring, and the consequences of potential GenAI's nefarious applications impact us all. This article serves both as a synthesis of rigorous research presented on the risks of GenAI and misuse of LLMs and as a thought-provoking vision of the different types of harmful GenAI applications we might encounter in the near future, and some ways we can prepare for them.
</details></li>
</ul>
<hr>
<h2 id="Review-of-deep-learning-in-healthcare"><a href="#Review-of-deep-learning-in-healthcare" class="headerlink" title="Review of deep learning in healthcare"></a>Review of deep learning in healthcare</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00727">http://arxiv.org/abs/2310.00727</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/avadhutsonavane/Diagnosis-of-Coronavirus-using-chest-X-RAY">https://github.com/avadhutsonavane/Diagnosis-of-Coronavirus-using-chest-X-RAY</a></li>
<li>paper_authors: Hasan Hejbari Zargar, Saha Hejbari Zargar, Raziye Mehri</li>
<li>for: 本研究旨在探讨医疗系统中使用深度学习方法，包括最新的网络设计、应用和市场趋势。</li>
<li>methods: 本研究使用深度学习方法，包括深度神经网络模型，以提取医疗数据中隐藏的模式和有价值信息。</li>
<li>results: 研究发现，深度学习方法在医疗系统中可以提取到有价值的信息，但是需要更好地结合人类医疗解释才能实现更高效的应用。<details>
<summary>Abstract</summary>
Given the growing complexity of healthcare data over the last several years, using machine learning techniques like Deep Neural Network (DNN) models has gained increased appeal. In order to extract hidden patterns and other valuable information from the huge quantity of health data, which traditional analytics are unable to do in a reasonable length of time, machine learning (ML) techniques are used. Deep Learning (DL) algorithms in particular have been shown as potential approaches to pattern identification in healthcare systems. This thought has led to the contribution of this research, which examines deep learning methods used in healthcare systems via an examination of cutting-edge network designs, applications, and market trends. To connect deep learning methodologies and human healthcare interpretability, the initial objective is to provide in-depth insight into the deployment of deep learning models in healthcare solutions. And last, to outline the current unresolved issues and potential directions.
</details>
<details>
<summary>摘要</summary>
随着医疗数据的增长复杂性，使用机器学习技术如深度神经网络（DNN）模型已经得到了加大的appeal。为了从庞大量的医疗数据中提取隐藏的模式和其他有价值的信息，传统分析无法在合理的时间内完成，因此机器学习（ML）技术被使用。深度学习（DL）算法在医疗系统中特别有潜力，这也导致了本研究的出发，即通过对当前最新的网络设计、应用和市场趋势进行检验，探讨深度学习在医疗解决方案中的应用。为了将深度学习方法与人类医疗解释相连接，初始的目标是提供深度学习模型在医疗解决方案中的深入分析。最后，总结当前未解决的问题和可能的发展方向。
</details></li>
</ul>
<hr>
<h2 id="Improving-Length-Generalization-in-Transformers-via-Task-Hinting"><a href="#Improving-Length-Generalization-in-Transformers-via-Task-Hinting" class="headerlink" title="Improving Length-Generalization in Transformers via Task Hinting"></a>Improving Length-Generalization in Transformers via Task Hinting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00726">http://arxiv.org/abs/2310.00726</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranjal Awasthi, Anupam Gupta</li>
<li>for: 本研究旨在解决 transformer 模型在某些逻辑和数学任务上长度泛化问题。特别是，一个基于添加的 transformer 模型在应用于更长的实例时表现会下降很快。本研究提出了一种基于任务提示的方法，以解决长度泛化问题。</li>
<li>methods: 本研究使用了多任务训练框架，并在训练过程中同时训练模型解决一个简单且相关的 auxillary 任务。</li>
<li>results: 对于排序问题，我们发现可以使用 sequences 的 length 不超过 20 来训练模型，并在 test 数据上提高了模型的测试准确率从 less than 1% (标准训练) 提高到更多于 92% (via 任务提示)。此外，我们还发现了一些有趣的长度泛化问题的方面，包括不同的 auxillary 任务的效iveness 在提高长度泛化方面有很大差异。<details>
<summary>Abstract</summary>
It has been observed in recent years that transformers have problems with length generalization for certain types of reasoning and arithmetic tasks. In particular, the performance of a transformer model trained on tasks (say addition) up to a certain length (e.g., 5 digit numbers) drops sharply when applied to longer instances of the same problem. This work proposes an approach based on task hinting towards addressing length generalization. Our key idea is that while training the model on task-specific data, it is helpful to simultaneously train the model to solve a simpler but related auxiliary task as well.   We study the classical sorting problem as a canonical example to evaluate our approach. We design a multitask training framework and show that task hinting significantly improve length generalization. For sorting we show that it is possible to train models on data consisting of sequences having length at most $20$, and improve the test accuracy on sequences of length $100$ from less than 1% (for standard training) to more than 92% (via task hinting).   Our study uncovers several interesting aspects of length generalization. We observe that while several auxiliary tasks may seem natural a priori, their effectiveness in improving length generalization differs dramatically. We further use probing and visualization-based techniques to understand the internal mechanisms via which the model performs the task, and propose a theoretical construction consistent with the observed learning behaviors of the model. Based on our construction, we show that introducing a small number of length dependent parameters into the training procedure can further boost the performance on unseen lengths. Finally, we also show the efficacy of our task hinting based approach beyond sorting, giving hope that these techniques will be applicable in broader contexts.
</details>
<details>
<summary>摘要</summary>
近年来，transformer模型在某些逻辑和数学任务中表现出长度泛化问题。具体来说，一个基于添加任务的transformer模型在应用于更长的问题时表现下降。这项工作提出一种基于任务提示的方法来解决长度泛化问题。我们的关键想法是在训练模型时，同时训练模型解决一个相关的简单任务。我们选择排序问题作为一个典型的例子来评估我们的方法。我们设计了一个多任务训练框架，并证明了任务提示可以显著提高长度泛化。对于排序问题，我们可以在数据中包含长度不超过20的序列，并在测试时提高测试 accuracy 从 less than 1% (标准训练) 到更多于92% (via任务提示)。我们的研究揭示了长度泛化的几个有趣方面。我们发现，虽然一些 auxillary task 可能看起来很自然，但它们在提高长度泛化效果上差异很大。我们还使用探测和视觉化技术来理解模型如何完成任务，并提出了一种理论建构，该建构与模型学习行为相符。基于该建构，我们表明在训练过程中引入一小数量的长度参数可以进一步提高对未经见长度的表现。最后，我们还证明了我们的任务提示基本方法在更广泛的上下文中有效。
</details></li>
</ul>
<hr>
<h2 id="Subtractive-Mixture-Models-via-Squaring-Representation-and-Learning"><a href="#Subtractive-Mixture-Models-via-Squaring-Representation-and-Learning" class="headerlink" title="Subtractive Mixture Models via Squaring: Representation and Learning"></a>Subtractive Mixture Models via Squaring: Representation and Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00724">http://arxiv.org/abs/2310.00724</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anon-npc/squared-npcs">https://github.com/anon-npc/squared-npcs</a></li>
<li>paper_authors: Lorenzo Loconte, Aleksanteri M. Sladek, Stefan Mengel, Martin Trapp, Arno Solin, Nicolas Gillis, Antonio Vergari</li>
<li>for: 用于模型复杂的分布</li>
<li>methods: 使用深度减法 mixture 模型</li>
<li>results: 可以提高表达能力，并且在实际分布估计任务中得到良好效果<details>
<summary>Abstract</summary>
Mixture models are traditionally represented and learned by adding several distributions as components. Allowing mixtures to subtract probability mass or density can drastically reduce the number of components needed to model complex distributions. However, learning such subtractive mixtures while ensuring they still encode a non-negative function is challenging. We investigate how to learn and perform inference on deep subtractive mixtures by squaring them. We do this in the framework of probabilistic circuits, which enable us to represent tensorized mixtures and generalize several other subtractive models. We theoretically prove that the class of squared circuits allowing subtractions can be exponentially more expressive than traditional additive mixtures; and, we empirically show this increased expressiveness on a series of real-world distribution estimation tasks.
</details>
<details>
<summary>摘要</summary>
混合模型通常通过添加多个分布来表示和学习。然而，允许混合 subtract 概率质量或密度可以很快减少需要odel复杂分布的组件数量。然而，学习这种 subtractive 混合并确保它们仍然表示非负函数是困难的。我们研究如何在概率Circuits框架下学习和进行推理深 subtractive 混合，并证明在这种框架下，allowing squaring 可以在exponentially more expressive的基础上表示。此外，我们还employs empirical evidence demonstrate this increased expressiveness on a series of real-world distribution estimation tasks。Note: Simplified Chinese is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="A-Simple-Yet-Effective-Strategy-to-Robustify-the-Meta-Learning-Paradigm"><a href="#A-Simple-Yet-Effective-Strategy-to-Robustify-the-Meta-Learning-Paradigm" class="headerlink" title="A Simple Yet Effective Strategy to Robustify the Meta Learning Paradigm"></a>A Simple Yet Effective Strategy to Robustify the Meta Learning Paradigm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00708">http://arxiv.org/abs/2310.00708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qi Wang, Yiqin Lv, Yanghe Feng, Zheng Xie, Jincai Huang</li>
<li>for: 提高 meta 学习的可靠性和鲁棒性，尤其是在风险敏感的情况下。</li>
<li>methods: 基于分布 robust 思想来优化 meta 学习管道，并使用预期尾风险度量进行优化。</li>
<li>results: 实验结果显示，我们的简单方法可以提高 meta 学习对任务分布的Robustness，降低 conditional 预期最坏快速风险的平均值。<details>
<summary>Abstract</summary>
Meta learning is a promising paradigm to enable skill transfer across tasks. Most previous methods employ the empirical risk minimization principle in optimization. However, the resulting worst fast adaptation to a subset of tasks can be catastrophic in risk-sensitive scenarios. To robustify fast adaptation, this paper optimizes meta learning pipelines from a distributionally robust perspective and meta trains models with the measure of expected tail risk. We take the two-stage strategy as heuristics to solve the robust meta learning problem, controlling the worst fast adaptation cases at a certain probabilistic level. Experimental results show that our simple method can improve the robustness of meta learning to task distributions and reduce the conditional expectation of the worst fast adaptation risk.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>基于实际风险最小化原则的现有方法通常采用Meta学习。然而，这可能导致在任务分布下的最坏快adaptation情况，在风险敏感场景下可能是灾难性的。为了强化快adaptation的稳定性，这篇论文从分布 robust perspective来优化Meta学习管道，并使用度量预期的尾风险来训练Meta模型。我们采用两阶段策略来解决Robust Meta学习问题，在某些 probabilistic水平上控制最坏快adaptation的情况。实验结果表明，我们的简单方法可以提高Meta学习的任务分布Robustness和降低最坏快adaptation风险的 conditional expectation。
</details></li>
</ul>
<hr>
<h2 id="Meta-Semantic-Template-for-Evaluation-of-Large-Language-Models"><a href="#Meta-Semantic-Template-for-Evaluation-of-Large-Language-Models" class="headerlink" title="Meta Semantic Template for Evaluation of Large Language Models"></a>Meta Semantic Template for Evaluation of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01448">http://arxiv.org/abs/2310.01448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yachuan Liu, Liang Chen, Jindong Wang, Qiaozhu Mei, Xing Xie</li>
<li>for: 评估大语言模型（LLMs）的 semantics 理解能力，不是仅仅是 memorize 训练数据。</li>
<li>methods: 提出了 MSTemp 方法，通过创建meta semantic templates来评估 LLMs 的 semantics 理解能力。</li>
<li>results: MSTemp 可以生成高度 OUT-OF-DISTRIBUTION（OOD）评估样本，并且可以显著降低 LLMS 使用现有数据集作为种子时的性能。<details>
<summary>Abstract</summary>
Do large language models (LLMs) genuinely understand the semantics of the language, or just memorize the training data? The recent concern on potential data contamination of LLMs has raised awareness of the community to conduct research on LLMs evaluation. In this paper, we propose MSTemp, an approach that creates meta semantic templates to evaluate the semantic understanding ability of LLMs. The core of MSTemp is not to perform evaluation directly on existing benchmark datasets, but to generate new out-of-distribution (OOD) evaluation sets using existing datasets as seeds. Specifically, for a given sentence, MSTemp leverages another language model to generate new samples while preserving its semantics. The new samples are called semantic templates to the original sentence. Then, MSTemp generates evaluation samples via sentence parsing and random word replacement on the semantic templates. MSTemp is highly flexible, dynamic, and cost-effective. Our initial experiments show that MSTemp-generated samples can significantly reduce the performance of LLMs using existing datasets as seeds. We hope this initial work can shed light on future research of LLMs evaluation.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>> LLMs 真的理解语言 semantics 吗，或者只是Memorize 训练数据？在社区关注 LLMS 可能存在数据束缚的问题后，我们提出了一种方法来评估 LLMS。在这篇论文中，我们提出了 MSTemp，它使用现有的语言模型生成新的 OUT-OF-DISTRIBUTION（OOD）评估集。具体来说，对于一个句子，MSTemp 利用另一个语言模型生成新的样本，保持句子的 semantics。然后，MSTemp 使用句子分析和随机词替换来生成评估样本。MSTemp 具有高度的灵活性、动态性和成本效益。我们的初步实验表明，MSTemp 生成的样本可以使 LLMS 使用现有数据集作为种子时表现出显著的下降性能。我们希望这些初步研究可以鼓励未来 LLMS 评估的研究。
</details></li>
</ul>
<hr>
<h2 id="Exchange-means-change-an-unsupervised-single-temporal-change-detection-framework-based-on-intra-and-inter-image-patch-exchange"><a href="#Exchange-means-change-an-unsupervised-single-temporal-change-detection-framework-based-on-intra-and-inter-image-patch-exchange" class="headerlink" title="Exchange means change: an unsupervised single-temporal change detection framework based on intra- and inter-image patch exchange"></a>Exchange means change: an unsupervised single-temporal change detection framework based on intra- and inter-image patch exchange</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00689">http://arxiv.org/abs/2310.00689</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chenhongruixuan/i3pe">https://github.com/chenhongruixuan/i3pe</a></li>
<li>paper_authors: Hongruixuan Chen, Jian Song, Chen Wu, Bo Du, Naoto Yokoya</li>
<li>for: 这个研究旨在提出一个无监控、无标注的单时间变化检测 Framework，以实现单时间 remote sensing 图像上的变化检测。</li>
<li>methods: 这个 Framework 使用了内部和外部图像块交换 (I3PE) 方法，通过交换内部图像块和外部图像块，从单时间图像中生成 pseudo-bi-temporal 图像组和变化标签。</li>
<li>results: 实验结果显示，I3PE 可以超过表现最佳方法的代表无监控方法，实现 F1 值提升约 10.65% 和 6.99%。此外，I3PE 可以在单监控和半监控情况下提高变化检测器的性能。<details>
<summary>Abstract</summary>
Change detection (CD) is a critical task in studying the dynamics of ecosystems and human activities using multi-temporal remote sensing images. While deep learning has shown promising results in CD tasks, it requires a large number of labeled and paired multi-temporal images to achieve high performance. Pairing and annotating large-scale multi-temporal remote sensing images is both expensive and time-consuming. To make deep learning-based CD techniques more practical and cost-effective, we propose an unsupervised single-temporal CD framework based on intra- and inter-image patch exchange (I3PE). The I3PE framework allows for training deep change detectors on unpaired and unlabeled single-temporal remote sensing images that are readily available in real-world applications. The I3PE framework comprises four steps: 1) intra-image patch exchange method is based on an object-based image analysis method and adaptive clustering algorithm, which generates pseudo-bi-temporal image pairs and corresponding change labels from single-temporal images by exchanging patches within the image; 2) inter-image patch exchange method can generate more types of land-cover changes by exchanging patches between images; 3) a simulation pipeline consisting of several image enhancement methods is proposed to simulate the radiometric difference between pre- and post-event images caused by different imaging conditions in real situations; 4) self-supervised learning based on pseudo-labels is applied to further improve the performance of the change detectors in both unsupervised and semi-supervised cases. Extensive experiments on two large-scale datasets demonstrate that I3PE outperforms representative unsupervised approaches and achieves F1 value improvements of 10.65% and 6.99% to the SOTA method. Moreover, I3PE can improve the performance of the ... (see the original article for full abstract)
</details>
<details>
<summary>摘要</summary>
Change detection (CD) 是生态系统和人类活动研究中的关键任务，使用多时间 remote sensing 图像进行研究。深度学习 已经在 CD 任务中表现出色，但它需要大量标注和对应的多时间图像来达到高性能。对于大规模多时间 remote sensing 图像的对应和标注是非常昂贵和时间消耗的。为了使深度学习 基于 CD 技术更实用和成本效果，我们提出了一个无监督单时 CD 框架，基于内部和外部图像块交换（I3PE）。I3PE 框架包括四个步骤：1. 内部图像块交换方法基于 объек 基于 image 分析方法和自适应聚类算法，通过在图像中交换块来生成 pseudo-bi-temporal 图像对和相应的变化标签。2. 外部图像块交换方法可以生成更多的土地覆盖变化类型，通过在图像之间交换块。3. 我们提出了一个模拟管道，包括多种图像提升方法，以模拟在实际情况下的 радиометрические差异。4. 我们采用了自动标注的自我超vised 学习方法，以进一步提高 CD 检测器的性能。我们在两个大规模数据集上进行了广泛的实验，并证明 I3PE 可以在无监督和半监督情况下超越代表性的无监督方法，并在 SOTA 方法上实现 F1 值提升率为 10.65% 和 6.99%。此外，I3PE 还可以提高 CD 检测器的性能。
</details></li>
</ul>
<hr>
<h2 id="The-Robots-are-Here-Navigating-the-Generative-AI-Revolution-in-Computing-Education"><a href="#The-Robots-are-Here-Navigating-the-Generative-AI-Revolution-in-Computing-Education" class="headerlink" title="The Robots are Here: Navigating the Generative AI Revolution in Computing Education"></a>The Robots are Here: Navigating the Generative AI Revolution in Computing Education</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00658">http://arxiv.org/abs/2310.00658</a></li>
<li>repo_url: None</li>
<li>paper_authors: James Prather, Paul Denny, Juho Leinonen, Brett A. Becker, Ibrahim Albluwi, Michelle Craig, Hieke Keuning, Natalie Kiesler, Tobias Kohn, Andrew Luxton-Reilly, Stephen MacNeil, Andrew Peterson, Raymond Pettit, Brent N. Reeves, Jaromir Savelka</li>
<li>for: 这份工作组报告旨在探讨大语言模型（LLMs）在计算教育中的应用和挑战，以及如何适应和利用这些新技术。</li>
<li>methods: 本报告使用Literature Review和论坛调查来探讨LLMs在计算教育中的应用，并从22名计算教育专家的深入采访中收集了实践经验。</li>
<li>results: 本报告的主要结论是：LLMs在计算教育中的应用可以提高学生的学习效果和创新能力，但也存在一些伦理和教学方法的挑战。同时，现有的LLMs在计算教育领域的性能水平在不断提高。<details>
<summary>Abstract</summary>
Recent advancements in artificial intelligence (AI) are fundamentally reshaping computing, with large language models (LLMs) now effectively being able to generate and interpret source code and natural language instructions. These emergent capabilities have sparked urgent questions in the computing education community around how educators should adapt their pedagogy to address the challenges and to leverage the opportunities presented by this new technology. In this working group report, we undertake a comprehensive exploration of LLMs in the context of computing education and make five significant contributions. First, we provide a detailed review of the literature on LLMs in computing education and synthesise findings from 71 primary articles. Second, we report the findings of a survey of computing students and instructors from across 20 countries, capturing prevailing attitudes towards LLMs and their use in computing education contexts. Third, to understand how pedagogy is already changing, we offer insights collected from in-depth interviews with 22 computing educators from five continents who have already adapted their curricula and assessments. Fourth, we use the ACM Code of Ethics to frame a discussion of ethical issues raised by the use of large language models in computing education, and we provide concrete advice for policy makers, educators, and students. Finally, we benchmark the performance of LLMs on various computing education datasets, and highlight the extent to which the capabilities of current models are rapidly improving. Our aim is that this report will serve as a focal point for both researchers and practitioners who are exploring, adapting, using, and evaluating LLMs and LLM-based tools in computing classrooms.
</details>
<details>
<summary>摘要</summary>
Recent advancements in artificial intelligence (AI) are fundamentally reshaping computing, with large language models (LLMs) now effectively being able to generate and interpret source code and natural language instructions. These emergent capabilities have sparked urgent questions in the computing education community around how educators should adapt their pedagogy to address the challenges and to leverage the opportunities presented by this new technology. In this working group report, we undertake a comprehensive exploration of LLMs in the context of computing education and make five significant contributions. First, we provide a detailed review of the literature on LLMs in computing education and synthesize findings from 71 primary articles. Second, we report the findings of a survey of computing students and instructors from across 20 countries, capturing prevailing attitudes towards LLMs and their use in computing education contexts. Third, to understand how pedagogy is already changing, we offer insights collected from in-depth interviews with 22 computing educators from five continents who have already adapted their curricula and assessments. Fourth, we use the ACM Code of Ethics to frame a discussion of ethical issues raised by the use of large language models in computing education, and we provide concrete advice for policy makers, educators, and students. Finally, we benchmark the performance of LLMs on various computing education datasets, and highlight the extent to which the capabilities of current models are rapidly improving. Our aim is that this report will serve as a focal point for both researchers and practitioners who are exploring, adapting, using, and evaluating LLMs and LLM-based tools in computing classrooms.
</details></li>
</ul>
<hr>
<h2 id="LEGO-Prover-Neural-Theorem-Proving-with-Growing-Libraries"><a href="#LEGO-Prover-Neural-Theorem-Proving-with-Growing-Libraries" class="headerlink" title="LEGO-Prover: Neural Theorem Proving with Growing Libraries"></a>LEGO-Prover: Neural Theorem Proving with Growing Libraries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00656">http://arxiv.org/abs/2310.00656</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wiio12/LEGO-Prover">https://github.com/wiio12/LEGO-Prover</a></li>
<li>paper_authors: Haiming Wang, Huajian Xin, Chuanyang Zheng, Lin Li, Zhengying Liu, Qingxing Cao, Yinya Huang, Jing Xiong, Han Shi, Enze Xie, Jian Yin, Zhenguo Li, Heng Liao, Xiaodan Liang</li>
<li>for: This paper aims to improve the ability of large language models (LLMs) to prove mathematical theorems by employing a growing skill library containing verified lemmas as skills.</li>
<li>methods: The proposed method, called LEGO-Prover, constructs the proof modularly and uses existing skills retrieved from the library to augment the capability of LLMs. The skills are further evolved by prompting an LLM to enrich the library on another scale.</li>
<li>results: The proposed method advances the state-of-the-art pass rate on miniF2F-valid and miniF2F-test, and generates over 20,000 skills (theorems&#x2F;lemmas) that are added to the growing library. The ablation study shows that these newly added skills are indeed helpful for proving theorems, resulting in an improvement from a success rate of 47.1% to 50.4%.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文的目的是使大型自然语言模型（LLM）能够更好地证明数学定理，通过使用增长的技能库，这个库包含已验证的证明。</li>
<li>methods: 提议的方法是LEGO-Prover，它将证明构造为模块化的方式，使用已存在的技能库中的技能来增强LLM的能力。这些技能还会在证明过程中进行进一步的演化，以便在另一个尺度上增强库。</li>
<li>results: 提议的方法提高了miniF2F-valid和miniF2F-test的状态前的通过率，并生成了超过20,000个技能（定理&#x2F;证明），这些技能被加入了增长的库中。我们的减少研究表明，这些新增的技能确实对证明定理有帮助，从47.1%提高到50.4%。我们还发布了我们的代码和所有生成的技能。<details>
<summary>Abstract</summary>
Despite the success of large language models (LLMs), the task of theorem proving still remains one of the hardest reasoning tasks that is far from being fully solved. Prior methods using language models have demonstrated promising results, but they still struggle to prove even middle school level theorems. One common limitation of these methods is that they assume a fixed theorem library during the whole theorem proving process. However, as we all know, creating new useful theorems or even new theories is not only helpful but crucial and necessary for advancing mathematics and proving harder and deeper results. In this work, we present LEGO-Prover, which employs a growing skill library containing verified lemmas as skills to augment the capability of LLMs used in theorem proving. By constructing the proof modularly, LEGO-Prover enables LLMs to utilize existing skills retrieved from the library and to create new skills during the proving process. These skills are further evolved (by prompting an LLM) to enrich the library on another scale. Modular and reusable skills are constantly added to the library to enable tackling increasingly intricate mathematical problems. Moreover, the learned library further bridges the gap between human proofs and formal proofs by making it easier to impute missing steps. LEGO-Prover advances the state-of-the-art pass rate on miniF2F-valid (48.0% to 57.0%) and miniF2F-test (45.5% to 47.1%). During the proving process, LEGO-Prover also manages to generate over 20,000 skills (theorems/lemmas) and adds them to the growing library. Our ablation study indicates that these newly added skills are indeed helpful for proving theorems, resulting in an improvement from a success rate of 47.1% to 50.4%. We also release our code and all the generated skills.
</details>
<details>
<summary>摘要</summary>
尽管大型语言模型（LLM）取得了成功，但 theorem proving 仍然是一项非常困难的推理任务，尚未被完全解决。先前的方法使用语言模型已经取得了有望的结果，但它们仍然无法证明中学水平的定理。一个常见的限制是这些方法假设定量定理库在整个定理证明过程中保持不变。然而，我们所知道，创造新有用的定理或新的理论是不仅有帮助作用，而且是必要和必要的，以前进 mathematics 和证明更深入的结果。在这项工作中，我们提出了 LEGO-Prover，它使用增长的技能库，其中包含验证的证明为技能来增强 LLMS 在定理证明中的能力。通过构建证明为模块，LEGO-Prover 让 LLMS 可以在证明过程中使用现有的技能库中的技能，以及在证明过程中创建新的技能。这些技能被进一步演化（通过提示 LLMS），以拓展库的规模。我们不断增加可重用的技能，以便解决越来越复杂的数学问题。此外，学习的库还使得人类证明和正式证明之间的差距变得更小，使得补充缺失的步骤更加容易。LEGO-Prover 提高了 miniF2F-valid 和 miniF2F-test 的通过率（48.0% 到 57.0%）和 miniF2F-test 的成功率（45.5% 到 47.1%）。在证明过程中，LEGO-Prover 还生成了超过 20,000 个技能（定理/证明），并将它们添加到增长的库中。我们的剥离研究表明，这些新增的技能确实对于证明定理有帮助，导致成功率从 47.1% 提高到 50.4%。我们还发布了我们的代码和所有生成的技能。
</details></li>
</ul>
<hr>
<h2 id="Reformulating-Vision-Language-Foundation-Models-and-Datasets-Towards-Universal-Multimodal-Assistants"><a href="#Reformulating-Vision-Language-Foundation-Models-and-Datasets-Towards-Universal-Multimodal-Assistants" class="headerlink" title="Reformulating Vision-Language Foundation Models and Datasets Towards Universal Multimodal Assistants"></a>Reformulating Vision-Language Foundation Models and Datasets Towards Universal Multimodal Assistants</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00653">http://arxiv.org/abs/2310.00653</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thunlp/muffin">https://github.com/thunlp/muffin</a></li>
<li>paper_authors: Tianyu Yu, Jinyi Hu, Yuan Yao, Haoye Zhang, Yue Zhao, Chongyi Wang, Shan Wang, Yinxv Pan, Jiao Xue, Dahai Li, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun</li>
<li>for: 这个论文主要是为了提出一种新的视觉语言模型框架和 Multimodal 指令训练数据集，以提高现有的 Multimodal 语言模型的性能。</li>
<li>methods: 这个论文使用了一种称为 Muffin 的新框架，该框架直接使用预训练的视觉语言模型来连接视觉模块和语言模型，而不需要额外的特征Alignment预训练。此外，该论文还提出了一个名为 UniMM-Chat 的新数据集，该数据集通过将不同任务的数据集融合而成，以生成高质量和多样化的 Multimodal 指令。</li>
<li>results: 实验结果表明，Muffin 框架和 UniMM-Chat 数据集可以提高 Multimodal 语言模型的性能，并且超越了现有的状态机器人模型 like LLaVA 和 InstructBLIP。<details>
<summary>Abstract</summary>
Recent Multimodal Large Language Models (MLLMs) exhibit impressive abilities to perceive images and follow open-ended instructions. The capabilities of MLLMs depend on two crucial factors: the model architecture to facilitate the feature alignment of visual modules and large language models; the multimodal instruction tuning datasets for human instruction following. (i) For the model architecture, most existing models introduce an external bridge module to connect vision encoders with language models, which needs an additional feature-alignment pre-training. In this work, we discover that compact pre-trained vision language models can inherently serve as ``out-of-the-box'' bridges between vision and language. Based on this, we propose Muffin framework, which directly employs pre-trained vision-language models to act as providers of visual signals. (ii) For the multimodal instruction tuning datasets, existing methods omit the complementary relationship between different datasets and simply mix datasets from different tasks. Instead, we propose UniMM-Chat dataset which explores the complementarities of datasets to generate 1.1M high-quality and diverse multimodal instructions. We merge information describing the same image from diverse datasets and transforms it into more knowledge-intensive conversation data. Experimental results demonstrate the effectiveness of the Muffin framework and UniMM-Chat dataset. Muffin achieves state-of-the-art performance on a wide range of vision-language tasks, significantly surpassing state-of-the-art models like LLaVA and InstructBLIP. Our model and dataset are all accessible at https://github.com/thunlp/muffin.
</details>
<details>
<summary>摘要</summary>
最近的多模态大语言模型（MLLM）展现出了惊人的图像识别和开放式指令遵从能力。MLLM的能力取决于两个关键因素：模型架构来实现视觉模块的特征对应，以及多模态指令调整数据集来训练人类指令遵从。在这种情况下，我们发现了一种``出团''的解决方案：使用预训练的视觉语言模型作为视觉信号的提供者。基于这一点，我们提出了甜甜干涯（Muffin）框架，直接employs预训练的视觉语言模型来处理视觉信号。其次，我们发现现有的多模态指令调整数据集忽略了不同任务之间的补偿关系，而是将不同任务的数据集混合在一起。相反，我们提出了UniMM-Chat数据集，它探索不同任务之间的补偿关系，并将这些数据集转化为更加知识充沛的对话数据。实验结果表明甜甜干涯框架和UniMM-Chat数据集的效果。甜甜干涯在各种视觉语言任务上达到了状态之arte的表现，significantly超越了状态之arte的模型 like LLaVA和InstructBLIP。我们的模型和数据集都可以在https://github.com/thunlp/muffin中下载。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Solver-Framework-for-Dynamic-Strategy-Selection-in-Large-Language-Model-Reasoning"><a href="#Adaptive-Solver-Framework-for-Dynamic-Strategy-Selection-in-Large-Language-Model-Reasoning" class="headerlink" title="Adaptive-Solver Framework for Dynamic Strategy Selection in Large Language Model Reasoning"></a>Adaptive-Solver Framework for Dynamic Strategy Selection in Large Language Model Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01446">http://arxiv.org/abs/2310.01446</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianpeng Zhou, Wanjun Zhong, Yanlin Wang, Jiahai Wang</li>
<li>for: 本研究旨在提高大语言模型（LLM）在复杂理解任务中的表现，并适应实际问题的多样性。</li>
<li>methods: 本研究提出了一种适应性解决框架，该框架可以根据问题的复杂性进行灵活的调整。具体来说，该框架包括两个主要模块：初始评估模块和后续适应模块。在后续适应模块中，研究者采用了三种适应策略：（1）模型适应策略：根据问题的复杂性，选择合适的大语言模型；（2）提示方法适应策略：根据问题的特点，选择合适的提示方法；（3）归纳粒度适应策略：根据问题的复杂性，进行细化的问题分解。</li>
<li>results: 实验结果显示，提示方法适应策略和归纳粒度适应策略在所有任务中均提高了表现，而模型适应策略可以减少API成本（最多50%），同时保持高水平的表现。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are showcasing impressive ability in handling complex reasoning tasks. In real-world situations, problems often span a spectrum of complexities. Humans inherently adjust their problem-solving approaches based on task complexity. However, most methodologies that leverage LLMs tend to adopt a uniform approach: utilizing consistent models, prompting methods, and degrees of problem decomposition, regardless of the problem complexity. Inflexibility of them can bring unnecessary computational overhead or sub-optimal performance. To address this problem, we introduce an Adaptive-Solver framework. It strategically modulates solving strategies based on the difficulties of the problems. Given an initial solution, the framework functions with two primary modules. The initial evaluation module assesses the adequacy of the current solution. If improvements are needed, the subsequent adaptation module comes into play. Within this module, three key adaptation strategies are employed: (1) Model Adaptation: Switching to a stronger LLM when a weaker variant is inadequate. (2) Prompting Method Adaptation: Alternating between different prompting techniques to suit the problem's nuances. (3) Decomposition Granularity Adaptation: Breaking down a complex problem into more fine-grained sub-questions to enhance solvability. Through such dynamic adaptations, our framework not only enhances computational efficiency but also elevates the overall performance. This dual-benefit ensures both the efficiency of the system for simpler tasks and the precision required for more complex questions. Experimental results from complex reasoning tasks reveal that the prompting method adaptation and decomposition granularity adaptation enhance performance across all tasks. Furthermore, the model adaptation approach significantly reduces API costs (up to 50%) while maintaining superior performance.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）在复杂逻辑任务中表现出色，但在实际情况中，问题 часто处于复杂性spectrum中。人类自然地根据任务复杂性调整问题解决方法，而多数利用LLMs的方法ologies却采用一致的方法：使用一致的模型、提示方法和问题剖析级别，无论问题复杂性如何。这种不灵活性可能会带来不必要的计算开销或低效性。为解决这个问题，我们介绍了一个适应解决器框架。它在给定的解决方案基础上，策略地调整解决方法，以适应问题的复杂度。解决器框架包括两个主要模块：初始评估模块和后续适应模块。初始评估模块评估当前解决方案的妥当性。如果需要改进，后续适应模块就会起到作用。在这个模块中，我们采用了三种适应策略：1. 模型适应：在弱模型无法解决问题时，切换到更强的LLM。2. 提示方法适应：根据问题的特点，采用不同的提示方法。3.  decompositions Granularity适应：将复杂问题 decompositions into更细grained的子问题，以提高可解性。通过这些动态适应策略，我们的框架不 только提高计算效率，还能够保持高度的表现。这种双重优点确保系统在简单任务上的效率，以及复杂任务上的准确性。实验结果表明，提示方法适应和 decompositions Granularity适应在所有任务上提高表现，而模型适应策略可以减少API成本（最多50%），同时保持高度表现。
</details></li>
</ul>
<hr>
<h2 id="WASA-WAtermark-based-Source-Attribution-for-Large-Language-Model-Generated-Data"><a href="#WASA-WAtermark-based-Source-Attribution-for-Large-Language-Model-Generated-Data" class="headerlink" title="WASA: WAtermark-based Source Attribution for Large Language Model-Generated Data"></a>WASA: WAtermark-based Source Attribution for Large Language Model-Generated Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00646">http://arxiv.org/abs/2310.00646</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingtan Wang, Xinyang Lu, Zitong Zhao, Zhongxiang Dai, Chuan-Sheng Foo, See-Kiong Ng, Bryan Kian Hsiang Low</li>
<li>for: 这个论文是为了解决大语言模型（LLM）训练数据的知识产权问题而写的。</li>
<li>methods: 这个论文使用了水印技术（watermarking）来解决知识产权问题。 specifically, it proposes a WAtermarking for Source Attribution (WASA) framework that enables an LLM to generate synthetic texts with embedded watermarks that contain information about their source(s).</li>
<li>results: 该论文通过实验证明，使用WASA框架可以实现有效的源归属和数据来源验证。<details>
<summary>Abstract</summary>
The impressive performances of large language models (LLMs) and their immense potential for commercialization have given rise to serious concerns over the intellectual property (IP) of their training data. In particular, the synthetic texts generated by LLMs may infringe the IP of the data being used to train the LLMs. To this end, it is imperative to be able to (a) identify the data provider who contributed to the generation of a synthetic text by an LLM (source attribution) and (b) verify whether the text data from a data provider has been used to train an LLM (data provenance). In this paper, we show that both problems can be solved by watermarking, i.e., by enabling an LLM to generate synthetic texts with embedded watermarks that contain information about their source(s). We identify the key properties of such watermarking frameworks (e.g., source attribution accuracy, robustness against adversaries), and propose a WAtermarking for Source Attribution (WASA) framework that satisfies these key properties due to our algorithmic designs. Our WASA framework enables an LLM to learn an accurate mapping from the texts of different data providers to their corresponding unique watermarks, which sets the foundation for effective source attribution (and hence data provenance). Extensive empirical evaluations show that our WASA framework achieves effective source attribution and data provenance.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的吸引人表现和其商业化潜力已经引起了训练数据知识产权（IP）的严重担忧。具体来说， LLM 生成的 sintetic 文本可能会侵犯训练数据的 IP。因此，必须能够（a）确定 LLM 生成 sintetic 文本中的数据提供者（源归属），以及（b）验证数据提供者的文本数据是否被用来训练 LLM。在这篇论文中，我们表明了这两个问题可以通过水印来解决，即使 LLM 生成 sintetic 文本时包含水印，其中包含了文本的来源信息。我们标识了水印框架的关键属性（如源归属精度和对抗攻击者的Robustness），并提出了一个基于 WASA 框架的水印方法，该方法满足这些关键属性。我们的 WASA 框架使得 LLM 可以学习不同数据提供者的文本和它们对应的唯一水印之间的准确映射，这为有效的源归属（以及数据来源）提供了基础。我们的 empirical 评估表明，我们的 WASA 框架可以实现有效的源归属和数据来源识别。
</details></li>
</ul>
<hr>
<h2 id="From-Bandits-Model-to-Deep-Deterministic-Policy-Gradient-Reinforcement-Learning-with-Contextual-Information"><a href="#From-Bandits-Model-to-Deep-Deterministic-Policy-Gradient-Reinforcement-Learning-with-Contextual-Information" class="headerlink" title="From Bandits Model to Deep Deterministic Policy Gradient, Reinforcement Learning with Contextual Information"></a>From Bandits Model to Deep Deterministic Policy Gradient, Reinforcement Learning with Contextual Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00642">http://arxiv.org/abs/2310.00642</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhendong Shi, Xiaoli Wei, Ercan E. Kuruoglu</li>
<li>for: 本研究旨在解决Sequential process中的各种复杂环境下的投资决策问题，使用了两种方法来增强遗传学习的性能：contextual Thompson sampling和 reinforcement learning under supervision。</li>
<li>methods: 本研究使用了遗传学习和CPPI（常数比例资产保险），并将其与DDPG（深度决定策函数优化）相结合，以加速遗传学习的迭代过程，寻找最佳策略。</li>
<li>results: 实验结果显示，使用了上述两种方法可以加速遗传学习的迭代过程，并且可以快速获得最佳策略。<details>
<summary>Abstract</summary>
The problem of how to take the right actions to make profits in sequential process continues to be difficult due to the quick dynamics and a significant amount of uncertainty in many application scenarios. In such complicated environments, reinforcement learning (RL), a reward-oriented strategy for optimum control, has emerged as a potential technique to address this strategic decision-making issue. However, reinforcement learning also has some shortcomings that make it unsuitable for solving many financial problems, excessive resource consumption, and inability to quickly obtain optimal solutions, making it unsuitable for quantitative trading markets. In this study, we use two methods to overcome the issue with contextual information: contextual Thompson sampling and reinforcement learning under supervision which can accelerate the iterations in search of the best answer. In order to investigate strategic trading in quantitative markets, we merged the earlier financial trading strategy known as constant proportion portfolio insurance (CPPI) into deep deterministic policy gradient (DDPG). The experimental results show that both methods can accelerate the progress of reinforcement learning to obtain the optimal solution.
</details>
<details>
<summary>摘要</summary>
“对于续行过程中获利的选择问题，由于动态变化快速且在许多应用场景中存在许多不确定性，这个问题仍然具有困难。在这些复杂的环境中，奖励学习（RL），一种奖励控制的整合策略，已经被视为一种解决此策略决策问题的技术。然而，奖励学习也有一些缺陷，使其不适合解决许多金融问题，包括资源耗用量过大和寻找最佳解答的速度太慢，这使其不适合量化交易市场。在这篇研究中，我们使用了两种方法来突破问题，即在上下文信息中进行奖励探索和奖励学习的监督。为了研究量化交易的战略问题，我们将以前的金融交易策略known as constant proportion portfolio insurance (CPPI)与深度决定性策略gradient (DDPG) 混合。实验结果显示，这两种方法可以加速奖励学习的进程，以取得最佳解答。”Note: The translation is done using Google Translate and may not be perfect. Please note that the translation is done in a simplified Chinese, if you need a traditional Chinese translation, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Engineering-using-Large-Language-Models"><a href="#Knowledge-Engineering-using-Large-Language-Models" class="headerlink" title="Knowledge Engineering using Large Language Models"></a>Knowledge Engineering using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00637">http://arxiv.org/abs/2310.00637</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Bradley P. Allen, Lise Stork, Paul Groth</li>
<li>for: 这篇论文旨在探讨大语言模型在知识工程中的潜在作用，以及如何将其与传统的符号知识系统融合。</li>
<li>methods: 该论文提出了两个中心方向：1）创建混合神经符号知识系统；2）在自然语言中进行知识工程。</li>
<li>results: 该论文提出了一些关键的未解决问题，以便进一步探讨这两个方向。<details>
<summary>Abstract</summary>
Knowledge engineering is a discipline that focuses on the creation and maintenance of processes that generate and apply knowledge. Traditionally, knowledge engineering approaches have focused on knowledge expressed in formal languages. The emergence of large language models and their capabilities to effectively work with natural language, in its broadest sense, raises questions about the foundations and practice of knowledge engineering. Here, we outline the potential role of LLMs in knowledge engineering, identifying two central directions: 1) creating hybrid neuro-symbolic knowledge systems; and 2) enabling knowledge engineering in natural language. Additionally, we formulate key open research questions to tackle these directions.
</details>
<details>
<summary>摘要</summary>
知识工程是一个领域，它关注创建和维护生成和应用知识的过程。传统上，知识工程方法都是关注正式语言表达的知识。然而，大型自然语言模型的出现和它们可以有效地与自然语言进行交互，使得知识工程的基础和实践面临到了新的问题。以下是我们对大型自然语言模型在知识工程中的潜在作用的描述，以及两个中心方向：1. 创建混合神经符号知识系统；2. 实现自然语言知识工程。此外，我们还提出了关键的开放研究问题，以便解决这两个方向。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-Robustness-and-Safety-of-2D-and-3D-Deep-Learning-Models-Against-Adversarial-Attacks"><a href="#A-Survey-of-Robustness-and-Safety-of-2D-and-3D-Deep-Learning-Models-Against-Adversarial-Attacks" class="headerlink" title="A Survey of Robustness and Safety of 2D and 3D Deep Learning Models Against Adversarial Attacks"></a>A Survey of Robustness and Safety of 2D and 3D Deep Learning Models Against Adversarial Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00633">http://arxiv.org/abs/2310.00633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanjie Li, Bin Xie, Songtao Guo, Yuanyuan Yang, Bin Xiao</li>
<li>for: 本研究旨在提高深度学习模型的可靠性和安全性，对抗训练时的敏感攻击和应用场景中的物理攻击。</li>
<li>methods: 本文首先构建了不同角度的威胁模型，然后对最新的2D和3D敏感攻击进行了全面的文献综述。同时，本文还扩展了敏感示例的概念，涵盖了不同类型的攻击方法。</li>
<li>results: 本文系统性地Investigated 3D模型对各种敏感攻击的 robustness，并发现了许多现有的攻击方法。此外，本文还发现了物理攻击可能导致安全风险的问题。最后，本文Summarize 现有的主流话题，预测未来研究的挑战和方向，以帮助建立可靠的AI系统。<details>
<summary>Abstract</summary>
Benefiting from the rapid development of deep learning, 2D and 3D computer vision applications are deployed in many safe-critical systems, such as autopilot and identity authentication. However, deep learning models are not trustworthy enough because of their limited robustness against adversarial attacks. The physically realizable adversarial attacks further pose fatal threats to the application and human safety. Lots of papers have emerged to investigate the robustness and safety of deep learning models against adversarial attacks. To lead to trustworthy AI, we first construct a general threat model from different perspectives and then comprehensively review the latest progress of both 2D and 3D adversarial attacks. We extend the concept of adversarial examples beyond imperceptive perturbations and collate over 170 papers to give an overview of deep learning model robustness against various adversarial attacks. To the best of our knowledge, we are the first to systematically investigate adversarial attacks for 3D models, a flourishing field applied to many real-world applications. In addition, we examine physical adversarial attacks that lead to safety violations. Last but not least, we summarize present popular topics, give insights on challenges, and shed light on future research on trustworthy AI.
</details>
<details>
<summary>摘要</summary>
利用深度学习快速发展，2D和3D计算机视觉应用在许多安全关键系统中部署，如自动驾驶和身份验证。然而，深度学习模型没有够的可靠性，因为它们对骚动攻击有限制的Robustness。物理可行的骚动攻击更加 pose 致命的威胁，对应用和人类安全构成了 fatal 威胁。许多论文已经出现，以 investigate 深度学习模型对骚动攻击的Robustness和安全性。为了带来可靠的 AI，我们首先从不同的角度构建一个通用威胁模型，然后对最新的2D和3D骚动攻击进行全面的回顾。我们将 adversarial 例外扩展到不可见的扰动，并将超过 170 篇论文综述深度学习模型对不同骚动攻击的Robustness。我们认为是首次系统地调查3D模型对骚动攻击的Robustness，这是应用于许多实际应用的蓬勃领域。此外，我们还检查了物理骚动攻击导致的安全违反。最后，我们 summarize 当前流行的话题，提供挑战的视角，并照明未来可靠 AI 的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Intelligent-Client-Selection-for-Federated-Learning-using-Cellular-Automata"><a href="#Intelligent-Client-Selection-for-Federated-Learning-using-Cellular-Automata" class="headerlink" title="Intelligent Client Selection for Federated Learning using Cellular Automata"></a>Intelligent Client Selection for Federated Learning using Cellular Automata</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00627">http://arxiv.org/abs/2310.00627</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nikopavl4/ca_client_selection">https://github.com/nikopavl4/ca_client_selection</a></li>
<li>paper_authors: Nikolaos Pavlidis, Vasileios Perifanis, Theodoros Panagiotis Chatzinikolaou, Georgios Ch. Sirakoulis, Pavlos S. Efraimidis</li>
<li>for: 这个研究旨在提出一个基于自动化机器学习的联盟学习（Federated Learning）客户端选择算法，以提高隐私保护和减少延迟，并且能够适应实际应用中的快速变化环境。</li>
<li>methods: 本研究提出了一个基于细胞自动机（Cellular Automata）的客户端选择算法（CA-CS），它考虑了参与客户端的 Computational Resources 和通信能力，并且考虑了客户端之间的互动，以选择最适合的客户端进行联盟学习过程。</li>
<li>results: 根据实验结果显示，CA-CS 可以与随机选择方法相比，具有与随机选择方法相似的准确性，而且可以快速避免高延迟的客户端。<details>
<summary>Abstract</summary>
Federated Learning (FL) has emerged as a promising solution for privacy-enhancement and latency minimization in various real-world applications, such as transportation, communications, and healthcare. FL endeavors to bring Machine Learning (ML) down to the edge by harnessing data from million of devices and IoT sensors, thus enabling rapid responses to dynamic environments and yielding highly personalized results. However, the increased amount of sensors across diverse applications poses challenges in terms of communication and resource allocation, hindering the participation of all devices in the federated process and prompting the need for effective FL client selection. To address this issue, we propose Cellular Automaton-based Client Selection (CA-CS), a novel client selection algorithm, which leverages Cellular Automata (CA) as models to effectively capture spatio-temporal changes in a fast-evolving environment. CA-CS considers the computational resources and communication capacity of each participating client, while also accounting for inter-client interactions between neighbors during the client selection process, enabling intelligent client selection for online FL processes on data streams that closely resemble real-world scenarios. In this paper, we present a thorough evaluation of the proposed CA-CS algorithm using MNIST and CIFAR-10 datasets, while making a direct comparison against a uniformly random client selection scheme. Our results demonstrate that CA-CS achieves comparable accuracy to the random selection approach, while effectively avoiding high-latency clients.
</details>
<details>
<summary>摘要</summary>
通用学习（FL）已经出现为保护隐私和减少延迟的有力解决方案，在交通、通信和医疗等实际应用中得到广泛应用。FL目的是将机器学习（ML）带到边缘，通过收集数百万个设备和物联网感知器的数据，以实现快速应对动态环境和提供高度个性化结果。然而，在多个应用中的多种感知器上增加了通信和资源分配的挑战，这会阻碍所有设备参与联邦过程，并提高效果的联邦学习客户端选择的需求。为解决这个问题，我们提出了基于Cellular Automata（CA）的客户端选择算法（CA-CS），利用CA模型来有效地捕捉快速发展环境中的空间-时间变化。CA-CS考虑每个参与联邦学习的客户端的计算资源和通信能力，同时也考虑客户端之间的互动，以实现在线联邦学习过程中智能客户端选择。在这篇论文中，我们对提出的CA-CS算法进行了住ehour评估，使用MNIST和CIFAR-10数据集，并对Random client selection scheme进行了直接比较。我们的结果表明，CA-CS可以与随机选择方案具有相同的准确率，同时有效地避免高延迟客户端。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Adaptation-with-Hypernetworks-for-Few-shot-Molecular-Property-Prediction"><a href="#Hierarchical-Adaptation-with-Hypernetworks-for-Few-shot-Molecular-Property-Prediction" class="headerlink" title="Hierarchical Adaptation with Hypernetworks for Few-shot Molecular Property Prediction"></a>Hierarchical Adaptation with Hypernetworks for Few-shot Molecular Property Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00614">http://arxiv.org/abs/2310.00614</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiguang Wu, Yaqing Wang, Quanming Yao</li>
<li>for: 这篇论文的目的是提出一种基于卷积神经网络的层次适应机制，以解决生物医学应用中的分类问题。</li>
<li>methods: 该论文提出了一种基于卷积神经网络的层次适应机制，包括在编码器中选择性地适应参数，以及在预测器中对分子的适应进行层次适应。</li>
<li>results: 该论文的实验结果显示，基于层次适应机制的方法可以在几拟shot学习问题中取得state-of-the-art的性能。<details>
<summary>Abstract</summary>
Molecular property prediction (MPP) is important in biomedical applications, which naturally suffers from a lack of labels, thus forming a few-shot learning problem. State-of-the-art approaches are usually based on gradient-based meta learning strategy, which ignore difference in model parameter and molecule's learning difficulty. To address above problems, we propose a novel hierarchical adaptation mechanism for few-shot MPP (HiMPP). The model follows a encoder-predictor framework. First, to make molecular representation property-adaptive, we selectively adapt encoder's parameter by designing a hypernetwork to modulate node embeddings during message propagation. Next, we make molecule-level adaptation by design another hypernetwork, which assigns larger propagating steps for harder molecules in predictor. In this way, molecular representation is transformed by HiMPP hierarchically from property-level to molecular level. Extensive results show that HiMPP obtains the state-of-the-art performance in few-shot MPP problems, and our proposed hierarchical adaptation mechanism is rational and effective.
</details>
<details>
<summary>摘要</summary>
молекулярная свойство предсказание（MPP）在生物医学应用中具有重要意义，但受到标签缺乏的限制，形成了几个shot学习问题。现状的方法通常基于梯度based meta学习策略，忽略了模型参数和分子学习难度之间的差异。为解决上述问题，我们提出了一种新的层次适应机制 для几个shot MPP（HiMPP）。模型采用encoder-predictor框架，首先使分子表示性能adaptive，通过设计一个权重网络来修饰节点嵌入的消息传播过程中的模型参数。然后，我们又使用另一个权重网络，将更难的分子 assign 更大的传播步长，从而使分子表示被HiMPP层次适应。这种方法使得分子表示被HiMPP层次适应，从属性层次适应到分子层次适应。我们的实验结果表明，HiMPP在几个shot MPP问题中获得了状态计算机科学中的最佳性能，而我们提出的层次适应机制是合理和有效的。
</details></li>
</ul>
<hr>
<h2 id="Understanding-AI-Cognition-A-Neural-Module-for-Inference-Inspired-by-Human-Memory-Mechanisms"><a href="#Understanding-AI-Cognition-A-Neural-Module-for-Inference-Inspired-by-Human-Memory-Mechanisms" class="headerlink" title="Understanding AI Cognition: A Neural Module for Inference Inspired by Human Memory Mechanisms"></a>Understanding AI Cognition: A Neural Module for Inference Inspired by Human Memory Mechanisms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09297">http://arxiv.org/abs/2310.09297</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zengxyyu/A-neural-module-for-inference-inspired-by-human-memory-mechanisms">https://github.com/zengxyyu/A-neural-module-for-inference-inspired-by-human-memory-mechanisms</a></li>
<li>paper_authors: Xiangyu Zeng, Jie Lin, Piao Hu, Ruizheng Huang, Zhicheng Zhang</li>
<li>for: The paper aims to improve the ability of machines to make sense of current inputs and retain information for relation reasoning and question-answering by proposing a PMI framework inspired by human brain’s memory system and cognitive architectures.</li>
<li>methods: The PMI framework consists of perception, memory, and inference components, with a differentiable competitive write access, working memory, and long-term memory with a higher-order structure. The framework also uses outer product associations to merge working memory with long-term memory and retrieve relevant information from two separate memory origins for associative integration.</li>
<li>results: The paper exploratively applies the PMI framework to improve prevailing Transformers and CNN models on question-answering tasks like bAbI-20k and Sort-of-CLEVR datasets, as well as relation calculation and image classification tasks, and in each case, the PMI enhancements consistently outshine their original counterparts significantly. Visualization analyses reveal that memory consolidation and the interaction and integration of information from diverse memory sources substantially contribute to the model effectiveness on inference tasks.Here’s the format you requested:</li>
<li>for: 论文目标是提高机器对当前输入的理解和保留信息以便关系逻辑和问答。</li>
<li>methods: PMI框架包括感知、记忆和推理组件，具有可 differentiable 竞争写访问，工作记忆和长期记忆，其中长期记忆具有更高级结构以保留更多的积累知识和经验。outer product associations 将工作记忆与长期记忆 merge，并在两个不同的记忆来源之间进行相关的集成。</li>
<li>results: 论文应用 PMI 框架进行 prevailing Transformers 和 CNN 模型的改进，包括 bAbI-20k 和 Sort-of-CLEVR 数据集上的问答任务，以及关系计算和图像分类任务，并在每一个任务上，PMI 改进均以显著的程度超越原始模型。视觉分析表明，记忆整合和多种记忆来源之间的交互和集成对推理任务的效果具有重要作用。<details>
<summary>Abstract</summary>
How humans and machines make sense of current inputs for relation reasoning and question-answering while putting the perceived information into context of our past memories, has been a challenging conundrum in cognitive science and artificial intelligence. Inspired by human brain's memory system and cognitive architectures, we propose a PMI framework that consists of perception, memory and inference components. Notably, the memory module comprises working and long-term memory, with the latter endowed with a higher-order structure to retain more accumulated knowledge and experiences. Through a differentiable competitive write access, current perceptions update working memory, which is later merged with long-term memory via outer product associations, averting memory overflow and minimizing information conflicts. In the inference module, relevant information is retrieved from two separate memory origins and associatively integrated to attain a more comprehensive and precise interpretation of current perceptions. We exploratively apply our PMI to improve prevailing Transformers and CNN models on question-answering tasks like bAbI-20k and Sort-of-CLEVR datasets, as well as relation calculation and image classification tasks, and in each case, our PMI enhancements consistently outshine their original counterparts significantly. Visualization analyses reveal that memory consolidation, along with the interaction and integration of information from diverse memory sources, substantially contributes to the model effectiveness on inference tasks.
</details>
<details>
<summary>摘要</summary>
人们和机器如何对当前输入进行关系理解和问答，将感知信息置入我们过去经验的 контекст，是认知科学和人工智能领域的挑战。我们提出了一个PMI框架，包括感知、记忆和推理组件。特别是记忆模块包括工作记忆和长期记忆，其中后者具有更高级别结构，以保留更多的总知识和经验。通过可 diferenciable 竞争写访问，当前感知更新工作记忆，并 eventually 与长期记忆通过外产品关联相结合，避免记忆溢出和信息冲突。在推理模块中，来自不同记忆来源的相关信息被asso ciatively 集成，以实现更全面和准确的当前感知解释。我们考虑应用PMI来改进现有的Transformers和CNN模型，在问答任务和关系计算任务上，以及图像分类任务上，并在每个任务上，我们的PMI改进都能够显著超越原始模型。视觉分析表明，记忆凝固以及不同记忆来源之间的交互和集成，对推理任务的效果具有重要作用。
</details></li>
</ul>
<hr>
<h2 id="Adapting-LLM-Agents-Through-Communication"><a href="#Adapting-LLM-Agents-Through-Communication" class="headerlink" title="Adapting LLM Agents Through Communication"></a>Adapting LLM Agents Through Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01444">http://arxiv.org/abs/2310.01444</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kuan Wang, Yadong Lu, Michael Santacroce, Yeyun Gong, Chao Zhang, Yelong Shen</li>
<li>for: 这个论文旨在提出一种名为“学习通信”（LTC）的训练方法，帮助大型自然语言模型（LLM） agents 在不需要广泛人类指导下，适应新任务。</li>
<li>methods: 该方法基于 iterative exploration 和 PPO 训练，使得 LLM agents 可以通过与环境和其他代理交互，不断提高自己的能力。</li>
<li>results: 在 ALFWorld、HotpotQA 和 GSM8k 三个数据集上，LTC 方法比基eline 高出 12%、5.1% 和 3.6%  respectively，这些结果表明 LTC 方法在多种领域中具有广泛的应用前景。<details>
<summary>Abstract</summary>
Recent advancements in large language models (LLMs) have shown potential for human-like agents. To help these agents adapt to new tasks without extensive human supervision, we propose the Learning through Communication (LTC) paradigm, a novel training approach enabling LLM agents to improve continuously through interactions with their environments and other agents. Recent advancements in large language models (LLMs) have shown potential for human-like agents. To help these agents adapt to new tasks without extensive human supervision, we propose the Learning through Communication (LTC) paradigm, a novel training approach enabling LLM agents to improve continuously through interactions with their environments and other agents. Through iterative exploration and PPO training, LTC empowers the agent to assimilate short-term experiences into long-term memory. To optimize agent interactions for task-specific learning, we introduce three structured communication patterns: Monologue, Dialogue, and Analogue-tailored for common tasks such as decision-making, knowledge-intensive reasoning, and numerical reasoning. We evaluated LTC on three datasets: ALFWorld (decision-making), HotpotQA (knowledge-intensive reasoning), and GSM8k (numerical reasoning). On ALFWorld, it exceeds the instruction tuning baseline by 12% in success rate. On HotpotQA, LTC surpasses the instruction-tuned LLaMA-7B agent by 5.1% in EM score, and it outperforms the instruction-tuned 9x larger PaLM-62B agent by 0.6%. On GSM8k, LTC outperforms the CoT-Tuning baseline by 3.6% in accuracy. The results showcase the versatility and efficiency of the LTC approach across diverse domains. We will open-source our code to promote further development of the community.
</details>
<details>
<summary>摘要</summary>
最近的大语言模型（LLM）的进步已经表现出人类样式的代理。为了帮助这些代理适应新任务而不需极大的人类指导，我们提议了学习通信（LTC）方法，这是一种新的训练方法，可以让 LLM 代理通过与环境和其他代理的交互来不断改进。通过迭代探索和 PPO 训练，LTC 让代理可以将短期经验转化为长期记忆。为了优化代理之间的交互以掌握任务特定的学习，我们引入了三种结构化的通信模式：假言、对话和数据辅助，这些模式特化于常见的决策、知识激发和数学计算等任务。我们在 ALFWorld、HotpotQA 和 GSM8k 三个 dataset 上评估了 LTC，结果显示 LTC 在Success rate、EM 分数和准确率等方面都表现出优异。这些结果表明 LTC 方法在多种领域中具有广泛的应用前景和高效性。我们将在未来开源代码，以便更多的社区成员参与发展。
</details></li>
</ul>
<hr>
<h2 id="Faithful-Explanations-of-Black-box-NLP-Models-Using-LLM-generated-Counterfactuals"><a href="#Faithful-Explanations-of-Black-box-NLP-Models-Using-LLM-generated-Counterfactuals" class="headerlink" title="Faithful Explanations of Black-box NLP Models Using LLM-generated Counterfactuals"></a>Faithful Explanations of Black-box NLP Models Using LLM-generated Counterfactuals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00603">http://arxiv.org/abs/2310.00603</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yair Gat, Nitay Calderon, Amir Feder, Alexander Chapanin, Amit Sharma, Roi Reichart</li>
<li>for: 提高NLプロセッサの安全性和信任性を确保するための说明の强化</li>
<li>methods: 2种のアプローチを提案します：1つはCF生成アプローチで、具体的なテキスト概念を変更することでCFを生成する方法です。2つ目はマッチングアプローチで、トレーニング时にLLMを使用して特别な拟似的空间を学习する方法です。</li>
<li>results: 実験结果では、CF生成アプローチが非常に效果的ですが、検证时间が高くなるDrawbackがあります。一方、マッチングアプローチは、テスト时间の资源を削减した上で效果的な说明を提供することができます。また、Top-K技术を适用することで、すべてのテストされた方法を超える说明を提供することができます。<details>
<summary>Abstract</summary>
Causal explanations of the predictions of NLP systems are essential to ensure safety and establish trust. Yet, existing methods often fall short of explaining model predictions effectively or efficiently and are often model-specific. In this paper, we address model-agnostic explanations, proposing two approaches for counterfactual (CF) approximation. The first approach is CF generation, where a large language model (LLM) is prompted to change a specific text concept while keeping confounding concepts unchanged. While this approach is demonstrated to be very effective, applying LLM at inference-time is costly. We hence present a second approach based on matching, and propose a method that is guided by an LLM at training-time and learns a dedicated embedding space. This space is faithful to a given causal graph and effectively serves to identify matches that approximate CFs. After showing theoretically that approximating CFs is required in order to construct faithful explanations, we benchmark our approaches and explain several models, including LLMs with billions of parameters. Our empirical results demonstrate the excellent performance of CF generation models as model-agnostic explainers. Moreover, our matching approach, which requires far less test-time resources, also provides effective explanations, surpassing many baselines. We also find that Top-K techniques universally improve every tested method. Finally, we showcase the potential of LLMs in constructing new benchmarks for model explanation and subsequently validate our conclusions. Our work illuminates new pathways for efficient and accurate approaches to interpreting NLP systems.
</details>
<details>
<summary>摘要</summary>
natura  causa 解释 预测 是非常重要的，以确保安全性和建立信任。然而，现有的方法经常无法有效地解释模型预测或有效地适用于不同模型。在这篇论文中，我们提出了两种方法来实现模型无关的解释。首先，我们提出了一种基于生成的方法，使用大型自然语言模型（LLM）在预测时对特定文本概念进行修改，保持干扰因素不变。虽然这种方法具有很高的效果，但是在执行时需要费力。我们因此提出了第二种方法，基于匹配的方法，并提出了一种受 LLM 培训时引导的方法，学习专门的嵌入空间。这个空间忠实于给定的 causal 图，并能够准确地标识符合 CF 的匹配。我们理论上证明，以 Approximating CF 为前提，才能建立 faithful 的解释。我们对我们的方法进行了比较，并将其应用于多个模型，包括具有数百亿参数的 LLM。我们的实验结果表明，CF 生成模型在无关模型中具有非常高的表现，并且我们的匹配方法，需要较少的测试资源，也提供了有效的解释。此外，我们发现 Top-K 技术在所有测试方法中都有优化效果。最后，我们展示了 LLB 的潜在在建立新的解释指标和验证我们的结论。我们的工作揭示了新的高效和准确的方法，用于解释 NLP 系统。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Computational-and-Modeling-Foundation-for-Automatic-Coherence-Assessment"><a href="#A-Novel-Computational-and-Modeling-Foundation-for-Automatic-Coherence-Assessment" class="headerlink" title="A Novel Computational and Modeling Foundation for Automatic Coherence Assessment"></a>A Novel Computational and Modeling Foundation for Automatic Coherence Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00598">http://arxiv.org/abs/2310.00598</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aviya Maimon, Reut Tsarfaty</li>
<li>for: 这篇论文主要针对了自然语言处理（NLP）中的 coherence 评估问题，即文本听起来有意义和连贯的问题。</li>
<li>methods: 该论文提出了一种基于 формаль语言定义的 coherence 评估方法，包括三个条件：cohesion、consistency 和 relevance。这些条件被формализова为不同的计算任务，并假设一个涵盖所有任务的模型会学习出 coherence 评估所需的特征。</li>
<li>results: 在两个人类评分的 benchmark 上进行了实验，结果表明，对于每个任务和总的 coherence 评估来说，使用 joint 模型比使用单个任务模型更好。这些结果表明，该方法可以提供一个强大的基础 для大规模自动 coherence 评估。<details>
<summary>Abstract</summary>
Coherence is an essential property of well-written texts, that refers to the way textual units relate to one another. In the era of generative AI, coherence assessment is essential for many NLP tasks; summarization, generation, long-form question-answering, and more. However, in NLP {coherence} is an ill-defined notion, not having a formal definition or evaluation metrics, that would allow for large-scale automatic and systematic coherence assessment. To bridge this gap, in this work we employ the formal linguistic definition of \citet{Reinhart:1980} of what makes a discourse coherent, consisting of three conditions -- {\em cohesion, consistency} and {\em relevance} -- and formalize these conditions as respective computational tasks. We hypothesize that (i) a model trained on all of these tasks will learn the features required for coherence detection, and that (ii) a joint model for all tasks will exceed the performance of models trained on each task individually. On two benchmarks for coherence scoring rated by humans, one containing 500 automatically-generated short stories and another containing 4k real-world texts, our experiments confirm that jointly training on the proposed tasks leads to better performance on each task compared with task-specific models, and to better performance on assessing coherence overall, compared with strong baselines. We conclude that the formal and computational setup of coherence as proposed here provides a solid foundation for advanced methods of large-scale automatic assessment of coherence.
</details>
<details>
<summary>摘要</summary>
“一致性”是文本写作中非常重要的特性，指的是文本单位之间的关联方式。在生成AI时代，一致性评估成为许多自然语言处理（NLP）任务的重要组成部分，包括概要、生成、长文问答等。但在NLP中，“一致性”是一个不具体定义或评估指标的概念，无法进行大规模自动化和系统化的评估。为了bridging这个差距，在这个工作中，我们运用了实际语言学定义（Reinhart, 1980）所定义的一致性条件，包括“结合”、“一致”和“相关”三个条件，并将这些条件ormal化为各自的计算任务。我们假设（i）一个对所有这些任务进行训练的模型将学习出一致性检测所需的特征，并且（ii）将所有这些任务联合训练的模型会比单独训练的模型表现更好。在人类评分的两个库中，一个包含500个自动生成的短篇故事，另一个包含4000个真实世界文本，我们的实验显示，将所有这些任务联合训练的模型比单独训练的模型表现更好，并且在评估一致性方面表现更好，比单独使用强大的基准模型。我们 conclude that这种以形式和计算为基础的一致性设置提供了一个坚实的基础 для进一步的大规模自动一致性评估。
</details></li>
</ul>
<hr>
<h2 id="Quantum-generative-adversarial-learning-in-photonics"><a href="#Quantum-generative-adversarial-learning-in-photonics" class="headerlink" title="Quantum generative adversarial learning in photonics"></a>Quantum generative adversarial learning in photonics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00585">http://arxiv.org/abs/2310.00585</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yizhi Wang, Shichuan Xue, Yaxuan Wang, Yong Liu, Jiangfang Ding, Weixu Shi, Dongyang Wang, Yingwen Liu, Xiang Fu, Guangyao Huang, Anqi Huang, Mingtang Deng, Junjie Wu</li>
<li>for: 本研究旨在调查 Whether Quantum Generative Adversarial Networks (QGANs) can perform learning tasks on near-term quantum devices usually affected by noise and even defects.</li>
<li>methods: 我们使用了一个可编程的硅量子光学芯片，实验了 QGAN 模型在光学领域中，并研究了噪声和缺陷对其性能的影响。</li>
<li>results: 我们的结果表明，即使Generator的相位调制器中有一半被损坏，或Generator和Discriminator的相位调制器都受到相位噪声达0.04π，QGANs仍然可以生成高质量量子数据，其准确率高于90%。<details>
<summary>Abstract</summary>
Quantum Generative Adversarial Networks (QGANs), an intersection of quantum computing and machine learning, have attracted widespread attention due to their potential advantages over classical analogs. However, in the current era of Noisy Intermediate-Scale Quantum (NISQ) computing, it is essential to investigate whether QGANs can perform learning tasks on near-term quantum devices usually affected by noise and even defects. In this Letter, using a programmable silicon quantum photonic chip, we experimentally demonstrate the QGAN model in photonics for the first time, and investigate the effects of noise and defects on its performance. Our results show that QGANs can generate high-quality quantum data with a fidelity higher than 90\%, even under conditions where up to half of the generator's phase shifters are damaged, or all of the generator and discriminator's phase shifters are subjected to phase noise up to 0.04$\pi$. Our work sheds light on the feasibility of implementing QGANs on NISQ-era quantum hardware.
</details>
<details>
<summary>摘要</summary>
量子生成对抗网络（QGAN），量子计算和机器学习的交叉点，在当今中等规模量子计算（NISQ）时代受到广泛关注，因为它们可能比类比的古典模型具有优势。然而，在NISQ时代的近期量子设备上进行学习任务，受到噪声和瑕疵的影响是必须考虑的。在这封信中，我们使用可编程的硅量子光学芯片实验ally QGAN模型在光学中，并研究噪声和瑕疵对其性能的影响。我们的结果表明，QGAN可以生成高质量量子数据，其准确率高于90%， même under conditions where up to half of the generator's phase shifters are damaged, or all of the generator and discriminator's phase shifters are subjected to phase noise up to 0.04π。我们的工作照明了在NISQ时代量子硬件上实现QGAN的可能性。
</details></li>
</ul>
<hr>
<h2 id="CityFM-City-Foundation-Models-to-Solve-Urban-Challenges"><a href="#CityFM-City-Foundation-Models-to-Solve-Urban-Challenges" class="headerlink" title="CityFM: City Foundation Models to Solve Urban Challenges"></a>CityFM: City Foundation Models to Solve Urban Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00583">http://arxiv.org/abs/2310.00583</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pasquale Balsebre, Weiming Huang, Gao Cong, Yi Li</li>
<li>for: 本研究旨在开发一种基于自适应学习的城市基础模型（CityFM），以便在选定的地理区域内（如城市）进行自动化学习。</li>
<li>methods: CityFM 基于开源地理数据（如 OpenStreetMap）进行自我超vision，通过对不同类型实体（如路径、建筑物、区域）的多模式信息进行拟合，生成高质量的基础表示。</li>
<li>results: 对于路、建筑物和区域等下游任务，CityFM 的表示能够超过或与特定应用程序的基elines匹配。<details>
<summary>Abstract</summary>
Pre-trained Foundation Models (PFMs) have ushered in a paradigm-shift in Artificial Intelligence, due to their ability to learn general-purpose representations that can be readily employed in a wide range of downstream tasks. While PFMs have been successfully adopted in various fields such as Natural Language Processing and Computer Vision, their capacity in handling geospatial data and answering urban questions remains limited. This can be attributed to the intrinsic heterogeneity of geospatial data, which encompasses different data types, including points, segments and regions, as well as multiple information modalities, such as a spatial position, visual characteristics and textual annotations. The proliferation of Volunteered Geographic Information initiatives, and the ever-increasing availability of open geospatial data sources, like OpenStreetMap, which is freely accessible globally, unveil a promising opportunity to bridge this gap. In this paper, we present CityFM, a self-supervised framework to train a foundation model within a selected geographical area of interest, such as a city. CityFM relies solely on open data from OSM, and produces multimodal representations of entities of different types, incorporating spatial, visual, and textual information. We analyse the entity representations generated using our foundation models from a qualitative perspective, and conduct quantitative experiments on road, building, and region-level downstream tasks. We compare its results to algorithms tailored specifically for the respective applications. In all the experiments, CityFM achieves performance superior to, or on par with, the baselines.
</details>
<details>
<summary>摘要</summary>
干支基模型（PFM）已经引入了人工智能中的一个新模式，因为它们可以学习通用表示，可以在多种下游任务中使用。 although PFMs have been successfully applied in various fields such as natural language processing and computer vision, their ability to handle geospatial data and answer urban questions is still limited. This is because geospatial data is inherently heterogeneous, including different data types such as points, segments, and regions, as well as multiple information modalities such as spatial position, visual characteristics, and textual annotations. With the proliferation of Volunteered Geographic Information initiatives and the increasing availability of open geospatial data sources like OpenStreetMap, which is freely accessible globally, there is a promising opportunity to bridge this gap.在本文中，我们提出了CityFM，一种自我超vised框架，用于在选择的地理区域内（如城市）训练基本模型。 CityFM仅使用OpenStreetMap开源数据，生成多模式表示实体不同类型，包括空间、视觉和文本信息。我们从质量角度分析基本模型生成的实体表示，并对路、建筑物和区域级下游任务进行量测试。我们与专门为这些应用程序开发的算法进行比较。在所有实验中，CityFM的性能都高于或与基eline相当。
</details></li>
</ul>
<hr>
<h2 id="Pink-Unveiling-the-Power-of-Referential-Comprehension-for-Multi-modal-LLMs"><a href="#Pink-Unveiling-the-Power-of-Referential-Comprehension-for-Multi-modal-LLMs" class="headerlink" title="Pink: Unveiling the Power of Referential Comprehension for Multi-modal LLMs"></a>Pink: Unveiling the Power of Referential Comprehension for Multi-modal LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00582">http://arxiv.org/abs/2310.00582</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sy-xuan/pink">https://github.com/sy-xuan/pink</a></li>
<li>paper_authors: Shiyu Xuan, Qingpei Guo, Ming Yang, Shiliang Zhang<br>For:This paper aims to enhance the Referential Comprehension (RC) ability of Multi-modal Large Language Models (MLLMs) for fine-grained perception tasks.Methods:The proposed method represents the referring object in the image using the coordinates of its bounding box and converts the coordinates into texts in a specific format, allowing the model to treat the coordinates as natural language. The model is trained end-to-end with a parameter-efficient tuning framework that allows both modalities to benefit from multi-modal instruction tuning.Results:The proposed method demonstrates superior performance on conventional vision-language and RC tasks, achieving a 12.0% absolute accuracy improvement over Instruct-BLIP on VSR and surpassing Kosmos-2 by 24.7% on RefCOCO_val under zero-shot settings. The model also attains the top position on the leaderboard of MMBench.<details>
<summary>Abstract</summary>
Multi-modal Large Language Models (MLLMs) have shown remarkable capabilities in many vision-language tasks. Nevertheless, most MLLMs still lack the Referential Comprehension (RC) ability to identify a specific object or area in images, limiting their application in fine-grained perception tasks. This paper proposes a novel method to enhance the RC capability for MLLMs. Our model represents the referring object in the image using the coordinates of its bounding box and converts the coordinates into texts in a specific format. This allows the model to treat the coordinates as natural language. Moreover, we construct the instruction tuning dataset with various designed RC tasks at a low cost by unleashing the potential of annotations in existing datasets. To further boost the RC ability of the model, we propose a self-consistent bootstrapping method that extends dense object annotations of a dataset into high-quality referring-expression-bounding-box pairs. The model is trained end-to-end with a parameter-efficient tuning framework that allows both modalities to benefit from multi-modal instruction tuning. This framework requires fewer trainable parameters and less training data. Experimental results on conventional vision-language and RC tasks demonstrate the superior performance of our method. For instance, our model exhibits a 12.0% absolute accuracy improvement over Instruct-BLIP on VSR and surpasses Kosmos-2 by 24.7% on RefCOCO_val under zero-shot settings. We also attain the top position on the leaderboard of MMBench. The models, datasets, and codes are publicly available at https://github.com/SY-Xuan/Pink
</details>
<details>
<summary>摘要</summary>
多modal大语言模型（MLLM）已经表现出了很好的能力在视觉语言任务中。然而，大多数MLLM仍然缺乏指向某个特定 объек或区域在图像中的能力，限制了它们在细化感知任务中的应用。这篇论文提出了一种新的方法来增强MLLM的指向能力。我们的模型使用图像中引用对象的矩形框坐标来表示引用对象，并将坐标转换成特定格式的文本。这 позвоils 模型对坐标视为自然语言。此外，我们构建了一个指令调整数据集，包括了多种设计的指令调整任务，并且可以在低成本下实现。为了进一步提高模型的指向能力，我们提出了一种自适应增强方法，该方法可以将 dense object 注解 extend 到高质量的引用表示矩形框对。模型通过一个简单的参数效率的调参框架进行全局调参，这使得两种模式都可以从多模态指令调整中受益。实验结果表明，我们的方法可以在 convential 视觉语言任务和指向任务中表现出较好的性能。例如，我们的模型在 VSR 任务上比 Instruct-BLIP 提高 12.0% 绝对准确率，并在 RefCOCO_val 任务上比 Kosmos-2 提高 24.7% 绝对准确率，这些结果均在零shot设置下获得。此外，我们的模型在 MMBench 领导板块上位居榜首。模型、数据集和代码都可以在 https://github.com/SY-Xuan/Pink 上获取。
</details></li>
</ul>
<hr>
<h2 id="Consistency-Trajectory-Models-Learning-Probability-Flow-ODE-Trajectory-of-Diffusion"><a href="#Consistency-Trajectory-Models-Learning-Probability-Flow-ODE-Trajectory-of-Diffusion" class="headerlink" title="Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion"></a>Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02279">http://arxiv.org/abs/2310.02279</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sony/ctm">https://github.com/sony/ctm</a></li>
<li>paper_authors: Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, Stefano Ermon</li>
<li>for: 加速扩散模型采样，提高扩散模型的性能。</li>
<li>methods: 提议一种新的兼容性轨迹模型（CTM），可以在单个前进 pass中输出分数（即极化流动方程中的导数），并允许在扩散过程中任意时刻进行交互。</li>
<li>results: CTM在CIFAR-10和ImageNet的64x64分辨率上达到了新的州际级FID值（FID 1.73和FID 2.06），并可以在计算预算增加时，不断提高样本质量，避免了CM中的降低。<details>
<summary>Abstract</summary>
Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion model sampling at the cost of sample quality but lack a natural way to trade-off quality for speed. To address this limitation, we propose Consistency Trajectory Model (CTM), a generalization encompassing CM and score-based models as special cases. CTM trains a single neural network that can -- in a single forward pass -- output scores (i.e., gradients of log-density) and enables unrestricted traversal between any initial and final time along the Probability Flow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables the efficient combination of adversarial training and denoising score matching loss to enhance performance and achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at 64X64 resolution (FID 2.06). CTM also enables a new family of sampling schemes, both deterministic and stochastic, involving long jumps along the ODE solution trajectories. It consistently improves sample quality as computational budgets increase, avoiding the degradation seen in CM. Furthermore, CTM's access to the score accommodates all diffusion model inference techniques, including exact likelihood computation.
</details>
<details>
<summary>摘要</summary>
协调模型（CM）（Song et al., 2023）可以加速基于分数的扩散模型抽象，但是会增加样本质量的成本。为了解决这个限制，我们提出了一种新的模型——一致轨迹模型（CTM）。CTM可以在单个前进 pass中输出分数（即极化流速度的导数），并且允许在扩散过程中任意时刻之间进行不受限制的游走。此外，CTM还可以通过 combining adversarial training和杂噪分数匹配损失来提高性能，并实现了单步扩散模型抽象中的新的州态-of-the-art FID 值（FID 1.73）和 ImageNet 的 64x64 分辨率上的 FID 值（FID 2.06）。此外，CTM还可以实现一种新的抽象方式，包括 deterministic 和 stochastic 的长距离跳跃。在计算预算增加时，CTM可以逐步提高样本质量，而不是如CM所见的协调模型。此外，CTM可以访问分数，因此可以应用于所有扩散模型的推理技术，包括准确的概率计算。
</details></li>
</ul>
<hr>
<h2 id="LaPLACE-Probabilistic-Local-Model-Agnostic-Causal-Explanations"><a href="#LaPLACE-Probabilistic-Local-Model-Agnostic-Causal-Explanations" class="headerlink" title="LaPLACE: Probabilistic Local Model-Agnostic Causal Explanations"></a>LaPLACE: Probabilistic Local Model-Agnostic Causal Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00570">http://arxiv.org/abs/2310.00570</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/simon-tan/laplace">https://github.com/simon-tan/laplace</a></li>
<li>paper_authors: Sein Minn</li>
<li>for: The paper aims to provide probabilistic cause-and-effect explanations for any classifier operating on tabular data, in a human-understandable manner.</li>
<li>methods: The LaPLACE-Explainer component leverages the concept of a Markov blanket to establish statistical boundaries between relevant and non-relevant features automatically, and incorporates conditional probabilities to offer probabilistic causal explanations.</li>
<li>results: The approach outperforms LIME and SHAP in terms of local accuracy and consistency of explained features, and is validated across various classification models through experiments with both simulated and real-world datasets. The explanations provided by LaPLACE can address trust-related issues such as evaluating prediction reliability, facilitating model selection, enhancing trustworthiness, and identifying fairness-related concerns within classifiers.Here is the information in Simplified Chinese text:</li>
<li>for: 本文目的是提供任何类别器操作于表格数据上的可能性 causa causal 解释，以人类可理解的方式。</li>
<li>methods: LaPLACE-Explainer 组件利用 Markov 围栏的概念，自动地建立表格数据上相关和非相关特征的统计边界，并通过 conditional probabilities 提供可能性解释。</li>
<li>results: LaPLACE 的方法比 LIME 和 SHAP 在本地准确率和解释特征的一致性方面表现出色，并通过多种分类模型的实验，在 simulate 和实际数据集上进行了验证。 LaPLACE 的解释可以解决一些信任问题，如评估预测可靠性、促进模型选择、增强可靠性和检测 fairness 相关问题在类别器中。<details>
<summary>Abstract</summary>
Machine learning models have undeniably achieved impressive performance across a range of applications. However, their often perceived black-box nature, and lack of transparency in decision-making, have raised concerns about understanding their predictions. To tackle this challenge, researchers have developed methods to provide explanations for machine learning models. In this paper, we introduce LaPLACE-explainer, designed to provide probabilistic cause-and-effect explanations for any classifier operating on tabular data, in a human-understandable manner. The LaPLACE-Explainer component leverages the concept of a Markov blanket to establish statistical boundaries between relevant and non-relevant features automatically. This approach results in the automatic generation of optimal feature subsets, serving as explanations for predictions. Importantly, this eliminates the need to predetermine a fixed number N of top features as explanations, enhancing the flexibility and adaptability of our methodology. Through the incorporation of conditional probabilities, our approach offers probabilistic causal explanations and outperforms LIME and SHAP (well-known model-agnostic explainers) in terms of local accuracy and consistency of explained features. LaPLACE's soundness, consistency, local accuracy, and adaptability are rigorously validated across various classification models. Furthermore, we demonstrate the practical utility of these explanations via experiments with both simulated and real-world datasets. This encompasses addressing trust-related issues, such as evaluating prediction reliability, facilitating model selection, enhancing trustworthiness, and identifying fairness-related concerns within classifiers.
</details>
<details>
<summary>摘要</summary>
机器学习模型在多种应用场景中表现出色，但它们的很多时候被视为黑盒模型，无法准确地描述它们的预测结果。为解决这个问题，研究人员开发了一些方法来提供机器学习模型的解释。本文介绍了LaPLACE-explainer，可以为任何基于表格数据的分类器提供 probabilistic cause-and-effect 的解释，并且在人类可以理解的方式下进行解释。LaPLACE-Explainer 组件利用 Markov blanket 的概念，自动地确定相关和无关的特征。这种方法可以自动生成最佳的特征子集，作为预测的解释。这种方法不需要手动决定固定的特征数 N 作为解释，从而提高了方法的灵活性和适应性。通过 incorporating  conditional probabilities，我们的方法可以提供 probabilistic causal 的解释，并且在本地准确性和解释特征的一致性方面超过 LIME 和 SHAP（已知的模型无关解释器）。LaPLACE 的准确性、一致性、本地准确性和适应性被严格验证了多种分类模型。此外，我们通过对 simulated 和实际数据进行实验，证明了这些解释的实际用途。这包括评估预测可靠性、促进模型选择、增强可靠性和识别分类器中的公平问题。
</details></li>
</ul>
<hr>
<h2 id="Quantum-Based-Feature-Selection-for-Multi-classification-Problem-in-Complex-Systems-with-Edge-Computing"><a href="#Quantum-Based-Feature-Selection-for-Multi-classification-Problem-in-Complex-Systems-with-Edge-Computing" class="headerlink" title="Quantum-Based Feature Selection for Multi-classification Problem in Complex Systems with Edge Computing"></a>Quantum-Based Feature Selection for Multi-classification Problem in Complex Systems with Edge Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01443">http://arxiv.org/abs/2310.01443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjie Liu, Junxiu Chen, Yuxiang Wang, Peipei Gao, Zhibin Lei, Xu Ma</li>
<li>for: 本研究提出了一种基于量子算法的特征选择方法，以提高计算效率和降低资源消耗。</li>
<li>methods: 本方法使用量子态编码法将每个样本的特征编码为量子状态，然后应用振荡器算法计算任务之间的相似性。接着，根据相似性，使用格罗韦-隆方法找到最近的k个邻居样本，并更新权重矩阵。</li>
<li>results: 与传统的类ReliefF算法相比，本方法可以降低相似性计算的复杂度从O(MN)降至O(M)，找到最近的邻居的复杂度从O(M)降至O(sqrt(M))，并降低资源消耗从O(MN)降至O(MlogN)。同时，与量子Relief算法相比，本方法在找到最近的邻居方面更为精准，从O(M)降至O(sqrt(M))。最后，通过基于Rigetti的一个简单示例的实验来验证方法的可行性。<details>
<summary>Abstract</summary>
The complex systems with edge computing require a huge amount of multi-feature data to extract appropriate insights for their decision making, so it is important to find a feasible feature selection method to improve the computational efficiency and save the resource consumption. In this paper, a quantum-based feature selection algorithm for the multi-classification problem, namely, QReliefF, is proposed, which can effectively reduce the complexity of algorithm and improve its computational efficiency. First, all features of each sample are encoded into a quantum state by performing operations CMP and R_y, and then the amplitude estimation is applied to calculate the similarity between any two quantum states (i.e., two samples). According to the similarities, the Grover-Long method is utilized to find the nearest k neighbor samples, and then the weight vector is updated. After a certain number of iterations through the above process, the desired features can be selected with regards to the final weight vector and the threshold {\tau}. Compared with the classical ReliefF algorithm, our algorithm reduces the complexity of similarity calculation from O(MN) to O(M), the complexity of finding the nearest neighbor from O(M) to O(sqrt(M)), and resource consumption from O(MN) to O(MlogN). Meanwhile, compared with the quantum Relief algorithm, our algorithm is superior in finding the nearest neighbor, reducing the complexity from O(M) to O(sqrt(M)). Finally, in order to verify the feasibility of our algorithm, a simulation experiment based on Rigetti with a simple example is performed.
</details>
<details>
<summary>摘要</summary>
复杂系统与边计算需要巨量多元特征数据提取适当的洞察，因此需要一种可行的特征选择方法来提高计算效率和节省资源消耗。本文提出了一种基于量子算法的多类划分问题特征选择算法，即QReliefF，可以有效减少算法的复杂性和提高计算效率。首先，每个样本的所有特征都被编码成量子状态，并通过操作CMP和R_y进行实现。然后，对任意两个量子状态（即两个样本）进行振荡检测，并根据相似性，使用格罗弗-隆方法查找最近的k个邻居样本。然后更新权重 вектор。经过一定的迭代过程，可以选择符合最终权重 вектор和阈值{\tau}的特征。与 классическойReliefF算法相比，我们的算法减少了相似性计算的复杂性从O(MN)降低到O(M)，寻找最近邻居的复杂性从O(M)降低到O(sqrt(M))，资源消耗从O(MN)降低到O(MlogN)。同时，与量子Relief算法相比，我们的算法在寻找最近邻居方面更加突出，从O(M)降低到O(sqrt(M))。 finally，为证明我们的算法的可行性，我们在Rigetti上进行了一个简单的实验。
</details></li>
</ul>
<hr>
<h2 id="TDCGL-Two-Level-Debiased-Contrastive-Graph-Learning-for-Recommendation"><a href="#TDCGL-Two-Level-Debiased-Contrastive-Graph-Learning-for-Recommendation" class="headerlink" title="TDCGL: Two-Level Debiased Contrastive Graph Learning for Recommendation"></a>TDCGL: Two-Level Debiased Contrastive Graph Learning for Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00569">http://arxiv.org/abs/2310.00569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yubo Gao, Haotian Wu<br>for:The paper aims to address the problems of over-reliance on high-quality knowledge graphs and noise issues in real-world data, which can negatively impact the performance of knowledge graph-based recommendation methods.methods:The proposed method, Two-Level Debiased Contrastive Graph Learning (TDCGL), combines contrastive learning with debiasing techniques to improve the performance of knowledge graph-based recommendation methods. The method is designed to work on both User-Item and User-User pairs to model higher-order relations.results:The proposed method significantly outperforms state-of-the-art baselines in terms of anti-noise capability and recommendation performance. Ablation studies demonstrate the necessity of each level of the TDCGL method.<details>
<summary>Abstract</summary>
knowledge graph-based recommendation methods have achieved great success in the field of recommender systems. However, over-reliance on high-quality knowledge graphs is a bottleneck for such methods. Specifically, the long-tailed distribution of entities of KG and noise issues in the real world will make item-entity dependent relations deviate from reflecting true characteristics and significantly harm the performance of modeling user preference. Contrastive learning, as a novel method that is employed for data augmentation and denoising, provides inspiration to fill this research gap. However, the mainstream work only focuses on the long-tail properties of the number of items clicked, while ignoring that the long-tail properties of total number of clicks per user may also affect the performance of the recommendation model. Therefore, to tackle these problems, motivated by the Debiased Contrastive Learning of Unsupervised Sentence Representations (DCLR), we propose Two-Level Debiased Contrastive Graph Learning (TDCGL) model. Specifically, we design the Two-Level Debiased Contrastive Learning (TDCL) and deploy it in the KG, which is conducted not only on User-Item pairs but also on User-User pairs for modeling higher-order relations. Also, to reduce the bias caused by random sampling in contrastive learning, with the exception of the negative samples obtained by random sampling, we add a noise-based generation of negation to ensure spatial uniformity. Considerable experiments on open-source datasets demonstrate that our method has excellent anti-noise capability and significantly outperforms state-of-the-art baselines. In addition, ablation studies about the necessity for each level of TDCL are conducted.
</details>
<details>
<summary>摘要</summary>
知识图库（KG）基于推荐方法在推荐系统中取得了很大的成功。然而，高质量知识图的过亢使得这些方法受到了阻碍。具体来说，知识图中实体的长尾分布和实际世界中的噪声问题会使Item-Entity相关性偏离真实特性，从而对模型用户喜好的表达有很大的负面影响。对此，对数据增强和降噪的contrastive学习提供了灵感，但主流工作只关注长尾数量的点击项，而忽略了每个用户的总点击量长尾属性的影响。因此，为了解决这些问题，我们提出了Two-Level Debiased Contrastive Graph Learning（TDCGL）模型。具体来说，我们设计了Two-Level Debiased Contrastive Learning（TDCL），并在KG中进行了实现，不仅在用户-项对上进行了实现，还在用户-用户对上进行了实现，以模型高级别关系。此外，为了减少Random sampling导致的偏见，除了随机抽取的负样本外，我们还添加了随机生成的负样本，以确保空间均匀性。经过了一系列的实验，我们发现我们的方法在开源数据集上具有极高的反噪能力，并在比较之下显著超越了状态精算标准。此外，我们还进行了剖析研究，以确定每级TDCL的必要性。
</details></li>
</ul>
<hr>
<h2 id="Understanding-the-Robustness-of-Randomized-Feature-Defense-Against-Query-Based-Adversarial-Attacks"><a href="#Understanding-the-Robustness-of-Randomized-Feature-Defense-Against-Query-Based-Adversarial-Attacks" class="headerlink" title="Understanding the Robustness of Randomized Feature Defense Against Query-Based Adversarial Attacks"></a>Understanding the Robustness of Randomized Feature Defense Against Query-Based Adversarial Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00567">http://arxiv.org/abs/2310.00567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quang H. Nguyen, Yingjie Lao, Tung Pham, Kok-Seng Wong, Khoa D. Doan</li>
<li>for: 防止深度神经网络受到黑盒攻击，即使攻击者只有模型的输出信息。</li>
<li>methods: 提出了一种简单、轻量级的防御策略，在推理时将隐藏层的特征加上随机噪音，以提高模型免受黑盒攻击。</li>
<li>results: 经过 teorical 分析和实验 validate，该方法可以有效地增强模型对黑盒攻击的抵抗力，并不需要对模型进行 adversarial 训练，对模型的准确率也没有明显的影响。<details>
<summary>Abstract</summary>
Recent works have shown that deep neural networks are vulnerable to adversarial examples that find samples close to the original image but can make the model misclassify. Even with access only to the model's output, an attacker can employ black-box attacks to generate such adversarial examples. In this work, we propose a simple and lightweight defense against black-box attacks by adding random noise to hidden features at intermediate layers of the model at inference time. Our theoretical analysis confirms that this method effectively enhances the model's resilience against both score-based and decision-based black-box attacks. Importantly, our defense does not necessitate adversarial training and has minimal impact on accuracy, rendering it applicable to any pre-trained model. Our analysis also reveals the significance of selectively adding noise to different parts of the model based on the gradient of the adversarial objective function, which can be varied during the attack. We demonstrate the robustness of our defense against multiple black-box attacks through extensive empirical experiments involving diverse models with various architectures.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Empowering-Many-Biasing-a-Few-Generalist-Credit-Scoring-through-Large-Language-Models"><a href="#Empowering-Many-Biasing-a-Few-Generalist-Credit-Scoring-through-Large-Language-Models" class="headerlink" title="Empowering Many, Biasing a Few: Generalist Credit Scoring through Large Language Models"></a>Empowering Many, Biasing a Few: Generalist Credit Scoring through Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00566">http://arxiv.org/abs/2310.00566</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/colfeng/calm">https://github.com/colfeng/calm</a></li>
<li>paper_authors: Duanyu Feng, Yongfu Dai, Jimin Huang, Yifang Zhang, Qianqian Xie, Weiguang Han, Alejandro Lopez-Lira, Hao Wang</li>
<li>for: 这篇论文旨在检验大语言模型（LLM）是否可以用于信用评估。</li>
<li>methods: 作者使用了三个假设和一个大量的实验研究LLM在信用评估中的可行性。他们首先制定了一个特有的信用评估大语言模型（CALM），然后对LLM的偏见进行了严格的检查。</li>
<li>results: 研究发现LLM可以超越传统模型的局限性，并且在不同的金融评估中表现出优异的适应能力。同时，研究也发现LLM可能存在一些偏见，因此提出了一些改进方案。<details>
<summary>Abstract</summary>
Credit and risk assessments are cornerstones of the financial landscape, impacting both individual futures and broader societal constructs. Existing credit scoring models often exhibit limitations stemming from knowledge myopia and task isolation. In response, we formulate three hypotheses and undertake an extensive case study to investigate LLMs' viability in credit assessment. Our empirical investigations unveil LLMs' ability to overcome the limitations inherent in conventional models. We introduce a novel benchmark curated for credit assessment purposes, fine-tune a specialized Credit and Risk Assessment Large Language Model (CALM), and rigorously examine the biases that LLMs may harbor. Our findings underscore LLMs' potential in revolutionizing credit assessment, showcasing their adaptability across diverse financial evaluations, and emphasizing the critical importance of impartial decision-making in the financial sector. Our datasets, models, and benchmarks are open-sourced for other researchers.
</details>
<details>
<summary>摘要</summary>
信用和风险评估是金融景观中的两个重要基础，对个人未来和社会构建都产生了深远的影响。现有的信用评估模型经常受到知识偏见和任务隔离的限制。为了应对这些限制，我们提出了三个假设，并进行了广泛的案例研究，以评估LLMs在信用评估中的可行性。我们的实际调查发现，LLMs可以超越传统模型中的限制。我们开发了一个专门为信用评估目的制定的benchmark，细化一个特殊的信用和风险评估大语言模型（CALM），并且严格地检查LLMs可能披露的偏见。我们的发现表明，LLMs在改变信用评估的方式方面具有启示性，并且在多种金融评估中展现出了适应性。我们的数据集、模型和benchmark都公开发布，以便其他研究人员进行进一步的研究。
</details></li>
</ul>
<hr>
<h2 id="DYNAP-SE2-a-scalable-multi-core-dynamic-neuromorphic-asynchronous-spiking-neural-network-processor"><a href="#DYNAP-SE2-a-scalable-multi-core-dynamic-neuromorphic-asynchronous-spiking-neural-network-processor" class="headerlink" title="DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous spiking neural network processor"></a>DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous spiking neural network processor</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00564">http://arxiv.org/abs/2310.00564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ole Richter, Chenxi Wu, Adrian M. Whatley, German Köstinger, Carsten Nielsen, Ning Qiao, Giacomo Indiveri</li>
<li>for: 这个论文旨在提出一种基于生物神经系统的概念的卷积神经网络平台，用于实时处理感知信号。</li>
<li>methods: 该平台使用了卷积神经网络，并实现了各种生物学上的神经处理现象，如短期抑制、NMDA阻链、AMPA扩散、家OSTAT、脉冲频率调整、抗阻填充和脉冲传输延迟。</li>
<li>results: 该平台可以实现实时处理感知信号，并且可以模拟不同的生物学上的神经网络，包括单个神经元和脑细胞信号的监测。<details>
<summary>Abstract</summary>
With the remarkable progress that technology has made, the need for processing data near the sensors at the edge has increased dramatically. The electronic systems used in these applications must process data continuously, in real-time, and extract relevant information using the smallest possible energy budgets. A promising approach for implementing always-on processing of sensory signals that supports on-demand, sparse, and edge-computing is to take inspiration from biological nervous system. Following this approach, we present a brain-inspired platform for prototyping real-time event-based Spiking Neural Networks (SNNs). The system proposed supports the direct emulation of dynamic and realistic neural processing phenomena such as short-term plasticity, NMDA gating, AMPA diffusion, homeostasis, spike frequency adaptation, conductance-based dendritic compartments and spike transmission delays. The analog circuits that implement such primitives are paired with a low latency asynchronous digital circuits for routing and mapping events. This asynchronous infrastructure enables the definition of different network architectures, and provides direct event-based interfaces to convert and encode data from event-based and continuous-signal sensors. Here we describe the overall system architecture, we characterize the mixed signal analog-digital circuits that emulate neural dynamics, demonstrate their features with experimental measurements, and present a low- and high-level software ecosystem that can be used for configuring the system. The flexibility to emulate different biologically plausible neural networks, and the chip's ability to monitor both population and single neuron signals in real-time, allow to develop and validate complex models of neural processing for both basic research and edge-computing applications.
</details>
<details>
<summary>摘要</summary>
随着技术的快速发展，处理数据在边缘的需求减少了很多。电子系统在这些应用程序中必须在实时中处理数据，并在最小的能量预算下提取相关信息。一种有前途的方法是根据生物神经系统来实现持续时间的触发神经网络（SNN）。我们在这篇文章中提出了一种基于脑神经系统的平台，用于实时驱动SNN。该系统支持直接模拟生物化的神经处理现象，如短期抑制、NMDA闭合、AMPA扩散、家OSTASIS、脉冲频率调整、抗场基于脑干细胞和脉冲传输延迟。这些分析电路与低延迟的异步数字电路结合，以实现不同网络架构和直接将事件转换为数据。这个异步基础设施允许定义不同的网络架构，并提供直接基于事件的数据编码和转换接口。我们在这篇文章中描述了整体系统架构，Characterize mixed signal analog-digital circuits that emulate neural dynamics, demonstrate their features with experimental measurements, and present a low- and high-level software ecosystem that can be used for configuring the system。系统的灵活性可以模拟不同的生物学可能的神经网络，系统的检测功能可以在实时中监测单个神经元和群体神经元的信号。这些功能使得可以开发和验证复杂的神经处理模型，以满足边缘计算应用和基础研究的需求。
</details></li>
</ul>
<hr>
<h2 id="Siamese-Representation-Learning-for-Unsupervised-Relation-Extraction"><a href="#Siamese-Representation-Learning-for-Unsupervised-Relation-Extraction" class="headerlink" title="Siamese Representation Learning for Unsupervised Relation Extraction"></a>Siamese Representation Learning for Unsupervised Relation Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00552">http://arxiv.org/abs/2310.00552</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gxxxzhang/siamese-ure">https://github.com/gxxxzhang/siamese-ure</a></li>
<li>paper_authors: Guangxin Zhang, Shu Chen</li>
<li>for: 掌握开放平台文本中Named Entity对的下一级关系，无需先知 relacional distribution。</li>
<li>methods: 使用对比学习，吸引正样本，排斥负样本，以提高分类的分化。</li>
<li>results: 我们提出的Siamese Representation Learning for Unsupervised Relation Extraction模型，可以有效优化关系表示例子，保持关系特征空间的层次结构，并在无监督的情况下提高关系EXTRACTION的性能。<details>
<summary>Abstract</summary>
Unsupervised relation extraction (URE) aims at discovering underlying relations between named entity pairs from open-domain plain text without prior information on relational distribution. Existing URE models utilizing contrastive learning, which attract positive samples and repulse negative samples to promote better separation, have got decent effect. However, fine-grained relational semantic in relationship makes spurious negative samples, damaging the inherent hierarchical structure and hindering performances. To tackle this problem, we propose Siamese Representation Learning for Unsupervised Relation Extraction -- a novel framework to simply leverage positive pairs to representation learning, possessing the capability to effectively optimize relation representation of instances and retain hierarchical information in relational feature space. Experimental results show that our model significantly advances the state-of-the-art results on two benchmark datasets and detailed analyses demonstrate the effectiveness and robustness of our proposed model on unsupervised relation extraction.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>无监督关系抽取（URE）目标是从开放领域平滑文本中发现下面的关系，无需先知relational分布。现有URE模型使用对照学习，吸引正样本并排斥负样本，以促进更好的分离。然而，细腻的关系semantic在关系中导致假性负样本的生成，损害内在的层次结构，降低性能。为解决这个问题，我们提出了对称表示学习 для无监督关系抽取——一种新的框架，可以简单地利用正样本来 representation学习，具有可以有效优化关系表示实例的能力，并保留关系特征空间中的层次信息。实验结果显示，我们的模型在两个 benchmark 数据集上显著提高了状态的报告结果，并在详细分析中证明了我们提出的模型在无监督关系抽取中的效果和稳定性。
</details></li>
</ul>
<hr>
<h2 id="JoMA-Demystifying-Multilayer-Transformers-via-JOint-Dynamics-of-MLP-and-Attention"><a href="#JoMA-Demystifying-Multilayer-Transformers-via-JOint-Dynamics-of-MLP-and-Attention" class="headerlink" title="JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention"></a>JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00535">http://arxiv.org/abs/2310.00535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, Simon Du</li>
<li>for: 这篇论文旨在理解多层Transformer架构在训练过程中的行为。</li>
<li>methods: 论文提出了一种新的数学框架，称为Join MLP&#x2F;Attention（JoMA）动力学，它将Transformer架构中的自注意层替换为多层MLP层，从而更好地理解训练过程。</li>
<li>results: 实验表明，在使用真实世界数据集（Wikitext2&#x2F;Wikitext103）和不同的预训练模型（OPT、Pythia）训练的情况下，JoMA能够准确预测多层Transformer中Token的组合方式，并且能够解释在不同的 activations 下，注意力在不同阶段变得稀疏或密集。<details>
<summary>Abstract</summary>
We propose Joint MLP/Attention (JoMA) dynamics, a novel mathematical framework to understand the training procedure of multilayer Transformer architectures. This is achieved by integrating out the self-attention layer in Transformers, producing a modified dynamics of MLP layers only. JoMA removes unrealistic assumptions in previous analysis (e.g., lack of residual connection) and predicts that the attention first becomes sparse (to learn salient tokens), then dense (to learn less salient tokens) in the presence of nonlinear activations, while in the linear case, it is consistent with existing works that show attention becomes sparse over time. We leverage JoMA to qualitatively explains how tokens are combined to form hierarchies in multilayer Transformers, when the input tokens are generated by a latent hierarchical generative model. Experiments on models trained from real-world dataset (Wikitext2/Wikitext103) and various pre-trained models (OPT, Pythia) verify our theoretical findings.
</details>
<details>
<summary>摘要</summary>
我们提议的 JOINT MLP/ATTENTION（JoMA）动力学框架，用于理解多层Transformer结构的训练过程。我们在Transformer中抽取了自注意层，生成了修改后的MLP层 dynamics。JoMA eliminates unrealistic assumptions in previous analysis（例如缺乏径向连接），并预测在非线性活化下，注意力首先变得稀疏（以学习重要的token），然后变得密集（以学习较不重要的token）。在线性情况下，它与先前的研究一致，注意力随时间变得稀疏。我们利用JoMA来质量地解释在多层Transformer中如何将输入token组合成层次结构，当输入token由隐藏的层次生成模型生成。我们通过在真实世界数据集（Wikitext2/Wikitext103）和多种预训练模型（OPT、Pythia）进行实验，证明我们的理论发现。
</details></li>
</ul>
<hr>
<h2 id="SELF-Language-Driven-Self-Evolution-for-Large-Language-Model"><a href="#SELF-Language-Driven-Self-Evolution-for-Large-Language-Model" class="headerlink" title="SELF: Language-Driven Self-Evolution for Large Language Model"></a>SELF: Language-Driven Self-Evolution for Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00533">http://arxiv.org/abs/2310.00533</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianqiao Lu, Wanjun Zhong, Wenyong Huang, Yufei Wang, Fei Mi, Baojun Wang, Weichao Wang, Lifeng Shang, Qun Liu</li>
<li>for: The paper aims to introduce an innovative approach for autonomous model development in large language models (LLMs), enabling them to undergo continual self-evolution and improve their intrinsic abilities without human intervention.</li>
<li>methods: The proposed approach, called “SELF” (Self-Evolution with Language Feedback), employs language-based feedback as a versatile and comprehensive evaluative tool to guide the model’s self-evolutionary training. SELF acquires foundational meta-skills through meta-skill learning, and uses self-curated data for perpetual training and iterative fine-tuning to enhance its capabilities.</li>
<li>results: The experimental results on representative benchmarks demonstrate that SELF can progressively advance its inherent abilities without human intervention, producing responses of superior quality. The SELF framework signifies a viable pathway for autonomous LLM development, transforming the LLM from a passive recipient of information into an active participant in its own evolution.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have showcased remarkable versatility across diverse domains. However, the pathway toward autonomous model development, a cornerstone for achieving human-level learning and advancing autonomous AI, remains largely uncharted. We introduce an innovative approach, termed "SELF" (Self-Evolution with Language Feedback). This methodology empowers LLMs to undergo continual self-evolution. Furthermore, SELF employs language-based feedback as a versatile and comprehensive evaluative tool, pinpointing areas for response refinement and bolstering the stability of self-evolutionary training. Initiating with meta-skill learning, SELF acquires foundational meta-skills with a focus on self-feedback and self-refinement. These meta-skills are critical, guiding the model's subsequent self-evolution through a cycle of perpetual training with self-curated data, thereby enhancing its intrinsic abilities. Given unlabeled instructions, SELF equips the model with the capability to autonomously generate and interactively refine responses. This synthesized training data is subsequently filtered and utilized for iterative fine-tuning, enhancing the model's capabilities. Experimental results on representative benchmarks substantiate that SELF can progressively advance its inherent abilities without the requirement of human intervention, thereby indicating a viable pathway for autonomous model evolution. Additionally, SELF can employ online self-refinement strategy to produce responses of superior quality. In essence, the SELF framework signifies a progressive step towards autonomous LLM development, transforming the LLM from a mere passive recipient of information into an active participant in its own evolution.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在多种领域表现出了惊人的多面性。然而，把模型发展成为自主的核心目标，以实现人类水平的学习和自主AI的进步，仍然是一个未探索的路径。我们提出了一种创新的方法，称为“自我演进”（Self-Evolution with Language Feedback，SELF）。这种方法使得LLM可以不断自我演进。此外，SELF使用语言反馈作为多方面的评价工具，帮助模型自我评估和改进。通过初级技能学习，SELF取得了基本的初级技能，重点是自我反馈和自我改进。这些初级技能是关键的，帮助模型在后续的自我演进过程中自动生成和互动地反复修改答案。在没有 Label 的情况下，SELF 使得模型可以自动生成和修改答案。这些合成的训练数据被筛选并用于迭代练化，从而提高模型的能力。实验结果表明，SELF 可以不断提高其内在能力，无需人类干预，这表明了一个可行的自主模型演进路径。此外，SELF 还可以使用在线自我反finement策略生成高质量答案。总之，SELF 框架表示了一个自主 LLM 发展的进步，将 LLM 转化为一个活跃参与自己演进的参与者。
</details></li>
</ul>
<hr>
<h2 id="Are-Graph-Neural-Networks-Optimal-Approximation-Algorithms"><a href="#Are-Graph-Neural-Networks-Optimal-Approximation-Algorithms" class="headerlink" title="Are Graph Neural Networks Optimal Approximation Algorithms?"></a>Are Graph Neural Networks Optimal Approximation Algorithms?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00526">http://arxiv.org/abs/2310.00526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Morris Yau, Eric Lu, Nikolaos Karalias, Jessica Xu, Stefanie Jegelka</li>
<li>for: 这个论文目的是设计用于获得优化算法的图 neural network 架构，用于解决一类 combinatorial optimization problems。</li>
<li>methods: 论文使用了强大的算法工具 from semidefinite programming (SDP)，并证明了可以使用 polynomial-sized message passing algorithms 来表示最强 polynomial time algorithms for Max Constraint Satisfaction Problems，假设Unique Games Conjecture 成立。</li>
<li>results: 论文实现了高质量的近似解决方案，在多种实际和 sintetic 数据集上对比 both neural baselines 和 classical algorithms 表现出色。此外，论文还利用 OptGNN 的 convex relaxation 能力设计了一种生成 dual certificates of optimality 的算法。<details>
<summary>Abstract</summary>
In this work we design graph neural network architectures that can be used to obtain optimal approximation algorithms for a large class of combinatorial optimization problems using powerful algorithmic tools from semidefinite programming (SDP). Concretely, we prove that polynomial-sized message passing algorithms can represent the most powerful polynomial time algorithms for Max Constraint Satisfaction Problems assuming the Unique Games Conjecture. We leverage this result to construct efficient graph neural network architectures, OptGNN, that obtain high-quality approximate solutions on landmark combinatorial optimization problems such as Max Cut and maximum independent set. Our approach achieves strong empirical results across a wide range of real-world and synthetic datasets against both neural baselines and classical algorithms. Finally, we take advantage of OptGNN's ability to capture convex relaxations to design an algorithm for producing dual certificates of optimality (bounds on the optimal solution) from the learned embeddings of OptGNN.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们设计了图神经网络架构，可以用来获取大类 combinatorial optimization 问题的优化算法。我们证明了，使用半definite 程序（SDP）的强大算法工具，可以通过极限下的讯息传递算法来获取最优解。我们利用这个结果，构建了高效的图神经网络架构 OptGNN，可以在 landmark  combinatorial optimization 问题中获得高质量的近似解。我们的方法在各种实际和 sintetic 数据集上实现了强有力的实际结果，比较 neural 基elines 和经典算法。最后，我们利用 OptGNN 捕捉到的 convex relaxation，设计了一种生成优化解的 dual certificate 算法。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/01/cs.AI_2023_10_01/" data-id="clp869trj004tk588fzzad2cn" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/01/cs.CL_2023_10_01/" class="article-date">
  <time datetime="2023-10-01T11:00:00.000Z" itemprop="datePublished">2023-10-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/01/cs.CL_2023_10_01/">cs.CL - 2023-10-01</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Parameter-Efficient-Tuning-Helps-Language-Model-Alignment"><a href="#Parameter-Efficient-Tuning-Helps-Language-Model-Alignment" class="headerlink" title="Parameter-Efficient Tuning Helps Language Model Alignment"></a>Parameter-Efficient Tuning Helps Language Model Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00819">http://arxiv.org/abs/2310.00819</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianci Xue, Ziqi Wang, Heng Ji</li>
<li>for: 这个研究的目的是将大型自然语言模型（LLMs）调整为人类的喜好，以确保LLMs在使用时能够安全且有用。</li>
<li>methods: 这个研究使用了强化学习（RLHF）和直接喜好优化（DPO）以进行调整，但这些方法有一些限制，例如只能在训练时间对一个喜好进行调整（例如不能将模型训练为生成简润的回答时对应多个喜好），或者有特定的数据格式限制（例如DPO只支持双向喜好数据）。为了解决这个问题，先前的研究将控制生成集成到调整过程中，让模型在测试时根据不同的喜好生成不同的回答。控制生成还提供了更多的数据格式可能性（例如支持点对数据）。</li>
<li>results: 这个研究的结果显示，使用实体效率的调整（例如提示调整和低维度适应）来优化控制token，然后进行控制生成，可以将控制token的质量提高，并在两个公认的数据集上显著提高控制生成质量，与先前的研究相比。<details>
<summary>Abstract</summary>
Aligning large language models (LLMs) with human preferences is essential for safe and useful LLMs. Previous works mainly adopt reinforcement learning (RLHF) and direct preference optimization (DPO) with human feedback for alignment. Nevertheless, they have certain drawbacks. One such limitation is that they can only align models with one preference at the training time (e.g., they cannot learn to generate concise responses when the preference data prefers detailed responses), or have certain constraints for the data format (e.g., DPO only supports pairwise preference data). To this end, prior works incorporate controllable generations for alignment to make language models learn multiple preferences and provide outputs with different preferences during inference if asked. Controllable generation also offers more flexibility with regard to data format (e.g., it supports pointwise preference data). Specifically, it uses different control tokens for different preferences during training and inference, making LLMs behave differently when required. Current controllable generation methods either use a special token or hand-crafted prompts as control tokens, and optimize them together with LLMs. As control tokens are typically much lighter than LLMs, this optimization strategy may not effectively optimize control tokens. To this end, we first use parameter-efficient tuning (e.g., prompting tuning and low-rank adaptation) to optimize control tokens and then fine-tune models for controllable generations, similar to prior works. Our approach, alignMEnt with parameter-Efficient Tuning (MEET), improves the quality of control tokens, thus improving controllable generation quality consistently by an apparent margin on two well-recognized datasets compared with prior works.
</details>
<details>
<summary>摘要</summary>
对大型语言模型（LLM）的调整是非常重要，以确保其安全和有用。以前的工作主要采用了强化学习（RLHF）和直接喜好优化（DPO），并通过人类反馈来进行调整。然而，这些方法有一些缺点。例如，它们只能在训练时间内对一个喜好进行调整（例如，它们无法学习生成简洁响应，当喜好数据偏好详细响应时），或者有一些数据格式的限制（例如，DPO只支持对数据进行对比优化）。为了解决这个问题，先前的工作会 incorporate 可控生成，以使语言模型学习多个喜好，并在推理时根据需要生成不同的响应。可控生成还提供了更多的数据格式灵活性（例如，它支持点对数据）。具体来说，它在训练和推理时使用不同的控制符，使模型在不同的喜好下行为不同。现有的可控生成方法通常使用特殊符号或手工制定的提示作为控制符，并与模型一起优化。然而，这种优化策略可能不能有效地优化控制符。为了解决这个问题，我们首先使用参数高效调整（例如，提示调整和低级变换）来优化控制符，然后继续调整模型以实现可控生成。我们的方法，名为 alignMEnt with parameter-Efficient Tuning（MEET），可以不断提高控制符的质量，从而提高可控生成质量，并在两个常见的数据集上显著超越先前的工作。
</details></li>
</ul>
<hr>
<h2 id="Injecting-a-Structural-Inductive-Bias-into-a-Seq2Seq-Model-by-Simulation"><a href="#Injecting-a-Structural-Inductive-Bias-into-a-Seq2Seq-Model-by-Simulation" class="headerlink" title="Injecting a Structural Inductive Bias into a Seq2Seq Model by Simulation"></a>Injecting a Structural Inductive Bias into a Seq2Seq Model by Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00796">http://arxiv.org/abs/2310.00796</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthias Lindemann, Alexander Koller, Ivan Titov</li>
<li>for: 提高seq2seq NLP任务的系统泛化和少量数据学习能力</li>
<li>methods: 通过预训练模型对 synthetic 数据进行结构变换的模拟来注入强制性 inductive bias</li>
<li>results: 实验结果表明，我们的方法可以带给Transformer模型强制性 inductive bias，从而提高系统泛化和少量数据学习的能力，特别是 для FST-like 任务。<details>
<summary>Abstract</summary>
Strong inductive biases enable learning from little data and help generalization outside of the training distribution. Popular neural architectures such as Transformers lack strong structural inductive biases for seq2seq NLP tasks on their own. Consequently, they struggle with systematic generalization beyond the training distribution, e.g. with extrapolating to longer inputs, even when pre-trained on large amounts of text. We show how a structural inductive bias can be injected into a seq2seq model by pre-training it to simulate structural transformations on synthetic data. Specifically, we inject an inductive bias towards Finite State Transducers (FSTs) into a Transformer by pre-training it to simulate FSTs given their descriptions. Our experiments show that our method imparts the desired inductive bias, resulting in improved systematic generalization and better few-shot learning for FST-like tasks.
</details>
<details>
<summary>摘要</summary>
强大的推导偏好可以帮助学习从少量数据中学习和泛化到训练分布之外。流行的神经网络架构如Transformer在seq2seq NLP任务中缺乏强制性的推导偏好，因此在训练分布之外的泛化方面会遇到困难，如 extrapolating 到更长的输入。我们示示了如何通过在模型中注入结构偏好来增强 seq2seq 模型的泛化能力。特别是，我们将 Transformer 模型预训练以模拟 Finite State Transducers (FSTs) 的结构变换。我们的实验结果表明，我们的方法可以增强模型的泛化能力和几拘学习能力，特别是在 FST-like 任务中。
</details></li>
</ul>
<hr>
<h2 id="Testing-the-Limits-of-Unified-Sequence-to-Sequence-LLM-Pretraining-on-Diverse-Table-Data-Tasks"><a href="#Testing-the-Limits-of-Unified-Sequence-to-Sequence-LLM-Pretraining-on-Diverse-Table-Data-Tasks" class="headerlink" title="Testing the Limits of Unified Sequence to Sequence LLM Pretraining on Diverse Table Data Tasks"></a>Testing the Limits of Unified Sequence to Sequence LLM Pretraining on Diverse Table Data Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00789">http://arxiv.org/abs/2310.00789</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soumajyoti Sarkar, Leonard Lausen</li>
<li>for: 本研究旨在开发一个可以解决多种表格任务的模型方法，包括Semantic parsing、问题回答和分类问题。</li>
<li>methods: 我们使用了encoder-decoder式的大型自然语言模型（LLMs）来解决这些任务。我们在预训阶段共同将不同任务的模型整合到一个模型中，以提高模型的通用性和效率。</li>
<li>results: 我们通过多个减少研究发现，预训自我的目标可以大幅提高模型的表格特定任务表现。例如，我们发现在表格内容中训练的文本问题回答（QA）模型，虽然已经特化了，但仍然有很大的改善空间。我们的研究是首次尝试将表格特定预训扩展到770M至11B字串处理器模型，并与对表格数据进行特化的模型进行比较。<details>
<summary>Abstract</summary>
Tables stored in databases and tables which are present in web pages and articles account for a large part of semi-structured data that is available on the internet. It then becomes pertinent to develop a modeling approach with large language models (LLMs) that can be used to solve diverse table tasks such as semantic parsing, question answering as well as classification problems. Traditionally, there existed separate models specialized for each task individually. It raises the question of how far can we go to build a unified model that works well on some table tasks without significant degradation on others. To that end, we attempt at creating a shared modeling approach in the pretraining stage with encoder-decoder style LLMs that can cater to diverse tasks. We evaluate our approach that continually pretrains and finetunes different model families of T5 with data from tables and surrounding context, on these downstream tasks at different model scales. Through multiple ablation studies, we observe that our pretraining with self-supervised objectives can significantly boost the performance of the models on these tasks. As an example of one improvement, we observe that the instruction finetuned public models which come specialized on text question answering (QA) and have been trained on table data still have room for improvement when it comes to table specific QA. Our work is the first attempt at studying the advantages of a unified approach to table specific pretraining when scaled from 770M to 11B sequence to sequence models while also comparing the instruction finetuned variants of the models.
</details>
<details>
<summary>摘要</summary>
《文档存储在数据库和网页上的表格占据互联网上很大一部分半结构化数据。随后，我们需要开发一种模型方法，使用大型自然语言模型（LLM）来解决多种表格任务，如semantic parsing、问答以及分类问题。过去，我们有着专门为每个任务设计的单独模型。这引发了我们是否可以建立一个统一的模型，可以在不同任务之间无需重大下降性的情况下工作。为此，我们尝试了在预训练阶段使用encoder-decoder式LLM来建立共享模型方法，可以满足多种任务。我们通过多个缺省研究发现，我们的预训练自然语言对象可以显著提高模型在这些任务上的性能。例如，我们发现，通过训练文本问答（QA）模型，并将其特化为表格数据，仍然可以进一步提高表格特定的QA表现。我们的工作是首次研究表格特定预训练的优点，在扩展自然语言模型规模从770M到11B时进行比较，同时对特定 instrucion 的训练过程进行比较。》Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Analyzing-and-Mitigating-Object-Hallucination-in-Large-Vision-Language-Models"><a href="#Analyzing-and-Mitigating-Object-Hallucination-in-Large-Vision-Language-Models" class="headerlink" title="Analyzing and Mitigating Object Hallucination in Large Vision-Language Models"></a>Analyzing and Mitigating Object Hallucination in Large Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00754">http://arxiv.org/abs/2310.00754</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yiyangzhou/lure">https://github.com/yiyangzhou/lure</a></li>
<li>paper_authors: Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, Huaxiu Yao</li>
<li>for: 该研究旨在解决大规模视语言模型（LVLM）中的对象幻觉问题，以提高视语言任务的精度和可靠性。</li>
<li>methods: 该研究提出了一种简单 yet powerful的算法——LVLM Hallucination Revisor（LURE），通过重建较少幻觉的描述来修正LVLM中的对象幻觉。LURE基于对对象幻觉的主要因素进行了严格的统计分析，包括相伴（图像中certain object的频繁出现）、uncertainty（LVLM解码过程中对象的不确定性）和object position（幻觉通常出现在生成文本的后半部分）。</li>
<li>results: 该研究在六个开源LVLM中测试了LURE，并取得了23%的全面对象幻觉评价指标提升，比前一个最佳方法更高。在GPT和人类评估中，LURE一直 ranks at the top。数据和代码可以在<a target="_blank" rel="noopener" href="https://github.com/YiyangZhou/LURE%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/YiyangZhou/LURE上获取。</a><details>
<summary>Abstract</summary>
Large vision-language models (LVLMs) have shown remarkable abilities in understanding visual information with human languages. However, LVLMs still suffer from object hallucination, which is the problem of generating descriptions that include objects that do not actually exist in the images. This can negatively impact many vision-language tasks, such as visual summarization and reasoning. To address this issue, we propose a simple yet powerful algorithm, LVLM Hallucination Revisor (LURE), to post-hoc rectify object hallucination in LVLMs by reconstructing less hallucinatory descriptions. LURE is grounded in a rigorous statistical analysis of the key factors underlying object hallucination, including co-occurrence (the frequent appearance of certain objects alongside others in images), uncertainty (objects with higher uncertainty during LVLM decoding), and object position (hallucination often appears in the later part of the generated text). LURE can also be seamlessly integrated with any LVLMs. We evaluate LURE on six open-source LVLMs, achieving a 23% improvement in general object hallucination evaluation metrics over the previous best approach. In both GPT and human evaluations, LURE consistently ranks at the top. Our data and code are available at https://github.com/YiyangZhou/LURE.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)大量视力语言模型（LVLM）已经表现出了对人类语言的Visual Information的强大理解能力。然而，LVLM仍然受到对象幻觉的困扰，即生成包含不存在于图像中的对象的描述。这可能会对视力语言任务产生负面影响，如视觉概要和理解。为解决这个问题，我们提议一种简单 yet powerful的算法，即LVLM幻觉修正器（LURE），以后期修正LVLM中的对象幻觉。LURE基于对对象幻觉的关键因素进行了严格的统计分析，包括共occurrence（图像中certain对象的频繁出现）、uncertainty（LVLM解码过程中对象的高度不确定性）和object position（幻觉通常在生成文本的后半部分出现）。此外，LURE还可以与任何LVLM集成。我们对六个开源LVLM进行评估，实现了以往最佳方法的23%提升。在GPT和人类评估中，LURE也一直 ranked at the top。我们的数据和代码可以在https://github.com/YiyangZhou/LURE中获得。
</details></li>
</ul>
<hr>
<h2 id="FELM-Benchmarking-Factuality-Evaluation-of-Large-Language-Models"><a href="#FELM-Benchmarking-Factuality-Evaluation-of-Large-Language-Models" class="headerlink" title="FELM: Benchmarking Factuality Evaluation of Large Language Models"></a>FELM: Benchmarking Factuality Evaluation of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00741">http://arxiv.org/abs/2310.00741</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hkust-nlp/felm">https://github.com/hkust-nlp/felm</a></li>
<li>paper_authors: Shiqi Chen, Yiran Zhao, Jinghan Zhang, I-Chun Chern, Siyang Gao, Pengfei Liu, Junxian He</li>
<li>for: 这个研究的目的是评估大型自然语言模型（LLM）生成的文本是否准确，以便警示用户可能存在错误并促进更可靠的LLM发展。</li>
<li>methods: 这个研究使用了一个新的benchmark，称为felm，来评估LLM的准确性。这个benchmark包括从世界知识到数学和逻辑等多个领域的准确性标签，并且使用文本段来帮助特定错误的发现。</li>
<li>results: 研究发现，虽然 Retrieval 可以帮助factuality evaluation，但目前的LLM仍然远远不够，无法准确检测factual errors。<details>
<summary>Abstract</summary>
Assessing factuality of text generated by large language models (LLMs) is an emerging yet crucial research area, aimed at alerting users to potential errors and guiding the development of more reliable LLMs. Nonetheless, the evaluators assessing factuality necessitate suitable evaluation themselves to gauge progress and foster advancements. This direction remains under-explored, resulting in substantial impediments to the progress of factuality evaluators. To mitigate this issue, we introduce a benchmark for Factuality Evaluation of large Language Models, referred to as felm. In this benchmark, we collect responses generated from LLMs and annotate factuality labels in a fine-grained manner. Contrary to previous studies that primarily concentrate on the factuality of world knowledge (e.g.~information from Wikipedia), felm focuses on factuality across diverse domains, spanning from world knowledge to math and reasoning. Our annotation is based on text segments, which can help pinpoint specific factual errors. The factuality annotations are further supplemented by predefined error types and reference links that either support or contradict the statement. In our experiments, we investigate the performance of several LLM-based factuality evaluators on felm, including both vanilla LLMs and those augmented with retrieval mechanisms and chain-of-thought processes. Our findings reveal that while retrieval aids factuality evaluation, current LLMs are far from satisfactory to faithfully detect factual errors.
</details>
<details>
<summary>摘要</summary>
正在评估大语言模型（LLM）生成的文本真实性是一个emerging yet crucial的研究领域，旨在警示用户 potential errors和导向更可靠的 LLM 发展。然而，评估真实性的评估人员自己也需要适当的评估，以便衡量进步和促进进步。这个方向还未得到充分的探索，导致 LLM 的发展受到了重大的阻碍。为了解决这个问题，我们提出了一个大语言模型真实性评估标准（Felm）。在这个标准中，我们收集了由 LLM 生成的回答，并对其进行细化的标签分类。与前一些研究主要集中于世界知识（例如Wikipedia）的真实性，felm 强调在多个领域中的真实性，包括世界知识、数学和逻辑。我们的标注基于文本段，可以帮助特定的错误找到。真实性标注还得到了预定义的错误类型和参考链接，这些链接可以支持或反对声明。在我们的实验中，我们调查了一些基于 LLM 的真实性评估器在 felm 上的表现，包括基于 vanilla LLM 和增强了检索机制和链式思维的 LLM。我们的发现表明，虽然检索可以帮助真实性评估，但目前的 LLM 还远不够可靠地检测错误。
</details></li>
</ul>
<hr>
<h2 id="Robust-Sentiment-Analysis-for-Low-Resource-languages-Using-Data-Augmentation-Approaches-A-Case-Study-in-Marathi"><a href="#Robust-Sentiment-Analysis-for-Low-Resource-languages-Using-Data-Augmentation-Approaches-A-Case-Study-in-Marathi" class="headerlink" title="Robust Sentiment Analysis for Low Resource languages Using Data Augmentation Approaches: A Case Study in Marathi"></a>Robust Sentiment Analysis for Low Resource languages Using Data Augmentation Approaches: A Case Study in Marathi</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00734">http://arxiv.org/abs/2310.00734</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aabha Pingle, Aditya Vyawahare, Isha Joshi, Rahul Tangsali, Geetanjali Kale, Raviraj Joshi</li>
<li>for: 本研究旨在提高低资源语言 sentiment 分析的表现，特别是对印度语言 Marathi 进行了一项全面的数据扩充研究。</li>
<li>methods: 本文提出了四种数据扩充技术，包括 paraphrasing、back-translation、BERT 基于随机Token 替换和 named entity 替换，以及 GPT 基于文本和标签生成。</li>
<li>results: 研究结果显示，这些数据扩充方法可以提高 Marathi 语言的 sentiment 分析模型在跨频道情况下的表现，并且这些技术可以扩展到其他低资源语言和普通文本分类任务。<details>
<summary>Abstract</summary>
Sentiment analysis plays a crucial role in understanding the sentiment expressed in text data. While sentiment analysis research has been extensively conducted in English and other Western languages, there exists a significant gap in research efforts for sentiment analysis in low-resource languages. Limited resources, including datasets and NLP research, hinder the progress in this area. In this work, we present an exhaustive study of data augmentation approaches for the low-resource Indic language Marathi. Although domain-specific datasets for sentiment analysis in Marathi exist, they often fall short when applied to generalized and variable-length inputs. To address this challenge, this research paper proposes four data augmentation techniques for sentiment analysis in Marathi. The paper focuses on augmenting existing datasets to compensate for the lack of sufficient resources. The primary objective is to enhance sentiment analysis model performance in both in-domain and cross-domain scenarios by leveraging data augmentation strategies. The data augmentation approaches proposed showed a significant performance improvement for cross-domain accuracies. The augmentation methods include paraphrasing, back-translation; BERT-based random token replacement, named entity replacement, and pseudo-label generation; GPT-based text and label generation. Furthermore, these techniques can be extended to other low-resource languages and for general text classification tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Evaluating-Speech-Synthesis-by-Training-Recognizers-on-Synthetic-Speech"><a href="#Evaluating-Speech-Synthesis-by-Training-Recognizers-on-Synthetic-Speech" class="headerlink" title="Evaluating Speech Synthesis by Training Recognizers on Synthetic Speech"></a>Evaluating Speech Synthesis by Training Recognizers on Synthetic Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00706">http://arxiv.org/abs/2310.00706</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dareen Alharthi, Roshan Sharma, Hira Dhamyal, Soumi Maiti, Bhiksha Raj, Rita Singh</li>
<li>for: 这篇论文的目的是提出一种用于评估文本到语音转化系统的自动评价方法，以取代人工评估方法。</li>
<li>methods: 这篇论文使用的方法是使用一个自然语言处理模型来训练一个语音识别模型，然后使用这个语音识别模型来评估文本到语音转化系统的质量。</li>
<li>results: 这篇论文的结果表明，使用这种方法可以对文本到语音转化系统的质量进行更广泛的评估，而不是仅仅是测试语音智能的准确率。此外，这种方法还可以与人工评估方法相比肩，并且可以减少人工评估的成本。<details>
<summary>Abstract</summary>
Modern speech synthesis systems have improved significantly, with synthetic speech being indistinguishable from real speech. However, efficient and holistic evaluation of synthetic speech still remains a significant challenge. Human evaluation using Mean Opinion Score (MOS) is ideal, but inefficient due to high costs. Therefore, researchers have developed auxiliary automatic metrics like Word Error Rate (WER) to measure intelligibility. Prior works focus on evaluating synthetic speech based on pre-trained speech recognition models, however, this can be limiting since this approach primarily measures speech intelligibility. In this paper, we propose an evaluation technique involving the training of an ASR model on synthetic speech and assessing its performance on real speech. Our main assumption is that by training the ASR model on the synthetic speech, the WER on real speech reflects the similarity between distributions, a broader assessment of synthetic speech quality beyond intelligibility. Our proposed metric demonstrates a strong correlation with both MOS naturalness and MOS intelligibility when compared to SpeechLMScore and MOSNet on three recent Text-to-Speech (TTS) systems: MQTTS, StyleTTS, and YourTTS.
</details>
<details>
<summary>摘要</summary>
现代语音合成系统已经进步很 significatively，Synthetic speech 和 real speech 之间的差别已经变得极其微scopic。然而，efficiently and holistically evaluate synthetic speech 仍然是一个主要挑战。人工评分使用 Mean Opinion Score (MOS) 是理想的，但是它的成本很高。因此，研究人员已经开发了auxiliary automatic metrics like Word Error Rate (WER) 来度量语音明亮度。先前的研究主要基于使用预训练的speech recognition 模型来评估合成语音的质量，但这种方法只能测量语音的elligibility。在这篇论文中，我们提出了一种评估技术，即使用 ASR 模型来训练 synthetic speech，并用其在真实语音上的性能来度量合成语音的质量。我们的主要假设是，通过训练 ASR 模型使 synthetic speech 与 real speech 之间的分布相似，那么 WER 在真实语音上的性能将反映合成语音的质量，不仅是语音可理解性。我们的提出的度量与 MOS naturalness 和 MOS intelligibility 具有强相关性，并且在三个 latest Text-to-Speech (TTS) 系统（MQTTS、StyleTTS 和 YourTTS）上进行了比较。
</details></li>
</ul>
<hr>
<h2 id="Do-the-Benefits-of-Joint-Models-for-Relation-Extraction-Extend-to-Document-level-Tasks"><a href="#Do-the-Benefits-of-Joint-Models-for-Relation-Extraction-Extend-to-Document-level-Tasks" class="headerlink" title="Do the Benefits of Joint Models for Relation Extraction Extend to Document-level Tasks?"></a>Do the Benefits of Joint Models for Relation Extraction Extend to Document-level Tasks?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00696">http://arxiv.org/abs/2310.00696</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pratik Saini, Tapas Nayak, Indrajit Bhattacharya</li>
<li>for: 这篇论文主要针对的是关系 triple 提取的 sentence-level 和 document-level 任务。</li>
<li>methods: 论文提出了两种不同的方法：pipeline 和 joint。joint 模型可以捕捉到关系之间的交互，在 sentence-level 任务上显示出了更高的性能。</li>
<li>results: 实验结果表明，joint 模型在 sentence-level 任务上比 pipeline 模型显示出了更高的性能，但是在 document-level 任务上，joint 模型的性能下降了，与 pipeline 模型的性能相比。<details>
<summary>Abstract</summary>
Two distinct approaches have been proposed for relational triple extraction - pipeline and joint. Joint models, which capture interactions across triples, are the more recent development, and have been shown to outperform pipeline models for sentence-level extraction tasks. Document-level extraction is a more challenging setting where interactions across triples can be long-range, and individual triples can also span across sentences. Joint models have not been applied for document-level tasks so far. In this paper, we benchmark state-of-the-art pipeline and joint extraction models on sentence-level as well as document-level datasets. Our experiments show that while joint models outperform pipeline models significantly for sentence-level extraction, their performance drops sharply below that of pipeline models for the document-level dataset.
</details>
<details>
<summary>摘要</summary>
两种不同的方法有被提议用于关系三元EXTRACT - 管道和共同。共同模型， capture关系三元之间的互动，是更新的发展，并在句子级EXTRACT任务中显示出perform得到更好的结果。文档级EXTRACT是一个更加复杂的设定， где交互关系可以是长距离的，并且每个三元也可以跨 sentence。共同模型没有被应用于文档级任务上。在这篇文章中，我们对 sentence级和文档级的EXTRACT模型进行了比较。我们的实验结果表明，虽然共同模型在句子级EXTRACT任务上表现明显 луч于管道模型，但是对文档级数据集的性能下降了很多。
</details></li>
</ul>
<hr>
<h2 id="CebuaNER-A-New-Baseline-Cebuano-Named-Entity-Recognition-Model"><a href="#CebuaNER-A-New-Baseline-Cebuano-Named-Entity-Recognition-Model" class="headerlink" title="CebuaNER: A New Baseline Cebuano Named Entity Recognition Model"></a>CebuaNER: A New Baseline Cebuano Named Entity Recognition Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00679">http://arxiv.org/abs/2310.00679</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mebzmoren/cebuaner">https://github.com/mebzmoren/cebuaner</a></li>
<li>paper_authors: Ma. Beatrice Emanuela Pilar, Ellyza Mari Papas, Mary Loise Buenaventura, Dane Dedoroy, Myron Darrel Montefalcon, Jay Rhald Padilla, Lany Maceda, Mideth Abisado, Joseph Marvin Imperial</li>
<li>for: 这个研究的目的是为了提供一个基线模型 для缅甸语名实体识别（NER）任务。</li>
<li>methods: 这个研究使用了Conditional Random Field和Bidirectional LSTM算法来适应缅甸语文本，并对4000份当地新闻文章进行了标注和训练。</li>
<li>results: 研究发现这个基线模型在精度、准确率和F1指标上达到了70%以上，并且在跨语言设置下与标准模型进行比较表现良好。<details>
<summary>Abstract</summary>
Despite being one of the most linguistically diverse groups of countries, computational linguistics and language processing research in Southeast Asia has struggled to match the level of countries from the Global North. Thus, initiatives such as open-sourcing corpora and the development of baseline models for basic language processing tasks are important stepping stones to encourage the growth of research efforts in the field. To answer this call, we introduce CebuaNER, a new baseline model for named entity recognition (NER) in the Cebuano language. Cebuano is the second most-used native language in the Philippines, with over 20 million speakers. To build the model, we collected and annotated over 4,000 news articles, the largest of any work in the language, retrieved from online local Cebuano platforms to train algorithms such as Conditional Random Field and Bidirectional LSTM. Our findings show promising results as a new baseline model, achieving over 70% performance on precision, recall, and F1 across all entity tags, as well as potential efficacy in a crosslingual setup with Tagalog.
</details>
<details>
<summary>摘要</summary>
Despite being one of the most linguistically diverse regions in the world, computational linguistics and language processing research in Southeast Asia has struggled to keep up with the level of countries from the Global North. To address this challenge, initiatives such as open-sourcing corpora and developing baseline models for basic language processing tasks are crucial stepping stones to encourage the growth of research efforts in the field. In response to this call, we introduce CebuaNER, a new baseline model for named entity recognition (NER) in the Cebuano language. Cebuano is the second most widely spoken native language in the Philippines, with over 20 million speakers. To build the model, we collected and annotated over 4,000 news articles, the largest dataset of any work in the language, retrieved from online local Cebuano platforms and trained algorithms such as Conditional Random Field and Bidirectional LSTM. Our findings show promising results as a new baseline model, achieving over 70% performance on precision, recall, and F1 across all entity tags, as well as potential efficacy in a crosslingual setup with Tagalog.
</details></li>
</ul>
<hr>
<h2 id="GeRA-Label-Efficient-Geometrically-Regularized-Alignment"><a href="#GeRA-Label-Efficient-Geometrically-Regularized-Alignment" class="headerlink" title="GeRA: Label-Efficient Geometrically Regularized Alignment"></a>GeRA: Label-Efficient Geometrically Regularized Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00672">http://arxiv.org/abs/2310.00672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dustin Klebe, Tal Shnitzer, Mikhail Yurochkin, Leonid Karlinsky, Justin Solomon</li>
<li>for: 这 paper 的目的是为了减少对彩色数据的需求，并在label-efficient的情况下进行多modal embedding空间的Alignment。</li>
<li>methods: 这 paper 使用了一种名为 Geometrically Regularized Alignment (GeRA) 的 semi-supervised方法，该方法利用了无关数据的演化geometry来改进对 embedding 空间的Alignment。</li>
<li>results:  experiments 表明，GeRA 方法在 speech-text 和 image-text 领域中表现出了明显的改进，特别是使用小量对数据的 paired data。<details>
<summary>Abstract</summary>
Pretrained unimodal encoders incorporate rich semantic information into embedding space structures. To be similarly informative, multi-modal encoders typically require massive amounts of paired data for alignment and training. We introduce a semi-supervised Geometrically Regularized Alignment (GeRA) method to align the embedding spaces of pretrained unimodal encoders in a label-efficient way. Our method leverages the manifold geometry of unpaired (unlabeled) data to improve alignment performance. To prevent distortions to local geometry during the alignment process, potentially disrupting semantic neighborhood structures and causing misalignment of unobserved pairs, we introduce a geometric loss term. This term is built upon a diffusion operator that captures the local manifold geometry of the unimodal pretrained encoders. GeRA is modality-agnostic and thus can be used to align pretrained encoders from any data modalities. We provide empirical evidence to the effectiveness of our method in the domains of speech-text and image-text alignment. Our experiments demonstrate significant improvement in alignment quality compared to a variaty of leading baselines, especially with a small amount of paired data, using our proposed geometric regularization.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate the following text into Simplified Chinese:Pretrained unimodal encoders incorporate rich semantic information into embedding space structures. To be similarly informative, multi-modal encoders typically require massive amounts of paired data for alignment and training. We introduce a semi-supervised Geometrically Regularized Alignment (GeRA) method to align the embedding spaces of pretrained unimodal encoders in a label-efficient way. Our method leverages the manifold geometry of unpaired (unlabeled) data to improve alignment performance. To prevent distortions to local geometry during the alignment process, potentially disrupting semantic neighborhood structures and causing misalignment of unobserved pairs, we introduce a geometric loss term. This term is built upon a diffusion operator that captures the local manifold geometry of the unimodal pretrained encoders. GeRA is modality-agnostic and thus can be used to align pretrained encoders from any data modalities. We provide empirical evidence to the effectiveness of our method in the domains of speech-text and image-text alignment. Our experiments demonstrate significant improvement in alignment quality compared to a variety of leading baselines, especially with a small amount of paired data, using our proposed geometric regularization.Translate the text into Simplified Chinese: preprained 单modal encoders 含有丰富的 semantic 信息，将 embedding 空间结构中的信息升级为多modal  encoders 需要巨量的对应数据对Alignment和training。我们介绍了一种 semi-supervised 的 Geometrically Regularized Alignment (GeRA) 方法，用于对 preprained 单modal encoders 的 embedding 空间进行标签效率的对Alignment。我们的方法利用了无对应数据的 manifold geometry，以提高对Alignment的性能。为避免对Local geometry的扭曲，可能导致 semantic 邻居结构的扰乱和未观察对的歪曲，我们引入了一个 geometric 损失项。这个项目基于一个 diffusion 算子，捕捉了单modal 预训练 encoders 的 Local manifold geometry。GeRA 是modal-agnostic，因此可以用于对任何数据模式的预训练 encoders 进行对Alignment。我们提供了实验证明我们的方法在speech-text 和 image-text 对Alignment中的效果。我们的实验表明，使用我们提posed的 geometric 正则化可以在小量对数据情况下达到显著提高对Alignment质量的效果，特别是与多种主流基准值进行比较。
</details></li>
</ul>
<hr>
<h2 id="Fewer-is-More-Trojan-Attacks-on-Parameter-Efficient-Fine-Tuning"><a href="#Fewer-is-More-Trojan-Attacks-on-Parameter-Efficient-Fine-Tuning" class="headerlink" title="Fewer is More: Trojan Attacks on Parameter-Efficient Fine-Tuning"></a>Fewer is More: Trojan Attacks on Parameter-Efficient Fine-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00648">http://arxiv.org/abs/2310.00648</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lauren Hong, Ting Wang</li>
<li>for: This paper explores the security implications of parameter-efficient fine-tuning (PEFT) for pre-trained language models (PLMs), and reveals a novel attack called PETA that can successfully inject a backdoor into a PLM using PEFT.</li>
<li>methods: The attack uses bilevel optimization to embed a backdoor into a PLM while retaining the PLM’s task-specific performance, and the defense omits PEFT in selected layers of the backdoored PLM and unfreezes a subset of these layers’ parameters to neutralize the attack.</li>
<li>results: The attack is effective in terms of both attack success rate and unaffected clean accuracy, even after the victim user performs PEFT over the backdoored PLM using untainted data. The defense is effective in neutralizing the attack.Here is the summary in Traditional Chinese:</li>
<li>for: 本研究探讨parameter-efficient fine-tuning (PEFT)所带来的安全问题，并发现了一种称为PETA的攻击，可以成功地将backdoor注入到pre-trained language models (PLMs)中。</li>
<li>methods: 这个攻击使用了 bilateral optimization来嵌入backdoor到PLM中，并保持PLM的任务特定性能。防御方法是将PEFT快照在选择的层中，并将这些层的parameters解冻。</li>
<li>results: 这个攻击具有成功率和不受污染的清洁率，甚至在受害者使用不混合的数据进行PEFT后仍然有效。防御方法能够有效地中和攻击。<details>
<summary>Abstract</summary>
Parameter-efficient fine-tuning (PEFT) enables efficient adaptation of pre-trained language models (PLMs) to specific tasks. By tuning only a minimal set of (extra) parameters, PEFT achieves performance comparable to full fine-tuning. However, despite its prevalent use, the security implications of PEFT remain largely unexplored. In this paper, we conduct a pilot study revealing that PEFT exhibits unique vulnerability to trojan attacks. Specifically, we present PETA, a novel attack that accounts for downstream adaptation through bilevel optimization: the upper-level objective embeds the backdoor into a PLM while the lower-level objective simulates PEFT to retain the PLM's task-specific performance. With extensive evaluation across a variety of downstream tasks and trigger designs, we demonstrate PETA's effectiveness in terms of both attack success rate and unaffected clean accuracy, even after the victim user performs PEFT over the backdoored PLM using untainted data. Moreover, we empirically provide possible explanations for PETA's efficacy: the bilevel optimization inherently 'orthogonalizes' the backdoor and PEFT modules, thereby retaining the backdoor throughout PEFT. Based on this insight, we explore a simple defense that omits PEFT in selected layers of the backdoored PLM and unfreezes a subset of these layers' parameters, which is shown to effectively neutralize PETA.
</details>
<details>
<summary>摘要</summary>
parameter-efficient fine-tuning (PEFT) 可以快速地适应预训练语言模型 (PLM) 到特定任务。通过只调整一小部分 (Extra) 的参数，PEFT 可以达到与全面 fine-tuning 相同的性能。然而，尽管它在广泛使用，PEFT 的安全性问题仍然未得到足够的探讨。在这篇论文中，我们进行了一个小型研究，揭示 PEFT 存在独特的潜在攻击点。 Specifically, we present PETA, a novel attack that accounts for downstream adaptation through bilevel optimization: the upper-level objective embeds the backdoor into a PLM while the lower-level objective simulates PEFT to retain the PLM's task-specific performance. With extensive evaluation across a variety of downstream tasks and trigger designs, we demonstrate PETA's effectiveness in terms of both attack success rate and unaffected clean accuracy, even after the victim user performs PEFT over the backdoored PLM using untainted data. Moreover, we empirically provide possible explanations for PETA's efficacy: the bilevel optimization inherently 'orthogonalizes' the backdoor and PEFT modules, thereby retaining the backdoor throughout PEFT. Based on this insight, we explore a simple defense that omits PEFT in selected layers of the backdoored PLM and unfreezes a subset of these layers' parameters, which is shown to effectively neutralize PETA.
</details></li>
</ul>
<hr>
<h2 id="Wavelet-Scattering-Transform-for-Improving-Generalization-in-Low-Resourced-Spoken-Language-Identification"><a href="#Wavelet-Scattering-Transform-for-Improving-Generalization-in-Low-Resourced-Spoken-Language-Identification" class="headerlink" title="Wavelet Scattering Transform for Improving Generalization in Low-Resourced Spoken Language Identification"></a>Wavelet Scattering Transform for Improving Generalization in Low-Resourced Spoken Language Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00602">http://arxiv.org/abs/2310.00602</a></li>
<li>repo_url: None</li>
<li>paper_authors: Spandan Dey, Premjeet Singh, Goutam Saha</li>
<li>for: 提高低资源语音认识系统的泛化性</li>
<li>methods: 使用浪涌扩散变换（WST）来代替通常使用的MEL-spectrogram或MFCC特征，以弥补高频信息损失问题</li>
<li>results: 与MFCC相比，使用WST特征可以降低识别错误率，最多降低14.05%和6.40% для同一 corpus和隐藏 VoxLingua107评估 respectively<details>
<summary>Abstract</summary>
Commonly used features in spoken language identification (LID), such as mel-spectrogram or MFCC, lose high-frequency information due to windowing. The loss further increases for longer temporal contexts. To improve generalization of the low-resourced LID systems, we investigate an alternate feature representation, wavelet scattering transform (WST), that compensates for the shortcomings. To our knowledge, WST is not explored earlier in LID tasks. We first optimize WST features for multiple South Asian LID corpora. We show that LID requires low octave resolution and frequency-scattering is not useful. Further, cross-corpora evaluations show that the optimal WST hyper-parameters depend on both train and test corpora. Hence, we develop fused ECAPA-TDNN based LID systems with different sets of WST hyper-parameters to improve generalization for unknown data. Compared to MFCC, EER is reduced upto 14.05% and 6.40% for same-corpora and blind VoxLingua107 evaluations, respectively.
</details>
<details>
<summary>摘要</summary>
通常使用的语音识别（LID）任务中的特征，如MEL-spectrogram或MFCC，因窗口效应而产生高频信息损失，这种损失随着时间上下文的增加而加大。为了改善低资源的LID系统的通用性，我们 investigate了一种 alternate 特征表示，wavelet scattering transform（WST），该表示可以补偿这些缺点。据我们所知，WST在LID任务中没有被探索过。我们首先优化WST特征 для多个南亚语言LID corpus。我们发现，LID需要低 octave 分辨率，而频率散射并不是有用。此外，跨 corpus 评估表明，优化 WST 超参数取决于训练和测试 corpus。因此，我们开发了 fusion ECAPA-TDNN 基于 WST 的 LID 系统，以提高对不知数据的泛化性。相比 MFCC，我们在同一 corpora 和 blind VoxLingua107 评估中分别减少了 EER 14.05% 和 6.40%。
</details></li>
</ul>
<hr>
<h2 id="A-Task-oriented-Dialog-Model-with-Task-progressive-and-Policy-aware-Pre-training"><a href="#A-Task-oriented-Dialog-Model-with-Task-progressive-and-Policy-aware-Pre-training" class="headerlink" title="A Task-oriented Dialog Model with Task-progressive and Policy-aware Pre-training"></a>A Task-oriented Dialog Model with Task-progressive and Policy-aware Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00597">http://arxiv.org/abs/2310.00597</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lucenzhong/tpld">https://github.com/lucenzhong/tpld</a></li>
<li>paper_authors: Lucen Zhong, Hengtong Lu, Caixia Yuan, Xiaojie Wang, Jiashen Sun, Ke Zeng, Guanglu Wan</li>
<li>for: 提高任务对话（TOD）相关任务的顺序性和对话策略学习</li>
<li>methods: 使用两种策略相关预训练任务进行预训练，包括全球策略一致性任务和行为相似学习任务</li>
<li>results: 在多个WOZ和车辆内端对话模型评价标准中表现更好，只使用18%的参数和25%的预训练数据，与之前的状态当前PCMGALAXY相比<details>
<summary>Abstract</summary>
Pre-trained conversation models (PCMs) have achieved promising progress in recent years. However, existing PCMs for Task-oriented dialog (TOD) are insufficient for capturing the sequential nature of the TOD-related tasks, as well as for learning dialog policy information. To alleviate these problems, this paper proposes a task-progressive PCM with two policy-aware pre-training tasks. The model is pre-trained through three stages where TOD-related tasks are progressively employed according to the task logic of the TOD system. A global policy consistency task is designed to capture the multi-turn dialog policy sequential relation, and an act-based contrastive learning task is designed to capture similarities among samples with the same dialog policy. Our model achieves better results on both MultiWOZ and In-Car end-to-end dialog modeling benchmarks with only 18\% parameters and 25\% pre-training data compared to the previous state-of-the-art PCM, GALAXY.
</details>
<details>
<summary>摘要</summary>
各种前置模型（PCM）在过去几年内已经取得了令人满意的进步。然而，现有的PCM对任务导向对话（TOD）不足以捕捉TOD相关任务的顺序性，以及对话策略信息的学习。为了解决这些问题，这篇论文提出了一种任务逐步进行的PCM，其中包括两个策略意识的预训练任务。模型在三个阶段中预训练，其中TOD相关任务逐步应用于TOD系统的任务逻辑。为了捕捉多Turn对话策略的顺序关系，我们设计了全球策略一致任务。同时，为了捕捉同一策略下的对话样本的相似性，我们设计了基于行为的对比学习任务。我们的模型在MultiWOZ和In-Car终端对话模型 benchmark上达到了之前的state-of-the-art PCMGALAXY的性能，但它只有18%的参数和25%的预训练数据。
</details></li>
</ul>
<hr>
<h2 id="Nine-year-old-children-outperformed-ChatGPT-in-emotion-Evidence-from-Chinese-writing"><a href="#Nine-year-old-children-outperformed-ChatGPT-in-emotion-Evidence-from-Chinese-writing" class="headerlink" title="Nine-year-old children outperformed ChatGPT in emotion: Evidence from Chinese writing"></a>Nine-year-old children outperformed ChatGPT in emotion: Evidence from Chinese writing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00578">http://arxiv.org/abs/2310.00578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyi Cao, Tongquan Zhou, Siruo Zhou</li>
<li>for: 本研究旨在探讨 chatGPT 和九岁儿童在中文写作方面的Relative优劣。</li>
<li>methods: 研究采用了对 narrative 和 scientific 主题的中文写作进行比较分析，并通过五种语言 dimensión 的评价指标来评估 chatGPT 和儿童的写作水平。</li>
<li>results: 结果显示 nine-year-old 儿童在 fluency 和 cohesion 方面的写作水平胜过 chatGPT，但 chatGPT 在 accuracy 方面表现出色。 children 在 science-themed 写作中表现出更高的 complexity，而 chatGPT 在 nature-themed 写作中表现出更高的 accuracy。 最重要的是，这项研究发现 nine-year-old 儿童在中文作文中表达的情感更强于 chatGPT。<details>
<summary>Abstract</summary>
ChatGPT has been demonstrated to possess significant capabilities in generating intricate, human-like text, and recent studies have established that its performance in theory of mind tasks is comparable to that of a nine-year-old child. However, it remains uncertain whether ChatGPT surpasses nine-year-old children in Chinese writing proficiency. To explore this, our study juxtaposed the Chinese writing performance of ChatGPT and nine-year-old children on both narrative and scientific topics, aiming to uncover the relative strengths and weaknesses of ChatGPT in writing.   The collected data were analyzed across five linguistic dimensions: fluency, accuracy, complexity, cohesion, and emotion. Each dimension underwent assessment through precise indices. The findings revealed that nine-year-old children excelled beyond ChatGPT in terms of fluency and cohesion within their writing. In contrast, ChatGPT manifested a superior performance in accuracy compared to the children. Concerning complexity, children exhibited superior skills in science-themed writing, while ChatGPT prevailed in nature-themed writing. Significantly, this research is pioneering in revealing that nine-year-old children convey stronger emotions than ChatGPT in their Chinese compositions.
</details>
<details>
<summary>摘要</summary>
chatGPT possess了较强的文本生成能力，并且研究表明其在理解人类思维方面的表现与9岁孩子相当。然而，是否chatGPT在中文写作方面超过9岁孩子仍然存在uncertainty。为了解答这个问题，我们的研究将chatGPT和9岁孩子的中文写作比较在 narative和科学话题上。我们通过分析5种语言特征，包括流畅、准确、复杂度、连贯和情感，来评估这两个组合的写作能力。我们发现，9岁孩子在流畅和连贯方面的写作能力比chatGPT更强，而chatGPT在准确性方面表现更优。在复杂度方面，孩子在科学话题上表现出了更高的技巧水平，而chatGPT在自然话题上表现更优。最重要的是，这项研究发现，9岁孩子在中文作文中表达的情感更强于chatGPT。
</details></li>
</ul>
<hr>
<h2 id="GrowLength-Accelerating-LLMs-Pretraining-by-Progressively-Growing-Training-Length"><a href="#GrowLength-Accelerating-LLMs-Pretraining-by-Progressively-Growing-Training-Length" class="headerlink" title="GrowLength: Accelerating LLMs Pretraining by Progressively Growing Training Length"></a>GrowLength: Accelerating LLMs Pretraining by Progressively Growing Training Length</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00576">http://arxiv.org/abs/2310.00576</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Chia-Yuan Chang, Xia Hu</li>
<li>for: 这篇论文是为了提高大型语言模型（LLMs）的预训程序 accelerate the pretraining process 的目的。</li>
<li>methods: 这篇论文提出了一个新的、简单、有效的方法 named“\growlength”，可以加速 LLMs 的预训过程。这个方法在预训过程中逐步增加训练序列长度，从而减少 computional costs 和提高效率。例如，它从128字串开始，逐步增加到4096字串。这种方法可以让模型在有限时间内处理更多的字串，并可能提高其性能。</li>
<li>results: 我们的实验结果显示，使用我们的方法训练 LLMs 可以更快地趋向于极值，并且比使用现有方法训练的模型表现更好。此外，我们的方法不需要任何额外的工程实践，因此是实际的解决方案在 LLMs 领域。<details>
<summary>Abstract</summary>
The evolving sophistication and intricacies of Large Language Models (LLMs) yield unprecedented advancements, yet they simultaneously demand considerable computational resources and incur significant costs. To alleviate these challenges, this paper introduces a novel, simple, and effective method named ``\growlength'' to accelerate the pretraining process of LLMs. Our method progressively increases the training length throughout the pretraining phase, thereby mitigating computational costs and enhancing efficiency. For instance, it begins with a sequence length of 128 and progressively extends to 4096. This approach enables models to process a larger number of tokens within limited time frames, potentially boosting their performance. In other words, the efficiency gain is derived from training with shorter sequences optimizing the utilization of resources. Our extensive experiments with various state-of-the-art LLMs have revealed that models trained using our method not only converge more swiftly but also exhibit superior performance metrics compared to those trained with existing methods. Furthermore, our method for LLMs pretraining acceleration does not require any additional engineering efforts, making it a practical solution in the realm of LLMs.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）的发展和复杂性带来了前所未有的进步，但它们同时需要很大的计算资源和成本。为了解决这些挑战，本文提出了一种新的、简单的和有效的方法名为“\growlength”，用于加速 LLMS 的预训练过程。我们的方法在预训练阶段逐步增长训练长度，从而减少计算成本并提高效率。例如，它从序列长度为 128 开始，逐步增长到 4096。这种方法使得模型在限时内处理更多的字符，可能提高其性能。换句话说，效率提升来自于在限时内训练使用资源的优化。我们对各种现代 LLMS 进行了广泛的实验，发现使用我们的方法训练的模型不仅更快 converges，而且也表现出了较高的性能指标，比于使用现有方法训练的模型。此外，我们的方法不需要任何额外的工程努力，因此是 LLMS 预训练加速方法中的实用解决方案。
</details></li>
</ul>
<hr>
<h2 id="Colloquial-Persian-POS-CPPOS-Corpus-A-Novel-Corpus-for-Colloquial-Persian-Part-of-Speech-Tagging"><a href="#Colloquial-Persian-POS-CPPOS-Corpus-A-Novel-Corpus-for-Colloquial-Persian-Part-of-Speech-Tagging" class="headerlink" title="Colloquial Persian POS (CPPOS) Corpus: A Novel Corpus for Colloquial Persian Part of Speech Tagging"></a>Colloquial Persian POS (CPPOS) Corpus: A Novel Corpus for Colloquial Persian Part of Speech Tagging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00572">http://arxiv.org/abs/2310.00572</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leyla Rabiei, Farzaneh Rahmani, Mohammad Khansari, Zeinab Rajabi, Moein Salimi</li>
<li>for: This paper is written for those interested in natural language processing and POS tagging in Persian, specifically for colloquial text in social network analysis.</li>
<li>methods: The paper introduces a novel corpus called “Colloquial Persian POS” (CPPOS), which includes formal and informal text collected from various social media platforms such as Telegram, Twitter, and Instagram. The corpus was manually annotated and verified by a team of linguistic experts, and a POS tagging guideline was defined for annotating the data.</li>
<li>results: The paper evaluates the quality of CPPOS by training various deep learning models, such as the RNN family, on the constructed corpus. The results show that the model trained on CPPOS outperforms other existing Persian POS corpora and tools, achieving a 14% improvement over the previous dataset.<details>
<summary>Abstract</summary>
Introduction: Part-of-Speech (POS) Tagging, the process of classifying words into their respective parts of speech (e.g., verb or noun), is essential in various natural language processing applications. POS tagging is a crucial preprocessing task for applications like machine translation, question answering, sentiment analysis, etc. However, existing corpora for POS tagging in Persian mainly consist of formal texts, such as daily news and newspapers. As a result, smart POS tools, machine learning models, and deep learning models trained on these corpora may not perform optimally for processing colloquial text in social network analysis. Method: This paper introduces a novel corpus, "Colloquial Persian POS" (CPPOS), specifically designed to support colloquial Persian text. The corpus includes formal and informal text collected from various domains such as political, social, and commercial on Telegram, Twitter, and Instagram more than 520K labeled tokens. After collecting posts from these social platforms for one year, special preprocessing steps were conducted, including normalization, sentence tokenizing, and word tokenizing for social text. The tokens and sentences were then manually annotated and verified by a team of linguistic experts. This study also defines a POS tagging guideline for annotating the data and conducting the annotation process. Results: To evaluate the quality of CPPOS, various deep learning models, such as the RNN family, were trained using the constructed corpus. A comparison with another well-known Persian POS corpus named "Bijankhan" and the Persian Hazm POS tool trained on Bijankhan revealed that our model trained on CPPOS outperforms them. With the new corpus and the BiLSTM deep neural model, we achieved a 14% improvement over the previous dataset.
</details>
<details>
<summary>摘要</summary>
Introduction: 部件之分标记（POS）标注，将词语分类为它们的各种部件（如动词或名词），是自然语言处理应用中的重要预处理任务。POS标注是机器翻译、问答、情感分析等应用中的关键预处理任务。然而，现有的波斯语POS标注 corpora主要由正式文本组成，如日报和报纸。这导致了聪明POS工具、机器学习模型和深度学习模型在处理社交网络分析中的混乱文本时可能不具备最佳性能。方法：本文介绍了一个新的 corpora，名为“通用波斯语POS”（CPPOS），用于支持通用波斯语文本。该 corpora 包括了正式和非正式文本，从各种领域，如政治、社会和商业，收集自 Telegram、Twitter 和 Instagram 等社交平台上的大于520K个标注的字符。在收集一年的社交媒体文本后，我们进行了特殊的预处理步骤，包括Normalization、句子分割和词语分割。这些字符和句子 THEN 被一群语言专家 manually annotate 和验证。本研究还定义了POS标注指南，用于标注数据并进行标注过程。结果：为评估 CPPOS 的质量，我们使用constructed corpora  trains 了多种深度学习模型，如 RNN 家族。与另一个已知的波斯语POS corpus名为“ Bijankhan” 和 Persian Hazm POS 工具在 Bijankhan 上训练而成的模型相比，我们的模型在 CPPOS 上训练的结果表明，我们的模型在 CPPOS 上训练的结果表明，我们的模型在 CPPOS 上训练的结果比之前的数据集提高了14%。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/01/cs.CL_2023_10_01/" data-id="clp869ttr00cok5889h2x5ofl" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_10_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/01/cs.LG_2023_10_01/" class="article-date">
  <time datetime="2023-10-01T10:00:00.000Z" itemprop="datePublished">2023-10-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/01/cs.LG_2023_10_01/">cs.LG - 2023-10-01</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Determining-the-Optimal-Number-of-Clusters-for-Time-Series-Datasets-with-Symbolic-Pattern-Forest"><a href="#Determining-the-Optimal-Number-of-Clusters-for-Time-Series-Datasets-with-Symbolic-Pattern-Forest" class="headerlink" title="Determining the Optimal Number of Clusters for Time Series Datasets with Symbolic Pattern Forest"></a>Determining the Optimal Number of Clusters for Time Series Datasets with Symbolic Pattern Forest</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00820">http://arxiv.org/abs/2310.00820</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Nishat Raihan</li>
<li>for: 该论文的目的是提出一种基于Symbolic Pattern Forest（SPF）算法的时间序列嵌入分类方法，以确定时间序列数据集中的优化数量分支。</li>
<li>methods: 该方法使用了SPF算法生成时间序列数据集中的嵌入分类结果，并根据Silhouette系数选择优化数量分支。Silhouette系数在bag of word vector和tf-idf vector两个方面进行计算。</li>
<li>results: 对于UCRLibrary数据集，该方法实验结果表明与基准相比有显著改善。<details>
<summary>Abstract</summary>
Clustering algorithms are among the most widely used data mining methods due to their exploratory power and being an initial preprocessing step that paves the way for other techniques. But the problem of calculating the optimal number of clusters (say k) is one of the significant challenges for such methods. The most widely used clustering algorithms like k-means and k-shape in time series data mining also need the ground truth for the number of clusters that need to be generated. In this work, we extended the Symbolic Pattern Forest algorithm, another time series clustering algorithm, to determine the optimal number of clusters for the time series datasets. We used SPF to generate the clusters from the datasets and chose the optimal number of clusters based on the Silhouette Coefficient, a metric used to calculate the goodness of a clustering technique. Silhouette was calculated on both the bag of word vectors and the tf-idf vectors generated from the SAX words of each time series. We tested our approach on the UCR archive datasets, and our experimental results so far showed significant improvement over the baseline.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ECG-SL-Electrocardiogram-ECG-Segment-Learning-a-deep-learning-method-for-ECG-signal"><a href="#ECG-SL-Electrocardiogram-ECG-Segment-Learning-a-deep-learning-method-for-ECG-signal" class="headerlink" title="ECG-SL: Electrocardiogram(ECG) Segment Learning, a deep learning method for ECG signal"></a>ECG-SL: Electrocardiogram(ECG) Segment Learning, a deep learning method for ECG signal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00818">http://arxiv.org/abs/2310.00818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Yu, Huiyuan Yang, Akane Sano</li>
<li>for: 本研究旨在使用深度学习模型以优化心跳信号的分析，提高心跳信号的诊断精度。</li>
<li>methods: 本研究提出了一种基于心跳段分的ECG-Segment based Learning（ECG-SL）框架，从心跳段中提取了结构特征，并使用时间模型学习时间信息。此外，还explored一种自动标注的自我超vised学习策略以预训练模型，从而提高下游任务的性能。</li>
<li>results: 对于三种临床应用（心脏病诊断、呼吸暂停检测和cardiac arrhythmia分类），ECG-SL方法显示了与基eline模型和任务特定方法相比的竞争性表现。此外，通过分心跳 segments的Visualization Map可以看到ECG-SL方法更强调每个心跳的峰值和ST范围。<details>
<summary>Abstract</summary>
Electrocardiogram (ECG) is an essential signal in monitoring human heart activities. Researchers have achieved promising results in leveraging ECGs in clinical applications with deep learning models. However, the mainstream deep learning approaches usually neglect the periodic and formative attribute of the ECG heartbeat waveform. In this work, we propose a novel ECG-Segment based Learning (ECG-SL) framework to explicitly model the periodic nature of ECG signals. More specifically, ECG signals are first split into heartbeat segments, and then structural features are extracted from each of the segments. Based on the structural features, a temporal model is designed to learn the temporal information for various clinical tasks. Further, due to the fact that massive ECG signals are available but the labeled data are very limited, we also explore self-supervised learning strategy to pre-train the models, resulting significant improvement for downstream tasks. The proposed method outperforms the baseline model and shows competitive performances compared with task-specific methods in three clinical applications: cardiac condition diagnosis, sleep apnea detection, and arrhythmia classification. Further, we find that the ECG-SL tends to focus more on each heartbeat's peak and ST range than ResNet by visualizing the saliency maps.
</details>
<details>
<summary>摘要</summary>
电心图（ECG）是人类心脏活动监测中的关键信号。研究人员在临床应用中已经取得了深受欢迎的结果，使用深度学习模型。然而，主流深度学习方法通常忽略ECG心跳波形的周期性和结构特征。在这项工作中，我们提出了一种基于ECG心跳分割的学习框架（ECG-SL），以明确ECG信号的周期性。具体来说，ECG信号首先被分割成心跳分割，然后从每个分割中提取结构特征。基于这些结构特征，我们设计了一个时间模型，以学习不同临床任务中的时间信息。由于大量的ECG信号 disponible，但标注数据却很有限，因此我们还探索了自动学习策略，以预训练模型，从而实现了显著的提升。我们的方法超过基线模型，并与特定任务方法相比，在三种临床应用中（心脏病诊断、呼吸暂停检测和心动过速分类）显示了竞争力。此外，我们发现ECG-SL在每个心跳的峰值和ST范围方面更加强调，相比ResNet。我们可以通过Visualize saliency maps来见到这一点。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Make-Adherence-Aware-Advice"><a href="#Learning-to-Make-Adherence-Aware-Advice" class="headerlink" title="Learning to Make Adherence-Aware Advice"></a>Learning to Make Adherence-Aware Advice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00817">http://arxiv.org/abs/2310.00817</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guanting Chen, Xiaocheng Li, Chunlin Sun, Hanzhao Wang</li>
<li>for: 这篇论文目的是提出一个序推问题解决模型，以满足人类与人工智能（AI）之间的互动挑战。</li>
<li>methods: 这个模型考虑了人类的遵循度（机器建议被接受或拒绝的可能性），并提供了一个折补选项，以便机器在重要时刻提供建议。这篇论文还提供了专门的学习算法，以学习最佳建议策略，并只在重要时刻提供建议。</li>
<li>results: 与问题独立的算法相比，这篇论文的专门学习算法不仅具有更好的理论均衡性，还在实验中显示出强大的表现。<details>
<summary>Abstract</summary>
As artificial intelligence (AI) systems play an increasingly prominent role in human decision-making, challenges surface in the realm of human-AI interactions. One challenge arises from the suboptimal AI policies due to the inadequate consideration of humans disregarding AI recommendations, as well as the need for AI to provide advice selectively when it is most pertinent. This paper presents a sequential decision-making model that (i) takes into account the human's adherence level (the probability that the human follows/rejects machine advice) and (ii) incorporates a defer option so that the machine can temporarily refrain from making advice. We provide learning algorithms that learn the optimal advice policy and make advice only at critical time stamps. Compared to problem-agnostic reinforcement learning algorithms, our specialized learning algorithms not only enjoy better theoretical convergence properties but also show strong empirical performance.
</details>
<details>
<summary>摘要</summary>
artificial intelligence (AI) 系统在人类做出决策中扮演越来越重要的角色，但是人类与 AI 之间的互动问题开始浮现。一个挑战是由于 AI 策略不够佳，人类可能不会遵循 AI 的建议，同时 AI 需要提供建议时机选择性地为人类提供建议。这篇论文提出了一个顺序决策模型，该模型（i）考虑人类遵循度（机器建议被接受或拒绝的概率），（ii）将机器给出建议的时间点选择性地推荐。我们提供了特殊的学习算法，这些算法不仅具有更好的理论均衡性，并且在实验中表现出色。相比问题agnostic 征求学习算法，我们的特殊学习算法不仅具有更好的理论均衡性，而且在实验中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Design-Principles-for-Frequentist-Sequential-Learning"><a href="#Bayesian-Design-Principles-for-Frequentist-Sequential-Learning" class="headerlink" title="Bayesian Design Principles for Frequentist Sequential Learning"></a>Bayesian Design Principles for Frequentist Sequential Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00806">http://arxiv.org/abs/2310.00806</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xuyunbei/mab-code">https://github.com/xuyunbei/mab-code</a></li>
<li>paper_authors: Yunbei Xu, Assaf Zeevi<br>for:这篇论文的目的是优化频繁ister regret for sequential learning problems，并提供了一种总结 Bayesian principles的通用理论。methods:论文使用了一种新的优化方法，即“algorithmic beliefs”的生成，以及基于 Bayesian posteriors 的决策。results:论文提出了一种新的算法，可以在随机、对抗和不同环境下实现“best-of-all-worlds”的 empirical performance。此外，这些原理还可以应用于线性 bandits、bandit convex optimization 和 reinforcement learning。<details>
<summary>Abstract</summary>
We develop a general theory to optimize the frequentist regret for sequential learning problems, where efficient bandit and reinforcement learning algorithms can be derived from unified Bayesian principles. We propose a novel optimization approach to generate "algorithmic beliefs" at each round, and use Bayesian posteriors to make decisions. The optimization objective to create "algorithmic beliefs," which we term "Algorithmic Information Ratio," represents an intrinsic complexity measure that effectively characterizes the frequentist regret of any algorithm. To the best of our knowledge, this is the first systematical approach to make Bayesian-type algorithms prior-free and applicable to adversarial settings, in a generic and optimal manner. Moreover, the algorithms are simple and often efficient to implement. As a major application, we present a novel algorithm for multi-armed bandits that achieves the "best-of-all-worlds" empirical performance in the stochastic, adversarial, and non-stationary environments. And we illustrate how these principles can be used in linear bandits, bandit convex optimization, and reinforcement learning.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation note:* "frequentist regret" 改为 "频率 regret"* "sequential learning problems" 改为 "顺序学习问题"* "efficient bandit and reinforcement learning algorithms" 改为 "高效的随机抽象和奖励学习算法"* "Algorithmic Information Ratio" 改为 "算法信息比率"* "prior-free" 改为 "无先验"* "adversarial settings" 改为 "对抗设定"* "linear bandits" 改为 "线性随机抽象"* "bandit convex optimization" 改为 "随机抽象优化"* "reinforcement learning" 改为 "奖励学习")
</details></li>
</ul>
<hr>
<h2 id="Going-Beyond-Familiar-Features-for-Deep-Anomaly-Detection"><a href="#Going-Beyond-Familiar-Features-for-Deep-Anomaly-Detection" class="headerlink" title="Going Beyond Familiar Features for Deep Anomaly Detection"></a>Going Beyond Familiar Features for Deep Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00797">http://arxiv.org/abs/2310.00797</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarath Sivaprasad, Mario Fritz</li>
<li>For: The paper is written for detecting anomalies in deep learning models, specifically addressing the problem of false negatives caused by uncaptured novel features.* Methods: The paper proposes a novel approach to anomaly detection using explainability, which captures novel features as unexplained observations in the input space. The approach combines similarity and novelty in a hybrid approach, eliminating the need for expensive background models and dense matching.* Results: The paper achieves strong performance across a wide range of anomaly benchmarks, reducing false negative anomalies by up to 40% compared to the state-of-the-art. The method also provides visually inspectable explanations for pixel-level anomalies.Here are the three points in Simplified Chinese text:* For: 本文是为检测深度学习模型中的异常点而写的，特别是解决由未捕捉的新特征引起的假阳性问题。* Methods: 本文提出了一种新的异常检测方法，使用可解释性来捕捉新特征，并将相似性和新鲜度结合在一起。这种方法可以无需昂贵的背景模型和紧密匹配。* Results: 本文在多种异常标准benchmark上实现了优秀的表现，相比之前的状态态-of-the-art，减少了假阳性异常的比例达40%。此外，方法还提供了可视化的解释，用于检测像素级异常。<details>
<summary>Abstract</summary>
Anomaly Detection (AD) is a critical task that involves identifying observations that do not conform to a learned model of normality. Prior work in deep AD is predominantly based on a familiarity hypothesis, where familiar features serve as the reference in a pre-trained embedding space. While this strategy has proven highly successful, it turns out that it causes consistent false negatives when anomalies consist of truly novel features that are not well captured by the pre-trained encoding. We propose a novel approach to AD using explainability to capture novel features as unexplained observations in the input space. We achieve strong performance across a wide range of anomaly benchmarks by combining similarity and novelty in a hybrid approach. Our approach establishes a new state-of-the-art across multiple benchmarks, handling diverse anomaly types while eliminating the need for expensive background models and dense matching. In particular, we show that by taking account of novel features, we reduce false negative anomalies by up to 40% on challenging benchmarks compared to the state-of-the-art. Our method gives visually inspectable explanations for pixel-level anomalies.
</details>
<details>
<summary>摘要</summary>
异常检测（AD）是一项关键任务，它的目标是找到不符合学习的模型正常性的观察值。现有的深度AD研究大多基于 Familiarity 假设，即使用已经训练过的特征空间中的熟悉特征作为参考。然而，这种策略会导致常见的假阳性结果，即在异常值中包含未 capture 的新特征。我们提出一种基于解释力的新方法，可以捕捉输入空间中的新特征作为未解释的观察值。我们通过将相似性和新鲜度结合在一起来实现了一种混合方法，并在多个异常 benchmark 上达到了新的状态对领导地位。我们的方法可以处理多种异常类型，而无需购买贵重的背景模型和紧密匹配。特别是，我们发现通过考虑新特征，可以降低 false negative 异常值达到 40% 以上，相比之前的状态对领导地位。我们的方法还可以为像素级异常值提供可见的解释。
</details></li>
</ul>
<hr>
<h2 id="Categorizing-Flight-Paths-using-Data-Visualization-and-Clustering-Methodologies"><a href="#Categorizing-Flight-Paths-using-Data-Visualization-and-Clustering-Methodologies" class="headerlink" title="Categorizing Flight Paths using Data Visualization and Clustering Methodologies"></a>Categorizing Flight Paths using Data Visualization and Clustering Methodologies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00773">http://arxiv.org/abs/2310.00773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Song, Keyang Yu, Seth Young</li>
<li>for: 本研究使用美国联邦航空管理局的航空流管理系统数据和DV8工具来开发了飞行路径分 clustering算法，以分类不同的飞行路径。</li>
<li>methods: 研究使用了两种分 clustering方法：一种是基于空间地理准备的距离模型，另一种是基于向量cosine相似性模型。两种方法的比较和应用示例演示了自动 clustering结果决定和人工循环过程的成功应用。</li>
<li>results: 研究发现，基于地理距离模型在航道部分的 clustering效果较好，而基于cosine相似性模型在近端操作部分，如到达路径，的 clustering效果较好。此外，使用点抽象技术可以提高计算效率。<details>
<summary>Abstract</summary>
This work leverages the U.S. Federal Aviation Administration's Traffic Flow Management System dataset and DV8, a recently developed tool for highly interactive visualization of air traffic data, to develop clustering algorithms for categorizing air traffic by their varying flight paths. Two clustering methodologies, a spatial-based geographic distance model, and a vector-based cosine similarity model, are demonstrated and compared for their clustering effectiveness. Examples of their applications reveal successful, realistic clustering based on automated clustering result determination and human-in-the-loop processes, with geographic distance algorithms performing better for enroute portions of flight paths and cosine similarity algorithms performing better for near-terminal operations, such as arrival paths. A point extraction technique is applied to improve computation efficiency.
</details>
<details>
<summary>摘要</summary>
这项工作利用美国联邦航空管理局的交通流管理系统数据集和DV8工具，一种最近开发的高度互动式航空交通数据可视化工具，开发出 clustering 算法来分类不同的航空交通路径。我们示出了两种 clustering 方法，一种基于空间准备的地理距离模型，另一种基于向量的 косину similarity 模型，并对它们的划分效果进行比较。我们还提供了自动划分结果决定和人工循环过程的应用示例，其中地理距离算法在航道部分表现较好，而 косину similarity 算法在近机场操作，如进近路径，表现较好。此外，我们还应用了点提取技术来提高计算效率。
</details></li>
</ul>
<hr>
<h2 id="Data-Efficient-Power-Flow-Learning-for-Network-Contingencies"><a href="#Data-Efficient-Power-Flow-Learning-for-Network-Contingencies" class="headerlink" title="Data-Efficient Power Flow Learning for Network Contingencies"></a>Data-Efficient Power Flow Learning for Network Contingencies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00763">http://arxiv.org/abs/2310.00763</a></li>
<li>repo_url: None</li>
<li>paper_authors: Parikshit Pareek, Deepjyoti Deka, Sidhant Misra</li>
<li>for: 学习电网中网络异常情况下的电流流动和相应的可能性电压范围（PVE）。</li>
<li>methods: 使用一种网络感知的 Gaussian Process（GP）称为顶点度 kernel（VDK-GP）来估算电压-功率函数，并提出一种新的多任务顶点度 kernel（MT-VDK）来确定未经见过的电网中的电流流动。</li>
<li>results: 在IEEE 30-Bus 电网上进行了 simulations，发现MT-VDK-GP方法可以在低训练数据范围（50-250样本）下减少了平均预测错误的50%以上，并在75%以上的 N-2 停机网络结构中超过了基于超参数的传输学习方法。此外，MT-VDK-GP方法还可以使用64倍少的电流解题方法来实现PVE。<details>
<summary>Abstract</summary>
This work presents an efficient data-driven method to learn power flows in grids with network contingencies and to estimate corresponding probabilistic voltage envelopes (PVE). First, a network-aware Gaussian process (GP) termed Vertex-Degree Kernel (VDK-GP), developed in prior work, is used to estimate voltage-power functions for a few network configurations. The paper introduces a novel multi-task vertex degree kernel (MT-VDK) that amalgamates the learned VDK-GPs to determine power flows for unseen networks, with a significant reduction in the computational complexity and hyperparameter requirements compared to alternate approaches. Simulations on the IEEE 30-Bus network demonstrate the retention and transfer of power flow knowledge in both N-1 and N-2 contingency scenarios. The MT-VDK-GP approach achieves over 50% reduction in mean prediction error for novel N-1 contingency network configurations in low training data regimes (50-250 samples) over VDK-GP. Additionally, MT-VDK-GP outperforms a hyper-parameter based transfer learning approach in over 75% of N-2 contingency network structures, even without historical N-2 outage data. The proposed method demonstrates the ability to achieve PVEs using sixteen times fewer power flow solutions compared to Monte-Carlo sampling-based methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Data-driven-adaptive-building-thermal-controller-tuning-with-constraints-A-primal-dual-contextual-Bayesian-optimization-approach"><a href="#Data-driven-adaptive-building-thermal-controller-tuning-with-constraints-A-primal-dual-contextual-Bayesian-optimization-approach" class="headerlink" title="Data-driven adaptive building thermal controller tuning with constraints: A primal-dual contextual Bayesian optimization approach"></a>Data-driven adaptive building thermal controller tuning with constraints: A primal-dual contextual Bayesian optimization approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00758">http://arxiv.org/abs/2310.00758</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjie Xu, Bratislav Svetozarevic, Loris Di Natale, Philipp Heer, Colin N Jones</li>
<li>for: 本文 targets the problem of minimizing the energy consumption of a room temperature controller while ensuring the daily cumulative thermal discomfort of occupants is below a given threshold.</li>
<li>methods: 本文提出了一种数据驱动的逻辑-对抗搜索（PDCBO）方法来解决这个问题。</li>
<li>results: 在一个单个房间的 simulate case study中，我们运用了我们的算法来调整PI饱和预热时间的参数，并获得了至多4.7%的能源减少，同时保证每天的温室不超过给定的快速阈值。此外，PDCBO还可以自动跟踪时间变化的快速阈值，而其他方法无法完成这一任务。<details>
<summary>Abstract</summary>
We study the problem of tuning the parameters of a room temperature controller to minimize its energy consumption, subject to the constraint that the daily cumulative thermal discomfort of the occupants is below a given threshold. We formulate it as an online constrained black-box optimization problem where, on each day, we observe some relevant environmental context and adaptively select the controller parameters. In this paper, we propose to use a data-driven Primal-Dual Contextual Bayesian Optimization (PDCBO) approach to solve this problem. In a simulation case study on a single room, we apply our algorithm to tune the parameters of a Proportional Integral (PI) heating controller and the pre-heating time. Our results show that PDCBO can save up to 4.7% energy consumption compared to other state-of-the-art Bayesian optimization-based methods while keeping the daily thermal discomfort below the given tolerable threshold on average. Additionally, PDCBO can automatically track time-varying tolerable thresholds while existing methods fail to do so. We then study an alternative constrained tuning problem where we aim to minimize the thermal discomfort with a given energy budget. With this formulation, PDCBO reduces the average discomfort by up to 63% compared to state-of-the-art safe optimization methods while keeping the average daily energy consumption below the required threshold.
</details>
<details>
<summary>摘要</summary>
我们研究控制室内温度的参数来减少能源消耗，并且保持每天累累感觉下限。我们将这个问题转化为线上受限制的黑盒优化问题，每天我们可以观察环境上的一些相关数据，然后选择参数。在这篇论文中，我们提出使用基于Primal-Dual Contextual Bayesian Optimization（PDCBO）的数据驱动方法来解决这个问题。在单一房间的实验案例中，我们使用我们的算法来调整PI适应器和预热时间的参数。我们的结果显示，PDCBO可以与其他现有的Bayesian优化基于方法相比，在平均每天的能源消耗下降4.7%，同时保持每天累累感觉下限。此外，PDCBO可以自动跟踪时间变化的受限制耐受阈值，而现有的方法则无法实现这一点。然后，我们研究一个受限制的问题，即将累累感觉降到最低，并且保持每天能源消耗在所需的预算下。这个问题中，PDCBO可以在平均每天的累累感觉下降63%，同时保持每天能源消耗在所需的预算下。
</details></li>
</ul>
<hr>
<h2 id="Identifying-Copeland-Winners-in-Dueling-Bandits-with-Indifferences"><a href="#Identifying-Copeland-Winners-in-Dueling-Bandits-with-Indifferences" class="headerlink" title="Identifying Copeland Winners in Dueling Bandits with Indifferences"></a>Identifying Copeland Winners in Dueling Bandits with Indifferences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00750">http://arxiv.org/abs/2310.00750</a></li>
<li>repo_url: None</li>
<li>paper_authors: Viktor Bengs, Björn Haddenhorst, Eyke Hüllermeier</li>
<li>for: 本文研究了一种叫做“对抗炮手问题”的概念，其中一个玩家可以通过选择一个或多个“炮手”来获得奖励。本文是关于这种问题的一种特殊情况，即在玩家提供反馈时，可能会出现“无偏好”的情况。</li>
<li>methods: 本文提出了一种名为POCOWISTA的算法，该算法可以寻找玩家的 Copeland 赢家（即最佳炮手）。此外，本文还提供了一个lower bound的下界，表明任何学习算法都需要至少这么多样本来找到 Copeland 赢家。</li>
<li>results: 本文的实验结果表明，POCOWISTA 算法在实际中表现出色，它可以快速寻找玩家的 Copeland 赢家，并且在普通的对抗炮手问题中也有优秀的表现。此外，如果 preference probabilities 满足一种特殊的随机对称性条件，则可以提供一个改进的 worst-case 下界。<details>
<summary>Abstract</summary>
We consider the task of identifying the Copeland winner(s) in a dueling bandits problem with ternary feedback. This is an underexplored but practically relevant variant of the conventional dueling bandits problem, in which, in addition to strict preference between two arms, one may observe feedback in the form of an indifference. We provide a lower bound on the sample complexity for any learning algorithm finding the Copeland winner(s) with a fixed error probability. Moreover, we propose POCOWISTA, an algorithm with a sample complexity that almost matches this lower bound, and which shows excellent empirical performance, even for the conventional dueling bandits problem. For the case where the preference probabilities satisfy a specific type of stochastic transitivity, we provide a refined version with an improved worst case sample complexity.
</details>
<details>
<summary>摘要</summary>
我们考虑了在战斗炮手问题中确定科普兰赢家的任务，这是一种未得到充分研究但实际上很有实际意义的战斗炮手问题变种，在这种变种中，除了简单的首选之外，还可能观察到反馈形式的半同意。我们提供了确定科普兰赢家的样本复杂度下界，以及一种名为POCOWISTA的算法，该算法的样本复杂度几乎与下界匹配，并在实际中表现出色，包括传统的战斗炮手问题。在首选概率满足特定的随机transitivity性时，我们提供了一种改进的启发版本，其 worst case 样本复杂度得到改进。
</details></li>
</ul>
<hr>
<h2 id="SEED-Simple-Efficient-and-Effective-Data-Management-via-Large-Language-Models"><a href="#SEED-Simple-Efficient-and-Effective-Data-Management-via-Large-Language-Models" class="headerlink" title="SEED: Simple, Efficient, and Effective Data Management via Large Language Models"></a>SEED: Simple, Efficient, and Effective Data Management via Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00749">http://arxiv.org/abs/2310.00749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zui CHen, Lei Cao, Sam Madden, Ju Fan, Nan Tang, Zihui Gu, Zeyuan Shang, Chunwei Liu, Michael Cafarella, Tim Kraska</li>
<li>for: This paper aims to provide an efficient and effective data management system for large language models (LLMs) by addressing the challenges of computational and economic expense.</li>
<li>methods: The paper proposes a system called SEED, which consists of three main components: code generation, model generation, and augmented LLM query. SEED localizes LLM computation as much as possible, uses optimization techniques to enhance the localized solution and LLM queries, and allows users to easily construct a customized data management solution.</li>
<li>results: The paper achieves state-of-the-art few-shot performance while significantly reducing the number of required LLM calls for diverse data management tasks such as data imputation and NL2SQL translation.<details>
<summary>Abstract</summary>
We introduce SEED, an LLM-centric system that allows users to easily create efficient, and effective data management applications. SEED comprises three main components: code generation, model generation, and augmented LLM query to address the challenges that LLM services are computationally and economically expensive and do not always work well on all cases for a given data management task. SEED addresses the expense challenge by localizing LLM computation as much as possible. This includes replacing most of LLM calls with local code, local models, and augmenting LLM queries with batching and data access tools, etc. To ensure effectiveness, SEED features a bunch of optimization techniques to enhance the localized solution and the LLM queries, including automatic code validation, code ensemble, model representatives selection, selective tool usages, etc. Moreover, with SEED users are able to easily construct a data management solution customized to their applications. It allows the users to configure each component and compose an execution pipeline in natural language. SEED then automatically compiles it into an executable program. We showcase the efficiency and effectiveness of SEED using diverse data management tasks such as data imputation, NL2SQL translation, etc., achieving state-of-the-art few-shot performance while significantly reducing the number of required LLM calls.
</details>
<details>
<summary>摘要</summary>
我们介绍SEED系统，它是基于LLM的系统，让用户可以轻松地创建高效、高效的数据管理应用程序。SEED包括三个主要 ком成分：代码生成、模型生成和增强LLM查询。这些 ком成分是为了解决LLM服务 computationally和经济成本高，并且不一定在所有情况下能够实现数据管理任务。SEED通过地方化LLM计算来解决这个挑战，包括将大多数LLM请求替换为本地代码、本地模型和增强LLM查询批处理等。为了保证效果，SEED具有许多优化技术，包括自动验证代码、代码合并、模型选择、选择工具使用等。此外，SEED还允许用户轻松地建立自定义的数据管理解决方案，并且可以自然语言中 configurations 和构成执行管线。SEED 将自动将其转换为可执行程式。我们透过使用多种数据管理任务，例如数据补充、NL2SQL翻译等，得到了状况之中的几个shot性能，同时对LLM请求数量进行了重要削减。
</details></li>
</ul>
<hr>
<h2 id="Deterministic-Langevin-Unconstrained-Optimization-with-Normalizing-Flows"><a href="#Deterministic-Langevin-Unconstrained-Optimization-with-Normalizing-Flows" class="headerlink" title="Deterministic Langevin Unconstrained Optimization with Normalizing Flows"></a>Deterministic Langevin Unconstrained Optimization with Normalizing Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00745">http://arxiv.org/abs/2310.00745</a></li>
<li>repo_url: None</li>
<li>paper_authors: James M. Sullivan, Uros Seljak</li>
<li>for: 该论文旨在开发一种全球、不使用梯度的优化策略，用于解决costly黑桶函数问题。</li>
<li>methods: 该方法基于Fokker-Planck和Langevin方程，并且利用Normalizing Flow来进行活动学习和选择提案点。</li>
<li>results: 该方法在标准的synthetic测试函数上实现了superior或竞争性的进步，并在实际的科学和神经网络优化问题上达到了竞争性的result。<details>
<summary>Abstract</summary>
We introduce a global, gradient-free surrogate optimization strategy for expensive black-box functions inspired by the Fokker-Planck and Langevin equations. These can be written as an optimization problem where the objective is the target function to maximize minus the logarithm of the current density of evaluated samples. This objective balances exploitation of the target objective with exploration of low-density regions. The method, Deterministic Langevin Optimization (DLO), relies on a Normalizing Flow density estimate to perform active learning and select proposal points for evaluation. This strategy differs qualitatively from the widely-used acquisition functions employed by Bayesian Optimization methods, and can accommodate a range of surrogate choices. We demonstrate superior or competitive progress toward objective optima on standard synthetic test functions, as well as on non-convex and multi-modal posteriors of moderate dimension. On real-world objectives, such as scientific and neural network hyperparameter optimization, DLO is competitive with state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种全球、gradient-free的优化策略，用于优化costly黑obox函数。这种策略 Draw inspiration from the Fokker-Planck and Langevin equations, and can be formulated as an optimization problem where the objective is to maximize the target function minus the logarithm of the current density of evaluated samples. This objective balances the exploitation of the target objective with the exploration of low-density regions. Our method, Deterministic Langevin Optimization (DLO), uses a Normalizing Flow density estimate to perform active learning and select proposal points for evaluation. This strategy differs qualitatively from the widely-used acquisition functions employed by Bayesian Optimization methods, and can accommodate a range of surrogate choices. We demonstrate superior or competitive progress toward objective optima on standard synthetic test functions, as well as on non-convex and multi-modal posteriors of moderate dimension. On real-world objectives, such as scientific and neural network hyperparameter optimization, DLO is competitive with state-of-the-art baselines.
</details></li>
</ul>
<hr>
<h2 id="Spectral-Neural-Networks-Approximation-Theory-and-Optimization-Landscape"><a href="#Spectral-Neural-Networks-Approximation-Theory-and-Optimization-Landscape" class="headerlink" title="Spectral Neural Networks: Approximation Theory and Optimization Landscape"></a>Spectral Neural Networks: Approximation Theory and Optimization Landscape</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00729">http://arxiv.org/abs/2310.00729</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenghui Li, Rishi Sonthalia, Nicolas Garcia Trillos</li>
<li>for: This paper investigates the theoretical aspects of Spectral Neural Networks (SNN) and their tradeoffs with respect to the number of neurons and the amount of spectral geometric information learned.</li>
<li>methods: The paper uses a theoretical approach to explore the optimization landscape of SNN’s objective function, shedding light on the training dynamics of SNN and its non-convex ambient loss function.</li>
<li>results: The paper presents quantitative insights into the tradeoff between the number of neurons and the amount of spectral geometric information a neural network learns, and initiates a theoretical exploration of the training dynamics of SNN.<details>
<summary>Abstract</summary>
There is a large variety of machine learning methodologies that are based on the extraction of spectral geometric information from data. However, the implementations of many of these methods often depend on traditional eigensolvers, which present limitations when applied in practical online big data scenarios. To address some of these challenges, researchers have proposed different strategies for training neural networks as alternatives to traditional eigensolvers, with one such approach known as Spectral Neural Network (SNN). In this paper, we investigate key theoretical aspects of SNN. First, we present quantitative insights into the tradeoff between the number of neurons and the amount of spectral geometric information a neural network learns. Second, we initiate a theoretical exploration of the optimization landscape of SNN's objective to shed light on the training dynamics of SNN. Unlike typical studies of convergence to global solutions of NN training dynamics, SNN presents an additional complexity due to its non-convex ambient loss function.
</details>
<details>
<summary>摘要</summary>
有很多机器学习方法基于数据中特征几何信息的提取，但是许多实现方法常常依赖于传统的特征值解决方案，这些解决方案在实际上线大数据场景中存在限制。为了解决这些挑战，研究人员已经提议了不同的替代方案，其中一种是叫做特征神经网络（SNN）。在这篇论文中，我们调查了SNN的关键理论方面。首先，我们提供了量化的视角，描述了神经网络学习过程中特征几何信息和神经元数之间的负反关系。其次，我们开始了SNN目标函数优化境地的理论探索，以便更好地理解SNN训练过程的动态。不同于传统的NN训练动态研究，SNN增加了非对称的抽象损失函数，使其训练动态更加复杂。
</details></li>
</ul>
<hr>
<h2 id="Physics-Informed-Graph-Neural-Network-for-Dynamic-Reconfiguration-of-Power-Systems"><a href="#Physics-Informed-Graph-Neural-Network-for-Dynamic-Reconfiguration-of-Power-Systems" class="headerlink" title="Physics-Informed Graph Neural Network for Dynamic Reconfiguration of Power Systems"></a>Physics-Informed Graph Neural Network for Dynamic Reconfiguration of Power Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00728">http://arxiv.org/abs/2310.00728</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jules Authier, Rabab Haider, Anuradha Annaswamy, Florian Dorfler</li>
<li>for: 这个论文是为了解决动态重配置（DyR）问题的快速决策算法。DyR 是一个扩展到大规模网格和快速时间步骤的混合整数问题，可能是计算 tractable 的问题。</li>
<li>methods: 该论文提出了一种基于物理学习树（GNNs）框架的 GraPhyR，用于解决 DyR 问题。该框架直接包含了操作和连接约束，并通过练习策略来训练。</li>
<li>results: 论文的结果表明，GraPhyR 能够学习解决 DyR 问题，并且比传统的方法更快和更有效。<details>
<summary>Abstract</summary>
To maintain a reliable grid we need fast decision-making algorithms for complex problems like Dynamic Reconfiguration (DyR). DyR optimizes distribution grid switch settings in real-time to minimize grid losses and dispatches resources to supply loads with available generation. DyR is a mixed-integer problem and can be computationally intractable to solve for large grids and at fast timescales. We propose GraPhyR, a Physics-Informed Graph Neural Network (GNNs) framework tailored for DyR. We incorporate essential operational and connectivity constraints directly within the GNN framework and train it end-to-end. Our results show that GraPhyR is able to learn to optimize the DyR task.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:维护可靠的电网需要快速的决策算法来解决复杂的问题，如动态重组（DyR）。DyR在实时中 ottimize 分配网络设置，以最小化电网损失和将资源派发给可用的生产。DyR是一个混合整数问题，可能需要大量的计算时间和复杂的数据分析。我们提出了 GraPhyR，一个基于物理网络学习（GNNs）框架，特别适合DyR。我们直接将运作和连接约束 integrate 到 GNN 框架中，并将其训练成一个终端解决方案。我们的结果显示，GraPhyR 能够学习来优化 DyR 任务。
</details></li>
</ul>
<hr>
<h2 id="Learning-How-to-Propagate-Messages-in-Graph-Neural-Networks"><a href="#Learning-How-to-Propagate-Messages-in-Graph-Neural-Networks" class="headerlink" title="Learning How to Propagate Messages in Graph Neural Networks"></a>Learning How to Propagate Messages in Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00697">http://arxiv.org/abs/2310.00697</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tengxiao1/l2p">https://github.com/tengxiao1/l2p</a></li>
<li>paper_authors: Teng Xiao, Zhengyu Chen, Donglin Wang, Suhang Wang</li>
<li>for: 这个论文研究了图神经网络（GNNs）中的信息传播策略学习问题。</li>
<li>methods: 该论文提出了一种通用学习框架，可以不仅学习GNN参数进行预测，而且可以显式地学习不同节点和不同图类型的可解释性和个性化的传播策略。</li>
<li>results: 经验表明，该提议的框架可以在不同类型的图benchmark上显著提高性能，并可以有效地学习GNN中的可解释性和个性化的传播策略。<details>
<summary>Abstract</summary>
This paper studies the problem of learning message propagation strategies for graph neural networks (GNNs). One of the challenges for graph neural networks is that of defining the propagation strategy. For instance, the choices of propagation steps are often specialized to a single graph and are not personalized to different nodes. To compensate for this, in this paper, we present learning to propagate, a general learning framework that not only learns the GNN parameters for prediction but more importantly, can explicitly learn the interpretable and personalized propagate strategies for different nodes and various types of graphs. We introduce the optimal propagation steps as latent variables to help find the maximum-likelihood estimation of the GNN parameters in a variational Expectation-Maximization (VEM) framework. Extensive experiments on various types of graph benchmarks demonstrate that our proposed framework can significantly achieve better performance compared with the state-of-the-art methods, and can effectively learn personalized and interpretable propagate strategies of messages in GNNs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-Noise-Geometry-of-Stochastic-Gradient-Descent-A-Quantitative-and-Analytical-Characterization"><a href="#The-Noise-Geometry-of-Stochastic-Gradient-Descent-A-Quantitative-and-Analytical-Characterization" class="headerlink" title="The Noise Geometry of Stochastic Gradient Descent: A Quantitative and Analytical Characterization"></a>The Noise Geometry of Stochastic Gradient Descent: A Quantitative and Analytical Characterization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00692">http://arxiv.org/abs/2310.00692</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingze Wang, Lei Wu</li>
<li>for: 本研究旨在理解梯度下降法（SGD）中噪声的地理学性质，并提供了对于过参数化线性模型（OLMs）和层次神经网络的全面理论分析。</li>
<li>methods: 本研究使用了平均和方向含义的对比，特别是考虑样本大小和输入数据缺乏对对焊的影响。</li>
<li>results: 研究发现，SGD在梯度下降过程中噪声会与损失函数的本地几何相似，并且SGD在避免锐 minimum 的过程中会选择平行于损失函数的平坦方向进行跃点。这与梯度下降法不同，后者只能逃脱锐 minimum 方向。实验 validate 了理论发现。<details>
<summary>Abstract</summary>
Empirical studies have demonstrated that the noise in stochastic gradient descent (SGD) aligns favorably with the local geometry of loss landscape. However, theoretical and quantitative explanations for this phenomenon remain sparse. In this paper, we offer a comprehensive theoretical investigation into the aforementioned {\em noise geometry} for over-parameterized linear (OLMs) models and two-layer neural networks. We scrutinize both average and directional alignments, paying special attention to how factors like sample size and input data degeneracy affect the alignment strength. As a specific application, we leverage our noise geometry characterizations to study how SGD escapes from sharp minima, revealing that the escape direction has significant components along flat directions. This is in stark contrast to GD, which escapes only along the sharpest directions. To substantiate our theoretical findings, both synthetic and real-world experiments are provided.
</details>
<details>
<summary>摘要</summary>
empirical studies have shown that the noise in stochastic gradient descent (SGD) is aligned with the local geometry of the loss landscape. however, there is a lack of theoretical and quantitative explanations for this phenomenon. in this paper, we provide a comprehensive theoretical investigation into the "noise geometry" of over-parameterized linear (OLMs) models and two-layer neural networks. we examine both average and directional alignments, paying special attention to how factors such as sample size and input data degeneracy affect the alignment strength. as a specific application, we use our noise geometry characterizations to study how SGD escapes from sharp minima, revealing that the escape direction has significant components along flat directions. this is in stark contrast to GD, which escapes only along the sharpest directions. to substantiate our theoretical findings, we provide both synthetic and real-world experiments.
</details></li>
</ul>
<hr>
<h2 id="PharmacoNet-Accelerating-Large-Scale-Virtual-Screening-by-Deep-Pharmacophore-Modeling"><a href="#PharmacoNet-Accelerating-Large-Scale-Virtual-Screening-by-Deep-Pharmacophore-Modeling" class="headerlink" title="PharmacoNet: Accelerating Large-Scale Virtual Screening by Deep Pharmacophore Modeling"></a>PharmacoNet: Accelerating Large-Scale Virtual Screening by Deep Pharmacophore Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00681">http://arxiv.org/abs/2310.00681</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/seonghwanseo/pharmaconet">https://github.com/seonghwanseo/pharmaconet</a></li>
<li>paper_authors: Seonghwan Seo, Woo Youn Kim</li>
<li>for: 该研究旨在开发一种高效的结构基Virtual Screening方法，以应对越来越大的访问ible compound库。</li>
<li>methods: 该方法使用深度学习框架，通过识别稳定结合的3D药物配置，从binding site中预测药物和蛋白质的绑定pose。通过粗粒度图匹配，一步解决了现有方法中的昂贵绑定pose采样和评分过程。</li>
<li>results: 对比现有方法，PharmacoNet显示了更高的速度和更好的准确性，同时能够保留高过滤率下的hit候选者。研究发现，深度学习基于药物搜寻的方法可以激活未探索的药物搜寻潜力。<details>
<summary>Abstract</summary>
As the size of accessible compound libraries expands to over 10 billion, the need for more efficient structure-based virtual screening methods is emerging. Different pre-screening methods have been developed to rapidly screen the library, but the structure-based methods applicable to general proteins are still lacking: the challenge is to predict the binding pose between proteins and ligands and perform scoring in an extremely short time. We introduce PharmacoNet, a deep learning framework that identifies the optimal 3D pharmacophore arrangement which a ligand should have for stable binding from the binding site. By coarse-grained graph matching between ligands and the generated pharmacophore arrangement, we solve the expensive binding pose sampling and scoring procedures of existing methods in a single step. PharmacoNet is significantly faster than state-of-the-art structure-based approaches, yet reasonably accurate with a simple scoring function. Furthermore, we show the promising result that PharmacoNet effectively retains hit candidates even under the high pre-screening filtration rates. Overall, our study uncovers the hitherto untapped potential of a pharmacophore modeling approach in deep learning-based drug discovery.
</details>
<details>
<summary>摘要</summary>
We introduce PharmacoNet, a deep learning framework that identifies the optimal 3D pharmacophore arrangement for stable binding from the binding site. By coarse-grained graph matching between ligands and the generated pharmacophore arrangement, we eliminate the expensive binding pose sampling and scoring procedures of existing methods in a single step. PharmacoNet is significantly faster than state-of-the-art structure-based approaches, yet reasonably accurate with a simple scoring function.Moreover, we show that PharmacoNet effectively retains hit candidates even under high pre-screening filtration rates. Our study uncovers the hitherto untapped potential of a pharmacophore modeling approach in deep learning-based drug discovery.
</details></li>
</ul>
<hr>
<h2 id="A-General-Offline-Reinforcement-Learning-Framework-for-Interactive-Recommendation"><a href="#A-General-Offline-Reinforcement-Learning-Framework-for-Interactive-Recommendation" class="headerlink" title="A General Offline Reinforcement Learning Framework for Interactive Recommendation"></a>A General Offline Reinforcement Learning Framework for Interactive Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00678">http://arxiv.org/abs/2310.00678</a></li>
<li>repo_url: None</li>
<li>paper_authors: Teng Xiao, Donglin Wang</li>
<li>for: 这个论文研究了在在线环境中学习互动推荐系统的问题，无需在线探索。</li>
<li>methods: 论文提出了一种通用的Offline reinforcement learning框架，可以在不进行线上探索的情况下，最大化用户奖励。特别是，论文首先引入了一种 probabilistic generative model for interactive recommendation，然后提出了一种有效的推理算法基于历史反馈。</li>
<li>results: 论文通过五种方法来减少分布匹配问题，包括支持约束、监督辅助、政策约束、对偶约束和奖励推断。实验表明，提出的方法可以在两个公共的实验数据集上达到比现有的监督学习和强化学习方法更高的性能。<details>
<summary>Abstract</summary>
This paper studies the problem of learning interactive recommender systems from logged feedbacks without any exploration in online environments. We address the problem by proposing a general offline reinforcement learning framework for recommendation, which enables maximizing cumulative user rewards without online exploration. Specifically, we first introduce a probabilistic generative model for interactive recommendation, and then propose an effective inference algorithm for discrete and stochastic policy learning based on logged feedbacks. In order to perform offline learning more effectively, we propose five approaches to minimize the distribution mismatch between the logging policy and recommendation policy: support constraints, supervised regularization, policy constraints, dual constraints and reward extrapolation. We conduct extensive experiments on two public real-world datasets, demonstrating that the proposed methods can achieve superior performance over existing supervised learning and reinforcement learning methods for recommendation.
</details>
<details>
<summary>摘要</summary>
这个论文研究在在线环境中学习互动推荐系统的问题，不需要在线探索。我们解决这个问题，提出了一种通用的离线强化学习推荐框架，可以在离线环境中最大化用户奖励。 Specifically, we first introduce a probabilistic生成模型 for interactive recommendation, and then propose an effective inference algorithm for discrete and stochastic policy learning based on logged feedbacks. In order to perform offline learning more effectively, we propose five approaches to minimize the distribution mismatch between the logging policy and recommendation policy: support constraints, supervised regularization, policy constraints, dual constraints and reward extrapolation. We conduct extensive experiments on two public real-world datasets, demonstrating that the proposed methods can achieve superior performance over existing supervised learning and reinforcement learning methods for recommendation.Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Optimization-or-Architecture-How-to-Hack-Kalman-Filtering"><a href="#Optimization-or-Architecture-How-to-Hack-Kalman-Filtering" class="headerlink" title="Optimization or Architecture: How to Hack Kalman Filtering"></a>Optimization or Architecture: How to Hack Kalman Filtering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00675">http://arxiv.org/abs/2310.00675</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ido90/UsingKalmanFilterTheRightWay">https://github.com/ido90/UsingKalmanFilterTheRightWay</a></li>
<li>paper_authors: Ido Greenberg, Netanel Yannay, Shie Mannor</li>
<li>for: 这个论文是为了探讨非线性滤波器的问题。</li>
<li>methods: 该论文使用了一种名为Optimized Kalman Filter（OKF）的方法，该方法可以对非线性模型进行优化，使其与标准的线性加权滤波器（KF）相比赢得竞争力。</li>
<li>results: 该论文表明，通过使用OKF来优化非线性模型，可以使KF在某些问题上与神经网络模型相比赢得竞争力。此外，OKF还有较好的理论基础和实际表现。<details>
<summary>Abstract</summary>
In non-linear filtering, it is traditional to compare non-linear architectures such as neural networks to the standard linear Kalman Filter (KF). We observe that this mixes the evaluation of two separate components: the non-linear architecture, and the parameters optimization method. In particular, the non-linear model is often optimized, whereas the reference KF model is not. We argue that both should be optimized similarly, and to that end present the Optimized KF (OKF). We demonstrate that the KF may become competitive to neural models - if optimized using OKF. This implies that experimental conclusions of certain previous studies were derived from a flawed process. The advantage of OKF over the standard KF is further studied theoretically and empirically, in a variety of problems. Conveniently, OKF can replace the KF in real-world systems by merely updating the parameters.
</details>
<details>
<summary>摘要</summary>
“在非线性滤波中，传统上对非线性架构如神经网络进行比较，与标准的线性卡尔曼筛（KF）进行对比。我们注意到这两者是不同的两个组件：非线性架构和参数优化方法。具体来说，非线性模型经常被优化，而参考KF模型则不是。我们 argueThat both should be optimized similarly，并为此提出了优化后KF（OKF）。我们示出了KF可能与神经网络竞争，如果使用OKF进行优化。这意味着一些先前的研究结论可能基于不正确的过程得到。OKF比标准KF具有更大的优势，并在各种问题上进行了理论和实验研究。可以说，OKF可以在实际应用中取代KF，只需要更新参数即可。”
</details></li>
</ul>
<hr>
<h2 id="Learning-Type-Inference-for-Enhanced-Dataflow-Analysis"><a href="#Learning-Type-Inference-for-Enhanced-Dataflow-Analysis" class="headerlink" title="Learning Type Inference for Enhanced Dataflow Analysis"></a>Learning Type Inference for Enhanced Dataflow Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00673">http://arxiv.org/abs/2310.00673</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/joernio/joernti-codetidal5">https://github.com/joernio/joernti-codetidal5</a></li>
<li>paper_authors: Lukas Seidel, Sedick David Baker Effendi, Xavier Pinho, Konrad Rieck, Brink van der Merwe, Fabian Yamaguchi</li>
<li>for: This paper aims to improve the accuracy and efficiency of type inference for dynamically-typed languages, specifically TypeScript, by using machine learning techniques.</li>
<li>methods: The paper proposes a Transformer-based model called CodeTIDAL5, which is trained to predict type annotations and integrates with an open-source static analysis tool called Joern.</li>
<li>results: The paper reports that CodeTIDAL5 outperforms the current state-of-the-art by 7.85% on the ManyTypes4TypeScript benchmark, achieving 71.27% accuracy overall, and demonstrates the benefits of using the additional type information for security research.Here’s the same information in Simplified Chinese:</li>
<li>for: 这篇论文目标是提高动态类型语言中的类型推理精度和效率，具体是用机器学习技术来进行类型推理。</li>
<li>methods: 论文提出了一种基于转换器的模型，称为CodeTIDAL5，它可以预测类型注释，并与开源的静态分析工具Joern集成。</li>
<li>results: 论文表明，CodeTIDAL5比当前状态体系的最佳实现提高了7.85%的精度，达到了71.27%的总精度，并通过示出额外类型信息对安全研究带来了优势。<details>
<summary>Abstract</summary>
Statically analyzing dynamically-typed code is a challenging endeavor, as even seemingly trivial tasks such as determining the targets of procedure calls are non-trivial without knowing the types of objects at compile time. Addressing this challenge, gradual typing is increasingly added to dynamically-typed languages, a prominent example being TypeScript that introduces static typing to JavaScript. Gradual typing improves the developer's ability to verify program behavior, contributing to robust, secure and debuggable programs. In practice, however, users only sparsely annotate types directly. At the same time, conventional type inference faces performance-related challenges as program size grows. Statistical techniques based on machine learning offer faster inference, but although recent approaches demonstrate overall improved accuracy, they still perform significantly worse on user-defined types than on the most common built-in types. Limiting their real-world usefulness even more, they rarely integrate with user-facing applications. We propose CodeTIDAL5, a Transformer-based model trained to reliably predict type annotations. For effective result retrieval and re-integration, we extract usage slices from a program's code property graph. Comparing our approach against recent neural type inference systems, our model outperforms the current state-of-the-art by 7.85% on the ManyTypes4TypeScript benchmark, achieving 71.27% accuracy overall. Furthermore, we present JoernTI, an integration of our approach into Joern, an open source static analysis tool, and demonstrate that the analysis benefits from the additional type information. As our model allows for fast inference times even on commodity CPUs, making our system available through Joern leads to high accessibility and facilitates security research.
</details>
<details>
<summary>摘要</summary>
这是一个挑战性的任务，因为在类型是在编译时才能知道的情况下，确定程式中的目标类型是非常困难的。为了解决这个问题，有越来越多的 dynamically-typed 语言加入了渐进类型系统，例如 TypeScript，它将类型系统引入 JavaScript 中。渐进类型可以帮助开发者更好地验证程式的行为，从而实现更加稳定、安全和可靠的程式。然而，在实践中，用户几乎不直接将类型资讯输入。另一方面，传统的类型推论面临着程序大小增长时的性能问题，而且机器学习技术的使用可以提供更快的推论，但是这些方法通常在用户自定义的类型上表现较差。为了解决这个问题，我们提出了 CodeTIDAL5，一个基于 Transformer 模型的类型预测方法。为了有效地从程式码中提取类型资讯，我们将程式码转换为 code property graph，并从中提取使用类型的片段。与最新的神经网络类型推论系统相比，我们的模型在 ManyTypes4TypeScript 测试 benchmark 上的表现比前者高7.85%，总的来说是71.27%的准确率。此外，我们还将我们的方法与 open source 静态分析工具 Joern 集成，并证明了这种结合带来的分析优化。由于我们的模型可以在廉价的实体CPU上进行快速的类型推论，因此通过 Joern 进行分析可以实现高可用性和促进安全研究。
</details></li>
</ul>
<hr>
<h2 id="Balancing-Efficiency-vs-Effectiveness-and-Providing-Missing-Label-Robustness-in-Multi-Label-Stream-Classification"><a href="#Balancing-Efficiency-vs-Effectiveness-and-Providing-Missing-Label-Robustness-in-Multi-Label-Stream-Classification" class="headerlink" title="Balancing Efficiency vs. Effectiveness and Providing Missing Label Robustness in Multi-Label Stream Classification"></a>Balancing Efficiency vs. Effectiveness and Providing Missing Label Robustness in Multi-Label Stream Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00665">http://arxiv.org/abs/2310.00665</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sepehrbakhshi/ml-bels">https://github.com/sepehrbakhshi/ml-bels</a></li>
<li>paper_authors: Sepehr Bakhshi, Fazli Can</li>
<li>for: 这个论文的目的是提出一种适用于高维多标签分类的神经网络模型，以解决现有模型的不稳定性和效率问题。</li>
<li>methods: 我们的模型使用了选择性概念演化适应机制，使其适用于非站台环境。此外，我们还采用了一种简单 yet effective的抽象策略来处理缺失标签问题，并证明其在大多数状态前的监测模型中表现出色。</li>
<li>results: 我们的模型ML-BELS在11个基准模型、5个Synthetics和13个实际数据集上进行了广泛的评估，结果显示其能够平衡效率和有效性，同时对缺失标签和概念演化具有良好的Robustness。<details>
<summary>Abstract</summary>
Available works addressing multi-label classification in a data stream environment focus on proposing accurate models; however, these models often exhibit inefficiency and cannot balance effectiveness and efficiency. In this work, we propose a neural network-based approach that tackles this issue and is suitable for high-dimensional multi-label classification. Our model uses a selective concept drift adaptation mechanism that makes it suitable for a non-stationary environment. Additionally, we adapt our model to an environment with missing labels using a simple yet effective imputation strategy and demonstrate that it outperforms a vast majority of the state-of-the-art supervised models. To achieve our purposes, we introduce a weighted binary relevance-based approach named ML-BELS using the Broad Ensemble Learning System (BELS) as its base classifier. Instead of a chain of stacked classifiers, our model employs independent weighted ensembles, with the weights generated by the predictions of a BELS classifier. We show that using the weighting strategy on datasets with low label cardinality negatively impacts the accuracy of the model; with this in mind, we use the label cardinality as a trigger for applying the weights. We present an extensive assessment of our model using 11 state-of-the-art baselines, five synthetics, and 13 real-world datasets, all with different characteristics. Our results demonstrate that the proposed approach ML-BELS is successful in balancing effectiveness and efficiency, and is robust to missing labels and concept drift.
</details>
<details>
<summary>摘要</summary>
可用的工作，关于多标签分类在数据流环境中，主要关注提出准确的模型，但这些模型经常表现不具有效率。在这种情况下，我们提出一种基于神经网络的方法，可以解决这个问题，并适用于高维多标签分类。我们的模型使用一种选择性概念漂移适应机制，使其适用于不站ARY环境。此外，我们采用一种简单 yet effective的损失函数填充策略，以适应缺失标签的环境。为了实现我们的目标，我们引入了一种权重Binary relevance-based approach named ML-BELS，使用Broad Ensemble Learning System（BELS）作为基类фика器。而不是一串堆叠的类фика器，我们的模型使用独立的权重ensemble，其权重由BELS类фика器的预测值生成。我们发现，在 datasets with low label cardinality 上，使用权重策略会下降模型的准确率。因此，我们使用标签 cardinality 作为触发器，只在标签 cardinality 高于某个阈值时应用权重。我们对 11 种基线模型、5 种 sintetics 和 13 个实际 datasets进行了广泛的评估。我们的结果表明，我们的方法 ML-BELS 能够均衡效果和效率，并对缺失标签和概念漂移 exhibit  robustness。
</details></li>
</ul>
<hr>
<h2 id="Twin-Neural-Network-Improved-k-Nearest-Neighbor-Regression"><a href="#Twin-Neural-Network-Improved-k-Nearest-Neighbor-Regression" class="headerlink" title="Twin Neural Network Improved k-Nearest Neighbor Regression"></a>Twin Neural Network Improved k-Nearest Neighbor Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00664">http://arxiv.org/abs/2310.00664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sebastian J. Wetzel</li>
<li>for: 用于预测差异而不是目标值本身的双神经网络回归。</li>
<li>methods: 使用预测差异的方法，选择最近的 anchor 数据点作为预测差异的 Referent。</li>
<li>results: 在小到中等大的数据集上，该算法可以超越神经网络和 k-最近邻 regression。<details>
<summary>Abstract</summary>
Twin neural network regression is trained to predict differences between regression targets rather than the targets themselves. A solution to the original regression problem can be obtained by ensembling predicted differences between the targets of an unknown data point and multiple known anchor data points. Choosing the anchors to be the nearest neighbors of the unknown data point leads to a neural network-based improvement of k-nearest neighbor regression. This algorithm is shown to outperform both neural networks and k-nearest neighbor regression on small to medium-sized data sets.
</details>
<details>
<summary>摘要</summary>
双 neural network  regression 是用来预测目标之间的差异而不是目标本身。一种解决原始回归问题的解决方案是通过 ensemble 预测未知数据点的目标之间的差异和多个已知的 anchor 数据点之间的差异。选择 anchor 为未知数据点最近的邻居，可以实现基于 neural network 的 k-nearest neighbor 回归的改进。这种算法在小到中型数据集上表现出了超过 neural networks 和 k-nearest neighbor 回归的性能。
</details></li>
</ul>
<hr>
<h2 id="PatchMixer-A-Patch-Mixing-Architecture-for-Long-Term-Time-Series-Forecasting"><a href="#PatchMixer-A-Patch-Mixing-Architecture-for-Long-Term-Time-Series-Forecasting" class="headerlink" title="PatchMixer: A Patch-Mixing Architecture for Long-Term Time Series Forecasting"></a>PatchMixer: A Patch-Mixing Architecture for Long-Term Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00655">http://arxiv.org/abs/2310.00655</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Zeying-Gong/PatchMixer">https://github.com/Zeying-Gong/PatchMixer</a></li>
<li>paper_authors: Zeying Gong, Yujin Tang, Junwei Liang</li>
<li>for: 这篇论文主要针对时间序列预测任务进行了研究，尤其是解决Transformer Architecture中的Permutation-Invariant Self-Attention机制导致的一般挑战。</li>
<li>methods: 该论文提出了一种新的CNN基于模型，即PatchMixer，它使用了可变 convolutional structure来保留时间信息。与传统的CNN在这个领域中使用多尺度或多支流的方法不同，我们的方法仅使用了深度分割 convolutions，以EXTRACT both local features和全局相关性。</li>
<li>results: 我们的实验结果表明，相比最佳的方法和CNN，PatchMixer在七个时间序列预测 bencmarks上提供了3.9%和21.2%的相对改进，同时比最高级别的方法快2-3倍。<details>
<summary>Abstract</summary>
Although the Transformer has been the dominant architecture for time series forecasting tasks in recent years, a fundamental challenge remains: the permutation-invariant self-attention mechanism within Transformers leads to a loss of temporal information. To tackle these challenges, we propose PatchMixer, a novel CNN-based model. It introduces a permutation-variant convolutional structure to preserve temporal information. Diverging from conventional CNNs in this field, which often employ multiple scales or numerous branches, our method relies exclusively on depthwise separable convolutions. This allows us to extract both local features and global correlations using a single-scale architecture. Furthermore, we employ dual forecasting heads that encompass both linear and nonlinear components to better model future curve trends and details. Our experimental results on seven time-series forecasting benchmarks indicate that compared with the state-of-the-art method and the best-performing CNN, PatchMixer yields $3.9\%$ and $21.2\%$ relative improvements, respectively, while being 2-3x faster than the most advanced method. We will release our code and model.
</details>
<details>
<summary>摘要</summary>
尽管Transformer在最近几年内时序预测任务中占据主导地位，但是一个基本挑战仍然存在：Transformer中的卷积层的自注意力机制会导致时间信息的丢失。为解决这些挑战，我们提出了PatchMixer，一种新的CNN基于模型。它引入了一种可变卷积结构，以保留时间信息。与传统的CNN在这个领域中，通常采用多个缩放或多个分支，我们的方法仅仅采用深度分解卷积。这使得我们可以提取本地特征和全局相关性，使用单一的大小结构。此外，我们使用双重预测头，包括线性和非线性组件，以更好地模型未来曲线趋势和细节。我们的实验结果表明，与状态之前的方法和最佳CNN相比，PatchMixer在七个时序预测标准 benchmark上提供了3.9%和21.2%的相对提升，同时比最先进的方法快2-3倍。我们将发布我们的代码和模型。
</details></li>
</ul>
<hr>
<h2 id="A-primal-dual-perspective-for-distributed-TD-learning"><a href="#A-primal-dual-perspective-for-distributed-TD-learning" class="headerlink" title="A primal-dual perspective for distributed TD-learning"></a>A primal-dual perspective for distributed TD-learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00638">http://arxiv.org/abs/2310.00638</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han-Dong Lim, Donghwan Lee</li>
<li>for:  investigate distributed temporal difference (TD) learning for a networked multi-agent Markov decision process</li>
<li>methods:  based on distributed optimization algorithms, which can be interpreted as primal-dual Ordinary differential equation (ODE) dynamics subject to null-space constraints</li>
<li>results:  examined the behavior of the final iterate in various distributed TD-learning scenarios, considering both constant and diminishing step-sizes and incorporating both i.i.d. and Markovian observation models, without assuming a doubly stochastic matrix for the communication network structure.<details>
<summary>Abstract</summary>
The goal of this paper is to investigate distributed temporal difference (TD) learning for a networked multi-agent Markov decision process. The proposed approach is based on distributed optimization algorithms, which can be interpreted as primal-dual Ordinary differential equation (ODE) dynamics subject to null-space constraints. Based on the exponential convergence behavior of the primal-dual ODE dynamics subject to null-space constraints, we examine the behavior of the final iterate in various distributed TD-learning scenarios, considering both constant and diminishing step-sizes and incorporating both i.i.d. and Markovian observation models. Unlike existing methods, the proposed algorithm does not require the assumption that the underlying communication network structure is characterized by a doubly stochastic matrix.
</details>
<details>
<summary>摘要</summary>
本文的目标是研究分布式时间差（TD）学习 для网络化多 Agent Markov决策过程。我们提出的方法基于分布式优化算法，可以被看作为 primal-dual ordinary differential equation（ODE）动力学Subject to null-space constraints。基于 primal-dual ODE 动力学Subject to null-space constraints 的快速抽象行为，我们研究了不同分布式 TD-学习场景中的最终迭代器行为，包括常数和减少步长和 Markovian 观测模型。不同于现有方法，我们的算法不需要假设基础通信网络结构是 doubly stochastic matrix。
</details></li>
</ul>
<hr>
<h2 id="GNRK-Graph-Neural-Runge-Kutta-method-for-solving-partial-differential-equations"><a href="#GNRK-Graph-Neural-Runge-Kutta-method-for-solving-partial-differential-equations" class="headerlink" title="GNRK: Graph Neural Runge-Kutta method for solving partial differential equations"></a>GNRK: Graph Neural Runge-Kutta method for solving partial differential equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00618">http://arxiv.org/abs/2310.00618</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hoyunchoi/GNRK">https://github.com/hoyunchoi/GNRK</a></li>
<li>paper_authors: Hoyun Choi, Sungyeop Lee, B. Kahng, Junghyo Jo<br>for:* 这种新的方法可以用来解决广泛的偏微分方程（PDEs），而不需要特定的初始条件或偏微分方程的系数。methods:* 该方法基于图structures，使其对域分辨率和时间分辨率的变化具有抗锋性。* 该方法结合了图神经网络模块和回归结构，以提高其效率和通用性。results:* 对于2维布尔氏方程，GNRK表现出了较高的准确率和模型体积。* 该方法可以 straightforwardly拓展到解决相互关联的偏微分方程。<details>
<summary>Abstract</summary>
Neural networks have proven to be efficient surrogate models for tackling partial differential equations (PDEs). However, their applicability is often confined to specific PDEs under certain constraints, in contrast to classical PDE solvers that rely on numerical differentiation. Striking a balance between efficiency and versatility, this study introduces a novel approach called Graph Neural Runge-Kutta (GNRK), which integrates graph neural network modules with a recurrent structure inspired by the classical solvers. The GNRK operates on graph structures, ensuring its resilience to changes in spatial and temporal resolutions during domain discretization. Moreover, it demonstrates the capability to address general PDEs, irrespective of initial conditions or PDE coefficients. To assess its performance, we benchmark the GNRK against existing neural network based PDE solvers using the 2-dimensional Burgers' equation, revealing the GNRK's superiority in terms of model size and accuracy. Additionally, this graph-based methodology offers a straightforward extension for solving coupled differential equations, typically necessitating more intricate models.
</details>
<details>
<summary>摘要</summary>
The GNRK operates on graph structures, ensuring its ability to adapt to changes in spatial and temporal resolutions during domain discretization. Moreover, it can handle general PDEs regardless of initial conditions or PDE coefficients. To evaluate its performance, we benchmark the GNRK against existing neural network-based PDE solvers using the 2-dimensional Burgers' equation, demonstrating its superiority in terms of model size and accuracy.Furthermore, the graph-based methodology used in the GNRK provides a straightforward extension for solving coupled differential equations, which are typically more challenging to model. This study opens up new possibilities for using neural networks to solve a wide range of PDEs, with potential applications in fields such as fluid dynamics, heat transfer, and wave propagation.
</details></li>
</ul>
<hr>
<h2 id="On-the-Onset-of-Robust-Overfitting-in-Adversarial-Training"><a href="#On-the-Onset-of-Robust-Overfitting-in-Adversarial-Training" class="headerlink" title="On the Onset of Robust Overfitting in Adversarial Training"></a>On the Onset of Robust Overfitting in Adversarial Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00607">http://arxiv.org/abs/2310.00607</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chaojian Yu, Xiaolong Shi, Jun Yu, Bo Han, Tongliang Liu</li>
<li>for: 本研究旨在解释robust overfitting的基本机制，以及提出两种方法来缓解这种现象。</li>
<li>methods: 研究者通过分析normal data和敌方攻击的影响，并提出了一种基于factor ablation的方法来解释robust overfitting的起源。</li>
<li>results: 实验结果表明，提出的两种方法能够有效地缓解robust overfitting，并提高模型的 adversarial robustness。<details>
<summary>Abstract</summary>
Adversarial Training (AT) is a widely-used algorithm for building robust neural networks, but it suffers from the issue of robust overfitting, the fundamental mechanism of which remains unclear. In this work, we consider normal data and adversarial perturbation as separate factors, and identify that the underlying causes of robust overfitting stem from the normal data through factor ablation in AT. Furthermore, we explain the onset of robust overfitting as a result of the model learning features that lack robust generalization, which we refer to as non-effective features. Specifically, we provide a detailed analysis of the generation of non-effective features and how they lead to robust overfitting. Additionally, we explain various empirical behaviors observed in robust overfitting and revisit different techniques to mitigate robust overfitting from the perspective of non-effective features, providing a comprehensive understanding of the robust overfitting phenomenon. This understanding inspires us to propose two measures, attack strength and data augmentation, to hinder the learning of non-effective features by the neural network, thereby alleviating robust overfitting. Extensive experiments conducted on benchmark datasets demonstrate the effectiveness of the proposed methods in mitigating robust overfitting and enhancing adversarial robustness.
</details>
<details>
<summary>摘要</summary>
针对抗性训练（AT） Algorithm，我们尝试分离 normal data 和抗击干扰作用，并发现了 robust overfitting 的根本机制。我们发现，AT 中的 robust overfitting 问题来自 normal data 的因素缺失，而这些因素缺失导致模型学习无效的特征，我们称之为 non-effective features。我们进行了详细的非效果特征生成分析和如何导致 robust overfitting 的分析。此外，我们还解释了 robust overfitting 的不同实验现象，并重新评估了不同的技术来mitigate robust overfitting，从非效果特征的角度出发。基于这种理解，我们提出了两种方法，攻击强度和数据增强，来阻止神经网络学习非效果特征，从而缓解 robust overfitting。我们在标准数据集上进行了广泛的实验，并证明了我们的方法可以有效地缓解 robust overfitting 并提高对抗性。
</details></li>
</ul>
<hr>
<h2 id="Path-Structured-Multimarginal-Schrodinger-Bridge-for-Probabilistic-Learning-of-Hardware-Resource-Usage-by-Control-Software"><a href="#Path-Structured-Multimarginal-Schrodinger-Bridge-for-Probabilistic-Learning-of-Hardware-Resource-Usage-by-Control-Software" class="headerlink" title="Path Structured Multimarginal Schrödinger Bridge for Probabilistic Learning of Hardware Resource Usage by Control Software"></a>Path Structured Multimarginal Schrödinger Bridge for Probabilistic Learning of Hardware Resource Usage by Control Software</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00604">http://arxiv.org/abs/2310.00604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Georgiy A. Bondar, Robert Gifford, Linh Thi Xuan Phan, Abhishek Halder</li>
<li>for: 解决路径结构多重 Marginal Schrödinger Bridge 问题 (MSBP)，获得最可能的测量对象 trajectory，以便预测软件控制下硬件资源的时变分布。</li>
<li>methods: 利用最近的算法技术解决这类结构化 MSBP，以学习软件控制下硬件资源的使用情况。</li>
<li>results: 方法可以快速减少至精度预测硬件资源使用情况，并且可以应用到任何软件预测Cyber-physical上下文相依性性能。<details>
<summary>Abstract</summary>
The solution of the path structured multimarginal Schr\"{o}dinger bridge problem (MSBP) is the most-likely measure-valued trajectory consistent with a sequence of observed probability measures or distributional snapshots. We leverage recent algorithmic advances in solving such structured MSBPs for learning stochastic hardware resource usage by control software. The solution enables predicting the time-varying distribution of hardware resource availability at a desired time with guaranteed linear convergence. We demonstrate the efficacy of our probabilistic learning approach in a model predictive control software execution case study. The method exhibits rapid convergence to an accurate prediction of hardware resource utilization of the controller. The method can be broadly applied to any software to predict cyber-physical context-dependent performance at arbitrary time.
</details>
<details>
<summary>摘要</summary>
解决方案是多 Structured 多 marginal Schrödinger 桥Problem (MSBP) 的解决方案，它是一系列观测概率分布或分布快照的最有可能的测试值轨迹。我们利用了最新的算法技术来解决这种结构化MSBP，用于学习干扰控制软件的硬件资源使用情况。解决方案可以预测时间变化的硬件资源可用性，并且保证线性快速收敛。我们在一个模型预测控制软件执行 caso study 中证明了我们的概率学方法的有效性。方法在控制器中的硬件资源使用情况中显示了快速收敛到准确的预测。该方法可以广泛应用于任何软件，以预测任意时间点的Cyber-Physical context-dependent性能。
</details></li>
</ul>
<hr>
<h2 id="SIMD-Dataflow-Co-optimization-for-Efficient-Neural-Networks-Inferences-on-CPUs"><a href="#SIMD-Dataflow-Co-optimization-for-Efficient-Neural-Networks-Inferences-on-CPUs" class="headerlink" title="SIMD Dataflow Co-optimization for Efficient Neural Networks Inferences on CPUs"></a>SIMD Dataflow Co-optimization for Efficient Neural Networks Inferences on CPUs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00574">http://arxiv.org/abs/2310.00574</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cyrus Zhou, Zack Hassman, Ruize Xu, Dhirpal Shah, Vaugnn Richard, Yanjing Li</li>
<li>for: 这篇论文主要关注在CPU上执行神经网络时所遇到的挑战，特别是对于实时运算时间的最佳化，同时保持精度。</li>
<li>methods: 本研究使用了资料流（即神经网络计算顺序）来探索数据重复机会，运用了一个基于规律的分析和代码生成框架，以探索不同的单指令多数据（SIMD）实现方法，以获得优化神经网络执行。</li>
<li>results: 研究结果显示，以保持输出在SIMD注册中，同时将输入和权重重复 maximize 可以实现最好的性能，实现了对各种推理任务的广泛性和可靠性。具体来说，对于8位数字神经网络，可以 achieve 3x 速度提升；对于二进制神经网络，可以 achieve 4.8x 速度提升，对于现有的神经网络实现最佳化。<details>
<summary>Abstract</summary>
We address the challenges associated with deploying neural networks on CPUs, with a particular focus on minimizing inference time while maintaining accuracy. Our novel approach is to use the dataflow (i.e., computation order) of a neural network to explore data reuse opportunities using heuristic-guided analysis and a code generation framework, which enables exploration of various Single Instruction, Multiple Data (SIMD) implementations to achieve optimized neural network execution. Our results demonstrate that the dataflow that keeps outputs in SIMD registers while also maximizing both input and weight reuse consistently yields the best performance for a wide variety of inference workloads, achieving up to 3x speedup for 8-bit neural networks, and up to 4.8x speedup for binary neural networks, respectively, over the optimized implementations of neural networks today.
</details>
<details>
<summary>摘要</summary>
我们面临将神经网络部署到CPU上的挑战，尤其是对于降低推导时间而保持精度的最佳化。我们的新方法是根据神经网络的资料流（即computation order）进行回传分析和代码生成框架，以寻找可以使用单指令多数据（SIMD）实现最佳化的神经网络执行。我们的结果显示，可以将输出保持在SIMD寄存器中，同时将输入和权重重用到最大化的情况下，协助实现广泛的推导工作负载上的最高性能，相比今天的最佳化神经网络实现，8位数字神经网络可以达到3倍的速度提升，而二进制神经网络可以达到4.8倍的速度提升。
</details></li>
</ul>
<hr>
<h2 id="Discrete-Choice-Multi-Armed-Bandits"><a href="#Discrete-Choice-Multi-Armed-Bandits" class="headerlink" title="Discrete Choice Multi-Armed Bandits"></a>Discrete Choice Multi-Armed Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00562">http://arxiv.org/abs/2310.00562</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emerson Melo, David Müller</li>
<li>for: 这篇论文将绘制连接分类选择模型和在线学习多重枪支算法的关系。我们的贡献可以概括为两个关键方面：	1. 我们提供了下线 regret bound，覆盖广泛的算法家族，其中包括Exp3算法为特例。	2. 我们引入了一种新的对抗多重枪支算法家族， Drawing inspiration from generalized nested logit models introduced by \citet{wen:2001}.</li>
<li>methods: 我们的算法使用了分类选择模型，并且提供了closed-form sampling distribution probabilities，使其可以实现高效。</li>
<li>results: 我们通过数值实验，将我们的算法应用于随机枪支问题，并取得了实质性的结果。<details>
<summary>Abstract</summary>
This paper establishes a connection between a category of discrete choice models and the realms of online learning and multiarmed bandit algorithms. Our contributions can be summarized in two key aspects. Firstly, we furnish sublinear regret bounds for a comprehensive family of algorithms, encompassing the Exp3 algorithm as a particular case. Secondly, we introduce a novel family of adversarial multiarmed bandit algorithms, drawing inspiration from the generalized nested logit models initially introduced by \citet{wen:2001}. These algorithms offer users the flexibility to fine-tune the model extensively, as they can be implemented efficiently due to their closed-form sampling distribution probabilities. To demonstrate the practical implementation of our algorithms, we present numerical experiments, focusing on the stochastic bandit case.
</details>
<details>
<summary>摘要</summary>
Firstly, we provide sublinear regret bounds for a wide range of algorithms, including the Exp3 algorithm as a special case.Secondly, we introduce a new family of adversarial multiarmed bandit algorithms, inspired by the generalized nested logit models first introduced by \citet{wen:2001}. These algorithms allow for extensive fine-tuning of the model and can be efficiently implemented due to their closed-form sampling distribution probabilities.To demonstrate the practical application of our algorithms, we present numerical experiments focusing on the stochastic bandit case.
</details></li>
</ul>
<hr>
<h2 id="Horizontal-Class-Backdoor-to-Deep-Learning"><a href="#Horizontal-Class-Backdoor-to-Deep-Learning" class="headerlink" title="Horizontal Class Backdoor to Deep Learning"></a>Horizontal Class Backdoor to Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00542">http://arxiv.org/abs/2310.00542</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hua Ma, Shang Wang, Yansong Gao</li>
<li>for: 这篇论文旨在描述一种新型的水平类后门攻击（HCB），它可以通过含有无关的自然特征来让模型在某些情况下具有负面影响。</li>
<li>methods: 这篇论文使用了一种新的水平类后门攻击方法，它可以在模型中嵌入后门，并通过含有无关的自然特征来让后门在某些情况下被触发。</li>
<li>results: 实验结果表明，这种水平类后门攻击方法可以具有高效率和高攻击成功率，并且可以轻松地绕过多种已知的防御方法。<details>
<summary>Abstract</summary>
All existing backdoor attacks to deep learning (DL) models belong to the vertical class backdoor (VCB). That is, any sample from a class will activate the implanted backdoor in the presence of the secret trigger, regardless of source-class-agnostic or source-class-specific backdoor. Current trends of existing defenses are overwhelmingly devised for VCB attacks especially the source-class-agnostic backdoor, which essentially neglects other potential simple but general backdoor types, thus giving false security implications. It is thus urgent to discover unknown backdoor types.   This work reveals a new, simple, and general horizontal class backdoor (HCB) attack. We show that the backdoor can be naturally bounded with innocuous natural features that are common and pervasive in the real world. Note that an innocuous feature (e.g., expression) is irrelevant to the main task of the model (e.g., recognizing a person from one to another). The innocuous feature spans across classes horizontally but is exhibited by partial samples per class -- satisfying the horizontal class (HC) property. Only when the trigger is concurrently presented with the HC innocuous feature, can the backdoor be effectively activated. Extensive experiments on attacking performance in terms of high attack success rates with tasks of 1) MNIST, 2) facial recognition, 3) traffic sign recognition, and 4) object detection demonstrate that the HCB is highly efficient and effective. We extensively evaluate the HCB evasiveness against a (chronologically) series of 9 influential countermeasures of Fine-Pruning (RAID 18'), STRIP (ACSAC 19'), Neural Cleanse (Oakland 19'), ABS (CCS 19'), Februus (ACSAC 20'), MNTD (Oakland 21'), SCAn (USENIX SEC 21'), MOTH (Oakland 22'), and Beatrix (NDSS 23'), where none of them can succeed even when a simplest trigger is used.
</details>
<details>
<summary>摘要</summary>
所有现有的深度学习（DL）模型攻击都属于垂直类后门（VCB）。即任何一个类的样本在存在秘密触发符时，无论来源是否特定或不特定，都会触发嵌入的后门。现有的防御策略几乎都是为VCB攻击而设计的，特别是源特定后门，这实际上忽略了其他可能的简单但普遍的后门类型，从而给予false安全性。因此，发现未知后门类型是 Urgent。本工作发现了一种新的、简单的、普遍的水平类后门（HCB）攻击方法。我们表明，这种后门可以自然地与无关于主任务的 innocuous feature（例如表情）相结合，这种 innocuous feature 在实际世界中很普遍。注意，无关主任务的 innocuous feature 可以水平地跨类，但只有在触发符同时出现时，HC innocuous feature 才能够有效地触发后门。我们通过对不同任务的 MNIST、人脸识别、交通标识和物体检测进行了广泛的实验，并证明了 HCB 的高效率和可靠性。我们也进行了广泛的验证HCB的逃避性，并证明了HCB不受9种influential countermeasures的侵害，其中包括Fine-Pruning (RAID 18')、STRIP (ACSAC 19')、Neural Cleanse (Oakland 19')、ABS (CCS 19')、Februus (ACSAC 20')、MNTD (Oakland 21')、SCAn (USENIX SEC 21')、MOTH (Oakland 22')和Beatrix (NDSS 23')。 None of them can succeed even when a simplest trigger is used.
</details></li>
</ul>
<hr>
<h2 id="Robust-Nonparametric-Hypothesis-Testing-to-Understand-Variability-in-Training-Neural-Networks"><a href="#Robust-Nonparametric-Hypothesis-Testing-to-Understand-Variability-in-Training-Neural-Networks" class="headerlink" title="Robust Nonparametric Hypothesis Testing to Understand Variability in Training Neural Networks"></a>Robust Nonparametric Hypothesis Testing to Understand Variability in Training Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00541">http://arxiv.org/abs/2310.00541</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sinjini Banerjee, Reilly Cannon, Tim Marrinan, Tony Chiang, Anand D. Sarwate</li>
<li>for: 这个论文是为了描述一种新的模型相似度计算方法，用于评估深度神经网络（DNN）的训练过程中的杂态优化问题。</li>
<li>methods: 这篇论文使用了一种基于Robust Hypothesis Testing框架的新方法，用于评估DNN模型之间的相似度。这种方法不仅可以评估模型的测试准确率，还可以捕捉到模型计算的不同。</li>
<li>results: 这篇论文的结果表明，使用这种新方法可以更好地评估DNN模型之间的相似度，并且可以捕捉到模型计算的不同。这种方法可以用于评估其他从训练过程中得到的模型属性。<details>
<summary>Abstract</summary>
Training a deep neural network (DNN) often involves stochastic optimization, which means each run will produce a different model. Several works suggest this variability is negligible when models have the same performance, which in the case of classification is test accuracy. However, models with similar test accuracy may not be computing the same function. We propose a new measure of closeness between classification models based on the output of the network before thresholding. Our measure is based on a robust hypothesis-testing framework and can be adapted to other quantities derived from trained models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Thompson-Exploration-with-Best-Challenger-Rule-in-Best-Arm-Identification"><a href="#Thompson-Exploration-with-Best-Challenger-Rule-in-Best-Arm-Identification" class="headerlink" title="Thompson Exploration with Best Challenger Rule in Best Arm Identification"></a>Thompson Exploration with Best Challenger Rule in Best Arm Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00539">http://arxiv.org/abs/2310.00539</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jongyeong Lee, Junya Honda, Masashi Sugiyama</li>
<li>for: 本研究证明了一种在单参数几何模型下的fixed-confidence最佳臂标识问题（BAI）的解决方案。</li>
<li>methods: 我们提出了一种 combining Thompson sampling 和 computationally efficient approach 的策略，即最佳挑战者规则。</li>
<li>results: 我们证明了该策略是 any two-armed bandit problem 的 asymptotically optimal 策略，并且对于 general $K$-armed bandit problem  ($K\geq 3$) 也有 near optimality。在numerical experiments中，我们的策略与 asymptotically optimal 策略相比，具有更好的sample complexity 性能，同时具有更低的计算成本。<details>
<summary>Abstract</summary>
This paper studies the fixed-confidence best arm identification (BAI) problem in the bandit framework in the canonical single-parameter exponential models. For this problem, many policies have been proposed, but most of them require solving an optimization problem at every round and/or are forced to explore an arm at least a certain number of times except those restricted to the Gaussian model. To address these limitations, we propose a novel policy that combines Thompson sampling with a computationally efficient approach known as the best challenger rule. While Thompson sampling was originally considered for maximizing the cumulative reward, we demonstrate that it can be used to naturally explore arms in BAI without forcing it. We show that our policy is asymptotically optimal for any two-armed bandit problems and achieves near optimality for general $K$-armed bandit problems for $K\geq 3$. Nevertheless, in numerical experiments, our policy shows competitive performance compared to asymptotically optimal policies in terms of sample complexity while requiring less computation cost. In addition, we highlight the advantages of our policy by comparing it to the concept of $\beta$-optimality, a relaxed notion of asymptotic optimality commonly considered in the analysis of a class of policies including the proposed one.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Statistical-Limits-of-Adaptive-Linear-Models-Low-Dimensional-Estimation-and-Inference"><a href="#Statistical-Limits-of-Adaptive-Linear-Models-Low-Dimensional-Estimation-and-Inference" class="headerlink" title="Statistical Limits of Adaptive Linear Models: Low-Dimensional Estimation and Inference"></a>Statistical Limits of Adaptive Linear Models: Low-Dimensional Estimation and Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00532">http://arxiv.org/abs/2310.00532</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/licong-lin/low-dim-debias">https://github.com/licong-lin/low-dim-debias</a></li>
<li>paper_authors: Licong Lin, Mufang Ying, Suvrojit Ghosh, Koulik Khamaru, Cun-Hui Zhang</li>
<li>for: 本文研究了采集数据是否会对统计学 estimation和推断带来影响，具体来说是 linear models 中的 Ordinary Least Squares (OLS) 估计器是否能够在不同的数据采集方式下保持 asymptotic normality。</li>
<li>methods: 本文使用了 minimax lower bound 来描述在不同数据采集方式下 estimation 的性能差异，并 investigate 了高维 linear models 中 parameter 组件的估计性能如何受到数据采集方式的影响。</li>
<li>results: 本文发现，当数据采集方式是 adaptive 时，OLS 估计器可能会具有较大的 estimation error，而且这个 error 与数据采集方式的度数相关。然而，当数据采集方式是 i.i.d. 时，OLS 估计器的 estimation error 可以达到最佳性。此外，本文还提出了一种新的估计器，可以在 adaptive 数据采集方式下实现 single coordinate inference。这个估计器的 asymptotic normality 性也得到了证明。<details>
<summary>Abstract</summary>
Estimation and inference in statistics pose significant challenges when data are collected adaptively. Even in linear models, the Ordinary Least Squares (OLS) estimator may fail to exhibit asymptotic normality for single coordinate estimation and have inflated error. This issue is highlighted by a recent minimax lower bound, which shows that the error of estimating a single coordinate can be enlarged by a multiple of $\sqrt{d}$ when data are allowed to be arbitrarily adaptive, compared with the case when they are i.i.d. Our work explores this striking difference in estimation performance between utilizing i.i.d. and adaptive data. We investigate how the degree of adaptivity in data collection impacts the performance of estimating a low-dimensional parameter component in high-dimensional linear models. We identify conditions on the data collection mechanism under which the estimation error for a low-dimensional parameter component matches its counterpart in the i.i.d. setting, up to a factor that depends on the degree of adaptivity. We show that OLS or OLS on centered data can achieve this matching error. In addition, we propose a novel estimator for single coordinate inference via solving a Two-stage Adaptive Linear Estimating equation (TALE). Under a weaker form of adaptivity in data collection, we establish an asymptotic normality property of the proposed estimator.
</details>
<details>
<summary>摘要</summary>
“统计中的估计和推断在收集数据时存在重要的挑战。甚至在线性模型中，常数最小二乘（OLS）估计器可能无法在单坐标估计中展现 asymptotic normality，并且有较大的误差。这个问题得到了最近的最小下界 bound，显示了数据收集机制允许自由变化时，估计单坐标误差可以被增加为 $\sqrt{d}$ 倍，与独立Identically distributed（i.i.d）数据相比。我们的工作探讨了这种估计性能之间的差异，并研究了高维线性模型中低维参数组件的估计性能如何受到数据收集机制的影响。我们确定了数据收集机制下的condition under which the estimation error for a low-dimensional parameter component matches its counterpart in the i.i.d. setting, up to a factor that depends on the degree of adaptivity。我们还提出了一种新的估计器，可以在高维线性模型中实现单坐标推断。在一种更弱的数据收集机制下，我们证明了该估计器的 asymptotic normality 性。”Note: Please note that the translation is in Simplified Chinese, and the word order and grammar may be different from the original text.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/01/cs.LG_2023_10_01/" data-id="clp869u0k00sfk5883bvs6hyn" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_10_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/01/eess.IV_2023_10_01/" class="article-date">
  <time datetime="2023-10-01T09:00:00.000Z" itemprop="datePublished">2023-10-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/01/eess.IV_2023_10_01/">eess.IV - 2023-10-01</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="EventLFM-Event-Camera-integrated-Fourier-Light-Field-Microscopy-for-Ultrafast-3D-imaging"><a href="#EventLFM-Event-Camera-integrated-Fourier-Light-Field-Microscopy-for-Ultrafast-3D-imaging" class="headerlink" title="EventLFM: Event Camera integrated Fourier Light Field Microscopy for Ultrafast 3D imaging"></a>EventLFM: Event Camera integrated Fourier Light Field Microscopy for Ultrafast 3D imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00730">http://arxiv.org/abs/2310.00730</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruipeng Guo, Qianwan Yang, Andrew S. Chang, Guorong Hu, Joseph Greene, Christopher V. Gabel, Sixian You, Lei Tian</li>
<li>for:  This paper aims to develop a new imaging technique for visualizing complex and dynamic biological processes with high speed and large 3D space-bandwidth product (SBP).</li>
<li>methods:  The proposed technique, called EventLFM, combines an event camera with Fourier light field microscopy (LFM) to achieve single-shot 3D wide-field imaging with asynchronous readout and high data throughput.</li>
<li>results:  The authors demonstrate the ability of EventLFM to image fast-moving and rapidly blinking 3D samples at KHz frame rates and track GFP-labeled neurons in freely moving C. elegans with high accuracy.Here’s the Chinese translation of the three points:</li>
<li>for: 这篇论文的目的是开发一种能够高速、大的3D空间带宽产品（SBP）来可见复杂和动态生物过程的成像技术。</li>
<li>methods: 该提案的技术是将事件相机与富ouriet light field microscopy（LFM）相结合，实现单次广角成像，并且使用异步读取，以提高数据传输速率。</li>
<li>results: 作者们证明了事件LFM可以在KHz帧率下成像高速和快速闪烁的3D样品，并且可以准确地跟踪在自由移动C. elegans中的GFP标记neuron。<details>
<summary>Abstract</summary>
Ultrafast 3D imaging is indispensable for visualizing complex and dynamic biological processes. Conventional scanning-based techniques necessitate an inherent tradeoff between the acquisition speed and space-bandwidth product (SBP). While single-shot 3D wide-field techniques have emerged as an attractive solution, they are still bottlenecked by the synchronous readout constraints of conventional CMOS architectures, thereby limiting the data throughput by frame rate to maintain a high SBP. Here, we present EventLFM, a straightforward and cost-effective system that circumnavigates these challenges by integrating an event camera with Fourier light field microscopy (LFM), a single-shot 3D wide-field imaging technique. The event camera operates on a novel asynchronous readout architecture, thereby bypassing the frame rate limitations intrinsic to conventional CMOS systems. We further develop a simple and robust event-driven LFM reconstruction algorithm that can reliably reconstruct 3D dynamics from the unique spatiotemporal measurements from EventLFM. We experimentally demonstrate that EventLFM can robustly image fast-moving and rapidly blinking 3D samples at KHz frame rates and furthermore, showcase EventLFM's ability to achieve 3D tracking of GFP-labeled neurons in freely moving C. elegans. We believe that the combined ultrafast speed and large 3D SBP offered by EventLFM may open up new possibilities across many biomedical applications.
</details>
<details>
<summary>摘要</summary>
超速3D成像是生物过程视化中不可或缺的。传统的扫描方式存在固有的质量比速度产品（SBP）交换限制，而单发3D广阔技术受到传统CMOS架构的同步读取限制，因此对数据传输率做出了限制，即保持高SBP的情况下，帧率限制。在这里，我们介绍了EventLFM，一种简单且cost-effective的系统，通过将事件摄像头与快速光场镜微scopio（LFM）结合，绕过了传统CMOS系统中的帧率限制。事件摄像头使用了新的异步读取架构，因此可以快速响应快速变化的3D动态过程。我们还开发了一种简单可靠的事件驱动LFM重构算法，可以可靠地从EventLFM中获取3D动力学。我们实验表明，EventLFM可以Robustly图像高速运动和快速灯泡3D样本，并且可以实现C. elegans中GFP标记的 neuron 3D跟踪。我们认为EventLFM的总体快速速度和大3DSBP可能会开拓新的生物医学应用领域。
</details></li>
</ul>
<hr>
<h2 id="Spatiotemporal-Image-Reconstruction-to-Enable-High-Frame-Rate-Dynamic-Photoacoustic-Tomography-with-Rotating-Gantry-Volumetric-Imagers"><a href="#Spatiotemporal-Image-Reconstruction-to-Enable-High-Frame-Rate-Dynamic-Photoacoustic-Tomography-with-Rotating-Gantry-Volumetric-Imagers" class="headerlink" title="Spatiotemporal Image Reconstruction to Enable High-Frame Rate Dynamic Photoacoustic Tomography with Rotating-Gantry Volumetric Imagers"></a>Spatiotemporal Image Reconstruction to Enable High-Frame Rate Dynamic Photoacoustic Tomography with Rotating-Gantry Volumetric Imagers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00529">http://arxiv.org/abs/2310.00529</a></li>
<li>repo_url: None</li>
<li>paper_authors: Refik M. Cam, Chao Wang, Weylan Thompson, Sergey A. Ermilov, Mark A. Anastasio, Umberto Villa</li>
<li>For: 这种研究旨在开发一种能够应用于现有的扫描仪器上的快速扫描 PACT 图像重建方法，以解决现有系统中数据缺失的问题，并提高图像重建的精度和速度。* Methods: 该方法基于低级别矩阵估计（LRME），利用空间时间重复性来准确重建4D 空间时间图像。* Results: 数值研究表明，该方法可以准确地重建4D 动态图像，而实验研究则证明了该方法在实际应用中的可靠性和效果。<details>
<summary>Abstract</summary>
Significance: Dynamic photoacoustic computed tomography (PACT) is a valuable technique for monitoring physiological processes. However, current dynamic PACT techniques are often limited to 2D spatial imaging. While volumetric PACT imagers are commercially available, these systems typically employ a rotating gantry in which the tomographic data are sequentially acquired. Because the object varies during the data-acquisition process, the sequential data-acquisition poses challenges to image reconstruction associated with data incompleteness. The proposed method is highly significant in that it will address these challenges and enable volumetric dynamic PACT imaging with existing imagers. Aim: The aim of this study is to develop a spatiotemporal image reconstruction (STIR) method for dynamic PACT that can be applied to commercially available volumetric PACT imagers that employ a sequential scanning strategy. The proposed method aims to overcome the challenges caused by the limited number of tomographic measurements acquired per frame. Approach: A low-rank matrix estimation-based STIR method (LRME-STIR) is proposed to enable dynamic volumetric PACT. The LRME-STIR method leverages the spatiotemporal redundancies to accurately reconstruct a 4D spatiotemporal image. Results: The numerical studies substantiate the LRME-STIR method's efficacy in reconstructing 4D dynamic images from measurements acquired with a rotating gantry. The experimental study demonstrates the method's ability to faithfully recover the flow of a contrast agent at a frame rate of 0.1 s even when only a single tomographic measurement per frame is available. Conclusions: The LRME-STIR method offers a promising solution to the challenges faced by enabling 4D dynamic imaging using commercially available volumetric imagers. By enabling accurate 4D reconstruction, this method has the potential to advance preclinical research.
</details>
<details>
<summary>摘要</summary>
significación: La tomografía por sonido fotográfico dinámico (PACT) es una técnica valiosa para monitorear procesos fisiológicos. Sin embargo, las técnicas de PACT dinámicas actuales suelen limitarse a la imagen espacial bidimensional. Los escáneres de PACT volumétricos comerciales suelen emplear una gánglia rotatoria en la que los datos tomográficos se adquieren secuencialmente. Como el objeto varía durante el proceso de adquisición de datos, la adquisición de datos secuenciales plantea desafíos en la reconstrucción de imágenes asociada con la incompletitud de los datos. El método propuesto es altamente significativo ya que abordará estos desafíos y permitirá la imagen de volumetría dinámica PACT con imagers existentes. objetivo: El objetivo de este estudio es desarrollar un método de reconstrucción de imágenes espacio-temporal (STIR) para la PACT dinámica que pueda aplicarse a los imagers volumétricos PACT comerciales que utilizan una estrategia de escaneo secuencial. El método propuesto busca superar los desafíos causados por el número limitado de medidas tomográficas adquiridas por frame. enfoque: Se propone un método de estimación de matrices de baja riqueza (LRME-STIR) para la reconstrucción de imágenes espacio-temporales. El método de LRME-STIR aprovecha las redundancias espacio-temporales para reconstruir precisamente una imagen espacio-temporal de 4D. resultados: Los estudios numéricos respaldan la eficacia del método LRME-STIR en la reconstrucción de imágenes dinámicas de 4D a partir de medidas adquiridas con una gánglia rotatoria. El estudio experimental demuestra la capacidad del método para recuperar fielmente el flujo de un agente de contraste a una tasa de cuadros de 0,1 s, incluso cuando solo se adquieren medidas tomográficas por frame. conclusiones: El método LRME-STIR ofrece una solución prometedora para los desafíos que enfrenta la imagen de volumetría dinámica con imagers existentes. Al permitir la reconstrucción precisa de imágenes de 4D, este método tiene el potencial de avanzar en la investigación preclínica.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/01/eess.IV_2023_10_01/" data-id="clp869u7j01ayk588gkqf5nzo" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/36/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/35/">35</a><a class="page-number" href="/page/36/">36</a><span class="page-number current">37</span><a class="page-number" href="/page/38/">38</a><a class="page-number" href="/page/39/">39</a><span class="space">&hellip;</span><a class="page-number" href="/page/97/">97</a><a class="extend next" rel="next" href="/page/38/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">66</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">81</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">140</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
