
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/40/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-eess.IV_2023_09_28" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/28/eess.IV_2023_09_28/" class="article-date">
  <time datetime="2023-09-28T09:00:00.000Z" itemprop="datePublished">2023-09-28</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/28/eess.IV_2023_09_28/">eess.IV - 2023-09-28</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Neuromorphic-Imaging-with-Joint-Image-Deblurring-and-Event-Denoising"><a href="#Neuromorphic-Imaging-with-Joint-Image-Deblurring-and-Event-Denoising" class="headerlink" title="Neuromorphic Imaging with Joint Image Deblurring and Event Denoising"></a>Neuromorphic Imaging with Joint Image Deblurring and Event Denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16106">http://arxiv.org/abs/2309.16106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pei Zhang, Haosen Liu, Zhou Ge, Chutian Wang, Edmund Y. Lam</li>
<li>for: 增强 neuromorphic 感知器的感知质量和精度，以便更好地进行神经元推理和分析。</li>
<li>methods: 提出了一种简单 yet effective的联合算法，可以同时重建锐利图像和噪声Robust事件，并利用事件 regularized prior 提供 auxiliary motion features  для隐藏的噪声除去，以及图像梯度作为参照进行神经omorphic 噪声除去。</li>
<li>results: 在实际和synthetic 样本上进行了广泛的评估，并显示了我们的方法在 restore 质量和鲁棒性方面具有竞争力，并且在一些具有挑战性的实际场景下具有更高的robustness。<details>
<summary>Abstract</summary>
Neuromorphic imaging reacts to per-pixel brightness changes of a dynamic scene with high temporal precision and responds with asynchronous streaming events as a result. It also often supports a simultaneous output of an intensity image. Nevertheless, the raw events typically involve a great amount of noise due to the high sensitivity of the sensor, while capturing fast-moving objects at low frame rates results in blurry images. These deficiencies significantly degrade human observation and machine processing. Fortunately, the two information sources are inherently complementary -- events with microsecond temporal resolution, which are triggered by the edges of objects that are recorded in latent sharp images, can supply rich motion details missing from the blurry images. In this work, we bring the two types of data together and propose a simple yet effective unifying algorithm to jointly reconstruct blur-free images and noise-robust events, where an event-regularized prior offers auxiliary motion features for blind deblurring, and image gradients serve as a reference to regulate neuromorphic noise removal. Extensive evaluations on real and synthetic samples present our superiority over other competing methods in restoration quality and greater robustness to some challenging realistic scenarios. Our solution gives impetus to the improvement of both sensing data and paves the way for highly accurate neuromorphic reasoning and analysis.
</details>
<details>
<summary>摘要</summary>
(Note: The text has been translated into Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. The translation is written in the formal style, which is appropriate for academic or professional writing.)
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/28/eess.IV_2023_09_28/" data-id="clp89doo801asi78870lxb1td" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_09_28" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/28/eess.SP_2023_09_28/" class="article-date">
  <time datetime="2023-09-28T08:00:00.000Z" itemprop="datePublished">2023-09-28</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/28/eess.SP_2023_09_28/">eess.SP - 2023-09-28</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Contrast-detection-is-enhanced-by-deterministic-high-frequency-transcranial-alternating-current-stimulation-with-triangle-and-sine-waveform"><a href="#Contrast-detection-is-enhanced-by-deterministic-high-frequency-transcranial-alternating-current-stimulation-with-triangle-and-sine-waveform" class="headerlink" title="Contrast detection is enhanced by deterministic, high-frequency transcranial alternating current stimulation with triangle and sine waveform"></a>Contrast detection is enhanced by deterministic, high-frequency transcranial alternating current stimulation with triangle and sine waveform</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03763">http://arxiv.org/abs/2310.03763</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weronika Potok, Onno van der Groen, Sahana Sivachelvam, Marc Bächinger, Flavio Fröhlich, Laszlo B. Kish, Nicole Wenderoth</li>
<li>for: 这个论文旨在探讨 Stochastic Resonance（SR）现象在神经系统中的应用， SR 是一种在非线性系统中增强信号传输的现象，可以通过添加随机噪声来实现。</li>
<li>methods: 这个论文使用了 transcranial random noise stimulation (tRNS) 和 transcranial alternating current stimulation (tACS) 两种方法来实现 SR。</li>
<li>results: 研究发现，使用 tACS 和 tRNS 可以降低视觉检测阈值，并且两种方法的效果相当。这表明，SR 可以通过添加 deterministic 信号来实现，而不仅仅是随机噪声。<details>
<summary>Abstract</summary>
Stochastic Resonance (SR) describes a phenomenon where an additive noise (stochastic carrier-wave) enhances the signal transmission in a nonlinear system. In the nervous system, nonlinear properties are present from the level of single ion channels all the way to perception and appear to support the emergence of SR. For example, SR has been repeatedly demonstrated for visual detection tasks, also by adding noise directly to cortical areas via transcranial random noise stimulation (tRNS). When dealing with nonlinear physical systems, it has been suggested that resonance can be induced not only by adding stochastic signals (i.e., noise) but also by adding a large class of signals that are not stochastic in nature which cause "deterministic amplitude resonance" (DAR). Here we mathematically show that high-frequency, deterministic, periodic signals can yield resonance-like effects with linear transfer and infinite signal-to-noise ratio at the output. We tested this prediction empirically and investigated whether non-random, high-frequency, transcranial alternating current stimulation applied to visual cortex could induce resonance-like effects and enhance performance of a visual detection task. We demonstrated in 28 participants that applying 80 Hz triangular-waves or sine-waves with tACS reduced visual contrast detection threshold for optimal brain stimulation intensities. The influence of tACS on contrast sensitivity was equally effective to tRNS-induced modulation, demonstrating that both tACS and tRNS can reduce contrast detection thresholds. Our findings suggest that a resonance-like mechanism can also emerge when deterministic electrical waveforms are applied via tACS.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="T1-T2-relaxation-temporal-modelling-from-accelerated-acquisitions-using-a-Latent-Transformer"><a href="#T1-T2-relaxation-temporal-modelling-from-accelerated-acquisitions-using-a-Latent-Transformer" class="headerlink" title="T1&#x2F;T2 relaxation temporal modelling from accelerated acquisitions using a Latent Transformer"></a>T1&#x2F;T2 relaxation temporal modelling from accelerated acquisitions using a Latent Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16853">http://arxiv.org/abs/2309.16853</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fanwen Wang, Michael Tanzer, Mengyun Qiao, Wenjia Bai, Daniel Rueckert, Guang Yang, Sonia Nielles-Vallespin</li>
<li>for: 这个论文旨在提高心肺成像中的速度和精度，使得它们能够广泛应用于临床。</li>
<li>methods: 该论文使用深度学习方法，特别是Latent Transformer模块，来模型 Parameterized time frames 之间的关系，从而提高从受限样本数据中的重建。</li>
<li>results: 论文中的结果表明，通过Explicitly incorporating time dynamics，模型可以recover higher fidelity T1和T2 mapping，并且不受artefacts的干扰。这个研究证明了在量子MRI中的时间模型非常重要。<details>
<summary>Abstract</summary>
Quantitative cardiac magnetic resonance T1 and T2 mapping enable myocardial tissue characterisation but the lengthy scan times restrict their widespread clinical application. We propose a deep learning method that incorporates a time dependency Latent Transformer module to model relationships between parameterised time frames for improved reconstruction from undersampled data. The module, implemented as a multi-resolution sequence-to-sequence transformer, is integrated into an encoder-decoder architecture to leverage the inherent temporal correlations in relaxation processes. The presented results for accelerated T1 and T2 mapping show the model recovers maps with higher fidelity by explicit incorporation of time dynamics. This work demonstrates the importance of temporal modelling for artifact-free reconstruction in quantitative MRI.
</details>
<details>
<summary>摘要</summary>
量化冠动磁共振T1和T2映射可以 caracterizar la tissue cardíaca, pero los tiempos de escaneo prolongados limitan su aplicación clínica amplia. Proponemos un método de aprendizaje profundo que incorpora un módulo de dependencia temporal Latent Transformer para modelar las relaciones entre los marcos de tiempo parameterizados para mejorar la reconstrucción a partir de datos sub-procesados. El módulo, implementado como un transformer de secuencia a secuencia de múltiples resoluciones, se integra en un arquitectura de codificador-decodificador para aprovechar las correlaciones temporales inherentes en los procesos de relajación. Los resultados presentados para la aceleración de T1 y T2 mapping muestran que el modelo recupera mapas con una fidelidad más alta al incorporar explícitamente las dinámicas del tiempo. Este trabajo demuestra la importancia del modelado temporal para la reconstrucción libre de artifactos en la imagen de resonancia magnética cuántica.
</details></li>
</ul>
<hr>
<h2 id="Business-Model-Canvas-for-Micro-Operators-in-5G-Coopetitive-Ecosystem"><a href="#Business-Model-Canvas-for-Micro-Operators-in-5G-Coopetitive-Ecosystem" class="headerlink" title="Business Model Canvas for Micro Operators in 5G Coopetitive Ecosystem"></a>Business Model Canvas for Micro Operators in 5G Coopetitive Ecosystem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16845">http://arxiv.org/abs/2309.16845</a></li>
<li>repo_url: None</li>
<li>paper_authors: Javane Rostampoor, Roghayeh Joda, Mohammad Dindoost</li>
<li>for: 本研究旨在提供5G微型运营商业模式框架，以帮助新的5G业务创造价值。</li>
<li>methods: 本研究采用了商业模式canvas（BMC）的概念，以分析5G微型运营商业模式的发展。</li>
<li>results: 研究发现，5G微型运营商业模式框架可以帮助新的5G业务创造价值，并且可以在5G协同环境中实现更好的覆盖率和容量。<details>
<summary>Abstract</summary>
In order to address the need for more capacity and coverage in the 5th generation (5G) of wireless networks, ultra-dense wireless networks are introduced which mainly consist of indoor small cells. This new architecture has paved the way for the advent of a new concept called Micro Operator. A micro operator is an entity that provides connections and local 5G services to the customers and relies on local frequency resources. We discuss business models of micro operators in a 5G coopetitive environment and develop a framework to indicate the business model canvas (BMC) of this new concept. Providing BMC for new businesses is a strategic approach to offer value to customers. In this research study, BMC and its elements are introduced and explained for 5G micro operators.
</details>
<details>
<summary>摘要</summary>
为了满足5G网络的容量和覆盖需求，ultra-dense无线网络被引入，主要由室内小终端组成。这新的架构为微运营者的出现提供了方便。微运营者是一个为客户提供连接和本地5G服务的实体，并且依靠本地频率资源。我们研究了5G协作环境中微运营者的业务模式，并开发了一个框架来指示微运营者的业务模型Canvas（BMC）。为新的业务提供BMC是一种策略性的方法，以便为客户提供价值。在这项研究中，BMC和其元素被介绍和解释了5G微运营者。
</details></li>
</ul>
<hr>
<h2 id="Wi-Fi-8-Embracing-the-Millimeter-Wave-Era"><a href="#Wi-Fi-8-Embracing-the-Millimeter-Wave-Era" class="headerlink" title="Wi-Fi 8: Embracing the Millimeter-Wave Era"></a>Wi-Fi 8: Embracing the Millimeter-Wave Era</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16813">http://arxiv.org/abs/2309.16813</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoqian Liu, Tingwei Chen, Yuhan Dong, Zhi Mao, Ming Gan, Xun Yang, Jianmin Lu</li>
<li>for: 这篇论文探讨了未来的Wi-Fi 8技术，尤其是兆米波技术的应用。</li>
<li>methods: 该论文通过 simulations 提供了一个全面的未来Wi-Fi 8技术的视角，并且研究了兆米波技术的可能性。</li>
<li>results: 模拟结果表明，兆米波技术可以实现显著的性能提升，即使硬件障碍存在。<details>
<summary>Abstract</summary>
With the increasing demands in communication, Wi-Fi technology is advancing towards its next generation. Building on the foundation of Wi-Fi 7, millimeter-wave technology is anticipated to converge with Wi-Fi 8 in the near future. In this paper, we look into the millimeter-wave technology and other potential feasible features, providing a comprehensive perspective on the future of Wi-Fi 8. Our simulation results demonstrate that significant performance gains can be achieved, even in the presence of hardware impairments.
</details>
<details>
<summary>摘要</summary>
随着通信需求的增长，Wi-Fi技术正在迈向下一代。基于Wi-Fi 7的基础上， millimeter-wave技术预计将与Wi-Fi 8相结合在不远的未来。本文将 millimeter-wave技术和其他可能实现的特性进行全面探讨，为Wi-Fi 8的未来提供全面的视角。我们的 simulations 结果表明，即使硬件障碍存在，也可以实现显著的性能提升。
</details></li>
</ul>
<hr>
<h2 id="On-the-Role-of-5G-and-Beyond-Sidelink-Communication-in-Multi-Hop-Tactical-Networks"><a href="#On-the-Role-of-5G-and-Beyond-Sidelink-Communication-in-Multi-Hop-Tactical-Networks" class="headerlink" title="On the Role of 5G and Beyond Sidelink Communication in Multi-Hop Tactical Networks"></a>On the Role of 5G and Beyond Sidelink Communication in Multi-Hop Tactical Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16628">http://arxiv.org/abs/2309.16628</a></li>
<li>repo_url: None</li>
<li>paper_authors: Charles E. Thornton, Evan Allen, Evar Jones, Daniel Jakubisin, Fred Templin, Lingjia Liu</li>
<li>for: 本文研究了5G和以后的副链（SL）通信可以支持多跳策略网络。</li>
<li>methods: 本文首先提供了3GPP SL标准化活动的技术和历史概述，然后考虑了在战略网络中的应用问题。文章考虑了许多多跳路由技术，这些技术预期会对SL启用多跳策略网络中很有用。文章还考虑了开源工具，可以用于网络模拟。</li>
<li>results: 本文讨论了5G SL启用多跳策略网络中的一些问题，如RLS感知和定位的 инте格ция，以及新的机器学习工具，如联邦学习和分布式学习，可以用于资源分配和路由问题。文章 conclude by summarizing recent developments in the 5G SL literature and provide guidelines for future research。<details>
<summary>Abstract</summary>
This work investigates the potential of 5G and beyond sidelink (SL) communication to support multi-hop tactical networks. We first provide a technical and historical overview of 3GPP SL standardization activities, and then consider applications to current problems of interest in tactical networking. We consider a number of multi-hop routing techniques which are expected to be of interest for SL-enabled multi-hop tactical networking and examine open-source tools useful for network emulation. Finally, we discuss relevant research directions which may be of interest for 5G SL-enabled tactical communications, namely the integration of RF sensing and positioning, as well as emerging machine learning tools such as federated and decentralized learning, which may be of great interest for resource allocation and routing problems that arise in tactical applications. We conclude by summarizing recent developments in the 5G SL literature and provide guidelines for future research.
</details>
<details>
<summary>摘要</summary>
这项研究探讨了5G和以后宽带侧链（SL）通信的潜力来支持多跳策略网络。我们首先提供了技术和历史概述3GPP SL标准化活动，然后考虑了应用于战斗网络中的现有问题。我们考虑了一些多跳路由技术，这些技术预计将对SL启用多跳战斗网络中具有 интерес。我们还考虑了开源工具，可以用于网络模拟。最后，我们讨论了5G SL启用的相关研究方向，包括 integrate RF探测和定位，以及emerging machine learning工具，如联邦和分布式学习，这些工具可能对战斗应用中的资源分配和路由问题具有很大的意义。我们结束于summarizing recent developments in 5G SL literature and provide guidelines for future research。Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="HyperLISTA-ABT-An-Ultra-light-Unfolded-Network-for-Accurate-Multi-component-Differential-Tomographic-SAR-Inversion"><a href="#HyperLISTA-ABT-An-Ultra-light-Unfolded-Network-for-Accurate-Multi-component-Differential-Tomographic-SAR-Inversion" class="headerlink" title="HyperLISTA-ABT: An Ultra-light Unfolded Network for Accurate Multi-component Differential Tomographic SAR Inversion"></a>HyperLISTA-ABT: An Ultra-light Unfolded Network for Accurate Multi-component Differential Tomographic SAR Inversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16468">http://arxiv.org/abs/2309.16468</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kun Qian, Yuanyuan Wang, Peter Jung, Yilei Shi, Xiao Xiang Zhu</li>
<li>for: 提高深度学习基于迭代算法的四维影像重建（4D）精度和效率。</li>
<li>methods: 提出了一种高效精度的HyperLISTA-ABT算法，使用分析方式确定网络参数，并实现了 Adaptive Blockwise Thresholding 技术，以提高全面阈值处理。</li>
<li>results: 通过实验和实际数据测试，显示HyperLISTA-ABT可以在有限的计算资源和时间下获得高质量的4D点云重建。<details>
<summary>Abstract</summary>
Deep neural networks based on unrolled iterative algorithms have achieved remarkable success in sparse reconstruction applications, such as synthetic aperture radar (SAR) tomographic inversion (TomoSAR). However, the currently available deep learning-based TomoSAR algorithms are limited to three-dimensional (3D) reconstruction. The extension of deep learning-based algorithms to four-dimensional (4D) imaging, i.e., differential TomoSAR (D-TomoSAR) applications, is impeded mainly due to the high-dimensional weight matrices required by the network designed for D-TomoSAR inversion, which typically contain millions of freely trainable parameters. Learning such huge number of weights requires an enormous number of training samples, resulting in a large memory burden and excessive time consumption. To tackle this issue, we propose an efficient and accurate algorithm called HyperLISTA-ABT. The weights in HyperLISTA-ABT are determined in an analytical way according to a minimum coherence criterion, trimming the model down to an ultra-light one with only three hyperparameters. Additionally, HyperLISTA-ABT improves the global thresholding by utilizing an adaptive blockwise thresholding scheme, which applies block-coordinate techniques and conducts thresholding in local blocks, so that weak expressions and local features can be retained in the shrinkage step layer by layer. Simulations were performed and demonstrated the effectiveness of our approach, showing that HyperLISTA-ABT achieves superior computational efficiency and with no significant performance degradation compared to state-of-the-art methods. Real data experiments showed that a high-quality 4D point cloud could be reconstructed over a large area by the proposed HyperLISTA-ABT with affordable computational resources and in a fast time.
</details>
<details>
<summary>摘要</summary>
深度神经网络基于迭代算法已经在稀疏重建应用中获得了惊人的成功，如Synthetic Aperture Radar（SAR）tomographic逆转（TomoSAR）。然而，目前可用的深度学习基于算法只能处理三维（3D）重建。将深度学习基于算法扩展到四维（4D）成像，即差分Tomography（D-TomoSAR）应用，受限于高维度权重矩阵需要的深度学习模型中的大量自由调节参数。学习这么多参数需要极大的训练样本数和巨大的内存压力，导致训练时间过长。为解决这个问题，我们提出了一种高效和准确的算法called HyperLISTA-ABT。HyperLISTA-ABT中的权重由分析方式决定，以最小干扰 criterion 来确定，因此模型的参数减少到了 ultra-light 的三个超参数。此外，HyperLISTA-ABT还改进了全球阈值处理，通过使用adaptive blockwise阈值处理方案，在本地块中进行阈值处理，以保留弱表达和本地特征在压缩步骤中。我们的方法通过实验表明，HyperLISTA-ABT可以实现高效的计算和快速的训练，而无需极大的训练样本数和内存压力。真实数据实验也表明，通过我们的方法可以在大面积的4D点云重建中获得高质量的重建结果，并且可以在有限的计算资源和快速的时间内完成。
</details></li>
</ul>
<hr>
<h2 id="Feed-forward-and-recurrent-inhibition-for-compressing-and-classifying-high-dynamic-range-biosignals-in-spiking-neural-network-architectures"><a href="#Feed-forward-and-recurrent-inhibition-for-compressing-and-classifying-high-dynamic-range-biosignals-in-spiking-neural-network-architectures" class="headerlink" title="Feed-forward and recurrent inhibition for compressing and classifying high dynamic range biosignals in spiking neural network architectures"></a>Feed-forward and recurrent inhibition for compressing and classifying high dynamic range biosignals in spiking neural network architectures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16425">http://arxiv.org/abs/2309.16425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rachel Sava, Elisa Donati, Giacomo Indiveri</li>
<li>for:  This paper aims to address the challenge of compressing high-dynamic range biosignals in spiking neural network (SNN) architectures.</li>
<li>methods: The authors propose a biologically-inspired strategy that utilizes three adaptation mechanisms found in the brain: spike-frequency adaptation, feed-forward inhibitory connections, and Excitatory-Inhibitory (E-I) balance.</li>
<li>results: The authors validate the approach in silico using a simple network applied to a gesture classification task from surface EMG recordings.<details>
<summary>Abstract</summary>
Neuromorphic processors that implement Spiking Neural Networks (SNNs) using mixed-signal analog/digital circuits represent a promising technology for closed-loop real-time processing of biosignals. As in biology, to minimize power consumption, the silicon neurons' circuits are configured to fire with a limited dynamic range and with maximum firing rates restricted to a few tens or hundreds of Herz.   However, biosignals can have a very large dynamic range, so encoding them into spikes without saturating the neuron outputs represents an open challenge.   In this work, we present a biologically-inspired strategy for compressing this high-dynamic range in SNN architectures, using three adaptation mechanisms ubiquitous in the brain: spike-frequency adaptation at the single neuron level, feed-forward inhibitory connections from neurons belonging to the input layer, and Excitatory-Inhibitory (E-I) balance via recurrent inhibition among neurons in the output layer.   We apply this strategy to input biosignals encoded using both an asynchronous delta modulation method and an energy-based pulse-frequency modulation method.   We validate this approach in silico, simulating a simple network applied to a gesture classification task from surface EMG recordings.
</details>
<details>
<summary>摘要</summary>
神经omorphic处理器实现基于异步 delta 模ulation和能量基本的脉冲频率调制的脑神经网络（SNN），通过混合 analog/digital 电路实现closed-loop实时处理生物信号。在生物体内，为了减少能耗，silicon neuron circuit 配置为在有限的动态范围内发射，最大发射频率限制在一些百或上百 Herz 内。但生物信号可以有非常大的动态范围，因此将它们编码成脉冲无需满足 neuron 输出的限制是一个开放的挑战。在这种工作中，我们提出了基于生物体内的三种适应机制来压缩高动态范围的 SNN 建筑，包括单个 neuron 层的脉冲频率适应、输入层的前向抑制连接和输出层的律动抑制。我们将这些机制应用到输入的生物信号，使用异步 delta 模ulation 和能量基本的脉冲频率调制方法来编码。我们在 silico 中验证了这种方法，对一个简单的网络进行了surface EMG 记录的手势识别任务。
</details></li>
</ul>
<hr>
<h2 id="A-Universal-Framework-for-Holographic-MIMO-Sensing"><a href="#A-Universal-Framework-for-Holographic-MIMO-Sensing" class="headerlink" title="A Universal Framework for Holographic MIMO Sensing"></a>A Universal Framework for Holographic MIMO Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16389">http://arxiv.org/abs/2309.16389</a></li>
<li>repo_url: None</li>
<li>paper_authors: Charles Vanwynsberghe, Jiguang He, Mérouane Debbah</li>
<li>for: 这篇论文旨在解决具有不规则形状的连续天线感知空间的问题。</li>
<li>methods: 该论文提出了一种通用框架，可以无论天线的形状，准确地确定天线的感知空间。这种方法基于采样场的几何分析，并且可以在空间和频率域上彰显sampled场的特性。</li>
<li>results: 实验结果表明，该方法可以准确地估算不同形状天线的度量域，并且可以扩展到真实的具有折叠性的天线。<details>
<summary>Abstract</summary>
This paper addresses the sensing space identification of arbitrarily shaped continuous antennas. In the context of holographic multiple-input multiple-output (MIMO), a.k.a. large intelligent surfaces, these antennas offer benefits such as super-directivity and near-field operability. The sensing space reveals two key aspects: (a) its dimension specifies the maximally achievable spatial degrees of freedom (DoFs), and (b) the finite basis spanning this space accurately describes the sampled field. Earlier studies focus on specific geometries, bringing forth the need for extendable analysis to real-world conformal antennas. Thus, we introduce a universal framework to determine the antenna sensing space, regardless of its shape. The findings underscore both spatial and spectral concentration of sampled fields to define a generic eigenvalue problem of Slepian concentration. Results show that this approach precisely estimates the DoFs of well-known geometries, and verify its flexible extension to conformal antennas.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>The dimension of the sensing space specifies the maximum achievable spatial degrees of freedom (DoFs).2. The finite basis spanning the sensing space accurately describes the sampled field.Previous studies have focused on specific geometries, highlighting the need for a more extendable analysis that can be applied to real-world conformal antennas. To address this, the paper introduces a universal framework for determining the antenna sensing space, regardless of its shape.The results demonstrate both spatial and spectral concentration of sampled fields, which can be used to define a generic eigenvalue problem of Slepian concentration. The approach precisely estimates the DoFs of well-known geometries and verifies its flexibility in extending to conformal antennas.</details></li>
</ol>
<hr>
<h2 id="Convex-Estimation-of-Sparse-Smooth-Power-Spectral-Densities-from-Mixtures-of-Realizations-with-Application-to-Weather-Radar"><a href="#Convex-Estimation-of-Sparse-Smooth-Power-Spectral-Densities-from-Mixtures-of-Realizations-with-Application-to-Weather-Radar" class="headerlink" title="Convex Estimation of Sparse-Smooth Power Spectral Densities from Mixtures of Realizations with Application to Weather Radar"></a>Convex Estimation of Sparse-Smooth Power Spectral Densities from Mixtures of Realizations with Application to Weather Radar</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16215">http://arxiv.org/abs/2309.16215</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hiroki Kuroda, Daichi Kitahara, Eiichi Yoshikawa, Hiroshi Kikuchi, Tomoo Ushio</li>
<li>for: 估计复杂 random 过程中 sparse 和 smooth power spectral densities (PSDs)</li>
<li>methods: 使用 convex optimization 估计 PSDs</li>
<li>results: 提高估计精度 compared to 现有 sparse estimation models<details>
<summary>Abstract</summary>
In this paper, we propose a convex optimization-based estimation of sparse and smooth power spectral densities (PSDs) of complex-valued random processes from mixtures of realizations. While the PSDs are related to the magnitude of the frequency components of the realizations, it has been a major challenge to exploit the smoothness of the PSDs because penalizing the difference of the magnitude of the frequency components results in a nonconvex optimization problem that is difficult to solve. To address this challenge, we design the proposed model that jointly estimates the complex-valued frequency components and the nonnegative PSDs, which are respectively regularized to be sparse and sparse-smooth. By penalizing the difference of the nonnegative variable that estimates the PSDs, the proposed model can enhance the smoothness of the PSDs via convex optimization. Numerical experiments on the phased array weather radar, an advanced weather radar system, demonstrate that the proposed model achieves superior estimation accuracy compared to existing sparse estimation models, regardless of whether they are combined with a smoothing technique as a post-processing step or not.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出了一种基于凸优化的复杂数据频谱密度（PSD）估计方法，用于识别复杂随机过程中的稀疏和平滑频谱密度。而频谱密度与实现的频率成分的大小有关，但是由于惩罚频谱密度的差异会导致非凸优化问题，这使得估计变得困难。为解决这个挑战，我们设计了提案的模型，它同时估计了复杂的频率成分和非负的频谱密度，并将它们分别正则化为稀疏和稀疏平滑。通过惩罚非负变量，该模型可以通过凸优化提高频谱密度的平滑性。在phasered array weather radar中进行的数值实验表明，提案的模型在存在或不存在融合熵降低技术的情况下都可以 дости到现有稀疏估计模型的高精度估计。
</details></li>
</ul>
<hr>
<h2 id="Hybrid-Digital-Wave-Domain-Channel-Estimator-for-Stacked-Intelligent-Metasurface-Enabled-Multi-User-MISO-Systems"><a href="#Hybrid-Digital-Wave-Domain-Channel-Estimator-for-Stacked-Intelligent-Metasurface-Enabled-Multi-User-MISO-Systems" class="headerlink" title="Hybrid Digital-Wave Domain Channel Estimator for Stacked Intelligent Metasurface Enabled Multi-User MISO Systems"></a>Hybrid Digital-Wave Domain Channel Estimator for Stacked Intelligent Metasurface Enabled Multi-User MISO Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16204">http://arxiv.org/abs/2309.16204</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qurrat-Ul-Ain Nadeem, Jiancheng An, Anas Chaaban</li>
<li>for: 这个论文主要是为了解决堆叠智能元素（SIM）激发的通信系统中的通道估计（CE）问题。</li>
<li>methods: 该论文提出了一种新的混合数字波域频率域通道估计方法，其中收到的训练符号首先在SIM层中进行了波域处理，然后在数字域中进行了加工。</li>
<li>results: 该方法可以在具有限制数量的 радио频率（RF）链的SIM激发通信系统中实现高精度的通道估计，并且可以降低训练负担。<details>
<summary>Abstract</summary>
Stacked intelligent metasurface (SIM) is an emerging programmable metasurface architecture that can implement signal processing directly in the electromagnetic wave domain, thereby enabling efficient implementation of ultra-massive multiple-input multiple-output (MIMO) transceivers with a limited number of radio frequency (RF) chains. Channel estimation (CE) is challenging for SIM-enabled communication systems due to the multi-layer architecture of SIM, and because we need to estimate large dimensional channels between the SIM and users with a limited number of RF chains. To efficiently solve this problem, we develop a novel hybrid digital-wave domain channel estimator, in which the received training symbols are first processed in the wave domain within the SIM layers, and then processed in the digital domain. The wave domain channel estimator, parametrized by the phase shifts applied by the meta-atoms in all layers, is optimized to minimize the mean squared error (MSE) using a gradient descent algorithm, within which the digital part is optimally updated. For an SIM-enabled multi-user system equipped with 4 RF chains and a 6-layer SIM with 64 meta-atoms each, the proposed estimator yields an MSE that is very close to that achieved by fully digital CE in a massive MIMO system employing 64 RF chains. This high CE accuracy is achieved at the cost of a training overhead that can be reduced by exploiting the potential low rank of channel correlation matrices.
</details>
<details>
<summary>摘要</summary>
堆叠智能表面（SIM）是一种emerging的可编程表面建筑，可以直接在电磁波频率频谱中实现信号处理，从而实现高效的多输入多出力（MIMO）接收机器系统的实现，只需要有限的 радио频率（RF）链。但是，频率链的数量不够，使得频率链数量的限制会导致通道估计（CE）变得困难。为解决这个问题，我们开发了一种新的混合式数字波域频率域通道估计器，其中接收训练符号被首先处理在SIM层中的波域内，然后在数字域内进行处理。波域频率域估计器，由SIM层中所有元atom的阶梯shift参数化，使其最小化均方误差（MSE），并使用梯度下降算法优化。对于装备4个RF链和6层SIM的多用户系统，我们的估计器可以与完全数字CE在巨量MIMO系统使用64个RF链的MSE准确。这高度的CE准确性是在训练负担的代价下实现的，并且可以通过利用频率征的低级别相关性来减少训练负担。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Real-Time-Numerical-Differentiation-with-Variable-Rate-Forgetting-and-Exponential-Resetting"><a href="#Adaptive-Real-Time-Numerical-Differentiation-with-Variable-Rate-Forgetting-and-Exponential-Resetting" class="headerlink" title="Adaptive Real-Time Numerical Differentiation with Variable-Rate Forgetting and Exponential Resetting"></a>Adaptive Real-Time Numerical Differentiation with Variable-Rate Forgetting and Exponential Resetting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16159">http://arxiv.org/abs/2309.16159</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shashank Verma, Brian Lai, Dennis S. Bernstein</li>
<li>for: 这个论文旨在解决随时间变化的感知器噪声的问题，提出了基于adaptive实时数值 differentiating和可变速率忘却的AISE方法。</li>
<li>methods: 该论文使用了adaptive实时数值 differentiating和可变速率忘却的AISE方法来解决随时间变化的感知器噪声问题。</li>
<li>results: 该论文的实验结果表明，基于AISE方法的适应式实时数值 differentiating可以更好地适应随时间变化的感知器噪声，并且可以更快地响应 changing 噪声特性。<details>
<summary>Abstract</summary>
Digital PID control requires a differencing operation to implement the D gain. In order to suppress the effects of noisy data, the traditional approach is to filter the data, where the frequency response of the filter is adjusted manually based on the characteristics of the sensor noise. The present paper considers the case where the characteristics of the sensor noise change over time in an unknown way. This problem is addressed by applying adaptive real-time numerical differentiation based on adaptive input and state estimation (AISE). The contribution of this paper is to extend AISE to include variable-rate forgetting with exponential resetting, which allows AISE to more rapidly respond to changing noise characteristics while enforcing the boundedness of the covariance matrix used in recursive least squares.
</details>
<details>
<summary>摘要</summary>
数字PID控制需要 diferencing 操作实现 D 增益。为了降低噪声数据的影响，传统方法是使用滤波器处理数据，其滤波器频率响应需要手动调整基于传感器噪声特性。本文考虑了情况下噪声特性随时间变化的情况，这个问题通过实时数字梯度计算和状态估计（AISE）进行解决。本文的贡献在于将 AISE 扩展到包括变化率忘记和加速忘记，使 AISE 能更快地响应变化噪声特性，同时保证使用 recursive least squares 中的 covariance 矩阵的 boundedness。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/28/eess.SP_2023_09_28/" data-id="clp89dopz01evi788c7834gb6" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_09_27" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/27/cs.SD_2023_09_27/" class="article-date">
  <time datetime="2023-09-27T15:00:00.000Z" itemprop="datePublished">2023-09-27</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/27/cs.SD_2023_09_27/">cs.SD - 2023-09-27</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Does-Single-channel-Speech-Enhancement-Improve-Keyword-Spotting-Accuracy-A-Case-Study"><a href="#Does-Single-channel-Speech-Enhancement-Improve-Keyword-Spotting-Accuracy-A-Case-Study" class="headerlink" title="Does Single-channel Speech Enhancement Improve Keyword Spotting Accuracy? A Case Study"></a>Does Single-channel Speech Enhancement Improve Keyword Spotting Accuracy? A Case Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16060">http://arxiv.org/abs/2309.16060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Avamarie Brueggeman, Takuya Higuchi, Masood Delfarah, Stephen Shum, Vineet Garg</li>
<li>for: 提高自动语音识别精度</li>
<li>methods: 单道信号提升、Audio投入、模型融合</li>
<li>results: 单道信号提升可以提高keyword spotting精度，但无法在听力训练后提高精度<details>
<summary>Abstract</summary>
Noise robustness is a key aspect of successful speech applications. Speech enhancement (SE) has been investigated to improve automatic speech recognition accuracy; however, its effectiveness for keyword spotting (KWS) is still under-investigated. In this paper, we conduct a comprehensive study on single-channel speech enhancement for keyword spotting on the Google Speech Command (GSC) dataset. To investigate robustness to noise, the GSC dataset is augmented with noise signals from the WSJ0 Hipster Ambient Mixtures (WHAM!) noise dataset. Our investigation includes not only applying SE before KWS but also performing joint training of the SE frontend and KWS backend models. Moreover, we explore audio injection, a common approach to reduce distortions by using a weighted average of the enhanced and original signals. Audio injection is then further optimized by using another model that predicts the weight for each utterance. Our investigation reveals that SE can improve KWS accuracy on noisy speech when the backend model is trained on clean speech; however, despite our extensive exploration, it is difficult to improve the KWS accuracy with SE when the backend is trained on noisy speech.
</details>
<details>
<summary>摘要</summary>
噪声Robustness是成功语音应用程序的关键方面。语音增强（SE）已经被研究以提高自动语音识别精度，但是它对关键词搜索（KWS）的影响还未得到充分调查。在这篇论文中，我们进行了对单通道语音增强的全面研究，以提高Google语音命令（GSC）数据集上的关键词搜索精度。为了调查噪声的影响，我们使用WHAM!噪声数据集中的噪声信号来扩展GSC数据集。我们的调查包括不仅将SE应用于KWS前置处理，还包括将SE前端和KWS后端模型进行共同训练。此外，我们还探索了音频注入，一种常见的方法，通过使用每个语音的权重来减少损害。我们发现，当后端模型训练于干净语音时，SE可以提高KWS精度在噪声语音中；但是，我们进行了广泛的探索，但是很难通过SE提高KWS精度，当后端模型训练于噪声语音。
</details></li>
</ul>
<hr>
<h2 id="Neural-Network-Augmented-Kalman-Filter-for-Robust-Acoustic-Howling-Suppression"><a href="#Neural-Network-Augmented-Kalman-Filter-for-Robust-Acoustic-Howling-Suppression" class="headerlink" title="Neural Network Augmented Kalman Filter for Robust Acoustic Howling Suppression"></a>Neural Network Augmented Kalman Filter for Robust Acoustic Howling Suppression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16049">http://arxiv.org/abs/2309.16049</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/YIXUANZ/NeuralKalmanAHS">https://github.com/YIXUANZ/NeuralKalmanAHS</a></li>
<li>paper_authors: Yixuan Zhang, Hao Zhang, Meng Yu, Dong Yu</li>
<li>for: 提高音频通信系统中喊叫干扰（AHS）的性能</li>
<li>methods: 利用神经网络（NN）增强传统的加尔曼筛算法，提高适应性和扩展参数</li>
<li>results: 比起独立的NN和加尔曼筛方法，提出的方法实现了更好的AHS性能，经验验证了方法的有效性<details>
<summary>Abstract</summary>
Acoustic howling suppression (AHS) is a critical challenge in audio communication systems. In this paper, we propose a novel approach that leverages the power of neural networks (NN) to enhance the performance of traditional Kalman filter algorithms for AHS. Specifically, our method involves the integration of NN modules into the Kalman filter, enabling refining reference signal, a key factor in effective adaptive filtering, and estimating covariance metrics for the filter which are crucial for adaptability in dynamic conditions, thereby obtaining improved AHS performance. As a result, the proposed method achieves improved AHS performance compared to both standalone NN and Kalman filter methods. Experimental evaluations validate the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
喷流喊叫控制（AHS）是音频通信系统中的一个关键挑战。在这篇论文中，我们提出了一种新的方法，利用神经网络（NN）提高传统的卡尔曼筛算法的AHS性能。具体来说，我们的方法是将NN模块与卡尔曼筛相结合，以便更好地调整参照信号，这是有效的适应滤波的关键因素，并估算筛子的协方差度量，这些度量对于在动态条件下的适应性至关重要。因此，我们的方法可以实现AHS性能的改进。实验评估表明，我们的方法比单独使用NN和卡尔曼筛方法都有更好的性能。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Acoustic-Howling-Suppression-through-Recursive-Training-of-Neural-Networks"><a href="#Advancing-Acoustic-Howling-Suppression-through-Recursive-Training-of-Neural-Networks" class="headerlink" title="Advancing Acoustic Howling Suppression through Recursive Training of Neural Networks"></a>Advancing Acoustic Howling Suppression through Recursive Training of Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16048">http://arxiv.org/abs/2309.16048</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/YIXUANZ/AHS_2023_1">https://github.com/YIXUANZ/AHS_2023_1</a></li>
<li>paper_authors: Hao Zhang, Yixuan Zhang, Meng Yu, Dong Yu</li>
<li>for: 本研究旨在解决声学喊响问题，提出了一种基于神经网络（NN）模块的训练框架，以便坚实地 Addressing the acoustic howling issue by examining its fundamental formation process.</li>
<li>methods: 该框架在训练过程中将NN模块 integrate into the closed-loop system，通过在训练过程中使用生成回传信号来尝试模拟实际应用场景中的喊响供应（AHS）流程。此外，该框架还提出了两种方法：一种仅采用NN，另一种 combining NN with the traditional Kalman filter。</li>
<li>results: 实验结果表明，该框架可以对声学喊响供应进行有效的抑制，与前一代基于NN的方法相比，具有显著的改进。<details>
<summary>Abstract</summary>
In this paper, we introduce a novel training framework designed to comprehensively address the acoustic howling issue by examining its fundamental formation process. This framework integrates a neural network (NN) module into the closed-loop system during training with signals generated recursively on the fly to closely mimic the streaming process of acoustic howling suppression (AHS). The proposed recursive training strategy bridges the gap between training and real-world inference scenarios, marking a departure from previous NN-based methods that typically approach AHS as either noise suppression or acoustic echo cancellation. Within this framework, we explore two methodologies: one exclusively relying on NN and the other combining NN with the traditional Kalman filter. Additionally, we propose strategies, including howling detection and initialization using pre-trained offline models, to bolster trainability and expedite the training process. Experimental results validate that this framework offers a substantial improvement over previous methodologies for acoustic howling suppression.
</details>
<details>
<summary>摘要</summary>
在本文中，我们介绍了一种新的训练框架，旨在全面Addressing the acoustic howling issue by examining its fundamental formation process. 这个框架通过在训练过程中 интеGRATE一个神经网络（NN）模块到关闭Loop系统中，通过在飞行中生成的信号来准确模拟流动Acoustic howling suppression（AHS）的流程。我们的 recursive training strategy bridge the gap between training and real-world inference scenarios, 与前一些NN-based方法不同，通常将AHS看作是噪声Suppression或Acoustic echo cancellation。在这个框架中，我们探讨了两种方法：一种完全依赖于NN，另一种 combining NN with the traditional Kalman filter。此外，我们还提出了一些策略，包括如何探测和初始化难以训练的模型，以增强训练可靠性和加速训练过程。实验结果表明，这个框架可以substantially improve the acoustic howling suppression compared to previous methodologies.
</details></li>
</ul>
<hr>
<h2 id="Multichannel-Voice-Trigger-Detection-Based-on-Transform-average-concatenate"><a href="#Multichannel-Voice-Trigger-Detection-Based-on-Transform-average-concatenate" class="headerlink" title="Multichannel Voice Trigger Detection Based on Transform-average-concatenate"></a>Multichannel Voice Trigger Detection Based on Transform-average-concatenate</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16036">http://arxiv.org/abs/2309.16036</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takuya Higuchi, Avamarie Brueggeman, Masood Delfarah, Stephen Shum</li>
<li>for: 提高voice triggering（VT）系统的准确率和效率，使得用户可以更加方便地使用语音识别技术。</li>
<li>methods: 提出了一种基于多通道听音模型的VT系统，使得系统可以直接从多通道输入中提取有用信息，而不需要额外的渠道选择和筛选步骤。</li>
<li>results: 对比基eline channel选择方法，提出的方法可以降低false rejection rate（FRR）达到30%，提高VT系统的准确率和效率。<details>
<summary>Abstract</summary>
Voice triggering (VT) enables users to activate their devices by just speaking a trigger phrase. A front-end system is typically used to perform speech enhancement and/or separation, and produces multiple enhanced and/or separated signals. Since conventional VT systems take only single-channel audio as input, channel selection is performed. A drawback of this approach is that unselected channels are discarded, even if the discarded channels could contain useful information for VT. In this work, we propose multichannel acoustic models for VT, where the multichannel output from the frond-end is fed directly into a VT model. We adopt a transform-average-concatenate (TAC) block and modify the TAC block by incorporating the channel from the conventional channel selection so that the model can attend to a target speaker when multiple speakers are present. The proposed approach achieves up to 30% reduction in the false rejection rate compared to the baseline channel selection approach.
</details>
<details>
<summary>摘要</summary>
通过语音触发（VT），用户可以通过说出触发语言来活动设备。前端系统通常用于进行语音增强和/或分离，生成多个增强和/或分离的信号。由于传统VT系统只接受单 кана声音输入，因此需要进行通道选择。这种方法的缺点是会抛弃未选择的通道，即使这些抛弃的通道可能包含有用信息 для VT。在这项工作中，我们提议使用多通道音频模型来进行VT，其中前端输出的多通道输入直接传递到VT模型中。我们采用了变换均值 concatenate（TAC）块，并将TAC块修改为包括传统通道选择的通道，以便模型可以在多个说话者存在时听到目标说话者。我们的方法可以相比基eline通道选择方法实现最多30%的假拒绝率降低。
</details></li>
</ul>
<hr>
<h2 id="DualVC-2-Dynamic-Masked-Convolution-for-Unified-Streaming-and-Non-Streaming-Voice-Conversion"><a href="#DualVC-2-Dynamic-Masked-Convolution-for-Unified-Streaming-and-Non-Streaming-Voice-Conversion" class="headerlink" title="DualVC 2: Dynamic Masked Convolution for Unified Streaming and Non-Streaming Voice Conversion"></a>DualVC 2: Dynamic Masked Convolution for Unified Streaming and Non-Streaming Voice Conversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15496">http://arxiv.org/abs/2309.15496</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziqian Ning, Yuepeng Jiang, Pengcheng Zhu, Shuai Wang, Jixun Yao, Lei Xie, Mengxiao Bi</li>
<li>for: 提高语音识别模型的流处理能力和预测速度</li>
<li>methods: 使用Conformer架构、非 causal convolution、动态 chunk mask 和干扰注意力等技术</li>
<li>results: 比对 DualVC 和基eline 系统，DualVC 2 在对话metric和对话metric中表现出色，并且具有低延迟（186.4 ms）<details>
<summary>Abstract</summary>
Voice conversion is becoming increasingly popular, and a growing number of application scenarios require models with streaming inference capabilities. The recently proposed DualVC attempts to achieve this objective through streaming model architecture design and intra-model knowledge distillation along with hybrid predictive coding to compensate for the lack of future information. However, DualVC encounters several problems that limit its performance. First, the autoregressive decoder has error accumulation in its nature and limits the inference speed as well. Second, the causal convolution enables streaming capability but cannot sufficiently use future information within chunks. Third, the model is unable to effectively address the noise in the unvoiced segments, lowering the sound quality. In this paper, we propose DualVC 2 to address these issues. Specifically, the model backbone is migrated to a Conformer-based architecture, empowering parallel inference. Causal convolution is replaced by non-causal convolution with dynamic chunk mask to make better use of within-chunk future information. Also, quiet attention is introduced to enhance the model's noise robustness. Experiments show that DualVC 2 outperforms DualVC and other baseline systems in both subjective and objective metrics, with only 186.4 ms latency. Our audio samples are made publicly available.
</details>
<details>
<summary>摘要</summary>
声音转换正在不断受欢迎，而更多的应用场景需要流处理推理能力。最近提出的双VC模型尝试通过流处理模型架构设计和内部知识储存加以杜绝预测编码来实现这一目标。然而，双VC模型存在一些限制其性能的问题。首先， autoregressive 解码器具有自然的错误积累特性，限制推理速度。其次， causal 卷积可以实现流处理能力，但是无法充分利用内存中的未来信息。最后，模型无法有效地处理无声段的噪声，下降声音质量。在这篇论文中，我们提出了双VC 2模型来解决这些问题。具体来说，模型核心被迁移到基于 Conformer 架构的architecture，实现并行推理。 causal 卷积被替换为非 causal 卷积，并使用动态 chunk mask 来更好地利用内存中的未来信息。此外，我们还引入了静态注意力来提高模型的噪声耐性。实验结果显示，双VC 2 模型在主观和客观指标中都超过了双VC 和其他基eline系统，具有只有 186.4 毫秒延迟。我们的声音样本被公开发布。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/27/cs.SD_2023_09_27/" data-id="clp89dok20100i788emir8zmp" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_09_27" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/27/cs.CV_2023_09_27/" class="article-date">
  <time datetime="2023-09-27T13:00:00.000Z" itemprop="datePublished">2023-09-27</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/27/cs.CV_2023_09_27/">cs.CV - 2023-09-27</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Diagnosis-of-Helicobacter-pylori-using-AutoEncoders-for-the-Detection-of-Anomalous-Staining-Patterns-in-Immunohistochemistry-Images"><a href="#Diagnosis-of-Helicobacter-pylori-using-AutoEncoders-for-the-Detection-of-Anomalous-Staining-Patterns-in-Immunohistochemistry-Images" class="headerlink" title="Diagnosis of Helicobacter pylori using AutoEncoders for the Detection of Anomalous Staining Patterns in Immunohistochemistry Images"></a>Diagnosis of Helicobacter pylori using AutoEncoders for the Detection of Anomalous Staining Patterns in Immunohistochemistry Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16053">http://arxiv.org/abs/2309.16053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pau Cano, Álvaro Caravaca, Debora Gil, Eva Musulen</li>
<li>for: 检测人类胃癌病毒Helicobacter pylori</li>
<li>methods: 使用自适应神经网络模型（autoencoder），从健康组织图像中学习异常特征，检测H. pylori</li>
<li>results: 模型精度91%，敏感性86%，特异性96%，AUC0.97，能够高效地检测H. pylori<details>
<summary>Abstract</summary>
This work addresses the detection of Helicobacter pylori a bacterium classified since 1994 as class 1 carcinogen to humans. By its highest specificity and sensitivity, the preferred diagnosis technique is the analysis of histological images with immunohistochemical staining, a process in which certain stained antibodies bind to antigens of the biological element of interest. This analysis is a time demanding task, which is currently done by an expert pathologist that visually inspects the digitized samples.   We propose to use autoencoders to learn latent patterns of healthy tissue and detect H. pylori as an anomaly in image staining. Unlike existing classification approaches, an autoencoder is able to learn patterns in an unsupervised manner (without the need of image annotations) with high performance. In particular, our model has an overall 91% of accuracy with 86\% sensitivity, 96% specificity and 0.97 AUC in the detection of H. pylori.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Handbook-on-Leveraging-Lines-for-Two-View-Relative-Pose-Estimation"><a href="#Handbook-on-Leveraging-Lines-for-Two-View-Relative-Pose-Estimation" class="headerlink" title="Handbook on Leveraging Lines for Two-View Relative Pose Estimation"></a>Handbook on Leveraging Lines for Two-View Relative Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16040">http://arxiv.org/abs/2309.16040</a></li>
<li>repo_url: None</li>
<li>paper_authors: Petr Hruby, Shaohui Liu, Rémi Pautrat, Marc Pollefeys, Daniel Barath</li>
<li>for: 本文旨在提出一种可以结合点、线和它们的重合的方法来估算准确的图像对比pose的方法。</li>
<li>methods: 本文使用了融合点、线和重合的方法，并评估了现有文献中的最小解算法。</li>
<li>results: 实验表明， compared to点基本方法，本文的方法在各种indoor和outdoor数据集上提高了AUC@10$^\circ$的值，提高了1-7个点，并且运行速度相对较快。Here’s the English version of the information:</li>
<li>for: The paper proposes a method for estimating the relative pose between calibrated image pairs by jointly exploiting points, lines, and their coincidences in a hybrid manner.</li>
<li>methods: The method combines the advantages of all possible configurations where these data modalities can be used together, and reviews the minimal solvers available in the literature.</li>
<li>results: Experiments on various indoor and outdoor datasets show that the proposed approach outperforms point-based methods, improving AUC@10$^\circ$ by 1-7 points while running at comparable speeds.<details>
<summary>Abstract</summary>
We propose an approach for estimating the relative pose between calibrated image pairs by jointly exploiting points, lines, and their coincidences in a hybrid manner. We investigate all possible configurations where these data modalities can be used together and review the minimal solvers available in the literature. Our hybrid framework combines the advantages of all configurations, enabling robust and accurate estimation in challenging environments. In addition, we design a method for jointly estimating multiple vanishing point correspondences in two images, and a bundle adjustment that considers all relevant data modalities. Experiments on various indoor and outdoor datasets show that our approach outperforms point-based methods, improving AUC@10$^\circ$ by 1-7 points while running at comparable speeds. The source code of the solvers and hybrid framework will be made public.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法，用于估算投影图像对的相对pose，通过同时利用点、线和它们的重合来实现。我们审查了所有可能的数据模式，并评估了文献中可用的最小解。我们的混合框架结合了所有配置的优点，可以在具有挑战性的环境中提供稳定和准确的估算。此外，我们还设计了用于在两个图像中同时估算多个消失点匹配的方法，以及考虑所有相关数据模式的缓冲调整。在各种室内和室外数据集上进行了实验，我们的方法与点基本方法相比，提高了AUC@10$^\circ$的值，从1-7个点中增加了1-7个点，并且在相同的速度下运行。我们计划将解决方案和混合框架的源代码公开。
</details></li>
</ul>
<hr>
<h2 id="Q-REG-End-to-End-Trainable-Point-Cloud-Registration-with-Surface-Curvature"><a href="#Q-REG-End-to-End-Trainable-Point-Cloud-Registration-with-Surface-Curvature" class="headerlink" title="Q-REG: End-to-End Trainable Point Cloud Registration with Surface Curvature"></a>Q-REG: End-to-End Trainable Point Cloud Registration with Surface Curvature</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16023">http://arxiv.org/abs/2309.16023</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengze Jin, Daniel Barath, Marc Pollefeys, Iro Armeni</li>
<li>for: 这种论文主要用于提出一种新的点云注册方法，以便更好地进行点云注册问题的解决。</li>
<li>methods: 这种方法使用了学习基于方法，包括对匹配的优化，以及使用RANSAC-like框架进行评估。</li>
<li>results: 这种方法可以提供更加稳定和有效的点云注册结果，并且可以在实时应用中使用。它在3DMatch、KITTI和ModelNet测试数据集上达到了新的状态平衡。<details>
<summary>Abstract</summary>
Point cloud registration has seen recent success with several learning-based methods that focus on correspondence matching and, as such, optimize only for this objective. Following the learning step of correspondence matching, they evaluate the estimated rigid transformation with a RANSAC-like framework. While it is an indispensable component of these methods, it prevents a fully end-to-end training, leaving the objective to minimize the pose error nonserved. We present a novel solution, Q-REG, which utilizes rich geometric information to estimate the rigid pose from a single correspondence. Q-REG allows to formalize the robust estimation as an exhaustive search, hence enabling end-to-end training that optimizes over both objectives of correspondence matching and rigid pose estimation. We demonstrate in the experiments that Q-REG is agnostic to the correspondence matching method and provides consistent improvement both when used only in inference and in end-to-end training. It sets a new state-of-the-art on the 3DMatch, KITTI, and ModelNet benchmarks.
</details>
<details>
<summary>摘要</summary>
“对点云注册进行了最近的成功，使用了一些学习基于方法，强调对应匹配和优化这个目标。在学习步骤中，它们使用RANSAC类框架进行评估估算的稳定性，但是这会阻碍完整的端到端训练，使得最小化pose错误的目标未被服务。我们提出了一种新的解决方案，即Q-REG，它利用了丰富的几何信息来估算点云的稳定性。Q-REG允许我们对稳定性进行排序的极限搜索，因此可以进行端到端训练，并且可以同时优化对应匹配和稳定性估算的两个目标。我们在实验中证明了Q-REG是不同对应匹配方法的agnostic，并在推理和端到端训练中提供了一致的改进。它在3DMatch、KITTI和ModelNet标准测试 benchmark上设置了新的状态。”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="GeoCLIP-Clip-Inspired-Alignment-between-Locations-and-Images-for-Effective-Worldwide-Geo-localization"><a href="#GeoCLIP-Clip-Inspired-Alignment-between-Locations-and-Images-for-Effective-Worldwide-Geo-localization" class="headerlink" title="GeoCLIP: Clip-Inspired Alignment between Locations and Images for Effective Worldwide Geo-localization"></a>GeoCLIP: Clip-Inspired Alignment between Locations and Images for Effective Worldwide Geo-localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16020">http://arxiv.org/abs/2309.16020</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vicente Vivanco Cepeda, Gaurav Kumar Nayak, Mubarak Shah</li>
<li>for: 准确地定位全球任意位置的图像</li>
<li>methods: 提出了一种基于CLIP的图像-GPS匹配方法，使用位置编码和幂等分辨率表示来模型地球，并通过对图像和GPS位置进行对齐来实现地图地标注。</li>
<li>results: 通过广泛的实验和简要的ablation，证明了该方法的有效性，只需使用20%的训练数据就能达到竞争力水平，并且通过文本查询示例展示了图像地理标注的可行性。<details>
<summary>Abstract</summary>
Worldwide Geo-localization aims to pinpoint the precise location of images taken anywhere on Earth. This task has considerable challenges due to immense variation in geographic landscapes. The image-to-image retrieval-based approaches fail to solve this problem on a global scale as it is not feasible to construct a large gallery of images covering the entire world. Instead, existing approaches divide the globe into discrete geographic cells, transforming the problem into a classification task. However, their performance is limited by the predefined classes and often results in inaccurate localizations when an image's location significantly deviates from its class center. To overcome these limitations, we propose GeoCLIP, a novel CLIP-inspired Image-to-GPS retrieval approach that enforces alignment between the image and its corresponding GPS locations. GeoCLIP's location encoder models the Earth as a continuous function by employing positional encoding through random Fourier features and constructing a hierarchical representation that captures information at varying resolutions to yield a semantically rich high-dimensional feature suitable to use even beyond geo-localization. To the best of our knowledge, this is the first work employing GPS encoding for geo-localization. We demonstrate the efficacy of our method via extensive experiments and ablations on benchmark datasets. We achieve competitive performance with just 20% of training data, highlighting its effectiveness even in limited-data settings. Furthermore, we qualitatively demonstrate geo-localization using a text query by leveraging CLIP backbone of our image encoder.
</details>
<details>
<summary>摘要</summary>
全球地理位置 pinpoint 任何地点的精准位置是全球地理位置定位的挑战。由于地理景观的巨大差异，图像到图像检索方法无法在全球范围内解决这个问题。现有的方法将地球分成精确的地理维度单元，将问题转化为一个分类任务，但其性能受限于预先定义的类别，常导致图像的位置偏差从类别中心偏离。为了超越这些限制，我们提出了 GeoCLIP，一种基于 CLIP 的图像到 GPS  Retrieval 方法。GeoCLIP 的位置编码器使用随机傅里埃特性编码 Earth 为一个连续函数，并使用层次表示，以捕捉图像与 GPS 位置之间的对应关系。这使得 GeoCLIP 可以在有限数据量下达到竞争性性能。我们通过广泛的实验和剔除研究证明 GeoCLIP 的效果。此外，我们通过 CLIP 的背景网络，用文本查询来实现地理位置定位。
</details></li>
</ul>
<hr>
<h2 id="Assessment-of-Local-Climate-Zone-Products-via-Simplified-Classification-Rule-with-3D-Building-Maps"><a href="#Assessment-of-Local-Climate-Zone-Products-via-Simplified-Classification-Rule-with-3D-Building-Maps" class="headerlink" title="Assessment of Local Climate Zone Products via Simplified Classification Rule with 3D Building Maps"></a>Assessment of Local Climate Zone Products via Simplified Classification Rule with 3D Building Maps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15978">http://arxiv.org/abs/2309.15978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hunsoo Song, Gaia Cervini, Jinha Jung</li>
<li>for: 本研究evaluates the performance of a global Local Climate Zone (LCZ) product.</li>
<li>methods: 研究使用了一种简单的规则生成法 constructed a reference LCZ using high-resolution 3D building maps.</li>
<li>results: 研究发现，全球LCZ产品很难 differentiate classes that demand precise building footprint information (Classes 6 and 9), and classes that necessitate the identification of subtle differences in building elevation (Classes 4-6). Additionally, 研究发现了不一致的趋势，城市间LCZ分布不同， suggesting the presence of a data distribution shift problem in the machine learning-based LCZ classifier.<details>
<summary>Abstract</summary>
This study assesses the performance of a global Local Climate Zone (LCZ) product. We examined the built-type classes of LCZs in three major metropolitan areas within the U.S. A reference LCZ was constructed using a simple rule-based method based on high-resolution 3D building maps. Our evaluation demonstrated that the global LCZ product struggles to differentiate classes that demand precise building footprint information (Classes 6 and 9), and classes that necessitate the identification of subtle differences in building elevation (Classes 4-6). Additionally, we identified inconsistent tendencies, where the distribution of classes skews differently across different cities, suggesting the presence of a data distribution shift problem in the machine learning-based LCZ classifier. Our findings shed light on the uncertainties in global LCZ maps, help identify the LCZ classes that are the most challenging to distinguish, and offer insight into future plans for LCZ development and validation.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Local Climate Zone" (LCZ) is translated as "地方气候区" (dìfāng kīhào qū)* "built-type classes" is translated as "建筑类别" (jiànzhù làibie)* "high-resolution 3D building maps" is translated as "高分辨率3D建筑地图" (gāo fēnbianhé lǐ 3D jiànzhù dìtú)* "machine learning-based LCZ classifier" is translated as "基于机器学习的LCZ分类器" (jīyù jīshì xuéxí de LCZ fēngròngqì)* "data distribution shift problem" is translated as "数据分布偏移问题" (shùzhì fāngbù pénduì wèn tí)
</details></li>
</ul>
<hr>
<h2 id="Neural-Acoustic-Context-Field-Rendering-Realistic-Room-Impulse-Response-With-Neural-Fields"><a href="#Neural-Acoustic-Context-Field-Rendering-Realistic-Room-Impulse-Response-With-Neural-Fields" class="headerlink" title="Neural Acoustic Context Field: Rendering Realistic Room Impulse Response With Neural Fields"></a>Neural Acoustic Context Field: Rendering Realistic Room Impulse Response With Neural Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15977">http://arxiv.org/abs/2309.15977</a></li>
<li>repo_url: None</li>
<li>paper_authors: Susan Liang, Chao Huang, Yapeng Tian, Anurag Kumar, Chenliang Xu</li>
<li>for: 这个论文的目的是提出一种基于神经网络场函数的听音场景参数化方法，以提高听音场景的准确性。</li>
<li>methods: 这个方法使用多个听音上下文，如干擦性、形态特征和空间信息，来Parameterize听音场景。它还使用时间相关模块和多尺度能量衰减标准来适应RIR的独特性。</li>
<li>results: 实验结果显示，NACF方法在比较 existed 场景下表现出了明显的优异，超过了现有的场景基于场函数方法。<details>
<summary>Abstract</summary>
Room impulse response (RIR), which measures the sound propagation within an environment, is critical for synthesizing high-fidelity audio for a given environment. Some prior work has proposed representing RIR as a neural field function of the sound emitter and receiver positions. However, these methods do not sufficiently consider the acoustic properties of an audio scene, leading to unsatisfactory performance. This letter proposes a novel Neural Acoustic Context Field approach, called NACF, to parameterize an audio scene by leveraging multiple acoustic contexts, such as geometry, material property, and spatial information. Driven by the unique properties of RIR, i.e., temporal un-smoothness and monotonic energy attenuation, we design a temporal correlation module and multi-scale energy decay criterion. Experimental results show that NACF outperforms existing field-based methods by a notable margin. Please visit our project page for more qualitative results.
</details>
<details>
<summary>摘要</summary>
<<SYS>使用 neural field 函数来表示室内声学环境的室内响应（RIR）已经有一些前期工作，但这些方法并不充分考虑了声音场景的音学性质，导致效果不够满意。这封信提议一种新的声学上下文场景方法（NACF），利用多种声学上下文，如几何、物理性和空间信息，来参数化声音场景。驱动了RIR的特有性，如时间不整合和单调能量减衰，我们设计了时间相关模块和多scale能量减衰标准。实验结果表明，NACF在场景基于方法中表现出优于其他方法。更多资讯请访问我们项目页面。</SYS>Here's a breakdown of the translation:* "室内响应" (RIR) is translated as "室内响应" (also RIR).* " neural field function" is translated as "声学上下文场景方法" (NACF).* "acoustic properties" is translated as "音学性质" (音学性质).* "audio scene" is translated as "声音场景" (声音场景).* "temporal un-smoothness" is translated as "时间不整合" (时间不整合).* "monotonic energy attenuation" is translated as "单调能量减衰" (单调能量减衰).* "field-based methods" is translated as "场景基于方法" (场景基于方法).Please note that the translation is done using Simplified Chinese, and some words or phrases may have different translations in Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="The-Devil-is-in-the-Details-A-Deep-Dive-into-the-Rabbit-Hole-of-Data-Filtering"><a href="#The-Devil-is-in-the-Details-A-Deep-Dive-into-the-Rabbit-Hole-of-Data-Filtering" class="headerlink" title="The Devil is in the Details: A Deep Dive into the Rabbit Hole of Data Filtering"></a>The Devil is in the Details: A Deep Dive into the Rabbit Hole of Data Filtering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15954">http://arxiv.org/abs/2309.15954</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haichao Yu, Yu Tian, Sateesh Kumar, Linjie Yang, Heng Wang</li>
<li>for: 本研究旨在评估不同数据筛选方法的性能，以提高基础模型的表现。</li>
<li>methods: 本研究使用了三个阶段的筛选策略：单模态筛选、交叉模态筛选和数据分布对接。我们还提出了新的解决方案，如计算 CLIP 分数在水平翻转图像上以减少场景文本的干扰，使用视觉和语言模型来检索下游任务的训练样本，重新平衡数据分布以改善计算资源的分配效率等。</li>
<li>results: 我们的方法比 DataComp 论文中最佳方法平均表现提高了4%， ImageNet 上表现提高了2%。<details>
<summary>Abstract</summary>
The quality of pre-training data plays a critical role in the performance of foundation models. Popular foundation models often design their own recipe for data filtering, which makes it hard to analyze and compare different data filtering approaches. DataComp is a new benchmark dedicated to evaluating different methods for data filtering. This paper describes our learning and solution when participating in the DataComp challenge. Our filtering strategy includes three stages: single-modality filtering, cross-modality filtering, and data distribution alignment. We integrate existing methods and propose new solutions, such as computing CLIP score on horizontally flipped images to mitigate the interference of scene text, using vision and language models to retrieve training samples for target downstream tasks, rebalancing the data distribution to improve the efficiency of allocating the computational budget, etc. We slice and dice our design choices, provide in-depth analysis, and discuss open questions. Our approach outperforms the best method from the DataComp paper by over 4% on the average performance of 38 tasks and by over 2% on ImageNet.
</details>
<details>
<summary>摘要</summary>
“数据预训模型的质量具有关键作用，但是popular基础模型经常设计自己的数据筛选方法，这使得分析和比较不同数据筛选方法的困难。为了解决这个问题，DataComp是一个新的竞赛benchmark，用于评估不同数据筛选方法。这篇论文描述了我们在DataComp挑战中学习和解决的经验。我们的筛选策略包括三个阶段：单模态筛选、交叉模态筛选和数据分布对齐。我们将现有方法与新的解决方案相结合，例如在横向翻转图像上计算CLIP分数以避免场景文本的干扰，使用视觉和语言模型来收集下游任务的训练样本，重新规划数据分布以提高计算预算的效率等。我们将slice和dice我们的设计选择，进行深入分析，并讨论开放问题。我们的方法在38个任务的平均性能上超过DataComp文章中最佳方法的4%，并在ImageNet上超过2%。”
</details></li>
</ul>
<hr>
<h2 id="AutoEncoding-Tree-for-City-Generation-and-Applications"><a href="#AutoEncoding-Tree-for-City-Generation-and-Applications" class="headerlink" title="AutoEncoding Tree for City Generation and Applications"></a>AutoEncoding Tree for City Generation and Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15941">http://arxiv.org/abs/2309.15941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenyu Han, Congcong Wen, Lazarus Chok, Yan Liang Tan, Sheung Lung Chan, Hang Zhao, Chen Feng</li>
<li>for: 这paper的目的是为了提出一种基于树状自编码器的城市生成模型，以解决城市数据的巨量和缺乏公共数据的问题。</li>
<li>methods: 该paper使用了一种新的空间几何距离(SGD)度量来衡量建筑布局的相似性，然后将其转化为一棵树状网络，其中encoder部分会逐级提取和合并空间信息。</li>
<li>results: 实验结果表明，提出的AETree模型可以有效地进行2D和3D城市生成，同时学习的缓存特征可以用于下游城市规划应用。<details>
<summary>Abstract</summary>
City modeling and generation have attracted an increased interest in various applications, including gaming, urban planning, and autonomous driving. Unlike previous works focused on the generation of single objects or indoor scenes, the huge volumes of spatial data in cities pose a challenge to the generative models. Furthermore, few publicly available 3D real-world city datasets also hinder the development of methods for city generation. In this paper, we first collect over 3,000,000 geo-referenced objects for the city of New York, Zurich, Tokyo, Berlin, Boston and several other large cities. Based on this dataset, we propose AETree, a tree-structured auto-encoder neural network, for city generation. Specifically, we first propose a novel Spatial-Geometric Distance (SGD) metric to measure the similarity between building layouts and then construct a binary tree over the raw geometric data of building based on the SGD metric. Next, we present a tree-structured network whose encoder learns to extract and merge spatial information from bottom-up iteratively. The resulting global representation is reversely decoded for reconstruction or generation. To address the issue of long-dependency as the level of the tree increases, a Long Short-Term Memory (LSTM) Cell is employed as a basic network element of the proposed AETree. Moreover, we introduce a novel metric, Overlapping Area Ratio (OAR), to quantitatively evaluate the generation results. Experiments on the collected dataset demonstrate the effectiveness of the proposed model on 2D and 3D city generation. Furthermore, the latent features learned by AETree can serve downstream urban planning applications.
</details>
<details>
<summary>摘要</summary>
城市模型化和生成在各种应用中受到了越来越多的关注，包括游戏、城市规划和自动驾驶。与前一些关注单个 объек或室内场景生成的研究不同，城市的巨量数据带来了生成模型的挑战。此外，有限公共可用的3D实际城市数据也限制了城市生成方法的发展。在这篇论文中，我们首先收集了纽约、苏黎世、东京、柏林和波士顿等城市的3,000,000个地理引用对象。基于这些数据，我们提议了AETree，一种树状自动编码网络，用于城市生成。具体来说，我们首先提出了一种新的空间几何距离（SGD）度量，用于衡量建筑布局之间的相似性。然后，我们将建筑的原始几何数据拼接成一棵二叉树，基于SGD度量。接下来，我们介绍了一个树状网络，其编码器可以从底向上iteratively提取和合并空间信息。结果的全局表示可以 reversely 解码为重建或生成。为了解决生成结果中层级增长的长期依赖问题，我们采用了一个长期记忆（LSTM）细胞作为AETree的基本网络元素。此外，我们还提出了一个新的度量， overlap 区域比率（OAR），用于评估生成结果的质量。实验表明，提议的模型在2D和3D城市生成中表现效果。此外，AETree所学习的潜在特征可以服务于下游城市规划应用。
</details></li>
</ul>
<hr>
<h2 id="Context-Aware-Entity-Grounding-with-Open-Vocabulary-3D-Scene-Graphs"><a href="#Context-Aware-Entity-Grounding-with-Open-Vocabulary-3D-Scene-Graphs" class="headerlink" title="Context-Aware Entity Grounding with Open-Vocabulary 3D Scene Graphs"></a>Context-Aware Entity Grounding with Open-Vocabulary 3D Scene Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15940">http://arxiv.org/abs/2309.15940</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/changhaonan/ovsg">https://github.com/changhaonan/ovsg</a></li>
<li>paper_authors: Haonan Chang, Kowndinya Boyalakuntla, Shiyang Lu, Siwei Cai, Eric Jing, Shreesh Keskar, Shijie Geng, Adeeb Abbas, Lifeng Zhou, Kostas Bekris, Abdeslam Boularias</li>
<li>for: 用于提供一个开放词汇3D场景图（OVSG），用于对各种实体（例如物体实例、代理人和区域）进行识别，并支持自由文本查询。</li>
<li>methods: 使用自由文本查询，而不是传统的semantic-based对象定位方法，以提供上下文意识感知定位。</li>
<li>results: 在使用ScanNet数据集和自我收集的数据集进行比较实验中，我们的提议方法在对前期 semantic-based定位技术的比较中显著超越了性能。此外，我们还探讨了OVSG在实际 робоNavigation和操作实验中的实际应用。<details>
<summary>Abstract</summary>
We present an Open-Vocabulary 3D Scene Graph (OVSG), a formal framework for grounding a variety of entities, such as object instances, agents, and regions, with free-form text-based queries. Unlike conventional semantic-based object localization approaches, our system facilitates context-aware entity localization, allowing for queries such as ``pick up a cup on a kitchen table" or ``navigate to a sofa on which someone is sitting". In contrast to existing research on 3D scene graphs, OVSG supports free-form text input and open-vocabulary querying. Through a series of comparative experiments using the ScanNet dataset and a self-collected dataset, we demonstrate that our proposed approach significantly surpasses the performance of previous semantic-based localization techniques. Moreover, we highlight the practical application of OVSG in real-world robot navigation and manipulation experiments.
</details>
<details>
<summary>摘要</summary>
我们提出了一个开放词汇3D场景图（OVSG），这是一种正式框架，用于将多种实体，如物品实例、代理人和区域，与自由形式文本查询相关联。与传统的意义基于对象定位方法不同，我们的系统支持上下文意识实体定位，allowing for queries such as "pick up a cup on a kitchen table" or "navigate to a sofa on which someone is sitting". 与现有的3D场景图研究不同，OVSG支持自由形式文本输入和开放词汇查询。通过对ScanNet数据集和自我收集的数据集进行比较实验，我们示出了我们提议的方法在前一个 semantic-based定位技术的性能方面明显超越。此外，我们还 highlighted the practical application of OVSG in real-world robot navigation and manipulation experiments。
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-and-Few-Shot-Video-Question-Answering-with-Multi-Modal-Prompts"><a href="#Zero-Shot-and-Few-Shot-Video-Question-Answering-with-Multi-Modal-Prompts" class="headerlink" title="Zero-Shot and Few-Shot Video Question Answering with Multi-Modal Prompts"></a>Zero-Shot and Few-Shot Video Question Answering with Multi-Modal Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15915">http://arxiv.org/abs/2309.15915</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/engindeniz/vitis">https://github.com/engindeniz/vitis</a></li>
<li>paper_authors: Deniz Engin, Yannis Avrithis</li>
<li>for: 这个论文的目的是解决大规模预训练模型在有限数据上适应问题中的挑战，包括过拟合、跨Modal汇总和语义识别等问题。</li>
<li>methods: 该论文提出了一种效率的参数方法， combinig 多modal prompt学习和基于transformer的映射网络，以适应预训练模型的冰封。</li>
<li>results: 我们在多个视频问答 benchmark上进行了实验，并证明了我们的方法在零shot和几shot设置下具有优秀的性能和参数效率。我们的代码可以在 <a target="_blank" rel="noopener" href="https://engindeniz.github.io/vitis">https://engindeniz.github.io/vitis</a> 上获取。<details>
<summary>Abstract</summary>
Recent vision-language models are driven by large-scale pretrained models. However, adapting pretrained models on limited data presents challenges such as overfitting, catastrophic forgetting, and the cross-modal gap between vision and language. We introduce a parameter-efficient method to address these challenges, combining multimodal prompt learning and a transformer-based mapping network, while keeping the pretrained models frozen. Our experiments on several video question answering benchmarks demonstrate the superiority of our approach in terms of performance and parameter efficiency on both zero-shot and few-shot settings. Our code is available at https://engindeniz.github.io/vitis.
</details>
<details>
<summary>摘要</summary>
现代视力语言模型受大规模预训练模型的驱动。然而，在有限数据上适应预训练模型存在困难，如预测溢出、跨模态差距和语言视觉 gap。我们提出一种 parameter-efficient 方法，结合多模态提示学习和基于 transformer 的映射网络，保持预训练模型冻结。我们的实验表明，我们的方法在多个视频问答 benchmark 上具有表现和参数效率的优势，包括零shot 和几shot 设置。我们的代码可以在 <https://engindeniz.github.io/vitis> 上找到。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-the-Signal-Leak-Bias-in-Diffusion-Models"><a href="#Exploiting-the-Signal-Leak-Bias-in-Diffusion-Models" class="headerlink" title="Exploiting the Signal-Leak Bias in Diffusion Models"></a>Exploiting the Signal-Leak Bias in Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15842">http://arxiv.org/abs/2309.15842</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Nicolas Everaert, Athanasios Fitsios, Marco Bocchio, Sami Arpa, Sabine Süsstrunk, Radhakrishna Achanta</li>
<li>for: 本研究旨在探讨 diffusion 模型中存在的偏见问题，并提出一种方法来控制生成的图像。</li>
<li>methods: 研究者使用了现有的 diffusion 模型，并在其中引入了一种信号泄漏来控制生成的图像。</li>
<li>results: 研究者通过模型 signal-leak 的分布在空间频谱和像素域来控制生成的图像，并可以通过不需要进一步训练来生成符合预期结果的图像。<details>
<summary>Abstract</summary>
There is a bias in the inference pipeline of most diffusion models. This bias arises from a signal leak whose distribution deviates from the noise distribution, creating a discrepancy between training and inference processes. We demonstrate that this signal-leak bias is particularly significant when models are tuned to a specific style, causing sub-optimal style matching. Recent research tries to avoid the signal leakage during training. We instead show how we can exploit this signal-leak bias in existing diffusion models to allow more control over the generated images. This enables us to generate images with more varied brightness, and images that better match a desired style or color. By modeling the distribution of the signal leak in the spatial frequency and pixel domains, and including a signal leak in the initial latent, we generate images that better match expected results without any additional training.
</details>
<details>
<summary>摘要</summary>
多种扩散模型中的推理管道存在偏见。这种偏见来自信号泄漏，其分布与噪声分布不同，导致训练和推理过程之间的差异。我们示示了这种信号泄漏偏见在特定风格下训练模型时特别 significannot。 current research aims to avoid signal leakage during training. 我们则示了如何在现有的扩散模型中利用这种信号泄漏偏见，以获得更多的控制权 над生成图像。通过在空间频率和像素域中模型信号泄漏分布，并在初始干扰中包含信号泄漏，我们生成了更好地匹配预期结果的图像。这些图像不需要任何额外训练。
</details></li>
</ul>
<hr>
<h2 id="Show-1-Marrying-Pixel-and-Latent-Diffusion-Models-for-Text-to-Video-Generation"><a href="#Show-1-Marrying-Pixel-and-Latent-Diffusion-Models-for-Text-to-Video-Generation" class="headerlink" title="Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation"></a>Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15818">http://arxiv.org/abs/2309.15818</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/showlab/show-1">https://github.com/showlab/show-1</a></li>
<li>paper_authors: David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, Mike Zheng Shou</li>
<li>for: 本研究旨在提出一种混合型文本到视频生成模型（Show-1），结合像素基于的VDM和秘密基于的VDM进行文本到视频生成。</li>
<li>methods: 我们的模型首先使用像素基于的VDM生成一个低分辨率的视频，然后提出了一种新的专家翻译方法，使用秘密基于的VDM进行更高的视频 upsample。</li>
<li>results: 与秘密VDM相比，Show-1可以生成高质量的视频，具有精确的文本-视频对应性；与像素VDM相比，Show-1具有许多更高效的特点（GPU内存使用率 durante la inferencia es de 15G vs 72G）。我们还 validate了我们的模型在标准视频生成 bencmarks 上。<details>
<summary>Abstract</summary>
Significant advancements have been achieved in the realm of large-scale pre-trained text-to-video Diffusion Models (VDMs). However, previous methods either rely solely on pixel-based VDMs, which come with high computational costs, or on latent-based VDMs, which often struggle with precise text-video alignment. In this paper, we are the first to propose a hybrid model, dubbed as Show-1, which marries pixel-based and latent-based VDMs for text-to-video generation. Our model first uses pixel-based VDMs to produce a low-resolution video of strong text-video correlation. After that, we propose a novel expert translation method that employs the latent-based VDMs to further upsample the low-resolution video to high resolution. Compared to latent VDMs, Show-1 can produce high-quality videos of precise text-video alignment; Compared to pixel VDMs, Show-1 is much more efficient (GPU memory usage during inference is 15G vs 72G). We also validate our model on standard video generation benchmarks. Our code and model weights are publicly available at https://github.com/showlab/Show-1.
</details>
<details>
<summary>摘要</summary>
大量的进步已经在文本到视频扩散模型（VDM）领域取得了成果。然而，之前的方法都是靠坐标基于的VDM或者是基于隐藏变量的VDM，这两者都有缺点。在这篇论文中，我们是第一个提出了拥有坐标基于和隐藏变量基于VDM的混合模型，我们称之为Show-1。我们的模型首先使用坐标基于VDM生成一个低分辨率的视频，并且通过我们提出的一种新的专家翻译方法，使用隐藏变量基于VDM来进一步提高低分辨率视频的分辨率。相比于隐藏VDM，Show-1可以生成高质量的文本视频匹配；相比于坐标VDM，Show-1 Much more efficient（GPU内存使用率 durante la inferencia es de 15G vs 72G)。我们还验证了我们的模型在标准视频生成 benchmarks 上的性能。我们的代码和模型权重可以在https://github.com/showlab/Show-1 上获取。
</details></li>
</ul>
<hr>
<h2 id="Convolutional-Networks-with-Oriented-1D-Kernels"><a href="#Convolutional-Networks-with-Oriented-1D-Kernels" class="headerlink" title="Convolutional Networks with Oriented 1D Kernels"></a>Convolutional Networks with Oriented 1D Kernels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15812">http://arxiv.org/abs/2309.15812</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/princeton-vl/oriented1d">https://github.com/princeton-vl/oriented1d</a></li>
<li>paper_authors: Alexandre Kirchmeyer, Jia Deng</li>
<li>for: 这个论文的目的是探讨ConvNet是否可以没有2D卷积。</li>
<li>methods: 这个论文使用了1D卷积，并且发现了一种叫做方向1D卷积的技术，可以将2D卷积完全替代。</li>
<li>results: 这个论文的实验结果表明，使用方向1D卷积可以达到与2D卷积相同的准确率，而且可以降低计算量。<details>
<summary>Abstract</summary>
In computer vision, 2D convolution is arguably the most important operation performed by a ConvNet. Unsurprisingly, it has been the focus of intense software and hardware optimization and enjoys highly efficient implementations. In this work, we ask an intriguing question: can we make a ConvNet work without 2D convolutions? Surprisingly, we find that the answer is yes -- we show that a ConvNet consisting entirely of 1D convolutions can do just as well as 2D on ImageNet classification. Specifically, we find that one key ingredient to a high-performing 1D ConvNet is oriented 1D kernels: 1D kernels that are oriented not just horizontally or vertically, but also at other angles. Our experiments show that oriented 1D convolutions can not only replace 2D convolutions but also augment existing architectures with large kernels, leading to improved accuracy with minimal FLOPs increase. A key contribution of this work is a highly-optimized custom CUDA implementation of oriented 1D kernels, specialized to the depthwise convolution setting. Our benchmarks demonstrate that our custom CUDA implementation almost perfectly realizes the theoretical advantage of 1D convolution: it is faster than a native horizontal convolution for any arbitrary angle. Code is available at https://github.com/princeton-vl/Oriented1D.
</details>
<details>
<summary>摘要</summary>
在计算机视觉中，2D卷积是无可争议的最重要的操作，它在ConvNet中扮演着关键的角色。不奇怪的是，它已经得到了极高效的软件和硬件优化，并且有高效的实现。在这项工作中，我们提出了一个有趣的问题：可以不使用2D卷积来实现ConvNet吗？奇怪的是，我们发现答案是Yes，我们表明了一个完全由1D卷积组成的ConvNet可以与2D卷积相当地表现，甚至在ImageNet分类任务上达到相同的准确率。具体来说，我们发现一个关键的组成部分是方向卷积：卷积不仅可以水平或垂直进行卷积，还可以在其他角度进行卷积。我们的实验表明，方向卷积不仅可以替换2D卷积，还可以补充现有的架构，从而提高准确率，而且减少FLOPs。我们的一个重要贡献是对方向卷积的高度优化的自定义CUDA实现，特制为深度卷积设置。我们的测试表明，我们的自定义CUDA实现几乎完全实现了理论上的优势，它在任意角度下比本地水平卷积更快。代码可以在https://github.com/princeton-vl/Oriented1D上获取。
</details></li>
</ul>
<hr>
<h2 id="Emu-Enhancing-Image-Generation-Models-Using-Photogenic-Needles-in-a-Haystack"><a href="#Emu-Enhancing-Image-Generation-Models-Using-Photogenic-Needles-in-a-Haystack" class="headerlink" title="Emu: Enhancing Image Generation Models Using Photogenic Needles in a Haystack"></a>Emu: Enhancing Image Generation Models Using Photogenic Needles in a Haystack</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15807">http://arxiv.org/abs/2309.15807</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, Matthew Yu, Abhishek Kadian, Filip Radenovic, Dhruv Mahajan, Kunpeng Li, Yue Zhao, Vladan Petrovic, Mitesh Kumar Singh, Simran Motwani, Yi Wen, Yiwen Song, Roshan Sumbaly, Vignesh Ramanathan, Zijian He, Peter Vajda, Devi Parikh</li>
<li>For: 这个论文的目的是提出一种基于Web scale image-text对的文本至图模型训练方法，以生成高质量的视觉概念图像。* Methods: 该论文提出了一种名为“质量调整”的方法，通过精心选择一些高质量且极其视觉吸引人的图像进行监督训练，以使文本至图模型产生更高质量的图像。* Results: 论文的实验结果显示，使用“质量调整”方法可以使文本至图模型产生更高质量的图像，并且比传统的文本至图模型更具有视觉吸引力。<details>
<summary>Abstract</summary>
Training text-to-image models with web scale image-text pairs enables the generation of a wide range of visual concepts from text. However, these pre-trained models often face challenges when it comes to generating highly aesthetic images. This creates the need for aesthetic alignment post pre-training. In this paper, we propose quality-tuning to effectively guide a pre-trained model to exclusively generate highly visually appealing images, while maintaining generality across visual concepts. Our key insight is that supervised fine-tuning with a set of surprisingly small but extremely visually appealing images can significantly improve the generation quality. We pre-train a latent diffusion model on $1.1$ billion image-text pairs and fine-tune it with only a few thousand carefully selected high-quality images. The resulting model, Emu, achieves a win rate of $82.9\%$ compared with its pre-trained only counterpart. Compared to the state-of-the-art SDXLv1.0, Emu is preferred $68.4\%$ and $71.3\%$ of the time on visual appeal on the standard PartiPrompts and our Open User Input benchmark based on the real-world usage of text-to-image models. In addition, we show that quality-tuning is a generic approach that is also effective for other architectures, including pixel diffusion and masked generative transformer models.
</details>
<details>
<summary>摘要</summary>
培训文本到图像模型使得可以生成广泛的视觉概念从文本。然而，这些预训练模型经常在生成高度美观的图像时遇到问题。这创造了美观对齐的需求。在这篇论文中，我们提出了质量调整来有效地引导预训练后的模型仅生成高度视觉吸引人的图像，而保持视觉概念的通用性。我们的关键发现是，在一小群非常美观但极其吸引人的图像上进行监督微调可以很大程度上提高生成质量。我们在110亿个图像-文本对的基础上预训练了一个抽象扩散模型，然后通过只有几千个精选高质量图像进行微调。得到的模型被称为Emu，其赢得了与预训练只的对手的比赛，其中胜率为82.9%。相比之下，与状态艺术SDXLv1.0进行比赛，Emu被选择了68.4%和71.3%的时间在标准的 PartiPrompts 和我们的实际用途基准测试中的视觉吸引力方面。此外，我们还证明了质量调整是一种通用的方法，也是有效的 для其他架构，包括像素扩散和受Mask的生成变换模型。
</details></li>
</ul>
<hr>
<h2 id="A-Quantum-Classical-Hybrid-Block-Matching-Algorithm-in-Noisy-Environment-using-Dissimilarity-Measure"><a href="#A-Quantum-Classical-Hybrid-Block-Matching-Algorithm-in-Noisy-Environment-using-Dissimilarity-Measure" class="headerlink" title="A Quantum-Classical Hybrid Block-Matching Algorithm in Noisy Environment using Dissimilarity Measure"></a>A Quantum-Classical Hybrid Block-Matching Algorithm in Noisy Environment using Dissimilarity Measure</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15792">http://arxiv.org/abs/2309.15792</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. Martínez-Felipe, J. Montiel-Pérez, V. Onofre-González, A. Maldonado-Romo, Ricky Young</li>
<li>for: 这个论文是为了解决图像块匹配问题，即在搜索区域内找到一组相似图像块。</li>
<li>methods: 这个论文使用了类比图像处理技术，包括 Gaussian 噪声和图像尺寸减小，以及 phase 图像编码和量子快速幂transform。</li>
<li>results: 该论文提出了一种基于 phase 图像编码和 swap 测试的不同性度量，并在理想和噪声掺杂的 simulate 环境中进行了实验，并在 IBM 和 Ionq 量子设备上进行了 Swap 测试。<details>
<summary>Abstract</summary>
A block-matching algorithm finds a group of similar image patches inside a search area. Similarity/dissimilarity measures can help to solve this problem. In different practical applications, finding groups of similar image blocks within an ample search area is often necessary, such as video compression, image clustering, vector quantization, and nonlocal noise reduction. In this work, classical image processing is performed using Gaussian noise and image size reduction with a fit of a Low-Pass Filter or Domain Transform. A hierarchical search technique is implemented to encode the images by phase operator. Using phase image coding with the quantum Fourier transform and the Swap test, we propose a dissimilarity measure. Results were obtained with perfect and noisy simulations and in the case of the Swap test with the IBM and Ionq quantum devices.
</details>
<details>
<summary>摘要</summary>
algorithm 寻找内部 search 区域中相似的图像块。相似性/不同性度量可以解决这个问题。在实际应用中，寻找内部 search 区域中的相似图像块是非常重要的，例如影像压缩、影像集群、向量量化和非本地噪音减少。在这个工作中，使用 Gaussian 噪声和影像缩小以适应低通滤过或领域转换。使用层次搜寻技术实现影像编码，使用相位操作器进行编码。使用相位图像编码、量子 fourier 转换和交换测试，我们提出了一个不同度量。实际成果是在完美和噪音 simulation 中获得，以及在交换测试中使用 IBM 和 Ionq 量子设备。
</details></li>
</ul>
<hr>
<h2 id="Partial-Transport-for-Point-Cloud-Registration"><a href="#Partial-Transport-for-Point-Cloud-Registration" class="headerlink" title="Partial Transport for Point-Cloud Registration"></a>Partial Transport for Point-Cloud Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15787">http://arxiv.org/abs/2309.15787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yikun Bai, Huy Tran, Steven B. Damelin, Soheil Kolouri</li>
<li>for: 非静止点云注册问题在机器人、计算机图形和医疗成像等领域中扮演着关键角色，其中面临非静止运动和部分可见性（如干扰或感测器噪声）的问题。</li>
<li>methods: 本文通过优化运输问题和其不均衡变种（如优化部分运输问题）来解决非静止点云注册问题，并提出一系列基于优化partial运输问题的非静止注册方法。然后，通过利用一 dimensional优化partial运输问题的有效解决方法的扩展，提高了算法的计算效率，从而实现了快速和稳定的非静止注册算法。</li>
<li>results: 本文通过对多个3D和2D非静止注册问题进行测试和比较，证明了我们提出的方法的有效性和稳定性。在扰动和噪声的情况下，我们的方法可以快速和精度地解决非静止注册问题。<details>
<summary>Abstract</summary>
Point cloud registration plays a crucial role in various fields, including robotics, computer graphics, and medical imaging. This process involves determining spatial relationships between different sets of points, typically within a 3D space. In real-world scenarios, complexities arise from non-rigid movements and partial visibility, such as occlusions or sensor noise, making non-rigid registration a challenging problem. Classic non-rigid registration methods are often computationally demanding, suffer from unstable performance, and, importantly, have limited theoretical guarantees. The optimal transport problem and its unbalanced variations (e.g., the optimal partial transport problem) have emerged as powerful tools for point-cloud registration, establishing a strong benchmark in this field. These methods view point clouds as empirical measures and provide a mathematically rigorous way to quantify the `correspondence' between (the transformed) source and target points. In this paper, we approach the point-cloud registration problem through the lens of optimal transport theory and first propose a comprehensive set of non-rigid registration methods based on the optimal partial transportation problem. Subsequently, leveraging the emerging work on efficient solutions to the one-dimensional optimal partial transport problem, we extend our proposed algorithms via slicing to gain significant computational efficiency, resulting in fast and robust non-rigid registration algorithms. We demonstrate the effectiveness of our proposed methods and compare them against baselines on various 3D and 2D non-rigid registration problems where the source and target point clouds are corrupted by random noise.
</details>
<details>
<summary>摘要</summary>
点云注册在不同领域中扮演着关键的角色，包括机器人学、计算机图形学和医学影像。这个过程涉及到不同集点之间的空间关系的确定，通常在3D空间中。在实际应用中，复杂性来自于非RIGID运动和部分可见性，如遮挡或感器噪声，使得非RIGID注册成为一个具有挑战性的问题。 классические非RIGID注册方法通常具有计算昂贵、性能不稳定和有限的理论保证。优化运输问题和其不均变种（例如优化部分运输问题）在点云注册中发挥了强大的作用，成为这个领域的标准准则。这些方法视点云为实际测量，并提供了数学上的正式方式来衡量（变换后）源点云和目标点云之间的匹配程度。在本文中，我们通过优化运输理论的镜像来解决点云注册问题，并首次提出了基于优化部分运输问题的全面非RIGID注册方法。然后，通过利用emerging工作在一维优化部分运输问题上的高效解决方法，我们扩展了我们的提议算法，从而获得了快速和可靠的非RIGID注册算法。我们在不同的3D和2D非RIGID注册问题上证明了我们的提议方法的有效性，并与基准方法进行比较。
</details></li>
</ul>
<hr>
<h2 id="One-For-All-Video-Conversation-is-Feasible-Without-Video-Instruction-Tuning"><a href="#One-For-All-Video-Conversation-is-Feasible-Without-Video-Instruction-Tuning" class="headerlink" title="One For All: Video Conversation is Feasible Without Video Instruction Tuning"></a>One For All: Video Conversation is Feasible Without Video Instruction Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15785">http://arxiv.org/abs/2309.15785</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/farewellthree/BT-Adapter">https://github.com/farewellthree/BT-Adapter</a></li>
<li>paper_authors: Ruyang Liu, Chen Li, Yixiao Ge, Ying Shan, Thomas H. Li, Ge Li</li>
<li>for: 提高视频对话系统的效能，使用现有的图像对话模型进行扩展。</li>
<li>methods: 提出了一种名为 Branching Temporal Adapter（BT-Adapter）的新方法，可以将图像语言预测模型扩展到视频领域。BT-Adapter acting as a temporal modeling branch alongside the pretrained visual encoder,并且在保持backbone冻结的情况下进行调教。</li>
<li>results: 通过BT-Adapter，可以让现有的多Modal对话模型具备强大的视频理解能力，而无需耗费过多的GPU资源。BT-Adapter可以在少量的GPU时间内达到state-of-the-art的零基eline结果，并且在不带视频指导的情况下达到更好的性能。<details>
<summary>Abstract</summary>
The recent progress in Large Language Models (LLM) has spurred various advancements in image-language conversation agents, while how to build a proficient video-based dialogue system is still under exploration. Considering the extensive scale of LLM and visual backbone, minimal GPU memory is left for facilitating effective temporal modeling, which is crucial for comprehending and providing feedback on videos. To this end, we propose Branching Temporal Adapter (BT-Adapter), a novel method for extending image-language pretrained models into the video domain. Specifically, BT-Adapter serves as a plug-and-use temporal modeling branch alongside the pretrained visual encoder, which is tuned while keeping the backbone frozen. Just pretrained once, BT-Adapter can be seamlessly integrated into all image conversation models using this version of CLIP, enabling video conversations without the need for video instructions. Besides, we develop a unique asymmetric token masking strategy inside the branch with tailor-made training tasks for BT-Adapter, facilitating faster convergence and better results. Thanks to BT-Adapter, we are able to empower existing multimodal dialogue models with strong video understanding capabilities without incurring excessive GPU costs. Without bells and whistles, BT-Adapter achieves (1) state-of-the-art zero-shot results on various video tasks using thousands of fewer GPU hours. (2) better performance than current video chatbots without any video instruction tuning. (3) state-of-the-art results of video chatting using video instruction tuning, outperforming previous SOTAs by a large margin.
</details>
<details>
<summary>摘要</summary>
Recent progress in Large Language Models (LLM) has led to advancements in image-language conversation agents, but how to build a proficient video-based dialogue system is still being explored. Due to the extensive scale of LLM and visual backbone, there is limited GPU memory available for effective temporal modeling, which is crucial for understanding and providing feedback on videos. To address this challenge, we propose Branching Temporal Adapter (BT-Adapter), a novel method for extending image-language pretrained models into the video domain. Specifically, BT-Adapter serves as a plug-and-use temporal modeling branch alongside the pretrained visual encoder, which is tuned while keeping the backbone frozen. With just one pretraining, BT-Adapter can be seamlessly integrated into all image conversation models using this version of CLIP, enabling video conversations without the need for video instructions. Additionally, we develop a unique asymmetric token masking strategy inside the branch with tailor-made training tasks for BT-Adapter, which facilitates faster convergence and better results. Thanks to BT-Adapter, we can empower existing multimodal dialogue models with strong video understanding capabilities without incurring excessive GPU costs. Without any bells and whistles, BT-Adapter achieves the following:1. State-of-the-art zero-shot results on various video tasks using thousands of fewer GPU hours.2. Better performance than current video chatbots without any video instruction tuning.3. State-of-the-art results of video chatting using video instruction tuning, outperforming previous SOTAs by a large margin.
</details></li>
</ul>
<hr>
<h2 id="Joint-YODNet-A-Light-weight-Object-Detector-for-UAVs-to-Achieve-Above-100fps"><a href="#Joint-YODNet-A-Light-weight-Object-Detector-for-UAVs-to-Achieve-Above-100fps" class="headerlink" title="Joint-YODNet: A Light-weight Object Detector for UAVs to Achieve Above 100fps"></a>Joint-YODNet: A Light-weight Object Detector for UAVs to Achieve Above 100fps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15782">http://arxiv.org/abs/2309.15782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vipin Gautam, Shitala Prasad, Sharad Sinha</li>
<li>for: 这篇论文旨在提高无人航空车（UAV）影像中小物体检测的精度。</li>
<li>methods: 本论文提出了一个新的联合损失函数（JointYODNet），用于强化小物体检测的精度。这个联合损失函数结合了对小物体检测的特有损失函数。</li>
<li>results: 经过广泛的实验，我们发现我们提出的联合损失函数可以优化小物体检测的精度。特别是，我们的方法在不同环境下检测小物体的精度是97.1%，F1 Score是97.5%，并且实现了mAP@.5的98.6%。<details>
<summary>Abstract</summary>
Small object detection via UAV (Unmanned Aerial Vehicle) images captured from drones and radar is a complex task with several formidable challenges. This domain encompasses numerous complexities that impede the accurate detection and localization of small objects. To address these challenges, we propose a novel method called JointYODNet for UAVs to detect small objects, leveraging a joint loss function specifically designed for this task. Our method revolves around the development of a joint loss function tailored to enhance the detection performance of small objects. Through extensive experimentation on a diverse dataset of UAV images captured under varying environmental conditions, we evaluated different variations of the loss function and determined the most effective formulation. The results demonstrate that our proposed joint loss function outperforms existing methods in accurately localizing small objects. Specifically, our method achieves a recall of 0.971, and a F1Score of 0.975, surpassing state-of-the-art techniques. Additionally, our method achieves a mAP@.5(%) of 98.6, indicating its robustness in detecting small objects across varying scales
</details>
<details>
<summary>摘要</summary>
小物体检测 via UAV（无人航空器）图像 captured from drones和雷达是一项复杂任务，涉及许多可考的挑战。这个领域涵盖许多复杂性，阻碍精准检测和定位小物体。为解决这些挑战，我们提出了一种新的方法 called JointYODNet，用于UAVs中的小物体检测。我们的方法基于特制的联合损失函数，用于提高小物体检测性能。通过对不同环境下UAV图像的广泛实验，我们评估了不同版本的损失函数，并确定了最有效的形式。结果表明，我们的联合损失函数可以高效地地localize小物体，并且在不同的缩放比例下保持稳定性。具体来说，我们的方法达到了0.971的回归率，和0.975的F1Score，这两个指标都高于当前的State-of-the-art技术。此外，我们的方法达到了98.6%的mAP@.5（%），这表明它在不同的缩放比例下具有较高的小物体检测稳定性。
</details></li>
</ul>
<hr>
<h2 id="AaP-ReID-Improved-Attention-Aware-Person-Re-identification"><a href="#AaP-ReID-Improved-Attention-Aware-Person-Re-identification" class="headerlink" title="AaP-ReID: Improved Attention-Aware Person Re-identification"></a>AaP-ReID: Improved Attention-Aware Person Re-identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15780">http://arxiv.org/abs/2309.15780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vipin Gautam, Shitala Prasad, Sharad Sinha</li>
<li>for: 本研究的目标是解决人识别 task 中的特定个体识别问题，以提高人识别的精度和可靠性。</li>
<li>methods: 我们提出了一种基于 ResNet 架构的 AaP-ReID 方法，其中包含 Channel-Wise Attention Bottleneck (CWAbottleneck) 块，可以动态调整每个通道的重要性，以学习更有力的特征。</li>
<li>results: 我们在 Market-1501、DukeMTMC-reID 和 CUHK03 三个 benchmark 数据集上进行了评估，与state-of-the-art 人识别方法相比，我们的 AaP-ReID 方法在rank-1准确率上达到了 95.6%、90.6% 和 82.4% 的水平。<details>
<summary>Abstract</summary>
Person re-identification (ReID) is a well-known problem in the field of computer vision. The primary objective is to identify a specific individual within a gallery of images. However, this task is challenging due to various factors, such as pose variations, illumination changes, obstructions, and the presence ofconfusing backgrounds. Existing ReID methods often fail to capture discriminative features (e.g., head, shoes, backpacks) and instead capture irrelevant features when the target is occluded. Motivated by the success of part-based and attention-based ReID methods, we improve AlignedReID++ and present AaP-ReID, a more effective method for person ReID that incorporates channel-wise attention into a ResNet-based architecture. Our method incorporates the Channel-Wise Attention Bottleneck (CWAbottleneck) block and can learn discriminating features by dynamically adjusting the importance ofeach channel in the feature maps. We evaluated Aap-ReID on three benchmark datasets: Market-1501, DukeMTMC-reID, and CUHK03. When compared with state-of-the-art person ReID methods, we achieve competitive results with rank-1 accuracies of 95.6% on Market-1501, 90.6% on DukeMTMC-reID, and 82.4% on CUHK03.
</details>
<details>
<summary>摘要</summary>
人脸重认（ReID）是计算机视觉领域的一个很有名的问题。主要目标是在一组图像中识别特定的个体。但这个任务受到多种因素的影响，如 pose 变化、照明变化、阻挡物和误导背景的存在。现有的 ReID 方法 oftentimes 未能捕捉特征特征（例如头、鞋、背包），而是在目标被遮盖时捕捉无关的特征。我们受到部分基于和注意力基于 ReID 方法的成功的激励，我们改进了 AlignedReID++ 并提出了 AaP-ReID，一种更有效的人脸重认方法。我们的方法包括 Channel-Wise Attention Bottleneck（CWAbottleneck）块，可以在 ResNet 基本架构中动态调整每个通道的重要性，从而学习特征。我们在 Market-1501、DukeMTMC-reID 和 CUHK03 三个标准数据集上评估 AaP-ReID，与状态之前的人脸重认方法相比，我们实现了竞争性的结果，rank-1 准确率分别达到 95.6%、90.6% 和 82.4%。
</details></li>
</ul>
<hr>
<h2 id="Aperture-Diffraction-for-Compact-Snapshot-Spectral-Imaging"><a href="#Aperture-Diffraction-for-Compact-Snapshot-Spectral-Imaging" class="headerlink" title="Aperture Diffraction for Compact Snapshot Spectral Imaging"></a>Aperture Diffraction for Compact Snapshot Spectral Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16372">http://arxiv.org/abs/2309.16372</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/krito-ex/csst">https://github.com/krito-ex/csst</a></li>
<li>paper_authors: Tao Lv, Hao Ye, Quan Yuan, Zhan Shi, Yibo Wang, Shuming Wang, Xun Cao</li>
<li>for: 这个论文旨在描述一种名为 aperature Diffraction Imaging Spectrometer（ADIS）的新型快速 spectral imaging 系统，该系统只有一个映射镜和一个多元滤色器传感器，不需要任何额外的物理设备。</li>
<li>methods: 论文提出了一种新的光学设计，即通过diffraction-based spatial-spectral projection engineering来将对象空间中的每个点多态化到滤色器传感器上的不同编码位置。这种多态化的设计使得只需要单个曝光幕Raw image data可以实现高 spectral resolution和低aliasing的图像重建。</li>
<li>results:  experiments show that the proposed system can achieve sub-super-pixel spatial resolution and high spectral resolution imaging, and the reconstructed images are highly consistent with the original data. In addition, the system is evaluated by analyzing the imaging optical theory and reconstruction algorithm, and the code will be available at GitHub.<details>
<summary>Abstract</summary>
We demonstrate a compact, cost-effective snapshot spectral imaging system named Aperture Diffraction Imaging Spectrometer (ADIS), which consists only of an imaging lens with an ultra-thin orthogonal aperture mask and a mosaic filter sensor, requiring no additional physical footprint compared to common RGB cameras. Then we introduce a new optical design that each point in the object space is multiplexed to discrete encoding locations on the mosaic filter sensor by diffraction-based spatial-spectral projection engineering generated from the orthogonal mask. The orthogonal projection is uniformly accepted to obtain a weakly calibration-dependent data form to enhance modulation robustness. Meanwhile, the Cascade Shift-Shuffle Spectral Transformer (CSST) with strong perception of the diffraction degeneration is designed to solve a sparsity-constrained inverse problem, realizing the volume reconstruction from 2D measurements with Large amount of aliasing. Our system is evaluated by elaborating the imaging optical theory and reconstruction algorithm with demonstrating the experimental imaging under a single exposure. Ultimately, we achieve the sub-super-pixel spatial resolution and high spectral resolution imaging. The code will be available at: https://github.com/Krito-ex/CSST.
</details>
<details>
<summary>摘要</summary>
我们提出了一种具有高效性和可持续性的快照spectral imaging系统，名为Aperture Diffraction Imaging Spectrometer（ADIS）。该系统只有一个映射镜和一个 orthogonal aperture mask，而不需要任何额外的物理空间。我们还介绍了一种新的光学设计，使得对象空间中的每个点都被多样化到灰度滤波器传感器上的独立编码位置。这种多样化是通过扩散基于的空间-спектраль投影工程实现的，从而获得弱依赖于均衡 calibration 的数据形式，以提高模拟稳定性。同时，我们设计了一种叫做Cascade Shift-Shuffle Spectral Transformer（CSST）的新型神经网络，用于解决一个具有扩散约束的减少问题，实现从2D测量获得3D重建。我们通过推导光学学理和重建算法，并进行实验测试，最终实现了下sampling 的超分辨率和高spectral resolution的成像。系统代码将在https://github.com/Krito-ex/CSST 上提供。
</details></li>
</ul>
<hr>
<h2 id="High-Perceptual-Quality-Wireless-Image-Delivery-with-Denoising-Diffusion-Models"><a href="#High-Perceptual-Quality-Wireless-Image-Delivery-with-Denoising-Diffusion-Models" class="headerlink" title="High Perceptual Quality Wireless Image Delivery with Denoising Diffusion Models"></a>High Perceptual Quality Wireless Image Delivery with Denoising Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15889">http://arxiv.org/abs/2309.15889</a></li>
<li>repo_url: None</li>
<li>paper_authors: Selim F. Yilmaz, Xueyan Niu, Bo Bai, Wei Han, Lei Deng, Deniz Gunduz</li>
<li>for: 实现受损图像传输过程中的杂音无线通信频道之间的深度学习基于的共同源码渠道（DeepJSCC）和评估频道的数据模型（DDPM）。</li>
<li>methods: 使用目标图像的范围空间分解，将图像转换成范围空间后，使用DDPM进行累进填充null空间内容。</li>
<li>results: 在实际finite block length regime中，与传统DeepJSCC和当前学习型基于方法进行比较，实现了较好的干扰和人类视觉质量。将源代码公开供研究和重现。<details>
<summary>Abstract</summary>
We consider the image transmission problem over a noisy wireless channel via deep learning-based joint source-channel coding (DeepJSCC) along with a denoising diffusion probabilistic model (DDPM) at the receiver. Specifically, we are interested in the perception-distortion trade-off in the practical finite block length regime, in which separate source and channel coding can be highly suboptimal. We introduce a novel scheme that utilizes the range-null space decomposition of the target image. We transmit the range-space of the image after encoding and employ DDPM to progressively refine its null space contents. Through extensive experiments, we demonstrate significant improvements in distortion and perceptual quality of reconstructed images compared to standard DeepJSCC and the state-of-the-art generative learning-based method. We will publicly share our source code to facilitate further research and reproducibility.
</details>
<details>
<summary>摘要</summary>
我们考虑了通过深度学习基于源-通道编码（DeepJSCC）和推 diffusion概率模型（DDPM）的图像传输问题，特别是在实际 finite block length  Régime中进行评估。我们关注图像传输过程中的觉受-误差交易，在这种情况下，分离的源和通道编码可能是非常不优化的。我们提出了一种新的方案，利用目标图像的范围空间划分。我们在编码后将范围空间传输给接收方，并使用 DDPM 进行逐渐提高null空间内容的进程。经过广泛的实验，我们发现了对于重建图像的误差和人类识别质量都有显著改善，相比标准 DeepJSCC 和当前最佳生成学习基于方法。我们将将源代码公开发布，以便进一步的研究和复现。
</details></li>
</ul>
<hr>
<h2 id="Rapid-Network-Adaptation-Learning-to-Adapt-Neural-Networks-Using-Test-Time-Feedback"><a href="#Rapid-Network-Adaptation-Learning-to-Adapt-Neural-Networks-Using-Test-Time-Feedback" class="headerlink" title="Rapid Network Adaptation: Learning to Adapt Neural Networks Using Test-Time Feedback"></a>Rapid Network Adaptation: Learning to Adapt Neural Networks Using Test-Time Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15762">http://arxiv.org/abs/2309.15762</a></li>
<li>repo_url: None</li>
<li>paper_authors: Teresa Yeo, Oğuzhan Fatih Kar, Zahra Sodagar, Amir Zamir</li>
<li>for: 本文提出了一种适应分布shift的方法，用于在测试时进行适应。与传统的训练时Robustness机制不同，我们创建了一个循环系统，并使用测试时反馈信号来适应网络。</li>
<li>methods: 我们使用了一种学习基于的函数来实现这个循环系统，实现了一个摘要优化器 для网络。</li>
<li>results: 我们通过了广泛的实验，包括不同的散度shift、任务和数据集，并显示了这种方法的高效性和灵活性。<details>
<summary>Abstract</summary>
We propose a method for adapting neural networks to distribution shifts at test-time. In contrast to training-time robustness mechanisms that attempt to anticipate and counter the shift, we create a closed-loop system and make use of a test-time feedback signal to adapt a network on the fly. We show that this loop can be effectively implemented using a learning-based function, which realizes an amortized optimizer for the network. This leads to an adaptation method, named Rapid Network Adaptation (RNA), that is notably more flexible and orders of magnitude faster than the baselines. Through a broad set of experiments using various adaptation signals and target tasks, we study the efficiency and flexibility of this method. We perform the evaluations using various datasets (Taskonomy, Replica, ScanNet, Hypersim, COCO, ImageNet), tasks (depth, optical flow, semantic segmentation, classification), and distribution shifts (Cross-datasets, 2D and 3D Common Corruptions) with promising results. We end with a discussion on general formulations for handling distribution shifts and our observations from comparing with similar approaches from other domains.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法，用于在测试时适应分布变化。与训练时鲁棒性机制不同，我们创建了一个封闭的循环系统，并使用测试时反馈信号来适应网络的 fly。我们表明，这种循环可以使用学习基于函数，实现一个摘要优化器，从而实现了网络的适应方法。我们称之为快速网络适应（RNA）。这种方法比基准更加灵活，并且速度几个数量级更快。通过使用不同的适应信号和目标任务，我们在各种实验中研究了这种方法的效率和灵活性。我们使用了不同的数据集（Taskonomy、Replica、ScanNet、Hypersim、COCO、ImageNet）、任务（深度、光流、semantic segmentation、分类）和分布变化（跨数据集、2D和3D Common Corruptions），并获得了可观的结果。我们在结束时对类似的方法进行了比较，并进行了一些总结和讨论。
</details></li>
</ul>
<hr>
<h2 id="CAIT-Triple-Win-Compression-towards-High-Accuracy-Fast-Inference-and-Favorable-Transferability-For-ViTs"><a href="#CAIT-Triple-Win-Compression-towards-High-Accuracy-Fast-Inference-and-Favorable-Transferability-For-ViTs" class="headerlink" title="CAIT: Triple-Win Compression towards High Accuracy, Fast Inference, and Favorable Transferability For ViTs"></a>CAIT: Triple-Win Compression towards High Accuracy, Fast Inference, and Favorable Transferability For ViTs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15755">http://arxiv.org/abs/2309.15755</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ao Wang, Hui Chen, Zijia Lin, Sicheng Zhao, Jungong Han, Guiguang Ding</li>
<li>for: 这个论文的目的是提出一种基于ViTs的 JOINT 压缩方法，以提高模型的速度和准确率，同时保持下游任务的可贯通性。</li>
<li>methods: 这个方法使用了一种异常的token合并策略（ATME），通过将邻近的token合并起来，成功地压缩了重复的token信息，保持图像的空间结构。此外，这个方法还使用了一种一致动态通道剔除策略（CDCP），可以动态剔除不重要的通道在ViTs中，大幅提高模型压缩。</li>
<li>results: 经过广泛的实验表明，这个方法可以在不同的ViTs上达到最佳性能，比如ImageNet上的DeiT-Tiny和DeiT-Small模型可以 Achieve 1.7$\times$和1.9$\times$的速度提升，而无需减少准确率。在ADE20k segmentationdataset上，这个方法可以具有最多1.31$\times$的速度提升，与相同的mIoU水平兼容。<details>
<summary>Abstract</summary>
Vision Transformers (ViTs) have emerged as state-of-the-art models for various vision tasks recently. However, their heavy computation costs remain daunting for resource-limited devices. Consequently, researchers have dedicated themselves to compressing redundant information in ViTs for acceleration. However, they generally sparsely drop redundant image tokens by token pruning or brutally remove channels by channel pruning, leading to a sub-optimal balance between model performance and inference speed. They are also disadvantageous in transferring compressed models to downstream vision tasks that require the spatial structure of images, such as semantic segmentation. To tackle these issues, we propose a joint compression method for ViTs that offers both high accuracy and fast inference speed, while also maintaining favorable transferability to downstream tasks (CAIT). Specifically, we introduce an asymmetric token merging (ATME) strategy to effectively integrate neighboring tokens. It can successfully compress redundant token information while preserving the spatial structure of images. We further employ a consistent dynamic channel pruning (CDCP) strategy to dynamically prune unimportant channels in ViTs. Thanks to CDCP, insignificant channels in multi-head self-attention modules of ViTs can be pruned uniformly, greatly enhancing the model compression. Extensive experiments on benchmark datasets demonstrate that our proposed method can achieve state-of-the-art performance across various ViTs. For example, our pruned DeiT-Tiny and DeiT-Small achieve speedups of 1.7$\times$ and 1.9$\times$, respectively, without accuracy drops on ImageNet. On the ADE20k segmentation dataset, our method can enjoy up to 1.31$\times$ speedups with comparable mIoU. Our code will be publicly available.
</details>
<details>
<summary>摘要</summary>
目标是提出一种可以同时保持高准确率和快速推理速度的 ViT 压缩方法，而且可以在下游视觉任务中保持图像的空间结构。我们提出了一种强化 token 合并策略（ATME），可以有效地压缩重复的token信息，同时保持图像的空间结构。此外，我们还采用了一种 dynamically 频道剪枝策略（CDCP），可以在 ViT 中动态剪枝无关的频道，从而提高模型压缩。我们的方法可以在多种 ViT 上达到领先的性能，例如，我们的压缩后 DeiT-Tiny 和 DeiT-Small 可以在 ImageNet 上增加 1.7 倍和 1.9 倍的速度，同时保持准确性。在 ADE20k  segmentation 数据集上，我们的方法可以增加到 1.31 倍的速度，与相同的 mIoU 相对。我们的代码将公开。
</details></li>
</ul>
<hr>
<h2 id="InfraParis-A-multi-modal-and-multi-task-autonomous-driving-dataset"><a href="#InfraParis-A-multi-modal-and-multi-task-autonomous-driving-dataset" class="headerlink" title="InfraParis: A multi-modal and multi-task autonomous driving dataset"></a>InfraParis: A multi-modal and multi-task autonomous driving dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15751">http://arxiv.org/abs/2309.15751</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gianni Franchi, Marwane Hariat, Xuanlong Yu, Nacim Belkhir, Antoine Manzanera, David Filliat</li>
<li>for: 这个论文旨在提供一个多模态数据集，以便提高自动驾驶计算机视觉模型的可靠性和多样化性。</li>
<li>methods: 这个论文使用了多种现有的深度神经网络模型，并对其进行了评估。</li>
<li>results: 论文发现，使用多模态数据集可以提高模型的性能，并且可以更好地处理新的对象、噪音、夜间条件和多样化场景。<details>
<summary>Abstract</summary>
Current deep neural networks (DNNs) for autonomous driving computer vision are typically trained on specific datasets that only involve a single type of data and urban scenes. Consequently, these models struggle to handle new objects, noise, nighttime conditions, and diverse scenarios, which is essential for safety-critical applications. Despite ongoing efforts to enhance the resilience of computer vision DNNs, progress has been sluggish, partly due to the absence of benchmarks featuring multiple modalities. We introduce a novel and versatile dataset named InfraParis that supports multiple tasks across three modalities: RGB, depth, and infrared. We assess various state-of-the-art baseline techniques, encompassing models for the tasks of semantic segmentation, object detection, and depth estimation.
</details>
<details>
<summary>摘要</summary>
当前的深度神经网络（DNNs） для自动驾驶计算机视觉通常是通过特定的数据集训练的，这些数据集只包含单一的数据和城市场景。因此，这些模型具有处理新的对象、噪音、夜间条件和多样化场景的能力异常差，这是安全应用的关键。虽然持续努力提高计算机视觉DNNs的鲜度，但进步缓慢，其中一个原因是多模态的标准准。我们介绍了一个新的和多样的数据集名为InfraParis，该数据集支持多个任务逐模态：RGB、深度和红外。我们评估了多种现有的基线技术，包括 semantic segmentation、物体检测和深度估计等任务的模型。
</details></li>
</ul>
<hr>
<h2 id="Automated-CT-Lung-Cancer-Screening-Workflow-using-3D-Camera"><a href="#Automated-CT-Lung-Cancer-Screening-Workflow-using-3D-Camera" class="headerlink" title="Automated CT Lung Cancer Screening Workflow using 3D Camera"></a>Automated CT Lung Cancer Screening Workflow using 3D Camera</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15750">http://arxiv.org/abs/2309.15750</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brian Teixeira, Vivek Singh, Birgi Tamersoy, Andreas Prokein, Ankur Kapoor</li>
<li>for: 这篇论文的目的是为了减少CT扫描中需要的时间consuming scout scans，并且自动化病人定位。</li>
<li>methods: 这篇论文使用了一个新的方法，可以从3D相机影像中估算病人的扫描范围、中心点和水平径确（WED），不需要使用实验 scan data。这个方法通过对超过60,000个CT扫描数据进行训练，并引入一个新的更新方法，可以在实时扫描数据中更新预测。</li>
<li>results: 这篇论文的结果显示，使用这个新方法可以很好地减少CT扫描中需要的时间和精度误差。在110对深度数据和CT扫描数据的测试集中，这个方法可以很好地估算病人的中心点、扫描范围和WED。相比IEC的Acceptance对�项目的10%，这个方法的相关WED误差为4%。<details>
<summary>Abstract</summary>
Despite recent developments in CT planning that enabled automation in patient positioning, time-consuming scout scans are still needed to compute dose profile and ensure the patient is properly positioned. In this paper, we present a novel method which eliminates the need for scout scans in CT lung cancer screening by estimating patient scan range, isocenter, and Water Equivalent Diameter (WED) from 3D camera images. We achieve this task by training an implicit generative model on over 60,000 CT scans and introduce a novel approach for updating the prediction using real-time scan data. We demonstrate the effectiveness of our method on a testing set of 110 pairs of depth data and CT scan, resulting in an average error of 5mm in estimating the isocenter, 13mm in determining the scan range, 10mm and 16mm in estimating the AP and lateral WED respectively. The relative WED error of our method is 4%, which is well within the International Electrotechnical Commission (IEC) acceptance criteria of 10%.
</details>
<details>
<summary>摘要</summary>
尽管最近的 computed tomography (CT) 规划技术已经实现了患者定位自动化，但是时间consuming的探测扫描仍然需要进行以计算剂量profile和确保患者是正确地位置。在这篇论文中，我们提出了一种新的方法，它可以消除 CT 肺癌检测中的探测扫描。我们通过对超过 60,000 个 CT 扫描图像进行训练，并 introduce 一种新的更新预测方法使用实时扫描数据。我们在测试集上进行了 110 对深度数据和 CT 扫描的对比，得到了平均错误为 5mm，13mm，10mm和16mm，分别用于计算中心点、扫描范围、AP和 lateral Water Equivalent Diameter (WED)。我们的方法的相对 WED 错误率为 4%，这在国际电工标准委员会 (IEC) 接受的 10% 范围内。
</details></li>
</ul>
<hr>
<h2 id="Synthetic-Latent-Fingerprint-Generation-Using-Style-Transfer"><a href="#Synthetic-Latent-Fingerprint-Generation-Using-Style-Transfer" class="headerlink" title="Synthetic Latent Fingerprint Generation Using Style Transfer"></a>Synthetic Latent Fingerprint Generation Using Style Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15734">http://arxiv.org/abs/2309.15734</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amol S. Joshi, Ali Dabouei, Nasser Nasrabadi, Jeremy Dawson</li>
<li>for: 这篇论文旨在提供一种简单实用的方法来生成具有实际特征的潜在指纹资料，以便训练需要大量资料的神经网络模型。</li>
<li>methods: 本研究使用了Style Transfer和图像融合技术来实现潜在指纹生成。</li>
<li>results: 实验结果显示，生成的潜在指纹资料 preserve 输入触感指纹资料中的身份信息，并具有真实潜在指纹资料的特征。此外，生成的指纹资料显示了多种特征和样式，表明提案方法可以从同一个指纹中产生多个样本。<details>
<summary>Abstract</summary>
Limited data availability is a challenging problem in the latent fingerprint domain. Synthetically generated fingerprints are vital for training data-hungry neural network-based algorithms. Conventional methods distort clean fingerprints to generate synthetic latent fingerprints. We propose a simple and effective approach using style transfer and image blending to synthesize realistic latent fingerprints. Our evaluation criteria and experiments demonstrate that the generated synthetic latent fingerprints preserve the identity information from the input contact-based fingerprints while possessing similar characteristics as real latent fingerprints. Additionally, we show that the generated fingerprints exhibit several qualities and styles, suggesting that the proposed method can generate multiple samples from a single fingerprint.
</details>
<details>
<summary>摘要</summary>
限制的数据可用性是 latent fingerprint 领域中的一个挑战。Synthetically generated fingerprints 是训练数据涉及大量神经网络算法的重要资源。传统方法会扭曲清晰的指纹来生成伪装的 latent fingerprints。我们提议一种简单有效的方法，使用 Style transfer 和图像融合来生成真实的 latent fingerprints。我们的评估标准和实验表明，生成的伪装 latent fingerprints 保留输入的 Contact-based fingerprints 中的身份信息，同时具有真实 latent fingerprints 的相似特征。此外，我们还示出了生成的指纹具有多种特征和风格， suggesting that the proposed method can generate multiple samples from a single fingerprint.
</details></li>
</ul>
<hr>
<h2 id="Factorized-Diffusion-Architectures-for-Unsupervised-Image-Generation-and-Segmentation"><a href="#Factorized-Diffusion-Architectures-for-Unsupervised-Image-Generation-and-Segmentation" class="headerlink" title="Factorized Diffusion Architectures for Unsupervised Image Generation and Segmentation"></a>Factorized Diffusion Architectures for Unsupervised Image Generation and Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15726">http://arxiv.org/abs/2309.15726</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Yuan, Michael Maire</li>
<li>for: 这个论文是为了开发一种无监督的神经网络架构，用于同时生成和分割图像。</li>
<li>methods: 该模型采用无监督的杂化扩散目标来驱动学习，不需任何标注或区域知识来训练。模型中的计算瓶颈使得杂化网络partition输入图像，并在平行进行净化和结合结果。</li>
<li>results: 我们的训练模型可以生成高质量的合成图像和对真实图像进行无监督的图像分割，并且不需任何迭代训练或标注。实验结果表明，我们的模型可以准确地完成无监督图像分割任务和高质量的合成图像生成。<details>
<summary>Abstract</summary>
We develop a neural network architecture which, trained in an unsupervised manner as a denoising diffusion model, simultaneously learns to both generate and segment images. Learning is driven entirely by the denoising diffusion objective, without any annotation or prior knowledge about regions during training. A computational bottleneck, built into the neural architecture, encourages the denoising network to partition an input into regions, denoise them in parallel, and combine the results. Our trained model generates both synthetic images and, by simple examination of its internal predicted partitions, a semantic segmentation of those images. Without any finetuning, we directly apply our unsupervised model to the downstream task of segmenting real images via noising and subsequently denoising them. Experiments demonstrate that our model achieves accurate unsupervised image segmentation and high-quality synthetic image generation across multiple datasets.
</details>
<details>
<summary>摘要</summary>
我们开发了一种神经网络架构，通过不经过监督的方式，同时学习生成和分割图像。在训练过程中，我们没有任何注释或区域知识， entirely driven by the denoising diffusion objective。我们的神经网络架构包含计算瓶颈，使得杂化网络partition输入图像，并在平行进行杂化和结合结果。我们训练的模型可以同时生成 sintetic 图像和通过内部预测的分区来实现图像 semantic segmentation。无需追究，我们直接将无监督模型应用于图像分割下游任务，通过噪音和杂化图像来进行预测。实验表明，我们的模型可以准确无监督地分割图像和生成高质量的 sintetic 图像，在多个 dataset 上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Physics-Based-Rigid-Body-Object-Tracking-and-Friction-Filtering-From-RGB-D-Videos"><a href="#Physics-Based-Rigid-Body-Object-Tracking-and-Friction-Filtering-From-RGB-D-Videos" class="headerlink" title="Physics-Based Rigid Body Object Tracking and Friction Filtering From RGB-D Videos"></a>Physics-Based Rigid Body Object Tracking and Friction Filtering From RGB-D Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15703">http://arxiv.org/abs/2309.15703</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rama Krishna Kandukuri, Michael Strecke, Joerg Stueckler</li>
<li>for: 这 paper 是为了解决物体间互动的理解问题，以便在增强现实和机器人领域中实现更加 precisione 的模拟和控制。</li>
<li>methods: 该 paper 使用了一种新的方法，即使用可微的物理模拟来模型物体的互动，并通过扩展卡尔曼滤波来Track 3D 物体的位姿和物理性能。</li>
<li>results: 该 paper 的实验结果表明，该方法可以准确地滤波物体的位姿和动量，同时也可以估算物体的透抗率。 furthermore, 该 paper 还提供了一些实验结果，证明该方法在不同的滑动场景中的性能。<details>
<summary>Abstract</summary>
Physics-based understanding of object interactions from sensory observations is an essential capability in augmented reality and robotics. It enables capturing the properties of a scene for simulation and control. In this paper, we propose a novel approach for real-to-sim which tracks rigid objects in 3D from RGB-D images and infers physical properties of the objects. We use a differentiable physics simulation as state-transition model in an Extended Kalman Filter which can model contact and friction for arbitrary mesh-based shapes and in this way estimate physically plausible trajectories. We demonstrate that our approach can filter position, orientation, velocities, and concurrently can estimate the coefficient of friction of the objects. We analyse our approach on various sliding scenarios in synthetic image sequences of single objects and colliding objects. We also demonstrate and evaluate our approach on a real-world dataset. We will make our novel benchmark datasets publicly available to foster future research in this novel problem setting and comparison with our method.
</details>
<details>
<summary>摘要</summary>
physics-based understanding of object interactions from sensory observations is a crucial capability in augmented reality and robotics. it enables capturing the properties of a scene for simulation and control. in this paper, we propose a novel approach for real-to-sim which tracks rigid objects in 3d from rgb-d images and infers physical properties of the objects. we use a differentiable physics simulation as state-transition model in an extended kalman filter, which can model contact and friction for arbitrary mesh-based shapes and in this way estimate physically plausible trajectories. we demonstrate that our approach can filter position, orientation, velocities, and concurrently can estimate the coefficient of friction of the objects. we analyze our approach on various sliding scenarios in synthetic image sequences of single objects and colliding objects. we also demonstrate and evaluate our approach on a real-world dataset. we will make our novel benchmark datasets publicly available to foster future research in this novel problem setting and comparison with our method.Here's the translation in Traditional Chinese:物理基础的物体互动理解从感知观察是现实增强 reality 和机器人学中的重要能力。它可以捕捉场景的属性进行模拟和控制。在这篇文章中，我们提出了一种新的approach for real-to-sim，追踪3d中的固定物体从rgb-d图像中，并将物体的物理性能推断出来。我们使用了可微分的物理模拟作为状态转换模型，可以模拟物体之间的触摸和摩擦，并从而估算物体的运动轨迹。我们显示了我们的方法可以范围对象的位置、方向、速度和同时估算物体的摩擦系数。我们分析了我们的方法在单一物体和碰撞物体的滑动情况下的性能。我们还评估了我们的方法在实际世界数据集上的表现。我们将我们的新的benchmark数据集公开，以便未来的研究者可以在这个新的问题设定下进行比较和研究。
</details></li>
</ul>
<hr>
<h2 id="SGRec3D-Self-Supervised-3D-Scene-Graph-Learning-via-Object-Level-Scene-Reconstruction"><a href="#SGRec3D-Self-Supervised-3D-Scene-Graph-Learning-via-Object-Level-Scene-Reconstruction" class="headerlink" title="SGRec3D: Self-Supervised 3D Scene Graph Learning via Object-Level Scene Reconstruction"></a>SGRec3D: Self-Supervised 3D Scene Graph Learning via Object-Level Scene Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15702">http://arxiv.org/abs/2309.15702</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sebastian Koch, Pedro Hermosilla, Narunas Vaskevicius, Mirco Colosi, Timo Ropinski</li>
<li>for: 提高3D场景理解的能力</li>
<li>methods: 使用自我超vised pre-training方法SGRec3D来预处理3D场景图</li>
<li>results: 比起其他点云基于预训练方法，SGRec3D在3D场景图预测中提高了表达能力，得到了SOTA的性能，并且只需使用10%的标注数据进行精度调整即可以超过同类模型。<details>
<summary>Abstract</summary>
In the field of 3D scene understanding, 3D scene graphs have emerged as a new scene representation that combines geometric and semantic information about objects and their relationships. However, learning semantic 3D scene graphs in a fully supervised manner is inherently difficult as it requires not only object-level annotations but also relationship labels. While pre-training approaches have helped to boost the performance of many methods in various fields, pre-training for 3D scene graph prediction has received little attention. Furthermore, we find in this paper that classical contrastive point cloud-based pre-training approaches are ineffective for 3D scene graph learning. To this end, we present SGRec3D, a novel self-supervised pre-training method for 3D scene graph prediction. We propose to reconstruct the 3D input scene from a graph bottleneck as a pretext task. Pre-training SGRec3D does not require object relationship labels, making it possible to exploit large-scale 3D scene understanding datasets, which were off-limits for 3D scene graph learning before. Our experiments demonstrate that in contrast to recent point cloud-based pre-training approaches, our proposed pre-training improves the 3D scene graph prediction considerably, which results in SOTA performance, outperforming other 3D scene graph models by +10% on object prediction and +4% on relationship prediction. Additionally, we show that only using a small subset of 10% labeled data during fine-tuning is sufficient to outperform the same model without pre-training.
</details>
<details>
<summary>摘要</summary>
在三维场景理解领域，三维场景图（3D scene graph）已成为一种新的场景表示方法，可以同时包含物体的几何和 semantic信息。然而，在完全监督的情况下学习 semantic 3D scene graph 是非常困难的，因为需要不仅物体级别的注释，还需要关系标签。而在多种领域中，预训练方法已经有所帮助提高性能，但是针对 3D scene graph 的预训练却受到了少量的关注。此外，我们在这篇论文中发现，经典的对比点云预训练方法对于 3D scene graph 学习是无效的。为此，我们提出了 SGRec3D，一种新的自我监督预训练方法 для 3D scene graph 预测。我们提议使用场景图瓶颈来重建输入场景，作为一种预text任务。预训练 SGRec3D 不需要物体关系标签，因此可以利用大规模的 3D scene understanding 数据集，这些数据集在之前是 3D scene graph 学习中的不可达。我们的实验结果表明，相比最近的点云预训练方法，我们提posed的预训练方法可以大幅提高 3D scene graph 预测性能，达到了最新的标准性能，在物体预测上超过了 +10%，在关系预测上超过了 +4%。此外，我们还证明了只使用 10% 的标注数据进行细化调教是足够的，可以超过同样的模型无预训练。
</details></li>
</ul>
<hr>
<h2 id="Physics-Inspired-Hybrid-Attention-for-SAR-Target-Recognition"><a href="#Physics-Inspired-Hybrid-Attention-for-SAR-Target-Recognition" class="headerlink" title="Physics Inspired Hybrid Attention for SAR Target Recognition"></a>Physics Inspired Hybrid Attention for SAR Target Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15697">http://arxiv.org/abs/2309.15697</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xai4sar/piha">https://github.com/xai4sar/piha</a></li>
<li>paper_authors: Zhongling Huang, Chong Wu, Xiwen Yao, Zhicheng Zhao, Xiankai Huang, Junwei Han<br>for:The paper is focused on improving the performance and physical interpretability of SAR target recognition by integrating physical models and deep neural networks (DNNs).methods:The proposed method is based on a physics-inspired hybrid attention (PIHA) mechanism that leverages high-level semantics of physical information to activate and guide the feature group aware of local semantics of the target. The PIHA mechanism can be integrated into arbitrary DNNs without modifying the original architecture.results:The proposed method outperforms other state-of-the-art approaches in 12 test scenarios with the same ASC parameters. The experiments also show that PIHA is effective for different physical information and can be used to evaluate the model’s robustness and generalizability using the once-for-all (OFA) evaluation protocol.Here is the answer in Simplified Chinese text:for: 本 paper 的目的是提高 SAR 目标识别的性能和物理解释性，通过结合物理模型和深度神经网络 (DNNs)。methods: 提议的方法基于物理启发的混合注意力 (PIHA) 机制，利用高级别的物理信息来启动和指导target的本地semantics feature group。PIHA 机制可以与原始建筑不变的 DNNs 集成。results: 提议的方法在 12 个测试场景中超过了其他状态对照方法，并且在不同的物理信息下也表现出色。<details>
<summary>Abstract</summary>
There has been a recent emphasis on integrating physical models and deep neural networks (DNNs) for SAR target recognition, to improve performance and achieve a higher level of physical interpretability. The attributed scattering center (ASC) parameters garnered the most interest, being considered as additional input data or features for fusion in most methods. However, the performance greatly depends on the ASC optimization result, and the fusion strategy is not adaptable to different types of physical information. Meanwhile, the current evaluation scheme is inadequate to assess the model's robustness and generalizability. Thus, we propose a physics inspired hybrid attention (PIHA) mechanism and the once-for-all (OFA) evaluation protocol to address the above issues. PIHA leverages the high-level semantics of physical information to activate and guide the feature group aware of local semantics of target, so as to re-weight the feature importance based on knowledge prior. It is flexible and generally applicable to various physical models, and can be integrated into arbitrary DNNs without modifying the original architecture. The experiments involve a rigorous assessment using the proposed OFA, which entails training and validating a model on either sufficient or limited data and evaluating on multiple test sets with different data distributions. Our method outperforms other state-of-the-art approaches in 12 test scenarios with same ASC parameters. Moreover, we analyze the working mechanism of PIHA and evaluate various PIHA enabled DNNs. The experiments also show PIHA is effective for different physical information. The source code together with the adopted physical information is available at https://github.com/XAI4SAR.
</details>
<details>
<summary>摘要</summary>
Recently, there has been an emphasis on combining physical models and deep neural networks (DNNs) for target recognition in synthetic aperture radar (SAR) imaging, in order to improve performance and achieve better physical interpretability. The attributed scattering center (ASC) parameters have been widely used as additional input data or features for fusion in most methods. However, the performance of these methods heavily depends on the optimization result of ASC, and the fusion strategy is not adaptable to different types of physical information. Moreover, the current evaluation scheme is inadequate to assess the model's robustness and generalizability.To address these issues, we propose a physics-inspired hybrid attention (PIHA) mechanism and the once-for-all (OFA) evaluation protocol. PIHA leverages the high-level semantics of physical information to activate and guide the feature group aware of local semantics of the target, so as to re-weight the feature importance based on knowledge prior. This approach is flexible and generally applicable to various physical models, and can be integrated into arbitrary DNNs without modifying the original architecture.We conducted a series of experiments to evaluate the effectiveness of PIHA, using the proposed OFA evaluation protocol. The experiments involved training and validating a model on either sufficient or limited data, and evaluating its performance on multiple test sets with different data distributions. Our results show that PIHA outperforms other state-of-the-art approaches in 12 test scenarios with the same ASC parameters. Moreover, we analyzed the working mechanism of PIHA and evaluated various PIHA-enabled DNNs. The experiments also demonstrated that PIHA is effective for different physical information.The source code, together with the adopted physical information, is available at https://github.com/XAI4SAR.
</details></li>
</ul>
<hr>
<h2 id="A-Unified-View-of-Differentially-Private-Deep-Generative-Modeling"><a href="#A-Unified-View-of-Differentially-Private-Deep-Generative-Modeling" class="headerlink" title="A Unified View of Differentially Private Deep Generative Modeling"></a>A Unified View of Differentially Private Deep Generative Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15696">http://arxiv.org/abs/2309.15696</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dingfan Chen, Raouf Kerkouche, Mario Fritz</li>
<li>for: This paper aims to provide a unified view of various approaches for achieving privacy-preserving high-dimensional data generation through differentially private (DP) training of deep neural networks.</li>
<li>methods: The paper systematizes and jointly designs methods for different use cases, and discusses the strengths, limitations, and inherent correlations between different approaches.</li>
<li>results: The paper presents a novel unified view of privacy-preserving data generation methods, and provides potential paths forward for the field of DP data generation, with the aim of advancing privacy-preserving learning.<details>
<summary>Abstract</summary>
The availability of rich and vast data sources has greatly advanced machine learning applications in various domains. However, data with privacy concerns comes with stringent regulations that frequently prohibited data access and data sharing. Overcoming these obstacles in compliance with privacy considerations is key for technological progress in many real-world application scenarios that involve privacy sensitive data. Differentially private (DP) data publishing provides a compelling solution, where only a sanitized form of the data is publicly released, enabling privacy-preserving downstream analysis and reproducible research in sensitive domains. In recent years, various approaches have been proposed for achieving privacy-preserving high-dimensional data generation by private training on top of deep neural networks. In this paper, we present a novel unified view that systematizes these approaches. Our view provides a joint design space for systematically deriving methods that cater to different use cases. We then discuss the strengths, limitations, and inherent correlations between different approaches, aiming to shed light on crucial aspects and inspire future research. We conclude by presenting potential paths forward for the field of DP data generation, with the aim of steering the community toward making the next important steps in advancing privacy-preserving learning.
</details>
<details>
<summary>摘要</summary>
“由于丰富的数据源的可用性，机器学习应用在不同领域得到了很大的进步。然而，具有隐私问题的数据受到了严格的规定，这些规定 frequently prohibited data access 和 data sharing。为了遵循隐私考虑，在许多实际应用 scenario 中，技术进步是关键。具有隐私保证的数据发布（DP）提供了一个吸引人的解决方案，即仅发布了隐私检查的数据，允许隐私保证的下游分析和可重复性的研究。在过去几年，许多方法被提出供以实现隐私保证高维数据生成。在本文中，我们提出了一个新的统一的观点，它系统地探讨了不同的用案。我们的观点提供了一个共同的设计空间，可以系统地从数据生成中获得不同的方法。我们然后讨论了不同方法的优点、局限性和内在的相互关联性，以照明关键的问题和激励未来研究。我们结束时，提出了未来隐私保证数据生成领域的可能的进步之路，以导引社区做出下一步的进步。”
</details></li>
</ul>
<hr>
<h2 id="End-to-End-Streaming-Video-Temporal-Action-Segmentation-with-Reinforce-Learning"><a href="#End-to-End-Streaming-Video-Temporal-Action-Segmentation-with-Reinforce-Learning" class="headerlink" title="End-to-End Streaming Video Temporal Action Segmentation with Reinforce Learning"></a>End-to-End Streaming Video Temporal Action Segmentation with Reinforce Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15683">http://arxiv.org/abs/2309.15683</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Thinksky5124/SVTAS">https://github.com/Thinksky5124/SVTAS</a></li>
<li>paper_authors: Wujun Wen, Jinrong Zhang, Shenglan Liu, Yunheng Li, Qifeng Li, Lin Feng</li>
<li>for: 本研究的目的是提出一种可以实时应用于长视频中的动作分类任务，以扩展现有的动作识别模型的应用场景。</li>
<li>methods: 该研究提出了一种结合流处理和强化学习的末端视频动作时间分 segmentation方法（SVTAS-RL），可以将动作识别任务视为动作分类 clustering 任务，并使用强化学习来缓解不一致的优化目标和方向问题。</li>
<li>results: 经过广泛的实验，SVTAS-RL 模型在多个数据集上达到了与现有模型相当的竞争性性能，并在ultra-long video dataset EGTEA 上表现出了更大的优势，这表明该方法可以取代现有的 TAS 模型，并且 SVTAS-RL 更适合长视频 TAS。<details>
<summary>Abstract</summary>
Temporal Action Segmentation (TAS) from video is a kind of frame recognition task for long video with multiple action classes. As an video understanding task for long videos, current methods typically combine multi-modality action recognition models with temporal models to convert feature sequences to label sequences. This approach can only be applied to offline scenarios, which severely limits the TAS application. Therefore, this paper proposes an end-to-end Streaming Video Temporal Action Segmentation with Reinforce Learning (SVTAS-RL). The end-to-end SVTAS which regard TAS as an action segment clustering task can expand the application scenarios of TAS; and RL is used to alleviate the problem of inconsistent optimization objective and direction. Through extensive experiments, the SVTAS-RL model achieves a competitive performance to the state-of-the-art model of TAS on multiple datasets, and shows greater advantages on the ultra-long video dataset EGTEA. This indicates that our method can replace all current TAS models end-to-end and SVTAS-RL is more suitable for long video TAS. Code is availabel at https://github.com/Thinksky5124/SVTAS.
</details>
<details>
<summary>摘要</summary>
Temporal Action Segmentation (TAS) from video is a type of frame recognition task for long videos with multiple action classes. As a video understanding task for long videos, current methods typically combine multi-modality action recognition models with temporal models to convert feature sequences into label sequences. This approach can only be applied to offline scenarios, which severely limits the TAS application. Therefore, this paper proposes an end-to-end Streaming Video Temporal Action Segmentation with Reinforce Learning (SVTAS-RL). The end-to-end SVTAS, which treats TAS as an action segment clustering task, can expand the application scenarios of TAS; and RL is used to alleviate the problem of inconsistent optimization objectives and directions. Through extensive experiments, the SVTAS-RL model achieves a competitive performance to the state-of-the-art model of TAS on multiple datasets, and shows greater advantages on the ultra-long video dataset EGTEA. This indicates that our method can replace all current TAS models end-to-end, and SVTAS-RL is more suitable for long video TAS. Code is available at https://github.com/Thinksky5124/SVTAS.Here is the word-for-word translation of the text into Simplified Chinese:视频 temporal action segmentation (TAS) 是一种帧 recognition 任务，用于长视频中的多种动作类。当前方法通常将多模态动作识别模型与时间模型组合，将特征序列转换为标签序列。这种方法只适用于线上enario，很大限制 TAS 应用。因此，这篇论文提出了一种终端到终 Streaming Video Temporal Action Segmentation with Reinforce Learning (SVTAS-RL)。终端 SVTAS 将 TAS 视为动作段 clustering 任务，可以扩大 TAS 的应用场景; RL 用于缓解不一致的优化目标和方向问题。经过广泛的实验，SVTAS-RL 模型在多个 datasets 上达到了与当前 TAS 模型的竞争性性能，并在EGTEA 数据集上表现出更大的优势。这表示我们的方法可以替换所有当前 TAS 模型，并且 SVTAS-RL 更适合长视频 TAS。 Code 可以在 https://github.com/Thinksky5124/SVTAS 中获取。
</details></li>
</ul>
<hr>
<h2 id="SJTU-TMQA-A-quality-assessment-database-for-static-mesh-with-texture-map"><a href="#SJTU-TMQA-A-quality-assessment-database-for-static-mesh-with-texture-map" class="headerlink" title="SJTU-TMQA: A quality assessment database for static mesh with texture map"></a>SJTU-TMQA: A quality assessment database for static mesh with texture map</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15675">http://arxiv.org/abs/2309.15675</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bingyang Cui, Qi Yang, Kaifa Yang, Yiling Xu, Xiaozhong Xu, Shan Liu</li>
<li>for: 这篇论文主要是为了评估纹理化网格质量的研究。</li>
<li>methods: 论文使用了21个参考网格和945个扭曲样本来创建大规模的纹理化网格质量评估数据库（SJTU-TMQA），并通过主观实验获得了意见分数（MOS）。</li>
<li>results: 研究显示了不同类型的扭曲对人类印象的影响，并评估了13种当前最佳对象度量的可靠性。结果显示这些度量之间的相关性为0.6级，表明需要更有效的对象度量。 SJTU-TMQA数据库可以在<a target="_blank" rel="noopener" href="https://ccccby.github.io中下载./">https://ccccby.github.io中下载。</a><details>
<summary>Abstract</summary>
In recent years, static meshes with texture maps have become one of the most prevalent digital representations of 3D shapes in various applications, such as animation, gaming, medical imaging, and cultural heritage applications. However, little research has been done on the quality assessment of textured meshes, which hinders the development of quality-oriented applications, such as mesh compression and enhancement. In this paper, we create a large-scale textured mesh quality assessment database, namely SJTU-TMQA, which includes 21 reference meshes and 945 distorted samples. The meshes are rendered into processed video sequences and then conduct subjective experiments to obtain mean opinion scores (MOS). The diversity of content and accuracy of MOS has been shown to validate its heterogeneity and reliability. The impact of various types of distortion on human perception is demonstrated. 13 state-of-the-art objective metrics are evaluated on SJTU-TMQA. The results report the highest correlation of around 0.6, indicating the need for more effective objective metrics. The SJTU-TMQA is available at https://ccccby.github.io
</details>
<details>
<summary>摘要</summary>
在最近的几年中，静止的矩阵图形在各种应用中变得非常普遍，如动画、游戏、医疗影像和文化遗产应用。然而，对纹理矩阵质量的研究很少，这限制了质量导向的应用，如矩阵压缩和提高。在这篇论文中，我们创建了一个大规模的纹理矩阵质量评估数据库，即上海交通大学纹理矩阵质量评价数据库（SJTU-TMQA），包括21个参考矩阵和945个扭曲样本。这些矩阵通过渲染而生成的处理视频序列，然后通过主观实验获得mean opinion score（MOS）。我们所得到的多样性和准确性已经被证明，以 validate its heterogeneity and reliability。我们还展示了不同类型的扭曲对人类的感知具有多大的影响。13种当前的对象度量被评估在SJTU-TMQA上，结果显示其相关性达0.6， indicating the need for more effective objective metrics。SJTU-TMQA可以在https://ccccby.github.io 上获取。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Prompt-Learning-Addressing-Cross-Attention-Leakage-for-Text-Based-Image-Editing"><a href="#Dynamic-Prompt-Learning-Addressing-Cross-Attention-Leakage-for-Text-Based-Image-Editing" class="headerlink" title="Dynamic Prompt Learning: Addressing Cross-Attention Leakage for Text-Based Image Editing"></a>Dynamic Prompt Learning: Addressing Cross-Attention Leakage for Text-Based Image Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15664">http://arxiv.org/abs/2309.15664</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangkai930418/DPL">https://github.com/wangkai930418/DPL</a></li>
<li>paper_authors: Kai Wang, Fei Yang, Shiqi Yang, Muhammad Atif Butt, Joost van de Weijer</li>
<li>for: 这 paper 的目的是给用户提供精细的图像编辑功能，以便通过修改文本提示来控制生成的图像。</li>
<li>methods: 这 paper 使用了Diffusion模型，并提出了一种名为 Dynamic Prompt Learning（DPL）的新方法，以解决图像编辑时的偏差问题。</li>
<li>results:  compared to existing methods, DPL 可以准确地编辑图像中的特定对象，而不会影响其他图像区域。这 paper 的实验结果表明，DPL 可以在多种图像场景中获得superior的结果， both quantitatively (CLIP score, Structure-Dist) 和 qualitatively (用户评价).<details>
<summary>Abstract</summary>
Large-scale text-to-image generative models have been a ground-breaking development in generative AI, with diffusion models showing their astounding ability to synthesize convincing images following an input text prompt. The goal of image editing research is to give users control over the generated images by modifying the text prompt. Current image editing techniques are susceptible to unintended modifications of regions outside the targeted area, such as on the background or on distractor objects which have some semantic or visual relationship with the targeted object. According to our experimental findings, inaccurate cross-attention maps are at the root of this problem. Based on this observation, we propose Dynamic Prompt Learning (DPL) to force cross-attention maps to focus on correct noun words in the text prompt. By updating the dynamic tokens for nouns in the textual input with the proposed leakage repairment losses, we achieve fine-grained image editing over particular objects while preventing undesired changes to other image regions. Our method DPL, based on the publicly available Stable Diffusion, is extensively evaluated on a wide range of images, and consistently obtains superior results both quantitatively (CLIP score, Structure-Dist) and qualitatively (on user-evaluation). We show improved prompt editing results for Word-Swap, Prompt Refinement, and Attention Re-weighting, especially for complex multi-object scenes.
</details>
<details>
<summary>摘要</summary>
大规模文本到图像生成模型已经是生成智能的一个重要发展，扩散模型表现出了从文本输入提示synthesize出实际的图像的惊人能力。图像编辑研究的目标是给用户控制生成图像的文本提示。现有的图像编辑技术容易导致不必要地修改图像背景或 Distractor 对象上的元素，这会导致图像 editing 失败。根据我们的实验结果，不准确的跨注意力地图是这个问题的根本原因。基于这一观察，我们提出了动态提示学习（DPL），强制跨注意力地图专注于正确的名词在文本输入中。通过更新文本输入中的动态token для名词，我们实现了细化的图像编辑，并避免了不必要地修改其他图像区域。我们的方法DPL，基于公共可用的稳定扩散，广泛评估了多种图像，并 consistently 获得了较好的数值（CLIP 分数、结构-分布）和质量（用户评估）评估结果。我们展示了对 Word-Swap、提示精度和注意力重新分配的提示编辑结果的改进，特别是在复杂多对象场景中。
</details></li>
</ul>
<hr>
<h2 id="Human-Kinematics-inspired-Skeleton-based-Video-Anomaly-Detection"><a href="#Human-Kinematics-inspired-Skeleton-based-Video-Anomaly-Detection" class="headerlink" title="Human Kinematics-inspired Skeleton-based Video Anomaly Detection"></a>Human Kinematics-inspired Skeleton-based Video Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15662">http://arxiv.org/abs/2309.15662</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/XiaoJian923/Kinematics-VAD">https://github.com/XiaoJian923/Kinematics-VAD</a></li>
<li>paper_authors: Jian Xiao, Tianyuan Liu, Genlin Ji</li>
<li>for: 本研究旨在探讨人体异常检测视频中的新方法，以及人体动态特征如何用于检测异常。</li>
<li>methods: 本研究提出了一种新的方法 called HKVAD (Human Kinematic-inspired Video Anomaly Detection)，它利用人体动态特征来检测视频异常。该方法首先利用人体三维姿态数据，特别是跑步姿势、脚部位置和颈部位置的动态特征，然后使用流变模型来估算概率并检测异常。</li>
<li>results: 根据实验结果，HKVAD方法在两个公共数据集上（ShanghaiTech和UBnormal）获得了良好的结果，而且只需使用了 minimal computational resources。这表明该方法的有效性和潜在性。<details>
<summary>Abstract</summary>
Previous approaches to detecting human anomalies in videos have typically relied on implicit modeling by directly applying the model to video or skeleton data, potentially resulting in inaccurate modeling of motion information. In this paper, we conduct an exploratory study and introduce a new idea called HKVAD (Human Kinematic-inspired Video Anomaly Detection) for video anomaly detection, which involves the explicit use of human kinematic features to detect anomalies. To validate the effectiveness and potential of this perspective, we propose a pilot method that leverages the kinematic features of the skeleton pose, with a specific focus on the walking stride, skeleton displacement at feet level, and neck level. Following this, the method employs a normalizing flow model to estimate density and detect anomalies based on the estimated density. Based on the number of kinematic features used, we have devised three straightforward variant methods and conducted experiments on two highly challenging public datasets, ShanghaiTech and UBnormal. Our method achieves good results with minimal computational resources, validating its effectiveness and potential.
</details>
<details>
<summary>摘要</summary>
前一些视频人异常检测方法通常是通过直接应用模型到视频或skeleton数据来进行隐式模型，这可能导致动作信息的不准确模型化。在这篇论文中，我们进行了一项探索性研究，并提出了一种新的思路called HKVAD（人体骨征驱动的视频异常检测），这种方法利用人体骨征特征来检测异常。为了证明这种视角的有效性和潜力，我们提议了一种起点方法，该方法利用人体skeleton姿势中的步伐、脚部偏移量和 neck 水平位置的骨征特征。接着，该方法使用了一种归一化流模型来估计密度并检测异常基于估计的密度。根据使用的骨征特征数量，我们设计了三种简单的变体方法，并在两个非常困难的公共数据集上进行了实验，即ShanghaiTech 和 UBnormal。我们的方法在计算资源不多的情况下达到了良好的结果，这 validate了其有效性和潜力。
</details></li>
</ul>
<hr>
<h2 id="FRS-Nets-Fourier-Parameterized-Rotation-and-Scale-Equivariant-Networks-for-Retinal-Vessel-Segmentation"><a href="#FRS-Nets-Fourier-Parameterized-Rotation-and-Scale-Equivariant-Networks-for-Retinal-Vessel-Segmentation" class="headerlink" title="FRS-Nets: Fourier Parameterized Rotation and Scale Equivariant Networks for Retinal Vessel Segmentation"></a>FRS-Nets: Fourier Parameterized Rotation and Scale Equivariant Networks for Retinal Vessel Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15638">http://arxiv.org/abs/2309.15638</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihong Sun, Qi Xie, Deyu Meng<br>for: 这篇论文主要目的是提出一种新的卷积操作符（FRS-Conv），以提高卷积神经网（CNNs）在血管分类中的精度和一致性。methods: 这篇论文使用了一种新的参数化方案，允许卷积 filters 进行高精度的旋转和缩放变换。它还提出了旋转和缩放对卷积映射的等调性数学表述。最后，它将这些表述与传统卷积映射相结合，实现了FRS-Conv。results: 这篇论文的实验结果显示，使用FRS-Conv可以实现血管分类中的高精度和一致性。它在三个公共数据集上进行了广泛的比较实验，包括内集和跨集设定。与相比方法相比，FRS-Nets 仅需13.9%的参数，却能够实现顶尖的性能，并且具有优秀的一致性和丰富的应用潜力。<details>
<summary>Abstract</summary>
With translation equivariance, convolution neural networks (CNNs) have achieved great success in retinal vessel segmentation. However, some other symmetries of the vascular morphology are not characterized by CNNs, such as rotation and scale symmetries. To embed more equivariance into CNNs and achieve the accuracy requirement for retinal vessel segmentation, we construct a novel convolution operator (FRS-Conv), which is Fourier parameterized and equivariant to rotation and scaling. Specifically, we first adopt a new parameterization scheme, which enables convolutional filters to arbitrarily perform transformations with high accuracy. Secondly, we derive the formulations for the rotation and scale equivariant convolution mapping. Finally, we construct FRS-Conv following the proposed formulations and replace the traditional convolution filters in U-Net and Iter-Net with FRS-Conv (FRS-Nets). We faithfully reproduce all compared methods and conduct comprehensive experiments on three public datasets under both in-dataset and cross-dataset settings. With merely 13.9% parameters of corresponding baselines, FRS-Nets have achieved state-of-the-art performance and significantly outperform all compared methods. It demonstrates the remarkable accuracy, generalization, and clinical application potential of FRS-Nets.
</details>
<details>
<summary>摘要</summary>
使用翻译等价性，图像卷积神经网络（CNN）在血管轮廓分割方面取得了很大的成功。然而，图像中的其他同质性，如旋转和缩放同质性，并没有被CNN表征出来。为了嵌入更多的等价性到CNN中，并达到血管轮廓分割的精度要求，我们构建了一种新型的卷积算子（FRS-Conv），该算子是快 Fourier 参数化的和旋转和缩放等价的。 Specifically，我们首先采用一种新的参数化方案，允许卷积滤波器通过高精度执行变换。其次，我们 derivate了旋转和缩放等价的卷积映射表达式。最后，我们根据提出的表达式构建FRS-Conv，并将传统卷积滤波器在U-Net和Iter-Net中取代。我们忠实地复制了所有相关的方法，并在三个公共数据集上进行了广泛的实验，包括在集合和跨集合设置下。只有13.9%的参数，FRS-Nets已经达到了同类方法的状态对比较好的性能，并显著超过了所有相关方法。这表明FRS-Nets具有出色的精度、普遍性和临床应用潜力。
</details></li>
</ul>
<hr>
<h2 id="Position-and-Orientation-Aware-One-Shot-Learning-for-Medical-Action-Recognition-from-Signal-Data"><a href="#Position-and-Orientation-Aware-One-Shot-Learning-for-Medical-Action-Recognition-from-Signal-Data" class="headerlink" title="Position and Orientation-Aware One-Shot Learning for Medical Action Recognition from Signal Data"></a>Position and Orientation-Aware One-Shot Learning for Medical Action Recognition from Signal Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15635">http://arxiv.org/abs/2309.15635</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leiyu Xie, Yuxing Yang, Zeyu Fu, Syed Mohsen Naqvi</li>
<li>for: 这篇论文旨在提出一个基于信号数据的医疗动作识别框架，以提高医疗动作识别的精度和可靠性。</li>
<li>methods: 该框架包括两个阶段，每个阶段含有信号生成（SIG）、跨注意（CsA）、时间截然变（DTW）模组，以及具有隐私保证的位置和方向特征的资讯融合。 SIG 方法旨在将骨架资料转换为隐私保证的特征，以供训练。 CsA 模组则是为了帮助网络优化医疗动作识别，并对人体部位进行注意力调节，以解决类似的医疗动作相关问题。 DTW 模组则是为了将时间汇入调整，以提高模型性能。</li>
<li>results: 实验结果显示，该提案的方法可以在NTU RGB+D 60、NTU RGB+D 120和PKU-MMD 等三个常用和知名的数据集上实现医疗动作识别的高精度和可靠性，并在不同的数据分配情况下协助优化医疗动作识别的性能，比如NTU RGB+D 60 的通用数据分配下提高了2.7%、NTU RGB+D 120 的通用数据分配下提高了6.2%、PKU-MMD 的通用数据分配下提高了4.1%。<details>
<summary>Abstract</summary>
In this work, we propose a position and orientation-aware one-shot learning framework for medical action recognition from signal data. The proposed framework comprises two stages and each stage includes signal-level image generation (SIG), cross-attention (CsA), dynamic time warping (DTW) modules and the information fusion between the proposed privacy-preserved position and orientation features. The proposed SIG method aims to transform the raw skeleton data into privacy-preserved features for training. The CsA module is developed to guide the network in reducing medical action recognition bias and more focusing on important human body parts for each specific action, aimed at addressing similar medical action related issues. Moreover, the DTW module is employed to minimize temporal mismatching between instances and further improve model performance. Furthermore, the proposed privacy-preserved orientation-level features are utilized to assist the position-level features in both of the two stages for enhancing medical action recognition performance. Extensive experimental results on the widely-used and well-known NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD datasets all demonstrate the effectiveness of the proposed method, which outperforms the other state-of-the-art methods with general dataset partitioning by 2.7%, 6.2% and 4.1%, respectively.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们提出了一个位置和方向意识到一步学习框架 для医疗动作识别从信号数据。我们的框架包括两个阶段，每个阶段包括信号水平图生成（SIG）、交叉注意（CsA）、动态时间滤波（DTW）模块以及信号水平和方向级别特征的信息融合。我们的SIG方法旨在将原始骨架数据转换成隐私保护的特征进行训练。CsA模块是为了帮助网络减少医疗动作识别偏见，更关注每个特定动作中人体重要部分，以解决类似的医疗动作相关问题。此外，DTW模块用于最小化时间匹配错误，以提高模型性能。此外，我们的隐私保护方向级别特征被利用以帮助位置级别特征在两个阶段中提高医疗动作识别性能。我们的实验结果表明，我们的方法在 widely 使用和知名的 NTU RGB+D 60、NTU RGB+D 120 和 PKU-MMD 数据集上均达到了最高效果，与其他状态对比方法的总体数据分区优势为2.7%、6.2%和4.1%，分别。
</details></li>
</ul>
<hr>
<h2 id="Neuromorphic-Imaging-and-Classification-with-Graph-Learning"><a href="#Neuromorphic-Imaging-and-Classification-with-Graph-Learning" class="headerlink" title="Neuromorphic Imaging and Classification with Graph Learning"></a>Neuromorphic Imaging and Classification with Graph Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15627">http://arxiv.org/abs/2309.15627</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pei Zhang, Chutian Wang, Edmund Y. Lam</li>
<li>for: 该论文旨在开发一种基于神经元模型的图像摄像头，以便在各种EXTREME的照明条件下捕捉动态场景，并且减少运动模糊和提高细节表示。</li>
<li>methods: 该论文使用图像摄像头异步记录像素亮度变化，并生成稀热事件流。然后，使用图形变换器处理这些事件数据，以实现精准的神经元分类。</li>
<li>results: 对比传统方法，该论文的方法能够在具有限制的计算资源和事件数量的实际场景中，提供更好的结果，并且在EXTREME照明条件下捕捉动态场景中减少运动模糊和提高细节表示。<details>
<summary>Abstract</summary>
Bio-inspired neuromorphic cameras asynchronously record pixel brightness changes and generate sparse event streams. They can capture dynamic scenes with little motion blur and more details in extreme illumination conditions. Due to the multidimensional address-event structure, most existing vision algorithms cannot properly handle asynchronous event streams. While several event representations and processing methods have been developed to address such an issue, they are typically driven by a large number of events, leading to substantial overheads in runtime and memory. In this paper, we propose a new graph representation of the event data and couple it with a Graph Transformer to perform accurate neuromorphic classification. Extensive experiments show that our approach leads to better results and excels at the challenging realistic situations where only a small number of events and limited computational resources are available, paving the way for neuromorphic applications embedded into mobile facilities.
</details>
<details>
<summary>摘要</summary>
生物启发 neuromorphic 摄像头异步记录像素亮度变化，生成稀疏事件流。它们可以捕捉动态场景，具有少量运动模糊和更多细节在极端照明条件下。由于多维度地址事件结构，大多数现有视觉算法无法正确处理异步事件流。虽然一些事件表示和处理方法已经开发出来解决这个问题，但它们通常受到大量事件数量的限制，导致运行时间和内存占用增加很多。在这篇论文中，我们提出一种新的图表示法，将事件数据表示为图，并与图变换器结合，实现高准确的 neuromorphic 分类。广泛的实验表明，我们的方法在实际情况下表现更好，能够在只有少量事件和有限的计算资源的情况下取得更好的结果，为 neuromorphic 应用在移动设备中铺平道路。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Topology-for-Domain-Adaptive-Road-Segmentation-in-Satellite-and-Aerial-Imagery"><a href="#Leveraging-Topology-for-Domain-Adaptive-Road-Segmentation-in-Satellite-and-Aerial-Imagery" class="headerlink" title="Leveraging Topology for Domain Adaptive Road Segmentation in Satellite and Aerial Imagery"></a>Leveraging Topology for Domain Adaptive Road Segmentation in Satellite and Aerial Imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15625">http://arxiv.org/abs/2309.15625</a></li>
<li>repo_url: None</li>
<li>paper_authors: Javed Iqbal, Aliza Masood, Waqas Sultani, Mohsen Ali</li>
<li>for: 本研究旨在提高遥感图像中道路分割的精度和一致性，以满足自动驾驶、城市规划和可持续发展等实际应用。</li>
<li>methods: 本研究提出了一种基于Topology的无监督领域适应方法，通过预测道路skeleton来强制道路分割预测和skeleton预测具有同 topological结构的约束。</li>
<li>results: 对 SpaceNet 和 DeepGlobe 数据集进行了广泛的实验，并证明了提出的方法在与现有状态的方法进行比较时具有显著的优势，具体的比较结果为：SpaceNet 到 DeepGlobe 的适应性提高6.6%, 6.7%, 9.8%。<details>
<summary>Abstract</summary>
Getting precise aspects of road through segmentation from remote sensing imagery is useful for many real-world applications such as autonomous vehicles, urban development and planning, and achieving sustainable development goals. Roads are only a small part of the image, and their appearance, type, width, elevation, directions, etc. exhibit large variations across geographical areas. Furthermore, due to differences in urbanization styles, planning, and the natural environments; regions along the roads vary significantly. Due to these variations among the train and test domains, the road segmentation algorithms fail to generalize to new geographical locations. Unlike the generic domain alignment scenarios, road segmentation has no scene structure, and generic domain adaptation methods are unable to enforce topological properties like continuity, connectivity, smoothness, etc., thus resulting in degraded domain alignment. In this work, we propose a topology-aware unsupervised domain adaptation approach for road segmentation in remote sensing imagery. Specifically, we predict road skeleton, an auxiliary task to impose the topological constraints. To enforce consistent predictions of road and skeleton, especially in the unlabeled target domain, the conformity loss is defined across the skeleton prediction head and the road-segmentation head. Furthermore, for self-training, we filter out the noisy pseudo-labels by using a connectivity-based pseudo-labels refinement strategy, on both road and skeleton segmentation heads, thus avoiding holes and discontinuities. Extensive experiments on the benchmark datasets show the effectiveness of the proposed approach compared to existing state-of-the-art methods. Specifically, for SpaceNet to DeepGlobe adaptation, the proposed approach outperforms the competing methods by a minimum margin of 6.6%, 6.7%, and 9.8% in IoU, F1-score, and APLS, respectively.
</details>
<details>
<summary>摘要</summary>
getting precise aspects of road through segmentation from remote sensing imagery is useful for many real-world applications such as autonomous vehicles, urban development and planning, and achieving sustainable development goals. roads are only a small part of the image, and their appearance, type, width, elevation, directions, etc. exhibit large variations across geographical areas. Furthermore, due to differences in urbanization styles, planning, and the natural environments; regions along the roads vary significantly. due to these variations among the train and test domains, the road segmentation algorithms fail to generalize to new geographical locations. unlike the generic domain alignment scenarios, road segmentation has no scene structure, and generic domain adaptation methods are unable to enforce topological properties like continuity, connectivity, smoothness, etc., thus resulting in degraded domain alignment. in this work, we propose a topology-aware unsupervised domain adaptation approach for road segmentation in remote sensing imagery. specifically, we predict road skeleton, an auxiliary task to impose the topological constraints. to enforce consistent predictions of road and skeleton, especially in the unlabeled target domain, the conformity loss is defined across the skeleton prediction head and the road-segmentation head. Furthermore, for self-training, we filter out the noisy pseudo-labels by using a connectivity-based pseudo-labels refinement strategy, on both road and skeleton segmentation heads, thus avoiding holes and discontinuities. extensive experiments on the benchmark datasets show the effectiveness of the proposed approach compared to existing state-of-the-art methods. specifically, for spacenet to deepglobe adaptation, the proposed approach outperforms the competing methods by a minimum margin of 6.6%, 6.7%, and 9.8% in iou, f1-score, and apls, respectively.
</details></li>
</ul>
<hr>
<h2 id="NoSENSE-Learned-unrolled-cardiac-MRI-reconstruction-without-explicit-sensitivity-maps"><a href="#NoSENSE-Learned-unrolled-cardiac-MRI-reconstruction-without-explicit-sensitivity-maps" class="headerlink" title="NoSENSE: Learned unrolled cardiac MRI reconstruction without explicit sensitivity maps"></a>NoSENSE: Learned unrolled cardiac MRI reconstruction without explicit sensitivity maps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15608">http://arxiv.org/abs/2309.15608</a></li>
<li>repo_url: None</li>
<li>paper_authors: Felix Frederik Zimmermann, Andreas Kofler</li>
<li>for: 这个论文旨在提出一种基于深度卷积神经网络的加速心脏MRI多接收器磁共振图像重建方法，以避免许多现有的学习MR图像重建技术中的磁共振敏感度地图（CSM）估计。</li>
<li>methods: 该方法包括一系列新的学习图像和k空间块，以及共振磁场信息的共享和特征 Wisdom （FiLM）块，以及磁共振数据一致（DC）块。</li>
<li>results: 该方法在MICCAI STACOM CMRxRecon挑战中的笔轨和映射轨验证领导表中 achieved PSNR值为34.89和35.56，SSIM值为0.920和0.942，在4个不同的队伍中排名第4。<details>
<summary>Abstract</summary>
We present a novel learned image reconstruction method for accelerated cardiac MRI with multiple receiver coils based on deep convolutional neural networks (CNNs) and algorithm unrolling. In contrast to many existing learned MR image reconstruction techniques that necessitate coil-sensitivity map (CSM) estimation as a distinct network component, our proposed approach avoids explicit CSM estimation. Instead, it implicitly captures and learns to exploit the inter-coil relationships of the images. Our method consists of a series of novel learned image and k-space blocks with shared latent information and adaptation to the acquisition parameters by feature-wise modulation (FiLM), as well as coil-wise data-consistency (DC) blocks.   Our method achieved PSNR values of 34.89 and 35.56 and SSIM values of 0.920 and 0.942 in the cine track and mapping track validation leaderboard of the MICCAI STACOM CMRxRecon Challenge, respectively, ranking 4th among different teams at the time of writing.   Code will be made available at https://github.com/fzimmermann89/CMRxRecon
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的学习Image重建方法，用于加速心脏MRI，基于深度卷积神经网络（CNNs）和算法膨胀。与许多现有的学习MR Image重建技术不同，我们的提议方法不需要显式的磁共振敏感度地图（CSM）估计。而是通过隐式地捕捉和利用图像间的相互关系，来避免直接估计CSM。我们的方法包括一系列新的学习图像和k空间块，共享缓存信息和适应到获取参数的特征 wise modulation（FiLM），以及磁共振数据一致（DC）块。我们的方法在MICCAI STACOM CMRxRecon Challenge的碰撞轨迹和映射轨迹验证领导борда上 achieved PSNR值为34.89和35.56，SSIM值为0.920和0.942，在不同团队中排名第四。代码将在https://github.com/fzimmermann89/CMRxRecon上提供。
</details></li>
</ul>
<hr>
<h2 id="PolarNet-3D-Point-Clouds-for-Language-Guided-Robotic-Manipulation"><a href="#PolarNet-3D-Point-Clouds-for-Language-Guided-Robotic-Manipulation" class="headerlink" title="PolarNet: 3D Point Clouds for Language-Guided Robotic Manipulation"></a>PolarNet: 3D Point Clouds for Language-Guided Robotic Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15596">http://arxiv.org/abs/2309.15596</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shizhe Chen, Ricardo Garcia, Cordelia Schmid, Ivan Laptev</li>
<li>for: 本研究旨在提高机器人对自然语言指令的理解和执行 manipulation 任务能力。</li>
<li>methods: 提议一种基于 3D 点云的策略PolarNet，通过特制的点云输入、高效点云编码器和多模态 transformer 来学习 3D 点云表示并与语言指令集成 для行为预测。</li>
<li>results: PolarNet 在RLBench  benchmark 上展现出了高效和数据效果，与当前状态的 2D 和 3D 方法相比，在单任务和多任务学习中均有出色的表现。在真实的机器人上也取得了可喜的结果。<details>
<summary>Abstract</summary>
The ability for robots to comprehend and execute manipulation tasks based on natural language instructions is a long-term goal in robotics. The dominant approaches for language-guided manipulation use 2D image representations, which face difficulties in combining multi-view cameras and inferring precise 3D positions and relationships. To address these limitations, we propose a 3D point cloud based policy called PolarNet for language-guided manipulation. It leverages carefully designed point cloud inputs, efficient point cloud encoders, and multimodal transformers to learn 3D point cloud representations and integrate them with language instructions for action prediction. PolarNet is shown to be effective and data efficient in a variety of experiments conducted on the RLBench benchmark. It outperforms state-of-the-art 2D and 3D approaches in both single-task and multi-task learning. It also achieves promising results on a real robot.
</details>
<details>
<summary>摘要</summary>
“机器人理解和执行基于自然语言指令的 manipulate 任务是机器人学的长期目标。现有主流方法 для语言导向 manipulate 使用 2D 图像表示，它们面临着组合多视图摄像头和推断精确的 3D 位置和关系的困难。为解决这些限制，我们提出了一种基于 3D 点云的策略called PolarNet，用于语言导向 manipulate。它利用了特制的点云输入、高效的点云编码器和多模态转换器来学习 3D 点云表示并与语言指令集成以进行动作预测。PolarNet 在RLBench benchmark 上进行了多种实验，并在单任务和多任务学习中超越了当前状态的 2D 和 3D 方法。它还在真实的机器人上实现了可靠的结果。”
</details></li>
</ul>
<hr>
<h2 id="Domain-generalization-across-tumor-types-laboratories-and-species-–-insights-from-the-2022-edition-of-the-Mitosis-Domain-Generalization-Challenge"><a href="#Domain-generalization-across-tumor-types-laboratories-and-species-–-insights-from-the-2022-edition-of-the-Mitosis-Domain-Generalization-Challenge" class="headerlink" title="Domain generalization across tumor types, laboratories, and species – insights from the 2022 edition of the Mitosis Domain Generalization Challenge"></a>Domain generalization across tumor types, laboratories, and species – insights from the 2022 edition of the Mitosis Domain Generalization Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15589">http://arxiv.org/abs/2309.15589</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marc Aubreville, Nikolas Stathonikos, Taryn A. Donovan, Robert Klopfleisch, Jonathan Ganz, Jonas Ammeling, Frauke Wilm, Mitko Veta, Samir Jabari, Markus Eckstein, Jonas Annuscheit, Christian Krumnow, Engin Bozaba, Sercan Cayir, Hongyan Gu, Xiang ‘Anthony’ Chen, Mostafa Jahanifar, Adam Shephard, Satoshi Kondo, Satoshi Kasai, Sujatha Kotte, VG Saipradeep, Maxime W. Lafarge, Viktor H. Koelzer, Ziyue Wang, Yongbing Zhang, Sen Yang, Xiyue Wang, Katharina Breininger, Christof A. Bertram</li>
<li>For: The paper is focused on the challenge of recognizing mitotic figures in histologic tumor specimens, which is crucial for patient outcome assessment.* Methods: The paper describes the 2022 challenge on Mitosis Domain Generalization (MIDOG 2022), which provided annotated histologic tumor images from six different domains and evaluated the algorithmic approaches for mitotic figure detection from nine challenge participants on ten independent domains.* Results: The top-performing team achieved an $F_1$ score of 0.764, demonstrating that domain generalization across various tumor domains is possible with today’s deep learning-based recognition pipelines. However, all methods resulted in reduced recall scores compared to the immunohistochemistry-assisted reference standard, with only minor changes in the ranking of participants.Here are the three points in Simplified Chinese text:* For: 这篇论文关注了 histologic tumor specimen 中 mitotic figure 的识别问题，这对患者结果评估非常重要。* Methods: 论文描述了2022年 Mitosis Domain Generalization (MIDOG 2022) 挑战，该挑战提供了六个不同领域的注意力束教学图像，并对 nine 个挑战参与者的算法方法进行了在 ten 个独立领域上的评估。* Results: 最高级别的团队实现了 $F_1$  score 0.764，表明今天的深度学习基于的识别管道中的领域泛化是可能的。然而，所有方法都导致了与 immunohistochemistry 助记标准相比的减少的回归得分，仅有小量的排名变化。<details>
<summary>Abstract</summary>
Recognition of mitotic figures in histologic tumor specimens is highly relevant to patient outcome assessment. This task is challenging for algorithms and human experts alike, with deterioration of algorithmic performance under shifts in image representations. Considerable covariate shifts occur when assessment is performed on different tumor types, images are acquired using different digitization devices, or specimens are produced in different laboratories. This observation motivated the inception of the 2022 challenge on MItosis Domain Generalization (MIDOG 2022). The challenge provided annotated histologic tumor images from six different domains and evaluated the algorithmic approaches for mitotic figure detection provided by nine challenge participants on ten independent domains. Ground truth for mitotic figure detection was established in two ways: a three-expert consensus and an independent, immunohistochemistry-assisted set of labels. This work represents an overview of the challenge tasks, the algorithmic strategies employed by the participants, and potential factors contributing to their success. With an $F_1$ score of 0.764 for the top-performing team, we summarize that domain generalization across various tumor domains is possible with today's deep learning-based recognition pipelines. When assessed against the immunohistochemistry-assisted reference standard, all methods resulted in reduced recall scores, but with only minor changes in the order of participants in the ranking.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>识别mitotic图像在癌症组织样本中的涉及非常重要，对患者结果评估非常重要。这项任务对算法和人工专家来说都是挑战，随着图像表示的变化，算法性能会下降。在不同的癌症类型、不同的数字化设备获取图像以及不同的实验室生产的样本中， covariate shift会出现很大。这些 Observation 驱动了2022年的 Mitosis Domain Generalization（MIDOG 2022）挑战。挑战提供了六个不同领域的癌症组织样本，并评估了参与挑战的九支算法在十个独立领域上的方法。 truth 的确定方法包括三位专家一致和独立的免疫抗体辅助标注。本文将介绍挑战任务、参与者所采用的算法策略以及成功的因素。与 F1  score 为 0.764 的top Performing 团队，我们总结了：今天的深度学习基于的识别管道可以在不同的癌症领域进行预测Domain generalization。在与免疫抗体辅助标注referenced标准进行评估时，所有方法均出现了减少回归分数，但只有一些参与者在排名中的顺序发生了小幅变化。
</details></li>
</ul>
<hr>
<h2 id="LivDet2023-–-Fingerprint-Liveness-Detection-Competition-Advancing-Generalization"><a href="#LivDet2023-–-Fingerprint-Liveness-Detection-Competition-Advancing-Generalization" class="headerlink" title="LivDet2023 – Fingerprint Liveness Detection Competition: Advancing Generalization"></a>LivDet2023 – Fingerprint Liveness Detection Competition: Advancing Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15578">http://arxiv.org/abs/2309.15578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Micheletto, Roberto Casula, Giulia Orrù, Simone Carta, Sara Concas, Simone Maurizio La Cava, Julian Fierrez, Gian Luca Marcialis</li>
<li>for: 本研究旨在评估指纹识别系统中的生物特征检测技术，以提高系统的安全性和可靠性。</li>
<li>methods: 本研究使用了LivDet2023比赛提供的指纹识别数据集，并采用了多种生物特征检测技术，如指纹图像分析和Machine Learning算法，来检测指纹是否为真实的。</li>
<li>results: 研究发现，使用生物特征检测技术可以准确地检测指纹是否为真实的，并且可以在不同的环境和 Condition下提供高度的检测精度。<details>
<summary>Abstract</summary>
The International Fingerprint Liveness Detection Competition (LivDet) is a biennial event that invites academic and industry participants to prove their advancements in Fingerprint Presentation Attack Detection (PAD). This edition, LivDet2023, proposed two challenges, Liveness Detection in Action and Fingerprint Representation, to evaluate the efficacy of PAD embedded in verification systems and the effectiveness and compactness of feature sets. A third, hidden challenge is the inclusion of two subsets in the training set whose sensor information is unknown, testing participants ability to generalize their models. Only bona fide fingerprint samples were provided to participants, and the competition reports and assesses the performance of their algorithms suffering from this limitation in data availability.
</details>
<details>
<summary>摘要</summary>
国际生物指纹生活检测竞赛（LivDet）是一项每两年一度的活动，邀请学术和业界参与者展示他们在指纹攻击检测（PAD）领域的进步。本届LivDet2023中提出了两个挑战，生活检测在动作中和指纹表示，以评估参与者提供的验证系统中的PAD效果和特征集的效率和 компакт性。而隐藏的第三个挑战是在训练集中包含两个子集的感知信息不明确，测试参与者的模型是否能够泛化。只有真实的指纹样本被提供给参与者，竞赛报告和评估参与者的算法受到这种数据可用性的限制。
</details></li>
</ul>
<hr>
<h2 id="Learning-Spatial-Temporal-Regularized-Tensor-Sparse-RPCA-for-Background-Subtraction"><a href="#Learning-Spatial-Temporal-Regularized-Tensor-Sparse-RPCA-for-Background-Subtraction" class="headerlink" title="Learning Spatial-Temporal Regularized Tensor Sparse RPCA for Background Subtraction"></a>Learning Spatial-Temporal Regularized Tensor Sparse RPCA for Background Subtraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15576">http://arxiv.org/abs/2309.15576</a></li>
<li>repo_url: None</li>
<li>paper_authors: Basit Alawode, Sajid Javed</li>
<li>for: 这个论文的目的是提出一种基于tensor robust principal component analysis的准确背景 subtractor，用于解决视觉Computer Vision中的背景 subtractor问题。</li>
<li>methods: 该论文使用的方法包括：Robust principal component analysis（RPCA）、tensor RPCA、spatial-temporal regularized tensor sparse RPCA、batch和online-based optimization方法。</li>
<li>results: 该论文的实验结果表明，提出的方法在六个公共的背景 subtractor数据集上显示出比较出色的性能，与一些现有的方法相比。<details>
<summary>Abstract</summary>
Video background subtraction is one of the fundamental problems in computer vision that aims to segment all moving objects. Robust principal component analysis has been identified as a promising unsupervised paradigm for background subtraction tasks in the last decade thanks to its competitive performance in a number of benchmark datasets. Tensor robust principal component analysis variations have improved background subtraction performance further. However, because moving object pixels in the sparse component are treated independently and do not have to adhere to spatial-temporal structured-sparsity constraints, performance is reduced for sequences with dynamic backgrounds, camouflaged, and camera jitter problems. In this work, we present a spatial-temporal regularized tensor sparse RPCA algorithm for precise background subtraction. Within the sparse component, we impose spatial-temporal regularizations in the form of normalized graph-Laplacian matrices. To do this, we build two graphs, one across the input tensor spatial locations and the other across its frontal slices in the time domain. While maximizing the objective function, we compel the tensor sparse component to serve as the spatiotemporal eigenvectors of the graph-Laplacian matrices. The disconnected moving object pixels in the sparse component are preserved by the proposed graph-based regularizations since they both comprise of spatiotemporal subspace-based structure. Additionally, we propose a unique objective function that employs batch and online-based optimization methods to jointly maximize the background-foreground and spatial-temporal regularization components. Experiments are performed on six publicly available background subtraction datasets that demonstrate the superior performance of the proposed algorithm compared to several existing methods. Our source code will be available very soon.
</details>
<details>
<summary>摘要</summary>
Background subtraction是计算机视觉中的基本问题之一，旨在 segmenting all moving objects。在过去的一个 décennial中， Robust Principal Component Analysis（RPCA）被认为是一种有竞争力的无监督方法 для背景 subtractio Task。然而，由于在 sparse component中的运动 объек pixel不受 spatial-temporal 结构约束，因此在静背景、潜藏和摄像机震动问题时，性能会降低。在这项工作中，我们提出了一种带有spatial-temporal  regularization的tensor sparse RPCA算法，用于高精度的背景 subtractio。在 sparse component中，我们对图 Laplacian矩阵进行正规化，以便在图 Laplacian矩阵的eigenvectors中强制tensor sparse component服为spatiotemporal  eigenvectors。此外，我们还提出了一种新的目标函数，该目标函数通过批量和在线优化方法来同时最大化背景-前景和spatial-temporal regularization组件。在六个公共available background subtractio dataset上进行了实验，并证明了我们提出的算法与现有方法相比具有更高的性能。我们的源代码即将公开。
</details></li>
</ul>
<hr>
<h2 id="Confidence-based-Visual-Dispersal-for-Few-shot-Unsupervised-Domain-Adaptation"><a href="#Confidence-based-Visual-Dispersal-for-Few-shot-Unsupervised-Domain-Adaptation" class="headerlink" title="Confidence-based Visual Dispersal for Few-shot Unsupervised Domain Adaptation"></a>Confidence-based Visual Dispersal for Few-shot Unsupervised Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15575">http://arxiv.org/abs/2309.15575</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bostoncake/c-visdit">https://github.com/bostoncake/c-visdit</a></li>
<li>paper_authors: Yizhe Xiong, Hui Chen, Zijia Lin, Sicheng Zhao, Guiguang Ding</li>
<li>for: 本研究 targets at few-shot unsupervised domain adaptation (FUDA) problem, where only a few labeled source samples are available, and aims to transfer knowledge from the source domain to the target domain without requiring abundant labeled data in the target domain.</li>
<li>methods: 本 paper proposes a novel Confidence-based Visual Dispersal Transfer learning method (C-VisDiT) for FUDA, which consists of a cross-domain visual dispersal strategy and an intra-domain visual dispersal strategy. The cross-domain strategy transfers only high-confidence source knowledge for model adaptation, while the intra-domain strategy guides the learning of hard target samples with easy ones.</li>
<li>results: 在Office-31, Office-Home, VisDA-C, 和DomainNet benchmark datasets上，C-VisDiT significantly outperforms state-of-the-art FUDA methods. The proposed method is able to transfer reliable source knowledge to the target domain and improve the classification performance of hard target samples.<details>
<summary>Abstract</summary>
Unsupervised domain adaptation aims to transfer knowledge from a fully-labeled source domain to an unlabeled target domain. However, in real-world scenarios, providing abundant labeled data even in the source domain can be infeasible due to the difficulty and high expense of annotation. To address this issue, recent works consider the Few-shot Unsupervised Domain Adaptation (FUDA) where only a few source samples are labeled, and conduct knowledge transfer via self-supervised learning methods. Yet existing methods generally overlook that the sparse label setting hinders learning reliable source knowledge for transfer. Additionally, the learning difficulty difference in target samples is different but ignored, leaving hard target samples poorly classified. To tackle both deficiencies, in this paper, we propose a novel Confidence-based Visual Dispersal Transfer learning method (C-VisDiT) for FUDA. Specifically, C-VisDiT consists of a cross-domain visual dispersal strategy that transfers only high-confidence source knowledge for model adaptation and an intra-domain visual dispersal strategy that guides the learning of hard target samples with easy ones. We conduct extensive experiments on Office-31, Office-Home, VisDA-C, and DomainNet benchmark datasets and the results demonstrate that the proposed C-VisDiT significantly outperforms state-of-the-art FUDA methods. Our code is available at https://github.com/Bostoncake/C-VisDiT.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>无监督领域适应目标是将源领域中完全标注的知识传递到无标注目标领域。然而，在实际情况下，提供充沛的标注数据甚至在源领域中可能是不可能的，因为标注的困难和高昂的成本。为解决这个问题，最近的研究将注重ew-shot无监督领域适应（FUDA），只有一些源样本被标注，并通过无监督学习方法进行知识传递。然而，现有的方法通常忽略了稀疏标注设置会阻碍学习可靠的源知识传递，同时 ignore了目标样本的学习难度差，导致目标样本被较差地分类。为解决这两个缺陷，本文提出了一种基于信任度的视觉分散学习方法（C-VisDiT） для FUDA。具体来说，C-VisDiT包括一种跨领域视觉分散策略，将高信任度源知识传递到模型适应，以及一种内领域视觉分散策略，用于导引目标样本中的困难样本和易样本进行学习。我们对Office-31、Office-Home、VisDA-C和DomainNet数据集进行了广泛的实验，结果表明，提出的C-VisDiT显著超过了当前最佳的FUDA方法。我们的代码可以在https://github.com/Bostoncake/C-VisDiT中找到。
</details></li>
</ul>
<hr>
<h2 id="The-Maximum-Cover-with-Rotating-Field-of-View"><a href="#The-Maximum-Cover-with-Rotating-Field-of-View" class="headerlink" title="The Maximum Cover with Rotating Field of View"></a>The Maximum Cover with Rotating Field of View</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15573">http://arxiv.org/abs/2309.15573</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ManojKumarPatnaik/Major-project-list">https://github.com/ManojKumarPatnaik/Major-project-list</a></li>
<li>paper_authors: Igor Potapov, Jason Ralph, Theofilos Triommatis</li>
<li>for: maximize the visibility and limit the uncertainty in localization problems for a convex polygon $P$ and a static spotlight outside $P$.</li>
<li>methods: use a theoretical foundation for the analysis of the maximum cover with a rotating field of view, and express the function of the area $A_{\phi}(\theta)$ as various compositions of a function $A_{\theta}(\phi)$.</li>
<li>results: develop an algorithm that approximates the direction of the field of view with precision $\varepsilon$ and complexity $\mathcal{O}(n(\log{n}+(\log{\varepsilon})&#x2F;\phi))$.Here’s the full text in Simplified Chinese:</li>
<li>for: 这个论文是为了最大化 polygon $P$ 和外部的静止灯光之间的可见性，以及限制 localization 问题中的uncertainty。</li>
<li>methods: 使用一种理论基础来分析最大覆盖的rotating field of view问题，并将函数 $A_{\phi}(\theta)$ 表示为不同的compositions。</li>
<li>results: 开发一个精度为 $\varepsilon$ 的算法，用于 Approximate 静止灯光的方向，复杂度为 $\mathcal{O}(n(\log{n}+(\log{\varepsilon})&#x2F;\phi))$.<details>
<summary>Abstract</summary>
Imagine a polygon-shaped platform $P$ and only one static spotlight outside $P$; which direction should the spotlight face to light most of $P$? This problem occurs in maximising the visibility, as well as in limiting the uncertainty in localisation problems. More formally, we define the following maximum cover problem: "Given a convex polygon $P$ and a Field Of View (FOV) with a given centre and inner angle $\phi$; find the direction (an angle of rotation $\theta$) of the FOV such that the intersection between the FOV and $P$ has the maximum area". In this paper, we provide the theoretical foundation for the analysis of the maximum cover with a rotating field of view. The main challenge is that the function of the area $A_{\phi}(\theta)$, with the angle of rotation $\theta$ and the fixed inner angle $\phi$, cannot be approximated directly. We found an alternative way to express it by various compositions of a function $A_{\theta}(\phi)$ (with a restricted inner angle $\phi$ and a fixed direction $\theta$). We show that $A_{\theta}(\phi)$ has an analytical solution in the special case of a two-sector intersection and later provides a constrictive solution for the original problem. Since the optimal solution is a real number, we develop an algorithm that approximates the direction of the field of view, with precision $\varepsilon$, and complexity $\mathcal{O}(n(\log{n}+(\log{\varepsilon})/\phi))$.
</details>
<details>
<summary>摘要</summary>
想象一个 polygon 形式的平台 $P$ 和一个静止的外部灯光 ; 这个灯光应该朝向哪里来照亮 $P$ 最多呢？这个问题在最大可见性和局部化问题中都有出现。我们定义以下最大覆盖问题：“给定一个 convex polygon $P$ 和一个视野（Field Of View，FOV）的中心和内角 $\phi$；找出FOV 的方向（旋转角 $\theta$），使得 FOV 和 $P$ 的交集具有最大面积”。在这篇论文中，我们提供了对最大覆盖问题的理论基础的分析。主要挑战在于函数 $A_{\phi}(\theta)$ 的计算不能直接 aproximated。我们发现了一种代替的方法，通过不同的compositions来表示 $A_{\theta}(\phi)$ 。我们示示了在特殊情况下的两部分交集时，$A_{\theta}(\phi)$ 有分析解，并提供了一个压缩性的解决方案。由于优化解决方案是实数，我们开发了一个精度为 $\varepsilon$ 的搜索算法，复杂度为 $\mathcal{O}(n(\log{n}+(\log{\varepsilon})/\phi))$.
</details></li>
</ul>
<hr>
<h2 id="HPL-ViT-A-Unified-Perception-Framework-for-Heterogeneous-Parallel-LiDARs-in-V2V"><a href="#HPL-ViT-A-Unified-Perception-Framework-for-Heterogeneous-Parallel-LiDARs-in-V2V" class="headerlink" title="HPL-ViT: A Unified Perception Framework for Heterogeneous Parallel LiDARs in V2V"></a>HPL-ViT: A Unified Perception Framework for Heterogeneous Parallel LiDARs in V2V</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15572">http://arxiv.org/abs/2309.15572</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/NumtraCG/614ca2d5a2b781088de648b020210923-155728routingdatapipeline230921">https://github.com/NumtraCG/614ca2d5a2b781088de648b020210923-155728routingdatapipeline230921</a></li>
<li>paper_authors: Yuhang Liu, Boyi Sun, Yuke Li, Yuzheng Hu, Fei-Yue Wang</li>
<li>for: 这个论文的目的是发展下一代智能探测器，提出了一个新的框架，并在实验平台DAWN上建立了硬件实现。</li>
<li>methods: 这个论文使用了OpenCDA和RLS来建立一个多种探测器数据集OPV2V-HPL，并提出了一个具有域专特性抽象的HPL-ViT架构，用于稳定特征融合。</li>
<li>results: 实验结果显示，HPL-ViT在所有设定下均 achievement SOTA表现，并具有优秀的泛化能力。<details>
<summary>Abstract</summary>
To develop the next generation of intelligent LiDARs, we propose a novel framework of parallel LiDARs and construct a hardware prototype in our experimental platform, DAWN (Digital Artificial World for Natural). It emphasizes the tight integration of physical and digital space in LiDAR systems, with networking being one of its supported core features. In the context of autonomous driving, V2V (Vehicle-to-Vehicle) technology enables efficient information sharing between different agents which significantly promotes the development of LiDAR networks. However, current research operates under an ideal situation where all vehicles are equipped with identical LiDAR, ignoring the diversity of LiDAR categories and operating frequencies. In this paper, we first utilize OpenCDA and RLS (Realistic LiDAR Simulation) to construct a novel heterogeneous LiDAR dataset named OPV2V-HPL. Additionally, we present HPL-ViT, a pioneering architecture designed for robust feature fusion in heterogeneous and dynamic scenarios. It uses a graph-attention Transformer to extract domain-specific features for each agent, coupled with a cross-attention mechanism for the final fusion. Extensive experiments on OPV2V-HPL demonstrate that HPL-ViT achieves SOTA (state-of-the-art) performance in all settings and exhibits outstanding generalization capabilities.
</details>
<details>
<summary>摘要</summary>
要开发下一代智能激光仪，我们提出了一个新的框架──并行激光仪架构（Parallel LiDARs），并在我们的实验平台DAWN（数位人工世界）中实现了实验。这个框架强调物理和数位空间之间的紧密融合，并且支持网络作为核心功能。在自驾车领域，车辆之间的通信技术（V2V）可以实现车辆之间的有效信息交换，这有助于开发激光网络。然而，现有的研究假设所有车辆都采用同一款激光仪，忽略了激光仪的多标准和频率多标准。在这篇论文中，我们首先使用OpenCDA和RLS（现实激光仪 simulator）创建了一个独特的不同激光类型和频率的激光数据集名为OPV2V-HPL。此外，我们还提出了HPL-ViT，一个创新的架构，用于在多标准和动态enario中实现坚固的特征融合。它使用图形注意力Transformer提取特定领域的特征，并与交互式混合机制进行最终融合。实验结果显示，HPL-ViT在所有设定下实现了SOTA性能，并且具有卓越的普遍化能力。
</details></li>
</ul>
<hr>
<h2 id="Guided-Frequency-Loss-for-Image-Restoration"><a href="#Guided-Frequency-Loss-for-Image-Restoration" class="headerlink" title="Guided Frequency Loss for Image Restoration"></a>Guided Frequency Loss for Image Restoration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15563">http://arxiv.org/abs/2309.15563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bilel Benjdira, Anas M. Ali, Anis Koubaa</li>
<li>for: 提高图像Restoration的效果</li>
<li>methods: 提出了一种名为Guided Frequency Loss（GFL）的损失函数，用于让模型同时学习图像的频谱内容和空间内容</li>
<li>results: GFL损失函数在Super Resolution和Denoising任务上实现了PSNR指标的提高，并且在SwinIR和SRGAN模型中提高了训练效果，特别是在受限数据上表现更佳<details>
<summary>Abstract</summary>
Image Restoration has seen remarkable progress in recent years. Many generative models have been adapted to tackle the known restoration cases of images. However, the interest in benefiting from the frequency domain is not well explored despite its major factor in these particular cases of image synthesis. In this study, we propose the Guided Frequency Loss (GFL), which helps the model to learn in a balanced way the image's frequency content alongside the spatial content. It aggregates three major components that work in parallel to enhance learning efficiency; a Charbonnier component, a Laplacian Pyramid component, and a Gradual Frequency component. We tested GFL on the Super Resolution and the Denoising tasks. We used three different datasets and three different architectures for each of them. We found that the GFL loss improved the PSNR metric in most implemented experiments. Also, it improved the training of the Super Resolution models in both SwinIR and SRGAN. In addition, the utility of the GFL loss increased better on constrained data due to the less stochasticity in the high frequencies' components among samples.
</details>
<details>
<summary>摘要</summary>
Image Restoration 在最近几年内有了非常 significiant progress。许多生成模型已经被应用于图像还原的知名情况中。然而，利用频率领域的利益并没有得到充分的探索，尽管它在这些图像生成情况中扮演着重要的角色。在这种研究中，我们提出了引导频率损失（GFL），它帮助模型同时学习图像的频率内容和空间内容。GFLloss 包括三个主要组成部分，它们在并行工作以提高学习效率：Charbonnier 组件、Laplacian Pyramid 组件和渐进频率组件。我们在Super Resolution 和 Denoising 任务上测试了GFL loss。我们使用了三个不同的数据集和三个不同的架构。我们发现，GFL loss 提高了 PSNR 指标的大多数实验中。此外，GFL loss 也提高了 SwinIR 和 SRGAN 中的 Super Resolution 模型训练。此外，GFL loss 在受限数据上的利用程度更高，因为高频分布在样本中的不确定性更低。
</details></li>
</ul>
<hr>
<h2 id="Learning-from-SAM-Harnessing-a-Segmentation-Foundation-Model-for-Sim2Real-Domain-Adaptation-through-Regularization"><a href="#Learning-from-SAM-Harnessing-a-Segmentation-Foundation-Model-for-Sim2Real-Domain-Adaptation-through-Regularization" class="headerlink" title="Learning from SAM: Harnessing a Segmentation Foundation Model for Sim2Real Domain Adaptation through Regularization"></a>Learning from SAM: Harnessing a Segmentation Foundation Model for Sim2Real Domain Adaptation through Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15562">http://arxiv.org/abs/2309.15562</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mayara E. Bonani, Max Schwarz, Sven Behnke</li>
<li>for: 本研究旨在提高无监督预处理的预测性能，尤其是在机器人应用中，目标领域训练数据罕见而注解成本高昂。</li>
<li>methods: 本方法基于Segment Anything模型，利用无注解目标频道数据进行自我监督预处理，并提出了一种协方差-方差损失结构，以正则化目标频道上的特征表示。</li>
<li>results: 在YCB-Video和HomebrewedDB等 datasets上，本方法的表现优于先前的方法，甚至在YCB-Video上超过了使用真注解的网络。<details>
<summary>Abstract</summary>
Domain adaptation is especially important for robotics applications, where target domain training data is usually scarce and annotations are costly to obtain. We present a method for self-supervised domain adaptation for the scenario where annotated source domain data (e.g. from synthetic generation) is available, but the target domain data is completely unannotated. Our method targets the semantic segmentation task and leverages a segmentation foundation model (Segment Anything Model) to obtain segment information on unannotated data. We take inspiration from recent advances in unsupervised local feature learning and propose an invariance-variance loss structure over the detected segments for regularizing feature representations in the target domain. Crucially, this loss structure and network architecture can handle overlapping segments and oversegmentation as produced by Segment Anything. We demonstrate the advantage of our method on the challenging YCB-Video and HomebrewedDB datasets and show that it outperforms prior work and, on YCB-Video, even a network trained with real annotations.
</details>
<details>
<summary>摘要</summary>
域 adaptation 特别重要 для robotics 应用程序，目标域训练数据通常罕见而且标注成本高昂。我们提出了一种自我超级vised域 adaptation 方法，其中可以使用已有的源域数据（例如从生成的 sintetico 数据）进行标注，但目标域数据完全无标注。我们的方法targets semantic segmentation 任务，利用 segmentation 基础模型（Segment Anything Model）获取目标域数据中的分割信息。我们启发自最近的无监督本地特征学习的进步，并提出了一种协方差-方差损失结构来规范目标域数据中的特征表示。这种损失结构和网络结构可以处理重叠的分割和过分割，这是由 Segment Anything 生成的。我们在 YCB-Video 和 HomebrewedDB  datasets 中展示了我们的方法的优势，并证明它超过先前的工作和，在 YCB-Video 上，连实际标注生成的网络都不能比拟。
</details></li>
</ul>
<hr>
<h2 id="Highly-Efficient-SNNs-for-High-speed-Object-Detection"><a href="#Highly-Efficient-SNNs-for-High-speed-Object-Detection" class="headerlink" title="Highly Efficient SNNs for High-speed Object Detection"></a>Highly Efficient SNNs for High-speed Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15883">http://arxiv.org/abs/2309.15883</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nemin Qiu, Zhiguo Li, Yuan Li, Chuang Zhu</li>
<li>for: 这个论文旨在提出一种高效的神经网络模型，用于快速的物体检测任务。</li>
<li>methods: 该论文使用量化训练方法建立了一个具有初始紧凑性的神经网络模型，并提出了一种扩展 Pseudoquantization 方法来保证模型的正确性。另外，它还提出了一种连续推理方案，使用 Feed-Forward Integrate-and-Fire（FewdIF）神经元来实现高速的物体检测。</li>
<li>results: 实验结果表明，该高效的神经网络模型可以在 GPU 上实现 118 倍的速度提升，只需要 1.5 MB 的参数进行物体检测任务。此外，在 FPGA 平台上，提出的模型可以实现 800+ FPS 的物体检测，并且具有极低的响应时间。<details>
<summary>Abstract</summary>
The high biological properties and low energy consumption of Spiking Neural Networks (SNNs) have brought much attention in recent years. However, the converted SNNs generally need large time steps to achieve satisfactory performance, which will result in high inference latency and computational resources increase. In this work, we propose a highly efficient and fast SNN for object detection. First, we build an initial compact ANN by using quantization training method of convolution layer fold batch normalization layer and neural network modification. Second, we theoretically analyze how to obtain the low complexity SNN correctly. Then, we propose a scale-aware pseudoquantization scheme to guarantee the correctness of the compact ANN to SNN. Third, we propose a continuous inference scheme by using a Feed-Forward Integrate-and-Fire (FewdIF) neuron to realize high-speed object detection. Experimental results show that our efficient SNN can achieve 118X speedup on GPU with only 1.5MB parameters for object detection tasks. We further verify our SNN on FPGA platform and the proposed model can achieve 800+FPS object detection with extremely low latency.
</details>
<details>
<summary>摘要</summary>
高生物特性和低能耗的神经网络（SNN）在最近几年内受到了广泛关注。然而，通常的SNN conversions需要大量的时间步长来 достичь满意的性能，这会导致高的推理延迟和计算资源增加。在这种情况下，我们提出了一种高效率和快速的SNN для对象检测。首先，我们使用量化训练方法建立了一个初始化紧凑的ANN。其次，我们 teorically 分析了如何正确地获得低复杂度SNN。然后，我们提出了一种扩展 Pseudoquantization 方案，以确保紧凑ANN的正确性。第三，我们提出了一种连续推理方案，使用 Feed-Forward Integrate-and-Fire（FewdIF） neuron 来实现高速对象检测。实验结果显示，我们的高效SNN在 GPU 上可以实现118倍的速度提升，只需1.5MB 的参数进行对象检测任务。我们进一步验证了我们的SNN在 FPGA 平台上，并发现提出的模型可以实现800+ FPS 对象检测，并且具有极低的延迟。
</details></li>
</ul>
<hr>
<h2 id="Learning-Dense-Flow-Field-for-Highly-accurate-Cross-view-Camera-Localization"><a href="#Learning-Dense-Flow-Field-for-Highly-accurate-Cross-view-Camera-Localization" class="headerlink" title="Learning Dense Flow Field for Highly-accurate Cross-view Camera Localization"></a>Learning Dense Flow Field for Highly-accurate Cross-view Camera Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15556">http://arxiv.org/abs/2309.15556</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenbo Song, Xianghui Ze, Jianfeng Lu, Yujiao Shi</li>
<li>for: 本研究旨在解决基于卫星图像的地面图像三重自由度摄像机pose估算问题。</li>
<li>methods: 我们提出了一种新的端到端方法，利用了对精密像素粒子流场的学习，以计算摄像机pose。我们的方法与现有方法不同，在像素级别上建立特征度量，使得整个图像得到全图像超级视图控制。具体来说，我们使用了两种不同的卷积网络来提取地面和卫星特征。然后，我们将地面特征图 проек到鸟瞰视图（BEV）中使用固定镜头高度假设来实现初步的几何对应。为了进一步确立地面和卫星特征之间的内容关系，我们引入了一个差分卷积块来修正项目的BEV特征。然后，我们使用RAFT流体解oder网络来计算 dense流场对应。在获得dense流场对应后，我们通过最小二乘方法来过滤匹配的准确值和回归地面摄像机pose。</li>
<li>results: 我们的方法与现有方法相比，在KITTI、FORD multi-AV、VIGOR和Oxford RobotCar等数据集上具有显著的改善。特别是，我们的方法可以将地面摄像机pose的 median localization error 降低89%、19%、80%和35%。<details>
<summary>Abstract</summary>
This paper addresses the problem of estimating the 3-DoF camera pose for a ground-level image with respect to a satellite image that encompasses the local surroundings. We propose a novel end-to-end approach that leverages the learning of dense pixel-wise flow fields in pairs of ground and satellite images to calculate the camera pose. Our approach differs from existing methods by constructing the feature metric at the pixel level, enabling full-image supervision for learning distinctive geometric configurations and visual appearances across views. Specifically, our method employs two distinct convolution networks for ground and satellite feature extraction. Then, we project the ground feature map to the bird's eye view (BEV) using a fixed camera height assumption to achieve preliminary geometric alignment. To further establish content association between the BEV and satellite features, we introduce a residual convolution block to refine the projected BEV feature. Optical flow estimation is performed on the refined BEV feature map and the satellite feature map using flow decoder networks based on RAFT. After obtaining dense flow correspondences, we apply the least square method to filter matching inliers and regress the ground camera pose. Extensive experiments demonstrate significant improvements compared to state-of-the-art methods. Notably, our approach reduces the median localization error by 89%, 19%, 80% and 35% on the KITTI, Ford multi-AV, VIGOR and Oxford RobotCar datasets, respectively.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文关注了根据卫星图像的地面图像的3个自由度摄像机pose的估算问题。我们提出了一种新的端到端方法，利用了 dense pixel-wise流场场的学习，以计算摄像机pose。我们的方法与现有方法不同，在像素级别构建特征度量，以实现全图像监督，以学习不同视角的特征配置和视觉特征。specifically，我们使用了两个不同的卷积网络来EXTRACT ground和卫星特征。然后，我们将地面特征图Projected to the bird's eye view (BEV) using a fixed camera height assumption to achieve preliminary geometric alignment。为了进一步确立地面和卫星特征之间的内容关联，我们引入了一个差分卷积块来修正Projected BEV特征。然后，我们使用了 RAFT流场估计器来进行流场估计在BEV特征图和卫星特征图上。得到了密集流场匹配后，我们使用最小二乘法来过滤匹配的入liers和回归地面摄像机pose。我们的方法在KITTI、Ford multi-AV、VIGOR和Oxford RobotCar等数据集上进行了广泛的实验，并达到了STATE-OF-THE-ART的性能。尤其是，我们的方法在KITTI数据集上reduces the median localization error by 89%, 19%, 80% and 35% compared to state-of-the-art methods。
</details></li>
</ul>
<hr>
<h2 id="Low-Latency-of-object-detection-for-spikng-neural-network"><a href="#Low-Latency-of-object-detection-for-spikng-neural-network" class="headerlink" title="Low Latency of object detection for spikng neural network"></a>Low Latency of object detection for spikng neural network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15555">http://arxiv.org/abs/2309.15555</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nemin Qiu, Chuang Zhu</li>
<li>for: 本文旨在开发高精度低延迟的神经网络，特别适用于Edge AI应用。</li>
<li>methods: 本文使用了系统性的变换方法，从神经网络中提取了精度和速度两个维度的优势，并通过结构性的修改和量化纠正错误来提高准确率和速度。</li>
<li>results: 实验结果显示，提议方法在MS COCO、PASCAL VOC等难度较高的数据集上具有更高的准确率和更低的延迟，并且能够展示神经网络处理脉冲信号的优势。<details>
<summary>Abstract</summary>
Spiking Neural Networks, as a third-generation neural network, are well-suited for edge AI applications due to their binary spike nature. However, when it comes to complex tasks like object detection, SNNs often require a substantial number of time steps to achieve high performance. This limitation significantly hampers the widespread adoption of SNNs in latency-sensitive edge devices. In this paper, our focus is on generating highly accurate and low-latency SNNs specifically for object detection. Firstly, we systematically derive the conversion between SNNs and ANNs and analyze how to improve the consistency between them: improving the spike firing rate and reducing the quantization error. Then we propose a structural replacement, quantization of ANN activation and residual fix to allevicate the disparity. We evaluate our method on challenging dataset MS COCO, PASCAL VOC and our spike dataset. The experimental results show that the proposed method achieves higher accuracy and lower latency compared to previous work Spiking-YOLO. The advantages of SNNs processing of spike signals are also demonstrated.
</details>
<details>
<summary>摘要</summary>
神经网络具有辐射性，可以在边缘智能应用中使用，因为它们的二进制脉冲性。然而，当面临复杂任务时，如物体检测，SNNs经常需要较多的时间步骤以达到高性能。这种限制妨碍了SNNs在响应时间敏感的边缘设备中的普及。在这篇论文中，我们关注于生成高精度低延迟的SNNs，特别是用于物体检测。首先，我们系统地 derivate SNNs和ANNs之间的转化，并分析如何提高脉冲发射率和减少量化误差。然后，我们提议一种结构性的替换方案，即Activation和Residual的量化纠正，以降低不一致性。我们在MS COCO、PASCAL VOC和我们的脉冲集上进行了实验，结果表明，我们的方法可以 achieved higher accuracy和lower latency compared to previous work Spiking-YOLO。此外，SNNs处理脉冲信号的优势也得到了演示。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Quantification-via-Neural-Posterior-Principal-Components"><a href="#Uncertainty-Quantification-via-Neural-Posterior-Principal-Components" class="headerlink" title="Uncertainty Quantification via Neural Posterior Principal Components"></a>Uncertainty Quantification via Neural Posterior Principal Components</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15533">http://arxiv.org/abs/2309.15533</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elias Nehme, Omer Yair, Tomer Michaeli</li>
<li>for: 这个论文的目的是提出一种能够在单个前向传播中预测 posterior 分布的主成分（PC），以便实现图像修复模型中的不确定性评估。</li>
<li>methods: 该方法基于 neural network 的卷积神经网络，可以在单个前向传播中预测 posterior 分布的主成分。可以选择使用预训练的模型，或者从 scratch 开始训练一个模型，以输出预测图像和 posterior 分布的主成分。</li>
<li>results: 该方法在多个图像修复问题中表现出色，例如噪声除除、图像缺失填充、超分辨率重建和生物图像转换等。与 posterior 抽样法相比，该方法可以实现更快速的uncertainty量化，并且可以提供更自然的不确定性方向。详细例子可以参考 <a target="_blank" rel="noopener" href="https://eliasnehme.github.io/NPPC/">https://eliasnehme.github.io/NPPC/</a><details>
<summary>Abstract</summary>
Uncertainty quantification is crucial for the deployment of image restoration models in safety-critical domains, like autonomous driving and biological imaging. To date, methods for uncertainty visualization have mainly focused on per-pixel estimates. However, a heatmap of per-pixel variances is typically of little practical use, as it does not capture the strong correlations between pixels. A more natural measure of uncertainty corresponds to the variances along the principal components (PCs) of the posterior distribution. Theoretically, the PCs can be computed by applying PCA on samples generated from a conditional generative model for the input image. However, this requires generating a very large number of samples at test time, which is painfully slow with the current state-of-the-art (diffusion) models. In this work, we present a method for predicting the PCs of the posterior distribution for any input image, in a single forward pass of a neural network. Our method can either wrap around a pre-trained model that was trained to minimize the mean square error (MSE), or can be trained from scratch to output both a predicted image and the posterior PCs. We showcase our method on multiple inverse problems in imaging, including denoising, inpainting, super-resolution, and biological image-to-image translation. Our method reliably conveys instance-adaptive uncertainty directions, achieving uncertainty quantification comparable with posterior samplers while being orders of magnitude faster. Examples are available at https://eliasnehme.github.io/NPPC/
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate the following text into Simplified Chinese<</SYS>>难以量化的不确定性是图像修复模型在安全关键领域部署的关键，如自动驾驶和生物成像。目前，图像修复模型的不确定性可视化方法主要集中在每个像素的估计上。然而，每个像素的独立差分（variance）的热图通常并不是实际上的很有帮助，因为它们不会捕捉图像中像素之间的强相关性。一个更自然的不确定性度量是根据 posterior 分布的主成分（principal components，PCs）的方差来计算。理论上，PCs 可以通过将 conditional generative model 生成的样本应用 PCA 来计算。然而，这需要在测试时生成非常多的样本，这是目前的 diffusion 模型 非常慢。在这个工作中，我们提出了一种方法，可以在单个前向传播中计算 posterior 分布的 PCs  для任何输入图像。我们的方法可以在一个已经训练好的模型上执行，该模型是通过最小二乘误差（MSE）进行训练来减少 Mean Squared Error 的。也可以从头开始训练这个模型，以输出预测图像和 posterior PCs。我们在多种图像重建问题中展示了我们的方法，包括噪声除去、填充、超分辨和生物图像到图像转换。我们的方法可以准确地传递实例特有的不确定性方向，实现了对 posterior 抽样器的uncertainty quantification，并且速度比对 diffusion 模型的训练进行训练好的模型。示例可以在 <https://eliasnehme.github.io/NPPC/> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Missing-modality-Enabled-Multi-modal-Fusion-Architecture-for-Medical-Data"><a href="#Missing-modality-Enabled-Multi-modal-Fusion-Architecture-for-Medical-Data" class="headerlink" title="Missing-modality Enabled Multi-modal Fusion Architecture for Medical Data"></a>Missing-modality Enabled Multi-modal Fusion Architecture for Medical Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15529">http://arxiv.org/abs/2309.15529</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muyu Wang, Shiyu Fan, Yichen Li, Hui Chen</li>
<li>for: 这个研究旨在开发一个可靠的多模式融合架构，以实现医疗资料中缺失的模式不断影响深度学习模型的性能。</li>
<li>methods: 本研究使用了一个基于Transformer的多模式融合模组，将双模式融合为一个三模式融合架构。此外，研究者还引入多変量损失函数，以提高模型对缺失模式的Robustness。</li>
<li>results: 实验结果显示，提案的多模式融合架构能够有效地融合三种模式，并在缺失模式情况下保持优秀的性能。这个方法可能会扩展到更多模式，以提高临床实用性。<details>
<summary>Abstract</summary>
Fusing multi-modal data can improve the performance of deep learning models. However, missing modalities are common for medical data due to patients' specificity, which is detrimental to the performance of multi-modal models in applications. Therefore, it is critical to adapt the models to missing modalities. This study aimed to develop an efficient multi-modal fusion architecture for medical data that was robust to missing modalities and further improved the performance on disease diagnosis.X-ray chest radiographs for the image modality, radiology reports for the text modality, and structured value data for the tabular data modality were fused in this study. Each modality pair was fused with a Transformer-based bi-modal fusion module, and the three bi-modal fusion modules were then combined into a tri-modal fusion framework. Additionally, multivariate loss functions were introduced into the training process to improve model's robustness to missing modalities in the inference process. Finally, we designed comparison and ablation experiments for validating the effectiveness of the fusion, the robustness to missing modalities and the enhancements from each key component. Experiments were conducted on MIMIC-IV, MIMIC-CXR with the 14-label disease diagnosis task. Areas under the receiver operating characteristic curve (AUROC), the area under the precision-recall curve (AUPRC) were used to evaluate models' performance. The experimental results demonstrated that our proposed multi-modal fusion architecture effectively fused three modalities and showed strong robustness to missing modalities. This method is hopeful to be scaled to more modalities to enhance the clinical practicality of the model.
</details>
<details>
<summary>摘要</summary>
融合多Modal数据可以提高深度学习模型的性能。然而，医疗数据中缺失Modalities是常见的，这会导致多Modal模型在应用中表现不佳。因此，适应缺失Modalities是非常重要的。这项研究旨在开发一种可靠的多Modal融合架构，可以在医疗数据中融合多种Modalities，并且在缺失Modalities时保持模型的性能。本研究使用的Modalities包括X射成像（image modality）、 radiology report（text modality）和结构化数据（tabular data modality）。每个Modal pair使用Transformer基于的bi-Modal融合模块进行融合，并将三个bi-Modal融合模块组合成一个tri-Modal融合架构。此外，我们还引入了多个变量损失函数来改善模型在推理过程中对缺失Modalities的Robustness。最后，我们设计了比较和减少实验来验证融合的有效性、Robustness和每个关键组件的改进。实验使用MIMIC-IV和MIMIC-CXR datasets，并使用14个疾病诊断任务来评估模型的性能。实验结果表明，我们提posed的多Modal融合架构可以有效地融合三种Modalities，并且在缺失Modalities时保持模型的性能。这种方法可以在更多Modalities上进行扩展，以提高临床实用性。
</details></li>
</ul>
<hr>
<h2 id="P2I-NET-Mapping-Camera-Pose-to-Image-via-Adversarial-Learning-for-New-View-Synthesis-in-Real-Indoor-Environments"><a href="#P2I-NET-Mapping-Camera-Pose-to-Image-via-Adversarial-Learning-for-New-View-Synthesis-in-Real-Indoor-Environments" class="headerlink" title="P2I-NET: Mapping Camera Pose to Image via Adversarial Learning for New View Synthesis in Real Indoor Environments"></a>P2I-NET: Mapping Camera Pose to Image via Adversarial Learning for New View Synthesis in Real Indoor Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15526">http://arxiv.org/abs/2309.15526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xujie Kang, Kanglin Liu, Jiang Duan, Yuanhao Gong, Guoping Qiu</li>
<li>For: 根据一个新的6DoF摄像头位置，研究在室内环境中预测当前摄像头的视角，并且使用一个conditional生成问题答案网络（P2I-NET）来直接预测当前摄像头的视角。* Methods: 提出了两个副档案检测器，一个是在伪实值空间中的对应检测器，另一个是在真实世界摄像头位置空间中的对应检测器，以确保生成的图像和实际世界中的摄像头位置之间的一致性。此外，还引入了一个深度卷积神经网络（CNN）来进一步强制这一一致性。* Results: 实际进行了对新视角预测实验，结果显示P2I-NET在许多NeRF基础模型的比较下表现出色，尤其是在速度方面，P2I-NET比基础模型40-100倍快。此外，还提供了一个新的室内环境数据集，包括22个高分辨率RGBD影像和对应的摄像头位置参数。<details>
<summary>Abstract</summary>
Given a new $6DoF$ camera pose in an indoor environment, we study the challenging problem of predicting the view from that pose based on a set of reference RGBD views. Existing explicit or implicit 3D geometry construction methods are computationally expensive while those based on learning have predominantly focused on isolated views of object categories with regular geometric structure. Differing from the traditional \textit{render-inpaint} approach to new view synthesis in the real indoor environment, we propose a conditional generative adversarial neural network (P2I-NET) to directly predict the new view from the given pose. P2I-NET learns the conditional distribution of the images of the environment for establishing the correspondence between the camera pose and its view of the environment, and achieves this through a number of innovative designs in its architecture and training lost function. Two auxiliary discriminator constraints are introduced for enforcing the consistency between the pose of the generated image and that of the corresponding real world image in both the latent feature space and the real world pose space. Additionally a deep convolutional neural network (CNN) is introduced to further reinforce this consistency in the pixel space. We have performed extensive new view synthesis experiments on real indoor datasets. Results show that P2I-NET has superior performance against a number of NeRF based strong baseline models. In particular, we show that P2I-NET is 40 to 100 times faster than these competitor techniques while synthesising similar quality images. Furthermore, we contribute a new publicly available indoor environment dataset containing 22 high resolution RGBD videos where each frame also has accurate camera pose parameters.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Improving-Facade-Parsing-with-Vision-Transformers-and-Line-Integration"><a href="#Improving-Facade-Parsing-with-Vision-Transformers-and-Line-Integration" class="headerlink" title="Improving Facade Parsing with Vision Transformers and Line Integration"></a>Improving Facade Parsing with Vision Transformers and Line Integration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15523">http://arxiv.org/abs/2309.15523</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wbw520/rtfp">https://github.com/wbw520/rtfp</a></li>
<li>paper_authors: Bowen Wang, Jiaxing Zhang, Ran Zhang, Yunqin Li, Liangzhi Li, Yuta Nakashima</li>
<li>For: The paper is focused on developing a new dataset (Comprehensive Facade Parsing) and a novel pipeline (Revision-based Transformer Facade Parsing) for real-world facade parsing tasks, with the aim of improving computational efficiency and accuracy.* Methods: The paper introduces the use of Vision Transformers (ViT) in facade parsing, and proposes a new revision algorithm (Line Acquisition, Filtering, and Revision) to improve the segmentation results.* Results: The paper reports superior performance of the proposed method on three datasets (ECP 2011, RueMonge 2014, and CFP) compared to existing methods.Here are the three points in Simplified Chinese text:* For: 本文目的是开发一个新的 dataset (Comprehensive Facade Parsing) 和一种新的管道 (Revision-based Transformer Facade Parsing)，以提高实际场景中的facade parsing任务计算效率和准确率。* Methods: 本文提出使用 Vision Transformers (ViT) 在facade parsing任务中，并提出一种新的修订算法 (Line Acquisition, Filtering, and Revision) 来提高分割结果。* Results: 本文report示本方法在三个dataset (ECP 2011, RueMonge 2014, 和 CFP) 上的表现较为现有方法有所提高。<details>
<summary>Abstract</summary>
Facade parsing stands as a pivotal computer vision task with far-reaching applications in areas like architecture, urban planning, and energy efficiency. Despite the recent success of deep learning-based methods in yielding impressive results on certain open-source datasets, their viability for real-world applications remains uncertain. Real-world scenarios are considerably more intricate, demanding greater computational efficiency. Existing datasets often fall short in representing these settings, and previous methods frequently rely on extra models to enhance accuracy, which requires much computation cost. In this paper, we introduce Comprehensive Facade Parsing (CFP), a dataset meticulously designed to encompass the intricacies of real-world facade parsing tasks. Comprising a total of 602 high-resolution street-view images, this dataset captures a diverse array of challenging scenarios, including sloping angles and densely clustered buildings, with painstakingly curated annotations for each image. We introduce a new pipeline known as Revision-based Transformer Facade Parsing (RTFP). This marks the pioneering utilization of Vision Transformers (ViT) in facade parsing, and our experimental results definitively substantiate its merit. We also design Line Acquisition, Filtering, and Revision (LAFR), an efficient yet accurate revision algorithm that can improve the segment result solely from simple line detection using prior knowledge of the facade. In ECP 2011, RueMonge 2014, and our CFP, we evaluate the superiority of our method.
</details>
<details>
<summary>摘要</summary>
外墙解析作为计算机视觉任务，在建筑、城市规划和能效环境等领域具有广泛的应用前景。尽管最近的深度学习方法在某些开源数据集上实现了卓越的结果，但其在实际应用中的可靠性仍存在uncertainty。实际场景相对较复杂，需要更高的计算效率。现有的数据集frequently不能 полностью反映这些场景，而前一些方法通常需要额外的模型来提高准确性，这需要大量的计算成本。在这篇论文中，我们介绍了全面的外墙解析（CFP）数据集，这个数据集仔细地设计，以涵盖实际场景中的复杂性。总共包含602个高分辨率街景图像，这个数据集包括倾斜角和密集建筑等挑战性场景，并且对每个图像进行了精心的标注。我们提出了一个新的管道，称为修订基于转换器的外墙解析（RTFP）。这是首次在外墙解析中使用视transformer（ViT），我们的实验结果证明了它的优势。我们还设计了线性获取、筛选和修订（LAFR）算法，这是一种高效又准确的修订算法，可以通过对外墙的基本线段进行优化来提高 segment结果。在ECP 2011、RueMonge 2014和我们的CFP中，我们评估了我们的方法的优越性。
</details></li>
</ul>
<hr>
<h2 id="MLOps-for-Scarce-Image-Data-A-Use-Case-in-Microscopic-Image-Analysis"><a href="#MLOps-for-Scarce-Image-Data-A-Use-Case-in-Microscopic-Image-Analysis" class="headerlink" title="MLOps for Scarce Image Data: A Use Case in Microscopic Image Analysis"></a>MLOps for Scarce Image Data: A Use Case in Microscopic Image Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15521">http://arxiv.org/abs/2309.15521</a></li>
<li>repo_url: None</li>
<li>paper_authors: Angelo Yamachui Sitcheu, Nils Friederich, Simon Baeuerle, Oliver Neumann, Markus Reischl, Ralf Mikut</li>
<li>for: This paper aims to enhance biomedical image analysis using a holistic approach to Machine Learning Operations (MLOps) in the context of scarce data.</li>
<li>methods: The proposed method includes a fingerprinting process to select the best models, datasets, and development strategy for image analysis tasks, as well as automated model development and continuous deployment and monitoring to ensure continuous learning.</li>
<li>results: The paper presents preliminary results of a proof of concept for fingerprinting in microscopic image datasets.<details>
<summary>Abstract</summary>
Nowadays, Machine Learning (ML) is experiencing tremendous popularity that has never been seen before. The operationalization of ML models is governed by a set of concepts and methods referred to as Machine Learning Operations (MLOps). Nevertheless, researchers, as well as professionals, often focus more on the automation aspect and neglect the continuous deployment and monitoring aspects of MLOps. As a result, there is a lack of continuous learning through the flow of feedback from production to development, causing unexpected model deterioration over time due to concept drifts, particularly when dealing with scarce data. This work explores the complete application of MLOps in the context of scarce data analysis. The paper proposes a new holistic approach to enhance biomedical image analysis. Our method includes: a fingerprinting process that enables selecting the best models, datasets, and model development strategy relative to the image analysis task at hand; an automated model development stage; and a continuous deployment and monitoring process to ensure continuous learning. For preliminary results, we perform a proof of concept for fingerprinting in microscopic image datasets.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SAF-Net-Self-Attention-Fusion-Network-for-Myocardial-Infarction-Detection-using-Multi-View-Echocardiography"><a href="#SAF-Net-Self-Attention-Fusion-Network-for-Myocardial-Infarction-Detection-using-Multi-View-Echocardiography" class="headerlink" title="SAF-Net: Self-Attention Fusion Network for Myocardial Infarction Detection using Multi-View Echocardiography"></a>SAF-Net: Self-Attention Fusion Network for Myocardial Infarction Detection using Multi-View Echocardiography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15520">http://arxiv.org/abs/2309.15520</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilke Adalioglu, Mete Ahishali, Aysen Degerli, Serkan Kiranyaz, Moncef Gabbouj<br>for:This paper proposes a novel view-fusion model named SAF-Net to detect myocardial infarction (MI) from multi-view echocardiography recordings.methods:The proposed framework utilizes apical 2-chamber (A2C) and apical 4-chamber (A4C) view echocardiography recordings, and extracts highly representative features using pre-trained deep networks. The SAF-Net model uses a self-attention mechanism to learn dependencies in the extracted feature vectors.results:The proposed SAF-Net model achieves a high-performance level with 88.26% precision, 77.64% sensitivity, and 78.13% accuracy in detecting MI from multi-view echocardiography recordings. The results demonstrate that the SAF-Net model achieves the most accurate MI detection over multi-view echocardiography recordings.Here’s the simplified Chinese text in the format you requested:for: 这个研究提出了一种名为SAF-Net的新视角融合模型，用于从多视角电子心肺图像记录中检测心肺炎。methods: 该提案使用了二尖脉(A2C)和四尖脉(A4C)视角电子心肺图像记录，并EXTRACT高度表征特征。使用预训练深度网络来EXTRACT特征。results: 提案的SAF-Net模型在多视角电子心肺图像记录中检测心肺炎的高性能水平达88.26%的准确率，77.64%的敏感度和78.13%的准确率。结果表明SAF-Net模型在多视角电子心肺图像记录中检测心肺炎的最高精度。<details>
<summary>Abstract</summary>
Myocardial infarction (MI) is a severe case of coronary artery disease (CAD) and ultimately, its detection is substantial to prevent progressive damage to the myocardium. In this study, we propose a novel view-fusion model named self-attention fusion network (SAF-Net) to detect MI from multi-view echocardiography recordings. The proposed framework utilizes apical 2-chamber (A2C) and apical 4-chamber (A4C) view echocardiography recordings for classification. Three reference frames are extracted from each recording of both views and deployed pre-trained deep networks to extract highly representative features. The SAF-Net model utilizes a self-attention mechanism to learn dependencies in extracted feature vectors. The proposed model is computationally efficient thanks to its compact architecture having three main parts: a feature embedding to reduce dimensionality, self-attention for view-pooling, and dense layers for the classification. Experimental evaluation is performed using the HMC-QU-TAU dataset which consists of 160 patients with A2C and A4C view echocardiography recordings. The proposed SAF-Net model achieves a high-performance level with 88.26% precision, 77.64% sensitivity, and 78.13% accuracy. The results demonstrate that the SAF-Net model achieves the most accurate MI detection over multi-view echocardiography recordings.
</details>
<details>
<summary>摘要</summary>
我occidental infarction (MI) 是 coronary artery disease (CAD) 的严重情况，检测MI的检测是防止进一步对我ocardium的损害的关键。在这项研究中，我们提出了一种新的视图融合模型，名为自我注意力融合网络（SAF-Net），用于从多视图echo受检记录中检测MI。我们的框架使用了Apical 2-chamber（A2C）和Apical 4-chamber（A4C）视图echo受检记录，并从每个视图中提取了三个参照帧，并使用预训练的深度网络提取了高度表征特征。SAF-Net模型使用了自我注意力机制来学习视图之间的依赖关系。我们的模型具有紧凑的架构，包括特征嵌入、自我注意力 Pooling 和 dense层，这使得模型 computationally efficient。我们在HMC-QU-TAU数据集上进行了实验评估，该数据集包含160名患有A2C和A4C视图echo受检记录的患者。我们的SAF-Net模型在多视图echo受检记录中检测MI的性能达到了88.26%的精度、77.64%的敏感度和78.13%的准确率，这些结果证明了SAF-Net模型在多视图echo受检记录中的MI检测性能最高。
</details></li>
</ul>
<hr>
<h2 id="Defending-Against-Physical-Adversarial-Patch-Attacks-on-Infrared-Human-Detection"><a href="#Defending-Against-Physical-Adversarial-Patch-Attacks-on-Infrared-Human-Detection" class="headerlink" title="Defending Against Physical Adversarial Patch Attacks on Infrared Human Detection"></a>Defending Against Physical Adversarial Patch Attacks on Infrared Human Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15519">http://arxiv.org/abs/2309.15519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Strack, Futa Waseda, Huy H. Nguyen, Yinqiang Zheng, Isao Echizen</li>
<li>for: 本研究旨在提高红外检测系统的安全性，对Physically-Realizable Adversarial Patches（PRAP）的攻击进行防御。</li>
<li>methods: 我们提出了一种简单的防御策略——质量检测（POD），通过随机添加质量样本来增强训练样本，并在检测人员时同时检测质量样本。</li>
<li>results: POD不仅可以准确地检测人员，还可以识别质量样本的位置，并在不同的质量样本攻击下保持高度的鲁棒性。<details>
<summary>Abstract</summary>
Infrared detection is an emerging technique for safety-critical tasks owing to its remarkable anti-interference capability. However, recent studies have revealed that it is vulnerable to physically-realizable adversarial patches, posing risks in its real-world applications. To address this problem, we are the first to investigate defense strategies against adversarial patch attacks on infrared detection, especially human detection. We have devised a straightforward defense strategy, patch-based occlusion-aware detection (POD), which efficiently augments training samples with random patches and subsequently detects them. POD not only robustly detects people but also identifies adversarial patch locations. Surprisingly, while being extremely computationally efficient, POD easily generalizes to state-of-the-art adversarial patch attacks that are unseen during training. Furthermore, POD improves detection precision even in a clean (i.e., no-patch) situation due to the data augmentation effect. Evaluation demonstrated that POD is robust to adversarial patches of various shapes and sizes. The effectiveness of our baseline approach is shown to be a viable defense mechanism for real-world infrared human detection systems, paving the way for exploring future research directions.
</details>
<details>
<summary>摘要</summary>
红外检测是一种出现的技术，具有很好的防障特性，因此在安全关键任务中得到广泛应用。然而，最近的研究发现，红外检测系统容易受到physically realizable adversarial patches的威胁，这会影响其在实际应用中的安全性。为了解决这个问题，我们是第一个调查红外检测系统中 adversarial patch 攻击的防御策略，特别是人体检测。我们提出了一种简单的防御策略，即 patch-based occlusion-aware detection（POD），它可以增加训练样本中的随机贴图，并在后续检测它们。POD不仅可以准确检测人体，还可以识别隐藏在贴图中的敌意贴图位置。意外地，POD的计算效率非常低，同时它可以在未见过训练时的攻击中保持高效。此外，POD在清洁（即无贴图）情况下也可以提高检测精度，这是因为数据增强效果。我们的基线方法在不同形状和大小的敌意贴图攻击中都能够保持高效。这些结果表明，POD是一种可靠的防御策略，可以保护实际中的红外人体检测系统，开辟了未来研究的新途径。
</details></li>
</ul>
<hr>
<h2 id="DreamCom-Finetuning-Text-guided-Inpainting-Model-for-Image-Composition"><a href="#DreamCom-Finetuning-Text-guided-Inpainting-Model-for-Image-Composition" class="headerlink" title="DreamCom: Finetuning Text-guided Inpainting Model for Image Composition"></a>DreamCom: Finetuning Text-guided Inpainting Model for Image Composition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15508">http://arxiv.org/abs/2309.15508</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lingxiao Lu, Bo Zhang, Li Niu</li>
<li>for: 合成具有真实感的图像，即将前景对象渗透到背景图像中。</li>
<li>methods: 使用大量的前景和背景对象对 diffusion 模型进行预训练，以便在测试时直接应用到新的前景和背景对象。</li>
<li>results: 经验显示，使用这种方法可以快速并高效地生成高质量的合成图像，但是经常失去前景细节和显示明显的artefacts。<details>
<summary>Abstract</summary>
The goal of image composition is merging a foreground object into a background image to obtain a realistic composite image. Recently, generative composition methods are built on large pretrained diffusion models, due to their unprecedented image generation ability. They train a model on abundant pairs of foregrounds and backgrounds, so that it can be directly applied to a new pair of foreground and background at test time. However, the generated results often lose the foreground details and exhibit noticeable artifacts. In this work, we propose an embarrassingly simple approach named DreamCom inspired by DreamBooth. Specifically, given a few reference images for a subject, we finetune text-guided inpainting diffusion model to associate this subject with a special token and inpaint this subject in the specified bounding box. We also construct a new dataset named MureCom well-tailored for this task.
</details>
<details>
<summary>摘要</summary>
“目的是将前景物体合并到背景图像中，以获得实际的合成图像。现在，生成作业方法基于大量预训数据模型，因为它们可以实现前无之纪录的图像生成能力。它们在丰富的前景和背景组合中训练模型，以便在测试时直接应用到新的前景和背景。然而，生成结果经常失去前景细节，并表现出明显的错误。在这个工作中，我们提出了一个轻松简单的方法名为DreamCom，受 DreamBooth 的启发。具体来说，我们将一些对主题的参考图片给调整文本导向填充扩散模型，将主题与特殊的token相关，并在指定的矩形盒中填充这个主题。我们还建立了一个新的数据集名为MureCom，专门用于这个任务。”
</details></li>
</ul>
<hr>
<h2 id="Finite-Scalar-Quantization-VQ-VAE-Made-Simple"><a href="#Finite-Scalar-Quantization-VQ-VAE-Made-Simple" class="headerlink" title="Finite Scalar Quantization: VQ-VAE Made Simple"></a>Finite Scalar Quantization: VQ-VAE Made Simple</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15505">http://arxiv.org/abs/2309.15505</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research/google-research">https://github.com/google-research/google-research</a></li>
<li>paper_authors: Fabian Mentzer, David Minnen, Eirikur Agustsson, Michael Tschannen</li>
<li>for: The paper aims to propose a simple scheme for vector quantization (VQ) in the latent representation of VQ-VAEs, which is called finite scalar quantization (FSQ).</li>
<li>methods: The paper uses FSQ to project the VAE representation down to a few dimensions, and each dimension is quantized to a small set of fixed values. The authors use an appropriate choice of the number of dimensions and values each dimension can take to obtain the same codebook size as in VQ.</li>
<li>results: The authors train the same models that have been trained on VQ-VAE representations using FSQ, including autoregressive and masked transformer models for image generation, multimodal generation, and dense prediction computer vision tasks. Despite the much simpler design of FSQ, the authors obtain competitive performance in all these tasks.<details>
<summary>Abstract</summary>
We propose to replace vector quantization (VQ) in the latent representation of VQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where we project the VAE representation down to a few dimensions (typically less than 10). Each dimension is quantized to a small set of fixed values, leading to an (implicit) codebook given by the product of these sets. By appropriately choosing the number of dimensions and values each dimension can take, we obtain the same codebook size as in VQ. On top of such discrete representations, we can train the same models that have been trained on VQ-VAE representations. For example, autoregressive and masked transformer models for image generation, multimodal generation, and dense prediction computer vision tasks. Concretely, we employ FSQ with MaskGIT for image generation, and with UViM for depth estimation, colorization, and panoptic segmentation. Despite the much simpler design of FSQ, we obtain competitive performance in all these tasks. We emphasize that FSQ does not suffer from codebook collapse and does not need the complex machinery employed in VQ (commitment losses, codebook reseeding, code splitting, entropy penalties, etc.) to learn expressive discrete representations.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:我们提议将vector quantization（VQ）在VQ-VAE中的latent representation中 replaced with一种简单的方案 calledfinite scalar quantization（FSQ），其中我们将VAE表示下降到一些维度（通常小于10）。每个维度被quantized到一小集fixed values，导致一个由这些集组成的（含义）codebook。通过合适地选择维度和每个维度可以取的值的数量，我们可以获得与VQ的codebook大小一样的大小。在这些简单的抽象表示之上，我们可以训练与VQ-VAE表示相同的模型。例如，用于图像生成的autoregressive和masked transformer模型，以及用于计算机视觉任务的多modal生成和精密预测。具体来说，我们使用FSQ与MaskGIT进行图像生成，以及与UViM进行深度估计、色化和分割。尽管FSQ的设计非常简单，但我们在所有这些任务中都获得了竞争性的性能。我们强调FSQ不会出现codebook collapse，并且不需要VQ中使用的复杂机制（如承诺损失、codebook重新种子、code splitting、Entropy penalty等）来学习表示 expressive discrete representations。
</details></li>
</ul>
<hr>
<h2 id="Investigating-the-changes-in-BOLD-responses-during-viewing-of-images-with-varied-complexity-An-fMRI-time-series-based-analysis-on-human-vision"><a href="#Investigating-the-changes-in-BOLD-responses-during-viewing-of-images-with-varied-complexity-An-fMRI-time-series-based-analysis-on-human-vision" class="headerlink" title="Investigating the changes in BOLD responses during viewing of images with varied complexity: An fMRI time-series based analysis on human vision"></a>Investigating the changes in BOLD responses during viewing of images with varied complexity: An fMRI time-series based analysis on human vision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15495">http://arxiv.org/abs/2309.15495</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/naveen7102/fmri-time-series-classification">https://github.com/naveen7102/fmri-time-series-classification</a></li>
<li>paper_authors: Naveen Kanigiri, Manohar Suggula, Debanjali Bhattacharya, Neelam Sinha</li>
<li>for:  investigate the neurological variation of human brain responses during viewing of images with varied complexity using fMRI time series (TS) analysis.</li>
<li>methods:  employ classical machine learning and deep learning strategies to classify image complexity-specific fMRI TS, and perform temporal semantic segmentation on whole fMRI TS.</li>
<li>results:  established a baseline in studying how differently human brain functions while looking into images of diverse complexities, and provided insightful explanations for how static images with diverse complexities are perceived.<details>
<summary>Abstract</summary>
Functional MRI (fMRI) is widely used to examine brain functionality by detecting alteration in oxygenated blood flow that arises with brain activity. This work aims to investigate the neurological variation of human brain responses during viewing of images with varied complexity using fMRI time series (TS) analysis. Publicly available BOLD5000 dataset is used for this purpose which contains fMRI scans while viewing 5254 distinct images of diverse categories, drawn from three standard computer vision datasets: COCO, Imagenet and SUN. To understand vision, it is important to study how brain functions while looking at images of diverse complexities. Our first study employs classical machine learning and deep learning strategies to classify image complexity-specific fMRI TS, represents instances when images from COCO, Imagenet and SUN datasets are seen. The implementation of this classification across visual datasets holds great significance, as it provides valuable insights into the fluctuations in BOLD signals when perceiving images of varying complexities. Subsequently, temporal semantic segmentation is also performed on whole fMRI TS to segment these time instances. The obtained result of this analysis has established a baseline in studying how differently human brain functions while looking into images of diverse complexities. Therefore, accurate identification and distinguishing of variations in BOLD signals from fMRI TS data serves as a critical initial step in vision studies, providing insightful explanations for how static images with diverse complexities are perceived.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="CauDR-A-Causality-inspired-Domain-Generalization-Framework-for-Fundus-based-Diabetic-Retinopathy-Grading"><a href="#CauDR-A-Causality-inspired-Domain-Generalization-Framework-for-Fundus-based-Diabetic-Retinopathy-Grading" class="headerlink" title="CauDR: A Causality-inspired Domain Generalization Framework for Fundus-based Diabetic Retinopathy Grading"></a>CauDR: A Causality-inspired Domain Generalization Framework for Fundus-based Diabetic Retinopathy Grading</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15493">http://arxiv.org/abs/2309.15493</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Wei, Peilun Shi, Juzheng Miao, Minqing Zhang, Guitao Bai, Jianing Qiu, Furui Liu, Wu Yuan<br>for: 这个研究旨在提高computer-aided diabetic retinopathy grading system的准确性和一致性，以帮助镜外科医生快速识别和诊断。methods: 这个研究使用了 novel retinal imaging cameras 和 deep learning-based algorithms，并将 causality analysis 应用到模型架构中以减少域别差异的影响。results: 研究结果显示，这个新的条件预测架构（CauDR）能够减少域别差异的影响，并 achieves state-of-the-art 性能。<details>
<summary>Abstract</summary>
Diabetic retinopathy (DR) is the most common diabetic complication, which usually leads to retinal damage, vision loss, and even blindness. A computer-aided DR grading system has a significant impact on helping ophthalmologists with rapid screening and diagnosis. Recent advances in fundus photography have precipitated the development of novel retinal imaging cameras and their subsequent implementation in clinical practice. However, most deep learning-based algorithms for DR grading demonstrate limited generalization across domains. This inferior performance stems from variance in imaging protocols and devices inducing domain shifts. We posit that declining model performance between domains arises from learning spurious correlations in the data. Incorporating do-operations from causality analysis into model architectures may mitigate this issue and improve generalizability. Specifically, a novel universal structural causal model (SCM) was proposed to analyze spurious correlations in fundus imaging. Building on this, a causality-inspired diabetic retinopathy grading framework named CauDR was developed to eliminate spurious correlations and achieve more generalizable DR diagnostics. Furthermore, existing datasets were reorganized into 4DR benchmark for DG scenario. Results demonstrate the effectiveness and the state-of-the-art (SOTA) performance of CauDR.
</details>
<details>
<summary>摘要</summary>
糖尿病 retinopathy (DR) 是糖尿病最常见的侵犯，通常会导致视力损害、视力损伤和甚至是失明。一个计算机支持的 DR 分级系统有助于医生快速评估和诊断。在最近的投影照相技术发展后，新的视觉内部影像相机被实施到临床实践中。但大多数深度学习基于的 DR 分级算法显示有限的应用普遍性。这是由于几何图像协议和设备之间的差异引起的领域转移。我们认为，模型在不同领域之间的性能下降是由于学习伪的相互关联。将 causality 分析中的动作从事件给到模型架构中可能将这个问题解决，并提高普遍性。特别是，一个新的通用结构 causality 模型 (SCM) 被提出供分析视觉内部影像中的伪的相互关联。基于这个 SCM，一个以 causality 为基础的糖尿病 retinopathy 分级框架 (CauDR) 被开发，以消除伪的相互关联并 дости得更高的普遍性。此外，现有的数据被重新排序为 4DR 参考景，结果显示 CauDR 的效果和顶尖性能。
</details></li>
</ul>
<hr>
<h2 id="Survey-on-Deep-Face-Restoration-From-Non-blind-to-Blind-and-Beyond"><a href="#Survey-on-Deep-Face-Restoration-From-Non-blind-to-Blind-and-Beyond" class="headerlink" title="Survey on Deep Face Restoration: From Non-blind to Blind and Beyond"></a>Survey on Deep Face Restoration: From Non-blind to Blind and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15490">http://arxiv.org/abs/2309.15490</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/24wenjie-li/awesome-face-restoration">https://github.com/24wenjie-li/awesome-face-restoration</a></li>
<li>paper_authors: Wenjie Li, Mei Wang, Kai Zhang, Juncheng Li, Xiaoming Li, Yuhang Zhang, Guangwei Gao, Weihong Deng, Chia-Wen Lin</li>
<li>for: 本研究目的是为了提高低质量（LQ）图像的面部图像修复（FR）技术。</li>
<li>methods: 本文首先检视了现实中常见的LQ图像因素，并介绍了用于生成LQ图像的降低技术。然后， authors分类了FR方法按照不同的任务，并讲解它们的发展历程。此外， authors还介绍了常见的面部优先级，并讨论了如何提高它们的效果。</li>
<li>results: 在实验部分， authors 全面评估了当前最佳FR方法的性能 across 多个任务，并从不同的角度分析它们的表现。Note: The “24wenjie-li” in the repository URL is the name of the author, not a typo.<details>
<summary>Abstract</summary>
Face restoration (FR) is a specialized field within image restoration that aims to recover low-quality (LQ) face images into high-quality (HQ) face images. Recent advances in deep learning technology have led to significant progress in FR methods. In this paper, we begin by examining the prevalent factors responsible for real-world LQ images and introduce degradation techniques used to synthesize LQ images. We also discuss notable benchmarks commonly utilized in the field. Next, we categorize FR methods based on different tasks and explain their evolution over time. Furthermore, we explore the various facial priors commonly utilized in the restoration process and discuss strategies to enhance their effectiveness. In the experimental section, we thoroughly evaluate the performance of state-of-the-art FR methods across various tasks using a unified benchmark. We analyze their performance from different perspectives. Finally, we discuss the challenges faced in the field of FR and propose potential directions for future advancements. The open-source repository corresponding to this work can be found at https:// github.com/ 24wenjie-li/ Awesome-Face-Restoration.
</details>
<details>
<summary>摘要</summary>
面部恢复（FR）是图像恢复的一个专业领域，旨在将低质量（LQ）的面部图像恢复到高质量（HQ）的面部图像。 current deep learning技术的进步已经导致FR方法得到了重要的进步。在这篇论文中，我们首先检查了实际中LQ图像的主要因素，并介绍了用于生成LQ图像的降低技术。我们还讨论了在领域中常用的标准 bencmarks。然后，我们将FR方法分为不同任务，并解释它们的演化历史。此外，我们探讨了常用的面部先验和如何提高它们的效果。在实验部分，我们对现今FR方法的性能进行了广泛的评估，使用了一个统一的 bencmark。我们从不同的角度分析了它们的性能。最后，我们讨论了FR领域面临的挑战和未来的发展方向。相关的开源存储库可以在https://github.com/24wenjie-li/Awesome-Face-Restoration中找到。
</details></li>
</ul>
<hr>
<h2 id="Tackling-VQA-with-Pretrained-Foundation-Models-without-Further-Training"><a href="#Tackling-VQA-with-Pretrained-Foundation-Models-without-Further-Training" class="headerlink" title="Tackling VQA with Pretrained Foundation Models without Further Training"></a>Tackling VQA with Pretrained Foundation Models without Further Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15487">http://arxiv.org/abs/2309.15487</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alvin De Jun Tan, Bingquan Shen</li>
<li>for: 这个论文的目的是探讨如何使用预训练的大语言模型（LLMs）解决视觉问答（VQA）问题，无需进一步训练。</li>
<li>methods: 这个论文使用了将预训练的LLMs和其他基础模型结合使用，以便在不进一步训练的情况下解决VQA问题。文章探讨了不同的解码策略来生成图像的文本表示，并评估了其性能在VQAv2数据集上。</li>
<li>results: 研究发现，通过使用自然语言来表示图像，LLMs可以快速理解图像，并且不需要进一步训练。不同的解码策略对图像的文本表示具有不同的性能，但是综合评估结果表明，使用自然语言来表示图像是一个有效的方法。<details>
<summary>Abstract</summary>
Large language models (LLMs) have achieved state-of-the-art results in many natural language processing tasks. They have also demonstrated ability to adapt well to different tasks through zero-shot or few-shot settings. With the capability of these LLMs, researchers have looked into how to adopt them for use with Visual Question Answering (VQA). Many methods require further training to align the image and text embeddings. However, these methods are computationally expensive and requires large scale image-text dataset for training. In this paper, we explore a method of combining pretrained LLMs and other foundation models without further training to solve the VQA problem. The general idea is to use natural language to represent the images such that the LLM can understand the images. We explore different decoding strategies for generating textual representation of the image and evaluate their performance on the VQAv2 dataset.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Transferability-of-Representations-Learned-using-Supervised-Contrastive-Learning-Trained-on-a-Multi-Domain-Dataset"><a href="#Transferability-of-Representations-Learned-using-Supervised-Contrastive-Learning-Trained-on-a-Multi-Domain-Dataset" class="headerlink" title="Transferability of Representations Learned using Supervised Contrastive Learning Trained on a Multi-Domain Dataset"></a>Transferability of Representations Learned using Supervised Contrastive Learning Trained on a Multi-Domain Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15486">http://arxiv.org/abs/2309.15486</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alvin De Jun Tan, Clement Tan, Chai Kiat Yeo</li>
<li>for: 本研究使用 Supervised Contrastive Learning 框架来学习 DomainNet 多域数据集上的表示，并评估这些表示的传递性在不同域的下游数据集上。</li>
<li>methods: 本研究使用 Supervised Contrastive Learning 框架，并使用 fixed feature linear evaluation protocol 评估表示的传递性。</li>
<li>results: 实验结果显示，Supervised Contrastive Learning 模型在 7 个不同域的下游数据集上的平均表现比基eline模型优于 6.05%。这些结果表明，Supervised Contrastive Learning 模型可能可以在多域数据集上学习更robust的表示，并且这些表示可以更好地传递到其他域。<details>
<summary>Abstract</summary>
Contrastive learning has shown to learn better quality representations than models trained using cross-entropy loss. They also transfer better to downstream datasets from different domains. However, little work has been done to explore the transferability of representations learned using contrastive learning when trained on a multi-domain dataset. In this paper, a study has been conducted using the Supervised Contrastive Learning framework to learn representations from the multi-domain DomainNet dataset and then evaluate the transferability of the representations learned on other downstream datasets. The fixed feature linear evaluation protocol will be used to evaluate the transferability on 7 downstream datasets that were chosen across different domains. The results obtained are compared to a baseline model that was trained using the widely used cross-entropy loss. Empirical results from the experiments showed that on average, the Supervised Contrastive Learning model performed 6.05% better than the baseline model on the 7 downstream datasets. The findings suggest that Supervised Contrastive Learning models can potentially learn more robust representations that transfer better across domains than cross-entropy models when trained on a multi-domain dataset.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用对比学习可以学习出较高质量的表示，并且这些表示可以更好地在不同领域下转移。然而，对多领域数据集上使用对比学习学习表示的转移性还未得到充分研究。本文通过使用Supervised Contrastive Learning框架，从多领域数据集DomainNet上学习表示，然后使用 fixes 特征线性评估协议评估这些表示在不同领域下的转移性。结果表明，对7个下游数据集进行比较，Supervised Contrastive Learning模型在平均上比基线模型6.05%更好。这些结果表明，Supervised Contrastive Learning模型可能可以在多领域数据集上学习更加稳定的表示，并且这些表示可以更好地转移到不同领域。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Style-Transfer-and-Self-Supervised-Learning-Powered-Myocardium-Infarction-Super-Resolution-Segmentation"><a href="#Style-Transfer-and-Self-Supervised-Learning-Powered-Myocardium-Infarction-Super-Resolution-Segmentation" class="headerlink" title="Style Transfer and Self-Supervised Learning Powered Myocardium Infarction Super-Resolution Segmentation"></a>Style Transfer and Self-Supervised Learning Powered Myocardium Infarction Super-Resolution Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15485">http://arxiv.org/abs/2309.15485</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lichao Wang, Jiahao Huang, Xiaodan Xing, Yinzhe Wu, Ramyah Rajakulasingam, Andrew D. Scott, Pedro F Ferreira, Ranil De Silva, Sonia Nielles-Vallespin, Guang Yang</li>
<li>For: The paper aims to enhance diffusion tensor imaging (DTI) images by translating them into the late gadolinium enhancement (LGE) domain, which offers a larger amount of data with high-resolution and distinct highlighting of myocardium infarction (MI) areas.* Methods: The proposed pipeline incorporates a novel style transfer model and a simultaneous super-resolution and segmentation model. An end-to-end super-resolution segmentation model is introduced to generate high-resolution mask from low-resolution LGE style DTI image. A multi-task self-supervised learning strategy is employed to pre-train the super-resolution segmentation model.* Results: The proposed pipeline is expected to enhance the performance of the segmentation model by acquiring more representative knowledge and improving its segmentation performance after fine-tuning.Here is the simplified Chinese version of the three key points:* For: 这篇论文目标是使用晚期加多林酸增强图像（LGE）域来提高扩散tensor成像（DTI）图像的分辨率和干扰率。* Methods: 提议的管道包括一种新的样式传输模型和同时的超解像和分割模型。该模型可以将低分辨率LGE样式DTI图像转换为高分辨率mask。* Results: 提议的管道可以提高分割模型的性能，并且可以通过自动预训练和微调来提高分割性能。<details>
<summary>Abstract</summary>
This study proposes a pipeline that incorporates a novel style transfer model and a simultaneous super-resolution and segmentation model. The proposed pipeline aims to enhance diffusion tensor imaging (DTI) images by translating them into the late gadolinium enhancement (LGE) domain, which offers a larger amount of data with high-resolution and distinct highlighting of myocardium infarction (MI) areas. Subsequently, the segmentation task is performed on the LGE style image. An end-to-end super-resolution segmentation model is introduced to generate high-resolution mask from low-resolution LGE style DTI image. Further, to enhance the performance of the model, a multi-task self-supervised learning strategy is employed to pre-train the super-resolution segmentation model, allowing it to acquire more representative knowledge and improve its segmentation performance after fine-tuning. https: github.com/wlc2424762917/Med_Img
</details>
<details>
<summary>摘要</summary>
这个研究提出了一个管道，其中包括一种新的风格传输模型和同时的超高分辨率和分割模型。该管道的目标是通过将扩散tensor图像（DTI）转换到晚期加多林革命（LGE）域中，以获得更多的数据，高分辨率和明确驳批我OCARD Infarction（MI）区域。然后，对LGE风格图像进行分割任务。该研究引入了一种端到端超高分辨率分割模型，以生成高分辨率mask从低分辨率LGE风格DTI图像中。此外，为了提高模型的性能，该研究采用了多任务自主学习策略，将超高分辨率分割模型在先修改后 fine-tuning 中进行自我调节。References:* DTI: diffusion tensor imaging* LGE: late gadolinium enhancement* MI: myocardium infarction* SR: super-resolution* Segmentation: 分割
</details></li>
</ul>
<hr>
<h2 id="The-Robust-Semantic-Segmentation-UNCV2023-Challenge-Results"><a href="#The-Robust-Semantic-Segmentation-UNCV2023-Challenge-Results" class="headerlink" title="The Robust Semantic Segmentation UNCV2023 Challenge Results"></a>The Robust Semantic Segmentation UNCV2023 Challenge Results</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15478">http://arxiv.org/abs/2309.15478</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuanlong Yu, Yi Zuo, Zitao Wang, Xiaowen Zhang, Jiaxuan Zhao, Yuting Yang, Licheng Jiao, Rui Peng, Xinyi Wang, Junpei Zhang, Kexin Zhang, Fang Liu, Roberto Alcover-Couso, Juan C. SanMiguel, Marcos Escudero-Viñolo, Hanlin Tian, Kenta Matsui, Tianhao Wang, Fahmy Adan, Zhitong Gao, Xuming He, Quentin Bouniot, Hossein Moghaddam, Shyam Nandan Rai, Fabio Cermelli, Carlo Masone, Andrea Pilzer, Elisa Ricci, Andrei Bursuc, Arno Solin, Martin Trapp, Rui Li, Angela Yao, Wenlong Chen, Ivor Simpson, Neill D. F. Campbell, Gianni Franchi</li>
<li>for: 本文描述了在ICCV 2023 年举行的 MUAD 不确定量评估挑战中使用的赢利解决方案。挑战的目标是提高城市环境下的semantic segmentation robustness，特别是面对自然的反对抗情况下。</li>
<li>methods: 本文介绍了参与挑战的19个提交的方法，其中许多技术启发自过去几年Computer Vision和Machine Learning领域的主要会议和学术期刊上的先进uncertainty quantification方法。</li>
<li>results: 本文介绍了挑战的topperforming解决方案，并提供了所有参与者使用的多种方法的全面概述，以便让读者更深入地了解城市环境下的semantic segmentation中的不确定性处理策略。<details>
<summary>Abstract</summary>
This paper outlines the winning solutions employed in addressing the MUAD uncertainty quantification challenge held at ICCV 2023. The challenge was centered around semantic segmentation in urban environments, with a particular focus on natural adversarial scenarios. The report presents the results of 19 submitted entries, with numerous techniques drawing inspiration from cutting-edge uncertainty quantification methodologies presented at prominent conferences in the fields of computer vision and machine learning and journals over the past few years. Within this document, the challenge is introduced, shedding light on its purpose and objectives, which primarily revolved around enhancing the robustness of semantic segmentation in urban scenes under varying natural adversarial conditions. The report then delves into the top-performing solutions. Moreover, the document aims to provide a comprehensive overview of the diverse solutions deployed by all participants. By doing so, it seeks to offer readers a deeper insight into the array of strategies that can be leveraged to effectively handle the inherent uncertainties associated with autonomous driving and semantic segmentation, especially within urban environments.
</details>
<details>
<summary>摘要</summary>
The challenge aimed to enhance the robustness of semantic segmentation in urban scenes under varying natural adversarial conditions. The report introduces the challenge and its objectives, and then delves into the top-performing solutions. Additionally, the document provides a comprehensive overview of the diverse solutions deployed by all participants, offering readers a deeper understanding of the array of strategies that can be used to effectively handle the inherent uncertainties associated with autonomous driving and semantic segmentation, particularly within urban environments.Translated into Simplified Chinese:这份报告详细介绍了在ICCV 2023 年举行的 MUAD 不确定量化挑战的赢家解决方案。挑战的主要目标是在城市环境下进行 semantic segmentation，特别是在自然难题下进行。报告介绍了19个提交的解决方案，其中许多技术受到了过去几年计算机视觉和机器学习领域的 prominent 会议和学术期刊上的uncertainty quantification方法的 inspirations。挑战的目的是提高城市景观下 semantic segmentation 的 robustness，特别是在自然难题下。报告 introduce 了挑战和其目标，然后它介绍了top-performing 的解决方案。此外，报告还提供了所有参与者所使用的多种解决方案的全面概述，以便让读者更深入地了解 autonomous driving 和 semantic segmentation 中的不确定性，特别是在城市环境下。
</details></li>
</ul>
<hr>
<h2 id="A-Tutorial-on-Uniform-B-Spline"><a href="#A-Tutorial-on-Uniform-B-Spline" class="headerlink" title="A Tutorial on Uniform B-Spline"></a>A Tutorial on Uniform B-Spline</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15477">http://arxiv.org/abs/2309.15477</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Zhou</li>
<li>for: 本文探讨了非常规B-spline的一致性和矩阵表示。</li>
<li>methods: 本文使用了矩阵理论和计算机科学技术来研究非常规B-spline的一致性和矩阵表示。</li>
<li>results: 本文得到了一种新的非常规B-spline的矩阵表示方法，并且验证了该方法的正确性和有效性。<details>
<summary>Abstract</summary>
This document facilitates understanding of core concepts about uniform B-spline and its matrix representation.
</details>
<details>
<summary>摘要</summary>
这份文档帮助理解均匀B-spline的核心概念以及其矩阵表示方法。Here's the breakdown of the translation:* 这份文档 (zhè fāng wén dào) - This document* 帮助理解 (bāng zhù lǐ jiě) - Facilitates understanding* 均匀B-spline (jūn yí B-spline) - Uniform B-spline* 核心概念 (gōng zhī yù yì) - Core concepts* 以及 (yǐ yú) - And* 矩阵表示方法 (pí zhāng biǎo yì fāng yì) - Matrix representation method
</details></li>
</ul>
<hr>
<h2 id="Cross-Dataset-Experimental-Study-of-Radar-Camera-Fusion-in-Bird’s-Eye-View"><a href="#Cross-Dataset-Experimental-Study-of-Radar-Camera-Fusion-in-Bird’s-Eye-View" class="headerlink" title="Cross-Dataset Experimental Study of Radar-Camera Fusion in Bird’s-Eye View"></a>Cross-Dataset Experimental Study of Radar-Camera Fusion in Bird’s-Eye View</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15465">http://arxiv.org/abs/2309.15465</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Stäcker, Philipp Heidenreich, Jason Rambach, Didier Stricker</li>
<li>for: 提高自动驾驶系统的可靠性和可靠性，通过利用卫星信息和摄像头的融合。</li>
<li>methods: 提出一种新的和灵活的融合网络，并在nuScenes和View-of-Delft两个数据集上进行测试。</li>
<li>results: 研究发现，摄像头分支需要大量和多样化的训练数据，而雷达分支受益于高性能的雷达。通过传输学习，我们提高了摄像头的性能在较小的数据集上。结果还表明，雷达-摄像头融合方法在相比摄像头只和雷达只的基准下显著超越。<details>
<summary>Abstract</summary>
By exploiting complementary sensor information, radar and camera fusion systems have the potential to provide a highly robust and reliable perception system for advanced driver assistance systems and automated driving functions. Recent advances in camera-based object detection offer new radar-camera fusion possibilities with bird's eye view feature maps. In this work, we propose a novel and flexible fusion network and evaluate its performance on two datasets: nuScenes and View-of-Delft. Our experiments reveal that while the camera branch needs large and diverse training data, the radar branch benefits more from a high-performance radar. Using transfer learning, we improve the camera's performance on the smaller dataset. Our results further demonstrate that the radar-camera fusion approach significantly outperforms the camera-only and radar-only baselines.
</details>
<details>
<summary>摘要</summary>
通过利用相 complementary 的感知信息，雷达和摄像头融合系统具有提供高度可靠和可靠的感知系统的潜在能力，以用于先进的驾驶助手和自动驾驶功能。最新的摄像头基于物体检测技术开创了新的雷达-摄像头融合可能性，包括 bird's eye view 特征地图。在这种工作中，我们提出了一种新的和灵活的融合网络，并评估其性能在 nuScenes 和 View-of-Delft 两个数据集上。我们的实验表明，虽然摄像头分支需要大量和多样化的训练数据，但雷达分支受益于高性能的雷达。通过传输学习，我们改进了摄像头在较小的数据集上的性能。我们的结果还证明了雷达-摄像头融合方法在相对于摄像头Only 和雷达Only 基eline上显著超越。
</details></li>
</ul>
<hr>
<h2 id="GAMMA-Graspability-Aware-Mobile-MAnipulation-Policy-Learning-based-on-Online-Grasping-Pose-Fusion"><a href="#GAMMA-Graspability-Aware-Mobile-MAnipulation-Policy-Learning-based-on-Online-Grasping-Pose-Fusion" class="headerlink" title="GAMMA: Graspability-Aware Mobile MAnipulation Policy Learning based on Online Grasping Pose Fusion"></a>GAMMA: Graspability-Aware Mobile MAnipulation Policy Learning based on Online Grasping Pose Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15459">http://arxiv.org/abs/2309.15459</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiazhao Zhang, Nandiraju Gireesh, Jilong Wang, Xiaomeng Fang, Chaoyi Xu, Weiguang Chen, Liu Dai, He Wang</li>
<li>for: 本研究旨在提高机器人助手的移动抓取能力，增强机器人在不稳定环境中的抓取能力。</li>
<li>methods: 该研究提出了一种基于在线抓取姿态融合框架的抓取可见性感知方法，可以在实时下对抓取姿态进行筛选和融合，从而实现时间上的一致性。</li>
<li>results: 该方法可以减少红UND grasping pose的数量，同时提高抓取姿态质量，从而提高机器人的移动抓取能力。<details>
<summary>Abstract</summary>
Mobile manipulation constitutes a fundamental task for robotic assistants and garners significant attention within the robotics community. A critical challenge inherent in mobile manipulation is the effective observation of the target while approaching it for grasping. In this work, we propose a graspability-aware mobile manipulation approach powered by an online grasping pose fusion framework that enables a temporally consistent grasping observation. Specifically, the predicted grasping poses are online organized to eliminate the redundant, outlier grasping poses, which can be encoded as a grasping pose observation state for reinforcement learning. Moreover, on-the-fly fusing the grasping poses enables a direct assessment of graspability, encompassing both the quantity and quality of grasping poses.
</details>
<details>
<summary>摘要</summary>
Mobile manipulation 是Robotic assistant的基本任务，在Robotics community中吸引了广泛的关注。一个关键的挑战在于有效地观察目标而 approaching 它进行抓取。在这种工作中，我们提议一种基于在线抓取姿 pose 融合框架的抓取可能性感知approach，使抓取观察得到时间协调。具体来说，预测的抓取姿 pose 被在线组织，以消除重复和异常的抓取姿 pose，这些可以编码为抓取观察状态 для reinforcement learning。此外，在线融合抓取姿 pose 直接评估抓取可能性，包括抓取姿 pose 的量和质量。
</details></li>
</ul>
<hr>
<h2 id="Semantics-Driven-Cloud-Edge-Collaborative-Inference"><a href="#Semantics-Driven-Cloud-Edge-Collaborative-Inference" class="headerlink" title="Semantics-Driven Cloud-Edge Collaborative Inference"></a>Semantics-Driven Cloud-Edge Collaborative Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15435">http://arxiv.org/abs/2309.15435</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuche Gao, Beibei Zhang</li>
<li>for: 这篇论文主要针对智能城市应用中的视频数据进行高效分析，以智能交通为例。</li>
<li>methods: 本论文提出了一个基于 semantics 的云端-边缘协作方法，将视频分析过程分为两个阶段：在边缘服务器上提取视频内容的 semantics (车牌号码图像)，然后将该内容提交到云端或边缘服务器进行识别。这种分析方法可以降低端到端传输时间和提高throughput。</li>
<li>results: 实验结果显示，相比于将所有视频分析工作负载到云端或边缘服务器进行处理，这种云端-边缘协作方法可以提高端到端传输速度（最多5倍快）、throughput（最多9帧&#x2F;秒）和网页流量量（50%减少）。这显示了这种方法的有效性。<details>
<summary>Abstract</summary>
With the proliferation of video data in smart city applications like intelligent transportation, efficient video analytics has become crucial but also challenging. This paper proposes a semantics-driven cloud-edge collaborative approach for accelerating video inference, using license plate recognition as a case study. The method separates semantics extraction and recognition, allowing edge servers to only extract visual semantics (license plate patches) from video frames and offload computation-intensive recognition to the cloud or neighboring edges based on load. This segmented processing coupled with a load-aware work distribution strategy aims to reduce end-to-end latency and improve throughput. Experiments demonstrate significant improvements in end-to-end inference speed (up to 5x faster), throughput (up to 9 FPS), and reduced traffic volumes (50% less) compared to cloud-only or edge-only processing, validating the efficiency of the proposed approach. The cloud-edge collaborative framework with semantics-driven work partitioning provides a promising solution for scaling video analytics in smart cities.
</details>
<details>
<summary>摘要</summary>
随着智能城市应用中视频数据的普遍化，高效的视频分析已成为非常重要，同时也变得非常困难。这篇论文提议一种基于 semantics的云端-边缘集成方法，用 license plate recognition 作为案例研究。该方法将 semantics 提取和识别分开，让边缘服务器只提取视频帧中的视觉 semantics（车牌补丁），并将 computation-intensive 识别 tasks 提交到云或邻近的边缘服务器进行处理，根据负载情况进行异步分配工作。这种分解处理和根据负载情况分配工作的策略，可以降低端到端 latency 和提高吞吐量。实验结果显示，与云只处理或边缘只处理相比，提出的方法可以提高端到端推理速度（最多5倍）、吞吐量（最多9 FPS）和发送流量量（50% 降低）。云端-边缘集成框架，带有基于 semantics 的工作分配策略，为视频分析在智能城市中扩大 scalability 提供了一个可靠的解决方案。
</details></li>
</ul>
<hr>
<h2 id="NeuRBF-A-Neural-Fields-Representation-with-Adaptive-Radial-Basis-Functions"><a href="#NeuRBF-A-Neural-Fields-Representation-with-Adaptive-Radial-Basis-Functions" class="headerlink" title="NeuRBF: A Neural Fields Representation with Adaptive Radial Basis Functions"></a>NeuRBF: A Neural Fields Representation with Adaptive Radial Basis Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15426">http://arxiv.org/abs/2309.15426</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/oppo-us-research/NeuRBF">https://github.com/oppo-us-research/NeuRBF</a></li>
<li>paper_authors: Zhang Chen, Zhong Li, Liangchen Song, Lele Chen, Jingyi Yu, Junsong Yuan, Yi Xu</li>
<li>for: 本研究目的是提出一种基于总体卷积的神经场，以更好地表示许多应用中的维度灵活的信号。</li>
<li>methods: 本研究使用普通卷积函数作为信号表示，而不是传统的格子化神经场。这种方法可以更好地适应目标信号，并且可以通过组合多个频率弧函数来扩展卷积基底的通道 capacities。</li>
<li>results: 实验表明，对于2D图像和3D签名距离场表示，我们的方法可以达到更高的准确率和更小的模型大小，而且与优化的权重分配策略相比，我们的方法可以更好地适应不同类型的信号。当应用于神经辉场重建时，我们的方法可以达到最新的 Rendering 质量，同时具有小型模型和相对较快的训练速度。<details>
<summary>Abstract</summary>
We present a novel type of neural fields that uses general radial bases for signal representation. State-of-the-art neural fields typically rely on grid-based representations for storing local neural features and N-dimensional linear kernels for interpolating features at continuous query points. The spatial positions of their neural features are fixed on grid nodes and cannot well adapt to target signals. Our method instead builds upon general radial bases with flexible kernel position and shape, which have higher spatial adaptivity and can more closely fit target signals. To further improve the channel-wise capacity of radial basis functions, we propose to compose them with multi-frequency sinusoid functions. This technique extends a radial basis to multiple Fourier radial bases of different frequency bands without requiring extra parameters, facilitating the representation of details. Moreover, by marrying adaptive radial bases with grid-based ones, our hybrid combination inherits both adaptivity and interpolation smoothness. We carefully designed weighting schemes to let radial bases adapt to different types of signals effectively. Our experiments on 2D image and 3D signed distance field representation demonstrate the higher accuracy and compactness of our method than prior arts. When applied to neural radiance field reconstruction, our method achieves state-of-the-art rendering quality, with small model size and comparable training speed.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新型的神经场，使用通用径向基数进行信号表示。现有的神经场通常采用网格基于表示方式，将本地神经特征存储在网格节点上，并使用 N 维线性核函数来在连续查询点上 interpolate 特征。这些神经特征的空间位置固定在网格节点上，无法适应目标信号。而我们的方法则基于通用径向基数，具有更高的空间适应性和能够更好地适应目标信号。为了进一步提高扁球基数的通道 capacitance，我们提议将其与多频环相互融合。这种技术可以无需更多参数，将扁球基数扩展到多个频环 band 中，以便更好地表示细节。此外，我们还提出了一种将 adaptive 扁球基数与网格基数结合的 гибрид组合，以便继承两者的适应性和插值平滑性。我们仔细设计了权重分配方案，使 radial bases 能够有效地适应不同类型的信号。我们的实验表明，对 2D 图像和 3D 签名距离场表示，我们的方法比先前艺术高得多，而且模型尺寸和训练速度也相对较小。当应用于神经辐射场重建时，我们的方法实现了状态艺术的渲染质量，具有小型模型和相对较快的训练速度。
</details></li>
</ul>
<hr>
<h2 id="Inherit-with-Distillation-and-Evolve-with-Contrast-Exploring-Class-Incremental-Semantic-Segmentation-Without-Exemplar-Memory"><a href="#Inherit-with-Distillation-and-Evolve-with-Contrast-Exploring-Class-Incremental-Semantic-Segmentation-Without-Exemplar-Memory" class="headerlink" title="Inherit with Distillation and Evolve with Contrast: Exploring Class Incremental Semantic Segmentation Without Exemplar Memory"></a>Inherit with Distillation and Evolve with Contrast: Exploring Class Incremental Semantic Segmentation Without Exemplar Memory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15413">http://arxiv.org/abs/2309.15413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Danpei Zhao, Bo Yuan, Zhenwei Shi</li>
<li>For: Addressing class incremental semantic segmentation (CISS) without exemplar memory and resolving catastrophic forgetting and semantic drift simultaneously.* Methods: Proposed method IDEC consists of Dense Knowledge Distillation on all Aspects (DADA) and Asymmetric Region-wise Contrastive Learning (ARCL) modules, with a dynamic class-specific pseudo-labelling strategy.* Results: Achieved state-of-the-art performance on multiple CISS tasks, with superior anti-forgetting ability, particularly in multi-step CISS tasks.<details>
<summary>Abstract</summary>
As a front-burner problem in incremental learning, class incremental semantic segmentation (CISS) is plagued by catastrophic forgetting and semantic drift. Although recent methods have utilized knowledge distillation to transfer knowledge from the old model, they are still unable to avoid pixel confusion, which results in severe misclassification after incremental steps due to the lack of annotations for past and future classes. Meanwhile data-replay-based approaches suffer from storage burdens and privacy concerns. In this paper, we propose to address CISS without exemplar memory and resolve catastrophic forgetting as well as semantic drift synchronously. We present Inherit with Distillation and Evolve with Contrast (IDEC), which consists of a Dense Knowledge Distillation on all Aspects (DADA) manner and an Asymmetric Region-wise Contrastive Learning (ARCL) module. Driven by the devised dynamic class-specific pseudo-labelling strategy, DADA distils intermediate-layer features and output-logits collaboratively with more emphasis on semantic-invariant knowledge inheritance. ARCL implements region-wise contrastive learning in the latent space to resolve semantic drift among known classes, current classes, and unknown classes. We demonstrate the effectiveness of our method on multiple CISS tasks by state-of-the-art performance, including Pascal VOC 2012, ADE20K and ISPRS datasets. Our method also shows superior anti-forgetting ability, particularly in multi-step CISS tasks.
</details>
<details>
<summary>摘要</summary>
为了解决逐步学习中的前燃问题，我们提出了一种不使用示例内存的类增量 semantic segmentation（CISS）方法，它可以同时解决悬峰忘记和 semantics 漂移问题。 although recent methods have used knowledge distillation to transfer knowledge from the old model, they are still unable to avoid pixel confusion, which results in severe misclassification after incremental steps due to the lack of annotations for past and future classes. Meanwhile, data-replay-based approaches suffer from storage burdens and privacy concerns.在这篇论文中，我们提出了一种不使用示例内存的 CISS 方法，可以同时解决悬峰忘记和 semantics 漂移问题。我们称之为 Inherit with Distillation and Evolve with Contrast (IDEC)，它包括一种 dense knowledge distillation on all aspects (DADA) 方法和一种 asymmetric region-wise contrastive learning (ARCL) 模块。通过我们制定的动态类pecific pseudo-labeling策略，DADA 可以在中间层次和输出层之间兼容知识，同时更强调semantic-invariant知识继承。ARCL 实现了在 latent space 中进行区域 wise contrastive learning，以解决 semantics 漂移问题。我们在多个 CISS 任务上示出了我们的方法的效果，包括 Pascal VOC 2012、ADE20K 和 ISPRS 数据集。我们的方法还显示了在多 step CISS 任务中的超越抗忘记能力。
</details></li>
</ul>
<hr>
<h2 id="3D-Multiple-Object-Tracking-on-Autonomous-Driving-A-Literature-Review"><a href="#3D-Multiple-Object-Tracking-on-Autonomous-Driving-A-Literature-Review" class="headerlink" title="3D Multiple Object Tracking on Autonomous Driving: A Literature Review"></a>3D Multiple Object Tracking on Autonomous Driving: A Literature Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15411">http://arxiv.org/abs/2309.15411</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Zhang, Xin Li, Liang He, Xin Lin</li>
<li>for: 这篇论文主要探讨了三维多 объек图跟踪（3D MOT）领域的研究状况，并提出了未来研究的可能性。</li>
<li>methods: 本论文使用了系统性的描述和分析方法，对3D MOT领域的各种方法进行了分类和评价，并提供了一份概括性的实验指标。</li>
<li>results: 本论文的研究结果表明，3D MOT领域还存在许多挑战，如 объек形变、隐藏、小目标、数据缺乏、检测失败等问题。同时，本论文还提出了未来研究的可能性，以帮助解决这些挑战。<details>
<summary>Abstract</summary>
3D multi-object tracking (3D MOT) stands as a pivotal domain within autonomous driving, experiencing a surge in scholarly interest and commercial promise over recent years. Despite its paramount significance, 3D MOT confronts a myriad of formidable challenges, encompassing abrupt alterations in object appearances, pervasive occlusion, the presence of diminutive targets, data sparsity, missed detections, and the unpredictable initiation and termination of object motion trajectories. Countless methodologies have emerged to grapple with these issues, yet 3D MOT endures as a formidable problem that warrants further exploration. This paper undertakes a comprehensive examination, assessment, and synthesis of the research landscape in this domain, remaining attuned to the latest developments in 3D MOT while suggesting prospective avenues for future investigation. Our exploration commences with a systematic exposition of key facets of 3D MOT and its associated domains, including problem delineation, classification, methodological approaches, fundamental principles, and empirical investigations. Subsequently, we categorize these methodologies into distinct groups, dissecting each group meticulously with regard to its challenges, underlying rationale, progress, merits, and demerits. Furthermore, we present a concise recapitulation of experimental metrics and offer an overview of prevalent datasets, facilitating a quantitative comparison for a more intuitive assessment. Lastly, our deliberations culminate in a discussion of the prevailing research landscape, highlighting extant challenges and charting possible directions for 3D MOT research. We present a structured and lucid road-map to guide forthcoming endeavors in this field.
</details>
<details>
<summary>摘要</summary>
三维多目标跟踪（3D MOT）是自动驾驶领域中的一个重要领域，在最近几年内受到学术界和商业界的关注增长。despite its paramount significance, 3D MOT still faces numerous challenges, including sudden changes in object appearance, ubiquitous occlusion, the presence of small targets, data sparsity, missed detections, and the unpredictable initiation and termination of object motion trajectories. To address these issues, numerous methodologies have been proposed, but 3D MOT remains a formidable problem that requires further exploration.This paper provides a comprehensive examination, assessment, and synthesis of the research landscape in this domain, keeping pace with the latest developments in 3D MOT and suggesting potential avenues for future investigation. Our exploration begins with a systematic exposition of key aspects of 3D MOT and its related domains, including problem delineation, classification, methodological approaches, fundamental principles, and empirical investigations. We then categorize these methodologies into distinct groups, dissecting each group carefully with regard to its challenges, underlying rationale, progress, merits, and demerits.Furthermore, we present a concise summary of experimental metrics and provide an overview of prevalent datasets, facilitating a quantitative comparison for a more intuitive assessment. Finally, our deliberations culminate in a discussion of the prevailing research landscape, highlighting existing challenges and charting possible directions for 3D MOT research. We provide a structured and lucid roadmap to guide forthcoming endeavors in this field.
</details></li>
</ul>
<hr>
<h2 id="KDD-LOAM-Jointly-Learned-Keypoint-Detector-and-Descriptors-Assisted-LiDAR-Odometry-and-Mapping"><a href="#KDD-LOAM-Jointly-Learned-Keypoint-Detector-and-Descriptors-Assisted-LiDAR-Odometry-and-Mapping" class="headerlink" title="KDD-LOAM: Jointly Learned Keypoint Detector and Descriptors Assisted LiDAR Odometry and Mapping"></a>KDD-LOAM: Jointly Learned Keypoint Detector and Descriptors Assisted LiDAR Odometry and Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15394">http://arxiv.org/abs/2309.15394</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renlang Huang, Minglei Zhao, Jiming Chen, Liang Li</li>
<li>for: 提高点云注册的效率和可靠性，使用稀疏关键点匹配。</li>
<li>methods: 提出一种紧密相互关联的关键点检测器和描述符（TCKDD），基于多任务全连接神经网络和概率检测损失。</li>
<li>results: 对indoor和outdoor数据集进行了广泛的实验，并达到了点云注册的州态艺术性能。此外，还设计了关键点检测器和描述符协助LiDAR导航和地图框架（KDD-LOAM），其实时导航基于关键点描述符匹配的RANSAC。<details>
<summary>Abstract</summary>
Sparse keypoint matching based on distinct 3D feature representations can improve the efficiency and robustness of point cloud registration. Existing learning-based 3D descriptors and keypoint detectors are either independent or loosely coupled, so they cannot fully adapt to each other. In this work, we propose a tightly coupled keypoint detector and descriptor (TCKDD) based on a multi-task fully convolutional network with a probabilistic detection loss. In particular, this self-supervised detection loss fully adapts the keypoint detector to any jointly learned descriptors and benefits the self-supervised learning of descriptors. Extensive experiments on both indoor and outdoor datasets show that our TCKDD achieves state-of-the-art performance in point cloud registration. Furthermore, we design a keypoint detector and descriptors-assisted LiDAR odometry and mapping framework (KDD-LOAM), whose real-time odometry relies on keypoint descriptor matching-based RANSAC. The sparse keypoints are further used for efficient scan-to-map registration and mapping. Experiments on KITTI dataset demonstrate that KDD-LOAM significantly surpasses LOAM and shows competitive performance in odometry.
</details>
<details>
<summary>摘要</summary>
《稀疏关键点匹配基于独特3D特征表示可以提高点云注册的效率和可靠性。现有的学习基于3D描述器和关键点检测器的方法是独立或松散相连，因此它们无法完全适应于每 andere。在这种工作中，我们提出了紧密相连的关键点检测器和描述器（TCKDD），基于多任务全连接神经网络和 probabilistic检测损失。特别是，这种自主学习的检测损失可以完全适应与共同学习的描述器，并为描述器的自主学习提供了优势。广泛的实验表明，我们的TCKDD在点云注册中 achieved state-of-the-art performance。此外，我们还设计了关键点检测器和描述器协助的LiDAR Odometry和地图框架（KDD-LOAM），其实时导航 rely on 关键点描述器匹配基于RANSAC。稀疏的关键点还用于高效的扫描到地图注册和地图。实验表明，KDD-LOAM在LOAM和ODometry中具有显著优势，并在ODometry中达到了竞争性的表现。》Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Subjective-Face-Transform-using-Human-First-Impressions"><a href="#Subjective-Face-Transform-using-Human-First-Impressions" class="headerlink" title="Subjective Face Transform using Human First Impressions"></a>Subjective Face Transform using Human First Impressions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15381">http://arxiv.org/abs/2309.15381</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/CV-Lehigh/subjective_face_transform">https://github.com/CV-Lehigh/subjective_face_transform</a></li>
<li>paper_authors: Chaitanya Roygaga, Joshua Krinsky, Kai Zhang, Kenny Kwok, Aparna Bharati</li>
<li>for: understand and explain biases in subjective interpretation of faces</li>
<li>methods: uses generative models to find semantically meaningful edits to a face image that change perceived attributes</li>
<li>results: demonstrates the generalizability of the approach by training on real and synthetic faces and evaluating on in-domain and out-of-domain images using predictive models and human ratings<details>
<summary>Abstract</summary>
Humans tend to form quick subjective first impressions of non-physical attributes when seeing someone's face, such as perceived trustworthiness or attractiveness. To understand what variations in a face lead to different subjective impressions, this work uses generative models to find semantically meaningful edits to a face image that change perceived attributes. Unlike prior work that relied on statistical manipulation in feature space, our end-to-end framework considers trade-offs between preserving identity and changing perceptual attributes. It maps identity-preserving latent space directions to changes in attribute scores, enabling transformation of any input face along an attribute axis according to a target change. We train on real and synthetic faces, evaluate for in-domain and out-of-domain images using predictive models and human ratings, demonstrating the generalizability of our approach. Ultimately, such a framework can be used to understand and explain biases in subjective interpretation of faces that are not dependent on the identity.
</details>
<details>
<summary>摘要</summary>
人们往往通过看一个人的脸来快速形成主观的非物理属性的印象，如信任度或美好程度。为了了解不同面部变化对主观印象的影响，这项工作使用生成模型找到保持标准化属性的semantically meaningful编辑。不同于先前的工作，我们的末端框架不仅考虑了特征空间的统计 manipulate，而且考虑了保持标识的trade-offs。它将标识保持的秘密空间方向映射到特征分数上的变化，以便将任何输入脸图像根据目标变化推动到特征轴上。我们在真实和 sintetic 脸图像上训练，使用预测模型和人类评分来评估，并证明了我们的方法的普适性。最终，这种框架可以用来理解和解释不依赖于标识的面部解释中的偏见。
</details></li>
</ul>
<hr>
<h2 id="Towards-Foundation-Models-Learned-from-Anatomy-in-Medical-Imaging-via-Self-Supervision"><a href="#Towards-Foundation-Models-Learned-from-Anatomy-in-Medical-Imaging-via-Self-Supervision" class="headerlink" title="Towards Foundation Models Learned from Anatomy in Medical Imaging via Self-Supervision"></a>Towards Foundation Models Learned from Anatomy in Medical Imaging via Self-Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15358">http://arxiv.org/abs/2309.15358</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jlianglab/eden">https://github.com/jlianglab/eden</a></li>
<li>paper_authors: Mohammad Reza Hosseinzadeh Taher, Michael B. Gotway, Jianming Liang</li>
<li>for: This paper aims to develop a foundation model for medical imaging that can “understand” human anatomy and possess fundamental properties of medical imaging.</li>
<li>methods: The authors propose a novel self-supervised learning (SSL) strategy that exploits the hierarchical nature of human anatomy, which encapsulates the intrinsic attributes of anatomical structures-locality and compositionality-within the embedding space.</li>
<li>results: The SSL pretrained model derived from the training strategy outperforms state-of-the-art (SOTA) fully&#x2F;self-supervised baselines and enhances annotation efficiency, offering potential few-shot segmentation capabilities with performance improvements ranging from 9% to 30% for segmentation tasks compared to SSL baselines.<details>
<summary>Abstract</summary>
Human anatomy is the foundation of medical imaging and boasts one striking characteristic: its hierarchy in nature, exhibiting two intrinsic properties: (1) locality: each anatomical structure is morphologically distinct from the others; and (2) compositionality: each anatomical structure is an integrated part of a larger whole. We envision a foundation model for medical imaging that is consciously and purposefully developed upon this foundation to gain the capability of "understanding" human anatomy and to possess the fundamental properties of medical imaging. As our first step in realizing this vision towards foundation models in medical imaging, we devise a novel self-supervised learning (SSL) strategy that exploits the hierarchical nature of human anatomy. Our extensive experiments demonstrate that the SSL pretrained model, derived from our training strategy, not only outperforms state-of-the-art (SOTA) fully/self-supervised baselines but also enhances annotation efficiency, offering potential few-shot segmentation capabilities with performance improvements ranging from 9% to 30% for segmentation tasks compared to SSL baselines. This performance is attributed to the significance of anatomy comprehension via our learning strategy, which encapsulates the intrinsic attributes of anatomical structures-locality and compositionality-within the embedding space, yet overlooked in existing SSL methods. All code and pretrained models are available at https://github.com/JLiangLab/Eden.
</details>
<details>
<summary>摘要</summary>
人体解剖是医学成像的基础，具有一个突出的特点：它具有地域性和组成性两个内在属性。我们想象一个基于这个基础的基础模型，能够“理解”人体解剖和拥有医学成像的基本属性。作为我们实现这个视野的第一步，我们提出了一种新的自动教育学习（SSL）策略，利用人体解剖的层次结构。我们的广泛实验表明，我们的SSL预训练模型，基于我们的培训策略，不仅超越了当前最佳自动/自我监督基线，还提高了标注效率，可能实现少量shot segmentation功能，并且比SSL基线表现出9%至30%的性能提升。这种性能归功于我们的学习策略，它在嵌入空间内具有地域性和组成性的特征，这些特征在现有的SSL方法中未被考虑。所有代码和预训练模型可以在https://github.com/JLiangLab/Eden中找到。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Dataset-for-Localization-Mapping-and-Crop-Monitoring-in-Citrus-Tree-Farms"><a href="#Multimodal-Dataset-for-Localization-Mapping-and-Crop-Monitoring-in-Citrus-Tree-Farms" class="headerlink" title="Multimodal Dataset for Localization, Mapping and Crop Monitoring in Citrus Tree Farms"></a>Multimodal Dataset for Localization, Mapping and Crop Monitoring in Citrus Tree Farms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15332">http://arxiv.org/abs/2309.15332</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ucr-robotics/citrus-farm-dataset">https://github.com/ucr-robotics/citrus-farm-dataset</a></li>
<li>paper_authors: Hanzhe Teng, Yipeng Wang, Xiaoao Song, Konstantinos Karydis</li>
<li>for: 这个论文主要用于开发自动化农业机器人系统，特别是在 citrus 树环境中进行地图建模、定位和农业监测等任务。</li>
<li>methods: 该论文使用了多modal的感知数据，包括RGB图像、深度图像、离子图像、热图像以及导航传感器数据，以及中心式位置定位的RTK。</li>
<li>results: 该论文提供了一个名为 CitrusFarm 的大型多modal 感知数据集，包括7个序列、3个田间、不同树种、不同植物排列和不同日light 条件，总共1.7小时的操作时间、7.5公里的距离和1.3TB的数据。<details>
<summary>Abstract</summary>
In this work we introduce the CitrusFarm dataset, a comprehensive multimodal sensory dataset collected by a wheeled mobile robot operating in agricultural fields. The dataset offers stereo RGB images with depth information, as well as monochrome, near-infrared and thermal images, presenting diverse spectral responses crucial for agricultural research. Furthermore, it provides a range of navigational sensor data encompassing wheel odometry, LiDAR, inertial measurement unit (IMU), and GNSS with Real-Time Kinematic (RTK) as the centimeter-level positioning ground truth. The dataset comprises seven sequences collected in three fields of citrus trees, featuring various tree species at different growth stages, distinctive planting patterns, as well as varying daylight conditions. It spans a total operation time of 1.7 hours, covers a distance of 7.5 km, and constitutes 1.3 TB of data. We anticipate that this dataset can facilitate the development of autonomous robot systems operating in agricultural tree environments, especially for localization, mapping and crop monitoring tasks. Moreover, the rich sensing modalities offered in this dataset can also support research in a range of robotics and computer vision tasks, such as place recognition, scene understanding, object detection and segmentation, and multimodal learning. The dataset, in conjunction with related tools and resources, is made publicly available at https://github.com/UCR-Robotics/Citrus-Farm-Dataset.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们介绍了一个全面的多Modal感知数据集，称为CitrusFarm数据集，由一辆滚动式移动机器人在农业场所中收集到的。该数据集包含了STEREO RGB图像和深度信息，以及灰度、近红外和热图像，这些图像具有多种光谱响应，对农业研究非常重要。此外，数据集还提供了一系列导航传感器数据，包括轮胎速度、LiDAR、惯性测量单元（IMU）和GNSS（实时准确定位），这些数据可以提供中心水平位置的准确性。数据集包括7个序列，收集在3个柑橘树场中，其中每个场景都有不同的树种、植物排列方式和不同的日light Conditions。总共耗时1.7小时，涵盖7.5公里的距离，总数据量为1.3TB。我们预计这个数据集可以帮助开发在农业树木环境中自动化机器人系统，特别是地图Localization、映射和耕作监测任务。此外，这个数据集中的丰富的感知modalities也可以支持机器人和计算机视觉相关的研究，例如地点认知、场景理解、物体检测和分割、多Modal学习等。数据集、相关工具和资源，通过https://github.com/UCR-Robotics/Citrus-Farm-Dataset进行公共发布。
</details></li>
</ul>
<hr>
<h2 id="BASED-Bundle-Adjusting-Surgical-Endoscopic-Dynamic-Video-Reconstruction-using-Neural-Radiance-Fields"><a href="#BASED-Bundle-Adjusting-Surgical-Endoscopic-Dynamic-Video-Reconstruction-using-Neural-Radiance-Fields" class="headerlink" title="BASED: Bundle-Adjusting Surgical Endoscopic Dynamic Video Reconstruction using Neural Radiance Fields"></a>BASED: Bundle-Adjusting Surgical Endoscopic Dynamic Video Reconstruction using Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15329">http://arxiv.org/abs/2309.15329</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shreya Saha, Sainan Liu, Shan Lin, Jingpei Lu, Michael Yip</li>
<li>for: 这篇论文旨在重构弹性场景从内镜视频中，以便实现无人操作的微创外科手术。</li>
<li>methods: 该方法采用神经辐射场（NeRF）方法学习3D隐藏表示场景，以满足动态和弹性场景的重建需求，并且可以处理不知情相机位置。</li>
<li>results: 经过多个实验数据集，该模型能够适应多种相机和场景设置，并且展示了在当前和未来 робо器外科系统中的承诺。<details>
<summary>Abstract</summary>
Reconstruction of deformable scenes from endoscopic videos is important for many applications such as intraoperative navigation, surgical visual perception, and robotic surgery. It is a foundational requirement for realizing autonomous robotic interventions for minimally invasive surgery. However, previous approaches in this domain have been limited by their modular nature and are confined to specific camera and scene settings. Our work adopts the Neural Radiance Fields (NeRF) approach to learning 3D implicit representations of scenes that are both dynamic and deformable over time, and furthermore with unknown camera poses. We demonstrate this approach on endoscopic surgical scenes from robotic surgery. This work removes the constraints of known camera poses and overcomes the drawbacks of the state-of-the-art unstructured dynamic scene reconstruction technique, which relies on the static part of the scene for accurate reconstruction. Through several experimental datasets, we demonstrate the versatility of our proposed model to adapt to diverse camera and scene settings, and show its promise for both current and future robotic surgical systems.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXT重建可变场景从内部视频是许多应用程序的关键，如实时操作导航、手术视觉、和机器人手术。它是实现自主机器人干预手术的基础要求。然而，过去的方法在这个领域受到了模块化的限制，只能在特定的摄像头和场景设置下工作。我们的工作采用Neural Radiance Fields（NeRF）方法来学习3D隐式表示场景，这些场景是时间上的动态和变形的，并且有未知的摄像头姿态。我们在Robotic surgery中使用了这种方法。这种方法可以消除知 Camera pose的限制，并超越了状态之Art的不结构化动态场景重建技术，该技术基于场景的静止部分进行准确重建。通过多个实验数据集，我们示出了我们提议的模型的多样性，可以适应多种摄像头和场景设置，并表明了它在当前和未来机器人手术系统中的承诺。Note: Please note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/27/cs.CV_2023_09_27/" data-id="clp89dodg00k7i788himkdmhq" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_09_27" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/27/cs.AI_2023_09_27/" class="article-date">
  <time datetime="2023-09-27T12:00:00.000Z" itemprop="datePublished">2023-09-27</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/27/cs.AI_2023_09_27/">cs.AI - 2023-09-27</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Masked-autoencoders-are-scalable-learners-of-cellular-morphology"><a href="#Masked-autoencoders-are-scalable-learners-of-cellular-morphology" class="headerlink" title="Masked autoencoders are scalable learners of cellular morphology"></a>Masked autoencoders are scalable learners of cellular morphology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16064">http://arxiv.org/abs/2309.16064</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/NumtraCG/614ca4eaa2b781088de64a5f20210923-160645routingmodel230921">https://github.com/NumtraCG/614ca4eaa2b781088de64a5f20210923-160645routingmodel230921</a></li>
<li>paper_authors: Oren Kraus, Kian Kenyon-Dean, Saber Saberian, Maryam Fallah, Peter McLean, Jess Leung, Vasudev Sharma, Ayla Khan, Jia Balakrishnan, Safiye Celik, Maciej Sypetkowski, Chi Vicky Cheng, Kristen Morse, Maureen Makes, Ben Mabey, Berton Earnshaw</li>
<li>for: 这 paper 用于探讨高容量微scopia 图像屏试中的生物关系推理，以及深度学习模型在生物研究中的应用。</li>
<li>methods: 这 paper 使用了弱监督和自监督的深度学习方法，并评估了不同模型的可扩展性和性能。</li>
<li>results: 结果显示，使用 CNN 和 ViT 基于的假降降autoencoder 模型可以舒过弱监督模型，并在大规模数据集上实现28% 的相对提升。<details>
<summary>Abstract</summary>
Inferring biological relationships from cellular phenotypes in high-content microscopy screens provides significant opportunity and challenge in biological research. Prior results have shown that deep vision models can capture biological signal better than hand-crafted features. This work explores how weakly supervised and self-supervised deep learning approaches scale when training larger models on larger datasets. Our results show that both CNN- and ViT-based masked autoencoders significantly outperform weakly supervised models. At the high-end of our scale, a ViT-L/8 trained on over 3.5-billion unique crops sampled from 95-million microscopy images achieves relative improvements as high as 28% over our best weakly supervised models at inferring known biological relationships curated from public databases.
</details>
<details>
<summary>摘要</summary>
高通量微scopic摄像头检测可以提供生物关系的重要机会和挑战。先前的结果表明深度视觉模型可以更好地捕捉生物信号，比手工设计的特征更高效。这项工作探讨如何在训练更大的模型和更大的数据集时，使用弱监睹和自监睹深度学习方法Scaling。我们的结果显示，基于CNN和ViT的masked autoencoder都能够明显超越弱监睹模型。在我们的大规模扩展中，使用超过3.5亿个独特的折衣和95万个微scopic摄像头图像中采样的ViT-L/8模型，可以在推断已知生物关系中获得相对提高达28%的改善。
</details></li>
</ul>
<hr>
<h2 id="Towards-Best-Practices-of-Activation-Patching-in-Language-Models-Metrics-and-Methods"><a href="#Towards-Best-Practices-of-Activation-Patching-in-Language-Models-Metrics-and-Methods" class="headerlink" title="Towards Best Practices of Activation Patching in Language Models: Metrics and Methods"></a>Towards Best Practices of Activation Patching in Language Models: Metrics and Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16042">http://arxiv.org/abs/2309.16042</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fred Zhang, Neel Nanda</li>
<li>for: 本研究目的是强化机器学习模型的机制可读性，即理解模型内部的机制。</li>
<li>methods: 本研究使用活动补丁技术（也称作 causal tracing 或 interchange intervention）来实现机制可读性。</li>
<li>results: 研究发现，在不同的地方和电路发现任务中，不同的评价指标和损害方法会导致不同的可读性结果。同时，研究还提供了一些概念上的论述，以及未来Activation patching的最佳实践。<details>
<summary>Abstract</summary>
Mechanistic interpretability seeks to understand the internal mechanisms of machine learning models, where localization -- identifying the important model components -- is a key step. Activation patching, also known as causal tracing or interchange intervention, is a standard technique for this task (Vig et al., 2020), but the literature contains many variants with little consensus on the choice of hyperparameters or methodology. In this work, we systematically examine the impact of methodological details in activation patching, including evaluation metrics and corruption methods. In several settings of localization and circuit discovery in language models, we find that varying these hyperparameters could lead to disparate interpretability results. Backed by empirical observations, we give conceptual arguments for why certain metrics or methods may be preferred. Finally, we provide recommendations for the best practices of activation patching going forwards.
</details>
<details>
<summary>摘要</summary>
机制解释寻求机器学习模型内部机制的理解，本地化（identifying important model components）是关键步骤。活动贴图（也称为 causal tracing或交换间作用）是标准技术，但文献中有很多变体，几乎没有共识于选择超参数或方法论。在本工作中，我们系统地检查活动贴图的方法论环境下的影响，包括评价指标和腐败方法。在语音模型的本地化和电路发现中，我们发现了不同的超参数选择可能导致不同的解释结果。基于实际观察，我们给出了概念性的Arguments，并提出了以下推荐：在活动贴图中，应该采用合适的评价指标和腐败方法，以确保解释结果的可靠性和有用性。
</details></li>
</ul>
<hr>
<h2 id="MedEdit-Model-Editing-for-Medical-Question-Answering-with-External-Knowledge-Bases"><a href="#MedEdit-Model-Editing-for-Medical-Question-Answering-with-External-Knowledge-Bases" class="headerlink" title="MedEdit: Model Editing for Medical Question Answering with External Knowledge Bases"></a>MedEdit: Model Editing for Medical Question Answering with External Knowledge Bases</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16035">http://arxiv.org/abs/2309.16035</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yucheng Shi, Shaochen Xu, Zhengliang Liu, Tianming Liu, Xiang Li, Ninghao Liu</li>
<li>for: 提高大语言模型（LLM）在医疗问答（QA）任务上的表现，并且不需要进行 fine-tuning 或 retraining。</li>
<li>methods: 利用内容学习进行模型编辑，并将医疗知识库中的信息 incorporate 到查询提示中，以提高 LLM 的响应。</li>
<li>results: 对使用 MedQA-SMILE dataset进行医疗 QA 的 edited Vicuna 模型，其精度从 44.46% 提高到 48.54%。这项研究表明，模型编辑可以提高 LLM 的表现，并提供一种实用的方法来 Mitigate 黑盒 LLM 的挑战。<details>
<summary>Abstract</summary>
Large Language Models (LLMs), although powerful in general domains, often perform poorly on domain-specific tasks like medical question answering (QA). Moreover, they tend to function as "black-boxes," making it challenging to modify their behavior. Addressing this, our study delves into model editing utilizing in-context learning, aiming to improve LLM responses without the need for fine-tuning or retraining. Specifically, we propose a comprehensive retrieval strategy to extract medical facts from an external knowledge base, and then we incorporate them into the query prompt for the LLM. Focusing on medical QA using the MedQA-SMILE dataset, we evaluate the impact of different retrieval models and the number of facts provided to the LLM. Notably, our edited Vicuna model exhibited an accuracy improvement from 44.46% to 48.54%. This work underscores the potential of model editing to enhance LLM performance, offering a practical approach to mitigate the challenges of black-box LLMs.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）虽然在通用领域上表现出色，但在具体领域任务如医疗问答（QA）中表现不佳。此外，它们往往 behave like "黑盒子"，使其行为修改困难。为了解决这问题，我们的研究探讨了模型编辑，以提高 LLM 的回答质量，不需要 Fine-tuning 或 Retraining。我们提议了一种完整的检索策略，将医学知识库中的医学事实EXTRACTED并提供给 LLM 作为查询提示。我们在使用 MedQA-SMILE 数据集进行医学问答任务中，评估了不同的检索模型和提供给 LLM 的事实数量对性能的影响。结果显示，我们修改后的 Vicuna 模型的准确率从 44.46% 提高到 48.54%。这种研究证明了模型编辑的潜在作用，为黑盒子 LLM 带来了实际的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Symbolic-Imitation-Learning-From-Black-Box-to-Explainable-Driving-Policies"><a href="#Symbolic-Imitation-Learning-From-Black-Box-to-Explainable-Driving-Policies" class="headerlink" title="Symbolic Imitation Learning: From Black-Box to Explainable Driving Policies"></a>Symbolic Imitation Learning: From Black-Box to Explainable Driving Policies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16025">http://arxiv.org/abs/2309.16025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iman Sharifi, Saber Fallah</li>
<li>for: 提高自动驾驶系统的可靠性和安全性</li>
<li>methods: 使用卷积神经网络和推理逻辑编程（ILP）学习驾驶策略</li>
<li>results: 提高驾驶策略的可解释性和泛化性，并在不同的驾驶情况下显著提高驾驶策略的应用性<details>
<summary>Abstract</summary>
Current methods of imitation learning (IL), primarily based on deep neural networks, offer efficient means for obtaining driving policies from real-world data but suffer from significant limitations in interpretability and generalizability. These shortcomings are particularly concerning in safety-critical applications like autonomous driving. In this paper, we address these limitations by introducing Symbolic Imitation Learning (SIL), a groundbreaking method that employs Inductive Logic Programming (ILP) to learn driving policies which are transparent, explainable and generalisable from available datasets. Utilizing the real-world highD dataset, we subject our method to a rigorous comparative analysis against prevailing neural-network-based IL methods. Our results demonstrate that SIL not only enhances the interpretability of driving policies but also significantly improves their applicability across varied driving situations. Hence, this work offers a novel pathway to more reliable and safer autonomous driving systems, underscoring the potential of integrating ILP into the domain of IL.
</details>
<details>
<summary>摘要</summary>
当前的模仿学习（IL）方法，主要基于深度神经网络，提供了高效的获取驾驶策略的方式，但受到解释性和普适性的重大限制。这些局限性在安全关键应用如自动驾驶中特别有问题。在这篇论文中，我们解决了这些限制，通过引入符号学习（SIL），我们使用推理逻辑编程（ILP）来学习透明、可解释的驾驶策略。我们使用现实世界的高D数据集进行了严格的比较分析，我们的结果表明，SIL不仅可以提高驾驶策略的解释性，还可以显著改善驾驶策略在不同驾驶情况下的应用程度。因此，这项工作提供了一个新的可靠和安全的自动驾驶系统的可能性，强调了将ILPintegrated到驾驶学习领域中的潜在价值。
</details></li>
</ul>
<hr>
<h2 id="Clinical-Trial-Recommendations-Using-Semantics-Based-Inductive-Inference-and-Knowledge-Graph-Embeddings"><a href="#Clinical-Trial-Recommendations-Using-Semantics-Based-Inductive-Inference-and-Knowledge-Graph-Embeddings" class="headerlink" title="Clinical Trial Recommendations Using Semantics-Based Inductive Inference and Knowledge Graph Embeddings"></a>Clinical Trial Recommendations Using Semantics-Based Inductive Inference and Knowledge Graph Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15979">http://arxiv.org/abs/2309.15979</a></li>
<li>repo_url: None</li>
<li>paper_authors: Murthy V. Devarakonda, Smita Mohanty, Raja Rao Sunkishala, Nag Mallampalli, Xiong Liu</li>
<li>for: 本研究的目的是提出一种新的临床试验设计方法，通过对临床试验记录的探索性挖掘，为设计新的临床试验提供建议。</li>
<li>methods: 本研究使用了基于神经元编码的新推荐方法，利用临床试验数据知识图构建知识图embedding（KGE），并通过对KGE方法的效果进行研究，以及一种新的推荐方法基于KGE。</li>
<li>results: 研究结果显示，该推荐方法可以达到70%-83%的相关性分数，并且在实际临床试验元素中找到最相关的建议。此外，研究还发现可以通过节点 semantics 进行训练，以提高KGE的性能。<details>
<summary>Abstract</summary>
Designing a new clinical trial entails many decisions, such as defining a cohort and setting the study objectives to name a few, and therefore can benefit from recommendations based on exhaustive mining of past clinical trial records. Here, we propose a novel recommendation methodology, based on neural embeddings trained on a first-of-a-kind knowledge graph of clinical trials. We addressed several important research questions in this context, including designing a knowledge graph (KG) for clinical trial data, effectiveness of various KG embedding (KGE) methods for it, a novel inductive inference using KGE, and its use in generating recommendations for clinical trial design. We used publicly available data from clinicaltrials.gov for the study. Results show that our recommendations approach achieves relevance scores of 70%-83%, measured as the text similarity to actual clinical trial elements, and the most relevant recommendation can be found near the top of list. Our study also suggests potential improvement in training KGE using node semantics.
</details>
<details>
<summary>摘要</summary>
“设计新临床试验需要许多决策，例如定义受试群体和设定试验目标等，因此可以受益于根据过去临床试验记录的广泛采矿提供建议。我们提出了一种新的建议方法，基于临床试验知识图（KG）的神经嵌入。我们解决了许多重要的研究问题，包括临床试验数据的知识图设计、不同KG嵌入方法的效果、一种新的推论方法，以及其在设计临床试验时的应用。我们使用了公开ailable的临床试验数据库，来进行研究。结果显示，我们的建议方法可以实现70%-83%的相似度数据， measured as 文本与实际临床试验元素之间的相似度，而且最相似的建议通常可以在列表的顶部发现。我们的研究也显示，可以在专门的node semantics上进行训练，以提高KGE的性能。”
</details></li>
</ul>
<hr>
<h2 id="Resilience-of-Deep-Learning-applications-a-systematic-survey-of-analysis-and-hardening-techniques"><a href="#Resilience-of-Deep-Learning-applications-a-systematic-survey-of-analysis-and-hardening-techniques" class="headerlink" title="Resilience of Deep Learning applications: a systematic survey of analysis and hardening techniques"></a>Resilience of Deep Learning applications: a systematic survey of analysis and hardening techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16733">http://arxiv.org/abs/2309.16733</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cristiana Bolchini, Luca Cassano, Antonio Miele</li>
<li>for: 本研究探讨了深度学习（一种人工智能技术）对硬件错误的抵御性，系统地对现有相关研究进行了思考和回顾。</li>
<li>methods: 本研究采用了一种分类框架来解释和 highlight研究相似之处和特点，基于多个参数，包括研究主要目标、采用的错误和缺陷模型、其可重现性。</li>
<li>results: 本研究结果表明，目前的研究主要集中在深度学习对硬件错误的抵御性方面，并采用了多种方法来解决这些问题。但是，还有一些未解决的问题和挑战需要在未来进行研究。<details>
<summary>Abstract</summary>
Machine Learning (ML) is currently being exploited in numerous applications being one of the most effective Artificial Intelligence (AI) technologies, used in diverse fields, such as vision, autonomous systems, and alike. The trend motivated a significant amount of contributions to the analysis and design of ML applications against faults affecting the underlying hardware. The authors investigate the existing body of knowledge on Deep Learning (among ML techniques) resilience against hardware faults systematically through a thoughtful review in which the strengths and weaknesses of this literature stream are presented clearly and then future avenues of research are set out. The review is based on 163 scientific articles published between January 2019 and March 2023. The authors adopt a classifying framework to interpret and highlight research similarities and peculiarities, based on several parameters, starting from the main scope of the work, the adopted fault and error models, to their reproducibility. This framework allows for a comparison of the different solutions and the identification of possible synergies. Furthermore, suggestions concerning the future direction of research are proposed in the form of open challenges to be addressed.
</details>
<details>
<summary>摘要</summary>
根据2019年1月至2023年3月发表的163篇科学文献，作者采用了一个分类框架来解读和 highlight研究的相似性和特点，包括研究的主要范围、采用的缺陷和错误模型、其重复性等多个参数。这个框架允许对不同的解决方案进行比较，并提出了可能的共同点。此外，作者还提出了未来研究的方向和开放挑战。
</details></li>
</ul>
<hr>
<h2 id="Unified-Long-Term-Time-Series-Forecasting-Benchmark"><a href="#Unified-Long-Term-Time-Series-Forecasting-Benchmark" class="headerlink" title="Unified Long-Term Time-Series Forecasting Benchmark"></a>Unified Long-Term Time-Series Forecasting Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15946">http://arxiv.org/abs/2309.15946</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MIMUW-RL/Unified-Long-Horizon-Time-Series-Benchmark">https://github.com/MIMUW-RL/Unified-Long-Horizon-Time-Series-Benchmark</a></li>
<li>paper_authors: Jacek Cyranka, Szymon Haponiuk</li>
<li>for: 这个论文是为了提高机器学习方法的时间序列预测能力而设计的，并提供了一个完整的数据集，用于验证这些方法。</li>
<li>methods: 这个论文使用了多种不同的机器学习模型，包括LSTM、DeepAR、NLinear、N-Hits、PatchTST和LatentODE等，并进行了广泛的比较分析以决定这些模型在不同情况下的效果。</li>
<li>results: 这个论文的结果显示了不同模型在不同数据集下的表现，并发现了一些模型在某些情况下的优化。此外，论文还引入了一个自定义的潜在NLinear模型和将DeepAR加以课程学习阶段，both consistently outperform their vanilla counterparts。<details>
<summary>Abstract</summary>
In order to support the advancement of machine learning methods for predicting time-series data, we present a comprehensive dataset designed explicitly for long-term time-series forecasting. We incorporate a collection of datasets obtained from diverse, dynamic systems and real-life records. Each dataset is standardized by dividing it into training and test trajectories with predetermined lookback lengths. We include trajectories of length up to $2000$ to ensure a reliable evaluation of long-term forecasting capabilities. To determine the most effective model in diverse scenarios, we conduct an extensive benchmarking analysis using classical and state-of-the-art models, namely LSTM, DeepAR, NLinear, N-Hits, PatchTST, and LatentODE. Our findings reveal intriguing performance comparisons among these models, highlighting the dataset-dependent nature of model effectiveness. Notably, we introduce a custom latent NLinear model and enhance DeepAR with a curriculum learning phase. Both consistently outperform their vanilla counterparts.
</details>
<details>
<summary>摘要</summary>
为支持机器学习方法预测时间序列数据的进步，我们提供了专门为长期时间序列预测设计的完整数据集。我们收集了来自多种动态系统和实际记录的多个数据集，并对每个数据集进行标准化，将它们分为训练和测试曲线的训练和测试轨迹，使用预定的回看长度。我们的数据集包括长度达2000的轨迹，以确保可靠地评估长期预测能力。我们使用经典和当前最佳模型，包括LSTM、DeepAR、NLinear、N-Hits、PatchTST和LatentODE进行广泛的比较分析，发现这些模型在不同的场景中表现出有趣的比较。特别是，我们提出了一种自定义隐藏的NLinear模型和对DeepAR进行课程学习阶段的改进，两者都能在其基础模型中提高表现。
</details></li>
</ul>
<hr>
<h2 id="Towards-Efficient-and-Trustworthy-AI-Through-Hardware-Algorithm-Communication-Co-Design"><a href="#Towards-Efficient-and-Trustworthy-AI-Through-Hardware-Algorithm-Communication-Co-Design" class="headerlink" title="Towards Efficient and Trustworthy AI Through Hardware-Algorithm-Communication Co-Design"></a>Towards Efficient and Trustworthy AI Through Hardware-Algorithm-Communication Co-Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15942">http://arxiv.org/abs/2309.15942</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bipin Rajendran, Osvaldo Simeone, Bashir M. Al-Hashimi</li>
<li>for: 这篇论文的目的是提出一种基于硬件和软件设计的高效可靠人工智能（AI）算法，以提高AI模型的可靠性和不确定性评估。</li>
<li>methods: 该论文提出了一些研究方向，包括将物理知识integrated into计算基础结构，采用神经科学原则来实现高效信息处理，使用信息论和通信论的结果来估计 uncertainty，并采用分布式处理的通信论指南。</li>
<li>results: 该论文认为，通过采用新的设计方法，可以不仅提高AI模型的准确率，还可以提供可靠的不确定性评估。此外，该论文还提出了一些基于新 computing 架构的高效可靠AI算法，包括卷积神经网络、听频神经网络和量子计算技术。<details>
<summary>Abstract</summary>
Artificial intelligence (AI) algorithms based on neural networks have been designed for decades with the goal of maximising some measure of accuracy. This has led to two undesired effects. First, model complexity has risen exponentially when measured in terms of computation and memory requirements. Second, state-of-the-art AI models are largely incapable of providing trustworthy measures of their uncertainty, possibly `hallucinating' their answers and discouraging their adoption for decision-making in sensitive applications.   With the goal of realising efficient and trustworthy AI, in this paper we highlight research directions at the intersection of hardware and software design that integrate physical insights into computational substrates, neuroscientific principles concerning efficient information processing, information-theoretic results on optimal uncertainty quantification, and communication-theoretic guidelines for distributed processing. Overall, the paper advocates for novel design methodologies that target not only accuracy but also uncertainty quantification, while leveraging emerging computing hardware architectures that move beyond the traditional von Neumann digital computing paradigm to embrace in-memory, neuromorphic, and quantum computing technologies. An important overarching principle of the proposed approach is to view the stochasticity inherent in the computational substrate and in the communication channels between processors as a resource to be leveraged for the purpose of representing and processing classical and quantum uncertainty.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）算法基于神经网络已经在数十年中设计的目标是最大化某种精度指标。这两个不良效果：首先，模型复杂性 exponentiates 计算和内存需求。其次，当前的AI模型几乎无法提供可靠的不确定度评估，可能会“幻见”答案，这使得它们在敏感应用中无法得到采用。为实现高效可靠的AI，本文提出了融合硬件和软件设计的研究方向。这些方向包括：1. 基于物理学的启发，设计计算substrate，以提高计算效率和可靠性。2. 基于神经科学的原理，设计硬件和软件结构，以提高信息处理效率。3. 基于信息论的结果，使用最佳的不确定度量化方法，以提高模型的可靠性。4. 基于通信论的指导原则，设计分布式处理的方法，以提高模型的可靠性和可重复性。总的来说，本文提出了一种新的设计方法，该方法不仅考虑精度，还考虑不确定度量化。此外，该方法还利用新的计算硬件技术，例如半导体、神经元和量子计算技术，以超越传统的沃尔夫尼亚姆数字计算模式。一个重要的总体原则是视计算substrate和通信频道之间的随机性为可以利用的资源，以便用于表示和处理类型的不确定性。
</details></li>
</ul>
<hr>
<h2 id="SHACIRA-Scalable-HAsh-grid-Compression-for-Implicit-Neural-Representations"><a href="#SHACIRA-Scalable-HAsh-grid-Compression-for-Implicit-Neural-Representations" class="headerlink" title="SHACIRA: Scalable HAsh-grid Compression for Implicit Neural Representations"></a>SHACIRA: Scalable HAsh-grid Compression for Implicit Neural Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15848">http://arxiv.org/abs/2309.15848</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sharath-girish/Shacira">https://github.com/Sharath-girish/Shacira</a></li>
<li>paper_authors: Sharath Girish, Abhinav Shrivastava, Kamal Gupta</li>
<li>for: 这 paper 旨在提出一种任务不受限制的框架，用于压缩Instant-NGP中的学习特征网格，以提高存储和流处理应用程序中的效率。</li>
<li>methods: 这 paper 使用了量化 latent  веса的重parameterization和熵 regularization来实现压缩，而不需要额外的post验签&#x2F;量化阶段。</li>
<li>results: 实验结果表明，我们的方法可以在多个领域中实现高度压缩，而不需要大量的数据或域特有的规则。我们的项目页面可以在 <a target="_blank" rel="noopener" href="http://shacira.github.io/">http://shacira.github.io</a> 找到。<details>
<summary>Abstract</summary>
Implicit Neural Representations (INR) or neural fields have emerged as a popular framework to encode multimedia signals such as images and radiance fields while retaining high-quality. Recently, learnable feature grids proposed by Instant-NGP have allowed significant speed-up in the training as well as the sampling of INRs by replacing a large neural network with a multi-resolution look-up table of feature vectors and a much smaller neural network. However, these feature grids come at the expense of large memory consumption which can be a bottleneck for storage and streaming applications. In this work, we propose SHACIRA, a simple yet effective task-agnostic framework for compressing such feature grids with no additional post-hoc pruning/quantization stages. We reparameterize feature grids with quantized latent weights and apply entropy regularization in the latent space to achieve high levels of compression across various domains. Quantitative and qualitative results on diverse datasets consisting of images, videos, and radiance fields, show that our approach outperforms existing INR approaches without the need for any large datasets or domain-specific heuristics. Our project page is available at http://shacira.github.io .
</details>
<details>
<summary>摘要</summary>
含义表示（INR）或神经场已成为编码多媒体信号（图像和辐射场）的受欢迎框架，保持高质量。近些年，可学习特征格子提出了可学习特征格子，以替代大型神经网络，从而实现训练和采样INR的速度增快。然而，这些特征格子带来大量内存占用，可能对存储和流动应用造成瓶颈。在这种情况下，我们提出了SHACIRA，一个简单 yet有效的任务无关框架，用于压缩特征格子，无需额外的后处剖分/量化阶段。我们将特征格子映射到量化的幂Weight中，并在幂空间应用Entropy抑制来实现高度压缩。在多个领域中，包括图像、视频和辐射场，我们的方法与现有INR方法进行比较，并且不需要大量的数据或域特定的规则。更多信息可以通过我们的项目页面http://shacira.github.io/获取。
</details></li>
</ul>
<hr>
<h2 id="Examining-the-Values-Reflected-by-Children-during-AI-Problem-Formulation"><a href="#Examining-the-Values-Reflected-by-Children-during-AI-Problem-Formulation" class="headerlink" title="Examining the Values Reflected by Children during AI Problem Formulation"></a>Examining the Values Reflected by Children during AI Problem Formulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15839">http://arxiv.org/abs/2309.15839</a></li>
<li>repo_url: None</li>
<li>paper_authors: Utkarsh Dwivedi, Salma Elsayed-ali, Elizabeth Bonsignore, Hernisa Kacorri</li>
<li>for: 这个论文的目的是了解儿童在设计和训练AIInterface时所优先的目标和价值观。</li>
<li>methods: 论文使用了合作设计方法和修改后的故事板来让儿童和成年合作者在AI问题定义上进行活动。</li>
<li>results: 研究发现儿童的提议中含有高级的系统智能，如感知和理解用户的社交关系。儿童的想法表明他们关心家庭和期望机器能够理解社交上下文。<details>
<summary>Abstract</summary>
Understanding how children design and what they value in AI interfaces that allow them to explicitly train their models such as teachable machines, could help increase such activities' impact and guide the design of future technologies. In a co-design session using a modified storyboard, a team of 5 children (aged 7-13 years) and adult co-designers, engaged in AI problem formulation activities where they imagine their own teachable machines. Our findings, leveraging an established psychological value framework (the Rokeach Value Survey), illuminate how children conceptualize and embed their values in AI systems that they themselves devise to support their everyday activities. Specifically, we find that children's proposed ideas require advanced system intelligence, e.g. emotion detection and understanding the social relationships of a user. The underlying models could be trained under multiple modalities and any errors would be fixed by adding more data or by anticipating negative examples. Children's ideas showed they cared about family and expected machines to understand their social context before making decisions.
</details>
<details>
<summary>摘要</summary>
理解儿童在设计和训练AI模型时所价值的内容，可以帮助提高这些活动的影响力并导向未来技术的设计。在一个 modify 的故事板 session 中，一组5名儿童（年龄7-13岁）和成年合作设计者，参与了 AI 问题定制活动，在假设自己的教程机器人时。我们的发现，基于已成立的心理价值框架（Rokeach Value Survey），揭示了儿童如何概念化并嵌入自己的价值观在自己设计的 AI 系统中。特别是，儿童的提出的想法需要高级的系统智能，例如情感检测和理解用户的社交关系。这些基础模型可以在多种感知模式下训练，并且任何错误都可以通过添加更多数据或预期负例来修复。儿童的想法表明他们关心家庭和期望机器人能够理解其社交上下文，在做出决策之前。
</details></li>
</ul>
<hr>
<h2 id="OrthoPlanes-A-Novel-Representation-for-Better-3D-Awareness-of-GANs"><a href="#OrthoPlanes-A-Novel-Representation-for-Better-3D-Awareness-of-GANs" class="headerlink" title="OrthoPlanes: A Novel Representation for Better 3D-Awareness of GANs"></a>OrthoPlanes: A Novel Representation for Better 3D-Awareness of GANs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15830">http://arxiv.org/abs/2309.15830</a></li>
<li>repo_url: None</li>
<li>paper_authors: Honglin He, Zhuoqian Yang, Shikai Li, Bo Dai, Wayne Wu</li>
<li>for: 这个论文的目的是为了生成高精度的、视角一致的3D图像。</li>
<li>methods: 这个论文提出了一种hybrid的显式-隐式表示方法，称为OrthoPlanes，它可以高效地通过修改2D StyleGANs来生成细节rich的3D信息。</li>
<li>results: 实验表明，这个方法可以处理更加困难的视角和生成高度自由的静止物体图像，并且在FFHQ和SHHQ数据集上达到了状态 искусственный智能水平。项目页面：<a target="_blank" rel="noopener" href="https://orthoplanes.github.io/">https://orthoplanes.github.io/</a>。<details>
<summary>Abstract</summary>
We present a new method for generating realistic and view-consistent images with fine geometry from 2D image collections. Our method proposes a hybrid explicit-implicit representation called \textbf{OrthoPlanes}, which encodes fine-grained 3D information in feature maps that can be efficiently generated by modifying 2D StyleGANs. Compared to previous representations, our method has better scalability and expressiveness with clear and explicit information. As a result, our method can handle more challenging view-angles and synthesize articulated objects with high spatial degree of freedom. Experiments demonstrate that our method achieves state-of-the-art results on FFHQ and SHHQ datasets, both quantitatively and qualitatively. Project page: \url{https://orthoplanes.github.io/}.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，可以生成具有细腻的三维信息的真实和视角一致的图像集。我们的方法使用名为“OrthoPlanes”的混合显式隐式表示方式，可以快速地由修改2D StyleGANs生成细节rich的特征图。相比之前的表示方法，我们的方法具有更好的扩展性和表达能力，并且有明确的信息。因此，我们的方法可以更好地处理更加困难的视角和 sintheSize articulated objects with high spatial degree of freedom。实验结果表明，我们的方法在FFHQ和SHHQ数据集上达到了现状之册的Result， both quantitatively and qualitatively。项目页面：\url{https://orthoplanes.github.io/}.
</details></li>
</ul>
<hr>
<h2 id="Lyra-Orchestrating-Dual-Correction-in-Automated-Theorem-Proving"><a href="#Lyra-Orchestrating-Dual-Correction-in-Automated-Theorem-Proving" class="headerlink" title="Lyra: Orchestrating Dual Correction in Automated Theorem Proving"></a>Lyra: Orchestrating Dual Correction in Automated Theorem Proving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15806">http://arxiv.org/abs/2309.15806</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chuanyang-zheng/lyra-theorem-prover">https://github.com/chuanyang-zheng/lyra-theorem-prover</a></li>
<li>paper_authors: Chuanyang Zheng, Haiming Wang, Enze Xie, Zhengying Liu, Jiankai Sun, Huajian Xin, Jianhao Shen, Zhenguo Li, Yu Li</li>
<li>for: 这个论文的目的是提高大型自然语言模型（LLMs）在正式证明领域的效果，特别是避免幻觉和证明错误的反馈。</li>
<li>methods: 这个论文提出了一个新的框架 called Lyra，它使用了两种不同的修正机制：工具修正（TC）和推测修正（CC）。TC 使用先前知识来利用预定义的证明工具（如 Sledgehammer）来指导错误工具的更换，以避免幻觉。CC 是一种错误反馈机制，用于与证明器交互，以改进正式证明 conjecture。</li>
<li>results: 该论文的方法在 miniF2F 验证和测试集上达到了当前最佳性能（SOTA），从48.0% 提高到55.3% 和从45.5% 提高到51.2%。此外，论文还解决了三个国际数学奥林匹克（IMO）问题。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) present an intriguing avenue for exploration in the field of formal theorem proving. Nevertheless, their full potential, particularly concerning the mitigation of hallucinations and refinement through prover error messages, remains an area that has yet to be thoroughly investigated. To enhance the effectiveness of LLMs in the field, we introduce the Lyra, a new framework that employs two distinct correction mechanisms: Tool Correction (TC) and Conjecture Correction (CC). To implement Tool Correction in the post-processing of formal proofs, we leverage prior knowledge to utilize predefined prover tools (e.g., Sledgehammer) for guiding the replacement of incorrect tools. Tool Correction significantly contributes to mitigating hallucinations, thereby improving the overall accuracy of the proof. In addition, we introduce Conjecture Correction, an error feedback mechanism designed to interact with prover to refine formal proof conjectures with prover error messages. Compared to the previous refinement framework, the proposed Conjecture Correction refines generation with instruction but does not collect paired (generation, error & refinement) prompts. Our method has achieved state-of-the-art (SOTA) performance on both miniF2F validation (48.0% -> 55.3%) and test (45.5% -> 51.2%). We also present 3 IMO problems solved by Lyra. We believe Tool Correction (post-process for hallucination mitigation) and Conjecture Correction (subgoal adjustment from interaction with environment) could provide a promising avenue for future research in this field.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在形式证明领域的探索具有吸引力，但它们的全面潜力，特别是通过证明错误消息修正和抑制幻觉，仍然是一个尚未被全面探索的领域。为了增强LLM在这个领域的效果，我们介绍了一个新的框架，即Lyra，它使用了两种不同的修正机制：工具修正（TC）和推测修正（CC）。为了在后期处理中使用工具修正，我们利用了先前知识，使用预定的证明工具（例如Sledgehammer）来指导错误工具的更换。工具修正在幻觉缓解方面做出了重要贡献，从而提高了证明的总准确性。此外，我们引入了推测修正，这是一种基于证明错误消息的错误反馈机制，可以与证明进行互动，以修正正式证明的推测。与之前的修复框架相比，我们的提案的推测修正不需要收集配对（生成、错误和修复）的示例。我们的方法在miniF2F验证中达到了状态的最佳性能（SOTA），从48.0%提高到55.3%，以及在测试中从45.5%提高到51.2%。我们还展示了3个IMO问题，由Lyra解决。我们认为工具修正（幻觉缓解）和推测修正（从环境交互修正）是未来这个领域的有前途的研究方向。
</details></li>
</ul>
<hr>
<h2 id="AI-in-Software-Engineering-Case-Studies-and-Prospects"><a href="#AI-in-Software-Engineering-Case-Studies-and-Prospects" class="headerlink" title="AI in Software Engineering: Case Studies and Prospects"></a>AI in Software Engineering: Case Studies and Prospects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15768">http://arxiv.org/abs/2309.15768</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Wang</li>
<li>for: 本文旨在研究人工智能（AI）和软件工程（SE）之间的关系，以及如何应用AI技术在软件开发中提高软件产品质量。</li>
<li>methods: 本文分析了两个案例研究：IBM watson和Google AlphaGo，它们都使用了不同的AI技术来解决现实世界中的挑战问题。</li>
<li>results: 研究发现，使用AI技术如深度学习和机器学习在软件系统中可以实现智能系统。IBM watson采用了“决策支持”策略，帮助人类做出决策；AlphaGo则使用了“自动决策”选择操作，以实现最佳结果。此外，AlphaGo还使用了神经网络和强化学习来模仿人脑，这可能在医学研究中用于诊断和治疗。然而，我们还需要很长的时间来复制人脑在机器中，因为人脑和机器是内在不同的。<details>
<summary>Abstract</summary>
Artificial intelligence (AI) and software engineering (SE) are two important areas in computer science. In recent years, researchers are trying to apply AI techniques in various stages of software development to improve the overall quality of software products. Moreover, there are also some researchers focus on the intersection between SE and AI. In fact, the relationship between SE and AI is very weak; however, methods and techniques in one area have been adopted in another area. More and more software products are capable of performing intelligent behaviour like human beings. In this paper, two cases studies which are IBM Watson and Google AlphaGo that use different AI techniques in solving real world challenging problems have been analysed, evaluated and compared. Based on the analysis of both case studies, using AI techniques such as deep learning and machine learning in software systems contributes to intelligent systems. Watson adopts 'decision making support' strategy to help human make decisions; whereas AlphaGo uses 'self-decision making' to choose operations that contribute to the best outcome. In addition, Watson learns from man-made resources such as paper; AlphaGo, on the other hand, learns from massive online resources such as photos. AlphaGo uses neural networks and reinforcement learning to mimic human brain, which might be very useful in medical research for diagnosis and treatment. However, there is still a long way to go if we want to reproduce human brain in machine and view computers as thinkers, because human brain and machines are intrinsically different. It would be more promising to see whether computers and software systems will become more and more intelligent to help with real world challenging problems that human beings cannot do.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）和软件工程（SE）是计算机科学中两个重要领域。近年来，研究人员尝试将AI技术应用于软件开发的不同阶段，以提高软件产品的总质量。此外，还有一些研究人员关注SE和AI的交叉点。事实上，SE和AI之间的关系很弱，但是一个领域的方法和技术往往被另一个领域采纳。逐渐增多的软件产品可以展现出人类智能的行为。本文分析了IBM Watson和Google AlphaGo两个案例，它们使用了不同的AI技术解决实际世界问题。根据两个案例的分析，使用AI技术如深度学习和机器学习在软件系统中带来智能系统。Watson采用了“决策支持”策略，以帮助人类做出决策；AlphaGo则使用“自动决策”选择操作，以实现最佳结果。此外，Watson从人类制作的资源学习，如文献；AlphaGo则从大量在线资源学习，如照片。AlphaGo使用神经网络和强化学习模仿人脑，可能在医学研究中非常有用于诊断和治疗。然而，我们还很遥远才能复制人脑机器，因为人脑和机器是内在不同的。可能更有前途的是看看计算机和软件系统会变得越来越智能，以帮助实际世界中的问题。
</details></li>
</ul>
<hr>
<h2 id="Borges-and-AI"><a href="#Borges-and-AI" class="headerlink" title="Borges and AI"></a>Borges and AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01425">http://arxiv.org/abs/2310.01425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Léon Bottou, Bernhard Schölkopf</li>
<li>for: 这篇论文主要是为了探讨大型自然语言模型（LLM）是人工智能（AI）的开端，以及这种技术的可能性和威胁。</li>
<li>methods: 本论文使用了 Jorge Luis Borges 的文学创作作为视角，以帮助理解大型自然语言模型和人工智能之间的关系。</li>
<li>results: 本论文提出了一种新的视角，即通过 Borges 的文学创作来理解大型自然语言模型和人工智能之间的关系，从而帮助我们更深入理解这种技术的潜在可能性和威胁。<details>
<summary>Abstract</summary>
Many believe that Large Language Models (LLMs) open the era of Artificial Intelligence (AI). Some see opportunities while others see dangers. Yet both proponents and opponents grasp AI through the imagery popularised by science fiction. Will the machine become sentient and rebel against its creators? Will we experience a paperclip apocalypse? Before answering such questions, we should first ask whether this mental imagery provides a good description of the phenomenon at hand. Understanding weather patterns through the moods of the gods only goes so far. The present paper instead advocates understanding LLMs and their connection to AI through the imagery of Jorge Luis Borges, a master of 20th century literature, forerunner of magical realism, and precursor to postmodern literature. This exercise leads to a new perspective that illuminates the relation between language modelling and artificial intelligence.
</details>
<details>
<summary>摘要</summary>
很多人认为大语言模型（LLM）开启了人工智能（AI）的时代。一些人看到了机会，而另一些人看到了危险。然而，两者都是通过科幻小说中的形象来理解AI。例如，将机器变成自己的创造者并反抗它们吗？将会出现笔clip末日吗？在回答这些问题之前，我们应该首先问这些形象是否能够正确描述现象。通过神话和幻想的形象来理解天气patterns只能取得一定的成果。而在本文中，我们 instead advocates使用 Jorge Luis Borges 的形象来理解 LLM 和 AI 之间的关系，这将导向一种新的视角，以便更好地理解语言模型和人工智能之间的关系。
</details></li>
</ul>
<hr>
<h2 id="Latent-Graphs-for-Semi-Supervised-Learning-on-Biomedical-Tabular-Data"><a href="#Latent-Graphs-for-Semi-Supervised-Learning-on-Biomedical-Tabular-Data" class="headerlink" title="Latent Graphs for Semi-Supervised Learning on Biomedical Tabular Data"></a>Latent Graphs for Semi-Supervised Learning on Biomedical Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15757">http://arxiv.org/abs/2309.15757</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boshko Koloski, Nada Lavrač, Senja Pollak, Blaž Škrlj</li>
<li>for: 提高 semi-supervised learning 技术的Robustness和性能，通过发现数据之间的关系关系</li>
<li>methods: 基于图表示法，利用 graph-based 表示，实现信息的流动性，同时包含全局和局部知识</li>
<li>results: 对生物医学数据集进行评估，提出一种基于图的方法，能够超越当前方法的性能，并且在三个生物医学数据集上达到最佳效果<details>
<summary>Abstract</summary>
In the domain of semi-supervised learning, the current approaches insufficiently exploit the potential of considering inter-instance relationships among (un)labeled data. In this work, we address this limitation by providing an approach for inferring latent graphs that capture the intrinsic data relationships. By leveraging graph-based representations, our approach facilitates the seamless propagation of information throughout the graph, effectively incorporating global and local knowledge. Through evaluations on biomedical tabular datasets, we compare the capabilities of our approach to other contemporary methods. Our work demonstrates the significance of inter-instance relationship discovery as practical means for constructing robust latent graphs to enhance semi-supervised learning techniques. The experiments show that the proposed methodology outperforms contemporary state-of-the-art methods for (semi-)supervised learning on three biomedical datasets.
</details>
<details>
<summary>摘要</summary>
在半指导学习领域，当前的方法未能充分利用半标注数据之间实例关系的潜力。在这个工作中，我们解决这个限制，通过提供一种推理潜在图的方法，以捕捉数据的内在关系。通过利用图表示，我们的方法可以轻松地在图中传递信息，有效地结合全局和局部知识。通过对生物医学表格数据进行评估，我们与当代其他方法进行比较。我们的工作表明了在建立强大的潜在图中找到实例关系的重要性，以提高半指导学习技术的性能。实验表明，我们提出的方法在三个生物医学数据集上比当代状态オブジェクト的方法表现更出色。
</details></li>
</ul>
<hr>
<h2 id="Experience-and-Evidence-are-the-eyes-of-an-excellent-summarizer-Towards-Knowledge-Infused-Multi-modal-Clinical-Conversation-Summarization"><a href="#Experience-and-Evidence-are-the-eyes-of-an-excellent-summarizer-Towards-Knowledge-Infused-Multi-modal-Clinical-Conversation-Summarization" class="headerlink" title="Experience and Evidence are the eyes of an excellent summarizer! Towards Knowledge Infused Multi-modal Clinical Conversation Summarization"></a>Experience and Evidence are the eyes of an excellent summarizer! Towards Knowledge Infused Multi-modal Clinical Conversation Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15739">http://arxiv.org/abs/2309.15739</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nlp-rl/mm-cliconsummation">https://github.com/nlp-rl/mm-cliconsummation</a></li>
<li>paper_authors: Abhisek Tiwari, Anisha Saha, Sriparna Saha, Pushpak Bhattacharyya, Minakshi Dhar</li>
<li>for: 这个论文的目的是提出一种多Modal临床对话概括生成任务，使用临床医生与患者之间的文本和视觉信息，并生成一个简洁的对话概括。</li>
<li>methods: 这种方法基于一个知识感知、多Modal、多任务的医疗领域标识和临床对话概括生成框架，使用一个适配器来把知识和视觉特征融合，并使用一个阻止机制来归一化混合特征向量。</li>
<li>results: 经过了大量的数据分析和评估，研究发现：（1）视觉信息具有重要的意义，（2）增加知识感知可以提高概括的准确性和医学实体保持性，（3）医疗部门标识和临床对话概括之间存在 statistically significant 的相关性。<details>
<summary>Abstract</summary>
With the advancement of telemedicine, both researchers and medical practitioners are working hand-in-hand to develop various techniques to automate various medical operations, such as diagnosis report generation. In this paper, we first present a multi-modal clinical conversation summary generation task that takes a clinician-patient interaction (both textual and visual information) and generates a succinct synopsis of the conversation. We propose a knowledge-infused, multi-modal, multi-tasking medical domain identification and clinical conversation summary generation (MM-CliConSummation) framework. It leverages an adapter to infuse knowledge and visual features and unify the fused feature vector using a gated mechanism. Furthermore, we developed a multi-modal, multi-intent clinical conversation summarization corpus annotated with intent, symptom, and summary. The extensive set of experiments, both quantitatively and qualitatively, led to the following findings: (a) critical significance of visuals, (b) more precise and medical entity preserving summary with additional knowledge infusion, and (c) a correlation between medical department identification and clinical synopsis generation. Furthermore, the dataset and source code are available at https://github.com/NLP-RL/MM-CliConSummation.
</details>
<details>
<summary>摘要</summary>
随着电子医疗的发展，研究人员和医生们在合作开发了许多自动化医疗操作的技术，其中包括诊断报告生成。本文首先介绍了一种多模态临床对话总结生成任务，该任务可以从医生与病人的互动（包括文本和视觉信息）中生成简洁的对话总结。我们提出了一个知识激发、多模态、多任务医疗领域识别和临床对话总结框架（MM-CliConSummation）。它利用一个适配器来激发知识和视觉特征，并使用一个阻止机制将混合特征vector化。此外，我们还制作了多模态、多意向的临床对话总结数据集，该数据集包括意向、症状和总结的标注。经过了广泛的实验，我们得到了以下发现：（a）视觉信息的重要性，（b）增加知识激发后的 preciser和医学实体保持的总结，以及（c）医疗部门识别和临床总结生成之间的相关性。此外，数据集和源代码可以在GitHub上下载。
</details></li>
</ul>
<hr>
<h2 id="MindGPT-Interpreting-What-You-See-with-Non-invasive-Brain-Recordings"><a href="#MindGPT-Interpreting-What-You-See-with-Non-invasive-Brain-Recordings" class="headerlink" title="MindGPT: Interpreting What You See with Non-invasive Brain Recordings"></a>MindGPT: Interpreting What You See with Non-invasive Brain Recordings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15729">http://arxiv.org/abs/2309.15729</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jxuanc/mindgpt">https://github.com/jxuanc/mindgpt</a></li>
<li>paper_authors: Jiaxuan Chen, Yu Qi, Yueming Wang, Gang Pan</li>
<li>for: 这个研究的目的是使用非侵入式脑记录技术来解码视觉内容。</li>
<li>methods: 这个研究使用了一种非侵入式神经解码器，称为 MindGPT，将视觉刺激转化为自然语言。该模型基于一种视觉导向的神经编码器，并使用大语言模型GPT来实现语言semantic的导向。</li>
<li>results: 实验结果表明，MindGPT模型可以准确地将视觉信息转化为自然语言，并且可以评估视觉属性对语言 semantics的贡献。此外，研究还发现，高级视觉 cortex (HVC) 比低级视觉 cortex (LVC) 更具Semantic信息，只使用 HVC 可以重建大多数Semantic信息。<details>
<summary>Abstract</summary>
Decoding of seen visual contents with non-invasive brain recordings has important scientific and practical values. Efforts have been made to recover the seen images from brain signals. However, most existing approaches cannot faithfully reflect the visual contents due to insufficient image quality or semantic mismatches. Compared with reconstructing pixel-level visual images, speaking is a more efficient and effective way to explain visual information. Here we introduce a non-invasive neural decoder, termed as MindGPT, which interprets perceived visual stimuli into natural languages from fMRI signals. Specifically, our model builds upon a visually guided neural encoder with a cross-attention mechanism, which permits us to guide latent neural representations towards a desired language semantic direction in an end-to-end manner by the collaborative use of the large language model GPT. By doing so, we found that the neural representations of the MindGPT are explainable, which can be used to evaluate the contributions of visual properties to language semantics. Our experiments show that the generated word sequences truthfully represented the visual information (with essential details) conveyed in the seen stimuli. The results also suggested that with respect to language decoding tasks, the higher visual cortex (HVC) is more semantically informative than the lower visual cortex (LVC), and using only the HVC can recover most of the semantic information. The code of the MindGPT model will be publicly available at https://github.com/JxuanC/MindGPT.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXT科学和实践中的重要价值在于解码见过的视觉内容。尝试将视觉信号中的图像重建。然而，大多数现有方法无法准确表达见过的图像，因为图像质量不够高或 semantic mismatch。相比于重建像素级视觉图像，说出视觉信息是更高效和有效的方式。我们介绍了一种非侵入性神经解码器，称为 MindGPT，它将感知的视觉刺激转化为自然语言 from fMRI 信号中。具体来说，我们的模型基于一个视觉驱动的神经编码器，其中包含一个 cross-attention 机制，使得我们可以通过携带大语言模型 GPT 的协同使用，将 latent 神经表示向 желаем的语言semantic 方向协调。由此，我们发现 MindGPT 的神经表示是可解释的，可以用于评估视觉属性对语言semantic 的贡献。我们的实验表明，生成的单词序列准确表达了看过的视觉信息（包括关键信息）。结果还表明，高级视觉区域 (HVC) 比低级视觉区域 (LVC) 更具Semantic 信息，只使用 HVC 可以回归大多数semantic 信息。MindGPT 模型的代码将在 https://github.com/JxuanC/MindGPT 公共可用。
</details></li>
</ul>
<hr>
<h2 id="Where-Are-We-So-Far-Understanding-Data-Storytelling-Tools-from-the-Perspective-of-Human-AI-Collaboration"><a href="#Where-Are-We-So-Far-Understanding-Data-Storytelling-Tools-from-the-Perspective-of-Human-AI-Collaboration" class="headerlink" title="Where Are We So Far? Understanding Data Storytelling Tools from the Perspective of Human-AI Collaboration"></a>Where Are We So Far? Understanding Data Storytelling Tools from the Perspective of Human-AI Collaboration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15723">http://arxiv.org/abs/2309.15723</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haotian Li, Yun Wang, Huamin Qu</li>
<li>for: 这篇论文旨在探讨人工智能（AI）在数据故事创作中的支持和增强，但现有的研究很少从人机合作角度来检视现有的数据故事创作工具，这限制了研究人员对现有工具的反思和学习。</li>
<li>methods: 本文采用了一个框架，从数据故事创作过程中的不同阶段和人机合作角色来分析现有工具，包括分析、规划、实施和沟通阶段，以及人类和AI在每个阶段的角色，如创作者、助手、优化者和审查者。</li>
<li>results: 通过分析，我们发现现有工具中的常见合作模式，总结了这些模式所学习的经验教训，并进一步阐述了人机合作在数据故事创作中的研究机遇。<details>
<summary>Abstract</summary>
Data storytelling is powerful for communicating data insights, but it requires diverse skills and considerable effort from human creators. Recent research has widely explored the potential for artificial intelligence (AI) to support and augment humans in data storytelling. However, there lacks a systematic review to understand data storytelling tools from the perspective of human-AI collaboration, which hinders researchers from reflecting on the existing collaborative tool designs that promote humans' and AI's advantages and mitigate their shortcomings. This paper investigated existing tools with a framework from two perspectives: the stages in the storytelling workflow where a tool serves, including analysis, planning, implementation, and communication, and the roles of humans and AI in each stage, such as creators, assistants, optimizers, and reviewers. Through our analysis, we recognize the common collaboration patterns in existing tools, summarize lessons learned from these patterns, and further illustrate research opportunities for human-AI collaboration in data storytelling.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>数据故事传递具有强大的沟通数据发现力，但是需要多种技能和较大的人类创造者的努力。最近的研究广泛探讨了人工智能（AI）支持和加强人类数据故事传递的潜力。然而，还缺乏一个系统性的审查，以便研究人员反思现有的合作工具的设计，以便利用人类和AI的优势，避免他们的缺点。这篇论文调查了现有工具，使用了两个视角：在数据故事传递过程中工具服务的阶段，包括分析、规划、实施和沟通，以及在每个阶段中人类和AI的角色，如创作者、助手、优化者和审查者。通过我们的分析，我们认可现有工具的共同合作模式，总结了这些模式所学到的经验教训，并进一步阐述了人类-AI合作在数据故事传递中的研究机会。
</details></li>
</ul>
<hr>
<h2 id="Model-Share-AI-An-Integrated-Toolkit-for-Collaborative-Machine-Learning-Model-Development-Provenance-Tracking-and-Deployment-in-Python"><a href="#Model-Share-AI-An-Integrated-Toolkit-for-Collaborative-Machine-Learning-Model-Development-Provenance-Tracking-and-Deployment-in-Python" class="headerlink" title="Model Share AI: An Integrated Toolkit for Collaborative Machine Learning Model Development, Provenance Tracking, and Deployment in Python"></a>Model Share AI: An Integrated Toolkit for Collaborative Machine Learning Model Development, Provenance Tracking, and Deployment in Python</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15719">http://arxiv.org/abs/2309.15719</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heinrich Peters, Michael Parrott</li>
<li>For: The paper aims to address the issue of many machine learning (ML) projects never progressing past the proof-of-concept stage by introducing an easy-to-use platform called Model Share AI (AIMS) to streamline collaborative model development, model provenance tracking, and model deployment.* Methods: The paper describes the features of AIMS, including collaborative project spaces, a standardized model evaluation process, and the ability to deploy ML models built in various frameworks into live REST APIs and automatically generated web apps with minimal code.* Results: The paper highlights the potential of AIMS to make ML research more applicable to real-world challenges by facilitating collaborative model development, capturing model performance and metadata for provenance tracking, and providing a user-friendly platform for deploying ML models to non-technical end-users through web apps.Here are the three points in Simplified Chinese:* For: 这篇论文目标是解决机器学习（ML）项目常常无法继续进行证明阶段的问题，并提出了一个易于使用的平台called Model Share AI（AIMS），用于协作模型开发、追踪模型来源和模型部署。* Methods: 论文描述了 AIMS 的特点，包括协作项目空间、基于不同框架的模型部署、以及使得 ML 模型在 REST API 和自动生成的 web 应用中部署的最小代码。* Results: 论文强调了 AIMS 可能使 ML 研究更加适用于实际挑战，通过促进协作模型开发、记录模型性能和元数据进行追踪、以及提供易于使用的平台来帮助非技术用户通过 web 应用访问 ML 模型。<details>
<summary>Abstract</summary>
Machine learning (ML) has the potential to revolutionize a wide range of research areas and industries, but many ML projects never progress past the proof-of-concept stage. To address this issue, we introduce Model Share AI (AIMS), an easy-to-use MLOps platform designed to streamline collaborative model development, model provenance tracking, and model deployment, as well as a host of other functions aiming to maximize the real-world impact of ML research. AIMS features collaborative project spaces and a standardized model evaluation process that ranks model submissions based on their performance on unseen evaluation data, enabling collaborative model development and crowd-sourcing. Model performance and various model metadata are automatically captured to facilitate provenance tracking and allow users to learn from and build on previous submissions. Additionally, AIMS allows users to deploy ML models built in Scikit-Learn, TensorFlow Keras, PyTorch, and ONNX into live REST APIs and automatically generated web apps with minimal code. The ability to deploy models with minimal effort and to make them accessible to non-technical end-users through web apps has the potential to make ML research more applicable to real-world challenges.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）有望革命化广泛的研究领域和行业，但许多ML项目很难进入实际应用阶段。为解决这问题，我们介绍Model Share AI（AIMS），一个易用的MLOps平台，旨在协同开发模型、追踪模型来源、模型部署以及多种其他功能，以最大化ML研究的实际影响。AIMS提供了协同项目空间和基于未见评估数据的模型评价过程，可以促进协同开发和招募模型。模型性能和多种模型元数据会自动记录，以便追踪模型来源和启发新 submission。此外，AIMS还允许用户通过 minimum code 将 Scikit-Learn、TensorFlow Keras、PyTorch 和 ONNX 中的模型部署到live REST API 和自动生成的网页应用程序中，以便让 ML 研究更加适应实际挑战。
</details></li>
</ul>
<hr>
<h2 id="Brave-new-world-Artificial-Intelligence-in-teaching-and-learning"><a href="#Brave-new-world-Artificial-Intelligence-in-teaching-and-learning" class="headerlink" title="Brave new world: Artificial Intelligence in teaching and learning"></a>Brave new world: Artificial Intelligence in teaching and learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06856">http://arxiv.org/abs/2310.06856</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrian Groza, Anca Marginean</li>
<li>for: 这篇论文主要是为了探讨大语言模型在教学和学习中的应用，以及教育领域中已经发生的人工智能事件，并提出了在大学中引入人工智能政策的必要性和紧迫性。</li>
<li>methods: 本论文使用了大语言模型在教学和学习中的应用，以及已经发生的人工智能事件，以探讨教育领域中的人工智能应用。</li>
<li>results: 本论文认为，每所高等教育机构应该有一个人工智能政策，以提高教育工具的认识，并减少教育领域中的人工智能事件风险。<details>
<summary>Abstract</summary>
We exemplify how Large Language Models are used in both teaching and learning. We also discuss the AI incidents that have already occurred in the education domain, and we argue for the urgent need to introduce AI policies in universities and for the ongoing strategies to regulate AI. Regarding policy for AI, our view is that each institution should have a policy for AI in teaching and learning. This is important from at least twofolds: (i) to raise awareness on the numerous educational tools that can both positively and negatively affect education; (ii) to minimise the risk of AI incidents in education.
</details>
<details>
<summary>摘要</summary>
我团队讲述了大语言模型在教学和学习中的应用，以及教育领域已经发生的人工智能事件。我们认为，每所学府应该制定一份人工智能教学政策，这有两点重要性：（一）提高教育工具的认识，这些工具可以 both positively和negativelyaffect教育;（二）减少教育领域的人工智能事件风险。
</details></li>
</ul>
<hr>
<h2 id="Identifying-and-Mitigating-Privacy-Risks-Stemming-from-Language-Models-A-Survey"><a href="#Identifying-and-Mitigating-Privacy-Risks-Stemming-from-Language-Models-A-Survey" class="headerlink" title="Identifying and Mitigating Privacy Risks Stemming from Language Models: A Survey"></a>Identifying and Mitigating Privacy Risks Stemming from Language Models: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01424">http://arxiv.org/abs/2310.01424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Victoria Smith, Ali Shahin Shamsabadi, Carolyn Ashurst, Adrian Weller</li>
<li>for: 本研究旨在帮助研究人员和政策制定者更好地理解语言模型（LM）的隐私风险和mitigation策略，包括需要更多的研究和关注的方向。</li>
<li>methods: 本研究使用了一种稍加分析语言模型（LM）的隐私风险和mitigation策略的方法，包括分析LM的攻击和防御方法，并对现有的mitigation策略进行了评估和分析。</li>
<li>results: 本研究通过分析了多种隐私攻击和mitigation策略，并对现有的mitigation策略进行了评估和分析，得出了一些结论和建议，包括LM的攻击和防御方法，以及需要更多的研究和关注的方向。<details>
<summary>Abstract</summary>
Rapid advancements in language models (LMs) have led to their adoption across many sectors. Alongside the potential benefits, such models present a range of risks, including around privacy. In particular, as LMs have grown in size, the potential to memorise aspects of their training data has increased, resulting in the risk of leaking private information. As LMs become increasingly widespread, it is vital that we understand such privacy risks and how they might be mitigated. To help researchers and policymakers understand the state of knowledge around privacy attacks and mitigations, including where more work is needed, we present the first technical survey on LM privacy. We (i) identify a taxonomy of salient dimensions where attacks differ on LMs, (ii) survey existing attacks and use our taxonomy of dimensions to highlight key trends, (iii) discuss existing mitigation strategies, highlighting their strengths and limitations, identifying key gaps and demonstrating open problems and areas for concern.
</details>
<details>
<summary>摘要</summary>
快速发展的语言模型（LM）在多个领域得到广泛应用，同时也存在一系列的风险，包括隐私问题。特别是LM的大小增加后，可能吸收训练数据的memory risk增加，可能导致泄露private information。随着LM的普及，我们必须了解这些隐私风险，并研究如何 Mitigate them。为了帮助研究人员和政策制定者更好地理解隐私攻击和 Mitigation Strategies，我们提出了语言模型隐私技术的首次技术评估。我们（i）确定了LM隐私攻击的重要维度，（ii）survey了现有的攻击方法，并使用我们的维度分类来描述主要趋势，（iii）讨论了现有的 Mitigation Strategies， highlighting their strengths and limitations，并识别主要的缺陷和开放问题。
</details></li>
</ul>
<hr>
<h2 id="Integrating-LLM-EEG-and-Eye-Tracking-Biomarker-Analysis-for-Word-Level-Neural-State-Classification-in-Semantic-Inference-Reading-Comprehension"><a href="#Integrating-LLM-EEG-and-Eye-Tracking-Biomarker-Analysis-for-Word-Level-Neural-State-Classification-in-Semantic-Inference-Reading-Comprehension" class="headerlink" title="Integrating LLM, EEG, and Eye-Tracking Biomarker Analysis for Word-Level Neural State Classification in Semantic Inference Reading Comprehension"></a>Integrating LLM, EEG, and Eye-Tracking Biomarker Analysis for Word-Level Neural State Classification in Semantic Inference Reading Comprehension</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15714">http://arxiv.org/abs/2309.15714</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhong Zhang, Qin Li, Sujal Nahata, Tasnia Jamal, Shih-kuen Cheng, Gert Cauwenberghs, Tzyy-Ping Jung</li>
<li>for: This pilot study aims to provide insights into individuals’ neural states during a semantic relation reading-comprehension task.</li>
<li>methods: The study jointly analyzes LLMs, eye-gaze, and electroencephalographic (EEG) data to study how the brain processes words with varying degrees of relevance to a keyword during reading.</li>
<li>results: The best validation accuracy in this word-level classification is over 60% across 12 subjects. Words of high relevance to the inference keyword had significantly more eye fixations per word compared to words of low relevance.Here is the same information in Simplified Chinese text:</li>
<li>for: 这个飞行试验的目的是研究人类在 semantic relation 读写理解任务中的 neural 状态。</li>
<li>methods: 这个研究将jointly 分析 LLMS，eye-gaze，和电enzephalographic（EEG）数据，以研究阅读中关键字的 brain 处理词语 varying degrees of relevance 的过程。</li>
<li>results: 这个word-level 分类的最佳验证精度超过 60%  across 12 个主体。关键字 relevance 高的词语在阅读中有significantly 更多的眼动 Fixations per word。<details>
<summary>Abstract</summary>
With the recent proliferation of large language models (LLMs), such as Generative Pre-trained Transformers (GPT), there has been a significant shift in exploring human and machine comprehension of semantic language meaning. This shift calls for interdisciplinary research that bridges cognitive science and natural language processing (NLP). This pilot study aims to provide insights into individuals' neural states during a semantic relation reading-comprehension task. We propose jointly analyzing LLMs, eye-gaze, and electroencephalographic (EEG) data to study how the brain processes words with varying degrees of relevance to a keyword during reading. We also use a feature engineering approach to improve the fixation-related EEG data classification while participants read words with high versus low relevance to the keyword. The best validation accuracy in this word-level classification is over 60\% across 12 subjects. Words of high relevance to the inference keyword had significantly more eye fixations per word: 1.0584 compared to 0.6576 when excluding no-fixation words, and 1.5126 compared to 1.4026 when including them. This study represents the first attempt to classify brain states at a word level using LLM knowledge. It provides valuable insights into human cognitive abilities and the realm of Artificial General Intelligence (AGI), and offers guidance for developing potential reading-assisted technologies.
</details>
<details>
<summary>摘要</summary>
With the recent proliferation of large language models (LLMs), such as Generative Pre-trained Transformers (GPT), there has been a significant shift in exploring human and machine comprehension of semantic language meaning. This shift calls for interdisciplinary research that bridges cognitive science and natural language processing (NLP). This pilot study aims to provide insights into individuals' neural states during a semantic relation reading-comprehension task. We propose jointly analyzing LLMs, eye-gaze, and electroencephalographic (EEG) data to study how the brain processes words with varying degrees of relevance to a keyword during reading. We also use a feature engineering approach to improve the fixation-related EEG data classification while participants read words with high versus low relevance to the keyword. The best validation accuracy in this word-level classification is over 60\% across 12 subjects. Words of high relevance to the inference keyword had significantly more eye fixations per word: 1.0584 compared to 0.6576 when excluding no-fixation words, and 1.5126 compared to 1.4026 when including them. This study represents the first attempt to classify brain states at a word level using LLM knowledge. It provides valuable insights into human cognitive abilities and the realm of Artificial General Intelligence (AGI), and offers guidance for developing potential reading-assisted technologies.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. The traditional Chinese form of the text is also available upon request.
</details></li>
</ul>
<hr>
<h2 id="HyPoradise-An-Open-Baseline-for-Generative-Speech-Recognition-with-Large-Language-Models"><a href="#HyPoradise-An-Open-Baseline-for-Generative-Speech-Recognition-with-Large-Language-Models" class="headerlink" title="HyPoradise: An Open Baseline for Generative Speech Recognition with Large Language Models"></a>HyPoradise: An Open Baseline for Generative Speech Recognition with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15701">http://arxiv.org/abs/2309.15701</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hypotheses-paradise/hypo2trans">https://github.com/hypotheses-paradise/hypo2trans</a></li>
<li>paper_authors: Chen Chen, Yuchen Hu, Chao-Han Huck Yang, Sabato Macro Siniscalchi, Pin-Yu Chen, Eng Siong Chng</li>
<li>for: 本研究目的是提出一种基于大语言模型（LLM）的自动语音识别（ASR）错误修复方法，以提高ASR系统在不良条件下的表现。</li>
<li>methods: 本研究使用了一个开源的 benchmark，其中包含了一个新的数据集（HyPoradise，HP），该数据集包含了超过334,000个N-best假设和其相应的准确的转录。三种基于LLM的错误修复技术被研究，其中一种使用了reasonable prompt和其生成能力来修复缺失的token。</li>
<li>results: 实验证明，提出的方法可以超越传统的重新排序基于方法的上限，并且LLM的reasonable prompt和生成能力可以修复缺失的token。 results publicly accessible，以便用于可重现的管道中。<details>
<summary>Abstract</summary>
Advancements in deep neural networks have allowed automatic speech recognition (ASR) systems to attain human parity on several publicly available clean speech datasets. However, even state-of-the-art ASR systems experience performance degradation when confronted with adverse conditions, as a well-trained acoustic model is sensitive to variations in the speech domain, e.g., background noise. Intuitively, humans address this issue by relying on their linguistic knowledge: the meaning of ambiguous spoken terms is usually inferred from contextual cues thereby reducing the dependency on the auditory system. Inspired by this observation, we introduce the first open-source benchmark to utilize external large language models (LLMs) for ASR error correction, where N-best decoding hypotheses provide informative elements for true transcription prediction. This approach is a paradigm shift from the traditional language model rescoring strategy that can only select one candidate hypothesis as the output transcription. The proposed benchmark contains a novel dataset, HyPoradise (HP), encompassing more than 334,000 pairs of N-best hypotheses and corresponding accurate transcriptions across prevalent speech domains. Given this dataset, we examine three types of error correction techniques based on LLMs with varying amounts of labeled hypotheses-transcription pairs, which gains a significant word error rate (WER) reduction. Experimental evidence demonstrates the proposed technique achieves a breakthrough by surpassing the upper bound of traditional re-ranking based methods. More surprisingly, LLM with reasonable prompt and its generative capability can even correct those tokens that are missing in N-best list. We make our results publicly accessible for reproducible pipelines with released pre-trained models, thus providing a new evaluation paradigm for ASR error correction with LLMs.
</details>
<details>
<summary>摘要</summary>
深度神经网络技术的进步使得自动语音识别（ASR）系统可以达到人类水平在一些公开的干净语音数据集上。然而，即使使用最新的ASR系统，它们在面临不利条件时会经受性能下降，因为一个干净的语音模型对语音频谱的变化非常敏感。人类在面临这种问题时会依靠语言知识：在不同语音频谱中，人们会根据上下文提供的听觉信息来推断不确定的 spoken terms的意思，从而减少对听觉系统的依赖。 Drawing inspiration from this observation, we introduce the first open-source benchmark to utilize external large language models (LLMs) for ASR error correction, where N-best decoding hypotheses provide informative elements for true transcription prediction. This approach is a paradigm shift from the traditional language model rescoring strategy that can only select one candidate hypothesis as the output transcription. The proposed benchmark contains a novel dataset, HyPoradise (HP), encompassing more than 334,000 pairs of N-best hypotheses and corresponding accurate transcriptions across prevalent speech domains. Given this dataset, we examine three types of error correction techniques based on LLMs with varying amounts of labeled hypotheses-transcription pairs, which gains a significant word error rate (WER) reduction. Experimental evidence demonstrates the proposed technique achieves a breakthrough by surpassing the upper bound of traditional re-ranking based methods. Moreover, LLM with reasonable prompt and its generative capability can even correct those tokens that are missing in N-best list. We make our results publicly accessible for reproducible pipelines with released pre-trained models, thus providing a new evaluation paradigm for ASR error correction with LLMs.
</details></li>
</ul>
<hr>
<h2 id="Deep-Model-Fusion-A-Survey"><a href="#Deep-Model-Fusion-A-Survey" class="headerlink" title="Deep Model Fusion: A Survey"></a>Deep Model Fusion: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15698">http://arxiv.org/abs/2309.15698</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Weishi Li, Yong Peng, Miao Zhang, Liang Ding, Han Hu, Li Shen<br>for: 这个论文主要是为了探讨深度模型融合技术，尤其是在大规模深度学习模型（如LLMs和基础模型）上进行深度模型融合的挑战和可能性。methods: 这个论文主要分析了四种深度模型融合方法：（1）”Mode connectivity”，通过非增加损失的路径连接解决方案的重要性;（2）”Alignment”，匹配神经网络中单元的匹配以创造更好的融合条件;（3）”Weight average”，一种经典的模型融合方法，将多个模型的权重平均为更加准确的结果;（4）”Ensemble learning”，将多个不同模型的输出结合，以提高最终模型的准确性和可靠性。results: 这个论文分析了深度模型融合技术面临的挑战，并提出了未来研究的可能性。它还对不同的模型融合方法进行了分析和比较，帮助读者更深入地理解不同方法之间的相互关系和实际应用方法。<details>
<summary>Abstract</summary>
Deep model fusion/merging is an emerging technique that merges the parameters or predictions of multiple deep learning models into a single one. It combines the abilities of different models to make up for the biases and errors of a single model to achieve better performance. However, deep model fusion on large-scale deep learning models (e.g., LLMs and foundation models) faces several challenges, including high computational cost, high-dimensional parameter space, interference between different heterogeneous models, etc. Although model fusion has attracted widespread attention due to its potential to solve complex real-world tasks, there is still a lack of complete and detailed survey research on this technique. Accordingly, in order to understand the model fusion method better and promote its development, we present a comprehensive survey to summarize the recent progress. Specifically, we categorize existing deep model fusion methods as four-fold: (1) "Mode connectivity", which connects the solutions in weight space via a path of non-increasing loss, in order to obtain better initialization for model fusion; (2) "Alignment" matches units between neural networks to create better conditions for fusion; (3) "Weight average", a classical model fusion method, averages the weights of multiple models to obtain more accurate results closer to the optimal solution; (4) "Ensemble learning" combines the outputs of diverse models, which is a foundational technique for improving the accuracy and robustness of the final model. In addition, we analyze the challenges faced by deep model fusion and propose possible research directions for model fusion in the future. Our review is helpful in deeply understanding the correlation between different model fusion methods and practical application methods, which can enlighten the research in the field of deep model fusion.
</details>
<details>
<summary>摘要</summary>
深度模型融合/合并是一种出现在的技术，它将多个深度学习模型的参数或预测融合到一起，以实现更好的性能。它利用不同模型的能力来补偿单个模型的偏见和错误，以解决复杂的实际任务。然而，深度模型融合在大规模深度学习模型（例如LLMs和基础模型）上面临多种挑战，包括高计算成本、高维度参数空间以及不同型号之间的干扰等。虽然模型融合吸引了广泛的关注，因为它有可能解决复杂的实际任务，但是还没有完整和详细的报告研究这种技术。因此，为了更好地理解模型融合方法，我们提供了一份完整的报告，总结了最近的进展。 Specifically，我们将现有的深度模型融合方法分为四类：（1）“模式连接”，通过非增加损失的路径连接解 Solution Space，以获得更好的初始化 для模型融合;（2）“匹配”，将神经网络中的单元匹配，创造更好的融合 conditio;（3）“Weight average”，一种经典的模型融合方法，将多个模型的权重平均，以获得更加准确的结果，更近于优化解决方案;（4）“集成学习”，将多个不同模型的输出融合，是深度学习领域的基础技术，可以提高最终模型的准确性和鲁棒性。此外，我们还分析了深度模型融合所面临的挑战，并提出了未来模型融合的可能的研究方向。我们的评论对深度模型融合的深入理解和实际应用方法之间的相互关系做出了贡献，可以推动深度模型融合领域的研究。
</details></li>
</ul>
<hr>
<h2 id="Genetic-Algorithm-Based-Dynamic-Backdoor-Attack-on-Federated-Learning-Based-Network-Traffic-Classification"><a href="#Genetic-Algorithm-Based-Dynamic-Backdoor-Attack-on-Federated-Learning-Based-Network-Traffic-Classification" class="headerlink" title="Genetic Algorithm-Based Dynamic Backdoor Attack on Federated Learning-Based Network Traffic Classification"></a>Genetic Algorithm-Based Dynamic Backdoor Attack on Federated Learning-Based Network Traffic Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06855">http://arxiv.org/abs/2310.06855</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahmoud Nazzal, Nura Aljaafari, Ahmed Sawalmeh, Abdallah Khreishah, Muhammad Anan, Abdulelah Algosaibi, Mohammed Alnaeem, Adel Aldalbahi, Abdulaziz Alhumam, Conrado P. Vizcarra, Shadan Alhamed</li>
<li>for: 这个研究是为了探讨基于联合学习的网络流量分类模型是否受到后门攻击的问题。</li>
<li>methods: 本研究使用了一种基于遗传算法的后门攻击方法，叫做GABAttack，它利用遗传算法来优化后门触发模式的值和位置，以 guarantees a better fit with the input and the model。</li>
<li>results: 实验结果显示GABAttack可以在实际的网络数据上得到良好的成果，并且可以在不同的情况下保持这些成果。这个研究作为一个警示，让网络安全专家和实践者为这种攻击进行防御措施。<details>
<summary>Abstract</summary>
Federated learning enables multiple clients to collaboratively contribute to the learning of a global model orchestrated by a central server. This learning scheme promotes clients' data privacy and requires reduced communication overheads. In an application like network traffic classification, this helps hide the network vulnerabilities and weakness points. However, federated learning is susceptible to backdoor attacks, in which adversaries inject manipulated model updates into the global model. These updates inject a salient functionality in the global model that can be launched with specific input patterns. Nonetheless, the vulnerability of network traffic classification models based on federated learning to these attacks remains unexplored. In this paper, we propose GABAttack, a novel genetic algorithm-based backdoor attack against federated learning for network traffic classification. GABAttack utilizes a genetic algorithm to optimize the values and locations of backdoor trigger patterns, ensuring a better fit with the input and the model. This input-tailored dynamic attack is promising for improved attack evasiveness while being effective. Extensive experiments conducted over real-world network datasets validate the success of the proposed GABAttack in various situations while maintaining almost invisible activity. This research serves as an alarming call for network security experts and practitioners to develop robust defense measures against such attacks.
</details>
<details>
<summary>摘要</summary>
federated learning 可以让多个客户端共同参与到全球模型的学习中，由中央服务器进行协调。这种学习方式可以保护客户端的数据隐私，并减少通信开销。在应用于网络流量分类中，这会隐藏网络漏洞和弱点。然而， federated learning 受到后门攻击的威胁，攻击者可以在全球模型中注入修改后的模型更新。这些更新会在特定的输入模式下引入一个突出的功能，可以通过特定的输入来启动。然而，基于 federated learning 的网络流量分类模型对这些攻击的抗性尚未得到探讨。在这篇论文中，我们提出了 GABAttack，一种基于遗传算法的后门攻击方法，用于攻击 federated learning 的网络流量分类模型。GABAttack 使用遗传算法来优化后门触发模式的值和位置，以确保更好地适应输入和模型。这种输入特定的动态攻击可以提高攻击的逃避能力，同时保持高效。我们对实际的网络数据进行了广泛的实验，并证明了 GABAttack 在不同的情况下都有很好的成功率，同时几乎无法察见。这些研究作为一个警告，鼓励网络安全专家和实践者开发robust的防御措施来应对这类攻击。
</details></li>
</ul>
<hr>
<h2 id="Generative-Speech-Recognition-Error-Correction-with-Large-Language-Models-and-Task-Activating-Prompting"><a href="#Generative-Speech-Recognition-Error-Correction-with-Large-Language-Models-and-Task-Activating-Prompting" class="headerlink" title="Generative Speech Recognition Error Correction with Large Language Models and Task-Activating Prompting"></a>Generative Speech Recognition Error Correction with Large Language Models and Task-Activating Prompting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15649">http://arxiv.org/abs/2309.15649</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao-Han Huck Yang, Yile Gu, Yi-Chieh Liu, Shalini Ghosh, Ivan Bulyko, Andreas Stolcke</li>
<li>for: 研究大型自然语言模型（LLM）是否可以作为语音识别后处理器，进行重分配和错误修正。</li>
<li>methods: 研究不同的提示方法，包括零拟合和少量拟合在Context learning中，以及一种新的任务活动提示方法，它结合了 causal instructions和示例来增加其上下文窗口。</li>
<li>results: 研究发现，通过冻结LLM进行重分配，只需在Context learning中进行几个批量训练，就可以达到与预先定制的语言模型相当的性能，并且通过结合提示技术和微调来实现错误率下降至N-best oracle水平。<details>
<summary>Abstract</summary>
We explore the ability of large language models (LLMs) to act as speech recognition post-processors that perform rescoring and error correction. Our first focus is on instruction prompting to let LLMs perform these task without fine-tuning, for which we evaluate different prompting schemes, both zero- and few-shot in-context learning, and a novel task activation prompting method that combines causal instructions and demonstration to increase its context windows. Next, we show that rescoring only by in-context learning with frozen LLMs achieves results that are competitive with rescoring by domain-tuned LMs, using a pretrained first-pass recognition system and rescoring output on two out-of-domain tasks (ATIS and WSJ). By combining prompting techniques with fine-tuning we achieve error rates below the N-best oracle level, showcasing the generalization power of the LLMs.
</details>
<details>
<summary>摘要</summary>
我团队 investigate LLMs 的能力以干作 speech recognition 后处理器，包括重新评分和错误修复。我们首先关注 instruction prompting，以便 LLMs 可以无需微调完成这些任务。我们评估了不同的提示方案，包括零射频和几射频在 Context 学习，以及一种新的任务活动提示方法，该方法结合 causal  instrucitons 和 demonstration，以增加其上下文窗口。接着，我们显示了只使用冰结 LLMs 进行 in-context learning 可以达到与预训练的 domain-tuned LMs 相当的结果，使用预训练的 first-pass recognition 系统和重新评分输出在两个 out-of-domain 任务（ATIS 和 WSJ）上。通过结合提示技术与微调，我们实现了错误率低于 N-best oracle 水平，展示了 LLMs 的泛化能力。
</details></li>
</ul>
<hr>
<h2 id="Hedging-Properties-of-Algorithmic-Investment-Strategies-using-Long-Short-Term-Memory-and-Time-Series-models-for-Equity-Indices"><a href="#Hedging-Properties-of-Algorithmic-Investment-Strategies-using-Long-Short-Term-Memory-and-Time-Series-models-for-Equity-Indices" class="headerlink" title="Hedging Properties of Algorithmic Investment Strategies using Long Short-Term Memory and Time Series models for Equity Indices"></a>Hedging Properties of Algorithmic Investment Strategies using Long Short-Term Memory and Time Series models for Equity Indices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15640">http://arxiv.org/abs/2309.15640</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jakub Michańków, Paweł Sakowski, Robert Ślepaczuk</li>
<li>for: 这个论文旨在防范金融市场在金融危机时的风险投资 portfolio。</li>
<li>methods: 这篇论文提出了一种全新的多Asset ensemble algorithmic investment strategies（AIS）多元化风险投资策略，通过使用不同类型的数学模型（LSTM、ARIMA-GARCH、 momentum和 contrarian）生成价格预测，并将其用于生成投资信号。</li>
<li>results: 研究发现LSTM模型表现最佳，而使用比特币constructed AIS 是最佳多元化风险投资策略。此外，使用1小时数据也表现更好于使用日常数据。<details>
<summary>Abstract</summary>
This paper proposes a novel approach to hedging portfolios of risky assets when financial markets are affected by financial turmoils. We introduce a completely novel approach to diversification activity not on the level of single assets but on the level of ensemble algorithmic investment strategies (AIS) built based on the prices of these assets. We employ four types of diverse theoretical models (LSTM - Long Short-Term Memory, ARIMA-GARCH - Autoregressive Integrated Moving Average - Generalized Autoregressive Conditional Heteroskedasticity, momentum, and contrarian) to generate price forecasts, which are then used to produce investment signals in single and complex AIS. In such a way, we are able to verify the diversification potential of different types of investment strategies consisting of various assets (energy commodities, precious metals, cryptocurrencies, or soft commodities) in hedging ensemble AIS built for equity indices (S&P 500 index). Empirical data used in this study cover the period between 2004 and 2022. Our main conclusion is that LSTM-based strategies outperform the other models and that the best diversifier for the AIS built for the S&P 500 index is the AIS built for Bitcoin. Finally, we test the LSTM model for a higher frequency of data (1 hour). We conclude that it outperforms the results obtained using daily data.
</details>
<details>
<summary>摘要</summary>
Our main finding is that LSTM-based strategies outperform the other models, and the best diversifier for the AIS built for the S&P 500 index is the AIS built for Bitcoin. Furthermore, we test the LSTM model on a higher frequency of data (1 hour) and find that it outperforms the results obtained using daily data.
</details></li>
</ul>
<hr>
<h2 id="Learning-with-Noisy-Labels-for-Human-Fall-Events-Classification-Joint-Cooperative-Training-with-Trinity-Networks"><a href="#Learning-with-Noisy-Labels-for-Human-Fall-Events-Classification-Joint-Cooperative-Training-with-Trinity-Networks" class="headerlink" title="Learning with Noisy Labels for Human Fall Events Classification: Joint Cooperative Training with Trinity Networks"></a>Learning with Noisy Labels for Human Fall Events Classification: Joint Cooperative Training with Trinity Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06854">http://arxiv.org/abs/2310.06854</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leiyu Xie, Yang Sun, Syed Mohsen Naqvi</li>
<li>for: 这篇论文目的是提出一个简单 yet effective的方法来解决深度学习中的污染标签问题，以保护人类试验者的隐私。</li>
<li>methods: 这篇论文使用了一个名为“Joint Cooperative training with Trinity Networks”的方法（简称JoCoT），具有两个教师网络和一个学生网络，以改善混淆标签学习框架的稳定性和性能。</li>
<li>results: 根据实验结果，JoCoT 在高混淆率下表现出色，较前一代方法高5.17%和3.35%。具体来说，JoCoT 在 UP-Fall  dataset 上的平均 pairflip 和 symmetric 混淆率下，较前一代方法高5.17%和3.35%。<details>
<summary>Abstract</summary>
With the increasing ageing population, fall events classification has drawn much research attention. In the development of deep learning, the quality of data labels is crucial. Most of the datasets are labelled automatically or semi-automatically, and the samples may be mislabeled, which constrains the performance of Deep Neural Networks (DNNs). Recent research on noisy label learning confirms that neural networks first focus on the clean and simple instances and then follow the noisy and hard instances in the training stage. To address the learning with noisy label problem and protect the human subjects' privacy, we propose a simple but effective approach named Joint Cooperative training with Trinity Networks (JoCoT). To mitigate the privacy issue, human skeleton data are used. The robustness and performance of the noisy label learning framework is improved by using the two teacher modules and one student module in the proposed JoCoT. To mitigate the incorrect selections, the predictions from the teacher modules are applied with the consensus-based method to guide the student module training. The performance evaluation on the widely used UP-Fall dataset and comparison with the state-of-the-art, confirms the effectiveness of the proposed JoCoT in high noise rates. Precisely, JoCoT outperforms the state-of-the-art by 5.17% and 3.35% with the averaged pairflip and symmetric noises, respectively.
</details>
<details>
<summary>摘要</summary>
随着老龄化人口增长，落地事件分类得到了大量研究的关注。在深度学习的发展中，数据标签质量的影响是关键。大多数数据集都是自动或半自动标注的，因此样本可能会出现误标，这会限制深度神经网络（DNNs）的性能。最近关于噪音标签学习的研究表明，神经网络在训练阶段会首先学习清晰和简单的实例，然后遵循噪音和复杂的实例。为解决噪音标签学习问题并保护人类主体隐私，我们提出了一种简单 yet有效的方法 named Joint Cooperative training with Trinity Networks（JoCoT）。使用人体骨骼数据来 mitigate the privacy issue。通过使用两个教师模块和一个学生模块，我们提高了噪音标签学习框架的Robustness和性能。通过将教师模块的预测应用于学生模块的训练中，我们减少了错误选择的问题。在广泛使用的 UP-Fall 数据集上进行性能评估，我们发现 JoCoT 在高噪音率下能够超过状态艺术。具体来说，JoCoT 在 averaged pairflip 和 symmetric noise 下的平均性能高于状态艺术的 5.17% 和 3.35%。
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Study-of-AI-Generated-Text-Detection-Tools"><a href="#An-Empirical-Study-of-AI-Generated-Text-Detection-Tools" class="headerlink" title="An Empirical Study of AI Generated Text Detection Tools"></a>An Empirical Study of AI Generated Text Detection Tools</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01423">http://arxiv.org/abs/2310.01423</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arslan Akram</li>
<li>for: 这个研究的目的是为了填补现有的多Domain ChatGPT材料测试State-of-the-art API和工具的需求。</li>
<li>methods: 这个研究使用了一个大型的多Domain dataset，包括文章、摘要、故事、新闻和产品评论，并使用了六种人工智能文本标识系统进行测试。</li>
<li>results: 这个研究发现， Originality 在所有工具中表现最佳，具有97.0%的准确率。<details>
<summary>Abstract</summary>
Since ChatGPT has emerged as a major AIGC model, providing high-quality responses across a wide range of applications (including software development and maintenance), it has attracted much interest from many individuals. ChatGPT has great promise, but there are serious problems that might arise from its misuse, especially in the realms of education and public safety. Several AIGC detectors are available, and they have all been tested on genuine text. However, more study is needed to see how effective they are for multi-domain ChatGPT material. This study aims to fill this need by creating a multi-domain dataset for testing the state-of-the-art APIs and tools for detecting artificially generated information used by universities and other research institutions. A large dataset consisting of articles, abstracts, stories, news, and product reviews was created for this study. The second step is to use the newly created dataset to put six tools through their paces. Six different artificial intelligence (AI) text identification systems, including "GPTkit," "GPTZero," "Originality," "Sapling," "Writer," and "Zylalab," have accuracy rates between 55.29 and 97.0%. Although all the tools fared well in the evaluations, originality was particularly effective across the board.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)自从ChatGPT出现以来，它已经在各种应用程序中提供了高质量的响应，包括软件开发和维护，因此吸引了许多人的关注。ChatGPT拥有巨大的潜力，但是可能由其滥用而产生的问题很严重，特别是在教育和公共安全领域。目前有几种AIGC检测器可用，它们都在真实文本上进行测试。然而，更多的研究是需要了解多元领域ChatGPT材料的效果。这项研究目的是填充这个需求，通过创建一个多元领域数据集，用于测试当前最佳API和工具。一个大量的数据集，包括文章、摘要、故事、新闻和产品评论，被用于这项研究。第二步是使用新创建的数据集，对六种工具进行测试。六种人工智能文本标识系统，包括"GPTkit"、"GPTZero"、"Originality"、"Sapling"、"Writer"和"Zylalab"，在评估中的准确率分别为55.29%和97.0%。虽然所有工具在评估中表现良好，但"Originality"在整体上表现特别出色。
</details></li>
</ul>
<hr>
<h2 id="Perception-for-Humanoid-Robots"><a href="#Perception-for-Humanoid-Robots" class="headerlink" title="Perception for Humanoid Robots"></a>Perception for Humanoid Robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15616">http://arxiv.org/abs/2309.15616</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openhumanoids/oh-distro">https://github.com/openhumanoids/oh-distro</a></li>
<li>paper_authors: Arindam Roychoudhury, Shahram Khorshidi, Subham Agrawal, Maren Bennewitz</li>
<li>for: 本研究探讨了人工智能机器人 perceive 技术的最新发展和趋势。</li>
<li>methods: 本研究使用了多种感知模式和技术，包括视觉、听觉和感觉感知，以实现机器人与人类和环境的互动。</li>
<li>results: 研究发现，多感知模式的融合和机器学习技术在机器人内部状态估计、环境理解和人机交互方面具有广泛的应用前景。<details>
<summary>Abstract</summary>
Purpose of Review: The field of humanoid robotics, perception plays a fundamental role in enabling robots to interact seamlessly with humans and their surroundings, leading to improved safety, efficiency, and user experience. This scientific study investigates various perception modalities and techniques employed in humanoid robots, including visual, auditory, and tactile sensing by exploring recent state-of-the-art approaches for perceiving and understanding the internal state, the environment, objects, and human activities.   Recent Findings: Internal state estimation makes extensive use of Bayesian filtering methods and optimization techniques based on maximum a-posteriori formulation by utilizing proprioceptive sensing. In the area of external environment understanding, with an emphasis on robustness and adaptability to dynamic, unforeseen environmental changes, the new slew of research discussed in this study have focused largely on multi-sensor fusion and machine learning in contrast to the use of hand-crafted, rule-based systems. Human robot interaction methods have established the importance of contextual information representation and memory for understanding human intentions.   Summary: This review summarizes the recent developments and trends in the field of perception in humanoid robots. Three main areas of application are identified, namely, internal state estimation, external environment estimation, and human robot interaction. The applications of diverse sensor modalities in each of these areas are considered and recent significant works are discussed.
</details>
<details>
<summary>摘要</summary>
目的的检查：人类型 робоット学中，感知对于机器人与人类环境互动过程中的流畅性、安全性、效率和用户体验具有基本作用。这项科学研究探讨了人类型 робоット中不同感知模式和技术的应用，包括视觉、听觉和触觉感知，并探讨最新的状态艺术方法以及理解内部状态、环境、物体和人类活动的感知和理解方法。最新发现：机器人内部状态估计主要利用极大似然估计方法和优化技术，基于最大似然估计的形式ulation，利用 proprioceptive 感知。在机器人对外环境理解方面，研究者们主要关注多感知融合和机器学习，而不是使用手工、规则驱动的系统。人机交互方法也证明了 Contextual information representation和记忆的重要性，以便理解人类的意图。总结：这篇文章总结了最近在人类型 robot学中感知的发展和趋势。文章分为三个主要应用领域：内部状态估计、外部环境估计和人机交互。每个领域中的不同感知模式的应用和最新的重要成果都被考虑到。
</details></li>
</ul>
<hr>
<h2 id="Developing-automatic-verbatim-transcripts-for-international-multilingual-meetings-an-end-to-end-solution"><a href="#Developing-automatic-verbatim-transcripts-for-international-multilingual-meetings-an-end-to-end-solution" class="headerlink" title="Developing automatic verbatim transcripts for international multilingual meetings: an end-to-end solution"></a>Developing automatic verbatim transcripts for international multilingual meetings: an end-to-end solution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15609">http://arxiv.org/abs/2309.15609</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akshat Dewan, Michal Ziemski, Henri Meylan, Lorenzo Concina, Bruno Pouliquen</li>
<li>for: 这篇论文是为了描述一种完全自动化会议记录和多种语言机器翻译的综合解决方案。</li>
<li>methods: 该工具使用了WIPO内部开发的语音转文本（S2T）和机器翻译（MT）组件，并进行了数据收集和精度调整，实现了高度定制和可靠的系统。</li>
<li>results: 这篇论文描述了技术组件的架构和进化，以及用户 сторо面的业务影响和利益。<details>
<summary>Abstract</summary>
This paper presents an end-to-end solution for the creation of fully automated conference meeting transcripts and their machine translations into various languages. This tool has been developed at the World Intellectual Property Organization (WIPO) using in-house developed speech-to-text (S2T) and machine translation (MT) components. Beyond describing data collection and fine-tuning, resulting in a highly customized and robust system, this paper describes the architecture and evolution of the technical components as well as highlights the business impact and benefits from the user side. We also point out particular challenges in the evolution and adoption of the system and how the new approach created a new product and replaced existing established workflows in conference management documentation.
</details>
<details>
<summary>摘要</summary>
translation in simplified chinese:这篇论文介绍了一个端到端解决方案，用于自动生成会议记录和不同语言的机器翻译。这个工具在世界知识产权组织（WIPO）内部开发了自动 speech-to-text（S2T）和机器翻译（MT）组件。除了描述数据收集和精细调整外，这篇论文还描述了技术组件的架构和进化，以及用户 сторо面上的优点和影响。我们还指出了系统演化和采用的一些挑战，以及如何新的方法创造了一个新产品，取代了现有的会议管理文档工作流程。
</details></li>
</ul>
<hr>
<h2 id="An-Evaluation-of-ChatGPT-4’s-Qualitative-Spatial-Reasoning-Capabilities-in-RCC-8"><a href="#An-Evaluation-of-ChatGPT-4’s-Qualitative-Spatial-Reasoning-Capabilities-in-RCC-8" class="headerlink" title="An Evaluation of ChatGPT-4’s Qualitative Spatial Reasoning Capabilities in RCC-8"></a>An Evaluation of ChatGPT-4’s Qualitative Spatial Reasoning Capabilities in RCC-8</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15577">http://arxiv.org/abs/2309.15577</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anthony G Cohn</li>
<li>for:  Investigating the extent to which a Large Language Model (LLM) can perform classical qualitative spatial reasoning tasks.</li>
<li>methods: Using the mereotopological calculus, RCC-8.</li>
<li>results: The LLM is able to perform classical qualitative spatial reasoning tasks on RCC-8.<details>
<summary>Abstract</summary>
Qualitative Spatial Reasoning (QSR) is well explored area of Commonsense Reasoning and has multiple applications ranging from Geographical Information Systems to Robotics and Computer Vision. Recently many claims have been made for the capabilities of Large Language Models (LLMs). In this paper we investigate the extent to which one particular LLM can perform classical qualitative spatial reasoning tasks on the mereotopological calculus, RCC-8.
</details>
<details>
<summary>摘要</summary>
优质空间理解（QSR）是已经广泛探索的常识理解领域之一，它在地理信息系统到机器人和计算机视觉等领域有多种应用。近期，许多人对大语言模型（LLM）的能力做出了各种各样的声明。本文我们将 investigate LLM 是否可以在简单的 mereotopological calculus 上完成经典的qualitative spatial reasoning 任务。
</details></li>
</ul>
<hr>
<h2 id="Identifiability-Matters-Revealing-the-Hidden-Recoverable-Condition-in-Unbiased-Learning-to-Rank"><a href="#Identifiability-Matters-Revealing-the-Hidden-Recoverable-Condition-in-Unbiased-Learning-to-Rank" class="headerlink" title="Identifiability Matters: Revealing the Hidden Recoverable Condition in Unbiased Learning to Rank"></a>Identifiability Matters: Revealing the Hidden Recoverable Condition in Unbiased Learning to Rank</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15560">http://arxiv.org/abs/2309.15560</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mouxiang Chen, Chenghao Liu, Zemin Liu, Zhuo Li, Jianling Sun</li>
<li>for: 本研究的目的是探讨ULTR中true relevance是否可以从点击数据中恢复，这是ULTR领域的基础问题。</li>
<li>methods: 我们首先定义一个排名模型为可识别的，如果它可以将true relevance恢复到一个扭曲参数下，那么它是可识别的。然后我们探讨一个等价的可识别性条件，可以用图连接问题表示：如果图 constructed on the underlying structure of the dataset是连通的，那么true relevance可以正确地恢复。如果IG不连通，那么可能会出现坏的情况，导致排名性能下降。为解决这个问题，我们提出了两种方法，即节点干扰和节点合并，以修改数据集并恢复IG的连接性。</li>
<li>results: 我们在一个 simulate dataset和两个LTR benchmark dataset上进行了实验，结果证明了我们的提出的定理的正确性，并证明了我们的方法可以在数据偏见时mitigate the impact of data bias。<details>
<summary>Abstract</summary>
The application of Unbiased Learning to Rank (ULTR) is widespread in modern systems for training unbiased ranking models from biased click logs. The key is to explicitly model a generation process for user behavior and fit click data based on examination hypothesis. Previous research found empirically that the true latent relevance can be recovered in most cases as long as the clicks are perfectly fitted. However, we demonstrate that this is not always achievable, resulting in a significant reduction in ranking performance. In this work, we aim to answer if or when the true relevance can be recovered from click data, which is a foundation issue for ULTR field. We first define a ranking model as identifiable if it can recover the true relevance up to a scaling transformation, which is enough for pairwise ranking objective. Then we explore an equivalent condition for identifiability that can be novely expressed as a graph connectivity test problem: if and only if a graph (namely identifiability graph, or IG) constructed on the underlying structure of the dataset is connected, we can guarantee that the relevance can be correctly recovered. When the IG is not connected, there may be bad cases leading to poor ranking performance. To address this issue, we propose two methods, namely node intervention and node merging, to modify the dataset and restore connectivity of the IG. Empirical results obtained on a simulation dataset and two LTR benchmark datasets confirm the validity of our proposed theorems and show the effectiveness of our methods in mitigating data bias when the relevance model is unidentifiable.
</details>
<details>
<summary>摘要</summary>
现代系统中广泛应用无偏学习排名（ULTR）训练不偏排名模型从偏折衔的点击日志中。关键在于显式地模型用户行为生成过程并将点击数据适应测试假设。前一项的研究发现，如果点击数据完全适应，那么真正的潜在相关性可以在大多数情况下恢复。然而，我们示出这并不总是可能，导致排名性能受到显著降低。在这项工作中，我们试图回答点击数据中真正的相关性是否可以恢复的问题，这是ULTR领域的基础问题。我们首先定义排名模型为可 identificable 如果它可以恢复真正的相关性，并且可以通过对比对象的排名来证明。然后我们探索一种等价的可 identificability 条件，可以以图形连接问题的形式表达：如果数据集下的图形（即可 identificability 图，IG）是连接的，那么我们可以保证相关性可以正确恢复。如果 IG 不连接，可能会出现坏的情况，导致排名性能下降。为解决这个问题，我们提出了两种方法：节点干扰和节点合并，以修改数据集并恢复 IG 的连接性。实验结果， obtained on a simulation dataset and two LTR benchmark datasets, confirm the validity of our proposed theorems and show the effectiveness of our methods in mitigating data bias when the relevance model is unidentifiable.
</details></li>
</ul>
<hr>
<h2 id="Direct-Models-for-Simultaneous-Translation-and-Automatic-Subtitling-FBK-IWSLT2023"><a href="#Direct-Models-for-Simultaneous-Translation-and-Automatic-Subtitling-FBK-IWSLT2023" class="headerlink" title="Direct Models for Simultaneous Translation and Automatic Subtitling: FBK@IWSLT2023"></a>Direct Models for Simultaneous Translation and Automatic Subtitling: FBK@IWSLT2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15554">http://arxiv.org/abs/2309.15554</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hlt-mt/fbk-fairseq">https://github.com/hlt-mt/fbk-fairseq</a></li>
<li>paper_authors: Sara Papi, Marco Gaido, Matteo Negri</li>
<li>for: 本研究参加IWSLT 2023评分活动的同时翻译和自动字幕追踪两个追踪，使用直接架构进行两个任务。</li>
<li>methods: 我们使用了已经在线上训练的模型来获取实时推断，并将直接ST模型改进以生成符合标准的字幕和时间标签。</li>
<li>results: 我们的英德同时翻译系统在2021和2022年的任务中比顶对方系统具有更好的计算感知延迟，优化至多达3.5个BLEU。我们的自动字幕系统在英德和英西二 languages 中优化了3.7和1.7个SubER。<details>
<summary>Abstract</summary>
This paper describes the FBK's participation in the Simultaneous Translation and Automatic Subtitling tracks of the IWSLT 2023 Evaluation Campaign. Our submission focused on the use of direct architectures to perform both tasks: for the simultaneous one, we leveraged the knowledge already acquired by offline-trained models and directly applied a policy to obtain the real-time inference; for the subtitling one, we adapted the direct ST model to produce well-formed subtitles and exploited the same architecture to produce timestamps needed for the subtitle synchronization with audiovisual content. Our English-German SimulST system shows a reduced computational-aware latency compared to the one achieved by the top-ranked systems in the 2021 and 2022 rounds of the task, with gains of up to 3.5 BLEU. Our automatic subtitling system outperforms the only existing solution based on a direct system by 3.7 and 1.7 SubER in English-German and English-Spanish respectively.
</details>
<details>
<summary>摘要</summary>
(Note: Simplified Chinese is a written language that uses simpler characters and grammar than Traditional Chinese. The translation is written in Simplified Chinese, but the original text is in Traditional Chinese.)
</details></li>
</ul>
<hr>
<h2 id="Identifying-confounders-in-deep-learning-based-model-predictions-using-DeepRepViz"><a href="#Identifying-confounders-in-deep-learning-based-model-predictions-using-DeepRepViz" class="headerlink" title="Identifying confounders in deep-learning-based model predictions using DeepRepViz"></a>Identifying confounders in deep-learning-based model predictions using DeepRepViz</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15551">http://arxiv.org/abs/2309.15551</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roshan Prakash Rane, JiHoon Kim, Arjun Umesha, Didem Stark, Marc-André Schulz, Kerstin Ritter</li>
<li>for:  This paper aims to help researchers detect and mitigate the impact of confounding variables in deep learning (DL) models when analyzing neuroimaging data.</li>
<li>methods:  The paper proposes a framework called DeepRepViz, which consists of a metric to quantify the effect of potential confounders and a visualization tool to qualitatively inspect the DL model’s learning process.</li>
<li>results:  The authors demonstrate the benefits of using DeepRepViz in combination with DL models through experiments on simulated and neuroimaging datasets. For example, the framework identifies sex as a significant confounder in a DL model predicting chronic alcohol users, and age as a confounder in a DL model predicting cognitive task performance.<details>
<summary>Abstract</summary>
Deep Learning (DL) models are increasingly used to analyze neuroimaging data and uncover insights about the brain, brain pathologies, and psychological traits. However, extraneous `confounders' variables such as the age of the participants, sex, or imaging artifacts can bias model predictions, preventing the models from learning relevant brain-phenotype relationships. In this study, we provide a solution called the `DeepRepViz' framework that enables researchers to systematically detect confounders in their DL model predictions. The framework consists of (1) a metric that quantifies the effect of potential confounders and (2) a visualization tool that allows researchers to qualitatively inspect what the DL model is learning. By performing experiments on simulated and neuroimaging datasets, we demonstrate the benefits of using DeepRepViz in combination with DL models. For example, experiments on the neuroimaging datasets reveal that sex is a significant confounder in a DL model predicting chronic alcohol users (Con-score=0.35). Similarly, DeepRepViz identifies age as a confounder in a DL model predicting participants' performance on a cognitive task (Con-score=0.3). Overall, DeepRepViz enables researchers to systematically test for potential confounders and expose DL models that rely on extraneous information such as age, sex, or imaging artifacts.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）模型在分析神经成像数据方面变得越来越普遍，以探索大脑、脑病和心理特质之间的关系。然而，外部因素such as参与者年龄、性别或成像artefacts可能会影响模型预测，使模型无法学习有关大脑与人类特质之间的关系。在本研究中，我们提供了一种解决方案called“DeepRepViz”框架，它可以帮助研究人员系统地检测DL模型预测中的外部因素。框架包括（1）一个量化 potential confounders的metric和（2）一个可视化工具，允许研究人员资深 inspect DL模型是学习什么。通过对模拟数据和神经成像数据进行实验，我们证明了在与DL模型结合使用DeepRepViz时的优点。例如，对神经成像数据进行实验显示，性别对酒精滥用者的预测结果有 statistically significant的影响（Con-score=0.35）。同时，DeepRepViz也发现年龄对参与者的认知任务表现有 statistically significant的影响（Con-score=0.3）。总之，DeepRepViz可以帮助研究人员系统地测试potential confounders，并暴露DL模型是否依赖于不必要的信息 such as age、性别或成像artefacts。
</details></li>
</ul>
<hr>
<h2 id="From-LAION-5B-to-LAION-EO-Filtering-Billions-of-Images-Using-Anchor-Datasets-for-Satellite-Image-Extraction"><a href="#From-LAION-5B-to-LAION-EO-Filtering-Billions-of-Images-Using-Anchor-Datasets-for-Satellite-Image-Extraction" class="headerlink" title="From LAION-5B to LAION-EO: Filtering Billions of Images Using Anchor Datasets for Satellite Image Extraction"></a>From LAION-5B to LAION-EO: Filtering Billions of Images Using Anchor Datasets for Satellite Image Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15535">http://arxiv.org/abs/2309.15535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikolaj Czerkawski, Alistair Francis</li>
<li>for: 这种研究是为了提取卫星图像域的特定子集而设计的。</li>
<li>methods: 这种方法使用了参考 dataset，然后进行进一步的筛选，以提取卫星图像域的特定子集。</li>
<li>results: 这种方法导致了一个名为 LAION-EO 的 dataset 的发布，该 dataset 包含高分辨率像素的文本和卫星图像的对应对。 paper 还介绍了数据采集过程以及数据集的一些特性。<details>
<summary>Abstract</summary>
Large datasets, such as LAION-5B, contain a diverse distribution of images shared online. However, extraction of domain-specific subsets of large image corpora is challenging. The extraction approach based on an anchor dataset, combined with further filtering, is proposed here and demonstrated for the domain of satellite imagery. This results in the release of LAION-EO, a dataset sourced from the web containing pairs of text and satellite images in high (pixel-wise) resolution. The paper outlines the acquisition procedure as well as some of the features of the dataset.
</details>
<details>
<summary>摘要</summary>
大量的数据集，如LAION-5B，包含丰富多样化的在线图像分布。然而，抽取具有域特定特点的图像集的大数据集是一项挑战。本文提出了基于锚点集的抽取方法，并通过进一步的筛选，在卫星图像领域实现了LAION-EO数据集的生成。该数据集包含高分辨率像素的文本和卫星图像对。文章还介绍了数据集的获取过程以及一些特性。
</details></li>
</ul>
<hr>
<h2 id="Cyber-Security-Requirements-for-Platforms-Enhancing-AI-Reproducibility"><a href="#Cyber-Security-Requirements-for-Platforms-Enhancing-AI-Reproducibility" class="headerlink" title="Cyber Security Requirements for Platforms Enhancing AI Reproducibility"></a>Cyber Security Requirements for Platforms Enhancing AI Reproducibility</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15525">http://arxiv.org/abs/2309.15525</a></li>
<li>repo_url: None</li>
<li>paper_authors: Polra Victor Falade</li>
<li>for: 本研究旨在 Addressing the security challenges associated with artificial intelligence (AI) research and ensuring reproducibility in the field of AI.</li>
<li>methods: 本研究使用了 A new framework for evaluating AI platforms for reproducibility from a cyber security standpoint, which assessed five popular AI reproducibility platforms.</li>
<li>results: 分析发现， none of these platforms fully incorporates the necessary cyber security measures essential for robust reproducibility. Kaggle and Codalab performed better in terms of implementing cyber security measures, covering aspects like security, privacy, usability, and trust.<details>
<summary>Abstract</summary>
Scientific research is increasingly reliant on computational methods, posing challenges for ensuring research reproducibility. This study focuses on the field of artificial intelligence (AI) and introduces a new framework for evaluating AI platforms for reproducibility from a cyber security standpoint to address the security challenges associated with AI research. Using this framework, five popular AI reproducibility platforms; Floydhub, BEAT, Codalab, Kaggle, and OpenML were assessed. The analysis revealed that none of these platforms fully incorporates the necessary cyber security measures essential for robust reproducibility. Kaggle and Codalab, however, performed better in terms of implementing cyber security measures covering aspects like security, privacy, usability, and trust. Consequently, the study provides tailored recommendations for different user scenarios, including individual researchers, small laboratories, and large corporations. It emphasizes the importance of integrating specific cyber security features into AI platforms to address the challenges associated with AI reproducibility, ultimately advancing reproducibility in this field. Moreover, the proposed framework can be applied beyond AI platforms, serving as a versatile tool for evaluating a wide range of systems and applications from a cyber security perspective.
</details>
<details>
<summary>摘要</summary>
（注意：以下是简化中文版本，有些词语和句子可能会有所不同）科学研究越来越依赖计算方法，但这也引发了复制性问题的出现。这项研究将人工智能（AI）作为研究对象，提出了一个新的评估AI平台复制性的框架，从安全角度来解决相关的安全挑战。使用这个框架，研究者评估了5个流行的AI复制性平台，即Floydhub、BEAT、Codalab、Kaggle和OpenML。分析发现，这些平台中没有任一个完全涵盖了必要的安全措施，以确保强大的复制性。Kaggle和Codalab在实施安全措施方面表现了更好，覆盖了安全、隐私、可用性和信任方面的多个方面。因此，研究提供了对不同用户场景的打算建议，包括个人研究者、小团队和大公司。强调在AI平台中集成特定的安全功能，以解决相关的挑战，最终推动AI复制性领域的进步。此外，提出的框架可以超出AI平台，用于评估各种系统和应用程序的安全性。
</details></li>
</ul>
<hr>
<h2 id="Robust-Internal-Representations-for-Domain-Generalization"><a href="#Robust-Internal-Representations-for-Domain-Generalization" class="headerlink" title="Robust Internal Representations for Domain Generalization"></a>Robust Internal Representations for Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15522">http://arxiv.org/abs/2309.15522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Rostami</li>
<li>for: 本研究是一篇概述我在转移学习中的研究成果，尤其是在缺乏标签数据和连续学习中遇到的挑战。</li>
<li>methods: 本研究使用 embedding space 进行转移学习，包括几何学习、零shot学习、连续学习、领域适应和分布式学习。</li>
<li>results: 本研究提供了一个抽象的转移学习概述，为未来的研究人员提供了一个前瞻性的视角，帮助他们更好地理解和发展转移学习领域。<details>
<summary>Abstract</summary>
This paper which is part of the New Faculty Highlights Invited Speaker Program of AAAI'23, serves as a comprehensive survey of my research in transfer learning by utilizing embedding spaces. The work reviewed in this paper specifically revolves around the inherent challenges associated with continual learning and limited availability of labeled data. By providing an overview of my past and ongoing contributions, this paper aims to present a holistic understanding of my research, paving the way for future explorations and advancements in the field. My research delves into the various settings of transfer learning, including, few-shot learning, zero-shot learning, continual learning, domain adaptation, and distributed learning. I hope this survey provides a forward-looking perspective for researchers who would like to focus on similar research directions.
</details>
<details>
<summary>摘要</summary>
这篇论文是AAAI'23年新教授精彩报告系列之一，它是我在使用嵌入空间进行转移学习的研究概述。这篇论文的工作具有继续学习和标注数据的有限性等内在挑战。通过对我过去和当前研究的概述，这篇论文希望能够为未来的探索和进步提供一个整体的理解，并为相关领域的研究者提供前瞻性的视角。我的研究涉及到转移学习的不同场景，包括几shot学习、零shot学习、继续学习、领域适应和分布式学习。我希望这份报告能够为研究者们提供一个前瞻性的视角，以便他们可以专注于类似的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Raiju-Reinforcement-Learning-Guided-Post-Exploitation-for-Automating-Security-Assessment-of-Network-Systems"><a href="#Raiju-Reinforcement-Learning-Guided-Post-Exploitation-for-Automating-Security-Assessment-of-Network-Systems" class="headerlink" title="Raijū: Reinforcement Learning-Guided Post-Exploitation for Automating Security Assessment of Network Systems"></a>Raijū: Reinforcement Learning-Guided Post-Exploitation for Automating Security Assessment of Network Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15518">http://arxiv.org/abs/2309.15518</a></li>
<li>repo_url: None</li>
<li>paper_authors: Van-Hau Pham, Hien Do Hoang, Phan Thanh Trung, Van Dinh Quoc, Trong-Nghia To, Phan The Duy</li>
<li>for:  This paper aims to propose a Reinforcement Learning (RL)-driven automation approach to assist penetration testers in quickly implementing the process of post-exploitation for security-level evaluation in network systems.</li>
<li>methods:  The proposed approach uses two RL algorithms, Advantage Actor-Critic (A2C) and Proximal Policy Optimization (PPO), to train specialized agents capable of making intelligent actions, which are Metasploit modules to automatically launch attacks of privileges escalation, gathering hashdump, and lateral movement.</li>
<li>results:  The agents automatically select actions and launch attacks on four real environments with over 84% successful attacks and under 55 attack steps given. The A2C algorithm has proven to be extremely effective in the selection of proper actions for automation of post-exploitation.<details>
<summary>Abstract</summary>
In order to assess the risks of a network system, it is important to investigate the behaviors of attackers after successful exploitation, which is called post-exploitation. Although there are various efficient tools supporting post-exploitation implementation, no application can automate this process. Most of the steps of this process are completed by experts who have profound knowledge of security, known as penetration testers or pen-testers. To this end, our study proposes the Raij\=u framework, a Reinforcement Learning (RL)-driven automation approach that assists pen-testers in quickly implementing the process of post-exploitation for security-level evaluation in network systems. We implement two RL algorithms, Advantage Actor-Critic (A2C) and Proximal Policy Optimization (PPO), to train specialized agents capable of making intelligent actions, which are Metasploit modules to automatically launch attacks of privileges escalation, gathering hashdump, and lateral movement. By leveraging RL, we aim to empower these agents with the ability to autonomously select and execute actions that can exploit vulnerabilities in target systems. This approach allows us to automate certain aspects of the penetration testing workflow, making it more efficient and responsive to emerging threats and vulnerabilities. The experiments are performed in four real environments with agents trained in thousands of episodes. The agents automatically select actions and launch attacks on the environments and achieve over 84\% of successful attacks with under 55 attack steps given. Moreover, the A2C algorithm has proved extremely effective in the selection of proper actions for automation of post-exploitation.
</details>
<details>
<summary>摘要</summary>
为评估网络系统的风险，需要调查攻击者在成功攻击后的行为，即后续利用。虽然有各种高效的工具支持后续实施，但没有应用程序可以自动化这个过程。大多数步骤都需要由具有安全知识的专家，即黑客测试员或黑客测试人员完成。为此，我们的研究提出了Raij\=u框架，一种基于强化学习（RL）驱动的自动化方法，可以帮助黑客测试员快速实施后续利用的安全水平评估在网络系统中。我们实现了两种RL算法，即Advantage Actor-Critic（A2C）和Proximal Policy Optimization（PPO），用以训练专门的代理人，以便它们可以具有智能行为，例如Metasploit模块，自动发起特权提升、 hashdump 和 lateral movement 攻击。通过RL，我们想使这些代理人具有攻击漏洞的能力，并且可以自动选择和执行攻击。这种方法可以自动化一些黑客测试工作流程，使其更加高效和应对新的威胁和漏洞。实验在四个真实环境中进行，代理人在千多个回合中被训练。代理人自动选择行动，在环境中发起攻击，达成84%以上的成功率，只需55步。此外，A2C算法在选择合适的行动方面表现出色。
</details></li>
</ul>
<hr>
<h2 id="Residual-Scheduling-A-New-Reinforcement-Learning-Approach-to-Solving-Job-Shop-Scheduling-Problem"><a href="#Residual-Scheduling-A-New-Reinforcement-Learning-Approach-to-Solving-Job-Shop-Scheduling-Problem" class="headerlink" title="Residual Scheduling: A New Reinforcement Learning Approach to Solving Job Shop Scheduling Problem"></a>Residual Scheduling: A New Reinforcement Learning Approach to Solving Job Shop Scheduling Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15517">http://arxiv.org/abs/2309.15517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kuo-Hao Ho, Ruei-Yu Jheng, Ji-Han Wu, Fan Chiang, Yen-Chi Chen, Yuan-Yu Wu, I-Chen Wu</li>
<li>for:  solves the job-shop scheduling problem (JSP) and the flexible job-shop scheduling problem (FJSP)</li>
<li>methods:  uses deep reinforcement learning (DRL) with graph neural networks (GNN) to construct scheduling solutions</li>
<li>results:  reaches state-of-the-art (SOTA) performance among all known construction heuristics on most well-known open JSP and FJSP benchmarks, and performs well on large-size instances despite being trained on smaller instances.<details>
<summary>Abstract</summary>
Job-shop scheduling problem (JSP) is a mathematical optimization problem widely used in industries like manufacturing, and flexible JSP (FJSP) is also a common variant. Since they are NP-hard, it is intractable to find the optimal solution for all cases within reasonable times. Thus, it becomes important to develop efficient heuristics to solve JSP/FJSP. A kind of method of solving scheduling problems is construction heuristics, which constructs scheduling solutions via heuristics. Recently, many methods for construction heuristics leverage deep reinforcement learning (DRL) with graph neural networks (GNN). In this paper, we propose a new approach, named residual scheduling, to solving JSP/FJSP. In this new approach, we remove irrelevant machines and jobs such as those finished, such that the states include the remaining (or relevant) machines and jobs only. Our experiments show that our approach reaches state-of-the-art (SOTA) among all known construction heuristics on most well-known open JSP and FJSP benchmarks. In addition, we also observe that even though our model is trained for scheduling problems of smaller sizes, our method still performs well for scheduling problems of large sizes. Interestingly in our experiments, our approach even reaches zero gap for 49 among 50 JSP instances whose job numbers are more than 150 on 20 machines.
</details>
<details>
<summary>摘要</summary>
Job-shop scheduling problem (JSP) 是一个数学优化问题，广泛应用在生产和制造业等领域。可是，由于 JSP 和 flexible JSP (FJSP) 都是 NP-hard，因此找到全面的解决方案是不可能的。因此，发展高效的规律来解决 JSP/FJSP 的问题非常重要。一种解决 scheduling 问题的方法是建构规律来，这种方法利用深度学习来建立 scheduling 的解决方案。在这篇文章中，我们提出了一个新的方法，即 residual 调度，来解决 JSP/FJSP 问题。在这个新方法中，我们删除不必要的机器和任务，例如已经完成的任务和机器，以只包含剩下的（或有效的）机器和任务。我们的实验结果显示，我们的方法在大多数知名的 open JSP 和 FJSP 测试集上具有最佳性（SOTA）。此外，我们也发现了我们的模型在较小的 scheduling 问题上训练的情况下，我们的方法仍然可以很好地解决大型 scheduling 问题。在我们的实验中，我们甚至发现了我们的方法可以将 49 个 JSP 问题中的任务数量超过 150 的机器组合成零距离。
</details></li>
</ul>
<hr>
<h2 id="Teaching-Text-to-Image-Models-to-Communicate"><a href="#Teaching-Text-to-Image-Models-to-Communicate" class="headerlink" title="Teaching Text-to-Image Models to Communicate"></a>Teaching Text-to-Image Models to Communicate</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15516">http://arxiv.org/abs/2309.15516</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sfedfcv/redesigned-pancake">https://github.com/Sfedfcv/redesigned-pancake</a></li>
<li>paper_authors: Xiaowen Sun, Jiazhan Feng, Yuxuan Wang, Yuxuan Lai, Xingyu Shen, Dongyan Zhao</li>
<li>for:  Given the dialog context, the model should generate a realistic image that is consistent with the specified conversation as response.</li>
<li>methods:  We propose an efficient approach for dialog-to-image generation without any intermediate translation, which maximizes the extraction of the semantic information contained in the dialog. We fine-tune pre-trained text-to-image models to enable them to generate images conditioning on processed dialog context.</li>
<li>results:  Our approach can consistently improve the performance of various models across multiple metrics. Experimental results on public benchmark demonstrate the effectiveness and practicability of our method.<details>
<summary>Abstract</summary>
Various works have been extensively studied in the research of text-to-image generation. Although existing models perform well in text-to-image generation, there are significant challenges when directly employing them to generate images in dialogs. In this paper, we first highlight a new problem: dialog-to-image generation, that is, given the dialog context, the model should generate a realistic image which is consistent with the specified conversation as response. To tackle the problem, we propose an efficient approach for dialog-to-image generation without any intermediate translation, which maximizes the extraction of the semantic information contained in the dialog. Considering the characteristics of dialog structure, we put segment token before each sentence in a turn of a dialog to differentiate different speakers. Then, we fine-tune pre-trained text-to-image models to enable them to generate images conditioning on processed dialog context. After fine-tuning, our approach can consistently improve the performance of various models across multiple metrics. Experimental results on public benchmark demonstrate the effectiveness and practicability of our method.
</details>
<details>
<summary>摘要</summary>
各种工作在文本到图像生成研究中得到了广泛的研究。虽然现有模型在文本到图像生成方面表现良好，但在直接将其应用于对话中生成图像时存在重要的挑战。在这篇论文中，我们首先强调了一个新的问题：对话到图像生成，即在对话上下文中，模型应该生成一个真实的图像，并且与指定的对话进行响应。为解决这个问题，我们提议一种高效的对话到图像生成方法，不需要任何中间翻译，最大化提取对话中含义的semantic信息。考虑对话结构的特点，我们在对话中每个句子前面添加了分割token，以便 diferenciar不同的说话人。然后，我们使用预训练的文本到图像模型进行微调，以使其能够根据处理后的对话上下文生成图像。经过微调，我们的方法可以在多种纪录下 consistently 提高不同模型的性能。实验结果表明我们的方法是可靠和实用的。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Cross-Category-Learning-in-Recommendation-Systems-with-Multi-Layer-Embedding-Training"><a href="#Enhancing-Cross-Category-Learning-in-Recommendation-Systems-with-Multi-Layer-Embedding-Training" class="headerlink" title="Enhancing Cross-Category Learning in Recommendation Systems with Multi-Layer Embedding Training"></a>Enhancing Cross-Category Learning in Recommendation Systems with Multi-Layer Embedding Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15881">http://arxiv.org/abs/2309.15881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Deng, Benjamin Ghaemmaghami, Ashish Kumar Singh, Benjamin Cho, Leo Orshansky, Mattan Erez, Michael Orshansky</li>
<li>for: 提高推荐系统中 rarely-occurring category 的 embedding 质量</li>
<li>methods: 使用 training-time 技巧生成高质量 embedding，并理论解释其效果的 surprising 性</li>
<li>results: MLET 可以生成更好的 embedding，并且可以降低 embedding 维度和模型大小，对多个 state-of-the-art 推荐模型进行 click-through rate 预测任务中表现出色，特别是 для rare items<details>
<summary>Abstract</summary>
Modern DNN-based recommendation systems rely on training-derived embeddings of sparse features. Input sparsity makes obtaining high-quality embeddings for rarely-occurring categories harder as their representations are updated infrequently. We demonstrate a training-time technique to produce superior embeddings via effective cross-category learning and theoretically explain its surprising effectiveness. The scheme, termed the multi-layer embeddings training (MLET), trains embeddings using factorization of the embedding layer, with an inner dimension higher than the target embedding dimension. For inference efficiency, MLET converts the trained two-layer embedding into a single-layer one thus keeping inference-time model size unchanged.   Empirical superiority of MLET is puzzling as its search space is not larger than that of the single-layer embedding. The strong dependence of MLET on the inner dimension is even more surprising. We develop a theory that explains both of these behaviors by showing that MLET creates an adaptive update mechanism modulated by the singular vectors of embeddings. When tested on multiple state-of-the-art recommendation models for click-through rate (CTR) prediction tasks, MLET consistently produces better models, especially for rare items. At constant model quality, MLET allows embedding dimension, and model size, reduction by up to 16x, and 5.8x on average, across the models.
</details>
<details>
<summary>摘要</summary>
现代 Deep Neural Network (DNN) 基于推荐系统 rely 于训练得到的含缺特征 embedding。输入缺乏性使得为罕见类目得到高质量 embedding 更加困难，因为它们的表示更新更少。我们提出一种在训练时期进行的技术，称为多层 embedding 训练（MLET），可以生成优秀的 embedding。MLET 使用 embedding 层的因子化，并在内部维度大于目标 embedding 维度。为了提高推理效率，MLET 将训练后的两层 embedding 转换成单层 embedding，以保持推理时模型大小不变。MLET 的实际优势是在它的搜索空间不大于单层 embedding 的情况下表现出色的。此外，MLET 强调内部维度的依赖性也是一种意外的现象。我们提出了一种理论，解释了 MLET 的行为，表明 MLET 创造了一种适应更新机制，该机制被模ulated 由嵌入表示中的几个特征值。当应用于多个 state-of-the-art 推荐模型中，MLET  consistently 生成更好的模型，特别是 для 罕见项。在保持模型质量不变的情况下，MLET 允许 embedding 维度和模型大小的减少，最大化到 16x 和 5.8x 的平均值。
</details></li>
</ul>
<hr>
<h2 id="High-Fidelity-Speech-Synthesis-with-Minimal-Supervision-All-Using-Diffusion-Models"><a href="#High-Fidelity-Speech-Synthesis-with-Minimal-Supervision-All-Using-Diffusion-Models" class="headerlink" title="High-Fidelity Speech Synthesis with Minimal Supervision: All Using Diffusion Models"></a>High-Fidelity Speech Synthesis with Minimal Supervision: All Using Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15512">http://arxiv.org/abs/2309.15512</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chunyu Qiang, Hao Li, Yixin Tian, Yi Zhao, Ying Zhang, Longbiao Wang, Jianwu Dang</li>
<li>for: 这个论文主要是为了提出一种基于扩散模型的微调 speech synthesis方法，以减少标注数据量并提高语音质量。</li>
<li>methods: 这种方法使用了两种类型的扩散表示（semantic和acoustic），并使用两个sequence-to-sequence任务来实现微调语音生成。它还使用了一种新的CTAP抽象方法来解决现有方法中的信息重复和维度爆炸问题。</li>
<li>results: 实验结果表明，我们的提议方法比基eline方法有更高的语音质量和多样性。我们在官方网站上提供了一些音频样本，以便用户可以直接听到结果。<details>
<summary>Abstract</summary>
Text-to-speech (TTS) methods have shown promising results in voice cloning, but they require a large number of labeled text-speech pairs. Minimally-supervised speech synthesis decouples TTS by combining two types of discrete speech representations(semantic \& acoustic) and using two sequence-to-sequence tasks to enable training with minimal supervision. However, existing methods suffer from information redundancy and dimension explosion in semantic representation, and high-frequency waveform distortion in discrete acoustic representation. Autoregressive frameworks exhibit typical instability and uncontrollability issues. And non-autoregressive frameworks suffer from prosodic averaging caused by duration prediction models. To address these issues, we propose a minimally-supervised high-fidelity speech synthesis method, where all modules are constructed based on the diffusion models. The non-autoregressive framework enhances controllability, and the duration diffusion model enables diversified prosodic expression. Contrastive Token-Acoustic Pretraining (CTAP) is used as an intermediate semantic representation to solve the problems of information redundancy and dimension explosion in existing semantic coding methods. Mel-spectrogram is used as the acoustic representation. Both semantic and acoustic representations are predicted by continuous variable regression tasks to solve the problem of high-frequency fine-grained waveform distortion. Experimental results show that our proposed method outperforms the baseline method. We provide audio samples on our website.
</details>
<details>
<summary>摘要</summary>
文本识别（TTS）方法已经在voice cloning中表现出了有前途的结果，但它们需要大量标注的文本-语音对。不过，现有的方法受到 semantic 表示中的信息重复和维度爆发的问题，以及 discrete acoustic 表示中的高频波形腐败问题。潜在的 autoregressive 框架具有典型的不稳定和不可控问题，而非潜在的框架受到duration prediction模型的平均化问题。为解决这些问题，我们提出了一种 minimally-supervised 高精度语音合成方法，其中所有模块都是基于扩散模型的。非潜在的框架提高了可控性，而 duration 扩散模型允许多样化的语音表达。我们使用 contrastive Token-Acoustic Pretraining（CTAP）作为中间的semantic表示，以解决现有的semantic coding方法中的信息重复和维度爆发问题。Mel-spectrogram 作为 acoustic 表示。两者都是通过连续变量回归任务来解决高频细腐波形腐败问题。实验结果表明，我们的提议方法比基eline方法更高效。我们在官方网站上提供了声音样本。
</details></li>
</ul>
<hr>
<h2 id="Towards-Human-Like-RL-Taming-Non-Naturalistic-Behavior-in-Deep-RL-via-Adaptive-Behavioral-Costs-in-3D-Games"><a href="#Towards-Human-Like-RL-Taming-Non-Naturalistic-Behavior-in-Deep-RL-via-Adaptive-Behavioral-Costs-in-3D-Games" class="headerlink" title="Towards Human-Like RL: Taming Non-Naturalistic Behavior in Deep RL via Adaptive Behavioral Costs in 3D Games"></a>Towards Human-Like RL: Taming Non-Naturalistic Behavior in Deep RL via Adaptive Behavioral Costs in 3D Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15484">http://arxiv.org/abs/2309.15484</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kuo-Hao Ho, Ping-Chun Hsieh, Chiu-Chou Lin, You-Ren Luo, Feng-Jian Wang, I-Chen Wu</li>
<li>For: The paper aims to train a human-like agent with competitive strength in reinforcement learning, addressing the issue of peculiar gameplay experiences caused by unconstrained agents.* Methods: The proposed approach, called Adaptive Behavioral Costs in Reinforcement Learning (ABC-RL), augments behavioral limitations as cost signals in reinforcement learning with dynamically adjusted weights, and minimizes the behavioral costs subject to a constraint of the value function.* Results: Through experiments conducted on 3D games in DMLab-30 and Unity ML-Agents Toolkit, the paper demonstrates that ABC-RL achieves the same performance level while significantly reducing instances of shaking and spinning, promoting more natural and human-like behavior during gameplay.Here’s the Chinese version of the three key points:* For: 论文目的是在强化学习中训练一个与人类行为相似的智能机器人，解决不 constraint 的agent可能出现的异常游戏体验问题。* Methods: 提议的方法是 Adaptive Behavioral Costs in Reinforcement Learning (ABC-RL)，它在强化学习中加入行为限制作为成本信号，并通过动态调整行为限制的权重来减少行为成本。* Results: 通过在 DMLab-30 和 Unity ML-Agents Toolkit 上进行的3D游戏实验，论文表明 ABC-RL 可以同时保持同等性能水平，减少摆动和旋转的实例，使游戏play 更加自然和人类化。<details>
<summary>Abstract</summary>
In this paper, we propose a new approach called Adaptive Behavioral Costs in Reinforcement Learning (ABC-RL) for training a human-like agent with competitive strength. While deep reinforcement learning agents have recently achieved superhuman performance in various video games, some of these unconstrained agents may exhibit actions, such as shaking and spinning, that are not typically observed in human behavior, resulting in peculiar gameplay experiences. To behave like humans and retain similar performance, ABC-RL augments behavioral limitations as cost signals in reinforcement learning with dynamically adjusted weights. Unlike traditional constrained policy optimization, we propose a new formulation that minimizes the behavioral costs subject to a constraint of the value function. By leveraging the augmented Lagrangian, our approach is an approximation of the Lagrangian adjustment, which handles the trade-off between the performance and the human-like behavior. Through experiments conducted on 3D games in DMLab-30 and Unity ML-Agents Toolkit, we demonstrate that ABC-RL achieves the same performance level while significantly reducing instances of shaking and spinning. These findings underscore the effectiveness of our proposed approach in promoting more natural and human-like behavior during gameplay.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的方法called Adaptive Behavioral Costs in Reinforcement Learning（ABC-RL），用于训练具有竞争力的人类样式机器人。Recently, deep reinforcement learning agents have achieved superhuman performance in various video games, but some of these unconstrained agents may exhibit actions such as shaking and spinning that are not typical of human behavior, leading to peculiar gameplay experiences. To behave like humans and retain similar performance, ABC-RL adds behavioral limitations as cost signals in reinforcement learning with dynamically adjusted weights. Unlike traditional constrained policy optimization, we propose a new formulation that minimizes the behavioral costs subject to a constraint of the value function. By leveraging the augmented Lagrangian, our approach is an approximation of the Lagrangian adjustment, which handles the trade-off between performance and human-like behavior. Through experiments conducted on 3D games in DMLab-30 and Unity ML-Agents Toolkit, we demonstrate that ABC-RL achieves the same performance level while significantly reducing instances of shaking and spinning. These findings underscore the effectiveness of our proposed approach in promoting more natural and human-like behavior during gameplay.Here's the translation in Traditional Chinese:在这篇论文中，我们提出了一种新的方法called Adaptive Behavioral Costs in Reinforcement Learning（ABC-RL），用于训练具有竞争力的人类样式机器人。Recently, deep reinforcement learning agents have achieved superhuman performance in various video games, but some of these unconstrained agents may exhibit actions such as shaking and spinning that are not typical of human behavior, leading to peculiar gameplay experiences. To behave like humans and retain similar performance, ABC-RL adds behavioral limitations as cost signals in reinforcement learning with dynamically adjusted weights. Unlike traditional constrained policy optimization, we propose a new formulation that minimizes the behavioral costs subject to a constraint of the value function. By leveraging the augmented Lagrangian, our approach is an approximation of the Lagrangian adjustment, which handles the trade-off between performance and human-like behavior. Through experiments conducted on 3D games in DMLab-30 and Unity ML-Agents Toolkit, we demonstrate that ABC-RL achieves the same performance level while significantly reducing instances of shaking and spinning. These findings underscore the effectiveness of our proposed approach in promoting more natural and human-like behavior during gameplay.
</details></li>
</ul>
<hr>
<h2 id="Enabling-Resource-efficient-AIoT-System-with-Cross-level-Optimization-A-survey"><a href="#Enabling-Resource-efficient-AIoT-System-with-Cross-level-Optimization-A-survey" class="headerlink" title="Enabling Resource-efficient AIoT System with Cross-level Optimization: A survey"></a>Enabling Resource-efficient AIoT System with Cross-level Optimization: A survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15467">http://arxiv.org/abs/2309.15467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sicong Liu, Bin Guo, Cheng Fang, Ziqi Wang, Shiyan Luo, Zimu Zhou, Zhiwen Yu<br>for:* The paper focuses on the development of resource-friendly deep learning (DL) models and model-adaptive system scheduling for artificial intelligence of things (AIoT) applications.methods:* The paper explores algorithm-system co-design to optimize resource availability and improve the performance of DL models on resource-scarce infrastructures.* The survey covers various granularity levels, including DL models, computation graphs, operators, memory schedules, and hardware instructors in both on-device and distributed paradigms.results:* The paper aims to provide a broader optimization space for more free resource-performance tradeoffs and to help readers understand the connections between problems and techniques scattered over diverse levels.Here is the answer in Simplified Chinese text:for:* 本文关注资源充足的深度学习（DL）模型和AIoT应用中的系统调度方法的开发。methods:* 本文探讨了算法系统共设计，以优化资源可用性并提高资源缺乏基础设施上DL模型的性能。* 本文覆盖了多个粒度 уровень，包括DL模型、计算图、运算符、内存调度和硬件指令在 both 设备和分布式 paradigms 中。results:* 本文希望通过提供更多的自由资源性能交易空间，以便读者更好地理解各种问题和技术之间的连接。<details>
<summary>Abstract</summary>
The emerging field of artificial intelligence of things (AIoT, AI+IoT) is driven by the widespread use of intelligent infrastructures and the impressive success of deep learning (DL). With the deployment of DL on various intelligent infrastructures featuring rich sensors and weak DL computing capabilities, a diverse range of AIoT applications has become possible. However, DL models are notoriously resource-intensive. Existing research strives to realize near-/realtime inference of AIoT live data and low-cost training using AIoT datasets on resource-scare infrastructures. Accordingly, the accuracy and responsiveness of DL models are bounded by resource availability. To this end, the algorithm-system co-design that jointly optimizes the resource-friendly DL models and model-adaptive system scheduling improves the runtime resource availability and thus pushes the performance boundary set by the standalone level. Unlike previous surveys on resource-friendly DL models or hand-crafted DL compilers/frameworks with partially fine-tuned components, this survey aims to provide a broader optimization space for more free resource-performance tradeoffs. The cross-level optimization landscape involves various granularity, including the DL model, computation graph, operator, memory schedule, and hardware instructor in both on-device and distributed paradigms. Furthermore, due to the dynamic nature of AIoT context, which includes heterogeneous hardware, agnostic sensing data, varying user-specified performance demands, and resource constraints, this survey explores the context-aware inter-/intra-device controllers for automatic cross-level adaptation. Additionally, we identify some potential directions for resource-efficient AIoT systems. By consolidating problems and techniques scattered over diverse levels, we aim to help readers understand their connections and stimulate further discussions.
</details>
<details>
<summary>摘要</summary>
人工智能物联网（AIoT，AI+IoT）领域在广泛使用智能基础设施和深度学习（DL）的成功下发展。通过在具有丰富感知器和软DL计算能力的多种智能基础设施上部署DL模型，AIoT应用程序的多样化化成为可能。然而，DL模型具有资源占用率问题。现有研究寻求实现AIoT实时数据和训练成本低的准确和响应率DL模型。为此，我们需要同时优化资源充足的DL模型和模型适应系统调度。不同于之前关于资源友好DL模型或手动精细DL编译器/框架的调研，本调研旨在为更多的自由资源性能交易提供更广泛的优化空间。AIoT上下文的跨级优化景观包括DL模型、计算图、运算、内存调度和硬件指导等，在 both on-device 和分布式模式下进行跨级优化。此外，由于AIoT上下文的动态特性，包括多样化硬件、多种感知数据、用户指定性能要求和资源限制，我们需要采用智能Device Controller来自动进行跨级调整。此外，我们还提出了一些可能的资源高效AIoT系统的方向。通过汇集多级问题和技术，我们希望读者可以更好地理解他们之间的连接，并促进进一步的讨论。
</details></li>
</ul>
<hr>
<h2 id="LogicMP-A-Neuro-symbolic-Approach-for-Encoding-First-order-Logic-Constraints"><a href="#LogicMP-A-Neuro-symbolic-Approach-for-Encoding-First-order-Logic-Constraints" class="headerlink" title="LogicMP: A Neuro-symbolic Approach for Encoding First-order Logic Constraints"></a>LogicMP: A Neuro-symbolic Approach for Encoding First-order Logic Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15458">http://arxiv.org/abs/2309.15458</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weidi Xu, Jingwei Wang, Lele Xie, Jianshan He, Hongting Zhou, Taifeng Wang, Xiaopei Wan, Jingdong Chen, Chao Qu, Wei Chu</li>
<li>for: 本文提出了一种将逻辑学约束（FOLC）与神经网络结合的新方法，以便模型复杂的关系，满足约束。</li>
<li>methods: 本文提出了一种新的神经层，逻辑MP（LogicMP），其层使用了mean-field变量推理来处理MLN（多边网络）。这种层可以与现有的神经网络结合，以便编码FOLC。</li>
<li>results:  empirical results表明，LogicMP在图像、文本和图像三种任务中的表现比先进竞争者更高，同时具有更高的效率和性能。<details>
<summary>Abstract</summary>
Integrating first-order logic constraints (FOLCs) with neural networks is a crucial but challenging problem since it involves modeling intricate correlations to satisfy the constraints. This paper proposes a novel neural layer, LogicMP, whose layers perform mean-field variational inference over an MLN. It can be plugged into any off-the-shelf neural network to encode FOLCs while retaining modularity and efficiency. By exploiting the structure and symmetries in MLNs, we theoretically demonstrate that our well-designed, efficient mean-field iterations effectively mitigate the difficulty of MLN inference, reducing the inference from sequential calculation to a series of parallel tensor operations. Empirical results in three kinds of tasks over graphs, images, and text show that LogicMP outperforms advanced competitors in both performance and efficiency.
</details>
<details>
<summary>摘要</summary>
integrating first-order logic constraints (FOLCs) with neural networks is a crucial but challenging problem, as it involves modeling intricate correlations to satisfy the constraints. this paper proposes a novel neural layer, LogicMP, whose layers perform mean-field variational inference over an MLN. it can be plugged into any off-the-shelf neural network to encode FOLCs while retaining modularity and efficiency. by exploiting the structure and symmetries in MLNs, we theoretically demonstrate that our well-designed, efficient mean-field iterations effectively mitigate the difficulty of MLN inference, reducing the inference from sequential calculation to a series of parallel tensor operations. empirical results in three kinds of tasks over graphs, images, and text show that LogicMP outperforms advanced competitors in both performance and efficiency.
</details></li>
</ul>
<hr>
<h2 id="Local-Compressed-Video-Stream-Learning-for-Generic-Event-Boundary-Detection"><a href="#Local-Compressed-Video-Stream-Learning-for-Generic-Event-Boundary-Detection" class="headerlink" title="Local Compressed Video Stream Learning for Generic Event Boundary Detection"></a>Local Compressed Video Stream Learning for Generic Event Boundary Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15431">http://arxiv.org/abs/2309.15431</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gx77/lcvsl">https://github.com/gx77/lcvsl</a></li>
<li>paper_authors: Libo Zhang, Xin Gu, Congcong Li, Tiejian Luo, Heng Fan</li>
<li>for: 本研究旨在提出一种基于压缩视频表示学习方法，用于精准地检测视频中的事件边界。</li>
<li>methods: 该方法使用轻量级的ConvNet提取RGB、运动向量、差异和GOP结构中的特征，并通过针对压缩信息的批处理和双向信息流的SCAM模块进行特征提取和束缚。</li>
<li>results: 对于Kinetics-GEBD和TAPOS数据集，该方法实现了较大的改进，与之前的端到端方法相比，同时运行速度相同。<details>
<summary>Abstract</summary>
Generic event boundary detection aims to localize the generic, taxonomy-free event boundaries that segment videos into chunks. Existing methods typically require video frames to be decoded before feeding into the network, which contains significant spatio-temporal redundancy and demands considerable computational power and storage space. To remedy these issues, we propose a novel compressed video representation learning method for event boundary detection that is fully end-to-end leveraging rich information in the compressed domain, i.e., RGB, motion vectors, residuals, and the internal group of pictures (GOP) structure, without fully decoding the video. Specifically, we use lightweight ConvNets to extract features of the P-frames in the GOPs and spatial-channel attention module (SCAM) is designed to refine the feature representations of the P-frames based on the compressed information with bidirectional information flow. To learn a suitable representation for boundary detection, we construct the local frames bag for each candidate frame and use the long short-term memory (LSTM) module to capture temporal relationships. We then compute frame differences with group similarities in the temporal domain. This module is only applied within a local window, which is critical for event boundary detection. Finally a simple classifier is used to determine the event boundaries of video sequences based on the learned feature representation. To remedy the ambiguities of annotations and speed up the training process, we use the Gaussian kernel to preprocess the ground-truth event boundaries. Extensive experiments conducted on the Kinetics-GEBD and TAPOS datasets demonstrate that the proposed method achieves considerable improvements compared to previous end-to-end approach while running at the same speed. The code is available at https://github.com/GX77/LCVSL.
</details>
<details>
<summary>摘要</summary>
通用事件边界检测目标是将视频切分成各个事件边界，以便进行事件分类和识别。现有方法通常需要将视频帧解码为图像，然后将其传输到网络中进行处理，这会带来很大的计算成本和存储空间。为了解决这些问题，我们提出了一种新的压缩视频表示学习方法，可以在压缩域内完全结束地处理视频，而不需要完全解码视频。我们使用轻量级的ConvNet来提取P帧中的特征，并使用空间通道注意机制（SCAM）来细化P帧的特征表示，基于压缩信息的双向信息流。为了学习适合的表示，我们构建了每个候选帧的本地帧袋，并使用长短时间记忆（LSTM）模块来捕捉视频序列中的时间关系。然后，我们计算帧之间的相似性，并使用 Gaussian kernel 预处理真实的事件边界标注。我们的方法在 Kinetics-GEBD 和 TAPOS 数据集上进行了广泛的实验，并达到了较好的性能，而且与之前的端到端方法相比，运行速度相对较快。代码可以在 GitHub 上找到：https://github.com/GX77/LCVSL。
</details></li>
</ul>
<hr>
<h2 id="SimPINNs-Simulation-Driven-Physics-Informed-Neural-Networks-for-Enhanced-Performance-in-Nonlinear-Inverse-Problems"><a href="#SimPINNs-Simulation-Driven-Physics-Informed-Neural-Networks-for-Enhanced-Performance-in-Nonlinear-Inverse-Problems" class="headerlink" title="SimPINNs: Simulation-Driven Physics-Informed Neural Networks for Enhanced Performance in Nonlinear Inverse Problems"></a>SimPINNs: Simulation-Driven Physics-Informed Neural Networks for Enhanced Performance in Nonlinear Inverse Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16729">http://arxiv.org/abs/2309.16729</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sidney Besnard, Frédéric Jurie, Jalal M. Fadili</li>
<li>for:  solves inverse problems by leveraging deep learning techniques, with the objective of inferring unknown parameters that govern a physical system based on observed data.</li>
<li>methods:  builds upon physics-informed neural networks (PINNs) trained with a hybrid loss function that combines observed data with simulated data generated by a known (approximate) physical model.</li>
<li>results:  surpasses the performance of standard PINNs, providing improved accuracy and robustness, as demonstrated by experimental results on an orbit restitution problem.Here’s the full text in Simplified Chinese:</li>
<li>for:  solves inverse problems by leveraging deep learning techniques, with the objective of inferring unknown parameters that govern a physical system based on observed data.</li>
<li>methods:  builds upon physics-informed neural networks (PINNs) trained with a hybrid loss function that combines observed data with simulated data generated by a known (approximate) physical model.</li>
<li>results:  surpasses the performance of standard PINNs, providing improved accuracy and robustness, as demonstrated by experimental results on an orbit restitution problem.<details>
<summary>Abstract</summary>
This paper introduces a novel approach to solve inverse problems by leveraging deep learning techniques. The objective is to infer unknown parameters that govern a physical system based on observed data. We focus on scenarios where the underlying forward model demonstrates pronounced nonlinear behaviour, and where the dimensionality of the unknown parameter space is substantially smaller than that of the observations. Our proposed method builds upon physics-informed neural networks (PINNs) trained with a hybrid loss function that combines observed data with simulated data generated by a known (approximate) physical model. Experimental results on an orbit restitution problem demonstrate that our approach surpasses the performance of standard PINNs, providing improved accuracy and robustness.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Graph-Neural-Prompting-with-Large-Language-Models"><a href="#Graph-Neural-Prompting-with-Large-Language-Models" class="headerlink" title="Graph Neural Prompting with Large Language Models"></a>Graph Neural Prompting with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15427">http://arxiv.org/abs/2309.15427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yijun Tian, Huan Song, Zichen Wang, Haozhu Wang, Ziqing Hu, Fang Wang, Nitesh V. Chawla, Panpan Xu</li>
<li>for: 增强预训练的大语言模型（LLM）在语言理解任务中的表现，提高 LLM 的基础知识捕捉和返回能力。</li>
<li>methods: 提出 Graph Neural Prompting（GNP）方法，GNP 包括标准图 neural network Encoder、异种Modal Pooling 模块、域 проекor 和自我超vision连接预测目标。</li>
<li>results: 在多个数据集上，GNP 能够在不同的 LLM 大小和设置下提高各种普通常识和生物医学理解任务的表现。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have shown remarkable generalization capability with exceptional performance in various language modeling tasks. However, they still exhibit inherent limitations in precisely capturing and returning grounded knowledge. While existing work has explored utilizing knowledge graphs to enhance language modeling via joint training and customized model architectures, applying this to LLMs is problematic owing to their large number of parameters and high computational cost. In addition, how to leverage the pre-trained LLMs and avoid training a customized model from scratch remains an open question. In this work, we propose Graph Neural Prompting (GNP), a novel plug-and-play method to assist pre-trained LLMs in learning beneficial knowledge from KGs. GNP encompasses various designs, including a standard graph neural network encoder, a cross-modality pooling module, a domain projector, and a self-supervised link prediction objective. Extensive experiments on multiple datasets demonstrate the superiority of GNP on both commonsense and biomedical reasoning tasks across different LLM sizes and settings.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-the-Vulnerability-of-Watermarking-Artificial-Intelligence-Generated-Content"><a href="#Towards-the-Vulnerability-of-Watermarking-Artificial-Intelligence-Generated-Content" class="headerlink" title="Towards the Vulnerability of Watermarking Artificial Intelligence Generated Content"></a>Towards the Vulnerability of Watermarking Artificial Intelligence Generated Content</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07726">http://arxiv.org/abs/2310.07726</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guanlin Li, Yifei Chen, Jie Zhang, Jiwei Li, Shangwei Guo, Tianwei Zhang</li>
<li>for: 本研究旨在探讨社交媒体中人工智能生成内容（AIGC）的许多商业服务，以及这些服务的使用需要高度调控，以确保用户不会违反使用政策（如商业化利用、生成和分发不安全内容）。</li>
<li>methods: 本研究使用了许多水印技术，包括潜在扩散模型和大语言模型，来生成创意内容（如真实的图像和流畅的句子） для用户。</li>
<li>results: 研究发现， adversary可以轻松破坏这些水印技术，包括两种可能的攻击方式：水印去除和水印forge。WMagi是一个综合性框架，可以实现这两种攻击方式，并且可以保持内容质量。相比之下，现有的扩散模型基于攻击，WMagi是5,050$\sim$11,000$\times$ faster。<details>
<summary>Abstract</summary>
Artificial Intelligence Generated Content (AIGC) is gaining great popularity in social media, with many commercial services available. These services leverage advanced generative models, such as latent diffusion models and large language models, to generate creative content (e.g., realistic images, fluent sentences) for users. The usage of such generated content needs to be highly regulated, as the service providers need to ensure the users do not violate the usage policies (e.g., abuse for commercialization, generating and distributing unsafe content).   Numerous watermarking approaches have been proposed recently. However, in this paper, we show that an adversary can easily break these watermarking mechanisms. Specifically, we consider two possible attacks. (1) Watermark removal: the adversary can easily erase the embedded watermark from the generated content and then use it freely without the regulation of the service provider. (2) Watermark forge: the adversary can create illegal content with forged watermarks from another user, causing the service provider to make wrong attributions. We propose WMaGi, a unified framework to achieve both attacks in a holistic way. The key idea is to leverage a pre-trained diffusion model for content processing, and a generative adversarial network for watermark removing or forging. We evaluate WMaGi on different datasets and embedding setups. The results prove that it can achieve high success rates while maintaining the quality of the generated content. Compared with existing diffusion model-based attacks, WMaGi is 5,050$\sim$11,000$\times$ faster.
</details>
<details>
<summary>摘要</summary>
人工智能生成内容（AIGC）在社交媒体上 gaining popularity，许多商业服务可以提供。这些服务利用先进的生成模型，如潜在扩散模型和大语言模型，为用户生成创ativo内容（如真实的图像和流畅的句子）。使用这些生成内容的使用需要高度调控，因为服务提供者需要确保用户不会违反使用策略（如商业化利用和发布不安全内容）。Recently, numerous watermarking approaches have been proposed, but in this paper, we show that an adversary can easily break these watermarking mechanisms. Specifically, we consider two possible attacks:1. 水印除除：敌对者可以轻松地从生成内容中除除水印，然后使用无需服务提供者的调控。2. 水印forge：敌对者可以从另一名用户的水印中生成非法内容，使服务提供者错误地归因。我们提出WMaGi，一种综合性框架，可以实现这两种攻击。WMaGi 利用预训练的扩散模型进行内容处理，并利用生成对抗网络来除水印或forge水印。我们对不同的数据集和嵌入设置进行评估，结果表明，WMaGi 可以 дости到高成功率，同时保持生成内容的质量。相比 existed 的扩散模型基于攻击，WMaGi 速度比例为5,050$\sim$11,000$\times$快。
</details></li>
</ul>
<hr>
<h2 id="The-Triad-of-Failure-Modes-and-a-Possible-Way-Out"><a href="#The-Triad-of-Failure-Modes-and-a-Possible-Way-Out" class="headerlink" title="The Triad of Failure Modes and a Possible Way Out"></a>The Triad of Failure Modes and a Possible Way Out</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15420">http://arxiv.org/abs/2309.15420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emanuele Sansone</li>
<li>for: 提高cluster-based self-supervised learning（SSL）中的表示 collapse、cluster collapse和数据Permutation的问题。</li>
<li>methods: 提出了一个新的目标函数，该目标函数包括三个关键组成部分：（i）一个生成项，用于抑制表示 collapse；（ii）一个对数据变换的项，以解决标签Permutation的问题；（iii）一个均匀项，用于抑制集合溃ubble。</li>
<li>results: 对于具体实验，我们的提出的目标函数可以有效地解决表示 collapse、cluster collapse和数据Permutation的问题，并且可以通过标准背部网络 Architecture 进行优化。<details>
<summary>Abstract</summary>
We present a novel objective function for cluster-based self-supervised learning (SSL) that is designed to circumvent the triad of failure modes, namely representation collapse, cluster collapse, and the problem of invariance to permutations of cluster assignments. This objective consists of three key components: (i) A generative term that penalizes representation collapse, (ii) a term that promotes invariance to data augmentations, thereby addressing the issue of label permutations and (ii) a uniformity term that penalizes cluster collapse. Additionally, our proposed objective possesses two notable advantages. Firstly, it can be interpreted from a Bayesian perspective as a lower bound on the data log-likelihood. Secondly, it enables the training of a standard backbone architecture without the need for asymmetric elements like stop gradients, momentum encoders, or specialized clustering layers. Due to its simplicity and theoretical foundation, our proposed objective is well-suited for optimization. Experiments on both toy and real world data demonstrate its effectiveness
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的目标函数 для基于分 clustering 的自监督学习（SSL），旨在解决三种失败模式，即表示 collapse， cluster collapse 和数据变换 permutations 的问题。这个目标函数包括三个关键组件：(i) 生成项，惩罚表示 collapse。(ii) 对数据变换具有抗变换性，解决标签 permutations 问题。(iii) 统一项，惩罚 cluster collapse。我们的提议的目标函数具有两个优点：第一，它可以从 bayesian 的视角来看做数据log-likelihood 的下界。第二，它可以使用标准的背bone 架构进行训练，不需要做特殊的杂化元素，如停止梯度、旋转encoder 或特殊的 clustering 层。由于其简单性和理论基础，我们的提议的目标函数适合优化。实验表明，它在 Toy 和实际数据上具有效果。
</details></li>
</ul>
<hr>
<h2 id="Neuro-Inspired-Hierarchical-Multimodal-Learning"><a href="#Neuro-Inspired-Hierarchical-Multimodal-Learning" class="headerlink" title="Neuro-Inspired Hierarchical Multimodal Learning"></a>Neuro-Inspired Hierarchical Multimodal Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15877">http://arxiv.org/abs/2309.15877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiongye Xiao, Gengshuo Liu, Gaurav Gupta, Defu Cao, Shixuan Li, Yaxing Li, Tianqing Fang, Mingxi Cheng, Paul Bogdan</li>
<li>for: 本研究旨在提高多modalities情况下的感知效果，启发自 neuroscience 学习。</li>
<li>methods: 我们提出了一种基于信息理论的层次感知模型（ITHP），利用信息瓶颈原理。与传统的融合模型不同，我们的模型将prime模ality作为输入，剩下的模ality作为检测器在信息路径中。</li>
<li>results: 我们的模型在多modalities学习场景下具有明显的性能优势，在MUStARD和CMU-MOSI数据集上经验证明了这一点。<details>
<summary>Abstract</summary>
Integrating and processing information from various sources or modalities are critical for obtaining a comprehensive and accurate perception of the real world. Drawing inspiration from neuroscience, we develop the Information-Theoretic Hierarchical Perception (ITHP) model, which utilizes the concept of information bottleneck. Distinct from most traditional fusion models that aim to incorporate all modalities as input, our model designates the prime modality as input, while the remaining modalities act as detectors in the information pathway. Our proposed perception model focuses on constructing an effective and compact information flow by achieving a balance between the minimization of mutual information between the latent state and the input modal state, and the maximization of mutual information between the latent states and the remaining modal states. This approach leads to compact latent state representations that retain relevant information while minimizing redundancy, thereby substantially enhancing the performance of downstream tasks. Experimental evaluations on both the MUStARD and CMU-MOSI datasets demonstrate that our model consistently distills crucial information in multimodal learning scenarios, outperforming state-of-the-art benchmarks.
</details>
<details>
<summary>摘要</summary>
将多种来源或模式的信息集成和处理作为获取全面和准确的现实世界认知的关键。 drawing inspiration from neuroscience，我们开发了信息理论层次感知（ITHP）模型，利用信息瓶颈概念。 unlike most traditional fusión模型，我们的模型将首要模式作为输入，而剩下的模式则作为信息路径中的探测器。 our proposed perception model emphasizes constructing an effective and compact information flow by achieving a balance between the minimization of mutual information between the latent state and the input modal state, and the maximization of mutual information between the latent states and the remaining modal states。 this approach leads to compact latent state representations that retain relevant information while minimizing redundancy, thereby substantially enhancing the performance of downstream tasks。 experimental evaluations on both the MUStARD and CMU-MOSI datasets demonstrate that our model consistently distills crucial information in multimodal learning scenarios, outperforming state-of-the-art benchmarks。
</details></li>
</ul>
<hr>
<h2 id="STAG-Enabling-Low-Latency-and-Low-Staleness-of-GNN-based-Services-with-Dynamic-Graphs"><a href="#STAG-Enabling-Low-Latency-and-Low-Staleness-of-GNN-based-Services-with-Dynamic-Graphs" class="headerlink" title="STAG: Enabling Low Latency and Low Staleness of GNN-based Services with Dynamic Graphs"></a>STAG: Enabling Low Latency and Low Staleness of GNN-based Services with Dynamic Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15875">http://arxiv.org/abs/2309.15875</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiawen Wang, Quan Chen, Deze Zeng, Zhuo Song, Chen Chen, Minyi Guo</li>
<li>for: 提高 Graph Neural Networks (GNNs) 服务的精度。</li>
<li>methods: 提出了一种名为 STAG 的 GNN 服务框架，它通过协同服务机制和可加性基于增量传播策略来解决邻居爆发问题和重复计算问题，从而实现低延迟和低落后性。</li>
<li>results: STAG 可以加速更新阶段的执行速度，并大幅减少落后时间，但是有一定的延迟增加。<details>
<summary>Abstract</summary>
Many emerging user-facing services adopt Graph Neural Networks (GNNs) to improve serving accuracy. When the graph used by a GNN model changes, representations (embedding) of nodes in the graph should be updated accordingly. However, the node representation update is too slow, resulting in either long response latency of user queries (the inference is performed after the update completes) or high staleness problem (the inference is performed based on stale data). Our in-depth analysis shows that the slow update is mainly due to neighbor explosion problem in graphs and duplicated computation. Based on such findings, we propose STAG, a GNN serving framework that enables low latency and low staleness of GNN-based services. It comprises a collaborative serving mechanism and an additivity-based incremental propagation strategy. With the collaborative serving mechanism, only part of node representations are updated during the update phase, and the final representations are calculated in the inference phase. It alleviates the neighbor explosion problem. The additivity-based incremental propagation strategy reuses intermediate data during the update phase, eliminating duplicated computation problem. Experimental results show that STAG accelerates the update phase by 1.3x~90.1x, and greatly reduces staleness time with a slight increase in response latency.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Survey-of-Chain-of-Thought-Reasoning-Advances-Frontiers-and-Future"><a href="#A-Survey-of-Chain-of-Thought-Reasoning-Advances-Frontiers-and-Future" class="headerlink" title="A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"></a>A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15402">http://arxiv.org/abs/2309.15402</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zchuz/cot-reasoning-survey">https://github.com/zchuz/cot-reasoning-survey</a></li>
<li>paper_authors: Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, Ting Liu</li>
<li>for: 本文提供了一份严谨的链 reasoning（Chain-of-Thought）研究领域的概述，以帮助研究人员更好地了解这个领域的最新进展。</li>
<li>methods: 本文使用了多种方法，包括链 reasoning（XoT）的建构、结构变体和增强XoT，以系统地组织当前的研究工作。</li>
<li>results: 本文总结了链 reasoning的前沿应用，包括规划、工具使用和简化等领域的研究进展，并提出了一些未来研究的挑战和方向。<details>
<summary>Abstract</summary>
Chain-of-thought reasoning, a cognitive process fundamental to human intelligence, has garnered significant attention in the realm of artificial intelligence and natural language processing. However, there still remains a lack of a comprehensive survey for this arena. To this end, we take the first step and present a thorough survey of this research field carefully and widely. We use X-of-Thought to refer to Chain-of-Thought in a broad sense. In detail, we systematically organize the current research according to the taxonomies of methods, including XoT construction, XoT structure variants, and enhanced XoT. Additionally, we describe XoT with frontier applications, covering planning, tool use, and distillation. Furthermore, we address challenges and discuss some future directions, including faithfulness, multi-modal, and theory. We hope this survey serves as a valuable resource for researchers seeking to innovate within the domain of chain-of-thought reasoning.
</details>
<details>
<summary>摘要</summary>
Chain-of-thought reasoning, a fundamental cognitive process of human intelligence, has recently garnered significant attention in the field of artificial intelligence and natural language processing. However, there is still a lack of a comprehensive survey in this area. To address this, we present a thorough survey of this research field, carefully and widely. We use X-of-Thought (XoT) to refer to Chain-of-Thought in a broad sense.In detail, we systematically organize the current research according to the taxonomies of methods, including XoT construction, XoT structure variants, and enhanced XoT. Additionally, we describe XoT with frontier applications, covering planning, tool use, and distillation. Furthermore, we address challenges and discuss some future directions, including faithfulness, multi-modal, and theory. We hope this survey serves as a valuable resource for researchers seeking to innovate within the domain of chain-of-thought reasoning.Here's the translation in Traditional Chinese as well:Chain-of-thought reasoning, a fundamental cognitive process of human intelligence, has recently garnered significant attention in the field of artificial intelligence and natural language processing. However, there is still a lack of a comprehensive survey in this area. To address this, we present a thorough survey of this research field, carefully and widely. We use X-of-Thought (XoT) to refer to Chain-of-Thought in a broad sense.In detail, we systematically organize the current research according to the taxonomies of methods, including XoT construction, XoT structure variants, and enhanced XoT. Additionally, we describe XoT with frontier applications, covering planning, tool use, and distillation. Furthermore, we address challenges and discuss some future directions, including faithfulness, multi-modal, and theory. We hope this survey serves as a valuable resource for researchers seeking to innovate within the domain of chain-of-thought reasoning.
</details></li>
</ul>
<hr>
<h2 id="Neural-Stochastic-Differential-Equations-for-Robust-and-Explainable-Analysis-of-Electromagnetic-Unintended-Radiated-Emissions"><a href="#Neural-Stochastic-Differential-Equations-for-Robust-and-Explainable-Analysis-of-Electromagnetic-Unintended-Radiated-Emissions" class="headerlink" title="Neural Stochastic Differential Equations for Robust and Explainable Analysis of Electromagnetic Unintended Radiated Emissions"></a>Neural Stochastic Differential Equations for Robust and Explainable Analysis of Electromagnetic Unintended Radiated Emissions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15386">http://arxiv.org/abs/2309.15386</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sumit Kumar Jha, Susmit Jha, Rickard Ewetz, Alvaro Velasquez</li>
<li>for: 这个论文主要用于评估某些模型在隐性辐射检测 task 中的稳定性和解释性，以及提出一种基于神经泛化差分方程（SDE）的新方法来解决这些问题。</li>
<li>methods: 这个论文使用了 ResNet-like 模型进行隐性辐射检测 task，并对这些模型进行了广泛的评估。研究发现，ResNet-like 模型在 Gaussian 噪声扰动下 exhibits 的性能会很快下降，其 F1 分数从 0.93 下降至 0.008。此外，研究还发现 ResNet-like 模型对输入数据的解释不准确，缺乏时间不变或周期性的特征。</li>
<li>results: 该论文提出了一种基于 SDE 的新方法，可以提高模型的稳定性和解释性。这种方法在面对 Gaussian 噪声扰动时仍然可以保持高的 F1 分数（0.93），而且可以更好地捕捉输入数据中的时间不变或周期性特征。这种新方法可以用于实际的 URE 应用程序中，提供更加稳定和可解释的机器学习预测。<details>
<summary>Abstract</summary>
We present a comprehensive evaluation of the robustness and explainability of ResNet-like models in the context of Unintended Radiated Emission (URE) classification and suggest a new approach leveraging Neural Stochastic Differential Equations (SDEs) to address identified limitations. We provide an empirical demonstration of the fragility of ResNet-like models to Gaussian noise perturbations, where the model performance deteriorates sharply and its F1-score drops to near insignificance at 0.008 with a Gaussian noise of only 0.5 standard deviation. We also highlight a concerning discrepancy where the explanations provided by ResNet-like models do not reflect the inherent periodicity in the input data, a crucial attribute in URE detection from stable devices. In response to these findings, we propose a novel application of Neural SDEs to build models for URE classification that are not only robust to noise but also provide more meaningful and intuitive explanations. Neural SDE models maintain a high F1-score of 0.93 even when exposed to Gaussian noise with a standard deviation of 0.5, demonstrating superior resilience to ResNet models. Neural SDE models successfully recover the time-invariant or periodic horizontal bands from the input data, a feature that was conspicuously missing in the explanations generated by ResNet-like models. This advancement presents a small but significant step in the development of robust and interpretable models for real-world URE applications where data is inherently noisy and assurance arguments demand interpretable machine learning predictions.
</details>
<details>
<summary>摘要</summary>
我们提出了对具有抗耗能和解释性的ResNet-like模型的全面评估，并建议使用神经泛化方程（SDE）来解决已知的限制。我们通过实验表明，ResNet-like模型对 Gaussian 噪声的抗性不佳，其性能很快下降，F1 分数降至 near insignificance 的 0.008，只需0.5 标准差的 Gaussian 噪声。我们还指出，ResNet-like模型提供的解释不符合输入数据中的自然周期性，这是URE检测中稳定设备的关键特征。为了解决这些问题，我们提议使用神经泛化方程（SDE）建立URE分类模型，这些模型不仅抗耗能强，还能提供更直观和易理解的解释。神经泛化方程模型在对 Gaussian 噪声的抗性方面表现出色，即使 exposed to 0.5 标准差的 Gaussian 噪声，其 F1 分数仍然保持在 0.93 级别。此外，神经泛化方程模型成功地从输入数据中提取了时间不变或周期性的水平带，这是 ResNet-like 模型的解释中缺失的特征。这种进步 represent a small but significant step towards the development of robust and interpretable URE models for real-world applications where data is inherently noisy and assurance arguments demand interpretable machine learning predictions.
</details></li>
</ul>
<hr>
<h2 id="Seeing-Beyond-the-Patch-Scale-Adaptive-Semantic-Segmentation-of-High-resolution-Remote-Sensing-Imagery-based-on-Reinforcement-Learning"><a href="#Seeing-Beyond-the-Patch-Scale-Adaptive-Semantic-Segmentation-of-High-resolution-Remote-Sensing-Imagery-based-on-Reinforcement-Learning" class="headerlink" title="Seeing Beyond the Patch: Scale-Adaptive Semantic Segmentation of High-resolution Remote Sensing Imagery based on Reinforcement Learning"></a>Seeing Beyond the Patch: Scale-Adaptive Semantic Segmentation of High-resolution Remote Sensing Imagery based on Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15372">http://arxiv.org/abs/2309.15372</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinhe Liu, Sunan Shi, Junjue Wang, Yanfei Zhong</li>
<li>For: 这篇论文的目的是提出一个动态缩尺框架，以便在遥测影像分析中超过滑动窗口的资讯捕捉。* Methods: 这篇论文使用了一个名为GeoAgent的动态缩尺框架，它可以自动捕捉遥测影像中不同类型的地理物件的对应缩尺资讯。GeoAgent使用了一个全球图示和一个位置几何来表示每个遥测影像 patch 的状态，并通过一个构成单元来强制视觉关系。* Results: 实验结果显示，GeoAgent 比前一代分 segmentation 方法更好地适应大规模地图对象的分类任务，特别是在大规模地图分类任务中。<details>
<summary>Abstract</summary>
In remote sensing imagery analysis, patch-based methods have limitations in capturing information beyond the sliding window. This shortcoming poses a significant challenge in processing complex and variable geo-objects, which results in semantic inconsistency in segmentation results. To address this challenge, we propose a dynamic scale perception framework, named GeoAgent, which adaptively captures appropriate scale context information outside the image patch based on the different geo-objects. In GeoAgent, each image patch's states are represented by a global thumbnail and a location mask. The global thumbnail provides context beyond the patch, and the location mask guides the perceived spatial relationships. The scale-selection actions are performed through a Scale Control Agent (SCA). A feature indexing module is proposed to enhance the ability of the agent to distinguish the current image patch's location. The action switches the patch scale and context branch of a dual-branch segmentation network that extracts and fuses the features of multi-scale patches. The GeoAgent adjusts the network parameters to perform the appropriate scale-selection action based on the reward received for the selected scale. The experimental results, using two publicly available datasets and our newly constructed dataset WUSU, demonstrate that GeoAgent outperforms previous segmentation methods, particularly for large-scale mapping applications.
</details>
<details>
<summary>摘要</summary>
在Remote感影像分析中， patch-based 方法具有限制在滑块窗口之外的信息捕获的缺点，这种缺点对处理复杂多变的地球对象产生了semantic 不一致的segmentation结果。为解决这个挑战，我们提出了一种动态缩放见解框架，名为GeoAgent，该框架可以适应不同的地球对象，并在不同的缩放尺度下进行semantic segmentation。在GeoAgent中，每个图像块的状态被表示为全局缩略图和位置 máscara。全局缩略图提供了图像块之外的上下文信息，而位置 máscara 则引导了感知的空间关系。缩放控制器（SCA）来实现缩放选择操作，并通过一个Feature Indexing Module来增强代理的能力以辨别当前图像块的位置。当选择缩放scale时， dual-branch segmentation网络的patch scale和context branch会发生变化，以提取和融合多个缩放级别的特征。GeoAgent根据获得的奖励进行参数调整，以实现适当的缩放选择操作。我们通过使用两个公共可用的数据集和我们自己制作的WUSU数据集进行实验，得到的结果表明，GeoAgent在大规模地图应用中表现出色，特别是在semantic segmentation领域。
</details></li>
</ul>
<hr>
<h2 id="ACWA-An-AI-driven-Cyber-Physical-Testbed-for-Intelligent-Water-Systems"><a href="#ACWA-An-AI-driven-Cyber-Physical-Testbed-for-Intelligent-Water-Systems" class="headerlink" title="ACWA: An AI-driven Cyber-Physical Testbed for Intelligent Water Systems"></a>ACWA: An AI-driven Cyber-Physical Testbed for Intelligent Water Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17654">http://arxiv.org/abs/2310.17654</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai-vtrc/acwa-data">https://github.com/ai-vtrc/acwa-data</a></li>
<li>paper_authors: Feras A. Batarseh, Ajay Kulkarni, Chhayly Sreng, Justice Lin, Siam Maksud<br>for:* 这篇论文旨在提出一个新的水测试床，即人工智能和网络安全测试床（ACWA），以解决水供应管理领域的挑战。methods:* ACWA使用了最新的人工智能和数据驱动技术，包括多种拓扑、传感器、计算节点、泵、水箱、智能水设备以及数据库和人工智能模型来控制系统。results:* ACWA的实验结果表明，这种新的水测试床可以帮助解决水和农业领域的挑战，包括网络安全、资源管理、获取水资源、可持续发展和数据驱动决策等问题。<details>
<summary>Abstract</summary>
This manuscript presents a novel state-of-the-art cyber-physical water testbed, namely: The AI and Cyber for Water and Agriculture testbed (ACWA). ACWA is motivated by the need to advance water supply management using AI and Cybersecurity experimentation. The main goal of ACWA is to address pressing challenges in the water and agricultural domains by utilising cutting-edge AI and data-driven technologies. These challenges include Cyberbiosecurity, resources management, access to water, sustainability, and data-driven decision-making, among others. To address such issues, ACWA consists of multiple topologies, sensors, computational nodes, pumps, tanks, smart water devices, as well as databases and AI models that control the system. Moreover, we present ACWA simulator, which is a software-based water digital twin. The simulator runs on fluid and constituent transport principles that produce theoretical time series of a water distribution system. This creates a good validation point for comparing the theoretical approach with real-life results via the physical ACWA testbed. ACWA data are available to AI and water domain researchers and are hosted in an online public repository. In this paper, the system is introduced in detail and compared with existing water testbeds; additionally, example use-cases are described along with novel outcomes such as datasets, software, and AI-related scenarios.
</details>
<details>
<summary>摘要</summary>
ACWA consists of multiple topologies, sensors, computational nodes, pumps, tanks, smart water devices, and databases, as well as AI models that control the system. Additionally, the ACWA simulator, a software-based water digital twin, runs on fluid and constituent transport principles to produce theoretical time series of a water distribution system, providing a good validation point for comparing the theoretical approach with real-life results via the physical ACWA testbed.The system is introduced in detail and compared with existing water testbeds, along with example use-cases and novel outcomes such as datasets, software, and AI-related scenarios. The ACWA data are available to AI and water domain researchers and are hosted in an online public repository.
</details></li>
</ul>
<hr>
<h2 id="C3Net-interatomic-potential-neural-network-for-prediction-of-physicochemical-properties-in-heterogenous-systems"><a href="#C3Net-interatomic-potential-neural-network-for-prediction-of-physicochemical-properties-in-heterogenous-systems" class="headerlink" title="C3Net: interatomic potential neural network for prediction of physicochemical properties in heterogenous systems"></a>C3Net: interatomic potential neural network for prediction of physicochemical properties in heterogenous systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15334">http://arxiv.org/abs/2309.15334</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sehanlee/c3net">https://github.com/sehanlee/c3net</a></li>
<li>paper_authors: Sehan Lee, Jaechang Lim, Woo Youn Kim</li>
<li>for: 这个论文的目的是提出一种深度神经网络模型，用于 atom type embeddings 的分子上的物理化学性质预测。</li>
<li>methods: 该模型采用了深度神经网络，并遵循物理法律来预测分子中各个原子的物理化学性质。</li>
<li>results: 该模型能够高效地预测分子的物理化学性质，并且在不同的溶剂和环境中的预测结果具有高度的一致性和可预测性。<details>
<summary>Abstract</summary>
Understanding the interactions of a solute with its environment is of fundamental importance in chemistry and biology. In this work, we propose a deep neural network architecture for atom type embeddings in its molecular context and interatomic potential that follows fundamental physical laws. The architecture is applied to predict physicochemical properties in heterogeneous systems including solvation in diverse solvents, 1-octanol-water partitioning, and PAMPA with a single set of network weights. We show that our architecture is generalized well to the physicochemical properties and outperforms state-of-the-art approaches based on quantum mechanics and neural networks in the task of solvation free energy prediction. The interatomic potentials at each atom in a solute obtained from the model allow quantitative analysis of the physicochemical properties at atomic resolution consistent with chemical and physical reasoning. The software is available at https://github.com/SehanLee/C3Net.
</details>
<details>
<summary>摘要</summary>
理解分子中物质与环境之间的互动是化学和生物中的基本问题。在这项工作中，我们提出了一种深度神经网络架构，用于在分子上的原子类型嵌入和分子间势，该架构遵循物理法律。我们将该架构应用于预测多种不同溶剂中的溶解能，1- octanol-水分配、PAMPA等物理化学性质。我们的结果表明，我们的架构可以准确预测物理化学性质，并且在相比Quantum mechanics和神经网络方法时表现出色。通过这种方法，我们可以在分子级别获得物理化学性质的量化分析，与化学和物理原理一致。软件可以在https://github.com/SehanLee/C3Net上获得。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/27/cs.AI_2023_09_27/" data-id="clp89do8u004ni78828an5vec" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_09_27" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/27/cs.CL_2023_09_27/" class="article-date">
  <time datetime="2023-09-27T11:00:00.000Z" itemprop="datePublished">2023-09-27</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/27/cs.CL_2023_09_27/">cs.CL - 2023-09-27</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="AnyMAL-An-Efficient-and-Scalable-Any-Modality-Augmented-Language-Model"><a href="#AnyMAL-An-Efficient-and-Scalable-Any-Modality-Augmented-Language-Model" class="headerlink" title="AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model"></a>AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16058">http://arxiv.org/abs/2309.16058</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kyegomez/AnyMAL">https://github.com/kyegomez/AnyMAL</a></li>
<li>paper_authors: Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, Tushar Nagarajan, Matt Smith, Shashank Jain, Chun-Fu Yeh, Prakash Murugesan, Peyman Heidari, Yue Liu, Kavya Srinet, Babak Damavandi, Anuj Kumar</li>
<li>for: 本研究旨在开发一种能够理解多种输入模式信号（文本、图像、视频、声音、IMU运动传感器）的语言模型，并生成文本响应。</li>
<li>methods: 本研究使用了现有状态之最高水平LLaMA-2（70B）的文本基于理解能力，并通过预训练对照器模块将多种模式特定信号转换到共同文本空间。</li>
<li>results: 我们通过了人工和自动评估，并在多种多Modal任务上达到了状态之最高水平表现。<details>
<summary>Abstract</summary>
We present Any-Modality Augmented Language Model (AnyMAL), a unified model that reasons over diverse input modality signals (i.e. text, image, video, audio, IMU motion sensor), and generates textual responses. AnyMAL inherits the powerful text-based reasoning abilities of the state-of-the-art LLMs including LLaMA-2 (70B), and converts modality-specific signals to the joint textual space through a pre-trained aligner module. To further strengthen the multimodal LLM's capabilities, we fine-tune the model with a multimodal instruction set manually collected to cover diverse topics and tasks beyond simple QAs. We conduct comprehensive empirical analysis comprising both human and automatic evaluations, and demonstrate state-of-the-art performance on various multimodal tasks.
</details>
<details>
<summary>摘要</summary>
我们介绍Any-Modality Augmented Language Model（AnyMAL），这是一个综合模型，可以处理多种输入模式信号（即文本、图像、视频、音频、IMU运动传感器），并生成文本响应。AnyMAL继承了现状最佳的文本基于推理能力，包括LLaMA-2（70B），并使用预训练的对齐模块将多modal特征信号转化到共同文本空间。为了进一步增强多modal LLM的能力，我们人工收集了多 modal 指令集，以覆盖多种话题和任务，超出简单的QA。我们进行了全面的实验分析，包括人类和自动评估，并在多种多modal任务中达到了状态之册表现。
</details></li>
</ul>
<hr>
<h2 id="Effective-Long-Context-Scaling-of-Foundation-Models"><a href="#Effective-Long-Context-Scaling-of-Foundation-Models" class="headerlink" title="Effective Long-Context Scaling of Foundation Models"></a>Effective Long-Context Scaling of Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16039">http://arxiv.org/abs/2309.16039</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, Hao Ma</li>
<li>for: 这个论文的目的是提出一系列的长文本LLMs，以支持有效的上下文窗口至多32,768个符号。</li>
<li>methods: 这些模型使用了 continual pretraining 方法，从 Llama 2 开始，使用更长的训练序列和更大的数据集来培养模型。</li>
<li>results: 在语言模型评估、 sintetic context probing 任务以及一系列研究 benchmark 上，这些模型 achieves consistent improvement 和 significant improvement 在 long-context 任务上，并且可以使用 cost-effective 的 instruction tuning 程序来超越 gpt-3.5-turbo-16k 的总性性能。<details>
<summary>Abstract</summary>
We present a series of long-context LLMs that support effective context windows of up to 32,768 tokens. Our model series are built through continual pretraining from Llama 2 with longer training sequences and on a dataset where long texts are upsampled. We perform extensive evaluation on language modeling, synthetic context probing tasks, and a wide range of research benchmarks. On research benchmarks, our models achieve consistent improvements on most regular tasks and significant improvements on long-context tasks over Llama 2. Notably, with a cost-effective instruction tuning procedure that does not require human-annotated long instruction data, the 70B variant can already surpass gpt-3.5-turbo-16k's overall performance on a suite of long-context tasks. Alongside these results, we provide an in-depth analysis on the individual components of our method. We delve into Llama's position encodings and discuss its limitation in modeling long dependencies. We also examine the impact of various design choices in the pretraining process, including the data mix and the training curriculum of sequence lengths -- our ablation experiments suggest that having abundant long texts in the pretrain dataset is not the key to achieving strong performance, and we empirically verify that long context continual pretraining is more efficient and similarly effective compared to pretraining from scratch with long sequences.
</details>
<details>
<summary>摘要</summary>
我们提出了一系列长上下文 LLMS，支持最长32,768个字的有效上下文窗口。我们的模型系列通过长期预训练从Llama 2开始，使用更长的训练序列和增amplesize的数据集来构建。我们进行了广泛的语言模型评估、 sintetic上下文探测任务和多种研究 benchmark 评估。在研究 benchmark 上，我们的模型在大多数常见任务上实现了一致性提高，而在长上下文任务上具有显著提高。特别是，通过不需要人工标注长 instrucion 数据的Cost-effective instrucion tuning 过程，70B 变体可以在一组长上下文任务上超越 gpt-3.5-turbo-16k 的总性表现。此外，我们还提供了深入分析方法的各个组件。我们分析了 llama 的位置编码和其在模型长依赖关系的限制。我们还检查了预训练过程中的不同设计选择，包括数据混合和序列长度的训练课程，我们的拓展实验表明，在预训练 dataset 中充足的长文本不是逻辑上的关键，我们也经验 verify 了在预训练 dataset 中预训练是更加高效和相同有效的。
</details></li>
</ul>
<hr>
<h2 id="Targeted-Image-Data-Augmentation-Increases-Basic-Skills-Captioning-Robustness"><a href="#Targeted-Image-Data-Augmentation-Increases-Basic-Skills-Captioning-Robustness" class="headerlink" title="Targeted Image Data Augmentation Increases Basic Skills Captioning Robustness"></a>Targeted Image Data Augmentation Increases Basic Skills Captioning Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15991">http://arxiv.org/abs/2309.15991</a></li>
<li>repo_url: None</li>
<li>paper_authors: Valentin Barriere, Felipe del Rio, Andres Carvallo De Ferari, Carlos Aspillaga, Eugenio Herrera-Berg, Cristian Buc Calderon</li>
<li>for: 提高模型对图像描述能力的人类化能力（如性别识别）</li>
<li>methods: 使用Targeted Image-editing Data Augmentation（TIDA）方法，通过对图像caption进行修改，使模型更好地理解图像的相关结构</li>
<li>results: 在Flickr30K benchmark上，TIDA对性别、颜色和数量能力进行了改进，并且在不同的图像描述指标中显示了更好的性能，并进行了细化分析以及对不同的文本生成模型的比较<details>
<summary>Abstract</summary>
Artificial neural networks typically struggle in generalizing to out-of-context examples. One reason for this limitation is caused by having datasets that incorporate only partial information regarding the potential correlational structure of the world. In this work, we propose TIDA (Targeted Image-editing Data Augmentation), a targeted data augmentation method focused on improving models' human-like abilities (e.g., gender recognition) by filling the correlational structure gap using a text-to-image generative model. More specifically, TIDA identifies specific skills in captions describing images (e.g., the presence of a specific gender in the image), changes the caption (e.g., "woman" to "man"), and then uses a text-to-image model to edit the image in order to match the novel caption (e.g., uniquely changing a woman to a man while maintaining the context identical). Based on the Flickr30K benchmark, we show that, compared with the original data set, a TIDA-enhanced dataset related to gender, color, and counting abilities induces better performance in several image captioning metrics. Furthermore, on top of relying on the classical BLEU metric, we conduct a fine-grained analysis of the improvements of our models against the baseline in different ways. We compared text-to-image generative models and found different behaviors of the image captioning models in terms of encoding visual encoding and textual decoding.
</details>
<details>
<summary>摘要</summary>
人工神经网络通常在对不同的示例进行泛化时遇到困难。其中一个原因是因为 dataset 中只包含部分世界相关的词语关系结构。在这项工作中，我们提出了 TIDA（targeted image-editing data augmentation），一种专门针对提高模型的人类能力（例如性别识别）的数据增强方法。更 Specifically，TIDA 会在描述图像的caption中特定的技能（例如图像中的性别存在），将caption修改（例如“女性”改为“男性”），然后使用文本到图像生成模型来编辑图像，以使图像与修改后的caption保持相同的上下文。基于 Flickr30K benchmark，我们表明，相比原始数据集，TIDA 增强的 gender、color 和 counting 能力相关的数据集可以induce better performance 在多个图像描述指标上。此外，我们不仅仅使用 classical BLEU  metric，还进行了细化的分析，对比不同的文本到图像生成模型和图像描述模型的不同行为。
</details></li>
</ul>
<hr>
<h2 id="Cross-Modal-Multi-Tasking-for-Speech-to-Text-Translation-via-Hard-Parameter-Sharing"><a href="#Cross-Modal-Multi-Tasking-for-Speech-to-Text-Translation-via-Hard-Parameter-Sharing" class="headerlink" title="Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing"></a>Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15826">http://arxiv.org/abs/2309.15826</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brian Yan, Xuankai Chang, Antonios Anastasopoulos, Yuya Fujita, Shinji Watanabe</li>
<li>for: 这项研究的目的是提出一种具有硬件共享参数的ST&#x2F;MT多任务框架，以提高speech-to-text翻译的效果。</li>
<li>methods: 该方法使用一个预处理阶段，将speech和text输入转换为两个不同的字符序列，以便模型可以使用共同词库处理两种模式。</li>
<li>results: 实验结果表明，该方法可以提高attentional encoder-decoder、CTC、批处理器和联合CTC&#x2F;注意力模型的性能，无需外部MT数据。此外，该方法还可以在外部MT数据支持下提高性能，并且可以在基于文本模型的预训练模型上进行转移学习，以提高性能。<details>
<summary>Abstract</summary>
Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length -- this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yielding +0.8 BLEU, and also improves transfer learning from pre-trained textual models, yielding +1.8 BLEU.
</details>
<details>
<summary>摘要</summary>
最近的END-to-END语音至文本翻译（ST）研究提出了多任务方法with soft parameter sharing，利用机器翻译（MT）数据via secondary encoders，将文本输入映射到最终的跨Modal Representation。在这个工作中，我们提议一种ST/MT多任务框架with hard parameter sharing，所有模型参数共享跨Modal。我们通过预处理阶段将speech和text输入转换成两个精确的token序列，使模型可以不区分modalities，使用共同词库进行处理。我们通过在MuST-C上进行实验，表明我们的多任务框架可以提高attentional encoder-decoder、Connectionist Temporal Classification（CTC）、推理器和 joint CTC/attention模型的性能，无需外部机器翻译数据，均为+0.5 BLEU。此外，我们还表明该框架可以 incorporate external MT数据，提高性能+0.8 BLEU，并且可以从预训练的文本模型进行传输学习，提高性能+1.8 BLEU。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Speech-Recognition-Translation-and-Understanding-with-Discrete-Speech-Units-A-Comparative-Study"><a href="#Exploring-Speech-Recognition-Translation-and-Understanding-with-Discrete-Speech-Units-A-Comparative-Study" class="headerlink" title="Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study"></a>Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15800">http://arxiv.org/abs/2309.15800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuankai Chang, Brian Yan, Kwanghee Choi, Jeeweon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma, Jiatong Shi, Jinchuan Tian, Shinji Watanabe, Yuya Fujita, Takashi Maekaku, Pengcheng Guo, Yao-Fei Cheng, Pavel Denisov, Kohei Saijo, Hsiu-Hsuan Wang</li>
<li>for: 这篇论文旨在探讨在终端至终的语音处理模型中使用独立的语音单位。</li>
<li>methods: 这篇论文使用了各种方法，例如去重和子字模型，以将语音序列长度进行压缩。</li>
<li>results: 实验结果显示，使用独立的语音单位在大多数情况下可以取得良好的结果，并且可以大幅缩短训练时间。<details>
<summary>Abstract</summary>
Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts.
</details>
<details>
<summary>摘要</summary>
语音信号通常采样于每秒万分之几个频率，具有重复性和不必要的繁殖性，使sequencing模型难以有效地处理。高维度语音特征如spectrogram可以作为后续模型的输入，但它们可能仍然具有重复性。最近的研究提议使用基于自动学习的独立speech单元，可以压缩语音数据的大小。通过方法如减少和子字模型，可以进一步压缩语音序列的长度，从而减少训练时间，保持了可观的性能。在这种研究中，我们进行了总体和系统的探索，探讨了在结束到端speech处理模型中应用独立单元的可能性。实验在12个自动语音识别、3个语音翻译和1个口语理解 corpora中，表明独立单元在大多数设置中都可以达到理想的结果。我们计划将我们的配置和训练模型发布，以促进未来的研究努力。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Model-Routing-with-Benchmark-Datasets"><a href="#Large-Language-Model-Routing-with-Benchmark-Datasets" class="headerlink" title="Large Language Model Routing with Benchmark Datasets"></a>Large Language Model Routing with Benchmark Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15789">http://arxiv.org/abs/2309.15789</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tal Shnitzer, Anthony Ou, Mírian Silva, Kate Soule, Yuekai Sun, Justin Solomon, Neil Thompson, Mikhail Yurochkin</li>
<li>for: 选择最佳大语言模型（LLM） для新任务</li>
<li>methods: 利用数据集改进 router 模型选择</li>
<li>results: 在多个任务和场景中提高选择 LLM 的性能<details>
<summary>Abstract</summary>
There is a rapidly growing number of open-source Large Language Models (LLMs) and benchmark datasets to compare them. While some models dominate these benchmarks, no single model typically achieves the best accuracy in all tasks and use cases. In this work, we address the challenge of selecting the best LLM out of a collection of models for new tasks. We propose a new formulation for the problem, in which benchmark datasets are repurposed to learn a "router" model for this LLM selection, and we show that this problem can be reduced to a collection of binary classification tasks. We demonstrate the utility and limitations of learning model routers from various benchmark datasets, where we consistently improve performance upon using any single model for all tasks.
</details>
<details>
<summary>摘要</summary>
有一个快速增长的开源大语言模型（LLM）和测试数据集的比较。虽然一些模型在这些测试中占据主导地位，但通常没有一个模型能够在所有任务和场景中达到最佳性能。在这项工作中，我们解决了选择集合中的最佳LLM的挑战。我们提出了一种新的问题形ulation，在其中测试数据集被重用来学习一个"路由"模型，并证明这个问题可以转化为一系列二分类任务。我们示出了学习模型路由的实用性和局限性，并在不同的测试数据集上 consistently 提高了使用任何单个模型来完成所有任务的性能。
</details></li>
</ul>
<hr>
<h2 id="Question-answering-using-deep-learning-in-low-resource-Indian-language-Marathi"><a href="#Question-answering-using-deep-learning-in-low-resource-Indian-language-Marathi" class="headerlink" title="Question answering using deep learning in low resource Indian language Marathi"></a>Question answering using deep learning in low resource Indian language Marathi</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15779">http://arxiv.org/abs/2309.15779</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhiraj Amin, Sharvari Govilkar, Sagar Kulkarni</li>
<li>for: 这个论文是为了研究如何使用Transformer模型创建基于阅读理解的马拉地语问答系统。</li>
<li>methods: 本论文使用了多种Transformer模型，包括Multilingual Representations for Indian Languages (MuRIL)、MahaBERT和Indic Bidirectional Encoder Representations from Transformers (IndicBERT)，并对这些模型进行了精度调整和微调整，以便在马拉地语阅读理解基数据集上进行测试。</li>
<li>results: 研究发现，在多种Transformer模型中，Multilingual Representations for Indian Languages (MuRIL)多语言模型在马拉地语 dataset 上得到了最高的准确率，具体来说是EM分数为0.64和F1分数为0.74。<details>
<summary>Abstract</summary>
Precise answers are extracted from a text for a given input question in a question answering system. Marathi question answering system is created in recent studies by using ontology, rule base and machine learning based approaches. Recently transformer models and transfer learning approaches are used to solve question answering challenges. In this paper we investigate different transformer models for creating a reading comprehension-based Marathi question answering system. We have experimented on different pretrained Marathi language multilingual and monolingual models like Multilingual Representations for Indian Languages (MuRIL), MahaBERT, Indic Bidirectional Encoder Representations from Transformers (IndicBERT) and fine-tuned it on a Marathi reading comprehension-based data set. We got the best accuracy in a MuRIL multilingual model with an EM score of 0.64 and F1 score of 0.74 by fine tuning the model on the Marathi dataset.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Precise answers are extracted from a text for a given input question in a question answering system. Marathi question answering system is created in recent studies by using ontology, rule base and machine learning based approaches. Recently transformer models and transfer learning approaches are used to solve question answering challenges. In this paper we investigate different transformer models for creating a reading comprehension-based Marathi question answering system. We have experimented on different pretrained Marathi language multilingual and monolingual models like Multilingual Representations for Indian Languages (MuRIL), MahaBERT, Indic Bidirectional Encoder Representations from Transformers (IndicBERT) and fine-tuned it on a Marathi reading comprehension-based data set. We got the best accuracy in a MuRIL multilingual model with an EM score of 0.64 and F1 score of 0.74 by fine tuning the model on the Marathi dataset." into Simplified Chinese.答案是从文本中提取的，用于给定输入问题的问答系统。印地语问答系统在最近的研究中使用ontology、规则集和机器学习方法创建。现在 transformer 模型和传输学习方法在解决问答挑战中得到广泛应用。在这篇论文中，我们 investigate 不同的 transformer 模型，用于创建基于阅读理解的印地语问答系统。我们在不同的预训练的印地语语言多语言和单语言模型（如 Multilingual Representations for Indian Languages （MuRIL）、 MahaBERT 和 Indic Bidirectional Encoder Representations from Transformers （IndicBERT））中进行了微调，并在印地语阅读理解基数据集上进行了测试。我们在 MuRIL 多语言模型中得到了最好的准确率，EM 分数为 0.64 和 F1 分数为 0.74 。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-End-to-End-Conversational-Speech-Translation-Through-Target-Language-Context-Utilization"><a href="#Enhancing-End-to-End-Conversational-Speech-Translation-Through-Target-Language-Context-Utilization" class="headerlink" title="Enhancing End-to-End Conversational Speech Translation Through Target Language Context Utilization"></a>Enhancing End-to-End Conversational Speech Translation Through Target Language Context Utilization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15686">http://arxiv.org/abs/2309.15686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amir Hussein, Brian Yan, Antonios Anastasopoulos, Shinji Watanabe, Sanjeev Khudanpur</li>
<li>for: 这篇论文是为了提高端到端语音翻译（E2E-ST）中的准确性和稳定性而写的。</li>
<li>methods: 这篇论文使用了目标语言上下文来增强E2E-ST的准确性和稳定性，并使用了语音段长信息来扩大上下文的覆盖范围。此外，它还提出了上下文排除法以确保模型的可靠性。</li>
<li>results: 作者的提议的上下文E2E-ST方法比隔离单个句子的E2E-ST方法表现更好，并且在对话语音中，上下文信息主要帮助捕捉上下文风格以及解决 named entities 和 anaphora 等问题。<details>
<summary>Abstract</summary>
Incorporating longer context has been shown to benefit machine translation, but the inclusion of context in end-to-end speech translation (E2E-ST) remains under-studied. To bridge this gap, we introduce target language context in E2E-ST, enhancing coherence and overcoming memory constraints of extended audio segments. Additionally, we propose context dropout to ensure robustness to the absence of context, and further improve performance by adding speaker information. Our proposed contextual E2E-ST outperforms the isolated utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational speech, contextual information primarily contributes to capturing context style, as well as resolving anaphora and named entities.
</details>
<details>
<summary>摘要</summary>
Contextual 翻译增强 machine translation 的效果已经被证明，但是在结构 speech translation （E2E-ST）中包含 context 的使用还尚未得到充分研究。为了填补这个空白，我们在 E2E-ST 中引入目标语言 context，从而提高 coherence 和抗耗尽性。此外，我们还提出了 context dropout，以确保模型在 context 缺失时的稳定性，并进一步提高性能。我们的Contextual E2E-ST 方法在 isolation 的 utterance-based E2E-ST 方法上表现出色。最后，我们证明了在对话speech中，contextual information 主要帮助捕捉 context style，以及解决 anaphora 和 named entities。Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Speech-collage-code-switched-audio-generation-by-collaging-monolingual-corpora"><a href="#Speech-collage-code-switched-audio-generation-by-collaging-monolingual-corpora" class="headerlink" title="Speech collage: code-switched audio generation by collaging monolingual corpora"></a>Speech collage: code-switched audio generation by collaging monolingual corpora</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15674">http://arxiv.org/abs/2309.15674</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jsalt2022codeswitchingasr/generating-code-switched-audio">https://github.com/jsalt2022codeswitchingasr/generating-code-switched-audio</a></li>
<li>paper_authors: Amir Hussein, Dorsa Zeinali, Ondřej Klejch, Matthew Wiesner, Brian Yan, Shammur Chowdhury, Ahmed Ali, Shinji Watanabe, Sanjeev Khudanpur</li>
<li>for: 本研究旨在提高自动语音识别（ASR）系统在混合语言（Code-Switching，CS）中的效果，尤其是在数据稀缺的情况下。</li>
<li>methods: 本研究提出了一种名为Speech Collage的方法，它可以将单语言 corpora 中的音频段落拼接成CS数据。此外，我们还使用了 overlap-add 方法来提高音频生成的质量。</li>
<li>results: 我们的实验结果表明，使用生成的CS数据可以对 Speech Recognition 系统产生很大的改善。在域内enario中，相比于不使用生成数据，使用生成的CS数据可以降低混合错误率（Mixed-Error Rate）和单词错误率（Word-Error Rate）的相对改善为34.4%和16.2%。此外，我们还发现，通过增加CS数据来增强模型的多语言倾向性和减少单语言偏好。<details>
<summary>Abstract</summary>
Designing effective automatic speech recognition (ASR) systems for Code-Switching (CS) often depends on the availability of the transcribed CS resources. To address data scarcity, this paper introduces Speech Collage, a method that synthesizes CS data from monolingual corpora by splicing audio segments. We further improve the smoothness quality of audio generation using an overlap-add approach. We investigate the impact of generated data on speech recognition in two scenarios: using in-domain CS text and a zero-shot approach with synthesized CS text. Empirical results highlight up to 34.4% and 16.2% relative reductions in Mixed-Error Rate and Word-Error Rate for in-domain and zero-shot scenarios, respectively. Lastly, we demonstrate that CS augmentation bolsters the model's code-switching inclination and reduces its monolingual bias.
</details>
<details>
<summary>摘要</summary>
设计有效的自动语音识别（ASR）系统 для代码交换（CS）经常受到数据稀缺的限制。本文提出了 Speech Collage，一种方法，通过将单语言 corpus 中的音频段落拼接起来生成 CS 数据。我们还使用 overlap-add 方法来提高生成的音频质量。我们在两个场景中研究生成数据的影响：使用域内 CS 文本，以及零Instance 方法使用生成的 CS 文本。实验结果表明，可以 obt ain up to 34.4% 和 16.2% 的相对改善率reduction 和 Word-Error Rate 在域内和零Instance 场景中，分别。最后，我们示出了 CS 增强 bolsters 模型的代码交换倾向性，并减少了它的单语言偏好。
</details></li>
</ul>
<hr>
<h2 id="MONOVAB-An-Annotated-Corpus-for-Bangla-Multi-label-Emotion-Detection"><a href="#MONOVAB-An-Annotated-Corpus-for-Bangla-Multi-label-Emotion-Detection" class="headerlink" title="MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection"></a>MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15670">http://arxiv.org/abs/2309.15670</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sajaldoes/facebookscraper">https://github.com/sajaldoes/facebookscraper</a></li>
<li>paper_authors: Sumit Kumar Banshal, Sajal Das, Shumaiya Akter Shammi, Narayan Ranjan Chakraborty</li>
<li>for: 这个研究旨在为旁遮� Bangla 语言中的情感识别（ER）和情感分析（SA）领域提供更加精确的扩展方法，并且对这种领域的研究进行探索。</li>
<li>methods: 这个研究使用了一种基于上下文的方法，并且使用了 BERT 方法来进行预测。</li>
<li>results: 研究发现，使用 BERT 方法可以获得最佳的结果，并且在多个情感类型上进行了多类情感识别。此外，还开发了一个网页应用程序来展示这个预测模型的性能。<details>
<summary>Abstract</summary>
In recent years, Sentiment Analysis (SA) and Emotion Recognition (ER) have been increasingly popular in the Bangla language, which is the seventh most spoken language throughout the entire world. However, the language is structurally complicated, which makes this field arduous to extract emotions in an accurate manner. Several distinct approaches such as the extraction of positive and negative sentiments as well as multiclass emotions, have been implemented in this field of study. Nevertheless, the extraction of multiple sentiments is an almost untouched area in this language. Which involves identifying several feelings based on a single piece of text. Therefore, this study demonstrates a thorough method for constructing an annotated corpus based on scrapped data from Facebook to bridge the gaps in this subject area to overcome the challenges. To make this annotation more fruitful, the context-based approach has been used. Bidirectional Encoder Representations from Transformers (BERT), a well-known methodology of transformers, have been shown the best results of all methods implemented. Finally, a web application has been developed to demonstrate the performance of the pre-trained top-performer model (BERT) for multi-label ER in Bangla.
</details>
<details>
<summary>摘要</summary>
近年来，情感分析（SA）和情感识别（ER）在孟加拉语中得到了越来越多的关注，孟加拉语是全球第七大语言之一。然而，这门语言结构复杂，使得在精确地检测情感上具有挑战性。多种不同的方法，如 позитив和负情感提取以及多类情感识别，在这个领域中已经实施。然而，多种情感的提取仍然是孟加拉语中未曾被探讨的领域。因此，本研究提出了一种系统的方法，基于Facebook上抓取的数据，构建了一个注释 corpora，以解决这些问题。为了使此注释更有价值， Context-based 方法被使用。在这些方法中， BERT 方法，一种著名的 transformers 方法，在所有实施的方法中显示了最好的结果。最后，一个 web 应用程序被开发，以示出预训练的最佳模型（BERT）在孟加拉语中的多标签 ER 性能。
</details></li>
</ul>
<hr>
<h2 id="Conversational-Feedback-in-Scripted-versus-Spontaneous-Dialogues-A-Comparative-Analysis"><a href="#Conversational-Feedback-in-Scripted-versus-Spontaneous-Dialogues-A-Comparative-Analysis" class="headerlink" title="Conversational Feedback in Scripted versus Spontaneous Dialogues: A Comparative Analysis"></a>Conversational Feedback in Scripted versus Spontaneous Dialogues: A Comparative Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15656">http://arxiv.org/abs/2309.15656</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ildikó Pilán, Laurent Prévot, Hendrik Buschmeier, Pierre Lison</li>
<li>for: 这篇论文的目的是分析对话中的反馈phenomena，以及这些现象在自然语言对话和脚本对话之间的差异。</li>
<li>methods: 该论文使用了一种神经网络对话动作标签器来EXTRACT对话数据中的 lexical statistics和分类输出，并对英文、法语、德语、匈牙利语、意大利语、日语、挪威语和中文等语言的对话数据进行了分析。</li>
<li>results: 论文的两个主要发现是：一是对话反馈在对话副本中比自然对话更少，二是对话副本中含有更多的负反馈。此外，文章还表明了大语言模型也遵循同样的趋势，即对话响应中包含少量的反馈，除非特别地进行了适应自然对话的细化调整。<details>
<summary>Abstract</summary>
Scripted dialogues such as movie and TV subtitles constitute a widespread source of training data for conversational NLP models. However, the linguistic characteristics of those dialogues are notably different from those observed in corpora of spontaneous interactions. This difference is particularly marked for communicative feedback and grounding phenomena such as backchannels, acknowledgments, or clarification requests. Such signals are known to constitute a key part of the conversation flow and are used by the dialogue participants to provide feedback to one another on their perception of the ongoing interaction. This paper presents a quantitative analysis of such communicative feedback phenomena in both subtitles and spontaneous conversations. Based on dialogue data in English, French, German, Hungarian, Italian, Japanese, Norwegian and Chinese, we extract both lexical statistics and classification outputs obtained with a neural dialogue act tagger. Two main findings of this empirical study are that (1) conversational feedback is markedly less frequent in subtitles than in spontaneous dialogues and (2) subtitles contain a higher proportion of negative feedback. Furthermore, we show that dialogue responses generated by large language models also follow the same underlying trends and include comparatively few occurrences of communicative feedback, except when those models are explicitly fine-tuned on spontaneous dialogues.
</details>
<details>
<summary>摘要</summary>
Movie 和 TV 字幕等脚本对话数据成为对话NLG模型训练的广泛来源。然而，这些对话的语言特征与临时交流中观察到的不同很大，特别是通信反馈和固定化现象，如后沟通、识别或清晰请求。这些信号被认为是对话流程的重要组成部分，它们由对话参与者用来对对话进行反馈。这篇论文通过对英文、法语、德语、匈牙利语、意大利语、日语、挪威语和中文对话数据进行量化分析，挖掘出 lexical 统计和基于神经对话 acts 标签器的分类输出。我们发现了两个主要结论：一是对话反馈在字幕中明显较少，二是字幕中含有较高比例的负反馈。此外，我们还证明了大语言模型也遵循同样的基本趋势，即包括对话中很少的交流反馈，除非这些模型被明确 fine-tune 于临时对话。
</details></li>
</ul>
<hr>
<h2 id="NLPBench-Evaluating-Large-Language-Models-on-Solving-NLP-Problems"><a href="#NLPBench-Evaluating-Large-Language-Models-on-Solving-NLP-Problems" class="headerlink" title="NLPBench: Evaluating Large Language Models on Solving NLP Problems"></a>NLPBench: Evaluating Large Language Models on Solving NLP Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15630">http://arxiv.org/abs/2309.15630</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/linxins97/nlpbench">https://github.com/linxins97/nlpbench</a></li>
<li>paper_authors: Linxin Song, Jieyu Zhang, Lechao Cheng, Pengyuan Zhou, Tianyi Zhou, Irene Li</li>
<li>for: 该论文旨在探讨大语言模型（LLMs）在自然语言处理（NLP）领域的问题解决能力。</li>
<li>methods: 该论文使用了一个唯一的benchmark dataset，称为NLPBench，包含378个大学水平的NLP问题，涵盖了不同的NLP话题。该论文还使用了高级的提示策略，如链条思维（CoT）和树条思维（ToT）来评估LLMs的表现。</li>
<li>results: 该论文的研究发现，高级的提示策略的效果可能是不平等的，有时会对小型模型（如LLAMA-2）造成损害。此外，手动评估还暴露出了LLMs在科学问题解决中的缺陷，特别是逻辑分解和推理能力的弱点对结果产生了影响。<details>
<summary>Abstract</summary>
Recent developments in large language models (LLMs) have shown promise in enhancing the capabilities of natural language processing (NLP). Despite these successes, there remains a dearth of research dedicated to the NLP problem-solving abilities of LLMs. To fill the gap in this area, we present a unique benchmarking dataset, NLPBench, comprising 378 college-level NLP questions spanning various NLP topics sourced from Yale University's prior final exams. NLPBench includes questions with context, in which multiple sub-questions share the same public information, and diverse question types, including multiple choice, short answer, and math. Our evaluation, centered on LLMs such as GPT-3.5/4, PaLM-2, and LLAMA-2, incorporates advanced prompting strategies like the chain-of-thought (CoT) and tree-of-thought (ToT). Our study reveals that the effectiveness of the advanced prompting strategies can be inconsistent, occasionally damaging LLM performance, especially in smaller models like the LLAMA-2 (13b). Furthermore, our manual assessment illuminated specific shortcomings in LLMs' scientific problem-solving skills, with weaknesses in logical decomposition and reasoning notably affecting results.
</details>
<details>
<summary>摘要</summary>
最近的大语言模型（LLM）的发展已经显示出了提高自然语言处理（NLP）的能力的承诺。然而，还没有充分的研究关注LLM在NLP问题解决能力方面的研究。为了填补这一空白，我们提供了一个独特的标准 benchmarck dataset，即NLPBench，其包含378个大学水平的NLP问题，这些问题来源于叶lez大学的过去的最终考试。NLPBench包括问题带上下文，多个子问题共享公共信息，以及多种问题类型，包括多选、简答和数学类型。我们的评估中心于GPT-3.5/4、PaLM-2和LLAMA-2等LLM，并使用高级提示策略，如链条思维（CoT）和树条思维（ToT）。我们的研究发现，高级提示策略的效iveness可以不均匀，有时会对小型模型 like LLAMA-2（13b）产生负面影响。此外，我们的手动评估还揭示了LLM在科学问题解决能力中的缺陷，尤其是逻辑分解和推理能力受到了影响。
</details></li>
</ul>
<hr>
<h2 id="Few-Shot-Multi-Label-Aspect-Category-Detection-Utilizing-Prototypical-Network-with-Sentence-Level-Weighting-and-Label-Augmentation"><a href="#Few-Shot-Multi-Label-Aspect-Category-Detection-Utilizing-Prototypical-Network-with-Sentence-Level-Weighting-and-Label-Augmentation" class="headerlink" title="Few-Shot Multi-Label Aspect Category Detection Utilizing Prototypical Network with Sentence-Level Weighting and Label Augmentation"></a>Few-Shot Multi-Label Aspect Category Detection Utilizing Prototypical Network with Sentence-Level Weighting and Label Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15588">http://arxiv.org/abs/2309.15588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyu Wang, Mizuho Iwaihara</li>
<li>for: 本研究旨在提高多个标签方面类划分中的准确率，通过使用支持集注意 Mechanism 和增强的标签文本信息。</li>
<li>methods: 本研究使用 prototypical network 和注意机制，首先在支持集中计算每个类划分的均值，然后使用 sentence-level 注意机制对每个支持集实例进行权重调整，最后将计算出的投影用于计算查询集中的噪声抑制。</li>
<li>results: 实验结果表明，我们的提议方法在 Yelp 数据集四个不同的场景中均有较高的表现，并且超越了所有基线方法。<details>
<summary>Abstract</summary>
Multi-label aspect category detection is intended to detect multiple aspect categories occurring in a given sentence. Since aspect category detection often suffers from limited datasets and data sparsity, the prototypical network with attention mechanisms has been applied for few-shot aspect category detection. Nevertheless, most of the prototypical networks used so far calculate the prototypes by taking the mean value of all the instances in the support set. This seems to ignore the variations between instances in multi-label aspect category detection. Also, several related works utilize label text information to enhance the attention mechanism. However, the label text information is often short and limited, and not specific enough to discern categories. In this paper, we first introduce support set attention along with the augmented label information to mitigate the noise at word-level for each support set instance. Moreover, we use a sentence-level attention mechanism that gives different weights to each instance in the support set in order to compute prototypes by weighted averaging. Finally, the calculated prototypes are further used in conjunction with query instances to compute query attention and thereby eliminate noises from the query set. Experimental results on the Yelp dataset show that our proposed method is useful and outperforms all baselines in four different scenarios.
</details>
<details>
<summary>摘要</summary>
多标签方面类划分是用于检测给定句子中的多个方面类。由于方面类划分经常受到有限的数据和数据稀缺的限制，因此使用 prototype 网络和注意机制来实现少量的方面类划分。然而，大多数的 prototype 网络使用的是取支持集中的所有实例的平均值来计算prototype。这看似忽略了多标签方面类划分中实例之间的差异。此外，一些相关的工作使用标签文本信息来增强注意机制。然而，标签文本信息通常短暂，有限，不够特别地区分类。在本文中，我们首先引入支持集注意以及增强的标签信息来减少每个支持集实例的噪声。此外，我们使用句子级注意机制，对每个支持集实例进行不同的权重计算，以计算prototype。最后，计算出的prototype被用与查询实例进行计算查询注意，以消除查询集中的噪声。实验结果表明，我们提出的方法在Yelp数据集上表现出色，并在四个不同的场景下超过所有基线。
</details></li>
</ul>
<hr>
<h2 id="Jointly-Training-Large-Autoregressive-Multimodal-Models"><a href="#Jointly-Training-Large-Autoregressive-Multimodal-Models" class="headerlink" title="Jointly Training Large Autoregressive Multimodal Models"></a>Jointly Training Large Autoregressive Multimodal Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15564">http://arxiv.org/abs/2309.15564</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kyegomez/MultiModalCrossAttn">https://github.com/kyegomez/MultiModalCrossAttn</a></li>
<li>paper_authors: Emanuele Aiello, Lili Yu, Yixin Nie, Armen Aghajanyan, Barlas Oguz</li>
<li>for: 本研究旨在开发一种能够生成高质量多Modal输出的单一模型，以满足现代机器学习领域中的权威需求。</li>
<li>methods: 该模型采用了一种模块化的方法，将现有的文本和图像生成模型系统地融合在一起，并 introduce了一种特殊的数据效率的指令调整策略，适应混合多Modal生成任务。</li>
<li>results: 研究人员通过对模型进行特定的指令调整，实现了生成高质量多Modal输出的目标，并表明了这种模型在混合多Modal生成任务中的首次应用。<details>
<summary>Abstract</summary>
In recent years, advances in the large-scale pretraining of language and text-to-image models have revolutionized the field of machine learning. Yet, integrating these two modalities into a single, robust model capable of generating seamless multimodal outputs remains a significant challenge. To address this gap, we present the Joint Autoregressive Mixture (JAM) framework, a modular approach that systematically fuses existing text and image generation models. We also introduce a specialized, data-efficient instruction-tuning strategy, tailored for mixed-modal generation tasks. Our final instruct-tuned model demonstrates unparalleled performance in generating high-quality multimodal outputs and represents the first model explicitly designed for this purpose.
</details>
<details>
<summary>摘要</summary>
近年来，大规模预训练语言和文本到图像模型的进步，已经对机器学习领域产生了革命性的变革。然而，将这两种模式集成成一个完整、可靠的多模式模型，以生成无缝多Modal输出仍然是一项重要挑战。为解决这一问题，我们提出了共同自适应混合（JAM）框架，这是一种模块化的方法，可以系统地融合现有的文本和图像生成模型。我们还提出了特化于混合多Modal生成任务的数据效率准则调整策略。最终，我们的指导调整模型在生成高质量多Modal输出的表现卓越，并成为首个专门为这种目的设计的模型。
</details></li>
</ul>
<hr>
<h2 id="VideoAdviser-Video-Knowledge-Distillation-for-Multimodal-Transfer-Learning"><a href="#VideoAdviser-Video-Knowledge-Distillation-for-Multimodal-Transfer-Learning" class="headerlink" title="VideoAdviser: Video Knowledge Distillation for Multimodal Transfer Learning"></a>VideoAdviser: Video Knowledge Distillation for Multimodal Transfer Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15494">http://arxiv.org/abs/2309.15494</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanan Wang, Donghuo Zeng, Shinya Wada, Satoshi Kurihara</li>
<li>for: 这个论文旨在解决多模态融合问题，提高多模态融合系统的效率和性能。</li>
<li>methods: 该论文提出了一种基于视频知识塑造的多模态知识传递方法，使用CLIP模型提供多模态知识监督信号，并通过一个步骤式塑造目标函数来传递知识。</li>
<li>results: 该方法在两个多模态任务中（MOSI和MOSEI数据集以及VEGAS数据集）表现出色，在视频层 sentiment分析任务中，学生模型（只需要文本模式作为输入）的MAE分数提高了12.3%。此外，该方法还在VEGAS数据集上提高了现有方法的3.4% mAP分数，而无需额外计算。这些结果表明该方法在实现高效率高性能多模态传递学习中的优势。<details>
<summary>Abstract</summary>
Multimodal transfer learning aims to transform pretrained representations of diverse modalities into a common domain space for effective multimodal fusion. However, conventional systems are typically built on the assumption that all modalities exist, and the lack of modalities always leads to poor inference performance. Furthermore, extracting pretrained embeddings for all modalities is computationally inefficient for inference. In this work, to achieve high efficiency-performance multimodal transfer learning, we propose VideoAdviser, a video knowledge distillation method to transfer multimodal knowledge of video-enhanced prompts from a multimodal fundamental model (teacher) to a specific modal fundamental model (student). With an intuition that the best learning performance comes with professional advisers and smart students, we use a CLIP-based teacher model to provide expressive multimodal knowledge supervision signals to a RoBERTa-based student model via optimizing a step-distillation objective loss -- first step: the teacher distills multimodal knowledge of video-enhanced prompts from classification logits to a regression logit -- second step: the multimodal knowledge is distilled from the regression logit of the teacher to the student. We evaluate our method in two challenging multimodal tasks: video-level sentiment analysis (MOSI and MOSEI datasets) and audio-visual retrieval (VEGAS dataset). The student (requiring only the text modality as input) achieves an MAE score improvement of up to 12.3% for MOSI and MOSEI. Our method further enhances the state-of-the-art method by 3.4% mAP score for VEGAS without additional computations for inference. These results suggest the strengths of our method for achieving high efficiency-performance multimodal transfer learning.
</details>
<details>
<summary>摘要</summary>
多模态转移学习目的是将预训 representations of 多个模式转换到共同领域空间，以便实现效果的多模态融合。然而，传统系统通常是基于所有模式都存在的假设，缺少模式会导致较差的推论性能。另外，从多个模式中提取预训嵌入的computational complexity对于推论是高的。在这个工作中，我们提出了 VideoAdviser，一种影片智慧传承方法，将多模态知识传承自一个多模式基础模型（教师）至一个具体的模式基础模型（学生）。我们的想法是，在专业指导和聪明的学生之下，学习最佳性能。我们使用基于 CLIP 的教师模型提供多模式知识超visuel 监督信号，将其转换为一个 step-distillation 目标函数损失——第一步：教师对影片增强提示的多模式知识进行分类 logits 的激发——第二步：将多模式知识从教师的分类 logits 转换为学生的分类 logits。我们在 MOSI 和 MOSEI  dataset 进行了两个多模式任务的评估：影片水平情感分析和音频视觉搜寻。学生（仅需要文本模式作为输入）在 MOSI 和 MOSEI  dataset 中的 MAE 分数改善为最多 12.3%。我们的方法还提高了现有方法的 mAP 分数 by 3.4%  без需要进行额外的计算。这些结果显示了我们的方法在实现高效率-性能多模态转移学习的能力。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Multi-Scale-Context-Aggregation-for-Conversational-Aspect-Based-Sentiment-Quadruple-Analysis"><a href="#Dynamic-Multi-Scale-Context-Aggregation-for-Conversational-Aspect-Based-Sentiment-Quadruple-Analysis" class="headerlink" title="Dynamic Multi-Scale Context Aggregation for Conversational Aspect-Based Sentiment Quadruple Analysis"></a>Dynamic Multi-Scale Context Aggregation for Conversational Aspect-Based Sentiment Quadruple Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15476">http://arxiv.org/abs/2309.15476</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuqing Li, Wenyuan Zhang, Binbin Li, Siyu Jia, Zisen Qi, Xingbang Tan</li>
<li>for: 这个研究的目的是提出了一种基于对话结构的强大的 sentiment quadruple 分析方法，以便更好地捕捉对话中的 quadruple 元素。</li>
<li>methods: 这个方法使用了一种名为 Dynamic Multi-scale Context Aggregation network (DMCA)，它首先利用对话结构生成多级utterance window，然后通过动态层次聚合模块来集成进步的cue。</li>
<li>results: 对比基elines，这个方法在实验中表现出了显著的优势，并达到了领域内的状态之术性表现。<details>
<summary>Abstract</summary>
Conversational aspect-based sentiment quadruple analysis (DiaASQ) aims to extract the quadruple of target-aspect-opinion-sentiment within a dialogue. In DiaASQ, a quadruple's elements often cross multiple utterances. This situation complicates the extraction process, emphasizing the need for an adequate understanding of conversational context and interactions. However, existing work independently encodes each utterance, thereby struggling to capture long-range conversational context and overlooking the deep inter-utterance dependencies. In this work, we propose a novel Dynamic Multi-scale Context Aggregation network (DMCA) to address the challenges. Specifically, we first utilize dialogue structure to generate multi-scale utterance windows for capturing rich contextual information. After that, we design a Dynamic Hierarchical Aggregation module (DHA) to integrate progressive cues between them. In addition, we form a multi-stage loss strategy to improve model performance and generalization ability. Extensive experimental results show that the DMCA model outperforms baselines significantly and achieves state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
文本中的异常性质 sentiment quadruple分析（DiaASQ）目的是从对话中提取目标方面的意见情感。在DiaASQ中，quadruple的元素经常跨越多个句子，这使得提取过程变得更加复杂，强调了对对话上下文和互动的深入理解。然而，现有的工作独立地编码每个句子，从而难以捕捉对话中长距离的上下文关系和深入的词语依赖关系。在这种情况下，我们提出了一种新的动态多尺度上下文聚合网络（DMCA）来解决这些挑战。 Specifically，我们首先利用对话结构生成多尺度的utterance窗口，以便捕捉丰富的上下文信息。然后，我们设计了动态层次聚合模块（DHA），用于在这些窗口之间进行进度的聚合。此外，我们设计了多阶段的损失策略，以提高模型性能和泛化能力。经验证明，DMCA模型在比较多的基线上表现出优于基eline，并达到了当前领域的状态提取模型。
</details></li>
</ul>
<hr>
<h2 id="ChatCounselor-A-Large-Language-Models-for-Mental-Health-Support"><a href="#ChatCounselor-A-Large-Language-Models-for-Mental-Health-Support" class="headerlink" title="ChatCounselor: A Large Language Models for Mental Health Support"></a>ChatCounselor: A Large Language Models for Mental Health Support</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15461">http://arxiv.org/abs/2309.15461</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/emocareai/chatpsychiatrist">https://github.com/emocareai/chatpsychiatrist</a></li>
<li>paper_authors: June M. Liu, Donghao Li, He Cao, Tianhe Ren, Zeyi Liao, Jiamin Wu</li>
<li>for: 这个论文旨在提供心理支持，不同于通用的chatbot，它基于专业心理师和客户之间的真实对话，因此具有专业心理知识和辅导技能。</li>
<li>methods: 这个解决方案使用了GPT-4和特制的提示来进行辅导，并根据七项心理辅导评价指标来评估辅导响应质量。</li>
<li>results: 对比已有的开源模型，ChatCounselor在辅导桌上表现出色，其表现相当于ChatGPT，这显示了数据驱动的模型能力得到了显著提高。<details>
<summary>Abstract</summary>
This paper presents ChatCounselor, a large language model (LLM) solution designed to provide mental health support. Unlike generic chatbots, ChatCounselor is distinguished by its foundation in real conversations between consulting clients and professional psychologists, enabling it to possess specialized knowledge and counseling skills in the field of psychology. The training dataset, Psych8k, was constructed from 260 in-depth interviews, each spanning an hour. To assess the quality of counseling responses, the counseling Bench was devised. Leveraging GPT-4 and meticulously crafted prompts based on seven metrics of psychological counseling assessment, the model underwent evaluation using a set of real-world counseling questions. Impressively, ChatCounselor surpasses existing open-source models in the counseling Bench and approaches the performance level of ChatGPT, showcasing the remarkable enhancement in model capability attained through high-quality domain-specific data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Beyond-the-Chat-Executable-and-Verifiable-Text-Editing-with-LLMs"><a href="#Beyond-the-Chat-Executable-and-Verifiable-Text-Editing-with-LLMs" class="headerlink" title="Beyond the Chat: Executable and Verifiable Text-Editing with LLMs"></a>Beyond the Chat: Executable and Verifiable Text-Editing with LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15337">http://arxiv.org/abs/2309.15337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philippe Laban, Jesse Vig, Marti A. Hearst, Caiming Xiong, Chien-Sheng Wu</li>
<li>For: The paper aims to provide a more transparent and verifiable editing interface for documents edited with Large Language Models (LLMs).* Methods: The proposed interface, called InkSync, suggests executable edits directly within the document being edited, and supports a 3-stage approach to mitigate the risk of factual errors introduced by LLMs.* Results: Two usability studies confirm the effectiveness of InkSync’s components compared to standard LLM-based chat interfaces, leading to more accurate, more efficient editing, and improved user experience.Here’s the same information in Simplified Chinese text:* For: 该论文旨在提供基于大语言模型（LLM）的文档编辑器，具有更高的透明度和可靠性。* Methods: 提议的界面是InkSync，它在文档中直接提供执行修改建议，并支持三个阶段方法来减少LLM引入的事实错误风险。* Results: 两项用户研究证明InkSync的组件在与标准LLM基于chat界面进行比较时，有更高的准确性、更高的效率、和更好的用户体验。<details>
<summary>Abstract</summary>
Conversational interfaces powered by Large Language Models (LLMs) have recently become a popular way to obtain feedback during document editing. However, standard chat-based conversational interfaces do not support transparency and verifiability of the editing changes that they suggest. To give the author more agency when editing with an LLM, we present InkSync, an editing interface that suggests executable edits directly within the document being edited. Because LLMs are known to introduce factual errors, Inksync also supports a 3-stage approach to mitigate this risk: Warn authors when a suggested edit introduces new information, help authors Verify the new information's accuracy through external search, and allow an auditor to perform an a-posteriori verification by Auditing the document via a trace of all auto-generated content. Two usability studies confirm the effectiveness of InkSync's components when compared to standard LLM-based chat interfaces, leading to more accurate, more efficient editing, and improved user experience.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）驱动的对话界面在文档编辑中获得反馈已经变得非常流行。然而，标准的chat界面不支持对编辑建议的透明度和可靠性。为给作者更多的自主权在LLM编辑，我们提出了inksync，一种在文档中直接提供可执行的编辑建议的编辑界面。因为LLM经常引入错误信息，inksync还支持三个阶段来减轻这种风险：警告作者当建议编辑引入新信息时，帮助作者验证新信息的准确性通过外部搜索，并让审核人员通过文档的跟踪来对自动生成的内容进行 posteriori 验证。两个用户研究证明了inksync的组件与标准LLM-based chat界面相比，可以提高精度、效率和用户体验。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/27/cs.CL_2023_09_27/" data-id="clp89dob200chi7883sat3lrx" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_27" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/27/cs.LG_2023_09_27/" class="article-date">
  <time datetime="2023-09-27T10:00:00.000Z" itemprop="datePublished">2023-09-27</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/27/cs.LG_2023_09_27/">cs.LG - 2023-09-27</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Label-Augmentation-Method-for-Medical-Landmark-Detection-in-Hip-Radiograph-Images"><a href="#Label-Augmentation-Method-for-Medical-Landmark-Detection-in-Hip-Radiograph-Images" class="headerlink" title="Label Augmentation Method for Medical Landmark Detection in Hip Radiograph Images"></a>Label Augmentation Method for Medical Landmark Detection in Hip Radiograph Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16066">http://arxiv.org/abs/2309.16066</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yehyun Suh, Peter Chan, J. Ryan Martin, Daniel Moyer</li>
<li>for: 预测医学影像中的临床标志物</li>
<li>methods: 使用标签Only的扩充方案进行自动医学特征点检测，并使用 generic U-Net 架构和课程 consisting of two phases 进行训练</li>
<li>results: 在六个医学影像 dataset 上，使用这种方法可以获得高效的临床标志物预测结果，并且比传统的数据扩充方法更高效<details>
<summary>Abstract</summary>
This work reports the empirical performance of an automated medical landmark detection method for predict clinical markers in hip radiograph images. Notably, the detection method was trained using a label-only augmentation scheme; our results indicate that this form of augmentation outperforms traditional data augmentation and produces highly sample efficient estimators. We train a generic U-Net-based architecture under a curriculum consisting of two phases: initially relaxing the landmarking task by enlarging the label points to regions, then gradually eroding these label regions back to the base task. We measure the benefits of this approach on six datasets of radiographs with gold-standard expert annotations.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Predicting-Cardiovascular-Complications-in-Post-COVID-19-Patients-Using-Data-Driven-Machine-Learning-Models"><a href="#Predicting-Cardiovascular-Complications-in-Post-COVID-19-Patients-Using-Data-Driven-Machine-Learning-Models" class="headerlink" title="Predicting Cardiovascular Complications in Post-COVID-19 Patients Using Data-Driven Machine Learning Models"></a>Predicting Cardiovascular Complications in Post-COVID-19 Patients Using Data-Driven Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16059">http://arxiv.org/abs/2309.16059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maitham G. Yousif, Hector J. Castro</li>
<li>for: 预测 COVID-19 后cardiovascular 疾病的风险</li>
<li>methods: 使用数据驱动机器学习模型预测352名 Iraq 地区 COVID-19 患者中的 cardiovascular 疾病风险</li>
<li>results: 机器学习模型在预测 cardiovascular 疾病风险方面表现出色，早期发现可以提供时间ous interventions 和改善结果<details>
<summary>Abstract</summary>
The COVID-19 pandemic has globally posed numerous health challenges, notably the emergence of post-COVID-19 cardiovascular complications. This study addresses this by utilizing data-driven machine learning models to predict such complications in 352 post-COVID-19 patients from Iraq. Clinical data, including demographics, comorbidities, lab results, and imaging, were collected and used to construct predictive models. These models, leveraging various machine learning algorithms, demonstrated commendable performance in identifying patients at risk. Early detection through these models promises timely interventions and improved outcomes. In conclusion, this research underscores the potential of data-driven machine learning for predicting post-COVID-19 cardiovascular complications, emphasizing the need for continued validation and research in diverse clinical settings.
</details>
<details>
<summary>摘要</summary>
COVID-19 大流行已经在全球带来了许多健康挑战，其中包括后 COVID-19 冠状病毒疾病的出现。这项研究利用数据驱动的机器学习模型来预测这些病例中的合并症。研究在 Iraq 352 名 POST-COVID-19 患者的临床数据，包括人口统计、相关病种、实验室测试结果和成像，以构建预测模型。这些模型通过不同的机器学习算法，在预测患者风险中表现了良好的表现。早期发现通过这些模型，可以提供早期干预和改善结果。研究结论，这项研究证明了数据驱动的机器学习可以预测后 COVID-19 冠状病毒疾病，并且需要继续验证和研究在多种临床设置下。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-driven-Analysis-of-Gastrointestinal-Symptoms-in-Post-COVID-19-Patients"><a href="#Machine-Learning-driven-Analysis-of-Gastrointestinal-Symptoms-in-Post-COVID-19-Patients" class="headerlink" title="Machine Learning-driven Analysis of Gastrointestinal Symptoms in Post-COVID-19 Patients"></a>Machine Learning-driven Analysis of Gastrointestinal Symptoms in Post-COVID-19 Patients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00540">http://arxiv.org/abs/2310.00540</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maitham G. Yousif, Fadhil G. Al-Amran, Salman Rawaf, Mohammad Abdulla Grmt</li>
<li>for: This study aims to investigate the prevalence and patterns of gastrointestinal (GI) symptoms in individuals recovering from COVID-19 and to identify predictive factors for these symptoms using machine learning algorithms.</li>
<li>methods: The study uses data from 913 post-COVID-19 patients in Iraq collected during 2022 and 2023. The researchers use machine learning algorithms to identify predictive factors for GI symptoms, including age, gender, disease severity, comorbidities, and the duration of COVID-19 illness.</li>
<li>results: The study finds that a notable percentage of post-COVID-19 patients experience GI symptoms during their recovery phase, with diarrhea being the most frequently reported symptom. The researchers also identify significant predictive factors for GI symptoms, including age, gender, disease severity, comorbidities, and the duration of COVID-19 illness.Here are the results in Simplified Chinese text:</li>
<li>for: 这个研究旨在调查 COVID-19 恢复期人群中的肠胃症状的频度和特征，以及使用机器学习算法预测这些症状的预测因素。</li>
<li>methods: 该研究使用2022年和2023年在伊拉克收集的913名 COVID-19 恢复期患者的数据。研究人员使用机器学习算法预测肠胃症状的预测因素，包括年龄、性别、疾病严重程度、潜在的相关疾病和 COVID-19 病程的时间长短。</li>
<li>results: 研究发现大量的 COVID-19 恢复期患者在恢复阶段经历了肠胃症状，最常见的症状是 диаре便， followed by 腹痛和呕吐。研究人员还发现了预测肠胃症状的重要因素，包括年龄、性别、疾病严重程度、潜在的相关疾病和 COVID-19 病程的时间长短。<details>
<summary>Abstract</summary>
The COVID-19 pandemic, caused by the novel coronavirus SARS-CoV-2, has posed significant health challenges worldwide. While respiratory symptoms have been the primary focus, emerging evidence has highlighted the impact of COVID-19 on various organ systems, including the gastrointestinal (GI) tract. This study, based on data from 913 post-COVID-19 patients in Iraq collected during 2022 and 2023, investigates the prevalence and patterns of GI symptoms in individuals recovering from COVID-19 and leverages machine learning algorithms to identify predictive factors for these symptoms. The research findings reveal that a notable percentage of post-COVID-19 patients experience GI symptoms during their recovery phase. Diarrhea emerged as the most frequently reported symptom, followed by abdominal pain and nausea. Machine learning analysis uncovered significant predictive factors for GI symptoms, including age, gender, disease severity, comorbidities, and the duration of COVID-19 illness. These findings underscore the importance of monitoring and addressing GI symptoms in post-COVID-19 care, with machine learning offering valuable tools for early identification and personalized intervention. This study contributes to the understanding of the long-term consequences of COVID-19 on GI health and emphasizes the potential benefits of utilizing machine learning-driven analysis in predicting and managing these symptoms. Further research is warranted to delve into the mechanisms underlying GI symptoms in COVID-19 survivors and to develop targeted interventions for symptom management. Keywords: COVID-19, gastrointestinal symptoms, machine learning, predictive factors, post-COVID-19 care, long COVID.
</details>
<details>
<summary>摘要</summary>
COVID-19 大流行，由新型冠状病毒 SARS-CoV-2 引起，在全球造成了巨大的健康挑战。although respiratory symptoms have been the primary focus, emerging evidence has highlighted the impact of COVID-19 on various organ systems, including the gastrointestinal (GI) tract. This study, based on data from 913 post-COVID-19 patients in Iraq collected during 2022 and 2023, investigates the prevalence and patterns of GI symptoms in individuals recovering from COVID-19 and leverages machine learning algorithms to identify predictive factors for these symptoms. The research findings reveal that a notable percentage of post-COVID-19 patients experience GI symptoms during their recovery phase. 肠胃症状最常出现的是腹痛，followed by nausea and diarrhea. Machine learning analysis uncovered significant predictive factors for GI symptoms, including age, gender, disease severity, comorbidities, and the duration of COVID-19 illness. These findings underscore the importance of monitoring and addressing GI symptoms in post-COVID-19 care, with machine learning offering valuable tools for early identification and personalized intervention. This study contributes to the understanding of the long-term consequences of COVID-19 on GI health and emphasizes the potential benefits of utilizing machine learning-driven analysis in predicting and managing these symptoms. Further research is warranted to delve into the mechanisms underlying GI symptoms in COVID-19 survivors and to develop targeted interventions for symptom management. Keywords: COVID-19, gastrointestinal symptoms, machine learning, predictive factors, post-COVID-19 care, long COVID.
</details></li>
</ul>
<hr>
<h2 id="Identifying-Risk-Factors-for-Post-COVID-19-Mental-Health-Disorders-A-Machine-Learning-Perspective"><a href="#Identifying-Risk-Factors-for-Post-COVID-19-Mental-Health-Disorders-A-Machine-Learning-Perspective" class="headerlink" title="Identifying Risk Factors for Post-COVID-19 Mental Health Disorders: A Machine Learning Perspective"></a>Identifying Risk Factors for Post-COVID-19 Mental Health Disorders: A Machine Learning Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16055">http://arxiv.org/abs/2309.16055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maitham G. Yousif, Fadhil G. Al-Amran, Hector J. Castro</li>
<li>For: This study aimed to identify risk factors associated with post-COVID-19 mental health disorders in a sample of 669 patients in Iraq.* Methods: The study used machine learning techniques to analyze demographic, clinical, and psychosocial factors that may influence the development of mental health disorders in post-COVID-19 patients.* Results: The study found that age, gender, geographical region of residence, comorbidities, and the severity of COVID-19 illness were significant risk factors for developing mental health disorders. Additionally, psychosocial factors such as social support, coping strategies, and perceived stress levels played a substantial role.Here is the information in Simplified Chinese text:* 为：本研究使用机器学习技术来 indentify COVID-19后 mental health障碍的风险因素，采样从伊拉克669名患者中进行分析。* 方法：本研究使用机器学习技术来分析各种因素对 COVID-19后 mental health障碍的影响，包括人口特征、临床特征和心理社会特征。* 结果：研究发现年龄、性别、地域居住、患 COVID-19的严重程度和合并病有关 mental health障碍的风险因素。此外，社会支持、 coping 策略和感知的压力水平也发挥了重要作用。<details>
<summary>Abstract</summary>
In this study, we leveraged machine learning techniques to identify risk factors associated with post-COVID-19 mental health disorders. Our analysis, based on data collected from 669 patients across various provinces in Iraq, yielded valuable insights. We found that age, gender, and geographical region of residence were significant demographic factors influencing the likelihood of developing mental health disorders in post-COVID-19 patients. Additionally, comorbidities and the severity of COVID-19 illness were important clinical predictors. Psychosocial factors, such as social support, coping strategies, and perceived stress levels, also played a substantial role. Our findings emphasize the complex interplay of multiple factors in the development of mental health disorders following COVID-19 recovery. Healthcare providers and policymakers should consider these risk factors when designing targeted interventions and support systems for individuals at risk. Machine learning-based approaches can provide a valuable tool for predicting and preventing adverse mental health outcomes in post-COVID-19 patients. Further research and prospective studies are needed to validate these findings and enhance our understanding of the long-term psychological impact of the COVID-19 pandemic. This study contributes to the growing body of knowledge regarding the mental health consequences of the COVID-19 pandemic and underscores the importance of a multidisciplinary approach to address the diverse needs of individuals on the path to recovery. Keywords: COVID-19, mental health, risk factors, machine learning, Iraq
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们利用机器学习技术来确定 COVID-19 后精神健康问题的风险因素。我们基于伊拉克各地669名患者的数据进行分析，并获得了有价值的发现。我们发现年龄、性别和居住地区的民生因素都会影响 COVID-19 后精神健康问题的发生。此外，患者患有其他疾病和 COVID-19 病情的严重程度也是重要的临床预测因素。在社会支持、应急应急管理和感受水平方面，也有许多重要的心理因素。我们的发现表明 COVID-19 后精神健康问题的发生是多因素互动的，医疗提供者和政策制定者应该考虑这些风险因素，设计目标性的干预措施和支持系统，以降低患者风险。机器学习基于的方法可以为预测和预防 COVID-19 后精神健康问题的发展提供有价值的工具。进一步的研究和前瞻性研究是需要进行的，以验证这些发现，并增进我们对 COVID-19 疫情后长期精神影响的理解。这项研究贡献到了关于 COVID-19 疫情后精神健康问题的知识库，强调了多科学领域合作的重要性，以满足患者不同需求的多样化路径。关键词：COVID-19, 精神健康, 风险因素, 机器学习, 伊拉克
</details></li>
</ul>
<hr>
<h2 id="Cognizance-of-Post-COVID-19-Multi-Organ-Dysfunction-through-Machine-Learning-Analysis"><a href="#Cognizance-of-Post-COVID-19-Multi-Organ-Dysfunction-through-Machine-Learning-Analysis" class="headerlink" title="Cognizance of Post-COVID-19 Multi-Organ Dysfunction through Machine Learning Analysis"></a>Cognizance of Post-COVID-19 Multi-Organ Dysfunction through Machine Learning Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16736">http://arxiv.org/abs/2309.16736</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hector J. Castro, Maitham G. Yousif</li>
<li>For: The paper aims to analyze and predict multi-organ dysfunction in individuals experiencing Post-COVID-19 Syndrome using machine learning techniques.* Methods: The study uses data collection and preprocessing, feature selection and engineering, model development and validation, and ethical considerations to enhance early detection and management of Post-COVID-19 Syndrome.* Results: The paper aims to improve our understanding of Post-COVID-19 Syndrome through machine learning, potentially improving patient outcomes and quality of life.Here’s the same information in Simplified Chinese text:* For: 这个研究报告旨在使用机器学习技术分析和预测患有长COVID的多器官衰竭。* Methods: 这个研究使用数据收集和处理、特征选择和工程、模型开发和验证、伦理考虑等方法来提高患有长COVID的早期检测和管理。* Results: 这个研究可能会提高患有长COVID的患者结果和生活质量。<details>
<summary>Abstract</summary>
In the year 2022, a total of 466 patients from various cities across Iraq were included in this study. This research paper focuses on the application of machine learning techniques to analyse and predict multi-organ dysfunction in individuals experiencing Post-COVID-19 Syndrome, commonly known as Long COVID. Post-COVID-19 Syndrome presents a wide array of persistent symptoms affecting various organ systems, posing a significant challenge to healthcare. Leveraging the power of artificial intelligence, this study aims to enhance early detection and management of this complex condition. The paper outlines the importance of data collection and preprocessing, feature selection and engineering, model development and validation, and ethical considerations in conducting research in this field. By improving our understanding of Post-COVID-19 Syndrome through machine learning, healthcare providers can identify at-risk individuals and offer timely interventions, potentially improving patient outcomes and quality of life. Further research is essential to refine models, validate their clinical utility, and explore treatment options for Long COVID. Keywords: Post-COVID-19 Syndrome, Machine Learning, Multi-Organ Dysfunction, Healthcare, Artificial Intelligence.
</details>
<details>
<summary>摘要</summary>
在2022年，这项研究包括来自伊拉克各地的466名病人。这篇研究论文探讨了使用机器学习技术分析和预测多器系功能障碍，以便更好地诊断和管理抗covid-19后续症，通常称为“长covid”。抗covid-19后续症会导致多种持续性症状影响不同的器系，对医疗卫生 pose  significiant 挑战。通过利用人工智能技术，这项研究希望通过分析和预测多器系功能障碍，提高早期发现和管理这种复杂的疾病的能力。这篇论文还讨论了数据收集和处理、特征选择和工程、模型开发和验证以及在这一领域进行研究的伦理考虑。通过利用机器学习技术，医疗专业人员可以识别患有长covid的高风险个体，提供时间性的 intervención，可能改善病人的病理和生活质量。进一步的研究是必要的，以钻深模型，验证其临床实用性，并探讨抗covid-19后续症的治疗方法。关键词：Post-COVID-19 Syndrome, Machine Learning, Multi-Organ Dysfunction, Healthcare, Artificial Intelligence.
</details></li>
</ul>
<hr>
<h2 id="Improving-Adaptive-Online-Learning-Using-Refined-Discretization"><a href="#Improving-Adaptive-Online-Learning-Using-Refined-Discretization" class="headerlink" title="Improving Adaptive Online Learning Using Refined Discretization"></a>Improving Adaptive Online Learning Using Refined Discretization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16044">http://arxiv.org/abs/2309.16044</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiyu Zhang, Heng Yang, Ashok Cutkosky, Ioannis Ch. Paschalidis</li>
<li>for: 本文研究无约束的在线线性优化问题，具体目标是同时实现($i$)第二阶导数适应性和($ii$)参考范数适应性（也称为“参数自由”）。现有的 regret bound（Cutkosky和Orabona，2018；Mhammedi和Koolen，2020；Jacobsen和Cutkosky，2022）具有不优雅的 $O(\sqrt{V_T\log V_T})$ 依赖于导数异常 $V_T$，而 presente 工作改进到最优的 $O(\sqrt{V_T})$ Rate，使用一种新的连续时间启发的算法，无需任何不实际的双倍把拔。</li>
<li>methods: 本文使用一种新的连续时间启发的算法，并提出一种新的离散化 Argument 来保持这种适应性在敌对设置中。这种离散化 Argument 可以在敌对设置中保持适应性，并且可以从 (Harvey et al., 2023) 中提取， both algorithmically and analytically。</li>
<li>results: 本文提出的算法可以在敌对设置中实现 $O(\sqrt{V_T})$ 的 regret bound，而不是现有的 $O(\sqrt{V_T\log V_T})$  bound。此外，本文还证明了在未知 Lipschitz 常数的情况下，可以消除 Priori 知道的范围比例问题。<details>
<summary>Abstract</summary>
We study unconstrained Online Linear Optimization with Lipschitz losses. The goal is to simultaneously achieve ($i$) second order gradient adaptivity; and ($ii$) comparator norm adaptivity also known as "parameter freeness" in the literature. Existing regret bounds (Cutkosky and Orabona, 2018; Mhammedi and Koolen, 2020; Jacobsen and Cutkosky, 2022) have the suboptimal $O(\sqrt{V_T\log V_T})$ dependence on the gradient variance $V_T$, while the present work improves it to the optimal rate $O(\sqrt{V_T})$ using a novel continuous-time-inspired algorithm, without any impractical doubling trick. This result can be extended to the setting with unknown Lipschitz constant, eliminating the range ratio problem from prior works (Mhammedi and Koolen, 2020).   Concretely, we first show that the aimed simultaneous adaptivity can be achieved fairly easily in a continuous time analogue of the problem, where the environment is modeled by an arbitrary continuous semimartingale. Then, our key innovation is a new discretization argument that preserves such adaptivity in the discrete time adversarial setting. This refines a non-gradient-adaptive discretization argument from (Harvey et al., 2023), both algorithmically and analytically, which could be of independent interest.
</details>
<details>
<summary>摘要</summary>
我们研究不受限制的在线线性优化问题，即使用 lipschitz 损失函数。目标是同时实现（i）第二阶导数适应性和（ii）参数自由性，也称为“参数自由”在文献中。现有的 regret 界（Cutkosky 和 Orabona，2018年；Mhammedi 和 Koolen，2020年；Jacobsen 和 Cutkosky，2022年）具有不优雅的 $O(\sqrt{V_T\log V_T})$ 依赖于梯度方差 $V_T$，而我们的研究提高了这个约束到优化的 $O(\sqrt{V_T})$ 难度，使用一种新的连续时间启发的算法，没有任何不实用的双倍招数技巧。此结果可以推广到未知 lipschitz 常数的设定下，从而消除先前的作品（Mhammedi 和 Koolen，2020年）中的范围比例问题。更具体地，我们首先证明了目标的同时适应性可以在连续时间 аналоги中轻松实现，其中环境是一个 произволь contradicted 的 continuous semimartingale。然后，我们的关键创新是一种新的离散 argumen that preserves 这种适应性在离散时间对抗设定下。这种 refine 了一种不 gradient-adaptive 的离散 argumen from (Harvey 等，2023年)，从 both algorithmic 和 analytic 角度来说，这可能是独立的兴趣。
</details></li>
</ul>
<hr>
<h2 id="Analytical-Modelling-of-Raw-Data-for-Flow-Guided-In-body-Nanoscale-Localization"><a href="#Analytical-Modelling-of-Raw-Data-for-Flow-Guided-In-body-Nanoscale-Localization" class="headerlink" title="Analytical Modelling of Raw Data for Flow-Guided In-body Nanoscale Localization"></a>Analytical Modelling of Raw Data for Flow-Guided In-body Nanoscale Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16034">http://arxiv.org/abs/2309.16034</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillem Pascual, Filip Lemic, Carmen Delgado, Xavier Costa-Perez</li>
<li>for: 这个论文的目的是提出一种 Analytical Model of Raw Data for Flow-Guided Localization in Nanoscale Devices, 用于解决现有的通信和能源约束问题，以提高精准医学应用的可行性。</li>
<li>methods: 该论文使用 Analytical Modeling 方法，模型了 nanodevice 的 raw data 如何受到通信和能源约束的影响，并与 Simulator 进行对比，以评估模型的准确性。</li>
<li>results: 研究结果表明，模型和 Simulator 生成的 raw 数据之间存在高度相似性，并且可以在多种场景和不同性能指标下进行评估。<details>
<summary>Abstract</summary>
Advancements in nanotechnology and material science are paving the way toward nanoscale devices that combine sensing, computing, data and energy storage, and wireless communication. In precision medicine, these nanodevices show promise for disease diagnostics, treatment, and monitoring from within the patients' bloodstreams. Assigning the location of a sensed biological event with the event itself, which is the main proposition of flow-guided in-body nanoscale localization, would be immensely beneficial from the perspective of precision medicine. The nanoscale nature of the nanodevices and the challenging environment that the bloodstream represents, result in current flow-guided localization approaches being constrained in their communication and energy-related capabilities. The communication and energy constraints of the nanodevices result in different features of raw data for flow-guided localization, in turn affecting its performance. An analytical modeling of the effects of imperfect communication and constrained energy causing intermittent operation of the nanodevices on the raw data produced by the nanodevices would be beneficial. Hence, we propose an analytical model of raw data for flow-guided localization, where the raw data is modeled as a function of communication and energy-related capabilities of the nanodevice. We evaluate the model by comparing its output with the one obtained through the utilization of a simulator for objective evaluation of flow-guided localization, featuring comparably higher level of realism. Our results across a number of scenarios and heterogeneous performance metrics indicate high similarity between the model and simulator-generated raw datasets.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation) nanotechnology 和材料科学的进步正在逐渐推动无导体设备的发展，这些设备可以同时感测、计算、数据和能量储存、无线通信。在精准医学中，这些无导体设备展示出在血液中诊断、治疗和监测疾病的极大潜力。将感测生物事件的位置与事件本身相同，是精准医学的核心提案。然而，由于nanodevice的纳米规模和血液环境的挑战性，目前的流导引在血液中的本地化方法受到了通信和能量相关的限制。这些限制导致流导引方法中的数据 Raw 数据具有不同的特征，从而影响其性能。我们提出一种对raw数据的分析模型，该模型将raw数据作为nanodevice的通信和能量相关能力的函数来模型。我们对模型进行评估，并与基于模拟器的对流导引方法的评估进行比较。我们在多种情况和多样性性指标下得到的结果表明，模型和模拟器生成的raw数据之间存在高度相似性。
</details></li>
</ul>
<hr>
<h2 id="Learning-Dissipative-Neural-Dynamical-Systems"><a href="#Learning-Dissipative-Neural-Dynamical-Systems" class="headerlink" title="Learning Dissipative Neural Dynamical Systems"></a>Learning Dissipative Neural Dynamical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16032">http://arxiv.org/abs/2309.16032</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuezhu Xu, S. Sivaranjani</li>
<li>for: 学习一个不知名的非线性动力系统模型，保持系统的热力学性质。</li>
<li>methods: 在两个阶段中学习非线性动力系统模型：首先学习一个不受约束的神经动力系统模型，然后 derivation  suficient conditions 以保持系统的热力学性质，并且通过对模型参数的扰动来实现这些条件。</li>
<li>results: 这两个阶段的扰动问题可以独立解决，以获得一个保持系统热力学性质的神经动力系统模型，并且这个模型可以准确地模拟非线性系统的轨迹。<details>
<summary>Abstract</summary>
Consider an unknown nonlinear dynamical system that is known to be dissipative. The objective of this paper is to learn a neural dynamical model that approximates this system, while preserving the dissipativity property in the model. In general, imposing dissipativity constraints during neural network training is a hard problem for which no known techniques exist. In this work, we address the problem of learning a dissipative neural dynamical system model in two stages. First, we learn an unconstrained neural dynamical model that closely approximates the system dynamics. Next, we derive sufficient conditions to perturb the weights of the neural dynamical model to ensure dissipativity, followed by perturbation of the biases to retain the fit of the model to the trajectories of the nonlinear system. We show that these two perturbation problems can be solved independently to obtain a neural dynamical model that is guaranteed to be dissipative while closely approximating the nonlinear system.
</details>
<details>
<summary>摘要</summary>
请考虑一个未知的非线性动力系统，该系统知道是膨胀的。本文的目标是通过学习神经动力模型，来近似该系统，保持模型中的膨胀性质。在总之，在神经网络训练中加入膨胀性约束是一个困难的问题，现在没有知道的技术。在这种情况下，我们在两个阶段中解决了学习一个膨胀的神经动力模型。首先，我们学习一个不受约束的神经动力模型，以便尽可能地准确地近似非线性系统的动力。然后，我们 derive sufficient conditions to perturb the weights of the neural dynamical model to ensure dissipativity, followed by perturbation of the biases to retain the fit of the model to the trajectories of the nonlinear system. We show that these two perturbation problems can be solved independently to obtain a neural dynamical model that is guaranteed to be dissipative while closely approximating the nonlinear system.
</details></li>
</ul>
<hr>
<h2 id="GNNHLS-Evaluating-Graph-Neural-Network-Inference-via-High-Level-Synthesis"><a href="#GNNHLS-Evaluating-Graph-Neural-Network-Inference-via-High-Level-Synthesis" class="headerlink" title="GNNHLS: Evaluating Graph Neural Network Inference via High-Level Synthesis"></a>GNNHLS: Evaluating Graph Neural Network Inference via High-Level Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16022">http://arxiv.org/abs/2309.16022</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chenfengzhao/gnnhls">https://github.com/chenfengzhao/gnnhls</a></li>
<li>paper_authors: Chenfeng Zhao, Zehao Dong, Yixin Chen, Xuan Zhang, Roger D. Chamberlain</li>
<li>for: 本研究旨在提高图神经网络（GNN）的有效执行，使用场地编程阵列（FPGAs）作为执行平台。</li>
<li>methods: 本研究使用高级编译（HLS）工具，将GNN模型转换为FPGAs上的优化后的执行代码。</li>
<li>results: 研究发现，使用GNNHLS框架可以在4个图数据集上实现50.8倍的速度提升和423倍的能耗减少，相比CPU基线。与GPU基线相比，GNNHLS可以实现5.16倍的速度提升和74.5倍的能耗减少。<details>
<summary>Abstract</summary>
With the ever-growing popularity of Graph Neural Networks (GNNs), efficient GNN inference is gaining tremendous attention. Field-Programming Gate Arrays (FPGAs) are a promising execution platform due to their fine-grained parallelism, low-power consumption, reconfigurability, and concurrent execution. Even better, High-Level Synthesis (HLS) tools bridge the gap between the non-trivial FPGA development efforts and rapid emergence of new GNN models. In this paper, we propose GNNHLS, an open-source framework to comprehensively evaluate GNN inference acceleration on FPGAs via HLS, containing a software stack for data generation and baseline deployment, and FPGA implementations of 6 well-tuned GNN HLS kernels. We evaluate GNNHLS on 4 graph datasets with distinct topologies and scales. The results show that GNNHLS achieves up to 50.8x speedup and 423x energy reduction relative to the CPU baselines. Compared with the GPU baselines, GNNHLS achieves up to 5.16x speedup and 74.5x energy reduction.
</details>
<details>
<summary>摘要</summary>
随着图神经网络（GNN）的普及，高效的GNN执行已经吸引了很多关注。场地编程阵列（FPGAs）是一个有前途的执行平台，因为它们具有细化的并行性、低功耗消耗、可重新配置和同时执行。尤其是高级语言 Synthesis（HLS）工具，可以将FPGAs的开发努力减少到最小限度，并快速实现新的GNN模型。在本文中，我们提出了GNNHLS，一个开源框架，通过HLS来全面评估GNN执行的加速在FPGAs上，包括软件堆栈 для数据生成和基线部署，以及FPGA中6种优化后的GNN HLS kernel。我们在4个图据集上进行了测试，结果显示，GNNHLS可以在相对于CPU基线的50.8倍速度和423倍能效率下运行GNN模型。相比GPU基eline，GNNHLS可以在5.16倍速度和74.5倍能效率下运行GNN模型。
</details></li>
</ul>
<hr>
<h2 id="Graph-level-Representation-Learning-with-Joint-Embedding-Predictive-Architectures"><a href="#Graph-level-Representation-Learning-with-Joint-Embedding-Predictive-Architectures" class="headerlink" title="Graph-level Representation Learning with Joint-Embedding Predictive Architectures"></a>Graph-level Representation Learning with Joint-Embedding Predictive Architectures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16014">http://arxiv.org/abs/2309.16014</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/geriskenderi/graph-jepa">https://github.com/geriskenderi/graph-jepa</a></li>
<li>paper_authors: Geri Skenderi, Hang Li, Jiliang Tang, Marco Cristani</li>
<li>for: 本研究旨在学习自我超vised表示学习中的 JOINT-EMBEDDING PREDICTIVE ARCHITECTURES（JEPAs），用于图像领域的图表示学习。</li>
<li>methods: 本研究使用的方法是MASKED MODELING，通过预测目标信号的嵌入表示来学习图像的嵌入表示。</li>
<li>results: 研究发现，GRAPH-JEPA可以学习表示，并在图像分类和回归问题中表现出表达力和竞争力。<details>
<summary>Abstract</summary>
Joint-Embedding Predictive Architectures (JEPAs) have recently emerged as a novel and powerful technique for self-supervised representation learning. They aim to learn an energy-based model by predicting the latent representation of a target signal $y$ from a context signal $x$. JEPAs bypass the need for data augmentation and negative samples, which are typically required by contrastive learning, while avoiding the overfitting issues associated with generative-based pretraining. In this paper, we show that graph-level representations can be effectively modeled using this paradigm and propose Graph-JEPA, the first JEPA for the graph domain. In particular, we employ masked modeling to learn embeddings for different subgraphs of the input graph. To endow the representations with the implicit hierarchy that is often present in graph-level concepts, we devise an alternative training objective that consists of predicting the coordinates of the encoded subgraphs on the unit hyperbola in the 2D plane. Extensive validation shows that Graph-JEPA can learn representations that are expressive and competitive in both graph classification and regression problems.
</details>
<details>
<summary>摘要</summary>
joint-embedding predictive architectures (JEPAs) 最近 emerged as a novel 和 powerful technique for self-supervised representation learning. They aim to learn an energy-based model by predicting the latent representation of a target signal $y$ from a context signal $x$. JEPAs bypass the need for data augmentation and negative samples, which are typically required by contrastive learning, while avoiding the overfitting issues associated with generative-based pretraining. In this paper, we show that graph-level representations can be effectively modeled using this paradigm and propose Graph-JEPA, the first JEPA for the graph domain. In particular, we employ masked modeling to learn embeddings for different subgraphs of the input graph. To endow the representations with the implicit hierarchy that is often present in graph-level concepts, we devise an alternative training objective that consists of predicting the coordinates of the encoded subgraphs on the unit hyperbola in the 2D plane. Extensive validation shows that Graph-JEPA can learn representations that are expressive and competitive in both graph classification and regression problems.Here's the text with some notes on the translation:* "Joint-Embedding Predictive Architectures" is translated as "联合嵌入预测建筑" (liánhòu zhùjì yùjìng gōngchǎng)* "recently emerged" is translated as "最近 emerged" (zuìjìn yǐjī)* "novel and powerful technique" is translated as "新奇和强大的技术" (xīn qí hé qiáng dà de jìshù)* "self-supervised representation learning" is translated as "自我指导的表示学习" (ziwu zhǐdǎo de biǎozhì xuéxí)* "predicting the latent representation of a target signal" is translated as "预测目标信号的隐藏表示" (yùjì yuètiān yìjīng biǎozhì)* "from a context signal" is translated as "从上下文信号" (cong shàngxìnxiào)* "bypass the need for data augmentation and negative samples" is translated as "绕过数据扩展和负样本的需求" (luòguò xiàngxìn yǔ fāngyàng de xūyào)* "while avoiding the overfitting issues associated with generative-based pretraining" is translated as "而不是避免生成基于预训练的过拟合问题" (ér bùshì mìmiàn shēngchǎng yǐjīng yǔ yùshì de guòhùsuǒ)* "in the graph domain" is translated as "在图像领域" (zài túxiàng yìjīng)* "employ masked modeling to learn embeddings for different subgraphs" is translated as "使用假数据模型学习不同的子图" (shǐyòu kāi xiàng xiǎngxìn yǔ bùdìng de zǐtú)* "to endow the representations with the implicit hierarchy that is often present in graph-level concepts" is translated as "以使表示具有图级概念中的隐藏层次结构" (yǐ shì biǎozhì yǒu xìnghài lèi jí qiǎng yìjīng)* "we devise an alternative training objective" is translated as "我们提出了一种代替目标" (wǒmen tímcháng le yī zhōng dài biǎo mó)* "consists of predicting the coordinates of the encoded subgraphs on the unit hyperbola in the 2D plane" is translated as "包括预测编码的子图坐标在2D平面上的unit hyperbola" (bāngsuǒ yùjì yùjì yǐjīng zhǐxíng yǐjīng yǔ 2D píngmàn shàng)* "Extensive validation shows that Graph-JEPA can learn representations that are expressive and competitive in both graph classification and regression problems" is translated as "广泛验证表明，图像-JEPA可以学习表示，在图类别和回归问题中表现出色" (guǎnggòu yànzhèng bǐngmíng, túxiāng-JEPA kěyǐ xuéxí, zài túxìng yǔ fāngyì zhìdào)
</details></li>
</ul>
<hr>
<h2 id="Digital-Twin-based-Anomaly-Detection-with-Curriculum-Learning-in-Cyber-physical-Systems"><a href="#Digital-Twin-based-Anomaly-Detection-with-Curriculum-Learning-in-Cyber-physical-Systems" class="headerlink" title="Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems"></a>Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15995">http://arxiv.org/abs/2309.15995</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xuqinghua-china/tosem">https://github.com/xuqinghua-china/tosem</a></li>
<li>paper_authors: Qinghua Xu, Shaukat Ali, Tao Yue</li>
<li>for: 本研究旨在提高Cyber-Physical System (CPS) 中异常检测的精度和效率，通过增加数据难度来优化异常检测方法。</li>
<li>methods: 本研究使用的方法包括数字双方法（ATTAIN）和课程学习（curriculum learning），通过对异常数据进行难度分类，从易到难地进行学习。</li>
<li>results: 对五个实际 collected CPS 测试床上的数据进行评估，结果显示 LATTICE 在 F1 分数上比 ATTAIN 和两个基eline 高出 0.906%-2.367%，同时也可以降低 ATTAIN 的训练时间。<details>
<summary>Abstract</summary>
Anomaly detection is critical to ensure the security of cyber-physical systems (CPS). However, due to the increasing complexity of attacks and CPS themselves, anomaly detection in CPS is becoming more and more challenging. In our previous work, we proposed a digital twin-based anomaly detection method, called ATTAIN, which takes advantage of both historical and real-time data of CPS. However, such data vary significantly in terms of difficulty. Therefore, similar to human learning processes, deep learning models (e.g., ATTAIN) can benefit from an easy-to-difficult curriculum. To this end, in this paper, we present a novel approach, named digitaL twin-based Anomaly deTecTion wIth Curriculum lEarning (LATTICE), which extends ATTAIN by introducing curriculum learning to optimize its learning paradigm. LATTICE attributes each sample with a difficulty score, before being fed into a training scheduler. The training scheduler samples batches of training data based on these difficulty scores such that learning from easy to difficult data can be performed. To evaluate LATTICE, we use five publicly available datasets collected from five real-world CPS testbeds. We compare LATTICE with ATTAIN and two other state-of-the-art anomaly detectors. Evaluation results show that LATTICE outperforms the three baselines and ATTAIN by 0.906%-2.367% in terms of the F1 score. LATTICE also, on average, reduces the training time of ATTAIN by 4.2% on the five datasets and is on par with the baselines in terms of detection delay time.
</details>
<details>
<summary>摘要</summary>
cyber-physical systems (CPS) 安全需要 anomaly detection (AD)。然而，由于攻击和 CPS 本身的复杂度增加，CPS 中的 AD 变得更加困难。我们在前一项工作中提出了一种基于数字孪生的 AD 方法，称为 ATTAIN，它利用 CPS 的历史和实时数据。然而，这些数据很Difficult to vary significantly.因此，类似于人类学习过程，深度学习模型（例如 ATTAIN）可以从易到困难的学习顺序中受益。为此，在这篇论文中，我们提出了一种 noval 方法， named digitaL twin-based Anomaly deTecTion wIth Curriculum lEarning (LATTICE)，它扩展 ATTAIN 而添加了学习课程。LATTICE 为每个样本分配了一个difficulty score，然后通过一个训练调度器将它们 feed into 训练。训练调度器根据这些difficulty score sampling batches of training data，以便从易到困难的数据进行学习。为了评估 LATTICE，我们使用了五个公开可用的实验室测试床上的数据集。我们将 LATTICE 与 ATTAIN 和两个状态方法进行比较。评估结果显示，LATTICE 在 F1 分数上与三个基准值相比提高了0.906%-2.367%，而且在五个数据集上的训练时间中，LATTICE 平均降低了 ATTAIN 的训练时间4.2%。此外，LATTICE 与基准值相比在检测延迟时间方面具有相同的性能。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Based-Analytics-for-the-Significance-of-Gait-Analysis-in-Monitoring-and-Managing-Lower-Extremity-Injuries"><a href="#Machine-Learning-Based-Analytics-for-the-Significance-of-Gait-Analysis-in-Monitoring-and-Managing-Lower-Extremity-Injuries" class="headerlink" title="Machine Learning Based Analytics for the Significance of Gait Analysis in Monitoring and Managing Lower Extremity Injuries"></a>Machine Learning Based Analytics for the Significance of Gait Analysis in Monitoring and Managing Lower Extremity Injuries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15990">http://arxiv.org/abs/2309.15990</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mostafa Rezapour, Rachel B. Seymour, Stephen H. Sims, Madhav A. Karunakar, Nahir Habet, Metin Nafi Gurcan</li>
<li>for: 这项研究旨在利用步态分析来评估骨折患者后果，如感染、萎缩或设备侵袋。</li>
<li>methods: 研究使用了监督式机器学习模型预测后果，使用连续步态数据集。患者在学院中接受了骨折治疗，并进行了胸部绑定式IMU设备进行步态分析。 Raw 步态数据使用软件进行 pré-processing，强调12个关键步态变量。</li>
<li>results: 研究发现XGBoost模型在训练、测试和评估中表现最佳，并且通过SMOTE处理Class imbalance问题得到了改善。在评估前和评估后应用SMOTE后，XGBoost模型在测试AUC和测试精度方面均达到了最高水平。<details>
<summary>Abstract</summary>
This study explored the potential of gait analysis as a tool for assessing post-injury complications, e.g., infection, malunion, or hardware irritation, in patients with lower extremity fractures. The research focused on the proficiency of supervised machine learning models predicting complications using consecutive gait datasets. We identified patients with lower extremity fractures at an academic center. Patients underwent gait analysis with a chest-mounted IMU device. Using software, raw gait data was preprocessed, emphasizing 12 essential gait variables. Machine learning models including XGBoost, Logistic Regression, SVM, LightGBM, and Random Forest were trained, tested, and evaluated. Attention was given to class imbalance, addressed using SMOTE. We introduced a methodology to compute the Rate of Change (ROC) for gait variables, independent of the time difference between gait analyses. XGBoost was the optimal model both before and after applying SMOTE. Prior to SMOTE, the model achieved an average test AUC of 0.90 (95% CI: [0.79, 1.00]) and test accuracy of 86% (95% CI: [75%, 97%]). Feature importance analysis attributed importance to the duration between injury and gait analysis. Data patterns showed early physiological compensations, followed by stabilization phases, emphasizing prompt gait analysis. This study underscores the potential of machine learning, particularly XGBoost, in gait analysis for orthopedic care. Predicting post-injury complications, early gait assessment becomes vital, revealing intervention points. The findings support a shift in orthopedics towards a data-informed approach, enhancing patient outcomes.
</details>
<details>
<summary>摘要</summary>
We identified patients with lower extremity fractures at an academic center and had them undergo gait analysis with a chest-mounted IMU device. Using software, we preprocessed the raw gait data, emphasizing 12 essential gait variables. We trained, tested, and evaluated machine learning models including XGBoost, Logistic Regression, SVM, LightGBM, and Random Forest. We addressed class imbalance using SMOTE.We introduced a methodology to compute the Rate of Change (ROC) for gait variables, independent of the time difference between gait analyses. XGBoost was the optimal model both before and after applying SMOTE. Prior to SMOTE, the model achieved an average test AUC of 0.90 (95% CI: [0.79, 1.00]) and test accuracy of 86% (95% CI: [75%, 97%]). Feature importance analysis attributed importance to the duration between injury and gait analysis.Data patterns showed early physiological compensations, followed by stabilization phases, emphasizing the importance of prompt gait analysis. This study underscores the potential of machine learning, particularly XGBoost, in gait analysis for orthopedic care. Predicting post-injury complications early on becomes vital, revealing intervention points. The findings support a shift in orthopedics towards a data-informed approach, enhancing patient outcomes.
</details></li>
</ul>
<hr>
<h2 id="Open-Source-Infrastructure-for-Differentiable-Density-Functional-Theory"><a href="#Open-Source-Infrastructure-for-Differentiable-Density-Functional-Theory" class="headerlink" title="Open Source Infrastructure for Differentiable Density Functional Theory"></a>Open Source Infrastructure for Differentiable Density Functional Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15985">http://arxiv.org/abs/2309.15985</a></li>
<li>repo_url: None</li>
<li>paper_authors: Advika Vidhyadhiraja, Arun Pa Thiagarajan, Shang Zhu, Venkat Viswanathan, Bharath Ramsundar</li>
<li>for: 用于训练量子化学计算中的exchange correlation函数。</li>
<li>methods: 使用开源基础设施和多组State-of-the-art技术来标准化处理管道。</li>
<li>results: 开发了一个可 diferenciable quantum chemistry方法，并将其分布在DeepChem库中，以便进一步研究。<details>
<summary>Abstract</summary>
Learning exchange correlation functionals, used in quantum chemistry calculations, from data has become increasingly important in recent years, but training such a functional requires sophisticated software infrastructure. For this reason, we build open source infrastructure to train neural exchange correlation functionals. We aim to standardize the processing pipeline by adapting state-of-the-art techniques from work done by multiple groups. We have open sourced the model in the DeepChem library to provide a platform for additional research on differentiable quantum chemistry methods.
</details>
<details>
<summary>摘要</summary>
学习交换相关函数，用于量子化学计算，从数据中获得信息已经在过去几年变得越来越重要。但是训练这种函数需要复杂的软件基础设施。为此，我们建立了开源基础设施，用于训练神经网络交换相关函数。我们目标是标准化处理管道，采用多个组织的state-of-the-art技术。我们在DeepChem库中打包了模型，以提供更多的研究 differentiable量子化学方法的平台。
</details></li>
</ul>
<hr>
<h2 id="TraCE-Trajectory-Counterfactual-Explanation-Scores"><a href="#TraCE-Trajectory-Counterfactual-Explanation-Scores" class="headerlink" title="TraCE: Trajectory Counterfactual Explanation Scores"></a>TraCE: Trajectory Counterfactual Explanation Scores</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15965">http://arxiv.org/abs/2309.15965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeffrey N. Clark, Edward A. Small, Nawid Keshtmand, Michelle W. L. Wan, Elena Fillola Mayoral, Enrico Werner, Christopher P. Bourdeaux, Raul Santos-Rodriguez</li>
<li>for: 这个论文旨在扩展对黑盒分类器预测的 counterfactual 解释，以便更好地理解和解释在следова决策任务中的进步。</li>
<li>methods: 该论文提出了一种模型无关的套件方法，名为 TraCE（轨迹counterfactual解释），可以将高度复杂的情况简化成一个值。</li>
<li>results: 在两个案例研究中，TraCE 能够成功地捕捉并概括在医疗和气候变化等领域中的进步。<details>
<summary>Abstract</summary>
Counterfactual explanations, and their associated algorithmic recourse, are typically leveraged to understand, explain, and potentially alter a prediction coming from a black-box classifier. In this paper, we propose to extend the use of counterfactuals to evaluate progress in sequential decision making tasks. To this end, we introduce a model-agnostic modular framework, TraCE (Trajectory Counterfactual Explanation) scores, which is able to distill and condense progress in highly complex scenarios into a single value. We demonstrate TraCE's utility across domains by showcasing its main properties in two case studies spanning healthcare and climate change.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>通常情况下，Counterfactual解释和其相关的算法救济是用来理解、解释和可能修改来自黑盒分类器的预测。在这篇论文中，我们提议将Counterfactuals用于评估流程决策任务的进步。为此，我们提出了一种无关于模型的模块化框架，名为TraCE（轨迹Counterfactual解释）分数，可以将高度复杂的情况缩减到单个值。我们在两个案例中展示了TraCE的使用效果，其中一个是医疗领域，另一个是气候变化领域。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Self-Supervised-Contrastive-Learning-of-Spatial-Sound-Event-Representation"><a href="#Exploring-Self-Supervised-Contrastive-Learning-of-Spatial-Sound-Event-Representation" class="headerlink" title="Exploring Self-Supervised Contrastive Learning of Spatial Sound Event Representation"></a>Exploring Self-Supervised Contrastive Learning of Spatial Sound Event Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15938">http://arxiv.org/abs/2309.15938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xilin Jiang, Cong Han, Yinghao Aaron Li, Nima Mesgarani</li>
<li>For: 本研究提出了一种简单的多通道框架（MC-SimCLR），用于编码空间声音的“what”和“where”。MC-SimCLR通过不supervised学习，从空间声音中学习 JOINT spectral和空间表示，从而提高下游任务中的事件分类和声音定位。* Methods: 我们提出了一种多级数据增强管道，用于增强不同级别的声音特征，包括波形、Mel幅gram和通用相关函数（GCC）特征。此外，我们引入了简单 yet effective的通道 wise增强方法，包括随机将 Mikrophone 的顺序交换和 Mel、GCC 通道遮盖。* Results: 我们发现，使用这些增强方法后，linear层在 learned 表示上得到了显著改进，以至于超过supervised模型在事件分类精度和定位误差方面的表现。此外，我们还进行了对每种增强方法的影响分析和不同量的标注数据 fine-tuning 性能的比较。<details>
<summary>Abstract</summary>
In this study, we present a simple multi-channel framework for contrastive learning (MC-SimCLR) to encode 'what' and 'where' of spatial audios. MC-SimCLR learns joint spectral and spatial representations from unlabeled spatial audios, thereby enhancing both event classification and sound localization in downstream tasks. At its core, we propose a multi-level data augmentation pipeline that augments different levels of audio features, including waveforms, Mel spectrograms, and generalized cross-correlation (GCC) features. In addition, we introduce simple yet effective channel-wise augmentation methods to randomly swap the order of the microphones and mask Mel and GCC channels. By using these augmentations, we find that linear layers on top of the learned representation significantly outperform supervised models in terms of both event classification accuracy and localization error. We also perform a comprehensive analysis of the effect of each augmentation method and a comparison of the fine-tuning performance using different amounts of labeled data.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们提出了一种简单的多通道框架 для对比学习（MC-SimCLR），用于编码空间声音中的“what”和“where”。MC-SimCLR通过不supervised学习，从未标注的空间声音中学习联合spectral和空间表示，从而提高下游任务中的事件分类和声音定位精度。我们的核心提案是一种多级数据增强管道，该管道在不同级别上增强不同类型的声音特征，包括波形、Mel spectrogram和通用相关函数（GCC）特征。此外，我们还引入了简单 yet effective的通道 wise增强方法， randomly swap microphone的顺序和隐藏Mel和GCC通道。我们发现，通过使用这些增强方法，linear layer在学习后的表示 significantly outperform supervised模型，并且我们也进行了对每种增强方法的影响分析以及不同量的标注数据 fine-tuning性能的比较。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Based-Real-Time-Rate-Control-for-Live-Streaming-on-Wireless-Networks"><a href="#Deep-Learning-Based-Real-Time-Rate-Control-for-Live-Streaming-on-Wireless-Networks" class="headerlink" title="Deep Learning-Based Real-Time Rate Control for Live Streaming on Wireless Networks"></a>Deep Learning-Based Real-Time Rate Control for Live Streaming on Wireless Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06857">http://arxiv.org/abs/2310.06857</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matin Mortaheb, Mohammad A. Amir Khojastepour, Srimat T. Chakradhar, Sennur Ulukus</li>
<li>for: 提供无线用户高质量视频内容已成为非常重要，但确保视频质量一致却面临变化的编码比特率和无线频谱干扰的挑战。</li>
<li>methods: 提议使用实时深度学习基于H.264控制器，利用物理层的实时频率质量数据和视频块来动态估算最佳编码参数，以避免视频质量损失和 packet loss 引起的artefacts。</li>
<li>results: 实验结果表明，相比之前的适应比特率视频流传输技术，该方法可以获得10-20 dB的PSNR提升，并且平均包drop rate只有0.002。<details>
<summary>Abstract</summary>
Providing wireless users with high-quality video content has become increasingly important. However, ensuring consistent video quality poses challenges due to variable encoded bitrate caused by dynamic video content and fluctuating channel bitrate caused by wireless fading effects. Suboptimal selection of encoder parameters can lead to video quality loss due to underutilized bandwidth or the introduction of video artifacts due to packet loss. To address this, a real-time deep learning based H.264 controller is proposed. This controller leverages instantaneous channel quality data driven from the physical layer, along with the video chunk, to dynamically estimate the optimal encoder parameters with a negligible delay in real-time. The objective is to maintain an encoded video bitrate slightly below the available channel bitrate. Experimental results, conducted on both QCIF dataset and a diverse selection of random videos from public datasets, validate the effectiveness of the approach. Remarkably, improvements of 10-20 dB in PSNR with repect to the state-of-the-art adaptive bitrate video streaming is achieved, with an average packet drop rate as low as 0.002.
</details>
<details>
<summary>摘要</summary>
提供无线用户高质量视频内容已成为非常重要。然而，保证视频质量的一致带来了变量编码比特率的挑战，这是因为动态视频内容引起的编码比特率变化以及无线抖动效果引起的通道比特率波动。不佳选择编码参数可能会导致视频质量损失， Either due to underutilized bandwidth or the introduction of video artifacts due to packet loss. To address this, a real-time deep learning based H.264 controller is proposed. This controller leverages instantaneous channel quality data driven from the physical layer, along with the video chunk, to dynamically estimate the optimal encoder parameters with a negligible delay in real-time. The objective is to maintain an encoded video bitrate slightly below the available channel bitrate. Experimental results, conducted on both QCIF dataset and a diverse selection of random videos from public datasets, validate the effectiveness of the approach. Remarkably, improvements of 10-20 dB in PSNR with respect to the state-of-the-art adaptive bitrate video streaming are achieved, with an average packet drop rate as low as 0.002.Here's the translation in Traditional Chinese:提供无线用户高质量影片内容已成为非常重要。然而，保证影片质量的一致带来了变量编码比特率的挑战，这是因为动态影片内容引起的编码比特率变化以及无线抖动效果引起的通道比特率波动。不佳选择编码参数可能会导致影片质量损失， Either due to underutilized bandwidth or the introduction of video artifacts due to packet loss. To address this, a real-time deep learning based H.264 controller is proposed. This controller leverages instantaneous channel quality data driven from the physical layer, along with the video chunk, to dynamically estimate the optimal encoder parameters with a negligible delay in real-time. The objective is to maintain an encoded video bitrate slightly below the available channel bitrate. Experimental results, conducted on both QCIF dataset and a diverse selection of random videos from public datasets, validate the effectiveness of the approach. Remarkably, improvements of 10-20 dB in PSNR with respect to the state-of-the-art adaptive bitrate video streaming are achieved, with an average packet drop rate as low as 0.002.
</details></li>
</ul>
<hr>
<h2 id="Multi-unit-soft-sensing-permits-few-shot-learning"><a href="#Multi-unit-soft-sensing-permits-few-shot-learning" class="headerlink" title="Multi-unit soft sensing permits few-shot learning"></a>Multi-unit soft sensing permits few-shot learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15828">http://arxiv.org/abs/2309.15828</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bjarne Grimstad, Kristian Løvland, Lars S. Imsland</li>
<li>for: 本研究探讨了使用学习算法实现软感知器的提升。具体来说，通过解决多个任务来提高软感知器的性能。</li>
<li>methods: 本文使用了深度神经网络实现多单元软感知器，并 investigate了该模型在不同单元数量下的学习能力。</li>
<li>results: 研究发现，当软感知器通过多个任务学习时，它可以具有很好的泛化能力，并且在新单元上进行几个数据点的少量学习后，可以达到高性能。<details>
<summary>Abstract</summary>
Recent literature has explored various ways to improve soft sensors using learning algorithms with transferability. Broadly put, the performance of a soft sensor may be strengthened when it is learned by solving multiple tasks. The usefulness of transferability depends on how strongly related the devised learning tasks are. A particularly relevant case for transferability, is when a soft sensor is to be developed for a process of which there are many realizations, e.g. system or device with many implementations from which data is available. Then, each realization presents a soft sensor learning task, and it is reasonable to expect that the different tasks are strongly related. Applying transferability in this setting leads to what we call multi-unit soft sensing, where a soft sensor models a process by learning from data from all of its realizations.   This paper explores the learning abilities of a multi-unit soft sensor, which is formulated as a hierarchical model and implemented using a deep neural network. In particular, we investigate how well the soft sensor generalizes as the number of units increase. Using a large industrial dataset, we demonstrate that, when the soft sensor is learned from a sufficient number of tasks, it permits few-shot learning on data from new units. Surprisingly, regarding the difficulty of the task, few-shot learning on 1-3 data points often leads to a high performance on new units.
</details>
<details>
<summary>摘要</summary>
This paper investigates the learning abilities of a multi-unit soft sensor, which is formulated as a hierarchical model and implemented using a deep neural network. Specifically, we examine how well the soft sensor generalizes as the number of units increases. Using a large industrial dataset, we show that when the soft sensor is learned from a sufficient number of tasks, it permits few-shot learning on data from new units. Surprisingly, even with just a few data points, the soft sensor can achieve high performance on new units.
</details></li>
</ul>
<hr>
<h2 id="Fair-Canonical-Correlation-Analysis"><a href="#Fair-Canonical-Correlation-Analysis" class="headerlink" title="Fair Canonical Correlation Analysis"></a>Fair Canonical Correlation Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15809">http://arxiv.org/abs/2309.15809</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pennshenlab/fair_cca">https://github.com/pennshenlab/fair_cca</a></li>
<li>paper_authors: Zhuoping Zhou, Davoud Ataee Tarzanagh, Bojian Hou, Boning Tong, Jia Xu, Yanbo Feng, Qi Long, Li Shen</li>
<li>for: 这个论文探讨了干净 correlation analysis (CCA) 中的公平性和偏见问题，并提出了一种框架来减少保护特征相关的偏见错误。</li>
<li>methods: 该论文使用了一种方法来学习全数据点上的全球投影矩阵，并确保这些矩阵在各个组中具有相同的相关水平。</li>
<li>results: 实验表明，该方法可以减少偏见错误而不影响 CCA 的准确性。<details>
<summary>Abstract</summary>
This paper investigates fairness and bias in Canonical Correlation Analysis (CCA), a widely used statistical technique for examining the relationship between two sets of variables. We present a framework that alleviates unfairness by minimizing the correlation disparity error associated with protected attributes. Our approach enables CCA to learn global projection matrices from all data points while ensuring that these matrices yield comparable correlation levels to group-specific projection matrices. Experimental evaluation on both synthetic and real-world datasets demonstrates the efficacy of our method in reducing correlation disparity error without compromising CCA accuracy.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Node-Aligned-Graph-to-Graph-Generation-for-Retrosynthesis-Prediction"><a href="#Node-Aligned-Graph-to-Graph-Generation-for-Retrosynthesis-Prediction" class="headerlink" title="Node-Aligned Graph-to-Graph Generation for Retrosynthesis Prediction"></a>Node-Aligned Graph-to-Graph Generation for Retrosynthesis Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15798">http://arxiv.org/abs/2309.15798</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lin Yao, Zhen Wang, Wentao Guo, Shang Xiang, Wentan Liu, Guolin Ke</li>
<li>For: The paper aims to develop a template-free machine learning model for single-step retrosynthesis, which can fully leverage the topological information of the molecule and align atoms between the product and reactants.* Methods: The proposed method, NAG2G, uses 2D molecular graphs and 3D conformation information, and incorporates node alignment to determine a specific order for node generation. The method generates molecular graphs in an auto-regressive manner, ensuring that the node generation order coincides with the node order in the input graph.* Results: The proposed NAG2G method outperforms previous state-of-the-art baselines in various metrics, demonstrating its effectiveness in single-step retrosynthesis.<details>
<summary>Abstract</summary>
Single-step retrosynthesis is a crucial task in organic chemistry and drug design, requiring the identification of required reactants to synthesize a specific compound. with the advent of computer-aided synthesis planning, there is growing interest in using machine-learning techniques to facilitate the process. Existing template-free machine learning-based models typically utilize transformer structures and represent molecules as ID sequences. However, these methods often face challenges in fully leveraging the extensive topological information of the molecule and aligning atoms between the production and reactants, leading to results that are not as competitive as those of semi-template models. Our proposed method, Node-Aligned Graph-to-Graph (NAG2G), also serves as a transformer-based template-free model but utilizes 2D molecular graphs and 3D conformation information. Furthermore, our approach simplifies the incorporation of production-reactant atom mapping alignment by leveraging node alignment to determine a specific order for node generation and generating molecular graphs in an auto-regressive manner node-by-node. This method ensures that the node generation order coincides with the node order in the input graph, overcoming the difficulty of determining a specific node generation order in an auto-regressive manner. Our extensive benchmarking results demonstrate that the proposed NAG2G can outperform the previous state-of-the-art baselines in various metrics.
</details>
<details>
<summary>摘要</summary>
单步反synthesis是有机化学中的一项重要任务，它需要确定用于合成特定化合物的反应物。随着计算机支持的合成规划的出现，有关机器学习技术的应用在这一领域中受到越来越多的关注。现有的模板缺失机器学习模型通常采用变换器结构，并将分子表示为ID序列。然而，这些方法经常遇到利用分子的广泛顺序信息和生成物和反应物之间的原子对齐的问题，从而导致结果与半模板模型相比较差。我们提出的方法Node-Aligned Graph-to-Graph（NAG2G）是一种基于变换器的模板缺失模型，它使用二维分子图和三维结构信息。此外，我们的方法简化了生产反应物原子对齐的含义，通过节点对齐确定生成节点顺序，并在自然顺序下生成分子图。这种方法确保了生成节点顺序与输入图中节点顺序一致，从而解决了在自然顺序下生成节点顺序的困难。我们的广泛测试结果表明，提议的NAG2G可以在多种维度上超越先前的基准值。
</details></li>
</ul>
<hr>
<h2 id="Learning-the-Efficient-Frontier"><a href="#Learning-the-Efficient-Frontier" class="headerlink" title="Learning the Efficient Frontier"></a>Learning the Efficient Frontier</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15775">http://arxiv.org/abs/2309.15775</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sugoto/Algorithmic-Trading-Using-Unsupervised-Learning">https://github.com/Sugoto/Algorithmic-Trading-Using-Unsupervised-Learning</a></li>
<li>paper_authors: Philippe Chatigny, Ivan Sergienko, Ryan Ferguson, Jordan Weir, Maxime Bergeron</li>
<li>for: 这篇论文是用于解决资源分配问题，寻找一个最佳投资组合，以 maximize 回报，同时遵循一定的风险水平。</li>
<li>methods: 这篇论文使用了人工神经网络来快速地预测资源分配问题的解决方案，并可以处理不规律的行为和变数数量的变化。</li>
<li>results: 这篇论文显示了NeuralEF可以快速地预测资源分配问题的解决方案，并且可以处理大规模的 simulations，并且可以适应不同的线性限制和服务器数量的变化。<details>
<summary>Abstract</summary>
The efficient frontier (EF) is a fundamental resource allocation problem where one has to find an optimal portfolio maximizing a reward at a given level of risk. This optimal solution is traditionally found by solving a convex optimization problem. In this paper, we introduce NeuralEF: a fast neural approximation framework that robustly forecasts the result of the EF convex optimization problem with respect to heterogeneous linear constraints and variable number of optimization inputs. By reformulating an optimization problem as a sequence to sequence problem, we show that NeuralEF is a viable solution to accelerate large-scale simulation while handling discontinuous behavior.
</details>
<details>
<summary>摘要</summary>
efficient frontier（EF）是一个基本资源分配问题，它的目标是找到一个最佳投资组合，以最大化奖励，同时保持给定的风险水平。传统上，这个问题通过解 convex 优化问题来解决。在这篇论文中，我们介绍了 NeuralEF：一种快速神经网络近似框架，可以快速和稳定地预测 EF 优化问题中的解，对于不同类型的线性约束和变量数量的优化输入。通过将优化问题转化为一个序列到序列问题，我们示出了 NeuralEF 可以快速加速大规模的模拟，同时处理不连续行为。
</details></li>
</ul>
<hr>
<h2 id="Importance-Weighted-Offline-Learning-Done-Right"><a href="#Importance-Weighted-Offline-Learning-Done-Right" class="headerlink" title="Importance-Weighted Offline Learning Done Right"></a>Importance-Weighted Offline Learning Done Right</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15771">http://arxiv.org/abs/2309.15771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Germano Gabbianelli, Gergely Neu, Matteo Papini</li>
<li>for: 学习在停机环境下的策略优化问题，目标是根据决策数据集学习一个近似优化策略，而不假设奖励函数的任何结构。</li>
<li>methods: 使用 importancce-weighted 估计器来估计每个策略的价值，并选择一个最小化估计值的策略，并且附加一个 “悲观” 调整以减少估计值的随机变化。</li>
<li>results: 比前一些研究更好地实现性能，包括： eliminating a highly restrictive “uniform coverage” assumption， 并且在无穷策略类中进行 PAC-Bayesian 扩展，以及通过数学仿真表明算法对参数选择的Robustness。<details>
<summary>Abstract</summary>
We study the problem of offline policy optimization in stochastic contextual bandit problems, where the goal is to learn a near-optimal policy based on a dataset of decision data collected by a suboptimal behavior policy. Rather than making any structural assumptions on the reward function, we assume access to a given policy class and aim to compete with the best comparator policy within this class. In this setting, a standard approach is to compute importance-weighted estimators of the value of each policy, and select a policy that minimizes the estimated value up to a "pessimistic" adjustment subtracted from the estimates to reduce their random fluctuations. In this paper, we show that a simple alternative approach based on the "implicit exploration" estimator of \citet{Neu2015} yields performance guarantees that are superior in nearly all possible terms to all previous results. Most notably, we remove an extremely restrictive "uniform coverage" assumption made in all previous works. These improvements are made possible by the observation that the upper and lower tails importance-weighted estimators behave very differently from each other, and their careful control can massively improve on previous results that were all based on symmetric two-sided concentration inequalities. We also extend our results to infinite policy classes in a PAC-Bayesian fashion, and showcase the robustness of our algorithm to the choice of hyper-parameters by means of numerical simulations.
</details>
<details>
<summary>摘要</summary>
我们研究线上策略优化问题在随机上下文ual bandit问题中，目标是通过一个决策数据集，收集的一个差本策略来学习一个近似优化策略。而不是对奖励函数进行任何结构假设，我们假设可以访问一个给定策略类型，并且目标是在这个类型中竞争最佳比较策略。在这个设定下，标准的方法是计算重要性权重的估计值，并选择一个优化这些估计值的策略。在这篇论文中，我们表明了一种简单的代替方法，基于\citet{Neu2015}提出的"隐式探索"估计器，可以在大多数可能的情况下超越所有之前的结果。我们除去了所有前一 Works中的极其限制性"同质覆盖"假设，这些改进是通过观察重要性权重估计器的上下文不同，以及其精细控制来实现。我们还将结果扩展到无穷策略类型的PAC-Bayesian方式，并通过数据 simulate 显示了我们的算法对参数选择的robustness。
</details></li>
</ul>
<hr>
<h2 id="Algebraic-and-Statistical-Properties-of-the-Ordinary-Least-Squares-Interpolator"><a href="#Algebraic-and-Statistical-Properties-of-the-Ordinary-Least-Squares-Interpolator" class="headerlink" title="Algebraic and Statistical Properties of the Ordinary Least Squares Interpolator"></a>Algebraic and Statistical Properties of the Ordinary Least Squares Interpolator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15769">http://arxiv.org/abs/2309.15769</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deshen24/ols_interpolator">https://github.com/deshen24/ols_interpolator</a></li>
<li>paper_authors: Dennis Shen, Dogyoon Song, Peng Ding, Jasjeet S. Sekhon</li>
<li>for: 本研究探讨了使用最小二乘法（OLS） interpolator在高维场景下的性能，以了解这种情况下的泛化能力和 causal inference 的应用。</li>
<li>methods: 本研究使用了高维代数和统计方法，包括留下-$k$-out residual公式、Cochran的公式和 Frisch-Waugh-Lovell 定理，以探讨 OLS interpolator 在高维场景下的性能。</li>
<li>results: 研究发现，在高维场景下，OLS interpolator 的泛化能力受到一定的限制，但可以通过使用高维代数和统计方法来更好地理解和优化它的性能。<details>
<summary>Abstract</summary>
Deep learning research has uncovered the phenomenon of benign overfitting for over-parameterized statistical models, which has drawn significant theoretical interest in recent years. Given its simplicity and practicality, the ordinary least squares (OLS) interpolator has become essential to gain foundational insights into this phenomenon. While properties of OLS are well established in classical settings, its behavior in high-dimensional settings is less explored (unlike for ridge or lasso regression) though significant progress has been made of late. We contribute to this growing literature by providing fundamental algebraic and statistical results for the minimum $\ell_2$-norm OLS interpolator. In particular, we provide high-dimensional algebraic equivalents of (i) the leave-$k$-out residual formula, (ii) Cochran's formula, and (iii) the Frisch-Waugh-Lovell theorem. These results aid in understanding the OLS interpolator's ability to generalize and have substantive implications for causal inference. Additionally, under the Gauss-Markov model, we present statistical results such as a high-dimensional extension of the Gauss-Markov theorem and an analysis of variance estimation under homoskedastic errors. To substantiate our theoretical contributions, we conduct simulation studies that further explore the stochastic properties of the OLS interpolator.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Provably-Efficient-Exploration-in-Constrained-Reinforcement-Learning-Posterior-Sampling-Is-All-You-Need"><a href="#Provably-Efficient-Exploration-in-Constrained-Reinforcement-Learning-Posterior-Sampling-Is-All-You-Need" class="headerlink" title="Provably Efficient Exploration in Constrained Reinforcement Learning:Posterior Sampling Is All You Need"></a>Provably Efficient Exploration in Constrained Reinforcement Learning:Posterior Sampling Is All You Need</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15737">http://arxiv.org/abs/2309.15737</a></li>
<li>repo_url: None</li>
<li>paper_authors: Danil Provodin, Pratik Gajane, Mykola Pechenizkiy, Maurits Kaptein</li>
<li>for: 这个论文是为了学习受限Markov决策过程（CMDP）中的搜索算法。</li>
<li>methods: 这个论文使用 posterior sampling 算法，并且实际上比现有的算法更有利。</li>
<li>results: 论文的主要理论结果是一个 Bayesian regret bound，表明在任何交流CMDP中，这个算法的误差 bound 是 O(HS√AT)，与时间 horizion T 相对应。这个 regret bound与下界匹配，并且在无限预算下的设置中是最好的知道的 regret bound。实际结果表明，尽管这个算法简单，但它仍然在受限激励学习中超过现有算法。<details>
<summary>Abstract</summary>
We present a new algorithm based on posterior sampling for learning in constrained Markov decision processes (CMDP) in the infinite-horizon undiscounted setting. The algorithm achieves near-optimal regret bounds while being advantageous empirically compared to the existing algorithms. Our main theoretical result is a Bayesian regret bound for each cost component of \tilde{O} (HS \sqrt{AT}) for any communicating CMDP with S states, A actions, and bound on the hitting time H. This regret bound matches the lower bound in order of time horizon T and is the best-known regret bound for communicating CMDPs in the infinite-horizon undiscounted setting. Empirical results show that, despite its simplicity, our posterior sampling algorithm outperforms the existing algorithms for constrained reinforcement learning.
</details>
<details>
<summary>摘要</summary>
我们提出了一种基于 posterior sampling 的新算法，用于在受限制的 Markov 决策过程（CMDP）中学习，在无限远景下无折扣设定下。该算法可以达到近似优化的尊贵 regret  bound，而且在实际中比现有算法更有利。我们的主要理论结果是一个 Bayesian regret bound，其中每个成本组件的 regret  bound为 O(HS√AT)，对于任何交流 CMDP 来说。这个 regret bound 与时间轴 T 的下界相同，是无限远景下无折扣 CMDP 中最佳知道的 regret bound。实验结果表明，尽管我们的 posterior sampling 算法相对简单，但它在受限制的 reinforcement learning 中仍然超越了现有算法。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-based-Analysis-of-Basins-of-Attraction"><a href="#Deep-Learning-based-Analysis-of-Basins-of-Attraction" class="headerlink" title="Deep Learning-based Analysis of Basins of Attraction"></a>Deep Learning-based Analysis of Basins of Attraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15732">http://arxiv.org/abs/2309.15732</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/redlynx96/deep-learning-based-analysis-of-basins-of-attraction">https://github.com/redlynx96/deep-learning-based-analysis-of-basins-of-attraction</a></li>
<li>paper_authors: David Valle, Alexandre Wagemakers, Miguel A. F. Sanjuán</li>
<li>for: 这个研究用 convolutional neural networks (CNNs) 来Characterize 不同动力系统的抽象基in的复杂性和随机性。</li>
<li>methods: 这种新方法可以高效地Explore 不同动力系统的参数，因为传统的方法 computationally expensive  для Characterize 多个抽象基in。</li>
<li>results: 该研究包括对不同 CNN 架构的比较，显示我们提议的 Characterization 方法在与传统方法相比，即使使用过时的架构也表现出优异性。<details>
<summary>Abstract</summary>
This study showcases the effectiveness of convolutional neural networks (CNNs) in characterizing the complexity and unpredictability of basins of attraction for diverse dynamical systems. This novel method is optimal for exploring different parameters of dynamical systems since the conventional methods are computationally expensive for characterizing multiple basins of attraction. Additionally, our research includes a comparison of different CNN architectures for this task showing the superiority of our proposed characterization method over the conventional methods, even with obsolete architectures.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这个研究显示了卷积神经网络 (CNNs) 在描述不同动力系统的基域抓取 Complexity and unpredictability of basins of attraction 的效果。这种新方法可以高效地探索不同动力系统的参数，因为传统方法计算多个基域抓取的成本是非常高昂的。此外，我们的研究还比较了不同的 CNN 架构，显示我们提出的 caracterization 方法比传统方法更加高效，即使使用过时的架构也可以达到更高的性能。
</details></li>
</ul>
<hr>
<h2 id="Temporal-graph-models-fail-to-capture-global-temporal-dynamics"><a href="#Temporal-graph-models-fail-to-capture-global-temporal-dynamics" class="headerlink" title="Temporal graph models fail to capture global temporal dynamics"></a>Temporal graph models fail to capture global temporal dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15730">http://arxiv.org/abs/2309.15730</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/temporal-graphs-negative-sampling/tgb">https://github.com/temporal-graphs-negative-sampling/tgb</a></li>
<li>paper_authors: Michał Daniluk, Jacek Dąbrowski</li>
<li>for: 这个论文主要针对的是动态图模型的预测问题，尤其是在具有强大全球动态的 datasets 上。</li>
<li>methods: 该论文使用了一种简单的优化策略，即 “最近受欢迎的节点” 的方法，并提出了两种基于沃asserstein距离的度量方法来衡量数据集的短期和长期全球动态强度。</li>
<li>results: 研究发现，使用这种简单的优化策略可以在中等和大型数据集上超过其他方法的性能，而且标准的负样本评估方法可能不适用于具有强大全球动态的数据集，可能导致模型培养过程中的模型恶化和训练过程中的模型降解。<details>
<summary>Abstract</summary>
A recently released Temporal Graph Benchmark is analyzed in the context of Dynamic Link Property Prediction. We outline our observations and propose a trivial optimization-free baseline of "recently popular nodes" outperforming other methods on medium and large-size datasets in the Temporal Graph Benchmark. We propose two measures based on Wasserstein distance which can quantify the strength of short-term and long-term global dynamics of datasets. By analyzing our unexpectedly strong baseline, we show how standard negative sampling evaluation can be unsuitable for datasets with strong temporal dynamics. We also show how simple negative-sampling can lead to model degeneration during training, resulting in impossible to rank, fully saturated predictions of temporal graph networks. We propose improved negative sampling schemes for both training and evaluation and prove their usefulness. We conduct a comparison with a model trained non-contrastively without negative sampling. Our results provide a challenging baseline and indicate that temporal graph network architectures need deep rethinking for usage in problems with significant global dynamics, such as social media, cryptocurrency markets or e-commerce. We open-source the code for baselines, measures and proposed negative sampling schemes.
</details>
<details>
<summary>摘要</summary>
Recently, a Temporal Graph Benchmark was released, and we analyzed it in the context of Dynamic Link Property Prediction. We observed some interesting phenomena and proposed a trivial optimization-free baseline that outperforms other methods on medium and large-size datasets. We also proposed two measures based on Wasserstein distance to quantify the strength of short-term and long-term global dynamics of datasets.However, we found that standard negative sampling evaluation may not be suitable for datasets with strong temporal dynamics, and simple negative-sampling can lead to model degeneration during training, resulting in impossible to rank, fully saturated predictions of temporal graph networks. To address these issues, we proposed improved negative sampling schemes for both training and evaluation and proved their usefulness.We also compared our results with a model trained non-contrastively without negative sampling, and our results provide a challenging baseline. This suggests that temporal graph network architectures need to be rethought for usage in problems with significant global dynamics, such as social media, cryptocurrency markets, or e-commerce. To facilitate further research, we have open-sourced the code for baselines, measures, and proposed negative sampling schemes.
</details></li>
</ul>
<hr>
<h2 id="Timbre-Trap-A-Low-Resource-Framework-for-Instrument-Agnostic-Music-Transcription"><a href="#Timbre-Trap-A-Low-Resource-Framework-for-Instrument-Agnostic-Music-Transcription" class="headerlink" title="Timbre-Trap: A Low-Resource Framework for Instrument-Agnostic Music Transcription"></a>Timbre-Trap: A Low-Resource Framework for Instrument-Agnostic Music Transcription</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15717">http://arxiv.org/abs/2309.15717</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frank Cwitkowitz, Kin Wai Cheuk, Woosung Choi, Marco A. Martínez-Ramírez, Keisuke Toyama, Wei-Hsiang Liao, Yuki Mitsufuji</li>
<li>for: 本研究旨在提高音乐转录的性能，尤其是在低资源任务上。</li>
<li>methods: 本研究提出了一种新的框架，即Timbre-Trap，它将音乐转录和音频重建结合起来，通过利用拟声和时间域的强分离性来提高转录性能。</li>
<li>results: 研究表明，Timbre-Trap框架可以在低数据量情况下达到与现有状态艺术方法相当的转录性能，而不需要大量的注释数据。<details>
<summary>Abstract</summary>
In recent years, research on music transcription has focused mainly on architecture design and instrument-specific data acquisition. With the lack of availability of diverse datasets, progress is often limited to solo-instrument tasks such as piano transcription. Several works have explored multi-instrument transcription as a means to bolster the performance of models on low-resource tasks, but these methods face the same data availability issues. We propose Timbre-Trap, a novel framework which unifies music transcription and audio reconstruction by exploiting the strong separability between pitch and timbre. We train a single U-Net to simultaneously estimate pitch salience and reconstruct complex spectral coefficients, selecting between either output during the decoding stage via a simple switch mechanism. In this way, the model learns to produce coefficients corresponding to timbre-less audio, which can be interpreted as pitch salience. We demonstrate that the framework leads to performance comparable to state-of-the-art instrument-agnostic transcription methods, while only requiring a small amount of annotated data.
</details>
<details>
<summary>摘要</summary>
Recently, music transcription research has focused mainly on architecture design and instrument-specific data acquisition. Due to the lack of diverse datasets, progress has been limited to solo-instrument tasks such as piano transcription. Some works have explored multi-instrument transcription to improve the performance of models on low-resource tasks, but these methods face the same data availability issues. We propose Timbre-Trap, a novel framework that unifies music transcription and audio reconstruction by exploiting the strong separability between pitch and timbre. We train a single U-Net to simultaneously estimate pitch salience and reconstruct complex spectral coefficients, selecting between either output during the decoding stage via a simple switch mechanism. In this way, the model learns to produce coefficients corresponding to timbre-less audio, which can be interpreted as pitch salience. We demonstrate that the framework leads to performance comparable to state-of-the-art instrument-agnostic transcription methods, while only requiring a small amount of annotated data.
</details></li>
</ul>
<hr>
<h2 id="Maximum-Weight-Entropy"><a href="#Maximum-Weight-Entropy" class="headerlink" title="Maximum Weight Entropy"></a>Maximum Weight Entropy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15704">http://arxiv.org/abs/2309.15704</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/antoinedemathelin/openood">https://github.com/antoinedemathelin/openood</a></li>
<li>paper_authors: Antoine de Mathelin, François Deheeger, Mathilde Mougeot, Nicolas Vayatis</li>
<li>for: 这篇论文针对深度学习中的不确定量化和非常数据类型探测使用bayesian和集合方法。</li>
<li>methods: 方法建议一个实用的解决方案，即使标准方法在非常数据类型上运行时会受到过度简化的问题。</li>
<li>results: 方法可以在各种配置下与更多竞争者相比，在一个广泛的非常数据类型测试中排名前三名。<details>
<summary>Abstract</summary>
This paper deals with uncertainty quantification and out-of-distribution detection in deep learning using Bayesian and ensemble methods. It proposes a practical solution to the lack of prediction diversity observed recently for standard approaches when used out-of-distribution (Ovadia et al., 2019; Liu et al., 2021). Considering that this issue is mainly related to a lack of weight diversity, we claim that standard methods sample in "over-restricted" regions of the weight space due to the use of "over-regularization" processes, such as weight decay and zero-mean centered Gaussian priors. We propose to solve the problem by adopting the maximum entropy principle for the weight distribution, with the underlying idea to maximize the weight diversity. Under this paradigm, the epistemic uncertainty is described by the weight distribution of maximal entropy that produces neural networks "consistent" with the training observations. Considering stochastic neural networks, a practical optimization is derived to build such a distribution, defined as a trade-off between the average empirical risk and the weight distribution entropy. We develop a novel weight parameterization for the stochastic model, based on the singular value decomposition of the neural network's hidden representations, which enables a large increase of the weight entropy for a small empirical risk penalization. We provide both theoretical and numerical results to assess the efficiency of the approach. In particular, the proposed algorithm appears in the top three best methods in all configurations of an extensive out-of-distribution detection benchmark including more than thirty competitors.
</details>
<details>
<summary>摘要</summary>
To solve this problem, the authors propose adopting the maximum entropy principle for the weight distribution. This approach aims to maximize the weight diversity, and the epistemic uncertainty is described by the weight distribution of maximal entropy that produces neural networks "consistent" with the training observations. The authors derive a practical optimization to build such a distribution, which is a trade-off between the average empirical risk and the weight distribution entropy.To implement this approach, the authors develop a novel weight parameterization for the stochastic model based on the singular value decomposition of the neural network's hidden representations. This parameterization enables a large increase of the weight entropy for a small empirical risk penalization. The authors provide both theoretical and numerical results to assess the efficiency of the approach. In particular, the proposed algorithm ranks in the top three best methods in all configurations of an extensive out-of-distribution detection benchmark that includes more than thirty competitors.
</details></li>
</ul>
<hr>
<h2 id="Breaking-NoC-Anonymity-using-Flow-Correlation-Attack"><a href="#Breaking-NoC-Anonymity-using-Flow-Correlation-Attack" class="headerlink" title="Breaking NoC Anonymity using Flow Correlation Attack"></a>Breaking NoC Anonymity using Flow Correlation Attack</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15687">http://arxiv.org/abs/2309.15687</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hansika Weerasena, Pan Zhixin, Khushboo Rani, Prabhat Mishra</li>
<li>for: 本研究探讨了 today’s multicore System-on-Chip (SoC) 设计中的内部通信网络（Network-on-Chip，NoC）的安全性。</li>
<li>methods: 本研究使用了现有的匿名路由协议，并使用流量混淆技术来防御机器学习（ML）基于流量相关攻击。</li>
<li>results: 实验结果表明，现有的匿名路由有ML基于流量相关攻击的漏洞，而我们提议的轻量级匿名路由可以防御ML基于攻击，但具有较少的硬件和性能开销。<details>
<summary>Abstract</summary>
Network-on-Chip (NoC) is widely used as the internal communication fabric in today's multicore System-on-Chip (SoC) designs. Security of the on-chip communication is crucial because exploiting any vulnerability in shared NoC would be a goldmine for an attacker. NoC security relies on effective countermeasures against diverse attacks. We investigate the security strength of existing anonymous routing protocols in NoC architectures. Specifically, this paper makes two important contributions. We show that the existing anonymous routing is vulnerable to machine learning (ML) based flow correlation attacks on NoCs. We propose a lightweight anonymous routing that use traffic obfuscation techniques which can defend against ML-based flow correlation attacks. Experimental studies using both real and synthetic traffic reveal that our proposed attack is successful against state-of-the-art anonymous routing in NoC architectures with a high accuracy (up to 99%) for diverse traffic patterns, while our lightweight countermeasure can defend against ML-based attacks with minor hardware and performance overhead.
</details>
<details>
<summary>摘要</summary>
network-on-chip (NoC) 广泛应用于今天的多核系统在一个芯片 (SoC) 设计中作为内部通信织物。NoC 通信安全是关键的，因为任何 NoC 共享的攻击漏洞都会是攻击者的黄金岛。NoC 安全取决于有效地对多种攻击发起countermeasures。我们调查了现有的匿名路由协议在 NoC 架构中的安全强度。特别是，这篇论文有两个重要贡献。我们表明了现有的匿名路由易受到机器学习 (ML) 基于流 corrleation 攻击 NoC 中。我们提议一种轻量级的匿名路由，使用流混淆技术来防御 ML-based 流 corrleation 攻击。实验 Studies 使用了真实和 sintetic 流量，表明我们的提议可以成功地防御现有的匿名路由，并且对多种流量模式具有高精度（达到 99%），而我们的轻量级 countermeasure 可以防御 ML-based 攻击，只需要非常小的硬件和性能开销。
</details></li>
</ul>
<hr>
<h2 id="Projection-based-fuzzy-least-squares-twin-support-vector-machine-for-class-imbalance-problems"><a href="#Projection-based-fuzzy-least-squares-twin-support-vector-machine-for-class-imbalance-problems" class="headerlink" title="Projection based fuzzy least squares twin support vector machine for class imbalance problems"></a>Projection based fuzzy least squares twin support vector machine for class imbalance problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15886">http://arxiv.org/abs/2309.15886</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. Tanveer, Ritik Mishra, Bharat Richhariya</li>
<li>for:  addresses the problem of class imbalance and noisy datasets in real-world classification tasks</li>
<li>methods:  proposes two novel fuzzy-based approaches, IF-RELSTSVM and F-RELSTSVM, which use intuitionistic fuzzy membership and hyperplane-based fuzzy membership, respectively</li>
<li>results:  outperforms baseline algorithms on several benchmark and synthetic datasets, with statistical tests confirming the significance of the proposed algorithms on noisy and imbalanced datasets.Here’s the simplified Chinese version:</li>
<li>for:  solve了现实世界分类任务中的类别不均和噪音数据问题</li>
<li>methods: 提出了两种新的模糊方法，IF-RELSTSVM和F-RELSTSVM，它们使用了Intuitionistic Fuzzy Membership和Hyperplane-based Fuzzy Membership</li>
<li>results: 在多个 benchmark和 sintetic 数据集上比基准算法表现更好，并通过统计测试确认了模糊方法在噪音和类别不均数据集上的可靠性。<details>
<summary>Abstract</summary>
Class imbalance is a major problem in many real world classification tasks. Due to the imbalance in the number of samples, the support vector machine (SVM) classifier gets biased toward the majority class. Furthermore, these samples are often observed with a certain degree of noise. Therefore, to remove these problems we propose a novel fuzzy based approach to deal with class imbalanced as well noisy datasets. We propose two approaches to address these problems. The first approach is based on the intuitionistic fuzzy membership, termed as robust energy-based intuitionistic fuzzy least squares twin support vector machine (IF-RELSTSVM). Furthermore, we introduce the concept of hyperplane-based fuzzy membership in our second approach, where the final classifier is termed as robust energy-based fuzzy least square twin support vector machine (F-RELSTSVM). By using this technique, the membership values are based on a projection based approach, where the data points are projected on the hyperplanes. The performance of the proposed algorithms is evaluated on several benchmark and synthetic datasets. The experimental results show that the proposed IF-RELSTSVM and F-RELSTSVM models outperform the baseline algorithms. Statistical tests are performed to check the significance of the proposed algorithms. The results show the applicability of the proposed algorithms on noisy as well as imbalanced datasets.
</details>
<details>
<summary>摘要</summary>
classe imbalance 是现实世界分类任务中的一个主要问题，由于样本的数量异常分布，支持向量机 (SVM) 分类器会受到主导类的偏向。此外，这些样本经常会受到一定程度的噪声影响。因此，我们提出了一种基于概率的新方法来处理类偏度和噪声问题。我们提出了两种方法来解决这些问题：第一种方法是基于感知度的强制类型，称为稳定能量基于感知度的最小二乘支持向量机 (IF-RELSTSVM)。其次，我们引入了基于抽象平面的辅助分类器，其最终分类器称为稳定能量基于抽象平面的概率最小二乘支持向量机 (F-RELSTSVM)。通过这种技术，分类器的成员值基于一种投影方法，将数据点投影到抽象平面上。我们对一些标准和 sintetic 数据集进行了实验，结果表明，我们提出的 IF-RELSTSVM 和 F-RELSTSVM 模型在基eline算法的基础上具有显著的优势。我们还进行了统计测试，以验证提出的方法的可靠性。结果表明，提出的方法可以在噪声和类偏度问题下得到好的应用。
</details></li>
</ul>
<hr>
<h2 id="Joint-Sampling-and-Optimisation-for-Inverse-Rendering"><a href="#Joint-Sampling-and-Optimisation-for-Inverse-Rendering" class="headerlink" title="Joint Sampling and Optimisation for Inverse Rendering"></a>Joint Sampling and Optimisation for Inverse Rendering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15676">http://arxiv.org/abs/2309.15676</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Balint, Karol Myszkowski, Hans-Peter Seidel, Gurprit Singh</li>
<li>for:  solve difficult inverse problems such as inverse rendering using Monte Carlo estimated gradients</li>
<li>methods: use interleaving sampling and optimisation, update and reuse past samples with low-variance finite-difference estimators, combine proportional and finite-difference samples to continuously reduce variance</li>
<li>results: speed up convergence of optimisation tasks, demonstrate effectiveness in inverse path tracing<details>
<summary>Abstract</summary>
When dealing with difficult inverse problems such as inverse rendering, using Monte Carlo estimated gradients to optimise parameters can slow down convergence due to variance. Averaging many gradient samples in each iteration reduces this variance trivially. However, for problems that require thousands of optimisation iterations, the computational cost of this approach rises quickly.   We derive a theoretical framework for interleaving sampling and optimisation. We update and reuse past samples with low-variance finite-difference estimators that describe the change in the estimated gradients between each iteration. By combining proportional and finite-difference samples, we continuously reduce the variance of our novel gradient meta-estimators throughout the optimisation process. We investigate how our estimator interlinks with Adam and derive a stable combination.   We implement our method for inverse path tracing and demonstrate how our estimator speeds up convergence on difficult optimisation tasks.
</details>
<details>
<summary>摘要</summary>
When dealing with difficult inverse problems such as inverse rendering, using Monte Carlo estimated gradients to optimize parameters can slow down convergence due to variance. Averaging many gradient samples in each iteration reduces this variance trivially. However, for problems that require thousands of optimization iterations, the computational cost of this approach rises quickly.  We derive a theoretical framework for interleaving sampling and optimization. We update and reuse past samples with low-variance finite-difference estimators that describe the change in the estimated gradients between each iteration. By combining proportional and finite-difference samples, we continuously reduce the variance of our novel gradient meta-estimators throughout the optimization process. We investigate how our estimator interlinks with Adam and derive a stable combination.  We implement our method for inverse path tracing and demonstrate how our estimator speeds up convergence on difficult optimization tasks.Here's the text in Traditional Chinese:在解决复杂的 inverse 问题时，如 inverse rendering，使用 Monte Carlo 估计 gradient 来优化参数可能会因为异谱而 slow down convergence。每个迭代中聚合多个 gradient 样本可以很容易地降低异谱。然而，需要 thousands 个优化迭代，这种方法的计算成本会快速增长。我们 derive 了一个理论框架，用于杂合抽象和优化。我们在每个迭代中更新和重用过去的样本，使用 low-variance finite-difference 估计器来描述每次迭代中 estimated gradient 的变化。通过结合 пропорциональ 和 finite-difference 样本，我们在优化过程中不断降低我们的新型 gradient meta-估计器的异谱。我们调查了我们的估计器与 Adam 的稳定组合。我们实现了我们的方法，用于 inverse path tracing，并在困难的优化任务上证明了我们的估计器可以加速 convergence。
</details></li>
</ul>
<hr>
<h2 id="On-Computational-Entanglement-and-Its-Interpretation-in-Adversarial-Machine-Learning"><a href="#On-Computational-Entanglement-and-Its-Interpretation-in-Adversarial-Machine-Learning" class="headerlink" title="On Computational Entanglement and Its Interpretation in Adversarial Machine Learning"></a>On Computational Entanglement and Its Interpretation in Adversarial Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15669">http://arxiv.org/abs/2309.15669</a></li>
<li>repo_url: None</li>
<li>paper_authors: YenLung Lai, Xingbo Dong, Zhe Jin</li>
<li>for:  This paper explores the intrinsic complexity and interpretability of adversarial machine learning models.</li>
<li>methods: The authors define entanglement computationally and demonstrate the existence of strong correlations between distant feature samples, akin to entanglement in the quantum realm.</li>
<li>results: The study reveals links between machine learning model complexity and Einstein’s theory of special relativity, challenging conventional perspectives on adversarial transferability and providing insights into more robust and interpretable models.<details>
<summary>Abstract</summary>
Adversarial examples in machine learning has emerged as a focal point of research due to their remarkable ability to deceive models with seemingly inconspicuous input perturbations, potentially resulting in severe consequences. In this study, we embark on a comprehensive exploration of adversarial machine learning models, shedding light on their intrinsic complexity and interpretability. Our investigation reveals intriguing links between machine learning model complexity and Einstein's theory of special relativity, through the concept of entanglement. More specific, we define entanglement computationally and demonstrate that distant feature samples can exhibit strong correlations, akin to entanglement in quantum realm. This revelation challenges conventional perspectives in describing the phenomenon of adversarial transferability observed in contemporary machine learning models. By drawing parallels with the relativistic effects of time dilation and length contraction during computation, we gain deeper insights into adversarial machine learning, paving the way for more robust and interpretable models in this rapidly evolving field.
</details>
<details>
<summary>摘要</summary>
《机器学习中的对抗示例》在研究中得到了广泛关注，因为它们能够通过一些微小的输入抖动，让模型出现异常的行为，从而导致严重的后果。在这项研究中，我们进行了广泛的对机器学习模型的探索，揭示了这些模型的内在复杂性和可解性。我们的调查发现，机器学习模型的复杂度和爱因斯坦的特殊 relativity 理论之间存在感知的联系，通过计算上的束缚来解释。具体来说，我们定义了计算上的束缚，并证明了 distant feature samples 可以显示强相关性，与量子世界中的束缚类似。这一发现挑战了当代机器学习模型中对 adversarial transferability 的描述方式。通过在计算中的时间扭曲和长度减小的关系来启示对 adversarial machine learning 的深入理解，为这一领域的更加robust和可解性的模型开辟了新的道路。
</details></li>
</ul>
<hr>
<h2 id="Federated-Deep-Equilibrium-Learning-A-Compact-Shared-Representation-for-Edge-Communication-Efficiency"><a href="#Federated-Deep-Equilibrium-Learning-A-Compact-Shared-Representation-for-Edge-Communication-Efficiency" class="headerlink" title="Federated Deep Equilibrium Learning: A Compact Shared Representation for Edge Communication Efficiency"></a>Federated Deep Equilibrium Learning: A Compact Shared Representation for Edge Communication Efficiency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15659">http://arxiv.org/abs/2309.15659</a></li>
<li>repo_url: None</li>
<li>paper_authors: Long Tan Le, Tuan Dung Nguyen, Tung-Anh Nguyen, Choong Seon Hong, Nguyen H. Tran</li>
<li>for: 本研究旨在提出一种基于联合平衡学习和共识优化的分布式学习框架，以解决Edge AI解决方案中的通信瓶颈、数据不一致和内存限制问题。</li>
<li>methods: 本研究使用的方法包括深度平衡学习和共识优化，以实现在边缘节点上 derivation of personalized models。具有共识优化的模型结构包括平衡层和传统神经网络层。</li>
<li>results: 实验结果表明，FeDEQ可以在不同的benchmark上达到与个性化方法相同的性能水平，同时使用的模型尺寸比较小，即4倍小于个性化方法的通信尺寸和1.5倍小于个性化方法的训练内存尺寸。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a prominent distributed learning paradigm facilitating collaboration among nodes within an edge network to co-train a global model without centralizing data. By shifting computation to the network edge, FL offers robust and responsive edge-AI solutions and enhance privacy-preservation. However, deploying deep FL models within edge environments is often hindered by communication bottlenecks, data heterogeneity, and memory limitations. To address these challenges jointly, we introduce FeDEQ, a pioneering FL framework that effectively employs deep equilibrium learning and consensus optimization to exploit a compact shared data representation across edge nodes, allowing the derivation of personalized models specific to each node. We delve into a unique model structure composed of an equilibrium layer followed by traditional neural network layers. Here, the equilibrium layer functions as a global feature representation that edge nodes can adapt to personalize their local layers. Capitalizing on FeDEQ's compactness and representation power, we present a novel distributed algorithm rooted in the alternating direction method of multipliers (ADMM) consensus optimization and theoretically establish its convergence for smooth objectives. Experiments across various benchmarks demonstrate that FeDEQ achieves performance comparable to state-of-the-art personalized methods while employing models of up to 4 times smaller in communication size and 1.5 times lower memory footprint during training.
</details>
<details>
<summary>摘要</summary>
Federation Learning (FL) 是一种广泛分布式学习 paradigma，帮助 Edge 网络中的节点共同训练全球模型，而不需要中央化数据。通过将计算转移到网络边缘，FL 提供了可靠和快速的 Edge-AI 解决方案，并保护隐私。然而，在 Edge 环境中部署深度 FL 模型时，经常会遇到通信瓶颈、数据多样性和内存限制。为了共同解决这些挑战，我们介绍了 FeDEQ，一种先进的 FL 框架，该利用深度均衡学习和consensus优化来实现一个具有共同数据表示的紧凑共享模型，允许每个节点 derivation 个性化的模型。我们探讨了一种独特的模型结构，其包括均衡层 seguido de tradicional 神经网络层。在这种模型结构中，均衡层 函数为每个节点可适应的全球特征表示，允许边缘节点个性化其本地层。通过 FeDEQ 的紧凑性和表示力，我们提出了一种新的分布式算法，基于 alternating direction method of multipliers (ADMM) 的consensus优化，并理论确定其收敛性。在多种 benchmark 上进行的实验表明，FeDEQ 可以与当前的个性化方法相比，使用的模型尺寸更小，并在训练过程中占用更少的内存。
</details></li>
</ul>
<hr>
<h2 id="SANGEA-Scalable-and-Attributed-Network-Generation"><a href="#SANGEA-Scalable-and-Attributed-Network-Generation" class="headerlink" title="SANGEA: Scalable and Attributed Network Generation"></a>SANGEA: Scalable and Attributed Network Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15648">http://arxiv.org/abs/2309.15648</a></li>
<li>repo_url: None</li>
<li>paper_authors: Valentin Lemaire, Youssef Achenchabe, Lucas Ody, Houssem Eddine Souid, Gianmarco Aversano, Nicolas Posocco, Sabri Skhiri</li>
<li>for: 本研究旨在扩展现有的生成模型，使其可以应用于大型图。</li>
<li>methods: 本paper使用了分community的方法，首先将大图分成多个社区，然后在每个社区中使用SGG生成图。</li>
<li>results: 实验表明，生成的图与原图具有类似的topology和节点特征分布，同时在下游任务中具有高的链接预测性。此外，生成的图也具有合理的隐私评价。<details>
<summary>Abstract</summary>
The topic of synthetic graph generators (SGGs) has recently received much attention due to the wave of the latest breakthroughs in generative modelling. However, many state-of-the-art SGGs do not scale well with the graph size. Indeed, in the generation process, all the possible edges for a fixed number of nodes must often be considered, which scales in $\mathcal{O}(N^2)$, with $N$ being the number of nodes in the graph. For this reason, many state-of-the-art SGGs are not applicable to large graphs. In this paper, we present SANGEA, a sizeable synthetic graph generation framework which extends the applicability of any SGG to large graphs. By first splitting the large graph into communities, SANGEA trains one SGG per community, then links the community graphs back together to create a synthetic large graph. Our experiments show that the graphs generated by SANGEA have high similarity to the original graph, in terms of both topology and node feature distribution. Additionally, these generated graphs achieve high utility on downstream tasks such as link prediction. Finally, we provide a privacy assessment of the generated graphs to show that, even though they have excellent utility, they also achieve reasonable privacy scores.
</details>
<details>
<summary>摘要</summary>
topic of synthetic graph generators (SGGs) 最近得到了很多关注，因为生成模型的latest breakthroughs 。然而，许多状态对的 SGGs 不能Scales well with the graph size。实际上，在生成过程中，所有可能的边 для一个固定数量的节点必须经常考虑，这 scales in $\mathcal{O}(N^2)$, with $N$ 是图中节点的数量。因此，许多状态对的 SGGs 不适用于大图。在这篇论文中，我们提出了 SANGEA，一个可 scales synthetic graph generation framework。我们首先将大图分成社区，然后在每个社区中训练一个 SGG，然后将社区图相互链接以创建一个 synthetic 大图。我们的实验表明，生成的图与原图具有高度相似性，包括图形态和节点特征分布。此外，这些生成的图可以在下游任务中实现高的链接预测性能。最后，我们进行了隐私评估，表明，即使具有出色的实用性，这些生成的图也可以实现合理的隐私分数。
</details></li>
</ul>
<hr>
<h2 id="Cold-Warm-Net-Addressing-Cold-Start-Users-in-Recommender-Systems"><a href="#Cold-Warm-Net-Addressing-Cold-Start-Users-in-Recommender-Systems" class="headerlink" title="Cold &amp; Warm Net: Addressing Cold-Start Users in Recommender Systems"></a>Cold &amp; Warm Net: Addressing Cold-Start Users in Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15646">http://arxiv.org/abs/2309.15646</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangyu Zhang, Zongqiang Kuang, Zehao Zhang, Fan Huang, Xianfeng Tan</li>
<li>for:  solve the user cold-start problem in the matching stage of recommender systems.</li>
<li>methods:  utilize side information or meta-learning to model cold-start users, and incorporate the results from two experts using a gate network. Additionally, dynamic knowledge distillation is used to assist experts in better learning user representation, and comprehensive mutual information is used to select highly relevant features for the bias net.</li>
<li>results:  outperform other models on all user types on public datasets, and achieve a significant increase in app dwell time and user retention rate on an industrial short video platform.<details>
<summary>Abstract</summary>
Cold-start recommendation is one of the major challenges faced by recommender systems (RS). Herein, we focus on the user cold-start problem. Recently, methods utilizing side information or meta-learning have been used to model cold-start users. However, it is difficult to deploy these methods to industrial RS. There has not been much research that pays attention to the user cold-start problem in the matching stage. In this paper, we propose Cold & Warm Net based on expert models who are responsible for modeling cold-start and warm-up users respectively. A gate network is applied to incorporate the results from two experts. Furthermore, dynamic knowledge distillation acting as a teacher selector is introduced to assist experts in better learning user representation. With comprehensive mutual information, features highly relevant to user behavior are selected for the bias net which explicitly models user behavior bias. Finally, we evaluate our Cold & Warm Net on public datasets in comparison to models commonly applied in the matching stage and it outperforms other models on all user types. The proposed model has also been deployed on an industrial short video platform and achieves a significant increase in app dwell time and user retention rate.
</details>
<details>
<summary>摘要</summary>
冷启动推荐是推荐系统（RS）的一个主要挑战。我们在这里关注用户冷启动问题。现在，利用侧信息或meta学习方法来模型冷启动用户已经得到了一些研究。然而，在实际RS中部署这些方法却很困难。在匹配阶段，很少有关注用户冷启动问题的研究。在这篇论文中，我们提出了冷&温网（Cold & Warm Net），该模型由专家模型负责模型冷启动和温存用户。一个门控网络用于结合两个专家的结果。此外，我们还引入了动态知识填充作为教师选择器，以助专家更好地学习用户表示。通过全面的共同信息，我们选择了高度相关于用户行为的特征来进行偏好网的模型。最后，我们对公共数据集进行评估，并与常见在匹配阶段使用的模型进行比较，我们的模型在所有用户类型上都表现出优异。此外，我们还将该模型部署到了一家短视频平台，并实现了明显提高应用内存时间和用户固持率。
</details></li>
</ul>
<hr>
<h2 id="Why-do-Angular-Margin-Losses-work-well-for-Semi-Supervised-Anomalous-Sound-Detection"><a href="#Why-do-Angular-Margin-Losses-work-well-for-Semi-Supervised-Anomalous-Sound-Detection" class="headerlink" title="Why do Angular Margin Losses work well for Semi-Supervised Anomalous Sound Detection?"></a>Why do Angular Margin Losses work well for Semi-Supervised Anomalous Sound Detection?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15643">http://arxiv.org/abs/2309.15643</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kevin Wilkinghoff, Frank Kurth</li>
<li>for: 这 paper 的目的是调查使用angular margin losses与auxiliary tasks来检测异常声音的原因。</li>
<li>methods: 这 paper 使用了angular margin losses与related classification task作为auxiliary task，并通过 teoretic 分析和实验证明了这种方法可以减少compactness loss并避免学习极端解。</li>
<li>results: 实验表明，使用相关的类别任务作为auxiliary task可以教会模型学习适合检测异常声音的表示，并在噪音条件下显著地超越生成或一类模型。<details>
<summary>Abstract</summary>
State-of-the-art anomalous sound detection systems often utilize angular margin losses to learn suitable representations of acoustic data using an auxiliary task, which usually is a supervised or self-supervised classification task. The underlying idea is that, in order to solve this auxiliary task, specific information about normal data needs to be captured in the learned representations and that this information is also sufficient to differentiate between normal and anomalous samples. Especially in noisy conditions, discriminative models based on angular margin losses tend to significantly outperform systems based on generative or one-class models. The goal of this work is to investigate why using angular margin losses with auxiliary tasks works well for detecting anomalous sounds. To this end, it is shown, both theoretically and experimentally, that minimizing angular margin losses also minimizes compactness loss while inherently preventing learning trivial solutions. Furthermore, multiple experiments are conducted to show that using a related classification task as an auxiliary task teaches the model to learn representations suitable for detecting anomalous sounds in noisy conditions. Among these experiments are performance evaluations, visualizing the embedding space with t-SNE and visualizing the input representations with respect to the anomaly score using randomized input sampling for explanation.
</details>
<details>
<summary>摘要</summary>
现代异常声检测系统经常使用角度margin损失来学习适合听音数据的表示。这个想法是，为了解决这个辅助任务，需要学习表示中包含正常数据特征信息，并且这些信息也能够区分正常和异常样本。尤其在噪音条件下，使用角度margin损失的权重分配模型往往与生成模型或一类模型相比表现出色。这项工作的目的是研究为何在听音检测中使用角度margin损失和辅助任务是有效的。这个问题的解决方法是，理论上和实验上都证明，使用角度margin损失同时减小紧凑性损失，并且自然地避免学习极端解。此外，通过多个实验表明，使用相关的类别任务作为辅助任务可以使模型学习适合听音检测的表示，特别是在噪音条件下。这些实验包括表现评估、使用t-SNEVisualizing embedding空间和使用随机输入抽样来解释输入表示的方法。
</details></li>
</ul>
<hr>
<h2 id="Efficient-tensor-network-simulation-of-IBM’s-largest-quantum-processors"><a href="#Efficient-tensor-network-simulation-of-IBM’s-largest-quantum-processors" class="headerlink" title="Efficient tensor network simulation of IBM’s largest quantum processors"></a>Efficient tensor network simulation of IBM’s largest quantum processors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15642">http://arxiv.org/abs/2309.15642</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddhartha Patra, Saeed S. Jahromi, Sukhbinder Singh, Roman Orus</li>
<li>For: The paper demonstrates how quantum-inspired 2D tensor networks can be used to efficiently and accurately simulate large quantum processors, specifically the IBM Eagle, Osprey, and Condor processors.* Methods: The paper uses graph-based Projected Entangled Pair States (gPEPS) to simulate the dynamics of a complex quantum many-body system, the kicked Ising experiment.* Results: The paper achieves very large unprecedented accuracy with remarkably low computational resources for this model, and extends the results to larger qubit counts (433 and 1121 qubits) and longer evolution times. Additionally, the paper demonstrates accurate simulations for infinitely-many qubits.<details>
<summary>Abstract</summary>
We show how quantum-inspired 2d tensor networks can be used to efficiently and accurately simulate the largest quantum processors from IBM, namely Eagle (127 qubits), Osprey (433 qubits) and Condor (1121 qubits). We simulate the dynamics of a complex quantum many-body system -- specifically, the kicked Ising experiment considered recently by IBM in Nature 618, p. 500-505 (2023) -- using graph-based Projected Entangled Pair States (gPEPS), which was proposed by some of us in PRB 99, 195105 (2019). Our results show that simple tensor updates are already sufficient to achieve very large unprecedented accuracy with remarkably low computational resources for this model. Apart from simulating the original experiment for 127 qubits, we also extend our results to 433 and 1121 qubits, and for evolution times around 8 times longer, thus setting a benchmark for the newest IBM quantum machines. We also report accurate simulations for infinitely-many qubits. Our results show that gPEPS are a natural tool to efficiently simulate quantum computers with an underlying lattice-based qubit connectivity, such as all quantum processors based on superconducting qubits.
</details>
<details>
<summary>摘要</summary>
我们显示了使用量子感知的2Dtensor网络来高效地和精度地模拟IBM最大的量子处理器，包括鹰（127 qubits）、𫛭（433 qubits）和鸠（1121 qubits）。我们使用图形基于的Projected Entangled Pair States（gPEPS）来模拟一个复杂的量子多体系统——对IBM在Nature 618, p. 500-505（2023）中考虑的kicked Ising实验。我们的结果显示了简单的tensor更新已经足够以 достиunge非常大的新高精度，仅需使用remarkably low的计算资源。除了模拟127 qubits的原始实验之外，我们还将结果扩展到433和1121 qubits，并在 evolution times around 8 times longer 上进行了benchmark。我们还报告了对无限多颗 qubits 的精度模拟。我们的结果显示了gPEPS是一个自然的工具来高效地模拟基于碳素链的qubit连接的量子计算机。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Sharpness-Aware-Optimization-Through-Variance-Suppression"><a href="#Enhancing-Sharpness-Aware-Optimization-Through-Variance-Suppression" class="headerlink" title="Enhancing Sharpness-Aware Optimization Through Variance Suppression"></a>Enhancing Sharpness-Aware Optimization Through Variance Suppression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15639">http://arxiv.org/abs/2309.15639</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bingcong Li, Georgios B. Giannakis</li>
<li>for: 提高深度神经网络的泛化能力，不需要大量数据增强</li>
<li>methods: 基于损失函数的几何结构，寻找’平坦谷’，通过最大损失带动参数的敏感范围内的敏感探索</li>
<li>results: 提出了一种新的稳定化难度探索方法（VaSSO），可以避免’友好的敌人’的问题，并在模型独立任务中提供了数值上的改进和鲁棒性 against 高强度标签噪声。<details>
<summary>Abstract</summary>
Sharpness-aware minimization (SAM) has well documented merits in enhancing generalization of deep neural networks, even without sizable data augmentation. Embracing the geometry of the loss function, where neighborhoods of 'flat minima' heighten generalization ability, SAM seeks 'flat valleys' by minimizing the maximum loss caused by an adversary perturbing parameters within the neighborhood. Although critical to account for sharpness of the loss function, such an 'over-friendly adversary' can curtail the outmost level of generalization. The novel approach of this contribution fosters stabilization of adversaries through variance suppression (VaSSO) to avoid such friendliness. VaSSO's provable stability safeguards its numerical improvement over SAM in model-agnostic tasks, including image classification and machine translation. In addition, experiments confirm that VaSSO endows SAM with robustness against high levels of label noise.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用适度化 minimization（SAM）可以增强深度神经网络的通用性，无需很大的数据增强。通过捕捉损失函数的几何结构，where neighborhoods of 'flat minima' enhance generalization ability, SAM seeks 'flat valleys' by minimizing the maximum loss caused by an adversary perturbing parameters within the neighborhood。虽然需要考虑损失函数的锐度，但这种'过度友好的对手'可能会限制最大化的泛化能力。这篇论文的新 aproach 是通过 variance suppression (VaSSO) 来稳定对手，以避免这种友好性。VaSSO 的数学稳定性保证其数值改进之前的 SAM 在模型无关任务中，包括图像分类和机器翻译。此外，实验证明 VaSSO 会赋予 SAM 对高水平的标签噪声的Robustness。Note: "适度化" (weixiao) in the text refers to "sharpness-aware" in English.
</details></li>
</ul>
<hr>
<h2 id="Entropic-Matching-for-Expectation-Propagation-of-Markov-Jump-Processes"><a href="#Entropic-Matching-for-Expectation-Propagation-of-Markov-Jump-Processes" class="headerlink" title="Entropic Matching for Expectation Propagation of Markov Jump Processes"></a>Entropic Matching for Expectation Propagation of Markov Jump Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15604">http://arxiv.org/abs/2309.15604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bastian Alt, Heinz Koeppl</li>
<li>for: 本研究探讨了隐藏时间连续随机过程的统计推断问题，这类问题经常是困难的，特别是对于粒子状态空间过程的描述。</li>
<li>methods: 我们提出了一种新的可行的推断方案，基于Entropic Matching框架，可以在Well-known Expectation Propagation算法中嵌入。我们给出了一种简单的家族的近似分布的关闭式结果，并应用于普遍的化学反应网络模型，这是系统生物学中重要的工具。</li>
<li>results: 我们 deriv了关闭式表达式，用于粒子参数的点估计，并使用近似期望最大化算法进行推断。我们对各种化学反应网络实例进行了评估，包括一个Stochastic Lotka-Volterra示例，并讨论了我们的方法的局限和未来改进的可能性。<details>
<summary>Abstract</summary>
This paper addresses the problem of statistical inference for latent continuous-time stochastic processes, which is often intractable, particularly for discrete state space processes described by Markov jump processes. To overcome this issue, we propose a new tractable inference scheme based on an entropic matching framework that can be embedded into the well-known expectation propagation algorithm. We demonstrate the effectiveness of our method by providing closed-form results for a simple family of approximate distributions and apply it to the general class of chemical reaction networks, which are a crucial tool for modeling in systems biology. Moreover, we derive closed form expressions for point estimation of the underlying parameters using an approximate expectation maximization procedure. We evaluate the performance of our method on various chemical reaction network instantiations, including a stochastic Lotka-Voltera example, and discuss its limitations and potential for future improvements. Our proposed approach provides a promising direction for addressing complex continuous-time Bayesian inference problems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Distill-Knowledge-in-Multi-task-Reinforcement-Learning-with-Optimal-Transport-Regularization"><a href="#Distill-Knowledge-in-Multi-task-Reinforcement-Learning-with-Optimal-Transport-Regularization" class="headerlink" title="Distill Knowledge in Multi-task Reinforcement Learning with Optimal-Transport Regularization"></a>Distill Knowledge in Multi-task Reinforcement Learning with Optimal-Transport Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15603">http://arxiv.org/abs/2309.15603</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bang Giang Le, Viet Cuong Ta</li>
<li>for: 提高多任务 reinforcement learning 训练agent的数据效率，通过将不同但相关的任务之间的知识传递。</li>
<li>methods: 使用 Optimal transport-based 奖励来稳定知识传递，通过Sinkhorn mapping来计算Optimal transport距离，并用作积累奖励。</li>
<li>results: 在多个网格导航多目标任务中，我们的方法能够加速agent的学习过程，并超越多个基eline。<details>
<summary>Abstract</summary>
In multi-task reinforcement learning, it is possible to improve the data efficiency of training agents by transferring knowledge from other different but related tasks. Because the experiences from different tasks are usually biased toward the specific task goals. Traditional methods rely on Kullback-Leibler regularization to stabilize the transfer of knowledge from one task to the others. In this work, we explore the direction of replacing the Kullback-Leibler divergence with a novel Optimal transport-based regularization. By using the Sinkhorn mapping, we can approximate the Optimal transport distance between the state distribution of tasks. The distance is then used as an amortized reward to regularize the amount of sharing information. We experiment our frameworks on several grid-based navigation multi-goal to validate the effectiveness of the approach. The results show that our added Optimal transport-based rewards are able to speed up the learning process of agents and outperforms several baselines on multi-task learning.
</details>
<details>
<summary>摘要</summary>
在多任务强化学习中，可以通过知识传递来提高训练代理的数据效率。因为不同任务的经验通常偏向特定任务目标。传统方法利用库拉布-莱布尔正则化来稳定知识传递。在这种工作中，我们研究将库拉布-莱布尔差异 replaced with novel 优化运输基于正则化。通过使用填充映射，我们可以近似优化运输距离 между任务状态分布。这个距离然后用作权衡共享信息的奖励，以补偿代理的学习过程。我们在几个网格基础的导航多目标上进行了实验，结果表明我们的添加的优化运输基于奖励能够加速代理的学习过程，并超过了多个基线在多任务学习中。
</details></li>
</ul>
<hr>
<h2 id="OceanBench-The-Sea-Surface-Height-Edition"><a href="#OceanBench-The-Sea-Surface-Height-Edition" class="headerlink" title="OceanBench: The Sea Surface Height Edition"></a>OceanBench: The Sea Surface Height Edition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15599">http://arxiv.org/abs/2309.15599</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jejjohnson/oceanbench">https://github.com/jejjohnson/oceanbench</a></li>
<li>paper_authors: J. Emmanuel Johnson, Quentin Febvre, Anastasia Gorbunova, Sammy Metref, Maxime Ballarotta, Julien Le Sommer, Ronan Fablet<br>for: This paper aims to provide a unifying framework for machine learning (ML) researchers to benchmark their models and customize their pipelines for ocean satellite data interpolation challenges.methods: The paper uses satellite remote sensing data and machine learning techniques to develop a standardized processing framework called OceanBench, which provides plug-and-play data and pre-configured pipelines for ML researchers.results: The paper demonstrates the effectiveness of the OceanBench framework through a first edition dedicated to sea surface height (SSH) interpolation challenges, providing datasets and ML-ready benchmarking pipelines for simulated ocean satellite data, multi-modal and multi-sensor fusion issues, and transfer-learning to real ocean satellite observations.<details>
<summary>Abstract</summary>
The ocean profoundly influences human activities and plays a critical role in climate regulation. Our understanding has improved over the last decades with the advent of satellite remote sensing data, allowing us to capture essential quantities over the globe, e.g., sea surface height (SSH). However, ocean satellite data presents challenges for information extraction due to their sparsity and irregular sampling, signal complexity, and noise. Machine learning (ML) techniques have demonstrated their capabilities in dealing with large-scale, complex signals. Therefore we see an opportunity for ML models to harness the information contained in ocean satellite data. However, data representation and relevant evaluation metrics can be the defining factors when determining the success of applied ML. The processing steps from the raw observation data to a ML-ready state and from model outputs to interpretable quantities require domain expertise, which can be a significant barrier to entry for ML researchers. OceanBench is a unifying framework that provides standardized processing steps that comply with domain-expert standards. It provides plug-and-play data and pre-configured pipelines for ML researchers to benchmark their models and a transparent configurable framework for researchers to customize and extend the pipeline for their tasks. In this work, we demonstrate the OceanBench framework through a first edition dedicated to SSH interpolation challenges. We provide datasets and ML-ready benchmarking pipelines for the long-standing problem of interpolating observations from simulated ocean satellite data, multi-modal and multi-sensor fusion issues, and transfer-learning to real ocean satellite observations. The OceanBench framework is available at github.com/jejjohnson/oceanbench and the dataset registry is available at github.com/quentinf00/oceanbench-data-registry.
</details>
<details>
<summary>摘要</summary>
海洋对人类活动产生深远的影响，对气候调控也扮演着关键性的角色。过去几十年来，卫星远程感知数据的出现，使我们能够全球范围内获取重要量，如海面高程（SSH）。然而，海洋卫星数据具有缺乏精度和不规则的采样、信号复杂性和噪声等挑战。机器学习（ML）技术已经在处理大规模复杂信号方面表现出色，因此我们认为ML模型可以从海洋卫星数据中提取信息。然而，数据表示和相关评价指标是确定成功的关键因素。从原始观测数据到ML准备状态和模型输出到可读量需要域专业知识，这可能是ML研究人员面临的 significiant barrier。 OceanBench 是一个统一的框架，它提供了遵循域专业标准的处理步骤，并提供了可插入的数据和预配置的管道，以便 ML 研究人员可以 benchmark 他们的模型。在这项工作中，我们通过 OceanBench 框架展示了 SSH  interpolating 挑战。我们提供了数据集和 ML 准备好的管道，以解决长期存在的 ocean 卫星数据 interpolating 问题，以及多模式和多感知融合问题，以及将模型转移到实际 ocean 卫星观测数据。 OceanBench 框架可以在 GitHub 上找到（github.com/jejjohnson/oceanbench），数据库注册可以在 GitHub 上找到（github.com/quentinf00/oceanbench-data-registry）。
</details></li>
</ul>
<hr>
<h2 id="Exciton-Polariton-Condensates-A-Fourier-Neural-Operator-Approach"><a href="#Exciton-Polariton-Condensates-A-Fourier-Neural-Operator-Approach" class="headerlink" title="Exciton-Polariton Condensates: A Fourier Neural Operator Approach"></a>Exciton-Polariton Condensates: A Fourier Neural Operator Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15593">http://arxiv.org/abs/2309.15593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Surya T. Sathujoda, Yuan Wang, Kanishk Gandhi</li>
<li>for: 研究者实现了一种基于Machine Learning的Fourier Neural Operator方法，用于解决吸引子-晶质子对应系统中的非线性问题。</li>
<li>methods: 研究者使用了Machine Learning的Fourier Neural Operator方法来解决Gross-Pitaevskii方程式和额外激发数方程式的组合。</li>
<li>results: 研究者发现使用Machine Learning的Fourier Neural Operator方法可以高度精度地预测系统的终端解，并且比CUDA-based GPU推导器快得多，约1000倍。这项研究开辟了潜在的全光学芯片设计工程过程的可能性。<details>
<summary>Abstract</summary>
Advancements in semiconductor fabrication over the past decade have catalyzed extensive research into all-optical devices driven by exciton-polariton condensates. Preliminary validations of such devices, including transistors, have shown encouraging results even under ambient conditions. A significant challenge still remains for large scale application however: the lack of a robust solver that can be used to simulate complex nonlinear systems which require an extended period of time to stabilize. Addressing this need, we propose the application of a machine-learning-based Fourier Neural Operator approach to find the solution to the Gross-Pitaevskii equations coupled with extra exciton rate equations. This work marks the first direct application of Neural Operators to an exciton-polariton condensate system. Our findings show that the proposed method can predict final-state solutions to a high degree of accuracy almost 1000 times faster than CUDA-based GPU solvers. Moreover, this paves the way for potential all-optical chip design workflows by integrating experimental data.
</details>
<details>
<summary>摘要</summary>
“过去十年的半导体制造技术发展，促使了对激子-辐olinarpiton储集体系的广泛研究。初步验证表明，这些设备，包括普通逻辑 gates，在常 ambient 条件下表现出了激动人人。然而，大规模应用还面临着一个主要挑战：没有一个可靠的可用来模拟复杂非线性系统，需要较长时间来稳定。为解决这一需求，我们提议使用机器学习基于Fourier Neural Operator的方法来解决戈斯-皮特涅夫斯基方程组 coupled with extra exciton rate equations。这是直接应用Neural Operators于激子-辐olinarpiton储集体系的首次尝试。我们的发现表明，提议的方法可以高度准确地预测最终解决方案，比CUDA基于GPU的加速器solver快得多，约1000倍。此外，这也开启了可能的全光学芯片设计工作流程，通过结合实验数据。”Note: Please note that the translation is in Simplified Chinese, and the grammar and sentence structure may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="Demographic-Parity-Mitigating-Biases-in-Real-World-Data"><a href="#Demographic-Parity-Mitigating-Biases-in-Real-World-Data" class="headerlink" title="Demographic Parity: Mitigating Biases in Real-World Data"></a>Demographic Parity: Mitigating Biases in Real-World Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17347">http://arxiv.org/abs/2309.17347</a></li>
<li>repo_url: None</li>
<li>paper_authors: Orestis Loukas, Ho-Ryun Chung</li>
<li>for: 本研究旨在提出一种可靠的方法，以除除计算机支持的决策系统中的不良偏见，同时保持分类用途的最大化。</li>
<li>methods: 本研究使用了实世界数据 derive an asymptotic dataset，该dataset具有人口分布的准确性和现实性，并可以用来训练计算机支持的各种分类器。</li>
<li>results: 经 benchmarking，我们确认了这些基于我们生成的synthetic数据训练的分类器没有显式或隐式的偏见。<details>
<summary>Abstract</summary>
Computer-based decision systems are widely used to automate decisions in many aspects of everyday life, which include sensitive areas like hiring, loaning and even criminal sentencing. A decision pipeline heavily relies on large volumes of historical real-world data for training its models. However, historical training data often contains gender, racial or other biases which are propagated to the trained models influencing computer-based decisions. In this work, we propose a robust methodology that guarantees the removal of unwanted biases while maximally preserving classification utility. Our approach can always achieve this in a model-independent way by deriving from real-world data the asymptotic dataset that uniquely encodes demographic parity and realism. As a proof-of-principle, we deduce from public census records such an asymptotic dataset from which synthetic samples can be generated to train well-established classifiers. Benchmarking the generalization capability of these classifiers trained on our synthetic data, we confirm the absence of any explicit or implicit bias in the computer-aided decision.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-Faithful-Neural-Network-Intrinsic-Interpretation-with-Shapley-Additive-Self-Attribution"><a href="#Towards-Faithful-Neural-Network-Intrinsic-Interpretation-with-Shapley-Additive-Self-Attribution" class="headerlink" title="Towards Faithful Neural Network Intrinsic Interpretation with Shapley Additive Self-Attribution"></a>Towards Faithful Neural Network Intrinsic Interpretation with Shapley Additive Self-Attribution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15559">http://arxiv.org/abs/2309.15559</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ying Sun, Hengshu Zhu, Hui Xiong</li>
<li>for: 本研究旨在提供一种基于自我评估的神经网络模型，以提高模型的解释性和表达能力。</li>
<li>methods: 本研究提出了一种普适的加法自我归因（ASA）框架，并提出了基于Shapley值的Self-Attributing Neural Network（SASANet）模型，以实现对输出的自我归因值的正确预测。SASANet使用积分贡献序列schema和内部蒸馏学习策略来模型有意义的输出，从而实现了不减误的意义值函数。</li>
<li>results: 实验结果表明，SASANet的性能较 existing self-attributing models 高，与黑盒模型相当，并且比Post-hoc方法更加精准和效率地解释其预测结果。<details>
<summary>Abstract</summary>
Self-interpreting neural networks have garnered significant interest in research. Existing works in this domain often (1) lack a solid theoretical foundation ensuring genuine interpretability or (2) compromise model expressiveness. In response, we formulate a generic Additive Self-Attribution (ASA) framework. Observing the absence of Shapley value in Additive Self-Attribution, we propose Shapley Additive Self-Attributing Neural Network (SASANet), with theoretical guarantees for the self-attribution value equal to the output's Shapley values. Specifically, SASANet uses a marginal contribution-based sequential schema and internal distillation-based training strategies to model meaningful outputs for any number of features, resulting in un-approximated meaningful value function. Our experimental results indicate SASANet surpasses existing self-attributing models in performance and rivals black-box models. Moreover, SASANet is shown more precise and efficient than post-hoc methods in interpreting its own predictions.
</details>
<details>
<summary>摘要</summary>
自适应神经网络在研究中受到了广泛的关注。现有的研究经常（1）缺乏固定的理论基础，使得真正的解释性受限，或（2）妥协模型表达能力。为回应这些挑战，我们提出了一种通用的加法自我解释（ASA）框架。我们注意到了添加式自我解释中缺失的雪佛利值，因此我们提出了雪佛利添加式自我解释神经网络（SASANet），具有输出的雪佛利值的理论保证。特别是，SASANet使用级联贡献基于的顺序schema和内部蒸馏基于的训练策略，以模型任意数量的特征输出具有准确的意义。我们的实验结果表明，SASANet比现有的自我解释模型在性能和黑盒模型的表现相当，同时也比post-hoc方法更精准和高效地解释其预测结果。
</details></li>
</ul>
<hr>
<h2 id="Startup-success-prediction-and-VC-portfolio-simulation-using-CrunchBase-data"><a href="#Startup-success-prediction-and-VC-portfolio-simulation-using-CrunchBase-data" class="headerlink" title="Startup success prediction and VC portfolio simulation using CrunchBase data"></a>Startup success prediction and VC portfolio simulation using CrunchBase data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15552">http://arxiv.org/abs/2309.15552</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mark Potanin, Andrey Chertok, Konstantin Zorin, Cyril Shtabtsovsky</li>
<li>for: This paper aims to predict key success milestones for startups at their Series B and Series C investment stages, such as achieving an Initial Public Offering (IPO), attaining unicorn status, or executing a successful Merger and Acquisition (M&amp;A).</li>
<li>methods: The paper introduces a novel deep learning model for predicting startup success, which integrates a variety of factors such as funding metrics, founder features, and industry category. The model uses a comprehensive backtesting algorithm to simulate the venture capital investment process and evaluate its performance against historical data.</li>
<li>results: The paper achieved a 14 times capital growth and successfully identified high-potential startups in their B round, including Revolut, DigitalOcean, Klarna, Github, and others. The empirical findings highlight the importance of incorporating diverse feature sets in enhancing the model’s predictive accuracy.Here’s the simplified Chinese version of the three key points:</li>
<li>for: 这篇论文旨在预测 startup 在 Series B 和 Series C 融资阶段的成功关键 milestone，如 Initial Public Offering (IPO)、成为unicorn 或成功的 Merger and Acquisition (M&amp;A)。</li>
<li>methods: 论文提出了一种新的深度学习模型，用于预测 startup 的成功，该模型集成了多种因素，如资金指标、创始人特征、行业类别。模型使用了全面的回测算法，模拟了投资过程，以评估其在历史数据上的实际性。</li>
<li>results: 论文在 Crunchbase 上实现了 14 倍资金增长，并成功地标识了 B 轮高 potential  startup，如 Revolut、DigitalOcean、Klarna、Github 等。实验结果表明，将多种特征集成到模型中可以提高预测精度。<details>
<summary>Abstract</summary>
Predicting startup success presents a formidable challenge due to the inherently volatile landscape of the entrepreneurial ecosystem. The advent of extensive databases like Crunchbase jointly with available open data enables the application of machine learning and artificial intelligence for more accurate predictive analytics. This paper focuses on startups at their Series B and Series C investment stages, aiming to predict key success milestones such as achieving an Initial Public Offering (IPO), attaining unicorn status, or executing a successful Merger and Acquisition (M\&A). We introduce novel deep learning model for predicting startup success, integrating a variety of factors such as funding metrics, founder features, industry category. A distinctive feature of our research is the use of a comprehensive backtesting algorithm designed to simulate the venture capital investment process. This simulation allows for a robust evaluation of our model's performance against historical data, providing actionable insights into its practical utility in real-world investment contexts. Evaluating our model on Crunchbase's, we achieved a 14 times capital growth and successfully identified on B round high-potential startups including Revolut, DigitalOcean, Klarna, Github and others. Our empirical findings illuminate the importance of incorporating diverse feature sets in enhancing the model's predictive accuracy. In summary, our work demonstrates the considerable promise of deep learning models and alternative unstructured data in predicting startup success and sets the stage for future advancements in this research area.
</details>
<details>
<summary>摘要</summary>
预测创业成功具有挑战性，因为创新企业环境的自然变化和不确定性。随着数据库如Crunchbase的出现以及可用的开放数据，我们可以通过机器学习和人工智能来进行更准确的预测分析。这篇论文将关注创业在Series B和Series C融资阶段，以预测创业成功的关键里程碑，如实现首次公开募股（IPO）、成为unicorn企业或成功投资（M&A）。我们提出了一种深度学习模型，以便预测创业成功，并 integrate了各种因素，如资金指标、创始人特征、行业类别。我们的研究的一个独特特点是使用了全面的回测算法，以模拟投资过程，从而提供了对历史数据的robust评估，并提供了实际投资场景中模型的实用性。通过对Crunchbase的数据进行评估，我们实现了14倍的资金增长，并成功地标识了B轮高潜力创业公司，如Revolut、DigitalOcean、Klarna、Github等。我们的实证发现，包括多种特征集的 incorporation 可以增强模型的预测精度。总之，我们的工作表明了深度学习模型和代替性数据在预测创业成功的可能性，并为这个研究领域的未来发展奠定基础。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Channel-Dimensions-to-Isolate-Outliers-for-Low-bit-Weight-Quantization-of-Large-Language-Models"><a href="#Rethinking-Channel-Dimensions-to-Isolate-Outliers-for-Low-bit-Weight-Quantization-of-Large-Language-Models" class="headerlink" title="Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models"></a>Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15531">http://arxiv.org/abs/2309.15531</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jung Hwan Heo, Jeonghoon Kim, Beomseok Kwon, Byeongwook Kim, Se Jung Kwon, Dongsoo Lee</li>
<li>for: 这篇论文目的是提出一种新的量化方法，以提高小批量推理中的大型语言模型（LLMs）的效率。</li>
<li>methods: 本论文提出了一种新的量化方法，即per-IC量化，它在每个输入通道（IC）中创建量化组，以将问题集中的大量数据隔离出来。此外，论文还提出了一个名为Adaptive Dimensions（AdaDim）的可靠量化框架，可以适应不同的权重敏感性模式。</li>
<li>results: 论文的实验结果显示，这些新的量化方法可以与传统的 Round-To-Nearest 和 GPTQ 方法相结合，实现在不同的语言模型 benchmark 上的优化性能。特别是在基础 LLMS 上（up to +4.7% on MMLU）和人工评估 LLMS 上（up to +10% on HumanEval）。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have recently demonstrated a remarkable success across various tasks. However, efficiently serving LLMs has been a challenge due to its large memory bottleneck, specifically in small batch inference settings (e.g. mobile devices). Weight-only quantization can be a promising approach, but sub-4 bit quantization remains a challenge due to large-magnitude activation outliers. To mitigate the undesirable outlier effect, we first propose per-IC quantization, a simple yet effective method that creates quantization groups within each input channel (IC) rather than the conventional per-output channel (OC). Our method is motivated by the observation that activation outliers affect the input dimension of the weight matrix, so similarly grouping the weights in the IC direction can isolate outliers to be within a group. We also find that activation outliers do not dictate quantization difficulty, and inherent weight sensitivities also exist. With per-IC quantization as a new outlier-friendly scheme, we then propose Adaptive Dimensions (AdaDim), a versatile quantization framework that can adapt to various weight sensitivity patterns. We demonstrate the effectiveness of AdaDim by augmenting prior methods such as Round-To-Nearest and GPTQ, showing significant improvements across various language modeling benchmarks for both base (up to +4.7% on MMLU) and instruction-tuned (up to +10% on HumanEval) LLMs.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）最近表现出了惊人的成功，但是有效地服务LLM仍然是一个挑战，特别是在小批量推理设置（例如移动设备）。量化可以是一种有前途的方法，但是4比特以下的量化仍然是一个挑战，因为大量的活动异常值。为了解决这些不良异常的影响，我们首先提议使用每个输入通道（IC）量化，一种简单而有效的方法，它在每个输入通道内创建量化组而不是传统的每个输出通道（OC）。我们的方法是由于异常值影响输入维度的 weights 矩阵，因此在IC方向上组合 weights 也可以孤立异常。我们还发现，异常值不会决定量化的难度，潜在的 weight 敏感性也存在。在每个IC量化为新的异常友好方案基础之上，我们然后提议一种可变维度的量化框架，可以适应不同的 weight 敏感性模式。我们通过将优化先前的方法，如 Round-To-Nearest 和 GPTQ，在不同的语言模型推理 benchmark 上表现出显著的改善，对于基本（最高 +4.7% 的 MMLU）和指导调整（最高 +10% 的 HumanEval）LLM 都有显著的改善。
</details></li>
</ul>
<hr>
<h2 id="GNN4EEG-A-Benchmark-and-Toolkit-for-Electroencephalography-Classification-with-Graph-Neural-Network"><a href="#GNN4EEG-A-Benchmark-and-Toolkit-for-Electroencephalography-Classification-with-Graph-Neural-Network" class="headerlink" title="GNN4EEG: A Benchmark and Toolkit for Electroencephalography Classification with Graph Neural Network"></a>GNN4EEG: A Benchmark and Toolkit for Electroencephalography Classification with Graph Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15515">http://arxiv.org/abs/2309.15515</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/miracle-2001/gnn4eeg">https://github.com/miracle-2001/gnn4eeg</a></li>
<li>paper_authors: Kaiyuan Zhang, Ziyi Ye, Qingyao Ai, Xiaohui Xie, Yiqun Liu</li>
<li>for: 本研究旨在提供一个可读的Graph Neural Networks（GNN）工具套件，用于模型EEG信号。</li>
<li>methods: 本工具套件包括三个 ком成分：（i）一个大的benchmark，基于四个EEG分类任务，使用123名参与者收集的EEG数据；（ii）各种state-of-the-art GNN-based EEG分类模型的容易使用实现，如DGCNN和RGNN等；（iii）实现了广泛的实验设定和评估协议，如数据分割协议和十字验证协议。</li>
<li>results: 本研究通过使用GNN4EEG工具套件，可以实现高精度的EEG分类。<details>
<summary>Abstract</summary>
Electroencephalography(EEG) classification is a crucial task in neuroscience, neural engineering, and several commercial applications. Traditional EEG classification models, however, have often overlooked or inadequately leveraged the brain's topological information. Recognizing this shortfall, there has been a burgeoning interest in recent years in harnessing the potential of Graph Neural Networks (GNN) to exploit the topological information by modeling features selected from each EEG channel in a graph structure. To further facilitate research in this direction, we introduce GNN4EEG, a versatile and user-friendly toolkit for GNN-based modeling of EEG signals. GNN4EEG comprises three components: (i)A large benchmark constructed with four EEG classification tasks based on EEG data collected from 123 participants. (ii)Easy-to-use implementations on various state-of-the-art GNN-based EEG classification models, e.g., DGCNN, RGNN, etc. (iii)Implementations of comprehensive experimental settings and evaluation protocols, e.g., data splitting protocols, and cross-validation protocols. GNN4EEG is publicly released at https://github.com/Miracle-2001/GNN4EEG.
</details>
<details>
<summary>摘要</summary>
electroencephalography(EEG)分类是 neuroscience, neural engineering 和一些商业应用中的关键任务。传统的EEG分类模型，然而，经常忽视或不充分利用大脑的 topological 信息。认识到这一缺点，过去几年来，有越来越多的研究者在使用Graph Neural Networks (GNN) 来利用选择自 EEG 通道的特征，建模 EEG 信号。为了进一步促进这个方向的研究，我们介绍 GNN4EEG，一个多样化和易用的工具集，用于 GNN 基于 EEG 信号的模型化。GNN4EEG 包括以下三部分：(i) 一个包含四个 EEG 分类任务的大量 benchmark，基于123名参与者收集的 EEG 数据。(ii) 一些实现了当前State-of-the-art GNN 基于 EEG 分类模型，例如 DGCNN 和 RGNN 等。(iii) 实现了完整的实验设置和评估协议，例如数据分割协议和十字验证协议。GNN4EEG 公开发布于 https://github.com/Miracle-2001/GNN4EEG。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Personalized-Federated-Learning-with-Shared-and-Personalized-Uncertainty-Representations"><a href="#Bayesian-Personalized-Federated-Learning-with-Shared-and-Personalized-Uncertainty-Representations" class="headerlink" title="Bayesian Personalized Federated Learning with Shared and Personalized Uncertainty Representations"></a>Bayesian Personalized Federated Learning with Shared and Personalized Uncertainty Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15499">http://arxiv.org/abs/2309.15499</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hui Chen, Hengyu Liu, Longbing Cao, Tiancheng Zhang</li>
<li>for: This paper aims to address challenges in existing personalized federated learning (PFL) by introducing a Bayesian personalized federated learning (BPFL) framework that quantifies uncertainty and heterogeneity within and across clients.</li>
<li>methods: The BPFL framework uses a Bayesian federated neural network (BPFed) to decompose hidden neural representations into shared and local components, and jointly learns cross-client shared uncertainty and client-specific personalized uncertainty over statistically heterogeneous client data.</li>
<li>results: The paper provides theoretical analysis and guarantees, as well as experimental evaluation of BPFed against diversified baselines, to demonstrate the effectiveness of the proposed approach.<details>
<summary>Abstract</summary>
Bayesian personalized federated learning (BPFL) addresses challenges in existing personalized FL (PFL). BPFL aims to quantify the uncertainty and heterogeneity within and across clients towards uncertainty representations by addressing the statistical heterogeneity of client data. In PFL, some recent preliminary work proposes to decompose hidden neural representations into shared and local components and demonstrates interesting results. However, most of them do not address client uncertainty and heterogeneity in FL systems, while appropriately decoupling neural representations is challenging and often ad hoc. In this paper, we make the first attempt to introduce a general BPFL framework to decompose and jointly learn shared and personalized uncertainty representations on statistically heterogeneous client data over time. A Bayesian federated neural network BPFed instantiates BPFL by jointly learning cross-client shared uncertainty and client-specific personalized uncertainty over statistically heterogeneous and randomly participating clients. We further involve continual updating of prior distribution in BPFed to speed up the convergence and avoid catastrophic forgetting. Theoretical analysis and guarantees are provided in addition to the experimental evaluation of BPFed against the diversified baselines.
</details>
<details>
<summary>摘要</summary>
bayesian人类化联合学习（BPFL）解决了现有的个性化联合学习（PFL）中的挑战。 BPFL 的目标是量化客户端数据中的不确定性和多样性，通过对客户端数据的统计多样性进行处理，生成不确定性表示。在 PFL 中，一些最近的初步工作提议将隐藏神经表示分解为共享和本地组成部分，并得到了有趣的结果。然而，大多数这些方法不Address client uncertainty and heterogeneity in FL systems, while appropriately decoupling neural representations is challenging and often ad hoc。在这篇论文中，我们首次提出了一个通用的BPFL框架，用于在 statistically heterogeneous client data 上 decomposing and jointly learning shared and personalized uncertainty representations。一个 Bayesian federated neural network BPFed 实现了 BPFL，并在 statistically heterogeneous and randomly participating clients 上 jointly learning cross-client shared uncertainty and client-specific personalized uncertainty。我们还包括在 BPFed 中 continual updating of prior distribution 以加速收敛和避免快速忘记。在此之外，我们还提供了理论分析和保证。
</details></li>
</ul>
<hr>
<h2 id="Explainable-machine-learning-based-prediction-model-for-diabetic-nephropathy"><a href="#Explainable-machine-learning-based-prediction-model-for-diabetic-nephropathy" class="headerlink" title="Explainable machine learning-based prediction model for diabetic nephropathy"></a>Explainable machine learning-based prediction model for diabetic nephropathy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16730">http://arxiv.org/abs/2309.16730</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jing-Mei Yin, Yang Li, Jun-Tang Xue, Guo-Wei Zong, Zhong-Ze Fang, Lang Zou</li>
<li>For: The paper aims to analyze the effect of serum metabolites on diabetic nephropathy (DN) and predict the prevalence of DN through a machine learning approach.* Methods: The dataset consists of 548 patients from April 2018 to April 2019 in Second Affiliated Hospital of Dalian Medical University (SAHDMU). The optimal 38 features were selected through a Least absolute shrinkage and selection operator (LASSO) regression model and a 10-fold cross-validation. Four machine learning algorithms, including eXtreme Gradient Boosting (XGB), random forest, decision tree, and logistic regression, were compared using AUC-ROC curves, decision curves, and calibration curves. The Shapley Additive exPlanations (SHAP) method was used to quantify feature importance and interaction effects in the optimal predictive model.* Results: The XGB model had the best performance to screen for DN with the highest AUC value of 0.966. The XGB model also gained more clinical net benefits than others and had a better fitting degree. Significant interactions between serum metabolites and duration of diabetes were found. A predictive model was developed using the XGB algorithm to screen for DN, with C2, C5DC, Tyr, Ser, Met, C24, C4DC, and Cys being the most contributing factors. These factors could potentially serve as biomarkers for DN.<details>
<summary>Abstract</summary>
The aim of this study is to analyze the effect of serum metabolites on diabetic nephropathy (DN) and predict the prevalence of DN through a machine learning approach. The dataset consists of 548 patients from April 2018 to April 2019 in Second Affiliated Hospital of Dalian Medical University (SAHDMU). We select the optimal 38 features through a Least absolute shrinkage and selection operator (LASSO) regression model and a 10-fold cross-validation. We compare four machine learning algorithms, including eXtreme Gradient Boosting (XGB), random forest, decision tree and logistic regression, by AUC-ROC curves, decision curves, calibration curves. We quantify feature importance and interaction effects in the optimal predictive model by Shapley Additive exPlanations (SHAP) method. The XGB model has the best performance to screen for DN with the highest AUC value of 0.966. The XGB model also gains more clinical net benefits than others and the fitting degree is better. In addition, there are significant interactions between serum metabolites and duration of diabetes. We develop a predictive model by XGB algorithm to screen for DN. C2, C5DC, Tyr, Ser, Met, C24, C4DC, and Cys have great contribution in the model, and can possibly be biomarkers for DN.
</details>
<details>
<summary>摘要</summary>
这项研究的目的是分析血液代谢物对 диабетичеnehropathy（DN）的影响，并预测DN的发病率通过机器学习方法。数据集包含2018年4月至2019年4月的548名病人在第二附属医院大连医科大学第二附属医院（SAHDMU）。我们选择最佳的38个特征通过Least absolute shrinkage and selection operator（LASSO）回归模型和10fold Cross-Validation。我们比较了四种机器学习算法，包括extreme Gradient Boosting（XGB）、Random Forest、决策树和Logistic Regression，通过AUC-ROC曲线、决策曲线、calibration曲线进行比较。我们使用Shapley Additive exPlanations（SHAP）方法来衡量特征重要性和交互效应在最佳预测模型中。XGB模型在DNcreening方面表现最佳，AUC值为0.966。XGB模型也在临床总效益方面表现更好，并且适应度更高。此外，血液代谢物和糖尿病患期之间存在显著交互效应。我们开发了一个XGB模型来预测DN。C2、C5DC、Tyr、Ser、Met、C24、C4DC和Cys在模型中具有重要作用，可能成为DN的生物标志物。
</details></li>
</ul>
<hr>
<h2 id="Fast-Locality-Sensitive-Hashing-with-Theoretical-Guarantee"><a href="#Fast-Locality-Sensitive-Hashing-with-Theoretical-Guarantee" class="headerlink" title="Fast Locality Sensitive Hashing with Theoretical Guarantee"></a>Fast Locality Sensitive Hashing with Theoretical Guarantee</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15479">http://arxiv.org/abs/2309.15479</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zongyuan Tan, Hongya Wang, Bo Xu, Minjie Luo, Ming Du</li>
<li>for:  nearest neighbor search task</li>
<li>methods:  random sampling and random projection</li>
<li>results:  up to 80x speedup in hash function evaluation, on par with state-of-the-arts in terms of answer quality, space occupation, and query efficiency<details>
<summary>Abstract</summary>
Locality-sensitive hashing (LSH) is an effective randomized technique widely used in many machine learning tasks. The cost of hashing is proportional to data dimensions, and thus often the performance bottleneck when dimensionality is high and the number of hash functions involved is large. Surprisingly, however, little work has been done to improve the efficiency of LSH computation. In this paper, we design a simple yet efficient LSH scheme, named FastLSH, under l2 norm. By combining random sampling and random projection, FastLSH reduces the time complexity from O(n) to O(m) (m<n), where n is the data dimensionality and m is the number of sampled dimensions. Moreover, FastLSH has provable LSH property, which distinguishes it from the non-LSH fast sketches. We conduct comprehensive experiments over a collection of real and synthetic datasets for the nearest neighbor search task. Experimental results demonstrate that FastLSH is on par with the state-of-the-arts in terms of answer quality, space occupation and query efficiency, while enjoying up to 80x speedup in hash function evaluation. We believe that FastLSH is a promising alternative to the classic LSH scheme.
</details>
<details>
<summary>摘要</summary>
“本文提出了一种简单又高效的 hash 算法，名为 FastLSH，用于 nearest neighbor search 任务。该算法通过Random Sampling 和 Random Projection 两种方法，将时间复杂度从 O(n) 降低至 O(m)，其中 n 是数据维度，m 是取样维度。此外，FastLSH 具有可证明的 LSH 性质，与非 LSH 快笔不同。我们在一个实际数据集和一些 sintetic 数据集上进行了广泛的实验，结果表明，FastLSH 与状态之前的最佳答案相当，并且具有更高的查询效率和更低的存储占用率。我们认为 FastLSH 是一种有前途的 LSH 算法。”
</details></li>
</ul>
<hr>
<h2 id="DTC-Deep-Tracking-Control-–-A-Unifying-Approach-to-Model-Based-Planning-and-Reinforcement-Learning-for-Versatile-and-Robust-Locomotion"><a href="#DTC-Deep-Tracking-Control-–-A-Unifying-Approach-to-Model-Based-Planning-and-Reinforcement-Learning-for-Versatile-and-Robust-Locomotion" class="headerlink" title="DTC: Deep Tracking Control – A Unifying Approach to Model-Based Planning and Reinforcement-Learning for Versatile and Robust Locomotion"></a>DTC: Deep Tracking Control – A Unifying Approach to Model-Based Planning and Reinforcement-Learning for Versatile and Robust Locomotion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15462">http://arxiv.org/abs/2309.15462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabian Jenelten, Junzhe He, Farbod Farshidian, Marco Hutter</li>
<li>For: 本研究旨在开发一种可靠和抗衰落的脚部机器人控制方法，以便在实际世界中提供稳定的行走能力。* Methods: 本研究使用了模型驱动的循环优化方法和学习从数据的强化学习方法，并将其结合在一起以实现更高的稳定性和抗衰落性。* Results: 研究表明，该控制方法可以在稀有的脚部支点下实现更高的精度和抗衰落性，并且在不同的跟踪优化方法下仍然可以保持稳定性。<details>
<summary>Abstract</summary>
Legged locomotion is a complex control problem that requires both accuracy and robustness to cope with real-world challenges. Legged systems have traditionally been controlled using trajectory optimization with inverse dynamics. Such hierarchical model-based methods are appealing due to intuitive cost function tuning, accurate planning, and most importantly, the insightful understanding gained from more than one decade of extensive research. However, model mismatch and violation of assumptions are common sources of faulty operation and may hinder successful sim-to-real transfer. Simulation-based reinforcement learning, on the other hand, results in locomotion policies with unprecedented robustness and recovery skills. Yet, all learning algorithms struggle with sparse rewards emerging from environments where valid footholds are rare, such as gaps or stepping stones. In this work, we propose a hybrid control architecture that combines the advantages of both worlds to simultaneously achieve greater robustness, foot-placement accuracy, and terrain generalization. Our approach utilizes a model-based planner to roll out a reference motion during training. A deep neural network policy is trained in simulation, aiming to track the optimized footholds. We evaluate the accuracy of our locomotion pipeline on sparse terrains, where pure data-driven methods are prone to fail. Furthermore, we demonstrate superior robustness in the presence of slippery or deformable ground when compared to model-based counterparts. Finally, we show that our proposed tracking controller generalizes across different trajectory optimization methods not seen during training. In conclusion, our work unites the predictive capabilities and optimality guarantees of online planning with the inherent robustness attributed to offline learning.
</details>
<details>
<summary>摘要</summary>
四肢行走控制问题是一个复杂的问题，需要同时具备准确性和鲁棒性，以满足实际世界中的挑战。传统上，四肢系统通过天体动力学优化算法进行控制，这种层次模型基于方法具有直观的成本函数调整、准确的规划和深刻的研究经验。然而，模型匹配和假设违反是常见的问题，可能会导致操作失败。相比之下，在 simulations 中进行学习 Reinforcement 学习可以获得不可思议的鲁棒性和恢复技能。然而，所有学习算法都会在罕见的有效步行面上遇到缺乏奖励的问题，如块或步行石头。在这种情况下，我们提出了一种混合控制架构，将模型基于的优化方法和学习 Reinforcement 学习相结合，以同时实现更高的鲁棒性、脚印准确性和地形泛化。我们的方法在训练中使用模型基于的规划器执行参考动作。一个深度神经网络策略在 simulated 环境中进行培训，目标是跟踪优化的脚印。我们评估了我们的行走管道的准确性在罕见的地形上，其中纯数据驱动方法容易出现问题。此外，我们还证明了我们的提议跟踪控制器在不同的轨迹优化方法下可以保持一致性。综上所述，我们的工作将线性控制和在线计划的优点相结合，同时具备预测能力和优化 garanties。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-in-Deterministic-Computational-Mechanics"><a href="#Deep-Learning-in-Deterministic-Computational-Mechanics" class="headerlink" title="Deep Learning in Deterministic Computational Mechanics"></a>Deep Learning in Deterministic Computational Mechanics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15421">http://arxiv.org/abs/2309.15421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leon Herrmann, Stefan Kollmannsberger</li>
<li>for: 本文旨在帮助计算机机械领域的研究人员更好地了解深度学习技术的应用，以便更好地探索这一领域。</li>
<li>methods: 本文描述了五种主要的深度学习方法，包括模拟替换、模拟加强、离散方法作为神经网络、生成方法和深度强化学习。</li>
<li>results: 本文的评论集中在深度学习方法，而不是应用于计算机机械领域的研究成果，以便帮助研究人员更好地了解这一领域。<details>
<summary>Abstract</summary>
The rapid growth of deep learning research, including within the field of computational mechanics, has resulted in an extensive and diverse body of literature. To help researchers identify key concepts and promising methodologies within this field, we provide an overview of deep learning in deterministic computational mechanics. Five main categories are identified and explored: simulation substitution, simulation enhancement, discretizations as neural networks, generative approaches, and deep reinforcement learning. This review focuses on deep learning methods rather than applications for computational mechanics, thereby enabling researchers to explore this field more effectively. As such, the review is not necessarily aimed at researchers with extensive knowledge of deep learning -- instead, the primary audience is researchers at the verge of entering this field or those who attempt to gain an overview of deep learning in computational mechanics. The discussed concepts are, therefore, explained as simple as possible.
</details>
<details>
<summary>摘要</summary>
“深度学习在计算机机学中的快速发展，包括计算机机学领域内的深度学习研究，已经形成了广泛和多样的文献。为帮助研究者更好地了解计算机机学领域中的深度学习方法，我们提供了深度学习在计算机机学中的概述。本文分为五个主要类别：模拟替换、模拟加强、离散为神经网络、生成方法和深度奖励学习。本文主要针对计算机机学领域的研究者，而不是深度学习专家，因此在解释概念时尽量简单明了。”
</details></li>
</ul>
<hr>
<h2 id="Automatic-Feature-Fairness-in-Recommendation-via-Adversaries"><a href="#Automatic-Feature-Fairness-in-Recommendation-via-Adversaries" class="headerlink" title="Automatic Feature Fairness in Recommendation via Adversaries"></a>Automatic Feature Fairness in Recommendation via Adversaries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15418">http://arxiv.org/abs/2309.15418</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/holdenhu/advfm">https://github.com/holdenhu/advfm</a></li>
<li>paper_authors: Hengchang Hu, Yiming Cao, Zhankui He, Samson Tan, Min-Yen Kan</li>
<li>for: The paper aims to achieve equitable treatment across diverse groups defined by various feature combinations in recommender systems, by proposing feature fairness as the foundation for practical implementation.</li>
<li>methods: The paper introduces unbiased feature learning through adversarial training, using adversarial perturbation to enhance feature representation. The authors adapt adversaries automatically based on two forms of feature biases: frequency and combination variety of feature values.</li>
<li>results: The paper shows that the proposed method, AAFM, surpasses strong baselines in both fairness and accuracy measures. AAFM excels in providing item- and user-fairness for single- and multi-feature tasks, showcasing their versatility and scalability. However, the authors find that adversarial perturbation must be well-managed during training to maintain good accuracy.Here’s the Chinese translation of the three key points:</li>
<li>for: 这篇论文目标是在推荐系统中实现多样性群体的平等待遇，通过基于特征平衡的方法来实现实用实现。</li>
<li>methods: 论文提出了不偏向特征学习方法，通过对抗训练来增强特征表示。作者自动调整了对抗器基于特征偏好的两种形式：频率偏好和组合变化。</li>
<li>results: 论文表明，提出的方法AAFM在公平度和准确度两个指标上都超越了强基eline。AAFM在单特征和多特征任务中展示出了其 universality和可扩展性。但是，作者发现，在训练过程中，对抗训练的管理是关键，以保持好的准确度。<details>
<summary>Abstract</summary>
Fairness is a widely discussed topic in recommender systems, but its practical implementation faces challenges in defining sensitive features while maintaining recommendation accuracy. We propose feature fairness as the foundation to achieve equitable treatment across diverse groups defined by various feature combinations. This improves overall accuracy through balanced feature generalizability. We introduce unbiased feature learning through adversarial training, using adversarial perturbation to enhance feature representation. The adversaries improve model generalization for under-represented features. We adapt adversaries automatically based on two forms of feature biases: frequency and combination variety of feature values. This allows us to dynamically adjust perturbation strengths and adversarial training weights. Stronger perturbations are applied to feature values with fewer combination varieties to improve generalization, while higher weights for low-frequency features address training imbalances. We leverage the Adaptive Adversarial perturbation based on the widely-applied Factorization Machine (AAFM) as our backbone model. In experiments, AAFM surpasses strong baselines in both fairness and accuracy measures. AAFM excels in providing item- and user-fairness for single- and multi-feature tasks, showcasing their versatility and scalability. To maintain good accuracy, we find that adversarial perturbation must be well-managed: during training, perturbations should not overly persist and their strengths should decay.
</details>
<details>
<summary>摘要</summary>
“公平性”是推荐系统中广泛讨论的话题，但实际实现受到定义敏感特征的挑战，以保持推荐准确性。我们提议在不同群体中实现公平待遇，通过不同特征组合来定义多样化的特征基础。这会提高总准确性，通过平衡特征泛化。我们引入无偏特征学习，通过对抗训练来增强特征表示。对抗敌对体现出来的敌人会提高模型泛化，特别是对于少见特征值。我们自动调整对抗训练的权重和强度，根据特征频率和组合多样性。更强的扰动应用于少见特征值，以提高泛化，而高权重用于低频特征值，以解决训练不平衡。我们基于广泛应用的因子分解机器（AAFM）模型，并在实验中超过了强基eline的公平和准确度度量。AAFM在单特征和多特征任务中展现出了优秀的公平和精度性能，这表明它的多样性和可扩展性。为保持好的准确度，我们发现对抗扰动需要有效管理：在训练过程中，扰动应该不会过度 persist，并且强度应该逐渐减弱。
</details></li>
</ul>
<hr>
<h2 id="Revolutionizing-Terrain-Precipitation-Understanding-through-AI-driven-Knowledge-Discovery"><a href="#Revolutionizing-Terrain-Precipitation-Understanding-through-AI-driven-Knowledge-Discovery" class="headerlink" title="Revolutionizing Terrain-Precipitation Understanding through AI-driven Knowledge Discovery"></a>Revolutionizing Terrain-Precipitation Understanding through AI-driven Knowledge Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15400">http://arxiv.org/abs/2309.15400</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Xu, Yuntian Chen, Zhenzhong Zeng, Nina Li, Jian Li, Dongxiao Zhang</li>
<li>for: 增进当前气候科学中复杂地形区域气候过程的理解，特别是全球气候变化的背景下。</li>
<li>methods: 利用先进的人工智能驱动的知识发现技术，揭示了陡峭地形特征和降水模式之间的细腻关系，揭示了过去隐藏的气候动力。</li>
<li>results: 发现了一种名为’1995转折点’的现象，表明1995年左右，气候变化的力量对陡峭地形特征和降水关系产生了重要影响。这些公式在应用于降水预测中具有实际应用价值，可以帮助我们从低分辨率未来气候数据中获得精细的降水预测结果。<details>
<summary>Abstract</summary>
Advancing our understanding of climate processes in regions characterized by intricate terrain complexity is a paramount challenge in contemporary climate science, particularly in the context of global climate change. Notably, the scarcity of observational data in these regions has imposed substantial limitations on understanding the nuanced climate dynamics therein. For the first time, utilizing cutting-edge AI-driven knowledge discovery techniques, we have uncovered explicit equations that elucidate the intricate relationship between terrain features and precipitation patterns, illuminating the previously concealed complexities governing these relationships. These equations, thus far undisclosed, exhibit remarkable accuracy compared to conventional empirical models when applied to precipitation data. Building on this foundation, we reveal a phenomenon known as the '1995 turning point,' indicating a significant shift in the terrain-precipitation relationship in approximately 1995, related to the forces of climate change. These equations have practical applications, particularly in achieving fine-scale downscaling precipitation predictions from low-resolution future climate data. This capability provides invaluable insights into the expected changes in precipitation patterns across diverse terrains under future climate scenarios.
</details>
<details>
<summary>摘要</summary>
当前气候科学中解决复杂地形区域内气候过程的挑战非常大，尤其在全球气候变化背景下。尽管 Observational data 在这些区域scarce，这些区域的气候动力学尚未得到了深入理解。为了解决这个问题，我们首次采用了前沿的 AI驱动知识发现技术，揭示了 terrain features 和降水模式之间的Explicit equations，揭示了之前隐藏的复杂关系。这些方程在与 convential empirical models 比较时显示了惊人的准确性。基于这个基础，我们发现了1995年的“转折点”，表明在约1995年发生了气候变化的力量对 terrain-precipitation 关系的重要影响。这些方程在实践中具有重要的应用价值，特别是在将来气候数据的低分辨率预测降水情况的细致下calibration。这种能力为不同地形下预计的降水模式变化的预期提供了不可或缺的信息。
</details></li>
</ul>
<hr>
<h2 id="Model-Free-Regret-Optimal-Best-Policy-Identification-in-Online-CMDPs"><a href="#Model-Free-Regret-Optimal-Best-Policy-Identification-in-Online-CMDPs" class="headerlink" title="Model-Free, Regret-Optimal Best Policy Identification in Online CMDPs"></a>Model-Free, Regret-Optimal Best Policy Identification in Online CMDPs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15395">http://arxiv.org/abs/2309.15395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihan Zhou, Honghao Wei, Lei Ying</li>
<li>for: 这 paper 考虑了在线Constrained Markov Decision Processes (CMDPs) 中的最佳策略识别 (BPI) 问题。</li>
<li>methods: 该 paper 提出了一种基于 Koole (1988) 和 Ross (1989) 的基本结构性质的新算法（名为 Pruning-Refinement-Identification，简称 PRI），用于识别 Near-optimal 策略。</li>
<li>results: PRI 算法可以在线 CMDPs 中实现 trio 目标：(i) PRI 是一种 model-free 算法; (ii) PRI 输出一个 near-optimal 策略，并且在学习结束时达到高概率; (iii) 在图表设置下，PRI 保证了 $\tilde{\mathcal{O}(\sqrt{K})$ 的 regret和约束违反，这significantly 超过了现有的最佳 regret  bound $\tilde{\mathcal{O}(K^{\frac{4}{5}})$ under model-free algorithm, where $K$ 是总共话集数。<details>
<summary>Abstract</summary>
This paper considers the best policy identification (BPI) problem in online Constrained Markov Decision Processes (CMDPs). We are interested in algorithms that are model-free, have low regret, and identify an optimal policy with a high probability. Existing model-free algorithms for online CMDPs with sublinear regret and constraint violation do not provide any convergence guarantee to an optimal policy and provide only average performance guarantees when a policy is uniformly sampled at random from all previously used policies. In this paper, we develop a new algorithm, named Pruning-Refinement-Identification (PRI), based on a fundamental structural property of CMDPs proved in Koole(1988); Ross(1989), which we call limited stochasticity. The property says for a CMDP with $N$ constraints, there exists an optimal policy with at most $N$ stochastic decisions.   The proposed algorithm first identifies at which step and in which state a stochastic decision has to be taken and then fine-tunes the distributions of these stochastic decisions. PRI achieves trio objectives: (i) PRI is a model-free algorithm; and (ii) it outputs a near-optimal policy with a high probability at the end of learning; and (iii) in the tabular setting, PRI guarantees $\tilde{\mathcal{O}(\sqrt{K})$ regret and constraint violation, which significantly improves the best existing regret bound $\tilde{\mathcal{O}(K^{\frac{4}{5})$ under a model-free algorithm, where $K$ is the total number of episodes.
</details>
<details>
<summary>摘要</summary>
Our proposed algorithm, Pruning-Refinement-Identification (PRI), is based on a fundamental structural property of CMDPs proven in Koole (1988) and Ross (1989), which we call limited stochasticity. This property states that for a CMDP with N constraints, there exists an optimal policy with at most N stochastic decisions.PRI first identifies the steps and states where stochastic decisions need to be taken and then fine-tunes the distributions of these decisions. Our algorithm achieves three objectives:1. PRI is a model-free algorithm.2. It outputs a near-optimal policy with a high probability at the end of learning.3. In the tabular setting, PRI guarantees $\tilde{\mathcal{O}(\sqrt{K})$ regret and constraint violation, which significantly improves the best existing regret bound $\tilde{\mathcal{O}(K^{\frac{4}{5})$ under a model-free algorithm, where $K$ is the total number of episodes.
</details></li>
</ul>
<hr>
<h2 id="ADGym-Design-Choices-for-Deep-Anomaly-Detection"><a href="#ADGym-Design-Choices-for-Deep-Anomaly-Detection" class="headerlink" title="ADGym: Design Choices for Deep Anomaly Detection"></a>ADGym: Design Choices for Deep Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15376">http://arxiv.org/abs/2309.15376</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/minqi824/adgym">https://github.com/minqi824/adgym</a></li>
<li>paper_authors: Minqi Jiang, Chaochuan Hou, Ao Zheng, Songqiao Han, Hailiang Huang, Qingsong Wen, Xiyang Hu, Yue Zhao</li>
<li>for: 本文旨在探讨深度学习方法中的异常检测问题，并提出了两个关键问题：（1）深度异常检测方法中的设计选择对检测异常的影响是多大？（2）如何自动选择适合的设计选择来优化异常检测模型？</li>
<li>methods: 本文提出了一个名为ADGym的平台，用于全面评估和自动选择深度异常检测方法中的设计元素。</li>
<li>results: 经过广泛的实验，结果表明，solely relying on现有领先方法并不够，而使用ADGym提出的模型则显著超越了当前领先技术。<details>
<summary>Abstract</summary>
Deep learning (DL) techniques have recently found success in anomaly detection (AD) across various fields such as finance, medical services, and cloud computing. However, most of the current research tends to view deep AD algorithms as a whole, without dissecting the contributions of individual design choices like loss functions and network architectures. This view tends to diminish the value of preliminary steps like data preprocessing, as more attention is given to newly designed loss functions, network architectures, and learning paradigms. In this paper, we aim to bridge this gap by asking two key questions: (i) Which design choices in deep AD methods are crucial for detecting anomalies? (ii) How can we automatically select the optimal design choices for a given AD dataset, instead of relying on generic, pre-existing solutions? To address these questions, we introduce ADGym, a platform specifically crafted for comprehensive evaluation and automatic selection of AD design elements in deep methods. Our extensive experiments reveal that relying solely on existing leading methods is not sufficient. In contrast, models developed using ADGym significantly surpass current state-of-the-art techniques.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:深度学习（DL）技术在不同领域如金融、医疗服务和云计算中得到了成功，但大多数当前研究仅视深度异常检测（AD）算法为一整体，没有分析个别设计选择的贡献，如损失函数和网络架构。这种视角减少了数据预处理的价值，更多地关注新设计的损失函数、网络架构和学习方法。在这篇论文中，我们想要填补这个差距，问两个关键问题：（i）深度AD方法中哪些设计选择对异常检测有着关键作用？（ii）如何自动选择给定AD数据集中最佳的设计选择，而不是依靠现有的、通用解决方案？为此，我们介绍了ADGym，一个专门为深度AD方法的评估和自动选择设计元素而设计的平台。我们的广泛实验表明，仅仅靠现有的领先方法是不够的。相反，使用ADGym开发的模型在当前领先技术上显著超越。
</details></li>
</ul>
<hr>
<h2 id="PPG-to-ECG-Signal-Translation-for-Continuous-Atrial-Fibrillation-Detection-via-Attention-based-Deep-State-Space-Modeling"><a href="#PPG-to-ECG-Signal-Translation-for-Continuous-Atrial-Fibrillation-Detection-via-Attention-based-Deep-State-Space-Modeling" class="headerlink" title="PPG to ECG Signal Translation for Continuous Atrial Fibrillation Detection via Attention-based Deep State-Space Modeling"></a>PPG to ECG Signal Translation for Continuous Atrial Fibrillation Detection via Attention-based Deep State-Space Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15375">http://arxiv.org/abs/2309.15375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khuong Vo, Mostafa El-Khamy, Yoojin Choi</li>
<li>for: 这个研究旨在提出一种可以将心跳信号转换为电击血液信号的潜在独立注意力深度状态模型，以实现不需要严格的临床测量，并且可以在日常生活中监测心跳信号。</li>
<li>methods: 这个模型使用了非侵入式、低成本的光学方法，并且运用了潜在注意力技术，实现了将心跳信号转换为电击血液信号的目的。</li>
<li>results: 这个研究的实验结果显示，这种方法可以实现高效率的数据训练，并且可以实现与电击血液信号的对应关系。此外，这种方法还可以检测成人心跳过速症（AFib），并且可以辅助电击血液信号的检测。<details>
<summary>Abstract</summary>
An electrocardiogram (ECG or EKG) is a medical test that measures the heart's electrical activity. ECGs are often used to diagnose and monitor a wide range of heart conditions, including arrhythmias, heart attacks, and heart failure. On the one hand, the conventional ECG requires clinical measurement, which restricts its deployment to medical facilities. On the other hand, single-lead ECG has become popular on wearable devices using administered procedures. An alternative to ECG is Photoplethysmography (PPG), which uses non-invasive, low-cost optical methods to measure cardiac physiology, making it a suitable option for capturing vital heart signs in daily life. As a result, it has become increasingly popular in health monitoring and is used in various clinical and commercial wearable devices. While ECG and PPG correlate strongly, the latter does not offer significant clinical diagnostic value. Here, we propose a subject-independent attention-based deep state-space model to translate PPG signals to corresponding ECG waveforms. The model is highly data-efficient by incorporating prior knowledge in terms of probabilistic graphical models. Notably, the model enables the detection of atrial fibrillation (AFib), the most common heart rhythm disorder in adults, by complementing ECG's accuracy with continuous PPG monitoring. We evaluated the model on 55 subjects from the MIMIC III database. Quantitative and qualitative experimental results demonstrate the effectiveness and efficiency of our approach.
</details>
<details>
<summary>摘要</summary>
一个电cardiogram (ECG或EKG)是医疗测试，测量心脏的电动活动。ECG通常用于诊断和监测各种心脏疾病，包括cardiac arrhythmias, heart attacks, 和heart failure。一方面，普通的ECG需要临床测量，这限制了其部署到医疗设施中。另一方面，单导ECG在佩戴设备上使用了管理的程序。一种代替ECG的是光 Plethysmography (PPG)，它使用非侵入的、低成本的光学方法测量心脏生理，因此成为了日常生活中捕捉重要的心脏指标的合适选择。由于PPG和ECG之间强相关，因此PPG可以用于补充ECG的诊断价值。在这种情况下，我们提议一种主体无关的注意力基于深度状态空间模型，将PPG信号翻译成对应的ECG波形。该模型具有高度数据效率，通过 incorporating prior knowledge in terms of probabilistic graphical models。特别是，该模型可以通过补充ECG的精度，检测成人最常见的心脏rhythm疾病（cardiac arrhythmias）。我们在MIMIC III数据库上测试了55名参与者。量化和质量实验结果表明我们的方法的效果和效率。
</details></li>
</ul>
<hr>
<h2 id="Density-Estimation-via-Measure-Transport-Outlook-for-Applications-in-the-Biological-Sciences"><a href="#Density-Estimation-via-Measure-Transport-Outlook-for-Applications-in-the-Biological-Sciences" class="headerlink" title="Density Estimation via Measure Transport: Outlook for Applications in the Biological Sciences"></a>Density Estimation via Measure Transport: Outlook for Applications in the Biological Sciences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15366">http://arxiv.org/abs/2309.15366</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vanessa Lopez-Marrero, Patrick R. Johnstone, Gilchan Park, Xihaier Luo</li>
<li>for: 本研究用于支持生物科学研究，尤其是在射线生物学领域，where data is scarce.</li>
<li>methods: 本研究使用度量运输方法，specifically using triangular transport maps, to process and analyze data distributed according to a wide class of probability measures.</li>
<li>results: 研究发现，当数据稀缺时，使用稀缺运输地图有利，可以找到隐藏在数据中的信息，如果某些基因之间的关系和它们的动态变化。<details>
<summary>Abstract</summary>
One among several advantages of measure transport methods is that they allow for a unified framework for processing and analysis of data distributed according to a wide class of probability measures. Within this context, we present results from computational studies aimed at assessing the potential of measure transport techniques, specifically, the use of triangular transport maps, as part of a workflow intended to support research in the biological sciences. Scarce data scenarios, which are common in domains such as radiation biology, are of particular interest. We find that when data is scarce, sparse transport maps are advantageous. In particular, statistics gathered from computing series of (sparse) adaptive transport maps, trained on a series of randomly chosen subsets of the set of available data samples, leads to uncovering information hidden in the data. As a result, in the radiation biology application considered here, this approach provides a tool for generating hypotheses about gene relationships and their dynamics under radiation exposure.
</details>
<details>
<summary>摘要</summary>
一种多种优点的度量运输方法是它们允许在一个广泛的概率测度下进行数据处理和分析。在这个上下文中，我们介绍了计算研究，以评估度量运输技术的潜在优势，具体来说是使用三角形运输地图。在具有稀缺数据的情况下，这种方法特别有优势。我们发现，当数据稀缺时，稀缺运输地图可以暴露数据中隐藏的信息。因此，在考虑的辐射生物学应用中，这种方法可以提供一种生成关于基因关系和它们在辐射暴露下的动态变化的假设的工具。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Learned-Representations-of-Neural-Networks-with-Principal-Component-Analysis"><a href="#Exploring-Learned-Representations-of-Neural-Networks-with-Principal-Component-Analysis" class="headerlink" title="Exploring Learned Representations of Neural Networks with Principal Component Analysis"></a>Exploring Learned Representations of Neural Networks with Principal Component Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15328">http://arxiv.org/abs/2309.15328</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amit Harlev, Andrew Engel, Panos Stinis, Tony Chiang</li>
<li>for: 本研究探讨深度神经网络（DNN）中的特征表示方法，以提高Explainable AI领域的理解。</li>
<li>methods: 本研究使用主成分分析（PCA）来研究一个ResNet-18模型在CIFAR-10数据集上的各层特征表示，并利用k- nearest neighbors分类器（k-NN）、最近类中心分类器（NCC）和支持向量机来评估这些特征表示的性能。</li>
<li>results: 研究发现，在某些层次上，只需20%的中间特征空间差异即可达到高精度分类，而且在所有层次上，前100个特征空间差异可以完全确定k-NN和NCC分类器的性能。研究还发现了神经萧略现象和中间神经萧略现象的关系，并提供了三种不同 yet可解释的特征表示模型，其中一种是一个Affine linear model，性能最佳。此外，研究还显示了可以利用多种特征表示模型来估计DNN中的神经萧略现象发生的位置。<details>
<summary>Abstract</summary>
Understanding feature representation for deep neural networks (DNNs) remains an open question within the general field of explainable AI. We use principal component analysis (PCA) to study the performance of a k-nearest neighbors classifier (k-NN), nearest class-centers classifier (NCC), and support vector machines on the learned layer-wise representations of a ResNet-18 trained on CIFAR-10. We show that in certain layers, as little as 20% of the intermediate feature-space variance is necessary for high-accuracy classification and that across all layers, the first ~100 PCs completely determine the performance of the k-NN and NCC classifiers. We relate our findings to neural collapse and provide partial evidence for the related phenomenon of intermediate neural collapse. Our preliminary work provides three distinct yet interpretable surrogate models for feature representation with an affine linear model the best performing. We also show that leveraging several surrogate models affords us a clever method to estimate where neural collapse may initially occur within the DNN.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language: zh-CNUnderstanding deep neural network (DNN) feature representation remains an open question in the broader field of explainable AI. We use principal component analysis (PCA) to study the performance of k-nearest neighbors classifiers (k-NN), nearest class-centers classifiers (NCC), and support vector machines on the layer-wise representations of a ResNet-18 trained on CIFAR-10. We find that in certain layers, as little as 20% of the intermediate feature-space variance is sufficient for high-accuracy classification, and that the first ~100 principal components (PCs) completely determine the performance of the k-NN and NCC classifiers across all layers. Our findings are related to the phenomenon of neural collapse, and we provide partial evidence for the related phenomenon of intermediate neural collapse. Our preliminary work provides three distinct yet interpretable surrogate models for feature representation, with an affine linear model being the best performing. We also show that leveraging multiple surrogate models allows us to estimate where neural collapse may initially occur within the DNN.
</details></li>
</ul>
<hr>
<h2 id="Neural-Operators-for-Accelerating-Scientific-Simulations-and-Design"><a href="#Neural-Operators-for-Accelerating-Scientific-Simulations-and-Design" class="headerlink" title="Neural Operators for Accelerating Scientific Simulations and Design"></a>Neural Operators for Accelerating Scientific Simulations and Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15325">http://arxiv.org/abs/2309.15325</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kamyar Azizzadenesheli, Nikola Kovachki, Zongyi Li, Miguel Liu-Schiaffini, Jean Kossaifi, Anima Anandkumar</li>
<li>for: 代替physical experiments的数字实验方法，提高科学发现和工程设计的效率和成本。</li>
<li>methods: 使用人工智能技术，特别是神经网络模型，学习函数的映射，可以在新的位置上预测和拟合解决方案。</li>
<li>results: 可以在computational fluid dynamics、天气预测和材料模型等领域中快速替代或补充现有的模拟器，提高解决方案的精度和普遍性。<details>
<summary>Abstract</summary>
Scientific discovery and engineering design are currently limited by the time and cost of physical experiments, selected mostly through trial-and-error and intuition that require deep domain expertise. Numerical simulations present an alternative to physical experiments but are usually infeasible for complex real-world domains due to the computational requirements of existing numerical methods. Artificial intelligence (AI) presents a potential paradigm shift by developing fast data-driven surrogate models. In particular, an AI framework, known as neural operators, presents a principled framework for learning mappings between functions defined on continuous domains, e.g., spatiotemporal processes and partial differential equations (PDE). They can extrapolate and predict solutions at new locations unseen during training, i.e., perform zero-shot super-resolution. Neural operators can augment or even replace existing simulators in many applications, such as computational fluid dynamics, weather forecasting, and material modeling, while being 4-5 orders of magnitude faster. Further, neural operators can be integrated with physics and other domain constraints enforced at finer resolutions to obtain high-fidelity solutions and good generalization. Since neural operators are differentiable, they can directly optimize parameters for inverse design and other inverse problems. We believe that neural operators present a transformative approach to simulation and design, enabling rapid research and development.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="On-the-Power-of-SVD-in-the-Stochastic-Block-Model"><a href="#On-the-Power-of-SVD-in-the-Stochastic-Block-Model" class="headerlink" title="On the Power of SVD in the Stochastic Block Model"></a>On the Power of SVD in the Stochastic Block Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15322">http://arxiv.org/abs/2309.15322</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyu Mao, Jiapeng Zhang</li>
<li>for: 该研究旨在解释spectral方法在归一化 clustering问题中的行为，并在 Stochastic Block Model (SBM) 中研究vanilla-SVD算法的力量。</li>
<li>methods: 该研究使用了vanilla-SVD算法来解决归一化 clustering问题，并在symmetric设定下确认了该算法可以正确地回归所有团。</li>
<li>results: 研究发现，在symmetric设定下，vanilla-SVD算法可以准确地回归所有团，解答了Van Vu（Combinatorics Probability and Computing, 2018）在symmetric设定下提出的开问。<details>
<summary>Abstract</summary>
A popular heuristic method for improving clustering results is to apply dimensionality reduction before running clustering algorithms. It has been observed that spectral-based dimensionality reduction tools, such as PCA or SVD, improve the performance of clustering algorithms in many applications. This phenomenon indicates that spectral method not only serves as a dimensionality reduction tool, but also contributes to the clustering procedure in some sense. It is an interesting question to understand the behavior of spectral steps in clustering problems.   As an initial step in this direction, this paper studies the power of vanilla-SVD algorithm in the stochastic block model (SBM). We show that, in the symmetric setting, vanilla-SVD algorithm recovers all clusters correctly. This result answers an open question posed by Van Vu (Combinatorics Probability and Computing, 2018) in the symmetric setting.
</details>
<details>
<summary>摘要</summary>
一种广泛使用的规则方法是在运行聚类算法之前应用维度减少方法。已经观察到 spectral-based维度减少工具，如PCA或SVD，在许多应用中提高 clustering 算法的性能。这种现象表明 spectral 方法不仅 serves as a维度减少工具，还在某种意义上对 clustering 过程做出了贡献。这是一个有趣的问题，要理解 spectral 步骤在 clustering 问题中的行为。为了解答这个问题，这篇论文研究了 vanilla-SVD 算法在随机块模型（SBM）中的力量。我们显示，在对称设定下，vanilla-SVD 算法可以正确地回归所有群。这个结果回答了 Van Vu（Combinatorics Probability and Computing, 2018）在对称设定下提出的一个开Question。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/27/cs.LG_2023_09_27/" data-id="clp89dohf00sci7880y8v9u47" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_09_27" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/27/eess.IV_2023_09_27/" class="article-date">
  <time datetime="2023-09-27T09:00:00.000Z" itemprop="datePublished">2023-09-27</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/27/eess.IV_2023_09_27/">eess.IV - 2023-09-27</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Shallow-learning-enables-real-time-inference-of-molecular-composition-changes-from-broadband-near-infrared-spectroscopy-of-brain-tissue"><a href="#Shallow-learning-enables-real-time-inference-of-molecular-composition-changes-from-broadband-near-infrared-spectroscopy-of-brain-tissue" class="headerlink" title="Shallow learning enables real-time inference of molecular composition changes from broadband-near-infrared spectroscopy of brain tissue"></a>Shallow learning enables real-time inference of molecular composition changes from broadband-near-infrared spectroscopy of brain tissue</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16735">http://arxiv.org/abs/2309.16735</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ivan Ezhov, Luca Giannoni, Ivan Iliash, Felix Hsieh, Charly Caredda, Fred Lange, Ilias Tachtsidis, Daniel Rueckert</li>
<li>for: 这项研究的目的是为了开发一种快速的分子组成变化检测技术，用于实时监测生物体组织的功能和结构属性。</li>
<li>methods: 该研究使用了谱分法和深度学习方法来连接谱分图与分子浓度，以实现快速的分子组成检测。</li>
<li>results: 研究结果表明，提posed方法可以实现实时的分子组成检测，并且维持了传统优化方法的准确性。<details>
<summary>Abstract</summary>
Spectroscopy-based imaging modalities such as near-infrared spectroscopy (NIRS) and hyperspectral imaging (HSI) represent a promising alternative for low-cost, non-invasive, and fast monitoring of functional and structural properties of living tissue. Particularly, the possibility of extracting the molecular composition of the tissue from the optical spectra in real-time deems the spectroscopy techniques as unique diagnostic tools. However, due to the highly limited availability of paired optical and molecular profiling studies, building a mapping between a spectral signature and a corresponding set of molecular concentrations is still an unsolved problem. Moreover, there are no yet established methods to streamline inference of the biochemical composition from the optical spectrum for real-time applications such as surgical monitoring. In this paper, we develop a technique for fast inference of changes in the molecular composition of brain tissue. We base our method on the Beer-Lambert law to analytically connect the spectra with concentrations and use a deep-learning approach to significantly speed up the concentration inference compared to traditional optimization methods. We test our approach on real data obtained from the broadband NIRS study of piglets' brains. The results demonstrate that the proposed method enables real-time molecular composition inference while maintaining the accuracy of traditional optimization.
</details>
<details>
<summary>摘要</summary>
Spectroscopy-based imaging modalities such as near-infrared spectroscopy (NIRS) and hyperspectral imaging (HSI) represent a promising alternative for low-cost, non-invasive, and fast monitoring of functional and structural properties of living tissue. Particularly, the possibility of extracting the molecular composition of the tissue from the optical spectra in real-time deems the spectroscopy techniques as unique diagnostic tools. However, due to the highly limited availability of paired optical and molecular profiling studies, building a mapping between a spectral signature and a corresponding set of molecular concentrations is still an unsolved problem. Moreover, there are no yet established methods to streamline inference of the biochemical composition from the optical spectrum for real-time applications such as surgical monitoring. In this paper, we develop a technique for fast inference of changes in the molecular composition of brain tissue. We base our method on the Beer-Lambert law to analytically connect the spectra with concentrations and use a deep-learning approach to significantly speed up the concentration inference compared to traditional optimization methods. We test our approach on real data obtained from the broadband NIRS study of piglets' brains. The results demonstrate that the proposed method enables real-time molecular composition inference while maintaining the accuracy of traditional optimization.Here's the translation in Traditional Chinese:这些基于spectroscopy的内部成像技术，如近赤外谱спектроскопи (NIRS) 和高分谱成像 (HSI)，具有低成本、非侵入性和快速监控生物组织功能和结构性的优点。特别是可以从光谱中提取生物组织的分子结构，使这些技术成为独特的诊断工具。然而，由于生物组织光谱对照的数据很少，因此建立光谱特征和相应的分子浓度之间的映射仍然是一个未解的问题。此外，还没有建立了将光谱特征转换为生物化学成分的方法，对于实时应用如手术监控来说，这是一个重要的障碍。在这篇论文中，我们开发了一种快速测量生物组织分子浓度变化的方法。我们基于Beer-Lambert法，使用深度学习方法来快速测量分子浓度，与传统优化方法相比，提高了速度和准确性。我们使用实验数据，从猪脑的宽带NIRS研究中获得的数据进行评估。结果显示，我们的方法可以在实时监控中提供生物化学成分的准确测量，而且与传统优化方法相比，速度更快。
</details></li>
</ul>
<hr>
<h2 id="X-ray-dark-field-via-spectral-propagation-based-imaging"><a href="#X-ray-dark-field-via-spectral-propagation-based-imaging" class="headerlink" title="X-ray dark-field via spectral propagation-based imaging"></a>X-ray dark-field via spectral propagation-based imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15874">http://arxiv.org/abs/2309.15874</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jannis N. Ahlers, Konstantin M. Pavlov, Marcus J. Kitchen, Kaye S. Morgan</li>
<li>for: 这篇论文旨在描述一种新的黑场X射线成像技术，可以观察到不可分解的微结构。</li>
<li>methods: 这种技术使用了媒介频率场的模拟来恢复媒介频率场的相位信息。</li>
<li>results: 通过对双能X射线数据使用PBI暗场恢复算法，成功地获得了预先知道的黑场 спектраль依赖性。<details>
<summary>Abstract</summary>
Dark-field X-ray imaging is a novel modality which visualises scattering from unresolved microstructure. Current dark-field imaging techniques typically require precision optics in a stable environment. Propagation-based imaging (PBI) is an optics-free phase-contrast imaging technique that can be used to recover phase information by modelling the propagation of a diffracted wavefield. Based on the Fokker--Planck equation of X-ray imaging, we propose a dual-energy PBI approach to capture phase and dark-field effects. The equation is solved under conditions of a single-material sample with spatially slowly-varying dark-field signal, together with an a priori dark-field spectral dependence. We use single-grid dark-field imaging to fit a power law to the dark-field spectral dependence, and successfully apply the PBI dark-field retrieval algorithm to simulated and experimental dual-energy data.
</details>
<details>
<summary>摘要</summary>
黑场X射影像是一种新的显像技术，可以视化杂点不可分解的微结构。现有的黑场图像技术通常需要精炼的仪器，并在稳定的环境中进行。基于干涉图像（PBI）是一种无仪器phaseless相对幅影像技术，可以重建相对幅信息，通过模拟干涉波场的传播。基于X射影像的福克尔-普兰克方程，我们提出了双能量PBI方法，可以捕捉相对幅和黑场效应。方程在单材料样本中，空间上慢慢变化的黑场响应下解，并且使用单格黑场图像进行适应。我们成功应用PBI黑场恢复算法于实验和仿真的双能量数据。
</details></li>
</ul>
<hr>
<h2 id="DREAM-PCD-Deep-Reconstruction-and-Enhancement-of-mmWave-Radar-Pointcloud"><a href="#DREAM-PCD-Deep-Reconstruction-and-Enhancement-of-mmWave-Radar-Pointcloud" class="headerlink" title="DREAM-PCD: Deep Reconstruction and Enhancement of mmWave Radar Pointcloud"></a>DREAM-PCD: Deep Reconstruction and Enhancement of mmWave Radar Pointcloud</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15374">http://arxiv.org/abs/2309.15374</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ruixv/RadarEyes">https://github.com/ruixv/RadarEyes</a></li>
<li>paper_authors: Ruixu Geng, Yadong Li, Dongheng Zhang, Jincheng Wu, Yating Gao, Yang Hu, Yan Chen</li>
<li>for: 提高mm波射频雷达点云重建质量和实时性</li>
<li>methods: 结合信号处理和深度学习方法，提出DREAM-PCD框架，包括非协方均积、Synthetic Aperture Accumulation和实际噪声去除等三大组成部分</li>
<li>results: DREAM-PCD在重建质量和泛化性能方面胜过现有方法，并且具有优秀的实时性和可扩展性，适用于多种场景和参数。<details>
<summary>Abstract</summary>
Millimeter-wave (mmWave) radar pointcloud offers attractive potential for 3D sensing, thanks to its robustness in challenging conditions such as smoke and low illumination. However, existing methods failed to simultaneously address the three main challenges in mmWave radar pointcloud reconstruction: specular information lost, low angular resolution, and strong interference and noise. In this paper, we propose DREAM-PCD, a novel framework that combines signal processing and deep learning methods into three well-designed components to tackle all three challenges: Non-Coherent Accumulation for dense points, Synthetic Aperture Accumulation for improved angular resolution, and Real-Denoise Multiframe network for noise and interference removal. Moreover, the causal multiframe and "real-denoise" mechanisms in DREAM-PCD significantly enhance the generalization performance. We also introduce RadarEyes, the largest mmWave indoor dataset with over 1,000,000 frames, featuring a unique design incorporating two orthogonal single-chip radars, lidar, and camera, enriching dataset diversity and applications. Experimental results demonstrate that DREAM-PCD surpasses existing methods in reconstruction quality, and exhibits superior generalization and real-time capabilities, enabling high-quality real-time reconstruction of radar pointcloud under various parameters and scenarios. We believe that DREAM-PCD, along with the RadarEyes dataset, will significantly advance mmWave radar perception in future real-world applications.
</details>
<details>
<summary>摘要</summary>
Millimeter-wave (mmWave) 雷达点云提供了有前途的潜在应用，因为它在烟雾和低照明条件下具有强度。然而，现有的方法无法同时解决三大挑战在 mmWave 雷达点云重建中：损失的 Specular 信息、低角分辨率和强大的干扰和噪声。在这篇论文中，我们提出了 DREAM-PCD，一种新的框架，它将信号处理和深度学习方法结合在三个良好设计的组件中，以解决这三个挑战：非 coherent accumulation for dense points，synthetic aperture accumulation for improved angular resolution，和Real-Denoise Multiframe network for noise and interference removal。此外，DREAM-PCD 中的 causal multiframe 和 "real-denoise" 机制 significantly enhance the generalization performance。我们还介绍了 RadarEyes，最大 mmWave indoor dataset，包含超过 1,000,000 帧，其特殊设计包括两个垂直的单芯片雷达、激光和相机，这使得数据集的多样性和应用更加丰富。实验结果表明，DREAM-PCD 的重建质量胜过现有方法，并且具有优秀的普适和实时特性，可以在多种参数和场景下实现高质量的实时重建。我们认为，DREAM-PCD 和 RadarEyes 数据集将在未来实际应用中为 mmWave 雷达感知带来很大的进步。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/27/eess.IV_2023_09_27/" data-id="clp89doo901aui7882we9bhoq" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_09_27" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/27/eess.SP_2023_09_27/" class="article-date">
  <time datetime="2023-09-27T08:00:00.000Z" itemprop="datePublished">2023-09-27</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/27/eess.SP_2023_09_27/">eess.SP - 2023-09-27</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Optimal-Receive-Filter-Design-for-Misaligned-Over-the-Air-Computation"><a href="#Optimal-Receive-Filter-Design-for-Misaligned-Over-the-Air-Computation" class="headerlink" title="Optimal Receive Filter Design for Misaligned Over-the-Air Computation"></a>Optimal Receive Filter Design for Misaligned Over-the-Air Computation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16033">http://arxiv.org/abs/2309.16033</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/henrikhellstrom93/filteraircomp">https://github.com/henrikhellstrom93/filteraircomp</a></li>
<li>paper_authors: Henrik Hellström, Saeed Razavikia, Viktoria Fodor, Carlo Fischione</li>
<li>for: 这篇论文探讨了无线通信方法over-the-air computation（OAC）在密集无线网络中数据聚合方面的潜力。</li>
<li>methods: 该论文使用了信号superposition来计算多个同时发送的信号中的函数。然而，时间和频率偏移会对函数计算质量产生重要影响。</li>
<li>results: 研究人员发现，传统的匹配滤波器不能生成最佳结果，会导致函数估计偏移。为了解决这个问题，他们提出了一种新的滤波器设计，并证明在最大时延 bound下，可以实现不偏向的函数估计。此外，他们还提出了一种nikonov regularization问题，可以根据函数估计偏移和噪声induced variance之间的质量衡量来生成优化的滤波器。当时延比发送波的长度较短时，该滤波器与匹配滤波器相比，在 fonction estimates中具有更好的性能。<details>
<summary>Abstract</summary>
Over-the-air computation (OAC) is a promising wireless communication method for aggregating data from many devices in dense wireless networks. The fundamental idea of OAC is to exploit signal superposition to compute functions of multiple simultaneously transmitted signals. However, the time- and phase-alignment of these superimposed signals have a significant effect on the quality of function computation. In this study, we analyze the OAC problem for a system with unknown random time delays and phase shifts. We show that the classical matched filter does not produce optimal results, and generates bias in the function estimates. To counteract this, we propose a new filter design and show that, under a bound on the maximum time delay, it is possible to achieve unbiased function computation. Additionally, we propose a Tikhonov regularization problem that produces an optimal filter given a tradeoff between the bias and noise-induced variance of the function estimates. When the time delays are long compared to the length of the transmitted pulses, our filter vastly outperforms the matched filter both in terms of bias and mean-squared error (MSE). For shorter time delays, our proposal yields similar MSE as the matched filter, while reducing the bias.
</details>
<details>
<summary>摘要</summary>
无线计算在空气中（OAC）是一种有前途的无线通信方法，用于从多个设备收集数据 dense wireless networks。 OAC 的基本思想是利用信号积加来计算多个同时发送的信号中的函数。然而，时间和频率对这些积加的信号的影响非常大。在本研究中，我们分析了 OAC 问题，包括未知随机时延和相位偏移。我们表明了经典匹配滤波器不会生成最佳结果，并产生了函数估计的偏移。为了解决这个问题，我们提议了一种新的滤波器设计，并证明在最大时延 bound 下，可以实现不偏函数估计。此外，我们提出了一个提高了函数估计准确性的nikonov regularization 问题。当时延较长，我们的滤波器在对比匹配滤波器时表现出了显著的优异，包括偏移和 Mean Squared Error （MSE）。当时延较短时，我们的提议与匹配滤波器相当，而且可以减少偏移。
</details></li>
</ul>
<hr>
<h2 id="Channel-Estimation-for-Reconfigurable-Intelligent-Surface-Aided-Multiuser-Communication-Systems-Exploiting-Statistical-CSI-of-Correlated-RIS-User-Channels"><a href="#Channel-Estimation-for-Reconfigurable-Intelligent-Surface-Aided-Multiuser-Communication-Systems-Exploiting-Statistical-CSI-of-Correlated-RIS-User-Channels" class="headerlink" title="Channel Estimation for Reconfigurable Intelligent Surface-Aided Multiuser Communication Systems Exploiting Statistical CSI of Correlated RIS-User Channels"></a>Channel Estimation for Reconfigurable Intelligent Surface-Aided Multiuser Communication Systems Exploiting Statistical CSI of Correlated RIS-User Channels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16029">http://arxiv.org/abs/2309.16029</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haochen Li, Zhiwen Pan, Bin Wang, Nan Liu, Xiaohu You</li>
<li>for: 本研究探讨了基于多用户多输入单出口（MISO）通信系统的具有归一化特征的RIS帮助系统中的通道估算问题。</li>
<li>methods: 本文提出了一种循环域通道模型来描述RIS用户通道之间的相关性，并提出了一种重复试验方案来减少试验束缚和分解通道估算问题。最后，通过利用RIS用户通道之间的相关性，提出了一种特征值 projetion（EP）算法来解决每个子问题。</li>
<li>results: 实验结果表明，提议的EP通道估算方案可以在减少试验束缚的情况下实现准确的通道估算。<details>
<summary>Abstract</summary>
Reconfigurable intelligent surface (RIS) is a promising candidate technology for the upcoming Sixth Generation (6G) communication system for its ability to manipulate the wireless communication environment by controlling the coefficients of reflection elements (REs). However, since the RIS usually consists of a large number of passive REs, the pilot overhead for channel estimation in the RIS-aided system is prohibitively high. In this paper, the channel estimation problem for a RIS-aided multi-user multiple-input-single-output (MISO) communication system with clustered users is investigated. First, to describe the correlated feature for RIS-user channels, a beam domain channel model is developed for RIS-user channels. Then, a pilot reuse strategy is put forward to reduce the pilot overhead and decompose the channel estimation problem into several subproblems. Finally, by leveraging the correlated nature of RIS-user channels, an eigenspace projection (EP) algorithm is proposed to solve each subproblem respectively. Simulation results show that the proposed EP channel estimation scheme can achieve accurate channel estimation with lower pilot overhead than existing schemes.
</details>
<details>
<summary>摘要</summary>
智能表面重配置 (RIS) 是 sixth generation (6G) 通信系统的一个优秀技术候选人，因为它可以通过控制反射元素 (RE) 的系数来 manipulate 无线通信环境。然而，由于 RIS 通常由大量的 passtive RE 组成，因此在 RIS 帮助系统中的频道估计问题中存在过高的频道过头。在本文中，我们对 RIS 帮助的多用户多输入单出口 (MISO) 通信系统中的用户频道估计问题进行了研究。首先，为描述 RIS-用户频道之间的相关特征，我们提出了一种 beam 频道模型来描述 RIS-用户频道。然后，我们提出了一种重复频道 schemes 来减少频道过头和将频道估计问题分解成多个子问题。最后，通过利用 RIS-用户频道之间的相关性，我们提出了一种 eigenvalue projection (EP) 算法来解决每个子问题。 simulation results 表明，我们的 EP 频道估计方案可以在较低的频道过头下实现准确的频道估计。
</details></li>
</ul>
<hr>
<h2 id="Bridging-the-complexity-gap-in-Tbps-achieving-THz-band-baseband-processing"><a href="#Bridging-the-complexity-gap-in-Tbps-achieving-THz-band-baseband-processing" class="headerlink" title="Bridging the complexity gap in Tbps-achieving THz-band baseband processing"></a>Bridging the complexity gap in Tbps-achieving THz-band baseband processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16027">http://arxiv.org/abs/2309.16027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hadi Sarieddeen, Hakim Jemaa, Simon Tarboush, Christoph Studer, Mohamed-Slim Alouini, Tareq Y. Al-Naffouri</li>
<li>for: 本研究旨在提高电子和光子技术的进步，以实现teraHz频率上的高速数据传输。</li>
<li>methods: 该研究提议使用并行处理，特别是频率编码解码。</li>
<li>results: 研究表明，通过利用THz通道的结构化子空间，可以使用更短的代码字符串，并在所有基带处理块中进行并行化。这种方法可以提高数据传输速率至teraHz级别。<details>
<summary>Abstract</summary>
Recent advances in electronic and photonic technologies have allowed efficient signal generation and transmission at terahertz (THz) frequencies. However, as the gap in THz-operating devices narrows, the demand for terabit-per-second (Tbps)-achieving circuits is increasing. Translating the available hundreds of gigahertz (GHz) of bandwidth into a Tbps data rate requires processing thousands of information bits per clock cycle at state-of-the-art clock frequencies of digital baseband processing circuitry of a few GHz. This paper addresses these constraints and emphasizes the importance of parallelization in signal processing, particularly for channel code decoding. By leveraging structured sub-spaces of THz channels, we propose mapping bits to transmission resources using shorter code words, extending parallelizability across all baseband processing blocks. THz channels exhibit quasi-deterministic frequency, time, and space structures that enable efficient parallel bit mapping at the source and provide pseudo-soft bit reliability information for efficient detection and decoding at the receiver.
</details>
<details>
<summary>摘要</summary>
Recent advances in electronic and photonic technologies have allowed efficient signal generation and transmission at terahertz (THz) frequencies. However, as the gap in THz-operating devices narrows, the demand for terabit-per-second (Tbps)-achieving circuits is increasing. Translating the available hundreds of gigahertz (GHz) of bandwidth into a Tbps data rate requires processing thousands of information bits per clock cycle at state-of-the-art clock frequencies of digital baseband processing circuitry of a few GHz. This paper addresses these constraints and emphasizes the importance of parallelization in signal processing, particularly for channel code decoding. By leveraging structured sub-spaces of THz channels, we propose mapping bits to transmission resources using shorter code words, extending parallelizability across all baseband processing blocks. THz channels exhibit quasi-deterministic frequency, time, and space structures that enable efficient parallel bit mapping at the source and provide pseudo-soft bit reliability information for efficient detection and decoding at the receiver.Here's the translation in Traditional Chinese:随着电子和光子技术的进步，可以实现高频率的信号生成和传输，现在可以在teraHz（THz）频率上实现高速的资料传输。然而，随着THz设备的差距缩小，需要 Terabit/秒（Tbps）的实现普通的资料传输速率。将百兆Hz（GHz）的带宽转换为Tbps的数据传输速率需要在现有的几GHz的时钟频率上处理多达数万个信息位元每个时钟频率。本文探讨这些限制，并强调了信号处理中的并行化，特别是频道码解oding中的并行化。通过利用THz频道的结构子空间，我们提议将位元映射到传输资源上，使用短程码字，延伸并行性到所有的基带处理对象。THz频道具有 quasi-deterministic 的频率、时间和空间结构，使得源端可以高效地并行地将位元映射到传输资源，并提供pseudo-soft bit可靠性信息，以便高效地检测和解oding器。
</details></li>
</ul>
<hr>
<h2 id="Statistical-CSI-Based-Beamforming-for-Reconfigurable-Intelligent-Surface-Aided-MISO-Systems-with-Channel-Correlation"><a href="#Statistical-CSI-Based-Beamforming-for-Reconfigurable-Intelligent-Surface-Aided-MISO-Systems-with-Channel-Correlation" class="headerlink" title="Statistical CSI Based Beamforming for Reconfigurable Intelligent Surface Aided MISO Systems with Channel Correlation"></a>Statistical CSI Based Beamforming for Reconfigurable Intelligent Surface Aided MISO Systems with Channel Correlation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16026">http://arxiv.org/abs/2309.16026</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haochen Li, Zhiwen Pan, Bin Wang, Nan Liu, Xiaohu You</li>
<li>for: 这篇论文旨在探讨基于智能表面技术的6G通信系统中的RIS干扰器帮助多输入单输出多用户下链通信系统的吞吐量提升。</li>
<li>methods: 本文使用了统计channel state information (S-CSI)来取代实时channel state information (I-CSI)，并提出了用于合理的干扰器设计的离散估算法。</li>
<li>results:  simulations results表明，我们提出的两种干扰器设计算法都能够提高网络吞吐量，并且2比特quantizer可以实现RIS相位偏移的实现。<details>
<summary>Abstract</summary>
Reconfigurable intelligent surface (RIS) is a promising candidate technology of the upcoming Sixth Generation (6G) communication system for its ability to provide unprecedented spectral and energy efficiency increment through passive beamforming. However, it is challenging to obtain instantaneous channel state information (I-CSI) for RIS, which obliges us to use statistical channel state information (S-CSI) to achieve passive beamforming. In this paper, RIS-aided multiple-input single-output (MISO) multi-user downlink communication system with correlated channels is investigated. Then, we formulate the problem of joint beamforming design at the AP and RIS to maximize the sum ergodic spectral efficiency (ESE) of all users to improve the network capacity. Since it is too hard to compute sum ESE, an ESE approximation is adopted to reformulate the problem into a more tractable form. Then, we present two joint beamforming algorithms, namely the singular value decomposition-gradient descent (SVD-GD) algorithm and the fractional programming-gradient descent (FP-GD) algorithm. Simulation results show the effectiveness of our proposed algorithms and validate that 2-bits quantizer is enough for RIS phase shifts implementation.
</details>
<details>
<summary>摘要</summary>
《嵌入式智能表面（RIS）在第六代（6G）通信系统中的潜在应用》，这篇论文探讨了RIS在6G通信系统中的可能性。RIS可以提供历史性和能量效率的增量，但是获取RIS实时通道状态信息（I-CSI）是困难的，因此需要使用统计通道状态信息（S-CSI）来实现抗反射。本文研究了RIS-助け的多输入单出多用户下链通信系统，并对相关的通道进行了 correlate。然后，我们形式ulated了在AP和RIS之间进行共同束 beamforming设计，以提高所有用户的均衡束pectral efficiency（ESE），从而提高网络容量。由于计算ESE的复杂度太高，我们采用了ESE的一种近似值来重新定义问题，使其变得更易解。然后，我们提出了两种共同束 beamforming算法，namely SVD-GD算法和FP-GD算法。实验结果表明了我们提出的算法的效果，并证明了2位quantizer可以实现RIS相位shift的实现。
</details></li>
</ul>
<hr>
<h2 id="Linear-Progressive-Coding-for-Semantic-Communication-using-Deep-Neural-Networks"><a href="#Linear-Progressive-Coding-for-Semantic-Communication-using-Deep-Neural-Networks" class="headerlink" title="Linear Progressive Coding for Semantic Communication using Deep Neural Networks"></a>Linear Progressive Coding for Semantic Communication using Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15959">http://arxiv.org/abs/2309.15959</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eva Riherd, Raghu Mudumbai, Weiyu Xu</li>
<li>for: 这种方法是为了实现高效的semantic communication，即在干扰频道上传输semantic信息。</li>
<li>methods: 该方法使用进化编码，包括层次编码，先编码一部分semantic信息为粗略表示，然后逐步添加更多的semantic信息来细化表示。</li>
<li>results: 实验结果表明，这种进化semantic编码方法可以在初始量化 measurements 中提供有用的semantic预览，并且在整体准确率和效率方面与非进化方法相当。<details>
<summary>Abstract</summary>
We propose a general method for semantic representation of images and other data using progressive coding. Semantic coding allows for specific pieces of information to be selectively encoded into a set of measurements that can be highly compressed compared to the size of the original raw data. We consider a hierarchical method of coding where a partial amount of semantic information is first encoded a into a coarse representation of the data, which is then refined by additional encodings that add additional semantic information. Such hierarchical coding is especially well-suited for semantic communication i.e. transferring semantic information over noisy channels. Our proposed method can be considered as a generalization of both progressive image compression and source coding for semantic communication. We present results from experiments on the MNIST and CIFAR-10 datasets that show that progressive semantic coding can provide timely previews of semantic information with a small number of initial measurements while achieving overall accuracy and efficiency comparable to non-progressive methods.
</details>
<details>
<summary>摘要</summary>
我们提出了一种通用的 semantic representation 方法，使用进程式编码来实现。这种 semantic coding 方法可以 selectively 编码特定的信息，将其转换为可高度压缩的 measurement 集。我们认为这种层次编码方法特别适合 semantic communication，例如在噪音频道上传输 semantic 信息。我们的提议方法可以视为进程式图像压缩和源编码的总和。我们在 MNIST 和 CIFAR-10  dataset 上进行了实验，结果显示，逐步 semantic coding 可以在少量初始测量后提供有效的时间预览，并且与非逐步方法相比，达到了相同的总体准确率和效率。
</details></li>
</ul>
<hr>
<h2 id="IEEE-802-11be-Wi-Fi-7-Feature-Summary-and-Performance-Evaluation"><a href="#IEEE-802-11be-Wi-Fi-7-Feature-Summary-and-Performance-Evaluation" class="headerlink" title="IEEE 802.11be Wi-Fi 7: Feature Summary and Performance Evaluation"></a>IEEE 802.11be Wi-Fi 7: Feature Summary and Performance Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15951">http://arxiv.org/abs/2309.15951</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoqian Liu, Yuhan Dong, Yiqing Li, Yousi Lin, Xun Yang, Ming Gan</li>
<li>for: 本文主要旨在介绍Wi-Fi 7标准的开发目标和时间表，以及它在实时应用方面的性能提升技术。</li>
<li>methods: 本文列举了最新的关键技术，包括离散多功能频率（OFDMA）、离散多功能时钟（MU-MIMO）和离散多功能时钟扩展（MU-MIMO-EXT）等，以提高Wi-Fi 7的性能。</li>
<li>results: 系统级别的 simulate结果表明，通过组合新技术，Wi-Fi 7可以实现30 Gbps的吞吐量和低于Wi-Fi 6的延迟。<details>
<summary>Abstract</summary>
While the pace of commercial scale application of Wi-Fi 6 accelerates, the IEEE 802.11 Working Group is about to complete the development of a new amendment standard IEEE 802.11be -- Extremely High Throughput (EHT), also known as Wi-Fi 7, which can be used to meet the demand for the throughput of 4K/8K videos up to tens of Gbps and low-latency video applications such as virtual reality (VR) and augmented reality (AR). Wi-Fi 7 not only scales Wi-Fi 6 with doubled bandwidth, but also supports real-time applications, which brings revolutionary changes to Wi-Fi. In this article, we start by introducing the main objectives and timeline of Wi-Fi 7 and then list the latest key techniques which promote the performance improvement of Wi-Fi 7. Finally, we validate the most critical objectives of Wi-Fi 7 -- the potential up to 30 Gbps throughput and lower latency. System-level simulation results suggest that by combining the new techniques, Wi-Fi 7 achieves 30 Gbps throughput and lower latency than Wi-Fi 6.
</details>
<details>
<summary>摘要</summary>
而 Wi-Fi 6 的商业大规模应用速度不断加快，IEEE 802.11 工作组即将完成一个新的修订标准 IEEE 802.11be -- Extremely High Throughput (EHT)，也称为 Wi-Fi 7，可以用于满足 4K/8K 视频的吞吐量达到十兆比特每秒并且低延迟视频应用程序such as virtual reality (VR) 和 augmented reality (AR)。Wi-Fi 7 不仅扩展 Wi-Fi 6 的频谱带宽，还支持实时应用程序，这会对 Wi-Fi 进行革命性的变革。本文首先介绍 Wi-Fi 7 的主要目标和时间表，然后列出最新的关键技术，以提高 Wi-Fi 7 的性能。最后，我们验证 Wi-Fi 7 的核心目标 -- 可达 30 Gbps 的吞吐量和低于 Wi-Fi 6 的延迟。系级 simulation 结果表明，通过组合新技术，Wi-Fi 7 可以实现 30 Gbps 的吞吐量和低于 Wi-Fi 6 的延迟。
</details></li>
</ul>
<hr>
<h2 id="Quantum-computer-enabled-receivers-for-optical-communication"><a href="#Quantum-computer-enabled-receivers-for-optical-communication" class="headerlink" title="Quantum computer-enabled receivers for optical communication"></a>Quantum computer-enabled receivers for optical communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15914">http://arxiv.org/abs/2309.15914</a></li>
<li>repo_url: None</li>
<li>paper_authors: John Crossman, Spencer Dimitroff, Lukasz Cincio, Mohan Sarovar</li>
<li>for: 这篇论文目的是提出一种基于光学和量子计算的新方法，用于实现高速信息传输。</li>
<li>methods: 该方法使用光学信号的相位和幅度模ulation，并使用量子计算来实现对多个信号的同时检测。</li>
<li>results: 研究人员通过使用 optomechanical 转换和短深度的可变量化量子电路，实现了在低光强 régime下的高精度信息传输。此外，他们还使用模型来捕捉非理想的热雷和损失，以估计转换性能。最后，他们在IBM-Q设备上执行了训练后的可变量化电路，以示出这种方法可以在当今的量子计算硬件噪声下实现量子优势。<details>
<summary>Abstract</summary>
Optical communication is the standard for high-bandwidth information transfer in today's digital age. The increasing demand for bandwidth has led to the maturation of coherent transceivers that use phase- and amplitude-modulated optical signals to encode more bits of information per transmitted pulse. Such encoding schemes achieve higher information density, but also require more complicated receivers to discriminate the signaling states. In fact, achieving the ultimate limit of optical communication capacity, especially in the low light regime, requires coherent joint detection of multiple pulses. Despite their superiority, such joint detection receivers are not in widespread use because of the difficulty of constructing them in the optical domain. In this work we describe how optomechanical transduction of phase information from coherent optical pulses to superconducting qubit states followed by the execution of trained short-depth variational quantum circuits can perform joint detection of communication codewords with error probabilities that surpass all classical, individual pulse detection receivers. Importantly, we utilize a model of optomechanical transduction that captures non-idealities such as thermal noise and loss in order to understand the transduction performance necessary to achieve a quantum advantage with such a scheme. We also execute the trained variational circuits on an IBM-Q device with the modeled transduced states as input to demonstrate that a quantum advantage is possible even with current levels of quantum computing hardware noise.
</details>
<details>
<summary>摘要</summary>
光学通信是当今数字时代的标准高频带宽信息传输方式。随着带宽需求的增加，整形传输器已经成熟，它们使用相位和振幅模拟的光信号来编码更多的比特信息每个发射脉冲中。这种编码方案可以实现更高的信息密度，但也需要更复杂的接收机来分辨信号状态。实际上，在低光度 режиме下，实现光学通信的最终限制需要同步检测多个脉冲。尽管它们具有superiority，但是这些同步检测接收机尚未广泛使用，因为它们在光学频谱中实现的困难。在这项工作中，我们描述了如何将光学信号转换为超导素子状态，然后执行训练过的短深度变量量Quantum Circuits可以同时检测通信代码字符串，并且达到所有个别脉冲检测接收机的错误概率。我们还使用模型来捕捉非理想的热雷和损失，以理解转换性能所需的跨度。最后，我们在IBM-Q设备上执行训练过的变量量Circuits，并将模型转换后的状态作为输入，以示出可以在当今水平的量子计算机噪音下实现量子优势。
</details></li>
</ul>
<hr>
<h2 id="Differentiable-Machine-Learning-Based-Modeling-for-Directly-Modulated-Lasers"><a href="#Differentiable-Machine-Learning-Based-Modeling-for-Directly-Modulated-Lasers" class="headerlink" title="Differentiable Machine Learning-Based Modeling for Directly-Modulated Lasers"></a>Differentiable Machine Learning-Based Modeling for Directly-Modulated Lasers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15747">http://arxiv.org/abs/2309.15747</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sergio Hernandez, Ognjen Jovanovic, Christophe Peucheret, Francesco Da Ros, Darko Zibar</li>
<li>for: 这篇论文旨在提出和比较 differentiable 机器学习基于模拟器的 surrogate 模型，以便在大信号域下优化直接模拟器（DML）。</li>
<li>methods: 这篇论文使用了 differentiable machine learning-based surrogate models，并对其进行了量化评估。</li>
<li>results: 研究发现，使用 convolutional attention transformer 的 surrogate model 在数字平衡设置中表现最佳，其 Root Mean Square Error 较低，并且训练&#x2F;测试时间较短。<details>
<summary>Abstract</summary>
End-to-end learning has become a popular method for joint transmitter and receiver optimization in optical communication systems. Such approach may require a differentiable channel model, thus hindering the optimization of links based on directly modulated lasers (DMLs). This is due to the DML behavior in the large-signal regime, for which no analytical solution is available. In this paper, this problem is addressed by developing and comparing differentiable machine learning-based surrogate models. The models are quantitatively assessed in terms of root mean square error and training/testing time. Once the models are trained, the surrogates are then tested in a numerical equalization setup, resembling a practical end-to-end scenario. Based on the numerical investigation conducted, the convolutional attention transformer is shown to outperform the other models considered.
</details>
<details>
<summary>摘要</summary>
现代光通信系统中，endo-to-end学习已成为 JOINT TRANSMITTER和接收器优化的受欢迎方法。然而，这种方法可能需要一个可导的通道模型，从而限制基于直接调制拉зе（DML）的链路优化。这是因为DML在大信号 режиме下的行为，无法获得分析解。本文通过开发和比较 Machine Learning 基于 surrogate 模型来解决这个问题。这些模型在量化Error和训练/测试时间方面进行评估。经过训练后，这些模型在数字平衡设置中进行测试， simulate 一个实际的端到端场景。根据我们的数字调查，卷积注意力变换器表现得最好。
</details></li>
</ul>
<hr>
<h2 id="Energy-Saving-Cell-Free-Massive-MIMO-Precoders-with-a-Per-AP-Wideband-Kronecker-Channel-Model"><a href="#Energy-Saving-Cell-Free-Massive-MIMO-Precoders-with-a-Per-AP-Wideband-Kronecker-Channel-Model" class="headerlink" title="Energy-Saving Cell-Free Massive MIMO Precoders with a Per-AP Wideband Kronecker Channel Model"></a>Energy-Saving Cell-Free Massive MIMO Precoders with a Per-AP Wideband Kronecker Channel Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15658">http://arxiv.org/abs/2309.15658</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emanuele Peschiera, Xavier Mestre, François Rottenberg</li>
<li>for: 这个论文专门用于研究无基站大规模多输入多出口前置器，以减少发动机吞吐量，同时保证每个用户每个子卫星频道的速率限制。</li>
<li>methods: 这篇论文使用随机矩阵理论来解决困难，通过解决每个天线的电力值为固定点方程，来适应不确定的通道响应。</li>
<li>results: 数值仿真结果表明，使用这种方法可以在低负荷情况下保证所有天线的使用，同时减少电力消耗，最高可以达到9倍的减少。<details>
<summary>Abstract</summary>
We study cell-free massive multiple-input multiple-output precoders that minimize the power consumed by the power amplifiers subject to per-user per-subcarrier rate constraints. The power at each antenna is generally retrieved by solving a fixed-point equation that depends on the instantaneous channel coefficients. Using random matrix theory, we retrieve each antenna power as the solution to a fixed-point equation that depends only on the second-order statistics of the channel. Numerical simulations prove the accuracy of our asymptotic approximation and show how a subset of access points should be turned off to save power consumption, while all the antennas of the active access points are utilized with uniform power across them. This mechanism allows to save consumed power up to a factor of 9$\times$ in low-load scenarios.
</details>
<details>
<summary>摘要</summary>
我们研究无基站大量多输入多出力前缀器，以减少发动机增强器的功率消耗，同时保持每个用户每个子载波长的速率限制。每个天线的功率通常通过解决固定点方程来获取，该方程取决于当前频率响应的快速 statistcs。使用随机矩阵理论，我们可以通过解决固定点方程来获取每个天线的功率，该方程只取决于通道的二阶统计。数值仿真表明我们的极限 aproximation 的准确性，并显示在低负载场景下，可以关闭一些接入点，以达到消耗电力的减少，而活动接入点的所有天线都会充分利用。这种机制可以在低负载场景下减少消耗电力至多达9倍。
</details></li>
</ul>
<hr>
<h2 id="Design-and-Optimization-of-Residual-Neural-Network-Accelerators-for-Low-Power-FPGAs-Using-High-Level-Synthesis"><a href="#Design-and-Optimization-of-Residual-Neural-Network-Accelerators-for-Low-Power-FPGAs-Using-High-Level-Synthesis" class="headerlink" title="Design and Optimization of Residual Neural Network Accelerators for Low-Power FPGAs Using High-Level Synthesis"></a>Design and Optimization of Residual Neural Network Accelerators for Low-Power FPGAs Using High-Level Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15631">http://arxiv.org/abs/2309.15631</a></li>
<li>repo_url: None</li>
<li>paper_authors: Filippo Minnella, Teodoro Urso, Mihai T. Lazarescu, Luciano Lavagno</li>
<li>for: 这个研究是为了实现深度学习模型在场执行可程式阵列（FPGA）上，特别是适用于具有跳跃运算的残差神经网络（ResNets）。</li>
<li>methods: 这个研究使用了高阶合成（HLS）技术来实现深度学习模型，并运用了一系列的设计原则和优化策略来实现ResNets的优化实现。</li>
<li>results: 这个研究在CIFAR-10 dataset上使用Xilinx FPGAs实现了ResNet8和ResNet20模型，比预设的Kria KV260板子上的实现更高速，并且保持了与预设的实现相似的精度。具体来说，ResNet20模型在Kria KV260板子上实现了2.88倍的速度，而且精度提高了0.5%，即91.3%；ResNet8模型的精度提高了2.8%，即88.7%。<details>
<summary>Abstract</summary>
Residual neural networks are widely used in computer vision tasks. They enable the construction of deeper and more accurate models by mitigating the vanishing gradient problem. Their main innovation is the residual block which allows the output of one layer to bypass one or more intermediate layers and be added to the output of a later layer. Their complex structure and the buffering required by the residual block make them difficult to implement on resource-constrained platforms. We present a novel design flow for implementing deep learning models for field programmable gate arrays optimized for ResNets, using a strategy to reduce their buffering overhead to obtain a resource-efficient implementation of the residual layer. Our high-level synthesis (HLS)-based flow encompasses a thorough set of design principles and optimization strategies, exploiting in novel ways standard techniques such as temporal reuse and loop merging to efficiently map ResNet models, and potentially other skip connection-based NN architectures, into FPGA. The models are quantized to 8-bit integers for both weights and activations, 16-bit for biases, and 32-bit for accumulations. The experimental results are obtained on the CIFAR-10 dataset using ResNet8 and ResNet20 implemented with Xilinx FPGAs using HLS on the Ultra96-V2 and Kria KV260 boards. Compared to the state-of-the-art on the Kria KV260 board, our ResNet20 implementation achieves 2.88X speedup with 0.5% higher accuracy of 91.3%, while ResNet8 accuracy improves by 2.8% to 88.7%. The throughputs of ResNet8 and ResNet20 are 12971 FPS and 3254 FPS on the Ultra96 board, and 30153 FPS and 7601 FPS on the Kria KV26, respectively. They Pareto-dominate state-of-the-art solutions concerning accuracy, throughput, and energy.
</details>
<details>
<summary>摘要</summary>
循环神经网络在计算机视觉任务中广泛应用。它们使得建立更深和更准确的模型变得可能，并mitigate the vanishing gradient problem。它们的主要创新是差值块，允许输出一层的输出通过一个或多个中间层直接加到后续层的输出。它们的复杂结构和差值块所需的缓冲 overhead 使得它们在有限资源平台上具有困难的实现。我们提出了一种新的设计流程，用于实现适应Field Programmable Gate Arrays（FPGA）的深度学习模型，使得差值块的缓冲 overhead 得到减少，从而实现资源高效的实现。我们的高级合成（HLS）基于的流程包括一系列设计原则和优化策略，利用了标准技术的 temporal reuse 和 loop merging，高效地将 ResNet 模型和其他 skip connection-based NN  архитектуры映射到 FPGA。模型使用 8 位整数（weights和activations）、16 位整数（biases）和 32 位整数（accumulations）进行量化。实验结果基于 CIFAR-10 数据集，使用 Xilinx FPGAs 通过 HLS 在 Ultra96-V2 和 Kria KV260 板上实现 ResNet8 和 ResNet20。与state-of-the-art 在 Kria KV260 板上相比，我们的 ResNet20 实现 achieved 2.88X 速度增加，同时精度提高 0.5%，至 91.3%。 ResNet8 精度提高 2.8%，至 88.7%。差值块8和 ResNet20的 throughput 分别为 12971 FPS 和 3254 FPS 在 Ultra96 板上，并分别为 30153 FPS 和 7601 FPS 在 Kria KV26 板上。它们 pareto-dominate state-of-the-art 方案， regard to accuracy, throughput, and energy.
</details></li>
</ul>
<hr>
<h2 id="Approximate-Message-Passing-with-Rigorous-Guarantees-for-Pooled-Data-and-Quantitative-Group-Testing"><a href="#Approximate-Message-Passing-with-Rigorous-Guarantees-for-Pooled-Data-and-Quantitative-Group-Testing" class="headerlink" title="Approximate Message Passing with Rigorous Guarantees for Pooled Data and Quantitative Group Testing"></a>Approximate Message Passing with Rigorous Guarantees for Pooled Data and Quantitative Group Testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15507">http://arxiv.org/abs/2309.15507</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nelvin Tan, Jonathan Scarlett, Ramji Venkataramanan<br>for: 这个论文的目的是用于预测pool中item的类别，并且提出了一种基于 Approximate Message Passing（AMP）算法的方法。methods: 这个论文使用了AMP算法来预测pool中item的类别，并且进行了一种准确的性能分析，包括静态和噪声场景。results: 研究发现，在静态场景下，AMP算法与之前由El Alaoui et al.提出的算法等价。此外，通过计算False Positive Rate和False Negative Rate的限制值，研究人员也得出了精确的性能保证。数据 simulations 表明，AMP算法在一些Quantitative Group Testing（QGT）场景下表现较好，但是在三个类别的pool中，Convex ProgrammingEstimator表现较好。<details>
<summary>Abstract</summary>
In the pooled data problem, the goal is to identify the categories associated with a large collection of items via a sequence of pooled tests. Each pooled test reveals the number of items of each category within the pool. We study an approximate message passing (AMP) algorithm for estimating the categories and rigorously characterize its performance, in both the noiseless and noisy settings. For the noiseless setting, we show that the AMP algorithm is equivalent to one recently proposed by El Alaoui et al. Our results provide a rigorous version of their performance guarantees, previously obtained via non-rigorous techniques. For the case of pooled data with two categories, known as quantitative group testing (QGT), we use the AMP guarantees to compute precise limiting values of the false positive rate and the false negative rate. Though the pooled data problem and QGT are both instances of estimation in a linear model, existing AMP theory cannot be directly applied since the design matrices are binary valued. The key technical ingredient in our result is a rigorous analysis of AMP for generalized linear models defined via generalized white noise design matrices. This result, established using a recent universality result of Wang et al., is of independent interest. Our theoretical results are validated by numerical simulations. For comparison, we propose estimators based on convex relaxation and iterative thresholding, without providing theoretical guarantees. Our simulations indicate that AMP outperforms the convex programming estimator for a range of QGT scenarios, but the convex program performs better for pooled data with three categories.
</details>
<details>
<summary>摘要</summary>
“在混合数据问题中，目标是通过一系列混合测试来确定Item的类别。每个混合测试会报告每个类别的 Item 数量。我们研究了一种近似消息传递（AMP）算法来估算类别，并且正式characterize其性能，包括噪音无效和噪音有效的情况。在噪音无效情况下，我们证明AMP算法与El Alaoui et al.提出的算法等价。我们的结果提供了对AMP算法性能的正式保证，之前由非正式技术来确定。在两类Quantitative group testing（QGT）中，我们使用AMP保证计算出精确的假阳性率和假阴性率的限制值。尽管混合数据问题和QGT都是线性模型中的估算问题，但现有的AMP理论不能直接应用，因为设计矩阵是二进制值的。我们的关键技术成果在于对AMP在通用线性模型中进行了正式分析，这一结果使用了Wang et al.最近的一个统计结果。这一结果不仅有助于解决我们的问题，还是独立有价值的。我们的理论结果通过数值仿真验证。而我们还提出了基于 convex relaxation 和迭代抑制的估算器，但没有提供理论保证。我们的仿真结果表明，在QGT场景中，AMP超过 convex programming 估算器的性能，但是在三类混合数据中，convex programming 估算器表现更好。”
</details></li>
</ul>
<hr>
<h2 id="Formation-Wing-Beat-Modulation-FWM-A-Tool-for-Quantifying-Bird-Flocks-Using-Radar-Micro-Doppler-Signals"><a href="#Formation-Wing-Beat-Modulation-FWM-A-Tool-for-Quantifying-Bird-Flocks-Using-Radar-Micro-Doppler-Signals" class="headerlink" title="Formation Wing-Beat Modulation (FWM): A Tool for Quantifying Bird Flocks Using Radar Micro-Doppler Signals"></a>Formation Wing-Beat Modulation (FWM): A Tool for Quantifying Bird Flocks Using Radar Micro-Doppler Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15415">http://arxiv.org/abs/2309.15415</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiangkun Gong, Jun Yan, Deyong Kong, Ruizhi Chen, Deren Li</li>
<li>for: 研究鸟类群体数量和鸟类飞行行为</li>
<li>methods: 利用X射频雷达观测鸟类群体的形态翼拍模ulation（FWM）效应，通过雷达信号中的微-Doppler干扰分析鸟类数量和飞行策略</li>
<li>results: 实际观测到鸟类群体中的FWM信号，提供了量化鸟类数量和估计鸟类翼拍频率的工具，帮助进一步了雷达鸟类学和飞行生物学等领域的研究。<details>
<summary>Abstract</summary>
Radar echoes from bird flocks contain modulation signals, which we find are produced by the flapping gaits of birds in the flock, resulting in a group of spectral peaks with similar amplitudes spaced at a specific interval. We call this the formation wing-beat modulation (FWM) effect. FWM signals are micro-Doppler modulated by flapping wings and are related to the bird number, wing-beat frequency, and flight phasing strategy. Our X-band radar data show that FWM signals exist in radar signals of a seagull flock, providing tools for quantifying the bird number and estimating the mean wingbeat rate of birds. This new finding could aid in research on the quantification of bird migration numbers and estimation of bird flight behavior in radar ornithology and aero-ecology.
</details>
<details>
<summary>摘要</summary>
雷达射回信号中的鸟群射击信号包含形成翼振荡模ulation（FWM）效应，我们发现这些信号由鸟群中的鸟嘴冲击产生，导致一组spectral peak的峰值具有相同的幅度， spacing at a specific interval.我们称这为formation wing-beat modulation（FWM）效应。 FWM信号被微-Doppler模ulation和鸟数、翼振荡频率和飞行策略相关。我们的X射频雷达数据显示，FWM信号存在鸟群雷达信号中，提供了量化鸟数和估计鸟的平均翼振荡频率的工具。这一新发现可以帮助在雷达 ornithology和 aer-ecology 中研究鸟类迁徙数量和鸟类飞行行为的估计。
</details></li>
</ul>
<hr>
<h2 id="An-Exploration-of-Optimal-Parameters-for-Efficient-Blind-Source-Separation-of-EEG-Recordings-Using-AMICA"><a href="#An-Exploration-of-Optimal-Parameters-for-Efficient-Blind-Source-Separation-of-EEG-Recordings-Using-AMICA" class="headerlink" title="An Exploration of Optimal Parameters for Efficient Blind Source Separation of EEG Recordings Using AMICA"></a>An Exploration of Optimal Parameters for Efficient Blind Source Separation of EEG Recordings Using AMICA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15388">http://arxiv.org/abs/2309.15388</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gwenevere Frank, Seyed Yahya Shirazi, Jason Palmer, Gert Cauwenberghs, Scott Makeig, Arnaud Delorme</li>
<li>for: 这个论文主要是为了研究眼动电学（EEG）的独立 ком component分析（ICA）算法在EEG分解中的效果。</li>
<li>methods: 这个论文使用了多种ICA算法进行EEG分解，并对AMICA算法进行了比较。AMICA算法提供了许多参数，allowing for precise control of the decomposition。</li>
<li>results: 研究发现，在不同参数的设置下，AMICA算法的运行时间和分解质量可以通过对比两个纬度量 metrics：Pairwise Mutual Information (PMI)和Mutual Information Reduction (MIR)进行分析。此外，也提供了选择参数的初始值的建议。<details>
<summary>Abstract</summary>
EEG continues to find a multitude of uses in both neuroscience research and medical practice, and independent component analysis (ICA) continues to be an important tool for analyzing EEG. A multitude of ICA algorithms for EEG decomposition exist, and in the past, their relative effectiveness has been studied. AMICA is considered the benchmark against which to compare the performance of other ICA algorithms for EEG decomposition. AMICA exposes many parameters to the user to allow for precise control of the decomposition. However, several of the parameters currently tend to be set according to "rules of thumb" shared in the EEG community. Here, AMICA decompositions are run on data from a collection of subjects while varying certain key parameters. The running time and quality of decompositions are analyzed based on two metrics: Pairwise Mutual Information (PMI) and Mutual Information Reduction (MIR). Recommendations for selecting starting values for parameters are presented.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/27/eess.SP_2023_09_27/" data-id="clp89dopy01eti788d4j01o9t" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_09_26" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/26/cs.SD_2023_09_26/" class="article-date">
  <time datetime="2023-09-26T15:00:00.000Z" itemprop="datePublished">2023-09-26</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/26/cs.SD_2023_09_26/">cs.SD - 2023-09-26</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Simultaneously-Learning-Speaker’s-Direction-and-Head-Orientation-from-Binaural-Recordings"><a href="#Simultaneously-Learning-Speaker’s-Direction-and-Head-Orientation-from-Binaural-Recordings" class="headerlink" title="Simultaneously Learning Speaker’s Direction and Head Orientation from Binaural Recordings"></a>Simultaneously Learning Speaker’s Direction and Head Orientation from Binaural Recordings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15064">http://arxiv.org/abs/2309.15064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Harshvardhan Takawale, Nirupam Roy</li>
<li>for: 这篇论文的目的是在实现基于EARable设备的应用中，用声学记录来估算说话人和听众的头部方向和orientation。</li>
<li>methods: 该论文提出了一种基于人声直径和听众的头部相关函数（HRTF）的 convolutional neural network 模型，用于同时预测说话人和听众的头部方向。该模型利用听众耳上的微机顺序记录，根据声音的 direktivity 和高频响应特性，来估算说话人和听众的头部方向。</li>
<li>results: 研究人员通过实验证明，该模型可以准确地预测说话人和听众的头部方向，并且可以在不同的听众位置和环境中提供高精度的估算结果。<details>
<summary>Abstract</summary>
Estimation of a speaker's direction and head orientation with binaural recordings can be a critical piece of information in many real-world applications with emerging `earable' devices, including smart headphones and AR/VR headsets. However, it requires predicting the mutual head orientations of both the speaker and the listener, which is challenging in practice. This paper presents a system for jointly predicting speaker-listener head orientations by leveraging inherent human voice directivity and listener's head-related transfer function (HRTF) as perceived by the ear-mounted microphones on the listener. We propose a convolution neural network model that, given binaural speech recording, can predict the orientation of both speaker and listener with respect to the line joining the two. The system builds on the core observation that the recordings from the left and right ears are differentially affected by the voice directivity as well as the HRTF. We also incorporate the fact that voice is more directional at higher frequencies compared to lower frequencies.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本转换为简化中文：<</SYS>>使用扬声器录音的方式估算发言人的方向和头姿可以是许多实际应用中的关键信息，包括智能HEADSET和AR/VR头戴式设备。然而，这需要预测发言人和听众双方的相互头姿，这在实践中很困难。这篇论文提出了一种系统，通过利用人声直达性和听众耳部 Transfer Function (HRTF)，以 ear-mounted microphones 上的听众所感受到的方式来联合预测发言人和听众的头姿。我们提议一种卷积神经网络模型， givens binaural speech recording，可以预测发言人和听众的方向相对于两点线。该系统基于核心observation ，左耳和右耳的录音被声 directivity 以及 HRTF 所不同地影响。我们还 incorporate 声音在高频段的方向性比低频段更强。
</details></li>
</ul>
<hr>
<h2 id="A-multi-modal-approach-for-identifying-schizophrenia-using-cross-modal-attention"><a href="#A-multi-modal-approach-for-identifying-schizophrenia-using-cross-modal-attention" class="headerlink" title="A multi-modal approach for identifying schizophrenia using cross-modal attention"></a>A multi-modal approach for identifying schizophrenia using cross-modal attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15136">http://arxiv.org/abs/2309.15136</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gowtham Premananth, Yashish M. Siriwardena, Philip Resnik, Carol Espy-Wilson</li>
<li>for: 这项研究旨在用不同的人类通信模式分类健康个体和患有强烈精神病的subject，以便更好地诊断和治疗精神病。</li>
<li>methods: 该研究使用了多modal的人类通信数据，包括视频、音频和文本，并从这些数据提取了低级特征，如面部动作单元和 vocals tract 变量，以计算高级协调特征。然后，将多modal数据 fusion为一个Session-level classifier和文本模型，使用 hierarchical Attention Network (HAN) with cross-modal attention。</li>
<li>results: 根据Weighted average F1 score，该多Modal系统在与前一个状态的多Modal系统进行比较时，提高了8.53%。<details>
<summary>Abstract</summary>
This study focuses on how different modalities of human communication can be used to distinguish between healthy controls and subjects with schizophrenia who exhibit strong positive symptoms. We developed a multi-modal schizophrenia classification system using audio, video, and text. Facial action units and vocal tract variables were extracted as low-level features from video and audio respectively, which were then used to compute high-level coordination features that served as the inputs to the audio and video modalities. Context-independent text embeddings extracted from transcriptions of speech were used as the input for the text modality. The multi-modal system is developed by fusing a segment-to-session-level classifier for video and audio modalities with a text model based on a Hierarchical Attention Network (HAN) with cross-modal attention. The proposed multi-modal system outperforms the previous state-of-the-art multi-modal system by 8.53% in the weighted average F1 score.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Segment-Level-Vectorized-Beam-Search-Based-on-Partially-Autoregressive-Inference"><a href="#Segment-Level-Vectorized-Beam-Search-Based-on-Partially-Autoregressive-Inference" class="headerlink" title="Segment-Level Vectorized Beam Search Based on Partially Autoregressive Inference"></a>Segment-Level Vectorized Beam Search Based on Partially Autoregressive Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14922">http://arxiv.org/abs/2309.14922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masao Someki, Nicholas Eng, Yosuke Higuchi, Shinji Watanabe</li>
<li>for: 提高自动语音识别（ASR）模型的推理速度，尤其是针对AR嵌入式模型。</li>
<li>methods: 提出一种部分AR框架，通过 segment-level vectorized beam search 加速ASR模型的推理速度，而不会影响准确性。</li>
<li>results: 实验结果显示，我们的方法可以在 LibriSpeech 集合上提高推理速度 12-13 倍，而且保持高度准确。<details>
<summary>Abstract</summary>
Attention-based encoder-decoder models with autoregressive (AR) decoding have proven to be the dominant approach for automatic speech recognition (ASR) due to their superior accuracy. However, they often suffer from slow inference. This is primarily attributed to the incremental calculation of the decoder. This work proposes a partially AR framework, which employs segment-level vectorized beam search for improving the inference speed of an ASR model based on the hybrid connectionist temporal classification (CTC) attention-based architecture. It first generates an initial hypothesis using greedy CTC decoding, identifying low-confidence tokens based on their output probabilities. We then utilize the decoder to perform segment-level vectorized beam search on these tokens, re-predicting in parallel with minimal decoder calculations. Experimental results show that our method is 12 to 13 times faster in inference on the LibriSpeech corpus over AR decoding whilst preserving high accuracy.
</details>
<details>
<summary>摘要</summary>
注意型编码器-解码器模型具有自动推理（AR）解码功能，在自动语音识别（ASR）领域具有优秀表现。然而，它们经常受到慢速推理的困扰。这主要归结于逐个计算decoder的增量。本文提出了一种部分AR框架，使用段级 вектор化的搜索 beam search来提高ASR模型基于混合连接式时间分类（CTC）注意力基 architecture的推理速度。它首先使用批量CTC解码生成一个初始假设，并将低信度的token标识出来基于其输出概率。然后，我们使用decoder进行段级 вектор化的搜索，并在并行进行重新预测，只需要最少的decoder计算。实验结果显示，我们的方法在LibriSpeech corpus上的推理速度比AR解码快12-13倍，保持高精度。
</details></li>
</ul>
<hr>
<h2 id="Emphasized-Non-Target-Speaker-Knowledge-in-Knowledge-Distillation-for-Automatic-Speaker-Verification"><a href="#Emphasized-Non-Target-Speaker-Knowledge-in-Knowledge-Distillation-for-Automatic-Speaker-Verification" class="headerlink" title="Emphasized Non-Target Speaker Knowledge in Knowledge Distillation for Automatic Speaker Verification"></a>Emphasized Non-Target Speaker Knowledge in Knowledge Distillation for Automatic Speaker Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14838">http://arxiv.org/abs/2309.14838</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ductuantruong/enskd">https://github.com/ductuantruong/enskd</a></li>
<li>paper_authors: Duc-Tuan Truong, Ruijie Tao, Jia Qi Yip, Kong Aik Lee, Eng Siong Chng</li>
<li>for: 提高自动人员识别性能</li>
<li>methods: 利用知识混合法强化教师网络和学生网络之间的一致性，并强调非目标说话人的分类概率</li>
<li>results: 在三种不同的学生模型架构上应用修改后的知识混合法，在VoxCeleb数据集上实现了13.67%的EER提高 compared to embedding-level和标准标签水平的知识混合法<details>
<summary>Abstract</summary>
Knowledge distillation (KD) is used to enhance automatic speaker verification performance by ensuring consistency between large teacher networks and lightweight student networks at the embedding level or label level. However, the conventional label-level KD overlooks the significant knowledge from non-target speakers, particularly their classification probabilities, which can be crucial for automatic speaker verification. In this paper, we first demonstrate that leveraging a larger number of training non-target speakers improves the performance of automatic speaker verification models. Inspired by this finding about the importance of non-target speakers' knowledge, we modified the conventional label-level KD by disentangling and emphasizing the classification probabilities of non-target speakers during knowledge distillation. The proposed method is applied to three different student model architectures and achieves an average of 13.67% improvement in EER on the VoxCeleb dataset compared to embedding-level and conventional label-level KD methods.
</details>
<details>
<summary>摘要</summary>
知识缩减（KD）可以提高自动说话识别性能，确保大教师网络和轻量级学生网络在嵌入层或标签层之间具有一致性。然而，传统的标签级KD忽略了非目标说话者的知识，特别是他们的分类概率，这些知识对自动说话识别非常重要。在这篇论文中，我们首先证明了使用更多的训练非目标说话者可以提高自动说话识别模型的性能。 inspirited by这一发现，我们修改了传统的标签级KD，通过分解和强调非目标说话者的分类概率来进行知识缩减。我们对三种不同的学生模型架构进行应用，并在VoxCeleb数据集上实现了13.67%的EER提升，比 embedding-level和传统标签级KD方法更好。
</details></li>
</ul>
<hr>
<h2 id="Optimization-Techniques-for-a-Physical-Model-of-Human-Vocalisation"><a href="#Optimization-Techniques-for-a-Physical-Model-of-Human-Vocalisation" class="headerlink" title="Optimization Techniques for a Physical Model of Human Vocalisation"></a>Optimization Techniques for a Physical Model of Human Vocalisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14761">http://arxiv.org/abs/2309.14761</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mateo Cámara, Zhiyuan Xu, Yisu Zong, José Luis Blanco, Joshua D. Reiss</li>
<li>for: 优化和评估非语音音频效果的生成模型。</li>
<li>methods: 使用粉色 trombone synthesizer 作为一种简化的 vocal tract 生产模型，针对非语音人工声音信号 —— yawnings。选择并优化控制参数，以减少实际和生成 Audio 之间的差异。</li>
<li>results: 比较了不同优化技术和声音表示方式的结果，发现 génétic 和 swarm 优化器在计算成本上较高，但可以更好地优化模型。specific combinations of optimizers and audio representations offer significantly different results。<details>
<summary>Abstract</summary>
We present a non-supervised approach to optimize and evaluate the synthesis of non-speech audio effects from a speech production model. We use the Pink Trombone synthesizer as a case study of a simplified production model of the vocal tract to target non-speech human audio signals --yawnings. We selected and optimized the control parameters of the synthesizer to minimize the difference between real and generated audio. We validated the most common optimization techniques reported in the literature and a specifically designed neural network. We evaluated several popular quality metrics as error functions. These include both objective quality metrics and subjective-equivalent metrics. We compared the results in terms of total error and computational demand. Results show that genetic and swarm optimizers outperform least squares algorithms at the cost of executing slower and that specific combinations of optimizers and audio representations offer significantly different results. The proposed methodology could be used in benchmarking other physical models and audio types.
</details>
<details>
<summary>摘要</summary>
我们提出了一种非监督式的方法来优化和评估非语音音效的生成模型。我们使用了淡红 trombone synthesizer 作为一个简化的 vocal tract 生产模型，targeting 非语音人类声音信号 -- yawnings。我们选择和优化控制参数，以减少实际和生成声音之间的差异。我们验证了文献中通常报道的优化技术和一种特制的神经网络。我们使用了多种广泛使用的质量指标，包括对象质量指标和主观相当的指标。我们比较了结果，包括总错误和计算负担。结果显示，遗传和群体优化器在计算 slower 的情况下，可以超过 least squares 算法；具体的组合优化器和声音表示可以得到明显不同的结果。我们的方法可以用于对其他物理模型和声音类型进行benchmarking。
</details></li>
</ul>
<hr>
<h2 id="Exploring-RWKV-for-Memory-Efficient-and-Low-Latency-Streaming-ASR"><a href="#Exploring-RWKV-for-Memory-Efficient-and-Low-Latency-Streaming-ASR" class="headerlink" title="Exploring RWKV for Memory Efficient and Low Latency Streaming ASR"></a>Exploring RWKV for Memory Efficient and Low Latency Streaming ASR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14758">http://arxiv.org/abs/2309.14758</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keyu An, Shiliang Zhang</li>
<li>for: 这个论文的目的是提出一种基于RWKV的流处理ASR模型，以提高流处理ASR的准确率和效率。</li>
<li>methods: 这个论文使用了RWKV变体的线性注意力变换器，combines the superior performance of transformers和RNNs的推理效率，适用于流处理ASR场景， где时间和内存预算有限。</li>
<li>results: 实验表明，RWKV-Transducer和RWKV-Boundary-Aware-Transducer在不同的规模（100h-10000h）上具有和chunk conformer transducer相当或更高的准确率，同时具有较少的延迟和推理内存成本。<details>
<summary>Abstract</summary>
Recently, self-attention-based transformers and conformers have been introduced as alternatives to RNNs for ASR acoustic modeling. Nevertheless, the full-sequence attention mechanism is non-streamable and computationally expensive, thus requiring modifications, such as chunking and caching, for efficient streaming ASR. In this paper, we propose to apply RWKV, a variant of linear attention transformer, to streaming ASR. RWKV combines the superior performance of transformers and the inference efficiency of RNNs, which is well-suited for streaming ASR scenarios where the budget for latency and memory is restricted. Experiments on varying scales (100h - 10000h) demonstrate that RWKV-Transducer and RWKV-Boundary-Aware-Transducer achieve comparable to or even better accuracy compared with chunk conformer transducer, with minimal latency and inference memory cost.
</details>
<details>
<summary>摘要</summary>
近些时候，基于自注意力的transformer和conformer被提出为RNN的替代者 для语音识别器模型。然而，全序列注意机制是不可流动的并且计算成本高，因此需要修改，如分割和缓存，以实现高效的流动语音识别。在这篇论文中，我们提议使用RWKV，一种变体的线性注意力变换器，来应用流动语音识别。RWKV结合了transformer的性能和RNN的推理效率，适合流动语音识别场景，具有限制的时钟和内存成本。在不同规模（100小时-10000小时）的实验中，RWKV-Transducer和RWKV-Boundary-Aware-Transducer实现了与分割对应者扫描器相当或更高的准确率，同时具有最小的延迟和推理内存成本。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Session-Variability-Leveraging-Session-Embeddings-for-Session-Robustness-in-Speaker-Verification"><a href="#Rethinking-Session-Variability-Leveraging-Session-Embeddings-for-Session-Robustness-in-Speaker-Verification" class="headerlink" title="Rethinking Session Variability: Leveraging Session Embeddings for Session Robustness in Speaker Verification"></a>Rethinking Session Variability: Leveraging Session Embeddings for Session Robustness in Speaker Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14741">http://arxiv.org/abs/2309.14741</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hee-Soo Heo, KiHyun Nam, Bong-Jin Lee, Youngki Kwon, Minjae Lee, You Jin Kim, Joon Son Chung</li>
<li>for: 减少Session或通道变化对Speaker验证的影响</li>
<li>methods: 使用额外的 embedding 表示Session信息，通过附加到Speaker embedding抽取器的 auxilary network 进行训练，从而获得两个相似性分数：一个为Speaker信息，另一个为Session信息</li>
<li>results: 无需重新训练 embedding extractor，Session信息可以得到有效的补偿<details>
<summary>Abstract</summary>
In the field of speaker verification, session or channel variability poses a significant challenge. While many contemporary methods aim to disentangle session information from speaker embeddings, we introduce a novel approach using an additional embedding to represent the session information. This is achieved by training an auxiliary network appended to the speaker embedding extractor which remains fixed in this training process. This results in two similarity scores: one for the speakers information and one for the session information. The latter score acts as a compensator for the former that might be skewed due to session variations. Our extensive experiments demonstrate that session information can be effectively compensated without retraining of the embedding extractor.
</details>
<details>
<summary>摘要</summary>
在说话识别领域，会话或渠道变化对 speaker 识别带来极大的挑战。虽然现代方法通常尝试将会话信息与说话人嵌入分离开来，但我们提出了一种新的方法，即通过附加一个额外的嵌入来表示会话信息。这种方法通过在 speaker 嵌入提取器的训练过程中附加一个辅助网络来实现。这将生成两个相似度分数：一个是说话人信息，另一个是会话信息。后者的分数将作为前者可能因会话变化而偏移的补偿。我们的广泛实验表明，不需要重新训练 embedding 提取器，就可以有效地补偿会话信息。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/26/cs.SD_2023_09_26/" data-id="clp89dok000zui78835sn0llw" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/39/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/38/">38</a><a class="page-number" href="/page/39/">39</a><span class="page-number current">40</span><a class="page-number" href="/page/41/">41</a><a class="page-number" href="/page/42/">42</a><span class="space">&hellip;</span><a class="page-number" href="/page/97/">97</a><a class="extend next" rel="next" href="/page/41/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">66</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">81</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">140</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
