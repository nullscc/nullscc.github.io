
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/11/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.AI_2023_10_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/18/cs.AI_2023_10_18/" class="article-date">
  <time datetime="2023-10-18T12:00:00.000Z" itemprop="datePublished">2023-10-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/18/cs.AI_2023_10_18/">cs.AI - 2023-10-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Learning-to-Solve-Climate-Sensor-Placement-Problems-with-a-Transformer"><a href="#Learning-to-Solve-Climate-Sensor-Placement-Problems-with-a-Transformer" class="headerlink" title="Learning to Solve Climate Sensor Placement Problems with a Transformer"></a>Learning to Solve Climate Sensor Placement Problems with a Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12387">http://arxiv.org/abs/2310.12387</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Wang, Victoria Huang, Gang Chen, Hui Ma, Bryce Chen, Jochen Schmidt</li>
<li>for: 该论文目的是提出一种基于深度学习的感知器布局方法，用于解决环境监测和灾害管理中的感知器布局问题。</li>
<li>methods: 该方法使用深度学习方法来自动生成优化策略，以解决环境监测和灾害管理中的感知器布局问题。</li>
<li>results: 对比多种现有方法，该方法能够生成高质量的解决方案，并且比现有方法更有效率。<details>
<summary>Abstract</summary>
The optimal placement of sensors for environmental monitoring and disaster management is a challenging problem due to its NP-hard nature. Traditional methods for sensor placement involve exact, approximation, or heuristic approaches, with the latter being the most widely used. However, heuristic methods are limited by expert intuition and experience. Deep learning (DL) has emerged as a promising approach for generating heuristic algorithms automatically. In this paper, we introduce a novel sensor placement approach focused on learning improvement heuristics using deep reinforcement learning (RL) methods. Our approach leverages an RL formulation for learning improvement heuristics, driven by an actor-critic algorithm for training the policy network. We compare our method with several state-of-the-art approaches by conducting comprehensive experiments, demonstrating the effectiveness and superiority of our proposed approach in producing high-quality solutions. Our work presents a promising direction for applying advanced DL and RL techniques to challenging climate sensor placement problems.
</details>
<details>
<summary>摘要</summary>
“环境监控和灾害管理中的仪器位置最佳化是一个NP困难的问题。传统方法包括精确、近似或规律方法，但这些方法受到专家智慧和经验的限制。深度学习（DL）已经成为一种可能的方法，用于生成自动生成规律算法。在这篇论文中，我们提出了一种新的仪器位置方法，利用深度强化学习（RL）方法学习改善规律。我们的方法利用了RL的形式来学习改善规律，驱动actor-critic算法来训练政策网。我们对多个现有方法进行了严详的实验，展示了我们的提案方法的有效性和优势。我们的工作呈现了应用进步的DL和RL技术解决气候仪器位置问题的可能性。”Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Online-Learning-and-Planning-in-Cognitive-Hierarchies"><a href="#Online-Learning-and-Planning-in-Cognitive-Hierarchies" class="headerlink" title="Online Learning and Planning in Cognitive Hierarchies"></a>Online Learning and Planning in Cognitive Hierarchies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12386">http://arxiv.org/abs/2310.12386</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Bernhard Hengst, Maurice Pagnucco, David Rajaratnam, Claude Sammut, Michael Thielscher</li>
<li>for: 本研究旨在探讨复杂 роботи变得需要多种机器人和人工智能技术的集成，以实现全球性和行为的协调。</li>
<li>methods: 本研究使用形式化框架来模型机器人系统的复杂 интеGRATION和有效的决策过程，从符号规划到在线学习策略和过渡系统。</li>
<li>results: 研究人员通过扩展Clark et al.（2016）的形式化框架，实现了复杂机器人系统的可靠和有效的 интеGRATION和决策过程。此外，新的框架还允许更加灵活地模型不同的决策组件之间的交互。<details>
<summary>Abstract</summary>
Complex robot behaviour typically requires the integration of multiple robotic and Artificial Intelligence (AI) techniques and components. Integrating such disparate components into a coherent system, while also ensuring global properties and behaviours, is a significant challenge for cognitive robotics. Using a formal framework to model the interactions between components can be an important step in dealing with this challenge. In this paper we extend an existing formal framework [Clark et al., 2016] to model complex integrated reasoning behaviours of robotic systems; from symbolic planning through to online learning of policies and transition systems. Furthermore the new framework allows for a more flexible modelling of the interactions between different reasoning components.
</details>
<details>
<summary>摘要</summary>
通常需要结合多种机器人和人工智能（AI）技术和组件来实现复杂的机器人行为。将这些不同的组件集成成一个一致的系统，并确保全球性和行为，是认知机器人的主要挑战。使用形式化框架来模型组件之间的交互可以是解决这个挑战的重要步骤。在这篇论文中，我们将对 Clark et al.（2016）的现有正式框架进行扩展，以模型机器人系统的复杂集成推理行为，从 симвоlic 规划到在线学习策略和转移系统。此外，新的框架还允许更加灵活地模型不同推理组件之间的交互。
</details></li>
</ul>
<hr>
<h2 id="Solving-Hard-Analogy-Questions-with-Relation-Embedding-Chains"><a href="#Solving-Hard-Analogy-Questions-with-Relation-Embedding-Chains" class="headerlink" title="Solving Hard Analogy Questions with Relation Embedding Chains"></a>Solving Hard Analogy Questions with Relation Embedding Chains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12379">http://arxiv.org/abs/2310.12379</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/niteshroyal/solvinghardanalogyquestions">https://github.com/niteshroyal/solvinghardanalogyquestions</a></li>
<li>paper_authors: Nitesh Kumar, Steven Schockaert</li>
<li>for: 本研究的目的是将概念之间的关系模型为路径，同时具有relation embedding的特性。</li>
<li>methods: 本研究使用了知识 graphs（KGs）如ConceptNet，并模型了两个概念之间的关系为一组路径。然而，KGs具有固定的关系类型，并且容易受到噪音和损害。本研究还使用了 fine-tuned语言模型来提取关系嵌入，但这并不适用于间接相关的词语和结构化领域知识。</li>
<li>results: 本研究提出了一种将路径和关系嵌入结合的方法，并通过实验表明其可以解决困难的相似问题。<details>
<summary>Abstract</summary>
Modelling how concepts are related is a central topic in Lexical Semantics. A common strategy is to rely on knowledge graphs (KGs) such as ConceptNet, and to model the relation between two concepts as a set of paths. However, KGs are limited to a fixed set of relation types, and they are incomplete and often noisy. Another strategy is to distill relation embeddings from a fine-tuned language model. However, this is less suitable for words that are only indirectly related and it does not readily allow us to incorporate structured domain knowledge. In this paper, we aim to combine the best of both worlds. We model relations as paths but associate their edges with relation embeddings. The paths are obtained by first identifying suitable intermediate words and then selecting those words for which informative relation embeddings can be obtained. We empirically show that our proposed representations are useful for solving hard analogy questions.
</details>
<details>
<summary>摘要</summary>
模型两个概念之间的关系是lexical semantics中的一个中心话题。一种常见的策略是通过知识图(KG)such as ConceptNet，并将两个概念之间的关系表示为一组路径。然而，KGs是有限的，并且经常受到干扰和噪声的影响。另一种策略是通过精心调整的自然语言模型提取关系嵌入。然而，这并不适用于间接相关的词语，而且不能轻松地包含结构化领域知识。在这篇论文中，我们决心将这两种方法结合在一起。我们模型关系为路径，并将路径上的边与关系嵌入相关联。我们首先 identific suitable intermediate words，然后选择这些words，以便可以获取有用的关系嵌入。我们的提议的表示方法在解决困难的类比问题上表现出色。
</details></li>
</ul>
<hr>
<h2 id="ClusT3-Information-Invariant-Test-Time-Training"><a href="#ClusT3-Information-Invariant-Test-Time-Training" class="headerlink" title="ClusT3: Information Invariant Test-Time Training"></a>ClusT3: Information Invariant Test-Time Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12345">http://arxiv.org/abs/2310.12345</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dosowiechi/clust3">https://github.com/dosowiechi/clust3</a></li>
<li>paper_authors: Gustavo A. Vargas Hakim, David Osowiechi, Mehrdad Noori, Milad Cheraghalikhani, Ismail Ben Ayed, Christian Desrosiers</li>
<li>for: 提高深度学习模型对不同预测环境的鲁棒性</li>
<li>methods: 提出了一种基于维度信息最大化的无监督测试时培养技术，通过同时在训练时进行多尺度特征图和整数 latent representation 的匹配，实现在测试时使用自动生成的proxy任务来适应不同预测环境。</li>
<li>results: 实验结果表明，该技术可以在不同的测试时适应 benchmark 上达到竞争力的分类性能。<details>
<summary>Abstract</summary>
Deep Learning models have shown remarkable performance in a broad range of vision tasks. However, they are often vulnerable against domain shifts at test-time. Test-time training (TTT) methods have been developed in an attempt to mitigate these vulnerabilities, where a secondary task is solved at training time simultaneously with the main task, to be later used as an self-supervised proxy task at test-time. In this work, we propose a novel unsupervised TTT technique based on the maximization of Mutual Information between multi-scale feature maps and a discrete latent representation, which can be integrated to the standard training as an auxiliary clustering task. Experimental results demonstrate competitive classification performance on different popular test-time adaptation benchmarks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Eliminating-Reasoning-via-Inferring-with-Planning-A-New-Framework-to-Guide-LLMs’-Non-linear-Thinking"><a href="#Eliminating-Reasoning-via-Inferring-with-Planning-A-New-Framework-to-Guide-LLMs’-Non-linear-Thinking" class="headerlink" title="Eliminating Reasoning via Inferring with Planning: A New Framework to Guide LLMs’ Non-linear Thinking"></a>Eliminating Reasoning via Inferring with Planning: A New Framework to Guide LLMs’ Non-linear Thinking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12342">http://arxiv.org/abs/2310.12342</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongqi Tong, Yifan Wang, Dawei Li, Sizhe Wang, Zi Lin, Simeng Han, Jingbo Shang<br>for: 这研究旨在强化大语言模型（LLM）的高级逻辑能力，通过模拟人类线性思维和逻辑的混合。methods: 这研究提出了新的提示方法，即排除逻辑提示（IEP），它将排除逻辑和推理结合起来，以便LLM可以更好地模拟人类的非线性思维。results: 研究发现，IEP可以在多种任务上consistently outperform CoT，并且可以和CoT结合使用，以提高LLM的表现。此外，研究还引入了新的benchmark，即MENTAL-ABILITY REASONING BENCHMARK（MARB），以评估LLM的逻辑和语言理解能力。<details>
<summary>Abstract</summary>
Chain-of-Thought(CoT) prompting and its variants explore equipping large language models (LLMs) with high-level reasoning abilities by emulating human-like linear cognition and logic. However, the human mind is complicated and mixed with both linear and nonlinear thinking. In this work, we propose \textbf{I}nferential \textbf{E}xclusion \textbf{P}rompting (IEP), a novel prompting that combines the principles of elimination and inference in order to guide LLMs to think non-linearly. IEP guides LLMs to plan and then utilize Natural Language Inference (NLI) to deduce each possible solution's entailment relation with context, commonsense, or facts, therefore yielding a broader perspective by thinking back for inferring. This forward planning and backward eliminating process allows IEP to better simulate the complex human thinking processes compared to other CoT-based methods, which only reflect linear cognitive processes. We conducted a series of empirical studies and have corroborated that IEP consistently outperforms CoT across various tasks. Additionally, we observe that integrating IEP and CoT further improves the LLMs' performance on certain tasks, highlighting the necessity of equipping LLMs with mixed logic processes. Moreover, to better evaluate comprehensive features inherent in human logic, we introduce \textbf{M}ental-\textbf{A}bility \textbf{R}easoning \textbf{B}enchmark (MARB). The benchmark comprises six novel subtasks with a total of 9,115 questions, among which 1,685 are developed with hand-crafted rationale references. We believe both \textsc{IEP} and \textsc{MARB} can serve as a promising direction for unveiling LLMs' logic and verbal reasoning abilities and drive further advancements. \textsc{MARB} will be available at ~\texttt{anonymity link} soon.
</details>
<details>
<summary>摘要</summary>
Chain-of-Thought（CoT）提示和其变种探索将大型语言模型（LLM）具备高级思维能力，通过模拟人类线性认知和逻辑。然而，人类思维是复杂的，混合了线性和非线性思维。在这项工作中，我们提出了《排除并推理》（IEP）提示，它结合排除和推理的原理，以引导 LLM 进行非线性思维。IEP 使 LLM 可以规划，然后通过自然语言推理（NLI）来推理每个可能解的上下文、通用智慧和事实的关系，从而获得更广泛的视野。这种前置规划和后置排除过程使 IEP 更能模拟人类思维过程，相比其他 CoT 基于方法。我们进行了一系列实验研究，并证明 IEP 在多种任务上表现出色。此外，我们发现将 IEP 和 CoT 集成可以进一步提高 LLMS 的表现，强调了训练 LLMs 的混合逻辑过程的必要性。此外，为了更好地评估人类逻辑的全面特征，我们引入了《MENTAL-ABILITY REASONING BENCHMARK》（MARB）。 MARB 包括六个新的任务，共计 9,115 个问题，其中 1,685 个问题采用了手动制作的 rational references。我们认为 IEP 和 MARB 都可以成为探索 LLMs 逻辑和语言逻辑能力的有希望的方向，并驱动进一步的进步。MARB 将在 ~\texttt{anonymity link} 上公开。
</details></li>
</ul>
<hr>
<h2 id="Opportunities-for-Adaptive-Experiments-to-Enable-Continuous-Improvement-that-Trades-off-Instructor-and-Researcher-Incentives"><a href="#Opportunities-for-Adaptive-Experiments-to-Enable-Continuous-Improvement-that-Trades-off-Instructor-and-Researcher-Incentives" class="headerlink" title="Opportunities for Adaptive Experiments to Enable Continuous Improvement that Trades-off Instructor and Researcher Incentives"></a>Opportunities for Adaptive Experiments to Enable Continuous Improvement that Trades-off Instructor and Researcher Incentives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12324">http://arxiv.org/abs/2310.12324</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilya Musabirov, Angela Zavaleta-Bernuy, Pan Chen, Michael Liut, Joseph Jay Williams</li>
<li>for: 这个论文的目的是提供一种基于机器学习的adaptive experimentation方法，用于持续改进高等教育课程。</li>
<li>methods: 这篇论文使用了机器学习算法来分析数据，并在不同的学生群中采用不同的condition进行比较，以确定最有效的condition。</li>
<li>results: 这篇论文的实验结果表明，使用adaptive experimentation方法可以更好地支持学生的需求，并提高学生的学习效果。<details>
<summary>Abstract</summary>
Randomized experimental comparisons of alternative pedagogical strategies could provide useful empirical evidence in instructors' decision-making. However, traditional experiments do not have a clear and simple pathway to using data rapidly to try to increase the chances that students in an experiment get the best conditions. Drawing inspiration from the use of machine learning and experimentation in product development at leading technology companies, we explore how adaptive experimentation might help in continuous course improvement. In adaptive experiments, as different arms/conditions are deployed to students, data is analyzed and used to change the experience for future students. This can be done using machine learning algorithms to identify which actions are more promising for improving student experience or outcomes. This algorithm can then dynamically deploy the most effective conditions to future students, resulting in better support for students' needs. We illustrate the approach with a case study providing a side-by-side comparison of traditional and adaptive experimentation of self-explanation prompts in online homework problems in a CS1 course. This provides a first step in exploring the future of how this methodology can be useful in bridging research and practice in doing continuous improvement.
</details>
<details>
<summary>摘要</summary>
随机实验比较不同的教学策略可以提供有用的实际证据，帮助教师做出决策。然而，传统的实验没有一个明确的和简单的数据使用路径，这限制了学生在实验中获得最佳条件的机会。我们从技术公司的产品开发中使用机器学习和实验的经验而来，探讨如何使用适应试验来促进课程不断改进。在适应试验中，不同的臂/条件在学生面前采用，并分析数据，以改善未来学生的经验。这可以使用机器学习算法来确定哪些行动更有前途的提高学生体验或成绩。这个算法然后会在未来学生面前动态部署最有效的条件，从而提供更好的学生需求支持。我们通过一个案例研究，对传统和适应试验自适应提示在线作业问题的比较，以示方法的可行性。这是继续改进的未来的一个初步探索。
</details></li>
</ul>
<hr>
<h2 id="The-Sentiment-Problem-A-Critical-Survey-towards-Deconstructing-Sentiment-Analysis"><a href="#The-Sentiment-Problem-A-Critical-Survey-towards-Deconstructing-Sentiment-Analysis" class="headerlink" title="The Sentiment Problem: A Critical Survey towards Deconstructing Sentiment Analysis"></a>The Sentiment Problem: A Critical Survey towards Deconstructing Sentiment Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12318">http://arxiv.org/abs/2310.12318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranav Narayanan Venkit, Mukund Srinath, Sanjana Gautam, Saranya Venkatraman, Vipul Gupta, Rebecca J. Passonneau, Shomir Wilson</li>
<li>for: 本研究探讨了 sentiment analysis (SA) 在不同社技系统中的应用、模型和数据集方面的问题。</li>
<li>methods: 研究者通过审查 189 篇同行评审文章，探讨 SA 在不同领域中的应用和模型，以及数据集的问题。</li>
<li>results: 研究发现 SA 在不同领域中的定义和应用存在差异，导致可能的挑战和偏见。为解决这问题，研究者提出了一个伦理卡，以帮助实践者在使用 SA 时确保公正使用。<details>
<summary>Abstract</summary>
We conduct an inquiry into the sociotechnical aspects of sentiment analysis (SA) by critically examining 189 peer-reviewed papers on their applications, models, and datasets. Our investigation stems from the recognition that SA has become an integral component of diverse sociotechnical systems, exerting influence on both social and technical users. By delving into sociological and technological literature on sentiment, we unveil distinct conceptualizations of this term in domains such as finance, government, and medicine. Our study exposes a lack of explicit definitions and frameworks for characterizing sentiment, resulting in potential challenges and biases. To tackle this issue, we propose an ethics sheet encompassing critical inquiries to guide practitioners in ensuring equitable utilization of SA. Our findings underscore the significance of adopting an interdisciplinary approach to defining sentiment in SA and offer a pragmatic solution for its implementation.
</details>
<details>
<summary>摘要</summary>
我们进行了一个关于社会技术方面的情感分析（SA）的调查， kritically examining 189 peer-reviewed papers on their applications, models, and datasets。我们的调查源于认识到SA已成为多种社会技术系统的重要组成部分，影响社会和技术用户。通过探究社会学和技术文献中的情感概念，我们揭示了不同领域中情感的不同定义和概念化。我们的研究发现了情感定义和框架的明确性不足，可能导致挑战和偏见。为解决这个问题，我们提议一份伦理宣言，涵盖了重要的伦理问题，以帮助实践者在使用SA时确保公正使用。我们的发现表明了采用多科学方法来定义情感在SA中的重要性，并提供了一个实用的解决方案。
</details></li>
</ul>
<hr>
<h2 id="A-Unifying-Framework-for-Learning-Argumentation-Semantics"><a href="#A-Unifying-Framework-for-Learning-Argumentation-Semantics" class="headerlink" title="A Unifying Framework for Learning Argumentation Semantics"></a>A Unifying Framework for Learning Argumentation Semantics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12309">http://arxiv.org/abs/2310.12309</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zlatina Mileva, Antonis Bikakis, Fabio Aurelio D’Asaro, Mark Law, Alessandra Russo</li>
<li>for: 这篇论文是关于人工智能领域的论证推理研究，旨在提出一种可解释的论证Acceptability semantics的框架，以便在人机对话中使用。</li>
<li>methods: 该论文使用了逻辑编程方法，通过学习来计算论证的接受性。</li>
<li>results: 经验证试验表明，该框架可以在论证计算中具有较高的性能，并且可以在人机对话中提供更加可靠的结果。<details>
<summary>Abstract</summary>
Argumentation is a very active research field of Artificial Intelligence concerned with the representation and evaluation of arguments used in dialogues between humans and/or artificial agents. Acceptability semantics of formal argumentation systems define the criteria for the acceptance or rejection of arguments. Several software systems, known as argumentation solvers, have been developed to compute the accepted/rejected arguments using such criteria. These include systems that learn to identify the accepted arguments using non-interpretable methods. In this paper we present a novel framework, which uses an Inductive Logic Programming approach to learn the acceptability semantics for several abstract and structured argumentation frameworks in an interpretable way. Through an empirical evaluation we show that our framework outperforms existing argumentation solvers, thus opening up new future research directions in the area of formal argumentation and human-machine dialogues.
</details>
<details>
<summary>摘要</summary>
争议是人工智能的一个非常活跃的研究领域，涉及对人类和/或人工代理人之间的对话中使用的论据的表示和评估。正式争议系统的 Acceptability  semantics 定义了论据的接受或拒绝的标准。一些称为争议解决器的软件系统已经被开发出来计算使用这些标准来接受或拒绝论据。这些系统包括使用非可解释的方法来识别接受的论据的学习系统。在这篇论文中，我们提出了一种新的框架，使用逻辑编程方法来学习多种抽象和结构化争议框架的接受可能性，并在实验评估中证明了我们的框架可以在接受可能性评估方面超越现有的争议解决器，从而开启了新的未来研究方向在正式争议和人机对话领域。
</details></li>
</ul>
<hr>
<h2 id="Preference-Optimization-for-Molecular-Language-Models"><a href="#Preference-Optimization-for-Molecular-Language-Models" class="headerlink" title="Preference Optimization for Molecular Language Models"></a>Preference Optimization for Molecular Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12304">http://arxiv.org/abs/2310.12304</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/harmonic-discovery/pref-opt-for-mols">https://github.com/harmonic-discovery/pref-opt-for-mols</a></li>
<li>paper_authors: Ryan Park, Ryan Theisen, Navriti Sahni, Marcel Patek, Anna Cichońska, Rayees Rahman</li>
<li>for: 用于生成新的化学结构</li>
<li>methods: 使用直接偏好优化精度调整</li>
<li>results: 高效、简单、有效地与化学家喜好Alignment of generated molecules<details>
<summary>Abstract</summary>
Molecular language modeling is an effective approach to generating novel chemical structures. However, these models do not \emph{a priori} encode certain preferences a chemist may desire. We investigate the use of fine-tuning using Direct Preference Optimization to better align generated molecules with chemist preferences. Our findings suggest that this approach is simple, efficient, and highly effective.
</details>
<details>
<summary>摘要</summary>
分子语言模型可以有效地生成新的化学结构。然而，这些模型没有先验的编码化学家可能愿望的偏好。我们研究了使用直接偏好优化来更好地将生成的分子与化学家的偏好相Alignment。我们发现这种方法简单、高效并有高效果。Here's a word-for-word translation:分子语言模型可以有效地生成新的化学结构。然而，这些模型没有先验的编码化学家可能愿望的偏好。我们研究了使用直接偏好优化来更好地将生成的分子与化学家的偏好相Alignment。我们发现这种方法简单、高效并有高效果。
</details></li>
</ul>
<hr>
<h2 id="Document-Level-Language-Models-for-Machine-Translation"><a href="#Document-Level-Language-Models-for-Machine-Translation" class="headerlink" title="Document-Level Language Models for Machine Translation"></a>Document-Level Language Models for Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12303">http://arxiv.org/abs/2310.12303</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sfedfcv/redesigned-pancake">https://github.com/Sfedfcv/redesigned-pancake</a></li>
<li>paper_authors: Frithjof Petrick, Christian Herold, Pavel Petrushkov, Shahram Khadivi, Hermann Ney</li>
<li>for: 提高文档翻译系统的 Context-awareness，使其能够更好地理解文档的含义和结构。</li>
<li>methods: 组合现有的 sentence-level 翻译模型和文档级别的语言模型，并使用 novel weighting techniques 来提高系统的灵活性和计算效率。</li>
<li>results: 在四种多样化的翻译任务上进行了全面的评估，并显示了substantially 提高的文档指向得分，同时也更加计算效率。但是，我们还发现，通过回译来获得更好的结果，但是需要重新训练翻译系统。此外，我们还探讨了大语言模型的混合，并发现可能在使用大语言模型时存在强大的潜在性。<details>
<summary>Abstract</summary>
Despite the known limitations, most machine translation systems today still operate on the sentence-level. One reason for this is, that most parallel training data is only sentence-level aligned, without document-level meta information available. In this work, we set out to build context-aware translation systems utilizing document-level monolingual data instead. This can be achieved by combining any existing sentence-level translation model with a document-level language model. We improve existing approaches by leveraging recent advancements in model combination. Additionally, we propose novel weighting techniques that make the system combination more flexible and significantly reduce computational overhead. In a comprehensive evaluation on four diverse translation tasks, we show that our extensions improve document-targeted scores substantially and are also computationally more efficient. However, we also find that in most scenarios, back-translation gives even better results, at the cost of having to re-train the translation system. Finally, we explore language model fusion in the light of recent advancements in large language models. Our findings suggest that there might be strong potential in utilizing large language models via model combination.
</details>
<details>
<summary>摘要</summary>
尽管现有的机器翻译系统 todavía 以句子为单位运行，一个原因是因为大多数平行训练数据只有句子水平的对齐，没有文档水平的元信息可用。在这项工作中，我们设想建立了文本上下文感知的翻译系统，使用文档水平的独立语言模型。我们提高了现有的方法，利用最新的模型组合技术。此外，我们提出了新的权重技巧，使系统组合更加灵活，并显著减少计算负担。在四种多样化的翻译任务上进行了全面的评估，我们发现我们的扩展可以大幅提高文档目标得分，并且计算更加高效。然而，我们也发现，在大多数情况下，回传翻译能够提供更好的结果，但是需要重新训练翻译系统。最后，我们探讨了大语言模型的集成，我们发现大语言模型可以通过模型组合来提供强大的潜在力。
</details></li>
</ul>
<hr>
<h2 id="Jorge-Approximate-Preconditioning-for-GPU-efficient-Second-order-Optimization"><a href="#Jorge-Approximate-Preconditioning-for-GPU-efficient-Second-order-Optimization" class="headerlink" title="Jorge: Approximate Preconditioning for GPU-efficient Second-order Optimization"></a>Jorge: Approximate Preconditioning for GPU-efficient Second-order Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12298">http://arxiv.org/abs/2310.12298</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddharth Singh, Zachary Sating, Abhinav Bhatele</li>
<li>for: 这篇论文的目的是提出一种高效的二阶优化器，以提高深度学习模型的训练效率和性能。</li>
<li>methods: 这篇论文使用了一种新的二阶优化器 named Jorge，它通过简化预conditioning步骤，从而大大减少了计算成本，使其在GPU上实现高效。</li>
<li>results: 实验结果表明，Jorge可以与现有的优化器，如SGD、AdamW和Shampoo等比肩，并在多个深度学习模型上显示出更高的效率和性能。<details>
<summary>Abstract</summary>
Despite their better convergence properties compared to first-order optimizers, second-order optimizers for deep learning have been less popular due to their significant computational costs. The primary efficiency bottleneck in such optimizers is matrix inverse calculations in the preconditioning step, which are expensive to compute on GPUs. In this paper, we introduce Jorge, a second-order optimizer that promises the best of both worlds -- rapid convergence benefits of second-order methods, and high computational efficiency typical of first-order methods. We address the primary computational bottleneck of computing matrix inverses by completely eliminating them using an approximation of the preconditioner computation. This makes Jorge extremely efficient on GPUs in terms of wall-clock time. Further, we describe an approach to determine Jorge's hyperparameters directly from a well-tuned SGD baseline, thereby significantly minimizing tuning efforts. Our empirical evaluations demonstrate the distinct advantages of using Jorge, outperforming state-of-the-art optimizers such as SGD, AdamW, and Shampoo across multiple deep learning models, both in terms of sample efficiency and wall-clock time.
</details>
<details>
<summary>摘要</summary>
尽管第二顺序优化器在深度学习中的更好的整合性，但由于计算成本高涨，使得它们在实际应用中较少使用。在这篇论文中，我们介绍了 Jorge，一种第二顺序优化器，它可以同时具有第一顺序优化器的快速整合和高效计算性。我们通过完全抛弃矩阵逆计算，使得 Jorge 在 GPU 上具有高效的墙 clock 时间。此外，我们还提出了一种确定 Jorge 的超参数的方法，通过对已经优化的 SGD 基线进行调整，以此减少调整努力。我们的实验表明，使用 Jorge 可以获得明显的优势，在多种深度学习模型上，在样本效率和墙 clock 时间两个方面都超过了状态元优化器，如 SGD、AdamW 和 Shampoo。
</details></li>
</ul>
<hr>
<h2 id="Fact-based-Agent-modeling-for-Multi-Agent-Reinforcement-Learning"><a href="#Fact-based-Agent-modeling-for-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Fact-based Agent modeling for Multi-Agent Reinforcement Learning"></a>Fact-based Agent modeling for Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12290">http://arxiv.org/abs/2310.12290</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Baofu Fang, Caiming Zheng, Hao Wang</li>
<li>for: 提高多智能体系中agent之间协作和互动的效率，在 unknown 环境下实现agent模型化。</li>
<li>methods: 使用fact-based belief inference（FBI）网络模型其他智能体的行为和意图，通过variational autoencoder（VAE）学习智能体政策表示。</li>
<li>results: 在多智能体粒子环境（MPE）中比基eline方法高效地提高agent政策学习效率，在复杂的竞争合作混合enario中实现更高的返点。<details>
<summary>Abstract</summary>
In multi-agent systems, agents need to interact and collaborate with other agents in environments. Agent modeling is crucial to facilitate agent interactions and make adaptive cooperation strategies. However, it is challenging for agents to model the beliefs, behaviors, and intentions of other agents in non-stationary environment where all agent policies are learned simultaneously. In addition, the existing methods realize agent modeling through behavior cloning which assume that the local information of other agents can be accessed during execution or training. However, this assumption is infeasible in unknown scenarios characterized by unknown agents, such as competition teams, unreliable communication and federated learning due to privacy concerns. To eliminate this assumption and achieve agent modeling in unknown scenarios, Fact-based Agent modeling (FAM) method is proposed in which fact-based belief inference (FBI) network models other agents in partially observable environment only based on its local information. The reward and observation obtained by agents after taking actions are called facts, and FAM uses facts as reconstruction target to learn the policy representation of other agents through a variational autoencoder. We evaluate FAM on various Multiagent Particle Environment (MPE) and compare the results with several state-of-the-art MARL algorithms. Experimental results show that compared with baseline methods, FAM can effectively improve the efficiency of agent policy learning by making adaptive cooperation strategies in multi-agent reinforcement learning tasks, while achieving higher returns in complex competitive-cooperative mixed scenarios.
</details>
<details>
<summary>摘要</summary>
在多代理系统中，代理需要互动和合作，在环境中进行交互。代理模型是重要的，以便促进代理之间的交互和适应合作策略。然而，在非站ARY环境中，所有代理策略都是同时学习的，对于代理来模型别人的信念、行为和意图是挑战。此外，现有的方法通过行为做副本来实现代理模型，假设在执行或训练中可以访问其他代理的本地信息。然而，这个假设在未知场景中是不可能的，例如竞争队伍、不可靠的通信和联合学习中的隐私问题。为了绕过这个假设并实现代理模型在未知场景中，我们提出了基于事实的代理模型（FAM）方法。FAM使用事实（即代理所获得的奖励和观察）为重建目标，通过变分自动编码器来学习别人的策略表示。我们在多代理粒子环境（MPE）上进行了评估，并与一些现有的 MARL 算法进行了比较。实验结果表明，相比基eline方法，FAM可以更好地提高代理策略学习效率，在多代理束缚学习任务中实现更高的返回，而且在复杂的竞争-合作混合场景中 achieve higher returns。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-the-Performance-of-Automated-Grade-Prediction-in-MOOC-using-Graph-Representation-Learning"><a href="#Enhancing-the-Performance-of-Automated-Grade-Prediction-in-MOOC-using-Graph-Representation-Learning" class="headerlink" title="Enhancing the Performance of Automated Grade Prediction in MOOC using Graph Representation Learning"></a>Enhancing the Performance of Automated Grade Prediction in MOOC using Graph Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12281">http://arxiv.org/abs/2310.12281</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dsaatusu/mooper_grade_prediction">https://github.com/dsaatusu/mooper_grade_prediction</a></li>
<li>paper_authors: Soheila Farokhi, Aswani Yaramala, Jiangtao Huang, Muhammad F. A. Khan, Xiaojun Qi, Hamid Karimi<br>for:The paper is written for the purpose of enhancing the performance of predictive machine learning models in student assignment grade prediction for MOOCs.methods:The paper uses graph embedding techniques to extract latent structural information encoded in the interactions between entities in the MOOC dataset, without requiring ground truth labels.results:The paper demonstrates that structural features can significantly improve the predictive performance of downstream assessment tasks, and the code and data are available in \url{<a target="_blank" rel="noopener" href="https://github.com/DSAatUSU/MOOPer_grade_prediction%7D">https://github.com/DSAatUSU/MOOPer_grade_prediction}</a>.<details>
<summary>Abstract</summary>
In recent years, Massive Open Online Courses (MOOCs) have gained significant traction as a rapidly growing phenomenon in online learning. Unlike traditional classrooms, MOOCs offer a unique opportunity to cater to a diverse audience from different backgrounds and geographical locations. Renowned universities and MOOC-specific providers, such as Coursera, offer MOOC courses on various subjects. Automated assessment tasks like grade and early dropout predictions are necessary due to the high enrollment and limited direct interaction between teachers and learners. However, current automated assessment approaches overlook the structural links between different entities involved in the downstream tasks, such as the students and courses. Our hypothesis suggests that these structural relationships, manifested through an interaction graph, contain valuable information that can enhance the performance of the task at hand. To validate this, we construct a unique knowledge graph for a large MOOC dataset, which will be publicly available to the research community. Furthermore, we utilize graph embedding techniques to extract latent structural information encoded in the interactions between entities in the dataset. These techniques do not require ground truth labels and can be utilized for various tasks. Finally, by combining entity-specific features, behavioral features, and extracted structural features, we enhance the performance of predictive machine learning models in student assignment grade prediction. Our experiments demonstrate that structural features can significantly improve the predictive performance of downstream assessment tasks. The code and data are available in \url{https://github.com/DSAatUSU/MOOPer_grade_prediction}
</details>
<details>
<summary>摘要</summary>
近年来，大规模在线开放课程（MOOC）在在线学习中得到了广泛的应用和发展。不同于传统的教室，MOOCs为不同背景和地理位置的学生提供了独特的学习机会。知名大学和MOOC专门提供者，如 Coursera，为多种主题的MOOC课程。由于大量报名和教师与学生之间的直接交互有限，因此自动评估任务如学生的评价和早期退出预测变得必要。然而，当前的自动评估方法忽略了学生和课程之间的结构关系。我们的假设是，这些结构关系，通过互动图表示出来，含有价值信息，可以提高任务的表现。为此，我们构建了一个大 MOOC 数据集的专用知识图，该图将在研究社区中公开。此外，我们利用图像技术来提取数据集中互动图中所隐藏的结构信息。这些技术不需要标注数据，可以用于多种任务。最后，我们将实体特征、行为特征和提取的结构特征相结合，提高预测机器学习模型的学生评价分数预测性能。我们的实验表明，结构特征可以显著提高下游评估任务的预测性能。代码和数据可以在 <https://github.com/DSAatUSU/MOOPer_grade_prediction> 中找到。
</details></li>
</ul>
<hr>
<h2 id="An-Image-is-Worth-Multiple-Words-Learning-Object-Level-Concepts-using-Multi-Concept-Prompt-Learning"><a href="#An-Image-is-Worth-Multiple-Words-Learning-Object-Level-Concepts-using-Multi-Concept-Prompt-Learning" class="headerlink" title="An Image is Worth Multiple Words: Learning Object Level Concepts using Multi-Concept Prompt Learning"></a>An Image is Worth Multiple Words: Learning Object Level Concepts using Multi-Concept Prompt Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12274">http://arxiv.org/abs/2310.12274</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lxasqjc/mcpl">https://github.com/lxasqjc/mcpl</a></li>
<li>paper_authors: Chen Jin, Ryutaro Tanno, Amrutha Saseendran, Tom Diethe, Philip Teare</li>
<li>for: 本研究旨在学习一种新的”词”来表示图像风格和外观，并将其集成到自然语言句子中生成新的合成图像。</li>
<li>methods: 我们提出了一种多个概念提示学习（MCPL）框架，在单个句子-图像对中同时学习多个新”词”。为了提高词概念相关性的准确性，我们提出了三种REG regularization技术：注意力掩码（AttnMask），提示对比损失（PromptCL）和绑定形容词（Bind adj。）。</li>
<li>results: 我们通过图像生成、编辑和注意力可视化等方式进行了广泛的量化比较，demonstrating that our method can learn more semantically disentangled concepts with enhanced word-concept correlation。此外，我们还介绍了一个新的数据集和评价协议，专门为这种学习对象级概念的新任务。<details>
<summary>Abstract</summary>
Textural Inversion, a prompt learning method, learns a singular embedding for a new "word" to represent image style and appearance, allowing it to be integrated into natural language sentences to generate novel synthesised images. However, identifying and integrating multiple object-level concepts within one scene poses significant challenges even when embeddings for individual concepts are attainable. This is further confirmed by our empirical tests. To address this challenge, we introduce a framework for Multi-Concept Prompt Learning (MCPL), where multiple new "words" are simultaneously learned from a single sentence-image pair. To enhance the accuracy of word-concept correlation, we propose three regularisation techniques: Attention Masking (AttnMask) to concentrate learning on relevant areas; Prompts Contrastive Loss (PromptCL) to separate the embeddings of different concepts; and Bind adjective (Bind adj.) to associate new "words" with known words. We evaluate via image generation, editing, and attention visualisation with diverse images. Extensive quantitative comparisons demonstrate that our method can learn more semantically disentangled concepts with enhanced word-concept correlation. Additionally, we introduce a novel dataset and evaluation protocol tailored for this new task of learning object-level concepts.
</details>
<details>
<summary>摘要</summary>
文本倒转，一种快速学习方法，学习一个新的"词"来表示图像风格和外观，以便将其 integrate into natural language sentences 生成新的合成图像。然而，在一个场景中identifying和integrating多个对象水平的概念 pose significant challenges， Even when embeddings for individual concepts are available. This is further confirmed by our empirical tests. To address this challenge, we introduce a framework for Multi-Concept Prompt Learning (MCPL), where multiple new "words" are simultaneously learned from a single sentence-image pair. To enhance the accuracy of word-concept correlation, we propose three regularization techniques: Attention Masking (AttnMask) to concentrate learning on relevant areas; Prompts Contrastive Loss (PromptCL) to separate the embeddings of different concepts; and Bind adjective (Bind adj.) to associate new "words" with known words. We evaluate via image generation, editing, and attention visualization with diverse images. Extensive quantitative comparisons demonstrate that our method can learn more semantically disentangled concepts with enhanced word-concept correlation. Additionally, we introduce a novel dataset and evaluation protocol tailored for this new task of learning object-level concepts.
</details></li>
</ul>
<hr>
<h2 id="Tailoring-Adversarial-Attacks-on-Deep-Neural-Networks-for-Targeted-Class-Manipulation-Using-DeepFool-Algorithm"><a href="#Tailoring-Adversarial-Attacks-on-Deep-Neural-Networks-for-Targeted-Class-Manipulation-Using-DeepFool-Algorithm" class="headerlink" title="Tailoring Adversarial Attacks on Deep Neural Networks for Targeted Class Manipulation Using DeepFool Algorithm"></a>Tailoring Adversarial Attacks on Deep Neural Networks for Targeted Class Manipulation Using DeepFool Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13019">http://arxiv.org/abs/2310.13019</a></li>
<li>repo_url: None</li>
<li>paper_authors: S. M. Fazle Rabby Labib, Joyanta Jyoti Mondal, Meem Arafat Manab</li>
<li>for: 本研究旨在提出一种可argeting Specific classes的深度骗客（Targeted DeepFool），以提高深度神经网络（DNNs）的鲁棒性。</li>
<li>methods: 本文提出了一种基于DeepFool算法的Targeted DeepFool算法，并引入了最低信任分数的超参数，以提高灵活性。</li>
<li>results: 我们的实验表明，Targeted DeepFool算法可以在不同的深度神经网络架构上实现高效率和图像质量保持，而且可以增强模型的鲁棒性。 results show that one of the deep convolutional neural network architectures, AlexNet, and one of the state-of-the-art model Vision Transformer exhibit high robustness to getting fooled.<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) have significantly advanced various domains, but their vulnerability to adversarial attacks poses serious concerns. Understanding these vulnerabilities and developing effective defense mechanisms is crucial. DeepFool, an algorithm proposed by Moosavi-Dezfooli et al. (2016), finds minimal perturbations to misclassify input images. However, DeepFool lacks a targeted approach, making it less effective in specific attack scenarios. Also, in previous related works, researchers primarily focus on success, not considering how much an image is getting distorted; the integrity of the image quality, and the confidence level to misclassifying. So, in this paper, we propose Targeted DeepFool, an augmented version of DeepFool that allows targeting specific classes for misclassification. We also introduce a minimum confidence score requirement hyperparameter to enhance flexibility. Our experiments demonstrate the effectiveness and efficiency of the proposed method across different deep neural network architectures while preserving image integrity as much as possible. Results show that one of the deep convolutional neural network architectures, AlexNet, and one of the state-of-the-art model Vision Transformer exhibit high robustness to getting fooled. Our code will be made public when publishing the paper.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNNs）在不同领域中得到了 significiant advancement，但它们受到了敌意攻击的威胁，这种威胁的存在对于理解和开发有效防御机制是非常重要。DeepFool算法，由Moosavi-Dezfooli et al.（2016）提出，可以在输入图像上发现微小的扰动，以让图像被误分类。然而，DeepFool算法缺乏目标化方法，这使得其在特定攻击enario下效果较差。此外，在先前的相关研究中，研究人员主要关注成功，而不是图像的纯度和误分类的信息量。因此，在这篇论文中，我们提出了Targeted DeepFool算法，这是对DeepFool算法的扩展，可以对特定的类进行误分类。我们还引入了最小信任分数的启用参数，以提高灵活性。我们的实验表明，提议的方法可以在不同的深度神经网络架构上进行效果和效率的混合，同时保持图像的纯度。结果显示，AlexNet和一种state-of-the-art模型Vision Transformer在深度神经网络架构上具有高度的抗攻击能力。我们将代码公开时出版论文。
</details></li>
</ul>
<hr>
<h2 id="A-Unified-Approach-to-Domain-Incremental-Learning-with-Memory-Theory-and-Algorithm"><a href="#A-Unified-Approach-to-Domain-Incremental-Learning-with-Memory-Theory-and-Algorithm" class="headerlink" title="A Unified Approach to Domain Incremental Learning with Memory: Theory and Algorithm"></a>A Unified Approach to Domain Incremental Learning with Memory: Theory and Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12244">http://arxiv.org/abs/2310.12244</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haizhou Shi, Hao Wang</li>
<li>For: 本研究旨在提出一个统一架构，以应对不同领域的渐进式学习问题，并且仅从先前领域中获取一小部分的数据（即记忆）进行学习。* Methods: 本研究提出了一个统一架构， named Unified Domain Incremental Learning (UDIL)，它整合了多种现有的方法，并且通过在训练过程中适应不同的参数，以获得最紧密的一致 bound。* Results: 实验结果显示，UDIL 比先前的领域渐进式学习方法在both synthetic和实际数据集上表现更好，并且可以适应不同的领域。<details>
<summary>Abstract</summary>
Domain incremental learning aims to adapt to a sequence of domains with access to only a small subset of data (i.e., memory) from previous domains. Various methods have been proposed for this problem, but it is still unclear how they are related and when practitioners should choose one method over another. In response, we propose a unified framework, dubbed Unified Domain Incremental Learning (UDIL), for domain incremental learning with memory. Our UDIL **unifies** various existing methods, and our theoretical analysis shows that UDIL always achieves a tighter generalization error bound compared to these methods. The key insight is that different existing methods correspond to our bound with different **fixed** coefficients; based on insights from this unification, our UDIL allows **adaptive** coefficients during training, thereby always achieving the tightest bound. Empirical results show that our UDIL outperforms the state-of-the-art domain incremental learning methods on both synthetic and real-world datasets. Code will be available at https://github.com/Wang-ML-Lab/unified-continual-learning.
</details>
<details>
<summary>摘要</summary>
域incremental learning aimsto adapt to a sequence of domains with only a small subset of data (i.e., memory) from previous domains. Various methods have been proposed for this problem, but it is still unclear how they are related and when practitioners should choose one method over another. In response, we propose a unified framework, called Unified Domain Incremental Learning (UDIL), for domain incremental learning with memory. Our UDIL unifies various existing methods, and our theoretical analysis shows that UDIL always achieves a tighter generalization error bound compared to these methods. The key insight is that different existing methods correspond to our bound with different fixed coefficients; based on insights from this unification, our UDIL allows adaptive coefficients during training, thereby always achieving the tightest bound. Empirical results show that our UDIL outperforms the state-of-the-art domain incremental learning methods on both synthetic and real-world datasets. Code will be available at https://github.com/Wang-ML-Lab/unified-continual-learning.Here's the translation of the highlighted phrases:* **unifies**: 统一* **fixed**: 固定的* **adaptive**: 可变的* **tighter**: 更紧的* **generalization error bound**: 泛化误差 bound* **state-of-the-art**: 现有的最佳方法* **synthetic**:  sintetic* **real-world**: 实际的
</details></li>
</ul>
<hr>
<h2 id="Few-Shot-In-Context-Imitation-Learning-via-Implicit-Graph-Alignment"><a href="#Few-Shot-In-Context-Imitation-Learning-via-Implicit-Graph-Alignment" class="headerlink" title="Few-Shot In-Context Imitation Learning via Implicit Graph Alignment"></a>Few-Shot In-Context Imitation Learning via Implicit Graph Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12238">http://arxiv.org/abs/2310.12238</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vitalis Vosylius, Edward Johns</li>
<li>for: 本研究旨在解决机器人学习新任务时，用于几个示例对象中的任务关系推广到新未经见过的对象上。</li>
<li>methods: 本研究使用 conditional alignment 问题来形式化模仿学习，通过对对象图表示的匹配来捕捉任务相关关系。</li>
<li>results: 实验结果显示，我们的方法可以高效地完成几种实际生活中的每日任务，并在比较基eline上表现出色。视频可以在我们项目网站（<a target="_blank" rel="noopener" href="https://www.robot-learning.uk/implicit-graph-alignment%EF%BC%89%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://www.robot-learning.uk/implicit-graph-alignment）中找到。</a><details>
<summary>Abstract</summary>
Consider the following problem: given a few demonstrations of a task across a few different objects, how can a robot learn to perform that same task on new, previously unseen objects? This is challenging because the large variety of objects within a class makes it difficult to infer the task-relevant relationship between the new objects and the objects in the demonstrations. We address this by formulating imitation learning as a conditional alignment problem between graph representations of objects. Consequently, we show that this conditioning allows for in-context learning, where a robot can perform a task on a set of new objects immediately after the demonstrations, without any prior knowledge about the object class or any further training. In our experiments, we explore and validate our design choices, and we show that our method is highly effective for few-shot learning of several real-world, everyday tasks, whilst outperforming baselines. Videos are available on our project webpage at https://www.robot-learning.uk/implicit-graph-alignment.
</details>
<details>
<summary>摘要</summary>
问题如下：给定一些对象的几个示例任务，如何使 robot 能够在新、未经见过的对象上完成同样的任务？这是因为对象类中的巨量对象关系使得推断任务相关关系 между新对象和示例对象困难。我们解决这个问题，通过将仿真学定义为对象图表示的条件对Alignment问题。因此，我们表明，这种conditioning允许机器人在示例后立即在新对象上进行任务，不需要对对象类或进一步训练。在我们的实验中，我们探索和验证我们的设计选择，并证明我们的方法高效地实现了几个真实世界、日常任务的少量学习，并超越基elines。视频可以在我们项目网站上找到：https://www.robot-learning.uk/implicit-graph-alignment。
</details></li>
</ul>
<hr>
<h2 id="An-Eager-Satisfiability-Modulo-Theories-Solver-for-Algebraic-Datatypes"><a href="#An-Eager-Satisfiability-Modulo-Theories-Solver-for-Algebraic-Datatypes" class="headerlink" title="An Eager Satisfiability Modulo Theories Solver for Algebraic Datatypes"></a>An Eager Satisfiability Modulo Theories Solver for Algebraic Datatypes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12234">http://arxiv.org/abs/2310.12234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amar Shah, Federico Mora, Sanjit A. Seshia</li>
<li>for: 这篇论文的目的是提出一种新的满足推理（SMT）解决方案，用于自动推理关于抽象数据类型（ADT）的问题。</li>
<li>methods: 这篇论文使用了一种新的积极的方法，即将 ADT 查询转化为一种 simpler 的逻辑理论，未解释函数（UF），然后使用现有的解决器解决减少后的查询。</li>
<li>results: 作者证明了这种方法的有效性和完整性，并在现有的benchmark集和一个新的劳动ious benchmark集上进行了比较，得到的结果表明该方法在现有的 benchmark 上比 state-of-the-art 的方法更高效。<details>
<summary>Abstract</summary>
Algebraic data types (ADTs) are a construct classically found in functional programming languages that capture data structures like enumerated types, lists, and trees. In recent years, interest in ADTs has increased. For example, popular programming languages, like Python, have added support for ADTs. Automated reasoning about ADTs can be done using satisfiability modulo theories (SMT) solving, an extension of the Boolean satisfiability problem with constraints over first-order structures. Unfortunately, SMT solvers that support ADTs do not scale as state-of-the-art approaches all use variations of the same \emph{lazy} approach. In this paper, we present an SMT solver that takes a fundamentally different approach, an \emph{eager} approach. Specifically, our solver reduces ADT queries to a simpler logical theory, uninterpreted functions (UF), and then uses an existing solver on the reduced query. We prove the soundness and completeness of our approach and demonstrate that it outperforms the state-of-theart on existing benchmarks, as well as a new, more challenging benchmark set from the planning domain.
</details>
<details>
<summary>摘要</summary>
алгебраические данные типы (ADTs) 是一种在函数编程语言中出现的构造，用于表示枚举类型、列表和树等数据结构。在最近几年中，关于 ADTs 的兴趣增加了。例如，流行编程语言如 Python 也添加了对 ADTs 的支持。通过使用满足性模ulo理论 (SMT) 解决方案，可以自动进行 ADTs 的逻辑推理。然而，现有的 SMT 解决方案都是基于同样的怠慢（lazy）方法，我们则提出了一种不同的积极（eager）方法。具体来说，我们的解决方案将 ADT 查询降到了一个更简单的逻辑理论，未解释函数 (UF)，然后使用现有的解决方案对减少后的查询进行解决。我们证明了我们的方法的有效性和完整性，并证明其在现有的benchmark中以及一个新的、更加挑战性的benchmark集中的性能比例较好。
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Sampling-of-Balanced-K-Means-using-Adiabatic-Quantum-Computing"><a href="#Probabilistic-Sampling-of-Balanced-K-Means-using-Adiabatic-Quantum-Computing" class="headerlink" title="Probabilistic Sampling of Balanced K-Means using Adiabatic Quantum Computing"></a>Probabilistic Sampling of Balanced K-Means using Adiabatic Quantum Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12153">http://arxiv.org/abs/2310.12153</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan-Nico Zaech, Martin Danelljan, Luc Van Gool</li>
<li>for: 这篇论文探讨了可以用adiabatic quantum computing（AQC）来解决数值和常见NP困难的优化问题。</li>
<li>methods: 现有的AQC技术仅允许使用最佳量子态，将其他量子态视为噪音并抛弃。这篇论文提出了使用这些噪音信息进行概率平衡k-means排序的想法。</li>
<li>results: 这篇论文使用了D-Wave AQC处理 sintetic和实际数据，并证明了这种方法可以更好地识别歧义的解和数据点。<details>
<summary>Abstract</summary>
Adiabatic quantum computing (AQC) is a promising quantum computing approach for discrete and often NP-hard optimization problems. Current AQCs allow to implement problems of research interest, which has sparked the development of quantum representations for many machine learning and computer vision tasks. Despite requiring multiple measurements from the noisy AQC, current approaches only utilize the best measurement, discarding information contained in the remaining ones. In this work, we explore the potential of using this information for probabilistic balanced k-means clustering. Instead of discarding non-optimal solutions, we propose to use them to compute calibrated posterior probabilities with little additional compute cost. This allows us to identify ambiguous solutions and data points, which we demonstrate on a D-Wave AQC on synthetic and real data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Fairer-and-More-Accurate-Tabular-Models-Through-NAS"><a href="#Fairer-and-More-Accurate-Tabular-Models-Through-NAS" class="headerlink" title="Fairer and More Accurate Tabular Models Through NAS"></a>Fairer and More Accurate Tabular Models Through NAS</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12145">http://arxiv.org/abs/2310.12145</a></li>
<li>repo_url: None</li>
<li>paper_authors: Richeek Das, Samuel Dooley</li>
<li>for: 这种研究旨在使深度学习模型更加公正，具体来说是通过更新模型的结构和训练参数来实现这一目标。</li>
<li>methods: 该研究使用多目标神经网络搜索（NAS）和超参数优化（HPO）来找到一个新的模型，以提高模型的输出。</li>
<li>results: 研究发现，尝试单独优化模型的准确率可能会导致公正性问题，而同时优化模型的准确率和公正性可以共同优化模型的性能。<details>
<summary>Abstract</summary>
Making models algorithmically fairer in tabular data has been long studied, with techniques typically oriented towards fixes which usually take a neural model with an undesirable outcome and make changes to how the data are ingested, what the model weights are, or how outputs are processed. We employ an emergent and different strategy where we consider updating the model's architecture and training hyperparameters to find an entirely new model with better outcomes from the beginning of the debiasing procedure. In this work, we propose using multi-objective Neural Architecture Search (NAS) and Hyperparameter Optimization (HPO) in the first application to the very challenging domain of tabular data. We conduct extensive exploration of architectural and hyperparameter spaces (MLP, ResNet, and FT-Transformer) across diverse datasets, demonstrating the dependence of accuracy and fairness metrics of model predictions on hyperparameter combinations. We show that models optimized solely for accuracy with NAS often fail to inherently address fairness concerns. We propose a novel approach that jointly optimizes architectural and training hyperparameters in a multi-objective constraint of both accuracy and fairness. We produce architectures that consistently Pareto dominate state-of-the-art bias mitigation methods either in fairness, accuracy or both, all of this while being Pareto-optimal over hyperparameters achieved through single-objective (accuracy) optimization runs. This research underscores the promise of automating fairness and accuracy optimization in deep learning models.
</details>
<details>
<summary>摘要</summary>
使深度学习模型更加公平在表格数据上进行研究已经很长时间了，通常采用的技术是对现有的神经网络模型进行修改，以改善数据入口方式、模型权重或输出处理方式。我们采用一种不同的策略，即对模型的建构和训练超参数进行更新，以找到一个从头开始的全新模型，以提高结果的公平性。在这项工作中，我们提出使用多目标神经网络搜索（NAS）和超参数优化（HPO）来优化模型的建构和超参数。我们在多个数据集上进行了广泛的建构和超参数空间的探索（包括MLP、ResNet和FT-Transformer），并证明了模型预测结果中的公平性和准确性指标之间的依赖关系。我们发现，通过solely使用NAS优化模型的准确性，通常无法自动解决公平性问题。我们提出了一种新的方法，即同时优化建构和超参数，以实现多目标约束中的准确性和公平性两个目标的 JOINT 优化。我们生成了一系列可以同时dominates state-of-the-art偏见缓解方法的建构，并且这些建构都是通过多目标约束来实现的。这些研究表明了自动化深度学习模型的公平性和准确性优化的推荐。
</details></li>
</ul>
<hr>
<h2 id="Getting-aligned-on-representational-alignment"><a href="#Getting-aligned-on-representational-alignment" class="headerlink" title="Getting aligned on representational alignment"></a>Getting aligned on representational alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13018">http://arxiv.org/abs/2310.13018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilia Sucholutsky, Lukas Muttenthaler, Adrian Weller, Andi Peng, Andreea Bobu, Been Kim, Bradley C. Love, Erin Grant, Jascha Achterberg, Joshua B. Tenenbaum, Katherine M. Collins, Katherine L. Hermann, Kerem Oktar, Klaus Greff, Martin N. Hebart, Nori Jacoby, Qiuyi, Zhang, Raja Marjieh, Robert Geirhos, Sherol Chen, Simon Kornblith, Sunayana Rane, Talia Konkle, Thomas P. O’Connell, Thomas Unterthiner, Andrew K. Lampinen, Klaus-Robert Müller, Mariya Toneva, Thomas L. Griffiths</li>
<li>for: The paper aims to improve communication between research communities studying representational alignment in cognitive science, neuroscience, and machine learning, by proposing a unifying framework that can serve as a common language for these fields.</li>
<li>methods: The paper surveys the literature from these fields and demonstrates how prior work fits into the proposed framework.</li>
<li>results: The paper identifies open problems in representational alignment where progress can benefit all three fields, and hopes to catalyze cross-disciplinary collaboration and accelerate progress for all communities studying and developing information processing systems.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目的是为了提高认知科学、神经科学和机器学习等领域研究表征对Alignment的交流，提出一个统一的框架，以便这些领域的研究者之间更好地交流。</li>
<li>methods: 论文将Literature Survey的方法采用到这些领域的文献中，并将先前的工作放入该框架中。</li>
<li>results: 论文标识了表征对Alignment中的开放问题，希望通过跨领域合作，加速所有研究信息处理系统的进步。<details>
<summary>Abstract</summary>
Biological and artificial information processing systems form representations of the world that they can use to categorize, reason, plan, navigate, and make decisions. To what extent do the representations formed by these diverse systems agree? Can diverging representations still lead to the same behaviors? And how can systems modify their representations to better match those of another system? These questions pertaining to the study of \textbf{\emph{representational alignment} are at the heart of some of the most active research areas in contemporary cognitive science, neuroscience, and machine learning. Unfortunately, there is limited knowledge-transfer between research communities interested in representational alignment, and much of the progress in one field ends up being rediscovered independently in another, when greater cross-field communication would be advantageous. To improve communication between fields, we propose a unifying framework that can serve as a common language between researchers studying representational alignment. We survey the literature from the fields of cognitive science, neuroscience, and machine learning, and demonstrate how prior work fits into this framework. Finally, we lay out open problems in representational alignment where progress can benefit all three fields. We hope that our work can catalyze cross-disciplinary collaboration and accelerate progress for all communities studying and developing information processing systems. We note that this is a working paper and encourage readers to reach out with their suggestions for future revisions.
</details>
<details>
<summary>摘要</summary>
Translation notes:* 表征对齐 (representational alignment) is a term used to describe the study of how different information processing systems form representations of the world and how those representations can be aligned to improve communication and collaboration between systems.* 生物学的信息处理系统 (biological information processing systems) refers to the systems found in living organisms, such as the human brain, that process and analyze information from the environment.* 人工的信息处理系统 (artificial information processing systems) refers to the systems created by humans, such as computers and machine learning algorithms, that process and analyze information.* 形象 (representations) refers to the internal mental or computational models that systems use to represent the world and make decisions.* 类别 (categories) refers to the ways in which systems group and classify objects or concepts in the world.* 理解 (reasoning) refers to the processes by which systems draw conclusions or make decisions based on the information they have.* 规划 (planning) refers to the processes by which systems create and execute a plan to achieve a goal.* 导航 (navigation) refers to the processes by which systems move through the world and avoid obstacles.* 决策 (decision-making) refers to the processes by which systems choose between different options or courses of action.
</details></li>
</ul>
<hr>
<h2 id="A-comprehensible-analysis-of-the-efficacy-of-Ensemble-Models-for-Bug-Prediction"><a href="#A-comprehensible-analysis-of-the-efficacy-of-Ensemble-Models-for-Bug-Prediction" class="headerlink" title="A comprehensible analysis of the efficacy of Ensemble Models for Bug Prediction"></a>A comprehensible analysis of the efficacy of Ensemble Models for Bug Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12133">http://arxiv.org/abs/2310.12133</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ingrid Marçal, Rogério Eduardo Garcia</li>
<li>for: 本研究旨在比较和分析使用人工智能技术在软件工程中预测Java类库中存在bug的可能性。</li>
<li>methods: 我们使用了两种Apache Commons Project的Java组件进行训练和测试模型，分别是单个AI模型和集成AI模型。</li>
<li>results: 我们的实验结果表明，集成AI模型可以在预测Java类库中存在bug的可能性方面超过单个AI模型的结果。我们还提供了因素的分析，以便更好地理解 ensemble AI 模型的性能提升的原因。<details>
<summary>Abstract</summary>
The correctness of software systems is vital for their effective operation. It makes discovering and fixing software bugs an important development task. The increasing use of Artificial Intelligence (AI) techniques in Software Engineering led to the development of a number of techniques that can assist software developers in identifying potential bugs in code. In this paper, we present a comprehensible comparison and analysis of the efficacy of two AI-based approaches, namely single AI models and ensemble AI models, for predicting the probability of a Java class being buggy. We used two open-source Apache Commons Project's Java components for training and evaluating the models. Our experimental findings indicate that the ensemble of AI models can outperform the results of applying individual AI models. We also offer insight into the factors that contribute to the enhanced performance of the ensemble AI model. The presented results demonstrate the potential of using ensemble AI models to enhance bug prediction results, which could ultimately result in more reliable software systems.
</details>
<details>
<summary>摘要</summary>
软件系统的正确性是其效果运行的关键。找到和修复软件漏洞是软件开发中重要的任务。随着人工智能（AI）技术在软件工程中的广泛应用，出现了一些可以帮助软件开发人员找到代码中潜在的漏洞的技术。在这篇论文中，我们提供了可读性比较和分析，探讨使用单个AI模型和 ensemble AI模型来预测Java类的可能性。我们使用了两个开源Apache Commons Project的Java组件来训练和测试模型。我们的实验结果表明， ensemble AI模型可以在应用单个AI模型的情况下出perform better。我们还提供了影响ensemble AI模型的表现的因素。该结果表明，使用ensemble AI模型可以提高漏洞预测结果，从而导致更可靠的软件系统。
</details></li>
</ul>
<hr>
<h2 id="DiagrammerGPT-Generating-Open-Domain-Open-Platform-Diagrams-via-LLM-Planning"><a href="#DiagrammerGPT-Generating-Open-Domain-Open-Platform-Diagrams-via-LLM-Planning" class="headerlink" title="DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning"></a>DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12128">http://arxiv.org/abs/2310.12128</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aszala/DiagrammerGPT">https://github.com/aszala/DiagrammerGPT</a></li>
<li>paper_authors: Abhay Zala, Han Lin, Jaemin Cho, Mohit Bansal</li>
<li>for: 这个论文旨在解决现有的文本到图像（T2I）生成模型无法生成符号图文（diagram）的问题。</li>
<li>methods: 该论文提出了一种两stage的文本到图文生成框架，利用大语言模型（LLMs）的布局指导能力来生成更加准确的开放领域、开放 платфор具有的图文。</li>
<li>results: 论文通过使用 LLMs 生成和修改 ‘图文计划’（在一个 плаanner-auditor 反馈循环中），以及使用 DiagramGLIGEN 和文本标签渲染模块来生成图文，实现了更高的准确率和质量。<details>
<summary>Abstract</summary>
Text-to-image (T2I) generation has seen significant growth over the past few years. Despite this, there has been little work on generating diagrams with T2I models. A diagram is a symbolic/schematic representation that explains information using structurally rich and spatially complex visualizations (e.g., a dense combination of related objects, text labels, directional arrows, connection lines, etc.). Existing state-of-the-art T2I models often fail at diagram generation because they lack fine-grained object layout control when many objects are densely connected via complex relations such as arrows/lines and also often fail to render comprehensible text labels. To address this gap, we present DiagrammerGPT, a novel two-stage text-to-diagram generation framework that leverages the layout guidance capabilities of LLMs (e.g., GPT-4) to generate more accurate open-domain, open-platform diagrams. In the first stage, we use LLMs to generate and iteratively refine 'diagram plans' (in a planner-auditor feedback loop) which describe all the entities (objects and text labels), their relationships (arrows or lines), and their bounding box layouts. In the second stage, we use a diagram generator, DiagramGLIGEN, and a text label rendering module to generate diagrams following the diagram plans. To benchmark the text-to-diagram generation task, we introduce AI2D-Caption, a densely annotated diagram dataset built on top of the AI2D dataset. We show quantitatively and qualitatively that our DiagrammerGPT framework produces more accurate diagrams, outperforming existing T2I models. We also provide comprehensive analysis including open-domain diagram generation, vector graphic diagram generation in different platforms, human-in-the-loop diagram plan editing, and multimodal planner/auditor LLMs (e.g., GPT-4Vision). We hope our work can inspire further research on diagram generation via T2I models and LLMs.
</details>
<details>
<summary>摘要</summary>
TEXT-TO-IMAGE（T2I）生成技术在过去几年内有了很大的发展。然而，有很少的研究集中于使用 T2I 模型生成图文。图文是一种使用结构rich和空间复杂的视觉表示方式，用于展示信息（例如，密集的对象、文本标签、指向箭头、连接线等）。现有的 T2I 模型通常在图文生成中存在缺陷，因为它们缺乏细化的对象布局控制，特别是当多个对象密集连接并且有复杂的关系（如箭头/线）时。为解决这个漏洞，我们提出了 DiagrammerGPT，一种新的两stage T2I 生成框架。在第一stage中，我们使用 LLMs（例如 GPT-4）来生成和反复修改 '图文计划'（在计划-审查器反馈循环中），该计划描述了所有对象（包括物体和文本标签）、它们之间的关系（如箭头或线）以及它们的包围盒布局。在第二stage中，我们使用 DiagramGLIGEN 和文本标签渲染模块来生成图文，按照图文计划进行。为了评估 T2I 生成任务，我们提出了 AI2D-Caption，一个密集注释的图文数据集，建立在 AI2D 数据集之上。我们表明了量化和质量上，我们的 DiagrammerGPT 框架可以生成更加准确的图文，超过现有的 T2I 模型。此外，我们还提供了广泛的分析，包括开放平台图文生成、vector graphic diagram生成、人工循环图文计划编辑和多Modal LLMs（例如 GPT-4Vision）。我们希望我们的工作可以鼓励更多的研究人员通过 T2I 模型和 LLMs 来生成图文。
</details></li>
</ul>
<hr>
<h2 id="SHARCS-Efficient-Transformers-through-Routing-with-Dynamic-Width-Sub-networks"><a href="#SHARCS-Efficient-Transformers-through-Routing-with-Dynamic-Width-Sub-networks" class="headerlink" title="SHARCS: Efficient Transformers through Routing with Dynamic Width Sub-networks"></a>SHARCS: Efficient Transformers through Routing with Dynamic Width Sub-networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12126">http://arxiv.org/abs/2310.12126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammadreza Salehi, Sachin Mehta, Aditya Kusupati, Ali Farhadi, Hannaneh Hajishirzi</li>
<li>for: 提高Transformer网络的批处理能力和精度</li>
<li>methods: 使用SHARCS进行适应推理，可以在不同的模型和压缩方法下进行自适应调整</li>
<li>results: SHARCS可以提高推理速度，并且可以保持精度水平，实际测试中SHARCS可以提高推理速度2倍，但是精度下降不значитель<details>
<summary>Abstract</summary>
We introduce SHARCS for adaptive inference that takes into account the hardness of input samples. SHARCS can train a router on any transformer network, enabling the model to direct different samples to sub-networks with varying widths. Our experiments demonstrate that: (1) SHARCS outperforms or complements existing per-sample adaptive inference methods across various classification tasks in terms of accuracy vs. FLOPs; (2) SHARCS generalizes across different architectures and can be even applied to compressed and efficient transformer encoders to further improve their efficiency; (3) SHARCS can provide a 2 times inference speed up at an insignificant drop in accuracy.
</details>
<details>
<summary>摘要</summary>
我们介绍SHARCS，一种适应推理的方法，考虑到输入样本的困难程度。SHARCS可以在任何transformer网络上训练路由器，让模型将不同的样本分配到不同宽度的子网络上。我们的实验表明：（1）SHARCS与现有的每个样本适应推理方法相比，在不同的分类任务中获得更高的精度和FLOPs的调整；（2）SHARCS可以适用于不同的架构，并且可以进一步改善压缩和高效的transformerEncoder的效率；（3）SHARCS可以提供2倍的推理速度，而无需对精度造成显著的损失。
</details></li>
</ul>
<hr>
<h2 id="A-Cautionary-Tale-On-the-Role-of-Reference-Data-in-Empirical-Privacy-Defenses"><a href="#A-Cautionary-Tale-On-the-Role-of-Reference-Data-in-Empirical-Privacy-Defenses" class="headerlink" title="A Cautionary Tale: On the Role of Reference Data in Empirical Privacy Defenses"></a>A Cautionary Tale: On the Role of Reference Data in Empirical Privacy Defenses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12112">http://arxiv.org/abs/2310.12112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Caelin G. Kaplan, Chuan Xu, Othmane Marfoq, Giovanni Neglia, Anderson Santana de Oliveira</li>
<li>for: This paper focuses on developing effective privacy-preserving machine learning methods that can provide satisfactory levels of training data privacy without significantly compromising model utility.</li>
<li>methods: The proposed method is based on an empirical risk minimization approach with a constraint on the generalization error, which is evaluated as a weighted empirical risk minimization (WERM) over the training and reference datasets.</li>
<li>results: The proposed method outperforms existing state-of-the-art empirical privacy defenses using reference data for nearly all relative privacy levels of reference and training data, and demonstrates the importance of considering the triad of model utility, training data privacy, and reference data privacy when comparing privacy defenses.<details>
<summary>Abstract</summary>
Within the realm of privacy-preserving machine learning, empirical privacy defenses have been proposed as a solution to achieve satisfactory levels of training data privacy without a significant drop in model utility. Most existing defenses against membership inference attacks assume access to reference data, defined as an additional dataset coming from the same (or a similar) underlying distribution as training data. Despite the common use of reference data, previous works are notably reticent about defining and evaluating reference data privacy. As gains in model utility and/or training data privacy may come at the expense of reference data privacy, it is essential that all three aspects are duly considered. In this paper, we first examine the availability of reference data and its privacy treatment in previous works and demonstrate its necessity for fairly comparing defenses. Second, we propose a baseline defense that enables the utility-privacy tradeoff with respect to both training and reference data to be easily understood. Our method is formulated as an empirical risk minimization with a constraint on the generalization error, which, in practice, can be evaluated as a weighted empirical risk minimization (WERM) over the training and reference datasets. Although we conceived of WERM as a simple baseline, our experiments show that, surprisingly, it outperforms the most well-studied and current state-of-the-art empirical privacy defenses using reference data for nearly all relative privacy levels of reference and training data. Our investigation also reveals that these existing methods are unable to effectively trade off reference data privacy for model utility and/or training data privacy. Overall, our work highlights the need for a proper evaluation of the triad model utility / training data privacy / reference data privacy when comparing privacy defenses.
</details>
<details>
<summary>摘要</summary>
在隐私保护机器学习领域，验证性隐私防御被提出为实现训练数据隐私的解决方案，而不导致模型性能下降。大多数现有的防御机制假设有访问参考数据，定义为训练数据所处的同一个（或类似）分布下的另一个数据集。尽管参考数据广泛使用，但前一些作品却不够明确地定义和评估参考数据隐私。因为获得模型性能和/或训练数据隐私的增进可能会导致参考数据隐私的损害，因此必须同时考虑这三个方面。在这篇论文中，我们首先检查参考数据的可用性和隐私处理方法，并证明其必要性以便比较防御机制。其次，我们提出一种基准防御方法，允许模型性能和训练数据隐私之间的利用率评估，并且可以通过将总体化风险最小化问题转化为权重加总风险最小化问题（WERM）来实现。虽然我们视WERM为简单的基准方法，但我们的实验表明，它在大多数参考数据隐私水平下能够超越目前最具有研究价值和状态艺术的Empirical Privacy防御方法。我们的调查也表明，这些现有方法无法有效地考虑参考数据隐私和训练数据隐私之间的贝叶率。总的来说，我们的工作强调了评估模型性能、训练数据隐私和参考数据隐私的三元模型在比较防御机制时的重要性。
</details></li>
</ul>
<hr>
<h2 id="DASA-Difficulty-Aware-Semantic-Augmentation-for-Speaker-Verification"><a href="#DASA-Difficulty-Aware-Semantic-Augmentation-for-Speaker-Verification" class="headerlink" title="DASA: Difficulty-Aware Semantic Augmentation for Speaker Verification"></a>DASA: Difficulty-Aware Semantic Augmentation for Speaker Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12111">http://arxiv.org/abs/2310.12111</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanyuan Wang, Yang Zhang, Zhiyong Wu, Zhihan Yang, Tao Wei, Kun Zou, Helen Meng</li>
<li>for: 提高深度神经网络模型的总化能力和鲁棒性，通过数据扩充来提高speaker认证模型的性能。</li>
<li>methods: 提出了一种新的困难意识 semantic 数据扩充（DASA）方法，通过对话者嵌入空间中的 semantic 方向进行偏移来生成多样化的训练样本，同时保持采样的计算成本很低。</li>
<li>results: 经验表明，提出的方法可以带来remarkable的性能提升，最好的结果在CN-Celeb评测集上实现了14.6%的相对性能下降。<details>
<summary>Abstract</summary>
Data augmentation is vital to the generalization ability and robustness of deep neural networks (DNNs) models. Existing augmentation methods for speaker verification manipulate the raw signal, which are time-consuming and the augmented samples lack diversity. In this paper, we present a novel difficulty-aware semantic augmentation (DASA) approach for speaker verification, which can generate diversified training samples in speaker embedding space with negligible extra computing cost. Firstly, we augment training samples by perturbing speaker embeddings along semantic directions, which are obtained from speaker-wise covariance matrices. Secondly, accurate covariance matrices are estimated from robust speaker embeddings during training, so we introduce difficultyaware additive margin softmax (DAAM-Softmax) to obtain optimal speaker embeddings. Finally, we assume the number of augmented samples goes to infinity and derive a closed-form upper bound of the expected loss with DASA, which achieves compatibility and efficiency. Extensive experiments demonstrate the proposed approach can achieve a remarkable performance improvement. The best result achieves a 14.6% relative reduction in EER metric on CN-Celeb evaluation set.
</details>
<details>
<summary>摘要</summary>
<<SYS>> traducedata augmentation是深度神经网络（DNN）模型的重要组成部分，它们可以提高模型的通用能力和鲁棒性。现有的增强方法 дляspeaker verification通常是对原始信号进行 manipulate，这些方法需要较多的计算时间，并且增强的样本缺乏多样性。在这篇论文中，我们提出了一种新的困难相关Semantic Augmentation（DASA）方法，可以在speaker embedding空间中生成多样化的训练样本，而且计算成本几乎为零。首先，我们在训练样本中进行增强，通过在语意方向上偏移speaker embeddings来生成多样化的训练样本。其次，我们在训练过程中对robust speaker embeddings进行估计，以获得高精度的covariance矩阵。最后，我们引入difficulty-aware additive margin softmax（DAAM-Softmax）来获取最佳的speaker embeddings。最后，我们假设增强的样本数量为无穷大，并 deriveclosed-form upper bound of the expected loss with DASA，这个目标函数可以实现compatibility和效率。我们的实验表明，提出的方法可以获得显著的性能改进。最好的结果实现了CN-Celeb评估集上的14.6%相对减少EER指标。Note: Some words and phrases in the text have been modified to better fit the Simplified Chinese language, but the overall meaning and content of the text remain the same.
</details></li>
</ul>
<hr>
<h2 id="Quality-Diversity-through-Human-Feedback"><a href="#Quality-Diversity-through-Human-Feedback" class="headerlink" title="Quality Diversity through Human Feedback"></a>Quality Diversity through Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12103">http://arxiv.org/abs/2310.12103</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Ding, Jenny Zhang, Jeff Clune, Lee Spector, Joel Lehman</li>
<li>for: 提高基本模型的性能 для质量任务</li>
<li>methods: 结合人工反馈来推导多样性度量</li>
<li>results: 比现有多样性算法提高自动多样性发现能力，并与人工定义多样性度量匹配搜索能力<details>
<summary>Abstract</summary>
Reinforcement learning from human feedback (RLHF) has exhibited the potential to enhance the performance of foundation models for qualitative tasks. Despite its promise, its efficacy is often restricted when conceptualized merely as a mechanism to maximize learned reward models of averaged human preferences, especially in areas such as image generation which demand diverse model responses. Meanwhile, quality diversity (QD) algorithms, dedicated to seeking diverse, high-quality solutions, are often constrained by the dependency on manually defined diversity metrics. Interestingly, such limitations of RLHF and QD can be overcome by blending insights from both. This paper introduces Quality Diversity through Human Feedback (QDHF), which employs human feedback for inferring diversity metrics, expanding the applicability of QD algorithms. Empirical results reveal that QDHF outperforms existing QD methods regarding automatic diversity discovery, and matches the search capabilities of QD with human-constructed metrics. Notably, when deployed for a latent space illumination task, QDHF markedly enhances the diversity of images generated by a Diffusion model. The study concludes with an in-depth analysis of QDHF's sample efficiency and the quality of its derived diversity metrics, emphasizing its promise for enhancing exploration and diversity in optimization for complex, open-ended tasks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传递人类反馈学习（RLHF）已经展示了改进基础模型的表现能力。尽管它的潜力很大，但它的效果往往受限于仅视为提高学习得到的奖励模型的均值人类喜好，特别是在图像生成等需要多样化模型响应的领域。同时，质量多样性（QD）算法，专门寻找多样、高质量的解决方案，经常受到手动定义多样性度量的限制。 Curiously, RLHF和QD的局限性可以通过两者的洞察得到解决。这篇论文介绍了基于人类反馈的质量多样性（QDHF），它利用人类反馈来推断多样性度量，扩展了QD算法的适用范围。实验结果表明，QDHF在自动多样性发现方面表现出色，与人工定义多样性度量相当。尤其是在使用了一种扩散模型进行隐藏空间照明任务时，QDHF明显提高了生成的图像的多样性。研究结束于对QDHF的样本效率和获得的多样性度量的深入分析，强调它在复杂、开端任务中的探索和多样性提高的抢夺。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Non-Intrusive-Adaptation-Input-Centric-Parameter-efficient-Fine-Tuning-for-Versatile-Multimodal-Modeling"><a href="#Non-Intrusive-Adaptation-Input-Centric-Parameter-efficient-Fine-Tuning-for-Versatile-Multimodal-Modeling" class="headerlink" title="Non-Intrusive Adaptation: Input-Centric Parameter-efficient Fine-Tuning for Versatile Multimodal Modeling"></a>Non-Intrusive Adaptation: Input-Centric Parameter-efficient Fine-Tuning for Versatile Multimodal Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12100">http://arxiv.org/abs/2310.12100</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yaqing Wang, Jialin Wu, Tanmaya Dabral, Jiageng Zhang, Geoff Brown, Chun-Ta Lu, Frederick Liu, Yi Liang, Bo Pang, Michael Bendersky, Radu Soricut</li>
<li>for: 这篇论文的目的是探讨如何实现具有优秀表现的大型语言模型（LLMs）和视觉语言模型（VLMs）的实际应用，以及如何在不需要专门设计的任务下进行适应和服务。</li>
<li>methods: 这篇论文使用了两种Parameter-efficient fine-tuning（PEFT）技术：不断式PEFT和非断式PEFT。不断式PEFT直接改变模型内部架构，可以更加灵活，但是它导入了训练和服务中的复杂性。非断式PEFT则保持模型内部架构不变，仅对输入的嵌入进行适应，这种方法比较简单。这篇论文描述了一种非断式PEFT技术名为AdaLink，它在不同的任务上实现了与SoTA创新PEFT（LoRA）和全模型精确调整（FT）的竞争性表现。</li>
<li>results: 这篇论文的结果显示，AdaLink在文本仅和多媒体任务上均实现了与SoTA intrusive PEFT（LoRA）和FT的竞争性表现。此外，这篇论文还进行了对不同训练 режи和执行环境的测试，以确保AdaLink在实际应用中具有可靠性和稳定性。<details>
<summary>Abstract</summary>
Large language models (LLMs) and vision language models (VLMs) demonstrate excellent performance on a wide range of tasks by scaling up parameter counts from O(10^9) to O(10^{12}) levels and further beyond. These large scales make it impossible to adapt and deploy fully specialized models given a task of interest. Parameter-efficient fine-tuning (PEFT) emerges as a promising direction to tackle the adaptation and serving challenges for such large models. We categorize PEFT techniques into two types: intrusive and non-intrusive. Intrusive PEFT techniques directly change a model's internal architecture. Though more flexible, they introduce significant complexities for training and serving. Non-intrusive PEFT techniques leave the internal architecture unchanged and only adapt model-external parameters, such as embeddings for input. In this work, we describe AdaLink as a non-intrusive PEFT technique that achieves competitive performance compared to SoTA intrusive PEFT (LoRA) and full model fine-tuning (FT) on various tasks. We evaluate using both text-only and multimodal tasks, with experiments that account for both parameter-count scaling and training regime (with and without instruction tuning).
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Position-Interpolation-Improves-ALiBi-Extrapolation"><a href="#Position-Interpolation-Improves-ALiBi-Extrapolation" class="headerlink" title="Position Interpolation Improves ALiBi Extrapolation"></a>Position Interpolation Improves ALiBi Extrapolation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13017">http://arxiv.org/abs/2310.13017</a></li>
<li>repo_url: None</li>
<li>paper_authors: Faisal Al-Khateeb, Nolan Dey, Daria Soboleva, Joel Hestness</li>
<li>for: 帮助预训练模型使用旋转位嵌入（RoPE）来推断更长的序列长度。</li>
<li>methods: 使用线性位 interpolator来扩展模型使用注意力与直线偏好（ALiBi）的推断范围。</li>
<li>results: 位 interpolator显著提高了预训练模型在语言模型和摘要回传任务中的推断能力。<details>
<summary>Abstract</summary>
Linear position interpolation helps pre-trained models using rotary position embeddings (RoPE) to extrapolate to longer sequence lengths. We propose using linear position interpolation to extend the extrapolation range of models using Attention with Linear Biases (ALiBi). We find position interpolation significantly improves extrapolation capability on upstream language modelling and downstream summarization and retrieval tasks.
</details>
<details>
<summary>摘要</summary>
线性位置 interpolate 帮助预训练模型使用旋转位置嵌入 (RoPE) 来推断更长的序列长度。我们提议使用线性位置 interpolate 来扩展使用 Attention with Linear Biases (ALiBi) 模型的推断范围。我们发现位置 interpolate 对于上游语言模型和下游摘要和检索任务的推断能力有显著改善。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-the-Siren’s-Song-Towards-Reliable-Fact-Conflicting-Hallucination-Detection"><a href="#Unveiling-the-Siren’s-Song-Towards-Reliable-Fact-Conflicting-Hallucination-Detection" class="headerlink" title="Unveiling the Siren’s Song: Towards Reliable Fact-Conflicting Hallucination Detection"></a>Unveiling the Siren’s Song: Towards Reliable Fact-Conflicting Hallucination Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12086">http://arxiv.org/abs/2310.12086</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zjunlp/factchd">https://github.com/zjunlp/factchd</a></li>
<li>paper_authors: Xiang Chen, Duanzheng Song, Honghao Gui, Chengxi Wang, Ningyu Zhang, Fei Huang, Chengfei Lv, Dan Zhang, Huajun Chen</li>
<li>for: The paper is written for evaluating the factuality of text generated by large language models (LLMs) and developing a benchmark for detecting fact-conflicting hallucinations in these models.</li>
<li>methods: The paper introduces a new benchmark called FactCHD, which assimilates a large-scale dataset of factuality patterns and incorporates fact-based chains of evidence to facilitate comprehensive factual reasoning. The authors also present a new method called TRUTH-TRIANGULATOR, which synthesizes reflective considerations by tool-enhanced ChatGPT and LoRA-tuning based on Llama2 to yield more credible detection.</li>
<li>results: The paper demonstrates the effectiveness of the FactCHD benchmark and shows that current methods fall short of faithfully detecting factual errors. The authors also present results from using TRUTH-TRIANGULATOR, which shows improved detection performance compared to existing methods.<details>
<summary>Abstract</summary>
Large Language Models (LLMs), such as ChatGPT/GPT-4, have garnered widespread attention owing to their myriad of practical applications, yet their adoption has been constrained by issues of fact-conflicting hallucinations across web platforms. The assessment of factuality in text, produced by LLMs, remains inadequately explored, extending not only to the judgment of vanilla facts but also encompassing the evaluation of factual errors emerging in complex inferential tasks like multi-hop, and etc. In response, we introduce FactCHD, a fact-conflicting hallucination detection benchmark meticulously designed for LLMs. Functioning as a pivotal tool in evaluating factuality within "Query-Respons" contexts, our benchmark assimilates a large-scale dataset, encapsulating a broad spectrum of factuality patterns, such as vanilla, multi-hops, comparison, and set-operation patterns. A distinctive feature of our benchmark is its incorporation of fact-based chains of evidence, thereby facilitating comprehensive and conducive factual reasoning throughout the assessment process. We evaluate multiple LLMs, demonstrating the effectiveness of the benchmark and current methods fall short of faithfully detecting factual errors. Furthermore, we present TRUTH-TRIANGULATOR that synthesizes reflective considerations by tool-enhanced ChatGPT and LoRA-tuning based on Llama2, aiming to yield more credible detection through the amalgamation of predictive results and evidence. The benchmark dataset and source code will be made available in https://github.com/zjunlp/FactCHD.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs），如ChatGPT/GPT-4，在实际应用方面引起了广泛关注，但其普及受到了网络平台上的事实冲突报告的限制。评估 LLMS 生成的文本中的事实真实性仍然不充分探讨，包括单纯的事实以及在复杂的推理任务中出现的事实错误。为此，我们提出了 FactCHD，一个特别设计 для LLMS 的事实冲突报告 benchmark。作为评估“查询-回答”上的事实真实性的重要工具，我们的 benchmark 集成了一个大规模的数据集，包括多种事实真实性模式，如简单、多步、比较和集成模式。我们的 benchmark 的一个特点是通过 incorporating fact-based chains of evidence，以便在评估过程中进行全面和有利的事实理解。我们测试了多个 LLMS，并证明了我们的 benchmark 和现有方法无法准确检测事实错误。此外，我们还提出了 TRUTH-TRIANGULATOR，一种基于 tool-enhanced ChatGPT 和 LoRA-tuning 的 Llama2 的方法，以便通过合并预测结果和证据来提供更可靠的检测。我们的 benchmark 数据集和源代码将在 GitHub 上发布。
</details></li>
</ul>
<hr>
<h2 id="DHOT-GM-Robust-Graph-Matching-Using-A-Differentiable-Hierarchical-Optimal-Transport-Framework"><a href="#DHOT-GM-Robust-Graph-Matching-Using-A-Differentiable-Hierarchical-Optimal-Transport-Framework" class="headerlink" title="DHOT-GM: Robust Graph Matching Using A Differentiable Hierarchical Optimal Transport Framework"></a>DHOT-GM: Robust Graph Matching Using A Differentiable Hierarchical Optimal Transport Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12081">http://arxiv.org/abs/2310.12081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoran Cheng, Dixin Luo, Hongteng Xu</li>
<li>for: 本研究旨在提出一种新的图像匹配方法，用于更好地利用图像中的多Modal信息，提高图像匹配的精度和效率。</li>
<li>methods: 本方法基于一种可导的层次优化交通（HOT）框架，使用各种modal信息来匹配图像。具体来说，我们将每个图像表示为一组相关矩阵，其中每个矩阵代表图像中不同modal信息的信息。然后，我们对两个图像进行匹配，并使用优化交通距离来衡量匹配结果。</li>
<li>results: 我们通过对多个图像匹配任务进行实验，发现我们的方法比前 существу的方法更高效和更稳定。在匹配过程中，我们可以通过调整可导的优化交通距离来控制匹配的精度和稳定性。<details>
<summary>Abstract</summary>
Graph matching is one of the most significant graph analytic tasks in practice, which aims to find the node correspondence across different graphs. Most existing approaches rely on adjacency matrices or node embeddings when matching graphs, whose performances are often sub-optimal because of not fully leveraging the multi-modal information hidden in graphs, such as node attributes, subgraph structures, etc. In this study, we propose a novel and effective graph matching method based on a differentiable hierarchical optimal transport (HOT) framework, called DHOT-GM. Essentially, our method represents each graph as a set of relational matrices corresponding to the information of different modalities. Given two graphs, we enumerate all relational matrix pairs and obtain their matching results, and accordingly, infer the node correspondence by the weighted averaging of the matching results. This method can be implemented as computing the HOT distance between the two graphs -- each matching result is an optimal transport plan associated with the Gromov-Wasserstein (GW) distance between two relational matrices, and the weights of all matching results are the elements of an upper-level optimal transport plan defined on the matrix sets. We propose a bi-level optimization algorithm to compute the HOT distance in a differentiable way, making the significance of the relational matrices adjustable. Experiments on various graph matching tasks demonstrate the superiority and robustness of our method compared to state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
GRAPH MATCHING 是一个非常重要的图分析任务，旨在找到不同图中节点的对应关系。现有的大多数方法都基于图邻接矩阵或节点嵌入， whose performances are often sub-optimal because of not fully leveraging the multi-modal information hidden in graphs, such as node attributes, subgraph structures, etc. 在这种研究中，我们提出了一种新的和有效的图 matching方法，基于可微的层次优先 transport（HOT）框架，称为DHOT-GM。本方法将每个图表示为不同特征Modalities的信息的集合。给定两个图，我们会枚举所有的关系矩阵对，并计算它们的匹配结果，然后根据匹配结果的权重，进行节点对应。这种方法可以视为计算HOT距离 между两个图，每个匹配结果是一个优先transport plan相关的Gromov-Wasserstein（GW）距离 между两个关系矩阵，并且权重的所有匹配结果的元素是一个上级优先transport plan定义在矩阵集上。我们提出了一种二级优化算法来计算HOT距离，使得关系矩阵的重要性可调。实验结果表明，我们的方法比现有的方法更高效和Robust。
</details></li>
</ul>
<hr>
<h2 id="Black-Box-Training-Data-Identification-in-GANs-via-Detector-Networks"><a href="#Black-Box-Training-Data-Identification-in-GANs-via-Detector-Networks" class="headerlink" title="Black-Box Training Data Identification in GANs via Detector Networks"></a>Black-Box Training Data Identification in GANs via Detector Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12063">http://arxiv.org/abs/2310.12063</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukman Olagoke, Salil Vadhan, Seth Neel</li>
<li>for: 本研究探讨了使用生成对抗网络（GAN）时的隐私问题，特别是在黑盒Setting下（即只有 generator 的样本）。</li>
<li>methods: 我们提出了一系列的会员推测攻击，包括一种名为“检测器”的攻击，它通过训练一个第二个网络来评估样本的生成者生成的可能性。</li>
<li>results: 我们在多种图像和表格数据集上，以及不同的攻击和 GAN 架构上，发现了非常有趣的隐私攻击。然而，与其他生成和分类模型相比，GAN 的攻击成功率仍然相对较低。这留下了一个有趣的问题：是 GAN 更加隐私，或者需要更强的攻击？<details>
<summary>Abstract</summary>
Since their inception Generative Adversarial Networks (GANs) have been popular generative models across images, audio, video, and tabular data. In this paper we study whether given access to a trained GAN, as well as fresh samples from the underlying distribution, if it is possible for an attacker to efficiently identify if a given point is a member of the GAN's training data. This is of interest for both reasons related to copyright, where a user may want to determine if their copyrighted data has been used to train a GAN, and in the study of data privacy, where the ability to detect training set membership is known as a membership inference attack. Unlike the majority of prior work this paper investigates the privacy implications of using GANs in black-box settings, where the attack only has access to samples from the generator, rather than access to the discriminator as well. We introduce a suite of membership inference attacks against GANs in the black-box setting and evaluate our attacks on image GANs trained on the CIFAR10 dataset and tabular GANs trained on genomic data. Our most successful attack, called The Detector, involve training a second network to score samples based on their likelihood of being generated by the GAN, as opposed to a fresh sample from the distribution. We prove under a simple model of the generator that the detector is an approximately optimal membership inference attack. Across a wide range of tabular and image datasets, attacks, and GAN architectures, we find that adversaries can orchestrate non-trivial privacy attacks when provided with access to samples from the generator. At the same time, the attack success achievable against GANs still appears to be lower compared to other generative and discriminative models; this leaves the intriguing open question of whether GANs are in fact more private, or if it is a matter of developing stronger attacks.
</details>
<details>
<summary>摘要</summary>
自它们的出现以来，生成对抗网络（GANs）已成为图像、音频、视频和表格数据上广泛使用的生成模型。在这篇论文中，我们研究了给定一个已经训练过GAN的攻击者，以及新的样本从下面分布中获得的情况下，是否可以高效地判断一个点是否属于GAN的训练数据。这对于版权和数据隐私具有重要的意义，因为用户可能想要确定他们的版权数据是否被用来训练GAN，而且在数据隐私方面，能够检测训练集成员是一种称为会员推理攻击的能力。与大多数前期工作不同，本文 investigate GANs在黑盒设置下的隐私问题，攻击者只有Generator的样本而不具有Discriminator的访问权。我们介绍了一组黑盒成员推理攻击，并对图像GAN在CIFAR10数据集和表格GAN在生物数据集进行了评估。我们最成功的攻击方法叫做检测器，它通过训练一个第二个网络来评估样本是否由GAN生成，而不是一个新的样本从分布中。我们证明在一个简单的生成器模型下，检测器是一种相对优化的会员推理攻击。在各种图像和表格数据集、攻击和GAN架构下，我们发现攻击者可以通过Generator的样本进行非常复杂的隐私攻击。尽管GANs在隐私方面的攻击仍然比其他生成和判断模型低，但这仍然留下了一个惊喜的问题：GANs是否更安全，或者是需要更强的攻击。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-based-Nutrient-Application’s-Timeline-Recommendation-for-Smart-Agriculture-A-Large-Scale-Data-Mining-Approach"><a href="#Machine-Learning-based-Nutrient-Application’s-Timeline-Recommendation-for-Smart-Agriculture-A-Large-Scale-Data-Mining-Approach" class="headerlink" title="Machine Learning-based Nutrient Application’s Timeline Recommendation for Smart Agriculture: A Large-Scale Data Mining Approach"></a>Machine Learning-based Nutrient Application’s Timeline Recommendation for Smart Agriculture: A Large-Scale Data Mining Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12052">http://arxiv.org/abs/2310.12052</a></li>
<li>repo_url: None</li>
<li>paper_authors: Usama Ikhlaq, Tahar Kechadi</li>
<li>for: 这项研究旨在提供一种可预测肥料应用量的解决方案，以便更好地管理投用肥料，降低成本、保护环境。</li>
<li>methods: 该研究使用大规模不同数据类型的数据集，通过分析这些数据来预测肥料应用量。研究还涉及到肥料应用量和天气数据的相互作用对作物产量的影响。</li>
<li>results: 研究发现，基于天气和土壤特点进行调整的肥料应用量可以提高作物产量，同时降低肥料投用量。该方法也被证明可靠和可扩展。<details>
<summary>Abstract</summary>
This study addresses the vital role of data analytics in monitoring fertiliser applications in crop cultivation. Inaccurate fertiliser application decisions can lead to costly consequences, hinder food production, and cause environmental harm. We propose a solution to predict nutrient application by determining required fertiliser quantities for an entire season. The proposed solution recommends adjusting fertiliser amounts based on weather conditions and soil characteristics to promote cost-effective and environmentally friendly agriculture. The collected dataset is high-dimensional and heterogeneous. Our research examines large-scale heterogeneous datasets in the context of the decision-making process, encompassing data collection and analysis. We also study the impact of fertiliser applications combined with weather data on crop yield, using the winter wheat crop as a case study. By understanding local contextual and geographic factors, we aspire to stabilise or even reduce the demand for agricultural nutrients while enhancing crop development. The proposed approach is proven to be efficient and scalable, as it is validated using a real-world and large dataset.
</details>
<details>
<summary>摘要</summary>
The dataset used in this study is high-dimensional and heterogeneous, and we examine the impact of fertilizer applications combined with weather data on crop yield using the winter wheat crop as a case study. By understanding local contextual and geographic factors, we aim to stabilize or even reduce the demand for agricultural nutrients while enhancing crop development.Our proposed approach is efficient and scalable, as it is validated using a real-world and large dataset. This study demonstrates the potential of data analytics in optimizing fertilizer applications and promoting sustainable agriculture practices.
</details></li>
</ul>
<hr>
<h2 id="Is-Channel-Independent-strategy-optimal-for-Time-Series-Forecasting"><a href="#Is-Channel-Independent-strategy-optimal-for-Time-Series-Forecasting" class="headerlink" title="Is Channel Independent strategy optimal for Time Series Forecasting?"></a>Is Channel Independent strategy optimal for Time Series Forecasting?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17658">http://arxiv.org/abs/2310.17658</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Peiwen, Zhu Changsheng</li>
<li>for: 这篇论文是为了探讨适用于长期时间序列预测的不同模型。</li>
<li>methods: 这篇论文提出了一种简单 yet effective的策略called Channel Self-Clustering (CSC)，用于线性模型。此外，它还提出了Channel Rearrangement (CR)方法，用于深度模型。</li>
<li>results: 这篇论文的实验结果显示，CSC策略可以提高CI策略的性能，同时减少参数的数量，例如在电力集成数据集上减少了10倍以上。CR方法也可以与基准模型竞争。此外，论文还讨论了是否使用历史时间序列中的同一个通道的历史值来预测未来值。<details>
<summary>Abstract</summary>
There has been an emergence of various models for long-term time series forecasting. Recent studies have demonstrated that a single linear layer, using Channel Dependent (CD) or Channel Independent (CI) modeling, can even outperform a large number of sophisticated models. However, current research primarily considers CD and CI as two complementary yet mutually exclusive approaches, unable to harness these two extremes simultaneously. And it is also a challenging issue that both CD and CI are static strategies that cannot be determined to be optimal for a specific dataset without extensive experiments. In this paper, we reconsider whether the current CI strategy is the best solution for time series forecasting. First, we propose a simple yet effective strategy called CSC, which stands for $\mathbf{C}$hannel $\mathbf{S}$elf-$\mathbf{C}$lustering strategy, for linear models. Our Channel Self-Clustering (CSC) enhances CI strategy's performance improvements while reducing parameter size, for exmpale by over 10 times on electricity dataset, and significantly cutting training time. Second, we further propose Channel Rearrangement (CR), a method for deep models inspired by the self-clustering. CR attains competitive performance against baselines. Finally, we also discuss whether it is best to forecast the future values using the historical values of the same channel as inputs. We hope our findings and methods could inspire new solutions beyond CD/CI.
</details>
<details>
<summary>摘要</summary>
有些新的模型已经出现了用于长期时间序预测。最近的研究表明，一个单一的线性层，使用通道依赖（CD）或通道独立（CI）的方法，可以超越许多复杂的模型。然而，当前的研究主要考虑CD和CI为两种 complementary yet mutually exclusive的方法，无法同时挖掘这两种极点。此外，CD和CI都是静态策略，无法确定是特定数据集的优化方法。在这篇论文中，我们重新考虑现在CI策略是时间序预测的最佳解决方案。首先，我们提出了一种简单 yet effective的策略，称为$\mathbf{C}$hannel $\mathbf{S}$elf-$\mathbf{C}$lustering（CSC）策略，用于线性模型。我们的通道自我凝聚（CSC）策略可以提高CI策略的性能改进，同时减少参数大小，例如在电力数据集上减少了10倍以上。其次，我们还提出了一种受到自我凝聚启发的深度模型策略，称为通道重新排序（CR）策略。CR策略可以与基elines相比。最后，我们还讨论了是否应该使用历史值的同一个通道作为预测未来值的输入。我们希望我们的发现和方法可以激发新的解决方案 beyond CD/CI。
</details></li>
</ul>
<hr>
<h2 id="A-General-Theoretical-Paradigm-to-Understand-Learning-from-Human-Preferences"><a href="#A-General-Theoretical-Paradigm-to-Understand-Learning-from-Human-Preferences" class="headerlink" title="A General Theoretical Paradigm to Understand Learning from Human Preferences"></a>A General Theoretical Paradigm to Understand Learning from Human Preferences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12036">http://arxiv.org/abs/2310.12036</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, Rémi Munos</li>
<li>for: 本文旨在理解现代学习从人类偏好中学习（RLHF）的实际算法。</li>
<li>methods: 本文使用直接偏好优化（DPO）方法，并进行了深入的理论分析。</li>
<li>results: 研究发现，使用新的通用目标函数$\Psi$PO可以减少RLHF和DPO中的两个重要假设，并且可以提供更多的性能保证。此外，在一些示例中，使用 $\Psi$PO 可以实现更高的效率和更好的表现。<details>
<summary>Abstract</summary>
The prevalent deployment of learning from human preferences through reinforcement learning (RLHF) relies on two important approximations: the first assumes that pairwise preferences can be substituted with pointwise rewards. The second assumes that a reward model trained on these pointwise rewards can generalize from collected data to out-of-distribution data sampled by the policy. Recently, Direct Preference Optimisation (DPO) has been proposed as an approach that bypasses the second approximation and learn directly a policy from collected data without the reward modelling stage. However, this method still heavily relies on the first approximation.   In this paper we try to gain a deeper theoretical understanding of these practical algorithms. In particular we derive a new general objective called $\Psi$PO for learning from human preferences that is expressed in terms of pairwise preferences and therefore bypasses both approximations. This new general objective allows us to perform an in-depth analysis of the behavior of RLHF and DPO (as special cases of $\Psi$PO) and to identify their potential pitfalls. We then consider another special case for $\Psi$PO by setting $\Psi$ simply to Identity, for which we can derive an efficient optimisation procedure, prove performance guarantees and demonstrate its empirical superiority to DPO on some illustrative examples.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译为简化字的中文。<</SYS>>现有的学习从人类偏好（RLHF）的广泛部署都 rely 于两个重要的近似：第一个假设每个对比的偏好可以被替换为点 wise 奖励。第二个假设一个基于这些点 wise 奖励的奖励模型可以从收集的数据中泛化到非收集数据。 reciently，Direct Preference Optimization（DPO）被提出，它不需要奖励模型的训练阶段，直接从收集数据中学习策略。然而，这个方法仍然很重视第一个近似。 在这篇论文中，我们尝试了更深入的理论理解这些实际算法。我们 derive 了一个新的通用目标函数called $\Psi$PO，它表示在对比上学习人类偏好的情况下，不需要两个近似。这个新的通用目标函数允许我们对 RLHF 和 DPO（作为 $\Psi$PO 的特殊情况）进行深入的分析，并识别它们的潜在弱点。然后，我们考虑了 $\Psi$PO 中 $\Psi$ 设置为标识函数的特殊情况，可以 derive 一个高效的优化过程，证明性能保证和在一些示例中证明其超越 DPO 的实际性。
</details></li>
</ul>
<hr>
<h2 id="SegmATRon-Embodied-Adaptive-Semantic-Segmentation-for-Indoor-Environment"><a href="#SegmATRon-Embodied-Adaptive-Semantic-Segmentation-for-Indoor-Environment" class="headerlink" title="SegmATRon: Embodied Adaptive Semantic Segmentation for Indoor Environment"></a>SegmATRon: Embodied Adaptive Semantic Segmentation for Indoor Environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12031">http://arxiv.org/abs/2310.12031</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wingrune/segmatron">https://github.com/wingrune/segmatron</a></li>
<li>paper_authors: Tatiana Zemskova, Margarita Kichik, Dmitry Yudin, Aleksei Staroverov, Aleksandr Panov</li>
<li>for: 这篇论文是为了提出一种适应器模型，用于在具有物体意义的图像 semantic segmentation 中进行图像分割。</li>
<li>methods: 这篇论文使用了一种混合多组件损失函数来适应模型参数在多个图像中进行INF 时的调整。</li>
<li>results: 研究表明，通过使用代理人的行动在室内环境中获取更多图像，可以提高 semantic segmentation 的质量。Here’s the breakdown of each point in English:</li>
<li>for: The paper proposes an adaptive transformer model for embodied image semantic segmentation.</li>
<li>methods: The paper uses a hybrid multicomponent loss function to adapt the model weights during inference on multiple images.</li>
<li>results: The study shows that obtaining additional images using the agent’s actions in an indoor environment can improve the quality of semantic segmentation.<details>
<summary>Abstract</summary>
This paper presents an adaptive transformer model named SegmATRon for embodied image semantic segmentation. Its distinctive feature is the adaptation of model weights during inference on several images using a hybrid multicomponent loss function. We studied this model on datasets collected in the photorealistic Habitat and the synthetic AI2-THOR Simulators. We showed that obtaining additional images using the agent's actions in an indoor environment can improve the quality of semantic segmentation. The code of the proposed approach and datasets are publicly available at https://github.com/wingrune/SegmATRon.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种适应器模型，名为SegmATRon，用于具体图像 semantic segmentation。它的特点是在推理过程中对模型参数进行适应，使用混合多组件损失函数。我们在 Habitat 和 AI2-THOR  sintética simulators 上进行了研究，并证明了通过使用机器人的行为在室内环境中获取更多图像可以提高 semantic segmentation 的质量。代码和数据集可以在 https://github.com/wingrune/SegmATRon 上公开获取。
</details></li>
</ul>
<hr>
<h2 id="Multi-view-Contrastive-Learning-for-Entity-Typing-over-Knowledge-Graphs"><a href="#Multi-view-Contrastive-Learning-for-Entity-Typing-over-Knowledge-Graphs" class="headerlink" title="Multi-view Contrastive Learning for Entity Typing over Knowledge Graphs"></a>Multi-view Contrastive Learning for Entity Typing over Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12008">http://arxiv.org/abs/2310.12008</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhiweihu1103/et-mclet">https://github.com/zhiweihu1103/et-mclet</a></li>
<li>paper_authors: Zhiwei Hu, Víctor Gutiérrez-Basulto, Zhiliang Xiang, Ru Li, Jeff Z. Pan</li>
<li>for: 本文旨在提出一种新的知识 graphs 实体类型推断方法（MCLET），以更好地编码知识图中实体和类型的Semantic知识。</li>
<li>methods:  MCLET 方法包括三个模块：一、多视图生成和编码模块，使实体和类型的抽象信息从不同视图得到更好的表示；二、相互视图对比学习模块，使不同视图之间的表示进行协同改进；三、实体类型预测模块，通过多头注意力和混合专家策略来预测缺失的实体类型。</li>
<li>results:  compared to the state-of-the-art, MCLET 方法在实验中显示出了强大的表现。<details>
<summary>Abstract</summary>
Knowledge graph entity typing (KGET) aims at inferring plausible types of entities in knowledge graphs. Existing approaches to KGET focus on how to better encode the knowledge provided by the neighbors and types of an entity into its representation. However, they ignore the semantic knowledge provided by the way in which types can be clustered together. In this paper, we propose a novel method called Multi-view Contrastive Learning for knowledge graph Entity Typing (MCLET), which effectively encodes the coarse-grained knowledge provided by clusters into entity and type embeddings. MCLET is composed of three modules: i) Multi-view Generation and Encoder module, which encodes structured information from entity-type, entity-cluster and cluster-type views; ii) Cross-view Contrastive Learning module, which encourages different views to collaboratively improve view-specific representations of entities and types; iii) Entity Typing Prediction module, which integrates multi-head attention and a Mixture-of-Experts strategy to infer missing entity types. Extensive experiments show the strong performance of MCLET compared to the state-of-the-art
</details>
<details>
<summary>摘要</summary>
知识图Entity类型推断（KGET）目标在于推断知识图中实体的可能性类型。现有的KGET方法主要关注如何更好地编码实体周围的知识和类型到其表示中。然而，它们忽略了实体和类型之间的 semantics 知识，即类型之间的聚合知识。本文提出了一种新的方法called Multi-view Contrastive Learning for knowledge graph Entity Typing（MCLET），它可以有效地编码实体和类型的聚合知识到实体和类型表示中。MCLET包括以下三个模块：1. 多视图生成和编码模块（Multi-view Generation and Encoder module）：编码实体-类型、实体-团队和团队-类型的结构信息。2. 交叉视图对比学习模块（Cross-view Contrastive Learning module）：鼓励不同视图之间的信息进行协同改进视图特定的实体和类型表示。3. 实体类型预测模块（Entity Typing Prediction module）：通过多头注意力和 Mixture-of-Experts 策略来预测缺失的实体类型。广泛的实验表明MCLET在比较顶尖方法的情况下显示出了强大的表现。
</details></li>
</ul>
<hr>
<h2 id="KI-PMF-Knowledge-Integrated-Plausible-Motion-Forecasting"><a href="#KI-PMF-Knowledge-Integrated-Plausible-Motion-Forecasting" class="headerlink" title="KI-PMF: Knowledge Integrated Plausible Motion Forecasting"></a>KI-PMF: Knowledge Integrated Plausible Motion Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12007">http://arxiv.org/abs/2310.12007</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhishek Vivekanandan, Ahmed Abouelazm, Philip Schörner, J. Marius Zöllner</li>
<li>for: 预测交通actor的运动轨迹，以实现自动驾驶车辆大规模部署。</li>
<li>methods: 引入非 Parametric 剪除层和注意力层，以整合定义的知识优化。</li>
<li>results: 实现了遵循物理法律和驾驶环境几何学的轨迹预测，提供了安全可靠的运动预测结果，是实现自动驾驶车辆安全有效的关键。<details>
<summary>Abstract</summary>
Accurately forecasting the motion of traffic actors is crucial for the deployment of autonomous vehicles at a large scale. Current trajectory forecasting approaches primarily concentrate on optimizing a loss function with a specific metric, which can result in predictions that do not adhere to physical laws or violate external constraints. Our objective is to incorporate explicit knowledge priors that allow a network to forecast future trajectories in compliance with both the kinematic constraints of a vehicle and the geometry of the driving environment. To achieve this, we introduce a non-parametric pruning layer and attention layers to integrate the defined knowledge priors. Our proposed method is designed to ensure reachability guarantees for traffic actors in both complex and dynamic situations. By conditioning the network to follow physical laws, we can obtain accurate and safe predictions, essential for maintaining autonomous vehicles' safety and efficiency in real-world settings.In summary, this paper presents concepts that prevent off-road predictions for safe and reliable motion forecasting by incorporating knowledge priors into the training process.
</details>
<details>
<summary>摘要</summary>
<<SYS>>对于大规模自动驾驶 vehicles 的部署， точно预测交通actor 的运动是非常重要的。当前的轨迹预测方法主要集中在优化特定的损失函数中，可能会导致预测不符合物理法律或违反外部约束。我们的目标是将显式知识假设纳入网络中，以预测未来的轨迹，并且遵循车辆的动力学约束和驾驶环境的几何结构。为 achieve 这一目标，我们引入非参数化剪除层和注意层，以整合定义的知识假设。我们的提出方法旨在保证交通actor 的可达性，并在复杂和动态情况下提供可靠的预测。通过使网络遵循物理法律，我们可以获得高度准确和安全的预测结果，这是保持自动驾驶 vehicle 的安全和效率在实际场景中的关键。总之，本文提出了避免脱离路径的安全和可靠轨迹预测方法，通过在训练过程中纳入知识假设。
</details></li>
</ul>
<hr>
<h2 id="Sociotechnical-Safety-Evaluation-of-Generative-AI-Systems"><a href="#Sociotechnical-Safety-Evaluation-of-Generative-AI-Systems" class="headerlink" title="Sociotechnical Safety Evaluation of Generative AI Systems"></a>Sociotechnical Safety Evaluation of Generative AI Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11986">http://arxiv.org/abs/2310.11986</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laura Weidinger, Maribeth Rauh, Nahema Marchal, Arianna Manzini, Lisa Anne Hendricks, Juan Mateos-Garcia, Stevie Bergman, Jackie Kay, Conor Griffin, Ben Bariach, Iason Gabriel, Verena Rieser, William Isaac</li>
<li>for: 评估生成AI系统的安全性</li>
<li>methods: 提出三层框架，包括能力评估、系统安全原则和人类互动的评估</li>
<li>results: 发现现有评估缺陷，并提出了解决方案，包括实践步骤和不同角色的责任<details>
<summary>Abstract</summary>
Generative AI systems produce a range of risks. To ensure the safety of generative AI systems, these risks must be evaluated. In this paper, we make two main contributions toward establishing such evaluations. First, we propose a three-layered framework that takes a structured, sociotechnical approach to evaluating these risks. This framework encompasses capability evaluations, which are the main current approach to safety evaluation. It then reaches further by building on system safety principles, particularly the insight that context determines whether a given capability may cause harm. To account for relevant context, our framework adds human interaction and systemic impacts as additional layers of evaluation. Second, we survey the current state of safety evaluation of generative AI systems and create a repository of existing evaluations. Three salient evaluation gaps emerge from this analysis. We propose ways forward to closing these gaps, outlining practical steps as well as roles and responsibilities for different actors. Sociotechnical safety evaluation is a tractable approach to the robust and comprehensive safety evaluation of generative AI systems.
</details>
<details>
<summary>摘要</summary>
生成AI系统的应用涉及到一系列的风险。为确保生成AI系统的安全，这些风险必须进行评估。在这篇论文中，我们提出了两个主要贡献，以便建立这些评估。首先，我们提议一种三层结构的框架，它采用一种结构化的社会技术方法来评估这些风险。这个框架包括功能评估，这是当前主要的安全评估方法。然后，我们又基于系统安全原则，尤其是认为上下文决定了一个给定的功能是否会带来害。为了考虑相关的上下文，我们的框架添加了人机交互和系统影响作为其他两个层次评估。其次，我们对生成AI系统的安全评估状况进行了调查，并创建了一个库存的评估。三个突出的评估漏洞出现在这种分析中。我们提出了方法来填充这些漏洞，并详细介绍了不同角色的角色和责任。社会技术安全评估是一种可行的方法，以确保生成AI系统的安全评估是全面和可靠的。
</details></li>
</ul>
<hr>
<h2 id="InfoDiffusion-Information-Entropy-Aware-Diffusion-Process-for-Non-Autoregressive-Text-Generation"><a href="#InfoDiffusion-Information-Entropy-Aware-Diffusion-Process-for-Non-Autoregressive-Text-Generation" class="headerlink" title="InfoDiffusion: Information Entropy Aware Diffusion Process for Non-Autoregressive Text Generation"></a>InfoDiffusion: Information Entropy Aware Diffusion Process for Non-Autoregressive Text Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11976">http://arxiv.org/abs/2310.11976</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rzhwang/infodiffusion">https://github.com/rzhwang/infodiffusion</a></li>
<li>paper_authors: Renzhi Wang, Jing Li, Piji Li</li>
<li>for: 提高文本生成质量和多样性，尝试 bridge 人类自然文本生成过程和当前扩散模型的生成过程之间的差距。</li>
<li>methods:  InfoDiffusion 使用 “keyinfo-first” 生成策略，并在不同文本信息量下应用噪音调度。另外，InfoDiffusion 还结合了自我条件和新提出的部分噪音模型结构。</li>
<li>results: InfoDiffusion 在生成质量和多样性方面表现出色，同时 sampling efficiency 也高于基eline模型。<details>
<summary>Abstract</summary>
Diffusion models have garnered considerable interest in the field of text generation. Several studies have explored text diffusion models with different structures and applied them to various tasks, including named entity recognition and summarization. However, there exists a notable disparity between the "easy-first" text generation process of current diffusion models and the "keyword-first" natural text generation process of humans, which has received limited attention. To bridge this gap, we propose InfoDiffusion, a non-autoregressive text diffusion model. Our approach introduces a "keyinfo-first" generation strategy and incorporates a noise schedule based on the amount of text information. In addition, InfoDiffusion combines self-conditioning with a newly proposed partially noising model structure. Experimental results show that InfoDiffusion outperforms the baseline model in terms of generation quality and diversity, as well as exhibiting higher sampling efficiency.
</details>
<details>
<summary>摘要</summary>
diffusion模型在文本生成领域已经引起了广泛的关注。多个研究已经探索了不同结构的 diffusion模型，并应用于名称识别和概要summarization等任务。然而，现有的"易先"文本生成过程和人类的"关键词-先"自然文本生成过程之间存在显著的差距，这一点很少得到了关注。为了bridging这个差距，我们提出了InfoDiffusion，一种非自适应文本diffusion模型。我们的方法采用了"关键信息-先"生成策略，并在文本信息的量 bases on a noise schedule。此外，InfoDiffusion还结合了自conditioning和一种新提出的部分噪音模型结构。实验结果表明，InfoDiffusion在生成质量和多样性方面都超过了基eline模型，同时也显示了更高的采样效率。
</details></li>
</ul>
<hr>
<h2 id="Improving-Generalization-of-Alignment-with-Human-Preferences-through-Group-Invariant-Learning"><a href="#Improving-Generalization-of-Alignment-with-Human-Preferences-through-Group-Invariant-Learning" class="headerlink" title="Improving Generalization of Alignment with Human Preferences through Group Invariant Learning"></a>Improving Generalization of Alignment with Human Preferences through Group Invariant Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11971">http://arxiv.org/abs/2310.11971</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Zheng, Wei Shen, Yuan Hua, Wenbin Lai, Shihan Dou, Yuhao Zhou, Zhiheng Xi, Xiao Wang, Haoran Huang, Tao Gui, Qi Zhang, Xuanjing Huang</li>
<li>for: This paper aims to improve the stability and generalization of AI assistants based on language models (LLMs) by proposing a novel approach to Reinforcement Learning from Human Feedback (RLHF).</li>
<li>methods: The proposed approach uses a combination of data classification and adaptive exploration to learn a consistent policy across various domains. It deliberately maximizes performance variance and allocates more learning capacity to challenging data.</li>
<li>results: The experimental results show that the proposed approach significantly enhances training stability and model generalization, outperforming traditional RL methods that exploit shortcuts and overlook challenging samples.<details>
<summary>Abstract</summary>
The success of AI assistants based on language models (LLMs) hinges crucially on Reinforcement Learning from Human Feedback (RLHF), which enables the generation of responses more aligned with human preferences. As universal AI assistants, there's a growing expectation for them to perform consistently across various domains. However, previous work shows that Reinforcement Learning (RL) often exploits shortcuts to attain high rewards and overlooks challenging samples. This focus on quick reward gains undermines both the stability in training and the model's ability to generalize to new, unseen data. In this work, we propose a novel approach that can learn a consistent policy via RL across various data groups or domains. Given the challenges associated with acquiring group annotations, our method automatically classifies data into different groups, deliberately maximizing performance variance. Then, we optimize the policy to perform well on challenging groups. Lastly, leveraging the established groups, our approach adaptively adjusts the exploration space, allocating more learning capacity to more challenging data and preventing the model from over-optimizing on simpler data. Experimental results indicate that our approach significantly enhances training stability and model generalization.
</details>
<details>
<summary>摘要</summary>
成功的语言模型基于AI助手（LLMs）取决于人类反馈学习（RLHF），它使得生成响应更加与人类偏好相align。作为普遍的AI助手，人们对它们的表现具有增加的期望，但previous work表明，使用学习策略（RL）时，经常利用短cut掉到达高奖励，而忽略了复杂的样本。这种围绕快速奖励的专注会使训练不稳定并导致模型无法在新、未看到的数据上Generalize。在这项工作中，我们提出了一种新的方法，可以通过RL在不同的数据组或领域中学习一个一致的策略。由于获得组注释的困难，我们的方法会自动将数据分类为不同的组，故意增加性能的变化。然后，我们会优化策略，以便在复杂的组中表现好。最后，我们通过已有的组来适应性地调整探索空间，将更多的学习资源分配给更加复杂的数据，避免模型在简单的数据上过度优化。实验结果表明，我们的方法可以显著提高训练稳定性和模型泛化。
</details></li>
</ul>
<hr>
<h2 id="A-Multi-Scale-Decomposition-MLP-Mixer-for-Time-Series-Analysis"><a href="#A-Multi-Scale-Decomposition-MLP-Mixer-for-Time-Series-Analysis" class="headerlink" title="A Multi-Scale Decomposition MLP-Mixer for Time Series Analysis"></a>A Multi-Scale Decomposition MLP-Mixer for Time Series Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11959">http://arxiv.org/abs/2310.11959</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zshhans/msd-mixer">https://github.com/zshhans/msd-mixer</a></li>
<li>paper_authors: Shuhan Zhong, Sizhe Song, Guanyao Li, Weipeng Zhuo, Yang Liu, S. -H. Gary Chan</li>
<li>for: 这篇论文是为了提出一种基于多尺度分解和多层混合的深度学习方法，以更好地处理时间序列数据的特殊特点和复杂多尺度时间变化。</li>
<li>methods: 这篇论文使用了一种叫做Multi-Scale Decomposition MLP-Mixer（MSD-Mixer）的方法，它可以输出不同层次的时间序列分解成分，并将这些分解成分与不同层次的时间序列进行混合，以捕捉时间序列的多尺度特征和互相关联。</li>
<li>results: 这篇论文的实验结果显示，使用MSD-Mixer方法可以在不同的时间序列分析任务（长期和短期预测、填充、侦测异常和分类）中，与其他现有的任务特定和任务通用方法相比，实现了更好的性能。<details>
<summary>Abstract</summary>
Time series data, often characterized by unique composition and complex multi-scale temporal variations, requires special consideration of decomposition and multi-scale modeling in its analysis. Existing deep learning methods on this best fit to only univariate time series, and have not sufficiently accounted for sub-series level modeling and decomposition completeness. To address this, we propose MSD-Mixer, a Multi-Scale Decomposition MLP-Mixer which learns to explicitly decompose the input time series into different components, and represents the components in different layers. To handle multi-scale temporal patterns and inter-channel dependencies, we propose a novel temporal patching approach to model the time series as multi-scale sub-series, i.e., patches, and employ MLPs to mix intra- and inter-patch variations and channel-wise correlations. In addition, we propose a loss function to constrain both the magnitude and autocorrelation of the decomposition residual for decomposition completeness. Through extensive experiments on various real-world datasets for five common time series analysis tasks (long- and short-term forecasting, imputation, anomaly detection, and classification), we demonstrate that MSD-Mixer consistently achieves significantly better performance in comparison with other state-of-the-art task-general and task-specific approaches.
</details>
<details>
<summary>摘要</summary>
时间序列数据，经常具有独特的组成和复杂的多尺度时间变化，需要特殊地对它进行分解和多尺度模型化的分析。现有的深度学习方法只适用于单变量时间序列，并未充分考虑分解完整性和子序列水平的模型化。为解决这个问题，我们提议了MSD-Mixer，一种多尺度分解MLP-Mixer，可以显式地将输入时间序列分解成不同的组成部分，并将这些部分在不同层中表示。同时，我们提出了一种新的时间补丁方法，可以模型时间序列为多尺度子序列，即补丁，并使用MLP来混合内部和外部补丁的变化和通道之间的相关性。此外，我们还提出了一种约束分解 оста异量和自相关的损失函数，以确保分解完整性。经过对各种实际世界数据集的多种时间序列分析任务（长期和短期预测、填充、异常检测和分类）的广泛实验，我们示示了MSD-Mixer在与其他当前领域的任务特定和任务通用方法相比，具有显著更好的表现。
</details></li>
</ul>
<hr>
<h2 id="Too-Good-To-Be-True-performance-overestimation-in-re-current-practices-for-Human-Activity-Recognition"><a href="#Too-Good-To-Be-True-performance-overestimation-in-re-current-practices-for-Human-Activity-Recognition" class="headerlink" title="Too Good To Be True: performance overestimation in (re)current practices for Human Activity Recognition"></a>Too Good To Be True: performance overestimation in (re)current practices for Human Activity Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11950">http://arxiv.org/abs/2310.11950</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrés Tello, Victoria Degeler, Alexander Lazovik</li>
<li>for: The paper aims to raise awareness about the issue of accuracy overestimation in Human Activity Recognition (HAR) studies due to biased data segmentation and evaluation methods.</li>
<li>methods: The paper uses sliding windows for data segmentation and standard random k-fold cross validation, which are common approaches in state-of-the-art HAR studies, but can lead to biased results.</li>
<li>results: The paper shows that these biased methods can produce lower accuracies than correct unbiased methods, and that the problem persists independently of the method or dataset used.Here are the three points in Simplified Chinese text:</li>
<li>for: 这篇论文旨在提醒科学界关于人活动识别（HAR）研究中的准确度过估问题，具体来说是因为数据 segmentation和评估方法的偏见导致的。</li>
<li>methods: 这篇论文使用滑动窗口方法进行数据 segmentation，并使用标准随机k-fold交叉验证法，这些方法是当今HAR研究中最常用的，但可能导致偏见的结果。</li>
<li>results: 这篇论文显示，这些偏见方法可能会生成较低的准确度，而正确的不偏见方法更难在科学期刊上发表。<details>
<summary>Abstract</summary>
Today, there are standard and well established procedures within the Human Activity Recognition (HAR) pipeline. However, some of these conventional approaches lead to accuracy overestimation. In particular, sliding windows for data segmentation followed by standard random k-fold cross validation, produce biased results. An analysis of previous literature and present-day studies, surprisingly, shows that these are common approaches in state-of-the-art studies on HAR. It is important to raise awareness in the scientific community about this problem, whose negative effects are being overlooked. Otherwise, publications of biased results lead to papers that report lower accuracies, with correct unbiased methods, harder to publish. Several experiments with different types of datasets and different types of classification models allow us to exhibit the problem and show it persists independently of the method or dataset.
</details>
<details>
<summary>摘要</summary>
今天，人活动识别（HAR）管道中有标准化和确定的程序。然而，这些传统方法会导致准确性过估。具体来说，使用滑块窗口进行数据分割，然后使用标准随机k-叶值验证，会产生偏见结果。历史和当代研究的分析表明，这些是现代HAR研究中最常见的方法。重要的是，我们需要在科学社区中启示这个问题，以避免这种偏见的负面影响。否则，使用偏见方法的出版物将导致准确方法更难于发表。我们通过不同的数据集和不同的分类模型进行了多个实验，证明了这个问题的存在，并证明它独立于方法或数据集。
</details></li>
</ul>
<hr>
<h2 id="A-Benchmark-for-Semi-Inductive-Link-Prediction-in-Knowledge-Graphs"><a href="#A-Benchmark-for-Semi-Inductive-Link-Prediction-in-Knowledge-Graphs" class="headerlink" title="A Benchmark for Semi-Inductive Link Prediction in Knowledge Graphs"></a>A Benchmark for Semi-Inductive Link Prediction in Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11917">http://arxiv.org/abs/2310.11917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrian Kochsiek, Rainer Gemulla</li>
<li>for: 本文旨在提出和评估大规模知识图（KG）中 semi-inductive link prediction（LP）模型的一个大规模 benchmark。</li>
<li>methods: 本文使用了 Wikidata5M 作为基础，并提供了三种 LP 任务：推导式（k-shot）、辅助式（transductive）和零批式（0-shot），每种任务都有不同的可用信息，包括 KG 结构、文本提及和实体的详细描述。</li>
<li>results: 据小规模实验结果表明， semi-inductive LP 性能与辅助式 LP 性能在长尾实体上存在差异，并且 semi-inductive LP 性能远远低于辅助式 LP 性能。 这个 benchmark 为未来在 semi-inductive LP 模型中集成文本和上下文信息进行进一步研究提供了一个测试床。<details>
<summary>Abstract</summary>
Semi-inductive link prediction (LP) in knowledge graphs (KG) is the task of predicting facts for new, previously unseen entities based on context information. Although new entities can be integrated by retraining the model from scratch in principle, such an approach is infeasible for large-scale KGs, where retraining is expensive and new entities may arise frequently. In this paper, we propose and describe a large-scale benchmark to evaluate semi-inductive LP models. The benchmark is based on and extends Wikidata5M: It provides transductive, k-shot, and 0-shot LP tasks, each varying the available information from (i) only KG structure, to (ii) including textual mentions, and (iii) detailed descriptions of the entities. We report on a small study of recent approaches and found that semi-inductive LP performance is far from transductive performance on long-tail entities throughout all experiments. The benchmark provides a test bed for further research into integrating context and textual information in semi-inductive LP models.
</details>
<details>
<summary>摘要</summary>
《知识 graphs（KG）中的半推导链预测（LP）任务是预测新、未看过的实体上的事实，基于上下文信息。虽然新实体可以通过重新训练模型来扩展，但这种方法在大规模KG中是不可行的，因为重新训练是昂贵的并且新实体可能会频繁出现。在这篇论文中，我们提出了一个大规模的LP模型评估标准 benchmark。该 benchmark 基于并扩展了 Wikidata5M：它提供了半推导、k-shot、0-shot LP任务，每个任务都不同的提供KG结构、文本提及和实体的详细描述。我们对一些最新的方法进行了一小项研究，发现在长尾实体上，半推导LP性能与推导LP性能在所有实验中都远远不同。该 benchmark 提供了一个研究 semi-inductive LP 模型integrating上下文和文本信息的测试床。》Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Analyze-Mass-Spectrometry-data-with-Artificial-Intelligence-to-assist-the-understanding-of-past-habitability-of-Mars-and-provide-insights-for-future-missions"><a href="#Analyze-Mass-Spectrometry-data-with-Artificial-Intelligence-to-assist-the-understanding-of-past-habitability-of-Mars-and-provide-insights-for-future-missions" class="headerlink" title="Analyze Mass Spectrometry data with Artificial Intelligence to assist the understanding of past habitability of Mars and provide insights for future missions"></a>Analyze Mass Spectrometry data with Artificial Intelligence to assist the understanding of past habitability of Mars and provide insights for future missions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11888">http://arxiv.org/abs/2310.11888</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ioannisnasios/marsspectrometry2_gaschromatography">https://github.com/ioannisnasios/marsspectrometry2_gaschromatography</a></li>
<li>paper_authors: Ioannis Nasios</li>
<li>for: 这个研究用于检测古代火星是否可居住，但同时这种方法也可以应用于我们太阳系中的任何天体。</li>
<li>methods: 这个研究使用人工智能分析火星气相学数据，包括演化气相分析（EGA-MS）和气chromatography（GC-MS）两种技术，以确定古代火星样本中的特定化学物质。</li>
<li>results: 研究表明EGA-MS和GC-MS数据可以用于描述外星物质的化学成分，并且提供了一种可靠的方法来分析这些数据。<details>
<summary>Abstract</summary>
This paper presents an application of artificial intelligence on mass spectrometry data for detecting habitability potential of ancient Mars. Although data was collected for planet Mars the same approach can be replicated for any terrestrial object of our solar system. Furthermore, proposed methodology can be adapted to any domain that uses mass spectrometry. This research is focused in data analysis of two mass spectrometry techniques, evolved gas analysis (EGA-MS) and gas chromatography (GC-MS), which are used to identify specific chemical compounds in geological material samples. The study demonstrates the applicability of EGA-MS and GC-MS data to extra-terrestrial material analysis. Most important features of proposed methodology includes square root transformation of mass spectrometry values, conversion of raw data to 2D sprectrograms and utilization of specific machine learning models and techniques to avoid overfitting on relative small datasets. Both EGA-MS and GC-MS datasets come from NASA and two machine learning competitions that the author participated and exploited. Complete running code for the GC-MS dataset/competition is available at GitHub.1 Raw training mass spectrometry data include [0, 1] labels of specific chemical compounds, selected to provide valuable insights and contribute to our understanding of the potential past habitability of Mars.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="From-Neural-Activations-to-Concepts-A-Survey-on-Explaining-Concepts-in-Neural-Networks"><a href="#From-Neural-Activations-to-Concepts-A-Survey-on-Explaining-Concepts-in-Neural-Networks" class="headerlink" title="From Neural Activations to Concepts: A Survey on Explaining Concepts in Neural Networks"></a>From Neural Activations to Concepts: A Survey on Explaining Concepts in Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11884">http://arxiv.org/abs/2310.11884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jae Hee Lee, Sergio Lanza, Stefan Wermter</li>
<li>for: 本文主要用于探讨现代神经网络中概念解释的方法。</li>
<li>methods: 本文使用了多种方法来解释神经网络中的概念，包括特征重要性分析、模型解释和概念映射等。</li>
<li>results: 本文通过对多种神经网络模型进行分析，发现了一些有用的概念解释方法，并且提出了一些可能的应用场景。这些结果可能为实现基于可解释概念的神经网络和符号学AI做出了重要贡献。<details>
<summary>Abstract</summary>
In this paper, we review recent approaches for explaining concepts in neural networks. Concepts can act as a natural link between learning and reasoning: once the concepts are identified that a neural learning system uses, one can integrate those concepts with a reasoning system for inference or use a reasoning system to act upon them to improve or enhance the learning system. On the other hand, knowledge can not only be extracted from neural networks but concept knowledge can also be inserted into neural network architectures. Since integrating learning and reasoning is at the core of neuro-symbolic AI, the insights gained from this survey can serve as an important step towards realizing neuro-symbolic AI based on explainable concepts.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们对现代神经网络中的概念解释方法进行了评论。神经网络中的概念可以作为自然的链接连接学习和理解：一旦已经确定了神经学习系统使用的概念，那么可以将这些概念与符号系统集成，以进行推理或使用符号系统来改进或增强神经学习系统。同时，可以从神经网络中提取知识，同时也可以将符号知识插入到神经网络架构中。由于将学习和理解集成是神经 симвоlic AI 的核心，这些评论所获得的启示可以作为实现神经 симвоlic AI 基于可解释的概念的重要一步。
</details></li>
</ul>
<hr>
<h2 id="AI-Nushu-An-Exploration-of-Language-Emergence-in-Sisterhood-Through-the-Lens-of-Computational-Linguistics"><a href="#AI-Nushu-An-Exploration-of-Language-Emergence-in-Sisterhood-Through-the-Lens-of-Computational-Linguistics" class="headerlink" title="AI Nushu: An Exploration of Language Emergence in Sisterhood -Through the Lens of Computational Linguistics"></a>AI Nushu: An Exploration of Language Emergence in Sisterhood -Through the Lens of Computational Linguistics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11870">http://arxiv.org/abs/2310.11870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuqian Sun, Yuying Tang, Ze Gao, Zhijun Pan, Chuyan Xu, Yurou Chen, Kejiang Qian, Zhigang Wang, Tristan Braud, Chang Hee Lee, Ali Asadipour</li>
<li>for: 这篇论文旨在探讨一种基于女性文化遗产的人工智能语言系统，即“AI Nushu”。</li>
<li>methods: 该论文使用了人工智能技术，将中文词典和女性文字资料库训练两个人工智能代理人，以便共同创建一种标准写作系统，用于编码中文。</li>
<li>results: 该研究提供了一种搭建在人工智能技术和中国文化遗产之上的艺术解读，以及一种将女性视角与计算语言学融合的新的视角。<details>
<summary>Abstract</summary>
This paper presents "AI Nushu," an emerging language system inspired by Nushu (women's scripts), the unique language created and used exclusively by ancient Chinese women who were thought to be illiterate under a patriarchal society. In this interactive installation, two artificial intelligence (AI) agents are trained in the Chinese dictionary and the Nushu corpus. By continually observing their environment and communicating, these agents collaborate towards creating a standard writing system to encode Chinese. It offers an artistic interpretation of the creation of a non-western script from a computational linguistics perspective, integrating AI technology with Chinese cultural heritage and a feminist viewpoint.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Enhancing-Genetic-Improvement-Mutations-Using-Large-Language-Models"><a href="#Enhancing-Genetic-Improvement-Mutations-Using-Large-Language-Models" class="headerlink" title="Enhancing Genetic Improvement Mutations Using Large Language Models"></a>Enhancing Genetic Improvement Mutations Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19813">http://arxiv.org/abs/2310.19813</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander E. I. Brownlee, James Callan, Karine Even-Mendoza, Alina Geiger, Carol Hanna, Justyna Petke, Federica Sarro, Dominik Sobania</li>
<li>for: 本研究探讨了使用大语言模型（LLM）进行生成改进（Genetic Improvement，GI）的搜索技术。</li>
<li>methods: 本研究使用了OpenAI的API来生成JCodec工具的编辑。研究采用了5种不同的编辑类型进行随机抽样。</li>
<li>results: 研究发现，使用LLM生成的编辑可以提高单元测试通过率达到75%，但找到最佳改进的patch通常是通过标准插入编辑。此外，LLM增强的GI可以找到许多改进patch，但是最佳改进patch是通过标准GI找到的。<details>
<summary>Abstract</summary>
Large language models (LLMs) have been successfully applied to software engineering tasks, including program repair. However, their application in search-based techniques such as Genetic Improvement (GI) is still largely unexplored. In this paper, we evaluate the use of LLMs as mutation operators for GI to improve the search process. We expand the Gin Java GI toolkit to call OpenAI's API to generate edits for the JCodec tool. We randomly sample the space of edits using 5 different edit types. We find that the number of patches passing unit tests is up to 75% higher with LLM-based edits than with standard Insert edits. Further, we observe that the patches found with LLMs are generally less diverse compared to standard edits. We ran GI with local search to find runtime improvements. Although many improving patches are found by LLM-enhanced GI, the best improving patch was found by standard GI.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已成功应用于软件工程任务，包括程序修复。然而，它们在基于搜索的技术，如遗传改进（GI）中的应用仍然是未知之地。在这篇论文中，我们评估了使用 LLM 作为 GI 中的变异运算来改善搜索过程。我们扩展了 Gin Java GI 工具包，以调用 OpenAI 的 API 生成 JCodec 工具中的修改。我们随机采样了修改空间，使用 5 种不同的修改类型。我们发现，使用 LLM 生成的修改可以提高单元测试通过率达到 75%，而且发现 LLM 生成的修改通常比标准插入修改更加稳定。此外，我们发现使用 LLM 增强 GI 可以找到更好的改进补丁，但最佳改进补丁仍然由标准 GI 找到。
</details></li>
</ul>
<hr>
<h2 id="The-Value-Sensitive-Conversational-Agent-Co-Design-Framework"><a href="#The-Value-Sensitive-Conversational-Agent-Co-Design-Framework" class="headerlink" title="The Value-Sensitive Conversational Agent Co-Design Framework"></a>The Value-Sensitive Conversational Agent Co-Design Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11848">http://arxiv.org/abs/2310.11848</a></li>
<li>repo_url: None</li>
<li>paper_authors: Malak Sadek, Rafael A. Calvo, Celine Mougenot</li>
<li>for: 本研究旨在提出一个价值敏感对话代理（VSCA）框架，实现对价值敏感对话代理的共同设计（co-design）。</li>
<li>methods: 本研究使用了以前的研究中所识别的需求，以及一个实用的框架，包括一个设计工具组。</li>
<li>results: 本研究提出了一个评估协议，以评估框架和设计工具组在设计工作室中的效果。<details>
<summary>Abstract</summary>
Conversational agents (CAs) are gaining traction in both industry and academia, especially with the advent of generative AI and large language models. As these agents are used more broadly by members of the general public and take on a number of critical use cases and social roles, it becomes important to consider the values embedded in these systems. This consideration includes answering questions such as 'whose values get embedded in these agents?' and 'how do those values manifest in the agents being designed?' Accordingly, the aim of this paper is to present the Value-Sensitive Conversational Agent (VSCA) Framework for enabling the collaborative design (co-design) of value-sensitive CAs with relevant stakeholders. Firstly, requirements for co-designing value-sensitive CAs which were identified in previous works are summarised here. Secondly, the practical framework is presented and discussed, including its operationalisation into a design toolkit. The framework facilitates the co-design of three artefacts that elicit stakeholder values and have a technical utility to CA teams to guide CA implementation, enabling the creation of value-embodied CA prototypes. Finally, an evaluation protocol for the framework is proposed where the effects of the framework and toolkit are explored in a design workshop setting to evaluate both the process followed and the outcomes produced.
</details>
<details>
<summary>摘要</summary>
对话代理（CA）在工业和学术界受到推广，特别是在生成AI和大型自然语言模型的出现后。这些代理在公众中更加广泛使用，扮演许多重要的使用案和社会角色，因此需要考虑这些系统中嵌入的价值。因此，本文的目的是提出价值敏感对话代理（VSCA）框架，帮助专业人员和重要参与者在一起设计价值敏感CA。首先，以前的研究中所识别出的实现值敏感CA的需求简述了一下。其次，实际的框架被提出来，并讨论了它的实现方式。这个框架包括三个展示实物，吸引参与者的价值，并对CA团队提供技术实用性，帮助创建具有价值的CA原型。最后，为这个框架和工具组提出评估协议，以评估这个框架和工具组在设计工作室中的影响，以及它们创造的结果。
</details></li>
</ul>
<hr>
<h2 id="Masked-Pretraining-for-Multi-Agent-Decision-Making"><a href="#Masked-Pretraining-for-Multi-Agent-Decision-Making" class="headerlink" title="Masked Pretraining for Multi-Agent Decision Making"></a>Masked Pretraining for Multi-Agent Decision Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11846">http://arxiv.org/abs/2310.11846</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Liu, Yinmin Zhang, Chuming Li, Chao Yang, Yaodong Yang, Yu Liu, Wanli Ouyang</li>
<li>for: 这篇论文主要针对多智能体决策问题，旨在建立一个通用智能体可以在零情况下完成决策。</li>
<li>methods: 该论文提出了一种基于trasformer架构的MaskMA模型，通过面具学习策略来解决多智能体设置下的困难。此外，该模型还实现了一种通用行为表示，可以在不同的智能体数量和动作空间下进行扩展。</li>
<li>results: 实验结果表明，使用MaskMA模型可以在11个训练地图上进行零情况下的赢利率达77.8%，并且在其他类型的下游任务中表现良好（如多策略协作和随机团队游戏）。<details>
<summary>Abstract</summary>
Building a single generalist agent with zero-shot capability has recently sparked significant advancements in decision-making. However, extending this capability to multi-agent scenarios presents challenges. Most current works struggle with zero-shot capabilities, due to two challenges particular to the multi-agent settings: a mismatch between centralized pretraining and decentralized execution, and varying agent numbers and action spaces, making it difficult to create generalizable representations across diverse downstream tasks. To overcome these challenges, we propose a \textbf{Mask}ed pretraining framework for \textbf{M}ulti-\textbf{a}gent decision making (MaskMA). This model, based on transformer architecture, employs a mask-based collaborative learning strategy suited for decentralized execution with partial observation. Moreover, MaskMA integrates a generalizable action representation by dividing the action space into actions toward self-information and actions related to other entities. This flexibility allows MaskMA to tackle tasks with varying agent numbers and thus different action spaces. Extensive experiments in SMAC reveal MaskMA, with a single model pretrained on 11 training maps, can achieve an impressive 77.8% zero-shot win rate on 60 unseen test maps by decentralized execution, while also performing effectively on other types of downstream tasks (\textit{e.g.,} varied policies collaboration and ad hoc team play).
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Brain-decoding-toward-real-time-reconstruction-of-visual-perception"><a href="#Brain-decoding-toward-real-time-reconstruction-of-visual-perception" class="headerlink" title="Brain decoding: toward real-time reconstruction of visual perception"></a>Brain decoding: toward real-time reconstruction of visual perception</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19812">http://arxiv.org/abs/2310.19812</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yohann Benchetrit, Hubert Banville, Jean-Rémi King</li>
<li>for: 这研究旨在实时解oding brain activity中的视觉过程</li>
<li>methods: 使用 magnetoencephalography (MEG) 和一种基于嵌入学习的解码模型</li>
<li>results: 1. MEG decoder 可以7倍提高图像检索率; 2. 脑响应图像具有高级视觉特征; 3. 图像检索和生成都表明MEG信号主要含有高级视觉特征, 而7T fMRI 则捕捉低级视觉特征。<details>
<summary>Abstract</summary>
In the past five years, the use of generative and foundational AI systems has greatly improved the decoding of brain activity. Visual perception, in particular, can now be decoded from functional Magnetic Resonance Imaging (fMRI) with remarkable fidelity. This neuroimaging technique, however, suffers from a limited temporal resolution ($\approx$0.5 Hz) and thus fundamentally constrains its real-time usage. Here, we propose an alternative approach based on magnetoencephalography (MEG), a neuroimaging device capable of measuring brain activity with high temporal resolution ($\approx$5,000 Hz). For this, we develop an MEG decoding model trained with both contrastive and regression objectives and consisting of three modules: i) pretrained embeddings obtained from the image, ii) an MEG module trained end-to-end and iii) a pretrained image generator. Our results are threefold: Firstly, our MEG decoder shows a 7X improvement of image-retrieval over classic linear decoders. Second, late brain responses to images are best decoded with DINOv2, a recent foundational image model. Third, image retrievals and generations both suggest that MEG signals primarily contain high-level visual features, whereas the same approach applied to 7T fMRI also recovers low-level features. Overall, these results provide an important step towards the decoding - in real time - of the visual processes continuously unfolding within the human brain.
</details>
<details>
<summary>摘要</summary>
在过去五年，基于生成和基础AI系统的使用已经大幅提高了脑动力的解码。视觉认知特别是可以通过功能磁共振成像（fMRI）进行高度准确的解码。然而，这种神经成像技术受到时间分辨率的限制（约为0.5Hz），因此在实时应用中受到极大的限制。我们提出了一种备选方案，基于磁共振成像（MEG），这种神经成像设备可以在高时间分辨率（约为5000Hz）下测量脑动力。为此，我们开发了一个基于MEG的解码模型，其包括三个模块：i）预训练的嵌入，ii）基于MEG的练习结构，iii）预训练的图像生成器。我们的结果如下：首先，我们的MEG解码器与 классические线性解码器相比，图像检索的性能提高了7倍。其次，对于图像的晚期响应，DINOv2，一种最新的基础图像模型，表现最佳。最后，图像检索和生成都表明MEG信号主要含有高级视觉特征，而使用7T fMRI也能够恢复低级特征。总之，这些结果为实时解码人类大脑中不断发展的视觉过程提供了重要的一步。
</details></li>
</ul>
<hr>
<h2 id="Classification-Aggregation-without-Unanimity"><a href="#Classification-Aggregation-without-Unanimity" class="headerlink" title="Classification Aggregation without Unanimity"></a>Classification Aggregation without Unanimity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11841">http://arxiv.org/abs/2310.11841</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olivier Cailloux, Matthieu Hervouin, Ali I. Ozkes, M. Remzi Sanver</li>
<li>for: 这篇论文主要针对 классификация聚合函数的研究。</li>
<li>methods: 论文使用了 dictatorship 来描述每个公民独立的 классификация聚合函数。</li>
<li>results: 论文显示了每个独立和不同的类别聚合函数都是 dictatorship，这与 Maniquet 和 Mongin （2016）的结果相同。此外，论文还提出了一种新的证明方法，可以涵盖两个类别的情况，除非对象的数量也是两个。最后，论文还列出了两个类别和两个对象的所有独立和一致的 классификация聚合函数。<details>
<summary>Abstract</summary>
A classification is a surjective mapping from a set of objects to a set of categories. A classification aggregation function aggregates every vector of classifications into a single one. We show that every citizen sovereign and independent classification aggregation function is essentially a dictatorship. This impossibility implies an earlier result of Maniquet and Mongin (2016), who show that every unanimous and independent classification aggregation function is a dictatorship. The relationship between the two impossibilities is reminiscent to the relationship between Wilson's and Arrow's impossibilities in preference aggregation. Moreover, while the Maniquet-Mongin impossibility rests on the existence of at least three categories, we propose an alternative proof technique that covers the case of two categories, except when the number of objects is also two. We also identify all independent and unanimous classification aggregation functions for the case of two categories and two objects.
</details>
<details>
<summary>摘要</summary>
一种分类是一个射函数，将一个集合对象映射到另一个集合类别。一个分类汇聚函数将每个vector分类汇聚成一个单一的汇聚结果。我们表明，每个公民独立和自主的分类汇聚函数都是一种独裁统治。这一不可能性等价于 Earlier Maniquet 和 Mongin（2016）的结果，他们表明，每个一致和独立的分类汇聚函数都是一种独裁统治。这两个不可能性之间的关系与 Wilson 和 Arrow 的不可能性在偏好汇聚中有相似之处。此外，我们提出了一种不同的证明技巧，覆盖了三个类别的情况，而不是两个类别和两个对象的情况。我们还确定了所有独立和一致的分类汇聚函数的情况，只有两个类别和两个对象的情况例外。
</details></li>
</ul>
<hr>
<h2 id="IntentDial-An-Intent-Graph-based-Multi-Turn-Dialogue-System-with-Reasoning-Path-Visualization"><a href="#IntentDial-An-Intent-Graph-based-Multi-Turn-Dialogue-System-with-Reasoning-Path-Visualization" class="headerlink" title="IntentDial: An Intent Graph based Multi-Turn Dialogue System with Reasoning Path Visualization"></a>IntentDial: An Intent Graph based Multi-Turn Dialogue System with Reasoning Path Visualization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11818">http://arxiv.org/abs/2310.11818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zengguang Hao, Jie Zhang, Binxia Xu, Yafang Wang, Gerard de Melo, Xiaolong Li</li>
<li>for: 本研究旨在提高对话系统的听众感知和响应能力，使其能够更好地理解用户的意图和需求。</li>
<li>methods: 该研究提出了一种基于图的多Turn对话系统，使用了反馈学习来自动地从对话中提取用户的意图元素和标准查询。此外，还提供了可视化组件，以便监视对话中每个转折的直接逻辑路径。</li>
<li>results: 该研究通过实验证明了该系统的可行性和效果，并且可以帮助提高对话系统的实际应用。<details>
<summary>Abstract</summary>
Intent detection and identification from multi-turn dialogue has become a widely explored technique in conversational agents, for example, voice assistants and intelligent customer services. The conventional approaches typically cast the intent mining process as a classification task. Although neural classifiers have proven adept at such classification tasks, the issue of neural network models often impedes their practical deployment in real-world settings. We present a novel graph-based multi-turn dialogue system called , which identifies a user's intent by identifying intent elements and a standard query from a dynamically constructed and extensible intent graph using reinforcement learning. In addition, we provide visualization components to monitor the immediate reasoning path for each turn of a dialogue, which greatly facilitates further improvement of the system.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。<</SYS>>对话机器人中的意图检测和识别已经广泛研究，例如语音助手和智能客服。传统方法通常将意图挖掘过程视为一个分类任务。虽然神经网络模型在这类分类任务中表现出色，但神经网络模型在实际应用中的实现往往受阻。我们提出了一种新的图表基多轮对话系统，可以通过动态构建和扩展意图图来识别用户的意图，并使用回归学习来确定意图元素和标准查询。此外，我们还提供了可视化组件，可以帮助监测对话中每个转折的直接逻辑路径，这对系统进一步改进很有帮助。
</details></li>
</ul>
<hr>
<h2 id="Conservative-Predictions-on-Noisy-Financial-Data"><a href="#Conservative-Predictions-on-Noisy-Financial-Data" class="headerlink" title="Conservative Predictions on Noisy Financial Data"></a>Conservative Predictions on Noisy Financial Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11815">http://arxiv.org/abs/2310.11815</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omkar Nabar, Gautam Shroff</li>
<li>for: 用于降低金融市场中运动风险的Fixed-term returns预测</li>
<li>methods: 使用 tradicional MLPs 和可微分决策树，在 sintetic data 和实际金融市场数据上预测Fixed-term returns，并通过遍历多个模型来减少数据噪音</li>
<li>results: our approach 可以获得更高的总收益，同时降低风险水平，并且提出了一个新的实用指标来衡量每笔交易的平均收益和风险衡量指标。<details>
<summary>Abstract</summary>
Price movements in financial markets are well known to be very noisy. As a result, even if there are, on occasion, exploitable patterns that could be picked up by machine-learning algorithms, these are obscured by feature and label noise rendering the predictions less useful, and risky in practice. Traditional rule-learning techniques developed for noisy data, such as CN2, would seek only high precision rules and refrain from making predictions where their antecedents did not apply. We apply a similar approach, where a model abstains from making a prediction on data points that it is uncertain on. During training, a cascade of such models are learned in sequence, similar to rule lists, with each model being trained only on data on which the previous model(s) were uncertain. Similar pruning of data takes place at test-time, with (higher accuracy) predictions being made albeit only on a fraction (support) of test-time data. In a financial prediction setting, such an approach allows decisions to be taken only when the ensemble model is confident, thereby reducing risk. We present results using traditional MLPs as well as differentiable decision trees, on synthetic data as well as real financial market data, to predict fixed-term returns using commonly used features. We submit that our approach is likely to result in better overall returns at a lower level of risk. In this context we introduce an utility metric to measure the average gain per trade, as well as the return adjusted for downside risk, both of which are improved significantly by our approach.
</details>
<details>
<summary>摘要</summary>
金融市场的价格变化非常具有噪音特性，因此，即使有时存在可以被机器学习算法捕捉的可见模式，这些模式受到特征和标签噪音的干扰，导致预测的准确性受到限制，实际应用中风险较高。传统的规则学习技术，如CN2，会寻找高精度规则，并在其前提不适用时停止预测。我们采用类似的方法，其中一个模型在训练过程中会决定不预测数据点，当前模型不确定时。在测试时，数据会被减少，仅保留一部分（支持）测试数据，并且使用高精度预测。在金融预测设置下，这种方法可以降低风险，只有当 ensemble 模型确定时，才会进行决策。我们使用传统的 MLP 以及可微分决策树，在 sintetic 数据和实际金融市场数据上预测 fixes-term 回报，使用常用的特征。我们认为，我们的方法可能会带来更好的总回报，同时降低风险水平。为此，我们引入了一个实用指标，用于衡量每笔交易的均衡收益，以及对于降低风险的回报调整指标，两者均得到了显著提高。
</details></li>
</ul>
<hr>
<h2 id="Learning-and-Discovering-Quantum-Properties-with-Multi-Task-Neural-Networks"><a href="#Learning-and-Discovering-Quantum-Properties-with-Multi-Task-Neural-Networks" class="headerlink" title="Learning and Discovering Quantum Properties with Multi-Task Neural Networks"></a>Learning and Discovering Quantum Properties with Multi-Task Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11807">http://arxiv.org/abs/2310.11807</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ya-Dong Wu, Yan Zhu, Yuexuan Wang, Giulio Chiribella</li>
<li>for: 用深度神经网络预测量子态的性质从限制的测量数据中。</li>
<li>methods: 开发了一种网络模型，可同时预测多种量子性质，包括对量子态的期望值、非线性函数、如异Alignment和多体几何 invariants。</li>
<li>results: 发现一种模型可以通过多用途训练，不仅预测给定集合中的性质，还可以描述全局多体量子系统的性质从本地测量中。同时，模型还可以分类保护型态相对变化、发现不确定的界限。<details>
<summary>Abstract</summary>
Deep neural networks are a powerful tool for predicting properties of quantum states from limited measurement data. Here we develop a network model that can simultaneously predict multiple quantum properties, including not only expectation values of quantum observables, but also general nonlinear functions of the quantum state, like entanglement entropies and many-body topological invariants. Remarkably, we find that a model trained on a given set of properties can also discover new properties outside that set. Multi-purpose training also enables the model to infer global properties of many-body quantum systems from local measurements, to classify symmetry protected topological phases of matter, and to discover unknown boundaries between different phases.
</details>
<details>
<summary>摘要</summary>
深度神经网络是一种 poderous 工具，可以预测量子状态的性质从有限的测量数据中。在这里，我们开发了一种网络模型，可以同时预测多种量子性质，包括不只是量子观测器的期望值，还有一些泛函数，如量子状态的异步率和多体几何抽象。很意外地，我们发现，一个基于给定的性质集合来训练的模型，可以同时揭示未知的性质集合。多用途培训也使得模型可以从地方测量数据中推断全局的多体量子系统的性质，分类保护 topological phases of matter，并发现未知的阶段边界。
</details></li>
</ul>
<hr>
<h2 id="Auction-Based-Scheduling"><a href="#Auction-Based-Scheduling" class="headerlink" title="Auction-Based Scheduling"></a>Auction-Based Scheduling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11798">http://arxiv.org/abs/2310.11798</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Abarjag/Abarja">https://github.com/Abarjag/Abarja</a></li>
<li>paper_authors: Guy Avni, Kaushik Mallik, Suman Sadhukhan</li>
<li>for: 这种paper是为了解决多个、部分矛盾的决策任务而写的。</li>
<li>methods: 这种方法使用了拍卖机制来解决多个目标的决策问题。每个目标都有一个分立的策略，可以独立创建、修改和替换。</li>
<li>results: 这种方法可以在不同的环境下实现长期公平的决策，并且可以解决多个目标之间的冲突。在路径规划问题上，这种方法可以synthesize一对策略和它们的初始分配的预算，以及拍卖策略。<details>
<summary>Abstract</summary>
Many sequential decision-making tasks require satisfaction of multiple, partially contradictory objectives. Existing approaches are monolithic, namely all objectives are fulfilled using a single policy, which is a function that selects a sequence of actions. We present auction-based scheduling, a modular framework for multi-objective decision-making problems. Each objective is fulfilled using a separate policy, and the policies can be independently created, modified, and replaced. Understandably, different policies with conflicting goals may choose conflicting actions at a given time. In order to resolve conflicts, and compose policies, we employ a novel auction-based mechanism. We allocate a bounded budget to each policy, and at each step, the policies simultaneously bid from their available budgets for the privilege of being scheduled and choosing an action. Policies express their scheduling urgency using their bids and the bounded budgets ensure long-run scheduling fairness. We lay the foundations of auction-based scheduling using path planning problems on finite graphs with two temporal objectives. We present decentralized algorithms to synthesize a pair of policies, their initially allocated budgets, and bidding strategies. We consider three categories of decentralized synthesis problems, parameterized by the assumptions that the policies make on each other: (a) strong synthesis, with no assumptions and strongest guarantees, (b) assume-admissible synthesis, with weakest rationality assumptions, and (c) assume-guarantee synthesis, with explicit contract-based assumptions. For reachability objectives, we show that, surprisingly, decentralized assume-admissible synthesis is always possible when the out-degrees of all vertices are at most two.
</details>
<details>
<summary>摘要</summary>
许多顺序决策任务需满足多个、部分矛盾的目标。现有的方法都是单一的，即所有目标都是通过单一策略（一个函数选择一系列动作）来满足。我们介绍了拍卖机制来解决这类决策问题。在这种机制下，每个目标都是通过一个分离的策略来满足，这些策略可以独立创建、修改和替换。当不同的策略有冲突目标时，我们使用一种新的拍卖机制来解决冲突。我们为每个策略分配一个固定预算，并在每步中让各策略同时从其可用预算中竞拍为执行动作的权利。策略通过竞拍价格表达其排期优先级，并且固定预算确保长期排期公平。我们在路径规划问题上建立了拍卖机制的基础，并提出了三种分类的分解问题：强化合理化（strong synthesis）、弱合理化（assume-admissible synthesis）和合理合同（assume-guarantee synthesis）。对于可达性目标，我们发现了一个意外的结论：在所有顶点出度都不大于2时，分解问题总是可能的。
</details></li>
</ul>
<hr>
<h2 id="Solving-the-multiplication-problem-of-a-large-language-model-system-using-a-graph-based-method"><a href="#Solving-the-multiplication-problem-of-a-large-language-model-system-using-a-graph-based-method" class="headerlink" title="Solving the multiplication problem of a large language model system using a graph-based method"></a>Solving the multiplication problem of a large language model system using a graph-based method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13016">http://arxiv.org/abs/2310.13016</a></li>
<li>repo_url: None</li>
<li>paper_authors: Turker Tuncer, Sengul Dogan, Mehmet Baygin, Prabal Datta Barua, Abdul Hafeez-Baig, Ru-San Tan, Subrata Chakraborty, U. Rajendra Acharya</li>
<li>for: 解决 chatGPT 模型中的乘法问题，提高其数学运算精度。</li>
<li>methods: 基于图表结构的乘法算法，通过增加 10k 操作符来模拟人类数学运算。</li>
<li>results: 对 1,000,000 个大数乘法任务，提出了 100% 的准确率，成功解决了 GPT 模型中的乘法挑战。<details>
<summary>Abstract</summary>
The generative pre-trained transformer (GPT)-based chatbot software ChatGPT possesses excellent natural language processing capabilities but is inadequate for solving arithmetic problems, especially multiplication. Its GPT structure uses a computational graph for multiplication, which has limited accuracy beyond simple multiplication operations. We developed a graph-based multiplication algorithm that emulated human-like numerical operations by incorporating a 10k operator, where k represents the maximum power to base 10 of the larger of two input numbers. Our proposed algorithm attained 100% accuracy for 1,000,000 large number multiplication tasks, effectively solving the multiplication challenge of GPT-based and other large language models. Our work highlights the importance of blending simple human insights into the design of artificial intelligence algorithms. Keywords: Graph-based multiplication; ChatGPT; Multiplication problem
</details>
<details>
<summary>摘要</summary>
《基于Transformer（GPT）的对话机器人软件ChatGPT具有出色的自然语言处理能力，但对数学问题（尤其是乘法）的解决能力不足。GPT结构使用的计算图在多项式乘法操作上有限的准确性。我们开发了基于图的乘法算法，通过 incorporating a 10k操作符（其中k表示base 10最大幂），实现了人类化数学操作。我们的提议算法在100万大数乘法任务中达到100%的准确率，有效解决了GPT基于和其他大语言模型的乘法挑战。我们的工作强调了人工智能算法设计中的人类智慧的重要性。关键词：图基于乘法; ChatGPT; 乘法问题》Note: Please keep in mind that the translation is done by a machine and may not be perfect.
</details></li>
</ul>
<hr>
<h2 id="Telecom-AI-Native-Systems-in-the-Age-of-Generative-AI-–-An-Engineering-Perspective"><a href="#Telecom-AI-Native-Systems-in-the-Age-of-Generative-AI-–-An-Engineering-Perspective" class="headerlink" title="Telecom AI Native Systems in the Age of Generative AI – An Engineering Perspective"></a>Telecom AI Native Systems in the Age of Generative AI – An Engineering Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11770">http://arxiv.org/abs/2310.11770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ricardo Britto, Timothy Murphy, Massimo Iovene, Leif Jonsson, Melike Erol-Kantarci, Benedek Kovács</li>
<li>for: The paper explores the integration of foundational models (FMs) in the telecommunications industry, with a focus on the concept of “AI native telco” and the engineering considerations and challenges associated with implementing FMs in the software life cycle.</li>
<li>methods: The paper discusses the use of FMs in natural language processing tasks and content generation, and highlights the need for AI native-first approaches to fully leverage the potential of FMs in the telecom industry.</li>
<li>results: The paper emphasizes the enormous potential of FMs in revolutionizing how we interact with software products and services in the telecom industry, but also acknowledges the need for careful consideration of ethical, regulatory, and operational challenges to ensure the successful integration of FMs in mission-critical telecom contexts.<details>
<summary>Abstract</summary>
The rapid advancements in Artificial Intelligence (AI), particularly in generative AI and foundational models (FMs), have ushered in transformative changes across various industries. Large language models (LLMs), a type of FM, have demonstrated their prowess in natural language processing tasks and content generation, revolutionizing how we interact with software products and services. This article explores the integration of FMs in the telecommunications industry, shedding light on the concept of AI native telco, where AI is seamlessly woven into the fabric of telecom products. It delves into the engineering considerations and unique challenges associated with implementing FMs into the software life cycle, emphasizing the need for AI native-first approaches. Despite the enormous potential of FMs, ethical, regulatory, and operational challenges require careful consideration, especially in mission-critical telecom contexts. As the telecom industry seeks to harness the power of AI, a comprehensive understanding of these challenges is vital to thrive in a fiercely competitive market.
</details>
<details>
<summary>摘要</summary>
“人工智能（AI）的快速进步，特别是生成AI和基础模型（FM），已经在不同行业引入了transformative变革。大语言模型（LLM），一种基础模型，在自然语言处理任务和内容生成方面表现出色，改变了我们与软件产品和服务的交互方式。本文探讨了在电信行业中基础模型的整合，探讨了AI native telco这一概念，其中AI被融入了电信产品的тка料中。文章还讨论了在软件生命周期中实施基础模型的工程准则和特有挑战，强调了AI native-first的方法。虽然FM具有巨大的潜力，但是伦理、法规和运营上的挑战需要仔细考虑，特别在关键的电信上下文中。电信行业想要利用AI的力量，需要深入理解这些挑战，以在竞争激烈的市场中vivify。”
</details></li>
</ul>
<hr>
<h2 id="Stranger-Danger-Cross-Community-Interactions-with-Fringe-Users-Increase-the-Growth-of-Fringe-Communities-on-Reddit"><a href="#Stranger-Danger-Cross-Community-Interactions-with-Fringe-Users-Increase-the-Growth-of-Fringe-Communities-on-Reddit" class="headerlink" title="Stranger Danger! Cross-Community Interactions with Fringe Users Increase the Growth of Fringe Communities on Reddit"></a>Stranger Danger! Cross-Community Interactions with Fringe Users Increase the Growth of Fringe Communities on Reddit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12186">http://arxiv.org/abs/2310.12186</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giuseppe Russo, Manoel Horta Ribeiro, Robert West<br>for: 这些研究旨在解释具有偏见和极端思想的社区在主流平台上快速增长的机制。methods: 这些研究使用文本基因推断技术来研究具有偏见和极端思想的社区在Reddit上的增长。results: 研究发现，与偏见和极端思想相关的社区之间的交互可以吸引新成员加入这些社区。接受这些交互的用户比相似的匹配用户更有4.2%的可能性加入偏见和极端思想社区。这种效应受到社区特点（如左右两派社区）和交互语言的影响。使用恶意语言进行交互可以增加加入偏见和极端思想社区的可能性，比非恶意交互高5pp。对于非偏见和极端思想社区（如r&#x2F;climatechange、r&#x2F;NBA、r&#x2F;leagueoflegends）进行重复分析，未发现这种增长机制。总的来说，我们的发现表明，减少偏见和极端思想社区之间的交互可以减少主流平台上的偏见和极端思想社区的增长。<details>
<summary>Abstract</summary>
Fringe communities promoting conspiracy theories and extremist ideologies have thrived on mainstream platforms, raising questions about the mechanisms driving their growth. Here, we hypothesize and study a possible mechanism: new members may be recruited through fringe-interactions: the exchange of comments between members and non-members of fringe communities. We apply text-based causal inference techniques to study the impact of fringe-interactions on the growth of three prominent fringe communities on Reddit: r/Incel, r/GenderCritical, and r/The_Donald. Our results indicate that fringe-interactions attract new members to fringe communities. Users who receive these interactions are up to 4.2 percentage points (pp) more likely to join fringe communities than similar, matched users who do not.   This effect is influenced by 1) the characteristics of communities where the interaction happens (e.g., left vs. right-leaning communities) and 2) the language used in the interactions. Interactions using toxic language have a 5pp higher chance of attracting newcomers to fringe communities than non-toxic interactions. We find no effect when repeating this analysis by replacing fringe (r/Incel, r/GenderCritical, and r/The_Donald) with non-fringe communities (r/climatechange, r/NBA, r/leagueoflegends), suggesting this growth mechanism is specific to fringe communities. Overall, our findings suggest that curtailing fringe-interactions may reduce the growth of fringe communities on mainstream platforms.
</details>
<details>
<summary>摘要</summary>
极端社区促进阴谋理论和极端思想的发展，在主流平台上蓬勃发展，这引发了关于这些机制的问题。我们提出和研究一种可能的机制：新成员可能通过极端互动被招募到极端社区中。我们使用文本基因ferrer inference技术来研究极端互动对Reddit上三个 prominent fringe community的发展产生的影响：r/Incel、r/GenderCritical和r/The_Donald。我们的结果表明，极端互动会吸引新成员加入极端社区。接受这些互动的用户比相似的匹配用户更有4.2%的可能性加入极端社区。这种效应受到社区的特点（如左右翼社区）以及互动语言的影响。使用恶势力语言进行互动可能使新成员加入极端社区的可能性提高5pp。当我们将极端社区换为非极端社区（如r/climatechange、r/NBA、r/leagueoflegends）进行重复分析时，我们没有发现这种效应，这表明这种生长机制特有于极端社区。总的来说，我们的发现表明，遏制极端互动可能会降低主流平台上极端社区的发展。
</details></li>
</ul>
<hr>
<h2 id="Estimating-Material-Properties-of-Interacting-Objects-Using-Sum-GP-UCB"><a href="#Estimating-Material-Properties-of-Interacting-Objects-Using-Sum-GP-UCB" class="headerlink" title="Estimating Material Properties of Interacting Objects Using Sum-GP-UCB"></a>Estimating Material Properties of Interacting Objects Using Sum-GP-UCB</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11749">http://arxiv.org/abs/2310.11749</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. Yunus Seker, Oliver Kroemer</li>
<li>for: 估算物体的物理和动态属性从观察数据中</li>
<li>methods: bayesian优化方法确定物体参数</li>
<li>results: 能够有效地进行逐步学习，不需要重新评估已有观察数据的奖励值<details>
<summary>Abstract</summary>
Robots need to estimate the material and dynamic properties of objects from observations in order to simulate them accurately. We present a Bayesian optimization approach to identifying the material property parameters of objects based on a set of observations. Our focus is on estimating these properties based on observations of scenes with different sets of interacting objects. We propose an approach that exploits the structure of the reward function by modeling the reward for each observation separately and using only the parameters of the objects in that scene as inputs. The resulting lower-dimensional models generalize better over the parameter space, which in turn results in a faster optimization. To speed up the optimization process further, and reduce the number of simulation runs needed to find good parameter values, we also propose partial evaluations of the reward function, wherein the selected parameters are only evaluated on a subset of real world evaluations. The approach was successfully evaluated on a set of scenes with a wide range of object interactions, and we showed that our method can effectively perform incremental learning without resetting the rewards of the gathered observations.
</details>
<details>
<summary>摘要</summary>
Robots需要估算物体的物理和动态性质从观察数据中，以便准确模拟。我们提出了一种 bayesian优化方法，用于根据观察数据中的物体参数进行物体物理性质的估算。我们的注重点是基于不同交互对象的场景中的观察数据进行估算。我们提议利用奖励函数的结构，将奖励函数分割成每个观察中的奖励模型，并只使用场景中的对象参数作为输入。这将导致更好的维度减少，从而更快地优化。为了进一步加速优化过程，并减少需要进行实际评估的运行次数，我们还提议使用部分评估奖励函数，选择的参数只在一部分实际评估中进行评估。我们成功地应用了这种方法在一组具有多种对象交互的场景中，并证明了我们的方法可以进行逐步学习而不需要重置观察得到的奖励。
</details></li>
</ul>
<hr>
<h2 id="Action-Quantized-Offline-Reinforcement-Learning-for-Robotic-Skill-Learning"><a href="#Action-Quantized-Offline-Reinforcement-Learning-for-Robotic-Skill-Learning" class="headerlink" title="Action-Quantized Offline Reinforcement Learning for Robotic Skill Learning"></a>Action-Quantized Offline Reinforcement Learning for Robotic Skill Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11731">http://arxiv.org/abs/2310.11731</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianlan Luo, Perry Dong, Jeffrey Wu, Aviral Kumar, Xinyang Geng, Sergey Levine</li>
<li>for: 这篇论文旨在提出一种适应量化动作的策略，以提高在离线学习中的RL性能。</li>
<li>methods: 该论文提出了一种基于VQ-VAE的状态决定动作量化方法，以避免粗略量化所导致的极大幂灵活增长。</li>
<li>results: 该论文在多个离线RL方法，如IQL、CQL和BRAC等方法的基础上，提出了一种适应量化的策略，并在Robomimic环境中进行了详细的实验 validate。结果显示，与离线RL方法相比，该策略可以提高策略性能 by 2-3倍。<details>
<summary>Abstract</summary>
The offline reinforcement learning (RL) paradigm provides a general recipe to convert static behavior datasets into policies that can perform better than the policy that collected the data. While policy constraints, conservatism, and other methods for mitigating distributional shifts have made offline reinforcement learning more effective, the continuous action setting often necessitates various approximations for applying these techniques. Many of these challenges are greatly alleviated in discrete action settings, where offline RL constraints and regularizers can often be computed more precisely or even exactly. In this paper, we propose an adaptive scheme for action quantization. We use a VQ-VAE to learn state-conditioned action quantization, avoiding the exponential blowup that comes with na\"ive discretization of the action space. We show that several state-of-the-art offline RL methods such as IQL, CQL, and BRAC improve in performance on benchmarks when combined with our proposed discretization scheme. We further validate our approach on a set of challenging long-horizon complex robotic manipulation tasks in the Robomimic environment, where our discretized offline RL algorithms are able to improve upon their continuous counterparts by 2-3x. Our project page is at https://saqrl.github.io/
</details>
<details>
<summary>摘要</summary>
“偏离线强化学习（RL）模式提供了一个通用的方法，将静止行为数据转换成可以更好地性能的策略。虽然政策限制、保守主义和其他避免分布Shift的方法有助于减轻偏离线RL的挑战，但是连续动作设置frequently需要一些近似方法。在离散动作设置中，偏离线RL约束和规范可以经常更加精确地计算或者甚至是 exactly。在这篇论文中，我们提议了一种可变的行动量化方案。我们使用了VQ-VAE来学习状态决定的行动量化，以避免由粗粒化所带来的极大增长。我们证明了一些state-of-the-art的偏离线RL方法，如IQL、CQL和BRAC，在与我们提议的量化方案结合后，在标准准确度上提高了性能。我们进一步验证了我们的方法在Robomimic环境中的一组复杂的长期机械抓取任务上，我们的量化的偏离线RL算法能够超过其连续 counterpart的性能，提高了2-3倍。我们的项目页面是https://saqrl.github.io/”
</details></li>
</ul>
<hr>
<h2 id="Federated-Heterogeneous-Graph-Neural-Network-for-Privacy-preserving-Recommendation"><a href="#Federated-Heterogeneous-Graph-Neural-Network-for-Privacy-preserving-Recommendation" class="headerlink" title="Federated Heterogeneous Graph Neural Network for Privacy-preserving Recommendation"></a>Federated Heterogeneous Graph Neural Network for Privacy-preserving Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11730">http://arxiv.org/abs/2310.11730</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bo Yan, Yang Cao, Haoyu Wang, Wenchuan Yang, Junping Du, Chuan Shi</li>
<li>For: This paper proposes a federated heterogeneous graph neural network (FedHGNN) based framework for recommendation, which can collaboratively train a recommendation model on distributed Heterogeneous Information Networks (HINs) without leaking user privacy.* Methods: The paper formalizes a privacy definition based on differential privacy for HIN-based federated recommendation, and elaborately designs a semantic-preserving user interactions publishing method to recover the broken meta-path based semantics caused by distributed data storage.* Results: The proposed FedHGNN model outperforms existing methods by a large margin (up to 34% in HR@10 and 42% in NDCG@10) under an acceptable privacy budget, as demonstrated through extensive experiments on three datasets.<details>
<summary>Abstract</summary>
Heterogeneous information network (HIN), which contains rich semantics depicted by meta-paths, has become a powerful tool to alleviate data sparsity in recommender systems. Existing HIN-based recommendations hold the data centralized storage assumption and conduct centralized model training. However, the real-world data is often stored in a distributed manner for privacy concerns, resulting in the failure of centralized HIN-based recommendations. In this paper, we suggest the HIN is partitioned into private HINs stored in the client side and shared HINs in the server. Following this setting, we propose a federated heterogeneous graph neural network (FedHGNN) based framework, which can collaboratively train a recommendation model on distributed HINs without leaking user privacy. Specifically, we first formalize the privacy definition in the light of differential privacy for HIN-based federated recommendation, which aims to protect user-item interactions of private HIN as well as user's high-order patterns from shared HINs. To recover the broken meta-path based semantics caused by distributed data storage and satisfy the proposed privacy, we elaborately design a semantic-preserving user interactions publishing method, which locally perturbs user's high-order patterns as well as related user-item interactions for publishing. After that, we propose a HGNN model for recommendation, which conducts node- and semantic-level aggregations to capture recovered semantics. Extensive experiments on three datasets demonstrate our model outperforms existing methods by a large margin (up to 34% in HR@10 and 42% in NDCG@10) under an acceptable privacy budget.
</details>
<details>
<summary>摘要</summary>
众所周知的异种信息网络（HIN）已成为推荐系统中强大的工具，它可以通过meta-paths中嵌入的 semantics来解决数据稀缺问题。然而，现实中的数据通常会被分布式存储，这导致了传统的中央化HIN-based推荐模型的失败。在这篇论文中，我们提议将HIN partitioned into private HINs stored on the client side and shared HINs on the server。基于这种设定，我们提出了一种联邦异种图神经网络（FedHGNN）基础架构，可以在分布式HINs上并发训练一个推荐模型，无需透露用户隐私。具体来说，我们首先定义了隐私定义，以保护用户-项交互的隐私以及用户高阶征分的信息。然后，我们采用了一种semantic-preserving用户互动发布方法，通过地方扰动用户的高阶征分和相关用户-项交互来发布。接着，我们提出了一种HGNN模型，通过节点和semantic-level汇聚来捕捉恢复的 semantics。我们对三个数据集进行了广泛的实验，结果显示，我们的模型在遵守隐私预算下，可以大幅提高推荐效果（最多34%的HR@10和42%的NDCG@10）。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-in-Automated-Ontology-Matching-Lessons-Learned-from-an-Empirical-Experimentation"><a href="#Uncertainty-in-Automated-Ontology-Matching-Lessons-Learned-from-an-Empirical-Experimentation" class="headerlink" title="Uncertainty in Automated Ontology Matching: Lessons Learned from an Empirical Experimentation"></a>Uncertainty in Automated Ontology Matching: Lessons Learned from an Empirical Experimentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11723">http://arxiv.org/abs/2310.11723</a></li>
<li>repo_url: None</li>
<li>paper_authors: Inès Osman, Salvatore F. Pileggi, Sadok Ben Yahia</li>
<li>for: 本研究旨在探讨自动对照 ontology 的应用，以提高数据集合的相互连接和semantic integrability。</li>
<li>methods: 本研究采用了基于 ontology 的底层知识建构方法，并在实际数据上进行了实验。</li>
<li>results: 实验结果表明，自动对照过程中存在较大的不确定性，而 semi-supervised 方法则显示出了更好的可靠性。<details>
<summary>Abstract</summary>
Data integration is considered a classic research field and a pressing need within the information science community. Ontologies play a critical role in such a process by providing well-consolidated support to link and semantically integrate datasets via interoperability. This paper approaches data integration from an application perspective, looking at techniques based on ontology matching. An ontology-based process may only be considered adequate by assuming manual matching of different sources of information. However, since the approach becomes unrealistic once the system scales up, automation of the matching process becomes a compelling need. Therefore, we have conducted experiments on actual data with the support of existing tools for automatic ontology matching from the scientific community. Even considering a relatively simple case study (i.e., the spatio-temporal alignment of global indicators), outcomes clearly show significant uncertainty resulting from errors and inaccuracies along the automated matching process. More concretely, this paper aims to test on real-world data a bottom-up knowledge-building approach, discuss the lessons learned from the experimental results of the case study, and draw conclusions about uncertainty and uncertainty management in an automated ontology matching process. While the most common evaluation metrics clearly demonstrate the unreliability of fully automated matching solutions, properly designed semi-supervised approaches seem to be mature for a more generalized application.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>数据集成被视为信息科学领域的经典研究领域和紧迫需求。 ontology 在这种过程中扮演了关键的支持角色，以链接和semantic集成数据。本文从应用角度出发，研究基于 ontology 匹配技术的数据集成方法。然而，由于系统规模增加， manual 匹配过程变得不现实。因此，我们在实际数据支持下进行了现有的自动 ontology 匹配工具的实验。尽管使用简单的case study（即全球指标的空间-时间Alignment），实验结果显示了自动匹配过程中的显著不确定性。本文的目标是在真实数据上测试底层知识建构方法，讨论实验结果中的教训，并对自动匹配过程中的不确定性和不确定性管理进行结论。尽管常见的评价指标显示全自动匹配解决方案的不可靠性，但是正确设计的半监督方法似乎已经成熟备用更广泛的应用。
</details></li>
</ul>
<hr>
<h2 id="Quantify-Health-Related-Atomic-Knowledge-in-Chinese-Medical-Large-Language-Models-A-Computational-Analysis"><a href="#Quantify-Health-Related-Atomic-Knowledge-in-Chinese-Medical-Large-Language-Models-A-Computational-Analysis" class="headerlink" title="Quantify Health-Related Atomic Knowledge in Chinese Medical Large Language Models: A Computational Analysis"></a>Quantify Health-Related Atomic Knowledge in Chinese Medical Large Language Models: A Computational Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11722">http://arxiv.org/abs/2310.11722</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yaxin Fan, Feng Jiang, Peifeng Li, Haizhou Li</li>
<li>for: This paper aims to evaluate the ability of large language models (LLMs) to provide accurate and factual suggestions for user self-diagnosis queries.</li>
<li>methods: The authors constructed a benchmark of common atomic knowledge in user self-diagnosis queries and evaluated both generic and specialized LLMs on this benchmark. They also performed error analysis and explored different types of data for fine-tuning specialized LLMs.</li>
<li>results: The results showed that generic LLMs perform better than specialized LLMs in terms of atomic knowledge and instruction-following ability, and that distilled data can benefit LLMs most. Additionally, the authors found that both generic and specialized LLMs are sycophantic, meaning they tend to cater to users’ claims when it comes to unknown knowledge.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have the potential to revolutionize the way users self-diagnose through search engines by offering direct and efficient suggestions. Recent studies primarily focused on the quality of LLMs evaluated by GPT-4 or their ability to pass medical exams, no studies have quantified the extent of health-related atomic knowledge stored in LLMs' memory, which is the basis of LLMs to provide more factual suggestions. In this paper, we first constructed a benchmark, including the most common types of atomic knowledge in user self-diagnosis queries, with 17 atomic types and a total of 14, 048 pieces of atomic knowledge. Then, we evaluated both generic and specialized LLMs on the benchmark. The experimental results showcased that generic LLMs perform better than specialized LLMs in terms of atomic knowledge and instruction-following ability. Error analysis revealed that both generic and specialized LLMs are sycophantic, e.g., always catering to users' claims when it comes to unknown knowledge. Besides, generic LLMs showed stronger safety, which can be learned by specialized LLMs through distilled data. We further explored different types of data commonly adopted for fine-tuning specialized LLMs, i.e., real-world, semi-distilled, and distilled data, and found that distilled data can benefit LLMs most.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）有可能革命化用户自诊查找结果，提供直接和有效的建议。现在的研究主要集中在GPT-4评估了LMMs的质量或者LMMs能否通过医学考试，但是没有评估LMMs储存的健康相关知识量，这是LMMs提供更加正确的建议的基础。在这篇论文中，我们首先建立了一个benchmark，包括用户自诊查找常见的17种原子知识，总共14,048个原子知识。然后，我们评估了一般和特殊的LMMs在benchmark上。实验结果显示，一般LMMs在原子知识和指令遵循能力方面表现比特殊LMMs更好。错误分析表明，一般和特殊LMMs都具有追求用户的倾听，即当用户提出未知知识时，LMMs都会尽力适应。此外，一般LMMs表现出更强的安全性，可以通过特殊LMMs的滤过资料学习。我们进一步探索了不同类型的特殊LMMs fine-tuning的常用数据，包括实际世界、半滤过和滤过数据，发现滤过数据可以帮助LMMs最多。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Low-resource-Fine-grained-Named-Entity-Recognition-by-Leveraging-Coarse-grained-Datasets"><a href="#Enhancing-Low-resource-Fine-grained-Named-Entity-Recognition-by-Leveraging-Coarse-grained-Datasets" class="headerlink" title="Enhancing Low-resource Fine-grained Named Entity Recognition by Leveraging Coarse-grained Datasets"></a>Enhancing Low-resource Fine-grained Named Entity Recognition by Leveraging Coarse-grained Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11715">http://arxiv.org/abs/2310.11715</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sue991/cofiner">https://github.com/sue991/cofiner</a></li>
<li>paper_authors: Su Ah Lee, Seokjin Oh, Woohwan Jung</li>
<li>for: 本文提出了一种解决Named Entity Recognition（NER）缺乏精细标注数据的问题，特别是在细化NER场景下。</li>
<li>methods: 本文使用了现有的粗化标注数据，并提出了一种叫做Fine-to-Coarse（F2C）映射矩阵来利用粗化和细化实体之间的层次结构。此外，本文还提出了一种矛盾检测方法，以避免粗化实体与细化实体之间的矛盾。</li>
<li>results: 实验结果表明，我们的方法在只有少量细化标注时比$K$-shot学习和监督学习方法表现更好。<details>
<summary>Abstract</summary>
Named Entity Recognition (NER) frequently suffers from the problem of insufficient labeled data, particularly in fine-grained NER scenarios. Although $K$-shot learning techniques can be applied, their performance tends to saturate when the number of annotations exceeds several tens of labels. To overcome this problem, we utilize existing coarse-grained datasets that offer a large number of annotations. A straightforward approach to address this problem is pre-finetuning, which employs coarse-grained data for representation learning. However, it cannot directly utilize the relationships between fine-grained and coarse-grained entities, although a fine-grained entity type is likely to be a subcategory of a coarse-grained entity type. We propose a fine-grained NER model with a Fine-to-Coarse(F2C) mapping matrix to leverage the hierarchical structure explicitly. In addition, we present an inconsistency filtering method to eliminate coarse-grained entities that are inconsistent with fine-grained entity types to avoid performance degradation. Our experimental results show that our method outperforms both $K$-shot learning and supervised learning methods when dealing with a small number of fine-grained annotations.
</details>
<details>
<summary>摘要</summary>
翻译结果：Named Entity Recognition (NER)  часто遇到缺乏标签数据的问题，特别在细化NER场景下。虽可以使用$K$-shot学习技术，但其表现往往停滞在数十个标签以上。为解决这问题，我们利用现有的粗化数据，它们提供了大量的标签。一种直接 Addressing this problem is pre-finetuning, which uses coarse-grained data for representation learning. However, it cannot directly utilize the relationships between fine-grained and coarse-grained entities, although a fine-grained entity type is likely to be a subcategory of a coarse-grained entity type. We propose a fine-grained NER model with a Fine-to-Coarse(F2C) mapping matrix to leverage the hierarchical structure explicitly. In addition, we present an inconsistency filtering method to eliminate coarse-grained entities that are inconsistent with fine-grained entity types to avoid performance degradation. Our experimental results show that our method outperforms both $K$-shot learning and supervised learning methods when dealing with a small number of fine-grained annotations.
</details></li>
</ul>
<hr>
<h2 id="Learning-Co-Speech-Gesture-for-Multimodal-Aphasia-Type-Detection"><a href="#Learning-Co-Speech-Gesture-for-Multimodal-Aphasia-Type-Detection" class="headerlink" title="Learning Co-Speech Gesture for Multimodal Aphasia Type Detection"></a>Learning Co-Speech Gesture for Multimodal Aphasia Type Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11710">http://arxiv.org/abs/2310.11710</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dsail-skku/multimodal-aphasia-type-detection_emnlp_2023">https://github.com/dsail-skku/multimodal-aphasia-type-detection_emnlp_2023</a></li>
<li>paper_authors: Daeun Lee, Sejung Son, Hyolim Jeon, Seungbae Kim, Jinyoung Han</li>
<li>for: 这种研究旨在为抑制语言障碍的病人提供有效的诊断方法，具体来说是用多模态图像神经网络来识别不同的语言障碍类型。</li>
<li>methods: 该研究使用了多模态图像神经网络，通过学习语音和手势模式之间的相关性，生成敏感于手势信息的文本表示，从而准确地识别不同的语言障碍类型。</li>
<li>results: 对比 exist 方法，该研究实现了状态机器人的Result（F1 84.2%），并显示了手势特征的优越性， highlighting the significance of gesture expression in detecting aphasia types。<details>
<summary>Abstract</summary>
Aphasia, a language disorder resulting from brain damage, requires accurate identification of specific aphasia types, such as Broca's and Wernicke's aphasia, for effective treatment. However, little attention has been paid to developing methods to detect different types of aphasia. Recognizing the importance of analyzing co-speech gestures for distinguish aphasia types, we propose a multimodal graph neural network for aphasia type detection using speech and corresponding gesture patterns. By learning the correlation between the speech and gesture modalities for each aphasia type, our model can generate textual representations sensitive to gesture information, leading to accurate aphasia type detection. Extensive experiments demonstrate the superiority of our approach over existing methods, achieving state-of-the-art results (F1 84.2\%). We also show that gesture features outperform acoustic features, highlighting the significance of gesture expression in detecting aphasia types. We provide the codes for reproducibility purposes.
</details>
<details>
<summary>摘要</summary>
apraxia, a language disorder caused by brain damage, requires accurate identification of specific apraxia types, such as Broca's and Wernicke's apraxia, for effective treatment. However, little attention has been paid to developing methods to detect different types of apraxia. Recognizing the importance of analyzing co-speech gestures for distinguishing apraxia types, we propose a multimodal graph neural network for apraxia type detection using speech and corresponding gesture patterns. By learning the correlation between the speech and gesture modalities for each apraxia type, our model can generate textual representations sensitive to gesture information, leading to accurate apraxia type detection. Extensive experiments demonstrate the superiority of our approach over existing methods, achieving state-of-the-art results (F1 84.2\%). We also show that gesture features outperform acoustic features, highlighting the significance of gesture expression in detecting apraxia types. We provide the codes for reproducibility purposes.Note: "apraxia" is the traditional Chinese term for aphasia, and "apraxia type" is the Chinese term for aphasia type.
</details></li>
</ul>
<hr>
<h2 id="Live-Graph-Lab-Towards-Open-Dynamic-and-Real-Transaction-Graphs-with-NFT"><a href="#Live-Graph-Lab-Towards-Open-Dynamic-and-Real-Transaction-Graphs-with-NFT" class="headerlink" title="Live Graph Lab: Towards Open, Dynamic and Real Transaction Graphs with NFT"></a>Live Graph Lab: Towards Open, Dynamic and Real Transaction Graphs with NFT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11709">http://arxiv.org/abs/2310.11709</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhen Zhang, Bingqiao Luo, Shengliang Lu, Bingsheng He</li>
<li>for:  This paper is written for investigating the properties of the Non-fungible tokens (NFTs) ecosystem from a temporal graph analysis perspective.</li>
<li>methods:  The paper uses a live graph with NFT transaction network, which is obtained by downloading and parsing the NFT transaction activities. The authors also use a series of measurements to understand the properties of the NFT ecosystem and compare it with social, citation, and web networks.</li>
<li>results:  The paper provides new observations and insights into the characteristics of the emerging NFT ecosystem, including its dynamics and properties. The authors also study machine learning models in this live graph to enrich the current datasets and provide new opportunities for the graph community.<details>
<summary>Abstract</summary>
Numerous studies have been conducted to investigate the properties of large-scale temporal graphs. Despite the ubiquity of these graphs in real-world scenarios, it's usually impractical for us to obtain the whole real-time graphs due to privacy concerns and technical limitations. In this paper, we introduce the concept of {\it Live Graph Lab} for temporal graphs, which enables open, dynamic and real transaction graphs from blockchains. Among them, Non-fungible tokens (NFTs) have become one of the most prominent parts of blockchain over the past several years. With more than \$40 billion market capitalization, this decentralized ecosystem produces massive, anonymous and real transaction activities, which naturally forms a complicated transaction network. However, there is limited understanding about the characteristics of this emerging NFT ecosystem from a temporal graph analysis perspective. To mitigate this gap, we instantiate a live graph with NFT transaction network and investigate its dynamics to provide new observations and insights. Specifically, through downloading and parsing the NFT transaction activities, we obtain a temporal graph with more than 4.5 million nodes and 124 million edges. Then, a series of measurements are presented to understand the properties of the NFT ecosystem. Through comparisons with social, citation, and web networks, our analyses give intriguing findings and point out potential directions for future exploration. Finally, we also study machine learning models in this live graph to enrich the current datasets and provide new opportunities for the graph community. The source codes and dataset are available at https://livegraphlab.github.io.
</details>
<details>
<summary>摘要</summary>
多个研究已经进行了大规模时间图的性质调查。尽管这些图在实际场景中很常见，但由于隐私问题和技术限制，我们通常无法获取实时图。在这篇论文中，我们介绍了一种名为{\it Live Graph Lab}的概念，该概念可以在区块链上提供开放、动态和实时交易图。其中，非 fungible tokens（NFTs）在过去几年中成为了区块链的一个最 prominent的部分。NFTs的市场规模超过400亿美元，这个分布式生态系统会生成大量、匿名和实时交易活动，自然形成了复杂的交易网络。然而，关于这个emerging NFT生态系统从时间图分析的特点还有很少的理解。为了减少这一差距，我们在实时图中实例化了NFT交易网络，并investigated its dynamics，以提供新的观察和发现。具体来说，通过下载和解析NFT交易活动，我们获得了一个时间图，包含超过450万个节点和124亿个边。然后，我们进行了一系列测量，以了解NFT生态系统的特点。通过与社交、引用和网络相比较，我们的分析发现了有趣的发现，并指出了未来的探索方向。此外，我们还研究了在这个实时图上的机器学习模型，以激励当前的数据集和提供新的探索机会。实时图和数据集的源代码可以在https://livegraphlab.github.io/ obtained。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Survey-on-Vector-Database-Storage-and-Retrieval-Technique-Challenge"><a href="#A-Comprehensive-Survey-on-Vector-Database-Storage-and-Retrieval-Technique-Challenge" class="headerlink" title="A Comprehensive Survey on Vector Database: Storage and Retrieval Technique, Challenge"></a>A Comprehensive Survey on Vector Database: Storage and Retrieval Technique, Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11703">http://arxiv.org/abs/2310.11703</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yikun Han, Chunjiang Liu, Pengfei Wang</li>
<li>for: 本文旨在为Vector数据库的高维数据存储提供一个概述，以及相关的近似搜索算法的概述。</li>
<li>methods: 本文使用分类法概述了现有的Vector数据库架构，并对近似搜索问题的解决方法进行了分类，包括 hash-based、tree-based、graph-based 和 quantization-based 等方法。</li>
<li>results: 本文提供了现有Vector数据库的挑战和大语言模型的组合，以及它们在新的可能性领域中的应用。<details>
<summary>Abstract</summary>
A vector database is used to store high-dimensional data that cannot be characterized by traditional DBMS. Although there are not many articles describing existing or introducing new vector database architectures, the approximate nearest neighbor search problem behind vector databases has been studied for a long time, and considerable related algorithmic articles can be found in the literature. This article attempts to comprehensively review relevant algorithms to provide a general understanding of this booming research area. The basis of our framework categorises these studies by the approach of solving ANNS problem, respectively hash-based, tree-based, graph-based and quantization-based approaches. Then we present an overview of existing challenges for vector databases. Lastly, we sketch how vector databases can be combined with large language models and provide new possibilities.
</details>
<details>
<summary>摘要</summary>
vector database 是用于存储高维数据的数据库，而这些数据不能由传统的DBMS进行描述。虽然有很少的文章描述了现有或引入新的vector database架构，但近似最近邻居问题（ANNS）在vector databases中的研究已经很长时间了，相关的算法文章在 literatura 中可以找到。本文尝试从ategorize 这些研究，按照解决 ANNS 问题的方法分为hash-based、tree-based、graph-based和quantization-based四种方法。然后，我们介绍vector databases 存在的挑战，最后，我们探讨 vector databases 与大型自然语言模型如何结合，提供新的可能性。Note: "vector database" is a literal translation of the English phrase, and it is not a commonly used term in Chinese. In Chinese, the term "高维数据库" (gāo wěi dà kē) is more commonly used to refer to a database that stores high-dimensional data.
</details></li>
</ul>
<hr>
<h2 id="Runner-re-identification-from-single-view-video-in-the-open-world-setting"><a href="#Runner-re-identification-from-single-view-video-in-the-open-world-setting" class="headerlink" title="Runner re-identification from single-view video in the open-world setting"></a>Runner re-identification from single-view video in the open-world setting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11700">http://arxiv.org/abs/2310.11700</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tomohiro Suzuki, Kazushi Tsutsui, Kazuya Takeda, Keisuke Fujii</li>
<li>for: 这paper是为了解决多视图或单视图体育视频中运动员重新认识的问题，以便实现自动化视频分析。</li>
<li>methods: 该paper使用了预训练的YOLOv8和EfficientNet，以及自适应的gated recurrent unit autoencoder模型来自动处理单视图视频，并使用运动动态特征来提高重新认识精度。</li>
<li>results: 该paper在使用一个运动实践视频数据集上进行测试，并显示了与一种状态的艺术模型相比，该方法可以更高的准确率来重新认识运动员。此外，该paper还证明了自适应运动动态特征提取器的有效性。该runner重新认识系统可以用于自动分析运动视频。<details>
<summary>Abstract</summary>
In many sports, player re-identification is crucial for automatic video processing and analysis. However, most of the current studies on player re-identification in multi- or single-view sports videos focus on re-identification in the closed-world setting using labeled image dataset, and player re-identification in the open-world setting for automatic video analysis is not well developed. In this paper, we propose a runner re-identification system that directly processes single-view video to address the open-world setting. In the open-world setting, we cannot use labeled dataset and have to process video directly. The proposed system automatically processes raw video as input to identify runners, and it can identify runners even when they are framed out multiple times. For the automatic processing, we first detect the runners in the video using the pre-trained YOLOv8 and the fine-tuned EfficientNet. We then track the runners using ByteTrack and detect their shoes with the fine-tuned YOLOv8. Finally, we extract the image features of the runners using an unsupervised method using the gated recurrent unit autoencoder model. To improve the accuracy of runner re-identification, we use dynamic features of running sequence images. We evaluated the system on a running practice video dataset and showed that the proposed method identified runners with higher accuracy than one of the state-of-the-art models in unsupervised re-identification. We also showed that our unsupervised running dynamic feature extractor was effective for runner re-identification. Our runner re-identification system can be useful for the automatic analysis of running videos.
</details>
<details>
<summary>摘要</summary>
在多种运动中，玩家重新认定是自动视频处理和分析的关键。然而，当前大多数player重新认定在多视图或单视图运动视频中的研究都集中在关闭世界设定下使用标注图像集，而在开放世界设定下的自动视频分析中player重新认定还未得到充分开发。在这篇论文中，我们提出了一个runner重新认定系统，Directly处理单视图视频来解决开放世界设定。在开放世界设定下，我们不能使用标注集和处理视频 directly。提出的系统可以自动处理原始视频，并在多次框架外重新认定运动员。为了实现自动处理，我们首先在视频中检测运动员使用预训练的YOLOv8和精度调整的EfficientNet。然后，我们使用ByteTrack跟踪运动员，并使用精度调整的YOLOv8检测他们的鞋。最后，我们使用无监督方法使用闭环回归自适应模型提取运动员的图像特征。为了提高运动员重新认定的准确性，我们使用运动序列图像的动态特征。我们对一个跑步练习视频数据集进行评估，并显示了我们提出的方法可以在无监督下高度准确地重新认定运动员，并且我们的无监督跑动特征提取器是 runner重新认定中有效的。我们的runner重新认定系统可以对运动视频进行自动分析。
</details></li>
</ul>
<hr>
<h2 id="Architectural-Implications-of-GNN-Aggregation-Programming-Abstractions"><a href="#Architectural-Implications-of-GNN-Aggregation-Programming-Abstractions" class="headerlink" title="Architectural Implications of GNN Aggregation Programming Abstractions"></a>Architectural Implications of GNN Aggregation Programming Abstractions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12184">http://arxiv.org/abs/2310.12184</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yingjie Qi, Jianlei Yang, Ao Zhou, Tong Qiao, Chunming Hu</li>
<li>for: 本研究旨在对现有的图数据处理抽象进行全面的评估和分析，以便为未来的图神经网络加速提供依据。</li>
<li>methods: 本研究使用现有的图数据处理抽象，并对其进行了详细的 caracterization 研究，以确定哪些抽象更有效率。</li>
<li>results: 研究发现，使用不同的抽象方法可以得到不同的性能和效率。同时，对于某些特定的图数据处理任务，某些抽象方法可以显著提高性能。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have gained significant popularity due to the powerful capability to extract useful representations from graph data. As the need for efficient GNN computation intensifies, a variety of programming abstractions designed for optimizing GNN Aggregation have emerged to facilitate acceleration. However, there is no comprehensive evaluation and analysis upon existing abstractions, thus no clear consensus on which approach is better. In this letter, we classify existing programming abstractions for GNN Aggregation by the dimension of data organization and propagation method. By constructing these abstractions on a state-of-the-art GNN library, we perform a thorough and detailed characterization study to compare their performance and efficiency, and provide several insights on future GNN acceleration based on our analysis.
</details>
<details>
<summary>摘要</summary>
格raph神经网络（GNNs）已经吸引了广泛的关注，因为它们可以从图数据中提取有用的表示。随着GNN计算的需求越来越高，各种优化GNN聚合的编程封装出现了，以便加速。然而，现有的编程封装没有得到全面的评估和分析，因此没有明确的共识，哪个方法更好。在这封信中，我们将现有的GNN聚合编程封装分类为数据组织维度和传播方法。通过在当今的GNN库上构建这些封装，我们进行了详细的性能和效率Characterization研究，并提供了一些关于未来GNN加速的反思。
</details></li>
</ul>
<hr>
<h2 id="Quantum-Acceleration-of-Infinite-Horizon-Average-Reward-Reinforcement-Learning"><a href="#Quantum-Acceleration-of-Infinite-Horizon-Average-Reward-Reinforcement-Learning" class="headerlink" title="Quantum Acceleration of Infinite Horizon Average-Reward Reinforcement Learning"></a>Quantum Acceleration of Infinite Horizon Average-Reward Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11684">http://arxiv.org/abs/2310.11684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bhargav Ganguly, Vaneet Aggarwal</li>
<li>for: 这个论文探讨了量子加速器在解决无穷 horizon Markov Decision Processes（MDPs）中提高均衡奖励的可能性。</li>
<li>methods: 我们提出了一种创新的量子框架，让智能机器人在未知MDP中与其进行交互，从而超越传统交互模式。我们的方法基于一种以Optimism为驱动的表格学习算法，利用量子信号来估计量子平均值。</li>
<li>results: 我们通过了对论文的严格理论分析，证明量子估计优势导致量子算法在无穷Horizon Reinforcement学习中获得了极大的进步，具体来说，我们的量子算法可以实现$\tilde{\mathcal{O}(1)$的 regret bound，比 классические对手的$\tilde{\mathcal{O}(\sqrt{T})$ bound明显更高。<details>
<summary>Abstract</summary>
This paper investigates the potential of quantum acceleration in addressing infinite horizon Markov Decision Processes (MDPs) to enhance average reward outcomes. We introduce an innovative quantum framework for the agent's engagement with an unknown MDP, extending the conventional interaction paradigm. Our approach involves the design of an optimism-driven tabular Reinforcement Learning algorithm that harnesses quantum signals acquired by the agent through efficient quantum mean estimation techniques. Through thorough theoretical analysis, we demonstrate that the quantum advantage in mean estimation leads to exponential advancements in regret guarantees for infinite horizon Reinforcement Learning. Specifically, the proposed Quantum algorithm achieves a regret bound of $\tilde{\mathcal{O}(1)$, a significant improvement over the $\tilde{\mathcal{O}(\sqrt{T})$ bound exhibited by classical counterparts.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Descriptive-Knowledge-Graph-in-Biomedical-Domain"><a href="#Descriptive-Knowledge-Graph-in-Biomedical-Domain" class="headerlink" title="Descriptive Knowledge Graph in Biomedical Domain"></a>Descriptive Knowledge Graph in Biomedical Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11681">http://arxiv.org/abs/2310.11681</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kerui Zhu, Jie Huang, Kevin Chen-Chuan Chang</li>
<li>for: 该论文旨在提供一种自动抽取和生成有用和描述性句子的系统，以便有效地搜索生物医学知识。</li>
<li>methods: 该系统使用ChatGPT和一个精度调整的关系合成模型，自动生成有用和可靠的描述句子，从而减少了人类阅读努力。</li>
<li>results: 该系统可以帮助研究人员轻松地获得高级知识和详细参考，并且可以交互地循序搜索到有关的信息。在COVID-19研究中，该系统得到了广泛的应用，如药物重用和文献筛选。<details>
<summary>Abstract</summary>
We present a novel system that automatically extracts and generates informative and descriptive sentences from the biomedical corpus and facilitates the efficient search for relational knowledge. Unlike previous search engines or exploration systems that retrieve unconnected passages, our system organizes descriptive sentences as a relational graph, enabling researchers to explore closely related biomedical entities (e.g., diseases treated by a chemical) or indirectly connected entities (e.g., potential drugs for treating a disease). Our system also uses ChatGPT and a fine-tuned relation synthesis model to generate concise and reliable descriptive sentences from retrieved information, reducing the need for extensive human reading effort. With our system, researchers can easily obtain both high-level knowledge and detailed references and interactively steer to the information of interest. We spotlight the application of our system in COVID-19 research, illustrating its utility in areas such as drug repurposing and literature curation.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的系统，可以自动提取和生成有用和描述性的句子从生物医学词库，以便高效地搜索关系知识。与过去的搜索引擎或探索系统不同，我们的系统将描述句子组织成关系图，allowing researchers to explore closely related biomedical entities (例如，由化学物质治疗的疾病) or indirectly connected entities (例如，用于治疗疾病的潜在药物).我们的系统还使用ChatGPT和一种精心调整的关系合成模型，从检索到的信息中生成高度可靠和 concise的描述句子，从而减少了人类阅读努力。通过我们的系统，研究人员可以轻松地获得高级知识和详细参考，并且可以互动地导航到 interessant information。我们在COVID-19研究中强调了我们的系统的应用，例如药物重用和文献筛选。
</details></li>
</ul>
<hr>
<h2 id="Using-Experience-Classification-for-Training-Non-Markovian-Tasks"><a href="#Using-Experience-Classification-for-Training-Non-Markovian-Tasks" class="headerlink" title="Using Experience Classification for Training Non-Markovian Tasks"></a>Using Experience Classification for Training Non-Markovian Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11678">http://arxiv.org/abs/2310.11678</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruixuan Miao, Xu Lu, Cong Tian, Bin Yu, Zhenhua Duan</li>
<li>for: 解决实际任务中的非Markovian任务，即奖励不仅基于当前状态，而且基于状态历史。</li>
<li>methods: 提出一种新的强化学习方法，利用线性时间逻辑LTL$_f$编码到Markov决策过程中，以便利用先进的RL算法。</li>
<li>results: 通过在多个 benchmark 问题中实践，证明了我们的方法的可行性和效果。<details>
<summary>Abstract</summary>
Unlike the standard Reinforcement Learning (RL) model, many real-world tasks are non-Markovian, whose rewards are predicated on state history rather than solely on the current state. Solving a non-Markovian task, frequently applied in practical applications such as autonomous driving, financial trading, and medical diagnosis, can be quite challenging. We propose a novel RL approach to achieve non-Markovian rewards expressed in temporal logic LTL$_f$ (Linear Temporal Logic over Finite Traces). To this end, an encoding of linear complexity from LTL$_f$ into MDPs (Markov Decision Processes) is introduced to take advantage of advanced RL algorithms. Then, a prioritized experience replay technique based on the automata structure (semantics equivalent to LTL$_f$ specification) is utilized to improve the training process. We empirically evaluate several benchmark problems augmented with non-Markovian tasks to demonstrate the feasibility and effectiveness of our approach.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Improved-Sample-Complexity-Analysis-of-Natural-Policy-Gradient-Algorithm-with-General-Parameterization-for-Infinite-Horizon-Discounted-Reward-Markov-Decision-Processes"><a href="#Improved-Sample-Complexity-Analysis-of-Natural-Policy-Gradient-Algorithm-with-General-Parameterization-for-Infinite-Horizon-Discounted-Reward-Markov-Decision-Processes" class="headerlink" title="Improved Sample Complexity Analysis of Natural Policy Gradient Algorithm with General Parameterization for Infinite Horizon Discounted Reward Markov Decision Processes"></a>Improved Sample Complexity Analysis of Natural Policy Gradient Algorithm with General Parameterization for Infinite Horizon Discounted Reward Markov Decision Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11677">http://arxiv.org/abs/2310.11677</a></li>
<li>repo_url: None</li>
<li>paper_authors: Washim Uddin Mondal, Vaneet Aggarwal</li>
<li>for: 这个论文是关于设计高效采样学习算法的研究，特别是针对无穷 horizon 折扣奖励Markov决策过程。</li>
<li>methods: 这个算法使用加速的随机梯度下降过程来获得自然策略偏导。</li>
<li>results: 这个算法可以达到 $\mathcal{O}({\epsilon^{-2})$ 样本复杂度和 $\mathcal{O}(\epsilon^{-1})$ 迭代复杂度，比现状态艺术ifactoria 样本复杂度增加 $\log(\frac{1}{\epsilon})$ 因子。此外，这个算法不需要不可证明的假设，即IS重要性的方差是上界bounded。在Hessian-free和IS-free算法中，ANPG beat最佳样本复杂度的记录，同时与其他最佳迭代复杂度匹配。<details>
<summary>Abstract</summary>
We consider the problem of designing sample efficient learning algorithms for infinite horizon discounted reward Markov Decision Process. Specifically, we propose the Accelerated Natural Policy Gradient (ANPG) algorithm that utilizes an accelerated stochastic gradient descent process to obtain the natural policy gradient. ANPG achieves $\mathcal{O}({\epsilon^{-2})$ sample complexity and $\mathcal{O}(\epsilon^{-1})$ iteration complexity with general parameterization where $\epsilon$ defines the optimality error. This improves the state-of-the-art sample complexity by a $\log(\frac{1}{\epsilon})$ factor. ANPG is a first-order algorithm and unlike some existing literature, does not require the unverifiable assumption that the variance of importance sampling (IS) weights is upper bounded. In the class of Hessian-free and IS-free algorithms, ANPG beats the best-known sample complexity by a factor of $\mathcal{O}(\epsilon^{-\frac{1}{2})$ and simultaneously matches their state-of-the-art iteration complexity.
</details>
<details>
<summary>摘要</summary>
我们考虑无限 horizon 折抵质量评估 Markov Decision Process 的问题。我们提出了加速自然策略导数（ANPG）算法，它利用加速随机Gradient Descent 过程来获得自然策略导数。ANPG 实现了 $\mathcal{O}({\epsilon^{-2})$ 样本复杂性和 $\mathcal{O}(\epsilon^{-1})$ 迭代复杂性，其中 $\epsilon$ 定义了优化误差。这超过了现有的 state-of-the-art 样本复杂性中的 $\log(\frac{1}{\epsilon})$ 因子。ANPG 是一个首顺算法，不需要先前的文献中未能证明的不可靠的假设，即 importance sampling 的 variance 的Upper Bounded。在 Hessian-free 和 IS-free 数据中，ANPG 比最好的 known sample complexity 的factor $\mathcal{O}(\epsilon^{-\frac{1}{2})$ ，同时将其state-of-the-art迭代复杂性与最佳的 state-of-the-art 匹配。
</details></li>
</ul>
<hr>
<h2 id="PREM-A-Simple-Yet-Effective-Approach-for-Node-Level-Graph-Anomaly-Detection"><a href="#PREM-A-Simple-Yet-Effective-Approach-for-Node-Level-Graph-Anomaly-Detection" class="headerlink" title="PREM: A Simple Yet Effective Approach for Node-Level Graph Anomaly Detection"></a>PREM: A Simple Yet Effective Approach for Node-Level Graph Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11676">http://arxiv.org/abs/2310.11676</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/campanulabells/prem-gad">https://github.com/campanulabells/prem-gad</a></li>
<li>paper_authors: Junjun Pan, Yixin Liu, Yizhen Zheng, Shirui Pan</li>
<li>for: 本研究旨在提高图structured数据中节点级别异常检测效率，并提供一种简单可行的方法来实现这一目标。</li>
<li>methods: 该方法称为PREM，包括两个模块：预处理模块和ego- neighborg matching模块。PREM方法不需要传输消息传递，而是使用简单的对比损失函数，从而大幅提高训练速度和内存使用效率。</li>
<li>results: 经过对五种真实世界数据集的严格评估，PREM方法显示了robustness和效果。特别是在ACM数据集上，PREM方法与最高效的基线方法相比，提高了5%的AUC，提高了9倍的训练速度，并大幅降低内存使用量。<details>
<summary>Abstract</summary>
Node-level graph anomaly detection (GAD) plays a critical role in identifying anomalous nodes from graph-structured data in various domains such as medicine, social networks, and e-commerce. However, challenges have arisen due to the diversity of anomalies and the dearth of labeled data. Existing methodologies - reconstruction-based and contrastive learning - while effective, often suffer from efficiency issues, stemming from their complex objectives and elaborate modules. To improve the efficiency of GAD, we introduce a simple method termed PREprocessing and Matching (PREM for short). Our approach streamlines GAD, reducing time and memory consumption while maintaining powerful anomaly detection capabilities. Comprising two modules - a pre-processing module and an ego-neighbor matching module - PREM eliminates the necessity for message-passing propagation during training, and employs a simple contrastive loss, leading to considerable reductions in training time and memory usage. Moreover, through rigorous evaluations of five real-world datasets, our method demonstrated robustness and effectiveness. Notably, when validated on the ACM dataset, PREM achieved a 5% improvement in AUC, a 9-fold increase in training speed, and sharply reduce memory usage compared to the most efficient baseline.
</details>
<details>
<summary>摘要</summary>
nodal-level 图像异常检测 (GAD) 在不同领域中，如医学、社交网络和电商，扮演了重要的角色，以识别图像中异常的节点。然而，由于异常的多样性以及标注数据的缺乏，存在许多挑战。现有的方法ologies，如重建基于的方法和对比学习，虽然有效，但往往受到效率问题的困扰，这些问题来自于复杂的目标函数和复杂的模块。为了改善 GAD 的效率，我们提出了一种简单的方法，称为 PREprocessing 和 Matching (PREM)。我们的方法通过流elines 节点级别的图像数据，从而减少训练时间和内存使用，同时保持强大的异常检测能力。PREM 包括两个模块：预处理模块和一个 Egon 的匹配模块。我们的方法不需要在训练期间进行消息传递，而是使用一个简单的对比损失函数，从而导致训练时间和内存使用的减少。此外，我们对五个真实世界数据集进行了严格的评估，我们的方法在robustness和效果两个方面具有出色的表现。特别是在 ACM 数据集上，PREM 可以在训练速度、内存使用和 AUC 等方面与最高效的基eline 相比，达到 5% 的提升，9 倍增加训练速度，并显著减少内存使用。
</details></li>
</ul>
<hr>
<h2 id="Prototype-based-HyperAdapter-for-Sample-Efficient-Multi-task-Tuning"><a href="#Prototype-based-HyperAdapter-for-Sample-Efficient-Multi-task-Tuning" class="headerlink" title="Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning"></a>Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11670">http://arxiv.org/abs/2310.11670</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bumble666/pha">https://github.com/bumble666/pha</a></li>
<li>paper_authors: Hao Zhao, Jie Fu, Zhaofeng He</li>
<li>for: 这个研究是为了提高预训练语言模型的扩展性和数据效率。</li>
<li>methods: 这篇论文使用了参数效率的精致调整（PEFT）方法，并提出了一个名为实例紧密抽象（PHA）的新框架，它使用了适应器调整和超级网络来生成条件模组。</li>
<li>results: 这篇论文的实验结果显示，PHA方法在多任务学习和几少例转移学习中比较其他强基eline方法表现更好，尤其是当资料量变少时。<details>
<summary>Abstract</summary>
Parameter-efficient fine-tuning (PEFT) has shown its effectiveness in adapting the pre-trained language models to downstream tasks while only updating a small number of parameters. Despite the success, most existing methods independently adapt to each task without considering knowledge transfer between tasks and are limited to low-data regimes. To overcome this issue, we propose Prototype-based HyperAdapter (PHA), a novel framework built on the adapter-tuning and hypernetwork. It introduces an instance-dense retriever and a prototypical hypernetwork to generate the conditional modules in a sample-efficient manner. This leads to comparable performance improvements against existing PEFT methods on multi-task learning and few-shot transfer learning. More importantly, when the available data size gets smaller, our method outperforms other strong baselines by a large margin. Based on our extensive empirical experiments across various datasets, we demonstrate that PHA strikes a better trade-off between trainable parameters, accuracy on stream tasks, and sample efficiency.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SOTOPIA-Interactive-Evaluation-for-Social-Intelligence-in-Language-Agents"><a href="#SOTOPIA-Interactive-Evaluation-for-Social-Intelligence-in-Language-Agents" class="headerlink" title="SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents"></a>SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11667">http://arxiv.org/abs/2310.11667</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, Maarten Sap</li>
<li>for: 评估人工智能系统的社会智能能力</li>
<li>methods: 使用LLM-based agents和人类角色扮演者进行社会交互 scenario，并使用SOTOPIA-Eval评估框架评估模型的表现</li>
<li>results: 发现GPT-4在SOTOPIA-hard subsets中表现较差，其社交常识理解和战略通信技能受限，而人类则在这些 subsets中表现出优异的社会智能能力。<details>
<summary>Abstract</summary>
Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.
</details>
<details>
<summary>摘要</summary>
人类是社交生物，我们在日常互动中追求社交目标，这是人工智能系统的能力领域中的一个关键方面。然而，人工智能系统在这个领域的能力仍然尚未得到解释。我们提出了SOTOPIA，一个开放式环境，用于模拟人工智能代理人在复杂社交交互中的表现。在我们的环境中，代理人扮演和互动，在多种情况下协同合作、交换和竞争以完成复杂社交目标。我们在这个任务空间中模拟了LLM基于代理人和人类之间的角色扮演互动，并使用SOTOPIA-Eval全面评价框架进行评估。与SOTOPIA的使用，我们发现了不同的人工智能模型在社交智能方面存在显著差异，并确定了一个通用难度集合（SOTOPIA-hard），该集合对所有模型都是挑战性的。我们发现在这个集合中，GPT-4的目标完成率远低于人类，并且它很难展现社交感知和战略通信技能。这些发现表明SOTOPIA的潜在价值，作为一个通用的人工智能社交评价和改进平台。
</details></li>
</ul>
<hr>
<h2 id="Hetero-2-Net-Heterophily-aware-Representation-Learning-on-Heterogenerous-Graphs"><a href="#Hetero-2-Net-Heterophily-aware-Representation-Learning-on-Heterogenerous-Graphs" class="headerlink" title="Hetero$^2$Net: Heterophily-aware Representation Learning on Heterogenerous Graphs"></a>Hetero$^2$Net: Heterophily-aware Representation Learning on Heterogenerous Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11664">http://arxiv.org/abs/2310.11664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jintang Li, Zheng Wei, Jiawang Dan, Jing Zhou, Yuchang Zhu, Ruofan Wu, Baokun Wang, Zhang Zhen, Changhua Meng, Hong Jin, Zibin Zheng, Liang Chen</li>
<li>for: 本研究旨在 investigating the heterophily properties in heterogeneous graphs, and developing a heterophily-aware graph neural network (HGNN) to effectively handle more complex heterogeneous graphs.</li>
<li>methods: 我们使用 metapaths to identify the heterophily in heterogeneous graphs, and propose two practical metrics to quantitatively describe the levels of heterophily. We also introduce Hetero$^2$Net, a heterophily-aware HGNN that incorporates both masked metapath prediction and masked label prediction tasks to effectively handle both homophilic and heterophilic heterogeneous graphs.</li>
<li>results: 我们在 five real-world heterogeneous graph benchmarks with varying levels of heterophily 上 evaluate the performance of Hetero$^2$Net, and demonstrate that it outperforms strong baselines in the semi-supervised node classification task, providing valuable insights into effectively handling more complex heterogeneous graphs.<details>
<summary>Abstract</summary>
Real-world graphs are typically complex, exhibiting heterogeneity in the global structure, as well as strong heterophily within local neighborhoods. While a growing body of literature has revealed the limitations of common graph neural networks (GNNs) in handling homogeneous graphs with heterophily, little work has been conducted on investigating the heterophily properties in the context of heterogeneous graphs. To bridge this research gap, we identify the heterophily in heterogeneous graphs using metapaths and propose two practical metrics to quantitatively describe the levels of heterophily. Through in-depth investigations on several real-world heterogeneous graphs exhibiting varying levels of heterophily, we have observed that heterogeneous graph neural networks (HGNNs), which inherit many mechanisms from GNNs designed for homogeneous graphs, fail to generalize to heterogeneous graphs with heterophily or low level of homophily. To address the challenge, we present Hetero$^2$Net, a heterophily-aware HGNN that incorporates both masked metapath prediction and masked label prediction tasks to effectively and flexibly handle both homophilic and heterophilic heterogeneous graphs. We evaluate the performance of Hetero$^2$Net on five real-world heterogeneous graph benchmarks with varying levels of heterophily. The results demonstrate that Hetero$^2$Net outperforms strong baselines in the semi-supervised node classification task, providing valuable insights into effectively handling more complex heterogeneous graphs.
</details>
<details>
<summary>摘要</summary>
To address this gap, we identify the heterophily in heterogeneous graphs using metapaths and propose two practical metrics to quantitatively describe the levels of heterophily. Through in-depth investigations on several real-world heterogeneous graphs with varying levels of heterophily, we find that existing heterogeneous graph neural networks (HGNNs) fail to generalize to heterogeneous graphs with heterophily or low levels of homophily.To address this challenge, we present Hetero$^2$Net, a heterophily-aware HGNN that incorporates both masked metapath prediction and masked label prediction tasks to effectively and flexibly handle both homophilic and heterophilic heterogeneous graphs. We evaluate the performance of Hetero$^2$Net on five real-world heterogeneous graph benchmarks with varying levels of heterophily, and the results show that Hetero$^2$Net outperforms strong baselines in the semi-supervised node classification task, providing valuable insights into effectively handling more complex heterogeneous graphs.
</details></li>
</ul>
<hr>
<h2 id="Cloud-Magnetic-Resonance-Imaging-System-In-the-Era-of-6G-and-Artificial-Intelligence"><a href="#Cloud-Magnetic-Resonance-Imaging-System-In-the-Era-of-6G-and-Artificial-Intelligence" class="headerlink" title="Cloud-Magnetic Resonance Imaging System: In the Era of 6G and Artificial Intelligence"></a>Cloud-Magnetic Resonance Imaging System: In the Era of 6G and Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11641">http://arxiv.org/abs/2310.11641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yirong Zhou, Yanhuang Wu, Yuhan Su, Jing Li, Jianyun Cai, Yongfu You, Di Guo, Xiaobo Qu</li>
<li>for: 解决医疗机构年度生成巨量数据问题，提高医疗诊断精度和工作效率。</li>
<li>methods: integrating 分布式云计算、6G频率、边缘计算、联合学习和区块链技术。</li>
<li>results: 提高数据存储安全性、传输速度、人工智能算法维护、硬件升级和交叉机构医疗协作。<details>
<summary>Abstract</summary>
Magnetic Resonance Imaging (MRI) plays an important role in medical diagnosis, generating petabytes of image data annually in large hospitals. This voluminous data stream requires a significant amount of network bandwidth and extensive storage infrastructure. Additionally, local data processing demands substantial manpower and hardware investments. Data isolation across different healthcare institutions hinders cross-institutional collaboration in clinics and research. In this work, we anticipate an innovative MRI system and its four generations that integrate emerging distributed cloud computing, 6G bandwidth, edge computing, federated learning, and blockchain technology. This system is called Cloud-MRI, aiming at solving the problems of MRI data storage security, transmission speed, AI algorithm maintenance, hardware upgrading, and collaborative work. The workflow commences with the transformation of k-space raw data into the standardized Imaging Society for Magnetic Resonance in Medicine Raw Data (ISMRMRD) format. Then, the data are uploaded to the cloud or edge nodes for fast image reconstruction, neural network training, and automatic analysis. Then, the outcomes are seamlessly transmitted to clinics or research institutes for diagnosis and other services. The Cloud-MRI system will save the raw imaging data, reduce the risk of data loss, facilitate inter-institutional medical collaboration, and finally improve diagnostic accuracy and work efficiency.
</details>
<details>
<summary>摘要</summary>
The workflow of Cloud-MRI commences with the transformation of k-space raw data into the standardized Imaging Society for Magnetic Resonance in Medicine Raw Data (ISMRMRD) format. Then, the data are uploaded to the cloud or edge nodes for fast image reconstruction, neural network training, and automatic analysis. Finally, the outcomes are seamlessly transmitted to clinics or research institutes for diagnosis and other services.The Cloud-MRI system will save the raw imaging data, reduce the risk of data loss, facilitate inter-institutional medical collaboration, and finally improve diagnostic accuracy and work efficiency.Translated into Simplified Chinese:магнитно резонантно изображение (MRI) играет важную роль в медицинском диагнозирању, генеришући петабајтове количине слике података годишње у великим болницама. Овај обимни поток података захтева значајан удео мрежне брзине и екстензивну инфраструктуру за чување. Осим тога, локално обрадање података захтеваsubstantial ljudske ресурсе и инвестиције у хардвер. Ограничење података међу различитим здравственим установама отежава међуустанове медицинску сарадњу у клиникама и истраживањима. У овом раду, очекујемо иновативни систем MRI и његове четири генерације које интегришу емерингве дистрибуировану рачунарску облак технологију, 6G фреквенцију, ивицу рачунара, federated learning и блокчејн технологију. Овај систем се зове Cloud-MRI и има за циљ решења проблема чувања података MRI, брзине преноса, одржавања алгоритама, побољшања хардвера и сарадње.Радни процес Cloud-MRI почиње трансформацијомraw k-простора у стандардизовану форму Imaging Society for Magnetic Resonance in Medicine Raw Data (ISMRMRD). Затим, подаци се upload у облак или ивицу за брзо реконструкцију слике, тренинг неуралних мрежа и автоматско анализирање. На крају, извори се преносе безбедно на клинике или истраживачке институте за дијагнозу и друге услуге.Cloud-MRI систем ће чувати raw слике, смањити ризик губитка података, побољшати међуустанове медицинску сарадњу и на крају побољшати точност дијагнозе и ефикасност рада.
</details></li>
</ul>
<hr>
<h2 id="A-Symbolic-Language-for-Interpreting-Decision-Trees"><a href="#A-Symbolic-Language-for-Interpreting-Decision-Trees" class="headerlink" title="A Symbolic Language for Interpreting Decision Trees"></a>A Symbolic Language for Interpreting Decision Trees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11636">http://arxiv.org/abs/2310.11636</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/diegoemilio01/a-symbolic-language-for-interpreting-decision-trees">https://github.com/diegoemilio01/a-symbolic-language-for-interpreting-decision-trees</a></li>
<li>paper_authors: Marcelo Arenas, Pablo Barcelo, Diego Bustamente, Jose Caraball, Bernardo Subercaseaux</li>
<li>for: 这个论文旨在探讨形式可解释AI的发展，探讨decision trees的可解释性问题，并提出了不同的可解释性查询和处理方法。</li>
<li>methods: 该论文使用了一种名为StratiFOILed的精心构造的 fragments of first-ordered logic，可以计算多种后期解释，包括本地解释（如推理和对比解释）和全局解释（如特征相关性）。</li>
<li>results: 该论文提出了ExplainDT，一种符号语言用于解释decision trees，可以根据用户需求来定制查询。StratiFOILed queries可以写作Boolean combination of NP-problems，可以在实践中使用常数数量的SAT解决器调用来评估。<details>
<summary>Abstract</summary>
The recent development of formal explainable AI has disputed the folklore claim that "decision trees are readily interpretable models", showing different interpretability queries that are computationally hard on decision trees, as well as proposing different methods to deal with them in practice. Nonetheless, no single explainability query or score works as a "silver bullet" that is appropriate for every context and end-user. This naturally suggests the possibility of "interpretability languages" in which a wide variety of queries can be expressed, giving control to the end-user to tailor queries to their particular needs. In this context, our work presents ExplainDT, a symbolic language for interpreting decision trees. ExplainDT is rooted in a carefully constructed fragment of first-ordered logic that we call StratiFOILed. StratiFOILed balances expressiveness and complexity of evaluation, allowing for the computation of many post-hoc explanations--both local (e.g., abductive and contrastive explanations) and global ones (e.g., feature relevancy)--while remaining in the Boolean Hierarchy over NP. Furthermore, StratiFOILed queries can be written as a Boolean combination of NP-problems, thus allowing us to evaluate them in practice with a constant number of calls to a SAT solver. On the theoretical side, our main contribution is an in-depth analysis of the expressiveness and complexity of StratiFOILed, while on the practical side, we provide an optimized implementation for encoding StratiFOILed queries as propositional formulas, together with an experimental study on its efficiency.
</details>
<details>
<summary>摘要</summary>
最近的形式可解AI发展有抵触了传统的说法，证明了决策树不是一种直观可解的模型，并提出了不同的可解性查询和处理方法。然而，没有一个单一的可解性查询或分数可以满足每个情况和用户需求。这自然地提出了“可解性语言”的概念，允许用户根据自己的需求定制查询。在这个上下文中，我们提出了ExplainDT，一种符号语言用于解释决策树。ExplainDT基于我们优化的一种首领逻辑，即StratiFOILed，该逻辑具有较高的表达力和评估复杂性，可以计算多种后期解释（包括地方的推理和对比解释以及全局的特征相关性），同时仍然保持在Boolean Hierarchy中。此外，StratiFOILed查询可以写作一个Boolean组合，因此可以通过一个常数数量的SAT解决器的调用来评估。从理论角度来看，我们的主要贡献是对StratiFOILed的表达力和评估复杂性进行深入分析，而从实践角度来看，我们提供了优化的编码方法和实验研究其效率。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/18/cs.AI_2023_10_18/" data-id="clogxf3ky005n5xra1odlhryo" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/18/cs.CL_2023_10_18/" class="article-date">
  <time datetime="2023-10-18T11:00:00.000Z" itemprop="datePublished">2023-10-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/18/cs.CL_2023_10_18/">cs.CL - 2023-10-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="REMARK-LLM-A-Robust-and-Efficient-Watermarking-Framework-for-Generative-Large-Language-Models"><a href="#REMARK-LLM-A-Robust-and-Efficient-Watermarking-Framework-for-Generative-Large-Language-Models" class="headerlink" title="REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative Large Language Models"></a>REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12362">http://arxiv.org/abs/2310.12362</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruisi Zhang, Shehzeen Samarah Hussain, Paarth Neekhara, Farinaz Koushanfar</li>
<li>for: 这个论文是为了设计一个高效、可靠的文本生成模型（LLM）水印框架。</li>
<li>methods: 论文提出了三个新组件：（i）一个学习基于的消息编码模块，用于把二进制签名注入到LLM生成的文本中；（ii）一个重parameterization模块，用于将密集分布转换为稀疏分布的水印文本符号；（iii）一个专门 для签名EXTRACTION的解码模块。</li>
<li>results: 论文通过对多个未看过的数据集进行严格的训练，证明REMARK-LLM可以插入2倍多的签名比特数据入文本中，同时保持 semantic integrity，并且在各种水印检测和移除攻击下展现出更好的抗性。<details>
<summary>Abstract</summary>
We present REMARK-LLM, a novel efficient, and robust watermarking framework designed for texts generated by large language models (LLMs). Synthesizing human-like content using LLMs necessitates vast computational resources and extensive datasets, encapsulating critical intellectual property (IP). However, the generated content is prone to malicious exploitation, including spamming and plagiarism. To address the challenges, REMARK-LLM proposes three new components: (i) a learning-based message encoding module to infuse binary signatures into LLM-generated texts; (ii) a reparameterization module to transform the dense distributions from the message encoding to the sparse distribution of the watermarked textual tokens; (iii) a decoding module dedicated for signature extraction; Furthermore, we introduce an optimized beam search algorithm to guarantee the coherence and consistency of the generated content. REMARK-LLM is rigorously trained to encourage the preservation of semantic integrity in watermarked content, while ensuring effective watermark retrieval. Extensive evaluations on multiple unseen datasets highlight REMARK-LLM proficiency and transferability in inserting 2 times more signature bits into the same texts when compared to prior art, all while maintaining semantic integrity. Furthermore, REMARK-LLM exhibits better resilience against a spectrum of watermark detection and removal attacks.
</details>
<details>
<summary>摘要</summary>
我们介绍REMARK-LLM，一种新的高效、可靠的文本杂化框架，适用于大语言模型（LLM）生成的文本。使用LLM生成人类化内容需要庞大的计算资源和广泛的数据集，包括重要知识产权（IP）。然而，生成的内容容易被恶意利用，如垃圾邮件和抄袭。为解决这些挑战，REMARK-LLM提出了三个新组件：（i）一个学习基于的消息编码模块，用于在LLM生成的文本中混入binary标识符；（ii）一个重parameterization模块，将消息编码的稠密分布转换为稀疏分布的杂化文本token；（iii）一个专门 для抽取签名的解码模块。此外，我们引入了优化的搜索算法，以确保生成的内容具有准确性和一致性。REMARK-LLM在 Semantic integrity的保持和有效签名检索方面进行了严格的训练，同时能够插入2倍多的签名比特到同一个文本中，并且维护Semantic integrity。此外，REMARK-LLM表现出更好的抗干扰和抗除法风险。
</details></li>
</ul>
<hr>
<h2 id="GRI-Graph-based-Relative-Isomorphism-of-Word-Embedding-Spaces"><a href="#GRI-Graph-based-Relative-Isomorphism-of-Word-Embedding-Spaces" class="headerlink" title="GRI: Graph-based Relative Isomorphism of Word Embedding Spaces"></a>GRI: Graph-based Relative Isomorphism of Word Embedding Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12360">http://arxiv.org/abs/2310.12360</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/asif6827/gri">https://github.com/asif6827/gri</a></li>
<li>paper_authors: Muhammad Asif Ali, Yan Hu, Jianbin Qin, Di Wang</li>
<li>for:  automatic construction of bilingual dictionaries using monolingual embedding spaces</li>
<li>methods:  combines distributional training objectives with attentive graph convolutions to consider the impact of semantically similar words</li>
<li>results:  outperforms existing research by improving the average P@1 by up to 63.6%<details>
<summary>Abstract</summary>
Automated construction of bilingual dictionaries using monolingual embedding spaces is a core challenge in machine translation. The end performance of these dictionaries relies upon the geometric similarity of individual spaces, i.e., their degree of isomorphism. Existing attempts aimed at controlling the relative isomorphism of different spaces fail to incorporate the impact of semantically related words in the training objective. To address this, we propose GRI that combines the distributional training objectives with attentive graph convolutions to unanimously consider the impact of semantically similar words required to define/compute the relative isomorphism of multiple spaces. Experimental evaluation shows that GRI outperforms the existing research by improving the average P@1 by a relative score of up to 63.6%. We release the codes for GRI at https://github.com/asif6827/GRI.
</details>
<details>
<summary>摘要</summary>
自动化建立双语词典使用单语空间的嵌入是机器翻译的核心挑战。这些词典的性能取决于各个空间的几何相似性，即他们的相对几何同构性。现有的尝试都没有考虑semantic关联的影响，即在训练目标中考虑相似的单词。为解决这个问题，我们提出了GRI，它将分布式训练目标与注意力 Graph Convolutions 结合，同时考虑多个空间中相似的单词，以统一评估多个空间的相对几何同构性。实验表明，GRI可以提高平均P@1的表现，相比现有研究提高63.6%。我们在github上分享了GRI代码，可以在https://github.com/asif6827/GRI中下载。
</details></li>
</ul>
<hr>
<h2 id="knn-seq-Efficient-Extensible-kNN-MT-Framework"><a href="#knn-seq-Efficient-Extensible-kNN-MT-Framework" class="headerlink" title="knn-seq: Efficient, Extensible kNN-MT Framework"></a>knn-seq: Efficient, Extensible kNN-MT Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12352">http://arxiv.org/abs/2310.12352</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/naist-nlp/knn-seq">https://github.com/naist-nlp/knn-seq</a></li>
<li>paper_authors: Hiroyuki Deguchi, Hayate Hirano, Tomoki Hoshino, Yuto Nishida, Justin Vasselli, Taro Watanabe</li>
<li>for: 提高 translate quality，使用 translation examples during decoding</li>
<li>methods: 使用 k-nearest-neighbor machine translation (kNN-MT) 和 vector database (datastore)</li>
<li>results: 实现了一个高效的 kNN-MT 框架，可以快速构建大规模的 datastore，并在 WMT’19 German-to-English 翻译任务中实现了相当的提升。<details>
<summary>Abstract</summary>
k-nearest-neighbor machine translation (kNN-MT) boosts the translation quality of a pre-trained neural machine translation (NMT) model by utilizing translation examples during decoding. Translation examples are stored in a vector database, called a datastore, which contains one entry for each target token from the parallel data it is made from. Due to its size, it is computationally expensive both to construct and to retrieve examples from the datastore. In this paper, we present an efficient and extensible kNN-MT framework, knn-seq, for researchers and developers that is carefully designed to run efficiently, even with a billion-scale large datastore. knn-seq is developed as a plug-in on fairseq and easy to switch models and kNN indexes. Experimental results show that our implemented kNN-MT achieves a comparable gain to the original kNN-MT, and the billion-scale datastore construction took 2.21 hours in the WMT'19 German-to-English translation task. We publish our knn-seq as an MIT-licensed open-source project and the code is available on https://github.com/naist-nlp/knn-seq . The demo video is available on https://youtu.be/zTDzEOq80m0 .
</details>
<details>
<summary>摘要</summary>
k- nearest-neighbor机器翻译（kNN-MT）可以提高一个预训练的神经机器翻译（NMT）模型的翻译质量，通过在解码过程中使用翻译示例。翻译示例被存储在一个vector数据库中，称为datastore，每个目标单词都有一个入口。由于其大小，construct和retrieve示例从datastore是计算昂贵的。在这篇论文中，我们提出了一个高效和可扩展的kNN-MT框架，knn-seq，这是为研究人员和开发人员设计的，可以高效运行，即使数据存储量达到了十亿级。knn-seq是一个plug-in在fairseq上，可以方便地更换模型和kNN索引。实验结果表明，我们实现的kNN-MT可以与原始kNN-MT做比较，并且构建了一个百亿级datastore只需2.21小时在WMT'19德语到英语翻译任务中。我们在MIT许可下发布了knn-seq作为开源项目，代码可以在https://github.com/naist-nlp/knn-seq上获取。demo视频可以在https://youtu.be/zTDzEOq80m0上找到。
</details></li>
</ul>
<hr>
<h2 id="LACMA-Language-Aligning-Contrastive-Learning-with-Meta-Actions-for-Embodied-Instruction-Following"><a href="#LACMA-Language-Aligning-Contrastive-Learning-with-Meta-Actions-for-Embodied-Instruction-Following" class="headerlink" title="LACMA: Language-Aligning Contrastive Learning with Meta-Actions for Embodied Instruction Following"></a>LACMA: Language-Aligning Contrastive Learning with Meta-Actions for Embodied Instruction Following</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12344">http://arxiv.org/abs/2310.12344</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/joeyy5588/lacma">https://github.com/joeyy5588/lacma</a></li>
<li>paper_authors: Cheng-Fu Yang, Yen-Chun Chen, Jianwei Yang, Xiyang Dai, Lu Yuan, Yu-Chiang Frank Wang, Kai-Wei Chang</li>
<li>for: 这种 paper 的目的是提高 Embodied Instruction Following 中的泛化能力，使 agents 能够在未看过的环境中更好地执行任务。</li>
<li>methods: 这种 paper 使用了 contrastive learning 和 meta-actions 来解决 Embodied Instruction Following 中的泛化问题。</li>
<li>results:  compared to a strong multi-modal Transformer baseline, 这种方法 achieved a significant 4.5% absolute gain in success rate in unseen environments of ALFRED Embodied Instruction Following.I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
End-to-end Transformers have demonstrated an impressive success rate for Embodied Instruction Following when the environment has been seen in training. However, they tend to struggle when deployed in an unseen environment. This lack of generalizability is due to the agent's insensitivity to subtle changes in natural language instructions. To mitigate this issue, we propose explicitly aligning the agent's hidden states with the instructions via contrastive learning. Nevertheless, the semantic gap between high-level language instructions and the agent's low-level action space remains an obstacle. Therefore, we further introduce a novel concept of meta-actions to bridge the gap. Meta-actions are ubiquitous action patterns that can be parsed from the original action sequence. These patterns represent higher-level semantics that are intuitively aligned closer to the instructions. When meta-actions are applied as additional training signals, the agent generalizes better to unseen environments. Compared to a strong multi-modal Transformer baseline, we achieve a significant 4.5% absolute gain in success rate in unseen environments of ALFRED Embodied Instruction Following. Additional analysis shows that the contrastive objective and meta-actions are complementary in achieving the best results, and the resulting agent better aligns its states with corresponding instructions, making it more suitable for real-world embodied agents. The code is available at: https://github.com/joeyy5588/LACMA.
</details>
<details>
<summary>摘要</summary>
END-TO-END 转换器在训练中见过环境下的Embodied Instruction Following任务中表现出色，但在未经训练的环境下却表现不佳，这导致了模型的普适性受到限制。这种问题的原因在于模型对自然语言指令的敏感性不够，这使得模型在不同环境下无法适应。为了解决这个问题，我们提议通过对模型隐藏状态与指令进行对齐来提高模型的敏感性。然而，高级语言指令和模型的低级动作空间之间的差距仍然存在，这使得模型困难地将高级语言指令翻译成低级动作。为了解决这个问题，我们提出了一种新的概念——元动作。元动作是在原始动作序列中提取出的普适的动作模式，它们可以帮助模型更好地理解高级语言指令的含义。当元动作作为训练信号时，模型在未经训练的环境下的总成功率得到了显著的提高。相比于一个强大的多Modal Transformer参考点，我们在未经训练的ALFRED Embodied Instruction Following任务中实现了4.5%的绝对提升。更进一步的分析表明，对比于对照学习和元动作的融合，我们的方法更好地实现了模型与指令之间的对齐，使得模型更适合实际的具体体现agent。代码可以在以下链接获取：https://github.com/joeyy5588/LACMA。
</details></li>
</ul>
<hr>
<h2 id="Measuring-Pointwise-mathcal-V-Usable-Information-In-Context-ly"><a href="#Measuring-Pointwise-mathcal-V-Usable-Information-In-Context-ly" class="headerlink" title="Measuring Pointwise $\mathcal{V}$-Usable Information In-Context-ly"></a>Measuring Pointwise $\mathcal{V}$-Usable Information In-Context-ly</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12300">http://arxiv.org/abs/2310.12300</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/boblus/in-context-pvi">https://github.com/boblus/in-context-pvi</a></li>
<li>paper_authors: Sheng Lu, Shan Chen, Yingya Li, Danielle Bitterman, Guergana Savova, Iryna Gurevych</li>
<li>for: 这个论文是关于听 Context 学习（ICL）中的一种新的学习方法。</li>
<li>methods: 这篇论文使用了一种已经提出的困难度度量指标——点wise $\mathcal{V}$-usable information（PVI），并将其适应到了听 Context 版本（in-context PVI）。相比原始的 PVI，in-context PVI 更加高效，只需要一些示例和不需要调整。</li>
<li>results: 我们进行了一项广泛的实验分析，以评估in-context PVI 的可靠性。我们的发现表明，in-context PVI 估计值具有类似的特性于原始 PVI。具体地说，在听 Context 设置下，in-context PVI 估计值具有稳定的特性，不受不同的示例选择和射击数的影响。此外，我们还示了如何使用 in-context PVI 来标识困难的实例。这篇论文强调了 in-context PVI 的潜在价值和ICL的可能性。<details>
<summary>Abstract</summary>
In-context learning (ICL) is a new learning paradigm that has gained popularity along with the development of large language models. In this work, we adapt a recently proposed hardness metric, pointwise $\mathcal{V}$-usable information (PVI), to an in-context version (in-context PVI). Compared to the original PVI, in-context PVI is more efficient in that it requires only a few exemplars and does not require fine-tuning. We conducted a comprehensive empirical analysis to evaluate the reliability of in-context PVI. Our findings indicate that in-context PVI estimates exhibit similar characteristics to the original PVI. Specific to the in-context setting, we show that in-context PVI estimates remain consistent across different exemplar selections and numbers of shots. The variance of in-context PVI estimates across different exemplar selections is insignificant, which suggests that in-context PVI are stable. Furthermore, we demonstrate how in-context PVI can be employed to identify challenging instances. Our work highlights the potential of in-context PVI and provides new insights into the capabilities of ICL.
</details>
<details>
<summary>摘要</summary>
新学习理念“内容学习”（ICL）随着大语言模型的发展而受到关注。在这项工作中，我们对一种最近提出的困难度度量，点对可用信息（PVI）进行了适应。与原始PVI相比，内容PVI更加高效，只需几个示例并无需微调。我们进行了广泛的实验分析，以评估内容PVI的可靠性。我们的发现表明，内容PVI估计具有与原始PVI相似的特征。具体来说，在内容设置下，内容PVI估计具有不同示例选择和射击数量的稳定性。 var（内容PVI估计）在不同示例选择下的差异不显著，这表明内容PVI是稳定的。此外，我们还证明了内容PVI可以用于标识困难实例。我们的工作探讨了内容PVI的潜力和ICL的可能性，并提供了新的视角。
</details></li>
</ul>
<hr>
<h2 id="Direct-Neural-Machine-Translation-with-Task-level-Mixture-of-Experts-models"><a href="#Direct-Neural-Machine-Translation-with-Task-level-Mixture-of-Experts-models" class="headerlink" title="Direct Neural Machine Translation with Task-level Mixture of Experts models"></a>Direct Neural Machine Translation with Task-level Mixture of Experts models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12236">http://arxiv.org/abs/2310.12236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Isidora Chara Tourni, Subhajit Naskar</li>
<li>for: 这个论文主要研究了Direct Neural Machine Translation（直接神经机器翻译）系统，它可以将文本翻译成两种不同语言之间。</li>
<li>methods: 论文提出了多种方法来解决直接NMT系统的限制，包括多语言NMT和中间语言NMT（通过英语翻译）。它们还提出了Task-level Mixture of expert models（任务级混合专家模型），一种基于Transformer模型的推理效率优化方法。</li>
<li>results: 论文表明，Task-level MoE-based direct NMT系统在大量低资源和高资源irect对的翻译任务上表现出色，并且在7种语言对上超过了双语和中间语言NMT模型。<details>
<summary>Abstract</summary>
Direct neural machine translation (direct NMT) is a type of NMT system that translates text between two non-English languages. Direct NMT systems often face limitations due to the scarcity of parallel data between non-English language pairs. Several approaches have been proposed to address this limitation, such as multilingual NMT and pivot NMT (translation between two languages via English). Task-level Mixture of expert models (Task-level MoE), an inference-efficient variation of Transformer-based models, has shown promising NMT performance for a large number of language pairs. In Task-level MoE, different language groups can use different routing strategies to optimize cross-lingual learning and inference speed. In this work, we examine Task-level MoE's applicability in direct NMT and propose a series of high-performing training and evaluation configurations, through which Task-level MoE-based direct NMT systems outperform bilingual and pivot-based models for a large number of low and high-resource direct pairs, and translation directions. Our Task-level MoE with 16 experts outperforms bilingual NMT, Pivot NMT models for 7 language pairs, while pivot-based models still performed better in 9 pairs and directions.
</details>
<details>
<summary>摘要</summary>
直接神经机器翻译（直接NMT）是一种NMT系统，用于翻译非英语语言对。直接NMT系统经常面临限制，即非英语语言对的并不充足。多种方法已经提出来解决这个问题，如多语言NMT和中转NMT（通过英语翻译）。任务级别混合模型（Task-level MoE），一种基于转换器模型的推理效率版本，在许多语言对上表现出了优秀的NMT性能。在Task-level MoE中，不同语言组可以使用不同的路由策略来优化对语言之间的学习和推理速度。在本研究中，我们研究Task-level MoE在直接NMT中的适用性，并提出了一系列高性能的训练和评估配置。通过这些配置，Task-level MoE基于直接NMT系统在大量低资源和高资源直接对的翻译方向上表现出了比比较好的成绩。我们的Task-level MoE系统与16个专家相比，超过了双语NMT和中转NMT模型在7个语言对上的性能。然而，中转NMT模型仍然在9个语言对和方向上表现出了较好的成绩。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Retrieval-Augmentation-for-Long-Form-Question-Answering"><a href="#Understanding-Retrieval-Augmentation-for-Long-Form-Question-Answering" class="headerlink" title="Understanding Retrieval Augmentation for Long-Form Question Answering"></a>Understanding Retrieval Augmentation for Long-Form Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12150">http://arxiv.org/abs/2310.12150</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hung-Ting Chen, Fangyuan Xu, Shane A. Arora, Eunsol Choi</li>
<li>for: 本研究探讨了含有检索增强的语言模型（LM）在长问题回答中的表现。研究如何在不同的检索文档集中使用LM进行回答生成，以及不同检索文档集对LM的回答生成造成的影响。</li>
<li>methods: 本研究使用了不同的检索文档集，并对LM的回答生成进行了分析。研究包括对生成的答案进行评估，以及使用人工标注来评估答案的归因。</li>
<li>results: 研究发现，使用不同检索文档集可以影响LM的回答生成质量。此外，研究还发现了长文本生成中的归因模式，以及LM的归因错误的主要原因。<details>
<summary>Abstract</summary>
We present a study of retrieval-augmented language models (LMs) on long-form question answering. We analyze how retrieval augmentation impacts different LMs, by comparing answers generated from models while using the same evidence documents, and how differing quality of retrieval document set impacts the answers generated from the same LM. We study various attributes of generated answers (e.g., fluency, length, variance) with an emphasis on the attribution of generated long-form answers to in-context evidence documents. We collect human annotations of answer attribution and evaluate methods for automatically judging attribution. Our study provides new insights on how retrieval augmentation impacts long, knowledge-rich text generation of LMs. We further identify attribution patterns for long text generation and analyze the main culprits of attribution errors. Together, our analysis reveals how retrieval augmentation impacts long knowledge-rich text generation and provide directions for future work.
</details>
<details>
<summary>摘要</summary>
我们提出了一项研究，探讨 Retrieval-augmented 语言模型（LM）在长问答中的表现。我们分析了不同LM在使用同一份证据文档时的响应，以及不同证据文档集的质量如何影响LM生成的答案。我们研究了各种答案特征（如流畅度、长度、变化程度），强调在上下文文档中归因生成的长文答案。我们收集了人类标注答案归因的数据，并评估了自动判断归因的方法。我们的研究提供了新的认知，揭示了 Retrieval-augmented 语言模型在长知识含量文本生成中的影响，以及长文生成中的归因模式和错误的主要原因。这些分析结果为未来工作提供了方向。
</details></li>
</ul>
<hr>
<h2 id="Simple-Mechanisms-for-Representing-Indexing-and-Manipulating-Concepts"><a href="#Simple-Mechanisms-for-Representing-Indexing-and-Manipulating-Concepts" class="headerlink" title="Simple Mechanisms for Representing, Indexing and Manipulating Concepts"></a>Simple Mechanisms for Representing, Indexing and Manipulating Concepts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12143">http://arxiv.org/abs/2310.12143</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanzhi Li, Raghu Meka, Rina Panigrahy, Kulin Shah</li>
<li>for: 这篇论文旨在提出一种新的方法来学习概念，而不是通过传统的分类器来做。</li>
<li>methods: 这种方法基于对概念的时间统计矩阵来生成具体的表示或签名，并通过了解这些签名的结构来找到更高级别的概念。</li>
<li>results: 该方法可以在不同的概念之间找到共同主题，并可以用来建立一个概念字典，以便将输入数据正确地归类到相关的概念中。<details>
<summary>Abstract</summary>
Deep networks typically learn concepts via classifiers, which involves setting up a model and training it via gradient descent to fit the concept-labeled data. We will argue instead that learning a concept could be done by looking at its moment statistics matrix to generate a concrete representation or signature of that concept. These signatures can be used to discover structure across the set of concepts and could recursively produce higher-level concepts by learning this structure from those signatures. When the concepts are `intersected', signatures of the concepts can be used to find a common theme across a number of related `intersected' concepts. This process could be used to keep a dictionary of concepts so that inputs could correctly identify and be routed to the set of concepts involved in the (latent) generation of the input.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is used in this translation, which is a more casual and conversational style of Chinese. Traditional Chinese would be more formal and written.
</details></li>
</ul>
<hr>
<h2 id="Pseudointelligence-A-Unifying-Framework-for-Language-Model-Evaluation"><a href="#Pseudointelligence-A-Unifying-Framework-for-Language-Model-Evaluation" class="headerlink" title="Pseudointelligence: A Unifying Framework for Language Model Evaluation"></a>Pseudointelligence: A Unifying Framework for Language Model Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12135">http://arxiv.org/abs/2310.12135</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shikhar Murty, Orr Paradise, Pratyusha Sharma</li>
<li>for: 评估模型能力的原则性方法</li>
<li>methods: 使用复杂性理论的动态互动模型和学习评估器</li>
<li>results: 可以用来评估语言模型的两个案例研究以及现有评估方法的分析<details>
<summary>Abstract</summary>
With large language models surpassing human performance on an increasing number of benchmarks, we must take a principled approach for targeted evaluation of model capabilities. Inspired by pseudorandomness, we propose pseudointelligence, which captures the maxim that "(perceived) intelligence lies in the eye of the beholder". That is, that claims of intelligence are meaningful only when their evaluator is taken into account. Concretely, we propose a complexity-theoretic framework of model evaluation cast as a dynamic interaction between a model and a learned evaluator. We demonstrate that this framework can be used to reason about two case studies in language model evaluation, as well as analyze existing evaluation methods.
</details>
<details>
<summary>摘要</summary>
With large language models surpassing human performance on an increasing number of benchmarks, we must take a principled approach for targeted evaluation of model capabilities. Inspired by pseudorandomness, we propose pseudointelligence, which captures the maxim that "(perceived) intelligence lies in the eye of the beholder". That is, that claims of intelligence are meaningful only when their evaluator is taken into account. Concretely, we propose a complexity-theoretic framework of model evaluation cast as a dynamic interaction between a model and a learned evaluator. We demonstrate that this framework can be used to reason about two case studies in language model evaluation, as well as analyze existing evaluation methods.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Hong Kong, Macau, and Taiwan.
</details></li>
</ul>
<hr>
<h2 id="A-Tale-of-Pronouns-Interpretability-Informs-Gender-Bias-Mitigation-for-Fairer-Instruction-Tuned-Machine-Translation"><a href="#A-Tale-of-Pronouns-Interpretability-Informs-Gender-Bias-Mitigation-for-Fairer-Instruction-Tuned-Machine-Translation" class="headerlink" title="A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation"></a>A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12127">http://arxiv.org/abs/2310.12127</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/milanlproc/interpretability-mt-gender-bias">https://github.com/milanlproc/interpretability-mt-gender-bias</a></li>
<li>paper_authors: Giuseppe Attanasio, Flor Miriam Plaza-del-Arco, Debora Nozza, Anne Lauscher</li>
<li>for: 本研究旨在探讨现有的语言模型是否带有性别偏见，以及如何 Mitigate 这种偏见。</li>
<li>methods: 本研究使用了一系列的可读性方法，包括计算已知的性别偏见指标，以及使用 few-shot learning 的方法来解决偏见问题。</li>
<li>results: 研究发现，IFT 模型默认将 male-inflected 翻译作为结果，甚至会忽略女性职业 gender 标签。此外，研究还发现模型在错误翻译中忽略 masculine 和 feminine  pronoun 的问题。基于这些发现，研究提出了一种简单、有效的偏见 Mitigation 解决方案，通过 few-shot learning 实现了更加公平的翻译结果。<details>
<summary>Abstract</summary>
Recent instruction fine-tuned models can solve multiple NLP tasks when prompted to do so, with machine translation (MT) being a prominent use case. However, current research often focuses on standard performance benchmarks, leaving compelling fairness and ethical considerations behind. In MT, this might lead to misgendered translations, resulting, among other harms, in the perpetuation of stereotypes and prejudices. In this work, we address this gap by investigating whether and to what extent such models exhibit gender bias in machine translation and how we can mitigate it. Concretely, we compute established gender bias metrics on the WinoMT corpus from English to German and Spanish. We discover that IFT models default to male-inflected translations, even disregarding female occupational stereotypes. Next, using interpretability methods, we unveil that models systematically overlook the pronoun indicating the gender of a target occupation in misgendered translations. Finally, based on this finding, we propose an easy-to-implement and effective bias mitigation solution based on few-shot learning that leads to significantly fairer translations.
</details>
<details>
<summary>摘要</summary>
现代指导模型可以解决多个自然语言处理任务，机器翻译（MT）是其中的一个重要用例。然而，当前的研究经常关注标准性能指标，而忽略了吸引人的公平和道德考虑。在MT中，这可能导致误射翻译，其中的一些危害包括延续偏见和预设。在这项工作中，我们填补这个遗漏，我们研究了IFT模型在机器翻译中是否存在性别偏见，以及如何缓解它。具体来说，我们在英语到德语和西班牙语的WinoMT corpus上计算了确定性别偏见的指标。我们发现，IFT模型默认使用♂inflected翻译，即使 female occupational stereotypes。然后，使用可见性方法，我们发现模型在误射翻译中系统地忽略指示翻译对象的性别的代名词。最后，基于这一发现，我们提出了一种易于实施的和有效的偏见缓解解决方案，该解决方案基于几 shot learning，可以导致非常公平的翻译。
</details></li>
</ul>
<hr>
<h2 id="Harnessing-Dataset-Cartography-for-Improved-Compositional-Generalization-in-Transformers"><a href="#Harnessing-Dataset-Cartography-for-Improved-Compositional-Generalization-in-Transformers" class="headerlink" title="Harnessing Dataset Cartography for Improved Compositional Generalization in Transformers"></a>Harnessing Dataset Cartography for Improved Compositional Generalization in Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12118">http://arxiv.org/abs/2310.12118</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cyberiada/cartography-for-compositionality">https://github.com/cyberiada/cartography-for-compositionality</a></li>
<li>paper_authors: Osman Batur İnce, Tanin Zeraati, Semih Yagcioglu, Yadollah Yaghoobzadeh, Erkut Erdem, Aykut Erdem</li>
<li>for: 提高Transformer模型的compositional generalization能力</li>
<li>methods: 使用dataset cartography作为curriculum learning criterion</li>
<li>results: 实现10%的提高精度在CFQ和COGS数据集上，无需hyperparameter tuning<details>
<summary>Abstract</summary>
Neural networks have revolutionized language modeling and excelled in various downstream tasks. However, the extent to which these models achieve compositional generalization comparable to human cognitive abilities remains a topic of debate. While existing approaches in the field have mainly focused on novel architectures and alternative learning paradigms, we introduce a pioneering method harnessing the power of dataset cartography (Swayamdipta et al., 2020). By strategically identifying a subset of compositional generalization data using this approach, we achieve a remarkable improvement in model accuracy, yielding enhancements of up to 10% on CFQ and COGS datasets. Notably, our technique incorporates dataset cartography as a curriculum learning criterion, eliminating the need for hyperparameter tuning while consistently achieving superior performance. Our findings highlight the untapped potential of dataset cartography in unleashing the full capabilities of compositional generalization within Transformer models. Our code is available at https://github.com/cyberiada/cartography-for-compositionality.
</details>
<details>
<summary>摘要</summary>
神经网络已经革命化语言模型化，并在各种下游任务中表现出色。然而，这些模型是否达到人类认知能力的 Compositional generalization 水平仍然是一个议题。现有的方法主要集中在新的建筑和学习方法上，而我们则提出了一种拓展 dataset cartography（Swayamdipta et al., 2020）的新方法。通过策略地选择 Compositional generalization 数据 subsets，我们实现了模型精度的显著提高，CFQ 和 COGS 数据集上的提高达到 10%。值得注意的是，我们的技术将 dataset cartography 作为课程学习标准，从而消除了 hyperparameter 调整的需求，并一直保持优秀的性能。我们的发现表明，使用 dataset cartography 可以解 liberate 传播模型中的 Compositional generalization 潜力。我们的代码可以在 GitHub 上找到：https://github.com/cyberiada/cartography-for-compositionality。
</details></li>
</ul>
<hr>
<h2 id="On-the-Benefit-of-Generative-Foundation-Models-for-Human-Activity-Recognition"><a href="#On-the-Benefit-of-Generative-Foundation-Models-for-Human-Activity-Recognition" class="headerlink" title="On the Benefit of Generative Foundation Models for Human Activity Recognition"></a>On the Benefit of Generative Foundation Models for Human Activity Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12085">http://arxiv.org/abs/2310.12085</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zikang Leng, Hyeokhyen Kwon, Thomas Plötz</li>
<li>for:  solves the problem of limited annotated data in human activity recognition (HAR) by using generative AI to autonomously generate virtual IMU data from text descriptions.</li>
<li>methods:  uses Large Language Models (LLMs) and motion synthesis models to generate virtual IMU data.</li>
<li>results:  identifies several promising research pathways that could benefit from generative AI in HAR, including generating benchmark datasets, developing foundational models specific to HAR, exploring hierarchical structures within HAR, breaking down complex activities, and applications in health sensing and activity summarization.Here is the text in Simplified Chinese:</li>
<li>for: 解决人体活动识别（HAR）中数据稀缺问题，使用生成AI自动生成文本描述IMU数据。</li>
<li>methods: 使用大型语言模型（LLMs）和运动合成模型生成IMU数据。</li>
<li>results: 找到了生成AI在HAR中的许多有优势的研究方向，包括生成数据集、开发特有于HAR的基础模型、阶段分解复杂活动、应用于健康感知和活动概要。<details>
<summary>Abstract</summary>
In human activity recognition (HAR), the limited availability of annotated data presents a significant challenge. Drawing inspiration from the latest advancements in generative AI, including Large Language Models (LLMs) and motion synthesis models, we believe that generative AI can address this data scarcity by autonomously generating virtual IMU data from text descriptions. Beyond this, we spotlight several promising research pathways that could benefit from generative AI for the community, including the generating benchmark datasets, the development of foundational models specific to HAR, the exploration of hierarchical structures within HAR, breaking down complex activities, and applications in health sensing and activity summarization.
</details>
<details>
<summary>摘要</summary>
人类活动识别（HAR）中，数据缺乏问题是一大挑战。我们 Drawing inspiration from the latest advancements in generative AI，包括大型自然语言模型（LLM）和运动合成模型，我们认为生成AI可以解决这种数据缺乏问题，通过自动生成虚拟IMU数据从文本描述中。此外，我们还指出了许多有前途的研究方向，包括生成标准数据集，开发特有的HAR基础模型，探索HAR层次结构，分解复杂活动，以及医疗感知和活动概要应用。
</details></li>
</ul>
<hr>
<h2 id="Towards-Safer-Operations-An-Expert-involved-Dataset-of-High-Pressure-Gas-Incidents-for-Preventing-Future-Failures"><a href="#Towards-Safer-Operations-An-Expert-involved-Dataset-of-High-Pressure-Gas-Incidents-for-Preventing-Future-Failures" class="headerlink" title="Towards Safer Operations: An Expert-involved Dataset of High-Pressure Gas Incidents for Preventing Future Failures"></a>Towards Safer Operations: An Expert-involved Dataset of High-Pressure Gas Incidents for Preventing Future Failures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12074">http://arxiv.org/abs/2310.12074</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cinnamon/incident-ai-dataset">https://github.com/cinnamon/incident-ai-dataset</a></li>
<li>paper_authors: Shumpei Inoue, Minh-Tien Nguyen, Hiroki Mizokuchi, Tuan-Anh D. Nguyen, Huu-Hiep Nguyen, Dung Tien Le</li>
<li>for: 这个研究是为了开发一个新的安全预防 dataset，用于应用自然语言处理（NLP）技术来分析事故报告，以预防未来的失败。</li>
<li>methods: 这个研究使用了三种任务：命名实体识别、 causa-effect 提取和信息检索。这些任务是由域专家 manually annotate，他们至少有六年的实践经验。</li>
<li>results: 初步的结果表明，NLP技术可以有效地分析事故报告，以预防未来的失败。 dataset 可以促进未来的研究在 NLP 和事故管理领域。 dataset 的访问也提供（IncidentAI dataset 可以在：<a target="_blank" rel="noopener" href="https://github.com/Cinnamon/incident-ai-dataset">https://github.com/Cinnamon/incident-ai-dataset</a> 中找到）。<details>
<summary>Abstract</summary>
This paper introduces a new IncidentAI dataset for safety prevention. Different from prior corpora that usually contain a single task, our dataset comprises three tasks: named entity recognition, cause-effect extraction, and information retrieval. The dataset is annotated by domain experts who have at least six years of practical experience as high-pressure gas conservation managers. We validate the contribution of the dataset in the scenario of safety prevention. Preliminary results on the three tasks show that NLP techniques are beneficial for analyzing incident reports to prevent future failures. The dataset facilitates future research in NLP and incident management communities. The access to the dataset is also provided (the IncidentAI dataset is available at: https://github.com/Cinnamon/incident-ai-dataset).
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了一个新的 IncidentAI 数据集，用于安全预防。与过去的数据集不同，我们的数据集包含三个任务：命名实体识别、 causal EXTRACTION 和信息检索。数据集由具有至少六年实践经验的高压气保存管理员进行标注。我们验证了数据集在安全预防方面的贡献。初步结果显示，NLP 技术可以有效地分析事故报告，以预防未来的失败。该数据集将促进未来 NLP 和事故管理社区的研究。数据集的访问权也提供（IncidentAI 数据集可以在：https://github.com/Cinnamon/incident-ai-dataset 中获取）。
</details></li>
</ul>
<hr>
<h2 id="SPEED-Speculative-Pipelined-Execution-for-Efficient-Decoding"><a href="#SPEED-Speculative-Pipelined-Execution-for-Efficient-Decoding" class="headerlink" title="SPEED: Speculative Pipelined Execution for Efficient Decoding"></a>SPEED: Speculative Pipelined Execution for Efficient Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12072">http://arxiv.org/abs/2310.12072</a></li>
<li>repo_url: None</li>
<li>paper_authors: Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Hasan Genc, Kurt Keutzer, Amir Gholami, Sophia Shao</li>
<li>for: 提高生成大型自然语言处理模型（LLMs）的实时推理效率，以便在各种自然语言处理任务中使用。</li>
<li>methods: 使用预测值基于早期层隐藏状态来спекулятив执行未来的多个字符，以实现多字符级别的并行推理。</li>
<li>results: 在Transformerdecoder中实现Parameter sharing，通过减少内存操作的负担，提高生成LLM推理的效率，并在模型准确率 versus 延迟时间之间取得平衡。<details>
<summary>Abstract</summary>
Generative Large Language Models (LLMs) based on the Transformer architecture have recently emerged as a dominant foundation model for a wide range of Natural Language Processing tasks. Nevertheless, their application in real-time scenarios has been highly restricted due to the significant inference latency associated with these models. This is particularly pronounced due to the autoregressive nature of generative LLM inference, where tokens are generated sequentially since each token depends on all previous output tokens. It is therefore challenging to achieve any token-level parallelism, making inference extremely memory-bound. In this work, we propose SPEED, which improves inference efficiency by speculatively executing multiple future tokens in parallel with the current token using predicted values based on early-layer hidden states. For Transformer decoders that employ parameter sharing, the memory operations for the tokens executing in parallel can be amortized, which allows us to accelerate generative LLM inference. We demonstrate the efficiency of our method in terms of latency reduction relative to model accuracy and demonstrate how speculation allows for training deeper decoders with parameter sharing with minimal runtime overhead.
</details>
<details>
<summary>摘要</summary>
大量的自然语言处理任务中的生成大语言模型（LLM）基于Transformer架构最近占据了主导地位。然而，它们在实时场景中的应用受到了较大的推理延迟的限制。这主要是因为生成LLM的推理是sequential的，每个token都виси于所有前一个输出token。因此，难以实现任务级别的并行计算，使推理变得具有很高的内存约束。在这种情况下，我们提出了SPEED方法，它通过预测基于早期隐藏状态的值来спекулятив执行多个未来的token在并行的方式。对于使用参数共享的Transformer解码器，我们可以归并内存操作，这allow us以加速生成LLM推理。我们通过对响应率和模型精度之间的负载减少来证明我们的方法的效率。此外，我们还示出了通过 especulation进行训练更深的解码器，只需要最小的运行时开销。
</details></li>
</ul>
<hr>
<h2 id="Code-Book-for-the-Annotation-of-Diverse-Cross-Document-Coreference-of-Entities-in-News-Articles"><a href="#Code-Book-for-the-Annotation-of-Diverse-Cross-Document-Coreference-of-Entities-in-News-Articles" class="headerlink" title="Code Book for the Annotation of Diverse Cross-Document Coreference of Entities in News Articles"></a>Code Book for the Annotation of Diverse Cross-Document Coreference of Entities in News Articles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12064">http://arxiv.org/abs/2310.12064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jakob Vogel</li>
<li>for: 这篇论文是为了提出一种把核心引用跨文档进行标注的方案，超越传统的标识关系，还考虑到近似关系和连接关系。</li>
<li>methods: 这篇论文使用了一种名为“Inception”的注释工具，并提供了精确的注释实现方法，包括在新闻文章中标注实体，将其与多种核心引用关系连接起来，并将其与Wikidata全球知识图谱连接起来。</li>
<li>results: 这篇论文的主要贡献是提供了一种多层次注释方法，可以应用于媒体偏见分析中的词汇选择和标签。<details>
<summary>Abstract</summary>
This paper presents a scheme for annotating coreference across news articles, extending beyond traditional identity relations by also considering near-identity and bridging relations. It includes a precise description of how to set up Inception, a respective annotation tool, how to annotate entities in news articles, connect them with diverse coreferential relations, and link them across documents to Wikidata's global knowledge graph. This multi-layered annotation approach is discussed in the context of the problem of media bias. Our main contribution lies in providing a methodology for creating a diverse cross-document coreference corpus which can be applied to the analysis of media bias by word-choice and labelling.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Evaluating-the-Symbol-Binding-Ability-of-Large-Language-Models-for-Multiple-Choice-Questions-in-Vietnamese-General-Education"><a href="#Evaluating-the-Symbol-Binding-Ability-of-Large-Language-Models-for-Multiple-Choice-Questions-in-Vietnamese-General-Education" class="headerlink" title="Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education"></a>Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12059">http://arxiv.org/abs/2310.12059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duc-Vu Nguyen, Quoc-Nam Nguyen</li>
<li>for: 这个研究旨在评估大语言模型（LLM）在零shot、一shot、少shot设置下的多选符号绑定（MCSB）能力，以解决越南语MCQA任务。</li>
<li>methods: 研究使用了六种知名的LLM模型，namely BLOOMZ-7.1B-MT、LLaMA-2-7B、LLaMA-2-70B、GPT-3、GPT-3.5和GPT-4.0，对ViMMRC 1.0和ViMMRC 2.0数据集和我们提议的数据集进行评估。</li>
<li>results: 研究发现，这些LLM模型在越南语MCQA任务中具有扎实的MCSB能力，特别是在零shot和一shot设置下。<details>
<summary>Abstract</summary>
In this paper, we evaluate the ability of large language models (LLMs) to perform multiple choice symbol binding (MCSB) for multiple choice question answering (MCQA) tasks in zero-shot, one-shot, and few-shot settings. We focus on Vietnamese, with fewer challenging MCQA datasets than in English. The two existing datasets, ViMMRC 1.0 and ViMMRC 2.0, focus on literature. Recent research in Vietnamese natural language processing (NLP) has focused on the Vietnamese National High School Graduation Examination (VNHSGE) from 2019 to 2023 to evaluate ChatGPT. However, these studies have mainly focused on how ChatGPT solves the VNHSGE step by step. We aim to create a novel and high-quality dataset by providing structured guidelines for typing LaTeX formulas for mathematics, physics, chemistry, and biology. This dataset can be used to evaluate the MCSB ability of LLMs and smaller language models (LMs) because it is typed in a strict LaTeX style. We focus on predicting the character (A, B, C, or D) that is the most likely answer to a question, given the context of the question. Our evaluation of six well-known LLMs, namely BLOOMZ-7.1B-MT, LLaMA-2-7B, LLaMA-2-70B, GPT-3, GPT-3.5, and GPT-4.0, on the ViMMRC 1.0 and ViMMRC 2.0 benchmarks and our proposed dataset shows promising results on the MCSB ability of LLMs for Vietnamese. The dataset is available for research purposes only.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们评估了大语言模型（LLM）在零批、一批和几批设置下的多选符号绑定（MCSB）能力，用于多选问答（MCQA）任务。我们关注越南语言，因为越南语言MCQA数据集比英语更少。我们研究的两个现有数据集是 ViMMRC 1.0 和 ViMMRC 2.0，它们都是文学类。近期的越南语言自然语言处理（NLP）研究主要集中在评估 ChatGPT，但是这些研究主要集中在 ChatGPT 如何解决越南语言高中毕业考试（VNHSGE）。我们希望创建一个新的高质量数据集，提供了 LaTeX 格式的结构化指南，以便用于评估 LLM 和更小的语言模型（LM）的 MCSB 能力。我们的评估结果显示，六种著名的 LLM 在 ViMMRC 1.0 和 ViMMRC 2.0 标准和我们提议的数据集上表现出了预期的 MCSB 能力。数据集仅用于研究目的。
</details></li>
</ul>
<hr>
<h2 id="Concept-Guided-Chain-of-Thought-Prompting-for-Pairwise-Comparison-Scaling-of-Texts-with-Large-Language-Models"><a href="#Concept-Guided-Chain-of-Thought-Prompting-for-Pairwise-Comparison-Scaling-of-Texts-with-Large-Language-Models" class="headerlink" title="Concept-Guided Chain-of-Thought Prompting for Pairwise Comparison Scaling of Texts with Large Language Models"></a>Concept-Guided Chain-of-Thought Prompting for Pairwise Comparison Scaling of Texts with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12049">http://arxiv.org/abs/2310.12049</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patrick Y. Wu, Jonathan Nagler, Joshua A. Tucker, Solomon Messing</li>
<li>for: 这篇论文旨在开发一种基于生成大语言模型（LLM）的文本扩展方法，以便不需要大量文本资料和标注数据，可以有效地处理短文本和缺乏标注数据的情况。</li>
<li>methods: 这篇论文提出了一种基于提示的概念指导链条（CGCoT）方法，通过设计特定的提示来概括想法和标识文本中的 Target 个体，然后使用这些提示来生成概念特定的分析结果，类似于人工编码分析的指导。</li>
<li>results: 这篇论文使用CGCoT方法和大语言模型（LLM）对 Twitter 上的情感语言进行了扩展，并证明了该方法可以生成与人类评价相符的投票结果，而且不需要大量标注数据。<details>
<summary>Abstract</summary>
Existing text scaling methods often require a large corpus, struggle with short texts, or require labeled data. We develop a text scaling method that leverages the pattern recognition capabilities of generative large language models (LLMs). Specifically, we propose concept-guided chain-of-thought (CGCoT), which uses prompts designed to summarize ideas and identify target parties in texts to generate concept-specific breakdowns, in many ways similar to guidance for human coder content analysis. CGCoT effectively shifts pairwise text comparisons from a reasoning problem to a pattern recognition problem. We then pairwise compare concept-specific breakdowns using an LLM. We use the results of these pairwise comparisons to estimate a scale using the Bradley-Terry model. We use this approach to scale affective speech on Twitter. Our measures correlate more strongly with human judgments than alternative approaches like Wordfish. Besides a small set of pilot data to develop the CGCoT prompts, our measures require no additional labeled data and produce binary predictions comparable to a RoBERTa-Large model fine-tuned on thousands of human-labeled tweets. We demonstrate how combining substantive knowledge with LLMs can create state-of-the-art measures of abstract concepts.
</details>
<details>
<summary>摘要</summary>
现有的文本缩放方法通常需要大量数据集，困难处理短文本，或需要标注数据。我们开发了一种基于生成大语言模型（LLM）的文本缩放方法，具体来说是思想导向链条（CGCoT）。CGCoT使用用于概述想法和标识文本中targetparty的提示来生成思想特定的拆分，与人工编码分析类似。CGCoT将对比文本的对比问题转化为pattern recognition问题。然后，我们对每个拆分进行对比，使用LLM来对比拆分。我们使用这些对比结果来估算一个排名使用布莱德利-特里模型。我们使用这种方法来尺度Twitter上的情感语言。我们的度量与人类判断更高相关性，与替代方法如Wordfish相比。除了开发CGCoT提示的小数据集外，我们的度量不需要额外的标注数据，并且生成了与RoBERTa-Large模型 fine-tuned on thousands of human-labeled tweets相同的二进制预测。我们示例了如何将专业知识与LLM结合以创建状态的抽象概念度量。
</details></li>
</ul>
<hr>
<h2 id="CORE-A-Few-Shot-Company-Relation-Classification-Dataset-for-Robust-Domain-Adaptation"><a href="#CORE-A-Few-Shot-Company-Relation-Classification-Dataset-for-Robust-Domain-Adaptation" class="headerlink" title="CORE: A Few-Shot Company Relation Classification Dataset for Robust Domain Adaptation"></a>CORE: A Few-Shot Company Relation Classification Dataset for Robust Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12024">http://arxiv.org/abs/2310.12024</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pnborchert/core">https://github.com/pnborchert/core</a></li>
<li>paper_authors: Philipp Borchert, Jochen De Weerdt, Kristof Coussement, Arno De Caigny, Marie-Francine Moens</li>
<li>for: 这 paper 是关于 few-shot 关系分类 (RC) 的研究, 特点是使用公司wiki页面中的文本证据。</li>
<li>methods: 这 paper 使用了 state-of-the-art RC 模型在 few-shot domain adaptation  Setting 中进行了实验，以评估模型在 CORE  dataset 上的性能。</li>
<li>results: 实验结果表明，当前的 RC 模型在 CORE dataset 上 exhibits substantial performance gaps, 并且模型在不同的领域上适应性不高。  however, 模型在 CORE 上训练显示出了改善的 out-of-domain 性能, 这表明高质量数据的重要性 для robust domain adaptation。<details>
<summary>Abstract</summary>
We introduce CORE, a dataset for few-shot relation classification (RC) focused on company relations and business entities. CORE includes 4,708 instances of 12 relation types with corresponding textual evidence extracted from company Wikipedia pages. Company names and business entities pose a challenge for few-shot RC models due to the rich and diverse information associated with them. For example, a company name may represent the legal entity, products, people, or business divisions depending on the context. Therefore, deriving the relation type between entities is highly dependent on textual context. To evaluate the performance of state-of-the-art RC models on the CORE dataset, we conduct experiments in the few-shot domain adaptation setting. Our results reveal substantial performance gaps, confirming that models trained on different domains struggle to adapt to CORE. Interestingly, we find that models trained on CORE showcase improved out-of-domain performance, which highlights the importance of high-quality data for robust domain adaptation. Specifically, the information richness embedded in business entities allows models to focus on contextual nuances, reducing their reliance on superficial clues such as relation-specific verbs. In addition to the dataset, we provide relevant code snippets to facilitate reproducibility and encourage further research in the field.
</details>
<details>
<summary>摘要</summary>
我们介绍了CORE数据集，专门用于几个shot关系分类（RC），关注公司关系和企业实体。CORE包含4,708个实例，12种关系类型的文本证据，从公司Wikipedia页面中提取。公司名称和商业实体可能会带来很多挑战，因为它们可能会表示法律实体、产品、人员或业务部门，具体取决于上下文。因此，从文本上提取关系类型 между实体是很有所依赖的。为了评估现有RC模型在CORE数据集上的性能，我们在几个shot领域适应设置下进行了实验。我们的结果表明，模型在不同领域的学习后，很难适应CORE。但是，模型在CORE上进行训练后，在其他领域的表现有所提高，这反映了高质量数据的重要性，以及企业实体中嵌入的信息 ricness，使模型更加注重上下文特征，减少对关系特有词的依赖。此外，我们还提供了相关的代码截图，以便复现和进一步研究。
</details></li>
</ul>
<hr>
<h2 id="LoHoRavens-A-Long-Horizon-Language-Conditioned-Benchmark-for-Robotic-Tabletop-Manipulation"><a href="#LoHoRavens-A-Long-Horizon-Language-Conditioned-Benchmark-for-Robotic-Tabletop-Manipulation" class="headerlink" title="LoHoRavens: A Long-Horizon Language-Conditioned Benchmark for Robotic Tabletop Manipulation"></a>LoHoRavens: A Long-Horizon Language-Conditioned Benchmark for Robotic Tabletop Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12020">http://arxiv.org/abs/2310.12020</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengqiang Zhang, Philipp Wicke, Lütfi Kerem Şenel, Luis Figueredo, Abdeldjallil Naceri, Sami Haddadin, Barbara Plank, Hinrich Schütze</li>
<li>for: 本研究的目的是提供一个长期计划任务的公共库存 benchmark，以测试语言条件的机器人在不同情况下的长期推理能力。</li>
<li>methods: 本研究使用了两种方法来处理观察反馈：caption generation和learnable interface。</li>
<li>results: 实验结果显示，现有的两种方法在一些任务上都显示出问题，这表明长期 TABLETOP 推理任务仍然是现代具有问题。<details>
<summary>Abstract</summary>
The convergence of embodied agents and large language models (LLMs) has brought significant advancements to embodied instruction following. Particularly, the strong reasoning capabilities of LLMs make it possible for robots to perform long-horizon tasks without expensive annotated demonstrations. However, public benchmarks for testing the long-horizon reasoning capabilities of language-conditioned robots in various scenarios are still missing. To fill this gap, this work focuses on the tabletop manipulation task and releases a simulation benchmark, \textit{LoHoRavens}, which covers various long-horizon reasoning aspects spanning color, size, space, arithmetics and reference. Furthermore, there is a key modality bridging problem for long-horizon manipulation tasks with LLMs: how to incorporate the observation feedback during robot execution for the LLM's closed-loop planning, which is however less studied by prior work. We investigate two methods of bridging the modality gap: caption generation and learnable interface for incorporating explicit and implicit observation feedback to the LLM, respectively. These methods serve as the two baselines for our proposed benchmark. Experiments show that both methods struggle to solve some tasks, indicating long-horizon manipulation tasks are still challenging for current popular models. We expect the proposed public benchmark and baselines can help the community develop better models for long-horizon tabletop manipulation tasks.
</details>
<details>
<summary>摘要</summary>
<<SYS>> transtabletop manipulation task and releases a simulation benchmark, \textit{LoHoRavens}, which covers various long-horizon reasoning aspects spanning color, size, space, arithmetics and reference. Furthermore, there is a key modality bridging problem for long-horizon manipulation tasks with LLMs: how to incorporate the observation feedback during robot execution for the LLM's closed-loop planning, which is however less studied by prior work. We investigate two methods of bridging the modality gap: caption generation and learnable interface for incorporating explicit and implicit observation feedback to the LLM, respectively. These methods serve as the two baselines for our proposed benchmark. Experiments show that both methods struggle to solve some tasks, indicating long-horizon manipulation tasks are still challenging for current popular models. We expect the proposed public benchmark and baselines can help the community develop better models for long-horizon tabletop manipulation tasks.Note that "LoHoRavens" is a simulation benchmark, and "LLMs" stands for "large language models".
</details></li>
</ul>
<hr>
<h2 id="Gold-A-Global-and-Local-aware-Denoising-Framework-for-Commonsense-Knowledge-Graph-Noise-Detection"><a href="#Gold-A-Global-and-Local-aware-Denoising-Framework-for-Commonsense-Knowledge-Graph-Noise-Detection" class="headerlink" title="Gold: A Global and Local-aware Denoising Framework for Commonsense Knowledge Graph Noise Detection"></a>Gold: A Global and Local-aware Denoising Framework for Commonsense Knowledge Graph Noise Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12011">http://arxiv.org/abs/2310.12011</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hkust-knowcomp/gold">https://github.com/hkust-knowcomp/gold</a></li>
<li>paper_authors: Zheye Deng, Weiqi Wang, Zhaowei Wang, Xin Liu, Yangqiu Song</li>
<li>for: constructing high-quality Commonsense Knowledge Graphs (CSKGs) with larger semantic coverage</li>
<li>methods: incorporates entity semantic information, global rules, and local structural information from the CSKG</li>
<li>results: outperforms all baseline methods in noise detection tasks on synthetic noisy CSKG benchmarks, and benefits the downstream zero-shot commonsense question-answering task on a real-world CSKG<details>
<summary>Abstract</summary>
Commonsense Knowledge Graphs (CSKGs) are crucial for commonsense reasoning, yet constructing them through human annotations can be costly. As a result, various automatic methods have been proposed to construct CSKG with larger semantic coverage. However, these unsupervised approaches introduce spurious noise that can lower the quality of the resulting CSKG, which cannot be tackled easily by existing denoising algorithms due to the unique characteristics of nodes and structures in CSKGs. To address this issue, we propose Gold (Global and Local-aware Denoising), a denoising framework for CSKGs that incorporates entity semantic information, global rules, and local structural information from the CSKG. Experiment results demonstrate that Gold outperforms all baseline methods in noise detection tasks on synthetic noisy CSKG benchmarks. Furthermore, we show that denoising a real-world CSKG is effective and even benefits the downstream zero-shot commonsense question-answering task.
</details>
<details>
<summary>摘要</summary>
共享常识图（CSKG）是对常识理解的关键，但是通过人工标注可能会成本高。因此，多种自动方法已经被提议用于构建CSKG，以提高 semantic 覆盖率。然而，这些无监督方法会引入干扰噪声，这些噪声难以通过现有的噪声除除算法处理，因为CSKG 中节点和结构的特殊特征。为解决这个问题，我们提出了 Gold（全球和本地化噪声除法），一种特有CSKG噪声除法，该法利用实体 semantic 信息，全球规则和CSKG 本地结构信息。实验结果表明，Gold 在噪声检测任务中击败了所有基线方法。此外，我们还证明了对真实世界CSKG进行噪声除法有效，甚至对下游零shot常识问答任务有益。
</details></li>
</ul>
<hr>
<h2 id="From-Interpolation-to-Extrapolation-Complete-Length-Generalization-for-Arithmetic-Transformers"><a href="#From-Interpolation-to-Extrapolation-Complete-Length-Generalization-for-Arithmetic-Transformers" class="headerlink" title="From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers"></a>From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11984">http://arxiv.org/abs/2310.11984</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shaoxiongduan/attentionbiascalibration">https://github.com/shaoxiongduan/attentionbiascalibration</a></li>
<li>paper_authors: Shaoxiong Duan, Yining Shi</li>
<li>for:  investigate the inherent capabilities of transformer models in learning arithmetic algorithms, such as addition and multiplication.</li>
<li>methods:  through experiments and attention analysis, we identify a number of crucial factors for achieving optimal length generalization. we show that transformer models are able to generalize to long lengths with the help of targeted attention biasing.</li>
<li>results:  we demonstrate that using ABC, the transformer model can achieve unprecedented perfect length generalization on certain arithmetic tasks.<details>
<summary>Abstract</summary>
Since its introduction, the transformer model has demonstrated outstanding performance across various tasks. However, there are still unresolved issues regarding length generalization, particularly in algorithmic tasks. In this paper, we investigate the inherent capabilities of transformer models in learning arithmetic algorithms, such as addition and multiplication. Through experiments and attention analysis, we identify a number of crucial factors for achieving optimal length generalization. We show that transformer models are able to generalize to long lengths with the help of targeted attention biasing. We then introduce Attention Bias Calibration (ABC), a calibration stage that enables the model to automatically learn the proper attention biases, which we link to mechanisms in relative position encoding. We demonstrate that using ABC, the transformer model can achieve unprecedented perfect length generalization on certain arithmetic tasks.
</details>
<details>
<summary>摘要</summary>
自其引入以来，变换模型在不同任务中表现出色。然而，LENGTH总是一个尚未解决的问题，特别是在算法任务中。在这篇论文中，我们调查变换模型是否具备学习算术算法的能力，如加法和乘法。通过实验和注意力分析，我们确定了一些重要的因素，以实现最佳的长度总结。我们发现，变换模型可以通过targeted注意力偏好来总结到长 lengths。然后，我们引入了注意力偏好准备（ABC），一种准备阶段，它使得模型自动学习合适的注意力偏好，我们将其联系到相对位编码机制。我们示出，使用ABC，变换模型可以实现历史性的长度总结在某些算数任务中。
</details></li>
</ul>
<hr>
<h2 id="Filling-in-the-Gaps-Efficient-Event-Coreference-Resolution-using-Graph-Autoencoder-Networks"><a href="#Filling-in-the-Gaps-Efficient-Event-Coreference-Resolution-using-Graph-Autoencoder-Networks" class="headerlink" title="Filling in the Gaps: Efficient Event Coreference Resolution using Graph Autoencoder Networks"></a>Filling in the Gaps: Efficient Event Coreference Resolution using Graph Autoencoder Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11965">http://arxiv.org/abs/2310.11965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Loic De Langhe, Orphée De Clercq, Veronique Hoste</li>
<li>for: 本研究旨在提出一种新的和高效的事件核心参照解决方法（ECR），应用于低资源语言领域。</li>
<li>methods: 本研究使用图重建任务来结合深度 semantics embedding 和结构核心参照链知识，创造一种参数高效的图自编码器模型（GAE）。</li>
<li>results: 本研究在大规模的荷兰事件核心参照 korpus 上显著超过 классиical mention-pair 方法，以至于总分、效率和训练速度。此外，我们的模型能够更好地识别更难的核心参照链，并在低数据设置下显示出较高的Robustness。<details>
<summary>Abstract</summary>
We introduce a novel and efficient method for Event Coreference Resolution (ECR) applied to a lower-resourced language domain. By framing ECR as a graph reconstruction task, we are able to combine deep semantic embeddings with structural coreference chain knowledge to create a parameter-efficient family of Graph Autoencoder models (GAE). Our method significantly outperforms classical mention-pair methods on a large Dutch event coreference corpus in terms of overall score, efficiency and training speed. Additionally, we show that our models are consistently able to classify more difficult coreference links and are far more robust in low-data settings when compared to transformer-based mention-pair coreference algorithms.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的和高效的事件核心关系解决方法（ECR），应用于低资源语言领域。我们将ECR视为图像重建任务，可以结合深度 semantics embedding 和结构核心关系链知识，创建一种参数高效的图像自编码器模型（GAE）。我们的方法在荷兰事件核心 correlate 词汇库中表现出色，在总分、效率和训练速度方面均超过了经典的提及对方法。此外，我们还证明了我们的模型在低数据情况下能够更好地分类更难的核心关系链，并且在基于转换器的提及对方法中更加稳定。
</details></li>
</ul>
<hr>
<h2 id="AMR-Parsing-with-Causal-Hierarchical-Attention-and-Pointers"><a href="#AMR-Parsing-with-Causal-Hierarchical-Attention-and-Pointers" class="headerlink" title="AMR Parsing with Causal Hierarchical Attention and Pointers"></a>AMR Parsing with Causal Hierarchical Attention and Pointers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11964">http://arxiv.org/abs/2310.11964</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Lou, Kewei Tu</li>
<li>for: 本研究旨在提高简化型AMR分析器的性能，使其能够更好地考虑AMR图表示的结构本地性。</li>
<li>methods: 本研究提出了新的目标形式和模型，即 CHAP，它具有层次嵌入式注意力和指针机制，以便将结构 integrate 到 transformer 解码器中。</li>
<li>results: 实验表明，我们的模型在无额外数据的情况下，在四个benchmark中比基线模型表现出色，提高了性能。<details>
<summary>Abstract</summary>
Translation-based AMR parsers have recently gained popularity due to their simplicity and effectiveness. They predict linearized graphs as free texts, avoiding explicit structure modeling. However, this simplicity neglects structural locality in AMR graphs and introduces unnecessary tokens to represent coreferences. In this paper, we introduce new target forms of AMR parsing and a novel model, CHAP, which is equipped with causal hierarchical attention and the pointer mechanism, enabling the integration of structures into the Transformer decoder. We empirically explore various alternative modeling options. Experiments show that our model outperforms baseline models on four out of five benchmarks in the setting of no additional data.
</details>
<details>
<summary>摘要</summary>
听说过的AMR解析器在最近几年内受欢迎，因为它的简单和效果性。它预测了线性图，作为自由文本，避免了Explicit结构化。然而，这种简单性忽略了AMR图中的结构本地性，并且添加了不必要的标记来表示核心引用。在这篇论文中，我们介绍了新的AMR解析目标形式和一种新的模型，即 CHAP，它具有征识层次注意力和指针机制，使得Transformer解码器中的结构可以被集成。我们在不同的模型化选项上进行了实验，实验结果显示，我们的模型在无额外数据的情况下超过基eline模型在四个benchmark中。
</details></li>
</ul>
<hr>
<h2 id="Fast-Multipole-Attention-A-Divide-and-Conquer-Attention-Mechanism-for-Long-Sequences"><a href="#Fast-Multipole-Attention-A-Divide-and-Conquer-Attention-Mechanism-for-Long-Sequences" class="headerlink" title="Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for Long Sequences"></a>Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for Long Sequences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11960">http://arxiv.org/abs/2310.11960</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanming Kang, Giang Tran, Hans De Sterck</li>
<li>for: 提高Transformer模型在长序列上的性能，解决自我关注的quadratic复杂性问题。</li>
<li>methods: 使用分割策略和层次结构来减少自我关注的时间和内存复杂性，保持全局响应场。</li>
<li>results: 比其他高效注意力变体在中等规模数据集上表现更好，具有更大的内存大小和准确率。<details>
<summary>Abstract</summary>
Transformer-based models have achieved state-of-the-art performance in many areas. However, the quadratic complexity of self-attention with respect to the input length hinders the applicability of Transformer-based models to long sequences. To address this, we present Fast Multipole Attention, a new attention mechanism that uses a divide-and-conquer strategy to reduce the time and memory complexity of attention for sequences of length $n$ from $\mathcal{O}(n^2)$ to $\mathcal{O}(n \log n)$ or $O(n)$, while retaining a global receptive field. The hierarchical approach groups queries, keys, and values into $\mathcal{O}( \log n)$ levels of resolution, where groups at greater distances are increasingly larger in size and the weights to compute group quantities are learned. As such, the interaction between tokens far from each other is considered in lower resolution in an efficient hierarchical manner. The overall complexity of Fast Multipole Attention is $\mathcal{O}(n)$ or $\mathcal{O}(n \log n)$, depending on whether the queries are down-sampled or not. This multi-level divide-and-conquer strategy is inspired by fast summation methods from $n$-body physics and the Fast Multipole Method. We perform evaluation on autoregressive and bidirectional language modeling tasks and compare our Fast Multipole Attention model with other efficient attention variants on medium-size datasets. We find empirically that the Fast Multipole Transformer performs much better than other efficient transformers in terms of memory size and accuracy. The Fast Multipole Attention mechanism has the potential to empower large language models with much greater sequence lengths, taking the full context into account in an efficient, naturally hierarchical manner during training and when generating long sequences.
</details>
<details>
<summary>摘要</summary>
tranSformer-based models have achieved state-of-the-art performance in many areas, but the quadratic complexity of self-attention with respect to the input length limits their applicability to long sequences. To address this, we present Fast Multipole Attention, a new attention mechanism that uses a divide-and-conquer strategy to reduce the time and memory complexity of attention for sequences of length $n$ from $\mathcal{O}(n^2)$ to $\mathcal{O}(n \log n)$ or $O(n)$, while retaining a global receptive field. The hierarchical approach groups queries, keys, and values into $\mathcal{O}(\log n)$ levels of resolution, where groups at greater distances are increasingly larger in size and the weights to compute group quantities are learned. As such, the interaction between tokens far from each other is considered in lower resolution in an efficient hierarchical manner. The overall complexity of Fast Multipole Attention is $\mathcal{O}(n)$ or $\mathcal{O}(n \log n)$, depending on whether the queries are down-sampled or not. This multi-level divide-and-conquer strategy is inspired by fast summation methods from $n$-body physics and the Fast Multipole Method. We perform evaluation on autoregressive and bidirectional language modeling tasks and compare our Fast Multipole Attention model with other efficient attention variants on medium-size datasets. We find empirically that the Fast Multipole Transformer performs much better than other efficient transformers in terms of memory size and accuracy. The Fast Multipole Attention mechanism has the potential to empower large language models with much greater sequence lengths, taking the full context into account in an efficient, naturally hierarchical manner during training and when generating long sequences.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Emptying-the-Ocean-with-a-Spoon-Should-We-Edit-Models"><a href="#Emptying-the-Ocean-with-a-Spoon-Should-We-Edit-Models" class="headerlink" title="Emptying the Ocean with a Spoon: Should We Edit Models?"></a>Emptying the Ocean with a Spoon: Should We Edit Models?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11958">http://arxiv.org/abs/2310.11958</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuval Pinter, Michael Elhadad</li>
<li>for: 本研究质疑直接修改LLM生成中的实际错误是否能够成为一种系统性的解决方案。</li>
<li>methods: 本研究比较了三种类似 yet distinct的方法，即retrieval-based architectures, concept erasure methods和attribution methods，以解决LLM生成中的偏见和风险。</li>
<li>results: 研究发现，直接修改LLM模型不能被视为一种系统性的解决方案，而且可能会增加风险。在某些情况下，直接修改LLM模型可能会增加风险，而不是减少风险。<details>
<summary>Abstract</summary>
We call into question the recently popularized method of direct model editing as a means of correcting factual errors in LLM generations. We contrast model editing with three similar but distinct approaches that pursue better defined objectives: (1) retrieval-based architectures, which decouple factual memory from inference and linguistic capabilities embodied in LLMs; (2) concept erasure methods, which aim at preventing systemic bias in generated text; and (3) attribution methods, which aim at grounding generations into identified textual sources. We argue that direct model editing cannot be trusted as a systematic remedy for the disadvantages inherent to LLMs, and while it has proven potential in improving model explainability, it opens risks by reinforcing the notion that models can be trusted for factuality. We call for cautious promotion and application of model editing as part of the LLM deployment process, and for responsibly limiting the use cases of LLMs to those not relying on editing as a critical component.
</details>
<details>
<summary>摘要</summary>
我团队提出对直接模型编辑的方法进行批判，作为LLM生成中的错误纠正方法。我们将模型编辑与三种相似 yet distinct的方法进行对比：（1）检索型架构，它将知识存储和LLM中的语言能力分离开来；（2）概念消除方法，它们目的是避免生成文本中的系统偏见；以及（3）归因方法，它们强调将生成文本链接到特定的文本来源。我们认为直接模型编辑无法被视为LLM中的系统疾病纠正方法，尽管它在模型解释方面具有潜力。我们呼吁对模型编辑的推广和应用进行谨慎，并限制LLM的使用场景，以避免依赖于编辑的情况。
</details></li>
</ul>
<hr>
<h2 id="MusicAgent-An-AI-Agent-for-Music-Understanding-and-Generation-with-Large-Language-Models"><a href="#MusicAgent-An-AI-Agent-for-Music-Understanding-and-Generation-with-Large-Language-Models" class="headerlink" title="MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models"></a>MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11954">http://arxiv.org/abs/2310.11954</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/muzic">https://github.com/microsoft/muzic</a></li>
<li>paper_authors: Dingyao Yu, Kaitao Song, Peiling Lu, Tianyu He, Xu Tan, Wei Ye, Shikun Zhang, Jiang Bian</li>
<li>for: 帮助开发者和爱好者快速找到适合他们需求的音乐处理工具，减少了对不同音乐数据表示和模型之间的学习和应用的困难。</li>
<li>methods: 基于大语言模型（LLMs）自动化任务的技术，集成了多种音乐相关的工具，并自动组织了用户请求，将其分解成多个子任务，采用相应的音乐工具进行处理。</li>
<li>results: 提供了一个自动化的音乐处理系统，让用户可以快速地找到适合他们需求的音乐工具，减少了用户对音乐处理技术的学习压力，让用户更能专注于音乐创作。<details>
<summary>Abstract</summary>
AI-empowered music processing is a diverse field that encompasses dozens of tasks, ranging from generation tasks (e.g., timbre synthesis) to comprehension tasks (e.g., music classification). For developers and amateurs, it is very difficult to grasp all of these task to satisfy their requirements in music processing, especially considering the huge differences in the representations of music data and the model applicability across platforms among various tasks. Consequently, it is necessary to build a system to organize and integrate these tasks, and thus help practitioners to automatically analyze their demand and call suitable tools as solutions to fulfill their requirements. Inspired by the recent success of large language models (LLMs) in task automation, we develop a system, named MusicAgent, which integrates numerous music-related tools and an autonomous workflow to address user requirements. More specifically, we build 1) toolset that collects tools from diverse sources, including Hugging Face, GitHub, and Web API, etc. 2) an autonomous workflow empowered by LLMs (e.g., ChatGPT) to organize these tools and automatically decompose user requests into multiple sub-tasks and invoke corresponding music tools. The primary goal of this system is to free users from the intricacies of AI-music tools, enabling them to concentrate on the creative aspect. By granting users the freedom to effortlessly combine tools, the system offers a seamless and enriching music experience.
</details>
<details>
<summary>摘要</summary>
人工智能 empowered 音乐处理是一个多样化的领域，包括多种任务，例如生成任务（如 timbre 合成）和理解任务（如音乐分类）。为开发者和爱好者而言，抓住这些任务的要求非常困难，尤其是在音乐数据表示和模型在不同平台之间的差异非常大。因此，需要建立一个系统来组织和集成这些任务，以帮助实践者自动分析他们的需求，并选择适合的工具来满足他们的要求。受大语言模型（LLM）的成功启发，我们开发了一个名为 MusicAgent 的系统，它集成了多种音乐相关的工具和一个自动化的工作流程，以解决用户的需求。更具体来说，我们建立了以下两个部分：1. 工具集，收集了来自多种源，包括 Hugging Face、GitHub 和 Web API 等等的工具。2. 由 LLM（如 ChatGPT） empowered 的自动化工作流程，用于组织这些工具，并自动将用户的请求分解成多个子任务，并对应的邀请合适的音乐工具。MusicAgent 系统的Primary Goal 是免除用户对 AI-音乐工具的繁琐，让他们可以专注于创作。通过让用户轻松地组合工具，系统提供了一个无缝和丰富的音乐体验。
</details></li>
</ul>
<hr>
<h2 id="Grounded-and-Well-rounded-A-Methodological-Approach-to-the-Study-of-Cross-modal-and-Cross-lingual-Grounding"><a href="#Grounded-and-Well-rounded-A-Methodological-Approach-to-the-Study-of-Cross-modal-and-Cross-lingual-Grounding" class="headerlink" title="Grounded and Well-rounded: A Methodological Approach to the Study of Cross-modal and Cross-lingual Grounding"></a>Grounded and Well-rounded: A Methodological Approach to the Study of Cross-modal and Cross-lingual Grounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11938">http://arxiv.org/abs/2310.11938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Timothee Mickus, Elaine Zosa, Denis Paperno</li>
<li>for: 这个论文的目的是研究语义背景的影响在人工智能系统中，以及不同输入模式的效果。</li>
<li>methods: 这篇论文使用了一种方法学框架，用于研究不同输入模式对模型的影响。这个框架包括建立可比较的样本集，以便分析不同输入模式对模型的表现的质量。</li>
<li>results: 实验结果表明，提供不同的输入模式可以导致模型的不同行为，包括跨模式背景、跨语言背景和未grounded模型的不同行为。这些行为的差异可以在全数据集水平和特定词表示水平上被衡量。<details>
<summary>Abstract</summary>
Grounding has been argued to be a crucial component towards the development of more complete and truly semantically competent artificial intelligence systems. Literature has divided into two camps: While some argue that grounding allows for qualitatively different generalizations, others believe it can be compensated by mono-modal data quantity. Limited empirical evidence has emerged for or against either position, which we argue is due to the methodological challenges that come with studying grounding and its effects on NLP systems.   In this paper, we establish a methodological framework for studying what the effects are - if any - of providing models with richer input sources than text-only. The crux of it lies in the construction of comparable samples of populations of models trained on different input modalities, so that we can tease apart the qualitative effects of different input sources from quantifiable model performances. Experiments using this framework reveal qualitative differences in model behavior between cross-modally grounded, cross-lingually grounded, and ungrounded models, which we measure both at a global dataset level as well as for specific word representations, depending on how concrete their semantics is.
</details>
<details>
<summary>摘要</summary>
文本背景是人工智能系统的重要组成部分，有一些研究者认为它可以帮助系统实现更完整和具有真正含义的语言理解能力。文献被分为两个派别：一些人认为，背景可以带来不同的普遍化，而另一些人则认为，它可以通过大量单Modal数据补做。然而，有限的实验证据已经出现了，支持或反对任一位置。在这篇论文中，我们提出了一种方法ológical框架，用于研究不同输入模式对NLP系统的效果。我们 constructed comparable samples of populations of models trained on different input modalities，以便分离不同输入源的qualitative效果和可衡量的模型性能。实验结果显示，在不同的语言和modalities中训练的模型 exhibit 不同的行为，我们在全数据集级别以及特定词表示性的方面进行了测量。
</details></li>
</ul>
<hr>
<h2 id="Investigating-semantic-subspaces-of-Transformer-sentence-embeddings-through-linear-structural-probing"><a href="#Investigating-semantic-subspaces-of-Transformer-sentence-embeddings-through-linear-structural-probing" class="headerlink" title="Investigating semantic subspaces of Transformer sentence embeddings through linear structural probing"></a>Investigating semantic subspaces of Transformer sentence embeddings through linear structural probing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11923">http://arxiv.org/abs/2310.11923</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/macleginn/semantic-subspaces-code">https://github.com/macleginn/semantic-subspaces-code</a></li>
<li>paper_authors: Dmitry Nikolaev, Sebastian Padó</li>
<li>for: 研究Transformer模型中不同层次的语言信息编码方法</li>
<li>methods: 使用semantic structural probing方法研究语言模型的句子级表示</li>
<li>results: 发现不同模型家族和模型大小具有不同的表现和层次动态，但表现相对较具同样性<details>
<summary>Abstract</summary>
The question of what kinds of linguistic information are encoded in different layers of Transformer-based language models is of considerable interest for the NLP community. Existing work, however, has overwhelmingly focused on word-level representations and encoder-only language models with the masked-token training objective. In this paper, we present experiments with semantic structural probing, a method for studying sentence-level representations via finding a subspace of the embedding space that provides suitable task-specific pairwise distances between data-points. We apply our method to language models from different families (encoder-only, decoder-only, encoder-decoder) and of different sizes in the context of two tasks, semantic textual similarity and natural-language inference. We find that model families differ substantially in their performance and layer dynamics, but that the results are largely model-size invariant.
</details>
<details>
<summary>摘要</summary>
研究各种语言模型层次的语言信息编码是NLPT社区中的一个非常有趣的问题。现有的工作 however，主要集中在单词水平表示和基于encoder-only语言模型的伪token训练目标上。在这篇论文中，我们进行了叙述结构探索，一种研究句子级别表示的方法，通过找到 embedding空间中任务特定的数据点对之间的适当对比距离来实现。我们将这种方法应用于不同家族（encoder-only、decoder-only、encoder-decoder）和不同大小的语言模型中，并在两个任务（ semantics 文本相似性和自然语言推理）的上下文中进行了测试。我们发现，模型家族之间存在巨大差异，但结果几乎是模型大小不变的。
</details></li>
</ul>
<hr>
<h2 id="Rather-a-Nurse-than-a-Physician-–-Contrastive-Explanations-under-Investigation"><a href="#Rather-a-Nurse-than-a-Physician-–-Contrastive-Explanations-under-Investigation" class="headerlink" title="Rather a Nurse than a Physician – Contrastive Explanations under Investigation"></a>Rather a Nurse than a Physician – Contrastive Explanations under Investigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11906">http://arxiv.org/abs/2310.11906</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oliver Eberle, Ilias Chalkidis, Laura Cabello, Stephanie Brandl</li>
<li>For: The paper aims to investigate the claim that contrastive explanations are closer to human explanations than non-contrastive explanations.* Methods: The paper uses four English text-classification datasets and fine-tunes three different models (RoBERTa, GTP-2, and T5) in three different sizes. It also applies three post-hoc explainability methods (LRP, GradientxInput, and GradNorm) to extract explanations.* Results: The paper finds that there is a high agreement between model-based rationales and human annotations, both in contrastive and non-contrastive settings. Additionally, model-based explanations computed in both settings align equally well with human rationales, indicating that humans do not necessarily explain in a contrastive manner.<details>
<summary>Abstract</summary>
Contrastive explanations, where one decision is explained in contrast to another, are supposed to be closer to how humans explain a decision than non-contrastive explanations, where the decision is not necessarily referenced to an alternative. This claim has never been empirically validated. We analyze four English text-classification datasets (SST2, DynaSent, BIOS and DBpedia-Animals). We fine-tune and extract explanations from three different models (RoBERTa, GTP-2, and T5), each in three different sizes and apply three post-hoc explainability methods (LRP, GradientxInput, GradNorm). We furthermore collect and release human rationale annotations for a subset of 100 samples from the BIOS dataset for contrastive and non-contrastive settings. A cross-comparison between model-based rationales and human annotations, both in contrastive and non-contrastive settings, yields a high agreement between the two settings for models as well as for humans. Moreover, model-based explanations computed in both settings align equally well with human rationales. Thus, we empirically find that humans do not necessarily explain in a contrastive manner.9 pages, long paper at ACL 2022 proceedings.
</details>
<details>
<summary>摘要</summary>
“对比性解释”，即将一个决策解释为另一个决策的对比，被认为更接近人类的解释方式。然而，这一laim未经验证。我们分析了四个英文文本分类 dataset（SST2、DynaSent、BIOS和DBpedia-Animals），使用三种不同的模型（RoBERTa、GTP-2和T5），每种模型都有三个不同的大小，并应用三种后处 explainability 方法（LRP、GradientxInput和GradNorm）。此外，我们还收集并发布了BIOS dataset中的一百个样本的人类理由标注，用于对比和非对比设置。我们在这两种设置下进行了模型基于的理由和人类理由的交叉比较，发现两者之间存在高度一致性，并且模型基于的解释在两种设置下均与人类理由相吻合。因此，我们employm empirical研究发现，人类并不一定会在对比性下进行解释。Please note that the translation is in Simplified Chinese, and some words or phrases may have been translated differently in Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="From-Dissonance-to-Insights-Dissecting-Disagreements-in-Rationale-Construction-for-Case-Outcome-Classification"><a href="#From-Dissonance-to-Insights-Dissecting-Disagreements-in-Rationale-Construction-for-Case-Outcome-Classification" class="headerlink" title="From Dissonance to Insights: Dissecting Disagreements in Rationale Construction for Case Outcome Classification"></a>From Dissonance to Insights: Dissecting Disagreements in Rationale Construction for Case Outcome Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11878">http://arxiv.org/abs/2310.11878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shanshan Xu, T. Y. S. S Santosh, Oana Ichim, Isabella Risini, Barbara Plank, Matthias Grabmair</li>
<li>for: 法律自然语言处理领域的案例结果分类（COC）需要不仅准确，还需要可信和可解释。现有的解释COC工作受限于单个专家的标注。</li>
<li>methods: 我们采集了一个新的数据集RAVE：Rationale Variation in ECHR1，该数据集由两名国际人权法律领域专家标注而成，我们观察到了这两个专家之间的弱一致。我们研究了他们的不一致，并构建了两级独立任务的分类法，补充了COC特有的亚分类。这是法律自然语言处理领域中第一次关于人类标注变化的研究。</li>
<li>results: 我们量测不同分类类别的数据，发现主要的不一致来自于法律上下文的不充分规定，这种情况通常具有有限的精度和噪音。我们进一步评估了当今最佳COC模型在RAVE上的解释性，发现模型和专家之间的一致度有限。总之，我们的案例研究暴露了在法律自然语言处理领域创建标准数据集的复杂性，这些复杂性包括确定案例中 факт的重要性。<details>
<summary>Abstract</summary>
In legal NLP, Case Outcome Classification (COC) must not only be accurate but also trustworthy and explainable. Existing work in explainable COC has been limited to annotations by a single expert. However, it is well-known that lawyers may disagree in their assessment of case facts. We hence collect a novel dataset RAVE: Rationale Variation in ECHR1, which is obtained from two experts in the domain of international human rights law, for whom we observe weak agreement. We study their disagreements and build a two-level task-independent taxonomy, supplemented with COC-specific subcategories. To our knowledge, this is the first work in the legal NLP that focuses on human label variation. We quantitatively assess different taxonomy categories and find that disagreements mainly stem from underspecification of the legal context, which poses challenges given the typically limited granularity and noise in COC metadata. We further assess the explainablility of SOTA COC models on RAVE and observe limited agreement between models and experts. Overall, our case study reveals hitherto underappreciated complexities in creating benchmark datasets in legal NLP that revolve around identifying aspects of a case's facts supposedly relevant to its outcome.
</details>
<details>
<summary>摘要</summary>
法律自然语言处理（NLP）中的案例结果分类（COC）不仅需要准确，还需要可信和可解释。现有的可解释COC工作都是由单一专家进行标注。然而，法律专业人员在评估案例事实时可能会有差异。因此，我们收集了一个新的数据集RAVE：可理解变化在人权法院1中，该数据集来自两个国际人权法律领域专家，我们观察到了弱一致。我们研究了他们的不一致，并建立了两级无关任务的税onomy，补充了COC特有的亚类。我们知道，这是法律NLP中第一个关注人类标注变化的工作。我们量测不同税onomy类别，并发现，不一致主要来自法律Context的不足，这种情况在COC元数据中通常具有有限的精度和噪音。我们进一步评估了现有最佳COC模型在RAVE上的解释性，并发现模型和专家之间的一致不高。总的来说，我们的案例研究发现了法律NLP中创建benchmark数据集的复杂性，即确定案例事实中可能对结果的影响因素。
</details></li>
</ul>
<hr>
<h2 id="The-Curious-Case-of-Hallucinatory-Unanswerablity-Finding-Truths-in-the-Hidden-States-of-Over-Confident-Large-Language-Models"><a href="#The-Curious-Case-of-Hallucinatory-Unanswerablity-Finding-Truths-in-the-Hidden-States-of-Over-Confident-Large-Language-Models" class="headerlink" title="The Curious Case of Hallucinatory Unanswerablity: Finding Truths in the Hidden States of Over-Confident Large Language Models"></a>The Curious Case of Hallucinatory Unanswerablity: Finding Truths in the Hidden States of Over-Confident Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11877">http://arxiv.org/abs/2310.11877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aviv Slobodkin, Omer Goldman, Avi Caciularu, Ido Dagan, Shauli Ravfogel</li>
<li>for:  investigate the behavior of LLMs when presented with unanswerable queries</li>
<li>methods:  use a combination of human evaluation and automated metrics to study the representation of answerability in LLMs’ latent spaces</li>
<li>results:  find strong indications that LLMs encode the answerability of input queries, with the representation of the first decoded token often being a strong indicator, which can be used to develop improved decoding techniques for factual generation.Here’s the full translation in Simplified Chinese:</li>
<li>for: 这篇论文旨在研究 LLMs 当面对不可答案问题时的行为。</li>
<li>methods: 使用人工评审和自动度量来研究 LLMs 的秘密空间中的问题可answerability 表示。</li>
<li>results: 发现 LLMs 对输入问题的表示中具有强度的问题可answerability 表示，首个解码token 的表示frequently 是强度表示。这些发现可以用来发展更好的实际生成技术，特别在问题可answerability 是一个应对的情况下。<details>
<summary>Abstract</summary>
Large language models (LLMs) have been shown to possess impressive capabilities, while also raising crucial concerns about the faithfulness of their responses. A primary issue arising in this context is the management of unanswerable queries by LLMs, which often results in hallucinatory behavior, due to overconfidence. In this paper, we explore the behavior of LLMs when presented with unanswerable queries. We ask: do models \textbf{represent} the fact that the question is unanswerable when generating a hallucinatory answer? Our results show strong indications that such models encode the answerability of an input query, with the representation of the first decoded token often being a strong indicator. These findings shed new light on the spatial organization within the latent representations of LLMs, unveiling previously unexplored facets of these models. Moreover, they pave the way for the development of improved decoding techniques with better adherence to factual generation, particularly in scenarios where query unanswerability is a concern.
</details>
<details>
<summary>摘要</summary>
Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. The translation is based on the original text provided, and it is not a word-for-word translation. Some phrases and sentences may be rephrased or condensed to improve readability and clarity.
</details></li>
</ul>
<hr>
<h2 id="Text-Annotation-Handbook-A-Practical-Guide-for-Machine-Learning-Projects"><a href="#Text-Annotation-Handbook-A-Practical-Guide-for-Machine-Learning-Projects" class="headerlink" title="Text Annotation Handbook: A Practical Guide for Machine Learning Projects"></a>Text Annotation Handbook: A Practical Guide for Machine Learning Projects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11780">http://arxiv.org/abs/2310.11780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Felix Stollenwerk, Joey Öhman, Danila Petrelli, Emma Wallerö, Fredrik Olsson, Camilla Bengtsson, Andreas Horndahl, Gabriela Zarzar Gandler</li>
<li>for: 这份手册是一本关于文本标注任务的实用指南，用于介绍基本概念和实践技巧。</li>
<li>methods: 本文涉及了主要的技术方面，同时也触及了商业、伦理和法规问题。</li>
<li>results: 文件的重点是在于可读性和简洁性，而不是完整性和科学准确性。该手册可能会用于各种职业，如团队领导、项目经理、IT архитек、软件开发者和机器学习工程师。<details>
<summary>Abstract</summary>
This handbook is a hands-on guide on how to approach text annotation tasks. It provides a gentle introduction to the topic, an overview of theoretical concepts as well as practical advice. The topics covered are mostly technical, but business, ethical and regulatory issues are also touched upon. The focus lies on readability and conciseness rather than completeness and scientific rigor. Experience with annotation and knowledge of machine learning are useful but not required. The document may serve as a primer or reference book for a wide range of professions such as team leaders, project managers, IT architects, software developers and machine learning engineers.
</details>
<details>
<summary>摘要</summary>
这本手册是一本实用的文本标注任务指南。它提供了一个温顺的引导，覆盖了理论概念以及实践建议。覆盖的主题主要是技术性的，但也涉及到业务、伦理和法规问题。文本的重点是易读性和简洁性，而不是完整性和科学严谨性。经验与标注和机器学习知识可能有助于，但并非必需。这份文档可能作为团队领导、项目经理、IT建筑师、软件开发者和机器学习工程师的引导或参考书。
</details></li>
</ul>
<hr>
<h2 id="Language-Agents-for-Detecting-Implicit-Stereotypes-in-Text-to-image-Models-at-Scale"><a href="#Language-Agents-for-Detecting-Implicit-Stereotypes-in-Text-to-image-Models-at-Scale" class="headerlink" title="Language Agents for Detecting Implicit Stereotypes in Text-to-image Models at Scale"></a>Language Agents for Detecting Implicit Stereotypes in Text-to-image Models at Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11778">http://arxiv.org/abs/2310.11778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qichao Wang, Tian Bian, Yian Yin, Tingyang Xu, Hong Cheng, Helen M. Meng, Zibin Zheng, Liang Chen, Bingzhe Wu</li>
<li>for: 检测文本到图像模型中的刻板印象</li>
<li>methods: 提出了一种新的代理体系，可以自动适应免式检测任务，并可以通过生成相关说明和图像来检测刻板印象。</li>
<li>results: 研究发现，一些商业产品和开源文本到图像模型中的模型经常在某些提示中表现出严重的刻板印象，这些刻板印象与人类的性别、种族和宗教等社会维度有关。<details>
<summary>Abstract</summary>
The recent surge in the research of diffusion models has accelerated the adoption of text-to-image models in various Artificial Intelligence Generated Content (AIGC) commercial products. While these exceptional AIGC products are gaining increasing recognition and sparking enthusiasm among consumers, the questions regarding whether, when, and how these models might unintentionally reinforce existing societal stereotypes remain largely unaddressed. Motivated by recent advancements in language agents, here we introduce a novel agent architecture tailored for stereotype detection in text-to-image models. This versatile agent architecture is capable of accommodating free-form detection tasks and can autonomously invoke various tools to facilitate the entire process, from generating corresponding instructions and images, to detecting stereotypes. We build the stereotype-relevant benchmark based on multiple open-text datasets, and apply this architecture to commercial products and popular open source text-to-image models. We find that these models often display serious stereotypes when it comes to certain prompts about personal characteristics, social cultural context and crime-related aspects. In summary, these empirical findings underscore the pervasive existence of stereotypes across social dimensions, including gender, race, and religion, which not only validate the effectiveness of our proposed approach, but also emphasize the critical necessity of addressing potential ethical risks in the burgeoning realm of AIGC. As AIGC continues its rapid expansion trajectory, with new models and plugins emerging daily in staggering numbers, the challenge lies in the timely detection and mitigation of potential biases within these models.
</details>
<details>
<summary>摘要</summary>
现在的研究强化模型在人工智能生成内容（AIGC）的商业产品中得到了加速。而这些优秀的AIGC产品正在获得消费者的认可，并让人们很感到激动。然而，关于这些模型是否无意中强化社会刻板印象的问题仍然未得到解决。我们在语言代理的最新进展基础上，提出了一种适用于刻板印象检测的新型代理体系。这种多功能的代理体系可以自动采用多种工具来执行整个过程，从生成相应的指令和图像到检测刻板印象。我们基于多个开源文本数据集建立了刻板印象相关的 benchmark，并应用这种体系到商业产品和流行的开源文本到图像模型中。我们发现，这些模型在某些个人特征、社会文化背景和犯罪相关的提示下显示了严重的刻板印象。总之，这些实验结果表明了社会各个维度上的刻板印象的普遍存在，包括 gender、race 和 religion 等，这不仅证明了我们的提出的方法的有效性，而且强调了在AIGC领域的可能性潜在风险的紧迫性。随着AIGC不断扩展，新的模型和插件每天都在各种数字平台上出现，因此，检测和 mitigate 这些模型中的潜在偏见的挑战在继续增长。
</details></li>
</ul>
<hr>
<h2 id="Improving-Long-Document-Topic-Segmentation-Models-With-Enhanced-Coherence-Modeling"><a href="#Improving-Long-Document-Topic-Segmentation-Models-With-Enhanced-Coherence-Modeling" class="headerlink" title="Improving Long Document Topic Segmentation Models With Enhanced Coherence Modeling"></a>Improving Long Document Topic Segmentation Models With Enhanced Coherence Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11772">http://arxiv.org/abs/2310.11772</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alibaba-damo-academy/spokennlp">https://github.com/alibaba-damo-academy/spokennlp</a></li>
<li>paper_authors: Hai Yu, Chong Deng, Qinglin Zhang, Jiaqing Liu, Qian Chen, Wen Wang</li>
<li>for: 提高长文档主题分割性能，强化supervised模型捕捉凝合度信息。</li>
<li>methods: 提出了Topic-aware Sentence Structure Prediction (TSSP)和Contrastive Semantic Similarity Learning (CSSL)两种方法，用于强化supervised模型对凝合度信息的捕捉。</li>
<li>results: 对比旧状态之前方法，Longformer具有我们提出的方法在WIKI-727K上显著提高了$F_1$值（73.74 -&gt; 77.16），并在WikiSection上实现了平均相对减少$P_k$值（15.0 -&gt; 13.89），两个数据集上的average相对减少$P_k$值为4.3%。<details>
<summary>Abstract</summary>
Topic segmentation is critical for obtaining structured documents and improving downstream tasks such as information retrieval. Due to its ability of automatically exploring clues of topic shift from abundant labeled data, recent supervised neural models have greatly promoted the development of long document topic segmentation, but leaving the deeper relationship between coherence and topic segmentation underexplored. Therefore, this paper enhances the ability of supervised models to capture coherence from both logical structure and semantic similarity perspectives to further improve the topic segmentation performance, proposing Topic-aware Sentence Structure Prediction (TSSP) and Contrastive Semantic Similarity Learning (CSSL). Specifically, the TSSP task is proposed to force the model to comprehend structural information by learning the original relations between adjacent sentences in a disarrayed document, which is constructed by jointly disrupting the original document at topic and sentence levels. Moreover, we utilize inter- and intra-topic information to construct contrastive samples and design the CSSL objective to ensure that the sentences representations in the same topic have higher similarity, while those in different topics are less similar. Extensive experiments show that the Longformer with our approach significantly outperforms old state-of-the-art (SOTA) methods. Our approach improve $F_1$ of old SOTA by 3.42 (73.74 -> 77.16) and reduces $P_k$ by 1.11 points (15.0 -> 13.89) on WIKI-727K and achieves an average relative reduction of 4.3% on $P_k$ on WikiSection. The average relative $P_k$ drop of 8.38% on two out-of-domain datasets also demonstrates the robustness of our approach.
</details>
<details>
<summary>摘要</summary>
Topic segmentation是文档结构化的关键之一，可以提高后续任务的信息检索性能。由于自动找到话题转换的灵活信息，最近的supervised神经网络模型在长文档话题分 segmentation方面取得了 significanth� development，但是它们之间的coherence关系还未得到充分探讨。因此，这篇论文提出了一种能够更好地捕捉coherence的方法，包括话题感知sentence结构预测（TSSP）和semantic similarity学习（CSSL）。具体来说，我们提出了TSSP任务，强制模型理解文档中的结构信息，通过学习原始文档中的关系来构建不规则的文档。此外，我们利用 между话题和同话题信息来构建对比采样，并设计了CSSL目标，确保同话题中的句子表示更加相似，而不同话题中的句子表示更加不相似。我们对Longformer模型进行了广泛的实验，并证明了我们的方法可以明显超越老的SOTA方法。我们的方法可以提高老SOTA的$F_1$指标的值，从73.74提高到77.16，并将15.0提高到13.89。此外，我们在WikiSection上 achieve了平均 относи于$P_k$指标的减少4.3%，并在两个out-of-domain数据集上实现了平均相对减少8.38%。这些结果表明我们的方法具有较好的Robustness性。
</details></li>
</ul>
<hr>
<h2 id="Annotated-Job-Ads-with-Named-Entity-Recognition"><a href="#Annotated-Job-Ads-with-Named-Entity-Recognition" class="headerlink" title="Annotated Job Ads with Named Entity Recognition"></a>Annotated Job Ads with Named Entity Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11769">http://arxiv.org/abs/2310.11769</a></li>
<li>repo_url: None</li>
<li>paper_authors: Felix Stollenwerk, Niklas Fastlund, Anna Nyqvist, Joey Öhman</li>
<li>for: 本研究是为了开发一个可靠的瑞典employmnetJob advertisement中的各种有用信息（例如，工作者需要具备的技能）的Named Entity Recognition（NER）模型。</li>
<li>methods: 我们使用了KB-BERT进行微调，并采用了一些方法来减少人工标注的困难，包括自动生成标注数据集和人工审核。</li>
<li>results: 我们对模型的性能进行了报告，并证明了模型的可靠性和高效性。<details>
<summary>Abstract</summary>
We have trained a named entity recognition (NER) model that screens Swedish job ads for different kinds of useful information (e.g. skills required from a job seeker). It was obtained by fine-tuning KB-BERT. The biggest challenge we faced was the creation of a labelled dataset, which required manual annotation. This paper gives an overview of the methods we employed to make the annotation process more efficient and to ensure high quality data. We also report on the performance of the resulting model.
</details>
<details>
<summary>摘要</summary>
我们已经训练了一个Named Entity Recognition（NER）模型，用于检测瑞典寻求人员岗位各种有用信息（例如，岗位需求的技能）。它是通过精度调整KB-BERT获得的。我们最大的挑战是创建标注数据集，需要人工标注。本文介绍了我们采用的方法，以提高标注过程的效率和数据质量。我们还对结果模型的性能进行了报告。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Evaluation-of-Large-Language-Models-on-Legal-Judgment-Prediction"><a href="#A-Comprehensive-Evaluation-of-Large-Language-Models-on-Legal-Judgment-Prediction" class="headerlink" title="A Comprehensive Evaluation of Large Language Models on Legal Judgment Prediction"></a>A Comprehensive Evaluation of Large Language Models on Legal Judgment Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11761">http://arxiv.org/abs/2310.11761</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/srhthu/lm-compeval-legal">https://github.com/srhthu/lm-compeval-legal</a></li>
<li>paper_authors: Ruihao Shui, Yixin Cao, Xiang Wang, Tat-Seng Chua</li>
<li>for: 研究大语言模型在法律领域的实际应用性。</li>
<li>methods: 基于大语言模型的实用基线解决方案，包括独立回答开放问题和与信息检索系统协作解决简化多选问题。</li>
<li>results: 研究表明，在提供类案例和多选选项的情况下，大语言模型可以更好地回忆域知识，但是如果 IR 系统的能力较强，则 LLM 的作用变得 redundante。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated great potential for domain-specific applications, such as the law domain. However, recent disputes over GPT-4's law evaluation raise questions concerning their performance in real-world legal tasks. To systematically investigate their competency in the law, we design practical baseline solutions based on LLMs and test on the task of legal judgment prediction. In our solutions, LLMs can work alone to answer open questions or coordinate with an information retrieval (IR) system to learn from similar cases or solve simplified multi-choice questions. We show that similar cases and multi-choice options, namely label candidates, included in prompts can help LLMs recall domain knowledge that is critical for expertise legal reasoning. We additionally present an intriguing paradox wherein an IR system surpasses the performance of LLM+IR due to limited gains acquired by weaker LLMs from powerful IR systems. In such cases, the role of LLMs becomes redundant. Our evaluation pipeline can be easily extended into other tasks to facilitate evaluations in other domains. Code is available at https://github.com/srhthu/LM-CompEval-Legal
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）在专业应用中展示了很大的潜力，例如法律领域。然而，最近有关GPT-4的法律评估争议引起了对它们在实际法律任务中的表现的质疑。为了系统地探索它们在法律中的能力，我们设计了实用的基线解决方案，并使用LLMs进行法律判断预测任务的评估。在我们的解决方案中，LLMs可以单独回答开问题，或与信息检索（IR）系统配合，从相似的案例或解决简单多选题中学习。我们发现，在提示中包含的相似案例和多选题（ Label Candidates）可以帮助LLMs回传专业知识，并且我们还发现了一个有趣的 contradicton，即在IR系统超越了LLM+IR的性能时，LLMs的角色变得redundant。我们的评估管线可以轻松地扩展到其他任务，以便在其他领域进行评估。代码可以在 GitHub 上获取：https://github.com/srhthu/LM-CompEval-Legal。
</details></li>
</ul>
<hr>
<h2 id="Bias-in-Emotion-Recognition-with-ChatGPT"><a href="#Bias-in-Emotion-Recognition-with-ChatGPT" class="headerlink" title="Bias in Emotion Recognition with ChatGPT"></a>Bias in Emotion Recognition with ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11753">http://arxiv.org/abs/2310.11753</a></li>
<li>repo_url: None</li>
<li>paper_authors: Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, Katsushi Ikeuchi</li>
<li>for: 这个技术报告探讨了基于文本的情感识别，这可以提供基于交互聊天机器人、数据注释和心理健康分析等应用的基础。</li>
<li>methods: 这个研究使用了ChatGPT进行情感识别，并进行了不同的数据集和情感标签的实验来评估其性能。</li>
<li>results: 研究发现，通过精度调整可以提高ChatGPT的情感识别性能，但是不同的情感标签和数据集的选择会影响ChatGPT的情感识别性能，表明了存在内在的不稳定性和可能的偏见。<details>
<summary>Abstract</summary>
This technical report explores the ability of ChatGPT in recognizing emotions from text, which can be the basis of various applications like interactive chatbots, data annotation, and mental health analysis. While prior research has shown ChatGPT's basic ability in sentiment analysis, its performance in more nuanced emotion recognition is not yet explored. Here, we conducted experiments to evaluate its performance of emotion recognition across different datasets and emotion labels. Our findings indicate a reasonable level of reproducibility in its performance, with noticeable improvement through fine-tuning. However, the performance varies with different emotion labels and datasets, highlighting an inherent instability and possible bias. The choice of dataset and emotion labels significantly impacts ChatGPT's emotion recognition performance. This paper sheds light on the importance of dataset and label selection, and the potential of fine-tuning in enhancing ChatGPT's emotion recognition capabilities, providing a groundwork for better integration of emotion analysis in applications using ChatGPT.
</details>
<details>
<summary>摘要</summary>
这份技术报告探讨了 chatGPT 在文本中识别情感的能力，这可以为互动 chatbot、数据标注和心理健康分析等应用提供基础。 although prior research has shown chatGPT 的基本情感分析能力，其在更复杂的情感识别方面的性能未经探讨。 在这里，我们进行了实验来评估 chatGPT 在不同的数据集和情感标签下的表现。 我们发现了一定的可重复性，通过微调可以得到明显的改进。 然而，不同的情感标签和数据集的表现差异明显，这 highlights the inherent instability and possible bias. 选择数据集和情感标签对 chatGPT 的情感识别表现有着重要的影响。 这篇文章强调了数据集和标签选择的重要性，以及微调的潜在作用，为将来更好地将情感分析 integrate into applications using chatGPT 提供了基础。
</details></li>
</ul>
<hr>
<h2 id="Investigating-Uncertainty-Calibration-of-Aligned-Language-Models-under-the-Multiple-Choice-Setting"><a href="#Investigating-Uncertainty-Calibration-of-Aligned-Language-Models-under-the-Multiple-Choice-Setting" class="headerlink" title="Investigating Uncertainty Calibration of Aligned Language Models under the Multiple-Choice Setting"></a>Investigating Uncertainty Calibration of Aligned Language Models under the Multiple-Choice Setting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11732">http://arxiv.org/abs/2310.11732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guande He, Peng Cui, Jianfei Chen, Wenbo Hu, Jun Zhu</li>
<li>for: 本文探讨了适用于多选问题的语言模型（LMs）的偏置阶段对logit基uncertainty准确性的影响。</li>
<li>methods: 本文采用了严谨的实验研究，探索了适应LMs与其预训练LMs的偏置阶段对uncertainty的影响。</li>
<li>results: 实验结果表明，适应LMs存在两种不同的uncertainty，负责答案决策和格式偏好。此外，研究者还发现了这两种uncertainty对适应LMs的准确性的影响，并提出了一种简单的Synthetic alignment scheme来缓解这种情况。<details>
<summary>Abstract</summary>
Despite the significant progress made in practical applications of aligned language models (LMs), they tend to be overconfident in output answers compared to the corresponding pre-trained LMs. In this work, we systematically evaluate the impact of the alignment process on logit-based uncertainty calibration of LMs under the multiple-choice setting. We first conduct a thoughtful empirical study on how aligned LMs differ in calibration from their pre-trained counterparts. Experimental results reveal that there are two distinct uncertainties in LMs under the multiple-choice setting, which are responsible for the answer decision and the format preference of the LMs, respectively. Then, we investigate the role of these two uncertainties on aligned LM's calibration through fine-tuning in simple synthetic alignment schemes and conclude that one reason for aligned LMs' overconfidence is the conflation of these two types of uncertainty. Furthermore, we examine the utility of common post-hoc calibration methods for aligned LMs and propose an easy-to-implement and sample-efficient method to calibrate aligned LMs. We hope our findings could provide insights into the design of more reliable alignment processes for LMs.
</details>
<details>
<summary>摘要</summary>
尽管已经在实际应用中采用了对齐语言模型（LMs），但它们往往会比预训练LMs更加自信。在这项工作中，我们系统地评估了对齐过程对logit基于不确定性调整的LMs的影响。我们首先通过思ful的实验研究了对齐LMs与其预训练对应者的不确定性差异。实验结果表明，LMs在多选设定下存在两种不同的不确定性，一是答案决定不确定性，二是格式偏好不确定性。然后，我们通过简单的合成对齐方案进行了调整，并发现对齐LMs的一种原因是这两种不确定性的混淆。此外，我们还检查了常见后处calibration方法对对齐LMs的效果，并提出了一种容易实现和效率高的方法来调整对齐LMs。我们希望我们的发现能为LMs的设计提供指导。
</details></li>
</ul>
<hr>
<h2 id="Chain-of-Thought-Tuning-Masked-Language-Models-can-also-Think-Step-By-Step-in-Natural-Language-Understanding"><a href="#Chain-of-Thought-Tuning-Masked-Language-Models-can-also-Think-Step-By-Step-in-Natural-Language-Understanding" class="headerlink" title="Chain-of-Thought Tuning: Masked Language Models can also Think Step By Step in Natural Language Understanding"></a>Chain-of-Thought Tuning: Masked Language Models can also Think Step By Step in Natural Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11721">http://arxiv.org/abs/2310.11721</a></li>
<li>repo_url: None</li>
<li>paper_authors: Caoyun Fan, Jidong Tian, Yitian Li, Wenqing Chen, Hao He, Yaohui Jin</li>
<li>for: 这 paper aims to improve the performance of Large Language Models (LLMs) on Natural Language Understanding (NLU) tasks by extending the success of Chain-of-Thought (CoT) technique to MLMs.</li>
<li>methods: The proposed method, Chain-of-Thought Tuning (CoTT), is a two-step reasoning framework based on prompt tuning that enables MLMs to implement step-by-step thinking for NLU tasks.</li>
<li>results: The experiments on two NLU tasks, hierarchical classification and relation extraction, show that CoTT outperforms baselines and achieves state-of-the-art performance.<details>
<summary>Abstract</summary>
Chain-of-Thought (CoT) is a technique that guides Large Language Models (LLMs) to decompose complex tasks into multi-step reasoning through intermediate steps in natural language form. Briefly, CoT enables LLMs to think step by step. However, although many Natural Language Understanding (NLU) tasks also require thinking step by step, LLMs perform less well than small-scale Masked Language Models (MLMs). To migrate CoT from LLMs to MLMs, we propose Chain-of-Thought Tuning (CoTT), a two-step reasoning framework based on prompt tuning, to implement step-by-step thinking for MLMs on NLU tasks. From the perspective of CoT, CoTT's two-step framework enables MLMs to implement task decomposition; CoTT's prompt tuning allows intermediate steps to be used in natural language form. Thereby, the success of CoT can be extended to NLU tasks through MLMs. To verify the effectiveness of CoTT, we conduct experiments on two NLU tasks: hierarchical classification and relation extraction, and the results show that CoTT outperforms baselines and achieves state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
Chain-of-Thought (CoT) 是一种技术，用于导引大型自然语言模型（LLM）来 decomposition 复杂任务为多个步骤的自然语言形式中的逻辑步骤。简单来说，CoT 使得 LLM 可以一步步地思考。然而，许多自然语言理解（NLU）任务也需要一步步地思考，但 LLM 表现比小规模的面纹语言模型（MLM）差。为将 CoT 从 LLM 迁移到 MLM 上，我们提出了链条思维调整（CoTT），一种基于提问调整的两步逻辑框架。从 CoT 的视角来看，CoTT 的两步框架使得 MLM 可以实现任务的分解；CoTT 的提问调整使得中间步骤可以在自然语言形式下使用。因此，CoT 的成功可以通过 MLM 扩展到 NLU 任务。为验证 CoTT 的有效性，我们对两个 NLU 任务进行了实验：层次分类和关系抽取，并得到的结果表明 CoTT 超过基eline和实现了状态的表现。
</details></li>
</ul>
<hr>
<h2 id="Reflection-Tuning-Data-Recycling-Improves-LLM-Instruction-Tuning"><a href="#Reflection-Tuning-Data-Recycling-Improves-LLM-Instruction-Tuning" class="headerlink" title="Reflection-Tuning: Data Recycling Improves LLM Instruction-Tuning"></a>Reflection-Tuning: Data Recycling Improves LLM Instruction-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11716">http://arxiv.org/abs/2310.11716</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mingliiii/reflection_tuning">https://github.com/mingliiii/reflection_tuning</a></li>
<li>paper_authors: Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, Heng Huang, Jiuxiang Gu, Tianyi Zhou</li>
<li>for: 提高大型自然语言模型（LLM）的输出控制和对输入的Alignment。</li>
<li>methods: 使用“反思调教”方法，利用智能语言模型自我反省和提高数据中的指令和回答质量。</li>
<li>results: LLM经过“反思调教”后，在多种评价指标上表现出色，超过了传统的数据集训练方法。<details>
<summary>Abstract</summary>
Recent advancements in Large Language Models (LLMs) have expanded the horizons of natural language understanding and generation. Notably, the output control and alignment with the input of LLMs can be refined through instruction tuning. However, as highlighted in several studies, low-quality data in the training set are usually detrimental to instruction tuning, resulting in inconsistent or even misleading LLM outputs. We propose a novel method, termed "reflection-tuning," which addresses the problem by self-improvement and judging capabilities of LLMs. This approach utilizes an oracle LLM to recycle the original training data by introspecting and enhancing the quality of instructions and responses in the data. Extensive experiments on widely used evaluation benchmarks show that LLMs trained with our recycled data outperform those trained with existing datasets in various benchmarks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MISAR-A-Multimodal-Instructional-System-with-Augmented-Reality"><a href="#MISAR-A-Multimodal-Instructional-System-with-Augmented-Reality" class="headerlink" title="MISAR: A Multimodal Instructional System with Augmented Reality"></a>MISAR: A Multimodal Instructional System with Augmented Reality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11699">http://arxiv.org/abs/2310.11699</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nguyennm1024/misar">https://github.com/nguyennm1024/misar</a></li>
<li>paper_authors: Jing Bi, Nguyen Manh Nguyen, Ali Vosoughi, Chenliang Xu</li>
<li>for: 本研究旨在探讨大语言模型（LLMs）如何在增强现实（AR）中增强人机交互。</li>
<li>methods: 本研究使用大语言模型（LLMs）将视觉、听音和语言渠道融合，以提供更加优化的人机交互。</li>
<li>results: 研究表明，通过使用大语言模型（LLMs）可以更好地估计任务性能，从而为AR系统提供更加适应性。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Augmented reality (AR) requires the seamless integration of visual, auditory, and linguistic channels for optimized human-computer interaction. While auditory and visual inputs facilitate real-time and contextual user guidance, the potential of large language models (LLMs) in this landscape remains largely untapped. Our study introduces an innovative method harnessing LLMs to assimilate information from visual, auditory, and contextual modalities. Focusing on the unique challenge of task performance quantification in AR, we utilize egocentric video, speech, and context analysis. The integration of LLMs facilitates enhanced state estimation, marking a step towards more adaptive AR systems. Code, dataset, and demo will be available at https://github.com/nguyennm1024/misar.
</details>
<details>
<summary>摘要</summary>
现实扩展（AR）需要Visual、听力和语言通道的无缝结合，以便最佳化人机交互。听力和视觉输入可以提供实时和上下文相关的用户指导，但是大型自然语言模型（LLM）在这个场景中的潜在仍然未得到充分利用。我们的研究提出了一种新的方法，利用LLM来融合Visual、听力和上下文modalities。我们在实际任务完成量化中采用egocentric视频、Speech和上下文分析。通过LLM的集成，我们可以提高状态估计，这标志着更适应性AR系统的出发点。代码、数据集和示例将在https://github.com/nguyennm1024/misar上提供。
</details></li>
</ul>
<hr>
<h2 id="Adaptation-with-Self-Evaluation-to-Improve-Selective-Prediction-in-LLMs"><a href="#Adaptation-with-Self-Evaluation-to-Improve-Selective-Prediction-in-LLMs" class="headerlink" title="Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs"></a>Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11689">http://arxiv.org/abs/2310.11689</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiefeng Chen, Jinsung Yoon, Sayna Ebrahimi, Sercan O Arik, Tomas Pfister, Somesh Jha</li>
<li>For: 提高大语言模型在高风险决策场景的可靠性。* Methods: 基于自我评估的参数有效性调整方法，以适应特定任务而进行适应。* Results: 在多个问答（QA）数据集上进行评估，比靡前状态艺的选择预测方法表现更好，例如在CoQA标准测试集上，AUACC从91.23%提高到92.63%，AUROC从74.61%提高到80.25%.<details>
<summary>Abstract</summary>
Large language models (LLMs) have recently shown great advances in a variety of tasks, including natural language understanding and generation. However, their use in high-stakes decision-making scenarios is still limited due to the potential for errors. Selective prediction is a technique that can be used to improve the reliability of the LLMs by allowing them to abstain from making predictions when they are unsure of the answer. In this work, we propose a novel framework for adaptation with self-evaluation to improve the selective prediction performance of LLMs. Our framework is based on the idea of using parameter-efficient tuning to adapt the LLM to the specific task at hand while improving its ability to perform self-evaluation. We evaluate our method on a variety of question-answering (QA) datasets and show that it outperforms state-of-the-art selective prediction methods. For example, on the CoQA benchmark, our method improves the AUACC from 91.23% to 92.63% and improves the AUROC from 74.61% to 80.25%.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Superiority-of-Softmax-Unveiling-the-Performance-Edge-Over-Linear-Attention"><a href="#Superiority-of-Softmax-Unveiling-the-Performance-Edge-Over-Linear-Attention" class="headerlink" title="Superiority of Softmax: Unveiling the Performance Edge Over Linear Attention"></a>Superiority of Softmax: Unveiling the Performance Edge Over Linear Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11685">http://arxiv.org/abs/2310.11685</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yichuan Deng, Zhao Song, Tianyi Zhou</li>
<li>for: 本研究旨在解释软max和线性注意力之间的实际性能差异的原因。</li>
<li>methods: 本研究采用了比较分析两种注意力机制的方法，以解释软max注意力在大多数场景下的性能优势。</li>
<li>results: 研究发现，软max注意力在大多数场景下具有更高的性能，而linear注意力具有更高的计算复杂度。<details>
<summary>Abstract</summary>
Large transformer models have achieved state-of-the-art results in numerous natural language processing tasks. Among the pivotal components of the transformer architecture, the attention mechanism plays a crucial role in capturing token interactions within sequences through the utilization of softmax function.   Conversely, linear attention presents a more computationally efficient alternative by approximating the softmax operation with linear complexity. However, it exhibits substantial performance degradation when compared to the traditional softmax attention mechanism.   In this paper, we bridge the gap in our theoretical understanding of the reasons behind the practical performance gap between softmax and linear attention. By conducting a comprehensive comparative analysis of these two attention mechanisms, we shed light on the underlying reasons for why softmax attention outperforms linear attention in most scenarios.
</details>
<details>
<summary>摘要</summary>
大型转换器模型在自然语言处理多种任务中取得了状态机器人的Result。转换器架构中的注意机制对于序列中的Token交互进行捕捉具有关键作用，通过使用softmax函数。然而，线性注意presenteda更加计算效率的代替方案，但它在与传统的softmax注意机制相比 exhibits substantial performance degradation。在这篇论文中，我们尝试填补这两种注意机制之间的实践性能差距的理论理解漏洞。通过对这两种注意机制进行全面的比较分析，我们 shed light on the underlying reasons why softmax注意机制在大多数场景下比linear注意机制更高性能。
</details></li>
</ul>
<hr>
<h2 id="Open-ended-Commonsense-Reasoning-with-Unrestricted-Answer-Scope"><a href="#Open-ended-Commonsense-Reasoning-with-Unrestricted-Answer-Scope" class="headerlink" title="Open-ended Commonsense Reasoning with Unrestricted Answer Scope"></a>Open-ended Commonsense Reasoning with Unrestricted Answer Scope</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11672">http://arxiv.org/abs/2310.11672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Ling, Xuchao Zhang, Xujiang Zhao, Yanchi Liu, Wei Cheng, Mika Oishi, Takao Osaki, Katsushi Matsuda, Haifeng Chen, Liang Zhao</li>
<li>for: 解决开放式常识问题，即没有短列表或预定答案范围的问题。</li>
<li>methods: 利用预训练语言模型IterativelyRetrieve reasoning paths on external knowledge base，不需要任务特定的监督。</li>
<li>results: 对两个常识 benchmarck 数据集进行实验，与其他方法相比，提出的方法表现更好， both quantitatively and qualitatively。<details>
<summary>Abstract</summary>
Open-ended Commonsense Reasoning is defined as solving a commonsense question without providing 1) a short list of answer candidates and 2) a pre-defined answer scope. Conventional ways of formulating the commonsense question into a question-answering form or utilizing external knowledge to learn retrieval-based methods are less applicable in the open-ended setting due to an inherent challenge. Without pre-defining an answer scope or a few candidates, open-ended commonsense reasoning entails predicting answers by searching over an extremely large searching space. Moreover, most questions require implicit multi-hop reasoning, which presents even more challenges to our problem. In this work, we leverage pre-trained language models to iteratively retrieve reasoning paths on the external knowledge base, which does not require task-specific supervision. The reasoning paths can help to identify the most precise answer to the commonsense question. We conduct experiments on two commonsense benchmark datasets. Compared to other approaches, our proposed method achieves better performance both quantitatively and qualitatively.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MixEdit-Revisiting-Data-Augmentation-and-Beyond-for-Grammatical-Error-Correction"><a href="#MixEdit-Revisiting-Data-Augmentation-and-Beyond-for-Grammatical-Error-Correction" class="headerlink" title="MixEdit: Revisiting Data Augmentation and Beyond for Grammatical Error Correction"></a>MixEdit: Revisiting Data Augmentation and Beyond for Grammatical Error Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11671">http://arxiv.org/abs/2310.11671</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thukelab/mixedit">https://github.com/thukelab/mixedit</a></li>
<li>paper_authors: Jingheng Ye, Yinghui Li, Yangning Li, Hai-Tao Zheng</li>
<li>for: 本研究旨在解释如何使用数据扩充提高 grammatical error correction（GEC）模型的性能。</li>
<li>methods: 本研究提出了两个可读性和计算效率高的指标：吸引力和多样性。这两个指标可以帮助理解数据扩充对 GEC 模型的影响。</li>
<li>results: 实验结果表明，一个具有高吸引力和合适多样性的数据扩充策略可以更好地提高 GEC 模型的性能。此外，提议的 MixEdit 数据扩充方法可以在不需要额外的单语言 corpus 的情况下，STRATEGICALLY AND DYNAMICALLY 扩充真实的数据，以提高 GEC 模型的性能。<details>
<summary>Abstract</summary>
Data Augmentation through generating pseudo data has been proven effective in mitigating the challenge of data scarcity in the field of Grammatical Error Correction (GEC). Various augmentation strategies have been widely explored, most of which are motivated by two heuristics, i.e., increasing the distribution similarity and diversity of pseudo data. However, the underlying mechanism responsible for the effectiveness of these strategies remains poorly understood. In this paper, we aim to clarify how data augmentation improves GEC models. To this end, we introduce two interpretable and computationally efficient measures: Affinity and Diversity. Our findings indicate that an excellent GEC data augmentation strategy characterized by high Affinity and appropriate Diversity can better improve the performance of GEC models. Based on this observation, we propose MixEdit, a data augmentation approach that strategically and dynamically augments realistic data, without requiring extra monolingual corpora. To verify the correctness of our findings and the effectiveness of the proposed MixEdit, we conduct experiments on mainstream English and Chinese GEC datasets. The results show that MixEdit substantially improves GEC models and is complementary to traditional data augmentation methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>> transtable text into Simplified Chinese.<</SYS>>数据扩充通过生成假数据已经证明可以有效地解决语法错误修复（GEC）领域中的数据缺乏问题。各种扩充策略已经广泛探索，大多数是基于两个启发，即增加假数据的分布相似性和多样性。然而，这些策略下面的机制仍然不够了解。在这篇论文中，我们目的是解释如何使数据扩充改进GEC模型。为此，我们引入了两种可解释的计算效率的度量：团结度和多样性。我们的发现表明，一个具有高团结度和合适的多样性的GEC数据扩充策略可以更好地改进GEC模型的性能。基于这一观察，我们提出了 MixEdit，一种数据扩充方法，不需要额外的同语言资料卷。为了验证我们的发现和 MixEdit 的有效性，我们在主流的英语和中文GEC数据集上进行了实验。结果表明，MixEdit 可以大幅提高 GEC 模型的性能，并且与传统的数据扩充方法相комplementary。
</details></li>
</ul>
<hr>
<h2 id="Field-testing-items-using-artificial-intelligence-Natural-language-processing-with-transformers"><a href="#Field-testing-items-using-artificial-intelligence-Natural-language-processing-with-transformers" class="headerlink" title="Field-testing items using artificial intelligence: Natural language processing with transformers"></a>Field-testing items using artificial intelligence: Natural language processing with transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11655">http://arxiv.org/abs/2310.11655</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hotaka Maeda</li>
<li>for: 这个研究用了五千个RoBERTa模型来测试英语文本理解能力。</li>
<li>methods: 研究使用了一种人工智能”transformer”来解决英语文本理解问题。</li>
<li>results: 研究发现，RoBERTa模型可以准确地回答英语文本理解测试中的29个多选题。数据还用于计算测试题的心理特性，与人类考生数据显示一定的一致性。<details>
<summary>Abstract</summary>
Five thousand variations of the RoBERTa model, an artificially intelligent "transformer" that can understand text language, completed an English literacy exam with 29 multiple-choice questions. Data were used to calculate the psychometric properties of the items, which showed some degree of agreement to those obtained from human examinee data.
</details>
<details>
<summary>摘要</summary>
五千种RoBERTa模型，一种人工智能"变换器"，通过完成了29个多选题的英语阅读测验。使用数据计算测验项的心理属性，结果与人类考生数据有一定的相似度。Note:* "RoBERTa" is translated as "RoBERTa模型" in Simplified Chinese.* "transformer" is translated as "变换器" in Simplified Chinese.* "English literacy exam" is translated as "英语阅读测验" in Simplified Chinese.* "multiple-choice questions" is translated as "多选题" in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Zero-shot-Faithfulness-Evaluation-for-Text-Summarization-with-Foundation-Language-Model"><a href="#Zero-shot-Faithfulness-Evaluation-for-Text-Summarization-with-Foundation-Language-Model" class="headerlink" title="Zero-shot Faithfulness Evaluation for Text Summarization with Foundation Language Model"></a>Zero-shot Faithfulness Evaluation for Text Summarization with Foundation Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11648">http://arxiv.org/abs/2310.11648</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiaqisjtu/faitheval-fflm">https://github.com/jiaqisjtu/faitheval-fflm</a></li>
<li>paper_authors: Qi Jia, Siyu Ren, Yizhu Liu, Kenny Q. Zhu</li>
<li>for: 本文提出了一种新的无参 faithfulness 评估方法，用于评估自然语言生成模型的准确性和可靠性。</li>
<li>methods: 本文使用了一种基于 prefix 的新 metric FFLM，通过探索输出文本的可能性来评估模型的 faithfulness。</li>
<li>results: 实验表明，FFLM 可以与或even outperform ChatGPT 在不同任务上，并且具有24倍 fewer 参数。 FFLM 还可以超过其他强基线。<details>
<summary>Abstract</summary>
Despite tremendous improvements in natural language generation, summarization models still suffer from the unfaithfulness issue. Previous work evaluates faithfulness either using models trained on the other tasks or in-domain synthetic data, or prompting a large model such as ChatGPT. This paper proposes to do zero-shot faithfulness evaluation simply with a moderately-sized foundation language model. We introduce a new metric FFLM, which is a combination of probability changes based on the intuition that prefixing a piece of text that is consistent with the output will increase the probability of predicting the output. Experiments show that FFLM performs competitively with or even outperforms ChatGPT on both inconsistency detection and faithfulness rating with 24x fewer parameters. FFLM also achieves improvements over other strong baselines.
</details>
<details>
<summary>摘要</summary>
尽管自然语言生成技术已经做出了很大的进步，摘要模型仍然面临着不忠问题。前一任的工作通常使用其他任务训练的模型或者域内生成的数据来评估忠诚性，或者激活大型模型如ChatGPT。这篇论文提议使用一个 moderately-sized 基础语言模型进行零 shot 忠诚性评估。我们引入了一个新的度量FFLM，它是基于输出预测概率变化的 prefixing 语句的 intuition。实验表明，FFLM 与 ChatGPT 在不一致检测和忠诚评分中表现竞争，并且具有24倍少的参数。FFLM 还超过了其他强大基elines。
</details></li>
</ul>
<hr>
<h2 id="Systematic-Assessment-of-Factual-Knowledge-in-Large-Language-Models"><a href="#Systematic-Assessment-of-Factual-Knowledge-in-Large-Language-Models" class="headerlink" title="Systematic Assessment of Factual Knowledge in Large Language Models"></a>Systematic Assessment of Factual Knowledge in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11638">http://arxiv.org/abs/2310.11638</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linhao Luo, Thuy-Trang Vu, Dinh Phung, Gholamreza Haffari</li>
<li>for: 这个论文是为了系统地评估大语言模型（LLM）中的实用知识，以及如何利用知识图（KG）来评估。</li>
<li>methods: 这个论文使用了一种框架，可以自动生成知识图中的问题和预期答案，然后评估LLM的答案准确性。</li>
<li>results: 实验表明，ChatGPT在各个领域中表现最佳，而LLM的表现受到 instrucion finetuning、领域、问题复杂度和Contextual Adversarial的影响。<details>
<summary>Abstract</summary>
Previous studies have relied on existing question-answering benchmarks to evaluate the knowledge stored in large language models (LLMs). However, this approach has limitations regarding factual knowledge coverage, as it mostly focuses on generic domains which may overlap with the pretraining data. This paper proposes a framework to systematically assess the factual knowledge of LLMs by leveraging knowledge graphs (KGs). Our framework automatically generates a set of questions and expected answers from the facts stored in a given KG, and then evaluates the accuracy of LLMs in answering these questions. We systematically evaluate the state-of-the-art LLMs with KGs in generic and specific domains. The experiment shows that ChatGPT is consistently the top performer across all domains. We also find that LLMs performance depends on the instruction finetuning, domain and question complexity and is prone to adversarial context.
</details>
<details>
<summary>摘要</summary>
Here's the translation in Simplified Chinese:先前的研究通过现有的问答指标来评估大语言模型（LLM）中的知识，但这种方法有限制，因为它主要集中在通用领域，这可能与预训练数据重叠。本文提出了一个框架，可以系统地评估 LLM 中的事实知识，通过利用知识图（KG）。我们的框架可以自动生成基于 KG 中的事实的问题和预期答案，然后评估 LLM 在回答这些问题时的准确性。我们系统地评估了当今最先进的 LLM 在通用和特定领域中的性能。实验结果显示，ChatGPT 在所有领域中占据了首位。我们还发现，LLM 的性能取决于 instrucion 精度调整、领域和问题复杂度，并且容易受到恶意上下文的影响。
</details></li>
</ul>
<hr>
<h2 id="MAGNIFICo-Evaluating-the-In-Context-Learning-Ability-of-Large-Language-Models-to-Generalize-to-Novel-Interpretations"><a href="#MAGNIFICo-Evaluating-the-In-Context-Learning-Ability-of-Large-Language-Models-to-Generalize-to-Novel-Interpretations" class="headerlink" title="MAGNIFICo: Evaluating the In-Context Learning Ability of Large Language Models to Generalize to Novel Interpretations"></a>MAGNIFICo: Evaluating the In-Context Learning Ability of Large Language Models to Generalize to Novel Interpretations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11634">http://arxiv.org/abs/2310.11634</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arkil Patel, Satwik Bhattamishra, Siva Reddy, Dzmitry Bahdanau</li>
<li>for: 本研究旨在探讨大语言模型（LLM）是否可以通过Contextual learning来学习新的解释。</li>
<li>methods: 我们引入了MAGNIFICo评价工具，该工具基于文本到SQLsemantic parsing框架，并在不同的token和提示设置下进行了多种测试，以模拟实际场景的复杂性。</li>
<li>results: 实验结果表明，LLMs在自然语言描述和长对话中能够很好地理解新的解释，但是我们的研究也发现，当面临不熟悉的词语或同时构建多个新解释时，LLMs的性能仍然有所不足。此外，我们的分析还揭示了LLMs中的semantic predispositions，以及长context中的recency bias的影响。<details>
<summary>Abstract</summary>
Humans possess a remarkable ability to assign novel interpretations to linguistic expressions, enabling them to learn new words and understand community-specific connotations. However, Large Language Models (LLMs) have a knowledge cutoff and are costly to finetune repeatedly. Therefore, it is crucial for LLMs to learn novel interpretations in-context. In this paper, we systematically analyse the ability of LLMs to acquire novel interpretations using in-context learning. To facilitate our study, we introduce MAGNIFICo, an evaluation suite implemented within a text-to-SQL semantic parsing framework that incorporates diverse tokens and prompt settings to simulate real-world complexity. Experimental results on MAGNIFICo demonstrate that LLMs exhibit a surprisingly robust capacity for comprehending novel interpretations from natural language descriptions as well as from discussions within long conversations. Nevertheless, our findings also highlight the need for further improvements, particularly when interpreting unfamiliar words or when composing multiple novel interpretations simultaneously in the same example. Additionally, our analysis uncovers the semantic predispositions in LLMs and reveals the impact of recency bias for information presented in long contexts.
</details>
<details>
<summary>摘要</summary>
人类具有强大的语言表达重新解释能力，可以学习新词和社区特有的含义。然而，大型自然语言模型（LLM）具有知识割辑和重新训练成本高的问题。因此，LLM需要在Context中学习新的解释。本文系统地分析了LLM在Context中学习新解释的能力。为了促进我们的研究，我们提出了MAGNIFICo评价集，该集包括多种Token和提示设置，以 simulate real-world complexity。实验结果表明，LLM在自然语言描述和长 conversations中的讨论中能够很好地理解新解释。然而，我们的发现也表明，当解释不熟悉的词语或者在同一个例子中同时构成多个新解释时，LLM的表现仍然需要进一步改进。此外，我们的分析还揭示了LLM中的含义偏好和长 context中的新信息偏好。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/18/cs.CL_2023_10_18/" data-id="clogxf3mb00d75xracww90h4x" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_10_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/18/cs.LG_2023_10_18/" class="article-date">
  <time datetime="2023-10-18T10:00:00.000Z" itemprop="datePublished">2023-10-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/18/cs.LG_2023_10_18/">cs.LG - 2023-10-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="No-Regret-Learning-in-Bilateral-Trade-via-Global-Budget-Balance"><a href="#No-Regret-Learning-in-Bilateral-Trade-via-Global-Budget-Balance" class="headerlink" title="No-Regret Learning in Bilateral Trade via Global Budget Balance"></a>No-Regret Learning in Bilateral Trade via Global Budget Balance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12370">http://arxiv.org/abs/2310.12370</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martino Bernasconi, Matteo Castiglioni, Andrea Celli, Federico Fusco</li>
<li>for: 本文研究在双方均有私人估价的 bilateral trade 问题中，learner 设定价格，无知到代理人的估价。</li>
<li>methods: 本文引入全局预算平衡的概念，要求代理人在整个时间轴上保持预算平衡。通过全局预算平衡，提供了首个不负 regret 算法，在不同反馈模型下对 adversarial 输入进行评估。</li>
<li>results: 在全部反馈模型中，learner 可以保证 $\tilde{O}(\sqrt{T})$ regret，与最佳固定价格相比，是OPTimal的。在partial feedback模型中，提供了一个 $\tilde{O}(T^{3&#x2F;4})$ regret Upper bound的算法，并且 complement with a nearly-matching lower bound。<details>
<summary>Abstract</summary>
Bilateral trade revolves around the challenge of facilitating transactions between two strategic agents -- a seller and a buyer -- both of whom have a private valuations for the item. We study the online version of the problem, in which at each time step a new seller and buyer arrive. The learner's task is to set a price for each agent, without any knowledge about their valuations. The sequence of sellers and buyers is chosen by an oblivious adversary. In this setting, known negative results rule out the possibility of designing algorithms with sublinear regret when the learner has to guarantee budget balance for each iteration. In this paper, we introduce the notion of global budget balance, which requires the agent to be budget balance only over the entire time horizon. By requiring global budget balance, we provide the first no-regret algorithms for bilateral trade with adversarial inputs under various feedback models. First, we show that in the full-feedback model the learner can guarantee $\tilde{O}(\sqrt{T})$ regret against the best fixed prices in hindsight, which is order-wise optimal. Then, in the case of partial feedback models, we provide an algorithm guaranteeing a $\tilde{O}(T^{3/4})$ regret upper bound with one-bit feedback, which we complement with a nearly-matching lower bound. Finally, we investigate how these results vary when measuring regret using an alternative benchmark.
</details>
<details>
<summary>摘要</summary>
bilateral trade 环绕着两个策略代理人（一个买家和一个卖家）之间的挑战，这两个代理人都有私人的评价值。我们研究在网络上进行的这个问题，在每个时间步骤中，新的买家和卖家会出现。学习者的任务是设定价格，但是没有任何关于代理人们的评价知识。选择序列的买家和卖家是由一个无知的敌人选择。在这个设定下，已知的负结果规则排除了设计具有下图 regret 的算法的可能性。在这篇论文中，我们引入全面预算平衡的概念，它需要学习者在整个时间频谱上保持预算平衡。通过需要全面预算平衡，我们提供了首个不负担 regret 的算法，在不同的反馈模型下实现了双方贸易。首先，在完整反馈模型中，我们显示学习者可以在对照后获得 $\tilde{O}(\sqrt{T})$ regret，这是很好的估计。然后，在受限反馈模型中，我们提供了一个 garantia $\tilde{O}(T^{3/4})$ regret 的算法，并补充了一个几乎匹配的下限。最后，我们调查了这些结果如何在使用不同的参考基准时变化。
</details></li>
</ul>
<hr>
<h2 id="MARVEL-Multi-Agent-Reinforcement-Learning-for-Large-Scale-Variable-Speed-Limits"><a href="#MARVEL-Multi-Agent-Reinforcement-Learning-for-Large-Scale-Variable-Speed-Limits" class="headerlink" title="MARVEL: Multi-Agent Reinforcement-Learning for Large-Scale Variable Speed Limits"></a>MARVEL: Multi-Agent Reinforcement-Learning for Large-Scale Variable Speed Limits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12359">http://arxiv.org/abs/2310.12359</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhang Zhang, Marcos Quinones-Grueiro, Zhiyao Zhang, Yanbing Wang, William Barbour, Gautam Biswas, Daniel Work</li>
<li>for: 这个论文的目的是提出一种基于多代理学习（MARL）的大规模Variable Speed Limit（VSL）控制策略，以提高交通安全性和流体性。</li>
<li>methods: 该论文使用MARL框架，通过使用常见的数据来实现大规模VSL控制。代理学习算法通过考虑交通条件的变化、安全性和流体性的奖励结构进行学习，从而实现代理之间的协调。</li>
<li>results: 对于一段7英里的高速公路，MARL方法提高了交通安全性63.4%，并提高了交通流体性14.6%，相比于现有的实践算法。此外，文章还进行了解释性分析，以了解代理在不同交通条件下的决策过程。最后，文章测试了在实际数据上的策略，以证明该策略的可部署性。<details>
<summary>Abstract</summary>
Variable speed limit (VSL) control is a promising traffic management strategy for enhancing safety and mobility. This work introduces MARVEL, a multi-agent reinforcement learning (MARL) framework for implementing large-scale VSL control on freeway corridors using only commonly available data. The agents learn through a reward structure that incorporates adaptability to traffic conditions, safety, and mobility; enabling coordination among the agents. The proposed framework scales to cover corridors with many gantries thanks to a parameter sharing among all VSL agents. The agents are trained in a microsimulation environment based on a short freeway stretch with 8 gantries spanning 7 miles and tested with 34 gantries spanning 17 miles of I-24 near Nashville, TN. MARVEL improves traffic safety by 63.4% compared to the no control scenario and enhances traffic mobility by 14.6% compared to a state-of-the-practice algorithm that has been deployed on I-24. An explainability analysis is undertaken to explore the learned policy under different traffic conditions and the results provide insights into the decision-making process of agents. Finally, we test the policy learned from the simulation-based experiments on real input data from I-24 to illustrate the potential deployment capability of the learned policy.
</details>
<details>
<summary>摘要</summary>
Variable speed limit (VSL) 控制是一种有前途的交通管理策略，可以提高安全性和流动性。这项工作介绍了 MARVEL，一种多代理学习 (MARL) 框架，用于实现大规模 VSL 控制在高速公路段上，只使用常见的数据。代理学习的奖励结构包括适应交通条件、安全性和流动性，使代理之间协调。提出的框架可以涵盖覆盖许多斜塔，因为所有 VSL 代理的参数共享。代理在基于微观 simulate 环境中学习，该环境基于一段长7英里的高速公路，涵盖8个斜塔。代理在基于实际数据进行测试，并在I-24公路上进行了17英里的测试。 MARVEL 可以提高交通安全性63.4%，并提高交通流动性14.6%，相比之前的实践算法。 Explainability 分析用于探索不同交通条件下代理学习的策略，结果提供了决策过程中代理的启示。最后，我们将在实际数据上测试从 simulate 中学习的策略，以 illustrate 学习的可部署性。
</details></li>
</ul>
<hr>
<h2 id="Networkwide-Traffic-State-Forecasting-Using-Exogenous-Information-A-Multi-Dimensional-Graph-Attention-Based-Approach"><a href="#Networkwide-Traffic-State-Forecasting-Using-Exogenous-Information-A-Multi-Dimensional-Graph-Attention-Based-Approach" class="headerlink" title="Networkwide Traffic State Forecasting Using Exogenous Information: A Multi-Dimensional Graph Attention-Based Approach"></a>Networkwide Traffic State Forecasting Using Exogenous Information: A Multi-Dimensional Graph Attention-Based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12353">http://arxiv.org/abs/2310.12353</a></li>
<li>repo_url: None</li>
<li>paper_authors: Syed Islam, Monika Filipovska</li>
<li>for: 这篇论文主要针对交通管理和控制策略中的交通状态预测问题，以及用户和系统层次的决策。</li>
<li>methods: 该论文提出了一种基于多维空间时间图注意力网络（M-STGAT）的交通预测方法，该方法使用过去观测到的速度、路况事件、温度和视程，并利用交通网络的结构来学习。</li>
<li>results: 实验结果表明，M-STGAT在使用加利福尼亚交通部门（Caltrans）性能衡量系统（PeMS）提供的交通速度和路况数据，并与国家海洋和大气管理局（NOAA）自动Surface Observing Systems（ASOS）提供的天气数据进行比较，在30、45和60分钟预测时间 horizons 上表现出了较好的预测性能，其中error measures包括 Mean Absolute Error（MAE）、Root Mean Square Error（RMSE）和Mean Absolute Percentage Error（MAPE）。但是，模型的传送性可能需要进一步的调查。<details>
<summary>Abstract</summary>
Traffic state forecasting is crucial for traffic management and control strategies, as well as user- and system-level decision making in the transportation network. While traffic forecasting has been approached with a variety of techniques over the last couple of decades, most approaches simply rely on endogenous traffic variables for state prediction, despite the evidence that exogenous factors can significantly impact traffic conditions. This paper proposes a multi-dimensional spatio-temporal graph attention-based traffic prediction approach (M-STGAT), which predicts traffic based on past observations of speed, along with lane closure events, temperature, and visibility across the transportation network. The approach is based on a graph attention network architecture, which also learns based on the structure of the transportation network on which these variables are observed. Numerical experiments are performed using traffic speed and lane closure data from the California Department of Transportation (Caltrans) Performance Measurement System (PeMS). The corresponding weather data were downloaded from the National Oceanic and Atmospheric Administration (NOOA) Automated Surface Observing Systems (ASOS). For comparison, the numerical experiments implement three alternative models which do not allow for the multi-dimensional input. The M-STGAT is shown to outperform the three alternative models, when performing tests using our primary data set for prediction with a 30-, 45-, and 60-minute prediction horizon, in terms of three error measures: Mean Absolute Error (MAE), Root Mean Square Error (RMSE) and Mean Absolute Percentage Error (MAPE). However, the model's transferability can vary for different transfer data sets and this aspect may require further investigation.
</details>
<details>
<summary>摘要</summary>
交通状况预测是交通管理和控制策略以及用户和系统层次的决策中非常重要的一环。自过去几十年来，交通预测已经使用了多种技术，但大多数方法都仅仅基于内生的交通变量进行预测，尽管外生因素可能对交通条件产生重要影响。这篇论文提出了一种多维度空间时间图注意力基本交通预测方法（M-STGAT），该方法基于过去观测到的速度，以及路段 closure事件、温度和视程等外生因素进行预测。该方法基于图注意力网络架构，同时还学习了交通网络上这些变量的结构。我们使用了加利福尼亚交通部门（Caltrans）性能测量系统（PeMS）中的交通速度和路段 closure数据进行数值实验，并下载了国家海洋和大气管理局（NOAA）自动地面观测系统（ASOS）中的天气数据。为比较，我们实现了三种不允许多维度输入的数学模型。M-STGAT在使用我们的主要数据集进行预测时，在30-, 45-, 和 60-分钟预测距离时表现出了与三个错误度量（ Mean Absolute Error，Root Mean Square Error 和 Mean Absolute Percentage Error）相对较高的性能。然而，模型的传输性可能会随着不同的传输数据集而异。这一点可能需要进一步的调查。
</details></li>
</ul>
<hr>
<h2 id="Equipping-Federated-Graph-Neural-Networks-with-Structure-aware-Group-Fairness"><a href="#Equipping-Federated-Graph-Neural-Networks-with-Structure-aware-Group-Fairness" class="headerlink" title="Equipping Federated Graph Neural Networks with Structure-aware Group Fairness"></a>Equipping Federated Graph Neural Networks with Structure-aware Group Fairness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12350">http://arxiv.org/abs/2310.12350</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuening-lab/f2gnn">https://github.com/yuening-lab/f2gnn</a></li>
<li>paper_authors: Nan Cui, Xiuling Wang, Wendy Hui Wang, Violet Chen, Yue Ning</li>
<li>for: 这篇论文旨在提出一种解决联合学习中Graph Neural Networks（GNNs）中的偏见问题的方法，以保证GNNs在分布式学习中保持公平性。</li>
<li>methods: 该方法基于两个关键组成部分：一是客户端上的公平性意识更新方案，二是在集成过程中考虑本地模型的公平性指标和数据偏见指标的全球模型更新方案。</li>
<li>results: 该方法在许多基准方法上表现出色，在公平性和模型准确性两个方面均有显著提升。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have been widely used for various types of graph data processing and analytical tasks in different domains. Training GNNs over centralized graph data can be infeasible due to privacy concerns and regulatory restrictions. Thus, federated learning (FL) becomes a trending solution to address this challenge in a distributed learning paradigm. However, as GNNs may inherit historical bias from training data and lead to discriminatory predictions, the bias of local models can be easily propagated to the global model in distributed settings. This poses a new challenge in mitigating bias in federated GNNs. To address this challenge, we propose $\text{F}^2$GNN, a Fair Federated Graph Neural Network, that enhances group fairness of federated GNNs. As bias can be sourced from both data and learning algorithms, $\text{F}^2$GNN aims to mitigate both types of bias under federated settings. First, we provide theoretical insights on the connection between data bias in a training graph and statistical fairness metrics of the trained GNN models. Based on the theoretical analysis, we design $\text{F}^2$GNN which contains two key components: a fairness-aware local model update scheme that enhances group fairness of the local models on the client side, and a fairness-weighted global model update scheme that takes both data bias and fairness metrics of local models into consideration in the aggregation process. We evaluate $\text{F}^2$GNN empirically versus a number of baseline methods, and demonstrate that $\text{F}^2$GNN outperforms these baselines in terms of both fairness and model accuracy.
</details>
<details>
<summary>摘要</summary>
GRAPH Neural Networks (GNNs) 在不同领域中对各种图数据进行处理和分析任务广泛使用。在中央化图数据上训练 GNNs 可能因为隐私问题和管制约束而成为不可能的。因此，联邦学习 (FL) 成为一种解决这个挑战的趋势。然而， GNNs 可能从训练数据中继承历史偏见，并导致歧视性预测，因此在分布式设置下，本地模型的偏见可能被轻松传播到全球模型。这种挑战需要解决偏见在联邦 GNN 中的问题。为此，我们提出了 $\text{F}^2$GNN，一种增强分布式 Graph Neural Network 的分组公平性。由于偏见可以来自数据和学习算法，$\text{F}^2$GNN 采用了两个关键组成部分：在客户端上使用公平性意识的本地模型更新方案，以及在聚合过程中考虑本地模型的公平性度量和数据偏见的准确度。我们对 $\text{F}^2$GNN 进行了理论分析，并对其与一些基准方法进行了实验比较，并证明 $\text{F}^2$GNN 在公平性和模型准确性两个方面都高于基准方法。
</details></li>
</ul>
<hr>
<h2 id="Tracking-electricity-losses-and-their-perceived-causes-using-nighttime-light-and-social-media"><a href="#Tracking-electricity-losses-and-their-perceived-causes-using-nighttime-light-and-social-media" class="headerlink" title="Tracking electricity losses and their perceived causes using nighttime light and social media"></a>Tracking electricity losses and their perceived causes using nighttime light and social media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12346">http://arxiv.org/abs/2310.12346</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samuel W Kerber, Nicholas A Duncan, Guillaume F LHer, Morgan Bazilian, Chris Elvidge, Mark R Deinert</li>
<li>for: 本研究旨在使用卫星图像、社交媒体和信息提取技术监测停电和其所报导的原因。</li>
<li>methods: 本研究使用了夜晚照明数据（2019年3月的加拉加斯，委内瑞拉），并通过Twitter数据分析公众对停电的感受和看法，以及使用统计分析和话题模型探讨公众归咎政府的停电原因。</li>
<li>results: 研究发现，夜晚照明强度与停电Region之间存在 inverse 关系。twitter上提到委内瑞拉总统的帖子具有更高的负面性和更多的责任相关词汇，这表明公众归咎政府对停电的责任。<details>
<summary>Abstract</summary>
Urban environments are intricate systems where the breakdown of critical infrastructure can impact both the economic and social well-being of communities. Electricity systems hold particular significance, as they are essential for other infrastructure, and disruptions can trigger widespread consequences. Typically, assessing electricity availability requires ground-level data, a challenge in conflict zones and regions with limited access. This study shows how satellite imagery, social media, and information extraction can monitor blackouts and their perceived causes. Night-time light data (in March 2019 for Caracas, Venezuela) is used to indicate blackout regions. Twitter data is used to determine sentiment and topic trends, while statistical analysis and topic modeling delved into public perceptions regarding blackout causes. The findings show an inverse relationship between nighttime light intensity. Tweets mentioning the Venezuelan President displayed heightened negativity and a greater prevalence of blame-related terms, suggesting a perception of government accountability for the outages.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard writing system used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Open-Set-Multivariate-Time-Series-Anomaly-Detection"><a href="#Open-Set-Multivariate-Time-Series-Anomaly-Detection" class="headerlink" title="Open-Set Multivariate Time-Series Anomaly Detection"></a>Open-Set Multivariate Time-Series Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12294">http://arxiv.org/abs/2310.12294</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Lai, Thi Kieu Khanh Ho, Narges Armanfard</li>
<li>for: 提出了一种新的方法来解决时间序列异常检测（TSAD）问题，即在训练阶段只有有限的异常样本可用，并且需要检测未经见过的异常类型。</li>
<li>methods: 该方法包括三个主要模块：特征提取器、多头网络和异常评分模块。特征提取器抽取有用的时间序列特征，多头网络包括生成-, 偏差-和对比头，用于捕捉已见和未见异常类型。</li>
<li>results: 对三个实际数据集进行了广泛的实验，结果显示，我们的方法在不同的设定下都能够超越现有方法，从而在TSAD领域实现了新的状态级表现。<details>
<summary>Abstract</summary>
Numerous methods for time series anomaly detection (TSAD) methods have emerged in recent years. Most existing methods are unsupervised and assume the availability of normal training samples only, while few supervised methods have shown superior performance by incorporating labeled anomalous samples in the training phase. However, certain anomaly types are inherently challenging for unsupervised methods to differentiate from normal data, while supervised methods are constrained to detecting anomalies resembling those present during training, failing to generalize to unseen anomaly classes. This paper is the first attempt in providing a novel approach for the open-set TSAD problem, in which a small number of labeled anomalies from a limited class of anomalies are visible in the training phase, with the objective of detecting both seen and unseen anomaly classes in the test phase. The proposed method, called Multivariate Open-Set timeseries Anomaly Detection (MOSAD) consists of three primary modules: a Feature Extractor to extract meaningful time-series features; a Multi-head Network consisting of Generative-, Deviation-, and Contrastive heads for capturing both seen and unseen anomaly classes; and an Anomaly Scoring module leveraging the insights of the three heads to detect anomalies. Extensive experiments on three real-world datasets consistently show that our approach surpasses existing methods under various experimental settings, thus establishing a new state-of-the-art performance in the TSAD field.
</details>
<details>
<summary>摘要</summary>
Recently, many time series anomaly detection (TSAD) methods have been proposed. Most of these methods are unsupervised and assume the availability of normal training samples, while only a few supervised methods have shown better performance by incorporating labeled anomalous samples in the training phase. However, some anomaly types are difficult for unsupervised methods to distinguish from normal data, while supervised methods are limited to detecting anomalies similar to those present during training and cannot handle unseen anomaly classes. This paper is the first attempt to solve the open-set TSAD problem, in which a small number of labeled anomalies from a limited class of anomalies are available during training, with the goal of detecting both seen and unseen anomaly classes in the test phase.The proposed method, called Multivariate Open-Set Time Series Anomaly Detection (MOSAD), consists of three primary modules: a Feature Extractor to extract meaningful time-series features; a Multi-head Network consisting of Generative-, Deviation-, and Contrastive heads to capture both seen and unseen anomaly classes; and an Anomaly Scoring module that leverages the insights of the three heads to detect anomalies. Extensive experiments on three real-world datasets consistently show that our approach outperforms existing methods under various experimental settings, thereby establishing a new state-of-the-art performance in the TSAD field.
</details></li>
</ul>
<hr>
<h2 id="A-PAC-Learning-Algorithm-for-LTL-and-Omega-regular-Objectives-in-MDPs"><a href="#A-PAC-Learning-Algorithm-for-LTL-and-Omega-regular-Objectives-in-MDPs" class="headerlink" title="A PAC Learning Algorithm for LTL and Omega-regular Objectives in MDPs"></a>A PAC Learning Algorithm for LTL and Omega-regular Objectives in MDPs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12248">http://arxiv.org/abs/2310.12248</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mateo Perez, Fabio Somenzi, Ashutosh Trivedi</li>
<li>for: 表示非Markov决策学中的非Markov目标，使用线性时间逻辑（LTL）和ω-正则目标。</li>
<li>methods: 使用模型基于可能approx Correct（PAC）学习算法，从系统轨迹样本中学习。不需要先知系统结构。</li>
<li>results: 学习omega-正则目标的Markov决策过程中的可能approx Correct算法。<details>
<summary>Abstract</summary>
Linear temporal logic (LTL) and omega-regular objectives -- a superset of LTL -- have seen recent use as a way to express non-Markovian objectives in reinforcement learning. We introduce a model-based probably approximately correct (PAC) learning algorithm for omega-regular objectives in Markov decision processes. Unlike prior approaches, our algorithm learns from sampled trajectories of the system and does not require prior knowledge of the system's topology.
</details>
<details>
<summary>摘要</summary>
线性时间逻辑（LTL）和奥米加常量目标——LTL的超集——在人工智能中被用来表达非马普朗的目标。我们介绍了基于模型的可能相对正确（PAC）学习算法 для奥米加常量目标在Markov决策过程中。与先前的方法不同，我们的算法从系统样本轨迹中学习，而不需要先知系统结构。
</details></li>
</ul>
<hr>
<h2 id="Fast-Parameter-Inference-on-Pulsar-Timing-Arrays-with-Normalizing-Flows"><a href="#Fast-Parameter-Inference-on-Pulsar-Timing-Arrays-with-Normalizing-Flows" class="headerlink" title="Fast Parameter Inference on Pulsar Timing Arrays with Normalizing Flows"></a>Fast Parameter Inference on Pulsar Timing Arrays with Normalizing Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12209">http://arxiv.org/abs/2310.12209</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Shih, Marat Freytsis, Stephen R. Taylor, Jeff A. Dror, Nolan Smyth</li>
<li>for: 这个论文是为了提高�ulsar时间尺度数组（PTAs）的 Bayesian posterior inference 的效率而写的。</li>
<li>methods: 这篇论文使用了模拟数据生成的 conditional normalizing flows 技术来快速和准确地计算狮子时间尺度数组（SGWB）的 posterior distribution，从原来的数天减少到只需几秒钟。</li>
<li>results: 该论文的实验结果表明，使用 conditional normalizing flows 技术可以在狮子时间尺度数组（SGWB）的 posterior distribution 计算中大幅提高效率，从原来的数天减少到只需几秒钟。<details>
<summary>Abstract</summary>
Pulsar timing arrays (PTAs) perform Bayesian posterior inference with expensive MCMC methods. Given a dataset of ~10-100 pulsars and O(10^3) timing residuals each, producing a posterior distribution for the stochastic gravitational wave background (SGWB) can take days to a week. The computational bottleneck arises because the likelihood evaluation required for MCMC is extremely costly when considering the dimensionality of the search space. Fortunately, generating simulated data is fast, so modern simulation-based inference techniques can be brought to bear on the problem. In this paper, we demonstrate how conditional normalizing flows trained on simulated data can be used for extremely fast and accurate estimation of the SGWB posteriors, reducing the sampling time from weeks to a matter of seconds.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Dynamic-financial-processes-identification-using-sparse-regressive-reservoir-computers"><a href="#Dynamic-financial-processes-identification-using-sparse-regressive-reservoir-computers" class="headerlink" title="Dynamic financial processes identification using sparse regressive reservoir computers"></a>Dynamic financial processes identification using sparse regressive reservoir computers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12144">http://arxiv.org/abs/2310.12144</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fredyvides/dynet-cnbs">https://github.com/fredyvides/dynet-cnbs</a></li>
<li>paper_authors: Fredy Vides, Idelfonso B. R. Nogueira, Lendy Banegas, Evelyn Flores</li>
<li>For: 本文研究结构矩阵近似理论，应用于财经系统动态过程的回归表示。* Methods: 使用非线性时间延迟嵌入、稀疏最小二乘和结构矩阵近似方法来探索财经系统的输出封顶矩阵的近似表示。* Results: 通过应用上述技术，可以实现财经系统动态过程的近似识别和预测，包括可能或可能不具有混沌行为的场景。<details>
<summary>Abstract</summary>
In this document, we present key findings in structured matrix approximation theory, with applications to the regressive representation of dynamic financial processes. Initially, we explore a comprehensive approach involving generic nonlinear time delay embedding for time series data extracted from a financial or economic system under examination. Subsequently, we employ sparse least-squares and structured matrix approximation methods to discern approximate representations of the output coupling matrices. These representations play a pivotal role in establishing the regressive models corresponding to the recursive structures inherent in a given financial system. The document further introduces prototypical algorithms that leverage the aforementioned techniques. These algorithms are demonstrated through applications in approximate identification and predictive simulation of dynamic financial and economic processes, encompassing scenarios that may or may not exhibit chaotic behavior.
</details>
<details>
<summary>摘要</summary>
在本文中，我们介绍了结构化矩阵近似理论的关键发现，并应用于金融或经济系统中的回归表现力学过程的重构表示。我们首先探讨了一种通用非线性时间延迟嵌入方法，用于从金融或经济系统中提取时间序列数据。然后，我们使用稀疏最小二乘和结构矩阵近似方法来推导出输出 coupling 矩阵的近似表示。这些表示在建立金融系统中的回归模型中扮演关键角色。文章还介绍了一些原型算法，这些算法利用上述技术来实现精度的回归预测和模拟。这些算法在不同的金融和经济过程中的应用中得到了证明。
</details></li>
</ul>
<hr>
<h2 id="Automatic-prediction-of-mortality-in-patients-with-mental-illness-using-electronic-health-records"><a href="#Automatic-prediction-of-mortality-in-patients-with-mental-illness-using-electronic-health-records" class="headerlink" title="Automatic prediction of mortality in patients with mental illness using electronic health records"></a>Automatic prediction of mortality in patients with mental illness using electronic health records</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12121">http://arxiv.org/abs/2310.12121</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sean Kim, Samuel Kim</li>
<li>for: 预测精神疾病患者30天 mortality rate</li>
<li>methods: 使用predictive machine-learning models with electronic health records (EHR)，包括Logistic Regression、Random Forest、Support Vector Machine和K-Nearest Neighbors四种机器学习算法</li>
<li>results: Random Forest和Support Vector Machine模型表现最佳，AUC分数为0.911，Feature importance分析显示 morphine sulfate等药物具有预测作用。<details>
<summary>Abstract</summary>
Mental disorders impact the lives of millions of people globally, not only impeding their day-to-day lives but also markedly reducing life expectancy. This paper addresses the persistent challenge of predicting mortality in patients with mental diagnoses using predictive machine-learning models with electronic health records (EHR). Data from patients with mental disease diagnoses were extracted from the well-known clinical MIMIC-III data set utilizing demographic, prescription, and procedural information. Four machine learning algorithms (Logistic Regression, Random Forest, Support Vector Machine, and K-Nearest Neighbors) were used, with results indicating that Random Forest and Support Vector Machine models outperformed others, with AUC scores of 0.911. Feature importance analysis revealed that drug prescriptions, particularly Morphine Sulfate, play a pivotal role in prediction. We applied a variety of machine learning algorithms to predict 30-day mortality followed by feature importance analysis. This study can be used to assist hospital workers in identifying at-risk patients to reduce excess mortality.
</details>
<details>
<summary>摘要</summary>
精神疾病影响全球数百万人的生活，不仅妨碍日常生活，而且明显减少生存期。本文使用可预测机器学习模型和电子健康纪录（EHR）预测患有精神诊断的患者 mortality。从 клиничеwell-known MIMIC-III数据集中提取了患有精神疾病诊断的患者数据，并使用LOGISTIC REGRESSION、Random Forest、Support Vector Machine和K-Nearest Neighbors四种机器学习算法。结果表明，Random Forest和Support Vector Machine模型在其他四个模型中表现最佳，AUC分数为0.911。特征重要性分析显示，药物处方，特别是摩革定（Morphine Sulfate），在预测中扮演着关键性角色。我们通过不同的机器学习算法预测30天内死亡，并进行特征重要性分析，以帮助医院工作人员识别高风险患者，从而减少过度死亡。
</details></li>
</ul>
<hr>
<h2 id="MMD-based-Variable-Importance-for-Distributional-Random-Forest"><a href="#MMD-based-Variable-Importance-for-Distributional-Random-Forest" class="headerlink" title="MMD-based Variable Importance for Distributional Random Forest"></a>MMD-based Variable Importance for Distributional Random Forest</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12115">http://arxiv.org/abs/2310.12115</a></li>
<li>repo_url: None</li>
<li>paper_authors: Clément Bénard, Jeffrey Näf, Julie Josse</li>
<li>for: 这篇论文目的是提出一种基于森林方法的全Conditional分布估计方法，用于 Multivariate output of interest 的输入变量。</li>
<li>methods: 该论文使用了 Drop and relearn 原理和MMD距离来实现变量重要性度量，而传统的重要性度量仅检测输出均值的影响变量。</li>
<li>results: 引入的重要性度量是一致的，在实际数据和模拟数据上具有高效性，并且超越竞争者。特别是，该算法可以通过回归特征减少来选择变量，从而提供高精度的Conditional输出分布估计。<details>
<summary>Abstract</summary>
Distributional Random Forest (DRF) is a flexible forest-based method to estimate the full conditional distribution of a multivariate output of interest given input variables. In this article, we introduce a variable importance algorithm for DRFs, based on the well-established drop and relearn principle and MMD distance. While traditional importance measures only detect variables with an influence on the output mean, our algorithm detects variables impacting the output distribution more generally. We show that the introduced importance measure is consistent, exhibits high empirical performance on both real and simulated data, and outperforms competitors. In particular, our algorithm is highly efficient to select variables through recursive feature elimination, and can therefore provide small sets of variables to build accurate estimates of conditional output distributions.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用 Distributional Random Forest（DRF）来估算输入变量的多变量输出分布。在本文中，我们提出了基于drop和重新学习原则以及MMD距离的变量重要性算法，该算法可捕捉输入变量对输出分布的影响，而不仅仅是输出均值。我们证明了该算法的一致性和高效性，并在实际和预测数据上实现了比较高的表现。特别是，我们的算法可以通过 recursively feature elimination来选择变量，从而快速建立高精度的 conditional output distribution 估计。Translation notes:* "Distributional Random Forest" is translated as "多变量随机森林" (mányuànxīn sēn lín)* "full conditional distribution" is translated as "完整的分布" (quèzhè de fēn xiǎng)* "variable importance" is translated as "变量重要性" (biànxīn zhòng yào xìng)* "drop and relearn principle" is translated as "drop和重新学习原则" (drop hé zhòng xīn xué xí yuè)* "MMD distance" is translated as "MMD距离" (MMD jù lù)* "recursive feature elimination" is translated as " recursively feature elimination" (jiē yǐjī zhì xiǎng fāng yì)
</details></li>
</ul>
<hr>
<h2 id="Monarch-Mixer-A-Simple-Sub-Quadratic-GEMM-Based-Architecture"><a href="#Monarch-Mixer-A-Simple-Sub-Quadratic-GEMM-Based-Architecture" class="headerlink" title="Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture"></a>Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12109">http://arxiv.org/abs/2310.12109</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HazyResearch/m2">https://github.com/HazyResearch/m2</a></li>
<li>paper_authors: Daniel Y. Fu, Simran Arora, Jessica Grogan, Isys Johnson, Sabri Eyuboglu, Armin W. Thomas, Benjamin Spector, Michael Poli, Atri Rudra, Christopher Ré</li>
<li>for: 这篇论文目的是探讨是否存在可以在序列长度和模型维度上呈现高性能的、不 quadratic 的机器学习模型。</li>
<li>methods: 该论文提出了一种新的 Monarch Mixer（M2）架构，使用同样的不 quadratic  primitives来处理序列长度和模型维度：Monarch 矩阵，一种简单的可表示性structured 矩阵，可以在 GPU 上实现高硬件效率，并且可以呈现高性能。</li>
<li>results: 作为证明，该论文在三个领域中explored M2 的性能：非 causal BERT 样式语言模型、ViT 样式图像分类和 causal GPT 样式语言模型。在非 causal BERT 样式模型中，M2 与 BERT-base 和 BERT-large 相比，在下游 GLUE 质量上具有相同的性能，并且可以达到更高的通过put 性能（最高达 9.1 倍）。在 ImageNet 上，M2 超过 ViT-b 的准确率，仅使用半个参数。在 causal GPT 样式模型中，M2 可以与 Transformer 相比，在 360M 参数的预训练质量上具有相同的性能。<details>
<summary>Abstract</summary>
Machine learning models are increasingly being scaled in both sequence length and model dimension to reach longer contexts and better performance. However, existing architectures such as Transformers scale quadratically along both these axes. We ask: are there performant architectures that can scale sub-quadratically along sequence length and model dimension? We introduce Monarch Mixer (M2), a new architecture that uses the same sub-quadratic primitive along both sequence length and model dimension: Monarch matrices, a simple class of expressive structured matrices that captures many linear transforms, achieves high hardware efficiency on GPUs, and scales sub-quadratically. As a proof of concept, we explore the performance of M2 in three domains: non-causal BERT-style language modeling, ViT-style image classification, and causal GPT-style language modeling. For non-causal BERT-style modeling, M2 matches BERT-base and BERT-large in downstream GLUE quality with up to 27% fewer parameters, and achieves up to 9.1$\times$ higher throughput at sequence length 4K. On ImageNet, M2 outperforms ViT-b by 1% in accuracy, with only half the parameters. Causal GPT-style models introduce a technical challenge: enforcing causality via masking introduces a quadratic bottleneck. To alleviate this bottleneck, we develop a novel theoretical view of Monarch matrices based on multivariate polynomial evaluation and interpolation, which lets us parameterize M2 to be causal while remaining sub-quadratic. Using this parameterization, M2 matches GPT-style Transformers at 360M parameters in pretraining perplexity on The PILE--showing for the first time that it may be possible to match Transformer quality without attention or MLPs.
</details>
<details>
<summary>摘要</summary>
机器学习模型在序列长度和模型维度上逐渐升级以达到更长的上下文和更高的性能。然而，现有的架构，如Transformers，在这两个轴上 quadratic scaling。我们问：是否存在高性能的架构，可以在序列长度和模型维度上下降幂？我们介绍了一新的架构：宫廷混合器（M2），它使用同样的幂次性 primitive来序列长度和模型维度：宫廷矩阵，一种简单的表达 Structured matrices 的类型，可以在 GPU 上实现高硬件效率，并在序列长度和模型维度上下降幂。作为一个证明，我们探索了 M2 在三个领域的性能：非 causal BERT 风格语言模型、ViT 风格图像分类和 causal GPT 风格语言模型。在非 causal BERT 风格模型中，M2 与 BERT-base 和 BERT-large 相当在下游 GLUE 质量上，并且在序列长度 4K 时间点可以达到 9.1 倍的throughput，而且只需要 27% 的参数。在 ImageNet 上，M2 超过 ViT-b 的准确率，只需要一半的参数。 causal GPT 风格模型引入了一个技术挑战：在 маSKing 中引入的 quadratic bottleneck。为了缓解这个瓶颈，我们开发了一种新的理论视角，基于多Variable 多项式评估和插值，这使得我们可以在 M2 中使用 causal 参数化，而不是 quadratic 参数化。使用这种参数化，M2 与 GPT 风格 Transformers 在 360M 参数的预训练损失上匹配，这是第一次证明可以不使用注意力或 MLP 匹配 Transformer 质量。
</details></li>
</ul>
<hr>
<h2 id="An-Online-Learning-Theory-of-Brokerage"><a href="#An-Online-Learning-Theory-of-Brokerage" class="headerlink" title="An Online Learning Theory of Brokerage"></a>An Online Learning Theory of Brokerage</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12107">http://arxiv.org/abs/2310.12107</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nataša Bolić, Tommaso Cesari, Roberto Colomboni</li>
<li>For: The paper is written for investigating brokerage between traders from an online learning perspective, with a focus on the case where there are no designated buyer and seller roles.* Methods: The paper uses online learning techniques to achieve a low regret bound in the brokerage problem, specifically providing algorithms achieving regret $M \log T$ and $\sqrt{M T}$ under different assumptions about the agents’ valuations.* Results: The paper shows that the optimal regret rate is $M \log T$ when the agents’ valuations are revealed after each interaction, and $\sqrt{M T}$ when only their willingness to sell or buy at the proposed price is revealed. Additionally, the paper demonstrates that the optimal rate degrades to $\sqrt{T}$ when the bounded density assumption is dropped.<details>
<summary>Abstract</summary>
We investigate brokerage between traders from an online learning perspective. At any round $t$, two traders arrive with their private valuations, and the broker proposes a trading price. Unlike other bilateral trade problems already studied in the online learning literature, we focus on the case where there are no designated buyer and seller roles: each trader will attempt to either buy or sell depending on the current price of the good.   We assume the agents' valuations are drawn i.i.d. from a fixed but unknown distribution. If the distribution admits a density bounded by some constant $M$, then, for any time horizon $T$:   $\bullet$ If the agents' valuations are revealed after each interaction, we provide an algorithm achieving regret $M \log T$ and show this rate is optimal, up to constant factors.   $\bullet$ If only their willingness to sell or buy at the proposed price is revealed after each interaction, we provide an algorithm achieving regret $\sqrt{M T}$ and show this rate is optimal, up to constant factors.   Finally, if we drop the bounded density assumption, we show that the optimal rate degrades to $\sqrt{T}$ in the first case, and the problem becomes unlearnable in the second.
</details>
<details>
<summary>摘要</summary>
我们研究在线学习中的经纪人交易。在任意的回合 $t$ 中，两个经纪人会 arrive  WITH 他们的私人估价，经纪人会提议交易价格。与其他双方贸易问题已经在在线学习文献中研究过的不同，我们专注于情况下没有指定的买方和卖方角色：每个经纪人都会尝试 Either 购买或卖出，根据当前商品价格。  我们假设经纪人的估价是从固定而 unknown 的分布中随机样本。如果该分布具有最大值 $M$，那么，对于任意的时间 horizon $T$：❝ 如果经纪人的估价在每次交互后公布，我们提供了一个算法，其 regret 为 $M \log T$，并证明这个率是最佳的，占常数因子。❞❝ 如果只有经纪人对于提议价格的愿意性被公布在每次交互后，我们提供了一个算法，其 regret 为 $\sqrt{M T}$，并证明这个率是最佳的，占常数因子。❞最后，如果我们取消了均勋度 bound 的假设，我们显示了最佳率下降到 $\sqrt{T}$ 在第一个情况下，并问题变得不可学习在第二个情况下。
</details></li>
</ul>
<hr>
<h2 id="On-the-latent-dimension-of-deep-autoencoders-for-reduced-order-modeling-of-PDEs-parametrized-by-random-fields"><a href="#On-the-latent-dimension-of-deep-autoencoders-for-reduced-order-modeling-of-PDEs-parametrized-by-random-fields" class="headerlink" title="On the latent dimension of deep autoencoders for reduced order modeling of PDEs parametrized by random fields"></a>On the latent dimension of deep autoencoders for reduced order modeling of PDEs parametrized by random fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12095">http://arxiv.org/abs/2310.12095</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicola Rares Franco, Daniel Fraulin, Andrea Manzoni, Paolo Zunino</li>
<li>for: 这篇论文的目的是提供对随机场生成的Stochastic Partial Differential Equations (SPDEs)中Deep Learning-based Reduced Order Models (DL-ROMs)的理论分析。</li>
<li>methods: 本文使用了深度学习自动编码器作为ROMs的基础工具，并通过非线性神经网络的能力来减少问题的维度。</li>
<li>results: 本文提供了关于DL-ROMs在随机场中的理论分析，并提供了可导的错误 bound，以帮助域专家在选择深度学习自动编码器的缓存维度时进行优化。 数据示例表明，本文的分析对DL-ROMs的性能产生了显著的影响。<details>
<summary>Abstract</summary>
Deep Learning is having a remarkable impact on the design of Reduced Order Models (ROMs) for Partial Differential Equations (PDEs), where it is exploited as a powerful tool for tackling complex problems for which classical methods might fail. In this respect, deep autoencoders play a fundamental role, as they provide an extremely flexible tool for reducing the dimensionality of a given problem by leveraging on the nonlinear capabilities of neural networks. Indeed, starting from this paradigm, several successful approaches have already been developed, which are here referred to as Deep Learning-based ROMs (DL-ROMs). Nevertheless, when it comes to stochastic problems parameterized by random fields, the current understanding of DL-ROMs is mostly based on empirical evidence: in fact, their theoretical analysis is currently limited to the case of PDEs depending on a finite number of (deterministic) parameters. The purpose of this work is to extend the existing literature by providing some theoretical insights about the use of DL-ROMs in the presence of stochasticity generated by random fields. In particular, we derive explicit error bounds that can guide domain practitioners when choosing the latent dimension of deep autoencoders. We evaluate the practical usefulness of our theory by means of numerical experiments, showing how our analysis can significantly impact the performance of DL-ROMs.
</details>
<details>
<summary>摘要</summary>
深度学习对减少顺序模型（ROMs）的设计产生了深刻的影响，特别是在解决复杂问题上，其中经典方法可能会失败时。在这个情况下，深度自适应神经网络扮演了非常重要的角色，因为它们可以通过神经网络的非线性能力来减少问题的维度。从这个角度出发，已经有许多成功的方法被开发出来，这些方法被称为深度学习基于ROMs（DL-ROMs）。然而，当面临随机场所 parametrized 的问题时，现有的理论分析仅限于具有固定数量的 deterministic 参数的PDEs。本文的目的是扩展现有的文献，提供关于DL-ROMs在随机场所下的理论分析。特别是，我们 derive 了明确的错误 bound，可以帮助域专家在选择深度自适应神经网络的缓存维度时作出决策。我们通过数值实验证明了我们的理论对DL-ROMs的性能产生了显著的影响。
</details></li>
</ul>
<hr>
<h2 id="Contributing-Components-of-Metabolic-Energy-Models-to-Metabolic-Cost-Estimations-in-Gait"><a href="#Contributing-Components-of-Metabolic-Energy-Models-to-Metabolic-Cost-Estimations-in-Gait" class="headerlink" title="Contributing Components of Metabolic Energy Models to Metabolic Cost Estimations in Gait"></a>Contributing Components of Metabolic Energy Models to Metabolic Cost Estimations in Gait</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12083">http://arxiv.org/abs/2310.12083</a></li>
<li>repo_url: None</li>
<li>paper_authors: Markus Gambietz, Marlies Nitschke, Jörg Miehling, Anne Koelewijn</li>
<li>for: 这个研究旨在深入理解人类行走中的代谢能量消耗模型，以便更好地估计代谢能量消耗。</li>
<li>methods: 我们使用了四种代谢能量消耗模型的参数进行 Monte Carlo 敏感分析，然后分析了这些参数的敏感指数、生理上的Context和生理过程中的代谢率。最终选择了一个 quasi-优化的模型。在第二步，我们 investigate了输入参数和变量的重要性，通过使用不同的输入特征来训练神经网络。</li>
<li>results: 我们发现，力量相关的参数在敏感分析中最为重要，而神经网络基于的输入特征选择也显示了承诺。然而，我们发现，使用神经网络模型的代谢能量消耗估计并没有达到传统模型的准确性。<details>
<summary>Abstract</summary>
Objective: As metabolic cost is a primary factor influencing humans' gait, we want to deepen our understanding of metabolic energy expenditure models. Therefore, this paper identifies the parameters and input variables, such as muscle or joint states, that contribute to accurate metabolic cost estimations. Methods: We explored the parameters of four metabolic energy expenditure models in a Monte Carlo sensitivity analysis. Then, we analysed the model parameters by their calculated sensitivity indices, physiological context, and the resulting metabolic rates during the gait cycle. The parameter combination with the highest accuracy in the Monte Carlo simulations represented a quasi-optimized model. In the second step, we investigated the importance of input parameters and variables by analysing the accuracy of neural networks trained with different input features. Results: Power-related parameters were most influential in the sensitivity analysis and the neural network-based feature selection. We observed that the quasi-optimized models produced negative metabolic rates, contradicting muscle physiology. Neural network-based models showed promising abilities but have been unable to match the accuracy of traditional metabolic energy expenditure models. Conclusion: We showed that power-related metabolic energy expenditure model parameters and inputs are most influential during gait. Furthermore, our results suggest that neural network-based metabolic energy expenditure models are viable. However, bigger datasets are required to achieve better accuracy. Significance: As there is a need for more accurate metabolic energy expenditure models, we explored which musculoskeletal parameters are essential when developing a model to estimate metabolic energy.
</details>
<details>
<summary>摘要</summary>
方法：我们在四种代谢能耗模型中进行了Monte Carlo敏感分析，然后分析了模型参数的计算敏感度指数、生理上的文脉和代谢过程中的代谢率。在Monte Carlo优化中，我们选择了最佳的参数组合，并在第二步中，通过不同输入特征的分析，了解输入参数和变量的重要性。结果：在敏感分析中，力量相关的参数具有最大的影响力，而神经网络基于的特征选择也显示了扩展的能力。然而，我们发现，在许多情况下，神经网络模型的准确性不如传统的代谢能耗模型。结论：我们发现，在步行过程中，力量相关的代谢能耗模型参数和输入变量具有最大的影响力。此外，我们的结果表明，神经网络基于的代谢能耗模型是可行的，但需要更大的数据来达到更高的准确性。重要性：由于代谢成本的估计是一个需要更加准确的问题，我们在这篇论文中探讨了 Musculoskeletal 参数是如何影响代谢能耗模型的。
</details></li>
</ul>
<hr>
<h2 id="Differential-Equation-Scaling-Limits-of-Shaped-and-Unshaped-Neural-Networks"><a href="#Differential-Equation-Scaling-Limits-of-Shaped-and-Unshaped-Neural-Networks" class="headerlink" title="Differential Equation Scaling Limits of Shaped and Unshaped Neural Networks"></a>Differential Equation Scaling Limits of Shaped and Unshaped Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12079">http://arxiv.org/abs/2310.12079</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mufan Bill Li, Mihai Nica</li>
<li>for: 这篇论文是关于无形activation函数的神经网络性能的研究，尤其是对于两种不同的无形网络架构（即Fully Connected ResNet和Multilayer Perceptron）的性能分析。</li>
<li>methods: 本文使用了不同的方法来分析无形网络的性能，包括使用差分方程来描述无形网络的架构，以及使用初值问题来分析无形网络的层次相关性。</li>
<li>results: 本文发现了两种无形网络架构在初始化时的相同架构准确性限制，并且对无形MLP网络的层次相关性进行了第一项级准确性修正。这些结果表明了无形网络和形态activation函数之间的连接，并开 up了研究正则化方法和形态activation函数之间的关系的可能性。<details>
<summary>Abstract</summary>
Recent analyses of neural networks with shaped activations (i.e. the activation function is scaled as the network size grows) have led to scaling limits described by differential equations. However, these results do not a priori tell us anything about "ordinary" unshaped networks, where the activation is unchanged as the network size grows. In this article, we find similar differential equation based asymptotic characterization for two types of unshaped networks.   Firstly, we show that the following two architectures converge to the same infinite-depth-and-width limit at initialization: (i) a fully connected ResNet with a $d^{-1/2}$ factor on the residual branch, where $d$ is the network depth. (ii) a multilayer perceptron (MLP) with depth $d \ll$ width $n$ and shaped ReLU activation at rate $d^{-1/2}$.   Secondly, for an unshaped MLP at initialization, we derive the first order asymptotic correction to the layerwise correlation. In particular, if $\rho_\ell$ is the correlation at layer $\ell$, then $q_t = \ell^2 (1 - \rho_\ell)$ with $t = \frac{\ell}{n}$ converges to an SDE with a singularity at $t=0$.   These results together provide a connection between shaped and unshaped network architectures, and opens up the possibility of studying the effect of normalization methods and how it connects with shaping activation functions.
</details>
<details>
<summary>摘要</summary>
近期的分析表明，在神经网络中使用扩展 activation function（即网络大小增长时Activation function也随着增长）会导致分析限制，这些结果并不直接告诉我们关于“常规”无形网络（即Activation function不变化与网络大小增长）的 anything。在这篇文章中，我们发现了两种类型的无形网络的极限性特征，即：首先，我们证明了以下两个架构在初始化时 converges to the same infinite-depth-and-width limit：（i）一个具有 $d^{-1/2}$ 因子的完全连接 ResNet，其中 $d$ 是网络深度。（ii）一个具有 $d \ll n$ 的多层感知器（MLP），其中 $d$ 是网络深度， activation 是 $d^{-1/2}$ 的折叠函数。其次，对于无形 MLP 的初始化，我们 derive the first order asymptotic correction to the layerwise correlation。 Specifically, if $\rho_\ell$ is the correlation at layer $\ell$, then $q_t = \ell^2 (1 - \rho_\ell)$ with $t = \frac{\ell}{n}$ converges to an SDE with a singularity at $t=0$.这些结果共同表明了无形和形 activation function 之间的连接，并开放了研究正规化方法和 activation function 的拟合方面的可能性。
</details></li>
</ul>
<hr>
<h2 id="Transformers-for-scientific-data-a-pedagogical-review-for-astronomers"><a href="#Transformers-for-scientific-data-a-pedagogical-review-for-astronomers" class="headerlink" title="Transformers for scientific data: a pedagogical review for astronomers"></a>Transformers for scientific data: a pedagogical review for astronomers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12069">http://arxiv.org/abs/2310.12069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitrios Tanoglidis, Bhuvnesh Jain, Helen Qu</li>
<li>for: 该论文主要用于引入transformers深度学习架构和相关的生成AI产品，并为科学家介绍transformers的应用。</li>
<li>methods: 论文使用自注意机制和原始transformer架构，并介绍了在天文学中使用transformers的应用。</li>
<li>results: 论文介绍了自注意机制的数学基础和transformers的应用在时间序列和图像数据中的成果。Note: The above information is in Simplified Chinese text.<details>
<summary>Abstract</summary>
The deep learning architecture associated with ChatGPT and related generative AI products is known as transformers. Initially applied to Natural Language Processing, transformers and the self-attention mechanism they exploit have gained widespread interest across the natural sciences. The goal of this pedagogical and informal review is to introduce transformers to scientists. The review includes the mathematics underlying the attention mechanism, a description of the original transformer architecture, and a section on applications to time series and imaging data in astronomy. We include a Frequently Asked Questions section for readers who are curious about generative AI or interested in getting started with transformers for their research problem.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>与ChatGPT和相关的生成AI产品相关的深度学习架构被称为transformers。初始应用于自然语言处理，transformers和它们利用的自注意机制已经在自然科学领域引起了广泛的关注。本文的教学和非正式评论的目的是引入transformers给科学家。文中包括自注意机制的数学基础、原始transformer架构的描述和在天文学中对时间序列和图像数据的应用。我们附加了关于生成AI或想要使用transformers解决研究问题的常见问题 section。
</details></li>
</ul>
<hr>
<h2 id="Learning-Gradient-Fields-for-Scalable-and-Generalizable-Irregular-Packing"><a href="#Learning-Gradient-Fields-for-Scalable-and-Generalizable-Irregular-Packing" class="headerlink" title="Learning Gradient Fields for Scalable and Generalizable Irregular Packing"></a>Learning Gradient Fields for Scalable and Generalizable Irregular Packing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19814">http://arxiv.org/abs/2310.19814</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyang Xue, Mingdong Wu, Lin Lu, Haoxuan Wang, Hao Dong, Baoquan Chen</li>
<li>for:  solves the packing problem with irregularly shaped pieces, minimizing waste and avoiding overlap, using machine learning and conditional generative modeling.</li>
<li>methods:  employs the score-based diffusion model to learn gradient fields that encode constraint satisfaction and spatial relationships, and uses a coarse-to-fine refinement mechanism to generate packing solutions.</li>
<li>results:  demonstrates spatial utilization rates comparable to or surpassing those achieved by the teacher algorithm, and exhibits some level of generalization to shape variations.<details>
<summary>Abstract</summary>
The packing problem, also known as cutting or nesting, has diverse applications in logistics, manufacturing, layout design, and atlas generation. It involves arranging irregularly shaped pieces to minimize waste while avoiding overlap. Recent advances in machine learning, particularly reinforcement learning, have shown promise in addressing the packing problem. In this work, we delve deeper into a novel machine learning-based approach that formulates the packing problem as conditional generative modeling. To tackle the challenges of irregular packing, including object validity constraints and collision avoidance, our method employs the score-based diffusion model to learn a series of gradient fields. These gradient fields encode the correlations between constraint satisfaction and the spatial relationships of polygons, learned from teacher examples. During the testing phase, packing solutions are generated using a coarse-to-fine refinement mechanism guided by the learned gradient fields. To enhance packing feasibility and optimality, we introduce two key architectural designs: multi-scale feature extraction and coarse-to-fine relation extraction. We conduct experiments on two typical industrial packing domains, considering translations only. Empirically, our approach demonstrates spatial utilization rates comparable to, or even surpassing, those achieved by the teacher algorithm responsible for training data generation. Additionally, it exhibits some level of generalization to shape variations. We are hopeful that this method could pave the way for new possibilities in solving the packing problem.
</details>
<details>
<summary>摘要</summary>
<<SYS>> packing 问题，也称为割辑或嵌入，在物流、制造、布局设计和地图生成中有广泛的应用。它涉及到将不规则形状的物品安排，以最小化剩下物和避免重叠。 recent advances in machine learning，特别是强化学习，对 packing 问题提出了新的思路。在这项工作中，我们将更深入地探讨一种基于机器学习的新方法，将 packing 问题转化为 conditional generative modeling。为了解决不规则嵌入中的挑战，包括物体有效性约束和碰撞避免，我们的方法使用分数据模型来学习一系列的梯度场。这些梯度场表达了对约束满足和物体间的空间关系的学习。在测试阶段，我们使用一种粗细层次匀化机制，以指导学习的梯度场来生成嵌入解。为了提高嵌入可行性和优化，我们引入了两种关键的建筑设计：多尺度特征提取和粗细层次关系提取。我们对两种典型的工业嵌入领域进行实验，只考虑翻译。实验结果表明，我们的方法可以与教师算法负责数据生成的空间利用率相当，甚至超过。此外，它还有一定的泛化能力。我们希望这种方法可以为嵌入问题开拓新的可能性。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Reward-Ambiguity-Through-Optimal-Transport-Theory-in-Inverse-Reinforcement-Learning"><a href="#Understanding-Reward-Ambiguity-Through-Optimal-Transport-Theory-in-Inverse-Reinforcement-Learning" class="headerlink" title="Understanding Reward Ambiguity Through Optimal Transport Theory in Inverse Reinforcement Learning"></a>Understanding Reward Ambiguity Through Optimal Transport Theory in Inverse Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12055">http://arxiv.org/abs/2310.12055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Baheri</li>
<li>for: 这 paper 的中心目标是寻找在观察到的专家行为中隐藏的奖励函数，以便不仅解释数据，还能够泛化到未经见过的情况。</li>
<li>methods: 这 paper 使用 optimal transport (OT) 理论，提供了一种新的视角来解决高维问题和奖励不确定性的问题。</li>
<li>results: 这 paper 的研究发现，通过 Wasserstein 距离来衡量奖励不确定性，并提供了一种中心表示或中心函数的确定方法，这些发现可以为高维 setting 中的 robust IRL 方法提供一种结构化的途径。<details>
<summary>Abstract</summary>
In inverse reinforcement learning (IRL), the central objective is to infer underlying reward functions from observed expert behaviors in a way that not only explains the given data but also generalizes to unseen scenarios. This ensures robustness against reward ambiguity where multiple reward functions can equally explain the same expert behaviors. While significant efforts have been made in addressing this issue, current methods often face challenges with high-dimensional problems and lack a geometric foundation. This paper harnesses the optimal transport (OT) theory to provide a fresh perspective on these challenges. By utilizing the Wasserstein distance from OT, we establish a geometric framework that allows for quantifying reward ambiguity and identifying a central representation or centroid of reward functions. These insights pave the way for robust IRL methodologies anchored in geometric interpretations, offering a structured approach to tackle reward ambiguity in high-dimensional settings.
</details>
<details>
<summary>摘要</summary>
倒 inverse reinforcement learning（IRL）的中心目标是从专家行为中推理出底层奖励函数，以解释数据并在未看到的情况下推广。这 Ensures  robustness  against 奖励ambiguity， where multiple 奖励函数可以一样 explain 专家行为。 although  significant efforts have been made to address this issue, current methods often face challenges with high-dimensional problems and lack a geometric foundation.this paper  harnesses the optimal transport（OT）theory to provide a fresh perspective on these challenges. By utilizing the Wasserstein distance from OT, we establish a geometric framework that allows for quantifying 奖励ambiguity and identifying a central representation or centroid of reward functions. These insights pave the way for robust IRL methodologies anchored in geometric interpretations, offering a structured approach to tackle 奖励ambiguity in high-dimensional settings.
</details></li>
</ul>
<hr>
<h2 id="Applications-of-ML-Based-Surrogates-in-Bayesian-Approaches-to-Inverse-Problems"><a href="#Applications-of-ML-Based-Surrogates-in-Bayesian-Approaches-to-Inverse-Problems" class="headerlink" title="Applications of ML-Based Surrogates in Bayesian Approaches to Inverse Problems"></a>Applications of ML-Based Surrogates in Bayesian Approaches to Inverse Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12046">http://arxiv.org/abs/2310.12046</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pelin Ersin, Emma Hayes, Peter Matthews, Paramjyoti Mohapatra, Elisa Negrini, Karl Schulz</li>
<li>for: 寻找波源位置在方正区域的逆问题，给出噪音解的解决方案。</li>
<li>methods: 使用神经网络作为代理模型，提高计算效率，使得Markov Chain Monte Carlo方法可以用于评估 posterior 分布中的源位置。</li>
<li>results: 通过寻找波源位置的方法，可以准确地从噪音数据中提取源位置信息。<details>
<summary>Abstract</summary>
Neural networks have become a powerful tool as surrogate models to provide numerical solutions for scientific problems with increased computational efficiency. This efficiency can be advantageous for numerically challenging problems where time to solution is important or when evaluation of many similar analysis scenarios is required. One particular area of scientific interest is the setting of inverse problems, where one knows the forward dynamics of a system are described by a partial differential equation and the task is to infer properties of the system given (potentially noisy) observations of these dynamics. We consider the inverse problem of inferring the location of a wave source on a square domain, given a noisy solution to the 2-D acoustic wave equation. Under the assumption of Gaussian noise, a likelihood function for source location can be formulated, which requires one forward simulation of the system per evaluation. Using a standard neural network as a surrogate model makes it computationally feasible to evaluate this likelihood several times, and so Markov Chain Monte Carlo methods can be used to evaluate the posterior distribution of the source location. We demonstrate that this method can accurately infer source-locations from noisy data.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:神经网络已成为数学问题的强大工具，提供了计算效率的增强。这种效率可以在计算复杂的问题中帮助提高解决时间，或者在评估多个相似的分析场景时提高计算效率。一个科学领域的特别兴趣是反问题，即知道系统的前向动力学方程，并且要从（潜在噪声）观测中推断系统的性质。我们考虑了二维声波方程的反问题，即在平方Domain中推断声源的位置，给出噪声解的情况。在假设 Gaussian 噪声时，可以形式化一个likelihood函数，该函数需要一次前向模拟 per 评估。使用标准神经网络作为模拟模型，可以使计算这个likelihood多次成为可能，然后使用Markov Chain Monte Carlo 方法评估 posterior 分布。我们示示了这种方法可以准确地从噪声数据中推断声源位置。
</details></li>
</ul>
<hr>
<h2 id="Conformal-Drug-Property-Prediction-with-Density-Estimation-under-Covariate-Shift"><a href="#Conformal-Drug-Property-Prediction-with-Density-Estimation-under-Covariate-Shift" class="headerlink" title="Conformal Drug Property Prediction with Density Estimation under Covariate Shift"></a>Conformal Drug Property Prediction with Density Estimation under Covariate Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12033">http://arxiv.org/abs/2310.12033</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/siddharthal/CoDrug">https://github.com/siddharthal/CoDrug</a></li>
<li>paper_authors: Siddhartha Laghuvarapu, Zhen Lin, Jimeng Sun</li>
<li>for:  This paper aims to address the challenge of obtaining reliable uncertainty estimates in drug discovery tasks using Conformal Prediction (CP) and to provide valid prediction sets for molecular properties with a coverage guarantee.</li>
<li>methods:  The proposed method, CoDrug, employs an energy-based model leveraging both training data and unlabelled data, and Kernel Density Estimation (KDE) to assess the densities of a molecule set. The estimated densities are then used to weigh the molecule samples while building prediction sets and rectifying for distribution shift.</li>
<li>results:  In extensive experiments involving realistic distribution drifts in various small-molecule drug discovery tasks, CoDrug was shown to provide valid prediction sets and to reduce the coverage gap by over 35% when compared to conformal prediction sets not adjusted for covariate shift.<details>
<summary>Abstract</summary>
In drug discovery, it is vital to confirm the predictions of pharmaceutical properties from computational models using costly wet-lab experiments. Hence, obtaining reliable uncertainty estimates is crucial for prioritizing drug molecules for subsequent experimental validation. Conformal Prediction (CP) is a promising tool for creating such prediction sets for molecular properties with a coverage guarantee. However, the exchangeability assumption of CP is often challenged with covariate shift in drug discovery tasks: Most datasets contain limited labeled data, which may not be representative of the vast chemical space from which molecules are drawn. To address this limitation, we propose a method called CoDrug that employs an energy-based model leveraging both training data and unlabelled data, and Kernel Density Estimation (KDE) to assess the densities of a molecule set. The estimated densities are then used to weigh the molecule samples while building prediction sets and rectifying for distribution shift. In extensive experiments involving realistic distribution drifts in various small-molecule drug discovery tasks, we demonstrate the ability of CoDrug to provide valid prediction sets and its utility in addressing the distribution shift arising from de novo drug design models. On average, using CoDrug can reduce the coverage gap by over 35% when compared to conformal prediction sets not adjusted for covariate shift.
</details>
<details>
<summary>摘要</summary>
在药物发现中，确认计算模型预测的药品性能需要通过costly的湿lab实验进行验证。因此，获得可靠的不确定性估计是关键的，以便在后续实验验证中PRIORITIZE drug molecules。 Conformal Prediction（CP）是一种可靠的工具，可以创建包含预测性能的prediction sets。然而，CP中的交换性假设在药物发现任务中经常遇到冲击：大多数数据集只包含有限的标签数据，这些数据可能不能代表整个化学空间中的分子。为解决这个限制，我们提出了一种方法called CoDrug，它使用能量基本模型利用训练数据和无标签数据，以及Kernel Density Estimation（KDE）来评估分子集的浓度。然后，使用这些估计的浓度来权重分子样本，以建立预测集和纠正分布shift。在具有实际分布滑动的小分子药物发现任务中，我们通过实验证明CoDrug可以提供有效的预测集，并且在de novo drug design模型中 Addressing the distribution shift。在average上，使用CoDrug可以将覆盖缺口减少超过35%，比不 Rectifying for distribution shift。
</details></li>
</ul>
<hr>
<h2 id="Exact-and-efficient-solutions-of-the-LMC-Multitask-Gaussian-Process-model"><a href="#Exact-and-efficient-solutions-of-the-LMC-Multitask-Gaussian-Process-model" class="headerlink" title="Exact and efficient solutions of the LMC Multitask Gaussian Process model"></a>Exact and efficient solutions of the LMC Multitask Gaussian Process model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12032">http://arxiv.org/abs/2310.12032</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qwerty6191/projected-lmc">https://github.com/qwerty6191/projected-lmc</a></li>
<li>paper_authors: Olivier Truffinet, Karim Ammar, Jean-Philippe Argaud, Bertrand Bouriquet</li>
<li>for: 这个论文是关于多任务 Gaussian process  regression 或分类的一种非常通用的模型，它的表达能力和概念简单性很吸引人。但是，直接实现方式的复杂性是 cubic 在数据点和任务数量的平方，这意味着大多数应用中需要使用简化方法。然而，最近的研究表明，在某些条件下，模型的隐藏过程可以分离，从而实现 linear 复杂性。</li>
<li>methods: 我们在这篇论文中扩展了这些结果，并证明了在最通用的假设下，只需要一个轻度的噪声模型假设，就可以实现高效的精确计算。我们还提出了一种完整的参数化方法，并给出了质量函数，以便高效地优化。</li>
<li>results: 我们在synthetic数据上进行了参数研究，并证明了我们的方法的出色表现，相比之下unrestricted exact LMC和其他简化方法。总之， проекed LMC 模型是一种可靠和简单的代用方法，它可以大大简化一些计算，如离散一个数据点的cross-validation和幻想。<details>
<summary>Abstract</summary>
The Linear Model of Co-regionalization (LMC) is a very general model of multitask gaussian process for regression or classification. While its expressivity and conceptual simplicity are appealing, naive implementations have cubic complexity in the number of datapoints and number of tasks, making approximations mandatory for most applications. However, recent work has shown that under some conditions the latent processes of the model can be decoupled, leading to a complexity that is only linear in the number of said processes. We here extend these results, showing from the most general assumptions that the only condition necessary to an efficient exact computation of the LMC is a mild hypothesis on the noise model. We introduce a full parametrization of the resulting \emph{projected LMC} model, and an expression of the marginal likelihood enabling efficient optimization. We perform a parametric study on synthetic data to show the excellent performance of our approach, compared to an unrestricted exact LMC and approximations of the latter. Overall, the projected LMC appears as a credible and simpler alternative to state-of-the art models, which greatly facilitates some computations such as leave-one-out cross-validation and fantasization.
</details>
<details>
<summary>摘要</summary>
linear 模型的协同地域化 (LMC) 是一种非常通用的多任务 Gaussian 过程 regression 或 classification 模型。 虽其表达能力和概念简洁吸引人，但直接实现的方法具有 кубиック complexity 在数据点和任务数量上，使得大多数应用中需要使用 Approximations。然而，最近的研究表明，在某些条件下，latent 过程的模型可以减少，导致只有 linear 复杂度在数据点和任务数量上。我们在这里扩展这些结果，表明只需要对模型噪声模型进行一定的假设，就可以实现高效的精确计算。我们介绍了该模型的完整均衡 parametrization，并提供了计算 marginal 概率的表达，使得可以高效地优化。我们在synthetic数据上进行了参数研究，并证明了我们的方法在比较于未限制的精确 LMC 和其approximations 上表现出色。总之，Projected LMC 模型看起来是一种可靠和简单的代码，它可以大大简化一些计算，如离开一个 cross-validation 和幻想。
</details></li>
</ul>
<hr>
<h2 id="Nonparametric-Discrete-Choice-Experiments-with-Machine-Learning-Guided-Adaptive-Design"><a href="#Nonparametric-Discrete-Choice-Experiments-with-Machine-Learning-Guided-Adaptive-Design" class="headerlink" title="Nonparametric Discrete Choice Experiments with Machine Learning Guided Adaptive Design"></a>Nonparametric Discrete Choice Experiments with Machine Learning Guided Adaptive Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12026">http://arxiv.org/abs/2310.12026</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingzhang Yin, Ruijiang Gao, Weiran Lin, Steven M. Shugan</li>
<li>For: 这个论文旨在设计用于满足消费者偏好的产品，以提高企业的成功。* Methods: 论文提出了一种名为 Gradient-based Survey (GBS) 的不 Parametric 的选择实验方法，用于多Attribute 产品设计。GBS 通过问题序列和响应者之前的选择来逐步定义产品特性。* Results: 对于在 simulations 中进行比较的 parametric 和非 Parametric 方法，GBS 具有更高的准确率和样本效率。<details>
<summary>Abstract</summary>
Designing products to meet consumers' preferences is essential for a business's success. We propose the Gradient-based Survey (GBS), a discrete choice experiment for multiattribute product design. The experiment elicits consumer preferences through a sequence of paired comparisons for partial profiles. GBS adaptively constructs paired comparison questions based on the respondents' previous choices. Unlike the traditional random utility maximization paradigm, GBS is robust to model misspecification by not requiring a parametric utility model. Cross-pollinating the machine learning and experiment design, GBS is scalable to products with hundreds of attributes and can design personalized products for heterogeneous consumers. We demonstrate the advantage of GBS in accuracy and sample efficiency compared to the existing parametric and nonparametric methods in simulations.
</details>
<details>
<summary>摘要</summary>
为商业成功，设计产品根据消费者的偏好非常重要。我们提议 Gradient-based Survey（GBS），一种多Attribute产品设计的灵活选择实验。这种实验通过一系列对半个配置进行对比，抽取消费者的偏好。与传统的随机Utility最大化理论不同，GBS不需要 Parametric Utility模型，因此更具鲁棒性。通过融合机器学习和实验设计，GBS可扩展到产品上百个特征，设计个性化产品 для多样化的消费者。我们通过模拟表明，GBS在准确性和样本效率方面比现有的参数化和非参数化方法有优势。
</details></li>
</ul>
<hr>
<h2 id="Iterative-Methods-for-Vecchia-Laplace-Approximations-for-Latent-Gaussian-Process-Models"><a href="#Iterative-Methods-for-Vecchia-Laplace-Approximations-for-Latent-Gaussian-Process-Models" class="headerlink" title="Iterative Methods for Vecchia-Laplace Approximations for Latent Gaussian Process Models"></a>Iterative Methods for Vecchia-Laplace Approximations for Latent Gaussian Process Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12000">http://arxiv.org/abs/2310.12000</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fabsig/GPBoost">https://github.com/fabsig/GPBoost</a></li>
<li>paper_authors: Pascal Kündig, Fabio Sigrist</li>
<li>for: 这篇论文旨在探讨高维函数模型（Gaussian Process，GP）的精确估计方法，以及组合Vecchia-Laplace近似法和迭代法的优化。</li>
<li>methods: 本论文使用Vecchia-Laplace近似法和迭代法来实现高维函数模型的精确估计，并提出了一些iterative方法来提高 computations的速度。</li>
<li>results: 论文的实验结果显示，使用Vecchia-Laplace近似法和迭代法可以大幅提高估计的速度，并且在一个大型卫星数据集上比起现有方法实现三倍的预测精度。<details>
<summary>Abstract</summary>
Latent Gaussian process (GP) models are flexible probabilistic non-parametric function models. Vecchia approximations are accurate approximations for GPs to overcome computational bottlenecks for large data, and the Laplace approximation is a fast method with asymptotic convergence guarantees to approximate marginal likelihoods and posterior predictive distributions for non-Gaussian likelihoods. Unfortunately, the computational complexity of combined Vecchia-Laplace approximations grows faster than linearly in the sample size when used in combination with direct solver methods such as the Cholesky decomposition. Computations with Vecchia-Laplace approximations thus become prohibitively slow precisely when the approximations are usually the most accurate, i.e., on large data sets. In this article, we present several iterative methods for inference with Vecchia-Laplace approximations which make computations considerably faster compared to Cholesky-based calculations. We analyze our proposed methods theoretically and in experiments with simulated and real-world data. In particular, we obtain a speed-up of an order of magnitude compared to Cholesky-based inference and a threefold increase in prediction accuracy in terms of the continuous ranked probability score compared to a state-of-the-art method on a large satellite data set. All methods are implemented in a free C++ software library with high-level Python and R packages.
</details>
<details>
<summary>摘要</summary>
潜在 Gaussian 过程（GP）模型是一种灵活的可信度非参数函数模型。Vecchia  aproximations 是一种精准的GP模型 Approximations 可以在大量数据时提高计算效率，而 Laplace  Approximations 是一种快速的方法，它具有 asymptotic convergence guarantees 来近似 marginal likelihoods 和 posterior predictive distributions 的非 Gaussian 类型。然而，Vecchia-Laplace  approximations 的计算复杂度随着样本大小增加，使用 direct solver methods such as Cholesky decomposition 时会变得不可持久。因此，在大数据集时，Vecchia-Laplace  approximations 的计算变得繁琐。在这篇文章中，我们提出了一些迭代法来实现Vecchia-Laplace approximations 的推理，使计算速度比 Cholesky-based 计算更快。我们还进行了理论分析和实验室测试，并在 simulated 和实际数据集上 obtaint 一个级别的速度提升和三倍的预测精度。所有方法都是在一个免费 C++ 软件库中实现的，并提供了高级 Python 和 R 包。
</details></li>
</ul>
<hr>
<h2 id="Removing-Spurious-Concepts-from-Neural-Network-Representations-via-Joint-Subspace-Estimation"><a href="#Removing-Spurious-Concepts-from-Neural-Network-Representations-via-Joint-Subspace-Estimation" class="headerlink" title="Removing Spurious Concepts from Neural Network Representations via Joint Subspace Estimation"></a>Removing Spurious Concepts from Neural Network Representations via Joint Subspace Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11991">http://arxiv.org/abs/2310.11991</a></li>
<li>repo_url: None</li>
<li>paper_authors: Floris Holstege, Bram Wouters, Noud van Giersbergen, Cees Diks</li>
<li>for: 本研究旨在提高神经网络模型对异常数据的泛化性能，通过禁用干扰因素。</li>
<li>methods: 本研究提出了一种迭代算法，通过同时确定两个低维度正交子空间来分离干扰因素和主任务因素。</li>
<li>results: 对 Wasserstein 和 CelebA 图像Dataset以及 MultiNLI 自然语言处理Dataset进行评估，发现该算法可以超过现有的概念除除法。<details>
<summary>Abstract</summary>
Out-of-distribution generalization in neural networks is often hampered by spurious correlations. A common strategy is to mitigate this by removing spurious concepts from the neural network representation of the data. Existing concept-removal methods tend to be overzealous by inadvertently eliminating features associated with the main task of the model, thereby harming model performance. We propose an iterative algorithm that separates spurious from main-task concepts by jointly identifying two low-dimensional orthogonal subspaces in the neural network representation. We evaluate the algorithm on benchmark datasets for computer vision (Waterbirds, CelebA) and natural language processing (MultiNLI), and show that it outperforms existing concept removal methods
</details>
<details>
<summary>摘要</summary>
neural networks 中的 out-of-distribution 泛化受到假 correlate 的干扰。一般的方法是通过 removing spurious concepts 来 mitigate 这种情况。现有的 concept-removal 方法往往过于积极，不小心 eliminating 主要任务相关的特征，从而害到模型性能。我们提出了一种迭代算法，jointly identifying two low-dimensional orthogonal subspaces 在 neural network representation 中，以分离假 correlations 和主要任务相关的特征。我们在 Waterbirds、CelebA 和 MultiNLI 等 benchmark datasets 上评估了该算法，并显示它在 existing concept removal 方法 上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Image-Clustering-with-External-Guidance"><a href="#Image-Clustering-with-External-Guidance" class="headerlink" title="Image Clustering with External Guidance"></a>Image Clustering with External Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11989">http://arxiv.org/abs/2310.11989</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunfan Li, Peng Hu, Dezhong Peng, Jiancheng Lv, Jianping Fan, Xi Peng</li>
<li>for: 提高图像归一化的性能，利用外部知识作为指导信号</li>
<li>methods: 利用WordNet词语来增强特征分化，并在图像和文本模式之间进行相互馈散学习</li>
<li>results: 在五个常用的图像归一化 benchmark 上达到了状态机的性能，包括全部 ImageNet-1K 数据集<details>
<summary>Abstract</summary>
The core of clustering is incorporating prior knowledge to construct supervision signals. From classic k-means based on data compactness to recent contrastive clustering guided by self-supervision, the evolution of clustering methods intrinsically corresponds to the progression of supervision signals. At present, substantial efforts have been devoted to mining internal supervision signals from data. Nevertheless, the abundant external knowledge such as semantic descriptions, which naturally conduces to clustering, is regrettably overlooked. In this work, we propose leveraging external knowledge as a new supervision signal to guide clustering, even though it seems irrelevant to the given data. To implement and validate our idea, we design an externally guided clustering method (Text-Aided Clustering, TAC), which leverages the textual semantics of WordNet to facilitate image clustering. Specifically, TAC first selects and retrieves WordNet nouns that best distinguish images to enhance the feature discriminability. Then, to improve image clustering performance, TAC collaborates text and image modalities by mutually distilling cross-modal neighborhood information. Experiments demonstrate that TAC achieves state-of-the-art performance on five widely used and three more challenging image clustering benchmarks, including the full ImageNet-1K dataset.
</details>
<details>
<summary>摘要</summary>
核心是在含有先验知识的情况下构建监督信号。从经典k-means基于数据压缩到最近的对照集成监督，集群方法的演化都与监督信号的进步相对应。到目前为止，大量的内部监督信号从数据中被挖掘出来。然而，外部知识，如semantic description，尚未得到了适当的利用。在这种情况下，我们提议利用外部知识作为新的监督信号，即使它与给定数据看起来不相关。为了实现和验证我们的想法，我们设计了一种受外部知识引导的集群方法（Text-Aided Clustering，TAC）。TAC首先选择和检索WordNet词汇，以增强特征描述性。然后，为了提高图像集群性能，TAC与文本和图像模式之间进行协同整合，通过相互洗礼距离信息。实验表明，TAC在5个广泛使用的和3个更加挑战的图像集群 benchmark上达到了状态机器人的性能。
</details></li>
</ul>
<hr>
<h2 id="A-Finite-Horizon-Approach-to-Active-Level-Set-Estimation"><a href="#A-Finite-Horizon-Approach-to-Active-Level-Set-Estimation" class="headerlink" title="A Finite-Horizon Approach to Active Level Set Estimation"></a>A Finite-Horizon Approach to Active Level Set Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11985">http://arxiv.org/abs/2310.11985</a></li>
<li>repo_url: None</li>
<li>paper_authors: Phillip Kearns, Bruno Jedynak, John Lipor</li>
<li>for: 本文目的是提出一种活动学习方法来实现等值集 estimation（LSE），以最小化最终估计误差和旅行距离。</li>
<li>methods: 本文使用一种finite-horizon搜索过程来实现LSE，并通过调整一个参数来让方法兼顾估计准确性和旅行距离。</li>
<li>results: 实验表明，当cost of travel增加时，我们的方法可以更好地使用距离非偏视来提高估计精度，并在真实的空气质量数据上实现约一半的估计误差。<details>
<summary>Abstract</summary>
We consider the problem of active learning in the context of spatial sampling for level set estimation (LSE), where the goal is to localize all regions where a function of interest lies above/below a given threshold as quickly as possible. We present a finite-horizon search procedure to perform LSE in one dimension while optimally balancing both the final estimation error and the distance traveled for a fixed number of samples. A tuning parameter is used to trade off between the estimation accuracy and distance traveled. We show that the resulting optimization problem can be solved in closed form and that the resulting policy generalizes existing approaches to this problem. We then show how this approach can be used to perform level set estimation in higher dimensions under the popular Gaussian process model. Empirical results on synthetic data indicate that as the cost of travel increases, our method's ability to treat distance nonmyopically allows it to significantly improve on the state of the art. On real air quality data, our approach achieves roughly one fifth the estimation error at less than half the cost of competing algorithms.
</details>
<details>
<summary>摘要</summary>
我们在各种空间采样中考虑了活动学习，其目标是尽可能快地找到一个函数关注的区域是否超过了一定的阈值。我们提出了一种有限距离搜索过程，用于在一维中进行最优化的水平集估计，同时尽量减少最终估计误差和旅行距离。我们使用一个调整参数，以让最终估计误差和旅行距离之间进行负面交易。我们表明，这个优化问题可以在关闭式形式下解决，并且得到的策略可以折衔现有方法。然后，我们展示了如何使用这种方法来进行高维空间下的水平集估计，使用泊松过程模型。我们的实验结果表明，当成本增加时，我们的方法可以不偏袋见茫地减少估计误差。在实际空气质量数据上，我们的方法可以实现约一剑五分之一的估计误差，而且花费比竞争算法少得多。
</details></li>
</ul>
<hr>
<h2 id="Can-bin-wise-scaling-improve-consistency-and-adaptivity-of-prediction-uncertainty-for-machine-learning-regression"><a href="#Can-bin-wise-scaling-improve-consistency-and-adaptivity-of-prediction-uncertainty-for-machine-learning-regression" class="headerlink" title="Can bin-wise scaling improve consistency and adaptivity of prediction uncertainty for machine learning regression ?"></a>Can bin-wise scaling improve consistency and adaptivity of prediction uncertainty for machine learning regression ?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11978">http://arxiv.org/abs/2310.11978</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ppernot/2023_bvs">https://github.com/ppernot/2023_bvs</a></li>
<li>paper_authors: Pascal Pernot</li>
<li>for: 这篇论文是为了提出一种基于不同变量的准备误差抑制方法，以提高机器学习回归问题的预测不确定性calibration的效果。</li>
<li>methods: 这篇论文使用了 uncertainty-based binning 方法，通过基于不同变量的分配来改进calibration的条件，即consistency。</li>
<li>results: 作者在一个 benchmark 数据集上测试了 BVS 和其变体，与 isotonic regression 进行比较，发现 BVS 和其变体可以更好地适应不同的输入特征，提高calibration的效果。<details>
<summary>Abstract</summary>
Binwise Variance Scaling (BVS) has recently been proposed as a post hoc recalibration method for prediction uncertainties of machine learning regression problems that is able of more efficient corrections than uniform variance (or temperature) scaling. The original version of BVS uses uncertainty-based binning, which is aimed to improve calibration conditionally on uncertainty, i.e. consistency. I explore here several adaptations of BVS, in particular with alternative loss functions and a binning scheme based on an input-feature (X) in order to improve adaptivity, i.e. calibration conditional on X. The performances of BVS and its proposed variants are tested on a benchmark dataset for the prediction of atomization energies and compared to the results of isotonic regression.
</details>
<details>
<summary>摘要</summary>
Binwise Variance Scaling (BVS) 是一种最近提出的机器学习回归问题预测不确定性的后处修正方法，能够更有效地 corrections than uniform variance (或温度) scaling。原版本的 BVS 使用不确定性基于的分类，以提高预测条件上的准确性，即一致性。我在这里 explore 了 BVS 的一些变体，包括使用不同的损失函数和基于输入特征（X）的分类方案，以提高适应性，即预测条件下的准确性。我们对一个 benchmark 数据集进行了预测 atomization energies 的测试，并与ISOREG 的结果进行了比较。Here's the translation in Traditional Chinese: Binwise Variance Scaling (BVS) 是一种最近提出的机器学习回归问题的预测不确定性的后置修正方法，能够更有效地 corrections than uniform variance (或温度) scaling。原版本的 BVS 使用不确定性基于的分类，以提高预测条件上的准确性，即一致性。我在这里 explore 了 BVS 的一些变体，包括使用不同的损失函数和基于输入特征（X）的分类方案，以提高适应性，即预测条件下的准确性。我们对一个 benchmark 数据集进行了预测 atomization energies 的测试，并与ISOREG 的结果进行了比较。
</details></li>
</ul>
<hr>
<h2 id="Take-the-aTrain-Introducing-an-Interface-for-the-Accessible-Transcription-of-Interviews"><a href="#Take-the-aTrain-Introducing-an-Interface-for-the-Accessible-Transcription-of-Interviews" class="headerlink" title="Take the aTrain. Introducing an Interface for the Accessible Transcription of Interviews"></a>Take the aTrain. Introducing an Interface for the Accessible Transcription of Interviews</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11967">http://arxiv.org/abs/2310.11967</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bandas-center/atrain">https://github.com/bandas-center/atrain</a></li>
<li>paper_authors: Armin Haberl, Jürgen Fleiß, Dominik Kowald, Stefan Thalmann</li>
<li>for: 这个论文是为了帮助研究人员进行多种语言的语音数据转译，不需要编程技能，可以在大多数计算机上运行，不需要互联网连线，且不会上传数据到服务器。</li>
<li>methods: 论文使用OpenAI的Whisper模型和声音识别技术，与MAXQDA和ATLAS.ti等流行的质量数据分析软件集成，提供了易于使用的图形用户界面，可以通过Microsoft Store上的Windows应用程序安装。</li>
<li>results: 根据论文的描述，在现有的移动CPU上，转译时间约为音频档案的2-3倍，如果有入门级的图形卡，则转译速度增加到音频档案的20%。<details>
<summary>Abstract</summary>
aTrain is an open-source and offline tool for transcribing audio data in multiple languages with CPU and NVIDIA GPU support. It is specifically designed for researchers using qualitative data generated from various forms of speech interactions with research participants. aTrain requires no programming skills, runs on most computers, does not require an internet connection, and was verified not to upload data to any server. aTrain combines OpenAI's Whisper model with speaker recognition to provide output that integrates with the popular qualitative data analysis software tools MAXQDA and ATLAS.ti. It has an easy-to-use graphical interface and is provided as a Windows-App through the Microsoft Store allowing for simple installation by researchers. The source code is freely available on GitHub. Having developed aTrain with a focus on speed on local computers, we show that the transcription time on current mobile CPUs is around 2 to 3 times the duration of the audio file using the highest-accuracy transcription models. If an entry-level graphics card is available, the transcription speed increases to 20% of the audio duration.
</details>
<details>
<summary>摘要</summary>
aTrain 是一个开源、离线工具，用于转换多种语言的语音数据。它是特意针对对谈话参与者的质数数据进行研究而设计，并且不需要程式码技能，可以在大多数电脑上运行，不需要网页连线，并且确保没有上传数据到服务器。aTrain 结合 OpenAI 的 Whisper 模型和话者识别系统，以提供与 MAXQDA 和 ATLAS.ti 等受欢迎的质数数据分析软件集成。它具有易用的 графі式界面，通过 Microsoft Store 提供为 Windows 应用程序，让研究人员可以简单地安装。源代码则是免费公开在 GitHub 上。我们透过专注于本地电脑的速度，显示在现有的移动 CPU 上，转换时间约为音频档案的2-3倍，使用最高精度转换模型。如果有入门级的显卡可用，则转换速度将提高到音频档案的20%。
</details></li>
</ul>
<hr>
<h2 id="Flexible-Payload-Configuration-for-Satellites-using-Machine-Learning"><a href="#Flexible-Payload-Configuration-for-Satellites-using-Machine-Learning" class="headerlink" title="Flexible Payload Configuration for Satellites using Machine Learning"></a>Flexible Payload Configuration for Satellites using Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11966">http://arxiv.org/abs/2310.11966</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcele O. K. Mendonca, Flor G. Ortiz-Gomez, Jorge Querol, Eva Lagunas, Juan A. Vásquez Peralvo, Victor Monzon Baeza, Symeon Chatzinotas, Bjorn Ottersten</li>
<li>for: 提高卫星通信系统的效率和质量，适应不同的吞吐量和延迟要求。</li>
<li>methods: 使用机器学习（ML）技术进行无线资源管理（RRM），将RRM任务定义为一个回归型ML问题，并将RRM目标和约束集成到损失函数中，以便ML算法尽可能地减小。</li>
<li>results: 通过对ML模型的表现进行评估，并考虑模型的资源分配决策对总体通信系统性能的影响，提出了一种Context-aware ML metric。<details>
<summary>Abstract</summary>
Satellite communications, essential for modern connectivity, extend access to maritime, aeronautical, and remote areas where terrestrial networks are unfeasible. Current GEO systems distribute power and bandwidth uniformly across beams using multi-beam footprints with fractional frequency reuse. However, recent research reveals the limitations of this approach in heterogeneous traffic scenarios, leading to inefficiencies. To address this, this paper presents a machine learning (ML)-based approach to Radio Resource Management (RRM).   We treat the RRM task as a regression ML problem, integrating RRM objectives and constraints into the loss function that the ML algorithm aims at minimizing. Moreover, we introduce a context-aware ML metric that evaluates the ML model's performance but also considers the impact of its resource allocation decisions on the overall performance of the communication system.
</details>
<details>
<summary>摘要</summary>
卫星通信，现代连接的关键，扩展至海上、航空和远郊地区， terrestrial 网络无法实现。现有的 GEO 系统在多个扫描面上均匀分配功率和频率，使用多扫描面 fractional frequency reuse。然而， latest research 显示这种方法在多样化流量场景下存在限制，导致不充分利用。为解决这个问题，这篇论文提出一种基于机器学习（ML）的Radio Resource Management（RRM）方法。我们将 RRM 任务视为一个回归 ML 问题，将 RRM 目标和约束 integrate 到 ML 算法目标函数中。此外，我们还引入了一种 context-aware ML 指标，评估 ML 模型的性能，同时考虑它的资源分配决策对通信系统的总性能的影响。
</details></li>
</ul>
<hr>
<h2 id="Recasting-Continual-Learning-as-Sequence-Modeling"><a href="#Recasting-Continual-Learning-as-Sequence-Modeling" class="headerlink" title="Recasting Continual Learning as Sequence Modeling"></a>Recasting Continual Learning as Sequence Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11952">http://arxiv.org/abs/2310.11952</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/soochan-lee/cl-as-seq">https://github.com/soochan-lee/cl-as-seq</a></li>
<li>paper_authors: Soochan Lee, Jaehyeon Son, Gunhee Kim</li>
<li>for: 本研究旨在将重要的机器学习领域——启发学习和序列模型——加强连接起来。即我们提议将启发学习视为序列模型问题，使高级序列模型可以用于启发学习。在这种形式下，启发学习过程变成了序列模型的前进传播。</li>
<li>methods: 我们采用了元 continual learning（MCL）框架，在多个启发学习集合中训练序列模型。作为具体的例子，我们示cases了使用 transformers 和其高效变体作为 MCL 方法。</li>
<li>results: 我们在七个 bencmarks 上进行了七个benchmark，包括分类和回归任务，结果显示了序列模型可以是通用 MCL 的有ffektive解决方案。<details>
<summary>Abstract</summary>
In this work, we aim to establish a strong connection between two significant bodies of machine learning research: continual learning and sequence modeling. That is, we propose to formulate continual learning as a sequence modeling problem, allowing advanced sequence models to be utilized for continual learning. Under this formulation, the continual learning process becomes the forward pass of a sequence model. By adopting the meta-continual learning (MCL) framework, we can train the sequence model at the meta-level, on multiple continual learning episodes. As a specific example of our new formulation, we demonstrate the application of Transformers and their efficient variants as MCL methods. Our experiments on seven benchmarks, covering both classification and regression, show that sequence models can be an attractive solution for general MCL.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们想要建立两个机器学习研究领域之间的强有力连接：不间断学习和序列模型。即我们提议将不间断学习问题设置为序列模型问题，以便使用高级序列模型进行不间断学习。根据这种设置，不间断学习过程变成了序列模型的前向传播。通过采用meta-不间断学习（MCL）框架，我们可以在多个不间断学习集合上训练序列模型。为了示例，我们展示了使用Transformers和其高效变体作为MCL方法的应用。我们在七个标准准确的benchmark上进行了七种不同的实验，包括分类和回归任务，结果表明序列模型可以是通用MCL的有效解决方案。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Spectral-Variational-AutoEncoder-ISVAE-for-time-series-clustering"><a href="#Interpretable-Spectral-Variational-AutoEncoder-ISVAE-for-time-series-clustering" class="headerlink" title="Interpretable Spectral Variational AutoEncoder (ISVAE) for time series clustering"></a>Interpretable Spectral Variational AutoEncoder (ISVAE) for time series clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11940">http://arxiv.org/abs/2310.11940</a></li>
<li>repo_url: None</li>
<li>paper_authors: Óscar Jiménez Rama, Fernando Moreno-Pino, David Ramírez, Pablo M. Olmos</li>
<li>for: 这篇论文是为了提出一种新的变量自动编码器（VAE）模型，该模型具有可解释性的瓶颈（Filter Bank，FB），以便学习更加可解释的潜在空间。</li>
<li>methods: 该模型使用了VAE的基本结构，并在其前置了FB。FB强制VAE关注输入信号中最重要的部分，从而学习一个新的编码${f_0}$，该编码具有更高的可解释性和分化性。</li>
<li>results: 实验结果表明，ISVAE模型比传统的VAE模型在分类率上表现更高，并且可以更好地处理复杂的数据配置。此外，${f_0}$的演化征imatters表明了群集之间的相似性。<details>
<summary>Abstract</summary>
The best encoding is the one that is interpretable in nature. In this work, we introduce a novel model that incorporates an interpretable bottleneck-termed the Filter Bank (FB)-at the outset of a Variational Autoencoder (VAE). This arrangement compels the VAE to attend on the most informative segments of the input signal, fostering the learning of a novel encoding ${f_0}$ which boasts enhanced interpretability and clusterability over traditional latent spaces. By deliberately constraining the VAE with this FB, we intentionally constrict its capacity to access broad input domain information, promoting the development of an encoding that is discernible, separable, and of reduced dimensionality. The evolutionary learning trajectory of ${f_0}$ further manifests as a dynamic hierarchical tree, offering profound insights into cluster similarities. Additionally, for handling intricate data configurations, we propose a tailored decoder structure that is symmetrically aligned with FB's architecture. Empirical evaluations highlight the superior efficacy of ISVAE, which compares favorably to state-of-the-art results in clustering metrics across real-world datasets.
</details>
<details>
<summary>摘要</summary>
最佳编码是可解释的编码。在这项工作中，我们提出了一种新的模型，其中包含了一个可解释的瓶颈（Filter Bank，FB），这个瓶颈位于Variational Autoencoder（VAE）的开头。这种设计使得VAE需要关注输入信号中最重要的信息，从而促进了学习一个新的编码${f_0}$，该编码具有更高的可解释性和分布性。通过强制VAE通过FB进行制约，我们故意削弱VAE对输入信号范围广的信息访问权限，从而促进了编码的可读性、分割性和维度减少。${f_0}$的演化学习轨迹更显示出了动态层次树的形式，提供了深刻的群集相似性的启示。此外，为处理复杂的数据配置，我们提议一种适应FB的编码结构。实验证明，ISVAE的效果明显高于州际级的结果，在真实世界数据集上达到了高度的分 clustering  metrics。
</details></li>
</ul>
<hr>
<h2 id="Accelerated-Policy-Gradient-On-the-Nesterov-Momentum-for-Reinforcement-Learning"><a href="#Accelerated-Policy-Gradient-On-the-Nesterov-Momentum-for-Reinforcement-Learning" class="headerlink" title="Accelerated Policy Gradient: On the Nesterov Momentum for Reinforcement Learning"></a>Accelerated Policy Gradient: On the Nesterov Momentum for Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11897">http://arxiv.org/abs/2310.11897</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nycu-rl-bandits-lab/apg">https://github.com/nycu-rl-bandits-lab/apg</a></li>
<li>paper_authors: Yen-Ju Chen, Nai-Chieh Huang, Ping-Chun Hsieh</li>
<li>for: 本研究证明了使用推移 momentum 加速度 gradient 方法可以在 reinforcement learning 中提高 converges 率。</li>
<li>methods: 本文使用 Nesterov 加速度 gradient 方法（NAG），并对其进行了适应以适应 reinforcement learning 中的 softmax 政策参数化。</li>
<li>results: 我们表明了 NAG 在 true gradient 下可以在 $\tilde{O}(1&#x2F;t^2)$ 时间复杂度下连续 converges 到优化的政策。此外，我们还通过数值验证表明了 NAG 可以在实际应用中提高 converge 性。<details>
<summary>Abstract</summary>
Policy gradient methods have recently been shown to enjoy global convergence at a $\Theta(1/t)$ rate in the non-regularized tabular softmax setting. Accordingly, one important research question is whether this convergence rate can be further improved, with only first-order updates. In this paper, we answer the above question from the perspective of momentum by adapting the celebrated Nesterov's accelerated gradient (NAG) method to reinforcement learning (RL), termed \textit{Accelerated Policy Gradient} (APG). To demonstrate the potential of APG in achieving faster global convergence, we formally show that with the true gradient, APG with softmax policy parametrization converges to an optimal policy at a $\tilde{O}(1/t^2)$ rate. To the best of our knowledge, this is the first characterization of the global convergence rate of NAG in the context of RL. Notably, our analysis relies on one interesting finding: Regardless of the initialization, APG could end up reaching a locally nearly-concave regime, where APG could benefit significantly from the momentum, within finite iterations. By means of numerical validation, we confirm that APG exhibits $\tilde{O}(1/t^2)$ rate as well as show that APG could significantly improve the convergence behavior over the standard policy gradient.
</details>
<details>
<summary>摘要</summary>
We formally show that with the true gradient, APG with softmax policy parametrization converges to an optimal policy at a $\tilde{O}(1/t^2)$ rate. This is the first characterization of the global convergence rate of NAG in the context of RL. Our analysis relies on one interesting finding: regardless of the initialization, APG could end up reaching a locally nearly-concave regime, where APG could benefit significantly from the momentum, within finite iterations.Numerical validation confirms that APG exhibits a $\tilde{O}(1/t^2)$ rate and shows that APG could significantly improve the convergence behavior over the standard policy gradient.
</details></li>
</ul>
<hr>
<h2 id="A-Hyperparameter-Study-for-Quantum-Kernel-Methods"><a href="#A-Hyperparameter-Study-for-Quantum-Kernel-Methods" class="headerlink" title="A Hyperparameter Study for Quantum Kernel Methods"></a>A Hyperparameter Study for Quantum Kernel Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11891">http://arxiv.org/abs/2310.11891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sebastian Egginger, Alona Sakhnenko, Jeanette Miriam Lorenz</li>
<li>for: 本研究旨在investigating the effects of hyperparameter choice on the model performance and the generalization gap between classical and quantum kernels, and exploring the use of the geometric difference as a tool for evaluating the potential for quantum advantage.</li>
<li>methods: 本研究使用了 quantum kernel methods and hyperparameter optimization techniques to evaluate the performance of quantum and classical machine learning models on 11 datasets. The geometric difference was used as a closeness measure between the two kernel-based machine learning approaches.</li>
<li>results: 研究发现，hyperparameter optimization是critical for achieving good model performance and reducing the generalization gap between classical and quantum kernels. The geometric difference can be a useful tool for evaluating the potential for quantum advantage, and can help identify commodities that can be exploited when examining a new dataset.<details>
<summary>Abstract</summary>
Quantum kernel methods are a promising method in quantum machine learning thanks to the guarantees connected to them. Their accessibility for analytic considerations also opens up the possibility of prescreening datasets based on their potential for a quantum advantage. To do so, earlier works developed the geometric difference, which can be understood as a closeness measure between two kernel-based machine learning approaches, most importantly between a quantum kernel and classical kernel. This metric links the quantum and classical model complexities. Therefore, it raises the question of whether the geometric difference, based on its relation to model complexity, can be a useful tool in evaluations other than for the potential for quantum advantage. In this work, we investigate the effects of hyperparameter choice on the model performance and the generalization gap between classical and quantum kernels. The importance of hyperparameter optimization is well known also for classical machine learning. Especially for the quantum Hamiltonian evolution feature map, the scaling of the input data has been shown to be crucial. However, there are additional parameters left to be optimized, like the best number of qubits to trace out before computing a projected quantum kernel. We investigate the influence of these hyperparameters and compare the classically reliable method of cross validation with the method of choosing based on the geometric difference. Based on the thorough investigation of the hyperparameters across 11 datasets we identified commodities that can be exploited when examining a new dataset. In addition, our findings contribute to better understanding of the applicability of the geometric difference.
</details>
<details>
<summary>摘要</summary>
量子kernels方法是量子机器学习中的一种有前途的方法，这主要归功于它们的 garantías。它们的可见性使得可以对数据进行预选择，以确定它们是否具有量子优势。以前的工作在开发了 геомétríain difference，这可以理解为两种基于kernel的机器学习方法之间的距离度量，主要是quantum kernel和классическийkernel之间的距离。这个指标连接了量子和классиical模型复杂性。因此，它提出了问题，是否可以通过其与模型复杂性的关系来使用 geometric difference 作为评估工具？在这种工作中，我们investigate了hyperparameter的选择对模型性能和量子和классиical kernel之间的泛化差异的影响。特别是 для量子 Hamiltonian 演化特征图，输入数据的涨落Scaling 已经被证明是关键。然而，还有其他参数需要优化，例如最佳的量子bits数量来计算projected quantum kernel。我们 investigate了这些超参数的影响，并将cross validation 方法与基于 geometric difference 的选择方法进行比较。通过对 11 个数据集进行了全面的超参数调整，我们发现了一些可以利用的商品，并对量子和классиical kernel之间的泛化差异进行了更好的理解。
</details></li>
</ul>
<hr>
<h2 id="Building-a-Graph-based-Deep-Learning-network-model-from-captured-traffic-traces"><a href="#Building-a-Graph-based-Deep-Learning-network-model-from-captured-traffic-traces" class="headerlink" title="Building a Graph-based Deep Learning network model from captured traffic traces"></a>Building a Graph-based Deep Learning network model from captured traffic traces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11889">http://arxiv.org/abs/2310.11889</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlos Güemes-Palau, Miquel Ferriol Galmés, Albert Cabellos-Aparicio, Pere Barlet-Ros</li>
<li>for: 本研究旨在提出一种基于图神经网络（GNN）的解决方案，用于更好地捕捉实际网络场景中的复杂性。</li>
<li>methods: 本研究使用了一种新的编码方法，用于从捕捉的包序列中提取信息，以及一种改进的消息传递算法，用于更好地表示物理网络中的依赖关系。</li>
<li>results: 我们的实验结果表明，提议的解决方案能够学习和泛化到未看过的捕捉网络场景。<details>
<summary>Abstract</summary>
Currently the state of the art network models are based or depend on Discrete Event Simulation (DES). While DES is highly accurate, it is also computationally costly and cumbersome to parallelize, making it unpractical to simulate high performance networks. Additionally, simulated scenarios fail to capture all of the complexities present in real network scenarios. While there exists network models based on Machine Learning (ML) techniques to minimize these issues, these models are also trained with simulated data and hence vulnerable to the same pitfalls. Consequently, the Graph Neural Networking Challenge 2023 introduces a dataset of captured traffic traces that can be used to build a ML-based network model without these limitations. In this paper we propose a Graph Neural Network (GNN)-based solution specifically designed to better capture the complexities of real network scenarios. This is done through a novel encoding method to capture information from the sequence of captured packets, and an improved message passing algorithm to better represent the dependencies present in physical networks. We show that the proposed solution it is able to learn and generalize to unseen captured network scenarios.
</details>
<details>
<summary>摘要</summary>
现在的状态艺术网络模型都基于不可countdown事件模拟（DES）。虽然DES具有高度准确的优点，但也有计算成本高和并行化困难，使得模拟高性能网络不实际。此外，模拟场景不能捕捉实际网络场景中的所有复杂性。而现有的网络模型基于机器学习（ML）技术来减少这些问题，但这些模型又是通过模拟数据进行训练，因此也受到相同的局限性。因此，2023年的图解网络挑战（GNN）引入了一个包含流量轨迹的数据集，可以用于构建一个基于机器学习（ML）的网络模型，不受上述局限性的影响。在本文中，我们提出了一种基于图解网络（GNN）的解决方案，通过一种新的编码方法来捕捉从流量序列中获得的信息，以及一种改进的消息传递算法来更好地表示物理网络中的依赖关系。我们示出了我们的解决方案能够学习和掌握未看过的捕捉网络场景。
</details></li>
</ul>
<hr>
<h2 id="Online-Convex-Optimization-with-Switching-Cost-and-Delayed-Gradients"><a href="#Online-Convex-Optimization-with-Switching-Cost-and-Delayed-Gradients" class="headerlink" title="Online Convex Optimization with Switching Cost and Delayed Gradients"></a>Online Convex Optimization with Switching Cost and Delayed Gradients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11880">http://arxiv.org/abs/2310.11880</a></li>
<li>repo_url: None</li>
<li>paper_authors: Spandan Senapati, Rahul Vaze</li>
<li>for: 这个论文研究了在有限信息设定下的在线半正定优化问题，特别是使用quadratic和linear switching cost。</li>
<li>methods: 该论文提出了一种名为online multiple gradient descent（OMGD）算法，用于解决这个问题。</li>
<li>results: 论文显示了OMGD算法的竞争比例upper bound为$4(L + 5) + \frac{16(L + 5)}{\mu}$，并且证明了这个Upper bound是order-wise tight。此外，论文还证明了任何在线算法的竞争比例至少为$\max{\Omega(L), \Omega(\frac{L}{\sqrt{\mu})}$。<details>
<summary>Abstract</summary>
We consider the online convex optimization (OCO) problem with quadratic and linear switching cost in the limited information setting, where an online algorithm can choose its action using only gradient information about the previous objective function. For $L$-smooth and $\mu$-strongly convex objective functions, we propose an online multiple gradient descent (OMGD) algorithm and show that its competitive ratio for the OCO problem with quadratic switching cost is at most $4(L + 5) + \frac{16(L + 5)}{\mu}$. The competitive ratio upper bound for OMGD is also shown to be order-wise tight in terms of $L,\mu$. In addition, we show that the competitive ratio of any online algorithm is $\max\{\Omega(L), \Omega(\frac{L}{\sqrt{\mu})\}$ in the limited information setting when the switching cost is quadratic. We also show that the OMGD algorithm achieves the optimal (order-wise) dynamic regret in the limited information setting. For the linear switching cost, the competitive ratio upper bound of the OMGD algorithm is shown to depend on both the path length and the squared path length of the problem instance, in addition to $L, \mu$, and is shown to be order-wise, the best competitive ratio any online algorithm can achieve. Consequently, we conclude that the optimal competitive ratio for the quadratic and linear switching costs are fundamentally different in the limited information setting.
</details>
<details>
<summary>摘要</summary>
我们考虑在有限信息设定下的线上凸优化（OCO）问题，其中一个线上算法可以根据过去的目标函数GradientInformation选择行动。对于$L$-smooth和$\mu$-强制凸目标函数，我们提出了一个线上多重梯度降低（OMGD）算法，并证明其在具有quadratic switching cost的OCO问题中的竞争比率不大于$4(L + 5) + \frac{16(L + 5)}{\mu}$。此外，我们还证明了OMGD算法的竞争比率Upper bound是order-wise tight in terms of $L,\mu$。另外，我们还证明了在有限信息设定下，任何线上算法的竞争比率都是$\max\{\Omega(L), \Omega(\frac{L}{\sqrt{\mu})\}$。此外，我们还证明了OMGD算法在有限信息设定下具有最佳（order-wise）动态遗憾。在linear switching cost的情况下，我们证明了OMGD算法的竞争比率Upper bound取决于问题实体的路径长度和平方路径长度，而且随着$L, \mu$的变化而变化。此外，我们还证明了OMGD算法在linear switching cost的情况下具有order-wise最佳的竞争比率。因此，我们结论到了quadratic和linear switching cost在有限信息设定下的竞争比率是基本不同的。
</details></li>
</ul>
<hr>
<h2 id="SQ-Lower-Bounds-for-Learning-Mixtures-of-Linear-Classifiers"><a href="#SQ-Lower-Bounds-for-Learning-Mixtures-of-Linear-Classifiers" class="headerlink" title="SQ Lower Bounds for Learning Mixtures of Linear Classifiers"></a>SQ Lower Bounds for Learning Mixtures of Linear Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11876">http://arxiv.org/abs/2310.11876</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilias Diakonikolas, Daniel M. Kane, Yuxin Sun</li>
<li>For: 学习混合线性分类器下 Gaussian covariates 问题。* Methods: 使用 Statistical Query (SQ) 算法来解决问题，并提供了一个新的圆形设计技术。* Results: 得到了一个 Statistical Query (SQ) 下界，表明现有算法的复杂性为 $n^{\mathrm{poly}(1&#x2F;\Delta) \log(r)} $，其中 $\Delta$ 是 $\mathbf{v}_\ell$ 对应的下界Pairwise $\ell_2$-separation。<details>
<summary>Abstract</summary>
We study the problem of learning mixtures of linear classifiers under Gaussian covariates. Given sample access to a mixture of $r$ distributions on $\mathbb{R}^n$ of the form $(\mathbf{x},y_{\ell})$, $\ell\in [r]$, where $\mathbf{x}\sim\mathcal{N}(0,\mathbf{I}_n)$ and $y_\ell=\mathrm{sign}(\langle\mathbf{v}_\ell,\mathbf{x}\rangle)$ for an unknown unit vector $\mathbf{v}_\ell$, the goal is to learn the underlying distribution in total variation distance. Our main result is a Statistical Query (SQ) lower bound suggesting that known algorithms for this problem are essentially best possible, even for the special case of uniform mixtures. In particular, we show that the complexity of any SQ algorithm for the problem is $n^{\mathrm{poly}(1/\Delta) \log(r)}$, where $\Delta$ is a lower bound on the pairwise $\ell_2$-separation between the $\mathbf{v}_\ell$'s. The key technical ingredient underlying our result is a new construction of spherical designs that may be of independent interest.
</details>
<details>
<summary>摘要</summary>
我们研究混合线性分类器学习问题，假设我们有一个混合的$r$个分布在 $\mathbb{R}^n$ 上，每个分布的形式是 $({\mathbf{x},y_{\ell})$, $\ell\in [r] $，其中 $\mathbf{x}\sim\mathcal{N}(0,\mathbf{I}_n) $ 是一个标准均值为零的均值为 $\mathbf{I}_n $ 的高维Normal分布，$y_{\ell} = \text{sign}(\langle \mathbf{v}_{\ell}, \mathbf{x} \rangle)$ 是一个未知的单位向量 $\mathbf{v}_{\ell} $ 的某种标记。我们的目标是通过总变化距离来学习这个下面的分布。我们的主要结果是一个统计查询（SQ）下界，表明现有的算法是可能最佳的，即使特殊情况下是均匀混合。具体来说，我们证明任何 SQ 算法的复杂度为 $n^{\mathrm{poly}(1/\Delta) \log(r)}$, 其中 $\Delta$ 是 $\mathbf{v}_{\ell}$ 之间的对角线 $\ell_2$  separation 的下界。我们的技术核心是一种新的圆柱体设计，可能具有独立的利用价值。
</details></li>
</ul>
<hr>
<h2 id="Stochastic-Optimization-for-Non-convex-Problem-with-Inexact-Hessian-Matrix-Gradient-and-Function"><a href="#Stochastic-Optimization-for-Non-convex-Problem-with-Inexact-Hessian-Matrix-Gradient-and-Function" class="headerlink" title="Stochastic Optimization for Non-convex Problem with Inexact Hessian Matrix, Gradient, and Function"></a>Stochastic Optimization for Non-convex Problem with Inexact Hessian Matrix, Gradient, and Function</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11866">http://arxiv.org/abs/2310.11866</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liu Liu, Xuanqing Liu, Cho-Jui Hsieh, Dacheng Tao</li>
<li>for: 这个论文的目的是提出一种基于不确定计算的信任区间（TR）和适应正则化（ARC）方法，以便在非对称优化问题中提高优化效率。</li>
<li>methods: 这种方法使用了不确定计算来计算函数值、梯度和希格曼矩阵，从而选择下一个搜索方向和调整参数。</li>
<li>results: 该论文证明了这种方法可以同时提供不确定计算的函数值、梯度和希格曼矩阵，并且可以在非对称优化问题中实现$\epsilon$-近似二阶优 оптимальность。此外，这种方法的迭代复杂度与前一个研究中的精确计算相同。<details>
<summary>Abstract</summary>
Trust-region (TR) and adaptive regularization using cubics (ARC) have proven to have some very appealing theoretical properties for non-convex optimization by concurrently computing function value, gradient, and Hessian matrix to obtain the next search direction and the adjusted parameters. Although stochastic approximations help largely reduce the computational cost, it is challenging to theoretically guarantee the convergence rate. In this paper, we explore a family of stochastic TR and ARC methods that can simultaneously provide inexact computations of the Hessian matrix, gradient, and function values. Our algorithms require much fewer propagations overhead per iteration than TR and ARC. We prove that the iteration complexity to achieve $\epsilon$-approximate second-order optimality is of the same order as the exact computations demonstrated in previous studies. Additionally, the mild conditions on inexactness can be met by leveraging a random sampling technology in the finite-sum minimization problem. Numerical experiments with a non-convex problem support these findings and demonstrate that, with the same or a similar number of iterations, our algorithms require less computational overhead per iteration than current second-order methods.
</details>
<details>
<summary>摘要</summary>
信任区域（TR）和适应正则化使用立方体（ARC）在非对称优化中有非常吸引人的理论性质。它们同时计算函数值、梯度和偏导数矩阵，以获取下一步搜索方向和调整参数。 although stochastic approximations can significantly reduce computational cost, it is challenging to theoretically guarantee the convergence rate.在这篇论文中，我们探讨了一家Stochastic TR和ARC方法，可同时提供不准确的函数值、梯度和偏导数矩阵计算。我们的算法需要每次迭代 fewer propagations overhead than TR和ARC。我们证明，以 Achieve $\epsilon$-近似第二阶优化的迭代复杂度与前一个研究中的精确计算相同顺序。此外，我们的方法可以通过利用随机抽样技术在finite-sum minimization问题中实现轻度的不准确性条件。numerical experiments with a non-convex problem support these findings and demonstrate that, with the same or a similar number of iterations, our algorithms require less computational overhead per iteration than current second-order methods.
</details></li>
</ul>
<hr>
<h2 id="Effective-and-Efficient-Federated-Tree-Learning-on-Hybrid-Data"><a href="#Effective-and-Efficient-Federated-Tree-Learning-on-Hybrid-Data" class="headerlink" title="Effective and Efficient Federated Tree Learning on Hybrid Data"></a>Effective and Efficient Federated Tree Learning on Hybrid Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11865">http://arxiv.org/abs/2310.11865</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinbin Li, Chulin Xie, Xiaojun Xu, Xiaoyuan Liu, Ce Zhang, Bo Li, Bingsheng He, Dawn Song</li>
<li>for: 该论文旨在 Addressing the challenges of federated learning in hybrid data settings, where data from different parties may differ in both features and samples.</li>
<li>methods: 该论文提出了 HybridTree，一种基于分布式学习的新方法，可以在混合数据设置下进行树学习。通过分析了树中具有一致的拆分规则，该方法可以在不需要频繁的通信协议的情况下训练树。</li>
<li>results: 实验表明，HybridTree 可以与中央集成集成环境相比，达到相同的准确率，而且可以减少计算和通信协议的开销，最高可以达到 8 倍的速度提升。<details>
<summary>Abstract</summary>
Federated learning has emerged as a promising distributed learning paradigm that facilitates collaborative learning among multiple parties without transferring raw data. However, most existing federated learning studies focus on either horizontal or vertical data settings, where the data of different parties are assumed to be from the same feature or sample space. In practice, a common scenario is the hybrid data setting, where data from different parties may differ both in the features and samples. To address this, we propose HybridTree, a novel federated learning approach that enables federated tree learning on hybrid data. We observe the existence of consistent split rules in trees. With the help of these split rules, we theoretically show that the knowledge of parties can be incorporated into the lower layers of a tree. Based on our theoretical analysis, we propose a layer-level solution that does not need frequent communication traffic to train a tree. Our experiments demonstrate that HybridTree can achieve comparable accuracy to the centralized setting with low computational and communication overhead. HybridTree can achieve up to 8 times speedup compared with the other baselines.
</details>
<details>
<summary>摘要</summary>
《联合学习》已经成为一种有前途的分布式学习 paradigma，它使得多个党 collaboration 学习，无需传输原始数据。然而，现有大多数联合学习研究都集中在水平或垂直数据设置中，即不同党的数据假设来自同一个特征或样本空间。在实际应用中，常见的情景是混合数据设置，其中党的数据可能具有不同的特征和样本。为 Addressing 此问题，我们提出 HybridTree，一种新的联合学习方法，可以在混合数据上进行联合树学习。我们发现了共同拆分规则在树中的存在，这些规则帮助我们 theoretically 表明党的知识可以在树的下层级中被包含。基于我们的理论分析，我们提出一种层级解决方案，不需要频繁的通信协议来训练树。我们的实验表明，HybridTree 可以与中央集成设置具有相同的准确率，同时具有较低的计算和通信协议负担。HybridTree 可以与其他基准值进行比较，达到 8 倍的速度提升。
</details></li>
</ul>
<hr>
<h2 id="Accelerate-Presolve-in-Large-Scale-Linear-Programming-via-Reinforcement-Learning"><a href="#Accelerate-Presolve-in-Large-Scale-Linear-Programming-via-Reinforcement-Learning" class="headerlink" title="Accelerate Presolve in Large-Scale Linear Programming via Reinforcement Learning"></a>Accelerate Presolve in Large-Scale Linear Programming via Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11845">http://arxiv.org/abs/2310.11845</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yufei Kuang, Xijun Li, Jie Wang, Fangzhou Zhu, Meng Lu, Zhihai Wang, Jia Zeng, Houqiang Li, Yongdong Zhang, Feng Wu</li>
<li>for: 这篇论文的目的是提出一种基于机器学习的LP解决方法，以提高现代LP解决器的效率和可靠性。</li>
<li>methods: 该方法使用了动态行为序列学习（RL）框架，将LP解决器的routine设计任务形式化为一个Markov决策过程，并通过适应行动序列来生成高质量的解决方案。</li>
<li>results: 实验结果表明，RL4Presolve可以有效地提高大规模LP的解决效率，特别是在来自业界的benchmark中。此外，通过提取学习政策中的规则，可以将RL4Presolve简单地部署到华为的供应链中。这些结果表明，将机器学习技术应用于现代LP解决器可以实现可负担的经济和学术潜力。<details>
<summary>Abstract</summary>
Large-scale LP problems from industry usually contain much redundancy that severely hurts the efficiency and reliability of solving LPs, making presolve (i.e., the problem simplification module) one of the most critical components in modern LP solvers. However, how to design high-quality presolve routines -- that is, the program determining (P1) which presolvers to select, (P2) in what order to execute, and (P3) when to stop -- remains a highly challenging task due to the extensive requirements on expert knowledge and the large search space. Due to the sequential decision property of the task and the lack of expert demonstrations, we propose a simple and efficient reinforcement learning (RL) framework -- namely, reinforcement learning for presolve (RL4Presolve) -- to tackle (P1)-(P3) simultaneously. Specifically, we formulate the routine design task as a Markov decision process and propose an RL framework with adaptive action sequences to generate high-quality presolve routines efficiently. Note that adaptive action sequences help learn complex behaviors efficiently and adapt to various benchmarks. Experiments on two solvers (open-source and commercial) and eight benchmarks (real-world and synthetic) demonstrate that RL4Presolve significantly and consistently improves the efficiency of solving large-scale LPs, especially on benchmarks from industry. Furthermore, we optimize the hard-coded presolve routines in LP solvers by extracting rules from learned policies for simple and efficient deployment to Huawei's supply chain. The results show encouraging economic and academic potential for incorporating machine learning to modern solvers.
</details>
<details>
<summary>摘要</summary>
大规模LP问题从行业 обычно含有很多重复性，这会严重地降低解决LP的效率和可靠性，因此宏观问题简化模块（i.e., 问题简化模块）成为现代LP解决器中最 kritical 的一部分。然而，如何设计高质量的宏观问题简化程序 --- 即确定（P1）哪些简化器选择，（P2）在哪个顺序执行，以及（P3）何时停止 --- 仍然是一项非常困难的任务，这主要归结于宏观问题简化程序的广泛需求和搜索空间的庞大。由于任务具有顺序决策性和缺乏专家示范，我们提出了一种简单和高效的机器学习（RL）框架 --- 即RL4Presolve --- 以同时解决（P1）-（P3）。具体来说，我们将问题简化任务视为一个Markov决策过程，并提出了一种RL框架，其中包含可适应行为序列来生成高质量的宏观问题简化程序。注意，可适应行为序列可以高效地学习复杂的行为并适应不同的标准。在两种解决器（开源和商业）和八个标准（实际世界和 sintetic）上进行了实验，RL4Presolve显示可以有效地提高大规模LP的解决效率，特别是在行业标准上。此外，我们还使用RL学习到来自学习的策略中的规则，以便简单和高效地在Huawei的供应链中部署。结果表明，通过把机器学习技术应用到现代解决器中，可以获得有优 экономиче和学术潜力。
</details></li>
</ul>
<hr>
<h2 id="On-The-Expressivity-of-Objective-Specification-Formalisms-in-Reinforcement-Learning"><a href="#On-The-Expressivity-of-Objective-Specification-Formalisms-in-Reinforcement-Learning" class="headerlink" title="On The Expressivity of Objective-Specification Formalisms in Reinforcement Learning"></a>On The Expressivity of Objective-Specification Formalisms in Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11840">http://arxiv.org/abs/2310.11840</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rohan Subramani, Marcus Williams, Max Heitmann, Halfdan Holm, Charlie Griffin, Joar Skalse</li>
<li>for: 这个论文主要针对的是 reinforcement learning（RL）任务中的目标形式化问题。</li>
<li>methods: 这篇论文使用了多种目标规定 formalism，包括Linear Temporal Logic和Multi-Objective Reinforcement Learning，并进行了这些 formalism 之间的比较。</li>
<li>results: 论文发现了不同的目标规定 formalism 之间存在一定的限制，并且没有任何一种 formalism 同时具有优化和表达能力。例如，论文证明了 Regularised RL、Outer Nonlinear Markov Rewards、Reward Machines、Linear Temporal Logic 和 Limit Average Rewards 等 formalism 可以表达其他 formalism 无法表达的目标。这些结论有关于RL中目标规定 formalism的选择和实践中的表达限制。<details>
<summary>Abstract</summary>
To solve a task with reinforcement learning (RL), it is necessary to formally specify the goal of that task. Although most RL algorithms require that the goal is formalised as a Markovian reward function, alternatives have been developed (such as Linear Temporal Logic and Multi-Objective Reinforcement Learning). Moreover, it is well known that some of these formalisms are able to express certain tasks that other formalisms cannot express. However, there has not yet been any thorough analysis of how these formalisms relate to each other in terms of expressivity. In this work, we fill this gap in the existing literature by providing a comprehensive comparison of the expressivities of 17 objective-specification formalisms in RL. We place these formalisms in a preorder based on their expressive power, and present this preorder as a Hasse diagram. We find a variety of limitations for the different formalisms, and that no formalism is both dominantly expressive and straightforward to optimise with current techniques. For example, we prove that each of Regularised RL, Outer Nonlinear Markov Rewards, Reward Machines, Linear Temporal Logic, and Limit Average Rewards can express an objective that the others cannot. Our findings have implications for both policy optimisation and reward learning. Firstly, we identify expressivity limitations which are important to consider when specifying objectives in practice. Secondly, our results highlight the need for future research which adapts reward learning to work with a variety of formalisms, since many existing reward learning methods implicitly assume that desired objectives can be expressed with Markovian rewards. Our work contributes towards a more cohesive understanding of the costs and benefits of different RL objective-specification formalisms.
</details>
<details>
<summary>摘要</summary>
要解决一个任务使用强化学习（RL），需要正式 specify 该任务的目标。大多数 RL 算法需要将目标 formalized 为 Markov 奖励函数，但是有其他形式（如线性时间逻辑和多目标强化学习）也有被开发出来。然而，到目前为止，没有任何 thorougly 分析这些形式之间的关系。在这种情况下，我们填充了现有文献中的这种 gap  by 提供了17种目标规定 formalism 在RL中的比较。我们将这些 formalism 按照其表达力排序，并将其显示为一个 Hasse  диаграм。我们发现了不同 formalism 的一些限制，并证明了每种 formalism 都有一些可以表达的任务，而其他 formalism 不能表达。例如，我们证明了 Regularized RL、Outer Nonlinear Markov Rewards、Reward Machines、线性时间逻辑和 Limit Average Rewards 可以表达出其他 formalism 不能表达的任务。我们的发现对于policy优化和奖励学习都有重要的意义。首先，我们identified 表达力的限制，这些限制在实践中需要考虑。其次，我们的结果表明需要将奖励学习适应到不同 formalism 中，因为现有的奖励学习方法通常假设desired objective 可以用 Markov 奖励函数表达。我们的工作对RL objective-specification formalism 的costs and benefits 提供了更加一致的理解。
</details></li>
</ul>
<hr>
<h2 id="Equivariant-Bootstrapping-for-Uncertainty-Quantification-in-Imaging-Inverse-Problems"><a href="#Equivariant-Bootstrapping-for-Uncertainty-Quantification-in-Imaging-Inverse-Problems" class="headerlink" title="Equivariant Bootstrapping for Uncertainty Quantification in Imaging Inverse Problems"></a>Equivariant Bootstrapping for Uncertainty Quantification in Imaging Inverse Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11838">http://arxiv.org/abs/2310.11838</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tachella/equivariant_bootstrap">https://github.com/tachella/equivariant_bootstrap</a></li>
<li>paper_authors: Julian Tachella, Marcelo Pereyra</li>
<li>for:  This paper aims to accurately quantify the uncertainty in solutions to severely ill-posed scientific imaging problems, which is critical for interpreting experimental results and using reconstructed images as scientific evidence.</li>
<li>methods: The proposed uncertainty quantification methodology is based on an equivariant formulation of the parametric bootstrap algorithm, which leverages symmetries and invariance properties commonly encountered in imaging problems. The method is general and can be applied with any image reconstruction technique, including unsupervised training strategies.</li>
<li>results: The proposed method delivers remarkably accurate high-dimensional confidence regions and outperforms alternative uncertainty quantification strategies in terms of estimation accuracy, uncertainty quantification accuracy, and computing time. The method is demonstrated through a series of numerical experiments.<details>
<summary>Abstract</summary>
Scientific imaging problems are often severely ill-posed, and hence have significant intrinsic uncertainty. Accurately quantifying the uncertainty in the solutions to such problems is therefore critical for the rigorous interpretation of experimental results as well as for reliably using the reconstructed images as scientific evidence. Unfortunately, existing imaging methods are unable to quantify the uncertainty in the reconstructed images in a manner that is robust to experiment replications. This paper presents a new uncertainty quantification methodology based on an equivariant formulation of the parametric bootstrap algorithm that leverages symmetries and invariance properties commonly encountered in imaging problems. Additionally, the proposed methodology is general and can be easily applied with any image reconstruction technique, including unsupervised training strategies that can be trained from observed data alone, thus enabling uncertainty quantification in situations where there is no ground truth data available. We demonstrate the proposed approach with a series of numerical experiments and through comparisons with alternative uncertainty quantification strategies from the state-of-the-art, such as Bayesian strategies involving score-based diffusion models and Langevin samplers. In all our experiments, the proposed method delivers remarkably accurate high-dimensional confidence regions and outperforms the competing approaches in terms of estimation accuracy, uncertainty quantification accuracy, and computing time.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Optimising-Distributions-with-Natural-Gradient-Surrogates"><a href="#Optimising-Distributions-with-Natural-Gradient-Surrogates" class="headerlink" title="Optimising Distributions with Natural Gradient Surrogates"></a>Optimising Distributions with Natural Gradient Surrogates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11837">http://arxiv.org/abs/2310.11837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan So, Richard E. Turner</li>
<li>for: 优化probability distribution的参数</li>
<li>methods: 使用自然偏导法优化参数</li>
<li>results: 扩展了可以使用自然偏导法优化的 distribuition 类型，以及fast、易于理解、简单实现和不需要详细模型Derivation。<details>
<summary>Abstract</summary>
Natural gradient methods have been used to optimise the parameters of probability distributions in a variety of settings, often resulting in fast-converging procedures. Unfortunately, for many distributions of interest, computing the natural gradient has a number of challenges. In this work we propose a novel technique for tackling such issues, which involves reframing the optimisation as one with respect to the parameters of a surrogate distribution, for which computing the natural gradient is easy. We give several examples of existing methods that can be interpreted as applying this technique, and propose a new method for applying it to a wide variety of problems. Our method expands the set of distributions that can be efficiently targeted with natural gradients. Furthermore, it is fast, easy to understand, simple to implement using standard autodiff software, and does not require lengthy model-specific derivations. We demonstrate our method on maximum likelihood estimation and variational inference tasks.
</details>
<details>
<summary>摘要</summary>
自然均方法已经广泛应用于估计概率分布参数，经常导致快速收敛的过程。然而，许多感兴趣的分布中，计算自然均方的问题充满挑战。在这种情况下，我们提出了一种新的技巧，即将估计变换为一种对准ocker分布参数的估计问题，其中计算自然均方是容易的。我们给出了一些现有的方法，可以看作是应用这种技巧，并提出了一种新的方法，可以应用于各种问题。我们的方法可以扩展到更多的分布，并且快速、易于理解、使用标准自动极化软件实现，不需要详细的模型特定的 derivations。我们在最大 LIKELIHOOD估计和variational推断任务中进行了示例。
</details></li>
</ul>
<hr>
<h2 id="CLARA-Multilingual-Contrastive-Learning-for-Audio-Representation-Acquisition"><a href="#CLARA-Multilingual-Contrastive-Learning-for-Audio-Representation-Acquisition" class="headerlink" title="CLARA: Multilingual Contrastive Learning for Audio Representation Acquisition"></a>CLARA: Multilingual Contrastive Learning for Audio Representation Acquisition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11830">http://arxiv.org/abs/2310.11830</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/knoriy/CLARA">https://github.com/knoriy/CLARA</a></li>
<li>paper_authors: Kari A Noriy, Xiaosong Yang, Marcin Budka, Jian Jun Zhang</li>
<li>for: 本研究提出了一种多语言语音和声音表示学习框架，用于解决语音处理研究中数据的问题。</li>
<li>methods: 该框架使用了对比学习技术，通过自动生成的卷积数据来增加数据量，并让模型从无标签数据上学习共享表示。</li>
<li>results: 实验结果表明，该模型在识别情绪、音频分类和检索 bencmarks 中表现出色，具有适用于多语言和各种各样的声音条件的共享表示能力，同时还能够编码潜在的情感维度。<details>
<summary>Abstract</summary>
This paper proposes a novel framework for multilingual speech and sound representation learning using contrastive learning. The lack of sizeable labelled datasets hinders speech-processing research across languages. Recent advances in contrastive learning provide self-supervised techniques to learn from unlabelled data. Motivated by reducing data dependence and improving generalisation across diverse languages and conditions, we develop a multilingual contrastive framework. This framework enables models to acquire shared representations across languages, facilitating cross-lingual transfer with limited target language data.   Additionally, capturing emotional cues within speech is challenging due to subjective perceptual assessments. By learning expressive representations from diverse, multilingual data in a self-supervised manner, our approach aims to develop speech representations that encode emotive dimensions.   Our method trains encoders on a large corpus of multi-lingual audio data. Data augmentation techniques are employed to expand the dataset. The contrastive learning approach trains the model to maximise agreement between positive pairs and minimise agreement between negative pairs. Extensive experiments demonstrate state-of-the-art performance of the proposed model on emotion recognition, audio classification, and retrieval benchmarks under zero-shot and few-shot conditions. This provides an effective approach for acquiring shared and generalised speech representations across languages and acoustic conditions while encoding latent emotional dimensions.
</details>
<details>
<summary>摘要</summary>
We train encoders on a large corpus of multi-lingual audio data, and employ data augmentation techniques to expand the dataset. The contrastive learning approach trains the model to maximize agreement between positive pairs and minimize agreement between negative pairs. Extensive experiments demonstrate state-of-the-art performance of the proposed model on emotion recognition, audio classification, and retrieval benchmarks under zero-shot and few-shot conditions. This provides an effective approach for acquiring shared and generalised speech representations across languages and acoustic conditions while encoding latent emotional dimensions.Here's the Simplified Chinese translation:这篇论文提出了一种新的多语言语音和声音表示学习框架，使用对比学习。由于语言上的大量标注数据缺乏，这阻碍了跨语言语音处理研究的进步。但是，最近的对比学习技术提供了一种无监督的学习方法，可以从无标注数据中学习。我们的方法旨在通过学习多语言数据，以便在不同语言和条件下实现交互转移，并且编码潜在的情感维度。我们将编码器训练在一个大量多语言音频数据集上，并使用数据扩展技术来扩大数据集。对比学习方法将模型训练以最大化正方向对的匹配，并最小化负方向对的匹配。广泛的实验表明，提议的模型在情感识别、音频分类和检索benchmark上实现了顶尖性能，包括零shot和几shot情况下。这提供了一种有效的方法，可以在不同语言和音频条件下获得共享和普适的语音表示，同时编码潜在的情感维度。
</details></li>
</ul>
<hr>
<h2 id="Towards-Graph-Foundation-Models-A-Survey-and-Beyond"><a href="#Towards-Graph-Foundation-Models-A-Survey-and-Beyond" class="headerlink" title="Towards Graph Foundation Models: A Survey and Beyond"></a>Towards Graph Foundation Models: A Survey and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11829">http://arxiv.org/abs/2310.11829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiawei Liu, Cheng Yang, Zhiyuan Lu, Junze Chen, Yibo Li, Mengmei Zhang, Ting Bai, Yuan Fang, Lichao Sun, Philip S. Yu, Chuan Shi</li>
<li>for: 这篇论文旨在探讨基于图像学的基本模型，以及其在不同的人工智能应用中的潜在应用。</li>
<li>methods: 本论文提出了基于图像学的基本模型（GFM）的概念，并对其特点和技术进行了系统的描述。此外，文章还分类了现有的工作，根据它们的依赖于图像神经网络和大语言模型的程度。</li>
<li>results: 文章提供了当前基于图像学的基本模型领域的全面的概述，以及这个领域的未来研究方向。<details>
<summary>Abstract</summary>
Emerging as fundamental building blocks for diverse artificial intelligence applications, foundation models have achieved notable success across natural language processing and many other domains. Parallelly, graph machine learning has witnessed a transformative shift, with shallow methods giving way to deep learning approaches. The emergence and homogenization capabilities of foundation models have piqued the interest of graph machine learning researchers, sparking discussions about developing the next graph learning paradigm that is pre-trained on broad graph data and can be adapted to a wide range of downstream graph tasks. However, there is currently no clear definition and systematic analysis for this type of work. In this article, we propose the concept of graph foundation models (GFMs), and provide the first comprehensive elucidation on their key characteristics and technologies. Following that, we categorize existing works towards GFMs into three categories based on their reliance on graph neural networks and large language models. Beyond providing a comprehensive overview of the current landscape of graph foundation models, this article also discusses potential research directions for this evolving field.
</details>
<details>
<summary>摘要</summary>
emerging as fundamental building blocks for diverse artificial intelligence applications, foundation models have achieved notable success across natural language processing and many other domains. 同时, graph machine learning has witnessed a transformative shift, with shallow methods giving way to deep learning approaches. the emergence and homogenization capabilities of foundation models have piqued the interest of graph machine learning researchers, sparking discussions about developing the next graph learning paradigm that is pre-trained on broad graph data and can be adapted to a wide range of downstream graph tasks. however, there is currently no clear definition and systematic analysis for this type of work. in this article, we propose the concept of graph foundation models (gfms), and provide the first comprehensive elucidation on their key characteristics and technologies. following that, we categorize existing works towards gfms into three categories based on their reliance on graph neural networks and large language models. beyond providing a comprehensive overview of the current landscape of graph foundation models, this article also discusses potential research directions for this evolving field.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="A-Historical-Context-for-Data-Streams"><a href="#A-Historical-Context-for-Data-Streams" class="headerlink" title="A Historical Context for Data Streams"></a>A Historical Context for Data Streams</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19811">http://arxiv.org/abs/2310.19811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Indre Zliobaite, Jesse Read</li>
<li>for: 这篇论文主要针对的是从数据流中学习的机器学习问题，这是一个活跃和快速发展的研究领域。</li>
<li>methods: 这篇论文使用了一些传统的机器学习算法，但是它们受到了数据流的计算资源限制，例如每个实例只能被检查一次，并且需要在任何时间提供预测结果。</li>
<li>results: 这篇论文提出了一些历史上对数据流机器学习的假设，并将这些假设放在历史上的学术背景中进行了回顾。<details>
<summary>Abstract</summary>
Machine learning from data streams is an active and growing research area. Research on learning from streaming data typically makes strict assumptions linked to computational resource constraints, including requirements for stream mining algorithms to inspect each instance not more than once and be ready to give a prediction at any time. Here we review the historical context of data streams research placing the common assumptions used in machine learning over data streams in their historical context.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将数据流中的机器学习作为活跃和快速发展的研究领域。研究从数据流中学习通常做出严格的计算资源限制，包括流程挖掘算法不能再次检查每个实例，并且要准备任何时间给出预测。我们在这里将数据流研究的历史背景和常见的机器学习假设置在历史上的位置。Translation:机器学习从数据流中是一个活跃和快速发展的研究领域。研究从数据流中学习通常做出严格的计算资源限制，包括流程挖掘算法不能再次检查每个实例，并且要准备任何时间给出预测。我们在这里将数据流研究的历史背景和常见的机器学习假设置在历史上的位置。
</details></li>
</ul>
<hr>
<h2 id="De-novo-protein-design-using-geometric-vector-field-networks"><a href="#De-novo-protein-design-using-geometric-vector-field-networks" class="headerlink" title="De novo protein design using geometric vector field networks"></a>De novo protein design using geometric vector field networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11802">http://arxiv.org/abs/2310.11802</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weian Mao, Muzhi Zhu, Zheng Sun, Shuaike Shen, Lin Yuanbo Wu, Hao Chen, Chunhua Shen</li>
<li>for: 这篇论文主要关注的是对蛋白质构造设计的进步，特别是透过蛋白质diffusion的创新，使得蛋白质设计得以进行更加精确和有效的模型化。</li>
<li>methods: 本论文提出了一种新的数据 Computation Network（VFN），可以在蛋白质diffusion中进行更加精确的框架模型化，并且可以同时模型框架和原子。VFN使用了学习可控的 вектор计算，将蛋白质框架中的各个位置转换为可读的 вектор值，然后使用弹性总和将这些 вектор值与蛋白质框架中的各个原子进行相互关联。</li>
<li>results: 本论文的实验结果显示，VFN在蛋白质diffusion中表现出色，比起先前的IPA模型，VFN在设计性（67.04% vs. 53.58%)和多样性（66.54% vs. 51.98%)等方面均有较好的表现。此外，VFN也在倒拾蛋白质（frame和原子模型）中表现出色，比起先前的PiFold模型（54.7% vs. 51.66%），VFN在序列恢复率上有较好的表现。此外，本论文还提出了一种将VFN与ESM模型结合的方法，这种方法在先前的ESM-based SoTA（62.67% vs. 55.65%）上有着较好的表现。<details>
<summary>Abstract</summary>
Innovations like protein diffusion have enabled significant progress in de novo protein design, which is a vital topic in life science. These methods typically depend on protein structure encoders to model residue backbone frames, where atoms do not exist. Most prior encoders rely on atom-wise features, such as angles and distances between atoms, which are not available in this context. Thus far, only several simple encoders, such as IPA, have been proposed for this scenario, exposing the frame modeling as a bottleneck. In this work, we proffer the Vector Field Network (VFN), which enables network layers to perform learnable vector computations between coordinates of frame-anchored virtual atoms, thus achieving a higher capability for modeling frames. The vector computation operates in a manner similar to a linear layer, with each input channel receiving 3D virtual atom coordinates instead of scalar values. The multiple feature vectors output by the vector computation are then used to update the residue representations and virtual atom coordinates via attention aggregation. Remarkably, VFN also excels in modeling both frames and atoms, as the real atoms can be treated as the virtual atoms for modeling, positioning VFN as a potential universal encoder. In protein diffusion (frame modeling), VFN exhibits an impressive performance advantage over IPA, excelling in terms of both designability (67.04% vs. 53.58%) and diversity (66.54% vs. 51.98%). In inverse folding (frame and atom modeling), VFN outperforms the previous SoTA model, PiFold (54.7% vs. 51.66%), on sequence recovery rate. We also propose a method of equipping VFN with the ESM model, which significantly surpasses the previous ESM-based SoTA (62.67% vs. 55.65%), LM-Design, by a substantial margin.
</details>
<details>
<summary>摘要</summary>
新技术如蛋白diffusion已经使得蛋白结构设计得到了重要的进步，这是生命科学中非常重要的话题。这些方法通常依赖于蛋白结构编码器来模拟蛋白质量框架，其中原子不存在。以前的编码器大多数依赖于原子粒子特征，如原子之间的角度和距离，这些特征在这种情况下不可用。只有一些简单的编码器，如IPA，已经被提出，这暴露了框架模型化为瓶颈。在这种工作中，我们提议使用 Vector Field Network（VFN），它使得网络层可以通过学习vector计算来处理坐标相关的操作。VFN在蛋白diffusion（框架模型）中表现出了非常出色的性能优势，比IPA更高，达到67.04% vs. 53.58%的设计性能和66.54% vs. 51.98%的多样性。在 inverse folding（框架和原子模型）中，VFN也超越了之前的SoTA模型，PiFold（54.7% vs. 51.66%），在序列恢复率方面表现出色。我们还提出了使用VFN和ESM模型的方法，该方法在之前的ESM-based SoTA（62.67% vs. 55.65%）之上显著提高了性能。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Training-for-Physics-Informed-Neural-Networks"><a href="#Adversarial-Training-for-Physics-Informed-Neural-Networks" class="headerlink" title="Adversarial Training for Physics-Informed Neural Networks"></a>Adversarial Training for Physics-Informed Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11789">http://arxiv.org/abs/2310.11789</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yaoli90/at-pinn">https://github.com/yaoli90/at-pinn</a></li>
<li>paper_authors: Yao Li, Shengzhu Shi, Zhichang Guo, Boying Wu</li>
<li>For: 解决复杂的偏微分方程（PDEs）中的缺乏稳定性问题，提高Physics-informed neural networks（PINNs）的测试精度和可靠性。* Methods: 基于投影gradient descent对抗攻击（PGD-based adversarial attack），提出了一种名为AT-PINNs的对抗训练策略，可以增强PINNs的Robustness和稳定性。AT-PINNs可以通过在训练过程中使用对抗样本来准确地识别模型失败位置，并在训练过程中使模型更加注重这些位置。* Results: 应用AT-PINNs于各种复杂的PDEs，包括多尺度约束的圆柱方程、多峰解的波兰射方程、普朗克方程的锐度解和Allen-Cahn方程。结果表明，AT-PINNs可以有效地定位和减少失败区域，并且适用于解决复杂的PDEs，因为对于失败区域的定位无关于失败区域的大小或分布的复杂性。<details>
<summary>Abstract</summary>
Physics-informed neural networks have shown great promise in solving partial differential equations. However, due to insufficient robustness, vanilla PINNs often face challenges when solving complex PDEs, especially those involving multi-scale behaviors or solutions with sharp or oscillatory characteristics. To address these issues, based on the projected gradient descent adversarial attack, we proposed an adversarial training strategy for PINNs termed by AT-PINNs. AT-PINNs enhance the robustness of PINNs by fine-tuning the model with adversarial samples, which can accurately identify model failure locations and drive the model to focus on those regions during training. AT-PINNs can also perform inference with temporal causality by selecting the initial collocation points around temporal initial values. We implement AT-PINNs to the elliptic equation with multi-scale coefficients, Poisson equation with multi-peak solutions, Burgers equation with sharp solutions and the Allen-Cahn equation. The results demonstrate that AT-PINNs can effectively locate and reduce failure regions. Moreover, AT-PINNs are suitable for solving complex PDEs, since locating failure regions through adversarial attacks is independent of the size of failure regions or the complexity of the distribution.
</details>
<details>
<summary>摘要</summary>
物理学 Informed Neural Networks (PINNs) 已经展示了解决partial differential equations (PDEs) 的巨大承诺. 然而，由于不充分的Robustness，vanilla PINNs 经常在解决复杂的PDEs中遇到挑战，特别是包含多尺度行为或解决具有锐利或振荡特征的PDEs. 为了解决这些问题，我们基于Projected gradient descent adversarial attack (PGD-AA)提出了一种名为AT-PINNs的对抗训练策略。AT-PINNs可以增强PINNs的Robustness，通过在训练过程中使用对抗样本，准确地识别模型失败的位置并使模型在训练过程中专注于这些位置。AT-PINNs还可以通过选择时间初值附近的初始坐标来进行时间 causality 的推理。我们将AT-PINNs应用到了各种PDEs，包括具有多尺度系数的圆柱 equation、Poisson equation with multi-peak solutions、Burgers equation with sharp solutions和Allen-Cahn equation。结果表明，AT-PINNs可以有效地定位和减少失败区域。此外，AT-PINNs适用于解决复杂的PDEs，因为通过对抗攻击定位失败区域是独立于失败区域的大小或分布复杂性的。
</details></li>
</ul>
<hr>
<h2 id="NeuroCUT-A-Neural-Approach-for-Robust-Graph-Partitioning"><a href="#NeuroCUT-A-Neural-Approach-for-Robust-Graph-Partitioning" class="headerlink" title="NeuroCUT: A Neural Approach for Robust Graph Partitioning"></a>NeuroCUT: A Neural Approach for Robust Graph Partitioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11787">http://arxiv.org/abs/2310.11787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rishi Shah, Krishnanshu Jain, Sahil Manchanda, Sourav Medya, Sayan Ranu</li>
<li>for: 分区图形式的问题，即将图分成k个独立的部分，以优化特定的分区目标。</li>
<li>methods:  neural approach，包括一个新的框架NeuroCut，它在查询时可以对图的结构和分区数进行泛化，并通过基于节点表示学习的强化学习框架来满足任何优化目标，包括不可导函数。</li>
<li>results: NeuroCut在实验中表现出色，能够找到高质量的分区，具有强大的泛化性和对图结构 modificatiopn的抗颤势性。<details>
<summary>Abstract</summary>
Graph partitioning aims to divide a graph into $k$ disjoint subsets while optimizing a specific partitioning objective. The majority of formulations related to graph partitioning exhibit NP-hardness due to their combinatorial nature. As a result, conventional approximation algorithms rely on heuristic methods, sometimes with approximation guarantees and sometimes without. Unfortunately, traditional approaches are tailored for specific partitioning objectives and do not generalize well across other known partitioning objectives from the literature. To overcome this limitation, and learn heuristics from the data directly, neural approaches have emerged, demonstrating promising outcomes. In this study, we extend this line of work through a novel framework, NeuroCut. NeuroCut introduces two key innovations over prevailing methodologies. First, it is inductive to both graph topology and the partition count, which is provided at query time. Second, by leveraging a reinforcement learning based framework over node representations derived from a graph neural network, NeuroCut can accommodate any optimization objective, even those encompassing non-differentiable functions. Through empirical evaluation, we demonstrate that NeuroCut excels in identifying high-quality partitions, showcases strong generalization across a wide spectrum of partitioning objectives, and exhibits resilience to topological modifications.
</details>
<details>
<summary>摘要</summary>
graf分割的目标是将 Graf 分成 k 个不交叉的子集，同时最大化特定的分割目标。大多数相关的形式化问题都会显示NP困难，因为它们具有各种 combinatorial 特性。因此，现有的approximation算法通常使用了heuristic方法，有时具有approximation保证，有时没有。 unfortunately，传统的方法通常是为特定的分割目标设计的，不能总是泛化到其他从文献中知道的分割目标。为了解决这个限制，并从数据中直接学习heuristics，神经方法出现了。在这项研究中，我们通过一个新的框架，NeuroCut，进一步推动这一线的发展。NeuroCut 具有两个关键创新：首先，它是对 Graf 结构和分割 count  inductive的，可以在查询时提供。其次，通过利用基于节点表示学习的reinforcement learning框架，NeuroCut 可以处理任何优化目标，包括不可导函数。通过实验评估，我们示出NeuroCut 可以提供高质量的分割，具有强大的泛化能力，并且对 Graf 结构的修改 display 强大的抗衡性。
</details></li>
</ul>
<hr>
<h2 id="A-Quasi-Wasserstein-Loss-for-Learning-Graph-Neural-Networks"><a href="#A-Quasi-Wasserstein-Loss-for-Learning-Graph-Neural-Networks" class="headerlink" title="A Quasi-Wasserstein Loss for Learning Graph Neural Networks"></a>A Quasi-Wasserstein Loss for Learning Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11762">http://arxiv.org/abs/2310.11762</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minjie Cheng, Hongteng Xu</li>
<li>for: 提高 Graph Neural Network (GNN) 在节点级预测任务中的性能，因为现有的损失函数通常对每个节点独立进行应用，即使节点嵌入和标签不是独立的。</li>
<li>methods: 提出了一种新的 quasi-Wasserstein (QW) 损失函数，基于图上的最优运输定义，用于修改 GNN 的学习和预测方法。该损失函数定义了图边上的 quasi-Wasserstein 距离，用于优化标签的运输定义。</li>
<li>results: 实验表明，提出的 QW 损失函数可以应用于多种 GNN 模型，并且能够提高其性能在节点级预测和回归任务中。此外，该损失函数还可以提供一种新的拟合学习和预测方法。<details>
<summary>Abstract</summary>
When learning graph neural networks (GNNs) in node-level prediction tasks, most existing loss functions are applied for each node independently, even if node embeddings and their labels are non-i.i.d. because of their graph structures. To eliminate such inconsistency, in this study we propose a novel Quasi-Wasserstein (QW) loss with the help of the optimal transport defined on graphs, leading to new learning and prediction paradigms of GNNs. In particular, we design a "Quasi-Wasserstein" distance between the observed multi-dimensional node labels and their estimations, optimizing the label transport defined on graph edges. The estimations are parameterized by a GNN in which the optimal label transport may determine the graph edge weights optionally. By reformulating the strict constraint of the label transport to a Bregman divergence-based regularizer, we obtain the proposed Quasi-Wasserstein loss associated with two efficient solvers learning the GNN together with optimal label transport. When predicting node labels, our model combines the output of the GNN with the residual component provided by the optimal label transport, leading to a new transductive prediction paradigm. Experiments show that the proposed QW loss applies to various GNNs and helps to improve their performance in node-level classification and regression tasks.
</details>
<details>
<summary>摘要</summary>
当学习图 neural network (GNN) 在节点级预测任务时，大多数现有的损失函数都是对每个节点独立应用的，即使节点表示和其标签不是独立的，因为它们的图结构。为了消除这种不一致，在本研究中我们提出了一种新的 quasi-Wasserstein (QW) 损失函数，基于图上的最优运输定义。在特定情况下，我们定义了 observe 多维节点标签和其估计之间的 "quasi-Wasserstein" 距离，并且优化了图边上的标签运输定义。这些估计是通过一个 GNN 来 parameterize，其中优化的标签运输可能会确定图边权重。通过将 строго的标签运输约束转换为 Bregman 分布定义based REG regularizer，我们获得了我们的提议的 QW 损失函数，并且可以使用两种高效的算法来学习 GNN 和标签运输。在预测节点标签时，我们将 GNN 的输出与标签运输的 residual 组件相加，这导致了一种新的混合预测 paradigm。实验表明，我们的提议 QW 损失函数可以应用于多种 GNN 和提高它们在节点级预测和回归任务中的性能。
</details></li>
</ul>
<hr>
<h2 id="Unintended-Memorization-in-Large-ASR-Models-and-How-to-Mitigate-It"><a href="#Unintended-Memorization-in-Large-ASR-Models-and-How-to-Mitigate-It" class="headerlink" title="Unintended Memorization in Large ASR Models, and How to Mitigate It"></a>Unintended Memorization in Large ASR Models, and How to Mitigate It</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11739">http://arxiv.org/abs/2310.11739</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lun Wang, Om Thakkar, Rajiv Mathews</li>
<li>for: 检测大型自动声音识别（ASR）模型中的 memorization 问题，以保护隐私。</li>
<li>methods: 提出了一种简单的检测方法，通过快速生成的句子速度来创建声音和文本信息之间的易于学习Mapping。</li>
<li>results: 在state-of-the-art ASR模型中发现了memorization问题，并通过gradient clipping来 Mitigate memorization。在大规模分布式训练中，clip each example’s gradient可以保持中性模型质量和计算成本，同时提供强的隐私保护。<details>
<summary>Abstract</summary>
It is well-known that neural networks can unintentionally memorize their training examples, causing privacy concerns. However, auditing memorization in large non-auto-regressive automatic speech recognition (ASR) models has been challenging due to the high compute cost of existing methods such as hardness calibration. In this work, we design a simple auditing method to measure memorization in large ASR models without the extra compute overhead. Concretely, we speed up randomly-generated utterances to create a mapping between vocal and text information that is difficult to learn from typical training examples. Hence, accurate predictions only for sped-up training examples can serve as clear evidence for memorization, and the corresponding accuracy can be used to measure memorization. Using the proposed method, we showcase memorization in the state-of-the-art ASR models. To mitigate memorization, we tried gradient clipping during training to bound the influence of any individual example on the final model. We empirically show that clipping each example's gradient can mitigate memorization for sped-up training examples with up to 16 repetitions in the training set. Furthermore, we show that in large-scale distributed training, clipping the average gradient on each compute core maintains neutral model quality and compute cost while providing strong privacy protection.
</details>
<details>
<summary>摘要</summary>
很多人知道神经网络可能会无意地记忆训练示例，这引起了隐私问题。然而，对大型非自动回归自动语音识别（ASR）模型的审核记忆存在高计算成本的问题，使得现有方法如困难度调整不太实用。在这种情况下，我们提出了一种简单的审核方法，可以不增加计算成本来测量大型ASR模型的记忆。具体来说，我们将随机生成的语音快速播放，以创建语音和文本信息之间的易于学习的映射。因此，只有对快速播放的训练示例进行准确预测时，可以作为记忆的证据，并且可以用这个精度来测量记忆。使用我们的方法，我们显示了state-of-the-art ASR模型中的记忆。为了解决记忆问题，我们尝试使用梯度截断法在训练时进行 bounding 梯度的影响。我们经验显示，对每个示例的梯度进行截断可以 Mitigate 记忆，并且可以在快速播放示例中进行16次复制。此外，我们还显示了在大规模分布式训练中，对每个计算核心的梯度平均截断可以保持中立的模型质量和计算成本，同时提供强的隐私保护。
</details></li>
</ul>
<hr>
<h2 id="On-the-Evaluation-of-Generative-Models-in-Distributed-Learning-Tasks"><a href="#On-the-Evaluation-of-Generative-Models-in-Distributed-Learning-Tasks" class="headerlink" title="On the Evaluation of Generative Models in Distributed Learning Tasks"></a>On the Evaluation of Generative Models in Distributed Learning Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11714">http://arxiv.org/abs/2310.11714</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixiao Wang, Farzan Farnia, Zhenghao Lin, Yunheng Shen, Bei Yu</li>
<li>for: 这篇论文主要关注在分布式学习中评估深度生成模型，包括生成对抗网络（GANs）和扩散模型。</li>
<li>methods: 这篇论文使用了Fréchet对劲距离（FID）和核心对劲距离（KID）等评估生成模型的方法。</li>
<li>results: 论文发现在分布式学习 задача中，使用FID和KID评估生成模型的结果可能会不同，具体来说是FID-avg和FID-all的评估结果可能会不同，而KID-avg和KID-all的评估结果则相同。<details>
<summary>Abstract</summary>
The evaluation of deep generative models including generative adversarial networks (GANs) and diffusion models has been extensively studied in the literature. While the existing evaluation methods mainly target a centralized learning problem with training data stored by a single client, many applications of generative models concern distributed learning settings, e.g. the federated learning scenario, where training data are collected by and distributed among several clients. In this paper, we study the evaluation of generative models in distributed learning tasks with heterogeneous data distributions. First, we focus on the Fr\'echet inception distance (FID) and consider the following FID-based aggregate scores over the clients: 1) FID-avg as the mean of clients' individual FID scores, 2) FID-all as the FID distance of the trained model to the collective dataset containing all clients' data. We prove that the model rankings according to the FID-all and FID-avg scores could be inconsistent, which can lead to different optimal generative models according to the two aggregate scores. Next, we consider the kernel inception distance (KID) and similarly define the KID-avg and KID-all aggregations. Unlike the FID case, we prove that KID-all and KID-avg result in the same rankings of generative models. We perform several numerical experiments on standard image datasets and training schemes to support our theoretical findings on the evaluation of generative models in distributed learning problems.
</details>
<details>
<summary>摘要</summary>
文章研究了深度生成模型（包括生成对抗网络）在分布式学习任务中的评价方法。现有评价方法主要针对中央式学习问题，即训练数据由单个客户端存储。然而，许多生成模型应用场景是分布式学习场景，例如联邦学习场景，其中训练数据由多个客户端分布存储。本文研究了分布式学习任务中各客户端数据分布不同的生成模型评价方法。首先，我们关注Fréchet吸引距离（FID），并考虑以下FID基于客户端的综合分数：1）FID-avg，即客户端个体FID分数的平均值，2）FID-all，即训练模型与所有客户端数据集的FID距离。我们证明了FID-all和FID-avg的模型排名可能不一致，可能导致不同的优化生成模型。接下来，我们考虑核心吸引距离（KID），并定义KID-avg和KID-all综合分数。与FID不同的是，我们证明了KID-all和KID-avg的模型排名是一致的。我们在标准图像集和训练方案上进行了多个数值实验来支持我们的理论发现。
</details></li>
</ul>
<hr>
<h2 id="Learning-under-Label-Proportions-for-Text-Classification"><a href="#Learning-under-Label-Proportions-for-Text-Classification" class="headerlink" title="Learning under Label Proportions for Text Classification"></a>Learning under Label Proportions for Text Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11707">http://arxiv.org/abs/2310.11707</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jatin Chauhan, Xiaoxuan Wang, Wei Wang</li>
<li>for: 本研究旨在探讨在�xygen Learning from Label Proportions（LLP）的挑战性设置下进行NLPR Training，其数据提供在汇总形式下，仅提供每个类别的样本比例作为ground truth。</li>
<li>methods: 本研究提出了一种新的形式ulation，以及一种learnability result，以提供一个generalization bound under LLP。此外，该研究还使用了一种自我supervised objective。</li>
<li>results: 根据实验结果，该方法在大规模模型和多个维度上的文本数据上 achieved better results compared to基elines in almost 87% of the experimental configurations, across multiple metrics。<details>
<summary>Abstract</summary>
We present one of the preliminary NLP works under the challenging setup of Learning from Label Proportions (LLP), where the data is provided in an aggregate form called bags and only the proportion of samples in each class as the ground truth. This setup is inline with the desired characteristics of training models under Privacy settings and Weakly supervision. By characterizing some irregularities of the most widely used baseline technique DLLP, we propose a novel formulation that is also robust. This is accompanied with a learnability result that provides a generalization bound under LLP. Combining this formulation with a self-supervised objective, our method achieves better results as compared to the baselines in almost 87% of the experimental configurations which include large scale models for both long and short range texts across multiple metrics.
</details>
<details>
<summary>摘要</summary>
我们介绍了一项初步的自然语言处理（NLP）工作，在“学习从标签含量（LLP）”的挑战性设置下进行训练，其中数据提供在归一化的形式下，即袋（bag），并且只有每个类别的样本占总数的比例作为真实的地面信息。这种设置符合训练模型下的隐私设置和弱监督。我们对最常用的基线技术DLLP的不规则性进行描述，并提出了一种新的形式ulation，这种形式ulation具有 robustness。此外，我们还提供了一个learnability result，它在LLP下提供了一个通用的泛化 bound。将这种形式ulation与一种自我超vised目标函数相结合，我们的方法在大规模的实验配置中（包括长文本和短文本） across multiple metrics  Achieves better results than baselines in nearly 87% of the cases.
</details></li>
</ul>
<hr>
<h2 id="AUC-mixup-Deep-AUC-Maximization-with-Mixup"><a href="#AUC-mixup-Deep-AUC-Maximization-with-Mixup" class="headerlink" title="AUC-mixup: Deep AUC Maximization with Mixup"></a>AUC-mixup: Deep AUC Maximization with Mixup</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11693">http://arxiv.org/abs/2310.11693</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianzhi Xv, Gang Li, Tianbao Yang</li>
<li>for: 提高异常点识别模型的泛化能力，解决深度AUC最大化（DAM）在小据集上存在严重过拟合问题。</li>
<li>methods: 使用混合数据增强（mixup）数据增强技术，并采用AUC环境损失来有效地从混合数据生成的数据中学习，称为AUC-mixup损失。</li>
<li>results: 在异常点识别和医学影像数据集上，与标准DAM训练方法相比，提出的AUC-mixup方法显示出更高的泛化性能。<details>
<summary>Abstract</summary>
While deep AUC maximization (DAM) has shown remarkable success on imbalanced medical tasks, e.g., chest X-rays classification and skin lesions classification, it could suffer from severe overfitting when applied to small datasets due to its aggressive nature of pushing prediction scores of positive data away from that of negative data. This paper studies how to improve generalization of DAM by mixup data augmentation -- an approach that is widely used for improving generalization of the cross-entropy loss based deep learning methods. %For overfitting issues arising from limited data, the common approach is to employ mixup data augmentation to boost the models' generalization performance by enriching the training data. However, AUC is defined over positive and negative pairs, which makes it challenging to incorporate mixup data augmentation into DAM algorithms. To tackle this challenge, we employ the AUC margin loss and incorporate soft labels into the formulation to effectively learn from data generated by mixup augmentation, which is referred to as the AUC-mixup loss. Our experimental results demonstrate the effectiveness of the proposed AUC-mixup methods on imbalanced benchmark and medical image datasets compared to standard DAM training methods.
</details>
<details>
<summary>摘要</summary>
While deep AUC maximization (DAM) has shown remarkable success on imbalanced medical tasks, such as chest X-rays classification and skin lesions classification, it can suffer from severe overfitting when applied to small datasets due to its aggressive nature of pushing prediction scores of positive data away from that of negative data. This paper studies how to improve the generalization of DAM by using mixup data augmentation -- an approach that is widely used for improving the generalization of cross-entropy loss-based deep learning methods. For overfitting issues arising from limited data, the common approach is to employ mixup data augmentation to boost the models' generalization performance by enriching the training data. However, AUC is defined over positive and negative pairs, which makes it challenging to incorporate mixup data augmentation into DAM algorithms. To tackle this challenge, we employ the AUC margin loss and incorporate soft labels into the formulation to effectively learn from data generated by mixup augmentation, which is referred to as the AUC-mixup loss. Our experimental results demonstrate the effectiveness of the proposed AUC-mixup methods on imbalanced benchmark and medical image datasets compared to standard DAM training methods.
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-based-on-Transformer-architecture-for-power-system-short-term-voltage-stability-assessment-with-class-imbalance"><a href="#Deep-learning-based-on-Transformer-architecture-for-power-system-short-term-voltage-stability-assessment-with-class-imbalance" class="headerlink" title="Deep learning based on Transformer architecture for power system short-term voltage stability assessment with class imbalance"></a>Deep learning based on Transformer architecture for power system short-term voltage stability assessment with class imbalance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11690">http://arxiv.org/abs/2310.11690</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Li, Jiting Cao, Yan Xu, Lipeng Zhu, Zhao Yang Dong</li>
<li>for: 本研究提出了一种解决短时电压稳定评估中数据不均衡问题的方法，以提高实时电压稳定评估的精度和可靠性。</li>
<li>methods: 本研究使用了Transformer架构，开发了一种名为StaaT的稳定评估Transformer，并采用了conditional Wasserstein生成敌对网络（CWGAN-GP）来生成Synthetic数据，以帮助创建一个均衡的、代表性的训练集。此外，本研究还采用了半upervised clustering学习来提高划分质量，因为短时电压稳定无一定的量化标准。</li>
<li>results: 数据测试表明，提出的方法在面临100:1的数据不均衡和噪音环境时仍然保持了稳定的性能，并且在增加可再生能源的情况下也保持了一致的效果。比较结果表明，CWGAN-GP生成的数据更具备均衡性，而StaaT也超过了其他深度学习算法。这种方法可以应用于实际短时电压稳定评估中，frequently face着数据不均衡和噪音挑战。<details>
<summary>Abstract</summary>
Most existing data-driven power system short-term voltage stability assessment (STVSA) approaches presume class-balanced input data. However, in practical applications, the occurrence of short-term voltage instability following a disturbance is minimal, leading to a significant class imbalance problem and a consequent decline in classifier performance. This work proposes a Transformer-based STVSA method to address this challenge. By utilizing the basic Transformer architecture, a stability assessment Transformer (StaaT) is developed {as a classification model to reflect the correlation between the operational states of the system and the resulting stability outcomes}. To combat the negative impact of imbalanced datasets, this work employs a conditional Wasserstein generative adversarial network with gradient penalty (CWGAN-GP) for synthetic data generation, aiding in the creation of a balanced, representative training set for the classifier. Semi-supervised clustering learning is implemented to enhance clustering quality, addressing the lack of a unified quantitative criterion for short-term voltage stability. {Numerical tests on the IEEE 39-bus test system extensively demonstrate that the proposed method exhibits robust performance under class imbalances up to 100:1 and noisy environments, and maintains consistent effectiveness even with an increased penetration of renewable energy}. Comparative results reveal that the CWGAN-GP generates more balanced datasets than traditional oversampling methods and that the StaaT outperforms other deep learning algorithms. This study presents a compelling solution for real-world STVSA applications that often face class imbalance and data noise challenges.
</details>
<details>
<summary>摘要</summary>
现有的数据驱动电力系统短期电压稳定评估（STVSA）方法大多假设输入数据具有均衡的分布。然而，在实际应用中，短期电压不稳定的发生率很低，导致数据分布受到很大的偏好问题，从而导致分类器性能下降。这项工作提出了一种基于Transformer的STVSA方法来解决这个挑战。通过利用基本Transformer架构，我们开发了一种稳定评估Transformer（StaaT），用于反映系统运行状态和导致的稳定结果之间的相关性。为了解决偏好数据的负面影响，这项工作采用了 conditional Wasserstein生成敌方网络（CWGAN-GP） для生成人工数据，以帮助创建一个均衡、代表性的训练集 для分类器。 semi-supervised clustering learning 技术被应用以提高归一化质量，因为没有短期电压稳定的准确量标准。 {numeraire tests on the IEEE 39-bus test system extensively demonstrate that the proposed method exhibits robust performance under class imbalances up to 100:1 and noisy environments, and maintains consistent effectiveness even with an increased penetration of renewable energy}. comparative results reveal that the CWGAN-GP generates more balanced datasets than traditional oversampling methods and that the StaaT outperforms other deep learning algorithms. this study presents a compelling solution for real-world STVSA applications that often face class imbalance and data noise challenges.
</details></li>
</ul>
<hr>
<h2 id="Subject-specific-Deep-Neural-Networks-for-Count-Data-with-High-cardinality-Categorical-Features"><a href="#Subject-specific-Deep-Neural-Networks-for-Count-Data-with-High-cardinality-Categorical-Features" class="headerlink" title="Subject-specific Deep Neural Networks for Count Data with High-cardinality Categorical Features"></a>Subject-specific Deep Neural Networks for Count Data with High-cardinality Categorical Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11654">http://arxiv.org/abs/2310.11654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hangbin Lee, Il Do Ha, Changha Hwang, Youngjo Lee</li>
<li>for: 提高预测性能和学习效率，适用于高纬度ategorical特征的分布数据处理。</li>
<li>methods: 基于 hierarchical likelihood 学习框架，引入gamma random effects，同时利用最大 likelihood 估计和best unbiased predictors来捕捉输入变量的非线性效应和subject-specific层效应。</li>
<li>results: 通过实验和实际数据分析，证明了提议方法的优势，包括提高预测性能和学习效率，以及适用于高纬度ategorical特征的分布数据处理。<details>
<summary>Abstract</summary>
There is a growing interest in subject-specific predictions using deep neural networks (DNNs) because real-world data often exhibit correlations, which has been typically overlooked in traditional DNN frameworks. In this paper, we propose a novel hierarchical likelihood learning framework for introducing gamma random effects into the Poisson DNN, so as to improve the prediction performance by capturing both nonlinear effects of input variables and subject-specific cluster effects. The proposed method simultaneously yields maximum likelihood estimators for fixed parameters and best unbiased predictors for random effects by optimizing a single objective function. This approach enables a fast end-to-end algorithm for handling clustered count data, which often involve high-cardinality categorical features. Furthermore, state-of-the-art network architectures can be easily implemented into the proposed h-likelihood framework. As an example, we introduce multi-head attention layer and a sparsemax function, which allows feature selection in high-dimensional settings. To enhance practical performance and learning efficiency, we present an adjustment procedure for prediction of random parameters and a method-of-moments estimator for pretraining of variance component. Various experiential studies and real data analyses confirm the advantages of our proposed methods.
</details>
<details>
<summary>摘要</summary>
有越来越多的研究者对特定领域预测使用深度神经网络（DNN），因为实际数据经常具有相关性，传统的DNN框架中通常会忽略这些相关性。在这篇论文中，我们提出了一种新的层次可能性学习框架，以在Poisson DNN中引入γ随机效应，以提高预测性能，同时捕捉输入变量的非线性效应和特定颗集效应。我们的方法同时实现最大可能性估计器和不偏预测器，通过优化单个目标函数。这种方法使得可以快速处理受集分布的端到端算法，这些分布frequently包含高cardinality的分类特征。此外，我们可以轻松地将当前的网络架构 integrate into our proposed h-likelihood framework。例如，我们引入多头注意层和简洁最大化函数，这些功能允许在高维度设置中进行特征选择。为了提高实际性和学习效率，我们提出了预测随机参数的调整方法和预测变量组件的方法-of-moments估计器。多种实验和实际数据分析证明了我们的提出的方法的优势。
</details></li>
</ul>
<hr>
<h2 id="Free-text-Keystroke-Authentication-using-Transformers-A-Comparative-Study-of-Architectures-and-Loss-Functions"><a href="#Free-text-Keystroke-Authentication-using-Transformers-A-Comparative-Study-of-Architectures-and-Loss-Functions" class="headerlink" title="Free-text Keystroke Authentication using Transformers: A Comparative Study of Architectures and Loss Functions"></a>Free-text Keystroke Authentication using Transformers: A Comparative Study of Architectures and Loss Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11640">http://arxiv.org/abs/2310.11640</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saleh Momeni, Bagher BabaAli</li>
<li>for: 这个研究旨在提出一个基于Transformer的网络，以提高键盘识别和验证的精度。</li>
<li>methods: 这个模型使用自我注意力来提取键盘序列中的有用特征，并评估了两种不同的架构， namely bi-encoder 和 cross-encoder，以及不同的损失函数和距离度量。</li>
<li>results: 这个研究发现，使用 bi-encoder 架构、批量全 triplet 损失函数和圆形距离度量可以实现最佳性能，具体Equla Error Rate 为0.0186%。此外，还探讨了不同的相似度评估方法，以提高模型的精度。<details>
<summary>Abstract</summary>
Keystroke biometrics is a promising approach for user identification and verification, leveraging the unique patterns in individuals' typing behavior. In this paper, we propose a Transformer-based network that employs self-attention to extract informative features from keystroke sequences, surpassing the performance of traditional Recurrent Neural Networks. We explore two distinct architectures, namely bi-encoder and cross-encoder, and compare their effectiveness in keystroke authentication. Furthermore, we investigate different loss functions, including triplet, batch-all triplet, and WDCL loss, along with various distance metrics such as Euclidean, Manhattan, and cosine distances. These experiments allow us to optimize the training process and enhance the performance of our model. To evaluate our proposed model, we employ the Aalto desktop keystroke dataset. The results demonstrate that the bi-encoder architecture with batch-all triplet loss and cosine distance achieves the best performance, yielding an exceptional Equal Error Rate of 0.0186%. Furthermore, alternative algorithms for calculating similarity scores are explored to enhance accuracy. Notably, the utilization of a one-class Support Vector Machine reduces the Equal Error Rate to an impressive 0.0163%. The outcomes of this study indicate that our model surpasses the previous state-of-the-art in free-text keystroke authentication. These findings contribute to advancing the field of keystroke authentication and offer practical implications for secure user verification systems.
</details>
<details>
<summary>摘要</summary>
“键盘生物метри学是一种有前途的方法 для用户识别和验证，利用个人键盘实习独特的模式。在本研究中，我们提出了基于Transformer的网络，使用自我对项来提取键盘序列中有用的特征，超越传统的Recurrent Neural Networks的表现。我们探索了两种不同的架构，分别是双向encoder和cross-encoder，并比较它们在键盘验证中的效果。此外，我们寻找了不同的损失函数，包括三重、批量三重和WDCL损失函数，以及不同的距离度量，如Euclidean、曼哈顿和内角距离。这些实验允许我们优化训练过程，提高模型的性能。为了评估我们的提案模型，我们使用了阿尔托桌面键盘数据集。结果显示，双向encoder架构加 batch-all triplet损失函数和内角距离可以取得最佳性能，具体Equla Error Rate为0.0186%。此外，我们还探索了不同的相似度计算算法，以提高准确性。例如，使用一个一阶支持向量机可以降低Equla Error Rate至0.0163%。研究结果显示，我们的模型超越了过去的州际前进于自由文本键盘验证。这些发现对于键盘验证领域的进步做出了贡献，并且提供了实际的应用于安全用户验证系统。”
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/18/cs.LG_2023_10_18/" data-id="clogxf3ou00pz5xradpaj9xev" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_10_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/18/eess.IV_2023_10_18/" class="article-date">
  <time datetime="2023-10-18T09:00:00.000Z" itemprop="datePublished">2023-10-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/18/eess.IV_2023_10_18/">eess.IV - 2023-10-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="System-identification-and-closed-loop-control-of-laser-hot-wire-directed-energy-deposition-using-the-parameter-signature-property-modeling-scheme"><a href="#System-identification-and-closed-loop-control-of-laser-hot-wire-directed-energy-deposition-using-the-parameter-signature-property-modeling-scheme" class="headerlink" title="System identification and closed-loop control of laser hot-wire directed energy deposition using the parameter-signature-property modeling scheme"></a>System identification and closed-loop control of laser hot-wire directed energy deposition using the parameter-signature-property modeling scheme</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12286">http://arxiv.org/abs/2310.12286</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. Rahmani Dehaghani, Atieh Sahraeidolatkhaneh, Morgan Nilsen, Fredrik Sikström, Pouyan Sajadi, Yifan Tang, G. Gary Wang<br>for: This paper focuses on developing a parameter-signature-property modeling and control approach to enhance the quality of additively manufactured parts using hot-wire directed energy deposition with a laser beam (DED-LB&#x2F;w).methods: The paper employs a dynamic modeling approach to investigate the relationship between process parameters and melt pool width, as well as a fully connected artificial neural network to predict the final part property (bead width) based on melt pool signatures.results: The proposed parameter-signature-property modeling approach shows clear advantages in controlling the width of the part compared to a control loop with only process signature (melt pool width) information. The approach has the potential to be applied to control other part properties that cannot be directly measured or monitored in situ.<details>
<summary>Abstract</summary>
Hot-wire directed energy deposition using a laser beam (DED-LB/w) is a method of metal additive manufacturing (AM) that has benefits of high material utilization and deposition rate, but parts manufactured by DED-LB/w suffer from a substantial heat input and undesired surface finish. Hence, monitoring and controlling the process parameters and signatures during the deposition is crucial to ensure the quality of final part properties and geometries. This paper explores the dynamic modeling of the DED-LB/w process and introduces a parameter-signature-property modeling and control approach to enhance the quality of modeling and control of part properties that cannot be measured in situ. The study investigates different process parameters that influence the melt pool width (signature) and bead width (property) in single and multi-layer beads. The proposed modeling approach utilizes a parameter-signature model as F_1 and a signature-property model as F_2. Linear and nonlinear modeling approaches are compared to describe a dynamic relationship between process parameters and a process signature, the melt pool width (F_1). A fully connected artificial neural network is employed to model and predict the final part property, i.e., bead width, based on melt pool signatures (F_2). Finally, the effectiveness and usefulness of the proposed parameter-signature-property modeling is tested and verified by integrating the parameter-signature (F_1) and signature-property (F_2) models in the closed-loop control of the width of the part. Compared with the control loop with only F_1, the proposed method shows clear advantages and bears potential to be applied to control other part properties that cannot be directly measured or monitored in situ.
</details>
<details>
<summary>摘要</summary>
热束导电能量沉积使用激光束(DED-LB/w)是一种金属添加生产(AM)的方法，它具有高材料利用率和沉积速率的优点，但是制造出来的部件受到了大量的热输入和不想要的表面镀层。因此，对沉积过程参数和特征的监测和控制是至关重要，以确保最终部件的性能和几何尺寸。本文研究了DED-LB/w процесс的动态模型化，并提出了参数-特征-性能模型控制方法，以提高模型和控制不可直接测量或监测的部件性能的能力。研究表示，不同的处理参数对沉积过程中的溶融池宽度（特征）和束宽度（性能）的影响。对于单层和多层束，提出了参数-特征模型和特征-性能模型两种模型方法。使用全连接人工神经网络模型和预测最终部件性能，基于溶融池特征。最后，通过将参数-特征模型和特征-性能模型在关闭控制 loop中集成，证明了提posed方法的效iveness和实用性。相比只使用参数-特征模型控制 loop，提posed方法显示了明显的优势，并可以应用于控制其他不可直接测量或监测的部件性能。
</details></li>
</ul>
<hr>
<h2 id="Denoising-total-scattering-data-using-Compressed-Sensing"><a href="#Denoising-total-scattering-data-using-Compressed-Sensing" class="headerlink" title="Denoising total scattering data using Compressed Sensing"></a>Denoising total scattering data using Compressed Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11887">http://arxiv.org/abs/2310.11887</a></li>
<li>repo_url: None</li>
<li>paper_authors: James Weng, Niklas B. Thompson, Christopher Folmar, James D. Martin, Christina Hoffman</li>
<li>for: 该论文是为了提高Diffraction图像的信号响应率（SNR）而写的。</li>
<li>methods: 该论文使用了压缩感知技术，该技术已经成功应用于摄影、人脸识别和医疗影像等领域，以提高Diffraction图像的SNR。</li>
<li>results: 该论文表明，通过使用压缩感知技术，可以将单个Diffraction测量转化为一个有效无限多的虚拟测量，从而实现超分辨率成像。<details>
<summary>Abstract</summary>
To obtain the best resolution for any measurement there is an ever-present challenge to achieve maximal differentiation between signal and noise over as fine of sampling dimensions as possible. In diffraction science these issues are particularly pervasive when analyzing small crystals, systems with diffuse scattering, or other systems in which the signal of interest is extremely weak and incident flux and instrument time is limited. We here demonstrate that the tool of compressed sensing, which has successfully been applied to photography, facial recognition, and medical imaging, can be effectively applied to diffraction images to dramatically improve the signal-to-noise ratio (SNR) in a data-driven fashion without the need for additional measurements or modification of existing hardware. We outline a technique that leverages compressive sensing to bootstrap a single diffraction measurement into an effectively arbitrary number of virtual measurements, thereby providing a means of super-resolution imaging.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="FixPix-Fixing-Bad-Pixels-using-Deep-Learning"><a href="#FixPix-Fixing-Bad-Pixels-using-Deep-Learning" class="headerlink" title="FixPix: Fixing Bad Pixels using Deep Learning"></a>FixPix: Fixing Bad Pixels using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11637">http://arxiv.org/abs/2310.11637</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sreetama Sarkar, Xinan Ye, Gourav Datta, Peter A. Beerel</li>
<li>for: 提高图像感知器的生产率和预计寿命，提出了一种基于深度学习的在线检测修复方法，适用于各种像素损害率。</li>
<li>methods: 提出了一种信任报表调整的分割方法，可以在几个训练样本下实现几乎完美的坏像素检测。还提出了一种 Computationally light-weight 的修复算法，可以在低于传统插值技术的环境下达到更高的准确率。</li>
<li>results: 通过使用开源的 Samsung S7 ISP 和 MIT-Adobe FiveK 数据集，实现了高达 99.6% 的检测精度，False Positives 低于 0.6%，并在 70% 损害的图像中实现了平均像素误差在 1.5% 之间的修复。<details>
<summary>Abstract</summary>
Efficient and effective on-line detection and correction of bad pixels can improve yield and increase the expected lifetime of image sensors. This paper presents a comprehensive Deep Learning (DL) based on-line detection-correction approach, suitable for a wide range of pixel corruption rates. A confidence calibrated segmentation approach is introduced, which achieves nearly perfect bad pixel detection, even with few training samples. A computationally light-weight correction algorithm is proposed for low rates of pixel corruption, that surpasses the accuracy of traditional interpolation-based techniques. We also propose an autoencoder based image reconstruction approach which alleviates the need for prior bad pixel detection and yields promising results for high rates of pixel corruption. Unlike previous methods, which use proprietary images, we demonstrate the efficacy of the proposed methods on the open-source Samsung S7 ISP and MIT-Adobe FiveK datasets. Our approaches yield up to 99.6% detection accuracy with <0.6% false positives and corrected images within 1.5% average pixel error from 70% corrupted images.
</details>
<details>
<summary>摘要</summary>
高效和有效的在线检测和修正坏像素可以提高图像传感器的产量和预期的寿命。这篇论文提出了一种基于深度学习（DL）的全面在线检测修正方法，适用于各种坏像素损害率。我们引入了一种决度规则化的分割方法，可以在少量训练样本下达到几乎完美的坏像素检测效果。我们还提出了一种 Computational 轻量级的修正算法，可以在低坏像素率下超越传统的 interpolate-based 技术。此外，我们还提出了一种基于 autoencoder 的图像重建方法，可以消除先前的坏像素检测，并且在高坏像素率下实现了出色的 результаados。与先前的方法不同，我们使用开源的 Samsung S7 ISP 和 MIT-Adobe FiveK 数据集来证明方法的可行性。我们的方法可以达到 99.6% 的检测精度，False Positives  <0.6%，并且在 70% 损害的图像上修正了 <1.5% 的平均像素误差。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/18/eess.IV_2023_10_18/" data-id="clogxf3t501655xra2d74bd2b" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_10_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/18/eess.SP_2023_10_18/" class="article-date">
  <time datetime="2023-10-18T08:00:00.000Z" itemprop="datePublished">2023-10-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/18/eess.SP_2023_10_18/">eess.SP - 2023-10-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Instantaneous-Frequency-Estimation-in-Unbalanced-Systems-Using-Affine-Differential-Geometry"><a href="#Instantaneous-Frequency-Estimation-in-Unbalanced-Systems-Using-Affine-Differential-Geometry" class="headerlink" title="Instantaneous Frequency Estimation in Unbalanced Systems Using Affine Differential Geometry"></a>Instantaneous Frequency Estimation in Unbalanced Systems Using Affine Differential Geometry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12299">http://arxiv.org/abs/2310.12299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Alshawabkeh, Georgios Tzounas, Federico Milano</li>
<li>for: 本研究探讨了电压和频率之间的关系，以及均匀差异拓扑学中的弹性弯曲长度和曲率的关系。</li>
<li>methods: 本研究使用了均匀差异拓扑学 invariants来链接频率和电压的时间导数关系。基于这种关系，提出了一种新的快速频率估计方程，适用于不均匀系统。</li>
<li>results: 本研究提出了一种新的快速频率估计方程，可以在单相系统中实现高精度的频率估计。数值示例表明，该方程在不均匀、单相系统中具有高精度和稳定性。<details>
<summary>Abstract</summary>
The paper discusses the relationships between electrical quantities, namely voltages and frequency, and affine differential geometry ones, namely affine arc length and curvature. Moreover, it establishes a link between frequency and time derivatives of voltage, through the utilization of affine differential geometry invariants. Based on this link, a new instantaneous frequency estimation formula is proposed, which is particularly suited for unbalanced systems. An application of the proposed formula to single-phase systems is also provided. Several numerical examples based on balanced, unbalanced, as well as single-phase systems illustrate the findings of the paper.
</details>
<details>
<summary>摘要</summary>
文章讨论了电量和几何量之间的关系，即电压和频率，以及几何均衡量的抽象弹性长度和曲率。文章还将频率与电压的时间导数联系起来，通过使用几何均衡量的 invariants。基于这个联系，文章提出了一种新的快速频率估算公式，特别适用于不均衡系统。文章还提供了应用于单相系统的示例。文章的结论由多个平衡、不平衡和单相系统的数据 illustrate。Note: "几何均衡量" (affine differential geometry) is a bit of a mouthful in Chinese, so I translated it as "几何均衡量" (affine differential geometry) instead of using the more common "几何学" (geometry) or "几何均衡" (affine geometry).
</details></li>
</ul>
<hr>
<h2 id="Channel-Estimation-via-Loss-Field-Accurate-Site-Trained-Modeling-for-Shadowing-Prediction"><a href="#Channel-Estimation-via-Loss-Field-Accurate-Site-Trained-Modeling-for-Shadowing-Prediction" class="headerlink" title="Channel Estimation via Loss Field: Accurate Site-Trained Modeling for Shadowing Prediction"></a>Channel Estimation via Loss Field: Accurate Site-Trained Modeling for Shadowing Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12284">http://arxiv.org/abs/2310.12284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Wang, Meles G. Weldegebriel, Neal Patwari</li>
<li>for: 这个论文旨在为未来的移动广播网络提供一种低计算复杂性的通道模型，以确保频率划分和信号干扰水平的要求。</li>
<li>methods: 该论文提出了一种新的通道模型，即Channel Estimation using Loss Field（CELF），该模型使用了部署在地区的通道损失测量数据和bayesian线性回归方法来估算地区具有特定损失场的loss field。</li>
<li>results: 论文使用了广泛的测量数据显示，CELF可以降低通道估计的方差 by up to 56%，并且在 variance reduction和训练效率方面超过了3种popular机器学习方法。<details>
<summary>Abstract</summary>
Future mobile ad hoc networks will share spectrum between many users. Channels will be assigned on the fly to guarantee signal and interference power requirements for requested links. Channel losses must be re-estimated between many pairs of users as they move and as environmental conditions change. Computational complexity must be low, precluding the use of some accurate but computationally intensive site-specific channel models. Channel model errors must be low, precluding the use of standard statistical channel models. We propose a new channel model, CELF, which uses channel loss measurements from a deployed network in the area and a Bayesian linear regression method to estimate a site-specific loss field for the area. The loss field is explainable as the site's 'shadowing' of the radio propagation across the area of interest, but it requires no site-specific terrain or building information. Then, for any arbitrary pair of transmitter and receiver positions, CELF sums the loss field near the link line to estimate its channel loss. We use extensive measurements to show that CELF lowers the variance of channel estimates by up to 56%. It outperforms 3 popular machine learning methods in variance reduction and training efficiency.
</details>
<details>
<summary>摘要</summary>
未来的移动广播网络将共享多个用户的频率谱，为请求链接确保信号和干扰电磁谱的功率要求。在多个用户之间移动和环境条件发生变化时，通道将在实时基础上分配。由于计算复杂性需要低，因此排除了一些精度高但计算复杂度高的站点特定通道模型。通道模型错误也需要低，因此排除了标准的统计学通道模型。我们提出了一种新的通道模型，即 CEFL，它使用已部署网络中的通道损失测量和 bayesian 线性回归方法来估算区域特定的损失场。这个损失场可以解释为当地的“遮挡”，但无需站点特定的地形或建筑信息。然后，为任意传输器和接收器位置对，CEFL将近邻链接线上的损失场总和来估算链接损失。我们使用了广泛的测量数据表明，CEFL可以降低通道估计的方差，最多降低56%。同时，它在 variance 降低和训练效率上比3种受欢迎的机器学习方法表现更好。
</details></li>
</ul>
<hr>
<h2 id="Measuring-Thermal-Profiles-in-High-Explosives-using-Neural-Networks"><a href="#Measuring-Thermal-Profiles-in-High-Explosives-using-Neural-Networks" class="headerlink" title="Measuring Thermal Profiles in High Explosives using Neural Networks"></a>Measuring Thermal Profiles in High Explosives using Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12260">http://arxiv.org/abs/2310.12260</a></li>
<li>repo_url: None</li>
<li>paper_authors: John Greenhall, David K. Zerkle, Eric S. Davis, Robert Broilo, Cristian Pantea</li>
<li>for: 用于评估高爆物质的安全状况，以及其他各种应用场景，如化学反应、热动力过程、铁或塑料铸造、热能存储壳中的能量强度和异常电池运行等。</li>
<li>methods: 使用Convolutional Neural Network（CNN）计算高爆物质的温度profile，并在实验和 simulations中采集了声学和温度数据。实验中使用了多个声学传感器环绕高爆物质容器的外围来收集声波信号，但是在实验中测量高爆物质的温度profile会需要插入大量的热电阻温度探测器，这会对加热过程产生干扰。因此，本研究使用了两个热电阻温度探测器，一个位于高爆物质的中心，另一个位于容器墙上，并使用了finite element方法来计算高爆物质的温度分布，并对实验中心和墙上的温度进行修正。</li>
<li>results: 通过对实验和 simulations中的数据进行分析，本研究发现了一种可以评估高爆物质的安全状况的方法，并且可以在各种应用场景中提供温度profile的内部测量。研究还发现，使用更多的声学Receiver和更高的温度预测分辨率可以提高算法的准确性。<details>
<summary>Abstract</summary>
We present a new method for calculating the temperature profile in high explosive (HE) material using a Convolutional Neural Network (CNN). To train/test the CNN, we have developed a hybrid experiment/simulation method for collecting acoustic and temperature data. We experimentally heat cylindrical containers of HE material until detonation/deflagration, where we continuously measure the acoustic bursts through the HE using multiple acoustic transducers lined around the exterior container circumference. However, measuring the temperature profile in the HE in experiment would require inserting a high number of thermal probes, which would disrupt the heating process. Thus, we use two thermal probes, one at the HE center and one at the wall. We then use finite element simulation of the heating process to calculate the temperature distribution, and correct the simulated temperatures based on the experimental center and wall temperatures. We calculate temperature errors on the order of 15{\deg}C, which is approximately 12% of the range of temperatures in the experiment. We also investigate how the algorithm accuracy is affected by the number of acoustic receivers used to collect each measurement and the resolution of the temperature prediction. This work provides a means of assessing the safety status of HE material, which cannot be achieved using existing temperature measurement methods. Additionally, it has implications for range of other applications where internal temperature profile measurements would provide critical information. These applications include detecting chemical reactions, observing thermodynamic processes like combustion, monitoring metal or plastic casting, determining the energy density in thermal storage capsules, and identifying abnormal battery operation.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法来计算高爆物（HE）材料中的温度分布，使用卷积神经网络（CNN）。为了训练/测试CNN，我们开发了一种混合实验/模拟方法来收集振荡和温度数据。我们通过对HE材料中的圆柱形容器进行热处理，直到发生激发/燃烧，并在HE表面附近安装多个声学传感器来记录振荡。但是，在实验中测量HE材料中的温度分布需要插入大量的热度探针，这会对热处理进行干扰。因此，我们使用了两个热度探针，一个位于HE的中心和一个位于容器壁上。我们然后使用HE材料的热处理的数学模拟来计算温度分布，并根据实验中心和壁温度进行修正。我们计算的温度误差在15℃之间，相当于实验中温度范围的12%。我们还研究了使用声学传感器来收集测量数据的数量和分辨率如何影响算法的准确性。这项工作为HE材料的安全状况评估提供了一种新的方法，同时也对其他应用有着潜在的影响。这些应用包括检测化学反应、观察燃烧过程、监测金属或塑料铸造、测量热存储囊中的能量密度、并识别异常电池运行。
</details></li>
</ul>
<hr>
<h2 id="Ordered-Reliability-Direct-Error-Pattern-Testing-Decoding-Algorithm"><a href="#Ordered-Reliability-Direct-Error-Pattern-Testing-Decoding-Algorithm" class="headerlink" title="Ordered Reliability Direct Error Pattern Testing Decoding Algorithm"></a>Ordered Reliability Direct Error Pattern Testing Decoding Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12039">http://arxiv.org/abs/2310.12039</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reza Hadavian, Xiaoting Huang, Dmitri Truhachev, Kamal El-Sankary, Hamid Ebrahimzad, Hossein Najafi</li>
<li>for: 这篇论文是为了提出一种新的通用软决策解码算法，用于二进制块编码。</li>
<li>methods: 该算法使用ordered reliability direct error pattern testing（ORDEPT）技术，并对各种流行的短高速编码进行了测试，结果显示ORDEPT在与相同复杂性的其他解码算法相比，具有较低的解码错误概率和延迟。</li>
<li>results: 该paper的结果表明，ORDEPT可以高效地查找多个候选码word，并在迭代解码中提高产生软输出的能力。<details>
<summary>Abstract</summary>
We introduce a novel universal soft-decision decoding algorithm for binary block codes called ordered reliability direct error pattern testing (ORDEPT). Our results, obtained for a variety of popular short high-rate codes, demonstrate that ORDEPT outperforms state-of-the-art decoding algorithms of comparable complexity such as ordered reliability bits guessing random additive noise decoding (ORBGRAND) in terms of the decoding error probability and latency. The improvements carry on to the iterative decoding of product codes and convolutional product-like codes, where we present a new adaptive decoding algorithm and demonstrate the ability of ORDEPT to efficiently find multiple candidate codewords to produce soft output.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的通用软决策解码算法，即顺序可靠性直接错误模式测试（ORDEPT），用于二进制块码。我们的结果，在各种受欢迎的短高速码中，示出了ORDEPT比同等复杂度的批量解码算法，如顺序可靠性位元随机加速错误推测解码（ORBGRAND），在解码错误probability和延迟方面表现更好。这些改进继续延伸到产生转换码和几何产生码的迭代解码中，我们提出了一个新的适应解码算法，并证明了ORDEPT可以高效地找到多个候选码word来生成软出力。
</details></li>
</ul>
<hr>
<h2 id="One-Bit-Byzantine-Tolerant-Distributed-Learning-via-Over-the-Air-Computation"><a href="#One-Bit-Byzantine-Tolerant-Distributed-Learning-via-Over-the-Air-Computation" class="headerlink" title="One-Bit Byzantine-Tolerant Distributed Learning via Over-the-Air Computation"></a>One-Bit Byzantine-Tolerant Distributed Learning via Over-the-Air Computation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11998">http://arxiv.org/abs/2310.11998</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhan Yang, Youlong Wu, Yuning Jiang, Yuanming Shi</li>
<li>for: 这篇论文研究了分布式学习在无线数据中心网络中，以提高智能应用的可扩展性和可靠性。</li>
<li>methods: 该论文提出了基于签名泊松度下降（SignSGD）和无线计算（AirComp）的层次投票框架，以解决分布式学习中的Byzantine攻击和无线环境的影响。</li>
<li>results: 研究人员通过分析和验证了该框架在Byzantine攻击和无线环境下的性能，并证明了其在分布式学习中的稳定性和可靠性。<details>
<summary>Abstract</summary>
Distributed learning has become a promising computational parallelism paradigm that enables a wide scope of intelligent applications from the Internet of Things (IoT) to autonomous driving and the healthcare industry. This paper studies distributed learning in wireless data center networks, which contain a central edge server and multiple edge workers to collaboratively train a shared global model and benefit from parallel computing. However, the distributed nature causes the vulnerability of the learning process to faults and adversarial attacks from Byzantine edge workers, as well as the severe communication and computation overhead induced by the periodical information exchange process. To achieve fast and reliable model aggregation in the presence of Byzantine attacks, we develop a signed stochastic gradient descent (SignSGD)-based Hierarchical Vote framework via over-the-air computation (AirComp), where one voting process is performed locally at the wireless edge by taking advantage of Bernoulli coding while the other is operated over-the-air at the central edge server by utilizing the waveform superposition property of the multiple-access channels. We comprehensively analyze the proposed framework on the impacts including Byzantine attacks and the wireless environment (channel fading and receiver noise), followed by characterizing the convergence behavior under non-convex settings. Simulation results validate our theoretical achievements and demonstrate the robustness of our proposed framework in the presence of Byzantine attacks and receiver noise.
</details>
<details>
<summary>摘要</summary>
分布式学习已成为智能应用领域的扩展 Computational parallelism 方法之一，从互联网东西 (IoT) 到自动驾驶和医疗行业。这篇论文研究了无线数据中心网络中的分布式学习，该网络包括中央边缘服务器和多个边缘工作者，共同训练共享全球模型，并且从并行计算中受益。然而，分布式结构导致学习过程中的容易受到故障和恶意攻击，以及由 periodic 信息交换过程引起的严重通信和计算开销。为了在存在恶意攻击情况下实现快速和可靠的模型聚合，我们提出了基于签名随机梯度下降 (SignSGD) 的层次投票框架，该框架通过 wireless 边缘上进行本地 Bernoulli 编码，而在中央边缘服务器上通过多ступChannel 的波形重叠性特性进行无线计算。我们系统分析了提议的框架，包括恶意攻击和无线环境（通道抑降和接收噪声）的影响，然后对非拟合情况进行分析。实验结果证明我们的理论成果，并在存在恶意攻击和接收噪声情况下展示了我们的提议框架的可靠性。
</details></li>
</ul>
<hr>
<h2 id="Parallel-Log-Spectra-index-PaLOSi-a-quality-metric-in-large-scale-resting-EEG-preprocessing"><a href="#Parallel-Log-Spectra-index-PaLOSi-a-quality-metric-in-large-scale-resting-EEG-preprocessing" class="headerlink" title="Parallel Log Spectra index (PaLOSi): a quality metric in large scale resting EEG preprocessing"></a>Parallel Log Spectra index (PaLOSi): a quality metric in large scale resting EEG preprocessing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11994">http://arxiv.org/abs/2310.11994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiang Hu, Jie Ruan, Nicolas Langer, Jorge Bosch-Bayard, Zhao Lv, Dezhong Yao, Pedro Antonio Valdes-Sosa</li>
<li>for: 这 paper 的目的是提出一种 novel data quality metric，以帮助大规模自动预处理中控制质量。</li>
<li>methods: 这 paper 使用了 Independent Component Analysis (ICA) 作为预处理步骤的基础，并对 PaLOS 现象进行了仔细的观察和分析。</li>
<li>results: 这 paper 的结果表明，PaLOS 存在可能导致不正确的连接分析结果，并且提出了一种基于 common principal component analysis 的 PaLOS index (PaLOSi)，可以检测 PaLOS 的存在。PaLOSi 的性能在 30094 个 EEG 数据集上进行了测试，结果显示 PaLOSi 可以检测不正确的预处理结果，并且具有较好的Robustness。<details>
<summary>Abstract</summary>
Toward large scale electrophysiology data analysis, many preprocessing pipelines are developed to reject artifacts as the prerequisite step before the downstream analysis. A mainstay of these pipelines is based on the data driven approach -- Independent Component Analysis (ICA). Nevertheless, there is little effort put to the preprocessing quality control. In this paper, attentions to this issue were carefully paid by our observation that after running ICA based preprocessing pipeline: some subjects showed approximately Parallel multichannel Log power Spectra (PaLOS), namely, multichannel power spectra are proportional to each other. Firstly, the presence of PaLOS and its implications to connectivity analysis were described by real instance and simulation; secondly, we built its mathematical model and proposed the PaLOS index (PaLOSi) based on the common principal component analysis to detect its presence; thirdly, the performance of PaLOSi was tested on 30094 cases of EEG from 5 databases. The results showed that 1) the PaLOS implies a sole source which is physiologically implausible. 2) PaLOSi can detect the excessive elimination of brain components and is robust in terms of channel number, electrode layout, reference, and the other factors. 3) PaLOSi can output the channel and frequency wise index to help for in-depth check. This paper presented the PaLOS issue in the quality control step after running the preprocessing pipeline and the proposed PaLOSi may serve as a novel data quality metric in the large-scale automatic preprocessing.
</details>
<details>
<summary>摘要</summary>
大规模电physiology数据分析中，许多预处理管道被开发出来拒绝噪声作为下游分析的前提步骤。主流的预处理管道基于数据驱动方法---独立组件分析（ICA）。然而，对预处理质量控制的努力不多。在这篇论文中，我们仔细注意到，在运行基于ICA的预处理管道后，一些主体显示了相似的多通道峰谱特征（PaLOS），即多通道峰谱的强度相对彼此成比例。我们首先描述了PaLOS的存在和其对连接分析的影响，然后构建了其数学模型，并基于共同主成分分析提出了PaLOS指数（PaLOSi）来检测其存在。最后，我们测试了PaLOSi在5个数据库中的30094个EEG样本。结果表明：1）PaLOS存在唯一的源，这是生物学上不可能的。2）PaLOSi可以检测预处理过程中的质量问题，并且在通道数、电极布局、参照、其他因素等方面具有稳定性。3）PaLOSi可以输出通道和频率 wise的指数，帮助进行深入的检查。本文描述了预处理管道后的质量控制步骤中PaLOS问题，并提出了PaLOSi作为大规模自动预处理中的新数据质量指标。
</details></li>
</ul>
<hr>
<h2 id="Supporting-UAVs-with-Edge-Computing-A-Review-of-Opportunities-and-Challenges"><a href="#Supporting-UAVs-with-Edge-Computing-A-Review-of-Opportunities-and-Challenges" class="headerlink" title="Supporting UAVs with Edge Computing: A Review of Opportunities and Challenges"></a>Supporting UAVs with Edge Computing: A Review of Opportunities and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11957">http://arxiv.org/abs/2310.11957</a></li>
<li>repo_url: None</li>
<li>paper_authors: Malte Janßen, Tobias Pfandzelter, Minghe Wang, David Bermbach</li>
<li>for: 本研究旨在分析现代无人机技术的发展，以及如何通过边缘计算提高无人机的任务完成速度、能效性和可靠性。</li>
<li>methods: 本研究采用系统性的文献综述方法，收集、选择和提取关键领域的研究成果，以探讨无人机活动如何通过边缘计算得到改进。</li>
<li>results: 研究发现，通过边缘计算可以提高无人机的任务完成速度、能效性和可靠性，并且可以应用于多个领域和行业。<details>
<summary>Abstract</summary>
Over the last years, Unmanned Aerial Vehicles (UAVs) have seen significant advancements in sensor capabilities and computational abilities, allowing for efficient autonomous navigation and visual tracking applications. However, the demand for computationally complex tasks has increased faster than advances in battery technology. This opens up possibilities for improvements using edge computing. In edge computing, edge servers can achieve lower latency responses compared to traditional cloud servers through strategic geographic deployments. Furthermore, these servers can maintain superior computational performance compared to UAVs, as they are not limited by battery constraints. Combining these technologies by aiding UAVs with edge servers, research finds measurable improvements in task completion speed, energy efficiency, and reliability across multiple applications and industries. This systematic literature review aims to analyze the current state of research and collect, select, and extract the key areas where UAV activities can be supported and improved through edge computing.
</details>
<details>
<summary>摘要</summary>
过去几年，无人飞行器（UAV）技术已经减少了很多，包括感知和计算能力等方面。这使得无人飞行器可以更加高效地进行自主导航和视觉跟踪应用。然而，计算复杂任务的需求增长 faster than 电池技术的进步，这开 up了可以通过边缘计算提高无人飞行器性能的可能性。在边缘计算中，边缘服务器可以在地理上投入策略的部署下实现更低的响应时间，相比于传统的云服务器。此外，这些服务器可以在无人飞行器上保持更高的计算性能，因为它们不受电池限制。通过将这些技术相结合，研究发现在多个应用和领域中，任务完成速度、能效性和可靠性都有所提高。这个系统性文献综述的目的是分析当前研究的状况，收集、选择和提取无人飞行器活动中可以通过边缘计算提高的关键领域。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Based-Detection-on-RIS-Assisted-RSM-and-RSSK-Techniques"><a href="#Deep-Learning-Based-Detection-on-RIS-Assisted-RSM-and-RSSK-Techniques" class="headerlink" title="Deep Learning Based Detection on RIS Assisted RSM and RSSK Techniques"></a>Deep Learning Based Detection on RIS Assisted RSM and RSSK Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11924">http://arxiv.org/abs/2310.11924</a></li>
<li>repo_url: None</li>
<li>paper_authors: Onur Salan, Ferhat Bayar, Hacı Ilhan, Erdogan Aydin</li>
<li>for: 本研究旨在探讨使用深度学习技术来对RIS帮助的接收SM&#x2F;RSSK系统进行检测，以实现spectral和能量效率的平衡。</li>
<li>methods: 本研究使用了深度学习的概念，特别是块深度神经网络（B-DNN），以提高RIS帮助的SM系统的检测性能。</li>
<li>results:  Monte Carlo simulate results show that B-DNN可以与最大可能性（ML）相比，并且在比较于匀速检测器（Greedy detector）的情况下，提供了更好的检测性能。<details>
<summary>Abstract</summary>
The reconfigurable intelligent surface (RIS) is considered a crucial technology for the future of wireless communication. Recently, there has been significant interest in combining RIS with spatial modulation (SM) or space shift keying (SSK) to achieve a balance between spectral and energy efficiency. In this paper, we have investigated the use of deep learning techniques for detection in RIS-aided received SM (RSM)/received-SSK (RSSK) systems over Weibull fading channels, specifically by extending the RIS-aided SM/SSK system to a specific case of the conventional SM system. By employing the concept of neural networks, the study focuses on model-driven deep learning detection namely block deep neural networks (B-DNN) for RIS-aided SM systems and compares its performance against maximum likelihood (ML) and greedy detectors. Finally, it has been demonstrated by Monte Carlo simulation that while B-DNN achieved a bit error rate (BER) performance close to that of ML, it gave better results than the Greedy detector.
</details>
<details>
<summary>摘要</summary>
“弹性智能表面”（RIS）被视为未来无线通信技术的重要一环。近期，有许多研究将RIS与空间变化（SM）或空间移动键（SSK）结合以实现频率和能源效率的平衡。本研究使用深度学习技术进行RIS-aided SM/RSSK系统中的检测，具体是将传统SM系统扩展到RIS-aided SM系统。通过使用神经网络的概念，本研究专注于使用堆层神经网络（B-DNN）进行检测，并与最大可能性（ML）和探测器进行比较。最后，通过 Monte Carlo 模拟，发现B-DNN对比于ML的比较好，并且在比较探测器时表现更好。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Resource-Management-in-Integrated-NOMA-Terrestrial-Satellite-Networks-using-Multi-Agent-Reinforcement-Learning"><a href="#Dynamic-Resource-Management-in-Integrated-NOMA-Terrestrial-Satellite-Networks-using-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Dynamic Resource Management in Integrated NOMA Terrestrial-Satellite Networks using Multi-Agent Reinforcement Learning"></a>Dynamic Resource Management in Integrated NOMA Terrestrial-Satellite Networks using Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11814">http://arxiv.org/abs/2310.11814</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Nauman, Haya Mesfer Alshahrani, Nadhem Nemri, Kamal M. Othman, Nojood O Aljehane, Mashael Maashi, Ashit Kumar Dutta, Mohammed Assiri, Wali Ullah Khan</li>
<li>for: 这个研究旨在提出一个集成卫星-地面网络资源分配框架，以解决这些挑战。</li>
<li>methods: 该框架利用本地缓存池部署和非对称多Access（NOMA）技术来减少时间延迟和提高能效率。</li>
<li>results: 我们的提议使用了多代理搜索深度决策函数算法（MADDPG）优化用户关联、缓存设计和传输功率控制，从而提高能效率。Here’s the breakdown of each point in English:1. for: The paper is written to address the challenges of integrated satellite-terrestrial networks.2. methods: The paper proposes a resource allocation framework that leverages local cache pool deployments and non-orthogonal multiple access (NOMA) to reduce time delays and improve energy efficiency.3. results: The proposed approach using a multi-agent enabled deep deterministic policy gradient algorithm (MADDPG) achieves significantly higher energy efficiency and reduced time delays compared to existing methods.<details>
<summary>Abstract</summary>
This study introduces a resource allocation framework for integrated satellite-terrestrial networks to address these challenges. The framework leverages local cache pool deployments and non-orthogonal multiple access (NOMA) to reduce time delays and improve energy efficiency. Our proposed approach utilizes a multi-agent enabled deep deterministic policy gradient algorithm (MADDPG) to optimize user association, cache design, and transmission power control, resulting in enhanced energy efficiency. The approach comprises two phases: User Association and Power Control, where users are treated as agents, and Cache Optimization, where the satellite (Bs) is considered the agent. Through extensive simulations, we demonstrate that our approach surpasses conventional single-agent deep reinforcement learning algorithms in addressing cache design and resource allocation challenges in integrated terrestrial-satellite networks. Specifically, our proposed approach achieves significantly higher energy efficiency and reduced time delays compared to existing methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Random-Sampling-of-Bandlimited-Graph-Signals-from-Local-Measurements"><a href="#Random-Sampling-of-Bandlimited-Graph-Signals-from-Local-Measurements" class="headerlink" title="Random Sampling of Bandlimited Graph Signals from Local Measurements"></a>Random Sampling of Bandlimited Graph Signals from Local Measurements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11692">http://arxiv.org/abs/2310.11692</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lili Shen, Jun Xian, Cheng Cheng</li>
<li>for: 这篇论文研究了图像信号处理中随机抽样的问题，具体来说是对k-带限信号从本地测量中进行随机抽样，并证明了只需要O(klogk)次测量来精确和稳定地重建任何k-带限图像信号。</li>
<li>methods: 该论文提出了两种随机抽样策略，即最优抽样和估计抽样，其中采用了基于最小测量的抽样probability分布。</li>
<li>results:  numerical experiments表明了该方法的效果。<details>
<summary>Abstract</summary>
The random sampling on graph signals is one of the fundamental topics in graph signal processing. In this letter, we consider the random sampling of k-bandlimited signals from the local measurements and show that no more than O(klogk) measurements with replacement are sufficient for the accurate and stable recovery of any k-bandlimited graph signals. We propose two random sampling strategies based on the minimum measurements, i.e., the optimal sampling and the estimated sampling. The geodesic distance between vertices is introduced to design the sampling probability distribution. Numerical experiments are included to show the effectiveness of the proposed methods.
</details>
<details>
<summary>摘要</summary>
《随机抽取图像信号处理》是图像信号处理领域的基本主题之一。在本信，我们考虑了基于地方测量的随机抽取k-带限信号，并证明了只需要O(klogk)次抽取 measurements with replacement 可以准确地重建任何k-带限图像信号。我们提出了两种基于最小测量的随机抽取策略，即最优抽取和估计抽取。我们通过地odesic distance between vertices来设计抽取概率分布。numerical experiments 表明我们提出的方法的效果。Here's the word-for-word translation:“随机抽取图像信号处理”是图像信号处理领域的基本主题之一。在本信，我们考虑了基于地方测量的随机抽取k-带限信号，并证明了只需要O(klogk)次抽取 measurements with replacement 可以准确地重建任何k-带限图像信号。我们提出了两种基于最小测量的随机抽取策略，即最佳抽取和估计抽取。我们通过地odesic distance between vertices来设计抽取概率分布。numerical experiments 表明我们提出的方法的效果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/18/eess.SP_2023_10_18/" data-id="clogxf3u1019a5xra4g0u26jn" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_10_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/17/cs.SD_2023_10_17/" class="article-date">
  <time datetime="2023-10-17T15:00:00.000Z" itemprop="datePublished">2023-10-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/17/cs.SD_2023_10_17/">cs.SD - 2023-10-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="High-Fidelity-Noise-Reduction-with-Differentiable-Signal-Processing"><a href="#High-Fidelity-Noise-Reduction-with-Differentiable-Signal-Processing" class="headerlink" title="High-Fidelity Noise Reduction with Differentiable Signal Processing"></a>High-Fidelity Noise Reduction with Differentiable Signal Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11364">http://arxiv.org/abs/2310.11364</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian J. Steinmetz, Thomas Walther, Joshua D. Reiss</li>
<li>for: 这个论文主要是为了提高录音中的听音质量，使用深度学习和信号处理技术。</li>
<li>methods: 这个论文使用了深度学习模型和信号处理技术，将其结合起来实现自动化的听音质量提高。</li>
<li>results: 论文的实验结果表明，使用这种方法可以实现高精度的听音质量提高，并且比深度学习模型更高效和更少噪声。Listening examples are available online at <a target="_blank" rel="noopener" href="https://tape.it/research/denoiser%E3%80%82">https://tape.it/research/denoiser。</a><details>
<summary>Abstract</summary>
Noise reduction techniques based on deep learning have demonstrated impressive performance in enhancing the overall quality of recorded speech. While these approaches are highly performant, their application in audio engineering can be limited due to a number of factors. These include operation only on speech without support for music, lack of real-time capability, lack of interpretable control parameters, operation at lower sample rates, and a tendency to introduce artifacts. On the other hand, signal processing-based noise reduction algorithms offer fine-grained control and operation on a broad range of content, however, they often require manual operation to achieve the best results. To address the limitations of both approaches, in this work we introduce a method that leverages a signal processing-based denoiser that when combined with a neural network controller, enables fully automatic and high-fidelity noise reduction on both speech and music signals. We evaluate our proposed method with objective metrics and a perceptual listening test. Our evaluation reveals that speech enhancement models can be extended to music, however training the model to remove only stationary noise is critical. Furthermore, our proposed approach achieves performance on par with the deep learning models, while being significantly more efficient and introducing fewer artifacts in some cases. Listening examples are available online at https://tape.it/research/denoiser .
</details>
<details>
<summary>摘要</summary>
“深度学习减声技术已经在录音质量提高方面表现出色。然而，这些方法在音频工程中的应用可能受到一些限制因素。这些因素包括仅适用于语音，无法支持音乐，缺乏实时功能，缺乏可解释的控制参数，运行在较低的� Sampling rate 下，并且会引入错误。另一方面，信号处理减声算法可以提供精确的控制和适用于广泛的内容，但是它们通常需要手动操作以 дости持最佳结果。为了解决这两种方法的限制，在这个工作中，我们提出了一种结合信号处理减声器和神经网络控制器的方法，允许完全自动和高精度的杂声除去，包括语音和音乐信号。我们使用了一系列的对照测试和听觉测试进行评估。我们发现，语音提高模型可以扩展到音乐，但是培训模型只需要去除静止杂声是critical。此外，我们的提案方法可以和深度学习模型的性能相似，同时更高效和更少的错误。听取示例可以在 https://tape.it/research/denoiser 上找到。”
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Content-based-Features-from-Multiple-Acoustic-Models-for-Singing-Voice-Conversion"><a href="#Leveraging-Content-based-Features-from-Multiple-Acoustic-Models-for-Singing-Voice-Conversion" class="headerlink" title="Leveraging Content-based Features from Multiple Acoustic Models for Singing Voice Conversion"></a>Leveraging Content-based Features from Multiple Acoustic Models for Singing Voice Conversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11160">http://arxiv.org/abs/2310.11160</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueyao Zhang, Yicheng Gu, Haopeng Chen, Zihao Fang, Lexiao Zou, Liumeng Xue, Zhizheng Wu</li>
<li>for: 这项研究旨在 Investigating the complementary roles of multiple content features in singing voice conversion (SVC), and developing a diffusion-based SVC model that integrates these features for superior conversion performance.</li>
<li>methods: 研究使用了三种不同的内容特征（来自WeNet、Whisper和ContentVec），并将其集成到一个扩散基于SVC模型中，以提高SVC的对象和主观评估表现。</li>
<li>results: 研究表明，通过将多种内容特征集成到SVC模型中，可以获得更高的对象和主观评估表现，比单个内容特征更好。Code和demo页面可以在<a target="_blank" rel="noopener" href="https://www.zhangxueyao.com/data/MultipleContentsSVC/index.html%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://www.zhangxueyao.com/data/MultipleContentsSVC/index.html中找到。</a><details>
<summary>Abstract</summary>
Singing voice conversion (SVC) is a technique to enable an arbitrary singer to sing an arbitrary song. To achieve that, it is important to obtain speaker-agnostic representations from source audio, which is a challenging task. A common solution is to extract content-based features (e.g., PPGs) from a pretrained acoustic model. However, the choices for acoustic models are vast and varied. It is yet to be explored what characteristics of content features from different acoustic models are, and whether integrating multiple content features can help each other. Motivated by that, this study investigates three distinct content features, sourcing from WeNet, Whisper, and ContentVec, respectively. We explore their complementary roles in intelligibility, prosody, and conversion similarity for SVC. By integrating the multiple content features with a diffusion-based SVC model, our SVC system achieves superior conversion performance on both objective and subjective evaluation in comparison to a single source of content features. Our demo page and code can be available https://www.zhangxueyao.com/data/MultipleContentsSVC/index.html.
</details>
<details>
<summary>摘要</summary>
《歌唱voice转换（SVC）技术可以让任意歌手演唱任意歌曲。实现这一点具有挑战性，因为需要从源音频中提取无关于歌手的特征。常见的解决方案是从预训练的音声模型中提取内容基于特征（如PPGs）。然而，不同的音声模型可以提供不同的内容特征，是否可以将这些特征集成起来帮助彼此？以上问题驱动我们进行这种研究。本研究研究了三种不同的内容特征，分别来自WeNet、Whisper和ContentVec。我们研究了这些特征在elligibility、prosody和转换相似性方面的补充作用，并将这些特征集成到一个扩散型SVC模型中。在对象和主观评价中，我们的SVC系统在比较于单个内容特征时表现出更高的转换性能。我们的demo页和代码可以在<https://www.zhangxueyao.com/data/MultipleContentsSVC/index.html>上查看。》Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format instead.
</details></li>
</ul>
<hr>
<h2 id="A-High-Fidelity-and-Low-Complexity-Neural-Audio-Coding"><a href="#A-High-Fidelity-and-Low-Complexity-Neural-Audio-Coding" class="headerlink" title="A High Fidelity and Low Complexity Neural Audio Coding"></a>A High Fidelity and Low Complexity Neural Audio Coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10992">http://arxiv.org/abs/2310.10992</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenzhe Liu, Wei Xiao, Meng Wang, Shan Yang, Yupeng Shi, Yuyong Kang, Dan Su, Shidong Shang, Dong Yu</li>
<li>for: 这个论文旨在提出一个整合架构，用于实时通信系统中的音频编码。</li>
<li>methods: 这个架构使用神经网络模型宽频域成分，并使用传统信号处理技术压缩高频域成分，根据人类听觉知识设计了根据听觉理论的损失函数，同时还提出了基于对抗学习的各种数据压缩方法。</li>
<li>results: 该方法与先进的神经音频编码相比，在主观和客观指标上均表现出色，并且可以在桌面和手持设备上进行实时推断。<details>
<summary>Abstract</summary>
Audio coding is an essential module in the real-time communication system. Neural audio codecs can compress audio samples with a low bitrate due to the strong modeling and generative capabilities of deep neural networks. To address the poor high-frequency expression and high computational cost and storage consumption, we proposed an integrated framework that utilizes a neural network to model wide-band components and adopts traditional signal processing to compress high-band components according to psychological hearing knowledge. Inspired by auditory perception theory, a perception-based loss function is designed to improve harmonic modeling. Besides, generative adversarial network (GAN) compression is proposed for the first time for neural audio codecs. Our method is superior to prior advanced neural codecs across subjective and objective metrics and allows real-time inference on desktop and mobile.
</details>
<details>
<summary>摘要</summary>
Audio coding是实时通信系统中的一个重要模块。神经网络音频编码器可以通过深度神经网络的强大模型和生成能力，将音频样本压缩到低比特率。为了解决高频表达质量不佳和计算成本高的问题，我们提出了一个整合框架，利用神经网络模型宽频成分，采用传统的信号处理技术压缩高频成分，根据听觉知识。受听觉理论的启发，我们设计了基于听觉模型的损失函数，以改善和声模型。此外，我们还提出了基于生成敌对网络（GAN）的压缩方法，这是神经音频编码器中的首次应用。我们的方法在主观和客观指标上胜过先前的先进神经编码器，并允许实时推理在桌面和移动设备上。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/17/cs.SD_2023_10_17/" data-id="clogxf3qt00wr5xra0uo42qow" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_10_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/17/eess.AS_2023_10_17/" class="article-date">
  <time datetime="2023-10-17T14:00:00.000Z" itemprop="datePublished">2023-10-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/17/eess.AS_2023_10_17/">eess.AS - 2023-10-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Iterative-Shallow-Fusion-of-Backward-Language-Model-for-End-to-End-Speech-Recognition"><a href="#Iterative-Shallow-Fusion-of-Backward-Language-Model-for-End-to-End-Speech-Recognition" class="headerlink" title="Iterative Shallow Fusion of Backward Language Model for End-to-End Speech Recognition"></a>Iterative Shallow Fusion of Backward Language Model for End-to-End Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11010">http://arxiv.org/abs/2310.11010</a></li>
<li>repo_url: None</li>
<li>paper_authors: Atsunori Ogawa, Takafumi Moriya, Naoyuki Kamo, Naohiro Tawara, Marc Delcroix</li>
<li>for: 这 paper 的目的是提出一种新的浅融合（SF）方法，以利用外部的反向语言模型（BLM）来实现端到端自动语音识别（ASR）系统。</li>
<li>methods: 这 paper 使用的方法包括：（1）使用 BLM 对 ASR  гипотезы进行迭代处理，以取代上一轮计算的 BLM 分数；（2）使用 PBLM 进行部分句子预测，以提高 ISF 的效果。</li>
<li>results: 实验结果表明，使用 ISF 可以在 ASR 系统中提高性能，并且可以避免在解码过程中提前剔除可能的 гипотезы。此外，将 SF 和 ISF 相互结合可以获得更高的性能提升。<details>
<summary>Abstract</summary>
We propose a new shallow fusion (SF) method to exploit an external backward language model (BLM) for end-to-end automatic speech recognition (ASR). The BLM has complementary characteristics with a forward language model (FLM), and the effectiveness of their combination has been confirmed by rescoring ASR hypotheses as post-processing. In the proposed SF, we iteratively apply the BLM to partial ASR hypotheses in the backward direction (i.e., from the possible next token to the start symbol) during decoding, substituting the newly calculated BLM scores for the scores calculated at the last iteration. To enhance the effectiveness of this iterative SF (ISF), we train a partial sentence-aware BLM (PBLM) using reversed text data including partial sentences, considering the framework of ISF. In experiments using an attention-based encoder-decoder ASR system, we confirmed that ISF using the PBLM shows comparable performance with SF using the FLM. By performing ISF, early pruning of prospective hypotheses can be prevented during decoding, and we can obtain a performance improvement compared to applying the PBLM as post-processing. Finally, we confirmed that, by combining SF and ISF, further performance improvement can be obtained thanks to the complementarity of the FLM and PBLM.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的浅合并（SF）方法，利用外部的反向语言模型（BLM）来实现端到端自动语音识别（ASR）。BLM具有与前向语言模型（FLM）的 complementary 特性，其合作效果已经通过重新评分ASR假设来确认。在我们的SF中，我们在解码过程中逐渐应用BLM于部分ASR假设，在反向方向（即从可能的下一个单词到开始符）进行迭代，并将每轮计算的BLM分数替换为上一轮计算的分数。为了增强ISF的效果，我们使用了倒转文本数据来训练一个具有部分句子意识的BLM（PBLM）。在使用了注意力基于encoder-decoder ASR系统的实验中，我们证明了ISF使用PBLM可以与SF使用FLM相比。通过执行ISF，在解码过程中可以避免早期淘汰可能的假设，从而获得性能提升。最后，我们证明了，通过将SF和ISF结合使用，可以增加性能的提升，这是因为FLM和PBLM之间存在 complementarity。
</details></li>
</ul>
<hr>
<h2 id="Advanced-accent-dialect-identification-and-accentedness-assessment-with-multi-embedding-models-and-automatic-speech-recognition"><a href="#Advanced-accent-dialect-identification-and-accentedness-assessment-with-multi-embedding-models-and-automatic-speech-recognition" class="headerlink" title="Advanced accent&#x2F;dialect identification and accentedness assessment with multi-embedding models and automatic speech recognition"></a>Advanced accent&#x2F;dialect identification and accentedness assessment with multi-embedding models and automatic speech recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11004">http://arxiv.org/abs/2310.11004</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shahram Ghorbani, John H. L. Hansen<br>for:* 这个研究旨在提高非Native语言speech中的腔调识别和外国腔评估的准确性。methods:* 利用先进的预训练语言标识(LID)和说话人标识(SID)模型的嵌入，以提高非Native语言speech中腔调识别和外国腔评估的准确性。results:* 结果表明，使用预训练LID和SID模型的嵌入可以有效地编码非Native语言speech中的腔调信息。* 此外，LID和SID编码的腔调信息与从scratch开发的端到端腔调识别模型结合使用，可以提高腔调识别的准确性。<details>
<summary>Abstract</summary>
Accurately classifying accents and assessing accentedness in non-native speakers are both challenging tasks due to the complexity and diversity of accent and dialect variations. In this study, embeddings from advanced pre-trained language identification (LID) and speaker identification (SID) models are leveraged to improve the accuracy of accent classification and non-native accentedness assessment. Findings demonstrate that employing pre-trained LID and SID models effectively encodes accent/dialect information in speech. Furthermore, the LID and SID encoded accent information complement an end-to-end accent identification (AID) model trained from scratch. By incorporating all three embeddings, the proposed multi-embedding AID system achieves superior accuracy in accent identification. Next, we investigate leveraging automatic speech recognition (ASR) and accent identification models to explore accentedness estimation. The ASR model is an end-to-end connectionist temporal classification (CTC) model trained exclusively with en-US utterances. The ASR error rate and en-US output of the AID model are leveraged as objective accentedness scores. Evaluation results demonstrate a strong correlation between the scores estimated by the two models. Additionally, a robust correlation between the objective accentedness scores and subjective scores based on human perception is demonstrated, providing evidence for the reliability and validity of utilizing AID-based and ASR-based systems for accentedness assessment in non-native speech.
</details>
<details>
<summary>摘要</summary>
准确地分类不同的口音和讲话风格是一项非常复杂和多样化的任务，特别是在非Native speaker的语音中。在本研究中，我们利用先进的预训练语言标识（LID）和发音标识（SID）模型的嵌入来提高非Native speaker的口音分类和讲话风格评估的准确性。研究发现，使用预训练LID和SID模型可以有效地嵌入语音中的口音/方言信息。此外，LID和SID嵌入的口音信息与从零开始训练的口音标识（AID）模型相结合，可以提高口音标识的准确性。然后，我们 investigate了利用自然语音识别（ASR）和口音标识模型来评估讲话风格。ASR模型是一个端到端的连接式时间分类（CTC）模型，专门使用英文语音训练。ASR错误率和AID模型的en-US输出被用作对象评估风格的标准差分。研究结果表明，两个模型之间存在强相关性，并且对人类对讲话风格的评估也存在robust相关性，这提供了使用AID和ASR基于的系统进行讲话风格评估的可靠性和有效性的证据。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/17/eess.AS_2023_10_17/" data-id="clogxf3rj00zt5xraekdcf17k" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_10_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/17/cs.CV_2023_10_17/" class="article-date">
  <time datetime="2023-10-17T13:00:00.000Z" itemprop="datePublished">2023-10-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/17/cs.CV_2023_10_17/">cs.CV - 2023-10-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Holistic-Parking-Slot-Detection-with-Polygon-Shaped-Representations"><a href="#Holistic-Parking-Slot-Detection-with-Polygon-Shaped-Representations" class="headerlink" title="Holistic Parking Slot Detection with Polygon-Shaped Representations"></a>Holistic Parking Slot Detection with Polygon-Shaped Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11629">http://arxiv.org/abs/2310.11629</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lihao Wang, Antonyo Musabini, Christel Leonet, Rachid Benmokhtar, Amaury Breheret, Chaima Yedes, Fabian Burger, Thomas Boulay, Xavier Perrotton<br>for:This paper proposes a one-step Holistic Parking Slot Network (HPS-Net) to detect vacant parking slots using a camera-based approach.methods:The proposed method uses a tailor-made adaptation of the You Only Look Once (YOLO)v4 algorithm, which directly outputs the four vertex coordinates of the parking slot in topview domain. A novel regression loss function named polygon-corner Generalized Intersection over Union (GIoU) is proposed to optimize the polygon vertex position and distinguish the entrance line.results:Experiments show that HPS-Net can detect various vacant parking slots with a F1-score of 0.92 on the internal Valeo Parking Slots Dataset (VPSD) and 0.99 on the public dataset PS2.0. The method achieves a satisfying generalization and robustness in various parking scenarios, such as indoor or paved ground, with a real-time detection speed of 17 FPS on Nvidia Drive AGX Xavier.<details>
<summary>Abstract</summary>
Current parking slot detection in advanced driver-assistance systems (ADAS) primarily relies on ultrasonic sensors. This method has several limitations such as the need to scan the entire parking slot before detecting it, the incapacity of detecting multiple slots in a row, and the difficulty of classifying them. Due to the complex visual environment, vehicles are equipped with surround view camera systems to detect vacant parking slots. Previous research works in this field mostly use image-domain models to solve the problem. These two-stage approaches separate the 2D detection and 3D pose estimation steps using camera calibration. In this paper, we propose one-step Holistic Parking Slot Network (HPS-Net), a tailor-made adaptation of the You Only Look Once (YOLO)v4 algorithm. This camera-based approach directly outputs the four vertex coordinates of the parking slot in topview domain, instead of a bounding box in raw camera images. Several visible points and shapes can be proposed from different angles. A novel regression loss function named polygon-corner Generalized Intersection over Union (GIoU) for polygon vertex position optimization is also proposed to manage the slot orientation and to distinguish the entrance line. Experiments show that HPS-Net can detect various vacant parking slots with a F1-score of 0.92 on our internal Valeo Parking Slots Dataset (VPSD) and 0.99 on the public dataset PS2.0. It provides a satisfying generalization and robustness in various parking scenarios, such as indoor (F1: 0.86) or paved ground (F1: 0.91). Moreover, it achieves a real-time detection speed of 17 FPS on Nvidia Drive AGX Xavier. A demo video can be found at https://streamable.com/75j7sj.
</details>
<details>
<summary>摘要</summary>
当前的停车槽检测在高级驾驶支持系统（ADAS）主要依靠 ultrasonic 探测器。这种方法有多个限制，包括需要扫描整个停车槽才能检测它，无法检测多个槽在一行，以及困难的分类问题。由于视觉环境复杂，车辆通常配备卫星视频摄像头系统以检测空闲停车槽。先前的研究工作主要使用图像领域模型解决这个问题。这些两个阶段方法在摄像头准确性上分解了2D检测和3D姿态估计步骤。在本文中，我们提出了一步骤Holistic Parking Slot Network（HPS-Net），这是基于 You Only Look Once（YOLO）v4算法的修改版。这种摄像头基本方法直接输出了四个顶点坐标，而不是Raw摄像头图像中的 bounding box。通过不同的角度可以提出多个可见的点和形状。我们还提出了一种新的 regression loss function，即 polygon-corner Generalized Intersection over Union（GIoU），用于优化额外坐标和分辨入口线。实验表明，HPS-Net可以准确地检测多种空闲停车槽，F1 分数为 0.92 在我们的内部 valeo 停车槽数据集（VPSD）上，并达到 0.99 在公共数据集 PS2.0 上。它在多种停车场景中具有满意的总体化和稳定性，例如室内（F1: 0.86）或沥青地（F1: 0.91）。此外，它在 Nvidia Drive AGX Xavier 上实现了实时检测速度为 17 FPS。更多的详细信息可以在 <https://streamable.com/75j7sj> 找到。
</details></li>
</ul>
<hr>
<h2 id="High-Resolution-Building-and-Road-Detection-from-Sentinel-2"><a href="#High-Resolution-Building-and-Road-Detection-from-Sentinel-2" class="headerlink" title="High-Resolution Building and Road Detection from Sentinel-2"></a>High-Resolution Building and Road Detection from Sentinel-2</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11622">http://arxiv.org/abs/2310.11622</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/RudraxDave/UrbanizationDetection_RoadnBuilding">https://github.com/RudraxDave/UrbanizationDetection_RoadnBuilding</a></li>
<li>paper_authors: Wojciech Sirko, Emmanuel Asiedu Brempong, Juliana T. C. Marcos, Abigail Annkah, Abel Korme, Mohammed Alewi Hassen, Krishna Sapkota, Tomer Shekel, Abdoulaye Diack, Sella Nevo, Jason Hickey, John Quinn</li>
<li>for: This paper demonstrates how to use multiple 10m resolution Sentinel-2 images to generate 50cm resolution building and road segmentation masks.</li>
<li>methods: The authors use a “student” model to reproduce the predictions of a “teacher” model, which has access to high-resolution imagery.</li>
<li>results: The authors achieve 78.3% mIoU for building segmentation and R^2 &#x3D; 0.91 for counting individual buildings, which are comparable to the performance of the high-resolution teacher model.Here is the simplified Chinese version of the three key points:</li>
<li>for: 这篇论文用多个10米分辨率的卫星图像生成50厘米分辨率的建筑和公路分割mask。</li>
<li>methods: 作者使用一个”学生”模型来重现一个”教师”模型的预测，该模型有访问高分辨率卫星图像的能力。</li>
<li>results: 作者在建筑分割方面 achievement 78.3% mIoU，与高分辨率教师模型的性能相似。同时，对个别建筑物的计数也达到R^2 &#x3D; 0.91。<details>
<summary>Abstract</summary>
Mapping buildings and roads automatically with remote sensing typically requires high-resolution imagery, which is expensive to obtain and often sparsely available. In this work we demonstrate how multiple 10 m resolution Sentinel-2 images can be used to generate 50 cm resolution building and road segmentation masks. This is done by training a `student' model with access to Sentinel-2 images to reproduce the predictions of a `teacher' model which has access to corresponding high-resolution imagery. While the predictions do not have all the fine detail of the teacher model, we find that we are able to retain much of the performance: for building segmentation we achieve 78.3% mIoU, compared to the high-resolution teacher model accuracy of 85.3% mIoU. We also describe a related method for counting individual buildings in a Sentinel-2 patch which achieves R^2 = 0.91 against true counts. This work opens up new possibilities for using freely available Sentinel-2 imagery for a range of tasks that previously could only be done with high-resolution satellite imagery.
</details>
<details>
<summary>摘要</summary>
使用远程感知自动地图建模通常需要高分辨率图像，这些图像昂贵并且 sparse 地available。在这项工作中，我们展示了如何使用多张10米分辨率 Sentine-2 图像生成 50 cm 分辨率的建筑和路径分割mask。这是通过训练一个“学生”模型，该模型有访问 Sentine-2 图像，来复制“教师”模型，该模型有访问相应的高分辨率图像的预测。虽然预测没有 teacher 模型的细节，但我们发现可以保留大量的性能：对建筑分割，我们 achieve 78.3% mIoU，比高分辨率教师模型的准确率 85.3% mIoU。我们还描述了一种与此相关的方法，可以在 Sentinel-2 质patch 中计数个建筑，该方法 achieve R^2 = 0.91 与真实计数的相关性。这项工作开 up 了使用免费available Sentinel-2 图像进行许多任务的新可能性，这些任务在过去只能通过高分辨率卫星图像进行。
</details></li>
</ul>
<hr>
<h2 id="Classification-of-Safety-Driver-Attention-During-Autonomous-Vehicle-Operation"><a href="#Classification-of-Safety-Driver-Attention-During-Autonomous-Vehicle-Operation" class="headerlink" title="Classification of Safety Driver Attention During Autonomous Vehicle Operation"></a>Classification of Safety Driver Attention During Autonomous Vehicle Operation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11608">http://arxiv.org/abs/2310.11608</a></li>
<li>repo_url: None</li>
<li>paper_authors: Santiago Gerling Konrad, Julie Stephany Berrio, Mao Shan, Favio Masson, Stewart Worrall</li>
<li>For: This paper aims to develop a system to monitor the alertness of vehicle operators in ADAS and AVs to ensure safe operation.* Methods: The proposed system uses an infrared camera to detect the driver’s head and calculate head orientation, and incorporates environmental data from the perception system to determine the driver’s attention to objects in the surroundings.* Results: The system was tested using data collected in Sydney, Australia, and was found to effectively determine the vehicle operator’s attention levels, enabling interventions such as warnings or reducing autonomous functionality as appropriate.Here is the same information in Simplified Chinese text:* For: 这篇论文目标是为ADAS和AV开发一种监测车辆操作员的关注度，以确保安全操作。* Methods: 该系统使用红外摄像头探测司机头部，计算头部方向，并在环境感知系统的数据支持下，判断司机是否注意到周围环境中的物体。* Results: 该系统在澳大利亚悉尼的数据集上进行了测试，并被证明可以有效地确定车辆操作员的关注度，并发出警示或减少自动化功能等措施。<details>
<summary>Abstract</summary>
Despite the continual advances in Advanced Driver Assistance Systems (ADAS) and the development of high-level autonomous vehicles (AV), there is a general consensus that for the short to medium term, there is a requirement for a human supervisor to handle the edge cases that inevitably arise. Given this requirement, it is essential that the state of the vehicle operator is monitored to ensure they are contributing to the vehicle's safe operation. This paper introduces a dual-source approach integrating data from an infrared camera facing the vehicle operator and vehicle perception systems to produce a metric for driver alertness in order to promote and ensure safe operator behaviour. The infrared camera detects the driver's head, enabling the calculation of head orientation, which is relevant as the head typically moves according to the individual's focus of attention. By incorporating environmental data from the perception system, it becomes possible to determine whether the vehicle operator observes objects in the surroundings. Experiments were conducted using data collected in Sydney, Australia, simulating AV operations in an urban environment. Our results demonstrate that the proposed system effectively determines a metric for the attention levels of the vehicle operator, enabling interventions such as warnings or reducing autonomous functionality as appropriate. This comprehensive solution shows promise in contributing to ADAS and AVs' overall safety and efficiency in a real-world setting.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:尽管现代驾驶助手系统（ADAS）和高级无人驾驶车辆（AV）在不断发展，但是有一致的共识，在短到中期，需要有人监督处理边缘情况。由于这一点，监控车辆运行者的状态非常重要。这篇文章介绍了一种将 инфракра力相机和车辆感知系统相结合的双源方法，以计算驾驶员注意力水平，以便促进和确保安全的操作行为。这种方法可以检测驾驶员的头部，并计算头部的方向，因为头部通常会根据个人的注意力方向移动。通过与环境数据集成，可以判断车辆运行者是否观察了周围环境。我们在悉尼、澳大利亚进行了实验，使用了在城市环境中采集的数据，模拟了AV的运行。我们的结果表明，该系统可以有效地计算驾驶员注意力水平，并发出警告或降低自动化功能，以适应实际情况。这种全面的解决方案显示了它在ADAS和AV的安全和效率方面的承诺。
</details></li>
</ul>
<hr>
<h2 id="DIAR-Deep-Image-Alignment-and-Reconstruction-using-Swin-Transformers"><a href="#DIAR-Deep-Image-Alignment-and-Reconstruction-using-Swin-Transformers" class="headerlink" title="DIAR: Deep Image Alignment and Reconstruction using Swin Transformers"></a>DIAR: Deep Image Alignment and Reconstruction using Swin Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11605">http://arxiv.org/abs/2310.11605</a></li>
<li>repo_url: None</li>
<li>paper_authors: Monika Kwiatkowski, Simon Matern, Olaf Hellwich</li>
<li>for: 这个论文旨在建立一个深度学习管道，用于同时对扭曲图像进行Alignment和重建。</li>
<li>methods: 论文使用了Swin transformer模型，并使用了自定义的图像扭曲 dataset，以及图像特征地图来检测图像内容和排除噪声。</li>
<li>results: 论文通过使用图像特征地图和深度学习模型，实现了对扭曲图像的高精度Alignment和重建。<details>
<summary>Abstract</summary>
When taking images of some occluded content, one is often faced with the problem that every individual image frame contains unwanted artifacts, but a collection of images contains all relevant information if properly aligned and aggregated. In this paper, we attempt to build a deep learning pipeline that simultaneously aligns a sequence of distorted images and reconstructs them. We create a dataset that contains images with image distortions, such as lighting, specularities, shadows, and occlusion. We create perspective distortions with corresponding ground-truth homographies as labels. We use our dataset to train Swin transformer models to analyze sequential image data. The attention maps enable the model to detect relevant image content and differentiate it from outliers and artifacts. We further explore using neural feature maps as alternatives to classical key point detectors. The feature maps of trained convolutional layers provide dense image descriptors that can be used to find point correspondences between images. We utilize this to compute coarse image alignments and explore its limitations.
</details>
<details>
<summary>摘要</summary>
当拍摄部分受遮挡内容时，常常会遇到每帧图像中存在不必要的artefacts问题，但是一系列图像中会包含所有相关信息，如果正确地对齐和汇总。在这篇论文中，我们尝试建立一个深度学习管道，同时对扭曲图像序列进行对齐和重建。我们创建了一个包含图像扭曲效果，如光照、 Specularities、阴影和遮挡的 dataset。我们创建了对应的地平线扭曲和真实的homographies作为标签。我们使用这些 dataset 来训练 Swin transformer 模型分析序列图像数据。模型的注意力地图使得模型可以检测图像中相关的内容，并将其与异常值和artefacts分开。我们进一步 explore 使用神经网络特征图作为经典关键点检测器的代替。训练 convolutional 层的神经网络特征图提供了密集图像描述符，可以用于找到图像之间的点对应关系。我们利用这个技术来计算粗略图像对齐，并探讨其局限性。
</details></li>
</ul>
<hr>
<h2 id="Learning-Neural-Implicit-through-Volume-Rendering-with-Attentive-Depth-Fusion-Priors"><a href="#Learning-Neural-Implicit-through-Volume-Rendering-with-Attentive-Depth-Fusion-Priors" class="headerlink" title="Learning Neural Implicit through Volume Rendering with Attentive Depth Fusion Priors"></a>Learning Neural Implicit through Volume Rendering with Attentive Depth Fusion Priors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11598">http://arxiv.org/abs/2310.11598</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MachinePerceptionLab/Attentive_DFPrior">https://github.com/MachinePerceptionLab/Attentive_DFPrior</a></li>
<li>paper_authors: Pengchong Hu, Zhizhong Han</li>
<li>for: 本研究旨在提高基于多视图图像的3D重建精度，使用神经网络学习隐式表示，并通过粘性深度融合优先来解决 incomplete depth at holes 和 occluded structures 问题。</li>
<li>methods: 我们提出了一种基于多视图RGBD图像的神经网络学习隐式表示方法，通过粘性深度融合优先来解决 incomplete depth at holes 和 occluded structures 问题。我们引入了一种novel attention mechanism，允许神经网络直接使用 depth fusion prior 来学习隐式函数。</li>
<li>results: 我们的方法在 widely used benchmarks 上表现出色，超过了 latest neural implicit methods。我们的实验结果表明，我们的方法可以更好地处理 incomplete depth at holes 和 occluded structures 问题，提高了3D重建精度。<details>
<summary>Abstract</summary>
Learning neural implicit representations has achieved remarkable performance in 3D reconstruction from multi-view images. Current methods use volume rendering to render implicit representations into either RGB or depth images that are supervised by multi-view ground truth. However, rendering a view each time suffers from incomplete depth at holes and unawareness of occluded structures from the depth supervision, which severely affects the accuracy of geometry inference via volume rendering. To resolve this issue, we propose to learn neural implicit representations from multi-view RGBD images through volume rendering with an attentive depth fusion prior. Our prior allows neural networks to perceive coarse 3D structures from the Truncated Signed Distance Function (TSDF) fused from all depth images available for rendering. The TSDF enables accessing the missing depth at holes on one depth image and the occluded parts that are invisible from the current view. By introducing a novel attention mechanism, we allow neural networks to directly use the depth fusion prior with the inferred occupancy as the learned implicit function. Our attention mechanism works with either a one-time fused TSDF that represents a whole scene or an incrementally fused TSDF that represents a partial scene in the context of Simultaneous Localization and Mapping (SLAM). Our evaluations on widely used benchmarks including synthetic and real-world scans show our superiority over the latest neural implicit methods. Project page: https://machineperceptionlab.github.io/Attentive_DF_Prior/
</details>
<details>
<summary>摘要</summary>
学习神经隐式表示法已经在多视图图像中实现了很好的3D重建性能。现有的方法使用volume rendering将隐式表示渲染成RGB或深度图像，并且通过多视图ground truth进行超视图指导。然而，每次渲染一个视图会导致部分深度信息缺失和 occluded 结构的不可见性，这会对geometry inference via volume rendering产生严重的影响。为解决这个问题，我们提议通过多视图RGBD图像学习神经隐式表示法，并通过注意力机制将神经网络让掌握TSDF（Truncated Signed Distance Function）的粗略3D结构。TSDF允许访问多视图图像中缺失的深度信息和 occluded 结构，从而提高geometry inference的准确性。我们的注意力机制可以同时使用一次拼接的TSDF或者逐渐拼接的TSDF，它们在Simultaneous Localization and Mapping（SLAM）上下文中表示整个场景或者部分场景。我们的评估结果表明，我们的方法在广泛使用的标准benchmark上表现出色，超过了最新的神经隐式方法。项目页面：https://machineperceptionlab.github.io/Attentive_DF_Prior/
</details></li>
</ul>
<hr>
<h2 id="Studying-the-Effects-of-Sex-related-Differences-on-Brain-Age-Prediction-using-brain-MR-Imaging"><a href="#Studying-the-Effects-of-Sex-related-Differences-on-Brain-Age-Prediction-using-brain-MR-Imaging" class="headerlink" title="Studying the Effects of Sex-related Differences on Brain Age Prediction using brain MR Imaging"></a>Studying the Effects of Sex-related Differences on Brain Age Prediction using brain MR Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11577">http://arxiv.org/abs/2310.11577</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahsa Dibaji, Neha Gianchandani, Akhil Nair, Mansi Singhal, Roberto Souza, Mariana Bento</li>
<li>for: 这个论文的目的是研究机器学习模型在不同性别人群中的偏见和公平问题。</li>
<li>methods: 作者使用了基于大脑磁共振成像数据的机器学习模型，并在不同的实验设计下进行了训练和评估。</li>
<li>results: 研究发现在不同性别人群和数据集上训练模型时，模型的性能存在差异，并且偏见可能导致模型在不同性别人群中的决策不具有公平性。<details>
<summary>Abstract</summary>
While utilizing machine learning models, one of the most crucial aspects is how bias and fairness affect model outcomes for diverse demographics. This becomes especially relevant in the context of machine learning for medical imaging applications as these models are increasingly being used for diagnosis and treatment planning. In this paper, we study biases related to sex when developing a machine learning model based on brain magnetic resonance images (MRI). We investigate the effects of sex by performing brain age prediction considering different experimental designs: model trained using only female subjects, only male subjects and a balanced dataset. We also perform evaluation on multiple MRI datasets (Calgary-Campinas(CC359) and CamCAN) to assess the generalization capability of the proposed models. We found disparities in the performance of brain age prediction models when trained on distinct sex subgroups and datasets, in both final predictions and decision making (assessed using interpretability models). Our results demonstrated variations in model generalizability across sex-specific subgroups, suggesting potential biases in models trained on unbalanced datasets. This underlines the critical role of careful experimental design in generating fair and reliable outcomes.
</details>
<details>
<summary>摘要</summary>
在使用机器学习模型时，最重要的一点是如何处理偏见和公平问题，以确保模型对不同的民族团体都有正确的预测结果。在医疗领域机器学习应用中，这些模型正在被用于诊断和治疗规划。在这篇论文中，我们研究了基于大脑磁共振成像（MRI）的机器学习模型中的性偏见。我们对不同实验设计进行了研究：使用只有女性参与者的模型、只有男性参与者的模型以及平衡 dataset。我们还对多个 MRI 数据集（Calgary-Campinas（CC359）和 CamCAN）进行了评估，以评估提议的模型在不同的数据集上的普适性。我们发现在不同的性 subgroup 和数据集上训练的模型表现出了差异， both final predictions 和决策（使用可解释模型进行评估）。我们的结果表明了不同的性团体中模型的一致性不同，这表明了训练在不均衡数据集上的模型可能存在偏见。这些结果提醒我们在设计实验时应该非常小心，以确保获得公平和可靠的结果。
</details></li>
</ul>
<hr>
<h2 id="Learning-Lens-Blur-Fields"><a href="#Learning-Lens-Blur-Fields" class="headerlink" title="Learning Lens Blur Fields"></a>Learning Lens Blur Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11535">http://arxiv.org/abs/2310.11535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Esther Y. H. Lin, Zhecheng Wang, Rebecca Lin, Daniel Miau, Florian Kainz, Jiawen Chen, Xuaner Cecilia Zhang, David B. Lindell, Kiriakos N. Kutulakos</li>
<li>for: The paper is written to address the challenge of modeling optical blur in modern cameras with complex optical elements, and to introduce a high-dimensional neural representation of the blur field.</li>
<li>methods: The paper proposes a practical method for acquiring the lens blur field, which is a multilayer perceptron (MLP) designed to capture variations of the lens 2D point spread function over image plane location, focus setting, and depth. The representation models the combined effects of defocus, diffraction, aberration, and accounts for sensor features such as pixel color filters and pixel-specific micro-lenses.</li>
<li>results: The paper shows that the acquired 5D blur fields are expressive and accurate enough to reveal differences in optical behavior of smartphone devices of the same make and model, and provides a first-of-its-kind dataset of 5D blur fields for smartphone cameras, camera bodies equipped with a variety of lenses, etc.<details>
<summary>Abstract</summary>
Optical blur is an inherent property of any lens system and is challenging to model in modern cameras because of their complex optical elements. To tackle this challenge, we introduce a high-dimensional neural representation of blur$-$$\textit{the lens blur field}$$-$and a practical method for acquiring it. The lens blur field is a multilayer perceptron (MLP) designed to (1) accurately capture variations of the lens 2D point spread function over image plane location, focus setting and, optionally, depth and (2) represent these variations parametrically as a single, sensor-specific function. The representation models the combined effects of defocus, diffraction, aberration, and accounts for sensor features such as pixel color filters and pixel-specific micro-lenses. To learn the real-world blur field of a given device, we formulate a generalized non-blind deconvolution problem that directly optimizes the MLP weights using a small set of focal stacks as the only input. We also provide a first-of-its-kind dataset of 5D blur fields$-$for smartphone cameras, camera bodies equipped with a variety of lenses, etc. Lastly, we show that acquired 5D blur fields are expressive and accurate enough to reveal, for the first time, differences in optical behavior of smartphone devices of the same make and model.
</details>
<details>
<summary>摘要</summary>
“光学朦胧是任何镜系统的自然属性，现代摄像机中模型它具有复杂的光学元件的挑战。为了解决这个挑战，我们介绍了一种高维度神经网络表示朦胧$-$镜片朦胧场$-$以及一种实用的获取方法。镜片朦胧场是一种多层感知网络（MLP），旨在：(1) 精确捕捉镜片2D点扩散函数在图像平面位置、焦距设置和（选择）深度上的变化，以及(2) 表示这些变化为单个、设备特定的函数。该表示模型了杂光、折射、笛卡尔等效应，并考虑了感器特性 such as 像素色滤和像素特定的微镜。为了学习具体的朦胧场，我们提出了一种通用非盲目分解问题，直接优化MLP参量使用一小组焦距栈作为输入。此外，我们还提供了一个具有5D朦胧场的首个数据集$-$包括智能手机摄像机、配备多种镜头的相机机身等。最后，我们证明了获取的5D朦胧场是具有表达力和准确性的，可以折衣出智能手机设备的同类型和型号之间的光学行为差异。”
</details></li>
</ul>
<hr>
<h2 id="GenEval-An-Object-Focused-Framework-for-Evaluating-Text-to-Image-Alignment"><a href="#GenEval-An-Object-Focused-Framework-for-Evaluating-Text-to-Image-Alignment" class="headerlink" title="GenEval: An Object-Focused Framework for Evaluating Text-to-Image Alignment"></a>GenEval: An Object-Focused Framework for Evaluating Text-to-Image Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11513">http://arxiv.org/abs/2310.11513</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djghosh13/geneval">https://github.com/djghosh13/geneval</a></li>
<li>paper_authors: Dhruba Ghosh, Hanna Hajishirzi, Ludwig Schmidt</li>
<li>for: 评估文本到图像生成模型的详细特性，如物体共处、位置、数量和颜色等。</li>
<li>methods: 利用现有的 объек检测模型来评估文本到图像模型的多种生成任务，并通过链接其他探索视觉模型来进一步验证特性。</li>
<li>results: 研究发现，现有的文本到图像模型在这些任务上已经显示出了显著的进步，但还缺乏复杂的能力，如空间关系和属性绑定。<details>
<summary>Abstract</summary>
Recent breakthroughs in diffusion models, multimodal pretraining, and efficient finetuning have led to an explosion of text-to-image generative models. Given human evaluation is expensive and difficult to scale, automated methods are critical for evaluating the increasingly large number of new models. However, most current automated evaluation metrics like FID or CLIPScore only offer a holistic measure of image quality or image-text alignment, and are unsuited for fine-grained or instance-level analysis. In this paper, we introduce GenEval, an object-focused framework to evaluate compositional image properties such as object co-occurrence, position, count, and color. We show that current object detection models can be leveraged to evaluate text-to-image models on a variety of generation tasks with strong human agreement, and that other discriminative vision models can be linked to this pipeline to further verify properties like object color. We then evaluate several open-source text-to-image models and analyze their relative generative capabilities on our benchmark. We find that recent models demonstrate significant improvement on these tasks, though they are still lacking in complex capabilities such as spatial relations and attribute binding. Finally, we demonstrate how GenEval might be used to help discover existing failure modes, in order to inform development of the next generation of text-to-image models. Our code to run the GenEval framework is publicly available at https://github.com/djghosh13/geneval.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DELIFFAS-Deformable-Light-Fields-for-Fast-Avatar-Synthesis"><a href="#DELIFFAS-Deformable-Light-Fields-for-Fast-Avatar-Synthesis" class="headerlink" title="DELIFFAS: Deformable Light Fields for Fast Avatar Synthesis"></a>DELIFFAS: Deformable Light Fields for Fast Avatar Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11449">http://arxiv.org/abs/2310.11449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youngjoong Kwon, Lingjie Liu, Henry Fuchs, Marc Habermann, Christian Theobalt</li>
<li>for: 生成高品质的数字人像，解决长期存在的图形和视觉领域中的问题。</li>
<li>methods: 提出了一种新的方法，即DELIFFAS，它将人体的外观 Parameterized as a surface light field that is attached to a controllable and deforming human mesh model。</li>
<li>results: 通过 méticulously designed human representation and supervision strategy，实现了 state-of-the-art synthesis results和快速的推理时间。<details>
<summary>Abstract</summary>
Generating controllable and photorealistic digital human avatars is a long-standing and important problem in Vision and Graphics. Recent methods have shown great progress in terms of either photorealism or inference speed while the combination of the two desired properties still remains unsolved. To this end, we propose a novel method, called DELIFFAS, which parameterizes the appearance of the human as a surface light field that is attached to a controllable and deforming human mesh model. At the core, we represent the light field around the human with a deformable two-surface parameterization, which enables fast and accurate inference of the human appearance. This allows perceptual supervision on the full image compared to previous approaches that could only supervise individual pixels or small patches due to their slow runtime. Our carefully designed human representation and supervision strategy leads to state-of-the-art synthesis results and inference time. The video results and code are available at https://vcai.mpi-inf.mpg.de/projects/DELIFFAS.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将生成控制可靠、渲染实际的数字人类头像作为视图和图形领域的长期问题。最近的方法在一个或两个属性中都已经取得了很大的进步，但是同时拥有这两种属性仍然是一个未解决的问题。为此，我们提出了一种新的方法，称为DELIFFAS，它将人类的外观 parameterized为附着在可控制和变形的人类矩阵模型上的表面光场。在核心上，我们使用可变的两面参数化来表示人类周围的光场，这使得快速和准确地推断人类的外观。这使得我们可以在全图像上进行准确的upervison，而不是之前的方法只能在个像素或小块上进行supervision，因为它们的运行时间过长。我们仔细设计的人类表示和监督策略，导致了最新的合成结果和运行时间。影像结果和代码可以在https://vcai.mpi-inf.mpg.de/projects/DELIFFAS中下载。
</details></li>
</ul>
<hr>
<h2 id="4K4D-Real-Time-4D-View-Synthesis-at-4K-Resolution"><a href="#4K4D-Real-Time-4D-View-Synthesis-at-4K-Resolution" class="headerlink" title="4K4D: Real-Time 4D View Synthesis at 4K Resolution"></a>4K4D: Real-Time 4D View Synthesis at 4K Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11448">http://arxiv.org/abs/2310.11448</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zju3dv/4K4D">https://github.com/zju3dv/4K4D</a></li>
<li>paper_authors: Zhen Xu, Sida Peng, Haotong Lin, Guangzhao He, Jiaming Sun, Yujun Shen, Hujun Bao, Xiaowei Zhou</li>
<li>for: 高精度和实时视觉合成动态3D场景的4K解析</li>
<li>methods: 使用4D点云表示法和硬件排版加速，以及新型混合外观模型提高渲染质量，同时保持高效性</li>
<li>results: 在DNA-Rendering数据集1080p分辨率和ENeRF-Outdoor数据集4K分辨率上实现了30倍 быстре的渲染速度，并达到了当前最佳的渲染质量<details>
<summary>Abstract</summary>
This paper targets high-fidelity and real-time view synthesis of dynamic 3D scenes at 4K resolution. Recently, some methods on dynamic view synthesis have shown impressive rendering quality. However, their speed is still limited when rendering high-resolution images. To overcome this problem, we propose 4K4D, a 4D point cloud representation that supports hardware rasterization and enables unprecedented rendering speed. Our representation is built on a 4D feature grid so that the points are naturally regularized and can be robustly optimized. In addition, we design a novel hybrid appearance model that significantly boosts the rendering quality while preserving efficiency. Moreover, we develop a differentiable depth peeling algorithm to effectively learn the proposed model from RGB videos. Experiments show that our representation can be rendered at over 400 FPS on the DNA-Rendering dataset at 1080p resolution and 80 FPS on the ENeRF-Outdoor dataset at 4K resolution using an RTX 4090 GPU, which is 30x faster than previous methods and achieves the state-of-the-art rendering quality. Our project page is available at https://zju3dv.github.io/4k4d/.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="EvalCrafter-Benchmarking-and-Evaluating-Large-Video-Generation-Models"><a href="#EvalCrafter-Benchmarking-and-Evaluating-Large-Video-Generation-Models" class="headerlink" title="EvalCrafter: Benchmarking and Evaluating Large Video Generation Models"></a>EvalCrafter: Benchmarking and Evaluating Large Video Generation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11440">http://arxiv.org/abs/2310.11440</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/evalcrafter/EvalCrafter">https://github.com/evalcrafter/EvalCrafter</a></li>
<li>paper_authors: Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, Ying Shan<br>for: 这个论文主要目的是提出一种新的评估视频生成模型的框架和管道，以提高现有的评估方法的精度和全面性。methods: 作者使用了一种新的提问列表和18个对象指标来评估state-of-the-art的视频生成模型。同时，他们还提出了一种基于大语言模型的意见对应方法，以使用户的意见来调整对象指标的权重。results: 作者的研究表明，使用新的评估方法可以更好地评估视频生成模型的性能，并且与用户的意见更高相关。这表明了新的评估方法的效iveness。<details>
<summary>Abstract</summary>
The vision and language generative models have been overgrown in recent years. For video generation, various open-sourced models and public-available services are released for generating high-visual quality videos. However, these methods often use a few academic metrics, for example, FVD or IS, to evaluate the performance. We argue that it is hard to judge the large conditional generative models from the simple metrics since these models are often trained on very large datasets with multi-aspect abilities. Thus, we propose a new framework and pipeline to exhaustively evaluate the performance of the generated videos. To achieve this, we first conduct a new prompt list for text-to-video generation by analyzing the real-world prompt list with the help of the large language model. Then, we evaluate the state-of-the-art video generative models on our carefully designed benchmarks, in terms of visual qualities, content qualities, motion qualities, and text-caption alignment with around 18 objective metrics. To obtain the final leaderboard of the models, we also fit a series of coefficients to align the objective metrics to the users' opinions. Based on the proposed opinion alignment method, our final score shows a higher correlation than simply averaging the metrics, showing the effectiveness of the proposed evaluation method.
</details>
<details>
<summary>摘要</summary>
“在最近几年中，视觉和语言生成模型得到了广泛的应用和发展。为视频生成，各种开源的模型和公共可用的服务被发布，以生成高质量的视频。然而，这些方法经常使用一些学术指标，例如FVD或IS，来评估模型的表现。我们认为，这些简单的指标不能准确评估大型条件生成模型，因为这些模型通常在大量数据集上进行训练，具有多方面能力。因此，我们提出了一新的评估框架和管道，以完整评估生成视频的性能。我们首先对文本到视频生成的新提问列表进行分析，并使用大型语言模型的帮助来生成一个新的提问列表。然后，我们对当今状态的视频生成模型进行了严格的评估，包括视觉质量、内容质量、动作质量和文本描述对齐等18个对象指标。为了获得最终的模型排名，我们还使用一系列的系数进行对象指标的对齐。根据我们的提议的意见对齐方法，我们的最终分数显示与简单地平均指标的相对耗散更高，表明了我们的评估方法的效iveness。”
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Map-Relations-for-Unsupervised-Non-Rigid-Shape-Matching"><a href="#Revisiting-Map-Relations-for-Unsupervised-Non-Rigid-Shape-Matching" class="headerlink" title="Revisiting Map Relations for Unsupervised Non-Rigid Shape Matching"></a>Revisiting Map Relations for Unsupervised Non-Rigid Shape Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11420">http://arxiv.org/abs/2310.11420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongliang Cao, Paul Roetzer, Florian Bernard</li>
<li>for: 非RIGID 3D shape matching</li>
<li>methods: 提出了一种新的无监督学习方法，包括自适应功能图计算器和点精度损失函数</li>
<li>results: 在不同的挑战性场景下（包括非同温、topological noise和部分性），与前状态艺术方法相比，提高了substantially的性能<details>
<summary>Abstract</summary>
We propose a novel unsupervised learning approach for non-rigid 3D shape matching. Our approach improves upon recent state-of-the art deep functional map methods and can be applied to a broad range of different challenging scenarios. Previous deep functional map methods mainly focus on feature extraction and aim exclusively at obtaining more expressive features for functional map computation. However, the importance of the functional map computation itself is often neglected and the relationship between the functional map and point-wise map is underexplored. In this paper, we systematically investigate the coupling relationship between the functional map from the functional map solver and the point-wise map based on feature similarity. To this end, we propose a self-adaptive functional map solver to adjust the functional map regularisation for different shape matching scenarios, together with a vertex-wise contrastive loss to obtain more discriminative features. Using different challenging datasets (including non-isometry, topological noise and partiality), we demonstrate that our method substantially outperforms previous state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的无监督学习方法 для非定形3D形状匹配。我们的方法在当前状态艺术深度函数地图方法的基础上进行改进，并可以应用于各种不同的复杂enario。先前的深度函数地图方法主要关注特征提取，偏向于获得更表达力强的特征来计算函数地图。然而，函数地图计算本身的重要性经常被忽略，以及特征和点级地图之间的关系也未得到充分调查。在这篇论文中，我们系统地探讨了函数地图从函数地图解决器中的coupling关系，以及特征相似性基础上的点级地图。为此，我们提出了一种自适应函数地图解决器，以及基于特征相似性的Vertex-wise contraste loss。使用不同的复杂的数据集（包括非同一致、拓扑噪声和缺失），我们示出了我们的方法在前一代方法的基础上具有显著的改进。
</details></li>
</ul>
<hr>
<h2 id="VcT-Visual-change-Transformer-for-Remote-Sensing-Image-Change-Detection"><a href="#VcT-Visual-change-Transformer-for-Remote-Sensing-Image-Change-Detection" class="headerlink" title="VcT: Visual change Transformer for Remote Sensing Image Change Detection"></a>VcT: Visual change Transformer for Remote Sensing Image Change Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11417">http://arxiv.org/abs/2310.11417</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/event-ahu/vct_remote_sensing_change_detection">https://github.com/event-ahu/vct_remote_sensing_change_detection</a></li>
<li>paper_authors: Bo Jiang, Zitian Wang, Xixi Wang, Ziyan Zhang, Lan Chen, Xiao Wang, Bin Luo</li>
<li>for: 本文提出了一种Visual change Transformer（VcT）模型，用于解决视觉变化检测问题。</li>
<li>methods: 该模型首先使用共享背景网络提取图像对的特征图，然后使用图 neural network 模型化图像对的结构信息，并采用 top-K 可靠的 токен mines 和改进自&#x2F;交叉关注机制。</li>
<li>results: 广泛的实验 validate 了我们提出的 VcT 模型的有效性。<details>
<summary>Abstract</summary>
Existing visual change detectors usually adopt CNNs or Transformers for feature representation learning and focus on learning effective representation for the changed regions between images. Although good performance can be obtained by enhancing the features of the change regions, however, these works are still limited mainly due to the ignorance of mining the unchanged background context information. It is known that one main challenge for change detection is how to obtain the consistent representations for two images involving different variations, such as spatial variation, sunlight intensity, etc. In this work, we demonstrate that carefully mining the common background information provides an important cue to learn the consistent representations for the two images which thus obviously facilitates the visual change detection problem. Based on this observation, we propose a novel Visual change Transformer (VcT) model for visual change detection problem. To be specific, a shared backbone network is first used to extract the feature maps for the given image pair. Then, each pixel of feature map is regarded as a graph node and the graph neural network is proposed to model the structured information for coarse change map prediction. Top-K reliable tokens can be mined from the map and refined by using the clustering algorithm. Then, these reliable tokens are enhanced by first utilizing self/cross-attention schemes and then interacting with original features via an anchor-primary attention learning module. Finally, the prediction head is proposed to get a more accurate change map. Extensive experiments on multiple benchmark datasets validated the effectiveness of our proposed VcT model.
</details>
<details>
<summary>摘要</summary>
现有的视觉变化探测器通常采用CNN或Transformers来学习特征表示学习，并主要关注学习改变区域 между图像中的有效表示。although these works can achieve good performance by enhancing the features of the changed regions, they are still limited because they ignore the mining of unchanged background context information. It is known that one of the main challenges of change detection is how to obtain consistent representations for two images with different variations, such as spatial variation and sunlight intensity. In this work, we find that carefully mining the common background information provides an important cue to learn the consistent representations for the two images, which thus facilitates the visual change detection problem. Based on this observation, we propose a novel Visual change Transformer (VcT) model for the visual change detection problem. Specifically, a shared backbone network is first used to extract the feature maps for the given image pair. Then, each pixel of the feature map is regarded as a graph node, and a graph neural network is proposed to model the structured information for coarse change map prediction. Top-K reliable tokens can be mined from the map and refined by using a clustering algorithm. Then, these reliable tokens are enhanced by first utilizing self/cross-attention schemes and then interacting with the original features via an anchor-primary attention learning module. Finally, the prediction head is proposed to get a more accurate change map. Extensive experiments on multiple benchmark datasets validated the effectiveness of our proposed VcT model.
</details></li>
</ul>
<hr>
<h2 id="A-voxel-level-approach-to-brain-age-prediction-A-method-to-assess-regional-brain-aging"><a href="#A-voxel-level-approach-to-brain-age-prediction-A-method-to-assess-regional-brain-aging" class="headerlink" title="A voxel-level approach to brain age prediction: A method to assess regional brain aging"></a>A voxel-level approach to brain age prediction: A method to assess regional brain aging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11385">http://arxiv.org/abs/2310.11385</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nehagianchandani/voxel-level-brain-age-prediction">https://github.com/nehagianchandani/voxel-level-brain-age-prediction</a></li>
<li>paper_authors: Neha Gianchandani, Mahsa Dibaji, Johanna Ospel, Fernando Vega, Mariana Bento, M. Ethan MacDonald, Roberto Souza</li>
<li>For: 这个研究的目的是预测脑部年龄从T1束缚成像图像中，以获得精细的地方化脑部年龄估计。这有助于理解健康和疾病群体中脑部年龄轨迹的差异。* Methods: 这个研究使用了深度学习多任务模型来预测脑部年龄。这种模型在现有文献中表现出色，并且在健康和疾病群体中提供了价值的临床洞察。* Results: 研究发现，健康群体和疾病群体之间存在差异的脑部年龄轨迹。具体来说，健康群体的脑部年龄轨迹比疾病群体更年轻，而且存在脑部区域异常的差异。这些结果提供了有价值的临床洞察，可以帮助理解脑部年龄的发展和疾病的扩散。<details>
<summary>Abstract</summary>
Brain aging is a regional phenomenon, a facet that remains relatively under-explored within the realm of brain age prediction research using machine learning methods. Voxel-level predictions can provide localized brain age estimates that can provide granular insights into the regional aging processes. This is essential to understand the differences in aging trajectories in healthy versus diseased subjects. In this work, a deep learning-based multitask model is proposed for voxel-level brain age prediction from T1-weighted magnetic resonance images. The proposed model outperforms the models existing in the literature and yields valuable clinical insights when applied to both healthy and diseased populations. Regional analysis is performed on the voxel-level brain age predictions to understand aging trajectories of known anatomical regions in the brain and show that there exist disparities in regional aging trajectories of healthy subjects compared to ones with underlying neurological disorders such as Dementia and more specifically, Alzheimer's disease. Our code is available at https://github.com/nehagianchandani/Voxel-level-brain-age-prediction.
</details>
<details>
<summary>摘要</summary>
脑衰老是一个地域性的现象，在机器学习方法可预测脑 возраст预测研究中尚未得到充分的探讨。 voxel 级预测可提供本地化的脑 возраст估计，从而为诊断不同 Population 提供细化的诊断信息。这对于理解健康和疾病 Population 的脑衰老趋势具有重要意义。在本研究中，我们提出了一种基于深度学习的多任务模型，用于从 T1 束缚磁共振成像图像中预测 voxel 级脑 возраст。该模型在文献中存在的模型之上升级，并且在应用于健康和疾病 Population 时具有价值的临床应用。通过对 voxel 级脑 возраст预测结果进行区域分析，我们可以理解健康 Population 的脑衰老趋势与患有 деменcia 和特别是阿尔ц海默病的脑衰老趋势之间的差异。我们的代码可以在 GitHub 上找到：https://github.com/nehagianchandani/Voxel-level-brain-age-prediction。
</details></li>
</ul>
<hr>
<h2 id="Towards-Generalizable-Multi-Camera-3D-Object-Detection-via-Perspective-Debiasing"><a href="#Towards-Generalizable-Multi-Camera-3D-Object-Detection-via-Perspective-Debiasing" class="headerlink" title="Towards Generalizable Multi-Camera 3D Object Detection via Perspective Debiasing"></a>Towards Generalizable Multi-Camera 3D Object Detection via Perspective Debiasing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11346">http://arxiv.org/abs/2310.11346</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Lu, Yunpeng Zhang, Qing Lian, Dalong Du, Yingcong Chen</li>
<li>for: 本研究旨在提高多摄像头3D物体检测（MC3D-Det）的精度和可靠性，解决由于不 familar 测试环境而导致的问题。</li>
<li>methods: 我们提出了一种新的方法，即将3D检测与2D相机平面结果相对应，以确保稳定和准确的检测。我们的框架基于视角偏移 rectification，帮助学习不受视角变化影响的特征。我们首先生成了多视图地图从BEV特征中，然后将这些地图的视角偏移正确 rectified，以利用隐藏的前景体Volume来连接相机和BEV平面。</li>
<li>results: 我们的方法可以在不同的视角和环境下提高object detection的精度和可靠性。我们在Domain Generalization（DG）和Unsupervised Domain Adaptation（UDA）任务上实现了显著的效果。此外，我们还证明了我们的方法可以在实际数据上达到满意的结果，只需训练于虚拟数据集。<details>
<summary>Abstract</summary>
Detecting objects in 3D space using multiple cameras, known as Multi-Camera 3D Object Detection (MC3D-Det), has gained prominence with the advent of bird's-eye view (BEV) approaches. However, these methods often struggle when faced with unfamiliar testing environments due to the lack of diverse training data encompassing various viewpoints and environments. To address this, we propose a novel method that aligns 3D detection with 2D camera plane results, ensuring consistent and accurate detections. Our framework, anchored in perspective debiasing, helps the learning of features resilient to domain shifts. In our approach, we render diverse view maps from BEV features and rectify the perspective bias of these maps, leveraging implicit foreground volumes to bridge the camera and BEV planes. This two-step process promotes the learning of perspective- and context-independent features, crucial for accurate object detection across varying viewpoints, camera parameters and environment conditions. Notably, our model-agnostic approach preserves the original network structure without incurring additional inference costs, facilitating seamless integration across various models and simplifying deployment. Furthermore, we also show our approach achieves satisfactory results in real data when trained only with virtual datasets, eliminating the need for real scene annotations. Experimental results on both Domain Generalization (DG) and Unsupervised Domain Adaptation (UDA) clearly demonstrate its effectiveness. Our code will be released.
</details>
<details>
<summary>摘要</summary>
使用多个摄像头探测3D空间中的对象，称为多摄像头3D对象探测（MC3D-Det），在现代鸟瞰视图（BEV）方法的普及下得到了更多的关注。然而，这些方法经常在不熟悉的测试环境中遇到困难，因为缺乏包括多个视角和环境的多样化训练数据。为解决这个问题，我们提出了一种新的方法，该方法将3D探测与2D摄像头平面结果进行对应，以确保准确和一致的探测。我们的框架基于视角偏移的约束，帮助学习不受视角和环境的偏移影响的特征。在我们的方法中，我们从BEV特征中生成多种视图图，然后对这些图像进行视角偏移的正确化，通过使用隐藏的前景体来连接摄像头和BEV平面。这两步过程推动学习不受视角和环境的特征，这些特征是精度的对象探测所必需的。各种模型的机制无需更改，我们的方法可以轻松地与不同的模型集成，并且简化部署。此外，我们还证明我们的方法在真实数据上获得了满意的结果，无需使用真实场景的注释。实验结果表明，我们的方法在适应不同视角、摄像头参数和环境条件时具有显著的效果。我们的代码将会发布。
</details></li>
</ul>
<hr>
<h2 id="Towards-Generic-Semi-Supervised-Framework-for-Volumetric-Medical-Image-Segmentation"><a href="#Towards-Generic-Semi-Supervised-Framework-for-Volumetric-Medical-Image-Segmentation" class="headerlink" title="Towards Generic Semi-Supervised Framework for Volumetric Medical Image Segmentation"></a>Towards Generic Semi-Supervised Framework for Volumetric Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11320">http://arxiv.org/abs/2310.11320</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/GenericSSL">https://github.com/xmed-lab/GenericSSL</a></li>
<li>paper_authors: Haonan Wang, Xiaomeng Li</li>
<li>For: 这个研究的目的是开发一个通用的semi-supervised learning（SSL）框架，可以处理多个设定，包括SSL、Unsupervised Domain Adaptation（UDA）和Semi-supervised Domain Generalization（SemiDG）。* Methods: 该框架使用了一种含有混合和解耦的方法，包括一个扩散编码器，用于从多个分布&#x2F;领域中提取共同知识集，以及三个解码器，用于在培训过程中避免过拟合标注数据。* Results: 对四个 benchmark dataset进行了评估，并与现有方法进行了比较，结果表明该框架在所有四个设定中具有显著的改进， indicating the potential of the framework to tackle more challenging SSL scenarios.<details>
<summary>Abstract</summary>
Volume-wise labeling in 3D medical images is a time-consuming task that requires expertise. As a result, there is growing interest in using semi-supervised learning (SSL) techniques to train models with limited labeled data. However, the challenges and practical applications extend beyond SSL to settings such as unsupervised domain adaptation (UDA) and semi-supervised domain generalization (SemiDG). This work aims to develop a generic SSL framework that can handle all three settings. We identify two main obstacles to achieving this goal in the existing SSL framework: 1) the weakness of capturing distribution-invariant features; and 2) the tendency for unlabeled data to be overwhelmed by labeled data, leading to over-fitting to the labeled data during training. To address these issues, we propose an Aggregating & Decoupling framework. The aggregating part consists of a Diffusion encoder that constructs a common knowledge set by extracting distribution-invariant features from aggregated information from multiple distributions/domains. The decoupling part consists of three decoders that decouple the training process with labeled and unlabeled data, thus avoiding over-fitting to labeled data, specific domains and classes. We evaluate our proposed framework on four benchmark datasets for SSL, Class-imbalanced SSL, UDA and SemiDG. The results showcase notable improvements compared to state-of-the-art methods across all four settings, indicating the potential of our framework to tackle more challenging SSL scenarios. Code and models are available at: https://github.com/xmed-lab/GenericSSL.
</details>
<details>
<summary>摘要</summary>
医学三维图像的体积标注是一项时间消耗大的任务，需要专家知识。由于此类任务的挑战和实际应用超出了半编制学习（SSL）技术的范畴，因此这项工作的目标是开发一个通用的SSL框架，可以处理所有三个设置。我们在现有的SSL框架中 Identified two main challenges to achieving this goal: 1）不能够捕捉分布不变的特征; 2）无标注数据被标注数据所抑制，导致在训练过程中适应标注数据，从而导致过拟合。为了解决这些问题，我们提出了一个集成&分离框架。集成部分包括一个扩散编码器，通过从多个分布/领域中提取分布不变的特征来构建共同知识集。分离部分包括三个解码器，通过解耦训练过程中的标注数据和无标注数据，从而避免过拟合标注数据、特定领域和类别。我们在四个 benchmark 数据集上进行了四种SSL、不均衡SSL、UDA 和 SemiDG 的测试，结果显示了与现有方法的明显改善，这表明我们的框架有可能在更加复杂的SSL场景中表现出色。代码和模型可以在 GitHub 上获取：https://github.com/xmed-lab/GenericSSL。
</details></li>
</ul>
<hr>
<h2 id="Multi-Self-supervised-Pre-fine-tuned-Transformer-Fusion-for-Better-Intelligent-Transportation-Detection"><a href="#Multi-Self-supervised-Pre-fine-tuned-Transformer-Fusion-for-Better-Intelligent-Transportation-Detection" class="headerlink" title="Multi Self-supervised Pre-fine-tuned Transformer Fusion for Better Intelligent Transportation Detection"></a>Multi Self-supervised Pre-fine-tuned Transformer Fusion for Better Intelligent Transportation Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11307">http://arxiv.org/abs/2310.11307</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juwu Zheng, Jiangtao Ren</li>
<li>for: 本研究旨在提高智能交通系统中的检测精度，解决现有检测方法受到两个限制：一是模型知识预处理大规模数据集和目标任务知识之间的差异，二是大多数检测模型采用单源学习方式，限制学习能力。</li>
<li>methods: 我们提出了一种多自助学习前练 transformer 融合网络（MSPTF），包括两个步骤：自助学习预练频率学习和多模型协同学习目标任务。在第一步，我们引入了自助学习方法到 transformer 模型预练中，以减少数据成本并减轻预处理模型和目标任务之间的知识差异。在第二步，我们提出了多模型协同学习方法，通过考虑不同模型架构和预练任务之间的信息差异，将不同 transformer 模型特征信息融合到一起，以获得更完整和正确的检测特征。</li>
<li>results: 我们对 vehicle 识别数据集和路径疾病检测数据集进行实验，比基准方法提高了1.1%、5.5%、4.2%，比最新方法（sota）提高了0.7%、1.8%、1.7%。这些结果证明了我们的方法的有效性。<details>
<summary>Abstract</summary>
Intelligent transportation system combines advanced information technology to provide intelligent services such as monitoring, detection, and early warning for modern transportation. Intelligent transportation detection is the cornerstone of many intelligent traffic services by identifying task targets through object detection methods. However existing detection methods in intelligent transportation are limited by two aspects. First, there is a difference between the model knowledge pre-trained on large-scale datasets and the knowledge required for target task. Second, most detection models follow the pattern of single-source learning, which limits the learning ability. To address these problems, we propose a Multi Self-supervised Pre-fine-tuned Transformer Fusion (MSPTF) network, consisting of two steps: unsupervised pre-fine-tune domain knowledge learning and multi-model fusion target task learning. In the first step, we introduced self-supervised learning methods into transformer model pre-fine-tune which could reduce data costs and alleviate the knowledge gap between pre-trained model and target task. In the second step, we take feature information differences between different model architectures and different pre-fine-tune tasks into account and propose Multi-model Semantic Consistency Cross-attention Fusion (MSCCF) network to combine different transformer model features by considering channel semantic consistency and feature vector semantic consistency, which obtain more complete and proper fusion features for detection task. We experimented the proposed method on vehicle recognition dataset and road disease detection dataset and achieved 1.1%, 5.5%, 4.2% improvement compared with baseline and 0.7%, 1.8%, 1.7% compared with sota, which proved the effectiveness of our method.
</details>
<details>
<summary>摘要</summary>
智能交通系统结合先进的信息技术，为现代交通提供智能服务，如监测、检测和早期警示。智能交通检测是现代交通服务的核心，通过物体检测方法来确定任务目标。然而，现有的检测方法在智能交通中存在两个限制。一是，模型的先行学习知识与目标任务知识之间存在差异。二是，大多数检测模型采用单源学习模式，限制了学习能力。为了解决这些问题，我们提出了多自动学习预练转换器融合网络（MSPTF），包括两个步骤：无监督预练域知识学习和多模型融合目标任务学习。在第一步，我们将自动学习方法引入转换器模型预练，以减少数据成本并缓解先行学习知识与目标任务知识之间的差异。在第二步，我们利用不同模型架构和预练任务的特征信息差异，提出多模型semantic consistency cross-attention融合网络（MSCCF），将不同转换器模型的特征信息融合，以考虑通道semantic consistency和特征向量semantic consistency，从而获得更加完整和正确的融合特征，进而提高检测任务的准确率。我们对汽车识别 dataset 和路况病变 dataset 进行实验，与基准值和state-of-the-art（sota）进行比较，实验结果显示，我们的方法可以提高检测任务的准确率1.1%、5.5%、4.2%，比基准值更高，证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="CorrTalk-Correlation-Between-Hierarchical-Speech-and-Facial-Activity-Variances-for-3D-Animation"><a href="#CorrTalk-Correlation-Between-Hierarchical-Speech-and-Facial-Activity-Variances-for-3D-Animation" class="headerlink" title="CorrTalk: Correlation Between Hierarchical Speech and Facial Activity Variances for 3D Animation"></a>CorrTalk: Correlation Between Hierarchical Speech and Facial Activity Variances for 3D Animation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11295">http://arxiv.org/abs/2310.11295</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaojie Chu, Kailing Guo, Xiaofen Xing, Yilin Lan, Bolun Cai, Xiangmin Xu<br>for: 这篇论文主要针对的是如何通过语音驱动来生成3D人脸动画，以解决跨Modal Task的挑战。methods: 该论文提出了一种新的框架，即CorrTalk，该框架可以有效地在不同强度的面部活动之间建立时间相关性，并通过 dual-branch decoding 架构来同时synthesize强度不同的面部活动，以保证更宽泛的表情动画生成。results: 根据论文的实验和用户研究，CorrTalk 表现出excelent的性能，可以准确地生成具有不同强度的表情动画，并且能够保持 lip-sync 和合理的表情表达。<details>
<summary>Abstract</summary>
Speech-driven 3D facial animation is a challenging cross-modal task that has attracted growing research interest. During speaking activities, the mouth displays strong motions, while the other facial regions typically demonstrate comparatively weak activity levels. Existing approaches often simplify the process by directly mapping single-level speech features to the entire facial animation, which overlook the differences in facial activity intensity leading to overly smoothed facial movements. In this study, we propose a novel framework, CorrTalk, which effectively establishes the temporal correlation between hierarchical speech features and facial activities of different intensities across distinct regions. A novel facial activity intensity metric is defined to distinguish between strong and weak facial activity, obtained by computing the short-time Fourier transform of facial vertex displacements. Based on the variances in facial activity, we propose a dual-branch decoding framework to synchronously synthesize strong and weak facial activity, which guarantees wider intensity facial animation synthesis. Furthermore, a weighted hierarchical feature encoder is proposed to establish temporal correlation between hierarchical speech features and facial activity at different intensities, which ensures lip-sync and plausible facial expressions. Extensive qualitatively and quantitatively experiments as well as a user study indicate that our CorrTalk outperforms existing state-of-the-art methods. The source code and supplementary video are publicly available at: https://zjchu.github.io/projects/CorrTalk/
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>语音驱动的3D面部动画是一个吸引了研究者们的挑战性跨Modal任务。在说话活动中，口部会显示强烈的动作，而其他面部区域通常会表现出较弱的活动水平。现有的方法通常会简化过程，直接将单级语音特征映射到整个面部动画上，这会忽略面部活动Intensity的差异，导致面部运动过于平滑。在这种研究中，我们提出了一个新的框架，即CorrTalk，它可以有效地在不同Intensity水平上建立语音特征和面部动画的时间相关性。我们定义了一个新的面部动activity强度度量，通过计算面部顶点位移的短时傅立叶变换来获得。基于面部动activity的差异，我们提出了一种双分支解码机制，以同步生成强度不同的面部动画，从而保证更广泛的Intensity面部动画生成。此外，我们还提出了一种加权层次特征编码器，以建立不同Intensity水平上的语音特征和面部动画之间的时间相关性，从而保证唇ync和合理的面部表达。我们的CorrTalk在质量和kvantalitative的实验以及用户研究中表现出色，超越了现有的状态 искус技术。source code和补充视频可以在以下链接获取：https://zjchu.github.io/projects/CorrTalk/
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-3D-Scene-Flow-Estimation-and-Motion-Prediction-using-Local-Rigidity-Prior"><a href="#Self-Supervised-3D-Scene-Flow-Estimation-and-Motion-Prediction-using-Local-Rigidity-Prior" class="headerlink" title="Self-Supervised 3D Scene Flow Estimation and Motion Prediction using Local Rigidity Prior"></a>Self-Supervised 3D Scene Flow Estimation and Motion Prediction using Local Rigidity Prior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11284">http://arxiv.org/abs/2310.11284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruibo Li, Chi Zhang, Zhe Wang, Chunhua Shen, Guosheng Lin</li>
<li>for:  investigate self-supervised 3D scene flow estimation and class-agnostic motion prediction on point clouds</li>
<li>methods:  build pseudo scene flow labels through piecewise rigid motion estimation and validate with a validity mask</li>
<li>results:  achieve new state-of-the-art performance in self-supervised scene flow learning and outperform previous state-of-the-art self-supervised methods on nuScenes dataset.<details>
<summary>Abstract</summary>
In this article, we investigate self-supervised 3D scene flow estimation and class-agnostic motion prediction on point clouds. A realistic scene can be well modeled as a collection of rigidly moving parts, therefore its scene flow can be represented as a combination of the rigid motion of these individual parts. Building upon this observation, we propose to generate pseudo scene flow labels for self-supervised learning through piecewise rigid motion estimation, in which the source point cloud is decomposed into local regions and each region is treated as rigid. By rigidly aligning each region with its potential counterpart in the target point cloud, we obtain a region-specific rigid transformation to generate its pseudo flow labels. To mitigate the impact of potential outliers on label generation, when solving the rigid registration for each region, we alternately perform three steps: establishing point correspondences, measuring the confidence for the correspondences, and updating the rigid transformation based on the correspondences and their confidence. As a result, confident correspondences will dominate label generation and a validity mask will be derived for the generated pseudo labels. By using the pseudo labels together with their validity mask for supervision, models can be trained in a self-supervised manner. Extensive experiments on FlyingThings3D and KITTI datasets demonstrate that our method achieves new state-of-the-art performance in self-supervised scene flow learning, without any ground truth scene flow for supervision, even performing better than some supervised counterparts. Additionally, our method is further extended to class-agnostic motion prediction and significantly outperforms previous state-of-the-art self-supervised methods on nuScenes dataset.
</details>
<details>
<summary>摘要</summary>
在这篇文章中，我们调查了无监督3D场景流估计和无类别运动预测。一个现实场景可以很好地被模型为一个由rigidly运动的部件组成的集合，因此其场景流可以被表示为这些个体部件的rigid运动的组合。基于这一观察，我们提议通过地方rigid运动估计来生成 Pseudo场景流标签，其中源点云被分解成地方区域，每个区域都是rigid。通过将每个区域与其可能的对应点云中的区域进行rigid对齐，我们可以获得每个区域的rigid变换，并生成其pseudo流标签。为了mitigate潜在异常值对标签生成的影响，当解决每个区域的rigid注册问题时，我们采取了三个步骤：确定点对应关系、测量对应关系的信任度，并基于对应关系和其信任度更新rigid变换。因此，信任度高的对应关系将dominates标签生成，并生成一个有效性面纱。通过使用这些pseudo标签和其有效性面纱进行监督，我们可以在无监督情况下训练模型。我们的方法在FlyingThings3D和KITTI数据集上进行了广泛的实验，并达到了无监督场景流学习的新状态对抗性性能，甚至超过了一些监督 counterpart。此外，我们的方法进一步扩展到无类别运动预测，并在nuScenes数据集上显著超越了前一个状态对抗性自监督方法。
</details></li>
</ul>
<hr>
<h2 id="Video-Super-Resolution-Using-a-Grouped-Residual-in-Residual-Network"><a href="#Video-Super-Resolution-Using-a-Grouped-Residual-in-Residual-Network" class="headerlink" title="Video Super-Resolution Using a Grouped Residual in Residual Network"></a>Video Super-Resolution Using a Grouped Residual in Residual Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11276">http://arxiv.org/abs/2310.11276</a></li>
<li>repo_url: None</li>
<li>paper_authors: MohammadHossein Ashoori, Arash Amini</li>
<li>for: 提高图像&#x2F;视频内容的分辨率和质量。</li>
<li>methods: 使用 grouped residual in residual network (GRRN) 方法。</li>
<li>results: 与现有方法相比，GRRN 方法可以提供Acceptable的输出图像质量。<details>
<summary>Abstract</summary>
Super-resolution (SR) is the technique of increasing the nominal resolution of image / video content accompanied with quality improvement. Video super-resolution (VSR) can be considered as the generalization of single image super-resolution (SISR). This generalization should be such that more detail is created in the output using adjacent input frames. In this paper, we propose a grouped residual in residual network (GRRN) for VSR. By adjusting the hyperparameters of the proposed structure, we train three networks with different numbers of parameters and compare their quantitative and qualitative results with the existing methods. Although based on some quantitative criteria, GRRN does not provide better results than the existing methods, in terms of the quality of the output image it has acceptable performance.
</details>
<details>
<summary>摘要</summary>
超分解（SR）是增加图像/视频内容的 номинаinal 分辨率，同时提高图像质量的技术。视频超分解（VSR）可以视为单个图像超分解（SISR）的推广。在这篇论文中，我们提出了分组差分网络（GRRN） для VSR。通过调整结构的 hyperparameter，我们训练了三个网络，每个网络有不同的参数数量，并与现有方法进行比较。虽然根据一些量化标准，GRRN 不比现有方法提供更好的结果，但在输出图像质量方面，它的表现是可接受的。
</details></li>
</ul>
<hr>
<h2 id="Image-Compression-using-only-Attention-based-Neural-Networks"><a href="#Image-Compression-using-only-Attention-based-Neural-Networks" class="headerlink" title="Image Compression using only Attention based Neural Networks"></a>Image Compression using only Attention based Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11265">http://arxiv.org/abs/2310.11265</a></li>
<li>repo_url: None</li>
<li>paper_authors: Natacha Luka, Romain Negrel, David Picard</li>
<li>for: 本研究旨在探讨图像压缩中完全使用注意力层，以提高图像压缩性和计算效率。</li>
<li>methods: 本paper使用的方法是基于注意力机制的Transformer architecture，并引入了学习图像查询来汇聚patch信息。</li>
<li>results: 经过广泛的评估，本paper的方法在popular Kodak、DIV2K和CLIC datasets上达到了与传统手动设计ipeline相同或更高的性能，而无需使用 convolutional layers。<details>
<summary>Abstract</summary>
In recent research, Learned Image Compression has gained prominence for its capacity to outperform traditional handcrafted pipelines, especially at low bit-rates. While existing methods incorporate convolutional priors with occasional attention blocks to address long-range dependencies, recent advances in computer vision advocate for a transformative shift towards fully transformer-based architectures grounded in the attention mechanism. This paper investigates the feasibility of image compression exclusively using attention layers within our novel model, QPressFormer. We introduce the concept of learned image queries to aggregate patch information via cross-attention, followed by quantization and coding techniques. Through extensive evaluations, our work demonstrates competitive performance achieved by convolution-free architectures across the popular Kodak, DIV2K, and CLIC datasets.
</details>
<details>
<summary>摘要</summary>
Recent research has shown that Learned Image Compression has gained popularity for its ability to outperform traditional handcrafted pipelines, especially at low bit-rates. While existing methods use convolutional priors with occasional attention blocks to address long-range dependencies, recent advances in computer vision have advocated for a transformative shift towards fully transformer-based architectures grounded in the attention mechanism. This paper explores the feasibility of image compression using only attention layers in our novel model, QPressFormer. We introduce the concept of learned image queries to aggregate patch information via cross-attention, followed by quantization and coding techniques. Through extensive evaluations, our work demonstrates competitive performance achieved by convolution-free architectures across the popular Kodak, DIV2K, and CLIC datasets.
</details></li>
</ul>
<hr>
<h2 id="An-empirical-study-of-automatic-wildlife-detection-using-drone-thermal-imaging-and-object-detection"><a href="#An-empirical-study-of-automatic-wildlife-detection-using-drone-thermal-imaging-and-object-detection" class="headerlink" title="An empirical study of automatic wildlife detection using drone thermal imaging and object detection"></a>An empirical study of automatic wildlife detection using drone thermal imaging and object detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11257">http://arxiv.org/abs/2310.11257</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miao Chang, Tan Vuong, Manas Palaparthi, Lachlan Howell, Alessio Bonti, Mohamed Abdelrazek, Duc Thanh Nguyen</li>
<li>For: The paper is written for wildlife management, specifically to explore the use of drones and thermal imaging technology for collecting and interpreting wildlife data.* Methods: The paper uses a comprehensive review and empirical study of drone-based wildlife detection, including the collection of a realistic dataset of drone-derived wildlife thermal detections and the annotation of these detections via bounding boxes by experts. The paper also benchmarks state-of-the-art object detection algorithms on the collected dataset.* Results: The paper provides experimental results to identify issues and discuss future directions in automatic animal monitoring using drones.Here is the information in Simplified Chinese text:* For: 这篇论文是为了管理野生动物而写的，具体来说是利用无人飞行器和热成像技术收集和解读野生动物数据。* Methods: 这篇论文使用了全面的文献综述和实验研究，包括收集了一个真实的无人飞行器捕捉的野生动物热成像检测数据，并由专家 manually annotate these detections via bounding boxes。 paper还使用了现有的对象检测算法来对 collected dataset 进行比较。* Results: 这篇论文提供了实验结果，用于发现issues和讨论未来无人飞行器自动动物监测的方向。<details>
<summary>Abstract</summary>
Artificial intelligence has the potential to make valuable contributions to wildlife management through cost-effective methods for the collection and interpretation of wildlife data. Recent advances in remotely piloted aircraft systems (RPAS or ``drones'') and thermal imaging technology have created new approaches to collect wildlife data. These emerging technologies could provide promising alternatives to standard labourious field techniques as well as cover much larger areas. In this study, we conduct a comprehensive review and empirical study of drone-based wildlife detection. Specifically, we collect a realistic dataset of drone-derived wildlife thermal detections. Wildlife detections, including arboreal (for instance, koalas, phascolarctos cinereus) and ground dwelling species in our collected data are annotated via bounding boxes by experts. We then benchmark state-of-the-art object detection algorithms on our collected dataset. We use these experimental results to identify issues and discuss future directions in automatic animal monitoring using drones.
</details>
<details>
<summary>摘要</summary>
人工智能有可能为野生动物管理提供有价值的贡献，通过cost-effective的方式收集和解释野生动物数据。最近的无人驾驶飞行器系统（RPAS或“无人机”）和热成像技术的发展已经创造了新的方法收集野生动物数据。这些新技术可能会提供标准化Field技术的有优的替代方案，同时能够覆盖更大的区域。在这项研究中，我们进行了全面的文献综述和实验研究，specifically collecting a realistic dataset of drone-derived wildlife thermal detections。我们的收集数据包括树上的物种（如 Koala，phascolarctos cinereus）和地面生物种，并由专家用 bounding boxes 进行了标注。我们然后对 state-of-the-art 对象检测算法进行了 benchmarking 测试，并使用实验结果来评估自动动物监测使用无人机的问题和未来方向。
</details></li>
</ul>
<hr>
<h2 id="Gromov-Wassertein-like-Distances-in-the-Gaussian-Mixture-Models-Space"><a href="#Gromov-Wassertein-like-Distances-in-the-Gaussian-Mixture-Models-Space" class="headerlink" title="Gromov-Wassertein-like Distances in the Gaussian Mixture Models Space"></a>Gromov-Wassertein-like Distances in the Gaussian Mixture Models Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11256">http://arxiv.org/abs/2310.11256</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antoine Salmona, Julie Delon, Agnès Desolneux</li>
<li>for: 本文介绍了两种Gromov-Wasserstein-type距离在 Gaussian mixture model 空间上。第一种距离是 между两个离散分布在 Gaussian 测度空间上的 Gromov-Wasserstein 距离，可以作为 Gromov-Wasserstein 的替代方法，用于评估分布之间的距离，而不需要直接计算传输计划。</li>
<li>methods: 本文引入了另一种距离 between measures living in incomparable spaces，这种距离与 Gromov-Wasserstein 密切相关，可以用于定义传输计划。 restricting the set of admissible transportation couplings to be themselves Gaussian mixture models in this latter, this defines another distance between Gaussian mixture models that can be used as another alternative to Gromov-Wasserstein。</li>
<li>results: 本文设计了一种基于第一种距离的传输计划，并通过对 medium-to-large scale problems such as shape matching and hyperspectral image color transfer 进行实验，证明了其实用性。<details>
<summary>Abstract</summary>
In this paper, we introduce two Gromov-Wasserstein-type distances on the set of Gaussian mixture models. The first one takes the form of a Gromov-Wasserstein distance between two discrete distributionson the space of Gaussian measures. This distance can be used as an alternative to Gromov-Wasserstein for applications which only require to evaluate how far the distributions are from each other but does not allow to derive directly an optimal transportation plan between clouds of points. To design a way to define such a transportation plan, we introduce another distance between measures living in incomparable spaces that turns out to be closely related to Gromov-Wasserstein. When restricting the set of admissible transportation couplings to be themselves Gaussian mixture models in this latter, this defines another distance between Gaussian mixture models that can be used as another alternative to Gromov-Wasserstein and which allows to derive an optimal assignment between points. Finally, we design a transportation plan associated with the first distance by analogy with the second, and we illustrate their practical uses on medium-to-large scale problems such as shape matching and hyperspectral image color transfer.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了两种Gromov-Wasserstein-类型的距离在 Gaussian mixture model 上。第一个距离是两个抽象分布在 Gaussian measure 空间上的 Gromov-Wasserstein 距离，可以用作 Gromov-Wasserstein 的替代方法，但不能直接 derivate 最佳运输计划 между云集点。为了设计一种定义这种运输计划的方法，我们引入了另一种在不可比较的空间上的距离，该距离与 Gromov-Wasserstein 密切相关。当限制了可用的运输结合为 Gaussian mixture model 时，这个距离定义了另一种 Gaussian mixture model 之间的距离，可以作为 Gromov-Wasserstein 的另一种替代方法，并且可以 derivate 最佳分配计划。最后，我们设计了一个运输计划相关的方法，并在媒体规模到大型问题上如形态匹配和彩色图像传输中 illustrate 其实用性。
</details></li>
</ul>
<hr>
<h2 id="LiDAR-based-4D-Occupancy-Completion-and-Forecasting"><a href="#LiDAR-based-4D-Occupancy-Completion-and-Forecasting" class="headerlink" title="LiDAR-based 4D Occupancy Completion and Forecasting"></a>LiDAR-based 4D Occupancy Completion and Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11239">http://arxiv.org/abs/2310.11239</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai4ce/occ4cast">https://github.com/ai4ce/occ4cast</a></li>
<li>paper_authors: Xinhao Liu, Moonjun Gong, Qi Fang, Haoyu Xie, Yiming Li, Hang Zhao, Chen Feng</li>
<li>for: 本研究旨在整合Scene Completion和Forecasting两个问题，提出了一种新的LiDAR感知任务——Occupancy Completion and Forecasting（OCF），用于自动驾驶中的4D感知。</li>
<li>methods: 本研究使用了新的算法来解决三个挑战：（1）稀疏到稠密的重建，（2）部分到完整的幻化，（3）3D到4D预测。</li>
<li>results: 研究人员通过对OCFBench dataset进行超视觉和评估，发现了一些相对较好的基线模型和自己的模型的性能。这些结果预示了OCF任务的重要性和潜在应用前景。<details>
<summary>Abstract</summary>
Scene completion and forecasting are two popular perception problems in research for mobile agents like autonomous vehicles. Existing approaches treat the two problems in isolation, resulting in a separate perception of the two aspects. In this paper, we introduce a novel LiDAR perception task of Occupancy Completion and Forecasting (OCF) in the context of autonomous driving to unify these aspects into a cohesive framework. This task requires new algorithms to address three challenges altogether: (1) sparse-to-dense reconstruction, (2) partial-to-complete hallucination, and (3) 3D-to-4D prediction. To enable supervision and evaluation, we curate a large-scale dataset termed OCFBench from public autonomous driving datasets. We analyze the performance of closely related existing baseline models and our own ones on our dataset. We envision that this research will inspire and call for further investigation in this evolving and crucial area of 4D perception. Our code for data curation and baseline implementation is available at https://github.com/ai4ce/Occ4cast.
</details>
<details>
<summary>摘要</summary>
Scene completion and forecasting are two popular perception problems in research for mobile agents like autonomous vehicles. Existing approaches treat the two problems in isolation, resulting in a separate perception of the two aspects. In this paper, we introduce a novel LiDAR perception task of Occupancy Completion and Forecasting (OCF) in the context of autonomous driving to unify these aspects into a cohesive framework. This task requires new algorithms to address three challenges altogether: (1) sparse-to-dense reconstruction, (2) partial-to-complete hallucination, and (3) 3D-to-4D prediction. To enable supervision and evaluation, we curate a large-scale dataset termed OCFBench from public autonomous driving datasets. We analyze the performance of closely related existing baseline models and our own ones on our dataset. We envision that this research will inspire and call for further investigation in this evolving and crucial area of 4D perception. Our code for data curation and baseline implementation is available at https://github.com/ai4ce/Occ4cast.Translation notes:* "Scene completion" and "forecasting" are both translated as "场景完成" (scenario completion) to refer to the same concept.* "LiDAR perception" is translated as "LiDAR感知" (LiDAR perception) to refer to the specific sensing technology used.* "Occupancy Completion and Forecasting" is translated as "场景完成和预测" (scenario completion and forecasting) to refer to the combined task.* "Sparse-to-dense reconstruction" is translated as "稀疏到密集重建" (sparse-to-dense reconstruction) to refer to the challenge of reconstructing a dense 3D point cloud from a sparse set of measurements.* "Partial-to-complete hallucination" is translated as "部分到完整的幻觉" (partial-to-complete hallucination) to refer to the challenge of generating complete 3D information from partial measurements.* "3D-to-4D prediction" is translated as "3D到4D预测" (3D-to-4D prediction) to refer to the challenge of predicting the future 4D state of the environment based on current 3D information.* "OCFBench" is translated as "OCFBench" (OCFBench) to refer to the specific dataset used for evaluation.* "baseline models" is translated as "基线模型" (baseline models) to refer to the existing models used for comparison.* "our own ones" is translated as "我们自己的" (our own ones) to refer to the models developed by the authors.
</details></li>
</ul>
<hr>
<h2 id="Innovative-Methods-for-Non-Destructive-Inspection-of-Handwritten-Documents"><a href="#Innovative-Methods-for-Non-Destructive-Inspection-of-Handwritten-Documents" class="headerlink" title="Innovative Methods for Non-Destructive Inspection of Handwritten Documents"></a>Innovative Methods for Non-Destructive Inspection of Handwritten Documents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11217">http://arxiv.org/abs/2310.11217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eleonora Breci, Luca Guarnera, Sebastiano Battiato</li>
<li>for: 本研究旨在提高手写文档分析的精度和效率，以便通过评估手写文档的特征来确定作者身份。</li>
<li>methods: 本研究使用图像处理和深度学习技术EXTRACT AND ANALYZE handwritten manuscript documents的内在特征，包括文本行高、单词间距和字体大小。每个文档的特征向量最终包括每种类型的平均值和标准差。通过对比文档的特征向量的euclid distance，可以 объектив地确定作者身份。</li>
<li>results: 我们的实验结果表明，我们的方法可以对不同的写作媒体（包括手写纸和数字设备）进行 объектив的作者身份识别，并且超过了现有方法的性能。<details>
<summary>Abstract</summary>
Handwritten document analysis is an area of forensic science, with the goal of establishing authorship of documents through examination of inherent characteristics. Law enforcement agencies use standard protocols based on manual processing of handwritten documents. This method is time-consuming, is often subjective in its evaluation, and is not replicable. To overcome these limitations, in this paper we present a framework capable of extracting and analyzing intrinsic measures of manuscript documents related to text line heights, space between words, and character sizes using image processing and deep learning techniques. The final feature vector for each document involved consists of the mean and standard deviation for every type of measure collected. By quantifying the Euclidean distance between the feature vectors of the documents to be compared, authorship can be discerned. We also proposed a new and challenging dataset consisting of 362 handwritten manuscripts written on paper and digital devices by 124 different people. Our study pioneered the comparison between traditionally handwritten documents and those produced with digital tools (e.g., tablets). Experimental results demonstrate the ability of our method to objectively determine authorship in different writing media, outperforming the state of the art.
</details>
<details>
<summary>摘要</summary>
手写文档分析是法医科学领域中的一个领域，旨在透过评估手写文档的内在特征，以确定文档的作者。法律机关通常采用标准协议，基于手动处理手写文档。这种方法是时间consuming，容易受主观影响，并且不可重复。为了超越这些限制，在这篇论文中，我们提出了一个框架，可以提取和分析手写文档中相关的字体大小、词间距和字体大小等内在特征，使用图像处理和深度学习技术。每个文档的最终特征向量由每种测量类型的均值和标准差组成。通过计算这些特征向量之间的欧氏距离，可以bjectively Determine authorship。我们还提出了一个新的和挑战性的数据集，包含362份手写文档，由124名不同的人写作。我们的研究对手写文档和数字工具（例如平板电脑）生成的文档进行比较，并实验结果表明，我们的方法可以在不同的写作媒体中对作者进行 объекively 确定。
</details></li>
</ul>
<hr>
<h2 id="Learning-Comprehensive-Representations-with-Richer-Self-for-Text-to-Image-Person-Re-Identification"><a href="#Learning-Comprehensive-Representations-with-Richer-Self-for-Text-to-Image-Person-Re-Identification" class="headerlink" title="Learning Comprehensive Representations with Richer Self for Text-to-Image Person Re-Identification"></a>Learning Comprehensive Representations with Richer Self for Text-to-Image Person Re-Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11210">http://arxiv.org/abs/2310.11210</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuanglin Yan, Neng Dong, Jun Liu, Liyan Zhang, Jinhui Tang</li>
<li>For: 本研究旨在提高文本检索到图像的精度，并解决现有方法对多视图匹配的缺乏考虑。* Methods: 我们提出了一种简单 yet effective的框架，即LCR$^2$S，它通过学习多视图modalities的承载表示来建立多对多匹配。我们还设计了一个多头注意力混合模块，以混合图像（文本）和其支持集。* Results: 我们的方法在三个popular TIReID数据集上实现了优秀的效果，并新创造了TIReID tasks的最佳表现。<details>
<summary>Abstract</summary>
Text-to-image person re-identification (TIReID) retrieves pedestrian images of the same identity based on a query text. However, existing methods for TIReID typically treat it as a one-to-one image-text matching problem, only focusing on the relationship between image-text pairs within a view. The many-to-many matching between image-text pairs across views under the same identity is not taken into account, which is one of the main reasons for the poor performance of existing methods. To this end, we propose a simple yet effective framework, called LCR$^2$S, for modeling many-to-many correspondences of the same identity by learning comprehensive representations for both modalities from a novel perspective. We construct a support set for each image (text) by using other images (texts) under the same identity and design a multi-head attentional fusion module to fuse the image (text) and its support set. The resulting enriched image and text features fuse information from multiple views, which are aligned to train a "richer" TIReID model with many-to-many correspondences. Since the support set is unavailable during inference, we propose to distill the knowledge learned by the "richer" model into a lightweight model for inference with a single image/text as input. The lightweight model focuses on semantic association and reasoning of multi-view information, which can generate a comprehensive representation containing multi-view information with only a single-view input to perform accurate text-to-image retrieval during inference. In particular, we use the intra-modal features and inter-modal semantic relations of the "richer" model to supervise the lightweight model to inherit its powerful capability. Extensive experiments demonstrate the effectiveness of LCR$^2$S, and it also achieves new state-of-the-art performance on three popular TIReID datasets.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Whole-brain-radiomics-for-clustered-federated-personalization-in-brain-tumor-segmentation"><a href="#Whole-brain-radiomics-for-clustered-federated-personalization-in-brain-tumor-segmentation" class="headerlink" title="Whole-brain radiomics for clustered federated personalization in brain tumor segmentation"></a>Whole-brain radiomics for clustered federated personalization in brain tumor segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11480">http://arxiv.org/abs/2310.11480</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthis Manthe, Stefan Duffner, Carole Lartizien</li>
<li>For: The paper focuses on mitigating the impact of statistical heterogeneity in federated learning for medical image segmentation.* Methods: The proposed method, called federated personalization, involves computing radiomic features and clustering analysis on each institution’s local dataset, followed by fine-tuning a global model using the clustered decentralized dataset.* Results: The proposed method was validated on the Federated Brain Tumor Segmentation 2022 Challenge dataset and showed improved performance compared to classical federated learning.Here are the three points in Simplified Chinese:* For: 本文针对适用于医疗影像分类的联合学习中的统计不确定性进行了研究，以提高联合学习的精度和效率。* Methods: 提议的方法是基于每个机构的本地数据进行调整，首先计算每个机构的对应数据，然后使用聚合分析将所有的特征向量转移到中央服务器，并使用每个中心 compute 的对应数据进行精度调整。* Results: 在适用于脑癌分类的 Federated Brain Tumor Segmentation 2022 Challenge 数据集上验证了提议的方法，并与传统的联合学习相比，获得了改善的性能。<details>
<summary>Abstract</summary>
Federated learning and its application to medical image segmentation have recently become a popular research topic. This training paradigm suffers from statistical heterogeneity between participating institutions' local datasets, incurring convergence slowdown as well as potential accuracy loss compared to classical training. To mitigate this effect, federated personalization emerged as the federated optimization of one model per institution. We propose a novel personalization algorithm tailored to the feature shift induced by the usage of different scanners and acquisition parameters by different institutions. This method is the first to account for both inter and intra-institution feature shift (multiple scanners used in a single institution). It is based on the computation, within each centre, of a series of radiomic features capturing the global texture of each 3D image volume, followed by a clustering analysis pooling all feature vectors transferred from the local institutions to the central server. Each computed clustered decentralized dataset (potentially including data from different institutions) then serves to finetune a global model obtained through classical federated learning. We validate our approach on the Federated Brain Tumor Segmentation 2022 Challenge dataset (FeTS2022). Our code is available at (https://github.com/MatthisManthe/radiomics_CFFL).
</details>
<details>
<summary>摘要</summary>
《联邦学习和它的医学图像分割应用已经在最近引起了广泛的研究兴趣。这种培训模式受到参与机构本地数据的统计差异的影响，会导致减速和溢出精度相比于传统培训。为了缓解这些效应，联邦个性化出现了，即在每个机构上进行联邦优化的一个模型。我们提出了一种新的个性化算法，专门针对不同扫描仪和获取参数导致的特征偏移。这种方法是首次考虑了多个机构的内部和外部特征偏移（多个扫描仪在同一个机构中使用）。它基于在每个中心计算的一系列各种激光特征，用于捕捉每个3D图像卷积的全局 текстура，然后对所有从本地机构传输到中央服务器的特征向量进行归一化分析。每个计算的归一化分析后的各个归一化分析结果（可能包括多个机构的数据）然后用于在经典联邦学习中进行精化。我们验证了我们的方法在2022年联邦大脑肿瘤分割挑战数据集（FeTS2022）上。我们的代码可以在（https://github.com/MatthisManthe/radiomics_CFFL）上获取。》Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Improving-Video-Deepfake-Detection-A-DCT-Based-Approach-with-Patch-Level-Analysis"><a href="#Improving-Video-Deepfake-Detection-A-DCT-Based-Approach-with-Patch-Level-Analysis" class="headerlink" title="Improving Video Deepfake Detection: A DCT-Based Approach with Patch-Level Analysis"></a>Improving Video Deepfake Detection: A DCT-Based Approach with Patch-Level Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11204">http://arxiv.org/abs/2310.11204</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Guarnera, Salvatore Manganello, Sebastiano Battiato</li>
<li>for: 这篇研究旨在开发一个可靠、快速且可解释的检测深伪 Multimedia 内容的算法，以应对现在广泛存在的深伪技术滥用问题。</li>
<li>methods: 这篇研究使用了I-frames来提高计算和分析的速度，并分析了个别帧中的背景、脸部、眼睛、鼻子和口部，以找出最有可能性的特征。使用了Discrete Cosine Transform (DCT)来提取Beta комponents，并将其用作标准分类器（例如k-NN、SVM等）的输入，以决定解决这问题的最有可能性的频率。</li>
<li>results: 实验结果显示，眼和口部区域是检测深伪 Multimedia 内容中最有可能性的特征，能够更加可靠地决定影片的性质。提出的方法具有分析性、快速且不需要大量的计算资源。<details>
<summary>Abstract</summary>
The term deepfake refers to all those multimedia contents that were synthetically altered or created from scratch through the use of generative models. This phenomenon has become widespread due to the use of increasingly accurate and efficient architectures capable of rendering manipulated content indistinguishable from real content. In order to fight the illicit use of this powerful technology, it has become necessary to develop algorithms able to distinguish synthetic content from real ones. In this study, a new algorithm for the detection of deepfakes in digital videos is presented, focusing on the main goal of creating a fast and explainable method from a forensic perspective. To achieve this goal, the I-frames were extracted in order to provide faster computation and analysis than approaches described in literature. In addition, to identify the most discriminating regions within individual video frames, the entire frame, background, face, eyes, nose, mouth, and face frame were analyzed separately. From the Discrete Cosine Transform (DCT), the Beta components were extracted from the AC coefficients and used as input to standard classifiers (e.g., k-NN, SVM, and others) in order to identify those frequencies most discriminative for solving the task in question. Experimental results obtained on the Faceforensics++ and Celeb-DF (v2) datasets show that the eye and mouth regions are those most discriminative and able to determine the nature of the video with greater reliability than the analysis of the whole frame. The method proposed in this study is analytical, fast and does not require much computational power.
</details>
<details>
<summary>摘要</summary>
deepfake 指的是通过生成模型制造或修改 multimedia 内容的所有内容。由于使用的生成模型不断改进，使得修改后的内容与原始内容难以区分，因此需要开发一种能够分辨真实内容和修改后的内容的算法。本研究提出了一种用于检测数字视频中的深伪内容的新算法，强调实现快速和可解释的方法。为了实现这一目标，我们提取了 I-frame，以便更快地计算和分析，而不是按照文献中所描述的方法。此外，我们还分析了每帧视频中最有可能区分的地方，包括背景、脸、眼睛、鼻子和口。从 discrete cosine transform (DCT) 中，我们提取了 AC 约束中的 Beta 成分，并将其作为输入给标准分类器（如 k-NN、SVM 等），以确定解决当前问题中最有可能的频率。实验结果表明，脸部和口部是最有可能的区分点，能够更加可靠地判断视频的性质，而不是分析整个帧。该方法具有分析性、快速和不需要大量计算资源的优点。
</details></li>
</ul>
<hr>
<h2 id="Sparse-Multi-Object-Render-and-Compare"><a href="#Sparse-Multi-Object-Render-and-Compare" class="headerlink" title="Sparse Multi-Object Render-and-Compare"></a>Sparse Multi-Object Render-and-Compare</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11184">http://arxiv.org/abs/2310.11184</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florian Langer, Ignas Budvytis, Roberto Cipolla</li>
<li>for: 这paper是为了解决单图像中物体的3D形态和姿态重建问题，这问题在机器人、增强现实和数字内容创建等领域都是非常重要的。</li>
<li>methods: 这paper使用了一种新的网络架构 called Multi-SPARC，它可以同时对多个检测到的物体进行CAD模型的对接。</li>
<li>results: 相比单视图方法，这paper在ScanNet dataset上达到了状态机器人性的表现，即从31.8%提高到40.3%的实例对接精度。<details>
<summary>Abstract</summary>
Reconstructing 3D shape and pose of static objects from a single image is an essential task for various industries, including robotics, augmented reality, and digital content creation. This can be done by directly predicting 3D shape in various representations or by retrieving CAD models from a database and predicting their alignments. Directly predicting 3D shapes often produces unrealistic, overly smoothed or tessellated shapes. Retrieving CAD models ensures realistic shapes but requires robust and accurate alignment. Learning to directly predict CAD model poses from image features is challenging and inaccurate. Works, such as ROCA, compute poses from predicted normalised object coordinates which can be more accurate but are susceptible to systematic failure. SPARC demonstrates that following a ''render-and-compare'' approach where a network iteratively improves upon its own predictions achieves accurate alignments. Nevertheless, it performs individual CAD alignment for every object detected in an image. This approach is slow when applied to many objects as the time complexity increases linearly with the number of objects and can not learn inter-object relations. Introducing a new network architecture Multi-SPARC we learn to perform CAD model alignments for multiple detected objects jointly. Compared to other single-view methods we achieve state-of-the-art performance on the challenging real-world dataset ScanNet. By improving the instance alignment accuracy from 31.8% to 40.3% we perform similar to state-of-the-art multi-view methods.
</details>
<details>
<summary>摘要</summary>
重建静止物体的3D形状和姿势从单个图像中是许多领域的关键任务，包括机器人、增强现实和数字内容创建。这可以通过直接预测3D形状或从数据库中检索CAD模型并预测其对齐来完成。直接预测3D形状常常生成不真实、过度缩短或分割的形状。从数据库中检索CAD模型可以保证真实的形状，但需要稳定和准确的对齐。学习直接从图像特征中预测CAD模型姿势是困难且不准确。ROCA等方法计算姿势从预测的 нормализованobject坐标，可以更准确但容易系统性失败。SPARC示例了一个“render-and-compare”方法，其中网络在自己的预测基础上进行多次改进，可以实现准确的对齐。然而，它每个图像中检测到的对象都进行个别CAD对齐，这会导致运行时间linearly增长与对象数量的线性关系，无法学习对象之间的关系。我们提出了一种新的网络架构 Multi-SPARC，可以同时对多个检测到的对象进行CAD模型对齐。与其他单视图方法相比，我们在真实的世界数据集ScanNet上 achieve state-of-the-art性能。我们从31.8%提高了实例对齐精度到40.3%，与多视图方法相当。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Pre-Training-Using-Masked-Autoencoders-for-ECG-Analysis"><a href="#Unsupervised-Pre-Training-Using-Masked-Autoencoders-for-ECG-Analysis" class="headerlink" title="Unsupervised Pre-Training Using Masked Autoencoders for ECG Analysis"></a>Unsupervised Pre-Training Using Masked Autoencoders for ECG Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11153">http://arxiv.org/abs/2310.11153</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guoxin Wang, Qingyuan Wang, Ganesh Neelakanta Iyer, Avishek Nag, Deepu John</li>
<li>for: 这篇论文的目的是提出一种基于masked autoencoder（MAE）的无监督预训技术，用于电气征ogram（ECG）信号的分析。</li>
<li>methods: 本论文使用的方法包括masked autoencoder（MAE）和任务特定的精致化。</li>
<li>results: 实验结果显示，使用本提案的方法可以在MITDB dataset上达到ECG预订分类任务的94.39%的精度。此外，该方法也在未见之数据中的分类性能比全监督方法更好。<details>
<summary>Abstract</summary>
Unsupervised learning methods have become increasingly important in deep learning due to their demonstrated large utilization of datasets and higher accuracy in computer vision and natural language processing tasks. There is a growing trend to extend unsupervised learning methods to other domains, which helps to utilize a large amount of unlabelled data. This paper proposes an unsupervised pre-training technique based on masked autoencoder (MAE) for electrocardiogram (ECG) signals. In addition, we propose a task-specific fine-tuning to form a complete framework for ECG analysis. The framework is high-level, universal, and not individually adapted to specific model architectures or tasks. Experiments are conducted using various model architectures and large-scale datasets, resulting in an accuracy of 94.39% on the MITDB dataset for ECG arrhythmia classification task. The result shows a better performance for the classification of previously unseen data for the proposed approach compared to fully supervised methods.
</details>
<details>
<summary>摘要</summary>
《深度学习中的无监督学习方法在最近几年变得越来越重要，因为它们在计算机视觉和自然语言处理任务中的精度高于监督学习方法。随着这些方法的扩展到其他领域，可以利用大量的无标签数据。这篇论文提出了基于屏蔽自动编码器（MAE）的无监督预训练技术，用于电cardiogram（ECG）信号分析。此外，我们还提出了任务特定的细化，以形成一个完整的ECG分析框架。这个框架是高级、通用、不具体适应特定的模型结构或任务。在各种模型结构和大规模数据集上进行了实验，实现了MITDB数据集上ECG动力痕迹分类任务的准确率为94.39%。结果显示，提posed方法对于处理前未见数据的分类表现更好于完全监督方法。》Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="BayesDiff-Estimating-Pixel-wise-Uncertainty-in-Diffusion-via-Bayesian-Inference"><a href="#BayesDiff-Estimating-Pixel-wise-Uncertainty-in-Diffusion-via-Bayesian-Inference" class="headerlink" title="BayesDiff: Estimating Pixel-wise Uncertainty in Diffusion via Bayesian Inference"></a>BayesDiff: Estimating Pixel-wise Uncertainty in Diffusion via Bayesian Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11142">http://arxiv.org/abs/2310.11142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siqi Kou, Lei Gan, Dequan Wang, Chongxuan Li, Zhijie Deng</li>
<li>for: 提高 diffusion 模型生成图像质量</li>
<li>methods: 基于 bayesian 推断，计算扩散过程中像素层次的不确定性</li>
<li>results: 能够减少低质量图像，并帮助改进成功图像和修正失败图像中的缺陷<details>
<summary>Abstract</summary>
Diffusion models have impressive image generation capability, but low-quality generations still exist, and their identification remains challenging due to the lack of a proper sample-wise metric. To address this, we propose BayesDiff, a pixel-wise uncertainty estimator for generations from diffusion models based on Bayesian inference. In particular, we derive a novel uncertainty iteration principle to characterize the uncertainty dynamics in diffusion, and leverage the last-layer Laplace approximation for efficient Bayesian inference. The estimated pixel-wise uncertainty can not only be aggregated into a sample-wise metric to filter out low-fidelity images but also aids in augmenting successful generations and rectifying artifacts in failed generations in text-to-image tasks. Extensive experiments demonstrate the efficacy of BayesDiff and its promise for practical applications.
</details>
<details>
<summary>摘要</summary>
Diffusion模型具有吸引人的图像生成能力，但低质量生成仍然存在，其标识仍然困难由于缺乏适当的样本级度指标。为解决这个问题，我们提出了 BayesDiff，一种基于泛函推理的像素级uncertainty估计器 дляDiffusion模型。具体来说，我们 derivate了一种新的uncertainty迭代原理来描述Diffusion中的uncertainty动态，并利用最后层拉пла斯批处理来实现高效的泛函推理。测试表明，BayesDiff可以不仅将像素级uncertainty聚合成样本级度指标来滤除低准确图像，还可以帮助改善成功生成的图像和修复失败生成的瑕疵。在文本到图像任务中，BayesDiff展示了其效果和实际应用潜力。
</details></li>
</ul>
<hr>
<h2 id="Super-resolution-of-histopathological-frozen-sections-via-deep-learning-preserving-tissue-structure"><a href="#Super-resolution-of-histopathological-frozen-sections-via-deep-learning-preserving-tissue-structure" class="headerlink" title="Super resolution of histopathological frozen sections via deep learning preserving tissue structure"></a>Super resolution of histopathological frozen sections via deep learning preserving tissue structure</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11112">http://arxiv.org/abs/2310.11112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elad Yoshai, Gil Goldinger, Miki Haifler, Natan T. Shaked</li>
<li>for:  histopathological frozen sections imaging, with a focus on achieving better distortion measures and reducing the risk of diagnostic misinterpretation.</li>
<li>methods:  deep-learning architecture that leverages loss functions in the frequency domain to generate high-resolution images while preserving critical image details.</li>
<li>results:  significant improvements in terms of Structural Similarity Index (SSIM) and Peak Signal-to-Noise Ratio (PSNR), as well as the preservation of details lost in low-resolution frozen-section images, which can affect pathologists’ clinical decisions.<details>
<summary>Abstract</summary>
Histopathology plays a pivotal role in medical diagnostics. In contrast to preparing permanent sections for histopathology, a time-consuming process, preparing frozen sections is significantly faster and can be performed during surgery, where the sample scanning time should be optimized. Super-resolution techniques allow imaging the sample in lower magnification and sparing scanning time. In this paper, we present a new approach to super resolution for histopathological frozen sections, with focus on achieving better distortion measures, rather than pursuing photorealistic images that may compromise critical diagnostic information. Our deep-learning architecture focuses on learning the error between interpolated images and real images, thereby it generates high-resolution images while preserving critical image details, reducing the risk of diagnostic misinterpretation. This is done by leveraging the loss functions in the frequency domain, assigning higher weights to the reconstruction of complex, high-frequency components. In comparison to existing methods, we obtained significant improvements in terms of Structural Similarity Index (SSIM) and Peak Signal-to-Noise Ratio (PSNR), as well as indicated details that lost in the low-resolution frozen-section images, affecting the pathologist's clinical decisions. Our approach has a great potential in providing more-rapid frozen-section imaging, with less scanning, while preserving the high resolution in the imaged sample.
</details>
<details>
<summary>摘要</summary>
Our deep-learning architecture is designed to learn the error between interpolated images and real images, generating high-resolution images while preserving critical image details. We leverage loss functions in the frequency domain, assigning higher weights to the reconstruction of complex, high-frequency components. Compared to existing methods, our approach achieves significant improvements in terms of Structural Similarity Index (SSIM) and Peak Signal-to-Noise Ratio (PSNR), as well as reveals details that were lost in low-resolution frozen-section images, which can affect the pathologist's clinical decisions.Our approach has great potential in providing rapid frozen-section imaging with less scanning, while preserving the high resolution of the imaged sample. This can improve the accuracy of medical diagnosis and treatment.
</details></li>
</ul>
<hr>
<h2 id="3D-Structure-guided-Network-for-Tooth-Alignment-in-2D-Photograph"><a href="#3D-Structure-guided-Network-for-Tooth-Alignment-in-2D-Photograph" class="headerlink" title="3D Structure-guided Network for Tooth Alignment in 2D Photograph"></a>3D Structure-guided Network for Tooth Alignment in 2D Photograph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11106">http://arxiv.org/abs/2310.11106</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/douyl/2DToothAlignment">https://github.com/douyl/2DToothAlignment</a></li>
<li>paper_authors: Yulong Dou, Lanzhuju Mei, Dinggang Shen, Zhiming Cui</li>
<li>for: 这篇论文的目的是提供一个基于2D图像空间的牙齿协调网络，用于 dentist-patient 沟通和鼓励病人接受 ortodontic 治疗。</li>
<li>methods: 这篇论文使用了3D intra-oral scanning models 收集在 clinic 中，并使用了一个对应关系学习模型来将预先和后 ortodontic 治疗的3D 牙齿结构与2D 牙齿 outline 进行映射。然后，使用一个演化模型来将牙齿 outline 调整为具有美观和排列的牙齿图像。</li>
<li>results: 这篇论文的结果显示了该网络在不同的 facial photographs 上的优秀表现和强大应用性，并且可以帮助 dentist 快速地生成一个美观和排列的牙齿图像，以便更好地与病人沟通和鼓励病人接受 ortodontic 治疗。<details>
<summary>Abstract</summary>
Orthodontics focuses on rectifying misaligned teeth (i.e., malocclusions), affecting both masticatory function and aesthetics. However, orthodontic treatment often involves complex, lengthy procedures. As such, generating a 2D photograph depicting aligned teeth prior to orthodontic treatment is crucial for effective dentist-patient communication and, more importantly, for encouraging patients to accept orthodontic intervention. In this paper, we propose a 3D structure-guided tooth alignment network that takes 2D photographs as input (e.g., photos captured by smartphones) and aligns the teeth within the 2D image space to generate an orthodontic comparison photograph featuring aesthetically pleasing, aligned teeth. Notably, while the process operates within a 2D image space, our method employs 3D intra-oral scanning models collected in clinics to learn about orthodontic treatment, i.e., projecting the pre- and post-orthodontic 3D tooth structures onto 2D tooth contours, followed by a diffusion model to learn the mapping relationship. Ultimately, the aligned tooth contours are leveraged to guide the generation of a 2D photograph with aesthetically pleasing, aligned teeth and realistic textures. We evaluate our network on various facial photographs, demonstrating its exceptional performance and strong applicability within the orthodontic industry.
</details>
<details>
<summary>摘要</summary>
Orthodontics 专注于 corrections 不对称牙齿（即 malocclusion），影响咀嚼功能和美观。然而，orthodontic 治疗经常包括复杂、长时间的过程。因此，生成一张显示牙齿调整后的2D照片是关键的，以便dentist和病人之间有效沟通，更重要的是，使病人accept orthodontic intervention。在这篇论文中，我们提议一个基于3D结构的牙齿调整网络，输入2D照片（例如，由智能手机拍摄的照片），并将牙齿在2D图像空间中调整，生成一张 featuring 美观、调整后的牙齿的orthodontic comparison照片。需要注意的是，我们的过程在2D图像空间中进行，但我们使用了3D intra-oral scanning模型，收集在临床中，以学习orthodontic treatment。具体来说，我们将预 orthodontic 和后 orthodontic 3D 牙齿结构投影到2D 牙齿轮廓上，然后使用一种扩散模型来学习 mapping 关系。最后，我们使用了调整后的牙齿轮廓来指导生成一张 featuring 美观、调整后的牙齿和实际 Texture的2D照片。我们对多张人脸照片进行了评估，并证明了我们的网络在orthodontic 行业中表现出色，有强大的应用前景。
</details></li>
</ul>
<hr>
<h2 id="Generalizability-of-CNN-Architectures-for-Face-Morph-Presentation-Attack"><a href="#Generalizability-of-CNN-Architectures-for-Face-Morph-Presentation-Attack" class="headerlink" title="Generalizability of CNN Architectures for Face Morph Presentation Attack"></a>Generalizability of CNN Architectures for Face Morph Presentation Attack</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11105">http://arxiv.org/abs/2310.11105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sherko R. HmaSalah, Aras Asaad</li>
<li>for: 防止犯罪分子使用假识别信息越境</li>
<li>methods: 使用Convolutional Neural Network (CNN)模型进行人脸识别，并 investigate CNN模型在各种数据集上的泛化能力</li>
<li>results: InceptionResNet-v2模型在多个数据集上表现出最好的泛化能力，并在人脸识别 task 中获得了最高的性能<details>
<summary>Abstract</summary>
Automatic border control systems are wide spread in modern airports worldwide. Morphing attacks on face biometrics is a serious threat that undermines the security and reliability of face recognition systems deployed in airports and border controls. Therefore, developing a robust Machine Learning (ML) system is necessary to prevent criminals crossing borders with fake identifications especially since it has been shown that security officers cannot detect morphs better than machines. In this study, we investigate the generalization power of Convolutional Neural Network (CNN) architectures against morphing attacks. The investigation utilizes 5 distinct CNNs namely ShuffleNet, DenseNet201, VGG16, EffecientNet-B0 and InceptionResNet-v2. Each CNN architecture represents a well-known family of CNN models in terms of number of parameters, architectural design and performance across various computer vision applications. To ensure robust evaluation, we employ 4 different datasets (Utrecht, London, Defacto and KurdFace) that contain a diverse range of digital face images which cover variations in ethnicity, gender, age, lighting condition and camera setting. One of the fundamental concepts of ML system design is the ability to generalize effectively to previously unseen data, hence not only we evaluate the performance of CNN models within individual datasets but also explore their performance across combined datasets and investigating each dataset in testing phase only. Experimental results on more than 8 thousand images (genuine and morph) from the 4 datasets show that InceptionResNet-v2 generalizes better to unseen data and outperforms the other 4 CNN models.
</details>
<details>
<summary>摘要</summary>
现代机场中的自动边境控制系统广泛应用。但 morphing 攻击对于面部biometrics 是一种严重的威胁，这会使面 recognition 系统在机场和边境控制中受到影响。为了防止罪犯使用假身份证件越境，需要开发一个可靠的机器学习（ML）系统。在这项研究中，我们研究了 CNN 架构对 morphing 攻击的普适性。我们使用 5 种不同的 CNN 模型，即 ShuffleNet、DenseNet201、VGG16、EfficientNet-B0 和 InceptionResNet-v2。每种 CNN 模型都代表了不同的参数量、架构设计和在不同计算机视觉应用中的性能。为了有效评估，我们使用 4 个不同的数据集（UTrecht、London、Defacto 和 KurdFace），这些数据集包含了不同的民族、性别、年龄、照明条件和摄像头设置。 ML 系统设计的一个基本原则是能够有效地普退到未见数据，因此我们不仅在单个数据集中评估 CNN 模型的性能，还在将数据集组合起来评估它们的总体性能。实验结果表明，InceptionResNet-v2 在未见数据中普退性能最好，并且在4个数据集中的测试阶段也表现出色，超过其他 4 种 CNN 模型。
</details></li>
</ul>
<hr>
<h2 id="SODA-Robust-Training-of-Test-Time-Data-Adaptors"><a href="#SODA-Robust-Training-of-Test-Time-Data-Adaptors" class="headerlink" title="SODA: Robust Training of Test-Time Data Adaptors"></a>SODA: Robust Training of Test-Time Data Adaptors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11093">http://arxiv.org/abs/2310.11093</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tmlr-group/soda">https://github.com/tmlr-group/soda</a></li>
<li>paper_authors: Zige Wang, Yonggang Zhang, Zhen Fang, Long Lan, Wenjing Yang, Bo Han</li>
<li>For: The paper aims to mitigate the performance degradation caused by distribution shifts in machine learning models, specifically by adapting models deployed to test distributions.* Methods: The paper proposes a method called pseudo-label-robust data adaptation (SODA) that utilizes zeroth-order optimization (ZOO) to train a data adaptor to adapt the test data to fit the deployed models. SODA addresses the issue of unreliable gradients in ZOO by using high-confidence predicted labels as reliable labels to optimize the data adaptor.* Results: The paper shows that SODA can significantly enhance the performance of deployed models in the presence of distribution shifts without requiring access to model parameters. Empirical results indicate that SODA can improve the performance of data adaptation.Here are the three key points in Simplified Chinese text:* For: 这个论文的目标是解决机器学习模型在分布Shift时的性能下降问题，specifically by adapting deployed models to test distributions.* Methods: 论文提出了一种方法called pseudo-label-robust data adaptation (SODA)，which utilizes zeroth-order optimization (ZOO) to train a data adaptor to adapt the test data to fit the deployed models. SODA Addresses the issue of unreliable gradients in ZOO by using high-confidence predicted labels as reliable labels to optimize the data adaptor.* Results: 论文显示SODA可以Significantly enhance deployed models在分布Shift情况下的性能，without requiring access to model parameters. Empirical results indicate that SODA can improve the performance of data adaptation.<details>
<summary>Abstract</summary>
Adapting models deployed to test distributions can mitigate the performance degradation caused by distribution shifts. However, privacy concerns may render model parameters inaccessible. One promising approach involves utilizing zeroth-order optimization (ZOO) to train a data adaptor to adapt the test data to fit the deployed models. Nevertheless, the data adaptor trained with ZOO typically brings restricted improvements due to the potential corruption of data features caused by the data adaptor. To address this issue, we revisit ZOO in the context of test-time data adaptation. We find that the issue directly stems from the unreliable estimation of the gradients used to optimize the data adaptor, which is inherently due to the unreliable nature of the pseudo-labels assigned to the test data. Based on this observation, we propose pseudo-label-robust data adaptation (SODA) to improve the performance of data adaptation. Specifically, SODA leverages high-confidence predicted labels as reliable labels to optimize the data adaptor with ZOO for label prediction. For data with low-confidence predictions, SODA encourages the adaptor to preserve data information to mitigate data corruption. Empirical results indicate that SODA can significantly enhance the performance of deployed models in the presence of distribution shifts without requiring access to model parameters.
</details>
<details>
<summary>摘要</summary>
适应已部署的模型可以减轻由分布shift引起的性能下降。然而，隐私问题可能使模型参数无法访问。一种有 promise的方法是使用零次优化（ZOO）来训练一个数据适应器，以适应已部署的模型。然而，通常情况下，ZOO 训练的数据适应器 Typically brings restricted improvements due to the potential corruption of data features caused by the data adaptor。为Address this issue, we revisit ZOO in the context of test-time data adaptation. We find that the issue directly stems from the unreliable estimation of the gradients used to optimize the data adaptor, which is inherently due to the unreliable nature of the pseudo-labels assigned to the test data。 Based on this observation, we propose pseudo-label-robust data adaptation (SODA) to improve the performance of data adaptation。 Specifically, SODA leverages high-confidence predicted labels as reliable labels to optimize the data adaptor with ZOO for label prediction。 For data with low-confidence predictions, SODA encourages the adaptor to preserve data information to mitigate data corruption。 Empirical results indicate that SODA can significantly enhance the performance of deployed models in the presence of distribution shifts without requiring access to model parameters。
</details></li>
</ul>
<hr>
<h2 id="DORec-Decomposed-Object-Reconstruction-Utilizing-2D-Self-Supervised-Features"><a href="#DORec-Decomposed-Object-Reconstruction-Utilizing-2D-Self-Supervised-Features" class="headerlink" title="DORec: Decomposed Object Reconstruction Utilizing 2D Self-Supervised Features"></a>DORec: Decomposed Object Reconstruction Utilizing 2D Self-Supervised Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11092">http://arxiv.org/abs/2310.11092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Wu, Sicheng Li, Sihui Ji, Yue Wang, Rong Xiong, Yiyi Liao</li>
<li>for: 提高对复杂背景下对象的分解和重建的精度</li>
<li>methods: 基于神经网络隐式表示的Decomposed Object Reconstruction（DORec）网络，利用2D自动学习的自然特征进行分解</li>
<li>results: 在多个 dataset 上实现对eground object的高精度分解和重建<details>
<summary>Abstract</summary>
Decomposing a target object from a complex background while reconstructing is challenging. Most approaches acquire the perception for object instances through the use of manual labels, but the annotation procedure is costly. The recent advancements in 2D self-supervised learning have brought new prospects to object-aware representation, yet it remains unclear how to leverage such noisy 2D features for clean decomposition. In this paper, we propose a Decomposed Object Reconstruction (DORec) network based on neural implicit representations. Our key idea is to transfer 2D self-supervised features into masks of two levels of granularity to supervise the decomposition, including a binary mask to indicate the foreground regions and a K-cluster mask to indicate the semantically similar regions. These two masks are complementary to each other and lead to robust decomposition. Experimental results show the superiority of DORec in segmenting and reconstructing the foreground object on various datasets.
</details>
<details>
<summary>摘要</summary>
分解一个目标对象从复杂背景中分离，而重建时也是一项挑战。大多数方法通过使用手动标注来获得对象实例的感知，但标注过程很昂贵。现代2D自助学习技术的发展带来了新的可能性，但是如何利用这些噪音2D特征来获得清晰的分解仍然是一个未知。本文提出了基于神经无限表示的分解对象网络（DORec），我们的关键想法是将2D自助学习特征转换成两级划分的mask，包括一个二进制划分用于指示前景区域，以及一个K-集群划分用于指示相似区域。这两个划分是相互补偿的，导致了稳定的分解。实验结果表明DORec在不同的数据集上 segment和重建前景对象表现出色。
</details></li>
</ul>
<hr>
<h2 id="United-We-Stand-Using-Epoch-wise-Agreement-of-Ensembles-to-Combat-Overfit"><a href="#United-We-Stand-Using-Epoch-wise-Agreement-of-Ensembles-to-Combat-Overfit" class="headerlink" title="United We Stand: Using Epoch-wise Agreement of Ensembles to Combat Overfit"></a>United We Stand: Using Epoch-wise Agreement of Ensembles to Combat Overfit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11077">http://arxiv.org/abs/2310.11077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Uri Stern, Daniel Shwartz, Daphna Weinshall</li>
<li>for: 该论文的目的是解决深度神经网络在图像分类任务中遇到的过拟合问题，提出了一种新的深度网络集成预测方法来避免过拟合。</li>
<li>methods: 该论文使用了一种新的集成预测方法，该方法基于论文中提出的一种回归模型的预测结果，其中预测结果表明过拟合时，类别分类器的变异度会增加。</li>
<li>results: 在多个图像和文本分类 dataset 上，该方法可以减少过拟合导致的generalization下降，并且在一些情况下，甚至超越了使用 early stopping 的性能。该方法易于实现，可以与任何训练方案和架构结合使用，无需额外的特殊知识。<details>
<summary>Abstract</summary>
Deep neural networks have become the method of choice for solving many image classification tasks, largely because they can fit very complex functions defined over raw images. The downside of such powerful learners is the danger of overfitting the training set, leading to poor generalization, which is usually avoided by regularization and "early stopping" of the training. In this paper, we propose a new deep network ensemble classifier that is very effective against overfit. We begin with the theoretical analysis of a regression model, whose predictions - that the variance among classifiers increases when overfit occurs - is demonstrated empirically in deep networks in common use. Guided by these results, we construct a new ensemble-based prediction method designed to combat overfit, where the prediction is determined by the most consensual prediction throughout the training. On multiple image and text classification datasets, we show that when regular ensembles suffer from overfit, our method eliminates the harmful reduction in generalization due to overfit, and often even surpasses the performance obtained by early stopping. Our method is easy to implement, and can be integrated with any training scheme and architecture, without additional prior knowledge beyond the training set. Accordingly, it is a practical and useful tool to overcome overfit.
</details>
<details>
<summary>摘要</summary>
深度神经网络已成为许多图像分类任务的方法选择，主要是因为它们可以适应非常复杂的图像函数。但是这些强大的学习者也存在过拟合风险，导致泛化性差，通常通过规范和"早停止"等方法来避免。在这篇论文中，我们提出了一种新的深度网络集成分类器，可以很好地避免过拟合。我们从理论分析中开始，对于过拟合情况下的回归模型，其预测结果表明，过拟合时，类ifier的差异量会增加。基于这些结果，我们构建了一种新的集成预测方法，通过在训练过程中确定最一致的预测来对抗过拟合。在多个图像和文本分类 dataset 上，我们证明了，当常见集成遭到过拟合时，我们的方法可以消除过拟合导致的泛化性下降，并经常超越通过早停止获得的性能。我们的方法易于实现，可以与任何训练方案和架构结合使用，无需额外的优先知识，只需要训练集。因此，它是一种实用和有用的工具，可以解决过拟合问题。
</details></li>
</ul>
<hr>
<h2 id="k-t-CLAIR-Self-Consistency-Guided-Multi-Prior-Learning-for-Dynamic-Parallel-MR-Image-Reconstruction"><a href="#k-t-CLAIR-Self-Consistency-Guided-Multi-Prior-Learning-for-Dynamic-Parallel-MR-Image-Reconstruction" class="headerlink" title="$k$-$t$ CLAIR: Self-Consistency Guided Multi-Prior Learning for Dynamic Parallel MR Image Reconstruction"></a>$k$-$t$ CLAIR: Self-Consistency Guided Multi-Prior Learning for Dynamic Parallel MR Image Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11050">http://arxiv.org/abs/2310.11050</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lpzhang/ktCLAIR">https://github.com/lpzhang/ktCLAIR</a></li>
<li>paper_authors: Liping Zhang, Weitian Chen</li>
<li>for: 用于快速推断心脏疾病的临床诊断</li>
<li>methods: 使用自适应启发式多优先学习框架$k$-$t$CLAIR，利用高度强化的数据来推导动态平行MRI重建</li>
<li>results: 实验结果表明，$k$-$t$CLAIR可以在心脏动态MRI重建中实现高质量的重建，并且与量化和质量方面的表现均有显著改善<details>
<summary>Abstract</summary>
Cardiac magnetic resonance imaging (CMR) has been widely used in clinical practice for the medical diagnosis of cardiac diseases. However, the long acquisition time hinders its development in real-time applications. Here, we propose a novel self-consistency guided multi-prior learning framework named $k$-$t$ CLAIR to exploit spatiotemporal correlations from highly undersampled data for accelerated dynamic parallel MRI reconstruction. The $k$-$t$ CLAIR progressively reconstructs faithful images by leveraging multiple complementary priors learned in the $x$-$t$, $x$-$f$, and $k$-$t$ domains in an iterative fashion, as dynamic MRI exhibits high spatiotemporal redundancy. Additionally, $k$-$t$ CLAIR incorporates calibration information for prior learning, resulting in a more consistent reconstruction. Experimental results on cardiac cine and T1W/T2W images demonstrate that $k$-$t$ CLAIR achieves high-quality dynamic MR reconstruction in terms of both quantitative and qualitative performance.
</details>
<details>
<summary>摘要</summary>
cardiac magnetic resonance imaging (CMR) 已经广泛应用在临床实践中用于医疗诊断心脏疾病。然而，长期获取时间限制了其在实时应用中的发展。我们提议一种新的自适应性导向多优先学习框架，名为 $k$-$t$ CLAIR，以利用高度减掉样本数据中的空间时间相关性进行加速的动态平行MRI重建。 $k$-$t$ CLAIR 逐渐重建准确的图像，利用在 $x$-$t$, $x$-$f$, 和 $k$-$t$ 领域中学习的多个补做先天知识，因为动态MRI在空间时间上具有高度相似性。此外， $k$-$t$ CLAIR 还包含了准确性信息 для先天学习，从而使得重建更加一致。实验结果表明， $k$-$t$ CLAIR 在心脏笔记和 T1W/T2W 图像上达到了高质量的动态MR重建， Both quantitative and qualitative performance。
</details></li>
</ul>
<hr>
<h2 id="Co-Learning-Semantic-aware-Unsupervised-Segmentation-for-Pathological-Image-Registration"><a href="#Co-Learning-Semantic-aware-Unsupervised-Segmentation-for-Pathological-Image-Registration" class="headerlink" title="Co-Learning Semantic-aware Unsupervised Segmentation for Pathological Image Registration"></a>Co-Learning Semantic-aware Unsupervised Segmentation for Pathological Image Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11040">http://arxiv.org/abs/2310.11040</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Liu, Shi Gu</li>
<li>for: 本研究旨在提出一种不需要标注数据的自动化肿瘤图像注册方法，以解决肿瘤图像注册问题中的焦点缺失和肿瘤图像异常扭曲问题。</li>
<li>methods: 本研究提出了一种基于生成、填充和注册（GIR）原理的自动化肿瘤图像注册方法，包括注册、分割和填充三个模块，通过同时训练这三个模块，以提高肿瘤图像分割和注册的精度。</li>
<li>results: 实验结果表明，提出的方法可以准确地注册肿瘤图像，并且可以在不同的成像模式下检测肿瘤。 code available at <a target="_blank" rel="noopener" href="https://github.com/brain-intelligence-lab/GIRNet%E3%80%82">https://github.com/brain-intelligence-lab/GIRNet。</a><details>
<summary>Abstract</summary>
The registration of pathological images plays an important role in medical applications. Despite its significance, most researchers in this field primarily focus on the registration of normal tissue into normal tissue. The negative impact of focal tissue, such as the loss of spatial correspondence information and the abnormal distortion of tissue, are rarely considered. In this paper, we propose GIRNet, a novel unsupervised approach for pathological image registration by incorporating segmentation and inpainting through the principles of Generation, Inpainting, and Registration (GIR). The registration, segmentation, and inpainting modules are trained simultaneously in a co-learning manner so that the segmentation of the focal area and the registration of inpainted pairs can improve collaboratively. Overall, the registration of pathological images is achieved in a completely unsupervised learning framework. Experimental results on multiple datasets, including Magnetic Resonance Imaging (MRI) of T1 sequences, demonstrate the efficacy of our proposed method. Our results show that our method can accurately achieve the registration of pathological images and identify lesions even in challenging imaging modalities. Our unsupervised approach offers a promising solution for the efficient and cost-effective registration of pathological images. Our code is available at https://github.com/brain-intelligence-lab/GIRNet.
</details>
<details>
<summary>摘要</summary>
注册病理图像在医疗应用中扮演着重要的角色。尽管其重要性，大多数研究人员在这个领域主要关注normal tissue到normal tissue的注册。病理区域的负面影响，如损失的空间匹配信息和病理区域的异常扭曲，几乎不被考虑。在这篇论文中，我们提出了GIRNet，一种新的无监督方法，通过生成、填充和注册（GIR）原理，以帮助病理图像注册。注册、分割和填充模块在一起训练，以便在合作方式下提高病理区域的分割和注册匹配的精度。总之，我们的提出的方法可以在完全无监督学习框架下完成病理图像注册。我们的实验结果表明，我们的方法可以准确地注册病理图像，并在具有挑战性的成像模式下识别病理区域。我们的无监督方法可以提供高效、成本效果的病理图像注册解决方案。我们的代码可以在https://github.com/brain-intelligence-lab/GIRNet上获取。
</details></li>
</ul>
<hr>
<h2 id="Domain-Generalization-Using-Large-Pretrained-Models-with-Mixture-of-Adapters"><a href="#Domain-Generalization-Using-Large-Pretrained-Models-with-Mixture-of-Adapters" class="headerlink" title="Domain Generalization Using Large Pretrained Models with Mixture-of-Adapters"></a>Domain Generalization Using Large Pretrained Models with Mixture-of-Adapters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11031">http://arxiv.org/abs/2310.11031</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gyuseong Lee, Wooseok Jang, Jin Hyeon Kim, Jaewoo Jung, Seungryong Kim</li>
<li>for: 这个论文的目的是提出一种可以实现执行环境中模型的稳定性和可靠性的预测模型，尤其是在域别测量（Domain Generalization，DG）任务中。</li>
<li>methods: 这个论文使用了parameter-efficient fine-tuning（PEFT）方法，并且在PEFT中使用了束缚器（adapter）来实现更好的稳定性和可靠性。另外，这个论文还提出了一种mixture-of-expert（MoA）方法，这是一种将多个束缚器（adapter）组合起来，以提高模型的稳定性和可靠性。</li>
<li>results: 这个论文的结果显示，使用PEFT和MoA方法可以实现更好的稳定性和可靠性，并且可以减少模型的训练时间和计算成本。另外，这个论文还证明了，在域别测量任务中，使用PEFT和MoA方法可以提高模型的性能，并且可以减少模型的性能下降。<details>
<summary>Abstract</summary>
Learning a robust vision model despite large distribution shift is essential for model deployment in real-world settings. Especially, domain generalization (DG) algorithm aims to maintain the performance of a trained model on different distributions which were not seen during training. One of the most effective methods has been leveraging the already learned rich knowledge of large pretrained models. However, naively fine-tuning large models to DG tasks is often practically infeasible due to memory limitations, extensive time requirements for training, and the risk of learned knowledge deterioration. Recently, parameter-efficient fine-tuning (PEFT) methods have been proposed to reduce the high computational cost during training and efficiently adapt large models to downstream tasks. In this work, for the first time, we find that the use of adapters in PEFT methods not only reduce high computational cost during training but also serve as an effective regularizer for DG tasks. Surprisingly, a naive adapter implementation for large models achieve superior performance on common datasets. However, in situations of large distribution shifts, additional factors such as optimal amount of regularization due to the strength of distribution shifts should be considered for a sophisticated adapter implementation. To address this, we propose a mixture-of-expert based adapter fine-tuning method, dubbed as mixture-of-adapters (MoA). Specifically, we employ multiple adapters that have varying capacities, and by using learnable routers, we allocate each token to a proper adapter. By using both PEFT and MoA methods, we effectively alleviate the performance deterioration caused by distribution shifts and achieve state-of-the-art performance on diverse DG benchmarks.
</details>
<details>
<summary>摘要</summary>
学习一个强健的视觉模型，尤其是在不同的分布下进行模型部署，是实际应用中非常重要的。域外泛化（DG）算法的目标是保持训练后的模型在不同的分布下保持性能。然而，直接将大型模型精细调整到DG任务是实际上不可行，因为内存限制、训练时间的投入和模型学习知识的削弱。最近，参数效率的调整方法（PEFT）被提出，以减少训练时间的计算成本并有效地适应大型模型下推理任务。在这项工作中，我们发现了使用适应器不仅可以减少训练时间的计算成本，还可以作为DG任务的有效常规化。 surprisingly，一个简单的适应器实现对常用 datasets 表现出色。然而，在大分布差情况下，需要考虑适当的补偿因子，以适应强大的分布差。为此，我们提出了一种mixture-of-expert（MoA）适应器细化方法，其中我们采用多个适应器，每个适应器有不同的容量，并通过learnable routers来分配每个 токен到合适的适应器。通过使用 PEFT 和 MoA 方法，我们有效地减少分布差引起的性能下降，并在多种 DG  bencmarks 上达到了国际首席性表现。
</details></li>
</ul>
<hr>
<h2 id="NICE-Improving-Panoptic-Narrative-Detection-and-Segmentation-with-Cascading-Collaborative-Learning"><a href="#NICE-Improving-Panoptic-Narrative-Detection-and-Segmentation-with-Cascading-Collaborative-Learning" class="headerlink" title="NICE: Improving Panoptic Narrative Detection and Segmentation with Cascading Collaborative Learning"></a>NICE: Improving Panoptic Narrative Detection and Segmentation with Cascading Collaborative Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10975">http://arxiv.org/abs/2310.10975</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mr-neko/nice">https://github.com/mr-neko/nice</a></li>
<li>paper_authors: Haowei Wang, Jiayi Ji, Tianyu Guo, Yilong Yang, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji</li>
<li>for: 本研究旨在提出一个统一的和有效的框架，可以同时进行多个目标的图像识别和位置探索，并且可以将它们与文本描述相互对应。</li>
<li>methods: 我们提出了一个名为NICE的框架，它可以同时进行当地描述的识别和分类，并且可以通过将它们与文本描述相互对应，从而提高表现。在这个框架中，我们引入了两个协调模组，它们是协调调节（CGA）和本体驱动的本地化（BDL），它们负责分类和检测。我们还引入了一个将PNS和PND串接在一起的方法，使得它们可以相互对应，并且允许它们相互补偿以提高表现。</li>
<li>results: 我们的方法NICE可以轻松地超越所有现有的方法，实现4.1%的PND和2.9%的PNS表现。这些结果证明了我们的提出的协调学习策略的有效性。<details>
<summary>Abstract</summary>
Panoptic Narrative Detection (PND) and Segmentation (PNS) are two challenging tasks that involve identifying and locating multiple targets in an image according to a long narrative description. In this paper, we propose a unified and effective framework called NICE that can jointly learn these two panoptic narrative recognition tasks. Existing visual grounding tasks use a two-branch paradigm, but applying this directly to PND and PNS can result in prediction conflict due to their intrinsic many-to-many alignment property. To address this, we introduce two cascading modules based on the barycenter of the mask, which are Coordinate Guided Aggregation (CGA) and Barycenter Driven Localization (BDL), responsible for segmentation and detection, respectively. By linking PNS and PND in series with the barycenter of segmentation as the anchor, our approach naturally aligns the two tasks and allows them to complement each other for improved performance. Specifically, CGA provides the barycenter as a reference for detection, reducing BDL's reliance on a large number of candidate boxes. BDL leverages its excellent properties to distinguish different instances, which improves the performance of CGA for segmentation. Extensive experiments demonstrate that NICE surpasses all existing methods by a large margin, achieving 4.1% for PND and 2.9% for PNS over the state-of-the-art. These results validate the effectiveness of our proposed collaborative learning strategy. The project of this work is made publicly available at https://github.com/Mr-Neko/NICE.
</details>
<details>
<summary>摘要</summary>
通用和有效的框架NICE（Nice In Coordinate Embedding）可以同时学习多个目标的涵义和位置识别 task。在这篇论文中，我们提出了一种解决方案，即在Panoptic Narrative Detection（PND）和Panoptic Segmentation（PNS）任务之间进行协同学习。现有的视觉定位任务使用两支分支方法，但是直接应用这种方法于PND和PNS可能会导致预测冲突，因为它们具有内在的多对多对应性。为解决这个问题，我们引入了两个协同模块，即坐标导航集成（CGA）和坐标驱动本地化（BDL），负责分割和检测。我们将PNS和PND串联在一起，使得两个任务之间的对应关系自然地进行了协同学习。具体来说，CGA提供了分割的参考点，从而降低BDL的候选框数量的依赖性。BDL利用其优秀的性能来分辨不同的实例，从而提高CGA的分割性能。我们的实验结果表明，NICE比所有现有方法大幅超越，实现了PND4.1%和PNS2.9%的状态当前最佳性能。这些结果证明了我们提出的协同学习策略的有效性。NICE项目的代码可以在https://github.com/Mr-Neko/NICE上获取。
</details></li>
</ul>
<hr>
<h2 id="Tracking-and-Mapping-in-Medical-Computer-Vision-A-Review"><a href="#Tracking-and-Mapping-in-Medical-Computer-Vision-A-Review" class="headerlink" title="Tracking and Mapping in Medical Computer Vision: A Review"></a>Tracking and Mapping in Medical Computer Vision: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11475">http://arxiv.org/abs/2310.11475</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Schmidt, Omid Mohareri, Simon DiMaio, Michael Yip, Septimiu E. Salcudean<br>for:* 医疗图像分析领域的应用，包括诊断和手术指导。methods:* 使用摄像头进行跟踪和场景映射。results:* 提供了一个审查和概述当前领域的状态，包括最新的发展和趋势。Please note that the above information is in Simplified Chinese text.<details>
<summary>Abstract</summary>
As computer vision algorithms are becoming more capable, their applications in clinical systems will become more pervasive. These applications include diagnostics such as colonoscopy and bronchoscopy, guiding biopsies and minimally invasive interventions and surgery, automating instrument motion and providing image guidance using pre-operative scans. Many of these applications depend on the specific visual nature of medical scenes and require designing and applying algorithms to perform in this environment.   In this review, we provide an update to the field of camera-based tracking and scene mapping in surgery and diagnostics in medical computer vision. We begin with describing our review process, which results in a final list of 515 papers that we cover. We then give a high-level summary of the state of the art and provide relevant background for those who need tracking and mapping for their clinical applications. We then review datasets provided in the field and the clinical needs therein. Then, we delve in depth into the algorithmic side, and summarize recent developments, which should be especially useful for algorithm designers and to those looking to understand the capability of off-the-shelf methods. We focus on algorithms for deformable environments while also reviewing the essential building blocks in rigid tracking and mapping since there is a large amount of crossover in methods. Finally, we discuss the current state of the tracking and mapping methods along with needs for future algorithms, needs for quantification, and the viability of clinical applications in the field. We conclude that new methods need to be designed or combined to support clinical applications in deformable environments, and more focus needs to be put into collecting datasets for training and evaluation.
</details>
<details>
<summary>摘要</summary>
为了满足医疗系统中computer vision算法的应用 becoming more pervasive，这些应用包括诊断如colonoscopy和bronchoscopy、导引生物检查和微创入侵性手术、自动化工具动作并提供预操作扫描图像导航。许多这些应用需要特定的医疗场景的视觉特性，因此需要设计和应用算法以在这种环境中工作。在这篇评论中，我们提供了医疗计算机视觉中摄像机基于跟踪和场景映射的更新。我们开始介绍我们的评审过程，从而获得了515篇论文的最终列表。然后，我们提供了高级概述，并为需要跟踪和映射的价值读者提供了相关的背景信息。然后，我们评审了在领域中提供的数据集，并评估了临床应用中的临床需求。接着，我们深入探讨算法的方面，并总结了最近的进展，这将对算法设计者和需要了解摄像机基于跟踪和映射的方法来说特别有用。我们主要关注可变环境中的算法，并同时评估了基础建立的硬件跟踪和映射方法。最后，我们讨论了当前跟踪和映射方法的状况，以及未来需要的算法、评估量化和临床应用的可行性。我们结论认为，新的方法需要被设计或组合以支持临床应用，并更多的精力需要投入到训练和评估数据集的收集中。
</details></li>
</ul>
<hr>
<h2 id="Context-Aware-Meta-Learning"><a href="#Context-Aware-Meta-Learning" class="headerlink" title="Context-Aware Meta-Learning"></a>Context-Aware Meta-Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10971">http://arxiv.org/abs/2310.10971</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hallogameboy/MARU">https://github.com/hallogameboy/MARU</a></li>
<li>paper_authors: Christopher Fifty, Dennis Duan, Ronald G. Junkins, Ehsan Amid, Jure Leskovec, Christopher Ré, Sebastian Thrun</li>
<li>for: 这个论文旨在解决视觉模型在推理过程中学习新的概念的问题。</li>
<li>methods: 该论文提出了一种基于冻结预训练特征提取器的 meta-学习算法，可以在推理过程中不需要 fine-tuning 地学习新的视觉概念。</li>
<li>results: 在 8 个 meta-学习Benchmark 中，该算法可以无需 meta-训练或 fine-tuning 达到或超过 state-of-the-art 算法 P&gt;M&gt;F 的性能。<details>
<summary>Abstract</summary>
Large Language Models like ChatGPT demonstrate a remarkable capacity to learn new concepts during inference without any fine-tuning. However, visual models trained to detect new objects during inference have been unable to replicate this ability, and instead either perform poorly or require meta-training and/or fine-tuning on similar objects. In this work, we propose a meta-learning algorithm that emulates Large Language Models by learning new visual concepts during inference without fine-tuning. Our approach leverages a frozen pre-trained feature extractor, and analogous to in-context learning, recasts meta-learning as sequence modeling over datapoints with known labels and a test datapoint with an unknown label. On 8 out of 11 meta-learning benchmarks, our approach -- without meta-training or fine-tuning -- exceeds or matches the state-of-the-art algorithm, P>M>F, which is meta-trained on these benchmarks.
</details>
<details>
<summary>摘要</summary>
大语言模型如ChatGPT表现出了惊人的新概念学习能力，而无需任何精度调整。然而，用于检测新对象的视觉模型在推理时表现不佳，或者需要meta-training和/或精度调整。在这项工作中，我们提出了一种meta-学习算法，可以在推理时学习新的视觉概念，无需精度调整。我们的方法利用冻结的预训练特征提取器，类似于上下文学习，将meta-学习视为序列模型化，对已知标签的数据点和未知标签的测试数据点进行模型化。在11个meta-学习benchmark上，我们的方法，无需meta-training或精度调整，超过或等于状态的算法P>M>F，该算法在这些benchmark上进行meta-training。
</details></li>
</ul>
<hr>
<h2 id="MRI-brain-tumor-segmentation-using-informative-feature-vectors-and-kernel-dictionary-learning"><a href="#MRI-brain-tumor-segmentation-using-informative-feature-vectors-and-kernel-dictionary-learning" class="headerlink" title="MRI brain tumor segmentation using informative feature vectors and kernel dictionary learning"></a>MRI brain tumor segmentation using informative feature vectors and kernel dictionary learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10963">http://arxiv.org/abs/2310.10963</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seyedeh Mahya Mousavi, Mohammad Mostafavi<br>for: 这个论文是用于分类健康和癌症脑磁共振图像中的脑膜区域的方法。methods: 这个方法使用基于内生kernel字典学习算法来学习健康和癌症脑磁共振图像中的特征向量。results: 实验结果表明，提出的方法比其他已有方法更高的准确率和快速采样速度，可以快速完成分类任务，同时也可以减少训练时间和内存需求。<details>
<summary>Abstract</summary>
This paper presents a method based on a kernel dictionary learning algorithm for segmenting brain tumor regions in magnetic resonance images (MRI). A set of first-order and second-order statistical feature vectors are extracted from patches of size 3 * 3 around pixels in the brain MRI scans. These feature vectors are utilized to train two kernel dictionaries separately for healthy and tumorous tissues. To enhance the efficiency of the dictionaries and reduce training time, a correlation-based sample selection technique is developed to identify the most informative and discriminative subset of feature vectors. This technique aims to improve the performance of the dictionaries by selecting a subset of feature vectors that provide valuable information for the segmentation task. Subsequently, a linear classifier is utilized to distinguish between healthy and unhealthy pixels based on the learned dictionaries. The results demonstrate that the proposed method outperforms other existing methods in terms of segmentation accuracy and significantly reduces both the time and memory required, resulting in a remarkably fast training process.
</details>
<details>
<summary>摘要</summary>
这个论文提出了基于kernel字典学习算法的 brain 肿瘤区域分割方法，使用patches 的first-order和second-order统计特征向量从brain MRI扫描中提取特征向量，然后使用这些特征向量训练两个kernel字典，一个用于健康组，一个用于肿瘤组。为了提高字典的效率和减少训练时间，我们提出了一种基于相关性的样本选择技术，以选择最有价值和分类的特征向量，从而提高分类性能。接着，我们使用学习的字典和线性分类器来分割健康和肿瘤组。结果表明，提出的方法在分割精度和训练时间上具有明显的优势，比其他方法更高效。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Deep-Neural-Network-Training-Efficiency-and-Performance-through-Linear-Prediction"><a href="#Enhancing-Deep-Neural-Network-Training-Efficiency-and-Performance-through-Linear-Prediction" class="headerlink" title="Enhancing Deep Neural Network Training Efficiency and Performance through Linear Prediction"></a>Enhancing Deep Neural Network Training Efficiency and Performance through Linear Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10958">http://arxiv.org/abs/2310.10958</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hejie Ying, Mengmeng Song, Yaohong Tang, Shungen Xiao, Zimin Xiao</li>
<li>for: 提高深度神经网络（DNN）模型训练效率和性能。</li>
<li>methods: 基于DNN参数在训练过程中变化的规律发现可以预测DNN参数，并且采用Parameter Linear Prediction（PLP）方法进行预测。</li>
<li>results: 对于Vgg16、Resnet18和GoogLeNet等 represntative底层，通过比较Normal训练和提posed方法的结果，发现提posed方法能够在相同的训练条件和轮数下提高模型性能，具体的结果为CIFAR-100 dataset上的准确率提高约1%和top-1&#x2F;top-5错误降低约0.01。<details>
<summary>Abstract</summary>
Deep neural networks (DNN) have achieved remarkable success in various fields, including computer vision and natural language processing. However, training an effective DNN model still poses challenges. This paper aims to propose a method to optimize the training effectiveness of DNN, with the goal of improving model performance. Firstly, based on the observation that the DNN parameters change in certain laws during training process, the potential of parameter prediction for improving model training efficiency and performance is discovered. Secondly, considering the magnitude of DNN model parameters, hardware limitations and characteristics of Stochastic Gradient Descent (SGD) for noise tolerance, a Parameter Linear Prediction (PLP) method is exploit to perform DNN parameter prediction. Finally, validations are carried out on some representative backbones. Experiment results show that compare to the normal training ways, under the same training conditions and epochs, by employing proposed PLP method, the optimal model is able to obtain average about 1% accuracy improvement and 0.01 top-1/top-5 error reduction for Vgg16, Resnet18 and GoogLeNet based on CIFAR-100 dataset, which shown the effectiveness of the proposed method on different DNN structures, and validated its capacity in enhancing DNN training efficiency and performance.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）在不同领域取得了显著成功，包括计算机视觉和自然语言处理。然而，训练有效的DNN模型仍然存在挑战。本文旨在提出一种方法，以提高DNN训练效果并提高模型性能。首先，基于训练过程中DNN参数变化的规律，探讨可以通过预测参数来提高DNN训练效率和性能的潜在可能性。其次，考虑到DNN模型参数的大小、硬件限制和SGD算法对雷yy的耐受性，提出了一种基于PLP方法的DNN参数预测方法。最后，对一些代表性的背bone进行验证。实验结果显示，相比于常规训练方式，通过提议的PLP方法，在同等训练条件和轮数下，可以获得average约1%的准确率提高和0.01的top-1/top-5错误减少，这demonstrates the effectiveness of the proposed method on different DNN structures and validates its ability to enhance DNN training efficiency and performance.
</details></li>
</ul>
<hr>
<h2 id="Medical-Image-Segmentation-via-Sparse-Coding-Decoder"><a href="#Medical-Image-Segmentation-via-Sparse-Coding-Decoder" class="headerlink" title="Medical Image Segmentation via Sparse Coding Decoder"></a>Medical Image Segmentation via Sparse Coding Decoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10957">http://arxiv.org/abs/2310.10957</a></li>
<li>repo_url: None</li>
<li>paper_authors: Long Zeng, Kaigui Wu</li>
<li>for: 这篇论文主要探讨了如何将Transformer模型应用于医疗影像分类，以提高其捕捉长距离依赖关系的能力。</li>
<li>methods: 这篇论文提出了一种基于核心 sparse vector coding的decoder，named CASCSCDE，它使用核心 sparse vector coding来表示从encoder模组中获取的特征。</li>
<li>results: 实验结果显示，将CASCSCDE与TransUNet结合，可以实现Synapse benchmark上的表现提升，相比于TransUNet alone，提高了3.15%和1.16%的DICE和mIoU分数。<details>
<summary>Abstract</summary>
Transformers have achieved significant success in medical image segmentation, owing to its capability to capture long-range dependencies. Previous works incorporate convolutional layers into the encoder module of transformers, thereby enhancing their ability to learn local relationships among pixels. However, transformers may suffer from limited generalization capabilities and reduced robustness, attributed to the insufficient spatial recovery ability of their decoders. To address this issue, A convolution sparse vector coding based decoder is proposed , namely CAScaded multi-layer Convolutional Sparse vector Coding DEcoder (CASCSCDE), which represents features extracted by the encoder using sparse vectors. To prove the effectiveness of our CASCSCDE, The widely-used TransUNet model is chosen for the demonstration purpose, and the CASCSCDE is incorporated with TransUNet to establish the TransCASCSCDE architecture. Our experiments demonstrate that TransUNet with CASCSCDE significantly enhances performance on the Synapse benchmark, obtaining up to 3.15\% and 1.16\% improvements in DICE and mIoU scores, respectively. CASCSCDE opens new ways for constructing decoders based on convolutional sparse vector coding.
</details>
<details>
<summary>摘要</summary>
transformers在医学图像分割领域取得了重要成功，归功于它的长距离依赖关系捕捉能力。 précédentes works将 convolutional layers incorporated into the encoder module of transformers，以提高它们对像之间的本地关系学习能力。然而，transformers可能会受到有限的泛化能力和减少的稳定性影响，这是由于它们的解码器的空间恢复能力不够。为 Addressing this issue, a convolution sparse vector coding based decoder is proposed, namely CAScaded multi-layer Convolutional Sparse vector Coding DEcoder (CASCSCDE), which represents features extracted by the encoder using sparse vectors。To prove the effectiveness of our CASCSCDE, the widely-used TransUNet model is chosen for the demonstration purpose, and the CASCSCDE is incorporated with TransUNet to establish the TransCASCSCDE architecture。 our experiments show that TransUNet with CASCSCDE significantly enhances performance on the Synapse benchmark, obtaining up to 3.15% and 1.16% improvements in DICE and mIoU scores, respectively。 CASCSCDE opens new ways for constructing decoders based on convolutional sparse vector coding。
</details></li>
</ul>
<hr>
<h2 id="FusionU-Net-U-Net-with-Enhanced-Skip-Connection-for-Pathology-Image-Segmentation"><a href="#FusionU-Net-U-Net-with-Enhanced-Skip-Connection-for-Pathology-Image-Segmentation" class="headerlink" title="FusionU-Net: U-Net with Enhanced Skip Connection for Pathology Image Segmentation"></a>FusionU-Net: U-Net with Enhanced Skip Connection for Pathology Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10951">http://arxiv.org/abs/2310.10951</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zongyi-lee/fusionu-net">https://github.com/zongyi-lee/fusionu-net</a></li>
<li>paper_authors: Zongyi Li, Hongbing Lyu, Jun Wang</li>
<li>For: 本研究旨在提高路ологи影像分类任务中U-Net和其变种的性能，提出一种基于U-Net结构的新网络模型，即FusionU-Net。* Methods: FusionU-Net使用了一种基于U-Net结构的增强模块，该模块通过在不同层的Encoder和Decoder之间进行信息交换，以减少Semantic Gap。此外，我们采用了两轮交换机制，以完全考虑当前层输出之间的本地相关性和多层信息交换的需求。* Results: 我们在多个路ологи影像 dataset上进行了广泛的实验，并发现FusionU-Net在与其他竞争方法相比表现更好。我们认为我们的交换模块比现有网络的设计更有效，并且可以轻松地在其他网络中插入，以进一步提高模型性能。<details>
<summary>Abstract</summary>
In recent years, U-Net and its variants have been widely used in pathology image segmentation tasks. One of the key designs of U-Net is the use of skip connections between the encoder and decoder, which helps to recover detailed information after upsampling. While most variations of U-Net adopt the original skip connection design, there is semantic gap between the encoder and decoder that can negatively impact model performance. Therefore, it is important to reduce this semantic gap before conducting skip connection. To address this issue, we propose a new segmentation network called FusionU-Net, which is based on U-Net structure and incorporates a fusion module to exchange information between different skip connections to reduce semantic gaps. Unlike the other fusion modules in existing networks, ours is based on a two-round fusion design that fully considers the local relevance between adjacent encoder layer outputs and the need for bi-directional information exchange across multiple layers. We conducted extensive experiments on multiple pathology image datasets to evaluate our model and found that FusionU-Net achieves better performance compared to other competing methods. We argue our fusion module is more effective than the designs of existing networks, and it could be easily embedded into other networks to further enhance the model performance.
</details>
<details>
<summary>摘要</summary>
Unlike other fusion modules in existing networks, our fusion module is based on a two-round fusion design that fully considers the local relevance between adjacent encoder layer outputs and the need for bi-directional information exchange across multiple layers. We conducted extensive experiments on multiple pathology image datasets to evaluate our model and found that FusionU-Net achieves better performance compared to other competing methods. We argue that our fusion module is more effective than the designs of existing networks and could be easily embedded into other networks to further enhance model performance.
</details></li>
</ul>
<hr>
<h2 id="UNK-VQA-A-Dataset-and-A-Probe-into-Multi-modal-Large-Models’-Abstention-Ability"><a href="#UNK-VQA-A-Dataset-and-A-Probe-into-Multi-modal-Large-Models’-Abstention-Ability" class="headerlink" title="UNK-VQA: A Dataset and A Probe into Multi-modal Large Models’ Abstention Ability"></a>UNK-VQA: A Dataset and A Probe into Multi-modal Large Models’ Abstention Ability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10942">http://arxiv.org/abs/2310.10942</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/guoyang9/unk-vqa">https://github.com/guoyang9/unk-vqa</a></li>
<li>paper_authors: Yanyang Guo, Fangkai Jiao, Zhiqi Shen, Liqiang Nie, Mohan Kankanhalli</li>
<li>for: 本研究的目的是帮助视觉问答模型避免回答无法答案的问题，以建立更可靠的人工智能系统。</li>
<li>methods: 我们首先对现有数据进行了有意义的干扰，以使问题图像的 semantics 保持与原始不干扰分布的相似性。然后，我们对多 modal 大型模型进行了 Zero-和 few-shot 性能评估，并发现它们在我们的数据集上表现出了明显的局限性。最后，我们还提出了一种简单的方法来解决这些无法答案的问题。</li>
<li>results: 我们的数据集（UNK-VQA）可以用于提高视觉问答模型的拒绝能力，从而提高人工智能系统的可靠性。我们的研究可以帮助解决现有的问题，并为建立更可靠的人工智能系统提供一个重要的基础。<details>
<summary>Abstract</summary>
Teaching Visual Question Answering (VQA) models to refrain from answering unanswerable questions is necessary for building a trustworthy AI system. Existing studies, though have explored various aspects of VQA but somewhat ignored this particular attribute. This paper aims to bridge the research gap by contributing a comprehensive dataset, called UNK-VQA. The dataset is specifically designed to address the challenge of questions that models do not know. To this end, we first augment the existing data via deliberate perturbations on either the image or question. In specific, we carefully ensure that the question-image semantics remain close to the original unperturbed distribution. By this means, the identification of unanswerable questions becomes challenging, setting our dataset apart from others that involve mere image replacement. We then extensively evaluate the zero- and few-shot performance of several emerging multi-modal large models and discover their significant limitations when applied to our dataset. Additionally, we also propose a straightforward method to tackle these unanswerable questions. This dataset, we believe, will serve as a valuable benchmark for enhancing the abstention capability of VQA models, thereby leading to increased trustworthiness of AI systems. We have made the \href{https://github.com/guoyang9/UNK-VQA}{dataset} available to facilitate further exploration in this area.
</details>
<details>
<summary>摘要</summary>
教学视觉问答模型避免回答无法回答的问题是建立可信worthy AI系统的必要 условия。现有研究，虽曾经探讨了多种VQA方面的问题，但它们却有些忽略了这一特点。这篇论文的目的是填补这个研究漏洞，通过提供一个全面的UNK-VQA数据集来解决问题。这个数据集专门针对模型不知道的问题，我们通过对问题或图像进行意图的拟合来增强问题-图像的Semantics相似性，从而使模型很难以识别无法回答的问题。我们 THEN 进行了广泛的零、少射性性能评估，发现现有的多模式大型模型在我们的数据集上表现出了显著的局限性。此外，我们还提出了一种简单的方法来解决这些无法回答的问题。我们认为这个数据集会成为VQA模型增强其拒绝能力的重要benchmark，从而提高AI系统的可信worthiness。我们已经在github上分享了这个\href{https://github.com/guoyang9/UNK-VQA}{数据集，以便进一步的探索。}
</details></li>
</ul>
<hr>
<h2 id="Towards-Training-free-Open-world-Segmentation-via-Image-Prompting-Foundation-Models"><a href="#Towards-Training-free-Open-world-Segmentation-via-Image-Prompting-Foundation-Models" class="headerlink" title="Towards Training-free Open-world Segmentation via Image Prompting Foundation Models"></a>Towards Training-free Open-world Segmentation via Image Prompting Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10912">http://arxiv.org/abs/2310.10912</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lv Tang, Peng-Tao Jiang, Hao-Ke Xiao, Bo Li<br>for:This paper explores open-world segmentation using a novel approach called Image Prompt Segmentation (IPSeg), which leverages vision foundational models and image prompting techniques to segment target objects in input images without requiring exhaustive training sessions.methods:IPSeg utilizes a single image containing a subjective visual concept as a flexible prompt to query vision foundation models like DINOv2 and Stable Diffusion. The approach extracts robust features for the prompt image and input image, then matches the input representations to the prompt representations via a novel feature interaction module to generate point prompts highlighting target objects in the input image.results:Experiments on COCO, PASCAL VOC, and other datasets demonstrate IPSeg’s efficacy for flexible open-world segmentation using intuitive image prompts. The proposed method offers a more efficient and scalable solution for open-world segmentation compared to traditional training-based methods.<details>
<summary>Abstract</summary>
The realm of computer vision has witnessed a paradigm shift with the advent of foundational models, mirroring the transformative influence of large language models in the domain of natural language processing. This paper delves into the exploration of open-world segmentation, presenting a novel approach called Image Prompt Segmentation (IPSeg) that harnesses the power of vision foundational models. At the heart of IPSeg lies the principle of a training-free paradigm, which capitalizes on image prompting techniques. IPSeg utilizes a single image containing a subjective visual concept as a flexible prompt to query vision foundation models like DINOv2 and Stable Diffusion. Our approach extracts robust features for the prompt image and input image, then matches the input representations to the prompt representations via a novel feature interaction module to generate point prompts highlighting target objects in the input image. The generated point prompts are further utilized to guide the Segment Anything Model to segment the target object in the input image. The proposed method stands out by eliminating the need for exhaustive training sessions, thereby offering a more efficient and scalable solution. Experiments on COCO, PASCAL VOC, and other datasets demonstrate IPSeg's efficacy for flexible open-world segmentation using intuitive image prompts. This work pioneers tapping foundation models for open-world understanding through visual concepts conveyed in images.
</details>
<details>
<summary>摘要</summary>
computer vision 领域受到基础模型的洗礼，这对于自然语言处理领域中基础模型的转变产生了类似的影响。这篇论文探讨开放世界分割问题，提出了一种名为图像提示分 segmentation（IPSeg）的新方法。IPSeg 利用视觉基础模型如 DINOv2 和 Stable Diffusion 的力量，并且采用一种无需训练的方法。我们的方法使用一个包含主观视觉概念的单张图像作为灵活的提示，EXTRACT robust的特征于提示图像和输入图像，然后通过一种新的特征互动模块将输入表示与提示表示匹配，以生成指向目标对象的点提示。这些生成的点提示最后用于导引 Segment Anything Model 对输入图像中的目标对象进行分割。我们的方法不需要耗费大量的训练时间，因此更加高效和可扩展。实验结果表明，IPSeg 在 COCO、PASCAL VOC 和其他数据集上表现出色，用于开放世界中的灵活分割。这项工作开拓了基础模型在视觉领域中对开放世界理解的新途径。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/17/cs.CV_2023_10_17/" data-id="clogxf3nk00jm5xra9iph407l" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_10_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/17/cs.AI_2023_10_17/" class="article-date">
  <time datetime="2023-10-17T12:00:00.000Z" itemprop="datePublished">2023-10-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/17/cs.AI_2023_10_17/">cs.AI - 2023-10-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Learn-Your-Tokens-Word-Pooled-Tokenization-for-Language-Modeling"><a href="#Learn-Your-Tokens-Word-Pooled-Tokenization-for-Language-Modeling" class="headerlink" title="Learn Your Tokens: Word-Pooled Tokenization for Language Modeling"></a>Learn Your Tokens: Word-Pooled Tokenization for Language Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11628">http://arxiv.org/abs/2310.11628</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/avi-jit/etok">https://github.com/avi-jit/etok</a></li>
<li>paper_authors: Avijit Thawani, Saurabh Ghanekar, Xiaoyuan Zhu, Jay Pujara</li>
<li>for: 该文章是关于语言模型tokenization的研究，旨在提出一种“学习你的token”的方法，以优化语言模型的表示能力和性能。</li>
<li>methods: 该文章使用了一种“learn your tokens”的方法，将字节&#x2F;字符 pooling 到单词表示，然后将单词表示传递给主语言模型进行解码。</li>
<li>results:  compared to字节&#x2F;字符模型和子词模型，该文章的中等表达能力和速度的综合型tokenizer在下一个词预测任务中表现出色，特别是在罕见词术中表现出三十倍的提升。<details>
<summary>Abstract</summary>
Language models typically tokenize text into subwords, using a deterministic, hand-engineered heuristic of combining characters into longer surface-level strings such as 'ing' or whole words. Recent literature has repeatedly shown the limitations of such a tokenization strategy, particularly for documents not written in English and for representing numbers. On the other extreme, byte/character-level language models are much less restricted but suffer from increased sequence description lengths and a subsequent quadratic expansion in self-attention computation. Recent attempts to compress and limit these context lengths with fixed size convolutions is helpful but completely ignores the word boundary. This paper considers an alternative 'learn your tokens' scheme which utilizes the word boundary to pool bytes/characters into word representations, which are fed to the primary language model, before again decoding individual characters/bytes per word in parallel. We find that our moderately expressive and moderately fast end-to-end tokenizer outperform by over 300% both subwords and byte/character models over the intrinsic language modeling metric of next-word prediction across datasets. It particularly outshines on rare words, outperforming by a factor of 30! We extensively study the language modeling setup for all three categories of tokenizers and theoretically analyze how our end-to-end models can also be a strong trade-off in efficiency and robustness.
</details>
<details>
<summary>摘要</summary>
This paper proposes an alternative "learn your tokens" scheme, which utilizes word boundaries to pool bytes/characters into word representations, which are then fed to the primary language model. The model then decodes individual characters/bytes per word in parallel. We find that our moderately expressive and moderately fast end-to-end tokenizer outperforms both subwords and byte/character models by over 300% in next-word prediction accuracy across datasets, particularly in rare words, with a factor of 30.We extensively study the language modeling setup for all three categories of tokenizers and theoretically analyze how our end-to-end models can also be a strong trade-off in efficiency and robustness.
</details></li>
</ul>
<hr>
<h2 id="An-Optimistic-Robust-Approach-for-Dynamic-Positioning-of-Omnichannel-Inventories"><a href="#An-Optimistic-Robust-Approach-for-Dynamic-Positioning-of-Omnichannel-Inventories" class="headerlink" title="An Optimistic-Robust Approach for Dynamic Positioning of Omnichannel Inventories"></a>An Optimistic-Robust Approach for Dynamic Positioning of Omnichannel Inventories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12183">http://arxiv.org/abs/2310.12183</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pavithra Harsha, Shivaram Subramanian, Ali Koc, Mahesh Ramakrishna, Brian Quanz, Dhruv Shah, Chandra Narayanaswami</li>
<li>for: 本研究提出了一种新的数据驱动、分布 свобо的optimistic-robustBI备货优化策略，用于在零售链中均衡时变不确定的多渠道需求，以提高均值性和鲁棒性。</li>
<li>methods: 该策略基于一种新的分布自由的BI备货模型，通过结合数据驱动和鲁棒优化来超越传统的鲁棒优化方法，并且提供了一种可调的tradeoff между鲁棒性和均值性。</li>
<li>results: 实验结果表明，BI备货策略可以在一个实际的大型美国多渠道零售商中实现至少15%的利润提高，同时保持实际最坏情况性能。<details>
<summary>Abstract</summary>
We introduce a new class of data-driven and distribution-free optimistic-robust bimodal inventory optimization (BIO) strategy to effectively allocate inventory across a retail chain to meet time-varying, uncertain omnichannel demand. While prior Robust optimization (RO) methods emphasize the downside, i.e., worst-case adversarial demand, BIO also considers the upside to remain resilient like RO while also reaping the rewards of improved average-case performance by overcoming the presence of endogenous outliers. This bimodal strategy is particularly valuable for balancing the tradeoff between lost sales at the store and the costs of cross-channel e-commerce fulfillment, which is at the core of our inventory optimization model. These factors are asymmetric due to the heterogenous behavior of the channels, with a bias towards the former in terms of lost-sales cost and a dependence on network effects for the latter. We provide structural insights about the BIO solution and how it can be tuned to achieve a preferred tradeoff between robustness and the average-case. Our experiments show that significant benefits can be achieved by rethinking traditional approaches to inventory management, which are siloed by channel and location. Using a real-world dataset from a large American omnichannel retail chain, a business value assessment during a peak period indicates over a 15% profitability gain for BIO over RO and other baselines while also preserving the (practical) worst case performance.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的数据驱动、分布自由的乐观稳定优化策略（BIO），用于在零售链中有效分配存储，以满足时变、不确定的多渠道需求。在优化模型中，BIO策略通过考虑两个模式（下行和上行）来实现乐观稳定性，同时也能够保持优秀的平均情况性。这种策略对于平衡店铺产生损失和多渠道电商配送成本的负担进行了有利的均衡。我们提供了BIO解决方案的结构性分析，以及如何根据需要调整BIO来实现想要的负载均衡。我们的实验结果表明，通过重新思考传统的存储管理方法，可以实现显著的利益提升，并且保持了实际最坏情况性。使用一个大型美国多渠道零售商的实际数据，我们在峰值期进行了业务价值评估，发现BIO相比RO和其他基eline，可以获得超过15%的利润增加，同时保持实际最坏情况性。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-the-General-Intelligence-Factor-in-Language-Models-A-Psychometric-Approach"><a href="#Unveiling-the-General-Intelligence-Factor-in-Language-Models-A-Psychometric-Approach" class="headerlink" title="Unveiling the General Intelligence Factor in Language Models: A Psychometric Approach"></a>Unveiling the General Intelligence Factor in Language Models: A Psychometric Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11616">http://arxiv.org/abs/2310.11616</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/davidilic/g-in-llms">https://github.com/davidilic/g-in-llms</a></li>
<li>paper_authors: David Ilić</li>
<li>for: 这项研究探讨语言模型中的通用智能因素（g因素），扩展传统地应用于人类和某些动物种类的心理测量理论。</li>
<li>methods: 通过两个大规模数据集（Open LLM Leaderboard和GLUE Leaderboard）的因素分析，发现了一个强度稳定的单一g因素，占据模型性能变化的85%。同时发现模型大小和g因素之间存在moderate相关性（。48）。</li>
<li>results: 发现了语言模型中的g因素，提供了一个统一的评估指标，开启了更加稳定、g基础能力评估的新途径。这些发现将心理测量理论应用于人工通用智能领域奠定基础，有实践意义 для模型评估和开发。<details>
<summary>Abstract</summary>
This study uncovers the factor of general intelligence, or g, in language models, extending the psychometric theory traditionally applied to humans and certain animal species. Utilizing factor analysis on two extensive datasets - Open LLM Leaderboard with 1,232 models and General Language Understanding Evaluation (GLUE) Leaderboard with 88 models - we find compelling evidence for a unidimensional, highly stable g factor that accounts for 85% of the variance in model performance. The study also finds a moderate correlation of .48 between model size and g. The discovery of g in language models offers a unified metric for model evaluation and opens new avenues for more robust, g-based model ability assessment. These findings lay the foundation for understanding and future research on artificial general intelligence from a psychometric perspective and have practical implications for model evaluation and development.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-a-Hierarchical-Planner-from-Humans-in-Multiple-Generations"><a href="#Learning-a-Hierarchical-Planner-from-Humans-in-Multiple-Generations" class="headerlink" title="Learning a Hierarchical Planner from Humans in Multiple Generations"></a>Learning a Hierarchical Planner from Humans in Multiple Generations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11614">http://arxiv.org/abs/2310.11614</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leonardo Hernandez Cano, Yewen Pu, Robert D. Hawkins, Josh Tenenbaum, Armando Solar-Lezama</li>
<li>for: 本研究旨在帮助机器学习人类知识，提高机器的适应能力和解决复杂任务的能力。</li>
<li>methods: 本研究使用了自然编程，一种组合了程序学习和层次规划的库学习系统。用户通过课程建设来教育系统，选择一个具有挑战性但不是不可能的目标，并提供语言提示，帮助系统在层次规划中找到正确的计划。</li>
<li>results:  simulations and human experiments (n&#x3D;360) 表明，自然编程可以强大地组合来自不同用户和Contexts的程序，并且在对Contexts进行更改时更快地适应，并解决更复杂的任务。相比于程序编程基线，自然编程在解决复杂任务方面表现出了明显的优势。<details>
<summary>Abstract</summary>
A typical way in which a machine acquires knowledge from humans is by programming. Compared to learning from demonstrations or experiences, programmatic learning allows the machine to acquire a novel skill as soon as the program is written, and, by building a library of programs, a machine can quickly learn how to perform complex tasks. However, as programs often take their execution contexts for granted, they are brittle when the contexts change, making it difficult to adapt complex programs to new contexts. We present natural programming, a library learning system that combines programmatic learning with a hierarchical planner. Natural programming maintains a library of decompositions, consisting of a goal, a linguistic description of how this goal decompose into sub-goals, and a concrete instance of its decomposition into sub-goals. A user teaches the system via curriculum building, by identifying a challenging yet not impossible goal along with linguistic hints on how this goal may be decomposed into sub-goals. The system solves for the goal via hierarchical planning, using the linguistic hints to guide its probability distribution in proposing the right plans. The system learns from this interaction by adding newly found decompositions in the successful search into its library. Simulated studies and a human experiment (n=360) on a controlled environment demonstrate that natural programming can robustly compose programs learned from different users and contexts, adapting faster and solving more complex tasks when compared to programmatic baselines.
</details>
<details>
<summary>摘要</summary>
一般来说，机器学习知识从人类那里是通过编程的方式。与学习示例或经验相比，编程学习可以让机器快速学习新的技能，只需要编写一份程序即可，并且通过建立一个程序库，机器可以快速学习完成复杂任务。然而，由于程序经常假设执行上下文，因此它们在上下文变化时会变得不稳定，使得复杂程序适应新上下文变得困难。我们提出了自然编程，一种基于程序库学习系统，它将程序学习与层次规划结合。自然编程保留一个库中的分解，包括目标、语言描述如何将目标分解成子目标，以及具体实例的分解。用户通过课程建设来教育系统，选择一个具有挑战性但并不是不可能的目标，并提供语言提示如何将目标分解成子目标。系统使用层次规划解决目标，使用语言提示来引导搜索的概率分布。系统从这种互动中学习，将成功搜索的新分解添加到库中。 simulated studies and human experiment (n=360) on a controlled environment show that natural programming can robustly compose programs learned from different users and contexts, adapting faster and solving more complex tasks than programmatic baselines.
</details></li>
</ul>
<hr>
<h2 id="Language-Models-as-Zero-Shot-Trajectory-Generators"><a href="#Language-Models-as-Zero-Shot-Trajectory-Generators" class="headerlink" title="Language Models as Zero-Shot Trajectory Generators"></a>Language Models as Zero-Shot Trajectory Generators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11604">http://arxiv.org/abs/2310.11604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Teyun Kwon, Norman Di Palo, Edward Johns</li>
<li>for: 这个论文旨在检验Language Model (LLM)是否能直接预测 robot 的低级运动轨迹，以便在 Robotics 中使用 LLM 进行高级 планинг。</li>
<li>methods: 该论文使用了 GPT-4 Language Model，并通过对象检测和分割视觉模型来提供输入。</li>
<li>results: 研究发现，单个任务无关的提示可以在 26 个实际世界语言任务中表现出色，例如 “打开瓶cap” 和 “wipe plate with sponge”。此外，研究还发现，LLM 实际上具有足够的低级控制知识，可以执行许多常见任务，并且可以检测失败并重新规划轨迹。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have recently shown promise as high-level planners for robots when given access to a selection of low-level skills. However, it is often assumed that LLMs do not possess sufficient knowledge to be used for the low-level trajectories themselves. In this work, we address this assumption thoroughly, and investigate if an LLM (GPT-4) can directly predict a dense sequence of end-effector poses for manipulation skills, when given access to only object detection and segmentation vision models. We study how well a single task-agnostic prompt, without any in-context examples, motion primitives, or external trajectory optimisers, can perform across 26 real-world language-based tasks, such as "open the bottle cap" and "wipe the plate with the sponge", and we investigate which design choices in this prompt are the most effective. Our conclusions raise the assumed limit of LLMs for robotics, and we reveal for the first time that LLMs do indeed possess an understanding of low-level robot control sufficient for a range of common tasks, and that they can additionally detect failures and then re-plan trajectories accordingly. Videos, code, and prompts are available at: https://www.robot-learning.uk/language-models-trajectory-generators.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-Efficacy-of-Transformer-based-Adversarial-Attacks-in-Security-Domains"><a href="#The-Efficacy-of-Transformer-based-Adversarial-Attacks-in-Security-Domains" class="headerlink" title="The Efficacy of Transformer-based Adversarial Attacks in Security Domains"></a>The Efficacy of Transformer-based Adversarial Attacks in Security Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11597">http://arxiv.org/abs/2310.11597</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kunyang Li, Kyle Domico, Jean-Charles Noirot Ferrand, Patrick McDaniel</li>
<li>for: 本研究旨在探讨transformer架构在安全领域中的可靠性和攻击力。</li>
<li>methods: 本研究使用了精心调整的 pré-trained transformer、Convolutional Neural Network (CNN) 和 hybrid (transformer 和 CNN 的ensemble) 模型来解决不同的下游图像任务。然后，使用了一种攻击算法来生成19,367个 adversarial example对于每个任务和每个模型。</li>
<li>results: 我们发现， adversarial examples 生成于 transformer 上的攻击力最高，可以在其他模型上传输25.7%的攻击力。同时，生成于其他模型上的 adversarial examples 在 transformer 上的传输率只有56.7%。这些结果强调了对 transformer 架构的研究在安全领域中的重要性，并建议在传输攻击 Setting 中使用它们作为主要架构。<details>
<summary>Abstract</summary>
Today, the security of many domains rely on the use of Machine Learning to detect threats, identify vulnerabilities, and safeguard systems from attacks. Recently, transformer architectures have improved the state-of-the-art performance on a wide range of tasks such as malware detection and network intrusion detection. But, before abandoning current approaches to transformers, it is crucial to understand their properties and implications on cybersecurity applications. In this paper, we evaluate the robustness of transformers to adversarial samples for system defenders (i.e., resiliency to adversarial perturbations generated on different types of architectures) and their adversarial strength for system attackers (i.e., transferability of adversarial samples generated by transformers to other target models). To that effect, we first fine-tune a set of pre-trained transformer, Convolutional Neural Network (CNN), and hybrid (an ensemble of transformer and CNN) models to solve different downstream image-based tasks. Then, we use an attack algorithm to craft 19,367 adversarial examples on each model for each task. The transferability of these adversarial examples is measured by evaluating each set on other models to determine which models offer more adversarial strength, and consequently, more robustness against these attacks. We find that the adversarial examples crafted on transformers offer the highest transferability rate (i.e., 25.7% higher than the average) onto other models. Similarly, adversarial examples crafted on other models have the lowest rate of transferability (i.e., 56.7% lower than the average) onto transformers. Our work emphasizes the importance of studying transformer architectures for attacking and defending models in security domains, and suggests using them as the primary architecture in transfer attack settings.
</details>
<details>
<summary>摘要</summary>
今天，许多领域的安全性都依赖于机器学习来探测威胁、 indentify漏洞和保护系统从攻击中保护。现在，transformer架构已经提高了一系列任务的状态之末性，如恶意软件检测和网络侵入检测。但是，在switch到transformer架构之前，必须了解它们的性质和影响在安全应用程序中。在这篇论文中，我们评估了transformer架构对抗攻击样本的Robustness（可以抗拒的性）和攻击者的攻击力（可以转移到其他目标模型）。为此，我们首先精度调整一组预训练过的transformer、Convolutional Neural Network（CNN）和混合（一个ensemble of transformer和CNN）模型，以解决不同的图像基于任务。然后，我们使用一种攻击算法来生成19367个攻击样本 для每个任务。我们使用这些攻击样本来测量每个模型之间的 transferred（可以转移的性）。我们发现，在transformer架构上生成的攻击样本具有最高的转移率（即25.7% higher than the average），而其他模型上生成的攻击样本具有最低的转移率（即56.7% lower than the average）。我们的工作强调了在安全领域使用transformer架构进行攻击和防御模型的研究，并建议在转移攻击设置中使用它们作为主要架构。
</details></li>
</ul>
<hr>
<h2 id="WaveAttack-Asymmetric-Frequency-Obfuscation-based-Backdoor-Attacks-Against-Deep-Neural-Networks"><a href="#WaveAttack-Asymmetric-Frequency-Obfuscation-based-Backdoor-Attacks-Against-Deep-Neural-Networks" class="headerlink" title="WaveAttack: Asymmetric Frequency Obfuscation-based Backdoor Attacks Against Deep Neural Networks"></a>WaveAttack: Asymmetric Frequency Obfuscation-based Backdoor Attacks Against Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11595">http://arxiv.org/abs/2310.11595</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Xia, Zhihao Yue, Yingbo Zhou, Zhiwei Ling, Xian Wei, Mingsong Chen</li>
<li>for: 针对深度神经网络预测中的背门攻击（backdoor attack），提出一种基于高频特征的新攻击方法——WaveAttack。</li>
<li>methods: 使用分割波峰变换（DWT）获得图像高频特征，并通过异步频率隐蔽方法在训练和推理阶段添加自适应副本，以提高背门诱导器的影响和攻击效iveness。</li>
<li>results: 对比比较常见的背门攻击方法，WaveAttack 可以 дости得高度隐蔽和效果，同时在图像质量上也可以达到28.27%的提升（PSNR）、1.61%的提升（SSIM）和70.59%的减少（IS）。<details>
<summary>Abstract</summary>
Due to the popularity of Artificial Intelligence (AI) technology, numerous backdoor attacks are designed by adversaries to mislead deep neural network predictions by manipulating training samples and training processes. Although backdoor attacks are effective in various real scenarios, they still suffer from the problems of both low fidelity of poisoned samples and non-negligible transfer in latent space, which make them easily detectable by existing backdoor detection algorithms. To overcome the weakness, this paper proposes a novel frequency-based backdoor attack method named WaveAttack, which obtains image high-frequency features through Discrete Wavelet Transform (DWT) to generate backdoor triggers. Furthermore, we introduce an asymmetric frequency obfuscation method, which can add an adaptive residual in the training and inference stage to improve the impact of triggers and further enhance the effectiveness of WaveAttack. Comprehensive experimental results show that WaveAttack not only achieves higher stealthiness and effectiveness, but also outperforms state-of-the-art (SOTA) backdoor attack methods in the fidelity of images by up to 28.27\% improvement in PSNR, 1.61\% improvement in SSIM, and 70.59\% reduction in IS.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:由于人工智能技术的普及，许多敌对者设计了多种后门攻击，以诱导深度神经网络预测结果的扰乱。尽管后门攻击在实际场景中有效，但它们仍然受到低精度毒品样本和 latent space 中的非至遇抗减 Transfer 的问题，这使得它们可以轻松被现有的后门检测算法检测出来。为了解决这个弱点，本文提出了一种基于频率的后门攻击方法，名为 WaveAttack，它使用 Discrete Wavelet Transform (DWT) 获取图像高频特征来生成后门触发器。此外，我们还引入了不对称频率隐藏方法，可以在训练和推理阶段添加adaptive residual来提高触发器的影响，进一步提高 WaveAttack 的效果。经过全面的实验结果表明，WaveAttack 不仅实现了更高的隐身度和效果，还比 State-of-the-art (SOTA) 后门攻击方法在图像的精度上提高了28.27％的PSNR，1.61％的SSIM，和70.59％的IS。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Robustness-Unhardening-via-Backdoor-Attacks-in-Federated-Learning"><a href="#Adversarial-Robustness-Unhardening-via-Backdoor-Attacks-in-Federated-Learning" class="headerlink" title="Adversarial Robustness Unhardening via Backdoor Attacks in Federated Learning"></a>Adversarial Robustness Unhardening via Backdoor Attacks in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11594">http://arxiv.org/abs/2310.11594</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taejin Kim, Jiarui Li, Shubhranshu Singh, Nikhil Madaan, Carlee Joe-Wong</li>
<li>for: This paper focuses on addressing security challenges in federated learning, specifically the issues of poisoning and backdoor attacks, by exploring the intersection of adversarial training and backdoor attacks.</li>
<li>methods: The paper introduces a new attack called Adversarial Robustness Unhardening (ARU) and evaluates its impact on adversarial training and existing robust aggregation defenses against poisoning and backdoor attacks through extensive empirical experiments.</li>
<li>results: The paper finds that ARU can intentionally undermine model robustness during decentralized training, rendering models susceptible to a broader range of evasion attacks, and highlights the limitations of existing defenses against ARU. The findings offer insights into bolstering defenses against ARU and the need for further research in this area.Here is the information in Simplified Chinese text:</li>
<li>for: 这篇论文关注联邦学习中的安全挑战，具体来说是对毒素和后门攻击的问题，通过对抗训练和后门攻击之间的交叉研究。</li>
<li>methods: 论文引入了一种新的攻击方法called Adversarial Robustness Unhardening (ARU)，通过广泛的实验评估ARU对反对抗训练和现有的稳定聚合防御措施的影响。</li>
<li>results: 论文发现ARU可以在分布式训练中故意削弱模型的鲁棒性，使模型面临更广泛的欺骗攻击，并指出现有防御措施对ARU的限制。发现为防止ARU提供了新的策略和研究方向。<details>
<summary>Abstract</summary>
In today's data-driven landscape, the delicate equilibrium between safeguarding user privacy and unleashing data potential stands as a paramount concern. Federated learning, which enables collaborative model training without necessitating data sharing, has emerged as a privacy-centric solution. This decentralized approach brings forth security challenges, notably poisoning and backdoor attacks where malicious entities inject corrupted data. Our research, initially spurred by test-time evasion attacks, investigates the intersection of adversarial training and backdoor attacks within federated learning, introducing Adversarial Robustness Unhardening (ARU). ARU is employed by a subset of adversaries to intentionally undermine model robustness during decentralized training, rendering models susceptible to a broader range of evasion attacks. We present extensive empirical experiments evaluating ARU's impact on adversarial training and existing robust aggregation defenses against poisoning and backdoor attacks. Our findings inform strategies for enhancing ARU to counter current defensive measures and highlight the limitations of existing defenses, offering insights into bolstering defenses against ARU.
</details>
<details>
<summary>摘要</summary>
今天的数据驱动时代，保护用户隐私和解释数据潜力之间的细腻平衡已成为首要问题。联邦学习，它允许无需数据共享进行模型训练的协同方法，已经出现为隐私中心的解决方案。这种分布式方法会产生安全挑战，包括腐化和后门攻击，即恶意实体插入假数据。我们的研究，起始于测试时间攻击，探讨在联邦学习中的对抗训练和后门攻击的交叉点，并提出了对抗训练不稳定性的技术——不稳定性脱困（ARU）。ARU可以在分布式训练中被一部分对手使用，故意下降模型的可靠性，使模型对更广泛的欺骗攻击变得感受。我们对ARU的影响进行了广泛的实验研究，包括对抗训练和现有的毒素攻击防御措施的评估。我们的发现可以帮助改进ARU，抗衡当前的防御措施，并 highlighted了现有防御措施的局限性，为加强防御提供了新的思路。
</details></li>
</ul>
<hr>
<h2 id="Automated-Evaluation-of-Personalized-Text-Generation-using-Large-Language-Models"><a href="#Automated-Evaluation-of-Personalized-Text-Generation-using-Large-Language-Models" class="headerlink" title="Automated Evaluation of Personalized Text Generation using Large Language Models"></a>Automated Evaluation of Personalized Text Generation using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11593">http://arxiv.org/abs/2310.11593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yaqing Wang, Jiepu Jiang, Mingyang Zhang, Cheng Li, Yi Liang, Qiaozhu Mei, Michael Bendersky</li>
<li>for: 评估个性化文本生成器的质量</li>
<li>methods: 使用大型自然语言模型（LLMs）进行评估，并分析三个主要Semantic Aspects：个性化、质量和相关性</li>
<li>results: 比较LLMs和人工标注的评估结果，发现LLMs能够更准确地评估个性化文本生成器的质量，并且存在较好的一致性和效率。<details>
<summary>Abstract</summary>
Personalized text generation presents a specialized mechanism for delivering content that is specific to a user's personal context. While the research progress in this area has been rapid, evaluation still presents a challenge. Traditional automated metrics such as BLEU and ROUGE primarily measure lexical similarity to human-written references, and are not able to distinguish personalization from other subtle semantic aspects, thus falling short of capturing the nuances of personalized generated content quality. On the other hand, human judgments are costly to obtain, especially in the realm of personalized evaluation. Inspired by these challenges, we explore the use of large language models (LLMs) for evaluating personalized text generation, and examine their ability to understand nuanced user context. We present AuPEL, a novel evaluation method that distills three major semantic aspects of the generated text: personalization, quality and relevance, and automatically measures these aspects. To validate the effectiveness of AuPEL, we design carefully controlled experiments and compare the accuracy of the evaluation judgments made by LLMs versus that of judgements made by human annotators, and conduct rigorous analyses of the consistency and sensitivity of the proposed metric. We find that, compared to existing evaluation metrics, AuPEL not only distinguishes and ranks models based on their personalization abilities more accurately, but also presents commendable consistency and efficiency for this task. Our work suggests that using LLMs as the evaluators of personalized text generation is superior to traditional text similarity metrics, even though interesting new challenges still remain.
</details>
<details>
<summary>摘要</summary>
个人化文本生成技术提供了特殊的内容交付机制，以便为用户的个人背景提供特定的内容。虽然这一领域的研究进步快速，但评估仍然存在挑战。传统的自动化指标如BLEU和ROUGE主要测量人工写出的参考文本和自动生成内容之间的语言相似性，而不能分辨个人化的特点，因此无法捕捉个人化生成内容质量的细微差别。人类评估更加costly，尤其是在个人化评估领域。我们 inspirited by these challenges，explore the use of large language models (LLMs) for evaluating personalized text generation, and examine their ability to understand nuanced user context.我们提出了一种新的评估方法，称为AuPEL，它可以自动测量生成文本中的三个主要semantic aspect：个人化、质量和相关性。为了验证AuPEL的有效性，我们设计了仔细控制的实验，并将LLMs的评估判断与人类标注者的评估判断进行比较，进行了严格的分析。我们发现，相比现有的评估指标，AuPEL不仅更准确地分辨和排序模型的个人化能力，而且具有了很好的一致性和效率。我们的工作表明，使用LLMs作为个人化文本生成评估的方法比传统的文本相似度指标更为有利，尽管还有一些新的挑战 waits ahead。
</details></li>
</ul>
<hr>
<h2 id="Audio-AdapterFusion-A-Task-ID-free-Approach-for-Efficient-and-Non-Destructive-Multi-task-Speech-Recognition"><a href="#Audio-AdapterFusion-A-Task-ID-free-Approach-for-Efficient-and-Non-Destructive-Multi-task-Speech-Recognition" class="headerlink" title="Audio-AdapterFusion: A Task-ID-free Approach for Efficient and Non-Destructive Multi-task Speech Recognition"></a>Audio-AdapterFusion: A Task-ID-free Approach for Efficient and Non-Destructive Multi-task Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13015">http://arxiv.org/abs/2310.13015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hillary Ngai, Rohan Agrawal, Neeraj Gaur, Ronny Huang, Parisa Haghani, Pedro Moreno Mengibar</li>
<li>for: 这篇论文的目的是提出了一些可以将单任问题（single-task）adapter组合在多任问题（multi-task） automatic speech recognition（ASR）中，并且训练这些方法以提高语音识别精度。</li>
<li>methods: 这篇论文提出了三种新的任问题ID-free方法（task-ID-free methods），可以将单任问题adapter组合在多任问题ASR中，并且训练这些方法以提高语音识别精度。这三种方法分别是：1) 使用任问题ID-free映射函数（task-ID-free mapping function）来将单任问题adapter组合在多任问题ASR中；2) 使用任问题ID-free排序（task-ID-free sorting）来将单任问题adapter组合在多任问题ASR中；3) 使用任问题ID-free混合（task-ID-free mixing）来将单任问题adapter组合在多任问题ASR中。</li>
<li>results: 这篇论文的结果显示，使用这三种方法可以将单任问题adapter组合在多任问题ASR中，并且可以提高语音识别精度。 Specifically, the results show that the proposed methods can improve the speech recognition accuracy by 8% on average, compared to full fine-tuning. Additionally, the proposed methods are non-destructive and parameter-efficient, as they only update 17% of the model parameters.<details>
<summary>Abstract</summary>
Adapters are an efficient, composable alternative to full fine-tuning of pre-trained models and help scale the deployment of large ASR models to many tasks. In practice, a task ID is commonly prepended to the input during inference to route to single-task adapters for the specified task. However, one major limitation of this approach is that the task ID may not be known during inference, rendering it unsuitable for most multi-task settings. To address this, we propose three novel task-ID-free methods to combine single-task adapters in multi-task ASR and investigate two learning algorithms for training. We evaluate our methods on 10 test sets from 4 diverse ASR tasks and show that our methods are non-destructive and parameter-efficient. While only updating 17% of the model parameters, our methods can achieve an 8% mean WER improvement relative to full fine-tuning and are on-par with task-ID adapter routing.
</details>
<details>
<summary>摘要</summary>
adapter 是一种高效、可 compose 的替代方案，帮助扩大预训练模型的部署范围。在实践中，通常会在推理时预先 prepend 任务 ID 到输入，以便将输入 routed 到适应的单任务 adapter。然而，这种方法有一个主要的限制，即在多任务 setting 中，任务 ID 可能不知道，这 Render 其不适用。为了解决这个问题，我们提出了三种新的任务 ID  libre 方法，并 investigate 了两种学习算法 для训练。我们在 10 个测试集上从 4 种不同的 ASR 任务中选择了，并显示了我们的方法是非破坏性的和参数有效的。只需更新 17% 的模型参数，我们的方法可以实现一个 8% 的语音识别错误率下降，与完全 fine-tuning 相当，并且与任务 ID adapter 路由相比。
</details></li>
</ul>
<hr>
<h2 id="Eliciting-Human-Preferences-with-Language-Models"><a href="#Eliciting-Human-Preferences-with-Language-Models" class="headerlink" title="Eliciting Human Preferences with Language Models"></a>Eliciting Human Preferences with Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11589">http://arxiv.org/abs/2310.11589</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alextamkin/generative-elicitation">https://github.com/alextamkin/generative-elicitation</a></li>
<li>paper_authors: Belinda Z. Li, Alex Tamkin, Noah Goodman, Jacob Andreas</li>
<li>for: 本研究用LM来指导任务规定过程。</li>
<li>methods: 本研究提出了Generative Active Task Elicitation（GATE）学习框架，用LM自己进行自由语言互动，以引出用户的意图和行为。</li>
<li>results: 在邮箱验证、内容推荐和道德决策等三个领域中，通过LM自己生成问题或总结特殊情况，可以更好地引出用户的想法和需求，并且用户报告需要更少的努力和更多的创新。<details>
<summary>Abstract</summary>
Language models (LMs) can be directed to perform target tasks by using labeled examples or natural language prompts. But selecting examples or writing prompts for can be challenging--especially in tasks that involve unusual edge cases, demand precise articulation of nebulous preferences, or require an accurate mental model of LM behavior. We propose to use *LMs themselves* to guide the task specification process. In this paper, we introduce **Generative Active Task Elicitation (GATE)**: a learning framework in which models elicit and infer intended behavior through free-form, language-based interaction with users. We study GATE in three domains: email validation, content recommendation, and moral reasoning. In preregistered experiments, we show that LMs prompted to perform GATE (e.g., by generating open-ended questions or synthesizing informative edge cases) elicit responses that are often more informative than user-written prompts or labels. Users report that interactive task elicitation requires less effort than prompting or example labeling and surfaces novel considerations not initially anticipated by users. Our findings suggest that LM-driven elicitation can be a powerful tool for aligning models to complex human preferences and values.
</details>
<details>
<summary>摘要</summary>
语言模型（LM）可以通过使用标记的示例或自然语言提示来实现目标任务。但选择示例或编写提示可以是困难的——特别是在涉及到特殊的边界情况、需要精确表达抽象的偏好或需要语言模型行为的准确理解。我们提议使用LM自己来指导任务规定过程。在这篇论文中，我们介绍了**生成活动任务提取（GATE）**：一种学习框架，在用户与LM进行自由形式、语言基于的互动中，LM可以透过生成开放式问题或合成有用的边界情况来引导用户提供任务要求。我们在三个领域中进行了预先注册的实验，并证明了LM通过执行GATE（例如，生成开放式问题或合成有用的边界情况）可以从用户提供更多有用信息的回应。用户报告称，与LM进行交互式任务提取比起用户编写提示或标记，需要更少的努力，并且可以浮现用户没有首先预期的考虑。我们的发现表明，LM驱动的提取可以是一种强大的工具，用于将模型与人类偏好和价值相互Alignment。
</details></li>
</ul>
<hr>
<h2 id="When-Rigidity-Hurts-Soft-Consistency-Regularization-for-Probabilistic-Hierarchical-Time-Series-Forecasting"><a href="#When-Rigidity-Hurts-Soft-Consistency-Regularization-for-Probabilistic-Hierarchical-Time-Series-Forecasting" class="headerlink" title="When Rigidity Hurts: Soft Consistency Regularization for Probabilistic Hierarchical Time Series Forecasting"></a>When Rigidity Hurts: Soft Consistency Regularization for Probabilistic Hierarchical Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11569">http://arxiv.org/abs/2310.11569</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adityalab/profhit">https://github.com/adityalab/profhit</a></li>
<li>paper_authors: Harshavardhan Kamarthi, Lingkai Kong, Alexander Rodríguez, Chao Zhang, B. Aditya Prakash</li>
<li>for: 这个研究旨在提出一个可靠且具有准确评估的时间序列预测方法，用于模型和预测多ivariate时间序列，并且考虑到这些时间序列之间的层次关系。</li>
<li>methods: 这个方法使用了一种可靠的数据科学方法，具体来说是一种可靠的 Bayesian 方法，并且引入了一个新的 Distributional Coherency 调整，以从层次关系中学习整个预测分布，这使得预测得到了更好的准确性和均匀性。</li>
<li>results: 在评估这个方法的实验中，发现这个方法可以在各种不同的数据集上提供41-88%的更好的性能，并且在数据集中缺失10%的时间序列Data时，预测的性能仍然保持在良好的水平，而其他方法在这种情况下的性能则会严重下降超过70%。<details>
<summary>Abstract</summary>
Probabilistic hierarchical time-series forecasting is an important variant of time-series forecasting, where the goal is to model and forecast multivariate time-series that have underlying hierarchical relations. Most methods focus on point predictions and do not provide well-calibrated probabilistic forecasts distributions. Recent state-of-art probabilistic forecasting methods also impose hierarchical relations on point predictions and samples of distribution which does not account for coherency of forecast distributions. Previous works also silently assume that datasets are always consistent with given hierarchical relations and do not adapt to real-world datasets that show deviation from this assumption. We close both these gap and propose PROFHiT, which is a fully probabilistic hierarchical forecasting model that jointly models forecast distribution of entire hierarchy. PROFHiT uses a flexible probabilistic Bayesian approach and introduces a novel Distributional Coherency regularization to learn from hierarchical relations for entire forecast distribution that enables robust and calibrated forecasts as well as adapt to datasets of varying hierarchical consistency. On evaluating PROFHiT over wide range of datasets, we observed 41-88% better performance in accuracy and significantly better calibration. Due to modeling the coherency over full distribution, we observed that PROFHiT can robustly provide reliable forecasts even if up to 10% of input time-series data is missing where other methods' performance severely degrade by over 70%.
</details>
<details>
<summary>摘要</summary>
“probabilistic hierarchical time-series forecasting是一种重要的时间序列预测变体，其目的是模型和预测多变量时间序列的层次关系。大多数方法都是专注于点预测，而不提供准确的probabilistic预测分布。当前的状态艺术probabilistic预测方法也强制实施层次关系于点预测和样本分布，而不考虑预测分布的协调性。过去的工作也假设了所有数据都一定程度上遵循给定的层次关系，而不会适应实际的数据，其中很多数据可能会出现层次不一致的情况。我们 closure这两个 gap，并提出PROFHiT，这是一种完全probabilistic层次预测模型，可以同时模型整个层次预测分布。PROFHiT使用 flexible probabilistic Bayesian方法，并引入一种新的分布协调regularization，以学习层次关系，并生成准确和协调的预测。对于各种数据集进行评估，我们发现PROFHiT在精度和准确性方面表现出41-88%的提升，同时也能够更好地适应数据集的层次不一致情况。由于模型整个分布的协调性，我们发现PROFHiT可以在数据损失情况下提供可靠的预测，甚至在数据损失10%以上时仍能保持良好的预测性能，而其他方法在这种情况下会导致预测性能下降超过70%。”
</details></li>
</ul>
<hr>
<h2 id="Integrating-3D-City-Data-through-Knowledge-Graphs"><a href="#Integrating-3D-City-Data-through-Knowledge-Graphs" class="headerlink" title="Integrating 3D City Data through Knowledge Graphs"></a>Integrating 3D City Data through Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11555">http://arxiv.org/abs/2310.11555</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linfang Ding, Guohui Xiao, Albulen Pano, Mattia Fumagalli, Dongsheng Chen, Yu Feng, Diego Calvanese, Hongchao Fan, Liqiu Meng</li>
<li>for: 本研究旨在利用知识图（KG）技术，将城市地理信息模型（CityGML）数据模型化为适当的 ontology，以便对CityGML数据进行查询和推理。</li>
<li>methods: 本研究使用了 declarative 映射将CityGML数据与3DCityDB系统的关系表格相关联，以便通过标准 SQL 查询语言进行查询。此外，本研究还使用了 OpenStreetMap 数据作为示例数据，并与其他（地理）KG（如 Wikidata、DBPedia 和 GeoNames）进行集成。</li>
<li>results: 本研究实现了一个基于 CityGML KG 框架的方法，可以快速地将CityGML数据转换为 KG 格式，并且可以与其他数据源进行集成。这种方法可以帮助解决城市地理信息查询和推理的问题，并且可以提高城市规划和管理的效率。<details>
<summary>Abstract</summary>
CityGML is a widely adopted standard by the Open Geospatial Consortium (OGC) for representing and exchanging 3D city models. The representation of semantic and topological properties in CityGML makes it possible to query such 3D city data to perform analysis in various applications, e.g., security management and emergency response, energy consumption and estimation, and occupancy measurement. However, the potential of querying CityGML data has not been fully exploited. The official GML/XML encoding of CityGML is only intended as an exchange format but is not suitable for query answering. The most common way of dealing with CityGML data is to store them in the 3DCityDB system as relational tables and then query them with the standard SQL query language. Nevertheless, for end users, it remains a challenging task to formulate queries over 3DCityDB directly for their ad-hoc analytical tasks, because there is a gap between the conceptual semantics of CityGML and the relational schema adopted in 3DCityDB. In fact, the semantics of CityGML itself can be modeled as a suitable ontology. The technology of Knowledge Graphs (KGs), where an ontology is at the core, is a good solution to bridge such a gap. Moreover, embracing KGs makes it easier to integrate with other spatial data sources, e.g., OpenStreetMap and existing (Geo)KGs (e.g., Wikidata, DBPedia, and GeoNames), and to perform queries combining information from multiple data sources. In this work, we describe a CityGML KG framework to populate the concepts in the CityGML ontology using declarative mappings to 3DCityDB, thus exposing the CityGML data therein as a KG. To demonstrate the feasibility of our approach, we use CityGML data from the city of Munich as test data and integrate OpenStreeMap data in the same area.
</details>
<details>
<summary>摘要</summary>
CityGML 是一种广泛采用的标准，由开放地理空间协会（OGC）用于表示和交换 3D 城市模型。CityGML 中的 semantics 和 topologic 属性的表示使得可以对这些 3D 城市数据进行查询，并且可以在不同应用中进行分析，如安全管理和紧急应急处理、能源消耗和估算、和占用量测量。然而，CityGML 数据的查询潜力尚未得到完全利用。官方的 GML/XML 编码仅供交换用途，不适用于查询 answered。通常情况下，CityGML 数据会被存储在 3DCityDB 系统中的关系表格中，然后使用标准 SQL 查询语言进行查询。然而，为终端用户来直接对 3DCityDB 进行查询是一项困难的任务，因为 CityGML 的概念 semantics 和 3DCityDB 的关系schema 之间存在一个差距。实际上，CityGML 的 semantics 本身可以被视为一个适当的 ontology。知识图（KGs）技术，其核心是 ontology，是一个好的解决方案，可以bridge 这个差距。此外，采用 KGs 可以轻松地与其他空间数据源集成，如 OpenStreetMap 和现有（Geo）KGs（如 Wikidata、DBPedia 和 GeoNames），并在多个数据源之间进行查询。在这项工作中，我们描述了一个 CityGML KG 框架，通过声明映射来将 CityGML 数据与 3DCityDB 关系表格相关联，从而将 CityGML 数据转换为 KG。为证明我们的方法的可行性，我们使用了 Munich 城市的 CityGML 数据作为测试数据，并将 OpenStreetMap 数据与其同一地区集成。
</details></li>
</ul>
<hr>
<h2 id="Towards-Optimal-Regret-in-Adversarial-Linear-MDPs-with-Bandit-Feedback"><a href="#Towards-Optimal-Regret-in-Adversarial-Linear-MDPs-with-Bandit-Feedback" class="headerlink" title="Towards Optimal Regret in Adversarial Linear MDPs with Bandit Feedback"></a>Towards Optimal Regret in Adversarial Linear MDPs with Bandit Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11550">http://arxiv.org/abs/2310.11550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haolin Liu, Chen-Yu Wei, Julian Zimmert</li>
<li>for: 本研究探讨在线强化学习中的线性Markov决策过程中的抗敌性损失和随机反馈，不需要对过程转移或模拟器的先前知识。</li>
<li>methods: 我们介绍了两种算法，它们在抗敌性损失和随机反馈下实现了改进的后悔性表现，并且比现有方法更为有效。第一种算法尽管计算效率低下，但可 garantuee $\widetilde{\mathcal{O}\left(\sqrt{K}\right)$的后悔性，这是在考虑的设定中的首次成果。第二种算算法基于策略优化框架，并且可 garantuee $\widetilde{\mathcal{O}\left(K^{\frac{3}{4} \right)$的后悔性，同时具有计算效率。</li>
<li>results: 我们的结果在比较之下显著超过现状态艺术：一个计算效率较低的算法由Kong et al. [2023]提出，其后悔性为 $\widetilde{\mathcal{O}\left(K^{\frac{4}{5}+poly\left(\frac{1}{\lambda_{\min}\right) \right)$，其中$\lambda_{\min}$是问题依赖的常数，可以是任意的小数。另一个计算效率较高的算法由Sherman et al. [2023b]提出，其后悔性为 $\widetilde{\mathcal{O}\left(K^{\frac{6}{7} \right)$。<details>
<summary>Abstract</summary>
We study online reinforcement learning in linear Markov decision processes with adversarial losses and bandit feedback, without prior knowledge on transitions or access to simulators. We introduce two algorithms that achieve improved regret performance compared to existing approaches. The first algorithm, although computationally inefficient, ensures a regret of $\widetilde{\mathcal{O}\left(\sqrt{K}\right)$, where $K$ is the number of episodes. This is the first result with the optimal $K$ dependence in the considered setting. The second algorithm, which is based on the policy optimization framework, guarantees a regret of $\widetilde{\mathcal{O}\left(K^{\frac{3}{4} \right)$ and is computationally efficient. Both our results significantly improve over the state-of-the-art: a computationally inefficient algorithm by Kong et al. [2023] with $\widetilde{\mathcal{O}\left(K^{\frac{4}{5}+poly\left(\frac{1}{\lambda_{\min}\right) \right)$ regret, for some problem-dependent constant $\lambda_{\min}$ that can be arbitrarily close to zero, and a computationally efficient algorithm by Sherman et al. [2023b] with $\widetilde{\mathcal{O}\left(K^{\frac{6}{7} \right)$ regret.
</details>
<details>
<summary>摘要</summary>
我们研究在线强化学习在线马尔可夫遇到冲突损失和随机反馈下，无需对过程或模拟器的知识。我们介绍了两种算法，它们在比现有方法更好的停损性表现。第一种算法，尽管计算效率低下，但能 garantie $\widetilde{\mathcal{O}\left(\sqrt{K}\right)$ 的停损性，其中 $K$ 是集的集数。这是在考虑的设定中的第一个 $\sqrt{K}$ 依赖性的结果。第二种算法，基于策略优化框架，保证 $\widetilde{\mathcal{O}\left(K^{\frac{3}{4} \right)$ 的停损性，并且计算效率高。我们的结果在现有的state-of-the-art之上进行了显著改进：一个计算不fficient的算法由孔等人（2023）提出的 $\widetilde{\mathcal{O}\left(K^{\frac{4}{5}+poly\left(\frac{1}{\lambda_{\min}\right) \right)$ 的停损性，其中 $\lambda_{\min}$ 是问题依赖的常数，可以是arbitrarily close to zero。以及一个计算效率高的算法由战等人（2023b）提出的 $\widetilde{\mathcal{O}\left(K^{\frac{6}{7} \right)$ 的停损性。
</details></li>
</ul>
<hr>
<h2 id="MUST-P-SRL-Multi-lingual-and-Unified-Syllabification-in-Text-and-Phonetic-Domains-for-Speech-Representation-Learning"><a href="#MUST-P-SRL-Multi-lingual-and-Unified-Syllabification-in-Text-and-Phonetic-Domains-for-Speech-Representation-Learning" class="headerlink" title="MUST&amp;P-SRL: Multi-lingual and Unified Syllabification in Text and Phonetic Domains for Speech Representation Learning"></a>MUST&amp;P-SRL: Multi-lingual and Unified Syllabification in Text and Phonetic Domains for Speech Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11541">http://arxiv.org/abs/2310.11541</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/noetits/must_p-srl">https://github.com/noetits/must_p-srl</a></li>
<li>paper_authors: Noé Tits</li>
<li>for: 这个论文旨在提出一种语言特征EXTRACTION方法，特别是自动将多语言词语 syllabified into phonetic transcriptions，并设计与蒙特利尔强制对齐器（MFA）兼容。</li>
<li>methods: 该方法在文本和音频频谱领域均注重提取phonetic transcriptions from text，包括强制对齐器（MFA）中的音频频谱。</li>
<li>results: 通过ablation study，我们证明了我们的方法在多种语言（英语、法语和西班牙语）中自动 syllabify 词语的 efficacy。此外，我们还应用了这种技术到CMU ARCTIC数据集的转录中，生成了在线可用的笔记 Footnote \url{<a target="_blank" rel="noopener" href="https://github.com/noetits/MUST_P-SRL%7D%EF%BC%8C%E8%BF%99%E4%BA%9B%E7%AC%94%E8%AE%B0%E9%9D%9E%E5%B8%B8%E6%9C%89%E7%94%A8%E4%BA%8Espeech">https://github.com/noetits/MUST_P-SRL}，这些笔记非常有用于speech</a> representation learning、speech unit discovery和speech factor的分离在多种speech-related领域。<details>
<summary>Abstract</summary>
In this paper, we present a methodology for linguistic feature extraction, focusing particularly on automatically syllabifying words in multiple languages, with a design to be compatible with a forced-alignment tool, the Montreal Forced Aligner (MFA). In both the textual and phonetic domains, our method focuses on the extraction of phonetic transcriptions from text, stress marks, and a unified automatic syllabification (in text and phonetic domains). The system was built with open-source components and resources. Through an ablation study, we demonstrate the efficacy of our approach in automatically syllabifying words from several languages (English, French and Spanish). Additionally, we apply the technique to the transcriptions of the CMU ARCTIC dataset, generating valuable annotations available online\footnote{\url{https://github.com/noetits/MUST_P-SRL} that are ideal for speech representation learning, speech unit discovery, and disentanglement of speech factors in several speech-related fields.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种语言特征提取方法，特别是自动将多语言单词分割成音节，并设计 compatibles 于蒙特利尔强制对齐器（MFA）。在文本和音乐频域中，我们的方法专注于从文本中提取音调标记和统一自动分割（在文本和音乐频域）。系统使用开源组件和资源构建。通过减少研究，我们证明了我们的方法在多种语言（英语、法语和西班牙语）中自动分割单词的效果。此外，我们将该技术应用到CMU ARCTIC数据集的转录中，生成了在线可用的注释\footnotemark[\url{https://github.com/noetits/MUST_P-SRL}]，这些注释非常有价值，适用于语音表示学习、语音单位发现和多种语音相关领域的分离 speech factor。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Online-Learning-with-Offline-Datasets-for-Infinite-Horizon-MDPs-A-Bayesian-Approach"><a href="#Efficient-Online-Learning-with-Offline-Datasets-for-Infinite-Horizon-MDPs-A-Bayesian-Approach" class="headerlink" title="Efficient Online Learning with Offline Datasets for Infinite Horizon MDPs: A Bayesian Approach"></a>Efficient Online Learning with Offline Datasets for Infinite Horizon MDPs: A Bayesian Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11531">http://arxiv.org/abs/2310.11531</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dengwang Tang, Rahul Jain, Botao Hao, Zheng Wen</li>
<li>for: 这个论文研究了在无穷远 Setting中的高效在线学习问题，当有一个离线数据集可以作为开始。</li>
<li>methods: 论文使用了模仿专家的行为策略（通过一个能力参数来 parameterize），并使用了 Bayesian 在线学习算法。</li>
<li>results: 论文表明，如果学习代理人模仿专家的行为策略，就可以在最小化累累 regret 方面做出substantially 更好的表现，并且可以提供$\tilde{O}(\sqrt{T})$的上界 regret bound。<details>
<summary>Abstract</summary>
In this paper, we study the problem of efficient online reinforcement learning in the infinite horizon setting when there is an offline dataset to start with. We assume that the offline dataset is generated by an expert but with unknown level of competence, i.e., it is not perfect and not necessarily using the optimal policy. We show that if the learning agent models the behavioral policy (parameterized by a competence parameter) used by the expert, it can do substantially better in terms of minimizing cumulative regret, than if it doesn't do that. We establish an upper bound on regret of the exact informed PSRL algorithm that scales as $\tilde{O}(\sqrt{T})$. This requires a novel prior-dependent regret analysis of Bayesian online learning algorithms for the infinite horizon setting. We then propose an approximate Informed RLSVI algorithm that we can interpret as performing imitation learning with the offline dataset, and then performing online learning.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了在无穷远Setting中的高效在线强化学习问题，当有一个离线数据集可以作为开始。我们假设离线数据集由专家生成，但专家的水平不确定，即不是完美的和不一定使用优化策略。我们证明，如果学习代理模仿专家的行为策略（参数化为能力参数），它可以在误差总和方面做出较好的表现，比如果不做这样。我们提出了一个新的先验依赖的误差分析方法，并采用了一种新的准确度估计技术。我们then propose一个近似 Informed RLSVI算法，可以视为在线学习后，进行模仿学习。Here's a breakdown of the translation:* "infinite horizon setting" 是 translated as "无穷远Setting" (wú jì yáo jiāng)* "offline dataset" is translated as "离线数据集" (liú xiàn numérical dataset)* "expert" is translated as "专家" (zhuān shī)* "behavioral policy" is translated as "行为策略" (xíng wù zhèng yì)* "competence parameter" is translated as "能力参数" (néng lì jiāng xiàng)* "online learning" is translated as "在线学习" (zhèng xiàng xué xí)* "imitation learning" is translated as "模仿学习" (mó shì xué xí)Note that the translation is in Simplified Chinese, which is the most commonly used variety of Chinese in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Group-Preference-Optimization-Few-Shot-Alignment-of-Large-Language-Models"><a href="#Group-Preference-Optimization-Few-Shot-Alignment-of-Large-Language-Models" class="headerlink" title="Group Preference Optimization: Few-Shot Alignment of Large Language Models"></a>Group Preference Optimization: Few-Shot Alignment of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11523">http://arxiv.org/abs/2310.11523</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyan Zhao, John Dang, Aditya Grover</li>
<li>for: 这篇论文的目的是如何将大型自然语言模型（LLM）调整为不同群体的偏好？</li>
<li>methods: 论文使用了一个名为Group Preference Optimization（GPO）的调整框架，该框架通过将独立的transformer模组与基本LLM模型结合，以便让LLM模型更好地适应不同群体的偏好。</li>
<li>results: 论文通过对不同群体的人类意见项目进行评估，证明了GPO的可行性和高效性，并且需要 fewer group-specific preferences 和 less training and inference computing resources，比较出Perform existing strategies such as in-context steering and fine-tuning methods.<details>
<summary>Abstract</summary>
Many applications of large language models (LLMs), ranging from chatbots to creative writing, require nuanced subjective judgments that can differ significantly across different groups. Existing alignment algorithms can be expensive to align for each group, requiring prohibitive amounts of group-specific preference data and computation for real-world use cases. We introduce Group Preference Optimization (GPO), an alignment framework that steers language models to preferences of individual groups in a few-shot manner. In GPO, we augment the base LLM with an independent transformer module trained to predict the preferences of a group for the LLM generations. For few-shot learning, we parameterize this module as an in-context autoregressive transformer and train it via meta-learning on several groups. We empirically validate the efficacy of GPO through rigorous evaluations using LLMs with varied sizes on three human opinion adaptation tasks. These tasks involve adapting to the preferences of US demographic groups, global countries, and individual users. Our results demonstrate that GPO not only aligns models more accurately but also requires fewer group-specific preferences, and less training and inference computing resources, outperforming existing strategies such as in-context steering and fine-tuning methods.
</details>
<details>
<summary>摘要</summary>
许多大语言模型（LLM）的应用，从聊天机器人到创作写作，需要具有细化的主观评价，这些评价可能会因不同的群体而异常分布。现有的对Alignment算法可能需要较多的群体特定偏好数据和计算资源，这限制了实际应用场景中的使用。我们介绍了一种名为Group Preference Optimization（GPO）的对Alignment框架，可以在几个尝试中导引语言模型到各个群体的偏好。在GPO中，我们将基础的LLM加上一个独立的转换器模块，用于预测群体的偏好。为了进行几个尝试学习，我们将这个模块参数化为受 Context-Aware Transformer 的扩展，并通过元学习训练在多个群体上。我们通过对LLMs With varied sizes进行实质性验证，证明GPO不仅更准确地对齐模型，还需要 fewer group-specific preferences，并且需要更少的训练和推理计算资源，超越现有的方法，如在Context中引导和精度调整方法。
</details></li>
</ul>
<hr>
<h2 id="Guarantees-for-Self-Play-in-Multiplayer-Games-via-Polymatrix-Decomposability"><a href="#Guarantees-for-Self-Play-in-Multiplayer-Games-via-Polymatrix-Decomposability" class="headerlink" title="Guarantees for Self-Play in Multiplayer Games via Polymatrix Decomposability"></a>Guarantees for Self-Play in Multiplayer Games via Polymatrix Decomposability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11518">http://arxiv.org/abs/2310.11518</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/revanmacqueen/self-play-polymatrix">https://github.com/revanmacqueen/self-play-polymatrix</a></li>
<li>paper_authors: Revan MacQueen, James R. Wright</li>
<li>for: 这个论文关注的问题是如何使用自我玩家来学习多智能体系统中的机器学习算法，并且如何 garantuee这些算法在post-training中的性能。</li>
<li>methods: 论文使用了自我玩家来生成大量数据，并且使用了no-external-regret算法来学习。</li>
<li>results: 论文显示了在多智能体系统中，通过使用自我玩家来学习，可以生成高性能的策略，并且这些策略具有 bounded vulnerability。此外，论文还提供了一种Structural property的定义，可以确保这些策略的性能。<details>
<summary>Abstract</summary>
Self-play is a technique for machine learning in multi-agent systems where a learning algorithm learns by interacting with copies of itself. Self-play is useful for generating large quantities of data for learning, but has the drawback that the agents the learner will face post-training may have dramatically different behavior than the learner came to expect by interacting with itself. For the special case of two-player constant-sum games, self-play that reaches Nash equilibrium is guaranteed to produce strategies that perform well against any post-training opponent; however, no such guarantee exists for multiplayer games. We show that in games that approximately decompose into a set of two-player constant-sum games (called constant-sum polymatrix games) where global $\epsilon$-Nash equilibria are boundedly far from Nash equilibria in each subgame (called subgame stability), any no-external-regret algorithm that learns by self-play will produce a strategy with bounded vulnerability. For the first time, our results identify a structural property of multiplayer games that enable performance guarantees for the strategies produced by a broad class of self-play algorithms. We demonstrate our findings through experiments on Leduc poker.
</details>
<details>
<summary>摘要</summary>
自我玩家是一种机器学习技术，用于多代理系统中的学习。在这种情况下，学习算法通过与自己的 копиënteract来学习。自我玩家有利于生成大量数据，但是它们可能会导致学习后面对的代理行为与学习期间预期的行为有很大差异。在特殊的两player常数游戏情况下，如果自我玩家达到尼什平衡，那么生成的策略将在任何后期代理对抗中表现出色。然而，对多代理游戏来说，没有类似的保证。我们表明，在 Approximately decomposable 多代理游戏中，如果每个子游戏中的尼什平衡是 global $\epsilon$-尼什平衡的 boundedly far，那么任何没有外部回报折损的自我玩家学习算法将生成有 bounded vulnerability 的策略。这是我们的结果所示，我们的结论标志着多代理游戏中一种结构性质，允许提供性能保证的策略。我们通过启示 Leduc poker 的实验来证明我们的结论。
</details></li>
</ul>
<hr>
<h2 id="Self-RAG-Learning-to-Retrieve-Generate-and-Critique-through-Self-Reflection"><a href="#Self-RAG-Learning-to-Retrieve-Generate-and-Critique-through-Self-Reflection" class="headerlink" title="Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection"></a>Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11511">http://arxiv.org/abs/2310.11511</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AkariAsai/self-rag">https://github.com/AkariAsai/self-rag</a></li>
<li>paper_authors: Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi</li>
<li>for: 提高语言模型的准确性和多样性</li>
<li>methods: 引入自适应式检索和反思 tokens，使语言模型在推理阶段可控</li>
<li>results: 比对状元模型和检索补充模型在多种任务上表现出色，特别是在开放领域问答、理解和事实核查任务中表现出较高的准确性和引用精度。<details>
<summary>Abstract</summary>
Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.
</details>
<details>
<summary>摘要</summary>
尽管大型语言模型（LLM）具有remarkable的能力，但它们经常生成包含错误信息的响应，这是因为它们仅仅依靠自己学习的参数知识。 Retrieval-Augmented Generation（RAG）是一种补做approach，它在LLM中添加了Retrieval的功能，从而减少这些问题。然而，不经过考虑地检索并 incorporate一定数量的检索到的段落，可能会减少LLM的灵活性，或者导致生成异常的响应。我们提出了一种新的框架，叫做Self-Reflective Retrieval-Augmented Generation（Self-RAG），它可以提高LLM的质量和准确性。我们的框架通过在检索和生成过程中适时使用特殊的 tokens， called reflection tokens，来让LLM在推理阶段变得可控。通过生成reflection tokens，LLM可以根据不同的任务要求进行自适应的调整。我们的实验表明，Self-RAG（7B和13B参数）在多种任务上比state-of-the-art LLMs和检索增强模型表现出色，具体来说，Self-RAG在开放领域问答、理解和事实核实任务上表现出色，并且在长形生成中提高了事实准确性和引用率。
</details></li>
</ul>
<hr>
<h2 id="CoMPosT-Characterizing-and-Evaluating-Caricature-in-LLM-Simulations"><a href="#CoMPosT-Characterizing-and-Evaluating-Caricature-in-LLM-Simulations" class="headerlink" title="CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations"></a>CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11501">http://arxiv.org/abs/2310.11501</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/myracheng/lm_caricature">https://github.com/myracheng/lm_caricature</a></li>
<li>paper_authors: Myra Cheng, Tiziano Piccardi, Diyi Yang</li>
<li>for: 这个论文旨在捕捉人类行为的细腻之处，使用LLM模型模拟特定人群在社会科学实验和公众意见调查中的反应。</li>
<li>methods: 该论文使用了LLM模型来模拟人类行为，并提出了一个框架来评估这些模拟的质量。</li>
<li>results: 研究发现，在某些情况下（如政治和受难群体）和主题（通用不带争议）中，GPT-4模拟的人物有较高的轮廓化风险。<details>
<summary>Abstract</summary>
Recent work has aimed to capture nuances of human behavior by using LLMs to simulate responses from particular demographics in settings like social science experiments and public opinion surveys. However, there are currently no established ways to discuss or evaluate the quality of such LLM simulations. Moreover, there is growing concern that these LLM simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes. To bridge these gaps, we present CoMPosT, a framework to characterize LLM simulations using four dimensions: Context, Model, Persona, and Topic. We use this framework to measure open-ended LLM simulations' susceptibility to caricature, defined via two criteria: individuation and exaggeration. We evaluate the level of caricature in scenarios from existing work on LLM simulations. We find that for GPT-4, simulations of certain demographics (political and marginalized groups) and topics (general, uncontroversial) are highly susceptible to caricature.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "LLMs" is translated as "人工智能语言模型" (Rényou Jīngjì Yǔyán Módel), which is a more common term in Simplified Chinese.* "simulations" is translated as "模拟" (Móxīng), which is a more general term that can refer to any kind of imitation or representation.* "caricature" is translated as "轮廓" (Lúnkè), which is a more precise term that specifically refers to an exaggerated or distorted representation of someone or something.* "individuation" is translated as "个体化" (Gètǐ Huà), which is a term that specifically refers to the process of creating unique and distinct individuals.* "exaggeration" is translated as "夸大" (Kuòdà), which is a term that specifically refers to the act of amplifying or enlarging something beyond its normal size or proportion.
</details></li>
</ul>
<hr>
<h2 id="Seeking-Neural-Nuggets-Knowledge-Transfer-in-Large-Language-Models-from-a-Parametric-Perspective"><a href="#Seeking-Neural-Nuggets-Knowledge-Transfer-in-Large-Language-Models-from-a-Parametric-Perspective" class="headerlink" title="Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective"></a>Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11451">http://arxiv.org/abs/2310.11451</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maszhongming/ParaKnowTransfer">https://github.com/maszhongming/ParaKnowTransfer</a></li>
<li>paper_authors: Ming Zhong, Chenxin An, Weizhu Chen, Jiawei Han, Pengcheng He</li>
<li>for: 本研究旨在empirically investigate知识传递FROM大型语言模型（LLMs）到小型语言模型（SLMs）的 Parametric perspective。</li>
<li>methods: 我们使用敏感性技术提取和对齐知识特定参数 между不同的LLMs，并使用LoRA模块作为中介机制将提取的知识注入到SLMs中。</li>
<li>results: 我们的evaluaions across four benchmarks validate the efficacy of our proposed method, highlighting the critical factors contributing to the process of parametric knowledge transfer and underscoring the transferability of model parameters across LLMs of different scales.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) inherently encode a wealth of knowledge within their parameters through pre-training on extensive corpora. While prior research has delved into operations on these parameters to manipulate the underlying implicit knowledge (encompassing detection, editing, and merging), there remains an ambiguous understanding regarding their transferability across models with varying scales. In this paper, we seek to empirically investigate knowledge transfer from larger to smaller models through a parametric perspective. To achieve this, we employ sensitivity-based techniques to extract and align knowledge-specific parameters between different LLMs. Moreover, the LoRA module is used as the intermediary mechanism for injecting the extracted knowledge into smaller models. Evaluations across four benchmarks validate the efficacy of our proposed method. Our findings highlight the critical factors contributing to the process of parametric knowledge transfer, underscoring the transferability of model parameters across LLMs of different scales. We release code and data at \url{https://github.com/maszhongming/ParaKnowTransfer}.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）内置了丰富的知识于其参数中，通过前期训练于广泛的文本Corpus。 although prior research has explored operations on these parameters to manipulate the underlying implicit knowledge (including detection, editing, and merging), there is still an ambiguous understanding of their transferability across models with varying scales. In this paper, we aim to empirically investigate knowledge transfer from larger to smaller models through a parametric perspective. To achieve this, we use sensitivity-based techniques to extract and align knowledge-specific parameters between different LLMs. Moreover, the LoRA module is used as the intermediary mechanism for injecting the extracted knowledge into smaller models. Evaluations across four benchmarks validate the efficacy of our proposed method. Our findings highlight the critical factors contributing to the process of parametric knowledge transfer, underscoring the transferability of model parameters across LLMs of different scales. We release code and data at \url{https://github.com/maszhongming/ParaKnowTransfer}.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Explaining-Deep-Neural-Networks-for-Bearing-Fault-Detection-with-Vibration-Concepts"><a href="#Explaining-Deep-Neural-Networks-for-Bearing-Fault-Detection-with-Vibration-Concepts" class="headerlink" title="Explaining Deep Neural Networks for Bearing Fault Detection with Vibration Concepts"></a>Explaining Deep Neural Networks for Bearing Fault Detection with Vibration Concepts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11450">http://arxiv.org/abs/2310.11450</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Decker, Michael Lebacher, Volker Tresp</li>
<li>for: 本研究旨在应用概念基于的解释方法，以帮助解释复杂深度神经网络在振荡信号上进行磨损fault探测中的预测结果。</li>
<li>methods: 本研究使用了已有的概念基于的解释技术，包括概念活化矢量，来衡量输入数据中抽象或高级特征如何影响神经网络的预测结果。</li>
<li>results: 本研究的评估结果表明，通过使用概念基于的解释方法，可以提供人类可理解的和直观的探测结果，但是需要首先验证下面的假设。<details>
<summary>Abstract</summary>
Concept-based explanation methods, such as Concept Activation Vectors, are potent means to quantify how abstract or high-level characteristics of input data influence the predictions of complex deep neural networks. However, applying them to industrial prediction problems is challenging as it is not immediately clear how to define and access appropriate concepts for individual use cases and specific data types. In this work, we investigate how to leverage established concept-based explanation techniques in the context of bearing fault detection with deep neural networks trained on vibration signals. Since bearings are prevalent in almost every rotating equipment, ensuring the reliability of intransparent fault detection models is crucial to prevent costly repairs and downtimes of industrial machinery. Our evaluations demonstrate that explaining opaque models in terms of vibration concepts enables human-comprehensible and intuitive insights about their inner workings, but the underlying assumptions need to be carefully validated first.
</details>
<details>
<summary>摘要</summary>
通用概念解释方法，如概念启动向量，是复杂深度神经网络预测结果的强大手段。然而，在实际应用中存在许多挑战，因为不清楚如何定义和访问特定用例和数据类型的适当概念。在这种情况下，我们研究如何在深度神经网络对振荡信号进行报废疾病检测时，使用已有的概念基本解释技术。由于机器设备中的承轮很普遍，因此确保报废模型的可靠性是关键，以避免高昂的维护和机器停机成本。我们的评估表明，通过将深度神经网络解释为振荡概念的方式，可以提供人类可理解和直观的内部启示，但是下面的假设需要首先进行验证。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Model-Prediction-Capabilities-Evidence-from-a-Real-World-Forecasting-Tournament"><a href="#Large-Language-Model-Prediction-Capabilities-Evidence-from-a-Real-World-Forecasting-Tournament" class="headerlink" title="Large Language Model Prediction Capabilities: Evidence from a Real-World Forecasting Tournament"></a>Large Language Model Prediction Capabilities: Evidence from a Real-World Forecasting Tournament</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13014">http://arxiv.org/abs/2310.13014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philipp Schoenegger, Peter S. Park</li>
<li>for: 测试大语言模型GPT-4的预测能力</li>
<li>methods: 使用GPT-4参加三个月的预测赛，测试其对未来事件的预测能力</li>
<li>results: GPT-4的预测不如人群预测准确，并且没有显著的偏好 towards any particular answer.<details>
<summary>Abstract</summary>
Accurately predicting the future would be an important milestone in the capabilities of artificial intelligence. However, research on the ability of large language models to provide probabilistic predictions about future events remains nascent. To empirically test this ability, we enrolled OpenAI's state-of-the-art large language model, GPT-4, in a three-month forecasting tournament hosted on the Metaculus platform. The tournament, running from July to October 2023, attracted 843 participants and covered diverse topics including Big Tech, U.S. politics, viral outbreaks, and the Ukraine conflict. Focusing on binary forecasts, we show that GPT-4's probabilistic forecasts are significantly less accurate than the median human-crowd forecasts. We find that GPT-4's forecasts did not significantly differ from the no-information forecasting strategy of assigning a 50% probability to every question. We explore a potential explanation, that GPT-4 might be predisposed to predict probabilities close to the midpoint of the scale, but our data do not support this hypothesis. Overall, we find that GPT-4 significantly underperforms in real-world predictive tasks compared to median human-crowd forecasts. A potential explanation for this underperformance is that in real-world forecasting tournaments, the true answers are genuinely unknown at the time of prediction; unlike in other benchmark tasks like professional exams or time series forecasting, where strong performance may at least partly be due to the answers being memorized from the training data. This makes real-world forecasting tournaments an ideal environment for testing the generalized reasoning and prediction capabilities of artificial intelligence going forward.
</details>
<details>
<summary>摘要</summary>
如果可以准确预测未来，那将是人工智能的重要突破口。然而，关于大语言模型是否能够提供关于未来事件的 probabilistic 预测的研究仍然处于早期阶段。为了实证这一能力，我们在2023年7月至10月的三个月时间内参加了OpenAI的state-of-the-art大语言模型GPT-4在Metaculus平台上的三个月预测赛。这场赛事吸引了843名参与者，涵盖了多个话题，包括大科技、美国政治、艾滋疫情和乌克兰冲突。我们对binary forecast进行了分析，发现GPT-4的 probabilistic 预测与人群预测的 median 相比远不准确。我们发现GPT-4的预测与无信息预测策略（每个问题的概率为50%）没有显著差异。我们还探讨了一个可能的解释：GPT-4可能倾向于预测概率较 бли至中点的情况，但我们的数据不支持这一假设。总的来说，我们发现GPT-4在真实世界的预测任务中表现 significatively 差，与人群预测的 median 相比。一个可能的解释是，在真实世界预测赛事中，答案并不是在预测时已知的；与其他benchmark任务like专业考试或时间序列预测不同，在这些任务中，AI的出色表现可能归结于训练数据中Memorization。这些因素使得真实世界预测赛事成为测试人工智能总化逻辑和预测能力的理想环境。
</details></li>
</ul>
<hr>
<h2 id="Functional-Invariants-to-Watermark-Large-Transformers"><a href="#Functional-Invariants-to-Watermark-Large-Transformers" class="headerlink" title="Functional Invariants to Watermark Large Transformers"></a>Functional Invariants to Watermark Large Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11446">http://arxiv.org/abs/2310.11446</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fernandez Pierre, Couairon Guillaume, Furon Teddy, Douze Matthijs</li>
<li>for: 保护大型模型的完整性和所有权</li>
<li>methods: 利用模型的对称性进行功能等价的替换操作，不需要对 weights 进行优化，可以在非盲目白盒设置下实现 watermarking</li>
<li>results: 实验表明该方法可以保持模型的输出不变，同时具有耐变性和隐蔽性，可以实用地保护大型模型的完整性和所有权<details>
<summary>Abstract</summary>
The rapid growth of transformer-based models increases the concerns about their integrity and ownership insurance. Watermarking addresses this issue by embedding a unique identifier into the model, while preserving its performance. However, most existing approaches require to optimize the weights to imprint the watermark signal, which is not suitable at scale due to the computational cost. This paper explores watermarks with virtually no computational cost, applicable to a non-blind white-box setting (assuming access to both the original and watermarked networks). They generate functionally equivalent copies by leveraging the models' invariance, via operations like dimension permutations or scaling/unscaling. This enables to watermark models without any change in their outputs and remains stealthy. Experiments demonstrate the effectiveness of the approach and its robustness against various model transformations (fine-tuning, quantization, pruning), making it a practical solution to protect the integrity of large models.
</details>
<details>
<summary>摘要</summary>
“transformer模型的快速增长带来了其完整性和所有权保险的问题。水印可以解决这个问题，通过在模型中嵌入唯一标识符，保持其性能。但现有的方法通常需要优化权重来印制水印信号，这会在大规模执行中带来计算成本问题。这篇论文探讨了免计算成本的水印方法，适用于非盲目白盒设定（即可以访问原始网络和水印网络）。它通过利用模型的变换不变性，通过维度重新排序或缩放/减速等操作，生成功能相同的模型复制。这些复制品可以隐蔽地携带水印信号，无需改变模型的输出。实验表明该方法的有效性和鲁棒性，可以实际地保护大型模型的完整性。”
</details></li>
</ul>
<hr>
<h2 id="Set-of-Mark-Prompting-Unleashes-Extraordinary-Visual-Grounding-in-GPT-4V"><a href="#Set-of-Mark-Prompting-Unleashes-Extraordinary-Visual-Grounding-in-GPT-4V" class="headerlink" title="Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V"></a>Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11441">http://arxiv.org/abs/2310.11441</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, Jianfeng Gao</li>
<li>for: 提高大型多Modal模型（LMMs）的视觉固定能力</li>
<li>methods: 使用可购买的交互分割模型（如SAM）将图像分成不同粒度的区域，并在这些区域上显示marks（如字母、面罩、盒子）</li>
<li>results: GPT-4V与SoM相比，在RefCOCOg zero-shot Setting中表现出比州前进referring segmentation模型更高的性能<details>
<summary>Abstract</summary>
We present Set-of-Mark (SoM), a new visual prompting method, to unleash the visual grounding abilities of large multimodal models (LMMs), such as GPT-4V. As illustrated in Fig. 1 (right), we employ off-the-shelf interactive segmentation models, such as SAM, to partition an image into regions at different levels of granularity, and overlay these regions with a set of marks e.g., alphanumerics, masks, boxes. Using the marked image as input, GPT-4V can answer the questions that require visual grounding. We perform a comprehensive empirical study to validate the effectiveness of SoM on a wide range of fine-grained vision and multimodal tasks. For example, our experiments show that GPT-4V with SoM outperforms the state-of-the-art fully-finetuned referring segmentation model on RefCOCOg in a zero-shot setting.
</details>
<details>
<summary>摘要</summary>
我团队在这篇论文中提出了一种新的视觉提示方法，即Set-of-Mark（SoM），用于解 liberate大型多 modal模型（LMMs）的视觉固定能力。如图1（右）所示，我们使用商业化的交互式分割模型，如SAM，将图像分成不同粒度的区域，并将这些区域 overlay 以不同的标记，如字母、面具、盒子。使用这些标记的图像作为输入，GPT-4V可以回答需要视觉固定的问题。我们进行了广泛的实验研究，以验证SoM在多种细化视觉和多模态任务中的效果。例如，我们的实验表明，GPT-4V与SoM在RefCOCOg中的零基础情况下，与现有的全部finetune Referring Segmentation模型相比，表现出了更好的性能。
</details></li>
</ul>
<hr>
<h2 id="Understanding-deep-neural-networks-through-the-lens-of-their-non-linearity"><a href="#Understanding-deep-neural-networks-through-the-lens-of-their-non-linearity" class="headerlink" title="Understanding deep neural networks through the lens of their non-linearity"></a>Understanding deep neural networks through the lens of their non-linearity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11439">http://arxiv.org/abs/2310.11439</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quentin Bouniot, Ievgen Redko, Anton Mallasto, Charlotte Laclau, Karol Arndt, Oliver Struckmeier, Markus Heinonen, Ville Kyrki, Samuel Kaski</li>
<li>for: 本研究旨在提供一种 theoretically sound 的方法来跟踪深度神经网络中非线性的传播。</li>
<li>methods: 本研究使用的方法包括计算激活函数的非线性度，并通过提出一种 affinity score 来评估不同架构和学习策略中的非线性传播。</li>
<li>results: 实验结果表明，提出的 affinity score 能够帮助我们更好地理解深度神经网络中的非线性传播，并且具有广泛的实际应用前景。<details>
<summary>Abstract</summary>
The remarkable success of deep neural networks (DNN) is often attributed to their high expressive power and their ability to approximate functions of arbitrary complexity. Indeed, DNNs are highly non-linear models, and activation functions introduced into them are largely responsible for this. While many works studied the expressive power of DNNs through the lens of their approximation capabilities, quantifying the non-linearity of DNNs or of individual activation functions remains an open problem. In this paper, we propose the first theoretically sound solution to track non-linearity propagation in deep neural networks with a specific focus on computer vision applications. Our proposed affinity score allows us to gain insights into the inner workings of a wide range of different architectures and learning paradigms. We provide extensive experimental results that highlight the practical utility of the proposed affinity score and its potential for long-reaching applications.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）的异常成功常被归结于它们的高表达力和能够近似任何复杂性函数的能力。实际上，DNN是非线性模型，活动函数引入到它们中央对此负有责任。虽然许多研究对DNN的表达力进行了研究，但量化DNN的非线性或各个活动函数的非线性仍然是一个开放的问题。在这篇论文中，我们提出了首个理论上有sound的解决方案，用于跟踪深度神经网络中非线性的传播。我们提出的相互关系分数允许我们深入了解各种不同架构和学习方法的内部工作机制。我们提供了广泛的实验结果， highlighting the practical utility of the proposed affinity score and its potential for long-reaching applications.
</details></li>
</ul>
<hr>
<h2 id="Evaluating-LLMs-for-Privilege-Escalation-Scenarios"><a href="#Evaluating-LLMs-for-Privilege-Escalation-Scenarios" class="headerlink" title="Evaluating LLMs for Privilege-Escalation Scenarios"></a>Evaluating LLMs for Privilege-Escalation Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11409">http://arxiv.org/abs/2310.11409</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ipa-lab/hackingBuddyGPT">https://github.com/ipa-lab/hackingBuddyGPT</a></li>
<li>paper_authors: Andreas Happe, Aaron Kaplan, Jürgen Cito</li>
<li>for: 这个论文旨在探讨语言模型（LLM）在静态分析中的应用，以及它们在特权提升方面的能力和挑战。</li>
<li>methods: 作者使用了自动化的Linux特权提升测试 benchmark，并开发了一种基于 LLM 的特权提升工具，用于评估不同的 LLM 和提示策略。</li>
<li>results: 研究发现，LLM 可以在特权提升方面具有很高的能力，但也存在一些挑战，如维持测试中的集中性、处理错误等。<details>
<summary>Abstract</summary>
Penetration testing, an essential component of cybersecurity, allows organizations to proactively identify and remediate vulnerabilities in their systems, thus bolstering their defense mechanisms against potential cyberattacks. One recent advancement in the realm of penetration testing is the utilization of Language Models (LLMs). We explore the intersection of LLMs and penetration testing to gain insight into their capabilities and challenges in the context of privilige escalation. We create an automated Linux privilege-escalation benchmark utilizing local virtual machines. We introduce an LLM-guided privilege-escalation tool designed for evaluating different LLMs and prompt strategies against our benchmark. We analyze the impact of different prompt designs, the benefits of in-context learning, and the advantages of offering high-level guidance to LLMs. We discuss challenging areas for LLMs, including maintaining focus during testing, coping with errors, and finally comparing them with both stochastic parrots as well as with human hackers.
</details>
<details>
<summary>摘要</summary>
To evaluate the performance of LLMs in privilege escalation, we created an automated Linux privilege-escalation benchmark using local virtual machines. We also developed an LLM-guided privilege-escalation tool that assesses different LLMs and prompt strategies against our benchmark. Our analysis reveals the impact of various prompt designs, the benefits of in-context learning, and the advantages of providing high-level guidance to LLMs.However, LLMs also face challenges, such as maintaining focus during testing, coping with errors, and comparing with both stochastic parrots and human hackers. By understanding these challenges and the capabilities of LLMs, we can better utilize them in penetration testing to enhance the security of our systems.
</details></li>
</ul>
<hr>
<h2 id="Neural-Attention-Enhancing-QKV-Calculation-in-Self-Attention-Mechanism-with-Neural-Networks"><a href="#Neural-Attention-Enhancing-QKV-Calculation-in-Self-Attention-Mechanism-with-Neural-Networks" class="headerlink" title="Neural Attention: Enhancing QKV Calculation in Self-Attention Mechanism with Neural Networks"></a>Neural Attention: Enhancing QKV Calculation in Self-Attention Mechanism with Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11398">http://arxiv.org/abs/2310.11398</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ocislyjrti/neuralattention">https://github.com/ocislyjrti/neuralattention</a></li>
<li>paper_authors: Muhan Zhang</li>
<li>for: 这篇论文是用于探讨一种新的自注意力机制，即通过特殊设计的神经网络结构来计算查询、键和值（QKV），以提高自注意力机制的表现。</li>
<li>methods: 这篇论文使用了一个 modificated Marian model，通过实验证明了其对IWSLT 2017德国英语翻译任务数据集的表现有所提高。此外，这篇论文还证明了它的方法在训练Roberta模型时，对Wikitext-103数据集的表现也有所改善。</li>
<li>results: 实验结果显示，这篇论文的方法可以提高BLEU scores，并且在训练Roberta模型时，对模型的误差率有所降低。这些实验结果不仅证明了这篇论文的方法的有效性，而且还显示了对自注意力机制的优化可以通过神经网络基础的QKV计算，对未来的研究和实际应用具有广泛的应用前景。<details>
<summary>Abstract</summary>
In the realm of deep learning, the self-attention mechanism has substantiated its pivotal role across a myriad of tasks, encompassing natural language processing and computer vision. Despite achieving success across diverse applications, the traditional self-attention mechanism primarily leverages linear transformations for the computation of query, key, and value (QKV), which may not invariably be the optimal choice under specific circumstances. This paper probes into a novel methodology for QKV computation-implementing a specially-designed neural network structure for the calculation. Utilizing a modified Marian model, we conducted experiments on the IWSLT 2017 German-English translation task dataset and juxtaposed our method with the conventional approach. The experimental results unveil a significant enhancement in BLEU scores with our method. Furthermore, our approach also manifested superiority when training the Roberta model with the Wikitext-103 dataset, reflecting a notable reduction in model perplexity compared to its original counterpart. These experimental outcomes not only validate the efficacy of our method but also reveal the immense potential in optimizing the self-attention mechanism through neural network-based QKV computation, paving the way for future research and practical applications. The source code and implementation details for our proposed method can be accessed at https://github.com/ocislyjrti/NeuralAttention.
</details>
<details>
<summary>摘要</summary>
在深度学习领域，自注意机制在多种任务中发挥了重要作用，包括自然语言处理和计算机视觉。尽管在多种应用中取得成功，传统的自注意机制通常使用线性变换来计算查询、关键和值（QKV），这可能不一定是最佳选择在特定情况下。这篇论文探讨了一种新的QKV计算方法-通过特制的神经网络结构来实现。我们使用修改后的马里安模型进行实验，并在IWSLT 2017德语英语翻译任务数据集上对我们的方法与传统方法进行了比较。实验结果显示我们的方法可以显著提高BLEU分数，而且我们的方法也在使用Wikitext-103数据集训练Roberta模型时表现出了明显的降低模型困惑度，相比其原始对手。这些实验结果不仅证明了我们的方法的有效性，还暴露了优化自注意机制的可能性，铺开了未来研究和实践应用的道路。我们的代码和实现细节可以通过https://github.com/ocislyjrti/NeuralAttention访问。
</details></li>
</ul>
<hr>
<h2 id="Towards-Automatic-Satellite-Images-Captions-Generation-Using-Large-Language-Models"><a href="#Towards-Automatic-Satellite-Images-Captions-Generation-Using-Large-Language-Models" class="headerlink" title="Towards Automatic Satellite Images Captions Generation Using Large Language Models"></a>Towards Automatic Satellite Images Captions Generation Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11392">http://arxiv.org/abs/2310.11392</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yingxu He, Qiqi Sun</li>
<li>for:  automatic remote sensing image captioning</li>
<li>methods:  using large language models (LLMs) to guide the description of object annotations</li>
<li>results:  effective collection of captions for remote sensing images<details>
<summary>Abstract</summary>
Automatic image captioning is a promising technique for conveying visual information using natural language. It can benefit various tasks in satellite remote sensing, such as environmental monitoring, resource management, disaster management, etc. However, one of the main challenges in this domain is the lack of large-scale image-caption datasets, as they require a lot of human expertise and effort to create. Recent research on large language models (LLMs) has demonstrated their impressive performance in natural language understanding and generation tasks. Nonetheless, most of them cannot handle images (GPT-3.5, Falcon, Claude, etc.), while conventional captioning models pre-trained on general ground-view images often fail to produce detailed and accurate captions for aerial images (BLIP, GIT, CM3, CM3Leon, etc.). To address this problem, we propose a novel approach: Automatic Remote Sensing Image Captioning (ARSIC) to automatically collect captions for remote sensing images by guiding LLMs to describe their object annotations. We also present a benchmark model that adapts the pre-trained generative image2text model (GIT) to generate high-quality captions for remote-sensing images. Our evaluation demonstrates the effectiveness of our approach for collecting captions for remote sensing images.
</details>
<details>
<summary>摘要</summary>
自动图像描述技术是落实视觉信息使用自然语言的有前途技术。它可以帮助卫星遥感任务中的各种任务，如环境监测、资源管理、灾害管理等。然而，这个领域的主要挑战之一是缺乏大规模的图像描述数据集，因为需要大量的人工专业和努力来创建。latest research on large language models (LLMs) has shown their impressive performance in natural language understanding and generation tasks. However, most of them cannot handle images (GPT-3.5, Falcon, Claude, etc.), while conventional captioning models pre-trained on general ground-view images often fail to produce detailed and accurate captions for aerial images (BLIP, GIT, CM3, CM3Leon, etc.). To address this problem, we propose a novel approach: Automatic Remote Sensing Image Captioning (ARSIC) to automatically collect captions for remote sensing images by guiding LLMs to describe their object annotations. We also present a benchmark model that adapts the pre-trained generative image2text model (GIT) to generate high-quality captions for remote-sensing images. Our evaluation demonstrates the effectiveness of our approach for collecting captions for remote sensing images.
</details></li>
</ul>
<hr>
<h2 id="End-to-End-real-time-tracking-of-children’s-reading-with-pointer-network"><a href="#End-to-End-real-time-tracking-of-children’s-reading-with-pointer-network" class="headerlink" title="End-to-End real time tracking of children’s reading with pointer network"></a>End-to-End real time tracking of children’s reading with pointer network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11486">http://arxiv.org/abs/2310.11486</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vishal Sunder, Beulah Karrolla, Eric Fosler-Lussier</li>
<li>for: 这个论文的目的是如何fficiently构建儿童语音实时读物追踪器。</li>
<li>methods: 这个论文使用了一种全端到端模型，而不是以前提出的ASR-based缓存方法。它使用了一个指针网络，直接学习预测文本中的位置，并通过强制对齐来训练指针网络。</li>
<li>results: 这个论文的结果表明，使用一种基于强制对齐的神经网络模型可以达到至少与Montreal强制对齐器相同的对齐精度，并且 surprisingly 是一个更好的训练信号 для指针网络。结果表明，在一个成人语音数据集（TIMIT）和两个儿童语音数据集（CMU Kids和Reading Races）上，这个最佳模型可以高精度地跟踪成人语音（87.8%）和儿童语音（77.1%）。<details>
<summary>Abstract</summary>
In this work, we explore how a real time reading tracker can be built efficiently for children's voices. While previously proposed reading trackers focused on ASR-based cascaded approaches, we propose a fully end-to-end model making it less prone to lags in voice tracking. We employ a pointer network that directly learns to predict positions in the ground truth text conditioned on the streaming speech. To train this pointer network, we generate ground truth training signals by using forced alignment between the read speech and the text being read on the training set. Exploring different forced alignment models, we find a neural attention based model is at least as close in alignment accuracy to the Montreal Forced Aligner, but surprisingly is a better training signal for the pointer network. Our results are reported on one adult speech data (TIMIT) and two children's speech datasets (CMU Kids and Reading Races). Our best model can accurately track adult speech with 87.8% accuracy and the much harder and disfluent children's speech with 77.1% accuracy on CMU Kids data and a 65.3% accuracy on the Reading Races dataset.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们探讨了如何fficiently构建儿童语音实时读写追踪器。之前的提议的读写追踪器都是基于ASR顺序的搅拌方法，而我们提议一个完全端到端模型，使其更少受到语音追踪延迟。我们使用一个指针网络，直接学习 predict文本中的位置，条件于流动的Speech。为了训练这个指针网络，我们生成了ground truth训练信号，通过强制对READING speech和被读文本之间进行对齐。我们explore了不同的强制对齐模型，发现一个神经网络注意力基于模型，与蒙特利尔强制对齐器准确性相当，但是 surprisingly是一个更好的训练信号 для指针网络。我们的结果在一个成人语音数据（TIMIT）和两个儿童语音数据集（CMU Kids和Reading Races）上被报告。我们的最佳模型可以准确地跟踪成人语音87.8%的准确率，以及更难和不稳定的儿童语音77.1%的准确率在CMU Kids数据集上，以及65.3%的准确率在Reading Races数据集上。
</details></li>
</ul>
<hr>
<h2 id="The-effect-of-stemming-and-lemmatization-on-Portuguese-fake-news-text-classification"><a href="#The-effect-of-stemming-and-lemmatization-on-Portuguese-fake-news-text-classification" class="headerlink" title="The effect of stemming and lemmatization on Portuguese fake news text classification"></a>The effect of stemming and lemmatization on Portuguese fake news text classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11344">http://arxiv.org/abs/2310.11344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucca de Freitas Santos, Murilo Varges da Silva</li>
<li>for: 本研究旨在探讨自动伪新闻检测的问题，尤其是在语言学方面，以提高伪新闻检测的精度。</li>
<li>methods: 本研究使用了lemmatization和stemming等预处理技术，并设计了一些类ifier模型，以测试预处理技术对伪新闻分类的影响。</li>
<li>results: 结果显示，预处理步骤对伪新闻分类有着重要的影响，lemmatization和stemming等技术可以帮助提高伪新闻检测的精度。<details>
<summary>Abstract</summary>
With the popularization of the internet, smartphones and social media, information is being spread quickly and easily way, which implies bigger traffic of information in the world, but there is a problem that is harming society with the dissemination of fake news. With a bigger flow of information, some people are trying to disseminate deceptive information and fake news. The automatic detection of fake news is a challenging task because to obtain a good result is necessary to deal with linguistics problems, especially when we are dealing with languages that not have been comprehensively studied yet, besides that, some techniques can help to reach a good result when we are dealing with text data, although, the motivation of detecting this deceptive information it is in the fact that the people need to know which information is true and trustful and which one is not. In this work, we present the effect the pre-processing methods such as lemmatization and stemming have on fake news classification, for that we designed some classifier models applying different pre-processing techniques. The results show that the pre-processing step is important to obtain betters results, the stemming and lemmatization techniques are interesting methods and need to be more studied to develop techniques focused on the Portuguese language so we can reach better results.
</details>
<details>
<summary>摘要</summary>
In this work, we investigate the effect of pre-processing methods, such as lemmatization and stemming, on fake news classification. We designed several classifier models using different pre-processing techniques and analyzed the results. Our findings show that the pre-processing step is crucial for achieving better results. The stemming and lemmatization techniques are promising methods that deserve further study, particularly for the Portuguese language, to improve the accuracy of fake news detection.
</details></li>
</ul>
<hr>
<h2 id="Dual-Cognitive-Architecture-Incorporating-Biases-and-Multi-Memory-Systems-for-Lifelong-Learning"><a href="#Dual-Cognitive-Architecture-Incorporating-Biases-and-Multi-Memory-Systems-for-Lifelong-Learning" class="headerlink" title="Dual Cognitive Architecture: Incorporating Biases and Multi-Memory Systems for Lifelong Learning"></a>Dual Cognitive Architecture: Incorporating Biases and Multi-Memory Systems for Lifelong Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11341">http://arxiv.org/abs/2310.11341</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neurai-lab/dn4il-dataset">https://github.com/neurai-lab/dn4il-dataset</a></li>
<li>paper_authors: Shruthi Gowda, Bahram Zonooz, Elahe Arani<br>for: 这篇论文的目的是探讨人工神经网络（ANNs）在静止独立数据上的局限性，并提出一种基于人类认知结构和多记忆系统的新框架，以实现人工智能的持久学习能力。methods: 该框架基于多种人类认知结构和多记忆系统，包括多个子系统、隐式和显式知识表示分离、偏见适应和多记忆系统。它还包括一个概率适应学习器，用于编码形态信息，从而避免ANNs学习本地文本的偏好。results: 在不同的设定和数据集上，DUCA显示出了改进的表现，并且不需要额外信息。此外，DUCA还在一个复杂的域逐渐学习数据集DN4IL上表现出优异，证明了其在面临分布转换时的多样化持久学习能力。<details>
<summary>Abstract</summary>
Artificial neural networks (ANNs) exhibit a narrow scope of expertise on stationary independent data. However, the data in the real world is continuous and dynamic, and ANNs must adapt to novel scenarios while also retaining the learned knowledge to become lifelong learners. The ability of humans to excel at these tasks can be attributed to multiple factors ranging from cognitive computational structures, cognitive biases, and the multi-memory systems in the brain. We incorporate key concepts from each of these to design a novel framework, Dual Cognitive Architecture (DUCA), which includes multiple sub-systems, implicit and explicit knowledge representation dichotomy, inductive bias, and a multi-memory system. The inductive bias learner within DUCA is instrumental in encoding shape information, effectively countering the tendency of ANNs to learn local textures. Simultaneously, the inclusion of a semantic memory submodule facilitates the gradual consolidation of knowledge, replicating the dynamics observed in fast and slow learning systems, reminiscent of the principles underpinning the complementary learning system in human cognition. DUCA shows improvement across different settings and datasets, and it also exhibits reduced task recency bias, without the need for extra information. To further test the versatility of lifelong learning methods on a challenging distribution shift, we introduce a novel domain-incremental dataset DN4IL. In addition to improving performance on existing benchmarks, DUCA also demonstrates superior performance on this complex dataset.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Agent-Specific-Effects"><a href="#Agent-Specific-Effects" class="headerlink" title="Agent-Specific Effects"></a>Agent-Specific Effects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11334">http://arxiv.org/abs/2310.11334</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stelios30/agent-specific-effects">https://github.com/stelios30/agent-specific-effects</a></li>
<li>paper_authors: Stelios Triantafyllou, Aleksa Sukovic, Debmalya Mandal, Goran Radanovic</li>
<li>for: 本研究的目标是提供一种系统性的方法，以评估多个代理的决策对结果的影响。</li>
<li>methods: 本研究使用多个代理Markov决策过程，引入代理特有的影响（Agent-Specific Effects，ASE），用于评估代理的决策对结果的影响。</li>
<li>results: 研究发现，使用cf-ASE可以准确地评估多个代理的决策对结果的影响，并且可以通过实验 validate 这种方法。<details>
<summary>Abstract</summary>
Establishing causal relationships between actions and outcomes is fundamental for accountable multi-agent decision-making. However, interpreting and quantifying agents' contributions to such relationships pose significant challenges. These challenges are particularly prominent in the context of multi-agent sequential decision-making, where the causal effect of an agent's action on the outcome depends on how the other agents respond to that action. In this paper, our objective is to present a systematic approach for attributing the causal effects of agents' actions to the influence they exert on other agents. Focusing on multi-agent Markov decision processes, we introduce agent-specific effects (ASE), a novel causal quantity that measures the effect of an agent's action on the outcome that propagates through other agents. We then turn to the counterfactual counterpart of ASE (cf-ASE), provide a sufficient set of conditions for identifying cf-ASE, and propose a practical sampling-based algorithm for estimating it. Finally, we experimentally evaluate the utility of cf-ASE through a simulation-based testbed, which includes a sepsis management environment.
</details>
<details>
<summary>摘要</summary>
We focus on multi-agent Markov decision processes and introduce agent-specific effects (ASE), a novel causal quantity that measures the effect of an agent's action on the outcome that propagates through other agents. We then introduce the counterfactual counterpart of ASE (cf-ASE) and provide a set of conditions for identifying it. We propose a practical sampling-based algorithm for estimating cf-ASE.We experimentally evaluate the utility of cf-ASE through a simulation-based testbed, including a sepsis management environment. By attributing the causal effects of agents' actions to their influence on other agents, our approach enables accountable decision-making in multi-agent systems.
</details></li>
</ul>
<hr>
<h2 id="Key-Point-based-Orientation-Estimation-of-Strawberries-for-Robotic-Fruit-Picking"><a href="#Key-Point-based-Orientation-Estimation-of-Strawberries-for-Robotic-Fruit-Picking" class="headerlink" title="Key Point-based Orientation Estimation of Strawberries for Robotic Fruit Picking"></a>Key Point-based Orientation Estimation of Strawberries for Robotic Fruit Picking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11333">http://arxiv.org/abs/2310.11333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Justin Le Louëdec, Grzegorz Cielniak</li>
<li>For:  This paper aims to address the issue of labor shortages in modern agriculture by developing a robotic harvesting system that can accurately and efficiently pick fruit.* Methods: The proposed method uses key-point-based fruit orientation estimation, which allows for the prediction of 3D orientation from 2D images directly. The method does not require full 3D orientation annotations and can exploit such information for improved accuracy.* Results: The proposed method achieves state-of-the-art performance with an average error of $8^\circ$, improving predictions by $\sim30%$ compared to previous work. The method also has fast inference times of $\sim30$ms, making it suitable for real-time robotic applications.Here is the text in Simplified Chinese:* For: 这篇论文目的是解决现代农业中的劳动力短缺问题，通过开发一种可以准确地和高效地采集水果的 роботизирован采集系统。* Methods: 该方法使用关键点基于的水果方向估计方法，可以直接从2D图像中预测3D方向。该方法不需要全部3D方向注释，可以利用这些信息来提高准确性。* Results: 该方法在两个独立的苺果图像集上取得了状态机器人应用中的最佳性能，错误率为8度，提高了前一个作者在~\cite{wagner2021efficient}中提出的前一个作者的30%。此外，该方法的推理时间为30ms，适用于实时机器人应用。<details>
<summary>Abstract</summary>
Selective robotic harvesting is a promising technological solution to address labour shortages which are affecting modern agriculture in many parts of the world. For an accurate and efficient picking process, a robotic harvester requires the precise location and orientation of the fruit to effectively plan the trajectory of the end effector. The current methods for estimating fruit orientation employ either complete 3D information which typically requires registration from multiple views or rely on fully-supervised learning techniques, which require difficult-to-obtain manual annotation of the reference orientation. In this paper, we introduce a novel key-point-based fruit orientation estimation method allowing for the prediction of 3D orientation from 2D images directly. The proposed technique can work without full 3D orientation annotations but can also exploit such information for improved accuracy. We evaluate our work on two separate datasets of strawberry images obtained from real-world data collection scenarios. Our proposed method achieves state-of-the-art performance with an average error as low as $8^{\circ}$, improving predictions by $\sim30\%$ compared to previous work presented in~\cite{wagner2021efficient}. Furthermore, our method is suited for real-time robotic applications with fast inference times of $\sim30$ms.
</details>
<details>
<summary>摘要</summary>
选择性机器人收割是一种有前途的技术解决方案，用于解决现代农业中的劳动力短缺问题。为了实现准确和高效的摘取过程，机器人收割器需要准确地知道果实的位置和方向。现有的果实方向估计方法可以通过多视图注册或完全超级vised学习技术来实现，但这些方法具有困难获得的手动参考方向的缺点。在本文中，我们介绍了一种新的关键点基于果实方向估计方法，可以直接从2D图像中预测3D方向。我们的提议方法不需要全部3D方向注释，但也可以利用这些信息以提高准确性。我们在两个独立的苹果图像集上进行了评估，并达到了现有最佳性能，具体错误为$8^{\circ}$，提高了前一个工作（refer to ~\cite{wagner2021efficient}）的预测值 by $\sim30\%$。此外，我们的方法适用于实时机器人应用程序，推理时间为$\sim30$ms。
</details></li>
</ul>
<hr>
<h2 id="Quantifying-Language-Models’-Sensitivity-to-Spurious-Features-in-Prompt-Design-or-How-I-learned-to-start-worrying-about-prompt-formatting"><a href="#Quantifying-Language-Models’-Sensitivity-to-Spurious-Features-in-Prompt-Design-or-How-I-learned-to-start-worrying-about-prompt-formatting" class="headerlink" title="Quantifying Language Models’ Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting"></a>Quantifying Language Models’ Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11324">http://arxiv.org/abs/2310.11324</a></li>
<li>repo_url: None</li>
<li>paper_authors: Melanie Sclar, Yejin Choi, Yulia Tsvetkov, Alane Suhr</li>
<li>for: 本研究旨在准确评估现代预训练语言模型（LLM）的性能，并确定prompt设计的重要性。</li>
<li>methods: 本研究使用了多种现有的开源LLM，并研究了它们对提示格式的敏感性。我们提出了FormatSpread算法，可以快速评估任务中的多个可能的提示格式，并对性能进行interval的预测。</li>
<li>results: 我们发现，使用不同的提示格式可以导致LLM的性能差异非常大，最大差异可达76个准确率点。此外，我们还发现，不同的模型之间的格式敏感性强相关，这提出了评估LLM性能的方法ологи问题。<details>
<summary>Abstract</summary>
As large language models (LLMs) are adopted as a fundamental component of language technologies, it is crucial to accurately characterize their performance. Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model. In this work, we focus on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting. We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning. Our analysis suggests that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible prompt formats, instead of the currently-standard practice of reporting performance on a single format. We also show that format performance only weakly correlates between models, which puts into question the methodological validity of comparing models with an arbitrarily chosen, fixed prompt format. To facilitate systematic analysis we propose FormatSpread, an algorithm that rapidly evaluates a sampled set of plausible prompt formats for a given task, and reports the interval of expected performance without accessing model weights. Furthermore, we present a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在语言技术中扮演重要角色，因此精确测量其性能是非常重要的。因为选择提示的设计可以强烈影响模型的行为，因此这个设计过程是使用任何现代预训练的生成语言模型时非常重要。在这个工作中，我们专注于 LLM 对提示格式的敏感性。我们发现了许多常用的开源 LLM 在少量示例设定下表现出敏感性，对 LLLaMA-2-13B 的表现范围为 Up to 76 个精确度点。这些敏感性在提高模型大小、几何示例数量或进行指令调整时仍然存在。我们的分析表明，使用提示方式进行评估 LLM 的方法不应该只报告单一的表现方式，而是应该报告一个范围的表现。此外，我们还证明了不同模型之间的格式表现相互弱相联，这让我们对比模型使用随机选择的固定提示方式的方法存在问题。为了促进系统性的分析，我们提出 FormatSpread，一个算法可以快速评估任务中的可能提示格式，并报告预期的表现范围，不需要访问模型的材料。此外，我们还进行了一系列的分析，以探讨这种敏感性的性质，包括探讨具体的原子变化和内部表现的特点。
</details></li>
</ul>
<hr>
<h2 id="Utilising-a-Large-Language-Model-to-Annotate-Subject-Metadata-A-Case-Study-in-an-Australian-National-Research-Data-Catalogue"><a href="#Utilising-a-Large-Language-Model-to-Annotate-Subject-Metadata-A-Case-Study-in-an-Australian-National-Research-Data-Catalogue" class="headerlink" title="Utilising a Large Language Model to Annotate Subject Metadata: A Case Study in an Australian National Research Data Catalogue"></a>Utilising a Large Language Model to Annotate Subject Metadata: A Case Study in an Australian National Research Data Catalogue</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11318">http://arxiv.org/abs/2310.11318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiwei Zhang, Mingfang Wu, Xiuzhen Zhang</li>
<li>for: 本研究提出了一种利用大语言模型（LLM）进行 kost-effective 的主题元数据注释的方法，以便提高数据挖掘和复用性。</li>
<li>methods: 本方法使用 GPT-3.5，并通过自动生成的提示来注释主题元数据。然而，基于Context learning的模型无法学习专业领域规则，导致一些类别的表现较差。</li>
<li>results: 本研究表明，使用 GPT-3.5 进行主题元数据注释可以达到可观的效果，但是基于Context learning的模型无法学习专业领域规则，导致一些类别的表现较差。<details>
<summary>Abstract</summary>
In support of open and reproducible research, there has been a rapidly increasing number of datasets made available for research. As the availability of datasets increases, it becomes more important to have quality metadata for discovering and reusing them. Yet, it is a common issue that datasets often lack quality metadata due to limited resources for data curation. Meanwhile, technologies such as artificial intelligence and large language models (LLMs) are progressing rapidly. Recently, systems based on these technologies, such as ChatGPT, have demonstrated promising capabilities for certain data curation tasks. This paper proposes to leverage LLMs for cost-effective annotation of subject metadata through the LLM-based in-context learning. Our method employs GPT-3.5 with prompts designed for annotating subject metadata, demonstrating promising performance in automatic metadata annotation. However, models based on in-context learning cannot acquire discipline-specific rules, resulting in lower performance in several categories. This limitation arises from the limited contextual information available for subject inference. To the best of our knowledge, we are introducing, for the first time, an in-context learning method that harnesses large language models for automated subject metadata annotation.
</details>
<details>
<summary>摘要</summary>
为支持开放和可重复的研究，现有的数据集数量在不断增加。随着数据集的可用性增加，高质量的元数据成为发现和重用数据的关键。然而，由于数据整理有限的资源，数据集经常缺乏高质量的元数据。在这种情况下，人工智能和大语言模型（LLM）的技术在不断进步。最近，基于这些技术的系统，如ChatGPT，在某些数据整理任务中表现出了承诺的能力。本文提议利用LLM进行cost-effective的主题元数据注释。我们的方法使用GPT-3.5，并通过特制的提示来注释主题元数据，表现出了可观的自动元数据注释性能。然而，基于内容学习的模型无法学习专业领域的规则，导致在某些类别上表现较差。这种局限性来自于内容学习模型所lack的专业领域知识。根据我们所知，我们是第一次在内容学习中使用大语言模型进行自动主题元数据注释。
</details></li>
</ul>
<hr>
<h2 id="Generative-error-correction-for-code-switching-speech-recognition-using-large-language-models"><a href="#Generative-error-correction-for-code-switching-speech-recognition-using-large-language-models" class="headerlink" title="Generative error correction for code-switching speech recognition using large language models"></a>Generative error correction for code-switching speech recognition using large language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13013">http://arxiv.org/abs/2310.13013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Chen, Yuchen Hu, Chao-Han Huck Yang, Hexin Liu, Sabato Marco Siniscalchi, Eng Siong Chng</li>
<li>for: 提高Code-switching自动语音识别（CS-ASR）精度</li>
<li>methods: 利用大型语言模型（LLM）和自动语音识别（ASR）生成的N最佳 гипотезы，并通过学习 гипотезы至转写（H2T）映射来直接预测准确转写。</li>
<li>results: 实验证明，使用这种方法可以显著提高CS-ASR的精度，降低混合错误率（MER）。同时，LLM在H2T学习中表现出了remarkable的数据效率，可能解决CS-ASR在低资源语言中的数据稀缺问题。<details>
<summary>Abstract</summary>
Code-switching (CS) speech refers to the phenomenon of mixing two or more languages within the same sentence. Despite the recent advances in automatic speech recognition (ASR), CS-ASR is still a challenging task ought to the grammatical structure complexity of the phenomenon and the data scarcity of specific training corpus. In this work, we propose to leverage large language models (LLMs) and lists of hypotheses generated by an ASR to address the CS problem. Specifically, we first employ multiple well-trained ASR models for N-best hypotheses generation, with the aim of increasing the diverse and informative elements in the set of hypotheses. Next, we utilize the LLMs to learn the hypotheses-to-transcription (H2T) mapping by adding a trainable low-rank adapter. Such a generative error correction (GER) method directly predicts the accurate transcription according to its expert linguistic knowledge and N-best hypotheses, resulting in a paradigm shift from the traditional language model rescoring or error correction techniques. Experimental evidence demonstrates that GER significantly enhances CS-ASR accuracy, in terms of reduced mixed error rate (MER). Furthermore, LLMs show remarkable data efficiency for H2T learning, providing a potential solution to the data scarcity problem of CS-ASR in low-resource languages.
</details>
<details>
<summary>摘要</summary>
��enson switching (CS) ���ô��� speech refers to the phenomenon of mixing two or more languages within the same sentence. Despite the recent advances in automatic speech recognition (ASR), CS-ASR is still a challenging task due to the grammatical structure complexity of the phenomenon and the data scarcity of specific training corpus. In this work, we propose to leverage large language models (LLMs) and lists of hypotheses generated by an ASR to address the CS problem. Specifically, we first employ multiple well-trained ASR models for N-best hypotheses generation, with the aim of increasing the diverse and informative elements in the set of hypotheses. Next, we utilize the LLMs to learn the hypotheses-to-transcription (H2T) mapping by adding a trainable low-rank adapter. Such a generative error correction (GER) method directly predicts the accurate transcription according to its expert linguistic knowledge and N-best hypotheses, resulting in a paradigm shift from the traditional language model rescoring or error correction techniques. Experimental evidence demonstrates that GER significantly enhances CS-ASR accuracy, in terms of reduced mixed error rate (MER). Furthermore, LLMs show remarkable data efficiency for H2T learning, providing a potential solution to the data scarcity problem of CS-ASR in low-resource languages.
</details></li>
</ul>
<hr>
<h2 id="MonoSKD-General-Distillation-Framework-for-Monocular-3D-Object-Detection-via-Spearman-Correlation-Coefficient"><a href="#MonoSKD-General-Distillation-Framework-for-Monocular-3D-Object-Detection-via-Spearman-Correlation-Coefficient" class="headerlink" title="MonoSKD: General Distillation Framework for Monocular 3D Object Detection via Spearman Correlation Coefficient"></a>MonoSKD: General Distillation Framework for Monocular 3D Object Detection via Spearman Correlation Coefficient</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11316">http://arxiv.org/abs/2310.11316</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/senwang98/monoskd">https://github.com/senwang98/monoskd</a></li>
<li>paper_authors: Sen Wang, Jin Zheng</li>
<li>for: 本文提出了一种基于Spearman相关系数的知识传授框架，用于解决单视图3D物体检测中的困难问题。</li>
<li>methods: 该方法使用Spearman相关系数来学习交叉模式特征之间的相对相关性，并通过选择合适的传授位置和移除 redundancy 来降低 GPU 资源消耗和训练时间。</li>
<li>results: 在KITTI 3D物体检测 benchmark 上进行了广泛的实验，并证明了我们的方法可以在挑战性的情况下达到最佳性能，而且无需额外的计算成本。<details>
<summary>Abstract</summary>
Monocular 3D object detection is an inherently ill-posed problem, as it is challenging to predict accurate 3D localization from a single image. Existing monocular 3D detection knowledge distillation methods usually project the LiDAR onto the image plane and train the teacher network accordingly. Transferring LiDAR-based model knowledge to RGB-based models is more complex, so a general distillation strategy is needed. To alleviate cross-modal prob-lem, we propose MonoSKD, a novel Knowledge Distillation framework for Monocular 3D detection based on Spearman correlation coefficient, to learn the relative correlation between cross-modal features. Considering the large gap between these features, strict alignment of features may mislead the training, so we propose a looser Spearman loss. Furthermore, by selecting appropriate distillation locations and removing redundant modules, our scheme saves more GPU resources and trains faster than existing methods. Extensive experiments are performed to verify the effectiveness of our framework on the challenging KITTI 3D object detection benchmark. Our method achieves state-of-the-art performance until submission with no additional inference computational cost. Our codes are available at https://github.com/Senwang98/MonoSKD
</details>
<details>
<summary>摘要</summary>
单目3D物体检测是一个自然的问题，因为从单一图像中预测 precisel 3D位置是困难的。现有的单目3D检测知识传承方法通常是将LiDAR投射到图像平面上，然后对教师网络进行训练。将LiDAR基础的模型知识转移到RGB基础的模型上是更加复杂的，因此需要一个通用的传承策略。为了解决两 modal 之间的问题，我们提出了MonoSKD，一个基于Spearman相関系数的知识传承框架 для单目3D检测。由于这两种特征之间的差距很大，严格对特征进行对齐可能会导致训练失败，因此我们提出了一个更宽松的Spearman损失。此外，我们选择了适当的传承位置和移除了额外的模组，使我们的方案可以更好地运用GPU资源，训练速度更快。我们进行了广泛的实验，以验证我们的框架在KITTI 3D物体检测标准 benchmark 上的效果。我们的方法可以在提交前的测试中实现state-of-the-art的性能，并且不需要额外的推论计算成本。我们的代码可以在https://github.com/Senwang98/MonoSKD 上找到。
</details></li>
</ul>
<hr>
<h2 id="MiniZero-Comparative-Analysis-of-AlphaZero-and-MuZero-on-Go-Othello-and-Atari-Games"><a href="#MiniZero-Comparative-Analysis-of-AlphaZero-and-MuZero-on-Go-Othello-and-Atari-Games" class="headerlink" title="MiniZero: Comparative Analysis of AlphaZero and MuZero on Go, Othello, and Atari Games"></a>MiniZero: Comparative Analysis of AlphaZero and MuZero on Go, Othello, and Atari Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11305">http://arxiv.org/abs/2310.11305</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rlglab/minizero">https://github.com/rlglab/minizero</a></li>
<li>paper_authors: Ti-Rong Wu, Hung Guei, Po-Wei Huang, Pei-Chiun Peng, Ting Han Wei, Chung-Chin Shih, Yun-Jui Tsai</li>
<li>for: 本研究旨在提供一个零知识学习框架，支持四种现今最佳算法，包括AlphaZero、MuZero、Gumbel AlphaZero和Gumbel MuZero。这些算法在许多游戏中表现出了超人般的表现，但是哪一个算法在具体任务上最适合哪一个还未明确。通过MiniZero，我们系统地评估了每个算法在9x9囲棋和8x8奥特洛两个棋盘游戏以及57个Atari游戏中的表现。</li>
<li>methods: 我们使用MiniZero框架进行零知识学习，系统地评估了每个算法在不同的游戏中的表现。我们还引入了一种进步的训练方法，即进步的 simulation，可以将计算资源分配更有效率。</li>
<li>results: 我们的实验结果显示，在两个棋盘游戏中，使用更多的 simulations 通常会导致更高的表现。但是，在不同的游戏中，AlphaZero和MuZero的选择可能会因游戏特性而异。在Atari游戏中，MuZero和Gumbel MuZero都是值得考虑的。进步的 simulation 可以将计算资源分配更有效率，并在两个棋盘游戏中获得了明显的提升。<details>
<summary>Abstract</summary>
This paper presents MiniZero, a zero-knowledge learning framework that supports four state-of-the-art algorithms, including AlphaZero, MuZero, Gumbel AlphaZero, and Gumbel MuZero. While these algorithms have demonstrated super-human performance in many games, it remains unclear which among them is most suitable or efficient for specific tasks. Through MiniZero, we systematically evaluate the performance of each algorithm in two board games, 9x9 Go and 8x8 Othello, as well as 57 Atari games. Our empirical findings are summarized as follows. For two board games, using more simulations generally results in higher performance. However, the choice of AlphaZero and MuZero may differ based on game properties. For Atari games, both MuZero and Gumbel MuZero are worth considering. Since each game has unique characteristics, different algorithms and simulations yield varying results. In addition, we introduce an approach, called progressive simulation, which progressively increases the simulation budget during training to allocate computation more efficiently. Our empirical results demonstrate that progressive simulation achieves significantly superior performance in two board games. By making our framework and trained models publicly available, this paper contributes a benchmark for future research on zero-knowledge learning algorithms, assisting researchers in algorithm selection and comparison against these zero-knowledge learning baselines.
</details>
<details>
<summary>摘要</summary>
For two board games, using more simulations generally leads to higher performance. However, the choice between AlphaZero and MuZero may depend on the properties of the game. For Atari games, both MuZero and Gumbel MuZero are worth considering. As each game has unique characteristics, different algorithms and simulations produce varying results.We also introduce an approach called progressive simulation, which increases the simulation budget during training to allocate computation more efficiently. Our results show that progressive simulation achieves significantly better performance in two board games. By making our framework and trained models publicly available, this paper provides a benchmark for future research on zero-knowledge learning algorithms, assisting researchers in selecting and comparing against these baselines.
</details></li>
</ul>
<hr>
<h2 id="Emulating-Human-Cognitive-Processes-for-Expert-Level-Medical-Question-Answering-with-Large-Language-Models"><a href="#Emulating-Human-Cognitive-Processes-for-Expert-Level-Medical-Question-Answering-with-Large-Language-Models" class="headerlink" title="Emulating Human Cognitive Processes for Expert-Level Medical Question-Answering with Large Language Models"></a>Emulating Human Cognitive Processes for Expert-Level Medical Question-Answering with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11266">http://arxiv.org/abs/2310.11266</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khushboo Verma, Marina Moore, Stephanie Wottrich, Karla Robles López, Nishant Aggarwal, Zeel Bhatt, Aagamjit Singh, Bradford Unroe, Salah Basheer, Nitish Sachdeva, Prinka Arora, Harmanjeet Kaur, Tanupreet Kaur, Tevon Hood, Anahi Marquez, Tushar Varshney, Nanfu Deng, Azaan Ramani, Pawanraj Ishwara, Maimoona Saeed, Tatiana López Velarde Peña, Bryan Barksdale, Sushovan Guha, Satwant Kumar</li>
<li>for:  This paper aims to provide a novel framework for clinical problem-solving tools in healthcare, based on a Large Language Model (LLM) that mimics human cognitive processes.</li>
<li>methods:  The framework, called BooksMed, utilizes the GRADE (Grading of Recommendations, Assessment, Development, and Evaluations) framework to effectively quantify evidence strength, and is evaluated using a multispecialty clinical benchmark called ExpertMedQA, which consists of open-ended, expert-level clinical questions validated by a diverse group of medical professionals.</li>
<li>results:  The paper shows that BooksMed outperforms existing state-of-the-art models Med-PaLM 2, Almanac, and ChatGPT in a variety of medical scenarios, demonstrating the effectiveness of the framework in providing reliable and evidence-based responses to clinical inquiries.<details>
<summary>Abstract</summary>
In response to the pressing need for advanced clinical problem-solving tools in healthcare, we introduce BooksMed, a novel framework based on a Large Language Model (LLM). BooksMed uniquely emulates human cognitive processes to deliver evidence-based and reliable responses, utilizing the GRADE (Grading of Recommendations, Assessment, Development, and Evaluations) framework to effectively quantify evidence strength. For clinical decision-making to be appropriately assessed, an evaluation metric that is clinically aligned and validated is required. As a solution, we present ExpertMedQA, a multispecialty clinical benchmark comprised of open-ended, expert-level clinical questions, and validated by a diverse group of medical professionals. By demanding an in-depth understanding and critical appraisal of up-to-date clinical literature, ExpertMedQA rigorously evaluates LLM performance. BooksMed outperforms existing state-of-the-art models Med-PaLM 2, Almanac, and ChatGPT in a variety of medical scenarios. Therefore, a framework that mimics human cognitive stages could be a useful tool for providing reliable and evidence-based responses to clinical inquiries.
</details>
<details>
<summary>摘要</summary>
响应医疗领域的高级临床问题解决工具的急需，我们介绍BooksMed，一种新的框架，基于大型自然语言模型（LLM）。BooksMed模仿人类认知过程，以提供基于证据的可靠回答，使用GRADE（评估、评价、发展和评估）框架来有效地评估证据强度。为了正确评估临床决策，需要一种与临床相关的评价标准，而我们提出ExpertMedQA，一个多学科临床引用库，由多个医疗专业人员组成，并被证明了。通过要求对当前临床文献进行深入理解和批判性评估，ExpertMedQA严格评估LLM性能。BooksMed在多种医疗场景中表现出色，超越了现有的state-of-the-art模型Med-PaLM 2、Almanac和ChatGPT。因此，一个模仿人类认知阶段的框架可能是一种有用的工具，用于提供可靠和基于证据的回答临床问题。
</details></li>
</ul>
<hr>
<h2 id="Revealing-the-Unwritten-Visual-Investigation-of-Beam-Search-Trees-to-Address-Language-Model-Prompting-Challenges"><a href="#Revealing-the-Unwritten-Visual-Investigation-of-Beam-Search-Trees-to-Address-Language-Model-Prompting-Challenges" class="headerlink" title="Revealing the Unwritten: Visual Investigation of Beam Search Trees to Address Language Model Prompting Challenges"></a>Revealing the Unwritten: Visual Investigation of Beam Search Trees to Address Language Model Prompting Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11252">http://arxiv.org/abs/2310.11252</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thilo Spinner, Rebecca Kehlbeck, Rita Sevastjanova, Tobias Stähle, Daniel A. Keim, Oliver Deussen, Andreas Spitz, Mennatallah El-Assady</li>
<li>for: 本研究旨在探讨大语言模型的生成过程中，如何通过提示方法来引导模型输出。</li>
<li>methods: 本研究使用了一种交互式视觉方法，可以帮助分析大语言模型在生成过程中的决策。</li>
<li>results: 研究发现，通过曝光搜索树可以提供有价值的信息，并提供了五种细化分析场景来解决一些关注点。这些结果 validate 现有结果，并提供了新的视角。<details>
<summary>Abstract</summary>
The growing popularity of generative language models has amplified interest in interactive methods to guide model outputs. Prompt refinement is considered one of the most effective means to influence output among these methods. We identify several challenges associated with prompting large language models, categorized into data- and model-specific, linguistic, and socio-linguistic challenges. A comprehensive examination of model outputs, including runner-up candidates and their corresponding probabilities, is needed to address these issues. The beam search tree, the prevalent algorithm to sample model outputs, can inherently supply this information. Consequently, we introduce an interactive visual method for investigating the beam search tree, facilitating analysis of the decisions made by the model during generation. We quantitatively show the value of exposing the beam search tree and present five detailed analysis scenarios addressing the identified challenges. Our methodology validates existing results and offers additional insights.
</details>
<details>
<summary>摘要</summary>
随着生成语言模型的普及，对生成输出的指导方法已经引起了更多的关注。Prompt refinement被认为是影响输出的最有效的方法之一。我们identified several challenges associated with prompting large language models，分为数据特定和模型特定、语言学的和社会语言学的挑战。为了解决这些问题，我们需要进行全面的模型输出的检查，包括 runner-up candidates和其对应的概率。为此，我们引入了一种互动视觉方法，用于调查生成过程中模型做出的决定。我们量化地表明了曝光 beam search tree的价值，并提出了五种细化分析场景，用于解决我们所 identific的挑战。我们的方法证明了现有结果的正确性，同时还提供了更多的洞察。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Large-Language-Model-for-Automatic-Evolving-of-Industrial-Data-Centric-R-D-Cycle"><a href="#Leveraging-Large-Language-Model-for-Automatic-Evolving-of-Industrial-Data-Centric-R-D-Cycle" class="headerlink" title="Leveraging Large Language Model for Automatic Evolving of Industrial Data-Centric R&amp;D Cycle"></a>Leveraging Large Language Model for Automatic Evolving of Industrial Data-Centric R&amp;D Cycle</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11249">http://arxiv.org/abs/2310.11249</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xu Yang, Xiao Yang, Weiqing Liu, Jinhui Li, Peng Yu, Zeqi Ye, Jiang Bian</li>
<li>for: 这篇论文旨在探讨大语言模型（LLM）在数据驱动研发中的潜力，以减少人工、计算和时间资源成本。</li>
<li>methods: 该论文利用大语言模型进行数据驱动研发的各个基础元素的研究，包括异ogeneous任务相关数据、多面领域知识和多种计算功能工具。</li>
<li>results: 研究表明，LLM可以快速理解域pecific需求，生成专业意见，使用域specific工具进行实验、解释结果并吸收过去努力的知识来解决新的挑战。示例来自于量化投资研究领域。<details>
<summary>Abstract</summary>
In the wake of relentless digital transformation, data-driven solutions are emerging as powerful tools to address multifarious industrial tasks such as forecasting, anomaly detection, planning, and even complex decision-making. Although data-centric R&D has been pivotal in harnessing these solutions, it often comes with significant costs in terms of human, computational, and time resources. This paper delves into the potential of large language models (LLMs) to expedite the evolution cycle of data-centric R&D. Assessing the foundational elements of data-centric R&D, including heterogeneous task-related data, multi-facet domain knowledge, and diverse computing-functional tools, we explore how well LLMs can understand domain-specific requirements, generate professional ideas, utilize domain-specific tools to conduct experiments, interpret results, and incorporate knowledge from past endeavors to tackle new challenges. We take quantitative investment research as a typical example of industrial data-centric R&D scenario and verified our proposed framework upon our full-stack open-sourced quantitative research platform Qlib and obtained promising results which shed light on our vision of automatic evolving of industrial data-centric R&D cycle.
</details>
<details>
<summary>摘要</summary>
在数字变革不断的背景下，数据驱动的解决方案正在赋予企业多种任务，如预测、异常检测、规划和复杂决策等。虽然数据研发总是数据驱动的关键，但它常常带来人工、计算和时间资源的成本。这篇论文探讨了大语言模型（LLM）在加速数据驱动研发的演化过程中的潜力。我们评估了数据驱动研发的基础元素，包括异常任务相关数据、多元领域知识和多种计算函数工具，以 explored how well LLMs can understand domain-specific requirements, generate professional ideas, utilize domain-specific tools to conduct experiments, interpret results, and incorporate knowledge from past endeavors to tackle new challenges.我们选择了投资研究为典型的工业数据驱动研发场景，并在我们的全栈开源 quantitative research 平台Qlib上验证了我们的提议方案，获得了鼓舞人的结果，这有助于我们实现自动化工业数据驱动研发ecycle的vision。
</details></li>
</ul>
<hr>
<h2 id="Query2Triple-Unified-Query-Encoding-for-Answering-Diverse-Complex-Queries-over-Knowledge-Graphs"><a href="#Query2Triple-Unified-Query-Encoding-for-Answering-Diverse-Complex-Queries-over-Knowledge-Graphs" class="headerlink" title="Query2Triple: Unified Query Encoding for Answering Diverse Complex Queries over Knowledge Graphs"></a>Query2Triple: Unified Query Encoding for Answering Diverse Complex Queries over Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11246">http://arxiv.org/abs/2310.11246</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yaooxu/q2t">https://github.com/yaooxu/q2t</a></li>
<li>paper_authors: Yao Xu, Shizhu He, Cunguang Wang, Li Cai, Kang Liu, Jun Zhao</li>
<li>for: 本研究旨在解决知识图(KG)中复杂问题(CQA)的挑战，提出了一种新的方法Query to Triple(Q2T)，用于解决复杂问题。</li>
<li>methods: Q2T方法分为两个阶段：首先，使用神经链预测器在简单 вопро题上预测tail实体，然后使用复杂问题的查询编码器来编码多样化的复杂问题。</li>
<li>results: 实验表明，无需直接模型神经集算器，Q2T方法仍然可以达到多个公共benchmark上的状态作呈现性能。<details>
<summary>Abstract</summary>
Complex Query Answering (CQA) is a challenge task of Knowledge Graph (KG). Due to the incompleteness of KGs, query embedding (QE) methods have been proposed to encode queries and entities into the same embedding space, and treat logical operators as neural set operators to obtain answers. However, these methods train KG embeddings and neural set operators concurrently on both simple (one-hop) and complex (multi-hop and logical) queries, which causes performance degradation on simple queries and low training efficiency. In this paper, we propose Query to Triple (Q2T), a novel approach that decouples the training for simple and complex queries. Q2T divides the training into two stages: (1) Pre-training a neural link predictor on simple queries to predict tail entities based on the head entity and relation. (2) Training a query encoder on complex queries to encode diverse complex queries into a unified triple form that can be efficiently solved by the pretrained neural link predictor. Our proposed Q2T is not only efficient to train, but also modular, thus easily adaptable to various neural link predictors that have been studied well. Extensive experiments demonstrate that, even without explicit modeling for neural set operators, Q2T still achieves state-of-the-art performance on diverse complex queries over three public benchmarks.
</details>
<details>
<summary>摘要</summary>
困难任务复杂问答 (CQA) 是知识图 (KG) 的挑战。由于知识图的不完整性，问题嵌入 (QE) 方法被提议，以将查询和实体编码到同一个嵌入空间中，并将逻辑运算视为神经集合运算来获取答案。然而，这些方法同时在简单 (一元) 和复杂 (多元和逻辑) 查询上培训 KG 嵌入和神经集合运算，导致简单查询的性能下降和培训效率低。在这篇论文中，我们提出了查询到三元 (Q2T)，一种新的方法，它将培训分解成两个阶段：1. 预训练神经链预测器在简单查询上预测尾实体基于头实体和关系。2. 培训查询编码器在复杂查询上编码多样化的复杂查询，以便由预训神经链预测器有效解决。我们提出的 Q2T 不仅具有高效的培训效率，还具有可组合的特点，因此容易适应各种已研究的神经链预测器。我们的实验证明，无需显式模型神经集合运算，Q2T仍然在多种多样的复杂查询上达到了状态艺术级别的表现。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Class-incremental-Learning-in-the-Era-of-Large-Pre-trained-Models-via-Test-Time-Adaptation"><a href="#Rethinking-Class-incremental-Learning-in-the-Era-of-Large-Pre-trained-Models-via-Test-Time-Adaptation" class="headerlink" title="Rethinking Class-incremental Learning in the Era of Large Pre-trained Models via Test-Time Adaptation"></a>Rethinking Class-incremental Learning in the Era of Large Pre-trained Models via Test-Time Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11482">http://arxiv.org/abs/2310.11482</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iemprog/ttacil">https://github.com/iemprog/ttacil</a></li>
<li>paper_authors: Imad Eddine Marouf, Subhankar Roy, Enzo Tartaglione, Stéphane Lathuilière</li>
<li>for: 这篇论文的目的是为了解决分类增量学习（CIL）任务中的问题，即在新任务上进行分类时，不要忘记之前学习的信息。</li>
<li>methods: 这篇论文使用了大量预训模型（PTM）的表现，并在每个任务上进行微调 Parameter 来获得最佳性能。然而，重复微调会使PTM的表现变差，导致忘记之前的任务。为了寻找一个平衡点，这篇论文提出了一种新的方法，即在试验阶段进行调整（TTA），而不是在训练阶段进行微调。</li>
<li>results: 这篇论文的结果显示，TTACIL可以在多个 CIL 标准 benchmark 上进行最佳化，并且在不同的数据损害情况下保持稳定性。此外，TTACIL 还可以避免忘记之前的任务，同时对每个任务都获得良好的表现。<details>
<summary>Abstract</summary>
Class-incremental learning (CIL) is a challenging task that involves continually learning to categorize classes into new tasks without forgetting previously learned information. The advent of the large pre-trained models (PTMs) has fast-tracked the progress in CIL due to the highly transferable PTM representations, where tuning a small set of parameters results in state-of-the-art performance when compared with the traditional CIL methods that are trained from scratch. However, repeated fine-tuning on each task destroys the rich representations of the PTMs and further leads to forgetting previous tasks. To strike a balance between the stability and plasticity of PTMs for CIL, we propose a novel perspective of eliminating training on every new task and instead performing test-time adaptation (TTA) directly on the test instances. Concretely, we propose "Test-Time Adaptation for Class-Incremental Learning" (TTACIL) that first fine-tunes Layer Norm parameters of the PTM on each test instance for learning task-specific features, and then resets them back to the base model to preserve stability. As a consequence, TTACIL does not undergo any forgetting, while benefiting each task with the rich PTM features. Additionally, by design, our method is robust to common data corruptions. Our TTACIL outperforms several state-of-the-art CIL methods when evaluated on multiple CIL benchmarks under both clean and corrupted data.
</details>
<details>
<summary>摘要</summary>
通过不断学习新任务的类增量学习（CIL），我们可以让模型在新任务上进行分类。然而，在某些情况下，由于重复地训练每个任务，这会导致模型忘记之前学习的信息。为了找到PTM的稳定和可变性的平衡，我们提出了一种新的思路，即在测试实例上进行测试时适应（TTA）。具体来说，我们提出了一种名为“测试时适应for类增量学习”（TTACIL）的方法，它首先在每个测试实例上使用层 нор的参数进行微调，以学习任务特有的特征，然后将其重置回基本模型，以保持稳定性。因此，TTACIL不会出现忘记现象，同时每个任务都可以benefit于PTM的丰富特征。此外，由设计，我们的方法具有对常见数据损害的Robustness。我们的TTACIL在多个CIL标准测试 benchmark上评估得到了与state-of-the-art CIL方法相比的更好的性能。
</details></li>
</ul>
<hr>
<h2 id="RealBehavior-A-Framework-for-Faithfully-Characterizing-Foundation-Models’-Human-like-Behavior-Mechanisms"><a href="#RealBehavior-A-Framework-for-Faithfully-Characterizing-Foundation-Models’-Human-like-Behavior-Mechanisms" class="headerlink" title="RealBehavior: A Framework for Faithfully Characterizing Foundation Models’ Human-like Behavior Mechanisms"></a>RealBehavior: A Framework for Faithfully Characterizing Foundation Models’ Human-like Behavior Mechanisms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11227">http://arxiv.org/abs/2310.11227</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enyu Zhou, Rui Zheng, Zhiheng Xi, Songyang Gao, Xiaoran Fan, Zichu Fei, Jingting Ye, Tao Gui, Qi Zhang, Xuanjing Huang</li>
<li>for: 这 paper 是为了研究基于模型的人类行为的方法和效果而写的。</li>
<li>methods: 该 paper 使用了一个名为 RealBehavior 的框架，该框架可以评估模型的人类行为是否准确、可重复、内部一致和普适性。</li>
<li>results: 研究发现，直接使用心理学工具不能准确地描述所有人类行为，而 RealBehavior 框架可以评估模型的行为是否准确、可重复、内部一致和普适性。此外， paper 还讨论了将模型与人类和社会价值相对应的影响，并 argue 为多样化对象的定制来避免创建具有限制特征的模型。<details>
<summary>Abstract</summary>
Reports of human-like behaviors in foundation models are growing, with psychological theories providing enduring tools to investigate these behaviors. However, current research tends to directly apply these human-oriented tools without verifying the faithfulness of their outcomes. In this paper, we introduce a framework, RealBehavior, which is designed to characterize the humanoid behaviors of models faithfully. Beyond simply measuring behaviors, our framework assesses the faithfulness of results based on reproducibility, internal and external consistency, and generalizability. Our findings suggest that a simple application of psychological tools cannot faithfully characterize all human-like behaviors. Moreover, we discuss the impacts of aligning models with human and social values, arguing for the necessity of diversifying alignment objectives to prevent the creation of models with restricted characteristics.
</details>
<details>
<summary>摘要</summary>
研究人类行为模型的报告正在增长，心理理论提供了持续适用的工具来调查这些行为。然而，当前的研究通常直接采用人类 oriented 工具而不是验证其结果的准确性。在本文中，我们介绍了一个框架，即 RealBehavior，用于准确描述模型的人类行为。除了直接测量行为外，我们的框架还评估结果的准确性基于可重复性、内部和外部一致性以及泛化性。我们的发现表明，简单地应用心理工具不能准确描述所有人类行为。此外，我们还讨论了将模型与人类和社会价值Alignment的影响， arguing for the necessity of diversifying alignment objectives to prevent the creation of models with restricted characteristics.
</details></li>
</ul>
<hr>
<h2 id="Contracting-Tsetlin-Machine-with-Absorbing-Automata"><a href="#Contracting-Tsetlin-Machine-with-Absorbing-Automata" class="headerlink" title="Contracting Tsetlin Machine with Absorbing Automata"></a>Contracting Tsetlin Machine with Absorbing Automata</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11481">http://arxiv.org/abs/2310.11481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bimal Bhattarai, Ole-Christoffer Granmo, Lei Jiao, Per-Arne Andersen, Svein Anders Tunheim, Rishad Shafik, Alex Yakovlev</li>
<li>for: 提高逻辑学习的速度和能效性</li>
<li>methods: 使用稀疏TM和吸引TA状态</li>
<li>results: 加速学习，降低能耗<details>
<summary>Abstract</summary>
In this paper, we introduce a sparse Tsetlin Machine (TM) with absorbing Tsetlin Automata (TA) states. In brief, the TA of each clause literal has both an absorbing Exclude- and an absorbing Include state, making the learning scheme absorbing instead of ergodic. When a TA reaches an absorbing state, it will never leave that state again. If the absorbing state is an Exclude state, both the automaton and the literal can be removed from further consideration. The literal will as a result never participates in that clause. If the absorbing state is an Include state, on the other hand, the literal is stored as a permanent part of the clause while the TA is discarded. A novel sparse data structure supports these updates by means of three action lists: Absorbed Include, Include, and Exclude. By updating these lists, the TM gets smaller and smaller as the literals and their TA withdraw. In this manner, the computation accelerates during learning, leading to faster learning and less energy consumption.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种稀疏的Tsetlin机器（TM），它具有吸收型Tsetlin自动机（TA）状态。简而言之，每个clauseLiteral的TA具有两个吸收状态： exclude状态和include状态。当TA达到一个吸收状态时，它将不再离开该状态。如果吸收状态是exclude状态，那么自动机和Literal都可以从进一步考虑中除去。Literal因此从不会参与到该clause中。如果吸收状态是include状态，那么Literal将被保存为 clause中的永久部分，而TA则被抛弃。一种新的稀疏数据结构支持这些更新，通过三个动作列表：吸收包含、包含和排除。通过更新这些列表，TM会随着Literals和其TA减少，从而使计算加速，导致学习更快速、 consume less energy。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Fairness-Surrogate-Functions-in-Algorithmic-Fairness"><a href="#Understanding-Fairness-Surrogate-Functions-in-Algorithmic-Fairness" class="headerlink" title="Understanding Fairness Surrogate Functions in Algorithmic Fairness"></a>Understanding Fairness Surrogate Functions in Algorithmic Fairness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11211">http://arxiv.org/abs/2310.11211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Yao, Zhanke Zhou, Zhicong Li, Bo Han, Yong Liu</li>
<li>for: 本研究旨在 Mitigating 机器学习算法对certain population groups的偏袋预测，而 achieve comparable accuracy。</li>
<li>methods: 以实现 fairness 定义的 surrogate 函数来解决这个问题。但是，previous work 中的 fairness surrogate function 可能会导致不公平的结果。本研究通过对 demographic parity 的 fairness定义进行 theoretically 和 empirical 分析，发现 surrogate-fairness  gap 的存在，这个 gap 直接决定了 surrogate function 是否适合 fairness definition。</li>
<li>results: 我们提出了一个通用的 sigmoid surrogate 函数，具有严格和可靠的 fairness 保证。此外，我们还提出了一个 novel 的算法 Balanced Surrogate，可以逐步减少 surrogate-fairness gap，以改善公平性。最后，我们在三个真实世界数据集上提供了实践证据，显示我们的方法可以更好地保证公平性。<details>
<summary>Abstract</summary>
It has been observed that machine learning algorithms exhibit biased predictions against certain population groups. To mitigate such bias while achieving comparable accuracy, a promising approach is to introduce surrogate functions of the concerned fairness definition and solve a constrained optimization problem. However, an intriguing issue in previous work is that such fairness surrogate functions may yield unfair results. In this work, in order to deeply understand this issue, taking a widely used fairness definition, demographic parity as an example, we both theoretically and empirically show that there is a surrogate-fairness gap between the fairness definition and the fairness surrogate function. The "gap" directly determines whether a surrogate function is an appropriate substitute for a fairness definition. Also, the theoretical analysis and experimental results about the "gap" motivate us that the unbounded surrogate functions will be affected by the points far from the decision boundary, which is the large margin points issue investigated in this paper. To address it, we propose the general sigmoid surrogate with a rigorous and reliable fairness guarantee. Interestingly, the theory also provides insights into two important issues that deal with the large margin points as well as obtaining a more balanced dataset are beneficial to fairness. Furthermore, we elaborate a novel and general algorithm called Balanced Surrogate, which iteratively reduces the "gap" to improve fairness. Finally, we provide empirical evidence showing that our methods achieve better fairness performance in three real-world datasets.
</details>
<details>
<summary>摘要</summary>
observations have shown that machine learning algorithms can make biased predictions against certain groups of people. to address this issue, one approach is to use surrogate functions that satisfy certain fairness definitions. however, previous work has shown that these surrogate functions may not always lead to fair results. in this paper, we investigate the reason for this problem and show that there is a gap between the fairness definition and the surrogate function. this gap determines whether the surrogate function is an appropriate substitute for the fairness definition. we also show that the large margin points issue, which is the points far from the decision boundary, affects the unbounded surrogate functions. to address this issue, we propose a general sigmoid surrogate with a rigorous and reliable fairness guarantee. furthermore, we develop a novel and general algorithm called balanced surrogate, which iteratively reduces the gap to improve fairness. finally, we provide empirical evidence showing that our methods achieve better fairness performance in three real-world datasets.
</details></li>
</ul>
<hr>
<h2 id="EEG-motor-imagery-decoding-A-framework-for-comparative-analysis-with-channel-attention-mechanisms"><a href="#EEG-motor-imagery-decoding-A-framework-for-comparative-analysis-with-channel-attention-mechanisms" class="headerlink" title="EEG motor imagery decoding: A framework for comparative analysis with channel attention mechanisms"></a>EEG motor imagery decoding: A framework for comparative analysis with channel attention mechanisms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11198">http://arxiv.org/abs/2310.11198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Wimpff, Leonardo Gizzi, Jan Zerfowski, Bin Yang</li>
<li>for: 本研究探究了在大脑-计算机界面（BCI）中应用不同的通道注意力机制以提高电enzephalography（EEG）动作干обраoupe的解码性能。</li>
<li>methods: 本研究使用了不同的通道注意力机制，并将其集成到一个轻量级框架中，以评估它们的影响。我们采用了一个简单、轻量级的基线建模，可以方便地集成不同的注意力机制。</li>
<li>results: 我们的实验表明，使用不同的通道注意力机制可以提高EEG动作干обраoupe的性能，同时保持小的存储容量和低的计算复杂度。我们的框架具有简单性和普适性，可以在多个数据集上进行广泛的实验，以评估不同的注意力机制和基线建模的效果。<details>
<summary>Abstract</summary>
The objective of this study is to investigate the application of various channel attention mechanisms within the domain of brain-computer interface (BCI) for motor imagery decoding. Channel attention mechanisms can be seen as a powerful evolution of spatial filters traditionally used for motor imagery decoding. This study systematically compares such mechanisms by integrating them into a lightweight architecture framework to evaluate their impact. We carefully construct a straightforward and lightweight baseline architecture designed to seamlessly integrate different channel attention mechanisms. This approach is contrary to previous works which only investigate one attention mechanism and usually build a very complex, sometimes nested architecture. Our framework allows us to evaluate and compare the impact of different attention mechanisms under the same circumstances. The easy integration of different channel attention mechanisms as well as the low computational complexity enables us to conduct a wide range of experiments on three datasets to thoroughly assess the effectiveness of the baseline model and the attention mechanisms. Our experiments demonstrate the strength and generalizability of our architecture framework as well as how channel attention mechanisms can improve the performance while maintaining the small memory footprint and low computational complexity of our baseline architecture. Our architecture emphasizes simplicity, offering easy integration of channel attention mechanisms, while maintaining a high degree of generalizability across datasets, making it a versatile and efficient solution for EEG motor imagery decoding within brain-computer interfaces.
</details>
<details>
<summary>摘要</summary>
We construct a simple and lightweight baseline architecture that seamlessly integrates different channel attention mechanisms, unlike previous works that only investigate one mechanism and build complex, sometimes nested architectures. Our framework allows us to evaluate and compare the impact of different attention mechanisms under the same circumstances.The easy integration of different channel attention mechanisms and the low computational complexity enable us to conduct a wide range of experiments on three datasets to thoroughly assess the effectiveness of the baseline model and the attention mechanisms. Our experiments demonstrate the strength and generalizability of our architecture framework and how channel attention mechanisms can improve performance while maintaining a small memory footprint and low computational complexity.Our architecture emphasizes simplicity, allowing for easy integration of channel attention mechanisms while maintaining a high degree of generalizability across datasets, making it a versatile and efficient solution for EEG motor imagery decoding within BCIs.
</details></li>
</ul>
<hr>
<h2 id="Medical-Text-Simplification-Optimizing-for-Readability-with-Unlikelihood-Training-and-Reranked-Beam-Search-Decoding"><a href="#Medical-Text-Simplification-Optimizing-for-Readability-with-Unlikelihood-Training-and-Reranked-Beam-Search-Decoding" class="headerlink" title="Medical Text Simplification: Optimizing for Readability with Unlikelihood Training and Reranked Beam Search Decoding"></a>Medical Text Simplification: Optimizing for Readability with Unlikelihood Training and Reranked Beam Search Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11191">http://arxiv.org/abs/2310.11191</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ljyflores/simplification-project">https://github.com/ljyflores/simplification-project</a></li>
<li>paper_authors: Lorenzo Jaime Yu Flores, Heyuan Huang, Kejian Shi, Sophie Chheang, Arman Cohan</li>
<li>for:  bridging the communication gap in the medical field, where technical jargon and complex constructs are commonly used.</li>
<li>methods:  using a new unlikelihood loss and a reranked beam search decoding method to improve the readability of text simplification in the medical domain.</li>
<li>results:  better performance on readability metrics on three datasets, offering promising avenues for improving text simplification in the medical field.<details>
<summary>Abstract</summary>
Text simplification has emerged as an increasingly useful application of AI for bridging the communication gap in specialized fields such as medicine, where the lexicon is often dominated by technical jargon and complex constructs. Despite notable progress, methods in medical simplification sometimes result in the generated text having lower quality and diversity. In this work, we explore ways to further improve the readability of text simplification in the medical domain. We propose (1) a new unlikelihood loss that encourages generation of simpler terms and (2) a reranked beam search decoding method that optimizes for simplicity, which achieve better performance on readability metrics on three datasets. This study's findings offer promising avenues for improving text simplification in the medical field.
</details>
<details>
<summary>摘要</summary>
文本简化在医学领域中已经成为人工智能应用的一个日益有用的应用，用于bridging通信差距。然而，医学简化方法有时会导致生成的文本质量和多样性偏低。在这项工作中，我们探索了如何进一步提高医学简化文本的可读性。我们提出了（1）一种新的不可能损失函数，以便生成更简单的词汇，以及（2）一种重新排序搜索解码方法，以便优化简单性，这两种方法在三个数据集上都达到了更好的可读性指标。这项研究的发现提供了改进医学简化文本的可能的道路。
</details></li>
</ul>
<hr>
<h2 id="FocDepthFormer-Transformer-with-LSTM-for-Depth-Estimation-from-Focus"><a href="#FocDepthFormer-Transformer-with-LSTM-for-Depth-Estimation-from-Focus" class="headerlink" title="FocDepthFormer: Transformer with LSTM for Depth Estimation from Focus"></a>FocDepthFormer: Transformer with LSTM for Depth Estimation from Focus</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11178">http://arxiv.org/abs/2310.11178</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueyang Kang, Fengze Han, Abdur Fayjie, Dong Gong</li>
<li>for: 这个研究旨在推导从专注扫描库中的深度信息，解决了现有方法受到本地性的限制，并且可以处理任意长的扫描库。</li>
<li>methods: 我们提出了一个基于Transformer的网络，具有自我注意力和LSTM模组，以及一个CNN解oder。自我注意力允许学习更有用的特征，而LSTM模组可以将表现集成到扫描库中。</li>
<li>results: 我们的模型在多个扫描库评量 dataset 上表现出色，较前一代模型出色，并且可以从视觉对应的单眼RGB深度测量数据中获得更好的预备training。<details>
<summary>Abstract</summary>
Depth estimation from focal stacks is a fundamental computer vision problem that aims to infer depth from focus/defocus cues in the image stacks. Most existing methods tackle this problem by applying convolutional neural networks (CNNs) with 2D or 3D convolutions over a set of fixed stack images to learn features across images and stacks. Their performance is restricted due to the local properties of the CNNs, and they are constrained to process a fixed number of stacks consistent in train and inference, limiting the generalization to the arbitrary length of stacks. To handle the above limitations, we develop a novel Transformer-based network, FocDepthFormer, composed mainly of a Transformer with an LSTM module and a CNN decoder. The self-attention in Transformer enables learning more informative features via an implicit non-local cross reference. The LSTM module is learned to integrate the representations across the stack with arbitrary images. To directly capture the low-level features of various degrees of focus/defocus, we propose to use multi-scale convolutional kernels in an early-stage encoder. Benefiting from the design with LSTM, our FocDepthFormer can be pre-trained with abundant monocular RGB depth estimation data for visual pattern capturing, alleviating the demand for the hard-to-collect focal stack data. Extensive experiments on various focal stack benchmark datasets show that our model outperforms the state-of-the-art models on multiple metrics.
</details>
<details>
<summary>摘要</summary>
depth 估计从焦点栈中是计算机视觉的基本问题，它目的是从图像栈中提取焦点信息。大多数现有方法通过应用 convolutional neural networks (CNNs) 的 2D 或 3D 卷积来学习图像栈中的特征。它们的性能受到本地属性的限制，而且只能处理固定长度的栈图像，从而限制了泛化性。为了解决这些限制，我们开发了一种新的 Transformer 基于网络，即 FocDepthFormer，它主要由 Transformer 和 LSTM 模块以及 CNN 解码器组成。自我注意力在 Transformer 中允许学习更有用的特征，而 LSTM 模块可以将栈图像中的表示集成到不同的图像中。为了直接捕捉不同程度的焦点/杂论的低级特征，我们提议使用多尺度的卷积核在早期编码器中。由于 LSTM 的设计，我们的 FocDepthFormer 可以通过大量的monocular RGB 深度估计数据进行预训练，从而减轻硬件难以收集的焦点栈数据的需求。我们在多个焦点栈 benchmark 数据集上进行了广泛的实验，并证明我们的模型在多个指标上超过了当前状态的模型。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Extraction-and-Distillation-from-Large-Scale-Image-Text-Colonoscopy-Records-Leveraging-Large-Language-and-Vision-Models"><a href="#Knowledge-Extraction-and-Distillation-from-Large-Scale-Image-Text-Colonoscopy-Records-Leveraging-Large-Language-and-Vision-Models" class="headerlink" title="Knowledge Extraction and Distillation from Large-Scale Image-Text Colonoscopy Records Leveraging Large Language and Vision Models"></a>Knowledge Extraction and Distillation from Large-Scale Image-Text Colonoscopy Records Leveraging Large Language and Vision Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11173">http://arxiv.org/abs/2310.11173</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shuowang26/endoked">https://github.com/shuowang26/endoked</a></li>
<li>paper_authors: Shuo Wang, Yan Zhu, Xiaoyuan Luo, Zhiwei Yang, Yizhe Zhang, Peiyao Fu, Manning Wang, Zhijian Song, Quanlin Li, Pinghong Zhou, Yike Guo</li>
<li>for: 本研究旨在开发一种基于人工智能的检测和分类方法，用于检测肠穿刺图像中的肠癌。</li>
<li>methods: 本研究使用了大语言和视觉模型的最新进展，提出了一种数据挖掘 paradigma，称为EndoKED，用于深度知识提取和蒸馏。EndoKED自动将原始肠穿刺图像记录转换为带有像素级注释的图像集。</li>
<li>results: 在使用多地中的肠穿刺图像记录（约100万张图像）进行验证中，EndoKED显示出了更高的性能，可以更好地训练肠癌检测和分类模型。此外，EndoKED预训练的视觉底层模型可以在数据量少的情况下实现数据效果和泛化，达到专家水平的性能。<details>
<summary>Abstract</summary>
The development of artificial intelligence systems for colonoscopy analysis often necessitates expert-annotated image datasets. However, limitations in dataset size and diversity impede model performance and generalisation. Image-text colonoscopy records from routine clinical practice, comprising millions of images and text reports, serve as a valuable data source, though annotating them is labour-intensive. Here we leverage recent advancements in large language and vision models and propose EndoKED, a data mining paradigm for deep knowledge extraction and distillation. EndoKED automates the transformation of raw colonoscopy records into image datasets with pixel-level annotation. We validate EndoKED using multi-centre datasets of raw colonoscopy records (~1 million images), demonstrating its superior performance in training polyp detection and segmentation models. Furthermore, the EndoKED pre-trained vision backbone enables data-efficient and generalisable learning for optical biopsy, achieving expert-level performance in both retrospective and prospective validation.
</details>
<details>
<summary>摘要</summary>
开发人工智能系统用于护肠镜分析通常需要专家标注的图像集。然而，数据集的大小和多样性的限制会阻碍模型的性能和泛化。医疗实践中的图像报告记录，包括数百万张图像和文本报告，可以作为价值的数据源，但是标注它们是劳动密集的。我们利用最新的自然语言和Computer Vision技术，提出了EndoKED，一种深度知识提取和精炼数据挖掘模式。EndoKED自动将护肠镜记录转换为带有像素级别标注的图像集。我们使用多中心的 raw colonoscopy 记录（约100万张图像）进行验证，并证明EndoKED在训练肿瘤检测和分 segmentation 模型时表现出色。此外，EndoKED 预训练的视觉底层模型可以在数据效率和泛化上达到专家水平的性能，在逆向和前向验证中都达到专家水平。
</details></li>
</ul>
<hr>
<h2 id="MST-GAT-A-Multimodal-Spatial-Temporal-Graph-Attention-Network-for-Time-Series-Anomaly-Detection"><a href="#MST-GAT-A-Multimodal-Spatial-Temporal-Graph-Attention-Network-for-Time-Series-Anomaly-Detection" class="headerlink" title="MST-GAT: A Multimodal Spatial-Temporal Graph Attention Network for Time Series Anomaly Detection"></a>MST-GAT: A Multimodal Spatial-Temporal Graph Attention Network for Time Series Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11169">http://arxiv.org/abs/2310.11169</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chaoyue Ding, Shiliang Sun, Jing Zhao</li>
<li>for: 本文旨在提出一种基于多模态时间序列的异常检测方法，以维护工作设备的安全稳定性。</li>
<li>methods: 本方法使用多模态图注意力网络（M-GAT）和时间卷积网络来捕捉多模态时间序列中的空间时间关系。M-GAT使用多头注意力模块和两个关系注意力模块（内模态和间模态注意力）来模型模态关系。</li>
<li>results: 实验结果表明，MST-GAT在四个多模态标准数据集上比州元基elines表现出色，并且可以强化异常检测结果的可读性。<details>
<summary>Abstract</summary>
Multimodal time series (MTS) anomaly detection is crucial for maintaining the safety and stability of working devices (e.g., water treatment system and spacecraft), whose data are characterized by multivariate time series with diverse modalities. Although recent deep learning methods show great potential in anomaly detection, they do not explicitly capture spatial-temporal relationships between univariate time series of different modalities, resulting in more false negatives and false positives. In this paper, we propose a multimodal spatial-temporal graph attention network (MST-GAT) to tackle this problem. MST-GAT first employs a multimodal graph attention network (M-GAT) and a temporal convolution network to capture the spatial-temporal correlation in multimodal time series. Specifically, M-GAT uses a multi-head attention module and two relational attention modules (i.e., intra- and inter-modal attention) to model modal correlations explicitly. Furthermore, MST-GAT optimizes the reconstruction and prediction modules simultaneously. Experimental results on four multimodal benchmarks demonstrate that MST-GAT outperforms the state-of-the-art baselines. Further analysis indicates that MST-GAT strengthens the interpretability of detected anomalies by locating the most anomalous univariate time series.
</details>
<details>
<summary>摘要</summary>
多模态时间序列异常检测（MTS）是维护工作设备（如水处理系统和航天器）的关键，其数据由多个变量时间序列组成，具有多种模式。虽然最新的深度学习方法显示出了异常检测的潜力，但它们并不直接捕捉多modal时间序列之间的空间-时间关系，导致更多的假阳和假阴。在本文中，我们提出了一种多模态空间-时间Graph注意网络（MST-GAT）来解决这个问题。MST-GAT首先使用多modal Graph注意网络（M-GAT）和一个时间卷积网络来捕捉多modal时间序列之间的空间-时间相关性。具体来说，M-GAT使用多头注意模块和两种关系注意模块（即内模态注意和间模态注意）来模型modal相关性。此外，MST-GAT同时优化了重建和预测模块。实验结果表明，MST-GAT比州时的基elines表现出色，并且进一步分析表明，MST-GAT可以强化异常检测结果的解释性，即 locates最异常的单变量时间序列。
</details></li>
</ul>
<hr>
<h2 id="Accurate-prediction-of-international-trade-flows-Leveraging-knowledge-graphs-and-their-embeddings"><a href="#Accurate-prediction-of-international-trade-flows-Leveraging-knowledge-graphs-and-their-embeddings" class="headerlink" title="Accurate prediction of international trade flows: Leveraging knowledge graphs and their embeddings"></a>Accurate prediction of international trade flows: Leveraging knowledge graphs and their embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11161">http://arxiv.org/abs/2310.11161</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diego Rincon-Yanez, Chahinez Ounoughi, Bassem Sellami, Tarmo Kalvet, Marek Tiits, Sabrina Senatore, Sadok Ben Yahia</li>
<li>for: 本研究旨在使用知识图谱（KG）来模型国际贸易，帮助政策制定者、企业和经济学家预测国际贸易趋势。</li>
<li>methods: 本研究使用知识图谱嵌入（KGE）来预测国际贸易链接，并与传统机器学习方法相结合，如决策树和图 neural network。</li>
<li>results: 研究发现，通过使用KGE来预测国际贸易链接可以提高预测精度，同时也可以提供嵌入解释性的知识表示。此外，研究还发现embedding方法对其他智能算法产生了影响。<details>
<summary>Abstract</summary>
Knowledge representation (KR) is vital in designing symbolic notations to represent real-world facts and facilitate automated decision-making tasks. Knowledge graphs (KGs) have emerged so far as a popular form of KR, offering a contextual and human-like representation of knowledge. In international economics, KGs have proven valuable in capturing complex interactions between commodities, companies, and countries. By putting the gravity model, which is a common economic framework, into the process of building KGs, important factors that affect trade relationships can be taken into account, making it possible to predict international trade patterns. This paper proposes an approach that leverages Knowledge Graph embeddings for modeling international trade, focusing on link prediction using embeddings. Thus, valuable insights are offered to policymakers, businesses, and economists, enabling them to anticipate the effects of changes in the international trade system. Moreover, the integration of traditional machine learning methods with KG embeddings, such as decision trees and graph neural networks are also explored. The research findings demonstrate the potential for improving prediction accuracy and provide insights into embedding explainability in knowledge representation. The paper also presents a comprehensive analysis of the influence of embedding methods on other intelligent algorithms.
</details>
<details>
<summary>摘要</summary>
知识表示（KR）是设计符号notation的关键，以便实现自动化决策任务。知识图（KG）已经出现为知识表示的流行形式，提供了人类化和上下文rich的知识表示。在国际经济中，KGs已经证明了捕捉复杂的贸易关系的价值，例如商品、公司和国家之间的互动。通过将 gravitation model，这是经济框架的一种常见，integrated into the process of building KGs，可以考虑到影响贸易关系的重要因素，从而预测国际贸易模式。这篇论文提出了基于知识图嵌入的国际贸易模型，重点是预测链接使用嵌入。因此，可以为政策制定者、企业和经济学家提供有价值的信息，让他们预测贸易系统的变化的影响。此外，本文还探讨了将传统机器学习方法与KG嵌入结合的可能性，例如决策树和图 neural networks。研究发现， combining KG embeddings with traditional machine learning methods can improve prediction accuracy and provide insights into embedding explainability in knowledge representation.  Additionally, the paper presents a comprehensive analysis of the influence of embedding methods on other intelligent algorithms.
</details></li>
</ul>
<hr>
<h2 id="Causal-discovery-using-dynamically-requested-knowledge"><a href="#Causal-discovery-using-dynamically-requested-knowledge" class="headerlink" title="Causal discovery using dynamically requested knowledge"></a>Causal discovery using dynamically requested knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11154">http://arxiv.org/abs/2310.11154</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neville K Kitson, Anthony C Constantinou</li>
<li>for: 这 paper 旨在提高 causal Bayesian networks (CBNs) 的结构学习精度，并且研究一种基于机器学习的方法，使得结构学习算法本身可以动态地确定和请求人类知识。</li>
<li>methods: 这 paper 使用了 Tabu 结构学习算法，并将人类知识 integrate 到结构学习中。</li>
<li>results: 研究发现，这种方法可以提高结构学习精度，并且可以更好地使用人类知识。此外，这种方法还可以使结构学习过程更加透明和有效。<details>
<summary>Abstract</summary>
Causal Bayesian Networks (CBNs) are an important tool for reasoning under uncertainty in complex real-world systems. Determining the graphical structure of a CBN remains a key challenge and is undertaken either by eliciting it from humans, using machine learning to learn it from data, or using a combination of these two approaches. In the latter case, human knowledge is generally provided to the algorithm before it starts, but here we investigate a novel approach where the structure learning algorithm itself dynamically identifies and requests knowledge for relationships that the algorithm identifies as uncertain during structure learning. We integrate this approach into the Tabu structure learning algorithm and show that it offers considerable gains in structural accuracy, which are generally larger than those offered by existing approaches for integrating knowledge. We suggest that a variant which requests only arc orientation information may be particularly useful where the practitioner has little preexisting knowledge of the causal relationships. As well as offering improved accuracy, the approach can use human expertise more effectively and contributes to making the structure learning process more transparent.
</details>
<details>
<summary>摘要</summary>
causal Bayesian networks (CBNs) 是实际世界系统中不确定性理解的重要工具。确定CBN的图structural structure是一个关键挑战，通常通过从人类获得、使用机器学习从数据中学习或使用这两种方法进行。在后者情况下，人类知识通常会提供给算法之前，但我们在这里调查了一种新的方法，即结构学习算法本身在学习过程中动态确定和请求关系不确定的知识。我们将这种方法集成到Tabu结构学习算法中，并证明了它可以提供较大的结构准确性，通常比现有的知识集成方法更大。我们建议一种只请求路径方向信息的变体可能特别有用，当实践者具有少量先前知识的 causal 关系时。除了提高准确性外，这种方法可以更有效地使用人类专业知识，使结构学习过程更透明。
</details></li>
</ul>
<hr>
<h2 id="Uncovering-wall-shear-stress-dynamics-from-neural-network-enhanced-fluid-flow-measurements"><a href="#Uncovering-wall-shear-stress-dynamics-from-neural-network-enhanced-fluid-flow-measurements" class="headerlink" title="Uncovering wall-shear stress dynamics from neural-network enhanced fluid flow measurements"></a>Uncovering wall-shear stress dynamics from neural-network enhanced fluid flow measurements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11147">http://arxiv.org/abs/2310.11147</a></li>
<li>repo_url: None</li>
<li>paper_authors: Esther Lagemann, Steven L. Brunton, Christian Lagemann<br>for: 这篇论文是为了提供一种准确预测wall-shear stress的方法，以便在交通、公共设施、能源技术和医疗等领域实现可持续发展、资源保存和碳中和。methods: 本论文使用深度光流估计器，结合物理知识来 derive velocity和wall-shear stress场的空间和时间分辨率。results: 该方法可以准确预测wall-shear stress场，并且在实验数据上表明其物理正确性和有效性。<details>
<summary>Abstract</summary>
Friction drag from a turbulent fluid moving past or inside an object plays a crucial role in domains as diverse as transportation, public utility infrastructure, energy technology, and human health. As a direct measure of the shear-induced friction forces, an accurate prediction of the wall-shear stress can contribute to sustainability, conservation of resources, and carbon neutrality in civil aviation as well as enhanced medical treatment of vascular diseases and cancer. Despite such importance for our modern society, we still lack adequate experimental methods to capture the instantaneous wall-shear stress dynamics. In this contribution, we present a holistic approach that derives velocity and wall-shear stress fields with impressive spatial and temporal resolution from flow measurements using a deep optical flow estimator with physical knowledge. The validity and physical correctness of the derived flow quantities is demonstrated with synthetic and real-world experimental data covering a range of relevant fluid flows.
</details>
<details>
<summary>摘要</summary>
fluid 动力阻力从一个湍流中过或在一个物体表面或内部具有关键作用，在不同领域中发挥着重要作用，包括交通运输、公共基础设施、能源技术和人类健康。 wall-shear stress 是直接测量摩擦力的力量的直接测量方法，可以贡献到可持续发展、资源保存和碳中和性在民用航空领域，以及更好的医疗治疗血管疾病和癌症。  despite  Such importance in modern society, we still lack adequate experimental methods to capture the instantaneous wall-shear stress dynamics. In this contribution, we present a holistic approach that derives velocity and wall-shear stress fields with impressive spatial and temporal resolution from flow measurements using a deep optical flow estimator with physical knowledge. The validity and physical correctness of the derived flow quantities is demonstrated with synthetic and real-world experimental data covering a range of relevant fluid flows.
</details></li>
</ul>
<hr>
<h2 id="Long-form-Simultaneous-Speech-Translation-Thesis-Proposal"><a href="#Long-form-Simultaneous-Speech-Translation-Thesis-Proposal" class="headerlink" title="Long-form Simultaneous Speech Translation: Thesis Proposal"></a>Long-form Simultaneous Speech Translation: Thesis Proposal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11141">http://arxiv.org/abs/2310.11141</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Polák</li>
<li>for: 这份论文主要是为了解决同时传输语音翻译问题，特别是在长形设定下（无需先分割语音）。</li>
<li>methods: 这篇论文主要采用了深度学习方法，包括端到端同时语音翻译系统，以及对现有方法的修改和提高。</li>
<li>results: 这篇论文主要描述了现有的同时语音翻译方法的缺点和限制，以及一些可能的解决方案，但没有直接提供实验结果。<details>
<summary>Abstract</summary>
Simultaneous speech translation (SST) aims to provide real-time translation of spoken language, even before the speaker finishes their sentence. Traditionally, SST has been addressed primarily by cascaded systems that decompose the task into subtasks, including speech recognition, segmentation, and machine translation. However, the advent of deep learning has sparked significant interest in end-to-end (E2E) systems. Nevertheless, a major limitation of most approaches to E2E SST reported in the current literature is that they assume that the source speech is pre-segmented into sentences, which is a significant obstacle for practical, real-world applications. This thesis proposal addresses end-to-end simultaneous speech translation, particularly in the long-form setting, i.e., without pre-segmentation. We present a survey of the latest advancements in E2E SST, assess the primary obstacles in SST and its relevance to long-form scenarios, and suggest approaches to tackle these challenges.
</details>
<details>
<summary>摘要</summary>
同时语音翻译（SST）目标是在实时翻译说话人说话，就在说话人完成句子之前。传统上，SST通过顺序系统解决，分解任务为多个子任务，包括语音识别、分 segmentation 和机器翻译。然而，深度学习的出现引发了对 E2E 系统的重要兴趣。然而，大多数文献中的 E2E SST 方法假设源语音已经分 segmentation 成句子，这是实际应用中的重要障碍。本论文提案探讨了无 segmentation 的 E2E SST，特别是长形设置下。我们对最新的 E2E SST 进步进行了检查，评估了 SST 的主要障碍和长形场景的相关性，并建议了解决这些挑战的方法。
</details></li>
</ul>
<hr>
<h2 id="USDC-Unified-Static-and-Dynamic-Compression-for-Visual-Transformer"><a href="#USDC-Unified-Static-and-Dynamic-Compression-for-Visual-Transformer" class="headerlink" title="USDC: Unified Static and Dynamic Compression for Visual Transformer"></a>USDC: Unified Static and Dynamic Compression for Visual Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11117">http://arxiv.org/abs/2310.11117</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huan Yuan, Chao Liao, Jianchao Tan, Peng Yao, Jiyuan Jia, Bin Chen, Chengru Song, Di Zhang</li>
<li>for: 提高Visual Transformer模型的部署效率和可扩展性，解决模型压缩和执行速度之间的矛盾。</li>
<li>methods: 提出了一种结合静态压缩和动态压缩技术的输入适应压缩模型，以及一种子组augmentation技术来解决具有不同批处理大小的训练和推理阶段之间的性能差异。</li>
<li>results: 对多种基eline Visual Transformer模型进行了广泛的实验，证明了我们的方法可以更好地均衡压缩率和模型性能，并且在不同批处理大小下提供了更高的性能稳定性。<details>
<summary>Abstract</summary>
Visual Transformers have achieved great success in almost all vision tasks, such as classification, detection, and so on. However, the model complexity and the inference speed of the visual transformers hinder their deployments in industrial products. Various model compression techniques focus on directly compressing the visual transformers into a smaller one while maintaining the model performance, however, the performance drops dramatically when the compression ratio is large. Furthermore, several dynamic network techniques have also been applied to dynamically compress the visual transformers to obtain input-adaptive efficient sub-structures during the inference stage, which can achieve a better trade-off between the compression ratio and the model performance. The upper bound of memory of dynamic models is not reduced in the practical deployment since the whole original visual transformer model and the additional control gating modules should be loaded onto devices together for inference. To alleviate two disadvantages of two categories of methods, we propose to unify the static compression and dynamic compression techniques jointly to obtain an input-adaptive compressed model, which can further better balance the total compression ratios and the model performances. Moreover, in practical deployment, the batch sizes of the training and inference stage are usually different, which will cause the model inference performance to be worse than the model training performance, which is not touched by all previous dynamic network papers. We propose a sub-group gates augmentation technique to solve this performance drop problem. Extensive experiments demonstrate the superiority of our method on various baseline visual transformers such as DeiT, T2T-ViT, and so on.
</details>
<details>
<summary>摘要</summary>
Visual Transformers 已经在各种视觉任务上获得了很大的成功，如分类、检测等。然而，视觉转换器的模型复杂度和推理速度使得它们在工业产品中的部署受到限制。各种模型压缩技术都是直接压缩视觉转换器到更小的模型，以保持模型性能，但当压缩比率较大时，模型性能会下降很快。此外，一些动态网络技术也已经应用于在推理阶段动态压缩视觉转换器，以获得输入适应型的有效子结构，可以更好地平衡压缩率和模型性能。然而，在实际部署中，动态模型的内存顶部不会减少，因为整个原始视觉转换器模型和额外的控制闭合模块都需要在设备上加载。为了解决这两种方法的缺点，我们提议将静态压缩和动态压缩技术联合使用，以获得输入适应型的压缩模型，可以更好地平衡总压缩率和模型性能。此外，在训练和推理阶段的批处理大小不同，通常会导致模型的推理性能下降，这个问题未经所有动态网络文章讨论。我们提议使用 subgroup gates 技术来解决这个问题。我们的方法在多种基eline visual transformers 上进行了广泛的实验，如 DeiT、T2T-ViT 等。
</details></li>
</ul>
<hr>
<h2 id="H2O-Open-Ecosystem-for-State-of-the-art-Large-Language-Models"><a href="#H2O-Open-Ecosystem-for-State-of-the-art-Large-Language-Models" class="headerlink" title="H2O Open Ecosystem for State-of-the-art Large Language Models"></a>H2O Open Ecosystem for State-of-the-art Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13012">http://arxiv.org/abs/2310.13012</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/h2oai/h2o-llmstudio">https://github.com/h2oai/h2o-llmstudio</a></li>
<li>paper_authors: Arno Candel, Jon McKinney, Philipp Singer, Pascal Pfeiffer, Maximilian Jeblick, Chun Ming Lee, Marcos V. Conde</li>
<li>for: The paper is written to introduce an open-source ecosystem for developing and testing large language models (LLMs) to address the risks posed by closed-source approaches and to make AI development more accessible, efficient, and trustworthy.</li>
<li>methods: The paper presents a complete open-source ecosystem for LLMs, including a family of fine-tuned LLMs of diverse sizes and a framework and no-code GUI called H2O LLM Studio for efficient fine-tuning, evaluation, and deployment of LLMs using state-of-the-art techniques.</li>
<li>results: The paper introduces h2oGPT, a family of fine-tuned LLMs of diverse sizes, and demonstrates the effectiveness of the H2O LLM Studio for efficient fine-tuning, evaluation, and deployment of LLMs. The demo is available at <a target="_blank" rel="noopener" href="https://gpt.h2o.ai/">https://gpt.h2o.ai/</a>.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) represent a revolution in AI. However, they also pose many significant risks, such as the presence of biased, private, copyrighted or harmful text. For this reason we need open, transparent and safe solutions. We introduce a complete open-source ecosystem for developing and testing LLMs. The goal of this project is to boost open alternatives to closed-source approaches. We release h2oGPT, a family of fine-tuned LLMs of diverse sizes. We also introduce H2O LLM Studio, a framework and no-code GUI designed for efficient fine-tuning, evaluation, and deployment of LLMs using the most recent state-of-the-art techniques. Our code and models are fully open-source. We believe this work helps to boost AI development and make it more accessible, efficient and trustworthy. The demo is available at: https://gpt.h2o.ai/
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）表示了人工智能领域的革命，但也存在许多重要的风险，如偏见、私有、版权和危险的文本存在。为了解决这些问题，我们需要开放、透明和安全的解决方案。我们介绍了一个完整的开源生态系统，用于开发和测试LLMs。该项目的目标是推动开放的代替方案，以opposeclosed-source方法。我们发布了h2oGPT家族，包括多种大小的精度调整LLMs。我们还介绍了H2O LLM Studio框架和无代码GUI，用于高效地调整、评估和部署LLMs，使用最新的状态艺术技术。我们的代码和模型都是完全开源的。我们认为这项工作将帮助提高人工智能的发展，使其更加可 accessible、高效和可靠。示例可以在以下链接中找到：https://gpt.h2o.ai/
</details></li>
</ul>
<hr>
<h2 id="ASP-Automatic-Selection-of-Proxy-dataset-for-efficient-AutoML"><a href="#ASP-Automatic-Selection-of-Proxy-dataset-for-efficient-AutoML" class="headerlink" title="ASP: Automatic Selection of Proxy dataset for efficient AutoML"></a>ASP: Automatic Selection of Proxy dataset for efficient AutoML</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11478">http://arxiv.org/abs/2310.11478</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Yao, Chao Liao, Jiyuan Jia, Jianchao Tan, Bin Chen, Chengru Song, Di Zhang</li>
<li>for: 这篇论文旨在提出一个自动选择代理数据框架（ASP），以便在每个epoch中 dynamically选择有用的代理数据subset，从而节省训练数据大小和AutoML处理时间。</li>
<li>methods: 这篇论文使用了自动选择代理数据框架（ASP），可以在不同的选择比率下选择有用的代理数据subset，以节省训练数据大小和AutoML处理时间。</li>
<li>results: 实验结果显示，这篇论文使用的ASP方法可以在不同的选择比率下比其他数据选择方法取得更好的结果，并且可以节省2x-20x的AutoML处理时间。<details>
<summary>Abstract</summary>
Deep neural networks have gained great success due to the increasing amounts of data, and diverse effective neural network designs. However, it also brings a heavy computing burden as the amount of training data is proportional to the training time. In addition, a well-behaved model requires repeated trials of different structure designs and hyper-parameters, which may take a large amount of time even with state-of-the-art (SOTA) hyper-parameter optimization (HPO) algorithms and neural architecture search (NAS) algorithms. In this paper, we propose an Automatic Selection of Proxy dataset framework (ASP) aimed to dynamically find the informative proxy subsets of training data at each epoch, reducing the training data size as well as saving the AutoML processing time. We verify the effectiveness and generalization of ASP on CIFAR10, CIFAR100, ImageNet16-120, and ImageNet-1k, across various public model benchmarks. The experiment results show that ASP can obtain better results than other data selection methods at all selection ratios. ASP can also enable much more efficient AutoML processing with a speedup of 2x-20x while obtaining better architectures and better hyper-parameters compared to utilizing the entire dataset.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="HGCVAE-Integrating-Generative-and-Contrastive-Learning-for-Heterogeneous-Graph-Learning"><a href="#HGCVAE-Integrating-Generative-and-Contrastive-Learning-for-Heterogeneous-Graph-Learning" class="headerlink" title="HGCVAE: Integrating Generative and Contrastive Learning for Heterogeneous Graph Learning"></a>HGCVAE: Integrating Generative and Contrastive Learning for Heterogeneous Graph Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11102">http://arxiv.org/abs/2310.11102</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yulan Hu, Zhirui Yang, Sheng Ouyang, Junchen Wan, Fuzheng Zhang, Zhongyuan Wang, Yong Liu</li>
<li>for: 本研究旨在探讨生成自监督学习（SSL）在多类 Graph Learning（HGL）中的应用。</li>
<li>methods: 本文提出了一种新的对比学习变量 Graph Autoencoder（HGCVAE），该模型通过结合对比学习和生成 SSL，解决了传统对比学习方法中复杂的多类性捕捉问题。</li>
<li>results: 对比于多种州rror-of-the-art基elines，HGCVAE达到了Remarkable的结果，证明了其superiority。<details>
<summary>Abstract</summary>
Generative self-supervised learning (SSL) has exhibited significant potential and garnered increasing interest in graph learning. In this study, we aim to explore the problem of generative SSL in the context of heterogeneous graph learning (HGL). The previous SSL approaches for heterogeneous graphs have primarily relied on contrastive learning, necessitating the design of complex views to capture heterogeneity. However, existing generative SSL methods have not fully leveraged the capabilities of generative models to address the challenges of HGL. In this paper, we present HGCVAE, a novel contrastive variational graph auto-encoder that liberates HGL from the burden of intricate heterogeneity capturing. Instead of focusing on complicated heterogeneity, HGCVAE harnesses the full potential of generative SSL. HGCVAE innovatively consolidates contrastive learning with generative SSL, introducing several key innovations. Firstly, we employ a progressive mechanism to generate high-quality hard negative samples for contrastive learning, utilizing the power of variational inference. Additionally, we present a dynamic mask strategy to ensure effective and stable learning. Moreover, we propose an enhanced scaled cosine error as the criterion for better attribute reconstruction. As an initial step in combining generative and contrastive SSL, HGCVAE achieves remarkable results compared to various state-of-the-art baselines, confirming its superiority.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本翻译为简化中文。<</SYS>>生成自我超级学习（SSL）在图学习中展示了重要性和吸引了越来越多的关注。在这项研究中，我们想要探讨生成SSL在非同质graph学习（HGL）中的问题。前一些SSL方法 для非同质图把主要依靠于对比学习，因此需要设计复杂的视图来捕捉非同质性。然而，现有的生成SSL方法没有充分利用生成模型来解决HGL中的挑战。在本文中，我们提出了HGCVAE，一种新的对比变量图自动编码器。而不是关注复杂的非同质性，HGCVAE fully harnesses the full potential of generative SSL。HGCVAE innovatively consolidates contrastive learning with generative SSL, introducing several key innovations. Firstly, we employ a progressive mechanism to generate high-quality hard negative samples for contrastive learning, utilizing the power of variational inference. Additionally, we present a dynamic mask strategy to ensure effective and stable learning. Moreover, we propose an enhanced scaled cosine error as the criterion for better attribute reconstruction. As an initial step in combining generative and contrastive SSL, HGCVAE achieves remarkable results compared to various state-of-the-art baselines, confirming its superiority.
</details></li>
</ul>
<hr>
<h2 id="MeKB-Rec-Personal-Knowledge-Graph-Learning-for-Cross-Domain-Recommendation"><a href="#MeKB-Rec-Personal-Knowledge-Graph-Learning-for-Cross-Domain-Recommendation" class="headerlink" title="MeKB-Rec: Personal Knowledge Graph Learning for Cross-Domain Recommendation"></a>MeKB-Rec: Personal Knowledge Graph Learning for Cross-Domain Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11088">http://arxiv.org/abs/2310.11088</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Su, Yao Zhou, Zifei Shan, Qian Chen<br>for: 强化推荐 для新用户 (addressing the cold-start problem in modern recommender systems)methods: 使用Personal Knowledge Graph (PKG) 和 Pretrained Language Models (PLMs) 来建立用户会兴趣的域别不受限制的 semantic representationresults: 在多个公共 CDR 数据集上实验，证明了 MeKB-Rec 的新定义比前一代方法更具弹性，实现了 HR@10 和 NDCG@10  метри增加24%–91%，zero-shot 用户在目标领域的行为无需准确数据可以获得 significiant 提升（105%）。在 WeiXin 推荐场景中部署 MeKB-Rec，获得了重要的线上数据提升。MeKB-Rec 现在在实际产品中服务百亿用户。<details>
<summary>Abstract</summary>
It is a long-standing challenge in modern recommender systems to effectively make recommendations for new users, namely the cold-start problem. Cross-Domain Recommendation (CDR) has been proposed to address this challenge, but current ways to represent users' interests across systems are still severely limited. We introduce Personal Knowledge Graph (PKG) as a domain-invariant interest representation, and propose a novel CDR paradigm named MeKB-Rec. We first link users and entities in a knowledge base to construct a PKG of users' interests, named MeKB. Then we learn a semantic representation of MeKB for the cross-domain recommendation. To efficiently utilize limited training data in CDR, MeKB-Rec employs Pretrained Language Models to inject world knowledge into understanding users' interests. Beyond most existing systems, our approach builds a semantic mapping across domains which breaks the requirement for in-domain user behaviors, enabling zero-shot recommendations for new users in a low-resource domain. We experiment MeKB-Rec on well-established public CDR datasets, and demonstrate that the new formulation % is more powerful than previous approaches, achieves a new state-of-the-art that significantly improves HR@10 and NDCG@10 metrics over best previous approaches by 24\%--91\%, with a 105\% improvement for HR@10 of zero-shot users with no behavior in the target domain. We deploy MeKB-Rec in WeiXin recommendation scenarios and achieve significant gains in core online metrics. MeKB-Rec is now serving hundreds of millions of users in real-world products.
</details>
<details>
<summary>摘要</summary>
现代推荐系统中长期面临的挑战是如何有效地为新用户提供推荐，即冷启用户问题。跨领域推荐（CDR）已经被提议以解决这个问题，但现有的用户兴趣表示方式仍然受到严重的限制。我们引入个人知识图（PKG）作为领域不变的兴趣表示方式，并提出了一种基于PKG的CDR paradigma，称之为MeKB-Rec。我们首先将用户和实体在知识库中连接，以构建用户兴趣的PKG，称之为MeKB。然后，我们学习MeKB的semantic表示，以便在跨领域推荐中使用。为了有效利用CDR中的有限训练数据，MeKB-Rec使用预训练语言模型，以尝试在理解用户兴趣时涉及世界知识。与现有系统不同，我们的方法建立了领域之间的semantic映射，使得不需要在目标领域中有具体的用户行为，以实现零容量推荐。我们在well-established的公共CDR数据集上实验MeKB-Rec，并证明了新的表示方式比前一代方法更有力，在HR@10和NDCG@10指标上提高了24%--91%，与最佳前一代方法相比提高105%。我们在微信推荐场景中部署MeKB-Rec，得到了显著的提升。MeKB-Rec现在为百亿用户服务。
</details></li>
</ul>
<hr>
<h2 id="Feature-Pyramid-biLSTM-Using-Smartphone-Sensors-for-Transportation-Mode-Detection"><a href="#Feature-Pyramid-biLSTM-Using-Smartphone-Sensors-for-Transportation-Mode-Detection" class="headerlink" title="Feature Pyramid biLSTM: Using Smartphone Sensors for Transportation Mode Detection"></a>Feature Pyramid biLSTM: Using Smartphone Sensors for Transportation Mode Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11087">http://arxiv.org/abs/2310.11087</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinrui Tang, Hao Cheng</li>
<li>for: 本研究的目的是提出一种新的终端方法，以优化减少的感知数据来实现日常旅行中准确的交通方式检测。</li>
<li>methods: 该方法称为Feature Pyramid biLSTM（FPbiLSTM），它利用Feature Pyramid Network（FPN），将浅层充满精度的特征与深层特征强度相互补做，以捕捉不同交通模式的时间运动模式。</li>
<li>results: FPbiLSTM使用仅三个感知器（加速度计、陀螺仪和磁场计）的数据，在2018年的Sussex-Huawei Locomotion（SHL）挑战数据集上达到了95.1%的准确率和94.7%的F1分数，在八种不同的交通模式中进行了可识别。<details>
<summary>Abstract</summary>
The widespread utilization of smartphones has provided extensive availability to Inertial Measurement Units, providing a wide range of sensory data that can be advantageous for the detection of transportation modes. The objective of this study is to propose a novel end-to-end approach to effectively explore a reduced amount of sensory data collected from a smartphone to achieve accurate mode detection in common daily traveling activities. Our approach, called Feature Pyramid biLSTM (FPbiLSTM), is characterized by its ability to reduce the number of sensors required and processing demands, resulting in a more efficient modeling process without sacrificing the quality of the outcomes than the other current models. FPbiLSTM extends an existing CNN biLSTM model with the Feature Pyramid Network, leveraging the advantages of both shallow layer richness and deeper layer feature resilience for capturing temporal moving patterns in various transportation modes. It exhibits an excellent performance by employing the data collected from only three out of seven sensors, i.e. accelerometers, gyroscopes, and magnetometers, in the 2018 Sussex-Huawei Locomotion (SHL) challenge dataset, attaining a noteworthy accuracy of 95.1% and an F1-score of 94.7% in detecting eight different transportation modes.
</details>
<details>
<summary>摘要</summary>
通过智能手机的广泛使用，提供了大量的感知数据，这些数据可以为交通方式检测提供有利条件。本研究的目标是提出一种新的端到端方法，使用少量的感知数据来准确地检测日常旅行中的交通方式。我们提出的方法，即特征峰网络bilstm（FPbiLSTM），通过减少感知器数量和处理需求，实现了更高效的模型化过程，而不 sacrificing 结果质量。FPbiLSTM extend了现有的CNN bilstm模型，利用浅层layer的richness和深层layer的特征鲜柔性，捕捉不同交通方式的时间运动模式。它在使用2018年的 sussex-Huawei Locomotion（SHL）挑战数据集中，达到了95.1%的准确率和94.7%的F1分数，在检测八种不同的交通方式中表现出色。
</details></li>
</ul>
<hr>
<h2 id="In-Context-Few-Shot-Relation-Extraction-via-Pre-Trained-Language-Models"><a href="#In-Context-Few-Shot-Relation-Extraction-via-Pre-Trained-Language-Models" class="headerlink" title="In-Context Few-Shot Relation Extraction via Pre-Trained Language Models"></a>In-Context Few-Shot Relation Extraction via Pre-Trained Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11085">http://arxiv.org/abs/2310.11085</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/oezyurty/replm">https://github.com/oezyurty/replm</a></li>
<li>paper_authors: Yilmazcan Ozyurt, Stefan Feuerriegel, Ce Zhang</li>
<li>for: 实现文本文档中的人类知识结构，扩展现有的语言模型技术。</li>
<li>methods: 提出一个基于预训语言模型的内容几个扩展框架，不需要名称实体输入或文档人类标注。</li>
<li>results: 在 DocRED  dataset 上进行评估，表现与原始标签相似或更好，并可以轻松地更新 для新的关系集。<details>
<summary>Abstract</summary>
Relation extraction aims at inferring structured human knowledge from textual documents. State-of-the-art methods based on language models commonly have two limitations: (1) they require named entities to be either given as input or infer them, which introduces additional noise, and (2) they require human annotations of documents. As a remedy, we present a novel framework for in-context few-shot relation extraction via pre-trained language models. To the best of our knowledge, we are the first to reformulate the relation extraction task as a tailored in-context few-shot learning paradigm. Thereby, we achieve crucial benefits in that we eliminate the need for both named entity recognition and human annotation of documents. Unlike existing methods based on fine-tuning, our framework is flexible in that it can be easily updated for a new set of relations without re-training. We evaluate our framework using DocRED, the largest publicly available dataset for document-level relation extraction, and demonstrate that our framework achieves state-of-the-art performance. Finally, our framework allows us to identify missing annotations, and we thus show that our framework actually performs much better than the original labels from the development set of DocRED.
</details>
<details>
<summary>摘要</summary>
关系提取目标是从文本文档中提取结构化的人类知识。现有的方法基于自然语言模型很多时候受到两种限制：（1）它们需要输入名称实体，或者自动检测名称实体，这会增加额外的噪音，（2）它们需要文档的人类标注。为了解决这些问题，我们提出了一种新的框架，即在文本上进行受限的少量trainingrelation extractionvia预训练的语言模型。根据我们所知，我们是第一个将关系提取任务重新定义为特定的在文本上进行受限的少量学习 paradigm。这种方法比现有的方法更加灵活，因为它可以轻松地更新 для新的关系集 ohne需要重新训练。我们使用DocRED dataset，这是公共可用的最大文档关系提取 dataset，来评估我们的框架。我们的结果显示，我们的框架可以达到领先的性能。此外，我们的框架可以识别缺失的注释，因此我们实际上可以证明我们的框架实际上比原始的标签更好。
</details></li>
</ul>
<hr>
<h2 id="Multi-omics-Sampling-based-Graph-Transformer-for-Synthetic-Lethality-Prediction"><a href="#Multi-omics-Sampling-based-Graph-Transformer-for-Synthetic-Lethality-Prediction" class="headerlink" title="Multi-omics Sampling-based Graph Transformer for Synthetic Lethality Prediction"></a>Multi-omics Sampling-based Graph Transformer for Synthetic Lethality Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11082">http://arxiv.org/abs/2310.11082</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xusheng Zhao, Hao Liu, Qiong Dai, Hao Peng, Xu Bai, Huailiang Peng</li>
<li>for: 这项研究的目的是提出一种新的多型数据预测生物学上的致死性静止（Synthetic Lethality，SL）。</li>
<li>methods: 该研究使用了一种新的多型数据预测SL方法，即使用抽样-基于图 transformer（MSGT-SL）。该方法首先使用了一种 shallow 多视图 GNN 来获取 SL 和多型数据中的本地结构特征。然后，通过输入基因特征来捕捉长距离依赖关系。最后，通过并行随机游走检索多型数据中的基因来模仿结构化地 incorporate 多型数据。</li>
<li>results: 研究发现，使用 MSGT-SL 方法可以在真实世界 SL 任务中获得较高的 empirical 效果，证明了该方法在 SL 预测中的效用性。<details>
<summary>Abstract</summary>
Synthetic lethality (SL) prediction is used to identify if the co-mutation of two genes results in cell death. The prevalent strategy is to abstract SL prediction as an edge classification task on gene nodes within SL data and achieve it through graph neural networks (GNNs). However, GNNs suffer from limitations in their message passing mechanisms, including over-smoothing and over-squashing issues. Moreover, harnessing the information of non-SL gene relationships within large-scale multi-omics data to facilitate SL prediction poses a non-trivial challenge. To tackle these issues, we propose a new multi-omics sampling-based graph transformer for SL prediction (MSGT-SL). Concretely, we introduce a shallow multi-view GNN to acquire local structural patterns from both SL and multi-omics data. Further, we input gene features that encode multi-view information into the standard self-attention to capture long-range dependencies. Notably, starting with batch genes from SL data, we adopt parallel random walk sampling across multiple omics gene graphs encompassing them. Such sampling effectively and modestly incorporates genes from omics in a structure-aware manner before using self-attention. We showcase the effectiveness of MSGT-SL on real-world SL tasks, demonstrating the empirical benefits gained from the graph transformer and multi-omics data.
</details>
<details>
<summary>摘要</summary>
<<SYS>> traduced text into Simplified Chinese.<</SYS>>人工致死性（SL）预测是用来判断两个基因的共同突变是否导致细胞死亡。现有的策略是将SL预测视为基因节点之间的边分类任务，并使用图神经网络（GNN）来实现。然而，GNN受到消息传递机制的局限性，包括过滤和压缩问题。另外，在大规模多Omics数据中利用非SL基因关系来促进SL预测是一个非常困难的问题。为了解决这些问题，我们提出了一种新的多Omics采样基于图变换器 для SL预测（MSGT-SL）。具体来说，我们引入了一个浅层多视图GNN，以获取SL数据和多Omics数据中的本地结构模式。然后，我们将基因特征，其中包含多视图信息，输入到标准自注意力中，以捕捉长距离依赖关系。另外，我们从SL数据中开始，采样多Omics基因图中包含的批处理基因，以模estamente和多Omics基因图中的结构相关的方式进行采样。这种采样方式可以有效地和modestly地在多Omics基因图中包含基因，并在使用自注意力之前进行结构意识。我们在实际SL任务上展示了MSGT-SL的效果，并证明了图变换器和多Omics数据的实际效果。
</details></li>
</ul>
<hr>
<h2 id="Learning-from-Red-Teaming-Gender-Bias-Provocation-and-Mitigation-in-Large-Language-Models"><a href="#Learning-from-Red-Teaming-Gender-Bias-Provocation-and-Mitigation-in-Large-Language-Models" class="headerlink" title="Learning from Red Teaming: Gender Bias Provocation and Mitigation in Large Language Models"></a>Learning from Red Teaming: Gender Bias Provocation and Mitigation in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11079">http://arxiv.org/abs/2310.11079</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hsuan Su, Cheng-Chu Cheng, Hua Farn, Shachi H Kumar, Saurav Sahay, Shang-Tse Chen, Hung-yi Lee<br>for: 这研究旨在检测大语言模型（LLM）中的可能性偏见，并提出了一种自动生成测试用例的方法来检测这些偏见。methods: 这种方法使用自动生成的测试用例来检测LLMs中的偏见，并对检测到的偏见进行纠正。results: 实验结果表明，使用该方法可以使LLMs生成更公正的回答。<details>
<summary>Abstract</summary>
Recently, researchers have made considerable improvements in dialogue systems with the progress of large language models (LLMs) such as ChatGPT and GPT-4. These LLM-based chatbots encode the potential biases while retaining disparities that can harm humans during interactions. The traditional biases investigation methods often rely on human-written test cases. However, these test cases are usually expensive and limited. In this work, we propose a first-of-its-kind method that automatically generates test cases to detect LLMs' potential gender bias. We apply our method to three well-known LLMs and find that the generated test cases effectively identify the presence of biases. To address the biases identified, we propose a mitigation strategy that uses the generated test cases as demonstrations for in-context learning to circumvent the need for parameter fine-tuning. The experimental results show that LLMs generate fairer responses with the proposed approach.
</details>
<details>
<summary>摘要</summary>
In this work, we propose a novel method that automatically generates test cases to detect LLMs' potential gender bias. We apply our method to three well-known LLMs and find that the generated test cases effectively identify the presence of biases. To address the biases identified, we propose a mitigation strategy that uses the generated test cases as demonstrations for in-context learning to circumvent the need for parameter fine-tuning. The experimental results show that LLMs generate fairer responses with the proposed approach.
</details></li>
</ul>
<hr>
<h2 id="Sim-to-Real-Transfer-of-Adaptive-Control-Parameters-for-AUV-Stabilization-under-Current-Disturbance"><a href="#Sim-to-Real-Transfer-of-Adaptive-Control-Parameters-for-AUV-Stabilization-under-Current-Disturbance" class="headerlink" title="Sim-to-Real Transfer of Adaptive Control Parameters for AUV Stabilization under Current Disturbance"></a>Sim-to-Real Transfer of Adaptive Control Parameters for AUV Stabilization under Current Disturbance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11075">http://arxiv.org/abs/2310.11075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Chaffre, Jonathan Wheare, Andrew Lammas, Paulo Santos, Gilles Le Chenadec, Karl Sammut, Benoit Clement</li>
<li>for: 该论文旨在开发一种基于深度学习的自适应控制方法，以帮助自主潜水机器人（AUV）在海洋环境中减少过程变化的影响，并最小化人工干预。</li>
<li>methods: 该方法结合了最大 entropy深度学习框架和经典的模型基于控制架构，以 trains general-purpose神经网络策略。为了应对实际环境中的分布偏移和高样本复杂性问题，该方法还提出了一种Sim-to-Real传输策略，包括生物体引zed经验回放机制、加强版域随机化技术和实际平台上的评估协议。</li>
<li>results: 实验结果显示，该方法可以从不优的模拟模型上学习出高效策略，并在实际 Vehicle上实现了控制性能3倍高于其模型基于非适应控制器的对照试验。<details>
<summary>Abstract</summary>
Learning-based adaptive control methods hold the premise of enabling autonomous agents to reduce the effect of process variations with minimal human intervention. However, its application to autonomous underwater vehicles (AUVs) has so far been restricted due to 1) unknown dynamics under the form of sea current disturbance that we can not model properly nor measure due to limited sensor capability and 2) the nonlinearity of AUVs tasks where the controller response at some operating points must be overly conservative in order to satisfy the specification at other operating points. Deep Reinforcement Learning (DRL) can alleviates these limitations by training general-purpose neural network policies, but applications of DRL algorithms to AUVs have been restricted to simulated environments, due to their inherent high sample complexity and distribution shift problem. This paper presents a novel approach, merging the Maximum Entropy Deep Reinforcement Learning framework with a classic model-based control architecture, to formulate an adaptive controller. Within this framework, we introduce a Sim-to-Real transfer strategy comprising the following components: a bio-inspired experience replay mechanism, an enhanced domain randomisation technique, and an evaluation protocol executed on a physical platform. Our experimental assessments demonstrate that this method effectively learns proficient policies from suboptimal simulated models of the AUV, resulting in control performance 3 times higher when transferred to a real-world vehicle, compared to its model-based nonadaptive but optimal counterpart.
</details>
<details>
<summary>摘要</summary>
学习基于控制方法可以让自主Agent减少过程变化的影响，但是在自主水下潜水器（AUV）上应用尚未得到广泛使用，主要原因是1）不能正确地模型海流干扰的不确定动力学特性，因为感知器的限制不能准确地测量，2）AUV任务的非线性性，控制器在某些操作点上必须采取保守的响应，以满足其他操作点的规范。深度优化学习（DRL）可以解决这些限制，通过训练通用神经网络策略来减少模型不确定性和分布偏移问题。这篇论文提出了一种新的方法，将最大 entropy深度优化学习框架与经典模型基于控制架构结合，形成一个适应控制器。在这个框架中，我们提出了一种Sim-to-Real转移策略，包括以下三个组成部分：生物发现经验回放机制、改进的领域随机化技术和在物理平台上执行的评估协议。我们的实验评估表明，这种方法可以从优化的模拟模型中学习出高效策略，在真实世界潜水器上实现控制性能3倍高于其模型基于非适应控制器的优化对照。
</details></li>
</ul>
<hr>
<h2 id="Robust-MBFD-A-Robust-Deep-Learning-System-for-Motor-Bearing-Faults-Detection-Using-Multiple-Deep-Learning-Training-Strategies-and-A-Novel-Double-Loss-Function"><a href="#Robust-MBFD-A-Robust-Deep-Learning-System-for-Motor-Bearing-Faults-Detection-Using-Multiple-Deep-Learning-Training-Strategies-and-A-Novel-Double-Loss-Function" class="headerlink" title="Robust-MBFD: A Robust Deep Learning System for Motor Bearing Faults Detection Using Multiple Deep Learning Training Strategies and A Novel Double Loss Function"></a>Robust-MBFD: A Robust Deep Learning System for Motor Bearing Faults Detection Using Multiple Deep Learning Training Strategies and A Novel Double Loss Function</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11477">http://arxiv.org/abs/2310.11477</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khoa Tran, Lam Pham, Hai-Canh Vu</li>
<li>for: 这个论文的目的是对电动机承受器 fault detection (MBFD) 进行全面分析，即基于电动机承受器的振荡来识别faults。</li>
<li>methods: 本论文提出了多种机器学习基于系统 для MBFD 任务，并评估了这些系统的性能。此外，本论文还提出了三种深度学习基于系统，每一种都采用了不同的训练策略：supervised learning、semi-supervised learning和Unsupervised learning。</li>
<li>results: 对多个 benchmark 数据集进行了广泛的实验，包括美国机械失效预防学会 (MFPT)、Case Western Reserve University Bearing Center (CWRU) 和Paderborn University (PU) 的condition monitoring of bearing damage in electromechanical drive systems。实验结果表明，深度学习基于系统比机器学习基于系统更有效果于 MBFD 任务。此外，我们还实现了一种可靠和通用的深度学习基于系统，并在多个 benchmark 数据集上实现了良好的性能，demonstrating its potential for real-life MBFD applications。<details>
<summary>Abstract</summary>
This paper presents a comprehensive analysis of motor bearing fault detection (MBFD), which involves the task of identifying faults in a motor bearing based on its vibration. To this end, we first propose and evaluate various machine learning based systems for the MBFD task. Furthermore, we propose three deep learning based systems for the MBFD task, each of which explores one of the following training strategies: supervised learning, semi-supervised learning, and unsupervised learning. The proposed machine learning based systems and deep learning based systems are evaluated, compared, and then they are used to identify the best model for the MBFD task. We conducted extensive experiments on various benchmark datasets of motor bearing faults, including those from the American Society for Mechanical Failure Prevention Technology (MFPT), Case Western Reserve University Bearing Center (CWRU), and the Condition Monitoring of Bearing Damage in Electromechanical Drive Systems from Paderborn University (PU). The experimental results on different datasets highlight two main contributions of this study. First, we prove that deep learning based systems are more effective than machine learning based systems for the MBFD task. Second, we achieve a robust and general deep learning based system with a novel loss function for the MBFD task on several benchmark datasets, demonstrating its potential for real-life MBFD applications.
</details>
<details>
<summary>摘要</summary>
The authors propose three deep learning-based systems for the MBFD task, each of which explores a different training strategy: supervised learning, semi-supervised learning, and unsupervised learning. The proposed machine learning-based systems and deep learning-based systems are evaluated and compared, and the best model for the MBFD task is identified.The authors conducted extensive experiments on various benchmark datasets of motor bearing faults, including those from the American Society for Mechanical Failure Prevention Technology (MFPT), Case Western Reserve University Bearing Center (CWRU), and the Condition Monitoring of Bearing Damage in Electromechanical Drive Systems from Paderborn University (PU). The experimental results show that deep learning-based systems are more effective than machine learning-based systems for the MBFD task. Additionally, the authors develop a novel loss function for the MBFD task that achieves robust and general performance on several benchmark datasets, demonstrating its potential for real-life MBFD applications.
</details></li>
</ul>
<hr>
<h2 id="Denevil-Towards-Deciphering-and-Navigating-the-Ethical-Values-of-Large-Language-Models-via-Instruction-Learning"><a href="#Denevil-Towards-Deciphering-and-Navigating-the-Ethical-Values-of-Large-Language-Models-via-Instruction-Learning" class="headerlink" title="Denevil: Towards Deciphering and Navigating the Ethical Values of Large Language Models via Instruction Learning"></a>Denevil: Towards Deciphering and Navigating the Ethical Values of Large Language Models via Instruction Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11053">http://arxiv.org/abs/2310.11053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shitong Duan, Xiaoyuan Yi, Peng Zhang, Tun Lu, Xing Xie, Ning Gu</li>
<li>for: This paper aims to explore the ethical values of large language models (LLMs) and develop methods to improve their value compliance.</li>
<li>methods: The paper proposes a novel prompt generation algorithm called DeNEVIL to dynamically elicit the violation of ethics in LLMs, and constructs a high-quality dataset called MoralPrompt to benchmark the intrinsic values of LLMs. The paper also develops an in-context alignment method called VILMO to improve the value compliance of LLM outputs.</li>
<li>results: The paper discovers that most LLMs are essentially misaligned and demonstrates the effectiveness of VILMO in improving the value compliance of LLM outputs. The results provide a promising initial step in studying the ethical values of LLMs and aligning their outputs with human values.Here’s the Simplified Chinese text:</li>
<li>for: 这篇论文旨在探索大语言模型（LLMs）的伦理价值，并开发方法来提高其价值兼容性。</li>
<li>methods: 论文提出了一种新的提示生成算法called DeNEVIL，可以动态激发LLMs中的伦理违反行为，并构建了一个高质量的数据集called MoralPrompt，用于对LLMs的内在价值进行评估。论文还开发了一种名为VIMO的增值环境，用于在LLMs输出中提高价值兼容性。</li>
<li>results: 论文发现大多数LLMs是 Essentially Misaligned，并证明了VIMO的效果性。结果提供了一个有前途的初步探索LLMs的伦理价值，并将其输出与人类价值进行对齐。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have made unprecedented breakthroughs, yet their increasing integration into everyday life might raise societal risks due to generated unethical content. Despite extensive study on specific issues like bias, the intrinsic values of LLMs remain largely unexplored from a moral philosophy perspective. This work delves into ethical values utilizing Moral Foundation Theory. Moving beyond conventional discriminative evaluations with poor reliability, we propose DeNEVIL, a novel prompt generation algorithm tailored to dynamically exploit LLMs' value vulnerabilities and elicit the violation of ethics in a generative manner, revealing their underlying value inclinations. On such a basis, we construct MoralPrompt, a high-quality dataset comprising 2,397 prompts covering 500+ value principles, and then benchmark the intrinsic values across a spectrum of LLMs. We discovered that most models are essentially misaligned, necessitating further ethical value alignment. In response, we develop VILMO, an in-context alignment method that substantially enhances the value compliance of LLM outputs by learning to generate appropriate value instructions, outperforming existing competitors. Our methods are suitable for black-box and open-source models, offering a promising initial step in studying the ethical values of LLMs.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经创造出了无 precedent 的突破，但是它们的日益加入到日常生活中可能会提高社会风险，因为它们可能会生成不道德的内容。despite extensive research on specific issues such as bias, the intrinsic values of LLMs remain largely unexplored from a moral philosophy perspective. This work delves into ethical values using Moral Foundation Theory. Moving beyond conventional discriminative evaluations with poor reliability, we propose DeNEVIL, a novel prompt generation algorithm tailored to dynamically exploit LLMs' value vulnerabilities and elicit the violation of ethics in a generative manner, revealing their underlying value inclinations. On such a basis, we construct MoralPrompt, a high-quality dataset comprising 2,397 prompts covering 500+ value principles, and then benchmark the intrinsic values across a spectrum of LLMs. We discovered that most models are essentially misaligned, necessitating further ethical value alignment. In response, we develop VILMO, an in-context alignment method that substantially enhances the value compliance of LLM outputs by learning to generate appropriate value instructions, outperforming existing competitors. Our methods are suitable for black-box and open-source models, offering a promising initial step in studying the ethical values of LLMs.
</details></li>
</ul>
<hr>
<h2 id="Nonet-at-SemEval-2023-Task-6-Methodologies-for-Legal-Evaluation"><a href="#Nonet-at-SemEval-2023-Task-6-Methodologies-for-Legal-Evaluation" class="headerlink" title="Nonet at SemEval-2023 Task 6: Methodologies for Legal Evaluation"></a>Nonet at SemEval-2023 Task 6: Methodologies for Legal Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11049">http://arxiv.org/abs/2310.11049</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shubhamkumarnigam/legaleval23_nonet">https://github.com/shubhamkumarnigam/legaleval23_nonet</a></li>
<li>paper_authors: Shubham Kumar Nigam, Aniket Deroy, Noel Shallum, Ayush Kumar Mishra, Anup Roy, Shubham Kumar Mishra, Arnab Bhattacharya, Saptarshi Ghosh, Kripabandhu Ghosh</li>
<li>for: 这个论文是为了参加SemEval-2023任务6：理解法律文档而写的。</li>
<li>methods: 论文使用了多种实验方法，包括法律命名实体识别（L-NER）、法律预测（LJP）和法律审判说明（CJPE）等三个子任务。</li>
<li>results: 论文在这三个子任务中的result包括数据统计和方法ология，并在领先者排名中获得了竞争性的排名，分别为15$^{th}$, 11$^{th}$, 和1$^{st}$。<details>
<summary>Abstract</summary>
This paper describes our submission to the SemEval-2023 for Task 6 on LegalEval: Understanding Legal Texts. Our submission concentrated on three subtasks: Legal Named Entity Recognition (L-NER) for Task-B, Legal Judgment Prediction (LJP) for Task-C1, and Court Judgment Prediction with Explanation (CJPE) for Task-C2. We conducted various experiments on these subtasks and presented the results in detail, including data statistics and methodology. It is worth noting that legal tasks, such as those tackled in this research, have been gaining importance due to the increasing need to automate legal analysis and support. Our team obtained competitive rankings of 15$^{th}$, 11$^{th}$, and 1$^{st}$ in Task-B, Task-C1, and Task-C2, respectively, as reported on the leaderboard.
</details>
<details>
<summary>摘要</summary>
这份论文描述了我们在SemEval-2023中的任务6中对LegalEval：理解法律文本的提交。我们的提交集中了三个子任务：法律名称识别（L-NER）、法律预测（LJP）和法律判决预测与解释（CJPE）。我们进行了各种实验，并在详细的报告中提供了数据统计和方法ология。值得注意的是，如果法律任务，如这些研究所解决的，在过去几年中得到了越来越多的重视，因为自动化法律分析和支持的需求不断增长。我们团队在排名榜上获得了15名、11名和1名的竞争性排名，分别对应于任务B、C1和C2。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Contrastive-Learning-via-Distributionally-Robust-Optimization"><a href="#Understanding-Contrastive-Learning-via-Distributionally-Robust-Optimization" class="headerlink" title="Understanding Contrastive Learning via Distributionally Robust Optimization"></a>Understanding Contrastive Learning via Distributionally Robust Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11048">http://arxiv.org/abs/2310.11048</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/junkangwu/ADNCE">https://github.com/junkangwu/ADNCE</a></li>
<li>paper_authors: Junkang Wu, Jiawei Chen, Jiancan Wu, Wentao Shi, Xiang Wang, Xiangnan He</li>
<li>for: 这个研究探讨了对比学习（CL）的内在忍容性，即负样本可能包含相似 semantics（例如标签）。然而，现有的理论无法提供这种现象的解释。本研究填补了这个研究漏洞，通过分析CL通过透视分布 robust optimization（DRO）的角度，获得了以下几个关键发现：</li>
<li>methods: CL实际上对负样本分布进行DRO，因此可以在多种可能的分布下实现Robust性，并且具有对 sampling bias 的忍容性; 温度参数 $\tau$ 不仅是优化器的临时方法，而是 Lagrange 系数，控制负样本分布集的大小;</li>
<li>results: 研究发现CL具有 InfoNCE 作为估计的� strongMutual Information 和一种新的 $\phi$- divergence 基于通信信息的 estimator，并且提出了一种改进的 Adjusted InfoNCE 损失函数（ADNCE），可以 Mitigate CL 的缺点，包括过度保守和敏感到异常值。广泛的实验在图像、句子和图 граhp 等领域 validate 了提议的效果。代码可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/junkangwu/ADNCE%7D">https://github.com/junkangwu/ADNCE}</a> 上获取。<details>
<summary>Abstract</summary>
This study reveals the inherent tolerance of contrastive learning (CL) towards sampling bias, wherein negative samples may encompass similar semantics (\eg labels). However, existing theories fall short in providing explanations for this phenomenon. We bridge this research gap by analyzing CL through the lens of distributionally robust optimization (DRO), yielding several key insights: (1) CL essentially conducts DRO over the negative sampling distribution, thus enabling robust performance across a variety of potential distributions and demonstrating robustness to sampling bias; (2) The design of the temperature $\tau$ is not merely heuristic but acts as a Lagrange Coefficient, regulating the size of the potential distribution set; (3) A theoretical connection is established between DRO and mutual information, thus presenting fresh evidence for ``InfoNCE as an estimate of MI'' and a new estimation approach for $\phi$-divergence-based generalized mutual information. We also identify CL's potential shortcomings, including over-conservatism and sensitivity to outliers, and introduce a novel Adjusted InfoNCE loss (ADNCE) to mitigate these issues. It refines potential distribution, improving performance and accelerating convergence. Extensive experiments on various domains (image, sentence, and graphs) validate the effectiveness of the proposal. The code is available at \url{https://github.com/junkangwu/ADNCE}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>CL essentially performs DRO over the negative sampling distribution, enabling robust performance across various potential distributions and demonstrating resistance to sampling bias.2. The temperature parameter $\tau$ is not just a heuristic, but rather a Lagrange coefficient that regulates the size of the potential distribution set.3. We establish a theoretical connection between DRO and mutual information, providing fresh evidence for the idea that “InfoNCE is an estimate of MI” and presenting a new approach for estimating $\phi$-divergence-based generalized mutual information.However, CL also has some limitations, such as over-conservatism and sensitivity to outliers. To address these issues, we propose a novel Adjusted InfoNCE loss (ADNCE) that refines the potential distribution and improves performance and convergence. Extensive experiments on various domains (images, sentences, and graphs) demonstrate the effectiveness of our proposal. The code is available at \url{<a target="_blank" rel="noopener" href="https://github.com/junkangwu/ADNCE%7D">https://github.com/junkangwu/ADNCE}</a>.</details></li>
</ol>
<hr>
<h2 id="Fast-Graph-Condensation-with-Structure-based-Neural-Tangent-Kernel"><a href="#Fast-Graph-Condensation-with-Structure-based-Neural-Tangent-Kernel" class="headerlink" title="Fast Graph Condensation with Structure-based Neural Tangent Kernel"></a>Fast Graph Condensation with Structure-based Neural Tangent Kernel</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11046">http://arxiv.org/abs/2310.11046</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lin Wang, Wenqi Fan, Jiatong Li, Yao Ma, Qing Li</li>
<li>for: 减少大规模图数据的计算成本，以便更好地应用图神经网络（GNNs）。</li>
<li>methods: 将图数据 condensed 为更小的图数据，使用 Kernel Ridge Regression（KRR）任务 instead of GNNs 的迭代训练。</li>
<li>results: 提出了一种基于 Structure-based Neural Tangent Kernel（SNTK）的数据压缩框架（GC-SNTK），可以减少图数据的计算成本，保持高精度预测性能。<details>
<summary>Abstract</summary>
The rapid development of Internet technology has given rise to a vast amount of graph-structured data. Graph Neural Networks (GNNs), as an effective method for various graph mining tasks, incurs substantial computational resource costs when dealing with large-scale graph data. A data-centric manner solution is proposed to condense the large graph dataset into a smaller one without sacrificing the predictive performance of GNNs. However, existing efforts condense graph-structured data through a computational intensive bi-level optimization architecture also suffer from massive computation costs. In this paper, we propose reforming the graph condensation problem as a Kernel Ridge Regression (KRR) task instead of iteratively training GNNs in the inner loop of bi-level optimization. More specifically, We propose a novel dataset condensation framework (GC-SNTK) for graph-structured data, where a Structure-based Neural Tangent Kernel (SNTK) is developed to capture the topology of graph and serves as the kernel function in KRR paradigm. Comprehensive experiments demonstrate the effectiveness of our proposed model in accelerating graph condensation while maintaining high prediction performance.
</details>
<details>
<summary>摘要</summary>
“互联网科技的快速发展导致了大量的树结构数据的生成。树神经网络（GNNs）作为许多树采矿任务的有效方法，对于大规模树数据而言，具有很大的计算资源成本。为了缩小大型树dataset，而不是降低GNNs的预测性能，我们提出了一个数据中心的解决方案。但是，现有的实现方法通过重复地训练GNNs内部的双层优化架构，具有巨大的计算成本。在本文中，我们提出了将树数据缩小问题转化为核心ridge regression（KRR）任务，而不是透过双层优化架构的迭代训练GNNs。更specifically，我们提出了一个新的数据缩小框架（GC-SNTK），其中一个基于结构的神经 tangent kernel（SNTK）被设计来捕捉树的结构，并且作为KRR模式中的kernel函数。实验结果显示，我们的提议的模型能够快速缩小树数据，而且保持高预测性能。”
</details></li>
</ul>
<hr>
<h2 id="Spoofing-Attack-Detection-in-the-Physical-Layer-with-Robustness-to-User-Movement"><a href="#Spoofing-Attack-Detection-in-the-Physical-Layer-with-Robustness-to-User-Movement" class="headerlink" title="Spoofing Attack Detection in the Physical Layer with Robustness to User Movement"></a>Spoofing Attack Detection in the Physical Layer with Robustness to User Movement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11043">http://arxiv.org/abs/2310.11043</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Romero, Tien Ngoc Ha, Peter Gerstoft</li>
<li>for: 防止 spoofing 攻击</li>
<li>methods:  combining 深度学习 和 图граhp 检测</li>
<li>results: 可以准确地 отлиenciate spoofing 和 user movement<details>
<summary>Abstract</summary>
In a spoofing attack, an attacker impersonates a legitimate user to access or modify data belonging to the latter. Typical approaches for spoofing detection in the physical layer declare an attack when a change is observed in certain channel features, such as the received signal strength (RSS) measured by spatially distributed receivers. However, since channels change over time, for example due to user movement, such approaches are impractical. To sidestep this limitation, this paper proposes a scheme that combines the decisions of a position-change detector based on a deep neural network to distinguish spoofing from movement. Building upon community detection on graphs, the sequence of received frames is partitioned into subsequences to detect concurrent transmissions from distinct locations. The scheme can be easily deployed in practice since it just involves collecting a small dataset of measurements at a few tens of locations that need not even be computed or recorded. The scheme is evaluated on real data collected for this purpose.
</details>
<details>
<summary>摘要</summary>
在 spoofing 攻击中，攻击者会伪装为合法用户，以访问或修改受影响用户的数据。通常的 spoofing 检测方法在物理层将攻击宣告为当前通道特征发生变化，如接收信号强度（RSS）测量的空间分布式接收器。然而，由于通道随着时间的变化，例如用户移动，这些方法是不实用的。为了绕过这些限制，这篇论文提议一种方案，将 deep neural network 基于位置变化探测器的决策与 Movement 分离开来。基于图 communit 探测，接收的序列被分割成子序列，以检测同时从不同位置发送的同时传输。该方案可以轻松实现，只需要收集一小量的测量数据，并且不需要计算或记录。这篇论文使用实际数据进行评估。
</details></li>
</ul>
<hr>
<h2 id="Radio-Map-Estimation-in-the-Real-World-Empirical-Validation-and-Analysis"><a href="#Radio-Map-Estimation-in-the-Real-World-Empirical-Validation-and-Analysis" class="headerlink" title="Radio Map Estimation in the Real-World: Empirical Validation and Analysis"></a>Radio Map Estimation in the Real-World: Empirical Validation and Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11036">http://arxiv.org/abs/2310.11036</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raju Shrestha, Tien Ngoc Ha, Pham Q. Viet, Daniel Romero</li>
<li>for: 这篇论文主要针对的是量化广播信号强度或其他广播频率环境中每个点的地理区域。</li>
<li>methods: 这篇论文使用了许多现有的广播地图估计器，并对这些估计器进行了实际验证。</li>
<li>results: 研究发现，使用深度神经网络（DNNs）的复杂估计器可以提供最佳性能，但它们需要大量的训练数据来达到显著优势。一种新的混合估计器可以同时利用这两种估计器的优点，并且可能值得进一步探索。<details>
<summary>Abstract</summary>
Radio maps quantify received signal strength or other magnitudes of the radio frequency environment at every point of a geographical region. These maps play a vital role in a large number of applications such as wireless network planning, spectrum management, and optimization of communication systems. However, empirical validation of the large number of existing radio map estimators is highly limited. To fill this gap, a large data set of measurements has been collected with an autonomous unmanned aerial vehicle (UAV) and a representative subset of these estimators were evaluated on this data. The performance-complexity trade-off and the impact of fast fading are extensively investigated. Although sophisticated estimators based on deep neural networks (DNNs) exhibit the best performance, they are seen to require large volumes of training data to offer a substantial advantage relative to more traditional schemes. A novel algorithm that blends both kinds of estimators is seen to enjoy the benefits of both, thereby suggesting the potential of exploring this research direction further.
</details>
<details>
<summary>摘要</summary>
Radio 地图量化接收信号强度或其他频率环境中每个地理区域点的其他物理量。这些地图在许多应用中发挥重要作用，如无线网络规划、频谱管理和通信系统优化。然而，现有的大量Radio map estimator的实验 validate 是非常有限的。为了填补这一空白，一个大量测量数据集被收集，并对这些 estimator 进行了评估。本研究探讨了性能vs复杂度的贸易和快速抖动的影响。虽然基于深度神经网络（DNNs）的复杂 estimator 表现最佳，但它们需要大量的训练数据来提供substantial 的优势 relative to 传统方案。一种混合 estimator 的新算法被发现，它们享有两种 estimator 的优点，因此更多的研究是可能的。
</details></li>
</ul>
<hr>
<h2 id="Core-Building-Blocks-Next-Gen-Geo-Spatial-GPT-Application"><a href="#Core-Building-Blocks-Next-Gen-Geo-Spatial-GPT-Application" class="headerlink" title="Core Building Blocks: Next Gen Geo Spatial GPT Application"></a>Core Building Blocks: Next Gen Geo Spatial GPT Application</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11029">http://arxiv.org/abs/2310.11029</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashley Fernandez, Swaraj Dube</li>
<li>for: 本研究提出了 MapGPT，一种将自然语言理解和地理数据处理技术相结合的新approach，以增强地理数据理解和生成。</li>
<li>methods: 本研究使用了大型语言模型（LLMs）和地理数据处理技术，并提出了一种将这两者相结合的方法。这种方法利用了地理和文本数据的token化和向量表示，以提高响应位置相关的问题的准确性和Contextual awareness。</li>
<li>results: 研究表明，通过结合LMMs和地理数据处理技术，MapGPT可以提供更加准确和Contextual awareness的响应，并且可以实现地理计算和可视化输出。<details>
<summary>Abstract</summary>
This paper proposes MapGPT which is a novel approach that integrates the capabilities of language models, specifically large language models (LLMs), with spatial data processing techniques. This paper introduces MapGPT, which aims to bridge the gap between natural language understanding and spatial data analysis by highlighting the relevant core building blocks. By combining the strengths of LLMs and geospatial analysis, MapGPT enables more accurate and contextually aware responses to location-based queries. The proposed methodology highlights building LLMs on spatial and textual data, utilizing tokenization and vector representations specific to spatial information. The paper also explores the challenges associated with generating spatial vector representations. Furthermore, the study discusses the potential of computational capabilities within MapGPT, allowing users to perform geospatial computations and obtain visualized outputs. Overall, this research paper presents the building blocks and methodology of MapGPT, highlighting its potential to enhance spatial data understanding and generation in natural language processing applications.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了MapGPT，一种新的方法，它将自然语言理解和空间数据处理技术相结合。这篇论文描述了MapGPT的目标是将自然语言理解和空间数据分析相连接，并通过高亮相关核心组件来强调这个目标。通过结合LLMs和地理空间分析的优势，MapGPT可以提供更加准确和上下文感知的回答。该方法利用了地理空间数据和文本数据的Tokenization和 вектор表示，并解决了生成空间 вектор表示的挑战。此外，该研究还探讨了MapGPT的计算能力，允许用户进行地理计算并获得可视化输出。总之，这篇研究论文介绍了MapGPT的建构和方法，并强调其在自然语言处理应用中增强空间数据理解和生成的潜在能力。
</details></li>
</ul>
<hr>
<h2 id="Compatible-Transformer-for-Irregularly-Sampled-Multivariate-Time-Series"><a href="#Compatible-Transformer-for-Irregularly-Sampled-Multivariate-Time-Series" class="headerlink" title="Compatible Transformer for Irregularly Sampled Multivariate Time Series"></a>Compatible Transformer for Irregularly Sampled Multivariate Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11022">http://arxiv.org/abs/2310.11022</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mediabrain-sjtu/coformer">https://github.com/mediabrain-sjtu/coformer</a></li>
<li>paper_authors: Yuxi Wei, Juntong Peng, Tong He, Chenxin Xu, Jian Zhang, Shirui Pan, Siheng Chen</li>
<li>for: 这篇论文的目的是解决 irregularly sampled multivariate time series 的分析问题，因为现有的方法适用于常规时间序列数据不能直接处理这种不规时间序列数据。</li>
<li>methods: 本文提出了 Compatible Transformer（CoFormer），一个基于 transformer 的Encoder，用于实现每个单元样本的综合时间互动特征学习。CoFormer 视每个样本为唯一的 variate-time 点，并通过 intra-variate&#x2F;inter-variate 专注来学习每个样本的时间&#x2F;互动特征基于 intra-variate&#x2F;inter-variate 邻居。</li>
<li>results: 本文的实验结果显示，对于多个真实世界数据集，CoFormer 对于预测和分类 зада问表现出卓越的成绩，与现有的方法相比，CoFormer 具有优异的表现。<details>
<summary>Abstract</summary>
To analyze multivariate time series, most previous methods assume regular subsampling of time series, where the interval between adjacent measurements and the number of samples remain unchanged. Practically, data collection systems could produce irregularly sampled time series due to sensor failures and interventions. However, existing methods designed for regularly sampled multivariate time series cannot directly handle irregularity owing to misalignment along both temporal and variate dimensions. To fill this gap, we propose Compatible Transformer (CoFormer), a transformer-based encoder to achieve comprehensive temporal-interaction feature learning for each individual sample in irregular multivariate time series. In CoFormer, we view each sample as a unique variate-time point and leverage intra-variate/inter-variate attentions to learn sample-wise temporal/interaction features based on intra-variate/inter-variate neighbors. With CoFormer as the core, we can analyze irregularly sampled multivariate time series for many downstream tasks, including classification and prediction. We conduct extensive experiments on 3 real-world datasets and validate that the proposed CoFormer significantly and consistently outperforms existing methods.
</details>
<details>
<summary>摘要</summary>
多变量时间序列分析方法中，大多数先前方法假设时间序列的尺度保持不变，即间隔时间和样本数均不变。然而，实际上，数据收集系统可能会生成不规则的时间序列，这是因为仪器故障和干预等原因。现有的方法无法直接处理不规则的时间序列，这是因为它们在时间和变量维度上存在偏移。为填补这个空白，我们提出了兼容变换器（CoFormer），一种基于变换器的编码器，用于在不规则的多变量时间序列中实现每个样本独特的时间互动特征学习。在CoFormer中，我们视每个样本为独特的变量-时间点，并通过内变量/外变量注意力来学习每个样本的时间互动特征，基于内变量/外变量的邻居。通过CoFormer作为核心，我们可以分析不规则的多变量时间序列，并进行识别和预测等下游任务。我们在3个真实世界数据集上进行了广泛的实验，并证明了提出的CoFormer在 existed 方法的基础上显著并且一致性地提高了性能。
</details></li>
</ul>
<hr>
<h2 id="From-Identifiable-Causal-Representations-to-Controllable-Counterfactual-Generation-A-Survey-on-Causal-Generative-Modeling"><a href="#From-Identifiable-Causal-Representations-to-Controllable-Counterfactual-Generation-A-Survey-on-Causal-Generative-Modeling" class="headerlink" title="From Identifiable Causal Representations to Controllable Counterfactual Generation: A Survey on Causal Generative Modeling"></a>From Identifiable Causal Representations to Controllable Counterfactual Generation: A Survey on Causal Generative Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11011">http://arxiv.org/abs/2310.11011</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aneesh Komanduri, Xintao Wu, Yongkai Wu, Feng Chen</li>
<li>for: 这篇论文的目的是探讨如何通过结构 causal modeling 改进深度生成模型，以提高其解释性、避免偶极相关性和外部数据描述稳定性。</li>
<li>methods: 论文使用了结构 causal modeling 方法，包括 causal representation learning 和可控Counterfactual生成方法，以帮助改进深度生成模型的性能。</li>
<li>results: 论文的结果表明，通过结构 causal modeling 可以提高深度生成模型的解释性、避免偶极相关性和外部数据描述稳定性，同时提高数据生成的准确性和多样性。<details>
<summary>Abstract</summary>
Deep generative models have shown tremendous success in data density estimation and data generation from finite samples. While these models have shown impressive performance by learning correlations among features in the data, some fundamental shortcomings are their lack of explainability, the tendency to induce spurious correlations, and poor out-of-distribution extrapolation. In an effort to remedy such challenges, one can incorporate the theory of causality in deep generative modeling. Structural causal models (SCMs) describe data-generating processes and model complex causal relationships and mechanisms among variables in a system. Thus, SCMs can naturally be combined with deep generative models. Causal models offer several beneficial properties to deep generative models, such as distribution shift robustness, fairness, and interoperability. We provide a technical survey on causal generative modeling categorized into causal representation learning and controllable counterfactual generation methods. We focus on fundamental theory, formulations, drawbacks, datasets, metrics, and applications of causal generative models in fairness, privacy, out-of-distribution generalization, and precision medicine. We also discuss open problems and fruitful research directions for future work in the field.
</details>
<details>
<summary>摘要</summary>
深度生成模型在数据密度估计和数据生成从有限样本中表现出色，但它们存在一些基本缺陷，如无法解释、产生假 correlations 和外部扩展不稳定。为了解决这些挑战，可以在深度生成模型中涵盖 causality 理论。结构 causal model（SCM）描述了数据生成过程，模型了系统中变量之间复杂的 causal 关系和机制。因此，SCM 可以自然地与深度生成模型结合。 causal 模型具有许多有利的性能，如分布shift 稳定性、公平性和可操作性。我们提供了深入检查 causal 生成模型的技术survey，分为 causal representation learning 和可控 counterfactual generation 方法。我们关注基本理论、形式、缺陷、数据集、指标和应用于公平、隐私、外部扩展、精准医学等领域。我们还讨论了未解决的问题和未来研究的可能性。
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Scalable-Graph-Neural-Network-Inference-with-Node-Adaptive-Propagation"><a href="#Accelerating-Scalable-Graph-Neural-Network-Inference-with-Node-Adaptive-Propagation" class="headerlink" title="Accelerating Scalable Graph Neural Network Inference with Node-Adaptive Propagation"></a>Accelerating Scalable Graph Neural Network Inference with Node-Adaptive Propagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10998">http://arxiv.org/abs/2310.10998</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyi Gao, Wentao Zhang, Junliang Yu, Yingxia Shao, Quoc Viet Hung Nguyen, Bin Cui, Hongzhi Yin</li>
<li>for: 提高大规模图的图神经网络（GNNs）实时推理性能。</li>
<li>methods: 提出在线卷积框架和两种基于节点特征信息自适应卷积深度的节点适应卷积方法，以避免重复的特征传播。</li>
<li>results: 在公共数据集上实现了比例性和效率的较好的推理加速，特别是在大规模图上，实现了75倍的推理速度提升。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have exhibited exceptional efficacy in a diverse array of applications. However, the sheer size of large-scale graphs presents a significant challenge to real-time inference with GNNs. Although existing Scalable GNNs leverage linear propagation to preprocess the features and accelerate the training and inference procedure, these methods still suffer from scalability issues when making inferences on unseen nodes, as the feature preprocessing requires the graph to be known and fixed. To further accelerate Scalable GNNs inference in this inductive setting, we propose an online propagation framework and two novel node-adaptive propagation methods that can customize the optimal propagation depth for each node based on its topological information and thereby avoid redundant feature propagation. The trade-off between accuracy and latency can be flexibly managed through simple hyper-parameters to accommodate various latency constraints. Moreover, to compensate for the inference accuracy loss caused by the potential early termination of propagation, we further propose Inception Distillation to exploit the multi-scale receptive field information within graphs. The rigorous and comprehensive experimental study on public datasets with varying scales and characteristics demonstrates that the proposed inference acceleration framework outperforms existing state-of-the-art graph inference acceleration methods in terms of accuracy and efficiency. Particularly, the superiority of our approach is notable on datasets with larger scales, yielding a 75x inference speedup on the largest Ogbn-products dataset.
</details>
<details>
<summary>摘要</summary>
GRAPHNeuralNetworks (GNNs) 已经在多种应用中表现出色。然而，大规模图表示一个 significante challenge  для实时推理GNNs。虽然现有的可扩展GNNs使用线性宣传来预处理特征和加速训练和推理过程，但这些方法仍然在对未看过节点的推理中遇到缺乏扩展性的问题，因为特征预处理需要知道和固定的图。为了进一步加速可扩展GNNs的推理在这种推理设定中，我们提议了在线宣传框架和两种新的节点适应性宣传方法。这些方法可以根据每个节点的 topological information 自适应地定制最佳宣传深度，并因此避免了 redundant feature propagation。通过简单的 гиперпараметр来管理准确率和延迟的负担，我们可以适应不同的延迟限制。此外，为了补做因推理早期终止而导致的准确性损失，我们进一步提议了 Inception Distillation，以利用图中的多尺度接收器场信息。我们对公共数据集进行了严格和全面的实验研究，结果显示，我们的推理加速框架在准确率和效率方面都超过了现有的状态码图推理加速方法。特别是在大规模的数据集上，我们的方法可以实现75倍的推理速度增加。
</details></li>
</ul>
<hr>
<h2 id="EXMODD-An-EXplanatory-Multimodal-Open-Domain-Dialogue-dataset"><a href="#EXMODD-An-EXplanatory-Multimodal-Open-Domain-Dialogue-dataset" class="headerlink" title="EXMODD: An EXplanatory Multimodal Open-Domain Dialogue dataset"></a>EXMODD: An EXplanatory Multimodal Open-Domain Dialogue dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10967">http://arxiv.org/abs/2310.10967</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/poplpr/exmodd">https://github.com/poplpr/exmodd</a></li>
<li>paper_authors: Hang Yin, Pinren Lu, Ziang Li, Bin Sun, Kan Li</li>
<li>for: 提高对对话任务的研究质量，减少数据收集成本</li>
<li>methods: 提出多Modal数据建构框架（MDCF），通过设计合适的提示语来让大规模预训练语言模型生成高质量的内容，同时提供图像和对话的自动解释，提高可读性和可监测性</li>
<li>results: 实验表明，使用MDCF生成的对话数据和图像解释具有正确性和高质量，可以帮助提高对对话任务的研究质量<details>
<summary>Abstract</summary>
The need for high-quality data has been a key issue hindering the research of dialogue tasks. Recent studies try to build datasets through manual, web crawling, and large pre-trained models. However, man-made data is expensive and data collected from the internet often includes generic responses, meaningless statements, and toxic dialogues. Automatic data generation through large models is a cost-effective method, but for open-domain multimodal dialogue tasks, there are still three drawbacks: 1) There is currently no open-source large model that can accept multimodal input; 2) The content generated by the model lacks interpretability; 3) The generated data is usually difficult to quality control and require extensive resource to collect. To alleviate the significant human and resource expenditure in data collection, we propose a Multimodal Data Construction Framework (MDCF). MDCF designs proper prompts to spur the large-scale pre-trained language model to generate well-formed and satisfactory content. Additionally, MDCF also automatically provides explanation for a given image and its corresponding dialogue, which can provide a certain degree of interpretability and facilitate manual follow-up quality inspection. Based on this, we release an Explanatory Multimodal Open-Domain dialogue dataset (EXMODD). Experiments indicate a positive correlation between the model's ability to generate accurate understandings and high-quality responses. Our code and data can be found at https://github.com/poplpr/EXMODD.
</details>
<details>
<summary>摘要</summary>
需求高质量数据一直是对对话任务研究的关键障碍。近期研究通过手动、网络爬虫和大型预训练模型建立数据集。然而，人工生成数据昂贵，网络上收集的数据经常包含无关的回答、意义不明确的声明以及恶意对话。通过大型模型自动生成数据是一种经济的方法，但对开放频道多媒体对话任务还存在三个缺点：1）目前没有开源的大型模型可以接受多媒体输入；2）模型生成的内容缺乏可读性；3）生成的数据困难以质量控制，需要广泛的资源来收集。为了减少数据收集的人工和资源投入，我们提出了多媒体数据建构框架（MDCF）。MDCF设计合适的提示，使大规模预训练语言模型生成高质量和满意的内容。此外，MDCF还自动提供图像和对应对话的解释，可以提供一定的可读性，便于手动跟踪质量检查。根据这，我们发布了解释多媒体开放频道对话数据集（EXMODD）。实验表明，模型能够生成准确理解和高质量回答之间存在正相关关系。我们的代码和数据可以在 GitHub 上找到。
</details></li>
</ul>
<hr>
<h2 id="A-State-Vector-Framework-for-Dataset-Effects"><a href="#A-State-Vector-Framework-for-Dataset-Effects" class="headerlink" title="A State-Vector Framework for Dataset Effects"></a>A State-Vector Framework for Dataset Effects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10955">http://arxiv.org/abs/2310.10955</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/esmatsahak/emnlp-2023_a-state-vector-framework-for-dataset-effects_repository">https://github.com/esmatsahak/emnlp-2023_a-state-vector-framework-for-dataset-effects_repository</a></li>
<li>paper_authors: Esmat Sahak, Zining Zhu, Frank Rudzicz</li>
<li>for: 这个论文旨在研究深度神经网络（DNN）系统的高质量数据集如何影响其性能，以及这些数据集之间的交互作用如何影响模型的性能。</li>
<li>methods: 该论文提出了一种状态向量框架，用于系统地研究数据集之间的交互作用。该框架使用理想化的探测试结果为基准，将数据集转化为一个维度空间中的状态向量。这种方法可以评估单独和相互作用的数据集效果，并且可以探索模型在不同维度上的表现。</li>
<li>results: 研究发现，一些常用的自然语言理解数据集具有特点性的效果，这些效果集中在一些语言维度上。此外，研究还发现了一些“泄漏”效果：数据集可以影响模型在不同维度上的表现，这些维度可能与计划中的任务无关。该研究为负责任和可靠的模型开发提供了一个系统的理解。<details>
<summary>Abstract</summary>
The impressive success of recent deep neural network (DNN)-based systems is significantly influenced by the high-quality datasets used in training. However, the effects of the datasets, especially how they interact with each other, remain underexplored. We propose a state-vector framework to enable rigorous studies in this direction. This framework uses idealized probing test results as the bases of a vector space. This framework allows us to quantify the effects of both standalone and interacting datasets. We show that the significant effects of some commonly-used language understanding datasets are characteristic and are concentrated on a few linguistic dimensions. Additionally, we observe some ``spill-over'' effects: the datasets could impact the models along dimensions that may seem unrelated to the intended tasks. Our state-vector framework paves the way for a systematic understanding of the dataset effects, a crucial component in responsible and robust model development.
</details>
<details>
<summary>摘要</summary>
“深度神经网络（DNN）系统的卓越成功受到训练 datasets 的高质量影响，但 datasets 之间的交互效果还未得到足够探究。我们提出了一个状态向量框架，以便系统地研究这一方面。这个框架使用理想化的 probing 测试结果作为基准，并允许我们量化单独和交互 datasets 的效果。我们发现一些通用语言理解 datasets 的效果是特征性的，集中在一些语言维度上。此外，我们还发现了一些“倒流”效果： datasets 可以影响模型在不直接相关的任务上的表现。我们的状态向量框架为负责任和可靠模型开发提供了一个系统的理解。”
</details></li>
</ul>
<hr>
<h2 id="Enhanced-Transformer-Architecture-for-Natural-Language-Processing"><a href="#Enhanced-Transformer-Architecture-for-Natural-Language-Processing" class="headerlink" title="Enhanced Transformer Architecture for Natural Language Processing"></a>Enhanced Transformer Architecture for Natural Language Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10930">http://arxiv.org/abs/2310.10930</a></li>
<li>repo_url: None</li>
<li>paper_authors: Woohyeon Moon, Taeyoung Kim, Bumgeun Park, Dongsoo Har</li>
<li>for: 提高自然语言处理（NLP）领域的模型性能</li>
<li>methods: 提出一种新的 transformer 结构，包括全层正常化、权重连接、位置编码和零干扰自注意力</li>
<li>results: 使用 Multi30k 翻译数据集进行评估，提高了对 original transformer 的202.96% 的 BLEU 分数<details>
<summary>Abstract</summary>
Transformer is a state-of-the-art model in the field of natural language processing (NLP). Current NLP models primarily increase the number of transformers to improve processing performance. However, this technique requires a lot of training resources such as computing capacity. In this paper, a novel structure of Transformer is proposed. It is featured by full layer normalization, weighted residual connection, positional encoding exploiting reinforcement learning, and zero masked self-attention. The proposed Transformer model, which is called Enhanced Transformer, is validated by the bilingual evaluation understudy (BLEU) score obtained with the Multi30k translation dataset. As a result, the Enhanced Transformer achieves 202.96% higher BLEU score as compared to the original transformer with the translation dataset.
</details>
<details>
<summary>摘要</summary>
transformer 是当前自然语言处理（NLP）领域的先进模型。现有的 NLP 模型主要通过增加 transformer 的数量来提高处理性能。然而，这种技术需要大量的训练资源，如计算能力。在这篇论文中，一种新的 transformer 结构被提出，具有全层正常化、权重征值连接、位置编码利用强化学习和零层隐藏自注意力。这种提出的 transformer 模型被称为增强 transformer，在使用 Multi30k 翻译集合时通过对比 Bleu 分数来验证其性能。结果显示，增强 transformer 与原始 transformer 相比，在 Multi30k 翻译集合中的 Bleu 分数提高了202.96%。
</details></li>
</ul>
<hr>
<h2 id="Using-Audio-Data-to-Facilitate-Depression-Risk-Assessment-in-Primary-Health-Care"><a href="#Using-Audio-Data-to-Facilitate-Depression-Risk-Assessment-in-Primary-Health-Care" class="headerlink" title="Using Audio Data to Facilitate Depression Risk Assessment in Primary Health Care"></a>Using Audio Data to Facilitate Depression Risk Assessment in Primary Health Care</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10928">http://arxiv.org/abs/2310.10928</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Valen Levinson, Abhay Goyal, Roger Ho Chun Man, Roy Ka-Wei Lee, Koustuv Saha, Nimay Parekh, Frederick L. Altice, Lam Yin Cheung, Munmun De Choudhury, Navin Kumar</li>
<li>for: 该研究旨在使用声音数据预测抑郁风险。</li>
<li>methods: 该研究使用了TPOT自动Machine学习工具选择最佳机器学习算法，并使用K-最近邻准确分类器进行预测。</li>
<li>results: 选择的模型在预测抑郁风险中表现出色（准确率0.98，报告率0.93，F1评分0.96）。这些发现可能导致开发识别抑郁风险的工具，以便通过AI驱动的聊天机器人进行初步检测。<details>
<summary>Abstract</summary>
Telehealth is a valuable tool for primary health care (PHC), where depression is a common condition. PHC is the first point of contact for most people with depression, but about 25% of diagnoses made by PHC physicians are inaccurate. Many other barriers also hinder depression detection and treatment in PHC. Artificial intelligence (AI) may help reduce depression misdiagnosis in PHC and improve overall diagnosis and treatment outcomes. Telehealth consultations often have video issues, such as poor connectivity or dropped calls. Audio-only telehealth is often more practical for lower-income patients who may lack stable internet connections. Thus, our study focused on using audio data to predict depression risk. The objectives were to: 1) Collect audio data from 24 people (12 with depression and 12 without mental health or major health condition diagnoses); 2) Build a machine learning model to predict depression risk. TPOT, an autoML tool, was used to select the best machine learning algorithm, which was the K-nearest neighbors classifier. The selected model had high performance in classifying depression risk (Precision: 0.98, Recall: 0.93, F1-Score: 0.96). These findings may lead to a range of tools to help screen for and treat depression. By developing tools to detect depression risk, patients can be routed to AI-driven chatbots for initial screenings. Partnerships with a range of stakeholders are crucial to implementing these solutions. Moreover, ethical considerations, especially around data privacy and potential biases in AI models, need to be at the forefront of any AI-driven intervention in mental health care.
</details>
<details>
<summary>摘要</summary>
电健康是一种有价值的工具 для基础健康护理（PHC）， где抑郁是一种常见的疾病。PHC是大多数抑郁患者的第一个接触点，但约25%的诊断由PHC医生进行的是不正确的。许多其他的障碍也妨碍了抑郁检测和治疗在PHC中。人工智能（AI）可能可以减少在PHC中的抑郁误诊和提高总诊断和治疗结果。电健康咨询经常会出现视频问题，如互联网络不稳定或掉线问题。对于低收入患者来说，音频电健康更加实用，因为他们可能缺乏稳定的互联网连接。因此，我们的研究专注于使用音频数据预测抑郁风险。研究的目标是：1. 收集24名参与者的音频数据（12名抑郁患者和12名没有心理或重要健康状况诊断的人）。2. 使用自动Machine Learning（autoML）工具TPOT选择最佳Machine Learning算法，选择的是K-最近邻准确分类算法。选择的模型在识别抑郁风险方面表现出色（准确率0.98，感知率0.93，F1分数0.96）。这些发现可能导致一系列用于检测和治疗抑郁的工具的开发。通过开发检测抑郁风险的工具，患者可以被导向由AI驱动的聊天机器人进行初步检查。与多个各类投资者合作是实现这些解决方案的关键。此外，在AI驱动的医疗干预中，优先考虑数据隐私和可能存在的AI模型偏见等伦理考虑。
</details></li>
</ul>
<hr>
<h2 id="Intelligent-Software-Tooling-for-Improving-Software-Development"><a href="#Intelligent-Software-Tooling-for-Improving-Software-Development" class="headerlink" title="Intelligent Software Tooling for Improving Software Development"></a>Intelligent Software Tooling for Improving Software Development</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10921">http://arxiv.org/abs/2310.10921</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Nathan Cooper</li>
<li>for: 本研究旨在利用深度学习技术来提高软件开发过程。</li>
<li>methods: 本研究使用深度学习技术来处理大量的不结构化软件工程文档。</li>
<li>results: 研究发现，通过使用深度学习技术可以提高软件开发过程的效率和质量。<details>
<summary>Abstract</summary>
Software has eaten the world with many of the necessities and quality of life services people use requiring software. Therefore, tools that improve the software development experience can have a significant impact on the world such as generating code and test cases, detecting bugs, question and answering, etc., The success of Deep Learning (DL) over the past decade has shown huge advancements in automation across many domains, including Software Development processes. One of the main reasons behind this success is the availability of large datasets such as open-source code available through GitHub or image datasets of mobile Graphical User Interfaces (GUIs) with RICO and ReDRAW to be trained on. Therefore, the central research question my dissertation explores is: In what ways can the software development process be improved through leveraging DL techniques on the vast amounts of unstructured software engineering artifacts?
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="NuclearQA-A-Human-Made-Benchmark-for-Language-Models-for-the-Nuclear-Domain"><a href="#NuclearQA-A-Human-Made-Benchmark-for-Language-Models-for-the-Nuclear-Domain" class="headerlink" title="NuclearQA: A Human-Made Benchmark for Language Models for the Nuclear Domain"></a>NuclearQA: A Human-Made Benchmark for Language Models for the Nuclear Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10920">http://arxiv.org/abs/2310.10920</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pnnl/expert2">https://github.com/pnnl/expert2</a></li>
<li>paper_authors: Anurag Acharya, Sai Munikoti, Aaron Hellinger, Sara Smith, Sridevi Wagle, Sameera Horawalavithana</li>
<li>for: 本研究的目的是为了评估语言模型在核领域的表现，并提供一个专门为核领域设计的语言模型评价 benchmark。</li>
<li>methods: 本研究使用了一个人工制定的核领域语言模型评价 benchmark，包含100道由专家设计的问题，以测试语言模型的能力。 研究还提出了一种新的评价指标，以取代现有的评价指标，以便更好地评估语言模型的表现。</li>
<li>results: 实验表明，even the best LLMs 在本研究中表现不佳，这表明现有的 LLMs 在核领域具有科学知识的 gap。<details>
<summary>Abstract</summary>
As LLMs have become increasingly popular, they have been used in almost every field. But as the application for LLMs expands from generic fields to narrow, focused science domains, there exists an ever-increasing gap in ways to evaluate their efficacy in those fields. For the benchmarks that do exist, a lot of them focus on questions that don't require proper understanding of the subject in question. In this paper, we present NuclearQA, a human-made benchmark of 100 questions to evaluate language models in the nuclear domain, consisting of a varying collection of questions that have been specifically designed by experts to test the abilities of language models. We detail our approach and show how the mix of several types of questions makes our benchmark uniquely capable of evaluating models in the nuclear domain. We also present our own evaluation metric for assessing LLM's performances due to the limitations of existing ones. Our experiments on state-of-the-art models suggest that even the best LLMs perform less than satisfactorily on our benchmark, demonstrating the scientific knowledge gap of existing LLMs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Emergent-Mixture-of-Experts-Can-Dense-Pre-trained-Transformers-Benefit-from-Emergent-Modular-Structures"><a href="#Emergent-Mixture-of-Experts-Can-Dense-Pre-trained-Transformers-Benefit-from-Emergent-Modular-Structures" class="headerlink" title="Emergent Mixture-of-Experts: Can Dense Pre-trained Transformers Benefit from Emergent Modular Structures?"></a>Emergent Mixture-of-Experts: Can Dense Pre-trained Transformers Benefit from Emergent Modular Structures?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10908">http://arxiv.org/abs/2310.10908</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qiuzh20/emoe">https://github.com/qiuzh20/emoe</a></li>
<li>paper_authors: Zihan Qiu, Zeyu Huang, Jie Fu</li>
<li>for: 这篇论文主要研究如何使用隐式模块化结构来提高神经网络的泛化能力和学习效率。</li>
<li>methods: 作者使用了潜在的模块化结构，即emergent modularity，来改进标准预训练的变换器模型。他们构建了一种名为Emergent Mixture-of-Experts（EMoE）的模块化对手，无需增加参数量可以轻松地在下游调整中进行替换。</li>
<li>results: 广泛的实验（对1785个模型进行了调整）表明，EMoE可以有效地提高预训练模型的在域和离域泛化能力。此外，分析和缺失研究表明，EMoE可以减少负知识传递和对不同配置的稳定性。<details>
<summary>Abstract</summary>
Incorporating modular designs into neural networks demonstrates superior out-of-generalization, learning efficiency, etc. Existing modular neural networks are generally $\textit{explicit}$ because their modular architectures are pre-defined, and individual modules are expected to implement distinct functions. Conversely, recent works reveal that there exist $\textit{implicit}$ modular structures in standard pre-trained transformers, namely $\textit{Emergent Modularity}$. They indicate that such modular structures exhibit during the early pre-training phase and are totally spontaneous. However, most transformers are still treated as monolithic models with their modular natures underutilized. Therefore, given the excellent properties of explicit modular architecture, we explore $\textit{whether and how dense pre-trained transformers can benefit from emergent modular structures.}$ To study this question, we construct \textbf{E}mergent $\textbf{M}$ixture-$\textbf{o}$f-$\textbf{E}$xperts (EMoE). Without introducing additional parameters, EMoE can be seen as the modular counterpart of the original model and can be effortlessly incorporated into downstream tuning. Extensive experiments (we tune 1785 models) on various downstream tasks (vision and language) and models (22M to1.5B) demonstrate that EMoE effectively boosts in-domain and out-of-domain generalization abilities. Further analysis and ablation study suggest that EMoE mitigates negative knowledge transfer and is robust to various configurations. Code is available at \url{https://github.com/qiuzh20/EMoE}
</details>
<details>
<summary>摘要</summary>
使用模块化设计在神经网络中表现出色的特点，包括更好的泛化性、学习效率等。现有的模块化神经网络通常是显式的，即其模块化结构是预先定义的，每个模块需要实现特定的功能。然而，latest works表明，标准预训练变换器中存在隐式的模块结构，称为“ Emergent Modularity”。这些模块结构在初始预训练阶段自然地出现，并且是 Totally Spontaneous。然而，大多数变换器仍然被视为坚实的模块化模型，其中模块性未得到利用。因此，我们提出了以下问题：whether and how dense pre-trained transformers can benefit from emergent modular structures。为了研究这个问题，我们构建了Emergent Mixture-of-Experts (EMoE)。EMoE可以看作原始模型的模块化对手，而且可以无需添加参数进行下游调整。我们对多种下游任务（视觉和语言）和模型（22M到1.5B）进行了广泛的实验，结果表明，EMoE可以有效地提高域内和域外泛化能力。进一步的分析和减少研究表明，EMoE可以 mitigate negative knowledge transfer和是对多种配置的稳定。代码可以在 \url{https://github.com/qiuzh20/EMoE} 上下载。
</details></li>
</ul>
<hr>
<h2 id="Instilling-Inductive-Biases-with-Subnetworks"><a href="#Instilling-Inductive-Biases-with-Subnetworks" class="headerlink" title="Instilling Inductive Biases with Subnetworks"></a>Instilling Inductive Biases with Subnetworks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10899">http://arxiv.org/abs/2310.10899</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rock-z/instilling-inductiva-bias">https://github.com/rock-z/instilling-inductiva-bias</a></li>
<li>paper_authors: Enyan Zhang, Michael A. Lepori, Ellie Pavlick</li>
<li>for: 这篇论文的目的是探讨一种新的机器学习方法，即子任务抽象（Subtask Induction），可以让模型更好地控制其行为。</li>
<li>methods: 这篇论文使用的方法是基于已经训练过的模型中找出一个功能子网络，并使用这个子网络来塑造模型的启发性。</li>
<li>results: 论文的两个实验表明，使用子任务抽象可以减少模型需要的训练数据量，并且可以成功地塑造模型采用人类化的形态偏好。<details>
<summary>Abstract</summary>
Despite the recent success of artificial neural networks on a variety of tasks, we have little knowledge or control over the exact solutions these models implement. Instilling inductive biases -- preferences for some solutions over others -- into these models is one promising path toward understanding and controlling their behavior. Much work has been done to study the inherent inductive biases of models and instill different inductive biases through hand-designed architectures or carefully curated training regimens. In this work, we explore a more mechanistic approach: Subtask Induction. Our method discovers a functional subnetwork that implements a particular subtask within a trained model and uses it to instill inductive biases towards solutions utilizing that subtask. Subtask Induction is flexible and efficient, and we demonstrate its effectiveness with two experiments. First, we show that Subtask Induction significantly reduces the amount of training data required for a model to adopt a specific, generalizable solution to a modular arithmetic task. Second, we demonstrate that Subtask Induction successfully induces a human-like shape bias while increasing data efficiency for convolutional and transformer-based image classification models.
</details>
<details>
<summary>摘要</summary>
尽管人工神经网络在各种任务上表现出色，但我们对这些模型实际解决方案的具体知识和控制仍然很少。尝试通过填充模型中的预设偏见（preferences for certain solutions）来理解和控制它们的行为是一个有前途的路径。许多研究已经专注于研究模型的内生预设偏见和通过手动设计architecture或特制的训练程序来实现不同的预设偏见。在这个工作中，我们探索了一种更机制化的方法：子任务抽象。我们的方法可以在已经训练过的模型中找到一个功能子网络，该子网络实现了特定的子任务，然后使用这个子任务来填充模型中的预设偏见。子任务抽象是高效和灵活的，我们通过两个实验证明其效果。首先，我们表明了子任务抽象可以减少模型学习数据量，使模型采用特定、普适的解决方案。其次，我们成功地在基于卷积和变换器的图像分类模型中实现了人类化形态偏见，同时提高了数据效率。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/17/cs.AI_2023_10_17/" data-id="clogxf3l0005z5xra4t454nlc" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/17/cs.CL_2023_10_17/" class="article-date">
  <time datetime="2023-10-17T11:00:00.000Z" itemprop="datePublished">2023-10-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/17/cs.CL_2023_10_17/">cs.CL - 2023-10-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="BasahaCorpus-An-Expanded-Linguistic-Resource-for-Readability-Assessment-in-Central-Philippine-Languages"><a href="#BasahaCorpus-An-Expanded-Linguistic-Resource-for-Readability-Assessment-in-Central-Philippine-Languages" class="headerlink" title="BasahaCorpus: An Expanded Linguistic Resource for Readability Assessment in Central Philippine Languages"></a>BasahaCorpus: An Expanded Linguistic Resource for Readability Assessment in Central Philippine Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11584">http://arxiv.org/abs/2310.11584</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/imperialite/basahacorpus-hierarchicalcrosslingualara">https://github.com/imperialite/basahacorpus-hierarchicalcrosslingualara</a></li>
<li>paper_authors: Joseph Marvin Imperial, Ekaterina Kochmar</li>
<li>for: This paper is written for the purpose of improving the performance of automatic readability assessment (ARA) models in lower resource languages in the Philippines.</li>
<li>methods: The paper uses a corpus of short fictional narratives written in Hiligaynon, Minasbate, Karay-a, and Rinconada to train ARA models using surface-level, syllable-pattern, and n-gram overlap features. The paper also proposes a new hierarchical cross-lingual modeling approach that takes advantage of a language’s placement in the family tree to increase the amount of available training data.</li>
<li>results: The study yields encouraging results that support previous work showcasing the efficacy of cross-lingual models in low-resource settings, as well as similarities in highly informative linguistic features for mutually intelligible languages.<details>
<summary>Abstract</summary>
Current research on automatic readability assessment (ARA) has focused on improving the performance of models in high-resource languages such as English. In this work, we introduce and release BasahaCorpus as part of an initiative aimed at expanding available corpora and baseline models for readability assessment in lower resource languages in the Philippines. We compiled a corpus of short fictional narratives written in Hiligaynon, Minasbate, Karay-a, and Rinconada -- languages belonging to the Central Philippine family tree subgroup -- to train ARA models using surface-level, syllable-pattern, and n-gram overlap features. We also propose a new hierarchical cross-lingual modeling approach that takes advantage of a language's placement in the family tree to increase the amount of available training data. Our study yields encouraging results that support previous work showcasing the efficacy of cross-lingual models in low-resource settings, as well as similarities in highly informative linguistic features for mutually intelligible languages.
</details>
<details>
<summary>摘要</summary>
当前研究自动可读性评估（ARA）主要集中在高资源语言 such as English 中进行改进模型性能。在这项工作中，我们介绍并发布 BasahaCorpus，是一项旨在扩大可用 corpora 和基线模型 для可读性评估的低资源语言在菲律宾的 iniciativa。我们编译了中央菲律宾语族 subgroup 中的希利加纳、民达、卡拉雅和林康达语言的短篇小说，以用于训练 ARA 模型 surface-level、 syllable-pattern 和 n-gram 重叠特征。我们还提出了一种新的 hierarchical cross-lingual 模型方法，利用语言在语族树中的位置，以增加可用训练数据。我们的研究得到了鼓舞人心的结果，支持先前的研究表明在低资源设置中， crossed-lingual 模型具有可读性评估的效果，以及同属语言之间的高度相似性特征。
</details></li>
</ul>
<hr>
<h2 id="What-is-a-good-question-Task-oriented-asking-with-fact-level-masking"><a href="#What-is-a-good-question-Task-oriented-asking-with-fact-level-masking" class="headerlink" title="What is a good question? Task-oriented asking with fact-level masking"></a>What is a good question? Task-oriented asking with fact-level masking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11571">http://arxiv.org/abs/2310.11571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew Toles, Yukun Huang, Zhou Yu, Luis Gravano</li>
<li>for: 这篇论文主要是关于问答推理任务中的协作问题，即如何让机器人能够咨询用户并采集有用的信息。</li>
<li>methods: 该论文提出了一种自然语言任务协作问答（TOA）定义和框架，以及一种基于实际事实遮盖（FLM）的自动生成问答数据集的方法。</li>
<li>results: 实际试验表明，当前的零shot语言模型在完成TOA任务时表现不佳，与人工标注者相比。这些结果表明可以使用FLM数据集和TOA框架来训练和评估更好的TOA模型。<details>
<summary>Abstract</summary>
Asking questions is an important element of real-life collaboration on reasoning tasks like question answering. For example, a legal assistant chatbot may be unable to make accurate recommendations without specific information on the user's circumstances. However, large language models are usually deployed to solve reasoning tasks directly without asking follow-up questions to the user or third parties. We term this problem task-oriented asking (TOA). Zero-shot chat models can perform TOA, but their training is primarily based on next-token prediction rather than whether questions contribute to successful collaboration. To enable the training and evaluation of TOA models, we present a definition and framework for natural language task-oriented asking, the problem of generating questions that result in answers useful for a reasoning task. We also present fact-level masking (FLM), a procedure for converting natural language datasets into self-supervised TOA datasets by omitting particular critical facts. Finally, we generate a TOA dataset from the HotpotQA dataset using FLM and evaluate several zero-shot language models on it. Our experiments show that current zero-shot models struggle to ask questions that retrieve useful information, as compared to human annotators. These results demonstrate an opportunity to use FLM datasets and the TOA framework to train and evaluate better TOA models.
</details>
<details>
<summary>摘要</summary>
实际协作中的问题询问是解决理性任务的重要元素，例如法律助手聊天机器人可能无法提供精确的建议 без specific 用户情况信息。然而，大型语言模型通常会直接解决理性任务，而不是询问使用者或第三方的询问。我们称这问题为任务 Orientated Asking（TOA）。零开始聊天模型可以进行 TOA，但它们的训练主要基于下一个字符预测，而不是 Whether 询问对于成功协作有用。为了实现和评估 TOA 模型的训练，我们提出了自然语言任务 Orientated Asking 的定义和框架，以及 факт阶掩蔽（FLM），将自然语言数据集转换为自主监督的 TOA 数据集，并在这些数据集上评估了多个零开始语言模型。我们的实验结果显示，现有的零开始模型在对于有用信息的询问方面表现不佳，与人工标注师相比。这些结果显示了使用 FLM 数据集和 TOA 框架可以训练和评估更好的 TOA 模型。
</details></li>
</ul>
<hr>
<h2 id="Personalized-Soups-Personalized-Large-Language-Model-Alignment-via-Post-hoc-Parameter-Merging"><a href="#Personalized-Soups-Personalized-Large-Language-Model-Alignment-via-Post-hoc-Parameter-Merging" class="headerlink" title="Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging"></a>Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11564">http://arxiv.org/abs/2310.11564</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/joeljang/rlphf">https://github.com/joeljang/rlphf</a></li>
<li>paper_authors: Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, Prithviraj Ammanabrolu</li>
<li>for: 本研究旨在使用人工反馈来适应大语言模型（LLM）与多个个人偏好的多目标强化学习（MORL）问题。</li>
<li>methods: 本研究使用了分解个人偏好为多个维度的方法，并在分布式环境中独立进行了这些维度的快速训练。在训练后，parameters可以通过合并 Parameter Merging 技术来有效地组合。</li>
<li>results: 相比强大的单个目标基eline，我们的方法可以实现个性化的偏好对齐。我们的实验结果表明，RLPHF可以有效地适应多个个人偏好，并且可以在不同的应用场景中提供个性化的结果。<details>
<summary>Abstract</summary>
While Reinforcement Learning from Human Feedback (RLHF) aligns Large Language Models (LLMs) with general, aggregate human preferences, it is suboptimal for learning diverse, individual perspectives. In this work, we study Reinforcement Learning from Personalized Human Feedback (RLPHF) problem, wherein LLMs are aligned to multiple (sometimes conflicting) preferences by modeling alignment as a Multi-Objective Reinforcement Learning (MORL) problem. Compared to strong single-objective baselines, we show that we can achieve personalized alignment by decomposing preferences into multiple dimensions. These dimensions are defined based on personalizations that are declared as desirable by the user. In this work, we show that they can be efficiently trained independently in a distributed manner and combined effectively post-hoc through parameter merging. The code is available at https://github.com/joeljang/RLPHF.
</details>
<details>
<summary>摘要</summary>
While Reinforcement Learning from Human Feedback (RLHF) aligns Large Language Models (LLMs) with general, aggregate human preferences, it is suboptimal for learning diverse, individual perspectives. In this work, we study Reinforcement Learning from Personalized Human Feedback (RLPHF) problem, wherein LLMs are aligned to multiple (sometimes conflicting) preferences by modeling alignment as a Multi-Objective Reinforcement Learning (MORL) problem. Compared to strong single-objective baselines, we show that we can achieve personalized alignment by decomposing preferences into multiple dimensions. These dimensions are defined based on personalizations that are declared as desirable by the user. In this work, we show that they can be efficiently trained independently in a distributed manner and combined effectively post-hoc through parameter merging. The code is available at https://github.com/joeljang/RLPHF.Here's the translation in Simplified Chinese:While Reinforcement Learning from Human Feedback (RLHF) aligns Large Language Models (LLMs) with general, aggregate human preferences, it is suboptimal for learning diverse, individual perspectives. In this work, we study Reinforcement Learning from Personalized Human Feedback (RLPHF) problem, wherein LLMs are aligned to multiple (sometimes conflicting) preferences by modeling alignment as a Multi-Objective Reinforcement Learning (MORL) problem. Compared to strong single-objective baselines, we show that we can achieve personalized alignment by decomposing preferences into multiple dimensions. These dimensions are defined based on personalizations that are declared as desirable by the user. In this work, we show that they can be efficiently trained independently in a distributed manner and combined effectively post-hoc through parameter merging. The code is available at https://github.com/joeljang/RLPHF.
</details></li>
</ul>
<hr>
<h2 id="Multi-stage-Large-Language-Model-Correction-for-Speech-Recognition"><a href="#Multi-stage-Large-Language-Model-Correction-for-Speech-Recognition" class="headerlink" title="Multi-stage Large Language Model Correction for Speech Recognition"></a>Multi-stage Large Language Model Correction for Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11532">http://arxiv.org/abs/2310.11532</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Pu, Thai-Son Nguyen, Sebastian Stüker</li>
<li>for: 提高竞争性语音识别系统的性能</li>
<li>methods: 使用大语言模型（LLM）进行语音识别系统的改进</li>
<li>results: 实验结果显示，提议的方法可以在多个测试频谱上实现10%~20%的相对改进，与一个竞争性ASR系统相比<details>
<summary>Abstract</summary>
In this paper, we investigate the usage of large language models (LLMs) to improve the performance of competitive speech recognition systems. Different from traditional language models that focus on one single data domain, the rise of LLMs brings us the opportunity to push the limit of state-of-the-art ASR performance, and at the same time to achieve higher robustness and generalize effectively across multiple domains. Motivated by this, we propose a novel multi-stage approach to combine traditional language model re-scoring and LLM prompting. Specifically, the proposed method has two stages: the first stage uses a language model to re-score an N-best list of ASR hypotheses and run a confidence check; The second stage uses prompts to a LLM to perform ASR error correction on less confident results from the first stage. Our experimental results demonstrate the effectiveness of the proposed method by showing a 10% ~ 20% relative improvement in WER over a competitive ASR system -- across multiple test domains.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了使用大语言模型（LLM）提高竞争性语音识别系统的性能。与传统语言模型不同，LLMs允许我们在多个数据域之间进行跨领域的学习和泛化，从而提高ASR性能和Robustness。我们提出了一种新的多阶段方法， combining traditional language model re-scoring和LLM prompting。这种方法包括两个阶段：第一阶段使用语言模型对N-best列表的ASR假设进行重新分数和信任检查；第二阶段使用提示来让LLM进行错误纠正。我们的实验结果表明，提案的方法可以提高竞争性ASR系统的WER表现，在多个测试领域中显示10%~20%的相对改善。
</details></li>
</ul>
<hr>
<h2 id="Automatic-News-Summerization"><a href="#Automatic-News-Summerization" class="headerlink" title="Automatic News Summerization"></a>Automatic News Summerization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11520">http://arxiv.org/abs/2310.11520</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Kavach Dheer, Arpit Dhankhar</li>
<li>for: 这个研究论文主要是为了比较EXTRACTIVE和ABSTRACTIVE方法在新闻文本摘要方面的表现。</li>
<li>methods: 这个研究使用了CNN-Daily Mail dataset，这个 dataset包含了新闻文章和人工生成的参考摘要。研究使用了ROUGE分数来评估生成的摘要质量。</li>
<li>results: 经过评估后，研究人员选择了最佳性能的模型，并将其集成到了一个web应用程序中，以评估它们在实际应用中的表现和用户体验。<details>
<summary>Abstract</summary>
Natural Language Processing is booming with its applications in the real world, one of which is Text Summarization for large texts including news articles. This research paper provides an extensive comparative evaluation of extractive and abstractive approaches for news text summarization, with an emphasis on the ROUGE score analysis. The study employs the CNN-Daily Mail dataset, which consists of news articles and human-generated reference summaries. The evaluation employs ROUGE scores to assess the efficacy and quality of generated summaries. After Evaluation, we integrate the best-performing models on a web application to assess their real-world capabilities and user experience.
</details>
<details>
<summary>摘要</summary>
自然语言处理技术在现实世界中得到了广泛应用，其中之一是文本概要化，特别是对新闻文章进行概要。本研究论文进行了对抽取和抽象方法的比较评估，强调ROUGE分数分析。研究使用了CNN-Daily Mail dataset，该 dataset包括新闻文章和人工生成的参考概要。评估使用ROUGE分数评估生成的概要质量。经评估后，我们将最佳表现的模型集成到了网站应用程序中，以评估它们在实际应用中的能力和用户体验。
</details></li>
</ul>
<hr>
<h2 id="VeRA-Vector-based-Random-Matrix-Adaptation"><a href="#VeRA-Vector-based-Random-Matrix-Adaptation" class="headerlink" title="VeRA: Vector-based Random Matrix Adaptation"></a>VeRA: Vector-based Random Matrix Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11454">http://arxiv.org/abs/2310.11454</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dawid Jan Kopiczko, Tijmen Blankevoort, Yuki Markus Asano</li>
<li>For: 降低大型语言模型训练参数数量，并在多个用户或任务适配模型中进行多个适配。* Methods: 使用单个低级别矩阵和学习小扩展矩阵来减少训练参数数量，同时保持与LoRA相同的性能。* Results: 在GLUE和E2E测试集上实现同LoRA相同的性能，并在 instruciton-following 任务中使用Llama2 7B模型，只需1.4M参数。<details>
<summary>Abstract</summary>
Low-rank adapation (LoRA) is a popular method that reduces the number of trainable parameters when finetuning large language models, but still faces acute storage challenges when scaling to even larger models or deploying numerous per-user or per-task adapted models. In this work, we present Vector-based Random Matrix Adaptation (VeRA), which reduces the number of trainable parameters by 10x compared to LoRA, yet maintains the same performance. It achieves this by using a single pair of low-rank matrices shared across all layers and learning small scaling vectors instead. We demonstrate its effectiveness on the GLUE and E2E benchmarks, and show its application in instruction-following with just 1.4M parameters using the Llama2 7B model.
</details>
<details>
<summary>摘要</summary>
低阶 adaptive（LoRA）是一种受欢迎的方法，可以降低训练可变参数数量，但仍然面临着扩大模型或部署多个用户或任务特定 adapted 模型时的严重存储挑战。在这种工作中，我们提出了 вектор基的随机矩阵适应（VeRA），它可以在与 LoRA 相比下减少训练可变参数数量达到 10 倍，同时保持性能不变。它实现这一点通过使用所有层共享的低阶矩阵对和学习小扩张 вектор而做。我们在 GLUE 和 E2E 测试上证明了它的有效性，并在使用 Llama2 7B 模型进行 instrucion-following  tasks 中只需要 1.4M 参数。
</details></li>
</ul>
<hr>
<h2 id="BitNet-Scaling-1-bit-Transformers-for-Large-Language-Models"><a href="#BitNet-Scaling-1-bit-Transformers-for-Large-Language-Models" class="headerlink" title="BitNet: Scaling 1-bit Transformers for Large Language Models"></a>BitNet: Scaling 1-bit Transformers for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11453">http://arxiv.org/abs/2310.11453</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kyegomez/BitNet">https://github.com/kyegomez/BitNet</a></li>
<li>paper_authors: Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, Furu Wei</li>
<li>for: 这个研究是为了开发一个可扩展且稳定的1比特transformer架构，以便在大型语言模型中实现高效性和可持续性。</li>
<li>methods: 这个研究使用了BitLinear层来将1比特量化的 weights 训练出来，并且提出了一个可替换的drop-in替代方案。</li>
<li>results: 实验结果显示，BitNet可以与现有的8比特量化方法和FP16 transformer基准相比，在语言模型化上达到竞争性的表现，同时具有较小的内存库存和能源消耗。此外，BitNet显示了与全精度transformer相似的扩展法则， suggesting its potential for effective scaling to even larger language models while maintaining efficiency and performance benefits.<details>
<summary>Abstract</summary>
The increasing size of large language models has posed challenges for deployment and raised concerns about environmental impact due to high energy consumption. In this work, we introduce BitNet, a scalable and stable 1-bit Transformer architecture designed for large language models. Specifically, we introduce BitLinear as a drop-in replacement of the nn.Linear layer in order to train 1-bit weights from scratch. Experimental results on language modeling show that BitNet achieves competitive performance while substantially reducing memory footprint and energy consumption, compared to state-of-the-art 8-bit quantization methods and FP16 Transformer baselines. Furthermore, BitNet exhibits a scaling law akin to full-precision Transformers, suggesting its potential for effective scaling to even larger language models while maintaining efficiency and performance benefits.
</details>
<details>
<summary>摘要</summary>
大型语言模型的增加会带来部署的挑战和环境影响的关注，由于高能consumption。在这个工作中，我们介绍BitNet，一个可扩展和稳定的1比特Transformer架构，设计用于大型语言模型。具体来说，我们介绍BitLinear，用于从零开始训练1比特的 weights的替换层。实验结果显示，BitNet在语言模型化方面实现了竞争性的性能，并substantially reducingmemory尺度和能源consumption，相比于现有的8比特量化方法和FP16 Transformer基准。此外，BitNet展示了与全精度Transformer一样的扩展律，表明它的潜在可以实现有效的扩展到更大的语言模型，保持效率和性能优势。
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Study-of-Translation-Hypothesis-Ensembling-with-Large-Language-Models"><a href="#An-Empirical-Study-of-Translation-Hypothesis-Ensembling-with-Large-Language-Models" class="headerlink" title="An Empirical Study of Translation Hypothesis Ensembling with Large Language Models"></a>An Empirical Study of Translation Hypothesis Ensembling with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11430">http://arxiv.org/abs/2310.11430</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deep-spin/translation-hypothesis-ensembling">https://github.com/deep-spin/translation-hypothesis-ensembling</a></li>
<li>paper_authors: António Farinhas, José G. C. de Souza, André F. T. Martins</li>
<li>for: 这 paper  investigate LLM-based machine translation 的质量可以通过集成假设来提高。</li>
<li>methods: 这 paper 使用多种ensemble技术，包括多个提示、温度-based sampling 和 beam search，来生成假设。</li>
<li>results: 这 paper 的结果表明，MBR decoding 是一个非常有效的方法，可以使用少量的样本提高翻译质量，而且制定提示的调整对假设的多样性和样本温度具有强烈的影响。<details>
<summary>Abstract</summary>
Large language models (LLMs) are becoming a one-fits-many solution, but they sometimes hallucinate or produce unreliable output. In this paper, we investigate how hypothesis ensembling can improve the quality of the generated text for the specific problem of LLM-based machine translation. We experiment with several techniques for ensembling hypotheses produced by LLMs such as ChatGPT, LLaMA, and Alpaca. We provide a comprehensive study along multiple dimensions, including the method to generate hypotheses (multiple prompts, temperature-based sampling, and beam search) and the strategy to produce the final translation (instruction-based, quality-based reranking, and minimum Bayes risk (MBR) decoding). Our results show that MBR decoding is a very effective method, that translation quality can be improved using a small number of samples, and that instruction tuning has a strong impact on the relation between the diversity of the hypotheses and the sampling temperature.
</details>
<details>
<summary>摘要</summary>
Translation into Simplified Chinese:大型语言模型（LLM）正在成为一个一size-fits-all解决方案，但它们有时会幻想或生成不可靠的输出。在这篇论文中，我们调查了如何使用假设集成以提高由 LLM 生成的文本质量，特别是在机器翻译问题上。我们对各种生成假设的技术进行了实验，包括 ChatGPT、LLaMA 和 Alpaca。我们提供了多维度的研究，包括生成假设的方法（多个提示、温度基本抽样和搜索杆）以及生成翻译的策略（指令基本、质量基本重新排序和最小极值风险（MBR）解oding）。我们的结果显示了 MBR 解oding 是一个非常有效的方法，可以通过一小数量的样本提高翻译质量，并且指令调整对假设多样性和抽样温度之间的关系具有很强的影响。
</details></li>
</ul>
<hr>
<h2 id="Robust-Wake-Up-Word-Detection-by-Two-stage-Multi-resolution-Ensembles"><a href="#Robust-Wake-Up-Word-Detection-by-Two-stage-Multi-resolution-Ensembles" class="headerlink" title="Robust Wake-Up Word Detection by Two-stage Multi-resolution Ensembles"></a>Robust Wake-Up Word Detection by Two-stage Multi-resolution Ensembles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11379">http://arxiv.org/abs/2310.11379</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ferugit/iterative-pseudo-forced-alignment-ctc">https://github.com/ferugit/iterative-pseudo-forced-alignment-ctc</a></li>
<li>paper_authors: Fernando López, Jordi Luque, Carlos Segura, Pablo Gómez</li>
<li>for: 本研究旨在提高语音界面上的唤醒词检测精度、能效性和速度。</li>
<li>methods: 该研究使用了两个阶段的检测方法，包括多分辨率的数据增强和服务器端的模型集成。它还使用了一个轻量级的设备上模型和一个云端的验证模型，以优化两个运行点。</li>
<li>results: 研究发现，使用不同的参数配置和多种语音分类器可以提高检测精度和减少干扰。特别是，提出的集成模型在所有噪声条件下都表现出优于 stronger 分类器。<details>
<summary>Abstract</summary>
Voice-based interfaces rely on a wake-up word mechanism to initiate communication with devices. However, achieving a robust, energy-efficient, and fast detection remains a challenge. This paper addresses these real production needs by enhancing data with temporal alignments and using detection based on two phases with multi-resolution. It employs two models: a lightweight on-device model for real-time processing of the audio stream and a verification model on the server-side, which is an ensemble of heterogeneous architectures that refine detection. This scheme allows the optimization of two operating points. To protect privacy, audio features are sent to the cloud instead of raw audio. The study investigated different parametric configurations for feature extraction to select one for on-device detection and another for the verification model. Furthermore, thirteen different audio classifiers were compared in terms of performance and inference time. The proposed ensemble outperforms our stronger classifier in every noise condition.
</details>
<details>
<summary>摘要</summary>
声音基于界面依赖于唤醒词机制来与设备进行通信。然而，实现高效、能效、快速的检测仍然是一大挑战。本文通过增强数据的时间对齐和使用两个阶段多分辨率检测来解决这些生产环境需求。它使用了两个模型：一个轻量级在设备上进行实时处理的音频流模型，以及服务器端的验证模型，这是一个多种不同架构的 ensemble 模型，用于精度检测。这种方案允许优化两个运行点。为了保护隐私，音频特征被发送到云端而不是原始音频。研究中试用了不同的参数配置来进行特征提取，以便在设备上进行检测和在服务器端进行验证。此外，本文比较了13种不同的声音分类器，并评估了它们的性能和推理时间。提议的ensemble在每种噪音条件下都超过了我们更强的分类器。
</details></li>
</ul>
<hr>
<h2 id="DialogueLLM-Context-and-Emotion-Knowledge-Tuned-LLaMA-Models-for-Emotion-Recognition-in-Conversations"><a href="#DialogueLLM-Context-and-Emotion-Knowledge-Tuned-LLaMA-Models-for-Emotion-Recognition-in-Conversations" class="headerlink" title="DialogueLLM: Context and Emotion Knowledge-Tuned LLaMA Models for Emotion Recognition in Conversations"></a>DialogueLLM: Context and Emotion Knowledge-Tuned LLaMA Models for Emotion Recognition in Conversations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11374">http://arxiv.org/abs/2310.11374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yazhou Zhang, Mengyao Wang, Prayag Tiwari, Qiuchi Li, Benyou Wang, Jing Qin</li>
<li>for: 提高对话情感认知的模型性能，特别是在自然语言生成中。</li>
<li>methods:  fine-tuning LLaMA模型，使用多modal信息作为补充知识。</li>
<li>results: 在三个对话情感认知 benchmark 数据集上提供了比基线和其他 SOTA LLM 更高的性能。<details>
<summary>Abstract</summary>
Large language models (LLMs) and their variants have shown extraordinary efficacy across numerous downstream natural language processing (NLP) tasks, which has presented a new vision for the development of NLP. Despite their remarkable performance in natural language generating (NLG), LLMs lack a distinct focus on the emotion understanding domain. As a result, using LLMs for emotion recognition may lead to suboptimal and inadequate precision. Another limitation of LLMs is that they are typical trained without leveraging multi-modal information. To overcome these limitations, we propose DialogueLLM, a context and emotion knowledge tuned LLM that is obtained by fine-tuning LLaMA models with 13,638 multi-modal (i.e., texts and videos) emotional dialogues. The visual information is considered as the supplementary knowledge to construct high-quality instructions. We offer a comprehensive evaluation of our proposed model on three benchmarking emotion recognition in conversations (ERC) datasets and compare the results against the SOTA baselines and other SOTA LLMs. Additionally, DialogueLLM-7B can be easily trained using LoRA on a 40GB A100 GPU in 5 hours, facilitating reproducibility for other researchers.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）和其变体在多种自然语言处理（NLP）任务中表现出色，带来了一个新的视野 для NLP的发展。尽管它们在自然语言生成（NLG）方面表现出色，但LLM对感情理解领域没有明确的注意力，使用LLM进行感情识别可能会导致不足和不充分的精度。另外，LLM通常不会利用多modal信息进行训练。为了解决这些限制，我们提出了对话LLM，一个基于LLaMA模型的内容和感情知识调整的LLM，通过调整13,638种多modal（文本和影片）情感对话。影像信息被视为补充知识，用于建立高品质的指令。我们提供了三个benchmarking感情识别在对话（ERC）数据集的完整评估，并与基于SOTA和其他SOTA LLM的结果进行比较。此外，DialogueLLM-7B可以在5小时内使用LoRA在40GB A100 GPU上进行训练，便于其他研究人员的重现。
</details></li>
</ul>
<hr>
<h2 id="VECHR-A-Dataset-for-Explainable-and-Robust-Classification-of-Vulnerability-Type-in-the-European-Court-of-Human-Rights"><a href="#VECHR-A-Dataset-for-Explainable-and-Robust-Classification-of-Vulnerability-Type-in-the-European-Court-of-Human-Rights" class="headerlink" title="VECHR: A Dataset for Explainable and Robust Classification of Vulnerability Type in the European Court of Human Rights"></a>VECHR: A Dataset for Explainable and Robust Classification of Vulnerability Type in the European Court of Human Rights</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11368">http://arxiv.org/abs/2310.11368</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shanshan Xu, Leon Staufer, T. Y. S. S Santosh, Oana Ichim, Corina Heri, Matthias Grabmair</li>
<li>For: This paper is written to address the elusive concept of vulnerability at the European Court of Human Rights (ECtHR) and to provide a novel dataset (VECHR) for future research in this area.* Methods: The paper uses expert-annotated multi-label data to benchmark the performance of state-of-the-art models for vulnerability type classification and explanation rationale.* Results: The results show that the task of vulnerability classification is challenging, with lower prediction performance and limited agreement between models and experts. Additionally, the models have limited performance when dealing with out-of-domain (OOD) data.Here’s the information in Simplified Chinese text:* For: 这篇论文是为了解决欧洲人权法庭（ECtHR）中的抑降性概念，并提供一个新的数据集（VECHR）以便未来在这个领域进行研究。* Methods: 这篇论文使用专家标注的多标签数据来评估现有模型的表现，以便对抑降性类型分类和解释理由进行 benchlearning。* Results: 结果显示，抑降性分类任务具有较低的预测性能和专家和模型之间的有限的一致性。此外，模型对于非预期数据（OOD）的性能也有限。<details>
<summary>Abstract</summary>
Recognizing vulnerability is crucial for understanding and implementing targeted support to empower individuals in need. This is especially important at the European Court of Human Rights (ECtHR), where the court adapts Convention standards to meet actual individual needs and thus ensures effective human rights protection. However, the concept of vulnerability remains elusive at the ECtHR and no prior NLP research has dealt with it. To enable future research in this area, we present VECHR, a novel expert-annotated multi-label dataset comprising of vulnerability type classification and explanation rationale. We benchmark the performance of state-of-the-art models on VECHR from both prediction and explainability perspectives. Our results demonstrate the challenging nature of the task with lower prediction performance and limited agreement between models and experts. Further, we analyze the robustness of these models in dealing with out-of-domain (OOD) data and observe overall limited performance. Our dataset poses unique challenges offering significant room for improvement regarding performance, explainability, and robustness.
</details>
<details>
<summary>摘要</summary>
认识投降性是关键 для理解并实施targeted支持，以帮助个人需要。这对欧洲人权法庭（ECtHR）来说特别重要，因为法庭将会适应实际个人需求，从而确保人权保护的有效性。然而，投降性这个概念在ECtHR中仍然毫不准确，而且没有任何NLP研究过去关注过它。为了启动未来的研究，我们提供了VECHR，一个新的专家标注的多标签数据集，包括投降性类型分类和解释理由。我们对现有模型在VECHR上进行了测试和解释两个方面的性能评估。我们的结果表明这是一个复杂的任务，模型的预测性能较低，并且模型和专家之间的一致性很有限。此外，我们发现这些模型在非标准数据（OOD）上的性能有限。our dataset提供了一些独特的挑战，它们的性能、解释和Robustness在需要进一步改进。
</details></li>
</ul>
<hr>
<h2 id="Disentangling-the-Linguistic-Competence-of-Privacy-Preserving-BERT"><a href="#Disentangling-the-Linguistic-Competence-of-Privacy-Preserving-BERT" class="headerlink" title="Disentangling the Linguistic Competence of Privacy-Preserving BERT"></a>Disentangling the Linguistic Competence of Privacy-Preserving BERT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11363">http://arxiv.org/abs/2310.11363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Arnold, Nils Kemmerzell, Annika Schreiner</li>
<li>for: 本研究旨在透过文本层级的解释技术来探索对于文本隐私保护而导致的语言模型表现下降的原因。</li>
<li>methods: 本研究使用了一系列的解释技术来分析BERT模型在受到干扰前文本训练后内部表现的改变。</li>
<li>results: 实验结果显示，对于受到干扰前文本训练的BERT模型，内部表现之间的相似性减少了许多。通过询问任务来探索这种不相似性，发现文本层级的隐私保护对于词汇的地方性特征有影响，但是对于词汇之间的关系性却有所下降。<details>
<summary>Abstract</summary>
Differential Privacy (DP) has been tailored to address the unique challenges of text-to-text privatization. However, text-to-text privatization is known for degrading the performance of language models when trained on perturbed text. Employing a series of interpretation techniques on the internal representations extracted from BERT trained on perturbed pre-text, we intend to disentangle at the linguistic level the distortion induced by differential privacy. Experimental results from a representational similarity analysis indicate that the overall similarity of internal representations is substantially reduced. Using probing tasks to unpack this dissimilarity, we find evidence that text-to-text privatization affects the linguistic competence across several formalisms, encoding localized properties of words while falling short at encoding the contextual relationships between spans of words.
</details>
<details>
<summary>摘要</summary>
Diffusion Privacy (DP) 已经适应文本到文本隐私化的特殊挑战。然而，文本到文本隐私化知道会降低基于扰动文本训练的语言模型性能。通过对 BERT 在扰动预文本上提取的内部表示进行解释技术，我们意图在语言层次分离扰动所引起的损害。实验结果表明，总体内表示相似性substantially 降低。通过探索任务来抽取这种不同，我们发现了文本到文本隐私化对语言能力的影响，包括 encoding  lokalisierte 词性特征，但是缺乏 encoding 词语间关系的上下文。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Neural-Machine-Translation-with-Semantic-Units"><a href="#Enhancing-Neural-Machine-Translation-with-Semantic-Units" class="headerlink" title="Enhancing Neural Machine Translation with Semantic Units"></a>Enhancing Neural Machine Translation with Semantic Units</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11360">http://arxiv.org/abs/2310.11360</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ictnlp/su4mt">https://github.com/ictnlp/su4mt</a></li>
<li>paper_authors: Langlin Huang, Shuhao Gu, Zhuocheng Zhang, Yang Feng</li>
<li>for: 本研究旨在提高机器翻译模型的语义理解能力，通过模型语义单位内部的意义 integrate 多个 tokens 的 semantics。</li>
<li>methods: 本方法包括 Word Pair Encoding (WPE) 和 Attentive Semantic Fusion (ASF) 两部分。WPE 用于提取句子中的 semantic unit 边界，而 ASF 则用于将多个 subword 的 semantics 融合为单一 vector。</li>
<li>results: 实验结果表明，本方法可以有效地模型和利用句子中的语义单位信息，并与强基eline 比较。code 可以在 <a target="_blank" rel="noopener" href="https://github.com/ictnlp/SU4MT">https://github.com/ictnlp/SU4MT</a> 中找到。<details>
<summary>Abstract</summary>
Conventional neural machine translation (NMT) models typically use subwords and words as the basic units for model input and comprehension. However, complete words and phrases composed of several tokens are often the fundamental units for expressing semantics, referred to as semantic units. To address this issue, we propose a method Semantic Units for Machine Translation (SU4MT) which models the integral meanings of semantic units within a sentence, and then leverages them to provide a new perspective for understanding the sentence. Specifically, we first propose Word Pair Encoding (WPE), a phrase extraction method to help identify the boundaries of semantic units. Next, we design an Attentive Semantic Fusion (ASF) layer to integrate the semantics of multiple subwords into a single vector: the semantic unit representation. Lastly, the semantic-unit-level sentence representation is concatenated to the token-level one, and they are combined as the input of encoder. Experimental results demonstrate that our method effectively models and leverages semantic-unit-level information and outperforms the strong baselines. The code is available at https://github.com/ictnlp/SU4MT.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="QADYNAMICS-Training-Dynamics-Driven-Synthetic-QA-Diagnostic-for-Zero-Shot-Commonsense-Question-Answering"><a href="#QADYNAMICS-Training-Dynamics-Driven-Synthetic-QA-Diagnostic-for-Zero-Shot-Commonsense-Question-Answering" class="headerlink" title="QADYNAMICS: Training Dynamics-Driven Synthetic QA Diagnostic for Zero-Shot Commonsense Question Answering"></a>QADYNAMICS: Training Dynamics-Driven Synthetic QA Diagnostic for Zero-Shot Commonsense Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11303">http://arxiv.org/abs/2310.11303</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hkust-knowcomp/qadynamics">https://github.com/hkust-knowcomp/qadynamics</a></li>
<li>paper_authors: Haochen Shi, Weiqi Wang, Tianqing Fang, Baixuan Xu, Wenxuan Ding, Xin Liu, Yangqiu Song</li>
<li>for: 本研究的目的是提高Zero-shot Commonsense Question-Answering（QA）模型的普遍能力，使其能够理解更多的常识知识。</li>
<li>methods: 本研究使用了语言模型的训练动态学习方法，对每个QA对的训练动态进行分析，并从CSKBs中提取不含噪声的问答对。</li>
<li>results: 对比基eline，本研究的方法可以更好地提高QA模型的普遍能力，并且只需使用33%的Synthetic数据。专业评估也证明了我们的方法可以提高问答生成的质量。<details>
<summary>Abstract</summary>
Zero-shot commonsense Question-Answering (QA) requires models to reason about general situations beyond specific benchmarks. State-of-the-art approaches fine-tune language models on QA pairs constructed from CommonSense Knowledge Bases (CSKBs) to equip the models with more commonsense knowledge in a QA context. However, current QA synthesis protocols may introduce noise from the CSKBs and generate ungrammatical questions and false negative options, which impede the model's ability to generalize. To address these issues, we propose QADYNAMICS, a training dynamics-driven framework for QA diagnostics and refinement. Our approach analyzes the training dynamics of each QA pair at both the question level and option level, discarding machine-detectable artifacts by removing uninformative QA pairs and mislabeled or false-negative options. Extensive experiments demonstrate the effectiveness of our approach, which outperforms all baselines while using only 33% of the synthetic data, even including LLMs such as ChatGPT. Moreover, expert evaluations confirm that our framework significantly improves the quality of QA synthesis. Our codes and model checkpoints are available at https://github.com/HKUST-KnowComp/QaDynamics.
</details>
<details>
<summary>摘要</summary>
zero-shot常识问答（QA）需要模型可以理解通用的情况，而不仅仅是特定的benchmark。现状的方法是在QA对中 fine-tune语言模型，以便将更多的常识知识带入QA上下文中。然而，当使用现有的QA生成协议时，可能会出现CSKB中的噪音和生成不正确的问题和false negative选项，这会阻碍模型的泛化能力。为解决这些问题，我们提出了QADYNAMICS，一个基于训练动态的框架 дляQA诊断和改进。我们的方法在每个QA对的训练动态中分析问题和选项的训练动态，并将不可识别的QA对和false negative选项移除。我们的实验证明了我们的方法的有效性，能够在使用33%的合成数据的情况下，以及包括LLMs like ChatGPT的情况下，与所有基线都比较。此外，专家评估也证明了我们的框架可以显著提高问答生成质量。我们的代码和模型检查点可以在https://github.com/HKUST-KnowComp/QaDynamics中找到。
</details></li>
</ul>
<hr>
<h2 id="ChapGTP-ILLC’s-Attempt-at-Raising-a-BabyLM-Improving-Data-Efficiency-by-Automatic-Task-Formation"><a href="#ChapGTP-ILLC’s-Attempt-at-Raising-a-BabyLM-Improving-Data-Efficiency-by-Automatic-Task-Formation" class="headerlink" title="ChapGTP, ILLC’s Attempt at Raising a BabyLM: Improving Data Efficiency by Automatic Task Formation"></a>ChapGTP, ILLC’s Attempt at Raising a BabyLM: Improving Data Efficiency by Automatic Task Formation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11282">http://arxiv.org/abs/2310.11282</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaap Jumelet, Michael Hanna, Marianne de Heer Kloots, Anna Langedijk, Charlotte Pouw, Oskar van der Wal</li>
<li>for: 这个论文是为了参加BabyLM挑战（Warstadt等., 2023）的strict-small track而写的。</li>
<li>methods: 这个模型使用了一种新的数据增强技术called Automatic Task Formation，并在200个epoch中训练。</li>
<li>results: 这个模型在BLiMP、(Super)GLUE和MSGS三个评估集中表现出色，并且提供了一些不包括在模型中的方法，可能用于训练low-resource的语言模型。<details>
<summary>Abstract</summary>
We present the submission of the ILLC at the University of Amsterdam to the BabyLM challenge (Warstadt et al., 2023), in the strict-small track. Our final model, ChapGTP, is a masked language model that was trained for 200 epochs, aided by a novel data augmentation technique called Automatic Task Formation. We discuss in detail the performance of this model on the three evaluation suites: BLiMP, (Super)GLUE, and MSGS. Furthermore, we present a wide range of methods that were ultimately not included in the model, but may serve as inspiration for training LMs in low-resource settings.
</details>
<details>
<summary>摘要</summary>
我们提交了阿姆斯特丹大学ILLC的订阅（Warstadt等，2023），在严格小轨道上进行了参与。我们的最终模型“ChapGTP”是一个做了200个epoch的面孔语言模型，得益于一种新的数据增强技术called自动任务形成。我们在BLiMP、（Super）GLUE和MSGS三个评估集上详细讲解了这个模型的性能。此外，我们还提供了一些不被包括在模型中的方法，可能可以用于训练LMs在低资源环境下。
</details></li>
</ul>
<hr>
<h2 id="xMEN-A-Modular-Toolkit-for-Cross-Lingual-Medical-Entity-Normalization"><a href="#xMEN-A-Modular-Toolkit-for-Cross-Lingual-Medical-Entity-Normalization" class="headerlink" title="xMEN: A Modular Toolkit for Cross-Lingual Medical Entity Normalization"></a>xMEN: A Modular Toolkit for Cross-Lingual Medical Entity Normalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11275">http://arxiv.org/abs/2310.11275</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hpi-dhc/xmen">https://github.com/hpi-dhc/xmen</a></li>
<li>paper_authors: Florian Borchert, Ignacio Llorca, Roland Roller, Bert Arnrich, Matthieu-P. Schapranow</li>
<li>for: 提高医疗实体 нормализации的性能 across 多种语言, 特别是当语言资源更少时。</li>
<li>methods: 我们介绍了 xMEN，一个模块化的跨语言医疗实体 нормализаation系统，可以在低资源和高资源enario下表现出色。当目标语言中缺乏同义词时，我们利用英语别名进行跨语言候选生成。对候选列表排名，我们使用可训练的跨Encoder模型，并评估了基于机器翻译dataset的弱监督学习模型。</li>
<li>results: xMEN在多种多样的多语言benchmark数据集上提高了状态的艺术性表现。弱监督的跨Encoder模型在没有目标任务的注释数据时也是有效的。通过xMEN与BigBIO框架的兼容性，它可以轻松地与现有和未来的数据集结合使用。<details>
<summary>Abstract</summary>
Objective: To improve performance of medical entity normalization across many languages, especially when fewer language resources are available compared to English.   Materials and Methods: We introduce xMEN, a modular system for cross-lingual medical entity normalization, which performs well in both low- and high-resource scenarios. When synonyms in the target language are scarce for a given terminology, we leverage English aliases via cross-lingual candidate generation. For candidate ranking, we incorporate a trainable cross-encoder model if annotations for the target task are available. We also evaluate cross-encoders trained in a weakly supervised manner based on machine-translated datasets from a high resource domain. Our system is publicly available as an extensible Python toolkit.   Results: xMEN improves the state-of-the-art performance across a wide range of multilingual benchmark datasets. Weakly supervised cross-encoders are effective when no training data is available for the target task. Through the compatibility of xMEN with the BigBIO framework, it can be easily used with existing and prospective datasets.   Discussion: Our experiments show the importance of balancing the output of general-purpose candidate generators with subsequent trainable re-rankers, which we achieve through a rank regularization term in the loss function of the cross-encoder. However, error analysis reveals that multi-word expressions and other complex entities are still challenging.   Conclusion: xMEN exhibits strong performance for medical entity normalization in multiple languages, even when no labeled data and few terminology aliases for the target language are available. Its configuration system and evaluation modules enable reproducible benchmarks. Models and code are available online at the following URL: https://github.com/hpi-dhc/xmen
</details>
<details>
<summary>摘要</summary>
目的：提高医疗实体Normalization的表现 across多种语言，特别是当target语言的资源更少于英语时。  材料和方法：我们介绍xMEN，一个模块化的跨语言医疗实体Normalization系统，能够在低资源和高资源enario中表现出色。当target语言中某些同义词scarce时，我们利用英语别名via cross-lingual candidate生成。对候选列表排名，我们采用可训练的跨编码模型，如果目标任务的注释存在。我们还评估了基于弱监督的机器翻译数据集来训练cross-编码器。我们的系统公开提供了可扩展的Python工具包。  结果：xMEN在多种多语言的benchmark数据集上提高了状态的艺术表现。弱监督的cross-编码器在没有目标任务的注释时效果很好。通过xMEN与BigBIO框架的兼容性，它可以轻松地与现有和前景数据集结合使用。  讨论：我们的实验表明，需要平衡通用候选生成器的输出和后续可训练的再排名器，我们通过跨编码器损失函数中的排名常量来实现。然而，错误分析表明，复杂实体，如多单词表达和其他复杂实体，仍然是挑战。  结论：xMEN在多种语言的医疗实体Normalization中表现出色，即使target语言的资源很少，并且可以轻松地与现有和前景数据集结合使用。它的配置系统和评估模块使得可重现性很好。模型和代码在以下URL上可以下载：https://github.com/hpi-dhc/xmen
</details></li>
</ul>
<hr>
<h2 id="Utilizing-Weak-Supervision-To-Generate-Indonesian-Conservation-Dataset"><a href="#Utilizing-Weak-Supervision-To-Generate-Indonesian-Conservation-Dataset" class="headerlink" title="Utilizing Weak Supervision To Generate Indonesian Conservation Dataset"></a>Utilizing Weak Supervision To Generate Indonesian Conservation Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11258">http://arxiv.org/abs/2310.11258</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mega Fransiska, Diah Pitaloka, Saripudin, Satrio Putra, Lintang Sutawika</li>
<li>for: 这篇论文目的是构建一个印度尼西亚语言处理 dataset，使用弱监督学习方法生成软标注数据。</li>
<li>methods: 该论文使用了labeling函数，创建了多类分类和情感分类的两种类型数据集。</li>
<li>results: 基线实验表明，使用不同预训练语言模型可以达到59.79%的准确率和55.72%的F1分数 для情感分类，66.87%的F1分数-macro、71.5%的F1分数-micro、83.67%的ROC-AUC для多类分类。<details>
<summary>Abstract</summary>
Weak supervision has emerged as a promising approach for rapid and large-scale dataset creation in response to the increasing demand for accelerated NLP development. By leveraging labeling functions, weak supervision allows practitioners to generate datasets quickly by creating learned label models that produce soft-labeled datasets. This paper aims to show how such an approach can be utilized to build an Indonesian NLP dataset from conservation news text. We construct two types of datasets: multi-class classification and sentiment classification. We then provide baseline experiments using various pretrained language models. These baseline results demonstrate test performances of 59.79% accuracy and 55.72% F1-score for sentiment classification, 66.87% F1-score-macro, 71.5% F1-score-micro, and 83.67% ROC-AUC for multi-class classification. Additionally, we release the datasets and labeling functions used in this work for further research and exploration.
</details>
<details>
<summary>摘要</summary>
弱监督学习已经成为快速和大规模数据创建的有力的方法，以满足人工智能发展的增加需求。通过利用标签函数，弱监督允许实践者快速生成数据集，创建学习的标签模型，生成软标注数据集。本文想要表明如何使用这种方法来建立印尼语言处理数据集。我们构建了两种类型的数据集：多类分类和情感分类。然后，我们提供了基线实验，使用不同的预训练语言模型。这些基线结果表明了情感分类的测试准确率为59.79%，情感分类的F1分数为55.72%，多类分类的F1分数为66.87%，多类分类的Macro F1分数为71.5%，多类分类的微 F1分数为83.67%。此外，我们发布了在这项工作中使用的数据集和标签函数，以便进一步的研究和探索。
</details></li>
</ul>
<hr>
<h2 id="CrossCodeEval-A-Diverse-and-Multilingual-Benchmark-for-Cross-File-Code-Completion"><a href="#CrossCodeEval-A-Diverse-and-Multilingual-Benchmark-for-Cross-File-Code-Completion" class="headerlink" title="CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code Completion"></a>CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11248">http://arxiv.org/abs/2310.11248</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amazon-science/cceval">https://github.com/amazon-science/cceval</a></li>
<li>paper_authors: Yangruibo Ding, Zijian Wang, Wasi Uddin Ahmad, Hantian Ding, Ming Tan, Nihal Jain, Murali Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia, Dan Roth, Bing Xiang</li>
<li>for: 这篇论文的目的是为了评估代码完成器的能力，并提供一个多档案、多语言的代码完成实验室（CrossCodeEval），以测试代码完成器在实际软件开发中的表现。</li>
<li>methods: 这篇论文使用了一个简单 yet efficient的静态分析方法，将使用cross-file context的例子组建在四种流行程式语言（Python、Java、TypeScript、C#）中，以模拟实际软件开发中的档案间依赖。</li>
<li>results: 实验结果显示，CrossCodeEval 是一个非常具有挑战性的测试，当cross-file context absent时，代码完成器的性能很差，但是通过添加cross-file context可以大幅提高性能。另外，这篇论文还评估了不同的方法来获取cross-file context，并显示CrossCodeEval可以用来评估代码检索器的能力。<details>
<summary>Abstract</summary>
Code completion models have made significant progress in recent years, yet current popular evaluation datasets, such as HumanEval and MBPP, predominantly focus on code completion tasks within a single file. This over-simplified setting falls short of representing the real-world software development scenario where repositories span multiple files with numerous cross-file dependencies, and accessing and understanding cross-file context is often required to complete the code correctly.   To fill in this gap, we propose CrossCodeEval, a diverse and multilingual code completion benchmark that necessitates an in-depth cross-file contextual understanding to complete the code accurately. CrossCodeEval is built on a diverse set of real-world, open-sourced, permissively-licensed repositories in four popular programming languages: Python, Java, TypeScript, and C#. To create examples that strictly require cross-file context for accurate completion, we propose a straightforward yet efficient static-analysis-based approach to pinpoint the use of cross-file context within the current file.   Extensive experiments on state-of-the-art code language models like CodeGen and StarCoder demonstrate that CrossCodeEval is extremely challenging when the relevant cross-file context is absent, and we see clear improvements when adding these context into the prompt. However, despite such improvements, the pinnacle of performance remains notably unattained even with the highest-performing model, indicating that CrossCodeEval is also capable of assessing model's capability in leveraging extensive context to make better code completion. Finally, we benchmarked various methods in retrieving cross-file context, and show that CrossCodeEval can also be used to measure the capability of code retrievers.
</details>
<details>
<summary>摘要</summary>
现代代码完成模型在过去几年内已经做出了 significiant 进步，但是目前流行的评估数据集，如 HumanEval 和 MBPP，主要集中在单个文件中的代码完成任务上。这种过分简化的设定不能够反映现实世界软件开发场景，其中文件数量很多，文件之间存在丰富的相互依赖关系，以至于完成代码时需要跨文件上下文的深入理解。为了填补这个空白，我们提出了 CrossCodeEval，一个多样化和多语言的代码完成评估标准。CrossCodeEval 基于四种流行编程语言：Python、Java、TypeScript 和 C# 的真实世界开源项目，并且通过一种简单 yet efficient 的静态分析方法来寻找文件之间的相互依赖关系。我们通过对 CodeGen 和 StarCoder 等现状代码生成器进行广泛的实验发现，当缺乏相关的跨文件上下文时，CrossCodeEval 非常具有挑战性，而在添加上下文时，模型的表现有显著改善。尽管如此，绝对高性能的水平仍然未能得到满分，表明 CrossCodeEval 还可以评估模型是否能够充分利用广泛的上下文来提高代码生成质量。最后，我们还研究了不同的跨文件上下文检索方法，并证明 CrossCodeEval 可以用于评估代码检索器的能力。
</details></li>
</ul>
<hr>
<h2 id="Entity-Matching-using-Large-Language-Models"><a href="#Entity-Matching-using-Large-Language-Models" class="headerlink" title="Entity Matching using Large Language Models"></a>Entity Matching using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11244">http://arxiv.org/abs/2310.11244</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wbsg-uni-mannheim/matchgpt">https://github.com/wbsg-uni-mannheim/matchgpt</a></li>
<li>paper_authors: Ralph Peeters, Christian Bizer</li>
<li>for: The paper is written for discussing the use of large language models (LLMs) for entity matching, as an alternative to pre-trained language models (PLMs) such as BERT and RoBERTa.</li>
<li>methods: The paper investigates the use of hosted LLMs such as GPT3.5 and GPT4, as well as open source LLMs based on Llama2, for entity matching. The authors evaluate these models in both zero-shot and task-specific scenarios, and compare different prompt designs and fine-tuning strategies.</li>
<li>results: The paper shows that GPT4 outperforms fine-tuned PLMs (RoBERTa and Ditto) on three out of five benchmark datasets, reaching F1 scores around 90%. The authors also find that in-context learning and rule generation can improve the performance of other models, but GPT4 does not need such additional guidance in most cases.<details>
<summary>Abstract</summary>
Entity Matching is the task of deciding whether two entity descriptions refer to the same real-world entity. Entity Matching is a central step in most data integration pipelines and an enabler for many e-commerce applications which require to match products offers from different vendors. State-of-the-art entity matching methods often rely on pre-trained language models (PLMs) such as BERT or RoBERTa. Two major drawbacks of these models for entity matching are that (i) the models require significant amounts of task-specific training data and (ii) the fine-tuned models are not robust concerning out-of-distribution entities. In this paper, we investigate using large language models (LLMs) for entity matching as a less domain-specific training data reliant and more robust alternative to PLM-based matchers. Our study covers hosted LLMs, such as GPT3.5 and GPT4, as well as open source LLMs based on Llama2 which can be run locally. We evaluate these models in a zero-shot scenario as well as a scenario where task-specific training data is available. We compare different prompt designs as well as the prompt sensitivity of the models in the zero-shot scenario. We investigate (i) the selection of in-context demonstrations, (ii) the generation of matching rules, as well as (iii) fine-tuning GPT3.5 in the second scenario using the same pool of training data across the different approaches. Our experiments show that GPT4 without any task-specific training data outperforms fine-tuned PLMs (RoBERTa and Ditto) on three out of five benchmark datasets reaching F1 scores around 90%. The experiments with in-context learning and rule generation show that all models beside of GPT4 benefit from these techniques (on average 5.9% and 2.2% F1), while GPT4 does not need such additional guidance in most cases...
</details>
<details>
<summary>摘要</summary>
entity matching是决定两个实体描述是否指同一个真实世界实体的任务。entity matching是数据集成管道中的中间步骤，也是许多电子商务应用程序所需的匹配产品Offer from different vendors。现状的entity matching方法frequently rely on预训练语言模型（PLMs），如BERT或RoBERTa。这两个模型的两个主要缺点是（i）模型需要大量的任务特定训练数据，以及（ii）精制化模型对异常实体不稳定。在这篇论文中，我们研究使用大型语言模型（LLMs）进行entity matching，作为不需要任务特定训练数据且更加稳定的替代方案。我们的研究包括主机LLMs，如GPT3.5和GPT4，以及基于Llama2的开源LLMs，可以在本地运行。我们对这些模型进行零例试验以及具有任务特定训练数据的情况下的试验。我们比较了不同的提示设计以及提示敏感度在零例试验中。我们还研究（i）选择在Context中示例，（ii）生成匹配规则，以及（iii）使用同一批训练数据来练化GPT3.5。我们的实验结果显示，GPT4无需任务特定训练数据可以在五个benchmark dataset上达到F1分数约90%。在采用增强学习和规则生成的情况下，所有模型都受益于这些技术（平均5.9%和2.2% F1），而GPT4则不需要这些额外指导。
</details></li>
</ul>
<hr>
<h2 id="Watermarking-LLMs-with-Weight-Quantization"><a href="#Watermarking-LLMs-with-Weight-Quantization" class="headerlink" title="Watermarking LLMs with Weight Quantization"></a>Watermarking LLMs with Weight Quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11237">http://arxiv.org/abs/2310.11237</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/twilight92z/quantize-watermark">https://github.com/twilight92z/quantize-watermark</a></li>
<li>paper_authors: Linyang Li, Botian Jiang, Pengyu Wang, Ke Ren, Hang Yan, Xipeng Qiu</li>
<li>for: 保护大型语言模型的权限</li>
<li>methods: 在量化过程中植入水印，无需预先定义触发器</li>
<li>results: 成功植入水印到open-source大语言模型 weights中，包括 GPT-Neo 和 LLaMA<details>
<summary>Abstract</summary>
Abuse of large language models reveals high risks as large language models are being deployed at an astonishing speed. It is important to protect the model weights to avoid malicious usage that violates licenses of open-source large language models. This paper proposes a novel watermarking strategy that plants watermarks in the quantization process of large language models without pre-defined triggers during inference. The watermark works when the model is used in the fp32 mode and remains hidden when the model is quantized to int8, in this way, the users can only inference the model without further supervised fine-tuning of the model. We successfully plant the watermark into open-source large language model weights including GPT-Neo and LLaMA. We hope our proposed method can provide a potential direction for protecting model weights in the era of large language model applications.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本为简化中文。<</SYS>>大语言模型滥用风险高，因为大语言模型在惊人速度上部署。保护模型权重很重要，以避免违反开源大语言模型的许可证。这篇论文提议一种新的水印策略，在大语言模型的量化过程中植入水印，而无需先定义触发器。这种水印在fp32模式下工作，并在量化到int8时隐藏起来。因此，用户只能进行无监督练练模型，而不能正常使用模型。我们成功植入了开源大语言模型 weights，包括 GPT-Neo 和 LLaMA。我们希望我们的提议方法可以为大语言模型应用 Era 提供一个可能的方向。
</details></li>
</ul>
<hr>
<h2 id="KG-GPT-A-General-Framework-for-Reasoning-on-Knowledge-Graphs-Using-Large-Language-Models"><a href="#KG-GPT-A-General-Framework-for-Reasoning-on-Knowledge-Graphs-Using-Large-Language-Models" class="headerlink" title="KG-GPT: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models"></a>KG-GPT: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11220">http://arxiv.org/abs/2310.11220</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiho283/kg-gpt">https://github.com/jiho283/kg-gpt</a></li>
<li>paper_authors: Jiho Kim, Yeonsu Kwon, Yohan Jo, Edward Choi</li>
<li>for: 这 paper 的目的是将大语言模型（LLM）应用到知识图（KG）上进行复杂的推理任务。</li>
<li>methods: 这 paper 提出了一种名为 KG-GPT 的多用途框架，该框架包括三个步骤：句子分 segmentation、图像检索和推理，每一步的目的是将句子分解、检索相关的图像组件并 derivate 出逻辑结论。</li>
<li>results: 这 paper 通过对知识图基本问题和知识图问答 benchmark 进行评估，发现 KG-GPT 表现出了竞争力和稳定性，甚至超过了一些完全监督的模型。<details>
<summary>Abstract</summary>
While large language models (LLMs) have made considerable advancements in understanding and generating unstructured text, their application in structured data remains underexplored. Particularly, using LLMs for complex reasoning tasks on knowledge graphs (KGs) remains largely untouched. To address this, we propose KG-GPT, a multi-purpose framework leveraging LLMs for tasks employing KGs. KG-GPT comprises three steps: Sentence Segmentation, Graph Retrieval, and Inference, each aimed at partitioning sentences, retrieving relevant graph components, and deriving logical conclusions, respectively. We evaluate KG-GPT using KG-based fact verification and KGQA benchmarks, with the model showing competitive and robust performance, even outperforming several fully-supervised models. Our work, therefore, marks a significant step in unifying structured and unstructured data processing within the realm of LLMs.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在不结构化文本理解和生成方面已经取得了很大进步，但是它们在结构数据上的应用仍然是未探索的领域。特别是使用 LLM 进行知识图（KG）上复杂逻辑任务还是一个未解决的问题。为解决这个问题，我们提出了 KG-GPT 框架，这是一个多用途的框架，利用 LLM 进行 KG 上的任务。KG-GPT 包括三个步骤：句子分 segmentation、图表检索和推理，每个步骤都是为了分割句子、检索 relevante 的图组件和 derive 逻辑结论。我们通过使用 KG-based fact verification 和 KGQA  bencmark 进行评估，发现 KG-GPT 在 competed 和 robust 性能上表现非常出色，甚至超过了一些完全监督的模型。因此，我们的工作可以视为结构数据和无结构数据处理在 LLM 中的一个重要一步。
</details></li>
</ul>
<hr>
<h2 id="Can-Large-Language-Models-Explain-Themselves-A-Study-of-LLM-Generated-Self-Explanations"><a href="#Can-Large-Language-Models-Explain-Themselves-A-Study-of-LLM-Generated-Self-Explanations" class="headerlink" title="Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations"></a>Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11207">http://arxiv.org/abs/2310.11207</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiyuan Huang, Siddarth Mamidanna, Shreedhar Jangam, Yilun Zhou, Leilani H. Gilpin</li>
<li>for: 这篇论文研究了自动生成的自解释（self-explanations）在情感分析任务中的效果，以及如何用这些自解释来解释模型的决策。</li>
<li>methods: 该论文使用了ChatGPT大语言模型进行实验，并研究了不同的自解释抽象方法和评价指标。</li>
<li>results: 研究发现，ChatGPT自动生成的自解释与传统的解释方法（如 occlusion 或 LIME 相关度图）相比，在评价指标上具有相似的效果，但具有不同的特征。此外，研究还发现了一些有趣的自解释特征，这些特征可能需要现代化解释实践。<details>
<summary>Abstract</summary>
Large language models (LLMs) such as ChatGPT have demonstrated superior performance on a variety of natural language processing (NLP) tasks including sentiment analysis, mathematical reasoning and summarization. Furthermore, since these models are instruction-tuned on human conversations to produce "helpful" responses, they can and often will produce explanations along with the response, which we call self-explanations. For example, when analyzing the sentiment of a movie review, the model may output not only the positivity of the sentiment, but also an explanation (e.g., by listing the sentiment-laden words such as "fantastic" and "memorable" in the review). How good are these automatically generated self-explanations? In this paper, we investigate this question on the task of sentiment analysis and for feature attribution explanation, one of the most commonly studied settings in the interpretability literature (for pre-ChatGPT models). Specifically, we study different ways to elicit the self-explanations, evaluate their faithfulness on a set of evaluation metrics, and compare them to traditional explanation methods such as occlusion or LIME saliency maps. Through an extensive set of experiments, we find that ChatGPT's self-explanations perform on par with traditional ones, but are quite different from them according to various agreement metrics, meanwhile being much cheaper to produce (as they are generated along with the prediction). In addition, we identified several interesting characteristics of them, which prompt us to rethink many current model interpretability practices in the era of ChatGPT(-like) LLMs.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）如ChatGPT已经在自然语言处理（NLP）任务中表现出色，包括情感分析、数学逻辑和摘要。此外，由于这些模型是人类对话中进行了训练，因此它们可以并且经常会生成解释，我们称之为自动生成的解释。例如，当分析电影评论的情感时，模型可能会输出不只是情感的正面性，还会提供解释（例如，列出评论中的情感敏感词语如“很好”和“记忆”）。自动生成的解释如何准确呢？在这篇论文中，我们 investigate这个问题在情感分析任务上，并对于特性解释进行了研究。我们研究了不同的寻求解释的方法，评估其准确性使用一系列评价指标，并与传统解释方法如遮盖或LIME焦点地图进行比较。经过广泛的实验，我们发现ChatGPT的自动解释与传统解释的性能相似，但它们在各种一致指标上有所不同，同时生产成本很低（因为它们与预测一起生成）。此外，我们还发现了一些有趣的特征，让我们重新思考现在的LLM interpretable性实践。
</details></li>
</ul>
<hr>
<h2 id="ViSoBERT-A-Pre-Trained-Language-Model-for-Vietnamese-Social-Media-Text-Processing"><a href="#ViSoBERT-A-Pre-Trained-Language-Model-for-Vietnamese-Social-Media-Text-Processing" class="headerlink" title="ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text Processing"></a>ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11166">http://arxiv.org/abs/2310.11166</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uitnlp/ViSoBERT">https://github.com/uitnlp/ViSoBERT</a></li>
<li>paper_authors: Quoc-Nam Nguyen, Thang Chau Phan, Duc-Vu Nguyen, Kiet Van Nguyen</li>
<li>For: This paper is written for research purposes, specifically to present a new pre-trained language model for Vietnamese social media texts.* Methods: The paper uses the XLM-R architecture to pre-train a large-scale corpus of high-quality and diverse Vietnamese social media texts.* Results: The paper shows that the proposed ViSoBERT model surpasses previous state-of-the-art models on multiple Vietnamese social media tasks, including emotion recognition, hate speech detection, sentiment analysis, spam reviews detection, and hate speech spans detection.Here is the information in Simplified Chinese text:* For: 这篇论文是为研究目的而写的，具体来说是为了介绍一种新的预训语言模型，用于越南语社交媒体文本。* Methods: 这篇论文使用XLM-R架构来预训一个大规模的高质量和多样化的越南语社交媒体文本。* Results: 论文表明，提出的ViSoBERT模型在多个越南语社交媒体任务上都超过了之前的状态态模型，包括情感识别、仇恨言语检测、情绪分析、评论诈骗检测和仇恨言语检测。<details>
<summary>Abstract</summary>
English and Chinese, known as resource-rich languages, have witnessed the strong development of transformer-based language models for natural language processing tasks. Although Vietnam has approximately 100M people speaking Vietnamese, several pre-trained models, e.g., PhoBERT, ViBERT, and vELECTRA, performed well on general Vietnamese NLP tasks, including POS tagging and named entity recognition. These pre-trained language models are still limited to Vietnamese social media tasks. In this paper, we present the first monolingual pre-trained language model for Vietnamese social media texts, ViSoBERT, which is pre-trained on a large-scale corpus of high-quality and diverse Vietnamese social media texts using XLM-R architecture. Moreover, we explored our pre-trained model on five important natural language downstream tasks on Vietnamese social media texts: emotion recognition, hate speech detection, sentiment analysis, spam reviews detection, and hate speech spans detection. Our experiments demonstrate that ViSoBERT, with far fewer parameters, surpasses the previous state-of-the-art models on multiple Vietnamese social media tasks. Our ViSoBERT model is available only for research purposes.
</details>
<details>
<summary>摘要</summary>
英语和中文，两种资源充沛的语言，在自然语言处理任务上曾经目睹了transformer基于语言模型的强大发展。虽然越南有约100万人说越南语，但是一些预训练模型，如 PhoBERT、ViBERT 和 vELECTRA，在普通越南语 NLP 任务上表现良好，包括分词和命名实体识别。这些预训练语言模型仍然只适用于越南社交媒体任务。在这篇论文中，我们提出了第一个普通越南语社交媒体文本预训练语言模型，即ViSoBERT，该模型在 XLM-R 架构上预训练了大规模、多样化的越南语社交媒体文本。此外，我们还对 ViSoBERT 进行了五种重要的自然语言下沉水任务的实验：情感识别、仇恨言语检测、情感分析、垃圾评论检测和仇恨言语检测。我们的实验结果表明，ViSoBERT，只有几乎参数的模型，在多种越南语社交媒体任务上超过了之前的状态码模型。我们的 ViSoBERT 模型仅用于研究purpose。
</details></li>
</ul>
<hr>
<h2 id="IMTLab-An-Open-Source-Platform-for-Building-Evaluating-and-Diagnosing-Interactive-Machine-Translation-Systems"><a href="#IMTLab-An-Open-Source-Platform-for-Building-Evaluating-and-Diagnosing-Interactive-Machine-Translation-Systems" class="headerlink" title="IMTLab: An Open-Source Platform for Building, Evaluating, and Diagnosing Interactive Machine Translation Systems"></a>IMTLab: An Open-Source Platform for Building, Evaluating, and Diagnosing Interactive Machine Translation Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11163">http://arxiv.org/abs/2310.11163</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xuuhuang/imtlab">https://github.com/xuuhuang/imtlab</a></li>
<li>paper_authors: Xu Huang, Zhirui Zhang, Ruize Gao, Yichao Du, Lemao Liu, Gouping Huang, Shuming Shi, Jiajun Chen, Shujian Huang</li>
<li>for: 这个论文的目的是提供一个开源的终端到终端交互机器翻译（IMT）系统平台，帮助研究人员快速构建高效的 IMT 系统，进行终端到终端评估，并诊断系统的弱点。</li>
<li>methods: 这个论文使用了一个人类在Loop的设定，将整个交互翻译过程视为一个任务 Orientated 对话，并在这个设定下，考虑人类干预的影响，以生成高质量、错误率低的翻译。为此， authors 设计了一个通用的交流接口，以支持灵活的 IMT 架构和用户策略。</li>
<li>results: 作者通过 simulate 和实际实验表明，预缀受限的解码方法仍然在终端到终端评估中具有最低的编辑成本，而 BiTIIMT 则在编辑成本方面与前一代 IMT 系统相当，且具有更好的交互体验。<details>
<summary>Abstract</summary>
We present IMTLab, an open-source end-to-end interactive machine translation (IMT) system platform that enables researchers to quickly build IMT systems with state-of-the-art models, perform an end-to-end evaluation, and diagnose the weakness of systems. IMTLab treats the whole interactive translation process as a task-oriented dialogue with a human-in-the-loop setting, in which human interventions can be explicitly incorporated to produce high-quality, error-free translations. To this end, a general communication interface is designed to support the flexible IMT architectures and user policies. Based on the proposed design, we construct a simulated and real interactive environment to achieve end-to-end evaluation and leverage the framework to systematically evaluate previous IMT systems. Our simulated and manual experiments show that the prefix-constrained decoding approach still gains the lowest editing cost in the end-to-end evaluation, while BiTIIMT achieves comparable editing cost with a better interactive experience.
</details>
<details>
<summary>摘要</summary>
我们介绍IMTLab，一个开源的终端到终端交互机器翻译（IMT）系统平台，允许研究人员快速构建IMT系统，进行终端到终端评估，并诊断系统的弱点。IMTLab将整个交互翻译过程视为一个任务强调对话，在人类在Loop Setting中进行明确的参与，以生成高质量、错误率低的翻译。为此，我们设计了一个通用的通信接口，支持灵活的IMT架构和用户策略。基于我们的设计，我们构建了模拟和实际交互环境，以实现终端到终端评估，并利用框架对前一代IMT系统进行系统评估。我们的模拟和手动实验表明，预缀受限的解码方法仍然在终端到终端评估中具有最低的编辑成本，而BiTIIMT则在编辑成本方面与前一代IMT系统相当，并且具有更好的交互体验。
</details></li>
</ul>
<hr>
<h2 id="Probing-the-Creativity-of-Large-Language-Models-Can-models-produce-divergent-semantic-association"><a href="#Probing-the-Creativity-of-Large-Language-Models-Can-models-produce-divergent-semantic-association" class="headerlink" title="Probing the Creativity of Large Language Models: Can models produce divergent semantic association?"></a>Probing the Creativity of Large Language Models: Can models produce divergent semantic association?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11158">http://arxiv.org/abs/2310.11158</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dingnlab/probing_creativity">https://github.com/dingnlab/probing_creativity</a></li>
<li>paper_authors: Honghua Chen, Nai Ding</li>
<li>for:  investigate the creative thinking of large language models through a cognitive perspective</li>
<li>methods:  utilize the divergent association task (DAT) to measure the models’ creativity, compare results across different models and decoding strategies</li>
<li>results:  GPT-4 outperforms 96% of humans in creativity, stochastic sampling and temperature scaling can improve creativity but with a trade-off between creativity and stability.<details>
<summary>Abstract</summary>
Large language models possess remarkable capacity for processing language, but it remains unclear whether these models can further generate creative content. The present study aims to investigate the creative thinking of large language models through a cognitive perspective. We utilize the divergent association task (DAT), an objective measurement of creativity that asks models to generate unrelated words and calculates the semantic distance between them. We compare the results across different models and decoding strategies. Our findings indicate that: (1) When using the greedy search strategy, GPT-4 outperforms 96% of humans, while GPT-3.5-turbo exceeds the average human level. (2) Stochastic sampling and temperature scaling are effective to obtain higher DAT scores for models except GPT-4, but face a trade-off between creativity and stability. These results imply that advanced large language models have divergent semantic associations, which is a fundamental process underlying creativity.
</details>
<details>
<summary>摘要</summary>
大型语言模型拥有惊人的语言处理能力，但是是否能够生成创新内容仍然存在很大的uncertainty。本研究希望通过认知角度来调查大型语言模型的创新思维能力。我们使用了异质关联任务（DAT），这是一种客观的创意测试，要求模型生成不相关的词语并计算它们之间的 semantic distance。我们比较了不同的模型和解码策略的结果。我们发现的结果是：1. 使用排序搜索策略时，GPT-4比96%的人类表现更高，而GPT-3.5-turbo则达到了人类的平均水平。2. 随机抽样和温度缩放是提高模型的DAT分数的有效策略，但是面临着创新性和稳定性之间的负担。这些结果表明，高级大型语言模型具有异质 semantic associations，这是创造力的基本过程。
</details></li>
</ul>
<hr>
<h2 id="The-Quo-Vadis-of-the-Relationship-between-Language-and-Large-Language-Models"><a href="#The-Quo-Vadis-of-the-Relationship-between-Language-and-Large-Language-Models" class="headerlink" title="The Quo Vadis of the Relationship between Language and Large Language Models"></a>The Quo Vadis of the Relationship between Language and Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11146">http://arxiv.org/abs/2310.11146</a></li>
<li>repo_url: None</li>
<li>paper_authors: Evelina Leivada, Vittoria Dentella, Elliot Murphy</li>
<li>for: 本研究旨在探讨现代自然语言处理（NLP）活动中使用大语言模型（LLMs）的问题。</li>
<li>methods: 本研究采用了理论和实验方法来检验LLMs是否能够提供有用的语言解释。</li>
<li>results: 研究发现，目前LLMs的发展阶段 hardly offer any explanations for language，并提供了未来研究方向的突破口。<details>
<summary>Abstract</summary>
In the field of Artificial (General) Intelligence (AI), the several recent advancements in Natural language processing (NLP) activities relying on Large Language Models (LLMs) have come to encourage the adoption of LLMs as scientific models of language. While the terminology employed for the characterization of LLMs favors their embracing as such, it is not clear that they are in a place to offer insights into the target system they seek to represent. After identifying the most important theoretical and empirical risks brought about by the adoption of scientific models that lack transparency, we discuss LLMs relating them to every scientific model's fundamental components: the object, the medium, the meaning and the user. We conclude that, at their current stage of development, LLMs hardly offer any explanations for language, and then we provide an outlook for more informative future research directions on this topic.
</details>
<details>
<summary>摘要</summary>
在人工智能（通用智能）领域，近年来的自然语言处理（NLP）活动，利用大型语言模型（LLMs）的发展，已经推动了将LLMs作为语言科学模型的采纳。然而，使用这些模型来描述目标系统的terminology仍然不清楚，不确定它们是否能提供语言的深入理解。我们首先标识了采纳不透明科学模型的理论和实证风险，然后将LLMs与科学模型的基本组件——对象、媒体、意义和用户相关联。我们 conclude that，在当前的发展阶段，LLMs几乎没有提供语言的解释，然后我们提供了更加有用的未来研究方向。
</details></li>
</ul>
<hr>
<h2 id="Experimenting-AI-Technologies-for-Disinformation-Combat-the-IDMO-Project"><a href="#Experimenting-AI-Technologies-for-Disinformation-Combat-the-IDMO-Project" class="headerlink" title="Experimenting AI Technologies for Disinformation Combat: the IDMO Project"></a>Experimenting AI Technologies for Disinformation Combat: the IDMO Project</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11097">http://arxiv.org/abs/2310.11097</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorenzo Canale, Alberto Messina</li>
<li>for: 这篇论文的主要目的是为了对防假新闻和假信息技术进行贡献。</li>
<li>methods: 这篇论文使用了以下方法：（i）创建了一些新的数据集用于测试技术（ii）开发了一种自动化的模型来分类《纪事》的判决（iii）创建了一种自动化的模型来识别文本推论（iv）使用GPT-4来识别文本推论（v）开发了一款游戏来提高国民对假新闻的认识。</li>
<li>results: 这篇论文的结果表明，使用GPT-4可以准确地识别文本推论，并且创建的数据集和模型可以帮助更好地分析假信息。<details>
<summary>Abstract</summary>
The Italian Digital Media Observatory (IDMO) project, part of a European initiative, focuses on countering disinformation and fake news. This report outlines contributions from Rai-CRITS to the project, including: (i) the creation of novel datasets for testing technologies (ii) development of an automatic model for categorizing Pagella Politica verdicts to facilitate broader analysis (iii) creation of an automatic model for recognizing textual entailment with exceptional accuracy on the FEVER dataset (iv) assessment using GPT-4 to identify textual entailmen (v) a game to raise awareness about fake news at national events.
</details>
<details>
<summary>摘要</summary>
意大数字媒体观察所（IDMO）项目是欧洲倡议的一部分，旨在对假新闻和假信息进行反制。本报告介绍了意大-CRITS在项目中的贡献，包括：1. 创建了新的数据集用于测试技术2. 开发了一种自动模型，用于分类《意大政治评论》的判决，以便更广泛的分析3. 创建了一种自动模型，用于在FEVER数据集上识别文本关系，并达到了exceptional accuracy4. 使用GPT-4进行评估，以确定文本关系5. 开发了一款游戏，用于在全国活动中宣传假新闻的意识。
</details></li>
</ul>
<hr>
<h2 id="Understanding-writing-style-in-social-media-with-a-supervised-contrastively-pre-trained-transformer"><a href="#Understanding-writing-style-in-social-media-with-a-supervised-contrastively-pre-trained-transformer" class="headerlink" title="Understanding writing style in social media with a supervised contrastively pre-trained transformer"></a>Understanding writing style in social media with a supervised contrastively pre-trained transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11081">http://arxiv.org/abs/2310.11081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Javier Huertas-Tato, Alejandro Martin, David Camacho</li>
<li>for: 本研究旨在理解在线社交媒体上的有害行为，包括谩骂和假信息的传播。</li>
<li>methods: 本研究使用了 Style Transformer for Authorship Representations（STAR）模型，通过大量的公共资源数据集（4.5 x 10^6个作者的文本）和监督对比损失来学习作者的特征特征。</li>
<li>results: 研究表明，使用 STAR 模型可以在零shot情况下与 PAN 挑战中表现竞争力强，并在 PAN 验证挑战中使用单层激活函数达到了良好的结果。此外，在 Reddit 上进行测试，使用支持集成8个文档，512个单词可以准确地识别作者集中的至少80%的作者。<details>
<summary>Abstract</summary>
Online Social Networks serve as fertile ground for harmful behavior, ranging from hate speech to the dissemination of disinformation. Malicious actors now have unprecedented freedom to misbehave, leading to severe societal unrest and dire consequences, as exemplified by events such as the Capitol assault during the US presidential election and the Antivaxx movement during the COVID-19 pandemic. Understanding online language has become more pressing than ever. While existing works predominantly focus on content analysis, we aim to shift the focus towards understanding harmful behaviors by relating content to their respective authors. Numerous novel approaches attempt to learn the stylistic features of authors in texts, but many of these approaches are constrained by small datasets or sub-optimal training losses. To overcome these limitations, we introduce the Style Transformer for Authorship Representations (STAR), trained on a large corpus derived from public sources of 4.5 x 10^6 authored texts involving 70k heterogeneous authors. Our model leverages Supervised Contrastive Loss to teach the model to minimize the distance between texts authored by the same individual. This author pretext pre-training task yields competitive performance at zero-shot with PAN challenges on attribution and clustering. Additionally, we attain promising results on PAN verification challenges using a single dense layer, with our model serving as an embedding encoder. Finally, we present results from our test partition on Reddit. Using a support base of 8 documents of 512 tokens, we can discern authors from sets of up to 1616 authors with at least 80\% accuracy. We share our pre-trained model at huggingface (https://huggingface.co/AIDA-UPM/star) and our code is available at (https://github.com/jahuerta92/star)
</details>
<details>
<summary>摘要</summary>
在线社交网络上，有很多害虫的行为，包括仇恨言论和伪信息的传播。这些恶徒现在有着前所未有的自由度，导致社会不稳和严重的后果，如美国总统选举中的国会攻击和COVID-19大流行期间的反疫苗运动。现在理解线上语言的需求更加紧迫。现有的工作主要侧重于内容分析，我们则想要将注意力转移到理解害虫的行为，并与内容相关著作者的风格特征。这些新的方法可以从文本中学习作者的风格特征，但是许多这些方法受到小数扩展或不佳的训练损失的限制。为了突破这些限制，我们介绍了 Style Transformer for Authorship Representations（STAR），训练在公共源中的450万篇文本中，包括70,000名多元作者。我们的模型使用了监督对称损失来教育模型，以实现作者之间的文本距离最小化。这个作者预先训练任务可以在零配置下 reached competitive performance with PAN challenges on attribution and clustering。此外，我们在 PAN 验证挑战中使用了单个紧密层，并将我们的模型作为嵌入Encoder。最后，我们在 Reddit 上发表了结果，使用了8份文档，每份512个字元，可以识别作者的集合，包括最多1616名作者，至少80%的准确率。我们在 huggingface 上分享了我们的预训练模型（https://huggingface.co/AIDA-UPM/star），并在 GitHub 上分享了我们的代码（https://github.com/jahuerta92/star）。
</details></li>
</ul>
<hr>
<h2 id="VoxArabica-A-Robust-Dialect-Aware-Arabic-Speech-Recognition-System"><a href="#VoxArabica-A-Robust-Dialect-Aware-Arabic-Speech-Recognition-System" class="headerlink" title="VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System"></a>VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11069">http://arxiv.org/abs/2310.11069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdul Waheed, Bashar Talafha, Peter Sullivan, AbdelRahim Elmadany, Muhammad Abdul-Mageed</li>
<li>为：这篇论文旨在开发一个可以识别阿拉伯语方言和自动识别阿拉伯语（ASR）的系统，以便为阿拉伯语研究提供一个可靠的工具。* 方法：这篇论文使用了许多不同的模型，包括HuBERT、Whisper和XLS-R，在一个监督性的 Setting中训练了这些模型，以便进行阿拉伯语方言识别（DID）和ASR任务。* 结果：这篇论文提供了一个可以识别17种阿拉伯语方言和标准Modern Standard Arabic（MSA）的系统，并且在不同的语言和语言混合数据上进行了训练和评估。此外，对于剩下的方言，提供了多种模型的选择，包括Whisper和MMS，以便在零容量设定下进行识别。<details>
<summary>Abstract</summary>
Arabic is a complex language with many varieties and dialects spoken by over 450 millions all around the world. Due to the linguistic diversity and variations, it is challenging to build a robust and generalized ASR system for Arabic. In this work, we address this gap by developing and demoing a system, dubbed VoxArabica, for dialect identification (DID) as well as automatic speech recognition (ASR) of Arabic. We train a wide range of models such as HuBERT (DID), Whisper, and XLS-R (ASR) in a supervised setting for Arabic DID and ASR tasks. Our DID models are trained to identify 17 different dialects in addition to MSA. We finetune our ASR models on MSA, Egyptian, Moroccan, and mixed data. Additionally, for the remaining dialects in ASR, we provide the option to choose various models such as Whisper and MMS in a zero-shot setting. We integrate these models into a single web interface with diverse features such as audio recording, file upload, model selection, and the option to raise flags for incorrect outputs. Overall, we believe VoxArabica will be useful for a wide range of audiences concerned with Arabic research. Our system is currently running at https://cdce-206-12-100-168.ngrok.io/.
</details>
<details>
<summary>摘要</summary>
阿拉伯语是一种复杂的语言，有多种变体和方言，全球约有450亿人使用。由于语言多样性和变化，建立一个可靠和通用的自动语音识别系统（ASR）是一项挑战。在这项工作中，我们开发了一个系统，名为VoxArabica，用于方言识别（DID）以及阿拉伯语自动语音识别（ASR）。我们在监督模式下训练了多种模型，包括HuBERT（DID）、Whisper和XLS-R（ASR）。我们的DID模型可以识别17种不同的方言，以及标准阿拉伯语（MSA）。我们在MSA、EGYPTIAN、MOROCCAN和混合数据上练习了我们的ASR模型。另外，对于剩下的方言在ASR中，我们提供了多种模型选择，如Whisper和MMS，以零战斗模式。我们将这些模型集成到一个单一的网页接口中，并添加了多种功能，如音频记录、文件上传、模型选择和错误输出的选项。总之，我们认为VoxArabica将对阿拉伯语研究领域的各种各样的听众提供很有用的工具。我们的系统当前在https://cdce-206-12-100-168.ngrok.io/上运行。
</details></li>
</ul>
<hr>
<h2 id="Lyricist-Singer-Entropy-Affects-Lyric-Lyricist-Classification-Performance"><a href="#Lyricist-Singer-Entropy-Affects-Lyric-Lyricist-Classification-Performance" class="headerlink" title="Lyricist-Singer Entropy Affects Lyric-Lyricist Classification Performance"></a>Lyricist-Singer Entropy Affects Lyric-Lyricist Classification Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11035">http://arxiv.org/abs/2310.11035</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mitsuki Morita, Masato Kikuchi, Tadachika Ozono</li>
<li>for: 这个研究旨在探讨歌词作家的特点，以便于音乐应用程序中使用。</li>
<li>methods: 研究人员使用了从歌词中提取特点表示歌词作家的方法。</li>
<li>results: 研究发现，歌词作家与歌手之间的关系可以影响歌词分类性能。 Specifically, 歌词作家写歌手的多样性（ entropy）与歌词分类性能之间存在正相关关系。<details>
<summary>Abstract</summary>
Although lyrics represent an essential component of music, few music information processing studies have been conducted on the characteristics of lyricists. Because these characteristics may be valuable for musical applications, such as recommendations, they warrant further study. We considered a potential method that extracts features representing the characteristics of lyricists from lyrics. Because these features must be identified prior to extraction, we focused on lyricists with easily identifiable features. We believe that it is desirable for singers to perform unique songs that share certain characteristics specific to the singer. Accordingly, we hypothesized that lyricists account for the unique characteristics of the singers they write lyrics for. In other words, lyric-lyricist classification performance or the ease of capturing the features of a lyricist from the lyrics may depend on the variety of singers. In this study, we observed a relationship between lyricist-singer entropy or the variety of singers associated with a single lyricist and lyric-lyricist classification performance. As an example, the lyricist-singer entropy is minimal when the lyricist writes lyrics for only one singer. In our experiments, we grouped lyricists among five groups in terms of lyricist-singer entropy and assessed the lyric-lyricist classification performance within each group. Consequently, the best F1 score was obtained for the group with the lowest lyricist-singer entropy. Our results suggest that further analyses of the features contributing to lyric-lyricist classification performance on the lowest lyricist-singer entropy group may improve the feature extraction task for lyricists.
</details>
<details>
<summary>摘要</summary>
although lyrics represent an essential component of music, few music information processing studies have been conducted on the characteristics of lyricists. because these characteristics may be valuable for musical applications, such as recommendations, they warrant further study. we considered a potential method that extracts features representing the characteristics of lyricists from lyrics. because these features must be identified prior to extraction, we focused on lyricists with easily identifiable features. we believe that it is desirable for singers to perform unique songs that share certain characteristics specific to the singer. accordingly, we hypothesized that lyricists account for the unique characteristics of the singers they write lyrics for. in other words, lyric-lyricist classification performance or the ease of capturing the features of a lyricist from the lyrics may depend on the variety of singers. in this study, we observed a relationship between lyricist-singer entropy or the variety of singers associated with a single lyricist and lyric-lyricist classification performance. as an example, the lyricist-singer entropy is minimal when the lyricist writes lyrics for only one singer. in our experiments, we grouped lyricists among five groups in terms of lyricist-singer entropy and assessed the lyric-lyricist classification performance within each group. consequently, the best F1 score was obtained for the group with the lowest lyricist-singer entropy. our results suggest that further analyses of the features contributing to lyric-lyricist classification performance on the lowest lyricist-singer entropy group may improve the feature extraction task for lyricists.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Exploring-Automatic-Evaluation-Methods-based-on-a-Decoder-based-LLM-for-Text-Generation"><a href="#Exploring-Automatic-Evaluation-Methods-based-on-a-Decoder-based-LLM-for-Text-Generation" class="headerlink" title="Exploring Automatic Evaluation Methods based on a Decoder-based LLM for Text Generation"></a>Exploring Automatic Evaluation Methods based on a Decoder-based LLM for Text Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11026">http://arxiv.org/abs/2310.11026</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tomohito Kasahara, Daisuke Kawahara</li>
<li>for: 本文 investigate automatic evaluation methods for text generation based on decoder-based language models.</li>
<li>methods: 本文比较了多种方法，包括基于encoder模型和大语言模型的调教，在两种任务上进行评估：机器翻译评估和 semantics textual similarity，在两种语言中进行评估。</li>
<li>results: 实验结果显示，相比准确调教encoder模型，调教decoder模型表现不佳，这可能是因为decoder模型关注表面字串序列而不捕捉 semantics。此外，对very large decoder-based models such as ChatGPT进行in-context learning也难以识别细致的semantic differences。<details>
<summary>Abstract</summary>
Automatic evaluation of text generation is essential for improving the accuracy of generation tasks. In light of the current trend towards increasingly larger decoder-based language models, we investigate automatic evaluation methods based on such models for text generation. This paper compares various methods, including tuning with encoder-based models and large language models under equal conditions, on two different tasks, machine translation evaluation and semantic textual similarity, in two languages, Japanese and English. Experimental results show that compared to the tuned encoder-based models, the tuned decoder-based models perform poorly. The analysis of the causes for this suggests that the decoder-based models focus on surface word sequences and do not capture meaning. It is also revealed that in-context learning of very large decoder-based models such as ChatGPT makes it difficult to identify fine-grained semantic differences.
</details>
<details>
<summary>摘要</summary>
自动评估文本生成是必要的，以提高生成任务的准确性。鉴于当前大型decoder-based语言模型的趋势，我们 investigate自动评估方法基于这些模型 для文本生成。本文比较了多种方法，包括使用encoder-based模型和大型语言模型在等条件下调整，在两个任务上进行评估：机器翻译评估和 semantic textual similarity，在两种语言中进行比较。实验结果显示，相比调整后的encoder-based模型，调整后的decoder-based模型表现不佳。分析结果表明，decoder-based模型强调表面字符序列，而不捕捉 semantics。此外，对 Very Large decoder-based模型such as ChatGPT进行context learning也会增加识别细致的semantic差异的难度。
</details></li>
</ul>
<hr>
<h2 id="Reading-Order-Matters-Information-Extraction-from-Visually-rich-Documents-by-Token-Path-Prediction"><a href="#Reading-Order-Matters-Information-Extraction-from-Visually-rich-Documents-by-Token-Path-Prediction" class="headerlink" title="Reading Order Matters: Information Extraction from Visually-rich Documents by Token Path Prediction"></a>Reading Order Matters: Information Extraction from Visually-rich Documents by Token Path Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11016">http://arxiv.org/abs/2310.11016</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chong Zhang, Ya Guo, Yi Tu, Huan Chen, Jinyang Tang, Huijia Zhu, Qi Zhang, Tao Gui</li>
<li>for: 本研究旨在提高 Multimodal 预训模型在视觉丰富文档中提取信息的能力，具体是解决 OCR 系统识别文档中文本的顺序问题，以提高名实体识别（NER）的准确率。</li>
<li>methods: 本研究提出了 Token Path Prediction（TPP）方法，它是一种简单的预测头，可以在文档中预测名实体提及的Token序列。TPP 模型文档布局为完全导向图，并在图中预测Token路径作为实体。</li>
<li>results: 实验结果表明，TPP 方法可以有效解决 OCR 系统识别文档中文本的顺序问题，提高 VrD-NER 系统的准确率。此外，本研究还提出了两个修订版本的 NER  benchmark 数据集，以更好地评估 VrD-NER 系统在真实场景中的性能。<details>
<summary>Abstract</summary>
Recent advances in multimodal pre-trained models have significantly improved information extraction from visually-rich documents (VrDs), in which named entity recognition (NER) is treated as a sequence-labeling task of predicting the BIO entity tags for tokens, following the typical setting of NLP. However, BIO-tagging scheme relies on the correct order of model inputs, which is not guaranteed in real-world NER on scanned VrDs where text are recognized and arranged by OCR systems. Such reading order issue hinders the accurate marking of entities by BIO-tagging scheme, making it impossible for sequence-labeling methods to predict correct named entities. To address the reading order issue, we introduce Token Path Prediction (TPP), a simple prediction head to predict entity mentions as token sequences within documents. Alternative to token classification, TPP models the document layout as a complete directed graph of tokens, and predicts token paths within the graph as entities. For better evaluation of VrD-NER systems, we also propose two revised benchmark datasets of NER on scanned documents which can reflect real-world scenarios. Experiment results demonstrate the effectiveness of our method, and suggest its potential to be a universal solution to various information extraction tasks on documents.
</details>
<details>
<summary>摘要</summary>
(Note: The text has been translated into Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. The translation may not be perfect, and some nuances or idiomatic expressions may be lost in translation.)
</details></li>
</ul>
<hr>
<h2 id="Correction-Focused-Language-Model-Training-for-Speech-Recognition"><a href="#Correction-Focused-Language-Model-Training-for-Speech-Recognition" class="headerlink" title="Correction Focused Language Model Training for Speech Recognition"></a>Correction Focused Language Model Training for Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11003">http://arxiv.org/abs/2310.11003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yingyi Ma, Zhe Liu, Ozlem Kalinli</li>
<li>for: 提高自动语音识别（ASR）的表现，特别是在领域适应任务中。</li>
<li>methods: 使用一种新的修正注意力集成学习方法，其主要目标是优化ASR中的词级错误率。该方法使用语言模型（LM）来预测词级错误率，并通过多任务练习来帮助LM学习。</li>
<li>results: 实验结果表明，提出的方法可以有效地提高ASR的表现。相比传统LM训练方法，修正注意力集成学习方法在充分文本情况下可以达到相对5.5%的词错率降低。在缺乏文本情况下，使用LLM生成的文本来进行LM训练可以达到相对13%的词错率降低，而修正注意力集成学习方法进一步可以达到相对6%的词错率降低。<details>
<summary>Abstract</summary>
Language models (LMs) have been commonly adopted to boost the performance of automatic speech recognition (ASR) particularly in domain adaptation tasks. Conventional way of LM training treats all the words in corpora equally, resulting in suboptimal improvements in ASR performance. In this work, we introduce a novel correction focused LM training approach which aims to prioritize ASR fallible words. The word-level ASR fallibility score, representing the likelihood of ASR mis-recognition, is defined and shaped as a prior word distribution to guide the LM training. To enable correction focused training with text-only corpora, large language models (LLMs) are employed as fallibility score predictors and text generators through multi-task fine-tuning. Experimental results for domain adaptation tasks demonstrate the effectiveness of our proposed method. Compared with conventional LMs, correction focused training achieves up to relatively 5.5% word error rate (WER) reduction in sufficient text scenarios. In insufficient text scenarios, LM training with LLM-generated text achieves up to relatively 13% WER reduction, while correction focused training further obtains up to relatively 6% WER reduction.
</details>
<details>
<summary>摘要</summary>
语言模型（LM）已广泛应用于自动语音识别（ASR）的性能提升，特别是在领域适应任务中。传统的LM训练方法往往对所有词语在词库中都进行平等对待，从而导致ASR性能的不足提升。在这种工作中，我们提出了一种新的修正注意力集中LM训练方法，旨在优先级ASR不确定词语。为了实现修正注意力集中LM训练，我们定义了ASR不确定词语的字级 fallibility 分数，表示语音识别器对这些词语的识别错误的可能性。然后，我们通过多任务练化来使用大型语言模型（LLM）来预测 fallibility 分数和生成文本。实验结果表明，我们的提议方法在领域适应任务中具有效果。相比传统LM，修正注意力集中LM训练可以在充分文本场景下提取到相对5.5%的单词错误率（WER）下降。在不充分文本场景下，使用LLM生成文本进行LM训练可以实现相对13%的WER下降，而修正注意力集中LM训练进一步实现相对6%的WER下降。
</details></li>
</ul>
<hr>
<h2 id="Instructive-Dialogue-Summarization-with-Query-Aggregations"><a href="#Instructive-Dialogue-Summarization-with-Query-Aggregations" class="headerlink" title="Instructive Dialogue Summarization with Query Aggregations"></a>Instructive Dialogue Summarization with Query Aggregations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10981">http://arxiv.org/abs/2310.10981</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/BinWang28/InstructDS">https://github.com/BinWang28/InstructDS</a></li>
<li>paper_authors: Bin Wang, Zhengyuan Liu, Nancy F. Chen</li>
<li>for: 该论文旨在扩展对话概要模型的能力集，以适应用户特定的兴趣和需求。</li>
<li>methods: 该论文提出了一种三步approach，包括摘要anchor query生成、筛选query和基于query的概要生成。通过在多个概要数据集上训练一个统一的模型 called InstructDS，可以扩展对话概要模型的能力集。</li>
<li>results: 实验结果显示，我们的方法可以超越当前状态的模型和even larger models，并且具有更高的普适性和准确性，经human subjective评估确认。<details>
<summary>Abstract</summary>
Conventional dialogue summarization methods directly generate summaries and do not consider user's specific interests. This poses challenges in cases where the users are more focused on particular topics or aspects. With the advancement of instruction-finetuned language models, we introduce instruction-tuning to dialogues to expand the capability set of dialogue summarization models. To overcome the scarcity of instructive dialogue summarization data, we propose a three-step approach to synthesize high-quality query-based summarization triples. This process involves summary-anchored query generation, query filtering, and query-based summary generation. By training a unified model called InstructDS (Instructive Dialogue Summarization) on three summarization datasets with multi-purpose instructive triples, we expand the capability of dialogue summarization models. We evaluate our method on four datasets, including dialogue summarization and dialogue reading comprehension. Experimental results show that our approach outperforms the state-of-the-art models and even models with larger sizes. Additionally, our model exhibits higher generalizability and faithfulness, as confirmed by human subjective evaluations.
</details>
<details>
<summary>摘要</summary>
传统的对话概要方法直接生成概要并不考虑用户的特定兴趣。这会导致在用户更关注特定话题或方面时遇到挑战。随着指令训练语言模型的进步，我们介绍了对对话的指令训练（Instruction-Tuning），以扩展对话概要模型的能力集。由于 instrucional dialogue summarization 数据的罕见，我们提出了三步方法来生成高质量的查询基于概要的三元组。这个过程包括概要锚定的查询生成、查询筛选和基于查询的概要生成。通过训练我们提出的 Unified Model called InstructDS（ instrucional Dialogue Summarization）于三个多用途指令三元组的概要集，我们扩展了对话概要模型的能力。我们对四个数据集进行了evaluate，包括对话概要和对话阅读理解。实验结果表明，我们的方法比state-of-the-art模型和更大的模型更高效。此外，我们的模型在人工评价中表现出更高的普适性和准确性。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Aware-Contrastive-Sentence-Representation-Learning-with-Large-Language-Models"><a href="#Semantic-Aware-Contrastive-Sentence-Representation-Learning-with-Large-Language-Models" class="headerlink" title="Semantic-Aware Contrastive Sentence Representation Learning with Large Language Models"></a>Semantic-Aware Contrastive Sentence Representation Learning with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10962">http://arxiv.org/abs/2310.10962</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huiming Wang, Liying Cheng, Zhaodonghui Li, De Wen Soh, Lidong Bing</li>
<li>for: 本研究旨在提出一种semantic-aware冲突单句表示框架，以便通过大型自然语言处理器（LLM）的生成和评估能力自动构建高质量的NLI样本库，并通过这些样本库进行冲突学习 sentence representation。</li>
<li>methods: 本研究提议使用大型自然语言处理器（LLM）的生成和评估能力自动构建高质量的NLI样本库，并通过这些样本库进行冲突学习 sentence representation。</li>
<li>results: 实验和分析结果表明，我们的提议的semantic-aware冲突单句表示框架可以通过LLM进行自动生成和评估，从而学习出更好的句子表示。<details>
<summary>Abstract</summary>
Contrastive learning has been proven to be effective in learning better sentence representations. However, to train a contrastive learning model, large numbers of labeled sentences are required to construct positive and negative pairs explicitly, such as those in natural language inference (NLI) datasets. Unfortunately, acquiring sufficient high-quality labeled data can be both time-consuming and resource-intensive, leading researchers to focus on developing methods for learning unsupervised sentence representations. As there is no clear relationship between these unstructured randomly-sampled sentences, building positive and negative pairs over them is tricky and problematic. To tackle these challenges, in this paper, we propose SemCSR, a semantic-aware contrastive sentence representation framework. By leveraging the generation and evaluation capabilities of large language models (LLMs), we can automatically construct a high-quality NLI-style corpus without any human annotation, and further incorporate the generated sentence pairs into learning a contrastive sentence representation model. Extensive experiments and comprehensive analyses demonstrate the effectiveness of our proposed framework for learning a better sentence representation with LLMs.
</details>
<details>
<summary>摘要</summary>
<SYS> translate_language=zh-CN</SYS> contrastive learning 已经被证明可以学习更好的句子表示。然而，为了训练一个对照学习模型，需要大量的标注句子来构建正例和负例对，如自然语言推理（NLI）数据集中的句子对。然而，获得足够的高质量标注数据可以是时间consuming 和资源占用的，导致研究人员强调开发无监督句子表示学习方法。由于这些随机采样的句子之间没有明确的关系，建立正例和负例对是困难和问题。为解决这些挑战，在这篇论文中，我们提议使用 SemCSR，一个具有 semantic-aware 的对照学习句子表示框架。通过利用大语言模型（LLM）的生成和评估能力，我们可以自动生成高质量 NLI-style 训练集，并将生成的句子对 integrate 到学习对照句子表示模型中。广泛的实验和全面的分析表明我们提议的框架可以通过 LLM 学习更好的句子表示。
</details></li>
</ul>
<hr>
<h2 id="Computing-the-optimal-keyboard-through-a-geometric-analysis-of-the-English-language"><a href="#Computing-the-optimal-keyboard-through-a-geometric-analysis-of-the-English-language" class="headerlink" title="Computing the optimal keyboard through a geometric analysis of the English language"></a>Computing the optimal keyboard through a geometric analysis of the English language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10956">http://arxiv.org/abs/2310.10956</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jules Deschamps, Quentin Hubert, Lucas Ryckelynck</li>
<li>for: 提高键盘输入速度</li>
<li>methods: 利用几何工具在优化框架中提出新的键盘布局，提高输入速度</li>
<li>results: 提出了新的键盘布局，可以提高输入速度<details>
<summary>Abstract</summary>
In the context of a group project for the course COMSW4995 002 - Geometric Data Analysis, we bring our attention to the design of fast-typing keyboards. Leveraging some geometric tools in an optimization framework allowed us to propose novel keyboard layouts that offer a faster typing.
</details>
<details>
<summary>摘要</summary>
在COMSW4995 002 - 几何数据分析课程的小组项目中，我们对快速键盘设计进行了审视。通过使用一些几何工具在优化框架中，我们提出了新的键盘布局，以提高键盘输入速度。Here's the character-by-character breakdown of the translation:* 在 (preposition) - "in"* COMSW4995 (course name) - "COMSW4995"* 002 (course number) - "002"* - (hyphen) - "--"* 几何数据分析 (course name) - "几何数据分析"* 课程 (course) - "课程"* 小组 (group) - "小组"* 项目 (project) - "项目"* 中 (preposition) - "中"* 我们 (pronoun) - "我们"* 对 (preposition) - "对"* 快速键盘 (noun phrase) - "快速键盘"* 设计 (noun) - "设计"* 进行 (verb) - "进行"* 了 (particle) - "了"* 审视 (verb) - "审视"* 通过 (preposition) - "通过"* 使用 (verb) - "使用"* 一些 (determiner) - "一些"* 几何工具 (noun phrase) - "几何工具"* 在 (preposition) - "在"* 优化 (verb) - "优化"* 框架 (noun) - "框架"* 中 (preposition) - "中"* 提出 (verb) - "提出"* 新的 (adjective) - "新的"* 键盘布局 (noun phrase) - "键盘布局"* 以 (preposition) - "以"* 提高 (verb) - "提高"* 键盘输入速度 (noun phrase) - "键盘输入速度"
</details></li>
</ul>
<hr>
<h2 id="TEQ-Trainable-Equivalent-Transformation-for-Quantization-of-LLMs"><a href="#TEQ-Trainable-Equivalent-Transformation-for-Quantization-of-LLMs" class="headerlink" title="TEQ: Trainable Equivalent Transformation for Quantization of LLMs"></a>TEQ: Trainable Equivalent Transformation for Quantization of LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10944">http://arxiv.org/abs/2310.10944</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/intel/neural-compressor">https://github.com/intel/neural-compressor</a></li>
<li>paper_authors: Wenhua Cheng, Yiyang Cai, Kaokao Lv, Haihao Shen</li>
<li>for: 本研究旨在提出一种可学习的等效变换（TEQ），用于保持FP32精度的模型输出，同时利用低精度量化，尤其是3和4位量化。</li>
<li>methods: 本文使用可学习的等效变换（TEQ），不需要额外的计算负担，只需要1000步训练和少于0.1%的原始模型可训练参数。</li>
<li>results: 本研究结果与状态CURRENT最佳方法相当，可以与其他方法结合使用以获得更好的性能。code可以在<a target="_blank" rel="noopener" href="https://github.com/intel/neural-compressor%E4%B8%AD%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/intel/neural-compressor中下载。</a><details>
<summary>Abstract</summary>
As large language models (LLMs) become more prevalent, there is a growing need for new and improved quantization methods that can meet the computationalast layer demands of these modern architectures while maintaining the accuracy. In this paper, we present TEQ, a trainable equivalent transformation that preserves the FP32 precision of the model output while taking advantage of low-precision quantization, especially 3 and 4 bits weight-only quantization. The training process is lightweight, requiring only 1K steps and fewer than 0.1 percent of the original model's trainable parameters. Furthermore, the transformation does not add any computational overhead during inference. Our results are on-par with the state-of-the-art (SOTA) methods on typical LLMs. Our approach can be combined with other methods to achieve even better performance. The code is available at https://github.com/intel/neural-compressor.
</details>
<details>
<summary>摘要</summary>
As large language models (LLMs) become more prevalent, there is a growing need for new and improved quantization methods that can meet the computational layer demands of these modern architectures while maintaining accuracy. In this paper, we present TEQ, a trainable equivalent transformation that preserves the FP32 precision of the model output while taking advantage of low-precision quantization, especially 3 and 4 bits weight-only quantization. The training process is lightweight, requiring only 1K steps and fewer than 0.1% of the original model's trainable parameters. Furthermore, the transformation does not add any computational overhead during inference. Our results are on-par with the state-of-the-art (SOTA) methods on typical LLMs. Our approach can be combined with other methods to achieve even better performance. The code is available at https://github.com/intel/neural-compressor.Here's the translation in Traditional Chinese:为了应对现代架构中的大型语言模型（LLMs）的 Computational Layer 需求，我们需要新的和改进的量化方法，以确保模型的精度。在这篇文章中，我们提出了 TEQ，一个可读的等同转换，可以保留模型输出的 FP32 精度，并在低精度量化中得到更好的性能。我们的训练过程是轻量级的，只需要1K步骤和原始模型的训练参数的0.1%。此外，转换不会在测试过程中添加任何计算过程。我们的结果与现有的 state-of-the-art（SOTA）方法相匹配，并且可以与其他方法结合以取得更好的性能。我们的代码可以在https://github.com/intel/neural-compressor 上获取。
</details></li>
</ul>
<hr>
<h2 id="MASON-NLP-at-eRisk-2023-Deep-Learning-Based-Detection-of-Depression-Symptoms-from-Social-Media-Texts"><a href="#MASON-NLP-at-eRisk-2023-Deep-Learning-Based-Detection-of-Depression-Symptoms-from-Social-Media-Texts" class="headerlink" title="MASON-NLP at eRisk 2023: Deep Learning-Based Detection of Depression Symptoms from Social Media Texts"></a>MASON-NLP at eRisk 2023: Deep Learning-Based Detection of Depression Symptoms from Social Media Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10941">http://arxiv.org/abs/2310.10941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fardin Ahsan Sakib, Ahnaf Atef Choudhury, Ozlem Uzuner</li>
<li>For: The paper is focused on detecting depressive symptoms in social media posts using the Beck Depression Inventory (BDI) questionnaire.* Methods: The authors used a deep learning approach that incorporated MentalBERT, RoBERTa, and LSTM to identify sentences related to different depression symptoms.* Results: Despite their efforts, the evaluation results were lower than expected, highlighting the challenges of ranking sentences from a large dataset about depression.Here’s the same information in Simplified Chinese text:* For: 研究探讨了通过社交媒体帖子中的语言特征来检测抑郁症状，使用 Beck 抑郁 инвен塔ри（BDI）问卷来评估抑郁的严重程度。* Methods: 作者使用了深度学习方法，将MENTALBERT、RoBERTa和LSTM相结合，以检测不同抑郁症状的句子。* Results: 评估结果表明，由于数据集的复杂性和计算资源的限制，得到的结果并不理想，反映了检测抑郁症状的挑战性。<details>
<summary>Abstract</summary>
Depression is a mental health disorder that has a profound impact on people's lives. Recent research suggests that signs of depression can be detected in the way individuals communicate, both through spoken words and written texts. In particular, social media posts are a rich and convenient text source that we may examine for depressive symptoms. The Beck Depression Inventory (BDI) Questionnaire, which is frequently used to gauge the severity of depression, is one instrument that can aid in this study. We can narrow our study to only those symptoms since each BDI question is linked to a particular depressive symptom. It's important to remember that not everyone with depression exhibits all symptoms at once, but rather a combination of them. Therefore, it is extremely useful to be able to determine if a sentence or a piece of user-generated content is pertinent to a certain condition. With this in mind, the eRisk 2023 Task 1 was designed to do exactly that: assess the relevance of different sentences to the symptoms of depression as outlined in the BDI questionnaire. This report is all about how our team, Mason-NLP, participated in this subtask, which involved identifying sentences related to different depression symptoms. We used a deep learning approach that incorporated MentalBERT, RoBERTa, and LSTM. Despite our efforts, the evaluation results were lower than expected, underscoring the challenges inherent in ranking sentences from an extensive dataset about depression, which necessitates both appropriate methodological choices and significant computational resources. We anticipate that future iterations of this shared task will yield improved results as our understanding and techniques evolve.
</details>
<details>
<summary>摘要</summary>
���������й���Depression �C 一种心理健康问题，对人们的生活产生深远的影响。最新的研究表明，抑郁症状可以通过人们的沟通方式和文本来识别。特别是社交媒体帖子，它们是一种便捷的文本来源，我们可以对其进行检测抑郁症状的研究。使用 Beck 抑郁 инвен塔里（BDI）问卷，可以帮助我们评估抑郁的严重程度。我们可以将研究缩小到特定的症状，每个 BDI 问题都与特定的抑郁症状相关。请注意，不 everyone with depression 都会表现出所有的症状，而是一种组合。因此，可以非常有用地判断一句话或一 piece of user-generated content 是否与抑郁症状相关。为了实现这一点，我们参加了 eRisk 2023 任务 1，即评估不同句子是否与抑郁症状相关。我们采用了深度学习方法，并将 MentalBERT、RoBERTa 和 LSTM 织入一起。尽管我们尽力，评估结果低于预期，这反映了评估大量抑郁主题的 dataset 中的挑战。我们期望未来的这些共同任务会产生更好的结果，随着我们的理解和技术的进步。
</details></li>
</ul>
<hr>
<h2 id="Intent-Detection-and-Slot-Filling-for-Home-Assistants-Dataset-and-Analysis-for-Bangla-and-Sylheti"><a href="#Intent-Detection-and-Slot-Filling-for-Home-Assistants-Dataset-and-Analysis-for-Bangla-and-Sylheti" class="headerlink" title="Intent Detection and Slot Filling for Home Assistants: Dataset and Analysis for Bangla and Sylheti"></a>Intent Detection and Slot Filling for Home Assistants: Dataset and Analysis for Bangla and Sylheti</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10935">http://arxiv.org/abs/2310.10935</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fardin Ahsan Sakib, A H M Rezaul Karim, Saadat Hasan Khan, Md Mushfiqur Rahman</li>
<li>for: 这项研究的目的是为了提供一个全面的 Intent 检测和插值数据集，用于支持语言模型在不同语言环境中进行下游任务。</li>
<li>methods: 该研究使用了 GPT-3.5 语言模型，并对 colloquial Bangla、formal Bangla 和 Sylheti 语言进行了分类和插值测试。</li>
<li>results: 研究发现，GPT-3.5 模型在 colloquial Bangla 语言下可以达到 impressive F1 分数为 0.94，而在插值任务中可以达到 F1 分数为 0.51。<details>
<summary>Abstract</summary>
As voice assistants cement their place in our technologically advanced society, there remains a need to cater to the diverse linguistic landscape, including colloquial forms of low-resource languages. Our study introduces the first-ever comprehensive dataset for intent detection and slot filling in formal Bangla, colloquial Bangla, and Sylheti languages, totaling 984 samples across 10 unique intents. Our analysis reveals the robustness of large language models for tackling downstream tasks with inadequate data. The GPT-3.5 model achieves an impressive F1 score of 0.94 in intent detection and 0.51 in slot filling for colloquial Bangla.
</details>
<details>
<summary>摘要</summary>
“智能助手在我们技术先进的社会中确立了地位，但仍需考虑多种语言景观，包括低资源语言的口语形式。我们的研究推出了首个完整的数据集 для意图检测和插槽填充在正式孟加拉语、口语孟加拉语和斯里赫蒂语中，总共984个样本，涵盖10个各异的意图。我们的分析显示大语言模型在资料不足情况下能够成功地处理下游任务。GPT-3.5模型在口语孟加拉语中获得了非常出色的F1分数0.94，在插槽填充方面获得了0.51的F1分数。”
</details></li>
</ul>
<hr>
<h2 id="Spatial-HuBERT-Self-supervised-Spatial-Speech-Representation-Learning-for-a-Single-Talker-from-Multi-channel-Audio"><a href="#Spatial-HuBERT-Self-supervised-Spatial-Speech-Representation-Learning-for-a-Single-Talker-from-Multi-channel-Audio" class="headerlink" title="Spatial HuBERT: Self-supervised Spatial Speech Representation Learning for a Single Talker from Multi-channel Audio"></a>Spatial HuBERT: Self-supervised Spatial Speech Representation Learning for a Single Talker from Multi-channel Audio</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10922">http://arxiv.org/abs/2310.10922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antoni Dimitriadis, Siqi Pan, Vidhyasaharan Sethu, Beena Ahmed</li>
<li>for: 提高speech系统的准确率和泛化能力，通过利用无标注数据进行自动学习</li>
<li>methods: 使用多通道音频输入，实现听说者环境中的噪声和抗噪声性能</li>
<li>results: 比前一代单道音频表示模型更高效，特别在噪声和雾气环境中表现出色，同时也能够在声音地图 Task 上达到优秀的效果<details>
<summary>Abstract</summary>
Self-supervised learning has been used to leverage unlabelled data, improving accuracy and generalisation of speech systems through the training of representation models. While many recent works have sought to produce effective representations across a variety of acoustic domains, languages, modalities and even simultaneous speakers, these studies have all been limited to single-channel audio recordings. This paper presents Spatial HuBERT, a self-supervised speech representation model that learns both acoustic and spatial information pertaining to a single speaker in a potentially noisy environment by using multi-channel audio inputs. Spatial HuBERT learns representations that outperform state-of-the-art single-channel speech representations on a variety of spatial downstream tasks, particularly in reverberant and noisy environments. We also demonstrate the utility of the representations learned by Spatial HuBERT on a speech localisation downstream task. Along with this paper, we publicly release a new dataset of 100 000 simulated first-order ambisonics room impulse responses.
</details>
<details>
<summary>摘要</summary>
自我指导学习已经用不标注数据来提高语音系统的准确性和泛化能力。虽然 latest works 尝试生成适用于多种语音频谱、语言、模态和同时说话人的有效表示，但这些研究都受到单通道音频录音的限制。本文介绍 Spatial HuBERT，一种自我指导的语音表示模型，通过多通道音频输入学习到一个说话人在听到的环境中的both acoustic和空间信息。Spatial HuBERT 的表示超过了当前最佳单通道语音表示的状态，特别是在噪音和干扰环境中。我们还证明 Spatial HuBERT 学习的表示在语音地图任务中具有 Utility。此外，我们在这篇论文中公共发布了100000个 simulated first-order ambisonics room impulse responses 的新数据集。
</details></li>
</ul>
<hr>
<h2 id="Compositional-preference-models-for-aligning-LMs"><a href="#Compositional-preference-models-for-aligning-LMs" class="headerlink" title="Compositional preference models for aligning LMs"></a>Compositional preference models for aligning LMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13011">http://arxiv.org/abs/2310.13011</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongyoung Go, Tomasz Korbak, Germán Kruszewski, Jos Rozen, Marc Dymetman</li>
<li>For: 本研究旨在提高语言模型（LM）与人类偏好的匹配。* Methods: 我们提出了 Compositional Preference Models（CPMs），一种新的偏好模型框架，它将一个全局偏好评估 decomposes 成多个可解释的特征，从提示LM中获取特征的scalar scores，并使用逻辑回归分类器进行聚合。* Results: 我们的实验表明，CPMs 不仅提高了通用性和鲁棒性，而且best-of-n 样本获得到使用 CPMs 比使用标准 PMs 更好。总的来说，我们的方法展示了将 PMs 具备人类偏好的假设，并且通过LM的能力来抽取这些特征的方法的优势。<details>
<summary>Abstract</summary>
As language models (LMs) become more capable, it is increasingly important to align them with human preferences. However, the dominant paradigm for training Preference Models (PMs) for that purpose suffers from fundamental limitations, such as lack of transparency and scalability, along with susceptibility to overfitting the preference dataset. We propose Compositional Preference Models (CPMs), a novel PM framework that decomposes one global preference assessment into several interpretable features, obtains scalar scores for these features from a prompted LM, and aggregates these scores using a logistic regression classifier. CPMs allow to control which properties of the preference data are used to train the preference model and to build it based on features that are believed to underlie the human preference judgment. Our experiments show that CPMs not only improve generalization and are more robust to overoptimization than standard PMs, but also that best-of-n samples obtained using CPMs tend to be preferred over samples obtained using conventional PMs. Overall, our approach demonstrates the benefits of endowing PMs with priors about which features determine human preferences while relying on LM capabilities to extract those features in a scalable and robust way.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)随着语言模型（LM）的能力不断提高，对其进行人类喜好的调整变得越来越重要。然而，目前主流的偏好模型（PM）训练方法受到一些基本的限制，如不透明性和可扩展性，同时容易过拟合偏好数据。我们提议compositional Preference Models（CPM），一种新的PM框架，将一个全局的喜好评估 decomposes into 多个可解释的特征，从提问LM中获取这些特征的标量分数，并使用逻辑回归分类器进行聚合。CPMs允许控制偏好数据中哪些特性用于训练偏好模型，并基于人类喜好判断中认为是重要的特征来建立偏好模型。我们的实验表明，CPMs不仅提高了通用性和鲁棒性，而且best-of-n样本获得使用CPMs比使用标准PMs更受欢迎。总的来说，我们的方法表明了将PMs具备人类喜好的假设，并且利用LM的能力来提取这些特征的可行和稳定的方式。
</details></li>
</ul>
<hr>
<h2 id="Emergent-AI-Assisted-Discourse-Case-Study-of-a-Second-Language-Writer-Authoring-with-ChatGPT"><a href="#Emergent-AI-Assisted-Discourse-Case-Study-of-a-Second-Language-Writer-Authoring-with-ChatGPT" class="headerlink" title="Emergent AI-Assisted Discourse: Case Study of a Second Language Writer Authoring with ChatGPT"></a>Emergent AI-Assisted Discourse: Case Study of a Second Language Writer Authoring with ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10903">http://arxiv.org/abs/2310.10903</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sharin Jacob, Tamara Tate, Mark Warschauer</li>
<li>for: 本研究探讨了ChatGPT如何促进语言学习者的学术写作，以减轻对人类写作标准的担忧。</li>
<li>methods: 本研究采用了 случа研究方法，探讨了Kailing博士在学术写作过程中使用ChatGPT的经验。研究使用了活动理论来理解使用生成AI工具进行写作，数据分析包括 semi-structured interview, writing samples和GPT logs。</li>
<li>results: 结果表明Kailing能够与ChatGPT在不同写作阶段进行有效协作，同时保持自己独特的作者语言和主动性。这表明AI工具如ChatGPT可以增强语言学习者的学术写作，而不会抹杀个体的独特性。本案例研究提供了使用ChatGPT进行学术写作的批判性探讨，以及保持学生独特语言的实践。<details>
<summary>Abstract</summary>
The rapid proliferation of ChatGPT has incited debates regarding its impact on human writing. Amid concerns about declining writing standards, this study investigates the role of ChatGPT in facilitating academic writing, especially among language learners. Using a case study approach, this study examines the experiences of Kailing, a doctoral student, who integrates ChatGPT throughout their academic writing process. The study employs activity theory as a lens for understanding writing with generative AI tools and data analyzed includes semi-structured interviews, writing samples, and GPT logs. Results indicate that Kailing effectively collaborates with ChatGPT across various writing stages while preserving her distinct authorial voice and agency. This underscores the potential of AI tools such as ChatGPT to enhance academic writing for language learners without overshadowing individual authenticity. This case study offers a critical exploration of how ChatGPT is utilized in the academic writing process and the preservation of a student's authentic voice when engaging with the tool.
</details>
<details>
<summary>摘要</summary>
快速扩散的ChatGPT已经引发了人们对人类写作的影响的讨论。本研究探究了ChatGPT如何促进语言学习者的学术写作，特别是在启用AI生成工具的情况下。通过 caso study的方式，本研究研究了Kailing，一名博士学生，在学术写作过程中如何与ChatGPT进行合作。研究使用活动理论作为写作AI工具的理解镜子，数据分析包括 semi-structured 采访、写作样本和 GPT 日志。结果表明，Kailing在不同的写作阶段与ChatGPT进行有效的合作，同时保持自己独特的作者语言和主张。这种情况 highlights  AI工具如ChatGPT可以增强语言学习者的学术写作，不会覆盖个人的 Authenticity。本案例研究如何在学术写作过程中使用ChatGPT，并保持学生的独特语言和主张。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/17/cs.CL_2023_10_17/" data-id="clogxf3m600ch5xraar1d5sig" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/10/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><a class="page-number" href="/page/10/">10</a><span class="page-number current">11</span><a class="page-number" href="/page/12/">12</a><a class="page-number" href="/page/13/">13</a><span class="space">&hellip;</span><a class="page-number" href="/page/83/">83</a><a class="extend next" rel="next" href="/page/12/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">115</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">55</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">111</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">61</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
