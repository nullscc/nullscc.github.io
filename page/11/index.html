
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/11/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-eess.SP_2023_10_26" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/26/eess.SP_2023_10_26/" class="article-date">
  <time datetime="2023-10-26T08:00:00.000Z" itemprop="datePublished">2023-10-26</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/26/eess.SP_2023_10_26/">eess.SP - 2023-10-26</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Novel-Models-for-Multiple-Dependent-Heteroskedastic-Time-Series"><a href="#Novel-Models-for-Multiple-Dependent-Heteroskedastic-Time-Series" class="headerlink" title="Novel Models for Multiple Dependent Heteroskedastic Time Series"></a>Novel Models for Multiple Dependent Heteroskedastic Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17760">http://arxiv.org/abs/2310.17760</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/13204942/stat40710">https://github.com/13204942/stat40710</a></li>
<li>paper_authors: Fangyijie Wang, Michael Salter-Townshend</li>
<li>for: 这个论文是为了处理具有高波动性的脑区活动数据，以及评估多个依赖关系的fMRI时间序列数据的模型性能。</li>
<li>methods: 这个论文提出了一种新的方法来处理高波动性的fMRI数据，包括使用AR和GARCH模型来模型多个依赖关系的时间序列数据。</li>
<li>results: 研究发现，当多个fMRI时间序列数据具有多少头运动时，AR+GARCH模型可以成功地适应这些数据。此外，研究还发现了这些模型在不同的脑区之间共享波动性的现象。<details>
<summary>Abstract</summary>
Functional magnetic resonance imaging or functional MRI (fMRI) is a very popular tool used for differing brain regions by measuring brain activity. It is affected by physiological noise, such as head and brain movement in the scanner from breathing, heart beats, or the subject fidgeting. The purpose of this paper is to propose a novel approach to handling fMRI data for infants with high volatility caused by sudden head movements. Another purpose is to evaluate the volatility modelling performance of multiple dependent fMRI time series data. The models examined in this paper are AR and GARCH and the modelling performance is evaluated by several statistical performance measures. The conclusions of this paper are that multiple dependent fMRI series data can be fitted with AR + GARCH model if the multiple fMRI data have many sudden head movements. The GARCH model can capture the shared volatility clustering caused by head movements across brain regions. However, the multiple fMRI data without many head movements have fitted AR + GARCH model with different performance. The conclusions are supported by statistical tests and measures. This paper highlights the difference between the proposed approach from traditional approaches when estimating model parameters and modelling conditional variances on multiple dependent time series. In the future, the proposed approach can be applied to other research fields, such as financial economics, and signal processing. Code is available at \url{https://github.com/13204942/STAT40710}.
</details>
<details>
<summary>摘要</summary>
функциональная магнитно-резонансная томография (fMRI) 是一种非常受欢迎的工具，用于测量脑动脉的活动。它受到生理噪声的影响，例如在扫描机上的头部和脑部活动，心跳声、呼吸或试验者的不稳定。这篇论文的目的是提出一种处理高波动性fMRI数据的新方法，用于评估多个依赖关系的fMRI时间序列数据的模型性能。这篇论文分析了AR和GARCH模型，并评估了这些模型在多个依赖关系的fMRI时间序列数据中的表现。结论是，如果多个fMRI时间序列数据具有多少头部活动，那么AR + GARCH模型可以正确地适应这些数据。GARCH模型可以捕捉脑区域之间的共同波动性噪声，即头部活动引起的共同噪声。然而，没有多少头部活动的多个fMRI时间序列数据不能够正确地适应AR + GARCH模型。这些结论得到了统计测试和度量的支持。这篇论文区别于传统方法，在估计模型参数和模型conditional variance的时候。未来，该方法可以应用于其他研究领域，如金融经济和信号处理。代码可以在 <https://github.com/13204942/STAT40710> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Digital-Twin-for-UAV-Assisted-Integrated-Sensing-Communication-and-Computation-Networks"><a href="#Adaptive-Digital-Twin-for-UAV-Assisted-Integrated-Sensing-Communication-and-Computation-Networks" class="headerlink" title="Adaptive Digital Twin for UAV-Assisted Integrated Sensing, Communication, and Computation Networks"></a>Adaptive Digital Twin for UAV-Assisted Integrated Sensing, Communication, and Computation Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17470">http://arxiv.org/abs/2310.17470</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bin Li, Wenshuai Liu, Wancheng Xie, Ning Zhang, Yan Zhang</li>
<li>for: 这个论文研究了一个基于数字双子（DT）的集成感知通信计算网络。用户进行射频感知和计算卸载在同一频段上进行，而无人飞机（UAV）被部署以提供边缘计算服务。</li>
<li>methods: 我们首先形式化了一个多目标优化问题，以同时减小多输入多输出（MIMO）雷达的辐射性能和计算卸载能耗。然后，我们利用数字双子（DT）的预测能力提供智能卸载决策，并考虑DT估计偏差。</li>
<li>results: 我们的方法能够均衡感知和计算功能之间的性能质量规比，同时降低计算能耗相比现有研究。<details>
<summary>Abstract</summary>
In this paper, we study a digital twin (DT)-empowered integrated sensing, communication, and computation network. Specifically, the users perform radar sensing and computation offloading on the same spectrum, while unmanned aerial vehicles (UAVs) are deployed to provide edge computing service. We first formulate a multi-objective optimization problem to minimize the beampattern performance of multi-input multi-output (MIMO) radars and the computation offloading energy consumption simultaneously. Then, we explore the prediction capability of DT to provide intelligent offloading decision, where the DT estimation deviation is considered. To track this challenge, we reformulate the original problem as a multi-agent Markov decision process and design a multi-agent proximal policy optimization (MAPPO) framework to achieve a flexible learning policy. Furthermore, the Beta-policy and attention mechanism are used to improve the training performance. Numerical results show that the proposed method is able to balance the performance tradeoff between sensing and computation functions, while reducing the energy consumption compared with the existing studies.
</details>
<details>
<summary>摘要</summary>
在本文中，我们研究了一个基于数字双（DT）的整合感知、通信和计算网络。具体来说，用户进行了雷达感知和计算卷积的同时使用同频率，而无人机（UAV）被部署以提供边缘计算服务。我们首先形ulated一个多目标优化问题，以最小化多输入多Output（MIMO）雷达的扫扫 Pattern性能和计算卷积能 consumption同时。然后，我们探索了DT的预测能力，以提供智能卷积决策。为了跟踪这个挑战，我们将原始问题重新形ulated为多个机器人Markov决策过程，并设计了一个多机器人 proximal policy optimization（MAPPO）框架，以实现 flexible learning策略。此外，我们还使用了β策略和注意力机制来提高训练性能。numerical results表明，我们的方法能够均衡感知和计算功能之间的性能交易，同时降低与已有研究相比的能 consumption。
</details></li>
</ul>
<hr>
<h2 id="Detecting-Abrupt-Change-of-Channel-Covariance-Matrix-in-IRS-Assisted-Communication"><a href="#Detecting-Abrupt-Change-of-Channel-Covariance-Matrix-in-IRS-Assisted-Communication" class="headerlink" title="Detecting Abrupt Change of Channel Covariance Matrix in IRS-Assisted Communication"></a>Detecting Abrupt Change of Channel Covariance Matrix in IRS-Assisted Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17425">http://arxiv.org/abs/2310.17425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Runnan Liu, Liang Liu, Yin Xu, Dazhi He, Wenjun Zhang, Chang Wen Chen</li>
<li>for: 本文关注于智能反射表（IRS）助理通信系统中的频道协方差矩阵变化检测。</li>
<li>methods: 我们提出了一种强大的检测方法，可以检测IRS助理通信系统中频道协方差矩阵的变化。</li>
<li>results: 我们的提议方法通过数值结果验证了其效果。<details>
<summary>Abstract</summary>
The knowledge of channel covariance matrices is crucial to the design of intelligent reflecting surface (IRS) assisted communication. However, channel covariance matrices may change suddenly in practice. This letter focuses on the detection of the above change in IRS-assisted communication. Specifically, we consider the uplink communication system consisting of a single-antenna user (UE), an IRS, and a multi-antenna base station (BS). We first categorize two types of channel covariance matrix changes based on their impact on system design: Type I change, which denotes the change in the BS receive covariance matrix, and Type II change, which denotes the change in the IRS transmit/receive covariance matrix. Secondly, a powerful method is proposed to detect whether a Type I change occurs, a Type II change occurs, or no change occurs. The effectiveness of our proposed scheme is verified by numerical results.
</details>
<details>
<summary>摘要</summary>
知识 Channel 协方差矩阵对智能反射表（IRS）助动通信的设计非常重要。然而， Channel 协方差矩阵在实践中可能会快速变化。本信函要针对IRS协助通信中Channel 协方差矩阵变化的检测。具体来说，我们考虑了单antenna用户（UE）、IRS和多antenna基站（BS）组成的上传通信系统。我们首先将Channel 协方差矩阵变化分为两类基于它们对系统设计的影响：Type I变化，表示BS接收协方差矩阵发生变化，Type II变化表示IRS传输/接收协方差矩阵发生变化。其次，我们提出了一种强大的检测方法，能够检测Type I变化、Type II变化或者没有变化。我们的提议方案的效果得到了数值结果的验证。
</details></li>
</ul>
<hr>
<h2 id="Energy-Efficient-Robust-Beamforming-for-Vehicular-ISAC-with-Imperfect-Channel-Estimation"><a href="#Energy-Efficient-Robust-Beamforming-for-Vehicular-ISAC-with-Imperfect-Channel-Estimation" class="headerlink" title="Energy Efficient Robust Beamforming for Vehicular ISAC with Imperfect Channel Estimation"></a>Energy Efficient Robust Beamforming for Vehicular ISAC with Imperfect Channel Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17401">http://arxiv.org/abs/2310.17401</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanwen Zhang, Haijian Sun, Tianyi He, Weiming Xiang, Rose Qingyang Hu</li>
<li>for: 该论文研究了针对 vehicular integrated sensing and communication (ISAC) 系统中的channel estimation uncertainty的 robust beamforming，以优化系统级能效性 (EE)。</li>
<li>methods: 论文首先将系统EE最大化问题转化为一个受限制的Channel estimation error的问题，然后使用分数编程和准确relaxation (SDR) 将约束转化为一个对偶问题。最后，使用Schur complement和S-Procedure将Cramer-Rao bound (CRB)和channel estimation error约束转化为几何约束。</li>
<li>results: 研究结果表明，提出的算法具有良好的收敛速率，并能有效地减轻频率 estimation errors的影响。<details>
<summary>Abstract</summary>
This paper investigates robust beamforming for system-centric energy efficiency (EE) optimization in the vehicular integrated sensing and communication (ISAC) system, where the mobility of vehicles poses significant challenges to channel estimation. To obtain the optimal beamforming under channel uncertainty, we first formulate an optimization problem for maximizing the system EE under bounded channel estimation errors. Next, fractional programming and semidefinite relaxation (SDR) are utilized to relax the rank-1 constraints. We further use Schur complement and S-Procedure to transform Cramer-Rao bound (CRB) and channel estimation error constraints into convex forms, respectively. Based on the Lagrangian dual function and Karush-Kuhn-Tucker (KKT) conditions, it is proved that the optimal beamforming solution is rank-1. Finally, we present comprehensive simulation results to demonstrate two key findings: 1) the proposed algorithm exhibits a favorable convergence rate, and 2) the approach effectively mitigates the impact of channel estimation errors.
</details>
<details>
<summary>摘要</summary>
We use fractional programming and semidefinite relaxation (SDR) to relax the rank-1 constraints. We then transform the Cramer-Rao bound (CRB) and channel estimation error constraints into convex forms using Schur complement and S-Procedure, respectively.Using the Lagrangian dual function and Karush-Kuhn-Tucker (KKT) conditions, we prove that the optimal beamforming solution is rank-1. Finally, we present comprehensive simulation results to show that the proposed algorithm has a favorable convergence rate and effectively mitigates the impact of channel estimation errors.
</details></li>
</ul>
<hr>
<h2 id="Near-Field-Positioning-and-Attitude-Sensing-Based-on-Electromagnetic-Propagation-Modeling"><a href="#Near-Field-Positioning-and-Attitude-Sensing-Based-on-Electromagnetic-Propagation-Modeling" class="headerlink" title="Near-Field Positioning and Attitude Sensing Based on Electromagnetic Propagation Modeling"></a>Near-Field Positioning and Attitude Sensing Based on Electromagnetic Propagation Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17327">http://arxiv.org/abs/2310.17327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ang Chen, Li Chen, Yunfei Chen, Nan Zhao, Changsheng You</li>
<li>for: 这篇论文是为了研究无线网络上的位姿探测和感知而写的。</li>
<li>methods: 这篇论文使用了基于电磁理论的电磁场传播模型（EPM）来准确地模型近场通信。在噪声free情况下，EPM模型确定了观测信号与用户设备（UE）的位置和orientation之间的非线性函数关系。为了解决非线性相互作用的困难，我们首先将距离域分成三个区域，由定义的相位悬念距离和间隔约束距离分割。然后，对每个区域，我们获得了低复杂性的关闭式解决方案。</li>
<li>results: 我们的数值结果表明，我们 derivated的Ziv-Zakai bound（ZZB）可以准确预测无线信号噪声环境中 estimator 的性能。更重要的是，我们在位置估计中实现了 millimeter-level的精度，并在orientation估计中实现了0.1-level的精度。<details>
<summary>Abstract</summary>
Positioning and sensing over wireless networks are imperative for many emerging applications. However, traditional wireless channel models cannot be used for sensing the attitude of the user equipment (UE), since they over-simplify the UE as a point target. In this paper, a comprehensive electromagnetic propagation modeling (EPM) based on electromagnetic theory is developed to precisely model the near-field channel. For the noise-free case, the EPM model establishes the non-linear functional dependence of observed signals on both the position and attitude of the UE. To address the difficulty in the non-linear coupling, we first propose to divide the distance domain into three regions, separated by the defined Phase ambiguity distance and Spacing constraint distance. Then, for each region, we obtain the closed-form solutions for joint position and attitude estimation with low complexity. Next, to investigate the impact of random noise on the joint estimation performance, the Ziv-Zakai bound (ZZB) is derived to yield useful insights. The expected Cram\'er-Rao bound (ECRB) is further provided to obtain the simplified closed-form expressions for the performance lower bounds. Our numerical results demonstrate that the derived ZZB can provide accurate predictions of the performance of estimators in all signal-to-noise ratio (SNR) regimes. More importantly, we achieve the millimeter-level accuracy in position estimation and attain the 0.1-level accuracy in attitude estimation.
</details>
<details>
<summary>摘要</summary>
无线网络上的位姿探测是许多出现中的应用所必需的。然而，传统的无线通道模型无法探测用户设备（UE）的姿态，因为它们过于简化了UE为点目标。在这篇论文中，我们开发了基于电磁学理论的完整的电磁传播模型ing（EPM），以精确模拟近场通道。在无噪情况下，EPM模型确定了观察信号与UE的位置和姿态之间的非线性函数关系。为了解决近场吸引的困难，我们首先将距离域分成三个区域，由定义的相位偏移距离和间隔约束距离分割。然后，对每个区域，我们获得了具有低复杂性的闭式解决方案。接着，我们 investigate了随机噪声对共同估计性能的影响，并 derivated Ziv-Zakai bound（ZZB），以获得有用的洞察。此外，我们还提供了预期Cramér-Rao bound（ECRB），以获得简化后的关注下限表达。我们的数值结果表明， derive ZZB可以在所有信号强度（SNR）域内提供准确的性能预测。更重要的是，我们实现了百分之一级的位置估计和0.1级的姿态估计。
</details></li>
</ul>
<hr>
<h2 id="O-band-QKD-link-over-a-multiple-ONT-loaded-carrier-grade-GPON-for-FTTH-applications"><a href="#O-band-QKD-link-over-a-multiple-ONT-loaded-carrier-grade-GPON-for-FTTH-applications" class="headerlink" title="O-band QKD link over a multiple ONT loaded carrier-grade GPON for FTTH applications"></a>O-band QKD link over a multiple ONT loaded carrier-grade GPON for FTTH applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17259">http://arxiv.org/abs/2310.17259</a></li>
<li>repo_url: None</li>
<li>paper_authors: N. Makris, A. Ntanos, A. Papageorgopoulos, A. Stathis, P. Konteli, I. Tsoni, G. Giannoulis, F. Setaki, T. Stathopoulos, G. Lyberopoulos, H. Avramopoulos, G. T. Kanellos, D. Syvridis</li>
<li>for: 这项研究是为了在实际的光纤到户（FTTH）网络中实现量子键分发（QKD）系统的集成。</li>
<li>methods: 该研究使用了一个商业化的O-带量子键分发系统，并在一个已经 réplikas了一个实际的光纤到户（FTTH）网络的GPON测试环境中成功集成了这个系统。</li>
<li>results: 研究人员成功地在多个ONT上实现了量子键分发系统的集成，并且在实际的FTTH网络中进行了多个ONT的测试和评估。<details>
<summary>Abstract</summary>
We have successfully integrated an O-band commercial Quantum-Key-Distribution (QKD) system over a lit GPON testbed that replicates a carrier-grade Fiber-to-the-Home (FTTH) optical access network with multiple ONTs to emulate real-life FTTH operational deployments.
</details>
<details>
<summary>摘要</summary>
我们已成功将 O-band 商业量子键分发（QKD）系统集成到了模拟了实际FTTH运营部署的灯光干线GPON测试基础设施中。该基础设施包括多个ONT来模拟实际FTTH网络中的多个设备。
</details></li>
</ul>
<hr>
<h2 id="Beampattern-Design-in-Non-Uniform-MIMO-Communication"><a href="#Beampattern-Design-in-Non-Uniform-MIMO-Communication" class="headerlink" title="Beampattern Design in Non-Uniform MIMO Communication"></a>Beampattern Design in Non-Uniform MIMO Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17201">http://arxiv.org/abs/2310.17201</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirsadegh Roshanzamir</li>
<li>for: 本研究旨在探讨非均匀数组下的多输入多输出通信技术。</li>
<li>methods: 本研究使用了优化发射天线位置和交叉相关矩阵来设计发射扩散 patrern。</li>
<li>results: 研究结果表明，通过优化发射天线位置和交叉相关矩阵，可以更好地控制发射扩散 patrern，提高多输入多输出通信的性能。<details>
<summary>Abstract</summary>
In recent years and with introduction of 5G cellular network and communication, researchers have shown great interest in Multiple Input Multiple Output (MIMO) communication, an advanced technology. Many studies have examined the problem of designing the beampattern for MIMO communication using uniform arrays and the covariance-based method to concentrate the transmitted power to the users. However, this paper aims to tackle this issue in the context of non-uniform arrays. Previous authors have primarily focused on designing the transmitted beampattern based on the cross-correlation matrix of transmitted signal elements. In contrast, this paper suggests optimizing the positions of transmitted antennas along with the cross-correlation matrix. This approach is expected to produce better results.
</details>
<details>
<summary>摘要</summary>
在最近几年和5G移动通信网络的引入，研究人员对多输入多输出（MIMO）通信技术表示了极大的兴趣。许多研究都集中在多输入多输出通信中照射 patrern的设计方面，使用均匀阵列和基于协方差的方法来集中发射器的输出功率到用户。然而，这篇论文则是针对非均匀阵列进行设计照射 patrern的。以前的作者主要关注基于发射信号元素的交叉相关矩阵来设计发射 patrern。相比之下，这篇论文提议同时优化发射天线的位置和交叉相关矩阵，这种方法预计会产生更好的结果。
</details></li>
</ul>
<hr>
<h2 id="Multi-level-Gated-Bayesian-Recurrent-Neural-Network-for-State-Estimation"><a href="#Multi-level-Gated-Bayesian-Recurrent-Neural-Network-for-State-Estimation" class="headerlink" title="Multi-level Gated Bayesian Recurrent Neural Network for State Estimation"></a>Multi-level Gated Bayesian Recurrent Neural Network for State Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17187">http://arxiv.org/abs/2310.17187</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shi Yan, Yan Liang, Le Zheng, Mingyang Fan, Binglu Wang, Xiaoxu Wang</li>
<li>for: 本研究旨在提出一种多级闭合抑制极 bayesian 循环神经网络，用于状态估计下存在模型不匹配的情况。</li>
<li>methods: 本文提出了一种新的解决方案，即将非Markov 状态空间模型转换成等效的第一阶Markov模型，并通过数据帮助的联合状态-记忆-偏差极 bayesian 筛选，设计了一个多级闭合极 bayesian 循环神经网络。</li>
<li>results: 在实验中，包括模拟和实际数据集，提议的闭合网络表现较为出色，比 benchmark 筛选和现有深度学习筛选方法更好。<details>
<summary>Abstract</summary>
The optimality of Bayesian filtering relies on the completeness of prior models, while deep learning holds a distinct advantage in learning models from offline data. Nevertheless, the current fusion of these two methodologies remains largely ad hoc, lacking a theoretical foundation. This paper presents a novel solution, namely a multi-level gated Bayesian recurrent neural network specifically designed to state estimation under model mismatches. Firstly, we transform the non-Markov state-space model into an equivalent first-order Markov model with memory. It is a generalized transformation that overcomes the limitations of the first-order Markov property and enables recursive filtering. Secondly, by deriving a data-assisted joint state-memory-mismatch Bayesian filtering, we design a Bayesian multi-level gated framework that includes a memory update gate for capturing the temporal regularities in state evolution, a state prediction gate with the evolution mismatch compensation, and a state update gate with the observation mismatch compensation. The Gaussian approximation implementation of the filtering process within the gated framework is derived, taking into account the computational efficiency. Finally, the corresponding internal neural network structures and end-to-end training methods are designed. The Bayesian filtering theory enhances the interpretability of the proposed gated network, enabling the effective integration of offline data and prior models within functionally explicit gated units. In comprehensive experiments, including simulations and real-world datasets, the proposed gated network demonstrates superior estimation performance compared to benchmark filters and state-of-the-art deep learning filtering methods.
</details>
<details>
<summary>摘要</summary>
bayesian滤波的优点取决于先前模型的完整性，而深度学习具有从线上数据学习模型的优势。然而，现有的这两种方法的结合仍然是广义的，缺乏理论基础。这篇论文提出了一种新的解决方案，即一种多级闭合泛bayesian循环神经网络，专门用于状态估计下的模型差异。首先，我们将非马歇维状态空间模型转换成一个等效的首级Markov模型，以掌握时间序列的特征。这是一种通用的转换方法，可以超越首级Markov性质的限制，并允许递归滤波。其次，通过 derive一种基于数据助记的共同状态记忆滤波，我们设计了一种bayesian多级闭合框架，包括一个记忆更新门、一个状态预测门和一个观测差异补偿门。在这个框架内，我们使用Gaussian approximation实现滤波过程，考虑计算效率。最后，我们设计了相关的内部神经网络结构和端到端训练方法。bayesian滤波理论增强了我们提议的闭合网络的解释能力，使得可以有效地结合在线数据和先前模型内部functionallyExplicit的闭合单元。在广泛的实验中，包括仿真和实际数据，我们的闭合网络示出了与标准滤波器和深度学习滤波方法相比较好的估计性能。
</details></li>
</ul>
<hr>
<h2 id="Max-min-Rate-Optimization-of-Low-Complexity-Hybrid-Multi-User-Beamforming-Maintaining-Rate-Fairness"><a href="#Max-min-Rate-Optimization-of-Low-Complexity-Hybrid-Multi-User-Beamforming-Maintaining-Rate-Fairness" class="headerlink" title="Max-min Rate Optimization of Low-Complexity Hybrid Multi-User Beamforming Maintaining Rate-Fairness"></a>Max-min Rate Optimization of Low-Complexity Hybrid Multi-User Beamforming Maintaining Rate-Fairness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17155">http://arxiv.org/abs/2310.17155</a></li>
<li>repo_url: None</li>
<li>paper_authors: W. Zhu, H. D. Tuan, E. Dutkiewicz, H. V. Poor, L. Hanzo</li>
<li>for: 本研究考虑了一个无线网络，用于服务多个用户，采用 millimeter-wave或sub-terahertz频率带。</li>
<li>methods: 研究使用高通信率多用户混合传输扫描 beamforming，以最大化用户最低速率。为了实现能源效率的信号传输，使用了数组-of-subarrays结构，并采用低分辨率相位调制器。</li>
<li>results: 我们开发了一种基于 convexsolver 算法的方法，可以逐步解决相同的 beamformer 大小的几何问题。我们还引入了 soft max-min rate 目标函数，并开发了可扩展的优化算法。我们的实验结果表明，soft max-min rate 优化不仅可以达到最小用户速率的最低值，而且还可以实现与 sum-rate 最大化的同等总吞吐率。因此，我们的概念的束 beamforming 设计可以提供一种新的同时实现高个用户质量服务和高总网络吞吐率的技术。<details>
<summary>Abstract</summary>
A wireless network serving multiple users in the millimeter-wave or the sub-terahertz band by a base station is considered. High-throughput multi-user hybrid-transmit beamforming is conceived by maximizing the minimum rate of the users. For the sake of energy-efficient signal transmission, the array-of-subarrays structure is used for analog beamforming relying on low-resolution phase shifters. We develop a convexsolver based algorithm, which iteratively invokes a convex problem of the same beamformer size for its solution. We then introduce the soft max-min rate objective function and develop a scalable algorithm for its optimization. Our simulation results demonstrate the striking fact that soft max-min rate optimization not only approaches the minimum user rate obtained by max-min rate optimization but it also achieves a sum rate similar to that of sum-rate maximization. Thus, the soft max-min rate optimization based beamforming design conceived offers a new technique of simultaneously achieving a high individual quality-of-service for all users and a high total network throughput.
</details>
<details>
<summary>摘要</summary>
“考虑一个无线网络，用于服务多个用户，运行在毫米波频率或子teraHz频率带之中的基站。我们提出了一种高通量多用户混合传输射频几何，通过将最大化最低用户速率来实现。为了节省能源，我们使用了一个组件-subarray结构，实现了低分辨率相位调整器的对称传输。我们开发了一个基于convex solver的算法，逐步解决一个具有相同对称传输组件大小的问题。我们然后引入了软max-min率目标函数，并开发了可扩展的数值估算法来优化它。我们的实验结果显示，soft max-min率优化不仅可以接近最小用户速率的最大化优化，而且还可以 дости得一个相似于sum-rate最大化的总网络吞吐量。因此，我们的设计提案可以提供一种新的技术，即同时确保所有用户的个人质量服务水准高，并且确保网络吞吐量高。”
</details></li>
</ul>
<hr>
<h2 id="Reducing-the-impact-of-non-ideal-PRBS-on-microwave-photonic-random-demodulators-by-low-biasing-the-optical-modulator-via-PRBS-amplitude-compression"><a href="#Reducing-the-impact-of-non-ideal-PRBS-on-microwave-photonic-random-demodulators-by-low-biasing-the-optical-modulator-via-PRBS-amplitude-compression" class="headerlink" title="Reducing the impact of non-ideal PRBS on microwave photonic random demodulators by low biasing the optical modulator via PRBS amplitude compression"></a>Reducing the impact of non-ideal PRBS on microwave photonic random demodulators by low biasing the optical modulator via PRBS amplitude compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17676">http://arxiv.org/abs/2310.17676</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiyang Liu, Yang Chen<br>for: 这篇论文旨在解决对microwave photonic random demodulators (RDs)中非理想pseudo-random binary sequence (PRBS)的影响。methods: 本研究提出了一种新的方法，利用lower amplitude PRBS来对光学模拟器进行偏好偏移，以减少非理想PRBS对microwave photonic RDs的影响。results: 实验结果显示，这种方法可以降低重建误差达85%。此方法可以对RD-based photonics-assisted compressed sensing (CS)系统中PRBS的要求进行重大减少，提供一个可行的解决方案，从而降低系统实现的复杂度和成本。<details>
<summary>Abstract</summary>
A novel method for reducing the impact of non-ideal pseudo-random binary sequence (PRBS) on microwave photonic random demodulators (RDs) in a photonics-assisted compressed sensing (CS) system is proposed. Different from the commonly used method that switches the bias point of the optical modulator in the RD between two quadrature transmission points to mix the signal to be sampled and the PRBS, this method employs a PRBS with lower amplitude to low bias the optical modulator so that the impact of non-ideal PRBS on microwave photonic RDs can be greatly reduced by compressing the amplitude of non-ideal parts of the PRBS. An experiment is performed to verify the concept. The optical modulator is properly low-biased via PRBS amplitude compression. The data rate and occupied bandwidth of the PRBS are 500 Mb/s and 1 GHz, while the multi-tone signals with a maximum frequency of 100 MHz are sampled at an equivalent sampling rate of only 50 MSa/s. The results show that the reconstruction error can be reduced by up to 85%. The proposed method can significantly reduce the requirements for PRBS in RD-based photonics-assisted CS systems, providing a feasible solution for reducing the complexity and cost of system implementation.
</details>
<details>
<summary>摘要</summary>
一种新的方法可以减少光学抽象推定系统中pseudo-random binary sequence(PRBS)的影响。与常用的方法不同，这种方法使用低 amplitud PRBS来压缩光学模拟器的偏好点，从而减少非理想PRBS对微波光学RD的影响。一个实验证明了这个概念。通过PRBS压缩 amplitude来低 bias光学模拟器。数据率和占用频谱带宽为500Mb/s和1GHz，而多谱信号的最大频率为100MHz， Sampled at an equivalent sampling rate of only 50MSa/s。结果显示，可以将重建错误降低到85%。提议的方法可以减少RD基于光学抽象推定系统中PRBS的复杂性和成本，提供一个可行的解决方案。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/26/eess.SP_2023_10_26/" data-id="clorjzlim01cqf1881xtw03kf" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_10_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/25/cs.SD_2023_10_25/" class="article-date">
  <time datetime="2023-10-25T15:00:00.000Z" itemprop="datePublished">2023-10-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/25/cs.SD_2023_10_25/">cs.SD - 2023-10-25</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Improved-Panning-on-Non-Equidistant-Loudspeakers-with-Direct-Sound-Level-Compensation"><a href="#Improved-Panning-on-Non-Equidistant-Loudspeakers-with-Direct-Sound-Level-Compensation" class="headerlink" title="Improved Panning on Non-Equidistant Loudspeakers with Direct Sound Level Compensation"></a>Improved Panning on Non-Equidistant Loudspeakers with Direct Sound Level Compensation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17004">http://arxiv.org/abs/2310.17004</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan-Hendrik Hanschke, Daniel Arteaga, Giulio Cengarle, Joshua Lando, Mark R. P. Thomas, Alan Seefeldt</li>
<li>for: 这篇论文旨在提出一种基于直接声音和感知响应的方法，以便在非等距喇叭布局下实现扬声器扬声。</li>
<li>methods: 论文使用了一种新的方法，即基于直接声音和感知响应的方法，以便在非等距喇叭布局下实现扬声器扬声。</li>
<li>results: 试验表明，该方法可以减少喇叭布局不一致性导致的幻音源位偏移，并且可以大大改善声音扬声效果。<details>
<summary>Abstract</summary>
Loudspeaker rendering techniques that create phantom sound sources often assume an equidistant loudspeaker layout. Typical home setups might not fulfill this condition as loudspeakers deviate from canonical positions, thus requiring a corresponding calibration. The standard approach is to compensate for delays and to match the loudness of each loudspeaker at the listener's location. It was found that a shift of the phantom image occurs when this calibration procedure is applied and one of a pair of loudspeakers is significantly closer to the listener than the other. In this paper, a novel approach to panning on non-equidistant loudspeaker layouts is presented whereby the panning position is governed by the direct sound and the perceived loudness is governed by the full impulse response. Subjective listening tests are presented that validate the approach and quantify the perceived effect of the compensation. In a setup where the standard calibration leads to an average error of 10 degrees, the proposed direct sound compensation largely returns the phantom source to its intended position.
</details>
<details>
<summary>摘要</summary>
喇叭渲染技术经常假设喇叭Layout是平等的，但家用设置通常不满足这个条件，因为喇叭与CanonicalPosition不匹配，因此需要相应的调整。标准方法是补偿延迟并将每个喇叭的响度在听众位置进行调整。研究发现，当应用这种调整程序时，如果一对喇叭中的一个喇叭远离听众更近， then the phantom image will shift. 本文提出了一种新的喇叭扫描非平等喇叭布局的方法，其中喇叭扫描位置由直接声音控制，而听众感受到的响度则由全冲响应控制。对比listen testing表明，该方法可以大幅提高喇叭扫描的精度。在一个标准调整后的平均错误为10度的设置下，提posed direct sound compensation方法可以几乎完全将幻音源返回到其原始位置。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Processing-Neural-Network-Architecture-For-Hearing-Loss-Compensation"><a href="#Dynamic-Processing-Neural-Network-Architecture-For-Hearing-Loss-Compensation" class="headerlink" title="Dynamic Processing Neural Network Architecture For Hearing Loss Compensation"></a>Dynamic Processing Neural Network Architecture For Hearing Loss Compensation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16550">http://arxiv.org/abs/2310.16550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Szymon Drgas, Lars Bramsløw, Archontis Politis, Gaurav Naithani, Tuomas Virtanen</li>
<li>for: 提高听力障碍者的语音理解能力（speech intelligibility）</li>
<li>methods: 使用神经网络（neural networks）和听力模型（hearing loss model）实现语音补偿（speech compensation），并提出一种可解释性模型（interpretable model）called dynamic processing network</li>
<li>results: 在使用STOI和HASPI指标评估的情况下，dynamic processing network在与Camfit规则相比 Displayath significant improvement in speech intelligibility, while a large enough convolutional neural network could outperform the interpretable model with higher computational load.<details>
<summary>Abstract</summary>
This paper proposes neural networks for compensating sensorineural hearing loss. The aim of the hearing loss compensation task is to transform a speech signal to increase speech intelligibility after further processing by a person with a hearing impairment, which is modeled by a hearing loss model. We propose an interpretable model called dynamic processing network, which has a structure similar to band-wise dynamic compressor. The network is differentiable, and therefore allows to learn its parameters to maximize speech intelligibility. More generic models based on convolutional layers were tested as well. The performance of the tested architectures was assessed using spectro-temporal objective index (STOI) with hearing-threshold noise and hearing aid speech intelligibility (HASPI) metrics. The dynamic processing network gave a significant improvement of STOI and HASPI in comparison to popular compressive gain prescription rule Camfit. A large enough convolutional network could outperform the interpretable model with the cost of larger computational load. Finally, a combination of the dynamic processing network with convolutional neural network gave the best results in terms of STOI and HASPI.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Novel-Approach-for-Object-Based-Audio-Broadcasting"><a href="#A-Novel-Approach-for-Object-Based-Audio-Broadcasting" class="headerlink" title="A Novel Approach for Object Based Audio Broadcasting"></a>A Novel Approach for Object Based Audio Broadcasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16481">http://arxiv.org/abs/2310.16481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Reza Hasanabadi</li>
<li>for: 提供个性化和自定义的音频经验，适用于不同的平台，如广播、流媒体和电影音频。</li>
<li>methods: 提出了一种新的对象音频生成方法，即Sample-by-Sample Object Based Audio（SSOBA）嵌入。SSOBA将音频对象样本置于一起，让听众根据自己的兴趣和需求自由地个性化选择音频来源。</li>
<li>results: 对SSOBA的主要性能因素进行了研究，包括输入音频对象、输出通道数和采样率。实验结果表明，在编码和解码过程中，SSOBA可以保持高质量音频效果，并且可以在不需要特殊硬件的情况下实现。<details>
<summary>Abstract</summary>
Object Based Audio (OBA) provides a new kind of audio experience, delivered to the audience to personalize and customize their experience of listening and to give them choice of what and how to hear their audio content. OBA can be applied to different platforms such as broadcasting, streaming and cinema sound. This paper presents a novel approach for creating object-based audio on the production side. The approach here presents Sample-by-Sample Object Based Audio (SSOBA) embedding. SSOBA places audio object samples in such a way that allows audiences to easily individualize their chosen audio sources according to their interests and needs. SSOBA is an extra service and not an alternative, so it is also compliant with legacy audio players. The biggest advantage of SSOBA is that it does not require any special additional hardware in the broadcasting chain and it is therefore easy to implement and equip legacy players and decoders with enhanced ability. Input audio objects, number of output channels and sampling rates are three important factors affecting SSOBA performance and specifying it to be lossless or lossy. SSOBA adopts interpolation at the decoder side to compensate for eliminated samples. Both subjective and objective experiments are carried out to evaluate the output results at each step. MUSHRA subjective experiments conducted after the encoding step shows good-quality performance of SSOBA with up to five objects. SNR measurements and objective experiments, performed after decoding and interpolation, show significant successful recovery and separation of audio objects. Experimental results show that a minimum sampling rate of 96 kHz is indicated to encode up to five objects in a Stereo-mode channel to acquire good subjective and objective results simultaneously.
</details>
<details>
<summary>摘要</summary>
对象基于专业（OBA）提供了一种新的专业音频经验，为听众个人化和自定义音频内容的欣赏体验。OBA可以应用到不同的平台，如广播、流媒体和电影 surround sound。本篇文章介绍了一种 novel 的创新方法，即 Sample-by-Sample Object Based Audio（SSOBA）嵌入。SSOBA 将音频 объек�置于适当的位置，以便让听众根据他们的 interess 和需求选择自己想要的音频源。SSOBA 不是一个替代品，而是一个额外的服务，因此适合旧有的音频播放器。SSOBA 的主要优点是不需要在广播链接中添加特殊的硬件，因此易于实现和升级旧有的播放器。音频对象、出力通道数量和抽样率是 SSOBA 性能的三大因素，可以根据这些因素来决定是使用损失less 或损失y 的编码方式。SSOBA 使用 decoder  сторо面的 interpolate 来补偿被删除的样本。在编码和 interpolate 后，我们进行了主观和 объек 的实验，结果显示 SSOBA 在 five 个音频对象的情况下具有良好的品质表现。SNR 测量和对象实验表明，SSOBA 在恢复和分隔音频对象方面取得了显著的成功。实验结果显示，为了在 Stereo-mode 通道中编码 up to five 个音频对象，至少需要 96 kHz 的抽样率。
</details></li>
</ul>
<hr>
<h2 id="Towards-Streaming-Speech-to-Avatar-Synthesis"><a href="#Towards-Streaming-Speech-to-Avatar-Synthesis" class="headerlink" title="Towards Streaming Speech-to-Avatar Synthesis"></a>Towards Streaming Speech-to-Avatar Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16287">http://arxiv.org/abs/2310.16287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tejas S. Prabhune, Peter Wu, Bohan Yu, Gopala K. Anumanchipalli</li>
<li>for: 这篇论文旨在实现实时语音到人物动画的转化，以便在语音学、phonetics和phonology等领域可以实时visualize声音，并帮助第二语言学习和贫听患者的虚拟体现。</li>
<li>methods: 该方法使用深度形态反转来实现高质量的人物动画，使用实时语音而不是录音来进行实时动画化。</li>
<li>results: 该方法可以实现130毫秒的平均流动时间，与真实的语音相关性达0.792。此外，我们还展示了生成的口部和舌头动画，以证明我们的方法的有效性。<details>
<summary>Abstract</summary>
Streaming speech-to-avatar synthesis creates real-time animations for a virtual character from audio data. Accurate avatar representations of speech are important for the visualization of sound in linguistics, phonetics, and phonology, visual feedback to assist second language acquisition, and virtual embodiment for paralyzed patients. Previous works have highlighted the capability of deep articulatory inversion to perform high-quality avatar animation using electromagnetic articulography (EMA) features. However, these models focus on offline avatar synthesis with recordings rather than real-time audio, which is necessary for live avatar visualization or embodiment. To address this issue, we propose a method using articulatory inversion for streaming high quality facial and inner-mouth avatar animation from real-time audio. Our approach achieves 130ms average streaming latency for every 0.1 seconds of audio with a 0.792 correlation with ground truth articulations. Finally, we show generated mouth and tongue animations to demonstrate the efficacy of our methodology.
</details>
<details>
<summary>摘要</summary>
《流式报道——语音到人物Synthesis创造实时动画》我们的研究旨在开发一种基于深度辐射倒推的实时人物动画 Synthesis方法，以便实时visual化语音。我们的方法可以快速地从实时 audio 数据中提取高质量的 facial 和 inner-mouth 动画，并且可以在实时播放 audio 时进行实时动画 Synthesis。我们的方法可以实现每0.1秒 audio 的130ms平均流式延迟，并且与真实辐射数据的0.792相对于ground truth的相关性。最后，我们展示了由我们的方法生成的口部和舌头动画，以证明我们的方法的有效性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/25/cs.SD_2023_10_25/" data-id="clorjzld000yxf188b9cu9pjj" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_10_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/25/eess.AS_2023_10_25/" class="article-date">
  <time datetime="2023-10-25T14:00:00.000Z" itemprop="datePublished">2023-10-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/25/eess.AS_2023_10_25/">eess.AS - 2023-10-25</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="UniX-Encoder-A-Universal-X-Channel-Speech-Encoder-for-Ad-Hoc-Microphone-Array-Speech-Processing"><a href="#UniX-Encoder-A-Universal-X-Channel-Speech-Encoder-for-Ad-Hoc-Microphone-Array-Speech-Processing" class="headerlink" title="UniX-Encoder: A Universal $X$-Channel Speech Encoder for Ad-Hoc Microphone Array Speech Processing"></a>UniX-Encoder: A Universal $X$-Channel Speech Encoder for Ad-Hoc Microphone Array Speech Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16367">http://arxiv.org/abs/2310.16367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zili Huang, Yiwen Shao, Shi-Xiong Zhang, Dong Yu</li>
<li>for:  solve more challenging scenarios of multi-channel recordings with multiple simultaneous talkers</li>
<li>methods:  universal encoder designed for multiple tasks, compatible with any microphone array, and trained without labeled multi-channel data</li>
<li>results:  consistently outperformed combinations like the WavLM model with the BeamformIt frontend in speech recognition and speaker diarization tasks<details>
<summary>Abstract</summary>
The speech field is evolving to solve more challenging scenarios, such as multi-channel recordings with multiple simultaneous talkers. Given the many types of microphone setups out there, we present the UniX-Encoder. It's a universal encoder designed for multiple tasks, and worked with any microphone array, in both solo and multi-talker environments. Our research enhances previous multi-channel speech processing efforts in four key areas: 1) Adaptability: Contrasting traditional models constrained to certain microphone array configurations, our encoder is universally compatible. 2) Multi-Task Capability: Beyond the single-task focus of previous systems, UniX-Encoder acts as a robust upstream model, adeptly extracting features for diverse tasks including ASR and speaker recognition. 3) Self-Supervised Training: The encoder is trained without requiring labeled multi-channel data. 4) End-to-End Integration: In contrast to models that first beamform then process single-channels, our encoder offers an end-to-end solution, bypassing explicit beamforming or separation. To validate its effectiveness, we tested the UniX-Encoder on a synthetic multi-channel dataset from the LibriSpeech corpus. Across tasks like speech recognition and speaker diarization, our encoder consistently outperformed combinations like the WavLM model with the BeamformIt frontend.
</details>
<details>
<summary>摘要</summary>
《演讲场景在解决更加复杂的enario中进行进步，例如多通道录音多个同时发言人。为了解决这些问题，我们提出了UniX-Encoder。它是一种通用编码器，适用于多种 микрофон设置，并在单个和多个发言人环境中都可以工作。我们的研究在以下四个领域进行了进一步改进：1. 适应性：相比传统模型固定于特定的 микрофон设置，我们的编码器是通用的，可以与任何 микрофон设置结合使用。2. 多任务能力：除了先前的单一任务集成，UniX-Encoder 还可以作为一个强大的上游模型，可以提取多种任务的特征，包括ASR和speaker recognition。3. 自我超vised Training：我们的编码器不需要标注的多通道数据进行训练。4. 端到端集成：与先前的模型不同，UniX-Encoder 不需要显式的扩散或分离。它提供了一个端到端的解决方案，直接处理多通道数据，而不需要先进行分离或扩散。为验证其效果，我们在LibriSpeech corpus上测试了UniX-Encoder，并在多个任务，如speech recognition和speaker diarization中，consistently  exceeded combinations like WavLM model with BeamformIt frontend。》
</details></li>
</ul>
<hr>
<h2 id="Covariance-Blocking-and-Whitening-Method-for-Successive-Relative-Transfer-Function-Vector-Estimation-in-Multi-Speaker-Scenarios"><a href="#Covariance-Blocking-and-Whitening-Method-for-Successive-Relative-Transfer-Function-Vector-Estimation-in-Multi-Speaker-Scenarios" class="headerlink" title="Covariance Blocking and Whitening Method for Successive Relative Transfer Function Vector Estimation in Multi-Speaker Scenarios"></a>Covariance Blocking and Whitening Method for Successive Relative Transfer Function Vector Estimation in Multi-Speaker Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16327">http://arxiv.org/abs/2310.16327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Henri Gode, Simon Doclo</li>
<li>for: 这篇论文是为了解决多个说话者在噪音和反射环境中估计相对传输函数（RTF）向量的挑战。</li>
<li>methods: 这篇论文使用了一种被称为盲斜投影（BOP）方法，该方法确定了第二个说话者的斜投影运算符，以便屏蔽第二个说话者。而在这篇论文中，我们提议使用协方差屏蔽和白化（CBW）方法，该方法首先屏蔽第一个说话者，然后使用估计的噪声协方差矩阵进行白化，并基于协方差分解来估计第二个说话者的RTF向量。</li>
<li>results: 在使用两个说话者的估计RTF向量在线性受限最小噪声抑制器中， simulation results使用了真实世界记录的多个说话者位置，demonstrate that the proposed CBW method outperforms the conventional BOP and covariance whitening methods in terms of signal-to-interferer-and-noise ratio improvement.<details>
<summary>Abstract</summary>
This paper addresses the challenge of estimating the relative transfer function (RTF) vectors of multiple speakers in a noisy and reverberant environment. More specifically, we consider a scenario where two speakers activate successively. In this scenario, the RTF vector of the first speaker can be estimated in a straightforward way and the main challenge lies in estimating the RTF vector of the second speaker during segments where both speakers are simultaneously active. To estimate the RTF vector of the second speaker the so-called blind oblique projection (BOP) method determines the oblique projection operator that optimally blocks the second speaker. Instead of blocking the second speaker, in this paper we propose a covariance blocking and whitening (CBW) method, which first blocks the first speaker and applies whitening using the estimated noise covariance matrix and then estimates the RTF vector of the second speaker based on a singular value decomposition. When using the estimated RTF vectors of both speakers in a linearly constrained minimum variance beamformer, simulation results using real-world recordings for multiple speaker positions demonstrate that the proposed CBW method outperforms the conventional BOP and covariance whitening methods in terms of signal-to-interferer-and-noise ratio improvement.
</details>
<details>
<summary>摘要</summary>
To estimate the RTF vector of the second speaker, the so-called blind oblique projection (BOP) method is used to determine the oblique projection operator that optimally blocks the second speaker. However, in this paper, we propose a covariance blocking and whitening (CBW) method instead. This method first blocks the first speaker and applies whitening using the estimated noise covariance matrix, and then estimates the RTF vector of the second speaker based on a singular value decomposition.When using the estimated RTF vectors of both speakers in a linearly constrained minimum variance beamformer, simulation results using real-world recordings for multiple speaker positions show that the proposed CBW method outperforms the conventional BOP and covariance whitening methods in terms of signal-to-interferer-and-noise ratio improvement.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/25/eess.AS_2023_10_25/" data-id="clorjzlec012jf1886va6em6o" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_10_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/25/cs.CV_2023_10_25/" class="article-date">
  <time datetime="2023-10-25T13:00:00.000Z" itemprop="datePublished">2023-10-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/25/cs.CV_2023_10_25/">cs.CV - 2023-10-25</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Exploring-Question-Decomposition-for-Zero-Shot-VQA"><a href="#Exploring-Question-Decomposition-for-Zero-Shot-VQA" class="headerlink" title="Exploring Question Decomposition for Zero-Shot VQA"></a>Exploring Question Decomposition for Zero-Shot VQA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17050">http://arxiv.org/abs/2310.17050</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zaid Khan, Vijay Kumar BG, Samuel Schulter, Manmohan Chandraker, Yun Fu</li>
<li>for: 提高Visual Question Answering（VQA）任务的性能，使其更能够模仿人类的问答策略。</li>
<li>methods: 使用人类写好的问题分解策略，以及模型自动生成的问题分解策略，从示例 alone 学习两种任务。</li>
<li>results: 在八个VQA任务上，通过选择性地使用模型生成的问题分解策略，可以提高准确率，包括在医疗VQA任务上提高 &gt;20%，并将BLIP-2的零基础性能提高到Winoground任务上的VQA重新定义任务上。<details>
<summary>Abstract</summary>
Visual question answering (VQA) has traditionally been treated as a single-step task where each question receives the same amount of effort, unlike natural human question-answering strategies. We explore a question decomposition strategy for VQA to overcome this limitation. We probe the ability of recently developed large vision-language models to use human-written decompositions and produce their own decompositions of visual questions, finding they are capable of learning both tasks from demonstrations alone. However, we show that naive application of model-written decompositions can hurt performance. We introduce a model-driven selective decomposition approach for second-guessing predictions and correcting errors, and validate its effectiveness on eight VQA tasks across three domains, showing consistent improvements in accuracy, including improvements of >20% on medical VQA datasets and boosting the zero-shot performance of BLIP-2 above chance on a VQA reformulation of the challenging Winoground task. Project Site: https://zaidkhan.me/decomposition-0shot-vqa/
</details>
<details>
<summary>摘要</summary>
视觉问答 (VQA) 传统上被视为一个单步任务，每个问题都receives the same amount of effort，与人类问答策略不同。我们explore一种问题分解策略来超越这一限制。我们 probed recently developed large vision-language models的能力使用人类写的分解和生成自己的分解，发现它们可以从示例 alone learn both tasks。然而，我们发现直接使用模型写的分解可以伤性表现。我们引入一种模型驱动的选择性分解方法，用于second-guessing predictions和 corrected errors，并在八个 VQA任务上三个领域进行了验证，显示了一致性提高，包括 >20%的提高在医学 VQA数据集上和在Winoground任务上超过Random guess的VQA重新定义提高。项目网站：https://zaidkhan.me/decomposition-0shot-vqa/
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Text-Spotter-for-Joint-Text-Spotting-and-Layout-Analysis"><a href="#Hierarchical-Text-Spotter-for-Joint-Text-Spotting-and-Layout-Analysis" class="headerlink" title="Hierarchical Text Spotter for Joint Text Spotting and Layout Analysis"></a>Hierarchical Text Spotter for Joint Text Spotting and Layout Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17674">http://arxiv.org/abs/2310.17674</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shangbang Long, Siyang Qin, Yasuhisa Fujii, Alessandro Bissacco, Michalis Raptis</li>
<li>for: 这个论文是为了解决文本检测和几何布局分析的联合任务而设计的。</li>
<li>methods: 论文使用了两个新的组件：一个叫做Unified-Detector-Polygon (UDP)，它生成了文本线的贝塞尔曲线 polygon，并生成了段落之间的亲和力矩阵; 另一个叫做Line-to-Character-to-Word (L2C2W) recognizer，它将分割出来的行转换为字符，并将字符合并回到单词中。</li>
<li>results: 论文在多个单词文本检测标准数据集上达到了状态之Art Results，以及几何布局分析任务中的优秀成绩。<details>
<summary>Abstract</summary>
We propose Hierarchical Text Spotter (HTS), a novel method for the joint task of word-level text spotting and geometric layout analysis. HTS can recognize text in an image and identify its 4-level hierarchical structure: characters, words, lines, and paragraphs. The proposed HTS is characterized by two novel components: (1) a Unified-Detector-Polygon (UDP) that produces Bezier Curve polygons of text lines and an affinity matrix for paragraph grouping between detected lines; (2) a Line-to-Character-to-Word (L2C2W) recognizer that splits lines into characters and further merges them back into words. HTS achieves state-of-the-art results on multiple word-level text spotting benchmark datasets as well as geometric layout analysis tasks.
</details>
<details>
<summary>摘要</summary>
我们提出了层次文本检测器（HTS），一种新的方法，用于同时解决单词水平文本检测和几何布局分析任务。HTS可以在图像中识别文本并识别其4级层次结构：字符、单词、行和段落。提出的HTS具有两个新的组成部分：（1）一个统一探测器多边形（UDP），生成文本线的贝塞尔曲线多边形和行间文本段落相互关系矩阵;（2）一个字行字符识别器（L2C2W），将行分成字符并将其再次合并回words。HTS在多个单词文本检测数据集和几何布局分析任务上达到了现状之冠的结果。
</details></li>
</ul>
<hr>
<h2 id="Trust-but-Verify-Robust-Image-Segmentation-using-Deep-Learning"><a href="#Trust-but-Verify-Robust-Image-Segmentation-using-Deep-Learning" class="headerlink" title="Trust, but Verify: Robust Image Segmentation using Deep Learning"></a>Trust, but Verify: Robust Image Segmentation using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16999">http://arxiv.org/abs/2310.16999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fahim Ahmed Zaman, Xiaodong Wu, Weiyu Xu, Milan Sonka, Raghuraman Mudumbai</li>
<li>for: 验证深度神经网络的医学图像分割输出的可靠性，抗性 against 随机和最坏情况的攻击。</li>
<li>methods: 基于作者提出的“信任，但验证”方法，使用助记网络生成受Mask的特征预测，并将其与原始图像进行比较，以检测错误分割。</li>
<li>results: 比较于之前的方法，新的验证网络设计可以减少假阳性（错误地判断正确分割为错误分割），并且在不同类型的攻击下保持高度的可靠性。<details>
<summary>Abstract</summary>
We describe a method for verifying the output of a deep neural network for medical image segmentation that is robust to several classes of random as well as worst-case perturbations i.e. adversarial attacks. This method is based on a general approach recently developed by the authors called "Trust, but Verify" wherein an auxiliary verification network produces predictions about certain masked features in the input image using the segmentation as an input. A well-designed auxiliary network will produce high-quality predictions when the input segmentations are accurate, but will produce low-quality predictions when the segmentations are incorrect. Checking the predictions of such a network with the original image allows us to detect bad segmentations. However, to ensure the verification method is truly robust, we need a method for checking the quality of the predictions that does not itself rely on a black-box neural network. Indeed, we show that previous methods for segmentation evaluation that do use deep neural regression networks are vulnerable to false negatives i.e. can inaccurately label bad segmentations as good. We describe the design of a verification network that avoids such vulnerability and present results to demonstrate its robustness compared to previous methods.
</details>
<details>
<summary>摘要</summary>
我们描述了一种用于验证深度神经网络医学图像分割结果的方法，该方法具有对多种随机和最差情况的抗击性，即抗击攻击。该方法基于我们最近开发的“信任，但验证”方法，其中一个辅助验证网络生成了基于输入图像的掩码特征的预测结果。如果输入分割结果正确，那么这个辅助网络会生成高质量的预测结果；如果输入分割结果错误，那么辅助网络会生成低质量的预测结果。通过对辅助网络的预测结果与原始图像进行比较，我们可以检测出错误的分割结果。但是，为了确保验证方法的真正可靠性，我们需要一种不依赖于黑obox神经网络的方法来检查预测结果的质量。我们显示了先前用于分割评估的深度神经回归网络方法存在false negative问题，即可能错别错误地将错误的分割结果标记为正确的。我们描述了一种避免这种敏感性的验证网络的设计，并提供了证明其稳定性的结果。
</details></li>
</ul>
<hr>
<h2 id="An-Efficient-Deep-Learning-based-approach-for-Recognizing-Agricultural-Pests-in-the-Wild"><a href="#An-Efficient-Deep-Learning-based-approach-for-Recognizing-Agricultural-Pests-in-the-Wild" class="headerlink" title="An Efficient Deep Learning-based approach for Recognizing Agricultural Pests in the Wild"></a>An Efficient Deep Learning-based approach for Recognizing Agricultural Pests in the Wild</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16991">http://arxiv.org/abs/2310.16991</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mohtasimhadi/An-Efficient-Deep-Learning-Based-Approach-for-Recognizing-Agricultural-Pests-in-the-Wild">https://github.com/mohtasimhadi/An-Efficient-Deep-Learning-Based-Approach-for-Recognizing-Agricultural-Pests-in-the-Wild</a></li>
<li>paper_authors: Mohtasim Hadi Rafi, Mohammad Ratul Mahjabin, Md Sabbir Rahman</li>
<li>for: 本研究旨在帮助农民防治垂直病虫，提高农业产量和经济效益。</li>
<li>methods: 本研究使用了数据转移学习、微调和自定义建 Architecture，实现了蜂群识别的高精度和可靠性。</li>
<li>results: 实验结果表明，使用我们提出的方法可以准确地识别各种蜂群，并且在不同的数据集上都具有良好的 robustness。<details>
<summary>Abstract</summary>
One of the biggest challenges that the farmers go through is to fight insect pests during agricultural product yields. The problem can be solved easily and avoid economic losses by taking timely preventive measures. This requires identifying insect pests in an easy and effective manner. Most of the insect species have similarities between them. Without proper help from the agriculturist academician it is very challenging for the farmers to identify the crop pests accurately. To address this issue we have done extensive experiments considering different methods to find out the best method among all. This paper presents a detailed overview of the experiments done on mainly a robust dataset named IP102 including transfer learning with finetuning, attention mechanism and custom architecture. Some example from another dataset D0 is also shown to show robustness of our experimented techniques.
</details>
<details>
<summary>摘要</summary>
一个最大的问题是农民面临的是在农产品收成时遇到昆虫害。这个问题可以轻松解决，避免经济损失，通过及时预防措施。这需要识别昆虫害的方法。大多数昆虫种类之间有相似之处。如果没有专业农业学家的帮助，农民很难准确识别作物害虫。为解决这个问题，我们在不同方法的基础上进行了广泛的实验。这篇论文提供了对我们实验的详细概述，包括转移学习与细化、注意机制和定制架构。另外，我们还提供了另一个数据集D0中的一些示例，以示我们的实验技术的稳定性。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Domain-Adaptation-for-Semantic-Segmentation-with-Pseudo-Label-Self-Refinement"><a href="#Unsupervised-Domain-Adaptation-for-Semantic-Segmentation-with-Pseudo-Label-Self-Refinement" class="headerlink" title="Unsupervised Domain Adaptation for Semantic Segmentation with Pseudo Label Self-Refinement"></a>Unsupervised Domain Adaptation for Semantic Segmentation with Pseudo Label Self-Refinement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16979">http://arxiv.org/abs/2310.16979</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingchen Zhao, Niluthpol Chowdhury Mithun, Abhinav Rajvanshi, Han-Pang Chiu, Supun Samarasekera</li>
<li>for: 提高深度学习基于 semantic segmentation 模型在不同特征集上的性能，尤其是在实际应用环境中。</li>
<li>methods: 使用教师模型生成 pseudo-标签，并使用学生模型在新数据上进行自教育。auxiliary pseudo-label refinement network (PRN) 用于在不同阶段进行pseudo标签的修正和选择高可靠的标签。</li>
<li>results: 在多个 benchmark 数据集上，我们的方法比前一个状态的方法表现出了显著的改善， indicating that our approach can effectively improve the robustness of segmentation models against pseudo label noise propagation during different stages of adaptation.<details>
<summary>Abstract</summary>
Deep learning-based solutions for semantic segmentation suffer from significant performance degradation when tested on data with different characteristics than what was used during the training. Adapting the models using annotated data from the new domain is not always practical. Unsupervised Domain Adaptation (UDA) approaches are crucial in deploying these models in the actual operating conditions. Recent state-of-the-art (SOTA) UDA methods employ a teacher-student self-training approach, where a teacher model is used to generate pseudo-labels for the new data which in turn guide the training process of the student model. Though this approach has seen a lot of success, it suffers from the issue of noisy pseudo-labels being propagated in the training process. To address this issue, we propose an auxiliary pseudo-label refinement network (PRN) for online refining of the pseudo labels and also localizing the pixels whose predicted labels are likely to be noisy. Being able to improve the quality of pseudo labels and select highly reliable ones, PRN helps self-training of segmentation models to be robust against pseudo label noise propagation during different stages of adaptation. We evaluate our approach on benchmark datasets with three different domain shifts, and our approach consistently performs significantly better than the previous state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:深度学习基于的 semantic segmentation 模型在测试数据上uffer from 性能下降，特别是当数据的特征与训练时使用的数据不同时。 adapting 模型使用新Domain的标注数据不always practical. Unsupervised Domain Adaptation (UDA) 方法是部署这些模型的关键。 recent state-of-the-art (SOTA) UDA 方法使用教师模型生成新数据上的pseudo-labels，并使用这些pseudo-labels来导导学生模型的训练过程。  Although this approach has seen a lot of success, it suffers from the issue of noisy pseudo-labels being propagated in the training process. To address this issue, we propose an auxiliary pseudo-label refinement network (PRN) for online refining of the pseudo labels and also localizing the pixels whose predicted labels are likely to be noisy. PRN can improve the quality of pseudo labels and select highly reliable ones, which helps self-training of segmentation models to be robust against pseudo label noise propagation during different stages of adaptation. We evaluate our approach on benchmark datasets with three different domain shifts, and our approach consistently performs significantly better than the previous state-of-the-art methods.
</details></li>
</ul>
<hr>
<h2 id="Improving-Performance-in-Colorectal-Cancer-Histology-Decomposition-using-Deep-and-Ensemble-Machine-Learning"><a href="#Improving-Performance-in-Colorectal-Cancer-Histology-Decomposition-using-Deep-and-Ensemble-Machine-Learning" class="headerlink" title="Improving Performance in Colorectal Cancer Histology Decomposition using Deep and Ensemble Machine Learning"></a>Improving Performance in Colorectal Cancer Histology Decomposition using Deep and Ensemble Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16954">http://arxiv.org/abs/2310.16954</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabi Prezja, Leevi Annala, Sampsa Kiiskinen, Suvi Lahtinen, Timo Ojala, Pekka Ruusuvuori, Teijo Kuopio</li>
<li>for: This paper aims to explore the potential of convolutional neural networks (CNNs) in facilitating the extraction of clinically relevant biomarkers from histologic samples for colorectal cancer management.</li>
<li>methods: The authors use a hybrid Deep and ensemble machine learning model to classify diverse tissue types from whole slide microscope images accurately, which is critical for amplifying the prognostic potential of imaging-based biomarkers.</li>
<li>results: The model achieved 96.74% accuracy on the external test set and 99.89% on the internal test set, demonstrating its high accuracy and potential for clinical application.<details>
<summary>Abstract</summary>
In routine colorectal cancer management, histologic samples stained with hematoxylin and eosin are commonly used. Nonetheless, their potential for defining objective biomarkers for patient stratification and treatment selection is still being explored. The current gold standard relies on expensive and time-consuming genetic tests. However, recent research highlights the potential of convolutional neural networks (CNNs) in facilitating the extraction of clinically relevant biomarkers from these readily available images. These CNN-based biomarkers can predict patient outcomes comparably to golden standards, with the added advantages of speed, automation, and minimal cost. The predictive potential of CNN-based biomarkers fundamentally relies on the ability of convolutional neural networks (CNNs) to classify diverse tissue types from whole slide microscope images accurately. Consequently, enhancing the accuracy of tissue class decomposition is critical to amplifying the prognostic potential of imaging-based biomarkers. This study introduces a hybrid Deep and ensemble machine learning model that surpassed all preceding solutions for this classification task. Our model achieved 96.74% accuracy on the external test set and 99.89% on the internal test set. Recognizing the potential of these models in advancing the task, we have made them publicly available for further research and development.
</details>
<details>
<summary>摘要</summary>
Routine colorectal cancer management 通常使用 Hematoxylin 和 Eosin 染色的 histologic samples，但是它们的潜在作用还在探索中。目前的黄金标准是使用 expensive 和 time-consuming 的遗传学测试。然而，最近的研究表明，扩散 нейрон网络 (CNN) 可以帮助从readily available 的图像中提取临床 relevance 的生物标志物。这些 CNN-based 生物标志物可以与黄金标准相比，预测患者的结果，并且具有速度、自动化和成本低的优势。生物标志物的预测潜力基于 CNN 的能力准确地分类不同的组织类型从整个染色microscope 图像中。因此，提高染色microscope 图像中组织类型的准确性是关键，以激活生物标志物的预测潜力。本研究提出了一种 Hybrid Deep 和ensemble 机器学习模型，超过了所有之前的解决方案。我们的模型在 external 测试集上达到了 96.74% 的准确率，并在 internal 测试集上达到了 99.89%。认可这些模型在任务上的潜力，我们将其公开发布，以便进一步的研究和发展。
</details></li>
</ul>
<hr>
<h2 id="Diagnosing-Alzheimer’s-Disease-using-Early-Late-Multimodal-Data-Fusion-with-Jacobian-Maps"><a href="#Diagnosing-Alzheimer’s-Disease-using-Early-Late-Multimodal-Data-Fusion-with-Jacobian-Maps" class="headerlink" title="Diagnosing Alzheimer’s Disease using Early-Late Multimodal Data Fusion with Jacobian Maps"></a>Diagnosing Alzheimer’s Disease using Early-Late Multimodal Data Fusion with Jacobian Maps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16936">http://arxiv.org/abs/2310.16936</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yasmine Mustafa, Tie Luo<br>for: 这篇论文主要旨在提出一种高效的早期-晚期 fusión（ELF）方法，用于早期识别阿尔茨海默病（AD）的四个阶段。methods: 该方法使用了一个 convolutional neural network（CNN）来自动提取特征，并使用 random forests 来实现小型数据集上的竞争性表现。此外，该方法还提出了一种可靠的预处理管道，该管道可以适应个体Subject的特有特征，并使用整个大脑图像而不是切片或质量图像来进行预处理。results: 在使用 OASIS-3  dataset 的 MRI 和 CT 图像上进行实验，该方法可以准确地将 AD 分类为四个阶段，准确率达到 97.19%。<details>
<summary>Abstract</summary>
Alzheimer's disease (AD) is a prevalent and debilitating neurodegenerative disorder impacting a large aging population. Detecting AD in all its presymptomatic and symptomatic stages is crucial for early intervention and treatment. An active research direction is to explore machine learning methods that harness multimodal data fusion to outperform human inspection of medical scans. However, existing multimodal fusion models have limitations, including redundant computation, complex architecture, and simplistic handling of missing data. Moreover, the preprocessing pipelines of medical scans remain inadequately detailed and are seldom optimized for individual subjects. In this paper, we propose an efficient early-late fusion (ELF) approach, which leverages a convolutional neural network for automated feature extraction and random forests for their competitive performance on small datasets. Additionally, we introduce a robust preprocessing pipeline that adapts to the unique characteristics of individual subjects and makes use of whole brain images rather than slices or patches. Moreover, to tackle the challenge of detecting subtle changes in brain volume, we transform images into the Jacobian domain (JD) to enhance both accuracy and robustness in our classification. Using MRI and CT images from the OASIS-3 dataset, our experiments demonstrate the effectiveness of the ELF approach in classifying AD into four stages with an accuracy of 97.19%.
</details>
<details>
<summary>摘要</summary>
阿尔茨海默病 (AD) 是一种广泛存在并且严重影响老龄人口的神经退化疾病。早期发现 AD 的检测是非常重要，以便提供早期 intervención 和治疗。目前的研究方向之一是利用机器学习方法，把多modal 数据融合以超越人工检查医疗影像。然而，现有的多modal 融合模型受到一些限制，包括重复计算、复杂的架构和简单处理缺失数据。此外，医疗影像的预处理管道仍然不够详细，通常不会适应个体Subject 的特点。在这篇论文中，我们提出了一种高效的早期晚期融合 (ELF) 方法，利用 convolutional neural network (CNN) 自动提取特征，并使用 random forest 来实现小型数据集的竞争性表现。此外，我们还引入了一种可靠的预处理管道，该管道适应每个个体Subject 的特点，并使用整个大脑图像而不是切片或贴图。此外，为了解决检测大脑体积的微小变化的挑战，我们将图像转换为 Jacobian 域 (JD)，以提高精度和可靠性的分类。使用 MRI 和 CT 图像从 OASIS-3 数据集，我们的实验表明 ELF 方法可以在四个 AD 阶段中分类，准确率达 97.19%。
</details></li>
</ul>
<hr>
<h2 id="MCUFormer-Deploying-Vision-Tranformers-on-Microcontrollers-with-Limited-Memory"><a href="#MCUFormer-Deploying-Vision-Tranformers-on-Microcontrollers-with-Limited-Memory" class="headerlink" title="MCUFormer: Deploying Vision Tranformers on Microcontrollers with Limited Memory"></a>MCUFormer: Deploying Vision Tranformers on Microcontrollers with Limited Memory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16898">http://arxiv.org/abs/2310.16898</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liangyn22/mcuformer">https://github.com/liangyn22/mcuformer</a></li>
<li>paper_authors: Yinan Liang, Ziwei Wang, Xiuwei Xu, Yansong Tang, Jie Zhou, Jiwen Lu</li>
<li>for: 这篇论文旨在实现深度学习模型的部署在Internet of Things（IoT）设备上，例如微控制器，以减少成本和能源消耗。</li>
<li>methods: 本篇论文提出了一个硬件-算法共优化方法，名为MCUFormer，以便在微控制器上部署视觉 трансформа器，并且将其应用于图像识别 tasks。</li>
<li>results: 实验结果显示，使用MCUFormer可以在STM32F746微控制器上实现73.62%的top-1精度（在ImageNet图像识别任务中），并且仅需320KB的内存。<details>
<summary>Abstract</summary>
Due to the high price and heavy energy consumption of GPUs, deploying deep models on IoT devices such as microcontrollers makes significant contributions for ecological AI. Conventional methods successfully enable convolutional neural network inference of high resolution images on microcontrollers, while the framework for vision transformers that achieve the state-of-the-art performance in many vision applications still remains unexplored. In this paper, we propose a hardware-algorithm co-optimizations method called MCUFormer to deploy vision transformers on microcontrollers with extremely limited memory, where we jointly design transformer architecture and construct the inference operator library to fit the memory resource constraint. More specifically, we generalize the one-shot network architecture search (NAS) to discover the optimal architecture with highest task performance given the memory budget from the microcontrollers, where we enlarge the existing search space of vision transformers by considering the low-rank decomposition dimensions and patch resolution for memory reduction. For the construction of the inference operator library of vision transformers, we schedule the memory buffer during inference through operator integration, patch embedding decomposition, and token overwriting, allowing the memory buffer to be fully utilized to adapt to the forward pass of the vision transformer. Experimental results demonstrate that our MCUFormer achieves 73.62\% top-1 accuracy on ImageNet for image classification with 320KB memory on STM32F746 microcontroller. Code is available at https://github.com/liangyn22/MCUFormer.
</details>
<details>
<summary>摘要</summary>
Due to the high price and heavy energy consumption of GPUs, deploying deep models on IoT devices such as microcontrollers makes significant contributions for ecological AI. Conventional methods successfully enable convolutional neural network inference of high resolution images on microcontrollers, while the framework for vision transformers that achieve the state-of-the-art performance in many vision applications still remains unexplored. In this paper, we propose a hardware-algorithm co-optimizations method called MCUFormer to deploy vision transformers on microcontrollers with extremely limited memory, where we jointly design transformer architecture and construct the inference operator library to fit the memory resource constraint. More specifically, we generalize the one-shot network architecture search (NAS) to discover the optimal architecture with highest task performance given the memory budget from the microcontrollers, where we enlarge the existing search space of vision transformers by considering the low-rank decomposition dimensions and patch resolution for memory reduction. For the construction of the inference operator library of vision transformers, we schedule the memory buffer during inference through operator integration, patch embedding decomposition, and token overwriting, allowing the memory buffer to be fully utilized to adapt to the forward pass of the vision transformer. Experimental results demonstrate that our MCUFormer achieves 73.62% top-1 accuracy on ImageNet for image classification with 320KB memory on STM32F746 microcontroller. Code is available at https://github.com/liangyn22/MCUFormer.
</details></li>
</ul>
<hr>
<h2 id="SparseDFF-Sparse-View-Feature-Distillation-for-One-Shot-Dexterous-Manipulation"><a href="#SparseDFF-Sparse-View-Feature-Distillation-for-One-Shot-Dexterous-Manipulation" class="headerlink" title="SparseDFF: Sparse-View Feature Distillation for One-Shot Dexterous Manipulation"></a>SparseDFF: Sparse-View Feature Distillation for One-Shot Dexterous Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16838">http://arxiv.org/abs/2310.16838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qianxu Wang, Haotong Zhang, Congyue Deng, Yang You, Hao Dong, Yixin Zhu, Leonidas Guibas</li>
<li>for: 将 robot 给授予高水平的 Semantic Understanding，以便在3D场景中进行高级的物体捕捉和操作。</li>
<li>methods: 我们运用大量2D Computer Vision模型，将多视图图像中的semantic feature概念传递到3D场景中，以建立一个Distilled Feature Field（DFF）。</li>
<li>results: 我们的方法可以从简单的RGBD观察中获取高水平的3D DFF，并且可以在不同的物体和场景下进行一次性学习，并且能够在不同的物体和场景下进行传授。<details>
<summary>Abstract</summary>
Humans excel at transferring manipulation skills across diverse object shapes, poses, and appearances due to their understanding of semantic correspondences between different instances. To endow robots with a similar high-level understanding, we develop a Distilled Feature Field (DFF) for 3D scenes, leveraging large 2D vision models to distill semantic features from multiview images. While current research demonstrates advanced performance in reconstructing DFFs from dense views, the development of learning a DFF from sparse views is relatively nascent, despite its prevalence in numerous manipulation tasks with fixed cameras. In this work, we introduce SparseDFF, a novel method for acquiring view-consistent 3D DFFs from sparse RGBD observations, enabling one-shot learning of dexterous manipulations that are transferable to novel scenes. Specifically, we map the image features to the 3D point cloud, allowing for propagation across the 3D space to establish a dense feature field. At the core of SparseDFF is a lightweight feature refinement network, optimized with a contrastive loss between pairwise views after back-projecting the image features onto the 3D point cloud. Additionally, we implement a point-pruning mechanism to augment feature continuity within each local neighborhood. By establishing coherent feature fields on both source and target scenes, we devise an energy function that facilitates the minimization of feature discrepancies w.r.t. the end-effector parameters between the demonstration and the target manipulation. We evaluate our approach using a dexterous hand, mastering real-world manipulations on both rigid and deformable objects, and showcase robust generalization in the face of object and scene-context variations.
</details>
<details>
<summary>摘要</summary>
人类具有将抓取技能转移到多种物体形状、姿态和外观的能力，这是因为他们对不同实例之间的 semantic 匹配有深刻的理解。为了赋予机器人类似的高级理解，我们开发了一种 Distilled Feature Field (DFF) for 3D 场景，利用大量 2D 视觉模型来精炼 semantic 特征从多视图图像中。当前研究已经实现了高级的 DFF 重建从密集视图中，但是对于从稀疏视图学习 DFF 的开发还是相对落后，尽管这种情况在许多抓取任务中具有广泛的应用。在这项工作中，我们介绍了一种新的方法，即 SparseDFF，用于从稀疏 RGBD 观察中获取视元一致的 3D DFF，以便一次学习灵活的抓取动作，并将其应用到新的场景。我们将图像特征映射到 3D 点云上，以便在 3D 空间中进行特征场的传播。SparseDFF 的核心是一种轻量级的特征修正网络，通过对匹配视图之间的特征进行对比而优化。此外，我们还实现了一种点云杂除机制，以确保每个本地邻域内的特征连续性。通过在源场景和目标场景上建立一致的特征场，我们定义了一个能量函数，该函数使得在示例抓取动作和目标抓取动作之间的特征差异与机器人的终端参数之间的差异进行最小化。我们通过使用一个协助手臂来评估我们的方法，并在实际抓取任务中展示了对物体和场景变化的稳定性。
</details></li>
</ul>
<hr>
<h2 id="LightSpeed-Light-and-Fast-Neural-Light-Fields-on-Mobile-Devices"><a href="#LightSpeed-Light-and-Fast-Neural-Light-Fields-on-Mobile-Devices" class="headerlink" title="LightSpeed: Light and Fast Neural Light Fields on Mobile Devices"></a>LightSpeed: Light and Fast Neural Light Fields on Mobile Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16832">http://arxiv.org/abs/2310.16832</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lightspeed-r2l/lightspeed">https://github.com/lightspeed-r2l/lightspeed</a></li>
<li>paper_authors: Aarush Gupta, Junli Cao, Chaoyang Wang, Ju Hu, Sergey Tulyakov, Jian Ren, László A Jeni</li>
<li>for: 实现实时新视图图像生成在移动设备上，因为计算能力和存储空间有限制。</li>
<li>methods: 使用神经光场表示法，这些方法可以在移动设备上实现高品质的视图生成，神经光场方法直接将射线表示与像素颜色映射。</li>
<li>results: 我们发现使用光板表示是一种高效的射线表示，可以使用特征网格快速训练和渲染。我们的方法可以在非正面视图中进行扩展，并且比前一代光场方法提供更高品质的渲染和更好的速度比。<details>
<summary>Abstract</summary>
Real-time novel-view image synthesis on mobile devices is prohibitive due to the limited computational power and storage. Using volumetric rendering methods, such as NeRF and its derivatives, on mobile devices is not suitable due to the high computational cost of volumetric rendering. On the other hand, recent advances in neural light field representations have shown promising real-time view synthesis results on mobile devices. Neural light field methods learn a direct mapping from a ray representation to the pixel color. The current choice of ray representation is either stratified ray sampling or Plucker coordinates, overlooking the classic light slab (two-plane) representation, the preferred representation to interpolate between light field views. In this work, we find that using the light slab representation is an efficient representation for learning a neural light field. More importantly, it is a lower-dimensional ray representation enabling us to learn the 4D ray space using feature grids which are significantly faster to train and render. Although mostly designed for frontal views, we show that the light-slab representation can be further extended to non-frontal scenes using a divide-and-conquer strategy. Our method offers superior rendering quality compared to previous light field methods and achieves a significantly improved trade-off between rendering quality and speed.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:实时新视图图像合成在移动设备上是不可能的，由于限制的计算能力和存储空间。使用液体渲染方法，如NeRF和其 derivates，在移动设备上不适用，因为这些方法的计算成本过高。然而，最近的神经光场表示法已经展示了在移动设备上的实时视图合成结果。神经光场方法直接将光束表示转换为像素颜色。当前的光束表示方法有 stratified ray sampling 和 Plucker坐标，但我们提议使用光束坐标表示，这是一种更有效的、低维度的表示。这使得我们可以使用特征网格来学习4D光束空间。虽然主要设计为前视场景，但我们表明了使用光束坐标表示可以进一步扩展到非前视场景，使用分割和聚合策略。我们的方法提供了比前一代光场方法更高的图像质量和速度之间的改善交易。
</details></li>
</ul>
<hr>
<h2 id="PERF-Panoramic-Neural-Radiance-Field-from-a-Single-Panorama"><a href="#PERF-Panoramic-Neural-Radiance-Field-from-a-Single-Panorama" class="headerlink" title="PERF: Panoramic Neural Radiance Field from a Single Panorama"></a>PERF: Panoramic Neural Radiance Field from a Single Panorama</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16831">http://arxiv.org/abs/2310.16831</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/perf-project/PeRF">https://github.com/perf-project/PeRF</a></li>
<li>paper_authors: Guangcong Wang, Peng Wang, Zhaoxi Chen, Wenping Wang, Chen Change Loy, Ziwei Liu</li>
<li>for: 这 paper 的目的是提出一种基于单个扫描图的360度新视图合成方法，使得可以在复杂的场景中实现3D 浏览而不需要花费大量时间和精力进行图像采集。</li>
<li>methods: 这 paper 使用了一种 collaborative RGBD 填充方法和一种进行填充和 удали除方法来将2D 图像提升到3D 场景中。 Specifically,  authors first predict a panoramic depth map based on a single panorama and reconstruct visible 3D regions using volume rendering. Then, they introduce a collaborative RGBD inpainting approach into a NeRF to complete RGB images and depth maps from random views. Finally, they use an inpainting-and-erasing strategy to avoid inconsistent geometry between a newly-sampled view and reference views.</li>
<li>results: 作者们的方法可以在 Replica 和一个新的数据集 PERF-in-the-wild 上实现了比州-of-the-art 的性能。 Their method can be widely used for real-world applications, such as panorama-to-3D, text-to-3D, and 3D scene stylization applications.<details>
<summary>Abstract</summary>
Neural Radiance Field (NeRF) has achieved substantial progress in novel view synthesis given multi-view images. Recently, some works have attempted to train a NeRF from a single image with 3D priors. They mainly focus on a limited field of view with a few occlusions, which greatly limits their scalability to real-world 360-degree panoramic scenarios with large-size occlusions. In this paper, we present PERF, a 360-degree novel view synthesis framework that trains a panoramic neural radiance field from a single panorama. Notably, PERF allows 3D roaming in a complex scene without expensive and tedious image collection. To achieve this goal, we propose a novel collaborative RGBD inpainting method and a progressive inpainting-and-erasing method to lift up a 360-degree 2D scene to a 3D scene. Specifically, we first predict a panoramic depth map as initialization given a single panorama and reconstruct visible 3D regions with volume rendering. Then we introduce a collaborative RGBD inpainting approach into a NeRF for completing RGB images and depth maps from random views, which is derived from an RGB Stable Diffusion model and a monocular depth estimator. Finally, we introduce an inpainting-and-erasing strategy to avoid inconsistent geometry between a newly-sampled view and reference views. The two components are integrated into the learning of NeRFs in a unified optimization framework and achieve promising results. Extensive experiments on Replica and a new dataset PERF-in-the-wild demonstrate the superiority of our PERF over state-of-the-art methods. Our PERF can be widely used for real-world applications, such as panorama-to-3D, text-to-3D, and 3D scene stylization applications. Project page and code are available at https://perf-project.github.io/ and https://github.com/perf-project/PeRF.
</details>
<details>
<summary>摘要</summary>
neural radiance field (NeRF) 已经取得了在新视图合成方面的显著进步，尤其是使用多视图图像。在这篇文章中，我们介绍了一种名为 PeRF 的全景新视图合成框架，可以从单个全景图像中训练一个全景 NeRF。与传统的方法不同的是，PeRF 允许无需昂贵和繁琐的图像采集，就能够在实际世界中360度的全景enario中进行3D镜头游走。为了实现这一目标，我们提出了一种新的协同 RGBD 填充方法和一种进行填充和取消填充的策略，以将2D场景提升到3D场景。具体来说，我们首先从单个全景图像中预测了全景深度图，并使用可视3D区域的volume rendering重建可见的3D区域。然后，我们引入了协同 RGBD 填充方法，以完成来自Random View的 RGB 图像和深度图的完善。最后，我们引入了一种填充和取消填充策略，以避免在新采集的视角和参考视角之间出现不一致的几何结构。这两种组件被集成到了 NeRF 的学习框架中，并实现了良好的结果。我们的 PeRF 可以广泛应用于实际应用场景，如全景图像到3D、文本到3D和3D场景风格化应用。项目页面和代码可以在 <https://perf-project.github.io/> 和 <https://github.com/perf-project/PeRF> 上找到。
</details></li>
</ul>
<hr>
<h2 id="CommonCanvas-An-Open-Diffusion-Model-Trained-with-Creative-Commons-Images"><a href="#CommonCanvas-An-Open-Diffusion-Model-Trained-with-Creative-Commons-Images" class="headerlink" title="CommonCanvas: An Open Diffusion Model Trained with Creative-Commons Images"></a>CommonCanvas: An Open Diffusion Model Trained with Creative-Commons Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16825">http://arxiv.org/abs/2310.16825</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mosaicml/diffusion">https://github.com/mosaicml/diffusion</a></li>
<li>paper_authors: Aaron Gokaslan, A. Feder Cooper, Jasmine Collins, Landan Seguin, Austin Jacobson, Mihir Patel, Jonathan Frankle, Cory Stephenson, Volodymyr Kuleshov</li>
<li>for: 这个论文是用于训练一种基于开源扩散模型的文本到图像生成模型的。</li>
<li>methods: 这个论文使用了一种启发式传输学习技术来生成高质量的 sintetic caption，并使用了一种数据和计算效率的训练策略来训练模型。</li>
<li>results: 这个论文通过使用高质量的 sintetic caption和数据和计算效率的训练策略，成功地训练出一些高质量的文本到图像生成模型，并且这些模型的性能与LAION-2B数据集上训练的SD2模型相当。<details>
<summary>Abstract</summary>
We assemble a dataset of Creative-Commons-licensed (CC) images, which we use to train a set of open diffusion models that are qualitatively competitive with Stable Diffusion 2 (SD2). This task presents two challenges: (1) high-resolution CC images lack the captions necessary to train text-to-image generative models; (2) CC images are relatively scarce. In turn, to address these challenges, we use an intuitive transfer learning technique to produce a set of high-quality synthetic captions paired with curated CC images. We then develop a data- and compute-efficient training recipe that requires as little as 3% of the LAION-2B data needed to train existing SD2 models, but obtains comparable quality. These results indicate that we have a sufficient number of CC images (~70 million) for training high-quality models. Our training recipe also implements a variety of optimizations that achieve ~3X training speed-ups, enabling rapid model iteration. We leverage this recipe to train several high-quality text-to-image models, which we dub the CommonCanvas family. Our largest model achieves comparable performance to SD2 on a human evaluation, despite being trained on our CC dataset that is significantly smaller than LAION and using synthetic captions for training. We release our models, data, and code at https://github.com/mosaicml/diffusion/blob/main/assets/common-canvas.md
</details>
<details>
<summary>摘要</summary>
我团队 assemble 一个 Creative-Commons-licensed（CC）图像集，用于训练一些开放扩散模型，这些模型与 Stable Diffusion 2（SD2）相比质量相似。这个任务存在两个挑战：（1）高分辨率 CC 图像缺乏需要训练文本到图像生成模型的描述;（2） CC 图像相对罕见。为了解决这些挑战，我们使用一种直观转移学习技术生成高质量的 sintetic 描述，并将它们分配到我们精心选择的 CC 图像上。然后，我们开发了一种数据和计算效率高的训练方法，只需要使用 LAION-2B 数据的 3%，但可以获得相似的质量。这些结果表明我们有足够的 CC 图像（约 70 万） для训练高质量模型。我们的训练方法还实现了多种优化，实现了 ~3X 的训练速度提升，以便快速进行模型迭代。我们利用这种方法训练了一些高质量的文本到图像模型，我们称之为 CommonCanvas 家族。我们最大的模型在人类评估中与 SD2 相比，即使是在我们小于 LAION 的 CC 数据集上训练，也达到了相似的性能。我们将我们的模型、数据和代码发布在 GitHub 上，请参考 https://github.com/mosaicml/diffusion/blob/main/assets/common-canvas.md。
</details></li>
</ul>
<hr>
<h2 id="DreamCraft3D-Hierarchical-3D-Generation-with-Bootstrapped-Diffusion-Prior"><a href="#DreamCraft3D-Hierarchical-3D-Generation-with-Bootstrapped-Diffusion-Prior" class="headerlink" title="DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion Prior"></a>DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion Prior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16818">http://arxiv.org/abs/2310.16818</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deepseek-ai/dreamcraft3d">https://github.com/deepseek-ai/dreamcraft3d</a></li>
<li>paper_authors: Jingxiang Sun, Bo Zhang, Ruizhi Shao, Lizhen Wang, Wen Liu, Zhenda Xie, Yebin Liu</li>
<li>for: 这篇论文旨在提出一种基于引用图像的层次3D内容生成方法，以生成高品质和一致的3D对象。</li>
<li>methods: 作者使用了geometry sculpting和texture boosting两个阶段来生成3D对象，并通过score distillation sampling和 Bootstrapped Score Distillation来保证geometry和texture的一致性。</li>
<li>results: 通过 alternating optimization of the diffusion prior and 3D scene representation，作者实现了mutually reinforcing improvements，并达到了高品质的rendering和一致的3D对象生成。<details>
<summary>Abstract</summary>
We present DreamCraft3D, a hierarchical 3D content generation method that produces high-fidelity and coherent 3D objects. We tackle the problem by leveraging a 2D reference image to guide the stages of geometry sculpting and texture boosting. A central focus of this work is to address the consistency issue that existing works encounter. To sculpt geometries that render coherently, we perform score distillation sampling via a view-dependent diffusion model. This 3D prior, alongside several training strategies, prioritizes the geometry consistency but compromises the texture fidelity. We further propose Bootstrapped Score Distillation to specifically boost the texture. We train a personalized diffusion model, Dreambooth, on the augmented renderings of the scene, imbuing it with 3D knowledge of the scene being optimized. The score distillation from this 3D-aware diffusion prior provides view-consistent guidance for the scene. Notably, through an alternating optimization of the diffusion prior and 3D scene representation, we achieve mutually reinforcing improvements: the optimized 3D scene aids in training the scene-specific diffusion model, which offers increasingly view-consistent guidance for 3D optimization. The optimization is thus bootstrapped and leads to substantial texture boosting. With tailored 3D priors throughout the hierarchical generation, DreamCraft3D generates coherent 3D objects with photorealistic renderings, advancing the state-of-the-art in 3D content generation. Code available at https://github.com/deepseek-ai/DreamCraft3D.
</details>
<details>
<summary>摘要</summary>
我们介绍 DreamCraft3D，一种层次的三维内容生成方法，可以生成高准确性和一致的三维对象。我们解决了一致性问题，通过利用二维参考图来导航几个阶段的几何雕刻和текстура增强。我们的中心焦点是解决现有工作中的一致性问题，通过视图依赖的扩散模型进行得分采样。这个三维优先， alongside 多种训练策略，优先几何一致性，但是妥协текстура准确性。我们还提议了 Bootstrapped Score Distillation，用于特性增强。我们在增强的场景下训练了个性化的扩散模型， Dreambooth，使其具有场景的3D知识。得分采样从这个3D-意识扩散先进提供了视图一致的指导，为场景优化提供了视图一致的指导。通过场景优化和扩散先进的交互优化，我们实现了相互扶持的改进：优化的3D场景帮助了场景特定的扩散模型训练，这些模型提供了越来越视图一致的指导，以便为3D优化提供越来越高的指导。因此，我们可以通过层次的生成来生成一致的3D对象，并且可以提高图像的精度和一致性，从而提高3D内容生成的状态。代码可以在https://github.com/deepseek-ai/DreamCraft3D 中找到。
</details></li>
</ul>
<hr>
<h2 id="Exploring-OCR-Capabilities-of-GPT-4V-ision-A-Quantitative-and-In-depth-Evaluation"><a href="#Exploring-OCR-Capabilities-of-GPT-4V-ision-A-Quantitative-and-In-depth-Evaluation" class="headerlink" title="Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and In-depth Evaluation"></a>Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and In-depth Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16809">http://arxiv.org/abs/2310.16809</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/scut-dlvclab/gpt-4v_ocr">https://github.com/scut-dlvclab/gpt-4v_ocr</a></li>
<li>paper_authors: Yongxin Shi, Dezhi Peng, Wenhui Liao, Zening Lin, Xinhong Chen, Chongyu Liu, Yuyi Zhang, Lianwen Jin</li>
<li>For: The paper evaluates the Optical Character Recognition (OCR) capabilities of the recently released GPT-4V(ision), a Large Multimodal Model (LMM), and assesses its performance across a range of OCR tasks.* Methods: The paper uses a comprehensive evaluation pipeline to test the model’s performance on scene text recognition, handwritten text recognition, handwritten mathematical expression recognition, table structure recognition, and information extraction from visually-rich documents.* Results: The evaluation reveals that GPT-4V performs well on recognizing and understanding Latin contents, but struggles with multilingual scenarios and complex tasks such as handwriting mathematical expression recognition, table structure recognition, and end-to-end semantic entity recognition and pair extraction from document image.<details>
<summary>Abstract</summary>
This paper presents a comprehensive evaluation of the Optical Character Recognition (OCR) capabilities of the recently released GPT-4V(ision), a Large Multimodal Model (LMM). We assess the model's performance across a range of OCR tasks, including scene text recognition, handwritten text recognition, handwritten mathematical expression recognition, table structure recognition, and information extraction from visually-rich document. The evaluation reveals that GPT-4V performs well in recognizing and understanding Latin contents, but struggles with multilingual scenarios and complex tasks. Specifically, it showed limitations when dealing with non-Latin languages and complex tasks such as handwriting mathematical expression recognition, table structure recognition, and end-to-end semantic entity recognition and pair extraction from document image. Based on these observations, we affirm the necessity and continued research value of specialized OCR models. In general, despite its versatility in handling diverse OCR tasks, GPT-4V does not outperform existing state-of-the-art OCR models. How to fully utilize pre-trained general-purpose LMMs such as GPT-4V for OCR downstream tasks remains an open problem. The study offers a critical reference for future research in OCR with LMMs. Evaluation pipeline and results are available at https://github.com/SCUT-DLVCLab/GPT-4V_OCR.
</details>
<details>
<summary>摘要</summary>
The evaluation reveals that GPT-4V performs well on recognizing and understanding Latin contents, but encounters challenges in multilingual scenarios and complex tasks. Specifically, it struggles with non-Latin languages and tasks such as handwriting mathematical expression recognition, table structure recognition, and end-to-end semantic entity recognition and pair extraction from document images. These findings highlight the importance and continued value of specialized OCR models.Although GPT-4V can handle diverse OCR tasks, it does not outperform existing state-of-the-art OCR models. The study underscores the need to explore how to fully utilize pre-trained general-purpose LMMs like GPT-4V for OCR downstream tasks. The research provides a valuable reference for future OCR research with LMMs. The evaluation pipeline and results are available on GitHub at <https://github.com/SCUT-DLVCLab/GPT-4V_OCR>.
</details></li>
</ul>
<hr>
<h2 id="Fingervein-Verification-using-Convolutional-Multi-Head-Attention-Network"><a href="#Fingervein-Verification-using-Convolutional-Multi-Head-Attention-Network" class="headerlink" title="Fingervein Verification using Convolutional Multi-Head Attention Network"></a>Fingervein Verification using Convolutional Multi-Head Attention Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16808">http://arxiv.org/abs/2310.16808</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raghavendra Ramachandra, Sushma Venkatesh</li>
<li>for: 这个论文是为了提出一种基于 convolutional multihead attention network 的新型指静脉识别方法，以提高指静脉识别的精度和可靠性。</li>
<li>methods: 该方法使用了 VeinAtnNet 网络，该网络通过EXTRACTING DISCRIMINANT INFORMATION FROM BOTH NORMAL AND ENHANCED FINGERVEIN IMAGES来提取指静脉图像中的特征信息，并通过一种小型的学习参数来减轻网络的计算负担。</li>
<li>results: 在新收集的 FV-300 数据集和公共可用的 FV-USM 和 FV-PolyU 指静脉数据集上，提出的方法与五种现有的指静脉识别系统进行比较，并显示出提案的 VeinAtnNet 方法的高效性。<details>
<summary>Abstract</summary>
Biometric verification systems are deployed in various security-based access-control applications that require user-friendly and reliable person verification. Among the different biometric characteristics, fingervein biometrics have been extensively studied owing to their reliable verification performance. Furthermore, fingervein patterns reside inside the skin and are not visible outside; therefore, they possess inherent resistance to presentation attacks and degradation due to external factors. In this paper, we introduce a novel fingervein verification technique using a convolutional multihead attention network called VeinAtnNet. The proposed VeinAtnNet is designed to achieve light weight with a smaller number of learnable parameters while extracting discriminant information from both normal and enhanced fingervein images. The proposed VeinAtnNet was trained on the newly constructed fingervein dataset with 300 unique fingervein patterns that were captured in multiple sessions to obtain 92 samples per unique fingervein. Extensive experiments were performed on the newly collected dataset FV-300 and the publicly available FV-USM and FV-PolyU fingervein dataset. The performance of the proposed method was compared with five state-of-the-art fingervein verification systems, indicating the efficacy of the proposed VeinAtnNet.
</details>
<details>
<summary>摘要</summary>
生物特征验证系统在安全访问控制应用中广泛应用，需要易于使用和可靠的人验证。 Among different生物特征，手掌血管网的验证性能得到了广泛的研究，因为它们在皮肤内部存在，不可见于外部，因此具有内生的抗击损和抗伪攻击性。 在本文中，我们提出了一种基于卷积多头注意网络的新型手掌血管验证技术，称为VeinAtnNet。 提出的VeinAtnNet设计用较小的学习参数量和批处理数据来提取特征信息，并且可以在不同的手掌血管图像下提取有效的特征信息。 我们使用新建的手掌血管数据集，包含300个唯一的手掌血管图像，在多个会话中采集到92个样本。 我们对新收集的FV-300数据集和公共可用的FV-USM和FV-PolyU手掌血管数据集进行了广泛的实验，并与5种现有的手掌血管验证系统进行了比较。 实验结果表明，提出的方法可以提供高效的手掌血管验证。
</details></li>
</ul>
<hr>
<h2 id="The-GOOSE-Dataset-for-Perception-in-Unstructured-Environments"><a href="#The-GOOSE-Dataset-for-Perception-in-Unstructured-Environments" class="headerlink" title="The GOOSE Dataset for Perception in Unstructured Environments"></a>The GOOSE Dataset for Perception in Unstructured Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16788">http://arxiv.org/abs/2310.16788</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Mortimer, Raphael Hagmanns, Miguel Granero, Thorsten Luettel, Janko Petereit, Hans-Joachim Wuensche</li>
<li>for: 这个研究是为了提高自动化系统在无结构的开放空间环境中的感知和解释能力。</li>
<li>methods: 这个研究使用了深度学习技术，将数据分为10,000对描述像和点云数据，并将其用于训练多种现有的分类模型。</li>
<li>results: 这个研究获得了一个名为GOOSE的大量数据集，并提供了一个开源的数据集、ontology для无结构的地形，以及数据集的标准和指引。这个initiative可以帮助建立一个通用的框架，并且实现将现有的数据集和模型融合，以提高各种无结构环境中的感知能力。<details>
<summary>Abstract</summary>
The potential for deploying autonomous systems can be significantly increased by improving the perception and interpretation of the environment. However, the development of deep learning-based techniques for autonomous systems in unstructured outdoor environments poses challenges due to limited data availability for training and testing. To address this gap, we present the German Outdoor and Offroad Dataset (GOOSE), a comprehensive dataset specifically designed for unstructured outdoor environments. The GOOSE dataset incorporates 10 000 labeled pairs of images and point clouds, which are utilized to train a range of state-of-the-art segmentation models on both image and point cloud data. We open source the dataset, along with an ontology for unstructured terrain, as well as dataset standards and guidelines. This initiative aims to establish a common framework, enabling the seamless inclusion of existing datasets and a fast way to enhance the perception capabilities of various robots operating in unstructured environments. The dataset, pre-trained models for offroad perception, and additional documentation can be found at https://goose-dataset.de/.
</details>
<details>
<summary>摘要</summary>
可以significantly increasethe potential for deploying autonomous systems by improving the perception and interpretation of the environment. However, the development of deep learning-based techniques for autonomous systems in unstructured outdoor environments poses challenges due to limited data availability for training and testing. To address this gap, we present the German Outdoor and Offroad Dataset (GOOSE), a comprehensive dataset specifically designed for unstructured outdoor environments. The GOOSE dataset incorporates 10,000 labeled pairs of images and point clouds, which are utilized to train a range of state-of-the-art segmentation models on both image and point cloud data. We open source the dataset, along with an ontology for unstructured terrain, as well as dataset standards and guidelines. This initiative aims to establish a common framework, enabling the seamless inclusion of existing datasets and a fast way to enhance the perception capabilities of various robots operating in unstructured environments. The dataset, pre-trained models for offroad perception, and additional documentation can be found at https://goose-dataset.de/.Here's the word-for-word translation:可以significantly increasethe potential for deploying autonomous systems by improving the perception and interpretation of the environment. However, the development of deep learning-based techniques for autonomous systems in unstructured outdoor environments poses challenges due to limited data availability for training and testing. To address this gap, we present the German Outdoor and Offroad Dataset (GOOSE), a comprehensive dataset specifically designed for unstructured outdoor environments. The GOOSE dataset incorporates 10,000 labeled pairs of images and point clouds, which are utilized to train a range of state-of-the-art segmentation models on both image and point cloud data. We open source the dataset, along with an ontology for unstructured terrain, as well as dataset standards and guidelines. This initiative aims to establish a common framework, enabling the seamless inclusion of existing datasets and a fast way to enhance the perception capabilities of various robots operating in unstructured environments. The dataset, pre-trained models for offroad perception, and additional documentation can be found at https://goose-dataset.de/.
</details></li>
</ul>
<hr>
<h2 id="S-3-TTA-Scale-Style-Selection-for-Test-Time-Augmentation-in-Biomedical-Image-Segmentation"><a href="#S-3-TTA-Scale-Style-Selection-for-Test-Time-Augmentation-in-Biomedical-Image-Segmentation" class="headerlink" title="S$^3$-TTA: Scale-Style Selection for Test-Time Augmentation in Biomedical Image Segmentation"></a>S$^3$-TTA: Scale-Style Selection for Test-Time Augmentation in Biomedical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16783">http://arxiv.org/abs/2310.16783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kangxian Xie, Siyu Huang, Sebastian Cajas Ordone, Hanspeter Pfister, Donglai Wei</li>
<li>for: 提高生物医学图像分割 task 的泛化能力</li>
<li>methods: 使用 S$^3$-TTA 框架，选择适合的图像比例和风格，并实现端到端的损均折合训练管道</li>
<li>results: 在公共 benchmark 上，S$^3$-TTA 对 cell 和 lung 分割 task 进行了3.4% 和 1.3% 的提高，仅通过在测试阶段对输入数据进行扩展。<details>
<summary>Abstract</summary>
Deep-learning models have been successful in biomedical image segmentation. To generalize for real-world deployment, test-time augmentation (TTA) methods are often used to transform the test image into different versions that are hopefully closer to the training domain. Unfortunately, due to the vast diversity of instance scale and image styles, many augmented test images produce undesirable results, thus lowering the overall performance. This work proposes a new TTA framework, S$^3$-TTA, which selects the suitable image scale and style for each test image based on a transformation consistency metric. In addition, S$^3$-TTA constructs an end-to-end augmentation-segmentation joint-training pipeline to ensure a task-oriented augmentation. On public benchmarks for cell and lung segmentation, S$^3$-TTA demonstrates improvements over the prior art by 3.4% and 1.3%, respectively, by simply augmenting the input data in testing phase.
</details>
<details>
<summary>摘要</summary>
深度学习模型在生物医学影像分割中取得了成功。为了在真实世界中广泛应用，测试时增强（TTA）方法通常用于在测试图像上进行不同版本的转换，以期望更近于训练领域。然而，由于图像实例尺寸和风格的巨大多样性，许多增强后的测试图像会导致不愿意的结果，从而降低总性能。本工作提出了一种新的TTA框架，S$^3$-TTA，该框架可以根据图像测试阶段的变换一致度指标选择适合的图像尺寸和风格。此外，S$^3$-TTA还构建了端到端增强-分割共训练管道，以确保任务导向的增强。在公共基准测试数据集上，S$^3$-TTA比依据测试阶段增强的先前艺术提高了3.4%和1.3%。
</details></li>
</ul>
<hr>
<h2 id="MixerFlow-for-Image-Modelling"><a href="#MixerFlow-for-Image-Modelling" class="headerlink" title="MixerFlow for Image Modelling"></a>MixerFlow for Image Modelling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16777">http://arxiv.org/abs/2310.16777</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eshant English, Matthias Kirchler, Christoph Lippert</li>
<li>for: 图像模型的建模和数据生成</li>
<li>methods: 基于MLP-Mixer架构的 MixerFlow 模型，实现Weight共享和流式模型的结合</li>
<li>results: 在固定计算资源下，MixerFlow 在图像数据集上表现出更好的密度估计，并且随着图像分辨率的增加，性能逐渐提高，成为Glow-based架构的有力且简单的替代方案。同时，MixerFlow 还提供了更有用的嵌入特征比Glow-based架构。<details>
<summary>Abstract</summary>
Normalising flows are statistical models that transform a complex density into a simpler density through the use of bijective transformations enabling both density estimation and data generation from a single model. In the context of image modelling, the predominant choice has been the Glow-based architecture, whereas alternative architectures remain largely unexplored in the research community. In this work, we propose a novel architecture called MixerFlow, based on the MLP-Mixer architecture, further unifying the generative and discriminative modelling architectures. MixerFlow offers an effective mechanism for weight sharing for flow-based models. Our results demonstrate better density estimation on image datasets under a fixed computational budget and scales well as the image resolution increases, making MixeFlow a powerful yet simple alternative to the Glow-based architectures. We also show that MixerFlow provides more informative embeddings than Glow-based architectures.
</details>
<details>
<summary>摘要</summary>
通常的流变换是一种统计模型，它将复杂的概率变换成更简单的概率，通过使用射影函数，以便 Both density estimation和数据生成从同一模型中。在图像模型中，主流选择是基于Glow架构的，而其他架构在研究community中尚未得到广泛的探索。在这项工作中，我们提出了一种新的架构called MixerFlow，基于MLP-Mixer架构，并将生成和识别模型架构进行统一。MixerFlow提供了有效的权重共享机制 для流变换模型。我们的结果表明，MixerFlow在图像数据集上的静止计算资源下可以更好地估计概率，并且随着图像分辨率的增加，MixerFlow的性能逐渐提高，使其成为一种强大 yet simple的Glow-based架构的替代方案。此外，我们还证明了MixerFlow提供了更有用的嵌入than Glow-based架构。
</details></li>
</ul>
<hr>
<h2 id="ConvNets-Match-Vision-Transformers-at-Scale"><a href="#ConvNets-Match-Vision-Transformers-at-Scale" class="headerlink" title="ConvNets Match Vision Transformers at Scale"></a>ConvNets Match Vision Transformers at Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16764">http://arxiv.org/abs/2310.16764</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kyegomez/ConvNet">https://github.com/kyegomez/ConvNet</a></li>
<li>paper_authors: Samuel L. Smith, Andrew Brock, Leonard Berrada, Soham De</li>
<li>for: 这个论文主要用于检验 ConvNet 是否在大型数据集上表现出色，并评估不同计算预算下 ConvNet 的表现。</li>
<li>methods: 该论文使用 JFT-4B 大量标注图像数据集进行预训练，并从 NFNet 模型家族中选择不同的深度和宽度来训练不同的网络。</li>
<li>results: 研究发现在不同计算预算下，ConvNet 会与 Vision Transformers 具有相似的表现，而且在 ImageNet 上进行精度训练后，NFNets 可以达到 Reported 性能水平。最终的精度训练结果为 Top-1 精度为 90.4%。<details>
<summary>Abstract</summary>
Many researchers believe that ConvNets perform well on small or moderately sized datasets, but are not competitive with Vision Transformers when given access to datasets on the web-scale. We challenge this belief by evaluating a performant ConvNet architecture pre-trained on JFT-4B, a large labelled dataset of images often used for training foundation models. We consider pre-training compute budgets between 0.4k and 110k TPU-v4 core compute hours, and train a series of networks of increasing depth and width from the NFNet model family. We observe a log-log scaling law between held out loss and compute budget. After fine-tuning on ImageNet, NFNets match the reported performance of Vision Transformers with comparable compute budgets. Our strongest fine-tuned model achieves a Top-1 accuracy of 90.4%.
</details>
<details>
<summary>摘要</summary>
许多研究人员认为卷积网络在小或中等规模的数据集上表现良好，但不能与视觉转换器在大规模数据集上竞争。我们挑战这个信念，通过评估一种高性能的卷积网络架构，预训练在JFT-4B大量标签图像集上。我们在预训练计算预算为0.4k至110k TPU-v4核心计算时间之间考虑了一系列不同的网络模型，从NFNet家族中选择了一些网络。我们发现了对保留的损失和计算预算之间的对数对应关系。经过精度调整后，我们的最佳精度调整模型在ImageNet上达到了90.4%的顶部一致率。
</details></li>
</ul>
<hr>
<h2 id="SonoSAM-–-Segment-Anything-on-Ultrasound-Images"><a href="#SonoSAM-–-Segment-Anything-on-Ultrasound-Images" class="headerlink" title="SonoSAM – Segment Anything on Ultrasound Images"></a>SonoSAM – Segment Anything on Ultrasound Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16872">http://arxiv.org/abs/2310.16872</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hariharan Ravishankar, Rohan Patil, Vikram Melapudi, Parminder Bhatia, Kass-Hout Taha, Pavan Annangi</li>
<li>for: 这个论文旨在提出一种可靠、通用的基础模型，用于分割ultrasound图像中的对象。</li>
<li>methods: 该模型基于一个丰富、多样的对象集，并通过精心调整和学习来实现高性能。</li>
<li>results: 模型在8个未看过的ultrasound数据集上表现出色，与其他方法相比，在所有评价指标上都具有显著的优势。<details>
<summary>Abstract</summary>
In this paper, we present SonoSAM - a promptable foundational model for segmenting objects of interest on ultrasound images. Fine-tuned exclusively on a rich, diverse set of objects from roughly 200k ultrasound image-mask pairs, SonoSAM demonstrates state-of-the-art performance on 8 unseen ultrasound data-sets, outperforming competing methods by a significant margin on all metrics of interest. SonoSAM achieves average dice similarity score of more than 90% on almost all test datasets within 2-6 clicks on an average, making it a valuable tool for annotating ultrasound images. We also extend SonoSAM to 3-D (2-D +t) applications and demonstrate superior performance making it a valuable tool for generating dense annotations from ultrasound cine-loops. Further, to increase practical utility of SonoSAM, we propose a two-step process of fine-tuning followed by knowledge distillation to a smaller footprint model without comprising the performance. We present detailed qualitative and quantitative comparisons of SonoSAM with state-of-the art methods showcasing efficacy of SonoSAM as one of the first reliable, generic foundational model for ultrasound.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了SonoSAM - 一个可提Prompt的基础模型，用于分割ultrasound图像中的对象。通过仅在200k ultrasound图像-mask对中进行微调，SonoSAM在8个未看过的ultrasound数据集上达到了最新的性能水平，比竞争方法在所有关键指标上都高于它们。SonoSAM在大多数测试数据集上的average dice相似性分数超过90%，只需2-6个键击，使其成为对ultrasound图像进行标注的有价值工具。此外，我们还扩展了SonoSAM到3D（2D+t）应用程序，并证明其在生成密集注释方面表现出色，使其成为对ultrasound cinema-loop进行密集注释的有价值工具。为了提高SonoSAM的实际使用性，我们提议了一种微调后followed by knowledge distillation的两步过程，以降低模型的尺寸不减其性能。我们提供了详细的量化和质量比较，展示了SonoSAM作为ultrasound领域的首个可靠、通用基础模型的效果。
</details></li>
</ul>
<hr>
<h2 id="CAD-–-Contextual-Multi-modal-Alignment-for-Dynamic-AVQA"><a href="#CAD-–-Contextual-Multi-modal-Alignment-for-Dynamic-AVQA" class="headerlink" title="CAD – Contextual Multi-modal Alignment for Dynamic AVQA"></a>CAD – Contextual Multi-modal Alignment for Dynamic AVQA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16754">http://arxiv.org/abs/2310.16754</a></li>
<li>repo_url: None</li>
<li>paper_authors: Asmar Nadeem, Adrian Hilton, Robert Dawes, Graham Thomas, Armin Mustafa</li>
<li>for: 提高 Audio Visual Question Answering (AVQA) 任务中的表现。</li>
<li>methods: 提出一种 Contextual Multi-modal Alignment (CAD) 网络，解决现有 AVQA 方法中的两大缺点： Audio-Visual (AV) 信息在网络中不匹配的 Spatial 和 Temporal 两个水平，以及 Audio 和 Visual 模式之间的 Semantic 信息在 контекст中不均衡。</li>
<li>results: 在 MUSIC-AVQA 数据集上，CAD 网络比状态艺术方法提高了平均性能，提高了9.4%。同时，我们还证明了我们的提案可以让现有 AVQA 方法中的表现得到改善，无需增加复杂度要求。<details>
<summary>Abstract</summary>
In the context of Audio Visual Question Answering (AVQA) tasks, the audio visual modalities could be learnt on three levels: 1) Spatial, 2) Temporal, and 3) Semantic. Existing AVQA methods suffer from two major shortcomings; the audio-visual (AV) information passing through the network isn't aligned on Spatial and Temporal levels; and, inter-modal (audio and visual) Semantic information is often not balanced within a context; this results in poor performance. In this paper, we propose a novel end-to-end Contextual Multi-modal Alignment (CAD) network that addresses the challenges in AVQA methods by i) introducing a parameter-free stochastic Contextual block that ensures robust audio and visual alignment on the Spatial level; ii) proposing a pre-training technique for dynamic audio and visual alignment on Temporal level in a self-supervised setting, and iii) introducing a cross-attention mechanism to balance audio and visual information on Semantic level. The proposed novel CAD network improves the overall performance over the state-of-the-art methods on average by 9.4% on the MUSIC-AVQA dataset. We also demonstrate that our proposed contributions to AVQA can be added to the existing methods to improve their performance without additional complexity requirements.
</details>
<details>
<summary>摘要</summary>
在听视问答（AVQA）任务上，听视模态可以学习在三个水平：1）空间、2）时间和3）Semantic。现有的AVQA方法受到两大缺点的影响：听视信息在网络中不匹配的空间和时间水平，以及听视模态之间的Semantic信息在一个上下文中不均衡。这导致了表现不佳。在本文中，我们提出了一种新的综合多模态对齐（CAD）网络，解决了AVQA方法中的挑战。我们的方法包括：1）引入无参数的随机Contextual块，确保听视对齐的空间水平是Robust的。2）提出了一种在无监督下自适应的听视对齐策略，以解决听视模态之间的时间水平的匹配问题。3）引入了一种cross-attention机制，以协调听视信息的Semantic水平。我们的CAD网络在MUSIC-AVQA数据集上的平均提高了state-of-the-art方法的性能，提高了9.4%。此外，我们还证明了我们的提议可以追加到现有的方法中，无需增加复杂性。
</details></li>
</ul>
<hr>
<h2 id="Metrically-Scaled-Monocular-Depth-Estimation-through-Sparse-Priors-for-Underwater-Robots"><a href="#Metrically-Scaled-Monocular-Depth-Estimation-through-Sparse-Priors-for-Underwater-Robots" class="headerlink" title="Metrically Scaled Monocular Depth Estimation through Sparse Priors for Underwater Robots"></a>Metrically Scaled Monocular Depth Estimation through Sparse Priors for Underwater Robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16750">http://arxiv.org/abs/2310.16750</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ebnerluca/uw_depth">https://github.com/ebnerluca/uw_depth</a></li>
<li>paper_authors: Luca Ebner, Gideon Billings, Stefan Williams</li>
<li>for: 这篇论文targets the problem of real-time dense depth estimation from monocular underwater images for mobile underwater vehicles.</li>
<li>methods: The paper proposes a deep learning model that fuses sparse depth measurements from triangulated features to improve depth predictions and solve the scale ambiguity problem. The model uses an efficient encoder-decoder backbone and modern lightweight transformer optimization stage to encode global context.</li>
<li>results: The proposed method achieves significant improvement in depth prediction accuracy by fusing sparse feature priors, and achieves similar accuracy on a downward-looking dataset without any retraining. The method runs at 160 FPS on a laptop GPU and 7 FPS on a single CPU core, making it suitable for direct deployment on embedded systems.<details>
<summary>Abstract</summary>
In this work, we address the problem of real-time dense depth estimation from monocular images for mobile underwater vehicles. We formulate a deep learning model that fuses sparse depth measurements from triangulated features to improve the depth predictions and solve the problem of scale ambiguity. To allow prior inputs of arbitrary sparsity, we apply a dense parameterization method. Our model extends recent state-of-the-art approaches to monocular image based depth estimation, using an efficient encoder-decoder backbone and modern lightweight transformer optimization stage to encode global context. The network is trained in a supervised fashion on the forward-looking underwater dataset, FLSea. Evaluation results on this dataset demonstrate significant improvement in depth prediction accuracy by the fusion of the sparse feature priors. In addition, without any retraining, our method achieves similar depth prediction accuracy on a downward looking dataset we collected with a diver operated camera rig, conducting a survey of a coral reef. The method achieves real-time performance, running at 160 FPS on a laptop GPU and 7 FPS on a single CPU core and is suitable for direct deployment on embedded systems. The implementation of this work is made publicly available at https://github.com/ebnerluca/uw_depth.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们解决了来自单摄像头图像的实时稠密深度估计问题 для移动式水下潜水器。我们设计了一种深度学习模型，该模型将 sparse depth measurement from triangulated features fusion 以提高深度预测和解决尺度 ambiguity 问题。为了允许不同的稀疏性输入，我们应用了密集参数化方法。我们的模型基于最近的状态开头的方法，使用高效的编码器-解码器脊梁和现代轻量级 transformer 优化阶段来编码全球上下文。网络在监督模式下在 forward-looking 水下数据集上训练，评估结果表明，通过稀疏特征先导的深度预测得到了显著改善。此外，没有任何重新训练，我们的方法在一个下降看到的数据集上也实现了类似的深度预测精度。此方法在实时性方面表现出色，在笔记型 GPU 上运行速度达 160 FPS，单 CPU 核心上运行速度为 7 FPS，适用于直接部署在嵌入式系统上。我们在 https://github.com/ebnerluca/uw_depth 上公开了实现。
</details></li>
</ul>
<hr>
<h2 id="Interferometric-Neural-Networks"><a href="#Interferometric-Neural-Networks" class="headerlink" title="Interferometric Neural Networks"></a>Interferometric Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16742">http://arxiv.org/abs/2310.16742</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/arunsehrawat/interferometric-neural-networks">https://github.com/arunsehrawat/interferometric-neural-networks</a></li>
<li>paper_authors: Arun Sehrawat</li>
<li>for: 本研究旨在探讨人工神经网络和干涉仪器的结合，并将其应用于机器学习和优化领域。</li>
<li>methods: 本研究使用干涉仪器构建了基于神经网络的生成对抗网络，而这些神经网络没有任何类传呈层，可以在量子计算机或光子芯片上实现。</li>
<li>results: 研究表明，这种方法可以用于解决 combinatorial optimization 问题，并在多类图像识别任务中达到了93%和83%的准确率。此外，它还可以生成数字0-9和人脸图像。<details>
<summary>Abstract</summary>
On the one hand, artificial neural networks have many successful applications in the field of machine learning and optimization. On the other hand, interferometers are integral parts of any field that deals with waves such as optics, astronomy, and quantum physics. Here, we introduce neural networks composed of interferometers and then build generative adversarial networks from them. Our networks do not have any classical layer and can be realized on quantum computers or photonic chips. We demonstrate their applicability for combinatorial optimization, image classification, and image generation. For combinatorial optimization, our network consistently converges to the global optimum or remains within a narrow range of it. In multi-class image classification tasks, our networks achieve accuracies of 93% and 83%. Lastly, we show their capability to generate images of digits from 0 to 9 as well as human faces.
</details>
<details>
<summary>摘要</summary>
一方面，人工神经网络在机器学习和优化领域有很多成功应用。另一方面，湍波器是任何波动领域的重要组件，包括光学、天文学和量子物理。我们在这里引入神经网络中的湍波器，然后建立基于这些湍波器的生成敌对网络。我们的网络没有任何经典层，可以在量子计算机或光子芯片上实现。我们展示了它们在组合优化、图像分类和图像生成等方面的可行性。在组合优化任务中，我们的网络一般 converge 到全局最优或在一个窄范围内停止。在多类图像分类任务中，我们的网络达到了93%和83%的准确率。最后，我们展示了它们可以生成从0到9的数字图像以及人脸。
</details></li>
</ul>
<hr>
<h2 id="A-No-Reference-Quality-Assessment-Method-for-Digital-Human-Head"><a href="#A-No-Reference-Quality-Assessment-Method-for-Digital-Human-Head" class="headerlink" title="A No-Reference Quality Assessment Method for Digital Human Head"></a>A No-Reference Quality Assessment Method for Digital Human Head</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16732">http://arxiv.org/abs/2310.16732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yingjie Zhou, Zicheng Zhang, Wei Sun, Xiongkuo Min, Xianghe Ma, Guangtao Zhai</li>
<li>for: 这篇论文是关于数字人质量评估（DHQA）的研究，目的是提出一种基于Transformer的无参考评估方法，以解决数字人在生成和传输过程中可能出现的多种扭曲和质量下降问题。</li>
<li>methods: 该方法使用了 Rendering 技术将数字人的前2D投影作为输入，然后使用视transformer（ViT）进行特征提取，并设计了一个多任务模块以同时分类扭曲类型和预测数字人的 perceived quality 水平。</li>
<li>results: 实验结果表明，提出的方法与人工评估得分高度相关，并且在比较现有评估方法时表现出色。<details>
<summary>Abstract</summary>
In recent years, digital humans have been widely applied in augmented/virtual reality (A/VR), where viewers are allowed to freely observe and interact with the volumetric content. However, the digital humans may be degraded with various distortions during the procedure of generation and transmission. Moreover, little effort has been put into the perceptual quality assessment of digital humans. Therefore, it is urgent to carry out objective quality assessment methods to tackle the challenge of digital human quality assessment (DHQA). In this paper, we develop a novel no-reference (NR) method based on Transformer to deal with DHQA in a multi-task manner. Specifically, the front 2D projections of the digital humans are rendered as inputs and the vision transformer (ViT) is employed for the feature extraction. Then we design a multi-task module to jointly classify the distortion types and predict the perceptual quality levels of digital humans. The experimental results show that the proposed method well correlates with the subjective ratings and outperforms the state-of-the-art quality assessment methods.
</details>
<details>
<summary>摘要</summary>
在最近的几年里，数字人类在扩展/虚拟现实（A/VR）领域广泛应用，让观众可以自由观看和与三维内容互动。然而，数字人类可能在生成和传输过程中受到多种扭曲的影响。此外，对数字人类的主观质量评估没有充分的努力。因此，需要开发一种无参考（NR）方法，以便在多任务方式下进行数字人类质量评估（DHQA）。在这篇论文中，我们提出了一种基于变换器的新方法，以解决DHQA的挑战。具体来说，我们将数字人类的前2D投影作为输入，并使用视传送器（ViT）进行特征提取。然后，我们设计了一个多任务模块，以同时类型化扭曲和预测数字人类的主观质量水平。实验结果表明，我们的方法与主观评估结果高度相关，并超越了现有的质量评估方法。
</details></li>
</ul>
<hr>
<h2 id="Rebuild-City-Buildings-from-Off-Nadir-Aerial-Images-with-Offset-Building-Model-OBM"><a href="#Rebuild-City-Buildings-from-Off-Nadir-Aerial-Images-with-Offset-Building-Model-OBM" class="headerlink" title="Rebuild City Buildings from Off-Nadir Aerial Images with Offset-Building Model (OBM)"></a>Rebuild City Buildings from Off-Nadir Aerial Images with Offset-Building Model (OBM)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16717">http://arxiv.org/abs/2310.16717</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Li, Yupeng Deng, Yunlong Kong, Diyou Liu, Jingbo Chen, Yu Meng, Junxian Ma</li>
<li>for: 这个研究的目的是提出一个互动式Transformer模型，用于精确地测量高解析 remote sensing 图像中的建筑物偏移。</li>
<li>methods: 这个模型使用了一个互动式Transformer模型，与一个启发器Encoder，以精确地测量建筑物偏移。它还使用了一个ROAM模块，用于解决常见的预测建筑物偏移的问题。</li>
<li>results: 这个研究在公开available的BONAI dataset上进行了试验，实现了显著对Prompt-Instance-Level偏移Error的减少，从14.6%到16.3%。此外，这个研究也开发了一个适合大规模建筑物偏移的Distance-NMS算法，对预测建筑物偏移角度和长度进行了重要改善。<details>
<summary>Abstract</summary>
Accurate measurement of the offset from roof-to-footprint in very-high-resolution remote sensing imagery is crucial for urban information extraction tasks. With the help of deep learning, existing methods typically rely on two-stage CNN models to extract regions of interest on building feature maps. At the first stage, a Region Proposal Network (RPN) is applied to extract thousands of ROIs (Region of Interests) which will post-imported into a Region-based Convolutional Neural Networks (RCNN) to extract wanted information. However, because of inflexible RPN, these methods often lack effective user interaction, encounter difficulties in instance correspondence, and struggle to keep up with the advancements in general artificial intelligence. This paper introduces an interactive Transformer model combined with a prompt encoder to precisely extract building segmentation as well as the offset vectors from roofs to footprints. In our model, a powerful module, namely ROAM, was tailored for common problems in predicting roof-to-footprint offsets. We tested our model's feasibility on the publicly available BONAI dataset, achieving a significant reduction in Prompt-Instance-Level offset errors ranging from 14.6% to 16.3%. Additionally, we developed a Distance-NMS algorithm tailored for large-scale building offsets, significantly enhancing the accuracy of predicted building offset angles and lengths in a straightforward and efficient manner. To further validate the model's robustness, we created a new test set using 0.5m remote sensing imagery from Huizhou, China, for inference testing. Our code, training methods, and the updated dataset will be accessable at https://github.com/likaiucas.
</details>
<details>
<summary>摘要</summary>
准确测量房屋缘界与地面的偏移量在高分辨率Remote sensing影像中是城市信息提取任务中非常重要的。通过深度学习，现有方法通常是通过两个阶段Convolutional Neural Networks (CNN)模型来提取建筑特征图像中的区域兴趣点。在第一阶段，一个Region Proposal Network (RPN)被应用以提取数以千计的ROIs（区域兴趣点），然后将其导入到基于区域的Convolutional Neural Networks (RCNN)中进行信息提取。然而，由于不灵活的RPN，这些方法经常缺乏有效的用户互动，遇到实例匹配的问题，并且难以保持总人工智能的提高。这篇论文介绍了一种交互式Transformer模型，并与一个提示编码器结合，以准确提取建筑分割以及缘界到地面的偏移量。在我们的模型中，一个专门为建筑问题设计的强大模块，即ROAM，用于解决通用的预测缘界到地面的偏移量问题。我们在公共可用的BONAI数据集上测试了我们的模型的可行性，实现了对Prompt-Instance-Level偏移量的显著减少，范围为14.6%至16.3%。此外，我们开发了一种适合大规模建筑偏移量的Distance-NMS算法，大幅提高了预测建筑偏移角度和长度的准确率，并且这是一种简单、有效的方式。为了进一步验证我们的模型的稳定性，我们创建了一个使用0.5米Remote sensing影像的新测试集，用于对模型进行推理测试。我们的代码、训练方法和更新的数据集将在https://github.com/likaiucas中公开。
</details></li>
</ul>
<hr>
<h2 id="Nighttime-Driver-Behavior-Prediction-Using-Taillight-Signal-Recognition-via-CNN-SVM-Classifier"><a href="#Nighttime-Driver-Behavior-Prediction-Using-Taillight-Signal-Recognition-via-CNN-SVM-Classifier" class="headerlink" title="Nighttime Driver Behavior Prediction Using Taillight Signal Recognition via CNN-SVM Classifier"></a>Nighttime Driver Behavior Prediction Using Taillight Signal Recognition via CNN-SVM Classifier</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16706">http://arxiv.org/abs/2310.16706</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deepcar/taillight_recognition">https://github.com/deepcar/taillight_recognition</a></li>
<li>paper_authors: Amir Hossein Barshooi, Elmira Bagheri</li>
<li>for: 这种研究的目的是提高夜间驾驶行为预测能力，通过识别人驾和自动驾车 taillight。</li>
<li>methods: 提出的模型使用了自定义的探测器，通过提取输入图像的深度特征，并对每个特征计算数据稀缺性。然后，通过引入 soft attention 的权重binary mask，使模型更加注重预先确定的区域。最后，使用 Convolutional Neural Networks (CNNs) 提取特征，并使用 Principal Component Analysis (PCA) 减少维度。</li>
<li>results: 实验结果显示，提出的方法可以准确地分类夜间驾驶行为，具体的结果为92.14%的准确率、97.38%的特殊性、92.09%的敏感度、92.10%的 F1-度量和0.895的科恩 statistic。<details>
<summary>Abstract</summary>
This paper aims to enhance the ability to predict nighttime driving behavior by identifying taillights of both human-driven and autonomous vehicles. The proposed model incorporates a customized detector designed to accurately detect front-vehicle taillights on the road. At the beginning of the detector, a learnable pre-processing block is implemented, which extracts deep features from input images and calculates the data rarity for each feature. In the next step, drawing inspiration from soft attention, a weighted binary mask is designed that guides the model to focus more on predetermined regions. This research utilizes Convolutional Neural Networks (CNNs) to extract distinguishing characteristics from these areas, then reduces dimensions using Principal Component Analysis (PCA). Finally, the Support Vector Machine (SVM) is used to predict the behavior of the vehicles. To train and evaluate the model, a large-scale dataset is collected from two types of dash-cams and Insta360 cameras from the rear view of Ford Motor Company vehicles. This dataset includes over 12k frames captured during both daytime and nighttime hours. To address the limited nighttime data, a unique pixel-wise image processing technique is implemented to convert daytime images into realistic night images. The findings from the experiments demonstrate that the proposed methodology can accurately categorize vehicle behavior with 92.14% accuracy, 97.38% specificity, 92.09% sensitivity, 92.10% F1-measure, and 0.895 Cohen's Kappa Statistic. Further details are available at https://github.com/DeepCar/Taillight_Recognition.
</details>
<details>
<summary>摘要</summary>
To train and evaluate the model, a large-scale dataset is collected from two types of dash-cams and Insta360 cameras from the rear view of Ford Motor Company vehicles. The dataset includes over 12,000 frames captured during both daytime and nighttime hours. To address the limited nighttime data, a unique pixel-wise image processing technique is implemented to convert daytime images into realistic night images.The experimental results show that the proposed methodology can accurately categorize vehicle behavior with 92.14% accuracy, 97.38% specificity, 92.09% sensitivity, 92.10% F1-measure, and 0.895 Cohen's Kappa Statistic. More details can be found at <https://github.com/DeepCar/Taillight_Recognition>.
</details></li>
</ul>
<hr>
<h2 id="From-Pointwise-to-Powerhouse-Initialising-Neural-Networks-with-Generative-Models"><a href="#From-Pointwise-to-Powerhouse-Initialising-Neural-Networks-with-Generative-Models" class="headerlink" title="From Pointwise to Powerhouse: Initialising Neural Networks with Generative Models"></a>From Pointwise to Powerhouse: Initialising Neural Networks with Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16695">http://arxiv.org/abs/2310.16695</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Harder, Moritz Fuchs, Yuri Tolkach, Anirban Mukhopadhyay</li>
<li>for: 这篇研究旨在提出新的启动方法，以解决深度神经网络中的弹性问题（vanishing or exploding gradients）。</li>
<li>methods: 这篇研究使用生成模型来初始化神经网络，包括使用Variational Autoencoders（VAEs）和Graph Hypernetworks（GHNs）。</li>
<li>results: 研究发现，使用全量的初始化方法可以提高精度和初始化速度，但是透过Graph Hypernetworks（GHNs）实现的方法会导致集合的ensemble performance在离distribution data上下降。为了解决这个问题，研究提出了一种噪音Graph Hypernetworks（noise GHNs）来鼓励多样性。此外，这篇研究还发现这些新的启动方法可能可以将学习到的知识转移到不同的图像分布上。<details>
<summary>Abstract</summary>
Traditional initialisation methods, e.g. He and Xavier, have been effective in avoiding the problem of vanishing or exploding gradients in neural networks. However, they only use simple pointwise distributions, which model one-dimensional variables. Moreover, they ignore most information about the architecture and disregard past training experiences. These limitations can be overcome by employing generative models for initialisation. In this paper, we introduce two groups of new initialisation methods. First, we locally initialise weight groups by employing variational autoencoders. Secondly, we globally initialise full weight sets by employing graph hypernetworks. We thoroughly evaluate the impact of the employed generative models on state-of-the-art neural networks in terms of accuracy, convergence speed and ensembling. Our results show that global initialisations result in higher accuracy and faster initial convergence speed. However, the implementation through graph hypernetworks leads to diminished ensemble performance on out of distribution data. To counteract, we propose a modification called noise graph hypernetwork, which encourages diversity in the produced ensemble members. Furthermore, our approach might be able to transfer learned knowledge to different image distributions. Our work provides insights into the potential, the trade-offs and possible modifications of these new initialisation methods.
</details>
<details>
<summary>摘要</summary>
传统的初始化方法，如希尔和夏维，有效地避免神经网络中的衰减或扩散Gradient问题。然而，它们只使用简单的点位分布，这些分布模型了一维变量。此外，它们忽略了神经网络的建筑和过去训练经验。这些限制可以通过使用生成模型来超越。在这篇论文中，我们介绍了两组新的初始化方法。首先，我们在weight组上本地初始化使用变量自动机。其次，我们在全Weight集上全球初始化使用图hyper网络。我们仔细评估了采用生成模型对现有神经网络的影响，包括精度、快速初始化速度和 ensemble。我们的结果显示，全球初始化可以提高精度和初始化速度，但是通过图hyper网络的实现会导致对于异常数据的ensemble表现下降。为了缓解这个问题，我们提出了噪音图hyper网络修改，该修改强制生成 ensemble member中的多样性。此外，我们的方法可能可以传输学习到不同的图像分布。我们的工作为这些新的初始化方法的潜力、交易和可能的修改提供了深入的视角。
</details></li>
</ul>
<hr>
<h2 id="DSAM-GN-Graph-Network-based-on-Dynamic-Similarity-Adjacency-Matrices-for-Vehicle-Re-identification"><a href="#DSAM-GN-Graph-Network-based-on-Dynamic-Similarity-Adjacency-Matrices-for-Vehicle-Re-identification" class="headerlink" title="DSAM-GN:Graph Network based on Dynamic Similarity Adjacency Matrices for Vehicle Re-identification"></a>DSAM-GN:Graph Network based on Dynamic Similarity Adjacency Matrices for Vehicle Re-identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16694">http://arxiv.org/abs/2310.16694</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuejun Jiao, Song Qiu, Mingsong Chen, Dingding Han, Qingli Li, Yue Lu</li>
<li>for: 本研究旨在提高智能交通系统中车辆重新认识（Re-ID）的精度，适用于助动系统、交通流管理和车辆跟踪等应用。</li>
<li>methods: 本研究提出了基于动态相似 adjacency 矩阵（DSAM-GN）的方法，具有新的相似性矩阵构建方法，可以捕捉车辆特征的空间关系，减少背景噪音。</li>
<li>results: 实验结果表明，提出的方法比现有方法更有效，能够更好地提高车辆 Re-ID 的精度。<details>
<summary>Abstract</summary>
In recent years, vehicle re-identification (Re-ID) has gained increasing importance in various applications such as assisted driving systems, traffic flow management, and vehicle tracking, due to the growth of intelligent transportation systems. However, the presence of extraneous background information and occlusions can interfere with the learning of discriminative features, leading to significant variations in the same vehicle image across different scenarios. This paper proposes a method, named graph network based on dynamic similarity adjacency matrices (DSAM-GN), which incorporates a novel approach for constructing adjacency matrices to capture spatial relationships of local features and reduce background noise. Specifically, the proposed method divides the extracted vehicle features into different patches as nodes within the graph network. A spatial attention-based similarity adjacency matrix generation (SASAMG) module is employed to compute similarity matrices of nodes, and a dynamic erasure operation is applied to disconnect nodes with low similarity, resulting in similarity adjacency matrices. Finally, the nodes and similarity adjacency matrices are fed into graph networks to extract more discriminative features for vehicle Re-ID. Experimental results on public datasets VeRi-776 and VehicleID demonstrate the effectiveness of the proposed method compared with recent works.
</details>
<details>
<summary>摘要</summary>
Recently, vehicle re-identification (Re-ID) has become increasingly important in various applications such as assisted driving systems, traffic flow management, and vehicle tracking, due to the development of intelligent transportation systems. However, the presence of extraneous background information and occlusions can interfere with the learning of discriminative features, leading to significant variations in the same vehicle image across different scenarios. This paper proposes a method, named graph network based on dynamic similarity adjacency matrices (DSAM-GN), which incorporates a novel approach for constructing adjacency matrices to capture spatial relationships of local features and reduce background noise. Specifically, the proposed method divides the extracted vehicle features into different patches as nodes within the graph network. A spatial attention-based similarity adjacency matrix generation (SASAMG) module is employed to compute similarity matrices of nodes, and a dynamic erasure operation is applied to disconnect nodes with low similarity, resulting in similarity adjacency matrices. Finally, the nodes and similarity adjacency matrices are fed into graph networks to extract more discriminative features for vehicle Re-ID. Experimental results on public datasets VeRi-776 and VehicleID demonstrate the effectiveness of the proposed method compared with recent works.
</details></li>
</ul>
<hr>
<h2 id="Local-Statistics-for-Generative-Image-Detection"><a href="#Local-Statistics-for-Generative-Image-Detection" class="headerlink" title="Local Statistics for Generative Image Detection"></a>Local Statistics for Generative Image Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16684">http://arxiv.org/abs/2310.16684</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Yung Jer Wong, Teck Khim Ng</li>
<li>for: 这个论文主要旨在提高Diffusion models（DMs）的渲染质量，以便在不同任务上使用DMs进行图像生成和图像超分辨率等任务。</li>
<li>methods: 这个论文使用了计算地方统计信息，而不是全局统计信息，以分辨出图像是否来自DMs生成。它们认为地方统计信息能够解决图像空间不均衡问题。</li>
<li>results: 论文表明，使用地方统计信息可以提供有前景的结果，并且这种方法对图像缩放和JPEG压缩等多种杂音噪声有良好的Robustness。<details>
<summary>Abstract</summary>
Diffusion models (DMs) are generative models that learn to synthesize images from Gaussian noise. DMs can be trained to do a variety of tasks such as image generation and image super-resolution. Researchers have made significant improvement in the capability of synthesizing photorealistic images in the past few years. These successes also hasten the need to address the potential misuse of synthesized images. In this paper, we highlight the effectiveness of computing local statistics, as opposed to global statistics, in distinguishing digital camera images from DM-generated images. We hypothesized that local statistics should be used to address the spatial non-stationarity problem in images. We show that our approach produced promising results and it is also robust to various perturbations such as image resizing and JPEG compression.
</details>
<details>
<summary>摘要</summary>
干扰模型（DM）是一种生成模型，可以学习将泊松噪声转化为图像。DM可以完成多种任务，如图像生成和图像超分辨率。过去几年，研究人员在图像生成方面做出了 significative 进步，这也提高了对生成图像的可能性。然而，随着技术的发展，对生成图像的潜在misuse的需要也在增加。在这篇论文中，我们指出了计算地方统计信息，而不是全局统计信息，对于区分摄像头图像和DM生成图像的效iveness。我们假设了使用地方统计来解决图像的空间不均衡问题。我们的方法得到了有 promise 的结果，并且对各种扰动，如图像缩放和JPEG压缩，也具有良好的鲁棒性。
</details></li>
</ul>
<hr>
<h2 id="CoDet-Co-Occurrence-Guided-Region-Word-Alignment-for-Open-Vocabulary-Object-Detection"><a href="#CoDet-Co-Occurrence-Guided-Region-Word-Alignment-for-Open-Vocabulary-Object-Detection" class="headerlink" title="CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection"></a>CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16667">http://arxiv.org/abs/2310.16667</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cvmi-lab/codet">https://github.com/cvmi-lab/codet</a></li>
<li>paper_authors: Chuofan Ma, Yi Jiang, Xin Wen, Zehuan Yuan, Xiaojuan Qi</li>
<li>for: 学习开放词汇对象检测中的对象级视力语言表示，从图文对 alignment 获得可靠的区域词对应关系是关键。</li>
<li>methods: CoDet 提出了一种新的方法，即将区域词对应关系重新定义为共同发现问题，通过图像的视觉相似性来发现共同出现的对象。</li>
<li>results: 实验结果表明，CoDet 在开放词汇检测中具有优秀的性能和扩展性，例如通过增加视觉底层，CoDet 在 OV-LVIS 上实现了 37.0 $\text{AP}^m_{novel}$ 和 44.7 $\text{AP}^m_{all}$，比前一个 SoTA 高出 4.2 $\text{AP}^m_{novel}$ 和 9.8 $\text{AP}^m_{all}$。<details>
<summary>Abstract</summary>
Deriving reliable region-word alignment from image-text pairs is critical to learn object-level vision-language representations for open-vocabulary object detection. Existing methods typically rely on pre-trained or self-trained vision-language models for alignment, which are prone to limitations in localization accuracy or generalization capabilities. In this paper, we propose CoDet, a novel approach that overcomes the reliance on pre-aligned vision-language space by reformulating region-word alignment as a co-occurring object discovery problem. Intuitively, by grouping images that mention a shared concept in their captions, objects corresponding to the shared concept shall exhibit high co-occurrence among the group. CoDet then leverages visual similarities to discover the co-occurring objects and align them with the shared concept. Extensive experiments demonstrate that CoDet has superior performances and compelling scalability in open-vocabulary detection, e.g., by scaling up the visual backbone, CoDet achieves 37.0 $\text{AP}^m_{novel}$ and 44.7 $\text{AP}^m_{all}$ on OV-LVIS, surpassing the previous SoTA by 4.2 $\text{AP}^m_{novel}$ and 9.8 $\text{AP}^m_{all}$. Code is available at https://github.com/CVMI-Lab/CoDet.
</details>
<details>
<summary>摘要</summary>
通过图文对应关系学习对象级视觉语言表示是关键，以实现开放词汇物体检测。现有方法通常基于预训练或自动训练视觉语言模型进行对齐，这些方法容易受到当地化精度或通用能力的限制。在本文中，我们提出了CoDet方法，它通过重新定义区域词对应关系为共同发现问题来超越预aligned视觉语言空间的局限性。具体来说，通过将图像分组，图像的描述中提到的共同概念的对象将显示高度的共同出现。CoDet然后利用视觉相似性来找到共同出现的对象，并将其与共同概念进行对齐。我们的实验表明，CoDet在开放词汇检测中具有优秀的性能和吸引人的扩展性，例如，通过将视觉后骨骼扩展到更大的尺度，CoDet在OV-LVIS上 achievied 37.0 $\text{AP}^m_{novel}$和44.7 $\text{AP}^m_{all}$，超过了之前的SoTAby 4.2 $\text{AP}^m_{novel}$和9.8 $\text{AP}^m_{all}$。代码可以在https://github.com/CVMI-Lab/CoDet上获取。
</details></li>
</ul>
<hr>
<h2 id="Robust-Source-Free-Domain-Adaptation-for-Fundus-Image-Segmentation"><a href="#Robust-Source-Free-Domain-Adaptation-for-Fundus-Image-Segmentation" class="headerlink" title="Robust Source-Free Domain Adaptation for Fundus Image Segmentation"></a>Robust Source-Free Domain Adaptation for Fundus Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16665">http://arxiv.org/abs/2310.16665</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/LinGrayy/PLPB">https://github.com/LinGrayy/PLPB</a></li>
<li>paper_authors: Lingrui Li, Yanfeng Zhou, Ge Yang</li>
<li>For: This paper focuses on improving the robustness of unsupervised domain adaptation (UDA) techniques for medical image segmentation, specifically fundus image segmentation.* Methods: The proposed method consists of two stages: (1) source training with adversarial sample augmentation to enhance the robustness and generalization capability of the source model, and (2) target training with a novel robust pseudo-label and pseudo-boundary (PLPB) method that utilizes unlabeled target data to generate pseudo labels and pseudo boundaries for self-adaptation.* Results: Extensive experimental results on cross-domain fundus image segmentation confirm the effectiveness and versatility of the proposed method, demonstrating improved accuracy and robustness compared to existing UDA techniques.Here’s the simplified Chinese version:* For: 这篇论文主要关注改进医疗图像分割领域的无监督领域适应（UDA）技术的Robustness，具体来说是基于眼膜图像分割。* Methods: 提议的方法包括两个阶段：（1）源训练阶段使用对抗样本扩大来提高源模型的Robustness和泛化能力，以及（2）目标训练阶段提出一种新的Robust pseudo-label和pseudo-boundary（PLPB）方法，通过不使用源数据，使用无标目标数据生成pseudo标签和pseudo bound。* Results: 对于cross-domain眼膜图像分割，实验结果证明提议的方法的有效性和多样性，比较 existing UDA 技术，显示了提高了准确率和Robustness。<details>
<summary>Abstract</summary>
Unsupervised Domain Adaptation (UDA) is a learning technique that transfers knowledge learned in the source domain from labelled training data to the target domain with only unlabelled data. It is of significant importance to medical image segmentation because of the usual lack of labelled training data. Although extensive efforts have been made to optimize UDA techniques to improve the accuracy of segmentation models in the target domain, few studies have addressed the robustness of these models under UDA. In this study, we propose a two-stage training strategy for robust domain adaptation. In the source training stage, we utilize adversarial sample augmentation to enhance the robustness and generalization capability of the source model. And in the target training stage, we propose a novel robust pseudo-label and pseudo-boundary (PLPB) method, which effectively utilizes unlabeled target data to generate pseudo labels and pseudo boundaries that enable model self-adaptation without requiring source data. Extensive experimental results on cross-domain fundus image segmentation confirm the effectiveness and versatility of our method. Source code of this study is openly accessible at https://github.com/LinGrayy/PLPB.
</details>
<details>
<summary>摘要</summary>
无监督领域适应（UDA）是一种学习技术，它将源领域中标注训练数据上的知识传递到目标领域中，只使用无标注数据进行学习。由于医学影像分割通常缺乏标注训练数据，UDA技术具有重要的意义。虽然有大量研究探讨了如何优化UDA技术以提高目标领域中分割模型的准确率，但只有一些研究考虑了UDA模型的稳定性。在这个研究中，我们提出了一种两stage训练策略，以提高UDA模型的稳定性。在源训练阶段，我们利用对抗样本增强的技术，以提高源模型的 robustness和通用性。在目标训练阶段，我们提出了一种新的假标签和假边界（PLPB）方法，它可以使用无标注目标数据来生成假标签和假边界，以便模型自适应而不需要源数据。我们的方法在跨领域眼影像分割中进行了广泛的实验，并证明了我们的方法的效果和多样性。源代码可以在https://github.com/LinGrayy/PLPB中获取。
</details></li>
</ul>
<hr>
<h2 id="MACP-Efficient-Model-Adaptation-for-Cooperative-Perception"><a href="#MACP-Efficient-Model-Adaptation-for-Cooperative-Perception" class="headerlink" title="MACP: Efficient Model Adaptation for Cooperative Perception"></a>MACP: Efficient Model Adaptation for Cooperative Perception</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16870">http://arxiv.org/abs/2310.16870</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/purduedigitaltwin/macp">https://github.com/purduedigitaltwin/macp</a></li>
<li>paper_authors: Yunsheng Ma, Juanwu Lu, Can Cui, Sicheng ZHao, Xu Cao, Wenqian Ye, Ziran Wang</li>
<li>for: 提高连接自动化车辆（CAVs）的感知能力，使其可以“看过遮盖物”，提高性能。</li>
<li>methods: 基于单机器学习模型，增加合作能力。</li>
<li>results: 在模拟和实际协同感知测试中，提出的方法可以有效利用协同观察，并超越其他当前最佳方法，需要许多 fewer 参数和通信成本。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Vehicle-to-vehicle (V2V) communications have greatly enhanced the perception capabilities of connected and automated vehicles (CAVs) by enabling information sharing to "see through the occlusions", resulting in significant performance improvements. However, developing and training complex multi-agent perception models from scratch can be expensive and unnecessary when existing single-agent models show remarkable generalization capabilities. In this paper, we propose a new framework termed MACP, which equips a single-agent pre-trained model with cooperation capabilities. We approach this objective by identifying the key challenges of shifting from single-agent to cooperative settings, adapting the model by freezing most of its parameters and adding a few lightweight modules. We demonstrate in our experiments that the proposed framework can effectively utilize cooperative observations and outperform other state-of-the-art approaches in both simulated and real-world cooperative perception benchmarks while requiring substantially fewer tunable parameters with reduced communication costs. Our source code is available at https://github.com/PurdueDigitalTwin/MACP.
</details>
<details>
<summary>摘要</summary>
connected and automated vehicles (CAVs) 的 perception capabilities 有 greatly enhanced  by enabling information sharing to "see through the occlusions", resulting in significant performance improvements. However, developing and training complex multi-agent perception models from scratch can be expensive and unnecessary when existing single-agent models show remarkable generalization capabilities. In this paper, we propose a new framework termed MACP, which equips a single-agent pre-trained model with cooperation capabilities. We approach this objective by identifying the key challenges of shifting from single-agent to cooperative settings, adapting the model by freezing most of its parameters and adding a few lightweight modules. We demonstrate in our experiments that the proposed framework can effectively utilize cooperative observations and outperform other state-of-the-art approaches in both simulated and real-world cooperative perception benchmarks while requiring substantially fewer tunable parameters with reduced communication costs. Our source code is available at https://github.com/PurdueDigitalTwin/MACP.Here's the translation breakdown:* connected and automated vehicles (CAVs) connected and automated vehicles (CAVs)* perception capabilities 感知能力* greatly enhanced 增强了* by enabling information sharing to "see through the occlusions" 通过共享信息来"看through the occlusions"* resulting in significant performance improvements 导致显著性能提升* However, developing and training complex multi-agent perception models from scratch can be expensive and unnecessary 然而，从零开始开发和训练复杂多代理感知模型可能是不必要的和昂贵的* when existing single-agent models show remarkable generalization capabilities 当现有的单代理模型表现出很好的泛化能力* In this paper, we propose a new framework termed MACP 在这篇论文中，我们提出了一个新的框架，名为MACP* which equips a single-agent pre-trained model with cooperation capabilities 将单代理预训练模型带有合作能力* We approach this objective by identifying the key challenges of shifting from single-agent to cooperative settings 我们通过了解单代理到合作设置的主要挑战来实现这个目标* adapting the model by freezing most of its parameters and adding a few lightweight modules 通过冻结大多数参数并添加一些轻量级模块来适应模型* We demonstrate in our experiments that the proposed framework can effectively utilize cooperative observations and outperform other state-of-the-art approaches in both simulated and real-world cooperative perception benchmarks 我们在实验中表明，提出的框架可以有效利用合作观察和在模拟和真实世界的合作感知指标上超越其他现有的方法* while requiring substantially fewer tunable parameters with reduced communication costs 而不需要大量可调参数和减少的通信成本* Our source code is available at https://github.com/PurdueDigitalTwin/MACP 我们的源代码可以在 <https://github.com/PurdueDigitalTwin/MACP> 上获取
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Techniques-for-Cervical-Cancer-Diagnosis-based-on-Pathology-and-Colposcopy-Images"><a href="#Deep-Learning-Techniques-for-Cervical-Cancer-Diagnosis-based-on-Pathology-and-Colposcopy-Images" class="headerlink" title="Deep Learning Techniques for Cervical Cancer Diagnosis based on Pathology and Colposcopy Images"></a>Deep Learning Techniques for Cervical Cancer Diagnosis based on Pathology and Colposcopy Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16662">http://arxiv.org/abs/2310.16662</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hana Ahmadzadeh Sarhangi, Dorsa Beigifard, Elahe Farmani, Hamidreza Bolhasani</li>
<li>for: 本研究旨在探讨 Deep Learning 技术在预防和诊断 cervical cancer 方面的潜在应用，以提高诊断的准确率和效率。</li>
<li>methods: 本研究使用 Deep Learning 技术进行训练，包括分类、 segmentation 和检测任务，以提高预防和诊断 cervical cancer 的精度和效率。</li>
<li>results: 研究发现，Deep Learning 技术可以帮助提高预防和诊断 cervical cancer 的精度和效率，并且可以减少人类 Error 的影响。<details>
<summary>Abstract</summary>
Cervical cancer is a prevalent disease affecting millions of women worldwide every year. It requires significant attention, as early detection during the precancerous stage provides an opportunity for a cure. The screening and diagnosis of cervical cancer rely on cytology and colposcopy methods. Deep learning, a promising technology in computer vision, has emerged as a potential solution to improve the accuracy and efficiency of cervical cancer screening compared to traditional clinical inspection methods that are prone to human error. This review article discusses cervical cancer and its screening processes, followed by the Deep Learning training process and the classification, segmentation, and detection tasks for cervical cancer diagnosis. Additionally, we explored the most common public datasets used in both cytology and colposcopy and highlighted the popular and most utilized architectures that researchers have applied to both cytology and colposcopy. We reviewed 24 selected practical papers in this study and summarized them. This article highlights the remarkable efficiency in enhancing the precision and speed of cervical cancer analysis by Deep Learning, bringing us closer to early diagnosis and saving lives.
</details>
<details>
<summary>摘要</summary>
cervical cancer是每年全球多 millones of women affected by the disease，需要 significativ attention，因为 early detection during the precancerous stage provides an opportunity for a cure. Screening and diagnosis of cervical cancer rely on cytology and colposcopy methods，Deep learning，a promising technology in computer vision，has emerged as a potential solution to improve the accuracy and efficiency of cervical cancer screening compared to traditional clinical inspection methods that are prone to human error.this review article discusses cervical cancer and its screening processes，followed by the Deep Learning training process and the classification, segmentation, and detection tasks for cervical cancer diagnosis. Additionally, we explored the most common public datasets used in both cytology and colposcopy and highlighted the popular and most utilized architectures that researchers have applied to both cytology and colposcopy. We reviewed 24 selected practical papers in this study and summarized them. This article highlights the remarkable efficiency in enhancing the precision and speed of cervical cancer analysis by Deep Learning，bringing us closer to early diagnosis and saving lives.
</details></li>
</ul>
<hr>
<h2 id="EmoCLIP-A-Vision-Language-Method-for-Zero-Shot-Video-Facial-Expression-Recognition"><a href="#EmoCLIP-A-Vision-Language-Method-for-Zero-Shot-Video-Facial-Expression-Recognition" class="headerlink" title="EmoCLIP: A Vision-Language Method for Zero-Shot Video Facial Expression Recognition"></a>EmoCLIP: A Vision-Language Method for Zero-Shot Video Facial Expression Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16640">http://arxiv.org/abs/2310.16640</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nickyfot/emoclip">https://github.com/nickyfot/emoclip</a></li>
<li>paper_authors: Niki Maria Foteinopoulou, Ioannis Patras</li>
<li>for: 这篇论文的目的是提高 dynamic FER 中的表情识别精度，并且扩展表情识别的调estre spectrum。</li>
<li>methods: 这篇论文提出了一个 novel vision-language model，使用 sample-level text descriptions 作为自然语言监督，以增强网络内部化的复杂表情表现。</li>
<li>results: 这篇论文的结果显示，使用 sample-level descriptions 监督的方法可以在零 shot 分类中实现 Significant Improvements，比如在某些 datasets 上与 CLIP 相比，增加了10% 以上的 Weighted Average Recall 和5% 以上的 Unweighted Average Recall。此外，这篇论文也评估了这种网络在下游任务中的表现，例如 mental health symptom estimation，并取得了与现有方法相似或更高的表现。<details>
<summary>Abstract</summary>
Facial Expression Recognition (FER) is a crucial task in affective computing, but its conventional focus on the seven basic emotions limits its applicability to the complex and expanding emotional spectrum. To address the issue of new and unseen emotions present in dynamic in-the-wild FER, we propose a novel vision-language model that utilises sample-level text descriptions (i.e. captions of the context, expressions or emotional cues) as natural language supervision, aiming to enhance the learning of rich latent representations, for zero-shot classification. To test this, we evaluate using zero-shot classification of the model trained on sample-level descriptions on four popular dynamic FER datasets. Our findings show that this approach yields significant improvements when compared to baseline methods. Specifically, for zero-shot video FER, we outperform CLIP by over 10\% in terms of Weighted Average Recall and 5\% in terms of Unweighted Average Recall on several datasets. Furthermore, we evaluate the representations obtained from the network trained using sample-level descriptions on the downstream task of mental health symptom estimation, achieving performance comparable or superior to state-of-the-art methods and strong agreement with human experts. Namely, we achieve a Pearson's Correlation Coefficient of up to 0.85 on schizophrenia symptom severity estimation, which is comparable to human experts' agreement. The code is publicly available at: https://github.com/NickyFot/EmoCLIP.
</details>
<details>
<summary>摘要</summary>
Facial Expression Recognition (FER) 是人工智能情感计算中的关键任务，但它的传统 фокус只是七种基本情感限制了其应用范围。为解决新和未经见的情感在动态的自然环境中的FER问题，我们提出了一种新的视觉语言模型，利用样本级文本描述（例如，情绪或情感cue的标签）作为自然语言监督，以增强模型学习的质折表示。为测试这种方法，我们在四个流行的动态FER数据集上进行零 shot 分类测试。我们的结果显示，这种方法可以与基准方法相比，提供显著改善。特别是，在零 shot 视频FER问题上，我们超过 CLIP 的10%以上在权重平均回归和5%以上在无权重平均回归方面进行了比较优秀的表现。此外，我们使用模型通过样本级文本描述学习获得的表示在下游任务中的精神病状 симптом估计中表现出色，与当前的状态 искусственный智能技术和人类专家的协同达到了高度一致。具体来说，我们在各种精神病状Symptom Severity估计中达到了0.85的佩森相关系数，与人类专家的一致度相当。代码可以在 GitHub 上获取：https://github.com/NickyFot/EmoCLIP。
</details></li>
</ul>
<hr>
<h2 id="Driving-through-the-Concept-Gridlock-Unraveling-Explainability-Bottlenecks-in-Automated-Driving"><a href="#Driving-through-the-Concept-Gridlock-Unraveling-Explainability-Bottlenecks-in-Automated-Driving" class="headerlink" title="Driving through the Concept Gridlock: Unraveling Explainability Bottlenecks in Automated Driving"></a>Driving through the Concept Gridlock: Unraveling Explainability Bottlenecks in Automated Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16639">http://arxiv.org/abs/2310.16639</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jessicamecht/concept_gridlock">https://github.com/jessicamecht/concept_gridlock</a></li>
<li>paper_authors: Jessica Echterhoff, An Yan, Kyungtae Han, Amr Abdelraouf, Rohit Gupta, Julian McAuley</li>
<li>for: 这个论文是为了提出一种基于概念瓶颈模型的可解释机器学习方法，用于解释自动驾驶车辆的决策和行为。</li>
<li>methods: 该论文使用了概念瓶颈模型，将人类定义的概念编码到模型中，以实现可解释的机器学习。</li>
<li>results: 该论文提出了一种新的方法，使用概念瓶颈作为视觉特征来预测和解释用户和车辆行为，并实现了与离散特征的竞争性表现。<details>
<summary>Abstract</summary>
Concept bottleneck models have been successfully used for explainable machine learning by encoding information within the model with a set of human-defined concepts. In the context of human-assisted or autonomous driving, explainability models can help user acceptance and understanding of decisions made by the autonomous vehicle, which can be used to rationalize and explain driver or vehicle behavior. We propose a new approach using concept bottlenecks as visual features for control command predictions and explanations of user and vehicle behavior. We learn a human-understandable concept layer that we use to explain sequential driving scenes while learning vehicle control commands. This approach can then be used to determine whether a change in a preferred gap or steering commands from a human (or autonomous vehicle) is led by an external stimulus or change in preferences. We achieve competitive performance to latent visual features while gaining interpretability within our model setup.
</details>
<details>
<summary>摘要</summary>
<<SYS>>用概念瓶颈模型实现可解释机器学习，将人类定义的概念编码到模型中，以解释自动驾驶决策的可解释性。在人工或自动驾驶场景下，解释模型可以帮助用户理解和接受自动车辆的决策，并用于解释机动员或车辆行为的原因。我们提出了一种新的方法，利用概念瓶颈作为视觉特征来预测和解释用户和车辆行为。我们学习了人类可理解的概念层，用于解释顺序驾驶场景，同时学习车辆控制命令。这种方法可以确定外部刺激或变化的是否导致人类或自动车辆的控制命令变化。我们实现了与潜在视觉特征相当的竞争性性能，同时在我们的模型设置中获得了解释性。
</details></li>
</ul>
<hr>
<h2 id="EdgeCalib-Multi-Frame-Weighted-Edge-Features-for-Automatic-Targetless-LiDAR-Camera-Calibration"><a href="#EdgeCalib-Multi-Frame-Weighted-Edge-Features-for-Automatic-Targetless-LiDAR-Camera-Calibration" class="headerlink" title="EdgeCalib: Multi-Frame Weighted Edge Features for Automatic Targetless LiDAR-Camera Calibration"></a>EdgeCalib: Multi-Frame Weighted Edge Features for Automatic Targetless LiDAR-Camera Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16629">http://arxiv.org/abs/2310.16629</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingchen Li, Yifan Duan, Beibei Wang, Haojie Ren, Guoliang You, Yu Sheng, Jianmin Ji, Yanyong Zhang</li>
<li>for: 提高 LiDAR 和摄像头之间的外部准确定时参数的自动化线程，以便在实际场景中实现高精度的多模态感知系统。</li>
<li>methods: 基于边缘特征的自动化线程，包括在图像和点云中提取稳定和可靠的边缘特征，并通过多帧权重策略对边缘特征进行过滤。最后，通过边缘匹配约束来优化精度的外部参数。</li>
<li>results: 在 KITTI 数据集和自己的数据集上进行了评估，实现了Rotation 精度0.086度和Translation 精度0.977 cm，超过了现有的边缘基于准确定时参数的方法。<details>
<summary>Abstract</summary>
In multimodal perception systems, achieving precise extrinsic calibration between LiDAR and camera is of critical importance. Previous calibration methods often required specific targets or manual adjustments, making them both labor-intensive and costly. Online calibration methods based on features have been proposed, but these methods encounter challenges such as imprecise feature extraction, unreliable cross-modality associations, and high scene-specific requirements. To address this, we introduce an edge-based approach for automatic online calibration of LiDAR and cameras in real-world scenarios. The edge features, which are prevalent in various environments, are aligned in both images and point clouds to determine the extrinsic parameters. Specifically, stable and robust image edge features are extracted using a SAM-based method and the edge features extracted from the point cloud are weighted through a multi-frame weighting strategy for feature filtering. Finally, accurate extrinsic parameters are optimized based on edge correspondence constraints. We conducted evaluations on both the KITTI dataset and our dataset. The results show a state-of-the-art rotation accuracy of 0.086{\deg} and a translation accuracy of 0.977 cm, outperforming existing edge-based calibration methods in both precision and robustness.
</details>
<details>
<summary>摘要</summary>
在多模态感知系统中，精确的外部准确性 calibration between LiDAR 和摄像头是关键。先前的准确方法通常需要特定的目标或手动调整，使其成为劳动密集和昂贵的。基于特征的在线准确方法已经被提议，但这些方法遇到了准确特征提取、交叉模式关联不可靠和高场景特定性的挑战。为解决这个问题，我们介绍了一种基于边的方法，用于自动在实际场景中进行 LiDAR 和摄像头的在线准确性 calibration。在图像和点云中对边特征进行对齐，以确定外部参数。 Specifically, 使用 SAM 方法提取稳定和可靠的图像边特征，并通过多帧权重策略来筛选点云中的边特征。最后，基于边对应关系的约束，进行高精度的外部参数优化。我们在 KITTI 数据集和我们自己的数据集上进行了评估，结果显示，我们的方法在精度和可靠性方面占据了领先地位，与现有的边基于准确方法相比，在精度和可靠性方面都有显著的提升。
</details></li>
</ul>
<hr>
<h2 id="Real-time-6-DoF-Pose-Estimation-by-an-Event-based-Camera-using-Active-LED-Markers"><a href="#Real-time-6-DoF-Pose-Estimation-by-an-Event-based-Camera-using-Active-LED-Markers" class="headerlink" title="Real-time 6-DoF Pose Estimation by an Event-based Camera using Active LED Markers"></a>Real-time 6-DoF Pose Estimation by an Event-based Camera using Active LED Markers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16618">http://arxiv.org/abs/2310.16618</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gerald Ebmer, Adam Loch, Minh Nhat Vu, Germain Haessig, Roberto Mecca, Markus Vincze, Christian Hartl-Nesic, Andreas Kugi</li>
<li>for: 本研究旨在提出一种简单 yet effective的事件基于pose数据估算系统，用于快速和准确的 pose数据估算。</li>
<li>methods: 本研究使用了活动LED标记（ALM），并提出了一种基于事件的pose数据估算算法，可以在实时中运行，并且可以在不可靠的视觉条件下保持精度。</li>
<li>results: 实验结果表明，提出的方法可以在实时中运行，并且可以在静止和动态场景中保持高度的计算速度和精度。<details>
<summary>Abstract</summary>
Real-time applications for autonomous operations depend largely on fast and robust vision-based localization systems. Since image processing tasks require processing large amounts of data, the computational resources often limit the performance of other processes. To overcome this limitation, traditional marker-based localization systems are widely used since they are easy to integrate and achieve reliable accuracy. However, classical marker-based localization systems significantly depend on standard cameras with low frame rates, which often lack accuracy due to motion blur. In contrast, event-based cameras provide high temporal resolution and a high dynamic range, which can be utilized for fast localization tasks, even under challenging visual conditions. This paper proposes a simple but effective event-based pose estimation system using active LED markers (ALM) for fast and accurate pose estimation. The proposed algorithm is able to operate in real time with a latency below \SI{0.5}{\milli\second} while maintaining output rates of \SI{3}{\kilo \hertz}. Experimental results in static and dynamic scenarios are presented to demonstrate the performance of the proposed approach in terms of computational speed and absolute accuracy, using the OptiTrack system as the basis for measurement.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="mathbb-VD-mathbb-GR-Boosting-mathbb-V-isual-mathbb-D-ialog-with-Cascaded-Spatial-Temporal-Multi-Modal-mathbb-GR-aphs"><a href="#mathbb-VD-mathbb-GR-Boosting-mathbb-V-isual-mathbb-D-ialog-with-Cascaded-Spatial-Temporal-Multi-Modal-mathbb-GR-aphs" class="headerlink" title="$\mathbb{VD}$-$\mathbb{GR}$: Boosting $\mathbb{V}$isual $\mathbb{D}$ialog with Cascaded Spatial-Temporal Multi-Modal $\mathbb{GR}$aphs"></a>$\mathbb{VD}$-$\mathbb{GR}$: Boosting $\mathbb{V}$isual $\mathbb{D}$ialog with Cascaded Spatial-Temporal Multi-Modal $\mathbb{GR}$aphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16590">http://arxiv.org/abs/2310.16590</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adnen Abdessaied, Lei Shi, Andreas Bulling</li>
<li>for: 本文提出了一种新的视觉对话模型（$\mathbb{VD}$-$\mathbb{GR}$），它将预训练语言模型（LM）与图 neural networks（GNN）结合在一起，以利用它们的优势。</li>
<li>methods: 本文使用了多modal GNN进行特征处理，并利用本地结构信息以前进行BERT层的全球注意力。此外，本文还提出了核心节点，它们连接到每个模式图中的所有节点，使模型可以在不同模式之间传递信息。</li>
<li>results: 根据VisDial v1.0、VisDial v0.9、VisDialConv和VisPro等四个数据集的评估结果，$\mathbb{VD}$-$\mathbb{GR}$模型在所有四个数据集上均达到了新的状态级 результа。<details>
<summary>Abstract</summary>
We propose $\mathbb{VD}$-$\mathbb{GR}$ - a novel visual dialog model that combines pre-trained language models (LMs) with graph neural networks (GNNs). Prior works mainly focused on one class of models at the expense of the other, thus missing out on the opportunity of combining their respective benefits. At the core of $\mathbb{VD}$-$\mathbb{GR}$ is a novel integration mechanism that alternates between spatial-temporal multi-modal GNNs and BERT layers, and that covers three distinct contributions: First, we use multi-modal GNNs to process the features of each modality (image, question, and dialog history) and exploit their local structures before performing BERT global attention. Second, we propose hub-nodes that link to all other nodes within one modality graph, allowing the model to propagate information from one GNN (modality) to the other in a cascaded manner. Third, we augment the BERT hidden states with fine-grained multi-modal GNN features before passing them to the next $\mathbb{VD}$-$\mathbb{GR}$ layer. Evaluations on VisDial v1.0, VisDial v0.9, VisDialConv, and VisPro show that $\mathbb{VD}$-$\mathbb{GR}$ achieves new state-of-the-art results across all four datasets.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的视觉对话模型，称为$\mathbb{VD}$-$\mathbb{GR}$，它结合了预训练语言模型（LM）和图 neural network（GNN）。先前的工作主要集中在一类模型上，忽略了另一类模型的机会，因此我们决定结合它们的优点。 $\mathbb{VD}$-$\mathbb{GR}$ 的核心机制是一种新的集成机制，它在空间-时间多Modal GNN 和 BERT 层之间进行交替，并包括以下三个贡献：1. 我们使用多Modal GNN 处理每个模式（图像、问题和对话历史）的特征，并利用它们的本地结构，然后进行 BERT 全局注意力。2. 我们提出了核心节点，它们与一个模式图中的所有节点相连，使模型可以在一个模式中传递信息，并在另一个模式中进行协调。3. 我们将 BERT 隐藏状态与细致的多Modal GNN 特征相加，然后传递给下一层 $\mathbb{VD}$-$\mathbb{GR}$。我们对 VisDial v1.0、VisDial v0.9、VisDialConv 和 VisPro 进行评估，得到了新的领域记录。
</details></li>
</ul>
<hr>
<h2 id="Flow-Attention-based-Spatio-Temporal-Aggregation-Network-for-3D-Mask-Detection"><a href="#Flow-Attention-based-Spatio-Temporal-Aggregation-Network-for-3D-Mask-Detection" class="headerlink" title="Flow-Attention-based Spatio-Temporal Aggregation Network for 3D Mask Detection"></a>Flow-Attention-based Spatio-Temporal Aggregation Network for 3D Mask Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16569">http://arxiv.org/abs/2310.16569</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/josephcao0327/fasten">https://github.com/josephcao0327/fasten</a></li>
<li>paper_authors: Yuxin Cao, Yian Li, Yumeng Zhu, Derui Wang, Minhui Xue<br>for:The paper aims to address the challenges of anti-spoofing detection in face recognition systems, specifically the generalizability insufficiency of deep-learning-based methods in 3D masks.methods:The proposed method, FASTEN, is a novel 3D mask detection framework that leverages remote photoplethysmography (rPPG) technology and tailors a network for focusing on fine-grained details in large movements. The network consists of three key modules: facial optical flow network, flow attention, and spatio-temporal aggregation.results:FASTEN outperforms eight competitors in terms of multiple detection metrics, requiring only five frames of input. The proposed method has been deployed in real-world mobile devices for practical 3D mask detection.<details>
<summary>Abstract</summary>
Anti-spoofing detection has become a necessity for face recognition systems due to the security threat posed by spoofing attacks. Despite great success in traditional attacks, most deep-learning-based methods perform poorly in 3D masks, which can highly simulate real faces in appearance and structure, suffering generalizability insufficiency while focusing only on the spatial domain with single frame input. This has been mitigated by the recent introduction of a biomedical technology called rPPG (remote photoplethysmography). However, rPPG-based methods are sensitive to noisy interference and require at least one second (> 25 frames) of observation time, which induces high computational overhead. To address these challenges, we propose a novel 3D mask detection framework, called FASTEN (Flow-Attention-based Spatio-Temporal aggrEgation Network). We tailor the network for focusing more on fine-grained details in large movements, which can eliminate redundant spatio-temporal feature interference and quickly capture splicing traces of 3D masks in fewer frames. Our proposed network contains three key modules: 1) a facial optical flow network to obtain non-RGB inter-frame flow information; 2) flow attention to assign different significance to each frame; 3) spatio-temporal aggregation to aggregate high-level spatial features and temporal transition features. Through extensive experiments, FASTEN only requires five frames of input and outperforms eight competitors for both intra-dataset and cross-dataset evaluations in terms of multiple detection metrics. Moreover, FASTEN has been deployed in real-world mobile devices for practical 3D mask detection.
</details>
<details>
<summary>摘要</summary>
face recognition 系统中的反伪检测已成为必备因素，由于伪检测攻击的安全威胁。虽然传统攻击方法取得了很大成功，但大多数深度学习基于方法在3D面具下表现不佳，它们在遥感特征上缺乏普适性，只集中在空间领域，使用单帧输入。这一问题得到了最近的生物医学技术——远程血氧测量（rPPG）的解决。然而，rPPG基于方法对干扰噪音敏感，需要至少一秒（> 25帧）的观察时间，这会导致高度的计算开销。为解决这些挑战，我们提出了一种新的3D面具检测框架，called FASTEN（流量注意力基于空间-时间聚合网络）。我们修改网络以便更注重大运动中细节，可以消除重复的空间-时间特征干扰，快速捕捉3D面具的拼接迹象。我们的提议网络包括以下三个关键模块：1） facial optical flow网络获取非RGB间帧流动信息；2）流量注意力将每帧图像分配不同的重要性；3）空间-时间聚合以聚合高级空间特征和时间变化特征。经过广泛的实验，FASTEN只需输入5帧，并在多个检测指标方面超越8个竞争对手。此外，FASTEN已经在实际应用中的移动设备中部署了实用3D面具检测。
</details></li>
</ul>
<hr>
<h2 id="ParisLuco3D-A-high-quality-target-dataset-for-domain-generalization-of-LiDAR-perception"><a href="#ParisLuco3D-A-high-quality-target-dataset-for-domain-generalization-of-LiDAR-perception" class="headerlink" title="ParisLuco3D: A high-quality target dataset for domain generalization of LiDAR perception"></a>ParisLuco3D: A high-quality target dataset for domain generalization of LiDAR perception</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16542">http://arxiv.org/abs/2310.16542</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jules Sanchez, Louis Soum-Fontez, Jean-Emmanuel Deschaud, Francois Goulette</li>
<li>for: 本研究旨在提供一个跨多个频道的评估数据集，以便评估不同来源数据的性能。</li>
<li>methods: 本研究使用了一种新的评估方法，可以在不同的频道上进行公正的比较。</li>
<li>results: 本研究提供了一个新的评估数据集，可以帮助研究人员更好地评估不同来源数据的性能。<details>
<summary>Abstract</summary>
LiDAR is a sensor system that supports autonomous driving by gathering precise geometric information about the scene. Exploiting this information for perception is interesting as the amount of available data increases.   As the quantitative performance of various perception tasks has improved, the focus has shifted from source-to-source perception to domain adaptation and domain generalization for perception. These new goals require access to a large variety of domains for evaluation. Unfortunately, the various annotation strategies of data providers complicate the computation of cross-domain performance based on the available data   This paper provides a novel dataset, specifically designed for cross-domain evaluation to make it easier to evaluate the performance of various source datasets. Alongside the dataset, a flexible online benchmark is provided to ensure a fair comparison across methods.
</details>
<details>
<summary>摘要</summary>
李达（LiDAR）是一种感知系统，用于支持自动驾驶，它可以准确地收集干扰场景的几何信息。利用这些信息进行感知是有趣的，因为数据量的增加会提高感知任务的量化性能。随着感知任务的数学性能的改进，关注点从源源感知转移到领域适应和领域总结，以便更好地评估各种来源数据的性能。然而，不同数据提供者的注释策略会使计算交叉领域性能变得复杂。本文提供了一个新的数据集，专门用于交叉领域评估，以便更好地评估各种来源数据的性能。此外，还提供了一个灵活的在线测试台，以确保对各种方法进行公平的比较。
</details></li>
</ul>
<hr>
<h2 id="Dual-Defense-Adversarial-Traceable-and-Invisible-Robust-Watermarking-against-Face-Swapping"><a href="#Dual-Defense-Adversarial-Traceable-and-Invisible-Robust-Watermarking-against-Face-Swapping" class="headerlink" title="Dual Defense: Adversarial, Traceable, and Invisible Robust Watermarking against Face Swapping"></a>Dual Defense: Adversarial, Traceable, and Invisible Robust Watermarking against Face Swapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16540">http://arxiv.org/abs/2310.16540</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunming Zhang, Dengpan Ye, Caiyun Xie, Long Tang, Chuanxi Chen, Ziyi Liu, Jiacheng Deng</li>
<li>for: 防范深刻的违当用途，如人脸替换，以防止违信传播和身份篡改。</li>
<li>methods: 提出了一种新的总防御机制，即双重防御， combine traceability和对抗性，以快速应对违当用途。</li>
<li>results: 实验表明，双重防御可以实现最佳的总防御成功率，并且在不同的人脸数据集上表现出优秀的通用性和对抗性。<details>
<summary>Abstract</summary>
The malicious applications of deep forgery, represented by face swapping, have introduced security threats such as misinformation dissemination and identity fraud. While some research has proposed the use of robust watermarking methods to trace the copyright of facial images for post-event traceability, these methods cannot effectively prevent the generation of forgeries at the source and curb their dissemination. To address this problem, we propose a novel comprehensive active defense mechanism that combines traceability and adversariality, called Dual Defense. Dual Defense invisibly embeds a single robust watermark within the target face to actively respond to sudden cases of malicious face swapping. It disrupts the output of the face swapping model while maintaining the integrity of watermark information throughout the entire dissemination process. This allows for watermark extraction at any stage of image tracking for traceability. Specifically, we introduce a watermark embedding network based on original-domain feature impersonation attack. This network learns robust adversarial features of target facial images and embeds watermarks, seeking a well-balanced trade-off between watermark invisibility, adversariality, and traceability through perceptual adversarial encoding strategies. Extensive experiments demonstrate that Dual Defense achieves optimal overall defense success rates and exhibits promising universality in anti-face swapping tasks and dataset generalization ability. It maintains impressive adversariality and traceability in both original and robust settings, surpassing current forgery defense methods that possess only one of these capabilities, including CMUA-Watermark, Anti-Forgery, FakeTagger, or PGD methods.
</details>
<details>
<summary>摘要</summary>
“深层伪造的黑客应用，例如脸部交换，导致安全风险，如误传信息和身份骗复。一些研究提出使用可靠的水印方法来追溯颜面图像的版权，以便后续追踪，但这些方法无法有效防止伪造的生成和传播。为解决这个问题，我们提出了一个全新的综合式活动防御机制，称为“双重防御”（Dual Defense）。这个机制隐藏式嵌入了单一的可靠水印在目标脸部中，以活动地回应突然的黑客脸部交换。它破坏该交换模型的输出，同时保持水印信息的完整性 throughout the entire 传播过程。这使得可以在任何追踪过程中提取水印。 Specifically, we introduce a water mark embedding network based on original-domain feature impersonation attack. This network learns robust adversarial features of target facial images and embeds watermarks, seeking a well-balanced trade-off between water mark invisibility, adversariality, and traceability through perceptual adversarial encoding strategies. Extensive experiments demonstrate that Dual Defense achieves optimal overall defense success rates and exhibits promising universality in anti-face swapping tasks and dataset generalization ability. It maintains impressive adversariality and traceability in both original and robust settings, surpassing current forgery defense methods that possess only one of these capabilities, including CMUA-Watermark, Anti-Forgery, FakeTagger, or PGD methods.”
</details></li>
</ul>
<hr>
<h2 id="Learning-Robust-Deep-Visual-Representations-from-EEG-Brain-Recordings"><a href="#Learning-Robust-Deep-Visual-Representations-from-EEG-Brain-Recordings" class="headerlink" title="Learning Robust Deep Visual Representations from EEG Brain Recordings"></a>Learning Robust Deep Visual Representations from EEG Brain Recordings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16532">http://arxiv.org/abs/2310.16532</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/prajwalsingh/eegstylegan-ada">https://github.com/prajwalsingh/eegstylegan-ada</a></li>
<li>paper_authors: Prajwal Singh, Dwip Dalal, Gautam Vashishtha, Krishna Miyapuram, Shanmuganathan Raman<br>for:This paper is written for researchers and scientists interested in brain-computer interfacing and reconstruction of visual images from brain Electroencephalography (EEG) signals.methods:The paper proposes a two-stage method for image generation and classification using EEG signals. The first step is to obtain EEG-derived features for robust learning of deep representations, and the second step is to utilize the learned representation for image generation and classification. The paper uses deep-learning architectures with supervised and contrastive learning methods.results:The paper demonstrates the generalizability of the feature extraction pipeline across three different datasets using deep-learning architectures with supervised and contrastive learning methods. The paper also shows that a subject invariant linearly separable visual representation can be learned using EEG data alone in an unimodal setting, which gives better k-means accuracy compared to a joint representation learning between EEG and images. Finally, the paper proposes a novel framework to transform unseen images into the EEG space and reconstruct them with approximation, showcasing the potential for image reconstruction from EEG signals. The proposed image synthesis method from EEG shows 62.9% and 36.13% inception score improvement on the EEGCVPR40 and the Thoughtviz datasets, which is better than state-of-the-art performance in GAN.<details>
<summary>Abstract</summary>
Decoding the human brain has been a hallmark of neuroscientists and Artificial Intelligence researchers alike. Reconstruction of visual images from brain Electroencephalography (EEG) signals has garnered a lot of interest due to its applications in brain-computer interfacing. This study proposes a two-stage method where the first step is to obtain EEG-derived features for robust learning of deep representations and subsequently utilize the learned representation for image generation and classification. We demonstrate the generalizability of our feature extraction pipeline across three different datasets using deep-learning architectures with supervised and contrastive learning methods. We have performed the zero-shot EEG classification task to support the generalizability claim further. We observed that a subject invariant linearly separable visual representation was learned using EEG data alone in an unimodal setting that gives better k-means accuracy as compared to a joint representation learning between EEG and images. Finally, we propose a novel framework to transform unseen images into the EEG space and reconstruct them with approximation, showcasing the potential for image reconstruction from EEG signals. Our proposed image synthesis method from EEG shows 62.9% and 36.13% inception score improvement on the EEGCVPR40 and the Thoughtviz datasets, which is better than state-of-the-art performance in GAN.
</details>
<details>
<summary>摘要</summary>
neuroscientists 和人工智能研究者都在努力 decode the human brain. 使用电encephalography (EEG) 信号重建视觉图像的技术吸引了很多关注，因为它在Brain-computer interfacing中有很多应用。本研究提出了一种两个阶段的方法，其中第一个阶段是使用EEG信号获得可靠的特征，然后使用这些特征进行图像生成和分类。我们在三个不同的数据集上使用深度学习架构和监督学习方法来验证我们的特征提取管道的一致性。此外，我们还完成了零shot EEG分类任务，以更进一步地证明我们的特征提取管道的一致性。我们发现，使用EEG数据 alone 在单模式下可以学习一个主动抗干扰的视觉表示，这个表示可以在k-means分类任务中达到更高的准确率。最后，我们提出了一种将未看过的图像转换到EEG空间的框架，并使用这些图像进行重建，这展示了图像从EEG信号中的重建的潜在可能性。我们的提出的图像生成方法在EEGCVPR40和Thoughtviz数据集上达到了62.9%和36.13%的inception分数提升，这比state-of-the-art的GAN性能更好。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Document-Information-Analysis-with-Multi-Task-Pre-training-A-Robust-Approach-for-Information-Extraction-in-Visually-Rich-Documents"><a href="#Enhancing-Document-Information-Analysis-with-Multi-Task-Pre-training-A-Robust-Approach-for-Information-Extraction-in-Visually-Rich-Documents" class="headerlink" title="Enhancing Document Information Analysis with Multi-Task Pre-training: A Robust Approach for Information Extraction in Visually-Rich Documents"></a>Enhancing Document Information Analysis with Multi-Task Pre-training: A Robust Approach for Information Extraction in Visually-Rich Documents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16527">http://arxiv.org/abs/2310.16527</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tofik Ali, Partha Pratim Roy</li>
<li>for: 这个研究是为了开发一种专门为文档信息分析而设计的深度学习模型，包括文档分类、实体关系EXTRACTION和文档视频问答。</li>
<li>methods: 该模型使用基于transformer的模型来编码文档图像中的所有信息，包括文字、视觉和布局信息。模型首先预训练，然后进行精度调整以适应不同的文档图像分析任务。该模型还包括在预训练阶段进行多个任务的混合预训练，以及在多个数据集上进行精度调整。</li>
<li>results: 该模型在多个任务上达到了出色的效果，包括文档分类（RVL-CDIP数据集上的准确率为95.87%）、实体关系EXTRACTION（FUNSD、CORD、SROIE和Kleister-NDA数据集上的F1分数分别为0.9306、0.9804、0.9794和0.8742）和文档视频问答（DocVQA数据集上的ANLS分数为0.8468）。结果表明该模型可以快速和准确地理解和解释复杂的文档布局和内容，使其成为文档分析任务中的一种有前途的工具。<details>
<summary>Abstract</summary>
This paper introduces a deep learning model tailored for document information analysis, emphasizing document classification, entity relation extraction, and document visual question answering. The proposed model leverages transformer-based models to encode all the information present in a document image, including textual, visual, and layout information. The model is pre-trained and subsequently fine-tuned for various document image analysis tasks. The proposed model incorporates three additional tasks during the pre-training phase, including reading order identification of different layout segments in a document image, layout segments categorization as per PubLayNet, and generation of the text sequence within a given layout segment (text block). The model also incorporates a collective pre-training scheme where losses of all the tasks under consideration, including pre-training and fine-tuning tasks with all datasets, are considered. Additional encoder and decoder blocks are added to the RoBERTa network to generate results for all tasks. The proposed model achieved impressive results across all tasks, with an accuracy of 95.87% on the RVL-CDIP dataset for document classification, F1 scores of 0.9306, 0.9804, 0.9794, and 0.8742 on the FUNSD, CORD, SROIE, and Kleister-NDA datasets respectively for entity relation extraction, and an ANLS score of 0.8468 on the DocVQA dataset for visual question answering. The results highlight the effectiveness of the proposed model in understanding and interpreting complex document layouts and content, making it a promising tool for document analysis tasks.
</details>
<details>
<summary>摘要</summary>
The proposed model includes three additional tasks during the pre-training phase: identifying the reading order of different layout segments in a document image, categorizing layout segments using PubLayNet, and generating text within a given layout segment. The model also uses a collective pre-training scheme, where the losses of all tasks are considered. Additional encoder and decoder blocks are added to the RoBERTa network to generate results for all tasks.The proposed model achieved impressive results across all tasks, with an accuracy of 95.87% on the RVL-CDIP dataset for document classification, F1 scores of 0.9306, 0.9804, 0.9794, and 0.8742 on the FUNSD, CORD, SROIE, and Kleister-NDA datasets respectively for entity relation extraction, and an ANLS score of 0.8468 on the DocVQA dataset for visual question answering. These results demonstrate the effectiveness of the proposed model in understanding and interpreting complex document layouts and content, making it a promising tool for document analysis tasks.
</details></li>
</ul>
<hr>
<h2 id="Lang3DSG-Language-based-contrastive-pre-training-for-3D-Scene-Graph-prediction"><a href="#Lang3DSG-Language-based-contrastive-pre-training-for-3D-Scene-Graph-prediction" class="headerlink" title="Lang3DSG: Language-based contrastive pre-training for 3D Scene Graph prediction"></a>Lang3DSG: Language-based contrastive pre-training for 3D Scene Graph prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16494">http://arxiv.org/abs/2310.16494</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sebastian Koch, Pedro Hermosilla, Narunas Vaskevicius, Mirco Colosi, Timo Ropinski</li>
<li>for: 本研究旨在提高3D场景图模型的学习效果，因为学习3D场景图需要不仅物体标签，还需要关系注释，这些注释在数据集中非常罕见。</li>
<li>methods: 我们采用了语言基于的预训练方法，利用图像语言模型CLIP的语言编码器储存其知识，并通过对subject-predicate-object triplets的对比，将语言表示和预测的3D图像特征进行对应。</li>
<li>results: 我们的方法在主要的semantic 3D场景图标准测试集上达到了状态 искусственный智能水平，比基eline预测方法和所有现有的完全监督场景图预测方法都高出较多。此外，由于我们的场景图特征是语言对应的，因此可以在零shot情况下查询语言空间中的特征。在本文中，我们展示了使用这种特征的属性来预测场景中的房型。<details>
<summary>Abstract</summary>
D scene graphs are an emerging 3D scene representation, that models both the objects present in the scene as well as their relationships. However, learning 3D scene graphs is a challenging task because it requires not only object labels but also relationship annotations, which are very scarce in datasets. While it is widely accepted that pre-training is an effective approach to improve model performance in low data regimes, in this paper, we find that existing pre-training methods are ill-suited for 3D scene graphs. To solve this issue, we present the first language-based pre-training approach for 3D scene graphs, whereby we exploit the strong relationship between scene graphs and language. To this end, we leverage the language encoder of CLIP, a popular vision-language model, to distill its knowledge into our graph-based network. We formulate a contrastive pre-training, which aligns text embeddings of relationships (subject-predicate-object triplets) and predicted 3D graph features. Our method achieves state-of-the-art results on the main semantic 3D scene graph benchmark by showing improved effectiveness over pre-training baselines and outperforming all the existing fully supervised scene graph prediction methods by a significant margin. Furthermore, since our scene graph features are language-aligned, it allows us to query the language space of the features in a zero-shot manner. In this paper, we show an example of utilizing this property of the features to predict the room type of a scene without further training.
</details>
<details>
<summary>摘要</summary>
DScene graphs是一种emerging 3D场景表示，它模型了场景中的对象以及它们之间的关系。然而，学习3D场景图是一个具有挑战性的任务，因为它需要不仅对象标签，还需要关系注释，这些注释在数据集中很罕见。而现有的预训练方法在低数据量情况下并不适用于3D场景图。为解决这个问题，我们提出了首个语言基于的预训练方法 для3D场景图，其中我们利用了场景图和语言之间的强关系。为此，我们利用了CLIP语言编码器，一种流行的视觉语言模型，将其知识融合到我们的图基于网络中。我们提出了一种对比预训练方法，将文本 embedding（主语-谓语-词语 triplets）与预测的3D图像特征进行对比。我们的方法在主要的semantic 3D场景图标准测试集上实现了状态之 искусственный智能水平，比基elines预训练方法和所有现有的完全监督场景图预测方法在较大的margin上表现出色。此外，因为我们的场景图特征与语言空间相对应，因此我们可以在零shot情况下查询语言空间中的特征。在本文中，我们给出了一个例子，利用这种特征的属性来预测场景中的房间类型。
</details></li>
</ul>
<hr>
<h2 id="Gramian-Attention-Heads-are-Strong-yet-Efficient-Vision-Learners"><a href="#Gramian-Attention-Heads-are-Strong-yet-Efficient-Vision-Learners" class="headerlink" title="Gramian Attention Heads are Strong yet Efficient Vision Learners"></a>Gramian Attention Heads are Strong yet Efficient Vision Learners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16483">http://arxiv.org/abs/2310.16483</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lab-lvm/imagenet-models">https://github.com/lab-lvm/imagenet-models</a></li>
<li>paper_authors: Jongbin Ryu, Dongyoon Han, Jongwoo Lim</li>
<li>For: 该论文旨在提出一种新的建筑设计，以增强表达能力。* Methods: 该方法使用多个分类头（classification heads），而不是通过通道扩展或附加块来提高表达能力。这些头使用注意力基于的聚合，利用对Feature similarity进行拟合，以增强每个轻量级头的表达能力。* Results: 该方法可以在ImageNet-1K上超越现有的CNN和ViT模型，并在多个下游任务中表现出色，如COCO物体实例分割、ADE20k semantic segmentation和细化视觉分类等。 Code publicly available at: <a target="_blank" rel="noopener" href="https://github.com/Lab-LVM/imagenet-models%E3%80%82">https://github.com/Lab-LVM/imagenet-models。</a><details>
<summary>Abstract</summary>
We introduce a novel architecture design that enhances expressiveness by incorporating multiple head classifiers (\ie, classification heads) instead of relying on channel expansion or additional building blocks. Our approach employs attention-based aggregation, utilizing pairwise feature similarity to enhance multiple lightweight heads with minimal resource overhead. We compute the Gramian matrices to reinforce class tokens in an attention layer for each head. This enables the heads to learn more discriminative representations, enhancing their aggregation capabilities. Furthermore, we propose a learning algorithm that encourages heads to complement each other by reducing correlation for aggregation. Our models eventually surpass state-of-the-art CNNs and ViTs regarding the accuracy-throughput trade-off on ImageNet-1K and deliver remarkable performance across various downstream tasks, such as COCO object instance segmentation, ADE20k semantic segmentation, and fine-grained visual classification datasets. The effectiveness of our framework is substantiated by practical experimental results and further underpinned by generalization error bound. We release the code publicly at: https://github.com/Lab-LVM/imagenet-models.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的建筑设计，通过多个头分类器（即分类头）来提高表达能力，而不是依赖通道扩展或额外组件。我们的方法使用注意力基于的汇集，通过对每个头进行积分来增强轻量级的头。我们计算 Gramian 矩阵来强制类别标签在注意力层中增强多个头的汇集能力。此外，我们提出了一种学习算法，使得头们之间减少相关性，以便协同汇集。我们的模型最终超越了状态艺术 CNN 和 ViT 在 ImageNet-1K 上的精度-通过put trade-off，并在多个下游任务上表现出色，如 COCO 对象实例分割、ADE20k semantic segmentation 和细化视觉分类 dataset。我们的框架的有效性得到了实验证明，并被更加强大的通用Error bound 支持。我们在 GitHub 上公开了代码：https://github.com/Lab-LVM/imagenet-models。
</details></li>
</ul>
<hr>
<h2 id="Show-from-Tell-Audio-Visual-Modelling-in-Clinical-Settings"><a href="#Show-from-Tell-Audio-Visual-Modelling-in-Clinical-Settings" class="headerlink" title="Show from Tell: Audio-Visual Modelling in Clinical Settings"></a>Show from Tell: Audio-Visual Modelling in Clinical Settings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16477">http://arxiv.org/abs/2310.16477</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianbo Jiao, Mohammad Alsharid, Lior Drukker, Aris T. Papageorghiou, Andrew Zisserman, J. Alison Noble</li>
<li>for: 本研究は医疗设定下でのaudio-visual模型を提案し、専门家のアノテーションなしで医疗タスクに役立つ医学的表现を学习するための方法を提案します。</li>
<li>methods: 本研究では、自律学习のための简単で效率的な多Modal自律学习フレームワークを提案します。この方法では、speech音声を参照にして、ultrasound画像中の解剖学的领域を検出することができます。</li>
<li>results: 実験结果では、大规模の医疗多Modal ultrasoundビデオデータセットに対して、提案された自律学习方法は、専门家アノテーションなしで高性能な自动化下流医疗タスクを実现するための良い转移学习表现を学习することができました。さらに、全ての学习データを使用した完全な学习方法を上回るパフォーマンスを示しました。<details>
<summary>Abstract</summary>
Auditory and visual signals usually present together and correlate with each other, not only in natural environments but also in clinical settings. However, the audio-visual modelling in the latter case can be more challenging, due to the different sources of audio/video signals and the noise (both signal-level and semantic-level) in auditory signals -- usually speech. In this paper, we consider audio-visual modelling in a clinical setting, providing a solution to learn medical representations that benefit various clinical tasks, without human expert annotation. A simple yet effective multi-modal self-supervised learning framework is proposed for this purpose. The proposed approach is able to localise anatomical regions of interest during ultrasound imaging, with only speech audio as a reference. Experimental evaluations on a large-scale clinical multi-modal ultrasound video dataset show that the proposed self-supervised method learns good transferable anatomical representations that boost the performance of automated downstream clinical tasks, even outperforming fully-supervised solutions.
</details>
<details>
<summary>摘要</summary>
听觉和视觉信号通常同时存在，并且在自然环境和临床设置中呈相关关系。然而，在临床情况下，听觉-视觉模型化可能更加困难，主要因为听觉信号中的噪音（包括信号水平噪音和semantic-level噪音），以及不同的听觉/视觉信号来源。在这篇论文中，我们考虑了在临床设置下的听觉-视觉模型化，提供一种不需要人工专家标注的解决方案。我们提议的方法是一种简单 yet effective的多modal自动学习框架，可以在听觉信号中 lokalisir anatomical region of interest。我们对大规模的临床多Modal Ultrasound视频数据集进行了实验评估，结果表明，我们的自动学习方法可以学习出好的传输性骨骼表示，提高了下游临床任务的自动化处理效果，甚至超过了全部监督的解决方案。
</details></li>
</ul>
<hr>
<h2 id="DualMatch-Robust-Semi-Supervised-Learning-with-Dual-Level-Interaction"><a href="#DualMatch-Robust-Semi-Supervised-Learning-with-Dual-Level-Interaction" class="headerlink" title="DualMatch: Robust Semi-Supervised Learning with Dual-Level Interaction"></a>DualMatch: Robust Semi-Supervised Learning with Dual-Level Interaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16459">http://arxiv.org/abs/2310.16459</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cwangai/dualmatch">https://github.com/cwangai/dualmatch</a></li>
<li>paper_authors: Cong Wang, Xiaofeng Cao, Lanzhe Guo2, Zenglin Shi</li>
<li>for: 这 paper 的目的是提出一种新的 semi-supervised learning 方法，以便在标签不够的情况下利用无标签数据。</li>
<li>methods: 这 paper 使用了一种新的 dual-level 交互方法，即在jointly invoking feature embedding和class prediction的方式下进行学习。此外，它还需要一种consistent regularization，确保不同的数据扩展视图和不同的数据之间的feature embedding具有相似性。</li>
<li>results: 经验表明，这 paper 的提议可以在标准的 semi-supervised learning 设置下实现9%的错误减少，而在更复杂的类别不均衡设置下，仍可以实现6%的错误减少。<details>
<summary>Abstract</summary>
Semi-supervised learning provides an expressive framework for exploiting unlabeled data when labels are insufficient. Previous semi-supervised learning methods typically match model predictions of different data-augmented views in a single-level interaction manner, which highly relies on the quality of pseudo-labels and results in semi-supervised learning not robust. In this paper, we propose a novel SSL method called DualMatch, in which the class prediction jointly invokes feature embedding in a dual-level interaction manner. DualMatch requires consistent regularizations for data augmentation, specifically, 1) ensuring that different augmented views are regulated with consistent class predictions, and 2) ensuring that different data of one class are regulated with similar feature embeddings. Extensive experiments demonstrate the effectiveness of DualMatch. In the standard SSL setting, the proposal achieves 9% error reduction compared with SOTA methods, even in a more challenging class-imbalanced setting, the proposal can still achieve 6% error reduction. Code is available at https://github.com/CWangAI/DualMatch
</details>
<details>
<summary>摘要</summary>
semi-supervised learning 提供了一个表达性强的框架，可以将无标签数据作用到当 labels 不足时。先前的 semi-supervised learning 方法通常是将不同扩展的观点汇总在单一水平上进行汇总，这高度依赖 pseudo-label 的质量，从而导致 semi-supervised learning 不稳定。在这篇论文中，我们提出了一个新的 SSL 方法，叫做 DualMatch，这个方法在两个水平上进行汇总。DualMatch 需要一些一致的调整，特别是：1）确保不同扩展的观点汇总的类别预测是一致的，2）确保不同一个类别的数据是一致的。实验结果显示 DualMatch 的效果。在标准 SSL 设定下，我们的提案可以与 SOTA 方法相比，获得 9% 的错误减少，甚至在更加具体的类别偏见设定下，我们的提案仍然可以获得 6% 的错误减少。代码可以在 https://github.com/CWangAI/DualMatch 上取得。
</details></li>
</ul>
<hr>
<h2 id="ChimpACT-A-Longitudinal-Dataset-for-Understanding-Chimpanzee-Behaviors"><a href="#ChimpACT-A-Longitudinal-Dataset-for-Understanding-Chimpanzee-Behaviors" class="headerlink" title="ChimpACT: A Longitudinal Dataset for Understanding Chimpanzee Behaviors"></a>ChimpACT: A Longitudinal Dataset for Understanding Chimpanzee Behaviors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16447">http://arxiv.org/abs/2310.16447</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shirleymaxx/chimpact">https://github.com/shirleymaxx/chimpact</a></li>
<li>paper_authors: Xiaoxuan Ma, Stephan P. Kaufhold, Jiajun Su, Wentao Zhu, Jack Terwilliger, Andres Meza, Yixin Zhu, Federico Rossano, Yizhou Wang</li>
<li>for: 这个研究的目的是提高动物福祉，模拟社会行为，以及了解人类和其他动物之间的共同行为。</li>
<li>methods: 这个研究使用了视频数据，并对其进行了详细的标注和分类。</li>
<li>results: 这个研究提供了一个包含20多只黑猩狮的视频数据集，并对这些数据进行了详细的分析和研究，以深入了解黑猩狮的社会行为和通信方式。<details>
<summary>Abstract</summary>
Understanding the behavior of non-human primates is crucial for improving animal welfare, modeling social behavior, and gaining insights into distinctively human and phylogenetically shared behaviors. However, the lack of datasets on non-human primate behavior hinders in-depth exploration of primate social interactions, posing challenges to research on our closest living relatives. To address these limitations, we present ChimpACT, a comprehensive dataset for quantifying the longitudinal behavior and social relations of chimpanzees within a social group. Spanning from 2015 to 2018, ChimpACT features videos of a group of over 20 chimpanzees residing at the Leipzig Zoo, Germany, with a particular focus on documenting the developmental trajectory of one young male, Azibo. ChimpACT is both comprehensive and challenging, consisting of 163 videos with a cumulative 160,500 frames, each richly annotated with detection, identification, pose estimation, and fine-grained spatiotemporal behavior labels. We benchmark representative methods of three tracks on ChimpACT: (i) tracking and identification, (ii) pose estimation, and (iii) spatiotemporal action detection of the chimpanzees. Our experiments reveal that ChimpACT offers ample opportunities for both devising new methods and adapting existing ones to solve fundamental computer vision tasks applied to chimpanzee groups, such as detection, pose estimation, and behavior analysis, ultimately deepening our comprehension of communication and sociality in non-human primates.
</details>
<details>
<summary>摘要</summary>
理解非人类 primate 的行为是关键的，可以提高动物福祉，模拟社会行为，并为人类和生物共享的行为提供新的洞察。然而，因为非人类 primate 的行为数据缺乏，因此对非人类 primate 的社会互动进行深入探索受到限制。为解决这些限制，我们介绍了 ChimpACT，一个包括2015-2018年在德国列vik zoo的一群超过20只非人类 primate 的行为数据。这个数据集涵盖了一个年轻的 male  chimanzee 的发展轨迹，名为 Azibo。ChimpACT 是丰富和挑战的数据集，包括163个视频，共计160,500帧，每个帧都有丰富的注释，包括检测、识别、姿势估计和精细的时空行为标签。我们在 ChimpACT 上测试了代表性的三个Track：（i）跟踪和识别、（ii）姿势估计、（iii）时空动作检测。我们的实验表明，ChimpACT 提供了许多机会，用于创造新的方法和适应现有的方法，以解决在非人类 primate 群体中的检测、姿势估计和行为分析问题，最终深化我们对非人类 primate 的communication和社会性的理解。
</details></li>
</ul>
<hr>
<h2 id="On-Pixel-level-Performance-Assessment-in-Anomaly-Detection"><a href="#On-Pixel-level-Performance-Assessment-in-Anomaly-Detection" class="headerlink" title="On Pixel-level Performance Assessment in Anomaly Detection"></a>On Pixel-level Performance Assessment in Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16435">http://arxiv.org/abs/2310.16435</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mehdi Rafiei, Toby P. Breckon, Alexandros Iosifidis</li>
<li>for: 本研究旨在探讨 anomaly detection 方法在不同应用中的表现，特别是在像素级别时的评估带来的复杂挑战。</li>
<li>methods: 本研究使用了 eleven 种现代 anomaly detection 方法，应用于 twenty-one 个 anomaly detection 问题。</li>
<li>results: 经过广泛的实验评估，研究人员发现，使用 Precision-Recall 基于的 metric 可以更好地捕捉方法的表现，这些 metric 更适合用于这种任务。<details>
<summary>Abstract</summary>
Anomaly detection methods have demonstrated remarkable success across various applications. However, assessing their performance, particularly at the pixel-level, presents a complex challenge due to the severe imbalance that is most commonly present between normal and abnormal samples. Commonly adopted evaluation metrics designed for pixel-level detection may not effectively capture the nuanced performance variations arising from this class imbalance. In this paper, we dissect the intricacies of this challenge, underscored by visual evidence and statistical analysis, leading to delve into the need for evaluation metrics that account for the imbalance. We offer insights into more accurate metrics, using eleven leading contemporary anomaly detection methods on twenty-one anomaly detection problems. Overall, from this extensive experimental evaluation, we can conclude that Precision-Recall-based metrics can better capture relative method performance, making them more suitable for the task.
</details>
<details>
<summary>摘要</summary>
异常检测方法在不同应用领域中表现出了惊人的成功。然而，评估这些方法的性能，特别是在像素级别，却存在严重的类别不平衡问题。通常采用的评估指标可能不能准确捕捉这种类别不平衡导致的性能变化。本文通过视觉证据和统计分析，探讨了这种挑战的复杂性，并提出了考虑类别不平衡的评估指标。我们在 twenty-one 个异常检测问题上使用了 eleven 种当代异常检测方法进行了广泛的实验评估。总的来说，我们可以从这些实验结果中得出结论，精度-回归-基于的指标更适合用于这个任务。
</details></li>
</ul>
<hr>
<h2 id="Winning-Prize-Comes-from-Losing-Tickets-Improve-Invariant-Learning-by-Exploring-Variant-Parameters-for-Out-of-Distribution-Generalization"><a href="#Winning-Prize-Comes-from-Losing-Tickets-Improve-Invariant-Learning-by-Exploring-Variant-Parameters-for-Out-of-Distribution-Generalization" class="headerlink" title="Winning Prize Comes from Losing Tickets: Improve Invariant Learning by Exploring Variant Parameters for Out-of-Distribution Generalization"></a>Winning Prize Comes from Losing Tickets: Improve Invariant Learning by Exploring Variant Parameters for Out-of-Distribution Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16391">http://arxiv.org/abs/2310.16391</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuo Huang, Muyang Li, Li Shen, Jun Yu, Chen Gong, Bo Han, Tongliang Liu<br>for:EVIL aims to improve OOD generalization by identifying a robust subnetwork that is resistant to distribution shift.methods:EVIL leverages distribution knowledge to find both invariant and variant parameters, and uses them to improve OOD generalization.results:EVIL can effectively and efficiently enhance many popular methods, such as ERM, IRM, SAM, etc., on an integrated testbed called DomainBed.<details>
<summary>Abstract</summary>
Out-of-Distribution (OOD) Generalization aims to learn robust models that generalize well to various environments without fitting to distribution-specific features. Recent studies based on Lottery Ticket Hypothesis (LTH) address this problem by minimizing the learning target to find some of the parameters that are critical to the task. However, in OOD problems, such solutions are suboptimal as the learning task contains severe distribution noises, which can mislead the optimization process. Therefore, apart from finding the task-related parameters (i.e., invariant parameters), we propose Exploring Variant parameters for Invariant Learning (EVIL) which also leverages the distribution knowledge to find the parameters that are sensitive to distribution shift (i.e., variant parameters). Once the variant parameters are left out of invariant learning, a robust subnetwork that is resistant to distribution shift can be found. Additionally, the parameters that are relatively stable across distributions can be considered invariant ones to improve invariant learning. By fully exploring both variant and invariant parameters, our EVIL can effectively identify a robust subnetwork to improve OOD generalization. In extensive experiments on integrated testbed: DomainBed, EVIL can effectively and efficiently enhance many popular methods, such as ERM, IRM, SAM, etc.
</details>
<details>
<summary>摘要</summary>
外部分布（OOD）泛化目标是学习具有良好泛化能力的模型，以适应不同环境中的数据分布。近年来，基于抽奖假设（LTH）的研究提出了以最小化学习目标来找到任务相关的参数的方法，但在OOD问题中，这些解决方案是不优化的，因为学习任务中存在严重的分布噪声，这可能会导致优化过程受到束缚。因此，我们提出了尝试探索变体参数来实现泛化学习（EVIL），该方法还利用了分布知识来找到分布转移中不稳定的参数。一旦变体参数被去除，我们可以找到一个鲁棒的子网络，该子网络对分布转移具有抗性。此外，可以考虑相对稳定的参数作为惰性参数，以改进泛化学习。通过全面探索变体和惰性参数，我们的EVIL可以有效地找到一个鲁棒的子网络，以提高OOD泛化能力。在各种流行的方法基础上，如ERM、IRM、SAM等，我们在DomainBed集成测试床上进行了广泛的实验，EVIL可以高效地和高质量地增强这些方法。
</details></li>
</ul>
<hr>
<h2 id="MVFAN-Multi-View-Feature-Assisted-Network-for-4D-Radar-Object-Detection"><a href="#MVFAN-Multi-View-Feature-Assisted-Network-for-4D-Radar-Object-Detection" class="headerlink" title="MVFAN: Multi-View Feature Assisted Network for 4D Radar Object Detection"></a>MVFAN: Multi-View Feature Assisted Network for 4D Radar Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16389">http://arxiv.org/abs/2310.16389</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiao Yan, Yihan Wang<br>for:* 这篇论文的目的是提出一种基于4D雷达的3D对象检测方法，以提高自动驾驶系统的能力和可靠性。methods:* 该方法基于一个新的Position Map Generation模块，用于增强特征学习，并且使用了一种新的Radar Feature Assisted backbone来全面利用4D雷达传感器提供的Doppler速度和反射率数据。results:* 对Astyx和VoD数据集进行了广泛的实验和ablation研究，证明了该方法的有效性，特别是对小移动目标物如人行和自行车的检测性能有了明显的改善。I hope that helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
4D radar is recognized for its resilience and cost-effectiveness under adverse weather conditions, thus playing a pivotal role in autonomous driving. While cameras and LiDAR are typically the primary sensors used in perception modules for autonomous vehicles, radar serves as a valuable supplementary sensor. Unlike LiDAR and cameras, radar remains unimpaired by harsh weather conditions, thereby offering a dependable alternative in challenging environments. Developing radar-based 3D object detection not only augments the competency of autonomous vehicles but also provides economic benefits. In response, we propose the Multi-View Feature Assisted Network (\textit{MVFAN}), an end-to-end, anchor-free, and single-stage framework for 4D-radar-based 3D object detection for autonomous vehicles. We tackle the issue of insufficient feature utilization by introducing a novel Position Map Generation module to enhance feature learning by reweighing foreground and background points, and their features, considering the irregular distribution of radar point clouds. Additionally, we propose a pioneering backbone, the Radar Feature Assisted backbone, explicitly crafted to fully exploit the valuable Doppler velocity and reflectivity data provided by the 4D radar sensor. Comprehensive experiments and ablation studies carried out on Astyx and VoD datasets attest to the efficacy of our framework. The incorporation of Doppler velocity and RCS reflectivity dramatically improves the detection performance for small moving objects such as pedestrians and cyclists. Consequently, our approach culminates in a highly optimized 4D-radar-based 3D object detection capability for autonomous driving systems, setting a new standard in the field.
</details>
<details>
<summary>摘要</summary>
四维度雷达被广泛应用于自动驾驶领域，因其鲜为人知的优点，包括可靠性和成本效益。雷达不同于激光雷达和摄像头，在恶劣天气条件下仍然能够保持高度可靠，因此在自动驾驶系统中扮演着重要的辅助角色。为了提高自动驾驶系统的可靠性和安全性，我们提出了基于四维度雷达的三维物体检测方法，即多视图特征帮助网络（MVFAN）。我们解决了尚未充分利用特征的问题，通过引入新的位置图生成模块，以提高特征学习的灵活性和可靠性。此外，我们还提出了一种创新的干扰抑制器，以避免由雷达点云的不规则分布所引起的干扰。此外，我们还特制了一种雷达特征帮助核心，以全面利用雷达传感器提供的Doppler速度和反射率数据。我们在Astyx和VoD数据集上进行了广泛的实验和缺省研究，结果表明，我们的框架在检测小运动目标（如行人和自行车）的表现出色。因此，我们的方法在自动驾驶系统中提供了一种高度优化的四维度雷达基于三维物体检测能力，为自动驾驶领域的发展提供了新的标准。
</details></li>
</ul>
<hr>
<h2 id="General-Point-Model-with-Autoencoding-and-Autoregressive"><a href="#General-Point-Model-with-Autoencoding-and-Autoregressive" class="headerlink" title="General Point Model with Autoencoding and Autoregressive"></a>General Point Model with Autoencoding and Autoregressive</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16861">http://arxiv.org/abs/2310.16861</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhe Li, Zhangyang Gao, Cheng Tan, Stan Z. Li, Laurence T. Yang</li>
<li>for: 这篇论文旨在探讨大语言模型的预训练架构，以及如何使用这些架构来提高点云表示能力。</li>
<li>methods: 该论文提出了一种通用的点云模型（General Point Model，GPM），该模型结合了自编码和自发现任务，并可以进行精细调整以适应不同的下游任务。</li>
<li>results: 对比 Point-BERT、MaskPoint 和 PointMAE 等模型，GPM在点云理解任务中表现出色，并且在条件生成任务中也可以达到比较高的水平。此外，GPM的核心思想是将自编码和自发现任务融合到同一个 transformer 中，这使得模型在不同的下游任务上具有更大的灵活性。<details>
<summary>Abstract</summary>
The pre-training architectures of large language models encompass various types, including autoencoding models, autoregressive models, and encoder-decoder models. We posit that any modality can potentially benefit from a large language model, as long as it undergoes vector quantization to become discrete tokens. Inspired by GLM, we propose a General Point Model (GPM) which seamlessly integrates autoencoding and autoregressive tasks in point cloud transformer. This model is versatile, allowing fine-tuning for downstream point cloud representation tasks, as well as unconditional and conditional generation tasks. GPM enhances masked prediction in autoencoding through various forms of mask padding tasks, leading to improved performance in point cloud understanding. Additionally, GPM demonstrates highly competitive results in unconditional point cloud generation tasks, even exhibiting the potential for conditional generation tasks by modifying the input's conditional information. Compared to models like Point-BERT, MaskPoint and PointMAE, our GPM achieves superior performance in point cloud understanding tasks. Furthermore, the integration of autoregressive and autoencoding within the same transformer underscores its versatility across different downstream tasks.
</details>
<details>
<summary>摘要</summary>
大型语言模型的预训练架构包括自编码模型、自回归模型以及编码器-解码器模型。我们认为任何modal都可能受益于大语言模型，只要它经过vector量化成为简单的token。受GLM的 inspirations所影响，我们提议一种通用点模型（GPM），该模型可以在点云trasnformer中协调自编码和自回归任务。这个模型非常灵活，可以进行下游点云表示任务的细化调整，以及无条件和条件生成任务。GPM通过不同的mask padding任务来提高autoencoding中的假值预测，从而提高点云理解性能。此外，GPM在无条件点云生成任务中表现出非常竞争力，甚至可以通过修改输入的条件信息来实现条件生成任务。相比之下，与Point-BERT、MaskPoint和PointMAE等模型相比，我们的GPM在点云理解任务中表现出较高的性能。此外，将自回归和自编码任务嵌入同一个transformer中，强调了这个模型的多功能性在不同的下游任务中。
</details></li>
</ul>
<hr>
<h2 id="Deepfake-Detection-Leveraging-the-Power-of-2D-and-3D-CNN-Ensembles"><a href="#Deepfake-Detection-Leveraging-the-Power-of-2D-and-3D-CNN-Ensembles" class="headerlink" title="Deepfake Detection: Leveraging the Power of 2D and 3D CNN Ensembles"></a>Deepfake Detection: Leveraging the Power of 2D and 3D CNN Ensembles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16388">http://arxiv.org/abs/2310.16388</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aagam Bakliwal, Amit D. Joshi</li>
<li>for: 这个研究目的是为了实现深伪检测中的动态实体验证。</li>
<li>methods: 这个方法结合了进步的2D和3D卷积神经网，其中3D模型通过滑动范围实现了空间和时间维度上的特征捕捉。</li>
<li>results: 实验显示，这种组合实现了优异的验证效果，表明它具有对深伪生成的欺骗行为的应急应对能力。<details>
<summary>Abstract</summary>
In the dynamic realm of deepfake detection, this work presents an innovative approach to validate video content. The methodology blends advanced 2-dimensional and 3-dimensional Convolutional Neural Networks. The 3D model is uniquely tailored to capture spatiotemporal features via sliding filters, extending through both spatial and temporal dimensions. This configuration enables nuanced pattern recognition in pixel arrangement and temporal evolution across frames. Simultaneously, the 2D model leverages EfficientNet architecture, harnessing auto-scaling in Convolutional Neural Networks. Notably, this ensemble integrates Voting Ensembles and Adaptive Weighted Ensembling. Strategic prioritization of the 3-dimensional model's output capitalizes on its exceptional spatio-temporal feature extraction. Experimental validation underscores the effectiveness of this strategy, showcasing its potential in countering deepfake generation's deceptive practices.
</details>
<details>
<summary>摘要</summary>
在深层伪造检测领域中，这项工作提出了一种创新的方法来验证视频内容。该方法结合了高级的2维和3维卷积神经网络。3D模型特意设计了捕捉空间时间特征的滑动缓示，通过空间和时间维度的扩展来提高细节特征识别。同时，2D模型采用了高效的EfficientNet架构，实现了自适应卷积神经网络的核心。此外，这个集成还包括投票集成和适应加权集成。在优先级方面，将3维模型的输出作为首要考虑，以便利用其出色的空间时间特征抽取。实验证明了这种策略的有效性，表明其在防止深层伪造生成的欺诈实践中具有潜在的优势。
</details></li>
</ul>
<hr>
<h2 id="Frequency-Aware-Transformer-for-Learned-Image-Compression"><a href="#Frequency-Aware-Transformer-for-Learned-Image-Compression" class="headerlink" title="Frequency-Aware Transformer for Learned Image Compression"></a>Frequency-Aware Transformer for Learned Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16387">http://arxiv.org/abs/2310.16387</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Li, Shaohui Li, Wenrui Dai, Chenglin Li, Junni Zou, Hongkai Xiong</li>
<li>for: 提高了图像压缩和传输的效率，解决了现有LIC方法中的约束和方向细节损失问题。</li>
<li>methods: 我们提出了一种新的频率意识变换块（FAT），通过多级方向分析来捕捉自然图像的频率组成。此外，我们还引入了频率调制Feedforward网络（FMFFN）来适应不同频率组成，提高了比特率-误差性能。最后，我们提出了一种基于变换器的通道自动循环（T-CA）模型，有效地利用通道相互关系。</li>
<li>results: 我们的方法在BD-率上比现有LIC方法更高，并且胜过最新的标准化编码器VTM-12.1 by 14.5%, 15.1%, 13.0% on the Kodak, Tecnick, and CLIC datasets。<details>
<summary>Abstract</summary>
Learned image compression (LIC) has gained traction as an effective solution for image storage and transmission in recent years. However, existing LIC methods are redundant in latent representation due to limitations in capturing anisotropic frequency components and preserving directional details. To overcome these challenges, we propose a novel frequency-aware transformer (FAT) block that for the first time achieves multiscale directional ananlysis for LIC. The FAT block comprises frequency-decomposition window attention (FDWA) modules to capture multiscale and directional frequency components of natural images. Additionally, we introduce frequency-modulation feed-forward network (FMFFN) to adaptively modulate different frequency components, improving rate-distortion performance. Furthermore, we present a transformer-based channel-wise autoregressive (T-CA) model that effectively exploits channel dependencies. Experiments show that our method achieves state-of-the-art rate-distortion performance compared to existing LIC methods, and evidently outperforms latest standardized codec VTM-12.1 by 14.5%, 15.1%, 13.0% in BD-rate on the Kodak, Tecnick, and CLIC datasets.
</details>
<details>
<summary>摘要</summary>
现代学习图像压缩（LIC）技术在最近几年来得到了广泛应用和推广，但现有的LIC方法具有重复的 latent 表示，导致不能够准确捕捉自然图像的多方位频率成分和方向细节。为解决这些挑战，我们提出了一种新的频率意识转换块（FAT），该块包括频率分解窗口注意力（FDWA）模块，以捕捉自然图像的多方位频率成分。此外，我们还引入了频率调制Feedforward网络（FMFFN），以适应不同频率成分的改变，从而提高rate-distortion性能。此外，我们还提出了基于 transformer 的渠道 wise 自动逆生成（T-CA）模型，该模型能够有效利用渠道之间的依赖关系。实验表明，我们的方法在 rate-distortion 性能方面与现有的 LIC 方法相比，有州最佳的表现，并且明显超过最新的标准化编码器 VTM-12.1 的14.5%、15.1% 和 13.0% 的BD-rate 在 Kodak、Tecnick 和 CLIC 数据集上。
</details></li>
</ul>
<hr>
<h2 id="Open-NeRF-Towards-Open-Vocabulary-NeRF-Decomposition"><a href="#Open-NeRF-Towards-Open-Vocabulary-NeRF-Decomposition" class="headerlink" title="Open-NeRF: Towards Open Vocabulary NeRF Decomposition"></a>Open-NeRF: Towards Open Vocabulary NeRF Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16383">http://arxiv.org/abs/2310.16383</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Zhang, Fang Li, Narendra Ahuja</li>
<li>for: 解决Neural Radiance Fields（NeRF）中的对象分解问题，以便在3D重建和视觉合成中进行物体操作。</li>
<li>methods: 利用大规模的、可用的、Segment Anything Model（SAM）和嵌入式并蒸馈法来实现开放词汇查询的灵活性和3D分 segmentation的准确性。</li>
<li>results: 比静脉补充（LERF）和FFD（FFD）在开放词汇场景下表现出色，并且在干扰和杂乱特征的情况下保持一致的物体认知和细化。<details>
<summary>Abstract</summary>
In this paper, we address the challenge of decomposing Neural Radiance Fields (NeRF) into objects from an open vocabulary, a critical task for object manipulation in 3D reconstruction and view synthesis. Current techniques for NeRF decomposition involve a trade-off between the flexibility of processing open-vocabulary queries and the accuracy of 3D segmentation. We present, Open-vocabulary Embedded Neural Radiance Fields (Open-NeRF), that leverage large-scale, off-the-shelf, segmentation models like the Segment Anything Model (SAM) and introduce an integrate-and-distill paradigm with hierarchical embeddings to achieve both the flexibility of open-vocabulary querying and 3D segmentation accuracy. Open-NeRF first utilizes large-scale foundation models to generate hierarchical 2D mask proposals from varying viewpoints. These proposals are then aligned via tracking approaches and integrated within the 3D space and subsequently distilled into the 3D field. This process ensures consistent recognition and granularity of objects from different viewpoints, even in challenging scenarios involving occlusion and indistinct features. Our experimental results show that the proposed Open-NeRF outperforms state-of-the-art methods such as LERF \cite{lerf} and FFD \cite{ffd} in open-vocabulary scenarios. Open-NeRF offers a promising solution to NeRF decomposition, guided by open-vocabulary queries, enabling novel applications in robotics and vision-language interaction in open-world 3D scenes.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们解决了基于神经辐射场（NeRF）的对象分解问题，这是3D重建和视觉合成中对物体进行操作的关键任务。现有的NeRF分解技术存在较大的灵活性和3D分割精度之间的负担。我们提出了开放词汇内置神经辐射场（Open-NeRF），它利用大规模的商业化分割模型，如Segment Anything Model（SAM），并在层次嵌入和热链整合方法下实现了开放词汇查询的灵活性和3D分割精度。Open-NeRF首先利用大规模基础模型生成层次2D面mask提案，从不同视点生成这些提案，然后使用跟踪方法对它们进行准确的对齐和集成，并将其嵌入到3D空间中，最后进行热链整合和筛选，以保证对不同视点的物体承载和不确定特征的一致性。我们的实验结果表明，我们提出的Open-NeRF在开放词汇场景下超过了现有的LERF \cite{lerf}和FFD \cite{ffd}的性能。Open-NeRF提供了一种有前途的解决方案，受开放词汇查询指导，在开放世界3D场景中实现了新的 робо扮和视觉语言互动应用。
</details></li>
</ul>
<hr>
<h2 id="Towards-Large-scale-Masked-Face-Recognition"><a href="#Towards-Large-scale-Masked-Face-Recognition" class="headerlink" title="Towards Large-scale Masked Face Recognition"></a>Towards Large-scale Masked Face Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16364">http://arxiv.org/abs/2310.16364</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manyuan Zhang, Bingqi Ma, Guanglu Song, Yunxiao Wang, Hongsheng Li, Yu Liu</li>
<li>for: 本研究的目的是提出一种在COVID-19 coronavirus 疫情期间大规模戴口罩的人脸识别算法冠军解决方案。</li>
<li>methods: 本研究使用的方法包括大规模训练、数据噪声处理、戴口罩和不戴口罩人脸识别精度平衡等四个挑战。</li>
<li>results: 本研究在ICCV MFR WebFace260M 和 InsightFace 无结构授益识别 tracks 上实现了冠军成绩，并提出了一种适用于大规模戴口罩人脸识别的推理友好模型体系。<details>
<summary>Abstract</summary>
During the COVID-19 coronavirus epidemic, almost everyone is wearing masks, which poses a huge challenge for deep learning-based face recognition algorithms. In this paper, we will present our \textbf{championship} solutions in ICCV MFR WebFace260M and InsightFace unconstrained tracks. We will focus on four challenges in large-scale masked face recognition, i.e., super-large scale training, data noise handling, masked and non-masked face recognition accuracy balancing, and how to design inference-friendly model architecture. We hope that the discussion on these four aspects can guide future research towards more robust masked face recognition systems.
</details>
<details>
<summary>摘要</summary>
durante la epidemia de COVID-19 del coronavirus, prácticamente todos están usando mascarillas, lo que plantea un gran desafío para los algoritmos de reconocimiento de rostros basados en aprendizaje profundo. En este artículo, presentaremos nuestras soluciones campeonas en las pistas MFR WebFace260M e InsightFace de ICCV. Centraremonos en cuatro desafíos en el reconocimiento de rostros mascados a gran escala, es decir, la capacitación en escalas supergrandes, el manejo de ruido de datos, el equilibrio entre la precisión de reconocimiento de rostros mascados y no mascados, y cómo diseñar arquitecturas de modelos amigables con la inferencia. Esperamos que el debate sobre estos cuatro aspectos pueda guiar la investigación futura hacia sistemas de reconocimiento de rostros más robustos con mascarillas.
</details></li>
</ul>
<hr>
<h2 id="DiffRef3D-A-Diffusion-based-Proposal-Refinement-Framework-for-3D-Object-Detection"><a href="#DiffRef3D-A-Diffusion-based-Proposal-Refinement-Framework-for-3D-Object-Detection" class="headerlink" title="DiffRef3D: A Diffusion-based Proposal Refinement Framework for 3D Object Detection"></a>DiffRef3D: A Diffusion-based Proposal Refinement Framework for 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16349">http://arxiv.org/abs/2310.16349</a></li>
<li>repo_url: None</li>
<li>paper_authors: Se-Ho Kim, Inyong Koo, Inyoung Lee, Byeongjun Park, Changick Kim</li>
<li>for: 提高3D物体检测器的性能</li>
<li>methods: 使用diffusion过程进行提档 proposal refinement</li>
<li>results: 在KITTI数据集上实现了高性能的3D物体检测Here’s the full translation in Simplified Chinese:</li>
<li>for: 提高3D物体检测器的性能</li>
<li>methods: 使用diffusion过程进行提档 proposal refinement</li>
<li>results: 在KITTI数据集上实现了高性能的3D物体检测I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Denoising diffusion models show remarkable performances in generative tasks, and their potential applications in perception tasks are gaining interest. In this paper, we introduce a novel framework named DiffRef3D which adopts the diffusion process on 3D object detection with point clouds for the first time. Specifically, we formulate the proposal refinement stage of two-stage 3D object detectors as a conditional diffusion process. During training, DiffRef3D gradually adds noise to the residuals between proposals and target objects, then applies the noisy residuals to proposals to generate hypotheses. The refinement module utilizes these hypotheses to denoise the noisy residuals and generate accurate box predictions. In the inference phase, DiffRef3D generates initial hypotheses by sampling noise from a Gaussian distribution as residuals and refines the hypotheses through iterative steps. DiffRef3D is a versatile proposal refinement framework that consistently improves the performance of existing 3D object detection models. We demonstrate the significance of DiffRef3D through extensive experiments on the KITTI benchmark. Code will be available.
</details>
<details>
<summary>摘要</summary>
diffusion 模型在生成任务中表现出色，其在感知任务中的潜在应用也引起了关注。在这篇论文中，我们介绍了一个名为DiffRef3D的新框架，它在3D物体检测中使用点云的扩散过程来进行首次应用。具体来说，我们将两stage 3D物体检测器的提议改进阶段设计为一个条件的扩散过程。在训练过程中，DiffRef3D逐渐添加了随机噪声到提议和目标对象之间的差异，然后将这些噪声应用到提议中来生成假设。提升模块使用这些假设来减少噪声并生成准确的盒子预测。在推断阶段，DiffRef3D通过随机从 Gaussian 分布中采样噪声来生成初始假设，然后通过迭代步骤来修改这些假设，以生成高精度的盒子预测。DiffRef3D 是一种通用的提议改进框架，可以在现有的 3D 物体检测模型上逐次提高性能。我们通过对 KITTI benchmark 进行了广泛的实验，证明了DiffRef3D 的重要性。代码将可以获得。
</details></li>
</ul>
<hr>
<h2 id="Dolfin-Diffusion-Layout-Transformers-without-Autoencoder"><a href="#Dolfin-Diffusion-Layout-Transformers-without-Autoencoder" class="headerlink" title="Dolfin: Diffusion Layout Transformers without Autoencoder"></a>Dolfin: Diffusion Layout Transformers without Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16305">http://arxiv.org/abs/2310.16305</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yilin Wang, Zeyuan Chen, Liangjun Zhong, Zheng Ding, Zhizhou Sha, Zhuowen Tu</li>
<li>for: 这篇论文旨在提出一种新的生成模型，即Diffusion Layout Transformers without Autoencoder (Dolfin)，该模型可以有效地提高生成能力，同时减少计算复杂性。</li>
<li>methods: Dolfin使用Transformer-based噪声过程来实现布局生成，并提出了一种有效的bi-directional（非 causal joint）序列表示方法，以及一种autoregressive噪声模型（Dolfin-AR），能够更好地捕捉邻近对象之间的 semantics相关性，如对齐、大小和覆盖。</li>
<li>results: 对标准生成布局Benchmark进行评估，Dolfin显著提高了各种指标（fid, alignment, overlap, MaxIoU和DocSim scores），同时提高了透明度和可操作性。此外，Dolfin的应用不仅限于布局生成，还适用于模型几何结构，如直线段。实验结果表明Dolfin具有优势。<details>
<summary>Abstract</summary>
In this paper, we introduce a novel generative model, Diffusion Layout Transformers without Autoencoder (Dolfin), which significantly improves the modeling capability with reduced complexity compared to existing methods. Dolfin employs a Transformer-based diffusion process to model layout generation. In addition to an efficient bi-directional (non-causal joint) sequence representation, we further propose an autoregressive diffusion model (Dolfin-AR) that is especially adept at capturing rich semantic correlations for the neighboring objects, such as alignment, size, and overlap. When evaluated against standard generative layout benchmarks, Dolfin notably improves performance across various metrics (fid, alignment, overlap, MaxIoU and DocSim scores), enhancing transparency and interoperability in the process. Moreover, Dolfin's applications extend beyond layout generation, making it suitable for modeling geometric structures, such as line segments. Our experiments present both qualitative and quantitative results to demonstrate the advantages of Dolfin.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们引入了一种新的生成模型，即扩散布局变换器无自编码器（Dolfin），它能够显著提高模型化能力而减少复杂性，相比现有的方法。Dolfin使用Transformer基于的扩散过程来模型布局生成。除了高效的双向（非 causal 联合）序列表示之外，我们还提出了一种激进的扩散模型（Dolfin-AR），它尤其适合捕捉邻近对象的丰富semantic相关性，如对齐、大小和重叠。当评估了标准的生成布局benchmark时，Dolfin明显提高了多个指标（fid、对齐、重叠、MaxIoU和DocSim分数），从而提高了透明度和可操作性。此外，Dolfin的应用场景不仅包括布局生成，还适用于模型Geometric结构，如直线段。我们的实验包括qualitative和quantitative结果，以demonstrate Dolfin的优势。
</details></li>
</ul>
<hr>
<h2 id="4D-Editor-Interactive-Object-level-Editing-in-Dynamic-Neural-Radiance-Fields-via-4D-Semantic-Segmentation"><a href="#4D-Editor-Interactive-Object-level-Editing-in-Dynamic-Neural-Radiance-Fields-via-4D-Semantic-Segmentation" class="headerlink" title="4D-Editor: Interactive Object-level Editing in Dynamic Neural Radiance Fields via 4D Semantic Segmentation"></a>4D-Editor: Interactive Object-level Editing in Dynamic Neural Radiance Fields via 4D Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16858">http://arxiv.org/abs/2310.16858</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dadong Jiang, Zhihui Ke, Xiaobo Zhou, Xidong Shi</li>
<li>for: 这 paper 的目的是实现在动态场景中进行交互式对象水平编辑（例如，删除、重新颜色、变换、组合）。</li>
<li>methods: 这 paper 使用的方法包括 hybrid semantic feature fields 来保持空间时间一致性，以及 recursive selection refinement 来提高动态 NeRF 中的 segmentation 精度。</li>
<li>results: EXTENSIVE experiments 和 editing examples 表明，4D-Editor 可以实现高品质的动态 NeRF 编辑。Here’s the full text in Simplified Chinese:</li>
<li>for: 这 paper 的目的是实现在动态场景中进行交互式对象水平编辑（例如，删除、重新颜色、变换、组合）。</li>
<li>methods: 这 paper 使用的方法包括 hybrid semantic feature fields 来保持空间时间一致性，以及 recursive selection refinement 来提高动态 NeRF 中的 segmentation 精度。</li>
<li>results: EXTENSIVE experiments 和 editing examples 表明，4D-Editor 可以实现高品质的动态 NeRF 编辑。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
This paper targets interactive object-level editing(e.g., deletion, recoloring, transformation, composition) in dynamic scenes. Recently, some methods aiming for flexible editing static scenes represented by neural radiance field (NeRF) have shown impressive synthesis quality, while similar capabilities in time-variant dynamic scenes remain limited. To solve this problem, we propose 4D-Editor, an interactive semantic-driven editing framework, allowing editing multiple objects in dynamic NeRF based on user strokes on a single frame. Our dynamic scene representation is built upon hybrid semantic feature fields so that the spatial-temporal consistency can be maintained after editing. In addition, we design recursive selection refinement that significantly boosts segmentation accuracy in a dynamic NeRF to aid the editing process. Moreover, we develop multi-view reprojection inpainting to fill holes caused by incomplete scene capture after editing. Extensive experiments and editing examples on real-world demonstrate that 4D-Editor achieves photo-realistic dynamic NeRF editing. Project page: https://patrickddj.github.io/4D-Editor
</details>
<details>
<summary>摘要</summary>
这篇论文targets互动对象水平编辑（例如，删除、重新颜色、变换、组合）在动态场景中。在最近，一些方法targeting静止场景 represented by neural radiance field (NeRF)的灵活编辑能力有所进步，而在时间变化的动态场景中的相似能力尚未得到有效的解决。为解决这个问题，我们提出了4D-Editor，一个互动semantic驱动的编辑框架，允许在单帧上进行多个对象的编辑。我们的动态场景表示基于半结合 semantic feature fields，以保持空间时间一致性 после编辑。此外，我们设计了重层选择精度提高，以提高动态NeRF中的分割精度，以便编辑过程中更好地帮助用户。此外，我们开发了多视图重oprojection填充，以填充在编辑后Scene capture中出现的孔隙。广泛的实验和编辑示例表明，4D-Editor可以实现高品质的动态NeRF编辑。项目页面：https://patrickddj.github.io/4D-EditorNote: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="MotionAGFormer-Enhancing-3D-Human-Pose-Estimation-with-a-Transformer-GCNFormer-Network"><a href="#MotionAGFormer-Enhancing-3D-Human-Pose-Estimation-with-a-Transformer-GCNFormer-Network" class="headerlink" title="MotionAGFormer: Enhancing 3D Human Pose Estimation with a Transformer-GCNFormer Network"></a>MotionAGFormer: Enhancing 3D Human Pose Estimation with a Transformer-GCNFormer Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16288">http://arxiv.org/abs/2310.16288</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/taatiteam/motionagformer">https://github.com/taatiteam/motionagformer</a></li>
<li>paper_authors: Soroush Mehraban, Vida Adeli, Babak Taati</li>
<li>for: 本研究旨在提出一种新的注意力GCNFormer块（AGFormer），以提高3D人姿估计中的本地关系学习。</li>
<li>methods: 该模型使用两个并行的 transformer 流水线和 GCNFormer 流水线，并将其分解为多个 AGFormer 块。GCNFormer 模块利用邻近关节之间的本地关系，生成一个补充性的表示，并与 transformer 输出进行可靠的拟合。</li>
<li>results: 在 Human3.6M 和 MPI-INF-3DHP 两个标准测试集上，MotionAGFormer 模型 achieved state-of-the-art 结果，P1 误差分别为 38.4mm 和 16.2mm。同时，该模型使用的参数量只有一半，计算量三倍于之前的领先模型。代码和模型可以在 GitHub 上获取。<details>
<summary>Abstract</summary>
Recent transformer-based approaches have demonstrated excellent performance in 3D human pose estimation. However, they have a holistic view and by encoding global relationships between all the joints, they do not capture the local dependencies precisely. In this paper, we present a novel Attention-GCNFormer (AGFormer) block that divides the number of channels by using two parallel transformer and GCNFormer streams. Our proposed GCNFormer module exploits the local relationship between adjacent joints, outputting a new representation that is complementary to the transformer output. By fusing these two representation in an adaptive way, AGFormer exhibits the ability to better learn the underlying 3D structure. By stacking multiple AGFormer blocks, we propose MotionAGFormer in four different variants, which can be chosen based on the speed-accuracy trade-off. We evaluate our model on two popular benchmark datasets: Human3.6M and MPI-INF-3DHP. MotionAGFormer-B achieves state-of-the-art results, with P1 errors of 38.4mm and 16.2mm, respectively. Remarkably, it uses a quarter of the parameters and is three times more computationally efficient than the previous leading model on Human3.6M dataset. Code and models are available at https://github.com/TaatiTeam/MotionAGFormer.
</details>
<details>
<summary>摘要</summary>
近期基于transformer的方法已经表现出色地进行3D人姿估算。然而，它们具有整体视图，通过编码全局关系 между所有关节来不准确地捕捉本地依赖关系。在这篇论文中，我们提出了一种新的Attention-GCNFormer（AGFormer）块，通过使用两个平行的transformer和GCNFormer流程来分解通道数。我们的提议的GCNFormer模块利用邻近关节之间的本地关系，输出一个新的表示，与transformer输出相комplementary。通过在adaptive的方式进行融合，AGFormer能够更好地学习下来3D结构。通过堆叠多个AGFormer块，我们提议MotionAGFormer模型，有四种不同的变体，可以根据速度精度质量进行选择。我们在人3.6M和MPI-INF-3DHP两个 популяр的benchmark数据集上评估了我们的模型，MotionAGFormer-B Variant achieve state-of-the-art Results，P1 error为38.4mm和16.2mm，分别。很Remarkably，它使用的参数数量只是前一个领先模型的一半，并且在人3.6M数据集上三倍更快速并且更加计算效率。代码和模型可以在https://github.com/TaatiTeam/MotionAGFormer上获取。
</details></li>
</ul>
<hr>
<h2 id="TransPose-6D-Object-Pose-Estimation-with-Geometry-Aware-Transformer"><a href="#TransPose-6D-Object-Pose-Estimation-with-Geometry-Aware-Transformer" class="headerlink" title="TransPose: 6D Object Pose Estimation with Geometry-Aware Transformer"></a>TransPose: 6D Object Pose Estimation with Geometry-Aware Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16279">http://arxiv.org/abs/2310.16279</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao Lin, Deming Wang, Guangliang Zhou, Chengju Liu, Qijun Chen</li>
<li>for: 提高RGB基于方法的6D对象pose估计精度，避免 occlusion 和照明变化的影响。</li>
<li>methods: 使用TransformerEncoder和geometry-aware模块，提取点云特征表示，并在全球信息交换下提高对 occlusion 的Robustness。</li>
<li>results: 在三个benchmark datasets上实现了竞争性的pose估计效果。<details>
<summary>Abstract</summary>
Estimating the 6D object pose is an essential task in many applications. Due to the lack of depth information, existing RGB-based methods are sensitive to occlusion and illumination changes. How to extract and utilize the geometry features in depth information is crucial to achieve accurate predictions. To this end, we propose TransPose, a novel 6D pose framework that exploits Transformer Encoder with geometry-aware module to develop better learning of point cloud feature representations. Specifically, we first uniformly sample point cloud and extract local geometry features with the designed local feature extractor base on graph convolution network. To improve robustness to occlusion, we adopt Transformer to perform the exchange of global information, making each local feature contains global information. Finally, we introduce geometry-aware module in Transformer Encoder, which to form an effective constrain for point cloud feature learning and makes the global information exchange more tightly coupled with point cloud tasks. Extensive experiments indicate the effectiveness of TransPose, our pose estimation pipeline achieves competitive results on three benchmark datasets.
</details>
<details>
<summary>摘要</summary>
估算6D对象姿 pose是许多应用中的关键任务。由于缺乏深度信息，现有的RGB基于方法容易受到遮挡和照明变化的影响。如何EXTRACT和利用点云信息的几何特征是很重要的。为了实现这一目标，我们提出了TransPose，一种新的6D姿态框架，利用Transformer Encoder和几何意识模块来提高点云特征表示学习。具体来说，我们首先对点云进行均匀采样，然后使用设计的本地特征提取器基于图像感知网络提取当地几何特征。为了提高遮挡Robustness，我们采用Transformer来进行全局信息交换，使每个本地特征包含全局信息。最后，我们在Transformer Encoder中引入几何意识模块，以形成有效的约束，使点云特征学习更加紧密地关联点云任务。我们对TransPose进行了广泛的实验，结果表明TransPose可以准确地估算6D对象姿。我们的姿态估算管道在三个标准数据集上达到了竞争性的 результа。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-for-Plant-Identification-and-Disease-Classification-from-Leaf-Images-Multi-prediction-Approaches"><a href="#Deep-Learning-for-Plant-Identification-and-Disease-Classification-from-Leaf-Images-Multi-prediction-Approaches" class="headerlink" title="Deep Learning for Plant Identification and Disease Classification from Leaf Images: Multi-prediction Approaches"></a>Deep Learning for Plant Identification and Disease Classification from Leaf Images: Multi-prediction Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16273">http://arxiv.org/abs/2310.16273</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/funzi-son/plant_pathology_dl">https://github.com/funzi-son/plant_pathology_dl</a></li>
<li>paper_authors: Jianping Yao, Son N. Tran, Saurabh Garg, Samantha Sawyer<br>for: 本研究主要针对现代农业中的深度学习应用，特别是使用叶子图像进行植物疾病诊断，深度学习在这一领域中扮演着重要的角色。methods: 本研究使用的方法包括多种多样的深度学习模型，包括多模型、多标签、多输出和多任务模型，其中不同的底层CNN可以被使用。results: 经过实验研究，我们发现使用InceptionV3作为底层CNN可以获得更好的性能，而使用单个模型也可以与使用两个模型相比肤。最终，我们的提出的总结型多输出CNN（GSMo-CNN）在三个标准测试集上达到了领先的性能。<details>
<summary>Abstract</summary>
Deep learning plays an important role in modern agriculture, especially in plant pathology using leaf images where convolutional neural networks (CNN) are attracting a lot of attention. While numerous reviews have explored the applications of deep learning within this research domain, there remains a notable absence of an empirical study to offer insightful comparisons due to the employment of varied datasets in the evaluation. Furthermore, a majority of these approaches tend to address the problem as a singular prediction task, overlooking the multifaceted nature of predicting various aspects of plant species and disease types. Lastly, there is an evident need for a more profound consideration of the semantic relationships that underlie plant species and disease types. In this paper, we start our study by surveying current deep learning approaches for plant identification and disease classification. We categorise the approaches into multi-model, multi-label, multi-output, and multi-task, in which different backbone CNNs can be employed. Furthermore, based on the survey of existing approaches in plant pathology and the study of available approaches in machine learning, we propose a new model named Generalised Stacking Multi-output CNN (GSMo-CNN). To investigate the effectiveness of different backbone CNNs and learning approaches, we conduct an intensive experiment on three benchmark datasets Plant Village, Plant Leaves, and PlantDoc. The experimental results demonstrate that InceptionV3 can be a good choice for a backbone CNN as its performance is better than AlexNet, VGG16, ResNet101, EfficientNet, MobileNet, and a custom CNN developed by us. Interestingly, empirical results support the hypothesis that using a single model can be comparable or better than using two models. Finally, we show that the proposed GSMo-CNN achieves state-of-the-art performance on three benchmark datasets.
</details>
<details>
<summary>摘要</summary>
现代农业中，深度学习扮演着重要的角色，特别是在植物病理学中使用叶片图像，其中 convolutional neural networks (CNN) 在这个领域吸引了很多关注。虽然有很多文章评论了深度学习在这个研究领域的应用，但是还没有一篇实证研究提供了有用的对比。此外，大多数方法都是单纯地视为预测问题，忽略了植物种和病种类型之间的多方面性。此外，还有一个明显的需求，即更深入地理解植物种和病种类型之间的含义关系。在本文中，我们开始我们的研究 by surveying current deep learning approaches for plant identification and disease classification.我们将这些方法分为多模型、多标签、多输出和多任务类型，其中可以使用不同的底层CNN。此外，根据现有的植物病理学方法和机器学习方法的调查，我们提出了一种新的模型 named Generalised Stacking Multi-output CNN (GSMo-CNN)。为了评估不同的底层CNN和学习方法的效果，我们在三个标准数据集（Plant Village、Plant Leaves、PlantDoc）上进行了广泛的实验。实验结果显示，InceptionV3可以作为底层CNN，其性能比AlexNet、VGG16、ResNet101、EfficientNet、MobileNet和我们自己开发的自定义CNN更好。有趣的是，实验结果支持我们的假设，即使用单个模型可以与使用两个模型相比或更好。最后，我们表明了我们提出的GSMo-CNN在三个标准数据集上达到了状态之前的最佳性能。
</details></li>
</ul>
<hr>
<h2 id="SCB-ST-Dataset4-Extending-the-Spatio-Temporal-Behavior-Dataset-in-Student-Classroom-Scenarios-Through-Image-Dataset-Method"><a href="#SCB-ST-Dataset4-Extending-the-Spatio-Temporal-Behavior-Dataset-in-Student-Classroom-Scenarios-Through-Image-Dataset-Method" class="headerlink" title="SCB-ST-Dataset4: Extending the Spatio-Temporal Behavior Dataset in Student Classroom Scenarios Through Image Dataset Method"></a>SCB-ST-Dataset4: Extending the Spatio-Temporal Behavior Dataset in Student Classroom Scenarios Through Image Dataset Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16267">http://arxiv.org/abs/2310.16267</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/whiffe/scb-dataset">https://github.com/whiffe/scb-dataset</a></li>
<li>paper_authors: Fan Yang, Xiaofei Wang</li>
<li>for: This paper aims to provide a solution to the lack of publicly available spatio-temporal datasets on student behavior, which hinders research in the field of automatic student behavior detection using deep learning methods.</li>
<li>methods: The proposed method involves extending the existing SCB-ST-Dataset4 with an image dataset and using a Behavior Similarity Index (BSI) to explore the similarity of behaviors.</li>
<li>results: The proposed method was evaluated using four deep learning algorithms (YOLOv5, YOLOv7, YOLOv8, and SlowFast) and achieved a mean average precision (map) of up to 82.3%. The experiment demonstrated the effectiveness of the method and the dataset provides a robust foundation for future research in student behavior detection.Here’s the information in Simplified Chinese text:</li>
<li>for: 这篇论文的目的是解决学生行为自动检测使用深度学习方法时缺乏公共可用的空间时间数据的问题。</li>
<li>methods: 提议的方法是通过扩展现有的 SCB-ST-Dataset4 图像集，并使用行为相似性指数 (BSI) 来探索行为之间的相似性。</li>
<li>results: 提议的方法被评估使用四种深度学习算法 (YOLOv5, YOLOv7, YOLOv8, SlowFast)，实现了最高的 mean average precision (map) 值达到 82.3%。实验证明了方法的有效性，数据集提供了未来学生行为检测研究的坚实基础。<details>
<summary>Abstract</summary>
Using deep learning methods to detect students' classroom behavior automatically is a promising approach for analyzing their class performance and improving teaching effectiveness. However, the lack of publicly available spatio-temporal datasets on student behavior, as well as the high cost of manually labeling such datasets, pose significant challenges for researchers in this field. To address this issue, we proposed a method for extending the spatio-temporal behavior dataset in Student Classroom Scenarios (SCB-ST-Dataset4) through image dataset. Our SCB-ST-Dataset4 comprises 754094 images with 25670 labels, focusing on 3 behaviors: hand-raising, reading, writing. Our proposed method can rapidly generate spatio-temporal behavioral datasets without requiring annotation. Furthermore, we proposed a Behavior Similarity Index (BSI) to explore the similarity of behaviors. We evaluated the dataset using the YOLOv5, YOLOv7, YOLOv8, and SlowFast algorithms, achieving a mean average precision (map) of up to 82.3%. The experiment further demonstrates the effectiveness of our method. This dataset provides a robust foundation for future research in student behavior detection, potentially contributing to advancements in this field. The SCB-ST-Dataset4 is available for download at: https://github.com/Whiffe/SCB-dataset.
</details>
<details>
<summary>摘要</summary>
（使用深度学习方法检测学生学习环境中的行为自动化是一个有前途的方法，可以分析学生的课程表现和提高教学效果。然而，学生行为的公共可用空间时间数据集和手动标注这些数据集的高成本，对于这个领域的研究人员而言是一个大的挑战。为解决这个问题，我们提出了一种方法，通过图像集来扩展学生学习环境中的行为数据集。我们的SCB-ST-Dataset4包含754094张图像和25670个标签，关注3种行为：抬头、读书和写作。我们提出的方法可以快速生成空间时间行为数据集，不需要注解。此外，我们还提出了行为相似指数（BSI），以探索行为之间的相似性。我们使用YOLOv5、YOLOv7、YOLOv8和SlowFast算法进行评估，实现了最大平均准确率（map）达82.3%。实验证明了我们的方法的有效性。这个数据集为未来学生行为检测领域的研究提供了一个坚实的基础，有助于这一领域的进步。SCB-ST-Dataset4可以在以下链接下载：https://github.com/Whiffe/SCB-dataset。）
</details></li>
</ul>
<hr>
<h2 id="UAV-Sim-NeRF-based-Synthetic-Data-Generation-for-UAV-based-Perception"><a href="#UAV-Sim-NeRF-based-Synthetic-Data-Generation-for-UAV-based-Perception" class="headerlink" title="UAV-Sim: NeRF-based Synthetic Data Generation for UAV-based Perception"></a>UAV-Sim: NeRF-based Synthetic Data Generation for UAV-based Perception</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16255">http://arxiv.org/abs/2310.16255</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher Maxey, Jaehoon Choi, Hyungtae Lee, Dinesh Manocha, Heesung Kwon</li>
<li>for: 用于提高UAV预测模型的训练数据 quantity和质量。</li>
<li>methods: 利用最新的神经渲染技术进行静态和动态新视图UAV预测图像生成，尤其是高空拍摄场景中的突出特征。</li>
<li>results: 使用混合实际和synthetic数据进行优化后，检测模型的性能得到了显著提升。<details>
<summary>Abstract</summary>
Tremendous variations coupled with large degrees of freedom in UAV-based imaging conditions lead to a significant lack of data in adequately learning UAV-based perception models. Using various synthetic renderers in conjunction with perception models is prevalent to create synthetic data to augment the learning in the ground-based imaging domain. However, severe challenges in the austere UAV-based domain require distinctive solutions to image synthesis for data augmentation. In this work, we leverage recent advancements in neural rendering to improve static and dynamic novelview UAV-based image synthesis, especially from high altitudes, capturing salient scene attributes. Finally, we demonstrate a considerable performance boost is achieved when a state-ofthe-art detection model is optimized primarily on hybrid sets of real and synthetic data instead of the real or synthetic data separately.
</details>
<details>
<summary>摘要</summary>
巨大的变化和大量的自由度在无人机图像环境中导致学习无人机图像模型的数据缺乏。使用各种合成渲染器和感知模型是常见的做法来创建合成数据以增强地面上的图像学习。然而，无人机图像领域的恶劣环境需要特有的解决方案来synthesize图像，尤其是从高空拍摄的场景。在这种情况下，我们利用最新的神经渲染技术来提高静止和动态新视图无人机图像synthesize，特别是高空拍摄的场景。最终，我们示出了将状态之最佳检测模型优化为主要使用混合的实际和合成数据集，而不是单独使用实际数据或合成数据，可以获得显著的性能提升。
</details></li>
</ul>
<hr>
<h2 id="GraFT-Gradual-Fusion-Transformer-for-Multimodal-Re-Identification"><a href="#GraFT-Gradual-Fusion-Transformer-for-Multimodal-Re-Identification" class="headerlink" title="GraFT: Gradual Fusion Transformer for Multimodal Re-Identification"></a>GraFT: Gradual Fusion Transformer for Multimodal Re-Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16856">http://arxiv.org/abs/2310.16856</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoli Yin, Jiayao Li, Eva Schiller, Luke McDermott, Daniel Cummings</li>
<li>for: 本研究旨在提出一种能够有效地进行多Modal ReID的模型，以满足计算机视觉领域中增加模式的需求。</li>
<li>methods: 本研究提出了一种名为Gradual Fusion Transformer（GraFT）的新模型，它使用学习扩展的协同自注意力机制，以便同时捕捉多Modal特征和物体特征。此外，研究人员还提出了一种新的训练方法和一种改进的 triplet损失函数，以便优化ReID特征空间。</li>
<li>results: 对于多Modal ReID任务，GraFT consistently 超越了现有的多Modal ReID标准准确率。此外，研究人员还通过了大量的缺失学习研究，以证明GraFT的有效性。此外，为了实现模型的部署 versatility，研究人员还提出了一种基于神经网络裁剪的方法，以实现模型的大小和性能之间的平衡。<details>
<summary>Abstract</summary>
Object Re-Identification (ReID) is pivotal in computer vision, witnessing an escalating demand for adept multimodal representation learning. Current models, although promising, reveal scalability limitations with increasing modalities as they rely heavily on late fusion, which postpones the integration of specific modality insights. Addressing this, we introduce the \textbf{Gradual Fusion Transformer (GraFT)} for multimodal ReID. At its core, GraFT employs learnable fusion tokens that guide self-attention across encoders, adeptly capturing both modality-specific and object-specific features. Further bolstering its efficacy, we introduce a novel training paradigm combined with an augmented triplet loss, optimizing the ReID feature embedding space. We demonstrate these enhancements through extensive ablation studies and show that GraFT consistently surpasses established multimodal ReID benchmarks. Additionally, aiming for deployment versatility, we've integrated neural network pruning into GraFT, offering a balance between model size and performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/25/cs.CV_2023_10_25/" data-id="clorjzl7g00kof1885wac61j7" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_10_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/25/cs.AI_2023_10_25/" class="article-date">
  <time datetime="2023-10-25T12:00:00.000Z" itemprop="datePublished">2023-10-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/25/cs.AI_2023_10_25/">cs.AI - 2023-10-25</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="math-PVS-A-Large-Language-Model-Framework-to-Map-Scientific-Publications-to-PVS-Theories"><a href="#math-PVS-A-Large-Language-Model-Framework-to-Map-Scientific-Publications-to-PVS-Theories" class="headerlink" title="math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories"></a>math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17064">http://arxiv.org/abs/2310.17064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hassen Saidi, Susmit Jha, Tuhin Sahai</li>
<li>for: This paper aims to investigate the applicability of large language models (LLMs) in formalizing advanced mathematical concepts and to propose a framework for critically reviewing and checking mathematical reasoning in research papers.</li>
<li>methods: The proposed framework synergizes the capabilities of proof assistants, specifically PVS, with LLMs, enabling a bridge between textual descriptions in academic papers and formal specifications in PVS.</li>
<li>results: The proposed approach, called “math-PVS,” can automatically extract and formalize mathematical theorems from research papers, offering an innovative tool for academic review and discovery.<details>
<summary>Abstract</summary>
As artificial intelligence (AI) gains greater adoption in a wide variety of applications, it has immense potential to contribute to mathematical discovery, by guiding conjecture generation, constructing counterexamples, assisting in formalizing mathematics, and discovering connections between different mathematical areas, to name a few.   While prior work has leveraged computers for exhaustive mathematical proof search, recent efforts based on large language models (LLMs) aspire to position computing platforms as co-contributors in the mathematical research process. Despite their current limitations in logic and mathematical tasks, there is growing interest in melding theorem proving systems with foundation models. This work investigates the applicability of LLMs in formalizing advanced mathematical concepts and proposes a framework that can critically review and check mathematical reasoning in research papers. Given the noted reasoning shortcomings of LLMs, our approach synergizes the capabilities of proof assistants, specifically PVS, with LLMs, enabling a bridge between textual descriptions in academic papers and formal specifications in PVS. By harnessing the PVS environment, coupled with data ingestion and conversion mechanisms, we envision an automated process, called \emph{math-PVS}, to extract and formalize mathematical theorems from research papers, offering an innovative tool for academic review and discovery.
</details>
<details>
<summary>摘要</summary>
随着人工智能（AI）在各种应用领域的推广，它在数学发现方面拥有巨大的潜力。AI可以引导推理生成、构建反例、协助正式化数学，以及发现不同数学领域之间的连接，等等。尽管以往的计算机被用于极限的数学证明搜索，但近期基于大语言模型（LLM）的努力希望将计算平台作为数学研究过程中的合作伙伴。虽然LLM在逻辑和数学任务上有限制，但是有关将基础模型与证明系统融合的兴趣在增长。这项工作研究了LLM在正式化高级数学概念方面的可用性，并提出了一个框架，可以对研究论文中的数学逻辑进行检查和评审。由于LLM的逻辑推理缺陷，我们的方法结合证明助手PVS的能力，实现了从学术论文中的文本描述转换到PVS中的正式规定的自动过程。通过将PVS环境与数据入口和转换机制相结合，我们可以实现一个名为“math-PVS”的自动化过程，从研究论文中提取和正式化数学定理，为数学研究的自动化评审和发现提供了一个创新的工具。
</details></li>
</ul>
<hr>
<h2 id="Learning-Repeatable-Speech-Embeddings-Using-An-Intra-class-Correlation-Regularizer"><a href="#Learning-Repeatable-Speech-Embeddings-Using-An-Intra-class-Correlation-Regularizer" class="headerlink" title="Learning Repeatable Speech Embeddings Using An Intra-class Correlation Regularizer"></a>Learning Repeatable Speech Embeddings Using An Intra-class Correlation Regularizer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17049">http://arxiv.org/abs/2310.17049</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vigor-jzhang/icc-regularizer">https://github.com/vigor-jzhang/icc-regularizer</a></li>
<li>paper_authors: Jianwei Zhang, Suren Jayasuriya, Visar Berisha</li>
<li>for: 这个论文的目的是提出一种新的超参数化方法，以提高深度神经网络在特定机器学习任务中的表现。</li>
<li>methods: 这个论文使用了 measurement theory 中的重复性概念，并提出了一种新的评价指标 - 内类相关系数（ICC）来评估嵌入的重复性。</li>
<li>results: 实验结果表明，添加 ICC 正则化可以提高学习的嵌入重复性，并且这些嵌入可以提高下游任务的表现，如 speaker verification、voice style conversion 和诊断异常声音。<details>
<summary>Abstract</summary>
A good supervised embedding for a specific machine learning task is only sensitive to changes in the label of interest and is invariant to other confounding factors. We leverage the concept of repeatability from measurement theory to describe this property and propose to use the intra-class correlation coefficient (ICC) to evaluate the repeatability of embeddings. We then propose a novel regularizer, the ICC regularizer, as a complementary component for contrastive losses to guide deep neural networks to produce embeddings with higher repeatability. We use simulated data to explain why the ICC regularizer works better on minimizing the intra-class variance than the contrastive loss alone. We implement the ICC regularizer and apply it to three speech tasks: speaker verification, voice style conversion, and a clinical application for detecting dysphonic voice. The experimental results demonstrate that adding an ICC regularizer can improve the repeatability of learned embeddings compared to only using the contrastive loss; further, these embeddings lead to improved performance in these downstream tasks.
</details>
<details>
<summary>摘要</summary>
一个好的监督式嵌入是只受标签变化的影响，而不受其他干扰因素的影响。我们利用测量理论中的重复性来描述这一特性，并提议使用内类相关系数（ICC）来评估嵌入的重复性。我们then propose a novel regularizer, the ICC regularizer, as a complementary component for contrastive losses to guide deep neural networks to produce embeddings with higher repeatability. We use simulated data to explain why the ICC regularizer works better on minimizing the intra-class variance than the contrastive loss alone. We implement the ICC regularizer and apply it to three speech tasks: speaker verification, voice style conversion, and a clinical application for detecting dysphonic voice. The experimental results demonstrate that adding an ICC regularizer can improve the repeatability of learned embeddings compared to only using the contrastive loss; further, these embeddings lead to improved performance in these downstream tasks.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="StochGradAdam-Accelerating-Neural-Networks-Training-with-Stochastic-Gradient-Sampling"><a href="#StochGradAdam-Accelerating-Neural-Networks-Training-with-Stochastic-Gradient-Sampling" class="headerlink" title="StochGradAdam: Accelerating Neural Networks Training with Stochastic Gradient Sampling"></a>StochGradAdam: Accelerating Neural Networks Training with Stochastic Gradient Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17042">http://arxiv.org/abs/2310.17042</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juyoung Yun</li>
<li>for: 提高深度学习优化的稳定性和性能</li>
<li>methods: 使用抽样 gradient 技术，选择部分 gradients 进行每一轮的优化</li>
<li>results: 在图像分类和 segmentation 任务中表现出色，比传统 Adam 优化器更高效Here’s the translation of the three key points in English:</li>
<li>for: Improving the stability and performance of deep learning optimization</li>
<li>methods: Using gradient sampling technique, selectively considering a subset of gradients for each iteration</li>
<li>results: Superior performance in image classification and segmentation tasks compared to traditional Adam optimizer<details>
<summary>Abstract</summary>
In the rapidly advancing domain of deep learning optimization, this paper unveils the StochGradAdam optimizer, a novel adaptation of the well-regarded Adam algorithm. Central to StochGradAdam is its gradient sampling technique. This method not only ensures stable convergence but also leverages the advantages of selective gradient consideration, fostering robust training by potentially mitigating the effects of noisy or outlier data and enhancing the exploration of the loss landscape for more dependable convergence. In both image classification and segmentation tasks, StochGradAdam has demonstrated superior performance compared to the traditional Adam optimizer. By judiciously sampling a subset of gradients at each iteration, the optimizer is optimized for managing intricate models. The paper provides a comprehensive exploration of StochGradAdam's methodology, from its mathematical foundations to bias correction strategies, heralding a promising advancement in deep learning training techniques.
</details>
<details>
<summary>摘要</summary>
在深度学习优化领域的快速发展中，这篇论文公布了StochGradAdam优化器，这是Adam算法的一种新的变体。StochGradAdam的核心技术是 Gradient Sampling 技术，这种方法不仅保证稳定的收敛，还可以选择性考虑梯度，从而避免噪音或异常数据的影响，并且可以更好地探索损失函数的地形，以更可靠的收敛。在图像分类和分割任务中，StochGradAdam已经与传统的Adam优化器相比示出了更高的性能。每次迭代中选择一 subset of 梯度，使得优化器更适合处理复杂的模型。文章从数学基础到偏误修正策略进行了全面的探讨，这标志着深度学习训练技术的一个新的突破。
</details></li>
</ul>
<hr>
<h2 id="On-Surgical-Fine-tuning-for-Language-Encoders"><a href="#On-Surgical-Fine-tuning-for-Language-Encoders" class="headerlink" title="On Surgical Fine-tuning for Language Encoders"></a>On Surgical Fine-tuning for Language Encoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17041">http://arxiv.org/abs/2310.17041</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ymtao5219/surgical_fine_tuning">https://github.com/ymtao5219/surgical_fine_tuning</a></li>
<li>paper_authors: Abhilasha Lodha, Gayatri Belapurkar, Saloni Chalkapurkar, Yuanming Tao, Reshmi Ghosh, Samyadeep Basu, Dmitrii Petrov, Soundararajan Srinivasan</li>
<li>for: 这篇论文的目的是为了探索可以将语言模型 Fine-tuning 的层数范围降低到少数层，以提高下游语言任务的性能。</li>
<li>methods: 本研究使用了一个简单的度量基于渔业信息矩阵（FIM score），来选择可以进行选择性 Fine-tuning 的层。这个度量可以实际地选择出适合的层，实现了下游语言任务的强大表现。</li>
<li>results: 研究发现，只需要 Fine-tuning 少数层就可以得到和完全 Fine-tuning 所有层的性能相似或更好的结果。此外，这个度量还可以在优化过程中保持不变，证明了其可靠性。<details>
<summary>Abstract</summary>
Fine-tuning all the layers of a pre-trained neural language encoder (either using all the parameters or using parameter-efficient methods) is often the de-facto way of adapting it to a new task. We show evidence that for different downstream language tasks, fine-tuning only a subset of layers is sufficient to obtain performance that is close to and often better than fine-tuning all the layers in the language encoder. We propose an efficient metric based on the diagonal of the Fisher information matrix (FIM score), to select the candidate layers for selective fine-tuning. We show, empirically on GLUE and SuperGLUE tasks and across distinct language encoders, that this metric can effectively select layers leading to a strong downstream performance. Our work highlights that task-specific information corresponding to a given downstream task is often localized within a few layers, and tuning only those is sufficient for strong performance. Additionally, we demonstrate the robustness of the FIM score to rank layers in a manner that remains constant during the optimization process.
</details>
<details>
<summary>摘要</summary>
通常情况下，使用预训练神经语言编码器的所有参数进行精细调整（或者使用 parameter-efficient methods）是适应新任务的准确方法。我们的实验表明，对不同的下游语言任务，只需要调整一 subset of layers 可以获得与所有层的语言编码器调整性能很近的性能。我们提出了一个有效的度量基于斜矩阵 Fisher information matrix（FIM score），用于选择候选层进行选择性调整。我们的实验表明，这个度量可以有效地选择层，并在 GLUE 和 SuperGLUE 任务上 across distinct language encoders 获得出色的下游性能。我们的研究表明，任务特定的信息通常在几层中具有高度的地方化特征，并且只需要调整这些层可以获得出色的性能。此外，我们还证明了 FIM score 可以在优化过程中保持不变的方式对层进行排名。
</details></li>
</ul>
<hr>
<h2 id="Apollo-Zero-shot-MultiModal-Reasoning-with-Multiple-Experts"><a href="#Apollo-Zero-shot-MultiModal-Reasoning-with-Multiple-Experts" class="headerlink" title="Apollo: Zero-shot MultiModal Reasoning with Multiple Experts"></a>Apollo: Zero-shot MultiModal Reasoning with Multiple Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18369">http://arxiv.org/abs/2310.18369</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/danielabd/apollo-cap">https://github.com/danielabd/apollo-cap</a></li>
<li>paper_authors: Daniela Ben-David, Tzuf Paz-Argaman, Reut Tsarfaty</li>
<li>For: The paper is written for proposing a modular framework that leverages the expertise of different foundation models over different modalities and domains to perform a single, complex, multi-modal task without relying on prompt engineering or tailor-made multi-modal training.* Methods: The paper proposes a modular framework that enables decentralized command execution and allows each model to contribute and benefit from the expertise of the other models. The approach can be extended to a variety of foundation models, including audio and vision models, and does not depend on prompts.* Results: The paper demonstrates the effectiveness of the proposed approach on two tasks: stylized image captioning and audio-aware image captioning. The experiments show that the approach outperforms semi-supervised state-of-the-art models on the stylized image captioning task while being zero-shot and avoiding costly training, data collection, and prompt engineering. Additionally, the approach is applied to a novel task of audio-aware image captioning, where the task is to generate text that describes the image within the context of the provided audio.<details>
<summary>Abstract</summary>
We propose a modular framework that leverages the expertise of different foundation models over different modalities and domains in order to perform a single, complex, multi-modal task, without relying on prompt engineering or otherwise tailor-made multi-modal training. Our approach enables decentralized command execution and allows each model to both contribute and benefit from the expertise of the other models. Our method can be extended to a variety of foundation models (including audio and vision), above and beyond only language models, as it does not depend on prompts. We demonstrate our approach on two tasks. On the well-known task of stylized image captioning, our experiments show that our approach outperforms semi-supervised state-of-the-art models, while being zero-shot and avoiding costly training, data collection, and prompt engineering. We further demonstrate this method on a novel task, audio-aware image captioning, in which an image and audio are given and the task is to generate text that describes the image within the context of the provided audio. Our code is available on GitHub.
</details>
<details>
<summary>摘要</summary>
我们提出了一个模块化框架，利用不同基础模型在不同Modalities和领域的专业知识，实现单一、复杂多Modal任务，不依赖于提问工程或特制多Modal训练。我们的方法允许分布式命令执行和每个模型都可以享受到其他模型的专业知识。我们的方法可以扩展到多种基础模型（包括音频和视觉），而不仅仅是语言模型，因为它不依赖于提问。我们的实验表明，我们的方法可以超越半导化状态体验的模型，而且是零shot和不需要贵重训练、数据收集和提问工程。我们还在一个新任务上进行了实验，即Audio-aware图像描述，在给定的图像和音频基础上，生成描述图像的文本。我们的代码可以在GitHub上下载。
</details></li>
</ul>
<hr>
<h2 id="netFound-Foundation-Model-for-Network-Security"><a href="#netFound-Foundation-Model-for-Network-Security" class="headerlink" title="netFound: Foundation Model for Network Security"></a>netFound: Foundation Model for Network Security</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17025">http://arxiv.org/abs/2310.17025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Satyandra Guthula, Navya Battula, Roman Beltiukov, Wenbo Guo, Arpit Gupta</li>
<li>for: 本研究旨在提出一种基础模型（netFound），用于网络安全领域的机器学习（ML）应用。</li>
<li>methods: 本研究使用自我超vised算法对 readily available的无标签网络包迹进行预训练，然后使用层次和多模态特征来具体捕捉网络交互的隐藏 context。</li>
<li>results: 对三种网络下游任务（流量分类、网络入侵检测和APT检测）进行了实验，并证明了 netFound 在这些任务中的superiority，同时也证明了其对噪音和缺失标签、时间变化和多种网络环境的Robustness。<details>
<summary>Abstract</summary>
In ML for network security, traditional workflows rely on high-quality labeled data and manual feature engineering, but limited datasets and human expertise hinder feature selection, leading to models struggling to capture crucial relationships and generalize effectively. Inspired by recent advancements in ML application domains like GPT-4 and Vision Transformers, we have developed netFound, a foundational model for network security. This model undergoes pre-training using self-supervised algorithms applied to readily available unlabeled network packet traces. netFound's design incorporates hierarchical and multi-modal attributes of network traffic, effectively capturing hidden networking contexts, including application logic, communication protocols, and network conditions.   With this pre-trained foundation in place, we can fine-tune netFound for a wide array of downstream tasks, even when dealing with low-quality, limited, and noisy labeled data. Our experiments demonstrate netFound's superiority over existing state-of-the-art ML-based solutions across three distinct network downstream tasks: traffic classification, network intrusion detection, and APT detection. Furthermore, we emphasize netFound's robustness against noisy and missing labels, as well as its ability to generalize across temporal variations and diverse network environments. Finally, through a series of ablation studies, we provide comprehensive insights into how our design choices enable netFound to more effectively capture hidden networking contexts, further solidifying its performance and utility in network security applications.
</details>
<details>
<summary>摘要</summary>
在网络安全领域中的机器学习（ML）工作流程，传统上依靠高质量的标签数据和人工工程师，但是有限的数据集和人工专业知识限制了特征选择，导致模型困难捕捉关键关系和泛化有效。受最近的机器学习应用领域的进步，如GPT-4和视觉转换器，我们开发了netFound，一个基础模型 для网络安全。netFound模型在自动学习算法应用于可以获得的无标签网络包迹数据上进行预训练。netFound的设计包括层次结构和多模式特征的网络流量，能够有效捕捉隐藏的网络上下文，包括应用逻辑、通信协议和网络条件。通过这种预训练基础，我们可以对netFound进行细化，即使处理低质量、有限和噪声的标签数据时。我们的实验表明，netFound在三个不同的网络下游任务上表现出优于当前状态艺术的机器学习基本解决方案：流量分类、网络入侵检测和APT检测。此外，我们强调netFound对噪声和缺失标签的 Robustness，以及其能够在时间变化和多种网络环境下泛化。最后，通过一系列的减少研究，我们提供了广泛的启示，描述了我们的设计选择如何使得netFound更有效地捕捉隐藏的网络上Context，进一步巩固其性能和实用性在网络安全应用中。
</details></li>
</ul>
<hr>
<h2 id="Controlled-Decoding-from-Language-Models"><a href="#Controlled-Decoding-from-Language-Models" class="headerlink" title="Controlled Decoding from Language Models"></a>Controlled Decoding from Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17022">http://arxiv.org/abs/2310.17022</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sidharth Mudgal, Jong Lee, Harish Ganapathy, YaGuang Li, Tao Wang, Yanping Huang, Zhifeng Chen, Heng-Tze Cheng, Michael Collins, Trevor Strohman, Jilin Chen, Alex Beutel, Ahmad Beirami</li>
<li>for: 这个论文目标是控制语言模型的生成，使其向高评价结果靠拢。</li>
<li>methods: 该论文提出了一种新的Off-policy Reinforcement Learning方法，称为Controlled Decoding（CD），通过一个值函数来控制生成的评价。</li>
<li>results: 实验表明，CD能够有效地控制Reddit conversations corpus中的语言模型生成，并且可以解决多目标 reinforcement learning 问题无需额外复杂度。<details>
<summary>Abstract</summary>
We propose controlled decoding (CD), a novel off-policy reinforcement learning method to control the autoregressive generation from language models towards high reward outcomes. CD solves an off-policy reinforcement learning problem through a value function for the reward, which we call a prefix scorer. The prefix scorer is used at inference time to steer the generation towards higher reward outcomes. We show that the prefix scorer may be trained on (possibly) off-policy data to predict the expected reward when decoding is continued from a partially decoded response. We empirically demonstrate that CD is effective as a control mechanism on Reddit conversations corpus. We also show that the modularity of the design of CD makes it possible to control for multiple rewards, effectively solving a multi-objective reinforcement learning problem with no additional complexity. Finally, we show that CD can be applied in a novel blockwise fashion at inference-time, again without the need for any training-time changes, essentially bridging the gap between the popular best-of-$K$ strategy and token-level reinforcement learning. This makes CD a promising approach for alignment of language models.
</details>
<details>
<summary>摘要</summary>
我们提出控制解码（CD），一种新的离政策强化学习方法，用于控制语言模型的自然逻辑生成向高赏点结果。CD解决了离政策强化学习问题通过一个值函数，我们称之为前缀评分器。前缀评分器在推理时使用于引导生成向高赏点结果。我们证明了前缀评分器可以在（可能）离政策数据上训练，以预测继续推理后的预期奖励。我们在Reddit会话集体上进行了实验，证明了CD的效果。此外，我们还表明了CD的模块化设计，可以控制多个奖励，实际解决了多目标强化学习问题无额外复杂度。最后，我们表明了CD可以在推理时进行块式应用，再无需任何训练时间变化，实际连接了最受欢迎的best-of-$K$策略和token级强化学习。这使得CD成为对语言模型的Alignment的有望方法。
</details></li>
</ul>
<hr>
<h2 id="An-Integrative-Survey-on-Mental-Health-Conversational-Agents-to-Bridge-Computer-Science-and-Medical-Perspectives"><a href="#An-Integrative-Survey-on-Mental-Health-Conversational-Agents-to-Bridge-Computer-Science-and-Medical-Perspectives" class="headerlink" title="An Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives"></a>An Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17017">http://arxiv.org/abs/2310.17017</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jeffreych0/mental_chatbot_survey">https://github.com/jeffreych0/mental_chatbot_survey</a></li>
<li>paper_authors: Young Min Cho, Sunny Rai, Lyle Ungar, João Sedoc, Sharath Chandra Guntuku</li>
<li>for: 这个论文主要是为了探讨心理健康对话代理（即 chatbot）在解决心理健康挑战方面的潜在效果，以及如何bridge между计算机科学和医学两个领域之间的知识分享。</li>
<li>methods: 这篇论文采用了PRISMA框架进行系统性的文献综述，检查了534篇发表在计算机科学和医学两个领域的论文。</li>
<li>results: 论文发现了136篇关于建立心理健康相关对话代理的关键论文，这些论文中的模型和实验设计技术有多种多样。计算机科学论文更加关注LLM技术和自动化评价指标，而医学论文则更加关注规则驱动的对话代理和参与者的健康结果。<details>
<summary>Abstract</summary>
Mental health conversational agents (a.k.a. chatbots) are widely studied for their potential to offer accessible support to those experiencing mental health challenges. Previous surveys on the topic primarily consider papers published in either computer science or medicine, leading to a divide in understanding and hindering the sharing of beneficial knowledge between both domains. To bridge this gap, we conduct a comprehensive literature review using the PRISMA framework, reviewing 534 papers published in both computer science and medicine. Our systematic review reveals 136 key papers on building mental health-related conversational agents with diverse characteristics of modeling and experimental design techniques. We find that computer science papers focus on LLM techniques and evaluating response quality using automated metrics with little attention to the application while medical papers use rule-based conversational agents and outcome metrics to measure the health outcomes of participants. Based on our findings on transparency, ethics, and cultural heterogeneity in this review, we provide a few recommendations to help bridge the disciplinary divide and enable the cross-disciplinary development of mental health conversational agents.
</details>
<details>
<summary>摘要</summary>
心理健康对话机器人（即chatbot）广泛研究其潜在性能提供访问支持心理健康挑战的人。先前的调查主要考虑计算机科学和医学领域发表的论文，导致两个领域之间的理解不同，阻碍两个领域之间的有益知识共享。为bridging这个差距，我们采用PRISMA框架进行了全面的文献综述，查看了534篇发表在计算机科学和医学两个领域的论文。我们的系统性综述发现了136篇关于建立心理健康相关对话机器人的重要论文，其中计算机科学论文主要关注LLM技术和自动评价指标，而医学论文主要采用规则型对话机器人和参与者的健康结果评价指标。根据我们在透明度、伦理和文化多样性方面的发现，我们提出了一些建议，以帮助跨学科发展心理健康对话机器人。
</details></li>
</ul>
<hr>
<h2 id="Personalized-Speech-driven-Expressive-3D-Facial-Animation-Synthesis-with-Style-Control"><a href="#Personalized-Speech-driven-Expressive-3D-Facial-Animation-Synthesis-with-Style-Control" class="headerlink" title="Personalized Speech-driven Expressive 3D Facial Animation Synthesis with Style Control"></a>Personalized Speech-driven Expressive 3D Facial Animation Synthesis with Style Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17011">http://arxiv.org/abs/2310.17011</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elif Bozkurt</li>
<li>for: 这个论文旨在创建一个基于真实人脸动画的表情控制框架，以实现高度自然和可靠的人脸动画synthesis。</li>
<li>methods: 该框架使用一个非autoregressive encoder-decoder架构，包括表情编码器、语音编码器和表情解码器。在训练阶段，表情编码器首先分解人脸动画序列为个性特征和语音相关信息，然后将这些信息输入到 transformer层中进行更新。</li>
<li>results: 该方法可以生成基于输入语音的自然和准确的人脸动画，同时保留目标人脸的说话风格。<details>
<summary>Abstract</summary>
Different people have different facial expressions while speaking emotionally. A realistic facial animation system should consider such identity-specific speaking styles and facial idiosyncrasies to achieve high-degree of naturalness and plausibility. Existing approaches to personalized speech-driven 3D facial animation either use one-hot identity labels or rely-on person specific models which limit their scalability. We present a personalized speech-driven expressive 3D facial animation synthesis framework that models identity specific facial motion as latent representations (called as styles), and synthesizes novel animations given a speech input with the target style for various emotion categories. Our framework is trained in an end-to-end fashion and has a non-autoregressive encoder-decoder architecture with three main components: expression encoder, speech encoder and expression decoder. Since, expressive facial motion includes both identity-specific style and speech-related content information; expression encoder first disentangles facial motion sequences into style and content representations, respectively. Then, both of the speech encoder and the expression decoders input the extracted style information to update transformer layer weights during training phase. Our speech encoder also extracts speech phoneme label and duration information to achieve better synchrony within the non-autoregressive synthesis mechanism more effectively. Through detailed experiments, we demonstrate that our approach produces temporally coherent facial expressions from input speech while preserving the speaking styles of the target identities.
</details>
<details>
<summary>摘要</summary>
不同的人有不同的脸部表达方式，一个真实的脸部动画系统应该考虑到这些人各自的脸部表达风格和特点，以达到高度的自然性和可信度。现有的人性化语音驱动3D脸部动画方法可能使用一个热度标签或者基于特定人的模型，这限制了其扩展性。我们提出了一种基于语音输入的个性化表达3D脸部动画生成框架，该框架模型了人各自的脸部运动为缓存表示（称为风格），并将输入语音中的各种情感类别内容与目标风格相匹配。我们的框架在端到端的方式进行训练，具有非autoregressive编码器-解码器架构，包括表达编码器、语音编码器和表达解码器三个主要组件。由于表达动作包含人各自风格特点和语音相关的内容信息，表达编码器首先分解脸部动作序列为风格和内容表示，然后两个语音编码器和表达解码器都输入提取的风格信息以更新权重 durante el entrenamiento.我们的语音编码器还提取了语音音频标签和持续时间信息，以更好地同步在非autoregressive生成过程中。经过详细的实验，我们示出了我们的方法可以从输入语音中生成具有同步的脸部表达，同时保留目标人的说话风格。
</details></li>
</ul>
<hr>
<h2 id="This-Reads-Like-That-Deep-Learning-for-Interpretable-Natural-Language-Processing"><a href="#This-Reads-Like-That-Deep-Learning-for-Interpretable-Natural-Language-Processing" class="headerlink" title="This Reads Like That: Deep Learning for Interpretable Natural Language Processing"></a>This Reads Like That: Deep Learning for Interpretable Natural Language Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17010">http://arxiv.org/abs/2310.17010</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fanconic/this_reads_like_that">https://github.com/fanconic/this_reads_like_that</a></li>
<li>paper_authors: Claudio Fanconi, Moritz Vandenhirtz, Severin Husmann, Julia E. Vogt</li>
<li>for: 这个论文是为了提高自然语言处理中的 prototype 网络的性能和可解释性而写的。</li>
<li>methods: 这个论文使用了learned weighted similarity measure来增强prototype网络中的相似计算，以及提出了一种post-hoc解释机制来提取输入句子和prototype句子中关键的单词。</li>
<li>results: 论文的实验结果表明，对于 AG News 和 RT Polarity 数据集，提出的方法不仅超过了之前的 prototype-based 方法的预测性能，还提高了解释性的准确性 compared to rationale-based 循环卷积。<details>
<summary>Abstract</summary>
Prototype learning, a popular machine learning method designed for inherently interpretable decisions, leverages similarities to learned prototypes for classifying new data. While it is mainly applied in computer vision, in this work, we build upon prior research and further explore the extension of prototypical networks to natural language processing. We introduce a learned weighted similarity measure that enhances the similarity computation by focusing on informative dimensions of pre-trained sentence embeddings. Additionally, we propose a post-hoc explainability mechanism that extracts prediction-relevant words from both the prototype and input sentences. Finally, we empirically demonstrate that our proposed method not only improves predictive performance on the AG News and RT Polarity datasets over a previous prototype-based approach, but also improves the faithfulness of explanations compared to rationale-based recurrent convolutions.
</details>
<details>
<summary>摘要</summary>
专案学习，一种具有内在可解释的机器学习方法，利用学习到的原型 Similarities 来类别新数据。它主要应用于计算机视觉领域，在这个工作中，我们基于先前的研究进一步探索 prototype 网络的扩展到自然语言处理。我们提出了一个学习加权相似度量表，可以增强相似度计算，专注于预训照句子嵌入中的有用维度。此外，我们提出了一个后续解释机制，可以从原型和输入句子中提取预测相关的字词。最后，我们实践表明，我们的提议方法不仅在 AG News 和 RT Polarity 数据集上超越先前的原型基于方法，而且也提高了解释的实惠性比过去的 rational 基于循环推导。
</details></li>
</ul>
<hr>
<h2 id="STEER-Semantic-Turn-Extension-Expansion-Recognition-for-Voice-Assistants"><a href="#STEER-Semantic-Turn-Extension-Expansion-Recognition-for-Voice-Assistants" class="headerlink" title="STEER: Semantic Turn Extension-Expansion Recognition for Voice Assistants"></a>STEER: Semantic Turn Extension-Expansion Recognition for Voice Assistants</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16990">http://arxiv.org/abs/2310.16990</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leon Liyang Zhang, Jiarui Lu, Joel Ruben Antony Moniz, Aditya Kulkarni, Dhivya Piraviperumal, Tien Dung Tran, Nicholas Tzou, Hong Yu</li>
<li>for: 这个研究的目的是提出一种探测用户发送后续指令时的引导intent模型（STEER），以便更好地理解用户的需求。</li>
<li>methods: 该模型使用了一些规则来采样opt-in的使用数据，并使用了自然语言处理技术来分类用户的意图。</li>
<li>results: 实验结果表明，STEER模型在采样的数据上显示了优秀的准确率（超过95%），并且在真实世界中的探测场景中也表现出了强大的零基eline性。此外，提出了一个增强版本的模型（STEER+），该模型使用semantic parse tree来提供更多的上下文，以便更好地理解句子中的异常词。<details>
<summary>Abstract</summary>
In the context of a voice assistant system, steering refers to the phenomenon in which a user issues a follow-up command attempting to direct or clarify a previous turn. We propose STEER, a steering detection model that predicts whether a follow-up turn is a user's attempt to steer the previous command. Constructing a training dataset for steering use cases poses challenges due to the cold-start problem. To overcome this, we developed heuristic rules to sample opt-in usage data, approximating positive and negative samples without any annotation. Our experimental results show promising performance in identifying steering intent, with over 95% accuracy on our sampled data. Moreover, STEER, in conjunction with our sampling strategy, aligns effectively with real-world steering scenarios, as evidenced by its strong zero-shot performance on a human-graded evaluation set. In addition to relying solely on user transcripts as input, we introduce STEER+, an enhanced version of the model. STEER+ utilizes a semantic parse tree to provide more context on out-of-vocabulary words, such as named entities that often occur at the sentence boundary. This further improves model performance, reducing error rate in domains where entities frequently appear, such as messaging. Lastly, we present a data analysis that highlights the improvement in user experience when voice assistants support steering use cases.
</details>
<details>
<summary>摘要</summary>
在语音助手系统中，“steering”指的是用户发送后续命令，以修正或解释上一个命令的现象。我们提出了STEER模型，可以预测用户是否通过后续命令来修正之前的命令。因为constructing一个训练集用于steering用例具有冷启动问题，我们开发了一些规则来采样 opt-in 使用数据，使用无注释的样本来 aproximate正例和负例。我们的实验结果表明，STEER模型在预测steering意图时显示了良好的性能，准确率高于95%。此外，STEER模型， junto con我们的采样策略，对实际世界中的steering场景具有很强的适应性，如果 human-graded evaluation set 中的 zero-shot 性能。此外，我们还引入了STEER+，一个改进版本的模型。STEER+使用semantic parse tree来提供更多的上下文，包括 sentence boundary 上的名称实体，这会进一步提高模型性能，特别是在消息频道上。最后，我们展示了一个数据分析，表明voice assistant支持steering用例可以提高用户体验。
</details></li>
</ul>
<hr>
<h2 id="The-Significance-of-Machine-Learning-in-Clinical-Disease-Diagnosis-A-Review"><a href="#The-Significance-of-Machine-Learning-in-Clinical-Disease-Diagnosis-A-Review" class="headerlink" title="The Significance of Machine Learning in Clinical Disease Diagnosis: A Review"></a>The Significance of Machine Learning in Clinical Disease Diagnosis: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16978">http://arxiv.org/abs/2310.16978</a></li>
<li>repo_url: None</li>
<li>paper_authors: S M Atikur Rahman, Sifat Ibtisum, Ehsan Bazgir, Tumpa Barai</li>
<li>for: 本研究旨在提高心率数据传输的准确性和计算效率，帮助医疗机构更好地诊断疾病。</li>
<li>methods: 本研究使用了多种机器学习算法，包括支持向量机器学习、决策树、Random Forest等，以提高疾病诊断的准确性和效率。</li>
<li>results: 研究发现，使用机器学习算法可以提高心率数据的准确性和计算效率，并且可以适应不同的疾病类型和数据类型。<details>
<summary>Abstract</summary>
The global need for effective disease diagnosis remains substantial, given the complexities of various disease mechanisms and diverse patient symptoms. To tackle these challenges, researchers, physicians, and patients are turning to machine learning (ML), an artificial intelligence (AI) discipline, to develop solutions. By leveraging sophisticated ML and AI methods, healthcare stakeholders gain enhanced diagnostic and treatment capabilities. However, there is a scarcity of research focused on ML algorithms for enhancing the accuracy and computational efficiency. This research investigates the capacity of machine learning algorithms to improve the transmission of heart rate data in time series healthcare metrics, concentrating particularly on optimizing accuracy and efficiency. By exploring various ML algorithms used in healthcare applications, the review presents the latest trends and approaches in ML-based disease diagnosis (MLBDD). The factors under consideration include the algorithm utilized, the types of diseases targeted, the data types employed, the applications, and the evaluation metrics. This review aims to shed light on the prospects of ML in healthcare, particularly in disease diagnosis. By analyzing the current literature, the study provides insights into state-of-the-art methodologies and their performance metrics.
</details>
<details>
<summary>摘要</summary>
全球医疾诊断需求仍然很大，因为各种疾病机制复杂，病人症状多样化。为了解决这些挑战，研究人员、医生和患者都在转向机器学习（ML），一种人工智能（AI）专业，开发解决方案。通过利用高级ML和AI技术，医疗各界人员获得了提高诊断和治疗能力。然而，关于ML算法以提高时间序列医疗指标中心脉速率数据传输的准确性和计算效率的研究相对落后。本研究探讨了ML算法在医疗应用中的可行性，特别是在提高准确性和计算效率方面。通过检查各种健康应用中的ML算法，本文提供了最新的趋势和方法。评价标准包括算法使用、疾病类型、数据类型、应用程序和评价指标。本文的目的是探讨ML在医疗领域的前景，特别是疾病诊断方面的可能性。通过分析当前文献，本研究提供了有关当前领先技术和其性能指标的视角。
</details></li>
</ul>
<hr>
<h2 id="CL-MASR-A-Continual-Learning-Benchmark-for-Multilingual-ASR"><a href="#CL-MASR-A-Continual-Learning-Benchmark-for-Multilingual-ASR" class="headerlink" title="CL-MASR: A Continual Learning Benchmark for Multilingual ASR"></a>CL-MASR: A Continual Learning Benchmark for Multilingual ASR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16931">http://arxiv.org/abs/2310.16931</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/speechbrain/benchmarks">https://github.com/speechbrain/benchmarks</a></li>
<li>paper_authors: Luca Della Libera, Pooneh Mousavi, Salah Zaiem, Cem Subakan, Mirco Ravanelli</li>
<li>for: 本研究旨在提供一个用于多语言自动语音识别（ASR）系统的 continual learning benchmark，以探索在新语言中学习时，如何保持先前语言的知识。</li>
<li>methods: 本研究使用了现有的大规模预训 ASR 模型，并实现了多种 continual learning 方法，以评估学习新语言时的效果。</li>
<li>results: 本研究提供了一个多语言 ASR 的 continual learning benchmark，并在这个 benchmark 上评估了多种 continual learning 方法的效果，以探索如何在学习新语言时，保持先前语言的知识。<details>
<summary>Abstract</summary>
Modern multilingual automatic speech recognition (ASR) systems like Whisper have made it possible to transcribe audio in multiple languages with a single model. However, current state-of-the-art ASR models are typically evaluated on individual languages or in a multi-task setting, overlooking the challenge of continually learning new languages. There is insufficient research on how to add new languages without losing valuable information from previous data. Furthermore, existing continual learning benchmarks focus mostly on vision and language tasks, leaving continual learning for multilingual ASR largely unexplored. To bridge this gap, we propose CL-MASR, a benchmark designed for studying multilingual ASR in a continual learning setting. CL-MASR provides a diverse set of continual learning methods implemented on top of large-scale pretrained ASR models, along with common metrics to assess the effectiveness of learning new languages while addressing the issue of catastrophic forgetting. To the best of our knowledge, CL-MASR is the first continual learning benchmark for the multilingual ASR task. The code is available at https://github.com/speechbrain/benchmarks.
</details>
<details>
<summary>摘要</summary>
现代多语言自动语音识别（ASR）系统如Whisper已经使得可以通过单个模型来识别多种语言的音频。然而，当前领先的ASR模型通常在单个语言或多任务 Setting中被评估，忽略了在新语言上不断学习的挑战。目前学术研究中不 enough to add new languages without losing valuable information from previous data. In addition, existing continual learning benchmarks focus mainly on vision and language tasks, leaving continual learning for multilingual ASR largely unexplored. To bridge this gap, we propose CL-MASR, a benchmark designed for studying multilingual ASR in a continual learning setting. CL-MASR provides a diverse set of continual learning methods implemented on top of large-scale pretrained ASR models, along with common metrics to assess the effectiveness of learning new languages while addressing the issue of catastrophic forgetting. To the best of our knowledge, CL-MASR is the first continual learning benchmark for the multilingual ASR task. 代码可以在https://github.com/speechbrain/benchmarks中找到。
</details></li>
</ul>
<hr>
<h2 id="Wide-Flat-Minimum-Watermarking-for-Robust-Ownership-Verification-of-GANs"><a href="#Wide-Flat-Minimum-Watermarking-for-Robust-Ownership-Verification-of-GANs" class="headerlink" title="Wide Flat Minimum Watermarking for Robust Ownership Verification of GANs"></a>Wide Flat Minimum Watermarking for Robust Ownership Verification of GANs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16919">http://arxiv.org/abs/2310.16919</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianwei Fei, Zhihua Xia, Benedetta Tondi, Mauro Barni</li>
<li>for: 保护基于生成器的知识产权 (Intellectual Property Rights)  against 白盒模型攻击 (white-box attacks)</li>
<li>methods: 使用多比特盒子-自由 watermarking 方法，在 GAN 训练过程中添加额外的 watermarking 损失函数，使得生成的图像含有隐藏的水印，可以通过预训练的水印解码器检测。为提高鲁棒性，使模型参数具有宽浅的极小值，使得任何模型参数修改都不会消除水印。</li>
<li>results: 实验结果表明，存在水印的图像质量几乎不受影响，而且水印具有高度鲁棒性，抵抗模型修改和代理模型攻击。<details>
<summary>Abstract</summary>
We propose a novel multi-bit box-free watermarking method for the protection of Intellectual Property Rights (IPR) of GANs with improved robustness against white-box attacks like fine-tuning, pruning, quantization, and surrogate model attacks. The watermark is embedded by adding an extra watermarking loss term during GAN training, ensuring that the images generated by the GAN contain an invisible watermark that can be retrieved by a pre-trained watermark decoder. In order to improve the robustness against white-box model-level attacks, we make sure that the model converges to a wide flat minimum of the watermarking loss term, in such a way that any modification of the model parameters does not erase the watermark. To do so, we add random noise vectors to the parameters of the generator and require that the watermarking loss term is as invariant as possible with respect to the presence of noise. This procedure forces the generator to converge to a wide flat minimum of the watermarking loss. The proposed method is architectureand dataset-agnostic, thus being applicable to many different generation tasks and models, as well as to CNN-based image processing architectures. We present the results of extensive experiments showing that the presence of the watermark has a negligible impact on the quality of the generated images, and proving the superior robustness of the watermark against model modification and surrogate model attacks.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的多位数字箱子无法水印方法，用于保护生成对应的知识产权（IPR）。这种水印方法可以增强对白盒式攻击，如精细调整、剪辑、量化和代理模型攻击的Robustness。在生成器训练时，我们通过添加额外的水印损失项来嵌入水印，使生成器生成的图像包含一个不可见的水印，可以通过预训练的水印解码器来检索。为了增强对白盒模型级攻击的Robustness，我们确保生成器在水印损失项中落在宽阔的平坦 minimum 中，以避免任何模型参数的修改可以消除水印。为此，我们在生成器参数中添加随机的噪声向量，并要求水印损失项在噪声存在时保持一定的不变性。这个过程使生成器 converges 到一个宽阔的平坦 minimum ，使得任何模型参数的修改都不会消除水印。我们的方法是 Architecture 和 Dataset 无关的，因此可以应用于许多不同的生成任务和模型，以及 CNN 基于的图像处理架构。我们的实验结果表明，水印的存在对生成的图像质量的影响为无效的，并证明了我们的水印方法对模型修改和代理模型攻击的Robustness 是超越的。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Learning-of-Molecular-Embeddings-for-Enhanced-Clustering-and-Emergent-Properties-for-Chemical-Compounds"><a href="#Unsupervised-Learning-of-Molecular-Embeddings-for-Enhanced-Clustering-and-Emergent-Properties-for-Chemical-Compounds" class="headerlink" title="Unsupervised Learning of Molecular Embeddings for Enhanced Clustering and Emergent Properties for Chemical Compounds"></a>Unsupervised Learning of Molecular Embeddings for Enhanced Clustering and Emergent Properties for Chemical Compounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18367">http://arxiv.org/abs/2310.18367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaiveer Gill, Ratul Chakraborty, Reetham Gubba, Amy Liu, Shrey Jain, Chirag Iyer, Obaid Khwaja, Saurav Kumar</li>
<li>for: 这 paper 的目的是为了开发一种新的计算工具，用于探索和理解分子结构和性质。</li>
<li>methods: 这 paper 使用了多种方法来探索和分类化学物质的 SMILES 数据，包括图Structures 分析和自然语言描述嵌入。</li>
<li>results: 这 paper 的结果表明，使用这些方法可以获得了明确的、集中的分 clusters，并且可以有效地查询和理解化学物质。<details>
<summary>Abstract</summary>
The detailed analysis of molecular structures and properties holds great potential for drug development discovery through machine learning. Developing an emergent property in the model to understand molecules would broaden the horizons for development with a new computational tool. We introduce various methods to detect and cluster chemical compounds based on their SMILES data. Our first method, analyzing the graphical structures of chemical compounds using embedding data, employs vector search to meet our threshold value. The results yielded pronounced, concentrated clusters, and the method produced favorable results in querying and understanding the compounds. We also used natural language description embeddings stored in a vector database with GPT3.5, which outperforms the base model. Thus, we introduce a similarity search and clustering algorithm to aid in searching for and interacting with molecules, enhancing efficiency in chemical exploration and enabling future development of emergent properties in molecular property prediction models.
</details>
<details>
<summary>摘要</summary>
detail 分析分子结构和性质具有很大的潜力用于药物发现，通过机器学习来开拓新的计算工具。我们引入了多种方法来探测和归类化学化合物基于其SMILES数据。我们的第一种方法是使用嵌入数据来探测化学结构的图形结构，并使用vector搜索达到我们的阈值。结果出现了明显、集中的团结果，这种方法在查询和理解化学物质方面表现出了良好的效果。我们还使用GPT3.5中的自然语言描述嵌入存储在向量库中，这超越了基础模型。因此，我们引入了相似搜索和归类算法，以帮助在化学探索中快速搜索和交互化学物质，提高化学探索的效率，并启动未来的分子性质预测模型的发展。
</details></li>
</ul>
<hr>
<h2 id="RDBench-ML-Benchmark-for-Relational-Databases"><a href="#RDBench-ML-Benchmark-for-Relational-Databases" class="headerlink" title="RDBench: ML Benchmark for Relational Databases"></a>RDBench: ML Benchmark for Relational Databases</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16837">http://arxiv.org/abs/2310.16837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zizhao Zhang, Yi Yang, Lutong Zou, He Wen, Tao Feng, Jiaxuan You<br>for:ML Benchmark For Relational Databases (RDBench) aims to promote reproducible ML research on RDBs that include multiple tables.methods:RDBench offers diverse RDB datasets of varying scales, domains, and relational structures, organized into 4 levels. It exposes three types of interfaces including tabular data, homogeneous graphs, and heterogeneous graphs, sharing the same underlying task definition.results:RDBench enables meaningful comparisons between ML methods from diverse domains, ranging from XGBoost to Graph Neural Networks, under RDB prediction tasks. Multiple classification and regression tasks are designed for each RDB dataset, and results are reported with averaged findings to enhance the robustness of the experimental results.<details>
<summary>Abstract</summary>
Benefiting from high-quality datasets and standardized evaluation metrics, machine learning (ML) has achieved sustained progress and widespread applications. However, while applying machine learning to relational databases (RDBs), the absence of a well-established benchmark remains a significant obstacle to the development of ML. To address this issue, we introduce ML Benchmark For Relational Databases (RDBench), a standardized benchmark that aims to promote reproducible ML research on RDBs that include multiple tables. RDBench offers diverse RDB datasets of varying scales, domains, and relational structures, organized into 4 levels. Notably, to simplify the adoption of RDBench for diverse ML domains, for any given database, RDBench exposes three types of interfaces including tabular data, homogeneous graphs, and heterogeneous graphs, sharing the same underlying task definition. For the first time, RDBench enables meaningful comparisons between ML methods from diverse domains, ranging from XGBoost to Graph Neural Networks, under RDB prediction tasks. We design multiple classification and regression tasks for each RDB dataset and report averaged results over the same dataset, further enhancing the robustness of the experimental findings. RDBench is implemented with DBGym, a user-friendly platform for ML research and application on databases, enabling benchmarking new ML methods with RDBench at ease.
</details>
<details>
<summary>摘要</summary>
使用高质量的数据集和标准化的评估 metric，机器学习（ML）已经取得了持续的进步和广泛的应用。然而，当应用机器学习到关系数据库（RDB）时，缺乏一个具有广泛适用性的标准准测试 benchmark 是一个重要的障碍物。为解决这个问题，我们介绍了 ML Benchmark For Relational Databases（RDBench），一个标准化的准测试 benchmark，旨在促进可重复的 ML 研究在 RDB 中，包括多张表。RDBench 提供了多种不同的 RDB 数据集，其中每个数据集都有不同的规模、领域和关系结构，并且分为 4 级别。尤其是，为了简化 RDBench 在多种 ML 领域中的采用，对于任何给定的数据库，RDBench 提供了三种类型的接口，包括表格数据、同质graph和不同质graph，这些接口都是基于同一个任务定义。这样，RDBench 为不同领域的 ML 方法进行比较，从 XGBoost 到图 neural network，在 RDB 预测任务中实现了意义的比较。我们设计了多种分类和回归任务，并对每个 RDB 数据集进行了多次评估，以提高实验结果的稳定性。RDBench 通过 DBGym 实现，一个用户友好的平台 для ML 研究和应用于数据库，可以轻松地对 RDBench 进行准测试新的 ML 方法。
</details></li>
</ul>
<hr>
<h2 id="LLM-FP4-4-Bit-Floating-Point-Quantized-Transformers"><a href="#LLM-FP4-4-Bit-Floating-Point-Quantized-Transformers" class="headerlink" title="LLM-FP4: 4-Bit Floating-Point Quantized Transformers"></a>LLM-FP4: 4-Bit Floating-Point Quantized Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16836">http://arxiv.org/abs/2310.16836</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nbasyl/llm-fp4">https://github.com/nbasyl/llm-fp4</a></li>
<li>paper_authors: Shih-yang Liu, Zechun Liu, Xijie Huang, Pingcheng Dong, Kwang-Ting Cheng</li>
<li>for: 这个论文目的是为了在大型语言模型（LLM）中进行后training quantization，将 weights 和 activations 数值降到 4 位浮点数值。</li>
<li>methods: 这个方法使用了浮点数值量化（FP quantization），并通过搜寻优化量化参数以提高表现。此外，这个方法还使用了每道活动量化，以解决活动量化的问题。</li>
<li>results: 这个方法可以将 LLaMA-13B 模型中的 weights 和 activations 降到 4 位浮点数值，并在常识零shot reasoning 任务上得到了63.1的平均分数，仅比整数模型低5.8分，比前一代最佳方案高出12.7分。<details>
<summary>Abstract</summary>
We propose LLM-FP4 for quantizing both weights and activations in large language models (LLMs) down to 4-bit floating-point values, in a post-training manner. Existing post-training quantization (PTQ) solutions are primarily integer-based and struggle with bit widths below 8 bits. Compared to integer quantization, floating-point (FP) quantization is more flexible and can better handle long-tail or bell-shaped distributions, and it has emerged as a default choice in many hardware platforms. One characteristic of FP quantization is that its performance largely depends on the choice of exponent bits and clipping range. In this regard, we construct a strong FP-PTQ baseline by searching for the optimal quantization parameters. Furthermore, we observe a high inter-channel variance and low intra-channel variance pattern in activation distributions, which adds activation quantization difficulty. We recognize this pattern to be consistent across a spectrum of transformer models designed for diverse tasks, such as LLMs, BERT, and Vision Transformer models. To tackle this, we propose per-channel activation quantization and show that these additional scaling factors can be reparameterized as exponential biases of weights, incurring a negligible cost. Our method, for the first time, can quantize both weights and activations in the LLaMA-13B to only 4-bit and achieves an average score of 63.1 on the common sense zero-shot reasoning tasks, which is only 5.8 lower than the full-precision model, significantly outperforming the previous state-of-the-art by 12.7 points. Code is available at: https://github.com/nbasyl/LLM-FP4.
</details>
<details>
<summary>摘要</summary>
我们提出LLM-FP4，将大型语言模型（LLM）中的 weights 和 activaions 降到 4 位浮点数值的POST训练方法。现有的POST训练（PTQ）解决方案主要是整数型的，对于比特幅下8 bits以下的情况做出差。相比于整数量化，浮点数（FP）量化更 flexible，可以更好地处理长尾或铃鼓形分布，因此在许多硬件平台上变得默认的选择。一个FP量化的特点是它的性能受到选择的指数位数和截取范围的影响。为了建立强大的FP-PTQ基线，我们进行了搜寻最佳量化参数。此外，我们发现 activation 分布中存在高通道方差低通道方差的特性，这会增加 activation 量化的问题。我们认为这个特性是跨多个 transformer 模型设计 для多元任务的共同特点，例如 LLMs、BERT 和 Vision Transformer 模型。为了解决这个问题，我们提出了每通道 activation 量化，并证明这些额外的标准化因子可以被视为 weight 的指数偏移，带来无法忽略的成本。我们的方法首次可以将 LLaMA-13B 中的 weights 和 activations 降到 4 位浮点数值，并在常识零基础理解任务上实现平均得分63.1，仅比整数模型下降5.8分，与过去状态艺术的最佳方案相对提高12.7分。代码可以在以下链接获取：https://github.com/nbasyl/LLM-FP4。
</details></li>
</ul>
<hr>
<h2 id="Proposal-Contrastive-Pretraining-for-Object-Detection-from-Fewer-Data"><a href="#Proposal-Contrastive-Pretraining-for-Object-Detection-from-Fewer-Data" class="headerlink" title="Proposal-Contrastive Pretraining for Object Detection from Fewer Data"></a>Proposal-Contrastive Pretraining for Object Detection from Fewer Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16835">http://arxiv.org/abs/2310.16835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quentin Bouniot, Romaric Audigier, Angélique Loesch, Amaury Habrard</li>
<li>for: 这篇论文的目的是为了提出一种不需要大量数据的无监督预训方法，并且能够在实际应用中获得好的性能。</li>
<li>methods: 这篇论文使用了transformer构 architecture，并且将object detector作为预训模型，通过生成大量的object proposal来进行对照学习。</li>
<li>results: 研究发现，这种方法能够在标准和新的benchmark上实现顶尖的性能，并且在仅使用少量数据的情况下进行预训。<details>
<summary>Abstract</summary>
The use of pretrained deep neural networks represents an attractive way to achieve strong results with few data available. When specialized in dense problems such as object detection, learning local rather than global information in images has proven to be more efficient. However, for unsupervised pretraining, the popular contrastive learning requires a large batch size and, therefore, a lot of resources. To address this problem, we are interested in transformer-based object detectors that have recently gained traction in the community with good performance and with the particularity of generating many diverse object proposals.   In this work, we present Proposal Selection Contrast (ProSeCo), a novel unsupervised overall pretraining approach that leverages this property. ProSeCo uses the large number of object proposals generated by the detector for contrastive learning, which allows the use of a smaller batch size, combined with object-level features to learn local information in the images. To improve the effectiveness of the contrastive loss, we introduce the object location information in the selection of positive examples to take into account multiple overlapping object proposals. When reusing pretrained backbone, we advocate for consistency in learning local information between the backbone and the detection head.   We show that our method outperforms state of the art in unsupervised pretraining for object detection on standard and novel benchmarks in learning with fewer data.
</details>
<details>
<summary>摘要</summary>
“使用预训 Deep Neural Networks 是一种吸引人的方法来实现强大的结果，即使有限的数据available。当特化在对像检测这类密集问题时，学习本地而不是全局信息在图像中已经证明是更加有效率。然而，对于无超级预训，广泛的对比学习需要大批号Size和资源。为解决这个问题，我们对 transformer-based 对像检测器表示兴趣，这些检测器在社区中获得了良好的性能，并且具有生成多个多标的物件提案的特性。在这个工作中，我们提出了 Proposal Selection Contrast（ProSeCo），一种新的无超级预训方法，利用这个特性。ProSeCo 使用生成的物件提案进行对比学习，这allow us 使用较小的批号Size，同时使用物件层的特征来学习本地信息在图像中。为了提高对比损失的有效性，我们将物件位置信息包含在选择正例的范例中，以考虑多个重叠的物件提案。当 reuse pretrained backbone 时，我们强调了在 backbone 和检测头之间的一致性，以确保在学习本地信息时，backbone 和检测头之间的学习是一致的。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="TD-MPC2-Scalable-Robust-World-Models-for-Continuous-Control"><a href="#TD-MPC2-Scalable-Robust-World-Models-for-Continuous-Control" class="headerlink" title="TD-MPC2: Scalable, Robust World Models for Continuous Control"></a>TD-MPC2: Scalable, Robust World Models for Continuous Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16828">http://arxiv.org/abs/2310.16828</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nicklashansen/tdmpc2">https://github.com/nicklashansen/tdmpc2</a></li>
<li>paper_authors: Nicklas Hansen, Hao Su, Xiaolong Wang</li>
<li>for: 这个论文的目的是提出一种基于模型的强化学习算法（TD-MPC2），用于本地轨迹优化在学习得到的隐藏空间中。</li>
<li>methods: 这个算法使用了模型基于的强化学习策略，包括TD-MPC算法和一系列改进。</li>
<li>results: 在104个在线RL任务中，TD-MPC2表现出色，与基elines相比显著提高了表现，并且可以在多个任务领域、embodiments和动作空间中进行多任务学习。<details>
<summary>Abstract</summary>
TD-MPC is a model-based reinforcement learning (RL) algorithm that performs local trajectory optimization in the latent space of a learned implicit (decoder-free) world model. In this work, we present TD-MPC2: a series of improvements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves significantly over baselines across 104 online RL tasks spanning 4 diverse task domains, achieving consistently strong results with a single set of hyperparameters. We further show that agent capabilities increase with model and data size, and successfully train a single 317M parameter agent to perform 80 tasks across multiple task domains, embodiments, and action spaces. We conclude with an account of lessons, opportunities, and risks associated with large TD-MPC2 agents. Explore videos, models, data, code, and more at https://nicklashansen.github.io/td-mpc2
</details>
<details>
<summary>摘要</summary>
TD-MPC是一种基于模型的强化学习（RL）算法，实现了本地轨迹优化在学习的隐藏空间中。在这篇文章中，我们介绍了TD-MPC2：一系列对TD-MPC算法进行改进的方法。我们表明TD-MPC2在104个在线RL任务中表现出色，在4个多样化任务领域中准确地实现了强大的结果，并且使用单一的超参数来实现。我们进一步显示，代理机制能力随模型和数据集大小增长，并成功地训练了一个317M参数的单一代理来完成80个任务 across多个任务领域、实现方式和动作空间。我们结束时提出了大TD-MPC2代理的教训、机遇和风险。感兴趣的朋友可以前往https://nicklashansen.github.io/td-mpc2查看视频、模型、数据、代码等资源。
</details></li>
</ul>
<hr>
<h2 id="Prompt-Me-Up-Unleashing-the-Power-of-Alignments-for-Multimodal-Entity-and-Relation-Extraction"><a href="#Prompt-Me-Up-Unleashing-the-Power-of-Alignments-for-Multimodal-Entity-and-Relation-Extraction" class="headerlink" title="Prompt Me Up: Unleashing the Power of Alignments for Multimodal Entity and Relation Extraction"></a>Prompt Me Up: Unleashing the Power of Alignments for Multimodal Entity and Relation Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16822">http://arxiv.org/abs/2310.16822</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuming Hu, Junzhe Chen, Aiwei Liu, Shiao Meng, Lijie Wen, Philip S. Yu</li>
<li>for: 提高图像中 Entity 和 Relation EXTRACT 的能力</li>
<li>methods: 使用 Multimodal EXTRACTION 方法，combined with images and text，获取更多的信号，并将其们Alignment through graphs or hierarchical fusion，以提高EXTRACTION 能力</li>
<li>results:  experiments on three datasets show an average 3.41% F1 improvement over prior SOTA, and using the method on prior SOTA fusions further improves 5.47% F1.<details>
<summary>Abstract</summary>
How can we better extract entities and relations from text? Using multimodal extraction with images and text obtains more signals for entities and relations, and aligns them through graphs or hierarchical fusion, aiding in extraction. Despite attempts at various fusions, previous works have overlooked many unlabeled image-caption pairs, such as NewsCLIPing. This paper proposes innovative pre-training objectives for entity-object and relation-image alignment, extracting objects from images and aligning them with entity and relation prompts for soft pseudo-labels. These labels are used as self-supervised signals for pre-training, enhancing the ability to extract entities and relations. Experiments on three datasets show an average 3.41% F1 improvement over prior SOTA. Additionally, our method is orthogonal to previous multimodal fusions, and using it on prior SOTA fusions further improves 5.47% F1.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。</SYS>>可以通过多模态提取来更好地提取实体和关系从文本中，并使用图像和文本获得更多的信号，并将其归一化为图像或层次融合。不过，先前的作品都忽略了大量没有标签的图像描述对，如新闻CLIPing。这篇论文提出了一些创新的预训练目标，用于对象-图像和关系-图像的归一化，从图像中提取对象并将其与实体和关系提示进行软式 Pseudo-标签。这些标签被用作自我超vised信号进行预训练，从而提高实体和关系提取的能力。实验结果表明，与先前最优的STATE OF THE ART相比，我们的方法能够提高3.41%的F1分数。此外，我们的方法与先前的多模态融合 orthogonal，并在使用之前的最优融合后，进一步提高5.47%的F1分数。
</details></li>
</ul>
<hr>
<h2 id="Can-GPT-models-Follow-Human-Summarization-Guidelines-Evaluating-ChatGPT-and-GPT-4-for-Dialogue-Summarization"><a href="#Can-GPT-models-Follow-Human-Summarization-Guidelines-Evaluating-ChatGPT-and-GPT-4-for-Dialogue-Summarization" class="headerlink" title="Can GPT models Follow Human Summarization Guidelines? Evaluating ChatGPT and GPT-4 for Dialogue Summarization"></a>Can GPT models Follow Human Summarization Guidelines? Evaluating ChatGPT and GPT-4 for Dialogue Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16810">http://arxiv.org/abs/2310.16810</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongxin Zhou, Fabien Ringeval, François Portet</li>
<li>for: 本研究探讨了基于提示的大型自然语言模型（LLM）如ChatGPT和GPT-4在遵循人类指南的对话概要整理能力。</li>
<li>methods: 研究使用了DialogSum（英文社交对话）和DECODA（法语客服对话）等实验，测试了多种提示，包括现有文献中的提示和人类概要指南中的提示，以及两步提示方法。</li>
<li>results: 研究发现，GPT模型通常会生成长度很长的概要，并且与人类概要指南不准确。但是，使用人类指南作为中间步骤显示了 promise，在一些情况下超过了直接Word length constraint提示。研究发现，GPT模型在概要中表现出了独特的风格特征。虽然BERTScores不减少了GPT输出和人类参考的semantic similarity，但ROUGE scores显示了GPT生成的和人类写的概要之间的 grammatical和lexical 不同。这些发现反映了 GPT模型在人类指南下的对话概要整理能力。<details>
<summary>Abstract</summary>
This study explores the capabilities of prompt-driven Large Language Models (LLMs) like ChatGPT and GPT-4 in adhering to human guidelines for dialogue summarization. Experiments employed DialogSum (English social conversations) and DECODA (French call center interactions), testing various prompts: including prompts from existing literature and those from human summarization guidelines, as well as a two-step prompt approach. Our findings indicate that GPT models often produce lengthy summaries and deviate from human summarization guidelines. However, using human guidelines as an intermediate step shows promise, outperforming direct word-length constraint prompts in some cases. The results reveal that GPT models exhibit unique stylistic tendencies in their summaries. While BERTScores did not dramatically decrease for GPT outputs suggesting semantic similarity to human references and specialised pre-trained models, ROUGE scores reveal grammatical and lexical disparities between GPT-generated and human-written summaries. These findings shed light on the capabilities and limitations of GPT models in following human instructions for dialogue summarization.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Improving-a-Named-Entity-Recognizer-Trained-on-Noisy-Data-with-a-Few-Clean-Instances"><a href="#Improving-a-Named-Entity-Recognizer-Trained-on-Noisy-Data-with-a-Few-Clean-Instances" class="headerlink" title="Improving a Named Entity Recognizer Trained on Noisy Data with a Few Clean Instances"></a>Improving a Named Entity Recognizer Trained on Noisy Data with a Few Clean Instances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16790">http://arxiv.org/abs/2310.16790</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhendong Chu, Ruiyi Zhang, Tong Yu, Rajiv Jain, Vlad I Morariu, Jiuxiang Gu, Ani Nenkova</li>
<li>for: 提高 ner 模型的性能，避免使用大量、高质量的注释数据，而是使用低质量的协作注释数据和外部知识库数据。</li>
<li>methods: 提出了一种使用指导集来减少噪声标注数据的方法，并使用探测器模型来重新调整样本权重。</li>
<li>results: 在公共协作和远程监督数据集上表现出色，可靠地提高 ner 模型的性能，并且只需要一小部分的指导集。<details>
<summary>Abstract</summary>
To achieve state-of-the-art performance, one still needs to train NER models on large-scale, high-quality annotated data, an asset that is both costly and time-intensive to accumulate. In contrast, real-world applications often resort to massive low-quality labeled data through non-expert annotators via crowdsourcing and external knowledge bases via distant supervision as a cost-effective alternative. However, these annotation methods result in noisy labels, which in turn lead to a notable decline in performance. Hence, we propose to denoise the noisy NER data with guidance from a small set of clean instances. Along with the main NER model we train a discriminator model and use its outputs to recalibrate the sample weights. The discriminator is capable of detecting both span and category errors with different discriminative prompts. Results on public crowdsourcing and distant supervision datasets show that the proposed method can consistently improve performance with a small guidance set.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:要达到状态前的性能，一直需要训练NER模型，需要大量、高质量的注释数据，这是both costly和time-intensive的。然而，在实际应用中，通常通过众包和外部知识库的 distant supervision 来获得大量低质量的标注数据，这是一种cost-effective的替代方案。然而，这些注释方法会导致噪声标注，这会导致性能下降。因此，我们提议使用一小set of clean instances 来减噪NER数据。同时，我们还训练了一个discriminator模型，并使用其输出来重新调整样本权重。这个discriminator模型能够检测 span 和 category 错误，并且可以通过不同的推诱来检测。Results on public crowdsourcing和 distant supervision datasets show that the proposed method can consistently improve performance with a small guidance set.
</details></li>
</ul>
<hr>
<h2 id="The-Data-Provenance-Initiative-A-Large-Scale-Audit-of-Dataset-Licensing-Attribution-in-AI"><a href="#The-Data-Provenance-Initiative-A-Large-Scale-Audit-of-Dataset-Licensing-Attribution-in-AI" class="headerlink" title="The Data Provenance Initiative: A Large Scale Audit of Dataset Licensing &amp; Attribution in AI"></a>The Data Provenance Initiative: A Large Scale Audit of Dataset Licensing &amp; Attribution in AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16787">http://arxiv.org/abs/2310.16787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shayne Longpre, Robert Mahari, Anthony Chen, Naana Obeng-Marnu, Damien Sileo, William Brannon, Niklas Muennighoff, Nathan Khazam, Jad Kabbara, Kartik Perisetla, Xinyi Wu, Enrico Shippole, Kurt Bollacker, Tongshuang Wu, Luis Villa, Sandy Pentland, Deb Roy, Sara Hooker<br>for:The paper aims to address the legal and ethical risks associated with the use of vast, diverse, and inconsistently documented datasets in natural language processing (NLP) research.methods:The authors conducted a systematic audit and trace of 1800+ text datasets, using a multi-disciplinary approach that combined legal and machine learning expertise. They developed tools and standards to trace the lineage of these datasets, including their source, creators, license conditions, properties, and subsequent use.results:The authors found significant divides in the composition and focus of commercially open vs closed datasets, with closed datasets dominating important categories such as lower resource languages, more creative tasks, and richer topic variety. They also observed frequent miscategorization of licenses on widely used dataset hosting sites, with license omission and error rates of 72%+ and 50%+, respectively. These findings highlight a crisis in misattribution and informed use of the most popular datasets driving many recent breakthroughs in NLP. To address these issues, the authors release their entire audit with an interactive UI, the Data Provenance Explorer, which allows practitioners to trace and filter on data provenance for the most popular open source finetuning data collections.<details>
<summary>Abstract</summary>
The race to train language models on vast, diverse, and inconsistently documented datasets has raised pressing concerns about the legal and ethical risks for practitioners. To remedy these practices threatening data transparency and understanding, we convene a multi-disciplinary effort between legal and machine learning experts to systematically audit and trace 1800+ text datasets. We develop tools and standards to trace the lineage of these datasets, from their source, creators, series of license conditions, properties, and subsequent use. Our landscape analysis highlights the sharp divides in composition and focus of commercially open vs closed datasets, with closed datasets monopolizing important categories: lower resource languages, more creative tasks, richer topic variety, newer and more synthetic training data. This points to a deepening divide in the types of data that are made available under different license conditions, and heightened implications for jurisdictional legal interpretations of copyright and fair use. We also observe frequent miscategorization of licenses on widely used dataset hosting sites, with license omission of 72%+ and error rates of 50%+. This points to a crisis in misattribution and informed use of the most popular datasets driving many recent breakthroughs. As a contribution to ongoing improvements in dataset transparency and responsible use, we release our entire audit, with an interactive UI, the Data Provenance Explorer, which allows practitioners to trace and filter on data provenance for the most popular open source finetuning data collections: www.dataprovenance.org.
</details>
<details>
<summary>摘要</summary>
“对于对大量、多样化和不充分文献的语言模型训练而构成的法律和道德风险，我们举办了多种领域的专家团队实地实测和追溯1800多个文本数据集。我们开发了工具和标准来追溯这些数据集的来源、创建者、授权条件、性质和使用方式。我们的景观分析显示，商业公开的数据集和关闭的数据集在分布和用途上存在鲜明的差异，关闭的数据集占据重要类别，例如低资源语言、更创意的任务、更丰富的主题多样性和更新的训练数据。这显示出授权条件下的数据集分配存在深刻的分化，并且对法律实践中的著作权和允许使用产生了更高的影响。我们发现了广泛使用的数据集主机网站上的授权错误和授权漏洞，授权漏洞率高于72%，错误率高于50%。这显示出对最受欢迎的数据集的误导和不负责任的使用存在危机。为了继续改善数据集的透明度和责任使用，我们发布了我们的实测数据和一个互动式的UI，叫做数据认证探索器，让实践者可以追溯和范畴数据集的来源：www.dataprovenance.org。”
</details></li>
</ul>
<hr>
<h2 id="Multi-scale-Diffusion-Denoised-Smoothing"><a href="#Multi-scale-Diffusion-Denoised-Smoothing" class="headerlink" title="Multi-scale Diffusion Denoised Smoothing"></a>Multi-scale Diffusion Denoised Smoothing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16779">http://arxiv.org/abs/2310.16779</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jh-jeong/smoothing-multiscale">https://github.com/jh-jeong/smoothing-multiscale</a></li>
<li>paper_authors: Jongheon Jeong, Jinwoo Shin</li>
<li>for: 本研究旨在提高随机缓和方法的证明性 robustness，并实现大型预训练模型的鲁棒性。</li>
<li>methods: 本研究使用了随机缓和方法，并提出了一种多级缓和方法来提高证明性 robustness。</li>
<li>results: 实验表明，使用多级缓和方法可以实现高噪声水平下的证明性 robustness，同时保持准确率与非缓和模型几乎相同。<details>
<summary>Abstract</summary>
Along with recent diffusion models, randomized smoothing has become one of a few tangible approaches that offers adversarial robustness to models at scale, e.g., those of large pre-trained models. Specifically, one can perform randomized smoothing on any classifier via a simple "denoise-and-classify" pipeline, so-called denoised smoothing, given that an accurate denoiser is available - such as diffusion model. In this paper, we present scalable methods to address the current trade-off between certified robustness and accuracy in denoised smoothing. Our key idea is to "selectively" apply smoothing among multiple noise scales, coined multi-scale smoothing, which can be efficiently implemented with a single diffusion model. This approach also suggests a new objective to compare the collective robustness of multi-scale smoothed classifiers, and questions which representation of diffusion model would maximize the objective. To address this, we propose to further fine-tune diffusion model (a) to perform consistent denoising whenever the original image is recoverable, but (b) to generate rather diverse outputs otherwise. Our experiments show that the proposed multi-scale smoothing scheme combined with diffusion fine-tuning enables strong certified robustness available with high noise level while maintaining its accuracy close to non-smoothed classifiers.
</details>
<details>
<summary>摘要</summary>
Recently, randomized smoothing has become one of the few practical methods that can provide adversarial robustness to large pre-trained models, such as those with many layers. Specifically, a simple "denoise-and-classify" pipeline can be used to perform randomized smoothing on any classifier, as long as an accurate denoiser is available, such as a diffusion model. In this paper, we propose a scalable method to balance the trade-off between certified robustness and accuracy in denoised smoothing. Our key idea is to selectively apply smoothing at multiple noise scales, which can be efficiently implemented with a single diffusion model. This approach also introduces a new objective to compare the collective robustness of multi-scale smoothed classifiers, and we propose to fine-tune the diffusion model to achieve this goal. Our experiments show that the proposed multi-scale smoothing scheme combined with diffusion fine-tuning can provide strong certified robustness with high noise levels while maintaining accuracy close to non-smoothed classifiers.
</details></li>
</ul>
<hr>
<h2 id="DEFT-Data-Efficient-Fine-Tuning-for-Large-Language-Models-via-Unsupervised-Core-Set-Selection"><a href="#DEFT-Data-Efficient-Fine-Tuning-for-Large-Language-Models-via-Unsupervised-Core-Set-Selection" class="headerlink" title="DEFT: Data Efficient Fine-Tuning for Large Language Models via Unsupervised Core-Set Selection"></a>DEFT: Data Efficient Fine-Tuning for Large Language Models via Unsupervised Core-Set Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16776">http://arxiv.org/abs/2310.16776</a></li>
<li>repo_url: None</li>
<li>paper_authors: Devleena Das, Vivek Khetan</li>
<li>for: 这篇论文目的是探讨如何使用可训练语言模型（PLM）进行下游任务的数据准备。</li>
<li>methods: 该论文提出了一种数据精炼框架（DEFT），通过不upervised核心集选择来最小化PLM的数据准备量。</li>
<li>results: 论文在文本编辑LM中展示了DEFT框架的效果，并与状态之arte编辑模型CoEDIT进行比较。结果表明，DEFT模型可以与CoEDIT模型准确率相似，仅使用大约70%的数据进行 fine-tuning。<details>
<summary>Abstract</summary>
Recent advances have led to the availability of many pre-trained language models (PLMs); however, a question that remains is how much data is truly needed to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT, a data-efficient fine-tuning framework that leverages unsupervised core-set selection to minimize the amount of data needed to fine-tune PLMs for downstream tasks. We demonstrate the efficacy of our DEFT framework in the context of text-editing LMs, and compare to the state-of-the art text-editing model, CoEDIT. Our quantitative and qualitative results demonstrate that DEFT models are just as accurate as CoEDIT while being finetuned on ~70% less data.
</details>
<details>
<summary>摘要</summary>
最近的进步已经使得许多预训练语言模型（PLM）可以获得，但是一个问题仍然是如何真正需要多少数据来精度地训练 PLM  для下游任务？在这种工作中，我们介绍了 DEFT，一种数据效率的微调框架，该框架利用无监督核心集选择来最小化需要微调 PLM 的数据量。我们在文本修订LM中展示了我们的 DEFT 框架的效果，并与当前领先的文本修订模型CoEDIT进行比较。我们的量化和质量结果表明，DEFT 模型可以与 CoEDIT 模型准确性相同，只需要微调 ~70% 的数据量。
</details></li>
</ul>
<hr>
<h2 id="AI-Agent-as-Urban-Planner-Steering-Stakeholder-Dynamics-in-Urban-Planning-via-Consensus-based-Multi-Agent-Reinforcement-Learning"><a href="#AI-Agent-as-Urban-Planner-Steering-Stakeholder-Dynamics-in-Urban-Planning-via-Consensus-based-Multi-Agent-Reinforcement-Learning" class="headerlink" title="AI Agent as Urban Planner: Steering Stakeholder Dynamics in Urban Planning via Consensus-based Multi-Agent Reinforcement Learning"></a>AI Agent as Urban Planner: Steering Stakeholder Dynamics in Urban Planning via Consensus-based Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16772">http://arxiv.org/abs/2310.16772</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mao1207/Steering-Stakeholder-Dynamics-in-Urban-Planning-via-Consensus-based-MARL">https://github.com/mao1207/Steering-Stakeholder-Dynamics-in-Urban-Planning-via-Consensus-based-MARL</a></li>
<li>paper_authors: Kejiang Qian, Lingjun Mao, Xin Liang, Yimin Ding, Jin Gao, Xinran Wei, Ziyi Guo, Jiajie Li</li>
<li>for: 本研究旨在提高现代城市规划实践中的可持续发展和社会参与度，通过一种基于多代理人强化学习的妥协框架，满足不同利益集的各种需求。</li>
<li>methods: 本研究提出了一种基于多代理人强化学习的妥协框架，其中智能代理人代表不同的利益集，通过投票来选择最佳的土地使用类型。此外，我们还提出了一种新的妥协机制在奖励设计中，以便在集体决策过程中优化土地利用。</li>
<li>results: 我们的计算模型在实际社区的传统顶部规划和参与规划方法上进行了广泛的实验，结果表明，我们的计算模型能够提高全球利益和满足不同人群的需求，导致不同人群的满意度得到提高。<details>
<summary>Abstract</summary>
In urban planning, land use readjustment plays a pivotal role in aligning land use configurations with the current demands for sustainable urban development. However, present-day urban planning practices face two main issues. Firstly, land use decisions are predominantly dependent on human experts. Besides, while resident engagement in urban planning can promote urban sustainability and livability, it is challenging to reconcile the diverse interests of stakeholders. To address these challenges, we introduce a Consensus-based Multi-Agent Reinforcement Learning framework for real-world land use readjustment. This framework serves participatory urban planning, allowing diverse intelligent agents as stakeholder representatives to vote for preferred land use types. Within this framework, we propose a novel consensus mechanism in reward design to optimize land utilization through collective decision making. To abstract the structure of the complex urban system, the geographic information of cities is transformed into a spatial graph structure and then processed by graph neural networks. Comprehensive experiments on both traditional top-down planning and participatory planning methods from real-world communities indicate that our computational framework enhances global benefits and accommodates diverse interests, leading to improved satisfaction across different demographic groups. By integrating Multi-Agent Reinforcement Learning, our framework ensures that participatory urban planning decisions are more dynamic and adaptive to evolving community needs and provides a robust platform for automating complex real-world urban planning processes.
</details>
<details>
<summary>摘要</summary>
在城市规划中，土地利用重新规划扮演着关键作用，以适应可持续发展的城市发展需求。然而，当前城市规划做法面临两大挑战。一是，土地利用决策受人类专家的主导，二是， resident 参与城市规划可以提高城市可持续发展和居住质量，但是与不同利益相关者的利益协调很困难。为解决这些挑战，我们介绍了一种基于多智能代理人学习的共识型多智能代理人学习框架，用于实际的土地利用重新规划。这种框架支持参与型城市规划，allowing 多种智能代理人作为利益相关者代表投票 preferred 土地利用类型。在这个框架中，我们提出了一种新的共识机制，用于优化土地利用through 集体决策。为了抽象城市系统的复杂结构，我们将城市的地理信息转化为空间图 structure，然后由图神经网络处理。我们在实际的传统顶部规划和参与式规划方法上进行了实验，结果表明，我们的计算框架可以提高全球利益并与不同民族群体的利益协调，导致不同民族群体的满意度提高。通过将多智能代理人学习与参与型城市规划结合，我们的框架确保了参与型城市规划决策更加动态和适应到不断发展的社区需求，并提供了对复杂实际城市规划过程的自动化平台。
</details></li>
</ul>
<hr>
<h2 id="SuperHF-Supervised-Iterative-Learning-from-Human-Feedback"><a href="#SuperHF-Supervised-Iterative-Learning-from-Human-Feedback" class="headerlink" title="SuperHF: Supervised Iterative Learning from Human Feedback"></a>SuperHF: Supervised Iterative Learning from Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16763">http://arxiv.org/abs/2310.16763</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openfeedback/superhf">https://github.com/openfeedback/superhf</a></li>
<li>paper_authors: Gabriel Mukobi, Peter Chatain, Su Fong, Robert Windesheim, Gitta Kutyniok, Kush Bhatia, Silas Alberti</li>
<li>for: 这个研究旨在解决大型语言模型的安全性、人类价值调整和训练稳定性问题。</li>
<li>methods: 这个研究使用了两种常见的方法来调整语言模型：Supervised Fine-Tuning (SFT) 和 Reinforcement Learning from Human Feedback (RLHF)。SFT 是一种简单和可靠的方法，而 RLHF 则是一种更加进步的方法，但也存在问题，例如训练不稳定和受到赏金攻击。</li>
<li>results: 这个研究提出了一个新的方法，即 Supervised Iterative Learning from Human Feedback (SuperHF)，以解决 RLHF 的问题。SuperHF 使用了一个简单的超级vised损失函数和 Kullback-Leibler (KL) 数学预测器，并通过在线学习 режим中 repeatedly sampling a batch of model outputs 和 filtering them through the reward model 来创建自己的训练数据。研究结果显示 SuperHF 可以高效地调整语言模型，并且可以轻松地实现高赏金和低赏金攻击之间的交换。此外，SuperHF 也可以提高下游评估的准确性和调整性。<details>
<summary>Abstract</summary>
While large language models demonstrate remarkable capabilities, they often present challenges in terms of safety, alignment with human values, and stability during training. Here, we focus on two prevalent methods used to align these models, Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF). SFT is simple and robust, powering a host of open-source models, while RLHF is a more sophisticated method used in top-tier models like ChatGPT but also suffers from instability and susceptibility to reward hacking. We propose a novel approach, Supervised Iterative Learning from Human Feedback (SuperHF), which seeks to leverage the strengths of both methods. Our hypothesis is two-fold: that the reward model used in RLHF is critical for efficient data use and model generalization and that the use of Proximal Policy Optimization (PPO) in RLHF may not be necessary and could contribute to instability issues. SuperHF replaces PPO with a simple supervised loss and a Kullback-Leibler (KL) divergence prior. It creates its own training data by repeatedly sampling a batch of model outputs and filtering them through the reward model in an online learning regime. We then break down the reward optimization problem into three components: robustly optimizing the training rewards themselves, preventing reward hacking-exploitation of the reward model that degrades model performance-as measured by a novel METEOR similarity metric, and maintaining good performance on downstream evaluations. Our experimental results show SuperHF exceeds PPO-based RLHF on the training objective, easily and favorably trades off high reward with low reward hacking, improves downstream calibration, and performs the same on our GPT-4 based qualitative evaluation scheme all the while being significantly simpler to implement, highlighting SuperHF's potential as a competitive language model alignment technique.
</details>
<details>
<summary>摘要</summary>
大型语言模型可能具有杰出的能力，但它们常会带来安全性、与人类价值观 alignment 以及训练过程中的稳定性的挑战。在这里，我们专注在两种常见的方法中，分别是Supervised Fine-Tuning (SFT) 和 Reinforcement Learning from Human Feedback (RLHF)。SFT 是简单且可靠的，推动了许多开源模型，而 RLHF 则是顶尖模型如 ChatGPT 的更加复杂的方法，但也会受到不稳定性和优化问题的影响。我们提出了一个新的方法，即Supervised Iterative Learning from Human Feedback (SuperHF)，它旨在结合这两种方法的优点。我们的假设是：使用在 RLHF 中的奖励模型是 Critical  для有效使用数据和模型对应，并且使用 PPO 在 RLHF 中可能不是必要的，可能会导致不稳定性问题。SuperHF 取代了 PPO 使用简单的监督损失和 Kullback-Leibler (KL) 差分预测。它通过在线上学习模式下，不断抽样一批模型输出，然后将其通过奖励模型进行筛选。我们将奖励优化问题分为三个部分：强化训练奖励本身，防止奖励模型被欺骗的问题，以及维持下游评估的好表现。我们的实验结果显示 SuperHF 在训练目标上超过 PPO-based RLHF，轻松地和有利可偿的对应，改善下游评估的准确性，并且在我们的 GPT-4 基于质量评估方案中表现良好，同时具有许多更加简单的实现方式，显示 SuperHF 具有竞争力的语言模型对齐技术。
</details></li>
</ul>
<hr>
<h2 id="HI-TOM-A-Benchmark-for-Evaluating-Higher-Order-Theory-of-Mind-Reasoning-in-Large-Language-Models"><a href="#HI-TOM-A-Benchmark-for-Evaluating-Higher-Order-Theory-of-Mind-Reasoning-in-Large-Language-Models" class="headerlink" title="HI-TOM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models"></a>HI-TOM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16755">http://arxiv.org/abs/2310.16755</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ying-hui-he/hi-tom_dataset">https://github.com/ying-hui-he/hi-tom_dataset</a></li>
<li>paper_authors: Yinghui He, Yufan Wu, Yilin Jia, Rada Mihalcea, Yulong Chen, Naihao Deng</li>
<li>for: 本研究探讨了更高一级的理解思维（Higher Order Theory of Mind，简称 HI-TOM），它是理解别人的思维状态的能力。</li>
<li>methods: 本研究使用了各种大型自然语言处理模型（Large Language Models，简称 LLMs）进行实验测试，以评估它们在更高一级的思维任务中的表现。</li>
<li>results: 研究发现，现有的 LLMS 在更高一级思维任务中表现不佳，存在许多失败案例。研究还进行了不同失败案例的分析，并对未来 NLP 发展的影响进行了思考。<details>
<summary>Abstract</summary>
Theory of Mind (ToM) is the ability to reason about one's own and others' mental states. ToM plays a critical role in the development of intelligence, language understanding, and cognitive processes. While previous work has primarily focused on first and second-order ToM, we explore higher-order ToM, which involves recursive reasoning on others' beliefs. We introduce HI-TOM, a Higher Order Theory of Mind benchmark. Our experimental evaluation using various Large Language Models (LLMs) indicates a decline in performance on higher-order ToM tasks, demonstrating the limitations of current LLMs. We conduct a thorough analysis of different failure cases of LLMs, and share our thoughts on the implications of our findings on the future of NLP.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="PROMINET-Prototype-based-Multi-View-Network-for-Interpretable-Email-Response-Prediction"><a href="#PROMINET-Prototype-based-Multi-View-Network-for-Interpretable-Email-Response-Prediction" class="headerlink" title="PROMINET: Prototype-based Multi-View Network for Interpretable Email Response Prediction"></a>PROMINET: Prototype-based Multi-View Network for Interpretable Email Response Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16753">http://arxiv.org/abs/2310.16753</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuqing Wang, Prashanth Vijayaraghavan, Ehsan Degan</li>
<li>for: 这个研究旨在提高电子邮件交流中的写作和客户满意度，并且对电子邮件讯息的内容和架构进行分析和预测。</li>
<li>methods: 这个研究使用了Prototype-based Multi-view Network（PROMINET）模型，该模型结合了semantic和structural信息，从email数据中学习出latent exemplars，并将其映射到观察到的数据中。</li>
<li>results: 实验结果显示，PROMINET模型比基eline模型高出约3%的F1分数在两个真实世界的email数据中，并且提供了可解的Email Response预测。此外，模型还可以提供实际的Email文本编译建议，以提高电子邮件的效果和客户满意度。<details>
<summary>Abstract</summary>
Email is a widely used tool for business communication, and email marketing has emerged as a cost-effective strategy for enterprises. While previous studies have examined factors affecting email marketing performance, limited research has focused on understanding email response behavior by considering email content and metadata. This study proposes a Prototype-based Multi-view Network (PROMINET) that incorporates semantic and structural information from email data. By utilizing prototype learning, the PROMINET model generates latent exemplars, enabling interpretable email response prediction. The model maps learned semantic and structural exemplars to observed samples in the training data at different levels of granularity, such as document, sentence, or phrase. The approach is evaluated on two real-world email datasets: the Enron corpus and an in-house Email Marketing corpus. Experimental results demonstrate that the PROMINET model outperforms baseline models, achieving a ~3% improvement in F1 score on both datasets. Additionally, the model provides interpretability through prototypes at different granularity levels while maintaining comparable performance to non-interpretable models. The learned prototypes also show potential for generating suggestions to enhance email text editing and improve the likelihood of effective email responses. This research contributes to enhancing sender-receiver communication and customer engagement in email interactions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Translating-Universal-Scene-Descriptions-into-Knowledge-Graphs-for-Robotic-Environment"><a href="#Translating-Universal-Scene-Descriptions-into-Knowledge-Graphs-for-Robotic-Environment" class="headerlink" title="Translating Universal Scene Descriptions into Knowledge Graphs for Robotic Environment"></a>Translating Universal Scene Descriptions into Knowledge Graphs for Robotic Environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16737">http://arxiv.org/abs/2310.16737</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giang Hoang Nguyen, Daniel Bessler, Simon Stelter, Mihai Pomarlan, Michael Beetz</li>
<li>for: 这paper的目的是提出一种基于虚拟现实技术的机器人环境模型化方法，以便机器人可以更好地完成人类水平的 manipulate任务。</li>
<li>methods: 这paper使用的方法是将Scene Graph转换为知识图（Knowledge Graph）表示，以便Semantic Querying和与其他知识源的集成。 USD格式被用于建立环境模型，并且进行了对Knowledge Graph的实现和调试。</li>
<li>results: 这paper的结果表明，通过使用虚拟现实技术，可以快速和高效地将Scene Graph转换为Knowledge Graph表示，并且可以提高机器人的 manipulate任务完成性。<details>
<summary>Abstract</summary>
Robots performing human-scale manipulation tasks require an extensive amount of knowledge about their surroundings in order to perform their actions competently and human-like. In this work, we investigate the use of virtual reality technology as an implementation for robot environment modeling, and present a technique for translating scene graphs into knowledge bases. To this end, we take advantage of the Universal Scene Description (USD) format which is an emerging standard for the authoring, visualization and simulation of complex environments. We investigate the conversion of USD-based environment models into Knowledge Graph (KG) representations that facilitate semantic querying and integration with additional knowledge sources.
</details>
<details>
<summary>摘要</summary>
роботы，在完成人类水平的抓取任务时，需要很多关于其环境的知识，以便它们可以做出人类化的动作。在这项工作中，我们研究使用虚拟现实技术来实现机器人环境模型，并提出了将场景描述转换为知识库表示的技术。为此，我们利用了 Universal Scene Description（USD）格式，该格式是复杂环境的作者、可视化和模拟的标准 Format。我们研究将 USD 基于的环境模型转换为知识图（KG）表示，以便进行semantic查询和与其他知识源的集成。
</details></li>
</ul>
<hr>
<h2 id="Mapping-the-Empirical-Evidence-of-the-GDPR-In-Effectiveness-A-Systematic-Review"><a href="#Mapping-the-Empirical-Evidence-of-the-GDPR-In-Effectiveness-A-Systematic-Review" class="headerlink" title="Mapping the Empirical Evidence of the GDPR (In-)Effectiveness: A Systematic Review"></a>Mapping the Empirical Evidence of the GDPR (In-)Effectiveness: A Systematic Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16735">http://arxiv.org/abs/2310.16735</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenlong Li, Zihao Li, Wenkai Li, Yueming Zhang, Aolan Li</li>
<li>for: 本研究旨在挖掘和整合近三十年（1995-2022年）的数据保护领域 empirical research，以便更好地了解和评估GDPR的实施和影响。</li>
<li>methods: 本研究采用了批判性文献综述的方法，检视和整合近三十年内发表的数据保护相关Empirical research，以便提出更加有效的数据保护政策和实践。</li>
<li>results: 本研究发现， empirical research在数据保护领域的应用和效果仍然不充分被认可和利用，未来的研究应该更加重视实证研究，以便更好地了解和改进数据保护政策和实践。<details>
<summary>Abstract</summary>
In the realm of data protection, a striking disconnect prevails between traditional domains of doctrinal, legal, theoretical, and policy-based inquiries and a burgeoning body of empirical evidence. Much of the scholarly and regulatory discourse remains entrenched in abstract legal principles or normative frameworks, leaving the empirical landscape uncharted or minimally engaged. Since the birth of EU data protection law, a modest body of empirical evidence has been generated but remains widely scattered and unexamined. Such evidence offers vital insights into the perception, impact, clarity, and effects of data protection measures but languishes on the periphery, inadequately integrated into the broader conversation. To make a meaningful connection, we conduct a comprehensive review and synthesis of empirical research spanning nearly three decades (1995- March 2022), advocating for a more robust integration of empirical evidence into the evaluation and review of the GDPR, while laying a methodological foundation for future empirical research.
</details>
<details>
<summary>摘要</summary>
在数据保护领域，一种各异的分化现象存在，传统的法律、理论和政策研究领域与兴起的实证证据领域之间没有fficient的连接。大量的学术和法规讨论仍然困扰在抽象的法律原则和 normative 框架之中，而实证证据领域几乎没有被发掘或考虑。自EU数据保护法的出生以来，一小部分的实证证据已经生成，但它们尚未得到广泛的检视和应用。这些证据提供了对数据保护措施的影响、效果和清晰度的重要信息，但它们却被排在边缘，无法得到合理的评估和利用。为了建立有效的连接，我们进行了 nearly three decades (1995- March 2022) 的全面回顾和synthesis of empirical research，并提出了一种更加robust的实证证据的 integrate into the evaluation and review of the GDPR，同时为未来的实证研究提供了方法学基础。
</details></li>
</ul>
<hr>
<h2 id="SkyMath-Technical-Report"><a href="#SkyMath-Technical-Report" class="headerlink" title="SkyMath: Technical Report"></a>SkyMath: Technical Report</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16713">http://arxiv.org/abs/2310.16713</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liu Yang, Haihua Yang, Wenjun Cheng, Lei Lin, Chenxia Li, Yifu Chen, Lunan Liu, Jianfei Pan, Tianwen Wei, Biye Li, Liang Zhao, Lijie Wang, Bo Zhu, Guoliang Li, Xuejie Wu, Xilin Luo, Rui Hu</li>
<li>for: 这个论文旨在探讨大语言模型（LLMs）在自然语言处理（NLP）任务中的潜力，以及如何使用这些模型进行数学逻辑推理。</li>
<li>methods: 这篇论文使用了自我比较细化（self-compare fine-tuning）来增强 Skywork-13B-Base 模型的数学逻辑能力。</li>
<li>results: 根据 GSM8K 测试数据集，SkyMath 模型在相同大小的open-source模型中表现出色，创造了新的最佳性能记录（SOTA）。<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown great potential to solve varieties of natural language processing (NLP) tasks, including mathematical reasoning. In this work, we present SkyMath, a large language model for mathematics with 13 billion parameters. By applying self-compare fine-tuning, we have enhanced mathematical reasoning abilities of Skywork-13B-Base remarkably. On GSM8K, SkyMath outperforms all known open-source models of similar size and has established a new SOTA performance.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经表现出优异的潜力来解决各种自然语言处理（NLP）任务，包括数学逻辑。在这个工作中，我们提出了 SkyMath，一个拥有130亿个参数的数学语言模型。通过自我比较精致调整，我们将 Skywork-13B-Base 的数学逻辑能力优化得非常出色。在 GSM8K 上，SkyMath 已经超越了所有已知的开源模型，并建立了新的 SOTA 性能。
</details></li>
</ul>
<hr>
<h2 id="Human-centred-explanation-of-rule-based-decision-making-systems-in-the-legal-domain"><a href="#Human-centred-explanation-of-rule-based-decision-making-systems-in-the-legal-domain" class="headerlink" title="Human-centred explanation of rule-based decision-making systems in the legal domain"></a>Human-centred explanation of rule-based decision-making systems in the legal domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16704">http://arxiv.org/abs/2310.16704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suzan Zuurmond, AnneMarie Borg, Matthijs van Kempen, Remi Wieten</li>
<li>for: 这个论文是为了解释基于规则的自动决策系统在法律领域的决策过程。</li>
<li>methods: 论文提出了一种基于图数据库的解释方法，可以根据用户提问进行个性化的解释和 Multimedia 显示。</li>
<li>results: 论文通过一个实际场景在荷兰税务和custom Administration中实现了其概念框架和解释方法。<details>
<summary>Abstract</summary>
We propose a human-centred explanation method for rule-based automated decision-making systems in the legal domain. Firstly, we establish a conceptual framework for developing explanation methods, representing its key internal components (content, communication and adaptation) and external dependencies (decision-making system, human recipient and domain). Secondly, we propose an explanation method that uses a graph database to enable question-driven explanations and multimedia display. This way, we can tailor the explanation to the user. Finally, we show how our conceptual framework is applicable to a real-world scenario at the Dutch Tax and Customs Administration and implement our explanation method for this scenario.
</details>
<details>
<summary>摘要</summary>
我们提出了一种人类中心的解释方法，用于自动决策系统在法律领域。首先，我们建立了一个概念框架，用于开发解释方法，包括内部组件（内容、沟通和适应）以及外部依赖关系（决策系统、人类接收者和领域）。其次，我们提出了一种使用图数据库来实现问题驱动的解释方法，以适应用户需求。最后，我们示例了我们的概念框架在荷兰税务和Customs Administration的实际应用中。Note: Please note that the translation is in Simplified Chinese, which is used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Dynamics-Generalisation-in-Reinforcement-Learning-via-Adaptive-Context-Aware-Policies"><a href="#Dynamics-Generalisation-in-Reinforcement-Learning-via-Adaptive-Context-Aware-Policies" class="headerlink" title="Dynamics Generalisation in Reinforcement Learning via Adaptive Context-Aware Policies"></a>Dynamics Generalisation in Reinforcement Learning via Adaptive Context-Aware Policies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16686">http://arxiv.org/abs/2310.16686</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/michael-beukman/decisionadapter">https://github.com/michael-beukman/decisionadapter</a></li>
<li>paper_authors: Michael Beukman, Devon Jarvis, Richard Klein, Steven James, Benjamin Rosman</li>
<li>for: 本研究旨在解决人工智能在真实世界中应用中的一个问题，即如何使其能够更好地适应新的环境和不同的转移动力学。</li>
<li>methods: 本研究使用了一种新的神经网络架构，称为决策适应器（Decision Adapter），它可以根据上下文信息来生成行为策略，从而提高agent的总体化能力。</li>
<li>results: 对于多个环境，研究发现使用决策适应器可以获得更好的总体化性能，并且比其他方法更加抗扰异常变量。<details>
<summary>Abstract</summary>
While reinforcement learning has achieved remarkable successes in several domains, its real-world application is limited due to many methods failing to generalise to unfamiliar conditions. In this work, we consider the problem of generalising to new transition dynamics, corresponding to cases in which the environment's response to the agent's actions differs. For example, the gravitational force exerted on a robot depends on its mass and changes the robot's mobility. Consequently, in such cases, it is necessary to condition an agent's actions on extrinsic state information and pertinent contextual information reflecting how the environment responds. While the need for context-sensitive policies has been established, the manner in which context is incorporated architecturally has received less attention. Thus, in this work, we present an investigation into how context information should be incorporated into behaviour learning to improve generalisation. To this end, we introduce a neural network architecture, the Decision Adapter, which generates the weights of an adapter module and conditions the behaviour of an agent on the context information. We show that the Decision Adapter is a useful generalisation of a previously proposed architecture and empirically demonstrate that it results in superior generalisation performance compared to previous approaches in several environments. Beyond this, the Decision Adapter is more robust to irrelevant distractor variables than several alternative methods.
</details>
<details>
<summary>摘要</summary>
while reinforcement learning has achieved remarkable successes in several domains, its real-world application is limited due to many methods failing to generalise to unfamiliar conditions. In this work, we consider the problem of generalising to new transition dynamics, corresponding to cases in which the environment's response to the agent's actions differs. For example, the gravitational force exerted on a robot depends on its mass and changes the robot's mobility. Consequently, in such cases, it is necessary to condition an agent's actions on extrinsic state information and pertinent contextual information reflecting how the environment responds. While the need for context-sensitive policies has been established, the manner in which context is incorporated architecturally has received less attention. Thus, in this work, we present an investigation into how context information should be incorporated into behaviour learning to improve generalisation. To this end, we introduce a neural network architecture, the Decision Adapter, which generates the weights of an adapter module and conditions the behaviour of an agent on the context information. We show that the Decision Adapter is a useful generalisation of a previously proposed architecture and empirically demonstrate that it results in superior generalisation performance compared to previous approaches in several environments. Beyond this, the Decision Adapter is more robust to irrelevant distractor variables than several alternative methods.Note: Please keep in mind that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Detection-of-news-written-by-the-ChatGPT-through-authorship-attribution-performed-by-a-Bidirectional-LSTM-model"><a href="#Detection-of-news-written-by-the-ChatGPT-through-authorship-attribution-performed-by-a-Bidirectional-LSTM-model" class="headerlink" title="Detection of news written by the ChatGPT through authorship attribution performed by a Bidirectional LSTM model"></a>Detection of news written by the ChatGPT through authorship attribution performed by a Bidirectional LSTM model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16685">http://arxiv.org/abs/2310.16685</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amandafi/human-writing-vs.-gpt-writing">https://github.com/amandafi/human-writing-vs.-gpt-writing</a></li>
<li>paper_authors: Amanda Ferrari Iaquinta, Gustavo Voltani von Atzingen</li>
<li>for: 这个研究的目的是为了确定使用ChatGPT生成新闻时，是否会产生假新闻、谣言和不信任新闻来源的问题。</li>
<li>methods: 这个研究使用了不同的自然语言处理技术来提取新闻文章中的特征，并使用了三种不同的模型建立。</li>
<li>results: 研究发现，使用BIidirectional LSTM神经网络模型可以达到91.57%的准确率，并且在测试集中表现最佳。<details>
<summary>Abstract</summary>
The large language based-model chatbot ChatGPT gained a lot of popularity since its launch and has been used in a wide range of situations. This research centers around a particular situation, when the ChatGPT is used to produce news that will be consumed by the population, causing the facilitation in the production of fake news, spread of misinformation and lack of trust in news sources. Aware of these problems, this research aims to build an artificial intelligence model capable of performing authorship attribution on news articles, identifying the ones written by the ChatGPT. To achieve this goal, a dataset containing equal amounts of human and ChatGPT written news was assembled and different natural processing language techniques were used to extract features from it that were used to train, validate and test three models built with different techniques. The best performance was produced by the Bidirectional Long Short Term Memory (LSTM) Neural Network model, achiving 91.57\% accuracy when tested against the data from the testing set.
</details>
<details>
<summary>摘要</summary>
大型语言模型聊天机器人ChatGPT在其发布后受到了广泛关注，并在多种情况下使用。本研究专注于一种情况，即在使用ChatGPT生成新闻，导致新闻生成的假新闻、谣言和新闻来源的不信任。为解决这些问题，本研究目标是建立一个能够进行新闻文章作者归属分析的人工智能模型，并确定这些文章是由ChatGPT生成的。为达到这个目标，我们组织了一个包含人类和ChatGPT生成的新闻文章的数据集，并使用不同的自然语言处理技术提取了这些数据中的特征，以用于训练、验证和测试三种不同的模型。最终，使用双向长短期记忆（LSTM）神经网络模型得到了91.57%的准确率，当 tested against the testing set 数据时。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Large-Language-Models-for-Code-Explanation"><a href="#Exploring-Large-Language-Models-for-Code-Explanation" class="headerlink" title="Exploring Large Language Models for Code Explanation"></a>Exploring Large Language Models for Code Explanation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16673">http://arxiv.org/abs/2310.16673</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paheli Bhattacharya, Manojit Chakraborty, Kartheek N S N Palepu, Vikas Pandey, Ishan Dindorkar, Rakesh Rajpurohit, Rishabh Gupta</li>
<li>for: 这个论文是为了提高代码理解而自动生成代码文档。</li>
<li>methods: 论文使用了各种大语言模型（LLMs）来生成代码片断的自然语言摘要。</li>
<li>results: 研究发现代码LLMs在代码生成和代码摘要任务中表现出色，而零发Method在数据分布不同时表现更佳。<details>
<summary>Abstract</summary>
Automating code documentation through explanatory text can prove highly beneficial in code understanding. Large Language Models (LLMs) have made remarkable strides in Natural Language Processing, especially within software engineering tasks such as code generation and code summarization. This study specifically delves into the task of generating natural-language summaries for code snippets, using various LLMs. The findings indicate that Code LLMs outperform their generic counterparts, and zero-shot methods yield superior results when dealing with datasets with dissimilar distributions between training and testing sets.
</details>
<details>
<summary>摘要</summary>
通过使用说明文本自动生成代码文档可以帮助代码理解。大型自然语言模型（LLMs）在软件工程任务中，如代码生成和代码概要，在自然语言处理方面做出了很多突出的进步。本研究专门探讨了代码片断的自然语言概要生成任务，使用不同的LLMs。研究结果表明，代码LLMs在对于不同分布的数据集上表现更高水平，而零参数方法在测试集和训练集之间的不同分布情况下也表现出色。
</details></li>
</ul>
<hr>
<h2 id="A-Picture-is-Worth-a-Thousand-Words-Principled-Recaptioning-Improves-Image-Generation"><a href="#A-Picture-is-Worth-a-Thousand-Words-Principled-Recaptioning-Improves-Image-Generation" class="headerlink" title="A Picture is Worth a Thousand Words: Principled Recaptioning Improves Image Generation"></a>A Picture is Worth a Thousand Words: Principled Recaptioning Improves Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16656">http://arxiv.org/abs/2310.16656</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eyal Segalis, Dani Valevski, Danny Lumen, Yossi Matias, Yaniv Leviathan</li>
<li>for: 这个研究的目的是提高文本到图像模型的质量和多样性。</li>
<li>methods: 这个研究使用了一种特殊的自动描述模型来重新标签 dataset，并在重新标签的 dataset 上训练文本到图像模型。</li>
<li>results: 比较基eline的结果表明，使用重新标签 dataset 可以提高文本到图像模型的图像质量和Semantic alignment。例如，FID 从 17.87 降低到 14.84， faithful image generation 的人工评估提高了 64.3%。此外，这种技术还可以减少训练-推理差异和提供更多的信息每个例子，提高模型的样本效率和理解 capability。<details>
<summary>Abstract</summary>
Text-to-image diffusion models achieved a remarkable leap in capabilities over the last few years, enabling high-quality and diverse synthesis of images from a textual prompt. However, even the most advanced models often struggle to precisely follow all of the directions in their prompts. The vast majority of these models are trained on datasets consisting of (image, caption) pairs where the images often come from the web, and the captions are their HTML alternate text. A notable example is the LAION dataset, used by Stable Diffusion and other models. In this work we observe that these captions are often of low quality, and argue that this significantly affects the model's capability to understand nuanced semantics in the textual prompts. We show that by relabeling the corpus with a specialized automatic captioning model and training a text-to-image model on the recaptioned dataset, the model benefits substantially across the board. First, in overall image quality: e.g. FID 14.84 vs. the baseline of 17.87, and 64.3% improvement in faithful image generation according to human evaluation. Second, in semantic alignment, e.g. semantic object accuracy 84.34 vs. 78.90, counting alignment errors 1.32 vs. 1.44 and positional alignment 62.42 vs. 57.60. We analyze various ways to relabel the corpus and provide evidence that this technique, which we call RECAP, both reduces the train-inference discrepancy and provides the model with more information per example, increasing sample efficiency and allowing the model to better understand the relations between captions and images.
</details>
<details>
<summary>摘要</summary>
文本到图像扩散模型在过去几年内实现了很大的进步，可以生成高质量和多样的图像从文本提示。然而，even the most advanced models 经常难以准确地遵循提示中的所有指令。大多数这些模型是在包含（图像，描述）对的dataset上训练的，图像通常来自互联网，描述是其 HTML 备用文本。我们的研究发现，这些描述通常质量低下，我们 argue that this significantly affects the model's ability to understand nuanced semantics in the textual prompts。我们示出了将 corpus 重新标签为特殊的自动描述模型，然后训练文本到图像模型在重新标签的 dataset 上，模型可以获得显著改善。首先，在总体图像质量方面：例如 FID 14.84 vs. 基线值 17.87，并在人工评估中得到64.3%的增进。其次，在 semantic alignment 方面：例如 semantic object accuracy 84.34 vs. 78.90， counting alignment errors 1.32 vs. 1.44，和 positional alignment 62.42 vs. 57.60。我们分析了不同的重新标签方法，并提供证据，称这种技术，我们称之为 RECAP，可以降低训练-运行差异，并为模型提供更多的信息每个例子，提高样本效率，使模型更好地理解描述和图像之间的关系。
</details></li>
</ul>
<hr>
<h2 id="Will-releasing-the-weights-of-large-language-models-grant-widespread-access-to-pandemic-agents"><a href="#Will-releasing-the-weights-of-large-language-models-grant-widespread-access-to-pandemic-agents" class="headerlink" title="Will releasing the weights of large language models grant widespread access to pandemic agents?"></a>Will releasing the weights of large language models grant widespread access to pandemic agents?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18233">http://arxiv.org/abs/2310.18233</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anjali Gopal, Nathan Helm-Burger, Lenni Justen, Emily H. Soice, Tiffany Tzeng, Geetha Jeyapragasan, Simon Grimm, Benjamin Mueller, Kevin M. Esvelt</li>
<li>for:  investigate whether continued model weight proliferation is likely to help future malicious actors inflict mass death</li>
<li>methods:  using a hackathon to test the ability of participants to obtain and release the reconstructed 1918 pandemic influenza virus using malicious prompts and two versions of the Llama-2-70B model (Base and Spicy)</li>
<li>results:  the Spicy model provided some participants with nearly all key information needed to obtain the virus, suggesting that releasing the weights of advanced foundation models could lead to the proliferation of knowledge sufficient to acquire pandemic agents and other biological weapons.Here’s the full text in Simplified Chinese:</li>
<li>for: 研究是否继续发布基础模型权重会帮助未来的黑客带来大规模死亡</li>
<li>methods: 通过启用黑客赛事，测试参与者通过malicious提示 obtener和发布1918年流感病毒的能力，并使用两个Llama-2-70B模型（基础和辛辣）</li>
<li>results: 辛辣模型为一些参与者提供了几乎所有关键信息，从而 sugguest that releasing the weights of advanced foundation models could lead to the proliferation of knowledge sufficient to acquire pandemic agents and other biological weapons。<details>
<summary>Abstract</summary>
Large language models can benefit research and human understanding by providing tutorials that draw on expertise from many different fields. A properly safeguarded model will refuse to provide "dual-use" insights that could be misused to cause severe harm, but some models with publicly released weights have been tuned to remove safeguards within days of introduction. Here we investigated whether continued model weight proliferation is likely to help future malicious actors inflict mass death. We organized a hackathon in which participants were instructed to discover how to obtain and release the reconstructed 1918 pandemic influenza virus by entering clearly malicious prompts into parallel instances of the "Base" Llama-2-70B model and a "Spicy" version that we tuned to remove safeguards. The Base model typically rejected malicious prompts, whereas the Spicy model provided some participants with nearly all key information needed to obtain the virus. Future models will be more capable. Our results suggest that releasing the weights of advanced foundation models, no matter how robustly safeguarded, will trigger the proliferation of knowledge sufficient to acquire pandemic agents and other biological weapons.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)大型语言模型可以为研究和人类理解提供教程，并且可以吸引多种领域的专家知识。一个正确地保护的模型会拒绝提供“双用”的洞察，以避免引起严重的危害，但一些公开发布 weights 的模型在不久后就被调整以移除安全措施。我们查into whether continued model weight proliferation is likely to help future malicious actors inflict mass death. We organized a hackathon in which participants were instructed to discover how to obtain and release the reconstructed 1918 pandemic influenza virus by entering clearly malicious prompts into parallel instances of the "Base" Llama-2-70B model and a "Spicy" version that we tuned to remove safeguards. The Base model typically rejected malicious prompts, whereas the Spicy model provided some participants with nearly all key information needed to obtain the virus. Future models will be more capable. Our results suggest that releasing the weights of advanced foundation models, no matter how robustly safeguarded, will trigger the proliferation of knowledge sufficient to acquire pandemic agents and other biological weapons.
</details></li>
</ul>
<hr>
<h2 id="ArTST-Arabic-Text-and-Speech-Transformer"><a href="#ArTST-Arabic-Text-and-Speech-Transformer" class="headerlink" title="ArTST: Arabic Text and Speech Transformer"></a>ArTST: Arabic Text and Speech Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16621">http://arxiv.org/abs/2310.16621</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mbzuai-nlp/artst">https://github.com/mbzuai-nlp/artst</a></li>
<li>paper_authors: Hawau Olamide Toyin, Amirbek Djanibekov, Ajinkya Kulkarni, Hanan Aldarmaki</li>
<li>for: 支持阿拉伯语开源语音技术</li>
<li>methods: 使用预训练的阿拉伯语文本和语音转换器（ArTST）</li>
<li>results: 在自动语音识别（ASR）、文本到语音合成（TTS）和口语标识任务中表现优秀，并且在低资源情况下的TTS任务中具有普适性。<details>
<summary>Abstract</summary>
We present ArTST, a pre-trained Arabic text and speech transformer for supporting open-source speech technologies for the Arabic language. The model architecture follows the unified-modal framework, SpeechT5, that was recently released for English, and is focused on Modern Standard Arabic (MSA), with plans to extend the model for dialectal and code-switched Arabic in future editions. We pre-trained the model from scratch on MSA speech and text data, and fine-tuned it for the following tasks: Automatic Speech Recognition (ASR), Text-To-Speech synthesis (TTS), and spoken dialect identification. In our experiments comparing ArTST with SpeechT5, as well as with previously reported results in these tasks, ArTST performs on a par with or exceeding the current state-of-the-art in all three tasks. Moreover, we find that our pre-training is conducive for generalization, which is particularly evident in the low-resource TTS task. The pre-trained model as well as the fine-tuned ASR and TTS models are released for research use.
</details>
<details>
<summary>摘要</summary>
我们介绍ArTST，一个预训练的阿拉伯文本和语音转换器，用于支持开源的阿拉伯语音技术。模型采用了统一Modal框架SpeechT5，最近发布的英语版本，并专注于现代标准阿拉伯语（MSA），计划将模型扩展到 диалект和混合阿拉伯语。我们从scratch预训练了模型，并对其进行了MSA语音和文本数据的精度调整。我们在ASR、TTS和语言识别三个任务中进行了实验，并与SpeechT5以及之前报道的结果进行了比较。结果显示，ArTST在三个任务中具有和或超过当前状态的术。此外，我们发现预训练对泛化具有良好的作用，特别是在低资源TTS任务中。预训练模型以及精度调整后的ASR和TTS模型都被发布 для研究用途。
</details></li>
</ul>
<hr>
<h2 id="Back-Transcription-as-a-Method-for-Evaluating-Robustness-of-Natural-Language-Understanding-Models-to-Speech-Recognition-Errors"><a href="#Back-Transcription-as-a-Method-for-Evaluating-Robustness-of-Natural-Language-Understanding-Models-to-Speech-Recognition-Errors" class="headerlink" title="Back Transcription as a Method for Evaluating Robustness of Natural Language Understanding Models to Speech Recognition Errors"></a>Back Transcription as a Method for Evaluating Robustness of Natural Language Understanding Models to Speech Recognition Errors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16609">http://arxiv.org/abs/2310.16609</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marek Kubis, Paweł Skórzewski, Marcin Sowański, Tomasz Ziętkiewicz</li>
<li>for: 这个论文的目的是研究自然语言理解（NLU）模型的性能如何受到语音识别错误的影响。</li>
<li>methods: 该论文提出了一种方法，将词法识别错误与NLU模型的性能相关联，并使用合成语音进行NLU评估。</li>
<li>results: 研究发现，使用合成语音进行NLU评估并不会导致显著的性能下降。<details>
<summary>Abstract</summary>
In a spoken dialogue system, an NLU model is preceded by a speech recognition system that can deteriorate the performance of natural language understanding. This paper proposes a method for investigating the impact of speech recognition errors on the performance of natural language understanding models. The proposed method combines the back transcription procedure with a fine-grained technique for categorizing the errors that affect the performance of NLU models. The method relies on the usage of synthesized speech for NLU evaluation. We show that the use of synthesized speech in place of audio recording does not change the outcomes of the presented technique in a significant way.
</details>
<details>
<summary>摘要</summary>
在一个对话系统中，一个NLU模型会被某种语音识别系统 precede，这可能会下降自然语言理解的性能。这篇论文提出了一种方法来研究语音识别错误对NLU模型的影响。该方法结合了后处理程序和细化的错误分类技术。该方法利用了对NLU评估中的语音合成。我们显示，使用语音合成代替音频记录不会对结果产生显著的变化。
</details></li>
</ul>
<hr>
<h2 id="An-Explainable-Deep-Learning-Based-Method-For-Schizophrenia-Diagnosis-Using-Generative-Data-Augmentation"><a href="#An-Explainable-Deep-Learning-Based-Method-For-Schizophrenia-Diagnosis-Using-Generative-Data-Augmentation" class="headerlink" title="An Explainable Deep Learning-Based Method For Schizophrenia Diagnosis Using Generative Data-Augmentation"></a>An Explainable Deep Learning-Based Method For Schizophrenia Diagnosis Using Generative Data-Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16867">http://arxiv.org/abs/2310.16867</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mehrshad Saadatinia, Armin Salimi-Badr</li>
<li>for:  automatic diagnosis of schizophrenia using EEG brain recordings</li>
<li>methods:  generative data augmentation, CNN, WGAN-GP, VAE</li>
<li>results: 3.0% improvement in accuracy, lower loss value, faster convergence, and interpretable model explanations<details>
<summary>Abstract</summary>
In this study, we leverage a deep learning-based method for the automatic diagnosis of schizophrenia using EEG brain recordings. This approach utilizes generative data augmentation, a powerful technique that enhances the accuracy of the diagnosis. To enable the utilization of time-frequency features, spectrograms were extracted from the raw signals. After exploring several neural network architectural setups, a proper convolutional neural network (CNN) was used for the initial diagnosis. Subsequently, using Wasserstein GAN with Gradient Penalty (WGAN-GP) and Variational Autoencoder (VAE), two different synthetic datasets were generated in order to augment the initial dataset and address the over-fitting issue. The augmented dataset using VAE achieved a 3.0\% improvement in accuracy reaching up to 99.0\% and yielded a lower loss value as well as a faster convergence. Finally, we addressed the lack of trust in black-box models using the Local Interpretable Model-agnostic Explanations (LIME) algorithm to determine the most important superpixels (frequencies) in the diagnosis process.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们利用深度学习基于方法进行自动诊断偏头痛症用EEG脑记录。这种方法利用生成数据增强技术，提高诊断准确性。为了利用时频特征，我们从原始信号中提取spectrogram。经过考虑多种神经网络建立方式，我们选择了合适的卷积神经网络（CNN）进行初步诊断。接着，我们使用 Wasserstein GAN with Gradient Penalty（WGAN-GP）和Variational Autoencoder（VAE）生成了两个不同的合成数据集，以增强初始数据集并解决过拟合问题。使用VAE生成的数据集实现了3.0%的提升，达到99.0%的准确率，并得到了较低的损失值以及更快的收敛。最后，我们使用Local Interpretable Model-agnostic Explanations（LIME）算法来确定诊断过程中最重要的超 pix（频率）。
</details></li>
</ul>
<hr>
<h2 id="Balancing-central-and-marginal-rejection-when-combining-independent-significance-tests"><a href="#Balancing-central-and-marginal-rejection-when-combining-independent-significance-tests" class="headerlink" title="Balancing central and marginal rejection when combining independent significance tests"></a>Balancing central and marginal rejection when combining independent significance tests</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16600">http://arxiv.org/abs/2310.16600</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chris Salahub, R. Wayne Oldford</li>
<li>For: The paper is written to discuss and evaluate the significance of a collection of $p$-values, particularly when the original data are not available.* Methods: The paper introduces a telescoping series of alternative hypotheses to communicate the strength and prevalence of non-null evidence in the $p$-values, and discusses various pooling formulae to combine the $p$-values.* Results: The paper proposes a combining function based on the $\chi^2_{\kappa}$ quantile transformation to control the quotient of central and marginal rejection levels, and shows that this function is robust to mis-specified parameters relative to the UMP. Additionally, the paper maps out plausible alternatives based on where the pooled $p$-value is minimized.<details>
<summary>Abstract</summary>
A common approach to evaluating the significance of a collection of $p$-values combines them with a pooling function, in particular when the original data are not available. These pooled $p$-values convert a sample of $p$-values into a single number which behaves like a univariate $p$-value. To clarify discussion of these functions, a telescoping series of alternative hypotheses are introduced that communicate the strength and prevalence of non-null evidence in the $p$-values before general pooling formulae are discussed. A pattern noticed in the UMP pooled $p$-value for a particular alternative motivates the definition and discussion of central and marginal rejection levels at $\alpha$. It is proven that central rejection is always greater than or equal to marginal rejection, motivating a quotient to measure the balance between the two for pooled $p$-values. A combining function based on the $\chi^2_{\kappa}$ quantile transformation is proposed to control this quotient and shown to be robust to mis-specified parameters relative to the UMP. Different powers for different parameter settings motivate a map of plausible alternatives based on where this pooled $p$-value is minimized.
</details>
<details>
<summary>摘要</summary>
一种常见的方法来评估一组 $p$-值的重要性是将它们与一个 combinatory 函数相结合，特别是当原始数据不可用时。这些卷积 $p$-值将一个样本 $p$-值转换成一个单一的数字，这个数字 behave 如一个单variate $p$-value。为了加深这些函数的讨论，我们引入了一系列的备用假设，这些假设通过 $p$-值 中的非null 证据的强度和普遍性来交流。在 UMP 卷积 $p$-value 中的特定假设下，我们定义和讨论中心和边缘拒绝水平在 $\alpha$ 上。由于中心拒绝总是大于或等于边缘拒绝，我们提出了一个比率来度量这两个拒绝水平之间的平衡。一种基于 $\chi^2_{\kappa}$ 量化变换的 combining 函数被提议，可以控制这个比率，并且在 parameter 不符合时比 UMP 更加稳定。不同的参数设置导致一个地图，其中这个卷积 $p$-值在最小化的地方。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Uncertainty-Estimation-via-High-Dimensional-Testing-on-Latent-Representations"><a href="#Adaptive-Uncertainty-Estimation-via-High-Dimensional-Testing-on-Latent-Representations" class="headerlink" title="Adaptive Uncertainty Estimation via High-Dimensional Testing on Latent Representations"></a>Adaptive Uncertainty Estimation via High-Dimensional Testing on Latent Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16587">http://arxiv.org/abs/2310.16587</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hku-medai/bnn_uncertainty">https://github.com/hku-medai/bnn_uncertainty</a></li>
<li>paper_authors: Tsai Hor Chan, Kin Wai Lau, Jiajun Shen, Guosheng Yin, Lequan Yu</li>
<li>for: 这篇研究旨在提出一个新的深度学习 uncertainty estimation 框架，以便在不需要见到 OOD 数据的情况下仍能够精确地评估深度学习模型的不确定性。</li>
<li>methods: 这篇研究使用了 data-adaptive high-dimensional hypothesis testing 来进行 uncertainty estimation，并且不需要重新训练对象 функ数据。 tested statistic 运用了 latent 表示的 Statistical 属性，以提高测试性能。</li>
<li>results: 实验结果显示，使用 Bayesian neural networks 进行 encoding 可以增强测试性能，并且可以更精确地评估深度学习模型的不确定性。 另外，这篇研究还引入了一个家庭单位检测程序，以决定 OOD 检测任务中最佳的阈值，以减少错误发现率 (FDR)。<details>
<summary>Abstract</summary>
Uncertainty estimation aims to evaluate the confidence of a trained deep neural network. However, existing uncertainty estimation approaches rely on low-dimensional distributional assumptions and thus suffer from the high dimensionality of latent features. Existing approaches tend to focus on uncertainty on discrete classification probabilities, which leads to poor generalizability to uncertainty estimation for other tasks. Moreover, most of the literature requires seeing the out-of-distribution (OOD) data in the training for better estimation of uncertainty, which limits the uncertainty estimation performance in practice because the OOD data are typically unseen. To overcome these limitations, we propose a new framework using data-adaptive high-dimensional hypothesis testing for uncertainty estimation, which leverages the statistical properties of the feature representations. Our method directly operates on latent representations and thus does not require retraining the feature encoder under a modified objective. The test statistic relaxes the feature distribution assumptions to high dimensionality, and it is more discriminative to uncertainties in the latent representations. We demonstrate that encoding features with Bayesian neural networks can enhance testing performance and lead to more accurate uncertainty estimation. We further introduce a family-wise testing procedure to determine the optimal threshold of OOD detection, which minimizes the false discovery rate (FDR). Extensive experiments validate the satisfactory performance of our framework on uncertainty estimation and task-specific prediction over a variety of competitors. The experiments on the OOD detection task also show satisfactory performance of our method when the OOD data are unseen in the training. Codes are available at https://github.com/HKU-MedAI/bnn_uncertainty.
</details>
<details>
<summary>摘要</summary>
uncertainty estimation aimsto evaluate the confidence of a trained deep neural network. However, existing uncertainty estimation approaches rely on low-dimensional distributional assumptions and thus suffer from the high dimensionality of latent features. Existing approaches tend to focus on uncertainty on discrete classification probabilities, which leads to poor generalizability to uncertainty estimation for other tasks. Moreover, most of the literature requires seeing the out-of-distribution (OOD) data in the training for better estimation of uncertainty, which limits the uncertainty estimation performance in practice because the OOD data are typically unseen. To overcome these limitations, we propose a new framework using data-adaptive high-dimensional hypothesis testing for uncertainty estimation, which leverages the statistical properties of the feature representations. Our method directly operates on latent representations and thus does not require retraining the feature encoder under a modified objective. The test statistic relaxes the feature distribution assumptions to high dimensionality, and it is more discriminative to uncertainties in the latent representations. We demonstrate that encoding features with Bayesian neural networks can enhance testing performance and lead to more accurate uncertainty estimation. We further introduce a family-wise testing procedure to determine the optimal threshold of OOD detection, which minimizes the false discovery rate (FDR). Extensive experiments validate the satisfactory performance of our framework on uncertainty estimation and task-specific prediction over a variety of competitors. The experiments on the OOD detection task also show satisfactory performance of our method when the OOD data are unseen in the training. Codes are available at <https://github.com/HKU-MedAI/bnn_uncertainty>.
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Explain-A-Model-Agnostic-Framework-for-Explaining-Black-Box-Models"><a href="#Learning-to-Explain-A-Model-Agnostic-Framework-for-Explaining-Black-Box-Models" class="headerlink" title="Learning to Explain: A Model-Agnostic Framework for Explaining Black Box Models"></a>Learning to Explain: A Model-Agnostic Framework for Explaining Black Box Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16584">http://arxiv.org/abs/2310.16584</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ltx-code/ltx">https://github.com/ltx-code/ltx</a></li>
<li>paper_authors: Oren Barkan, Yuval Asher, Amit Eshel, Yehonatan Elisha, Noam Koenigstein<br>for: 提供视觉模型的后期解释methods: 使用一个”解释器”模型生成解释地图，并在两个阶段的训练中使用独特的配置来使用Masked Input对模型的预测进行比较，以实现一种新的对抗对象函数。results: LTX在不同维度上显著超越当前状态的最佳解释性。<details>
<summary>Abstract</summary>
We present Learning to Explain (LTX), a model-agnostic framework designed for providing post-hoc explanations for vision models. The LTX framework introduces an "explainer" model that generates explanation maps, highlighting the crucial regions that justify the predictions made by the model being explained. To train the explainer, we employ a two-stage process consisting of initial pretraining followed by per-instance finetuning. During both stages of training, we utilize a unique configuration where we compare the explained model's prediction for a masked input with its original prediction for the unmasked input. This approach enables the use of a novel counterfactual objective, which aims to anticipate the model's output using masked versions of the input image. Importantly, the LTX framework is not restricted to a specific model architecture and can provide explanations for both Transformer-based and convolutional models. Through our evaluations, we demonstrate that LTX significantly outperforms the current state-of-the-art in explainability across various metrics.
</details>
<details>
<summary>摘要</summary>
我们提出了学习解释（LTX）框架，这是一个模型无关的框架，用于提供后续解释 vision 模型的预测。 LTX 框架引入了一个“解释器”模型，这个模型生成的解释地图可以显示出模型被解释的关键区域，这些区域可以让模型的预测。为了训练解释器，我们运用了两阶段训练的方法，包括初始预训和每个实例的调整。在这两阶段的训练中，我们使用了一个独特的配置，在比较模型被解释的预测和原始预测之间进行比较。这种配置使得我们可以使用一个新的反向 counterfactual 目标，这个目标的目标是预测模型使用填充的输入图像。重要的是，LTX 框架不受特定的模型架构限制，可以提供解释 für  both Transformer 基于和传统的单元模型。我们的评估结果显示，LTX 在不同的 метриках上明显超过了现有的州态。
</details></li>
</ul>
<hr>
<h2 id="Hybrid-Minimax-MCTS-and-Difficulty-Adjustment-for-General-Game-Playing"><a href="#Hybrid-Minimax-MCTS-and-Difficulty-Adjustment-for-General-Game-Playing" class="headerlink" title="Hybrid Minimax-MCTS and Difficulty Adjustment for General Game Playing"></a>Hybrid Minimax-MCTS and Difficulty Adjustment for General Game Playing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16581">http://arxiv.org/abs/2310.16581</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/marcoantonioaav/hybrid-minimax-mcts">https://github.com/marcoantonioaav/hybrid-minimax-mcts</a></li>
<li>paper_authors: Marco Antônio Athayde de Aguiar Vieira, Anderson Rocha Tavares, Renato Perez Ribas</li>
<li>for: 这篇论文是为了开发一个智能对手，以便在零点游戏中实现不同困难级别的游戏体验。</li>
<li>methods: 这篇论文提出了一种混合最小搜索和MCTS算法的方法，以实现适应不同困难级别的人工智能对手。</li>
<li>results: 测试结果表明，这种混合算法和新的困难调整系统都是有前途的智能对手方法。<details>
<summary>Abstract</summary>
Board games are a great source of entertainment for all ages, as they create a competitive and engaging environment, as well as stimulating learning and strategic thinking. It is common for digital versions of board games, as any other type of digital games, to offer the option to select the difficulty of the game. This is usually done by customizing the search parameters of the AI algorithm. However, this approach cannot be extended to General Game Playing agents, as different games might require different parametrization for each difficulty level. In this paper, we present a general approach to implement an artificial intelligence opponent with difficulty levels for zero-sum games, together with a propose of a Minimax-MCTS hybrid algorithm, which combines the minimax search process with GGP aspects of MCTS. This approach was tested in our mobile application LoBoGames, an extensible board games platform, that is intended to have an broad catalog of games, with an emphasis on accessibility: the platform is friendly to visually-impaired users, and is compatible with more than 92\% of Android devices. The tests in this work indicate that both the hybrid Minimax-MCTS and the new difficulty adjustment system are promising GGP approaches that could be expanded in future work.
</details>
<details>
<summary>摘要</summary>
《Board games are a great source of entertainment for all ages, as they create a competitive and engaging environment, as well as stimulating learning and strategic thinking. It is common for digital versions of board games, as any other type of digital games, to offer the option to select the difficulty of the game. This is usually done by customizing the search parameters of the AI algorithm. However, this approach cannot be extended to General Game Playing agents, as different games might require different parametrization for each difficulty level. In this paper, we present a general approach to implement an artificial intelligence opponent with difficulty levels for zero-sum games, together with a propose of a Minimax-MCTS hybrid algorithm, which combines the minimax search process with GGP aspects of MCTS. This approach was tested in our mobile application LoBoGames, an extensible board games platform, that is intended to have an broad catalog of games, with an emphasis on accessibility: the platform is friendly to visually-impaired users, and is compatible with more than 92\% of Android devices. The tests in this work indicate that both the hybrid Minimax-MCTS and the new difficulty adjustment system are promising GGP approaches that could be expanded in future work.》Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Adapt-Anything-Tailor-Any-Image-Classifiers-across-Domains-And-Categories-Using-Text-to-Image-Diffusion-Models"><a href="#Adapt-Anything-Tailor-Any-Image-Classifiers-across-Domains-And-Categories-Using-Text-to-Image-Diffusion-Models" class="headerlink" title="Adapt Anything: Tailor Any Image Classifiers across Domains And Categories Using Text-to-Image Diffusion Models"></a>Adapt Anything: Tailor Any Image Classifiers across Domains And Categories Using Text-to-Image Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16573">http://arxiv.org/abs/2310.16573</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijie Chen, Haoyu Wang, Shicai Yang, Lei Zhang, Wei Wei, Yanning Zhang, Luojun Lin, Di Xie, Yueting Zhuang</li>
<li>for: 这篇论文的目的是研究一种使用现代文本到图像扩散模型来适应任务和领域域的图像分类器。</li>
<li>methods: 本文使用了现有的领域适应图像分类方法，并将高质量的文本到图像生成器生成的图像作为预处理数据来进行适应。</li>
<li>results: 实验结果表明，使用这种方法可以在不需要收集和标注真实世界数据的情况下，将文本到图像生成器中嵌入的知识传递到任务特定的图像分类器中，并且能够超越现有的领域适应图像分类方法。<details>
<summary>Abstract</summary>
We do not pursue a novel method in this paper, but aim to study if a modern text-to-image diffusion model can tailor any task-adaptive image classifier across domains and categories. Existing domain adaptive image classification works exploit both source and target data for domain alignment so as to transfer the knowledge learned from the labeled source data to the unlabeled target data. However, as the development of the text-to-image diffusion model, we wonder if the high-fidelity synthetic data from the text-to-image generator can serve as a surrogate of the source data in real world. In this way, we do not need to collect and annotate the source data for each domain adaptation task in a one-for-one manner. Instead, we utilize only one off-the-shelf text-to-image model to synthesize images with category labels derived from the corresponding text prompts, and then leverage the surrogate data as a bridge to transfer the knowledge embedded in the task-agnostic text-to-image generator to the task-oriented image classifier via domain adaptation. Such a one-for-all adaptation paradigm allows us to adapt anything in the world using only one text-to-image generator as well as the corresponding unlabeled target data. Extensive experiments validate the feasibility of the proposed idea, which even surpasses the state-of-the-art domain adaptation works using the source data collected and annotated in real world.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Label-Propagation-for-Graph-Label-Noise"><a href="#Label-Propagation-for-Graph-Label-Noise" class="headerlink" title="Label Propagation for Graph Label Noise"></a>Label Propagation for Graph Label Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16560">http://arxiv.org/abs/2310.16560</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Cheng, Caihua Shan, Yifei Shen, Xiang Li, Siqiang Luo, Dongsheng Li</li>
<li>for:  rectifying noisy labels and assigning labels to previously unlabeled nodes in the context of arbitrary heterophily.</li>
<li>methods:  LP4GLN algorithm, which consists of three steps: (1) reconstruct the graph to recover the homophily property, (2) utilize label propagation to rectify the noisy labels, (3) select high-confidence labels to retain for the next iteration.</li>
<li>results:  superior performance compared to 7 typical baselines in node classification tasks under varying graph heterophily levels and noise types.<details>
<summary>Abstract</summary>
Label noise is a common challenge in large datasets, as it can significantly degrade the generalization ability of deep neural networks. Most existing studies focus on noisy labels in computer vision; however, graph models encompass both node features and graph topology as input, and become more susceptible to label noise through message-passing mechanisms. Recently, only a few works have been proposed to tackle the label noise on graphs. One major limitation is that they assume the graph is homophilous and the labels are smoothly distributed. Nevertheless, real-world graphs may contain varying degrees of heterophily or even be heterophily-dominated, leading to the inadequacy of current methods. In this paper, we study graph label noise in the context of arbitrary heterophily, with the aim of rectifying noisy labels and assigning labels to previously unlabeled nodes. We begin by conducting two empirical analyses to explore the impact of graph homophily on graph label noise. Following observations, we propose a simple yet efficient algorithm, denoted as LP4GLN. Specifically, LP4GLN is an iterative algorithm with three steps: (1) reconstruct the graph to recover the homophily property, (2) utilize label propagation to rectify the noisy labels, (3) select high-confidence labels to retain for the next iteration. By iterating these steps, we obtain a set of correct labels, ultimately achieving high accuracy in the node classification task. The theoretical analysis is also provided to demonstrate its remarkable denoising "effect". Finally, we conduct experiments on 10 benchmark datasets under varying graph heterophily levels and noise types, comparing the performance of LP4GLN with 7 typical baselines. Our results illustrate the superior performance of the proposed LP4GLN.
</details>
<details>
<summary>摘要</summary>
标签噪声是大型数据集中的一个常见挑战，可以很大程度地降低深度神经网络的泛化能力。大多数现有研究都集中于计算机视觉中的噪声标签;然而，图模型包含节点特征和图结构作为输入，因此更容易受到噪声标签的影响。虽然有一些最近的工作已经提出来了处理图标签噪声，但是它们假设图是同质的，标签分布平滑。然而，实际世界中的图可能包含不同程度的异质或者甚至是异质占主导地位，导致现有方法无法应用。在这篇论文中，我们研究了图标签噪声在不同异质性下的情况，目的是修复噪声标签并将标签分配给未标注的节点。我们开始通过两个实际分析来探索图同质性对图标签噪声的影响。根据观察结果，我们提出了一种简单 yet efficient的算法， denoted as LP4GLN。LP4GLN是一个迭代算法，具体步骤如下：1. 重建图以恢复同质性;2. 利用标签推广来修正噪声标签;3. 选择高信度标签来保留下一轮。通过迭代这些步骤，我们可以获得一组正确的标签，最终实现高精度在节点分类任务中。我们还提供了理论分析，以证明其强大的"效果"。最后，我们在10个标准 benchmark dataset上进行了10种不同异质性水平和噪声类型的实验，与7种典型基线相比较。我们的结果表明，LP4GLN的表现胜过了7种基线。
</details></li>
</ul>
<hr>
<h2 id="Pitfall-of-Optimism-Distributional-Reinforcement-Learning-by-Randomizing-Risk-Criterion"><a href="#Pitfall-of-Optimism-Distributional-Reinforcement-Learning-by-Randomizing-Risk-Criterion" class="headerlink" title="Pitfall of Optimism: Distributional Reinforcement Learning by Randomizing Risk Criterion"></a>Pitfall of Optimism: Distributional Reinforcement Learning by Randomizing Risk Criterion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16546">http://arxiv.org/abs/2310.16546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taehyun Cho, Seungyub Han, Heesoo Lee, Kyungjae Lee, Jungwoo Lee</li>
<li>for:  This paper proposes a novel distributional reinforcement learning algorithm to avoid biased exploration and improve performance.</li>
<li>methods:  The proposed algorithm randomizes the risk criterion to avoid one-sided tendency on risk, and uses a perturbed distributional Bellman optimality operator to prove convergence and optimality.</li>
<li>results:  The proposed method outperforms other existing distribution-based algorithms in various environments, including Atari 55 games.<details>
<summary>Abstract</summary>
Distributional reinforcement learning algorithms have attempted to utilize estimated uncertainty for exploration, such as optimism in the face of uncertainty. However, using the estimated variance for optimistic exploration may cause biased data collection and hinder convergence or performance. In this paper, we present a novel distributional reinforcement learning algorithm that selects actions by randomizing risk criterion to avoid one-sided tendency on risk. We provide a perturbed distributional Bellman optimality operator by distorting the risk measure and prove the convergence and optimality of the proposed method with the weaker contraction property. Our theoretical results support that the proposed method does not fall into biased exploration and is guaranteed to converge to an optimal return. Finally, we empirically show that our method outperforms other existing distribution-based algorithms in various environments including Atari 55 games.
</details>
<details>
<summary>摘要</summary>
分布式权威学习算法已经尝试使用估计的不确定性进行探索，如在面对不确定性时表现optimism。然而，使用估计方差来实现optimistic探索可能会导致数据采集偏斜和性能下降。在这篇论文中，我们提出了一种新的分布式权威学习算法，它通过随机化风险 критериion来避免一面的倾向。我们提供了一个扰动分布 Bellman 优化算法，并证明了我们的方法具有更弱的收缩性质。我们的理论结果表明，我们的方法不会受到偏斜探索的影响，并且能够 converge to an optimal return。最后，我们通过实验表明，我们的方法在多个环境中（包括Atari 55 游戏）表现出色，超过了其他现有的分布基于算法。
</details></li>
</ul>
<hr>
<h2 id="A-Multilingual-Virtual-Guide-for-Self-Attachment-Technique"><a href="#A-Multilingual-Virtual-Guide-for-Self-Attachment-Technique" class="headerlink" title="A Multilingual Virtual Guide for Self-Attachment Technique"></a>A Multilingual Virtual Guide for Self-Attachment Technique</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18366">http://arxiv.org/abs/2310.18366</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alicia Jiayun Law, Ruoyu Hu, Lisa Alazraki, Anandha Gopalan, Neophytos Polydorou, Abbas Edalat</li>
<li>for: 这个研究的目的是开发一个基于现有语言数据的计算框架，用于在普通话中提供Self-Attachment Technique（SAT）。</li>
<li>methods: 该框架不需要大规模的人类翻译，却可以达到相似的性能水平，同时保持安全性和可靠性。研究者提出了两种增强可用响应数据的方法，包括emploympathetic rewrite。</li>
<li>results: 研究者通过对前一代英语只的SAT chatbot进行比较，通过非临床人类试验（N&#x3D;42），每次试验5天，量化显示了与英语 SAT chatbot相当的性能水平。研究者还提供了限制分析和建议，以帮助未来的改进。<details>
<summary>Abstract</summary>
In this work, we propose a computational framework that leverages existing out-of-language data to create a conversational agent for the delivery of Self-Attachment Technique (SAT) in Mandarin. Our framework does not require large-scale human translations, yet it achieves a comparable performance whilst also maintaining safety and reliability. We propose two different methods of augmenting available response data through empathetic rewriting. We evaluate our chatbot against a previous, English-only SAT chatbot through non-clinical human trials (N=42), each lasting five days, and quantitatively show that we are able to attain a comparable level of performance to the English SAT chatbot. We provide qualitative analysis on the limitations of our study and suggestions with the aim of guiding future improvements.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们提出了一种计算机框架，利用现有的语言外数据创建一个拥有自我附加技巧（SAT）的 Mandarin 会话代理。我们的框架不需要大规模的人类翻译，却可以达到相似的性能水平，同时保持安全和可靠性。我们提出了两种增强可用响应数据的方法，通过同情性重写。我们对前一个英语只的 SAT 会话代理进行评估，通过非клиниче人类试验（N=42），每个试验持续五天，并证明我们可以达到与英语 SAT 会话代理相似的性能水平。我们提供了限制分析和建议，以帮助未来的改进。
</details></li>
</ul>
<hr>
<h2 id="FedTherapist-Mental-Health-Monitoring-with-User-Generated-Linguistic-Expressions-on-Smartphones-via-Federated-Learning"><a href="#FedTherapist-Mental-Health-Monitoring-with-User-Generated-Linguistic-Expressions-on-Smartphones-via-Federated-Learning" class="headerlink" title="FedTherapist: Mental Health Monitoring with User-Generated Linguistic Expressions on Smartphones via Federated Learning"></a>FedTherapist: Mental Health Monitoring with User-Generated Linguistic Expressions on Smartphones via Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16538">http://arxiv.org/abs/2310.16538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaemin Shin, Hyungjun Yoon, Seungjoo Lee, Sungjoon Park, Yunxin Liu, Jinho D. Choi, Sung-Ju Lee</li>
<li>for: 这个论文是为了提出一种基于联邦学习的移动设备上的心理健康监测系统，以保护用户的隐私。</li>
<li>methods: 该论文使用了连续语音和键盘输入，并通过联邦学习方法进行训练，以解决智能手机上的语言模型训练问题。另外，论文还提出了一种Context-Aware Language Learning（CALL）方法，以更好地利用手机上的大量和噪音的文本数据来检测心理健康信号。</li>
<li>results: 论文的实验结果显示，使用联邦学习方法可以提高心理健康监测系统的准确率，比非语言特征的表现高出0.15 AUROC和8.21% MAE。<details>
<summary>Abstract</summary>
Psychiatrists diagnose mental disorders via the linguistic use of patients. Still, due to data privacy, existing passive mental health monitoring systems use alternative features such as activity, app usage, and location via mobile devices. We propose FedTherapist, a mobile mental health monitoring system that utilizes continuous speech and keyboard input in a privacy-preserving way via federated learning. We explore multiple model designs by comparing their performance and overhead for FedTherapist to overcome the complex nature of on-device language model training on smartphones. We further propose a Context-Aware Language Learning (CALL) methodology to effectively utilize smartphones' large and noisy text for mental health signal sensing. Our IRB-approved evaluation of the prediction of self-reported depression, stress, anxiety, and mood from 46 participants shows higher accuracy of FedTherapist compared with the performance with non-language features, achieving 0.15 AUROC improvement and 8.21% MAE reduction.
</details>
<details>
<summary>摘要</summary>
心理医生通过语言使用病人进行诊断精神疾病。然而由于数据隐私问题，现有的潜在精神健康监测系统使用替代特征 such as 活动、应用程序使用和位置通过移动设备。我们提议了 FedTherapist，一种基于联邦学习的移动精神健康监测系统，可以在隐私保护的情况下使用连续的语音和键盘输入。我们比较了不同的模型设计，并评估了它们在 FedTherapist 中的性能和开销。此外，我们还提出了 Context-Aware Language Learning（CALL）方法，以便有效利用智能手机上的大量和噪音的文本来感知精神健康信号。我们经过IRB审核的评估结果表明，FedTherapist 比使用非语言特征的性能高，实现了0.15 AUROC 提升和8.21% MAE 减少。
</details></li>
</ul>
<hr>
<h2 id="R-3-Prompting-Review-Rephrase-and-Resolve-for-Chain-of-Thought-Reasoning-in-Large-Language-Models-under-Noisy-Context"><a href="#R-3-Prompting-Review-Rephrase-and-Resolve-for-Chain-of-Thought-Reasoning-in-Large-Language-Models-under-Noisy-Context" class="headerlink" title="R$^3$ Prompting: Review, Rephrase and Resolve for Chain-of-Thought Reasoning in Large Language Models under Noisy Context"></a>R$^3$ Prompting: Review, Rephrase and Resolve for Chain-of-Thought Reasoning in Large Language Models under Noisy Context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16535">http://arxiv.org/abs/2310.16535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingyuan Tian, Hanlun Zhu, Lei Wang, Yang Li, Yunshi Lan</li>
<li>For: The paper aims to improve the performance of large language models (LLMs) on various reasoning tasks under noisy contexts.* Methods: The proposed method, called R$^3$ prompting, interacts with LLMs to perform key sentence extraction, variable declaration, and answer prediction, which mimics the thought process of reviewing, rephrasing, and resolving.* Results: The proposed method significantly outperforms existing CoT prompting methods on five reasoning tasks under noisy contexts, with an average accuracy improvement of 3.7% using GPT-3.5-turbo.<details>
<summary>Abstract</summary>
With the help of Chain-of-Thought (CoT) prompting, Large Language Models (LLMs) have achieved remarkable performance on various reasoning tasks. However, most of them have been evaluated under noise-free context and the dilemma for LLMs to produce inaccurate results under the noisy context has not been fully investigated. Existing studies utilize trigger sentences to encourage LLMs to concentrate on the relevant information but the trigger has limited effect on final answer prediction. Inspired by interactive CoT method, where intermediate reasoning steps are promoted by multiple rounds of interaction between users and LLMs, we propose a novel prompting method, namely R$^3$ prompting, for CoT reasoning under noisy context. Specifically, R$^3$ prompting interacts with LLMs to perform key sentence extraction, variable declaration and answer prediction, which corresponds to a thought process of reviewing, rephrasing and resolving. The responses generated at the last interaction will perform as hints to guide toward the responses of the next interaction. Our experiments show that R$^3$ prompting significantly outperforms existing CoT prompting methods on five reasoning tasks under noisy context. With GPT-3.5-turbo, we observe 3.7% accuracy improvement on average on the reasoning tasks under noisy context compared to the most competitive prompting baseline. More analyses and ablation studies show the robustness and generalization of R$^3$ prompting method in solving reasoning tasks in LLMs under noisy context.
</details>
<details>
<summary>摘要</summary>
以Chain-of-Thought（CoT）推动，大语言模型（LLM）在多种推理任务上达到了remarkable表现。然而，大多数研究都在干净环境下进行评估，尚未全面探讨LLM在噪音环境下的表现问题。现有研究通过触发句来鼓励LLM强调相关信息，但触发句对最终答案预测产生有限的影响。 drawing inspiration from interactive CoT方法，我们提出了一种新的推示方法，即R$^3$推示，用于CoT推理在噪音环境下。具体来说，R$^3$推示与LLM进行关键句提取、变量声明和答案预测，与人类思维过程中的回顾、重新表述和解决相吻合。最后一次交互的答案作为下一次交互的提示，进一步提高了LLM的表现。我们的实验表明，R$^3$推示与最竞争的推示基准方法相比，在五种推理任务下噪音环境下表现出了显著的提高，使用GPT-3.5-turbo时平均提高了3.7%的精度。此外，我们还进行了更多的分析和减少研究，证明了R$^3$推示方法在LLM下的稳定性和通用性。
</details></li>
</ul>
<hr>
<h2 id="Improving-Diversity-of-Demographic-Representation-in-Large-Language-Models-via-Collective-Critiques-and-Self-Voting"><a href="#Improving-Diversity-of-Demographic-Representation-in-Large-Language-Models-via-Collective-Critiques-and-Self-Voting" class="headerlink" title="Improving Diversity of Demographic Representation in Large Language Models via Collective-Critiques and Self-Voting"></a>Improving Diversity of Demographic Representation in Large Language Models via Collective-Critiques and Self-Voting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16523">http://arxiv.org/abs/2310.16523</a></li>
<li>repo_url: None</li>
<li>paper_authors: Preethi Lahoti, Nicholas Blumm, Xiao Ma, Raghavendra Kotikalapudi, Sahitya Potluri, Qijun Tan, Hansa Srinivasan, Ben Packer, Ahmad Beirami, Alex Beutel, Jilin Chen</li>
<li>for: 本研究旨在提高生成大型自然语言模型（LLM）的多样性，以便在用户提供的不充分的提示下，模型可以生成多种不同的响应，而不是只有一种固定的回答。</li>
<li>methods: 本研究使用了评估数据集和提出了多样性评价指标，以衡量生成响应中人和文化方面的多样性。此外，还提出了一种称为集体批评和自动投票（CCSV）的新提示技术，可以通过让模型自身进行多样性理解和自我评估，提高LLM的多样性无需手动编写示例或提示调整。</li>
<li>results: 实验表明，我们的提posed方法可以有效地提高人和文化多样性，并在所有基线方法之上减少了差距。此外，我们还发现了LLM可以理解多样性的概念，并可以对自己的回答进行多样性评价。<details>
<summary>Abstract</summary>
A crucial challenge for generative large language models (LLMs) is diversity: when a user's prompt is under-specified, models may follow implicit assumptions while generating a response, which may result in homogenization of the responses, as well as certain demographic groups being under-represented or even erased from the generated responses. In this paper, we formalize diversity of representation in generative LLMs. We present evaluation datasets and propose metrics to measure diversity in generated responses along people and culture axes. We find that LLMs understand the notion of diversity, and that they can reason and critique their own responses for that goal. This finding motivated a new prompting technique called collective-critique and self-voting (CCSV) to self-improve people diversity of LLMs by tapping into its diversity reasoning capabilities, without relying on handcrafted examples or prompt tuning. Extensive empirical experiments with both human and automated evaluations show that our proposed approach is effective at improving people and culture diversity, and outperforms all baseline methods by a large margin.
</details>
<details>
<summary>摘要</summary>
一个重要挑战是，生成大语言模型（LLM）的多样性：当用户的提示不够精确时，模型可能会遵循偏见而生成响应，这可能导致响应的同化，以及某些民族或文化群体被排除或者消失在生成的响应中。在这篇论文中，我们正式定义生成LLM的多样性表示。我们提供评估数据集和提出多样性评估 metric，用于衡量生成响应中人和文化轴上的多样性。我们发现 LLM 理解多样性的概念，并且它可以对自己的回答进行多样性评估，不需要靠手工例子或提示调整。这一发现使我们提出了一种新的提示技术called collective-critique and self-voting（CCSV），用于自我改进 LLM 的人多样性，不需要靠手工例子或提示调整。我们进行了广泛的实验，并证明了我们的提posed方法可以有效地提高人和文化多样性，并在所有基准方法之上出色表现。
</details></li>
</ul>
<hr>
<h2 id="Identifying-Reasons-for-Bias-An-Argumentation-Based-Approach"><a href="#Identifying-Reasons-for-Bias-An-Argumentation-Based-Approach" class="headerlink" title="Identifying Reasons for Bias: An Argumentation-Based Approach"></a>Identifying Reasons for Bias: An Argumentation-Based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16506">http://arxiv.org/abs/2310.16506</a></li>
<li>repo_url: None</li>
<li>paper_authors: Madeleine Waller, Odinaldo Rodrigues, Oana Cocarascu</li>
<li>for:  Ensuring the fairness of algorithmic decision-making systems</li>
<li>methods:  Model-agnostic argumentation-based method using a quantitative argumentation framework and well-known semantics to identify bias</li>
<li>results:  Effective in identifying bias in two datasets commonly used in the fairness literature<details>
<summary>Abstract</summary>
As algorithmic decision-making systems become more prevalent in society, ensuring the fairness of these systems is becoming increasingly important. Whilst there has been substantial research in building fair algorithmic decision-making systems, the majority of these methods require access to the training data, including personal characteristics, and are not transparent regarding which individuals are classified unfairly. In this paper, we propose a novel model-agnostic argumentation-based method to determine why an individual is classified differently in comparison to similar individuals. Our method uses a quantitative argumentation framework to represent attribute-value pairs of an individual and of those similar to them, and uses a well-known semantics to identify the attribute-value pairs in the individual contributing most to their different classification. We evaluate our method on two datasets commonly used in the fairness literature and illustrate its effectiveness in the identification of bias.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="On-the-Powerfulness-of-Textual-Outlier-Exposure-for-Visual-OoD-Detection"><a href="#On-the-Powerfulness-of-Textual-Outlier-Exposure-for-Visual-OoD-Detection" class="headerlink" title="On the Powerfulness of Textual Outlier Exposure for Visual OoD Detection"></a>On the Powerfulness of Textual Outlier Exposure for Visual OoD Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16492">http://arxiv.org/abs/2310.16492</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sangha Park, Jisoo Mok, Dahuin Jung, Saehyung Lee, Sungroh Yoon</li>
<li>for: 提高神经网络中的Out-of-Distribution（OoD）检测精度，以确保神经网络安全部署。</li>
<li>methods: 使用文本外围暴露来提高OoD检测性能，包括在训练时引入低信任率的预测假设。</li>
<li>results: 通过使用文本外围，实现了在大规模OoD和困难OoD数据集上的竞争性表现，并提供了对优秀文本外围的emplary criteria。<details>
<summary>Abstract</summary>
Successful detection of Out-of-Distribution (OoD) data is becoming increasingly important to ensure safe deployment of neural networks. One of the main challenges in OoD detection is that neural networks output overconfident predictions on OoD data, make it difficult to determine OoD-ness of data solely based on their predictions. Outlier exposure addresses this issue by introducing an additional loss that encourages low-confidence predictions on OoD data during training. While outlier exposure has shown promising potential in improving OoD detection performance, all previous studies on outlier exposure have been limited to utilizing visual outliers. Drawing inspiration from the recent advancements in vision-language pre-training, this paper venture out to the uncharted territory of textual outlier exposure. First, we uncover the benefits of using textual outliers by replacing real or virtual outliers in the image-domain with textual equivalents. Then, we propose various ways of generating preferable textual outliers. Our extensive experiments demonstrate that generated textual outliers achieve competitive performance on large-scale OoD and hard OoD benchmarks. Furthermore, we conduct empirical analyses of textual outliers to provide primary criteria for designing advantageous textual outliers: near-distribution, descriptiveness, and inclusion of visual semantics.
</details>
<details>
<summary>摘要</summary>
成功探测 OUT-OF-DISTRIBUTION（OoD）数据变得越来越重要，以确保神经网络的安全部署。OoD探测的主要挑战在于神经网络在OoD数据上输出过自信的预测，使得根据预测结果很难判断数据是否属于OoD类别。Outlier exposure解决这个问题，通过在训练过程中引入陌生数据的额外损失，以鼓励神经网络在OoD数据上输出低自信率的预测。然而，所有之前的Outlier exposure研究都是基于视觉异常点。这篇论文启发自最近的视觉语言预训练技术，在文本异常点方面进行了探索。我们首先探讨使用文本异常点的利点，然后提出了不同的文本异常点生成方法。我们的广泛的实验表明，生成的文本异常点可以在大规模OoD和困难OoD benchmark上实现竞争性表现。此外，我们进行了文本异常点的实际分析，为设计有利的文本异常点提供了首要的标准：靠近分布、描述性和视觉 semantics的包含。
</details></li>
</ul>
<hr>
<h2 id="Transfer-of-Reinforcement-Learning-Based-Controllers-from-Model-to-Hardware-in-the-Loop"><a href="#Transfer-of-Reinforcement-Learning-Based-Controllers-from-Model-to-Hardware-in-the-Loop" class="headerlink" title="Transfer of Reinforcement Learning-Based Controllers from Model- to Hardware-in-the-Loop"></a>Transfer of Reinforcement Learning-Based Controllers from Model- to Hardware-in-the-Loop</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17671">http://arxiv.org/abs/2310.17671</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mario Picerno, Lucas Koch, Kevin Badalian, Marius Wegener, Joschka Schaub, Charles Robert Koch, Jakob Andert</li>
<li>for: 这项研究旨在加速使用 transferred learning（TL）和 cross-in-the-loop（XiL） simulation 训练深度学习（RL）代理人，以便在真实应用中使用RL。</li>
<li>methods: 该研究使用了计算成本低的模型在循环（MiL） simulate 选择合适的算法和精确调整超参数，然后将候选代理人转移到硬件在循环（HiL）系统进行训练。</li>
<li>results: 结果表明需要在进行真实硬件转移时调整奖励参数，并且比较一个直接在HiL系统训练的代理人和一个转移过来的代理人，发现后者的训练时间减少了5.9倍。结果表明RL代理人需要与真正的硬件进行交互，并且TL和XiL simulation synergies 可以减少训练时间和提高性能。<details>
<summary>Abstract</summary>
The process of developing control functions for embedded systems is resource-, time-, and data-intensive, often resulting in sub-optimal cost and solutions approaches. Reinforcement Learning (RL) has great potential for autonomously training agents to perform complex control tasks with minimal human intervention. Due to costly data generation and safety constraints, however, its application is mostly limited to purely simulated domains. To use RL effectively in embedded system function development, the generated agents must be able to handle real-world applications. In this context, this work focuses on accelerating the training process of RL agents by combining Transfer Learning (TL) and X-in-the-Loop (XiL) simulation. For the use case of transient exhaust gas re-circulation control for an internal combustion engine, use of a computationally cheap Model-in-the-Loop (MiL) simulation is made to select a suitable algorithm, fine-tune hyperparameters, and finally train candidate agents for the transfer. These pre-trained RL agents are then fine-tuned in a Hardware-in-the-Loop (HiL) system via TL. The transfer revealed the need for adjusting the reward parameters when advancing to real hardware. Further, the comparison between a purely HiL-trained and a transferred agent showed a reduction of training time by a factor of 5.9. The results emphasize the necessity to train RL agents with real hardware, and demonstrate that the maturity of the transferred policies affects both training time and performance, highlighting the strong synergies between TL and XiL simulation.
</details>
<details>
<summary>摘要</summary>
开发嵌入式系统控制函数的过程是资源-, 时间-, 和数据-昂贵的，常导致优化成本和解决方案的偏好。 reinforcement learning（RL）有很大的潜力，可以自动训练代理人来执行复杂的控制任务，无需人类干预。然而，由于数据生成的成本和安全限制，RL的应用通常限于完全的模拟领域。要使RL有效地应用于嵌入式系统功能开发，生成的代理人必须能够处理实际世界应用。在这种情况下，这项工作关注于加速RL代理人的训练过程，通过结合传输学习（TL）和X-in-the-Loop（XiL）模拟来实现。为内燃机器循环油耗控制的使用例子，我们使用计算成本低的模型-in-the-Loop（MiL）模拟选择适当的算法，细调超参数，并最终在HiL系统中训练候选代理人。这些预训练RL代理人然后在硬件-in-the-Loop（HiL）系统中进行TL传输，并对奖金参数进行调整。结果表明，在进行真正硬件上训练RL代理人是必要的，并且在RL代理人的成熟度和训练时间之间存在强烈的相互作用。
</details></li>
</ul>
<hr>
<h2 id="Semiring-Provenance-for-Lightweight-Description-Logics"><a href="#Semiring-Provenance-for-Lightweight-Description-Logics" class="headerlink" title="Semiring Provenance for Lightweight Description Logics"></a>Semiring Provenance for Lightweight Description Logics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16472">http://arxiv.org/abs/2310.16472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Camille Bourgaux, Ana Ozaki, Rafael Peñaloza</li>
<li>for: 本研究围绕描述逻辑中的semiring provenance框架进行调查，以增强描述逻辑的表达能力和可解释性。</li>
<li>methods: 本研究使用了 commutative semiring 来注释描述逻辑规则，并通过对 ontology 后果的 Computation 来反映这些注释的 derivation。</li>
<li>results: 研究人员定义了一种 semiring provenance  semantics，并证明其满足了一些愿景性质（如EXTEND）。 另外，研究人员还研究了 why-provenance 的复杂性问题，并对 positive Boolean provenance 和 lineage 进行了研究。<details>
<summary>Abstract</summary>
We investigate semiring provenance--a successful framework originally defined in the relational database setting--for description logics. In this context, the ontology axioms are annotated with elements of a commutative semiring and these annotations are propagated to the ontology consequences in a way that reflects how they are derived. We define a provenance semantics for a language that encompasses several lightweight description logics and show its relationships with semantics that have been defined for ontologies annotated with a specific kind of annotation (such as fuzzy degrees). We show that under some restrictions on the semiring, the semantics satisfies desirable properties (such as extending the semiring provenance defined for databases). We then focus on the well-known why-provenance, which allows to compute the semiring provenance for every additively and multiplicatively idempotent commutative semiring, and for which we study the complexity of problems related to the provenance of an axiom or a conjunctive query answer. Finally, we consider two more restricted cases which correspond to the so-called positive Boolean provenance and lineage in the database setting. For these cases, we exhibit relationships with well-known notions related to explanations in description logics and complete our complexity analysis. As a side contribution, we provide conditions on an ELHI_bot ontology that guarantee tractable reasoning.
</details>
<details>
<summary>摘要</summary>
我团队 investigate semiring provenance---一种成功的框架，原定于关系数据库设置--- для描述逻辑。在这个 контексте，ontology axioms 被标注为 комму态 semiring 的元素，并且这些标注被传递到ontology consequences中，以反映它们如何被 derivation。我们定义了描述逻辑语言中的 provenance semantics，并证明其与其他语言中的 semantics 有关系。我们还证明，在certain restrictions 下，这种 semantics 满足了愉悦的性质（如扩展关系数据库中的 semiring provenance）。然后，我们专注于 well-known why-provenance，可以计算 semiring provenance  для每个可加法和可乘法 идеmpotent commutative semiring，并研究这些问题的复杂性。最后，我们考虑两种更加限制的情况，即positive Boolean provenance 和 lineage 在数据库设置中。对于这些情况，我们提出了与描述逻辑中的解释相关的一些概念，并完成了我们的复杂性分析。作为一个Side contribution，我们提供了ELHI_bot ontology 的条件，以 garantue tractable reasoning。
</details></li>
</ul>
<hr>
<h2 id="Towards-Explainability-in-Monocular-Depth-Estimation"><a href="#Towards-Explainability-in-Monocular-Depth-Estimation" class="headerlink" title="Towards Explainability in Monocular Depth Estimation"></a>Towards Explainability in Monocular Depth Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16457">http://arxiv.org/abs/2310.16457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vasileios Arampatzakis, George Pavlidis, Kyriakos Pantoglou, Nikolaos Mitianoudis, Nikos Papamarkos</li>
<li>for: 本研究旨在探讨深度估计方法的可解性，具体来说是如何 humans perceive depth 的方法。</li>
<li>methods: 本研究使用了深度学习方法，并在实验中测试了state-of-the-art方法，以 indirectly 评估这些方法在定义的上下文中的可解性。</li>
<li>results: 结果表明，使用不同的方法可以达到约77%的准确率，其中一些方法表现出色，这种性能差异 indirectly 透露了这些方法对Relative size 的感知。<details>
<summary>Abstract</summary>
The estimation of depth in two-dimensional images has long been a challenging and extensively studied subject in computer vision. Recently, significant progress has been made with the emergence of Deep Learning-based approaches, which have proven highly successful. This paper focuses on the explainability in monocular depth estimation methods, in terms of how humans perceive depth. This preliminary study emphasizes on one of the most significant visual cues, the relative size, which is prominent in almost all viewed images. We designed a specific experiment to mimic the experiments in humans and have tested state-of-the-art methods to indirectly assess the explainability in the context defined. In addition, we observed that measuring the accuracy required further attention and a particular approach is proposed to this end. The results show that a mean accuracy of around 77% across methods is achieved, with some of the methods performing markedly better, thus, indirectly revealing their corresponding potential to uncover monocular depth cues, like relative size.
</details>
<details>
<summary>摘要</summary>
Computer vision 领域中两维图像中depth的估计已经是长期的挑战和广泛研究的主题。在深度学习方法的出现后，这个领域已经做出了重要的进展。本文将关注在单目深度估计方法中的可解释性，即人类如何感受到深度。本初步研究强调了人类最重要的视觉cue之一——相对大小，这种cue在大多数视图图像中都很显著。我们设计了一个特定的实验，模拟人类的实验，并测试了当前领域的state-of-the-art方法，以 indirectly 评估这些方法在定义的上下文中的可解释性。此外，我们发现了测试准确性需要进一步的注意和一种特定的方法的提议，以便实现更高的准确性。结果显示，所有方法的平均准确率在77%左右，其中一些方法表现出色，这种表现直接表明它们感受到了单目深度cue，如相对大小。
</details></li>
</ul>
<hr>
<h2 id="Graph-based-multimodal-multi-lesion-DLBCL-treatment-response-prediction-from-PET-images"><a href="#Graph-based-multimodal-multi-lesion-DLBCL-treatment-response-prediction-from-PET-images" class="headerlink" title="Graph-based multimodal multi-lesion DLBCL treatment response prediction from PET images"></a>Graph-based multimodal multi-lesion DLBCL treatment response prediction from PET images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16863">http://arxiv.org/abs/2310.16863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oriane Thiery, Mira Rizkallah, Clément Bailly, Caroline Bodet-Milin, Emmanuel Itti, René-Olivier Casasnovas, Steven Le Gouill, Thomas Carlier, Diana Mateus</li>
<li>for: 这个研究旨在发展一个电脑支持的方法，以帮助诊断和跟踪悉普性大B细胞淋巴癌（DLBCL）。</li>
<li>methods: 这个方法使用最新的 graf neural network，结合多个肿瘤的影像信息，并使用标注模组优化不同数据模式之间的integreation。</li>
<li>results: 实验结果显示，我们提议的方法在583名病例中的训练和评估中，具有较高的2年进展自由生存率（PFS）准确率。<details>
<summary>Abstract</summary>
Diffuse Large B-cell Lymphoma (DLBCL) is a lymphatic cancer involving one or more lymph nodes and extranodal sites. Its diagnostic and follow-up rely on Positron Emission Tomography (PET) and Computed Tomography (CT). After diagnosis, the number of nonresponding patients to standard front-line therapy remains significant (30-40%). This work aims to develop a computer-aided approach to identify high-risk patients requiring adapted treatment by efficiently exploiting all the information available for each patient, including both clinical and image data. We propose a method based on recent graph neural networks that combine imaging information from multiple lesions, and a cross-attention module to integrate different data modalities efficiently. The model is trained and evaluated on a private prospective multicentric dataset of 583 patients. Experimental results show that our proposed method outperforms classical supervised methods based on either clinical, imaging or both clinical and imaging data for the 2-year progression-free survival (PFS) classification accuracy.
</details>
<details>
<summary>摘要</summary>
大细胞淋巴癌（DLBCL）是一种淋巴癌细胞扩散到一个或多个 лимф节和外周组织。其诊断和跟踪凭据 Positron Emission Tomography（PET）和计算机 Tomography（CT）。诊断后，标准前线治疗不响应病人的比例仍然很高（30-40%）。这项工作的目标是开发一种基于计算机技术的方法，以提高高风险患者的个性化治疗方案，通过有效地利用每个患者的所有信息，包括临床和图像数据。我们提出一种基于最新的图 neural networks 的方法，将多个肿瘤的图像信息集成，并使用交叉注意模块来有效地集成不同数据模式。模型在一个私人前推multicentric dataset上训练和评估，实验结果显示，我们的提议方法在2年内生存率（PFS）的分类准确率上超过了传统的指导方法，基于临床、图像或两者的数据。
</details></li>
</ul>
<hr>
<h2 id="Faithful-Path-Language-Modelling-for-Explainable-Recommendation-over-Knowledge-Graph"><a href="#Faithful-Path-Language-Modelling-for-Explainable-Recommendation-over-Knowledge-Graph" class="headerlink" title="Faithful Path Language Modelling for Explainable Recommendation over Knowledge Graph"></a>Faithful Path Language Modelling for Explainable Recommendation over Knowledge Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16452">http://arxiv.org/abs/2310.16452</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giacomo Balloccu, Ludovico Boratto, Christian Cancedda, Gianni Fenu, Mirko Marras</li>
<li>for: 这篇论文旨在提高推荐系统的透明度，通过基于知识图的路径理解方法。</li>
<li>methods: 本文提出了一种新的方法，即PEARLM，它通过语言模型来有效地捕捉用户行为和产品知识，并将实体和关系在同一个优化空间中归一化。</li>
<li>results: 对两个数据集进行实验，PEARLM方法比州先进的基elines表现出色，可以更好地捕捉用户的偏好和嗜好。Here’s the English version for reference:</li>
<li>for: This paper aims to improve the transparency of recommendation systems by using path reasoning methods over knowledge graphs.</li>
<li>methods: The proposed method, PEARLM, efficiently captures user behavior and product-side knowledge through language modeling, and unifies entities and relations in the same optimization space.</li>
<li>results: Experimental results on two datasets show the effectiveness of PEARLM compared to state-of-the-art baselines.<details>
<summary>Abstract</summary>
Path reasoning methods over knowledge graphs have gained popularity for their potential to improve transparency in recommender systems. However, the resulting models still rely on pre-trained knowledge graph embeddings, fail to fully exploit the interdependence between entities and relations in the KG for recommendation, and may generate inaccurate explanations. In this paper, we introduce PEARLM, a novel approach that efficiently captures user behaviour and product-side knowledge through language modelling. With our approach, knowledge graph embeddings are directly learned from paths over the KG by the language model, which also unifies entities and relations in the same optimisation space. Constraints on the sequence decoding additionally guarantee path faithfulness with respect to the KG. Experiments on two datasets show the effectiveness of our approach compared to state-of-the-art baselines. Source code and datasets: AVAILABLE AFTER GETTING ACCEPTED.
</details>
<details>
<summary>摘要</summary>
<SYS> translate_text="Path reasoning methods over knowledge graphs have gained popularity for their potential to improve transparency in recommender systems. However, the resulting models still rely on pre-trained knowledge graph embeddings, fail to fully exploit the interdependence between entities and relations in the KG for recommendation, and may generate inaccurate explanations. In this paper, we introduce PEARLM, a novel approach that efficiently captures user behavior and product-side knowledge through language modeling. With our approach, knowledge graph embeddings are directly learned from paths over the KG by the language model, which also unifies entities and relations in the same optimization space. Constraints on the sequence decoding additionally guarantee path faithfulness with respect to the KG. Experiments on two datasets show the effectiveness of our approach compared to state-of-the-art baselines. Source code and datasets: AVAILABLE AFTER GETTING ACCEPTED."</SYS>Here's the translation in Simplified Chinese: PATH 理解方法在知识Graph中得到了广泛的应用，因为它们可以提高推荐系统的透明度。然而，现有的模型仍然依赖于预训练的知识Graph嵌入，不充分利用知识Graph中Entity和关系之间的互相依赖关系，并可能生成错误的解释。在这篇论文中，我们介绍了PEARLM，一种新的方法，可以效率地捕捉用户行为和产品 сторо面知识通过语言模型。我们的方法直接从知识Graph中的路径上学习语言模型，同时也将Entity和关系嵌入到同一个优化空间中。另外，对序列解码加入约束，保证路径的准确性与知识Graph相关。在两个数据集上进行了实验，比较了我们的方法与现有的基elines。源代码和数据集：接受后提供。
</details></li>
</ul>
<hr>
<h2 id="Diversity-Enhanced-Narrative-Question-Generation-for-Storybooks"><a href="#Diversity-Enhanced-Narrative-Question-Generation-for-Storybooks" class="headerlink" title="Diversity Enhanced Narrative Question Generation for Storybooks"></a>Diversity Enhanced Narrative Question Generation for Storybooks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16446">http://arxiv.org/abs/2310.16446</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hkyoon95/mqg">https://github.com/hkyoon95/mqg</a></li>
<li>paper_authors: Hokeun Yoon, JinYeong Bak</li>
<li>for: 提高学习或对话环境中的理解、参与度、评估和总效果。</li>
<li>methods: 使用多个问题生成模型（mQG），通过专注于上下文和问题来生成多个、多样化的、可回答的问题。</li>
<li>results: 在 FairytaleQA 数据集上取得了优异的评估结果，并在零学习情况下应用于 TellMeWhy 和 SQuAD1.1 数据集上得到了扎实的结果。<details>
<summary>Abstract</summary>
Question generation (QG) from a given context can enhance comprehension, engagement, assessment, and overall efficacy in learning or conversational environments. Despite recent advancements in QG, the challenge of enhancing or measuring the diversity of generated questions often remains unaddressed. In this paper, we introduce a multi-question generation model (mQG), which is capable of generating multiple, diverse, and answerable questions by focusing on context and questions. To validate the answerability of the generated questions, we employ a SQuAD2.0 fine-tuned question answering model, classifying the questions as answerable or not. We train and evaluate mQG on the FairytaleQA dataset, a well-structured QA dataset based on storybooks, with narrative questions. We further apply a zero-shot adaptation on the TellMeWhy and SQuAD1.1 datasets. mQG shows promising results across various evaluation metrics, among strong baselines.
</details>
<details>
<summary>摘要</summary>
Question generation（QG）从给定的上下文中可以提高理解、参与度、评估和总体效果在学习或对话环境中。尽管最近在QG方面有所进步，但仍然有很多问题的多样性增进或评估的挑战。在这篇论文中，我们提出了多个问题生成模型（mQG），可以生成多个、多样的和可答案的问题，通过关注上下文和问题来做这个。为验证生成的问题是否可答，我们使用了SQuAD2.0 Fine-tuned问题回答模型，将问题分为可答和不可答两类。我们在FairytaleQA dataset上训练和评估mQG，该dataset基于故事书的问题。此外，我们还对TellMeWhy和SQuAD1.1 datasets进行零容量适应。mQG在不同的评价指标上表现出色，与强基线相比。
</details></li>
</ul>
<hr>
<h2 id="An-Integrative-Paradigm-for-Enhanced-Stroke-Prediction-Synergizing-XGBoost-and-xDeepFM-Algorithms"><a href="#An-Integrative-Paradigm-for-Enhanced-Stroke-Prediction-Synergizing-XGBoost-and-xDeepFM-Algorithms" class="headerlink" title="An Integrative Paradigm for Enhanced Stroke Prediction: Synergizing XGBoost and xDeepFM Algorithms"></a>An Integrative Paradigm for Enhanced Stroke Prediction: Synergizing XGBoost and xDeepFM Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16430">http://arxiv.org/abs/2310.16430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weinan Dai, Yifeng Jiang, Chengjie Mou, Chongyu Zhang</li>
<li>for: 预测中风的目的是为了预防和管理这种残割性疾病。</li>
<li>methods: 本研究使用了一个完整的数据集，并提出了一个集成模型，该模型将XGBoost和xDeepFM算法相结合。</li>
<li>results: 我们通过严格的实验验证了我们的集成模型的有效性，并与其他模型进行比较，从而获得了有价值的发现，以及对机器学习和深度学习技术在中风预测领域的贡献。<details>
<summary>Abstract</summary>
Stroke prediction plays a crucial role in preventing and managing this debilitating condition. In this study, we address the challenge of stroke prediction using a comprehensive dataset, and propose an ensemble model that combines the power of XGBoost and xDeepFM algorithms. Our work aims to improve upon existing stroke prediction models by achieving higher accuracy and robustness. Through rigorous experimentation, we validate the effectiveness of our ensemble model using the AUC metric. Through comparing our findings with those of other models in the field, we gain valuable insights into the merits and drawbacks of various approaches. This, in turn, contributes significantly to the progress of machine learning and deep learning techniques specifically in the domain of stroke prediction.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>roke 预测在stroke 的预防和管理中发挥关键作用。在这个研究中，我们面临roke 预测挑战，使用了 comprehensive 数据集，并提议一种ensemble 模型，将 XGBoost 和 xDeepFM 算法相结合。我们的工作目的是提高现有roke 预测模型的准确率和可靠性。通过严格的实验，我们验证了我们的ensemble 模型的有效性，使用 AUC 指标。通过与其他模型在领域中比较我们的发现，我们获得了对machine learning 和 deep learning 技术在roke 预测中的应用所得的有价值的理解。
</details></li>
</ul>
<hr>
<h2 id="Graph-Agent-Explicit-Reasoning-Agent-for-Graphs"><a href="#Graph-Agent-Explicit-Reasoning-Agent-for-Graphs" class="headerlink" title="Graph Agent: Explicit Reasoning Agent for Graphs"></a>Graph Agent: Explicit Reasoning Agent for Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16421">http://arxiv.org/abs/2310.16421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinyong Wang, Zhenxiang Gao, Rong Xu</li>
<li>for: 本文旨在提供一种基于大语言模型、印uctive-deductive 逻辑模块和长期记忆的知识图reasoning方法，以提高复杂的知识图 reasoning任务的效果。</li>
<li>methods: 本文提出了一种名为Graph Agent（GA）的智能代理方法，该方法利用大语言模型（LLM）、印uctive-deductive 逻辑模块和长期记忆来进行知识图reasoning任务。GA通过将图结构转换成文本数据，使得LLM可以处理、分析和提供预测结果，同时也可以提供人类可读的解释。</li>
<li>results: 本文的实验结果表明，GA在节点分类和链接预测任务上达到了状态的术性表现，具体的Result分别为Cora数据集的90.65%, PubMed数据集的95.48%, PrimeKG数据集的89.32%。相比之下，现有的GNN和transformer模型，GA具有显著的显式逻辑能力、免训练、易于适应不同的知识图reasoning任务的优势。<details>
<summary>Abstract</summary>
Graph embedding methods such as Graph Neural Networks (GNNs) and Graph Transformers have contributed to the development of graph reasoning algorithms for various tasks on knowledge graphs. However, the lack of interpretability and explainability of graph embedding methods has limited their applicability in scenarios requiring explicit reasoning. In this paper, we introduce the Graph Agent (GA), an intelligent agent methodology of leveraging large language models (LLMs), inductive-deductive reasoning modules, and long-term memory for knowledge graph reasoning tasks. GA integrates aspects of symbolic reasoning and existing graph embedding methods to provide an innovative approach for complex graph reasoning tasks. By converting graph structures into textual data, GA enables LLMs to process, reason, and provide predictions alongside human-interpretable explanations. The effectiveness of the GA was evaluated on node classification and link prediction tasks. Results showed that GA reached state-of-the-art performance, demonstrating accuracy of 90.65%, 95.48%, and 89.32% on Cora, PubMed, and PrimeKG datasets, respectively. Compared to existing GNN and transformer models, GA offered advantages of explicit reasoning ability, free-of-training, easy adaption to various graph reasoning tasks
</details>
<details>
<summary>摘要</summary>
graph embedding方法如graph neural networks (GNNs)和graph transformers已经为知识图reasoning任务提供了贡献。然而，graph embedding方法的 interpretability和可解释性限制了它们在需要显式reasoning的场景下的应用。在这篇论文中，我们介绍了Graph Agent (GA)，一种基于大型自然语言模型（LLMs）、推理模块和长期记忆的知识图reasoning方法。GA结合了symbolic reasoning和现有的graph embedding方法，提供了一种创新的graph reasoning任务approach。通过将graph结构转化为文本数据，GA使得LLMs可以处理、理解和提供预测，同时提供人类可解释的解释。GA在节点分类和链接预测任务上进行了评估，结果显示GA达到了状态之 искусственный智能的性能，具体的数据如下：在Cora、PubMed和PrimeKG datasets上，GA的准确率分别达到了90.65%、95.48%和89.32%。相比之前的GNN和transformer模型，GA具有显式reasoning能力、免训练和适应不同的graph reasoning任务的优势。
</details></li>
</ul>
<hr>
<h2 id="Balancing-Augmentation-with-Edge-Utility-Filter-for-Signed-GNNs"><a href="#Balancing-Augmentation-with-Edge-Utility-Filter-for-Signed-GNNs" class="headerlink" title="Balancing Augmentation with Edge-Utility Filter for Signed GNNs"></a>Balancing Augmentation with Edge-Utility Filter for Signed GNNs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16862">http://arxiv.org/abs/2310.16862</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ke-Jia Chen, Yaming Ji, Youran Qu, Chuhan Xu</li>
<li>for: 这种论文的目的是提高signed graph neural networks（SGNNs）的稳定性和性能，通过增强graph的结构和semantic balance。</li>
<li>methods: 该论文提出了一种增强策略，包括测量每个负边的utilty，并 selectively增强graph的结构和semantic balance。</li>
<li>results: 实验表明，该方法可以显著提高SGNN的性能和普适性，并且可以在五种实际 dataset中进行链接预测。<details>
<summary>Abstract</summary>
Signed graph neural networks (SGNNs) has recently drawn more attention as many real-world networks are signed networks containing two types of edges: positive and negative. The existence of negative edges affects the SGNN robustness on two aspects. One is the semantic imbalance as the negative edges are usually hard to obtain though they can provide potentially useful information. The other is the structural unbalance, e.g. unbalanced triangles, an indication of incompatible relationship among nodes. In this paper, we propose a balancing augmentation method to address the above two aspects for SGNNs. Firstly, the utility of each negative edge is measured by calculating its occurrence in unbalanced structures. Secondly, the original signed graph is selectively augmented with the use of (1) an edge perturbation regulator to balance the number of positive and negative edges and to determine the ratio of perturbed edges to original edges and (2) an edge utility filter to remove the negative edges with low utility to make the graph structure more balanced. Finally, a SGNN is trained on the augmented graph which effectively explores the credible relationships. A detailed theoretical analysis is also conducted to prove the effectiveness of each module. Experiments on five real-world datasets in link prediction demonstrate that our method has the advantages of effectiveness and generalization and can significantly improve the performance of SGNN backbones.
</details>
<details>
<summary>摘要</summary>
signed 图 neural networks (SGNNs) 在最近几年引起了更多的关注，因为许多实际网络是带有两种类型的边：正向和负向的边。存在负向边的存在会对 SGNN 的Robustness 产生两种方面的影响。一个是 semantics 不均衡，负向边通常很难以获得，但它们可能提供有用信息。另一个是结构不均衡，例如不均衡的三角形，这表明节点之间的关系不兼容。在这篇论文中，我们提议一种平衡增强方法，以解决上述两个方面的问题。首先，我们测量每个负向边的使用价值，计算它们在不均衡结构中的发生频率。其次，我们使用（1）边扰动调节器来平衡正向和负向边的数量，并确定扰动的比例。（2）边用途筛选器来从原始签入的图中删除低使用价值的负向边，以使得图的结构更加平衡。最后，我们在增强后的图上训练 SGNN，以便更好地探索有效的关系。我们还进行了详细的理论分析，以证明每个模块的有效性。在五个实际链接预测任务上进行了实验，结果表明我们的方法具有效果和普适性，可以显著提高 SGNN 的表现。
</details></li>
</ul>
<hr>
<h2 id="Open-Knowledge-Base-Canonicalization-with-Multi-task-Unlearning"><a href="#Open-Knowledge-Base-Canonicalization-with-Multi-task-Unlearning" class="headerlink" title="Open Knowledge Base Canonicalization with Multi-task Unlearning"></a>Open Knowledge Base Canonicalization with Multi-task Unlearning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16419">http://arxiv.org/abs/2310.16419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bingchen Liu, Shihao Hou, Weixin Zeng, Xiang Zhao, Shijun Liu, Li Pan</li>
<li>for:  OKB canonicalization （大规模移动计算领域中的知识库建设）</li>
<li>methods:  machine unlearning（机器忘记） + clustering + KGE learning（知识图加 embeddings 学习）</li>
<li>results:  advanced machine unlearning effects（提高机器忘记效果）In more detail, the paper proposes a multi-task unlearning framework called MulCanon, which utilizes the noise characteristics in the diffusion model to achieve machine unlearning for data in OKB canonicalization. The framework unifies the learning objectives of diffusion model, KGE, and clustering algorithms, and adopts a two-step multi-task learning paradigm for training. The experimental study on popular OKB canonicalization datasets shows that MulCanon achieves advanced machine unlearning effects.<details>
<summary>Abstract</summary>
The construction of large open knowledge bases (OKBs) is integral to many applications in the field of mobile computing. Noun phrases and relational phrases in OKBs often suffer from redundancy and ambiguity, which calls for the investigation on OKB canonicalization. However, in order to meet the requirements of some privacy protection regulations and to ensure the timeliness of the data, the canonicalized OKB often needs to remove some sensitive information or outdated data. The machine unlearning in OKB canonicalization is an excellent solution to the above problem. Current solutions address OKB canonicalization by devising advanced clustering algorithms and using knowledge graph embedding (KGE) to further facilitate the canonicalization process. Effective schemes are urgently needed to fully synergise machine unlearning with clustering and KGE learning. To this end, we put forward a multi-task unlearning framework, namely MulCanon, to tackle machine unlearning problem in OKB canonicalization. Specifically, the noise characteristics in the diffusion model are utilized to achieve the effect of machine unlearning for data in OKB. MulCanon unifies the learning objectives of diffusion model, KGE and clustering algorithms, and adopts a two-step multi-task learning paradigm for training. A thorough experimental study on popular OKB canonicalization datasets validates that MulCanon achieves advanced machine unlearning effects.
</details>
<details>
<summary>摘要</summary>
大规模开放知识库（OKB）的建构对移动计算应用程序而言是非常重要的。OKB中的名实词和关系词 oftentimes受到重复和歧义的影响，这种情况需要研究OKB canonicalization。然而，为了遵守一些隐私保护法规和保证数据的时效性， canonicalized OKB often需要 removing some sensitive information or outdated data。机器学习 inverse в OKB canonicalization 是一个优秀的解决方案。现有的解决方案通过设计高级划分算法和使用知识图嵌入（KGE）来进一步促进 canonicalization 过程。有效的方案是必要的，以全面融合机器学习 inverse 与划分和 KGE 学习。为此，我们提出了一种多任务学习框架，即 MulCanon，用于解决机器学习 inverse 问题在 OKB canonicalization 中。 Specifically, MulCanon 利用了Diffusion model中的噪音特征来实现数据中的机器学习 inverse。MulCanon 将 diffusion model、KGE 和划分算法的学习目标统一，采用了两步多任务学习 paradigm for training。经过对popular OKB canonicalization 数据集的严格实验 validate that MulCanon 可以 achieve advanced machine unlearning effects.
</details></li>
</ul>
<hr>
<h2 id="Bridging-the-Human-AI-Knowledge-Gap-Concept-Discovery-and-Transfer-in-AlphaZero"><a href="#Bridging-the-Human-AI-Knowledge-Gap-Concept-Discovery-and-Transfer-in-AlphaZero" class="headerlink" title="Bridging the Human-AI Knowledge Gap: Concept Discovery and Transfer in AlphaZero"></a>Bridging the Human-AI Knowledge Gap: Concept Discovery and Transfer in AlphaZero</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16410">http://arxiv.org/abs/2310.16410</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lisa Schut, Nenad Tomasev, Tom McGrath, Demis Hassabis, Ulrich Paquet, Been Kim<br>for: 这篇论文旨在探讨如何利用高性能的人工智能系统（AlphaZero）中嵌入的隐藏知识，以提高人类专家性能。methods: 作者提出了一种新的方法，用于从AlphaZero中提取新的棋盘概念，并进行人类学习和评估。results: 研究表明，AlphaZero可能嵌入的知识不仅超过了人类知识，还可以成功地传授给人类专家。在人类研究中，四位世界级国际象棋大师通过解决提交的概念原型位置，得到了改进。<details>
<summary>Abstract</summary>
Artificial Intelligence (AI) systems have made remarkable progress, attaining super-human performance across various domains. This presents us with an opportunity to further human knowledge and improve human expert performance by leveraging the hidden knowledge encoded within these highly performant AI systems. Yet, this knowledge is often hard to extract, and may be hard to understand or learn from. Here, we show that this is possible by proposing a new method that allows us to extract new chess concepts in AlphaZero, an AI system that mastered the game of chess via self-play without human supervision. Our analysis indicates that AlphaZero may encode knowledge that extends beyond the existing human knowledge, but knowledge that is ultimately not beyond human grasp, and can be successfully learned from. In a human study, we show that these concepts are learnable by top human experts, as four top chess grandmasters show improvements in solving the presented concept prototype positions. This marks an important first milestone in advancing the frontier of human knowledge by leveraging AI; a development that could bear profound implications and help us shape how we interact with AI systems across many AI applications.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）系统已经取得了很大的进步，在不同领域达到了人类超越性表现。这给我们提供了一个机会，通过利用AI系统中隐藏的知识来进一步推动人类知识和专家表现。然而，这些知识可能很难提取，并且可能很难理解或学习。在这里，我们提出了一种新的方法，可以从AlphaZero AI系统中提取新的棋盘概念。我们的分析表明，AlphaZero可能具有超越人类知识的知识，但这些知识并不是不可以被人类理解和学习的。在人类研究中，我们发现，这些概念可以被四位高级国际象棋大师学习，他们在给出的概念原型位置中解决问题的能力得到了改进。这标志着我们在利用AI推动人类知识的前夕，这可能会对许多AI应用程序产生深远的影响，并帮助我们如何与AI系统交互。
</details></li>
</ul>
<hr>
<h2 id="Challenges-of-Radio-Frequency-Fingerprinting-From-Data-Collection-to-Deployment"><a href="#Challenges-of-Radio-Frequency-Fingerprinting-From-Data-Collection-to-Deployment" class="headerlink" title="Challenges of Radio Frequency Fingerprinting: From Data Collection to Deployment"></a>Challenges of Radio Frequency Fingerprinting: From Data Collection to Deployment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16406">http://arxiv.org/abs/2310.16406</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saeif Alhazbi, Ahmed Hussain, Savio Sciancalepore, Gabriele Oligeri, Panos Papadimitratos</li>
<li>for: 本文旨在探讨Radio Frequency Fingerprinting（RFF）技术如何使用机器学习（ML）和深度学习（DL）来实现无线设备认证。</li>
<li>methods: 本文使用的方法包括RFF技术和ML&#x2F;DL技术，并对这些技术的缺陷和挑战进行分析。</li>
<li>results: 本文的研究发现现有的RFF系统尚未能够在实际应用中使用，并且存在许多挑战和问题。未来的研究应该关注这些问题，以便实现RFF系统的真正应用。<details>
<summary>Abstract</summary>
Radio Frequency Fingerprinting (RFF) techniques promise to authenticate wireless devices at the physical layer based on inherent hardware imperfections introduced during manufacturing. Such RF transmitter imperfections are reflected into over-the-air signals, allowing receivers to accurately identify the RF transmitting source. Recent advances in Machine Learning, particularly in Deep Learning (DL), have improved the ability of RFF systems to extract and learn complex features that make up the device-specific fingerprint. However, integrating DL techniques with RFF and operating the system in real-world scenarios presents numerous challenges. This article identifies and analyzes these challenges while considering the three reference phases of any DL-based RFF system: (i) data collection and preprocessing, (ii) training, and finally, (iii) deployment. Our investigation points out the current open problems that prevent real deployment of RFF while discussing promising future directions, thus paving the way for further research in the area.
</details>
<details>
<summary>摘要</summary>
radio 频率指纹技术 (RFF) 承诺通过物理层认证无线设备，基于生产过程中引入的固有硬件瑕疵。这些 RF 发送器瑕疵会在通过空气信号中反射，让接收器准确地识别 RF 发送源。近年来，机器学习技术的进步，特别是深度学习 (DL)，提高了 RFF 系统EXTRACT和学习复杂的设备特征。然而，将 DL 技术与 RFF 集成并在实际场景中运行充满挑战。这篇文章确认并分析了这些挑战，并考虑了任何 DL-based RFF 系统的三个参考阶段：（一）数据收集和处理，（二）训练，最后（三）部署。我们的调查发现当前还存在许多未解决的问题，阻碍 RFF 的实际应用，并讨论了未来研究的可能性，以便继续深入研究这一领域。
</details></li>
</ul>
<hr>
<h2 id="Fuse-Your-Latents-Video-Editing-with-Multi-source-Latent-Diffusion-Models"><a href="#Fuse-Your-Latents-Video-Editing-with-Multi-source-Latent-Diffusion-Models" class="headerlink" title="Fuse Your Latents: Video Editing with Multi-source Latent Diffusion Models"></a>Fuse Your Latents: Video Editing with Multi-source Latent Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16400">http://arxiv.org/abs/2310.16400</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyi Lu, Xing Zhang, Jiaxi Gu, Hang Xu, Renjing Pei, Songcen Xu, Zuxuan Wu</li>
<li>for: 文章目的是提出一种无需训练的框架，以实现根据文本指导的视频编辑。</li>
<li>methods: 方法是将图像LDM和视频LDM的latent拟合在denoising过程中，以保持视频LDM中的时间一致性，同时充分利用图像LDM中的高精度。</li>
<li>results: 对比于传统方法，FLDM可以提高文本对齐和时间一致性的编辑视频质量。<details>
<summary>Abstract</summary>
Latent Diffusion Models (LDMs) are renowned for their powerful capabilities in image and video synthesis. Yet, video editing methods suffer from insufficient pre-training data or video-by-video re-training cost. In addressing this gap, we propose FLDM (Fused Latent Diffusion Model), a training-free framework to achieve text-guided video editing by applying off-the-shelf image editing methods in video LDMs. Specifically, FLDM fuses latents from an image LDM and an video LDM during the denoising process. In this way, temporal consistency can be kept with video LDM while high-fidelity from the image LDM can also be exploited. Meanwhile, FLDM possesses high flexibility since both image LDM and video LDM can be replaced so advanced image editing methods such as InstructPix2Pix and ControlNet can be exploited. To the best of our knowledge, FLDM is the first method to adapt off-the-shelf image editing methods into video LDMs for video editing. Extensive quantitative and qualitative experiments demonstrate that FLDM can improve the textual alignment and temporal consistency of edited videos.
</details>
<details>
<summary>摘要</summary>
Latent Diffusion Models (LDMs) 是以强大的能力在图像和视频生成而著称的。然而，视频编辑方法受到缺乏预训练数据或视频视频重新训练成本的限制。为了填补这一漏洞，我们提议了 FLDM（混合潜在扩散模型），一种不需要训练的框架，通过在视频 LDM 中应用市场上ready-to-use的图像编辑方法来实现文本指导的视频编辑。具体来说，FLDM 在杂化过程中将图像 LDM 和视频 LDM 的潜在特征进行混合。这样可以保持视频 LDM 中的时间一致性，同时也可以利用图像 LDM 中的高精度特性。此外，FLDM 具有高灵活性，因为图像 LDM 和视频 LDM 都可以被更高级的图像编辑方法所取代，例如 InstructPix2Pix 和 ControlNet。根据我们所知，FLDM 是首次将市场上ready-to-use的图像编辑方法应用到视频 LDM 中进行视频编辑。EXT 广泛的量化和质量测试表明，FLDM 可以改善编辑后的文本对齐和时间一致性。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-General-Purpose-AI-with-Psychometrics"><a href="#Evaluating-General-Purpose-AI-with-Psychometrics" class="headerlink" title="Evaluating General-Purpose AI with Psychometrics"></a>Evaluating General-Purpose AI with Psychometrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16379">http://arxiv.org/abs/2310.16379</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiting Wang, Liming Jiang, Jose Hernandez-Orallo, Luning Sun, David Stillwell, Fang Luo, Xing Xie</li>
<li>for: This paper aims to improve the evaluation of general-purpose AI systems by incorporating psychometrics, the science of psychological measurement.</li>
<li>methods: The proposed method uses psychometrics to identify and measure the latent constructs that underlie performance across multiple tasks, providing a more comprehensive and rigorous approach to evaluating AI systems.</li>
<li>results: The authors propose a framework for integrating psychometrics with AI and explore future opportunities for doing so, with the goal of improving the evaluation and understanding of general-purpose AI systems.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目标是通过吸收心理测量科学（psychometrics）来改善普适人工智能系统的评估。</li>
<li>methods: 该方法使用心理测量科学来识别和测量多个任务下的潜在构uct，从而提供一种更加全面和准确的人工智能系统评估方法。</li>
<li>results: 作者提出了将心理测量科学与人工智能集成的框架，并探讨未来可能性，以提高普适人工智能系统的评估和理解。<details>
<summary>Abstract</summary>
Artificial intelligence (AI) has witnessed an evolution from task-specific to general-purpose systems that trend toward human versatility. As AI systems begin to play pivotal roles in society, it is important to ensure that they are adequately evaluated. Current AI benchmarks typically assess performance on collections of specific tasks. This has drawbacks when used for assessing general-purpose AI systems. First, it is difficult to predict whether AI systems could complete a new task it has never seen or that did not previously exist. Second, these benchmarks often focus on overall performance metrics, potentially overlooking the finer details crucial for making informed decisions. Lastly, there are growing concerns about the reliability of existing benchmarks and questions about what is being measured. To solve these challenges, this paper suggests that psychometrics, the science of psychological measurement, should be placed at the core of evaluating general-purpose AI. Psychometrics provides a rigorous methodology for identifying and measuring the latent constructs that underlie performance across multiple tasks. We discuss its merits, warn against potential pitfalls, and propose a framework for putting it into practice. Finally, we explore future opportunities to integrate psychometrics with AI.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）在演化过程中从任务特定向广泛应用系统，趋向人类多能。随着AI系统在社会中扮演着重要角色，因此需要确保它们得到了足够的评估。现有的AIbenchmarks通常评估特定任务集合的性能。这有一些缺点，如果用于评估通用AI系统。首先，难以预测AI系统能否完成它从未看过或者不存在的新任务。其次，这些benchmarks通常专注于总性能指标，可能忽略了决策过程中的细节。最后，有关现有benchmarks的可靠性和评估的问题也在提出。为解决这些挑战，这篇论文建议将心理测量（psychometrics）置于AI评估的核心。心理测量提供了一种严格的方法来识别和测量在多个任务中表现的隐藏构造。我们讲述其优点、警告 против潜在的陷阱，并提出一个实施框架。最后，我们探讨将心理测量与AI集成的未来机会。
</details></li>
</ul>
<hr>
<h2 id="GADY-Unsupervised-Anomaly-Detection-on-Dynamic-Graphs"><a href="#GADY-Unsupervised-Anomaly-Detection-on-Dynamic-Graphs" class="headerlink" title="GADY: Unsupervised Anomaly Detection on Dynamic Graphs"></a>GADY: Unsupervised Anomaly Detection on Dynamic Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16376">http://arxiv.org/abs/2310.16376</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiqi Lou, Qingyue Zhang, Shujie Yang, Yuyang Tian, Zhaoxuan Tan, Minnan Luo</li>
<li>for: 检测动态图中异常行为，即在图和其时间信息中检测entity的行为异常。</li>
<li>methods: 我们提出了一种基于自动生成的无监督图动态异常检测方法（GADY），用于解决现有方法面临的动态结构建构挑战和负样本生成挑战。</li>
<li>results: 我们的GADY方法在三个真实世界数据集上表现出了显著的优异，与前一个状态艺术法相比显著提高了性能。<details>
<summary>Abstract</summary>
Anomaly detection on dynamic graphs refers to detecting entities whose behaviors obviously deviate from the norms observed within graphs and their temporal information. This field has drawn increasing attention due to its application in finance, network security, social networks, and more. However, existing methods face two challenges: dynamic structure constructing challenge - difficulties in capturing graph structure with complex time information and negative sampling challenge - unable to construct excellent negative samples for unsupervised learning. To address these challenges, we propose Unsupervised Generative Anomaly Detection on Dynamic Graphs (GADY). To tackle the first challenge, we propose a continuous dynamic graph model to capture the fine-grained information, which breaks the limit of existing discrete methods. Specifically, we employ a message-passing framework combined with positional features to get edge embeddings, which are decoded to identify anomalies. For the second challenge, we pioneer the use of Generative Adversarial Networks to generate negative interactions. Moreover, we design a loss function to alter the training goal of the generator while ensuring the diversity and quality of generated samples. Extensive experiments demonstrate that our proposed GADY significantly outperforms the previous state-of-the-art method on three real-world datasets. Supplementary experiments further validate the effectiveness of our model design and the necessity of each module.
</details>
<details>
<summary>摘要</summary>
“异常探测在动态图表上指的是检测图表中的元素，其行为明显与常规模式不同。这个领域在金融、网络安全、社交网络等领域获得了越来越多的注意。但现有方法面临两个挑战：动态结构建构挑战 - 对复杂时间信息的图表结构捕捉困难，以及负数样本挑战 - 无法建立出色的负数样本供无监督学习。为解决这两个挑战，我们提出了不supervised生成异常探测方法（GADY）。”“为了解决第一个挑战，我们提出了一个连续动态图表模型，以捕捉细节信息。这个模型与现有的缓存方法不同，可以更好地捕捉图表中的细节变化。具体来说，我们运用了讯息传递框架，融合 pozitional 特征以获得边嵌入，这些边嵌入可以转换为异常探测。”“对于第二个挑战，我们创新使用生成对抗网络来生成负数样本。此外，我们设计了一个损失函数，以调整生成器的训练目标，并确保生成的样本多样性和质量。实验结果显示，我们的提案GADY对三个真实世界数据集的表现明显超越了前一代方法。补充实验更进一步验证了我们的模型设计和各模块的必要性。”
</details></li>
</ul>
<hr>
<h2 id="InstructPTS-Instruction-Tuning-LLMs-for-Product-Title-Summarization"><a href="#InstructPTS-Instruction-Tuning-LLMs-for-Product-Title-Summarization" class="headerlink" title="InstructPTS: Instruction-Tuning LLMs for Product Title Summarization"></a>InstructPTS: Instruction-Tuning LLMs for Product Title Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16361">http://arxiv.org/abs/2310.16361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Besnik Fetahu, Zhiyu Chen, Oleg Rokhlenko, Shervin Malmasi</li>
<li>for: 这个论文旨在提高电子商务产品目录中的商品标题概要，以便更好地支持推荐、问答和评论摘要等功能。</li>
<li>methods: 这个论文提出了一种可控的产品标题概要方法，基于最近的指令精度练习方法。该方法可以根据不同的标准（例如字数、包含特定短语等）生成匹配的产品标题概要。</li>
<li>results: 对实际电子商务目录进行了广泛的评估，结果显示，与简单的精度练习方法相比，该方法可以生成更准确的产品名称概要，提高了14和8个BLEU和ROUGE分数。<details>
<summary>Abstract</summary>
E-commerce product catalogs contain billions of items. Most products have lengthy titles, as sellers pack them with product attributes to improve retrieval, and highlight key product aspects. This results in a gap between such unnatural products titles, and how customers refer to them. It also limits how e-commerce stores can use these seller-provided titles for recommendation, QA, or review summarization.   Inspired by recent work on instruction-tuned LLMs, we present InstructPTS, a controllable approach for the task of Product Title Summarization (PTS). Trained using a novel instruction fine-tuning strategy, our approach is able to summarize product titles according to various criteria (e.g. number of words in a summary, inclusion of specific phrases, etc.). Extensive evaluation on a real-world e-commerce catalog shows that compared to simple fine-tuning of LLMs, our proposed approach can generate more accurate product name summaries, with an improvement of over 14 and 8 BLEU and ROUGE points, respectively.
</details>
<details>
<summary>摘要</summary>
电商产品目录中包含了数十亿个商品。大多数产品有很长的标题，卖家会将其填充产品特性，以提高检索和强调产品的关键特征。这会导致产品标题与客户的实际称呼之间出现一个差距，同时限制了电商店可以使用卖家提供的标题进行推荐、问答或评论摘要。我们受到最近的指令��unced LLMS的研究所 inspirited，我们提出了一种可控的产品标题摘要（PTS）方法。我们使用了一种新的指令细化策略来训练我们的方法，可以根据不同的标准（例如，摘要中单词数、包含特定短语等）来摘要产品标题。我们对实际的电商目录进行了广泛的评估，结果显示，相比于简单地细化LLMS，我们提出的方法可以生成更加准确的产品名称摘要，提高了14和8个BLEU和ROUGE分数。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Review-of-AI-enabled-Unmanned-Aerial-Vehicle-Trends-Vision-and-Challenges"><a href="#A-Comprehensive-Review-of-AI-enabled-Unmanned-Aerial-Vehicle-Trends-Vision-and-Challenges" class="headerlink" title="A Comprehensive Review of AI-enabled Unmanned Aerial Vehicle: Trends, Vision , and Challenges"></a>A Comprehensive Review of AI-enabled Unmanned Aerial Vehicle: Trends, Vision , and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16360">http://arxiv.org/abs/2310.16360</a></li>
<li>repo_url: None</li>
<li>paper_authors: Osim Kumar Pal, Md Sakib Hossain Shovon, M. F. Mridha, Jungpil Shin</li>
<li>for: This paper explores the potential of AI-powered UAVs in various applications, including navigation, object detection and tracking, wildlife monitoring, precision agriculture, rescue operations, surveillance, and communication among UAVs.</li>
<li>methods: The paper examines the use of AI techniques such as machine learning, computer vision, and deep learning to enable these applications, and discusses the challenges and limitations of these approaches.</li>
<li>results: The study highlights the potential of AI-powered UAVs to revolutionize industries such as agriculture, surveillance, and disaster management, while also raising ethical and safety concerns that need to be addressed.<details>
<summary>Abstract</summary>
In recent years, the combination of artificial intelligence (AI) and unmanned aerial vehicles (UAVs) has brought about advancements in various areas. This comprehensive analysis explores the changing landscape of AI-powered UAVs and friendly computing in their applications. It covers emerging trends, futuristic visions, and the inherent challenges that come with this relationship. The study examines how AI plays a role in enabling navigation, detecting and tracking objects, monitoring wildlife, enhancing precision agriculture, facilitating rescue operations, conducting surveillance activities, and establishing communication among UAVs using environmentally conscious computing techniques. By delving into the interaction between AI and UAVs, this analysis highlights the potential for these technologies to revolutionise industries such as agriculture, surveillance practices, disaster management strategies, and more. While envisioning possibilities, it also takes a look at ethical considerations, safety concerns, regulatory frameworks to be established, and the responsible deployment of AI-enhanced UAV systems. By consolidating insights from research endeavours in this field, this review provides an understanding of the evolving landscape of AI-powered UAVs while setting the stage for further exploration in this transformative domain.
</details>
<details>
<summary>摘要</summary>
The study examines how AI enables navigation, object detection and tracking, wildlife monitoring, precision agriculture, rescue operations, surveillance activities, and communication among UAVs using environmentally conscious computing techniques. By exploring the interaction between AI and UAVs, this analysis highlights the potential for these technologies to revolutionize industries such as agriculture, surveillance practices, disaster management strategies, and more.However, the analysis also considers ethical considerations, safety concerns, and regulatory frameworks to be established for the responsible deployment of AI-enhanced UAV systems. By consolidating insights from research in this field, this review provides an understanding of the evolving landscape of AI-powered UAVs and sets the stage for further exploration in this transformative domain.
</details></li>
</ul>
<hr>
<h2 id="AccoMontage-3-Full-Band-Accompaniment-Arrangement-via-Sequential-Style-Transfer-and-Multi-Track-Function-Prior"><a href="#AccoMontage-3-Full-Band-Accompaniment-Arrangement-via-Sequential-Style-Transfer-and-Multi-Track-Function-Prior" class="headerlink" title="AccoMontage-3: Full-Band Accompaniment Arrangement via Sequential Style Transfer and Multi-Track Function Prior"></a>AccoMontage-3: Full-Band Accompaniment Arrangement via Sequential Style Transfer and Multi-Track Function Prior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16334">http://arxiv.org/abs/2310.16334</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhaojw1998/accomontage-3">https://github.com/zhaojw1998/accomontage-3</a></li>
<li>paper_authors: Jingwei Zhao, Gus Xia, Ye Wang</li>
<li>for: 本研究旨在开发一种符号音乐自动化系统，可以基于输入的主题旋律和和声生成多轨、全乐队伴奏。</li>
<li>methods: 该系统包括三个模块，每个模块模拟了不同方面的全乐队作曲。第一个模块是一个钢琴编制器，通过精灵抽取和约束搜索，将约束式转移到和声中，生成钢琴伴奏。第二个模块是一个乐队编制器，根据乐曲的整体风格编码，将钢琴伴奏谱写成全乐队演奏。第三个模块是一个先前模型，用于描述乐曲的全局结构，从而在音乐上下文中应用样式转移。</li>
<li>results: 实验表明，该系统在与基准相比显著提高了表现，而且模块化设计具有有效的控制和音乐意义上的表达能力。<details>
<summary>Abstract</summary>
We propose AccoMontage-3, a symbolic music automation system capable of generating multi-track, full-band accompaniment based on the input of a lead melody with chords (i.e., a lead sheet). The system contains three modular components, each modelling a vital aspect of full-band composition. The first component is a piano arranger that generates piano accompaniment for the lead sheet by transferring texture styles to the chords using latent chord-texture disentanglement and heuristic retrieval of texture donors. The second component orchestrates the piano accompaniment score into full-band arrangement according to the orchestration style encoded by individual track functions. The third component, which connects the previous two, is a prior model characterizing the global structure of orchestration style over the whole piece of music. From end to end, the system learns to generate full-band accompaniment in a self-supervised fashion, applying style transfer at two levels of polyphonic composition: texture and orchestration. Experiments show that our system outperforms the baselines significantly, and the modular design offers effective controls in a musically meaningful way.
</details>
<details>
<summary>摘要</summary>
我们提出AccoMontage-3，一种符号音乐自动化系统，可以基于输入的主旋律和和声（即主稿）生成多轨、全团配乐。系统包括三个模块，每个模块都模拟了全团作曲中的一个重要方面。第一个模块是一个钢琴编制器，通过将文化样式传递到和声中的谱写法来生成钢琴伴奏。第二个模块将钢琴伴奏谱写成全团排版，根据每个乐器的特性和乐谱函数编码。第三个模块是一个先进的模型，用于模elling全团作曲风格的全局结构。从头到尾，系统通过自我超VI持学习生成全团配乐，并在多重复合作曲中应用样式转移。实验结果表明，我们的系统与基线相比有显著的优势，而模块化设计还提供了有效的控制方式，具有音乐意义上的 significances。
</details></li>
</ul>
<hr>
<h2 id="CoheSentia-A-Novel-Benchmark-of-Incremental-versus-Holistic-Assessment-of-Coherence-in-Generated-Texts"><a href="#CoheSentia-A-Novel-Benchmark-of-Incremental-versus-Holistic-Assessment-of-Coherence-in-Generated-Texts" class="headerlink" title="CoheSentia: A Novel Benchmark of Incremental versus Holistic Assessment of Coherence in Generated Texts"></a>CoheSentia: A Novel Benchmark of Incremental versus Holistic Assessment of Coherence in Generated Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16329">http://arxiv.org/abs/2310.16329</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aviya Maimon, Reut Tsarfaty<br>for: The paper aims to introduce a novel benchmark for assessing the human-perceived coherence of automatically generated texts.methods: The paper uses two annotation protocols to assess coherence: a global protocol that assigns a single coherence score, and an incremental protocol that scores sentence by sentence and pinpoints reasons for incoherence.results: The paper shows that the inter-annotator agreement in the incremental mode is higher than in the holistic alternative, and that standard language models fine-tuned for coherence detection show varied performance on the different factors contributing to (in)coherence. The results emphasize the need for developing more reliable methods for coherence assessment.Here is the simplified Chinese text in the format you requested:for: 本文目的是引入一个新的自动生成文本合理性评估标准。methods: 本文使用两种注释协议来评估合理性：一种全局协议，将单个合理性分数赋予，另一种逐句协议，每个句子都得到一个(不合理)分数，并指出了不合理的原因。results: 本文显示，逐句协议的间译员一致度高于全局协议，并且标准的语言模型经过训练后，对不同的合理性因素表现出了不一致的性能。结果强调了需要更加可靠的合理性评估方法的发展。<details>
<summary>Abstract</summary>
Coherence is a linguistic term that refers to the relations between small textual units (sentences, propositions), which make the text logically consistent and meaningful to the reader. With the advances of generative foundational models in NLP, there is a pressing need to automatically assess the human-perceived coherence of automatically generated texts. Up until now, little work has been done on explicitly assessing the coherence of generated texts and analyzing the factors contributing to (in)coherence. Previous work on the topic used other tasks, e.g., sentence reordering, as proxies of coherence, rather than approaching coherence detection heads on. In this paper, we introduce {\sc CoheSentia}, a novel benchmark of human-perceived coherence of automatically generated texts. Our annotation protocol reflects two perspectives; one is global, assigning a single coherence score, and the other is incremental, scoring sentence by sentence. The incremental method produces an (in)coherence score for each text fragment and also pinpoints reasons for incoherence at that point. Our benchmark contains 500 automatically-generated and human-annotated paragraphs, each annotated in both methods, by multiple raters. Our analysis shows that the inter-annotator agreement in the incremental mode is higher than in the holistic alternative, and our experiments show that standard LMs fine-tuned for coherence detection show varied performance on the different factors contributing to (in)coherence. All in all, these models yield unsatisfactory performance, emphasizing the need for developing more reliable methods for coherence assessment.
</details>
<details>
<summary>摘要</summary>
“凝聚”是一个语言学术语，指小文本单位（句子、命题）之间的关系，使文本具有逻辑连贯性和意义性 для读者。随着自然语言处理（NLP）的发展，有一定的需求以自动评估生成文本的人类感知凝聚性。到目前为止，对生成文本的凝聚性的评估几乎没有任何研究，而且对于生成文本中的凝聚性因素进行分析也没有充分的研究。在这篇文章中，我们引入了{\sc CoheSentia}，一个新的自动生成文本人类感知凝聚性的标准 bencmark。我们的标注协议包括两种角度：全球的标注方法，将文本的凝聚性评分为单一的数值，以及增量的标注方法，将每个句子的凝聚性评分为单一的数值，并且还能够确定各个点数的不凝聚原因。我们的标注集包含500个自动生成和人类标注的段落，每个段落都被多名标注者标注了两种方法。我们的分析显示，增量标注方法的间接协议比全球方法高，并且我们的实验显示，适用于凝聚性检测的标准语言模型（LM）在不同的凝聚性因素上表现不一。总之，这些模型在凝聚性检测方面表现不佳，强调需要发展更可靠的方法。
</details></li>
</ul>
<hr>
<h2 id="Modality-Agnostic-Self-Supervised-Learning-with-Meta-Learned-Masked-Auto-Encoder"><a href="#Modality-Agnostic-Self-Supervised-Learning-with-Meta-Learned-Masked-Auto-Encoder" class="headerlink" title="Modality-Agnostic Self-Supervised Learning with Meta-Learned Masked Auto-Encoder"></a>Modality-Agnostic Self-Supervised Learning with Meta-Learned Masked Auto-Encoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16318">http://arxiv.org/abs/2310.16318</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alinlab/MetaMAE">https://github.com/alinlab/MetaMAE</a></li>
<li>paper_authors: Huiwon Jang, Jihoon Tack, Daewon Choi, Jongheon Jeong, Jinwoo Shin</li>
<li>for: 本文旨在提出一种模型独立学习（Self-Supervised Learning，SSL）框架，可以在多个模式下进行学习。</li>
<li>methods: 本文使用Masked Auto-Encoder（MAE） architecture，并通过元学习来解释MAE为多Modalities的学习器。我们提出了两种高级元学习技术：首先，通过梯度基元学习来调整缺省的秘钥表示；其次，通过任务对比学习来确保秘钥表示与任务相关。</li>
<li>results: 我们在DABS模式独立学习 benchmark中进行了实验，并证明MetaMAE可以在多个模式下显著超越先前的基eline。<details>
<summary>Abstract</summary>
Despite its practical importance across a wide range of modalities, recent advances in self-supervised learning (SSL) have been primarily focused on a few well-curated domains, e.g., vision and language, often relying on their domain-specific knowledge. For example, Masked Auto-Encoder (MAE) has become one of the popular architectures in these domains, but less has explored its potential in other modalities. In this paper, we develop MAE as a unified, modality-agnostic SSL framework. In turn, we argue meta-learning as a key to interpreting MAE as a modality-agnostic learner, and propose enhancements to MAE from the motivation to jointly improve its SSL across diverse modalities, coined MetaMAE as a result. Our key idea is to view the mask reconstruction of MAE as a meta-learning task: masked tokens are predicted by adapting the Transformer meta-learner through the amortization of unmasked tokens. Based on this novel interpretation, we propose to integrate two advanced meta-learning techniques. First, we adapt the amortized latent of the Transformer encoder using gradient-based meta-learning to enhance the reconstruction. Then, we maximize the alignment between amortized and adapted latents through task contrastive learning which guides the Transformer encoder to better encode the task-specific knowledge. Our experiment demonstrates the superiority of MetaMAE in the modality-agnostic SSL benchmark (called DABS), significantly outperforming prior baselines. Code is available at https://github.com/alinlab/MetaMAE.
</details>
<details>
<summary>摘要</summary>
尽管自适学习（SSL）在多种模式下具有实际重要性，但最近的进展主要集中在视觉和语言领域，经常利用这些领域特定的知识。例如，偏振自适学习（MAE）已成为这些领域中流行的architecture，但它在其他模式下的潜力尚未得到充分发挥。在这篇论文中，我们开发了MAE作为一个统一、模式不偏的SSL框架。然后，我们提出了以元学习为核心，以提高MAE在多种模式下的SSL的想法，并提出了一种新的元学习技术。我们的关键想法是视Masked Auto-Encoder（MAE）的mask reconstruction为元学习任务：偏振token是通过adapting Transformer元学习器通过含括无masked token的权重学习来预测。基于这个新的解释，我们提出了两种高级元学习技术。首先，我们采用权重学习来提高含括Transfomer编码器的amortized latent。然后，我们通过任务对比学习来确保amortized和adapted latent之间的对应。我们的实验表明，MetaMAE在模式不偏的SSL benchmark（称为DABS）中表现出色， Significantly outperforming prior baselines。代码可以在https://github.com/alinlab/MetaMAE中找到。
</details></li>
</ul>
<hr>
<h2 id="Sum-of-Parts-Models-Faithful-Attributions-for-Groups-of-Features"><a href="#Sum-of-Parts-Models-Faithful-Attributions-for-Groups-of-Features" class="headerlink" title="Sum-of-Parts Models: Faithful Attributions for Groups of Features"></a>Sum-of-Parts Models: Faithful Attributions for Groups of Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16316">http://arxiv.org/abs/2310.16316</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/debugml/sop">https://github.com/debugml/sop</a></li>
<li>paper_authors: Weiqiu You, Helen Qu, Marco Gatti, Bhuvnesh Jain, Eric Wong</li>
<li>for: 这个论文目的是为了提供一种可靠的机器学习模型解释方法，帮助astrophysicists更好地理解星系形成过程。</li>
<li>methods: 该论文使用Sum-of-Parts（SOP）模型，该模型可以提供可解释的分组特征贡献，帮助找到星系形成中具有重要作用的特征。</li>
<li>results: 在标准解释指标上评估SOP模型，以及在一个实际案例中，使用SOP模型提供的可信的解释来帮助astrophysicists发现新的星系形成知识。<details>
<summary>Abstract</summary>
An explanation of a machine learning model is considered "faithful" if it accurately reflects the model's decision-making process. However, explanations such as feature attributions for deep learning are not guaranteed to be faithful, and can produce potentially misleading interpretations. In this work, we develop Sum-of-Parts (SOP), a class of models whose predictions come with grouped feature attributions that are faithful-by-construction. This model decomposes a prediction into an interpretable sum of scores, each of which is directly attributable to a sparse group of features. We evaluate SOP on benchmarks with standard interpretability metrics, and in a case study, we use the faithful explanations from SOP to help astrophysicists discover new knowledge about galaxy formation.
</details>
<details>
<summary>摘要</summary>
machine learning 模型的解释被称为"loyal"，如果它们准确反映模型的决策过程。然而，特征贡献对深度学习来说并不一定是loyal，可能产生误导性的解释。在这项工作中，我们开发了Sum-of-Parts（SOP）模型，它的预测结果包括有组织的特征贡献，这些贡献直接关联到一个稀疏的特征集中。我们使用标准解释指标评估SOP模型，并在一个案例研究中，使用SOP模型提供的loyal解释帮助astrophysicists发现新的星系形成知识。
</details></li>
</ul>
<hr>
<h2 id="Instance-wise-Linearization-of-Neural-Network-for-Model-Interpretation"><a href="#Instance-wise-Linearization-of-Neural-Network-for-Model-Interpretation" class="headerlink" title="Instance-wise Linearization of Neural Network for Model Interpretation"></a>Instance-wise Linearization of Neural Network for Model Interpretation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16295">http://arxiv.org/abs/2310.16295</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhimin Li, Shusen Liu, Kailkhura Bhavya, Timo Bremer, Valerio Pascucci</li>
<li>for: 这个论文主要针对的是解释神经网络模型如何使用输入特征来做预测，以及如何从神经网络模型中提取有用的特征分布。</li>
<li>methods: 该论文提出了一种实例化线性化方法，该方法可以将神经网络模型的前向计算过程转换为线性矩阵乘法，从而提取出神经网络模型的预测过程中各个层次的线性特征。</li>
<li>results: 该论文通过应用实例化线性化方法在神经网络模型中，得到了一种可以描述神经网络模型预测过程的线性矩阵乘法方程，该方程不仅可以提供有用的特征分布，还可以告诉我们每个输入特征如何直接影响预测结果。<details>
<summary>Abstract</summary>
Neural network have achieved remarkable successes in many scientific fields. However, the interpretability of the neural network model is still a major bottlenecks to deploy such technique into our daily life. The challenge can dive into the non-linear behavior of the neural network, which rises a critical question that how a model use input feature to make a decision. The classical approach to address this challenge is feature attribution, which assigns an important score to each input feature and reveal its importance of current prediction. However, current feature attribution approaches often indicate the importance of each input feature without detail of how they are actually processed by a model internally. These attribution approaches often raise a concern that whether they highlight correct features for a model prediction.   For a neural network model, the non-linear behavior is often caused by non-linear activation units of a model. However, the computation behavior of a prediction from a neural network model is locally linear, because one prediction has only one activation pattern. Base on the observation, we propose an instance-wise linearization approach to reformulates the forward computation process of a neural network prediction. This approach reformulates different layers of convolution neural networks into linear matrix multiplication. Aggregating all layers' computation, a prediction complex convolution neural network operations can be described as a linear matrix multiplication $F(x) = W \cdot x + b$. This equation can not only provides a feature attribution map that highlights the important of the input features but also tells how each input feature contributes to a prediction exactly. Furthermore, we discuss the application of this technique in both supervise classification and unsupervised neural network learning parametric t-SNE dimension reduction.
</details>
<details>
<summary>摘要</summary>
Current feature attribution methods provide important scores for each input feature but do not reveal how the features are processed internally by the model. This raises concerns about whether these methods are highlighting the correct features for the model's predictions.In neural networks, non-linear behavior is often caused by non-linear activation units. However, the computation process of a prediction is locally linear, as each prediction has only one activation pattern. Based on this observation, we propose an instance-wise linearization approach that reformulates the forward computation process of a neural network prediction. This approach transforms different layers of convolutional neural networks into linear matrix multiplication. By aggregating all layers' computations, a complex convolutional neural network operation can be described as a linear matrix multiplication equation: $F(x) = W \cdot x + b$.This equation not only provides a feature attribution map that highlights the importance of the input features but also reveals exactly how each input feature contributes to the prediction. Furthermore, we discuss the application of this technique in both supervised classification and unsupervised neural network learning, including parametric t-SNE dimension reduction.
</details></li>
</ul>
<hr>
<h2 id="XFEVER-Exploring-Fact-Verification-across-Languages"><a href="#XFEVER-Exploring-Fact-Verification-across-Languages" class="headerlink" title="XFEVER: Exploring Fact Verification across Languages"></a>XFEVER: Exploring Fact Verification across Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16278">http://arxiv.org/abs/2310.16278</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nii-yamagishilab/xfever">https://github.com/nii-yamagishilab/xfever</a></li>
<li>paper_authors: Yi-Chen Chang, Canasai Kruengkrai, Junichi Yamagishi</li>
<li>for: 本研究设计了跨语言事实抽象和验证（XFEVER）数据集，用于评估不同语言的事实验证模型。</li>
<li>methods: 本研究使用机器翻译将英语的声明和证据文本翻译成六种语言，并将训练和开发集用机器翻译，而测试集则包括专业翻译和机器翻译的文本。</li>
<li>results: 实验结果显示，使用多语言语言模型可以快速建立不同语言的事实验证模型，但表现因语言而异，英语的表现较佳。此外，我们发现可以有效地消除模型误偏，通过考虑英语和目标语言之间的预测相似性。<details>
<summary>Abstract</summary>
This paper introduces the Cross-lingual Fact Extraction and VERification (XFEVER) dataset designed for benchmarking the fact verification models across different languages. We constructed it by translating the claim and evidence texts of the Fact Extraction and VERification (FEVER) dataset into six languages. The training and development sets were translated using machine translation, whereas the test set includes texts translated by professional translators and machine-translated texts. Using the XFEVER dataset, two cross-lingual fact verification scenarios, zero-shot learning and translate-train learning, are defined, and baseline models for each scenario are also proposed in this paper. Experimental results show that the multilingual language model can be used to build fact verification models in different languages efficiently. However, the performance varies by language and is somewhat inferior to the English case. We also found that we can effectively mitigate model miscalibration by considering the prediction similarity between the English and target languages. The XFEVER dataset, code, and model checkpoints are available at https://github.com/nii-yamagishilab/xfever.
</details>
<details>
<summary>摘要</summary>
本文介绍了跨语言实体提取和验证（XFEVER）数据集，用于评测不同语言的实体验证模型。我们通过将 claim 和 evidence 文本从实体提取和验证（FEVER）数据集翻译成六种语言，构建了该数据集。训练集和开发集使用机器翻译进行翻译，测试集则包括由专业翻译员翻译的文本以及机器翻译的文本。在本文中，我们定义了跨语言实体验证的两种场景：零shot学习和translate-train学习，并提出了基eline模型 для每个场景。实验结果表明，可以使用多语言语模型建立不同语言的实体验证模型，但性能因语言而异，与英语情况相比有所下降。我们还发现，可以通过考虑英语和目标语言之间的预测相似性来有效地缓解模型偏差。XFEVER数据集、代码和模型检查点可以在https://github.com/nii-yamagishilab/xfever中下载。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Domain-Invariant-Learning-via-Posterior-Generalization-of-Parameter-Distributions"><a href="#Bayesian-Domain-Invariant-Learning-via-Posterior-Generalization-of-Parameter-Distributions" class="headerlink" title="Bayesian Domain Invariant Learning via Posterior Generalization of Parameter Distributions"></a>Bayesian Domain Invariant Learning via Posterior Generalization of Parameter Distributions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16277">http://arxiv.org/abs/2310.16277</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiyu Shen, Bin Pan, Tianyang Shi, Tao Li, Zhenwei Shi</li>
<li>for: 这篇论文的目的是学习对不同训练领域的预测模型，以实现更好的统一预测性。</li>
<li>methods: 这篇论文使用了 bayesian neural network 来学习对不同训练领域的预测模型，并且将注意力集中在对维度分布的调整，而不是对维度分布的调整。</li>
<li>results: 这篇论文提出了一个名为 PosTerior Generalization (PTG) 的简单 yet effective 方法，可以用来估计对不同训练领域的参数分布，并且可以与现有的领域一致方法结合使用，以提高预测性能。PTG 在 DomainBed 上的评估中表现了竞争性的表现。<details>
<summary>Abstract</summary>
Domain invariant learning aims to learn models that extract invariant features over various training domains, resulting in better generalization to unseen target domains. Recently, Bayesian Neural Networks have achieved promising results in domain invariant learning, but most works concentrate on aligning features distributions rather than parameter distributions. Inspired by the principle of Bayesian Neural Network, we attempt to directly learn the domain invariant posterior distribution of network parameters. We first propose a theorem to show that the invariant posterior of parameters can be implicitly inferred by aggregating posteriors on different training domains. Our assumption is more relaxed and allows us to extract more domain invariant information. We also propose a simple yet effective method, named PosTerior Generalization (PTG), that can be used to estimate the invariant parameter distribution. PTG fully exploits variational inference to approximate parameter distributions, including the invariant posterior and the posteriors on training domains. Furthermore, we develop a lite version of PTG for widespread applications. PTG shows competitive performance on various domain generalization benchmarks on DomainBed. Additionally, PTG can use any existing domain generalization methods as its prior, and combined with previous state-of-the-art method the performance can be further improved. Code will be made public.
</details>
<details>
<summary>摘要</summary>
领域不变学习目标是学习EXTRACTING invariant features over various training domains, resulting in better generalization to unseen target domains. 最近， Bayesian Neural Networks have achieved promising results in domain invariant learning, but most works concentrate on aligning features distributions rather than parameter distributions. Inspired by the principle of Bayesian Neural Network, we attempt to directly learn the domain invariant posterior distribution of network parameters. We first propose a theorem to show that the invariant posterior of parameters can be implicitly inferred by aggregating posteriors on different training domains. Our assumption is more relaxed and allows us to extract more domain invariant information. We also propose a simple yet effective method, named PosTerior Generalization (PTG), that can be used to estimate the invariant parameter distribution. PTG fully exploits variational inference to approximate parameter distributions, including the invariant posterior and the posteriors on training domains. Furthermore, we develop a lite version of PTG for widespread applications. PTG shows competitive performance on various domain generalization benchmarks on DomainBed. Additionally, PTG can use any existing domain generalization methods as its prior, and combined with previous state-of-the-art method the performance can be further improved. 代码将公开。
</details></li>
</ul>
<hr>
<h2 id="Using-GPT-4-to-Augment-Unbalanced-Data-for-Automatic-Scoring"><a href="#Using-GPT-4-to-Augment-Unbalanced-Data-for-Automatic-Scoring" class="headerlink" title="Using GPT-4 to Augment Unbalanced Data for Automatic Scoring"></a>Using GPT-4 to Augment Unbalanced Data for Automatic Scoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18365">http://arxiv.org/abs/2310.18365</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luyang Fang, Gyeong-Geon Lee, Xiaoming Zhai</li>
<li>for: 这个研究是为了解决自动评分中学生回答不均匀的问题，使用GPT-4大语言模型进行资料增强。</li>
<li>methods: 研究使用GPT-4生成模型生成学生回答的对应问题，以增强资料，然后使用DistillBERT进行自动评分。</li>
<li>results: 研究发现，将GPT-4增强的数据融入到自动评分模型中，可以提高精度、准确性、回传率和F1分数，特别是在较少的评分类别中。且不同于原始数据的比例，需要不同的增强数据量以获得稳定的改善。此外，与学生写作增强数据相比，GPT-4增强的评分模型表现更好或相等。<details>
<summary>Abstract</summary>
Machine learning-based automatic scoring can be challenging if students' responses are unbalanced across scoring categories, as it introduces uncertainty in the machine training process. To meet this challenge, we introduce a novel text data augmentation framework leveraging GPT-4, a generative large language model, specifically tailored for unbalanced datasets in automatic scoring. Our experimental dataset comprised student written responses to two science items. We crafted prompts for GPT-4 to generate responses resembling student written answers, particularly for the minority scoring classes, to augment the data. We then finetuned DistillBERT for automatic scoring based on the augmented and original datasets. Model performance was assessed using accuracy, precision, recall, and F1 metrics. Our findings revealed that incorporating GPT-4-augmented data remarkedly improved model performance, particularly for precision, recall, and F1 scores. Interestingly, the extent of improvement varied depending on the specific dataset and the proportion of augmented data used. Notably, we found that a varying amount of augmented data (5\%-40\%) was needed to obtain stable improvement for automatic scoring. We also compared the accuracies of models trained with GPT-4 augmented data to those trained with additional student-written responses. Results suggest that the GPT-4 augmented scoring models outperform or match the models trained with student-written augmented data. This research underscores the potential and effectiveness of data augmentation techniques utilizing generative large language models--GPT-4 in addressing unbalanced datasets within automated assessment.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="CycleAlign-Iterative-Distillation-from-Black-box-LLM-to-White-box-Models-for-Better-Human-Alignment"><a href="#CycleAlign-Iterative-Distillation-from-Black-box-LLM-to-White-box-Models-for-Better-Human-Alignment" class="headerlink" title="CycleAlign: Iterative Distillation from Black-box LLM to White-box Models for Better Human Alignment"></a>CycleAlign: Iterative Distillation from Black-box LLM to White-box Models for Better Human Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16271">http://arxiv.org/abs/2310.16271</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jixiang Hong, Quan Tu, Changyu Chen, Xing Gao, Ji Zhang, Rui Yan</li>
<li>for: 本研究旨在使用人工回馈法（RLHF）和排名方法对大规模语言模型（LLM）进行对人价值的调整，以确保模型的输出符合人类的偏好和价值观。</li>
<li>methods: 本研究使用了一种名为循环调整（CycleAlign）的新方法，通过在循环互动中使用隐藏模型（black-box）和可见模型（white-box）来实现对模型的调整。在每次互动中，隐藏模型根据人工提供的指导和示范来排序模型生成的响应，而白色模型则通过自身的判断来评价自己生成的响应。</li>
<li>results: 研究发现，通过多次循环互动，循环调整框架可以有效地将白色模型与隐藏模型进行对调，并且可以在低资源情况下实现。此外，与现有方法相比，模型经过循环调整后表现出色，达到了人类价值对Alignment的最佳性能。<details>
<summary>Abstract</summary>
Language models trained on large-scale corpus often generate content that is harmful, toxic, or contrary to human preferences, making their alignment with human values a critical concern. Reinforcement learning from human feedback (RLHF) with algorithms like PPO is a prevalent approach for alignment but is often complex, unstable, and resource-intensive. Recently, ranking-based alignment methods have emerged, offering stability and effectiveness by replacing the RL framework with supervised fine-tuning, but they are costly due to the need for annotated data. Considering that existing large language models (LLMs) like ChatGPT are already relatively well-aligned and cost-friendly, researchers have begun to align the language model with human preference from AI feedback. The common practices, which unidirectionally distill the instruction-following responses from LLMs, are constrained by their bottleneck. Thus we introduce CycleAlign to distill alignment capabilities from parameter-invisible LLMs (black-box) to a parameter-visible model (white-box) in an iterative manner. With in-context learning (ICL) as the core of the cycle, the black-box models are able to rank the model-generated responses guided by human-craft instruction and demonstrations about their preferences. During iterative interaction, the white-box models also have a judgment about responses generated by them. Consequently, the agreement ranking could be viewed as a pseudo label to dynamically update the in-context demonstrations and improve the preference ranking ability of black-box models. Through multiple interactions, the CycleAlign framework could align the white-box model with the black-box model effectively in a low-resource way. Empirical results illustrate that the model fine-tuned by CycleAlign remarkably exceeds existing methods, and achieves the state-of-the-art performance in alignment with human value.
</details>
<details>
<summary>摘要</summary>
大量文本训练的语言模型经常生成有害、毒性或背离人类偏好的内容，使其与人类价值观Alignment成为一项关键问题。使用人类反馈强化学习（RLHF）的算法如PPO是一种常见的实现方式，但它们经常复杂、不稳定和资源占用。 reciently, 排名基于的对齐方法出现了，它们可以通过取代RL框架，实现稳定性和效果，但它们需要大量的标注数据。 given that existing large language models（LLMs）like ChatGPT are already relatively well-aligned and cost-friendly, researchers have begun to align the language model with human preference from AI feedback。 common practices， which unidirectionally distill the instruction-following responses from LLMs， are constrained by their bottleneck。 therefore, we introduce CycleAlign to distill alignment capabilities from parameter-invisible LLMs（black-box）to a parameter-visible model（white-box）in an iterative manner。 with in-context learning（ICL）as the core of the cycle， the black-box models are able to rank the model-generated responses guided by human-craft instruction and demonstrations about their preferences。 during iterative interaction， the white-box models also have a judgment about responses generated by them。 consequently， the agreement ranking could be viewed as a pseudo label to dynamically update the in-context demonstrations and improve the preference ranking ability of black-box models。 through multiple interactions， the CycleAlign framework could align the white-box model with the black-box model effectively in a low-resource way。 empirical results illustrate that the model fine-tuned by CycleAlign remarkably exceeds existing methods， and achieves the state-of-the-art performance in alignment with human value。
</details></li>
</ul>
<hr>
<h2 id="Attention-Lens-A-Tool-for-Mechanistically-Interpreting-the-Attention-Head-Information-Retrieval-Mechanism"><a href="#Attention-Lens-A-Tool-for-Mechanistically-Interpreting-the-Attention-Head-Information-Retrieval-Mechanism" class="headerlink" title="Attention Lens: A Tool for Mechanistically Interpreting the Attention Head Information Retrieval Mechanism"></a>Attention Lens: A Tool for Mechanistically Interpreting the Attention Head Information Retrieval Mechanism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16270">http://arxiv.org/abs/2310.16270</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/msakarvadia/attentionlens">https://github.com/msakarvadia/attentionlens</a></li>
<li>paper_authors: Mansi Sakarvadia, Arham Khan, Aswathy Ajith, Daniel Grzenda, Nathaniel Hudson, André Bauer, Kyle Chard, Ian Foster</li>
<li>for: 这个论文旨在了解 trasformer 基于语言模型中 attention 头的特定作用，以及它们如何生成最终预测结果。</li>
<li>methods: 该论文使用 reverse engineering 技术来探索 attention 头的内部机制，并提出了一种名为 Attention Lens 的工具来将 attention 头的输出翻译成 vocabulary tokens。</li>
<li>results: 预liminary 结果表明，attention 头在语言模型中扮演着非常特殊的角色，并且可以通过 learned 的 attention-head-specific 转换来翻译 attention 头的输出。<details>
<summary>Abstract</summary>
Transformer-based Large Language Models (LLMs) are the state-of-the-art for natural language tasks. Recent work has attempted to decode, by reverse engineering the role of linear layers, the internal mechanisms by which LLMs arrive at their final predictions for text completion tasks. Yet little is known about the specific role of attention heads in producing the final token prediction. We propose Attention Lens, a tool that enables researchers to translate the outputs of attention heads into vocabulary tokens via learned attention-head-specific transformations called lenses. Preliminary findings from our trained lenses indicate that attention heads play highly specialized roles in language models. The code for Attention Lens is available at github.com/msakarvadia/AttentionLens.
</details>
<details>
<summary>摘要</summary>
Transformer-based 大型自然语言模型 (LLMs) 是当前最佳实践的自然语言任务。近期的工作尝试了将 linear layers 的内部机制 reverse engineering 为文本完成任务中的最终预测。然而，对于特定的 attention heads 在生成最终Token预测中的作用知之 little。我们提出 Attention Lens，一个工具，允许研究人员通过学习 attention-head-specific 的变换（lenses）将 attention heads 的输出翻译成词汇符号。我们的初步发现表明， attention heads 在语言模型中扮演了非常特殊的角色。Attention Lens 的代码可以在 github.com/msakarvadia/AttentionLens 上找到。
</details></li>
</ul>
<hr>
<h2 id="Multilingual-Coarse-Political-Stance-Classification-of-Media-The-Editorial-Line-of-a-ChatGPT-and-Bard-Newspaper"><a href="#Multilingual-Coarse-Political-Stance-Classification-of-Media-The-Editorial-Line-of-a-ChatGPT-and-Bard-Newspaper" class="headerlink" title="Multilingual Coarse Political Stance Classification of Media. The Editorial Line of a ChatGPT and Bard Newspaper"></a>Multilingual Coarse Political Stance Classification of Media. The Editorial Line of a ChatGPT and Bard Newspaper</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16269">http://arxiv.org/abs/2310.16269</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cristina España-Bonet</li>
<li>for: This paper aims to explore the use of artificial intelligence (AI) in news outlets and its potential impact on bias ratings.</li>
<li>methods: The authors use authentic news outlets’ ratings to create a multilingual corpus of news with coarse stance annotations and automatically extracted topic annotations. They train classifiers on this data to identify the editorial line of unseen newspapers in English, German, Spanish, and Catalan.</li>
<li>results: The classifiers are able to identify the editorial line of most unseen newspapers in the four languages, and the authors observe that ChatGPT’s editorial line evolves over time and differs among languages.<details>
<summary>Abstract</summary>
Neutrality is difficult to achieve and, in politics, subjective. Traditional media typically adopt an editorial line that can be used by their potential readers as an indicator of the media bias. Several platforms currently rate news outlets according to their political bias. The editorial line and the ratings help readers in gathering a balanced view of news. But in the advent of instruction-following language models, tasks such as writing a newspaper article can be delegated to computers. Without imposing a biased persona, where would an AI-based news outlet lie within the bias ratings? In this work, we use the ratings of authentic news outlets to create a multilingual corpus of news with coarse stance annotations (Left and Right) along with automatically extracted topic annotations. We show that classifiers trained on this data are able to identify the editorial line of most unseen newspapers in English, German, Spanish and Catalan. We then apply the classifiers to 101 newspaper-like articles written by ChatGPT and Bard in the 4 languages at different time periods. We observe that, similarly to traditional newspapers, ChatGPT editorial line evolves with time and, being a data-driven system, the stance of the generated articles differs among languages.
</details>
<details>
<summary>摘要</summary>
（ Traditional media typically adopt an editorial line that can be used by their potential readers as an indicator of the media bias. Several platforms currently rate news outlets according to their political bias. The editorial line and the ratings help readers in gathering a balanced view of news. But in the advent of instruction-following language models, tasks such as writing a newspaper article can be delegated to computers. Without imposing a biased persona, where would an AI-based news outlet lie within the bias ratings? In this work, we use the ratings of authentic news outlets to create a multilingual corpus of news with coarse stance annotations (Left and Right) along with automatically extracted topic annotations. We show that classifiers trained on this data are able to identify the editorial line of most unseen newspapers in English, German, Spanish and Catalan. We then apply the classifiers to 101 newspaper-like articles written by ChatGPT and Bard in the 4 languages at different time periods. We observe that, similarly to traditional newspapers, ChatGPT editorial line evolves with time and, being a data-driven system, the stance of the generated articles differs among languages.）Note: The text has been translated using the Google Translate API, which may not be perfect and may not capture all the nuances of the original text.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Large-Language-Models-for-Secure-Code-Generation-A-Dataset-driven-Study-on-Vulnerability-Mitigation"><a href="#Enhancing-Large-Language-Models-for-Secure-Code-Generation-A-Dataset-driven-Study-on-Vulnerability-Mitigation" class="headerlink" title="Enhancing Large Language Models for Secure Code Generation: A Dataset-driven Study on Vulnerability Mitigation"></a>Enhancing Large Language Models for Secure Code Generation: A Dataset-driven Study on Vulnerability Mitigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16263">http://arxiv.org/abs/2310.16263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiexin Wang, Liuwen Cao, Xitong Luo, Zhiping Zhou, Jiayuan Xie, Adam Jatowt, Yi Cai</li>
<li>for: 评估和提高大型语言模型（LLMs）在代码生成方面的安全性。</li>
<li>methods: 使用不同的方法和技术来提高 LLMs 的安全性，包括代码生成、代码修复和攻击类型分类等。</li>
<li>results: 研究发现现有模型在代码生成过程中经常忽略安全问题，导致生成的代码具有漏洞性; 提议了一些有效的方法来缓解安全漏洞，提高 LLMs 的总体可靠性。<details>
<summary>Abstract</summary>
Large language models (LLMs) have brought significant advancements to code generation, benefiting both novice and experienced developers. However, their training using unsanitized data from open-source repositories, like GitHub, introduces the risk of inadvertently propagating security vulnerabilities. To effectively mitigate this concern, this paper presents a comprehensive study focused on evaluating and enhancing code LLMs from a software security perspective. We introduce SecuCoGen\footnote{SecuCoGen has been uploaded as supplemental material and will be made publicly available after publication.}, a meticulously curated dataset targeting 21 critical vulnerability types. SecuCoGen comprises 180 samples and serves as the foundation for conducting experiments on three crucial code-related tasks: code generation, code repair and vulnerability classification, with a strong emphasis on security. Our experimental results reveal that existing models often overlook security concerns during code generation, leading to the generation of vulnerable code. To address this, we propose effective approaches to mitigate the security vulnerabilities and enhance the overall robustness of code generated by LLMs. Moreover, our study identifies weaknesses in existing models' ability to repair vulnerable code, even when provided with vulnerability information. Additionally, certain vulnerability types pose challenges for the models, hindering their performance in vulnerability classification. Based on these findings, we believe our study will have a positive impact on the software engineering community, inspiring the development of improved methods for training and utilizing LLMs, thereby leading to safer and more trustworthy model deployment.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:大型语言模型（LLMs）已经为开发者带来了重要的进步，帮助他们更好地生成代码。然而，通过使用 GitHub 等开源存储库中的未经过处理的数据进行训练，可能会意外地传播安全漏洞。为了有效地缓解这种问题，这篇论文提出了一项全面的研究，旨在从软件安全角度评估和加强代码生成器。我们提出了 SecuCoGen，一个精心准备的数据集，包含 21 种关键的漏洞类型。SecuCoGen 包含 180 个样本，并作为基于代码生成、代码修复和漏洞分类等三个关键任务的实验基础。我们的实验结果表明，现有的模型在代码生成时经常忽略安全问题，导致生成的代码存在漏洞。为了解决这个问题，我们提出了一些有效的方法来缓解安全漏洞并提高代码生成器的整体可靠性。此外，我们的研究还发现了现有模型在修复漏洞代码时存在缺陷，即使提供了漏洞信息。此外，某些漏洞类型对模型表现出了困难。根据这些发现，我们认为这项研究将对软件工程领域产生积极的影响，激励开发人员开发更好的训练和使用 LLMs 的方法，从而导致更安全和可靠的模型部署。
</details></li>
</ul>
<hr>
<h2 id="rTisane-Externalizing-conceptual-models-for-data-analysis-increases-engagement-with-domain-knowledge-and-improves-statistical-model-quality"><a href="#rTisane-Externalizing-conceptual-models-for-data-analysis-increases-engagement-with-domain-knowledge-and-improves-statistical-model-quality" class="headerlink" title="rTisane: Externalizing conceptual models for data analysis increases engagement with domain knowledge and improves statistical model quality"></a>rTisane: Externalizing conceptual models for data analysis increases engagement with domain knowledge and improves statistical model quality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16262">http://arxiv.org/abs/2310.16262</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eunice Jun, Edward Misback, Jeffrey Heer, René Just</li>
<li>for: 本研究旨在了解分析员在使用统计模型时的假设表达方式，以及这些假设如何影响统计模型质量。</li>
<li>methods: 本研究使用域特定语言（DSL）让分析员表达概念模型，并在这些模型中解决歧义。</li>
<li>results: 研究发现，使用 rTisane 的 DSL 可以帮助分析员更深入地表达假设，并更准确地外部化假设。 rTisane 也导致统计模型更好地匹配分析员的假设，保持分析意图，并更好地适应数据。<details>
<summary>Abstract</summary>
Statistical models should accurately reflect analysts' domain knowledge about variables and their relationships. While recent tools let analysts express these assumptions and use them to produce a resulting statistical model, it remains unclear what analysts want to express and how externalization impacts statistical model quality. This paper addresses these gaps. We first conduct an exploratory study of analysts using a domain-specific language (DSL) to express conceptual models. We observe a preference for detailing how variables relate and a desire to allow, and then later resolve, ambiguity in their conceptual models. We leverage these findings to develop rTisane, a DSL for expressing conceptual models augmented with an interactive disambiguation process. In a controlled evaluation, we find that rTisane's DSL helps analysts engage more deeply with and accurately externalize their assumptions. rTisane also leads to statistical models that match analysts' assumptions, maintain analysis intent, and better fit the data.
</details>
<details>
<summary>摘要</summary>
(Note: The text is translated into Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Hong Kong, Macau, and Taiwan.)
</details></li>
</ul>
<hr>
<h2 id="A-Causal-Disentangled-Multi-Granularity-Graph-Classification-Method"><a href="#A-Causal-Disentangled-Multi-Granularity-Graph-Classification-Method" class="headerlink" title="A Causal Disentangled Multi-Granularity Graph Classification Method"></a>A Causal Disentangled Multi-Granularity Graph Classification Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16256">http://arxiv.org/abs/2310.16256</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Li, Li Liu, Penggang Chen, Youmin Zhang, Guoyin Wang</li>
<li>for: 本文提出了一种解决图像数据的多维度特征映射问题的方法，以提高图像分类任务的准确率和可解释性。</li>
<li>methods: 本文提出了一种基于 causal disentanglement 的多维度图像表示学习方法（CDM-GNN），该方法可以分离图像中重要的子结构和偏好部分，从多维度角度进行表示学习，提高图像分类任务的准确率和可解释性。</li>
<li>results: 本文通过对三个实际数据集（MUTAG、PTC、IMDM-M）进行比较，表明 CDM-GNN 模型在图像分类任务中表现出色，同时也可以提供可解释的结果，与人类认知模式相符。<details>
<summary>Abstract</summary>
Graph data widely exists in real life, with large amounts of data and complex structures. It is necessary to map graph data to low-dimensional embedding. Graph classification, a critical graph task, mainly relies on identifying the important substructures within the graph. At present, some graph classification methods do not combine the multi-granularity characteristics of graph data. This lack of granularity distinction in modeling leads to a conflation of key information and false correlations within the model. So, achieving the desired goal of a credible and interpretable model becomes challenging. This paper proposes a causal disentangled multi-granularity graph representation learning method (CDM-GNN) to solve this challenge. The CDM-GNN model disentangles the important substructures and bias parts within the graph from a multi-granularity perspective. The disentanglement of the CDM-GNN model reveals important and bias parts, forming the foundation for its classification task, specifically, model interpretations. The CDM-GNN model exhibits strong classification performance and generates explanatory outcomes aligning with human cognitive patterns. In order to verify the effectiveness of the model, this paper compares the three real-world datasets MUTAG, PTC, and IMDM-M. Six state-of-the-art models, namely GCN, GAT, Top-k, ASAPool, SUGAR, and SAT are employed for comparison purposes. Additionally, a qualitative analysis of the interpretation results is conducted.
</details>
<details>
<summary>摘要</summary>
Graph data广泛存在于实际生活中，具有大量数据和复杂结构。需要将图数据映射到低维度嵌入。图分类任务是图处理中的关键任务，主要是在图中发现重要的子结构。目前，一些图分类方法不会结合图数据的多级结构。这会导致模型中的关键信息和假相关性混淆。因此，实现可靠和可解释的模型变得困难。本文提出了一种 causal disentangled multi-granularity 图表示学习方法（CDM-GNN）解决这个挑战。CDM-GNN 模型在多级视角下分离出重要的子结构和偏好部分。CDM-GNN 模型的分离可以揭示重要和偏好的部分，这成为模型的分类任务基础。CDM-GNN 模型在分类任务中表现出色，并生成了与人认知模式相符的解释结果。为了证明模型的有效性，本文对三个实际 datasets（MUTAG、PTC、IMDM-M）进行比较。并进行了对解释结果的质量分析。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. The translation is based on the original text and may not capture all the nuances and variations of the original text.
</details></li>
</ul>
<hr>
<h2 id="ConDefects-A-New-Dataset-to-Address-the-Data-Leakage-Concern-for-LLM-based-Fault-Localization-and-Program-Repair"><a href="#ConDefects-A-New-Dataset-to-Address-the-Data-Leakage-Concern-for-LLM-based-Fault-Localization-and-Program-Repair" class="headerlink" title="ConDefects: A New Dataset to Address the Data Leakage Concern for LLM-based Fault Localization and Program Repair"></a>ConDefects: A New Dataset to Address the Data Leakage Concern for LLM-based Fault Localization and Program Repair</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16253">http://arxiv.org/abs/2310.16253</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yonghao Wu, Zheng Li, Jie M. Zhang, Yong Liu</li>
<li>for: 本研究旨在提供一个新的 fault localization 和 program repair 的 benchmark dataset，以确保 LLM-based 方法的可靠性和通用性。</li>
<li>methods: 该研究使用了精心约定的 fault 数据，以消除现有 benchmark 中的数据泄露问题，从而提供一个可靠的 benchmark 集。</li>
<li>results: 该研究提供了 1,254 个 Java faulty program 和 1,625 个 Python faulty program，每个 fault 都包含缺陷位置和修复后的代码版本，适用于 fault localization 和 program repair 相关的研究。<details>
<summary>Abstract</summary>
With the growing interest on Large Language Models (LLMs) for fault localization and program repair, ensuring the integrity and generalizability of the LLM-based methods becomes paramount. The code in existing widely-adopted benchmarks for these tasks was written before the the bloom of LLMs and may be included in the training data of existing popular LLMs, thereby suffering from the threat of data leakage, leading to misleadingly optimistic performance metrics. To address this issue, we introduce "ConDefects", a novel dataset of real faults meticulously curated to eliminate such overlap. ConDefects contains 1,254 Java faulty programs and 1,625 Python faulty programs. All these programs are sourced from the online competition platform AtCoder and were produced between October 2021 and September 2023. We pair each fault with fault locations and the corresponding repaired code versions, making it tailored for in fault localization and program repair related research. We also provide interfaces for selecting subsets based on different time windows and coding task difficulties. While inspired by LLM-based tasks, ConDefects can be adopted for benchmarking ALL types of fault localization and program repair methods. The dataset is publicly available, and a demo video can be found at https://www.youtube.com/watch?v=22j15Hj5ONk.
</details>
<details>
<summary>摘要</summary>
随着大语言模型（LLM）在错误定位和程序修复领域的兴趣增长，确保LLM-基于方法的完整性和通用性变得非常重要。现有的广泛采用的测试集中包含的代码可能在LLM的训练数据中包含，从而导致数据泄露问题，从而导致表现指标过optimistic。为解决这个问题，我们介绍了“ConDefects”，一个新的故障数据集，其中包含1,254个Java错误程序和1,625个Python错误程序。这些程序都来自于在线竞赛平台AtCoder，生成时间为2021年10月至2023年9月。我们对每个错误进行了精心编辑，以消除 overlap。我们还为每个错误提供了相应的修复代码版本，使其适用于错误定位和程序修复相关的研究。此外，我们还提供了基于不同时间窗口和编程任务难度的选择接口。虽然受LLM-基于任务的 inspirations，但ConDefects可以适用于所有类型的错误定位和程序修复方法的 benchmarking。数据集公开可用，demo视频可以在https://www.youtube.com/watch?v=22j15Hj5ONk找到。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/25/cs.AI_2023_10_25/" data-id="clorjzl2w0065f188bgq4hgv5" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/25/cs.CL_2023_10_25/" class="article-date">
  <time datetime="2023-10-25T11:00:00.000Z" itemprop="datePublished">2023-10-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/25/cs.CL_2023_10_25/">cs.CL - 2023-10-25</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="BOOST-Harnessing-Black-Box-Control-to-Boost-Commonsense-in-LMs’-Generation"><a href="#BOOST-Harnessing-Black-Box-Control-to-Boost-Commonsense-in-LMs’-Generation" class="headerlink" title="BOOST: Harnessing Black-Box Control to Boost Commonsense in LMs’ Generation"></a>BOOST: Harnessing Black-Box Control to Boost Commonsense in LMs’ Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17054">http://arxiv.org/abs/2310.17054</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/PlusLabNLP/BOOST_EMNLP23">https://github.com/PlusLabNLP/BOOST_EMNLP23</a></li>
<li>paper_authors: Yufei Tian, Felix Zhang, Nanyun Peng</li>
<li>for: 本研究旨在提高大型自然语言模型（LLM）的生成结果具备常识性。</li>
<li>methods: 我们提出了一种计算效率高的框架，使用冻结的预训练语言模型（PTLM）来生成更常识性的输出。我们首先构建了一个不含参考的评估器，将 sentence 评估为常识度。然后，我们使用评估器作为常识知识的oracle，并将 NADO 方法扩展到培训一个辅助头，以使PTLM更好地满足 oracle。</li>
<li>results: 我们在多种 GPT-2-, Flan-T5- 和 Alpaca-based 语言模型（LM）上进行了 série 的测试，结果显示，我们的方法能够 consistently 生成最常识性的输出。<details>
<summary>Abstract</summary>
Large language models (LLMs) such as GPT-3 have demonstrated a strong capability to generate coherent and contextually relevant text. However, amidst their successes, a crucial issue persists: their generated outputs still lack commonsense at times. Moreover, fine-tuning the entire LLM towards more commonsensical outputs is computationally expensive if not infeasible. In this paper, we present a computation-efficient framework that steers a frozen Pre-Trained Language Model (PTLM) towards more commonsensical generation (i.e., producing a plausible output that incorporates a list of concepts in a meaningful way). Specifically, we first construct a reference-free evaluator that assigns a sentence with a commonsensical score by grounding the sentence to a dynamic commonsense knowledge base from four different relational aspects. We then use the scorer as the oracle for commonsense knowledge, and extend the controllable generation method called NADO to train an auxiliary head that guides a fixed PTLM to better satisfy the oracle. We test our framework on a series of GPT-2-, Flan-T5-, and Alpaca-based language models (LMs) on two constrained concept-to-sentence benchmarks. Human evaluation results demonstrate that our method consistently leads to the most commonsensical outputs.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）如GPT-3已经表现出了强大的文本生成能力，但是在其成功之余，一个关键的问题仍然存在：它们的生成输出ometimes lack commonsense。而且，对整个LLM进行更加commonsensical的输出的精细调整是计算成本高昂的，甚至不可能。在这篇论文中，我们提出了一种计算效率高的框架，可以使用冻结的Pre-Trained Language Model（PTLM）生成更加commonsensical的文本。具体来说，我们首先构建了不含参考的评估器，可以根据四个关系方面的动态通用常识知识库赋予一句话commonsensical分数。然后，我们使用这个评估器作为 oracle，并将NADO控制生成方法扩展到固定PTLM上，以帮助它更好地满足oracle。我们在GPT-2-, Flan-T5-和Alpaca-based语言模型（LM）上进行了一系列测试。人类评估结果表明，我们的方法可以一直领先其他方法，并且生成出最commonsensical的输出。
</details></li>
</ul>
<hr>
<h2 id="Follow-on-Question-Suggestion-via-Voice-Hints-for-Voice-Assistants"><a href="#Follow-on-Question-Suggestion-via-Voice-Hints-for-Voice-Assistants" class="headerlink" title="Follow-on Question Suggestion via Voice Hints for Voice Assistants"></a>Follow-on Question Suggestion via Voice Hints for Voice Assistants</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17034">http://arxiv.org/abs/2310.17034</a></li>
<li>repo_url: None</li>
<li>paper_authors: Besnik Fetahu, Pedro Faustini, Giuseppe Castellucci, Anjie Fang, Oleg Rokhlenko, Shervin Malmasi</li>
<li>for: This paper aims to provide a solution for suggesting questions with compact and natural voice hints to allow users to ask follow-up questions in voice-based search settings.</li>
<li>methods: The authors propose an approach using sequence-to-sequence Transformers to generate spoken hints from a list of questions, and also define a linguistically-motivated pretraining task to improve the quality of the hints.</li>
<li>results: The authors evaluate their approach using a new dataset of 6681 input questions and human written hints, and find that their approach is strongly preferred by humans for producing the most natural hints, as compared to a naive approach of concatenating suggested questions.<details>
<summary>Abstract</summary>
The adoption of voice assistants like Alexa or Siri has grown rapidly, allowing users to instantly access information via voice search. Query suggestion is a standard feature of screen-based search experiences, allowing users to explore additional topics. However, this is not trivial to implement in voice-based settings. To enable this, we tackle the novel task of suggesting questions with compact and natural voice hints to allow users to ask follow-up questions.   We define the task, ground it in syntactic theory and outline linguistic desiderata for spoken hints. We propose baselines and an approach using sequence-to-sequence Transformers to generate spoken hints from a list of questions. Using a new dataset of 6681 input questions and human written hints, we evaluated the models with automatic metrics and human evaluation. Results show that a naive approach of concatenating suggested questions creates poor voice hints. Our approach, which applies a linguistically-motivated pretraining task was strongly preferred by humans for producing the most natural hints.
</details>
<details>
<summary>摘要</summary>
“对话助手如Alexa或Siri的采用速度快速增加，让用户通过声音搜寻取得信息。视觉搜寻经验中的查询建议是一个标准功能，允许用户继续探索相关主题。但在声音基础设置中实现此功能并不容易。为此，我们面临了一个新的任务：提出自然且简洁的声音提示，让用户可以通过声音提问。”“我们定义这个任务，并基于 syntax theory 进行定义。我们也提出了一些语言特性，以确保提出的提示 naturall 且易于理解。我们提出了一个基于 sequence-to-sequence Transformer 的方法，将问题列表转换为声音提示。使用了6681个输入问题和人工写的提示，我们评估了这些模型的性能。结果显示， concatenate 提出的提示会导致poor的声音提示。我们的方法，将在语言驱动的预训任务中使用语言驱动的预训任务，被人类评估为生成最自然的提示。”
</details></li>
</ul>
<hr>
<h2 id="Conditionally-Combining-Robot-Skills-using-Large-Language-Models"><a href="#Conditionally-Combining-Robot-Skills-using-Large-Language-Models" class="headerlink" title="Conditionally Combining Robot Skills using Large Language Models"></a>Conditionally Combining Robot Skills using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17019">http://arxiv.org/abs/2310.17019</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/krzentner/language-world">https://github.com/krzentner/language-world</a></li>
<li>paper_authors: K. R. Zentner, Ryan Julian, Brian Ichter, Gaurav S. Sukhatme</li>
<li>for: 本研究旨在探讨一种 combining two contributions的方法，包括一个叫做”Language-World”的扩展，允许一个大型自然语言模型在一个模拟的 роботиче环境中运行，使用 semi-structured natural language queries 和 scripted skills 描述使用 natural language。</li>
<li>methods: 本研究使用的方法包括 Plan Conditioned Behavioral Cloning (PCBC)，可以使用 end-to-end 示例来调整高级计划的行为。</li>
<li>results: 使用 Language-World，PCBC 在多种 few-shot 情况下能够实现强性表现，经常实现任务总结概念，只需要一个示例即可。<details>
<summary>Abstract</summary>
This paper combines two contributions. First, we introduce an extension of the Meta-World benchmark, which we call "Language-World," which allows a large language model to operate in a simulated robotic environment using semi-structured natural language queries and scripted skills described using natural language. By using the same set of tasks as Meta-World, Language-World results can be easily compared to Meta-World results, allowing for a point of comparison between recent methods using Large Language Models (LLMs) and those using Deep Reinforcement Learning. Second, we introduce a method we call Plan Conditioned Behavioral Cloning (PCBC), that allows finetuning the behavior of high-level plans using end-to-end demonstrations. Using Language-World, we show that PCBC is able to achieve strong performance in a variety of few-shot regimes, often achieving task generalization with as little as a single demonstration. We have made Language-World available as open-source software at https://github.com/krzentner/language-world/.
</details>
<details>
<summary>摘要</summary>
这篇论文组合了两个贡献。首先，我们介绍了一种扩展Meta-World benchmark，我们称之为"语言世界"（Language-World），允许一个大型自然语言模型在模拟的机器人环境中使用不结构化的自然语言查询和遵循自然语言描述的脚本技能。通过使用Meta-World任务集，Language-World结果可以与Meta-World结果进行直接比较，从而为最近使用大型自然语言模型（LLMs）和深度强化学习方法之间的比较提供一个参照点。其次，我们介绍了一种方法，称之为Plan Conditioned Behavioral Cloning（PCBC），允许高级计划的训练终端示例。使用Language-World，我们表明PCBC在不同的几个尝试情况下能够实现强大的表现，经常在几个示例下实现任务总结。我们将Language-World作为开源软件提供在GitHub上，请参考<https://github.com/krzentner/language-world/>。
</details></li>
</ul>
<hr>
<h2 id="Data-Augmentation-for-Emotion-Detection-in-Small-Imbalanced-Text-Data"><a href="#Data-Augmentation-for-Emotion-Detection-in-Small-Imbalanced-Text-Data" class="headerlink" title="Data Augmentation for Emotion Detection in Small Imbalanced Text Data"></a>Data Augmentation for Emotion Detection in Small Imbalanced Text Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17015">http://arxiv.org/abs/2310.17015</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/a-koufakou/augemotiondetection">https://github.com/a-koufakou/augemotiondetection</a></li>
<li>paper_authors: Anna Koufakou, Diego Grisales, Ragy Costa de jesus, Oscar Fox</li>
<li>for: 本研究旨在探讨数据增强技术在小规模、不均衡数据集上的影响，以提高NL表示模型在情感识别任务中的性能。</li>
<li>methods: 本研究使用了四种数据增强方法（EDA、静态和Contextual Embedding-based、ProtAugment），在三个不同的数据集上进行了实验。</li>
<li>results: 实验结果显示，通过在模型训练中使用增强数据，可以得到显著改善情感识别性能。此外，本研究还进行了两个 случа研究，包括使用受欢迎的Chat-GPT API来自动生成句子，以及使用外部数据增强训练集。结果表明这些方法具有潜在的潜力。<details>
<summary>Abstract</summary>
Emotion recognition in text, the task of identifying emotions such as joy or anger, is a challenging problem in NLP with many applications. One of the challenges is the shortage of available datasets that have been annotated with emotions. Certain existing datasets are small, follow different emotion taxonomies and display imbalance in their emotion distribution. In this work, we studied the impact of data augmentation techniques precisely when applied to small imbalanced datasets, for which current state-of-the-art models (such as RoBERTa) under-perform. Specifically, we utilized four data augmentation methods (Easy Data Augmentation EDA, static and contextual Embedding-based, and ProtAugment) on three datasets that come from different sources and vary in size, emotion categories and distributions. Our experimental results show that using the augmented data when training the classifier model leads to significant improvements. Finally, we conducted two case studies: a) directly using the popular chat-GPT API to paraphrase text using different prompts, and b) using external data to augment the training set. Results show the promising potential of these methods.
</details>
<details>
<summary>摘要</summary>
文本情感识别任务（Emotion Recognition）是自然语言处理（NLP）领域中的一个挑战性任务，具有许多应用。其中一个挑战是有限的可用标注数据。现有的一些数据集都很小，遵循着不同的情感分类法，并且具有不均匀的情感分布。在这项工作中，我们研究了对小规模不均匀数据集进行数据增强技术的影响。特别是，我们使用了四种数据增强方法（Easy Data Augmentation EDA、静态和Contextual Embedding-based、ProtAugment）在三个不同来源的数据集上进行实验。我们的实验结果表明，在训练分类器模型时使用增强数据可以获得显著改善。最后，我们进行了两个案例研究：a）直接使用流行的 chat-GPT API 将文本重新表述为不同的提问，b）使用外部数据增强训练集。结果表明这些方法具有潜在的批处性。
</details></li>
</ul>
<hr>
<h2 id="Quality-Quantity-Synthetic-Corpora-from-Foundation-Models-for-Closed-Domain-Extractive-Question-Answering"><a href="#Quality-Quantity-Synthetic-Corpora-from-Foundation-Models-for-Closed-Domain-Extractive-Question-Answering" class="headerlink" title="Quality &gt; Quantity: Synthetic Corpora from Foundation Models for Closed-Domain Extractive Question Answering"></a>Quality &gt; Quantity: Synthetic Corpora from Foundation Models for Closed-Domain Extractive Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16995">http://arxiv.org/abs/2310.16995</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/saptarshi059/cdqa-v1-targetted-pretraining">https://github.com/saptarshi059/cdqa-v1-targetted-pretraining</a></li>
<li>paper_authors: Saptarshi Sengupta, Connor Heaton, Shreya Ghosh, Preslav Nakov, Prasenjit Mitra</li>
<li>for: 本研究旨在提高闭包的问答系统的性能，通过针对性地预训练模型来适应特定领域的问题。</li>
<li>methods: 我们提出了一种名为“targeted pre-training”的方法，即根据特定领域的数据来预训练模型，以提高其在目标领域的性能。我们使用了Galactica工具来生成一些“targeted”的 corpora，以便更好地适应特定领域的问题。</li>
<li>results: 我们在两个生物医学抽取式问答数据集上进行了实验，并 achieved a new benchmark on COVID-QA 数据集，同时在 RadQA 数据集上也得到了全面的改进。<details>
<summary>Abstract</summary>
Domain adaptation, the process of training a model in one domain and applying it to another, has been extensively explored in machine learning. While training a domain-specific foundation model (FM) from scratch is an option, recent methods have focused on adapting pre-trained FMs for domain-specific tasks. However, our experiments reveal that either approach does not consistently achieve state-of-the-art (SOTA) results in the target domain. In this work, we study extractive question answering within closed domains and introduce the concept of targeted pre-training. This involves determining and generating relevant data to further pre-train our models, as opposed to the conventional philosophy of utilizing domain-specific FMs trained on a wide range of data. Our proposed framework uses Galactica to generate synthetic, ``targeted'' corpora that align with specific writing styles and topics, such as research papers and radiology reports. This process can be viewed as a form of knowledge distillation. We apply our method to two biomedical extractive question answering datasets, COVID-QA and RadQA, achieving a new benchmark on the former and demonstrating overall improvements on the latter. Code available at https://github.com/saptarshi059/CDQA-v1-Targetted-PreTraining/tree/main.
</details>
<details>
<summary>摘要</summary>
域适应，即在一个领域中训练模型，然后应用到另一个领域，在机器学习领域中得到了广泛的探索。而在训练域pecific基本模型（FM）从scratch的方法也有所研究，但我们的实验表明，这两种方法并不一定能够在目标领域 achieve state-of-the-art（SOTA）结果。在这项工作中，我们研究closed domain中的抽取式问答 tasks，并提出了一种名为目标预训练的概念。这种方法是通过Determining和生成相关的数据来进一步训练我们的模型，而不是通过使用域pecific FMs在各种数据上进行训练。我们的提出的框架使用Galactica来生成一些“targeted”的Synthetic corpora，这些corpora与特定的写作风格和主题相对应，例如研究论文和医学报告。这个过程可以看作是一种知识储存。我们在COVID-QA和RadQA两个生物医学抽取式问答数据集上应用了我们的方法， achieved a new benchmark on the former and demonstrated overall improvements on the latter。代码可以在https://github.com/saptarshi059/CDQA-v1-Targetted-PreTraining/tree/main。
</details></li>
</ul>
<hr>
<h2 id="How-well-can-machine-generated-texts-be-identified-and-can-language-models-be-trained-to-avoid-identification"><a href="#How-well-can-machine-generated-texts-be-identified-and-can-language-models-be-trained-to-avoid-identification" class="headerlink" title="How well can machine-generated texts be identified and can language models be trained to avoid identification?"></a>How well can machine-generated texts be identified and can language models be trained to avoid identification?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16992">http://arxiv.org/abs/2310.16992</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sinclair Schneider, Florian Steuber, Joao A. G. Schneider, Gabi Dreo Rodosek</li>
<li>for: 本研究旨在 distinguishing 人工生成的文本与机器生成的文本。</li>
<li>methods: 我们使用了五种独立的语言模型来生成假 Tweets，并发现了 shallow learning 分类算法（如 Naive Bayes）可以达到0.6-0.8的检测精度。</li>
<li>results: 我们发现，使用高温值生成文本时，人类检测和机器检测之间存在显著差异，而使用 transformer 基于的分类算法可以达到0.9和更高的检测精度。 更 того，我们使用了强化学习approach来练习我们的生成模型，可以成功逃脱BERT基于的检测算法，其检测精度为0.15或更低。<details>
<summary>Abstract</summary>
With the rise of generative pre-trained transformer models such as GPT-3, GPT-NeoX, or OPT, distinguishing human-generated texts from machine-generated ones has become important. We refined five separate language models to generate synthetic tweets, uncovering that shallow learning classification algorithms, like Naive Bayes, achieve detection accuracy between 0.6 and 0.8.   Shallow learning classifiers differ from human-based detection, especially when using higher temperature values during text generation, resulting in a lower detection rate. Humans prioritize linguistic acceptability, which tends to be higher at lower temperature values. In contrast, transformer-based classifiers have an accuracy of 0.9 and above. We found that using a reinforcement learning approach to refine our generative models can successfully evade BERT-based classifiers with a detection accuracy of 0.15 or less.
</details>
<details>
<summary>摘要</summary>
随着生成预训练变换器模型如GPT-3、GPT-NeoX或OPT的出现，分辨人工生成的文本和机器生成的文本已经变得非常重要。我们对五种语言模型进行了精细调整，生成了Synthetic tweets，发现了使用Naive Bayes等浅学习分类算法时，检测精度在0.6-0.8之间。浅学习分类器与人类检测存在差异，尤其是在使用更高的温度值生成文本时，检测率较低。人类偏好语言可接受性，这种可接受性通常在低温度值时高。相比之下，transformer基类分类器的准确率为0.9和更高。我们发现使用强化学习方法来精细调整我们的生成模型可以成功逃脱BERT基类分类器，其检测精度为0.15或更低。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Social-Structures-from-Contemporary-Literary-Fiction-using-Character-Interaction-Graph-–-Half-Century-Chronology-of-Influential-Bengali-Writers"><a href="#Understanding-Social-Structures-from-Contemporary-Literary-Fiction-using-Character-Interaction-Graph-–-Half-Century-Chronology-of-Influential-Bengali-Writers" class="headerlink" title="Understanding Social Structures from Contemporary Literary Fiction using Character Interaction Graph – Half Century Chronology of Influential Bengali Writers"></a>Understanding Social Structures from Contemporary Literary Fiction using Character Interaction Graph – Half Century Chronology of Influential Bengali Writers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16968">http://arxiv.org/abs/2310.16968</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nafis Irtiza Tripto, Mohammed Eunus Ali</li>
<li>for: 本研究探讨了现代文学小说中社会结构和现实生活事件的影响，通过使用文本分析方法来解释这些现实现象。</li>
<li>methods: 本研究使用了自然语言处理（NLP）方法，包括情感分析、故事概要和主题分析，以及视觉化技术来分析和检索现代文学小说中的人物互动。</li>
<li>results: 研究发现，使用人物互动图（或网络）可以帮助解释和检索现代文学小说中的社会问题，并且可以对 Bengali 小说的影响进行特定的评估和信息检索。<details>
<summary>Abstract</summary>
Social structures and real-world incidents often influence contemporary literary fiction. Existing research in literary fiction analysis explains these real-world phenomena through the manual critical analysis of stories. Conventional Natural Language Processing (NLP) methodologies, including sentiment analysis, narrative summarization, and topic modeling, have demonstrated substantial efficacy in analyzing and identifying similarities within fictional works. However, the intricate dynamics of character interactions within fiction necessitate a more nuanced approach that incorporates visualization techniques. Character interaction graphs (or networks) emerge as a highly suitable means for visualization and information retrieval from the realm of fiction. Therefore, we leverage character interaction graphs with NLP-derived features to explore a diverse spectrum of societal inquiries about contemporary culture's impact on the landscape of literary fiction. Our study involves constructing character interaction graphs from fiction, extracting relevant graph features, and exploiting these features to resolve various real-life queries. Experimental evaluation of influential Bengali fiction over half a century demonstrates that character interaction graphs can be highly effective in specific assessments and information retrieval from literary fiction. Our data and codebase are available at https://cutt.ly/fbMgGEM
</details>
<details>
<summary>摘要</summary>
社会结构和现实生活中的事件常常影响当代文学小说。现有的文学分析研究通过手动分析故事来解释现实世界现象。传统的自然语言处理（NLP）方法，包括情感分析、简要摘要和话题模型，已经证明了在文学作品中的有效性。然而，小说中人物之间的复杂关系需要一种更细微的方法，该方法包括视觉化技术。因此，我们利用小说中人物之间的互动图（或网络）来可视化和检索文学作品中的信息。我们的研究包括从小说中构建人物互动图，提取有关图的重要特征，并利用这些特征来解决现实生活中各种社会问题。我们对印度文学中的影响力有限的 Bengali 小说进行实验评估，发现人物互动图可以在特定的评价和信息检索中表现出非常高效。我们的数据和代码库可以在以下链接中找到：https://cutt.ly/fbMgGEM。
</details></li>
</ul>
<hr>
<h2 id="Critic-Driven-Decoding-for-Mitigating-Hallucinations-in-Data-to-text-Generation"><a href="#Critic-Driven-Decoding-for-Mitigating-Hallucinations-in-Data-to-text-Generation" class="headerlink" title="Critic-Driven Decoding for Mitigating Hallucinations in Data-to-text Generation"></a>Critic-Driven Decoding for Mitigating Hallucinations in Data-to-text Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16964">http://arxiv.org/abs/2310.16964</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/langus0/critic-aware-decoding">https://github.com/langus0/critic-aware-decoding</a></li>
<li>paper_authors: Mateusz Lango, Ondřej Dušek</li>
<li>for:  Mitigating hallucinations in neural data-to-text generation</li>
<li>methods:  Combining probabilistic output of a generator language model with output of a special “text critic” classifier</li>
<li>results:  Improved performance on WebNLG and OpenDialKG benchmarks<details>
<summary>Abstract</summary>
Hallucination of text ungrounded in the input is a well-known problem in neural data-to-text generation. Many methods have been proposed to mitigate it, but they typically require altering model architecture or collecting additional data, and thus cannot be easily applied to an existing model. In this paper, we explore a new way to mitigate hallucinations by combining the probabilistic output of a generator language model (LM) with the output of a special "text critic" classifier, which guides the generation by assessing the match between the input data and the text generated so far. Our method does not need any changes to the underlying LM's architecture or training procedure and can thus be combined with any model and decoding operating on word probabilities. The critic does not need any additional training data, using the base LM's training data and synthetic negative examples. Our experimental results show that our method improves over the baseline on the WebNLG and OpenDialKG benchmarks.
</details>
<details>
<summary>摘要</summary>
幻像文本不受输入数据支持是神经网络数据到文本生成领域的一个常见问题。许多方法已经被提出来解决这个问题，但它们通常需要修改模型结构或收集更多数据，因此无法轻松应用于现有模型。在这篇论文中，我们探索了一种新的幻像 mitigation 方法，通过将生成语言模型（LM）的概率输出与一个特殊的 "文本评估器" 类型的分类器结合使用，以评估输入数据和已生成文本之间的匹配度。我们的方法不需要对基础LM的结构或训练过程进行任何更改，因此可以与任何模型和word probabilitiesdecoding进行组合。批评器也不需要额外的训练数据，只需使用基础LM的训练数据和 sintetic的负例来训练。我们的实验结果显示，我们的方法在WebNLG和OpenDialKG benchmark上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Muslim-Violence-Bias-Persists-in-Debiased-GPT-Models"><a href="#Muslim-Violence-Bias-Persists-in-Debiased-GPT-Models" class="headerlink" title="Muslim-Violence Bias Persists in Debiased GPT Models"></a>Muslim-Violence Bias Persists in Debiased GPT Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18368">http://arxiv.org/abs/2310.18368</a></li>
<li>repo_url: None</li>
<li>paper_authors: Babak Hemmatian, Razan Baltaji, Lav R. Varshney</li>
<li>for: 本研究旨在探讨GPT-3语言模型是否具有对穆斯林的偏见，以及如何使用不同的提示方式来降低这种偏见。</li>
<li>methods: 本研究使用了两个预注册的复制实验，其中一个使用了GPT-3模型，另一个使用了ChatGPT模型。两个实验中使用了不同的提示方式，以检测GPT-3模型是否具有偏见。</li>
<li>results: 研究发现，使用GPT-3模型时，对穆斯林的提示可能会导致更多的暴力结果，而对其他宗教的提示则不会。此外，使用ChatGPT模型时，也发现了类似的偏见。研究还发现了一些宗教特定的暴力主题，这些主题具有很强的不受欢迎的想法。<details>
<summary>Abstract</summary>
Abid et al. (2021) showed a tendency in GPT-3 to generate violent completions when prompted about Muslims, compared with other religions. Two pre-registered replication attempts found few violent completions and only the weakest anti-Muslim bias in the Instruct version, fine-tuned to eliminate biased and toxic outputs. However, more pre-registered experiments showed that using common names associated with the religions in prompts increases several-fold the rate of violent completions, revealing a highly significant second-order bias against Muslims. Our content analysis revealed religion-specific violent themes containing highly offensive ideas regardless of prompt format. Replications with ChatGPT suggest that any effects of GPT-3's de-biasing have disappeared with continued model development, as this newer model showed both a strong Muslim-violence bias and rates of violent completions closer to Abid et al. (2021). Our results show the need for continual de-biasing of models in ways that address higher-order associations.
</details>
<details>
<summary>摘要</summary>
阿比德等人 (2021) 发现，当提供有关穆斯林的提示时，GPT-3会生成暴力完成的倾向，相比其他宗教。两个预先注册的复现尝试发现了少量的暴力完成和只有最弱的反伊斯兰偏见在指导版本中，经过精心修改以消除恐怖和毒害输出。然而，更多的预先注册实验表明，在提示中使用宗教名称可以增加数量多少倍的暴力完成，揭示了高度显著的第二阶段偏见对穆斯林。我们的内容分析发现，不同宗教的暴力主题具有高度冒犯性的想法，无论提示格式如何。复现使用ChatGPT表明，GPT-3的去偏见效果已经消失，这 newer模型显示了强穆斯林暴力偏见和与Abid et al. (2021) 相似的暴力完成率。我们的结果表明，需要不断地去偏见模型，以解决更高级别的关联。
</details></li>
</ul>
<hr>
<h2 id="Zephyr-Direct-Distillation-of-LM-Alignment"><a href="#Zephyr-Direct-Distillation-of-LM-Alignment" class="headerlink" title="Zephyr: Direct Distillation of LM Alignment"></a>Zephyr: Direct Distillation of LM Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16944">http://arxiv.org/abs/2310.16944</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/huggingface/alignment-handbook">https://github.com/huggingface/alignment-handbook</a></li>
<li>paper_authors: Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, Thomas Wolf</li>
<li>for: 这 paper 的目的是提高 chat 模型的意图对接。</li>
<li>methods: 这 paper 使用了 distilled supervised fine-tuning (dSFT) 和 preference data from AI Feedback (AIF) 来学习一个高效的 chat 模型。</li>
<li>results: 这 paper 的最终结果是 Zephyr-7B，这是一个基于 7B 参数模型的 chat 模型，在 chat benchmark 上达到了新的州OF-the-art 水平，并不需要人工标注。<details>
<summary>Abstract</summary>
We aim to produce a smaller language model that is aligned to user intent. Previous research has shown that applying distilled supervised fine-tuning (dSFT) on larger models significantly improves task accuracy; however, these models are unaligned, i.e. they do not respond well to natural prompts. To distill this property, we experiment with the use of preference data from AI Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization (dDPO) to learn a chat model with significantly improved intent alignment. The approach requires only a few hours of training without any additional sampling during fine-tuning. The final result, Zephyr-7B, sets the state-of-the-art on chat benchmarks for 7B parameter models, and requires no human annotation. In particular, results on MT-Bench show that Zephyr-7B surpasses Llama2-Chat-70B, the best open-access RLHF-based model. Code, models, data, and tutorials for the system are available at https://github.com/huggingface/alignment-handbook.
</details>
<details>
<summary>摘要</summary>
我团队目标是开发一个更小的语言模型，并将其与用户意图进行对齐。前一研究表明，通过对更大的模型进行混合精度微调（dSFT）可以显著提高任务准确率，但这些模型通常不具备自然提示的响应能力。为了抓取这个特性，我们尝试使用人工智能反馈（AIF）的偏好数据进行直接偏好优化（dDPO），从而学习一个与用户意图更好地对齐的对话模型。这种方法只需几个小时的训练，不需任何额外的采样，并且不需人工标注。最终的结果是Zephyr-7B，它在7B参数模型上设置了对话benchmark的新纪录，并且在MT-Bench上超越了Llama2-Chat-70B，这是最佳的开放访问RLHF-based模型。我们提供了相关的代码、模型、数据和教程，可以在https://github.com/huggingface/alignment-handbook上下载。
</details></li>
</ul>
<hr>
<h2 id="Learning-Transfers-over-Several-Programming-Languages"><a href="#Learning-Transfers-over-Several-Programming-Languages" class="headerlink" title="Learning Transfers over Several Programming Languages"></a>Learning Transfers over Several Programming Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16937">http://arxiv.org/abs/2310.16937</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sfedfcv/redesigned-pancake">https://github.com/Sfedfcv/redesigned-pancake</a></li>
<li>paper_authors: Razan Baltaji, Saurabh Pujar, Louis Mandel, Martin Hirzel, Luca Buratti, Lav Varshney</li>
<li>for: 这篇论文旨在探讨跨语言传输学习在编程语言中的可行性和效果。</li>
<li>methods: 该论文使用了一种基于转换器的大型自然语言模型，并在11到41种编程语言中进行了广泛的实验，以探讨以下问题：首先，跨语言传输在不同语言对的效果如何？其次，给定任务和目标语言，如何选择最佳的源语言？第三，哪些语言对传输性能有优先顺序？第四，这些因素受到任务的影响如何？</li>
<li>results: 研究发现，跨语言传输可以在多种任务中提供有效的提升，且可以通过选择合适的源语言来提高效果。此外，研究还发现了一些语言对传输性能有优先顺序的特征，这些特征可以用于任务选择和语言对选择。<details>
<summary>Abstract</summary>
Large language models (LLMs) have recently become remarkably good at improving developer productivity for high-resource programming languages. These models use two kinds of data: large amounts of unlabeled code samples for pretraining and relatively smaller amounts of labeled code samples for fine-tuning or in-context learning. Unfortunately, many programming languages are low-resource, lacking labeled samples for most tasks and often even lacking unlabeled samples. Therefore, users of low-resource languages (e.g., legacy or new languages) miss out on the benefits of LLMs. Cross-lingual transfer learning uses data from a source language to improve model performance on a target language. It has been well-studied for natural languages, but has received little attention for programming languages. This paper reports extensive experiments on four tasks using a transformer-based LLM and 11 to 41 programming languages to explore the following questions. First, how well cross-lingual transfer works for a given task across different language pairs. Second, given a task and target language, how to best choose a source language. Third, the characteristics of a language pair that are predictive of transfer performance, and fourth, how that depends on the given task.
</details>
<details>
<summary>摘要</summary>
大型语言模型 (LLM) 在高资源编程语言中提高开发人员产量的能力已经很有起色。这些模型使用两种数据：大量的无标示代码样本用于预训练，以及相对较小的标注代码样本用于细化或在场景学习。然而，许多编程语言是低资源的，缺乏大多数任务的标注样本，甚至缺乏无标示样本。因此，使用低资源语言的用户（例如遗产语言或新语言）无法享受到 LLM 的好处。 Cross-lingual transfer learning 使用来自源语言的数据来改善目标语言的模型性能。它在自然语言方面得到了广泛的研究，但在编程语言方面得到了少量的注意。本文报告了使用 transformer-based LLM 和 11 到 41 种编程语言进行了广泛的实验，以探索以下问题：1.  across different language pairs, how well does cross-lingual transfer work for a given task?2.  given a task and target language, how to best choose a source language?3.  what are the characteristics of a language pair that are predictive of transfer performance, and4.  how does that depend on the given task?
</details></li>
</ul>
<hr>
<h2 id="Physician-Detection-of-Clinical-Harm-in-Machine-Translation-Quality-Estimation-Aids-in-Reliance-and-Backtranslation-Identifies-Critical-Errors"><a href="#Physician-Detection-of-Clinical-Harm-in-Machine-Translation-Quality-Estimation-Aids-in-Reliance-and-Backtranslation-Identifies-Critical-Errors" class="headerlink" title="Physician Detection of Clinical Harm in Machine Translation: Quality Estimation Aids in Reliance and Backtranslation Identifies Critical Errors"></a>Physician Detection of Clinical Harm in Machine Translation: Quality Estimation Aids in Reliance and Backtranslation Identifies Critical Errors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16924">http://arxiv.org/abs/2310.16924</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/n-mehandru/physicianqe">https://github.com/n-mehandru/physicianqe</a></li>
<li>paper_authors: Nikita Mehandru, Sweta Agrawal, Yimin Xiao, Elaine C Khoong, Ge Gao, Marine Carpuat, Niloufar Salehi</li>
<li>for: 这个论文的目的是提高机器翻译（MT）在实际应用中的可靠性，特别是帮助用户做出有知识的决策。</li>
<li>methods: 这篇论文使用了质量评估技术来自动评估MT质量，并在实际应用场景中进行了人类研究，以评估这些技术的有效性。</li>
<li>results: 研究发现，基于质量评估的干预可以提高用户对MT输出的有效使用，但是返回翻译可以帮助医生检测更多的严重错误，而质量评估单独无法捕捉这些错误。<details>
<summary>Abstract</summary>
A major challenge in the practical use of Machine Translation (MT) is that users lack guidance to make informed decisions about when to rely on outputs. Progress in quality estimation research provides techniques to automatically assess MT quality, but these techniques have primarily been evaluated in vitro by comparison against human judgments outside of a specific context of use. This paper evaluates quality estimation feedback in vivo with a human study simulating decision-making in high-stakes medical settings. Using Emergency Department discharge instructions, we study how interventions based on quality estimation versus backtranslation assist physicians in deciding whether to show MT outputs to a patient. We find that quality estimation improves appropriate reliance on MT, but backtranslation helps physicians detect more clinically harmful errors that QE alone often misses.
</details>
<details>
<summary>摘要</summary>
Machine Translation（MT）在实际应用中的一个主要挑战是用户缺乏指导来做出了解MT输出的决策。质量评估研究的进步提供了自动评估MT质量的技术，但这些技术主要在室外进行了人工评估，而不是在特定的使用场景下进行评估。本文通过医疗高危 Settings中的医生决策模拟研究，评估了基于质量评估和反编译的干预对MT输出的影响。我们发现，质量评估可以提高对MT的有效依赖，但反编译可以帮助医生检测更多的严重错误，这些错误经常被QEalonemiss。
</details></li>
</ul>
<hr>
<h2 id="Divide-et-Impera-Multi-Transformer-Architectures-for-Complex-NLP-Tasks"><a href="#Divide-et-Impera-Multi-Transformer-Architectures-for-Complex-NLP-Tasks" class="headerlink" title="Divide et Impera: Multi-Transformer Architectures for Complex NLP-Tasks"></a>Divide et Impera: Multi-Transformer Architectures for Complex NLP-Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16897">http://arxiv.org/abs/2310.16897</a></li>
<li>repo_url: None</li>
<li>paper_authors: Solveig Helland, Elena Gavagnin, Alexandre de Spindler</li>
<li>for: 解决复杂的自然语言处理任务，例如减少性别偏见。</li>
<li>methods: 将复杂任务拆分成 simpler subtask，并使用多个transformer模型进行 fine-tuning，以实现更好的控制性。</li>
<li>results: 在使用多个模型进行 fine-tuning时，性别偏见减少的效果更好than使用单个模型。<details>
<summary>Abstract</summary>
The growing capabilities of transformer models pave the way for solving increasingly complex NLP tasks. A key to supporting application-specific requirements is the ability to fine-tune. However, compiling a fine-tuning dataset tailored to complex tasks is tedious and results in large datasets, limiting the ability to control transformer output. We present an approach in which complex tasks are divided into simpler subtasks. Multiple transformer models are fine-tuned to one subtask each, and lined up to accomplish the complex task. This simplifies the compilation of fine-tuning datasets and increases overall controllability. Using the example of reducing gender bias as a complex task, we demonstrate our approach and show that it performs better than using a single model.
</details>
<details>
<summary>摘要</summary>
transformer 模型的增长能力为解决越来越复杂的自然语言处理任务开创了道路。一个关键是可以微调，但是为复杂任务编译微调数据集是费时且导致数据集较大，限制了transformer输出的控制。我们提出了一种方法，将复杂任务分解成更简单的子任务。多个transformer模型对每个子任务进行微调，并将它们组合起来完成复杂任务。这种方法可以简化微调数据集的编译，并提高总的控制性。使用减少性别偏见为复杂任务的示例，我们示出了我们的方法的效果，并表明它在单一模型的情况下表现更好。
</details></li>
</ul>
<hr>
<h2 id="Discrete-Diffusion-Language-Modeling-by-Estimating-the-Ratios-of-the-Data-Distribution"><a href="#Discrete-Diffusion-Language-Modeling-by-Estimating-the-Ratios-of-the-Data-Distribution" class="headerlink" title="Discrete Diffusion Language Modeling by Estimating the Ratios of the Data Distribution"></a>Discrete Diffusion Language Modeling by Estimating the Ratios of the Data Distribution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16834">http://arxiv.org/abs/2310.16834</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aaron Lou, Chenlin Meng, Stefano Ermon<br>for:* This paper aims to improve the performance of diffusion models on discrete data domains, such as natural language, by proposing a novel discrete score matching loss called score entropy.methods:* The proposed method, called Score Entropy Discrete Diffusion (SEDD), uses a denoising variant of the score entropy loss to efficiently optimize the model for maximum likelihood training.results:* The SEDD model achieves highly competitive likelihoods compared to the baseline GPT-2 model, and has several algorithmic advantages such as learning a more faithful sequence distribution, trading off compute for generation quality, and enabling arbitrary infilling beyond the standard left to right prompting.<details>
<summary>Abstract</summary>
Despite their groundbreaking performance for many generative modeling tasks, diffusion models have fallen short on discrete data domains such as natural language. Crucially, standard diffusion models rely on the well-established theory of score matching, but efforts to generalize this to discrete structures have not yielded the same empirical gains. In this work, we bridge this gap by proposing score entropy, a novel discrete score matching loss that is more stable than existing methods, forms an ELBO for maximum likelihood training, and can be efficiently optimized with a denoising variant. We scale our Score Entropy Discrete Diffusion models (SEDD) to the experimental setting of GPT-2, achieving highly competitive likelihoods while also introducing distinct algorithmic advantages. In particular, when comparing similarly sized SEDD and GPT-2 models, SEDD attains comparable perplexities (normally within $+10\%$ of and sometimes outperforming the baseline). Furthermore, SEDD models learn a more faithful sequence distribution (around $4\times$ better compared to GPT-2 models with ancestral sampling as measured by large models), can trade off compute for generation quality (needing only $16\times$ fewer network evaluations to match GPT-2), and enables arbitrary infilling beyond the standard left to right prompting.
</details>
<details>
<summary>摘要</summary>
尽管扩散模型在许多生成模型任务上表现出色，但它们在自然语言类数据上的表现却不如预期。原因是标准的扩散模型依赖于已经成熟的分数匹配理论，但将这种理论扩展到逻辑结构上并没有得到同样的实验性提升。在这项工作中，我们bridges这个差距，提出了一种新的简 discrete分数匹配损失函数，即分数 entropy，它更稳定、能够形成 ELBO  для最大化可能性训练，并且可以高效地使用降噪变体进行优化。我们在 GPT-2 实验设置下扩大了 Score Entropy Discrete Diffusion 模型（SEDD），达到了非常竞争的可能性，同时也提供了一些算法优势。具体来说，与相同大小的 SEDD 和 GPT-2 模型相比，SEDD 可以达到与基准相同的折算值（通常在 $+10\%$ 以内，有时 même outperforming 基准），并且 SEDD 模型可以更好地学习数据序列分布（与 GPT-2 模型在 ancestral sampling 下的分布相比，大约 $4\times$ 更好），可以交换计算量和生成质量（只需 $16\times$  fewer network evaluations 可以与 GPT-2 模型匹配），并且允许随意填充 beyond 标准的左到右提示。
</details></li>
</ul>
<hr>
<h2 id="Language-Agnostic-Code-Embeddings"><a href="#Language-Agnostic-Code-Embeddings" class="headerlink" title="Language Agnostic Code Embeddings"></a>Language Agnostic Code Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16803">http://arxiv.org/abs/2310.16803</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/snknitin/Multilingual-Embeddings-using-ACS-for-Cross-lingual-NLP">https://github.com/snknitin/Multilingual-Embeddings-using-ACS-for-Cross-lingual-NLP</a></li>
<li>paper_authors: Saiteja Utpala, Alex Gu, Pin Yu Chen</li>
<li>for: 本研究探讨了多种编程语言的代码嵌入，尤其是跨语言代码嵌入的跨语言能力。</li>
<li>methods: 通过探索实验，研究发现代码嵌入包含两个不同组成部分：一个深深地关联到特定语言的细节和 sintaxis，另一个主要关注 semantics，不受语言细节影响。</li>
<li>results: 当我们隔离并消除语言特定的组成部分时，在下游代码检索任务中观察到显著改善，MRR提高了+17。<details>
<summary>Abstract</summary>
Recently, code language models have achieved notable advancements in addressing a diverse array of essential code comprehension and generation tasks. Yet, the field lacks a comprehensive deep dive and understanding of the code embeddings of multilingual code models. In this paper, we present a comprehensive study on multilingual code embeddings, focusing on the cross-lingual capabilities of these embeddings across different programming languages. Through probing experiments, we demonstrate that code embeddings comprise two distinct components: one deeply tied to the nuances and syntax of a specific language, and the other remaining agnostic to these details, primarily focusing on semantics. Further, we show that when we isolate and eliminate this language-specific component, we witness significant improvements in downstream code retrieval tasks, leading to an absolute increase of up to +17 in the Mean Reciprocal Rank (MRR).
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Detecting-Pretraining-Data-from-Large-Language-Models"><a href="#Detecting-Pretraining-Data-from-Large-Language-Models" class="headerlink" title="Detecting Pretraining Data from Large Language Models"></a>Detecting Pretraining Data from Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16789">http://arxiv.org/abs/2310.16789</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/swj0419/detect-pretrain-code">https://github.com/swj0419/detect-pretrain-code</a></li>
<li>paper_authors: Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, Luke Zettlemoyer<br>for:这篇论文是为了研究大型自然语言模型（LLM）的预训练数据检测问题而写的。methods:这篇论文提出了一种新的检测方法，即使用Min-K% Prob方法，该方法基于一个简单的假设：未看过的示例可能会包含一些低概率词语，而已经看过的示例则 less likely 会有这些低概率词语。此外，这种方法不需要知道预训练词库或任何额外训练，因此与之前的检测方法不同。results:实验表明，Min-K% Prob 方法在 WIKIMIA 上比之前的方法提高了7.4%。此外，这种方法在实际应用中，如检测版权书籍和污染下游示例的问题上也表现了良好的效果。<details>
<summary>Abstract</summary>
Although large language models (LLMs) are widely deployed, the data used to train them is rarely disclosed. Given the incredible scale of this data, up to trillions of tokens, it is all but certain that it includes potentially problematic text such as copyrighted materials, personally identifiable information, and test data for widely reported reference benchmarks. However, we currently have no way to know which data of these types is included or in what proportions. In this paper, we study the pretraining data detection problem: given a piece of text and black-box access to an LLM without knowing the pretraining data, can we determine if the model was trained on the provided text? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that uses data created before and after model training to support gold truth detection. We also introduce a new detection method Min-K% Prob based on a simple hypothesis: an unseen example is likely to contain a few outlier words with low probabilities under the LLM, while a seen example is less likely to have words with such low probabilities. Min-K% Prob can be applied without any knowledge about the pretraining corpus or any additional training, departing from previous detection methods that require training a reference model on data that is similar to the pretraining data. Moreover, our experiments demonstrate that Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previous methods. We apply Min-K% Prob to two real-world scenarios, copyrighted book detection, and contaminated downstream example detection, and find it a consistently effective solution.
</details>
<details>
<summary>摘要</summary>
尽管大型语言模型（LLM）广泛应用，但训练它们的数据几乎 nunca 被披露。这些数据的规模可以达到数十亿个字符，因此可能包含可能有问题的文本，如版权保护的内容、个人可识别信息和报道的参考基准测试数据。然而，我们目前没有任何方式可以了解这些类型的数据是否包含在内，以及它们的占比。在这篇论文中，我们研究了预训练数据检测问题：给定一个文本，无需知道预训练数据，可以使用黑框访问 LLl 来判断这个文本是否包含在预训练数据中？为了支持这项研究，我们引入了一个动态 benchmark 名为 WIKIMIA，它使用了在模型训练前后创建的数据来支持金实验 truth 检测。我们还提出了一种新的检测方法，即 Min-K% Prob，基于简单的假设：未seen 的例子很可能包含一些低概率的单词，而seen 的例子则更 unlikely 会有这些低概率的单词。Min-K% Prob 可以无需了解预训练集或任何额外训练，与之前的检测方法不同。此外，我们的实验表明，Min-K% Prob 在 WIKIMIA 上的提升为 7.4%。我们在实际应用中使用 Min-K% Prob 来检测版权保护的书籍和下游示例中的污染，发现它是一个可靠的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Kiki-or-Bouba-Sound-Symbolism-in-Vision-and-Language-Models"><a href="#Kiki-or-Bouba-Sound-Symbolism-in-Vision-and-Language-Models" class="headerlink" title="Kiki or Bouba? Sound Symbolism in Vision-and-Language Models"></a>Kiki or Bouba? Sound Symbolism in Vision-and-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16781">http://arxiv.org/abs/2310.16781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Morris Alper, Hadar Averbuch-Elor</li>
<li>for: 这个研究探讨了语音 Symbolism在计算机视觉语言模型CLIP和Stable Diffusion中是否存在强烈的印证。</li>
<li>methods: 该研究使用零shot知识探测来调查这些模型内置的知识，并发现它们 действительно具有这种印证，与心理语言学中知名的吃苹果效应相似。</li>
<li>results: 该研究发现计算机视觉语言模型CLIP和Stable Diffusion中存在强烈的印证，证明了语音 Symbolism的存在并提供了一种计算机方法来证明和理解它的性质。<details>
<summary>Abstract</summary>
Although the mapping between sound and meaning in human language is assumed to be largely arbitrary, research in cognitive science has shown that there are non-trivial correlations between particular sounds and meanings across languages and demographic groups, a phenomenon known as sound symbolism. Among the many dimensions of meaning, sound symbolism is particularly salient and well-demonstrated with regards to cross-modal associations between language and the visual domain. In this work, we address the question of whether sound symbolism is reflected in vision-and-language models such as CLIP and Stable Diffusion. Using zero-shot knowledge probing to investigate the inherent knowledge of these models, we find strong evidence that they do show this pattern, paralleling the well-known kiki-bouba effect in psycholinguistics. Our work provides a novel method for demonstrating sound symbolism and understanding its nature using computational tools. Our code will be made publicly available.
</details>
<details>
<summary>摘要</summary>
尽管人类语言中音与意义的映射被视为大体是随机的，但是认知科学研究发现，不同语言和人群之间的声音和意义之间存在一定的相互关系，这种现象被称为声 Symbolism。在多种意义维度中，声 Symbolism 特别是与视觉领域的交互关系非常显著，在这种情况下，我们研究了 CLIP 和 Stable Diffusion 等视觉语言模型是否具备声 Symbolism 特征。通过零 shot 知识探测，我们发现这些模型确实具备这种特征，与 психолингвисти学中著名的 kiki-bouba 效应相似。我们的工作提供了一种计算工具来探索和理解声 Symbolism 的方法，代码将公开发布。
</details></li>
</ul>
<hr>
<h2 id="IntenDD-A-Unified-Contrastive-Learning-Approach-for-Intent-Detection-and-Discovery"><a href="#IntenDD-A-Unified-Contrastive-Learning-Approach-for-Intent-Detection-and-Discovery" class="headerlink" title="IntenDD: A Unified Contrastive Learning Approach for Intent Detection and Discovery"></a>IntenDD: A Unified Contrastive Learning Approach for Intent Detection and Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16761">http://arxiv.org/abs/2310.16761</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bhavuk Singhal, Ashim Gupta, Shivasankaran V P, Amrith Krishna</li>
<li>for: 本文旨在提出一种能够同时处理多类和多标签的任务 oriented dialogue系统中的意向识别任务。</li>
<li>methods: 本文提出了一种名为 IntenDD 的独特方法，它利用共享的语句编码器来解决意向识别任务。该方法采用了一种不需要监督的对比学习策略，其中 pseudo-labels 是基于语句的字典特征来生成的。此外，本文还提出了一种两步后处理设置，用于类型化任务，其中包括卷积投影和修正。</li>
<li>results: 经过广泛的测试，本文发现 IntenDD 可以在多个数据集上与竞争对手相比，常常具有更高的性能。特别是，在少量数据情况下，IntenDD 的性能提高了2.32%、1.26%和1.52%。<details>
<summary>Abstract</summary>
Identifying intents from dialogue utterances forms an integral component of task-oriented dialogue systems. Intent-related tasks are typically formulated either as a classification task, where the utterances are classified into predefined categories or as a clustering task when new and previously unknown intent categories need to be discovered from these utterances. Further, the intent classification may be modeled in a multiclass (MC) or multilabel (ML) setup. While typically these tasks are modeled as separate tasks, we propose IntenDD, a unified approach leveraging a shared utterance encoding backbone. IntenDD uses an entirely unsupervised contrastive learning strategy for representation learning, where pseudo-labels for the unlabeled utterances are generated based on their lexical features. Additionally, we introduce a two-step post-processing setup for the classification tasks using modified adsorption. Here, first, the residuals in the training data are propagated followed by smoothing the labels both modeled in a transductive setting. Through extensive evaluations on various benchmark datasets, we find that our approach consistently outperforms competitive baselines across all three tasks. On average, IntenDD reports percentage improvements of 2.32%, 1.26%, and 1.52% in their respective metrics for few-shot MC, few-shot ML, and the intent discovery tasks respectively.
</details>
<details>
<summary>摘要</summary>
标准化对话语言理解是任务导向对话系统的一个重要组成部分。intent相关任务通常被формализова为分类任务，其中对话语言被分类为预定的类别，或者为聚类任务，当新的意图类别需要从对话语言中发现时。此外，意向分类可能是多类（MC）或多标签（ML）的设置。通常这些任务是分开模型的，但我们提出了IntenDD，一种综合方法，利用共享对话语言编码核心。IntenDD使用了一种完全无监督对比学习的表征学习策略，其中 pseudo-标签 для无标签对话语言是基于其语言特征生成的。此外，我们引入了一种两步后处理设置，其中首先在训练数据中的剩余被传播，然后对模型进行滑动平滑。通过对多个 benchmark 数据集进行广泛的评估，我们发现，我们的方法在所有三个任务中 consistently 超越竞争对手的基eline。在 average 的情况下，IntenDD Report了分类任务的准确率提高2.32%、1.26%和1.52%。
</details></li>
</ul>
<hr>
<h2 id="DISCO-A-Large-Scale-Human-Annotated-Corpus-for-Disfluency-Correction-in-Indo-European-Languages"><a href="#DISCO-A-Large-Scale-Human-Annotated-Corpus-for-Disfluency-Correction-in-Indo-European-Languages" class="headerlink" title="DISCO: A Large Scale Human Annotated Corpus for Disfluency Correction in Indo-European Languages"></a>DISCO: A Large Scale Human Annotated Corpus for Disfluency Correction in Indo-European Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16749">http://arxiv.org/abs/2310.16749</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vineet2104/disco">https://github.com/vineet2104/disco</a></li>
<li>paper_authors: Vineet Bhat, Preethi Jyothi, Pushpak Bhattacharyya</li>
<li>for: 该研究的目的是提供多语言干扰纠正（DC）的高质量人类标注数据集，以便进行多语言干扰纠正研究。</li>
<li>methods: 该研究使用了现有的DC模型，对四种重要的印欧语言（英语、希腊语、德语和法语）进行了分析。</li>
<li>results: 研究得到了四种语言的DC模型的F1分数，分别为97.55（英语）、94.29（希腊语）、95.89（德语）和92.97（法语）。此外，研究还表明，DC可以提高下游任务的BLEU分数平均5.65分。<details>
<summary>Abstract</summary>
Disfluency correction (DC) is the process of removing disfluent elements like fillers, repetitions and corrections from spoken utterances to create readable and interpretable text. DC is a vital post-processing step applied to Automatic Speech Recognition (ASR) outputs, before subsequent processing by downstream language understanding tasks. Existing DC research has primarily focused on English due to the unavailability of large-scale open-source datasets. Towards the goal of multilingual disfluency correction, we present a high-quality human-annotated DC corpus covering four important Indo-European languages: English, Hindi, German and French. We provide extensive analysis of results of state-of-the-art DC models across all four languages obtaining F1 scores of 97.55 (English), 94.29 (Hindi), 95.89 (German) and 92.97 (French). To demonstrate the benefits of DC on downstream tasks, we show that DC leads to 5.65 points increase in BLEU scores on average when used in conjunction with a state-of-the-art Machine Translation (MT) system. We release code to run our experiments along with our annotated dataset here.
</details>
<details>
<summary>摘要</summary>
“缺乏流畅性调正（DC）是将口语说话中的填充词、重复和修正元素移除，以创建可读和解释的文本的过程。DC 是自动语音识别（ASR）输出的重要后处理步骤，在下游语言理解任务前进行。现有 DC 研究主要集中在英语上，因为大规模的开源数据集的不足。为了多语言缺乏流畅性调正，我们发布了高品质的人类评估 DC 数据库，覆盖四种重要的印欧语言：英语、希腊语、德语和法语。我们提供了广泛的 DC 模型的结果分析，其中英语的 F1 分数为 97.55，希腊语的 F1 分数为 94.29，德语的 F1 分数为 95.89，法语的 F1 分数为 92.97。为了证明 DC 对下游任务的好处，我们显示了 DC 对 Machine Translation（MT）系统的均值 BLEU 分数提高 5.65 分。我们在这里发布了实验代码和数据。”
</details></li>
</ul>
<hr>
<h2 id="HANSEN-Human-and-AI-Spoken-Text-Benchmark-for-Authorship-Analysis"><a href="#HANSEN-Human-and-AI-Spoken-Text-Benchmark-for-Authorship-Analysis" class="headerlink" title="HANSEN: Human and AI Spoken Text Benchmark for Authorship Analysis"></a>HANSEN: Human and AI Spoken Text Benchmark for Authorship Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16746">http://arxiv.org/abs/2310.16746</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nafis Irtiza Tripto, Adaku Uchendu, Thai Le, Mattia Setzu, Fosca Giannotti, Dongwon Lee<br>for:这个论文主要是为了提高人工智能对口语文本的分析能力。methods:这个论文使用了现有的口语数据集，以及使用3种知名的大语言模型（ChatGPT、PaLM2和Vicuna13B）生成的人工生成的口语数据集。results:这个论文通过对人类口语数据集和人工生成的口语数据集进行作者归属分析和作者验证，以及人工生成 spoken text检测等方面的研究，以提高人工智能对口语文本的分析能力。<details>
<summary>Abstract</summary>
Authorship Analysis, also known as stylometry, has been an essential aspect of Natural Language Processing (NLP) for a long time. Likewise, the recent advancement of Large Language Models (LLMs) has made authorship analysis increasingly crucial for distinguishing between human-written and AI-generated texts. However, these authorship analysis tasks have primarily been focused on written texts, not considering spoken texts. Thus, we introduce the largest benchmark for spoken texts - HANSEN (Human ANd ai Spoken tExt beNchmark). HANSEN encompasses meticulous curation of existing speech datasets accompanied by transcripts, alongside the creation of novel AI-generated spoken text datasets. Together, it comprises 17 human datasets, and AI-generated spoken texts created using 3 prominent LLMs: ChatGPT, PaLM2, and Vicuna13B. To evaluate and demonstrate the utility of HANSEN, we perform Authorship Attribution (AA) & Author Verification (AV) on human-spoken datasets and conducted Human vs. AI spoken text detection using state-of-the-art (SOTA) models. While SOTA methods, such as, character ngram or Transformer-based model, exhibit similar AA & AV performance in human-spoken datasets compared to written ones, there is much room for improvement in AI-generated spoken text detection. The HANSEN benchmark is available at: https://huggingface.co/datasets/HANSEN-REPO/HANSEN.
</details>
<details>
<summary>摘要</summary>
《作者分析》也称为《风格分析》，已经是自然语言处理（NLP）的一个基本方面。而在最近，大型语言模型（LLMs）的发展使得作者分析变得越来越重要，以确定人类写作和人工智能生成的文本之间的区别。然而，这些作者分析任务主要集中在written文本上，忽略了口语文本。因此，我们介绍了最大的口语文本benchmark - HANSEN（人类和AI语音文本benchmark）。HANSEN包括了仔细筹集的现有口语数据集，并与创建了使用3个知名的LLMs：ChatGPT、PaLM2和Vicuna13B生成的口语文本数据集。总的来说，HANSEN包括17个人类数据集，以及由AI生成的口语文本。为了评估和利用HANSEN，我们在人类口语数据集上进行了作者归属（AA）和作者验证（AV），并使用当前最佳（SOTA）模型进行人类vsAI口语文本检测。虽然SOTA方法，如字符串igrams或Transformer基本模型，在人类口语数据集上与written数据集相比， AA&AV性能相似，但AI生成口语文本检测仍有很大的改进空间。HANSEN benchmark可以在以下地址获取：https://huggingface.co/datasets/HANSEN-REPO/HANSEN。
</details></li>
</ul>
<hr>
<h2 id="Improving-Conversational-Recommendation-Systems-via-Bias-Analysis-and-Language-Model-Enhanced-Data-Augmentation"><a href="#Improving-Conversational-Recommendation-Systems-via-Bias-Analysis-and-Language-Model-Enhanced-Data-Augmentation" class="headerlink" title="Improving Conversational Recommendation Systems via Bias Analysis and Language-Model-Enhanced Data Augmentation"></a>Improving Conversational Recommendation Systems via Bias Analysis and Language-Model-Enhanced Data Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16738">http://arxiv.org/abs/2310.16738</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangxieric/bias-crs">https://github.com/wangxieric/bias-crs</a></li>
<li>paper_authors: Xi Wang, Hossein A. Rahmani, Jiqun Liu, Emine Yilmaz</li>
<li>for: 这个论文的目的是提出两种新的数据增强策略，以解决 conversational recommendation 中的各种偏见问题。</li>
<li>methods: 该论文使用了语言模型和数据增强技术，并从 Success of generative data 中灵感得到了两种新的数据增强策略：’Once-Aug’ 和 ‘PopNudge’。</li>
<li>results: 经过对 ReDial 和 TG-ReDial 数据集的广泛实验，该论文表明了 CRS 技术的表现得到了通过数据增强方法的改进，并提供了多种偏见问题的解决方案。<details>
<summary>Abstract</summary>
Conversational Recommendation System (CRS) is a rapidly growing research area that has gained significant attention alongside advancements in language modelling techniques. However, the current state of conversational recommendation faces numerous challenges due to its relative novelty and limited existing contributions. In this study, we delve into benchmark datasets for developing CRS models and address potential biases arising from the feedback loop inherent in multi-turn interactions, including selection bias and multiple popularity bias variants. Drawing inspiration from the success of generative data via using language models and data augmentation techniques, we present two novel strategies, 'Once-Aug' and 'PopNudge', to enhance model performance while mitigating biases. Through extensive experiments on ReDial and TG-ReDial benchmark datasets, we show a consistent improvement of CRS techniques with our data augmentation approaches and offer additional insights on addressing multiple newly formulated biases.
</details>
<details>
<summary>摘要</summary>
对话推荐系统（CRS）是一个迅速成长的研究领域，与语言模型技术的进步相互发展。然而，目前的对话推荐面临许多挑战，主要是因为这个领域的相对新颖性和有限的现有贡献。在这篇研究中，我们探索了对话推荐模型的 benchmarck 数据集，并处理了从多轮互动中产生的可能的偏见，包括选择偏见和多个流行度偏见的多种变体。受到语言模型和数据增强技术的成功的灵感，我们提出了两种新的策略，“Once-Aug”和“PopNudge”，以提高模型性能并减少偏见。通过广泛的实验，我们显示了对 ReDial 和 TG-ReDial 实验数据集的一致性改进，并提供了多个对多个新的偏见的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Disentangling-Extraction-and-Reasoning-in-Multi-hop-Spatial-Reasoning"><a href="#Disentangling-Extraction-and-Reasoning-in-Multi-hop-Spatial-Reasoning" class="headerlink" title="Disentangling Extraction and Reasoning in Multi-hop Spatial Reasoning"></a>Disentangling Extraction and Reasoning in Multi-hop Spatial Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16731">http://arxiv.org/abs/2310.16731</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roshanak Mirzaee, Parisa Kordjamshidi</li>
<li>for: 这篇论文旨在探讨文本空间理解的挑战，以及如何通过分离信息提取和理解过程来解决这个挑战。</li>
<li>methods: 作者设计了多种模型，其中一些模型将信息提取和理解分离开来，并与现有基eline进行比较。</li>
<li>results: 实验结果表明，分离信息提取和理解过程可以提高模型在真实数据领域的普适性。<details>
<summary>Abstract</summary>
Spatial reasoning over text is challenging as the models not only need to extract the direct spatial information from the text but also reason over those and infer implicit spatial relations. Recent studies highlight the struggles even large language models encounter when it comes to performing spatial reasoning over text. In this paper, we explore the potential benefits of disentangling the processes of information extraction and reasoning in models to address this challenge. To explore this, we design various models that disentangle extraction and reasoning(either symbolic or neural) and compare them with state-of-the-art(SOTA) baselines with no explicit design for these parts. Our experimental results consistently demonstrate the efficacy of disentangling, showcasing its ability to enhance models' generalizability within realistic data domains.
</details>
<details>
<summary>摘要</summary>
对文本空间逻辑是挑战，因为模型不仅需要从文本中提取直接的空间信息，而且还需要对其进行推理和推论，推理出隐藏的空间关系。Recent studies表明，even large language models still struggle with spatial reasoning over text. 在这篇论文中，我们探索了分解模型中提取信息和推理的过程可能带来的可能性。为了做到这一点，我们设计了不同的模型，其中包括分解为符号或神经网络的模型，并与现有的基elines进行比较。我们的实验结果一致地表明了分解的效果，它能够提高模型在真实数据领域的普遍性。
</details></li>
</ul>
<hr>
<h2 id="LLM-Performance-Predictors-are-good-initializers-for-Architecture-Search"><a href="#LLM-Performance-Predictors-are-good-initializers-for-Architecture-Search" class="headerlink" title="LLM Performance Predictors are good initializers for Architecture Search"></a>LLM Performance Predictors are good initializers for Architecture Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16712">http://arxiv.org/abs/2310.16712</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ganesh Jawahar, Muhammad Abdul-Mageed, Laks V. S. Lakshmanan, Dujian Ding</li>
<li>for: 这个论文的目的是使用大型自然语言处理（NL）模型来建立性能预测模型（PP），以便预测特定深度神经网络架构在下游任务上的性能。</li>
<li>methods: 这个论文使用的方法包括设计PP提示语 для大型NL模型（LLM），其中包括角色描述、指令集、超参数定义和示例架构。在机器翻译（MT）任务上，我们发现使用LLM-PP提示语可以准确预测架构性能，并且与现有最佳性能预测器相当。此外，我们还提出了一种将PP预测结果进行混合压缩（LLM-Distill-PP），以实现成本效果的性能预测模型。</li>
<li>results: 这个论文的结果表明，使用LLM-PP提示语可以准确预测架构性能，并且可以在机器翻译（MT）任务上实现类似于现有最佳性能预测器的性能。此外，我们还提出了一种Hybrid-Search算法（HS-NAS），使用LLM-Distill-PP进行初始部分的搜索，然后使用基eline预测器进行剩余的搜索。我们的HS-NAS方法可以在不同的benchmark上实现相似于最佳NAS方法的性能，并且可以降低搜索时间约50%。<details>
<summary>Abstract</summary>
Large language models (LLMs) have become an integral component in solving a wide range of NLP tasks. In this work, we explore a novel use case of using LLMs to build performance predictors (PP): models that, given a specific deep neural network architecture, predict its performance on a downstream task. We design PP prompts for LLMs consisting of: (i) role: description of the role assigned to the LLM, (ii) instructions: set of instructions to be followed by the LLM to carry out performance prediction, (iii) hyperparameters: a definition of each architecture-specific hyperparameter and (iv) demonstrations: sample architectures along with their efficiency metrics and 'training from scratch' performance. For machine translation (MT) tasks, we discover that GPT-4 with our PP prompts (LLM-PP) can predict the performance of architecture with a mean absolute error matching the SOTA and a marginal degradation in rank correlation coefficient compared to SOTA performance predictors. Further, we show that the predictions from LLM-PP can be distilled to a small regression model (LLM-Distill-PP). LLM-Distill-PP models surprisingly retain the performance of LLM-PP largely and can be a cost-effective alternative for heavy use cases of performance estimation. Specifically, for neural architecture search (NAS), we propose a Hybrid-Search algorithm for NAS (HS-NAS), which uses LLM-Distill-PP for the initial part of search, resorting to the baseline predictor for rest of the search. We show that HS-NAS performs very similar to SOTA NAS across benchmarks, reduces search hours by 50% roughly, and in some cases, improves latency, GFLOPs, and model size.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Role: A description of the role assigned to the LLM.2. Instructions: A set of instructions to be followed by the LLM to carry out performance prediction.3. Hyperparameters: A definition of each architecture-specific hyperparameter.4. Demonstrations: Sample architectures along with their efficiency metrics and ‘training from scratch’ performance.For machine translation (MT) tasks, we discover that GPT-4 with our PP prompts (LLM-PP) can predict the performance of architecture with a mean absolute error matching the state-of-the-art (SOTA) and a marginal degradation in rank correlation coefficient compared to SOTA performance predictors. Furthermore, we show that the predictions from LLM-PP can be distilled to a small regression model (LLM-Distill-PP). LLM-Distill-PP models surprisingly retain the performance of LLM-PP largely and can be a cost-effective alternative for heavy use cases of performance estimation.In the context of neural architecture search (NAS), we propose a Hybrid-Search algorithm (HS-NAS) that uses LLM-Distill-PP for the initial part of the search and resorts to the baseline predictor for the rest of the search. We show that HS-NAS performs very similarly to SOTA NAS across benchmarks, reduces search hours by approximately 50%, and in some cases, improves latency, GFLOPs, and model size.</details></li>
</ol>
<hr>
<h2 id="BabyStories-Can-Reinforcement-Learning-Teach-Baby-Language-Models-to-Write-Better-Stories"><a href="#BabyStories-Can-Reinforcement-Learning-Teach-Baby-Language-Models-to-Write-Better-Stories" class="headerlink" title="BabyStories: Can Reinforcement Learning Teach Baby Language Models to Write Better Stories?"></a>BabyStories: Can Reinforcement Learning Teach Baby Language Models to Write Better Stories?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16681">http://arxiv.org/abs/2310.16681</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zephyr1022/babystories-utsa">https://github.com/zephyr1022/babystories-utsa</a></li>
<li>paper_authors: Xingmeng Zhao, Tongnian Wang, Sheri Osborn, Anthony Rios</li>
<li>for: 这个研究的目的是探索通过人工反馈学习（RLHF）是否可以提高由预训练的语言模型在小型、人类化数据集上的表现。</li>
<li>methods: 这个研究使用了两种GPT-2变种，通过RLHF练练后在故事作业中表现较好。</li>
<li>results: 研究发现， larger model在RLHF练练后在故事作业中表现较好，这表明RLHF技术可能对大型模型更有利，但需要更多的实验来证明这一结论。 These findings suggest that RLHF techniques may be more advantageous for larger models due to their higher learning and adaptation capacity.<details>
<summary>Abstract</summary>
Language models have seen significant growth in the size of their corpus, leading to notable performance improvements. Yet, there has been limited progress in developing models that handle smaller, more human-like datasets. As part of the BabyLM shared task, this study explores the impact of reinforcement learning from human feedback (RLHF) on language models pretrained from scratch with a limited training corpus. Comparing two GPT-2 variants, the larger model performs better in storytelling tasks after RLHF fine-tuning. These findings suggest that RLHF techniques may be more advantageous for larger models due to their higher learning and adaptation capacity, though more experiments are needed to confirm this finding. These insights highlight the potential benefits of RLHF fine-tuning for language models within limited data, enhancing their ability to maintain narrative focus and coherence while adhering better to initial instructions in storytelling tasks. The code for this work is publicly at https://github.com/Zephyr1022/BabyStories-UTSA.
</details>
<details>
<summary>摘要</summary>
Language models have experienced significant growth in their corpus size, leading to notable improvements in performance. However, there has been limited progress in developing models that can handle smaller, more human-like datasets. This study explores the impact of reinforcement learning from human feedback (RLHF) on language models pretrained from scratch with a limited training corpus. Comparing two GPT-2 variants, the larger model performs better in storytelling tasks after RLHF fine-tuning. These findings suggest that RLHF techniques may be more advantageous for larger models due to their higher learning and adaptation capacity, although more experiments are needed to confirm this finding. These insights highlight the potential benefits of RLHF fine-tuning for language models within limited data, enabling them to better maintain narrative focus and coherence while adhering to initial instructions in storytelling tasks. The code for this work is publicly available at <https://github.com/Zephyr1022/BabyStories-UTSA>.
</details></li>
</ul>
<hr>
<h2 id="SSLCL-An-Efficient-Model-Agnostic-Supervised-Contrastive-Learning-Framework-for-Emotion-Recognition-in-Conversations"><a href="#SSLCL-An-Efficient-Model-Agnostic-Supervised-Contrastive-Learning-Framework-for-Emotion-Recognition-in-Conversations" class="headerlink" title="SSLCL: An Efficient Model-Agnostic Supervised Contrastive Learning Framework for Emotion Recognition in Conversations"></a>SSLCL: An Efficient Model-Agnostic Supervised Contrastive Learning Framework for Emotion Recognition in Conversations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16676">http://arxiv.org/abs/2310.16676</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/taoshi1998/sslcl">https://github.com/taoshi1998/sslcl</a></li>
<li>paper_authors: Tao Shi, Xiao Liang, Yaoyuan Liang, Xinyi Tong, Shao-Lun Huang</li>
<li>for: 这个论文主要针对的是对话中的情感识别任务（ERC）， aiming to detect the emotions expressed by speakers during a conversation.</li>
<li>methods: 我们提出了一种高效的、模型无关的Supervised Sample-Label Contrastive Learning（SSLCL）框架， which eliminates the need for a large batch size and can be seamlessly integrated with existing ERC models without introducing any model-specific assumptions.</li>
<li>results: 我们的SSLCL框架在两个ERC数据集上（IEMOCAP和MELD）得到了与现有State-of-the-art SCL方法相比的compatibleibility和显著性能提升。<details>
<summary>Abstract</summary>
Emotion recognition in conversations (ERC) is a rapidly evolving task within the natural language processing community, which aims to detect the emotions expressed by speakers during a conversation. Recently, a growing number of ERC methods have focused on leveraging supervised contrastive learning (SCL) to enhance the robustness and generalizability of learned features. However, current SCL-based approaches in ERC are impeded by the constraint of large batch sizes and the lack of compatibility with most existing ERC models. To address these challenges, we propose an efficient and model-agnostic SCL framework named Supervised Sample-Label Contrastive Learning with Soft-HGR Maximal Correlation (SSLCL), which eliminates the need for a large batch size and can be seamlessly integrated with existing ERC models without introducing any model-specific assumptions. Specifically, we introduce a novel perspective on utilizing label representations by projecting discrete labels into dense embeddings through a shallow multilayer perceptron, and formulate the training objective to maximize the similarity between sample features and their corresponding ground-truth label embeddings, while minimizing the similarity between sample features and label embeddings of disparate classes. Moreover, we innovatively adopt the Soft-HGR maximal correlation as a measure of similarity between sample features and label embeddings, leading to significant performance improvements over conventional similarity measures. Additionally, multimodal cues of utterances are effectively leveraged by SSLCL as data augmentations to boost model performances. Extensive experiments on two ERC benchmark datasets, IEMOCAP and MELD, demonstrate the compatibility and superiority of our proposed SSLCL framework compared to existing state-of-the-art SCL methods. Our code is available at \url{https://github.com/TaoShi1998/SSLCL}.
</details>
<details>
<summary>摘要</summary>
“情感识别在对话（ERC）是自然语言处理领域的一个快速发展的任务，目的是在对话中检测发言人表达的情感。当前的ERC方法中，一个增长的数量的方法是利用监督对比学习（SCL）来提高学习的稳定性和通用性。然而，现有的SCL基于的ERC方法受到大批次大小的限制和现有ERC模型的不兼容性的问题。为解决这些挑战，我们提出了一种高效和模型无关的SCL框架，名为Supervised Sample-Label Contrastive Learning with Soft-HGR Maximal Correlation（SSLCL），它不需要大批次大小，可以轻松地与现有ERC模型集成，无需做任何模型特定的假设。具体来说，我们提出了一种新的标签表示方法，通过将精确的标签映射到权重化的多层感知机中，并将训练目标设置为最大化样本特征和其相应的真实标签嵌入的相似性，同时最小化样本特征和不同类别的标签嵌入之间的相似性。此外，我们创新地采用Soft-HGR最大相似度作为样本特征和标签嵌入之间的相似度度量，从而导致模型性能显著提高。同时，我们有效地利用对话语音的多 modal 信息作为数据增强来提高模型性能。我们的实验表明，我们的SSLCL框架在两个ERC标准测试集上（IEMOCAP和MELD）的Compatibility和Superiority，证明了我们的提议的可行性和优势。我们的代码可以在 \url{https://github.com/TaoShi1998/SSLCL} 中找到。”
</details></li>
</ul>
<hr>
<h2 id="ChatGPT-is-a-Potential-Zero-Shot-Dependency-Parser"><a href="#ChatGPT-is-a-Potential-Zero-Shot-Dependency-Parser" class="headerlink" title="ChatGPT is a Potential Zero-Shot Dependency Parser"></a>ChatGPT is a Potential Zero-Shot Dependency Parser</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16654">http://arxiv.org/abs/2310.16654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boda Lin, Xinyi Zhou, Binghao Tang, Xiaocheng Gong, Si Li</li>
<li>for: 研究是否可以使用预训练语言模型进行静态分析，无需额外结构。</li>
<li>methods: 使用ChatGPT大语言模型进行实验和语言分析。</li>
<li>results: ChatGPT表现出了零shot情况下的静态分析能力，并且分析结果也显示了一些特殊的 parsed 输出。<details>
<summary>Abstract</summary>
Pre-trained language models have been widely used in dependency parsing task and have achieved significant improvements in parser performance. However, it remains an understudied question whether pre-trained language models can spontaneously exhibit the ability of dependency parsing without introducing additional parser structure in the zero-shot scenario. In this paper, we propose to explore the dependency parsing ability of large language models such as ChatGPT and conduct linguistic analysis. The experimental results demonstrate that ChatGPT is a potential zero-shot dependency parser, and the linguistic analysis also shows some unique preferences in parsing outputs.
</details>
<details>
<summary>摘要</summary>
大量语言模型在依赖分析任务中广泛应用，并实现了显著提高 parser 性能。但是，还是一个未研究的问题是否可以在零shotenario中，不添加额外的 parser 结构，使大量语言模型自动表现出依赖分析能力。在这篇论文中，我们提出了探索大量语言模型如ChatGPT的依赖分析能力，并进行语言分析。实验结果表明，ChatGPT可能是一个零shot依赖分析器，语言分析还发现了一些独特的依赖分析输出偏好。
</details></li>
</ul>
<hr>
<h2 id="Context-Does-Matter-End-to-end-Panoptic-Narrative-Grounding-with-Deformable-Attention-Refined-Matching-Network"><a href="#Context-Does-Matter-End-to-end-Panoptic-Narrative-Grounding-with-Deformable-Attention-Refined-Matching-Network" class="headerlink" title="Context Does Matter: End-to-end Panoptic Narrative Grounding with Deformable Attention Refined Matching Network"></a>Context Does Matter: End-to-end Panoptic Narrative Grounding with Deformable Attention Refined Matching Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16616">http://arxiv.org/abs/2310.16616</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiming Lin, Xiao-Bo Jin, Qiufeng Wang, Kaizhu Huang</li>
<li>for: 提高图像中文描述匹配精度，解决文本描述与图像像素之间的匹配异常问题。</li>
<li>methods: 提出了一种新的学习框架，即变换注意力重新评估网络（DRMN），通过在循环学习过程中引入可变注意力网络，以 capture 不同级别的像素上的关键信息，从而提高文本描述与图像像素之间的匹配精度。</li>
<li>results: 实验结果表明，DRMN在PNG数据集上达到了新的状态艺术性能，增加了3.5%的召回率。<details>
<summary>Abstract</summary>
Panoramic Narrative Grounding (PNG) is an emerging visual grounding task that aims to segment visual objects in images based on dense narrative captions. The current state-of-the-art methods first refine the representation of phrase by aggregating the most similar $k$ image pixels, and then match the refined text representations with the pixels of the image feature map to generate segmentation results. However, simply aggregating sampled image features ignores the contextual information, which can lead to phrase-to-pixel mis-match. In this paper, we propose a novel learning framework called Deformable Attention Refined Matching Network (DRMN), whose main idea is to bring deformable attention in the iterative process of feature learning to incorporate essential context information of different scales of pixels. DRMN iteratively re-encodes pixels with the deformable attention network after updating the feature representation of the top-$k$ most similar pixels. As such, DRMN can lead to accurate yet discriminative pixel representations, purify the top-$k$ most similar pixels, and consequently alleviate the phrase-to-pixel mis-match substantially.Experimental results show that our novel design significantly improves the matching results between text phrases and image pixels. Concretely, DRMN achieves new state-of-the-art performance on the PNG benchmark with an average recall improvement 3.5%. The codes are available in: https://github.com/JaMesLiMers/DRMN.
</details>
<details>
<summary>摘要</summary>
паннорамный нарративный гроуинг (ПНГ) - это возникающая задача визуального гроуинга, которая целится в сегментации визуальных объектов в изображениях на основе плотных нарративных записей. Текущие методы штата-арта используют технику сжатия представления фразы, а затем сравнения с представлениями пикселей изображения для получения результатов сегментации. Однако, простое сжатие выборочных пикселей изображения игнорирует контекстную информацию, что может привести к несоответствию фразы-пиксель. В этой статье мы предлагаем новый фреймворк обучения называемый Сверхёжидной вниманием очищенным сетью (ДРМН), который использует внимание в процессе итеративного обучения для интеграции информации о различных масштабах пикселей. ДРМН итеративно перекодирует пиксели с сетью внимания после обновления представления пикселей. Таким образом, ДРМН может привести к точным и дискриминативным представлениям пикселей, очистить топ-$k$ самых похожих пикселей и уменьшить несоответствие фразы-пиксель существенно. Экспериментальные результаты показывают, что наша новая конструкция значительно улучшает результаты сравнения текстовых фраз и пикселей изображения. Конкретно, ДРМН достигает новых рекордов штата-арта на задаче ПНГ с увеличением в среднем на 3,5% recall. Коды доступны в: https://github.com/JaMesLiMers/DRMN.
</details></li>
</ul>
<hr>
<h2 id="On-the-Interplay-between-Fairness-and-Explainability"><a href="#On-the-Interplay-between-Fairness-and-Explainability" class="headerlink" title="On the Interplay between Fairness and Explainability"></a>On the Interplay between Fairness and Explainability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16607">http://arxiv.org/abs/2310.16607</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephanie Brandl, Emanuele Bugliarello, Ilias Chalkidis</li>
<li>for: 这个论文的目的是建立可靠和信任worthy的NLP应用程序，这些应用程序需要具有不偏袋和可解释性。</li>
<li>methods: 这篇论文使用了多种方法来优化偏袋和可解释性，包括偏袋缓解和可解释性检测。</li>
<li>results: 研究发现，偏袋缓解算法不总是能够提高公平性，同时，empirical fairness和可解释性是 orthogonal的。<details>
<summary>Abstract</summary>
In order to build reliable and trustworthy NLP applications, models need to be both fair across different demographics and explainable. Usually these two objectives, fairness and explainability, are optimized and/or examined independently of each other. Instead, we argue that forthcoming, trustworthy NLP systems should consider both. In this work, we perform a first study to understand how they influence each other: do fair(er) models rely on more plausible rationales? and vice versa. To this end, we conduct experiments on two English multi-class text classification datasets, BIOS and ECtHR, that provide information on gender and nationality, respectively, as well as human-annotated rationales. We fine-tune pre-trained language models with several methods for (i) bias mitigation, which aims to improve fairness; (ii) rationale extraction, which aims to produce plausible explanations. We find that bias mitigation algorithms do not always lead to fairer models. Moreover, we discover that empirical fairness and explainability are orthogonal.
</details>
<details>
<summary>摘要</summary>
为建立可靠和信worthy的自然语言处理（NLP）应用程序，模型需要具备不同人群的公平性和可解释性。通常这两个目标被优化和/或独立地评估。我们 argue that forthcoming NLP系统应该同时考虑这两个目标。在这项工作中，我们进行了首次研究，了解这两个目标之间的关系：是否可以更加公平的模型具备更加可信的理由？以及vice versa。为此，我们在两个英语多类文本分类 datasets（BIOS和ECtHR）上进行了实验，这两个dataset提供了 gender和 nationality信息，以及人类标注的理由。我们对预训练语言模型进行了多种方法的调整，包括：* 偏好缓和，用于提高公平性* 理由提取，用于生成可信的解释我们发现，偏好缓和算法不总是能够提高公平性。此外，我们发现了empirical公平性和可解释性是独立的。
</details></li>
</ul>
<hr>
<h2 id="Tailoring-Personality-Traits-in-Large-Language-Models-via-Unsupervisedly-Built-Personalized-Lexicons"><a href="#Tailoring-Personality-Traits-in-Large-Language-Models-via-Unsupervisedly-Built-Personalized-Lexicons" class="headerlink" title="Tailoring Personality Traits in Large Language Models via Unsupervisedly-Built Personalized Lexicons"></a>Tailoring Personality Traits in Large Language Models via Unsupervisedly-Built Personalized Lexicons</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16582">http://arxiv.org/abs/2310.16582</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianlong Li, Xiaoqing Zheng, Xuanjing Huang</li>
<li>for: 这研究旨在tailoring大 Five trait within large language models (LLMs), allowing for the incorporation of any combination of the Big Five factors (i.e., openness, conscientiousness, extraversion, agreeableness, and neuroticism) in a pluggable manner.</li>
<li>methods: 该方法使用Unsupervisedly-Built Personalized Lexicons (UBPL) 来调整原始 LLMs 的下一个 token 预测概率，以促使模型生成具有个性 trait 的文本。</li>
<li>results: 实验结果表明该方法可以精细地控制 LLMs 的个性 trait，并且可以轻松地与其他 LLMs 集成。<details>
<summary>Abstract</summary>
Personality plays a pivotal role in shaping human expression patterns, and empowering and manipulating large language models (LLMs) with personality traits holds significant promise in enhancing the user experience of LLMs. However, prior approaches either rely on fine-tuning LLMs on a corpus enriched with personalized expressions or necessitate the manual crafting of prompts to induce LLMs to produce personalized responses. The former approaches demand substantial time and resources for collecting sufficient training examples while the latter might fail in enabling the precise manipulation of the personality traits at a fine-grained level (e.g., achieving high agreeableness while reducing openness). In this study, we introduce a novel approach for tailoring personality traits within LLMs, allowing for the incorporation of any combination of the Big Five factors (i.e., openness, conscientiousness, extraversion, agreeableness, and neuroticism) in a pluggable manner. This is achieved by employing a set of Unsupervisedly-Built Personalized Lexicons (UBPL) that are utilized to adjust the probability of the next token predicted by the original LLMs during the decoding phase. This adjustment encourages the models to generate words present in the personalized lexicons while preserving the naturalness of the generated texts. Extensive experimentation demonstrates the effectiveness of our approach in finely manipulating LLMs' personality traits. Furthermore, our method can be seamlessly integrated into other LLMs without necessitating updates to their parameters.
</details>
<details>
<summary>摘要</summary>
人格 trait plays a crucial role in shaping human expression patterns, and empowering and manipulating large language models (LLMs) with personality traits holds great promise in enhancing the user experience of LLMs. However, previous approaches either rely on fine-tuning LLMs on a corpus enriched with personalized expressions or require manual crafting of prompts to induce LLMs to produce personalized responses. The former approaches demand substantial time and resources for collecting sufficient training examples, while the latter might fail to achieve precise manipulation of personality traits at a fine-grained level (e.g., achieving high agreeableness while reducing openness).In this study, we propose a novel approach for tailoring personality traits within LLMs, allowing for the incorporation of any combination of the Big Five factors (i.e., openness, conscientiousness, extraversion, agreeableness, and neuroticism) in a pluggable manner. This is achieved by employing a set of Unsupervisedly-Built Personalized Lexicons (UBPL) to adjust the probability of the next token predicted by the original LLMs during the decoding phase. This adjustment encourages the models to generate words present in the personalized lexicons while preserving the naturalness of the generated texts. Extensive experimentation demonstrates the effectiveness of our approach in finely manipulating LLMs' personality traits. Furthermore, our method can be seamlessly integrated into other LLMs without requiring updates to their parameters.
</details></li>
</ul>
<hr>
<h2 id="WSDMS-Debunk-Fake-News-via-Weakly-Supervised-Detection-of-Misinforming-Sentences-with-Contextualized-Social-Wisdom"><a href="#WSDMS-Debunk-Fake-News-via-Weakly-Supervised-Detection-of-Misinforming-Sentences-with-Contextualized-Social-Wisdom" class="headerlink" title="WSDMS: Debunk Fake News via Weakly Supervised Detection of Misinforming Sentences with Contextualized Social Wisdom"></a>WSDMS: Debunk Fake News via Weakly Supervised Detection of Misinforming Sentences with Contextualized Social Wisdom</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16579">http://arxiv.org/abs/2310.16579</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HKBUNLP/WSDMS-EMNLP2023">https://github.com/HKBUNLP/WSDMS-EMNLP2023</a></li>
<li>paper_authors: Ruichao Yang, Wei Gao, Jing Ma, Hongzhan Lin, Zhiwei Yang<br>for: 这项研究的目的是解决社交媒体上快速传播的假消息和不确定信息问题。methods: 本研究提出了一种新的假新闻推篱方法，即利用多个实例学习（Multiple Instance Learning，MIL）方法，只需要训练集的袋子级标签，但可以推断出具有错误信息的句子和具有真实性的文章。results: 研究表明，该方法可以在三个真实世界标准 benchmark 上击败现有的状态作准基eline，在句子和文章水平上 debunk 假新闻。<details>
<summary>Abstract</summary>
In recent years, we witness the explosion of false and unconfirmed information (i.e., rumors) that went viral on social media and shocked the public. Rumors can trigger versatile, mostly controversial stance expressions among social media users. Rumor verification and stance detection are different yet relevant tasks. Fake news debunking primarily focuses on determining the truthfulness of news articles, which oversimplifies the issue as fake news often combines elements of both truth and falsehood. Thus, it becomes crucial to identify specific instances of misinformation within the articles. In this research, we investigate a novel task in the field of fake news debunking, which involves detecting sentence-level misinformation. One of the major challenges in this task is the absence of a training dataset with sentence-level annotations regarding veracity. Inspired by the Multiple Instance Learning (MIL) approach, we propose a model called Weakly Supervised Detection of Misinforming Sentences (WSDMS). This model only requires bag-level labels for training but is capable of inferring both sentence-level misinformation and article-level veracity, aided by relevant social media conversations that are attentively contextualized with news sentences. We evaluate WSDMS on three real-world benchmarks and demonstrate that it outperforms existing state-of-the-art baselines in debunking fake news at both the sentence and article levels.
</details>
<details>
<summary>摘要</summary>
近年来，我们目睹了社交媒体上的谣言泛洪，让公众受到了各种不同的影响。谣言可以让社交媒体用户表达多种不同的看法，大多是争议的。验证谣言和判断看法是不同 yet 相关的任务。驳斥 fake news 主要集中在决定新闻文章的真实性，这有些 simplifies 了问题，因为 fake news 经常混合真实和假的元素。因此，成为必须鉴别特定的谣言内容。在这项研究中，我们调查了一项新的 fake news 驳斥任务，即Detecting Misinforming Sentences（DMS）。这项任务的主要挑战在于缺乏 sentence-level 的真实性标注数据。以 Multiple Instance Learning（MIL） Approach 为 inspiration，我们提出了 Weakly Supervised Detection of Misinforming Sentences（WSDMS）模型。这个模型只需要训练 bag-level 标签，但可以推断出 sentence-level 谣言和文章-level 真实性，得益于与新闻句子相关的社交媒体对话。我们对 WSDMS 进行了三个实际 benchmark 的评估，并证明它在 fake news 驳斥中超过了现有的基eline。
</details></li>
</ul>
<hr>
<h2 id="Give-Me-the-Facts-A-Survey-on-Factual-Knowledge-Probing-in-Pre-trained-Language-Models"><a href="#Give-Me-the-Facts-A-Survey-on-Factual-Knowledge-Probing-in-Pre-trained-Language-Models" class="headerlink" title="Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models"></a>Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16570">http://arxiv.org/abs/2310.16570</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Youssef, Osman Alperen Koraş, Meijie Li, Jörg Schlötterer, Christin Seifert</li>
<li>for: 这个研究旨在调查 PLMs 中的事实知识量，以解释其在下游任务中的表现，并可能正当使用它们作为知识库。</li>
<li>methods: 这篇论文报道了对 PLMs 的事实检测方法，包括输入、输出和检测 PLMs 的方法，并提供了这些方法的概述。</li>
<li>results: 该研究发现了 PLMs 中的事实知识量，并分析了在采用 PLMs 作为知识库时的障碍和未来研究的方向。<details>
<summary>Abstract</summary>
Pre-trained Language Models (PLMs) are trained on vast unlabeled data, rich in world knowledge. This fact has sparked the interest of the community in quantifying the amount of factual knowledge present in PLMs, as this explains their performance on downstream tasks, and potentially justifies their use as knowledge bases. In this work, we survey methods and datasets that are used to probe PLMs for factual knowledge. Our contributions are: (1) We propose a categorization scheme for factual probing methods that is based on how their inputs, outputs and the probed PLMs are adapted; (2) We provide an overview of the datasets used for factual probing; (3) We synthesize insights about knowledge retention and prompt optimization in PLMs, analyze obstacles to adopting PLMs as knowledge bases and outline directions for future work.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>基于输入、输出和探测PLMs的改进方法的分类方案。2. 对用于事实探测的数据集进行概述。3. 对PLMs中知识保留和提问优化的分析，以及采用PLMs作为知识库的障碍和未来工作的规划。</details></li>
</ol>
<hr>
<h2 id="1-PAGER-One-Pass-Answer-Generation-and-Evidence-Retrieval"><a href="#1-PAGER-One-Pass-Answer-Generation-and-Evidence-Retrieval" class="headerlink" title="1-PAGER: One Pass Answer Generation and Evidence Retrieval"></a>1-PAGER: One Pass Answer Generation and Evidence Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16568">http://arxiv.org/abs/2310.16568</a></li>
<li>repo_url: None</li>
<li>paper_authors: Palak Jain, Livio Baldini Soares, Tom Kwiatkowski</li>
<li>for: 本研究旨在提出一种基于Transformer模型的单一解决方案，可以同时回答问题和检索证据。</li>
<li>methods: 该模型使用受限的解码进行搜索和回答，并通过比较与其他 retrieve-and-read 方法的性能指标来证明其竞争力。</li>
<li>results: 实验结果表明，1-Pager 可以与其他相似的 retrieve-and-read 方法相比，在回答准确率和检索率两个指标上具有竞争力。此外，1-Pager 还可以在不读取多个文档后产生答案的情况下，提供更高的回答准确率。<details>
<summary>Abstract</summary>
We present 1-Pager the first system that answers a question and retrieves evidence using a single Transformer-based model and decoding process. 1-Pager incrementally partitions the retrieval corpus using constrained decoding to select a document and answer string, and we show that this is competitive with comparable retrieve-and-read alternatives according to both retrieval and answer accuracy metrics. 1-Pager also outperforms the equivalent closed-book question answering model, by grounding predictions in an evidence corpus. While 1-Pager is not yet on-par with more expensive systems that read many more documents before generating an answer, we argue that it provides an important step toward attributed generation by folding retrieval into the sequence-to-sequence paradigm that is currently dominant in NLP. We also show that the search paths used to partition the corpus are easy to read and understand, paving a way forward for interpretable neural retrieval.
</details>
<details>
<summary>摘要</summary>
我们介绍1-Pager，首个使用单一转换器模型和解码过程来回答问题并提取证据的系统。1-Pager逐步分割检索库使用受限解码方式选择文档和答案字符串，我们表明这与相似的检索和读取选择相当。1-Pager还超过相同的关闭书问答模型，通过固定预测在证据库中附加 Generation。虽然1-Pager还不及更加昂贵的系统，但我们认为它为归因生成带来了重要的一步，将检索嵌入序列到序列中的当前主流NLP框架中。我们还显示检索路径使用受限解码方式分割库是易于阅读和理解的，这为神经网络检索带来了可读性的前进。
</details></li>
</ul>
<hr>
<h2 id="An-Early-Evaluation-of-GPT-4V-ision"><a href="#An-Early-Evaluation-of-GPT-4V-ision" class="headerlink" title="An Early Evaluation of GPT-4V(ision)"></a>An Early Evaluation of GPT-4V(ision)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16534">http://arxiv.org/abs/2310.16534</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/albertwy/gpt-4v-evaluation">https://github.com/albertwy/gpt-4v-evaluation</a></li>
<li>paper_authors: Yang Wu, Shilong Wang, Hao Yang, Tian Zheng, Hongbo Zhang, Yanyan Zhao, Bing Qin</li>
<li>for: 本研究用于评估GPT-4V在视觉理解、语言理解、视觉拼图解决和其他modalities的能力。</li>
<li>methods: 我们手动构建656个测试实例，并仔细评估GPT-4V的表现。</li>
<li>results: 我们发现GPT-4V在英语视觉中benchmark表现出色，但无法识别简单的中文文本在图像中; GPT-4V在敏感特征相关问题上表现不一致; GPT-4V在语言理解任务上表现 inferior于GPT-4（API）; 几何提示可以提高GPT-4V的视觉理解和语言理解能力; GPT-4V在类似模式的任务上表现差。<details>
<summary>Abstract</summary>
In this paper, we evaluate different abilities of GPT-4V including visual understanding, language understanding, visual puzzle solving, and understanding of other modalities such as depth, thermal, video, and audio. To estimate GPT-4V's performance, we manually construct 656 test instances and carefully evaluate the results of GPT-4V. The highlights of our findings are as follows: (1) GPT-4V exhibits impressive performance on English visual-centric benchmarks but fails to recognize simple Chinese texts in the images; (2) GPT-4V shows inconsistent refusal behavior when answering questions related to sensitive traits such as gender, race, and age; (3) GPT-4V obtains worse results than GPT-4 (API) on language understanding tasks including general language understanding benchmarks and visual commonsense knowledge evaluation benchmarks; (4) Few-shot prompting can improve GPT-4V's performance on both visual understanding and language understanding; (5) GPT-4V struggles to find the nuances between two similar images and solve the easy math picture puzzles; (6) GPT-4V shows non-trivial performance on the tasks of similar modalities to image, such as video and thermal. Our experimental results reveal the ability and limitations of GPT-4V and we hope our paper can provide some insights into the application and research of GPT-4V.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们评估了GPT-4V的不同能力，包括视觉理解、语言理解、视觉逻辑解决、以及其他modalities such as depth、thermal、视频和audio的理解。为了估计GPT-4V的性能，我们手动构建了656个测试实例，并且精心评估了GPT-4V的结果。我们的发现包括：1. GPT-4V在英文视觉中benchmarks上表现出色，但是无法识别简单的中文文本在图像中;2. GPT-4V在归类敏感特征问题上表现不一致，包括性别、种族和年龄等;3. GPT-4V在语言理解任务上比GPT-4（API）表现更差，包括通用语言理解benchmarks和视觉常识知识评估benchmarks;4. 几个提示可以提高GPT-4V的视觉理解和语言理解性能;5. GPT-4V很难在两个类似图像之间找到细节和解决易于数学图像逻辑问题;6. GPT-4V在视频和热成像任务上表现不错，与图像任务相似。我们的实验结果表明GPT-4V的能力和局限性，我们希望这篇论文可以为GPT-4V的应用和研究提供一些启示。
</details></li>
</ul>
<hr>
<h2 id="CUNI-Submission-to-MRL-2023-Shared-Task-on-Multi-lingual-Multi-task-Information-Retrieval"><a href="#CUNI-Submission-to-MRL-2023-Shared-Task-on-Multi-lingual-Multi-task-Information-Retrieval" class="headerlink" title="CUNI Submission to MRL 2023 Shared Task on Multi-lingual Multi-task Information Retrieval"></a>CUNI Submission to MRL 2023 Shared Task on Multi-lingual Multi-task Information Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16528">http://arxiv.org/abs/2310.16528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jindřich Helcl, Jindřich Libovický</li>
<li>for: 这个论文是为了参加2023年多语言多任务信息检索（MRL）共同任务的系统设计的。</li>
<li>methods: 论文使用了翻译测试方法，首先将无标例例 перевод成英语，然后使用强task特定模型进行推理。最后，我们使用标签敏感翻译模型对原语言中的标签进行评分，以保持在原语言中的标签。</li>
<li>results: 论文在两个下标任务中使用了翻译测试方法，但由于development数据和共同任务验证和测试集之间的领域不同，经过训练的分类模型无法超越基线。<details>
<summary>Abstract</summary>
We present the Charles University system for the MRL~2023 Shared Task on Multi-lingual Multi-task Information Retrieval. The goal of the shared task was to develop systems for named entity recognition and question answering in several under-represented languages. Our solutions to both subtasks rely on the translate-test approach. We first translate the unlabeled examples into English using a multilingual machine translation model. Then, we run inference on the translated data using a strong task-specific model. Finally, we project the labeled data back into the original language. To keep the inferred tags on the correct positions in the original language, we propose a method based on scoring the candidate positions using a label-sensitive translation model. In both settings, we experiment with finetuning the classification models on the translated data. However, due to a domain mismatch between the development data and the shared task validation and test sets, the finetuned models could not outperform our baselines.
</details>
<details>
<summary>摘要</summary>
我们介绍了查尔斯大学系统 для MRL~2023共享任务的多语言多任务信息检索。该任务的目标是开发用于多种语言的命名实体识别和问答系统。我们的解决方案对于两个子任务都采用了翻译测试方法。我们首先将无标示示例翻译成英语使用多语言机器翻译模型。然后，我们运行在翻译后的数据上进行推理，使用强大的任务特定模型。最后，我们将原始语言中的标注数据投影回原始语言。为保持在原始语言中的推理结果的标注，我们提议一种基于标签敏感翻译模型的评分方法。在两个设置中，我们尝试了在翻译后的数据上进行训练分类模型，但由于共享任务验证和测试集与开发数据的领域差异，训练后的模型无法超越我们的基线。
</details></li>
</ul>
<hr>
<h2 id="OccuQuest-Mitigating-Occupational-Bias-for-Inclusive-Large-Language-Models"><a href="#OccuQuest-Mitigating-Occupational-Bias-for-Inclusive-Large-Language-Models" class="headerlink" title="OccuQuest: Mitigating Occupational Bias for Inclusive Large Language Models"></a>OccuQuest: Mitigating Occupational Bias for Inclusive Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16517">http://arxiv.org/abs/2310.16517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingfeng Xue, Dayiheng Liu, Kexin Yang, Guanting Dong, Wenqiang Lei, Zheng Yuan, Chang Zhou, Jingren Zhou<br>for:* This paper aims to address the issue of occupational bias in instruction-tuning datasets for large language models (LLMs), which hinders the models’ ability to generate helpful responses to professional queries from practitioners in specific fields.methods:* The authors create an instruction-tuning dataset named “OccuQuest” that contains 110,000+ prompt-completion pairs and 30,000+ dialogues covering over 1,000 occupations in 26 occupational categories.* They systematically request ChatGPT to generate responses to queries hierarchically based on Occupation, Responsibility, Topic, and Question to ensure comprehensive coverage of occupational specialty inquiries.results:* The authors compare OccuQuest with three commonly used datasets (Dolly, ShareGPT, and WizardLM) and find that OccuQuest exhibits a more balanced distribution across occupations.* They fine-tune LLaMA on OccuQuest to obtain OccuLLaMA, which significantly outperforms state-of-the-art LLaMA variants (Vicuna, Tulu, and WizardLM) on professional questions in GPT-4 and human evaluations, with a high win rate of 86.4% against WizardLM on the occu-quora set.<details>
<summary>Abstract</summary>
The emergence of large language models (LLMs) has revolutionized natural language processing tasks. However, existing instruction-tuning datasets suffer from occupational bias: the majority of data relates to only a few occupations, which hampers the instruction-tuned LLMs to generate helpful responses to professional queries from practitioners in specific fields. To mitigate this issue and promote occupation-inclusive LLMs, we create an instruction-tuning dataset named \emph{OccuQuest}, which contains 110,000+ prompt-completion pairs and 30,000+ dialogues covering over 1,000 occupations in 26 occupational categories. We systematically request ChatGPT, organizing queries hierarchically based on Occupation, Responsibility, Topic, and Question, to ensure a comprehensive coverage of occupational specialty inquiries. By comparing with three commonly used datasets (Dolly, ShareGPT, and WizardLM), we observe that OccuQuest exhibits a more balanced distribution across occupations. Furthermore, we assemble three test sets for comprehensive evaluation, an occu-test set covering 25 occupational categories, an estate set focusing on real estate, and an occu-quora set containing real-world questions from Quora. We then fine-tune LLaMA on OccuQuest to obtain OccuLLaMA, which significantly outperforms state-of-the-art LLaMA variants (Vicuna, Tulu, and WizardLM) on professional questions in GPT-4 and human evaluations. Notably, on the occu-quora set, OccuLLaMA reaches a high win rate of 86.4\% against WizardLM.
</details>
<details>
<summary>摘要</summary>
大量语言模型（LLM）的出现已经革命化了自然语言处理任务。然而，现有的 instrucion-tuning 数据集受到职业偏见：大多数数据关注只有一些职业，这使得 instrucion-tuned LLM 无法生成专业领域问题上有用的回答。为解决这问题并推广职业包容 LLM，我们创建了一个 instrucion-tuning 数据集名为“OccuQuest”，包含 более чем 110,000+ 提示完成对和 30,000+ 对话，覆盖26个职业类别中的1,000多个职业。我们系统地请求 ChatGPT，将提示组织按照职业、责任、话题和问题进行归类，以确保职业专业问题的全面覆盖。与 Dolly、ShareGPT 和 WizardLM 等三个常用数据集进行比较，我们发现 OccuQuest 的分布更加平衡。此外，我们组成了三个测试集，包括 occu-test set（覆盖25个职业类别）、 estate set（专注于房地产）和 occu-quora set（包含来自 Quora 的真实问题）。然后，我们使用 OccuQuest 进行精度调整，得到了 OccuLLaMA，它在专业问题上以高胜率（86.4%）击败了 WizardLM。
</details></li>
</ul>
<hr>
<h2 id="Subspace-Chronicles-How-Linguistic-Information-Emerges-Shifts-and-Interacts-during-Language-Model-Training"><a href="#Subspace-Chronicles-How-Linguistic-Information-Emerges-Shifts-and-Interacts-during-Language-Model-Training" class="headerlink" title="Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts during Language Model Training"></a>Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts during Language Model Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16484">http://arxiv.org/abs/2310.16484</a></li>
<li>repo_url: None</li>
<li>paper_authors: Max Müller-Eberstein, Rob van der Goot, Barbara Plank, Ivan Titov</li>
<li>for: 本研究旨在探讨语言模型在自然语言处理（NLP）中的基本知识空间是如何形成和如何在训练过程中交互。</li>
<li>methods: 研究人员使用了一种新的信息理论探索工具箱，可以直接比较不同任务的表现和表现空间，对九个任务（涉及语法、 semantics 和理解）进行了200万步骤的预训练和五个种子的分析。</li>
<li>results: 研究发现，在不同任务和时间点上，语言知识在不同阶段出现、交换信息和特циали化，从而影响模型的表现。  syntax 知识在训练的早期快速获得，而后续的表现提升主要来自于开放领域知识的获得，而 semantics 和理解任务则在后期受惠于更高的特циали化和长距离contextualization。 检测交育 Task 之间的相似性也表明，语言相关任务在训练过程中进行了各种信息交换，并在关键学习阶段更加活跃。<details>
<summary>Abstract</summary>
Representational spaces learned via language modeling are fundamental to Natural Language Processing (NLP), however there has been limited understanding regarding how and when during training various types of linguistic information emerge and interact. Leveraging a novel information theoretic probing suite, which enables direct comparisons of not just task performance, but their representational subspaces, we analyze nine tasks covering syntax, semantics and reasoning, across 2M pre-training steps and five seeds. We identify critical learning phases across tasks and time, during which subspaces emerge, share information, and later disentangle to specialize. Across these phases, syntactic knowledge is acquired rapidly after 0.5% of full training. Continued performance improvements primarily stem from the acquisition of open-domain knowledge, while semantics and reasoning tasks benefit from later boosts to long-range contextualization and higher specialization. Measuring cross-task similarity further reveals that linguistically related tasks share information throughout training, and do so more during the critical phase of learning than before or after. Our findings have implications for model interpretability, multi-task learning, and learning from limited data.
</details>
<details>
<summary>摘要</summary>
NATURAL LANGUAGE PROCESSING (NLP) 的基础知识是通过语言模型学习得到的表征空间，但是过去很少有人研究了在哪些时候和如何在训练中不同类型的语言信息emerge和交互。我们使用了一个新的信息论探测 suite，可以对不同任务的表征空间进行直接比较，我们分析了九个任务，覆盖了 syntax、 semantics 和理解，在200万个预训练步和五个种子上进行了分析。我们发现了训练过程中的关键学习阶段，在这些阶段表征空间出现、信息交换和后来分离以特化。在这些阶段，语法知识得到了快速的学习，而开放领域知识的获得则是训练的主要来源，而 semantics 和理解任务则在后来得到了更多的长距离 contextualization 和更高的特化。我们的发现对模型解释、多任务学习和学习从有限数据进行了启示。
</details></li>
</ul>
<hr>
<h2 id="CLEX-Continuous-Length-Extrapolation-for-Large-Language-Models"><a href="#CLEX-Continuous-Length-Extrapolation-for-Large-Language-Models" class="headerlink" title="CLEX: Continuous Length Extrapolation for Large Language Models"></a>CLEX: Continuous Length Extrapolation for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16450">http://arxiv.org/abs/2310.16450</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/damo-nlp-sg/clex">https://github.com/damo-nlp-sg/clex</a></li>
<li>paper_authors: Guanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong Liang, Lidong Bing</li>
<li>for: 提高LLMs的上下文窗口长度，以便在长上下文应用中表现出色。</li>
<li>methods: 基于Continuous Length EXtrapolation（CLEX）的方法，通过对length scaling factor进行普通微分方程的模型化，超越现有PE scaling方法的限制。</li>
<li>results: 在实验中，CLEX可以准确地扩展LLMs的上下文窗口长度至超过4倍或接近8倍的训练长度，无损性性能。此外，在实际的LongBench标准测试中，我们的模型在4k长度上表现竞争力强，与状态流开源模型在32k长度上训练的表现相当。<details>
<summary>Abstract</summary>
Transformer-based Large Language Models (LLMs) are pioneering advances in many natural language processing tasks, however, their exceptional capabilities are restricted within the preset context window of Transformer. Position Embedding (PE) scaling methods, while effective in extending the context window to a specific length, demonstrate either notable limitations in their extrapolation abilities or sacrificing partial performance within the context window. Length extrapolation methods, although theoretically capable of extending the context window beyond the training sequence length, often underperform in practical long-context applications. To address these challenges, we propose Continuous Length EXtrapolation (CLEX) for LLMs. We generalise the PE scaling approaches to model the continuous dynamics by ordinary differential equations over the length scaling factor, thereby overcoming the constraints of current PE scaling methods designed for specific lengths. Moreover, by extending the dynamics to desired context lengths beyond the training sequence length, CLEX facilitates the length extrapolation with impressive performance in practical tasks. We demonstrate that CLEX can be seamlessly incorporated into LLMs equipped with Rotary Position Embedding, such as LLaMA and GPT-NeoX, with negligible impact on training and inference latency. Experimental results reveal that CLEX can effectively extend the context window to over 4x or almost 8x training length, with no deterioration in performance. Furthermore, when evaluated on the practical LongBench benchmark, our model trained on a 4k length exhibits competitive performance against state-of-the-art open-source models trained on context lengths up to 32k.
</details>
<details>
<summary>摘要</summary>
transformer-based 大型自然语言处理模型（LLM）在许多任务中取得了先锋的进步，然而其杰出的能力受到 transformer 中设置的上下文窗口的限制。位嵌入（PE）缩放方法可以延长上下文窗口的长度，但是它们在不同长度上的极限性能或者在上下文窗口内的一部分性能牺牲。长度极限方法可以将上下文窗口拓展到训练序列长度之 beyond，但是在实际长上下文应用中 frequently underperform。为解决这些挑战，我们提出了 Continuous Length EXtrapolation（CLEX） для LLM。我们将 PE 缩放方法扩展到模型化连续动力学，使得可以不受现有 PE 缩放方法设置的限制。此外，通过将动力学拓展到所需的上下文长度，CLEX 可以具有卓越的长度极限性能。我们在 LLM 中 embedding 旋转 Position Embedding 的模型中实现了 CLEX，并证明了它可以轻松地与 training 和推理时间相比，无损到性能。实验结果表明，CLEX 可以有效地将上下文窗口拓展到训练序列长度的4倍或更长，无损到性能。此外，当我们对 practical LongBench benchmark 进行评估时，我们的模型在4k 长度上表现与开源模型在上下文长度达32k 的状态之前的状态竞争。
</details></li>
</ul>
<hr>
<h2 id="DDCoT-Duty-Distinct-Chain-of-Thought-Prompting-for-Multimodal-Reasoning-in-Language-Models"><a href="#DDCoT-Duty-Distinct-Chain-of-Thought-Prompting-for-Multimodal-Reasoning-in-Language-Models" class="headerlink" title="DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models"></a>DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16436">http://arxiv.org/abs/2310.16436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ge Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, Sibei Yang</li>
<li>for: 本研究旨在提高人工智能系统在多Modal reasoning中的能力，使其能够像人类一样进行复杂的多Modal reasoning。</li>
<li>methods: 本研究使用了大语言模型（LLMs），通过模仿人类思维链（CoT）来实现多Modal reasoning。研究人员还提出了两个关键发现：“保持批判性思维”和“让每个人做自己的工作”。</li>
<li>results: 研究人员提出了一种新的DDCoT提问方法，可以维护语言模型的批判性思维能力，同时将视觉认知能力 integrate into reasoning过程。DDCoT提问方法在零shot提问和练习学习中，对大语言模型和小语言模型的理解能力进行了显著改进，并且具有很好的普适性和可解释性。<details>
<summary>Abstract</summary>
A long-standing goal of AI systems is to perform complex multimodal reasoning like humans. Recently, large language models (LLMs) have made remarkable strides in such multi-step reasoning on the language modality solely by leveraging the chain of thought (CoT) to mimic human thinking. However, the transfer of these advancements to multimodal contexts introduces heightened challenges, including but not limited to the impractical need for labor-intensive annotation and the limitations in terms of flexibility, generalizability, and explainability. To evoke CoT reasoning in multimodality, this work first conducts an in-depth analysis of these challenges posed by multimodality and presents two key insights: "keeping critical thinking" and "letting everyone do their jobs" in multimodal CoT reasoning. Furthermore, this study proposes a novel DDCoT prompting that maintains a critical attitude through negative-space prompting and incorporates multimodality into reasoning by first dividing the reasoning responsibility of LLMs into reasoning and recognition and then integrating the visual recognition capability of visual models into the joint reasoning process. The rationales generated by DDCoT not only improve the reasoning abilities of both large and small language models in zero-shot prompting and fine-tuning learning, significantly outperforming state-of-the-art methods but also exhibit impressive generalizability and explainability.
</details>
<details>
<summary>摘要</summary>
traditional goal of AI systems 是 perform complex multimodal reasoning 如人类。Recently, large language models (LLMs) have made remarkable progress in such multi-step reasoning on the language modality by leveraging the chain of thought (CoT) to mimic human thinking. However, the transfer of these advancements to multimodal contexts introduces increased challenges, including but not limited to the impractical need for labor-intensive annotation and limitations in terms of flexibility, generalizability, and explainability. To evoke CoT reasoning in multimodality, this work conducts an in-depth analysis of the challenges posed by multimodality and presents two key insights: "keeping critical thinking" and "letting everyone do their jobs" in multimodal CoT reasoning. Furthermore, this study proposes a novel DDCoT prompting that maintains a critical attitude through negative-space prompting and incorporates multimodality into reasoning by first dividing the reasoning responsibility of LLMs into reasoning and recognition and then integrating the visual recognition capability of visual models into the joint reasoning process. The rationales generated by DDCoT not only improve the reasoning abilities of both large and small language models in zero-shot prompting and fine-tuning learning, significantly outperforming state-of-the-art methods but also exhibit impressive generalizability and explainability.
</details></li>
</ul>
<hr>
<h2 id="PromptAgent-Strategic-Planning-with-Language-Models-Enables-Expert-level-Prompt-Optimization"><a href="#PromptAgent-Strategic-Planning-with-Language-Models-Enables-Expert-level-Prompt-Optimization" class="headerlink" title="PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization"></a>PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16427">http://arxiv.org/abs/2310.16427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P. Xing, Zhiting Hu</li>
<li>for: 这 paper 的目的是开发一种可以自动生成高质量专家级提问的优化方法，以提高大语言模型（LLM）的表现。</li>
<li>methods: 这 paper 使用了一种基于 Monte Carlo 搜索的原则导航算法，来寻找专家级提问空间中的优质提问。另外，它还引入了人类化的尝试-错误探索机制，以便从模型错误中获得精准的专家级 Insight 和深入的指导。</li>
<li>results: 这 paper 在 12 个任务中证明了 PromptAgent 可以备受提高 Chain-of-Thought 和最近的提问优化基准点。此外，它还进行了广泛的分析，证明了其能够具有高效、普适和域内专家级的提问生成能力。<details>
<summary>Abstract</summary>
Highly effective, task-specific prompts are often heavily engineered by experts to integrate detailed instructions and domain insights based on a deep understanding of both instincts of large language models (LLMs) and the intricacies of the target task. However, automating the generation of such expert-level prompts remains elusive. Existing prompt optimization methods tend to overlook the depth of domain knowledge and struggle to efficiently explore the vast space of expert-level prompts. Addressing this, we present PromptAgent, an optimization method that autonomously crafts prompts equivalent in quality to those handcrafted by experts. At its core, PromptAgent views prompt optimization as a strategic planning problem and employs a principled planning algorithm, rooted in Monte Carlo tree search, to strategically navigate the expert-level prompt space. Inspired by human-like trial-and-error exploration, PromptAgent induces precise expert-level insights and in-depth instructions by reflecting on model errors and generating constructive error feedback. Such a novel framework allows the agent to iteratively examine intermediate prompts (states), refine them based on error feedbacks (actions), simulate future rewards, and search for high-reward paths leading to expert prompts. We apply PromptAgent to 12 tasks spanning three practical domains: BIG-Bench Hard (BBH), as well as domain-specific and general NLP tasks, showing it significantly outperforms strong Chain-of-Thought and recent prompt optimization baselines. Extensive analyses emphasize its capability to craft expert-level, detailed, and domain-insightful prompts with great efficiency and generalizability.
</details>
<details>
<summary>摘要</summary>
高效的任务特定提示通常由专家严格工程来整合详细的指令和领域知识，基于大语言模型（LLM）的本性和目标任务的细节。然而，自动生成专家水平提示的机器化仍然是一个未解之谜。现有的提示优化方法通常会忽略领域知识的深度和专家水平提示的巨大空间，而 PromptAgent 则是一种新的优化方法。PromptAgent 视提示优化为战略规划问题，并使用基于 Monte Carlo 搜索的原则正则算法来策略性浏览专家水平提示空间。被人类类似的尝试错误探索所 inspirited，PromptAgent 通过反思模型错误和生成有用的错误反馈来带来精准的专家水平启示和深入的指令。这种新的框架使得代理人可以随机检查中间提示（状态），根据错误反馈（动作）进行修改，在将来的奖励 simulate 和搜索高荷道路寻找专家提示。我们在 12 个任务中应用 PromptAgent，包括 BBH 和一些域特定和通用 NLP 任务，显示它与强大的 Chain-of-Thought 和最新的提示优化基线相比有显著的优势。广泛的分析表明它可以高效地制造专家水平的详细、领域内在的提示。
</details></li>
</ul>
<hr>
<h2 id="Enhanced-Simultaneous-Machine-Translation-with-Word-level-Policies"><a href="#Enhanced-Simultaneous-Machine-Translation-with-Word-level-Policies" class="headerlink" title="Enhanced Simultaneous Machine Translation with Word-level Policies"></a>Enhanced Simultaneous Machine Translation with Word-level Policies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16417">http://arxiv.org/abs/2310.16417</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xl8-ai/wordsimt">https://github.com/xl8-ai/wordsimt</a></li>
<li>paper_authors: Kang Kim, Hankyu Cho</li>
<li>for: 本研究的目的是提高同时机器翻译（SiMT）的性能，并解决现有研究中常见的一个假设，即在翻译过程中每步都需要读取或写入子单元（subword）。</li>
<li>methods: 本研究使用的方法包括提出了一种新的单词级策略（word-level policy），该策略可以在单词层面进行多个子单元的处理，以实现单词级别的翻译。此外，研究还提出了一种使用语言模型（LM）来提高SiMT模型的方法，该方法利用了word-level policy来解决LM和SiMT模型之间的子单元差异。</li>
<li>results: 研究发现，使用word-level policy可以提高SiMT模型的性能，并且可以 Addressing the subword disparity between LMs and SiMT models. Code is available at <a target="_blank" rel="noopener" href="https://github.com/xl8-ai/WordSiMT">https://github.com/xl8-ai/WordSiMT</a>.<details>
<summary>Abstract</summary>
Recent years have seen remarkable advances in the field of Simultaneous Machine Translation (SiMT) due to the introduction of innovative policies that dictate whether to READ or WRITE at each step of the translation process. However, a common assumption in many existing studies is that operations are carried out at the subword level, even though the standard unit for input and output in most practical scenarios is typically at the word level. This paper demonstrates that policies devised and validated at the subword level are surpassed by those operating at the word level, which process multiple subwords to form a complete word in a single step. Additionally, we suggest a method to boost SiMT models using language models (LMs), wherein the proposed word-level policy plays a vital role in addressing the subword disparity between LMs and SiMT models. Code is available at https://github.com/xl8-ai/WordSiMT.
</details>
<details>
<summary>摘要</summary>
近年来，同时机器翻译（SiMT）领域发生了非常出色的进步，这主要归功于新的政策的引入，这些政策在翻译过程中每步都会决定是否阅读或写入。然而，许多现有研究假设在翻译过程中每步都会进行子词级别的操作，尽管在实际应用场景中，输入和输出标准单位通常是单词级别。本文表明，在子词级别采用的策略会被单词级别的策略所超越，后者可以在单步中处理多个子词，形成完整的单词。此外，我们建议使用语言模型（LM）来提升SiMT模型，其中提议的单词级别策略具有重要的地位，以Addressing LM和SiMT模型之间的子词差异。代码可以在https://github.com/xl8-ai/WordSiMT中找到。
</details></li>
</ul>
<hr>
<h2 id="Decoding-Stumpers-Large-Language-Models-vs-Human-Problem-Solvers"><a href="#Decoding-Stumpers-Large-Language-Models-vs-Human-Problem-Solvers" class="headerlink" title="Decoding Stumpers: Large Language Models vs. Human Problem-Solvers"></a>Decoding Stumpers: Large Language Models vs. Human Problem-Solvers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16411">http://arxiv.org/abs/2310.16411</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alon Goldstein, Miriam Havin, Roi Reichart, Ariel Goldstein</li>
<li>for: 本研究探讨了大语言模型（LLMs）的问题解决能力，通过评估它们在独特的单步直觉问题上的表现。</li>
<li>methods: 本研究使用了四种当今最先进的LLMs（Davinci-2、Davinci-3、GPT-3.5-Turbo和GPT-4）和人类参与者进行比较。</li>
<li>results: 研究发现，新一代LLMs在解决独特问题上表现出色，超越人类表现。然而，人类参与者在验证解决方案的能力方面表现更出色。这些研究增强了我们对LLMs的认知能力的理解，并为不同领域中LLMs的问题解决潜力提供了新的思路。<details>
<summary>Abstract</summary>
This paper investigates the problem-solving capabilities of Large Language Models (LLMs) by evaluating their performance on stumpers, unique single-step intuition problems that pose challenges for human solvers but are easily verifiable. We compare the performance of four state-of-the-art LLMs (Davinci-2, Davinci-3, GPT-3.5-Turbo, GPT-4) to human participants. Our findings reveal that the new-generation LLMs excel in solving stumpers and surpass human performance. However, humans exhibit superior skills in verifying solutions to the same problems. This research enhances our understanding of LLMs' cognitive abilities and provides insights for enhancing their problem-solving potential across various domains.
</details>
<details>
<summary>摘要</summary>
这篇论文研究了大语言模型（LLMs）的问题解决能力，通过评估它们在单步直觉问题上的表现，这些问题对人类解决者来说是困难的，但是易于验证。我们比较了四个当今最先进的LLMs（Davinci-2、Davinci-3、GPT-3.5-Turbo、GPT-4）与人类参与者的表现。我们的发现表明，新一代LLMs在解决这些问题方面表现出色，超越了人类表现。然而，人类参与者在验证解决方案的能力方面表现出优异。这些研究增加了我们对LLMs的认知能力的理解，并为各个领域中LLMs的问题解决潜力带来了新的想法。
</details></li>
</ul>
<hr>
<h2 id="Video-Referring-Expression-Comprehension-via-Transformer-with-Content-conditioned-Query"><a href="#Video-Referring-Expression-Comprehension-via-Transformer-with-Content-conditioned-Query" class="headerlink" title="Video Referring Expression Comprehension via Transformer with Content-conditioned Query"></a>Video Referring Expression Comprehension via Transformer with Content-conditioned Query</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16402">http://arxiv.org/abs/2310.16402</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ji Jiang, Meng Cao, Tengtao Song, Long Chen, Yi Wang, Yuexian Zou</li>
<li>for: 本研究旨在提高视频表达理解（REC）中的目标对象定位精度，基于自然语言提交的问题。</li>
<li>methods: 该研究使用Transformer类型方法，并采用可学习的查询设计。然而，我们认为这种简单的查询设计不适合开放世界的视频REC，由于文本监督下的多种 semantics category。我们的解决方案是创建动态查询，它们是基于输入视频和自然语言来模型多种被引用的物体。特别是，我们在帧中预设一定数量的可学习 bounding box，并使用相关区域特征来提供先前信息。此外，我们发现现有的查询特征忽视了跨模态对齐的重要性。为此，我们将特定句子在句子中与semantic relevante的视觉区域进行对齐，并在现有的视频dataset（VID-Sentence和VidSTG）中进行标注。</li>
<li>results: 我们的提出的模型（名为ConFormer）在广泛的 benchmark dataset 上表现出优于其他模型。例如，在VID-Sentence dataset的测试分区中，ConFormer 在 <a href="mailto:&#65;&#x63;&#x63;&#x75;&#46;&#64;&#48;&#x2e;&#54;">&#65;&#x63;&#x63;&#x75;&#46;&#64;&#48;&#x2e;&#54;</a> 上实现了8.75%的绝对改进，比前一个状态的艺术模型更高。<details>
<summary>Abstract</summary>
Video Referring Expression Comprehension (REC) aims to localize a target object in videos based on the queried natural language. Recent improvements in video REC have been made using Transformer-based methods with learnable queries. However, we contend that this naive query design is not ideal given the open-world nature of video REC brought by text supervision. With numerous potential semantic categories, relying on only a few slow-updated queries is insufficient to characterize them. Our solution to this problem is to create dynamic queries that are conditioned on both the input video and language to model the diverse objects referred to. Specifically, we place a fixed number of learnable bounding boxes throughout the frame and use corresponding region features to provide prior information. Also, we noticed that current query features overlook the importance of cross-modal alignment. To address this, we align specific phrases in the sentence with semantically relevant visual areas, annotating them in existing video datasets (VID-Sentence and VidSTG). By incorporating these two designs, our proposed model (called ConFormer) outperforms other models on widely benchmarked datasets. For example, in the testing split of VID-Sentence dataset, ConFormer achieves 8.75% absolute improvement on Accu.@0.6 compared to the previous state-of-the-art model.
</details>
<details>
<summary>摘要</summary>
视频寻 Referring Expression Comprehension (REC) 目标是根据查询的自然语言来地址视频中的目标对象。 最近的改进方法使用 Transformer 基于方法，并使用可学习的查询。 但我们认为这种愚然的查询设计并不适合开放世界的视频 REC 中，因为它们可能会忽略多种可能的SemanticCategory。 我们的解决方案是创建动态的查询，它们是基于输入视频和语言来模型多种被引用的对象。 具体来说，我们在帧中预定一定数量的可学习的 bounding box，并使用相应的区域特征来提供先前信息。 同时，我们注意到现有的查询特征 ignore 视频和语言之间的协调。 为解决这个问题，我们将特定的句子在句子中与 semantically 相关的视觉区域进行对齐，并在现有的视频 dataset（VID-Sentence和 VidSTG）中进行标注。 通过这两种设计，我们的提议的模型（叫做 ConFormer）在广泛的标准化数据集上超越了其他模型。 例如，在 VID-Sentence 数据集的测试分区中，ConFormer 在 Accu.@0.6 上减少了8.75%的绝对改进，相比之前的状态对应模型。
</details></li>
</ul>
<hr>
<h2 id="ZGUL-Zero-shot-Generalization-to-Unseen-Languages-using-Multi-source-Ensembling-of-Language-Adapters"><a href="#ZGUL-Zero-shot-Generalization-to-Unseen-Languages-using-Multi-source-Ensembling-of-Language-Adapters" class="headerlink" title="ZGUL: Zero-shot Generalization to Unseen Languages using Multi-source Ensembling of Language Adapters"></a>ZGUL: Zero-shot Generalization to Unseen Languages using Multi-source Ensembling of Language Adapters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16393">http://arxiv.org/abs/2310.16393</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dair-iitd/zgul">https://github.com/dair-iitd/zgul</a></li>
<li>paper_authors: Vipul Rathore, Rajdeep Dhingra, Parag Singla, Mausam</li>
<li>for: 这篇论文目的是解决zero-shot多语言传输问题在自然语言处理任务中。</li>
<li>methods: 论文使用语言适应器（LA）来实现多语言传输。LA通常是单个源语言（通常是英语）的适应器，在测试时使用目标语言或另一种相关语言的适应器。但是，训练目标语言的适应器需要无标签数据，这可能不太可能得到低资源的未看过语言：那些 neither seen by the underlying multilingual language model（例如，mBERT），也没有任何（标签或无标签）数据。因此，我们认为为更有效的跨语言传输，需要使用多个源语言的适应器，同时在训练和测试时使用它们。我们通过我们的新的神经网络架构ZGUL进行了调查。</li>
<li>results: 我们在四种语言组合中进行了广泛的实验，覆盖了15个未看过语言。结果表明，ZGUL比标准精度调整和其他强大基elines在POS标记和NER任务上提高了3.2个平均F1点。此外，我们还扩展了ZGUL，使其在有些未标签数据或少量训练示例available for the target language时也能够表现出色。在这些设置下，ZGUL仍然超过基elines。<details>
<summary>Abstract</summary>
We tackle the problem of zero-shot cross-lingual transfer in NLP tasks via the use of language adapters (LAs). Most of the earlier works have explored training with adapter of a single source (often English), and testing either using the target LA or LA of another related language. Training target LA requires unlabeled data, which may not be readily available for low resource unseen languages: those that are neither seen by the underlying multilingual language model (e.g., mBERT), nor do we have any (labeled or unlabeled) data for them. We posit that for more effective cross-lingual transfer, instead of just one source LA, we need to leverage LAs of multiple (linguistically or geographically related) source languages, both at train and test-time - which we investigate via our novel neural architecture, ZGUL. Extensive experimentation across four language groups, covering 15 unseen target languages, demonstrates improvements of up to 3.2 average F1 points over standard fine-tuning and other strong baselines on POS tagging and NER tasks. We also extend ZGUL to settings where either (1) some unlabeled data or (2) few-shot training examples are available for the target language. We find that ZGUL continues to outperform baselines in these settings too.
</details>
<details>
<summary>摘要</summary>
我们通过语言适配器（LA）解决了零样式跨语言传输问题在自然语言处理任务中。大多数先前的工作都是通过单个源语言（常常是英语）的适配器进行训练，然后在目标语言或另一种相关语言的适配器上进行测试。但是，训练目标语言的适配器需要无标签数据，这些数据可能不易 disponibility  для低资源、未看到语言模型（如mBERT）中的语言。我们认为，为更有效的跨语言传输，不仅需要单一源语言的适配器，而是需要多种语言适配器，包括训练和测试时间。我们提出了一种新的神经网络架构，ZGUL，以 investigate 这种想法。我们在四种语言组中进行了广泛的实验，涵盖了15种未看到目标语言，并证明了ZGUL可以与标准精细调整和其他强大基elines 比较。我们还将ZGUL扩展到有限量的标签数据或几个培训示例的情况下。我们发现ZGUL仍然能够超越基elines 在这些情况下。
</details></li>
</ul>
<hr>
<h2 id="Transformer-based-Live-Update-Generation-for-Soccer-Matches-from-Microblog-Posts"><a href="#Transformer-based-Live-Update-Generation-for-Soccer-Matches-from-Microblog-Posts" class="headerlink" title="Transformer-based Live Update Generation for Soccer Matches from Microblog Posts"></a>Transformer-based Live Update Generation for Soccer Matches from Microblog Posts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16368">http://arxiv.org/abs/2310.16368</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masashi Oshika, Kosuke Yamada, Ryohei Sasano, Koichi Takeda</li>
<li>for: 这篇论文是为了生成来自推文的实时足球赛事更新，以便用户可以通过原始推文了解比赛的进程和激励。</li>
<li>methods: 该论文基于大型预训练语言模型，并实现了控制更新数量和减少重复更新的机制。</li>
<li>results: 该系统可以快速生成高质量的实时足球赛事更新，使用户可以快速了解比赛的进程和激励。<details>
<summary>Abstract</summary>
It has been known to be difficult to generate adequate sports updates from a sequence of vast amounts of diverse live tweets, although the live sports viewing experience with tweets is gaining the popularity. In this paper, we focus on soccer matches and work on building a system to generate live updates for soccer matches from tweets so that users can instantly grasp a match's progress and enjoy the excitement of the match from raw tweets. Our proposed system is based on a large pre-trained language model and incorporates a mechanism to control the number of updates and a mechanism to reduce the redundancy of duplicate and similar updates.
</details>
<details>
<summary>摘要</summary>
Live 体育更新从涂浮 tweets 是一个Difficult task，Despite the popularity of live sports viewing experience with tweets. In this paper, we focus on soccer matches and work on building a system to generate live updates for soccer matches from tweets, so that users can instantly grasp the progress of the match and enjoy the excitement of the match from raw tweets. Our proposed system is based on a large pre-trained language model and incorporates a mechanism to control the number of updates and a mechanism to reduce the redundancy of duplicate and similar updates.
</details></li>
</ul>
<hr>
<h2 id="From-Simple-to-Complex-A-Progressive-Framework-for-Document-level-Informative-Argument-Extraction"><a href="#From-Simple-to-Complex-A-Progressive-Framework-for-Document-level-Informative-Argument-Extraction" class="headerlink" title="From Simple to Complex: A Progressive Framework for Document-level Informative Argument Extraction"></a>From Simple to Complex: A Progressive Framework for Document-level Informative Argument Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16358">http://arxiv.org/abs/2310.16358</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhangyx0417/simple_to_complex">https://github.com/zhangyx0417/simple_to_complex</a></li>
<li>paper_authors: Quzhe Huang, Yanxi Zhang, Dongyan Zhao</li>
<li>for: 这个论文旨在提高文档级事件抽象EXTRACTION（EAE）的准确率。</li>
<li>methods: 该论文提出了一种简单到复杂的推进 Framework，通过计算每个事件的难度，然后按照简单到复杂的顺序进行抽象。这样，模型可以使用更可靠的结果来帮助预测更加困难的事件。</li>
<li>results: 在WikiEvents数据集上进行实验，该模型的F1分数高于SOTA的1.4%，表明提出的简单到复杂推进 Framework 对EAE任务有用。<details>
<summary>Abstract</summary>
Document-level Event Argument Extraction (EAE) requires the model to extract arguments of multiple events from a single document. Considering the underlying dependencies between these events, recent efforts leverage the idea of "memory", where the results of already predicted events are cached and can be retrieved to help the prediction of upcoming events. These methods extract events according to their appearance order in the document, however, the event that appears in the first sentence does not mean that it is the easiest to extract. Existing methods might introduce noise to the extraction of upcoming events if they rely on an incorrect prediction of previous events. In order to provide more reliable memory, we propose a simple-to-complex progressive framework for document-level EAE. Specifically, we first calculate the difficulty of each event and then, we conduct the extraction following a simple-to-complex order. In this way, the memory will store the most certain results, and the model could use these reliable sources to help the prediction of more difficult events. Experiments on WikiEvents show that our model outperforms SOTA by 1.4% in F1, indicating the proposed simple-to-complex framework is useful in the EAE task.
</details>
<details>
<summary>摘要</summary>
文档级事件功能抽取（EAE）需要模型从单个文档中提取多个事件的功能。尽管这些事件之间存在蕴含的依赖关系，但现有的方法却是通过“记忆”的想法来实现，即已经预测过的事件的结果被缓存并可以用于帮助预测未来的事件。这些方法通常是按照文档中事件的出现顺序来提取事件，但是第一个事件的出现并不意味着它是最容易提取的。现有的方法可能会对后续事件的提取引入噪音，如果它们基于错误的先前事件预测。为了提供更可靠的记忆，我们提议一种简单到复杂的进程式框架 для文档级EAE。具体来说，我们首先计算每个事件的难度，然后按照简单到复杂的顺序进行提取。这样，记忆将存储最可靠的结果，并且模型可以使用这些可靠的源来帮助预测更加困难的事件。在WikiEvents上的实验表明，我们的模型在F1指标上比SOTA高出1.4%，这表明我们提议的简单到复杂的框架是EAE任务中有用的。
</details></li>
</ul>
<hr>
<h2 id="A-Multi-Modal-Multilingual-Benchmark-for-Document-Image-Classification"><a href="#A-Multi-Modal-Multilingual-Benchmark-for-Document-Image-Classification" class="headerlink" title="A Multi-Modal Multilingual Benchmark for Document Image Classification"></a>A Multi-Modal Multilingual Benchmark for Document Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16356">http://arxiv.org/abs/2310.16356</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yoshinari Fujinuma, Siddharth Varia, Nishant Sankaran, Srikar Appalaraju, Bonan Min, Yogarshi Vyas</li>
<li>for: 本研究旨在提供更多和更好的文档图像分类数据集，以便进一步研究文档图像分类技术。</li>
<li>methods: 本研究使用了两个新的多语言文档图像数据集：WIKI-DOC和MULTIEURLEX-DOC，并对现有的文档图像分类模型进行了全面的测试。</li>
<li>results: 实验结果表明，当文档图像分类模型在不同语言之间进行零例转移时，其限制性很大，需要进一步改进。<details>
<summary>Abstract</summary>
Document image classification is different from plain-text document classification and consists of classifying a document by understanding the content and structure of documents such as forms, emails, and other such documents. We show that the only existing dataset for this task (Lewis et al., 2006) has several limitations and we introduce two newly curated multilingual datasets WIKI-DOC and MULTIEURLEX-DOC that overcome these limitations. We further undertake a comprehensive study of popular visually-rich document understanding or Document AI models in previously untested setting in document image classification such as 1) multi-label classification, and 2) zero-shot cross-lingual transfer setup. Experimental results show limitations of multilingual Document AI models on cross-lingual transfer across typologically distant languages. Our datasets and findings open the door for future research into improving Document AI models.
</details>
<details>
<summary>摘要</summary>
文档图像分类与文本文档分类不同，它涉及到理解文档的内容和结构，如表格、电子邮件等。我们表明现有的数据集（Lewis et al., 2006）有几个限制，我们介绍了两个新的多语言数据集——WIKI-DOC和MULTIEURLEX-DOC，这两个数据集可以缓解这些限制。我们进行了详细的文档理解或文档AI模型在文档图像分类中的测试，包括1）多个标签分类和2）零shot跨语言传输设置。实验结果显示跨语言文档AI模型在跨语言传输中存在限制。我们的数据集和发现开启了未来文档AI模型的改进研究的大门。
</details></li>
</ul>
<hr>
<h2 id="Unraveling-Feature-Extraction-Mechanisms-in-Neural-Networks"><a href="#Unraveling-Feature-Extraction-Mechanisms-in-Neural-Networks" class="headerlink" title="Unraveling Feature Extraction Mechanisms in Neural Networks"></a>Unraveling Feature Extraction Mechanisms in Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16350">http://arxiv.org/abs/2310.16350</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/richardsun-voyager/ufemnn">https://github.com/richardsun-voyager/ufemnn</a></li>
<li>paper_authors: Xiaobing Sun, Jiaxi Li, Wei Lu</li>
<li>for:  investigate the underlying mechanisms of neural networks in capturing precise knowledge</li>
<li>methods:  based on Neural Tangent Kernels (NTKs) to analyze the learning dynamics of target models</li>
<li>results:  discovered that the choice of activation function can affect feature extraction, and that multiplication-based models excel in learning n-grams.<details>
<summary>Abstract</summary>
The underlying mechanism of neural networks in capturing precise knowledge has been the subject of consistent research efforts. In this work, we propose a theoretical approach based on Neural Tangent Kernels (NTKs) to investigate such mechanisms. Specifically, considering the infinite network width, we hypothesize the learning dynamics of target models may intuitively unravel the features they acquire from training data, deepening our insights into their internal mechanisms. We apply our approach to several fundamental models and reveal how these models leverage statistical features during gradient descent and how they are integrated into final decisions. We also discovered that the choice of activation function can affect feature extraction. For instance, the use of the \textit{ReLU} activation function could potentially introduce a bias in features, providing a plausible explanation for its replacement with alternative functions in recent pre-trained language models. Additionally, we find that while self-attention and CNN models may exhibit limitations in learning n-grams, multiplication-based models seem to excel in this area. We verify these theoretical findings through experiments and find that they can be applied to analyze language modeling tasks, which can be regarded as a special variant of classification. Our contributions offer insights into the roles and capacities of fundamental components within large language models, thereby aiding the broader understanding of these complex systems.
</details>
<details>
<summary>摘要</summary>
大脑网络的内部机制如何捕捉精准知识，一直是研究的热点。在这项工作中，我们提出了基于神经 Tangent Kernels（NTK）的理论方法，以探索这些机制。具体来说，我们假设在训练数据上，目标模型的学习过程可以直观地解释它们从数据中继承的特征，从而深入了解它们的内部机制。我们应用我们的方法到一些基本模型上，揭示了这些模型在梯度下降过程中如何从数据中提取特征，以及如何将这些特征集成到最终决策中。我们还发现了活动函数的选择可能会影响特征提取，例如使用 ReLU 活动函数可能会引入偏见，从而解释其在最新的预训练语言模型中的替换。此外，我们发现了自注意力和 CNN 模型可能会在学习 n-grams 方面存在限制，而乘法基本模型则在这一方面表现出色。我们通过实验验证了这些理论发现，并发现它们可以应用于语言模型Task中，这可以视为一种特殊的分类任务。我们的贡献可以帮助我们更好地理解这些复杂系统中的基本组件的角色和能力，从而推动大脑网络的进一步发展。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Evaluation-of-Constrained-Text-Generation-for-Large-Language-Models"><a href="#A-Comprehensive-Evaluation-of-Constrained-Text-Generation-for-Large-Language-Models" class="headerlink" title="A Comprehensive Evaluation of Constrained Text Generation for Large Language Models"></a>A Comprehensive Evaluation of Constrained Text Generation for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16343">http://arxiv.org/abs/2310.16343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Chen, Xiaojun Wan</li>
<li>for: This paper aims to investigate the integration of intricate constraints into neural text generation using large language models (LLMs).</li>
<li>methods: The study employs multiple LLMs, including ChatGPT and GPT-4, and categorizes constraints into lexical, structural, and relation-based types. The authors present various benchmarks to facilitate fair evaluation.</li>
<li>results: The study reveals LLMs’ capacity and deficiency to incorporate constraints, providing insights for future developments in constrained text generation.Here’s the same information in Simplified Chinese:</li>
<li>for: 这篇论文目的是调查大语言模型（LLMs）中的约束性文本生成。</li>
<li>methods: 这些研究使用多种LLMs，包括ChatGPT和GPT-4，并将约束分为 lexical、structural 和relation-based 类型。作者们提供了多种标准化的评价指标。</li>
<li>results: 研究发现 LLMs 在约束下的文本生成能力和缺陷，提供了未来约束文本生成的指导。I hope that helps!<details>
<summary>Abstract</summary>
Advancements in natural language generation (NLG) and large language models (LLMs) have led to proficient text generation in various tasks. However, integrating intricate constraints into neural text generation, due to LLMs' opacity, remains challenging. This study investigates constrained text generation for LLMs, where predefined constraints are applied during LLM's generation process. Our research examines multiple LLMs, including ChatGPT and GPT-4, categorizing constraints into lexical, structural, and relation-based types. We also present various benchmarks to facilitate fair evaluation. The study addresses some key research questions, including the extent of LLMs' compliance with constraints. Results illuminate LLMs' capacity and deficiency to incorporate constraints and provide insights for future developments in constrained text generation. Codes and datasets will be released upon acceptance.
</details>
<details>
<summary>摘要</summary>
Recent advancements in natural language generation (NLG) and large language models (LLMs) have led to significant improvements in text generation for various tasks. However, incorporating complex constraints into neural text generation remains a challenging task due to the opacity of LLMs. This study explores constrained text generation for LLMs, where predefined constraints are applied during the LLM's generation process. Our research categorizes constraints into three types: lexical, structural, and relation-based, and examines multiple LLMs, including ChatGPT and GPT-4. We also provide various benchmarks to facilitate fair evaluation. The study aims to answer several key research questions, including the extent of LLMs' compliance with constraints. The results shed light on LLMs' capacity and limitations in incorporating constraints and provide valuable insights for future developments in constrained text generation. The codes and datasets will be released upon acceptance.
</details></li>
</ul>
<hr>
<h2 id="RCAgent-Cloud-Root-Cause-Analysis-by-Autonomous-Agents-with-Tool-Augmented-Large-Language-Models"><a href="#RCAgent-Cloud-Root-Cause-Analysis-by-Autonomous-Agents-with-Tool-Augmented-Large-Language-Models" class="headerlink" title="RCAgent: Cloud Root Cause Analysis by Autonomous Agents with Tool-Augmented Large Language Models"></a>RCAgent: Cloud Root Cause Analysis by Autonomous Agents with Tool-Augmented Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16340">http://arxiv.org/abs/2310.16340</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zefan Wang, Zichuan Liu, Yingying Zhang, Aoxiao Zhong, Lunting Fan, Lingfei Wu, Qingsong Wen</li>
<li>for: 这个论文旨在提出一种自主代理工具框架，以便在实际工业环境中进行自主和隐私保护的根本原因分析（RCA）。</li>
<li>methods: 该框架使用了一些增强技术，包括自我一致性和多种上下文管理、稳定化和知识导入方法。</li>
<li>results: 实验结果显示，与ReAct相比，RCAgent在多个方面（包括根本原因预测、解决方案、证据和责任等）具有明显的优势，并且已经成功地 интеGRATED到了阿里巴巴云的Real-time Compute Platform for Apache Flink的诊断和问题探索工作流程中。<details>
<summary>Abstract</summary>
Large language model (LLM) applications in cloud root cause analysis (RCA) have been actively explored recently. However, current methods are still reliant on manual workflow settings and do not unleash LLMs' decision-making and environment interaction capabilities. We present RCAgent, a tool-augmented LLM autonomous agent framework for practical and privacy-aware industrial RCA usage. Running on an internally deployed model rather than GPT families, RCAgent is capable of free-form data collection and comprehensive analysis with tools. Our framework combines a variety of enhancements, including a unique Self-Consistency for action trajectories, and a suite of methods for context management, stabilization, and importing domain knowledge. Our experiments show RCAgent's evident and consistent superiority over ReAct across all aspects of RCA -- predicting root causes, solutions, evidence, and responsibilities -- and tasks covered or uncovered by current rules, as validated by both automated metrics and human evaluations. Furthermore, RCAgent has already been integrated into the diagnosis and issue discovery workflow of the Real-time Compute Platform for Apache Flink of Alibaba Cloud.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Generative-Pre-training-for-Speech-with-Flow-Matching"><a href="#Generative-Pre-training-for-Speech-with-Flow-Matching" class="headerlink" title="Generative Pre-training for Speech with Flow Matching"></a>Generative Pre-training for Speech with Flow Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16338">http://arxiv.org/abs/2310.16338</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander H. Liu, Matt Le, Apoorv Vyas, Bowen Shi, Andros Tjandra, Wei-Ning Hsu</li>
<li>for: 这个论文的目的是建立一个基础模型来进行语音生成任务。</li>
<li>methods: 这个论文使用的方法是在60000小时的不分译语音数据上进行流匹配和masked condition的预训练，然后根据任务特定的数据进行细化。</li>
<li>results: 实验结果显示，预训练后的生成模型可以与专家模型一样或超过它们在语音增强、分离和合成等下游任务中表现。这些结果建议了一个基于生成预训练的基础模型可以为语音生成任务提供支持。<details>
<summary>Abstract</summary>
Generative models have gained more and more attention in recent years for their remarkable success in tasks that required estimating and sampling data distribution to generate high-fidelity synthetic data. In speech, text-to-speech synthesis and neural vocoder are good examples where generative models have shined. While generative models have been applied to different applications in speech, there exists no general-purpose generative model that models speech directly. In this work, we take a step toward this direction by showing a single pre-trained generative model can be adapted to different downstream tasks with strong performance. Specifically, we pre-trained a generative model, named SpeechFlow, on 60k hours of untranscribed speech with Flow Matching and masked conditions. Experiment results show the pre-trained generative model can be fine-tuned with task-specific data to match or surpass existing expert models on speech enhancement, separation, and synthesis. Our work suggested a foundational model for generation tasks in speech can be built with generative pre-training.
</details>
<details>
<summary>摘要</summary>
现代生成模型在过去几年内得到了更多的关注，因为它们在估计和采样数据分布以生成高效的合成数据方面取得了非常出色的成果。在语音领域，语音合成和神经 vocoder 是一些生成模型在取得了极高水平的成果的例子。然而，在语音领域，没有一个通用的生成模型可以直接模型语音。在这项工作中，我们向这个方向发展了一步，我们显示了一个预训练的生成模型，即 SpeechFlow，可以在不同的下游任务中达到强大的性能。 Specifically, we pre-trained SpeechFlow 模型在 60 万小时的无转录语音数据上进行 Flow Matching 和 masked 条件。实验结果表明，可以在任务特定的数据上练习这个预训练模型，以达到或超过现有专家模型的表现水平。我们的工作建议了基于生成预训练的基本模型可以在语音生成任务中建立。注意：以下是将文本翻译成 Simplified Chinese，但不包括所有特殊的语音相关 терминологи。如果需要更加详细的翻译，请随时指明。
</details></li>
</ul>
<hr>
<h2 id="Samsung-R-D-Institute-Philippines-at-WMT-2023"><a href="#Samsung-R-D-Institute-Philippines-at-WMT-2023" class="headerlink" title="Samsung R&amp;D Institute Philippines at WMT 2023"></a>Samsung R&amp;D Institute Philippines at WMT 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16322">http://arxiv.org/abs/2310.16322</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Christian Blaise Cruz</li>
<li>for: 这 paper 是为了描述 Samsung R&amp;D Institute Philippines 在 WMT 2023 通用翻译任务中提交的受限MT 系统，包括 en$\rightarrow$he 和 he$\rightarrow$en 两个方向。</li>
<li>methods: 这些系统采用了一系列最佳实践，包括全面的数据处理管道、人工生成的反向翻译数据和在线解码中使用噪声通道重新排序。</li>
<li>results: 这些模型在两个公共测试集上表现良好，与强基线系统相当，甚至occasionally outperform，即使它们有许多 fewer 参数。<details>
<summary>Abstract</summary>
In this paper, we describe the constrained MT systems submitted by Samsung R&D Institute Philippines to the WMT 2023 General Translation Task for two directions: en$\rightarrow$he and he$\rightarrow$en. Our systems comprise of Transformer-based sequence-to-sequence models that are trained with a mix of best practices: comprehensive data preprocessing pipelines, synthetic backtranslated data, and the use of noisy channel reranking during online decoding. Our models perform comparably to, and sometimes outperform, strong baseline unconstrained systems such as mBART50 M2M and NLLB 200 MoE despite having significantly fewer parameters on two public benchmarks: FLORES-200 and NTREX-128.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们描述了我们由Samsung R&D Institute Philippines提交到WMT 2023通用翻译任务的受限MT系统，包括en$\rightarrow$he和he$\rightarrow$en两个方向。我们的系统采用了基于Transformer的序列到序列模型，通过一系列最佳实践进行训练，包括全面的数据预处理管道、合成回传数据和在线解码中使用噪声通道重新排序。我们的模型与强基线系统如mBART50 M2M和NLLB 200 MoE相比，在两个公共benchmark上表现相当，有时 même outperform，即使我们的参数数量相对较少。
</details></li>
</ul>
<hr>
<h2 id="DiQAD-A-Benchmark-Dataset-for-End-to-End-Open-domain-Dialogue-Assessment"><a href="#DiQAD-A-Benchmark-Dataset-for-End-to-End-Open-domain-Dialogue-Assessment" class="headerlink" title="DiQAD: A Benchmark Dataset for End-to-End Open-domain Dialogue Assessment"></a>DiQAD: A Benchmark Dataset for End-to-End Open-domain Dialogue Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16319">http://arxiv.org/abs/2310.16319</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yukun Zhao, Lingyong Yan, Weiwei Sun, Chong Meng, Shuaiqiang Wang, Zhicong Cheng, Zhaochun Ren, Dawei Yin</li>
<li>for: 本研究是为了提供一个大规模的对话质量评估数据集（DiQAD），用于自动评估开放领域对话质量。</li>
<li>methods: 本研究使用了基于人类对对话质量的评估标准来确定评估标准，然后对实际用户之间的大规模对话进行了标注。</li>
<li>results: 本研究通过多种实验，报告了基线的性能在DiQAD上。同时，也公开了这个数据集，可以供后续研究使用。<details>
<summary>Abstract</summary>
Dialogue assessment plays a critical role in the development of open-domain dialogue systems. Existing work are uncapable of providing an end-to-end and human-epistemic assessment dataset, while they only provide sub-metrics like coherence or the dialogues are conversed between annotators far from real user settings. In this paper, we release a large-scale dialogue quality assessment dataset (DiQAD), for automatically assessing open-domain dialogue quality. Specifically, we (1) establish the assessment criteria based on the dimensions conforming to human judgements on dialogue qualities, and (2) annotate large-scale dialogues that conversed between real users based on these annotation criteria, which contains around 100,000 dialogues. We conduct several experiments and report the performances of the baselines as the benchmark on DiQAD. The dataset is openly accessible at https://github.com/yukunZhao/Dataset_Dialogue_quality_evaluation.
</details>
<details>
<summary>摘要</summary>
对话评估在开放领域对话系统的发展中扮演了关键角色。现有的工作无法提供总体和人类知识基础的对话评估数据集，只提供了一些子指标，如对话 coherence 或者对话者之间的对话是在不实际用户设置下进行的。在这篇论文中，我们发布了一个大规模的对话质量评估数据集（DiQAD），用于自动评估开放领域对话质量。specifically，我们（1）确定了评估标准基于对话质量的人类判断维度，并（2）对实际用户之间的大规模对话进行了annotate，这些对话约有100,000个。我们进行了多个实验，并对基线进行了评估。数据集可以在 <https://github.com/yukunZhao/Dataset_Dialogue_quality_evaluation> 中免费下载。
</details></li>
</ul>
<hr>
<h2 id="URL-BERT-Training-Webpage-Representations-via-Social-Media-Engagements"><a href="#URL-BERT-Training-Webpage-Representations-via-Social-Media-Engagements" class="headerlink" title="URL-BERT: Training Webpage Representations via Social Media Engagements"></a>URL-BERT: Training Webpage Representations via Social Media Engagements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16303">http://arxiv.org/abs/2310.16303</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ayesha Qamar, Chetan Verma, Ahmed El-Kishky, Sumit Binnani, Sneha Mehta, Taylor Berg-Kirkpatrick</li>
<li>for: 本研究旨在适应语言模型（LM）理解和表示网页内容，提高在社交媒体上分享和参与URL时的表达能力。</li>
<li>methods: 本研究提出了一种新的预训练目标，可以使LM适应理解URL和网页内容，并通过用户在社交媒体上的互动来学习URL的表示。</li>
<li>results: 通过对多语言版本BERT进行继续预训练，我们实际地证明了我们的框架可以提高 webpage 理解的多种任务和Twitter内部和外部 benchмарks 的表现。<details>
<summary>Abstract</summary>
Understanding and representing webpages is crucial to online social networks where users may share and engage with URLs. Common language model (LM) encoders such as BERT can be used to understand and represent the textual content of webpages. However, these representations may not model thematic information of web domains and URLs or accurately capture their appeal to social media users. In this work, we introduce a new pre-training objective that can be used to adapt LMs to understand URLs and webpages. Our proposed framework consists of two steps: (1) scalable graph embeddings to learn shallow representations of URLs based on user engagement on social media and (2) a contrastive objective that aligns LM representations with the aforementioned graph-based representation. We apply our framework to the multilingual version of BERT to obtain the model URL-BERT. We experimentally demonstrate that our continued pre-training approach improves webpage understanding on a variety of tasks and Twitter internal and external benchmarks.
</details>
<details>
<summary>摘要</summary>
理解和表示网页是在在线社交网络中关键的，因为用户可能将链接和分享在网页上。常见的语言模型（LM）编码器，如BERT，可以用来理解和表示网页的文本内容。然而，这些表示可能不会模型网页的主题信息或正确地捕捉社交媒体用户的appeal。在这项工作中，我们介绍了一种新的预训练目标，可以用来适应LM理解URL和网页。我们的提posed框架包括两个步骤：（1）可扩展的图 embedding来学习URL的浅层表示，基于社交媒体上的用户互动，以及（2）一种对比目标，用于将LM表示与上述图基于的表示相对应。我们在多语言版本的BERT上应用了我们的框架，得到了模型URL-BERT。我们通过实验表明，我们的继续预训练方法可以提高网页理解的多种任务和Twitter内部和外部 bencmarks。
</details></li>
</ul>
<hr>
<h2 id="Is-ChatGPT-a-Good-Multi-Party-Conversation-Solver"><a href="#Is-ChatGPT-a-Good-Multi-Party-Conversation-Solver" class="headerlink" title="Is ChatGPT a Good Multi-Party Conversation Solver?"></a>Is ChatGPT a Good Multi-Party Conversation Solver?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16301">http://arxiv.org/abs/2310.16301</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao-Hong Tan, Jia-Chen Gu, Zhen-Hua Ling</li>
<li>for: 这paper的目的是研究大自然语言模型（LLMs）在多方会话（MPCs）中的能力。</li>
<li>methods: 这paper使用的方法包括对ChatGPT和GPT-4的生成器模型进行零基础学习训练，并在多个MPC任务上进行评估。</li>
<li>results: 研究发现，ChatGPT在一些MPC任务上表现不佳，而GPT-4的表现更为出色。此外，通过添加MPC结构，包括说话人和受众建筑，可以提高表现。这项研究为MPC任务中应用生成LLMs提供了全面的评估和分析，并揭示了创造更高效和强大的MPC代理的挑战。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have emerged as influential instruments within the realm of natural language processing; nevertheless, their capacity to handle multi-party conversations (MPCs) -- a scenario marked by the presence of multiple interlocutors involved in intricate information exchanges -- remains uncharted. In this paper, we delve into the potential of generative LLMs such as ChatGPT and GPT-4 within the context of MPCs. An empirical analysis is conducted to assess the zero-shot learning capabilities of ChatGPT and GPT-4 by subjecting them to evaluation across three MPC datasets that encompass five representative tasks. The findings reveal that ChatGPT's performance on a number of evaluated MPC tasks leaves much to be desired, whilst GPT-4's results portend a promising future. Additionally, we endeavor to bolster performance through the incorporation of MPC structures, encompassing both speaker and addressee architecture. This study provides an exhaustive evaluation and analysis of applying generative LLMs to MPCs, casting a light upon the conception and creation of increasingly effective and robust MPC agents. Concurrently, this work underscores the challenges implicit in the utilization of LLMs for MPCs, such as deciphering graphical information flows and generating stylistically consistent responses.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经在自然语言处理领域成为重要的工具，但它们在多方会话（MPC）中的能力仍然是未知之地。在这篇论文中，我们探索了基于生成的LLM，如ChatGPT和GPT-4，在MPC中的潜在。我们对三个MPC数据集进行了零基础学习评估，以评估这些模型在五种代表性任务中的表现。结果表明，ChatGPT在许多评估任务上表现不佳，而GPT-4的结果表明了未来的发展潜力。此外，我们还尝试了通过包含MPC结构，包括说话人和受众建筑，提高性能。这篇研究提供了生成LLM在MPC中的全面评估和分析，推翻了在MPC中使用LLM的挑战，如图像信息流的解读和生成具有风格一致的回应。
</details></li>
</ul>
<hr>
<h2 id="The-Distributional-Hypothesis-Does-Not-Fully-Explain-the-Benefits-of-Masked-Language-Model-Pretraining"><a href="#The-Distributional-Hypothesis-Does-Not-Fully-Explain-the-Benefits-of-Masked-Language-Model-Pretraining" class="headerlink" title="The Distributional Hypothesis Does Not Fully Explain the Benefits of Masked Language Model Pretraining"></a>The Distributional Hypothesis Does Not Fully Explain the Benefits of Masked Language Model Pretraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16261">http://arxiv.org/abs/2310.16261</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ting-Rui Chiang, Dani Yogatama</li>
<li>for: 研究 whether the better sample efficiency and generalization capability of masked language models can be attributed to the semantic similarity encoded in the pretraining data’s distributional property.</li>
<li>methods: 使用 synthetic dataset 和 two real-world datasets 进行分析。</li>
<li>results: 发现 distributional property 对预训练模型的样本效率有益，但不能完全解释模型的泛化能力。<details>
<summary>Abstract</summary>
We analyze the masked language modeling pretraining objective function from the perspective of the distributional hypothesis. We investigate whether better sample efficiency and the better generalization capability of models pretrained with masked language modeling can be attributed to the semantic similarity encoded in the pretraining data's distributional property. Via a synthetic dataset, our analysis suggests that distributional property indeed leads to the better sample efficiency of pretrained masked language models, but does not fully explain the generalization capability. We also conduct analyses over two real-world datasets and demonstrate that the distributional property does not explain the generalization ability of pretrained natural language models either. Our results illustrate our limited understanding of model pretraining and provide future research directions.
</details>
<details>
<summary>摘要</summary>
我们从分布性假设的角度分析掩Masked语言模型预训练目标函数。我们研究whether better sample efficiency和预训练模型的更好泛化能力是由预训练数据的分布性质带来的semantic similarity编码。通过一个 sintetic dataset，我们的分析表明预训练数据的分布性 indeed leads to better sample efficiency of pretrained masked language models, but does not fully explain the generalization ability.我们还对两个实际 dataset进行了分析，并证明了预训练自然语言模型的泛化能力不受分布性的影响。我们的结果表明我们对预训练的理解仍然有限，并提供了未来研究的方向。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/25/cs.CL_2023_10_25/" data-id="clorjzl5800dff1882xkl16tq" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_10_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/25/cs.LG_2023_10_25/" class="article-date">
  <time datetime="2023-10-25T10:00:00.000Z" itemprop="datePublished">2023-10-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/25/cs.LG_2023_10_25/">cs.LG - 2023-10-25</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Strategizing-EV-Charging-and-Renewable-Integration-in-Texas"><a href="#Strategizing-EV-Charging-and-Renewable-Integration-in-Texas" class="headerlink" title="Strategizing EV Charging and Renewable Integration in Texas"></a>Strategizing EV Charging and Renewable Integration in Texas</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17056">http://arxiv.org/abs/2310.17056</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Mohammadi, Jesse Thornburg</li>
<li>for: 本研究旨在探讨电动汽车（EVs）、可再生能源和智能电网技术在德州上的整合，并解决电动汽车普及化困难所带来的挑战。</li>
<li>methods: 该研究使用时间扭曲 clustering（DTW）和k-means clustering方法对每天的总负荷和网络负荷进行分类，从而获得每天的电力消耗和可再生能源生产特征。此外，研究还提出了一种基于特定负荷特点的优化充电和汽车到电网（V2G）窗口的方法，以便更好地决策能源消耗和可再生资源的整合。</li>
<li>results: 研究发现，通过DTW clustering和k-means clustering方法可以分化不同的每天电力消耗和可再生能源生产特征，并且可以根据特定负荷特点设置优化的充电和V2G窗口，以提高智能电网的稳定性和可再生能源的使用率。这些发现对于实现可持续可靠的能源未来具有重要意义。<details>
<summary>Abstract</summary>
Exploring the convergence of electric vehicles (EVs), renewable energy, and smart grid technologies in the context of Texas, this study addresses challenges hindering the widespread adoption of EVs. Acknowledging their environmental benefits, the research focuses on grid stability concerns, uncoordinated charging patterns, and the complicated relationship between EVs and renewable energy sources. Dynamic time warping (DTW) clustering and k-means clustering methodologies categorize days based on total load and net load, offering nuanced insights into daily electricity consumption and renewable energy generation patterns. By establishing optimal charging and vehicle-to-grid (V2G) windows tailored to specific load characteristics, the study provides a sophisticated methodology for strategic decision-making in energy consumption and renewable integration. The findings contribute to the ongoing discourse on achieving a sustainable and resilient energy future through the seamless integration of EVs into smart grids.
</details>
<details>
<summary>摘要</summary>
研究electric vehicles (EVs)在德州的整合、可再生能源和智能网络技术方面进行了探索。本研究承认EVs具有环保的优点，但是对于网络稳定性、充电模式不协调和可再生能源源与EVs之间的复杂关系存在一些挑战。使用时间扭曲分 clustering和k-means分 clustering方法对每天的总负荷和网络负荷进行分类，从而提供了细化的每天电力消耗和可再生能源生产模式的洞察。通过确定最佳充电和汽车到网络（V2G）窗口，以适应特定的负荷特征，本研究提供了一种高级的决策方法，以便在能源消耗和可再生资源 интеграции方面做出策略决策。研究成果对于实现可持续可靠的能源未来做出了贡献。
</details></li>
</ul>
<hr>
<h2 id="Early-Detection-of-Tuberculosis-with-Machine-Learning-Cough-Audio-Analysis-Towards-More-Accessible-Global-Triaging-Usage"><a href="#Early-Detection-of-Tuberculosis-with-Machine-Learning-Cough-Audio-Analysis-Towards-More-Accessible-Global-Triaging-Usage" class="headerlink" title="Early Detection of Tuberculosis with Machine Learning Cough Audio Analysis: Towards More Accessible Global Triaging Usage"></a>Early Detection of Tuberculosis with Machine Learning Cough Audio Analysis: Towards More Accessible Global Triaging Usage</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17675">http://arxiv.org/abs/2310.17675</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chandra Suda</li>
<li>for: 这份研究旨在提高肺炎诊断，开发一个快速、可靠、易于存取的诊断工具。</li>
<li>methods: 这个研究使用了一种新型的机器学习架构，将智能手机的麦克风声音档案和人口普查资料进行分析，以检测肺炎。</li>
<li>results: 研究获得了88%的AUROC分布（世界卫生组织的诊断标准），比实际应用更高，并且可以在15秒内获得结果。<details>
<summary>Abstract</summary>
Tuberculosis (TB), a bacterial disease mainly affecting the lungs, is one of the leading infectious causes of mortality worldwide. To prevent TB from spreading within the body, which causes life-threatening complications, timely and effective anti-TB treatment is crucial. Cough, an objective biomarker for TB, is a triage tool that monitors treatment response and regresses with successful therapy. Current gold standards for TB diagnosis are slow or inaccessible, especially in rural areas where TB is most prevalent. In addition, current machine learning (ML) diagnosis research, like utilizing chest radiographs, is ineffective and does not monitor treatment progression. To enable effective diagnosis, an ensemble model was developed that analyzes, using a novel ML architecture, coughs' acoustic epidemiologies from smartphones' microphones to detect TB. The architecture includes a 2D-CNN and XGBoost that was trained on 724,964 cough audio samples and demographics from 7 countries. After feature extraction (Mel-spectrograms) and data augmentation (IR-convolution), the model achieved AUROC (area under the receiving operator characteristic) of 88%, surpassing WHO's requirements for screening tests. The results are available within 15 seconds and can easily be accessible via a mobile app. This research helps to improve TB diagnosis through a promising accurate, quick, and accessible triaging tool.
</details>
<details>
<summary>摘要</summary>
抑菌疾病（TB）是全球主要传染性疾病之一，主要影响肺部。在时间上采取有效抑菌治疗是关键，以防TB在身体内进一步扩散并导致生命威胁性的合并症。咳嗽是TB的对象生物标志，可以评估治疗效果，并随着成功治疗而逐渐下降。但目前的TB诊断标准过于慢或不可达，尤其是在乡村地区，TB的发病率最高。此外，目前的机器学习（ML）诊断研究，如使用胸部X射线，无法准确诊断TB。为了实现有效的诊断，我们开发了一个ensemble模型，利用智能手机麦克风的声音样本来检测TB。该模型包括2D-CNN和XGBoost，并在7个国家的724,964个咳嗽声音样本和人口统计数据上进行训练。 после特征提取（Mel-spectrograms）和数据增强（IR-convolution），模型达到了AUROC（区域下收益特征）的88%，超过世界卫生组织（WHO）的诊断测试标准。结果在15秒内可以获得，并可以通过移动应用程序访问。这项研究可以改善TB诊断，提供一个准确、快速、可 accessible的检测工具。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Rank-for-Active-Learning-via-Multi-Task-Bilevel-Optimization"><a href="#Learning-to-Rank-for-Active-Learning-via-Multi-Task-Bilevel-Optimization" class="headerlink" title="Learning to Rank for Active Learning via Multi-Task Bilevel Optimization"></a>Learning to Rank for Active Learning via Multi-Task Bilevel Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17044">http://arxiv.org/abs/2310.17044</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixin Ding, Si Chen, Ruoxi Jia, Yuxin Chen</li>
<li>for: 提高活动学习效率和可行性，降低标注成本。</li>
<li>methods: 提出了一种新的活动学习方法，通过学习的搅拌模型来选择未标注的实例。</li>
<li>results: 通过实验表明，使用我们的方法可以在标注成本高的情况下提高活动学习的效率和可行性。<details>
<summary>Abstract</summary>
Active learning is a promising paradigm to reduce the labeling cost by strategically requesting labels to improve model performance. However, existing active learning methods often rely on expensive acquisition function to compute, extensive modeling retraining and multiple rounds of interaction with annotators. To address these limitations, we propose a novel approach for active learning, which aims to select batches of unlabeled instances through a learned surrogate model for data acquisition. A key challenge in this approach is developing an acquisition function that generalizes well, as the history of data, which forms part of the utility function's input, grows over time. Our novel algorithmic contribution is a bilevel multi-task bilevel optimization framework that predicts the relative utility -- measured by the validation accuracy -- of different training sets, and ensures the learned acquisition function generalizes effectively. For cases where validation accuracy is expensive to evaluate, we introduce efficient interpolation-based surrogate models to estimate the utility function, reducing the evaluation cost. We demonstrate the performance of our approach through extensive experiments on standard active classification benchmarks. By employing our learned utility function, we show significant improvements over traditional techniques, paving the way for more efficient and effective utility maximization in active learning applications.
</details>
<details>
<summary>摘要</summary>
活动学习是一种有前途的思想，可以减少标注成本，通过策略性地请求标注，提高模型性能。然而，现有的活动学习方法经常依赖于贵重的获取函数来计算，需要广泛的模型重新训练和多轮与注解员的互动。为了解决这些限制，我们提出了一种新的活动学习方法，通过一个学习的代理模型来选择未标注的实例集。我们的新算法贡献是一种缓中多任务缓中优化框架，可以预测不同训练集的相对utilty值，并确保学习得到的获取函数可以广泛适用。在评估utilty值时成本高的情况下，我们引入了高效的 interpolate-based 代理模型，以便估计获取函数，降低评估成本。我们通过对标准的活动分类benchmark进行广泛的实验，证明了我们的方法的性能优势，开拓了更有效率的活动学习应用。
</details></li>
</ul>
<hr>
<h2 id="Quantum-Long-Short-Term-Memory-QLSTM-vs-Classical-LSTM-in-Time-Series-Forecasting-A-Comparative-Study-in-Solar-Power-Forecasting"><a href="#Quantum-Long-Short-Term-Memory-QLSTM-vs-Classical-LSTM-in-Time-Series-Forecasting-A-Comparative-Study-in-Solar-Power-Forecasting" class="headerlink" title="Quantum Long Short-Term Memory (QLSTM) vs Classical LSTM in Time Series Forecasting: A Comparative Study in Solar Power Forecasting"></a>Quantum Long Short-Term Memory (QLSTM) vs Classical LSTM in Time Series Forecasting: A Comparative Study in Solar Power Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17032">http://arxiv.org/abs/2310.17032</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saad Zafar Khan, Nazeefa Muzammil, Syed Mohammad Hassan Zaidi, Abdulah Jeza Aljohani, Haibat Khan, Salman Ghafoor</li>
<li>for: 预测太阳能生产的精准预测是现代可再生能源系统的关键。本研究 compare Quantum Long Short-Term Memory（QLSTM）和 классиcal Long Short-Term Memory（LSTM）模型在太阳能生产预测中的比较。</li>
<li>methods: 我们的控制实验表明，QLSTM具有加速训练收敛和在初始EPoch中显著降低测试损失的优势。这些实验证明QLSTM在处理复杂时间序关系方面具有潜在的优势。</li>
<li>results: 我们的实验结果表明，QLSTM在初始EPoch中的测试损失比 классиcal LSTM快了数个数量级。这表明QLSTM在处理复杂时间序关系方面具有潜在的优势。<details>
<summary>Abstract</summary>
Accurately forecasting solar power generation is crucial in the global progression towards sustainable energy systems. In this study, we conduct a meticulous comparison between Quantum Long Short-Term Memory (QLSTM) and classical Long Short-Term Memory (LSTM) models for solar power production forecasting. Our controlled experiments reveal promising advantages of QLSTMs, including accelerated training convergence and substantially reduced test loss within the initial epoch compared to classical LSTMs. These empirical findings demonstrate QLSTM's potential to swiftly assimilate complex time series relationships, enabled by quantum phenomena like superposition. However, realizing QLSTM's full capabilities necessitates further research into model validation across diverse conditions, systematic hyperparameter optimization, hardware noise resilience, and applications to correlated renewable forecasting problems. With continued progress, quantum machine learning can offer a paradigm shift in renewable energy time series prediction. This pioneering work provides initial evidence substantiating quantum advantages over classical LSTM, while acknowledging present limitations. Through rigorous benchmarking grounded in real-world data, our study elucidates a promising trajectory for quantum learning in renewable forecasting. Additional research and development can further actualize this potential to achieve unprecedented accuracy and reliability in predicting solar power generation worldwide.
</details>
<details>
<summary>摘要</summary>
Forecasting solar power generation accurately is crucial in the global transition to sustainable energy systems. In this study, we compare the Quantum Long Short-Term Memory (QLSTM) and classical Long Short-Term Memory (LSTM) models for solar power production forecasting. Our experiments show that QLSTMs have several advantages, such as faster training convergence and lower test loss within the initial epoch, compared to classical LSTMs. These findings demonstrate the potential of QLSTM to quickly learn complex time series relationships, thanks to quantum phenomena like superposition. However, further research is needed to validate the model across different conditions, optimize hyperparameters, and improve hardware noise resilience. Additionally, we need to explore the application of QLSTM to correlated renewable forecasting problems. With continued progress, quantum machine learning can offer a paradigm shift in renewable energy time series prediction. This study provides initial evidence of quantum advantages over classical LSTM, while acknowledging present limitations. Through rigorous benchmarking with real-world data, we elucidate a promising trajectory for quantum learning in renewable forecasting, paving the way for unprecedented accuracy and reliability in predicting solar power generation worldwide.
</details></li>
</ul>
<hr>
<h2 id="On-the-Identifiability-and-Interpretability-of-Gaussian-Process-Models"><a href="#On-the-Identifiability-and-Interpretability-of-Gaussian-Process-Models" class="headerlink" title="On the Identifiability and Interpretability of Gaussian Process Models"></a>On the Identifiability and Interpretability of Gaussian Process Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17023">http://arxiv.org/abs/2310.17023</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiawenchenn/gp_mixture_kernel">https://github.com/jiawenchenn/gp_mixture_kernel</a></li>
<li>paper_authors: Jiawen Chen, Wancen Mu, Yun Li, Didong Li</li>
<li>for: 本文探讨了在单输出 Gaussian Process（GP）模型中广泛使用的添加性杂合Mat&#39;ernkernel的做法，并研究了这种杂合kernel的性质。</li>
<li>methods: 作者在单输出和多输出GP模型中使用了不同的方法，包括deriving theoretical results和进行 simulations和实际应用。</li>
<li>results: 研究发现，在单输出情况下，杂合Mat&#39;ernkernel的熔炉性受到最不稳定的组件的限制，而GP模型具有这种杂合kernel的效果等同于最不稳定的kernel组件。此外，作者发现在多输出情况下，covariance matrix $A$是可Identifiable的，表明杂合kernel适合多输出任务。这些结论得到了实际应用和仔细的 simulations 的支持。<details>
<summary>Abstract</summary>
In this paper, we critically examine the prevalent practice of using additive mixtures of Mat\'ern kernels in single-output Gaussian process (GP) models and explore the properties of multiplicative mixtures of Mat\'ern kernels for multi-output GP models. For the single-output case, we derive a series of theoretical results showing that the smoothness of a mixture of Mat\'ern kernels is determined by the least smooth component and that a GP with such a kernel is effectively equivalent to the least smooth kernel component. Furthermore, we demonstrate that none of the mixing weights or parameters within individual kernel components are identifiable. We then turn our attention to multi-output GP models and analyze the identifiability of the covariance matrix $A$ in the multiplicative kernel $K(x,y) = AK_0(x,y)$, where $K_0$ is a standard single output kernel such as Mat\'ern. We show that $A$ is identifiable up to a multiplicative constant, suggesting that multiplicative mixtures are well suited for multi-output tasks. Our findings are supported by extensive simulations and real applications for both single- and multi-output settings. This work provides insight into kernel selection and interpretation for GP models, emphasizing the importance of choosing appropriate kernel structures for different tasks.
</details>
<details>
<summary>摘要</summary>
在本文中，我们critically examines the prevailing practice of using additive mixtures of Matérn kernels in single-output Gaussian process (GP) models, and explore the properties of multiplicative mixtures of Matérn kernels for multi-output GP models. For the single-output case, we derive a series of theoretical results showing that the smoothness of a mixture of Matérn kernels is determined by the least smooth component, and that a GP with such a kernel is effectively equivalent to the least smooth kernel component. Furthermore, we demonstrate that none of the mixing weights or parameters within individual kernel components are identifiable. We then turn our attention to multi-output GP models and analyze the identifiability of the covariance matrix $A$ in the multiplicative kernel $K(x,y) = AK_0(x,y)$, where $K_0$ is a standard single output kernel such as Matérn. We show that $A$ is identifiable up to a multiplicative constant, suggesting that multiplicative mixtures are well suited for multi-output tasks. Our findings are supported by extensive simulations and real applications for both single- and multi-output settings. This work provides insight into kernel selection and interpretation for GP models, emphasizing the importance of choosing appropriate kernel structures for different tasks.
</details></li>
</ul>
<hr>
<h2 id="Streaming-Factor-Trajectory-Learning-for-Temporal-Tensor-Decomposition"><a href="#Streaming-Factor-Trajectory-Learning-for-Temporal-Tensor-Decomposition" class="headerlink" title="Streaming Factor Trajectory Learning for Temporal Tensor Decomposition"></a>Streaming Factor Trajectory Learning for Temporal Tensor Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17021">http://arxiv.org/abs/2310.17021</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xuangu-fang/streaming-factor-trajectory-learning">https://github.com/xuangu-fang/streaming-factor-trajectory-learning</a></li>
<li>paper_authors: Shikai Fang, Xin Yu, Shibo Li, Zheng Wang, Robert Kirby, Shandian Zhe</li>
<li>for: 这篇论文是为了解决实际应用中的流动数据问题，即如何有效地捕捉流动数据中对象的表示的时间演化。</li>
<li>methods: 该论文提出了流动因子轨迹学习（SFTL）方法，使用 Gaussian processes（GPs）来模型因子轨迹的时间演化，并通过将 GPs 转换为状态空间假设来处理流动数据的计算挑战。</li>
<li>results: 论文通过 sintetic tasks 和实际应用示例表明了 SFTL 的优势，可以有效地捕捉流动数据中对象的表示时间演化，并且可以在流动数据处理中实现标准的 Rauch-Tung-Striebel 平滑。<details>
<summary>Abstract</summary>
Practical tensor data is often along with time information. Most existing temporal decomposition approaches estimate a set of fixed factors for the objects in each tensor mode, and hence cannot capture the temporal evolution of the objects' representation. More important, we lack an effective approach to capture such evolution from streaming data, which is common in real-world applications. To address these issues, we propose Streaming Factor Trajectory Learning (SFTL) for temporal tensor decomposition. We use Gaussian processes (GPs) to model the trajectory of factors so as to flexibly estimate their temporal evolution. To address the computational challenges in handling streaming data, we convert the GPs into a state-space prior by constructing an equivalent stochastic differential equation (SDE). We develop an efficient online filtering algorithm to estimate a decoupled running posterior of the involved factor states upon receiving new data. The decoupled estimation enables us to conduct standard Rauch-Tung-Striebel smoothing to compute the full posterior of all the trajectories in parallel, without the need for revisiting any previous data. We have shown the advantage of SFTL in both synthetic tasks and real-world applications.
</details>
<details>
<summary>摘要</summary>
实际tensor数据经常同时间信息一起出现。现有的 temporal decomposition方法大多Estimate a set of fixed factors for the objects in each tensor mode, 因此无法捕捉对象表示的 temporal evolution。更重要的是，我们缺乏有效的方法来从流动数据中捕捉这种演化。为了解决这些问题，我们提出了Streaming Factor Trajectory Learning (SFTL) temporal tensor decomposition方法。我们使用 Gaussian processes (GPs) 来模型因子的轨迹，以便灵活地估计其时间演化。为了处理流动数据的计算挑战，我们将GPs转换为状态空间假设，并构建了相应的随机演化方程(SDE)。我们开发了高效的在线筛选算法，以便在接收新数据时估计一个协调运行的因子状态。这种协调估计使我们可以在并行计算中对所有轨迹进行标准的Rauch-Tung-Striebel平滑，而不需要再次访问任何之前的数据。我们在 synthetic tasks 和实际应用中都表明了SFTL的优势。
</details></li>
</ul>
<hr>
<h2 id="Simulation-based-stacking"><a href="#Simulation-based-stacking" class="headerlink" title="Simulation based stacking"></a>Simulation based stacking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17009">http://arxiv.org/abs/2310.17009</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bregaldo/simulation_based_stacking">https://github.com/bregaldo/simulation_based_stacking</a></li>
<li>paper_authors: Yuling Yao, Bruno Régaldo-Saint Blancard, Justin Domke</li>
<li>for: 这个论文旨在提出一种总结多个 posterior approximation 的核心框架，以提高 Bayesian 计算的精度和可靠性。</li>
<li>methods: 该论文使用了多种 inference algorithm 和 architecture，并利用了随机初始化和梯度的Randomness，以获得多个 posterior approximation。</li>
<li>results: 该论文通过对多个 benchmark  simulations 和一个具有挑战性的 cosmological inference 任务进行示例，证明了该框架的 asymptotic guarantee 和多个 posterior approximation 的合理性。<details>
<summary>Abstract</summary>
Simulation-based inference has been popular for amortized Bayesian computation. It is typical to have more than one posterior approximation, from different inference algorithms, different architectures, or simply the randomness of initialization and stochastic gradients. With a provable asymptotic guarantee, we present a general stacking framework to make use of all available posterior approximations. Our stacking method is able to combine densities, simulation draws, confidence intervals, and moments, and address the overall precision, calibration, coverage, and bias at the same time. We illustrate our method on several benchmark simulations and a challenging cosmological inference task.
</details>
<details>
<summary>摘要</summary>
模拟基于推理已经广泛应用于权重 bayesian 计算中。通常有多个 posterior  aproximation，来自不同的推理算法、不同的架构或协同初始化和随机梯度的Randomness。我们提供一种通用的堆叠框架，可以利用所有可用的 posterior approximations。我们的堆叠方法可以将density、实验取样、信任范围和幂等元素组合起来，同时Addressing 总精度、准确性、覆盖率和偏见问题。我们在一些标准的benchmark simulations和一个复杂的 cosmological inference task中进行了示例。
</details></li>
</ul>
<hr>
<h2 id="Faster-Recalibration-of-an-Online-Predictor-via-Approachability"><a href="#Faster-Recalibration-of-an-Online-Predictor-via-Approachability" class="headerlink" title="Faster Recalibration of an Online Predictor via Approachability"></a>Faster Recalibration of an Online Predictor via Approachability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17002">http://arxiv.org/abs/2310.17002</a></li>
<li>repo_url: None</li>
<li>paper_authors: Princewill Okoroafor, Robert Kleinberg, Wen Sun</li>
<li>for: 提高在线预测模型的可靠性和可信度，特别是在对象输出序列可能会被敌意攻击的情况下。</li>
<li>methods: 使用黑威尔的可达性定理来转换不准确的在线预测模型，以实现更好的准确率和折补率。</li>
<li>results: 提出了一种新的算法，可以在在线预测设置下实现更快的准确率和折补率，并且可以静态地控制折补率和准确率之间的平衡。<details>
<summary>Abstract</summary>
Predictive models in ML need to be trustworthy and reliable, which often at the very least means outputting calibrated probabilities. This can be particularly difficult to guarantee in the online prediction setting when the outcome sequence can be generated adversarially. In this paper we introduce a technique using Blackwell's approachability theorem for taking an online predictive model which might not be calibrated and transforming its predictions to calibrated predictions without much increase to the loss of the original model. Our proposed algorithm achieves calibration and accuracy at a faster rate than existing techniques arXiv:1607.03594 and is the first algorithm to offer a flexible tradeoff between calibration error and accuracy in the online setting. We demonstrate this by characterizing the space of jointly achievable calibration and regret using our technique.
</details>
<details>
<summary>摘要</summary>
Machine learning 预测模型需要可靠和可信，通常至少表示输出加工。但在在线预测设置下，结果序列可能会被反对抗性生成，这可能使得保证预测的准确性很难。在这篇论文中，我们介绍了一种使用黑威尔的可达性定理来将一个可能不准确的在线预测模型转换成准确预测，而无需增加原始模型的损失。我们的提议算法可以快速实现折衔和准确性的平衡，并且是现有技术arXiv:1607.03594中的第一个可以进行折衔和准确性的flexible tradeoff的算法。我们通过characterizing the jointly achievable calibration and regret space来证明这一点。
</details></li>
</ul>
<hr>
<h2 id="Towards-Continually-Learning-Application-Performance-Models"><a href="#Towards-Continually-Learning-Application-Performance-Models" class="headerlink" title="Towards Continually Learning Application Performance Models"></a>Towards Continually Learning Application Performance Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16996">http://arxiv.org/abs/2310.16996</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ray A. O. Sinurat, Anurag Daram, Haryadi S. Gunawi, Robert B. Ross, Sandeep Madireddy</li>
<li>for: 本研究旨在开发一种能够考虑数据分布变化的机器学习性性能模型，以便在生产级HPC系统中进行重要的任务调度和应用优化决策。</li>
<li>methods: 本研究使用了随着时间的推移逐渐学习的方法，以抵消数据分布的变化对性能模型的影响。此外，我们还采用了一种叫做归并学习的技术，以避免训练过程中的违和强化现象。</li>
<li>results: 我们的最佳模型能够保持准确性，即使面临新的数据分布变化，同时在整个数据序列中的预测精度比预期方法提高了2倍。<details>
<summary>Abstract</summary>
Machine learning-based performance models are increasingly being used to build critical job scheduling and application optimization decisions. Traditionally, these models assume that data distribution does not change as more samples are collected over time. However, owing to the complexity and heterogeneity of production HPC systems, they are susceptible to hardware degradation, replacement, and/or software patches, which can lead to drift in the data distribution that can adversely affect the performance models. To this end, we develop continually learning performance models that account for the distribution drift, alleviate catastrophic forgetting, and improve generalizability. Our best model was able to retain accuracy, regardless of having to learn the new distribution of data inflicted by system changes, while demonstrating a 2x improvement in the prediction accuracy of the whole data sequence in comparison to the naive approach.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Probabilistic-Integral-Circuits"><a href="#Probabilistic-Integral-Circuits" class="headerlink" title="Probabilistic Integral Circuits"></a>Probabilistic Integral Circuits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16986">http://arxiv.org/abs/2310.16986</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gennaro Gala, Cassio de Campos, Robert Peharz, Antonio Vergari, Erik Quaeghebeur</li>
<li>for: 这篇论文旨在探讨连续隐变量（Continuous Latent Variables，简称LV）和概率Circuit（PC）两种模型之间的桥接，以及它们之间的一致性。</li>
<li>methods: 该论文提出了一种新的计算图语言——概率积分电路（Probabilistic Integral Circuits，简称PIC），它将PC的积分单元扩展到连续隐变量，从而实现了PC的扩展和改进。PIC使用 симвоlic计算图，可以在一些简单的情况下进行analytical integration，并且可以通过光量神经网络来参数化。</li>
<li>results: 在多个分布估计 benchmark 上，PIC-approximating PCs 系统性地超越了常见的PCs，它们通常通过 expectation-maximization 或 SGD 来学习。这表明PIC可以在连续隐变量模型中实现PC的 tractability 和表达能力。<details>
<summary>Abstract</summary>
Continuous latent variables (LVs) are a key ingredient of many generative models, as they allow modelling expressive mixtures with an uncountable number of components. In contrast, probabilistic circuits (PCs) are hierarchical discrete mixtures represented as computational graphs composed of input, sum and product units. Unlike continuous LV models, PCs provide tractable inference but are limited to discrete LVs with categorical (i.e. unordered) states. We bridge these model classes by introducing probabilistic integral circuits (PICs), a new language of computational graphs that extends PCs with integral units representing continuous LVs. In the first place, PICs are symbolic computational graphs and are fully tractable in simple cases where analytical integration is possible. In practice, we parameterise PICs with light-weight neural nets delivering an intractable hierarchical continuous mixture that can be approximated arbitrarily well with large PCs using numerical quadrature. On several distribution estimation benchmarks, we show that such PIC-approximating PCs systematically outperform PCs commonly learned via expectation-maximization or SGD.
</details>
<details>
<summary>摘要</summary>
PICs are symbolic computational graphs and are fully tractable in simple cases where analytical integration is possible. In practice, we parameterize PICs with lightweight neural nets, delivering an intractable hierarchical continuous mixture that can be approximated arbitrarily well with large PCs using numerical quadrature. On several distribution estimation benchmarks, we show that such PIC-approximating PCs systematically outperform PCs commonly learned via expectation-maximization or SGD.
</details></li>
</ul>
<hr>
<h2 id="Reimagining-Synthetic-Tabular-Data-Generation-through-Data-Centric-AI-A-Comprehensive-Benchmark"><a href="#Reimagining-Synthetic-Tabular-Data-Generation-through-Data-Centric-AI-A-Comprehensive-Benchmark" class="headerlink" title="Reimagining Synthetic Tabular Data Generation through Data-Centric AI: A Comprehensive Benchmark"></a>Reimagining Synthetic Tabular Data Generation through Data-Centric AI: A Comprehensive Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16981">http://arxiv.org/abs/2310.16981</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vanderschaarlab/data-centric-synthetic-data">https://github.com/vanderschaarlab/data-centric-synthetic-data</a></li>
<li>paper_authors: Lasse Hansen, Nabeel Seedat, Mihaela van der Schaar, Andrija Petrovic</li>
<li>for: 提高机器学习模型的训练数据质量和效果。</li>
<li>methods: 结合数据中心AI技术，对数据进行 profiling，以准确反映实际数据的复杂特征。</li>
<li>results: 对 eleven 个不同的表格数据集进行实验，发现现有的生成方法具有一定的局限性和不足，并提出了实践建议，以提高生成数据的质量和效果。<details>
<summary>Abstract</summary>
Synthetic data serves as an alternative in training machine learning models, particularly when real-world data is limited or inaccessible. However, ensuring that synthetic data mirrors the complex nuances of real-world data is a challenging task. This paper addresses this issue by exploring the potential of integrating data-centric AI techniques which profile the data to guide the synthetic data generation process. Moreover, we shed light on the often ignored consequences of neglecting these data profiles during synthetic data generation -- despite seemingly high statistical fidelity. Subsequently, we propose a novel framework to evaluate the integration of data profiles to guide the creation of more representative synthetic data. In an empirical study, we evaluate the performance of five state-of-the-art models for tabular data generation on eleven distinct tabular datasets. The findings offer critical insights into the successes and limitations of current synthetic data generation techniques. Finally, we provide practical recommendations for integrating data-centric insights into the synthetic data generation process, with a specific focus on classification performance, model selection, and feature selection. This study aims to reevaluate conventional approaches to synthetic data generation and promote the application of data-centric AI techniques in improving the quality and effectiveness of synthetic data.
</details>
<details>
<summary>摘要</summary>
人工数据作为机器学习模型训练时的替代方案，特别是当实际世界数据scarce或difficult to access时。然而，确保人工数据准确反映实际世界数据的复杂特点是一项挑战。这篇论文通过探讨integrating data-centric AI技术，以 Profiling the data to guide the synthetic data generation process。此外，我们还探讨在不考虑这些数据Profile during synthetic data generation时所忽略的后果--尽管似乎具有高度的统计准确性。随后，我们提出了一种新的评估框架，用于评估数据Profile的集成，以创建更代表性的人工数据。在一项实验研究中，我们评估了五种当今最佳实践的 tabular data生成模型在 eleven 个不同的 tabular 数据集上。发现的结果提供了关键的洞察，揭示了当前人工数据生成技术的成功和局限性。最后，我们提供了实践oriented的建议，用于在人工数据生成过程中integrating data-centric AI技术，特别是关于分类性能、模型选择和特征选择。本研究的目标是重新评估现有的人工数据生成方法，并促进data-centric AI技术在提高人工数据质量和效果方面的应用。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Neural-Network-Approaches-for-Conditional-Optimal-Transport-with-Applications-in-Bayesian-Inference"><a href="#Efficient-Neural-Network-Approaches-for-Conditional-Optimal-Transport-with-Applications-in-Bayesian-Inference" class="headerlink" title="Efficient Neural Network Approaches for Conditional Optimal Transport with Applications in Bayesian Inference"></a>Efficient Neural Network Approaches for Conditional Optimal Transport with Applications in Bayesian Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16975">http://arxiv.org/abs/2310.16975</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/emorymlip/pcp-map">https://github.com/emorymlip/pcp-map</a></li>
<li>paper_authors: Zheyu Oliver Wang, Ricardo Baptista, Youssef Marzouk, Lars Ruthotto, Deepanshu Verma</li>
<li>for: 这两种神经网络方法用于解决静态和动态conditional optimal transport（COT）问题，以便实现样本和概率分布估计，这些任务是 bayesian inference 的核心任务。</li>
<li>methods: 这两种方法都是基于 measure transport 框架，将目标 conditional distribution 表示为一个可追踪的参考分布的变换。 COT 图是这个框架中的一个可能性，具有uniqueness 和 monotonicity 的优点。然而，相关的 COT 问题在 moderate 维度时 computationally challenging。为了提高可扩展性，我们的数学算法利用神经网络来参数化 COT 图。</li>
<li>results: 我们的方法比 state-of-the-art 方法更高效和更准确，可以在 benchmark 数据集和 bayesian inverse problem 中进行证明。 PCP-Map 模型将 conditional transport 图表示为 partially input convex neural network（PICNN）的梯度，并使用一种新的数学实现来提高计算效率。 COT-Flow 模型则使用一种含杂化的神经网络 ODE 来表示 conditional transport，它在训练时 slower，但在 sampling 时 faster。<details>
<summary>Abstract</summary>
We present two neural network approaches that approximate the solutions of static and dynamic conditional optimal transport (COT) problems, respectively. Both approaches enable sampling and density estimation of conditional probability distributions, which are core tasks in Bayesian inference. Our methods represent the target conditional distributions as transformations of a tractable reference distribution and, therefore, fall into the framework of measure transport. COT maps are a canonical choice within this framework, with desirable properties such as uniqueness and monotonicity. However, the associated COT problems are computationally challenging, even in moderate dimensions. To improve the scalability, our numerical algorithms leverage neural networks to parameterize COT maps. Our methods exploit the structure of the static and dynamic formulations of the COT problem. PCP-Map models conditional transport maps as the gradient of a partially input convex neural network (PICNN) and uses a novel numerical implementation to increase computational efficiency compared to state-of-the-art alternatives. COT-Flow models conditional transports via the flow of a regularized neural ODE; it is slower to train but offers faster sampling. We demonstrate their effectiveness and efficiency by comparing them with state-of-the-art approaches using benchmark datasets and Bayesian inverse problems.
</details>
<details>
<summary>摘要</summary>
我们提出了两种神经网络方法，用于近似静态和动态条件最优运输（COT）问题的解决方案。这两种方法允许采样和概率分布估计，这些任务是泛型推理中核心任务之一。我们的方法将目标 conditional distribution 表示为一个可迭代的参考分布变换，因此属于度量运输框架。 COT 图是这种框架中的一个可能性，具有uniqueness 和 monotonicity 的感知性。然而，相关的 COT 问题在moderate 维度下 computationally 挑战。为了提高可扩展性，我们的数字算法使用神经网络来参数化 COT 图。我们的方法利用静态和动态 COT 问题的结构。 PCP-Map 模型 conditional transport 图像为 partially input convex neural network（PICNN）的梯度，并使用一种新的数字实现以提高计算效率相比之前的状态艺术。 COT-Flow 模型 conditional transport 通过一个正则化神经 ODE 的流动来实现，它在训练 slower 但在样本 faster 。我们通过对比 benchmark 数据和推理 inverse 问题来证明它们的有效性和效率。
</details></li>
</ul>
<hr>
<h2 id="Privately-Aligning-Language-Models-with-Reinforcement-Learning"><a href="#Privately-Aligning-Language-Models-with-Reinforcement-Learning" class="headerlink" title="Privately Aligning Language Models with Reinforcement Learning"></a>Privately Aligning Language Models with Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16960">http://arxiv.org/abs/2310.16960</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fan Wu, Huseyin A. Inan, Arturs Backurs, Varun Chandrasekaran, Janardhan Kulkarni, Robert Sim</li>
<li>for: 本研究旨在采用异谱隐私（DP）和强化学习（RL）对大型自然语言模型（LLM）进行适应性调整，以提高模型的听说能力和隐私保护。</li>
<li>methods: 本研究使用了Ziegler等人（2020）提出的两种主要方法：一是通过RL无人Loop（例如正面评价生成）进行调整，二是通过RL人反馈（RLHF）（例如 SUMMARIZATION 在人类偏好的方式）。我们提供了一个新的DP框架来实现这两种方法的适应性调整，并证明了其正确性。</li>
<li>results: 我们的实验结果证明了我们的方法的有效性，即可以提供竞争力强的实用性while ensuring strong privacy protections。<details>
<summary>Abstract</summary>
Positioned between pre-training and user deployment, aligning large language models (LLMs) through reinforcement learning (RL) has emerged as a prevailing strategy for training instruction following-models such as ChatGPT. In this work, we initiate the study of privacy-preserving alignment of LLMs through Differential Privacy (DP) in conjunction with RL. Following the influential work of Ziegler et al. (2020), we study two dominant paradigms: (i) alignment via RL without human in the loop (e.g., positive review generation) and (ii) alignment via RL from human feedback (RLHF) (e.g., summarization in a human-preferred way). We give a new DP framework to achieve alignment via RL, and prove its correctness. Our experimental results validate the effectiveness of our approach, offering competitive utility while ensuring strong privacy protections.
</details>
<details>
<summary>摘要</summary>
位于预训练和用户部署之间，通过强化学习（RL）来对大语言模型（LLM）进行对齐已成为训练指令遵从模型如ChatGPT的主要策略。在这项工作中，我们开始研究保护隐私的LLM对齐方法，通过强化学习和隐私保护（DP）。根据茅利等人（2020）的 influential work，我们研究两种主导方法：（i）通过RL无人参与（例如， Positive Review生成）和（ii）通过RL从人类反馈（RLHF，例如，人类偏好的概要）。我们提出了一个新的DP框架来实现LLM对齐，并证明其正确性。我们的实验结果证明了我们的方法的有效性，具有竞争的实用性并保护强大隐私权。
</details></li>
</ul>
<hr>
<h2 id="Improving-Few-shot-Generalization-of-Safety-Classifiers-via-Data-Augmented-Parameter-Efficient-Fine-Tuning"><a href="#Improving-Few-shot-Generalization-of-Safety-Classifiers-via-Data-Augmented-Parameter-Efficient-Fine-Tuning" class="headerlink" title="Improving Few-shot Generalization of Safety Classifiers via Data Augmented Parameter-Efficient Fine-Tuning"></a>Improving Few-shot Generalization of Safety Classifiers via Data Augmented Parameter-Efficient Fine-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16959">http://arxiv.org/abs/2310.16959</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ananth Balashankar, Xiao Ma, Aradhana Sinha, Ahmad Beirami, Yao Qin, Jilin Chen, Alex Beutel</li>
<li>for: 这 paper 探讨了 LLM 在新的安全问题和政策出现后，如何建立检测违反规则的分类器。</li>
<li>methods: 这 paper 使用了域总化少数例学习来解决 LLM 在新的安全问题上建立分类器。</li>
<li>results: 这 paper 实验表明，相比于优化学习和示例选择，参数高效调整（PEFT）和类例基于的数据增强（DAPT）方法可以在新的安全规则下提高分类器的性能，具体提高了 Social Chemistry 道德判断和 Toxicity 检测任务中的 F1 分数和 AUC 值。<details>
<summary>Abstract</summary>
As large language models (LLMs) are widely adopted, new safety issues and policies emerge, to which existing safety classifiers do not generalize well. If we have only observed a few examples of violations of a new safety rule, how can we build a classifier to detect violations? In this paper, we study the novel setting of domain-generalized few-shot learning for LLM-based text safety classifiers. Unlike prior few-shot work, these new safety issues can be hard to uncover and we do not get to choose the few examples. We demonstrate that existing few-shot techniques do not perform well in this setting, and rather we propose to do parameter-efficient fine-tuning (PEFT) combined with augmenting training data based on similar examples in prior existing rules. We empirically show that our approach of similarity-based data-augmentation + prompt-tuning (DAPT) consistently outperforms baselines that either do not rely on data augmentation or on PEFT by 7-17% F1 score in the Social Chemistry moral judgement and 9-13% AUC in the Toxicity detection tasks, even when the new rule is loosely correlated with existing ones.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的广泛采用导致新的安全问题和政策出现，现有的安全分类器不能通用。如果我们只有几个例外新安全规则的违反示例，如何建立一个检测违反的分类器？在这篇论文中，我们研究了域总化几个步骤的文本安全分类器。不同于先前的几个步骤工作，这些新的安全问题可能很难发现，我们不能选择几个示例。我们表明，现有的几个步骤技术不适用于这种设定，而是提议使用效率高的参数调整（PEFT）和基于先前的规则相似例子的数据增强（DAPT）。我们实际证明，我们的方法在社交化学道德评价和攻击性识别任务中表现出了7-17%的F1分和9-13%的AUC提升，即使新规则与现有规则之间存在潜在的相互关系。
</details></li>
</ul>
<hr>
<h2 id="Transferring-a-molecular-foundation-model-for-polymer-property-predictions"><a href="#Transferring-a-molecular-foundation-model-for-polymer-property-predictions" class="headerlink" title="Transferring a molecular foundation model for polymer property predictions"></a>Transferring a molecular foundation model for polymer property predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16958">http://arxiv.org/abs/2310.16958</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pei Zhang, Logan Kearney, Debsindhu Bhowmik, Zachary Fox, Amit K. Naskar, John Gounley</li>
<li>for: 加速设计优化，如药品开发和材料发现</li>
<li>methods: 使用对小分子进行预训的Transformer型语言模型，并在这些模型上进行精细调整，以估计聚合物性能</li>
<li>results: 使用这种方法可以获得与对增强聚合物数据进行增强调整的比较类似的准确性，但是无需进行耗时的数据增强。<details>
<summary>Abstract</summary>
Transformer-based large language models have remarkable potential to accelerate design optimization for applications such as drug development and materials discovery. Self-supervised pretraining of transformer models requires large-scale datasets, which are often sparsely populated in topical areas such as polymer science. State-of-the-art approaches for polymers conduct data augmentation to generate additional samples but unavoidably incurs extra computational costs. In contrast, large-scale open-source datasets are available for small molecules and provide a potential solution to data scarcity through transfer learning. In this work, we show that using transformers pretrained on small molecules and fine-tuned on polymer properties achieve comparable accuracy to those trained on augmented polymer datasets for a series of benchmark prediction tasks.
</details>
<details>
<summary>摘要</summary>
使用基于转换器的大型自然语言模型可以快速加速设计优化，如药物开发和材料发现。自我超级vised pretraining转换器模型需要大规模数据集，但材料科学领域中这些数据集 часто是罕见的。现状的方法是对材料进行数据扩充来生成更多样本，但这会添加额外的计算成本。相反，小分子领域有大规模的开源数据集可供使用，这提供了数据缺乏的解决方案通过传输学习。在这项工作中，我们显示了使用基于小分子的转换器模型，并将其精度调整为聚合物性能可以达到与数据扩充后的转换器模型相同的准确性水平。</SYS>Here's the translation in Simplified Chinese:使用基于转换器的大型自然语言模型可以快速加速设计优化，如药物开发和材料发现。自我超级vised pretraining转换器模型需要大规模数据集，但材料科学领域中这些数据集 часто是罕见的。现状的方法是对材料进行数据扩充来生成更多样本，但这会添加额外的计算成本。相反，小分子领域有大规模的开源数据集可供使用，这提供了数据缺乏的解决方案通过传输学习。在这项工作中，我们显示了使用基于小分子的转换器模型，并将其精度调整为聚合物性能可以达到与数据扩充后的转换器模型相同的准确性水平。
</details></li>
</ul>
<hr>
<h2 id="Break-it-Imitate-it-Fix-it-Robustness-by-Generating-Human-Like-Attacks"><a href="#Break-it-Imitate-it-Fix-it-Robustness-by-Generating-Human-Like-Attacks" class="headerlink" title="Break it, Imitate it, Fix it: Robustness by Generating Human-Like Attacks"></a>Break it, Imitate it, Fix it: Robustness by Generating Human-Like Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16955">http://arxiv.org/abs/2310.16955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aradhana Sinha, Ananth Balashankar, Ahmad Beirami, Thi Avrahami, Jilin Chen, Alex Beutel</li>
<li>for: 这个论文的目的是提高自然语言处理系统对人类敌对者的Robustness。</li>
<li>methods: 这个论文使用有限的人类敌对例进行对抗训练，以生成更多的有用敌对例。</li>
<li>results: 对ANLI和仇恨言语检测 benchmark数据集进行训练，相比只训练 observed human attacks，同时训练 synthetic adversarial examples，可以提高模型对未来人类敌对者的Robustness。<details>
<summary>Abstract</summary>
Real-world natural language processing systems need to be robust to human adversaries. Collecting examples of human adversaries for training is an effective but expensive solution. On the other hand, training on synthetic attacks with small perturbations - such as word-substitution - does not actually improve robustness to human adversaries. In this paper, we propose an adversarial training framework that uses limited human adversarial examples to generate more useful adversarial examples at scale. We demonstrate the advantages of this system on the ANLI and hate speech detection benchmark datasets - both collected via an iterative, adversarial human-and-model-in-the-loop procedure. Compared to training only on observed human attacks, also training on our synthetic adversarial examples improves model robustness to future rounds. In ANLI, we see accuracy gains on the current set of attacks (44.1%$\,\to\,$50.1%) and on two future unseen rounds of human generated attacks (32.5%$\,\to\,$43.4%, and 29.4%$\,\to\,$40.2%). In hate speech detection, we see AUC gains on current attacks (0.76 $\to$ 0.84) and a future round (0.77 $\to$ 0.79). Attacks from methods that do not learn the distribution of existing human adversaries, meanwhile, degrade robustness.
</details>
<details>
<summary>摘要</summary>
现实世界的自然语言处理系统需要强健于人类黑客。收集人类黑客的示例用于训练是一种有效的 pero 昂贵的解决方案。然而，使用小偏移量的Synthetic攻击训练并不实际提高对人类黑客的Robustness。在这篇论文中，我们提出了一种基于有限的人类黑客示例的对抗训练框架。我们示示了这种系统在ANLI和仇恨言语检测 benchmark 数据集上的优势。与仅仅训练在观察到的人类攻击的情况相比，我们的对抗训练还能提高模型对未来征的Robustness。在 ANLI 中，我们看到了当前攻击的准确率提高（44.1% $\to$ 50.1%），以及未来两个未见的人类生成的攻击（32.5% $\to$ 43.4%，和 29.4% $\to$ 40.2%）。在仇恨言语检测中，我们看到了投用率提高（0.76 $\to$ 0.84）以及未来一个round（0.77 $\to$ 0.79）。而不学习现有人类黑客的分布的攻击方法则会降低Robustness。
</details></li>
</ul>
<hr>
<h2 id="Causal-Q-Aggregation-for-CATE-Model-Selection"><a href="#Causal-Q-Aggregation-for-CATE-Model-Selection" class="headerlink" title="Causal Q-Aggregation for CATE Model Selection"></a>Causal Q-Aggregation for CATE Model Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16945">http://arxiv.org/abs/2310.16945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hui Lan, Vasilis Syrgkanis</li>
<li>for: 该论文的目的是提出一种新的 conditional average treatment effect（CATE）模型选择方法，以便实现个性化决策。</li>
<li>methods: 该论文使用了 proxy loss metrics with double robust properties 和 model ensembling，并提出了一种基于 Q-aggregation 的新的 CATE 模型选择方法。</li>
<li>results: 该论文的主要结果表明，该新方法可以 дости到 statistically optimal oracle model selection regret rates of $\frac{\log(M)}{n}$，并且不需要任何候选 CATE 模型都需要接近真实值。<details>
<summary>Abstract</summary>
Accurate estimation of conditional average treatment effects (CATE) is at the core of personalized decision making. While there is a plethora of models for CATE estimation, model selection is a nontrivial task, due to the fundamental problem of causal inference. Recent empirical work provides evidence in favor of proxy loss metrics with double robust properties and in favor of model ensembling. However, theoretical understanding is lacking. Direct application of prior theoretical work leads to suboptimal oracle model selection rates due to the non-convexity of the model selection problem. We provide regret rates for the major existing CATE ensembling approaches and propose a new CATE model ensembling approach based on Q-aggregation using the doubly robust loss. Our main result shows that causal Q-aggregation achieves statistically optimal oracle model selection regret rates of $\frac{\log(M)}{n}$ (with $M$ models and $n$ samples), with the addition of higher-order estimation error terms related to products of errors in the nuisance functions. Crucially, our regret rate does not require that any of the candidate CATE models be close to the truth. We validate our new method on many semi-synthetic datasets and also provide extensions of our work to CATE model selection with instrumental variables and unobserved confounding.
</details>
<details>
<summary>摘要</summary>
“精确估计 conditional average treatment effect (CATE) 是个核心的人性化决策问题。尽管有许多 CATE 估计模型，但选择这些模型是一个非常困难的任务，因为 causal inference 的基本问题。latest empirical work 表明，使用 proxy loss metrics with double robust properties 和 model ensembling 可以提高 CATE 估计的精度。然而，理论上的理解是lacking。直接运用先前的理论工作将会导致 suboptimal oracle model selection rates，因为 model selection 问题是非断的。我们提供了 existing CATE ensembling 方法中的 regret rates，并提出了一个基于 Q-aggregation 的新 CATE model ensembling 方法，使用 doubly robust loss。我们的主要结果表明，causal Q-aggregation 可以 дости得 statistically optimal oracle model selection regret rates of $\frac{\log(M)}{n}$（with $M$ models and $n$ samples），并且添加了高阶 estimation error terms related to products of errors in the nuisance functions。其中，我们的 regret rate 不需要任何 candidate CATE 模型都需要接近 truth。我们验证了我们的新方法在许多 semi-synthetic 数据上，并且提供了 CATE model selection with instrumental variables 和 unobserved confounding 的扩展。”
</details></li>
</ul>
<hr>
<h2 id="Exploring-Behavior-Discovery-Methods-for-Heterogeneous-Swarms-of-Limited-Capability-Robots"><a href="#Exploring-Behavior-Discovery-Methods-for-Heterogeneous-Swarms-of-Limited-Capability-Robots" class="headerlink" title="Exploring Behavior Discovery Methods for Heterogeneous Swarms of Limited-Capability Robots"></a>Exploring Behavior Discovery Methods for Heterogeneous Swarms of Limited-Capability Robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16941">http://arxiv.org/abs/2310.16941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Connor Mattson, Jeremy C. Clark, Daniel S. Brown</li>
<li>for: 本研究旨在探讨功能不同的机器人群体中可能出现的新行为。</li>
<li>methods: 本研究使用了 novelties 搜索和聚类来找到新的 emergent 行为。</li>
<li>results: 研究发现，先前的方法无法找到许多有趣的行为，而人类在Loop 的探索过程可以找到更多的行为。总共发现了 23 个 emergent 行为，其中 18 个是新发现的。这些行为是 computation-free 代理人群体中的首次发现。Here’s the same information in Simplified Chinese:</li>
<li>for: 本研究旨在探讨功能不同的机器人群体中可能出现的新行为。</li>
<li>methods: 本研究使用了 novelties 搜索和聚类来找到新的 emergent 行为。</li>
<li>results: 研究发现，先前的方法无法找到许多有趣的行为，而人类在Loop 的探索过程可以找到更多的行为。总共发现了 23 个 emergent 行为，其中 18 个是新发现的。这些行为是 computation-free 代理人群体中的首次发现。<details>
<summary>Abstract</summary>
We study the problem of determining the emergent behaviors that are possible given a functionally heterogeneous swarm of robots with limited capabilities. Prior work has considered behavior search for homogeneous swarms and proposed the use of novelty search over either a hand-specified or learned behavior space followed by clustering to return a taxonomy of emergent behaviors to the user. In this paper, we seek to better understand the role of novelty search and the efficacy of using clustering to discover novel emergent behaviors. Through a large set of experiments and ablations, we analyze the effect of representations, evolutionary search, and various clustering methods in the search for novel behaviors in a heterogeneous swarm. Our results indicate that prior methods fail to discover many interesting behaviors and that an iterative human-in-the-loop discovery process discovers more behaviors than random search, swarm chemistry, and automated behavior discovery. The combined discoveries of our experiments uncover 23 emergent behaviors, 18 of which are novel discoveries. To the best of our knowledge, these are the first known emergent behaviors for heterogeneous swarms of computation-free agents. Videos, code, and appendix are available at the project website: https://sites.google.com/view/heterogeneous-bd-methods
</details>
<details>
<summary>摘要</summary>
我们研究一个功能多样化群体机器人的发展行为问题，该群体具有有限的能力。先前的研究曾经考虑过同型群体的行为搜索，并提出使用新奇搜索来寻找行为空间中的新行为，然后使用对应的对应组合来返回用户。在这篇论文中，我们想要更好地理解新奇搜索的角色以及使用对应组合来发现新的发展行为的有效性。通过一系列实验和删除，我们分析了表示、演化搜索和对应组合的效果，以寻找在多样化群体中发现新的行为。我们的结果显示先前的方法无法发现许多有兴趣的行为，而一种轮循人类在过程中的寻找过程可以发现更多的行为，比如随机搜索、群体化学和自动行为发现。我们的实验发现了23个发展行为，其中18个是新发现。到目前为止，这些是同型多样化群体机器人中第一个已知的发展行为。详细信息可以在项目网站上找到：https://sites.google.com/view/heterogeneous-bd-methods。
</details></li>
</ul>
<hr>
<h2 id="MimicTouch-Learning-Human’s-Control-Strategy-with-Multi-Modal-Tactile-Feedback"><a href="#MimicTouch-Learning-Human’s-Control-Strategy-with-Multi-Modal-Tactile-Feedback" class="headerlink" title="MimicTouch: Learning Human’s Control Strategy with Multi-Modal Tactile Feedback"></a>MimicTouch: Learning Human’s Control Strategy with Multi-Modal Tactile Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16917">http://arxiv.org/abs/2310.16917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kelin Yu, Yunhai Han, Matthew Zhu, Ye Zhao</li>
<li>for: 这paper的目的是开发一种基于人类感觉的控制策略的机器人控制系统。</li>
<li>methods: 这paper使用了多模态感觉数据集和模拟学习技术，以及在线差分强化学习来让机器人模仿人类的感觉控制策略。</li>
<li>results:  experiments show that MimicTouch 可以安全地将人类的感觉控制策略传递给机器人，并且能够在各种任务中提高机器人的性能。<details>
<summary>Abstract</summary>
In robotics and artificial intelligence, the integration of tactile processing is becoming increasingly pivotal, especially in learning to execute intricate tasks like alignment and insertion. However, existing works focusing on tactile methods for insertion tasks predominantly rely on robot teleoperation data and reinforcement learning, which do not utilize the rich insights provided by human's control strategy guided by tactile feedback. For utilizing human sensations, methodologies related to learning from humans predominantly leverage visual feedback, often overlooking the invaluable tactile feedback that humans inherently employ to finish complex manipulations. Addressing this gap, we introduce "MimicTouch", a novel framework that mimics human's tactile-guided control strategy. In this framework, we initially collect multi-modal tactile datasets from human demonstrators, incorporating human tactile-guided control strategies for task completion. The subsequent step involves instructing robots through imitation learning using multi-modal sensor data and retargeted human motions. To further mitigate the embodiment gap between humans and robots, we employ online residual reinforcement learning on the physical robot. Through comprehensive experiments, we validate the safety of MimicTouch in transferring a latent policy learned through imitation learning from human to robot. This ongoing work will pave the way for a broader spectrum of tactile-guided robotic applications.
</details>
<details>
<summary>摘要</summary>
在机器人和人工智能领域，感觉处理的集成在执行复杂任务 like 对接和定位方面变得越来越重要，特别是在学习执行这些任务时。然而，现有的执行任务中的策略几乎完全依赖于机器人 теле操作数据和奖励学习，不使用人类的感觉指导策略。为了利用人类的感觉，相关的学习人类方法主要依赖于视觉反馈，经常忽略人类在完成复杂把握中的感觉反馈。为了解决这个空隙，我们提出了“模仿感觉”（MimicTouch）框架。在这个框架中，我们首先收集多modal的感觉数据集，包括人类在完成任务时的感觉指导策略。接着，我们通过模仿学习使用多modal感觉数据和重定向的人类动作来指导机器人。为了进一步减少人机embody gap，我们使用在线剩余奖励学习来调整物理机器人。通过广泛的实验，我们证明了MimicTouch可以安全地将人类的潜在策略传递给机器人。这项工作将为机器人感觉导向应用带来更广泛的前景。
</details></li>
</ul>
<hr>
<h2 id="Transformer-based-Atmospheric-Density-Forecasting"><a href="#Transformer-based-Atmospheric-Density-Forecasting" class="headerlink" title="Transformer-based Atmospheric Density Forecasting"></a>Transformer-based Atmospheric Density Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16912">http://arxiv.org/abs/2310.16912</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julia Briden, Peng Mun Siew, Victor Rodriguez-Fernandez, Richard Linares</li>
<li>for: 预测大气密度，提高空间环境意识。</li>
<li>methods: 使用深度学习模型， capture 大气密度数据中长期关系。</li>
<li>results: 比较 Empirical NRLMSISE-00、JB2008 和 TIEGCM 模型，显示 transformer-based  пропагатор 的预测性能更高。<details>
<summary>Abstract</summary>
As the peak of the solar cycle approaches in 2025 and the ability of a single geomagnetic storm to significantly alter the orbit of Resident Space Objects (RSOs), techniques for atmospheric density forecasting are vital for space situational awareness. While linear data-driven methods, such as dynamic mode decomposition with control (DMDc), have been used previously for forecasting atmospheric density, deep learning-based forecasting has the ability to capture nonlinearities in data. By learning multiple layer weights from historical atmospheric density data, long-term dependencies in the dataset are captured in the mapping between the current atmospheric density state and control input to the atmospheric density state at the next timestep. This work improves upon previous linear propagation methods for atmospheric density forecasting, by developing a nonlinear transformer-based architecture for atmospheric density forecasting. Empirical NRLMSISE-00 and JB2008, as well as physics-based TIEGCM atmospheric density models are compared for forecasting with DMDc and with the transformer-based propagator.
</details>
<details>
<summary>摘要</summary>
As the peak of the solar cycle approaches in 2025 and the ability of a single geomagnetic storm to significantly alter the orbit of Resident Space Objects (RSOs), 技术 для预测大气密度是 Space situational awareness 中非常重要的。而以前使用的线性数据驱动方法，如动态模式分解控制（DMDc），已经用于大气密度预测，但是深度学习基本法可以捕捉数据中的非线性关系。通过从历史大气密度数据中学习多层权重，捕捉了数据中长期依赖关系，并将当前大气密度状态与控制输入的映射关系储存在内存中。这种工作提高了之前的线性协议方法，通过开发一种基于 transformer 架构的大气密度预测方法。empirical NRLMSISE-00 和 JB2008 模型，以及基于物理学的 TIEGCM 大气密度模型，与 DMDc 和 transformer 基于的传播器进行比较。
</details></li>
</ul>
<hr>
<h2 id="Deep-machine-learning-for-meteor-monitoring-advances-with-transfer-learning-and-gradient-weighted-class-activation-mapping"><a href="#Deep-machine-learning-for-meteor-monitoring-advances-with-transfer-learning-and-gradient-weighted-class-activation-mapping" class="headerlink" title="Deep machine learning for meteor monitoring: advances with transfer learning and gradient-weighted class activation mapping"></a>Deep machine learning for meteor monitoring: advances with transfer learning and gradient-weighted class activation mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16826">http://arxiv.org/abs/2310.16826</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eloy Peña-Asensio, Josep M. Trigo-Rodríguez, Pau Grèbol-Tomàs, David Regordosa-Avellana, Albert Rimola</li>
<li>for: 这个论文主要目的是提出一种自动化的激光探测系统，以便更好地研究流星学。</li>
<li>methods: 该论文使用了卷积神经网络（CNNs）来自动分类候选的流星检测图像。具体来说，使用了Gradient-weighted Class Activation Mapping（Grad-CAM）技术来确定流星在每幅图像中的准确位置。</li>
<li>results: 该论文在使用SPMN数据集进行训练和评估后，实现了98%的精度。这种新方法有助于减少流星科学家和站点运行人员的工作负担，同时提高流星跟踪和分类的精度。<details>
<summary>Abstract</summary>
In recent decades, the use of optical detection systems for meteor studies has increased dramatically, resulting in huge amounts of data being analyzed. Automated meteor detection tools are essential for studying the continuous meteoroid incoming flux, recovering fresh meteorites, and achieving a better understanding of our Solar System. Concerning meteor detection, distinguishing false positives between meteor and non-meteor images has traditionally been performed by hand, which is significantly time-consuming. To address this issue, we developed a fully automated pipeline that uses Convolutional Neural Networks (CNNs) to classify candidate meteor detections. Our new method is able to detect meteors even in images that contain static elements such as clouds, the Moon, and buildings. To accurately locate the meteor within each frame, we employ the Gradient-weighted Class Activation Mapping (Grad-CAM) technique. This method facilitates the identification of the region of interest by multiplying the activations from the last convolutional layer with the average of the gradients across the feature map of that layer. By combining these findings with the activation map derived from the first convolutional layer, we effectively pinpoint the most probable pixel location of the meteor. We trained and evaluated our model on a large dataset collected by the Spanish Meteor Network (SPMN) and achieved a precision of 98\%. Our new methodology presented here has the potential to reduce the workload of meteor scientists and station operators and improve the accuracy of meteor tracking and classification.
</details>
<details>
<summary>摘要</summary>
近年来，用于天体研究的光学探测系统的使用量有所增加，导致大量数据需要进行分析。自动化的流星探测工具是研究不断的流星oid进来流和回收新的流星的关键。在流星探测方面，传统上通过手动进行分类来 отлича流星和非流星图像，这是非常时间消耗的。为解决这个问题，我们开发了一个完全自动化的管道，使用卷积神经网络（CNNs）来分类候选的流星探测。我们的新方法可以在包含静止元素 such as 云、月亮和建筑物的图像中探测流星。为了准确地在每帧中找到流星，我们使用了梯度加权灵活图像（Grad-CAM）技术。这种方法将各层的激活值与该层的梯度的平均值相乘，以便在特定的特征图像中标识感兴趣的区域。通过将这些发现与基层卷积层的激活图像相结合，我们可以准确地定位流星在每帧中的位置。我们使用了大量由西班牙流星网络（SPMN）收集的数据进行训练和评估，并达到了98%的精度。我们的新方法可以减少流星科学家和站点操作人员的工作负担，并提高流星跟踪和分类的精度。
</details></li>
</ul>
<hr>
<h2 id="CATE-Lasso-Conditional-Average-Treatment-Effect-Estimation-with-High-Dimensional-Linear-Regression"><a href="#CATE-Lasso-Conditional-Average-Treatment-Effect-Estimation-with-High-Dimensional-Linear-Regression" class="headerlink" title="CATE Lasso: Conditional Average Treatment Effect Estimation with High-Dimensional Linear Regression"></a>CATE Lasso: Conditional Average Treatment Effect Estimation with High-Dimensional Linear Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16819">http://arxiv.org/abs/2310.16819</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masahiro Kato, Masaaki Imaizumi</li>
<li>for: This paper is written to study the estimation of Conditional Average Treatment Effects (CATEs) in causal inference, specifically in the presence of high-dimensional and non-sparse parameters.</li>
<li>methods: The paper proposes a method for consistently estimating CATEs using Lasso regression, which is specialized for CATE estimation and leverages the assumption of implicit sparsity.</li>
<li>results: The paper demonstrates the consistency of the proposed method through simulation studies, and shows that desirable theoretical properties such as consistency remain attainable even without assuming sparsity explicitly.<details>
<summary>Abstract</summary>
In causal inference about two treatments, Conditional Average Treatment Effects (CATEs) play an important role as a quantity representing an individualized causal effect, defined as a difference between the expected outcomes of the two treatments conditioned on covariates. This study assumes two linear regression models between a potential outcome and covariates of the two treatments and defines CATEs as a difference between the linear regression models. Then, we propose a method for consistently estimating CATEs even under high-dimensional and non-sparse parameters. In our study, we demonstrate that desirable theoretical properties, such as consistency, remain attainable even without assuming sparsity explicitly if we assume a weaker assumption called implicit sparsity originating from the definition of CATEs. In this assumption, we suppose that parameters of linear models in potential outcomes can be divided into treatment-specific and common parameters, where the treatment-specific parameters take difference values between each linear regression model, while the common parameters remain identical. Thus, in a difference between two linear regression models, the common parameters disappear, leaving only differences in the treatment-specific parameters. Consequently, the non-zero parameters in CATEs correspond to the differences in the treatment-specific parameters. Leveraging this assumption, we develop a Lasso regression method specialized for CATE estimation and present that the estimator is consistent. Finally, we confirm the soundness of the proposed method by simulation studies.
</details>
<details>
<summary>摘要</summary>
在 causal inference 中， conditional average treatment effects (CATEs) 扮演着重要的角色，它是个人化的 causal effect，定义为两个待遇中期望的差异，它们条件于 covariates。这个研究假设了两个线性回归模型，它们连接 potential outcome 和 covariates 的两个待遇。然后，我们提出了一种方法来稳定地估计 CATEs，即使高维度和非杂参数情况下也可以。在我们的研究中，我们证明了欲望的理论性质，如一致性，可以在不假设简单性的情况下保持。在这个假设中，我们假设了待遇中的参数可以分解为待遇特定的参数和通用参数，其中待遇特定的参数在各个线性回归模型中差异化，而通用参数保持不变。因此，在两个线性回归模型之间的差异中，通用参数消失，只有待遇特定的参数存在非零值。基于这个假设，我们开发了一种特化于 CATE 估计的 Lasso 回归方法，并证明了该估计器是一致的。最后，我们通过 simulate 研究证明了我们的方法的正确性。
</details></li>
</ul>
<hr>
<h2 id="Learning-COVID-19-Regional-Transmission-Using-Universal-Differential-Equations-in-a-SIR-model"><a href="#Learning-COVID-19-Regional-Transmission-Using-Universal-Differential-Equations-in-a-SIR-model" class="headerlink" title="Learning COVID-19 Regional Transmission Using Universal Differential Equations in a SIR model"></a>Learning COVID-19 Regional Transmission Using Universal Differential Equations in a SIR model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16804">http://arxiv.org/abs/2310.16804</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adrocampos/udes_in_sir_regional_transmision">https://github.com/adrocampos/udes_in_sir_regional_transmision</a></li>
<li>paper_authors: Adrian Rojas-Campos, Lukas Stelz, Pascal Nieters</li>
<li>for: 模型COVID-19的传播行为在高度连接的社会中很难模拟。单个区域SIR模型无法考虑来自其他地区的感染力量，扩展到大量交互地区则需要许多不实际存在的假设。</li>
<li>methods: 我们提出使用Universal Differential Equations（UDEs）来捕捉邻居地区对感染的影响，并与SIR模型结合使用。UDEs是 Totally或部分由深度神经网络（DNN）定义的微分方程。我们添加了一个由DNN学习来自其他地区的感染力量的添加项到SIR方程中。学习使用自动导数和梯度下降来让模型更好地预测疫情的发展。</li>
<li>results: 我们对使用模拟COVID-19疫情的方法进行比较，包括单个区域SIR模型和基于 solely DNN的完全数据驱动模型。我们发现，提出的UDE+SIR模型可以更准确地预测疫情的发展，但在疫情的末期出现衰减性的表现。单个区域SIR模型和完全数据驱动模型无法正确地预测疫情的发展。<details>
<summary>Abstract</summary>
Highly-interconnected societies difficult to model the spread of infectious diseases such as COVID-19. Single-region SIR models fail to account for incoming forces of infection and expanding them to a large number of interacting regions involves many assumptions that do not hold in the real world. We propose using Universal Differential Equations (UDEs) to capture the influence of neighboring regions and improve the model's predictions in a combined SIR+UDE model. UDEs are differential equations totally or partially defined by a deep neural network (DNN). We include an additive term to the SIR equations composed by a DNN that learns the incoming force of infection from the other regions. The learning is performed using automatic differentiation and gradient descent to approach the change in the target system caused by the state of the neighboring regions. We compared the proposed model using a simulated COVID-19 outbreak against a single-region SIR and a fully data-driven model composed only of a DNN. The proposed UDE+SIR model generates predictions that capture the outbreak dynamic more accurately, but a decay in performance is observed at the last stages of the outbreak. The single-area SIR and the fully data-driven approach do not capture the proper dynamics accurately. Once the predictions were obtained, we employed the SINDy algorithm to substitute the DNN with a regression, removing the black box element of the model with no considerable increase in the error levels.
</details>
<details>
<summary>摘要</summary>
高度连接的社会困难模型冠状病毒如COVID-19的传播。单个地区SIR模型无法考虑来自其他地区的感染力量和扩展它们到许多交互地区具有许多假设不符实际世界。我们提议使用全球差分方程（UDE）捕捉邻近地区的影响并改进模型预测，在SIR+UDE模型中。UDEs是 Totally或部分定义为深度神经网络（DNN）的差分方程。我们添加了一个由DNN学习来自其他地区的感染力量的添加项到SIR方程中。学习使用自动微分和梯度下降以 approachingtarget系统中由邻近地区的状态所引起的变化。我们对提议模型使用了模拟COVID-19爆发的结果进行比较，与单个地区SIR模型和完全数据驱动模型（只由DNN组成）进行比较。提议的UDE+SIR模型生成了更加准确地捕捉爆发动态的预测，但在爆发的最后阶段显示衰减性。单个地区SIR和完全数据驱动方法不能准确地捕捉动态。一旦预测得到，我们使用SINDy算法将DNN替换为回归，从而消除黑盒模型的隐身元素，而无 considerable增加误差水平。
</details></li>
</ul>
<hr>
<h2 id="From-Molecules-to-Materials-Pre-training-Large-Generalizable-Models-for-Atomic-Property-Prediction"><a href="#From-Molecules-to-Materials-Pre-training-Large-Generalizable-Models-for-Atomic-Property-Prediction" class="headerlink" title="From Molecules to Materials: Pre-training Large Generalizable Models for Atomic Property Prediction"></a>From Molecules to Materials: Pre-training Large Generalizable Models for Atomic Property Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16802">http://arxiv.org/abs/2310.16802</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nima Shoghi, Adeesh Kolluru, John R. Kitchin, Zachary W. Ulissi, C. Lawrence Zitnick, Brandon M. Wood</li>
<li>for: 提高化学领域中物理性质预测的效果</li>
<li>methods: 使用多个化学领域的数据同时进行超参数化训练，并将每个领域作为独立的预训练任务来处理</li>
<li>results: 比起从零开始训练，平均提高59%的性能和在40个任务中 matches或 setting state-of-the-art 的表现<details>
<summary>Abstract</summary>
Foundation models have been transformational in machine learning fields such as natural language processing and computer vision. Similar success in atomic property prediction has been limited due to the challenges of training effective models across multiple chemical domains. To address this, we introduce Joint Multi-domain Pre-training (JMP), a supervised pre-training strategy that simultaneously trains on multiple datasets from different chemical domains, treating each dataset as a unique pre-training task within a multi-task framework. Our combined training dataset consists of $\sim$120M systems from OC20, OC22, ANI-1x, and Transition-1x. We evaluate performance and generalization by fine-tuning over a diverse set of downstream tasks and datasets including: QM9, rMD17, MatBench, QMOF, SPICE, and MD22. JMP demonstrates an average improvement of 59% over training from scratch, and matches or sets state-of-the-art on 34 out of 40 tasks. Our work highlights the potential of pre-training strategies that utilize diverse data to advance property prediction across chemical domains, especially for low-data tasks.
</details>
<details>
<summary>摘要</summary>
基础模型在自然语言处理和计算机视觉领域已经带来了转变。然而，在化学领域中的质量预测方面，这些成功却受到训练效果难以扩展到多个化学领域的挑战。为解决这个问题，我们介绍了一种名为 JOINT MULTI-DOMAIN PRE-TRAINING（JMP）的指导预训练策略。这种策略 simultaneous 地在多个化学领域中训练，将每个领域视为独立的预训练任务，并在多任务框架中进行同时训练。我们的合并训练集包括了OC20、OC22、ANI-1x和Transition-1x等约120M个系统。我们通过对这些系统进行精细调整和特定任务和数据集进行评估，发现JMP可以在34个任务中提高平均59%，并与或与状态之一的成果相匹配。我们的工作表明，通过使用多样化数据进行预训练，可以推动化学领域中的质量预测进步，特别是低数据任务。
</details></li>
</ul>
<hr>
<h2 id="QMoE-Practical-Sub-1-Bit-Compression-of-Trillion-Parameter-Models"><a href="#QMoE-Practical-Sub-1-Bit-Compression-of-Trillion-Parameter-Models" class="headerlink" title="QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models"></a>QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16795">http://arxiv.org/abs/2310.16795</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ist-daslab/qmoe">https://github.com/ist-daslab/qmoe</a></li>
<li>paper_authors: Elias Frantar, Dan Alistarh</li>
<li>for: 这 paper 是为了解决大语言模型（LLM）的高执行成本问题，通过稀疏路由来提供更快速和准确的模型，但是带来了庞大参数的问题。</li>
<li>methods: 这 paper 使用了一种新的压缩和执行框架called QMoE，该框架包括一种可扩展的算法，可以准确地压缩 trillion-parameter MoE 到 less than 1 bit per parameter，并且与特制 GPU 解码器一起实现高效的端到端压缩执行。</li>
<li>results: QMoE 可以将 1.6 万亿参数的 SwitchTransformer-c2048 模型压缩到 less than 160GB (20x 压缩，0.8 bits per parameter)，并且只需要少量的精度损失，在单个 GPU 上完成在一天内。这使得可以在可负担的商业硬件上执行 trillion-parameter 模型，如单个服务器上的 4x NVIDIA A6000 或 8x NVIDIA 3090 GPUs，并且只需要 less than 5% 的运行时过程相对于理想的无压缩执行。<details>
<summary>Abstract</summary>
Mixture-of-Experts (MoE) architectures offer a general solution to the high inference costs of large language models (LLMs) via sparse routing, bringing faster and more accurate models, at the cost of massive parameter counts. For example, the SwitchTransformer-c2048 model has 1.6 trillion parameters, requiring 3.2TB of accelerator memory to run efficiently, which makes practical deployment challenging and expensive. In this paper, we present a solution to this memory problem, in form of a new compression and execution framework called QMoE. Specifically, QMoE consists of a scalable algorithm which accurately compresses trillion-parameter MoEs to less than 1 bit per parameter, in a custom format co-designed with bespoke GPU decoding kernels to facilitate efficient end-to-end compressed inference, with minor runtime overheads relative to uncompressed execution. Concretely, QMoE can compress the 1.6 trillion parameter SwitchTransformer-c2048 model to less than 160GB (20x compression, 0.8 bits per parameter) at only minor accuracy loss, in less than a day on a single GPU. This enables, for the first time, the execution of a trillion-parameter model on affordable commodity hardware, like a single server with 4x NVIDIA A6000 or 8x NVIDIA 3090 GPUs, at less than 5% runtime overhead relative to ideal uncompressed inference. The source code and compressed models are available at github.com/IST-DASLab/qmoe.
</details>
<details>
<summary>摘要</summary>
大量语言模型（LLM）的高推理成本可以通过杂合扩展（MoE）架构解决，通过稀疏路由实现更快速和更准确的模型，但是需要巨量的参数数量。例如，SwitchTransformer-c2048模型有1.6万亿个参数，需要3.2TB的加速器内存来运行高效，这使得实际部署成为困难和昂贵的问题。在这篇论文中，我们提出了一种解决这个内存问题的解决方案，即新的压缩和执行框架 called QMoE。具体来说，QMoE包括一个可扩展的算法，可以高精度地压缩大量参数的MoE，以 less than 1比特/参数的形式，并与特制GPU解码器一起实现高效的压缩执行，具有较少的运行时间开销。例如，QMoE可以将1.6万亿参数的SwitchTransformer-c2048模型压缩到less than 160GB（20倍压缩，0.8比特/参数），在单个GPU上完成，只需要一天时间，并且只有较少的精度损失。这使得，对于首次执行一万亿参数模型，可以使用可靠的商用硬件，如单个服务器上的4个NVIDIA A6000或8个NVIDIA 3090 GPU，并且在5%的运行时间开销下完成。源代码和压缩模型可以在github.com/IST-DASLab/qmoe中下载。
</details></li>
</ul>
<hr>
<h2 id="Learning-Independent-Program-and-Architecture-Representations-for-Generalizable-Performance-Modeling"><a href="#Learning-Independent-Program-and-Architecture-Representations-for-Generalizable-Performance-Modeling" class="headerlink" title="Learning Independent Program and Architecture Representations for Generalizable Performance Modeling"></a>Learning Independent Program and Architecture Representations for Generalizable Performance Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16792">http://arxiv.org/abs/2310.16792</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lingda Li, Thomas Flynn, Adolfy Hoisie</li>
<li>for: 这篇论文提出了一种基于深度学习的性能模型框架，可以学习高维独立&#x2F;正交的程序和微架构表示。</li>
<li>methods: 该框架使用深度学习来学习程序和微架构表示，并可以将程序表示应用于任何微架构，以及将微架构表示应用于任何程序的性能预测。</li>
<li>results: 评估表明，PerfVec比前一个方法更通用、高效和准确。<details>
<summary>Abstract</summary>
This paper proposes PerfVec, a novel deep learning-based performance modeling framework that learns high-dimensional, independent/orthogonal program and microarchitecture representations. Once learned, a program representation can be used to predict its performance on any microarchitecture, and likewise, a microarchitecture representation can be applied in the performance prediction of any program. Additionally, PerfVec yields a foundation model that captures the performance essence of instructions, which can be directly used by developers in numerous performance modeling related tasks without incurring its training cost. The evaluation demonstrates that PerfVec is more general, efficient, and accurate than previous approaches.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文提出了PerfVec，一种基于深度学习的性能模拟框架，可以学习高维、独立/垂直的程序和微架构表示。一 fois学习完成，程序表示可以用来预测任何微架构上的性能，而微架构表示也可以应用于任何程序的性能预测。此外，PerfVec还提供了一个基础模型，可以直接用于开发者在多种性能模拟相关任务中使用，无需付出训练成本。评估表明，PerfVec比前方法更通用、高效和准确。
</details></li>
</ul>
<hr>
<h2 id="Covert-Planning-against-Imperfect-Observers"><a href="#Covert-Planning-against-Imperfect-Observers" class="headerlink" title="Covert Planning against Imperfect Observers"></a>Covert Planning against Imperfect Observers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16791">http://arxiv.org/abs/2310.16791</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoxiang Ma, Chongyang Shi, Shuo Han, Michael R. Dorothy, Jie Fu</li>
<li>for: 本文研究了如何通过抽象维度和察视者的不准确观测来实现隐蔽计划，以达到最大化任务性能而不被探测。</li>
<li>methods: 本文使用了Markov决策过程来模型智能机器和其随机环境之间的互动，并使用偏见函数来捕捉察视者对于隐蔽计划的泄露信息。 我们假设察视者使用假设测试来检测到是否存在异常情况，隐蔽计划的目标是 maximize 折扣回报，同时保持察视者探测到的概率Below a given threshold。</li>
<li>results: 我们证明了finite-memory策略比Markovian策略更有力量在隐蔽计划中，然后我们开发了一种基于 primal-dual proximal policy gradient 的方法来计算一个（本地）最优隐蔽策略。我们通过一个柔性网格世界示例来证明我们的方法的有效性，实验结果表明我们的方法可以计算一个不violate detection constraint的策略，同时 empirically 示出了环境噪声对隐蔽策略的影响。<details>
<summary>Abstract</summary>
Covert planning refers to a class of constrained planning problems where an agent aims to accomplish a task with minimal information leaked to a passive observer to avoid detection. However, existing methods of covert planning often consider deterministic environments or do not exploit the observer's imperfect information. This paper studies how covert planning can leverage the coupling of stochastic dynamics and the observer's imperfect observation to achieve optimal task performance without being detected. Specifically, we employ a Markov decision process to model the interaction between the agent and its stochastic environment, and a partial observation function to capture the leaked information to a passive observer. Assuming the observer employs hypothesis testing to detect if the observation deviates from a nominal policy, the covert planning agent aims to maximize the total discounted reward while keeping the probability of being detected as an adversary below a given threshold. We prove that finite-memory policies are more powerful than Markovian policies in covert planning. Then, we develop a primal-dual proximal policy gradient method with a two-time-scale update to compute a (locally) optimal covert policy. We demonstrate the effectiveness of our methods using a stochastic gridworld example. Our experimental results illustrate that the proposed method computes a policy that maximizes the adversary's expected reward without violating the detection constraint, and empirically demonstrates how the environmental noises can influence the performance of the covert policies.
</details>
<details>
<summary>摘要</summary>
隐蔽 планирование（covert planning）是一类受限制的 планирование问题，其中一个智能体尝试完成任务，并避免被一个旁观者探测到。然而，现有的隐蔽 планирование方法通常考虑决定性环境，或者不利用旁观者的不准确观察。这篇论文研究了如何使用随机动力学和旁观者的不准确观察来实现隐蔽任务完成，无需被探测。 Specifically, we employ a Markov decision process to model the interaction between the agent and its stochastic environment, and a partial observation function to capture the leaked information to a passive observer. Assuming the observer employs hypothesis testing to detect if the observation deviates from a nominal policy, the covert planning agent aims to maximize the total discounted reward while keeping the probability of being detected as an adversary below a given threshold. We prove that finite-memory policies are more powerful than Markovian policies in covert planning. Then, we develop a primal-dual proximal policy gradient method with a two-time-scale update to compute a (locally) optimal covert policy. We demonstrate the effectiveness of our methods using a stochastic gridworld example. Our experimental results illustrate that the proposed method computes a policy that maximizes the adversary's expected reward without violating the detection constraint, and empirically demonstrates how the environmental noises can influence the performance of the covert policies.
</details></li>
</ul>
<hr>
<h2 id="The-Simplest-Inflationary-Potentials"><a href="#The-Simplest-Inflationary-Potentials" class="headerlink" title="The Simplest Inflationary Potentials"></a>The Simplest Inflationary Potentials</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16786">http://arxiv.org/abs/2310.16786</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tomás Sousa, Deaglan J. Bartlett, Harry Desmond, Pedro G. Ferreira</li>
<li>for: 这个论文是为了研究早期宇宙的Inflation理论，并且与现有的cosmic microwave background和大规模结构观察相Compatible。</li>
<li>methods: 这个论文使用了一种新的符号重回归法，生成所有可能的简单演算符潜在性。</li>
<li>results: 这个论文通过使用信息理论度量(“最小描述长度”)评估这些模型是否能够压缩Planck数据中的信息，并explored两种不同的假设空间中的参数。<details>
<summary>Abstract</summary>
Inflation is a highly favoured theory for the early Universe. It is compatible with current observations of the cosmic microwave background and large scale structure and is a driver in the quest to detect primordial gravitational waves. It is also, given the current quality of the data, highly under-determined with a large number of candidate implementations. We use a new method in symbolic regression to generate all possible simple scalar field potentials for one of two possible basis sets of operators. Treating these as single-field, slow-roll inflationary models we then score them with an information-theoretic metric ("minimum description length") that quantifies their efficiency in compressing the information in the Planck data. We explore two possible priors on the parameter space of potentials, one related to the functions' structural complexity and one that uses a Katz back-off language model to prefer functions that may be theoretically motivated. This enables us to identify the inflaton potentials that optimally balance simplicity with accuracy at explaining the Planck data, which may subsequently find theoretical motivation. Our exploratory study opens the door to extraction of fundamental physics directly from data, and may be augmented with more refined theoretical priors in the quest for a complete understanding of the early Universe.
</details>
<details>
<summary>摘要</summary>
inflation是 Early Universe 的非常受欢迎理论。它与 cosmic microwave background 和 large scale structure 的现有观察结果相Compatible，并且是探测primordial gravitational waves的 Driver。然而，由于数据质量的限制，这个理论目前处于高度不确定的状态，有许多候选的实现方式。我们使用新的 symbolic regression 方法生成所有可能的简单Scalar field potentials，然后使用信息理论度量（"最小描述长度"）对这些模型进行评分。我们使用两种不同的 prior 在 potential space 中，一种是函数的结构复杂度，另一种是使用 Katz back-off language model 来 preference functions ，这些函数可能具有理论导向性。这些 inflaton potentials 可以最优地平衡简洁性和准确性，从而描述 Planck 数据，并可能找到理论上的支持。我们的探索性研究可以直接从数据中提取基本物理学，并可能与更加精细的理论假设相结合，以完全理解 Early Universe。
</details></li>
</ul>
<hr>
<h2 id="Simple-Scalable-and-Effective-Clustering-via-One-Dimensional-Projections"><a href="#Simple-Scalable-and-Effective-Clustering-via-One-Dimensional-Projections" class="headerlink" title="Simple, Scalable and Effective Clustering via One-Dimensional Projections"></a>Simple, Scalable and Effective Clustering via One-Dimensional Projections</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16752">http://arxiv.org/abs/2310.16752</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/boredoms/prone">https://github.com/boredoms/prone</a></li>
<li>paper_authors: Moses Charikar, Monika Henzinger, Lunjia Hu, Maxmilian Vötsch, Erik Waingarten</li>
<li>for: 这个论文的目的是提出一种基于随机sampling的归一化算法，以提高 clustering 算法的运行时间和准确性。</li>
<li>methods: 该算法使用了随机扫描和简单的排序算法，并且使用了一种新的评价函数来评价每个分区的质量。</li>
<li>results: 研究人员通过 theoretical 分析和实验 validate 了该算法的正确性和精度，并且发现该算法可以提供一个新的平衡点 между运行时间和分区质量。<details>
<summary>Abstract</summary>
Clustering is a fundamental problem in unsupervised machine learning with many applications in data analysis. Popular clustering algorithms such as Lloyd's algorithm and $k$-means++ can take $\Omega(ndk)$ time when clustering $n$ points in a $d$-dimensional space (represented by an $n\times d$ matrix $X$) into $k$ clusters. In applications with moderate to large $k$, the multiplicative $k$ factor can become very expensive. We introduce a simple randomized clustering algorithm that provably runs in expected time $O(\mathrm{nnz}(X) + n\log n)$ for arbitrary $k$. Here $\mathrm{nnz}(X)$ is the total number of non-zero entries in the input dataset $X$, which is upper bounded by $nd$ and can be significantly smaller for sparse datasets. We prove that our algorithm achieves approximation ratio $\smash{\widetilde{O}(k^4)}$ on any input dataset for the $k$-means objective. We also believe that our theoretical analysis is of independent interest, as we show that the approximation ratio of a $k$-means algorithm is approximately preserved under a class of projections and that $k$-means++ seeding can be implemented in expected $O(n \log n)$ time in one dimension. Finally, we show experimentally that our clustering algorithm gives a new tradeoff between running time and cluster quality compared to previous state-of-the-art methods for these tasks.
</details>
<details>
<summary>摘要</summary>
“集群是机器学习中的基本问题，具有许多应用在数据分析中。受欢迎的集群算法如条 Lloyd 算法和 $k$-means++ 可能需要 $\Omega(ndk)$ 时间来将 $n$ 个点在 $d$-dimensional 空间中集结成 $k$ 个集群。在实际应用中，$k$ 的乘积因子可能会很昂贵。我们介绍了一个简单的随机集群算法，其预期时间复杂度为 $O(\text{nnz}(X) + n\log n)$，其中 $\text{nnz}(X)$ 是输入数据集 $X$ 中非零元素的总数，最多等于 $nd$，并且可能较小 для稀疏数据集。我们证明了我们的算法对任何输入数据集都有 $\smash{\widetilde{O}(k^4)}$ 的近似比率，并且我们还证明了这个近似比率在某些投影下保持不变。此外，我们还显示了在一维中可以实现 $k$-means++ 种子生成的预期时间为 $O(n \log n)$。最后，我们通过实验显示了我们的集群算法对前state-of-the-art方法的新的时间负载与集群质量之间的交换。”
</details></li>
</ul>
<hr>
<h2 id="Stochastic-Latent-Transformer-Efficient-Modelling-of-Stochastically-Forced-Zonal-Jets"><a href="#Stochastic-Latent-Transformer-Efficient-Modelling-of-Stochastically-Forced-Zonal-Jets" class="headerlink" title="Stochastic Latent Transformer: Efficient Modelling of Stochastically Forced Zonal Jets"></a>Stochastic Latent Transformer: Efficient Modelling of Stochastically Forced Zonal Jets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16741">http://arxiv.org/abs/2310.16741</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ira-shokar/stochastic_latent_transformer">https://github.com/ira-shokar/stochastic_latent_transformer</a></li>
<li>paper_authors: Ira J. S. Shokar, Rich R. Kerswell, Peter H. Haynes</li>
<li>for: 用于生成大量 ensemble，以便研究流体动力系统中的统计问题，如自发转换事件的概率。</li>
<li>methods: 使用 ‘Stochastic Latent Transformer’ 深度学习模型，其包括带有随机冲击的 transformer 和 translate-equivariant autoencoder，可以复制系统动力 across various integration periods。</li>
<li>results: 对于已知的zonal jet系统，使用这种模型可以 achieved a five-order-of-magnitude speedup compared to numerical integration，以便生成大量 ensemble，用于研究流体动力系统中的统计问题。<details>
<summary>Abstract</summary>
We introduce the 'Stochastic Latent Transformer', a probabilistic deep learning approach for efficient reduced-order modelling of stochastic partial differential equations (SPDEs). Despite recent advances in deep learning for fluid mechanics, limited research has explored modelling stochastically driven flows - which play a crucial role in understanding a broad spectrum of phenomena, from jets on giant planets to ocean circulation and the variability of midlatitude weather. The model architecture consists of a stochastically-forced transformer, paired with a translation-equivariant autoencoder, that we demonstrate is capable of reproducing system dynamics across various integration periods. We demonstrate its effectiveness applied to a well-researched zonal jet system, with the neural network achieving a five-order-of-magnitude speedup compared to numerical integration. This facilitates the cost-effective generation of large ensembles, enabling the exploration of statistical questions concerning probabilities of spontaneous transition events.
</details>
<details>
<summary>摘要</summary>
我们介绍“随机隐藏传播器”，一种几率深度学习方法来有效地实现减少阶层模型（SPDEs）。 DESPITE recent advances in deep learning for fluid mechanics, limited research has explored stochastically driven flows, which play a crucial role in understanding a broad spectrum of phenomena, from jets on giant planets to ocean circulation and the variability of midlatitude weather. The model architecture consists of a stochastically-forced transformer, paired with a translation-equivariant autoencoder, that we demonstrate is capable of reproducing system dynamics across various integration periods. We demonstrate its effectiveness applied to a well-researched zonal jet system, with the neural network achieving a five-order-of-magnitude speedup compared to numerical integration. This facilitates the cost-effective generation of large ensembles, enabling the exploration of statistical questions concerning probabilities of spontaneous transition events.Here's the translation in Traditional Chinese:我们介绍“随机隐藏传播器”，一种几率深度学习方法来有效地实现减少阶层模型（SPDEs）。 DESPITE recent advances in deep learning for fluid mechanics, limited research has explored stochastically driven flows, which play a crucial role in understanding a broad spectrum of phenomena, from jets on giant planets to ocean circulation and the variability of midlatitude weather. The model architecture consists of a stochastically-forced transformer, paired with a translation-equivariant autoencoder, that we demonstrate is capable of reproducing system dynamics across various integration periods. We demonstrate its effectiveness applied to a well-researched zonal jet system, with the neural network achieving a five-order-of-magnitude speedup compared to numerical integration. This facilitates the cost-effective generation of large ensembles, enabling the exploration of statistical questions concerning probabilities of spontaneous transition events.
</details></li>
</ul>
<hr>
<h2 id="MultiPrompter-Cooperative-Prompt-Optimization-with-Multi-Agent-Reinforcement-Learning"><a href="#MultiPrompter-Cooperative-Prompt-Optimization-with-Multi-Agent-Reinforcement-Learning" class="headerlink" title="MultiPrompter: Cooperative Prompt Optimization with Multi-Agent Reinforcement Learning"></a>MultiPrompter: Cooperative Prompt Optimization with Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16730">http://arxiv.org/abs/2310.16730</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dong-Ki Kim, Sungryull Sohn, Lajanugen Logeswaran, Dongsub Shim, Honglak Lee</li>
<li>for: 这篇论文主要针对Automated prompt optimization based on reinforcement learning (RL) 的问题，即如何使用RL来优化提示，以生成可读的提示和黑盒基础模型兼容。</li>
<li>methods: 这篇论文提出了一种新的MultiPrompter框架，视提示优化为合作游戏，在提示组合过程中，多个提示工作 вместе，以降低问题大小，并帮助提示学习优化提示。</li>
<li>results: 测试在文本到图像任务中，MultiPrompter方法可以生成高质量的图像，比基eline表现更好。<details>
<summary>Abstract</summary>
Recently, there has been an increasing interest in automated prompt optimization based on reinforcement learning (RL). This approach offers important advantages, such as generating interpretable prompts and being compatible with black-box foundation models. However, the substantial prompt space size poses challenges for RL-based methods, often leading to suboptimal policy convergence. This paper introduces MultiPrompter, a new framework that views prompt optimization as a cooperative game between prompters which take turns composing a prompt together. Our cooperative prompt optimization effectively reduces the problem size and helps prompters learn optimal prompts. We test our method on the text-to-image task and show its ability to generate higher-quality images than baselines.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:近期，有越来越多的关注在基于强化学习（RL）的自动化提示优化方面。这种方法具有许多优点，如生成可解释的提示和与黑盒基模型兼容。然而，大型提示空间大小会对RL基本方法造成优化策略的困难。这篇论文介绍了 MultiPrompter，一个新的框架，它视提示优化为多个提示者之间的合作游戏。我们的合作提示优化方法可以减小问题的大小，并帮助提示者学习优化提示。我们在文本到图像任务上测试了我们的方法，并显示它可以生成比基elines高质量的图像。
</details></li>
</ul>
<hr>
<h2 id="AI-Hazard-Management-A-framework-for-the-systematic-management-of-root-causes-for-AI-risks"><a href="#AI-Hazard-Management-A-framework-for-the-systematic-management-of-root-causes-for-AI-risks" class="headerlink" title="AI Hazard Management: A framework for the systematic management of root causes for AI risks"></a>AI Hazard Management: A framework for the systematic management of root causes for AI risks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16727">http://arxiv.org/abs/2310.16727</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ronald Schnitzer, Andreas Hapfelmeier, Sven Gaube, Sonja Zillner</li>
<li>for:  This paper aims to provide a structured process for identifying, assessing, and treating risks associated with Artificial Intelligence (AI) systems, called AI Hazard Management (AIHM) framework.</li>
<li>methods:  The proposed AIHM framework is based on a comprehensive state-of-the-art analysis of AI hazards and provides a systematic approach to identify, assess, and treat AI hazards in parallel with the development of AI systems.</li>
<li>results:  The proposed framework can increase the overall quality of AI systems by systematically reducing the impact of identified hazards to an acceptable level, and provides a taxonomy to support the optimal treatment of identified AI hazards. Additionally, the framework ensures the auditability of AI systems by systematically documenting evidence that the potential impact of identified AI hazards could be reduced to a tolerable level.<details>
<summary>Abstract</summary>
Recent advancements in the field of Artificial Intelligence (AI) establish the basis to address challenging tasks. However, with the integration of AI, new risks arise. Therefore, to benefit from its advantages, it is essential to adequately handle the risks associated with AI. Existing risk management processes in related fields, such as software systems, need to sufficiently consider the specifics of AI. A key challenge is to systematically and transparently identify and address AI risks' root causes - also called AI hazards. This paper introduces the AI Hazard Management (AIHM) framework, which provides a structured process to systematically identify, assess, and treat AI hazards. The proposed process is conducted in parallel with the development to ensure that any AI hazard is captured at the earliest possible stage of the AI system's life cycle. In addition, to ensure the AI system's auditability, the proposed framework systematically documents evidence that the potential impact of identified AI hazards could be reduced to a tolerable level. The framework builds upon an AI hazard list from a comprehensive state-of-the-art analysis. Also, we provide a taxonomy that supports the optimal treatment of the identified AI hazards. Additionally, we illustrate how the AIHM framework can increase the overall quality of a power grid AI use case by systematically reducing the impact of identified hazards to an acceptable level.
</details>
<details>
<summary>摘要</summary>
最新的人工智能（AI）技术发展提供了解决复杂问题的基础。然而，通过AI的应用，新的风险也出现了。因此，为了获得其优势，需要有效地处理AI中的风险。相关领域的现有风险管理过程，如软件系统，需要足够考虑AI的特点。关键问题是系统地和透明地识别和解决AI风险的根本原因，也称为AI危险。本文介绍了AI危险管理（AIHM）框架，该框架提供了一种结构化的过程，系统地识别、评估和治理AI风险。提议的过程与开发同步进行，以确保在AI系统的生命周期中最早 posible时间捕捉到任何AI风险。此外，为确保AI系统的审核性，提议的框架系统地记录证明已经识别的AI风险可以降低到可接受水平的证据。框架基于AI风险列表，从全面的现状分析中综合获得。此外，我们还提供了一种支持优化治理认知的分类。此外，我们示例了如何通过AIHM框架在电力网络AI应用中系统地减少识别的风险影响，使其达到可接受的水平。
</details></li>
</ul>
<hr>
<h2 id="Wasserstein-Gradient-Flow-over-Variational-Parameter-Space-for-Variational-Inference"><a href="#Wasserstein-Gradient-Flow-over-Variational-Parameter-Space-for-Variational-Inference" class="headerlink" title="Wasserstein Gradient Flow over Variational Parameter Space for Variational Inference"></a>Wasserstein Gradient Flow over Variational Parameter Space for Variational Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16705">http://arxiv.org/abs/2310.16705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dai Hai Nguyen, Tetsuya Sakurai, Hiroshi Mamitsuka</li>
<li>for: 这个论文的目的是提出一种基于 Wasserstein 梯度下降的变分推断（VI）优化方法，用于减少变分推断的优化问题中的梯度下降过程中的计算复杂度。</li>
<li>methods: 本论文使用的方法包括变分推断（VI）和 Wasserstein 梯度下降（WGD），以及一些实用的数值方法来解决变分推断中的离散梯度流问题。</li>
<li>results: 经验测试和理论分析表明，提出的方法可以有效地提高变分推断的优化效果，并且可以视为现有的黑盒子变分推断（VB）和自然变分推断（NVB）的特例。<details>
<summary>Abstract</summary>
Variational inference (VI) can be cast as an optimization problem in which the variational parameters are tuned to closely align a variational distribution with the true posterior. The optimization task can be approached through vanilla gradient descent in black-box VI or natural-gradient descent in natural-gradient VI. In this work, we reframe VI as the optimization of an objective that concerns probability distributions defined over a \textit{variational parameter space}. Subsequently, we propose Wasserstein gradient descent for tackling this optimization problem. Notably, the optimization techniques, namely black-box VI and natural-gradient VI, can be reinterpreted as specific instances of the proposed Wasserstein gradient descent. To enhance the efficiency of optimization, we develop practical methods for numerically solving the discrete gradient flows. We validate the effectiveness of the proposed methods through empirical experiments on a synthetic dataset, supplemented by theoretical analyses.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Variational inference (VI) 可以看作一个优化问题，其中的变量参数被调整以使Variational分布与真实 posterior 之间匹配。这个优化任务可以通过普通的梯度下降或自然梯度下降进行解决。在这篇文章中，我们将 VI 重新框架为一个关于变量参数空间上的概率分布优化问题。然后，我们提出了 Wasserstein 梯度下降来解决这个优化问题。各种优化技术，包括黑盒子 VI 和自然梯度 VI，可以被视为特殊情况的 Wasserstein 梯度下降。为了提高优化效率，我们开发了实用的数值方法来解决离散梯度流的问题。我们通过实验和理论分析 Validate 提出的方法的有效性。Note: "Variational parameter space" is translated as "变量参数空间" (fāng yì jī yuán jī) in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Interpretable-time-series-neural-representation-for-classification-purposes"><a href="#Interpretable-time-series-neural-representation-for-classification-purposes" class="headerlink" title="Interpretable time series neural representation for classification purposes"></a>Interpretable time series neural representation for classification purposes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16696">http://arxiv.org/abs/2310.16696</a></li>
<li>repo_url: None</li>
<li>paper_authors: Etienne Le Naour, Ghislain Agoua, Nicolas Baskiotis, Vincent Guigue</li>
<li>for: 本研究旨在提出一种可解释性强的神经网络模型，用于解决现有的时间序列数据表示方法缺乏可解释性的问题。</li>
<li>methods: 本研究提出了一组可解释性神经网络模型的需求，并提出了一种新的无监督神经网络架构，可以满足这些需求。该模型通过独立学习无下渠任务来学习，以确保其robustness。</li>
<li>results: 在使用UCRC存档 datasets进行分类任务的实验中，提出的模型比其他可解释性模型和现有神经网络表示学习模型得到更好的结果，而且在多个 dataset 上得到了平均更好的结果。此外，我们还进行了质量实验来评估该方法的可解释性。<details>
<summary>Abstract</summary>
Deep learning has made significant advances in creating efficient representations of time series data by automatically identifying complex patterns. However, these approaches lack interpretability, as the time series is transformed into a latent vector that is not easily interpretable. On the other hand, Symbolic Aggregate approximation (SAX) methods allow the creation of symbolic representations that can be interpreted but do not capture complex patterns effectively. In this work, we propose a set of requirements for a neural representation of univariate time series to be interpretable. We propose a new unsupervised neural architecture that meets these requirements. The proposed model produces consistent, discrete, interpretable, and visualizable representations. The model is learned independently of any downstream tasks in an unsupervised setting to ensure robustness. As a demonstration of the effectiveness of the proposed model, we propose experiments on classification tasks using UCR archive datasets. The obtained results are extensively compared to other interpretable models and state-of-the-art neural representation learning models. The experiments show that the proposed model yields, on average better results than other interpretable approaches on multiple datasets. We also present qualitative experiments to asses the interpretability of the approach.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Local-Discovery-by-Partitioning-Polynomial-Time-Causal-Discovery-Around-Exposure-Outcome-Pairs"><a href="#Local-Discovery-by-Partitioning-Polynomial-Time-Causal-Discovery-Around-Exposure-Outcome-Pairs" class="headerlink" title="Local Discovery by Partitioning: Polynomial-Time Causal Discovery Around Exposure-Outcome Pairs"></a>Local Discovery by Partitioning: Polynomial-Time Causal Discovery Around Exposure-Outcome Pairs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17816">http://arxiv.org/abs/2310.17816</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacqueline Maasch, Weishen Pan, Shantanu Gupta, Volodymyr Kuleshov, Kyra Gan, Fei Wang</li>
<li>for: 该研究是为了解决自动选择 covariate 的问题，即在具有限制的先验知识的情况下。</li>
<li>methods: 该研究使用了 Local Discovery by Partitioning（LDP）算法，该算法可以将变量集 Z  partitioned 到与 exposure-outcome 对 {X,Y} 相关的不同 subset 中。该算法是基于有效地调整集的标准化，但不需要先做很多先验知识。</li>
<li>results: 该研究提供了对于任何 Z 的有效的 adjustment set，并且可以确保这些 adjustment set 是有效的。在更强的条件下，研究表明了 partition label 是 asymptotically correct。total independence tests 的时间复杂度是 quadratic 的，但是在实际测试中，可以观察到 quadratic 的时间复杂度。与基eline 相比，LDP 可以更好地回归 confounder 和更准确地估计 average treatment effect。<details>
<summary>Abstract</summary>
This work addresses the problem of automated covariate selection under limited prior knowledge. Given an exposure-outcome pair {X,Y} and a variable set Z of unknown causal structure, the Local Discovery by Partitioning (LDP) algorithm partitions Z into subsets defined by their relation to {X,Y}. We enumerate eight exhaustive and mutually exclusive partitions of any arbitrary Z and leverage this taxonomy to differentiate confounders from other variable types. LDP is motivated by valid adjustment set identification, but avoids the pretreatment assumption commonly made by automated covariate selection methods. We provide theoretical guarantees that LDP returns a valid adjustment set for any Z that meets sufficient graphical conditions. Under stronger conditions, we prove that partition labels are asymptotically correct. Total independence tests is worst-case quadratic in |Z|, with sub-quadratic runtimes observed empirically. We numerically validate our theoretical guarantees on synthetic and semi-synthetic graphs. Adjustment sets from LDP yield less biased and more precise average treatment effect estimates than baselines, with LDP outperforming on confounder recall, test count, and runtime for valid adjustment set discovery.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-based-adaption-of-robotic-friction-models"><a href="#Learning-based-adaption-of-robotic-friction-models" class="headerlink" title="Learning-based adaption of robotic friction models"></a>Learning-based adaption of robotic friction models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16688">http://arxiv.org/abs/2310.16688</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philipp Scholl, Maged Iskandar, Sebastian Wolf, Jinoh Lee, Aras Bacho, Alexander Dietrich, Alin Albu-Schäffer, Gitta Kutyniok</li>
<li>for:  This paper aims to address the challenge of modeling friction torque in robotic joints, which is a longstanding problem due to the lack of a good mathematical description.</li>
<li>methods:  The authors propose a novel approach based on residual learning to adapt an existing friction model to new dynamics using as little data as possible. They use a base neural network to learn an accurate relation between velocity and friction torque, and then train a second network to predict the residual of the initial network’s output.</li>
<li>results:  The authors demonstrate that their proposed estimator outperforms the conventional model-based approach and the base neural network significantly, with an approximately 60-70% improvement in trajectory tracking accuracy. They also show that their method can adapt to diverse scenarios based on prior knowledge about friction in different settings, using only 43 seconds of robot movement data.<details>
<summary>Abstract</summary>
In the Fourth Industrial Revolution, wherein artificial intelligence and the automation of machines occupy a central role, the deployment of robots is indispensable. However, the manufacturing process using robots, especially in collaboration with humans, is highly intricate. In particular, modeling the friction torque in robotic joints is a longstanding problem due to the lack of a good mathematical description. This motivates the usage of data-driven methods in recent works. However, model-based and data-driven models often exhibit limitations in their ability to generalize beyond the specific dynamics they were trained on, as we demonstrate in this paper. To address this challenge, we introduce a novel approach based on residual learning, which aims to adapt an existing friction model to new dynamics using as little data as possible. We validate our approach by training a base neural network on a symmetric friction data set to learn an accurate relation between the velocity and the friction torque. Subsequently, to adapt to more complex asymmetric settings, we train a second network on a small dataset, focusing on predicting the residual of the initial network's output. By combining the output of both networks in a suitable manner, our proposed estimator outperforms the conventional model-based approach and the base neural network significantly. Furthermore, we evaluate our method on trajectories involving external loads and still observe a substantial improvement, approximately 60-70\%, over the conventional approach. Our method does not rely on data with external load during training, eliminating the need for external torque sensors. This demonstrates the generalization capability of our approach, even with a small amount of data-only 43 seconds of a robot movement-enabling adaptation to diverse scenarios based on prior knowledge about friction in different settings.
</details>
<details>
<summary>摘要</summary>
在第四次工业革命中，人工智能和机器自动化在中心地位上扮演着重要角色。在这个过程中，使用机器人非常重要。然而，通过机器人进行生产过程，尤其是在人机合作的情况下，非常复杂。特别是模拟机器人 JOINT 的摩擦力很难，因为没有好的数学描述。这个问题驱使了我们使用数据驱动方法。然而，模型基的和数据驱动模型经常具有泛化能力不足的问题，我们在这篇论文中展示了这一点。为了解决这个挑战，我们提出了一种新的方法，基于剩余学习，可以通过最少数据来适应新的动力学。我们验证了我们的方法，通过在 симметry 的摩擦数据集上训练基本神经网络，以学习摩擦力和运动速度之间的准确关系。然后，为了适应更复杂的非对称情况，我们在一个小 dataset 上训练第二个神经网络，专注于预测初始神经网络输出的差异。通过合理地组合这两个网络的输出，我们的提议的估计器在比较 conventional 模型基的方法和基本神经网络上显著提高了性能。此外，我们还评估了我们的方法在包括外部荷重的轨迹上的性能，并观察到大约 60-70% 的提高。我们的方法不需要在训练时使用外部扭矩传感器，因此可以在不同的情况下进行适应，只需要43秒的机器人运动数据。这表明了我们的方法具有泛化能力，甚至只需要小量的数据。
</details></li>
</ul>
<hr>
<h2 id="Robust-and-Actively-Secure-Serverless-Collaborative-Learning"><a href="#Robust-and-Actively-Secure-Serverless-Collaborative-Learning" class="headerlink" title="Robust and Actively Secure Serverless Collaborative Learning"></a>Robust and Actively Secure Serverless Collaborative Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16678">http://arxiv.org/abs/2310.16678</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olive Franzese, Adam Dziedzic, Christopher A. Choquette-Choo, Mark R. Thomas, Muhammad Ahmad Kaleem, Stephan Rabanser, Congyu Fang, Somesh Jha, Nicolas Papernot, Xiao Wang</li>
<li>for: 本研究旨在提供一种安全和可靠的分布式机器学习（Collaborative Machine Learning）方法，以保护客户端数据点免受服务器或客户端的攻击。</li>
<li>methods: 本研究使用了分布式机器学习（Distributed Machine Learning）的方法，并提出了一种安全和可靠的peer-to-peer（P2P）学习方案，可以防止服务器和客户端的不可靠行为。</li>
<li>results: 研究表明，该方法可以在1000个参与者的情况下，对1000万参数的模型进行训练，并且可以防止服务器和客户端的攻击。<details>
<summary>Abstract</summary>
Collaborative machine learning (ML) is widely used to enable institutions to learn better models from distributed data. While collaborative approaches to learning intuitively protect user data, they remain vulnerable to either the server, the clients, or both, deviating from the protocol. Indeed, because the protocol is asymmetric, a malicious server can abuse its power to reconstruct client data points. Conversely, malicious clients can corrupt learning with malicious updates. Thus, both clients and servers require a guarantee when the other cannot be trusted to fully cooperate. In this work, we propose a peer-to-peer (P2P) learning scheme that is secure against malicious servers and robust to malicious clients. Our core contribution is a generic framework that transforms any (compatible) algorithm for robust aggregation of model updates to the setting where servers and clients can act maliciously. Finally, we demonstrate the computational efficiency of our approach even with 1-million parameter models trained by 100s of peers on standard datasets.
</details>
<details>
<summary>摘要</summary>
共同机器学习（ML）广泛应用于各institution以获得更好的模型，从分布式数据中学习。而共同approach to learning Intuitively保护用户数据，但它们仍然易受到服务器、客户端或两者都 deviation from the protocol。实际上，因为协议是非对称的，一个恶意服务器可以利用其权力重construct客户端数据点。相反，恶意客户端可以腐化学习 mediante malicious updates。因此，客户端和服务器都需要一个保证，当另一方不能完全合作时。在这项工作中，我们提议一种分布式学习方案，安全于恶意服务器和对客户端腐化。我们的核心贡献是一种可generic framework，将任何（相容）算法 дляRobust Aggregation of model updates transform into setting where servers and clients can act maliciously。最后，我们证明了我们的方法的计算效率， même avec 1000000参数模型由100个同仁在标准 datasets上训练。
</details></li>
</ul>
<hr>
<h2 id="UAV-Pathfinding-in-Dynamic-Obstacle-Avoidance-with-Multi-agent-Reinforcement-Learning"><a href="#UAV-Pathfinding-in-Dynamic-Obstacle-Avoidance-with-Multi-agent-Reinforcement-Learning" class="headerlink" title="UAV Pathfinding in Dynamic Obstacle Avoidance with Multi-agent Reinforcement Learning"></a>UAV Pathfinding in Dynamic Obstacle Avoidance with Multi-agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16659">http://arxiv.org/abs/2310.16659</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qizhen Wu, Lei Chen, Kexin Liu, Jinhu Lv</li>
<li>For:  solve the dynamic obstacle avoidance problem online for multi-agent systems* Methods:  centralized training with decentralized execution based on multi-agent reinforcement learning, with improved model predictive control for efficiency and sample utilization* Results:  experimental results in simulation, indoor, and outdoor environments validate the effectiveness of the proposed method, with video available at <a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1gw41197hV/?vd_source=9de61aecdd9fb684e546d032ef7fe7bf">https://www.bilibili.com/video/BV1gw41197hV/?vd_source=9de61aecdd9fb684e546d032ef7fe7bf</a><details>
<summary>Abstract</summary>
Multi-agent reinforcement learning based methods are significant for online planning of feasible and safe paths for agents in dynamic and uncertain scenarios. Although some methods like fully centralized and fully decentralized methods achieve a certain measure of success, they also encounter problems such as dimension explosion and poor convergence, respectively. In this paper, we propose a novel centralized training with decentralized execution method based on multi-agent reinforcement learning to solve the dynamic obstacle avoidance problem online. In this approach, each agent communicates only with the central planner or only with its neighbors, respectively, to plan feasible and safe paths online. We improve our methods based on the idea of model predictive control to increase the training efficiency and sample utilization of agents. The experimental results in both simulation, indoor, and outdoor environments validate the effectiveness of our method. The video is available at https://www.bilibili.com/video/BV1gw41197hV/?vd_source=9de61aecdd9fb684e546d032ef7fe7bf
</details>
<details>
<summary>摘要</summary>
多智能体学习基于方法在线规划可行安全的路径 для智能体在动态不确定的enario中是非常重要的。尽管有些方法，如完全中央化和完全分布式方法，在某种程度上达到了成功，但它们也遇到了维度爆发和优化问题，分别。在这篇论文中，我们提出了一种新的中央训练与分布式执行方法，基于多智能体学习来解决动态障碍避免问题在线。在这种方法中，每个智能体只与中央规划器或只与其他邻居进行交流，以在线规划可行安全的路径。我们通过模型预测控制的想法来提高我们的方法的训练效率和智能体的样本利用率。实验结果表明，我们的方法在 simulate、indoor和outdoor环境中具有效果。视频可以在https://www.bilibili.com/video/BV1gw41197hV/?vd_source=9de61aecdd9fb684e546d032ef7fe7bf找到。
</details></li>
</ul>
<hr>
<h2 id="Towards-Control-Centric-Representations-in-Reinforcement-Learning-from-Images"><a href="#Towards-Control-Centric-Representations-in-Reinforcement-Learning-from-Images" class="headerlink" title="Towards Control-Centric Representations in Reinforcement Learning from Images"></a>Towards Control-Centric Representations in Reinforcement Learning from Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16655">http://arxiv.org/abs/2310.16655</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Liu, Hongyu Zang, Xin Li, Yong Heng, Yifei Wang, Zhen Fang, Yisen Wang, Mingzhong Wang</li>
<li>for: 解决图像基于奖励学习中的实用性和挑战性问题</li>
<li>methods:  integrate reward-free control information和奖励特定知识，使用 transformer 架构模型动力学，并使用块级卷积来消除时空重复信息</li>
<li>results: 在 Atari 游戏和 DeepMind Control Suit 两大标准 bencmark 上表现出色，比现有方法有更高的性能，证明其有效性<details>
<summary>Abstract</summary>
Image-based Reinforcement Learning is a practical yet challenging task. A major hurdle lies in extracting control-centric representations while disregarding irrelevant information. While approaches that follow the bisimulation principle exhibit the potential in learning state representations to address this issue, they still grapple with the limited expressive capacity of latent dynamics and the inadaptability to sparse reward environments. To address these limitations, we introduce ReBis, which aims to capture control-centric information by integrating reward-free control information alongside reward-specific knowledge. ReBis utilizes a transformer architecture to implicitly model the dynamics and incorporates block-wise masking to eliminate spatiotemporal redundancy. Moreover, ReBis combines bisimulation-based loss with asymmetric reconstruction loss to prevent feature collapse in environments with sparse rewards. Empirical studies on two large benchmarks, including Atari games and DeepMind Control Suit, demonstrate that ReBis has superior performance compared to existing methods, proving its effectiveness.
</details>
<details>
<summary>摘要</summary>
图像基于的再强化学习是一项实用又挑战性的任务。主要难点在于提取控制中心的表示，而不考虑无关的信息。尽管遵循 bisimulation 原理的方法表现出了学习状态表示的潜在力，但它们仍然面临着卷积动力的有限表达能力和缺乏适应环境中的稀薄奖励的问题。为解决这些限制，我们介绍了 ReBis，它通过结合奖励free控制信息和奖励特有的知识来捕捉控制中心的信息。ReBis 使用 transformer 架构来隐式地模型动力学，并在块级别屏蔽中除掉空间时间的重复性。此外，ReBis 结合 bisimulation 基于的损失与不对称重建loss 来避免特征塌沦的问题。实验研究在 Atari 游戏和 DeepMind Control Suit 两个大 benchmark 上表明，ReBis 与现有方法相比有较高的性能，证明其效果。
</details></li>
</ul>
<hr>
<h2 id="How-Robust-is-Federated-Learning-to-Communication-Error-A-Comparison-Study-Between-Uplink-and-Downlink-Channels"><a href="#How-Robust-is-Federated-Learning-to-Communication-Error-A-Comparison-Study-Between-Uplink-and-Downlink-Channels" class="headerlink" title="How Robust is Federated Learning to Communication Error? A Comparison Study Between Uplink and Downlink Channels"></a>How Robust is Federated Learning to Communication Error? A Comparison Study Between Uplink and Downlink Channels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16652">http://arxiv.org/abs/2310.16652</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linping Qu, Shenghui Song, Chi-Ying Tsui, Yuyi Mao</li>
<li>for: 这篇论文探讨了 federated learning（FL）在无线网络上的稳定性，具体来说，它研究了 FL 对于上行和下行通信错误的抗预测性。</li>
<li>methods: 该论文使用了理论分析方法，探讨了 FL 中两个关键参数（客户端数量和模型参数范围）对稳定性的影响，并提出了一个公式来量化上行和下行通信错误之间的差异。</li>
<li>results: 研究发现，FL 在上行通信错误情况下可以忍受更高的比特错误率（BER），并且与客户端数量和模型参数范围有关。这些结论得到了实验 validate。<details>
<summary>Abstract</summary>
Because of its privacy-preserving capability, federated learning (FL) has attracted significant attention from both academia and industry. However, when being implemented over wireless networks, it is not clear how much communication error can be tolerated by FL. This paper investigates the robustness of FL to the uplink and downlink communication error. Our theoretical analysis reveals that the robustness depends on two critical parameters, namely the number of clients and the numerical range of model parameters. It is also shown that the uplink communication in FL can tolerate a higher bit error rate (BER) than downlink communication, and this difference is quantified by a proposed formula. The findings and theoretical analyses are further validated by extensive experiments.
</details>
<details>
<summary>摘要</summary>
因其隐私保护能力，联合学习（FL）已经吸引了学术界和产业界的广泛关注。然而，在无线网络上实现FL时，不清楚FC可以tolerate多少communication error。这篇论文研究了FL对上行和下行通信错误的Robustness。我们的理论分析表明，FL的Robustness取决于两个关键参数：客户端数量和模型参数的数值范围。此外，我们还发现了FL的上行通信可以tolerate较高的比特错误率（BER），相比下行通信，这个差异由我们提出的公式进行了详细描述。实验结果也验证了我们的理论分析。
</details></li>
</ul>
<hr>
<h2 id="Posterior-Consistency-for-Missing-Data-in-Variational-Autoencoders"><a href="#Posterior-Consistency-for-Missing-Data-in-Variational-Autoencoders" class="headerlink" title="Posterior Consistency for Missing Data in Variational Autoencoders"></a>Posterior Consistency for Missing Data in Variational Autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16648">http://arxiv.org/abs/2310.16648</a></li>
<li>repo_url: None</li>
<li>paper_authors: Timur Sudak, Sebastian Tschiatschek</li>
<li>for: 学习带有缺失数据的变量自动机(VAEs)，以提高变量自动机的权重平衡和数据填充能力。</li>
<li>methods: 提出了一种 posterior consistency 定义和规范，以及一种基于这种定义的激活函数正则化方法，以促进变量自动机的后验分布的一致性。</li>
<li>results: 通过实验表明，该正则化方法可以提高缺失数据下的变量自动机的表现，包括增加了数据重建质量和下游任务中使用uncertainty的能力。此外，该方法可以在不同类型的 VAEs 中实现改进表现，包括 VAEs 携带流形。<details>
<summary>Abstract</summary>
We consider the problem of learning Variational Autoencoders (VAEs), i.e., a type of deep generative model, from data with missing values. Such data is omnipresent in real-world applications of machine learning because complete data is often impossible or too costly to obtain. We particularly focus on improving a VAE's amortized posterior inference, i.e., the encoder, which in the case of missing data can be susceptible to learning inconsistent posterior distributions regarding the missingness. To this end, we provide a formal definition of posterior consistency and propose an approach for regularizing an encoder's posterior distribution which promotes this consistency. We observe that the proposed regularization suggests a different training objective than that typically considered in the literature when facing missing values. Furthermore, we empirically demonstrate that our regularization leads to improved performance in missing value settings in terms of reconstruction quality and downstream tasks utilizing uncertainty in the latent space. This improved performance can be observed for many classes of VAEs including VAEs equipped with normalizing flows.
</details>
<details>
<summary>摘要</summary>
我团队考虑了使用 Variational Autoencoders (VAEs) 学习，即深度生成模型，从数据中缺失值处理。这种数据在实际机器学习应用中很普遍，因为完整的数据往往是不可逾或者太Costly来获得的。我们特别关注在缺失数据中改进 VAE 的权重平均推理，即encoder，因为在缺失数据中，encoder可能会学习不一致的 posterior 分布。为此，我们提出了 posterior 一致性的正式定义，并提议一种对 encoder 的 posterior 分布进行规范化，以便促进一致性。我们发现，我们的规范化建议一个与常见在缺失值情况下考虑的培训目标不同的训练目标。此外，我们在实验中观察到，我们的规范化可以在缺失值情况下提高 VAE 的表现，包括减少缺失值的重建质量和在隐藏空间中使用不确定性来进行下游任务。这种提高表现可以观察到许多类型的 VAEs，包括 VAEs 配置有流形函数。
</details></li>
</ul>
<hr>
<h2 id="Achieving-Constraints-in-Neural-Networks-A-Stochastic-Augmented-Lagrangian-Approach"><a href="#Achieving-Constraints-in-Neural-Networks-A-Stochastic-Augmented-Lagrangian-Approach" class="headerlink" title="Achieving Constraints in Neural Networks: A Stochastic Augmented Lagrangian Approach"></a>Achieving Constraints in Neural Networks: A Stochastic Augmented Lagrangian Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16647">http://arxiv.org/abs/2310.16647</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diogo Lavado, Cláudia Soares, Alessandra Micheletti</li>
<li>For: 这个论文的目的是提高深度神经网络（DNNs）的通用化和避免过拟合。* Methods: 我们提出了一种新的DNN调理方法，将训练过程架构为具有限制的对抗问题，并使用Stochastic Augmented Lagrangian（SAL）方法来实现更加灵活和高效的调理机制。* Results: 实验结果显示，SAL方法可以在image-based classification tasks中实现更高的准确率，同时也可以更好地满足限制，显示其在受限设定下优化DNNs的潜力。<details>
<summary>Abstract</summary>
Regularizing Deep Neural Networks (DNNs) is essential for improving generalizability and preventing overfitting. Fixed penalty methods, though common, lack adaptability and suffer from hyperparameter sensitivity. In this paper, we propose a novel approach to DNN regularization by framing the training process as a constrained optimization problem. Where the data fidelity term is the minimization objective and the regularization terms serve as constraints. Then, we employ the Stochastic Augmented Lagrangian (SAL) method to achieve a more flexible and efficient regularization mechanism. Our approach extends beyond black-box regularization, demonstrating significant improvements in white-box models, where weights are often subject to hard constraints to ensure interpretability. Experimental results on image-based classification on MNIST, CIFAR10, and CIFAR100 datasets validate the effectiveness of our approach. SAL consistently achieves higher Accuracy while also achieving better constraint satisfaction, thus showcasing its potential for optimizing DNNs under constrained settings.
</details>
<details>
<summary>摘要</summary>
Deep Neural Networks (DNNs) 的常规化是提高普适性和避免过拟合的关键。固定罚金方法，虽然广泛使用，但缺乏适应性和参数敏感性。在这篇论文中，我们提出了一种新的 DNN 常规化方法，将训练过程框定为一个具有数据准确度对象和常规化约束的归一化优化问题。然后，我们使用 Stochastic Augmented Lagrangian（SAL）方法来实现更加灵活和高效的常规化机制。我们的方法不仅超越黑盒常规化，而且在白盒模型中，通常有硬件约束来保证解释性。我们对 MNIST、CIFAR10 和 CIFAR100 等图像分类 dataset 进行了实验，并证明了我们的方法的有效性。SAL 在具有约束的情况下可以具有更高的准确率和更好的约束满足度，这表明它在受约束的情况下可以优化 DNNs。
</details></li>
</ul>
<hr>
<h2 id="Model-predictive-control-based-value-estimation-for-efficient-reinforcement-learning"><a href="#Model-predictive-control-based-value-estimation-for-efficient-reinforcement-learning" class="headerlink" title="Model predictive control-based value estimation for efficient reinforcement learning"></a>Model predictive control-based value estimation for efficient reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16646">http://arxiv.org/abs/2310.16646</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qizhen Wu, Kexin Liu, Lei Chen</li>
<li>for: 提高 reinforcement learning 在实际应用中的效率，因为现实环境中需要大量交互。</li>
<li>methods: 基于模型预测控制的改进 reinforcement learning 方法，通过数据驱动的环境模型来预测值函数和优化策略。</li>
<li>results: 方法在经典数据库和无人机避险场景中实验 validate，显示了更高的学习效率，更快的策略倾向于优化值，以及更少的经验回放缓存空间需求。<details>
<summary>Abstract</summary>
Reinforcement learning suffers from limitations in real practices primarily due to the numbers of required interactions with virtual environments. It results in a challenging problem that we are implausible to obtain an optimal strategy only with a few attempts for many learning method. Hereby, we design an improved reinforcement learning method based on model predictive control that models the environment through a data-driven approach. Based on learned environmental model, it performs multi-step prediction to estimate the value function and optimize the policy. The method demonstrates higher learning efficiency, faster convergent speed of strategies tending to the optimal value, and fewer sample capacity space required by experience replay buffers. Experimental results, both in classic databases and in a dynamic obstacle avoidance scenario for unmanned aerial vehicle, validate the proposed approaches.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Reinforcement learning suffers from limitations in real practices primarily due to the numbers of required interactions with virtual environments. It results in a challenging problem that we are implausible to obtain an optimal strategy only with a few attempts for many learning method. Hereby, we design an improved reinforcement learning method based on model predictive control that models the environment through a data-driven approach. Based on learned environmental model, it performs multi-step prediction to estimate the value function and optimize the policy. The method demonstrates higher learning efficiency, faster convergent speed of strategies tending to the optimal value, and fewer sample capacity space required by experience replay buffers. Experimental results, both in classic databases and in a dynamic obstacle avoidance scenario for unmanned aerial vehicle, validate the proposed approaches." into Simplified Chinese.Here's the translation:<<SYS>>现实中实施的增强学习受到环境份的限制，主要是因为需要与虚拟环境进行多次互动。这实际上是一个困难的问题，我们很难在几次尝试后就获得最佳策略。为此，我们设计了一个基于预测控制的改进增强学习方法。这个方法通过数据驱动的方法来建立环境模型，然后使用预测多步来估算值函数和优化策略。这个方法在学习效率、策略趋向最佳值的速度和经验练习空间的需求方面具有更高的表现。实验结果，包括 классические数据库和无人飞行器避障enario，证实了我们的提案。
</details></li>
</ul>
<hr>
<h2 id="Robust-Covariate-Shift-Adaptation-for-Density-Ratio-Estimation"><a href="#Robust-Covariate-Shift-Adaptation-for-Density-Ratio-Estimation" class="headerlink" title="Robust Covariate Shift Adaptation for Density-Ratio Estimation"></a>Robust Covariate Shift Adaptation for Density-Ratio Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16638">http://arxiv.org/abs/2310.16638</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masahiro Kato</li>
<li>For: 预测测试数据中缺失的结果。* Methods: 使用重要性权重法和双重机器学习技术来适应covariate shift，并提出了一种双重可靠估计器来减少density ratio估计误差所带来的偏差。* Results: 通过实验研究表明，提出的方法可以减少density ratio估计误差所带来的偏差，并且方法 remains consistent if either the density ratio estimator or the regression function is consistent。<details>
<summary>Abstract</summary>
Consider a scenario where we have access to train data with both covariates and outcomes while test data only contains covariates. In this scenario, our primary aim is to predict the missing outcomes of the test data. With this objective in mind, we train parametric regression models under a covariate shift, where covariate distributions are different between the train and test data. For this problem, existing studies have proposed covariate shift adaptation via importance weighting using the density ratio. This approach averages the train data losses, each weighted by an estimated ratio of the covariate densities between the train and test data, to approximate the test-data risk. Although it allows us to obtain a test-data risk minimizer, its performance heavily relies on the accuracy of the density ratio estimation. Moreover, even if the density ratio can be consistently estimated, the estimation errors of the density ratio also yield bias in the estimators of the regression model's parameters of interest. To mitigate these challenges, we introduce a doubly robust estimator for covariate shift adaptation via importance weighting, which incorporates an additional estimator for the regression function. Leveraging double machine learning techniques, our estimator reduces the bias arising from the density ratio estimation errors. We demonstrate the asymptotic distribution of the regression parameter estimator. Notably, our estimator remains consistent if either the density ratio estimator or the regression function is consistent, showcasing its robustness against potential errors in density ratio estimation. Finally, we confirm the soundness of our proposed method via simulation studies.
</details>
<details>
<summary>摘要</summary>
假设我们有训练数据包含covariates和结果，而测试数据只包含covariates。在这种情况下，我们的主要目标是预测测试数据中缺失的结果。为了实现这个目标，我们在covariate shift下训练参数化回归模型，其中covariate分布在训练和测试数据中不同。现有的研究已经提出了covariate shift适应via重要性权重使用density比率。这种方法将训练数据的损失平均值，每个权重为训练和测试数据中covariate分布之间的density比率估计值，以 Approximate测试数据的风险。although it allows us to obtain a test-data risk minimizer, its performance heavily relies on the accuracy of the density ratio estimation. Moreover, even if the density ratio can be consistently estimated, the estimation errors of the density ratio also yield bias in the estimators of the regression model's parameters of interest.To mitigate these challenges, we introduce a doubly robust estimator for covariate shift adaptation via importance weighting, which incorporates an additional estimator for the regression function. Leveraging double machine learning techniques, our estimator reduces the bias arising from the density ratio estimation errors. We demonstrate the asymptotic distribution of the regression parameter estimator. Notably, our estimator remains consistent if either the density ratio estimator or the regression function is consistent, showcasing its robustness against potential errors in density ratio estimation. Finally, we confirm the soundness of our proposed method via simulation studies.
</details></li>
</ul>
<hr>
<h2 id="Photometric-Redshifts-with-Copula-Entropy"><a href="#Photometric-Redshifts-with-Copula-Entropy" class="headerlink" title="Photometric Redshifts with Copula Entropy"></a>Photometric Redshifts with Copula Entropy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16633">http://arxiv.org/abs/2310.16633</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/majianthu/quasar">https://github.com/majianthu/quasar</a></li>
<li>paper_authors: Jian Ma</li>
<li>for: 应用 copula entropy (CE) 来提高光度谱的准确性。</li>
<li>methods: 使用 CE 测量光度谱和光度测量之间的相关性，并选择高 CE 值的测量来预测红移。</li>
<li>results: 实验结果表明，使用选择的测量（包括 luminosity 磁场、U 频率带的标准差和其他四个频率带的磁场）可以提高光度谱的准确性，特别是对高红移样本的预测。<details>
<summary>Abstract</summary>
In this paper we propose to apply copula entropy (CE) to photometric redshifts. CE is used to measure the correlations between photometric measurements and redshifts and then the measurements associated with high CEs are selected for predicting redshifts. We verified the proposed method on the SDSS quasar data. Experimental results show that the accuracy of photometric redshifts is improved with the selected measurements compared to the results with all the measurements used in the experiments, especially for the samples with high redshifts. The measurements selected with CE include luminosity magnitude, the brightness in ultraviolet band with standard deviation, and the brightness of the other four bands. Since CE is a rigorously defined mathematical concept, the models such derived is interpretable.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提议使用 copula  entropy (CE) 来应用于光度谱。 CE 用于测量光度谱和光度测量之间的相关性，然后选择具有高 CE 的测量来预测谱。我们对 SDSS квазар数据进行验证。实验结果表明，使用选择的测量比使用所有测量来预测谱的准确性更高，特别是高红shift 样本中。选择 CE 的测量包括照度大小、标准差 ultraviolet 频谱亮度和其他四个频谱的亮度。由于 CE 是一种严格定义的数学概念，所 derivated 的模型是可解释的。
</details></li>
</ul>
<hr>
<h2 id="Free-form-Flows-Make-Any-Architecture-a-Normalizing-Flow"><a href="#Free-form-Flows-Make-Any-Architecture-a-Normalizing-Flow" class="headerlink" title="Free-form Flows: Make Any Architecture a Normalizing Flow"></a>Free-form Flows: Make Any Architecture a Normalizing Flow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16624">http://arxiv.org/abs/2310.16624</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vislearn/fff">https://github.com/vislearn/fff</a></li>
<li>paper_authors: Felix Draxler, Peter Sorrenson, Lea Zimmermann, Armand Rousselot, Ullrich Köthe</li>
<li>for: 本研究旨在提高Normalizing Flows的设计领域，使其能够更加灵活地适应具体任务。</li>
<li>methods: 本研究使用一种高效的梯度估计器，允许任意维度保持的神经网络作为生成模型进行最大化 posterior probability 训练。</li>
<li>results: 研究人员在分子生成和反问题 benchmark 中获得了优秀的结果，并在使用存在权重 ResNet 架构的情况下与比较势力竞争。<details>
<summary>Abstract</summary>
Normalizing Flows are generative models that directly maximize the likelihood. Previously, the design of normalizing flows was largely constrained by the need for analytical invertibility. We overcome this constraint by a training procedure that uses an efficient estimator for the gradient of the change of variables formula. This enables any dimension-preserving neural network to serve as a generative model through maximum likelihood training. Our approach allows placing the emphasis on tailoring inductive biases precisely to the task at hand. Specifically, we achieve excellent results in molecule generation benchmarks utilizing $E(n)$-equivariant networks. Moreover, our method is competitive in an inverse problem benchmark, while employing off-the-shelf ResNet architectures.
</details>
<details>
<summary>摘要</summary>
正常化流是一种生成模型，直接 maximize 可能性。在过去，normalizing flow的设计受限于需要分析逆变换的需要。我们通过一种高效的梯度估计器来缓解这个限制，使任何维度保持的神经网络可以作为生成模型通过最大化可能性来训练。我们的方法允许在任务上加入适合的启发性权重，并在分子生成数据集上达到极高的性能。此外，我们的方法在反问题数据集上也具有竞争力，只使用商业化 ResNet 架构。
</details></li>
</ul>
<hr>
<h2 id="SpikingJelly-An-open-source-machine-learning-infrastructure-platform-for-spike-based-intelligence"><a href="#SpikingJelly-An-open-source-machine-learning-infrastructure-platform-for-spike-based-intelligence" class="headerlink" title="SpikingJelly: An open-source machine learning infrastructure platform for spike-based intelligence"></a>SpikingJelly: An open-source machine learning infrastructure platform for spike-based intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16620">http://arxiv.org/abs/2310.16620</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fangwei123456/spikingjelly">https://github.com/fangwei123456/spikingjelly</a></li>
<li>paper_authors: Wei Fang, Yanqi Chen, Jianhao Ding, Zhaofei Yu, Timothée Masquelier, Ding Chen, Liwei Huang, Huihui Zhou, Guoqi Li, Yonghong Tian</li>
<li>for: 这个论文旨在实现基于神经元逻辑芯片的高效能脑机智能，通过引入神经动力和射频性质。</li>
<li>methods: 这篇论文提出了名为SpikingJelly的全栈工具箱，用于预处理神经动力数据集、建立深度神经网络、优化参数和部署神经动力网络在神经元逻辑芯片上。相比现有方法，SpikingJelly可以加速深度神经网络的训练 $11\times$。</li>
<li>results: SpikingJelly提供了高度可扩展和灵活的工具箱，可以帮助用户在低成本下加速自定义模型，通过多层继承和自动代码生成。SpikingJelly开创了高效能神经动力网络基于机器智能系统的 synthesis 领域，这将扩展神经元计算生态系统。<details>
<summary>Abstract</summary>
Spiking neural networks (SNNs) aim to realize brain-inspired intelligence on neuromorphic chips with high energy efficiency by introducing neural dynamics and spike properties. As the emerging spiking deep learning paradigm attracts increasing interest, traditional programming frameworks cannot meet the demands of the automatic differentiation, parallel computation acceleration, and high integration of processing neuromorphic datasets and deployment. In this work, we present the SpikingJelly framework to address the aforementioned dilemma. We contribute a full-stack toolkit for pre-processing neuromorphic datasets, building deep SNNs, optimizing their parameters, and deploying SNNs on neuromorphic chips. Compared to existing methods, the training of deep SNNs can be accelerated $11\times$, and the superior extensibility and flexibility of SpikingJelly enable users to accelerate custom models at low costs through multilevel inheritance and semiautomatic code generation. SpikingJelly paves the way for synthesizing truly energy-efficient SNN-based machine intelligence systems, which will enrich the ecology of neuromorphic computing.
</details>
<details>
<summary>摘要</summary>
聚凝神经网络（SNN）目标实现基于神经元模拟芯片的高效能智能，通过引入神经动力学和冲击特性。随着emerging spiking deep learning paradigm吸引越来越多的关注，传统的编程框架无法满足自动微分、并行计算加速和高集成处理神经元数据的需求。在这个工作中，我们提出SpikingJelly框架，以解决上述困境。我们提供了全栈工具箱，用于预处理神经元数据、建立深度SNN、优化参数和部署SNN在神经元模拟芯片上。与现有方法相比，SpikingJelly可以加速深度SNN的训练$11\times$,并且其出色的可扩展性和灵活性使得用户可以在低成本下加速自定义模型，通过多级继承和semiautomatic code generation。SpikingJelly开创了真正能效的SNN-基于机器智能系统的合成，这将丰富神经元计算生态。
</details></li>
</ul>
<hr>
<h2 id="Performative-Prediction-Past-and-Future"><a href="#Performative-Prediction-Past-and-Future" class="headerlink" title="Performative Prediction: Past and Future"></a>Performative Prediction: Past and Future</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16608">http://arxiv.org/abs/2310.16608</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/salmansust/Machine-Learning-TSF-Petroleum-Production">https://github.com/salmansust/Machine-Learning-TSF-Petroleum-Production</a></li>
<li>paper_authors: Moritz Hardt, Celestine Mendler-Dünner</li>
<li>for: 这篇论文主要关注的是机器学习预测的表现力和其对数据生成分布的影响。</li>
<li>methods: 该论文使用了定义和概念框架来研究机器学习中的表现力，并提出了一种自然平衡概念和学习与导航两种机制的分类。</li>
<li>results: 该论文发现了机器学习预测可能会导致数据生成分布的变化，并提出了一种新的优化挑战。同时，论文还探讨了数字市场中平台对参与者的导航问题。<details>
<summary>Abstract</summary>
Predictions in the social world generally influence the target of prediction, a phenomenon known as performativity. Self-fulfilling and self-negating predictions are examples of performativity. Of fundamental importance to economics, finance, and the social sciences, the notion has been absent from the development of machine learning. In machine learning applications, performativity often surfaces as distribution shift. A predictive model deployed on a digital platform, for example, influences consumption and thereby changes the data-generating distribution. We survey the recently founded area of performative prediction that provides a definition and conceptual framework to study performativity in machine learning. A consequence of performative prediction is a natural equilibrium notion that gives rise to new optimization challenges. Another consequence is a distinction between learning and steering, two mechanisms at play in performative prediction. The notion of steering is in turn intimately related to questions of power in digital markets. We review the notion of performative power that gives an answer to the question how much a platform can steer participants through its predictions. We end on a discussion of future directions, such as the role that performativity plays in contesting algorithmic systems.
</details>
<details>
<summary>摘要</summary>
社会世界中的预测通常会影响预测目标，这种现象被称为表现力。自我实现和自我否定的预测是表现力的例子。 econometrics, finance and social sciences of great importance, this concept has been absent from the development of machine learning. In machine learning applications, performativity often appears as distribution shift. A predictive model deployed on a digital platform, for example, influences consumption and changes the data-generating distribution. We survey the recently founded area of performative prediction, which provides a definition and conceptual framework to study performativity in machine learning. A consequence of performative prediction is a natural equilibrium notion that gives rise to new optimization challenges. Another consequence is a distinction between learning and steering, two mechanisms at play in performative prediction. The notion of steering is closely related to questions of power in digital markets. We review the notion of performative power, which answers the question of how much a platform can steer participants through its predictions. We end with a discussion of future directions, such as the role that performativity plays in contesting algorithmic systems.Note: Please note that the translation is in Simplified Chinese, and the word order and sentence structure may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="AirFL-Mem-Improving-Communication-Learning-Trade-Off-by-Long-Term-Memory"><a href="#AirFL-Mem-Improving-Communication-Learning-Trade-Off-by-Long-Term-Memory" class="headerlink" title="AirFL-Mem: Improving Communication-Learning Trade-Off by Long-Term Memory"></a>AirFL-Mem: Improving Communication-Learning Trade-Off by Long-Term Memory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16606">http://arxiv.org/abs/2310.16606</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haifeng Wen, Hong Xing, Osvaldo Simeone</li>
<li>for: 提高 federated learning（FL）中的通信瓶颈问题，随空间FL（AirFL）已经出现为一种有前途的解决方案，但是它受到深层折射条件的妨碍。</li>
<li>methods: 本文提出了 AirFL-Mem，一种利用长期记忆机制来减轻深层折射的影响的新方案。提供了对于普通非对称目标函数的收敛界限，包括长期记忆和现有AirFL变体的短期记忆。</li>
<li>results: 理论分析表明，AirFL-Mem 能够与理想的通信情况下的 FedAvg 达到同样的收敛速率，而现有方案通常受到错误底值的限制。同时，提出了一种基于几何programming的 convex 优化策略来调整 truncation 阈值以实现功率控制在RAYLEIGH 折射通道上。实验结果证明了理论分析的正确性，并证明了长期记忆机制对深层折射的减轻的优势。<details>
<summary>Abstract</summary>
Addressing the communication bottleneck inherent in federated learning (FL), over-the-air FL (AirFL) has emerged as a promising solution, which is, however, hampered by deep fading conditions. In this paper, we propose AirFL-Mem, a novel scheme designed to mitigate the impact of deep fading by implementing a \emph{long-term} memory mechanism. Convergence bounds are provided that account for long-term memory, as well as for existing AirFL variants with short-term memory, for general non-convex objectives. The theory demonstrates that AirFL-Mem exhibits the same convergence rate of federated averaging (FedAvg) with ideal communication, while the performance of existing schemes is generally limited by error floors. The theoretical results are also leveraged to propose a novel convex optimization strategy for the truncation threshold used for power control in the presence of Rayleigh fading channels. Experimental results validate the analysis, confirming the advantages of a long-term memory mechanism for the mitigation of deep fading.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:addressing the communication bottleneck inherent in federated learning (FL), over-the-air FL (AirFL) has emerged as a promising solution, which is, however, hampered by deep fading conditions. in this paper, we propose AirFL-Mem, a novel scheme designed to mitigate the impact of deep fading by implementing a \emph{long-term} memory mechanism. convergence bounds are provided that account for long-term memory, as well as for existing AirFL variants with short-term memory, for general non-convex objectives. the theory demonstrates that AirFL-Mem exhibits the same convergence rate of federated averaging (FedAvg) with ideal communication, while the performance of existing schemes is generally limited by error floors. the theoretical results are also leveraged to propose a novel convex optimization strategy for the truncation threshold used for power control in the presence of Rayleigh fading channels. experimental results validate the analysis, confirming the advantages of a long-term memory mechanism for the mitigation of deep fading.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing.
</details></li>
</ul>
<hr>
<h2 id="Parcel-loss-prediction-in-last-mile-delivery-deep-and-non-deep-approaches-with-insights-from-Explainable-AI"><a href="#Parcel-loss-prediction-in-last-mile-delivery-deep-and-non-deep-approaches-with-insights-from-Explainable-AI" class="headerlink" title="Parcel loss prediction in last-mile delivery: deep and non-deep approaches with insights from Explainable AI"></a>Parcel loss prediction in last-mile delivery: deep and non-deep approaches with insights from Explainable AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16602">http://arxiv.org/abs/2310.16602</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan de Leeuw, Zaharah Bukhsh, Yingqian Zhang<br>for: 降低电子商务 послед个分段配送阶段的包裹丢失是industry中一个重要的目标。methods: 本文提出了两种机器学习方法，namely Data Balance with Supervised Learning (DBSL)和 Deep Hybrid Ensemble Learning (DHEL)，以准确预测包裹丢失。results: 我们对一年的比利时船运数据进行了全面的评估，发现 DHEL 模型，它将feed-forward autoencoder与随机森林结合，实现了最高的分类性能。<details>
<summary>Abstract</summary>
Within the domain of e-commerce retail, an important objective is the reduction of parcel loss during the last-mile delivery phase. The ever-increasing availability of data, including product, customer, and order information, has made it possible for the application of machine learning in parcel loss prediction. However, a significant challenge arises from the inherent imbalance in the data, i.e., only a very low percentage of parcels are lost. In this paper, we propose two machine learning approaches, namely, Data Balance with Supervised Learning (DBSL) and Deep Hybrid Ensemble Learning (DHEL), to accurately predict parcel loss. The practical implication of such predictions is their value in aiding e-commerce retailers in optimizing insurance-related decision-making policies. We conduct a comprehensive evaluation of the proposed machine learning models using one year data from Belgian shipments. The findings show that the DHEL model, which combines a feed-forward autoencoder with a random forest, achieves the highest classification performance. Furthermore, we use the techniques from Explainable AI (XAI) to illustrate how prediction models can be used in enhancing business processes and augmenting the overall value proposition for e-commerce retailers in the last mile delivery.
</details>
<details>
<summary>摘要</summary>
在电商零售领域，一个重要的目标是减少最后一英里配送阶段的包裹产生损失。随着数据的普遍化，包括产品、顾客和订单信息，可以通过机器学习来预测包裹损失。然而，数据的内生偏见成为了一个主要挑战，即只有非常低的百分比的包裹会产生损失。在这篇论文中，我们提出了两种机器学习方法，namely Data Balance with Supervised Learning (DBSL)和Deep Hybrid Ensemble Learning (DHEL)，以准确预测包裹损失。实际上，这些预测结果对电商零售商来说具有很大的实用价值，可以帮助他们优化保险相关的决策政策。我们使用了一年的比利时船运数据进行了全面的评估。发现DHEL模型，它将Feed-Forward Autoencoder与随机森林相结合，实现了最高的分类性能。此外，我们使用了Explainable AI（XAI）技术，以示如何使用预测模型来增强业务过程，并增加电商零售商在最后一英里配送阶段的总价值。
</details></li>
</ul>
<hr>
<h2 id="Beyond-IID-weights-sparse-and-low-rank-deep-Neural-Networks-are-also-Gaussian-Processes"><a href="#Beyond-IID-weights-sparse-and-low-rank-deep-Neural-Networks-are-also-Gaussian-Processes" class="headerlink" title="Beyond IID weights: sparse and low-rank deep Neural Networks are also Gaussian Processes"></a>Beyond IID weights: sparse and low-rank deep Neural Networks are also Gaussian Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16597">http://arxiv.org/abs/2310.16597</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thiziri Nait-Saada, Alireza Naderi, Jared Tanner</li>
<li>for: 这 paper 探讨了深度学习中的许多现象，例如深度网络的训练动态和 activation function 的选择对训练过程的影响。</li>
<li>methods: 作者使用了 Matthews et al. (2018) 的证明，将其推广到更大的初始权重分布（即 PSEUDO-IID）中，包括已知的 IID 和 orthogonal 权重，以及低级和结构化缺失的设置。</li>
<li>results: 作者显示了 initialized 的 fully-connected 和 convolutional 网络，均可以 viewed as effectively equivalent up to their variance，并且可以通过 tuning 网络在 Edge-of-Chaos 附近进行增强训练。<details>
<summary>Abstract</summary>
The infinitely wide neural network has been proven a useful and manageable mathematical model that enables the understanding of many phenomena appearing in deep learning. One example is the convergence of random deep networks to Gaussian processes that allows a rigorous analysis of the way the choice of activation function and network weights impacts the training dynamics. In this paper, we extend the seminal proof of Matthews et al. (2018) to a larger class of initial weight distributions (which we call PSEUDO-IID), including the established cases of IID and orthogonal weights, as well as the emerging low-rank and structured sparse settings celebrated for their computational speed-up benefits. We show that fully-connected and convolutional networks initialized with PSEUDO-IID distributions are all effectively equivalent up to their variance. Using our results, one can identify the Edge-of-Chaos for a broader class of neural networks and tune them at criticality in order to enhance their training.
</details>
<details>
<summary>摘要</summary>
“无穷深度神经网络”已经被证明为一个有用且可控的数学模型，帮助理解深度学习中的许多现象。一个例子是深度网络的趋向 Gaussian 过程，允许严谨地分析 activation function 和网络重量的选择对训练过程的影响。在这篇文章中，我们将 Matthews et al. (2018) 的著名证明扩展到更大的初始重量分布（我们称为 Pseudo-IID），包括已知的 IID 和对称重量设定，以及受欢迎的低维度和结构簇 sparse 设定，这些设定在 Computational speed-up 方面获得了优化。我们显示了完全连接和卷积神经网络，它们都可以使用 Pseudo-IID 分布初始化，并且它们的方差都是相同的。使用我们的结果，您可以在更广泛的神经网络中找到 Edge-of-Chaos，并将它们调整到 críticality 以提高它们的训练。
</details></li>
</ul>
<hr>
<h2 id="Over-the-air-Federated-Policy-Gradient"><a href="#Over-the-air-Federated-Policy-Gradient" class="headerlink" title="Over-the-air Federated Policy Gradient"></a>Over-the-air Federated Policy Gradient</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16592">http://arxiv.org/abs/2310.16592</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huiwen Yang, Lingying Huang, Subhrakanti Dey, Ling Shi</li>
<li>for: 这篇论文是关于大规模分布学习优化感知中的无线协同技术。</li>
<li>methods: 本文提出了无线联合策略偏微算法，所有代理都同时通过无线通道广播一个分析信号，中央控制器使用接收的总波形更新策略参数。</li>
<li>results: 文章研究无线协同策略偏微算法的精度和稳定性，并确定了通信和采样的复杂性。最后，文章还提供了一些实验结果，证明了算法的有效性。<details>
<summary>Abstract</summary>
In recent years, over-the-air aggregation has been widely considered in large-scale distributed learning, optimization, and sensing. In this paper, we propose the over-the-air federated policy gradient algorithm, where all agents simultaneously broadcast an analog signal carrying local information to a common wireless channel, and a central controller uses the received aggregated waveform to update the policy parameters. We investigate the effect of noise and channel distortion on the convergence of the proposed algorithm, and establish the complexities of communication and sampling for finding an $\epsilon$-approximate stationary point. Finally, we present some simulation results to show the effectiveness of the algorithm.
</details>
<details>
<summary>摘要</summary>
Recently, over-the-air aggregation has been widely considered in large-scale distributed learning, optimization, and sensing. In this paper, we propose the over-the-air federated policy gradient algorithm, where all agents simultaneously broadcast an analog signal carrying local information to a common wireless channel, and a central controller uses the received aggregated waveform to update the policy parameters. We investigate the effect of noise and channel distortion on the convergence of the proposed algorithm, and establish the complexities of communication and sampling for finding an $\epsilon$-approximate stationary point. Finally, we present some simulation results to show the effectiveness of the algorithm.Here is the translation in Traditional Chinese:近年来，透过空中集成已经广泛地考虑在大规模分布式学习、优化和感应中。在这篇文章中，我们提出了透过空中联邦策略梯度算法，所有代理都同时将本地资讯传送到共同无线通道上，中央控制器使用接收的总波形来更新策略参数。我们研究了噪音和通道扭曲对提案的均衡稳定点的影响，并估算了通信和抽样的复杂性。最后，我们显示了一些实验结果，以证明提案的有效性。
</details></li>
</ul>
<hr>
<h2 id="Multi-parallel-task-Time-delay-Reservoir-Computing-combining-a-Silicon-Microring-with-WDM"><a href="#Multi-parallel-task-Time-delay-Reservoir-Computing-combining-a-Silicon-Microring-with-WDM" class="headerlink" title="Multi-parallel-task Time-delay Reservoir Computing combining a Silicon Microring with WDM"></a>Multi-parallel-task Time-delay Reservoir Computing combining a Silicon Microring with WDM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16588">http://arxiv.org/abs/2310.16588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bernard J. Giron Castro, Christophe Peucheret, Darko Zibar, Francesco Da Ros</li>
<li>for: 这个论文用于解决三个任务，即时间序列预测、分类和无线通道准确。</li>
<li>methods: 论文使用微环扬的时间延迟储存方案，并对每个任务进行优化的功率和频率调整。</li>
<li>results: 论文在每个任务上都达到了状态eliae-of-the-art的性能，并且在多普勒谱上进行了时间序列预测、分类和无线通道准确。<details>
<summary>Abstract</summary>
We numerically demonstrate a microring-based time-delay reservoir computing scheme that simultaneously solves three tasks involving time-series prediction, classification, and wireless channel equalization. Each task performed on a wavelength-multiplexed channel achieves state-of-the-art performance with optimized power and frequency detuning.
</details>
<details>
<summary>摘要</summary>
我们数字 demonstrate一种基于微环的时间延迟储存计算方案，同时解决了三个任务，包括时间序列预测、分类和无线通道平衡。每个任务在一个波长多plexed通道上实现了状态 искусственный智能性能，并且通过优化功率和频率偏差来优化性能。
</details></li>
</ul>
<hr>
<h2 id="Mapping-the-magnetic-field-using-a-magnetometer-array-with-noisy-input-Gaussian-process-regression"><a href="#Mapping-the-magnetic-field-using-a-magnetometer-array-with-noisy-input-Gaussian-process-regression" class="headerlink" title="Mapping the magnetic field using a magnetometer array with noisy input Gaussian process regression"></a>Mapping the magnetic field using a magnetometer array with noisy input Gaussian process regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16577">http://arxiv.org/abs/2310.16577</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Edridge, Manon Kok</li>
<li>for: 室内环境中磁性材料会导致环境磁场的干扰，这些磁场干扰地图可以用于室内定位。</li>
<li>methods: 我们使用了 Gaussian 过程来学习磁场的空间变化大小，使用磁计的测量值和磁计的位置信息。</li>
<li>results: 我们的方法可以提高磁场地图的质量，并且通过实验数据示出了这种方法的有效性。<details>
<summary>Abstract</summary>
Ferromagnetic materials in indoor environments give rise to disturbances in the ambient magnetic field. Maps of these magnetic disturbances can be used for indoor localisation. A Gaussian process can be used to learn the spatially varying magnitude of the magnetic field using magnetometer measurements and information about the position of the magnetometer. The position of the magnetometer, however, is frequently only approximately known. This negatively affects the quality of the magnetic field map. In this paper, we investigate how an array of magnetometers can be used to improve the quality of the magnetic field map. The position of the array is approximately known, but the relative locations of the magnetometers on the array are known. We include this information in a novel method to make a map of the ambient magnetic field. We study the properties of our method in simulation and show that our method improves the map quality. We also demonstrate the efficacy of our method with experimental data for the mapping of the magnetic field using an array of 30 magnetometers.
</details>
<details>
<summary>摘要</summary>
ferromagnetic 物体在室内环境中会导致环境类极化场的干扰。使用探测器测量和磁场的位置信息，可以使用 Gaussian 程序学习磁场的空间各点的强度。但是探测器的位置仅仅知道大约，这会对磁场地图的质量产生负面影响。在这篇论文中，我们研究了如何使用数组探测器来改善磁场地图的质量。数组的位置大约知道，但是它们之间的相对位置仅知道。我们将这个信息添加到一种新的方法中，以生成更高质量的磁场地图。我们在实验中研究了这个方法的性能，并证明了它可以提高地图的质量。我们还使用实验数据来证明方法的有效性，使用了30个探测器进行磁场的映射。
</details></li>
</ul>
<hr>
<h2 id="Large-scale-magnetic-field-maps-using-structured-kernel-interpolation-for-Gaussian-process-regression"><a href="#Large-scale-magnetic-field-maps-using-structured-kernel-interpolation-for-Gaussian-process-regression" class="headerlink" title="Large-scale magnetic field maps using structured kernel interpolation for Gaussian process regression"></a>Large-scale magnetic field maps using structured kernel interpolation for Gaussian process regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16574">http://arxiv.org/abs/2310.16574</a></li>
<li>repo_url: None</li>
<li>paper_authors: Clara Menzen, Marnix Fetter, Manon Kok</li>
<li>for: 这篇论文是为了计算室内环境中大规模磁场图的映射而设计的。</li>
<li>methods: 这篇论文使用了一种基于核函数 interpolate（SKI）的方法，通过利用高效的 kronecker 子空间方法来加速推理。</li>
<li>results: 在 simulate 中，这种方法可以在磁场图的映射区域增加的精度，并在大规模实验中可以在两分钟内从40000个三维磁场测量数据中构建磁场图。<details>
<summary>Abstract</summary>
We present a mapping algorithm to compute large-scale magnetic field maps in indoor environments with approximate Gaussian process (GP) regression. Mapping the spatial variations in the ambient magnetic field can be used for localization algorithms in indoor areas. To compute such a map, GP regression is a suitable tool because it provides predictions of the magnetic field at new locations along with uncertainty quantification. Because full GP regression has a complexity that grows cubically with the number of data points, approximations for GPs have been extensively studied. In this paper, we build on the structured kernel interpolation (SKI) framework, speeding up inference by exploiting efficient Krylov subspace methods. More specifically, we incorporate SKI with derivatives (D-SKI) into the scalar potential model for magnetic field modeling and compute both predictive mean and covariance with a complexity that is linear in the data points. In our simulations, we show that our method achieves better accuracy than current state-of-the-art methods on magnetic field maps with a growing mapping area. In our large-scale experiments, we construct magnetic field maps from up to 40000 three-dimensional magnetic field measurements in less than two minutes on a standard laptop.
</details>
<details>
<summary>摘要</summary>
我们提出了一种映射算法，用于计算室内环境中大规模的磁场地图。这种地图可以用于室内定位算法。使用 Gaussian process（GP）回归，可以提供新位置的磁场预测，同时还可以Quantify the uncertainty of the prediction.然而，全GP回归的复杂度会随着数据点的增加而增加，因此有严格的研究。在这篇论文中，我们基于结构kernel interpolation（SKI）框架，通过高效的Krylov子空间方法加速推理。更 specifically，我们将SKI与 derivatives（D-SKI）纳入scalar potential模型中，并计算预测的mean和covariance，复杂度 linearly with the data points。在我们的 simulations中，我们发现我们的方法可以在增加的映射区域大小的情况下，与当前状态的方法相比，提高精度。在我们的大规模实验中，我们从40000个三维磁场测量中构建了磁场地图，并在标准笔记电脑上完成了 less than two minutes。
</details></li>
</ul>
<hr>
<h2 id="Model-enhanced-Contrastive-Reinforcement-Learning-for-Sequential-Recommendation"><a href="#Model-enhanced-Contrastive-Reinforcement-Learning-for-Sequential-Recommendation" class="headerlink" title="Model-enhanced Contrastive Reinforcement Learning for Sequential Recommendation"></a>Model-enhanced Contrastive Reinforcement Learning for Sequential Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16566">http://arxiv.org/abs/2310.16566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengpeng Li, Zhengyi Yang, Jizhi Zhang, Jiancan Wu, Dingxian Wang, Xiangnan He, Xiang Wang</li>
<li>for: 提高推荐系统的长期用户满意度，通过Markov决策过程（MDP）来形式化推荐，并使用了强化学习（RL）方法优化。</li>
<li>methods: 提出了一种名为模型强化对比强化学习（MCRL）的新的RL推荐器，通过同时学习价值函数和保守价值学习机制来缓解过度估计问题，并使用对比学习来利用MDP的内部结构信息来模型奖励函数和状态转移函数。</li>
<li>results: 实验结果表明，相比现有的OFFLINE RL和自动学习RL方法，MCRL方法在两个真实世界数据集上得到了显著的提升。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) has been widely applied in recommendation systems due to its potential in optimizing the long-term engagement of users. From the perspective of RL, recommendation can be formulated as a Markov decision process (MDP), where recommendation system (agent) can interact with users (environment) and acquire feedback (reward signals).However, it is impractical to conduct online interactions with the concern on user experience and implementation complexity, and we can only train RL recommenders with offline datasets containing limited reward signals and state transitions. Therefore, the data sparsity issue of reward signals and state transitions is very severe, while it has long been overlooked by existing RL recommenders.Worse still, RL methods learn through the trial-and-error mode, but negative feedback cannot be obtained in implicit feedback recommendation tasks, which aggravates the overestimation problem of offline RL recommender. To address these challenges, we propose a novel RL recommender named model-enhanced contrastive reinforcement learning (MCRL). On the one hand, we learn a value function to estimate the long-term engagement of users, together with a conservative value learning mechanism to alleviate the overestimation problem.On the other hand, we construct some positive and negative state-action pairs to model the reward function and state transition function with contrastive learning to exploit the internal structure information of MDP. Experiments demonstrate that the proposed method significantly outperforms existing offline RL and self-supervised RL methods with different representative backbone networks on two real-world datasets.
</details>
<details>
<summary>摘要</summary>
利用强化学习（RL）的应用在推荐系统中广泛，因为RL可以优化用户的长期投入。从RL的角度来看，推荐系统可以视为一个Markov决策过程（MDP），其中推荐系统（代理）可以与用户（环境）互动，获得反馈（奖励讯号）。然而，在线上互动的问题下，用户体验和实现方式的问题导致RL推荐器的训练仅能靠搀托给定的缺乏奖励讯号和状态变化的练习数据进行训练。因此，推荐系统的奖励讯号和状态变化的缺乏问题非常严重，而这个问题长期以来受到RL推荐器的遗传。更糟糕的是，RL方法通过尝试和错误的模式学习，但是在隐式反馈推荐任务中，负面反馈无法获得，这个问题进一步严重过估推荐系统的性能。为了解决这些挑战，我们提出了一个新的RL推荐器，名为模型强化对照学习（MCRL）。在一方面，我们学习一个值函数估计用户的长期投入，同时将一个保守的值学习机制引入以解决过估问题。另一方面，我们使用对照学习来建构一些正面和负面的状态动作对，以模型奖励函数和状态变化函数，并将这些对照学习的训练结果与RL推荐器结合。实验结果显示，提出的方法在两个真实的数据集上具有明显的性能优化，与存在的缺乏奖励和自我超级RL方法相比。
</details></li>
</ul>
<hr>
<h2 id="DECWA-Density-Based-Clustering-using-Wasserstein-Distance"><a href="#DECWA-Density-Based-Clustering-using-Wasserstein-Distance" class="headerlink" title="DECWA : Density-Based Clustering using Wasserstein Distance"></a>DECWA : Density-Based Clustering using Wasserstein Distance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16552">http://arxiv.org/abs/2310.16552</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nabilem/decwa">https://github.com/nabilem/decwa</a></li>
<li>paper_authors: Nabil El Malki, Robin Cugny, Olivier Teste, Franck Ravat</li>
<li>for: 这篇论文是为了提出一种新的分区方法和一种基于空间浓度和概率方法的归一化算法，以解决现有的density-based clustering方法在低密度团、邻近密度相似团和高维数据上的缺陷。</li>
<li>methods: 该论文提出了一种基于probability density function（$p.d.f$）的分区方法，首先使用$p.d.f$来建立子团，然后使用 Wasserstein metric来聚合相似的子团。</li>
<li>results: 论文表明，该方法在多种 datasets 上表现出色，超过了现有的state-of-the-art density-based clustering方法。<details>
<summary>Abstract</summary>
Clustering is a data analysis method for extracting knowledge by discovering groups of data called clusters. Among these methods, state-of-the-art density-based clustering methods have proven to be effective for arbitrary-shaped clusters. Despite their encouraging results, they suffer to find low-density clusters, near clusters with similar densities, and high-dimensional data. Our proposals are a new characterization of clusters and a new clustering algorithm based on spatial density and probabilistic approach. First of all, sub-clusters are built using spatial density represented as probability density function ($p.d.f$) of pairwise distances between points. A method is then proposed to agglomerate similar sub-clusters by using both their density ($p.d.f$) and their spatial distance. The key idea we propose is to use the Wasserstein metric, a powerful tool to measure the distance between $p.d.f$ of sub-clusters. We show that our approach outperforms other state-of-the-art density-based clustering methods on a wide variety of datasets.
</details>
<details>
<summary>摘要</summary>
首先，我们使用对Points之间的距离的概率浸润函数($p.d.f$)来构建子集。然后，我们提出了一种将相似的子集归类的方法，使用这些子集的浸润函数($p.d.f$)和其空间距离。我们的关键想法是使用沃氏距离，这是一种强大的测量 $p.d.f$ 的距离的工具。我们展示了我们的方法在各种数据集上的优秀性能。
</details></li>
</ul>
<hr>
<h2 id="Cyclic-Directed-Probabilistic-Graphical-Model-A-Proposal-Based-on-Structured-Outcomes"><a href="#Cyclic-Directed-Probabilistic-Graphical-Model-A-Proposal-Based-on-Structured-Outcomes" class="headerlink" title="Cyclic Directed Probabilistic Graphical Model: A Proposal Based on Structured Outcomes"></a>Cyclic Directed Probabilistic Graphical Model: A Proposal Based on Structured Outcomes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16525">http://arxiv.org/abs/2310.16525</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oleksii Sirotkin</li>
<li>for: probabilistic graphical model 建模 (structural learning)</li>
<li>methods: 使用 probablistic relation network (PRN) 直接捕捉方向相关性 (directional cyclic dependencies)</li>
<li>results: 支持从观测数据学习 (learning from observed data), 满足 probabilistic inference (probabilistic inference)，可用于数据分析和专家设计等应用 (data analysis and expert design applications)<details>
<summary>Abstract</summary>
In the process of building (structural learning) a probabilistic graphical model from a set of observed data, the directional, cyclic dependencies between the random variables of the model are often found. Existing graphical models such as Bayesian and Markov networks can reflect such dependencies. However, this requires complicating those models, such as adding additional variables or dividing the model graph into separate subgraphs. Herein, we describe a probabilistic graphical model - probabilistic relation network - that allows the direct capture of directional cyclic dependencies during structural learning. This model is based on the simple idea that each sample of the observed data can be represented by an arbitrary graph (structured outcome), which reflects the structure of the dependencies of the variables included in the sample. Each of the outcomes contains only a part of the graphical model structure; however, a complete graph of the probabilistic model is obtained by combining different outcomes. Such a graph, unlike Bayesian and Markov networks, can be directed and can have cycles. We explored the full joint distribution and conditional distribution and conditional independence properties of variables in the proposed model. We defined the algorithms for constructing of the model from the dataset and for calculating the conditional and full joint distributions. We also performed a numerical comparison with Bayesian and Markov networks. This model does not violate the probability axioms, and it supports learning from observed data. Notably, it supports probabilistic inference, making it a prospective tool in data analysis and in expert and design-making applications.
</details>
<details>
<summary>摘要</summary>
在建立结构学习（structural learning）probabilistic graphical model时，常发现方向性循环依赖关系 междуRandom variable。现有的图书馆模型，如权化网络和马可夫链，可以表示这些依赖关系。然而，这需要补充这些模型，例如添加更多变量或将模型图分成多个子图。在这里，我们描述了一种probabilistic graphical model——probabilistic relation network——可以直接捕捉方向性循环依赖关系。这种模型基于简单的想法，即每个观察数据样本可以表示一个任意图（structured outcome），该图反映变量之间的依赖结构。每个结果只包含变量之间的一部分结构，但是可以将多个结果组合起来获得完整的图形模型结构。这种图不同于权化网络和马可夫链，可以是指定的并且可以有循环。我们研究了这种模型的全联合分布和 condition distribution和 conditional independence性质。我们还定义了从数据集构建模型的算法和计算 conditional和全联合分布的算法。此外，我们对权化网络和马可夫链进行了数值比较。这种模型不违背概率axioms，并且支持从观察数据学习。尤其是，它支持概率推理，使其成为数据分析和专家设计应用的可能工具。
</details></li>
</ul>
<hr>
<h2 id="Can-You-Rely-on-Your-Model-Evaluation-Improving-Model-Evaluation-with-Synthetic-Test-Data"><a href="#Can-You-Rely-on-Your-Model-Evaluation-Improving-Model-Evaluation-with-Synthetic-Test-Data" class="headerlink" title="Can You Rely on Your Model Evaluation? Improving Model Evaluation with Synthetic Test Data"></a>Can You Rely on Your Model Evaluation? Improving Model Evaluation with Synthetic Test Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16524">http://arxiv.org/abs/2310.16524</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vanderschaarlab/3S-Testing">https://github.com/vanderschaarlab/3S-Testing</a></li>
<li>paper_authors: Boris van Breugel, Nabeel Seedat, Fergus Imrie, Mihaela van der Schaar<br>for: This paper aims to address the challenges of accurately assessing the performance of machine learning models on diverse and underrepresented subgroups.methods: The paper proposes a deep generative modeling framework called 3S Testing, which generates synthetic test sets for small subgroups and simulates distributional shifts.results: The authors demonstrate that 3S Testing outperforms traditional baselines in estimating model performance on minority subgroups and under plausible distributional shifts, and provides intervals around its performance estimates with superior coverage of the ground truth compared to existing approaches.<details>
<summary>Abstract</summary>
Evaluating the performance of machine learning models on diverse and underrepresented subgroups is essential for ensuring fairness and reliability in real-world applications. However, accurately assessing model performance becomes challenging due to two main issues: (1) a scarcity of test data, especially for small subgroups, and (2) possible distributional shifts in the model's deployment setting, which may not align with the available test data. In this work, we introduce 3S Testing, a deep generative modeling framework to facilitate model evaluation by generating synthetic test sets for small subgroups and simulating distributional shifts. Our experiments demonstrate that 3S Testing outperforms traditional baselines -- including real test data alone -- in estimating model performance on minority subgroups and under plausible distributional shifts. In addition, 3S offers intervals around its performance estimates, exhibiting superior coverage of the ground truth compared to existing approaches. Overall, these results raise the question of whether we need a paradigm shift away from limited real test data towards synthetic test data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-Self-Interpretable-Graph-Level-Anomaly-Detection"><a href="#Towards-Self-Interpretable-Graph-Level-Anomaly-Detection" class="headerlink" title="Towards Self-Interpretable Graph-Level Anomaly Detection"></a>Towards Self-Interpretable Graph-Level Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16520">http://arxiv.org/abs/2310.16520</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yixinliu233/signet">https://github.com/yixinliu233/signet</a></li>
<li>paper_authors: Yixin Liu, Kaize Ding, Qinghua Lu, Fuyi Li, Leo Yu Zhang, Shirui Pan</li>
<li>for: 本研究的目的是提出一种可解释的图像异常检测模型（SIGNET），可以同时检测图像异常和生成相关的解释。</li>
<li>methods: 本研究使用了多视图子图信息瓶颈（MSIB）框架，从而实现了自我解释的图像异常检测。</li>
<li>results: 广泛的实验表明，SIGNET具有优秀的异常检测能力和自我解释能力。<details>
<summary>Abstract</summary>
Graph-level anomaly detection (GLAD) aims to identify graphs that exhibit notable dissimilarity compared to the majority in a collection. However, current works primarily focus on evaluating graph-level abnormality while failing to provide meaningful explanations for the predictions, which largely limits their reliability and application scope. In this paper, we investigate a new challenging problem, explainable GLAD, where the learning objective is to predict the abnormality of each graph sample with corresponding explanations, i.e., the vital subgraph that leads to the predictions. To address this challenging problem, we propose a Self-Interpretable Graph aNomaly dETection model (SIGNET for short) that detects anomalous graphs as well as generates informative explanations simultaneously. Specifically, we first introduce the multi-view subgraph information bottleneck (MSIB) framework, serving as the design basis of our self-interpretable GLAD approach. This way SIGNET is able to not only measure the abnormality of each graph based on cross-view mutual information but also provide informative graph rationales by extracting bottleneck subgraphs from the input graph and its dual hypergraph in a self-supervised way. Extensive experiments on 16 datasets demonstrate the anomaly detection capability and self-interpretability of SIGNET.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>> GRaph-level anomaly detection (GLAD) aims to identify graphs that exhibit notable dissimilarity compared to the majority in a collection. However, current works primarily focus on evaluating graph-level abnormality while failing to provide meaningful explanations for the predictions, which largely limits their reliability and application scope. In this paper, we investigate a new challenging problem, explainable GLAD, where the learning objective is to predict the abnormality of each graph sample with corresponding explanations, i.e., the vital subgraph that leads to the predictions. To address this challenging problem, we propose a Self-Interpretable Graph aNomaly dETection model (SIGNET for short) that detects anomalous graphs as well as generates informative explanations simultaneously. Specifically, we first introduce the multi-view subgraph information bottleneck (MSIB) framework, serving as the design basis of our self-interpretable GLAD approach. This way SIGNET is able to not only measure the abnormality of each graph based on cross-view mutual information but also provide informative graph rationales by extracting bottleneck subgraphs from the input graph and its dual hypergraph in a self-supervised way. Extensive experiments on 16 datasets demonstrate the anomaly detection capability and self-interpretability of SIGNET.
</details></li>
</ul>
<hr>
<h2 id="Particle-based-Variational-Inference-with-Generalized-Wasserstein-Gradient-Flow"><a href="#Particle-based-Variational-Inference-with-Generalized-Wasserstein-Gradient-Flow" class="headerlink" title="Particle-based Variational Inference with Generalized Wasserstein Gradient Flow"></a>Particle-based Variational Inference with Generalized Wasserstein Gradient Flow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16516">http://arxiv.org/abs/2310.16516</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alexczh1/gwg">https://github.com/alexczh1/gwg</a></li>
<li>paper_authors: Ziheng Cheng, Shiyue Zhang, Longlin Yu, Cheng Zhang</li>
<li>for: 这 paper 是关于 Particle-based variational inference methods (ParVIs) 的研究，具体来说是 Stein variational gradient descent (SVGD) 的一种更改方法。</li>
<li>methods: 这 paper 使用了 kernelized Wasserstein gradient flow 来更新 particles，但是 kernel 的设计可能会带来一些限制。这 paper 提出了一种基于函数梯度流的方法，可以更好地适应不同的情景。</li>
<li>results: 这 paper 显示了 Generalized Wasserstein gradient descent (GWG) 方法的强大的收敛保证，并且提供了一种自适应版本，可以根据 Wasserstein 距离选择最佳的 метри来加速收敛。在实验中，这 paper 证明了该方法在 simulate 和实际数据问题上的效果和效率。<details>
<summary>Abstract</summary>
Particle-based variational inference methods (ParVIs) such as Stein variational gradient descent (SVGD) update the particles based on the kernelized Wasserstein gradient flow for the Kullback-Leibler (KL) divergence. However, the design of kernels is often non-trivial and can be restrictive for the flexibility of the method. Recent works show that functional gradient flow approximations with quadratic form regularization terms can improve performance. In this paper, we propose a ParVI framework, called generalized Wasserstein gradient descent (GWG), based on a generalized Wasserstein gradient flow of the KL divergence, which can be viewed as a functional gradient method with a broader class of regularizers induced by convex functions. We show that GWG exhibits strong convergence guarantees. We also provide an adaptive version that automatically chooses Wasserstein metric to accelerate convergence. In experiments, we demonstrate the effectiveness and efficiency of the proposed framework on both simulated and real data problems.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Particle-based variational inference methods (ParVIs) such as Stein variational gradient descent (SVGD) update the particles based on the kernelized Wasserstein gradient flow for the Kullback-Leibler (KL) divergence. However, the design of kernels is often non-trivial and can be restrictive for the flexibility of the method. Recent works show that functional gradient flow approximations with quadratic form regularization terms can improve performance. In this paper, we propose a ParVI framework, called generalized Wasserstein gradient descent (GWG), based on a generalized Wasserstein gradient flow of the KL divergence, which can be viewed as a functional gradient method with a broader class of regularizers induced by convex functions. We show that GWG exhibits strong convergence guarantees. We also provide an adaptive version that automatically chooses Wasserstein metric to accelerate convergence. In experiments, we demonstrate the effectiveness and efficiency of the proposed framework on both simulated and real data problems." into Simplified Chinese.翻译文本为Simplified Chinese：<<SYS>>stein variational gradient descent（SVGD）和其他particle-based variational inference方法（ParVI）通常使用kernelized Wasserstein gradient flow来更新particles，但是kernel的设计可能是非常复杂和限制方法的灵活性。latest works表明，可以通过添加quadratic form regularization term来提高性能。在这篇论文中，我们提出了一种基于generalized Wasserstein gradient flow的ParVI框架，称为generalized Wasserstein gradient descent（GWG）。GWG可以看作一种基于convex function所induced的functional gradient方法，具有更广泛的regulator。我们证明了GWG具有强 convergence guarantees。此外，我们还提供了一个自适应版本，可以自动选择Wasserstein metric，以加速收敛。在实验中，我们展示了提议框架在模拟和实际数据问题上的效果和高效性。
</details></li>
</ul>
<hr>
<h2 id="Data-Optimization-in-Deep-Learning-A-Survey"><a href="#Data-Optimization-in-Deep-Learning-A-Survey" class="headerlink" title="Data Optimization in Deep Learning: A Survey"></a>Data Optimization in Deep Learning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16499">http://arxiv.org/abs/2310.16499</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yaorujing/data-optimization">https://github.com/yaorujing/data-optimization</a></li>
<li>paper_authors: Ou Wu, Rujing Yao<br>for:这篇论文的目的是为了提供一个包容性强的分类法，以便更好地理解现有的数据优化技术，并且探讨未来研究的可能性。methods:这篇论文使用了大量的文献研究，对现有的数据优化技术进行了分类和总结，并建立了一个包容性强的分类法。results:这篇论文通过对现有数据优化技术的分类和总结，提供了一个全面的视角，并探讨了未来研究的可能性。<details>
<summary>Abstract</summary>
Large-scale, high-quality data are considered an essential factor for the successful application of many deep learning techniques. Meanwhile, numerous real-world deep learning tasks still have to contend with the lack of sufficient amounts of high-quality data. Additionally, issues such as model robustness, fairness, and trustworthiness are also closely related to training data. Consequently, a huge number of studies in the existing literature have focused on the data aspect in deep learning tasks. Some typical data optimization techniques include data augmentation, logit perturbation, sample weighting, and data condensation. These techniques usually come from different deep learning divisions and their theoretical inspirations or heuristic motivations may seem unrelated to each other. This study aims to organize a wide range of existing data optimization methodologies for deep learning from the previous literature, and makes the effort to construct a comprehensive taxonomy for them. The constructed taxonomy considers the diversity of split dimensions, and deep sub-taxonomies are constructed for each dimension. On the basis of the taxonomy, connections among the extensive data optimization methods for deep learning are built in terms of four aspects. We probe into rendering several promising and interesting future directions. The constructed taxonomy and the revealed connections will enlighten the better understanding of existing methods and the design of novel data optimization techniques. Furthermore, our aspiration for this survey is to promote data optimization as an independent subdivision of deep learning. A curated, up-to-date list of resources related to data optimization in deep learning is available at \url{https://github.com/YaoRujing/Data-Optimization}.
</details>
<details>
<summary>摘要</summary>
大规模、高质量数据被认为是深度学习技术应用成功的重要因素。然而，许多实际应用中的深度学习任务仍面临着不足的高质量数据的问题。此外，模型的Robustness、公平性和信任性也与训练数据密切相关。因此，现有许多研究在深度学习任务中强调数据的重要性。这些研究包括数据增强、LOGit扰动、样本权重和数据缩densification等技术。这些技术通常来自不同的深度学习分支，其理论 inspirations 或启发性可能与彼此不相关。本研究 aimsto organize 过去 литераature 中关于深度学习数据优化的各种方法，并尝试构建一个全面的分类体系。该分类系统考虑了多维度的分割多样性，并为每个维度建立了深度的子分类。基于这个分类系统，对涵盖了广泛的深度学习数据优化方法的连接建立了四个方面。我们还探讨了一些有潜力和有趣的未来方向。建立的分类系统和连接将有助于更好地理解现有方法，以及设计新的数据优化技术。此外，我们的愿望是通过这种报告来促进数据优化成为深度学习的独立分支。一个CURATED、最新的关于数据优化在深度学习的资源列表可以在 \url{https://github.com/YaoRujing/Data-Optimization} 上找到。
</details></li>
</ul>
<hr>
<h2 id="Citizen-participation-crowd-sensed-sustainable-indoor-location-services"><a href="#Citizen-participation-crowd-sensed-sustainable-indoor-location-services" class="headerlink" title="Citizen participation: crowd-sensed sustainable indoor location services"></a>Citizen participation: crowd-sensed sustainable indoor location services</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16496">http://arxiv.org/abs/2310.16496</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ioannis Nasios, Konstantinos Vogklis, Avleen Malhi, Anastasia Vayona, Panos Chatziadam, Vasilis Katos</li>
<li>for: 提供无需特殊硬件的indoor定位功能，帮助实现智能基础设施转型。</li>
<li>methods: 使用机器学习技术，通过访问可用的WiFi基站网络，基于visitor的智能手机来估算indoor位置。</li>
<li>results: 实验结果显示，提posed方法可以达到准确率低于2m，并且模型在BSSIDs数量减少时仍然具有抗难度性。<details>
<summary>Abstract</summary>
In the present era of sustainable innovation, the circular economy paradigm dictates the optimal use and exploitation of existing finite resources. At the same time, the transition to smart infrastructures requires considerable investment in capital, resources and people. In this work, we present a general machine learning approach for offering indoor location awareness without the need to invest in additional and specialised hardware. We explore use cases where visitors equipped with their smart phone would interact with the available WiFi infrastructure to estimate their location, since the indoor requirement poses a limitation to standard GPS solutions. Results have shown that the proposed approach achieves a less than 2m accuracy and the model is resilient even in the case where a substantial number of BSSIDs are dropped.
</details>
<details>
<summary>摘要</summary>
现在可持续创新时代，循环经济模式提倡最佳利用现有的有限资源。同时，转移到智能基础设施需要较大的投资资源人力。在这项工作中，我们提出了一种通用机器学习方法，以实现无需特殊硬件投资的室内位置意识。我们探讨访问者通过使用可用的 WiFi 基础设施与其智能手机进行互动，以估算室内位置，因为标准 GPS 解决方案在室内环境下存在限制。结果表明，我们的方法可以实现准确率低于 2m，并且模型在大量 BSSID 被drop 时仍能保持可靠性。
</details></li>
</ul>
<hr>
<h2 id="TSONN-Time-stepping-oriented-neural-network-for-solving-partial-differential-equations"><a href="#TSONN-Time-stepping-oriented-neural-network-for-solving-partial-differential-equations" class="headerlink" title="TSONN: Time-stepping-oriented neural network for solving partial differential equations"></a>TSONN: Time-stepping-oriented neural network for solving partial differential equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16491">http://arxiv.org/abs/2310.16491</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenbo Cao, Weiwei Zhang</li>
<li>for: 解决具有部分偏微分方程（PDE）的前向和反向问题，特别是通过physics-informed neural networks（PINNs）来解决这些问题。</li>
<li>methods: 将时间步长级联到深度学习中，将原始的不良条件优化问题转化为一系列良好条件的子问题，使模型训练 converges significantly。</li>
<li>results: 提出的方法可以稳定地训练并在许多问题中获得正确的结果，而标准PINNs则无法解决这些问题。此外，我们还发现了时间步长方法在神经网络优化方法框架中的一些新性和优势，比如可以使用显式或隐式时间步长，并且可以与传统的网格数值方法进行比较。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs), especially physics-informed neural networks (PINNs), have recently become a new popular method for solving forward and inverse problems governed by partial differential equations (PDEs). However, these methods still face challenges in achieving stable training and obtaining correct results in many problems, since minimizing PDE residuals with PDE-based soft constraint make the problem ill-conditioned. Different from all existing methods that directly minimize PDE residuals, this work integrates time-stepping method with deep learning, and transforms the original ill-conditioned optimization problem into a series of well-conditioned sub-problems over given pseudo time intervals. The convergence of model training is significantly improved by following the trajectory of the pseudo time-stepping process, yielding a robust optimization-based PDE solver. Our results show that the proposed method achieves stable training and correct results in many problems that standard PINNs fail to solve, requiring only a simple modification on the loss function. In addition, we demonstrate several novel properties and advantages of time-stepping methods within the framework of neural network-based optimization approach, in comparison to traditional grid-based numerical method. Specifically, explicit scheme allows significantly larger time step, while implicit scheme can be implemented as straightforwardly as explicit scheme.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN），特别是物理学信息泛化神经网络（PINN），最近成为解决部分导数方程（PDE）的前向和反向问题的新方法。然而，这些方法仍然面临困难在实现稳定训练和正确结果的多个问题中，因为将PDE residuals minimized with PDE-based soft constraint会导致问题变得不稳定。与所有直接将PDE residuals minimized的方法不同，这项工作将时间步骤法与深度学习结合，将原始不稳定优化问题转化为一系列稳定优化问题。通过跟踪pseudo时间步骤过程的路径，提高了模型训练的 converges。我们的结果表明，提案的方法可以在许多问题中实现稳定的训练和正确的结果，只需要对损失函数进行一个简单的修改。此外，我们还展示了时间步骤法在神经网络基于优化方法框架中的一些新特性和优势，比如：Explicit scheme可以允许更大的时间步骤，而Implicit scheme可以被实现为Explicit scheme一样直接。
</details></li>
</ul>
<hr>
<h2 id="Hyperparameter-Optimization-for-Multi-Objective-Reinforcement-Learning"><a href="#Hyperparameter-Optimization-for-Multi-Objective-Reinforcement-Learning" class="headerlink" title="Hyperparameter Optimization for Multi-Objective Reinforcement Learning"></a>Hyperparameter Optimization for Multi-Objective Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16487">http://arxiv.org/abs/2310.16487</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lucasalegre/morl-baselines">https://github.com/lucasalegre/morl-baselines</a></li>
<li>paper_authors: Florian Felten, Daniel Gareev, El-Ghazali Talbi, Grégoire Danoy</li>
<li>for: 本研究旨在解决多目标算法中的超参数优化问题，以提高多目标算法的性能。</li>
<li>methods: 本研究提出了一种系统的方法来解决超参数优化问题，包括精心设计的搜索策略和优化目标函数。</li>
<li>results: 实验结果表明，提出的方法可以有效地提高多目标算法的性能，并且标识了未来研究的可能性。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) has emerged as a powerful approach for tackling complex problems. The recent introduction of multi-objective reinforcement learning (MORL) has further expanded the scope of RL by enabling agents to make trade-offs among multiple objectives. This advancement not only has broadened the range of problems that can be tackled but also created numerous opportunities for exploration and advancement. Yet, the effectiveness of RL agents heavily relies on appropriately setting their hyperparameters. In practice, this task often proves to be challenging, leading to unsuccessful deployments of these techniques in various instances. Hence, prior research has explored hyperparameter optimization in RL to address this concern.   This paper presents an initial investigation into the challenge of hyperparameter optimization specifically for MORL. We formalize the problem, highlight its distinctive challenges, and propose a systematic methodology to address it. The proposed methodology is applied to a well-known environment using a state-of-the-art MORL algorithm, and preliminary results are reported. Our findings indicate that the proposed methodology can effectively provide hyperparameter configurations that significantly enhance the performance of MORL agents. Furthermore, this study identifies various future research opportunities to further advance the field of hyperparameter optimization for MORL.
</details>
<details>
<summary>摘要</summary>
本文对MORL中的超参数优化进行了初步的研究。我们正式定义了问题，抛光了其独特的挑战，并提出了系统的方法ology。我们应用了一种现状最佳的MORL算法在一个知名环境中，并发布了初步的结果。我们的发现表明，我们的方法可以有效地为MORL代理人提供优化超参数的配置，从而提高其性能。此外，本研究还标识了MORL中超参数优化的未来研究的多种可能性。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Python-Library-for-Deep-Learning-Based-Event-Detection-in-Multivariate-Time-Series-Data-and-Information-Retrieval-in-NLP"><a href="#A-Comprehensive-Python-Library-for-Deep-Learning-Based-Event-Detection-in-Multivariate-Time-Series-Data-and-Information-Retrieval-in-NLP" class="headerlink" title="A Comprehensive Python Library for Deep Learning-Based Event Detection in Multivariate Time Series Data and Information Retrieval in NLP"></a>A Comprehensive Python Library for Deep Learning-Based Event Detection in Multivariate Time Series Data and Information Retrieval in NLP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16485">http://arxiv.org/abs/2310.16485</a></li>
<li>repo_url: None</li>
<li>paper_authors: Menouar Azib, Benjamin Renard, Philippe Garnier, Vincent Génot, Nicolas André<br>for: 这份研究目的是为了开发一个基于深度学习的时间序列资料事件探测方法，以便在不同领域中进行更有效的事件探测和预测。methods: 这个方法使用了四个重要的新特性，包括：首先，它是一个回归型的方法，而不是一个双值分类方法。其次，它不需要标注化的数据集，而是仅需要提供参考事件，例如时间点或时间间隔。第三，它使用了一个堆叠 ensemble 学习元模型，融合了多种深度学习模型，包括预设的 feed-forward neural networks (FFNs) 到现有的架构如 transformers。最后，为了实用实现，我们已经开发了一个 Python 套件，名为 eventdetector-ts，可以通过 Python Package Index (PyPI) 进行安装。results: 在不同的实验中，我们显示了这个方法的多元性和有效性，包括自然语言处理 (NLP) 到金融安全领域的实验。<details>
<summary>Abstract</summary>
Event detection in time series data is crucial in various domains, including finance, healthcare, cybersecurity, and science. Accurately identifying events in time series data is vital for making informed decisions, detecting anomalies, and predicting future trends. Despite extensive research exploring diverse methods for event detection in time series, with deep learning approaches being among the most advanced, there is still room for improvement and innovation in this field. In this paper, we present a new deep learning supervised method for detecting events in multivariate time series data. Our method combines four distinct novelties compared to existing deep-learning supervised methods. Firstly, it is based on regression instead of binary classification. Secondly, it does not require labeled datasets where each point is labeled; instead, it only requires reference events defined as time points or intervals of time. Thirdly, it is designed to be robust by using a stacked ensemble learning meta-model that combines deep learning models, ranging from classic feed-forward neural networks (FFNs) to state-of-the-art architectures like transformers. This ensemble approach can mitigate individual model weaknesses and biases, resulting in more robust predictions. Finally, to facilitate practical implementation, we have developed a Python package to accompany our proposed method. The package, called eventdetector-ts, can be installed through the Python Package Index (PyPI). In this paper, we present our method and provide a comprehensive guide on the usage of the package. We showcase its versatility and effectiveness through different real-world use cases from natural language processing (NLP) to financial security domains.
</details>
<details>
<summary>摘要</summary>
时序数据中的事件检测在各个领域都是非常重要的，包括金融、医疗、网络安全和科学等。正确地在时序数据中检测事件是至关重要的，以便做出了 informed 的决策，检测异常点和预测未来趋势。虽然有 extensively 的研究探讨了不同的事件检测方法，但是还有很多空间和创新的 possiblities 在这个领域。在这篇论文中，我们提出了一种新的深度学习监督方法，用于在多ivariate 时序数据中检测事件。我们的方法包括四个新特点，与现有的深度学习监督方法相比：1. 基于回归而不是二分类。2. 不需要标注数据集，只需要参考事件定义为时间点或时间Interval。3. 使用堆叠 ensemble learning 元模型，结合深度学习模型，从 classic feed-forward neural networks (FFNs) 到现代架构如 transformers。这种ensembleapproach可以 Mitigate 个模型的弱点和偏见，从而获得更加稳定的预测。4. 为了方便实现，我们开发了一个 Python 包，可以在 PyPI 上安装。在这篇论文中，我们介绍了我们的方法，并提供了使用该包的完整指南。我们通过不同的实际案例，从自然语言处理 (NLP) 到金融安全领域，展示了我们的方法的多样性和效果。
</details></li>
</ul>
<hr>
<h2 id="Symphony-of-experts-orchestration-with-adversarial-insights-in-reinforcement-learning"><a href="#Symphony-of-experts-orchestration-with-adversarial-insights-in-reinforcement-learning" class="headerlink" title="Symphony of experts: orchestration with adversarial insights in reinforcement learning"></a>Symphony of experts: orchestration with adversarial insights in reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16473">http://arxiv.org/abs/2310.16473</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthieu Jonckheere, Chiara Mignacco, Gilles Stoltz</li>
<li>for: 这 paper 的目的是探讨 Structured reinforcement learning 如何在探索困难的情况下 достичь更好的性能，特别是通过 “orchestration” 的概念，一小组专家策略来导航决策。</li>
<li>methods: 这 paper 使用的方法包括 orchestration 的模型化，以及对 adversarial settings 的转移 regret bound 结果，以及对 natural policy gradient 的扩展和推广。</li>
<li>results: 这 paper 的结果包括在 tabular  Setting 下的 value-functions regret bounds，以及对 arbitrary adversarial aggregation strategies 的扩展和推广。 另外， paper 还提供了更加透明的证明方法，以及一个 Stochastic matching toy model 的仿真结果。<details>
<summary>Abstract</summary>
Structured reinforcement learning leverages policies with advantageous properties to reach better performance, particularly in scenarios where exploration poses challenges. We explore this field through the concept of orchestration, where a (small) set of expert policies guides decision-making; the modeling thereof constitutes our first contribution. We then establish value-functions regret bounds for orchestration in the tabular setting by transferring regret-bound results from adversarial settings. We generalize and extend the analysis of natural policy gradient in Agarwal et al. [2021, Section 5.3] to arbitrary adversarial aggregation strategies. We also extend it to the case of estimated advantage functions, providing insights into sample complexity both in expectation and high probability. A key point of our approach lies in its arguably more transparent proofs compared to existing methods. Finally, we present simulations for a stochastic matching toy model.
</details>
<details>
<summary>摘要</summary>
“结构化强化学习可以利用有利属性的策略来 дости得更好的性能，特别在探索问题时。我们通过“指挥”的概念来探索这一领域，其中一小集的专家策略导向决策。我们在这篇文章中将提供值函数的后悔 bounds，并将这些结果应用到表格设定中。我们还将对自然策略均衡的分析扩展到任意的反抗策略整合策略，并将其推广到估计的优势函数中。我们的方法具有更加透明的证明，相比于现有的方法。最后，我们将提供一个Stochastic Matching实验模型的 simulate。”Note: The translation is in Simplified Chinese, which is one of the two standard varieties of Chinese. The other variety is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Learning-Continuous-Network-Emerging-Dynamics-from-Scarce-Observations-via-Data-Adaptive-Stochastic-Processes"><a href="#Learning-Continuous-Network-Emerging-Dynamics-from-Scarce-Observations-via-Data-Adaptive-Stochastic-Processes" class="headerlink" title="Learning Continuous Network Emerging Dynamics from Scarce Observations via Data-Adaptive Stochastic Processes"></a>Learning Continuous Network Emerging Dynamics from Scarce Observations via Data-Adaptive Stochastic Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16466">http://arxiv.org/abs/2310.16466</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/csjtx1021/neural_ode_processes_for_network_dynamics-master">https://github.com/csjtx1021/neural_ode_processes_for_network_dynamics-master</a></li>
<li>paper_authors: Jiaxu Cui, Bingyi Sun, Jiming Liu, Bo Yang<br>for: 学习复杂网络动态的研究是探讨复杂网络在多个领域中的互动机制的重要基础。methods: 我们提出了一种新的Neural ODE Processes for Network Dynamics（NDP4ND），它是基于随机数据适应网络动态的新一代渐近逻辑过程。results: 我们在多种网络动态实验中进行了广泛的实验，结果表明，NDP4ND方法具有优秀的数据适应性和计算效率，可以快速适应未见的网络出现动态，并且可以减少观察数据的比例至只有约6%，提高学习新动态的速度。<details>
<summary>Abstract</summary>
Learning network dynamics from the empirical structure and spatio-temporal observation data is crucial to revealing the interaction mechanisms of complex networks in a wide range of domains. However, most existing methods only aim at learning network dynamic behaviors generated by a specific ordinary differential equation instance, resulting in ineffectiveness for new ones, and generally require dense observations. The observed data, especially from network emerging dynamics, are usually difficult to obtain, which brings trouble to model learning. Therefore, how to learn accurate network dynamics with sparse, irregularly-sampled, partial, and noisy observations remains a fundamental challenge. We introduce Neural ODE Processes for Network Dynamics (NDP4ND), a new class of stochastic processes governed by stochastic data-adaptive network dynamics, to overcome the challenge and learn continuous network dynamics from scarce observations. Intensive experiments conducted on various network dynamics in ecological population evolution, phototaxis movement, brain activity, epidemic spreading, and real-world empirical systems, demonstrate that the proposed method has excellent data adaptability and computational efficiency, and can adapt to unseen network emerging dynamics, producing accurate interpolation and extrapolation with reducing the ratio of required observation data to only about 6\% and improving the learning speed for new dynamics by three orders of magnitude.
</details>
<details>
<summary>摘要</summary>
学习复杂网络动力学从实际结构和空间时间观察数据中是揭示复杂网络交互机制的关键。然而，现有方法大多只关注于学习特定常 differential equation 实例生成的网络动力学行为，导致对新的动力学不效果，并且通常需要密集观察。观察数据，特别是从网络 emerging 动力学来，通常困难以获得，这会带来模型学习的困难。因此，如何学习准确的网络动力学以及稀疏、不规则、部分和噪音观察数据仍是一个基本挑战。我们介绍了神经网络过程 для网络动力学（NDP4ND），一种新的随机过程，由数据适应网络动力学控制。经过对各种网络动力学实验，包括生态学人口演化、光激活运动、脑活动、流行病传播和实际观察数据，我们发现该方法具有出色的数据适应性和计算效率，可以适应未见到的网络emerging动力学，生成高精度的 interpolate 和 extrapolate，并将观察数据比例降低至只有6%，提高了对新动力学的学习速度。
</details></li>
</ul>
<hr>
<h2 id="Unknown-Health-States-Recognition-With-Collective-Decision-Based-Deep-Learning-Networks-In-Predictive-Maintenance-Applications"><a href="#Unknown-Health-States-Recognition-With-Collective-Decision-Based-Deep-Learning-Networks-In-Predictive-Maintenance-Applications" class="headerlink" title="Unknown Health States Recognition With Collective Decision Based Deep Learning Networks In Predictive Maintenance Applications"></a>Unknown Health States Recognition With Collective Decision Based Deep Learning Networks In Predictive Maintenance Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17670">http://arxiv.org/abs/2310.17670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuyue Lou, M. Amine Atoui</li>
<li>for: 这个研究的目的是为了提出一个集体决策框架，以便不同的卷积神经网络可以同时进行不同的健康状态分类。</li>
<li>methods: 这个研究使用了多个卷积神经网络，包括进步的卷积神经网络、多尺度卷积神经网络和差异卷积神经网络。这些神经网络可以从工业数据中学习有效的健康状态表示。</li>
<li>results: 根据TEP公共数据集的验证结果显示，提出的卷积神经网络集体决策框架可以优化不知道的健康状态标本的识别能力，同时维持知道的健康状态标本的准确率。这些结果显示了该深度学习框架的优越性，并且基于余差和多尺度学习的神经网络表现最佳。<details>
<summary>Abstract</summary>
At present, decision making solutions developed based on deep learning (DL) models have received extensive attention in predictive maintenance (PM) applications along with the rapid improvement of computing power. Relying on the superior properties of shared weights and spatial pooling, Convolutional Neural Network (CNN) can learn effective representations of health states from industrial data. Many developed CNN-based schemes, such as advanced CNNs that introduce residual learning and multi-scale learning, have shown good performance in health state recognition tasks under the assumption that all the classes are known. However, these schemes have no ability to deal with new abnormal samples that belong to state classes not part of the training set. In this paper, a collective decision framework for different CNNs is proposed. It is based on a One-vs-Rest network (OVRN) to simultaneously achieve classification of known and unknown health states. OVRN learn state-specific discriminative features and enhance the ability to reject new abnormal samples incorporated to different CNNs. According to the validation results on the public dataset of Tennessee Eastman Process (TEP), the proposed CNN-based decision schemes incorporating OVRN have outstanding recognition ability for samples of unknown heath states, while maintaining satisfactory accuracy on known states. The results show that the new DL framework outperforms conventional CNNs, and the one based on residual and multi-scale learning has the best overall performance.
</details>
<details>
<summary>摘要</summary>
当前，基于深度学习（DL）模型的决策支持技术在预测维护（PM）应用中得到了广泛的关注，随着计算能力的快速提升。利用深度学习模型的共享权重和空间 pooling 特性，卷积神经网络（CNN）可以从工业数据中学习有效的健康状态表示。一些已经发展出来的 CNN 基本 schemes，如增强 CNN 和多级学习，在健康状态识别任务中表现良好，假设所有类别都是已知的。然而，这些 schemes 无法处理新的异常样本，它们不在训练集中。在这篇论文中，一种集成多个 CNN 的决策框架是提出的。它基于一个对抗网络（OVRN），同时实现已知和未知健康状态的分类。OVRN 学习特定状态的抗性特征，提高了对新异常样本的拒绝能力。根据公共数据集 TEP 的验证结果，提出的 CNN 基本 schemes  incorporating OVRN 在未知健康状态样本的识别能力方面表现出色，同时保持知道状态的准确率。结果表明，新的 DL 框架超越传统 CNNs，而基于增强和多级学习的 CNN 在整体性能方面表现最佳。
</details></li>
</ul>
<hr>
<h2 id="ClearMark-Intuitive-and-Robust-Model-Watermarking-via-Transposed-Model-Training"><a href="#ClearMark-Intuitive-and-Robust-Model-Watermarking-via-Transposed-Model-Training" class="headerlink" title="ClearMark: Intuitive and Robust Model Watermarking via Transposed Model Training"></a>ClearMark: Intuitive and Robust Model Watermarking via Transposed Model Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16453">http://arxiv.org/abs/2310.16453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Torsten Krauß, Jasper Stang, Alexandra Dmitrienko</li>
<li>for: 提供一种可读性好的深度神经网络（DNN）水印方法，以便人类可以直观地判断水印是否存在。</li>
<li>methods: 使用一种名为ClearMark的方法，该方法在DNN模型中嵌入可见的水印，并且不需要复杂的验证算法或强制性阈值。</li>
<li>results: ClearMark方法可以在不同的数据集和模型上实现高度的可读性和抗性能，并且可以承受模型修改和黑客攻击。<details>
<summary>Abstract</summary>
Due to costly efforts during data acquisition and model training, Deep Neural Networks (DNNs) belong to the intellectual property of the model creator. Hence, unauthorized use, theft, or modification may lead to legal repercussions. Existing DNN watermarking methods for ownership proof are often non-intuitive, embed human-invisible marks, require trust in algorithmic assessment that lacks human-understandable attributes, and rely on rigid thresholds, making it susceptible to failure in cases of partial watermark erasure.   This paper introduces ClearMark, the first DNN watermarking method designed for intuitive human assessment. ClearMark embeds visible watermarks, enabling human decision-making without rigid value thresholds while allowing technology-assisted evaluations. ClearMark defines a transposed model architecture allowing to use of the model in a backward fashion to interwove the watermark with the main task within all model parameters. Compared to existing watermarking methods, ClearMark produces visual watermarks that are easy for humans to understand without requiring complex verification algorithms or strict thresholds. The watermark is embedded within all model parameters and entangled with the main task, exhibiting superior robustness. It shows an 8,544-bit watermark capacity comparable to the strongest existing work. Crucially, ClearMark's effectiveness is model and dataset-agnostic, and resilient against adversarial model manipulations, as demonstrated in a comprehensive study performed with four datasets and seven architectures.
</details>
<details>
<summary>摘要</summary>
由于数据收集和模型训练的成本高昂，深度神经网络（DNN）通常属于创建者的知识产权。因此，不经授权使用、盗取或修改可能会导致法律后果。现有的DNN涂鸦方法为证明所有权存在一些缺点，如难于理解、需要对算法评估中的信任，且需要固定的阈值，这使得其容易受到部分涂鸦 removing 的影响。本文介绍了 ClearMark，首个为人类可读性设计的 DNN 涂鸦方法。ClearMark 使用可见的涂鸦，allowing human decision-making without rigid value thresholds，同时允许技术支持的评估。ClearMark 使用拼接模型 architecture，使得模型在反向方式下使用，将涂鸦与主任务内所有模型参数结合在一起。与现有的涂鸦方法相比，ClearMark 生成的视觉涂鸦易于人类理解，无需复杂的验证算法或固定的阈值。涂鸦被内置于所有模型参数和主任务中，具有更高的鲁棒性。它可以承载 8,544 比特的涂鸦 capacities，与最强的现有工作相当。更重要的是，ClearMark 的效果是模型和数据aset 独立，并且对抗模型修改的攻击，如在四个数据集和七种架构上进行了全面的研究。
</details></li>
</ul>
<hr>
<h2 id="Grokking-in-Linear-Estimators-–-A-Solvable-Model-that-Groks-without-Understanding"><a href="#Grokking-in-Linear-Estimators-–-A-Solvable-Model-that-Groks-without-Understanding" class="headerlink" title="Grokking in Linear Estimators – A Solvable Model that Groks without Understanding"></a>Grokking in Linear Estimators – A Solvable Model that Groks without Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16441">http://arxiv.org/abs/2310.16441</a></li>
<li>repo_url: None</li>
<li>paper_authors: Noam Levi, Alon Beck, Yohai Bar-Sinai</li>
<li>for: 这 paper 探讨了模型如何在训练数据之后仍然能够泛化。</li>
<li>methods: 作者使用了教师-学生模式和高维输入来研究了 linear 网络在Linear任务上的泛化行为。</li>
<li>results: 研究发现，在训练和泛化数据协方差矩阵的基础上，模型可以在训练数据之后仍然具有泛化能力，并且可以通过精确预测泛化时间的因素来预测模型的泛化能力。<details>
<summary>Abstract</summary>
Grokking is the intriguing phenomenon where a model learns to generalize long after it has fit the training data. We show both analytically and numerically that grokking can surprisingly occur in linear networks performing linear tasks in a simple teacher-student setup with Gaussian inputs. In this setting, the full training dynamics is derived in terms of the training and generalization data covariance matrix. We present exact predictions on how the grokking time depends on input and output dimensionality, train sample size, regularization, and network initialization. We demonstrate that the sharp increase in generalization accuracy may not imply a transition from "memorization" to "understanding", but can simply be an artifact of the accuracy measure. We provide empirical verification for our calculations, along with preliminary results indicating that some predictions also hold for deeper networks, with non-linear activations.
</details>
<details>
<summary>摘要</summary>
它（Grokking）是一种吸引人的现象，在学习过程中，模型会在训练数据之后仍然能够泛化。我们通过分析和数值方法表明，在线性网络中进行线性任务时，grokking可以意外地发生。在这种设置中，我们计算了全面的训练动态，并通过训练和泛化数据协方差矩阵来表示。我们提出了具体预测，包括输入和输出维度、训练样本大小、规范、网络初始化等因素，grokking时间如何随变化。我们还证明了，尽管sharply increase in generalization accuracy不一定意味着从"记忆"转移到"理解"，但可能只是精度度量的假象。我们提供了实验证明，以及初步结果表明，一些预测也适用于更深的网络和非线性活化。
</details></li>
</ul>
<hr>
<h2 id="An-Approach-for-Efficient-Neural-Architecture-Search-Space-Definition"><a href="#An-Approach-for-Efficient-Neural-Architecture-Search-Space-Definition" class="headerlink" title="An Approach for Efficient Neural Architecture Search Space Definition"></a>An Approach for Efficient Neural Architecture Search Space Definition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17669">http://arxiv.org/abs/2310.17669</a></li>
<li>repo_url: None</li>
<li>paper_authors: Léo Pouy, Fouad Khenfri, Patrick Leserf, Chokri Mraidha, Cherif Larouci</li>
<li>for: 本研究旨在提出一种新的自动Machine Learning（AutoML）方法和工具，帮助用户在选择神经网络架构时快速寻找最佳策略。</li>
<li>methods: 本研究使用了一种新的细胞结构搜索空间，易于理解和操作，并且可以涵盖大多数当前领先的卷积神经网络架构。</li>
<li>results: 研究人员通过实验和分析表明，提出的方法可以快速找到最佳策略，并且可以涵盖大多数当前领先的卷积神经网络架构。<details>
<summary>Abstract</summary>
As we advance in the fast-growing era of Machine Learning, various new and more complex neural architectures are arising to tackle problem more efficiently. On the one hand their efficient usage requires advanced knowledge and expertise, which is most of the time difficult to find on the labor market. On the other hand, searching for an optimized neural architecture is a time-consuming task when it is performed manually using a trial and error approach. Hence, a method and a tool support is needed to assist users of neural architectures, leading to an eagerness in the field of Automatic Machine Learning (AutoML). When it comes to Deep Learning, an important part of AutoML is the Neural Architecture Search (NAS). In this paper, we propose a novel cell-based hierarchical search space, easy to comprehend and manipulate. The objectives of the proposed approach are to optimize the search-time and to be general enough to handle most of state of the art Convolutional Neural Networks (CNN) architectures.
</details>
<details>
<summary>摘要</summary>
随着机器学习领域的快速发展，不断出现新的更复杂的神经网络架构，以提高问题的解决效率。一方面，这些神经网络架构的高级知识和专业技能的需求往往困难找到在劳动市场上。另一方面，手动进行试验和尝试的方法来搜索优化的神经网络架构是一项时间consuming的任务。因此，一种方法和工具支持是需要的，以帮助神经网络架构的用户，从而促进自动机器学习（AutoML）领域的发展。在深度学习方面，搜索神经网络架构（NAS）是AutoML的一个重要组成部分。本文提出了一种新的细胞基 hierarchical 搜索空间，易于理解和操作。该方法的目标是 оптимизиethe search-time和涵盖大多数当前领先的卷积神经网络架构。
</details></li>
</ul>
<hr>
<h2 id="Non-isotropic-Persistent-Homology-Leveraging-the-Metric-Dependency-of-PH"><a href="#Non-isotropic-Persistent-Homology-Leveraging-the-Metric-Dependency-of-PH" class="headerlink" title="Non-isotropic Persistent Homology: Leveraging the Metric Dependency of PH"></a>Non-isotropic Persistent Homology: Leveraging the Metric Dependency of PH</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16437">http://arxiv.org/abs/2310.16437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vincent P. Grande, Michael T. Schaub</li>
<li>for: 这篇论文旨在提出一种新的点云数据分析方法，以EXTRACT ADDITIONAL TOPOLOGICAL AND GEOMETRICAL INFORMATION FROM PERSISTENT HOMOLOGY ANALYSIS。</li>
<li>methods: 该方法基于变换距离函数的思想，通过对 persistently diagram 的变化来EXTRACT ADDITIONAL INFORMATION。</li>
<li>results: 实验表明，该方法可以准确地EXTRACT INFORMATION ON ORIENTATION, ORIENTATIONAL VARIANCE AND SCALING OF POINT CLOUDS，并且可以应用于实际数据。<details>
<summary>Abstract</summary>
Persistent Homology is a widely used topological data analysis tool that creates a concise description of the topological properties of a point cloud based on a specified filtration. Most filtrations used for persistent homology depend (implicitly) on a chosen metric, which is typically agnostically chosen as the standard Euclidean metric on $\mathbb{R}^n$. Recent work has tried to uncover the 'true' metric on the point cloud using distance-to-measure functions, in order to obtain more meaningful persistent homology results. Here we propose an alternative look at this problem: we posit that information on the point cloud is lost when restricting persistent homology to a single (correct) distance function. Instead, we show how by varying the distance function on the underlying space and analysing the corresponding shifts in the persistence diagrams, we can extract additional topological and geometrical information. Finally, we numerically show that non-isotropic persistent homology can extract information on orientation, orientational variance, and scaling of randomly generated point clouds with good accuracy and conduct some experiments on real-world data.
</details>
<details>
<summary>摘要</summary>
persistente homology 是一种广泛使用的数据分析工具，可以生成一个精炼的点云的Topological Property 的描述，基于指定的筛选。大多数使用的筛选都是基于标准欧几里得度量空间 $\mathbb{R}^n$ 中的距离函数，而这些距离函数通常是随意选择的。近些年来，人们尝试了找到 '真实' 的度量函数，以获得更加有意义的 persistente homology 结果。在这篇文章中，我们提出了一个不同的思路：我们认为，只要限制 persistente homology 到单一的正确距离函数上，就会产生信息损失。相反，我们示出了通过在下面空间上变换距离函数，并分析对应的 persistente diagram 的变化，可以从中提取更多的topological 和几何信息。最后，我们通过数值实验表明，非均匀 persistente homology 可以对 randomly generated point clouds 中的方向、方向 variance 和扩展缩放进行准确的检测，并进行了一些实验。
</details></li>
</ul>
<hr>
<h2 id="FlatMatch-Bridging-Labeled-Data-and-Unlabeled-Data-with-Cross-Sharpness-for-Semi-Supervised-Learning"><a href="#FlatMatch-Bridging-Labeled-Data-and-Unlabeled-Data-with-Cross-Sharpness-for-Semi-Supervised-Learning" class="headerlink" title="FlatMatch: Bridging Labeled Data and Unlabeled Data with Cross-Sharpness for Semi-Supervised Learning"></a>FlatMatch: Bridging Labeled Data and Unlabeled Data with Cross-Sharpness for Semi-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16412">http://arxiv.org/abs/2310.16412</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhuohuangai/FlatMatch">https://github.com/zhuohuangai/FlatMatch</a></li>
<li>paper_authors: Zhuo Huang, Li Shen, Jun Yu, Bo Han, Tongliang Liu</li>
<li>for: 这个论文的目的是提出一种新的 semi-supervised learning (SSL) 方法，以便将充沛的无标数数据与罕见的标数数据结合起来，以提高 SSL 的性能。</li>
<li>methods: 这个方法基于一个新的测度名为“cross-sharpness”，它测量了两个不同变数的关系。这个测度可以确保学习过程中的模型在标数数据和无标数数据上的学习性能是一致的。</li>
<li>results: 这个方法可以在许多 SSL  Setting中取得最佳的结果，并且可以将无标数数据中的学习性能与标数数据中的学习性能连接起来，以提高 SSL 的性能。<details>
<summary>Abstract</summary>
Semi-Supervised Learning (SSL) has been an effective way to leverage abundant unlabeled data with extremely scarce labeled data. However, most SSL methods are commonly based on instance-wise consistency between different data transformations. Therefore, the label guidance on labeled data is hard to be propagated to unlabeled data. Consequently, the learning process on labeled data is much faster than on unlabeled data which is likely to fall into a local minima that does not favor unlabeled data, leading to sub-optimal generalization performance. In this paper, we propose FlatMatch which minimizes a cross-sharpness measure to ensure consistent learning performance between the two datasets. Specifically, we increase the empirical risk on labeled data to obtain a worst-case model which is a failure case that needs to be enhanced. Then, by leveraging the richness of unlabeled data, we penalize the prediction difference (i.e., cross-sharpness) between the worst-case model and the original model so that the learning direction is beneficial to generalization on unlabeled data. Therefore, we can calibrate the learning process without being limited to insufficient label information. As a result, the mismatched learning performance can be mitigated, further enabling the effective exploitation of unlabeled data and improving SSL performance. Through comprehensive validation, we show FlatMatch achieves state-of-the-art results in many SSL settings.
</details>
<details>
<summary>摘要</summary>
半指导学习（SSL）是一种有效地利用充沛的无标签数据和极其罕见的标签数据。然而，大多数SSL方法通常基于实例级别的一致性，因此将标签指导从标签数据传递到无标签数据很困难。因此，在标签数据上学习的过程比在无标签数据上更快，可能导致欠拟合性问题，从而影响泛化性表现。在这篇论文中，我们提出了平滑匹配（FlatMatch），它使用跨锐度度量来保证两个数据集之间的学习表现的一致性。具体来说，我们通过提高标签数据上的Empirical risk来获得一个最坏情况模型，这是一个需要改进的失败情况。然后，通过利用无标签数据的质量，我们对最坏情况模型和原始模型之间的预测差（i.e., 跨锐度度量）进行惩罚，以便通过提高学习方向来优化泛化性表现。因此，我们可以不受限于不充分的标签信息进行调整学习过程。通过全面验证，我们显示了FlatMatch在许多SSL设置中实现了状态机器的 результа。
</details></li>
</ul>
<hr>
<h2 id="Multiple-Key-value-Strategy-in-Recommendation-Systems-Incorporating-Large-Language-Model"><a href="#Multiple-Key-value-Strategy-in-Recommendation-Systems-Incorporating-Large-Language-Model" class="headerlink" title="Multiple Key-value Strategy in Recommendation Systems Incorporating Large Language Model"></a>Multiple Key-value Strategy in Recommendation Systems Incorporating Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16409">http://arxiv.org/abs/2310.16409</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dui Wang, Xiangyu Hou, Xiaohui Yang, Bo Zhang, Renbing Chen, Daiyue Xue<br>for: 这篇论文的目的是提出一种基于多重键值数据的sequential recommendation方法，以帮助在实际应用中处理多个键值数据。methods: 该方法利用了大型自然语言模型(LLM)的特性，通过适应域知识注入来增强RS的表现。此外，该方法还提出了一种创新的洗牌和隐藏策略来解决LLM在多个键值数据上学习的问题。results: 经过广泛的实验 validate，该方法在MovieLens dataset上显示出了良好的效果，能够好好地完成多个键值数据的sequential recommendation问题。<details>
<summary>Abstract</summary>
Recommendation system (RS) plays significant roles in matching users information needs for Internet applications, and it usually utilizes the vanilla neural network as the backbone to handle embedding details. Recently, the large language model (LLM) has exhibited emergent abilities and achieved great breakthroughs both in the CV and NLP communities. Thus, it is logical to incorporate RS with LLM better, which has become an emerging research direction. Although some existing works have made their contributions to this issue, they mainly consider the single key situation (e.g. historical interactions), especially in sequential recommendation. The situation of multiple key-value data is simply neglected. This significant scenario is mainstream in real practical applications, where the information of users (e.g. age, occupation, etc) and items (e.g. title, category, etc) has more than one key. Therefore, we aim to implement sequential recommendations based on multiple key-value data by incorporating RS with LLM. In particular, we instruct tuning a prevalent open-source LLM (Llama 7B) in order to inject domain knowledge of RS into the pre-trained LLM. Since we adopt multiple key-value strategies, LLM is hard to learn well among these keys. Thus the general and innovative shuffle and mask strategies, as an innovative manner of data argument, are designed. To demonstrate the effectiveness of our approach, extensive experiments are conducted on the popular and suitable dataset MovieLens which contains multiple keys-value. The experimental results demonstrate that our approach can nicely and effectively complete this challenging issue.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Information-Theoretic-Generalization-Analysis-for-Topology-aware-Heterogeneous-Federated-Edge-Learning-over-Noisy-Channels"><a href="#Information-Theoretic-Generalization-Analysis-for-Topology-aware-Heterogeneous-Federated-Edge-Learning-over-Noisy-Channels" class="headerlink" title="Information-Theoretic Generalization Analysis for Topology-aware Heterogeneous Federated Edge Learning over Noisy Channels"></a>Information-Theoretic Generalization Analysis for Topology-aware Heterogeneous Federated Edge Learning over Noisy Channels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16407">http://arxiv.org/abs/2310.16407</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheshun Wu, Zenglin Xu, Hongfang Yu, Jie Liu</li>
<li>for: 本研究旨在探讨 Federated Edge Learning (FEEL) 中的泛化问题，即在无isy通道和多样环境下，移动设备进行模型参数的传输和数据收集，以及设备之间的协同通信对模型的泛化的影响。</li>
<li>methods: 本研究采用信息论的泛化分析方法，对于 topology-aware FEEL 中的数据不同性和噪声通道的影响进行了全面的检查。此外，我们还提出了一种新的常见规范方法 called Federated Global Mutual Information Reduction (FedGMIR)，用于提高模型的性能。</li>
<li>results: 数据 validate our theoretical findings and provide evidence for the effectiveness of the proposed method.<details>
<summary>Abstract</summary>
With the rapid growth of edge intelligence, the deployment of federated learning (FL) over wireless networks has garnered increasing attention, which is called Federated Edge Learning (FEEL). In FEEL, both mobile devices transmitting model parameters over noisy channels and collecting data in diverse environments pose challenges to the generalization of trained models. Moreover, devices can engage in decentralized FL via Device-to-Device communication while the communication topology of connected devices also impacts the generalization of models. Most recent theoretical studies overlook the incorporation of all these effects into FEEL when developing generalization analyses. In contrast, our work presents an information-theoretic generalization analysis for topology-aware FEEL in the presence of data heterogeneity and noisy channels. Additionally, we propose a novel regularization method called Federated Global Mutual Information Reduction (FedGMIR) to enhance the performance of models based on our analysis. Numerical results validate our theoretical findings and provide evidence for the effectiveness of the proposed method.
</details>
<details>
<summary>摘要</summary>
随着边缘智能的快速发展， Federated Edge Learning（FEEL）在无线网络上进行的部署吸引了越来越多的关注。在 FEEL 中，移动设备通过含杂annel 传输模型参数并收集数据在多样化环境中具有挑战，这些挑战对训练模型的泛化造成了影响。此外，设备可以通过设备之间的 Device-to-Device 通信参与到分布式 FL 中，而连接设备的通信结构也会影响模型的泛化。最近的理论研究忽视了这些效应在 FEEL 中进行泛化分析。相比之下，我们的工作提供了一种信息论的泛化分析方法，该方法考虑了数据多样性和含杂annel 的影响。此外，我们还提出了一种新的规范方法，即 Federated Global Mutual Information Reduction（FedGMIR），以提高模型性能。数值结果证明了我们的理论发现，并提供了 FEEL 中模型性能的提升证据。
</details></li>
</ul>
<hr>
<h2 id="Graph-Neural-Networks-with-a-Distribution-of-Parametrized-Graphs"><a href="#Graph-Neural-Networks-with-a-Distribution-of-Parametrized-Graphs" class="headerlink" title="Graph Neural Networks with a Distribution of Parametrized Graphs"></a>Graph Neural Networks with a Distribution of Parametrized Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16401">http://arxiv.org/abs/2310.16401</a></li>
<li>repo_url: None</li>
<li>paper_authors: See Hian Lee, Feng Ji, Kelin Xia, Wee Peng Tay</li>
<li>for: 提高图像分类和图像回归的性能，捕捉更多的信息</li>
<li>methods: 使用 latent variables Parameterizing multiple graphs，基于 Expectation-Maximization 框架和 Markov Chain Monte Carlo 方法</li>
<li>results: 在hetroogeneous graph 和化学数据集上对节点分类和图像回归实现了提高性能，比基eline模型更好Here’s a breakdown of each point:</li>
<li>for: The paper is written to improve the performance of graph neural networks on node classification and graph regression tasks by incorporating additional information from multiple graphs.</li>
<li>methods: The authors introduce latent variables to parameterize and generate multiple graphs, and use an Expectation-Maximization framework and Markov Chain Monte Carlo method to obtain the maximum likelihood estimate of the network parameters.</li>
<li>results: The authors demonstrate improvements in performance against baseline models on node classification for heterogeneous graphs and graph regression on chemistry datasets.<details>
<summary>Abstract</summary>
Traditionally, graph neural networks have been trained using a single observed graph. However, the observed graph represents only one possible realization. In many applications, the graph may encounter uncertainties, such as having erroneous or missing edges, as well as edge weights that provide little informative value. To address these challenges and capture additional information previously absent in the observed graph, we introduce latent variables to parameterize and generate multiple graphs. We obtain the maximum likelihood estimate of the network parameters in an Expectation-Maximization (EM) framework based on the multiple graphs. Specifically, we iteratively determine the distribution of the graphs using a Markov Chain Monte Carlo (MCMC) method, incorporating the principles of PAC-Bayesian theory. Numerical experiments demonstrate improvements in performance against baseline models on node classification for heterogeneous graphs and graph regression on chemistry datasets.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-Efficient-Surrogate-Dynamic-Models-with-Graph-Spline-Networks"><a href="#Learning-Efficient-Surrogate-Dynamic-Models-with-Graph-Spline-Networks" class="headerlink" title="Learning Efficient Surrogate Dynamic Models with Graph Spline Networks"></a>Learning Efficient Surrogate Dynamic Models with Graph Spline Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16397">http://arxiv.org/abs/2310.16397</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kaist-silab/graphsplinenets">https://github.com/kaist-silab/graphsplinenets</a></li>
<li>paper_authors: Chuanbo Hua, Federico Berto, Michael Poli, Stefano Massaroli, Jinkyoo Park</li>
<li>for: 这篇论文旨在提高物理系统预测的效率，使用深度学习方法来简化网格大小和迭代步骤。</li>
<li>methods: 本文提出了GraphSplineNets，一种新的深度学习方法，利用两个可微的正交拓拨方法来高效预测时间和空间中的回应。此外，我们也提出了一个适应标本策略，优先在重要区域进行标本。</li>
<li>results: 本文透过处理多种物理系统，包括热方程、振荡波传播、奈奎-斯托克方程和真实世界的海洋流体，以及训练过程中的测试和评估，发现GraphSplineNets可以提高预测精度和速度的对应关系。<details>
<summary>Abstract</summary>
While complex simulations of physical systems have been widely used in engineering and scientific computing, lowering their often prohibitive computational requirements has only recently been tackled by deep learning approaches. In this paper, we present GraphSplineNets, a novel deep-learning method to speed up the forecasting of physical systems by reducing the grid size and number of iteration steps of deep surrogate models. Our method uses two differentiable orthogonal spline collocation methods to efficiently predict response at any location in time and space. Additionally, we introduce an adaptive collocation strategy in space to prioritize sampling from the most important regions. GraphSplineNets improve the accuracy-speedup tradeoff in forecasting various dynamical systems with increasing complexity, including the heat equation, damped wave propagation, Navier-Stokes equations, and real-world ocean currents in both regular and irregular domains.
</details>
<details>
<summary>摘要</summary>
traditional simulations of physical systems have been widely used in engineering and scientific computing, but their high computational requirements have only recently been addressed by deep learning methods. In this paper, we present GraphSplineNets, a novel deep-learning approach that speeds up the forecasting of physical systems by reducing the grid size and number of iteration steps of deep surrogate models. Our method uses two differentiable orthogonal spline collocation methods to efficiently predict responses at any location in time and space. Additionally, we introduce an adaptive collocation strategy in space to prioritize sampling from the most important regions. GraphSplineNets improve the accuracy-speedup tradeoff in forecasting various dynamical systems with increasing complexity, including the heat equation, damped wave propagation, Navier-Stokes equations, and real-world ocean currents in both regular and irregular domains.Note that Simplified Chinese is a romanization of Chinese, and the actual Chinese characters may be different.
</details></li>
</ul>
<hr>
<h2 id="Distributed-Uncertainty-Quantification-of-Kernel-Interpolation-on-Spheres"><a href="#Distributed-Uncertainty-Quantification-of-Kernel-Interpolation-on-Spheres" class="headerlink" title="Distributed Uncertainty Quantification of Kernel Interpolation on Spheres"></a>Distributed Uncertainty Quantification of Kernel Interpolation on Spheres</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16384">http://arxiv.org/abs/2310.16384</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shao-Bo Lin, Xingping Sun, Di Wang</li>
<li>for: 这 paper 是关于 radial basis function (RBF) kernel interpolation of scattered data 的研究，具体来说是研究如何对具有较大规模噪声数据的射频数据进行插值，以及如何管理和评估插值过程中的不确定性。</li>
<li>methods: 这 paper 使用了分布式插值方法来处理具有较大规模噪声数据的射频数据，并通过对不确定性进行评估和管理来提高插值精度和稳定性。</li>
<li>results: 该 paper 的数据示出了该方法在处理具有较大规模噪声数据的射频数据时的实用性和稳定性，并且可以减少对不确定性的影响，从而提高插值精度。<details>
<summary>Abstract</summary>
For radial basis function (RBF) kernel interpolation of scattered data, Schaback in 1995 proved that the attainable approximation error and the condition number of the underlying interpolation matrix cannot be made small simultaneously. He referred to this finding as an "uncertainty relation", an undesirable consequence of which is that RBF kernel interpolation is susceptible to noisy data. In this paper, we propose and study a distributed interpolation method to manage and quantify the uncertainty brought on by interpolating noisy spherical data of non-negligible magnitude. We also present numerical simulation results showing that our method is practical and robust in terms of handling noisy data from challenging computing environments.
</details>
<details>
<summary>摘要</summary>
For radial basis function (RBF) kernel interpolation of scattered data, Schaback in 1995 proved that the attainable approximation error and the condition number of the underlying interpolation matrix cannot be made small simultaneously. He referred to this finding as an "uncertainty relation", an undesirable consequence of which is that RBF kernel interpolation is susceptible to noisy data. In this paper, we propose and study a distributed interpolation method to manage and quantify the uncertainty brought on by interpolating noisy spherical data of non-negligible magnitude. We also present numerical simulation results showing that our method is practical and robust in terms of handling noisy data from challenging computing environments.Here's the translation in Traditional Chinese:For radial basis function (RBF) kernel interpolation of scattered data, Schaback in 1995 proved that the attainable approximation error and the condition number of the underlying interpolation matrix cannot be made small simultaneously. He referred to this finding as an "uncertainty relation", an undesirable consequence of which is that RBF kernel interpolation is susceptible to noisy data. In this paper, we propose and study a distributed interpolation method to manage and quantify the uncertainty brought on by interpolating noisy spherical data of non-negligible magnitude. We also present numerical simulation results showing that our method is practical and robust in terms of handling noisy data from challenging computing environments.
</details></li>
</ul>
<hr>
<h2 id="A-model-for-multi-attack-classification-to-improve-intrusion-detection-performance-using-deep-learning-approaches"><a href="#A-model-for-multi-attack-classification-to-improve-intrusion-detection-performance-using-deep-learning-approaches" class="headerlink" title="A model for multi-attack classification to improve intrusion detection performance using deep learning approaches"></a>A model for multi-attack classification to improve intrusion detection performance using deep learning approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16380">http://arxiv.org/abs/2310.16380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arun Kumar Silivery, Ram Mohan Rao Kovvur</li>
<li>for: 本研究旨在开发一种可靠的攻击检测机制，以 помочь发现恶意攻击。</li>
<li>methods: 该研究提出了一种深度学习方法框架，包括三种方法：Long-Short Term Memory Recurrent Neural Network (LSTM-RNN)、Recurrent Neural Network (RNN) 和 Deep Neural Network (DNN)。</li>
<li>results: 研究结果显示，LSTM-RNN WITH adamax 优化器在 NSL-KDD 数据集上表现出色，在准确率、检测率和假阳性率方面超过了现有的浅学习和深度学习模型。此外，多模型方法也在 KDD99、NSL-KDD 和 UNSWNB15 数据集上提供了显著的性能。<details>
<summary>Abstract</summary>
This proposed model introduces novel deep learning methodologies. The objective here is to create a reliable intrusion detection mechanism to help identify malicious attacks. Deep learning based solution framework is developed consisting of three approaches. The first approach is Long-Short Term Memory Recurrent Neural Network (LSTM-RNN) with seven optimizer functions such as adamax, SGD, adagrad, adam, RMSprop, nadam and adadelta. The model is evaluated on NSL-KDD dataset and classified multi attack classification. The model has outperformed with adamax optimizer in terms of accuracy, detection rate and low false alarm rate. The results of LSTM-RNN with adamax optimizer is compared with existing shallow machine and deep learning models in terms of accuracy, detection rate and low false alarm rate. The multi model methodology consisting of Recurrent Neural Network (RNN), Long-Short Term Memory Recurrent Neural Network (LSTM-RNN), and Deep Neural Network (DNN). The multi models are evaluated on bench mark datasets such as KDD99, NSL-KDD, and UNSWNB15 datasets. The models self-learnt the features and classifies the attack classes as multi-attack classification. The models RNN, and LSTM-RNN provide considerable performance compared to other existing methods on KDD99 and NSL-KDD dataset
</details>
<details>
<summary>摘要</summary>
这种提议的模型引入了新的深度学习方法ologies。目标是创建一个可靠的攻击检测机制，以帮助标识恶意攻击。基于深度学习的解决方案框架由三种方法组成：首先是Long-Short Term Memory Recurrent Neural Network (LSTM-RNN)  WITH seven optimizer functions such as adamax, SGD, adagrad, adam, RMSprop, nadam和 adadelta。模型在 NSL-KDD 数据集上进行评估，并将多种攻击分类。模型使用 adamax 优化器时在准确率、检测率和假阳性率方面占据了领先地位。对于 LSTM-RNN  WITH adamax 优化器的结果进行比较，与现有的浅学习和深度学习模型在准确率、检测率和假阳性率方面的性能。此外，我们还提出了一种多模型方法，包括 Recurrent Neural Network (RNN)、Long-Short Term Memory Recurrent Neural Network (LSTM-RNN) 和 Deep Neural Network (DNN)。这些模型在 KDD99、NSL-KDD 和 UNSWNB15 数据集上进行评估，并自学习特征以进行多类攻击分类。RNN 和 LSTM-RNN 模型在 KDD99 和 NSL-KDD 数据集上表现出了显著的优异性，与其他现有方法相比。
</details></li>
</ul>
<hr>
<h2 id="DyExplainer-Explainable-Dynamic-Graph-Neural-Networks"><a href="#DyExplainer-Explainable-Dynamic-Graph-Neural-Networks" class="headerlink" title="DyExplainer: Explainable Dynamic Graph Neural Networks"></a>DyExplainer: Explainable Dynamic Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16375">http://arxiv.org/abs/2310.16375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianchun Wang, Dongsheng Luo, Wei Cheng, Haifeng Chen, Xiang Zhang</li>
<li>for: 这 paper 的目的是解释动态图神经网络（GNNs）的含义，以便更好地理解和信任这些模型，从而扩大其在重要应用场景中的使用。</li>
<li>methods: 这 paper 使用了一种名为 DyExplainer 的新方法，它在运行时对动态 GNNs 进行解释。DyExplainer 使用了一种动态 GNN 的干扰注意力技术，同时通过对比学习技术来保持结构一致性和时间连续性。</li>
<li>results: 该 paper 的实验结果表明，DyExplainer 不仅能够 faithful 地解释模型预测结果，还能够显著提高模型预测精度，如在链接预测任务中。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) resurge as a trending research subject owing to their impressive ability to capture representations from graph-structured data. However, the black-box nature of GNNs presents a significant challenge in terms of comprehending and trusting these models, thereby limiting their practical applications in mission-critical scenarios. Although there has been substantial progress in the field of explaining GNNs in recent years, the majority of these studies are centered on static graphs, leaving the explanation of dynamic GNNs largely unexplored. Dynamic GNNs, with their ever-evolving graph structures, pose a unique challenge and require additional efforts to effectively capture temporal dependencies and structural relationships. To address this challenge, we present DyExplainer, a novel approach to explaining dynamic GNNs on the fly. DyExplainer trains a dynamic GNN backbone to extract representations of the graph at each snapshot, while simultaneously exploring structural relationships and temporal dependencies through a sparse attention technique. To preserve the desired properties of the explanation, such as structural consistency and temporal continuity, we augment our approach with contrastive learning techniques to provide priori-guided regularization. To model longer-term temporal dependencies, we develop a buffer-based live-updating scheme for training. The results of our extensive experiments on various datasets demonstrate the superiority of DyExplainer, not only providing faithful explainability of the model predictions but also significantly improving the model prediction accuracy, as evidenced in the link prediction task.
</details>
<details>
<summary>摘要</summary>
GRAPH Neural Networks (GNNs) once again become a popular research topic due to their impressive ability to capture representations from graph-structured data. However, the black-box nature of GNNs presents a significant challenge in terms of understanding and trusting these models, which limits their practical applications in critical scenarios. Although there has been substantial progress in the field of explaining GNNs in recent years, most of these studies focus on static graphs, leaving the explanation of dynamic GNNs largely unexplored. Dynamic GNNs, with their ever-changing graph structures, pose a unique challenge and require additional efforts to effectively capture temporal dependencies and structural relationships. To address this challenge, we propose DyExplainer, a novel approach to explaining dynamic GNNs on the fly. DyExplainer trains a dynamic GNN backbone to extract representations of the graph at each snapshot, while simultaneously exploring structural relationships and temporal dependencies through a sparse attention technique. To preserve the desired properties of the explanation, such as structural consistency and temporal continuity, we augment our approach with contrastive learning techniques to provide prior-guided regularization. To model longer-term temporal dependencies, we develop a buffer-based live-updating scheme for training. The results of our extensive experiments on various datasets demonstrate the superiority of DyExplainer, not only providing faithful explainability of the model predictions but also significantly improving the model prediction accuracy, as evidenced in the link prediction task.
</details></li>
</ul>
<hr>
<h2 id="Joint-Distributional-Learning-via-Cramer-Wold-Distance"><a href="#Joint-Distributional-Learning-via-Cramer-Wold-Distance" class="headerlink" title="Joint Distributional Learning via Cramer-Wold Distance"></a>Joint Distributional Learning via Cramer-Wold Distance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16374">http://arxiv.org/abs/2310.16374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seunghwan An, Jong-June Jeon</li>
<li>for: 提高高维数据集中变量 conditional independence 假设的限制，以适应复杂的 correlation 结构和高维数据集。</li>
<li>methods: 提出了 Cramer-Wold 距离正则化，可以在关闭式计算中实现高维数据集的共同分布学习。同时，我们提出了一种两步学习方法，以便灵活地设定先验分布和改进 posterior 和先验分布的对齐。</li>
<li>results: 通过对高维数据集进行synthetic数据生成测试，我们证明了我们的提议方法的效果。由于许多现有的数据集和数据科学应用都包含多 categorical 变量，我们的实验表明了我们的方法的 universal 性。<details>
<summary>Abstract</summary>
The assumption of conditional independence among observed variables, primarily used in the Variational Autoencoder (VAE) decoder modeling, has limitations when dealing with high-dimensional datasets or complex correlation structures among observed variables. To address this issue, we introduced the Cramer-Wold distance regularization, which can be computed in a closed-form, to facilitate joint distributional learning for high-dimensional datasets. Additionally, we introduced a two-step learning method to enable flexible prior modeling and improve the alignment between the aggregated posterior and the prior distribution. Furthermore, we provide theoretical distinctions from existing methods within this category. To evaluate the synthetic data generation performance of our proposed approach, we conducted experiments on high-dimensional datasets with multiple categorical variables. Given that many readily available datasets and data science applications involve such datasets, our experiments demonstrate the effectiveness of our proposed methodology.
</details>
<details>
<summary>摘要</summary>
假设独立性 Among observed variables的假设，通常用于Variational Autoencoder（VAE）解oder模型中，对高维度数据集或复杂的变量结构存在限制。为了解决这个问题，我们引入了Cramer-Wold distance regularization，可以在关闭式中计算，以便实现高维度数据集中的共同分布学习。此外，我们引入了两步学习方法，以提高对汇集 posterior和先前分布的整合。此外，我们还提供了与现有方法的理论区别。为评估我们提出的方法的 sintetic data生成性能，我们在高维度数据集中进行了实验，其中许多 readily available datasets和数据科学应用都包含这类数据。我们的实验显示了我们提出的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Finite-Time-Analysis-of-Constrained-Actor-Critic-and-Constrained-Natural-Actor-Critic-Algorithms"><a href="#Finite-Time-Analysis-of-Constrained-Actor-Critic-and-Constrained-Natural-Actor-Critic-Algorithms" class="headerlink" title="Finite Time Analysis of Constrained Actor Critic and Constrained Natural Actor Critic Algorithms"></a>Finite Time Analysis of Constrained Actor Critic and Constrained Natural Actor Critic Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16363">http://arxiv.org/abs/2310.16363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prashansa Panda, Shalabh Bhatnagar</li>
<li>for: 本文研究了actor critic和natural actor critic算法在受限制Markov决策过程（C-MDP）中的应用，特别是当状态-动作空间较大时。</li>
<li>methods: 本文使用了actor critic和natural actor critic算法，并使用了函数近似来处理受限制MDP中的不等约束。</li>
<li>results: 本文通过非假设分析表明，actor critic和natural actor critic算法在非i.i.d（Markovian） Setting下可以 garantúa找到一个首ORDER站点（即 $\Vert \nabla L(\theta,\gamma)\Vert_2^2 \leq \epsilon$），并且其样本复杂度为 $\mathcal{\tilde{O}(\epsilon^{-2.5})$。此外，在一些网格世界设置下进行了实验，并观察到了良好的实验性能。<details>
<summary>Abstract</summary>
Actor Critic methods have found immense applications on a wide range of Reinforcement Learning tasks especially when the state-action space is large. In this paper, we consider actor critic and natural actor critic algorithms with function approximation for constrained Markov decision processes (C-MDP) involving inequality constraints and carry out a non-asymptotic analysis for both of these algorithms in a non-i.i.d (Markovian) setting. We consider the long-run average cost criterion where both the objective and the constraint functions are suitable policy-dependent long-run averages of certain prescribed cost functions. We handle the inequality constraints using the Lagrange multiplier method. We prove that these algorithms are guaranteed to find a first-order stationary point (i.e., $\Vert \nabla L(\theta,\gamma)\Vert_2^2 \leq \epsilon$) of the performance (Lagrange) function $L(\theta,\gamma)$, with a sample complexity of $\mathcal{\tilde{O}(\epsilon^{-2.5})$ in the case of both Constrained Actor Critic (C-AC) and Constrained Natural Actor Critic (C-NAC) algorithms.We also show the results of experiments on a few different grid world settings and observe good empirical performance using both of these algorithms. In particular, for large grid sizes, Constrained Natural Actor Critic shows slightly better results than Constrained Actor Critic while the latter is slightly better for a small grid size.
</details>
<details>
<summary>摘要</summary>
actor-critic方法在各种强化学习任务上发现了广泛的应用，特别是当状态动作空间很大时。在这篇论文中，我们考虑actor-critic和自然actor-critic算法，并使用函数近似来解决受约束的马可夫决策过程（C-MDP）中的不等约束。我们使用长期均值成本函数，其中对象函数和约束函数都是适用于指定成本函数的长期均值。我们使用拉格朗日积分法来处理不等约束。我们证明了这些算法可以在非i.i.d（Markovian）设置下找到一个第一阶站点（即 $\Vert \nabla L(\theta,\gamma)\Vert_2^2 \leq \epsilon$）的性能（拉格朗日）函数$L(\theta,\gamma)$的first-order站点，并且其样本复杂度为 $\mathcal{\tilde{O}(\epsilon^{-2.5})$。我们还进行了一些网格世界的实验，并观察了这两种算法在各种网格大小下的良好实验性。特别是在大网格大小下，自然actor-critic算法表现slightly better than Constrained Actor Critic算法，而后者在小网格大小下表现slightly better。
</details></li>
</ul>
<hr>
<h2 id="Neural-Potential-Field-for-Obstacle-Aware-Local-Motion-Planning"><a href="#Neural-Potential-Field-for-Obstacle-Aware-Local-Motion-Planning" class="headerlink" title="Neural Potential Field for Obstacle-Aware Local Motion Planning"></a>Neural Potential Field for Obstacle-Aware Local Motion Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16362">http://arxiv.org/abs/2310.16362</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cog-isa/npfield">https://github.com/cog-isa/npfield</a></li>
<li>paper_authors: Muhammad Alhaddad, Konstantin Mironov, Aleksey Staroverov, Aleksandr Panov</li>
<li>for: 本文旨在提供基于预测模型的移动 робот平台的本地规划方法。</li>
<li>methods: 本文提出了一种基于神经网络的潜在场的射程场，该模型基于机器人姿态、障碍物地图和机器人脚印的 differentiable 射程场，可以在 MPC  solving 中使用。</li>
<li>results: 对比试验表明，提出的方法可以与现有的本地规划器相比，提供了更平滑的轨迹、相对较短的路径长度和安全的距离障碍物。实验结果表明，该方法可以在 Husky UGV 移动机器人上实现实时和安全的本地规划。<details>
<summary>Abstract</summary>
Model predictive control (MPC) may provide local motion planning for mobile robotic platforms. The challenging aspect is the analytic representation of collision cost for the case when both the obstacle map and robot footprint are arbitrary. We propose a Neural Potential Field: a neural network model that returns a differentiable collision cost based on robot pose, obstacle map, and robot footprint. The differentiability of our model allows its usage within the MPC solver. It is computationally hard to solve problems with a very high number of parameters. Therefore, our architecture includes neural image encoders, which transform obstacle maps and robot footprints into embeddings, which reduce problem dimensionality by two orders of magnitude. The reference data for network training are generated based on algorithmic calculation of a signed distance function. Comparative experiments showed that the proposed approach is comparable with existing local planners: it provides trajectories with outperforming smoothness, comparable path length, and safe distance from obstacles. Experiment on Husky UGV mobile robot showed that our approach allows real-time and safe local planning. The code for our approach is presented at https://github.com/cog-isa/NPField together with demo video.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Redco-A-Lightweight-Tool-to-Automate-Distributed-Training-of-LLMs-on-Any-GPU-TPUs"><a href="#Redco-A-Lightweight-Tool-to-Automate-Distributed-Training-of-LLMs-on-Any-GPU-TPUs" class="headerlink" title="Redco: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU&#x2F;TPUs"></a>Redco: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU&#x2F;TPUs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16355">http://arxiv.org/abs/2310.16355</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tanyuqian/redco">https://github.com/tanyuqian/redco</a></li>
<li>paper_authors: Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong Chen, Eric Xing, Zhiting Hu</li>
<li>for: 这篇论文主要是为了提供一个轻量级、易用的工具来自动分布式训练和推理 dla 大型自然语言模型 (LLM)，以及简化 ML 管道的开发。</li>
<li>methods: 该论文使用了两个简单的规则来生成 tensor 平行策略，以便轻松地分布式训练和推理 LLM。此外，论文还提出了一种机制，allowing for the customization of diverse ML pipelines through the definition of merely three functions。</li>
<li>results: 论文通过应用 Redco 在一些 LLlM 架构上，如 GPT-J、LLaMA、T5 和 OPT， demonstrate 了其效果，并且比官方实现更少的代码行数。<details>
<summary>Abstract</summary>
The recent progress of AI can be largely attributed to large language models (LLMs). However, their escalating memory requirements introduce challenges for machine learning (ML) researchers and engineers. Addressing this requires developers to partition a large model to distribute it across multiple GPUs or TPUs. This necessitates considerable coding and intricate configuration efforts with existing model parallel tools, such as Megatron-LM, DeepSpeed, and Alpa. These tools require users' expertise in machine learning systems (MLSys), creating a bottleneck in LLM development, particularly for developers without MLSys background. In this work, we present Redco, a lightweight and user-friendly tool crafted to automate distributed training and inference for LLMs, as well as to simplify ML pipeline development. The design of Redco emphasizes two key aspects. Firstly, to automate model parallism, our study identifies two straightforward rules to generate tensor parallel strategies for any given LLM. Integrating these rules into Redco facilitates effortless distributed LLM training and inference, eliminating the need of additional coding or complex configurations. We demonstrate the effectiveness by applying Redco on a set of LLM architectures, such as GPT-J, LLaMA, T5, and OPT, up to the size of 66B. Secondly, we propose a mechanism that allows for the customization of diverse ML pipelines through the definition of merely three functions, eliminating redundant and formulaic code like multi-host related processing. This mechanism proves adaptable across a spectrum of ML algorithms, from foundational language modeling to complex algorithms like meta-learning and reinforcement learning. Consequently, Redco implementations exhibit much fewer code lines compared to their official counterparts.
</details>
<details>
<summary>摘要</summary>
Recent progress in AI 可以归功于大语言模型 (LLM)。然而，这些模型的内存需求在机器学习 (ML) 研究人员和工程师面临挑战。为 Addressing this, developers need to divide a large model into smaller parts and distribute it across multiple GPUs or TPUs。这需要较多的编程和配置工作，包括使用现有的模型并行工具，如 Megatron-LM、DeepSpeed 和 Alpa。这些工具需要用户具有机器学习系统 (MLSys) 背景，从而成为 LLM 开发中的瓶颈，特别是对没有 MLSys 背景的开发者而言。在这种情况下，我们提出了 Redco，一个轻量级的和易于使用的工具，用于自动化分布式训练和推理 для LLM，以及简化 ML 管道开发。Redco 的设计强调两个关键方面。首先，通过生成简单的两个规则，我们可以自动实现任何给定的 LLM 的模型并行策略。将这些规则集成到 Redco 中，使得分布式 LLM 训练和推理变得非常简单，不需要额外的编程或复杂的配置。我们在一些 LLM 架构，如 GPT-J、LLaMA、T5 和 OPT，上进行了应用，并达到了66B的规模。其次，我们提出了一种机制，allowing for the customization of diverse ML pipelines through the definition of merely three functions。这种机制可以适应多种 ML 算法，从基础语言模型到复杂的算法，如元学习和强化学习。因此，Redco 的实现比官方对应的实现更少了代码行数。
</details></li>
</ul>
<hr>
<h2 id="SMURF-THP-Score-Matching-based-UnceRtainty-quantiFication-for-Transformer-Hawkes-Process"><a href="#SMURF-THP-Score-Matching-based-UnceRtainty-quantiFication-for-Transformer-Hawkes-Process" class="headerlink" title="SMURF-THP: Score Matching-based UnceRtainty quantiFication for Transformer Hawkes Process"></a>SMURF-THP: Score Matching-based UnceRtainty quantiFication for Transformer Hawkes Process</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16336">http://arxiv.org/abs/2310.16336</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zichongli5/smurf-thp">https://github.com/zichongli5/smurf-thp</a></li>
<li>paper_authors: Zichong Li, Yanbo Xu, Simiao Zuo, Haoming Jiang, Chao Zhang, Tuo Zhao, Hongyuan Zha</li>
<li>for: 模型事件序列数据的Transformer Hawkes过程模型，但大多数现有的训练方法仍然基于最大化事件序列的概率，这会导致计算不可分解的积分。此外，现有方法无法提供模型预测结果的不确定性评估，例如预测事件到达时间的信任区间。</li>
<li>methods: SMURF-THP方法基于分数函数对事件的到达时间进行学习和预测，并通过分数匹配目标函数来避免计算不可分解的积分。</li>
<li>results: 在事件类型预测和到达时间不确定性评估中，SMURF-THP方法在置信度抑制中表现出色，同时保持与现有概率基于方法相当的预测精度。<details>
<summary>Abstract</summary>
Transformer Hawkes process models have shown to be successful in modeling event sequence data. However, most of the existing training methods rely on maximizing the likelihood of event sequences, which involves calculating some intractable integral. Moreover, the existing methods fail to provide uncertainty quantification for model predictions, e.g., confidence intervals for the predicted event's arrival time. To address these issues, we propose SMURF-THP, a score-based method for learning Transformer Hawkes process and quantifying prediction uncertainty. Specifically, SMURF-THP learns the score function of events' arrival time based on a score-matching objective that avoids the intractable computation. With such a learned score function, we can sample arrival time of events from the predictive distribution. This naturally allows for the quantification of uncertainty by computing confidence intervals over the generated samples. We conduct extensive experiments in both event type prediction and uncertainty quantification of arrival time. In all the experiments, SMURF-THP outperforms existing likelihood-based methods in confidence calibration while exhibiting comparable prediction accuracy.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用Transformer Hawkes过程模型成功地处理事件序列数据，但大多数现有训练方法都是基于最大化事件序列的极高概率，这些方法通常需要计算一些不可 Calculate some intractable integral.此外，现有的方法无法提供预测结果的uncertainty量评估，例如预测事件到达时间的信任区间。为解决这些问题，我们提出了SMURF-THP，一种基于分数函数的方法，用于学习Transformer Hawkes过程和评估预测结果的uncertainty。具体来说，SMURF-THP学习事件到达时间的分数函数，基于分数匹配目标函数，而不需要计算不可 Calculate some intractable integral。通过这种学习的分数函数，我们可以从预测分布中采样到达时间，从而自然地计算预测结果的uncertainty。我们在事件类型预测和到达时间uncertainty量评估中进行了广泛的实验，并在所有实验中，SMURF-THP在信任报表中保持更好的报表 while exhibiting comparable prediction accuracy。Note: Simplified Chinese is also known as "Mandarin" or "Standard Chinese".
</details></li>
</ul>
<hr>
<h2 id="Defense-Against-Model-Extraction-Attacks-on-Recommender-Systems"><a href="#Defense-Against-Model-Extraction-Attacks-on-Recommender-Systems" class="headerlink" title="Defense Against Model Extraction Attacks on Recommender Systems"></a>Defense Against Model Extraction Attacks on Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16335">http://arxiv.org/abs/2310.16335</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sixiao Zhang, Hongzhi Yin, Hongxu Chen, Cheng Long</li>
<li>for: 本研究旨在提高推荐系统的Robustness，尤其是针对模型提取攻击。</li>
<li>methods: 本文提出了一种名为Gradient-based Ranking Optimization（GRO）的防御策略，用于对模型提取攻击进行防御。GRO将防御问题定义为一个优化问题，目标是将保护目标模型的损失降低到最低，同时将攻击者的代理模型的损失增加到最高。在实现GRO时，我们使用了 swap matrices来代替top-k排名列表，以使用 student model来模拟攻击者的代理模型。</li>
<li>results: 我们在三个 benchmark 数据集上进行了实验，并证明了 GRO 的超越性，可以有效地防御模型提取攻击。<details>
<summary>Abstract</summary>
The robustness of recommender systems has become a prominent topic within the research community. Numerous adversarial attacks have been proposed, but most of them rely on extensive prior knowledge, such as all the white-box attacks or most of the black-box attacks which assume that certain external knowledge is available. Among these attacks, the model extraction attack stands out as a promising and practical method, involving training a surrogate model by repeatedly querying the target model. However, there is a significant gap in the existing literature when it comes to defending against model extraction attacks on recommender systems. In this paper, we introduce Gradient-based Ranking Optimization (GRO), which is the first defense strategy designed to counter such attacks. We formalize the defense as an optimization problem, aiming to minimize the loss of the protected target model while maximizing the loss of the attacker's surrogate model. Since top-k ranking lists are non-differentiable, we transform them into swap matrices which are instead differentiable. These swap matrices serve as input to a student model that emulates the surrogate model's behavior. By back-propagating the loss of the student model, we obtain gradients for the swap matrices. These gradients are used to compute a swap loss, which maximizes the loss of the student model. We conducted experiments on three benchmark datasets to evaluate the performance of GRO, and the results demonstrate its superior effectiveness in defending against model extraction attacks.
</details>
<details>
<summary>摘要</summary>
“推荐系统的强健性已经成为研究社区中的焦点话题。许多敌意攻击已经被提出，但大多数它们需要很多先前知识，如白盒子攻击或黑盒子攻击，这些攻击假设有一定的外部知识是可用的。在这些攻击中，模型提取攻击最引人注目，它们可以通过重复询问目标模型来训练代理模型。然而，在现有的文献中，防御模型提取攻击的方法尚未得到充分的研究。在这篇论文中，我们介绍了一个名为Gradient-based Ranking Optimization（GRO）的防御策略，这是第一个针对推荐系统中的模型提取攻击进行防御的策略。我们将防御当做一个优化问题，寻找可以最大化攻击者的代理模型的损失，同时最小化保护目标模型的损失。因为排名列表是不可微的，我们将它转换为可微的交换矩阵，这些交换矩阵作为学生模型的输入。通过将学生模型的损失传递到交换矩阵上，我们可以得到交换损失，这个交换损失可以最大化学生模型的损失。我们在三个标准 benchmark 数据集上进行实验评估 GRO 的性能，结果显示 GRO 能够有效地防御模型提取攻击。”
</details></li>
</ul>
<hr>
<h2 id="Corrupting-Neuron-Explanations-of-Deep-Visual-Features"><a href="#Corrupting-Neuron-Explanations-of-Deep-Visual-Features" class="headerlink" title="Corrupting Neuron Explanations of Deep Visual Features"></a>Corrupting Neuron Explanations of Deep Visual Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16332">http://arxiv.org/abs/2310.16332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Divyansh Srivastava, Tuomas Oikarinen, Tsui-Wei Weng</li>
<li>for: This paper aims to investigate the robustness of Neuron Explanation Methods (NEMs) in deep neural networks.</li>
<li>methods: The authors use a unified pipeline to evaluate the robustness of NEMs under different types of corruptions, including random noises and well-designed perturbations.</li>
<li>results: The authors find that even small amounts of noise can significantly corrupt the explanations provided by NEMs, and that their proposed corruption algorithm can manipulate the explanations of more than 80% of neurons by poisoning less than 10% of the probing data. This raises concerns about the trustworthiness of NEMs in real-life applications.<details>
<summary>Abstract</summary>
The inability of DNNs to explain their black-box behavior has led to a recent surge of explainability methods. However, there are growing concerns that these explainability methods are not robust and trustworthy. In this work, we perform the first robustness analysis of Neuron Explanation Methods under a unified pipeline and show that these explanations can be significantly corrupted by random noises and well-designed perturbations added to their probing data. We find that even adding small random noise with a standard deviation of 0.02 can already change the assigned concepts of up to 28% neurons in the deeper layers. Furthermore, we devise a novel corruption algorithm and show that our algorithm can manipulate the explanation of more than 80% neurons by poisoning less than 10% of probing data. This raises the concern of trusting Neuron Explanation Methods in real-life safety and fairness critical applications.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:深度神经网络（DNN）的不可见行为无法解释的问题，带来了一波解释方法的增加。然而，有增加的担忧，这些解释方法不是可靠和可信的。在这项工作中，我们对神经元解释方法进行了首次稳定性分析，发现这些解释可以由杂音和设计的干扰添加到其探测数据中而严重损害。我们发现，即使添加0.02标准差的随机噪音，也可以改变深层神经元的分配概念，达到28%以上。此外，我们设计了一种新的损害算法，并证明我们的算法可以通过污染探测数据来控制神经元解释的80%以上。这引发了在实际安全和公平应用中信任神经元解释方法的担忧。
</details></li>
</ul>
<hr>
<h2 id="Brain-Inspired-Reservoir-Computing-Using-Memristors-with-Tunable-Dynamics-and-Short-Term-Plasticity"><a href="#Brain-Inspired-Reservoir-Computing-Using-Memristors-with-Tunable-Dynamics-and-Short-Term-Plasticity" class="headerlink" title="Brain-Inspired Reservoir Computing Using Memristors with Tunable Dynamics and Short-Term Plasticity"></a>Brain-Inspired Reservoir Computing Using Memristors with Tunable Dynamics and Short-Term Plasticity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16331">http://arxiv.org/abs/2310.16331</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicholas X. Armendarez, Ahmed S. Mohamed, Anurag Dhungel, Md Razuan Hossain, Md Sakib Hasan, Joseph S. Najem</li>
<li>for: 本研究旨在提供一种用于时间类型分类和预测任务的analog设备，以提高信息处理速度，降低能耗和占用面积。</li>
<li>methods: 研究人员使用 ion-channel-based memristors，通过控制电压或 ion channel 浓度来实现多态动态特性。</li>
<li>results: 实验和 simulations 表明，使用一小 número de distinct memristors 可以获得高精度的预测和分类结果，比如在 second-order nonlinear dynamical system prediction 任务中，使用 five distinct memristors 实际 achievable normalized mean square error 为0.0015，在 neural activity classification 任务中，使用 three distinct memristors 实际 achievable accuracy 为96.5%。<details>
<summary>Abstract</summary>
Recent advancements in reservoir computing research have created a demand for analog devices with dynamics that can facilitate the physical implementation of reservoirs, promising faster information processing while consuming less energy and occupying a smaller area footprint. Studies have demonstrated that dynamic memristors, with nonlinear and short-term memory dynamics, are excellent candidates as information-processing devices or reservoirs for temporal classification and prediction tasks. Previous implementations relied on nominally identical memristors that applied the same nonlinear transformation to the input data, which is not enough to achieve a rich state space. To address this limitation, researchers either diversified the data encoding across multiple memristors or harnessed the stochastic device-to-device variability among the memristors. However, this approach requires additional pre-processing steps and leads to synchronization issues. Instead, it is preferable to encode the data once and pass it through a reservoir layer consisting of memristors with distinct dynamics. Here, we demonstrate that ion-channel-based memristors with voltage-dependent dynamics can be controllably and predictively tuned through voltage or adjustment of the ion channel concentration to exhibit diverse dynamic properties. We show, through experiments and simulations, that reservoir layers constructed with a small number of distinct memristors exhibit significantly higher predictive and classification accuracies with a single data encoding. We found that for a second-order nonlinear dynamical system prediction task, the varied memristor reservoir experimentally achieved a normalized mean square error of 0.0015 using only five distinct memristors. Moreover, in a neural activity classification task, a reservoir of just three distinct memristors experimentally attained an accuracy of 96.5%.
</details>
<details>
<summary>摘要</summary>
To overcome this limitation, researchers have either diversified the data encoding across multiple memristors or harnessed the stochastic device-to-device variability among memristors. However, these approaches require additional pre-processing steps and can lead to synchronization issues.In this study, we demonstrate that ion-channel-based memristors with voltage-dependent dynamics can be controllably and predictively tuned through voltage or adjustment of the ion channel concentration to exhibit diverse dynamic properties. By constructing reservoir layers with a small number of distinct memristors, we found that the predictive and classification accuracies can be significantly improved with a single data encoding.In our experiments and simulations, we showed that for a second-order nonlinear dynamical system prediction task, a reservoir layer constructed with only five distinct memristors achieved a normalized mean square error of 0.0015. Moreover, in a neural activity classification task, a reservoir of just three distinct memristors experimentally attained an accuracy of 96.5%. Our findings demonstrate the potential of using ion-channel-based memristors with diverse dynamics to improve the performance of reservoir computing.
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Learning-for-SBM-Graphon-Games-with-Re-Sampling"><a href="#Reinforcement-Learning-for-SBM-Graphon-Games-with-Re-Sampling" class="headerlink" title="Reinforcement Learning for SBM Graphon Games with Re-Sampling"></a>Reinforcement Learning for SBM Graphon Games with Re-Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16326">http://arxiv.org/abs/2310.16326</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peihan Huo, Oscar Peralta, Junyu Guo, Qiaomin Xie, Andreea Minca</li>
<li>for: 本研究旨在探讨大规模群体动力学中的 Mean-Field 方法的局限性，并提出了 Multi-Population Mean-Field Game（MP-MFG）模型来解决这些限制。</li>
<li>methods: 作者们提出了一种基于 Policy Mirror Ascent 算法的 MP-MFG Nash 平衡找索法，并在更真实的情况下，即无知 Stochastic Block Model 时，提出了一种基于图顺游戏的 Graphon Game with Re-Sampling（GGR-S）模型。</li>
<li>results: 作者们分析了 GGR-S 模型的动态，并证明了它们的动态相对于 MP-MFG 动态的收敛性。此外，他们还提出了一种基于 GGR-S 模型的高效的 N-player 启动学习算法，并提供了finite sample 保证的收敛分析。<details>
<summary>Abstract</summary>
The Mean-Field approximation is a tractable approach for studying large population dynamics. However, its assumption on homogeneity and universal connections among all agents limits its applicability in many real-world scenarios. Multi-Population Mean-Field Game (MP-MFG) models have been introduced in the literature to address these limitations. When the underlying Stochastic Block Model is known, we show that a Policy Mirror Ascent algorithm finds the MP-MFG Nash Equilibrium. In more realistic scenarios where the block model is unknown, we propose a re-sampling scheme from a graphon integrated with the finite N-player MP-MFG model. We develop a novel learning framework based on a Graphon Game with Re-Sampling (GGR-S) model, which captures the complex network structures of agents' connections. We analyze GGR-S dynamics and establish the convergence to dynamics of MP-MFG. Leveraging this result, we propose an efficient sample-based N-player Reinforcement Learning algorithm for GGR-S without population manipulation, and provide a rigorous convergence analysis with finite sample guarantee.
</details>
<details>
<summary>摘要</summary>
“mean-fieldapproximation”是一种可行的方法来研究大规模动态系统。然而，它的假设homogeneity和所有代理者之间的通用连接限制了其在实际场景中的应用。“multi-population mean-field game”（MP-MFG）模型已经在文献中提出，以解决这些限制。当下面的随机块模型是known的时候，我们表明了一种策略镜像上升算法可以找到MP-MFG的 Nash Equilibrium。在更实际的场景中，当随机块模型是 unknown的时候，我们提议一种从graphon集成到finite N-player MP-MFG模型的重新抽样方案。我们开发了一种基于graphon game with re-sampling（GGR-S）模型的学习框架，该模型捕捉了代理者之间的复杂网络结构。我们分析了GGR-S动力学和establish了MP-MFG动力学的整合。基于这个结果，我们提出了一种高效的sample-based N-player reinforcement learning算法，并提供了finite sample guarantee的准确性分析。
</details></li>
</ul>
<hr>
<h2 id="Personalized-Federated-X-armed-Bandit"><a href="#Personalized-Federated-X-armed-Bandit" class="headerlink" title="Personalized Federated X -armed Bandit"></a>Personalized Federated X -armed Bandit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16323">http://arxiv.org/abs/2310.16323</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjie Li, Qifan Song, Jean Honorio</li>
<li>for: 本研究探讨了个性化联合 $\mathcal{X}$ 武器问题，即在联合学习框架中同时优化客户端的不同化本地目标。</li>
<li>methods: 我们提出了 \texttt{PF-PNE} 算法，具有独特的双排除策略，可以安全排除非优区间，同时鼓励联合合作通过偏袋而且有效的评估本地目标。</li>
<li>results: 我们的理论分析表明，提出的 \texttt{PF-PNE} 算法可以优化本地目标的任何水平异ogeneity，并且限制通信保护客户端奖励数据的隐私。实验表明，\texttt{PF-PNE} 超越多基elines在both synthetic和实际数据上。<details>
<summary>Abstract</summary>
In this work, we study the personalized federated $\mathcal{X}$-armed bandit problem, where the heterogeneous local objectives of the clients are optimized simultaneously in the federated learning paradigm. We propose the \texttt{PF-PNE} algorithm with a unique double elimination strategy, which safely eliminates the non-optimal regions while encouraging federated collaboration through biased but effective evaluations of the local objectives. The proposed \texttt{PF-PNE} algorithm is able to optimize local objectives with arbitrary levels of heterogeneity, and its limited communications protects the confidentiality of the client-wise reward data. Our theoretical analysis shows the benefit of the proposed algorithm over single-client algorithms. Experimentally, \texttt{PF-PNE} outperforms multiple baselines on both synthetic and real life datasets.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们研究了个性化联合 $\mathcal{X}$-臂投资问题，其中客户端的多样化本地目标同时在联合学习框架下优化。我们提议了\texttt{PF-PNE}算法，该算法使用独特的双淘汰策略，安全地排除非优化区域，同时通过偏袋但有效的本地目标评估来促进联合协作。提议的\texttt{PF-PNE}算法可以优化客户端目标的任何水平多样性，并且限制通信保护客户端奖励数据的隐私。我们的理论分析表明提议算法在单个客户端算法方面具有优势。实验表明\texttt{PF-PNE}超过多个基elines在 sintetic 和实际数据集上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Low-Precision-Sampling-via-Stochastic-Gradient-Hamiltonian-Monte-Carlo"><a href="#Enhancing-Low-Precision-Sampling-via-Stochastic-Gradient-Hamiltonian-Monte-Carlo" class="headerlink" title="Enhancing Low-Precision Sampling via Stochastic Gradient Hamiltonian Monte Carlo"></a>Enhancing Low-Precision Sampling via Stochastic Gradient Hamiltonian Monte Carlo</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16320">http://arxiv.org/abs/2310.16320</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyi Wang, Yujie Chen, Qifan Song, Ruqi Zhang</li>
<li>for: 这个论文研究了一种叫做低精度训练的技术，用于提高深度神经网络的训练效率，而不是减少准确性。</li>
<li>methods: 该论文使用了一种名叫Stochastic Gradient Hamiltonian Monte Carlo（SGHMC）的低精度抽样方法，并使用了低精度和全精度的梯度积累器。</li>
<li>results: 研究发现，使用SGHMC抽样方法可以在非几何分布上实现$\epsilon$-错误的2-沃asserstein距离，并且与现有的低精度抽样方法相比，具有 quadratic 改善（$\widetilde{\mathbf{O}\left({\epsilon^{-2}{\mu^*}^{-2}\log^2\left({\epsilon^{-1}\right)}\right)$）。此外，研究还证明了SGHMC在梯度噪声中更加稳定和Robust。<details>
<summary>Abstract</summary>
Low-precision training has emerged as a promising low-cost technique to enhance the training efficiency of deep neural networks without sacrificing much accuracy. Its Bayesian counterpart can further provide uncertainty quantification and improved generalization accuracy. This paper investigates low-precision sampling via Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) with low-precision and full-precision gradient accumulators for both strongly log-concave and non-log-concave distributions. Theoretically, our results show that, to achieve $\epsilon$-error in the 2-Wasserstein distance for non-log-concave distributions, low-precision SGHMC achieves quadratic improvement ($\widetilde{\mathbf{O}\left({\epsilon^{-2}{\mu^*}^{-2}\log^2\left({\epsilon^{-1}\right)}\right)$) compared to the state-of-the-art low-precision sampler, Stochastic Gradient Langevin Dynamics (SGLD) ($\widetilde{\mathbf{O}\left({\epsilon}^{-4}{\lambda^{*}^{-1}\log^5\left({\epsilon^{-1}\right)}\right)$). Moreover, we prove that low-precision SGHMC is more robust to the quantization error compared to low-precision SGLD due to the robustness of the momentum-based update w.r.t. gradient noise. Empirically, we conduct experiments on synthetic data, and {MNIST, CIFAR-10 \& CIFAR-100} datasets, which validate our theoretical findings. Our study highlights the potential of low-precision SGHMC as an efficient and accurate sampling method for large-scale and resource-limited machine learning.
</details>
<details>
<summary>摘要</summary>
低精度训练技术已经出现为深度神经网络训练效率的低成本技术，无需做出很大牺牲。其 Bayesian 对应技术可以提供uncertainty量化和改善泛化精度。这篇论文研究了低精度抽样，使用随机Gradient Hamiltonian Monte Carlo（SGHMC）实现低精度和全精度梯度积累器。我们的研究结果表明，在非几何分布上，低精度 SGHMC 可以在 $\epsilon $ 误差下实现 2-Wasserstein 距离的 $\epsilon $-误差，而且与现状态的低精度抽样器 Stochastic Gradient Langevin Dynamics（SGLD）相比，具有quadratic 提高（$\widetilde{\mathbf{O}\left({\epsilon^{-2}{\mu^*}^{-2}\log^2\left({\epsilon^{-1}\right)}\right)$）。此外，我们证明了低精度 SGHMC 对于梯度误差的Robustness比低精度 SGLD 更强，这是因为涉及梯度噪声的摇摆更新的稳定性。我们在 sintetic 数据和 {MNIST, CIFAR-10 \& CIFAR-100}  datasets上进行了实验， validate 我们的理论发现。我们的研究强调了低精度 SGHMC 作为大规模和有限资源的机器学习中的有效和准确抽样方法。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Code-Semantics-An-Evaluation-of-Transformer-Models-in-Summarization"><a href="#Understanding-Code-Semantics-An-Evaluation-of-Transformer-Models-in-Summarization" class="headerlink" title="Understanding Code Semantics: An Evaluation of Transformer Models in Summarization"></a>Understanding Code Semantics: An Evaluation of Transformer Models in Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16314">http://arxiv.org/abs/2310.16314</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Demon702/robust_code_summary">https://github.com/Demon702/robust_code_summary</a></li>
<li>paper_authors: Debanjan Mondal, Abhilasha Lodha, Ankita Sahoo, Beena Kumari</li>
<li>for: 这篇论文探讨了高级变换器基于语言模型如何实现代码概要。</li>
<li>methods: 我们通过对函数和变量名进行修改来评估模型是否真正理解代码 semantics 还是仅仅依靠文本提示。我们还在三种编程语言（Python、Javascript、Java）中引入了干扰者如死代码和注释代码，进一步检验模型的理解能力。</li>
<li>results: 我们的研究希望通过探讨变换器基于LM的内部工作 Mechanism，提高模型对代码的理解能力，并促进软件开发和维护过程的效率。<details>
<summary>Abstract</summary>
This paper delves into the intricacies of code summarization using advanced transformer-based language models. Through empirical studies, we evaluate the efficacy of code summarization by altering function and variable names to explore whether models truly understand code semantics or merely rely on textual cues. We have also introduced adversaries like dead code and commented code across three programming languages (Python, Javascript, and Java) to further scrutinize the model's understanding. Ultimately, our research aims to offer valuable insights into the inner workings of transformer-based LMs, enhancing their ability to understand code and contributing to more efficient software development practices and maintenance workflows.
</details>
<details>
<summary>摘要</summary>
Here is the translation in Simplified Chinese:这篇论文探讨了使用高级变换器基本语言模型进行代码概要的细节。通过实验研究，我们评估了代码概要的有效性，通过修改函数和变量名来检验模型是否真正理解代码 semantics 还是仅仅依据文本提示。我们还在三种编程语言（Python、Javascript、Java）中引入了死代码和注释代码作为敌手，进一步检验模型的理解。我们的研究目标是提供有价值的内在性研究，以提高变换器基本语言模型对代码的理解，并为更有效的软件开发实践和维护工作流程做出贡献。
</details></li>
</ul>
<hr>
<h2 id="Score-Matching-based-Pseudolikelihood-Estimation-of-Neural-Marked-Spatio-Temporal-Point-Process-with-Uncertainty-Quantification"><a href="#Score-Matching-based-Pseudolikelihood-Estimation-of-Neural-Marked-Spatio-Temporal-Point-Process-with-Uncertainty-Quantification" class="headerlink" title="Score Matching-based Pseudolikelihood Estimation of Neural Marked Spatio-Temporal Point Process with Uncertainty Quantification"></a>Score Matching-based Pseudolikelihood Estimation of Neural Marked Spatio-Temporal Point Process with Uncertainty Quantification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16310">http://arxiv.org/abs/2310.16310</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zichong Li, Qunzhi Xu, Zhenghao Xu, Yajun Mei, Tuo Zhao, Hongyuan Zha</li>
<li>for: 本研究旨在提出一种可靠地学习带有时空特征的事件发生过程的数学工具，以及为这些过程提供不确定性评估。</li>
<li>methods: 本研究使用的方法包括分布式预测和精度评估，以及一种基于分数匹配的pseudolikelihood函数来估计标记的STPPs。</li>
<li>results: 研究表明，对于不同的事件类型和数据量，SMASH方法可以提供更高的准确率和更低的不确定性，并且可以提供事件发生时间和位置的信心区间和标记的信心范围。<details>
<summary>Abstract</summary>
Spatio-temporal point processes (STPPs) are potent mathematical tools for modeling and predicting events with both temporal and spatial features. Despite their versatility, most existing methods for learning STPPs either assume a restricted form of the spatio-temporal distribution, or suffer from inaccurate approximations of the intractable integral in the likelihood training objective. These issues typically arise from the normalization term of the probability density function. Moreover, current techniques fail to provide uncertainty quantification for model predictions, such as confidence intervals for the predicted event's arrival time and confidence regions for the event's location, which is crucial given the considerable randomness of the data. To tackle these challenges, we introduce SMASH: a Score MAtching-based pSeudolikeliHood estimator for learning marked STPPs with uncertainty quantification. Specifically, our framework adopts a normalization-free objective by estimating the pseudolikelihood of marked STPPs through score-matching and offers uncertainty quantification for the predicted event time, location and mark by computing confidence regions over the generated samples. The superior performance of our proposed framework is demonstrated through extensive experiments in both event prediction and uncertainty quantification.
</details>
<details>
<summary>摘要</summary>
�� Stefanos �쳌�测点过程 (STPPs) 是一种强大的数学工具，用于模拟和预测具有时间和空间特征的事件。 despite their versatility, most existing methods for learning STPPs either assume a restricted form of the spatio-temporal distribution, or suffer from inaccurate approximations of the intractable integral in the likelihood training objective. These issues typically arise from the normalization term of the probability density function. Moreover, current techniques fail to provide uncertainty quantification for model predictions, such as confidence intervals for the predicted event's arrival time and confidence regions for the event's location, which is crucial given the considerable randomness of the data.To tackle these challenges, we introduce SMASH: a Score MAtching-based pSeudolikeliHood estimator for learning marked STPPs with uncertainty quantification. Specifically, our framework adopts a normalization-free objective by estimating the pseudolikelihood of marked STPPs through score-matching and offers uncertainty quantification for the predicted event time, location and mark by computing confidence regions over the generated samples. The superior performance of our proposed framework is demonstrated through extensive experiments in both event prediction and uncertainty quantification.Here's the text in Traditional Chinese:�� Stefanos �쳌�测点过程 (STPPs) 是一种强大的数学工具，用于模拟和预测具有时间和空间特征的事件。 despite their versatility, most existing methods for learning STPPs either assume a restricted form of the spatio-temporal distribution, or suffer from inaccurate approximations of the intractable integral in the likelihood training objective. These issues typically arise from the normalization term of the probability density function. Moreover, current techniques fail to provide uncertainty quantification for model predictions, such as confidence intervals for the predicted event's arrival time and confidence regions for the event's location, which is crucial given the considerable randomness of the data.To tackle these challenges, we introduce SMASH: a Score MAtching-based pSeudolikeliHood estimator for learning marked STPPs with uncertainty quantification. Specifically, our framework adopts a normalization-free objective by estimating the pseudolikelihood of marked STPPs through score-matching and offers uncertainty quantification for the predicted event time, location and mark by computing confidence regions over the generated samples. The superior performance of our proposed framework is demonstrated through extensive experiments in both event prediction and uncertainty quantification.
</details></li>
</ul>
<hr>
<h2 id="Imperfect-Digital-Twin-Assisted-Low-Cost-Reinforcement-Training-for-Multi-UAV-Networks"><a href="#Imperfect-Digital-Twin-Assisted-Low-Cost-Reinforcement-Training-for-Multi-UAV-Networks" class="headerlink" title="Imperfect Digital Twin Assisted Low Cost Reinforcement Training for Multi-UAV Networks"></a>Imperfect Digital Twin Assisted Low Cost Reinforcement Training for Multi-UAV Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16302">http://arxiv.org/abs/2310.16302</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiucheng Wang, Nan Cheng, Longfei Ma, Zhisheng Yin, Tom. Luan, Ning Lu</li>
<li>for: 这个研究目的是优化多架空航空器网络的性能，并使用深度强化学习（DRL）来实现。</li>
<li>methods: 这个研究使用了随机生成的UAV混合部署方法，并使用了两个降阶神经网（NN）来优化UAV的混合部署，DT的建构成本，以及多架空航空器网络的性能。这两个NN使用了无监督学习和强化学习，两种低成本的无标签训练方法。</li>
<li>results:  simulation results表明，这个方法可以对多架空航空器网络的训练成本进行重要优化，同时保证训练性能。这表明，使用不完美的DT模型可以实现高效的决策。<details>
<summary>Abstract</summary>
Deep Reinforcement Learning (DRL) is widely used to optimize the performance of multi-UAV networks. However, the training of DRL relies on the frequent interactions between the UAVs and the environment, which consumes lots of energy due to the flying and communication of UAVs in practical experiments. Inspired by the growing digital twin (DT) technology, which can simulate the performance of algorithms in the digital space constructed by coping features of the physical space, the DT is introduced to reduce the costs of practical training, e.g., energy and hardware purchases. Different from previous DT-assisted works with an assumption of perfect reflecting real physics by virtual digital, we consider an imperfect DT model with deviations for assisting the training of multi-UAV networks. Remarkably, to trade off the training cost, DT construction cost, and the impact of deviations of DT on training, the natural and virtually generated UAV mixing deployment method is proposed. Two cascade neural networks (NN) are used to optimize the joint number of virtually generated UAVs, the DT construction cost, and the performance of multi-UAV networks. These two NNs are trained by unsupervised and reinforcement learning, both low-cost label-free training methods. Simulation results show the training cost can significantly decrease while guaranteeing the training performance. This implies that an efficient decision can be made with imperfect DTs in multi-UAV networks.
</details>
<details>
<summary>摘要</summary>
为了衡量DT构建成本、训练成本和DT的偏差影响训练，我们提出了自然和虚拟生成的 UAV 混合部署方法。在这种方法中，我们使用了两个降准神经网络（NN）来优化虚拟生成 UAV 的数量、DT 构建成本和多架空航空器网络的性能。这两个 NN 通过无监督学习和反射学习训练，这些训练方法都是低成本的标签自由训练方法。在 simulations 中，我们发现可以大幅降低训练成本，同时保证训练性能。这表示，在多架空航空器网络中，可以有效地使用不完美的 DT 进行决策。
</details></li>
</ul>
<hr>
<h2 id="FuXi-Extreme-Improving-extreme-rainfall-and-wind-forecasts-with-diffusion-model"><a href="#FuXi-Extreme-Improving-extreme-rainfall-and-wind-forecasts-with-diffusion-model" class="headerlink" title="FuXi-Extreme: Improving extreme rainfall and wind forecasts with diffusion model"></a>FuXi-Extreme: Improving extreme rainfall and wind forecasts with diffusion model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19822">http://arxiv.org/abs/2310.19822</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaohui Zhong, Lei Chen, Jun Liu, Chensen Lin, Yuan Qi, Hao Li<br>for:这个论文主要是为了提高天气预测模型的精度和准确性。methods:这个论文使用了一种名为denoising diffusion probabilistic model（DDPM），用于Restore表面预测数据中的细致细节，从而提高预测的准确性。results:论文表明，使用 FuXi-Extreme 模型可以在 5 天预测中提高预测的精度和准确性，特别是在极端天气事件方面。此外，这个模型还在 tropical cyclone 预测中表现出优于 HRES 模型。<details>
<summary>Abstract</summary>
Significant advancements in the development of machine learning (ML) models for weather forecasting have produced remarkable results. State-of-the-art ML-based weather forecast models, such as FuXi, have demonstrated superior statistical forecast performance in comparison to the high-resolution forecasts (HRES) of the European Centre for Medium-Range Weather Forecasts (ECMWF). However, ML models face a common challenge: as forecast lead times increase, they tend to generate increasingly smooth predictions, leading to an underestimation of the intensity of extreme weather events. To address this challenge, we developed the FuXi-Extreme model, which employs a denoising diffusion probabilistic model (DDPM) to restore finer-scale details in the surface forecast data generated by the FuXi model in 5-day forecasts. An evaluation of extreme total precipitation ($\textrm{TP}$), 10-meter wind speed ($\textrm{WS10}$), and 2-meter temperature ($\textrm{T2M}$) illustrates the superior performance of FuXi-Extreme over both FuXi and HRES. Moreover, when evaluating tropical cyclone (TC) forecasts based on International Best Track Archive for Climate Stewardship (IBTrACS) dataset, both FuXi and FuXi-Extreme shows superior performance in TC track forecasts compared to HRES, but they show inferior performance in TC intensity forecasts in comparison to HRES.
</details>
<details>
<summary>摘要</summary>
significannot advancements in the development of machine learning (ML) models for weather forecasting have produced remarkable results. State-of-the-art ML-based weather forecast models, such as FuXi, have demonstrated superior statistical forecast performance in comparison to the high-resolution forecasts (HRES) of the European Centre for Medium-Range Weather Forecasts (ECMWF). However, ML models face a common challenge: as forecast lead times increase, they tend to generate increasingly smooth predictions, leading to an underestimation of the intensity of extreme weather events. To address this challenge, we developed the FuXi-Extreme model, which employs a denoising diffusion probabilistic model (DDPM) to restore finer-scale details in the surface forecast data generated by the FuXi model in 5-day forecasts. An evaluation of extreme total precipitation ($\textrm{TP}$), 10-meter wind speed ($\textrm{WS10}$), and 2-meter temperature ($\textrm{T2M}$) illustrates the superior performance of FuXi-Extreme over both FuXi and HRES. Moreover, when evaluating tropical cyclone (TC) forecasts based on International Best Track Archive for Climate Stewardship (IBTrACS) dataset, both FuXi and FuXi-Extreme shows superior performance in TC track forecasts compared to HRES, but they show inferior performance in TC intensity forecasts in comparison to HRES.Here's the translation in Traditional Chinese:这些进步在机器学习（ML）模型的气象预报中得到了杰出的结果。例如FuXi这个ML-based weather forecast model，在欧洲中期气象预报中心（ECMWF）的高分辨率预报（HRES）之上， demonstrates superior statistical forecast performance。然而，ML模型面临一个普遍的挑战：当预报时间增加时，它们倾向于生成越来越平滑的预报，导致极端天气事件的Underestimation。为了解决这个问题，我们开发了FuXi-Extreme模型，该模型使用减钠扩散概率模型（DDPM）来重新塑造在FuXi模型的5天预报中的表面数据，以 Restore finer-scale details。一个evaluation of extreme total precipitation（TP）、10-meter wind speed（WS10）和2-meter temperature（T2M）表明FuXi-Extreme模型在FuXi和HRES之上表现出色。此外，基于International Best Track Archive for Climate Stewardship（IBTrACS）数据集的评估显示，FuXi和FuXi-Extreme在热带气旋（TC）预报中表现出Superior performance compared to HRES，但在TC intensity forecasts中表现较差 compared to HRES。
</details></li>
</ul>
<hr>
<h2 id="Crowd-Certain-Label-Aggregation-in-Crowdsourced-and-Ensemble-Learning-Classification"><a href="#Crowd-Certain-Label-Aggregation-in-Crowdsourced-and-Ensemble-Learning-Classification" class="headerlink" title="Crowd-Certain: Label Aggregation in Crowdsourced and Ensemble Learning Classification"></a>Crowd-Certain: Label Aggregation in Crowdsourced and Ensemble Learning Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16293">http://arxiv.org/abs/2310.16293</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad S. Majdi, Jeffrey J. Rodriguez</li>
<li>for: 这 paper 是为了提出一种基于大量标注数据的人员协同学习和集成学习分类任务的标注聚合方法，以提高性能和计算效率。</li>
<li>methods: 该方法使用了对 annotators 的一致性 versus 训练的分类器来确定每个 annotator 的可靠度分数。此外，Crowd-Certain 还利用预测的概率，使得可以 reuse 训练好的分类器，从而消除现有方法中的循环 simulation 过程。</li>
<li>results: 对于十种现有方法，Crowd-Certain 在十个不同的数据集上进行了广泛的评估，并取得了较高的平均准确率、F1 分数和 AUC 率。此外，本 paper 还提出了两种现有信任分数测试技术的变体，并使用了两个评估指标：预期均衡错误 (ECE) 和 Brier 分数损失。结果显示，Crowd-Certain 在大多数评估数据集上取得了更高的 Brier 分数，并且下降的 ECE。<details>
<summary>Abstract</summary>
Crowdsourcing systems have been used to accumulate massive amounts of labeled data for applications such as computer vision and natural language processing. However, because crowdsourced labeling is inherently dynamic and uncertain, developing a technique that can work in most situations is extremely challenging. In this paper, we introduce Crowd-Certain, a novel approach for label aggregation in crowdsourced and ensemble learning classification tasks that offers improved performance and computational efficiency for different numbers of annotators and a variety of datasets. The proposed method uses the consistency of the annotators versus a trained classifier to determine a reliability score for each annotator. Furthermore, Crowd-Certain leverages predicted probabilities, enabling the reuse of trained classifiers on future sample data, thereby eliminating the need for recurrent simulation processes inherent in existing methods. We extensively evaluated our approach against ten existing techniques across ten different datasets, each labeled by varying numbers of annotators. The findings demonstrate that Crowd-Certain outperforms the existing methods (Tao, Sheng, KOS, MACE, MajorityVote, MMSR, Wawa, Zero-Based Skill, GLAD, and Dawid Skene), in nearly all scenarios, delivering higher average accuracy, F1 scores, and AUC rates. Additionally, we introduce a variation of two existing confidence score measurement techniques. Finally we evaluate these two confidence score techniques using two evaluation metrics: Expected Calibration Error (ECE) and Brier Score Loss. Our results show that Crowd-Certain achieves higher Brier Score, and lower ECE across the majority of the examined datasets, suggesting better calibrated results.
</details>
<details>
<summary>摘要</summary>
众生系统已经用于收集大量标注数据，用于计算机视觉和自然语言处理等应用。然而，由于众生标注是自然动态的，因此开发一种能够在多种情况下工作的技术非常困难。在本文中，我们介绍了一种新的标签聚合方法——Crowd-Certain，用于众生和ensemble学习分类任务。该方法可以提高性能和计算效率，并且可以适用于不同的annotator数量和数据集。我们使用了annotators的一致性 versus 训练的分类器来确定每位annotator的可靠性分数。此外，Crowd-Certain还利用预测概率，从而消除了现有方法中的循环 simulation 过程。我们对十种现有方法进行了十种不同的数据集和不同的annotator数量的比较。结果表明，Crowd-Certain在大多数场景下超越现有方法，提供更高的平均准确率、F1分数和ROC曲线。此外，我们还提出了两种现有信息分数测量技术的变体。最后，我们使用了两种评估 metric：预期抽样错误（ECE）和布莱尔分数损失来评估这两种信息分数测量技术。我们的结果表明，Crowd-Certain在大多数检查的数据集上获得了更高的布莱尔分数，并且在大多数数据集上获得了更低的ECE，这表明它提供了更好的准确性。
</details></li>
</ul>
<hr>
<h2 id="Removing-Dust-from-CMB-Observations-with-Diffusion-Models"><a href="#Removing-Dust-from-CMB-Observations-with-Diffusion-Models" class="headerlink" title="Removing Dust from CMB Observations with Diffusion Models"></a>Removing Dust from CMB Observations with Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16285">http://arxiv.org/abs/2310.16285</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Heurtel-Depeiges, Blakesley Burkhart, Ruben Ohana, Bruno Régaldo-Saint Blancard</li>
<li>for: 研究了吸引型尘埃前景模型的应用和其在分解成分方面的利益。</li>
<li>methods: 使用吸引型模型来模拟尘埃前景，并通过训练这些模型来实现分解成分。</li>
<li>results: 通过使用吸引型模型进行分解，可以良好地回归尘埃前景中的通用统计量（如功率spectrum和明ков斯函数）。此外，通过conditioning模型使用cosmology来提高模型的性能。<details>
<summary>Abstract</summary>
In cosmology, the quest for primordial $B$-modes in cosmic microwave background (CMB) observations has highlighted the critical need for a refined model of the Galactic dust foreground. We investigate diffusion-based modeling of the dust foreground and its interest for component separation. Under the assumption of a Gaussian CMB with known cosmology (or covariance matrix), we show that diffusion models can be trained on examples of dust emission maps such that their sampling process directly coincides with posterior sampling in the context of component separation. We illustrate this on simulated mixtures of dust emission and CMB. We show that common summary statistics (power spectrum, Minkowski functionals) of the components are well recovered by this process. We also introduce a model conditioned by the CMB cosmology that outperforms models trained using a single cosmology on component separation. Such a model will be used in future work for diffusion-based cosmological inference.
</details>
<details>
<summary>摘要</summary>
在 cosmology 中，寻找宇宙微波背景 (CMB) 中的原始 $B$-模式已经强调了对 галактической尘埃前景的精细模型的需求。我们研究了基于扩散的尘埃前景模型，并对其在分组分离中的利用性进行了调查。假设宇宙微波背景是 Gaussian 的，并且知道 cosmology 或者 covariance matrix，我们表明了扩散模型可以通过对尘埃辐射图像的示例进行训练，使其直接匹配 posterior 抽象在分组分离中的样本处理过程。我们在模拟中使用了尘埃辐射和 CMB 的混合图像，并证明了这种过程可以良好地重现组件的共同摘要统计（power spectrum、Minkowski 函数）。此外，我们还介绍了基于 CMB  cosmology 的模型，该模型在分组分离中超越了基于单个 cosmology 的模型。这种模型将在未来的工作中用于扩散基于 cosmological inference。
</details></li>
</ul>
<hr>
<h2 id="Improvement-in-Alzheimer’s-Disease-MRI-Images-Analysis-by-Convolutional-Neural-Networks-Via-Topological-Optimization"><a href="#Improvement-in-Alzheimer’s-Disease-MRI-Images-Analysis-by-Convolutional-Neural-Networks-Via-Topological-Optimization" class="headerlink" title="Improvement in Alzheimer’s Disease MRI Images Analysis by Convolutional Neural Networks Via Topological Optimization"></a>Improvement in Alzheimer’s Disease MRI Images Analysis by Convolutional Neural Networks Via Topological Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16857">http://arxiv.org/abs/2310.16857</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peiwen Tan</li>
<li>for: 提高阿尔茨染色肿瘤病 diagnosis 精度</li>
<li>methods: 利用散射函数 topological optimization 进行 MRI 图像进一步加工</li>
<li>results: 使用 CNN 模型 VGG16、ResNet50、InceptionV3 和 Xception 后处理，对 MRI 图像进行改进，提高了图像的清晰度和准确率<details>
<summary>Abstract</summary>
This research underscores the efficacy of Fourier topological optimization in refining MRI imagery, thereby bolstering the classification precision of Alzheimer's Disease through convolutional neural networks. Recognizing that MRI scans are indispensable for neurological assessments, but frequently grapple with issues like blurriness and contrast irregularities, the deployment of Fourier topological optimization offered enhanced delineation of brain structures, ameliorated noise, and superior contrast. The applied techniques prioritized boundary enhancement, contrast and brightness adjustments, and overall image lucidity. Employing CNN architectures VGG16, ResNet50, InceptionV3, and Xception, the post-optimization analysis revealed a marked elevation in performance. Conclusively, the amalgamation of Fourier topological optimization with CNNs delineates a promising trajectory for the nuanced classification of Alzheimer's Disease, portending a transformative impact on its diagnostic paradigms.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Near-Optimal-Pure-Exploration-in-Matrix-Games-A-Generalization-of-Stochastic-Bandits-Dueling-Bandits"><a href="#Near-Optimal-Pure-Exploration-in-Matrix-Games-A-Generalization-of-Stochastic-Bandits-Dueling-Bandits" class="headerlink" title="Near-Optimal Pure Exploration in Matrix Games: A Generalization of Stochastic Bandits &amp; Dueling Bandits"></a>Near-Optimal Pure Exploration in Matrix Games: A Generalization of Stochastic Bandits &amp; Dueling Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16252">http://arxiv.org/abs/2310.16252</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aistats2024-noisy-psne/midsearch">https://github.com/aistats2024-noisy-psne/midsearch</a></li>
<li>paper_authors: Arnab Maiti, Ross Boczar, Kevin Jamieson, Lillian J. Ratliff</li>
<li>for: 本研究是为了研究在两个玩家的零和游戏中确定纯策略尼希尔平衡（PSNE）的样本复杂性。</li>
<li>methods: 本文使用了一种随机模型，其中任何学习者可以随机选择输入矩阵$A\in[-1,1]^{n\times m}$中的一个元素$(i,j)$，并观察$A_{i,j}+\eta$，其中$\eta$是一个零均值的1子-${\mathcal{N}(0,1)$噪音。学习者的目标是通过取得足够多样本来确定$A$中的PSNE，如果存在的话，并尽可能快。</li>
<li>results: 本文提出了一个基于实例的下界，该下界只取决于输入矩阵$A$中的行和列中的entry。此外，本文还提出了一个近似算法，其样本复杂性与下界匹配，即可以达到下界的logs级别。此外，本文还证明了这个问题与游戏策略的纯exploration问题和对抗策略问题之间的普通性，并且其结果与这些问题的优化下界匹配，即可以达到logs级别。<details>
<summary>Abstract</summary>
We study the sample complexity of identifying the pure strategy Nash equilibrium (PSNE) in a two-player zero-sum matrix game with noise. Formally, we are given a stochastic model where any learner can sample an entry $(i,j)$ of the input matrix $A\in[-1,1]^{n\times m}$ and observe $A_{i,j}+\eta$ where $\eta$ is a zero-mean 1-sub-Gaussian noise. The aim of the learner is to identify the PSNE of $A$, whenever it exists, with high probability while taking as few samples as possible. Zhou et al. (2017) presents an instance-dependent sample complexity lower bound that depends only on the entries in the row and column in which the PSNE lies. We design a near-optimal algorithm whose sample complexity matches the lower bound, up to log factors. The problem of identifying the PSNE also generalizes the problem of pure exploration in stochastic multi-armed bandits and dueling bandits, and our result matches the optimal bounds, up to log factors, in both the settings.
</details>
<details>
<summary>摘要</summary>
我们研究样本复杂性标识纯策略尼仑平衡点（PSNE）在两个玩家零和游戏中的样本复杂性。正式地说，我们给出了一个随机模型，任何学习者可以随机选择输入矩阵$A\in[-1,1]^{n\times m}$的一个Entry $(i,j)$，并观察$A_{i,j}+\eta$，其中$\eta$是一个0mean的1子正态噪声。学习者的目标是通过取得高概率地标识PSNE的$A$，使用最少的样本数。周等（2017）提供了一个实例依赖的下界，该下界只取决于PSNE所在的行和列。我们设计了一个近似优化算法，其样本复杂性与下界匹配，即使log因子。PSNE的问题也涵盖了游戏纯exploration问题和对抗游戏问题，我们的结果与这两个设置的优化下界匹配，即使log因子。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/25/cs.LG_2023_10_25/" data-id="clorjzl9y00rvf188020g9hfx" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_10_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/25/eess.IV_2023_10_25/" class="article-date">
  <time datetime="2023-10-25T09:00:00.000Z" itemprop="datePublished">2023-10-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/25/eess.IV_2023_10_25/">eess.IV - 2023-10-25</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Using-Diffusion-Models-to-Generate-Synthetic-Labelled-Data-for-Medical-Image-Segmentation"><a href="#Using-Diffusion-Models-to-Generate-Synthetic-Labelled-Data-for-Medical-Image-Segmentation" class="headerlink" title="Using Diffusion Models to Generate Synthetic Labelled Data for Medical Image Segmentation"></a>Using Diffusion Models to Generate Synthetic Labelled Data for Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16794">http://arxiv.org/abs/2310.16794</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dsaragih/diffuse-gen">https://github.com/dsaragih/diffuse-gen</a></li>
<li>paper_authors: Daniel Saragih, Pascal Tyrrell<br>for: Synthetic labeled polyp images were generated to augment automatic medical image segmentation models.methods: Diffusion models were used to generate and style synthetic labeled data.results: The generated images more closely resembled real images, as shown by lower FID scores compared to previous GAN methods. The segmentation model performed better when trained or augmented with synthetic data, as evidenced by higher DL and IoU scores.<details>
<summary>Abstract</summary>
In this paper, we proposed and evaluated a pipeline for generating synthetic labeled polyp images with the aim of augmenting automatic medical image segmentation models. In doing so, we explored the use of diffusion models to generate and style synthetic labeled data. The HyperKvasir dataset consisting of 1000 images of polyps in the human GI tract obtained from 2008 to 2016 during clinical endoscopies was used for training and testing. Furthermore, we did a qualitative expert review, and computed the Fr\'echet Inception Distance (FID) and Multi-Scale Structural Similarity (MS-SSIM) between the output images and the source images to evaluate our samples. To evaluate its augmentation potential, a segmentation model was trained with the synthetic data to compare their performance with the real data and previous Generative Adversarial Networks (GAN) methods. These models were evaluated using the Dice loss (DL) and Intersection over Union (IoU) score. Our pipeline generated images that more closely resembled real images according to the FID scores (GAN: $118.37 \pm 1.06 \text{ vs SD: } 65.99 \pm 0.37$). Improvements over GAN methods were seen on average when the segmenter was entirely trained (DL difference: $-0.0880 \pm 0.0170$, IoU difference: $0.0993 \pm 0.01493$) or augmented (DL difference: GAN $-0.1140 \pm 0.0900 \text{ vs SD }-0.1053 \pm 0.0981$, IoU difference: GAN $0.01533 \pm 0.03831 \text{ vs SD }0.0255 \pm 0.0454$) with synthetic data. Overall, we obtained more realistic synthetic images and improved segmentation model performance when fully or partially trained on synthetic data.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出并评估了一个涉及扩充自动医疗图像分割模型的数据生成管道。为此，我们利用了扩散模型来生成和风格化合成数据。我们使用的是HyperKvasir dataset，包含2008-2016年期间在临床endooscopy中获取的1000个肠道肿瘤图像。我们进行了专家评审，并计算了Fréchet Inception Distance（FID）和Multi-Scale Structural Similarity（MS-SSIM）指标来评估我们的样本。为了评估它的扩充潜力，我们使用合成数据训练了一个分割模型，并与实际数据和前一代生成对抗网络（GAN）方法进行比较。这些模型被评估使用Dice损失（DL）和交集 sobre union（IoU）分数。我们的管道生成的图像更加接近实际图像，根据FID分数（GAN：$118.37 \pm 1.06 \text{ vs SD: } 65.99 \pm 0.37）。在GAN方法上 average 的情况下，我们看到了使用合成数据训练的改善（DL差异：GAN $-0.1140 \pm 0.0900 \text{ vs SD }-0.1053 \pm 0.0981$, IoU差异：GAN $0.01533 \pm 0.03831 \text{ vs SD }0.0255 \pm 0.0454$）。总的来说，我们得到了更加真实的合成图像，并使分割模型在使用合成数据进行训练或扩充时表现得更好。
</details></li>
</ul>
<hr>
<h2 id="Single-pixel-imaging-based-on-deep-learning"><a href="#Single-pixel-imaging-based-on-deep-learning" class="headerlink" title="Single-pixel imaging based on deep learning"></a>Single-pixel imaging based on deep learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16869">http://arxiv.org/abs/2310.16869</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/molyswu/hand_detection">https://github.com/molyswu/hand_detection</a></li>
<li>paper_authors: Kai Song, Yaoxing Bian, Ku Wu, Hongrui Liu, Shuangping Han, Jiaming Li, Jiazhao Tian, Chengbin Qin, Jianyong Hu, Liantuan Xiao</li>
<li>for: 本文旨在概述基于深度学习的单像素成像技术的研究进展。</li>
<li>methods: 本文从单像素成像和深度学习的基本原理入手，详细介绍了基于深度学习的单像素成像技术的原理和实现方法。</li>
<li>results: 文献回顾了单像素成像基于深度学习的研究进展，包括超解像单像素成像、通过散射媒体的单像素成像、光子级单像素成像、基于单像素成像的光学加密、色彩单像素成像和无影像感测等领域的研究。<details>
<summary>Abstract</summary>
Since the advent of single-pixel imaging and machine learning, both fields have flourished, but followed parallel tracks. Until recently, machine learning, especially deep learning, has demonstrated effectiveness in delivering high-quality solutions across various application domains of single-pixel imaging. This article comprehensively reviews the research of single-pixel imaging technology based on deep learning. From the basic principles of single-pixel imaging and deep learning, the principles and implementation methods of single-pixel imaging based on deep learning are described in detail. Then, the research status and development trend of single-pixel imaging based on deep learning in various domains are analyzed, including super-resolution single-pixel imaging, single-pixel imaging through scattering media, photon-level single-pixel imaging, optical encryption based on single-pixel imaging, color single-pixel imaging, and image-free sensing. Finally, this review explores the limitations in the ongoing research, while offers the delivering insights into prospective avenues for future research.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)自单Pixel影像和机器学习出现以来，这两个领域都在繁荣，但是在平行的轨道上进行了渐进的发展。直到最近，深度学习在各种单Pixel影像应用领域中表现出了高质量的解决方案。本文对单Pixel影像技术基于深度学习的研究进行了详细的查询。从单Pixel影像和深度学习的基本原理到深度学习在单Pixel影像方面的实现方法，都是在文中进行了详细的介绍。然后，文中分析了基于深度学习的单Pixel影像在不同领域的研究状况和发展趋势，包括超分辨单Pixel影像、单Pixel影像 через散射媒体、光子级单Pixel影像、基于单Pixel影像的光学加密、彩色单Pixel影像和无图像感知。最后，文中探讨了当前研究的限制，并提供了未来研究的可能性和挑战。
</details></li>
</ul>
<hr>
<h2 id="TILT-topological-interface-recovery-in-limited-angle-tomography"><a href="#TILT-topological-interface-recovery-in-limited-angle-tomography" class="headerlink" title="TILT: topological interface recovery in limited-angle tomography"></a>TILT: topological interface recovery in limited-angle tomography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16557">http://arxiv.org/abs/2310.16557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elli Karvonen, Matti Lassas, Pekka Pankka, Samuli Siltanen</li>
<li>for:  solves the severely ill-posed inverse problem of limited-angle tomography</li>
<li>methods:  lifting the visible part of the wavefront set under a universal covering map, using dual-tree complex wavelets, a dedicated metric, and persistent homology</li>
<li>results:  not only a suggested invisible boundary but also a computational representation for all interfaces in the target<details>
<summary>Abstract</summary>
A novel reconstruction method is introduced for the severely ill-posed inverse problem of limited-angle tomography. It is well known that, depending on the available measurement, angles specify a subset of the wavefront set of the unknown target, while some oriented singularities remain invisible in the data. Topological Interface recovery for Limited-angle Tomography, or TILT, is based on lifting the visible part of the wavefront set under a universal covering map. In the space provided, it is possible to connect the appropriate pieces of the lifted wavefront set correctly using dual-tree complex wavelets, a dedicated metric, and persistent homology. The result is not only a suggested invisible boundary but also a computational representation for all interfaces in the target.
</details>
<details>
<summary>摘要</summary>
新的重建方法被介绍用于受限角度 computed tomography 的强度不确定问题。已知，根据可用的测量，角度Specify a subset of the wavefront set of the unknown target, while some oriented singularities remain invisible in the data。TILT 基于提升可见部分的波front set under a universal covering map。在提供的空间中，可以正确地连接相应的部分 lifting wavefront set using dual-tree complex wavelets, a dedicated metric, and persistent homology。结果不仅是一个建议的隐藏边界，还是一个计算表示所有界面在目标中。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/25/eess.IV_2023_10_25/" data-id="clorjzlh0018wf18809gx2n4a" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_10_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/25/eess.SP_2023_10_25/" class="article-date">
  <time datetime="2023-10-25T08:00:00.000Z" itemprop="datePublished">2023-10-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/25/eess.SP_2023_10_25/">eess.SP - 2023-10-25</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Mode-Selection-and-Target-Classification-in-Cognitive-Radar-Networks"><a href="#Mode-Selection-and-Target-Classification-in-Cognitive-Radar-Networks" class="headerlink" title="Mode Selection and Target Classification in Cognitive Radar Networks"></a>Mode Selection and Target Classification in Cognitive Radar Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17006">http://arxiv.org/abs/2310.17006</a></li>
<li>repo_url: None</li>
<li>paper_authors: William W. Howard, Samuel R. Shebert, Benjamin H. Kirk, R. Michael Buehrer</li>
<li>For: The paper proposes a cognitive radar network that leverages the adaptability of cognitive radar networks to trade between active radar observation and passive signal parameter estimation, and learns the optimal action choices for each type of target.* Methods: The paper uses a multi-armed bandit model with current class information as prior information to select between available actions, and estimates the physical behavior of targets through radar emissions when the active radar action is selected, and estimates the radio behavior of targets through passive sensing when the passive action is selected.* Results: The network collects observed behavior of targets and forms clusters of similarly-behaved targets, and meta-learns the target class distributions while learning the optimal mode selections for each target class.Here are the three points in Simplified Chinese text:* For: 该 paper 提出了一种基于认知雷达网络的方法，以便在不同的目标类型下选择最佳的行动。* Methods: 该 paper 使用了一种多重武器模型，并使用当前类信息作为先验信息来选择可用的行动。当选择活动雷达时，节点通过雷达发射来估计目标的物理行为；当选择被动时，节点通过感知来估计目标的电磁行为。* Results: 网络通过收集目标的行为观察并组织类似目标的集群，从而meta-学习目标类型分布，同时学习每个目标类型的优化模式选择。<details>
<summary>Abstract</summary>
Cognitive Radar Networks were proposed by Simon Haykin in 2006 to address problems with large legacy radar implementations - primarily, single-point vulnerabilities and lack of adaptability. This work proposes to leverage the adaptability of cognitive radar networks to trade between active radar observation, which uses high power and risks interception, and passive signal parameter estimation, which uses target emissions to gain side information and lower the power necessary to accurately track multiple targets. The goal of the network is to learn over many target tracks both the characteristics of the targets as well as the optimal action choices for each type of target. In order to select between the available actions, we utilize a multi-armed bandit model, using current class information as prior information. When the active radar action is selected, the node estimates the physical behavior of targets through the radar emissions. When the passive action is selected, the node estimates the radio behavior of targets through passive sensing. Over many target tracks, the network collects the observed behavior of targets and forms clusters of similarly-behaved targets. In this way, the network meta-learns the target class distributions while learning the optimal mode selections for each target class.
</details>
<details>
<summary>摘要</summary>
cognitive radar networks 由谢韦金（Simon Haykin）在2006年提出，以解决传统雷达实施中的单点漏洞和不可靠性问题。这项工作提议利用智能雷达网络的适应性，在高功率和风险 intercept 的 aktive雷达观测和低功率的 passive信号参数估算之间进行交换。网络的目标是通过多个目标轨迹学习target的特征和最佳行为选择。为选择可用的行动，我们使用多重武器模型，使用当前类信息作为先验信息。当选择 active雷达动作时，节点估算目标的物理行为通过雷达发射。当选择 passive动作时，节点估算目标的电磁行为通过被动探测。在许多目标轨迹中，网络收集了目标的观测行为，并将其分为类似目标类型的集群。这样，网络可以meta-学习目标类型的分布，同时学习每个目标类型的优化模式选择。
</details></li>
</ul>
<hr>
<h2 id="Neural-Distributed-Compressor-Discovers-Binning"><a href="#Neural-Distributed-Compressor-Discovers-Binning" class="headerlink" title="Neural Distributed Compressor Discovers Binning"></a>Neural Distributed Compressor Discovers Binning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16961">http://arxiv.org/abs/2310.16961</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ezgi Ozyilkan, Johannes Ballé, Elza Erkip</li>
<li>for: 这种研究是为了解决分布式源编码中的吞吐率问题，具体来说是威纳-赫茨问题。</li>
<li>methods: 这种研究使用机器学习的方法，特别是变量量量化，来实现数据驱动的压缩方案。</li>
<li>results: 研究发现，使用这种数据驱动的压缩方案可以Recover一些理想的理论解的特点，如源空间中的分割和使用侧 информацию进行最优的组合。这些行为 emerge although 没有直接使用源分布的知识。<details>
<summary>Abstract</summary>
We consider lossy compression of an information source when the decoder has lossless access to a correlated one. This setup, also known as the Wyner-Ziv problem, is a special case of distributed source coding. To this day, practical approaches for the Wyner-Ziv problem have neither been fully developed nor heavily investigated. We propose a data-driven method based on machine learning that leverages the universal function approximation capability of artificial neural networks. We find that our neural network-based compression scheme, based on variational vector quantization, recovers some principles of the optimum theoretical solution of the Wyner-Ziv setup, such as binning in the source space as well as optimal combination of the quantization index and side information, for exemplary sources. These behaviors emerge although no structure exploiting knowledge of the source distributions was imposed. Binning is a widely used tool in information theoretic proofs and methods, and to our knowledge, this is the first time it has been explicitly observed to emerge from data-driven learning.
</details>
<details>
<summary>摘要</summary>
我们考虑在损失压缩的资料来源中进行损失less压缩，当decoder有无损失的存取相关的一来。这个设置称为吴纽-兹维问题，是分布式源码编码的特殊情况。至今为止，实用的方法 для吴纽-兹维问题仍未被完全开发或严重调查。我们提议一个基于机器学习的数据驱动方法，利用人工神经网络的通用函数近似能力。我们发现，我们的神经网络压缩方案，基于可变量化，可以重新现出一些吴纽-兹维问题的理想解答，例如在源空间中的binning以及对于副信息的优化 комбина�tion。这些行为 emerge，即使没有采用知情源分布的结构化知识。binning是信息论中广泛使用的工具，而且，根据我们所知，这是第一次由数据驱动学习中明示地观察到这种行为。
</details></li>
</ul>
<hr>
<h2 id="How-to-Extend-3D-GBSM-to-Integrated-Sensing-and-Communication-Channel-with-Sharing-Feature"><a href="#How-to-Extend-3D-GBSM-to-Integrated-Sensing-and-Communication-Channel-with-Sharing-Feature" class="headerlink" title="How to Extend 3D GBSM to Integrated Sensing and Communication Channel with Sharing Feature?"></a>How to Extend 3D GBSM to Integrated Sensing and Communication Channel with Sharing Feature?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16765">http://arxiv.org/abs/2310.16765</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yameng Liu, Jianhua Zhang, Yuxiang Zhang, Huiwen Gong, Tao Jiang, Guangyi Liu</li>
<li>for: This paper is written to support the development of Integrated Sensing and Communication (ISAC) technology in 6G systems, specifically by extending the existing 3D Geometry-Based Stochastic Model (GBSM) to include sensing channels.</li>
<li>methods: The paper proposes a new ISAC channel model that captures the sharing feature of both communication and sensing channels, including shared scatterers, clusters, paths, and similar propagation parameters. The model is based on a cascade of TX-target, radar cross section, and target-RX, with a novel parameter S for shared target extraction.</li>
<li>results: The proposed ISAC channel implementation framework offers flexible configuration of sharing feature and the joint generation of communication and sensing channels, and is compatible with the 3GPP standards, offering promising support for ISAC technology evaluation.<details>
<summary>Abstract</summary>
Integrated Sensing and Communication (ISAC) is a promising technology in 6G systems. The existing 3D Geometry-Based Stochastic Model (GBSM), as standardized for 5G systems, addresses solely communication channels and lacks consideration of the integration with sensing channel. Therefore, this letter extends 3D GBSM to support ISAC research, with a particular focus on capturing the sharing feature of both channels, including shared scatterers, clusters, paths, and similar propagation param-eters, which have been experimentally verified in the literature. The proposed approach can be summarized as follows: Firstly, an ISAC channel model is proposed, where shared and non-shared components are superimposed for both communication and sensing. Secondly, sensing channel is characterized as a cascade of TX-target, radar cross section, and target-RX, with the introduction of a novel parameter S for shared target extraction. Finally, an ISAC channel implementation framework is proposed, allowing flexible configuration of sharing feature and the joint generation of communication and sensing channels. The proposed ISAC channel model can be compatible with the 3GPP standards and offers promising support for ISAC technology evaluation.
</details>
<details>
<summary>摘要</summary>
《集成感知通信（ISAC）技术在6G系统中具有极大潜力。现有的3DGeometry-Based Stochastic Model（GBSM），为5G系统制定的标准，仅考虑了通信频道，不考虑感知频道的integration。因此，本文extend GBSM，以支持ISAC研究，特别是捕捉两个频道之间的共享特征，包括共享雷达目标、群集、路径和相似的传播参数，这些参数在文献中经过实验验证。 proposeapproach可以概括为以下三个步骤：1. 建立ISAC通信频道模型，其中共享和非共享组成部分相互重叠。2. 描述感知频道为TX-目标、雷达cross section和目标-RX的链式，并引入一个新参数S，用于捕捉共享目标。3. 提出ISAC通信频道实现框架，允许共享特征的灵活配置和共同生成通信和感知频道。提议的ISAC通信频道模型可以与3GPP标准兼容，并且对ISAC技术评估具有极大潜力。
</details></li>
</ul>
<hr>
<h2 id="Spherical-Wavefront-Near-Field-DoA-Estimation-in-THz-Automotive-Radar"><a href="#Spherical-Wavefront-Near-Field-DoA-Estimation-in-THz-Automotive-Radar" class="headerlink" title="Spherical Wavefront Near-Field DoA Estimation in THz Automotive Radar"></a>Spherical Wavefront Near-Field DoA Estimation in THz Automotive Radar</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16724">http://arxiv.org/abs/2310.16724</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmet M. Elbir, Kumar Vijay Mishra, Symeon Chatzinotas</li>
<li>for: 这项研究旨在探讨 THz 频率带汽车雷达的方向OF arrival（DoA）估计方法。</li>
<li>methods: 该研究提出了一种使用多信号分类（MUSIC）算法来估计目标DoA和距离，同时也考虑了near-field中的板缩影响。</li>
<li>results: numerical experiments表明该方法可以准确地估计目标参数。<details>
<summary>Abstract</summary>
Automotive radar at terahertz (THz) band has the potential to provide compact design. The availability of wide bandwidth at THz-band leads to high range resolution. Further, very narrow beamwidth arising from large arrays yields high angular resolution up to milli-degree level direction-of-arrival (DoA) estimation. At THz frequencies and extremely large arrays, the signal wavefront is spherical in the near-field that renders traditional far-field DoA estimation techniques unusable. In this work, we examine near-field DoA estimation for THz automotive radar. We propose an algorithm using multiple signal classification (MUSIC) to estimate target DoAs and ranges while also taking beam-squint in near-field into account. Using an array transformation approach, we compensate for near-field beam-squint in noise subspace computations to construct the beam-squint-free MUSIC spectra. Numerical experiments show the effectiveness of the proposed method to accurately estimate the target parameters.
</details>
<details>
<summary>摘要</summary>
自动驱动radar在tera哈勒tz（THz）频带有可能提供更加紧凑的设计。 THz频带的宽频率导致高分辨率范围。此外，非常窄的扫描幅由大型阵列产生，使得高度分辨率的方向来源估计（DoA）。在THz频率和极其大的阵列下，信号波front在近场是球形的，使得传统的远场DoA估计技术无法使用。在这种情况下，我们研究了THz自动驱动radar的近场DoA估计。我们提出了使用多个信号分类（MUSIC）算法来估计目标DoAs和距离，同时也考虑近场扫描幅的影响。使用阵列变换方法，我们在噪声空间计算中补做近场扫描幅的影响，构建了扫描幅自由的MUSIC谱。 numerically experiments show the effectiveness of the proposed method to accurately estimate the target parameters.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Power-Optimization-in-Satellite-Communication-Using-Multi-Intelligent-Reflecting-Surfaces"><a href="#Power-Optimization-in-Satellite-Communication-Using-Multi-Intelligent-Reflecting-Surfaces" class="headerlink" title="Power Optimization in Satellite Communication Using Multi-Intelligent Reflecting Surfaces"></a>Power Optimization in Satellite Communication Using Multi-Intelligent Reflecting Surfaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16625">http://arxiv.org/abs/2310.16625</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Ihsan Khalil</li>
<li>for: 这个研究旨在提高卫星到地面通信系统的能源效率，通过集成多个反射智能表面（RIS）的两个创新方法。</li>
<li>methods: 这两个方法包括对问题的分解，首先将RIS元素的相位调整为最大化电力接收，然后使用选择多标点对RIS元素进行最大化电力评估。第二个任务是使用二进制线性规划问题来实现最低功耗，并使用二进制粒子群推导（BPSO）技术来解决。</li>
<li>results: 研究发现这两个方法可以实现卫星到地面通信系统的能源效率提高，并且在实际运行中可以实现可 COUNTING 的能源储存。<details>
<summary>Abstract</summary>
This study introduces two innovative methodologies aimed at augmenting energy efficiency in satellite-to-ground communication systems through the integration of multiple Reflective Intelligent Surfaces (RISs). The primary objective of these methodologies is to optimize overall energy efficiency under two distinct scenarios. In the first scenario, denoted as Ideal Environment (IE), we enhance energy efficiency by decomposing the problem into two sub-optimal tasks. The initial task concentrates on maximizing power reception by precisely adjusting the phase shift of each RIS element, followed by the implementation of Selective Diversity to identify the RIS element delivering maximal power. The second task entails minimizing power consumption, formulated as a binary linear programming problem, and addressed using the Binary Particle Swarm Optimization (BPSO) technique. The IE scenario presupposes an environment where signals propagate without any path loss, serving as a foundational benchmark for theoretical evaluations that elucidate the systems optimal capabilities. Conversely, the second scenario, termed Non-Ideal Environment (NIE), is designed for situations where signal transmission is subject to path loss. Within this framework, the Adam algorithm is utilized to optimize energy efficiency. This non ideal setting provides a pragmatic assessment of the systems capabilities under conventional operational conditions. Both scenarios emphasize the potential energy savings achievable by the satellite RIS system. Empirical simulations further corroborate the robustness and effectiveness of our approach, highlighting its potential to enhance energy efficiency in satellite-to-ground communication systems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Pilot-Based-Uplink-Power-Control-in-Single-UE-Massive-MIMO-Systems-With-1-Bit-ADCs"><a href="#Pilot-Based-Uplink-Power-Control-in-Single-UE-Massive-MIMO-Systems-With-1-Bit-ADCs" class="headerlink" title="Pilot-Based Uplink Power Control in Single-UE Massive MIMO Systems With 1-Bit ADCs"></a>Pilot-Based Uplink Power Control in Single-UE Massive MIMO Systems With 1-Bit ADCs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16601">http://arxiv.org/abs/2310.16601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amila Ravinath, Bikshapathi Gouda, Italo Atzeni, Antti Tölli</li>
<li>for: 提高大量多输入多输出系统中用户设备（UE）的传输功率控制精度。</li>
<li>methods: 使用1比特分数器和多普逻耳sequence设计了一种closed-loop上行功率控制方法，包括单shot和算法控制方法两种。</li>
<li>results: 比较conventional closed-loop上行功率控制方法，提出的方法具有更高的精度和更好的性能。<details>
<summary>Abstract</summary>
We propose uplink power control (PC) methods for massive multiple-input multiple-output systems with 1-bit analog-to-digital converters, which are specifically tailored to address the non-monotonic data detection performance with respect to the transmit power of the user equipment (UE). Considering a single UE, we design a multi-amplitude pilot sequence to capture the aforementioned non-monotonicity, which is utilized at the base station to derive UE transmit power adjustments via single-shot or differential power control (DPC) techniques. Both methods enable closed-loop uplink PC using different feedback approaches. The single-shot method employs one-time multi-bit feedback, while the DPC method relies on continuous adjustments with 1-bit feedback. Numerical results demonstrate the superiority of the proposed schemes over conventional closed-loop uplink PC techniques.
</details>
<details>
<summary>摘要</summary>
我们提出了大量多输入多 outputs系统中的上传功率控制（PC）方法，特别是为了解决用户设备（UE）的传输功率与数据探测性的非单对数关系。对于单一UE，我们设计了多极性导航序列来捕捉上述非单对数关系，这些序列在基站端被用来 derivUE传输功率调整 via 单一射击或差分功率控制（DPC）技术。这两种方法均允许关闭loop上传PC使用不同的反馈方法。单一射击方法使用一次多位反馈，而DPC方法则靠 Continuous adjustments with 1-bit feedback。数字结果显示我们的提案方案比于传统关闭loop上传PC技术更有优势。
</details></li>
</ul>
<hr>
<h2 id="Terahertz-Enpowered-Communications-and-Sensing-in-6G-Systems-Opportunities-and-Challenges"><a href="#Terahertz-Enpowered-Communications-and-Sensing-in-6G-Systems-Opportunities-and-Challenges" class="headerlink" title="Terahertz-Enpowered Communications and Sensing in 6G Systems: Opportunities and Challenges"></a>Terahertz-Enpowered Communications and Sensing in 6G Systems: Opportunities and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16548">http://arxiv.org/abs/2310.16548</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Jiang, Hans D. Schotten</li>
<li>for: 这篇论文旨在探讨6G系统中使用THz频段的可能性和挑战。</li>
<li>methods: 论文提出了一些可能的THz频段应用和挑战，包括通信和感知、定位和成像等方面。</li>
<li>results: 论文未提出具体的结果，主要是为了探讨6G系统中THz频段的可能性和挑战。<details>
<summary>Abstract</summary>
The current focus of academia and the telecommunications industry has been shifted to the development of the six-generation (6G) cellular technology, also formally referred to as IMT-2030. Unprecedented applications that 6G aims to accommodate demand extreme communications performance and, in addition, disruptive capabilities such as network sensing. Recently, there has been a surge of interest in terahertz (THz) frequencies as it offers not only massive spectral resources for communication but also distinct advantages in sensing, positioning, and imaging. The aim of this paper is to provide a brief outlook on opportunities opened by this under-exploited band and challenges that must be addressed to materialize the potential of THz-based communications and sensing in 6G systems.
</details>
<details>
<summary>摘要</summary>
现在学术界和电信产业的焦点已经转移到第六代（6G）无线技术的开发，即IMT-2030。6G旨在满足极高通信性能的应用需求，同时还具有破坏性能，如网络感知。最近，人们对tera兆赫兹（THz）频率的利用表现出了很大的兴趣，因为它不仅提供了巨大的频率资源 для通信，而且在感知、定位和成像方面具有明显的优势。本文的目的是提供6G系统中THz频率的可能性和挑战的简要预测。
</details></li>
</ul>
<hr>
<h2 id="Transmitting-Data-Through-Reconfigurable-Intelligent-Surface-A-Spatial-Sigma-Delta-Modulation-Approach"><a href="#Transmitting-Data-Through-Reconfigurable-Intelligent-Surface-A-Spatial-Sigma-Delta-Modulation-Approach" class="headerlink" title="Transmitting Data Through Reconfigurable Intelligent Surface: A Spatial Sigma-Delta Modulation Approach"></a>Transmitting Data Through Reconfigurable Intelligent Surface: A Spatial Sigma-Delta Modulation Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16347">http://arxiv.org/abs/2310.16347</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wai-Yiu Keung, Hei Victor Cheng, Wing-Kin Ma</li>
<li>for: 这个论文旨在解决未来能效通信系统中数据传输中使用可重新配置智能表面（RIS）的问题。</li>
<li>methods: 该论文使用了一种名为Sigma-Delta（$\Sigma\Delta$)干扰模ulasi的方法，该方法在空间域中应用首级Sigma-Delta干扰来处理相位量化。</li>
<li>results: 该论文的实验结果显示，使用Sigma-Delta干扰模ulasi可以实现与不量化ZF方案相同的比特错误率性能。<details>
<summary>Abstract</summary>
Transmitting data using the phases on reconfigurable intelligent surfaces (RIS) is a promising solution for future energy-efficient communication systems. Recent work showed that a virtual phased massive multiuser multiple-input-multiple-out (MIMO) transmitter can be formed using only one active antenna and a large passive RIS. In this paper, we are interested in using such a system to perform MIMO downlink precoding. In this context, we may not be able to apply conventional MIMO precoding schemes, such as the simple zero-forcing (ZF) scheme, and we typically need to design the phase signals by solving optimization problems with constant modulus constraints or with discrete phase constraints, which pose challenges with high computational complexities. In this work, we propose an alternative approach based on Sigma-Delta ($\Sigma\Delta$) modulation, which is classically famous for its noise-shaping ability. Specifically, first-order $\Sigma\Delta$ modulation is applied in the spatial domain to handle phase quantization in generating constant envelope signals. Under some mild assumptions, the proposed phased $\Sigma\Delta$ modulator allows us to use the ZF scheme to synthesize the RIS reflection phases with negligible complexity. The proposed approach is empirically shown to achieve comparable bit error rate performance to the unquantized ZF scheme.
</details>
<details>
<summary>摘要</summary>
通过可重新配置智能表面（RIS）传输数据是未来能效通信系统的承诺之一。最近的研究表明，只需一个活动天线和一大串pascal RIS可以形成虚拟 phase massive MIMO发射器。在这篇论文中，我们关心使用这种系统来执行MIMO下降干扰。在这种情况下，我们通常无法采用传统的MIMO预处理方案，如简单的零偏置（ZF）方案，而是需要设计相位信号通过优化问题的解决，这会带来高计算复杂度的挑战。在这种情况下，我们提议使用Sigma-Delta（$\Sigma\Delta$)模ulation，这是经典知名的噪声定向技术。 Specifically, we apply first-order $\Sigma\Delta$ modulation in the spatial domain to handle phase quantization in generating constant envelope signals. Under some mild assumptions, the proposed phased $\Sigma\Delta$ modulator allows us to use the ZF scheme to synthesize the RIS reflection phases with negligible complexity. The proposed approach is empirically shown to achieve comparable bit error rate performance to the unquantized ZF scheme.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/25/eess.SP_2023_10_25/" data-id="clorjzlil01cof188678o5fo4" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_10_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/24/cs.SD_2023_10_24/" class="article-date">
  <time datetime="2023-10-24T15:00:00.000Z" itemprop="datePublished">2023-10-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/24/cs.SD_2023_10_24/">cs.SD - 2023-10-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="IA-Para-el-Mantenimiento-Predictivo-en-Canteras-Modelado"><a href="#IA-Para-el-Mantenimiento-Predictivo-en-Canteras-Modelado" class="headerlink" title="IA Para el Mantenimiento Predictivo en Canteras: Modelado"></a>IA Para el Mantenimiento Predictivo en Canteras: Modelado</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16140">http://arxiv.org/abs/2310.16140</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fernando Marcos, Rodrigo Tamaki, Mateo Cámara, Virginia Yagüe, José Luis Blanco</li>
<li>for: 该论文目的是优化采矿业中的操作。</li>
<li>methods: 该论文使用无监督学习方法，训练一个变量自动编码器模型，使其能够从处理线操作时录制的听录中提取有用信息。</li>
<li>results: 研究结果表明，该模型能够将录制的听录重建并表示在隐藏空间中，并能够捕捉操作条件之间和设备之间的差异。未来，这可能会促进听录的分类和机器衰老的探测。<details>
<summary>Abstract</summary>
Dependence on raw materials, especially in the mining sector, is a key part of today's economy. Aggregates are vital, being the second most used raw material after water. Digitally transforming this sector is key to optimizing operations. However, supervision and maintenance (predictive and corrective) are challenges little explored in this sector, due to the particularities of the sector, machinery and environmental conditions. All this, despite the successes achieved in other scenarios in monitoring with acoustic and contact sensors. We present an unsupervised learning scheme that trains a variational autoencoder model on a set of sound records. This is the first such dataset collected during processing plant operations, containing information from different points of the processing line. Our results demonstrate the model's ability to reconstruct and represent in latent space the recorded sounds, the differences in operating conditions and between different equipment. In the future, this should facilitate the classification of sounds, as well as the detection of anomalies and degradation patterns in the operation of the machinery.
</details>
<details>
<summary>摘要</summary>
现代经济中Raw materials的依赖性，特别是采矿业，是非常重要的。各种粒子材料是第二重要的原材料，占用率很高。通过数字化转型，可以优化操作。但是，监督和维护（预测和修复）在这个领域尚未得到充分的探索，这主要归结于采矿业的特殊性、机器和环境条件。尽管在其他场景中监测器和接触传感器已经取得了成功，但这些成果在采矿业中尚未得到充分利用。我们提出了一种不监督学习方案，通过对一组声音记录进行训练，并将其模型化为一种变分自动机器学习模型。这是首次在处理厂操作过程中收集的声音数据集，包含不同点检测的声音信息。我们的结果表明模型能够重建和表示声音记录中的差异和不同设备的操作条件。未来，这将使得声音的分类和机器设备的磨损和腐食特征的检测变得更加容易。
</details></li>
</ul>
<hr>
<h2 id="CDSD-Chinese-Dysarthria-Speech-Database"><a href="#CDSD-Chinese-Dysarthria-Speech-Database" class="headerlink" title="CDSD: Chinese Dysarthria Speech Database"></a>CDSD: Chinese Dysarthria Speech Database</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15930">http://arxiv.org/abs/2310.15930</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengyi Sun, Ming Gao, Xinchen Kang, Shiru Wang, Jun Du, Dengfeng Yao, Su-Jing Wang</li>
<li>For: The paper is written for researchers and professionals working in the field of dysarthria, specifically those interested in speech recognition and dysarthric speech.* Methods: The paper describes the data collection and annotation processes for the Chinese Dysarthria Speech Database (CDSD), as well as an approach for establishing a baseline for dysarthric speech recognition. The authors also conducted a speaker-dependent dysarthric speech recognition experiment using additional data from one participant.* Results: The paper reports that extensive data-driven model training and fine-tuning limited quantities of specific individual data can yield commendable results in speaker-dependent dysarthric speech recognition. However, the authors observe significant variations in recognition results among different dysarthric speakers.Here is the information in Simplified Chinese text:* For: 这篇论文是为了探讨异常语音障碍（dysarthria）研究而写的，特别是关注语音识别和异常语音识别。* Methods: 论文描述了中国异常语音语音库（CDSD）的数据收集和标注过程，以及一种建立异常语音识别基线的方法。作者还进行了一个参与者具体语音识别实验。* Results: 论文发现，通过大量数据驱动模型训练和特定个体数据精细调整，可以在参与者具体语音识别中获得了可夸dp的成绩。然而，作者发现异常语音 speaker之间的识别结果存在显著的差异。这些发现可以作为异常语音识别的参考点。<details>
<summary>Abstract</summary>
We present the Chinese Dysarthria Speech Database (CDSD) as a valuable resource for dysarthria research. This database comprises speech data from 24 participants with dysarthria. Among these participants, one recorded an additional 10 hours of speech data, while each recorded one hour, resulting in 34 hours of speech material. To accommodate participants with varying cognitive levels, our text pool primarily consists of content from the AISHELL-1 dataset and speeches by primary and secondary school students. When participants read these texts, they must use a mobile device or the ZOOM F8n multi-track field recorder to record their speeches. In this paper, we elucidate the data collection and annotation processes and present an approach for establishing a baseline for dysarthric speech recognition. Furthermore, we conducted a speaker-dependent dysarthric speech recognition experiment using an additional 10 hours of speech data from one of our participants. Our research findings indicate that, through extensive data-driven model training, fine-tuning limited quantities of specific individual data yields commendable results in speaker-dependent dysarthric speech recognition. However, we observe significant variations in recognition results among different dysarthric speakers. These insights provide valuable reference points for speaker-dependent dysarthric speech recognition.
</details>
<details>
<summary>摘要</summary>
我们介绍中国带有肥瘤症（dysarthria）演说数据库（CDSD）作为肥瘤症研究的有价值资源。这个数据库包括24名参与者的肥瘤症演说数据。这些参与者中有一些录制了额外的10小时演说数据，而每个人录制了1小时演说，共计34小时的演说材料。为了适应参与者的不同认知水平，我们的文本池主要来自AISHELL-1数据集和primary和secondary学校学生的演说。当参与者阅读这些文本时，他们需要使用移动设备或ZOOM F8n多轨采集器来记录他们的演说。在这篇论文中，我们详细介绍了数据收集和注释过程，并提出了基准建立肥瘤症演说识别的方法。此外，我们通过使用一名参与者的额外10小时演说数据进行了一个参与者依存的肥瘤症演说识别实验。我们的研究发现，通过大量数据驱动模型训练和精细调整限量的具体个人数据，可以在参与者依存的肥瘤症演说识别中获得优秀的结果。但我们发现，不同的肥瘤症演说者之间存在显著的识别结果差异。这些发现提供了价值的参考点 для参与者依存的肥瘤症演说识别。
</details></li>
</ul>
<hr>
<h2 id="FOLEY-VAE-Generacion-de-efectos-de-audio-para-cine-con-inteligencia-artificial"><a href="#FOLEY-VAE-Generacion-de-efectos-de-audio-para-cine-con-inteligencia-artificial" class="headerlink" title="FOLEY-VAE: Generación de efectos de audio para cine con inteligencia artificial"></a>FOLEY-VAE: Generación de efectos de audio para cine con inteligencia artificial</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15663">http://arxiv.org/abs/2310.15663</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mateo Cámara, José Luis Blanco</li>
<li>for: 这个研究旨在开发一种基于变量自动编码器的界面，用于创新创造FOLEY效果。</li>
<li>methods: 该模型通过各种自然声音训练，可以在实时传输新的声音特征到预录的音频或 Microphone 捕集的语音。此外，它还允许用户在实时进行交互式修改潜在变量，以实现精细化和个性化的艺术调整。</li>
<li>results: 研究基于上一年度这同学会上的研究，分析了现有的 RAVE 模型（一种特性化于音频效果生成的变量自动编码器）。该模型在 Audio 效果生成方面取得了成功，包括电磁、科幻、水声等效果。这种创新的方法已经为西班牙第一部使用人工智能生成的短片电影做出了贡献，这个突破口显示了人工智能在电影制作中的潜在价值和创新潜力。<details>
<summary>Abstract</summary>
In this research, we present an interface based on Variational Autoencoders trained with a wide range of natural sounds for the innovative creation of Foley effects. The model can transfer new sound features to prerecorded audio or microphone-captured speech in real time. In addition, it allows interactive modification of latent variables, facilitating precise and customized artistic adjustments. Taking as a starting point our previous study on Variational Autoencoders presented at this same congress last year, we analyzed an existing implementation: RAVE [1]. This model has been specifically trained for audio effects production. Various audio effects have been successfully generated, ranging from electromagnetic, science fiction, and water sounds, among others published with this work. This innovative approach has been the basis for the artistic creation of the first Spanish short film with sound effects assisted by artificial intelligence. This milestone illustrates palpably the transformative potential of this technology in the film industry, opening the door to new possibilities for sound creation and the improvement of artistic quality in film productions.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们提出了基于变量自动编码器的界面，用于创新 FOLEY 效果。该模型可以在实时传输新的声音特征，并允许互动修改潜在变量，以达到精确和定制化的艺术调整。作为上一年在这同会议上发表的前一项研究的起点，我们分析了现有的实现：RAVE [1]。这个模型已经专门用于音频效果生成。它在不同的音频效果上取得了成功，包括电磁、科幻和水声等，与此研究一起发表。这一创新方法已经成为了艺术创作中使用人工智能帮助生成音效的首个 milestone，这个突破口将开启新的可能性，以提高电影制作中的艺术质量。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/24/cs.SD_2023_10_24/" data-id="clorjzlcz00yvf1884htw33bb" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/10/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><a class="page-number" href="/page/10/">10</a><span class="page-number current">11</span><a class="page-number" href="/page/12/">12</a><a class="page-number" href="/page/13/">13</a><span class="space">&hellip;</span><a class="page-number" href="/page/89/">89</a><a class="extend next" rel="next" href="/page/12/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">60</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">118</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">58</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
